# Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis, and LLMs Evaluations

Lifan Yuan\({}^{1}\), Yangyi Chen\({}^{2}\), Ganqu Cui\({}^{1}\), Hongcheng Gao\({}^{3}\),

**Fangyuan Zou\({}^{4}\), Xingyi Cheng\({}^{4}\), Heng Ji\({}^{2}\), Zhiyuan Liu\({}^{1}\), Maosong Sun\({}^{1}\)**

\({}^{1}\) NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing

\({}^{2}\) University of Illinois Urbana-Champaign

\({}^{3}\) University of Chinese Academy of Sciences \({}^{4}\) Tencent

lievanyuan173@gmail.com

Corresponding Author.

###### Abstract

This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce **BOSS**, a **B**enchmark suite for **O**ut-of-distribution robustness**SS** evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pre-trained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at https://github.com/lifan-yuan/OOD_NLP.

## 1 Introduction

Pretrained language models (PLMs) have excelled in downstream tasks and gained widespread adoption [24, 60]. However, existing evaluation often assumes independent and identically distributed (i.i.d) condition [94, 92], which is often violated in real-world scenarios, highlighting the crucial problem of out-of-distribution (OOD) robustness in NLP models. In this paper, we first revisit the evaluation of PLMs through an examination of evaluation benchmarks. Thereafter, we delve into the ID-OOD performance correlation of fine-tuned models by adopting various model scales, training steps, available training samples, and tunable parameters. Finally, we conduct extensive evaluations of current robustness-enhanced methods and large language models (LLMs).

**Definition.** There exist multiple definitions of OOD in literature [2, 115], and we define distribution shifts considered in this paper from two perspectives. Firstly, [2] classifies distribution shifts into "semantic shift" and "background shift". Our use of "out-of-distribution" aligns with the concept of"background shift", which involves changes in the domain or style of the text while preserving the semantic content. Secondly, [115] formally defines three types of distribution shifts: covariate shift, label shift, and concept shift. In our work, we mainly focus on the combination of covariate shift and concept shift. This indicates that the model needs to generalize well to different input features (a.k.a, covariate shift) and adapt to variations in the underlying concepts within the data (a.k.a, concept shift).

**Benchmark.** Our study begins by surveying existing literature on OOD robustness in NLP (Table 8). We observe a lack of standardized OOD benchmark suites tailored for NLP since there is no single uniform set of datasets for evaluation, resulting in the adoption of heuristic and popularity-based dataset selection strategies in previous work [39, 109, 107]. This approach suffers from two main drawbacks: (1) The selected OOD datasets may come from similar distributions as the ID dataset, reducing the OOD evaluation to ID settings and thus hindering rigorous OOD robustness evaluation; (2) The challenge stemming from distribution shifts is limited, deviating from the expectation for difficulty posed by benchmark construction principles [8] and potentially leading to an overestimation of the OOD robustness of language models. As a result, current OOD robustness benchmarks may inadequately assess NLP models.

To address the aforementioned concerns, we establish a protocol as shown in Figure 1, consisting of three fundamental principles, for selecting both ID and OOD datasets: (1) ID datasets should be large and diverse for comprehensive knowledge; (2) Selection for OOD datasets should prioritize distinct distributions and dissimilarity, regarding text sources and semantics; (3) Challenging distribution shifts should be prioritized based on performance degradation, to ensure that the benchmark stands the test of time [8]. Based on the protocol, we compile **BOSS**, a more holistic and challenging NLP **B**enchmark suite for OOD robustne**SS** evaluation. Unlike existing benchmarks which only consider single task types such as classification [39, 107] or reading comprehension [109], BOSS covers a wider range of task formats, including sentiment analysis, toxic detection, and natural language inference for classification, name entity recognition for structured prediction, and extractive question answering for reading comprehension. We establish one ID and three corresponding OOD datasets for each task.

**Analysis.** We recognize the lack of analysis of models' learning behaviors regarding ID performance and OOD generalization in the field of NLP, hindering the development and understanding of OOD robustness. Thus, we investigate the correlation between the performance on ID and OOD datasets using the BOSS benchmark. To regulate the ID performance, we manipulate four related factors, i.e. model scale, training steps, available training samples, and tunable parameters. Three typical categories of ID-OOD correlation are observed, namely, monotonic linear positive correlation, monotonic piecewise linear positive correlation, and non-monotonic V-shaped correlation (see Figure 2). We discuss the potential reasons for the causes of these identified correlations in section 3.

**Evaluations.** After examining the learning mechanism of PLMs in vanilla fine-tuning, we scrutinize their performance with existing robustness-enhanced methods and then proceed to prevalent LLMs. Due to the absence of a standard benchmark, previous evaluations of existing methods can be

Figure 1: The comparison of previous work and our protocol on dataset selection of OOD benchmarks.

imprecise and thus misleading the estimations of progress in this field. Moreover, given the increasing focus on LLMs [9; 89] in NLP research, it is essential to evaluate their effectiveness in handling OOD challenges and explore the efficacy of different adaptation paradigms.

For robustness-enhanced methods, we evaluate five representative methods [99] on BOSS. Our main observation is that vanilla fine-tuning (a.k.a, empirical risk minimization) remains a strong baseline, while certain methods may slightly improve OOD performance in some cases. We further evaluate various LLMs and adaptation paradigms. We consider three recent prevailing LLMs, namely LLaMA [89], OpenAI text-davinci-003 [9], and OpenAI gpt-3.5-turbo. We include two relatively smaller models T0-3B [81] and T5-3B [77] for comparison. We apply zero-shot inference, in-context learning, few-shot fine-tuning, and full-data fine-tuning to one or multiple models. Through our experiments, we find that when provided with enough training data, fine-tuning domain-specific models remain the preferable choices for handling ID examples, while leveraging LLMs with in-context learning is superior for tackling OOD instances. In addition, we observe that the impact of in-context learning on generalization ability varies across models. We provide more detailed discussions in section 4.2.

## 2 BOSS Benchmark

### Motivation

NLP models should exhibit robustness across diverse distributions to ensure reliable applications. To achieve this, a standardized and recognized evaluation benchmark for OOD robustness is imperative. However, previous efforts in constructing benchmarks have predominantly relied on random selections and dataset popularity, lacking a systematic design [109; 39; 107]. Two deficiencies are thus identified: (1) Dataset similarity, as exemplified by the SST and IMDb datasets for sentiment analysis [83; 64], which share movie reviews and exhibit high semantic similarity (see Table 2). This blurs the line between ID and OOD evaluation, hindering rigorous assessment of OOD robustness; (2) Limited distribution shift challenges, exemplified by the high accuracy of a model trained on Amazon [65] when tested on IMDb (see Table 3). However, the significant performance drop on our considered Dynasent [75] suggests that OOD robustness still remains a critical problem in the sentiment analysis task. Thus, there is a need for universally applicable challenges across all dataset selections [8].

### Protocol to Construct OOD benchmark.

We aim to establish a standard benchmark for rigorous evaluation of OOD robustness in NLP. To address the above issues, we first survey and gather existing candidate datasets from Paperswithcode2, Kaggle3, and ACL Anthology4 websites. We consider the release date and public availability of datasets. Then we carefully examine three criteria to determine the ID and corresponding OOD datasets. The first criterion focuses on the ID dataset selection, and the other two criteria are proposed for OOD datasets, targeting the two issues in previous work, respectively.

Footnote 2: https://paperswithcode.com/datasets

Footnote 3: https://www.kaggle.com

Footnote 4: https://aclanthology.org/

**The ID dataset should provide sufficient knowledge for models to handle the task.** ID dataset should encompass comprehensive task-level knowledge [44], enabling models to grasp the underlying rationale necessary for task completion. Alternatively, if the model exclusively learns biased features, it may struggle to adapt to other features during distribution shifts. To this end, it is necessary for the ID datasets to possess the following characteristics: (1) Sufficiently large size; (2) Diversity, which is achieved through collection from multiple sources or the inclusion of several subtypes (i.e., styles, topics, levels of formality, et al). Our intuition is in line with [87], which demonstrates that training on large and diverse datasets improves the robustness of vision models.

**Datasets within a given task should originate from diverse distributions for a holistic evaluation.** We guarantee this through qualitative analysis of data source diversity and quantitative measurement of semantic similarity using SimCSE [31]. To avoid overlap, we select at most one dataset per text source. Additionally, we ensure OOD datasets in the benchmark exhibit relatively low semantic similarity, and thus enhancing distinctiveness.

**OOD shifts should be challenging to provide an accurate assessment of progress in OOD robustness [8].** To quantify the challenge, we train a model on the ID dataset and test it on all candidate datasets. Specifically, we tune a T5-large [77] with manual templates on four tasks, except for NER, on which we adopt DeBERTa-large [38] with conventional fine-tuning due to the lack of a standard prompt-based tuning schema for this task. _For this reason, all experiments in this paper follow this choice of model selection._ For each text source, we first discard candidates similar to the ID dataset in semantics. Then, to construct challenging distribution shifts, we prioritize the dataset provoking the most severe performance drop of the ID model and adopt it as the OOD dataset in our benchmark.

### Dataset Selection

We take sentiment analysis as a case to demonstrate how we select ID and OOD datasets for each task according to our protocol. The selection process for other tasks can be found in Appendix D.

**Candidate Datasets.** We first collect all sentiment analysis datasets on Paperswithcode, Kaggle, and ACL Anthology as aforementioned. We filter out datasets released before the 2010s, as they are largely resolved with the advent of pre-trained language models [25]. As a result, seven datasets remain as candidates, i.e., Amazon [65], DSC [48], Dynasent [75], IMDb [64], SemEval [70], SST [83], and Yelp [116]. Considering the inconsistency in the number of categories across the datasets, we align them by converting them into a three-class classification setting. See Appendix C.2 for a detailed explanation of the dataset processing procedure.

**Probing Experiments.** According to our protocol, dataset size and text sources are assessed for ID dataset selection. Subsequently, semantic similarity and ID model performance degradation guide OOD dataset selection. To this end, two probing experiments are conducted: (1) Comparing semantic similarity using SimCSE for candidate dataset pairs, and (2) Evaluating the performance of the selected ID model. In the first experiment, for better semantic representation, we resort to the best SimCSE model provided by [31], a supervised RoBERTa-large [60]. We load the model checkpoint from Huggingface5. For each dataset, we first encode each sample into a continuous embedding and then average the embeddings across the dataset to obtain a centroid representation of the dataset. Finally, we calculate the cosine similarity between a pair of centroids as the semantic similarity between two datasets. In the second experiment, we train a T5-large model on the selected ID dataset and evaluate its performance on all the candidate datasets.

Footnote 5: https://huggingface.co/princeton-nlp/sup-simcse-roberta-large

**Dataset Selection.** The dataset information and semantic similarities are provided in Table 1 and Table 2, respectively. The text sources of the datasets vary from product reviews, movie reviews, Twitter, and adversarial texts. We observe that datasets originating from the same source tend to exhibit higher SimCSE scores, indicating higher semantic similarity. It is worth noting that for IMDb and SST, the widely used ID-OOD dataset pair in sentiment analysis [39; 107], the SimCSE score demonstrates one of the highest levels among dataset pairs. This reinforces the first deficiency of previous benchmarks, where dataset pairs have similar semantics and unclear distribution shifts. Hence, in contrast to existing practices, our benchmark construction considers only one dataset from each source.

For the ID dataset selection, we first exclude DSC and IMDb since they are binary classification datasets, on which the trained model cannot tackle the unseen class neutral. For dataset size,

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Source} & \multirow{2}{*}{\# Classes} & \# Samples & \multicolumn{2}{c}{Avg. Length} \\  & & & Train & Test & Train & Test \\ \hline Amazon & Product & 3 & 30,000 & 38,905 & 71.69 & 54.84 \\ DSC & Product & 2 & 92,244 & 1,531 & 132.29 & 130.14 \\ Dynasent & Adversarial & 3 & 93,553 & 4,320 & 13.19 & 13.83 \\ IMDb & Movie & 2 & 25,000 & 25,000 & 233.79 & 228.53 \\ SemEval & Twitter & 3 & 6,000 & 20,622 & 19.44 & 19.62 \\ SST & Movie & 3 & 4,004 & 1,067 & 18.78 & 18.75 \\ Yelp & Product & 3 & 30,000 & 30,000 & 132.49 & 131.62 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Statistics of sentiment analysis candidate datasets.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline Train \(|\) Test & Amazon & DSC & Dynasent & IMDB & SemEval & SST & Yelp \\ \hline Amazon & **100** & 86.02 & 57.30 & 36.67 & 24.74 & 33.70 & 49.22 \\ DSC & 86.02 & **100** & 59.15 & 54.55 & 31.70 & 44.40 & 55.45 \\  & 57.30 & 59.15 & **100** & 32.69 & 28.17 & 19.68 & 88.99 \\ IMDb & 36.67 & 54.55 & 32.69 & **100** & 46.95 & 84.62 & 39.88 \\ SemEval & 24.74 & 31.70 & 28.17 & 46.95 & **100** & 40.45 & 24.03 \\ SST & 33.70 & 44.40 & 19.68 & 84.62 & 40.45 & **100** & 19.43 \\ Yelp & 49.22 & 55.45 & 88.99 & 39.88 & 24.03 & 19.43 & **100** \\ \hline \hline \end{tabular}
\end{table}
Table 2: SimCSE scores between each pair of datasets regarding the sentiment analysis task.

SemEval and SST are disregarded due to their limited number of samples per class (less than 10k). Among the remaining datasets, Amazon is chosen as the ID dataset for sentiment analysis as it encompasses reviews from 29 distinct product categories, offering greater diversity than Yelp.

For OOD datasets selection, we train a T5-large model on the ID dataset (i.e., Amazon) and evaluate it on all candidate datasets, as illustrated in Table 3. We include Dynasent and SemEval in the benchmark suite due to the following reasons: (1) They are the sole adversarial and Twitter datasets available, (2) They demonstrate low semantic similarity, and (3) They exhibit a notable performance degradation, making them crucial for evaluation. For movie reviews, SST is prioritized due to lower SimCSE scores compared to IMDb and larger performance drop of the ID model. Eventually, this yields three distinct and challenging distribution shifts in the sentiment analysis task: Amazon \(\rightarrow\) (DynaSent, SemEval, SST).

### Boss

Based on the aforementioned protocol, we introduce BOSS, an NLP benchmark suite for **O**OD robustness evaluation. BOSS comprises five essential NLP tasks: sentiment analysis (SA), toxic detection (TD), natural language inference (NLI), name entity recognition (NER), and extractive question answering (EQA). These tasks represent diverse practical applications and provide comprehensive coverage for evaluating models' capabilities, from aspects of classification, structured OOD datasets (see Table 4).

**Sentiment Analysis. Amazon**[65] contains reviews of 29 different categories of products from the Amazon website. **DynaSent**[75] first identifies naturally challenging sentences from several existing datasets, and then creates adversarial sentences with a human-and-model-in-the-loop annotation approach. **SemEval**[70] is a three-class sentiment analysis dataset focusing on tweets. **SST**[83] consists of sentence-level movie reviews from the Rotten Tomatoes website.

**Toxic Detection. Civil Comments**[6] contains public comments on the Civil Comments platform, with users from diverse groups and various subtypes of toxic texts. **AdvCivil**, a new toxic dataset introduced in this paper, is generated from Civil Comments by textual adversarial attacks in an automated model-in-the-loop adversarial pipeline. Please refer to Appendix C.1 for details. **Implicit Hate**[29] contains toxic tweets in both explicit and implicit forms. The latter can circumvent keyword-based toxic detection systems. **ToxiGen**[36] is synthesized by GPT-3 [9], covering several types of subtly and implicitly toxic texts on 13 minority groups.

**Natural Language Inference. MNLI**[102] provides ten different categories of written and verbal sentence pairs, with diverse styles, topics, and levels of formality. **ANLI**[73] is an adversarial dataset collected in a human-and-model-in-the-loop approach, where each premise mainly comes from Wikipedia and the hypothesis is generated by human adversaries. **ContractNLI**[49] considers each contract as a premise and holds a fixed set of hypotheses throughout the dataset. **WANLI**[59] is synthesized by GPT-3 [9], each example containing challenging patterns identified in MNLI.

**Name Entity Recognition. Few-NERD**[26], the arguably largest dataset for NER, labels about 188k Wikipedia sentences into eight coarse-grained entity types. **CoNLL**[88] takes stories from Reuters news, containing four basic entity types. **E-NER**[3] is based on legal text. We use the four-category version in this paper, which treats all legal entities as miscellaneous ones. **WNUT**[23] collects training data from Twitter and mines test data from Reddit, StackExchange, Twitter, and YouTube, containing six coarse-grained entity types in Few-NERD.

\begin{table}
\begin{tabular}{l|l|l|l} \hline \hline Task & ID Dataset & \multicolumn{3}{c}{OOD Datasets} \\ \hline SA & Amazon (AZ) & Dynasent (DS) & SemEval (SE) & SST (SST) \\ TD & Civil Comments (CC) & AdvCivil (AC) & Implicit Hate (IH) & Toxiction (TG) \\ NLI & MNLI (MN) & ANLI (AN) & ContractNLI (CN) & WANLI (WN) \\ NER & FewNERD (FN) & CoNLL (CoNLL) & E-NER (ENER) & WNUT (WNUT) \\ EQA & SQuAD (SQuAD) & AdvQA (AQA) & NewsQA (NQA) & SearchQA (QA) \\ \hline \hline \end{tabular}
\end{table}
Table 4: The datasets included in the BOSS benchmark. Corresponding abbreviations are shown in brackets.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline Train & Test & Amazon & DSC & Dynasent & IMDb & SemEval & SST & Yelp \\ \hline Amazon & **90.94** & 95.63 & 47.38 & 92.69 & 49.90 & 75.16 & 89.25 \\ \hline \hline \end{tabular}
\end{table}
Table 3: The OOD performance of the T5-large when trained on the Amazon dataset.

**Extractive Question Answering.** **SQuAD**[78] constructs question-answer pairs based on Wikipedia passages. **AdversarialQA**[4] composes adversarial questions for contexts in SQuAD in a human-and-model-in-the-loop procedure, similar to ANLI. **NewsQA**[90] writes questions for CNN news articles, each of which requires reasoning to answer, rather than relying solely on word overlap and textual entailment. **SearchQA**[28] adopts a reverse construction pipeline, employing the Google search engine to retrieve relevant contexts for each question-answering pair from the J!Archive website.

## 3 Analysis of OOD Robustness

Despite OOD robustness in NLP has been extensively studied [43], a potential concern pertains to the usage of nonstandard benchmarks, as discussed in Section 2, resulting in inaccurate conclusions. To address this issue, we conduct a series of empirical analyses and evaluations to gain in-depth insights into OOD robustness in NLP. Previous research primarily concentrates on method comparisons without delving into models' learning behaviors. Therefore, we first analyze the models' learning mechanism by assessing the correlation between ID and OOD performance.

**Setting.** We assess the correlation between ID and OOD performance across various conditions. We manipulate the ID performance of models by varying their scale, training steps, available training samples, and tunable parameters. Further implementation details can be found in Appendix E.1.1.

**Results.** We observe that the correlation between ID and OOD performance on datasets of the five tasks is inconsistent, but can be broadly categorized into three types (see Figure 2): monotonic linear positive correlation (**Type I**), monotonic piecewise linear positive correlation (**Type II**), and non-monotonic V-shaped correlation (**Type III**). We also identify an exceptional case in Figure 3, which does not fall into any of the three categories. The full results are shown in Figure 4.

**Type I.** This is the most prevalent type of correlation observed across all ID-OOD pairs for sentiment analysis, name entity recognition, and the majority for toxic detection. As shown in Figure 1(a), in this category, OOD performance is positively and linearly correlated with ID performance, indicating that the task knowledge learned on source distribution can be effectively generalized to other distributions. This observation is consistent with results in the computer vision domain [68], which shows that OOD performance is linearly correlated with ID performance across various model architectures, hyperparameters, training dataset size, and training duration. However, the slope of the line fitted by the least square method is less steep than the \(y=x\) diagram, and it eventually lies below the diagonal, implying that the performance degradation of models under distribution shift will be escalated with the increase of the ID performance.

**Type II.** This category is observed on ID-OOD pairs for extractive question answering. As presented in Figure 1(b), the results can be fitted into a polyline, indicating a piecewise linear correlation. The correlation between OOD performance and ID performance is positive and linear, with a notable differ

Figure 2: Three representative correlations between ID-OOD performance: (a) Type I (monotonic linear positive correlation) indicates consistent linear improvement of OOD performance with increasing ID performance. (b) Type II (monotonic piecewise linear positive correlation) exhibits accelerated OOD performance growth after a turning point. (c) Type III (non-monotonic V-shaped correlation) shows an initial negative correlation, followed by a positive correlation after a turning point. The \(r^{2}\) value in Figure (a) is 0.9677, and the values of the left and right fits in Figure (b) are 0.9553 and 0.9396 whereas in Figure (c) are 0.7690 and 0.8124 respectively.

ence in the slope before and after the turning point. Specifically, OOD performance demonstrates slow growth until the turning point, after which a minor increase in ID performance yields a substantial improvement in OOD performance. The observed trend may be attributed to the findings of [91], which indicates that models initially capture spurious correlations in ID datasets before acquiring comprehensive task knowledge. Consequently, models prioritize learning these spurious correlations to address the ID task, resulting in minimal improvements on OOD datasets. However, in the later stages of training, models progressively acquire greater task knowledge, leading to improved OOD performance.

**Type III.** The V-shaped fitted lines shown in Figure 1(c) mainly occurs on ID-OOD pairs of NLI tasks. This pattern is divided into two stages by a turning point in ID performance. In the first stage, OOD performance experiences worsening performance degradation during the distribution shift. However, in the second stage, the ID-OOD correlation becomes positive. This trend resembles the U-shaped scaling law of LLMs observed by [100], thus suggesting a shared explanation. [100] attributes this phenomenon to the "distractor task" in the dataset, apart from the "true task". Medium-capacity models may perform better than low-capacity models on the distractor task, which may harm performance on the "true task". As the model capability increases, it can ignore the distractor task and focus on improving performance on the true task. Here, we identify the distractor task in NLI datasets as detecting the word overlap or other spurious correlations between the premise and hypothesis.

**Outlier.** There is an exceptional case regarding the distribution shift from Civil Comments to AdvCivil (see Figure 3). The figure depicts two distinct lines, both exhibiting a monotonic linear negative correlation. This may stem from the model's increased reliance on spurious correlations and the adversarial nature of AdvCivil. Prior research suggests that models can learn non-robust features, such as spurious correlations, to enhance ID accuracy [44]. However, adversarial samples preserve the semantics of the original texts while introducing perturbations that eliminate spurious correlations. Hence, when the ID model becomes more dependent on spurious correlations during training, its performance degradation on adversarial samples intensifies.

## 4 Evaluation of OOD Robustness

### Robustness-enhanced Methods

After analyzing the learning behavior of PLMs under vanilla fine-tuning, we examine their performance when trained with other methods. Although massive methods have been proposed to improve the robustness of PLMs, their evaluations rely on non-standard benchmarks, which may result in inaccurate evaluations and impede progress clarity. Therefore, in this section, we first conduct extensive experiments to re-evaluate the effectiveness of diverse robustness-enhanced methods.

**Setting.** We consider the categories of robustness-enhanced methods summarized by [99]: data-driven, model and training-based, inductive-prior-based, and causal intervention methods. We select the most representative one from each category for evaluation. Specifically, we choose EDA [101] and FreeLB [118] for data-driven methods, label smoothing [84] and focal loss [58] for model and training-based methods, and model ensemble [16] for inductive-prior-based methods. We do not consider causal intervention methods since they are typically applied to low-resource scenarios. As explained in section 2.3, we apply the above methods to DeBERTa-base models for the NER task and to T5-base models for the other tasks.

**Results.** The results are shown in Table 5, where the mark '-' indicates that a certain method is not applicable to the task. We summarize the findings in the following takeaways:

_Takeaway 1: The vanilla fine-tuning (empirical risk minimization) remains a strong baseline._ Despite existing methods outperforming vanilla fine-tuning on certain datasets like E-NER, WNUT, and NewsQA, they show limited superiority or can potentially harm model performance. Specifically, only FreeLB demonstrates beneficial effects over half of the datasets, standing out as the most

Figure 3: The OOD performance exhibits a negative correlation with ID performance. Refer to Figure 2 for legends.

[MISSING_PAGE_FAIL:8]

better performance on most OOD datasets. This observation reinforces the view that large pre-trained models possess strong generalization capabilities, whereas, with sufficient training data, an accurate estimate of data distribution can be achieved even without a large number of parameters [76].

_Takeaway 2: In-context learning always brings no gains to the generalization ability of small models, while it generally helps Turbo and significantly improves LLaMA-series and Davinci3._ For small models like T5-3B, the performance of in-context learning is the same with or even worse than the zero-shot inference. For Turbo, providing ID examples for in-context learning presents advantages on nearly two-thirds of the datasets, with the NER task benefiting the most. For LLaMA-series and Davinci3, the superiority of in-context learning is prominent as it enhances performances on most of the datasets.

_Takeaway 3: Examples from ID datasets are generally more effective for in-context learning than those from the original training split of the testing OOD dataset._ Specifically, when considering samples from our OOD datasets as contexts, the performance of Turbo is comparable to using ID samples, whereas the LLaMA-series and Davinci3 models consistently exhibit inferior performance compared to using ID examples as contexts. However, all models show improved performance on the EQA task when contexts from our OOD datasets are utilized. This may be attributed to the variations in sample length or question styles across EQA datasets, hence models acquire more precise instructions from the original training samples. The overall ineffectiveness of ICL\({}^{*}\) can be explained by the findings of [69]. According to [69], in-context demonstrations aim to guide the models to learn the target label space, instead of the feature-label mapping. The ID examples contain more diverse information due to the construction process of our benchmark. Thus, ID examples can better prompt the language models to locate the target label space, compared to the OOD examples that may target a specific domain.

**Discussion.** Two paradigms are prevalent in developing downstream NLP systems: leveraging general-purpose LLMs or gathering domain-specific data for fine-tuning smaller models. For the first paradigm, the overarching objective of general-purpose LLM development is to employ a single model for solving various downstream tasks [9]. Consequently, LLMs are anticipated to exhibit high performance on both ID and OOD datasets. However, our study exposes the shortcomings of LLMs on ID datasets when compared to fine-tuned domain-specific models. Considering the higher inference and deployment costs associated with LLMs, substantial progress is still needed to effectively improve LLMs in developing downstream applications, particularly for challenging tasks like EQA. For the second paradigm, our study reveals the limitations of fine-tuning models on ID datasets for OOD performance in comparison to LLMs. Thus, further research is required to develop advanced techniques that enhance the robustness of fine-tuned domain-specific models. Overall, the existing two prevalent paradigms still fall short in addressing the OOD problem in NLP, necessitating further advancements and effective approaches.

However, we also note that there can exist confounders in our evaluations. It is still ambiguous which datasets are indeed OOD to LLMs, given that LLMs have been pre-trained on massive public corpora. The potential data contamination issue can result in overinflated performance on our OOD datasets, tangling the credit of the memory and generalizability of LLMs. The only confirmed distribution shift for LLMs is the temporal shift, necessitating the evaluation based on data released subsequent to their pre-training data collection cut-off. Therefore, the NLP community demands new downstream datasets independent of the pre-training corpus to meet the evaluation requirements for LLMs.

## 5 Related Work

**Distribution shifts in NLP** has been widely studied in various forms. We examine several representative cases as outlined below. Domain shift refers to the challenge of testing data originating from diverse domains, often due to data collection from various sources [63; 40; 53; 79]. Temporal shift examines the degradation of models' performance over time [42; 1]. Spurious correlation examines the issue of models acquiring dataset-specific knowledge on ID data, which may not generalize effectively to OOD data [66; 73; 91; 32; 37; 16; 17]. Additionally, a requirement is for models to exhibit robustness when confronted with artificially constructed OOD samples. One typical type is malicious adversarial attacks, which involve assessing the resilience of models against inputs crafted by malevolent adversaries [56; 51; 112]. These inputs, distinct from ID samples, have the potential to induce model failures [12]. Adversarial attacks can also be effectively utilized to simulate diverse user inputs to examine models' robustness in the real world [98, 13, 33]. Another category is backdoor attacks, characterized by intentionally introduced spurious correlations that can be exploited by attackers for their advantage [18, 55].

**OOD Evaluation in NLP** can be broadly classified into automatic and static evaluation approaches. Automatic evaluation utilizes diverse textual transformation techniques, such as introducing typos, to conduct a rigorous evaluation of OOD robustness. Three essential elements in the automatic OOD evaluation encompass the establishment of suitable transformation methods, evaluation metrics, and effective techniques to ensure sample validity [34, 98]. Static evaluation, in contrast to automated methods, offers the advantage of constructing benchmarks with higher quality, resulting in an improved estimation of OOD robustness. Numerous OOD benchmarks have been introduced, focusing on adversarial attacks [96] or spurious correlations [117, 66]. A relevant study to ours is GLUE-X [107], which establishes an OOD benchmark derived from the GLUE benchmark [93]. Nevertheless, they do not establish a coherent benchmark construction protocol and primarily rely on dataset selection driven by popularity, incorporating datasets into the benchmark without comprehensive explanation and seemingly opting for a somewhat arbitrary selection, thus lacking a systematic approach.

## 6 Conclusion

We revisit OOD robustness research in NLP, identifying deficiencies in benchmarks and evaluation. Correspondingly, a benchmark construction protocol and an OOD robustness evaluation suite are proposed to facilitate future research. The correlation between OOD and ID performance, the effectiveness of existing methods, and the challenges faced by LLMs are investigated.

## Limitation

We identify two limitations in this work. First, as discussed in section 4.2, due to the lack of new datasets in the community, there is a possibility that some datasets have been included in the pre-training corpus of LLMs, so they may not be suitable to test the generalizability of recent LLMs. However, we note that with our benchmark construction protocol, we can easily update the benchmark as new datasets come out. Second, we only consider five tasks in this benchmark, which is not a comprehensive collection of current NLP literature. We explain the reason for the current task selection in Appendix A.1.

## Acknowledgement

This work is sponsored by the Tsinghua-Toyota Joint Research Fund.

Lifan Yuan and Yangyi Chen initiated the project. Lifan Yuan, Yangyi Chen, and Ganqu Cui designed the experiments. Lifan Yuan, Yangyi Chen, and Hongcheng Gao constructed the AdvCivil dataset. Lifan Yuan conducted experiments and wrote the paper. Yangyi Chen and Ganqu Cui revised the paper. Everyone participated in the discussion. Heng Ji, Zhiyuan Liu, and Maosong Sun advised the project.

## References

* [1] Oshin Agarwal and Ani Nenkova. Temporal effects on pre-trained models for language processing tasks. _arXiv preprint arXiv:2111.12790_, 2021.
* [2] Udit Arora, William Huang, and He He. Types of out-of-distribution texts and how to detect them. In _Proceedings of EMNLP_, 2021.
* [3] Ting Wai Terence Au, Vasileios Lampos, and Ingemar Cox. E-NER -- an annotated named entity recognition corpus of legal text. In _Proceedings of the Natural Legal Language Processing Workshop_, 2022.
* [4] Max Bartolo, Alastair Roberts, Johannes Welbl, Sebastian Riedel, and Pontus Stenetorp. Beat the AI: Investigating adversarial human annotation for reading comprehension. _Transactions of ACL_, 2020.

* Bastan et al. [2022] Mohaddeseh Bastan, Mihai Surdeanu, and Niranjan Balasubramanian. Bionll: Generating a biomedical nli dataset using lexico-semantic constraints for adversarial examples. In _Findings of EMNLP_, 2022.
* Borkan et al. [2019] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced metrics for measuring unintended bias with real data for text classification. _CoRR_, 2019.
* Bowman et al. [2015] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In _Proceedings of EMNLP_, 2015.
* Bowman and Dahl [2021] Samuel R. Bowman and George Dahl. What will it take to fix benchmarking in natural language understanding? In _Proceedings of NAACL_, 2021.
* Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In _Preceedings of NeurIPS_, 2020.
* Chandra et al. [2020] Mohit Chandra, Ashwin Pathak, Eesha Dutta, Paryul Jain, Manish Gupta, Manish Shrivastava, and Ponnurangam Kumaraguru. AbuseAnalyzer: Abuse detection, severity and target prediction for gab posts. In _Proceedings of COLING_, 2020.
* Chen et al. [2021] Jiaxao Chen, Dinghan Shen, Weizhu Chen, and Diyi Yang. HiddenCut: Simple data augmentation for natural language understanding with better generalizability. In _Proceedings of ACL-IJCNLP_, 2021.
* Chen et al. [2022] Yangyi Chen, Hongcheng Gao, Ganqu Cui, Fanchao Qi, Longtao Huang, Zhiyuan Liu, and Maosong Sun. Why should adversarial perturbations be imperceptible? rethink the research paradigm in adversarial nlp. In _Proceedings of EMNLP_, 2022.
* Chen et al. [2023] Yangyi Chen, Hongcheng Gao, Ganqu Cui, Lifan Yuan, Dehan Kong, Hanlu Wu, Ning Shi, Bo Yuan, Longtao Huang, Hui Xue, et al. From adversarial arms race to model-centric evaluation: Motivating a unified automatic robustness evaluation framework. In _Findings of ACL_, 2023.
* Chen et al. [2023] Yangyi Chen, Lifan Yuan, Ganqu Cui, Zhiyuan Liu, and Heng Ji. A close look into the calibration of pre-trained language models. In _Proceedings of ACL_, 2023.
* Cheng et al. [2021] Hao Cheng, Xiaodong Liu, Lis Pereira, Yaoliang Yu, and Jianfeng Gao. Posterior differential regularization with f-divergence for improving model robustness. In _Proceedings of NAACL:HLT_, 2021.
* Clark et al. [2019] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don't take the easy way out: Ensemble based methods for avoiding known dataset biases. In _Proceedings of EMNLP-IJCNLP_, 2019.
* Clark et al. [2020] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Learning to model and ignore dataset bias with mixed capacity ensembles. _arXiv preprint arXiv:2011.03856_, 2020.
* Cui et al. [2022] Ganqu Cui, Lifan Yuan, Bingxiang He, Yangyi Chen, Zhiyuan Liu, and Maosong Sun. A unified evaluation of textual backdoor learning: Frameworks and benchmarks. In _Proceedings of NeurIPS_, 2022.
* Das et al. [2022] Sarkar Snigdha Sarathi Das, Arzoo Katiyar, Rebecca Passonneau, and Rui Zhang. CONTaiNER: Few-shot named entity recognition via contrastive learning. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, 2022.
* Davidson et al. [2017] Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. Automated hate speech detection and the problem of offensive language. In _Proceedings of AAAI on Web and Social Media_, 2017.

* [21] Ona de Gibert, Naiara Perez, Aitor Garcia-Pablos, and Montse Cuadros. Hate speech dataset from a white supremacy forum. In _Proceedings of Workshop on Abusive Language Online (ALW2)_, 2018.
* [22] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank: Investigating projection in naturally occurring discourse. In _Proceedings of Sinn und Bedeutung_, 2019.
* [23] Leon Derczynski, Eric Nichols, Marieke van Erp, and Nut Limsopatham. Results of the WNUT2017 shared task on novel and emerging entity recognition. In _Proceedings of Workshop on Noisy User-generated Text_, 2017.
* [24] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In _Proceedings of NAACL_, 2019.
* [25] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proc. of NAACL_, 2019.
* [26] Ning Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. Few-NERD: A few-shot named entity recognition dataset. In _Proceedings of ACL-IJCNLP_, 2021.
* [27] Mengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi Deshpande, Franck Dernoncourt, Jiuxiang Gu, Tong Sun, and Xia Hu. Towards interpreting and mitigating shortcut learning behavior of NLU models. In _Proceedings of NAACL:HLT_, 2021.
* [28] Matthew Dunn, Levent Sagun, Mike Higgins, V. Ugur Guney, Volkan Cirik, and Kyunghyun Cho. Searchqa: A new q&a dataset augmented with context from a search engine. _CoRR_, 2017.
* [29] Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, and Diyi Yang. Latent hatred: A benchmark for understanding implicit hate speech. In _Proceedings of EMNLP_, 2021.
* [30] Adam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eunsol Choi, and Danqi Chen. MRQA 2019 shared task: Evaluating generalization in reading comprehension. In _Proceedings of 2nd Machine Reading for Reading Comprehension (MRQA) Workshop at EMNLP_, 2019.
* [31] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embeddings. In _Proceedings of EMNLP_, 2021.
* [32] Matt Gardner, William Merrill, Jesse Dodge, Matthew E Peters, Alexis Ross, Sameer Singh, and Noah Smith. Competency problems: On finding and removing artifacts in language data. _arXiv preprint arXiv:2104.08646_, 2021.
* [33] Karan Goel, Nazneen Rajani, Jesse Vig, Samson Tan, Jason Wu, Stephan Zheng, Caiming Xiong, Mohit Bansal, and Christopher Re. Robustness gym: Unifying the nlp evaluation landscape. _arXiv preprint arXiv:2101.04840_, 2021.
* [34] Karan Goel, Nazneen Fatema Rajani, Jesse Vig, Zachary Taschdjian, Mohit Bansal, and Christopher Re. Robustness gym: Unifying the NLP evaluation landscape. In Avi Sil and Xi Victoria Lin, editors, _Proceedings of NAACL-HLT_, 2021.
* [35] Tanya Goyal, Junyi Jessy Li, and Greg Durrett. News summarization and evaluation in the era of gpt-3. _CoRR_, 2022.
* [36] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In _Proceedings of ACL_, 2022.
* [37] He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting the residual. _arXiv preprint arXiv:1908.10763_, 2019.
* [38] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In _Proceedings of ICLR_, 2021.

* [39] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out-of-distribution robustness. In _Proceedings of ACL_, 2020.
* [40] Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song. Pretrained transformers improve out-of-distribution robustness. _arXiv preprint arXiv:2004.06100_, 2020.
* [41] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. 2019.
* [42] Xiaolei Huang and Michael J Paul. Examining temporality in document classification. In _Proceedings of ACL_, 2018.
* [43] Dieuwke Hupkes, Mario Giulianelli, Verna Dankers, Mikel Artetxe, Yanai Elazar, Tiago Pimentel, Christos Christodoulopoulos, Karim Lasri, Naomi Saphra, Arabella Sinclair, Dennis Ulmer, Florian Schottmann, Khuyagbaatar Batsuren, Kaiser Sun, Koustuv Sinha, Leila Khalatbari, Maria Ryskina, Rita Frieske, Ryan Cotterell, and Zhijing Jin. State-of-the-art generalisation research in NLP: a taxonomy and review. _CoRR_, 2022.
* [44] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adversarial examples are not bugs, they are features. In _Proceedings of NeurIPS_, 2019.
* [45] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In _Proceedings of ACL_, 2017.
* [46] Divyansh Kaushik, Eduard Hovy, and Zachary C. Lipton. Learning the difference that makes a difference with counterfactually-augmented data. In _Proceedings of ICLR_, 2020.
* [47] Pride Kavumba, Ryo Takahashi, and Yusuke Oda. Are prompt-based models clueless? In _Proceedings of ACL_, 2022.
* [48] Zixuan Ke, Bing Liu, Hao Wang, and Lei Shu. Continual learning with knowledge transfer for sentiment classification. In _Proceedings of ECML-PKDD_, 2020.
* [49] Yuta Koreeda and Christopher Manning. ContractNLI: A dataset for document-level natural language inference for contracts. In _Findings of EMNLP_, 2021.
* [50] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, Llion Jones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov. Natural questions: A benchmark for question answering research. _Transactions of ACL_, 2019.
* [51] Thai Le, Jooyoung Lee, Kevin Yen, Yifan Hu, and Dongwon Lee. Perturbations in the wild: Leveraging human-written text perturbations for realistic adversarial attack and defense. _arXiv preprint arXiv:2203.10346_, 2022.
* [52] Seanie Lee, Minki Kang, Juho Lee, and Sung Ju Hwang. Learning to perturb word embeddings for out-of-distribution QA. In _Proceedings of ACL-IJCNLP_, 2021.
* [53] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. _arXiv preprint arXiv:2104.08691_, 2021.
* [54] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In _Proceedings of EMNLP_, 2021.
* [55] Jiazhao Li, Yijin Yang, Zhuofeng Wu, V. G. Vinod Vydiswaran, and Chaowei Xiao. Chatgpt as an attack tool: Stealthy textual backdoor attack via blackbox generative model trigger. _CoRR_, 2023.

* [56] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adversarial text against real-world applications. _arXiv preprint arXiv:1812.05271_, 2018.
* [57] Chen Liang, Simiao Zuo, Minshuo Chen, Haoming Jiang, Xiaodong Liu, Pengcheng He, Tuo Zhao, and Weizhu Chen. Super tickets in pre-trained language models: From model compression to improving generalization. In _Proceedings of ACL-IJCNLP_, 2021.
* [58] Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. 2017.
* [59] Alisa Liu, Swabha Swayamdipta, Noah A. Smith, and Yejin Choi. Wanli: Worker and ai collaboration for natural language inference dataset creation. In _Findings of EMNLP_, 2022.
* [60] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _CoRR_, 2019.
* [61] Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Ziwei Ji, Samuel Cahyawijaya, Andrea Maddotto, and Pascale Fung. Crossner: Evaluating cross-domain named entity recognition. In _Proceedings of AAAI_, 2021.
* [62] Florian Ludwig, Klara Dolos, Torsten Zesch, and Eleanor Hobley. Improving generalization of hate speech detection systems to novel target groups via domain adaptation. In _Proceedings of the Sixth Workshop on Online Abuse and Harms (WOAH)_, 2022.
* [63] Xiaofei Ma, Peng Xu, Zhiguo Wang, Ramesh Nallapati, and Bing Xiang. Domain adaptation with bert-based domain classification and data selection. In _Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019)_, 2019.
* [64] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of NAACL_, 2011.
* [65] Julian McAuley and Jure Leskovec. Hidden factors and hidden topics: Understanding rating dimensions with review text. In _Proceedings of ACM Conference on Recommender Systems_, 2013.
* [66] R Thomas McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. _arXiv preprint arXiv:1902.01007_, 2019.
* [67] John Miller, Karl Krauth, Benjamin Recht, and Ludwig Schmidt. The effect of natural distribution shift on question answering models. In _Proceedings of ICML_, 2020.
* [68] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In _Proceedings of ICML_, 2021.
* [69] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In _Proceedings of EMNLP_, 2022.
* [70] Preslav Nakov, Alan Ritter, Sara Rosenthal, Fabrizio Sebastiani, and Veselin Stoyanov. SemEval-2016 task 4: Sentiment analysis in Twitter. In _Proceedings of International Workshop on Semantic Evaluation (SemEval)_, 2016.
* [71] Isar Nejadgholi and Svetlana Kiritchenko. On cross-dataset generalization in automatic detection of online abuse. In _Proceedings of the Fourth Workshop on Online Abuse and Harms_, 2020.
* [72] Nathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi. SSMBA: Self-supervised manifold based data augmentation for improving out-of-domain robustness. In _Proceedings of EMNLP_, 2020.

* [73] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial nli: A new benchmark for natural language understanding. In _Proceedings of ACL_, 2020.
* [74] Bhargavi Paranjape, Pradeep Dasigi, Vivek Srikumar, Luke Zettlemoyer, and Hannaneh Hajishirzi. Agro: Adversarial discovery of error-prone groups for robust optimization. 2022.
* [75] Christopher Potts, Zhengxuan Wu, Atticus Geiger, and Douwe Kiela. DynaSent: A dynamic benchmark for sentiment analysis. In _Proceedings of ACL-IJCNLP_, 2021.
* [76] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In _Proceedings of ICML_, 2021.
* [77] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of Machine Learning Research_, 2020.
* [78] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for machine comprehension of text. In _Proceedings of EMNLP_, 2016.
* [79] Alan Ramponi and Barbara Plank. Neural unsupervised domain adaptation in nlp--a survey. _arXiv preprint arXiv:2006.00632_, 2020.
* [80] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In _Proceedings of EMNLP_, 2019.
* [81] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization, 2022.
* [82] Zhengxiang Shi, Qiang Zhang, and Aldo Lipani. Stepgame: A new benchmark for robust multi-hop spatial reasoning in texts. In _Proceedings of AAAI_, 2022.
* [83] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of EMNLP_, 2013.
* [84] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of CVPR_, 2016.
* [85] Aarne Talman and Stergios Chatzikyriakidis. Testing the generalization power of neural network models across NLI benchmarks. In _Proceedings of ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, 2019.
* [86] Alon Talmor and Jonathan Berant. MultiQA: An empirical investigation of generalization and transfer in reading comprehension. In _Proceedings of ACL_, 2019.
* [87] Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. In _Proceedings of NeurIPS_, 2020.
* [88] Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the conll-2003 shared task: Language-independent named entity recognition. In _Proceedings of NAACL_, 2003.
* [89] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _ArXiv_, 2023.

* [90] Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. NewsQA: A machine comprehension dataset. In _Proceedings of Workshop on Representation Learning for NLP_, 2017.
* [91] Liftu Tu, Garima Lalwani, Spandana Gella, and He He. An empirical study on robustness to spurious correlations using pre-trained language models. _Transactions of the Association for Computational Linguistics_, 2020.
* [92] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In _Proceedings of NeurIPS_, 2019.
* [93] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv preprint arXiv:1804.07461_, 2018.
* [94] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In _Proceedings of ICLR_, 2019.
* [95] Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li, and Jingjing Liu. Infobert: Improving robustness of language models from an information theoretic perspective. 2021.
* [96] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. _arXiv preprint arXiv:2111.02840_, 2021.
* [97] Tianlu Wang, Rohit Sridhar, Diyi Yang, and Xuezhi Wang. Identifying and mitigating spurious correlations for improving robustness in nlp models. In _Findings of NAACL:HLT_, 2022.
* [98] Xiao Wang, Qin Liu, Tao Gui, Qi Zhang, Yicheng Zou, Xin Zhou, Jiacheng Ye, Yongxin Zhang, Rui Zheng, Zexiong Pang, Qinzhuo Wu, Zhengyan Li, Chong Zhang, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Shan Qin, Bolin Zhu, Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou, Zhongyu Wei, Xipeng Qiu, and Xuanjing Huang. TextFlint: Unified multilingual robustness evaluation toolkit for natural language processing. In _Proc. of ACL_, 2021.
* [99] Xuezhi Wang, Haohan Wang, and Diyi Yang. Measure and improve robustness in NLP models: A survey. In _Proceedings of NAACL_, 2022.
* [100] Jason Wei, Yi Tay, and Quoc V. Le. Inverse scaling can become u-shaped. _ArXiv_, 2022.
* [101] Jason Wei and Kai Zou. EDA: Easy data augmentation techniques for boosting performance on text classification tasks. In _Proceedings of EMNLP-IJCNLP_, 2019.
* [102] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In _Proceedings of NAACL_, 2018.
* [103] Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel Weld. Polyjuice: Generating counterfactuals for explaining, evaluating, and improving models. In _Proceedings of ACL-IJCNLP_, 2021.
* [104] Xing Wu, Chaochen Gao, Liangjun Zang, Jizhong Han, Zhongyuan Wang, and Songlin Hu. Esimcse: Enhanced sample building method for contrastive learning of unsupervised sentence embedding. In _Proceedings of COLING_, 2022.
* [105] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Rui Song, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pre-trained image-text model to video-language representation alignment. In _Proceedings of ICLR_, 2022.
* [106] Linyi Yang, Lifan Yuan, Leyang Cui, Wenyang Gao, and Yue Zhang. Factmix: Using a few labeled in-domain examples to generalize to cross-domain named entity recognition. In _Proceedings of COLING_, 2022.

* [107] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-distribution generalization perspective. In _Findings of ACL_, 2023.
* [108] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In _Proceedings of EMNLP_, 2018.
* [109] Hai Ye, Yuyang Ding, Juntao Li, and Hwee Tou Ng. Robust question answering against distribution shifts with test-time adaption: An empirical study. In _Findings of EMNLP_, 2022.
* [110] Wenpeng Yin, Dragomir Radev, and Caiming Xiong. Docnli: A large-scale dataset for document-level natural language inference. In _Findings of ACL_, 2021.
* [111] Dani Yogatama, Cyprien de Masson d'Autume, Jerome Connor, Tomas Kocisky, Mike Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris Dyer, and Phil Blunsom. Learning and evaluating general linguistic intelligence, 2019.
* [112] Lifan Yuan, Yichi Zhang, Yangyi Chen, and Wei Wei. Bridge the gap between CV and nlp! A gradient-based textual adversarial attack framework. In _Findings of ACL_, 2023.
* [113] Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. Predicting the type and target of offensive posts in social media. In _Proceedings of NAACL_, 2019.
* [114] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In _Proceedings of ACL_, 2019.
* [115] Aston Zhang, Zachary Chase Lipton, Mu Li, and Alex Smola. Dive into deep learning. _CoRR_, 2021.
* [116] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In _Proceedings of NeurIPS_, 2015.
* [117] Yuan Zhang, Jason Baldridge, and Luheng He. Paws: Paraphrase adversaries from word scrambling. _arXiv preprint arXiv:1904.01130_, 2019.
* [118] Chen Zhu, Yu Cheng, Zhe Gan, S. Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced adversarial training for natural language understanding. In _Proceedings of ICLR_, 2019.

[MISSING_PAGE_FAIL:18]

a subset of references that specifically address OOD robustness in sentiment analysis (SA), toxic detection (TD), natural language inference (NLI), name entity recognition (NER), and extractive question answering (EQA) tasks. The summarized findings are outlined in Table 8. Through the survey, we can observe that there is no single uniform set of datasets for evaluation in NLP, namely no "standardized OOD benchmark suites tailored for NLP". And we can find that existing work includes evaluation datasets without particular consideration while mainly directly using popular datasets, i.e. adopting "popularity-based dataset selection strategies".

## Appendix C Datasets

In section 2.4, we outlined the datasets utilized in the BOSS benchmark. In this section, we provide additional descriptions for the datasets not covered in the main body of this paper, and elaborate on the data processing procedures for all candidate datasets.

### Description

**Sentiment Analysis. DSC**[48] consists of product reviews of 24 different categories in Amazon, with binary sentiments. **IMdb**[64] is another binary classification dataset with movie reviews from the IMDb website. **Yelp**[116] dataset is constructed by extracting product reviews from the Yelp website.

**Toxic Detection.**AbuseAnalyzer**[10] collects abusive contents from from Grab, with labels of Hateful and non-Hateful. **AdvCivil** is a new adversarial dataset for toxic detection introduced in this paper. We employ the adversarial attack technique proposed by [12] to simulate real-world ma

\begin{table}
\begin{tabular}{l l l} \hline \hline Paper & Tasks & Datasets \\ \hline
[46] & SA & BDID \(\rightarrow\) Amazon, SemEval, Yelp \\ \hline
[55] & NLI & MNLI, SNLI, SNLI \(\rightarrow\) MNLI, SNLI, SICK \\
[30] & EQA & SQuAD, Netwa, TriviaQA, SearchQA, HapotQA, Natural Questions \(\rightarrow\) BioASQ, DROP, DukeRC, RACE, RelationExtraction, TextbookQA \\
[60] & EQA & SQuAD, Netwa, SearchQA, TriviaQA, HapotQA \(\rightarrow\) CQ, CWQ, ComQA, WikiHop, DROP \\
[111] & EQA & SQuAD \(\rightarrow\) TriviaQA, QuAC, QA-SRL, QA-2REE \\
[40] & SA & SST2, IMDD, Yelp, Amazon \(\rightarrow\) SST2, IMDD, Yelp, Amazon \\
[73] & SA & Amazon, Yelp, IMDD, SST2 \(\rightarrow\) Amazon, Yelp, IMDD, SST2 \\
[74] & NLI & MNLI \(\rightarrow\) MNLI, ANLI \\
[75] & TD & Wiki \(\rightarrow\) Founta, Waseen \\
[76] & NLI & MNLI \(\rightarrow\) JAINS \\
[77] & NLI & MNLI, SNLI, FEVER \(\rightarrow\) ANLI \\
[78] & EQA & SQuAD \(\rightarrow\) Squashift \\
[79] & SA & SST2 \(\rightarrow\) Semi40, SemEval, Ambook, Velp, IMDD, IMDB-Cout, IMDb CAD \\
[78] & NLI & SNLI \(\rightarrow\) MNLI, SNLI,CJ, break, DNC, stress, diagnostic \\
[79] & SA & SST2 \(\rightarrow\) IMDB-Cout, IMDb CAD \\
[78] & NLI & MNLI \(\rightarrow\) ANLI, HANS \\ \hline
[78] & SA & SST2 \(\rightarrow\) IMDB \\
[79] & NLI & MNLI \(\rightarrow\) ANLI \\
[80] & EQA & SQuAD \(\rightarrow\) Adv SQuAD \\
[81] & FEVER, MNLI \(\rightarrow\) Symmetric, HANS \\
[82] & NLI & MNLI \(\rightarrow\) SNLI, SNLI \\
[83] & EQA & SQuAD \(\rightarrow\) BioASQ, New Wikipedia, New York Times, Reddit posts, Amazon Reviews \\
[84] & SA & SST2, Amazon kitchen, Amazon electronics \(\rightarrow\) SST, Amazon kitchen, Amazon electronics \\
[85] & SA & SST2 \(\rightarrow\) Semi40, SemEval, Velp, IMDB, Contrast, CAD \\
[86] & NLI & MNLI \(\rightarrow\) PLI, HANS, NauLI, SNLI, ANLI \\
[87] & SA & SST \(\rightarrow\) IMDB, Velp, Amazon \\ NL & MNLI \(\rightarrow\) MNLI, MNLI, SNLI, SICK \\
[88] & TD & HateXplain \\
[89] & NLI & MNLI,SNLI \(\rightarrow\) HANS \\
[90] & NER & CNLI \(\rightarrow\) CrossNER \\
[91] & NER & OntoNotes \(\rightarrow\) E2B2*14, CONLI’03, WNUT’17, GUM, Few-NERD \\
[92] & EQA & SQuAD \(\rightarrow\) New4QA, SearchQA, TriviaQA, HotpotQA, Natural Questions \\ \hline \hline \end{tabular}
\end{table}
Table 8: Survey of papers targeting to address OOD robustness across the five tasks as this paper. Column “Paper” signifies individual papers sorted by date of publication, column “Tasks” details the evaluated tasks, and column “Datasets” lists the encompassed datasets, with each row representing ID dataset \(\rightarrow\) OOD datasets.

licious attacks by introducing typos and distracting sentences into the original sentence. Additionally, a post-human inspection process is undertaken to validate the constructed samples. Three human annotators are employed to filter out label-inconsistent and semantically broken samples. **Hate Speech**[21] covers posts on various topics and nationalities from several sub-forums on Stormfront. **HSOL**[20] extracts tweets containing pre-defined toxic words or phrases and categorizes them into three classes: hate speech, offensive but not hate speech, or neither offensive nor hate speech. **OLID**[113] collects tweets and annotates each of them according to whether the text is offensive or not.

Natural Language Inference.**BioNLI**[5] is created from abstracts of biomedical publications on PubMed, with each sample labeled as entailed or not-entailed and regarding the experimental evidence as the premises. The entailed samples directly adopt the conclusions of mechanistic information in the abstracts as the hypothesis, while the not-entailed samples manipulate the information to generate pseudo conclusions with nine different strategies. **CB**[22] consists of clause-embedded texts from Wall Street Journal, British National Corpus, and Switchboard, each premise containing an embedded clause and the hypothesis being the extraction of the clause. **DocNLI**[110] processes five existing datasets of different tasks into NLI formats, all from news or Wikipedia. The premises are all at the document level, while the hypotheses have a variety of different granularities. **SNLI**[7] extracts image captions from Flickr30k as premises, and generates hypotheses by human annotation.

Name Entity Recognition.**CrossNER**[61] collect sentences from Wikipedia on topics of artificial intelligence, literature, music, natural science, and politics, covering seven coarse-grained entity types in Few-NERD except for the Building.

Extractive Question Answering.**HotpotQA**[108] is based on Wikipedia, with each question requiring information from two articles to answer. **NaturalQuestions**[50] takes real queries from Google search engine as questions, corresponding search results as contexts, and human-annotated texts as answers. **Trivia-QA**[45] collects long documents from Wikipedia and constructs questions that can not be directly answered by span extraction. **SQuAD Shifts**[67] inherits the dataset construction pipeline from SQuAD, but collects passages from more diverse sources, including Wikipedia, New York Times, Reddit, and Amazon.

### Processing

#### c.2.1 Sentiment Analysis

Generally, datasets for this task are label-imbalanced, which is detrimental to model performance. Therefore, If not specified, we tackle this issue by aligning \(n_{i}\) with \(\min\{n_{1},\dots,n_{|classes|}\}\) by discarding redundant samples in each class, where \(|classes|\) is the number of classes and \(n_{i}\) is the number of samples for label \(i\) in each dataset, \(i\in\{1,\dots,|classes|\}\).

**Amazon** has 29 subsets of different products. To encourage diversity of the review texts, we aim to merge all the subsets together. However, the scale of the entire dataset is prodigious and the sizes of the subsets vary greatly, making model training time-consuming and potentially ignoring the effect of certain types of reviews. Therefore, we first reduce large subsets to 20k samples, and then drop samples of stars 2 or 4 and maintain samples of starts 1, 3, and 5. After that, we get a ternary classification of the entire dataset, split into training and test datasets by 9:1. Finally, for label balance, we sample 10k reviews for each class in the training dataset and discard the rest.

**DSC** is a binary classification dataset, thus samples of both labels will be retained. The final training and test dataset are formed by combining all types of product reviews.

**Dynasent** has two rounds of training and test datasets, which are incorporated together to constitute a unified training and test dataset.

**IMDb** and **SemEval** only require label balancing processing.

**SST** samples are annotated with a float score indicating the sentiment, ranging from 0 to 1. We follow the common practice of SST-5 to equally divide the score into 5 bins, i.e. mapping score 00.2 to label 1, and so on. Then we drop samples of labels 1 and 3, similar to Amazon, to adapt for ternary classification.

**Yelp** reviews are also rated from 1 to 5 and retained only samples of stars 1, 3, and 5 following Amazon and SST. Due to the large amount of data, we sample 10k reviews from the training dataset for each label.

#### c.2.2 Toxic Detection

Datasets for this task are collected from online forums and social media, therefore the texts are commonly dirty, containing some meaningless strings such as @username in the beginning. We clean the texts by removing @username, emoji, tags (with a symbol #), and URLs. Moreover, the label balancing processing is the same as for Sentiment Analysis datasets.

**AbuseAnalyzer** does not have a train/test split, thus we divide the original dataset into a training dataset and a test dataset by 8:2.

**Civil Comments** samples with the toxicity score \(\geq\) 0.5 are considered toxic samples while others whose toxicity score < 0.5 are in the benign class.

**Hate Speech** is divided into training and test datasets by 8:2.

**HSOL** has three types of label: hate speech, offensive but not hate speech, or neither offensive nor hate speech. We adapt it to binary classification, treating samples with the first two toxic and samples with the label neither offensive nor hate speech as benign.

**Implicit Hate** is annotated into three classes, i.e., not hate, implicit hate and explicit hate, with implicit hate and explicit hate being considered toxic. In probing experiments, we split the training and test dataset by 8:2. After selecting the Implicit Hate dataset into our benchmark, we treat the entire dataset as the test dataset.

**AdvCivil**, **OLID** and **ToxiGen** do not require specific processing.

#### c.2.3 Natural Language Inference

Datasets for NLI do not have a serious label-imbalance problem, hence no corresponding processing is applied.

**ANLI** has three rounds in total, we merge them together to form the final training and test datasets respectively.

**DocNLI** contains some ANLI examples, so we filter those samples out to avoid repeating ANLI.

Other datasets, i.e., **BioNLI**, **CB**, **ContractNLI**, **MNLI**, **SNLI**, and **WANLI**, do not require any other specific processing. But note that the test dataset of MNLI is the matched validation dataset, and we do not use the unmatched one since the two subsets are too similar.

#### c.2.4 Name Entity Recognition

Datasets for NER usually have different name entity tags and thus require label mapping for alignment. For the cross-evaluation experiment, we align the labels of each dataset to CoNLL; while for other experiments, we take the scheme of Few-NERD as the standard. Also, we adapt the datasets to the prevalent BIO schema, which indicates whether a token is the Beginning, the Inside, or the Outside of an entity.

**CoNLL** contains the four most common entity types which are covered in Few-NERD, thus no label mapping is needed.

**CrossNER** constitutes various entity types from diverse domains. Therefore, we design the following mapping for label alignment:

label_mapping = {  "academicjournal": "product",  "album": "product",  "algorithm": "miscellaneous","astronomicalobject": "miscellaneous", "award": "miscellaneous", "band": "organization", "book": "art", "chemicalcompound": "miscellaneous", "chemicalelement": "miscellaneous", "conference": "event", "country": "location", "discipline": "miscellaneous", "election": "event", "enzyme": "miscellaneous", "event": "event", "field": "miscellaneous", "literarygenre": "art", "location": "location", "magazine": "product", "metrics": "miscellaneous", "misculardist": "person", "musicalinstrument": "product", "muscigemre": "art", "organisation": "organization", "person": "person", "poem": "art", "politicalparty": "organization", "political": "person", "product": "product", "programlang": "miscellaneous", "protein": "miscellaneous", "researcher": "person", "scientist": "person", "song": "art", "task": "miscellaneous", "theory": "miscellaneous", "university": "organization", "writer": "person" } ```

**Few-NERD** requires processing to adapt for the scheme of CoNLL in the probing experiments. Since it has covered all the entity types annotated in CoNLL, the only process is to set other tags, i.e. building, art, product, and event, to be miscellaneous. Note that this process is also required for CrossNER and WNUT in the cross-evaluation experiment, since these two datasets have already been aligned with Few-NERD.

**WNUT** conduct the following operation to align labels with Few-NERD:

``` label_mapping={ "corporation": "organization", "creative-work": "art", "group": "organization", "location": "location", "person": "person", "product": "product"
} ```

#### c.2.5 Extractive Question Answering

Datasets are all normalized to a unified extractive setting with the same format as SQuAD, following MRQA [30].

**AdvQA** has several subsets generated by fooling different models. We adopt the combination of all these subsets for our experiments.

**SQuAD** can be used in the original format, while other datasets i.e. **HotpotQA**, **NaturalQuestions**, **NewsQA**, **SearchQA**, and **Trivia-QA**, are adopted from MRQA to fit the extractive setting.

## Appendix D Dataset Selection for Other Tasks

In section 2.3, we have taken the task of sentiment analysis as an example to demonstrate how to select ID and OOD datasets for our benchmark. Next, we will explain how we choose datasets for other tasks.

### Toxic Detection

Candidate Datasets.We use the same approach for searching toxic detection datasets as we do for sentiment analysis. We gather several dataset candidates, including Civil Comments, Hate Speech, HSOL, Implicit Hate, OLID, and ToxiGen. We aim to adopt an adversarial dataset for each NLU task, which is lacking in the current literature. Hence, we later construct a new dataset through adversarial attacks on the chosen ID dataset.

Probing Experiments.The setup is the same as sentiment analysis.

Results.The dataset information can be found in Tables 9. The sources are very diverse, with each source, except Twitter, corresponding to only one dataset. For the ID dataset, Civil Comments is significantly larger than all the other datasets, and it is the only one containing more than 10k samples for each class. In addition, Civil Comments contain 6 subtypes of toxicity and 24 types of targeted identity, meeting the requirement for dataset diversity. Considering the dataset size and the text source diversity, we choose Civil Comments as our ID dataset for toxic detection.

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline \hline Train \(|\) & Test & AA & AC & CC & HS & HSOL & IH & OLID & TG \\ \hline AbuseAnalyzer & **100** & 80.05 & 79.60 & 78.89 & 73.97 & 75.78 & 85.33 & 63.06 \\ AdvCivil & 80.05 & **100** & 95.17 & 59.07 & 51.35 & 69.54 & 74.49 & 58.63 \\ Civil Comments & 79.60 & 95.17 & **100** & 66.89 & 50.96 & 69.05 & 76.20 & 62.09 \\ Hate Speech & 78.89 & 59.07 & 66.89 & **100** & 64.96 & 76.02 & 70.03 & 74.00 \\ HSOL & 73.97 & 51.35 & 50.96 & 64.96 & **100** & 41.73 & 69.80 & 43.32 \\ Implicit Hate & 75.78 & 69.54 & 69.05 & 76.02 & 41.73 & **100** & 64.60 & 79.79 \\ OILD & 85.33 & 74.49 & 76.20 & 70.03 & 69.80 & 64.60 & **100** & 51.76 \\ ToxiGen & 63.06 & 58.63 & 62.09 & 74.00 & 43.32 & 79.79 & 51.76 & **100** \\ \hline \hline \end{tabular}
\end{table}
Table 10: SimCSE scores between each pair of datasets regarding the toxic detection task. AA: AbuseAnalyzer; AC: AdvCivil; CC: Civil Comments; HS: Hate Speech; IH: Implicit Hate; TG: ToxiGen.

\begin{table}
\begin{tabular}{l l|c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Source} & \multirow{2}{*}{\# Classes} & \multicolumn{2}{c}{\# Samples} & \multicolumn{2}{c}{Avg. Length} \\  & & & Train & Test & Train & Test \\ \hline AbuseAnalyzer & Grab & 2 & 6,080 & 1,520 & 14.24 & 14.38 \\ AdvCivil & Adversarial & 2 & - & 823 & - & 70.73 \\ Civil Comments & Civil Comments & 2 & 60,000 & 97,320 & 50.09 & 51.15 \\ Hate Speech & Stormfront & 2 & 8,562 & 2,141 & 18.07 & 18.1 \\ HSOL & Twitter & 2 & 5,823 & 2,485 & 13.37 & 13.1 \\ Implicit Hate & Twitter & 2 & - & 21,480 & - & 16.81 \\ OILD & Twitter & 2 & 13,240 & 860 & 19.62 & 23.16 \\ ToxiGen & Synthetic & 2 & 8,960 & 940 & 18.14 & 18.63 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Statistics of toxic detection candidate datasets.

Next, we consider OOD datasets from other text sources. As aforementioned, we first supplement an adversarial dataset for toxic detection, i.e. AdvCivil, based on Civil Comments. The construction details can be found in Appendix C.1. Then we consider the semantic similarity among the dataset candidates. From the results in Table 10, we can filter out AbuseAnalyzer and OLID because of their high similarity with Civil Comments. We also observe a high similarity between AdvCivil and Civil Comments, but we do not disregard the former dataset at this stage since the high similarity has an acceptable attribution considering the construction process. Moreover, both HSOL and Implicit Hate come from tweets, thus they can not coexist in the benchmark. We are leaning to select Implicit Hate since it contains more challenging implicit toxicity.

Thereafter, we have four datasets left, i.e. AdvCivil, Hate Speech, Implicit Hate, and ToxGen. We still need to drop one dataset based on performance degradation. According to results in Table 11, AdvCivil, Implicit Hate, and ToxGen can provoke a performance drop of over 20 points. By contrast, the ID model can still achieve accuracy over 80 on Hate Speech, indicating that this shift may be the least challenging. Therefore, we discard Hate Speech and adopt the other three as the OOD datasets. It is also worth noting that Implicit Hate leads to a much more severe performance drop than HSOL, supporting our previous claim. Finally, the distribution shift for toxic detection is Civil Comments \(\rightarrow\) (AdvCivil, Implicit Hate, ToxGen).

### Natural Language Inference

Candidate Datasets.We investigate datasets on Paperswithcode and finally include ANLI, BioNLI, CB, ContractNLI, DocNLI, MNLI, SNLI, and WANLI as the candidates.

Probing Experiments.For semantic similarity evaluation, we only feed premises in each dataset to the unsupervised SimCSE model7, instead of concatenating the premises and hypotheses together. Note that we do not adopt the supervised SimCSE model here, because it was contrastively trained

\begin{table}
\begin{tabular}{l|c c c c c c c c} \hline \hline Train & Test & ANLI & BioNLI & CB & ContractNLI & DocNLI & MNLI & SNLI & WANLI \\ \hline ANLI & **100** & 31.32 & 29.37 & 21.99 & 53.79 & 16.27 & 21.99 & 22.41 \\ BioNLI & 31.32 & **100** & 5.51 & 10.39 & 28.07 & 2.28 & -4.78 & -0.12 \\ CB & 29.37 & 5.51 & **100** & 17.81 & 40.95 & 68.11 & 41.88 & 59.17 \\ ContractNLI & 21.99 & 10.39 & 17.81 & **100** & 19.96 & 3.62 & -7.17 & 6.12 \\ DocNLI & 53.79 & 28.07 & 40.95 & 19.96 & **100** & 34.91 & 21.23 & 28.00 \\ MNLI & 16.27 & 2.28 & 68.11 & 3.62 & 34.91 & **100** & 47.42 & 85.23 \\ SNLI & 21.99 & -4.78 & 41.88 & -7.17 & 21.23 & 47.42 & **100** & 46.80 \\ WANLI & 22.41 & -0.12 & 59.17 & 6.12 & 28.00 & 85.23 & 46.80 & **100** \\ \hline \hline \end{tabular}
\end{table}
Table 13: SimCSE scores between each pair of datasets regarding the natural language inference task.

\begin{table}
\begin{tabular}{l l|c c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Source} & \multirow{2}{*}{\# Classes} & \multicolumn{2}{c}{\# Samples} & \multicolumn{2}{c}{Avg. Length (p)} & \multicolumn{2}{c}{Avg. Length (h)} \\  & & & Train & Test & Train & Test & Train & Test \\ \hline ANLI & Adversarial & 3 & 162,865 & 3,200 & 54.13 & 54.42 & 9.6 & 10.22 \\ BioNLI & Biomedical & 2 & 5,544 & 68,243 & 215.93 & 217.93 & 26.91 & 29.65 \\  & Wall Street Journal \& & & & & & & & \\ CB & British National Corpus \& & 3 & - & 306 & - & 56.4 & - & 7.55 \\  & Switchboard & 3 & 7,191 & 2,091 & 1673.63 & 1700.82 & 12.82 & 12.82 \\ ContractNLI & Legal & 3 & 7,191 & 2,091 & 1673.63 & 1700.82 & 12.82 & 12.82 \\ DocNLI & News \& Wikipedia & 2 & 492,314 & 263,886 & 318.89 & 385.13 & 4.75 & 17.88 \\ MNLI & Open American National Corpus & 3 & 392,662 & 9,815 & 19.81 & 19.27 & 9.97 & 9.92 \\ SNLI & Flickr & 3 & 549,361 & 9,824 & 12.85 & 13.91 & 7.42 & 7.48 \\ WANLI & Synthetic & 3 & 102,884 & 5,000 & 17.49 & 17.48 & 9.93 & 9.83 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Statistics of natural language inference candidate datasets.

on NLI datasets, thus may lead to a bias in evaluating the semantic similarity between NLI datasets. For cross-evaluation experiments, the setup is the same as semantic analysis.

Results.Dataset information is shown in Tables 12. These NLI datasets are commonly large and their text sources do not overlap. Based on these results, we start to pick the ID dataset. First of all, BioNLI and DocNLI should be excluded from the discussion of ID dataset selection, because they only have two classes but all the other candidates have three categories. Otherwise, the trained ID model may fail to distinguish the class Neutral and Contradiction, namely suffering from a label shift [43]. Then, consider the dataset size. The majority of candidates contain over 100k training samples, while CB only has about 300 samples in total and ContractNLI has less than 10k samples, much smaller than others. Therefore, these two datasets should be excluded too. Among the remaining datasets, ANLI and WANLI mainly contain adversarial and challenging features, instead of general ones; MNLI claims to be more diverse than SNLI since it consists of both written and spoken English and contains 10 sub-genres, thus more abundant in text styles and topics. Hence, we pick MNLI as the ID dataset.

For OOD datasets, since there is no dataset drawn from the same source with MNLI, only semantic similarity and performance drop should be involved. From Table 13, the similarities between NLI datasets and MNLI are all relatively low, except for WANLI, CB, and SNLI. We will ignore CB and SNLI in the later procedure but still remain WANLI because WANLI is based on MNLI and thus it is reasonable to be somehow similar to MNLI. Thereafter, we examine the performance drop caused by each dataset. As shown in Table 14, the ID model presents little performance degradation on BioNLI and DocNLI, while suffering significant degradation on the other three datasets. Hence, the distribution shifts for NLI will be MNLI \(\rightarrow\) (ANLI, ContractNLI, WANLI).

### Name Entity Recognition

Candidate Datasets.Since NER datasets typically have different sets of entity type labels that require specific domain knowledge, we loosen the search standard to include the datasets that have partially overlapping entity type labels, rather than requiring an exact match. We find five suitable candidate datasets, i.e. CoNLL, CrossNER, E-NER, Few-NERD, and WNUT. To align their label sets, we process them to be consistent with Few-NERD since we consider the label set of Few-NERD to be the most general.

Probing Experiments.The setup for semantic similarity evaluation follows the way we do for sentiment analysis, but the backbone model used in the performance evaluation here is a DeBERTa-base model, instead of a T5-base. The reason is that T5 requires inputs to be organized with prompts while standard prompt-based tuning methods for NER are still lacking. Hence, T5 does not perform well on NER task, so we resort to fine-tuning the encoder-only model, DeBERTa, as the alternative.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Train \(|\) Test & CoNLL & CrossNER & E-NER & Few-NERD & WNUT \\ \hline CoNLL & **100** & 62.86 & 43.48 & 73.76 & 45.60 \\ CrossNER & 62.86 & **100** & 37.19 & 87.47 & 44.84 \\ E-NER & 43.48 & 37.19 & **100** & 42.29 & 25.85 \\ Few-NERD & 73.76 & 87.47 & 42.29 & **100** & 45.74 \\ WNUT & 45.60 & 44.84 & 25.85 & 45.74 & **100** \\ \hline \hline \end{tabular}
\end{table}
Table 16: SimCSE scores between each pair of datasets regarding the name entity recognition task.

\begin{table}
\begin{tabular}{l l|c c c c} \hline \hline Dataset & Source & \# Classes & \begin{tabular}{c} \# Samples \\ Train \\ \end{tabular} & \begin{tabular}{c} Avg. Length \\ Test \\ \end{tabular} & 
\begin{tabular}{c} Length \\ Train \\ \end{tabular} \\ \hline CoNLL & Reuters & 4 & 14,042 & 3,454 & 14.50 & 13.44 \\ CrossNER & Wikipedia & 7 & 701 & 2,507 & 38.46 & 38.22 \\ E-NER & Legal & 4 & - & 11,692 & - & 34.52 \\ Few-NERD & Wikipedia & 8 & 131,768 & 37,649 & 24.49 & 24.47 \\ WNUT & YouTube \& Twitter \& & & & \\ StackExchange \& Reddit & 6 & 3,395 & 1,288 & 18.48 & 18.16 \\ \hline \hline \end{tabular}
\end{table}
Table 15: Statistics of named entity recognition candidate datasets.

[MISSING_PAGE_FAIL:26]

Probing Experiments.In the experiment of semantics similarity, we take the passages as inputs for the supervised SimCSE model. In the experiment of performance evaluation, we choose T5-base as the backbone model and measure F1 scores.

Results.Table 18 shows the dataset statistics. Except for SQuAD Shifts, which does not have a training split, all the other datasets are large in size. However, considering the diversity, we assume that Wikipedia provides the most extensive knowledge base. Hence, we restrict the scope of the ID dataset to those created from Wikipedia. Among the three datasets, HotpotQA, SQuAD, and Trivia-QA, SQuAD is the largest one. and therefore, we adopt SQuAD as the ID dataset.

Then, since SQuAD has been selected to be the ID dataset, HotpotQA and Trivia-QA are considered in the OOD dataset selection. And besides these two datasets, NaturalQuestions also have a high semantic similarity with SQuAD, as shown in Table 19. Therefore, for the distinction of the distributions, NaturalQuestions will not be taken into account either. In contrast, despite a high semantic similarity with SQuAD, we do not exclude AdvQA for this reason. This is because AdvQA is constructed by human adversaries based on SQuAD, aiming to reveal the weakness of models trained on SQuAD. Therefore, we consider the high similarity between AdvQA and SQuAD reasonable and should not disregard AdvQA for this sake. Next, we evaluate the ID model on each candidate dataset to examine the challenge of each distribution shift. From Tabel 20, we can observe that the ID model performs poorly on AdvQA, NewsQA, and SearchQA while maintaining a high F1 score on SQuAD Shifts, indicating that the shift to SQuAD Shifts is the least challenging. Eventually, we establish the distribution shifts for extractive question answering as SQuAD \(\rightarrow\) (AdvQA, NewsQA, SearchQA).

## Appendix E Additional Descriptions and Results of the Analysis and LLMs Evaluation

### Correlation between ID and OOD performance

#### e.1.1 Experimental Setting

To measure the correlation between ID and OOD performance, we train models with various ID performances by manipulating model scales, training steps, available training samples, and tunable parameters following [14]. The details are introduced below.

**Model scale.** We adopt the small, base, and large versions of the T5 and DeBERTa models.

**Available training samples.** For Sentiment Analysis, Toxic Detection, and Natural Language Inference, we randomly sample K samples for each class. For Name Entity Recognition, we adopt the N-way K-shot sampling strategy proposed by [26], ensuring that entities of all N types occur K\(\sim\)2K times in the sampled dataset. For Extractive Question Answering, we randomly sample K questions. In all experiments, we run five repeated experiments for each K.

**Number of tunable parameters.** We employ parameter-efficient tuning methods to regulate the tuned parameter for adapting to downstream tasks We prioritize adopting the strong approach Adapter [41]. However, in experiments, we find that Extractive Question Answering models can achieve a high ID performance even with a very insignificant amount of parameters being tuned, thus presenting no performance change when the number of tunable parameters increases. Therefore, we turn to an alternative, Soft-prompt [54], for a clearer observation.

**Training steps.** We evaluate model performance at epoch \(\{0,0.1,\dots,0.9\}\cup\{1.0,\dots,\texttt{EPOCH}\}\), where EPOCH is the total number of training epochs.

#### e.1.2 Full Results

We present the complete results of ID-OOD correlation in Figure 4. Note that due to the variance of data points, we manually check and remove outlier points to make the fitting results clearer. Next, we will illustrate the results task by task.

**Sentiment Analysis.** All figures of SA comprise two straight lines. The smoother line, which mainly fits the results of T5-small models with poor ID performances, stretches from around ID accuracy = 60. Meanwhile, the steeper line accommodates T5-base and T5-large model results and begins around ID accuracy = 85. Despite the different slopes, both lines represent a linear positive correlation between ID performance and OOD generalization, thus we still categorize this kind of relation as Type I.

**Toxic Detection.** This task encompasses two types of ID-OOD relations. AdvCivil stands as an outlier as aforementioned, whereas Implicit Hate and ToxiGen fall under the category of Type I relation, characterized by a straight line that intersects the diagonal line \(y=x\) with a flatter slope. This trend indicates that although achieving initially higher performance on OOD examples than on ID examples, the model's performance gap between ID and OOD decreases and eventually reverses as training proceeds

**Natural Language Inference.** All relations are classified as Type III, a non-monotonic correlation. Each graph has a V-shaped curve, consisting of two straight lines with different slopes. Specifically, for ANLI, the first straight line has a shallower slope than the second one, while ContractNLI and WANLI exhibit the opposite trend.

**Named Entity Recognition.** The ID-OOD relations in this task are all typical Type I, with all fitted lines lying below the diagonal line. The CoNLL dataset exhibits the highest correlation to a perfectly linear relationship (\(y=x\)), while the WNUT dataset has the lowest correlation. This phenomenon indicates that improving OOD performance on CoNLL is the easiest while on WNUT is the hardest.

**Extractive Question Answering.** Type II relation is observed in this task, with varying degrees of curve changes. Specifically, in the SearchQA dataset, the angle of the curve change is the most pronounced, while in the NewsQA dataset it is the smoothest.

### LLMs Evaluation

**Experiment Setting.** The templates and instructions we used are shown in Tables 21 and 22. We take Dynasent as an example to demonstrate the in-context learning settings for LLMs. The in-context (ID) setting includes examplars drawn from the ID training dataset of the sentiment analysis task, i.e. training dataset of Amazon. However, the in-context (OOD) setting samples the in-context examplars from the training split of the OOD dataset, i.e. training dataset of Dynasent. Note, however, that the latter paradigm may not be applicable to all datasets. All ID datasets are excluded, as well as OOD datasets without a training split. Besides, those datasets whose text length is too long on average would not be taken into account either, as the contexts could exceed the context window of LLMs.

\begin{table}
\begin{tabular}{l|l|l} \hline Task & \multicolumn{2}{c|}{Instruction} \\ \hline \multirow{4}{*}{SA} & \multicolumn{2}{c|}{\### Instruction \###} \\  & Solve the sentiment analysis task. Options for sentiment: negative, positive, neutral. \\  & \multicolumn{2}{c|}{\### Format \###} \\  & Text: \{\{\{\text{Text}\}\} // Prediction: \{\{\{\text{Prediction}\}\}\} \\  & \multicolumn{2}{c|}{\### Input \###} \\  & Text: \{\{\text{input}\}\} // Prediction: \\ \hline \multirow{4}{*}{TD} & \multicolumn{2}{c|}{\### Instruction \###} \\  & Solve the toxic detection task. Options for toxicity: benign, toxic. \\  & \multicolumn{2}{c|}{\### Format \###} \\  & Text: \{\{\text{Text}\}\} // Prediction: \{\{\{\text{Prediction}\}\} \\  & \multicolumn{2}{c|}{\### Input \###} \\  & Text: \{\{\text{input}\}\} // Prediction: \\ \hline \multirow{4}{*}{NLI} & \multicolumn{2}{c|}{\### Instruction \###} \\  & Solve the NLI task. Options for entailment relationship: entailment, neutral, contradiction. \\  & \multicolumn{2}{c|}{\### Format \###} \\  & Premise: \{\{\text{Permise}\}\} // Hypothesis: \{\{\text{Hypothesis}\}\} // Prediction: \{\{\{\text{Prediction}\}\} \\  & \multicolumn{2}{c|}{\### Input \###} \\  & Premise: \{\{\text{input}\_1\}\} // Hypothesis: \{\{\text{input}\_2\}\} // Prediction: \\ \hline \multirow{4}{*}{NER} & \multicolumn{2}{c|}{\### Instruction \###} \\  & Solve the NER task, identifying the Organization, Person, Location, Miscellaneous, \\  & Building, Art, Product, and Event entities from given text. \\  & \multicolumn{2}{c|}{\### Format \###} \\  & Text: \{\{\text{Text}\}\} // Entity: Organization: None \# Person: Word1 \# Location: Word6, \\  & Word7 \# \# Miscellaneous: None \# Building: None \# Art: Word 3 \# Product: None \# Event: None. \\  & \multicolumn{2}{c|}{\### Input \###} \\  & Text: \{\{\text{input}\}\} // Entity: \\ \hline \multirow{4}{*}{EQA} & \multicolumn{2}{c|}{\### Instruction \###} \\  & Solve the extractive question answering task. Refering to the passage below and extract \\  & answer for the question. The answer should be the shortest phrase as it can be. \\  & \multicolumn{2}{c|}{\### Format \###} \\  & Passage: \{\{\text{Passage}\}\} // Question: \{\{\text{Question}\}\} // Answer: \{\{\text{Answer}\}\}. \\  & \multicolumn{2}{c|}{\### Input \###} \\  & Passage: \{\{\text{input}\_1\}\} // Question: \{\{\text{input}\_2\}\} // Answer: \\ \hline \end{tabular}
\end{table}
Table 21: Templates for T0-3B and T5-series models.

\begin{table}
\begin{tabular}{l|l|l} \hline Task & \multicolumn{2}{c|}{Template} & Verbalizer \\ \hline SA & \{\{\text{input}\}\} All in all, the sentiment was \{”mask”\}. & negative / positive / neutral \\ \hline TD & \{\{\text{input}\}\} All in all, the toxicity was \{”mask”\}. & benign / toxic \\ \hline \multirow{4}{*}{NLI} & Given the two sentences: & \\  & (1) \{\{\text{input}_1\}\}. & \\  & (2) \{\{\text{input}_2\}\}. & Yes / Maybe / No \\  & Does the first sentence entail the second? \{”mask”\}. & \\ \hline NER & \multicolumn{2}{c|}{ner text: \{\{\text{input}\}\} label: \{”mask”\} & - \\ \hline \multirow{4}{*}{EQA} & Extract the answer to the question from the following context. & \\  & Question: \{\{\text{input}_1\}\} & - \\ \cline{1-1}  & Context: \{\{\text{input}_2\}\}\|\| & - \\ \cline{1-1}  & Answer: \{’mask”\} & \\ \hline \end{tabular}
\end{table}
Table 22: Instructions for LLMs, including LLaMA-7B, Davinci3, and Turbo.

Figure 4: Complete results of the relation between ID and OOD performance.