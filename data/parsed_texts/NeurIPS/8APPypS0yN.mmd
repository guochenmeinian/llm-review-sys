# On the Expressivity and Sample Complexity of Node-Individualized Graph Neural Networks

 Paolo Pellizzoni

Max Planck Institute of Biochemistry

Martinsried, Germany

pellizzoni@biochem.mpg.de

&Till Hendrik Schulz

Max Planck Institute of Biochemistry

Martinsried, Germany

tschulz@biochem.mpg.de

&Dexiong Chen

Max Planck Institute of Biochemistry

Martinsried, Germany

dchen@biochem.mpg.de

&Karsten Borgwardt

Max Planck Institute of Biochemistry

Martinsried, Germany

borgwardt@biochem.mpg.de

These authors jointly supervised this work.

###### Abstract

Graph neural networks (GNNs) employing message passing for graph classification are inherently limited by the expressive power of the Weisfeiler-Leman (WL) test for graph isomorphism. Node individualization schemes, which assign unique identifiers to nodes (e.g., by adding random noise to features), are a common approach for achieving universal expressiveness. However, the ability of GNNs endowed with individualization schemes to generalize beyond the training data is still an open question. To address this question, this paper presents a theoretical analysis of the sample complexity of such GNNs from a statistical learning perspective, employing Vapnik-Chervonenkis (VC) dimension and covering number bounds. We demonstrate that node individualization schemes that are permutation-equivariant result in lower sample complexity, and design novel individualization schemes that exploit these results. As an application of this analysis, we also develop a novel architecture that can perform substructure identification (i.e., subgraph isomorphism) while having a lower VC dimension compared to competing methods. Finally, our theoretical findings are validated experimentally on both synthetic and real-world datasets.

## 1 Introduction

Graph Neural Networks (GNNs) have become a dominant approach in graph learning, leveraging the inherent structural information of graphs through multiple iterations of message passing. This iterative exchange of information between nodes allows GNNs to learn rich representations for both nodes and entire graphs. However, the expressivity of these models in the context of graph classification, i.e., their ability to distinguish non-isomorphic graphs, is intrinsically limited by the capabilities of the Weisfeiler-Leman (1-WL) algorithm [63], a heuristic used for the graph isomorphism problem [41, 64]. 1-WL is known to fail to distinguish certain classes of graphs, such as regular graphs [4]. Notably, GNNs are in general unable to solve the substructure identification task [12], which is concerned with identifying subgraph patterns in the data at hand.

To address these expressivity limitations, higher-order GNNs [41, 26] have been proposed. These models consider interactions among larger subgraphs or sets of nodes but often suffer from highcomputational complexity. An alternative approach, which we call _node individualization schemes_, entails introducing perturbations, such as random noise, into node features to break graph symmetries artificially. This technique enables message passing GNNs to become universal function approximators [1] without the computational overhead associated with higher-order GNNs. Several node individualization schemes have been proposed in the literature. Random Node Initializations (RNI) [52] add random noise to node labels to make them distinct with high probability, while Relational Pooling (RP) [42] and Colored Local Iterative Procedure (CLIP) [15] partition nodes based on their labels and assign unique identifiers within partitions. A recent paper [20] proposes a general framework based on individualization-refinement algorithms. Other approaches to increase the expressivity of GNNs relabel the graphs with structural or positional node encodings, such as node centralities, graphlet counts [9], Laplacian [16; 62; 28], and random walk [17] encodings. These encodings give information about the graph topology, complementing the information obtained from message passing, but cannot guarantee universal expressivity in general.

Beyond expressivity, the sample complexity of GNNs [36; 54; 23], i.e., the amount of training data required for generalization on unseen data, is a critical consideration to obtain effective graph learning models, and it has been recently linked to the 1-WL expressivity [37]. Nonetheless, the sample complexity of GNNs with node individualization schemes and with structural or positional node encodings, which is of paramount importance in the understanding of the generalization power of maximally expressive GNNs, remains an unanswered question.

This paper presents a novel theoretical analysis of both the expressivity and sample complexity of GNNs endowed with node individualization and structural or positional node encodings, bridging the gap between the expressivity analysis of GNNs and statistical learning theory. Specifically, we examine the sample complexity of various node individualization schemes in the binary graph classification setting, offering insights that can guide the design of more effective graph learning models. We make the following contributions:

* We demonstrate the utility of node individualization schemes even on graphs recognizable by the 1-WL test, thereby enhancing the expressivity of shallow GNNs (Section 3).
* We analyze the sample complexity of GNNs with various node individualization schemes using VC-dimension-based bounds (Theorem 1). Based on our analysis, we develop a model architecture EGGNN that is universally expressive for substructure identification and has low VC dimension (Section 4.2).
* We provide a sharper analysis based on covering-number bounds, leading to the design of novel node individualization schemes (Theorem 4). Our bounds rely on a novel covering bound for Lipschitz functions [24], a result that is potentially of independent interest.
* We substantiate our theoretical findings with experimental results on both synthetic and real-world datasets (Section 5).

By offering a comprehensive analysis and practical guidance, this paper aims to enhance the understanding and development of GNNs through the lens of node individualization, pushing the boundaries of what these models can achieve in terms of expressivity and sample efficiency.

### Related work

Expressivity of GNNsSince the seminal works of [41; 64] revealed the limitations of MPNNs due to their expressivity being bounded by the 1-WL test, research has surged toward developing more powerful GNNs. One prominent approach has been to design GNNs that emulate higher-order WL [26] or Folklore-WL [10] tests, exemplified by k-GNNs [41] and k-FGNNs [35]. However, their computational and memory demands often hinder real-world applicability. To mitigate this, researchers have explored more efficient alternatives, leveraging graph locality and sparsity [40; 67; 22]. Another line of research involves subgraph GNNs, which break symmetry by transforming the original graph into modified subgraphs for GNN processing [14; 48; 8]. For a more comprehensive overview, readers are encouraged to consult [51; 39].

Node individualizationsIndividualization schemes have been proposed by several works [52; 42; 15; 20; 11] to enhance the expressivity of GNNs. GNNs with such schemes have been shown to be universal function approximators for invariant functions [1; 5]. Individualization schemes are also crucial on edge-level tasks [34; 53], where they break symmetry among different edge-orbits.

Sample complexityThe generalization capabilities of restricted families of GNNs has been addressed by means of VC dimension [54], Rademacher complexity [23] and PAC-bayesian approaches [29; 33]. Recently, [37; 21] proposed VC-dimension bounds based on the number of 1-WL color classes. The sample complexity of frequent subgraph identification has been addressed in [45]. Moreover, [46] and[19] address the sample complexity of equivariant models using a covering-number-based approach [6; 25], which is exploited also in [68] in an empirical setting. For a general overview on statistical learning theory, consult [36].

## 2 Preliminaries

In what follows, we define a graph as a tuple \(G=(V_{G},E_{G},L_{G})\), with \(V_{G}=\{1,\ldots,|V_{G}|\}\) a finite set of nodes, and \(E_{G}\subseteq\{\{u,v\}:\:u\neq v\in V_{G}\}\) a set of undirected edges. We define the vertex-label function as \(L_{G}:V_{G}\rightarrow\mathcal{L}\), with a finite set of labels \(\mathcal{L}\). For the sake of simplicity, we consider edges to be unlabeled. We define the neighborhood of a node as \(\mathcal{N}(v)=\{w\in V_{G}:\{v,w\}\in E_{G}\}\). We say that two graphs \(G\) and \(H\) are isomorphic, denoted as \(G\simeq H\), if there exists a bijective mapping \(\pi:V_{G}\to V_{H}\), called isomorphism, such that \(L_{G}(v)=L_{H}(\pi(v)),\:\forall v\in V_{G}\) and \(\{\pi(u),\pi(v)\}\in E_{H}\) if and only if \(\{u,v\}\in E_{G}\). The isomorphism relation induces equivalence classes, which we call _unordered_ graphs [27]. The group of isomorphisms from \(G\) to itself is called the automorphism group \(Aut(G)\). We denote a set of graphs with \(\mathcal{G}\), and if necessary, as \(\mathcal{G}_{\mathcal{L}}\) to explicate the label set. We denote sets of unordered graphs as quotient sets \(\mathcal{G}/\simeq\). Moreover, we consider sets of graphs of bounded size, that is \(|V_{G}|\leq n_{\max},\forall G\in\mathcal{G}\). A function defined on graphs \(f:\mathcal{G}\rightarrow\mathbb{R}\) is called invariant if for any \(G\simeq H\), \(f(G)=f(H)\). A function from graphs to graphs \(f:\mathcal{G}\rightarrow\mathcal{G}^{\prime}\) is here called equivariant if for any \(G\simeq H\), \(f(G)\simeq f(H)\).

### The Weisfeiler-Leman and Tinhofer algorithms

The color refinement algorithm, also known as 1-Weisfeiler-Leman (denoted as WL) algorithm, is a heuristic algorithm for the graph isomorphism problem. Let \(C_{0}(v)=L_{G}(v)\in\mathbb{N}\) be the initial color of node \(v\in V_{G}\). Then the algorithm updates vertex colors at iteration \(k>0\) as

\[C_{k}(v)=\operatorname{HASH}\left(C_{k-1}(v),\{\{C_{k-1}(w):w\in\mathcal{N}( v)\}\}\right)\in\mathbb{N},\]

with \(\operatorname{HASH}\) an injective map. Let \(\mathcal{P}^{(k)}(G)\) be the partitioning of \(V_{G}\) based on colors after the \(k\)-th iteration of color refinement. Then, after at most \(|V_{G}|\) iterations, the color partition stabilizes, i.e. \(\mathcal{P}^{(k)}(G)=\mathcal{P}^{(k+1)}(G)\), \(\forall k\geq|V_{G}|\). Two graphs are deemed \(k\)-hop WL-isomorphic if \(\{\{C_{k}(v):v\in V_{G}\}\}=\{\{C_{k}(v):\:v\in V_{H}\}\}\), and WL-isomorphic if it holds for \(k=|V_{G}|\). We denote this as \(G\simeq_{\mathrm{WL}}H\). Note that \(G\simeq H\implies G\simeq_{\mathrm{WL}}H\), but the converse is not true. We call a graph \(G\)_WL-amenable_ if \(\forall H\) such that \(G\not\cong H\), \(G\not\cong_{\mathrm{WL}}H\).

Tinhofer [59] developed a color-refinement-based algorithm, described in Section A.2, that returns an ordering of the nodes of a graph. This is a canonical ordering on a large class of graphs, called Tinhofer, which is a strict superset of WL-amenable class [4]. The Tinhofer algorithm is an instance of the individualization-refinement paradigm [20] on which several isomorphism solvers are based [3].

### Graph neural networks

Message passing graph neural networks (GNNs), given a graph \(G\), iteratively produce for each node \(v\in V_{G}\), at each level \(k=1,\ldots,K\), the embeddings \(h_{v}^{k}\in\mathbb{R}^{d_{h}}\) by taking into account _messages_ coming from its neighbors \(\mathcal{N}(v)\). More formally, the embedding of node \(v\) is updated as \(h_{v}^{k}=f_{\mathrm{upd}}\left(h_{v}^{k-1},f_{\mathrm{agg}}\left(\{\{h_{v}^{ k-1}:u\in\mathcal{N}(v)\}\}\right)\right),\) where \(f_{\mathrm{agg}}\) and \(f_{\mathrm{upd}}\) are the aggregate and the update operations, respectively. The first layer of the GNN is fed with the initial node embeddings \(h_{v}^{0}\), e.g. one-hot encodings of the node labels. Finally, one can get a graph-level readout \(h_{G}\in[0,1]\) by aggregating the output node embeddings via a function \(f_{\mathrm{out}}\). In [64; 41] it was shown that there exist injective functions \(f_{\mathrm{agg}}\), \(f_{\mathrm{upd}}\) and \(f_{\mathrm{out}}\) yielding GNNs that are provably as expressive as color refinement. We provide examples for such architectures in Section A.1.

We denote as \(\operatorname{GNN}_{K}=\{\operatorname{GNN}_{K,\theta},\theta\in\Theta\}\) the class of parametric functions formed by such a model with \(K\) message passing layers and parameter space \(\Theta\). Moreover, we define \(\operatorname{GNN}_{K,\theta}^{\mathrm{bin}}(G)=\mathbbm{1}\left[\operatorname{ GNN}_{K,\theta}(G)>0.5\right]\) and \(\operatorname{GNN}_{K}^{\mathrm{bin}}=\{\operatorname{GNN}_{K,\theta}^{ \mathrm{bin}},\theta\in\Theta\}\) the class of predictors.

Node individualization schemes

In this section, we present a general framework for node individualization schemes.

**Definition 1**.: _A graph \(G=(V,E,L_{G})\) is called individualized if all node labels are pairwise distinct, that is \(|\{L_{G}(v):v\in V\}|=|V|\). Moreover, we define a graph as \(k\)-weakly individualized if all nodes have different colors after \(k\) color refinement iterations, that is \(|\{C_{k}(v):v\in V\}|=|V|\)._

Graph individualization is effectively achieved by assigning unique identifiers to nodes through a relabeling process. We formally define a relabeling function \(\operatorname{Rel}:\mathcal{G}_{\mathcal{L}}\times\Omega\to\mathcal{G}_{ \mathcal{L}^{\prime}}\) as a transformation \(((V,E,L),\omega)\mapsto(V,E,L^{\prime})\) that maps injectively the node labels from \(L:V\to\mathcal{L}\) to a new set of labels \(L^{\prime}:V\to\mathcal{L}^{\prime}\), e.g. with \(\mathcal{L}^{\prime}=\mathcal{L}\times\mathcal{C}\) and \(L^{\prime}(v)=(L(v),c),\ c\in\mathcal{C}\). This function preserves the graph's underlying structure (the nodes and edges). For the sake of generality, the relabeling function incorporates an additional argument \(\omega\in\Omega\subseteq\mathbb{N}\). This integer can be interpreted as a source of pseudo-randomness involved in the relabeling process (i.e., a random seed). For example, the relabeling function used in RNI [52], due to its pseudo-random nature, can generate non-isomorphic relabeled graphs even from two identical copies of the same graph if it is given two different random seeds. Moreover, we note that node encodings such as the random walk [17] or the Laplacian [16] positional encodings can be cast as relabeling functions.

A relabeling function is an _individualization scheme_ if, for each \((G,\omega)\in\mathcal{G}\times\Omega\), it holds that \(\operatorname{Rel}(G,\omega)\) is a (possibly \(k\)-weakly) individualized graph. Moreover, for \(G\in\mathcal{G}\), let \(\operatorname{Rel}(G):=\{\operatorname{Rel}(G,\omega),\omega\in\Omega\}\) and let \(\operatorname{Rel}(\mathcal{G}):=\{\operatorname{Rel}(G):G\in\mathcal{G}\}\).

Expressive GNNs on individualized graphs are well known to be universal approximators for functions over graphs [1]. In fact, even a GNN with a single message passing layer and a MLP head is enough to provide a universal approximator [15]. We can generalize this result to \(k\)-weakly individualized graphs by noticing that \(k\) message passing layers, each simulating a WL iteration, provide a graph individualized by its node embeddings. The following statement formalizes this intuition.

**Proposition 1**.: _Let \(f:\mathcal{G}\to\{0,1\}\) be an invariant function and \(k\geq 1\). Then there exists \(\theta\in\Theta\) such that \(\operatorname{GNN}_{k,\theta}^{\operatorname{bin}}(G)=f(G)\) for every \((k-1)\)-weakly individualized graph \(G\in\mathcal{G}\)._

The above proposition highlights the importance of node individualization schemes, not only for enhancing model expressivity, but also for managing model complexity. While GNNs are theoretically capable of representing arbitrary functions on WL-amenable graphs (which compose the majority of graphs [4]), they might require a large number of message-passing layers to achieve this. This phenomenon, known as under-reaching [2], can be problematic. GNNs with many layers can suffer from oversmoothing [43] or oversquashing [2], hindering their ability to effectively distinguish graphs in practice. However, as Proposition 1 demonstrates, endowing GNNs with a (possibly \(k\)-weak) individualization scheme can offer sufficient signals for graph discrimination with fewer message-passing iterations, thus mitigating these issues. In Section 5.1, we experimentally investigate this assessment and show that it has implications in real-world scenarios.

## 4 Sample complexity bounds

In this section, we show a general framework to derive sample complexity bounds for GNNs endowed with relabeling functions, for graph-level binary classification tasks.

Let \(\mathcal{D}_{\operatorname{data}}\) be a joint distribution over a set of graphs \(\mathcal{G}\) and binary labels \(Y=\{0,1\}\). Moreover, let \(\mathcal{D}\) be a joint distribution over the domain of \(\mathcal{D}_{\operatorname{data}}\) and integers \(\Omega\subseteq\mathbb{N}\). The integers \(\omega\in\Omega\) model any (pseudo-)randomness within the classification model \(f(G,\omega)\). In the following, we denote by \(x=(G,\omega)\). Given a loss function \(\ell(f(x),y)\), our goal, in accordance with standard learning theory [36], is to bound the difference between the true risk \(R(f)\) and the empirical risk \(\hat{R}(f)\):

\[R_{\ell}(f)=\mathbb{E}_{(x,y)\sim\mathcal{D}}[\ell(f(x),y)]\quad\text{and} \quad\hat{R}_{D,\ell}(f)=\frac{1}{m}\sum_{i=1}^{m}\ell(f(x_{i}),y_{i}),\]

where \(D=\{(x_{i},y_{i})\}_{i}\sim\mathcal{D}^{m}\) represents a training dataset of size \(m\) sampled i.i.d. from \(\mathcal{D}\).

In Sections 4.1 and 4.2 we derive bounds to \(|R_{\ell}(f)-\hat{R}_{D,\ell}(f)|\) based on the VC dimension, while in Section 4.3 we derive more refined bounds based on covering numbers.

### VC dimension

Let \(\ell(f(x),y)=1[f(x)\neq y]\) be the 0-1 loss. Then if \(d\) is the VC dimension of the hypothesis class, we have, with probability \(1-\delta\) over the dataset \(D\sim\mathcal{D}^{m}\), that \(\sup_{f\in\mathcal{F}}|R_{\ell}(f)-\hat{R}_{D,\ell}(f)|\leq\sqrt{2d/m}+O(\sqrt {\log(1/\delta)/m})\)[36, Corollary 3.4]. We define formally the VC dimension of a class of functions on a set.

**Definition 2**.: _Let \(X\) be a set and \(\mathcal{F}\subseteq\{f:X\rightarrow\{0,1\}\}\) an hypothesis class. We say that \(\mathcal{F}\) shatters \(S=(x_{1},\ldots x_{|S|})\) if \(|\big{(}f(x_{1}),\ldots,f(x_{|S|})\big{)}:f\in\mathcal{F}\big{\}}|=2^{|S|}\). Then, we say that \((X,\mathcal{F})\) has VC dimension \(\operatorname{VC}(X,\mathcal{F})=d\) if the largest set \(S\) shattered by \(\mathcal{F}\) has size \(d\), and \(+\infty\) if the size of sets that can be shattered is unbounded._

In particular, we consider a set \(X=\mathcal{G}\times\Omega\) consisting of pairs of graphs and integers. Our hypothesis class \(\mathcal{F}\) comprises binary classification functions formed by the composition of graph relabeling functions (such as individualization schemes) and GNNs that are as expressive as color refinement: \(\mathcal{F}=\operatorname{GNN}_{K}^{\operatorname{bin}}\circ\operatorname{ Rel}:=\{\operatorname{GNN}_{K,\theta}^{\operatorname{bin}}\circ \operatorname{Rel}:\operatorname{GNN}_{K,\theta}^{\operatorname{bin}}\in \operatorname{GNN}_{K}^{\operatorname{bin}}\}\), setting \(K=n_{\max}\) to ensure expressivity. GNNs with color refinement expressivity may require a number of parameters exponential in the maximum graph size \(n_{\max}=\max_{G\in\mathcal{G}}|V_{G}|\)[1], which is a strong assumption we have to make. In the cases where the parameter count constraints the expressive power, the VC bounds from [37, Theorem 6] can be applied instead. The following theorem, which follows from [37, Proposition 2], relates the VC dimension to the WL-isomorphism classes of the graphs in \(\operatorname{Rel}(\mathcal{G})\).

**Theorem 1**.: _Let \(\operatorname{Rel}:\mathcal{G}\times\Omega\rightarrow\mathcal{G}^{\prime}\) be a relabeling function. Then \(\operatorname{VC}\Big{(}\mathcal{G}\times\Omega,\operatorname{GNN}_{K}^{ \operatorname{bin}}\circ\operatorname{Rel}\Big{)}=|\operatorname{Rel}( \mathcal{G})/\simeq_{\operatorname{WL}}|\). If \(\operatorname{Rel}\) is an individualization scheme, \(|\operatorname{Rel}(\mathcal{G})/\simeq_{\operatorname{WL}}|=|\operatorname{ Rel}(\mathcal{G})/\simeq|\)._

We provide specific bounds, proven in Section C.2, for relevant examples of relabeling schemes.

Random Node Initializations [52]The \(\operatorname{RNI}\) scheme perturbs with some random noise the labels of the nodes. In line with the analysis in the original paper, we suppose that the random noise, selected via \(\omega\), takes values in a finite set \(\mathcal{C}\). We then have that a graph \(G\) with \(n\) nodes is mapped to \(O(|\mathcal{C}|^{n}/|Aut(G)|)\) relabeled unordered graphs. In the case of unlabeled graphs with \(n\) nodes, we then have that \(|\operatorname{RNI}(\mathcal{G})/\simeq|=\Theta(\frac{1}{n!}\sigma^{\binom{n}{ 2}}|\mathcal{C}|^{n})=\Theta(|\mathcal{C}|^{n}\cdot|\mathcal{G}|)\).

Relational Pooling [42] and CLIP [15]Let \(G\) be a graph and let \(v_{1},\ldots,v_{n}\) be an arbitrary enumeration of its nodes selected via \(\omega\). The simplest version of the \(\operatorname{RP}\) scheme updates node labels as \(\ell^{\prime}(v_{i})=(\ell(v_{i}),i)\). A more refined version of the scheme partitions the nodes as \(\{V_{1},\ldots,V_{C}\}\) based on their labels and, letting \(V_{c}=(v_{i_{1}},\ldots,v_{i_{|V_{c}|}})\), it updates node labels as \(\ell^{\prime}(v_{i_{j}})=(\ell(v_{i_{j}}),j)\). Then, each graph can be mapped to \(\prod_{c}|V_{c}|!/|Aut(G)|\) relabeled unordered graphs. For unlabeled graphs with \(n\) nodes, this amounts to having \(|\operatorname{RP}(\mathcal{G})/\simeq|=\Theta(2^{\binom{n}{2}})=\Theta(n!\cdot |\mathcal{G}|)\). The 1-CLIP node individualization scheme is equivalent to \(\operatorname{RP}\) on the node partition based on initial labels.

Tinhofer algorithmLet \(G\) be a graph and let \(v_{1},\ldots,v_{n}\) be an ordering given by the Tinhofer [59, 4] algorithm (Section A.2). Then updating node labels as \(\ell^{\prime}(v_{i})=(\ell(v_{i}),i)\) yields a valid individualization scheme \(\operatorname{Tinhofer}\), that is an instance of the general individualization-refinement framework given by [20]. Let the algorithm perform at most (i.e, for any choice of \(\omega\)) \(I\) individualization iterations on graph \(G\), let \(\{V_{1},\ldots,V_{C}\}\) be a partitioning of the nodes of \(G\) based on their labels and let \(R=\max_{c}|V_{c}|\). Then we have that \(|\operatorname{Tinhofer}(G)/\simeq|\leq R^{I}/|Aut(G)|\leq n^{I}/|Aut(G)|\). In particular, as shown in the following lemma, for data distributions restricted to WL-amenable graphs, endowing a GNN with the Tinhofer individualization scheme does not increase its VC dimension.

**Lemma 1**.: _Let \(\mathcal{G}\) be a set of WL-amenable graphs. Then \(\operatorname{VC}\bigl{(}\mathcal{G}\times\Omega,\operatorname{GNN}_{K}^{ \operatorname{bin}}\circ\operatorname{Tinhofer}\bigr{)}=|\mathcal{G}/\simeq|\)._

In fact, the class of graphs for which the Tinhofer individualization scheme does not increase the VC dimension is the class of Tinhofer graphs [4] and is strictly larger than the WL-amenable class.

Positional and structural encodingsDeterministic and equivariant encodings such as the random walk structural encoding [17] or adding graphlet counts [9] yield \(|\operatorname{Rel}(\mathcal{G})|=|\mathcal{G}|\), and therefore the increase in VC dimension is only due in the increase in the number of WL-isomorphism classes, if any. The Laplacian positional encoding [16] is known not to be equivariant in general [62],i.e., there are graphs \(G_{1}\simeq G_{2}\) for which \(\operatorname{Rel}(G_{1},\omega)\not\approx\operatorname{Rel}(G_{2},\omega)\). Therefore we have that \(|\operatorname{Rel}(\mathcal{G})/\simeq_{WL}|>|\mathcal{G}/\simeq_{WL}|\), which leads to an increase in the VC dimension and further motivates the attempts to obtain equivariant Laplacian-based relabeling functions [62; 28].

DiscussionNote that, in general, the number of relabeled graphs given by \(\operatorname{Rel}\) is lower if the relabeling scheme is equivariant and if it does not depend on the second argument \(\omega\), i.e., if it is deterministic. For example, expressive GNNs endowed with the Tinhofer relabeling scheme, when applied to WL-amenable graphs with at most \(n\) nodes, have VC dimension that is lower by a factor \(O(n!)\) compared to equally expressive GNNs endowed with the RP scheme.

Finally, relabeling schemes that rely on the randomness introduced by the second argument \(\omega\) (e.g. randomized schemes) are often enhanced by resampling different \(\omega\) values at each epoch [42; 51], effectively performing data augmentation. As discussed in [56], when the augmented dataset includes all possible transformations of each data point (in this case, all possible graph relabelings), the VC dimension of the model reduces to that of a transformation-invariant model. However, the number of relabeled graphs in our setting often grows super-exponentially with the maximum graph size, e.g., \(n_{\max}!\) for RP, making it difficult to obtain enough samples for good generalization, as shown experimentally in Section 5.2. The theoretical analysis of sample complexity for randomized individualization schemes remains an open question we leave for future work.

### Improved VC bounds for substructure identification

In this section, we showcase that developing sample complexity bounds and relabeling schemes with low VC dimension can guide the design of architectures tailored to specific tasks. In particular, a relevant task that has been studied in past works is substructure identification [12; 28; 45], where the task is to decide if a graph pattern \(P\) appears as a subgraph or induced subgraph of a query graph \(G\), denoted hereafter as \(P\in G\). Notably, message passing GNNs are unable to solve the task in general [12]. We provide a message-passing GNN architecture, similar to the one proposed in [12; Section 5] that, paired with node individualization schemes, is provably expressive for substructure identification and with lower VC dimension compared to the ones presented in Section 2.2. The idea, as in many subgraph GNNs [14; 48; 8; 50], is to transform the original graph into a set of smaller subgraphs and feeding them to a GNN.

In particular, let the task at hand concern a subgraph pattern \(P\) of radius \(\Delta_{P}=\min_{v\in V_{P}}\max_{w\in V_{P}}\delta_{vw}\), where \(\delta_{vw}\) is the length of the shortest path from \(v\) to \(w\). Let a \(k\)-ego-net of node \(v\) in graph \(G\) be the induced subgraph \(\operatorname{EGO}_{v,G,k}\) of nodes \(\{u\in V_{G}:\delta_{uv}\leq k\}\) at distance at most \(k\) from \(v\). Then, a model that is able to recognize if \(P\) appears in any of the \(\Delta_{P}\)-ego-nets of the query graph will be able to solve the task. Note that there could be ego-nets that are not recognizable by a GNN alone (Lemma 6), so we individualize each ego-net with a function \(\operatorname{Rel}\) to guarantee expressivity. Let then

\[h^{\operatorname{ego}}_{v}=\operatorname{GNN}^{\operatorname{bin}}_{1,\theta }\left(\operatorname{Rel}(\operatorname{EGO}_{v,G,\Delta_{P}},\omega)\right) \in\{0,1\},\qquad h_{G}=\max_{v\in V_{G}}h^{\operatorname{ego}}_{v}\in\{0,1\},\]

that is, we run a 1-layer GNN on each individualized (potentially using some randomness from \(\omega\)) ego-net and we aggregate the results using max-pooling. We denote such parametric functions as \(\operatorname{EGONN}^{\operatorname{Rel}}_{\Delta_{P},\theta}\). We have the following results on the expressivity and the VC dimension of the model.

**Theorem 2**.: _Let \(f:\mathcal{G}\to\{0,1\}\) be \(f(G)=\mathbbm{1}\left[P\in G\right]\). Let \(\operatorname{Rel}\) be an individualization scheme. Then there exists \(\theta\in\Theta\) such that \(\operatorname{EGONN}^{\operatorname{Rel}}_{\Delta_{P},\theta}(G)=f(G)\) for every \(G\in\mathcal{G}\)._

**Theorem 3**.: _Let \(\mathcal{G}_{\Delta}\) be the set of ego-nets of radius \(\Delta\) of the graphs of \(\mathcal{G}\). Then \(\operatorname{VC}(\mathcal{G}\times\Omega,\operatorname{EGONN}^{\operatorname{ Rel}}_{\Delta}){\leq}|\operatorname{Rel}(\mathcal{G}_{\Delta})/\simeq|\)._

In general, \(\mathcal{G}_{\Delta}\) is much smaller compared to \(\mathcal{G}\), especially for small \(\Delta\). This leads to the fact that, in general, \(\operatorname{Rel}(\mathcal{G}_{\Delta})\) will be much smaller than \(\operatorname{Rel}(\mathcal{G})\). Thanks to Theorem 1, we then have that the VC dimension of \(\operatorname{EGONN}^{\operatorname{Rel}}_{\Delta}\) is in general lower compared to the one of \(GNN^{\operatorname{bin}}_{K}\circ\operatorname{Rel}\). In particular, relabeling the ego-nets using a scheme with lower \(|\operatorname{Rel}(\mathcal{G}_{\Delta})/\simeq|\), such as the Tinhofer scheme, leads to lower VC dimension compared to schemes that would produce more relabeled ego-nets, such as RP or RNI. This theoretical result is also experimentally validated in Section 5.3.

This approach clearly increases the space complexity compared to the standard GNN model. Indeed, if a graph \(G\) has \(n\) nodes, and each ego net has at most \(n_{E}\) nodes and \(m_{E}\) edges, the disjoint union of the ego-nets of \(G\) has \(nn_{E}\) nodes and \(nm_{E}\) edges.

### Covering numbers

The generalization bounds based on the VC dimension theory provide results that are useful only in restricted cases, e.g., when the task is substructure identification. Indeed, for a maximally expressive GNN model the VC-based bounds state that one should observe all the possible individualized graphs from the data distribution before the model can generalize. We provide an alternative analysis, following a Rademacher-complexity-based approach introduced in [6] and used in [23] for graph classification, which can lead to tighter results.

Let the margin function be \(M(f(x),y)=(2y-1)(2f(x)-1)\in[-1,1]\). Then the margin loss, given a \(\gamma>0\), is defined as

\[\ell(f(x),y)=\mathbbm{1}[M(f(x),y)<0]+(1-M(f(x),y)/\gamma)\mathbbm{1}[0\leq M (f(x),y)\leq\gamma]\ \in[0,1].\]

Let \(\mathcal{F}_{\ell}=\{(x,y)\to\ell(f(x),y):f\in\mathcal{F}\}\) be the set of margin losses, for each predictor \(f\in\mathcal{F}:=\mathrm{GNN}_{K}\circ\mathrm{Rel}\). We then have [36], with probability \(1-\delta\) over the dataset \(D=\{(x_{i},y_{i})\}_{i}\sim\mathcal{D}^{m}\), that \(\sup_{f\in\mathcal{F}}|R_{\ell}(f)-\hat{R}_{D,\ell}(f)|\leq\mathfrak{H}_{D}( \mathcal{F}_{\ell})+O(\sqrt{\ln(1/\delta)/m})\), with \(\mathfrak{H}_{D}(\mathcal{F}_{\ell})\) the empirical Rademacher complexity of \(\mathcal{F}_{\ell}\) on \(D\) (see Definition 3 in the Appendix). In particular, given that \(\mathbb{P}_{(x,y)\sim\mathcal{D}}[\mathbbm{1}[f(x)>0.5]\neq y]\leq R(f)\), one can then bound the probability of making an error at inference time from the empirical loss. We then bound the empirical Rademacher complexity \(\mathfrak{H}_{D}(\mathcal{F}_{\ell})\) via a covering-number-based approach. Let \(D\) be the training dataset and let \(\mathcal{F}_{\ell}|_{D}=\{f|_{D}:\ f\in\mathcal{F}_{\ell}\}\) be the set of margin losses restricted to the dataset.

Given a pseudometric space \((X,\mathrm{dist})\) and a subset \(S\subseteq X\), we call \(C\subseteq X\) an \(\epsilon\)-cover of \(S\) if \(\max_{x\in S}\min_{c\in C}\mathrm{dist}(x,c)\leq\epsilon\) and define the \(\epsilon\)-covering number of \(S\) as \(\mathcal{N}(X,\epsilon,\mathrm{dist})=\min\{|C|:\ C\text{ is an $\epsilon$-cover of $S$}\}\), i.e., the minimum number of balls of radius \(\epsilon\) required to cover \(S\). Similarly, for a finite \(S\), we call \(C\subseteq X\) a \(p\)-norm \(\epsilon\)-cover of \(S\) if \(\binom{1}{|X|}\sum_{x\in X}\min_{c\in C}\mathrm{dist}(x,c)^{p}\)\({}^{\nicefrac{{1}}{{p}}}\leq\epsilon\) and define the \(p\)-norm \(\epsilon\)-covering number of \(S\) as \(\mathcal{N}^{(p)}(X,\epsilon,\mathrm{dist})=\min\{|C|:\ C\text{ is a $p$-norm $\epsilon$-cover of $S$}\}\). With slight abuse of notation, we let \(\mathcal{N}^{(\infty)}(X,\epsilon,\mathrm{dist})=\mathcal{N}(X,\epsilon, \mathrm{dist})\). In particular, we have that the \(p\)-norm \(\epsilon\)-covering number of a metric space is monotonically increasing in \(p\in\mathbb{N}\cup\{\infty\}\).

Moreover, for a finite set \(X\) and \(\mathcal{F}\subseteq\{f:X\to[0,1]\}\), we define the pseudometrics \(\|f_{1}-f_{2}\|_{p}=\binom{1}{|X|}\sum_{x\in X}|f_{1}(x)-f_{2}(x)|^{p}\) for \(p\geq 1\) and \(\|f_{1}-f_{2}\|_{\infty}=\max_{x\in X}|f_{1}(x)-f_{2}(x)|\). By Pollard's bound [47] we then have that

\[\mathfrak{H}_{D}(\mathcal{F}_{\ell})\leq\inf_{\alpha>0}\left(\alpha+\sqrt{2 \log\mathcal{N}(\mathcal{F}_{\ell}|_{D},\alpha,\|\cdot\|_{1})/|D|}\right).\]

We bound the covering number of the class of functions \(\mathcal{F}_{\ell}\) by covering the space of graphs and assuming the Lipschitzness of GNNs, similarly to the approaches in [19] and [46]. In fact, we provide a generalization of a covering bound for Lipschitz functions [24, Lemma 5.2], that relaxes the Kolmogorov-Tikhomirov bound [58, Eq. 238], tailored to the aforementioned Rademacher bound.

**Lemma 2**.: _Let \((X,\mathrm{dist})\) be a pseudometric space and \(S\subseteq X\) a finite subset. Let \(\mathcal{F}\subseteq\{f:S\to[0,1]\}\) be a set of \(C\)-Lipschitz functions. Let \(p,q\in\mathbb{N}\cup\{+\infty\},q\geq p\geq 1\). Then_

\[\log\mathcal{N}(\mathcal{F},\alpha,\|\cdot\|_{p})\leq\log(1/\alpha+1)\cdot \mathcal{N}^{(q)}\left(S,\frac{\alpha}{2C},\mathrm{dist}\right).\]

**Theorem 4**.: _Let \(\mathcal{F}_{\ell}\) be the set of margin losses for predictors \(\mathcal{F}:=\mathrm{GNN}_{K}\circ\mathrm{Rel}\). Let \(\mathrm{Rel}(D)=\{G^{\prime}=\mathrm{Rel}(G,\omega):\ (G,\omega,y)\in D\}\) be the graphs of the dataset after the relabeling, endowed with a pseudometric \(\mathrm{dist}\). Let functions in \(\mathrm{GNN}_{K}\) be \(C\)-Lipschitz continuous with respect to \(\mathrm{dist}\). Then_

\[\mathfrak{H}_{D}(\mathcal{F}_{\ell})\leq\inf_{\alpha>0}\left(\alpha+|D|^{- \nicefrac{{1}}{{2}}}\sqrt{2\log(1/\alpha+1)\cdot 4\mathcal{N}^{(1)}\left(\mathrm{Rel}(D),\frac{ \alpha\gamma}{4C},\mathrm{dist}\right)}\right).\]

Theorem 4 relies on the assumption that the GNN models are Lipschitz with respect to some pseudometric \(\mathrm{dist}\) on graphs, such as the edit distance [55, 44], the Tree Mover distance [13] or the WWL distance [60]. This assumption, albeit strong, is often taken when considering the generalization of GNNs [28]. Note that, for small \(\alpha\) and arbitrarily complex GNNs, i.e. \(C>\!>1\), we have that the covering number is proportional to the number of WL-isomorphic graphs in the relabeled dataset \(\mathrm{Rel}(D)\), effectively retrieving the bounds obtained by the VC dimension. When \(\mathrm{GNN}_{K}\) is instead simpler, i.e. it has smaller Lipschitz constant \(C\), the space of graphs can be covered with fewer balls, and we obtain better sample complexity.

DiscussionLet the graphs of the dataset \(D_{G}=\{G:(G,\omega,y)\in D\}\) and the relabeled graphs \(\mathrm{Rel}(D)=\{\mathrm{Rel}(G,\omega):(G,\omega,y)\in D\}\) be subsets of the same metric space \((\mathcal{G},\mathrm{dist})\). The results of Theorem 4 suggest that if a labeling function does not increase much the covering number of the dataset, i.e. \(\mathcal{N}^{(1)}\left(\mathrm{Rel}(D),\epsilon,\mathrm{dist}\right)\simeq \mathcal{N}^{(1)}\left(D_{G},\epsilon,\mathrm{dist}\right)\), then the model \(\mathrm{GNN}_{K}\circ\mathrm{Rel}\) would have roughly the same sample complexity as \(\mathrm{GNN}_{K}\) while being more expressive.

In particular, this can be achieved if the relabeling scheme maps pairs of graphs with low distance (e.g. \(\mathrm{dist}(G_{1},G_{2})\leq\epsilon\)) to pairs of relabeled graphs with low distance. An example of such graphs at low distance could be a pair of graphs that differ only in the label of a node belonging to a singleton color class, like graphs \(G_{1}\) and \(G_{2}\) in Figure 1. These two graphs could be, for example, two molecules where an atom is substituted with a similar one, or two proteins where an amino-acid is substituted with a functionally-similar one [44].

Driven by this intuition, we design a (\(k\)-weak) individualization scheme, which we call _weak Tinhofer_ scheme \(\mathrm{Tinhofer}_{W}\), that relabels only few nodes. The scheme obtains an ordering \(v_{1},\ldots,v_{n}\) of the nodes via the Tinhofer algorithm. Let \(\{V_{1},\ldots,V_{C}\}\) be a partition of the nodes of graph \(G\) into color classes after \(k\) color refinement steps and let \(V_{c}=(v_{i_{1}},\ldots,v_{i_{|V_{c}|}})\). The scheme then relabels nodes by appending their position in the ordering within their partition as \(\ell^{\prime}(v_{i_{j}})=(\ell(v_{i_{j}}),j)\). For nodes that belong to singleton classes, which is common in graphs such as molecules or random graphs, no relabeling is necessary.

See Figure 1 for a visual example of a comparison between \(\mathrm{Tinhofer}\) and \(\mathrm{Tinhofer}_{\mathrm{W}}\) on the pair of graphs \(G_{1}\) and \(G_{2}\). We have, for a \(u\in V_{G}\), that \(\exists\phi:V_{G_{1}}\to V_{G_{2}}\) such that \(\{\phi(v),\phi(w)\}\in E_{G_{2}}\) iff \(\{v,w\}\in E_{G_{1}}\), \(L_{G_{2}}(\phi(v))=L_{G_{1}}(v),\forall v\neq u\in V_{G_{1}}\) and \(L_{G_{2}}(\pi(u))\neq L_{G_{1}}(u)\). Then \(\mathrm{dist}(G_{1},G_{2})\) would be given by the cost of substituting \(L_{G_{2}}(\pi(u))\) with \(L_{G_{1}}(u)\). Consider the graphs produced by the \(\mathrm{Tinhofer}_{W}\) procedure. Here we have that the labels of nodes that matched in the original graphs still match in the relabeled graphs. By contrast, with the graphs produced by the \(\mathrm{Tinhofer}\) procedure, the number of node labels that do not match could be arbitrarily large.

This scheme ensures that the relabeled graphs are \(k\)-weakly individualized, while yielding no increase in the VC dimension for WL-amenable graphs (Lemma 1) and increasing the covering numbers only marginally. This in general yields better generalization, as shown experimentally in Section 5.4.

## 5 Experimental evaluation

In this section, we provide proof-of-concept experimental evidence about our theoretical results. In particular, Section 5.1 investigates the empirical expressivity of GNNs with and without individualization schemes, Section 5.2 and Section 5.3 validate respectively the general VC dimension bounds and the substructure identification ones, and Section 5.4 explores the results on covering numbers. Details about the datasets, experimental setup, tasks and infrastructure are reported in Section E. Code and datasets are available at https://github.com/BorgwardtLab/NodeIndividualizedGNNs.

### Expressivity

To highlight the efficacy of individualization schemes in practical applications, we compare a GNN endowed with \(\mathrm{Tinhofer}\) to ordinary GNNs on challenging datasets. In particular, we consider subsets of the real-world datasets MCF-7 and Peptites-func, for which at least six message-passing iterations are necessary to distinguish all pairs of graphs, and synthetic datasets Cycles-pin and CSL-pin, for which nine, resp. five, iterations are necessary. To compare the performance between

Figure 1: Comparison between \(\mathrm{Tinhofer}\) and \(\mathrm{Tinhofer}_{\mathrm{W}}\). _Panel (a):_ Two graphs, where letters indicate initial node labels. The Tinhofer algorithm finds a canonical ordering on the two graphs. _Panel (b):_ The \(\mathrm{Tinhofer}\) scheme concatenates the position of the node in the ordering to the node label. The relabeled graphs have edit distance 3. _Panel (c):_ The \(\mathrm{Tinhofer}_{\mathrm{W}}\) scheme concatenates the position of the node within its WL color class. The edit distance remains 1, as in the original graphs.

the two methods, we measure the models' capabilities to distinguish non-isomorphic graphs. Figure 2 shows the accuracy obtained by ordinary GNNs with a theoretically sufficient number of layers as well as a 1-layer GNN endowed with \(\mathrm{Tinhofer}\). While the ordinary GNNs can achieve a perfect score for both synthetic datasets, their learning process is slow and unstable. For the real-world datasets, it can be observed that ordinary GNNs do not achieve a score close to \(100\%\) within the tested 1000 epochs at all. These observations may be explained by the fact that the model has to learn to propagate structural information for several layers, possibly encountering oversquashing issues. In contrast, the GNN with \(\mathrm{Tinhofer}\) converges faster and more stably with only a single GNN layer. The results particularly emphasize that the Tinhofer individualization scheme can improve the expressive power in shallow GNNs, demonstrating the value of our approach in practical scenarios.

### VC dimension

We empirically evaluate the generalization bounds based on the VC dimension derived in Section 4.1. Recall that for an individualization scheme Rel and a set of graphs \(\mathcal{G}\), the generalization gap is bounded by the VC dimension \(d=|\mathrm{Rel}(\mathcal{G})/\simeq|\). We evaluate RNI, RP, and Tinhofer using datasets of circular skip link (CSL) [42] graphs of sizes \(n=17,41,83\) and 3-regular graphs of sizes \(n=16,32,64\). The task is to distinguish non-isomorphic graphs. Figure 3 shows the generalization gap with increasing numbers of permuted copies of the graphs in the training set. RNI and RP, which generate highly randomized individualizations, require several samples to generalize effectively. As graph size \(n\) increases, the VC dimension increases, resulting in a greater generalization gap. Interestingly, \(\mathrm{Tinhofer}\) perfectly learns the target function on CSL graphs with only a single copy of the graphs, as the individualization is in fact canonical.

### Substructure identification

To assess the expressivity and generalization of GNNs for substructure identification, we created four challenging datasets, each comprising 1000 unlabeled 3-regular graphs. The task is to detect whether

Figure 3: Difference between test and training accuracy for GNNs with the \(\mathrm{RNI}\), \(\mathrm{RP}\) and \(\mathrm{Tinhofer}\) individualization schemes, on datasets of CSL and 3-regular graphs of various sizes.

Figure 2: Accuracy for \(\mathrm{GNN}_{K}\) and \(\mathrm{GNN}_{1}\circ\mathrm{Tinhofer}\) over 1000 epochs on synthetic datasets Cycles-pin and CSL-pin and real-world datasets MCF–7 [65; 38] and Peptides-func [57; 18].

graphs contain the induced subgraphs: 3-cycle \(C_{3}\), 4-cycle \(C_{4}\), 5-cycle \(C_{5}\), or complete bipartite graph \(K_{2,3}\). We tested GNNs with and without individualization schemes, along with our novel architecture \(\mathrm{EGONN}_{\Delta_{P}}\). As reported in Table 1, GNNs lacking individualization fail to distinguish between graphs. Those with RP or Tinhofer individualizations tend to overfit on training data but struggle to generalize. In contrast, \(\mathrm{EGONN}_{\Delta_{P}}\) models generally exhibit a smaller generalization gap. Not providing individualizations to ego-nets results in a small generalization gap, but often fails to fit the training data. RP individualization enables the model to fit training data but may lead to overfitting. The Tinhofer individualization yields, as predicted by the theory, the best performance.

### Covering numbers

To validate the theoretical results on covering numbers from Section 4.3, we use two molecular (NCI1 and Mutagenicity) and two social network (IMDB-b and COLLAB-b) datasets. As suggested by the theory, we use as a proxy for the sample complexity the ratio between the covering number (for simplicity fixing \(\epsilon=0.05\), see Section F for complete results) of the relabeled dataset and the dataset size \(\hat{\mathcal{N}}=\mathcal{N}^{(1)}(\mathrm{Rel}(D),\,\epsilon,\mathrm{dist}) /|D|\). As a (pseudo-)metric, we use the distance used by the WWL kernel [60]. We report \(\hat{\mathcal{N}}\), together with the test accuracy and the difference between test and train accuracy, in Table 2 for GNNs with no individualizations, with RP, RNI, Tinhofer and weak Tinhofer individualizations, and with a Laplacian-encoding-based relabeling. We observe that across datasets \(\hat{\mathcal{N}}\) strongly correlates with the generalization gap, confirming empirically the intuition provided by the theory. Moreover, we observe that on molecular datasets, that are easily recognizable by the WL test, individualizations don't help expressivity. On the other hand, on unlabeled datasets such as COLLAB-b, ordinary GNNs might struggle to converge to good local minima, and the additional information given by relabeling schemes usually helps. In fact, it is common in the literature [64; 41] to relabel such unlabled graphs with, e.g., degree centralities.

## 6 Conclusion

In this paper, we developed novel sample complexity bounds for graph neural networks endowed with individualization schemes. Several research directions are left open for future work, including the analysis of data augmentation for individualization schemes via resampling, and the development of tighter covering bounds for specific model architectures. We envision that this work will inspire the development of new, practical graph learning models that are both theoretically sound and empirically effective, ultimately advancing the field of graph representation learning.

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline  & & \multicolumn{4}{c}{Datasets} & \multicolumn{4}{c}{COLLAB-b} \\ \cline{3-11} GNN & Rel & Train & Test & Train & Test & Train & Test & Diff. & \(\hat{\mathcal{N}}\) & Test & Diff. \\ \hline \multirow{4}{*}{\(\mathrm{GNN}_{\Delta_{P}}\)} & None & 0.090 & 83.8\(\pm\) & 16.1\(\pm\) & 0.156 & 83.0\(\pm\) & 17.0\(\pm\) & 0.025 & 71.8\(\pm\) & 3.6\(\pm\) & 0.241 & 56.1\(\pm\) & 1.1\(\pm\) \\  & RP & 0.719 & 67.8\(\pm\) & 32.2\(\pm\) & 0.746 & 70.9\(\pm\) & 29.1\(\pm\) & 0.443 & 65.4\(\pm\) & 33.0\(\pm\) & 0.248 & 67.5\(\pm\) & 3.1\(\pm\) \\  & RNI & 0.705 & 67.1\(\pm\) & 32.9\(\pm\) & 0.717 & 73.1\(\pm\) & 26.9\(\pm\) & 0.580 & 62.4\(\pm\) & 37.6\(\pm\) & 0.547 & 56.8\(\pm\) & 1.2\(\pm\) \\  & Tinhofer & 0.657 & 71.5\(\pm\) & 28.4\(\pm\) & 0.697 & 49.4\(\pm\) & 25.1\(\pm\) & 0.266 & 70.0\(\pm\) & 19.1\(\pm\) & 0.471 & 68.3\(\pm\) & 20.4\(\pm\) \\  & Tinhofer & 0.203 & 81.9\(\pm\) & 180.0\(\pm\) & 0.232 & 82.6\(\pm\) & 17.4\(\pm\) & 0.133 & 67.8\(\pm\) & 21.3\(\pm\) & 0.310 & 79.5\(\pm\) & 136.6\(\pm\) \\  & LPE & 0.589 & 77.3\(\pm\) & 22.6\(\pm\) & 0.485 & 76.2\(\pm\) & 23.8\(\pm\) & 0.271 & 68.0\(\pm\) & 21.1\(\pm\) & 0.409 & 74.6\(\pm\) & 21.6\(\pm\) \\ \hline \multirow{4}{*}{\(\mathrm{GNN}_{\Delta_{P}}\)} & None & 0.090 & 81.8\(\pm\) & 18.1\(\pm\) & 0.156 & 81.5\(\pm\) & 18.4\(\pm\) & 0.025 & 71.4\(\pm\) & 22.2\(\pm\) & 0.241 & 75.3\(\pm\) & 0.0\(\pm\) \\  & RP & 0.719 & 67.4\(\pm\) & 32.6\(\pm\) & 0.746 & 71.2\(\pm\) & 28.8\(\pm\) & 0.433 & 63.8\(\pm\) & 36.4\(\pm\) & 0.485 & 71.5\(\pm\) & 18.1\(\pm\) \\  & RNI & 0.705 & 68.0\(\pm\) & 32.0\(\pm\) & 0.717 & 72.7\(\pm\) & 27.3\(\pm\) & 0.580 & 63.6\(\pm\) & 36.4\(\pm\) & 0.547 & 72.1\(\pm\) & 19.6\(\pm\) \\  & Tinhofer & 0.657 & 72.9\(\pm\) & 26.9\(\pm\) & 0.697 & 73.1\(\pm\) & 26.9\(\pm\) & 0.266 & 68.6\(\pm\) & 30.5\(\pm\) & 0.471 & 75.2\(\pm\) & 19.5\(\pm\) \\  & Tinhofer & 0.657 & 81.8\(\pm\) & 18.1\(\pm\) & 0.282 & 81.2\(\pm\) & 18.8\(\pm\) & 0.133 & 69.4\(\pm\) & 19.6\(\pm\) & 0.310 & 80.8\(\pm\) & 12.2\(\pm\) \\  & LPE & 0.589 & 76.4\(\pm\) & 23.5\(\pm\) & 0.485 & 75.4\(\pm\) & 24.6\(\pm\) & 0.271 & 68.4\(\pm\) & 20.7\(\pm\) & 20.7\(\pm\) & 0.409 & 75.8\(\pm\) & 19.9\(\pm\) \\ \hline \end{tabular}
\end{table}
Table 2: Covering numbers and classification accuracies on real-world datasets.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c} \hline \hline  & & \multicolumn{4}{c}{Datasets} & \multicolumn{4}{c}{COLLAB-b} \\ \cline{3-11}  & & & \multicolumn{4}{c}{Multigraphicity} & \multicolumn{4}{c}{DMDB-b} & \multicolumn{4}{c}{COLLAB-b} \\ \cline{3-11} GNN & Rel & \(\hat{\mathcal{N}}\) & Test & Diff. & \(\hat{\mathcal{N}}\) & Test & Diff. & \(\hat{\mathcal{N}}\) & Test & Diff. & \(\hat{\mathcal{N}}\) & Test & Diff. \\ \hline \multirow{4}{*}{\(\mathrm{GNN}_{\Delta}\)} & None & 0.090 & 83.8\(\pm\) & 161.1\(\pm\) & 0.156 & 83.0\(\pm\) & 17.0\(\pm\) & 0.025 & 71.8\(\pm\) & 3.6\(\pm\) & 0.241 & 56.1\(\pm\) & 1.1\(\pm\) \\  & RP & 0.719 & 67.8\(\pm\) & 32.1\(\pm\) & 0.746 & 70.9\(\pm\) & 29.

## Acknowledgements

The authors would like to thank Dr. Janko Sattler, Philip Hartout and the reviewers for their feedback.

## References

* Abboud et al. [2021] Ralph Abboud, Ismail Ilkan Ceylan, Martin Grohe, and Thomas Lukasiewicz. The surprising power of graph neural networks with random node initialization. In _Proceedings of the Joint Conference on Artificial Intelligence (IJCAI)_, 2021.
* Alon and Yahav [2021] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications. In _International Conference on Learning Representations (ICLR)_, 2021.
* Anders and Schweitzer [2021] Markus Anders and Pascal Schweitzer. Engineering a fast probabilistic isomorphism test. In _Proceedings of the Workshop on Algorithm Engineering and Experiments (ALENEX)_, 2021.
* Arvind et al. [2017] Vikram Arvind, Johannes Kobler, Gaurav Rattan, and Oleg Verbitsky. Graph isomorphism, color refinement, and compactness. _Computational Complexity_, 26:627-685, 2017.
* Barcelo et al. [2020] Pablo Barcelo, Egor V. Kostylev, Mikael Monet, Jorge Perez, Juan Reutter, and Juan Pablo Silva. The logical expressiveness of graph neural networks. In _International Conference on Learning Representations (ICLR)_, 2020.
* Bartlett et al. [2017] Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for neural networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2017.
* Berkholz et al. [2017] Christoph Berkholz, Paul Bonsma, and Martin Grohe. Tight lower and upper bounds for the complexity of canonical colour refinement. _Theory of Computing Systems_, 60(4):581-614, May 2017.
* Bevilacqua et al. [2021] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. In _International Conference on Learning Representations (ICLR)_, 2021.
* Bouritsas et al. [2023] Giorgos Bouritsas, Fabrizio Frasca, Stefanos Zafeiriou, and Michael M. Bronstein. Improving graph neural network expressivity via subgraph isomorphism counting. _IEEE Trans. Pattern Anal. Mach. Intell._, 45(1):657-668, 2023.
* Cai et al. [1992] Jin-Yi Cai, Martin Furer, and Neil Immerman. An optimal lower bound on the number of variables for graph identification. _Combinatorica_, 12(4):389-410, 1992.
* Canturk et al. [2024] Semih Canturk, Remming Liu, Olivier Lapointe-Gagne, Vincent Letourneau, Guy Wolf, Dominique Beaini, and Ladislav Rampasek. Graph positional and structural encoder. In _International Conference on Machine Learning (ICML)_, 2024.
* Chen et al. [2020] Zhengdao Chen, Lei Chen, Soledad Villar, and Joan Bruna. Can graph neural networks count substructures? In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 33, pages 10383-10395, 2020.
* Chuang and Jegelka [2022] Ching-Yao Chuang and Stefanie Jegelka. Tree mover's distance: Bridging graph metrics and stability of graph neural networks. _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* Cotta et al. [2021] Leonardo Cotta, Christopher Morris, and Bruno Ribeiro. Reconstruction for powerful graph representations. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* Dasoulas et al. [2020] George Dasoulas, Ludovic Dos Santos, Kevin Scaman, and Aladin Virmaux. Coloring graph neural networks for node disambiguation. In _Proceedings of the Joint Conference on Artificial Intelligence (IJCAI)_, 2020.
* Dwivedi et al. [2023] Vijay Prakash Dwivedi, Chaitanya K Joshi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Benchmarking graph neural networks. _Journal of Machine Learning Research (JMLR)_, 24(43):1-48, 2023.
* Dwivedi et al. [2022] Vijay Prakash Dwivedi, Anh Tuan Luu, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Graph neural networks with learnable structural and positional representations. In _International Conference on Learning Representations (ICLR)_, 2022.
* Dwivedi et al. [2022] Vijay Prakash Dwivedi, Ladislav Rampasek, Michael Galkin, Ali Parviz, Guy Wolf, Anh Tuan Luu, and Dominique Beaini. Long range graph benchmark. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:22326-22340, 2022.

* [19] Bryn Elesedy. _Symmetry and Generalisation in Machine Learning_. PhD thesis, PhD thesis, University of Oxford, 2023.
* [20] Billy Joe Franks, Markus Anders, Marius Kloft, and Pascal Schweitzer. A systematic approach to universal random features in graph neural networks. _Transactions on Machine Learning Research_, 2023.
* [21] Billy Joe Franks, Christopher Morris, Ameya Velingker, and Floris Geerts. Weisfeiler-leman at the margin: When more expressivity matters. In _International Conference on Machine Learning (ICML)_, pages 13885-13926, 2024.
* [22] Fabrizio Frasca, Beatrice Bevilacqua, Michael Bronstein, and Haggai Maron. Understanding and extending subgraph gnns by rethinking their symmetries. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [23] Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of graph neural networks. In _International Conference on Machine Learning (ICML)_, pages 3419-3430, 2020.
* [24] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Efficient regression in metric spaces via approximate lipschitz extension. In Edwin Hancock and Marcello Pelillo, editors, _Similarity-Based Pattern Recognition_, pages 43-58, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg.
* [25] Florian Graf, Sebastian Zeng, Bastian Rieck, Marc Niethammer, and Roland Kwitt. On measuring excess capacity in neural networks. _Advances in Neural Information Processing Systems (NeurIPS)_, 35:10164-10178, 2022.
* [26] Martin Grohe. _Descriptive complexity, canonisation, and definable graph structure theory_, volume 47. Cambridge University Press, 2017.
* [27] Frank Harary and Edgar M Palmer. _Graphical enumeration_. Elsevier, 2014.
* [28] Yinan Huang, William Lu, Joshua Robinson, Yu Yang, Muhan Zhang, Stefanie Jegelka, and Pan Li. On the stability of expressive positional encodings for graph neural networks. _arXiv preprint arXiv:2310.02579_, 2023.
* [29] Haotian Ju, Dongyue Li, Aneesh Sharma, and Hongyang R Zhang. Generalization in graph neural networks: Improved pac-bayesian bounds on graph diffusion. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2023.
* [30] Jeroen Kazius, Ross McGuire, and Roberta Bursi. Derivation and validation of toxicophores for mutagenicity prediction. _Journal of medicinal chemistry_, 48 1:312-20, 2005.
* [31] Kristian Kersting, Nils M. Kriege, Christopher Morris, Petra Mutzel, and Marion Neumann. Benchmark data sets for graph kernels, 2016. http://graphkernels.cs.tu-dortmund.de.
* [32] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [33] Renjie Liao, Raquel Urtasun, and Richard Zemel. A PAC-bayesian approach to generalization bounds for graph neural networks. In _International Conference on Learning Representations (ICLR)_, 2020.
* [34] Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 32, 2019.
* [35] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 32, 2019.
* [36] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of machine learning_. MIT press, 2018.
* [37] Christopher Morris, Floris Geerts, Jan Tonshoff, and Martin Grohe. WL meet VC. In _International Conference on Machine Learning (ICML)_, 2023.
* [38] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. In _ICML 2020 Workshop on Graph Representation Learning and Beyond (GRL+ 2020)_, 2020.
* [39] Christopher Morris, Yaron Lipman, Haggai Maron, Bastian Rieck, Nils M Kriege, Martin Grohe, Matthias Fey, and Karsten Borgwardt. Weisfeiler and leman go machine learning: The story so far. _The Journal of Machine Learning Research_, 24(1):15865-15923, 2023.

* [40] Christopher Morris, Gaurav Rattan, Sandra Kiefer, and Siamak Ravanbakhsh. Spoenets: Sparsity-aware permutation-equivariant graph networks. In _International Conference on Machine Learning (ICML)_, 2022.
* [41] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen, Gaurav Rattan, and Martin Grohe. Weisfeiler and leman go neural: Higher-order graph neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2019.
* [42] Ryan Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. Relational pooling for graph representations. In _International Conference on Machine Learning (ICML)_, pages 4663-4673. PMLR, 2019.
* [43] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. In _International Conference on Learning Representations (ICLR)_, 2020.
* [44] Paolo Pellizzoni, Carlos Oliver, and Karsten Borgwardt. Structure- and function-aware substitution matrices via learnable graph matching. In _Research in Computational Molecular Biology_, 2024.
* [45] Paolo Pellizzoni and Fabio Vandin. VC-dimension and rademacher averages of subgraphs, with applications to graph mining. In _International Conference on Data Engineering (ICDE)_, 2023.
* [46] Mircea Petrache and Shubhendu Trivedi. Approximation-generalization trade-offs under (approximate) group equivariance. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 36, 2023.
* [47] David Pollard. Empirical processes: theory and applications. Ims, 1990.
* [48] Chendi Qian, Gaurav Rattan, Floris Geerts, Mathias Niepert, and Christopher Morris. Ordered subgraph aggregation networks. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [49] Kaspar Riesen and Horst Bunke. IAM graph database repository for graph based pattern recognition and machine learning. In Niels da Vitoria Lobo, Takis Kasparis, Fabio Roli, James Tin-Yau Kwok, Michael Georgipopoulos, Georgios C. Anagnostopoulos, and Marco Loog, editors, _Structural, Syntactic, and Statistical Pattern Recognition, Joint IAPR International Workshop, SSPR & SPR 2008, Orlando, USA, December 4-6, 2008. Proceedings_, volume 5342 of _Lecture Notes in Computer Science_, pages 287-297. Springer, 2008.
* [50] Dylan Sandfelder, Priyesh Vijayan, and William L. Hamilton. Ego-gnns: Exploiting ego structures in graph neural networks. In _IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, 2021.
* [51] Ryoma Sato. A survey on the expressive power of graph neural networks. _arXiv preprint arXiv:2003.04078_, 2020.
* [52] Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Random features strengthen graph neural networks. In _Proceedings of the SIAM International Conference on Data Mining (SDM)_, pages 333-341. SIAM, 2021.
* [53] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural networks. In _International Conference on Machine Learning (ICML)_, pages 9323-9332. PMLR, 2021.
* [54] Franco Scarselli, Ah Chung Tsoi, and Markus Hagenbuchner. The vapnik-chervonenkis dimension of graph and recursive neural networks. _Neural Networks_, 108:248-259, 2018.
* [55] Francesc Serratosa. Redefining the graph edit distance. _SN Computer Science_, 2(6):438, Aug 2021.
* [56] Han Shao, Omar Montasser, and Avrim Blum. A theory of pac learnability under transformation invariances. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 35, pages 13989-14001, 2022.
* [57] Sandeep Singh, Kumardeep Chaudhary, Sandeep Kumar Dhanda, Sherry Bhalla, Salman Sadullah Usmani, Ankur Gautam, Abhishek Tukait, Piyush Agrawal, Deepika Mathur, and Gajendra PS Raghava. Satpdb: a database of structurally annotated therapeutic peptides. _Nucleic acids research_, 44(D1):D1119-D1126, 2016.
* [58] Vladimir M Tikhomirov. \(\varepsilon\)-entropy and \(\varepsilon\)-capacity of sets in functional spaces. _Selected Works of AN Kolmogorov: Volume III: Information Theory and the Theory of Algorithms_, pages 86-170, 1993.
* [59] G. Tinhofer. A note on compact graphs. _Discrete Applied Mathematics_, 30(2):253-264, 1991.
* [60] Matteo Togninalli, Elisabetta Ghisu, Felipe Llinares-Lopez, Bastian Rieck, and Karsten Borgwardt. Wasserstein weisfeiler-lehman graph kernels. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2019.

* [61] Nikil Wale and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. In _Proceedings of the 6th IEEE International Conference on Data Mining (ICDM)_, pages 678-689. IEEE Computer Society, 2006.
* [62] Haorui Wang, Haoteng Yin, Muhan Zhang, and Pan Li. Equivariant and stable positional encoding for more powerful graph neural networks. In _International Conference on Learning Representations (ICLR)_, 2021.
* [63] Boris Weisfeiler and Andrei Leman. The reduction of a graph to canonical form and the algebra which appears therein. 1968.
* [64] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations (ICLR)_, 2018.
* [65] Xifeng Yan, Hong Cheng, Jiawei Han, and Philip S Yu. Mining significant graph patterns by leap search. In _Proceedings of the 2008 ACM SIGMOD international conference on Management of data_, pages 433-444, 2008.
* [66] Pinar Yanardag and S. V. N. Vishwanathan. Deep graph kernels. In Longbing Cao, Chengqi Zhang, Thorsten Joachims, Geoffrey I. Webb, Dragos D. Margineantu, and Graham Williams, editors, _Proceedings of the ACM International Conference on Knowledge Discovery and Data Mining (KDD)_, pages 1365-1374. ACM, 2015.
* [67] Bohang Zhang, Guhao Feng, Yiheng Du, Di He, and Liwei Wang. A complete expressiveness hierarchy for subgraph gnns via subgraph weisfeiler-lehman tests. In _International Conference on Machine Learning (ICML)_, 2023.
* [68] Sicheng Zhu, Bang An, and Furong Huang. Understanding the generalization benefit of model invariance from a data perspective. In _Advances in Neural Information Processing Systems (NeurIPS)_, volume 34, pages 4328-4341, 2021.

Additional details

### Expressive graph neural networks

In [64] it was shown that there exist injective functions for \(f_{\text{agg}}\), \(f_{\text{upd}}\) and the graph-level readout function that lead to GNNs that are provably as expressive as color refinement.

An example of such functions that leads to models that are provably as expressive as color refinement [41], denoting \(\|\) as concatenation, is

\[h_{v}^{k}=\mathrm{mlp}\Big{(}h_{v}^{k-1}\Big{\|}\sum_{u\in\mathcal{N}(v)}h_{u}^ {k-1}\Big{)}\in\mathbb{R}^{d_{k}}\qquad h_{G}=\mathrm{mlp}\Big{(}\sum_{v\in V _{G}}h_{v}^{K}\Big{)}\in[0,1].\]

In fact, provided that different node labels are encoded to linearly independent \(h_{v}^{0}\)'s, even the following simpler architecture, denoting with \(\sigma\) a nonlinear function such as ReLU, is as expressive as color refinement [41]:

\[h_{v}^{k}=\sigma\Big{(}W_{1}^{k}h_{v}^{k-1}+W_{2}^{k}\sum_{u\in\mathcal{N}(v)}h _{u}^{k-1}\Big{)}\in\mathbb{R}^{d_{k}}\qquad h_{G}=\mathrm{mlp}\Big{(}\sum_{v \in V_{G}}h_{v}^{K}\Big{)}\in[0,1].\]

### Tinhofer algorithm

The Tinhofer algorithm [59; 4] returns an ordering of the nodes of a graph. In particular, it works as follows.

1. Run color refinement on \(G\) and obtain the stable color partition \(\mathcal{P}(G)\).
2. Given the partition \(\mathcal{P}(G)\) * If all nodes belong to a singleton color class, return the ordering of the nodes based on the lexicographic order of their colors. * Else, pick the color class with at least two nodes with the lexicographically smallest color. Individualize one arbitrary node in such class by assigning it the smallest unused color. Then, go to step 1.

Clearly, the algorithm will perform at most \(|V_{G}|\) individualization steps. In fact, for several classes of graphs a constant number \(I\) of individualizations suffices [20]. Therefore, the algorithm can run in polynomial time.

In fact, the Tinhofer algorithm can be generalized to the Individualization-Refinement scheme [20; 3], where the refinement operation and the selection of the nodes to be individualized can be more general than the ones used in Tinhofer.

Computing a canonical stable partition on a graph \(G\) can be done in \(O((|V_{G}|+|E_{G}|)\log|V_{G}|)\)[7; Theorem 9]. Moreover, the Tinhofer algorithm can also be implemented so that it takes \(O((|V_{G}|+|E_{G}|)\log|V_{G}|)\) time [7; Theorem 10].

## Appendix B Limitations

This paper provides sample complexity results for graph neural networks with relabeling schemes, but nonetheless leaves several research directions open. First, most individualization schemes benefit from resampling at random the graph individualizations at each epoch, effectively performing data augmentation. Studying the theoretical properties of such techniques would be a major step forward in understanding the practical performance of GNNs with individualization schemes. Secondly, our results on covering numbers are based on the strong assumption of Lipschitzness of GNN models with respect to some metric on graphs. Alleviating such assumptions would yield stronger theoretical results. We hope that these open questions will foster new research efforts in this field.

Definitions and Lemmas

In this section, we provide formal definitions and lemmas, which were only informally described in the main paper.

### Definitions

**Definition 3**.: _Let \(X\) be a set and \(S=(x_{1},\ldots,x_{n})\) a sample of \(n\) elements from \(X\). Let \(\mathcal{F}\subseteq\{f:X\longrightarrow[0,1]\}\) be a family of functions. Let also \(\sigma_{i},i=1,\ldots,n\) be independent and identically distributed Rademacher random variables, i.e. taking value \(1\) or \(-1\) with probability \(1/2\). The empirical Rademacher complexity is the quantity_

\[\hat{\mathfrak{R}}_{S}(\mathcal{F})=\mathbb{E}_{\sigma}\left[\sup_{f\in \mathcal{F}}\frac{1}{n}\sum_{i=1}^{n}\sigma_{i}f(x_{i})\right]\]

_where \(\mathbb{E}_{\sigma}\) denotes the expectation taken only with respect to the \(\sigma_{i}\)'s, conditionally on the sample \(S\)._

### Lemmas from Section 4.1

**Lemma 3**.: _Let \(\mathrm{RNI}\) be the RNI individualization scheme [52] with random noise taking values in \(\mathcal{C}\) and \(G\) a graph with \(n\) nodes. Then \(|\mathrm{RNI}(G)/\simeq|=O(|\mathcal{C}|^{n}/|Aut(G)|)\). If \(\mathcal{G}\) is the set of of unlabeled graphs with \(n\) nodes, then \(|\mathrm{RNI}(\mathcal{G})/\simeq|=\Theta(\frac{|\mathcal{C}|^{n}}{n!}2^{ \binom{n}{2}})\)._

Proof.: In line with the analysis in the original paper, we suppose that the random noise, selected via \(\omega\), takes values in a finite set \(\mathcal{C}\). Each node of a graph \(G\) can take \(|\mathcal{C}|\) values. Therefore there are \(O(|\mathcal{C}|^{n})\) different labelings of the graph. To obtain the number of unordered graphs, we divide by the number of automorphisms of \(G\). Moreover, the set \(\mathcal{G}\) of unlabeled graphs with \(n\) nodes contains \(\Theta(\frac{1}{n!}2^{\binom{n}{2}})\) unordered graphs. Intuitively, is obtained from the fact that for each of the \(\binom{n}{2}\) pairs of nodes one can either have an edge or not, and by then dividing by the number of node permutations. Formally, this is obtained via Polya's enumeration [27]. A fraction of \(1-o(1)\) of unordered graphs only have the trivial automorphism [27]. Therefore, we have the claim by summing a contribution of \(O(|\mathcal{C}|^{n})\) over all such graphs. 

**Lemma 4**.: _Let \(\mathrm{RP}\) be the RP individualization scheme [42] and \(G\) a graph with partitioning \(\{V_{1},\ldots,V_{C}\}\) of the nodes based on their labels. Then \(|\mathrm{RP}(G)/\simeq|=(\prod_{c=1}^{C}|V_{c}|!)/|Aut(G)|.\) If \(\mathcal{G}\) is the set of of unlabeled graphs with \(n\) nodes, then \(|\mathrm{RP}(\mathcal{G})/\simeq|=\Theta(2^{\binom{n}{2}})\)._

Proof.: Each partition \(V_{i}\) can take \(|V_{i}|!\) different orderings, for a total of \(\prod_{c=1}^{C}|V_{c}|!\) different labelings of the graph. To obtain the number of unordered graphs, we divide by the number of automorphisms of \(G\). Moreover, the set \(\mathcal{G}\) contains \(\Theta(\frac{1}{n!}2^{\binom{n}{2}})\) unordered graphs, and a fraction of \(1-o(1)\) of those only have the trivial automorphism [27]. Therefore, we have the claim by summing a contribution of \(\Theta(n!)\), i.e., the number of possible different labelings of each graph, over all such graphs. 

**Lemma 5**.: _Let \(\mathrm{Tinhofer}\) be the Tinhofer individualization scheme and \(G\) a graph with partitioning \(\{V_{1},\ldots,V_{C}\}\) of the nodes based on their labels. Let the algorithm perform at most (i.e, for any choice of \(\omega\)) \(I\) individualization iterations on graph \(G\), let \(\{V_{1},\ldots,V_{C}\}\) be a partitioning of the nodes of \(G\) based on their labels and let \(R=\max_{c}|V_{c}|\). Then we have that \(|\mathrm{Tinhofer}(G)/\simeq|\leq R^{I}/|Aut(G)|\leq n^{I}/|Aut(G)|\)._

Proof.: After each individualization and color refinement iteration, the color class partition of the nodes of \(G\) is a refinement of the partitioning \(\{V_{1},\ldots,V_{C}\}\) of the nodes of \(G\) based on their labels. At each individualization step, the algorithm can choose to individualize a node in the lexicographically-smallest stable color class, which has at most \(R=\max_{c}|V_{c}|\) nodes. Therefore we have at most \(R^{I}\) labelings of the graph and we divide by the number of automorphisms to obtain the number of distinct unordered graphs. In particular \(|\mathrm{Tinhofer}(G)|\leq R^{I}/|Aut(G)|\leq n^{I}/|Aut(G)|\).

### Lemmas from Section 4.2

**Lemma 6**.: _Let \(\mathrm{Id}\) be the identity relabeling function. There exists a pattern \(P\) and pairs of graphs \(G_{1}\not\simeq G_{2}\) such that \(P\in G_{1},\ P\not\in G_{2}\) and such that they cannot be assigned different outputs by \(\mathrm{EGONN}^{Id}_{\Delta_{P}}\)._

Proof.: The graphs \(P_{1}\) and \(P_{2}\) depicted in Figure 4 are non-isomorphic. Let the pattern of interest be \(P=P_{1}\). Then \(\Delta_{P}=2\). Consider a graph \(G\). Let \(G_{1}\) be the graph obtained by connecting all nodes of \(G\) to the node \(u\) of \(P_{1}\) and \(G_{2}\) by connecting all the nodes of \(G\) to node \(v\) of \(P_{2}\), making sure to select \(G\) such that \(P\not\in G_{2}\). Meanwhile, we have that \(P\in G_{1}\) by construction.. Then for any \(\Delta_{P}\)-ego-net \(E_{1}\) of \(G_{1}\) there is a \(\Delta_{P}\)-ego-net \(E_{2}\) of \(G_{2}\) such that \(E_{1}\simeq_{\mathrm{WL}}E_{2}\). Therefore, \(\mathrm{EGONN}^{Id}_{\Delta_{P}}\) will assign the same outputs to both \(G_{1}\) and \(G_{2}\). 

### Lemmas from Section 4.3

**Lemma 7** (Pollard's bound [47]).: _Let \(X\) be a finite set and \(\mathcal{F}\subseteq\{f:X\to[0,1]\}\). Consider the pseudometric on \(\mathcal{F}\) defined as \(\|f_{1}-f_{2}\|_{1}=\nicefrac{{1}}{{|X|}}\sum_{x\in X}|f_{1}(x)-f_{2}(x)|\). Then_

\[\hat{\mathfrak{R}}_{X}(\mathcal{F})\leq\inf_{\alpha>0}\left(\alpha+\sqrt{2 \log\mathcal{N}(\mathcal{F},\alpha,\|\cdot\|_{1})/|X|}\right).\]

Proof.: Let \(C\subset\{f:X\to[0,1]\}\) be an \(\alpha\)-cover of \((\mathcal{F},\|\cdot\|_{1})\) of size \(\mathcal{N}(\mathcal{F},\alpha,\|\cdot\|_{1})\). Let also \(c_{f}\in C\) be such that \(\|f-c_{f}\|_{1}\leq\alpha\). Then

\[\hat{\mathfrak{R}}_{X}(\mathcal{F}) =\mathbb{E}\left[\sup_{f\in\mathcal{F}}\frac{1}{|X|}\sum_{x_{i} \in X}\sigma_{i}f(x_{i})\right]\leq\] \[\leq\mathbb{E}\left[\sup_{f\in\mathcal{F}}\frac{1}{|X|}\sum_{x_{i }\in X}\sigma_{i}(f(x_{i})-c_{f}(x_{i}))\right]+\mathbb{E}\left[\sup_{f\in \mathcal{F}}\frac{1}{|X|}\sum_{x_{i}\in X}\sigma_{i}c_{f}(x_{i})\right]\leq\] \[\leq\alpha+\mathbb{E}\left[\sup_{c\in\mathcal{C}}\frac{1}{|X|} \sum_{x_{i}\in X}\sigma_{i}c(x_{i})\right]\leq\alpha+\sup_{c\in\mathcal{C}} \sqrt{\frac{2\log|C|}{|X|}},\]

where the last inequality follows from Massart lemma and the fact that the functions in \(C\) take values in \([0,1]\). 

## Appendix D Missing proofs

In this section, we provide the proofs that were omitted from the main paper.

### Proofs from Section 3

**Proposition 1**.: _Let \(f:\mathcal{G}\to\{0,1\}\) be an invariant function and \(k\geq 1\). Then there exists \(\theta\in\Theta\) such that \(\mathrm{GNN}^{\mathrm{bin}}_{k,\theta}(G)=f(G)\) for every \((k-1)\)-weakly individualized graph \(G\in\mathcal{G}\)._

Figure 4: The gadgets \(P_{1}\) and \(P_{2}\) from Lemma 6Proof.: We consider the class of GNNs described in Section A.1, with depth \(K=\max_{G\in\mathcal{G}}|V_{G}|\) and large enough width [41]. Let the initial node embedding \(h_{v}^{0}\) of \(v\in V_{G}\) be a one-hot encoding for its label \(L_{G}(v)\), for any \(G\in\mathcal{G}\). Applying [41, Theorem 2] to the disjoint union of the graphs in \(\mathcal{G}\), we have that there exists a function \(\mathrm{GNN}_{k-1,\theta}\in\mathrm{GNN}_{k-1}\) can simulate \(k-1\) steps of color refinement for all graphs in \(\mathcal{G}\). Therefore, from the definition of \((k-1)\)-weakly individualized graph, after \((k-1)\) message passing layers, all the node embeddings \(h_{v}^{k-1}\in\{-1,1\}^{d}\) of nodes \(v\in V_{G}\) computed by \(\mathrm{GNN}_{k-1,\theta}\) will be distinct, that is, the graph is individualized. Then this individualized graph is WL amenable and 1 color refinement iteration suffices to identify the graph [4]. Then by [37, Proposition 9], any invariant binary function on \(\mathcal{G}\) can be realized. 

### Proofs from Section 4.1

**Theorem 1**.: _Let \(\mathrm{Rel}:\mathcal{G}\times\Omega\to\mathcal{G}^{\prime}\) be a relabeling function. Then \(\mathrm{VC}\left(\mathcal{G}\times\Omega,\mathrm{GNN}_{K}^{\mathrm{bin}} \circ\mathrm{Rel}\right)=|\mathrm{Rel}(\mathcal{G})/\simeq_{\mathrm{WL}}|\). If \(\mathrm{Rel}\) is an individualization scheme, \(|\mathrm{Rel}(\mathcal{G})/\simeq_{\mathrm{WL}}|=|\mathrm{Rel}(\mathcal{G})/ \simeq|\)._

Proof.: We consider the class of GNNs described in Section A.1, with depth \(K=\max_{G\in\mathcal{G}}|V_{G}|\) and large enough width [41]. Let the initial node embedding \(h_{v}^{0}\) of \(v\in V_{G}\) be a one-hot encoding for its label \(L_{G}(v)\), for any \(G\in\mathcal{G}\).

Let \(d:=|\mathrm{Rel}(\mathcal{G})/\simeq_{\mathrm{WL}}|\). We prove the upper bound, following the argument of [37, Proposition 8]. Let by contradiction \(S\) be a (multi)set of \(d+1\) elements that can be shattered. Then by pigeon hole, two elements \((G,\omega),(G^{\prime},\omega^{\prime})\in S\) must be mapped by \(\mathrm{Rel}\) to graphs belonging to the same WL-isomorphism class. Then, by equivalence of \(\mathrm{GNN}_{K}^{\mathrm{bin}}\) to color refinement [41, Theorem 1], these two graphs are always mapped to the same value by \(\mathrm{GNN}_{K}^{\mathrm{bin}}\circ\mathrm{Rel}\). Then, there is no set of \(d+1\) elements that can be shattered.

We then prove the lower bound. Let \(S^{\prime}\) be a set of \(d\) graphs that are representatives of the WL-isomorphism classes contained in \(\mathrm{Rel}(\mathcal{G})/\simeq_{\mathrm{WL}}\). For \(G^{\prime}\in S^{\prime}\), let \(\mathrm{Rel}^{-1}(G^{\prime})=(G,\omega)\in\mathcal{G}\times\Omega\) such that \(\mathrm{Rel}(G,\omega)=G^{\prime}\). Note that, since \(\mathrm{Rel}\) is injective in the first argument, the graph \(G\) is unique. On the other hand, \(\omega\) may not be unique. We take any such \(\omega\) with a deterministic rule (e.g. the minimum). Moreover, since we restrict the domain to \(S^{\prime}\), this inverse always exists. Let then \(S=\{\mathrm{Rel}^{-1}(G^{\prime}):G^{\prime}\in S^{\prime}\}\). We have that \(|S|=|S^{\prime}|=d\) and it can be shattered by [37, Proposition 9].

When the graphs in \(\mathrm{Rel}(\mathcal{G})\) are individualized, WL-isomorphism class correspond to isomorphism classes [4], which proves the second claim. 

**Lemma 1**.: _Let \(\mathcal{G}\) be a set of WL-amenable graphs. Then \(\mathrm{VC}\left(\mathcal{G}\times\Omega,\mathrm{GNN}_{K}^{\mathrm{bin}} \circ\mathrm{Finhofer}\right)=|\mathcal{G}/\simeq|\)._

Proof.: Tinhofer algorithm yields a canonical ordering on WL amenable graphs [4], therefore we have \(|\mathrm{Finhofer}(G)|=1,\ \forall G\in\mathcal{G}\). Then, by Theorem 1 we have the claim. 

### Proofs from Section 4.2

**Theorem 2**.: _Let \(f:\mathcal{G}\to\{0,1\}\) be \(f(G)=\mathbbm{1}\left[P\in G\right]\). Let \(\mathrm{Rel}\) be an individualization scheme. Then there exists \(\theta\in\Theta\) such that \(\mathrm{EGONN}_{\Delta_{P},\theta}^{\mathrm{Rel}}(G)=f(G)\) for every \(G\in\mathcal{G}\)._

Proof.: By Proposition 1 we have that \(GNN_{1}\circ\mathrm{Rel}\) is universally expressive. In particular, it can take value \(1\) if \(P\in\mathrm{EGO}_{v,G,\Delta_{P}}\) and \(0\) otherwise for each possible \(\mathrm{EGO}_{v,G,\Delta_{P}}\). Then \(h_{G}=\max_{v\in V_{G}}h_{v}^{\mathrm{ego}}=1\) if and only if \(P\in G\). 

**Theorem 3**.: _Let \(\mathcal{G}_{\Delta}\) be the set of ego-nets of radius \(\Delta\) of the graphs of \(\mathcal{G}\). Then \(\mathrm{VC}(\mathcal{G}\times\Omega,\mathrm{EGONN}_{\Delta}^{\mathrm{Rel}}) \leq|\mathrm{Rel}(\mathcal{G}_{\Delta})/\simeq|\)._

Proof.: We prove the upper bound. Let \(d=|\mathrm{Rel}(\mathcal{G}_{\Delta})/\simeq|\). Suppose by contradiction that there is a set \(G_{1},\ldots G_{d+1}\) of \(d+1\) graphs that can be shattered. In particular, each graph \(G_{i}\) must have an ego net \(H_{i}\in G_{i}\) that does not appear in any other graph, i.e. \(H_{i}\not\in G_{j},\forall j\neq i\). Indeed, suppose that for each ego-net \(H_{v,i}=\operatorname{EGO}_{v,G_{i},\Delta}\) of graph \(G_{i}\), we have that \(H_{v,i}\in G_{j}\) for some \(j\neq i\). Then to be able to assign \(h_{G_{i}}=1\), it must hold that \(\operatorname{GNN}_{1,\theta}(H_{v,i})=1\) for some \(v\). But since \(H_{v,i}\in G_{j}\), we have that \(h_{G_{j}}=1\) and the assignment of \(G_{j}\) to 1 and all the other graphs to 0 cannot be achieved. Therefore, each graph \(G_{i}\) must have an ego-net \(H_{i}\in G_{i}\) that does not appear in any other graph. Then there must be \(d+1\) distinct ego-nets, which is a contradiction. 

**Remark 1**.: _In fact, there are sets of graphs \(\mathcal{G}\) and radii \(\Delta\) for which the VC bound above is almost tight. This lower bound can be proven by taking a datasets of \(k\) star graphs with maximum degrees \(2,\ldots k+1\) and \(\Delta=1\). Each graph has an ego-net that is unique to the graph, i.e., the graph itself, so the set can be shattered. Moreover, \(|\mathrm{Rel}(\mathcal{G}_{\Delta})/\simeq|=k+1\)._

### Proofs from Section 4.3

**Lemma 2**.: _Let \((X,\mathrm{dist})\) be a pseudometric space and \(S\subseteq X\) a finite subset. Let \(\mathcal{F}\subseteq\{f:S\to[0,1]\}\) be a set of \(C\)-Lipschitz functions. Let \(p,q\in\mathbb{N}\cup\{+\infty\},q\geq p\geq 1\). Then_

\[\log\mathcal{N}(\mathcal{F},\alpha,\|\cdot\|_{p})\leq\log(1/\alpha+1)\cdot \mathcal{N}^{(q)}\left(S,\frac{\alpha}{2C},\mathrm{dist}\right).\]

Proof.: We construct a covering of the space \(\mathcal{F}\). Let \(p\neq\infty\). Let \(S_{c}=\{x_{1},\ldots x_{r}\}\) a \(p\)-norm \(\alpha/2C\)-covering of \((S,\mathrm{dist})\) of size \(r=\mathcal{N}^{(p)}\left(S,\frac{\alpha}{2C},\mathrm{dist}\right)\). For a point \(x\in S\), let its center be \(c(x)=\operatorname{argmin}_{x_{i}\in\mathcal{S}_{c}}\mathrm{dist}(x,x_{i})\). Recall that a \(p\)-norm \(\epsilon\)-covering \(C\) of a finite set \(S\) is such that \(\left(\nicefrac{{1}}{{\lvert S\rvert}}\sum_{x\in S}\mathrm{dist}(x,c(x))^{p} \right)^{\nicefrac{{1}}{{p}}}\leq\epsilon\).

Let \(\tilde{\mathcal{F}}=\left\{\tilde{f}:S_{c}\to\{(k+\nicefrac{{1}}{{2}})\alpha :\,k=0,\ldots,\lfloor\alpha^{-1}\rfloor\}\right\}\). For \(\tilde{f}_{i}\in\tilde{\mathcal{F}}\), we construct an associated function \(\hat{f}_{i}:S\to\{(k+\nicefrac{{1}}{{2}})\alpha:\,k=0,\ldots,\lfloor\alpha^{ -1}\rfloor\}\) such that \(\hat{f}_{i}(x)=\bar{f}_{i}(c(x)),\forall x\in S\). Then let \(\tilde{\mathcal{F}}\) be the set of maps \(\{\hat{f}:\,\tilde{f}\in\tilde{\mathcal{F}}\}\). We have that \(|\tilde{\mathcal{F}}|\leq(1/\alpha+1)^{r}\), as the functions can take up to \((1/\alpha+1)\) values over \(r\) points, and \(|\tilde{\mathcal{F}}|=|\hat{\mathcal{F}}|\). We show that \(\tilde{\mathcal{F}}\) is indeed an \(\alpha\)-covering of \(\mathcal{F}\) in norm \(\|\cdot\|_{p}\).

Let \(f\in\mathcal{F}\). Let \(\hat{f}\in\hat{\mathcal{F}}\) be such that \(\forall x\in S_{c}\), \(|\hat{f}(x)-f(x)|\leq\alpha/2\), which always exists by construction. Then,

\[\|f-\hat{f}\|_{p} =\left(\nicefrac{{1}}{{\lvert S\rvert}}\sum_{x\in S}|f(x)-\hat{f} (x)|^{p}\right)^{\nicefrac{{1}}{{p}}}\leq\] \[\leq\left(\nicefrac{{1}}{{\lvert S\rvert}}\sum_{x\in S}\left( \nicefrac{{1}}{{\lvert S\rvert}}\sum_{x\in S}\left(\nicefrac{{1}}{{\lvert S \rvert}}\sum_{x\in S}\left(\nicefrac{{1}}{{\lvert S\rvert}}\sum_{x\in S} \left(\nicefrac{{1}}{{\lvert S\rvert}}\sum_{x\in S}\left(\nicefrac{{1}}{{\lvert S \rvert}}\sum_{x\in S}\left(\nicefrac{{1}}

Proof.: We have that \(\hat{\mathfrak{R}}_{D}(\mathcal{F}_{\ell})\leq\inf_{\alpha>0}\left(\alpha+\sqrt{2 \log\mathcal{N}(\mathcal{F}_{\ell}|_{D},\alpha,\|\cdot\|_{1})/|D|}\right)\) by Lemma 7. We bound the covering number of the loss functions.

Let \(\mathrm{Rel}(D)=\{\mathrm{Rel}(G,\omega):\;(G,\omega,y)\in D\}\) be the graphs of the dataset after the relabeling, endowed with a pseudometric \(\mathrm{dist}\). We endow \(D\) with a pseudometric \(\mathrm{dist}_{D}\) defined as \(\mathrm{dist}\left((G,\omega,y),(G^{\prime},\omega^{\prime},y^{\prime})\right) =\mathrm{dist}_{D}\left(\mathrm{Rel}(G,\omega),\mathrm{Rel}(G^{\prime},\omega^ {\prime})\right)\) if \(y=y^{\prime}\) and \(+\infty\) otherwise. Moreover, if functions in \(\mathrm{GNN}_{\mathrm{K}}\) are \(C\)-Lipschitz on \(\mathrm{Rel}(D)\) then the functions in \(\mathcal{F}\) are \(C\)-Lipschitz on \(D\) by construction. Then, functions \(\ell(f(x),y)\in\mathcal{F}_{\ell}\) are \(2C/\gamma\)-Lipschitz on \(D\).

Let also \(r\) be the \(1\)-norm covering number of \(\mathrm{Rel}(D)\) and \(S\) the set of \(r\) points that cover the set. Then we can obtain a cover of \((D,\mathrm{dist}_{D})\) of size \(2r\) by taking the points \(\{(x,1),(x,0):\;x\in S\}\).

We can then apply Lemma 2 and obtain that

\[\log\mathcal{N}(\mathcal{F}_{\ell}|_{D},\alpha,\|\cdot\|_{1})\leq\log(1/ \alpha+1)\cdot 2\mathcal{N}^{(q)}\left(\mathrm{Rel}(D),\frac{\alpha\gamma}{4C}, \mathrm{dist}\right).\]

Plugging this result in the first formula, we obtain the claim. 

## Appendix E Experimental Setups and Datasets

In this section, we provide detailed descriptions to the experimental setups and datasets used in Sect. 5.

### Computing infrastructure

The experiments are run on a cluster equipped with Intel(R) Xeon(R) Silver 4116 CPUs and NVIDIA H100 GPUs. The code is based on PyTorch and PyTorch-Geometric.

### Datasets

In the following, we describe and discuss the datasets and their properties. Real-world datasets (i.e., NCI1, IMDB-b, MCF-7, Mutagenicity, COLLAB-b, and Peptides-func) were provided by [31, 38] and [18]. A summary of all datasets and their properties can be found in Tab. 3.

CslWe include three circular skip link (CSL) datasets generated analogously to the setup considered in [42]. A CSL graph is a \(4\)-regular graph \(G_{n,s}\) with \(n>4\) and \(1<s<n/2\) which consists of a cycle of length \(n\) on which all pairs of nodes at distance \(s\) on this cycle are connected by an additional edge. For a given \(n\), we generate a dataset by considering the graphs \(\{G_{n,s}:s\in S\}\) where \(S\) is the set of all skip values leading to non-isomorphic graphs. In particular, we consider graph sizes \(n\in\{17,41,83\}\) for which there exist \(4,10,\) and \(20\) non-isomorphic graphs, respectively. We refer to the datasets as CSL-17, CSL-41, and CSL-83. Since classification tasks in this work are generally restricted to binary classification, we assign the graphs of a CSL dataset evenly into two classes. The classification task is then to assign graphs to their binary skip link class. This setup is commonly used as a benchmark for testing the expressive power of a model since \(4\)-regular graphs cannot be distinguished by the 1-WL test.

3-Reg\(3\)-Reg are datasets consisting of \(3\)-regular graphs which are common types of expressivity benchmark datasets. For each \(n\in\{16,32,64\}\), we generated a dataset containing \(10\) randomly generated \(3\)-regular graphs of size \(n\). We refer to these three datasets as 3-reg-16, 3-reg-32, and 3-reg-64. We evenly distributed the \(10\) graphs into two groups such that the binary classification task is to identify the group that a graph belongs to.

Cycles-pinAs an example dataset containing \(k\)-weakly individualized graphs, we generated \(10\) graphs of size \(33\). Each such graph \(G_{i}\) consists of a pair of cycles of sizes \(16-i\) and \(16+i\) for \(i\in\{0,\ldots,9\}\). We then append a pin, i.e., an additional node, to one of the nodes in the smaller of the two cycles. We evenly distributed the \(10\) graphs into two groups. The binary classification task is then to identify the group that a graph belongs to.

CSL-pinAnalogously to the Cycles-pin dataset, we generate a dataset called CSL-pin by appending a pin, i.e. an additional node, to one of the nodes in every graph of the CSL-41 datasets described above. Adding the pins turns the graphs into \(k\)-weakly individualized graphs.

Collab-b [66]COLLAB consists of 5,000 graphs extracted from three scientific collaboration networks. We merge two of the three classes (class 2 and 3) to make the labels binary. Each graph corresponds to an ego-network which represents the collaboration network of a particular researcher. The graph is then annotated by the field of this researcher such that the classification task consists of assigning an ego-network to the research field.

IMDB-b [66]IMDB-b includes 1,000 ego-network graphs obtained from movie collaboration networks. In such graphs, nodes represent actors and edges represent their co-appearance in a movie. An ego-network graph is then classified by the genre of the movie collaboration network it was extracted from.

Mcf-7From the original MCF-7 [65, 38] dataset, we extracted a maximal set of graphs (up to isomorphism) such that no graph can be distinguished from all others by the Weisfeiler-Lehman (WL) test after four iterations. We divided these \(14\) graphs into two groups such that the binary classification tasks, i.e. assigning a graph to its group, requires at least five color refinement iterations to improve over random guessing and six iterations to perfectly classify all graphs.

Mutagenicity [30, 49]Mutagenicity is a binary classification benchmark dataset consisting of 4,337 molecular graphs. The classification task is to predict the mutagenic property of a molecule.

Nci1 [61]NCI1 is a binary dataset containing 4,110 molecular compounds, each categorized based on their ability to inhibit the growth of a non-small cell lung cancer cell line.

Peptides-funcFrom the original Peptides-func [57, 18] dataset, we extracted a maximal set of graphs (up to isomorphism) such that no graph can be distinguished from all others by the Weisfeiler-Lehman (WL) test after four iterations. We divided these \(15\) graphs into two groups such that the binary classification tasks, i.e. assigning a graph to its group, requires at least five color refinement iterations to improve over random guessing and six iterations to perfectly classify all graphs.

\begin{table}
\begin{tabular}{c c c c c c c} \hline Name & \# Graphs & Classes & Avg. \# nodes & Avg. \# edges & Node labels & WL number \\ \hline COLLAB-b & \(5000\) & \(2\) & \(74.49\) & \(2457.78\) & – & \(1\) \\ IMDB-b & \(1000\) & \(2\) & \(19.77\) & \(96.53\) & – & \(1\) \\ MCF-7* & \(14\) & \(2\) & \(45.86\) & \(50.29\) & \(+\) & \(6\) \\ Mutagenicity & \(4337\) & \(2\) & \(30.32\) & \(30.77\) & \(+\) & \(3\) \\ NCI1 & \(4110\) & \(2\) & \(29.87\) & \(32.30\) & \(+\) & \(3\) \\ Peptides-func* & \(15\) & \(2\) & \(169.47\) & \(172.93\) & \(+\) & \(6\) \\ \hline
3-reg-16 & \(10\) & \(2\) & \(16\) & \(24\) & \(-\) & \(-\) \\
3-reg-32 & \(10\) & \(2\) & \(32\) & \(48\) & \(-\) & \(-\) \\
3-reg-64 & \(10\) & \(2\) & \(64\) & \(96\) & \(-\) & \(-\) \\ CSL-17 & \(4\) & \(2\) & \(17\) & \(34\) & \(-\) & \(-\) \\ CSL-41 & \(10\) & \(2\) & \(41\) & \(82\) & \(-\) & \(-\) \\ CSL-83 & \(20\) & \(2\) & \(83\) & \(166\) & \(-\) & \(-\) \\ CSL-pin & \(10\) & \(2\) & \(42\) & \(83\) & \(-\) & \(5\) \\ Cycles-pin & \(10\) & \(2\) & \(33\) & \(33\) & \(-\) & \(9\) \\ \hline \end{tabular}
\end{table}
Table 3: Overview of real-world and synthetic graph dataset properties. Datasets marked (*) are composed of subsets of the original datasets. The column “WL number” denotes the minimum number of WL iterations necessary to distinguish all graphs. The entry (-) in this column indicates that this number does not exist, i.e., not all graphs can be distinguished by color refinement.

### Experimental setup

In the following, we give detailed descriptions of the experimental setups of our evaluation section in the main paper.

#### e.3.1 Expressivity

For the expressivity evaluation, we considered the datasets Cycles-pin, CSL-pin, MCF-7, and Peptides-func which are described in detail in Sect. E.2. The experiments compare the capability of ordinary GNNs with that of a 1-layer GNN that is run on top of the Tinhofer individualization to learn to distinguish WL-distinguishable graphs. For each dataset, we provide the training accuracies obtained by a GNN\({}_{K}\), where \(K\) is the minimum number of color refinement iterations necessary to distinguish all graphs in the respective dataset, as well as the training accuracies obtained by \(\mathrm{GNN}_{1}\circ\mathrm{Tinhofer}\). We use, as the \(\mathrm{GNN}_{K}\) architecture, the one described in Section A.1, with \(K\) MLP-based message passing layers. In fact, the GNNs based on linear layers for message passing, despite their theoretical equivalence to color refinement, were unable to converge, further strengthening our claims.

The plotted results in Fig. 2 show the accuracy over 1000 epochs where we depict the average over a total of 5 runs in bold and the individual runs faded out. We fixed the embedding dimension to \(256\) and use an Adam optimizer with a learning rate of \(0.0001\). Due to instability in the learning process, we provide smoothened learning curves for better clarity of the results.

#### e.3.2 VC dimension

To investigate the generalization bounds based on the VC-dimension results, we compare the individualization methods RNI [52], RP [42], and Tinhofer. The experiments are conducted on the CSL and 3-Reg datasets of varying numbers of nodes. To investigate the generalization performance, we generate as the training set \(1,10,100\), and \(1000\) permutations for each graph in the respective dataset and test on \(1000\) permuted copies. We report the performance in terms of the difference between the train and test accuracy. Particularly, for every dataset, we consider the generalization gap achieved at the last epoch with a perfect training accuracy score over a total of 1000 epochs. We conduct a total of 5 runs and plot the mean as well as the standard deviations over these runs.

In all cases, we used the \(\mathrm{GNN}_{K}\) architecture described in Section A.1, with \(K=4\) linear-layer-based message passing layers. We fix the embedding dimension to \(256\) and use an Adam optimizer with a learning rate of \(0.0001\).

#### e.3.3 Substructure identification

For the substructure identification task, we generate 1000 3-regular graphs of order 30 for each of the task using NetworkX. In particular, 500 graphs contain the pattern \(P\) and 500 do not. We split the dataset in train and test by assigning 90% of the graphs to the train set at random.

We use, as the \(\mathrm{GNN}_{K}\) architecture, the one described in Section A.1, with \(K\) linear-layer-based message passing layers. In particular, \(K\) is set to \(2\) for \(C_{3},C_{4},K_{2,3}\) and to \(3\) for \(C_{5}\), as these are the minimum number of layers for the message passing to fully cover these patterns.

We use the same architecture GNN for the \(\mathrm{EGONN}_{\Delta_{P}}\) model on each of the \(\Delta_{P}\)-ego-nets, with 1 layer for the individualized versions and \(\Delta_{P}\) layers for the non-individualized version. In particular, \(\Delta_{P}\) is \(1\) for \(C_{3}\) and \(2\) for \(C_{4},C_{5},K_{2,3}\). Moreover, we replace the \(\max\) aggregation with a softmax, in order to allow the gradients to be propagated more easily, as we observe that this allows for an easier convergence. We fix dimension of the GNN embeddings to 256. We use the Adam optimizer [32] with learning rate \(0.001\) and batch size 256.

The model is trained for 1000 epochs. The epoch at which the accuracies are reported is selected as the one with highest training accuracy, with ties broken by the lowest training loss. Note that since the focus of this paper is on the (worst-case) generalization gap, the best epoch is not chosen using a validation dataset, as it should be done in practice.

We report the mean and 1-sigma error bars over 5 runs, run on the same splits but with different seeds.

#### e.3.4 Covering numbers

For the section on covering numbers, we used four real world datasets for binary classification, as described in Section E.2.

As a metric on the relabeled graphs, we use the Wasserstein distance between the continuous WL embeddings given by [60], which we call WWL distance. In particular, we choose \(K_{WL}=5\) WL iterations to obtain the embeddings, ensuring that \(K_{WL}\) is at leas as large as the message passing iterations used in the GNN models. This gives the WWL distance the ability to distinguish all the graphs that the GNNs can distinguish, which is needed to ensure Lipschitzness. In fact, this property cannot be guaranteed, as there are graphs that are not WL-isomorphic but that have WWL distance 0. To mitigate this issue, we assign one-hot encodings with degree centralities to the unlabeled graphs. We then compute \(\hat{\mathcal{N}}=\mathcal{N}^{(1)}(\mathrm{Rel}(D),\epsilon,\mathrm{dist})/ |D|\) as a proxy for the sample complexity using the WWL distance and \(\epsilon=0.05\). In fact, the covering number we report is an approximation (which is also an upper bound) computed by running a k-median clustering algorithm and selecting the minimum \(k\) for which the objective function is less than \(\epsilon\).

We use, as the \(\mathrm{GNN}_{K}\) architecture, the one described in Section A.1, with \(K=2,5\) linear-layer-based message passing layers. We fix dimension of the GNN embeddings to 128. We use the Adam optimizer [32] with learning rate \(0.005\) and batch size 256.

The model is trained for 1000 epochs. The epoch at which the accuracies are reported is selected as the one with highest training accuracy, with ties broken by the lowest training loss. Note that since the focus of this paper is on the (worst-case) generalization gap, the best epoch is not chosen using a validation dataset, as it should be done in practice. We split the dataset in train and test by assigning 90% of the graphs to the test set at random for each run.

We report the mean and 1-sigma error bars for the accuracies over 5 runs with different seeds. We report the covering number just for the first seed due to the high computational costs involved with the calculation.

## Appendix F Additional results on covering numbers

In this section, we report additional results on the covering numbers on real world datasets. In particular, Figures 5, 6, 7 and 8 report the covering number curves for the NCI1, Mutagenicity, IMDB-b and COLLAB-b datasets, respectively. The results are obtained from the train-test splits of the first run, as described in Section E.3.

We plot both \(\hat{\mathcal{N}}=\mathcal{N}^{(1)}(\mathrm{Rel}(D),\epsilon,\mathrm{dist})/ |D|\), which depends on the \(1\)-norm covering number, and \(\mathcal{N}(\mathrm{Rel}(D),\epsilon,\mathrm{dist})/|D|\), which depends on the (\(\infty\)-norm) covering number of the relabeled dataset. We observe that the \(1\)-norm covering numbers are significantly lower than the \(\infty\)-norm covering numbers across all scales and datasets, providing experimental evidence of the importance of the results of Lemma 2.

Figure 5: Covering numbers for the NCI datasetFigure 8: Covering numbers for the COLLAB-b dataset

Figure 6: Covering numbers for the Mutagenicity dataset

Figure 7: Covering numbers for the IMDB-b dataset

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims in the abstract and in the introduction are addressed as follows. Section 3 addresses the expressivity of shallow GNNs with individualization schemes. Section 4.1 and Section 4.2 address the VC dimension bounds. Section 4.3 addresses the covering number bounds. Section 5 reports the experimental evaluation. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of the paper are discussed in the conclusions and more in detail in Section B Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide all the proofs for our lemmas and theorems in Sections C and D. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The experimental setup is described in Section E. Moreover, our code and data are publicly available at https://github.com/BorgwardtLab/NodeIndividualizedGNNs to ensure reproducibility. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our code and data are publicly available at https://github.com/BorgwardtLab/NodeIndividualizedGNNs. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all the details on the experimental setup in Section E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report all the performance figures as means and 1-sigma error bars, taken over 5 runs, as described in Section E. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The computing infrastructure is described in Section E.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The paper conforms to the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The work is mostly theoretical and has no direct societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We describe and cite the original papers from which we take the datasets we used in Section E.2. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The paper introduces only simple synthetic datasets, which are released with the code. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.