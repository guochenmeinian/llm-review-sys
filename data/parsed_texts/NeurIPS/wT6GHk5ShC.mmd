Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective

 Xinhao Yao\({}^{1}\), Xiaolin Hu\({}^{1}\), Shenzhi Yang\({}^{1}\), Yong Liu\({}^{1}\)

\({}^{1}\)Renmin University of China, Beijing, China

{yaoxinhao021978, xiaolinhu, yangshenzhi2003, liuyonggsai}@ruc.edu.cn

Corresponding author.

###### Abstract

Pre-trained large language models (LLMs) based on Transformer have demonstrated striking in-context learning (ICL) abilities. With a few demonstration input-label pairs, they can predict the label for an unseen input without any parameter updates. In this paper, we show an exciting phenomenon that SVD-based weight pruning can enhance ICL performance, and more surprising, pruning weights in deep layers often results in more stable performance improvements than in shallow layers. However, the underlying mechanism of those findings still remains an open question. To reveal those findings, we conduct an in-depth theoretical analysis by presenting the implicit gradient descent (GD) trajectories of ICL and giving the mutual information based generalization bounds of ICL via full implicit GD trajectories. This helps us reasonably explain the surprising experimental findings. Besides, based on all our experimental and theoretical insights, we intuitively propose a simple, model-compression and derivative-free algorithm for downstream tasks in enhancing ICL inference. Experiments on benchmark datasets and open source LLMs display the method effectiveness2.

Footnote 2: The code is available at https://github.com/chen123CtrlS/EnhancingICL_SVDPruning.

## 1 Introduction

Recently, large language models (LLMs) based on the Transformer architecture [43] have emerged striking in-context learning (ICL) capabilities: Given a prompt containing demonstration sample and a test data, the model can make a prediction for the test data and achieve excellent performance without any parameter updates [7, 26, 34, 6]. This leads considerable works that aim to shed light on it [54, 21, 59, 3, 10, 16, 2, 46, 35].

In this paper, we show our surprising findings in ICL inference by experimentally analyzing the effect of singular value decomposition (SVD)-based pruning on the model performance at different depth layers. As demonstrated in Figure 1: (i) SVD-based weight pruning can enhance ICL performance. Across all cases, it is evident that SVD-based weight pruning can generally enhance model performance at various depth layers, compared to the baseline performance without weight pruning (indicated by the dashed lines); (ii) Pruning weights in deep layers often results in more stable performance improvements than in shallow layers. Specially, deep layers weight matrices can be drastically reduced without much degradation in model performance, and may even get large improvements on it, while the model performance collapses after a sharp reduction at the shallow layers (see Section 2 for details). A similar case in Sharma et al. [38] notes that a large portion of singular values can be removed from linear layers in large language models without affecting or even improving reasoning performance. However, the underlying mechanism of this phenomenon still remains a mystery, and this paper seeks to explore the issue from the following two aspects.

**Why this phenomenon?** We conduct an in-depth theoretical analysis to explain the findings. To be more specific, we first analyse the ICL form with Transformer from the perspective of ICL as implicit gradient descent fine-tuning, and we present the implicit gradient descent trajectories of ICL in **Theorem 1** in Section 3.2. Afterwards, we use the information-theoretic approaches [55, 32, 51] to give the generalization bounds of ICL via full implicit GD trajectories in **Theorem 2** in Section 3.3, explaining (**Q1**) why SVD-based weight pruning can enhance ICL performance? and (**Q2**) why do deep and shallow layers exhibit different behaviors when their weight matrices are drastically reduced?

**How to better use this phenomenon (Q3)?** Based on all our experimental and theoretical insights, we intuitively propose a simple, derivative-free and model-compression algorithm in Section 3.4 for downstream tasks in enhancing ICL inference, providing developers and researchers with an effective approach for handling downstream tasks. Experimental results for our algorithm on benchmark datasets [39, 58, 8, 12, 5, 11, 18] and open source LLMs [48, 42] verify that the method can visibly influence performance across different language understanding tasks in ICL inference.

Our primary objective is to establish a general theoretical framework that uncovers the underlying mechanism behind the phenomenon that SVD-based weight pruning can enhance ICL performance. Based on our theoretical insights, one can design new ICL algorithms. Accordingly, we did not directly compare our approach with other pruning methods. The algorithm in Section 3.4 is presented solely to illustrate how theoretical analysis can guide experimental procedures effectively.

Figure 1: The effect of weight pruning across different layer types. The figure shows the phenomenon observed on the benchmark datasets (SST-2, RTE, COPA) and open source LLMs (GPT-J-6B and LLAMA2-7B). Each sub-figure corresponds only to the indicated type of dataset, model and module. Notice that this figure mainly focuses on exhibiting the impact of weight pruning to the first two and the last two layers of the model and different colors are used to distinguish between these layers. The dashed line represents the pretrained model performance without SVD. We operate on the whole of MLP or ATTN and specifically marked the points of highest performance. The amount of weight pruning is severe, for instance, the highest model performance sometimes occurs at a clipping rate of 0.995. This is about 99.5% of the matrix’s original rank. For the definitions of “deep” and “shallow”, please refer to Appendix B.2.

### Related Works

**Model compression.** In recent years, there has been growing theoretical and experimental evidence that models can be significantly pruned with very little drop in accuracy, thereby significantly reducing their storage requirements. To name a few, Frankle and Carbin [15] indicate that neural networks can typically have over 90% of their weights eliminated with little to no loss in performance. Analogous results have been demonstrated in both feed-forward and convolutional networks used for image classification [24; 49; 22; 37]. More specifically, Sharma et al. [38] present that careful pruning done at specific layers of Transformer models can produce significant boosts in performance on some tasks. The discovery of this phenomenon heightens interest in the connection between generalization and over-parameterization [56; 57], it also spurs research into developing pruning strategies that facilitate efficient model inference [30]. Additionally, the works [19; 40] find that performance remains nearly unchanged until a significant portion (up to half) of the layers are removed.

**In-context learning and gradient descent.** In order to better understand the ICL capabilities, considerable works try to understand ICL capabilities from the perspective of gradient descent. Irie et al. [21] and Dai et al. [10] explain ICL as implicit fine-tuning by figuring out a dual form of gradient descent for linear Transformer attention. However, the linear attention setting is less commonly used than Softmax attention in the LLMs and the details of gradient descent such as the choice of loss function and training data have not been clearly defined. Therefore, by using weight construction, von Oswald et al. [46] show the equivalence of linear self-attention mechanism and gradient descent on a linear regression task and Akyurek et al. [3] prove that based on gradient descent and closed-form ridge regression, Transformers can learn linear models as learning algorithms. Further without using weight construction, Ren and Liu [35] connect Softmax attention with kernels and then give a novel interpretation that ICL with Transformer is really equivalent to a contrastive learning pattern.

**Information-theoretic generalization bounds.** The information-theoretic generalization bounds have been developed to analyze the expected generalization error of a learning algorithm. Given that they are dependent of distribution and algorithm, they are ideal tools to study the generalization behaviour of models performed with a specific algorithm. Russo and Zou [36] and Xu and Raginsky [55] first propose the Mutual information (MI) based bounds, which are then strengthened by additional techniques [4; 31; 20; 51]. Specifically, Negrea et al. [31] derive MI-based bounds by developing a PAC-Bayes-like bounding technique and Wang and Mao [51] develop generalization bounds for SGD via constructing an auxiliary iterative noisy process.

## 2 SVD-Based Weight Pruning can Enhance ICL

This section shows our surprising findings in ICL inference by performing a motivating analysis of three benchmark datasets in conjunction with two open source LLMs, the details are as follows. We choose GPT-J (6B,28 layers) [48] and LLAMA2 (7B,32 layers) [42] as our primary models for investigation, owing to their robust ICL performance and moderate model size, which align with our hardware resource. The attention (ATTN) layers are made up of key, query, value, out matrices both in GPT-J-6B and LLAMA2-7B. The mlp (MLP) layers in GPT-J-6B consist of input, output matrices, while the MLP layers in LLAMA2-7B are made up of up, gate, down matrices. For datasets, we use SST-2 [39] for sentiment analysis, RTE [5] for textual entailment and COPA [18] for causal inference. Details regarding datasets and how they were used are shown in Appendix C.1 and C.3.

Specially, we use the optimal rank-\(r\) approximation mentioned later as SVD-based weight pruning method to show the effect of weight pruning across different layer and module types in Section 2.1, then we further analyze the effect of the ICL shot numbers on it in Section 2.2.

**The optimal rank-\(r\) approximation and SVD.** Given a matrix \(\mathbf{W}\in\mathbb{R}^{m\times n}\) and a constant \(r\leq\min(m,n),r\in\mathbb{N}\). Eckart-Young-Mirsky theorem [14] provides an optimal solution \(\mathbf{W}^{*}(r)=\mathbf{U}_{:r}\mathbf{\Sigma}_{:r}\mathbf{V}_{:r}^{T}, \mathbf{U}_{:r},\mathbf{V}_{:r}\) are matrices containing the singular vectors corresponding to the largest \(r\) singular values \([\sigma]_{1}^{r}\). Let \(\xi=1-\frac{r}{\min(m,n)}\in(0,1)\) be the clipping rate.

### Effect of Weight Pruning across Different Layer and Module Types

We plot the results of applying various amounts of clipping rate \(\xi\) to each module matrices in the Transformer architecture on the corresponding evaluation index for each dataset, as depicted in Figure1. These plots are grouped, such that each sub-figure corresponds only to the indicated type of dataset, model and module. Notice that this investigation mainly focuses on assessing the impact of weight pruning to the first two and the last two layers of the model to further clarify the impact of layer depth on the model's performance and different colors are used to distinguish between these layers.

All sub-figures clearly show an interesting phenomenon about these models in ICL inference: SVD-based weight pruning can enhance ICL performance in both shallow and deep layers across different module types. More surprising, deep layers weight matrices can be drastically reduced without much degradation in model performance, and may even get large improvements on it. This suggests that pruning weights in deep layers can effectively reduce model complexity while maintaining or enhancing model performance. And the model performance collapses to 0.5 (the expectation of a random guess in the binary task) after a sharp reduction at the shallow layers by contrast, indicating a higher sensitivity of the model to changes in shallow layers.

Based on the surprising findings in ICL inference mentioned above, three questions are certain to arise: (**Q1**) Why SVD-based weight pruning can enhance ICL performance? (**Q2**) Why do deep and shallow layers exhibit different behaviors when their weight matrices are drastically reduced? (**Q3**) How can we better use the phenomena about ICL in downstream tasks? We will address the first two questions theoretically in Section 3.3 and give a heuristic algorithm in Section 3.4 to answer the last.

### Effect of Different ICL Shot Numbers

Given that ICL can achieve higher performance with more demonstrations [23; 1], we further analyze the non-ignorable effect of different ICL shot numbers. To control for other influencing factors, we focus on SST-2 dataset [39] and retain the same test set for a single random seed as Section 2.1.

Figure 2: The effect of different ICL shot numbers is not uniform. Here we show the effect of different ICL shot numbers on the phenomenon mentioned in Section 2.1 as studied on the SST-2 dataset. Each row represents the results of the same shot numbers in different layers and modules, and each column represents the results of the different shot numbers in different layers of the same module. We also specifically marked the points of highest performance.

As shown in Figure 2, we compare the settings of three different ICL shot numbers: 0, 4 and 10. Following this, we analyze how the phenomenon changes across different shot numbers.

Firstly, we note that without weight pruning, the performance of the model improves with an increase in the number of ICL shots. Which is consistent with prior works. Besides, for every shot number, a phenomenon consistent with what is described in Section 2.1 is observed: SVD-based weight pruning can enhance ICL performance, pruning deep layer weight is more stable than pruning shallow weight. Last but not the least, Figure 2 also demonstrates roughly that with a decrease in the number of shots, the rate of performance collapse in the model slows down after a sharp reduction at the shallow layer. Intuitively, this is because LLMs exhibit a shift in focus in the ICL setting, which results in a reduced scope of the output space. The more shots there are, the more pronounced the shift becomes, and this also leads to a faster collapse. We also theoretically discuss it in Section 3.2 and 3.3 (**Remark 6**).

## 3 Theoretical Analysis Results

In this section, we first describe the core components of our study by reviewing some basic notations in Section 3.1, and present the implicit gradient descent trajectories of ICL in Section 3.2. Afterwards, we give a mutual information based generalization bounds of ICL via full implicit GD trajectories in Section 3.3. Based on all our experimental and theoretical insights, we intuitively propose a derivative-free and effective method for downstream tasks in enhancing ICL inference in Section 3.4. Complete proofs can be found in the Appendix. For ease of qualitative analysis, our theoretical analysis is mainly focuses on linear attention setting. We discuss the standard Softmax attention setting in Appendix A.2 and feed-forward (MLP) layers in Appendix A.3.

### Preliminaries

We let \(\mathcal{H}\) be the instance space and \(\mu\) be an unknown distribution on \(\mathcal{H}\), specifying random variable \(\mathbf{h}\). In ICL setting, the model accepts a sequence of input \(\mathbf{H}=[\mathbf{H}_{s},\mathbf{h}_{N+1}]\) drawn i.i.d. from \(\mu\), where \(\mathbf{H}_{s}=[\mathbf{h}_{1},\mathbf{h}_{2},...,\mathbf{h}_{N}]\) represents the demonstration sample and \(\mathbf{h}_{N+1}\) is the test data. In the information-theoretic analysis framework, we let \(\mathcal{W}\in\mathbb{R}^{d}\) be the space of hypotheses related to the model, and Transformer performs an implicit stochastic learning algorithm \(\mathcal{A}\) which takes the demonstration sample \(\mathbf{H}_{s}\) as its input and outputs a hypothesis \(W\in\mathcal{W}\) according to some conditional distribution \(Q_{W|\mathbf{H}_{s}}\). Similar to previous works [55; 51], we give the definition of expected generalization error.

**Expected generalization error.** Given a loss function \(\ell:\mathcal{W}\times\mathcal{H}\rightarrow\mathbb{R}^{+}\), where \(\ell(w,\mathbf{h})\) measures the "unfitness" or "error" of any \(\mathbf{h}\in\mathcal{H}\) with respect to a hypothesis \(w\in\mathcal{W}\). We take \(\ell\) as a continuous function and assume that \(\ell\) is differentiable almost everywhere with respect to \(w\). The goal of learning is to find a hypothesis \(w\) that minimizes the population risk, and for any \(w\in\mathcal{W}\), the population risk is defined as \(L_{\mu}(w)\triangleq\mathbb{E}_{\mathbf{h}\sim\mu}[\ell(w,\mathbf{h})]\). However, since \(\mu\) via the sample \(\mathbf{H}_{s}\) can only be partially observed, we instead turn to use the empirical risk, defined as \(L_{\mathbf{H}_{s}}(w)\triangleq\frac{1}{N}\sum_{i=1}^{N}\ell(w,\mathbf{h}_{i})\). Then the expected generalization error of \(\mathcal{A}\) is defined as

\[\widetilde{\text{error}}\triangleq\mathbb{E}_{W,\mathbf{H}_{s}}[L_{\mu}(W)-L_{ \mathbf{H}_{s}}(W)],\]

where the expectation is taken over \((\mathbf{H}_{s},W)\sim\mu^{N}\otimes Q_{W|\mathbf{H}_{s}}\).

**In-context learning with Transformers3.** By prompt design, most context semantic understanding tasks can be unified into classification tasks. Simplify the form of each token in \(\mathbf{H}\) to \(\mathbf{h}_{i}=[\mathbf{x}_{i},\mathbf{y}_{i}]\), where \([\mathbf{x}_{i}]_{1}^{N}\in\mathbb{R}^{din}\) and \([\mathbf{y}_{i}]_{1}^{N}\in\mathbb{R}^{dout}\) are encoded input text and corresponding labels respectively. The test token has the form \(\mathbf{h}_{N+1}=[\mathbf{x}_{N+1},mask]\), where \(mask\) is the label needed to predict, and usually set \(0\) as its initialization. Therefore, the form of attention with residual connection4 is as follows:

Footnote 3: Please refer to Appendix B.1 for an explanation on how Eq.(1) and Eq.(2) can utilize the same mask.

Footnote 4: In real-world scenarios, the residual connection module in Transformers is indispensable.

\[\hat{\mathbf{H}}=\mathbf{H}+\mathbf{W}_{V}\mathbf{H}\mathbf{M}\text{ Softmax}\left(\frac{((\mathbf{W}_{K}\mathbf{H})^{T}\mathbf{W}_{Q}\mathbf{H})}{ \sqrt{d_{scale}}}\right),\] (1)where \(\mathbf{W}_{V}\),\(\mathbf{W}_{K}\),\(\mathbf{W}_{Q}\in\mathbb{R}^{(dout+din)\times(dout+din)}\) are projection matrix, and the mask matrix \(\mathbf{M}=\left(\begin{array}{cc}\mathbf{I}_{N\times N}&0\\ 0&0\end{array}\right)\) is included in the attention. Specifically, \(\hat{\mathbf{h}}_{N+1}\) can be formulated as

\[\hat{\mathbf{h}}_{N+1}=\mathbf{h}_{N+1}+\mathbf{W}_{V}\mathbf{H}\text{SM} \text{ofmax}\left(\frac{((\mathbf{W}_{K}\mathbf{H})^{T}\mathbf{W}_{Q}\mathbf{ h}_{N+1})}{\sqrt{d_{scale}}}\right),\] (2)

and for ease of qualitative analysis, Eq.(2) can be approximated as a relaxed linear attention mechanism by removing the Softmax operation and scale factor:

\[\hat{\mathbf{h}}_{N+1}=\mathbf{h}_{N+1}+\mathbf{W}_{V}\mathbf{H}\mathbf{M}( \mathbf{W}_{K}\mathbf{H})^{T}\mathbf{W}_{Q}\mathbf{h}_{N+1}.\] (3)

### The Implicit Gradient Descent Trajectories of ICL

To begin with, we present the implicit gradient descent of ICL, inspired by [21, 10, 35]. These works describe how ICL with attention can be connected to a meta-optimizer which produces implicit gradient. The following lemma demonstrates the result.

**Lemma 1** (The Implicit Gradient Descent of ICL in a Single Linear Attention Layer).: _Consider a Transformer consists of a single linear layer attention with residual connection, parameterized by \(\mathbf{W}_{V}\),\(\mathbf{W}_{K}\),\(\mathbf{W}_{Q}\) as in Eq.(3). Same to Section 3.1, let \(\mathbf{H}=[\mathbf{H}_{s},\mathbf{h}_{N+1}]\) be the input, where \(\mathbf{H}_{s}=[\mathbf{h}_{1},\mathbf{h}_{2},...,\mathbf{h}_{N}]\) represents the demonstration sample and \(\mathbf{h}_{N+1}\) is the test data. And let \(\hat{\mathbf{h}}_{N+1}\) be the single layer output. Then, it holds that_

\[\Delta\mathbf{W}_{icl}=\mathbf{W}_{V}\mathbf{H}_{s}(\mathbf{W}_{K}\mathbf{H}_ {s})^{T}\mathbf{W}_{Q}=\left(\sum_{i=1}^{N}\mathbf{W}_{V}\mathbf{h}_{i}\otimes \mathbf{W}_{K}\mathbf{h}_{i}\right)\mathbf{W}_{Q},\]

\[\hat{\mathbf{h}}_{N+1}=\mathbf{h}_{N+1}+\Delta\mathbf{W}_{icl}\mathbf{h}_{N+1},\]

_where \(\mathbf{W}_{V}\mathbf{H}_{s}\) is regarded as the meta-gradient of ICL, which is used to generate the implicit gradient matrix \(\Delta\mathbf{W}_{icl}\) to act on the final feature output. See Appendix A.1 for a proof._

**Remark 1**.: _It is worth noting that \(\text{rank}(\Delta\mathbf{W}_{icl})\leq\text{rank}(\mathbf{H}_{s})\leq N\) by rank relations in matrix multiplication, indicating that this is a low-rank operation. This explains why the effect of different ICL shot numbers is not uniform in Section 2.2. Details of the discussion in standard Softmax attention setting can be found in Appendix A.2. Plus, a significant body of work has discovered that the ICL capabilities of models are not robust to the order of ICL sample. For instance, Lu et al. [28] observed that large language models (LLMs) are sensitive to the sequence of ICL examples, and Liu et al. [27] reported that the context-based ICL performance exhibits a U-shaped curve--models tend to perform better with information that appears at the beginning or at the end of the input context. This aligns with observations from actual model training where the sequence of training samples affects outcomes especially when training with a batch size of \(1\)._

According to the above, we further analyze the implicit gradient descent trajectories of ICL and provide the following Theorem. We will denote \(\Delta\mathbf{W}_{icl}^{t}\) by \(\Delta\mathbf{W}_{t}\) when there is no ambiguity, where \(t\) represents \(t\)-th layer.

**Theorem 1** (The Implicit Gradient Descent Trajectories of ICL).: _Consider a Transformer as a stack of \(L\) linear attention blocks with residual connection, parameterized by \([\mathbf{W}_{V}^{t}]_{1}^{L},[\mathbf{W}_{K}^{t}]_{1}^{L},[\mathbf{W}_{Q}^{t} ]_{1}^{L}\). Denote \([\mathbf{h}_{i}^{t}]_{1}^{N+1}\) as the output of the \(t\)-th layer, \([\mathbf{h}_{i}^{0}]_{1}^{N+1}\) as the initial input. Then for \(t\in[L]\), it holds that_

\[\mathbf{G}_{t}=\Delta\mathbf{W}_{t}(1+\mathbf{W}_{t-1})=\Delta \mathbf{W}_{t}(1+\mathbf{W}_{0}+\sum_{j=1}^{t-1}\mathbf{G}_{j}),\] \[\mathbf{h}_{N+1}^{t}=\mathbf{h}_{N+1}^{0}+\sum_{j=1}^{t}\mathbf{G} _{t}\mathbf{h}_{N+1}^{0}=\mathbf{h}_{N+1}^{0}+\mathbf{W}_{t}\mathbf{h}_{N+1}^ {0},\]

_where \(\Delta\mathbf{W}_{t}\triangleq\left(\sum_{i=1}^{N}\mathbf{W}_{V}^{t}\mathbf{ h}_{i}^{t-1}\otimes\mathbf{W}_{K}^{t}\mathbf{h}_{i}^{t-1}\right)\mathbf{W}_{Q}^{t}\), \(\mathbf{W}_{0}=0,\mathbf{W}_{t}=\mathbf{W}_{t-1}+\mathbf{G}_{t}\) and \(\mathbf{G}_{1}=\Delta\mathbf{W}_{1}\). See Appendix A.4 for a proof._

**Remark 2**.: _Note that the exclusion of Transformer weight \(([\mathbf{W}_{K}^{t}]_{1}^{L},[\mathbf{W}_{Q}^{t}]_{1}^{L},[\mathbf{W}_{V}^{t }]_{1}^{L})\) implies that \(\mathbf{G}_{t}\) is only dependent on \(\mathbf{W}_{t-1}\) and \(\mathbf{H}_{s}\), this is consistent with gradient descent in terms of relevance._

### Generalization Bounds of ICL via Full Implicit GD Trajectories

In this part, for simplicity of representation, we flatten the weight matrix into a vector form \((\mathbf{vec}(\mathbf{W}_{t})=W_{t}\in\mathbb{R}^{d},\mathbf{vec}(\mathbf{G}_{t })=G_{t}\in\mathbb{R}^{d})\) and conduct our analysis within the weight and implicit gradient space of hypotheses \(\mathcal{W}\) as detailed in Section 3.1. Notably, Ahn et al. [2] observe that, with the optimal parameters, a single layer linear Transformer implements a single step of preconditioned gradient descent and the preconditioning matrix not only adapts to the distribution of input data but also to the variance caused by data inadequacy. Garg et al. [16] show empirically that the trained Transformer is able to in-context learn the class of linear functions with respect to the prompt distribution, performing comparably to the optimal least squares estimator for any number of in-context examples considered, as exhibited in Figure 4(a).

In addition, we also make a discussion on the noise of the implicit gradient in Section C.4, our analytical discussion indicates that the implicit gradients produced by Transformers in practical applications are noisy due to factors such as the extent of model pre-training and data characteristics (e.g., ICL shot number). We first present the assumption used in this subsection.

Let \(G_{t}\triangleq\frac{1}{N}\sum_{i=1}^{N}\nabla\ell_{i}\) be the best implicit gradient of ICL that the model can produce, \(\tilde{G}_{t}\triangleq\frac{1}{b}\sum_{i=1}^{b}\nabla\ell_{i}\) be the implicit gradient of ICL generated by the model in practical applications. \(N\) is the threshold and \(b\) is the the shot number in the actual input defined in Section C.4. And \(V_{t}\triangleq G_{t}-\tilde{G}_{t}\) is the gradient noise caused by shot number, \(C_{t}\triangleq\frac{N-b}{b(N-1)}(\frac{1}{N}\sum_{i=1}^{N}\nabla\ell_{i} \nabla\ell_{i}^{T}-G_{t}G_{t}^{T})\) is the implicit gradient noise covariance. Similar to Wang and Mao [51]'s assumption in SGD, we approximate \(V_{t}\) up to its second moment.

**Assumption 1**.: _Assume the implicit gradient noise \(V_{t}\) follows a Gaussian distribution, i.e., \(V_{t}\sim\mathcal{N}(0,C_{t})\), then in ICL implicit gradient descent trajectories,_

\[W_{t}=W_{t-1}-\eta\tilde{G}_{t}=W_{t-1}-\eta G_{t}+\eta C_{t}^{1/2}N_{t},\] (4)

_where \(N_{t}\sim\mathcal{N}(0,I_{d})\) is the standard Gaussian5._

Footnote 5: Consider the continuous SDE: \(dW=-\nabla L_{\mathbf{H}_{s}}(W)dt+[\eta C(W)]^{1/2}d\theta_{t}\), where \(C(W)\) is the gradient noise covariance at \(W\) and \(\theta_{t}\) is a Wiener process. We can view Eq. (4) as discretization of the SDE.

**Remark 3**.: _Regarding the validation of this assumption, empirical evidence from works [53, 25], suggests that SGD and Eq.(4) can achieve the similar testing performance. Additionally, we refer readers to some recent works [21, 16, 3, 10, 2, 46, 35], where the authors empirically verify that in-context learning of Transformer can achieve the similar testing performance to SGD. Together suggesting that studying Eq.(4) is arguably sufficient to understand generalization properties of ICL._

**Theorem 1** indicates that the initial parameter \(W_{0}=0\), which is independent of all other random variables. And an \(L\)-layer Transformer does implicit GD of ICL stops after \(L\) updates, outputting \(W_{L}\) as the implicit learned parameter. Our main results are mutual information based expected generalization error bounds of ICL, as presented in **Theorem 2**.

**Theorem 2** (The Generalization Bounds of ICL via Full Implicit Gradient Descent Trajectories).: _Under the conditions of **Theorem 1** and **Assumption 1**, assume the implicit gradient noise covariance \(C_{t}\) is a positive-define matrix, the loss \(\ell(w,\mathbf{h})\) is R-subGaussian for any \(w\in\mathcal{W}\in\mathbb{R}^{d}\), then_

\[\widetilde{\text{error}}\leq\sqrt{\frac{R^{2}}{N}\sum_{t=1}^{L}\mathbb{E}_{ \mathbf{W}_{t-1}}^{\mathbf{H}_{s}}\left[d\log\left(\frac{\left\|\Delta\mathbf{ W}_{t}\right\|_{F}^{2}\cdot\left\|1+\sum_{j=1}^{t-1}\mathbf{G}_{j} \right\|_{F}^{2}+\operatorname{tr}\{C_{t}\}}{d}\right)-\operatorname{tr}\{ \log C_{t}\}\right]},\]

_where \(\mathbf{vec}(\mathbf{G}_{t})\in\mathbb{R}^{d}\), \(\Delta\mathbf{W}_{t}\triangleq\left(\sum_{i=1}^{N}\mathbf{W}_{V}^{t}\mathbf{ h}_{i}^{t-1}\otimes\mathbf{W}_{K}^{t}\mathbf{h}_{i}^{t-1}\right)\mathbf{W}_{Q}^{t}\) and \(\mathbf{G}_{t}=\Delta\mathbf{W}_{t}(1+\mathbf{W}_{0}+\sum_{j=1}^{t-1}\mathbf{ G}_{j})=\Delta\mathbf{W}_{t}(1+\mathbf{W}_{t-1})\). \(\operatorname{tr}\{\cdot\}\) denotes the trace of a matrix, \(\left\|\cdot\right\|_{F}\) denotes the Frobenius norm of a matrix and \(\mathbb{E}_{Y}^{X}\) is the conditional expectation. Proof details in Appendix A.5._

**Remark 4** (Deal with **Q1**).: _Theorem 2 indicates that one can control the generalization performance of ICL via controlling the implicit gradient norm along the entire ICL implicit GD trajectories. Specifically, modulating the norm of \([\mathbf{G}_{t}]_{1}^{L}\) or \([\Delta\mathbf{W}_{t}]_{1}^{L}\) may enhance performance when utilizing ICL. Note that controlling implicit gradient norm can also control the magnitude of the trace of implicitgradient noise covariance. This elucidates why weight pruning through SVD, even if it only alters a single weight matrix, can confer advantages on the performance of Transformers in ICL inference. We will present an example below demonstrating how weight pruning can affect the norm of \(\mathbf{G}_{t}\) or \(\Delta\mathbf{W}_{t}\), thereby influencing the expected generalization error. Additional examples provided in Appendix A.6. This is also why, as illustrated in Figure 1, the highest model performance sometimes occurs at a clipping rate of 0.995. Furthermore, this could elucidate the utility of normalization in Transformers. However, weight pruning also impacts the expressive power, implying that increased pruning does not invariably lead to better outcomes. This clarifies why the highest model performance may also occur at clipping rates lower than 0.995._

**Example 1** (Prune \(\mathbf{W}_{Q},\mathbf{W}_{K},\mathbf{W}_{V}\)).: _Consider \(\Delta\mathbf{W}_{k}=\left(\sum_{i=1}^{N}\mathbf{W}_{V}^{k}\mathbf{h}_{i}^{k-1 }\otimes\mathbf{W}_{K}^{k}\mathbf{h}_{i}^{k-1}\right)\mathbf{W}_{Q}^{k}\) when \(t=k\). And we primarily consider the changes in a upper bound written as \(UB(\left\|\Delta\mathbf{W}_{k}\right\|_{F}^{2})\triangleq\sum_{i=1}^{N}\left\| \mathbf{W}_{V}^{k}\mathbf{h}_{i}^{k-1}\otimes\mathbf{W}_{K}^{k}\mathbf{h}_{i} ^{k-1}\right\|_{F}^{2}\left\|\mathbf{W}_{Q}^{k}\right\|_{F}^{2}\). Let \(r\) represents the remained rank and \(\delta\) represents the potential noise consisting of parts with small singular values, that is, \(\mathbf{W}_{V}^{k}=\mathbf{W}_{V_{r}}^{k}+\delta_{V}=\mathbf{U}_{:r}^{V} \mathbf{\Sigma}_{:r}^{V}(\mathbf{V}_{:r}^{V})^{T}+\delta_{V}\), the same operation is applied to \(\mathbf{W}_{Q}^{k}\) and \(\mathbf{W}_{K}^{k}\) as well. Then we have_

\[\left\|\mathbf{W}_{V}^{k}\mathbf{h}_{i}^{k-1}\right\|_{2}^{2} =\left\|(\mathbf{W}_{V_{r}}^{k}+\delta_{V})\mathbf{h}_{i}^{k-1} \right\|_{2}^{2}\] \[=\left\|\mathbf{W}_{V_{r}}^{k}\mathbf{h}_{i}^{k-1}\right\|_{2}^{ 2}+2(\mathbf{W}_{V_{r}}^{k}\mathbf{h}_{i}^{k-1})^{T}(\delta_{V}\mathbf{h}_{i} ^{k-1})+\left\|\delta_{V}\mathbf{h}_{i}^{k-1}\right\|_{2}^{2}\] \[=\left\|\mathbf{W}_{V_{r}}^{k}\mathbf{h}_{i}^{k-1}\right\|_{2}^{ 2}+\left\|\delta_{V}\mathbf{h}_{i}^{k-1}\right\|_{2}^{2}\geq\left\|\mathbf{W} _{V_{r}}^{k}\mathbf{h}_{i}^{k-1}\right\|_{2}^{2},\]

_where \(\mathbf{W}_{V_{r}}^{k}=\mathbf{U}_{:r}^{V}\mathbf{\Sigma}_{:r}^{V}(\mathbf{V }_{:r}^{V})^{T}\) and \(\delta_{V}=\mathbf{U}_{:r}^{V}\mathbf{\Sigma}_{:r}^{V}(\mathbf{V}_{:r}^{V})^{T}\), and \(\mathbf{U}_{:r},\mathbf{U}_{r}\), are orthometric (properties of SVD), and further have_

\[UB(\left\|\Delta\mathbf{W}_{k}\right\|_{F}^{2}) \geq\sum_{i=1}^{N}\left\|\mathbf{W}_{V_{r}}^{k}\mathbf{h}_{i}^{k- 1}\otimes\mathbf{W}_{K}^{k}\mathbf{h}_{i}^{k-1}\right\|_{F}^{2}\left\| \mathbf{W}_{Q}^{k}\right\|_{F}^{2}\] (*) \[\geq\sum_{i=1}^{N}\left\|\mathbf{W}_{V_{r}}^{k}\mathbf{h}_{i}^{k- 1}\otimes\mathbf{W}_{K_{r}}^{k}\mathbf{h}_{i}^{k-1}\right\|_{F}^{2}\left\| \mathbf{W}_{Q}^{k}\right\|_{F}^{2}\] \[\geq\sum_{i=1}^{N}\left\|\mathbf{W}_{V_{r}}^{k}\mathbf{h}_{i}^{k- 1}\otimes\mathbf{W}_{K_{r}}^{k}\mathbf{h}_{i}^{k-1}\right\|_{F}^{2}\left\| \mathbf{W}_{Q_{r}}^{k}\right\|_{F}^{2}=UB(\left\|\Delta\mathbf{W}_{k}(r)\right\| _{F}^{2}),\] (**)

_where Eq. (*) is by6_

Footnote 6: For any vector \(\mathbf{a}\) and \(\mathbf{b}\), \(\text{rank}(\mathbf{a}\otimes\mathbf{b})=1\), so the matrix (\(\mathbf{a}\otimes\mathbf{b}\)) only has one non-zero singular value. Combined with \(||\mathbf{P}||_{F}=\sqrt{\sum_{i}\sigma_{i}^{2}(\mathbf{P})}\) and singular value is nonnegative, we can get \(||\mathbf{a}\otimes\mathbf{b}||_{F}=\sqrt{\sum_{i}\sigma_{i}^{2}(\mathbf{a} \otimes\mathbf{b})}=\max_{i}[\sigma_{i}(\mathbf{a}\otimes\mathbf{b})]\), therefore, the unique non-zero singular value will decrease after performing SVD on \(\mathbf{a}\) and/or \(\mathbf{b}\).

\[\left\|\mathbf{vec}(\mathbf{A})\otimes\mathbf{vec}(\mathbf{B})\right\|_{F}= \sqrt{\sum_{i}\sum_{j}|\mathbf{vec}(\mathbf{A})_{i}\mathbf{vec}(\mathbf{B})_{ j}|^{2}}=\max_{i}|\sigma_{i}(\mathbf{vec}(\mathbf{A})\otimes\mathbf{vec}( \mathbf{B}))|\]

_and Eq. (**) is by \(\left\|\mathbf{P}\right\|_{F}=\sqrt{\sum_{i}\sigma_{i}^{2}(\mathbf{P})}\) for any matrix \(\mathbf{A},\mathbf{B}\) and \(\left\|\mathbf{P}\right\|_{F}\geq\left\|\mathbf{P}(r)\right\|_{F}\). \(UB(\left\|\Delta\mathbf{W}_{k}(r)\right\|_{F}^{2})\) is the upper bound on \(\left\|\Delta\mathbf{W}_{k}\right\|_{F}^{2}\) after using SVD._

**Remark 5** (Deal with **Q2**).: _It is notable that \(\mathbf{G}_{t}\) is highly correlated with the sequence \((\Delta\mathbf{W}_{1},...,\Delta\mathbf{W}_{t})\). More precisely, adjusting the weight matrix of the \(k\)-th layer, will invariably impact the norm of \([\Delta\mathbf{W}_{t}]_{t>k}^{L}\), further influence \([\mathbf{G}_{t}]_{t>k}^{L}\). When we calibrate the weight matrix of the \(k\)-th layer, the span of affected weight updates \(\Delta\mathbf{W}_{t}\) encompasses \(L-k+1\) matrices, this indicates that the deeper the layer of the adjusted parameters is, the fewer the number of \(\mathbf{G}_{t}\) affected. It also suggests that tweaks to the deep layers yield a more steadfast influence on the global norm of \([\mathbf{G}_{t}]_{1}^{L}\), thereby exerting a steadier influence over generalization performance. The enhanced stability observed in adjusting deeper layers, as demonstrated in our experimental findings in Section 2, can be theoretically explained based on the analysis presented above._

**Remark 6** (How should **Theorem 2** be interpreted?).: _Expected generalization error (**Theorem 2**) = population risk (\(L_{\mu}\)) - empirical risk (\(L_{\mathbf{H}_{s}}\)). More specifically, on the one hand, **Theorem 2** showsthat clipping weights controls the F-norm of the implicit gradient \(([\Delta\mathbf{W}_{t}]_{1}^{L}/[\mathbf{G}_{t}]_{1}^{L})\), which helps reduce the expected generalization error. On the other hand, we can evaluate the empirical risk \((L_{\mathbf{H}_{s}})\) by assessing the model's performance on the validation set. If the generalization error is known, it is possible to estimate the population risk (\(L_{\mu}\)). Therefore, the most challenging aspect is addressing the generalization error. Furthermore, we will illustrate the interpretation of **Theorem 2** from the perspectives of pruning methods and the ICL shot numbers. (i) Adjusting the F-norm of \(([\Delta\mathbf{W}_{t}]_{1}^{L}/[\mathbf{G}_{t}]_{1}^{L})\) could enhance performance when utilizing ICL, suggesting that other weight-based pruning methods may also be effective. For example, magnitude-based pruning [52] directly controls the matrix F-norm. Certainly, there are also some layer-based pruning methods (e.g., drop-layer method [19]), we discuss in detail in Appendix B.6. (ii) Reducing the ICL shot numbers from \(N\) to \(N^{{}^{\prime}}\) can indeed lead to more robust outcomes specifically in the "SVD weight pruning" operation, rather than in the model's overall performance. Experimentally, in Section 2.2, we analyze the effect of different ICL shot numbers. As shown in Figure 2, with a decrease in the number of shots, the rate of performance collapse in the model slows down after a sharp reduction at the shallow layer. Theoretically, \(\Delta\mathbf{W}(N)-\Delta\mathbf{W}(N^{{}^{\prime}})=\left(\sum_{i=N^{{}^{ \prime}}+1}^{N}\mathbf{W}_{V}\mathbf{h}_{i}\otimes\mathbf{W}_{K}\mathbf{h}_{i} \right)\mathbf{W}_{Q}\), suggesting that the implicit gradient for \(N\) is expected to be more sensitive than that for \(N^{{}^{\prime}}\)._

### Applications of Our Theoretical Understanding

In this portion, we deal with **Q3**. We further explore the relationship between model performance improvement and SVD, as illustrated by a simple case study.

**Case 1**.: _Assuming that \(\mathbf{W}\) has two unit eigenvectors \(\mathbf{x}_{1},\mathbf{x}_{2}\), based on the properties of eigenvectors:_

\[\mathbf{W}\mathbf{x_{1}}=\lambda_{1}\mathbf{x_{1}},\mathbf{W}\mathbf{x_{2}}= \lambda_{2}\mathbf{x_{2}},\]

_and for a vector \(\mathbf{b}\) on the hyperplane, it can be decomposed into a linear combination of two eigenvectors:_

\[\mathbf{b}=l_{1}\mathbf{x_{1}}+l_{2}\mathbf{x_{2}}=\mathbf{W}\left(\frac{l_{1 }}{\lambda_{1}}\mathbf{x_{1}}+\frac{l_{2}}{\lambda_{2}}\mathbf{x_{2}}\right)= \mathbf{W}\mathbf{x},\]

_where \(l_{1},l_{2}\) are coefficients and \(\lambda_{1},\lambda_{2}\) are eigenvalues. Notice that if \(\lambda_{1}\gg\lambda_{2}\), when point \(\mathbf{b}\) moves in the direction of \(\mathbf{x_{1}}\), the value of \(l_{1}\) changes but the solution set \(\mathbf{x}\) changes insignificantly. Conversely, if it moves in the direction of \(\mathbf{x}_{2}\), the value of \(l_{2}\) changes and the solution set \(\mathbf{x}\) changes dramatically, this is an ill-conditioned7 problem whose solution is highly biased under small perturbations._

Footnote 7: Now consider a linear system \(\mathbf{W}\mathbf{x}=\mathbf{b}\), where the solution set \(\mathbf{x}\) is highly sensitive to the coefficients of \(\mathbf{W}\) and \(\mathbf{b}\). In such cases, the system of equations is termed ill-conditioned.

Given that SVD can be considered to play a role in noise reduction in ICL inference, and that this noise can cause significant disturbances to model output. We thus introduce the matrix ill-conditionedness, which can be measured by the Matrix condition number defined as follows.

**Matrix condition number8.**\(Cond(\mathbf{W})=||\mathbf{W}||_{p}||\mathbf{W}^{-1}||_{p}\). Specifically, the condition number \(Cond(\mathbf{W})=\frac{\sigma_{max}}{\sigma_{min}}\) when \(p=2\), where \(\sigma_{max},\sigma_{min}\) denote the maximum and minimum singular values respectively. Generally, for matrices of the same order, the higher the condition number of a matrix is, the greater its ill-conditionedness will be.

Footnote 8: Matrix condition number is an option, any indicator that can guide the control of norms is a potential option. These matrices are ill-conditioned for they satisfy \(\sigma_{max}\gg\sigma_{min}\).

Figure 3: 2-norm condition number of GPTJ-6B&LLAMA2-7B. The condition numbers for MLP are significantly lower than those for ATTN. In deeper layers, condition numbers tend to be higher. These matrices are ill-conditioned for they satisfy \(\sigma_{max}\gg\sigma_{min}\).

Then we analyze the matrix condition number of the models as shown in Figure 3. We observe that the condition number of deeper layers (especially the last layers) in the model is generally higher, indicating that the condition number may serve as a reference for adjusting the model.

Based on all our exciting insights, we find it intuitive to design ICL improvements based on them, especially in downstream tasks. We propose a method where we first select layers with the top-k largest condition numbers and then identify the layer with the largest number among these. We perform a greedy search for the optimal clipping rate \(\xi^{*}\) on the validation set and subsequently evaluate it on the test set. This procedure is reported in **Algorithm 1** in Appendix C.2.

**Experimental setup.** We conduct experiments for **Algorithm 1** on widely adopted benchmark datasets, including SST-2 [39], AGNEWS [58], EMOC [8], MRPC [12], RTE [5], CB [11], COPA [18]. Please refer to Appendix C.1 and C.3 for more detailed dataset and prompt settings. And for models, we use the same models as in Section 2.

**Experimental results.** As Figure 4 shows, the results indicate that **Algorithm 1** can visibly influence performance across different language understanding tasks in ICL inference. Specially, the effectiveness of **Algorithm 1** in enhancing performance varies not only by the model but also significantly by the task, indicating a potential task-specific and model-specific threshold for the benefits derived from algorithmic enhancements. More notably, **Algorithm 1** is a derivative-free optimization method and compresses the model to some extent, providing developers and researchers with an effective approach for handling downstream tasks. We also invite readers to refer to Appendices B.3, B.4 and B.5 for discussions on _What would be the effect of pruning only a single module?_, _Why optimal clipping rate \(\xi\) varies?_ and _What would happen if we apply the same clipping rate to other datasets_?

## 4 Conclusion and Limitation

In this paper, we show our surprising findings in ICL inference: SVD-based weight pruning can enhance ICL performance both in shallow and deep layers across different module types, and pruning weights in deep layers often results in more stable performance improvements than in shallow layers. We conduct an in-depth theoretical analysis and explanation of these findings. Specifically, we first present the implicit gradient descent trajectories of ICL, afterwards, we give a mutual information based generalization bounds of ICL via full implicit GD trajectories. Based on all our experimental and theoretical insights, we intuitively propose a derivative-free and effective method for downstream tasks in enhancing ICL inference. However, further studies are required on (i) how to extend our generalization theory to a more standard Transformer architecture, (ii) do the results about ICL hold true for tasks beyond natural language processing and (iii) how to minimize the cost of searching the optimal clipping rate. Those will deepen our understanding of the ICL capabilities.

Figure 4: The Model Performance on Test set by different tasks. The results are obtained by comparing four scenarios: ICL (GPT-J-6B), ICL+Algorithm1 (GPT-J-6B), ICL (LLAMA2-7B) and ICL+Algorithm1 (LLAMA2-7B). ICL+Algorithm1 demonstrates superior results over only ICL on different tasks. See Appendix C.5 for detailed numbers.

## Acknowledgments

This research was supported by National Natural Science Foundation of China (No.62476277, No.6207623), Beijing Natural Science Foundation (No.4222029), CCF-ALIMAMA TECH Kangaroo Fund(No.CCF-ALIMAMA OF 2024008), and Huawei-Renmin University joint program on Information Retrieval. We also acknowledge the support provided by the fund for building worldclass universities (disciplines) of Renmin University of China and by the funds from Beijing Key Laboratory of Big Data Management and Analysis Methods, Gaoling School of Artificial Intelligence, Renmin University of China, from Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education, from Intelligent Social Governance Interdisciplinary Platform, Major Innovation & Planning Interdisciplinary Platform for the "DoubleFirst Class" Initiative, Renmin University of China, from Public Policy and Decision-making Research Lab of Renmin University of China, and from Public Computing Cloud, Renmin University of China.

## References

* [1]R. Agarwal, A. Singh, L. M. Zhang, B. Bohnet, S. Chan, A. Anand, Z. Abbas, A. Nova, J. D. Co-Reyes, E. Chu, F. Behbahani, A. Faust, and H. Larochelle (2024) Many-shot in-context learning. arXiv preprint arXiv:2404.11018. Cited by: SS1.
* [2]K. Ahn, X. Cheng, H. Daneshmand, and S. Sra (2023) Transformers learn to implement preconditioned gradient descent for in-context learning. In Advances in Neural Information Processing Systems (37th edition), pp. 45614-45650. Cited by: SS1.
* [3]E. Akyurek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou (2023) What learning algorithm is in-context learning? investigations with linear models. In International Conference on Learning Representations, Cited by: SS1.
* [4]A. Asadi, E. Abbe, and S. Verdu (2018) Chaining mutual information and tightening generalization bounds. In Advances in Neural Information Processing Systems (32th edition), pp. 7245-7254. Cited by: SS1.
* [5]L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo (2009) The fifth pascal recognizing textual entailment challenge. In Text Analysis Conference, Cited by: SS1.
* Workshop on Challenges & Perspectives in Creating Large Language Models, pp. 95-136. Cited by: SS1.
* [7]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. In Advances in Neural Information Processing Systems (34th edition), pp. 1877-1901. Cited by: SS1.
* [8]A. Chatterjee, K. N. Narahari, M. Joshi, and P. Agrawal (2019) Semeval-2019 task 3: emocontext contextual emotion detection in text. In Proceedings of International Workshop on Semantic Evaluation (13th edition), pp. 39-48. Cited by: SS1.
* [9]K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, D. Belanger, L. J. Colwell, and A. Weller (2021) Rethinking attention with performers. In International Conference on Learning Representations, Cited by: SS1.
* [10]D. Dai, Y. Sun, L. Dong, Y. Hao, S. Ma, Z. Sui, and F. Wei (2023) Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics, pp. 4005-4019. Cited by: SS1.
* [11]M. De Marneffe, M. Simons, and J. Tonhauser (2019) The commitmentbank: investigating projection in naturally occurring discourse. In Sim and Bedeutung, Vol. 23, pp. 107-124. Cited by: SS1.
* [12]W. B. Dolan and C. Brockett (2005) Automatically constructing a corpus of sentential paraphrases. In Proceedings of International Workshop on Paraphrasing (3th edition), Cited by: SS1.
* [13]Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang (2022) GIm: general language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360. Cited by: SS1.

* [14] Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. _Psychometrika_, 1 (3):211-218, 1936.
* [15] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Training pruned neural networks. In _International Conference on Learning Representations_, 2018.
* [16] Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. In _Advances in Neural Information Processing Systems (36th edition)_, pages 30583-30598, 2022.
* [17] Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. In _Proceedings of Conference on Empirical Methods in Natural Language Processing (26th edition)_, pages 5484-5495, 2021.
* Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012)_, pages 394-398, 2012.
* [19] Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso, and Daniel A Roberts. The unreasonable ineffectiveness of the deeper layers. _arXiv preprint arXiv:2403.17887_, 2024.
* [20] Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M Roy, and Gintare Karolina Dziugaite. Sharpened generalization bounds based on conditional mutual information and an application to noisy, iterative algorithms. In _Advances in Neural Information Processing Systems (34th edition)_, pages 9925-9935, 2020.
* [21] Kazuki Irie, Robert Csordas, and Jurgen Schmidhuber. The dual form of neural networks revisited: Connecting test time predictions to training patterns via spotlights of attention. In _International Conference on Machine Learning_, pages 9639-9659, 2022.
* [22] Mikhail Khodak, Neil Tenenholtz, Lester Mackey, and Nicolo Fusi. Initialization and regularization of factorized neural layers. In _International Conference on Learning Representations_, 2021.
* [23] Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and Lingpeng Kong. In-context learning with many demonstration examples. _arXiv preprint arXiv:2302.04931_, 2023.
* [24] Yawei Li, Shuhang Gu, Luc Van Gool, and Radu Timofte. Learning filter basis for convolutional neural network compression. In _Proceedings of the IEEE International Conference on Computer Vision_, 2019.
* [25] Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling sgd with stochastic differential equations (sdes). In _Advances in Neural Information Processing Systems (34th edition)_, pages 12712-12725, 2021.
* [26] Opher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. Jurassic-1: Technical details and evaluation. Technical report, AI21 Labs, 2021.
* [27] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. In _Transactions of the Association for Computational Linguistics (12th edition)_, pages 157-173, 2024.
* [28] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 8086-8098, 2022.
* [29] J. Mercer. Functions of positive and negative type, and their connection with the theory of integral equations. _Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character_, 209:415-446, 1909. URL http://www.jstor.org/stable/91043.
* [30] Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional neural networks for resource efficient inference. In _International Conference on Learning Representations_, 2017.
* [31] Jeffrey Negrea, Mahdi Haghifam, Gintare Karolina Dziugaite, Ashish Khisti, and Daniel M Roy. Information-theoretic generalization bounds for sgld via data-dependent estimates. In _Advances in Neural Information Processing Systems (33th edition)_, pages 11013-11023, 2019.
* [32] Yury Polyanskiy and Yihong Wu. Lecture notes on information theory. Lecture Notes for 6.441 (MIT), ECE 563 (UIUC), STAT 364 (Yale), 2019.

* Qiu et al. [2024] Zihan Qiu, Zeyu Huang, Youcheng Huang, and Jie Fu. Empirical study on updating key-value memories in transformer feed-forward layers. In _International Conference on Learning Representations_, 2024.
* Rae et al. [2022] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods, analysis & insights from training gopher. _arXiv preprint arXiv:2112.11446_, 2022.
* Ren and Liu [2023] Ruifeng Ren and Yong Liu. In-context learning with transformer is really equivalent to a contrastive learning pattern. _arXiv preprint arXiv:2310.13220_, 2023.
* Russo and Zou [2016] Daniel Russo and James Zou. Controlling bias in adaptive data analysis using information theory. In _Artificial Intelligence and Statistics_, pages 1232-1240, 2016.
* Schothofer et al. [2022] Steffen Schothofer, Emanuele Zangrando, Jonas Kusch, Gianluca Ceruti, and Francesco Tudisco. Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations. In _Advances in Neural Information Processing Systems (36th edition)_, pages 20051-20063, 2022.
* Sharma et al. [2024] Pratyusha Sharma, Jordan T Ash, and Dipendra Misra. The truth is in there: Improving reasoning in language models with layer-selective rank reduction. In _International Conference on Learning Representations_, 2024.
* Socher et al. [2013] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing_, pages 1631-1642, 2013.
* Turuvekere Sreenivas et al. [2024] Sharath Turuvekere Sreenivas, Saurav Muralidharan, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Llm pruning and distillation in practice: The minitron approach. _arXiv preprint arXiv:2408.11796_, 2024.
* Tian et al. [2024] Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi Chen, and Simon Du. Joma: Demystifying multilayer transformers via joint dynamics of mlp and attention. In _International Conference on Learning Representations_, 2024.
* Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems (31th edition)_, 2017.
* Vig and Belinkov [2019] Jesse Vig and Yonatan Belinkov. Analyzing the structure of attention in a transformer language model. In _Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pages 63-76, 2019. URL https://aclanthology.org/W19-4808.
* Voita et al. [2019] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In _Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics_, pages 5797-5808, 2019. URL https://aclanthology.org/P19-1580.
* Oswald et al. [2023] Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In _International Conference on Machine Learning_, pages 35151-35174, 2023.
* Wang et al. [2019] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding systems. In _Advances in Neural Information Processing Systems (33th edition)_, 2019.
* Wang and Komatsuzaki [2021] Ben Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language model. https://github.com/kingoflolz/mesh-transformer-jax, May 2021.
* Wang et al. [2021] Hongyi Wang, Saurabh Agarwal, and Dimitris Papailiopoulos. Pufferfish: Communication-efficient models at no extra cost. In _Proceedings of Machine Learning and Systems 3_, 2021.
* Wang et al. [2023] Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. Label words are anchors: An information flow perspective for understanding in-context learning. In _Empirical Methods in Natural Language Processing_, 2023.

* [51] Ziqiao Wang and Yongyi Mao. Two facets of sde under an information-theoretic lens: Generalization of sgd via training trajectories and via terminal states. _arXiv preprint arXiv:2211.10691_, 2022.
* [52] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In _Advances in Neural Information Processing Systems (30th edition)_, 2016.
* [53] Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and Zhanxing Zhu. On the noisy gradient descent that generalizes as sgd. In _International Conference on Machine Learning_, 2020.
* [54] Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In _International Conference on Learning Representations_, 2021.
* [55] Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. In _Advances in Neural Information Processing Systems (31th edition)_, pages 2524-2533, 2017.
* [56] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. In _International Conference on Learning Representations_, 2017.
* [57] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* [58] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In _Advances in Neural Information Processing Systems (29th edition)_, pages 649-657, 2015.
* [59] Yufeng Zhang, Fengzhuo Zhang, Zhuoran Yang, and Zhaoran Wang. What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization. _arXiv preprint arXiv:2305.19420_, 2023.

Omitted Proofs and Additional Results

### Proof of Lemma 1

Proof.: Given that numerous studies [21; 10; 35] describe ICL with attention can be connected to a meta-optimizer which produces implicit gradient, we show the dual form between linear attention and gradient descent. First, consider a very simple linear layer,

\[F(\mathbf{x})=\mathbf{W}\mathbf{x},\]

where \(\mathbf{W}\in\mathbb{R}^{dout\times din}\) is the projection matrix. Given training inputs \([\mathbf{x}_{i}]_{i=1}^{N}\in\mathbb{R}^{din}\) with their labels \([\mathbf{y}_{i}]_{i=1}^{N}\in\mathbb{R}^{dout}\) and the loss function \(\mathcal{L}\) with learning rate \(\eta\), gradient descent process produces the corresponding back-propagation signals \([\mathbf{e}_{i}]_{i=1}^{N}\in\mathbb{R}^{dout}\), where \(\mathbf{e}_{i}=-\eta\left(\nabla_{\mathbf{y}_{i}^{{}^{\prime}}}\mathcal{L}\right)\) and \(\mathbf{y}_{i}^{{}^{\prime}}=\mathbf{W}\mathbf{x}_{i}\). During test time, we can use a trained linear layer,

\[\hat{F}(\mathbf{x}_{test})=\hat{\mathbf{W}}\mathbf{x}_{test}=(\mathbf{W}+ \Delta\mathbf{W})\mathbf{x}_{test}=\mathbf{W}\mathbf{x}_{test}+\left(\sum_{i= 1}^{N}\mathbf{e}_{i}\otimes\mathbf{x}_{i}\right)\mathbf{x}_{test},\] (5)

where \(\otimes\) denotes the outer product according to the chain rule of differentiation. On the other hand, this process can be associated with linear attention,

\[LinearAttn(\mathbf{V},\mathbf{K},\mathbf{q})=\mathbf{V}\mathbf{K}^{T}\mathbf{ q}=\sum_{i=1}^{N}\mathbf{v}_{i}(\mathbf{k}_{i}^{T}\mathbf{q})=\sum_{i=1}^{N}( \mathbf{v}_{i}\otimes\mathbf{k}_{i})\mathbf{q},\] (6)

where \([\mathbf{k}_{i}]_{1}^{N},[\mathbf{v}_{i}]_{1}^{N}\in\mathbb{R}^{din}\) represent the key and value vectors respectively, forming the key and value matrix \(\mathbf{K},\mathbf{V}\in\mathbb{R}^{din\times N}\) in the attention mechanism. Based on Eq.(6), rewrite Eq.(5) as:

\[\hat{F}(\mathbf{x}_{test})=\mathbf{W}\mathbf{x}_{test}+\left(\sum_{i=1}^{N} \mathbf{e}_{i}\otimes\mathbf{x}_{i}\right)\mathbf{x}_{test}=\mathbf{W} \mathbf{x}_{test}+LinearAttn(\mathbf{E},\mathbf{X},\mathbf{x}_{test}).\] (7)

Then let's go back to the ICL process and approximate standard attention Eq.(2) as a relaxed linear attention mechanism by removing the Softmax operation and scale factor:

\[\begin{split} F_{icl}(\mathbf{h}_{N+1})\approx\hat{\mathbf{h}}_{ N+1}&=\mathbf{h}_{N+1}+\mathbf{W}_{V}\mathbf{H}\mathbf{M}( \mathbf{W}_{K}\mathbf{H})^{T}\mathbf{W}_{Q}\mathbf{h}_{N+1}\\ &=\mathbf{h}_{N+1}+\mathbf{W}_{V}\mathbf{H}_{s}(\mathbf{W}_{K} \mathbf{H}_{s})^{T}\mathbf{W}_{Q}\mathbf{h}_{N+1}\\ &=\mathbf{h}_{N+1}+\mathbf{W}_{0}\mathbf{h}_{N+1}+\mathbf{W}_{V} \mathbf{H}_{s}(\mathbf{W}_{K}\mathbf{H}_{s})^{T}\mathbf{W}_{Q}\mathbf{h}_{N+1} \\ &=\mathbf{h}_{N+1}+\mathbf{W}_{0}\mathbf{h}_{N+1}+\left(\sum_{i= 1}^{N}\mathbf{W}_{V}\mathbf{h}_{i}\otimes\mathbf{W}_{K}\mathbf{h}_{i}\right) \mathbf{W}_{Q}\mathbf{h}_{N+1}\\ &=\mathbf{h}_{N+1}+\mathbf{W}_{0}\mathbf{h}_{N+1}+LinearAttn( \mathbf{W}_{V}\mathbf{H}_{s},\mathbf{W}_{K}\mathbf{H}_{s},\mathbf{W}_{Q} \mathbf{h}_{N+1})\\ &=\mathbf{h}_{N+1}+\mathbf{W}_{0}\mathbf{h}_{N+1}+\Delta\mathbf{ W}_{icl}\mathbf{h}_{N+1}\end{split}\] (8)

Where \(\mathbf{W}_{0}=0\), and \(\mathbf{W}_{V}\mathbf{H}_{s}\) is regarded as the meta-gradient of ICL, which is used to generate the implicit gradient matrix \(\Delta\mathbf{W}_{icl}\) to act on the final feature output. 

### Softmax Attention and ICL Implicit GD

Inspired by Ren and Liu [35], by connecting Softmax Attention with Kernels, we can interpret ICL as a gradient descent process in contrast learning pattern. For simplicity, as in Section 3.1, scale factors are removed as follows:

\[\mathbf{A}=\text{Softmax}((\mathbf{W}_{K}\mathbf{H})^{T}\mathbf{W}_{Q}\mathbf{ H}),\mathbf{A}_{u}=\exp((\mathbf{W}_{K}\mathbf{H})^{T}\mathbf{W}_{Q}\mathbf{H}),\] (9)

\[\mathbf{A}=\mathbf{D}^{-1}\mathbf{A}_{u},\mathbf{D}=diag(\mathbf{A}_{u}, \mathbf{1}_{N}).\]

where \(\exp(\cdot)\) is element-wise. Following Choromanski et al. [9] and Ren and Liu [35], a Softmax kernel function \(\mathbf{K}_{\text{Softmax}}:\mathbb{R}^{dout}\times\mathbb{R}^{dout}\to \mathbb{R}_{+}\) is defined as:

\[\mathbf{K}_{\text{Softmax}}(\mathbf{x},\mathbf{y})=\exp(\mathbf{x}^{T} \mathbf{y})=\exp\left(\frac{\|\mathbf{x}\|^{2}}{2}\right)\mathbf{K}_{gauss}( \mathbf{x},\mathbf{y})\exp\left(\frac{\|\mathbf{y}\|^{2}}{2}\right),\] (10)where \(\mathbf{K}_{gauss}(\mathbf{x},\mathbf{y})=\exp\left(-\frac{\|\mathbf{x}-\mathbf{y}\| ^{2}}{2}\right)\) is the Gaussian kernel with a variance \(\sigma^{2}=1\). According to Mercer's theorem [29], there exists some mapping function \(\phi\) satisfying \(K_{\text{Softmax}}(\mathbf{x},\mathbf{y})=\phi(\mathbf{x})^{T}\phi(\mathbf{y})\). Thus, rewrite Eq.(9) as:

\[\begin{split}\mathbf{A}_{u}(i,j)&=\exp((\mathbf{W}_ {K}\mathbf{h}_{i})^{T}\mathbf{W}_{Q}\mathbf{h}_{j})\\ &=\mathbf{K}_{\text{Softmax}}(\mathbf{W}_{K}\mathbf{h}_{i}, \mathbf{W}_{Q}\mathbf{h}_{j})\\ &=\phi(\mathbf{W}_{K}\mathbf{h}_{i})^{T}\phi(\mathbf{W}_{Q} \mathbf{h}_{j}),\end{split}\] (11)

then, connect with Eq.(2) we have,

\[\begin{split}\hat{\mathbf{h}}_{N+1}&=\mathbf{h}_ {N+1}+\mathbf{W}_{V}\mathbf{H}\text{M}\text{ Softmax}\left(\frac{((\mathbf{W}_{K}\mathbf{H})^{T}\mathbf{W}_{Q}\mathbf{h}_{N+1 })}{\sqrt{d_{scale}}}\right)\\ &\approx\mathbf{h}_{N+1}+\mathbf{W}_{V}\mathbf{H}\text{ Softmax}\left(((\mathbf{W}_{K}\mathbf{H})^{T}\mathbf{W}_{Q}\mathbf{h}_{N+1}) \right)\\ &=\mathbf{h}_{N+1}+\frac{1}{D^{{}^{\prime}}}\mathbf{W}_{V}[ \mathbf{H}_{s},\mathbf{h}_{N+1}]\mathbf{M}[\phi(\mathbf{W}_{K}\mathbf{H}_{s} ),\phi(\mathbf{W}_{K}\mathbf{h}_{N+1})]^{T}\phi(\mathbf{W}_{Q}\mathbf{h}_{N+1}) \\ &=\mathbf{h}_{N+1}+\frac{1}{D^{{}^{\prime}}}\mathbf{W}_{V}\mathbf{ H}_{s}\phi(\mathbf{W}_{K}\mathbf{H}_{s})^{T}\phi(\mathbf{W}_{Q}\mathbf{h}_{N+1}) \\ &=\mathbf{h}_{N+1}+\frac{1}{D^{{}^{\prime}}}\left[\sum_{i=1}^{N}( \mathbf{W}_{V}\mathbf{H}_{s})_{i}\otimes\phi(\mathbf{W}_{K}\mathbf{H}_{s})_{i }\right]\phi(\mathbf{W}_{Q}\mathbf{h}_{N+1})\\ &=\mathbf{h}_{N+1}+\Delta\mathbf{W}_{icl}^{{}^{\prime}}\phi( \mathbf{W}_{Q}\mathbf{h}_{N+1}),\end{split}\] (12)

where \(D^{{}^{\prime}}=\mathbf{1}_{N}\phi(\mathbf{W}_{K}\mathbf{H}_{s})^{T}\phi( \mathbf{W}_{Q}\mathbf{h}_{N+1})+\phi(\mathbf{W}_{K}\mathbf{h}_{N+1})^{T}\phi( \mathbf{W}_{Q}\mathbf{h}_{N+1})\) represents a constant for the normalized attention scores. It can be considered that the mapping function \(\phi\), or rather the effect of the Softmax function, is to project the original features into a higher-dimensional space to capture more profound features. Subsequently, learning and meta-optimization are conducted under this new feature space. It is also noteworthy that \(\text{rank}(\Delta\mathbf{W}_{icl}^{{}^{\prime}})\leq\text{rank}(\mathbf{H}_{ s})\leq N\).

### Feed-Forward and ICL Implicit GD

The LLMs based on the Transformer architecture not only contain an Attention mechanism, but also include feed-forward (MLP) layers. It can be estimated that the feed-forward layers account for approximately two-thirds of the parameters in a Transformer model [45; 44]. Thus, MLP layers are essential for LLMs and there has been considerable work hypothesizing and analyzing the model's feed-forward layers. For example, Geva et al. [17], Qiu et al. [33] show that feed-forward layers in Transformer based language models operate as key-value memories, Tian et al. [41] propose Joint MLP/Attention (JoMA) dynamics by integrating out the self-attention layers in Transformers, analyzes joint training of MLP and self-attention layers, and qualitatively explains dynamics of multi-layer Transformers. Based on the inspiration above, we take MLP into consideration and rewrite Eq. (3) as:

\[\begin{split}\hat{\mathbf{h}}_{N+1}&=\mathbf{h}_{N+1 }+\text{MLP}(\mathbf{W}_{V}\mathbf{H}\mathbf{M}(\mathbf{W}_{K}\mathbf{H})^{T} \mathbf{W}_{Q}\mathbf{h}_{N+1})\\ &=\mathbf{h}_{N+1}+\mathbf{W}_{out}\text{relu}(\mathbf{W}_{in} \mathbf{W}_{V}\mathbf{H}\mathbf{M}(\mathbf{W}_{K}\mathbf{H})^{T}\mathbf{W}_{Q }\mathbf{h}_{N+1}),\end{split}\]

where \(\mathbf{W}_{V}\),\(\mathbf{W}_{K}\),\(\mathbf{W}_{Q}\)\(\in\mathbb{R}^{(dim1)\times(dout+din)},\mathbf{W}_{in}\in\mathbb{R}^{(dim2)\times(dim1)}, \mathbf{W}_{out}\in\mathbb{R}^{(dout+din)\times(dim2)}\) are projection matrices and \(\text{relu}(\cdot)\) is the activation function. And we can further relax the activation function for ease of qualitative analysis:

\[\begin{split}\hat{\mathbf{h}}_{N+1}&=\mathbf{h}_{N+1 }+\mathbf{W}_{out}\mathbf{W}_{in}\mathbf{W}_{V}\mathbf{H}\mathbf{M}(\mathbf{W}_ {K}\mathbf{H})^{T}\mathbf{W}_{Q}\mathbf{h}_{N+1}\\ &=\mathbf{h}_{N+1}+\mathbf{W}_{MLP}\mathbf{W}_{V}\mathbf{H} \mathbf{M}(\mathbf{W}_{K}\mathbf{H})^{T}\mathbf{W}_{Q}\mathbf{h}_{N+1},\end{split}\]

where \(\mathbf{W}_{MLP}\in\mathbb{R}^{(dout+din)\times(dim1)}\), which can be seen as a dimensional adaptation. And then do the similar derivation that we do in Appendix A.1, we can get \(\Delta\mathbf{W}_{icl}^{{}^{\prime\prime}}=\mathbf{W}_{MLP}\left(\sum_{i=1}^{N} \mathbf{W}_{V}\mathbf{h}_{i}\otimes\mathbf{W}_{K}\mathbf{h}_{i}\right)\mathbf{W }_{Q}\).

### Proof of Theorem 1

Proof.: Similar to A.1, Pay attention to the test token when \(i=N+1\),

\[\mathbf{h}_{N+1}^{1} =\mathbf{h}_{N+1}^{0}+\Delta\mathbf{W}_{1}\mathbf{h}_{N+1}^{0},\] \[\mathbf{h}_{N+1}^{2} =\mathbf{h}_{N+1}^{1}+\Delta\mathbf{W}_{2}\mathbf{h}_{N+1}^{1}\] \[=\mathbf{h}_{N+1}^{0}+\Delta\mathbf{W}_{1}\mathbf{h}_{N+1}^{0}+ \Delta\mathbf{W}_{2}(1+\Delta\mathbf{W}_{1})\mathbf{h}_{N+1}^{0},\] \[\mathbf{h}_{N+1}^{3} =\mathbf{h}_{N+1}^{2}+\Delta\mathbf{W}_{3}\mathbf{h}_{N+1}^{2}\] \[=\mathbf{h}_{N+1}^{0}+\Delta\mathbf{W}_{1}\mathbf{h}_{N+1}^{0}+ \Delta\mathbf{W}_{2}(1+\Delta\mathbf{W}_{1})\mathbf{h}_{N+1}^{0}\] \[\quad+\Delta\mathbf{W}_{3}\left(1+\Delta\mathbf{W}_{1}+\Delta \mathbf{W}_{2}(1+\Delta\mathbf{W}_{1})\right)\mathbf{h}_{N+1}^{0},\] \[\quad......\]

Thus, we give the implicit GD trajectories \([\mathbf{G}_{t}]_{1}^{L}\) of ICL, where \(\mathbf{W}_{t}=\mathbf{W}_{t-1}+\mathbf{G}_{t}\),

\[\mathbf{W}_{0} =0,\] \[\mathbf{G}_{1} =\Delta\mathbf{W}_{1},\] \[\mathbf{G}_{2} =\Delta\mathbf{W}_{2}(1+\Delta\mathbf{W}_{1})=\Delta\mathbf{W}_{2 }(1+\mathbf{G}_{1}),\] \[\mathbf{G}_{3} =\Delta\mathbf{W}_{3}\left(1+\Delta\mathbf{W}_{1}+\Delta\mathbf{ W}_{2}(1+\Delta\mathbf{W}_{1})\right)=\Delta\mathbf{W}_{3}(1+\mathbf{G}_{1}+ \mathbf{G}_{2}),\] \[......\] \[\mathbf{G}_{L} =\Delta\mathbf{W}_{L}(1+\sum_{t=1}^{L-1}\mathbf{G}_{t}).\]

On the one hand, Eq.(13) shows \(\mathbf{G}_{t}=\Delta\mathbf{W}_{t}(1+\mathbf{W}_{t-1})\), on the other hand, \(\Delta\mathbf{W}_{t}\triangleq\left(\sum_{i=1}^{N}\mathbf{W}_{V}^{t}\mathbf{h }_{i}^{t-1}\otimes\mathbf{W}_{K}^{t}\mathbf{h}_{i}^{t-1}\right)\mathbf{W}_{Q}^ {t}\), which is only depend on \(([\mathbf{h}_{i}^{0}]_{1}^{N},[\mathbf{W}_{K}^{t}]_{1}^{L},[\mathbf{W}_{Q}^{t }]_{1}^{L},[\mathbf{W}_{V}^{t}]_{1}^{L})\). So the exclusion of Transformer weight \(([\mathbf{W}_{K}^{t}]_{1}^{L},[\mathbf{W}_{Q}^{t}]_{1}^{L},[\mathbf{W}_{V}^{t }]_{1}^{L})\) implies that \(\mathbf{G}_{t}\) is only dependent on \(\mathbf{W}_{t-1}\) and \(\mathbf{H}_{s}\). 

And the prediction can be read from the corresponding position in \(L\)-th layer output \(h_{N+1}^{L}\) as follows,

\[\mathbf{h}_{N+1}^{L}=\left(\begin{array}{c}*\\ \mathbf{y}_{pred}\end{array}\right)=\mathbf{h}_{N+1}^{0}+\mathbf{W}_{L} \mathbf{h}_{N+1}^{0}=(1+\mathbf{W}_{L})\left(\begin{array}{c}\mathbf{x}_{N+ 1}\\ mask\end{array}\right).\]

### Proof of Theorem 2

The origin form of the mutual information based bound is predicated on a sample-specific MI, which quantifies the shared information between the output variable \(W\) and the input sample set \(\mathbf{H}_{s}\). The following lemma shows the result:

**Lemma 2**.: _(Xu and Raginsky [55, Theorem 1.]). Assume the loss \(\ell(w,\mathbf{h})\) is R-subGaussian for any \(w\in\mathcal{W}\), then_

\[\widehat{\mathit{error}}\leq\sqrt{\frac{2R^{2}}{N}I(W;\mathbf{H}_{S})},\]

_where \(I(W;\mathbf{H}_{S})=D_{KL}(Q_{W,\mathbf{H}_{S}}\|Q_{W}\otimes Q_{\mathbf{H}_{S }})\) is the mutual information and \(D_{KL}\) denotes the \(KL\) divergence._

Unroll the terminal parameters' mutual information \(I(W;\mathbf{H}_{S})\) to the full trajectories' mutual information will get:

**Lemma 3**.: _(Wang and Mao [51, Lemma 4.]). Let **Assumption 1** hold, then \(I(W_{L};\mathbf{H}_{S})\leq\sum_{t=1}^{L}I(-G_{t}+C_{t}^{1/2}N_{t};\mathbf{H}_ {S}|W_{t-1})\). Let \(-G_{t}+C_{t}^{1/2}N_{t}\triangleq\mathcal{G}_{t}\)._Proof.: Recall **Assumption 1** Eq.(4), Wang and Mao [51] get,

\[I(W_{L};\mathbf{H}_{S}) =I(W_{L-1}+\eta(-G_{L}+C_{L}^{1/2}N_{L});\mathbf{H}_{S})\] \[\leq I(W_{L},\eta(-G_{L}+C_{L}^{1/2}N_{L});\mathbf{H}_{S})\] (14) \[=I(W_{L-1};\mathbf{H}_{S})+I(\eta(-G_{L}+C_{L}^{1/2}N_{L}); \mathbf{H}_{S}|W_{L-1})\] (15) \[=I(W_{L-1};\mathbf{H}_{S})+I(-G_{L}+C_{L}^{1/2}N_{L};\mathbf{H}_{S }|W_{L-1})\] \[\leq\sum_{t=1}^{L}I(-G_{t}+C_{t}^{1/2}N_{t};\mathbf{H}_{S}|W_{t-1} )+I(W_{0};\mathbf{H}_{S})\] \[=\sum_{t=1}^{L}I(-G_{t}+C_{t}^{1/2}N_{t};\mathbf{H}_{S}|W_{t-1}).\]

where Eq. (14) is by the data processing inequality (e.g., \(Z-(X,Y)-(X+Y)\) form a Markov chain then \(I(X+Y,Z)\leq I(X,Y;Z)\)), Eq. (15) is by the chain rule of the mutual information, and learning rate \(\eta\) is dropped since mutual information is scale-invariant. \(I(W_{0};\mathbf{H}_{S})=0\) for \(W_{0}\) is independent of all other random variables in **Theorem 1** and **Assumption 1**. 

Besides, we present the variational representation of mutual information:

**Lemma 4**.: _(Polyanskiy and Wu [32, Corollary 3.1.]). For two random variables \(X\) and \(Y\), we have \(I(X;Y)=\inf_{P}\mathbb{E}_{X}[D_{KL}(Q_{Y|X}\|P)]\), where the infimum is achieved at \(P=Q_{Y}\)._

**Lemma 5**.: _(Wang and Mao [51, Lemma 5.]). At every time step \(t\), let \(P_{\tilde{N}_{t}|W_{t-1}}\)be any distribution satisfying \(D_{KL}(P_{\tilde{\sigma}_{t}|W_{t-1}}\|P_{\tilde{N}_{t}|W_{t-1}})<\infty\), we have \(I(\mathcal{G}_{t};\mathbf{H}_{s}|W_{t-1})=\mathbb{E}_{W_{t-1}}\left[\inf_{P_{ \tilde{N}_{t}|W_{t-1}}}\mathbb{E}_{\mathbf{H}_{S}}^{W_{t-1}}\left[D_{KL}(Q_{ \mathcal{G}_{t}|\mathbf{H}_{s},W_{t-1}}\|P_{\tilde{N}_{t}|W_{t-1}})\right]\right]\), where the infimum is achieved when \(P_{\tilde{N}_{t}|W_{t-1}}=Q_{\mathcal{G}_{t}|W_{t-1}}\). The KL divergence may be viewed as an estimate of the sensitivity of the full batch implicit gradient to a specific demonstration sample \(\mathbf{H}_{s}=H_{s}\)._

From **Lemma 5**, every choice of \(P_{\tilde{N}_{t}|W_{t-1}}\) gives rise to an upper bound of the MI of interest via \(I(\mathcal{G}_{t};\mathbf{H}_{s}|W_{t-1})\leq\mathbb{E}_{W_{t-1}}\left[\mathbb{ E}_{\mathbf{H}_{s}}^{W_{t-1}}\left[D_{KL}(Q_{\mathcal{G}_{t}|\mathbf{H}_{s},W_{t-1}} \|P_{\tilde{N}_{t}|W_{t-1}})\right]\right]\). Same to Wang and Mao [51], choose an isotropic Gaussian prior \(P_{\tilde{N}_{t}|W_{t-1}}=\mathcal{N}(\tilde{g}_{t},\sigma_{t}^{2}I_{d})\), where both \(\tilde{g}_{t}\) and \(\sigma_{t}\) are only allowed to depend on \(W_{t-1}\) under **Theorem 1**, and optimize the KL divergence in **Lemma 5** over \(\sigma_{t}\) for a fixed \(\tilde{g}_{t}\). Additionally, Under **Theorem 1** where we define the implicit GD trajectories of ICL, assume \(C_{t}\) is a positive-definite matrix, for any \(t\in[L]\), we have,

\[I\left(-G_{t}+C_{t}^{1/2}N_{t};\mathbf{H}_{s}|W_{t-1}=w_{t-1}\right)\] \[\leq\inf_{\tilde{g}_{t},\sigma_{t}}\mathbb{E}_{\mathbf{H}_{s}} \left[D_{KL}\left(P_{-G_{t}+C_{t}^{1/2}N_{t}|W_{t-1}=w_{t-1},\mathbf{H}_{s}=H_{ s}}\|P_{-\tilde{g}_{t}+\sigma_{t}N_{t}|W_{t-1}=w_{t-1}}\right)\right]\] \[=\inf_{\tilde{g}_{t},\sigma_{t}}\mathbb{E}_{\mathbf{H}_{s}} \left[\frac{1}{2}\log\left(\frac{\det(\sigma_{t}^{2}I_{d})}{\det(C_{t})}\right) -\frac{1}{2}d+\frac{1}{2\sigma_{t}^{2}}(G_{t}-\tilde{g}_{t})^{T}I_{d}^{-1}(G_ {t}-\tilde{g}_{t})+\frac{1}{2\sigma_{t}^{2}}\mathrm{tr}\left\{I_{d}^{-1}C_{t }\right\}\right]\] (16) \[=\frac{1}{2}\inf_{\tilde{g}_{t},\sigma_{t}}\mathbb{E}_{\mathbf{H} _{s}}\left[\frac{1}{\sigma_{t}^{2}}(\|G_{t}-\tilde{g}_{t}\|_{2}^{2}+\mathrm{tr }\left\{C_{t}\right\})+d\log\sigma_{t}^{2}-d-\mathrm{tr}\left\{\log C_{t} \right\}\right]\] (17)

where Eq. (16) is by the KL divergence between two Gaussian distributions: \(D_{KL}(p\|q)=\frac{1}{2}\left[\log\frac{\det(\Sigma_{q})}{\det(\Sigma_{p})}-k+( \mu_{p}-\mu_{q})^{T}\Sigma_{q}^{-1}(\mu_{p}-\mu_{q})+\mathrm{tr}(\Sigma_{q}^{-1} \Sigma_{p})\right],\) and Eq. (17) is due to the fact that \(G_{t}^{T}G_{t}=\mathrm{tr}(G_{t}G_{t}^{T})\) and \(\log\det(C_{t})=\mathrm{tr}(\log C_{t})\) when \(C_{t}\) is a positive-definite matrix.

Let \(A_{1}(t)=\mathbb{E}_{\mathbf{H}_{s}}\big{[}\|G_{t}-\tilde{g}_{t}\|_{2}^{2}+ \mathrm{tr}\left\{C_{t}\right\}\big{]}\) and \(A_{2}(t)=\mathbb{E}_{\mathbf{H}_{s}}[\mathrm{tr}\left\{\log C_{t}\right\}]\) and fix \(\tilde{g}_{t}\) can rewrite Eq. (17) as,

\[\frac{1}{2}\inf_{\sigma_{t}\geq 0}\frac{1}{\sigma_{t}^{2}}A_{1}(t)+d \log\sigma_{t}^{2}-d-A_{2}(t)\] \[=\frac{1}{2}d\log\frac{A_{1}(t)}{d}-\frac{1}{2}A_{2}(t),\]where the optimal \(\sigma^{*}=\sqrt{\frac{A_{i}(t)}{d}}\) when we take the derivative of \(\sigma_{t}\). Then combine **Lemma 2** and **Lemma 3**, we can finish the proof of the following lemma.

**Lemma 6**.: _Under the conditions of **Theorem 1**, assume the implicit gradient noise covariance \(C_{t}\) is a positive-define matrix, the loss \(\ell(w,\mathbf{h})\) is R-subGaussian for any \(w\in\mathcal{W}\in\mathbb{R}^{d}\). For any \(t\in[L]\), let \(\tilde{g_{t}}\) be any constant vector for a given \(w_{t-1}\), then_

\[\widetilde{\text{error}} \leq\sqrt{\frac{R^{2}}{N}\sum_{t=1}^{L}\mathbb{E}_{W_{t-1}}\left[ d\log(A_{1}(t)/d)-A_{2}(t)\right]}\] \[=\sqrt{\frac{R^{2}}{N}\sum_{t=1}^{L}\mathbb{E}_{W_{t-1}}^{\mathbf{ H}_{t}}\left[d\log\left(\frac{\left\|\mathbf{vec}(\Delta\mathbf{W}_{t}(1+ \sum_{j=1}^{t-1}\mathbf{G}_{j}))-\tilde{g_{t}}\right\|_{2}^{2}+\mathrm{tr}\{C _{t}\}}{d}\right)-\mathrm{tr}\{\log C_{t}\}\right]},\]

_where \(A_{1}(t)=\mathbb{E}_{W_{t-1}}^{\mathbf{H}_{s}}\left[\left\|G_{t}-\tilde{g_{t}} \right\|_{2}^{2}+\mathrm{tr}\{C_{t}\}\right],A_{2}(t)=\mathbb{E}_{W_{t-1}}^{ \mathbf{H}_{s}}\left[\mathrm{tr}\{\log C_{t}\}\right]\), \(\mathbf{vec}(\mathbf{G}_{t})=G_{t}\), \(\mathrm{tr}\{\cdot\}\) denotes the trace of a matrix and \(\mathbb{E}_{Y}^{X}\) is the conditional expectation._

Further, if we let \(\tilde{g_{t}}=0\), reverse the process of flattening the weight matrix into a vector form, and by Eq. (19) we obtain

\[\left\|\mathbf{vec}(\Delta\mathbf{W}_{t}(1+\sum_{j=1}^{t-1} \mathbf{G}_{j})))-\tilde{g_{t}}\right\|_{2}^{2}+\mathrm{tr}\{C_{t}\} =\left\|\Delta\mathbf{W}_{t}(1+\sum_{j=1}^{t-1}\mathbf{G}_{j}) \right\|_{F}^{2}+\mathrm{tr}\{C_{t}\}\] \[\leq\left\|\Delta\mathbf{W}_{t}\right\|_{F}^{2}\cdot\left\|1+ \sum_{j=1}^{t-1}\mathbf{G}_{j}\right\|_{F}^{2}+\mathrm{tr}\{C_{t}\}.\]

Then plug everything into **Lemma 6**, we conclude the proof.

### Examples of Remark 4

First, we will present several expressions that will be utilized later. For any matrix \(\mathbf{A}\) and \(\mathbf{B}\), we have,

\[\left\|\mathbf{vec}(\mathbf{A})\otimes\mathbf{vec}(\mathbf{B}) \right\|_{F}=\sqrt{\sum_{i}\sum_{j}\left|\mathbf{vec}(\mathbf{A})_{i}\mathbf{ vec}(\mathbf{B})_{j}\right|^{2}}=\max_{i}\left|\sigma_{i}(\mathbf{vec}( \mathbf{A})\otimes\mathbf{vec}(\mathbf{B}))\right|\] (18) \[\left\|\mathbf{A}\mathbf{B}\right\|_{F}\leq\left\|\mathbf{A} \right\|_{F}\left\|\mathbf{B}\right\|_{F}\] (19) \[\left\|\mathbf{A}+\mathbf{B}\right\|_{F}\leq\left\|\mathbf{A} \right\|_{F}+\left\|\mathbf{B}\right\|_{F}\] (20)

Same in **Lemma 1**, \(\Delta\mathbf{W}_{icl}=\left(\sum_{i=1}^{N}\mathbf{W}_{V}\mathbf{h}_{i} \otimes\mathbf{W}_{K}\mathbf{h}_{i}\right)\mathbf{W}_{Q}\), let \(r\) represents the remained rank and \(\delta\) represents the potential noise consisting of parts with small singular values. Some SVD controlling the norm of \(\Delta\mathbf{W}_{icl}\) examples are as follows.

**Example 2** (Prune \(\mathbf{W}_{Q}\)).: _Suppose we decompose \(\mathbf{W}_{Q}\) by SVD, \(\mathbf{W}_{Q}=\mathbf{W}_{Q_{r}}+\delta_{Q}=\mathbf{U}_{\pi r}^{Q}\mathbf{ \Sigma}_{:r}^{Q}(\mathbf{V}_{:r}^{Q})^{T}+\delta_{Q}\), let \(\mathbf{z}\triangleq\left(\sum_{i=1}^{N}\mathbf{W}_{V}\mathbf{h}_{i}\otimes \mathbf{W}_{K}\mathbf{h}_{i}\right)\) for simplicity._

\[\left\|\Delta\mathbf{W}_{icl}\right\|_{F} =\left\|\mathbf{z}\mathbf{U}_{:r}^{Q}\mathbf{\Sigma}_{:r}^{Q}( \mathbf{V}_{:r}^{Q})^{T}+\mathbf{z}\delta_{Q}\right\|_{F}\] \[\leq\left\|\mathbf{z}\mathbf{W}_{Q_{r}}\right\|_{F}+\left\| \mathbf{z}\delta_{Q}\right\|_{F},\]

_where the inequality part takes advantage of the Eq. (20), so the upper bound on \(\left\|\Delta\mathbf{W}_{icl}\right\|_{F}\) is decreasing when using SVD._

**Example 3** (Prune \(\mathbf{W}_{V}\) or \(\mathbf{W}_{K}\)).: _Suppose we decompose \(\mathbf{W}_{V}\) by SVD, \(\mathbf{W}_{V}=\mathbf{W}_{V_{r}}+\delta_{V}=\mathbf{U}_{:r}^{V}\mathbf{ \Sigma}_{:r}^{V}(\mathbf{V}_{:r}^{V})^{T}+\delta_{V}\), and consider the upper bound defined in **Example 1**,_

\[UB(\left\|\Delta\mathbf{W}\right\|_{F}^{2}) =\sum_{i=1}^{N}\left\|\mathbf{W}_{V}\mathbf{h}_{i}\otimes\mathbf{W} _{K}\mathbf{h}_{i}\right\|_{F}^{2}\left\|\mathbf{W}_{Q}\right\|_{F}^{2}\] \[\geq\sum_{i=1}^{N}\left\|\mathbf{W}_{V_{r}}\mathbf{h}_{i}\otimes \mathbf{W}_{K}\mathbf{h}_{i}\right\|_{F}^{2}\left\|\mathbf{W}_{Q}\right\|_{F}^{2},\] (21)_where Eq. (21) is by Eq. (18), so the upper bound on \(\left\|\Delta\mathbf{W}_{icl}\right\|_{F}\) is also decreasing. The same is true for decomposing \(\mathbf{W}_{K}\)._

**Example 4** (Prune \(\mathbf{W}_{MLP}\)).: _Suppose we decompose \(\mathbf{W}_{MLP}\) by SVD, \(\mathbf{W}_{MLP}=\mathbf{W}_{MLP_{r}}+\delta_{MLP}=\mathbf{U}_{:r}\mathbf{ \Sigma}_{:r}\mathbf{V}_{:r}^{T}+\delta_{MLP}\), let \(\mathbf{z}\triangleq\left(\sum_{i=1}^{N}\mathbf{W}_{V}\mathbf{h}_{i}\otimes \mathbf{W}_{K}\mathbf{h}_{i}\right)\mathbf{W}_{Q}\) for simplicity._

\[\left\|\Delta\mathbf{W}_{icl}^{{}^{\prime\prime}}\right\|_{F} =\left\|\mathbf{U}_{:r}\mathbf{\Sigma}_{:r}\mathbf{V}_{:r}^{T} \mathbf{z}+\delta_{MLP}\mathbf{z}\right\|_{F}\] \[\leq\left\|\mathbf{W}_{MLP_{r}}\mathbf{z}\right\|_{F}+\left\| \delta_{MLP}\mathbf{z}\right\|_{F},\]

_where recall Appendix A.3, \(\Delta\mathbf{W}_{icl}^{{}^{\prime\prime}}=\mathbf{W}_{MLP}\left(\sum_{i=1} ^{N}\mathbf{W}_{V}\mathbf{h}_{i}\otimes\mathbf{W}_{K}\mathbf{h}_{i}\right) \mathbf{W}_{Q}\). And the inequality part takes advantage of the Eq. (20), so the upper bound on \(\left\|\Delta\mathbf{W}_{icl}^{{}^{\prime\prime}}\right\|_{F}\) is decreasing when using SVD._

## Appendix B More Discussions

### Explanation of the Mask Matrix

Our notation here follows the related work [2], which explains: Note that the prompt is asymmetric since the label for \(\mathbf{x}_{N+1}\) is excluded from the input. To reflect this asymmetric structure, the mask matrix \(\mathbf{M}\) is included in the attention. More specifically, if you pay attention to the \((N+1)\)-th item, \(\mathbf{M}\) is supposed to represent a causal mask (For \(\mathbf{H}\in\mathbb{R}^{(dout+din)\times(N+1)}\), \(\mathbf{HM}=(\mathbf{h}_{1},...,\mathbf{h}_{N},\mathbf{h}_{N+1})\mathbf{M}=( \mathbf{h}_{1},...,\mathbf{h}_{N},0)=\mathbf{H}_{s}\)). Besides, this mask method is used in GLM training [13]: Part A tokens can attend to each other, but cannot attend to any tokens in B. Part B tokens can attend to Part A and antecedents in B, but cannot attend to any subsequent tokens in B. So it is reasonable for Eq.(1) and Eq.(2) use the same mask.

### Definitions of Deep and Shallow

Indeed, there is no universally accepted definition for the terms "deep" and "shallow" as their interpretation can be subjective and dependent on the reference frame (e.g., model size). Intuitively, in this paper, our definition is similar to that of work [50]: "shallow" layers refer to those closer to the input, while "deep" layers are closer to the output. In Figure 1 and Figure 2 (Section 2), "shallow" typically denotes the first few layers, and "deep" denotes the last few layers of the network.

### The Effect of Pruning Only a Single Module

In our theory, the example (pruning only \(\mathbf{W}_{K}\) or \(\mathbf{W}_{V}\)) is provided in Appendix A.6, demonstrating how weight pruning can affect the norm of \([\Delta\mathbf{W}_{t}]_{t}^{L}\mathbf{\lceil G}_{t}\mathbf{\rceil}_{t}^{L}\). Thus, it may confer advantages on the performance of Transformers in ICL inference. However, in **Theorem 2** (**Remark 5**), it also suggests that modifications to the shallow layers have a less steady impact. Additionally, please review the supplementary experimental results provided below. (We choose key projection matrix \(\mathbf{W}_{K}\) and select the 3-layer (large matrix condition number in Figure 3) of GPT-J-6B, others are the same to Section 2.1).

### Why Optimal Clipping Rate Varies?

As we mentioned in Section 3.3 and Section C.4, the implicit gradients produced by Transformers in practical applications are noisy due to factors such as the extent of model pre-training and data characteristics (e.g., ICL shot number/task difficulty). Therefore, \([\Delta\mathbf{W}_{t}]_{t}^{L}/[\mathbf{G}_{t}]_{t}^{L}\) in **Theorem 1** have varies noise. That is why optimal \(\xi\) varies.

**Gradient quality derived from context (i.e., \(\mathbf{G}_{t}\)).**

* In **Theorem 1** (Remark 2), \(\mathbf{G}_{t}\) is only dependent on \(\mathbf{W}_{t-1}\) and \(\mathbf{H}_{s}\), this is consistent with gradient descent in terms of relevance (Conventionally, gradients in training are only related to the current parameters and the training samples).
* In real-world training scenarios, SGD computes gradient by selecting a small batch of samples per iteration. This approach approximates the true gradient, inherently introducing noise. Similarly, in In-Context Learning, a small subset of samples (context examples) is used to generate implicit gradient (i.e., \(\mathbf{G}_{t}\)), which also results in the introduction of noise.

### Apply the Same Clipping Rate to Other Datasets

As we mentioned in Appendix B.4, \([\Delta\mathbf{W}_{t}]_{t}^{L}/[\mathbf{G}_{t}]_{t}^{L}\) in **Theorem 1** exhibit varying levels of noise, causing the optimal clipping rate to vary among different tasks, as it is dependent on the specific task and data. So if we apply the clipping rate (0.95) as used in SST-2 to other datasets, the model performance can either improve or deteriorate. Additionally, It is possible to conduct a certain number of experiments to find a range of optimal clipping rate that is broadly applicable.

### What Would Happen if the Layer Was Dropped Entirely?

Dropping the layer could be the best option specifically for optimizing generalization error. Here's a detailed analysis:

(1) In our theoretical framework, we model each layer of the Transformer do a single iteration of implicit gradient descent (ICL) in **Theorem 1**. This scientific analysis references [46, 3, 10].

(2) In **Theorem 2**, L-layer Transformer:

Expected generalization error = population risk (\(L_{\mu}\)) - empirical risk (\(L_{\mathbf{H}_{s}}\))

Expected generalization error = (only show the main part). If you drop the entire layer, it will change from \(\sum_{t=1}^{L}\) to \(\sum_{t=1}^{L-1}\). Therefore, dropping the layer may in fact be the best option for generalization error.

(3) However, according to the traditional statistical learning viewpoint, performance can be bounded by the sum of optimization error and generalization error (Please see **Remark 6** for "How should **Theorem 2** be interpreted?").

Thus, during the pruning process, there is a trade-off between optimization and generalization. Therefore, as pruning increases, the model's performance tends to first improve and then decline (the drop-layer method [19] does not harm but also does not improve), as demonstrated in the experiments shown in Figure 1.

In conclusion, dropping the entire layer can be a potential method (best option for generalization error) in our theoretical framework, but it may not necessarily be the best option for model performance.

## Appendix C Extension to Experiments

**Computational resources.** We use a single NVIDIA GeForce RTX 3090 GPU and most tests run take 1-5 hours depending on dataset size and context length.

### Prompts

### Algorithm1

``` Require: Pretrained model \(\mathcal{M}\), dataset \(\mathcal{D}\), predefined clipping rate candidate set \(\mathcal{C}\), predefined module \(\mathcal{O}\in[\)ATTN, MLP\(]\) Split \(\mathcal{D}\) into ICL demonstration sample set \(\mathcal{H}_{s}\), validation set \(\mathcal{V}\), and test set \(\mathcal{T}\) Compute condition numbers for all layers in \(\mathcal{M}\) Select layers \(\mathcal{L}\) with top-\(k\) largest condition numbers given \(\mathcal{O}\) Select the largest layer number \(l\) from \(\mathcal{L}\) Initialize \(\xi^{*}=0\) Initialize \(score^{*}=0\) for each \(\xi\) in \(\mathcal{C}\)do \(\mathcal{M}^{{}^{\prime}}=Clip(\mathcal{M},l,\xi,\mathcal{O})\) \(score=Evaluate(\mathcal{M}^{{}^{\prime}},\mathcal{V},\mathcal{H}_{s})\) if\(score\)>\(score^{*}\)then \(\xi^{*}=\xi\) \(score^{*}=score\) endif endfor \(\mathcal{M}^{*}=Clip(\mathcal{M},l,\xi^{*},\mathcal{O})\) \(test\ score=Evaluate(\mathcal{M}^{*},\mathcal{T},\mathcal{H}_{s})\) Output \(test\ score\) on \(\mathcal{T}\) ```

**Algorithm 1** Search the Optimal Clipping Rate for Downstream Tasks

**The details.** Firstly, the details of the algorithm can be reviewed in the code provided. Specifically, the set of clipping rate candidates is predefined. In our study, the clipping rate candidates are set as shown in Figures 1 and 2: [0, 0.1, 0.5, 0.75, 0.9, 0.95, 0.99, 0.995]. Besides, we analyze the impact of different hyperparameters through comparative experiments (details in Section 2 and Section 3.4), as detailed below. (1) Clipping rate \(\xi\):We search for the optimal \(\xi\) in the predefined clipping rate candidates. (2) Predefined module \(\mathcal{O}\): The module containing the target pruning weights, which can be chosen from \([k\_proj,q\_proj,v\_proj,out\_proj,fc\_in,fc\_out,all,mlp,attn]\). (3) Selected layer \(l\) : The layer containing the target pruning weights. For example: In Section 2, we mainly focus on comparing the impact of weight pruning to the first **row and the last two layers** of the model. (4) ICL shot number : The demonstration number in ICL, we analyze the effect of different ICL shot numbers in Section 2.2.

\begin{table}
\begin{tabular}{c c l} \hline \hline
**Dataset** & **Type** & **Prompt** \\ \hline SST-2 & (cls.) & text: \textless{}textless{} sentiment: \textless{}label> \\ AGNEWS & (cls.) & text: \textless{}textless{} classification: \textless{}label> \\ EMOC & (cls.) & text: \textless{}textless{} sentiment: \textless{}label> \\ MRPC & (cls.) & sentence1: \textless{}sentence1\textgreater{} sentence2: \textless{}sentence2\textgreater{} label : \textless{}label> \\ RTE & (cls.) & \textless{}premise> Does this mean that \textless{}hypothesis\textgreater{} is true? select Yes \\  & & or No? \textless{}label> \\ CB & (cls.) & Suppose \textless{}premise> Can we infer that \textless{}hypothesis\textgreater{}? Yes, No, or \\  & & Maybe? \textless{}label> \\ COPA & (mch.) & \textless{}premise>\textless{}question\textgreater{}choice\textgreater{} \\ \hline \hline \end{tabular}
\end{table}
Table 1: The prompts of the datasets used in our experiments. Here regard the types of tasks: classification, multiple-choice as (cls.), (mch.). \(\lnot\) represents input from the dataset. For (mch.) tasks, we put in different candidates in the prompt and calculate the average log-likelihood of each candidate, and select the candidate with the highest score.

### Dataset Details

For experiments, we consider numerous classification datasets, including: SST-2 [39], AGNEWS [58], EMOC [8], MRPC [12], RTE [5], CB [11]. We also include a multiple-choice dataset COPA [18].

**SST-2.** SST-2 focuses on sentiment analysis, specifically the task of determining the sentiment of movie reviews. The dataset consists of sentences labeled as having a positive or negative sentiment. These sentences are divided into a training set, a validation set, and a test set. The training set includes about 67,349 sentences, whereas the validation set contains around 872 sentences. The test set comprises approximately 1,821 sentences without labels. We random sample \(N\)(10) data from the train set as demonstration sample, 8,000 data from the train set as validation set to search the optimal clipping rate, the rest 59,000+ data as test set.

**AGNews.** The AGNews dataset is a collection designed for text classification tasks, specifically for news categorization. It consists of news articles gathered from the AG's corpus of news on the web, which is categorized into four main topics: World, Sports, Business, and Science/Technology. In terms of structure, the AGNews dataset comprises approximately 120,000 training samples and 7,600 test samples. We random sample \(N\)(8) data from the train set as demonstration sample, 8000 data from the train set as validation set to search the optimal clipping rate, the 7,600 test samples as test set.

**EmoC.** The EmoC dataset focuses on contextual emotion detection in text. It consists of short text conversations extracted from three-turn English Tweets. The dataset is annotated with four emotion labels: happiness, sadness, anger, and others, we relabel the dataset with happiness and others. The training set includes 30,160 samples, while the test set comprises 5,509 samples. We random sample \(N\)(10) data from the train set as demonstration sample, 5,000 data from the train set as validation set to search the optimal clipping rate, the 5,509 test samples as test set.

**MRPC.** The MRPC dataset focuses on paraphrase detection in text. It consists of sentence pairs automatically extracted from online news sources. The dataset is annotated with binary labels indicating whether the sentences are paraphrases of each other. The training set includes 3,668 pairs, the validation set includes 408 pairs, while the test set comprises about 1,725 pairs. We random sample \(N\)(10) data from the train set as demonstration sample, the 408 pairs from validation set to search the optimal clipping rate, the 1,725 test samples as test set.

**RTE.** Recognizing Textual Entailment (RTE) task, which involves assessing the relationship between a pair of sentences--a premise and a hypothesis. The objective of the task is to determine whether the hypothesis can be logically inferred from the premise. Specifically, the model must evaluate whether the relationship between the two sentences is one of "entailment," where the content of the hypothesis is directly or indirectly supported by the premise, or "non-entailment," which includes both contradiction and neutrality. In RTE from SuperGLUE [47], the training set includes about 2,490 items, whereas the validation set contains around 277 items. We random sample \(N\)(10) data from the train set as demonstration sample, the remain items from train set to search the optimal clipping rate, the 277 validation samples as test set.

**CB.** The CommitmentBank (CB) dataset is part of the SuperGLUE benchmark and focuses on textual entailment with an emphasis on pragmatic inference. The task involves determining whether a hypothesis can be logically inferred from a given text, which in this dataset, typically comprises a premise followed by a hypothesis. The training set includes about 250 items, whereas the validation set contains around 56 items. We random sample \(N\)(15) data from the train set as demonstration sample, the remain 235 items from train set to search the optimal clipping rate, the 56 validation samples as test set.

**COPA.** The Choice of Plausible Alternatives (COPA) dataset is specifically designed to evaluate causal reasoning abilities in natural language processing models. This dataset presents a task where models must determine causal relationships within simple scenarios. Each question in COPA consists of a premise and two possible choices, one of which is the correct cause or effect of the premise. COPA's dataset is relatively straightforward and consists of 500 questions split evenly into training (400) and validation (100) sets. We random sample \(N\)(10) data from the train set as demonstration sample, the 200 items from train set to search the optimal clipping rate, the 100 validation samples as test set.

### Noise Discussion of the Implicit Gradient

As mentioned in Garg et al. [16]'s work, the trained model is able to learn unseen linear functions from in-context examples with performance comparable to the optimal least squares estimator, this can be seen in Figure 4(a). More importantly, the number of in-context examples (shot number) plays a significant role. In this case, the error approaches zero only when the shot number is greater than or equal to \(d\). This indicates that the implicit gradients of ICL are influenced by the shot number, and there exists a threshold \(N\), when the actual shot number \(b\) is below this threshold, the implicit gradients are noisy. On the other hand, following the same setting of [16] in figure 4(a), we compare the performance of the Trained Transformer at different train steps as shown in figure 4(b). This indicates that the performance of the model varies with the number of train steps, which also means that the implicit gradients of ICL generated by the model are influenced by the extent of its pre-training.

In conclusion, actual implicit gradient descent involves noise, which primarily stems from the shot number and the degree of model pre-training.

### The Detailed Results of Algorithm 1

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model Name & Layer Number & Module Name & Optimal \(\xi^{*}\) & Test Acc Improve \\ \hline GPT-J-6B & 26 & MLP & 0.95 & 0.7527 - **0.8437**(\(\uparrow\)) \\ GPT-J-6B & 27 & ATTN & 0.995 & 0.7527 - 0.7642(\(\uparrow\)) \\ LLAMA2-7B & 30 & MLP & 0.99 & 0.9228 - 0.9257(\(\uparrow\)) \\ LLAMA2-7B & 30 & ATTN & 0.95 & 0.9228 - **0.9287**(\(\uparrow\)) \\ \hline \hline \end{tabular}
\end{table}
Table 2: The results of Algorithm 1 on SST-2

Figure 5: Evaluating the trained Transformer on in-context learning linear functions. (a) Garg et al. [16] consider the class of linear functions \(\mathcal{F}=\{f|f(x)=w^{T}x,w\in\mathbb{R}^{d}\}\), in \(d\) dimensions where \(d=20\). They sample \(x_{1},\dots,x_{k},x_{\text{query}},\) and \(w\) independently from the isotropic Gaussian distribution \(N(0,I_{d})\). They then compute each \(y_{i}=w^{T}x_{i}\) and construct the prompt as \(P=(x_{1},y_{1},x_{2},y_{2},\dots,x_{k},y_{k},x_{\text{query}})\). This figure plots the normalized squared error of the Transformer \((M(P)-w^{T}x_{\text{query}})^{2}/d)\), the errors are normalized so that the trivial zero estimator achieves an error of 1 (dashed line). Besides, when the number of in-context examples reaches the problem dimension d (here 20), least squares achieves 0 error while the Transformer achieves an error of 0.02. (b) We follow the same setting of [16] in (a) to compare the Trained Transformer with different train steps.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model Name & Layer Number & Module Name & Optimal \(\xi^{*}\) & Test Acc Improve \\ \hline GPT-J-6B & 26 & MLP & 0.99 & 0.56884 - **0.57971\((\uparrow)\)** \\ GPT-J-6B & 27 & ATTN & 0 & 0.56884 - 0.56884\((-)\) \\ LLAMA2-7B & 30 & MLP & 0.75 & 0.56159 - 0.57608\((\uparrow)\) \\ LLAMA2-7B & 30 & ATTN & 0.9 & 0.56159 - **0.57971\((\uparrow)\)** \\ \hline \hline \end{tabular}
\end{table}
Table 6: The results of Algorithm 1 on RTE

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model Name & Layer Number & Module Name & Optimal \(\xi^{*}\) & Test Acc Improve \\ \hline GPT-J-6B & 26 & MLP & 0.95 & 0.58181 - **0.67272\((\uparrow)\)** \\ GPT-J-6B & 27 & ATTN & 0 & 0.58181 - 0.58181\((-)\) \\ LLAMA2-7B & 30 & MLP & 0.95 & 0.83636 - 0.85454\((\uparrow)\) \\ LLAMA2-7B & 30 & ATTN & 0.99 & 0.83636 - **0.87272\((\uparrow)\)** \\ \hline \hline \end{tabular}
\end{table}
Table 7: The results of Algorithm 1 on CB

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model Name & Layer Number & Module Name & Optimal \(\xi^{*}\) & Test Acc Improve \\ \hline GPT-J-6B & 26 & MLP & 0.1 & 0.76434 - 0.76947\((\uparrow)\) \\ GPT-J-6B & 27 & ATTN & 0.95 & 0.76434 - **0.77026\((\uparrow)\)** \\ LLAMA2-7B & 30 & MLP & 0.995 & 0.77026 - **0.84881\((\uparrow)\)** \\ LLAMA2-7B & 30 & ATTN & 0.1 & 0.77026 - 0.77039\((\uparrow)\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: The results of Algorithm 1 on AGNEWS

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model Name & Layer Number & Module Name & Optimal \(\xi^{*}\) & Test Acc Improve \\ \hline GPT-J-6B & 26 & MLP & 0.95 & 0.58181 - 0.58181\((-)\) \\ LLAMA2-7B & 30 & MLP & 0.95 & 0.83636 - 0.85454\((\uparrow)\) \\ LLAMA2-7B & 30 & ATTN & 0.99 & 0.83636 - **0.87272\((\uparrow)\)** \\ \hline \hline \end{tabular}
\end{table}
Table 8: The results of Algorithm 1 on COPA

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model Name & Layer Number & Module Name & Optimal \(\xi^{*}\) & Test Acc Improve \\ \hline GPT-J-6B & 26 & MLP & 0.95 & 0.58 - 0.58(\(-\)) \\ LLAMA2-7B & 30 & MLP & 0.99 & 0.58 - 0.58(\(-\)) \\ LLAMA2-7B & 30 & MLP & 0.5 & 0.57 - **0.62**(\(\uparrow)\) \\ LLAMA2-7B & 30 & ATTN & 0.99 & 0.57 - 0.56(\(\downarrow)\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: The results of Algorithm 1 on COPA

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Model Name & Layer Number & Module Name & Optimal \(\xi^{*}\) & Test Acc Improve \\ \hline GPT-J-6B & 26 & MLP & 0.99 & 0.66492 - 0.66492\((-)\) \\ GPT-J-6B & 27 & ATTN & 0.995 & 0.66492 - 0.66492\((-)\) \\ LLAMA2-7B & 30 & MLP & 0.5 & 0.6666 - **0.67536\((\uparrow)\)** \\ LLAMA2-7B & 30 & ATTN & 0.99 & 0.6666 - 0.66608\((\downarrow)\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: The results of Algorithm 1 on MRPC

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper discusses enhancements in in-context learning (ICL) performance for pre-trained large language models (LLMs), particularly focusing on the effects of SVD-based weight pruning. Key contributions made include: What is the phenomenon? Why this phenomenon? How to better use this phenomenon? Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Section 4, we discuss the limitations of our work and identify them as valuable future work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: The full set of assumptions and complete proofs can be found in the Appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: First, our code is available at the anonymous url https://anonymous.4open.science/r/EnhancingICL_SVDPruning-5383/ (camera ready https://github.com/chen123CtrlS/EnhancingICL_SVDPruning). Second, details regarding datasets and how they were used are shown in Appendix C.1 and C.3. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: For details on data and model download, please refer to the code. our code is available at the anonymous url https://anonymous.4open.science/r/EnhancingICL_SVDPruning-5383/(camera ready https://github.com/chen123CtrlS/EnhancingICL_SVDPruning). Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: First, details regarding datasets and how they were used are shown in Appendix C.1 and C.3. Second, how hyperparameters were chosen is also detailed in paper and code. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: The primary focus of our paper is on the theoretical analysis of the underlying mechanism of a phenomenon we discovered. While we have conducted experiments on various publicly available datasets and different models to support our theoretical findings, we did not include discussions on error bars. Our experiments across different datasets and models reflect to some extent the correspondence between theory and practice. And due to limitations in computational resources, we did not report error bars, confidence intervals, or conduct statistical significance tests. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In Section 3.4, we provide sufficient information on the computer resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform with the NeurIPS Code of Ethics in every respect, and we make sure to preserve anonymity. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts**Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our work mainly focuses theoretically analysing the underlying mechanism of a phenomenon that we found. For positive societal impacts, understanding the underlying mechanism can contribute to use ICL. For negative societal impacts, the understanding of the mechanism can not be used in harmful ways. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]Justification: We cite the original papers that produced the code packages or datasets. For example, in Section C.4 we cite the original owner of Figure 5a. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.