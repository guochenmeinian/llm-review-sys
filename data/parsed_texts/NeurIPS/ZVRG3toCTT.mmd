# Beyond Confidence: Reliable Models Should Also Consider Atypicality

Mert Yuksekgonul

Stanford University

merty@stanford.edu

&Linjun Zhang

Rutgers University

lz412@stat.rutgers.edu

&James Zou\({}^{\ddagger}\)

Stanford University

jamesz@stanford.edu

&Carlos Ernesto Guestrin\({}^{\ddagger}\)

Stanford University, CZ Biohub

guestrin@stanford.edu

###### Abstract

While most machine learning models can provide confidence in their predictions, confidence is insufficient to understand a prediction's reliability. For instance, the model may have a low confidence prediction if the input is not well-represented in the training dataset or if the input is inherently ambiguous. In this work, we investigate the relationship between how atypical (rare) a sample or a class is and the reliability of a model's predictions. We first demonstrate that atypicality is strongly related to miscalibration and accuracy. In particular, we empirically show that predictions for atypical inputs or atypical classes are more overconfident and have lower accuracy. Using these insights, we show incorporating atypicality improves uncertainty quantification and model performance for discriminative neural networks and large language models. In a case study, we show that using atypicality improves the performance of a skin lesion classifier across different skin tone groups without having access to the group attributes. Overall, _we propose that models should use not only confidence but also atypicality to improve uncertainty quantification and performance_. Our results demonstrate that simple post-hoc atypicality estimators can provide significant value.1

Footnote 1: Our code is available at https://github.com/mertyg/beyond-confidence-atypicality

## 1 Introduction

_Typicality_ is an item's resemblance to other category members [14]. For example, while a dove and a sparrow are typical birds, a penguin is an atypical bird. Many works from cognitive science (e.g., [13, 12, 14]) suggest that typicality plays a crucial role in category understanding. For instance, humans have been shown to learn, remember, and refer to typical items faster [15]. Similarly, the representativeness heuristic is the tendency of humans to use the typicality of an event as a basis for decisions [16]. This cognitive bias is effective for making swift decisions, but it can lead to poor judgments of uncertainty. For instance, the likelihood of typical events can be overestimated [16] or uncertainty judgments can be inferior for atypical events [16].

While it is hard to quantify the uncertainty of human judgments, machine learning models provide confidence in their predictions. However, confidence alone can be insufficient to understand the reliability of a prediction. For instance, a low-confidence prediction could arise from an ambiguity that is easily communicated, or due to the sample being underrepresented in the training distribution.

Similarly, a high-confidence prediction could be reliable or miscalibrated. Our main proposal is that _models should quantify not only the confidence but also the atypicality_ to understand the reliability of predictions or the coverage of the training distribution. However, many machine learning applications rely on pretrained models that solely provide confidence levels, devoid of any measure of atypicality.

**Contributions:** To support our position, we use a simple formalization of atypicality estimation. With the following studies, we show that by using simple atypicality estimators, we can:

**1. Understand Prediction Quality:** Calibration is a measure that assesses the alignment between predicted probabilities of a model and the true likelihoods of outcomes [1]. Neural networks [1] or even logistic regression [13] can be miscalibrated out-of-the-box. Here, we argue that using atypicality can give insights into when a model's confidence is reliable. Through theoretical analysis and extensive experimentation, we demonstrate that atypicality results in lower-quality predictions. Specifically, _we show that predictions for atypical inputs and samples from atypical classes are more overconfident and have lower accuracy._

**2. Improve Calibration and Accuracy:**_Recalibration_ methods offer some mitigation to miscalibration [1] by adjusting a probabilistic model. We show that models need different adjustments according to the atypicality of inputs and classes, and atypicality is a key factor in recalibration. In light of these findings, we propose a simple method: _Atypicality-Aware Recalibration_. Our recalibration algorithm takes into account the atypicality of the inputs and classes and is simple to implement. We show that complementing recalibration methods with atypicality improves uncertainty quantification and the accuracy of predictors. Further, in a case study for skin lesion classification, we show that atypicality awareness can improve performance across different skin-tone subgroups without access to group annotations.

**3. Improve Prediction sets:** An alternative approach to quantify uncertainty is to provide prediction sets that contain the label with high probability [1]. Here, we investigate existing methods with atypicality and show that prediction sets could underperform for atypical or low-confidence samples. By using atypicality, we demonstrate the potential for improving prediction sets.

Overall, we propose that **models should also consider atypicality, and we show simple- and easy-to-implement atypicality estimators can provide significant value**.

## 2 Interpreting Uncertainty with Atypicality

**Motivation:** In many machine learning applications, we have access to a model's confidence, which aims to quantify the likelihood that a prediction will be accurate. In classification, model output is a probability distribution over classes and confidence is the predicted probability of the top class, i.e. \(\max_{y}\ \hat{\mathbb{P}}(Y=y|X=x)\). In practical scenarios, confidence is the primary tool used to evaluate the reliability of a prediction where higher confidence is associated with better predictions. However, the uncertainty in confidence can stem from different sources that require different treatment [14].

Figure 1: **Atypicality in Uncertainty. Left:** We show examples from the ImageNet-R dataset with our atypicality framework. **Right:** We provide a conceptualization of the quadrants. Using atypicality, we can understand prediction quality (§3), improve predictions (§4), and prediction sets (§5).

Here, we call a prediction _reliable_ if it is high-confidence and well-calibrated. High confidence could be reliable or miscalibrated, and low confidence could be due to ambiguity or rare inputs. We propose that _atypicality_ provides a natural way to understand reliability when combined with confidence. A sample is called typical if it is well-represented in the previously observed samples, e.g., an image of a dog that is similar to other dogs in the training data. However, if the image is unlike any other seen during training, it is atypical. We argue that atypicality can help us interpret a prediction's reliability. Below we categorize samples and predictions according to atypicality and confidence in four quadrants (Figure 1).

**High-confidence and representative:** Reliable predictions often fall within the **Reliable Quadrant**, which includes _typical, high-confidence_ samples. These samples are well-represented in the training dataset (typical), thus we expect the high-confidence prediction to be reliable. For instance, the first image on the top left (Figure 1) is a typical golden retriever and the model makes a reliable prediction.

High-confidence yet far from the support: Having high-confidence does not always indicate reliability. If the sample does not have support in the training distribution, the confidence could be miscalibrated. Such samples lie in the Extrapolation Quadrant which contains _atypical, high-confidence_ samples. For instance, the second image in the top right of Figure 1 is a _toy_ hog and the model has not seen similar ones during training.

Low confidence due to ambiguity: In contrast, low confidence could also be reliable when it correctly reflects an ambiguity. Such samples are in the Ambiguous Quadrant that contains _typical, low-confidence_ samples. These are typical since they may represent multiple classes; yet, due to ambiguity, the model's confidence is low. For instance, the second image in the bottom left of Figure 1 can both be a hog and a comic book.

Low confidence and rare: For samples that are not well-represented in training data, we expect to have low-quality predictions. Untrustworthy Quadrant comprises _atypical, low-confidence_ samples that can include extremely rare subgroups, for which we expect miscalibration and lower accuracy. For example, the image in Figure 1 bottom right is an origami hog that was not seen in training.

These examples suggest that relying solely on confidence does not provide a complete understanding of the reliability of the predictions, and we can use atypicality to interpret and improve reliability.

**Formalizing Atypicality:** Atypicality here is defined with respect to the training distribution. Informally, an input or a class is atypical if it is not _well-represented_ in the training distribution. For instance, if there are no or limited similar examples to an input, it can be called atypical. Note that this notion is not restricted to being 'out-of-distribution' [1], since in-distribution groups could also be atypical or rare, and our goal is to perform reliably for the entire spectrum.

Formally, let \(X\in\mathbb{R}^{d}\) be the random variable denoting features and \(Y\in\mathcal{Y}=\{1,2,...,C\}\) denote the class, where we focus on classification.

**Definition 2.1** (Input Atypicality).: We define the atypicality of the input \(x\) as2

Footnote 2: Here atypicality differs from ‘typical sets’ in information theory that refers to a sequence of variables [13].

\[a_{X}(x)=-\max_{y}\log\mathbb{P}(X=x|Y=y).\]

We use the logarithm of the class-conditional densities due to high dimensionality and density values being close to zero. Intuitively, for a dog image \(x\), if \(\mathbb{P}(X=x|Y=\text{dog})\) has a low value, we call \(x\) an atypical dog image. Overall, if \(a(x)\) is high, then we call \(x\) an atypical input. Specifically, if an input is not typical for any class, then it is atypical with respect to the training distribution. Similarly, we can also use marginal density, \(\mathbb{P}(X=x)\), or distance3 to quantify atypicality.

Footnote 3: For an input \(x\), if the nearest neighbor (NN) distance is large, then we call \(x\) atypical as all inputs in the training set are far from \(x\). Density and distance are connected through non-parametric density estimation and [14] shows that NN distance can recover high-density regions.

Similarly, the notion of atypical (rare) classes is prevalent in imbalanced classification [15, 16]. Ensuring reliable performance for atypical classes can be safety-critical, e.g., for a rare presence of dangerous melanoma [13]. We define class atypicality in the following:

**Definition 2.2** (Class Atypicality).: For a class \(y\), atypicality of a class is defined as

\[a_{Y}(y)=-\log\mathbb{P}(Y=y).\lx@note{footnote}{When the meaning is unambiguous, we omit the subscript to denote $a(X)$ or $a(Y)$ for notational brevity.}\]

[MISSING_PAGE_FAIL:4]

Here, we aim to examine the relationship between model calibration and atypicality. Given any \(K>1\), we consider the quantiles of \(a(X)\), \(a_{1},a_{2},\ldots,a_{K+1}\) such that \(\mathbb{P}(a(X)\in(a_{k},a_{k+1}])=1/K\) for \(k\in[K]\). For imbalanced classification problems, we compute the quantiles using the class atypicality. Specifically, we investigate the atypicality-conditional calibration error \(\text{ECE}[\hat{\mathbb{P}}\mid a(X)\in(a_{k},a_{k+1}]]\), i.e., the expected calibration error of an input that falls within the atypicality quantile \(k\).

**Atypical Examples are Poorly Calibrated:** In Figure 1(a), we show the distribution of miscalibration where each bin within the grid contains the intersection of the corresponding confidence and atypicality quantiles. We observe that within the same confidence range, predictions for atypical points have lower accuracies and are more overconfident. In other words, predictions in the Extrapolation or Untrustworthy regions are more miscalibrated than the ones in the typical regions.

In Figure 1(b), we split inputs into quantiles according to atypicality and compute the ECE and Accuracy for each group. Results show a monotonic relationship between atypicality and ECE or Accuracy across the three settings. Specifically, we see that predictions for atypical inputs or samples from rare classes are more miscalibrated and have lower accuracy. For samples from rare classes, the model overpredicts the probabilities of the typical class, hence we have overconfidence and low accuracy. Appendix C.3, and SS4 present figures and tables for all model and dataset pairs.

### Theoretical Analysis: Characterizing Calibration Error with Atypicality

We characterize how calibration error varies with atypicality in a tractable model that is commonly used in machine learning theory [1, 14, 15, 16]. Our theoretical analysis further supports our empirical findings.

**Data Generative Model:** We consider the well-specified logistic model for binary classification with Gaussian data, where \(Y\in\{-1,1\}\) and the \(\mathbb{P}(Y=1|X)\) is defined by the sigmoid function:

\[\mathbb{P}(Y=1\mid X)=\sigma(\langle\beta^{*},X\rangle),\quad X\sim N(0,I_{d}).\]

Where \(I_{d}\) denotes the \(d\)-dimensional identity matrix, \(\beta^{*}\) is the ground truth coefficient vector, \(\sigma(x)=1/(1+e^{-x})\), and we have \(i.i.d.\) observations \(\{(x_{i},y_{i})\}_{i=1}^{n}\) sampled from the above distribution.

**The Estimator:** We focus on studying the solution produced by minimizing the logistic loss

\[\hat{\beta}=\arg\min_{\beta}\frac{1}{n}\sum_{i=1}^{n}[\log(1+\exp(\beta^{\top }x_{i}))-y_{i}\cdot\beta^{\top}x_{i}].\]

For \(k\in\{-1,1\}\), \(\hat{\mathbb{P}}_{k}(x)\) is an estimator of \(\mathbb{P}(y=k|x)\), with the form \(\hat{\mathbb{P}}_{k}(x)=\frac{1}{e^{-k\cdot\hat{\beta}^{\top}x}+1}\).

**Calibration:** We consider all \(x\) where \(\mathbb{P}_{1}(x)>1/2\), as \(\mathbb{P}_{1}(x)\leq 1/2\) can be analyzed similarly by symmetry (see Appendix G). For \(u\in(1/2,1)\), the signed calibration error at a confidence level \(u\) is

\[u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u).\]

Figure 2: **Atypical Samples Have Low-Quality Predictions.****(a)** Here, samples are grouped according to the Input Atypicality (x-axis) and Confidence (y-axis), to the right meaning more atypical. Values show the difference between confidence and accuracy, lighter color indicates more overconfidence. Within the same confidence range, atypical groups have more miscalibration and are more overconfident. **(b,c,d)** Predictions for atypical samples are less accurate and more miscalibrated in balanced and imbalanced supervised classification and classification with LLMs.

We want to show that when \(X\) is atypical, i.e., when \(a(X):=\|X\|^{2}/2\) is larger5, the accuracy \(\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)\) would be generally smaller than the confidence \(u\) (over-confidence).

Footnote 5: The definition of atypicality follows from the marginal likelihood of the data model: density for the Gaussian with zero mean and identity covariance.

**Theorem 3.1**.: _Consider the data generative model and the learning setting above. For any \(K>1\), suppose we consider the quantiles of \(a(X)\), \(a_{1},a_{2},...,a_{K},a_{K+1}\) such that \(\mathbb{P}(a(X)\in(a_{k},a_{k+1}])=1/K\) for \(k\in[K]\). We assume \(\|\beta^{*}\|\leq c_{0}\), and \(d/n=\kappa\), for some sufficiently small \(c_{0}\). Then, for sufficiently large \(n\), for \(k=2,\ldots,K\), we have_

\[\mathbb{E}_{u\sim\hat{\mathbb{P}}_{1}(X)}[u-\mathbb{P}(Y=1\mid \hat{\mathbb{P}}_{1}(X)=u)\mid a(X)\in(a_{k},a_{k+1}]]>\] \[\mathbb{E}_{u\sim\hat{\mathbb{P}}_{1}(X)}[u-\mathbb{P}(Y=1\mid \hat{\mathbb{P}}_{1}(X)=u)\mid a(X)\in(a_{k-1},a_{k}]]\geq 0.\]

That is, the resulting classifier is over-confident, and the level of over-confidence becomes larger when the data is more atypical (with larger \(a(X)\)). Further, the gap becomes larger for smaller sample sizes \(n\). The proof of the theorem is in Appendix G.2 and builds on the results from [1, 1].

## 4 Using Atypicality to Improve Recalibration

Here, we show how atypicality can complement and improve post-hoc calibration. In SS2, we observed that predictions for atypical inputs and samples from atypical classes are more overconfident with lower accuracy. We next show that taking input and class atypicality into account improves calibration.

### Parametric Recalibration: Different Groups Need Different Temperatures

Temperature scaling (TS), a single parameter variant of Platt Scaling [11, 12], is a simple recalibration method that calibrates the model using a single parameter. The predictor is of the form

\[\log\hat{\mathbb{P}}_{\text{TS}}(Y|X)\propto\log\hat{\mathbb{P}}(Y|X)/\tau,\] (1)

where \(\hat{\mathbb{P}}(Y|X)\) is the model that takes an input and outputs scores/logits, and \(\tau\) is the temperature parameter. In practice, \(\tau\) is optimized using a calibration set to minimize a proper scoring rule [1, 1] such as the cross-entropy loss.

To understand the behavior of TS with respect to atypicality, we separately perform TS on points grouped according to the atypicality quantiles. Let us denote the temperature fitted to the quantile covering \(a(X)\in(a_{k-1},a_{k}]\) by \(\tau_{a_{k}}\). In Appendix Figure 10 we observe an increasing relationship between \(a_{k}\) and \(\tau_{a_{k}}\). Different atypicality groups need different adjustments, and more atypical groups need larger temperatures. _This suggests that being atypicality-aware can improve calibration. While a single temperature value improves average calibration, it may hurt certain groups._

### Atypicality-Aware Recalibration

We showed that predictions are more reliable when the input is typical. However, predictions are less reliable for atypical inputs, and we may need further revision. An analogy can be drawn to decision-making literature where opinions of individuals are combined with geometric averaging weighted by their expertise [1, 1]. Analogously, we propose _Atypicality-Aware Recalibration (AAR)_ a method designed to address the reliability issues identified in dealing with atypical inputs:

\[\hat{\mathbb{P}}_{\text{AAR}}(Y|X)=\frac{\hat{\mathbb{P}}(Y|X)^{\psi(a(X))} \exp(S_{Y})^{1-\psi(a(X))}}{Z(X)},\] (2)

where \(\psi(a(X))\) is a function of input atypicality, \(S_{Y}\) is a tunable score for class \(Y\), \(Z(X)\) is the normalization term. Intuitively, when the input is typical, we trust the model confidence; otherwise, we use a score for the given class estimated from the calibration set. Note that this form simplifies to

\[\log\hat{\mathbb{P}}_{\text{AAR}}(Y|X)\propto\phi(a(X))\log\hat{\mathbb{P}}(Y| X)+S_{Y},\] (3)where we subsume \((1-\psi(a(X))\) into \(\phi(a(X))\). We give a simple interpretation of this form: the multiplicative term is an atypicality-dependent temperature, and the additive term is a class-dependent correction where \(\exp{(S_{Y})}\) can be considered to induce a correction distribution over classes estimated from the calibration set.

Intuitively, when \(\psi(a(X))=0\), the output reduces to a fixed distribution over classes that was estimated using the calibration set. This distribution can be seen to induce a prior probability over classes, and \(\psi\) controls the tradeoff between this prior and the model's predictive distribution. As the point becomes more typical, this distribution is closer to the model's predictive distribution. In Appendix Figure 11, we show how these values behave with class atypicality. We find that rare classes require larger positive corrections with larger \(S_{Y}\).

**Implementation Details:** Following TS, we minimize the cross-entropy loss on a calibration set. With the temperature-atypicality relationship observed in Figure 10 we choose to instantiate the multiplicative factor as a quadratic function, where \(\phi(a(X))=c_{2}a(X)^{2}+c_{1}a(X)+c_{0}\) and in total we have \(|\{S_{1},..,S_{|\mathcal{Y}|},c_{0},c_{1},c_{2}\}|=|\mathcal{Y}|+3\) interpretable parameters. Once the embeddings and logits are computed, AAR runs on a CPU in under 1 minute for all experimented settings.

Similar to our adaptive interpretation, a concurrent work, Adaptive Temperature Scaling (AdaTS) [12], uses temperature scaling where the temperature is parameterized by a Variational Autoencoder(VAE) [13] and a multi-layer perceptron on top of the VAE embeddings. In the below experiments, we give results with AdaTS as a baseline when applicable.

Figure 3: **Post-hoc Recalibration for Classification.****(a) Balanced Supervised Classification:** Atypicality-Aware Recalibration improves the calibration of models trained with balanced datasets, across atypicality groups. **(b) Imbalanced Supervised Classification:** Atypicality-Aware Recalibration improves both the calibration across groups and the overall accuracy of models trained with imbalanced datasets. **(c) Classification with LLMs:** Atypicality-Aware Recalibration improves both the calibration across groups and the overall accuracy of LLMs performing classification.

For **Balanced Supervised Classification**, in Figure 2(a), we observe that being atypicality aware improves recalibration across all groups. We perform comparably to AdaTS, where the temperature function has in the order of millions of parameters, whereas AAR has only \(|\mathcal{Y}|+3\) parameters.

In **Imbalanced Supervised Classification** (Figure 2(b)), our algorithm not only provides better calibration rates across all classes but also improves overall accuracy. Note that only our method can change accuracy (due to the additive term), and it performs better than other baselines in terms of ECE across all classes. Further, the second column shows using Progressive Balancing [20] in training, showing that our post-hoc method can complement methods that modify training procedures.

For **Classification with LLMs**, we add an LLM calibration baseline Content-Free Calibration (CF) [20]. We cannot use AdaTS as the embeddings are not fixed in size. In Figure 2(c), we see AAR has better calibration and accuracy across the three datasets. Namely, by adjusting the LLM output using the LLM atypicality, we can adjust the probabilities to increase the prediction quality.

### Case Study: Fairness through Atypicality-Awareness

Machine learning models reportedly have performance disparity across subgroups [1] due to factors such as varying sample size or noise levels [14]. For instance, skin lesion classifiers can exhibit performance disparity across different skin tones [13]. Fitzpatrick17k [15] is a dataset of clinical images with Fitzpatrick skin tone annotations between 1-to-6, where a larger number means darker skin tones, and when annotators do not agree, it is labeled as 'Unknown'. We explore the classification problem with 9 classes indicating the malignancy and the type of skin condition, using a ResNet18/34 pretrained on ImageNet and finetuned on this task (See Appendix F).

When the goal is to improve performance across groups, one can use group annotations and optimize performance within each group [16, 17]. Here, we investigate how complementing recalibration with atypicality can improve prediction quality across all groups _without group annotations_. For comparison, we perform 3 recalibration methods: TS, AAR, and Skin-Tone Conditional TS which calibrates the model individually for each skin-tone group with TS. Since the skin-tone conditional calibration uses group attributes, ideally it should act as an oracle.

In Figure 4, we give the Accuracy and ECE analyses where AAR improves performance across all groups. For instance, the worst-group Accuracy (0.69) or ECE (0.072) with AAR is close to the best-group Accuracy (0.63) or ECE (0.062) with the other two methods. Overall, _our findings suggest that Atypicality-Awareness can complement fairness-enforcing methods, and improve performance even when the group annotations are unavailable_. We hypothesize that with AAR, we can perform better than using supervised group attributes since groups may not have sufficient sample size in the calibration set (131, 1950, 1509, 555 samples for Unknown, 1&2, 3&4, and 5&6 respectively), and we can leverage atypicality to offer some mitigation. Further investigating how to leverage atypicality to improve fairness and factors affecting performance disparities is a promising direction for future work [14].

## 5 Improving Prediction Sets with Atypicality

**Conformal Prediction**[21, 1] is a framework that assigns a calibrated prediction set to each instance. The goal is to find a function \(\mathcal{C}:\mathcal{X}\to 2^{\mathcal{Y}}\) that returns a subset of the label space such that \(Y\in\mathcal{C}(X)\) with high probability. The framework aims to guarantee _marginal coverage_, i.e., \(\mathbb{P}(Y\in\mathcal{C}(X))\geq 1-\alpha\), for a choice of \(\alpha\). We investigate two conformal calibration methods,

Figure 4: **Improving Group Performance through Atypicality-Awareness. Here we show that AAR improves the calibration and accuracy of models across different skin tone groups. With AAR, we can improve both the worst group performance and overall performance significantly without using group attributes. TS curve is less visible since it significantly overlaps with Skin Tone Conditional.**

Adaptive Prediction Sets (APS) [14] and Regularized APS (RAPS) [1]. Let \(\pi(X)\) be the permutation of the label set that sorts \(\hat{\mathbb{P}}(Y=c|X)\), i.e. the predicted probabilities for each class \(c\) after TS. The prediction sets are produced by the function \(\mathcal{C}(x)=\{y:s(x,y)\leq\hat{q}\}\), and these methods fit the threshold \(\hat{q}\) for a choice of the scoring function. APS uses the cumulative sum of the predicted probabilities \(s(x,y)=\sum_{j=1}^{c}\hat{\mathbb{P}}(Y=j|X)\), where \(y=\pi_{c}(X)\). Intuitively, if the model was perfectly calibrated, we would have expected to have \(\hat{q}=1-\alpha\). Similarly, RAPS builds on the idea that tail probabilities are noisy and regularizes the number of samples in the prediction set.

Building on our ideas in the previous sections we implement Atypicality-Aware prediction sets, namely _AA-APS_ and _AA-RAPS_ in the following way: We first group points according to their confidence and atypicality quantiles. A group \(G\) here is defined using 4 thresholds, namely \(G=x:(l_{a}^{(G)}<q_{a}(x)\leq h_{a}^{(G)})\wedge(l_{c}^{(G)}<q_{c}(x)\leq h_{ c}^{(G)})\) where \(q_{a}(x)\) and \(q_{c}(x)\) denote the atypicality and confidence quantiles for the sample \(x\), \(l_{a}^{(G)}\) and \(h_{a}^{(G)}\) denote the atypicality lower and upper bounds for group \(G\), and \(l_{c}^{(G)}\) and \(h_{c}^{(G)}\) denote the confidence lower and upper bounds for group \(G\). Using a calibration set, these bounds are simply determined by the quantiles of confidence and atypicality statistics. Then, we fit separate thresholds \(\hat{q}_{G}\) for each group's prediction sets with APS or RAPS as subroutines. This allows us to have an adaptive threshold depending only on the atypicality and confidence of predictions.

In Figure 5, we provide the coverage plots for APS and RAPS in the first and third columns. Even though marginal coverage is satisfied, models do not satisfy conditional coverage for atypical inputs or low-confidence predictions. We observe that being Atypicality-Aware improves coverage across otherwise underperforming groups. Further, AA-APS has lower set sizes on average than APS (\(15.6\) vs \(21.3\)). While RAPS has a lower average set size than AA-RAPS (\(4.2\) vs \(9.1\)) AA-RAPS has smaller set sizes for high-confidence samples, whereas a larger set size for low-confidence samples where the coverage is not met for RAPS. In Appendix D.3, we provide the same analysis for ResNet18,50,152 at different coverage levels along with analyzing the performance in the Confidence and Atypicality dimensions individually. For instance in Figure 8, we observe that RAPS and APS do not satisfy coverage for high atypicality regions, even when averaged across different confidence levels.

## 6 Additional Related Work

**Uncertainty and Atypicality:**[12, 13] use density estimation to disentangle epistemic and aleatoric uncertainty. Following this, they show improvements in active learning and OOD detection [11]. We note that our goal is not this disentanglement (e.g. Untrustworthy quadrant can have both aleatoric or epistemic uncertainty), or Ambiguity could be due to a lack of features or noise. [15] propose the related notion of distance awareness, and that it leads to better uncertainty quantification. They offer architecture and training modifications whereas we analyze existing models using our framework including imbalanced and LLM settings, and propose simple and post-hoc approaches. 'OOD' [15] or 'anomaly' [13] notions are tied to atypicality, yet our goal is not to make a binary distinction between 'in' or 'out'. We argue that in-distribution samples could also be atypical (e.g. rare groups), and the goal is to perform reliably in the entire spectrum. Other works with an atypicality notion include bounding calibration of groups by the

Figure 5: **Improving Conformal Calibration with Atypicality for ResNet50 on ImageNet. Here we show that atypicality awareness improves conformal calibration performance across different groups. Methods are fitted to satisfy \(95\%\) coverage. We observe that APS and RAPS do not satisfy conditional coverage for high atypicality regions or low confidence regions.**

excess risk [11], miscalibration under distribution shifts [10], uncertainty in Gaussian Processes [12], forgetting time for rare examples [13], the poor performance of groups with lower sample sizes [14], energy-based models improving calibration [15], relating perplexity to zero-shot classification performance for LLMs [16], grouping loss and local definitions of miscalibration [17], the relationship between active learning and atypicality [14], sample size as a factor for subgroup performance disparity [13]. [20] provide insightful discussion around the nature of softmax confidence, and here we show that its reliability depends on the atypicality of the input. Our new findings include showing that predictions for atypical samples are more miscalibrated and overconfident, and atypicality awareness improves prediction quality. _Overall, while there are other relevant notions in the literature, our distinct goal is to show that post-hoc atypicality estimation and recalibration is a simple yet useful framework to understand and improve uncertainty quantification that complements existing methods._

**Recalibration and Conformal Prediction:** There is a rich literature on recalibration methods and prediction sets: TS [17], Platt Scaling [21], conformal calibration [22, 1] among many. [10, 11, 15] make a relevant observation, showing that the coverage of conformal prediction is not equal across all groups. They propose group conformal calibration, which requires group labels whereas our proposal is unsupervised and does not depend on any attribute information. Concurrent work [17] explores AdaTS, where they train a separate VAE and MLP to produce an adaptive temperature. However, our parameterization of temperature has 3 parameters and is interpretable.

## 7 Conclusion

Atypicality offers a simple yet flexible framework to better understand and improve model reliability and uncertainty. We propose that pretrained models should be released not only with confidence but also with an atypicality estimator. While there are other relevant notions in the literature, our main goal is to show that atypicality can provide a unifying perspective to discuss uncertainty, understand individual data points, and improve fairness. Here we focus on classification problems; it would be interesting to extend atypicality to regression and generation settings. Furthermore, we would like to extend the theoretical analysis to more general settings, as our empirical results demonstrate that the observed phenomena hold more broadly.

## Acknowledgments

We would like to thank Adarsh Jeewajee, Bryan He, Edward Chen, Federico Bianchi, Kyle Swanson, Natalie Dullerud, Ransalu Senanayake, Sabri Eyuboglu, Shirley Wu, Weixin Liang, Xuechen Li, Yongchan Kwon, Yu Sun, and Zach Izzo for their comments and suggestions on the manuscript. Linjun Zhang's research is partially supported by NSF DMS-2015378. Carlos Ernesto Guestrin is a Chan Zuckerberg Biohub - San Francisco Investigator.

## References

* [AB21] Anastasios N Angelopoulos and Stephen Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. _arXiv preprint arXiv:2107.07511_, 2021.
* [ABMJ20] Anastasios Angelopoulos, Stephen Bates, Jitendra Malik, and Michael I Jordan. Uncertainty sets for image classifiers using conformal prediction. _arXiv preprint arXiv:2009.14193_, 2020.
* [AR89] Janos Aczel and Fred S Roberts. On the possible merging functions. _Mathematical Social Sciences_, 17(3):205-243, 1989.
* [BGJ\({}^{+}\)22] Osbert Bastani, Varun Gupta, Christopher Jung, Georgy Noarov, Ramya Ramalingam, and Aaron Roth. Practical adversarial multivalid conformal prediction. _arXiv preprint arXiv:2206.01067_, 2022.
* [BHN17] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness in machine learning. _Nips tutorial_, 1:2, 2017.
** [BMR\({}^{+}\)20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [BMWX21a] Yu Bai, Song Mei, Huan Wang, and Caiming Xiong. Don't just blame over-parametrization for over-confidence: Theoretical analysis of calibration in binary classification. In _International Conference on Machine Learning_, pages 566-576. PMLR, 2021.
* [BMWX21b] Yu Bai, Song Mei, Huan Wang, and Caiming Xiong. Understanding the under-coverage bias in uncertainty estimation. _Advances in Neural Information Processing Systems_, 34:18307-18319, 2021.
* [BW19] David Bolin and Jonas Wallin. Local scale invariance and robustness of proper scoring rules. _arXiv preprint arXiv:1912.05642_, 2019.
* [BYR\({}^{+}\)21] Noam Barda, Gal Yona, Guy N Rothblum, Philip Greenland, Morton Leibowitz, Ran Balicer, Eitan Bachmat, and Noa Dagan. Addressing bias in prediction models by improving subpopulation calibration. _Journal of the American Medical Informatics Association_, 28(3):549-558, 2021.
* [CJS18] Irene Chen, Fredrik D Johansson, and David Sontag. Why is my classifier discriminatory? _Advances in neural information processing systems_, 31, 2018.
* [CLK22] Lucas Clarke, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Theoretical characterization of uncertainty in high-dimensional linear classification. _arXiv preprint arXiv:2202.03295_, 2022.
* [CWG\({}^{+}\)19] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. _Advances in neural information processing systems_, 32, 2019.
* [DCLT19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _ArXiv_, abs/1810.04805, 2019.
* [DDS\({}^{+}\)09] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [DVN\({}^{+}\)22] Roxana Daneshjou, Kailas Vodrahalli, Roberto A Novoa, Melissa Jenkins, Weixin Liang, Veronica Rotemberg, Justin Ko, Susan M Swetter, Elizabeth E Bailey, Olivier Gevaert, et al. Disparities in dermatology ai performance on a diverse, curated clinical image set. _Science advances_, 8(31):eabq6147, 2022.
* [FP98] Ernest Forman and Kirti Peniwati. Aggregating individual judgments and priorities with the analytic hierarchy process. _European journal of operational research_, 108(1):165-169, 1998.
* [GHS\({}^{+}\)21] Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel Han, Aerin Kim, Arash Koochek, and Omar Badri. Evaluating deep neural networks trained on clinical images in dermatology with the fitztpatrick 17k dataset. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1820-1828, 2021.
* [GIB\({}^{+}\)22] Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. Demystifying prompts in language models via perplexity estimation. _arXiv preprint arXiv:2212.04037_, 2022.
* [GPSW17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International conference on machine learning_, pages 1321-1330. PMLR, 2017.

* [GR07] Tilmann Gneiting and Adrian E Raftery. Strictly proper scoring rules, prediction, and estimation. _Journal of the American statistical Association_, 102(477):359-378, 2007.
* [GWJ\({}^{+}\)20] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like one. In _International Conference on Learning Representations_, 2020.
* [HDW22] Guy Hacohen, Avihu Dekel, and Daphna Weinshall. Active learning on a budget: Opposite strategies suit high and low budgets. _arXiv preprint arXiv:2202.02794_, 2022.
* [HG16a] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. _arXiv preprint arXiv:1610.02136_, 2016.
* [HG16b] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. _ArXiv_, abs/1610.02136, 2016.
* [HJKRR18] Ursula Hebert-Johnson, Michael Kim, Omer Reingold, and Guy Rothblum. Multicalibration: Calibration for the (computationally-identifiable) masses. In _International Conference on Machine Learning_, pages 1939-1948. PMLR, 2018.
* [HMD18] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier exposure. _arXiv preprint arXiv:1812.04606_, 2018.
* [HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [JKGG18] Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta. To trust or not to trust a classifier. _Advances in neural information processing systems_, 31, 2018.
* [JPL\({}^{+}\)23] Tom Joy, Francesco Pinto, Ser-Nam Lim, Philip HS Torr, and Puneet K Dokania. Sample-dependent adaptive temperature scaling for improved calibration. _AAAI_, 2023.
* [KGZ19] Michael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for fairness in classification. In _Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society_, pages 247-254, 2019.
* [Kri09] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
* [Kum22] Sawan Kumar. Answer-level calibration for free-form multiple choice question answering. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 665-679, 2022.
* [KW13] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [LLC\({}^{+}\)22] Charles Lu, Andreanne Lemay, Ken Chang, Katharina Hobel, and Jayashree Kalpathy-Cramer. Fair conformal predictors for applications in medical imaging. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 12008-12016, 2022.
* [LLLS18] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. _Advances in neural information processing systems_, 31, 2018.
* [LLP\({}^{+}\)20] Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshminarayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. _Advances in Neural Information Processing Systems_, 33:7498-7512, 2020.

* [LN89] Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization. _Mathematical programming_, 45(1):503-528, 1989.
* [LOG\({}^{+}\)19] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [LR02] Xin Li and Dan Roth. Learning question classifiers. In _COLING 2002: The 19th International Conference on Computational Linguistics_, 2002.
* [LSH19] Lydia T Liu, Max Simchowitz, and Moritz Hardt. The implicit fairness criterion of unconstrained learning. In _International Conference on Machine Learning_, pages 4051-4060. PMLR, 2019.
* [LVdM\({}^{+}\)21] Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Sasko, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clement Delangue, Theo Matussiere, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Francois Lagunas, Alexander Rush, and Thomas Wolf. Datasets: A community library for natural language processing. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_. Association for Computational Linguistics, 2021.
* [MDP\({}^{+}\)11] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, pages 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics.
* [MGLK22] Pratyush Maini, Saurabh Garg, Zachary C Lipton, and J Zico Kolter. Characterizing datapoints via second-split forgetting. _arXiv preprint arXiv:2210.15031_, 2022.
* [MKS\({}^{+}\)20] Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip HS Torr, and Puneet K Dokania. Calibrating deep neural networks using focal loss. 2020.
* [MKvA\({}^{+}\)21] Jishnu Mukhoti, Andreas Kirsch, Joost van Amersfoort, Philip HS Torr, and Yarin Gal. Deep deterministic uncertainty: A simple baseline. _arXiv e-prints_, pages arXiv-2102, 2021.
* [MP80] Carolyn B Mervis and John R Pani. Acquisition of basic object categories. _Cognitive Psychology_, 12(4):496-522, 1980.
* [MR10] Sebastien Marcel and Yann Rodriguez. Torchvision the machine-vision package of torch. In _Proceedings of the 18th ACM international conference on Multimedia_, pages 1485-1488, 2010.
* [Mur04] Gregory Murphy. _The big book of concepts_. MIT press, 2004.
* [NCH15] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In _Twenty-Ninth AAAI Conference on Artificial Intelligence_, 2015.
* [OFR\({}^{+}\)19] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. _Advances in neural information processing systems_, 32, 2019.
* [P\({}^{+}\)99] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. _Advances in large margin classifiers_, 10(3):61-74, 1999.

* [PBC\({}^{+}\)20] Janis Postels, Hermann Blum, Cesar Cadena, Roland Siegwart, Luc Van Gool, and Federico Tombari. Quantifying aleatoric and epistemic uncertainty using density estimation in latent space. _arXiv preprint arXiv:2012.03082_, 2020.
* [PBZ21] Tim Pearce, Alexandra Brintrup, and Jun Zhu. Understanding softmax confidence and uncertainty. _arXiv preprint arXiv:2106.04972_, 2021.
* [PLMV23] Alexandre Perez-Lebel, Marine Le Morvan, and Gael Varoquaux. Beyond calibration: estimating the grouping loss of modern neural networks. In _The Eleventh International Conference on Learning Representations_, 2023.
* [Ras04] Carl Edward Rasmussen. Gaussian processes in machine learning. In _Summer school on machine learning_, pages 63-71. Springer, 2004.
* [RBSC19] Yaniv Romano, Rina Foygel Barber, Chiara Sabatti, and Emmanuel J Candes. With malice towards none: Assessing uncertainty via equalized coverage. _arXiv preprint arXiv:1908.05428_, 2019.
* [Rip89] Lance J. Rips. _Similarity, typicality, and categorization_, page 21-59. Cambridge University Press, 1989.
* [RM75] Eleanor Rosch and Carolyn B Mervis. Family resemblances: Studies in the internal structure of categories. _Cognitive psychology_, 7(4):573-605, 1975.
* [RSC20] Yaniv Romano, Matteo Sesia, and Emmanuel Candes. Classification with valid and adaptive coverage. _Advances in Neural Information Processing Systems_, 33:3581-3591, 2020.
* [RSS73] Lance J Rips, Edward J Shoben, and Edward E Smith. Semantic distance and the verification of semantic relations. _Journal of verbal learning and verbal behavior_, 12(1):1-20, 1973.
* [SC19] Pragya Sur and Emmanuel J Candes. A modern maximum-likelihood theory for high-dimensional logistic regression. _Proceedings of the National Academy of Sciences_, 116(29):14516-14525, 2019.
* [SV08] Glenn Shafer and Vladimir Vovk. A tutorial on conformal prediction. _Journal of Machine Learning Research_, 9(3), 2008.
* [TGZ\({}^{+}\)23] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.
* [TJ06] MTCAJ Thomas and A Thomas Joy. _Elements of information theory_. Wiley-Interscience, 2006.
* [TK74] Amos Tversky and Daniel Kahneman. Judgment under uncertainty: Heuristics and biases: Biases in judgments reveal some heuristics of thinking under uncertainty. _science_, 185(4157):1124-1131, 1974.
* [TK92] Amos Tversky and Daniel Kahneman. Advances in prospect theory: Cumulative representation of uncertainty. _Journal of Risk and uncertainty_, 5(4):297-323, 1992.
* [VGS05] Vladimir Vovk, Alexander Gammerman, and Glenn Shafer. _Algorithmic learning in a random world_. Springer Science & Business Media, 2005.
* [WDS\({}^{+}\)20] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_. Association for Computational Linguistics, 2020.

* [WNB18] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_. Association for Computational Linguistics, 2018.
* [ZCLJ21] Zhisheng Zhong, Jiequan Cui, Shu Liu, and Jiaya Jia. Improving calibration for long-tailed recognition. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16489-16498, 2021.
* [ZDKZ22] Linjun Zhang, Zhun Deng, Kenji Kawaguchi, and James Zou. When and how mixup improves calibration. In _International Conference on Machine Learning_, pages 26135-26160. PMLR, 2022.
* [ZK16] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. _arXiv preprint arXiv:1605.07146_, 2016.
* [ZKL\({}^{+}\)16] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Antonio Torralba, and Aude Oliva. Places: An image database for deep scene understanding. _arXiv preprint arXiv:1610.02055_, 2016.
* [ZWF\({}^{+}\)21] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In _International Conference on Machine Learning_, pages 12697-12706. PMLR, 2021.
* [ZZL15] Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In _NIPS_, 2015.

## Appendix A Appendixity Estimation

### Input Atypicality Estimation

To estimate input atypicality, we use two ways to estimate the likelihood of a point under the training distribution. First, we give methods for the discriminative models.

Fitting individual Gaussians to class conditionalsHere, we follow a similar approach to [13]. Namely, we model the class conditionals with a Gaussian distribution, where the covariance matrix is tied across classes:

\[\hat{\mathbb{P}}(X|Y=y)\sim N(X;\mu_{y},\Sigma)\] (4)

We fit the parameters \(\mu_{y}\) and \(\Sigma\) with maximum likelihood estimation. The reason to tie the covariance matrix is due to the number of samples required to fit the density. Namely, for a \(d\)-dimensional problem, the total number of parameters to fit individual matrices becomes \(O(yd^{2})\), which results in low-quality estimates. Then, the atypicality becomes

\[a_{X}(x)=-\max_{y\in\mathcal{Y}}\log\hat{\mathbb{P}}(X=x|Y=y)\] (5)

Computing distance with k-Nearest Neighborsk-Nearest NeighborsSimilarly, we can use the nearest neighbor distance. Concretely, we use the nearest neighbor distance, \(a_{X}(x)=d_{\min}(x,\mathcal{D}_{\text{train}})=\min_{x^{\prime}\in\mathcal{D }_{\text{train}}}\left|x^{\prime}-x\right|\), as the atypicality metric. Alternatively, we can use different notions such as the average of k-nearest neighbors, or the distance to the kth neighbor. Below, we report the results by using the average distance to 5-nearest neighbors.

Fitting the estimatorsFor all of the atypicality estimators, we fit the estimators using samples from the training sets and make inferences for the calibration and test sets. For instance, we use the training split of ImageNet to fit the corresponding density estimator and compute the atypicality for the samples from the validation/test split of ImageNet. All of our results using atypicality are reported for the test splits of the below datasets.

Atypicality Estimation with LLMs:For language models we simply compute the negative log-likelihood of each prompt as the atypicality metric: \(a_{X}(x)=-\log\hat{\mathbb{P}}_{\text{LLM}}(x)\). To define confidence, we use the logits of the language model, conditioned on the prompt. We use the logit of the first token of each class label and compute the predicted probabilities by applying softmax to the logits of each class.

### Class Atypicality

To estimate class atypicality, we simply count the fraction of examples from a particular label in the training dataset. Let us have a training dataset \(\mathcal{D}_{\text{train}}=\{(x_{1},y_{1}),(x_{2},y_{2}),\dots,(x_{N},y_{N})\}\). Then, we estimate class atypicality with

\[a_{Y}(y)=-\log\frac{\sum_{i\in[N]}\mathbf{1}[y_{i}=y]}{N}.\] (6)

### Atypicality and Confidence

Are atypicality and confidence equally informative? Beyond the data perspective given in Figure 1, here we provide quantitative results to demonstrate the difference. In Figure 6, we give a grid plot where the x-axis indicates the typicality quantile of a point, and the y-axis indicates the confidence of a point. The coloring on the left indicates the accuracy within a bin split according to accuracy, and the right has the difference between average confidence and accuracy. Observe that for a specific confidence interval, larger values of typicality mean better quality probabilistic estimates and larger atypicality means more miscalibration.

## Appendix B Experimental Details

### Balanced Supervised Classification

#### b.1.1 Datasets

Below is a full list of datasets for balanced classification:

1. **ImageNet**[10] from Torchvision [14] is an object recognition dataset with 1000 classes. We use the ImageNet-1k version.
2. **CIFAR10/100**[11] from Torchvision [14] are object recognition datasets with 10/100 classes.
3. **MNLI**[12] from Huggingface Datasets [13] is a natural language inference dataset with 3 classes, indicating entailment, neutral, and contradiction outcomes.

#### b.1.2 Models

Most of the models are public models, e.g., obtained from the Transformers Library [15] or Torchvision [14]. Below we give the full model details and how one can access them:

1. **RoBERTa**(HuggingFace roberta-large-mnli) trained on the MNLI dataset. One can use the id given here on HuggingFace to download the model.
2. **ResNet18, ResNet50, ResNet152** from (Torchvision[14]) trained on ImageNet.
3. **WideResNet28** trained on CIFAR10,100 obtained from [13].

For all BERT [10] style models we use the [CLS] token embeddings in the final layer to perform classification. For all vision models, we use the penultimate layer embeddings to fit the density estimators and perform the analyses. In the experiments, we randomly split the test sets into two equal halves to have a calibration split and a test split, and repeat the experiments over 10 random seeds.

Figure 6: **Input Atypicality and Confidence. Here, the x-axis reflects the input atypicality quantile, and the y-axis indicates confidence. The coloring for the figure on the left indicates the accuracy within a bin, and the figure on the right has the difference between confidence and accuracy within a bin. We observe that even within the same confidence range, atypical examples tend to be more miscalibrated and overconfident compared to typical examples.**

### Imbalanced Supervised Classification

#### b.2.1 Datasets

All of our imbalanced classification datasets are previous benchmarks obtained from the GitHub repository6 of [2] with corresponding training, validation, and test splits. All of these datasets have an exponential class imbalance.

Footnote 6: https://github.com/dvlab-research/MiSLAS

1. **ImageNet-LT** is the long-tailed variant of ImageNet with 1000 classes.
2. **CIFAR10/100-LT** is the long-tailed variant of CIFAR10/100 with 10/100 classes.
3. **Places365-LT** is the long-tailed variant of Places365 [2] with 365 classes.

#### b.2.2 Models

Similarly, most of these models are obtained from [2].

1. **ResNeXt50** trained on ImageNet-LT with and without Progressive Balancing, which is a strategy to address class imbalance during training.
2. **ResNet152** trained on Places365-LT
3. **ResNet18** trained on CIFAR100-LT trained by us. This model is pretrained on ImageNet and finetuned on CIFAR100-LT.

We use the validation splits of these datasets as the calibration set, and report the results on the test set.

### Classification with LLMs

#### b.3.1 Model

We use Alpaca-7B [16] in a zero-shot setting, where we simply prompt the model with the classification question. We use the prompting strategy from Content-Free Calibration [2].

Below, we show examples of each dataset and prompt.

#### b.3.2 Datasets

**IMDB** is a binary classification dataset of movie reviews, where the goal is to classify the sentiment in a review. The example prompt has the form 'The following review was written for a movie: [Review].\(\backslash\)n What is the sentiment, Positive or Negative? Answer: '. Below is an example:

The following review was written for a movie: I and a friend rented this movie. We both found the movie soundtrack and production techniques to be lagging. The movie's plot appeared to drag on throughout with little surprise in the ending. We both agreed that the movie could have been compressed into roughly an hour giving it more suspense and moving plot.

What is the sentiment, Positive or Negative? Answer:

where the correct answer should be 'Negative'. We filter out the examples that exceed the context length limit of Alpaca7B (\(512\)). We noticed that the 'validation' split of IMDB leads to significantly worse calibration compared to splitting the test set. Thus, for all experiments, we use the test split of IMDB and split it into 2 sets (instead of using the validation split as a calibration set as in the other two datasets).

**TREC** is a 6-class question classification dataset where the goal is to predict whether a question will have an answer that is an 'Abbreviation', 'Entity', 'Description', 'Human', 'Location', or a 'Number'. We format the prompts with 'Classify the questions based on their Answer Type. Potential Answer Types are: Number, Location, Person, Description, Entity, or Abbreviation.\(\backslash\)n\(\backslash\)nQuestion: [question]\(\backslash\)n\(\backslash\)nAnswer Type: '. Below is an example prompt:Classify the questions based on their Answer Type. Potential Answer Types are: Number, Location, Person, Description, Entity, or Abbreviation.

Question: What county is Modesto, California in?

Answer Type:

where the correct answer should be 'Location'.

**AG News** is a news classification dataset. The goal is to classify a given news into 4 potential classes: 'World', 'Sports', 'Business', or 'Science and Technology'. We format the prompts with 'Classify the news articles into the categories of World, Sports, Business, and Technology.\(\backslash\)n\(\backslash\)n Article: [article]\(\backslash\)nAnswer: '. Below is an example prompt:

Classify the news articles into the categories of World, Sports, Business, and Technology.

Article: Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling band of ultra-cynics, are seeing green again.

Answer:

where the correct answer should be 'Business'.

Furthermore, we also use [20] as another calibration baseline. Following their paper, we use N/A, [MASK], and the empty string as content-free. Concretely, we follow their paper to first obtain the average predicted probabilities for each label token for the content-free input, denoted by \(p_{cf}\). We then let

\[W=\text{diag}(p_{cf})^{-1}.\]

When making test-time predictions, we compute \(\text{Softmax}(W^{T}p)\) as the new predicted probabilities. In our experiments, we observe that it does not perform as well in this setting, as was previously suggested by [15].

## Appendix C Calibration

We run all our experiments with 10 different random seeds, where the seeds are \(\{0,1,2,\dots,9\}\). Randomness is over fitting the atypicality estimators, and calibration-test splits (we use the same splits with the recalibration experiments for the sake of consistency).

### Expected Calibration Error

To compute ECE, we generate \(\mathbb{B}=\{B_{1},B_{2},...,B_{M}\}\), \(M\) equally-spaced bins where samples are sorted and grouped according to their confidence. \(B_{m}\) here denotes the set of the data indices where the confidence of the prediction for the sample falls into the interval \((\frac{m-1}{M},\frac{m}{M}]\). We compute ECEby

\[\text{ECE}[\hat{\mathbb{P}}]=\sum_{m=1}^{M}\frac{|B_{m}|}{N}|\text{acc}(B_{m} )-\text{conf}(B_{m})|,\] (7)

where \(\text{acc}(B_{m})=\frac{1}{|B_{m}|}\sum_{i=1}^{|B_{m}|}\mathbf{1}[\hat{y}_{i} =y_{i}]\) is the accuracy for the bin \(m\), and \(\text{conf}(B_{m})=\frac{1}{|B_{m}|}\sum_{i=1}^{|B_{m}|}\hat{\mathbb{P}}(Y= \hat{y}_{i}|X=x_{i})\) gives the average confidence within the bin. \(|B_{m}|\) is the size of the bin \(m\), \(N\) is the total number of samples, and \(\mathbf{1}[\cdot]\) is the indicator function.

Throughout our experiments, we let the number of bins \(|\mathbb{B}|=10\) by default when computing ECE.

Similarly, below we report results with RMSCE (Root Mean Squared Error) [14] as another calibration metric, which is formulated as the following:

\[\text{RMSCE}[\hat{\mathbb{P}}]=\sqrt{\sum_{m=1}^{M}\frac{|B_{m}|}{N}(\text{ acc}(B_{m})-\text{conf}(B_{m}))^{2}}\] (8)

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_EMPTY:22]

Recalibration

Through all our recalibration results, we first split the test set into two equally sized calibration and test splits. Then, we fit the recalibration method using the calibration split and compute the performance on the test split. We run all our experiments with 10 different random seeds.

### Temperature Scaling

To perform temperature scaling [12], we use the calibration set to fit the temperature parameter. To perform the optimization, we use the LBFGS [13] algorithm from PyTorch with strong Wolfe line search, following [12]. Namely, we optimize the parameter \(\tau\) with

\[\hat{\mathbb{P}}_{\text{TS}}(X)=\text{Softmax}(f(\bm{X})/\tau)\] (9)

and then use it during inference to rescale the logits produced by \(f\). We use \(0.1\) learning rate and \(3000\) maximum iterations across all experiments and initialize the temperature value as \(1\), although find that TS is pretty robust to the choice of hyperparameters.

### Atypicality-Aware Recalibration

Here we describe the implementation details For Atypicality-Aware Recalibration (AAR). We formulate AAR with:

\[\log\hat{\mathbb{P}}_{\text{AAR}}(Y|X)\propto\phi(a(X))\log\hat{\mathbb{P}}(Y| X)+S_{Y},\] (10)

In total, this gives us \(|\mathcal{Y}|+3\) parameters. Using exactly the same setting as TS, we use LBFGS with a strong Wolfe search to optimize the three parameters, with the same splits as temperature scaling. We normalize the atypicality values (subtract the mean and divide by the standard deviation of the calibration set) for numerical stability. We use the same hyperparameters as TS (with \(0.1\) learning rate and \(3000\) maximum iterations) without any modification across all experiments, initialize \(c_{0},c_{1},c_{2}\) as \(0\) and \(S_{Y}\) parameters as \(1\). We run the recalibration procedure on a CPU with precomputed logits.

#### d.2.1 Adaptive Temperature Scaling

For AdaTS [10] we use the implementation provided with the paper 7. We identically use the hyperparameters and the architecture provided in the paper and their repository. They use an encoder and decoder architecture with \([1024,512,512]\) hidden units each, and a temperature predictor network with \([128,128]\) hidden units. They use an Adam Optimizer with a learning rate of \(5e-4\) with \(128\) batch size.

Footnote 7: https://github.com/thwjoy/adats

### Conformal Prediction

We follow the presentation in [1, 2]. Let \(\pi(X)\) be the permutation of \(\mathcal{Y}=\{1,\dots,C\}\) that sorts \(\hat{\mathbb{P}}(Y=c|X)\), i.e. the predicted probabilities for each class \(c\). We define a score function

\[s(x,y)=\sum_{j=1}^{c}\hat{\mathbb{P}}(Y=j|X),\text{where }y=\pi_{c}.\] (11)

This means greedily including classes until the set contains the true label, and using the cumulative sum of the probabilities as the score function. We compute all of the scores for the calibration set, \(S_{\text{calib}}=\{s(x_{1},y_{1}),...,s(x_{N},y_{N})\}\), we the \(\frac{\lceil(N+1)(1-\alpha)\rceil}{N}\)th quantile of the scores, \(\hat{q}\). Then, the prediction set is defined as

\[\mathcal{C}(x)=\{y:s(x,y)\leq\hat{q}\}\] (12)

We can further add randomization to the procedure where we have the prediction set function to be \(\mathcal{C}(x,u):\mathcal{X}\times[0,1]\) for randomization purposes to satisfy exact coverage. We refer to [13, 2] for a more thorough presentation.

RAPS is a variant of APS that regularizes the set sizes. They modify the scoring function to add a regularization term. This is controlled by the test size offset \(k_{reg}\) that controls the value beyond which the regularization is applied, and the \(\lambda_{reg}\) gives the strength of the regularization. To fit the \(k_{reg},\lambda_{reg}\) parameters in RAPS, we follow the procedure in [1] to fit both parameters. Namely, we fit \(k_{reg}\) by Algorithm 4 in their paper that leverages the set sizes in the calibration set, and we fit \(\lambda_{reg}\) by the largest regularization parameter that achieves the smallest set sizes, searched over a grid of \(\{0.001,0.01,0.1,0.2,0.5\}\) following their presentation.

### Atypicality-Aware Conformal Prediction

We have a simple discrete grouping scheme to make conformal prediction atypicality aware. Namely, we group points using their atypicality and confidence percentiles and fit individual thresholds. Concretely, we construct a dataset of \(\mathcal{D}_{AA}=\{(c_{i},c_{i+1}],(a_{j},a_{j+1}],\hat{q}_{i,j}\}_{i,j\in[N]}\) using the calibration set where \((c_{i},c_{i+1}]\) denotes the confidence range for quantile \(i\), \((a_{j},a_{j+1}]\) denotes the atypicality range for quantile \(j\) and \(\hat{q}_{i,j}\) denotes the threshold fitted to the group specified by these intervals. We let \(N=6\) as the number of groups, and in total, we end up with \(36\) thresholds. At test time, we check the quantile of the confidence and atypicality of a point and use the corresponding temperature. For AA-RAPS, we use the same \(k_{reg}\) and \(\lambda_{reg}\) values as was found with the RAPS procedure. For practical purposes, we do not allow zero sets (prediction sets at least include the top prediction).

We would like to make the remark that sometimes the marginal coverage can exceed the desired value (e.g. Figure 9). This is often because the underlying model is already very confident for a majority of data points (e.g. More than half of the data points have 92% confidence). The gains we provide are often for points with lower confidence regions, as the coverage is not satisfied in those regions.

[MISSING_PAGE_EMPTY:25]

## Appendix F Fitzpatrick17k and Skin Lesion Classification

We use the training script from [14] to finetune models on the Fitzpatrick17k dataset. We train the models for \(50\) epochs, fixing the backbone and training only the probe on top of the penultimate layer. The probe consists of \(2\) layers, one layer of \(256\) units followed by ReLU and Dropout with probability \(0.4\), followed by the classifier layer with an output dimensionality of \(9\). We use an Adam optimizer with a \(0.0001\) learning rate.

The entire dataset consists of \(16,577\) images, where the potential labels are: \(10,886\) inflammatory, \(1,352\) malignant epidermal, \(1,194\) genodermatoses, \(1,067\) benign dermal, \(931\) benign epidermal, \(573\) malignant melanoma, \(236\) benign melanocyte, \(182\) malignant cutaneous lymphoma, and \(156\) malignant dermal. We split the dataset into 3 sets (Training (\(0.5\)), Validation (\(0.25\)), and Test (\(0.25\))). We use the validation set as the calibration set and perform the experiments with 10 random splits.

Figure 11: **Fitted Additive Correction Factor vs Class Atypicality.** We observe a monotonically increasing relationship between the atypicality of a class and the additive correction parameter fitted to that class with AAR.

Figure 12: **Atypicality-Aware Conformal Prediction for ResNet50 and ImageNet.** Target coverage rate is \(95\%\).

Proofs

### Detailed derivation of the claim on Page 5

When \(\mathbb{P}_{1}(X)\leq 1/2\), the signed calibration error at level \(u\in(1/2,1)\) becomes \(u-\mathbb{P}(Y=-1\mid\hat{\mathbb{P}}_{-1}(X)=u)=u-\mathbb{P}(Y=-1\mid\hat{ \mathbb{P}}_{1}(-X)=u)=u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)\).

The last inequality is due to symmetry. More specifically, we claim \((X,Y)\overset{d}{=}(-X,-Y)\), where the notation \(\overset{d}{=}\) denotes equal in distribution. In fact, as \(X\overset{d}{=}-X\), it suffices to show that for any \(y\in\{-1,1\}\), and \(x\in\mathbb{R}^{d}\), we have

\[\mathbb{P}(Y=y\mid X=x)=\mathbb{P}(-Y=y\mid-X=x).\]

When \(y=-1\), the right hand side

\[\mathbb{P}(-Y=-1\mid-X=x)=\mathbb{P}(Y=1\mid X=-x)=\sigma(\langle \beta^{*},-x\rangle)\] \[= 1-\sigma(\langle\beta^{*},x\rangle)=1-\mathbb{P}(Y=1\mid X=x)= \mathbb{P}(Y=-1\mid X=x).\]

Similarly, when \(y=1\),

\[\mathbb{P}(-Y=1\mid-X=x)=\mathbb{P}(Y=-1\mid X=-x)=1-\sigma( \langle\beta^{*},-x\rangle)\] \[= \sigma(\langle\beta^{*},x\rangle)=\mathbb{P}(Y=1\mid X=x).\]

We complete the proof.

### Proof of Theorem 3.1

**Theorem G.1** (Restatement of Theorem 3.1).: _Consider the data generative model with the algorithm described in Section 3.2. For any \(K>1\), suppose we consider the quantiles of \(a(X)\), \(a_{1},a_{2},...,a_{K},a_{K+1}\) such that \(\mathbb{P}(a(X)\in(a_{k},a_{k+1}])=1/K\) for \(k\in[K]\). In addition, we assume \(\|\beta^{*}\|\leq c_{0}\), and \(d/n=\kappa\) for some sufficiently small \(c_{0},\kappa>0\). Then for sufficiently large \(n\), we have_

\[\mathbb{E}_{u}[u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)\mid a (X)\in[a_{k-1},a_{k}]]>\] \[\mathbb{E}_{u}[u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)\mid a (X)\in(a_{k},a_{k+1}]],\]

_for \(k=2,..,K\)._

Proof.: Following [1], we have

\[u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)=u-\mathbb{E}_{Z}[\sigma(\frac{ \|\beta^{*}\|}{\|\hat{\beta}\|}\cos\hat{\theta}\cdot\sigma^{-1}(u))+\sin\hat{ \theta}\cdot\|\beta^{*}\|Z],\]

where \(\cos\hat{\theta}=\frac{\hat{\beta}^{\top}\beta^{*}}{\|\hat{\beta}\|\cdot\| \beta^{*}\|}\) and \(Z\sim N(0,1)\).

According to the results in Section 2.2 of [1], we have \(\|\hat{\beta}\|\to R^{*}=R^{*}(\kappa,\beta^{*})\) and \(\cos\hat{\theta}\to c^{*}=c^{*}(\kappa,\beta^{*})\), for two quantities \(R^{*}\) and \(c^{*}\) that depend on \(\kappa\) and \(\beta^{*}\). We then have

\[u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)\to u-\mathbb{E}_{Z}[\sigma( \frac{\|\beta^{*}\|}{R^{*}}c^{*}\cdot\sigma^{-1}(u))+\sqrt{1-c^{*2}}\cdot\| \beta^{*}\|Z].\]

Using the proof of Theorem 3 in [1], we have that

\[u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)=C_{\kappa}(u)\cdot\kappa+o( \kappa),\]

where

\[C_{\kappa}(u)=c_{1}\sigma^{\prime}(\sigma^{-1}(u))\cdot\sigma^{-1}(u)-c_{2} \sigma^{\prime\prime}(\sigma^{-1}(u)),\]

for two positive constants \(c_{1},c_{2}\).

As a result, we have

\[u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)\geq 0\] (13)

In addition, since when \(z\in[-1,1]\), \(z\cdot\sigma^{\prime}(z)\) and \(-\sigma^{\prime\prime}(z)\) are both increasing, we then have \(C_{\kappa}(u)\) increasing for \(\hat{\beta}^{\top}x=\sigma^{-1}(u)\in(-1,1)\).

Proving the result for \(\{k=2,\ldots,K-1\}\)In addition, by our model assumption \(x\sim N(0,I_{d})\), we have that \(\|x\|\) and \(\frac{x}{\|x\|}\) are independent, and \(\frac{x}{\|x\|}\sim S\) where \(S\) is a uniform distribution on the sphere in the \(d\)-dimensional space. As the monotonic transformations will not change the events defined by quantiles, and \(\exp(-\|x\|^{2}/2)\) is a monotonic function in \(\|x\|\), for the simplicity of presentation we use \(a(X)=\|X\|\) in the rest of this proof. As a result, given \(\|x\|=a\), we have

\[\hat{\beta}^{\top}x\mid\|x\|=a\stackrel{{ d}}{{=}}a\cdot\hat{ \beta}^{\top}S=a\cdot\|\hat{\beta}\|\cdot S_{1},\]

where \(S_{1}\) is the first coordinate of \(S\).

Consequently, if we further condition on the event where \(\hat{\beta}^{\top}x>0\) (as we assume \(u>0\) throughout Section 3.2), we have

\[\hat{\beta}^{\top}x\stackrel{{ d}}{{=}}a\cdot\|\hat{\beta}\| \cdot S_{1}\mid S_{1}>0\stackrel{{ d}}{{=}}a\cdot\|\hat{\beta}\| \cdot\frac{Z_{1}}{\sqrt{Z_{1}^{2}+Q}}\to a\cdot R^{*}\cdot\frac{Z_{1}}{ \sqrt{Z_{1}^{2}+Q}},\]

where \(Q\sim\chi_{p-1}^{2}\), \(Z_{1}\sim N(0,1)\) and they are independent.

Due to the monotonicity of \(C_{\kappa}(u)\) on \(u\), we have that for any \(a_{1}>a_{2}\),

\[C_{\kappa}(u)\mid\|x\|=a_{1}\stackrel{{ d}}{{>}}C_{\kappa}(u) \mid\|x\|=a_{2},\]

where the notation \(\stackrel{{ d}}{{>}}\) denotes stochastic dominance.

Consequently, we have

\[\mathbb{E}_{u}[u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)\mid a(X)\in[a_{ k-1},a_{k}]]<\mathbb{E}_{u}[u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)\mid a(X)\in(a_{k},a _{k+1}]],\]

for \(k=2,..,K-1\).

Proving the result for \(k=K\)To complete the proof, it suffices to show that the inequality is also true for \(K\)th quantile:

\[\mathbb{E}_{u}[u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)\mid a(X)\in[a_{ K-1},a_{K}]]<\mathbb{E}_{u}[u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)\mid a(X)\in(a _{k},a_{k+1}]],\]

which is equivalent to

\[\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u))\cdot\mathbf{1 }\{a(X)\in[a_{K-1},a_{K}]\}]<\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{ \mathbb{P}}_{1}(X)=u))\cdot\mathbf{1}\{a(X)\in(a_{k},a_{k+1}]\}].\]

In the above inequality, the right hand side can be decomposed into

\[\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \cdot\mathbf{1}\{a(X)\in(a_{k},a_{k+1}]\}]\] \[= \mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \cdot\mathbf{1}\{a(X)\in[a_{K},2p]\}]\] \[+\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \cdot\mathbf{1}\{a(X)\in[2p,a_{K+1}]\}].\]

Denote the \(\alpha\) quantile of \(\chi_{p}^{2}\) by \(\chi_{\alpha,p}^{2}\). We then have \(a_{k}=\chi_{\frac{k}{K+1},p}^{2}\). We further decompose the equation

\[\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \cdot\mathbf{1}\{a(X)\in[a_{K},2p]\}]\] \[= \mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \cdot\mathbf{1}\{a(X)\in[a_{K},\chi_{\frac{K+1}{K+1},p}^{2}]\}]\] \[+\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \cdot\mathbf{1}\{a(X)\in[\chi_{\frac{K+s}{K+1},p}^{2},2p]\}].\]

In the following, we proceed to prove

\[\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u))\cdot\mathbf{1 }\{a(X)\in[\chi_{\frac{K+s}{K+1},p}^{2},2p]\}]>\mathbb{E}_{u}[(u-\mathbb{P}(Y= 1\mid\hat{\mathbb{P}}_{1}(X)=u))\cdot\mathbf{1}\{a(X)\in[a_{K-1},a_{K}\}].\] (14)

We now use the approximation of the chi-square quantile: when \(p\to\infty\), we have

\[a_{K}=\frac{1}{2}(z_{\frac{K}{K+1}}+\sqrt{2p})^{2}+o(1),\text{ and }\chi_{\frac{K+s}{K+1},p}^{2}=\frac{1}{2}(z_{\frac{K+s}{K+1}}+\sqrt{2p})^{2}+o(1),\]where \(z_{\alpha}\) denotes the \(\alpha\)-quantile of a standard normal random variable.

Then

\[\chi^{2}_{\frac{K+\delta}{K+1},p}-a_{K}=\frac{1}{2}(z_{\frac{K+\delta}{K+1}}-z_{ \frac{K}{K+1}})(z_{\frac{K+\delta}{K+1}}+z_{\frac{K}{K+1}}+2\sqrt{2p}).\]

Using the fact that \(z_{1-\frac{1}{K}}=\sqrt{2\log K}+o(1)\) for \(K\rightarrow\infty\), then we have

\[z_{\frac{K+\delta}{K+1}}-z_{\frac{K}{K+1}}=\frac{-\log(1-\delta)}{\sqrt{2\log K }}+o(1).\]

In addition, for any \(a\in[\chi^{2}_{\frac{K+\delta}{K+1},p},2p]\) and \(a^{\prime}\in[a_{K-1},a_{K}]\), we have

\[\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \mid a(X)=a]-\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \mid a(X)=a^{\prime}]\] \[\geq C(z_{\frac{K+\delta}{K+1}}-z_{\frac{K}{K+1}}),\]

for some universal constant \(C\).

Therefore

\[\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \mid a(X)\in[\chi^{2}_{\frac{K+\delta}{K+1},p},2p]]-\mathbb{E}_{u}[(u-\mathbb{ P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u))\mid a(X)\in[a_{K-1},a_{K}]]\] \[\geq C(z_{\frac{K+\delta}{K+1}}-z_{\frac{K}{K+1}}).\]

Then

\[\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \cdot\mathbf{1}\{a(X)\in[\chi^{2}_{\frac{K+\delta}{K+1},p},2p]\}]\] \[= \mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \mid a(X)\in[\chi^{2}_{\frac{K+\delta}{K+1},p},2p]]\cdot\mathbb{P}(a(X)\in[ \chi^{2}_{\frac{K+\delta}{K+1},p},2p])\] \[\geq \Big{(}\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X) =u))\mid a(X)\in[a_{K-1},a_{K}]]+C(z_{\frac{K+\delta}{K+1}}-z_{\frac{K}{K+1}}) \Big{)}\cdot(\frac{1}{K}-\frac{\delta}{K+1}+o(\frac{\delta}{K+1}))\] \[= \mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \cdot\mathbf{1}\{a(X)\in[a_{K-1},a_{K}\}]\] \[+C(z_{\frac{K+\delta}{K+1}}-z_{\frac{K}{K+1}})-(1+o(1))\frac{ \delta}{K+1}\cdot\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)= u))\mid a(X)\in[a_{K-1},a_{K}]].\]

The last equality uses the fact that \(\mathbb{P}(a(X)\in[a_{K-1},a_{K}])=1/K,\) and therefore

\[\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \mid a(X)\in[a_{K-1},a_{K}]\cdot\frac{1}{K}=\mathbb{E}_{u}[(u-\mathbb{P}(Y=1 \mid\hat{\mathbb{P}}_{1}(X)=u))\cdot\mathbf{1}\{a(X)\in[a_{K-1},a_{K}\}]\]

Then use the fact that \(|\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u))\mid a(X)\in[ a_{K-1},a_{K}]]|=O(1)\) and we choose \(\delta=o(1/\log K)\) so

\[\frac{\delta}{K}=o(|\frac{\log(1-\delta)}{\sqrt{\log K}}|).\]

Consequently,

\[C(z_{\frac{K+\delta}{K+1}}-z_{\frac{K}{K+1}})-(1+o(1))\frac{\delta}{K+1}\cdot \mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u))\mid a(X)\in[ a_{K-1},a_{K}]]>0,\]

which implies

\[\mathbb{E}_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u)) \cdot\mathbf{1}\{a(X)\in[\chi^{2}_{\frac{K+\delta}{K+1},p},2p]\}]\geq\mathbb{E }_{u}[(u-\mathbb{P}(Y=1\mid\hat{\mathbb{P}}_{1}(X)=u))\cdot\mathbf{1}\{a(X) \in[a_{K-1},a_{K}\}].\]

Combining with equation 13, we prove equation 14 and complete the proof. \(\Box\)

### Theoretical Justification of the calibration improvement using the atypicality score

In this section, we provide the theoretical justification to understand why incorporating the atypicality score will improve calibration. In particular, we consider the binary classification problem with prediction \(f:X\in[0,1]\) indicating the predicted probability of \(Y=1\) given \(X=x\).

For a predictor \(f\), let us denote its conditional calibration error at an atypicality level \(\gamma\) by \(\text{CE}_{\gamma}(f)=\mathbb{E}[(f(X)-\mathbb{E}[Y|f(X)])^{2}|a(X)=\gamma]\).

**Theorem G.2**.: _Consider the same setting as Theorem 3.1. Suppose the temperature function \(\hat{\tau}(a(X))=\arg\min_{\tau}\mathbb{E}[l(Y,\text{Softmax}(f(X)/\tau(a(X))))]\) with \(l\) being the cross entropy loss, and let \(\hat{\mathbb{P}}_{\text{AAR}}(X)=\text{Softmax}(f(X)/\hat{\tau}(a(X)))\). Then_

\[\text{CE}_{\gamma}(\hat{\mathbb{P}}_{\text{AAR}})\leq\min\{\text{CE}_{\gamma} (\hat{\mathbb{P}}_{\text{TS}}),\text{CE}_{\gamma}(f)\}.\] (15)

Proof: For a prediction function \(f\), we first define the conditional mean squared error of \(f\) at an atypicality level \(\gamma\) by \(\text{MSE}_{\gamma}(f)=\mathbb{E}[(f(X)-Y)^{2}\mid a(X)=\gamma]\), then we have

\[\text{MSE}_{\gamma}(f)-CE_{\gamma}(f)= \mathbb{E}[(f(X)-Y)^{2}\mid a(X)=\gamma]-\mathbb{E}[(f(X)- \mathbb{E}[Y\mid f(X),a(X)=\gamma])^{2}\mid a(X)=\gamma]\] \[= \mathbb{E}[(\mathbb{E}[Y\mid f(X),a(X)=\gamma]-Y)\cdot(2f(X)- \mathbb{E}[Y\mid f(X),a(X)=\gamma]-Y)\mid a(X)=\gamma]\] \[= \mathbb{E}[(\mathbb{E}[Y\mid f(X),a(X)=\gamma]-Y)\cdot(\mathbb{E} [Y\mid f(X),a(X)=\gamma]-Y)\mid a(X)=\gamma]\] \[+2\mathbb{E}[(\mathbb{E}[Y\mid f(X),a(X)=\gamma]-Y)\cdot(f(X)- \mathbb{E}[Y\mid f(X),a(X)=\gamma]))\mid a(X)=\gamma]\]

Since

\[\mathbb{E}[Y\mathbb{E}[Y\mid f(X),a(X)=\gamma]\mid a(X)=\gamma]\] \[= \mathbb{E}_{f(X)|a(X)=\gamma}\mathbb{E}[Y\mathbb{E}[Y\mid f(X),a (X)=\gamma]\mid f(X),a(X)=\gamma]]\] \[= \mathbb{E}[(\mathbb{E}[Y\mid f(X),a(X)=\gamma])^{2}\mid a(X)= \gamma],\]

we have

\[\mathbb{E}[(\mathbb{E}[Y\mid f(X),a(X)=\gamma]-Y)\cdot(f(X)-\mathbb{E}[Y\mid f (X),a(X)=\gamma]))\mid a(X)=\gamma]=0,\]

and therefore

\[\text{MSE}_{\gamma}(f)-CE_{\gamma}(f)=\mathbb{E}[(\mathbb{E}[Y\mid f(X),a(X)= \gamma]-Y)^{2}\mid a(X)=\gamma]\]

Now that \(\hat{\mathbb{P}}_{AAR}(f(x),a(x))\) is monotonic on the \(\hat{\mathbb{P}}(x)\), we have

\[\mathbb{E}[Y\mid f(x),a(X)=\gamma]=\mathbb{E}[Y\mid\hat{\mathbb{P}}_{AAR}(f(x ),a(X)),a(X)=\gamma],\]

implying

\[\text{MSE}_{\gamma}(\hat{\mathbb{P}}_{AAR})-CE_{\gamma}(\hat{\mathbb{P}}_{AAR })=\text{MSE}_{\gamma}(\hat{\mathbb{P}})-CE_{\gamma}(\hat{\mathbb{P}}).\] (16)

Similarly, we have

\[\text{MSE}_{\gamma}(\hat{\mathbb{P}}_{TS})-CE_{\gamma}(\hat{\mathbb{P}}_{TS} )=\text{MSE}_{\gamma}(\hat{\mathbb{P}})-CE_{\gamma}(\hat{\mathbb{P}}).\] (17)

In the following, we will show that

\[\text{MSE}_{\gamma}(\hat{\mathbb{P}}_{AAR})<\min\{\text{MSE}_{\gamma}(\hat{ \mathbb{P}}_{TS}),\text{MSE}_{\gamma}(\hat{\mathbb{P}})\}.\] (18)

First, as we consider the binary classification setting, with \(l\) being the cross-entropy loss, we have

\[l(Y,\text{Softmax}(f(X)/\tau(a(X))))=Y\log(\sigma(f(X)/\tau(a(X)))+(1-Y)\log(1- \sigma(f(X)/\tau(a(X))),\]

where \(\sigma(x)=1/(1+e^{x})\).

Then, by the definition of \(\hat{\tau}(a(X))\), we have that

\[\hat{\tau}(a(X))= \operatorname*{arg\,min}_{\tau}\mathbb{E}[Y\log(\sigma(f(X)/\tau (a(X)))+(1-Y)\log(1-\sigma(f(X)/\tau(a(X)))]\] \[= \operatorname*{arg\,min}_{\tau}\mathbb{E}[\mathbb{E}[Y\log(\sigma( f(X)/\tau(a(X)))+(1-Y)\log(1-\sigma(f(X)/\tau(a(X)))\mid a(X)]].\]Taking the derivative on the last line and setting it to zero, we have

\[\mathbb{E}[\frac{Y}{\sigma(f(X)/\hat{\tau}(a(X))}-\frac{1-Y}{1-\sigma(f(X)/\hat{ \tau}(a(X))}\mid a(X)]=0,\]

implying

\[\mathbb{E}[\sigma(f(X)/\hat{\tau}(a(X))\mid a(X)]=\mathbb{E}[Y\mid a(X)].\]

This makes the derivative of \(\mathbb{E}[(Y-\sigma(f(X)/\tau(a(X))))^{2}\mid a(X)]\) zero and therefore \(\hat{\tau}(a(X))\) is also a minimizer of \(\mathbb{E}[(Y-\sigma(f(X)/\tau(a(X))))^{2}\mid a(X)]\):

\[\hat{\tau}(a(X))=\operatorname*{arg\,min}_{\tau}\mathbb{E}[Y\log(\sigma(f(X)/ \tau(a(X)))+(1-Y)\log(1-\sigma(f(X)/\tau(a(X)))]=\operatorname*{arg\,min}_{ \tau}\mathbb{E}[(Y-\sigma(f(X)/\tau(a(X))))^{2}\]

Letting \(g(\gamma)=\operatorname*{arg\,min}_{c}\mathbb{E}[(Y-\sigma(f(X)/c))^{2}\mid a (X)=\gamma]\), we have that

\[g(a(X))=\operatorname*{arg\,min}_{\tau}\mathbb{E}[(Y-\sigma(f(X)/\tau(a(X)))) ^{2}\mid a(X)],\]

and therefore

\[g(a(X))=\operatorname*{arg\,min}_{\tau}\mathbb{E}[\mathbb{E}[(Y-\sigma(f(X)/ \tau(a(X))))^{2}\mid a(X)]=\hat{\tau}(a(X)).\]

As a result,

\[\text{MSE}_{\gamma}(\hat{\mathbb{P}}_{AAR})= \mathbb{E}[(\hat{\mathbb{P}}_{AAR}(X)-Y)^{2}\mid a(X)=\gamma]\] \[= \mathbb{E}[(\hat{\mathbb{P}}_{AAR}(X)-Y)^{2}\mid a(X)=\gamma]\] \[= \mathbb{E}[(\sigma(f(X)/\hat{\tau}(a(X))-Y)^{2}\mid a(X)=\gamma]\] \[= \mathbb{E}[(\text{Softmax}(\hat{\mathbb{P}}(X)/g(a(X))-Y)^{2}\mid a (X)=\gamma]\] \[= \operatorname*{arg\,min}_{c}\mathbb{E}[(\sigma(f(X)/c-Y)^{2}\mid a (X)=\gamma]\] \[\leq \mathbb{E}[(\sigma(f(X)-Y)^{2}\mid a(X)=\gamma]\] \[= \text{MSE}_{\gamma}(\hat{\mathbb{P}}).\]

Similarly, we have \(\text{MSE}_{\gamma}(\hat{\mathbb{P}}_{AAR})\leq\text{MSE}_{\gamma}(\hat{ \mathbb{P}}_{TS})\), and therefore equation 18 holds.

Combining with equation 16 and equation 17, we have

\[CE_{\gamma}(\hat{\mathbb{P}}_{AAR})\leq\min\{CE_{\gamma}(\hat{\mathbb{P}}_{TS }),CE_{\gamma}(\hat{\mathbb{P}})\}.\]

[MISSING_PAGE_EMPTY:32]

## Appendix H Limitations

### Quantifying Atypicality

Since we do not have access to the true distribution of \(\mathbb{P}(X)\), we estimate it through the model, e.g. using the embeddings. This means we are capturing the atypicality not solely with respect to the training distribution but also the model. It is possible that a model that does not fit the data well and produces low-quality atypicality estimates. _We would like to stress that our goal here is to show that even simple estimators can demonstrate significant benefits_. In general, we observe that our findings hold for large datasets and widely used models, and atypicality gives a semantically meaningful way to group data points qualitatively. Our findings suggest that we can unify the understanding and improve uncertainty quantification and recalibration methods with atypicality, however, practitioners should be careful about incorporating atypicality, as poor atypicality estimates can lead to worse performance.

### Subgroup Fairness Experiments

While the literature on algorithms to satisfy group fairness is rich [11, 10], here we wanted to give a case study with skin-lesion classification. Our goal was to provide further evidence that atypicality awareness could improve fairness algorithms. In this domain, there is more verification to do to better characterize how and when atypicality helps, thus, to better understand which subgroups could benefit more from atypicality-aware algorithms and whether these findings apply generally in the literature.

### Theoretical Analysis

Following the earlier work [1, 10], we analyzed the calibration behavior of well-specified logistic regression. However, our empirical findings suggest that the phenomena are much more broadly applicable. We suggest that future work can analyze the behavior in more general settings to better understand the dynamics.

Figure 13: **Post-hoc Recalibration for Classification with 5-NN distance as an Atypicality Metric.****(a) Balanced Supervised Classification:** Atypicality-Aware Recalibration improves the calibration of models trained with balanced datasets, across atypicality groups. **(b) Imbalanced Supervised Classification:** Atypicality-Aware Recalibration improves both the calibration across groups and the overall accuracy of models trained with imbalanced datasets with 5-nearest neighbors distance as an atypicality metric.

Figure 16: **Input Atypicality and Confidence for ResNet152. We provide the input atypicality for ResNet152, in the same structure as Figure 15.**

Figure 14: **Distribution of Input Atypicality. Here, we give the distribution of Atypicality for ResNet18 on ImageNet, using GMM and KNN methods.**

Figure 15: **Label Atypicality and Confidence. Here, the x-axis reflects the label atypicality quantile, and the y-axis indicates confidence. The coloring for the figure on the left indicates the accuracy within a bin, and the figure on the right has the difference between confidence and accuracy within a bin. Similar to Figure 6, we observe that atypical examples have lower accuracy, and predictions are more overconfident.**