# Star-Shaped Denoising Diffusion Probabilistic Models

Andrey Okhotin

HSE University, MSU University

Moscow, Russia

andrey.okhotin@gmail.com

&Dmitry Molchanov

BAYESG

Budva, Montenegro

dmolch111@gmail.com

&Vladimir Arkhipkin

Sber AI

Moscow, Russia

arkhipkin.v98@gmail.com

&Grigory Bartosh

AMLab, Informatics Institute

University of Amsterdam

Amsterdam, Netherlands

g.bartosh@uva.nl

&Viktor Ohanesian

Independent Researcher

v.v.oganesyan@gmail.com

&Aibek Alanov

AIRI, HSE University

Moscow, Russia

alanov.aibek@gmail.com

&Dmitry Vetrov

Constructor University

Bremen, Germany

dvetrov@constructor.university

Equal contribution

###### Abstract

Denoising Diffusion Probabilistic Models (DDPMs) provide the foundation for the recent breakthroughs in generative modeling. Their Markovian structure makes it difficult to define DDPMs with distributions other than Gaussian or discrete. In this paper, we introduce Star-Shaped DDPM (SS-DDPM). Its _star-shaped diffusion process_ allows us to bypass the need to define the transition probabilities or compute posteriors. We establish duality between star-shaped and specific Markovian diffusions for the exponential family of distributions and derive efficient algorithms for training and sampling from SS-DDPMs. In the case of Gaussian distributions, SS-DDPM is equivalent to DDPM. However, SS-DDPMs provide a simple recipe for designing diffusion models with distributions such as Beta, von Mises-Fisher, Dirichlet, Wishart and others, which can be especially useful when data lies on a constrained manifold. We evaluate the model in different settings and find it competitive even on image data, where Beta SS-DDPM achieves results comparable to a Gaussian DDPM. Our implementation is available at https://github.com/andrey-okhotin/star-shaped.

## 1 Introduction

Deep generative models have shown outstanding sample quality in a wide variety of modalities. Generative Adversarial Networks (GANs) (Goodfellow et al., 2014; Karras et al., 2021), autoregressive models (Ramesh et al., 2021), Variational Autoencoders (Kingma and Welling, 2013; Rezende et al., 2014), Normalizing Flows (Grathwohl et al., 2018; Chen et al., 2019) and energy-based models (Xiao et al., 2020) show impressive abilities to synthesize objects. However, GANs are not robust to the choice of architecture and optimization method (Arjovsky et al., 2017; Gulrajani et al., 2017; Karras et al., 2019; Brock et al., 2018), and they often fail to cover modes in data distribution (Zhao et al., 2018; Thanh-Tung and Tran, 2020). Likelihood-based models avoid mode collapse but may overestimate the probability in low-density regions (Zhang et al., 2021).

Recently, diffusion probabilistic models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have received a lot of attention. These models generate samples using a trained Markov process that starts with white noise and iteratively removes noise from the sample. Recent works have shown that diffusion models can generate samples comparable in quality or even better than GANs (Song et al., 2020; Dhariwal and Nichol, 2021), while they do not suffer from mode collapse by design, and also they have a log-likelihood comparable to autoregressive models (Kingma et al., 2021). Moreover, diffusion models show these results in various modalities such as images (Saharia et al., 2021), sound (Popov et al., 2021; Liu et al., 2022) and shapes (Luo and Hu, 2021; Zhou et al., 2021).

The main principle of diffusion models is to destroy information during the forward process and then restore it during the reverse process. In conventional diffusion models like denoising diffusion probabilistic models (DDPM) destruction of information occurs through the injection of Gaussian noise, which is reasonable for some types of data, such as images. However, for data distributed on manifolds, bounded volumes, or with other features, the injection of Gaussian noise can be unnatural, breaking the data structure. Unfortunately, it is not clear how to replace the noise distribution within traditional diffusion models. The problem is that we have to maintain a connection between the distributions defining the Markov noising process that gradually destroys information and its marginal distributions. While some papers explore other distributions, such as delta functions (Bansal et al., 2022) or Gamma distribution (Nachmani et al., 2021), they provide ad hoc solutions for special cases that are not easily generalized.

In this paper, we present Star-Shaped Denoising Diffusion Probabilistic Models (SS-DDPM), a new approach that generalizes Gaussian DDPM to an exponential family of noise distributions. In SS-DDPM, one only needs to define marginal distributions at each diffusion step (see Figure 1). We provide a derivation of SS-DDPM, design efficient sampling and training algorithms, and show its equivalence to DDPM (Ho et al., 2020) in the case of Gaussian noise. Then, we outline a number of practical considerations that aid in training and applying SS-DDPMs. In Section 5, we demonstrate the ability of SS-DDPM to work with distributions like von Mises-Fisher, Dirichlet and Wishart. Finally, we evaluate SS-DDPM on image and text generation. Categorical SS-DDPM matches the performance of Multinomial Text Diffusion (Hoogeboom et al., 2021) on the text8 dataset, while our Beta diffusion model achieves results, comparable to a Gaussian DDPM on CIFAR-10.

## 2 Theory

### DDPMs

We start with a brief introduction of DDPMs. The Gaussian DDPM (Ho et al., 2020) is defined as a forward (diffusion) process \(q^{\textsc{DDPM}}(x_{0:T})\) and a corresponding reverse (denoising) process \(p_{\theta}^{\textsc{DDPM}}(x_{0:T})\). The forward process is defined as a Markov chain with Gaussian conditionals:

\[q^{\textsc{DDPM}}(x_{0:T}) =q(x_{0}){\prod_{t=1}^{T}}q^{\textsc{DDPM}}(x_{t}|x_{t-1}),\] (1) \[q^{\textsc{DDPM}}(x_{t}|x_{t-1}) =\mathcal{N}\left(x_{t};\sqrt{1-\beta_{t}}x_{t-1},\beta_{t}\textbf {I}\right),\] (2)

where \(q(x_{0})\) is the data distribution. Parameters \(\beta_{t}\) are typically chosen in advance and fixed, defining the noise schedule of the diffusion process. The noise schedule is chosen in such a way that the final \(x_{T}\) no longer depends on \(x_{0}\) and follows a standard Gaussian distribution \(q^{\textsc{DDPM}}(x_{T})=\mathcal{N}\left(x_{T};0,\textbf{I}\right)\)

Figure 1: Markovian forward processes of DDPM (left) and the star-shaped forward process of SS-DDPM (right).

The reverse process \(p_{\theta}^{\textsc{DDPM}}(x_{0:T})\) follows a similar structure and constitutes the generative part of the model:

\[p_{\theta}^{\textsc{DDPM}}(x_{0:T}) =q^{\textsc{DDPM}}(x_{T})\underset{t=1}{\overset{T}{\prod}}p_{ \theta}^{\textsc{DDPM}}(x_{t-1}|x_{t}),\] (3) \[p_{\theta}^{\textsc{DDPM}}(x_{t-1}|x_{t}) =\mathcal{N}\left(x_{t-1};\mu_{\theta}(x_{t},t),\Sigma_{\theta}(x _{t},t)\right).\] (4)

The forward process \(q^{\textsc{DDPM}}(x_{0:T})\) of DDPM is typically fixed, and all the parameters of the model are contained in the generative part of the model \(p_{\theta}^{\textsc{DDPM}}(x_{0:T})\). These parameters are tuned to maximize the variational lower bound (VLB) on the likelihood of the training data:

\[\mathcal{L}^{\textsc{DDPM}}(\theta) =\mathbb{E}_{q^{\textsc{DDPM}}}\left[\log p_{\theta}^{\textsc{ DDPM}}(x_{0}|x_{1})-\underset{t=2}{\overset{T}{\sum}}D_{KL}\left(q^{\textsc{DDPM}} (x_{t-1}|x_{t},x_{0})\,\|\,p_{\theta}^{\textsc{DDPM}}(x_{t-1}|x_{t})\right)\right]\] (5) \[\mathcal{L}^{\textsc{DDPM}}(\theta) \rightarrow\underset{\theta}{\max}\] (6)

One of the main challenges in defining DDPMs is the computation of the posterior \(q^{\textsc{DDPM}}(x_{t-1}|x_{t},x_{0})\). Specifically, the transition probabilities \(q^{\textsc{DDPM}}(x_{t}|x_{t-1})\) have to be defined in such a way that this posterior is tractable. Specific DDPM-like models are available for Gaussian (Ho et al., 2020), Categorical (Hoogeboom et al., 2021) and Gamma (Kawar et al., 2022) distributions. Defining such models remains challenging in more general cases.

### Star-Shaped DDPMs

As previously discussed, extending the DDPMs to other distributions poses significant challenges. In light of these difficulties, we propose to construct a model that only relies on marginal distributions \(q(x_{t}|x_{0})\) in its definition and the derivation of the loss function.

We define star-shaped diffusion as a _non-Markovian_ forward process \(q^{\textsc{S}\textsc{S}}(x_{0:T})\) that has the following structure:

\[q^{\textsc{S}\textsc{S}}(x_{0:T})=q(x_{0})\underset{t=1}{\overset{T}{\prod}}q^ {\textsc{S}\textsc{S}}(x_{t}|x_{0}),\] (7)

where \(q(x_{0})\) is the data distribution. We note that in contrast to DDPM all noisy variables \(x_{t}\) are conditionally independent given \(x_{0}\) instead of constituting a Markov chain. This structure of the forward process allows us to utilize other noise distributions, which we discuss in more detail later.

### Defining the reverse model

In DDPMs the true reverse model \(q^{\textsc{DDPM}}(x_{0:T})\) has a Markovian structure (Ho et al., 2020), allowing for an efficient sequential generation algorithm:

\[q^{\textsc{DDPM}}(x_{0:T})=q^{\textsc{DDPM}}(x_{T})\underset{t=1}{\overset{T}{ \prod}}q^{\textsc{DDPM}}(x_{t-1}|x_{t}).\] (8)

Figure 2: Model structure of DDPM and SS-DDPM.

For the star-shaped diffusion, however, the Markovian assumption breaks:

\[q^{\textsc{ss}}(x_{0:T})=q^{\textsc{ss}}(x_{T})\prod_{t=1}^{T}q^{\textsc{ss}}(x_{ t-1}|x_{t:T}).\] (9)

Consequently, we now need to approximate the true reverse process by a parametric model which is conditioned on the whole tail \(x_{t:T}\).

\[p^{\textsc{ss}}_{\theta}(x_{0:T})=p^{\textsc{ss}}_{\theta}(x_{T})\prod_{t=1}^{ T}p^{\textsc{ss}}_{\theta}(x_{t-1}|x_{t:T}).\] (10)

It is crucial to use the whole tail \(x_{t:T}\) rather than just one variable \(x_{t}\) when predicting \(x_{t-1}\) in a star-shaped model. As we show in Appendix B, if we try to approximate the true reverse process with a Markov model, we introduce a substantial irreducible gap into the variational lower bound. Such a sampling procedure fails to generate realistic samples, as can be seen in Figure 3.

Intuitively, in DDPMs the information about \(x_{0}\) that is contained in \(x_{t+1}\) is nested into the information about \(x_{0}\) that is contained in \(x_{t}\). That is why knowing \(x_{t}\) allows us to discard \(x_{t+1}\). In star-shaped diffusion, however, all variables contain independent pieces of information about \(x_{0}\) and should all be taken into account when making predictions.

We can write down the variational lower bound as follows:

\[\mathcal{L}^{\textsc{ss}}(\theta)=\mathbb{E}_{q^{\textsc{ss}}}\left[\log p_{ \theta}(x_{0}|x_{1:T})-\sum_{t=2}^{T}\!D_{KL}\left(q^{\textsc{ss}}(x_{t-1}|x_{ 0})\,\|\,p^{\textsc{ss}}_{\theta}(x_{t-1}|x_{t:T})\right]\right.\] (11)

With this VLB, we only need the marginal distributions \(q(x_{t-1}|x_{0})\) to define and train the model, which allows us to use a wider variety of noising distributions. Since conditioning the predictive model \(p_{\theta}(x_{t-1}|x_{t:T})\) on the whole tail \(x_{t:T}\) is typically impractical, we propose a more efficient way to implement the reverse process next.

### Efficient tail conditioning

Instead of using the full tail \(x_{t:T}\), we would like to define some statistic \(G_{t}=\mathcal{G}_{t}(x_{t:T})\) that would extract all information about \(x_{0}\) from the tail \(x_{t:T}\). Formally speaking, we call \(G_{t}\) a _sufficient tail statistic_ if the following equality holds:

\[q^{\textsc{ss}}(x_{t-1}|x_{t:T})=q^{\textsc{ss}}(x_{t-1}|G_{t}).\] (12)

One way to define \(G_{t}\) is to concatenate all the variables \(x_{t:T}\) into a single vector. This, however, is impractical, as its dimension would grow with the size of the tail \(T-t+1\).

The Pitman-Koopman-Darmois (Pitman, 1936) theorem (PKD) states that exponential families admit a sufficient statistic with constant dimensionality. It also states that no other distribution admits one: if such a statistic were to exist, the distribution has to be a member of the exponential family. Inspired by the PKD, we turn to the exponential family of distributions. In the case of star-shaped diffusion, we cannot apply the PKD directly, as it was formulated for i.i.d. samples and our samples are not identically distributed. However, we can still define a sufficient tail statistic \(G_{t}\) for a specific subset of the exponential family, which we call an _exponential family with linear parameterization_:

**Theorem 1**.: _Assume the forward process of a star-shaped model takes the following form:_

\[q^{\textsc{ss}}(x_{t}|x_{0}) =h_{t}(x_{t})\exp\left\{\eta_{t}(x_{0})^{\mathsf{T}}\mathcal{T}(x _{t})-\Omega_{t}(x_{0})\right\},\] (13) \[\eta_{t}(x_{0}) =A_{t}f(x_{0})+b_{t}.\] (14)

Figure 3: Markov reverse process fails to recover realistic images from star-shaped diffusion, while a general reverse process produces realistic images. The top row is equivalent to DDIM at \(\sigma_{t}^{2}=1-\alpha_{t-1}\). A similar effect was also observed by Bansal et al. (2022).

_Let \(G_{t}\) be a tail statistic, defined as follows:_

\[G_{t}=\mathcal{G}_{t}(x_{t:T})=\sum_{s=t}^{T}A_{s}^{\mathsf{T}}\mathcal{T}(x_{s}).\] (15)

_Then, \(G_{t}\) is a sufficient tail statistic:_

\[q^{\,\textsc{ss}}(x_{t-1}|x_{t:T})=q^{\,\textsc{ss}}(x_{t-1}|G_{t}).\] (16)

Here definition (13) is the standard definition of the exponential family, where \(h_{t}(x_{t})\) is _the base measure_, \(\eta_{t}(x_{0})\) is the vector of _natural parameters_ with corresponding _sufficient statistics_\(\mathcal{T}(x_{t})\), and \(\Omega_{t}(x_{0})\) is _the log-partition function_. The key assumption added is the _linear parameterization_ of the natural parameters (14). We provide the proof in Appendix C. When \(A_{t}\) is scalar, we denote it as \(a_{t}\) instead.

For the most part, the premise of Theorem 1 restricts the parameterization of the distributions rather than the family of the distributions involved. As we discuss in Appendix F, we found it easy to come up with linear parameterization for a wide range of distributions in the exponential family. For example, we can obtain a linear parameterization for the Beta distribution \(q(x_{t}|x_{0})=\mathrm{Beta}(x_{t};\alpha_{t},\beta_{t})\) using \(x_{0}\) as the mode of the distribution and introducing a new concentration parameter \(\nu_{t}\):

\[\alpha_{t} =1+\nu_{t}x_{0},\] (17) \[\beta_{t} =1+\nu_{t}(1-x_{0}).\] (18)

In this case, \(\eta_{t}(x_{0})=\nu_{t}x_{0}\), \(\mathcal{T}(x_{t})=\log\frac{x_{t}}{1-x_{t}}\), and we can use equation (15) to define the sufficient tail statistic \(G_{t}\). We provide more examples in Appendix F. We also provide an implementation-ready reference sheet for a wide range of distributions in the exponential family in Table 6.

We suspect that, just like in PKD, this trick is only possible for a subset of the exponential family. In the general case, the dimensionality of the sufficient tail statistic \(G_{t}\) would have to grow with the size of the tail \(x_{t:T}\). It is still possible to apply SS-DDPM in this case, however, crafting the (now only approximately) sufficient statistic \(G_{t}\) would require more careful consideration and we leave it for future work.

### Final model definition

To maximize the VLB (11), each step of the reverse process should approximate the true reverse distribution:

\[p_{\theta}^{\,\textsc{ss}}(x_{t-1}|x_{t:T})\approx q^{\,\textsc{ss}}(x_{t-1}| x_{t:T})=\int q^{\,\textsc{ss}}(x_{t-1}|x_{0})q^{\,\textsc{ss}}(x_{0}|x_{t:T})dx _{0}.\] (19)

Similarly to DDPM (Ho et al., 2020), we choose to approximate \(q^{\,\textsc{ss}}(x_{0}|x_{t:T})\) with a delta function centered at the prediction of some model \(x_{\theta}(\mathcal{G}_{t}(x_{t:T}),t)\). This results in the following definition of the reverse process of SS-DDPM:

\[p_{\theta}^{\,\textsc{ss}}(x_{t-1}|x_{t:T})=\left.q^{\,\textsc{ss}}(x_{t-1}|x _{0})\right|_{x_{0}=x_{\theta}(\mathcal{G}_{t}(x_{t:T}),t)}.\] (20)

The distribution \(p_{\theta}^{\,\textsc{ss}}(x_{0}|x_{1:T})\) can be fixed to some small-variance distribution \(p_{\theta}^{\,\textsc{ss}}(x_{0}|\hat{x}_{0})\) centered at the final prediction \(\hat{x}_{0}=x_{\theta}(\mathcal{G}_{1}(x_{1:T}),1)\), similar to the dequantization term, commonly used in DDPM. If this distribution has no trainable parameters, the corresponding term can be removed from the training objective. This dequantization distribution would then only be used for log-likelihood estimation and, optionally, for sampling.

Together with the forward process (7) and the VLB objective (11), this concludes the general definition of the SS-DDPM model. The model structure is illustrated in Figure 2. The corresponding training and sampling algorithms are provided in Algorithms 1 and 2.

The resulting model is similar to DDPM in spirit. We follow the same principles when designing the forward process: starting from a low-variance distribution, centered at \(x_{0}\) at \(t=1\), we gradually increase the entropy of the distribution \(q^{\,\textsc{ss}}(x_{t}|x_{0})\) until there is no information shared between \(x_{0}\) and \(x_{t}\) at \(t=T\).

We provide concrete definitions for Beta, Gamma, Dirichlet, von Mises, von Mises-Fisher, Wishart, Gaussian and Categorical distributions in Appendix F.

### Duality between star-shaped and Markovian diffusion

While the variables \(x_{1:T}\) follow a star-shaped diffusion process, the corresponding tail statistics \(G_{1:T}\) form a Markov chain:

\[G_{t}=\sum_{s=t}^{T}A_{s}^{\mathsf{T}}\mathcal{T}(x_{s})=G_{t+1}+A_{t}^{\mathsf{ T}}\mathcal{T}(x_{t}),\] (21)

since \(x_{t}\) is conditionally independent from \(G_{t+2:T}\) given \(G_{t+1}\) (see Appendix E for details). Moreover, we can rewrite the probabilistic model in terms of \(G_{t}\) and see that variables \((x_{0},G_{1:T})\) form a (not necessarily Gaussian) DDPM.

In the case of Gaussian distributions, this duality makes SS-DDPM and DDPM equivalent. This equivalence can be shown explicitly:

**Theorem 2**.: _Let \(\widetilde{\alpha}_{t}^{\textsc{dDPM}}\) define the noising schedule for a DDPM model (1-2) via \(\beta_{t}=(\overline{\alpha}_{t-1}^{\textsc{dDPM}}-\overline{\alpha}_{t}^{ \textsc{dDPM}})/\overline{\alpha}_{t-1}^{\textsc{dDPM}}\). Let \(q^{\textsc{ss}}(x_{0:T})\) be a Gaussian SS-DDPM forward process with the following noising schedule and sufficient tail statistic:_

\[q^{\textsc{ss}}(x_{t}|x_{0}) =\mathcal{N}\left(x_{t};\sqrt{\widetilde{\alpha}_{t}^{\textsc{ss }}}x_{0},1-\overline{\alpha}_{t}^{\textsc{ss}}\right),\] (22) \[\mathcal{G}_{t}(x_{t:T}) =\frac{1-\overline{\alpha}_{t}^{\textsc{dDPM}}}{\sqrt{\widetilde {\alpha}_{t}^{\textsc{dDPM}}}}\sum_{s=t}^{T}\frac{\sqrt{\overline{\alpha}_{s }^{\textsc{ss}}}x_{s}}{1-\overline{\alpha}_{s}^{\textsc{ss}}},\text{ where}\] (23) \[\frac{\overline{\alpha}_{t}^{\textsc{ss}}}{1-\overline{\alpha}_{ t}^{\textsc{ss}}} =\frac{\overline{\alpha}_{t}^{\textsc{dDPM}}}{1-\overline{\alpha}_{t}^{\textsc{ dDPM}}}-\frac{\overline{\alpha}_{t+1}^{\textsc{dDPM}}}{1-\overline{\alpha}_{t+1}^{ \textsc{dDPM}}}.\] (24)

_Then the tail statistic \(G_{t}\) follows a Gaussian DDPM noising process \(q^{\textsc{dDPM}}(x_{0:T})|_{x_{1:T}=G_{1:T}}\) defined by the schedule \(\overline{\alpha}_{t}^{\textsc{dDPM}}\). Moreover, the corresponding reverse processes and VLB objectives are also equivalent._

We show this equivalence in Appendix D. We make use of this connection when choosing the noising schedule for other distributions.

This equivalence means that SS-DDPM is a direct generalization of Gaussian DDPM. While admitting the Gaussian case, SS-DDPM can also be used to implicitly define a non-Gaussian DDPM in the space of sufficient tail statistics for a wide range of distributions.

## 3 Practical considerations

While the model is properly defined, there are several practical considerations that are important for the efficiency of star-shaped diffusion.

Choosing the right scheduleIt is important to choose the right noising schedule for a SS-DDPM model. It significantly depends on the number of diffusion steps \(T\) and behaves differently given different noising schedules, typical to DDPMs. This is illustrated in Figure 4, where we show the noising schedules for Gaussian SS-DDPMs that are equivalent to DDPMs with the same cosine schedule.

Since the variables \(G_{t}\) follow a DDPM-like process, we would like to somehow reuse those DDPM noising schedules that are already known to work well. For Gaussian distributions, we can transform a DDPM noising schedule into the corresponding SS-DDPM noising schedule analytically by equating \(I(x_{0};G_{t})=I(x_{0};x_{t}^{\textsc{DDPM}})\). In general case, we look for schedules that have approximately the same level of mutual information \(I(x_{0};G_{t})\) as the corresponding mutual information \(I(x_{0};x_{t}^{\textsc{DDPM}})\) for a DDPM model for all timesteps \(t\). We estimate the mutual information using Kraskov (Kraskov et al., 2004) and DSIVI (Molchanov et al., 2019) estimators and build a look-up table to match the noising schedules. This procedure is described in more detail in Appendix G. The resulting schedule for the Beta SS-DDPM is illustrated in Figure 5. Note how with the right schedule appropriately normalized tail statistics \(G_{t}\) look and function similarly to the samples \(x_{t}\) from the corresponding Gaussian DDPM. We further discuss this in Appendix H.

Implementing the samplerDuring sampling, we can grow the tail statistic \(G_{t}\) without any overhead, as described in Algorithm 2. However, during training, we need to sample the tail statistic for each object to estimate the loss function. For this we need to sample the full tail \(x_{t:T}\) from the forward process \(q^{\textsc{SS}}(x_{t:T}|x_{0})\), and then compute the tail statistic \(G_{t}\). In practice, this does not add a noticeable overhead and can be computed in parallel to the training process if needed.

Reducing the number of stepsWe can sample from DDPMs more efficiently by skipping some timestamps. This wouldn't work for SS-DDPM, because changing the number of steps would require changing the noising schedule and, consequently, retraining the model.

However, we can still use a similar trick to reduce the number of function evaluations. Instead of skipping some timestamps \(x_{t_{1}+1:t_{2}-1}\), we can draw them from the forward process using the current prediction \(x_{\theta}(G_{t_{2}},t_{2})\), and then use these samples to obtain the tail statistic \(G_{t_{1}}\). For Gaussian SS-DDPM this is equivalent to skipping these timestamps in the corresponding DDPM. In general case, it amounts to approximating the reverse process with a different reverse process:

\[p_{\theta}^{\textsc{SS}}(x_{t_{1}:t_{2}}|G_{t_{2}})=\prod_{t=t_{1}}^{t_{2}}\left. q^{\textsc{SS}}(x_{t}|x_{0})\right|_{x_{0}=x_{\theta}(G_{t},t)}\approx\prod_{t=t_{1}}^ {t_{2}}\left.q^{\textsc{SS}}(x_{t}|x_{0})\right|_{x_{0}=x_{\theta}(G_{t_{2}},t _{2})}.\] (25)

We observe a similar dependence on the number of function evaluations for SS-DDPMs and DDPMs.

Time-dependent tail normalizationAs defined in Theorem 1, the tail statistics can have vastly different scales for different timestamps. The values of coefficients \(a_{t}\) can range from thousandths when \(t\) approaches \(T\) to thousands when \(t\) approaches zero. To make the tail statistics suitable for use in neural networks, proper normalization is crucial. In most cases, we collect the time-dependent means and variances of the tail statistics across the training dataset and normalize the tail statistics to zero mean and unit variance. We further discuss this issue in Appendix H.

Architectural choicesTo make training the model easier, we make some minor adjustments to the neural network architecture and the loss function.

Figure 4: The noising schedule \(\overline{a}_{t}^{\textsc{SS}}\) for Gaussian star-shaped diffusion, defined for different numbers of steps \(T\) using eq. (24). All the corresponding equivalent DDPMs have the same cosine schedule \(\overline{a}_{t}^{\textsc{DDPM}}\).

Figure 5: Top: samples \(x_{t}\) from a Gaussian DDPM forward process with a cosine noise schedule. Bottom: samples \(G_{t}\) from a Beta SS-DDPM forward process with a noise schedule obtained by matching the mutual information. Middle: corresponding samples \(x_{t}\) from that Beta SS-DDPM forward process. The tail statistics have the same level of noise as \(x_{t}^{\textsc{DDPM}}\), while the samples \(x_{t}^{\textsc{BetaSS}}\) are diffused much faster.

Our neural networks \(x_{\theta}(G_{t},t)\) take the tail statistic \(G_{t}\) as an input and are expected to produce an estimate of \(x_{0}\) as an output. In SS-DDPM the data \(x_{0}\) might lie on some manifold, like the unit sphere or the space of positive definite matrices. Therefore, we need to map the neural network output to that manifold. We do that on a case-by-case basis, as described in Appendices I-L.

Different terms of the VLB can have drastically different scales. For this reason, it is common practice to train DDPMs with a modified loss function like \(L_{simple}\) rather than the VLB to improve the stability of training (Ho et al., 2020). Similarly, we can optimize a reweighted variational lower bound when training SS-DDPMs.

## 4 Related works

Our work builds upon Denoising Diffusion Probabilistic Models (Ho et al., 2020). Interest in diffusion models has increased recently due to their impressive results in image (Ho et al., 2020; Song et al., 2020; Dariwal and Nichol, 2021) and audio (Popov et al., 2021; Liu et al., 2022) generation.

SS-DDPM is most closely related to DDPM. Like DDPM, we only rely on variational inference when defining and working with our model. SS-DDPM can be seen as a direct generalization of DDPM and essentially is a recipe for defining DDPMs with non-Gaussian distributions. The underlying non-Gaussian DDPM is constructed implicitly and can be seen as dual to the star-shaped formulation that we use throughout the paper.

Other ways to construct non-Gaussian DDPMs include Binomial diffusion (Sohl-Dickstein et al., 2015), Multinomial diffusion (Hoogeboom et al., 2021) and Gamma diffusion (Nachmani et al., 2021). Each of these works presents a separate derivation of the resulting objective, and extending them to other distributions is not straightforward. On the other hand, SS-DDPM provides a single recipe for a wide range of distributions.

DDPMs have several important extensions. Song et al. (2020) provide a family of non-Markovian diffusions that all result in the same training objective as DDPM. One of them, denoted Denoising Diffusion Implicit Model (DDIM), results in an efficient deterministic sampling algorithm that requires a much lower number of function evaluations than conventional stochastic sampling. Their derivations also admit a star-shaped forward process (at \(\sigma_{t}^{2}=1-\alpha_{t-1}\)), however, the model is not studied in the star-shaped regime. Their reverse process remains Markovian, which we show to not be sufficient to invert a star-shaped forward process. Denoising Diffusion Restoration Models (Kawar et al., 2022) provide a way to solve general linear inverse problems using a trained DDPM. They can be used for image restoration, inpainting, colorization and other conditional generation tasks. Both DDIMs and DDRMs rely on the explicit form of the underlying DDPM model and are derived for Gaussian diffusion. Extending these models to SS-DDPMs is a promising direction for future work.

Song et al. (2020) established the connection between DDPMs and models based on score matching. This connection gives rise to continuous-time variants of the models, deterministic solutions and more precise density estimation using ODEs. We suspect that a similar connection might hold for SS-DDPMs as well, and it can be investigated further in future works.

Other works that present diffusion-like models with other types of noise or applied to manifold data, generally stem from score matching rather than variational inference. Flow Matching (Lipman et al., 2022) is an alternative probabilistic framework that works with any differentiable degradation process. De Bortoli et al. (2022) and Huang et al. (2022) extended score matching to Riemannian manifolds, and Chen and Lipman (2023) proposed Riemannian Flow Matching. Bansal et al. (2022) proposed Cold Diffusion, a non-probabilistic approach to reversing general degradation processes.

To the best of our knowledge, for the first time, an approach for constructing diffusion without a consecutive process was proposed by Rissanen et al. (2022) (IHDM) and further expanded on by Daras et al. (2022) and Hoogeboom and Salimans (2022). IHDM uses a similar star-shaped structure that results in a similar variational lower bound. Adding a deterministic process based on the heat equation allows the authors to keep the reverse process Markovian without having to introduce the tail statistics. As IHDM heavily relies on blurring rather than adding noise, the resulting diffusion dynamics become very different. Conceptually our work is much closer to DDPM than IHDM.

Experiments

We evaluate SS-DDPM with different families of noising distributions. The experiment setup, training details and hyperparameters are listed in Appendices I-L.

Synthetic dataWe consider two examples of star-shaped diffusion processes with Dirichlet and Wishart noise to generate data from the probabilistic simplex and from the manifold of p.d. matrices respectively. We compare them to DDPM, where the predictive network \(x_{\theta}(x_{t},t)\) is parameterized to always satisfy the manifold constraints. As seen in Table 1, using the appropriate distribution results in a better approximation. The data and modeled distributions are illustrated in Table 3. This shows the ability of SS-DDPM to work with different distributions and generate data from exotic domains.

Geodesic dataWe apply SS-DDPM to a geodesic dataset of fires on the Earth's surface (EOSDIS, 2020) using a three-dimensional von Mises-Fisher distribution. The resulting samples and the source data are illustrated in Table 3. We find that SS-DDPM is not too sensitive to the distribution family and can fit data in different domains.

Discrete dataCategorical SS-DDPM is similar to Multinomial Text Diffusion (MTD, (Hoogeboom et al., 2021)). However, unlike in the Gaussian case, these models are not strictly equivalent. We follow a similar setup to MTD and apply Categorical SS-DDPM to unconditional text generation on the text8 dataset (Mahoney, 2011). As shown in Table 2, SS-DDPM achieves similar results to MTD, allowing to use different distributions in a unified manner. While D3PM (Austin et al., 2021) provides some improvements, we follow a simpler setup from MTD. We expect the improvements from D3PM to directly apply to Categorical SS-DDPM.

Image dataFinally, we evaluate SS-DDPM on CIFAR-10. Since the training data is constrained to a \([0,1]\) segment, we use Beta distributions. We evaluate our model with various numbers of generation steps, as described in equation (25), and report the resulting Frechet Inception Distance (FID, (Heusel et al., 2017)) in Figure 6. Beta SS-DDPM achieves comparable quality with the Improved DDPM (Nichol and Dhariwal, 2021) and is slightly better on lower numbers of steps. As expected, DDIM performs better when the number of diffusion steps is low, but both SS-DDPM and DDPM outperform DDIM on longer runs. The best FID score achieved by Beta SS-DDPM is \(3.17\). Although the FID curves for DDPM and DDIM do not achieve this score in Figure 6, Ho et al. (2020) reported an FID score of \(3.17\) for \(1000\) DDPM steps, meaning that SS-DDPM performs similarly to DDPM in this setting.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & Dirichlet & Wishart \\ \hline DDPM & \(0.200\) & \(0.096\) \\ SS-DDPM & \(\mathbf{0.011}\) & \(\mathbf{0.037}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: KL divergence between the real data distribution and the model distribution \(D_{KL}\left(q(x_{0})\,\|\,p_{\theta}(x_{0})\right)\) for Gaussian DDPM and SS-DDPM.

\begin{table}
\begin{tabular}{c c} \hline \hline Model & NLL (bits/char) \\ \hline MTD & \(\leq 1.72\) \\ SS-DDPM & \(\leq\mathbf{1.69}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparison of Categorical SS-DDPM and Multinomial Text Diffusion on text8. NLL is estimated via ELBO.

Figure 6: Quality of images, generated using Beta SS-DDPM, DDPM and DDIM with different numbers of sampling steps. Models are trained and evaluated on CIFAR-10. DDPM and DDIM results were taken from Nichol and Dhariwal (2021).

## 6 Conclusion

We propose an alternative view on diffusion-like probabilistic models. We reveal the duality between star-shaped and Markov diffusion processes that allows us to go beyond Gaussian noise by switching to a star-shaped formulation. It allows us to define diffusion-like models with arbitrary noising distributions and to establish diffusion processes on specific manifolds. We propose an efficient way to construct a reverse process for such models in the case when the noising process lies in a general subset of the exponential family and show that star-shaped diffusion models can be trained on a variety of domains with different noising distributions. On image data, star-shaped diffusion with Beta distributed noise attains comparable performance to Gaussian DDPM, challenging the optimality of Gaussian noise in this setting. The star-shaped formulation opens new applications of diffusion-like probabilistic models, especially for data from exotic domains where domain-specific non-Gaussian diffusion is more appropriate.

## Acknowledgements

We are grateful to Sergey Kholkin for additional experiments with von Mises-Fisher SS-DDPM on geodesic data and to Tingir Badmaev for helpful recommendations in experiments on image data. We'd also like to thank Viacheslav Meshchaninov for sharing his knowledge of diffusion models for text data. This research was supported in part through computational resources of HPC facilities at HSE University. The results on image data (**Image data** in Section 5; Section L) were obtained by Andrey Okhotin and Aibek Alanov with the support of the grant for research centers in the field of AI provided by the Analytical Center for the Government of the Russian Federation (ACRF) in accordance with the agreement on the provision of subsidies (identifier of the agreement 000000D730321P5Q0002) and the agreement with HSE University No. 70-2021-00139.

## References

* Arjovsky et al. (2017) Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein gan. arxiv 2017. _arXiv preprint arXiv:1701.07875_, 30:4, 2017.
* Arjovsky et al. (2018)

\begin{table}
\begin{tabular}{c c c} \hline \hline von Mises–Fisher & Dirichlet & Wishart \\ \hline \hline \end{tabular}
\end{table}
Table 3: Experiments results. The first row is real data and the second is generated samples. For the von Mises–Fisher and Dirichlet models, we show two-dimensional histograms of samples. For the Wishart model, we draw ellipses, corresponding to the p.d. matrices \(x_{0}\) and \(x_{\theta}\). The darker the pixel, the more ellipses pass through that pixel.

Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den Berg, R. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* Bansal et al. (2022) Bansal, A., Borgnia, E., Chu, H.-M., Li, J. S., Kazemi, H., Huang, F., Goldblum, M., Geiping, J., and Goldstein, T. Cold diffusion: Inverting arbitrary image transforms without noise. _arXiv preprint arXiv:2208.09392_, 2022.
* Brock et al. (2018) Brock, A., Donahue, J., and Simonyan, K. Large scale gan training for high fidelity natural image synthesis. _arXiv preprint arXiv:1809.11096_, 2018.
* Chen & Lipman (2023) Chen, R. T. and Lipman, Y. Riemannian flow matching on general geometries. _arXiv preprint arXiv:2302.03660_, 2023.
* Chen et al. (2019) Chen, R. T., Behrmann, J., Duvenaud, D. K., and Jacobsen, J.-H. Residual flows for invertible generative modeling. _Advances in Neural Information Processing Systems_, 32, 2019.
* Daras et al. (2022) Daras, G., Delbracio, M., Talebi, H., Dimakis, A. G., and Milanfar, P. Soft diffusion: Score matching for general corruptions. _arXiv preprint arXiv:2209.05442_, 2022.
* De Bortoli et al. (2022) De Bortoli, V., Mathieu, E., Hutchinson, M., Thornton, J., Teh, Y. W., and Doucet, A. Riemannian score-based generative modeling. _arXiv preprint arXiv:2202.02763_, 2022.
* Dhariwal & Nichol (2021) Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. _arXiv preprint arXiv:2105.05233_, 2021.
* EOSDIS (2020) EOSDIS. Land, atmosphere near real-time capability for eos (lance) system operated by nasa's earth science data and information system (esdis). https://earthdata.nasa.gov/earth-observation-data/near-real-time/firms/active-fire-data, 2020.
* Goodfellow et al. (2014) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. _Advances in neural information processing systems_, 27, 2014.
* Grathwohl et al. (2018) Grathwohl, W., Chen, R. T., Bettencourt, J., Sutskever, I., and Duvenaud, D. Ffjord: Free-form continuous dynamics for scalable reversible generative models. _arXiv preprint arXiv:1810.01367_, 2018.
* Gulrajani et al. (2017) Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. Improved training of wasserstein gans. _Advances in neural information processing systems_, 30, 2017.
* He et al. (2015) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. corr abs/1512.03385 (2015), 2015.
* Heusel et al. (2017) Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.
* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. _arXiv preprint arXiv:2006.11239_, 2020.
* Hoogeboom & Salimans (2022) Hoogeboom, E. and Salimans, T. Blurring diffusion models. _arXiv preprint arXiv:2209.05557_, 2022.
* Hoogeboom et al. (2021) Hoogeboom, E., Nielsen, D., Jaini, P., Forre, P., and Welling, M. Argmax flows and multinomial diffusion: Learning categorical distributions. _Advances in Neural Information Processing Systems_, 34:12454-12465, 2021.
* Huang et al. (2022) Huang, C.-W., Aghajohari, M., Bose, J., Panangaden, P., and Courville, A. C. Riemannian diffusion models. _Advances in Neural Information Processing Systems_, 35:2750-2761, 2022.
* Karras et al. (2019) Karras, T., Laine, S., and Aila, T. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 4401-4410, 2019.
* Krizhevsky et al. (2014)Karras, T., Aittala, M., Laine, S., Harkonen, E., Hellsten, J., Lehtinen, J., and Aila, T. Alias-free generative adversarial networks. _Advances in Neural Information Processing Systems_, 34, 2021.
* Kawar et al. (2022) Kawar, B., Elad, M., Ermon, S., and Song, J. Denoising diffusion restoration models. In _Advances in Neural Information Processing Systems_, 2022.
* Kingma and Ba (2015) Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In _International Conference on Learning Representations_, 2015.
* Kingma and Welling (2013) Kingma, D. P. and Welling, M. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* Kingma et al. (2021) Kingma, D. P., Salimans, T., Poole, B., and Ho, J. Variational diffusion models. _arXiv preprint arXiv:2107.00630_, 2, 2021.
* Kraskov et al. (2004) Kraskov, A., Stogbauer, H., and Grassberger, P. Estimating mutual information. _Physical review E_, 69(6):066138, 2004.
* Lipman et al. (2022) Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. _arXiv preprint arXiv:2210.02747_, 2022.
* Liu et al. (2022) Liu, S., Su, D., and Yu, D. Diffgan-tts: High-fidelity and efficient text-to-speech with denoising diffusion gans. _arXiv preprint arXiv:2201.11972_, 2022.
* Luo and Hu (2021) Luo, S. and Hu, W. Diffusion probabilistic models for 3d point cloud generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 2837-2845, 2021.
* Mahoney (2011) Mahoney, M. Large text compression benchmark, 2011. URL http://www.mattmahoney.net/dc/text.html.
* Molchanov et al. (2019) Molchanov, D., Kharitonov, V., Sobolev, A., and Vetrov, D. Doubly semi-implicit variational inference. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pp. 2593-2602. PMLR, 2019.
* Nachmani et al. (2021) Nachmani, E., Roman, R. S., and Wolf, L. Denoising diffusion gamma models. _arXiv preprint arXiv:2110.05948_, 2021.
* Nichol and Dhariwal (2021) Nichol, A. and Dhariwal, P. Improved denoising diffusion probabilistic models. _arXiv preprint arXiv:2102.09672_, 2021.
* Pitman (1936) Pitman, E. J. G. Sufficient statistics and intrinsic accuracy. _Mathematical Proceedings of the Cambridge Philosophical Society_, 32(4):567-579, 1936. doi: 10.1017/S0305004100019307.
* Popov et al. (2021) Popov, V., Vovk, I., Gogoryan, V., Sadekova, T., and Kudinov, M. Grad-tts: A diffusion probabilistic model for text-to-speech. In _International Conference on Machine Learning_, pp. 8599-8608. PMLR, 2021.
* Ramesh et al. (2021) Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pp. 8821-8831. PMLR, 2021.
* Rezende et al. (2014) Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In _International conference on machine learning_, pp. 1278-1286. PMLR, 2014.
* Rissanen et al. (2022) Rissanen, S., Heinonen, M., and Solin, A. Generative modelling with inverse heat dissipation. _arXiv preprint arXiv:2206.13397_, 2022.
* Saharia et al. (2021) Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D. J., and Norouzi, M. Image super-resolution via iterative refinement. _arXiv preprint arXiv:2104.07636_, 2021.
* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pp. 2256-2265. PMLR, 2015.
* Sohl-Dickstein et al. (2015)Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020a.
* Song et al. (2020) Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. _arXiv preprint arXiv:2011.13456_, 2020b.
* Thanh-Tung and Tran (2020) Thanh-Tung, H. and Tran, T. Catastrophic forgetting and mode collapse in gans. In _2020 International Joint Conference on Neural Networks (IJCNN)_, pp. 1-10. IEEE, 2020.
* Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Xiao et al. (2020) Xiao, Z., Kreis, K., Kautz, J., and Vahdat, A. Vaebm: A symbiosis between variational autoencoders and energy-based models. _arXiv preprint arXiv:2010.00654_, 2020.
* Yin and Zhou (2018) Yin, M. and Zhou, M. Semi-implicit variational inference. In _International Conference on Machine Learning_, pp. 5660-5669. PMLR, 2018.
* Zhang et al. (2021) Zhang, L., Goldstein, M., and Ranganath, R. Understanding failures in out-of-distribution detection with deep generative models. In _International Conference on Machine Learning_, pp. 12427-12436. PMLR, 2021.
* Zhao et al. (2018) Zhao, S., Ren, H., Yuan, A., Song, J., Goodman, N., and Ermon, S. Bias and generalization in deep generative models: An empirical study. _Advances in Neural Information Processing Systems_, 31, 2018.
* Zhou et al. (2021) Zhou, L., Du, Y., and Wu, J. 3d shape generation and completion through point-voxel diffusion. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 5826-5835, 2021.

## Appendix A Variational lower bound for the SS-DDPM model

\[\mathcal{L}^{\textsc{ss}}(\theta) =\mathbb{E}_{q^{\textsc{ss}}(x_{0:T})}\log\frac{p_{\theta}^{ \textsc{ss}}(x_{0:T})}{q^{\textsc{ss}}(x_{1:T}|x_{0})}=\mathbb{E}_{q^{\textsc{ss }}(x_{0:T})}\log\frac{p_{\theta}^{\textsc{ss}}(x_{0}|x_{1:T})p_{\theta}^{ \textsc{ss}}(x_{T})\prod_{t=2}^{T}p_{\theta}^{\textsc{ss}}(x_{t-1}|x_{t:T})}{ \prod_{t=1}^{T}q^{\textsc{ss}}(x_{t}|x_{0})}=\] (26) \[=\mathbb{E}_{q^{\textsc{ss}}(x_{0:T})}\left[\log p_{\theta}^{ \textsc{ss}}(x_{0}|x_{1:T})+\sum_{t=2}^{T}\log\frac{p_{\theta}^{\textsc{ss}}(x_ {t-1}|x_{t:T})}{q^{\textsc{ss}}(x_{t-1}|x_{0})}+\underbrace{\log}\frac{p_{ \theta}^{\textsc{ss}}(x_{T})}{q^{\textsc{ss}}(x_{T}|x_{0})}\right]=\] (27) \[=\mathbb{E}_{q^{\textsc{ss}}(x_{0:T})}\left[\log p_{\theta}^{ \textsc{ss}}(x_{0}|x_{1:T})-\sum_{t=2}^{T}D_{KL}\left(q^{\textsc{ss}}(x_{t-1}|x _{0})\,\|\,p_{\theta}^{\textsc{ss}}(x_{t-1}|x_{t:T})\right)\right]\] (28)

## Appendix B True reverse process for Markovian and Star-Shaped DDPM

If the forward process is Markovian, the corresponding true reverse process is Markovian too:

\[q^{\textsc{dDPM}}(x_{t-1}|x_{t:T})=\frac{q^{\textsc{dDPM}}(x_{t-1:T})}{q^{ \textsc{dDPM}}(x_{t:T})}=\frac{q^{\textsc{dDPM}}(x_{t-1})q^{\textsc{dDPM}}(x _{t}|x_{t-1})\prod_{s=t+1}^{T}q^{\textsc{dDPM}}(\widehat{x_{s}|x_{s-1}})}{q^{ \textsc{dDPM}}(x_{t})\prod_{s=t+1}^{T}q^{\textsc{dDPM}}(\widehat{x_{s}|x_{s- 1}})}=q^{\textsc{dDPM}}(x_{t-1}|x_{t})\] (29)

\[q^{\textsc{dDPM}}(x_{0:T})=q(x_{0})\prod_{t=1}^{T}q^{\textsc{dDPM}}(x_{t}|x_{ t-1})=q^{\textsc{dDPM}}(x_{T})\prod_{t=1}^{T}q^{\textsc{dDPM}}(x_{t-1}|x_{t:T})=q^{ \textsc{dDPM}}(x_{T})\prod_{t=1}^{T}q^{\textsc{dDPM}}(x_{t-1}|x_{t})\] (30)

For star-shaped models, however, the reverse process has a general structure that cannot be reduced further:

\[q^{\textsc{ss}}(x_{0:T})=q(x_{0})\prod_{t=1}^{T}q^{\textsc{ss}}(x_{t}|x_{0})=q ^{\textsc{ss}}(x_{T})\prod_{t=1}^{T}q^{\textsc{ss}}(x_{t-1}|x_{t:T})\] (31)

In this case, a Markovian reverse process can be a very poor approximation to the true reverse process. Choosing such an approximation adds an irreducible gap to the variational lower bound:

\[\mathcal{L}^{\textsc{ss}}_{Markov}(\theta)=\mathbb{E}_{q^{ \textsc{ss}}(x_{0:T})}\log\frac{p_{\theta}(x_{T})\prod_{t=1}^{T}p_{\theta}(x_ {t-1}|x_{t})}{q^{\textsc{ss}}(x_{1:T}|x_{0})}=\] (32) \[=\mathbb{E}_{q^{\textsc{ss}}(x_{0:T})}\log\frac{p_{\theta}(x_{T}) \prod_{t=1}^{T}p_{\theta}(x_{t-1}|x_{t})q(x_{0})\prod_{t=1}^{T}q^{\textsc{ss} }(x_{t-1}|x_{t})}{q^{\textsc{ss}}(x_{T})\prod_{t=1}^{T}q^{\textsc{ss}}(x_{t-1}| x_{t:T})\prod_{t=1}^{T}}=\] (33) \[=\mathbb{E}_{q^{\textsc{ss}}(x_{0:T})}\left[\underbrace{\log q(x_ {0})-\underbrace{D_{KL}\left(q^{\textsc{ss}}(x_{T})\,\|\,p_{\theta}(x_{T}) \right)-\sum_{t=1}^{T}D_{KL}\left(q^{\textsc{ss}}(x_{t-1}|x_{t})\,\|\,p_{\theta }(x_{t-1}|x_{t})\right)}_{\text{Reducible}}-\right.}_{\text{Reducible}}\] (34) \[\left.-\underbrace{\sum_{t=1}^{T}D_{KL}\left(q^{\textsc{ss}}(x_{t- 1}|x_{t:T})\,\|\,q^{\textsc{ss}}(x_{t-1}|x_{t})\right)}_{\text{Irreducible}}\right]\] (35)

Intuitively, there is little information shared between \(x_{t-1}\) and \(x_{t}\), as they are conditionally independent given \(x_{0}\). Therefore, we would expect the distribution \(q^{\textsc{ss}}(x_{t-1}|x_{t})\) to have a much higher entropy than the distribution \(q^{\textsc{ss}}(x_{t-1}|x_{t:T})\), making the irreducible gap (35) large. The dramatic effect of this gap is illustrated in Figure 3.

This gap can also be computed analytically for Gaussian DDPMs when the data is coming from a standard Gaussian distribution \(q(x_{0})=\mathcal{N}\left(x_{0};0,1\right)\). According to equations (34-35), the best Markovian reverse process in this case is \(p_{\theta}(x_{t-1}|x_{t})=q^{\textsc{ss}}(x_{t-1}|x_{t})\). It results in the following value of the variational lower bound:

\[\mathcal{L}^{\textsc{ss}}_{Markov}=-\mathcal{H}[q(x_{0})]+\mathcal{H}[q^{ \textsc{ss}}(x_{0:T})]-\mathcal{H}[q^{\textsc{ss}}(x_{T})]-\frac{1}{2}\sum_{t =1}^{T}\left[1+\log(2\pi(1-\overline{\alpha}^{\textsc{ss}}_{t-1}\overline{ \alpha}^{\textsc{ss}}_{t}))\right]\] (36)If the reverse process is matched exactly (and has a general structure), the variational lower bound reduces to the negative entropy of the data distribution:

\[\mathcal{L}_{*}^{\textsc{ss}}=\mathbb{E}_{q^{\textsc{ss}}(x_{0:T})}\log\frac{p_{ *}^{\textsc{ss}}(x_{0:T})}{q^{\textsc{ss}}(x_{1:T}|x_{0})}=\] (37)

\[=\mathbb{E}_{q^{\textsc{ss}}(x_{0:T})}\log\frac{q^{\textsc{ss}}(x_{0:T})}{q^{ \textsc{ss}}(x_{1:T}|x_{0})}=\] (38)

\[=\mathbb{E}_{q(x_{0})}\log q(x_{0})=-\frac{1}{2}\log(2\pi)-\frac{1}{2}\] (39)

As shown in Figure 7, the Markovian approximation adds a substantial irreducible gap to the variational lower bound.

## Appendix C Proof of Theorem 1

We start by establishing the following lemma. It would allow us to change the variables in the condition of a conditional distribution. Essentially, we would like to show that if a variable \(y\) is only dependent on \(x\) through some function \(h(x)\), we can write the distribution \(p(y|x)\) as \(p(y|z)|_{z=h(x)}\).

**Lemma 1**.: _Assume random variables \(x\) and \(y\) follow a joint distribution \(p(x,y)=p(x)p(y|x)\), where \(p(y|x)\) is given by \(f(y,h(x))\). Then \(p(y|x)=p(y|z)|_{z=h(x)}\), where the variable \(z\) is defined as \(z=h(x)\)._

Proof.: We can write down the joint distribution over all variables as follows:

\[p(x,y,z)=p(x)f(y,h(x))\delta(z-h(x)).\] (40)

By integrating \(x\) out, we get

\[p(y,z)=\int p(x,y,z)dx =\int p(x)f(y,h(x))\delta(z-h(x))dx=\int p(x)f(y,z)\delta(z-h(x))dx=\] (41) \[=f(y,z)\int p(x)\delta(z-h(x))dx=f(y,z)p(z)\] (42)

Finally, we obtain

\[p(y|z)|_{z=h(x)}=\left.\frac{p(y,z)}{p(z)}\right|\big{|}_{z=h(x)}=f(y,z)|_{z=h (x)}=f(y,h(x))=p(y|x)\] (43)

**Theorem 1**.: _Assume the forward process of a star-shaped model takes the following form:_

\[q^{\textsc{ss}}(x_{t}|x_{0}) =h_{t}(x_{t})\exp\left\{\eta_{t}(x_{0})^{\mathsf{T}}\mathcal{T}(x _{t})-\Omega_{t}(x_{0})\right\},\] (13) \[\eta_{t}(x_{0}) =A_{t}f(x_{0})+b_{t}.\] (14)

_Let \(G_{t}\) be a tail statistic, defined as follows:_

\[G_{t}=\mathcal{G}_{t}(x_{t:T})=\sum_{s=t}^{T}\!A_{s}^{\mathsf{T}}\mathcal{T}( x_{s}).\] (15)

_Then, \(G_{t}\) is a sufficient tail statistic:_

\[q^{\textsc{ss}}(x_{t-1}|x_{t:T})=q^{\textsc{ss}}(x_{t-1}|G_{t}).\] (16)

Proof.: \[q^{\textsc{ss}}(x_{t-1}|x_{t:T})=\int q^{\textsc{ss}}(x_{t-1}|x_{0})q^{ \textsc{ss}}(x_{0}|x_{t:T})dx_{0}\] (44)

\[q^{\textsc{ss}}(x_{0}|x_{t:T})=\frac{q(x_{0})\prod_{s=t}^{T}q^{\textsc{ss}}(x _{s}|x_{0})}{q^{\textsc{ss}}(x_{t:T})}=\frac{q(x_{0})}{q^{\textsc{ss}}(x_{t:T} )}\left(\prod_{s=t}^{T}h_{s}(x_{s})\right)\exp\left\{\sum_{s=t}^{T}\left(\eta_ {s}(x_{0})^{\mathsf{T}}\mathcal{T}(x_{s})-\Omega_{s}(x_{0})\right)\right\}=\] (45)

Figure 7: Variational lower bound for a Gaussian Star-Shaped DDPM model computed with the true reverse process and with the best Markovian approximation.

\[q^{{}^{\text{SDPM}}}(x_{t}|x_{t-1}) =\mathcal{N}\left(x_{t};\sqrt{\overline{\alpha}_{t}^{{}^{\text{DDPM} }}}x_{0},(1-\overline{\alpha}_{t}^{{}^{\text{DDPM}}})\mathbf{I}\right)\] (55) \[\alpha_{s}^{{}^{\text{DDPM}}} =1-\beta_{s}^{{}^{\text{DDPM}}}\] (53) \[\overline{\alpha}_{t}^{{}^{\text{DDPM}}} =\prod_{s=1}^{t}\alpha_{s}^{{}^{\text{DDPM}}}\] (54) \[q^{{}^{\text{DDPM}}}(x_{t}|x_{0}) =\mathcal{N}\left(x_{t};\sqrt{\overline{\alpha}_{t}^{{}^{\text{DDPM} }}}x_{0},(1-\overline{\alpha}_{t}^{{}^{\text{DDPM}}})\mathbf{I}\right)\] (55) \[\tilde{\mu}_{t}(x_{t},x_{0}) =\frac{\sqrt{\widetilde{\alpha}_{t-1}^{{}^{\text{DDPM}}}}\beta_{t} }{1-\widetilde{\alpha}_{t}^{{}^{\text{DDPM}}}}x_{0}+\frac{\sqrt{\alpha_{t}^{ {}^{\text{DDPM}}}}(1-\overline{\alpha}_{t-1}^{{}^{\text{DDPM}}})}{1- \widetilde{\alpha}_{t}^{{}^{\text{DDPM}}}}x_{t}\] (56) \[\tilde{\beta}_{t}^{{}^{\text{DDPM}}} =\frac{1-\overline{\alpha}_{t-1}^{{}^{\text{DDPM}}}}{1- \widetilde{\alpha}_{t}^{{}^{\text{DDPM}}}}\beta_{t}\] (57) \[q^{{}^{\text{DDPM}}}(x_{t-1}|x_{t},x_{0}) =\mathcal{N}\left(x_{t-1};\tilde{\mu}_{t}(x_{t},x_{0}),\tilde{ \beta}_{t}^{{}^{\text{DDPM}}}\mathbf{I}\right)\] (58)

[MISSING_PAGE_FAIL:17]

\[=\frac{1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}{\sqrt{ \overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}}\left(\frac{\overline{\alpha}_{t-1}^ {\text{\tiny DDPM}}}{1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}-\frac{ \overline{\alpha}_{t}^{\text{\tiny DDPM}}}{1-\overline{\alpha}_{t-1}^{\text{ \tiny DDPM}}}\right)x_{\theta}(G_{t},t)+\frac{\sqrt{\alpha_{t}^{\text{\tiny DDPM }}}(1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}})}{1-\overline{\alpha}_{t}^ {\text{\tiny DDPM}}}G_{t}=\] (73) \[=\left(\sqrt{\overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}-\frac{( 1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}})\sqrt{\overline{\alpha}_{t-1}^ {\text{\tiny DDPM}}}\alpha_{t}^{\text{\tiny DDPM}}}{1-\overline{\alpha}_{t}^ {\text{\tiny DDPM}}}\right)x_{\theta}(G_{t},t)+\frac{\sqrt{\alpha_{t}^{\text{ \tiny DDPM}}}(1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}})}{1-\overline{ \alpha}_{t}^{\text{\tiny DDPM}}}G_{t}=\] (74) \[=\left(\frac{1-\overline{\alpha}_{t}^{\text{\tiny DDPM}}-(1- \overline{\alpha}_{t-1}^{\text{\tiny DDPM}})\alpha_{t}^{\text{\tiny DDPM}}}{1 -\overline{\alpha}_{t}^{\text{\tiny DDPM}}}\right)\sqrt{\overline{\alpha}_{t- 1}^{\text{\tiny DDPM}}}x_{\theta}(G_{t},t)+\frac{\sqrt{\alpha_{t}^{\text{ \tiny DDPM}}}(1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}})}{1-\overline{ \alpha}_{t}^{\text{\tiny DDPM}}}G_{t}=\] (75) \[=\left(\frac{1-\overline{\alpha}_{t}^{\text{\tiny DDPM}}-\alpha_ {t}^{\text{\tiny DDPM}}+\overline{\alpha}_{t}^{\text{\tiny DDPM}}}{1- \overline{\alpha}_{t}^{\text{\tiny DDPM}}}\right)\sqrt{\overline{\alpha}_{t- 1}^{\text{\tiny DDPM}}}x_{\theta}(G_{t},t)+\frac{\sqrt{\alpha_{t}^{\text{ \tiny DDPM}}}(1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}})}{1-\overline{ \alpha}_{t}^{\text{\tiny DDPM}}}G_{t}=\] (76) \[=\frac{\sqrt{\overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}\beta_{t }}{1-\overline{\alpha}_{t}^{\text{\tiny DDPM}}}x_{\theta}(G_{t},t)+\frac{\sqrt {\alpha_{t}^{\text{\tiny DDPM}}}(1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}} )}{1-\overline{\alpha}_{t}^{\text{\tiny DDPM}}}G_{t}\] (77)

Now, let's derive the variance:

\[\mathbb{D}_{p_{\theta}^{\text{\tiny SS}}(G_{t-1}|G_{t})}G_{t-1}= \left(\frac{1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}{\sqrt{\overline{ \alpha}_{t-1}^{\text{\tiny DSDPM}}}}\frac{\sqrt{\overline{\alpha}_{t-1}^{\text {\tiny SS}}}}{1-\overline{\alpha}_{t-1}^{\text{\tiny SS}}}\right)^{2}(1- \overline{\alpha}_{t-1}^{\text{\tiny SS}})=\] (78) \[=\left(\frac{1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}{\sqrt {\overline{\alpha}_{t-1}^{\text{\tiny ODPM}}}}\sqrt{\overline{1-\overline{ \alpha}_{t-1}^{\text{\tiny SS}}}}\right)^{2}=\frac{(1-\overline{\alpha}_{t-1}^ {\text{\tiny DDPM}})^{2}}{\overline{\alpha}_{t-1}^{\text{\tiny SS}}}\frac{ \overline{\alpha}_{t-1}^{\text{\tiny SS}}}{1-\overline{\alpha}_{t-1}^{\text{ \tiny SS}}}=\frac{(1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}})^{2}}{1- \overline{\alpha}_{t-1}^{\text{\tiny ODPM}}}\frac{(\overline{\alpha}_{t-1}^{ \text{\tiny DDPM}}}{1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}-\frac{ \overline{\alpha}_{t}^{\text{\tiny DDPM}}}{1-\overline{\alpha}_{t}^{\text{ \tiny DDPM}}}\right)=\] (79) \[=(1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}})\left(1-\frac{(1- \overline{\alpha}_{t-1}^{\text{\tiny DDPM}})\alpha_{t}^{\text{\tiny DDPM}}}{1 -\overline{\alpha}_{t}^{\text{\tiny DDPM}}}\right)=\frac{(1-\overline{\alpha}_ {t-1}^{\text{\tiny DDPM}})(1-\overline{\alpha}_{t}^{\text{\tiny DDPM}}-(1- \overline{\alpha}_{t-1}^{\text{\tiny DDPM}})\alpha_{t}^{\text{\tiny DDPM}}}{1 -\overline{\alpha}_{t}^{\text{\tiny DDPM}}}=\] (80) \[=\frac{1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}{1-\overline {\alpha}_{t}^{\text{\tiny DDPM}}}(1-\overline{\alpha}_{t}^{\text{\tiny DDPM}}- \alpha_{t}^{\text{\tiny DDPM}}+\overline{\alpha}_{t}^{\text{\tiny DDPM}})= \frac{1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}{1-\overline{\alpha}_{t }^{\text{\tiny DDPM}}}\beta_{t}^{\text{\tiny DDPM}}=\tilde{\beta}_{t}^{\text{ \tiny DDPM}}\] (81)

Therefore,

\[p_{\theta}^{\text{\tiny SS}}(G_{t-1}|G_{t})=\mathcal{N}\left(G_{t-1};\frac{ \sqrt{\overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}\beta_{t}}{1-\overline{ \alpha}_{t}^{\text{\tiny DDPM}}}x_{\theta}(G_{t},t)+\frac{\sqrt{\alpha_{t}^{ \text{\tiny DDPM}}}(1-\overline{\alpha}_{t-1}^{\text{\tiny DDPM}})}{1- \overline{\alpha}_{t}^{\text{\tiny DDPM}}}G_{t},\tilde{\beta}_{t}^{\text{\tiny DDPM }}\right)=\left.p_{\theta}^{\text{\tiny DDPM}}(x_{t-1}|x_{t})\right|_{x_{t-1,t }=G_{t-1,t}}\] (82)

Finally, we show that the variational lower bounds are the same.

\[\mathcal{L}^{\text{\tiny SS}}(\theta)=\mathbb{E}_{q^{\text{\tiny SS }}(x_{0:T})}\left[\log p_{\theta}^{\text{\tiny SS}}(x_{0}|G_{1})-\sum_{t=2}^{T}D_{KL} \left(q^{\text{\tiny SS}}(x_{t-1}|x_{0})\left\|\,p_{\theta}^{\text{\tiny SS }}(x_{t-1}|G_{t})\right)\right]=\] (83) \[=\mathbb{E}_{q^{\text{\tiny SS}}(x_{0:T})}\left[\log p_{\theta}^{ \text{\tiny SS}}(x_{0}|G_{1})-\sum_{t=2}^{T}D_{KL}\left(q^{\text{\tiny SS }}(x_{t-1}|x_{0})\left\|\,q^{\text{\tiny SS}}(x_{t-1}|x_{0})\right|_{x_{0}=x _{\theta}(G_{t},t)}\right)\right]\] (84)

Since \(G_{1}\) from the star-shaped model is the same as \(x_{1}\) from the DDPM model, the first term \(\log p_{\theta}^{\text{\tiny BS}}(x_{0}|G_{1})\) coincides with \(\log p_{\theta}^{\text{\tiny DDPM}}(x_{0}|x_{1})|_{x_{1}=G_{1}}\).

\[D_{KL}\left(q^{\text{\tiny SS}}(x_{t-1}|x_{0})\left\|\,q^{\text{ \tiny SS}}(x_{t-1}|x_{0})\right|_{x_{0}=x_{\theta}(G_{t},t)}\right)= \frac{\left(\sqrt{\overline{\alpha}_{t-1}^{\text{\tiny SS}}}(x_{0}-x_{\theta}(G_ {t},t))\right)^{2}}{2(1-\overline{\alpha}_{t-1}^{\text{\tiny SS}})}=\] (85) \[=\frac{\overline{\alpha}_{t-1}^{\text{\tiny SS}}}{2(1-\overline{ \alpha}_{t-1}^{\text{\tiny SS}})}(x_{0}-x_{\theta}(G_{t},t))^{2}=\frac{1}{2} \left(\frac{\overline{\alpha}_{t-1}^{\text{\tiny DDPM}}}{1-\overline{ \alpha}_{t-1}^{\text{\tiny DDPM}}}-\frac{\overlineDuality between star-shaped and Markovian diffusion: general case

We assume that \(\mathcal{T}(x_{t})\) and all matrices \(A_{t}\) (except possibly \(A_{T}\)) are invertible. Then, there is a bijection between the set of tail statistics \(G_{t:T}\) and the tail \(x_{t:T}\).

First, we show that the variables \(G_{1:T}\) form a Markov chain.

\[q(G_{t-1}|G_{t:T})=q(G_{t-1}|x_{t:T})=\int q(G_{t-1}|x_{0},x_{t:T})q(x_{0}|x_{t: T})dx_{0}=\] (90)

\[=\int q(G_{t-1}|G_{t},x_{0})q(x_{0}|G_{t})dx_{0}=q(G_{t-1}|G_{t})\] (91)

\[q(x_{0},G_{t:T})=q(x_{0}|G_{1:T})\prod_{t=2}^{T}q(G_{t-1}|G_{t:T})=q(x_{0}|G_{1 })\prod_{t=2}^{T}q(G_{t-1}|G_{t})=q(x_{0})q(G_{1}|x_{0})\prod_{t=2}^{T}q(G_{t} |G_{t-1})\] (92)

The last equation holds since the reverse of a Markov chain is also a Markov chain.

This means that a star-shaped diffusion process on \(x_{1:T}\) implicitly defines some Markovian diffusion process on the tail statistics \(G_{1:T}\). Due to the definition of \(G_{t-1}=G_{t}+A_{t-1}^{\mathsf{T}}\mathcal{T}(x_{t-1})\), we can write down the following factorization of that process:

\[q(x_{0},G_{t:T})=q(x_{0})q(G_{T})\prod_{t=2}^{T}q(G_{t-1}|G_{t},x_{0})\] (93)

The posteriors \(q(G_{t-1}|G_{t},x_{0})\) can then be computed by a change of variables:

\[q(G_{t-1}|G_{t},x_{0})=\left.q(x_{t-1}|x_{0})\right|_{x_{t-1}= \mathcal{T}^{-1}\left(A_{t-1}^{-\mathsf{T}}(G_{t-1}-G_{t})\right)}\cdot\left| \det\left[\frac{d}{dG_{t-1}}\mathcal{T}^{-1}\left(A_{t-1}^{-\mathsf{T}}(G_{t -1}-G_{t})\right)\right]\right|\] (94)

This also allows us to define the reverse model like it was defined in DDPM:

\[p_{\theta}(G_{t-1}|G_{t})=\left.q(G_{t-1}|G_{t},x_{0})\right|_{x_{0}=x_{ \theta}(G_{t},t)}\] (95)

This definition is consistent with the reverse process of SS-DDPM \(p_{\theta}(x_{t-1}|x_{t:T})=\left.q(x_{t-1}|x_{0})\right|_{x_{0}=x_{\theta}( G_{t},t)}\). Now that both the forward and reverse processes are defined, we can write down the corresponding variational lower bound. Because the model is structured exactly like a DDPM, the variational lower bound is going to look the same. However, we can show that it is equivalent to the variational lower bound of SS-DDPM:

\[\mathcal{L}^{\text{\tiny Dual}}(\theta)=\mathbb{E}_{q(x_{0},G_{1:T})}\left[ \log p_{\theta}(x_{0}|G_{1})-\sum_{t=2}^{T}D_{KL}\left(q(G_{t-1}|G_{t},x_{0}) \,\|\,p_{\theta}(G_{t-1}|G_{t})\right)-D_{KL}\left(q(G_{T}|x_{0})\,\|\,p_{ \theta}(G_{T})\right)\right]=\] (96)

\[=\mathbb{E}_{q(x_{0},G_{1:T})}\left[\log p_{\theta}(x_{0}|G_{1})-\sum_{t=2}^ {T}\mathbb{E}_{q(G_{t-1}|G_{t},x_{0})}\log\frac{q(G_{t-1}|G_{t},x_{0})}{p_{ \theta}(G_{t-1}|G_{t})}-D_{KL}\left(q(x_{T}|x_{0})\,\|\,p_{\theta}(x_{T}) \right)\right]=\] (97)

\[=\mathbb{E}_{q(x_{0:T})}\left[\log p_{\theta}(x_{0}|G_{1})-\sum_{t=2}^{T} \mathbb{E}_{q(x_{t-1}|x_{0})}\log\frac{q(x_{t-1}|x_{0})}{p_{\theta}(x_{t-1}|G_ {t})}-D_{KL}\left(q(x_{T}|x_{0})\,\|\,p_{\theta}(x_{T})\right)\right]=\] (98)

\[=\mathbb{E}_{q(x_{0:T})}\left[\log p_{\theta}(x_{0}|G_{1})-\sum_{t=2}^{T}D_{KL }\left(q(x_{t-1}|x_{0})\,\|\,p_{\theta}(x_{t-1}|G_{t})\right)-D_{KL}\left(q(x_ {T}|x_{0})\,\|\,p_{\theta}(x_{T})\right)\right]=\mathcal{L}^{\text{\tiny SS}}(\theta)\] (99)

As we can see, there are two equivalent ways to write down the model. One is SS-DDPM, a star-shaped diffusion model, where we only need to define the marginal transition probabilities \(q(x_{t}|x_{0})\). Another way is to rewrite the model in terms of the tail statistics \(G_{t:T}\). This way we obtain a non-Gaussian DDPM that is implicitly defined by the star-shaped model. Because of this equivalence, we see the star-shaped diffusion of \(x_{1:T}\) and the Markovian diffusion of \(G_{1:T}\) as a pair of dual processes.

SS-DDPM in different families

The main principles for designing a SS-DDPM model are similar to designing a DDPM model. When \(t\) goes to zero, we wish to recover the Dirac delta, centered at \(x_{0}\):

\[q^{\textsc{ss}}(x_{t}|x_{0})\xrightarrow[t\to 0]{}\delta(x_{t}-x_{0})\] (100)

When \(t\) goes to \(T\), we wish to obtain some standard distribution that doesn't depend on \(x_{0}\):

\[q^{\textsc{ss}}(x_{t}|x_{0})\xrightarrow[t\to T]{}q^{\textsc{ss}}_{T}(x_{T})\] (101)

In exponential families with linear parameterization of the natural parameter \(\eta_{t}(x_{0})=a_{t}f(x_{0})+b_{t}\), we can define the schedule by choosing the parameters \(a_{t}\) and \(b_{t}\) that satisfy conditions (100-101). After that, we can use Theorem 1 to define the tail statistic \(\mathcal{G}_{t}(x_{t:T})\) using the sufficient statistic \(\mathcal{T}(x_{t})\) of the corresponding family. However, as shown in the following sections, in some cases linear parameterization admits a simpler sufficient tail statistic.

The following sections contain examples of defining the SS-DDPM model for different families. These results are summarized in Table 6.

### Gaussian

\[q^{\textsc{ss}}(x_{t}|x_{0})=\mathcal{N}\left(x_{t};\sqrt{\alpha ^{\textsc{ss}}_{t}}x_{0},(1-\overline{\alpha}^{\textsc{ss}}_{t})I\right)\] (102)

Since Gaussian SS-DDPM is equivalent to a Markovian DDPM, it is natural to directly reuse the schedule from a Markovian DDPM. As we show in Theorem 2, given a Markovian DDPM defined by \(\overline{\alpha}^{\textsc{dDPM}}_{t}\), the following parameterization will produce the same process in the space of tail statistics:

\[\frac{\overline{\alpha}^{\textsc{ss}}_{t}}{1-\overline{\alpha}^ {\textsc{ss}}_{t}} =\frac{\overline{\alpha}^{\textsc{dDPM}}_{t}}{1-\overline{\alpha}^ {\textsc{dDPM}}_{t}}-\frac{\overline{\alpha}^{\textsc{dDPM}}_{t+1}}{1- \overline{\alpha}^{\textsc{dDPM}}_{t+1}}\] (103) \[\mathcal{G}_{t}(x_{t:T}) =\frac{1-\overline{\alpha}^{\textsc{dDPM}}_{t}}{\sqrt{\overline {\alpha}^{\textsc{dDPM}}_{t}}}\sum_{s=t}^{T}\frac{\sqrt{\overline{\alpha}^{ \textsc{ss}}_{s}}x_{s}}{1-\overline{\alpha}^{\textsc{ss}}_{s}}\] (104)

The KL divergence is computed as follows:

\[D_{KL}\left(q^{\textsc{ss}}(x_{t}|x_{0})\,\|\,p^{\textsc{ss}}_{ \theta}(x_{t}|G_{t})\right)=\frac{\overline{\alpha}^{\textsc{ss}}_{t}(x_{0}-x _{\theta})^{2}}{2(1-\overline{\alpha}^{\textsc{ss}}_{t})}\] (105)

### Beta

\[q(x_{t}|x_{0})=\mathrm{Beta}(x_{t};\alpha_{t},\beta_{t})\] (106)

There are many ways to define a noising schedule. We choose to fix the mode of the distribution at \(x_{0}\) and introduce a concentration parameter \(\nu_{t}\), parameterizing \(\alpha_{t}=1+\nu_{t}x_{0}\) and \(\beta_{t}=1+\nu_{t}(1-x_{0})\). By setting \(\nu_{t}\) to zero, we recover a uniform distribution, and by setting it to infinity we obtain the Dirac delta, centered at \(x_{0}\). Generally, the Beta distribution has a two-dimensional sufficient statistic \(\mathcal{T}(x)=\begin{pmatrix}\log x\\ \log(1-x)\end{pmatrix}\). However, under this parameterization, we can derive a one-dimensional tail statistic:

\[q(x_{t}|x_{0})\propto\exp\left\{(\alpha_{t}-1)\log x_{t}+(\beta _{t}-1)\log(1-x_{t})\right\}=\exp\left\{\nu_{t}x_{0}\log x_{t}+\nu_{t}(1-x_{0 })\log(1-x_{t})\right\}=\] (107) \[=\exp\left\{\nu_{t}x_{0}\log\frac{x_{t}}{1-x_{t}}+\nu_{t}\log(1-x _{t})\right\}=\exp\left\{\eta_{t}(x_{0})\mathcal{T}(x_{t})+\log h_{t}(x_{t})\right\}\] (108)

Therefore we can use \(\eta_{t}(x_{0})=\nu_{t}x_{0}\) and \(\mathcal{T}(x)=\log\frac{x}{1-x}\) to define the tail statistic:

\[\mathcal{G}_{t}(x_{t:T})=\sum_{s=t}^{T}\nu_{s}\log\frac{x_{s}}{1-x_{s}}\] (109)

The KL divergence can then be calculated as follows:

\[D_{KL}\left(q^{\textsc{ss}}(x_{t}|x_{0})\,\|\,p^{\textsc{ss}}_{ \theta}(x_{t}|G_{t})\right)=\log\frac{\mathrm{Beta}(\alpha_{t}(x_{\theta}), \beta_{t}(x_{\theta}))}{\mathrm{Beta}(\alpha_{t}(x_{0}),\beta_{t}(x_{0}))}+ \nu_{t}(x_{0}-x_{\theta})(\psi(\alpha_{t}(x_{0}))-\psi(\beta_{t}(x_{0})))\] (110)

### Dirichlet

\[q(x_{t}|x_{0})=\mathrm{Dirichlet}(x_{t};\alpha_{t}^{1},\ldots,\alpha_{t}^{K})\] (111)

Similarly to the Beta distribution, we choose to fix the mode of the distribution at \(x_{0}\) and introduce a concentration parameter \(\nu_{t}\), parameterizing \(\alpha_{t}^{k}=1+\nu_{t}x_{0}^{k}\). This corresponds to using the natural parameter \(\eta_{t}(x_{0})=\nu_{t}x_{0}\). By setting \(\nu_{t}\) to zero, we recover a uniform distribution, and by setting it to infinity we obtain the Dirac delta, centered at \(x_{0}\). We can use the sufficient statistic \(\mathcal{T}(x)=(\log x^{1},\ldots,\log x^{K})^{\mathsf{T}}\) to define the tail statistic:

\[\mathcal{G}_{t}(x_{t:T})=\sum_{s=t}^{T}\nu_{s}(\log x_{s}^{1},\ldots,\log x_{s} ^{K})^{\mathsf{T}}\] (112)

The KL divergence can then be calculated as follows:

\[D_{KL}\left(q^{\textsc{ss}}(x_{t}|x_{0})\,\|\,p_{\theta}^{\textsc{ss}}(x_{t}| G_{t})\right)=\sum_{k=1}^{K}\left[\log\frac{\Gamma(\alpha_{t}^{k}(x_{\theta}))}{ \Gamma(\alpha_{t}^{k}(x_{0}))}+\nu_{t}(x_{0}^{k}-x_{\theta}^{k})\psi(\alpha_{ t}^{k}(x_{0}))\right]\] (113)

### Categorical

\[q(x_{t}|x_{0})=\mathrm{Cat}(x_{t};p_{t})\] (114)

In this case, we mimic the definition of the categorical diffusion model, used in D3PM. The noising process is parameterized by the probability vector \(p_{t}=x_{0}\overline{Q}_{t}\). By setting \(\overline{Q}_{0}\) to identity, we recover the Dirac delta, centered at \(x_{0}\).

In this parameterization, the natural parameter admits linearization (after some notation abuse):

\[\eta_{t}(x_{0})=\log(x_{0}\overline{Q}_{t})=x_{0}\log\overline{Q}_{t}\text{, where}\] (115)

\(\log\) is taken element-wise, and we assume \(0\log 0=0\).

The sufficient statistic here is a vector \(x_{t}^{\mathsf{T}}\). Therefore, the tail statistic can be defined as follows:

\[\mathcal{G}_{t}(x_{t:T})=\sum_{s=t}^{T}\left(\log\overline{Q}_{s}\cdot x_{s}^ {\mathsf{T}}\right)=\sum_{s=t}^{T}\log(\overline{Q}_{s}x_{s}^{\mathsf{T}})\] (116)

The KL divergence can then be calculated as follows:

\[D_{KL}\left(q^{\textsc{ss}}(x_{t}|x_{0})\,\|\,p_{\theta}^{\textsc{ss}}(x_{t}| G_{t})\right)=\sum_{i=1}^{D}(x_{0}\overline{Q}_{t})_{i}\log\frac{(x_{0} \overline{Q}_{t})_{i}}{(x_{\theta}\overline{Q}_{t})_{i}}\] (117)

Figure 8: Visualization of the forward process in Dirichlet SS-DDPM on a three-dimensional probabilistic simplex.

Note that, unlike in the Gaussian case, the categorical star-shaped diffusion is not equivalent to the categorical DDPM. The main difference here is that the input \(G_{t}\) to the predictive model \(x_{\theta}(G_{t})\) is now a continuous vector instead of a one-hot vector.

As discussed in Section H, proper normalization is important for training SS-DDPMs. In the case of Categorical distributions, we can use the \(\mathrm{SoftMax}(\cdot)\) function to normalize the tail statistics without breaking sufficiency:

\[\tilde{G}_{t}=\mathrm{SoftMax}\left(\mathcal{G}_{t}(x_{t:T})\right)\] (118)

To see this, we can retrace the steps of the proof of Theorem 1:

\[q^{\mathrm{ss}}(x_{0}|x_{t:T})\propto q(x_{0})\exp\left\{x_{0}G_{t}\right\}=q( x_{0})\exp\left\{x_{0}\log\tilde{G}_{t}+\log\sum_{i=1}^{d}\exp\left(G_{t} \right)_{i}\right\}\propto q(x_{0})\exp\left\{x_{0}\log\tilde{G}_{t}\right\}\] (119)

Therefore,

\[q^{\mathrm{ss}}(x_{0}|x_{t:T})=\frac{q(x_{0})\exp\left\{x_{0}\log\tilde{G}_{t }\right\}}{\sum_{\tilde{x}_{0}}q(\tilde{x}_{0})\exp\left\{\tilde{x}_{0}\log \tilde{G}_{t}\right\}}=q^{\mathrm{ss}}(x_{0}|\tilde{G}_{t})\] (120)

Also, Categorical distributions admit a convenient way to compute fractions involving \(q(G_{t}|x_{0})\):

\[q(G_{t}|x_{0})=\sum_{x_{t:T}}\left(\prod_{s\geq t}q(x_{s}|x_{0})\right) \mathbbm{1}\left[G_{t}=\mathcal{G}_{t}(x_{t:T})\right]=\] (121)

\[=\sum_{x_{t:T}}\exp\left\{\sum_{s\geq t}x_{0}\log(\overline{Q}_{s})x_{s}^{\mathrm {T}}\right\}\mathbbm{1}\left[G_{t}=\mathcal{G}_{t}(x_{t:T})\right]=\sum_{x_{t: T}}\exp\left\{x_{0}G_{t}\right\}\mathbbm{1}\left[G_{t}=\mathcal{G}_{t}(x_{t:T}) \right]=\] (122)

\[=\exp\left\{x_{0}G_{t}\right\}\sum_{x_{t:T}}\mathbbm{1}\left[G_{t}=\mathcal{G }_{t}(x_{t:T})\right]=\exp\left\{x_{0}G_{t}\right\}\cdot\#_{G_{t}}\] (123)

This allows us to define the likelihood term \(p(x_{0}|x_{1:T})\) as follows:

\[p(x_{0}|G_{1})=\frac{q(G_{1}|x_{0})\tilde{p}(x_{0})}{\sum_{\tilde{x}_{0}}q(G_ {1}|\tilde{x}_{0})\tilde{p}(\tilde{x}_{0})}=\frac{\exp\left\{x_{0}G_{1}\right\} \tilde{p}(x_{0})}{\sum_{\tilde{x}_{0}}\exp\left\{\tilde{x}_{0}G_{1}\right\} \tilde{p}(\tilde{x}_{0})},\] (124)

where \(\tilde{p}(x_{0})\) can be defined as the frequency of the token \(x_{0}\) in the dataset.

It also allows us to estimate the mutual information between \(x_{0}\) and \(G_{t}\):

\[I(x_{0};G_{t})=\mathbb{E}_{q(x_{0})q(G_{t}|x_{0})}\log\frac{q(G_{t}|x_{0})}{q( G_{t})}=\mathbb{E}_{q(x_{0})q(G_{t}|x_{0})}\log\frac{q(G_{t}|x_{0})}{\sum_{ \tilde{x}_{0}}q(G_{t}|\tilde{x}_{0})q(\tilde{x}_{0})}=\] (125)

\[=\mathbb{E}_{q(x_{0})q(G_{t}|x_{0})}\log\frac{\exp\left\{x_{0}G_{t}\right\} \cdot\#_{G_{t}}}{\sum_{\tilde{x}_{0}}\exp\left\{\tilde{x}_{0}G_{t}\right\} \cdot\#_{G_{t}}\cdot q(\tilde{x}_{0})}=\mathbb{E}_{q(x_{0})q(G_{t}|x_{0})} \log\frac{\exp\left\{x_{0}G_{t}\right\}}{\sum_{\tilde{x}_{0}}\exp\left\{\tilde {x}_{0}G_{t}\right\}q(\tilde{x}_{0})}\] (126)

It can be estimated using Monte Carlo. We use it when defining the noising schedule for Categorical SS-DDPM.

### von Mises

\[q(x_{t}|x_{0})=\mathrm{vonMises}(x_{t};x_{0},\kappa_{t})\] (127)

The von Mises distribution has two parameters, the mode \(x\) and the concentration \(\kappa\). It is natural to set the mode of the noising distribution to \(x_{0}\) and vary the concentration parameter \(\kappa_{t}\). When \(\kappa_{t}\) goes to infinity, the von Mises distribution approaches the Dirac delta, centered at \(x_{0}\). When \(\kappa_{t}\) goes to 0, it approaches a uniform distribution on a unit circle. The sufficient statistic is \(\mathcal{T}(x_{t})=\begin{pmatrix}\cos x_{t}\\ \sin x_{t}\end{pmatrix}\), and the corresponding natural parameter is \(\eta_{t}(x_{0})=\kappa_{t}\begin{pmatrix}\cos x_{0}\\ \sin x_{0}\end{pmatrix}\). The tail statistic \(\mathcal{G}_{t}(x_{t:T})\) is therefore defined as follows:

\[\mathcal{G}_{t}(x_{t:T})=\sum_{s=t}^{T}\kappa_{s}\begin{pmatrix}\cos x_{s}\\ \sin x_{s}\end{pmatrix}\] (128)

The KL divergence term can be calculated as follows:

\[D_{KL}\left(q^{\mathrm{ss}}(x_{t}|x_{0})\,\|\,p^{\mathrm{ss}}_{\theta}(x_{t}|G_{ t})\right)=\kappa_{t}\frac{I_{1}(\kappa_{t})}{I_{0}(\kappa_{t})}(1-\cos(x_{0}-x_{ \theta}))\] (129)

### von Mises-Fisher

\[q(x_{t}|x_{0})=\mathrm{vMF}(x_{t};x_{0},\kappa_{t})\] (130)

Similar to the one-dimensional case, we set the mode of the distribution to \(x_{0}\), and define the schedule using the concentration parameter \(\kappa_{t}\). When \(\kappa_{t}\) goes to infinity, the von Mises-Fisher distribution approaches the Dirac delta, centered at \(x_{0}\). When \(\kappa_{t}\) goes to 0, it approaches a uniform distribution on a unit sphere. The sufficient statistic is \(\mathcal{T}(x)=x\), and the corresponding natural parameter is \(\eta_{t}(x_{0})=\kappa_{t}x_{0}\). The tail statistic \(\mathcal{G}_{t}(x_{t:T})\) is therefore defined as follows:

\[\mathcal{G}_{t}(x_{t:T})=\sum_{s=t}^{T}\kappa_{s}x_{s}\] (131)

The KL divergence term can be calculated as follows:

\[D_{KL}\left(q^{\mbox{\tiny{SS}}}(x_{t}|x_{0})\,\|\,p_{\theta}^{ \mbox{\tiny{SS}}}(x_{t}|G_{t})\right)=\kappa_{t}\frac{I_{K/2}(\kappa_{t})}{I _{K/2-1}(\kappa_{t})}x_{0}^{\mbox{\tiny{T}}}(x_{0}-x_{\theta})\] (132)

### Gamma

\[q(x_{t}|x_{0})=\Gamma(x_{t};\alpha_{t},\beta_{t})\] (133)

There are many ways to define a schedule. We choose to interpolate the mean of the distribution from \(x_{0}\) at \(t=0\) to \(1\) at \(t=T\). This can be achieved with the following parameterization:

\[\beta_{t}(x_{0})=\alpha_{t}(\xi_{t}+(1-\xi_{t})x_{0}^{-1})\] (134)

The mean of the distribution is \(\frac{\alpha_{t}}{\beta_{t}}\), and the variance is \(\frac{\alpha_{t}}{\beta_{t}}\). Therefore, we recover the Dirac delta, centered at \(x_{0}\), when we set \(\xi_{t}\) to \(0\) and \(\alpha_{t}\) to infinity. To achieve some standard distribution that doesn't depend on \(x_{0}\), we can set \(\xi_{t}\) to \(1\) and \(\alpha_{t}\) to some fixed value \(\alpha_{T}\).

In this parameterization, the natural parameters are \(\alpha_{t}-1\) and \(-\beta_{t}(x_{0})\), and the corresponding sufficient statistics are \(\log x_{t}\) and \(x_{t}\). Since the parameter \(\alpha_{t}\) doesn't depend on \(x_{0}\), we only need the sufficient statistic \(\mathcal{T}(x_{t})=x_{t}\) to define the tail statistic:

\[\mathcal{G}(x_{t:T})=\sum_{s=t}^{T}\alpha_{s}(1-\xi_{s})x_{s}\] (135)

The KL divergence can be computed as follows:

\[D_{KL}\left(q^{\mbox{\tiny{SS}}}(x_{t}|x_{0})\,\|\,p_{\theta}^{ \mbox{\tiny{SS}}}(x_{t}|G_{t})\right)=\alpha_{t}\left[\log\frac{\beta_{t}(x_{ 0})}{\beta_{t}(x_{\theta})}+\frac{\beta_{t}(x_{\theta})}{\beta_{t}(x_{0})}-1\right]\] (136)

Figure 9: Visualization of the forward process of the von Mises–Fisher SS-DDPM on the three-dimensional unit sphere.

### Wishart

\[q(X_{t}|X_{0})=\mathcal{W}_{p}(X_{t};V_{t},n_{t})\] (137)

The natural parameters for the Wishart distribution are \(-\frac{1}{2}V_{t}^{-1}\) and \(\frac{n_{t}-p-1}{2}\). To achieve linear parameterization, we need to linearly parameterize the inverse of \(V_{t}\) rather than \(V_{t}\) directly. Similar to the Gamma distribution, we interpolate the mean of the distribution from \(X_{0}\) at \(t=0\) to \(I\) at \(t=T\). This can be achieved with the following parameterization:

\[\mu_{t}(X_{0}) =\xi_{t}I+(1-\xi_{t})X_{0}^{-1}\] (138) \[V_{t}(X_{0}) =n_{t}^{-1}\mu_{t}^{-1}(X_{0})\] (139)

We recover the Dirac delta, centered at \(X_{0}\), when we set \(\xi_{t}\) to \(0\) and \(n_{t}\) to infinity. To achieve some standard distribution that doesn't depend on \(X_{0}\), we set \(\xi_{t}\) to \(1\) and \(n_{t}\) to some fixed value \(n_{T}\).

The sufficient statistic for this distribution is \(\mathcal{T}(X_{t})=\begin{pmatrix}X_{t}\\ \log|X_{t}|\end{pmatrix}\). Since the parameter \(n_{t}\) doesn't depend on \(X_{0}\), we don't need the corresponding sufficient statistic \(\log|X_{t}|\) and can use \(\mathcal{T}(X_{t})=X_{t}\) to define the tail statistic:

\[\mathcal{G}(X_{t:T},t)=\sum_{s=t}^{T}n_{s}(1-\xi_{s})X_{s}\] (140)

The KL divergence can then be calculated as follows:

\[D_{KL}\left(q^{{}^{\mathrm{SS}}}(x_{t}|x_{0})\,\|\,p^{{}^{ \mathrm{SS}}}_{\theta}(x_{t}|G_{t})\right)=-\frac{n_{t}}{2}\left[\log\left|V_{ t}^{-1}(X_{\theta})V_{t}(X_{0})\right|-\mathrm{tr}\left(V_{t}^{-1}(X_{\theta})V_{ t}(X_{0})\right)+p\right]\] (141)

## Appendix G Choosing the noise schedule

In DDPMs, we train a neural network to predict \(x_{0}\) given the current noisy sample \(x_{t}\). In SS-DDPMs, we use the tail statistic \(G_{t}\) as the neural network input instead. Therefore, it is natural to search for a schedule, where the "level of noise" in \(G_{t}\) is similar to the "level of noise" in \(x_{t}\) from some DDPM model. Similarly to D3PM, we formalize the "level of noise" as the mutual information between the clean and noisy samples. For SS-DDPM it would be \(I^{{}^{\mathrm{SS}}}(x_{0};G_{t})\), and for DDPM it would be \(I^{{}^{\mathrm{DDPM}}}(x_{0};x_{t})\). We would like to start from a well-performing DDPM model and define a similar schedule by matching \(I^{{}^{\mathrm{SS}}}(x_{0};G_{t})\approx I^{{}^{\mathrm{DDPM}}}(x_{0};x_{t})\). Since Gaussian SS-DDPM is equivalent to DDPM, the desired schedule can be found in Theorem 2. In other cases, however, it is difficult to find a matching schedule.

Figure 10: Visualization of the forward process in Wishart SS-DDPM on positive definite matrices of size \(2\times 2\).

In our experiments, we found the following heuristic to work well enough. First, we find a Gaussian SS-DDPM schedule that is equivalent to the DDPM with the desired schedule. We denote the corresponding mutual information as \(I^{\textsc{ss}}_{N}(x_{0};G_{t})=I^{\textsc{DDPM}}(x_{0};x_{t})\). Then, we can match it in the original space of \(x_{t}\) and hope that the resulting mutual information in the space of tail statistics is close too:

\[I^{\textsc{ss}}(x_{0};x_{t})\approx I^{\textsc{ss}}_{\mathcal{N}}(x_{0};x_{t}) \xrightarrow{?}I^{\textsc{ss}}(x_{0};G_{t})\approx I^{\textsc{ss}}_{\mathcal{N }}(x_{0};G_{t})\] (142)

Assuming the schedule is parameterized by a single parameter \(\nu\), we can build a look-up table \(I^{\textsc{ss}}(x_{0};x_{\nu})\) for a range of parameters \(\nu\). Then we can use binary search to build a schedule to match the mutual information \(I^{\textsc{ss}}(x_{0};x_{t})\) to the mutual information schedule \(I^{\textsc{ss}}_{\mathcal{N}}(x_{0};x_{t})\). While this procedure doesn't allow to match the target schedule _exactly_, it provides a good enough approximation and allows to obtain an adequate schedule. We used this procedure to find the schedule for the Beta diffusion in our experiments with image generation.

To build the look-up table, we need a robust way to estimate the mutual information. The target mutual information \(I^{\textsc{ss}}_{\mathcal{N}}(x_{0};x_{t})\) can be computed analytically when the data \(q(x_{0})\) follows a Gaussian distribution. When the data follows an arbitrary distribution, it can be approximated with a Gaussian mixture and the mutual information can be calculated using numerical integration. Estimating the mutual information for arbitrary noising distributions is more difficult. We find that the Kraskov estimator (Kraskov et al., 2004) works well when the mutual information is high (\(I>2\)). When the mutual information is lower, we build a different estimator using DSIVI bounds (Molchanov et al., 2019).

\[I^{\textsc{ss}}(x_{0};x_{\nu})=D_{KL}\left(q^{\textsc{ss}}(x_{0},x_{t})\,\|\,q ^{\textsc{ss}}(x_{0})q^{\textsc{ss}}(x_{t})\right)=\mathcal{H}[x_{t}]- \mathcal{H}[x_{t}|x_{0}]\] (143)

This conditional entropy is available in closed form for many distributions in the exponential family. Since the marginal distribution \(q^{\textsc{ss}}(x_{0},x_{t})=\int q^{\textsc{ss}}(x_{t}|x_{0})q(x_{0})dx_{0}\) is a semi-implicit distribution (Yin and Zhou, 2018), we can use the DSIVI sandwich (Molchanov et al., 2019) to obtain an upper and lower bound on the marginal entropy \(\mathcal{H}[x_{t}]\):

\[\mathcal{H}[x_{t}]=-\mathbb{E}_{q^{\textsc{ss}}}\log q^{\textsc{ss}}(x_{t})=- \mathbb{E}_{q^{\textsc{ss}}}\log\int q^{\textsc{ss}}(x_{t}|x_{0})q(x_{0})dx_{0}\] (144)

\[\mathcal{H}[x_{t}]\geq-\mathbb{E}_{x_{0}^{0:K}\sim q(x_{0})} \mathbb{E}_{x_{t}\sim q(x_{t}|x_{0})|_{x_{0}=x_{0}^{0}}}\log\frac{1}{K+1}\sum_ {k=0}^{K}q^{\textsc{ss}}(x_{t}|x_{0}^{k})\] (145) \[\mathcal{H}[x_{t}]\leq-\mathbb{E}_{x_{0}^{0:K}\sim q(x_{0})} \mathbb{E}_{x_{t}\sim q(x_{t}|x_{0})|_{x_{0}=x_{0}^{0}}}\log\frac{1}{K}\sum_ {k=1}^{K}q^{\textsc{ss}}(x_{t}|x_{0}^{k})\] (146)

These bounds are asymptotically exact and can be estimated using Monte Carlo. We use \(K=1000\) when the mutual information is high (\(\frac{1}{2}\leq I<2\)), \(K=100\) when the mutual information is lower (\(0.002\leq I<\frac{1}{2}\)), and estimate the expectations using \(M=10^{8}K^{-1}\) samples for each timestamp. For values \(I>2\) we use the Kraskov estimator with \(M=10^{5}\) samples and \(k=10\) neighbors. For values \(I<0.002\) we fit an exponential curve \(i(t)=e^{at+b}\) to interpolate between the noisy SIVI estimates, obtained with \(K=50\) and \(M=10^{5}\).

For evaluating the mutual information \(I(x_{0};G_{t})\) between the clean data and the tail statistics, we use the Kraskov estimator with \(k=10\) and \(M=10^{5}\).

The mutual information look-up table for the Beta star-shaped diffusion, as well as the used estimations of the mutual information, are presented in Figure 11. The resulting schedule for the beta diffusion is presented in Figure 12, and the comparison of the mutual information schedules for the tail statistics between Beta SS-DDPM and the referenced Gaussian SS-DDPM is presented in Figure 13.

For Categorical SS-DDPM we estimate the mutual information using Monte Carlo. We then choose the noising schedule to match the cosine schedule used by Hoogeboom et al. (2021) using a similar technique.

## Appendix H Normalizing the tail statistics

We illustrate different strategies for normalizing the tail statistics in Figure 14. Normalizing by the sum of coefficients is not enough, therefore we resort to matching the mean and the variance empirically. We refer to this trick as time-dependent tail normalization.

Also, proper normalization allows us to visualize the tail statistics by projecting them back into the original domain using \(\mathcal{T}^{-1}(\tilde{G}_{t})\). The effect of normalization is illustrated in Figure 15.

## Appendix I Synthetic data

We compare the performance of DDPM and SS-DDPM on synthetic tasks with exotic data domains. In these tasks, we train and sample from DDPM and SS-DDPM using \(T=64\) steps. We use an MLP with 3 hidden layers of size 512, swish activations and residual connections through hidden layers (He et al., 2015). We use sinusoidal positional time embeddings (Vaswani et al., 2017) of size \(32\) and concatenate them with the normalized tail statistics \(\tilde{G}_{t}\). We add a mapping to the corresponding domain on top of the network. During training, we use gradient clipping and EMA weights to improve stability. All models on synthetic data were trained for 350k iterations with batch size 128. In all our experiments with SS-DDPM on synthetic data we use time-dependent tail normalization. We use DDPM with linear schedule and \(L_{simple}\) or \(L_{vlb}\) as a loss function. We choose between \(L_{simple}\) and \(L_{vlb}\) objective based on the KL divergence between the data distribution and the model distribution \(D_{KL}\left(q(x_{0})\,\|\,p_{\theta}(x_{0})\right)\). To make an honest comparison, we precompute normalization statistics for DDPM in the same way that we do in time-dependent tail normalization. In DDPM, a neural network makes predictions using \(x_{t}\) as an input, so we precompute normalization statistics for \(x_{t}\) and fix them during the training and sampling stages.

Probabilistic simplexWe evaluate Dirichlet SS-DDPM on a synthetic problem of generating objects on a three-dimensional probabilistic simplex. We use a mixture of three Dirichlet distributions with different parameters as trainingdata. The Dirichlet SS-DDPM forward process is illustrated in Figure 8. To map the predictions to the domain, we put the Softmax function on the top of the MLP. We optimize Dirichlet SS-DDPM on the VLB objective without any modifications using Adam with a learning rate of \(0.0004\). The DDPM was trained on \(L_{vlb}\) using Adam with a learning rate of \(0.0002\). An illustration of the samples is presented in Table 4.

Symmetric positive definite matricesWe evaluate Wishart SS-DDPM on a synthetic problem of generating symmetric positive definite matrices of size \(2\times 2\). We use a mixture of three Wishart distributions with different parameters as training data. The Wishart SS-DDPM forward processes are illustrated in Figure 10. For the case of symmetric positive definite matrices \(V\), MLP predicts a lower triangular factor \(L_{\theta}\) from the Cholesky decomposition \(V_{\theta}=L_{\theta}L_{\theta}^{\intercal}\). For stability of sampling from the Wishart distribution and estimation of the loss function, we add a scalar matrix \(10^{-4}\mathbf{I}\) to the predicted symmetric positive definite matrix \(V_{\theta}\). In Wishart SS-DDPM we use an analogue of \(L_{simple}\) as a loss function. To improve the training stability, we divide each KL term corresponding to a timestamp \(t\) by \(n_{t}\), the de-facto concentration parameter

\begin{table}
\begin{tabular}{c c c} \hline \hline Original & DDPM & Dirichlet SS-DDPM \\ \hline \hline \end{tabular}
\end{table}
Table 4: Generating objects from three-dimensional probabilistic simplex.

Figure 14: Example trajectories of the normalized tail statistics with different normalization strategies. The trajectories are coming from a single dimension of Beta SS-DDPM.

from the used noising schedule (see Table 6). Wishart SS-DDPM was trained using Adam with a learning rate of \(0.0004\). The DDPM was trained on \(L_{simple}\) using Adam with a learning rate of \(0.0004\). Since positive definite \(2\times 2\) matrices can be interpreted as ellipses, we visualize the samples by drawing the corresponding ellipses. An illustration of the samples is presented in Table 5.

## Appendix J Geodesic data

We evaluate von Mises-Fisher SS-DDPM on the problem of restoring the distribution on the sphere from empirical data of fires on the Earth's surface (EOSDIS, 2020). We illustrate points on the sphere using a 2D projection of the Earth's map. We train and sample from von Mises-Fisher SS-DDPM using \(T=100\) steps. The forward process is illustrated in Figure 9. We use the same MLP architecture as described in Appendix I and also use time-dependent tail normalization. To map the prediction onto the unit sphere, we normalize the three-dimensional output of the MLP. We optimize \(L_{vll}\) using the AdamW optimizer with a learning rate of 0.0002 and exponential decay with \(\gamma=0.999997\). The model is trained for \(2,000,000\) iterations with batch size \(100\). For inference, we also use EMA weights with a decay of \(0.9999\).

## Appendix K Discrete data

We evaluate Categorical SS-DDPM on the task of unconditional character-level text generation on the text8 dataset. We evaluate our model on sequences of length 256. Using the property of Categorical SS-DDPM, that we can directly compute mutual information between \(x_{0}\) and \(G_{t}\) (see Appendix F.4), we match the noising schedule of \(G_{t}\) to the noising schedule of \(x_{t}\) by Austin et al. (2021). We optimize Categorical SS-DDPM on the VLB objective. Following Austin et al. (2021), we use the default T5 encoder architecture with \(12\) layers, \(12\) heads, mlp dim \(3072\) and qkv dim \(768\). We add positional embeddings to the sequence of tail statistics \(G_{t}\) and also add the sinusoidal positional time embedding to the beginning. We use Adam (Kingma and Ba, 2015) with learning rate \(5\times 10^{-4}\) with a \(10,000\)-step learning rate warmup, but instead of inverse sqrt decay, we use exponential decay with \(\gamma=0.999995\). We use a standard \(90,000,000/5,000,000/500,000\) train-test-validation split and train neural network for \(512\) epochs (time costs when using \(3\) NVIDIA A100 GPUs: training took approx. \(112\) hours and estimating NLL on the test set took approx. \(2.5\) hours). Since the Softmax function does not break the sufficiency of tail statistic in Categorical SS-DDPMs (see Appendix F.4), we can use the Softmax function as an efficient input normalization for the neural network. Categorical SS-DDPM also provides us with a convenient way to define \(\log p(x_{0}|G_{1})\), and we make use of it to estimate the NLL score (see Appendix F.4).

## Appendix L Image data

In our experiments with Beta SS-DDPM, we use the NCSN++ neural network architecture and train strategy by Song et al. (2020) (time costs when using \(4\) NVIDIA 1080 GPUs: training took approx. \(96\) hours, sampling of \(50,000\) images took approx. \(10\) hours). We put a sigmoid function on the top of NCSN++ to map the predictions to the data domain. We use time-dependent tail normalization and train Beta SS-DDPM with \(T=1000\). We matched the noise schedule in Beta SS-DDPM to the cosine noise schedule in Gaussian DDPM (Nichol and Dhariwal, 2021) in terms of mutual information, as described in Appendix G.

\begin{table}
\begin{tabular}{c c c} \hline \hline Original & DDPM & Wishart SS-DDPM \\ \hline \hline \end{tabular}
\end{table}
Table 5: Generating symmetric positive definite \(2\times 2\) matrices.

Changing discretization

When sampling from DDPMs, we can skip some timestamps to trade off computations for the quality of the generated samples. For example, we can generate \(x_{t_{1}}\) from \(x_{t_{2}}\) in one step, without generating the intermediate variables \(x_{t_{1}+1:t_{2}-1}\):

\[\bar{p}_{\theta}^{\text{\tiny DDPM}}(x_{t_{1}}|x_{t_{2}})=q^{\text{\tiny DDPM}} (x_{t_{1}}|x_{t_{2}},x_{0})|_{x_{0}=x_{\theta}^{\text{\tiny DDPM}}(x_{t_{2}},t _{2})}=\] (147)

\[=\mathcal{N}\left(x_{t_{1}};\frac{\sqrt{\bar{\alpha}_{t_{1}}^{\text{\tiny DDPM }}}\left(1-\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}}\right)}{1-\overline{ \alpha}_{t_{1}}^{\text{\tiny DDPM}}}x_{\theta}^{\text{\tiny DDPM}}(x_{t_{2}},t _{2})+\frac{\sqrt{\frac{\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}}}{ \overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}}\left(1-\overline{\alpha}_{t_{ 1}}^{\text{\tiny DDPM}}\right)}}{1-\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM }}}x_{t_{2}},\frac{1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}{1- \overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}}}\left(1-\overline{\alpha}_{t_{ 2}}^{\text{\tiny DDPM}}\right)\right)=\] (148)

\[=\mathcal{N}\left(x_{t_{1}};\frac{\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM }}-\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}}}{\sqrt{\overline{\alpha}_{t_ {1}}^{\text{\tiny DDPM}}}(1-\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}})}x_ {\theta}^{\text{\tiny DDPM}}(x_{t_{2}},t_{2})+\frac{\sqrt{\overline{\alpha}_{ t_{2}}^{\text{\tiny DDPM}}}(1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}})}{ \sqrt{\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}(1-\overline{\alpha}_{t_ {2}}^{\text{\tiny DDPM}})}x_{t_{2}},\frac{(1-\overline{\alpha}_{t_{1}}^{\text {\tiny DDPM}})(\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}-\overline{\alpha} _{t_{2}}^{\text{\tiny DDPM}})}{(1-\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM }})\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}\right)\] (149)

In the general case of SS-DDPM, we can't just skip the variables. If we skip the variables, the corresponding tail statistics will become atypical and the generative process will fail. To keep the tail statistics adequate, we can sample all the intermediate variables \(x_{t}\) but do it in a way that doesn't use additional function evaluations:

\[G_{t_{1}} =\sum_{s=t_{1}}^{t_{2}-1}A_{s}^{\text{\tiny T}}\mathcal{T}(x_{s}) +G_{t_{2}},\text{ where}\] (150) \[x_{s} \sim q(x_{s}|x_{0})|_{x_{0}=x_{\theta}^{\text{\tiny SS}}(G_{t_{2} },t_{2})}\] (151)

In case of Gaussian SS-DDPM this trick is equivalent to skipping the variables in DDPM:

\[G_{t_{1}}=\frac{1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}{\sqrt{ \overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}}\sum_{s=t_{1}}^{t_{2}-1}\frac{ \sqrt{\overline{\alpha}_{s}^{\text{\tiny SS}}}x_{s}}{1-\overline{\alpha}_{s}^ {\text{\tiny SS}}}+\frac{1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}{ \sqrt{\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}}\frac{\sqrt{\overline{ \alpha}_{t_{2}}^{\text{\tiny DDPM}}}}{1-\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM }}}G_{t_{2}}=\] (152)

\[=\frac{1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}{\sqrt{\overline{ \alpha}_{t_{1}}^{\text{\tiny DDPM}}}}\sum_{s=t_{1}}^{t_{2}-1}\frac{\overline{ \alpha}_{s}^{\text{\tiny SS}}}{1-\overline{\alpha}_{s}^{\text{\tiny SS}}}x_{ \theta}^{\text{\tiny SS}}(G_{t_{2}},t_{2})+\frac{1-\overline{\alpha}_{t_{1}}^{ \text{\tiny DDPM}}}{\sqrt{\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}}\sum_{ s=t_{1}}^{t_{2}-1}\sqrt{\frac{\overline{\alpha}_{s}^{\text{\tiny SS}}}{1- \overline{\alpha}_{s}^{\text{\tiny SS}}}}\epsilon_{s}+\frac{\sqrt{\overline{ \alpha}_{t_{2}}^{\text{\tiny DDPM}}}(1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM }})}{\sqrt{\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}(1-\overline{\alpha} _{t_{2}}^{\text{\tiny DDPM}})}G_{t_{2}}\] (153)

\[\mathbb{E}G_{t_{1}} =\frac{1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}{\sqrt{ \overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}}\left(\frac{\overline{\alpha}_{t _{1}}^{\text{\tiny DDPM}}}{1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}- \frac{\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}}}{1-\overline{\alpha}_{t_{2 }}^{\text{\tiny DDPM}}}\right)x_{\theta}^{\text{\tiny SS}}(G_{t_{2}},t_{2})+ \frac{\sqrt{\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}}}(1-\overline{\alpha} _{t_{1}}^{\text{\tiny DDPM}})}{\sqrt{\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM }}}(1-\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}})}G_{t_{2}}=\] (154)

\[=\frac{(1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}})(\overline{\alpha}_{t_{ 1}}^{\text{\tiny DDPM}}-\overline{\alpha}_{t_{2}})}{\sqrt{\overline{\alpha}_{t_ {1}}^{\text{\tiny DDPM}}}(1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}})(1- \overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}})}x_{\theta}^{\text{\tiny SS}}(G_{t_{2} },t_{2})+\frac{\sqrt{\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}}}(1-\overline{ \alpha}_{t_{1}}^{\text{\tiny DDPM}})}{\sqrt{\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM }}}(1-\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}})}G_{t_{2}}\] (156)

\[\mathbb{D}G_{t_{1}} =\frac{(1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}})^{2}}{ \overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}}\sum_{s=t_{1}}^{t_{2}-1}\frac{ \overline{\alpha}_{s}^{\text{\tiny SS}}}{1-\overline{\alpha}_{s}^{\text{\tiny SS }}}=\frac{(1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}})^{2}}{\overline{\alpha}_{t _{1}}^{\text{\tiny DDPM}}}\frac{\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}- \overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}}}{(1-\overline{\alpha}_{t_{1}}^{ \text{\tiny DDPM}})(1-\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}})}=\] (157) \[=\frac{(1-\overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}})( \overline{\alpha}_{t_{1}}^{\text{\tiny DDPM}}-\overline{\alpha}_{t_{2}}^{\text{ \tiny DDPM}})}{(1-\overline{\alpha}_{t_{2}}^{\text{\tiny DDPM}})\overline{\alpha}_{t_ {1}}^{\text{\tiny DDPM}}}\] (158)

In general case, this trick can be formalized as the following approximation to the reverse process:

\[p_{\theta}^{\text{\tiny SS}}(x_{t_{1}:t_{2}}|G_{t_{2}})=\prod_{t=t_{1}}^{t_{2}}q^{ \text{\tiny SS}}(x_{t}|x_{0})|_{x_{0}=x

\begin{table}
\begin{tabular}{l l l l} \hline \hline Distribution & Noising schedule & Tail statistic & KL divergence \\ \(q^{\text{\tiny SS}}(x_{t}|x_{0})\) & & \(\mathcal{G}_{t}(x_{t,T})\) & \(D_{KL}\left(q^{\text{\tiny SS}}(x_{t}|x_{0})\,\|\,p_{\theta}^{\text{\tiny SS}}(x _{t}|x_{t+1,T})\right)\) \\ \hline Gaussian & & & \\ \(\mathcal{N}\left(x_{t};\sqrt{\delta_{t}}x_{0},1-\overline{\alpha}_{t}\right)\) & \(1\underset{t\to 0}{\leftarrow}\overline{\alpha}_{t}\underset{t\to T}{ \leftarrow}0\) & \(\frac{1-\overline{\alpha}_{t}^{\prime}}{\sqrt{\delta_{t}^{\prime}}}\sum_{s=t }^{T}\frac{\sqrt{\delta_{t}}x_{s}}{1-\delta_{s}}\), & \\ \(x_{t}\in\mathbb{R}\) & & where \(\overline{\alpha}_{t}^{\prime}=\frac{\sum_{s=t}^{T}\frac{\overline{\alpha}_{t} }{1-\delta_{s}}}{1+\sum_{s=t}^{T}\frac{\overline{\alpha}_{t}}{1-\delta_{s}}}\) & \(\frac{\overline{\alpha}_{t}(x_{0}-x_{0})^{2}}{2(1-\overline{\alpha}_{t})}\) \\ \hline Beta & & \(\alpha_{t}(x_{0})=1+\nu_{t}x_{0}\) & \\ Beta \((x_{t};\alpha_{t}(x_{0}),\beta_{t}(x_{0}))\) & \(\beta_{t}(x_{0})=1+\nu_{t}(1-x_{0})\) & \(\sum_{s=t}^{T}\nu_{s}\log\frac{x_{s}}{1-x_{s}}\) & \(\log\frac{\text{Beta}(\alpha_{t}(x_{0}),\beta_{t}(x_{0}))}{\text{Beta}(\alpha _{t}(x_{0}),\beta_{t}(x_{0}))}+\\ \(x_{t}\in[0,1]\) & \(+\infty\underset{t\to 0}{\leftarrow}\nu_{t}\underset{t\to T}{ \longrightarrow}0\) & \(+\nu_{t}(x_{0}-x_{\theta})(\psi(\alpha_{t}(x_{0}))-\psi(\beta_{t}(x_{0})))\) \\ Dirichlet & & \\ Dir \((x_{t};\alpha_{t}^{1}(x_{0}),\ldots,\alpha_{t}^{K}(x_{0}))\) & \(\alpha_{t}^{k}(x_{0})=1+\nu_{t}x_{0}^{k}\) & \(\sum_{s=t}^{K}\left[\log\frac{\Gamma(\alpha_{t}^{k}(x_{\theta}))}{\Gamma( \alpha_{t}^{k}(x_{0}))}+\right.\) \\ \(x_{t}\in[0,1]^{K}\) & \(+\infty\underset{t\to 0}{\leftarrow}\nu_{t}\underset{t\to T}{ \longrightarrow}0\) & \(+\quad\nu_{t}(x_{0}^{k}-x_{\theta}^{k})\psi(\alpha_{t}^{k}(x_{0}))\right]\) \\ \hline Categorical & & \\ Cat \((x_{t};p_{t}(x_{0}))\) & \(p_{t}(x_{0})=x_{0}\overline{Q}_{t}\) & \(\sum_{s=t}^{T}\log\left(\overline{Q}_{s}x_{s}^{\intercal}\right)\) & \(\sum_{i=1}^{D}(p_{t}(x_{0}))_{i}\log\frac{(p_{t}(x_{0}))_{i}}{(p_{t}(x_{\theta }))_{i}}\) \\ \(x_{t}\in\{0,1\}^{D}\) & \(I\underset{t\to 0}{\leftarrow}\overline{Q}_{t}\underset{t\to T}{ \leftarrow}\overline{Q}_{T}\) & \(\sum_{i=t}^{T}\log\left(\overline{Q}_{s}x_{s}^{\intercal}\right)\) & \(\sum_{i=1}^{D}(p_{t}(x_{0}))_{i}\log\frac{(p_{t}(x_{0}))_{i}}{(p_{t}(x_{\theta }))_{i}}\) \\ \(\sum_{i=1}^{D}x_{t}^{\intercal}=1\) & & \\ von Mises & & \\ vM \((x_{t};x_{0},\kappa_{t})\) & \(+\infty\underset{t\to 0}{\leftarrow}\kappa_{t}\underset{t\to T}{ \longrightarrow}0\) & \(\sum_{s=t}^{T}\kappa_{s}\left(\begin{matrix}\cos x_{s}\\ \sin x_{s}\end{matrix}\right)\) & \(\kappa_{t}\frac{I_{1}(\kappa_{t})}{I_{0}(\kappa_{t})}(1-\cos(x_{0}-x_{\theta}))\) \\ \(x_{t}\in[-\pi,\pi]\) & & \\ \end{tabular}
\end{table}
Table 6: Examples of SS-DDPM in different families. There are many different ways to parameterize the distributions and the corresponding schedules. Further details are discussed in Appendix F.1-F.8.