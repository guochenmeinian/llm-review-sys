# Fused Gromov-Wasserstein Graph Mixup for

Graph-level Classifications

 Xinyu Ma\({}^{1}\)   Xu Chu\({}^{2}\)1   Yasha Wang\({}^{1,3}\)   Yang Lin\({}^{1}\)   Junfeng Zhao\({}^{1}\)

**Liantao Ma\({}^{1,3}\)   Wenwu Zhu\({}^{2}\)**

\({}^{1}\)School of Computer Science, Peking University

\({}^{2}\)Department of Computer Science and Technology, Tsinghua University

\({}^{3}\)National Research and Engineering Center of Software Engineering, Peking University

maxinyu@pku.edu.cn, chu_xu@mail.tsinghua.edu.cn

Corresponding Author

###### Abstract

Graph data augmentation has shown superiority in enhancing generalizability and robustness of GNNs in graph-level classifications. However, existing methods primarily focus on the augmentation in the graph signal space and the graph structure space independently, neglecting the joint interaction between them. In this paper, we address this limitation by formulating the problem as an optimal transport problem that aims to find an optimal inter-graph node matching strategy considering the interactions between graph structures and signals. To solve this problem, we propose a novel graph mixup algorithm called FGWMixup, which seeks a "midpoint" of source graphs in the Fused Gromov-Wasserstein (FGW) metric space. To enhance the scalability of our method, we introduce a relaxed FGW solver that accelerates FGWMixup by improving the convergence rate from \(\mathcal{O}(t^{-1})\) to \(\mathcal{O}(t^{-2})\). Extensive experiments conducted on five datasets using both classic (MPNNs) and advanced (Graphormers) GNN backbones demonstrate that FGWMixup effectively improves the generalizability and robustness of GNNs. Codes are available at https://github.com/ArthurLeoM/FGWMixup.

## 1 Introduction

In recent years, Graph Neural Networks (GNNs) [1; 2] have demonstrated promising capabilities in graph-level classifications, including molecular property prediction [3; 4], social network classification [5], healthcare prediction [6; 7], etc. Nevertheless, similar to other successfully deployed deep neural networks, GNNs also suffer from data insufficiency and perturbation, requiring the application of regularization techniques to improve generalizability and robustness [8]. Data augmentation is widely adopted for regularization in deep learning. It involves creating new training data by applying various semantic-invariant transformations to the original data, such as cropping or rotating images in computer vision [9], randomly inserting and rephrasing words in natural language [10; 11], etc. The augmented data fortify deep neural networks (DNNs) against potential noise and outliers underlying insufficient samples, enabling DNNs to learn more robust and representative features.

Data augmentation for GNNs requires a unique design due to the distinctive properties of attributed graphs [12], such as irregular sizes, misaligned nodes, and diverse topologies, which are not encountered when dealing with data in the Euclidean spaces such as images and tabular data. Generally, the augmentation for GNNs necessitates the consideration of two intertwined yet complementary input spaces, namely the graph signal space \(\mathcal{X}\) and the graph structure space \(\mathcal{A}\), which are mapped to an aligned latent space \(\mathcal{H}\) with GNNs. The graph signal space \(\mathcal{X}\) consists of node features. CurrentGNNs rely on parameterized nonlinear transformations to process graph signals, which can be efficiently encoded and serve as crucial inputs for downstream predictions. Therefore, the augmentation of graph signals is significant for regularizing the GNN parameter space. On the other hand, the graph structure space \(\mathcal{A}\) consists of information about graph topology. Traditional Message Passing Neural Networks (MPNNs), such as GCN and GIN, perform feature aggregation based on edge connectivity. Great efforts [13; 14; 15; 16] have been further exerted on enhancing the expressive power of GNNs [17], which carry out new GNN architectures with stronger sensitivity to graph topology compared with MPNNs. Hence, a good augmentation method should also consider the graph structure space.

Recently, huge efforts have been made to design graph data augmentation methods based on the two spaces. Mainstream research [18; 19; 20; 21; 22] considers the augmentation in the graph signal space and the graph structure space **independently**. For instance, ifMixup [19] conducts Euclidean mixup in the graph signal space, yet fails to preserve key topologies of the original graphs. \(\mathcal{G}\)-Mixup [20] realizes graph structure mixup based on the estimated graphons, yet fails to assign semantically meaningful graph signals. In fact, the graph signal and structure spaces are not isolated from each other, and there are strong entangling relations between them [23]. Therefore, **a joint modeling of the interaction between the two spaces is essential for conducting graph data augmentation** (joint modeling problem for short).

Aiming at graph data augmentation and addressing the joint modeling problem, we design a novel graph mixup [24] method considering the interaction of the two spaces during the mixup procedure. The key idea is to solve a proper inter-graph node matching strategy in a metric space that measures the distance with respect to both graph signals and structures. We propose to compute the distance metric by solving an optimal transport (OT) problem [25]. The OT problem solves the optimal coupling between nodes across graphs in the fused Gromov-Wasserstein metric space [26], wherein the distance between points takes the interaction between the graph signals and structures into account. Specifically, following [26], graphs can be modeled as probability distributions embedded in the product metric space \(\mathcal{X}\times\mathcal{A}\). Our objective is to solve an augmented graph that minimizes the weighted sum of transportation distances between the distributions of the source graphs and the objective graph in this metric space. Developing from the Gromov-Wasserstein metric [27], Fused Gromov-Wasserstein (FGW) distance [26] has been designed to calculate the transportation distance between two unregistered probability distributions defined on different product metric spaces comprising two components, such as graph signals and structures, which defines a proper distance metric for attributed graphs. In short words, the solved graph is the augmented mixup graph in a space considering the interaction between the graph signals and structures.

However, trivially adopting FGW distance solvers [28; 26] is not scalable to large graph datasets due to a heavy computation burden. The computational bottleneck of FGW-based solvers is the nested triple-loop optimization [29], mainly due to the polytope constraint for the coupling matrix in the FGW solver. Inspired by [30], we disentangle the polytope constraint into two simplex constraints for rows and columns respectively, thence executing mirror descent with projections on the two simplexes in an alternating manner to approximate the original constraint. We prove that with a bounded gap with the ideal optimum, we may optimize the entire algorithm into a double-loop structure at convergence rate \(\mathcal{O}(t^{-2})\), improving the \(\mathcal{O}(t^{-1})\) rate of traditional FGW solvers.

In summary, we highlight the contributions of this paper: We address the challenge of enhancing the generalizability and robustness of GNNs by proposing a novel graph data augmentation method that models the interaction between the graph signal and structure spaces. We formulate our objective as an OT problem and propose a novel graph mixup algorithm dubbed FGWMixup that seeks a "midpoint" of two graphs defined in the graph structure-signal product metric space. We employ FGW as the distance metric and speed up FGWMixup by relaxing the polytope constraint into disentangled simplex constraints, reducing the complexity from nested triple loops to double loops and meanwhile improve the convergence rate of FGWMixup. Extensive experiments are conducted on five datasets and four classic (MPNNs) and advanced (Graphormers) backbones. The results demonstrate that our method substantially improves the performance of GNNs in terms of their generalizability and robustness.

## 2 Methodology

In this section, we formally introduce the proposed graph data augmentation method dubbed FGWMixup. We first introduce Fused Gromov-Wasserstein (FGW) distance that presents a distance metric between graphs considering the interaction of graph signal and structure spaces.

Then we propose our optimization objective of graph mixup based on the FGW metric and the algorithmic solutions. Finally, we present our acceleration strategy. In the following we denote \(\Delta_{n}:=\{\bm{\mu}\in\mathbb{R}_{+}^{n}|\sum_{i}\mu_{i}=1\}\) as the probability simplex with \(n\)-bins, and \(\mathbb{S}_{n}(\mathbb{A})\) as the set of symmetric matrices of size \(n\) taking values in \(\mathbb{A}\subset\mathbb{R}\).

### Fused Gromov-Wasserstein Distance

In OT problems, an undirected attributed graph \(G\) with \(n\) nodes is defined as a tuple \((\bm{\mu},\bm{X},\bm{A})\). \(\bm{\mu}\in\Delta_{n}\) denotes the probability measure of nodes within the graph, which can be modeled as the relative importance weights of graph nodes. Common choices of \(\bm{\mu}\) are uniform distributions [31] (\(\bm{\mu}=\bm{1}_{n}/n\)) and degree distributions [32] (\(\bm{\mu}=[\deg(v_{i})]_{i}/(\sum_{i}\deg(v_{i}))\) ). \(\bm{X}=(\bm{x}^{(1)},\cdots,\bm{x}^{(n)})^{\top}\in\mathbb{R}^{n\times d}\) denotes the node feature matrix with \(d\)-dimensional feature on each node. \(\bm{A}\in\mathbb{S}_{n}(\mathbb{R})\) denotes a matrix that encodes structural relationships between nodes, which can be selected from adjacency matrix, shortest path distance matrix or other distance metrics based on the graph topologies. Given two graphs \(G_{1}=(\bm{\mu}_{1},\bm{X}_{1},\bm{A}_{1})\) and \(G_{2}=(\bm{\mu}_{2},\bm{X}_{2},\bm{A}_{2})\) of sizes \(n_{1}\) and \(n_{2}\) respectively, Fused Gromov-Wasserstein distance can be defined as follows:

\[\mathrm{FGW}_{q}(G_{1},G_{2})=\min_{\bm{\pi}\in\Pi(\bm{\mu}_{1},\bm{\mu}_{2}) }\sum_{i,j,k,l}\left((1-\alpha)d\left(\bm{x}_{1}^{(i)},\bm{x}_{2}^{(j)}\right) ^{q}+\alpha\left|\bm{A_{1}}(i,k)-\bm{A_{2}}(j,l)\right|^{q}\right)\pi_{i,j} \pi_{k,l},\] (1)

where \(\Pi(\bm{\mu}_{1},\bm{\mu}_{2}):=\{\bm{\pi}\in\mathbb{R}_{+}^{n_{1}\times n_{2 }}|\bm{\pi}\bm{1}_{n_{2}}=\bm{\mu}_{1},\bm{\pi}^{\top}\bm{1}_{n_{1}}=\bm{\mu}_ {2}\}\) is the set of all valid couplings between node distributions \(\bm{\mu}_{1}\) and \(\bm{\mu}_{2}\), \(d(\cdot,\cdot)\) is the distance metric in the feature space, and \(\alpha\in[0,1]\) is the weight that trades off between the Gromov-Wasserstein cost on graph structure and Wasserstein cost on graph signals. The FGW distance is formulated as an optimization problem that aims to determine the optimal coupling between nodes in a fused metric space considering the interaction of graph structure and node features. The optimal coupling matrix \(\bm{\pi}^{*}\) serves as a soft node matching strategy that tends to match two pairs of nodes from different graphs that have both similar graph structural properties (such as \(k\)-hop connectivity, defined by \(\bm{A}\)) and similar node features (such as Euclidean similarity, defined by \(d(\cdot,\cdot)\)). In fact, FGW also defines a strict distance metric on the graph space \((\Delta,\mathcal{X},\mathcal{A})\) when \(q=1\) and \(\bm{A}\)s are distance matrices, and defines a semi-metric whose triangle inequality is relaxed by \(2^{q-1}\) for \(q>1\). In practice, we usually choose \(q=2\) and Euclidean distance for \(d(\cdot,\cdot)\) to calculate FGW distance.

Solving FGW DistanceSolving FGW distance is a non-convex optimization problem, whose non-convexity comes from the quadratic term of the GW distance. There has been a line of research contributing to solving this problem. [26] proposes to apply the conditional gradient (CG) algorithm to solve this problem, and [28] presents an entropic approximation of the problem and solves the optimization through Mirror Descent (MD) algorithm according to KL-divergence.

### Solving Graph Mixup in the FGW Metric Space

Building upon the FGW distance and its properties, we propose a novel graph mixup method dubbed \(\mathtt{FGWMixup}\). Formally, our objective is to solve a synthetic graph \(\tilde{G}=(\tilde{\bm{\mu}},\tilde{\bm{X}},\tilde{\bm{A}})\) of size \(\tilde{n}\) that minimizes the weighted sum of FGW distances between \(\tilde{G}\) and two source graphs \(G_{1}=(\bm{\mu}_{1},\bm{X}_{1},\bm{A}_{1})\) and \(G_{2}=(\bm{\mu}_{2},\bm{X}_{2},\bm{A}_{2})\) respectively. The optimization objective is as follows:

\[\arg\min_{\tilde{G}\in(\Delta_{\tilde{n}},\mathbb{R}^{\tilde{n}\times d}, \tilde{\mathbb{S}}_{\tilde{n}}(\mathbb{R}))}\lambda\mathrm{FGW}(\tilde{G},G_{ 1})+(1-\lambda)\mathrm{FGW}(\tilde{G},G_{2}),\] (2)

where \(\lambda\in[0,1]\) is a scalar mixing ratio, usually sampled from a Beta(\(k,k\)) distribution with hyperparameter \(k\). This optimization problem formulates the graph mixup problem as an OT problem that aims to find the optimal graph \(\tilde{G}\) at the "midpoint" of \(G_{1}\) and \(G_{2}\) in terms of both graph signals and structures. When the optimum \(\tilde{G}^{*}\) is reached, the solutions of FGW distances between \(\tilde{G}^{*}\) and the source graphs \(G_{1},G_{2}\) provide the node matching strategies that minimize the costs of aligning graph structures and graph signals. The label of \(\tilde{G}\) is then assigned as \(y_{\tilde{G}}=\lambda y_{G_{1}}+(1-\lambda)y_{G_{2}}\).

In practice, we usually fix the node probability distribution of \(\tilde{G}\) with a uniform distribution (i.e., \(\tilde{\bm{\mu}}=\bm{1}_{\tilde{n}}/\tilde{n}\)) [26]. Then, Eq.2 can be regarded as a nested bi-level optimization problem, which composes the upper-level optimization _w.r.t._ the node feature matrix \(\tilde{\bm{X}}\) and the graph structure \(\tilde{\bm{A}}\)and the lower-level optimization _w.r.t._ the couplings between current \(\tilde{G}\) and \(G_{1},G_{2}\) denotes as \(\bm{\pi}_{1},\bm{\pi}_{2}\). Inspired by [28; 26; 33], we propose to solve Eq.2 using a Block Coordinate Descent algorithm, which iteratively minimizes the lower-level and the upper-level with a nested loop framework. The algorithm is presented in Alg.1. The inner loop solves the lower-level problem (i.e., FGW distance and the optimal couplings \(\bm{\pi}_{1}^{(k)},\bm{\pi}_{2}^{(k)}\)) based on \(\tilde{\bm{X}}^{(k)},\tilde{\bm{A}}^{(k)}\) at the \(k\)-th outer loop iteration, which is non-convex and requires optimization algorithms introduced in Section 2.1. The outer loop solves the upper-level problem (i.e., minimizing the weighted sum of FGW distances), which is a convex quadratic optimization problem w.r.t. \(\tilde{\bm{X}}^{(k)}\) and \(\tilde{\bm{A}}^{(k)}\) with exact analytical solution (i.e., Line 7,8).

```
1:Input:\(\tilde{\bm{\mu}}\), \(G_{1}=(\bm{\mu}_{1},\bm{X}_{1},\bm{A}_{1})\), \(G_{2}=(\bm{\mu}_{2},\bm{X}_{2},\bm{A}_{2})\)
2:Optimizing:\(\tilde{\bm{X}}\in\mathbb{R}^{\tilde{n}\times d},\tilde{\bm{A}}\in\mathbb{S}_{ \tilde{n}}(\mathbb{R}),\bm{\pi}_{1}\in\Pi(\tilde{\bm{\mu}},\bm{\mu}_{1}),\bm{ \pi}_{2}\in\Pi(\tilde{\bm{\mu}},\bm{\mu}_{2})\).
3:for\(k\) in outer iterations and not converged do:
4:\(\tilde{G}^{(k)}:=(\tilde{\bm{\mu}},\tilde{\bm{X}}^{(k)},\tilde{\bm{A}}^{(k)})\)
5: Solve \(\arg\min_{\bm{\pi}_{1}^{(k)}}\operatorname{FGW}(\tilde{G}^{(k)},G_{1})\) with MD or CG (inner iterations)
6: Solve \(\arg\min_{\bm{\pi}_{2}^{(k)}}\operatorname{FGW}(\tilde{G}^{(k)},G_{2})\) with MD or CG (inner iterations)
7: Update \(\tilde{\bm{A}}^{(k+1)}\leftarrow\frac{1}{\tilde{\bm{\mu}}\tilde{\bm{\mu}}^{ \star}}(\lambda\bm{\pi}_{1}^{(k)}\bm{A}_{1}\bm{\pi}_{1}^{(k)}{}^{\top}+(1- \lambda)\bm{\pi}_{2}^{(k)}\bm{A}_{2}\bm{\pi}_{2}^{(k)}{}^{\top})\)
8: Update \(\tilde{\bm{X}}^{(k+1)}\leftarrow\lambda\mathrm{diag}(1/\tilde{\bm{\mu}})\bm{ \pi}_{1}^{(k)}\bm{X}_{1}+(1-\lambda)\mathrm{diag}(1/\tilde{\bm{\mu}})\bm{\pi}_ {2}^{(k)}\bm{X}_{2}\)
9:endfor
10:return\(\tilde{G}^{(k)},y_{\tilde{G}}=\lambda y_{G_{1}}+(1-\lambda)y_{G_{2}}\) ```

**Algorithm 1**FGWMixup: Solving Eq.2 with BCD Algorithm

### Accelerating FGWMixup

Algorithm 1 provides a practical solution to optimize Eq.2, whereas the computation complexity is relatively high. Specifically, Alg.1 adopts a nested double-loop framework, where the inner loop is the FGW solver that optimizes the couplings \(\bm{\pi}\), and the outer loop updates the optimal feature matrix \(\tilde{\bm{X}}\) and graph structure \(\tilde{\bm{A}}\) accordingly. However, the most common FGW solvers, such as MD and CG, require another nested double-loop algorithm. This algorithm invokes (Bregman) projected gradient descent type methods to address the non-convex optimization of FGW, which involves taking a gradient step in the outer loop and projecting to the polytope-constrained feasible set _w.r.t._ the couplings in the inner loop (e.g., using Sinkhorn [30] or Dykstra [34] iterations). Consequently, this makes the entire mixup algorithm a triple-loop framework, resulting in a heavy computation burden.

Therefore, we attempt to design a method that efficiently accelerates the algorithm. There lie two efficiency bottlenecks of Alg.1: 1) the polytope constraints of couplings makes the FGW solver a nested double-loop algorithm, 2) the strict projection of couplings to the feasible sets probably modifies the gradient step to another direction that deviates from the original navigation of the gradient, possibly leading to a slower convergence rate. In order to alleviate the problems, we are motivated to slightly relax the feasibility constraints of couplings to speed up the algorithm. Inspired by [29; 30], we do not strictly project \(\bm{\pi}\) to fit the polytope constraint \(\Pi(\bm{\mu}_{i},\bm{\mu}_{j}):=\{\bm{\pi}\in\mathbb{R}_{+}^{n_{1}\times n_{2}}| \bm{\pi}\bm{1}_{n_{2}}=\bm{\mu}_{1},\bm{\pi}^{\top}\bm{1}_{n_{1}}=\bm{\mu}_{2}\}\) after taking each gradient step. Instead, we relax the constraint into two simplex constraints of rows and columns respectively (i.e., \(\Pi_{1}:=\{\bm{\pi}\in\mathbb{R}_{+}^{n_{1}\times n_{2}}|\bm{\pi}\bm{1}_{n_{2}}= \bm{\mu}_{1}\}\), \(\Pi_{2}:=\{\bm{\pi}\in\mathbb{R}_{+}^{n_{1}\times n_{2}}|\bm{\pi}^{\top}\bm{1}_ {n_{1}}=\bm{\mu}_{2}\}\)), and project \(\bm{\pi}\) to the relaxed constraints \(\Pi_{1}\) and \(\Pi_{2}\) in an alternating fashion. The accelerated algorithm is presented in Alg.2, dubbed FGWMixup\({}_{\ast}\).

Alg.2 mainly substitutes Line 5 and Line 6 of Alg.1 with a single-loop FGW distance solver (Line 7-12 in Alg.2) that relaxes the joint polytope constraints of the couplings. Specifically, we remove the constant term in FGW distance, and denote the equivalent metric as \(\overline{\operatorname{FGW}}(G_{1},G_{2})\). The optimization objective of \(\overline{\operatorname{FGW}}(G_{1},G_{2})\) can be regarded as a function of \(\bm{\pi}\), and we name it the FGW function \(f(\bm{\pi})\) (See Appendix A.1 for details). Then we employ entropic regularization on \(f(\bm{\pi})\) and select Mirror Descent as the core algorithm for the FGW solver. With the negative entropy \(\phi(x)=\sum_{i}x_{i}\log x_{i}\) as the Bregman projection, the MD update takes the form of:

\[\bm{\pi}\leftarrow\bm{\pi}\odot\exp(-\gamma\nabla_{\bm{\pi}}f(\bm{\pi})),\quad \bm{\pi}\leftarrow\operatorname{Proj}_{\bm{\pi}\in\Pi_{i}}(\bm{\pi}):=\arg \min_{\bm{\pi}^{\star}\in\Pi_{i}}\left\|\bm{\pi}^{\star}-\bm{\pi}\right\|,\] (3)

where \(\gamma\) is the step size. The subgradient of FGW _w.r.t._ the coupling \(\bm{\pi}\) can be calculated as:

\[\nabla_{\bm{\pi}}f(\bm{\pi})=(1-\alpha)\bm{D}-4\alpha\bm{A}_{1}\bm{\pi}\bm{A}_{2},\] (4)where \(\bm{D}=\left(d(\bm{X}_{1}[i],\bm{X}_{2}[j])\right)_{n_{j}\times n_{2}}\) is the distance matrix of node features between two graphs. The detailed derivation can be found in Appendix A.2.

Our relaxation is conducted in Line 9 and 11, where the couplings are projected to the row and column simplexes alternately instead of directly to the strict polytope. Although this relaxation may sacrifice some feasibility due to the relaxed projection, the efficiency of the algorithm has been greatly promoted. On the one hand, noted that \(\Pi_{1}\) and \(\Pi_{2}\) are both simplexes, the Bregman projection _w.r.t._ the negative entropy of a simplex can be extremely efficiently conducted without invoking extra iterative optimizations (i.e., Line 9, 11), which simplifies the FGW solver from a nested double-loop framework to a single-loop one. On the other hand, the relaxed constraint may also increase the convergence efficiency due to a closer optimization path to the unconstrained gradient descent.

We also provide some theoretical results to justify our algorithm. Proposition 1 presents a convergence rate analysis on our algorithm. Taking \(1/\gamma\) as the entropic regularization coefficient, our FGW solver can be formulated as Sinkhorn iterations, whose convergence rate can be optimized from \(\mathcal{O}(t^{-1})\) to \(\mathcal{O}(t^{-2})\) by conducting marginal constraint relaxation. Proposition 2 presents a controlled gap between the optima given by the relaxed single-loop FGW solver and the strict FGW solver.

**Proposition 1**.: _Let \((\mathcal{X},\mu),(\mathcal{Y},\nu)\) be Polish probability spaces, and \(\pi\in\Pi(\mu,\nu)\) the probability measure on \(\mathcal{X}\times\mathcal{Y}\) with marginals \(\mu,\nu\). Let \(\pi_{t}\) be the Sinkhorn iterations \(\pi_{2t}=\arg\min_{\Pi(*,\nu)}H(\cdot|\pi_{2t-1}),\pi_{2t+1}=\arg\min_{\Pi(\mu,*)}H(\cdot|\pi_{2t})\), where \(H(p|q)=-\sum_{i}p_{i}\log\frac{q_{i}}{p_{i}}\) is the Kullback-Leibler divergence, and \(\Pi(*,\nu)\) is the set of measures with second marginal \(\nu\) and arbitrary first marginal (\(\Pi(\mu,*)\) is defined analogously). Let \(\pi^{*}\) be the unique optimal solution. We have the convergence rate as follows:_

\[H(\pi_{t}|\pi^{*})+H(\pi^{*}|\pi_{t})=\mathcal{O}(t^{-1}),\] (5) \[H(\mu_{t}|\mu)+H(\mu|\mu_{t})+H(\nu_{t}|\nu)+H(\nu|\nu_{t})= \mathcal{O}(t^{-2}),\] (6)

Proposition 1 implies the faster convergence of marginal constraints than the strict joint constraint. This entails that with \(t\) Sinkhorn iterations of solving FGW, the solution of the relaxed FGW solver moves further than the strict one. This will benefit the convergence rate of the whole algorithm with a larger step size of \(\bm{\pi}_{i}^{(k)}\) in each outer iteration.

**Proposition 2**.: _Let \(C_{1},C_{2}\) be two convex sets, and \(f(\pi)\) denote the FGW function w.r.t. \(\pi\). We denote \(\mathcal{X}\) as the critical point set of strict FGW that solves \(\min_{\pi}f(\pi)+\mathbb{I}_{\pi\in C_{1}}+\mathbb{I}_{\pi\in C_{2}}\), defined by: \(\mathcal{X}=\{\pi\in C_{1}\cap C_{2}:0\in\nabla f(\pi)+\mathcal{N}_{C_{1}}( \pi)+\mathcal{N}_{C_{2}}(\pi)\}\), and \(\mathcal{N}_{C}(\pi)\) is the normal cone to at \(\pi\). The fix-point set \(\mathcal{X}_{rel}\) of the relaxed FGW solving \(\min_{\pi,\omega}f(\pi)+f(\omega)+h(\pi)+h(\omega)\) is defined by: \(\mathcal{X}_{rel}=\{\pi\in C_{1},\omega\in C_{2}:0\in\nabla f(\pi)+\rho(\nabla h (\omega)-\nabla h(\pi))+\mathcal{N}_{C_{1}}(\pi),\mathrm{and}\ 0\in\nabla f( \omega)+\rho(\nabla h(\pi)-\nabla h(\omega))+\mathcal{N}_{C_{2}}(\omega)\}\) where \(h(\cdot)\) is the Bregman projection function. Then, the gap between \(\mathcal{X}_{rel}\) and \(\mathcal{X}\) satisfies:_

\[\exists\tau\in\mathbb{R},\ \forall(\pi^{*},\omega^{*})\in\mathcal{X}_{rel},\ \mathrm{dist}(\frac{\pi^{*}+\omega^{*}}{2},\mathcal{X}):=\min_{x\in\mathcal{X} }\left\|\frac{\pi^{*}+\omega^{*}}{2}-x\right\|\leq\tau/\rho.\] (7)

This bounded gap ensures the correctness of FGWMixup\({}_{*}\) that as long as we select a step size \(\gamma=1/\rho\) that is small enough, the scale of the upper bound \(\tau/\rho\) will be sufficiently small to ensure the convergence to the ideal optimum of our algorithm. The detailed proofs of Propositions 1 and 2 are presented in Appendix B.

## 3 Experiments

### Experimental Settings

DatasetsWe evaluate our methods with five widely-used graph classification tasks from the graph benchmark dataset collection TUDataset [35]: NCI1 and NCI109 [36; 37] for small molecule classification, PROTEINS [38] for protein categorization, and IMDB-B and IMDB-M [5] for social networks classification. Noted that there are no node features in IMDB-B and IMDB-M datasets, we augment the two datasets with node degree features as in [2; 19; 20]. Detailed statistics on these datasets are reported in Appendix D.1.

BackbonesMost existing works select traditional MPNNs such as GCN and GIN as the backbone. However, traditional MPNNs exhibit limited expressive power and sensitivity to graph structures (upper bounded by 1-Weisfeiler-Lehman (1-WL) test [17; 37]), while there exist various advanced GNN architectures [13; 14; 16] with stronger expressive power (upper bound promoted to 3-WL test). Moreover, the use of a global pooling layer (e.g., mean pooling) in graph classification models may further deteriorate their perception of intricate graph topologies. These challenges may undermine the reliability of the conclusions regarding the effectiveness of graph structure augmentation. Therefore, we attempt to alleviate the problems from two aspects. 1) We modify the READOUT approach of GIN and GCN to save as much structural information as we can. Following [39], we apply a virtual node that connects with all other nodes and use the final latent representation of the virtual node to conduct READOUT. The two modified backbones are dubbed vGIN and vGCN. 2) We select two Transformer-based GNNs with stronger expressive power as the backbones, namely Graphormer [39] and Graphormer-GD [16]. They are proven to be more sensitive to graph structures, and Graphormer-GD is even capable of perceiving cut edges and cut vertices in graphs. Detailed information about the selected backbones is introduced in Appendix D.2.

Comparison BaselinesWe select the following data augmentation methods as the comparison baselines. DropEdge [40] randomly removes a certain ratio of edges from the input graphs. DropNode [41] randomly removes a certain portion of nodes as well as the edge connections. M-Mixup [42] conducts Euclidean mixup in the latent spaces, which interpolates the graph representations after the READOUT function. ifMixup [19] applies an arbitrary node matching strategy to conduct mixup on graph node signals, without preserving key topologies of original graphs. \(\mathcal{G}\)-Mixup [20] conducts Euclidean addition on the estimated graphons of different classes of graphs to conduct class-level graph mixup. We also present the performances of the vanilla backbones, as well as our proposed method w/ and w/o acceleration, denoted as FGWMixup\({}_{*}\) and FGWMixup respectively.

Experimental SetupsFor a fair comparison, we employ the same set of hyperparameter configurations for all data augmentation methods in each backbone architecture. For all datasets, We randomly hold out a test set comprising 10% of the entire dataset and employ 10-fold cross-validation on the remaining data. We report the average and standard deviation of the accuracy on the test set over the best models selected from the 10 folds. This setting is more realistic than reporting results from validation sets in a simple 10-fold CV and allows a better understanding of the generalizability [43]. We implement our backbones and mixup algorithms based on Deep Graph Library (DGL) [44] and Python Optimal Transport (POT) [45] open-source libraries. More experimental and implementation details are introduced in Appendix D.3 and D.4.

### Experimental Results

Main ResultsWe compare the performance of various GNN backbones on five benchmark datasets equipped with different graph data augmentation methods and summarize the results in Table 1. As is shown in the table, FGWMixup and FGWMixup\({}_{*}\) consistently outperform all other SOTA baseline methods, which obtain 13 and 7 best performances out of all the 20 settings respectively. The superiority is mainly attributed to the adoption of the FGW metric. Existing works hardly consider the node matching problem to align two unregistered graphs embedded in the signal-structure fused metric spaces, whereas the FGW metric searches the optimum from all possible couplings and conducts semantic-invariant augmentation guided by the optimal node matching strategy. Remarkably, despite introducing some infeasibility for acceleration, FGWMixup\({}_{*}\) maintains consistent performance due to the theoretically controlled gap with the ideal optimum, as demonstrated in Prop. 1. Moreover, FGWMixup and FGWMixup\({}_{*}\) both effectively improve the performance and generalizability of various GNNs. Specifically, our methods achieve an average relative improvement of 1.79% on MPNN backbones and 2.67% on Graphormer-based backbones when predicting held-out unseen samples. Interestingly, most SOTA graph data augmentation methods fail to bring notable improvements and even degrade the performance of Graphormers, which exhibit stronger expressive power and conduct more comprehensive interactions between graph signal and structure spaces. For instance, none of the baseline methods improve GraphormerGD on NCI109 and NCI1 datasets. However, our methods show even larger improvements on Graphormers compared to MPNNs, as we explicitly consider the interactions between the graph signal and structure spaces during the mixup process. In particular, the relative improvements of FGWMixup reach up to 7.94% on Graphormer and 2.95% on GraphormerGD. All the results above validate the effectiveness of our methods. Furthermore, we also conduct experiments on large-scale OGB [46] datasets, and the results are provided in Appendix E.5.

Robustness against Label CorruptionsIn this subsection, we evaluate the robustness of our methods against noisy labels. Practically, we introduce random label corruptions (i.e., switching to another random label) with ratios of 20%, 40%, and 60% on the IMDB-B and NCI1 datasets. We employ vGCN as the backbone model and summarize the results in Table 2. The results evidently demonstrate that the mixup-series methods consistently outperform the in-sample augmentation method (DropEdge) by a significant margin. This improvement can be attributed to the soft-labeling strategy employed by mixup, which reduces the model's sensitivity to individual mislabeled instances and encourages the model to conduct more informed predictions based on the overall distribution of the blended samples. Notably, among all the mixup methods, FGWMixup and FGWMixup\({}_{*}\) exhibit stable and consistently superior performance under noisy label conditions. In conclusion, our methods effectively enhance the robustness of GNNs against label corruptions.

Analysis on Mixup EfficiencyWe run FGWMixup and FGWMixup\({}_{*}\) individually with identical experimental settings (including stopping criteria and hyper-parameters such as mixup ratio, etc.) on the same computing device and investigate the run-time computational efficiencies. Table 3 illustrates

\begin{table}
\begin{tabular}{c c|c c|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{PROTIONS} & \multicolumn{2}{c|}{NSCI} & \multicolumn{2}{c|}{NSCI10} & \multicolumn{2}{c|}{NIC109} & \multicolumn{2}{c}{IMDB-B} & \multicolumn{2}{c}{IMDB-B} \\  & \multicolumn{2}{c|}{VIGN} & \multicolumn{2}{c|}{vGCN} & \multicolumn{2}{c|}{vGIN} & \multicolumn{2}{c|}{vGCN} & \multicolumn{2}{c|}{vGIN} & \multicolumn{2}{c|}{vGIN} & \multicolumn{2}{c}{vGIN} & \multicolumn{2}{c}{vGCN} & \multicolumn{2}{c}{vGIN} & \multicolumn{2}{c}{vGCN} \\ \hline vanilla & 74.93(0.3) & 74.75(26.0) & 76.98(18.7) & 76.91(18.0) & 75.70(18.5) & 75.89(13.5) & 710.34(96.9) & 72.30(34.3) & 49.00(26.4) & 49.47(3.76) \\ \multirow{2}{*}{DropEdge} & 73.95(20.9) & 74.48(14.6) & 76.42(85.0) & 76.62(42.0) & 75.62(83.0) & 75.77(53.3) & 73.30(32.9) & 73.94(72.6) & 69.49(3.52) \\  & 29.70(28.4) & 74.88(16.7) & 76.42(85.5) & 76.62(42.7) & 75.90(28.5) & 77.57(18.9) & 75.03(18.5) & 73.30(32.9) & 73.95(58.0) & 50.00(3.41) \\  & 29.70(28.4) & 74.88(29.5) & 74.68(19.1) & 77.26(27.1) & 79.30(28.5) & 75.41(19.9) & 75.20(28.5) & 72.04(45.3) & 49.13(12.5) & 49.47(25.6) \\  & 29.70(28.4) & 74.60(28.4) & 77.16(18.7) & 77.26(26.3) & 75.39(18.7) & 76.74(18.5) & 76.34(18.5) & 52.03(38.9) & 72.40(44.5) & 49.73(16.49) \\  & 29.70(28.4) & 74.52(28.8) & 74.61(24.1) & 77.97(18.8) & 75.55(22.5) & 76.38(11.3) & 76.34(18.9) & 72.04(48.5) & 49.73(16.49) & 49.73(16.47) \\  & 29.70(28.4) & 75.01(28.8) & 76.04(21.9) & 77.97(18.8) & 75.55(22.5) & 76.38(11.3) & 77.44(18.9) & 72.04(48.5) & 49.73(16.49) & 49.63(19.5) \\  & 29.70(28.4) & 75.01(28.8) & 76.04(21.9) & 77.97(18.8) & 75.55(22.5) & 76.38(11.3) & 77.03(30.69) & 77.40(28.45) & 49.73(13.49) & 49.60(3.90) \\  & 29.70(28.4) & 75.02(38.0) & 76.03(19.9) & **73.23(26.5)** & **73.27(26.4)** & 76.40(26.1) & **76.79(21.5)** & **73.03(0.69)** & 73.05(12.5) & **49.80(28.5)** & **50.80(40.46)** \\  & 29.70(28.3) & **75.23(29.3)** & 73.27(27.1) & **73.07(24.7)** & **76.40(24.6)** & **76.40(26.0)** & **76.52(1.59)** & **73.50(45.44)** & **74.00(29.09)** & **49.20(38.38)** & **50.47(45.44)** \\ \hline \hline \multirow{2}{*}{Methods} & \multicolumn{2}{c|}{Graphormer Gabnormer GD/Gapher GD/Graphormer Gabnormer GD/Grapher Gabnormer GD/Grapher Grapher GD/Gapher GD} \\  & \multicolumn{2}{c|}{vGIN} & \multicolumn{2}{c|}{vGIN} & \multicolumn{2}{c|}{vGIN} & \multicolumn{2}{c}{vGIN} & \multicolumn{2}{c}{vGIN} & \multicolumn{2}{c}{vGIN} & \multicolumn{2}{c}{vGIN} & \multicolumn{2}{c}{vGCN} & \multicolumn{2}{c}{vGIN} & \multicolumn{2}{c}{vGIN} & \multicolumn{2}{c}{vGIN} \\ \cline{2-12}  & 75.47(16.1) & 76.02(2) & 61.56(30.7) & 77.92(21.6) & 76.92(21.5) & 65.34(30.4) & 74.93(12.2) & 72.10(56.5) & 71.50(24.0) & 48.87(14.0) & 47.72(29.9) \\ \multirow{2}{*}{DropEdge} & 72.00(28.5) & 72.15(23.2) & 76.23(29.1) & 76.24(26.4) & 76.52(30.3) & 74.23(72.7) & 71.60(55.8) & 72.30(18.9) & 73.05(18.4) & 48.47(40.8) & 47.67(28.5) \\  & 29.70(28.4) & 75.20(28.3) & 76.28(39.4) & 60.46(28.1) & 78.62(10.45) & 65.37(30.34) & 74.78(20.7) & 71.60(5.8) & 71.30(38.1) & 48.47(40.8) & 47.67(28.38) \\  & 29.70(28.4) & 75.11(38.9) & 74.93(8.3) & 62.31(48.8) & 74.51(45.4) & 65.42(79.7) & 74.61(43.68) & 71.61(10.43) & 71.10(40.3) & 70.54(24.0) & 49.61(42.45) & 48.00(30.35) \\  &the average time spent on the mixup procedures of FGWMixup and FGWMixup\({}_{*}\) per fold. We can observe that FGWMixup\({}_{*}\) decreases the mixup time cost by a distinct margin, providing at least 2.03\(\times\) and up to 3.46\(\times\) of efficiency promotion. The results are consistent with our theoretical analysis of the convergence rate improvements, as shown in Proposition 1. More detailed efficiency statistics and discussions are introduced in Appendix E.3.

Infeasibility Analysis on the Single-loop FGW Solver in FGWMixup.We conduct an experiment to analyze the infeasibility of our single-loop FGW solver compared with the strict CG solver. Practically, we randomly select 1,000 pairs of graphs from PROTEINS dataset and apply the two solvers to calculate the FGW distance between each pair of graphs. The distances of the \(i\)-th pair of graphs calculated by the strict solver and the relaxed solver are denoted as \(d_{i}\) and \(d_{i}^{*}\), respectively. We report the following metrics for comparison:

* **MAE**: Mean absolute error of FGW distance, i.e., \(\frac{1}{N}\sum|d_{i}-d_{i}^{*}|\).
* **MAPE**: Mean absolute percentage error of FGW distance, i.e., \(\frac{1}{N}\sum\frac{|d_{i}-d_{i}^{*}|}{d_{i}}\).
* **mean-FGW**: Mean FGW distance given by the strict CG solver, i.e., \(\frac{1}{N}\sum d_{i}\).
* **mean-FGW***: Mean FGW distance given by the single-loop solver, i.e., \(\frac{1}{N}\sum d_{i}^{*}\).
* **T-diff**: L2-norm of the difference between two transportation plan matrices (divided by the size of the matrix for normalization).

The results are shown in Table 4. We can observe that the MAPE is only 0.0748, which means the FGW distance estimated by the single-loop relaxed solver is only 7.48% different from the strict CG solver. Moreover, the absolute error is around 0.01, which is quite small compared with the absolute value of FGW distances (~0.21). We can also find that the L2-norm of the difference between two transportation plan matrices is only 0.0006, which means two solvers give quite similar transportation plans. All the results imply that the single-loop solver will not produce huge infeasibility or make the estimation of FGW distance inaccurate.

### Further Analyses

Effects of the Trade-off Coefficient \(\alpha\)We provide sensitivity analysis w.r.t. the trade-off coefficient \(\alpha\) of our proposed mixup methods valued from {0.05, 0.5, 0.95, 1.0} on two backbones and two

\begin{table}
\begin{tabular}{c c c c c} \hline \hline MAE & MAPE & mean-FGW & mean-FGW* & T-diff \\ \hline
0.0126(0.0170) & 0.0748(0.1022) & 0.2198 & 0.2143 & 0.0006(0.0010) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Infeasibility analysis on the single-loop FGW solver in FGWMixup\({}_{*}\).

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{3}{c|}{IMDB-B} & \multicolumn{3}{c}{NCI1} \\  & 20\% & 40\% & 60\% & 20\% & 40\% & 60\% \\ \hline vanilla & 70.00(5.16) & 59.70(5.06) & 47.90(4.30) & 70.58(1.29) & 61.95(2.19) & 48.25(4.87) \\ DropEdge & 68.30(5.85) & 59.40(5.00) & 50.10(1.92) & 69.51(2.27) & 60.32(2.60) & 49.61(1.28) \\ M-Mixup & 70.70(5.90) & 59.70(5.87) & 50.90(1.81) & 71.53(2.75) & 63.24(2.59) & 48.66(3.02) \\ \(\mathcal{G}\)-Mixup & 67.50(4.52) & 59.10(4.74) & 49.40(2.87) & 72.46(1.95) & 63.26(4.39) & 50.01(1.26) \\ FGWMixup\({}_{*}\) & 70.10(4.39) & **61.90(6.17)** & 50.80(3.19) & **72.92(1.56)** & 62.99(1.35) & **50.12(3.51)** \\ FGWMixup\({}_{*}\) & **70.80(3.97)** & 61.80(5.69) & **51.00(1.54)** & 72.75(2.29) & **63.55(2.60)** & 50.02(3.38) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Experimental results of robustness against label corruption with different ratios.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline \multirow{2}{*}{Datasets} & \multicolumn{3}{c}{Avg. Mixup Time (s) / Fold} \\  & PROTEINS & NCI1 & NCI109 & IMDB-B & IMDB-M \\ \hline FGWMixup & 802.24 & 1711.45 & 1747.24 & 296.62 & 212.53 \\ FGWMixup\({}_{*}\) & **394.57** & **637.41** & **608.61** & **85.69** & **74.53** \\ Speedup & 2.03\(\times\) & 2.67\(\times\) & 2.74\(\times\) & 3.46\(\times\) & 2.85\(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Comparisons of algorithm execution efficiency between FGWMixup and FGWMixup\({}_{*}\).

datasets. Note that \(\alpha\) controls the weights of the node feature alignment and graph structure alignment costs, and \(\alpha=1.0\) falls back to the case of GW metric where node features are not incorporated. From the results shown in Table 5, we can observe that: 1) when FGW falls back to GW (\(\alpha\) =1), where node features are no longer taken into account, the performance will significantly decay (generally the worst among all investigated \(\alpha\) values). This demonstrates the importance of solving the joint modeling problem in graph mixup tasks. 2) \(\alpha\)=0.95 is the best setting in most cases. This empirically implies that it is better to conduct more structural alignment in graph mixup. In practice, we set \(\alpha\) to 0.95 for all of our reported results.

Effects of Mixup Graph SizesWe investigate the performance of FGWMixup and FGWMixup\({}_{*}\) with various mixup graph sizes (node numbers), including adaptive graph sizes (i.e., weighted average size of the mixup source graphs, \(\bar{n}=\lambda n_{1}+(1-\lambda)n_{2}\), which is selected as our method) and fixed graph sizes (i.e., the median size of all training graphs, 0.5 \(\times\) and 2 \(\times\) the median). The ablation studies are composed on NCI1 and PROTEINS datasets using vGCN backbone as motivating examples, and the results are illustrated in Fig.1. We can observe that the best performance is steadily obtained when selecting graph sizes with an adaptive strategy, whereas the fixed graph sizes lead to unstable results. Specifically, selecting the median or larger size may occasionally yield comparable performances, yet selecting the smaller size can result in an overall performance decay of over 1%. This phenomenon is associated with the graph size generalization problem [47; 48] that describes the performance degradation of GNNs caused by the graph size distributional shift between training and testing data. The fixed strategy may aggravate this problem, particularly for small graphs that struggle to generalize to larger ones. In contrast, the adaptive strategy can potentially combat this distributional shift by increasing the data diversity and reach better test time performance.

Effects of GNN DepthsTo validate the improvement of our methods across various model depths, we evaluate the performance of FGWMixup and FGWMixup\({}_{*}\) using vGCNs equipped with different numbers (3-8) of layers. We experiment on NCI1 and PROTEINS datasets, and the results are illustrated in Fig.2. We can observe that our methods consistently improve the performance on NCI1 under all GCN depth settings by a significant margin. The same conclusion is also true for most cases on PROTEINS except for the 7-layer vGCN. These results indicate a universal improvement of our methods on GNNs with various depths.

Other DiscussionsMore further analyses are introduced in the Appendix, including qualitative analyses of our mixup results (see Appendix E.1), further discussions on \(\mathcal{G}\)-Mixup (see Appendix E.2), and sensitivity analysis of the hyperparameter \(k\) in Beta distribution where mixup weights are sampled from (see Appendix E.4).

Figure 1: Test performance of our methods with different graph size settings on NCI1 and PROTEINS datasets using vGCNs as the backbone.

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{4}{c}{PROTEINS} & \multicolumn{4}{c}{NCI1} \\  & \(\alpha\)=0.95 & \(\alpha\)=0.05 & \(\alpha\)=1.0 & \(\alpha\)=0.95 & \(\alpha\)=0.5 & \(\alpha\)=0.05 & \(\alpha\)=1.0 \\ \hline vGIN-FGMMixup & 75.02(3.86) & **75.30(2.58)** & 74.86(2.40) & 74.57(2.62) & **78.32(2.65)** & 77.42(1.93) & 77.62(2.37) & 75.91(2.93) \\ vGCN-FGMMixup & **76.01(3.19)** & 75.47(3.56) & 74.93(2.74) & 74.40(3.57) & **78.37(2.40)** & 77.93(1.68) & 78.00(1.00) & 77.27(0.92) \\ vGIN-FGMMixup, & **75.20(3.30)** & 74.57(3.30) & 74.39(3.01) & 73.94(3.86) & **77.27(2.71)** & 77.23(2.47) & 76.59(2.14) & 77.20(1.69) \\ vGCN-FGMMixup, & **75.20(3.03)** & 74.57(3.52) & 74.84(3.16) & 74.66(2.91) & 78.47(1.74) & 77.66(1.48) & **78.93(1.91)** & 77.71(1.97) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Experimental results of different \(\alpha\) on PROTEINS and NCI1 datasets.

## 4 Related Works

Graph Data AugmentationThere are currently two mainstream perspectives of graph data augmentation for graph-level classifications. A line of research concentrates on graph signal augmentation [41, 18, 19]. Node Attribute Masking [41] assigns random node features for a certain ratio of graph nodes. IfMixup [19] conducts Euclidean mixup on node features from different graphs with arbitrary node alignment strategy, whereas it damages the critical topologies (e.g., rings, bipartiblity) of the source graphs. Another line of work focuses on graph structure augmentation [49, 20, 21, 22, 50, 41, 51]. Approaches like Subgraph[49, 41], DropEdge[40] and GAug[50] conduct removals or additions of graph nodes or edges to generate new graph structures. Graph Transplant[21] and Submix[22] realize CutMix-like augmentations on graphs, which cut off a subgraph from the original graph and replace it with another. \(\mathcal{G}\)-Mixup [20] estimates a graph structure generator (i.e., graphon) for each class of graphs and conducts Euclidean addition on graphons to realize structural mixup. However, as GNNs can capture the complex interaction between the two entangled yet complementary input spaces, it is essential to model this interaction for a comprehensive graph data augmentation. Regrettably, current works have devoted little attention to this interaction.

Optimal Transport on GraphsBuilding upon traditional Optimal Transport (OT) [25] methods (e.g., Wasserstein distance[52]), graph OT allows defining a very general distance metric between the structured/relational data points embedded in different metric spaces, where the data points are modeled as probability distributions. It proceeds by solving a coupling between the distributions that minimizes a specific cost. The solution of graph OT hugely relies on the (Fused) Gromov-Wasserstein (GW) [27, 26] distance metric. Further works employ the GW couplings for solving tasks such as graph node matching and partitioning [53, 32, 54], and utilize GW distance as a common metric in graph metric learning frameworks [55, 31, 56]. Due to the high complexity of the GW-series algorithm, another line of research [28, 57, 58, 29] concentrates on boosting the computational efficiency. In our work, we formulate graph mixup as an FGW-based OT problem that solves the optimal node matching strategy minimizing the alignment costs of graph structures and signals. Meanwhile, we attempt to accelerate the mixup procedure with a faster convergence rate.

## 5 Conclusion and Limitation

In this work, we introduce a novel graph data augmentation method for graph-level classifications dubbed FGWMixup. FGWMixup formulates the mixup of two graphs as an optimal transport problem aiming to seek a "midpoint" of two graphs embedded in the graph signal-structure fused metric space. We employ the FGW distance metric to solve the problem and further propose an accelerated algorithm that improves the convergence rate from \(\mathcal{O}(t^{-1})\) to \(\mathcal{O}(t^{-2})\) for better scalability of our method. Comprehensive experiments demonstrate the effectiveness of FGWMixup in terms of enhancing the performance, generalizability, and robustness of GNNs, and also validate the efficiency and correctness of our acceleration strategy.

Despite the promising results obtained in our work, it is important to acknowledge its limitations. Our focus has primarily been on graph data augmentation for graph-level classifications. However, it still remains a challenging question to better exploit the interactions between the graph signal and structure spaces for data augmentation in other graph prediction tasks, such as link prediction and node classification. Moreover, we experiment with four classic (MPNNs) and advanced (Graphormers) GNNs, while there remain other frameworks that could be taken into account. We expect to carry out a graph classification benchmark with more comprehensive GNN frameworks in our future work.

Figure 2: Test performance of our methods using vGCNs with varying numbers of layers on NCI1 and PROTEINS datasets.

## Acknowledgements

This work was supported by the National Natural Science Foundation of China (No.82241052).

## References

* [1] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.
* [2] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_.
* [3] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In _International Conference on Machine Learning_, pages 1263-1272. PMLR, 2017.
* [4] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical Science_, 9(2):513-530, 2018.
* [5] Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In _Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, pages 1365-1374, 2015.
* [6] Xinyu Ma, Yasha Wang, Xu Chu, Liantao Ma, Wen Tang, Junfeng Zhao, Ye Yuan, and Guoren Wang. Patient health representation learning via correlational sparse prior of medical features. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* [7] Yongxin Xu, Xu Chu, Kai Yang, Zhiyuan Wang, Peinie Zou, Hongxin Ding, Junfeng Zhao, Yasha Wang, and Bing Xie. Seqcare: Sequential training with external medical knowledge graph for diagnosis prediction in healthcare data. In _Proceedings of the ACM Web Conference 2023_, pages 2819-2830, 2023.
* [8] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* [9] Suorong Yang, Weikang Xiao, Mengcheng Zhang, Suhan Guo, Jian Zhao, and Furao Shen. Image data augmentation for deep learning: A survey. _arXiv preprint arXiv:2204.08610_, 2022.
* [10] Jason Wei and Kai Zou. Eda: Easy data augmentation techniques for boosting performance on text classification tasks. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 6382-6388, 2019.
* [11] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classification. _Advances in Neural Information Processing Systems_, 28, 2015.
* [12] Zhiqiang Xu, Yiping Ke, Yi Wang, Hong Cheng, and James Cheng. A model-based approach to attributed graph clustering. In _Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data_, pages 505-516, 2012.
* [13] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* [14] Lingxiao Zhao, Wei Jin, Leman Akoglu, and Neil Shah. From stars to subgraphs: Uplifting any gnn with local structure awareness. _arXiv preprint arXiv:2110.03753_, 2021.
* [15] Beatrice Bevilacqua, Fabrizio Frasca, Derek Lim, Balasubramaniam Srinivasan, Chen Cai, Gopinath Balamurugan, Michael M Bronstein, and Haggai Maron. Equivariant subgraph aggregation networks. _arXiv preprint arXiv:2110.02910_, 2021.
* [16] Bohang Zhang, Shengjie Luo, Liwei Wang, and Di He. Rethinking the expressive power of gnns via graph biconnectivity. _arXiv preprint arXiv:2301.09505_, 2023.

* [17] Pan Li and Jure Leskovec. The expressive power of graph neural networks. _Graph Neural Networks: Foundations, Frontiers, and Applications_, pages 63-98, 2022.
* [18] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, Juncheng Liu, and Bryan Hooi. Nodeaug: Semi-supervised node classification with data augmentation. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 207-217, 2020.
* [19] Hongyu Guo and Yongyi Mao. ifmixup: Towards intrusion-free graph mixup for graph classification. _arXiv e-prints_, pages arXiv-2110, 2021.
* [20] Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-mixup: Graph data augmentation for graph classification. In _International Conference on Machine Learning_, pages 8230-8248. PMLR, 2022.
* [21] Joonhyung Park, Hajin Shim, and Eunho Yang. Graph transplant: Node saliency-guided graph mixup with local structure preservation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7966-7974, 2022.
* [22] Jaemin Yoo, Sooyeon Shim, and U Kang. Model-agnostic augmentation for accurate graph classification. In _Proceedings of the ACM Web Conference 2022_, pages 1281-1291, 2022.
* [23] Antonio Ortega, Pascal Frossard, Jelena Kovacevic, Jose MF Moura, and Pierre Vandergheynst. Graph signal processing: Overview, challenges, and applications. _Proceedings of the IEEE_, 106(5):808-828, 2018.
* [24] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* [25] Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data science. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019.
* [26] Vayer Titouan, Nicolas Courty, Romain Tavenard, and Remi Flamary. Optimal transport for structured data with application on graphs. In _International Conference on Machine Learning_, pages 6275-6284. PMLR, 2019.
* [27] Facundo Memoli. Gromov-wasserstein distances and the metric approach to object matching. _Foundations of Computational Mathematics_, 11:417-487, 2011.
* [28] Gabriel Peyre, Marco Cuturi, and Justin Solomon. Gromov-wasserstein averaging of kernel and distance matrices. In _International Conference on Machine Learning_, pages 2664-2672. PMLR, 2016.
* [29] Jiajin Li, Jianheng Tang, Lemin Kong, Huikang Liu, Jia Li, Anthony Man-Cho So, and Jose Blanchet. A convergent single-loop algorithm for relaxation of gromov-wasserstein in graph data. _arXiv preprint arXiv:2303.06595_, 2023.
* [30] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. _Advances in Neural Information Processing Systems_, 26, 2013.
* [31] Cedric Vincent-Cuaz, Remi Flamary, Marco Corneli, Titouan Vayer, and Nicolas Courty. Template based graph neural network with optimal transport distances. _arXiv preprint arXiv:2205.15733_, 2022.
* [32] Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-wasserstein learning for graph matching and node embedding. In _International Conference on Machine Learning_, pages 6932-6941. PMLR, 2019.
* [33] Remi Flamary, Nicolas Courty, Devis Tuia, and Alain Rakotomamonjy. Optimal transport with laplacian regularization: Applications to domain adaptation and shape matching. In _NIPS Workshop on Optimal Transport and Machine Learning OTML_, 2014.
* [34] Meyer Scetbon, Marco Cuturi, and Gabriel Peyre. Low-rank sinkhorn factorization. In _International Conference on Machine Learning_, pages 9344-9354. PMLR, 2021.

* [35] Christopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. Tudataset: A collection of benchmark datasets for learning with graphs. _arXiv preprint arXiv:2007.08663_, 2020.
* [36] Nikil Wale and George Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. In _Sixth International Conference on Data Mining (ICDM'06)_, pages 678-689. IEEE, 2006.
* [37] Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-lehman graph kernels. _Journal of Machine Learning Research_, 12(9), 2011.
* [38] Karsten M Borgwardt, Cheng Soon Ong, Stefan Schonauer, SVN Vishwanathan, Alex J Smola, and Hans-Peter Kriegel. Protein function prediction via graph kernels. _Bioinformatics_, 21(suppl_1):i47-i56, 2005.
* [39] Chengxuan Ying, Tianleng Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? _Advances in Neural Information Processing Systems_, 34:28877-28888, 2021.
* [40] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolutional networks on node classification. _arXiv preprint arXiv:1907.10903_, 2019.
* [41] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang Wang, and Yang Shen. Graph contrastive learning with augmentations. _Advances in Neural Information Processing Systems_, 33:5812-5823, 2020.
* [42] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Mixup for node and graph classification. In _Proceedings of the Web Conference 2021_, pages 3663-3674, 2021.
* [43] Yoshua Bengio and Yves Grandvalet. No unbiased estimator of the variance of k-fold cross-validation. _Advances in Neural Information Processing Systems_, 16, 2003.
* [44] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan Yu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang. Deep graph library: A graph-centric, highly-performant package for graph neural networks. _arXiv preprint arXiv:1909.01315_, 2019.
* [45] Remi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurelie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenlos, Kilian Fatras, Nemo Fournier, Leo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. _Journal of Machine Learning Research_, 22(78):1-8, 2021.
* [46] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_, 33:22118-22133, 2020.
* [47] Davide Buffelli, Pietro Lio, and Fabio Vandin. Sizeshiftreg: a regularization method for improving size-generalization in graph neural networks. In _Advances in Neural Information Processing Systems_, 2022.
* [48] Beatrice Bevilacqua, Yangze Zhou, and Bruno Ribeiro. Size-invariant graph representations for graph classification extrapolations. In _International Conference on Machine Learning_, pages 837-851. PMLR, 2021.
* [49] Yiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Graphrcop: Subgraph cropping for graph classification. _arXiv preprint arXiv:2009.10564_, 2020.
* [50] Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data augmentation for graph neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 11015-11023, 2021.

* [51] Hyeonjin Park, Seunghun Lee, Sihyeon Kim, Jinyoung Park, Jisu Jeong, Kyung-Min Kim, Jung-Woo Ha, and Hyunwoo J Kim. Metropolis-hastings data augmentation for graph neural networks. _Advances in Neural Information Processing Systems_, 34:19010-19020, 2021.
* [52] Cedric Villani. _Topics in Optimal Transportation_. Number 58. American Mathematical Soc., 2003.
* [53] Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph partitioning and matching. _Advances in Neural Information Processing Systems_, 32, 2019.
* [54] Samir Chowdhury and Tom Needham. Generalized spectral clustering via gromov-wasserstein learning. In _International Conference on Artificial Intelligence and Statistics_, pages 712-720. PMLR, 2021.
* [55] Cedric Vincent-Cuaz, Titouan Vayer, Remi Flamary, Marco Corneli, and Nicolas Courty. Online graph dictionary learning. In _International Conference on Machine Learning_, pages 10564-10574. PMLR, 2021.
* [56] Benson Chen, Gary Becigneul, Octavian-Eugen Ganea, Regina Barzilay, and Tommi Jaakkola. Optimal transport graph neural networks. _arXiv preprint arXiv:2006.04804_, 2020.
* [57] Cedric Vincent-Cuaz, Remi Flamary, Marco Corneli, Titouan Vayer, and Nicolas Courty. Semi-relaxed gromov-wasserstein divergence with applications on graphs. _arXiv preprint arXiv:2110.02753_, 2021.
* [58] Meyer Scetbon, Gabriel Peyre, and Marco Cuturi. Linear-time gromov wasserstein distances using low rank couplings and costs. In _International Conference on Machine Learning_, pages 19347-19365. PMLR, 2022.
* [59] Promit Ghosal and Marcel Nutz. On the convergence rate of sinkhorn's algorithm. _arXiv preprint arXiv:2212.06000_, 2022.
* [60] Heinz H Bauschke. Projection algorithms and monotone operators. 1996.
* [61] Zhi-Quan Luo and Paul Tseng. Error bound and convergence analysis of matrix splitting algorithms for the affine variational inequality problem. _SIAM Journal on Optimization_, 2(1):43-54, 1992.
* [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in Neural Information Processing Systems_, 30, 2017.
* [63] Douglas J Klein and Milan Randic. Resistance distance. _Journal of Mathematical Chemistry_, 12:81-95, 1993.
* [64] Hongteng Xu, Peilin Zhao, Junzhou Huang, and Dixin Luo. Learning graphon autoencoders for generative graph modeling. _arXiv preprint arXiv:2105.14244_, 2021.
* [65] Nikolay Sakharnykh and Braun Hugo. Efficient maximum flow allgorithm.

Problem Properties

### FGW Distance Metric

Fused Gromov-Wasserstein Distance [26] are defined in Eq.1. It can also be rewritten as follows:

\[\mathrm{FGW}(G_{1},G_{2})=\min_{\bm{\pi}\in\Pi(\bm{\mu}_{1},\bm{\mu}_{2})}\langle (1-\alpha)\bm{D}+\alpha L(\bm{A}_{1},\bm{A}_{2})\otimes\bm{\pi},\bm{\pi}\rangle,\] (8)

where \(\bm{D}=\{d(\bm{x}_{i}^{(1)},\bm{x}_{j}^{(2)})\}_{ij}\) is the distance matrix of node features, and \(L(\bm{A}_{1},\bm{A}_{2})=\{|\bm{A}_{1}(i,k)-\bm{A}_{2}(j,\ell)|\}_{i,j,k,\ell}\) is a 4-dimensional tensor that depicts the structural distances. We denote \(\langle\bm{U},\bm{V}\rangle=\mathrm{tr}(\bm{U}^{\top}\bm{V})\) as the matrix scalar product. The \(\otimes\) operator conducts tensor-matrix multiplication as: \(\mathcal{L}\otimes T\ \stackrel{{\text{def.}}}{{=}}\ \left(\sum_{k,\ell} \mathcal{L}_{i,j,k,\ell}T_{k,\ell}\right)_{i,j}\). Then according to Proposition 1 in [28], when we take the \(\ell_{2}\)-norm to calculate the structural distance, we have:

\[L(\bm{A}_{1},\bm{A}_{2})\otimes\bm{\pi}=c_{\bm{A}_{1},\bm{A}_{2}}-2\bm{A}_{1} \bm{\pi}\bm{A}_{2},\] (9)

where \(c_{\bm{A}_{1},\bm{A}_{2}}=\bm{A}_{1}^{\otimes 2}\bm{\mu}_{1}\bm{1}_{n_{2}}^{\top} +\bm{1}_{n_{1}}\bm{\mu}_{2}^{\top}\bm{A}_{2}^{\otimes 2}\), and \({}^{\otimes 2}\) denotes the Hadamard square (i.e., \(\bm{U}^{\otimes 2}=\bm{U}\odot\bm{U}\)). Taking Eq.9 into Eq.8, we can rewrite the minimizer as:

\[\mathrm{RHS} =(1-\alpha)\langle\bm{D},\bm{\pi}\rangle+\overbrace{\alpha\langle \bm{A}_{1}^{\otimes 2}\bm{\mu}_{1}\bm{1}_{n_{2}}^{\top},\bm{\pi}\rangle+\alpha \langle\bm{1}_{n_{1}}\bm{\mu}_{2}^{\top}\bm{A}_{2}^{\otimes 2},\bm{\pi}\rangle}-2 \alpha\langle\bm{A}_{1}\bm{\pi}\bm{A}_{2},\bm{\pi}\rangle\] (10) \[=\underbrace{\alpha(\bm{A}_{1}^{\otimes 2}\bm{\mu}_{1}^{\top} \bm{\mu}_{1}+\bm{A}_{2}^{\otimes 2}\bm{\mu}_{2}^{\top}\bm{\mu}_{2})}_{\mathrm{ constant}}+\langle(1-\alpha)\bm{D}-2\alpha\bm{A}_{1}\bm{\pi}\bm{A}_{2},\bm{\pi}\rangle\]

The items in the overbrace become a constant because the marginals of \(\bm{\pi}\) are fixed as \(\bm{\mu}_{1}\) and \(\bm{\mu}_{2}\), respectively. Therefore, we can remove the constant term and formulate an equivalent optimization objective denoted as \(\overline{\mathrm{FGW}}(G_{1},G_{2})\), which can be rewritten as:

\[\overline{\mathrm{FGW}}(G_{1},G_{2}) =\min_{\bm{\pi}\in\Pi(\bm{\mu}_{1},\bm{\mu}_{2})}\langle(1-\alpha )\bm{D}-2\alpha\bm{A}_{1}\bm{\pi}\bm{A}_{2},\bm{\pi}\rangle\] (11) \[=\min_{\bm{\pi}\in\Pi(\bm{\mu}_{1},\bm{\mu}_{2})}(1-\alpha)\, \mathrm{tr}(\bm{\pi}^{\top}\bm{D})-2\alpha\,\mathrm{tr}(\bm{\pi}^{\top}\bm{A} _{1}\bm{\pi}\bm{A}_{2}).\]

We denote the above optimization objective as a function \(f(\bm{\pi})\) w.r.t. \(\bm{\pi}\), and we call it the FGW function in the main text.

### Mirror Descent and the Relaxed FGW Solver in \(\mathrm{FGWMixup}_{*}\)

**Definition 1** (Normal Cone).: _Given any set \(\mathcal{C}\) and point \(x\in\mathcal{C}\), we can define **normal cone** as:_

\[\mathcal{N}_{\mathcal{C}}(x)=\{g:g^{\top}x\geq g^{\top}y,\forall y\in\mathcal{ C}\}\]

There are some properties of normal cone. The normal cone is always convex, and \(\mathrm{Proj}_{\mathcal{C}}(x+z)=x,\forall x\in\mathcal{C},z\in\mathcal{N}_{ \mathcal{C}}(x)\) always holds.

**Proposition 3** (The Update of Mirror Descent).: _The update of Mirror Descent takes the form of \(x^{(k+1)}=\arg\min_{x\in\mathcal{X}}D_{\phi}(x,y^{(k+1)})\)), where \(\phi(\cdot)\) is a Bregman Projection, and \(\nabla\phi(y^{(k+1)})=\nabla\phi(\omega^{(k)})-\nabla f(\pi)/\rho\)._

Proof.: For a constrained optimization problem \(\min_{x\in\mathcal{X}}f(x),\mathcal{X}\subset\mathcal{D}\), Mirror Descent iteratively updates \(x\) with \(x^{(k+1)}=\arg\min_{x\in\mathcal{X}}\nabla f(x^{(k)})^{\top}x+\rho D_{\phi}(x,x ^{(k)})\) with Bregman divergence \(D_{\phi}(x,y):=\phi(x)-\phi(y)-\langle\nabla\phi(y),x-y\rangle\). We further have:

\[x^{(k+1)}=\arg\min_{x\in\mathcal{X}}\nabla f(x^{(k)})^{\top}x+ \rho D_{\phi}(x,x^{(k)})\] (12) \[\Rightarrow 0\in\nabla f(x^{(k)})^{\top}/\rho+\nabla\phi(x^{(k+1)})-\nabla \phi(x^{(k)})+\mathcal{N}_{\mathcal{X}}(x^{(k+1)})\] \[\Rightarrow x^{(k+1)}=\arg\min_{x\in\mathcal{X}}D_{\phi}(x,y^{(k+1)})),\ \mathrm{ where}\ \nabla\phi(y^{(k+1)})=\nabla\phi(x^{(k)})-\nabla f(x^{(k)})/\rho\]This updating procedure is vividly depicted by Fig.3. For each update of \(x^{(k)}\), MD conducts the following steps:

1. Apply mirror mapping (\(\nabla\phi(\cdot)\)) on \(x^{(k)}\) from the primal space \(\mathcal{D}\) to the dual space \(\nabla\phi(\mathcal{D})\).
2. Take a gradient step in the dual space, which is \(\delta^{(k+1)}=\nabla\phi(x^{(k)})-\nabla f(x^{(k)})/\rho\).
3. Inversely mapping (\(\nabla\phi^{-1}(\cdot)\)) the dual space back to the primal space, which is \(y^{(k+1)}=\nabla\phi^{-1}(\delta^{(k+1)})\), also written as \(\nabla\phi(y^{(k+1)})=\nabla\phi(x^{(k)})-\nabla f(x^{(k)})/\rho\).
4. The updated point may fall out of the constraints of \(\mathcal{X}\). Thus, Bregman projection is conducted to project \(y^{(k+1)}\) to \(x^{(k+1)}\in\mathcal{X}\), where \(x^{(k+1)}=\arg\min_{x\in\mathcal{X}}D_{\phi}(x,y^{(k+1)}))\).

The relaxed FGW conducts projections to constraints of rows and columns alternately, which is written as \(\min_{\pi}f(\pi)+\mathbb{I}_{\{\pi\in\Pi_{1}\}}+\mathbb{I}_{\{\pi\in\Pi_{2}\}}\), where \(\mathbb{I}\) is the indicator function. We adopt the operator splitting strategy, and the optimization can be formulated as:

\[F_{\rho}(\pi,\omega)=f(\pi)+f(\omega)+\mathbb{I}_{\{\pi\in\Pi_{1}\}}+\mathbb{I }_{\{\omega\in\Pi_{2}\}}\] (13)

We apply Mirror Descent algorithm with Bregman divergence \(D_{\phi}(x,y)\) to solve this problem, whose update takes the form of:

\[\begin{split}\pi^{(k+1)}&=\arg\min_{\pi\in\Pi_{1}} \nabla f(\pi)^{\top}\pi+\rho D_{\phi}\left(\pi,\omega^{(k)}\right)\\ \omega^{(k+1)}&=\arg\min_{\omega\in\Pi_{2}}\nabla f( \omega)^{\top}\omega+\rho D_{\phi}\left(\omega,\pi^{(k+1)}\right)\end{split}\] (14)

Let's take the MD update of \(\pi\) as an example. Taking relative entropy as \(\phi(x)=\sum_{i}x_{i}\log x_{i}\), we can solve \(y^{(k+1)}\) (conducting steps 1 to 3) with exact analytical solutions. Specifically, \(\nabla\phi(x)=\sum_{i}(1+\log x_{i})\), and the update w.r.t. \(\pi\) writes:

\[\begin{split}\pi^{(k+1)}&=\arg\min_{\pi\in\Pi_{1}}D _{\phi}(\pi,y^{(k+1)}),\;\mathrm{where}\;\log(y^{(k+1)})=\log(\omega^{(k)})- \nabla f(\omega^{(k)})/\rho\\ \Rightarrow\pi^{(k+1)}&=\arg\min_{\pi\in\Pi_{1}}D_{ \phi}(\pi,y^{(k+1)}),\;\mathrm{where}\;y^{(k+1)}=\omega^{(k)}\odot\exp(- \nabla f(\omega^{(k)})/\rho).\end{split}\] (15)

Furthermore, under this setting, we also have an exact solution of the projection step 4\(\arg\min_{\pi\in\Pi_{1}}D_{\phi}(\pi,y^{(k+1)}))=\mathrm{diag}(\mu_{1\cdot}y^ {(k+1)}1_{n_{2}})y^{(k+1)}\). Then the MD update procedure can be formulated according to Prop. 3 as follows:

\[\pi^{(k+1)}\leftarrow\omega^{(k)}\odot\exp(-\nabla f(\omega^{(k)})/\rho),\pi ^{(k+1)}\leftarrow\mathrm{diag}(\mu_{1\cdot}/\pi^{(k+1)}1_{n_{2}})\pi^{(k+1)}\] (16)

Furthermore, the sub-gradient of our FGW optimizing objective _w.r.t._\(\pi\) is calculated as follows:

\[\nabla f(\pi)=\nabla_{\pi}\mathrm{FGW}(G_{1},G_{2})=(1-\alpha)\bm{D}-4\alpha \bm{A}_{1}\pi\bm{A}_{2}.\] (17)

Taking Eq.17 into Eq.16, we have the update of \(\pi\), and a similar procedure is conducted for \(\omega\). Setting \(\rho\) to \(1/\gamma\), the final update procedure is shown in Line 8-11 in Alg.2.

Figure 3: The illustration of the MD procedure.

Proofs of Theoretical Results

### Proof of Proposition 1

Proof.: From Corollary 4.6 of [59] we have: for any Sinkhorn iteration optimizing \(\pi\in\Pi(\mu,\nu)\) on two different marginals alternately \(\mu,\nu\), let \(t_{1}=\inf\{t\geq 0:H(\mu_{2t}|\mu)\leq 1\}-1\), when iterations \(t\geq 2t_{1}\):

\[\begin{split} H\left(\mu_{2t}\mid\mu\right)+H\left(\mu\mid\mu_{2 t}\right)&\leq 10\frac{C_{1}^{2}\lor H\left(\pi_{*}\mid R\right)}{ \left(\lfloor t/2\rfloor-t_{1}\right)t}=\mathcal{O}\left(t^{-2}\right),\\ H\left(\pi_{2t}\mid\pi_{*}\right)+H\left(\pi_{*}\mid\pi_{2t} \right)&\leq 5\frac{C_{1}^{2}\vee\left(H\left(\pi_{*}\mid R \right)^{1/2}C_{1}\right)}{\sqrt{\left(\lfloor t/2\rfloor-t_{1}\right)t}}= \mathcal{O}\left(t^{-1}\right),\end{split}\] (18)

where \(C_{1}\) is a constant, and \(R\) is a fixed reference measure. 

### Proof of Proposition 2

**Definition 2** (Bounded Linear Regularity, Definition 5.6 in [60]).: _Let \(C_{1},C_{2},\cdots,C_{N}\) be closed convex subsets of \(\mathbb{R}^{n}\) with a non-empty intersection \(C\). We call the set \(\{C_{1},C_{2},\cdots,C_{N}\}\) is **bounded linearly regular** if for every bounded subset \(\mathbb{B}\) of \(\mathbb{R}^{n}\), there exists a constant \(\kappa>0\) such that_

\[d(x,C)\leq\kappa\max_{i\in\{1,\ldots,N\}}d(x,C_{i}),\forall x\in\mathbb{B},\ \mathrm{where}\ d(x,\mathcal{C}):=\min_{x^{\prime}\in\mathcal{C}}\lVert x-x^{ \prime}\rVert.\]

The BLR condition naturally holds if all the \(C_{i}\) are polyhedral sets.

**Definition 3** (Strong Convexity).: _A differentiable function \(f(\cdot)\) is **strongly convex** with the constant \(m\), if the following inequality holds for all points \(x,y\) in its domain:_

\[(\nabla f(x)-\nabla f(y))^{\top}(x-y)\geq m\lVert x-y\rVert^{2},\]

_where \(\lVert\cdot\rVert\) is the corresponding norm._

In Mirror Descent algorithm, the Bregman divergence \(D_{h}(x,y)\) requires \(h\) to be strongly convex.

Proof.: Recall that the fixed point set of relaxed FGW is \(\mathcal{X}_{rel}=\{\pi^{*}\in C_{1},\omega^{*}\in C_{2}:0\in\nabla f(\pi^{*}) +\rho(\nabla h(\omega^{*})-\nabla h(\pi^{*}))+\mathcal{N}_{C_{1}}(\pi^{*}), \mathrm{and}\ 0\in\nabla f(\omega^{*})+\rho(\nabla h(\pi^{*})-\nabla h(\omega^{*}))+ \mathcal{N}_{C_{2}}(\omega^{*})\}\). Let \(p\in\mathcal{N}_{C_{1}}(\pi^{*})\) and \(q\in\mathcal{N}_{C_{2}}(\omega^{*})\), \(\mathcal{X}_{rel}\) is denoted as:

\[\begin{split}\mathcal{X}_{rel}=\{\pi^{*}\in C_{1},\omega^{*}\in C _{2}:&\nabla f(\pi^{*})+\rho(\nabla h(\omega^{*})-\nabla h(\pi^{* }))+p=0,p\in\mathcal{N}_{C_{1}}(\pi^{*})\\ &\nabla f(\omega^{*})+\rho(\nabla h(\pi^{*})-\nabla h(\omega^{*}) )+q=0,q\in\mathcal{N}_{C_{2}}(\omega^{*})\}\end{split}\] (19)

We denote \(\hat{\pi}=\mathrm{Proj}_{C_{1}\cap C_{2}}(\pi^{*})\). According to Definition 2, as \(C_{1}\) and \(C_{2}\) are both polyhedral constraints and satisfy the BLR condition, we have:

\[\begin{split}\lVert\hat{\pi}-\pi^{*}\rVert+\lVert\hat{\pi}- \omega^{*}\rVert&\leq 2\left\lVert\hat{\pi}-\pi^{*}\right\rVert+ \left\lVert\pi^{*}-\omega^{*}\right\rVert\\ &=2d\left(\pi^{*},C_{1}\cap C_{2}\right)+\left\lVert\pi^{*}- \omega^{*}\right\rVert\\ &\leq\left(2\kappa+1\right)\left\lVert\pi^{*}-\omega^{*}\right\rVert.\end{split}\] (20)

The first inequality holds based on the triangle inequality of the norm distances, and the second inequality holds based on the definition of point-to-space distance in Def.2. Moreover, according to the definition of \(\mathcal{X}_{rel}\), for any \((\pi^{*},\omega^{*})\in\mathcal{X}_{rel}\), we have:

\[\nabla f\left(\omega^{*}\right)^{\top}\left(\hat{\pi}-\pi^{*} \right)+\rho\left(\nabla h\left(\pi^{*}\right)-\nabla h\left(\omega^{*}\right) \right)^{\top}\left(\hat{\pi}-\pi^{*}\right)+p^{\top}\left(\hat{\pi}-\pi^{*} \right)=0,p\in\mathcal{N}_{C_{1}}\left(\pi^{*}\right)\] (21) \[\nabla f\left(\pi^{*}\right)^{\top}\left(\hat{\pi}-\omega^{*} \right)+\rho\left(\nabla h\left(\omega^{*}\right)-\nabla h\left(\pi^{*}\right) \right)^{\top}\left(\hat{\pi}-\omega^{*}\right)+q^{\top}\left(\hat{\pi}-\omega^ {*}\right)=0,q\in\mathcal{N}_{C_{2}}\left(\omega^{*}\right)\]

Summing up the equations above, we have:

\[\begin{split}&\nabla f\left(\omega^{*}\right)^{\top}\left(\hat{\pi}- \pi^{*}\right)+\nabla f\left(\pi^{*}\right)^{\top}\left(\hat{\pi}-\omega^{*} \right)+\rho\left(\nabla h\left(\pi^{*}\right)-\nabla h\left(\omega^{*} \right)\right)^{\top}\left(\omega^{*}-\pi^{*}\right)+p^{\top}\left(\hat{\pi}- \pi^{*}\right)+q^{\top}\left(\hat{\pi}-\omega^{*}\right)\\ &\stackrel{{(a)}}{{=}}\nabla f\left(\omega^{*}\right)^{ \top}\left(\hat{\pi}-\pi^{*}\right)+\nabla f\left(\pi^{*}\right)^{\top} \left(\hat{\pi}-\omega^{*}\right)-\rho\left(D_{h}\left(\pi^{*},\omega^{*} \right)+D_{h}\left(\omega^{*},\pi^{*}\right)\right)+p^{\top}\left(\hat{\pi}- \pi^{*}\right)+q^{\top}\left(\hat{\pi}-\omega^{*}\right)=0\end{split}\] (22)The \((a)\) equation holds based on the definition of Bregman Divergence \(D_{h}(x,y):=h(x)-h(y)-\langle\nabla h(y),x-y\rangle\). Furthermore, due to the property of normal cones \(\mathcal{N}_{C_{1}}(\pi^{*}),\mathcal{N}_{C_{2}}(\omega^{*})\), we have \(p^{\top}\left(\pi-\pi^{*}\right)\leq 0,\forall\pi\in C_{1}\) and \(q^{\top}\left(\omega-\omega^{*}\right)\leq 0,\forall\omega\in C_{2}\). Taking \(\pi\) and \(\omega\) as \(\hat{\pi}\in C_{1}\cap C_{2}\), we have \(p^{\top}\left(\hat{\pi}-\pi^{*}\right)+q^{\top}\left(\hat{\pi}-\omega^{*} \right)\leq 0\). Therefore, from 22 we have the following inequality:

\[\begin{split}\nabla f\left(\omega^{*}\right)^{\top}\left(\hat{ \pi}-\pi^{*}\right)+\nabla f\left(\pi^{*}\right)^{\top}\left(\hat{\pi}-\omega^ {*}\right)-\rho\left(D_{h}\left(\pi^{*},\omega^{*}\right)+D_{h}\left(\omega^{ *},\pi^{*}\right)\right)\geq 0\\ \Rightarrow\rho\left(D_{h}\left(\pi^{*},\omega^{*}\right)+D_{h} \left(\omega^{*},\pi^{*}\right)\right)\leq\nabla f\left(\omega^{*}\right)^{ \top}\left(\hat{\pi}-\pi^{*}\right)+\nabla f\left(\pi^{*}\right)^{\top}\left( \hat{\pi}-\omega^{*}\right)\\ \leq\|f\left(\omega^{*}\right)\||\hat{\pi}-\pi^{*}\|+\|\nabla f \left(\pi^{*}\right)\||\hat{\pi}-\omega^{*}\|\\ \overset{(b)}{\leq}C_{f}(\|\hat{\pi}-\pi^{*}\|+\|\hat{\pi}- \omega^{*}\|)\\ \overset{(20)}{\leq}C_{f}(2\kappa+1)\|\pi^{*}-\omega^{*}\|.\end{split}\] (23)

The \((b)\) inequality holds as the optimization objective is a quadratic function with bounded constraints, which means the gradient norm has a natural constant upper bound \(C_{f}\). For the left hand side of the inequality above, the sum of Bregman divergences is also lower bounded by

\[D_{h}\left(\pi^{*},\omega^{*}\right)+D_{h}\left(\omega^{*},\pi^{*}\right)= \left(\nabla h\left(\pi^{*}\right)-\nabla h\left(\omega^{*}\right)\right)^{ \top}\left(\pi^{*}-\omega^{*}\right)\geq\sigma\|\pi^{*}-\omega^{*}\|^{2},\] (24)

as \(h(\cdot)\) is \(\sigma\)-strongly convex in the definition of Bregman Divergence. Therefore, combining (24) and (23), we have:

\[\|\pi^{*}-\omega^{*}\|\leq\frac{C_{f}(2\kappa+1)}{\sigma\rho}\] (25)

Based on this important result, we analyze the distance between \(\mathcal{X}\) and \(\mathcal{X}_{rel}\), which is given by \(\mathrm{dist}(\frac{\pi^{*}+\omega^{*}}{2},\mathcal{X}):=\min_{x\in\mathcal{X }}\left\|\frac{\pi^{*}+\omega^{*}}{2}-x\right\|\), \(\pi^{*}\in C_{1},\omega^{*}\in C_{2}\). By adding up the two equations in 19 depicting the conditions on \(\pi^{*}\) and \(\omega^{*}\) of \(\mathcal{X}_{rel}\), we have:

\[\nabla f(\pi^{*})+\nabla f(\omega^{*})+p+q=0\overset{(c)}{\Rightarrow}\nabla f (\frac{\pi^{*}+\omega^{*}}{2})+\frac{p+q}{2}=0\] (26)

The \((c)\) derivation holds due to the linear property of the subgradient of FGW w.r.t. \(\pi\) (see Eq.17). Invoking this equality, we present the following upper bound analysis on \(\mathrm{dist}(\frac{\pi^{*}+\omega^{*}}{2},\mathcal{X})\):

\[\begin{split}\mathrm{dist}\left(\frac{\pi^{*}+w^{*}}{2}, \mathcal{X}\right)&\overset{(d)}{\leq}\epsilon\left\|\frac{\pi^{ *}+w^{*}}{2}-\mathrm{proj}_{C_{1}\cap C_{2}}\left(\frac{\pi^{*}+w^{*}}{2}- \nabla f\left(\frac{\pi^{*}+w^{*}}{2}\right)\right)\right\|\\ &\overset{(e)}{=}\epsilon\left\|\frac{\pi^{*}+w^{*}}{2}-\mathrm{ proj}_{C_{1}\cap C_{2}}\left(\frac{\pi^{*}+p+w^{*}+q}{2}\right)\right\|\\ &\overset{(f)}{=}\frac{\epsilon}{2}\left\|\mathrm{proj}_{C_{1}} \left(\pi^{*}+p\right)+\mathrm{proj}_{C_{1}}\left(w^{*}+q\right)-2\mathrm{proj} _{C_{1}\cap C_{2}}\left(\frac{\pi^{*}+p+w^{*}+q}{2}\right)\right\|\\ &\overset{(g)}{\leq}\frac{M\epsilon}{2}\left\|\pi^{*}-w^{*}\right\|,\end{split}\] (27)

The \((d)\) inequality comes from the Luo-Tseng error bound condition [61] and Prop.3.1 in [29]. The \((e)\) holds based on Eq.26. The \((f)\) equality holds based on the property of the normal cone, which is \(\mathrm{Proj}_{C}(x+z)=x,\forall x\in C,z\in\mathcal{N}_{C}(x)\). The \((g)\) inequality holds by invoking Lemma 3.2 in [29], which tells \(\left\|\mathrm{proj}_{C_{1}}(x)+\mathrm{proj}_{C_{2}}(y)-2\,\mathrm{proj}_{C_{ 1}\cap C_{2}}\left(\frac{x+y}{2}\right)\right\|\leqslant M\left\|\mathrm{ proj}_{C_{1}}(x)-\mathrm{proj}_{C_{2}}(y)\right\|\). Finally, combining (25) and (27), we can obtain the desired result, i.e.,

\[\forall(\pi^{*},\omega^{*})\in\mathcal{X}_{rel},\ \mathrm{dist}(\frac{\pi^{*}+\omega^{*}}{2}, \mathcal{X})\leq\frac{(2\kappa+1)\epsilon C_{f}M}{2\sigma\rho}=\tau/\rho\] (28)

where \(\tau:=\frac{(2\kappa+1)\epsilon C_{f}M}{2\sigma}\) is a constant. 

## Appendix C Complete Mixup Procedure

We provide the complete mixup procedure of our method in Algorithm 3. We conduct augmentation only on the training set, and we mixup graphs from every pair of classes. For a specific mixup of two graphs, we apply our proposed algorithm \(\text{FGWMixup}_{(*)}\) (Alg.1, 2) to obtain the synthetic mixup graph \(\tilde{G}=(\tilde{\bm{\mu}},\tilde{\bm{X}},\tilde{\bm{A}})\). However, the numerical solution does not ensure a discrete adjacency matrix \(\tilde{\bm{A}}\). Thus, we adopt a thresholding strategy to discretize \(\tilde{\bm{A}}\), setting entries below and above the threshold to 0 and 1, respectively. The threshold is linearly searched between the maximum and minimum entries of \(\tilde{\bm{A}}\), aiming to minimize the density difference between the mixup graph and the original graphs. The mixup graphs are added to the training set for augmentation, and the training, validating and testing procedures follow the traditional paradigms.

```
1:Input: Training set \(\mathcal{G}=\{(G_{i},y_{i})\}_{i}\), mixup ratio \(\beta\)
2:Output: Augmented training set \(\tilde{\mathcal{G}}\)
3:\(N=|\mathcal{G}|\), \(N_{y}=|\{y_{i}\}_{i}|\), \(\tilde{\mathcal{G}}=\mathcal{G}\)
4:for\(i\neq j\) in range (\(N_{y}\)) do
5:for\(k\) in range (\(2\beta N/N_{y}(N_{y}-1)\)) do
6: Randomly sample \(G_{i}=(\bm{\mu}_{i},\bm{X}_{i},\bm{A}_{i})\) from class \(y_{i}\) and \(G_{j}=(\bm{\mu}_{j},\bm{X}_{j},\bm{A}_{j})\) from \(y_{j}\)
7: Generate new mixup graph and its label \(\tilde{G},\tilde{y}=\text{FGWMixup}_{(*)}(G_{i},G_{j})\), \(\tilde{G}=(\tilde{\bm{\mu}},\tilde{\bm{X}},\tilde{\bm{A}})\).
8: Search the threshold \(\theta\) to discretize \(\tilde{\bm{A}}\): \(\tilde{\bm{A}}\leftarrow\{1\) if \(a_{ij}\geq\theta\text{ else }0\}_{ij}\), which minimizes the density differences between \(\tilde{\bm{A}}\) and \(\bm{A}_{1}\), \(\bm{A}_{2}\).
9:\(\tilde{\mathcal{G}}\leftarrow\tilde{\mathcal{G}}\cup\{\tilde{G}\}\)
10:endfor
11:endfor
12:return\(\tilde{\mathcal{G}}\) ```

**Algorithm 3** The whole mixup procedure

## Appendix D Experimental Settings

### Dataset Information

PROTEINS, NCI1, NCI109, IMDB-BINARY (IMDB-B) and IMDB-MULTI (IMDB-M) are the five benchmark graph classification datasets used in our experiments. When preprocessing the datasets, we remove all the isolated nodes (i.e., 0-degree nodes) in order to avoid invalid empty message passing of MPNNs. We present the detailed statistics of the preprocessed datasets in Table 6, including the number of graphs, the average/median node numbers, the average/median edge numbers, the dimension of node features, and the number of classes.

### Backbones

We adopt two categories of GNN frameworks as the backbones. The first category is the virtual-node-enhanced Message Passing Neural Networks (MPNNs), including vGCN and vGIN. The second category is the Graph Transformer-based networks, including Graphormer and GraphormerGD. The details of these GNN layers are listed as follows:

* **vGCN**[1]: Graph Convolution Network is developed from the spectral GNNs with 1-order approximation of Chebychev polynomial expansion. The update of features can be regarded as message passing procedure. The graph convolution operator of each layer in the vGCN is

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Datasets & graphs & avg nodes & med nodes & avg edges & med edges & feat dim & classes \\ \hline PROTEINS & 1113 & 39.05 & 26 & 72.82 & 98 & 3 & 2 \\ NCI1 & 4110 & 29.76 & 27 & 32.30 & 58 & 37 & 2 \\ NCI109 & 4127 & 29.57 & 26 & 32.13 & 58 & 38 & 2 \\ IMDB-B & 1000 & 19.77 & 17 & 193.06 & 260 & N/A & 2 \\ IMDB-M & 1500 & 13.00 & 10 & 131.87 & 144 & N/A & 3 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Detailed statistics of experimented datasets.

defined as: \(\mathbf{X}^{(l+1)}=\sigma(\mathbf{\hat{A}}\mathbf{X}^{(l)}\mathbf{W}^{(l)})\), where \(\mathbf{\hat{A}}\) is the renormalized adjacency matrix, and \(\mathbf{W}\) is the learnable parameters. We apply ReLU as the activation function \(\sigma(\cdot)\).
* **vGIN**[2]: Graph Isomorphism Network takes the idea from the Weisfeiler-Lehman kernel, and define the feature update of each layer through explicit message passing procedure. The GIN layer takes the form of: \(x_{v}^{(l+1)}=\mathrm{MLP}^{(l+1)}\left(\left(1+\epsilon^{(l+1)}\right)\cdot x _{v}^{(l)}+\sum_{u\in\mathcal{N}(v)}x_{u}^{(l)}\right)\), where \(\epsilon\) is a learnable scalar. We apply two-layer MLP with ReLU activation in our implementation.
* **Graphormer**[39]: Graphormer adopts the idea of the Transformer [62] architecture, which designs a multi-head global QKV-attention across all graph nodes, and encode graph topologies as positional encodings (spatial encoding in Graphormers). A Graphormer layer conducts feature updating as follows: \[\mathbf{A}^{h}\left(\mathbf{X}^{(l)}\right) =\mathrm{softmax}\left(\mathbf{X}^{(l)}\mathbf{W}_{Q}^{l,h}( \mathbf{X}^{(l)}\mathbf{W}_{K}^{l,h})^{\top}+\phi^{l,h}(\mathbf{D})\right);\] \[\mathbf{\hat{X}}^{(l)} =\mathbf{X}^{(l)}+\sum_{h=1}^{H}\mathbf{A}^{h}\left(\mathbf{X}^ {(l)}\right)\mathbf{X}^{(l)}\mathbf{W}_{V}^{l,h}\mathbf{W}_{O}^{l,h};\] \[\mathbf{X}^{(l+1)} =\mathbf{\hat{X}}^{(l)}+\mathrm{GELU}\left(\mathbf{\hat{X}}^{(l) }\mathbf{W}_{1}^{l}\right)\mathbf{W}_{2}^{l},\] where \(\phi(\cdot)\) is the spatial encoding module implemented as embedding tables, and \(\mathbf{D}\) is the shortest path distance of the graph. \(\mathbf{A}\) is the attention matrix.
* **GraphormerGD**[16]: GraphormerGD is an enhanced version of Graphormer, where the graph topologies are encoded with a _General Distance_ metric defined by the combination of the shortest path distance (SPD) and the resistance distance [63] (RD). It mainly modifies the calculation of the attention matrix \(\mathbf{A}\) in Graphormer: \[\mathbf{A}^{h}\left(\mathbf{X}^{(l)}\right)=\phi_{1}^{l,h}(\mathbf{D}_{sp}, \mathbf{D}_{r})\odot\mathrm{softmax}\left(\mathbf{X}^{(l)}\mathbf{W}_{Q}^{l,h}(\mathbf{X}^{(l)}\mathbf{W}_{K}^{l,h})^{\top}+\phi_{2}^{l,h}(\mathbf{D}_{ sp},\mathbf{D}_{r})\right)\] where \(\phi(\cdot)\) is an MLP taking SPD embedding and RD gaussian kernel embedding as inputs.

For all the backbones, we apply the virtual node READOUT approach (which is the official READOUT function of Graphormers) instead of traditional global pooling. We apply 6 network layers and take 64 as the hidden dimension for all the backbones on all datasets. GraphormerGD has a special model design on PROTEINS dataset, which applies 5 layers and 32 as the hidden dimension due to the GPU memory limitation.

### Experimental Environment

Hardware EnvironmentThe experiments in this work are conducted on two machines: one with 8 Nvidia RTX3090 GPUs and Intel Xeon E5-2680 CPUs, one with 2 Nvidia RTX8000 GPUs and Intel Xeon Gold 6230 CPUs.

Software EnvironmentOur experiments are implemented with Python 3.9, PyTorch 1.11.0, Deep Graph Library (DGL) [44] 1.0.2, and Python Optimal Transport (POT) [45] 0.8.2. The implementation of all the backbone layers are based on their DGL implementations. For the FGW solver, FGWMixup uses the official FGW distance solver implemented in POT, and FGWMixup\({}_{*}\) uses the accelerated FGW algorithm implemented on our own.

### Implementation Details

For our mixup algorithm FGWMixup, we follow [31] and apply the Euclidean distance as the metric measuring the distance of the node features and the graph adjacency matrix as the structural distances between nodes. We sample the mixup weight \(\lambda\) from the distribution \(\mathrm{Beta}(0.2,0.2)\) and the trade-off coefficient of the structure and signal costs \(\alpha\) are tuned in {0.05, 0.5, 0.95}. We set the size of the generated mixup graphs as the weighted average of the source graphs, which is \(\tilde{n}=\lambda n_{1}+(1-\lambda)n_{2}\). The maximum number of iterations of FGWMixupis set to 200 for the outer loop optimizing \(\mathbf{X}\) and \(\mathbf{A}\), and 300 for the inner loop optimizing couplings \(\mathbf{\pi}\). The stopping criteria of our algorithm are the relative update of the optimization objective reaching below a threshold, which is set to 5e-4. For FGWMixup\({}_{*}\), we select the step size of MD \(\gamma\) from {0.1, 1, 10}. The mixup ratio (i.e., the proportion of mixup samples to the original training samples) is set to 0.25.

For the training of GNNs, MPNNs are trained for 400 epochs and Graphormers are trained for 300 epochs, both using AdamW optimizer with a weight decay rate of 5e-4. The batch size of GNNs are chosen from {32, 128}, and the learning rate is chosen from {1e-3, 5e-4, 1e-4}. Dropout is employed with a fixed dropout rate 0.5 to prevent overfitting. All the hyperparameters are fine-tuned by grid search on validation sets. For a fair comparison, we employ the same set of hyperparameter configurations for all data augmentation methods in each backbone architecture. For more details, please check our code published online at https://github.com/ArthurLeoM/FGWMixup.

## Appendix E Further Experimental Analyses

### Qualitative Analyses

We present two mixup examples of FGWMixup in Figure 4 to demonstrate that augmented graphs by FGWMixup can preserve key topologies of the original graphs and have semantically meaningful node features simultaneously. In Example 1, we can observe that the mixup graph adopts an overall trident structure and a substructure (marked green) from \(G_{1}\), and adopts several substructures from \(G_{2}\) (marked red), finally formulating a new graph sample combined properties from both graphs. In Example 2, we can observe that the mixup graph is quite similar to \(G_{2}\), but breaks the connection of two marked (red arrow pointed) edges and formulates two disconnected subgraphs, which is identical to the overall structure of \(G_{1}\). Moreover, in both examples, we can observe that the preserved substructures are not only topologically alike, but also highly consistent in node features. The two examples demonstrate that FGWMixup can both preserve key topologies and generate semantically meaningful node features.

Figure 4: Two examples of FGWMixup. In each example, the subfigures on the left and middle are the original graphs to be mixed up, denoted as \(G_{1}\) and \(G_{2}\), respectively. The subtitle denotes the mixup ratio \(\lambda\). The subfigure on the right is the synthetic mixup graph. The node features are one-hot encoded and distinguished with the feature ID and corresponding color.

### Further Discussions on \(\mathcal{G}\)-Mixup

In this subsection, we conduct further analyses on \(\mathcal{G}\)-Mixup for a better understanding of its differences from our methods.

\(\mathcal{G}\)**-Mixup with GW Graphon Estimator**\(\mathcal{G}\)-Mixup does not originally apply GW metrics to estimate the graphons (as they introduce in Table 1 and 6 in their paper [20]). In our implementation, we select the USVT method as the graphon estimator for \(\mathcal{G}\)-Mixup. Yet it is also practical to apply GW metric to estimate graphons as an ablation study. Here, we also provide the experimental results of \(\mathcal{G}\)-Mixup+GW and \(\mathcal{G}\)-Mixup+GW* (GW with our single-loop solver) on PROTEINS as shown in Table 7.

No matter what metrics are applied for the graphon estimation, we want to emphasize that \(\mathcal{G}\)-Mixup does not model the joint distribution of graph structure and signal spaces, but regards them as two disentangled and independent factors. However, our main contribution and the greatest advantage compared with \(\mathcal{G}\)-Mixup comes from the joint modeling of graph structure and signal spaces. This also explains why FGWMixup can outperform.

Extend \(\mathcal{G}\)-Mixup with FGW-based Attributed Graphon EstimatorIn [64], Xu et al. introduce an attributed graphon estimation algorithm, which makes us naturally think of an extended version of \(\mathcal{G}\)-Mixup. With the attributed graphon, \(\mathcal{G}\)-Mixup can also consider the joint modeling problem. Though it is not the contribution and the core idea from \(\mathcal{G}\)-Mixup to address our proposed joint modeling problem, we are also interested in whether this extension can improve the performance of \(\mathcal{G}\)-Mixup. Hence, we conduct experiments with the extended version of \(\mathcal{G}\)-Mixup using FGW as the attributed graphon estimator (denoted as \(\mathcal{G}\)-Mixup+FGW) on PROTEINS and NCI1 datasets, and the results are shown in Table 8.

We can observe that FGW does not significantly improve the performance of \(\mathcal{G}\)-Mixup and still cannot outperform our methods. The main reason lies in that the node matching problem remains unsolved using the linear interpolation strategy of two graphons introduced in \(\mathcal{G}\)-Mixup. Though the intra-class node matching has been done with FGW graphon estimation, the graphons of different classes are not ensured with an aligned node distribution, which requires the inter-class node matching for an appropriate mixup. This is the core reason for the limited performance of \(\mathcal{G}\)-Mixup series methods.

### Algorithm Efficiency

In this subsection, we provide additional details regarding the runtime efficiency analyses.

Further Comparisons between FGWMixup **and** FGWMixup.Table 9 presents the average time spent on a single iteration of \(\bm{\pi}\) update (i.e, FGW solver convergence) and the average iterations taken for updating \(\bm{X}\) and \(\bm{A}\) until convergence. We can observe that the convergence of a single FGW procedure does not appear to be significantly accelerated. This is because the gradient step

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Backbone & \(\mathcal{G}\)-Mixup & \(\mathcal{G}\)-Mixup+GW & \(\mathcal{G}\)-Mixup+GW* & FGWMixup \\ \hline vGIN & 74.84(2.99) & 74.48(2.54) & 74.03(4.46) & **75.02(3.86)** \\ vGCN & 74.57(2.88) & 74.57(3.18) & 74.84(2.15) & **76.01(3.19)** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Experimental results of different graphon estimators of \(\mathcal{G}\)-Mixup on PROTEINS.

\begin{table}
\begin{tabular}{c c|c c c} \hline \hline Dataset & Backbone & \(\mathcal{G}\)-Mixup & \(\mathcal{G}\)-Mixup+FGW & FGWMixup \\ \hline \multirow{2}{*}{PROTEINS} & vGIN & 74.84(2.99) & 74.66(3.51) & **75.02(3.86)** \\  & vGCN & 74.57(2.88) & 74.30(2.85) & **76.01(3.19)** \\ \hline \multirow{2}{*}{NCI1} & vGIN & 77.79(1.88) & 78.18(1.73) & **78.37(2.40)** \\  & vGCN & 76.42(1.79) & 75.91(1.54) & **78.32(2.65)** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Experimental results of \(\mathcal{G}\)-Mixup extended with FGW attributed graphon estimator on PROTEINS and NCI1.

has been taken twice for an alternating projection process, which incurs double time for the gradient calculation (with a complexity of \(O(n^{3})\), \(n\) is the graph size). In contrast, the strict solver only takes one gradient step for each projection. Therefore, despite a faster convergence rate of the relaxed FGW, the extra gradient step makes the overall convergence time of the relaxed FGW solver similar to that of the strict FGW solver. However, a notable improvement is observed in the convergence rate of the outer loop responsible for updating \(\bm{X}\) and \(\bm{A}\), resulting from the larger step of \(\bm{\pi}\) that the relaxed FGW provides, as shown in Prop.1. Considering the two factors above, the overall mixup time has been efficiently decreased by up to 3.46 \(\times\) with \(\texttt{FGWMixup}_{*}\).

Comparisons between Our Methods and Compared BaselinesWe present the averaged efficiencies of different mixup methods and time spent on training vanilla backbones of each fold on PROTEINS and NCI1 datasets in Table 10. As the efficiency of \(\mathcal{G}\)-Mixup hugely relies on the graphon estimation approach, we also include \(\mathcal{G}\)-Mixup with GW graphon estimators (denoted as G-Mixup+GW) in the table.

We can observe that our methods and \(\mathcal{G}\)-Mixup+GW are slower than the other data augmentation methods. The main reason is that the complexity of calculating (F)GW distances between two graphs is cubic (\(\mathcal{O}(mn^{2}+nm^{2})\))[28], where \(m,n\) are the sizes of two graphs. Moreover, when calculating barycenters, we need an outer loop with \(T\) iterations and \(M\) graphs. In total, the time complexity of mixing up two graphs of size \(n\) is \(\mathcal{O}(MTn^{3})\). \(\texttt{FGWMixup}_{*}\) boosts the efficiency by enhancing the convergence rate and reducing the required iterations \(T\) (see Table 9 for more details), whereas \(\mathcal{G}\)-Mixup+GW will have to go over the whole dataset to calculate graphons, which is much more time-consuming than \(\texttt{FGWMixup}\).

However, sacrificing complexity to pursue higher performance has been the recent trend of technical development, e.g. GPT-4. Moreover, we believe that the current time complexity of \(\texttt{FGWMixup}_{*}\) is still acceptable compared with our performance improvements, as most compared graph augmentation methods cannot effectively enhance the model performance as shown in Table 1.

More importantly, in practice, the main computational bottleneck of (F)GW-based method is the OT network flow CPU solver in the current implementation based on the most widely used POT lib. In other words, GPU-based network flow algorithms have not been applied in current computation frameworks. Moreover, mini-batch parallelization is not yet deployed in POT. However, recent works [65] from NVIDIA have focused on accelerating network flow algorithms on GPU, which may probably be equipped on CUDA and allow a huge acceleration for GW solvers in the near future. Hence, we firmly believe that the fact that our method is not yet optimized in parallel on GPUs is only a temporary problem. Just as some other works (e.g., MLP, LSTM, etc.) that have brought enormous contributions in the past era, they are initially slow when proposed, but have become efficient with fast-following hardware supports.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Dataset & DropEdge & DropNode & \(\mathcal{G}\)-Mixup & ifMixup & \(\mathcal{G}\)-Mixup+GW & \(\texttt{FGWMixup}\) & \(\texttt{FGWMixup}_{*}\) \\ \hline PROTEINS & 0.192 & 0.229 & 6.34 & 2.08 & 2523.78 & 802.24 & 394.57 \\ NCI1 & 0.736 & 0.810 & 10.31 & 5.67 & 9657.48 & 1711.45 & 637.41 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Average mixup efficiency (clock time spent, seconds) of all compared baselines on each fold of PROTEINS and NCI1 datasets.

\begin{table}
\begin{tabular}{c|c c|c c c} \hline \hline Dataset & \begin{tabular}{c} Single FGW Iter. Time (s) \\ \(\texttt{FGWMixup}_{*}\) \\ \end{tabular} & \begin{tabular}{c} Avg. Outer Loop Converge Iter. (\#) \\ \(\texttt{FGWMixup}_{*}\) \\ \end{tabular} & \begin{tabular}{c} \(\texttt{FGWMixup}_{*}\) \\ \end{tabular} & \begin{tabular}{c} \(\texttt{FGWMixup}_{*}\) \\ \end{tabular} & 
\begin{tabular}{c} Speedup \\ \(\texttt{FGWMixup}\) \\ \end{tabular} \\ \hline PROTEINS & **0.0181** & 0.0225 & **80.64** & 153.69 & 1.91\(\times\) \\ NCI1 & 0.0140 & **0.0120** & **54.25** & 170.23 & 3.14\(\times\) \\ NCI109 & 0.0136 & **0.0123** & **53.12** & 169.98 & 3.20\(\times\) \\ IMDB-B & 0.0162 & **0.0119** & **20.03** & 95.48 & 4.77\(\times\) \\ IMDB-M & 0.0081 & **0.0060** & **14.26** & 60.91 & 4.27\(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: More algorithm execution efficiency details of \(\texttt{FGWMixup}\) and \(\texttt{FGWMixup}_{*}\).

### Sensitivity Analysis on Varying Beta Distribution Parameter \(k\)

We empirically follow the setting in [24] to select the beta distribution parameter \(k=0.2\) where the mixup method is first proposed. We also provide the sensitivity analysis of the Beta distribution parameter \(k\) on PROTEINS and NCI1 datasets in Table 11.

From the results, we can find that \(\mathrm{Beta}(0.2,0.2)\) is the overall best-performed setting. There are also a few circumstances where the other settings outperform. In our opinion, different datasets and backbones prefer different optimal settings of \(k\). However, we should choose the one that is overall the best across various settings.

### Experiments on OGB Datasets

We also conduct experiments on several OGB benchmark datasets[46], including ogbg-molhiv and ogbg-molhace. We conduct binary classification (molecular property prediction) on these datasets with AUROC (Area Under Receiver Operating Characteristic) as the reported metric. The split of training, validating, and testing datasets are provided by the OGB benchmark. In spite of the existence of edge features in OGB datasets, in this work, both of our GNN backbones and augmentation methods do not encode or consider the edge features. We select vGCN and vGIN (5 layers and 256 hidden dimensions) as the backbones and train the models five times with different random seeds, and we report the average and standard deviation of AUROCs as the results. The dataset information and the predicting performances are listed in Table 12.

From the table, we can observe evident improvements on both datasets and backbones with our methods. FGWMixup, obtains the best performance on ogbg-molhiv dataset, with 2.69% and 3.46% relative improvements on vGCN and vGIN respectively, and FGWMixup obtains the best on ogbg-molhace dataset with 1.14% and 8.86% relative improvements. Meanwhile, our methods provide a much more stable model performance where we reduce the variance by a significant margin, which demonstrates that our methods can resist the potential noises underlying the data and increase the robustness of GNNs. These additional experimental results further indicate the effectiveness of our methods in improving the performance of GNNs in terms of their generalizability and robustness.

\begin{table}
\begin{tabular}{c|c c c c|c c c} \hline \hline \multirow{2}{*}{Methods} & \multicolumn{4}{c|}{PROTEINS} & \multicolumn{4}{c}{NCI1} \\  & \(k\)=0.2 & \(k\)=0.5 & \(k\)=1.0 & \(k\)=2.5 & \(k\)=0.2 & \(k\)=0.5 & \(k\)=1.0 & \(k\)=2.5 \\ \hline vGIN-FQMMixup & **75.02(3.36)** & **75.02(2.67)** & 79.493(2.93) & 74.30(1.07) & **78.32(2.65)** & 76.37(2.06) & 76.59(2.39) & 77.71(2067) \\ vGCN-FQMMixup & **75.01(3.19)** & 75.47(3.12) & 75.95(2.58) & 74.30(1.42) & **78.37(2.40)** & 78.00(1.40) & 77.98(1.50) & 77.96(1.73) \\ vGIN-FQMMixup & 75.20(3.30) & 73.59(2.38) & 73.86(2.81) & **76.10(2.97)** & 77.27(2.71) & 77.25(2.09) & **77.32(1.78)** & 77.01(2.14) \\ vGCN-FQMMixup, & 75.20(3.03) & 74.29(4.62) & **75.38(3.41)** & 74.21(4.52) & 78.47(1.74) & **78.71(1.49)** & 78.10(1.71) & 77.96(1.02) \\ \hline \hline \end{tabular}
\end{table}
Table 11: Experimental results of different \(k\) on PROTEINS and NCI1 datasets.

\begin{table}
\begin{tabular}{c c c c} \hline \hline \multicolumn{2}{c}{**Stats**} & **ogbg-molhiv** & **ogbg-molbace** \\ \hline Graphs & 41,127 & 1,513 \\ Avg. Nodes & 25.5 & 34.1 \\ Avg. Edges & 27.5 & 36.9 \\ Feature Dim. & 9 & 9 \\ \hline \hline
**Backbones** & **Methods** & **ogbg-molhiv** & **ogbg-molbace** \\ \hline vGCN & vanilla & 73.72(2.45) & 76.62(4.59) \\ DropEdge & 74.07(2.68) & 75.31(4.57) \\ \(\mathcal{G}\)-Mixup & 74.65(1.42) & 69.80(6.41) \\ FGWMixup & 75.24(2.78) & **77.49(2.71)** \\ FGWMixup\({}_{*}\) & **75.70(1.15)** & 76.40(2.45) \\ \hline vGIN & vanilla & 72.61(1.01) & 69.77(1.33) \\ DropEdge & 72.97(1.61) & 72.68(4.92) \\ \(\mathcal{G}\)-Mixup & 74.52(1.58) & 73.74(7.07) \\ FGWMixup & 72.68(1.47) & **75.95(2.42)** \\ FGWMixup\({}_{*}\) & **75.12(1.02)** & 74.02(2.18) \\ \hline \hline \end{tabular}
\end{table}
Table 12: Statistics and experimental results on OGB benchmark datasets. The reported metrics are AUROCs taking the form of avg.(stddev.).