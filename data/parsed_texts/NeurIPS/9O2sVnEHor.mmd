# Weisfeiler and Leman Go Loopy: A New Hierarchy

for Graph Representational Learning

Raffaele Paolino\({}^{*,1,2}\)  **Sohir Maskey\({}^{*,1}\)** Pascal Welke\({}^{3}\)  **Gitta Kutyniok\({}^{1,2,4,5}\)

\({}^{1}\)Department of Mathematics, LMU Munich

\({}^{2}\)Munich Center for Machine Learning (MCML)

\({}^{3}\)Faculty of Computer Science, TU Wien

\({}^{4}\)Institute for Robotics and Mechatronics, DLR-German Aerospace Center

\({}^{5}\)Department of Physics and Technology, University of Tromso

Equal contribution.

Corresponding authors: paolino@math.lmu.de, maskey@math.lmu.de.

###### Abstract

We introduce \(r\)-loopy Weisfeiler-Leman (\(r\)-\(\ell\)WL), a novel hierarchy of graph isomorphism tests and a corresponding GNN framework, \(r\)-\(\ell\)MPNN, that can count cycles up to length \(r{+}2\). Most notably, we show that \(r\)-\(\ell\)WL can count homomorphisms of cactus graphs. This extends 1-WL, which can only count homomorphisms of trees and, in fact, we prove that \(r\)-\(\ell\)WL is incomparable to \(k\)-WL for any fixed \(k\). We empirically validate the expressive and counting power of \(r\)-\(\ell\)MPNN on several synthetic datasets and demonstrate the scalability and strong performance on various real-world datasets, particularly on sparse graphs. Our code is available on GitHub.

## 1 Introduction

Graph Neural Networks (GNNs) (Scarselli et al., 2009; Bronstein et al., 2017) have become a prevalent architecture for processing graph-structured data, contributing significantly to various applied sciences, such as drug discovery (Stokes et al., 2020), recommender systems (Fan et al., 2019), and fake news detection (Monti et al., 2019).

Among various architectures, Message Passing Neural Networks (MPNNs) (Gilmer et al., 2017) are widely used in practice, as they encompass only local computation, leading to fast and scalable models. Despite their success, the representational power of MPNNs is bounded by the Weisfeiler-Leman (WL) test, a classical algorithm for graph isomorphism testing (Xu et al., 2019; Morris et al., 2019). This limitation hinders MPNNs from recognizing basic substructures like cycles (Chen et al., 2020). However, specific substructures can be crucial in many applications. For example, in organic chemistry, the presence of cycles can impact various chemical properties of the underlying molecules (Deshpande et al., 2002; Koyuturk et al., 2004). Therefore, it is crucial to investigate whether GNNs can count certain substructures and to design architectures that surpass the limited power of MPNNs.

Several models have been proposed to surpass the limitations of WL. Many of these models draw inspiration from higher-order WL variants (Morris et al., 2019), enabling them to count a broader range of substructures. For instance, GNNs emulating \(3\)-WL can count cycles up to length \(7\). However, this increased expressivity comes at a high computational cost, as \(3\)-WL does not respect the sparsity of real-world graphs, posing serious scalability issues. Hence, there is a critical need to design expressive GNNs that respect the inherent sparsity of real-world graphs (Morris et al., 2023).

Main Contributions.We introduce a novel class of color refinement algorithms called _\(r\)-loopy Weisfeiler-Leman test (\(r\)-\(\ell\)WL)_ and a corresponding class of GNNs named _\(r\)-loopy Graph Isomorphism Networks (\(r\)-\(\ell\)GIN)_. The key idea is to collect messages not only from neighboring nodes but also from the paths connecting any two distinct neighboring nodes, as illustrated in Figure 1. This approach enhances the resulting GNNs' expressivity beyond 1-WL. In particular, \(r\)-\(\ell\)WL can count cycles up to length \(r+2\), even surpassing the \(k\)-WL hierarchy.

Furthermore, we prove that \(r\)-\(\ell\)WL can homomorphism-count any cactus graph with cycles up to length \(r+2\). Cactus graphs are valuable due to their structural properties and simplicity, making them useful for modeling in areas such as electrical engineering (Nishi et al., 1986) and computational biology (Paten et al., 2011). For instance, aromatic compounds often form cactus graphs, where the molecular core, usually a cycle, is connected to functional groups (e.g., carboxyl groups) that can significantly impact the properties of the molecule. Thus, the ability to homomorphism-count cactus graphs can enhance model performance, and it allows us to compare the expressive power of \(r\)-\(\ell\)WL with other popular GNNs in a quantitative manner (Barcelo et al., 2021; B. Zhang et al., 2024). Specifically, we show that \(r\)-\(\ell\)WL is more expressive than GNNs that include explicit homomorphism counts of cycle graphs, known as \(\mathcal{F}\)-Hom-GNNs (Barcelo et al., 2021). Additionally, \(1\)-\(\ell\)WL can already separate infinitely many graphs that Subgraph \(k\)-GNNs (Frasca et al., 2022; Qian et al., 2022) cannot (see, e.g., Figure 7). The higher expressivity, paired with the local computations, highlights the enhanced potential of \(r\)-\(\ell\)GIN, showing its competitive performance and the efficiency of its forward pass on real-world datasets, see Section 7.

## 2 Related Work

The notion of expressivity in standard neural networks is linked to the ability to approximate any continuous function (Cybenko, 1989; Hornik et al., 1989). In contrast, GNN expressivity is measured by the ability to distinguish non-isomorphic graphs. According to the Stone-Weierstrass theorem, these criteria are equivalent (Chen et al., 2019; Dasoulas et al., 2021): a network that can distinguish all graphs can approximate any continuous function. Therefore, research often focuses on determining which graphs a GNN can distinguish (Morris et al., 2023).

Xu et al. (2019) and Morris et al. (2019) proved that the expressive power of MPNNs is bounded by \(1\)-WL. Subsequent works (Maron et al., 2018; Morris et al., 2019, 2020) introduced higher-order GNNs that have the same expressive power as \(k\)-WL or its local variants (Geerts et al., 2022). Although these networks are universal (Maron et al., 2019; Keriven et al., 2019), their exponential time and space complexity in \(k\) renders them impractical. Abboud et al. (2022) proposed \(k\)-hop GNNs which aggregate information from \(k\)-hop neighbors, thus, enhancing expressivity beyond \(1\)-WL but within \(3\)-WL (Feng et al., 2022). Michel et al. (2023) and Graziani et al. (2024) construct GNNs that process paths emanating from each node to overcome \(1\)-WL. Subgraph GNNs (Bevilacqua et al., 2021; You et al., 2021; Frasca et al., 2022; Huang et al., 2022) surpass \(1\)-WL by decomposing the initial input

Figure 1: Visual depiction of \(r\)-\(\ell\)GIN: During preprocessing, we calculate the path neighborhoods \(\mathcal{N}_{r}(v)\) for each node \(v\) in the graph \(G\). Paths of varying lengths are processed separately using simple GINs, and their embeddings are pooled to obtain the final graph embedding. The forward complexity scales linearly with the sizes of \(\mathcal{N}_{r}(v)\), enabling efficient computation on sparse graphs.

graph into a bag of subgraphs. However, subgraph GNNs are upper-bounded by 3-WL (Frasca et al., 2022). A different line of work leverages positional encoding through unique node identifiers (Vignac et al., 2020), random features (Abboud et al., 2021; Sato et al., 2021) or eigenvectors (Lim et al., 2022; Maskey et al., 2022) to augment the expressive power of MPNNs.

While the predominant approach for gauging the expressive power of GNNs is within the \(k\)-WL hierarchy, such a measure is inherently qualitative, as it cannot shed light on substructures a particular GNN can encode. Lovasz (1967) showed that _homomorphism counts_ is a _complete graph invariant_, meaning two graphs are isomorphic if and only if their homomorphism counts are identical. Building on this result, B. Zhang et al. (2024) advocate for homomorphism-count as a quantitative measure of expressivity, as GNN architectures can homomorphism-count particular families of motifs. Tinhofer (1986, 1991) established that 1-WL is equivalent to counting homomorphisms from graphs with tree-width one, while Dell et al. (2018) proved the equivalence between \(k\)-WL and the ability to count homomorphisms from graphs with tree-width \(k\). Nguyen et al. (2020), Barcelo et al. (2021), Welke et al. (2023), and Jin et al. (2024) used homomorphism counts to develop expressive GNNs.

Manually augmenting node features with homomorphism counts can be disadvantageous as performance depends on the chosen substructures. This can be alleviated by designing domain-agnostic GNNs that can learn structural information suitable for the task at hand. For instance, higher-order GNNs can count a large class of substructures as homomorphisms (B. Zhang et al., 2024), but they suffer from scalability issues. We propose \(r\)-\(\ell\)WL and \(r\)-\(\ell\)GIN, which can count homomorphisms of cactus graphs without adding explicit substructure counts. Our method is scalable to large datasets, particularly when the graphs in these datasets are sparse.

## 3 Preliminaries

Let \(\mathcal{G}\) be the set of all simple and undirected graphs, and let \(G\in\mathcal{G}\). We denote the set of nodes by \(V(G)\) and the set of edges by \(E(G)\). The _direct neighborhood_ of a node \(v\in V(G)\) is defined as \(\mathcal{N}(v)\coloneqq\{u\in V(G)\mid\ \{v,u\}\in E(G)\}\).

**Definition 1**.: _Let \(F,G\in\mathcal{G}\). A homomorphism from \(F\) to \(G\) is a map \(h:V(F)\to V(G)\) such that \(\{u,v\}\in E(F)\) implies \(\{h(u),h(v)\}\in E(G)\). A subgraph isomorphism is an injective homomorphism._

Intuitively, a homomorphism from \(F\) to \(G\) is an edge-preserving map. A subgraph isomorphism ensures that \(F\) actually occurs as a subgraph of \(G\). Consequently, it also maps distinct edges to distinct edges. A visual explanation can be found in Figure 5. We denote by \(\operatorname{Hom}(F,G)\) the set of homomorphisms from \(F\) to \(G\) and by \(\operatorname{hom}(F,G)\) its cardinality. Similarly, we denote by \(\operatorname{Sub}(F,G)\) the set of subgraph isomorphisms from \(F\) to \(G\) and by and \(\operatorname{sub}(F,G)\) its cardinality.

### Graph Invariants

In order to unify different expressivity measures, we recall the definition of graph invariants.

**Definition 2**.: _Let \(P\) be a designated set, referred to as the palette. A graph invariant is a function \(\zeta:\mathcal{G}\to P\) such that \(\zeta(G)=\zeta(H)\) for all isomorphic pairs \(G,H\in\mathcal{G}\). \(\zeta\) is a complete graph invariant if \(\zeta(G)\neq\zeta(F)\) for all non-isomorphic pairs \(G,F\in\mathcal{G}\)._

Complete graph invariants have maximal expressive power. However, no polynomial-time algorithm to compute a complete graph invariant is known. To compare the expressive power of different graph invariants, such as graph colorings and GNN architectures, we introduce the following definition.

**Definition 3**.: _Let \(\gamma,\zeta\) be two graph invariants. We say that \(\gamma\) is more powerful than \(\zeta\) (\(\gamma\sqsubseteq\zeta\)) if for every pair \(G,H\in\mathcal{G}\), \(\gamma(G)=\gamma(H)\) implies \(\zeta(G)=\zeta(H)\). We say that \(\gamma\) is strictly more powerful than \(\zeta\) if \(\gamma\sqsubseteq\zeta\) and there exists a pair \(F,G\in\mathcal{G}\) such that \(\gamma(G)\neq\gamma(H)\) and \(\zeta(G)=\zeta(H)\)._

### Message Passing Neural Networks and Weisfeiler-Leman

Message passing is an iterative algorithm that updates the _colors_ of each node \(v\in V(G)\) as

\[c^{(t+1)}(v)\gets f^{(t+1)}\left(c^{(t)}(v),g^{(t+1)}\left(\left\{\left\{c ^{(t)}(u)\mid u\in\mathcal{N}(v)\right\}\right\}\right)\right). \tag{1}\]The graph output after \(t\) iterations is given by

\[c^{(t)}(G)\coloneqq h\left(\left\{\left\{c^{(t)}(v)\mid v\in V(G)\right\}\right\} \right\}\right).\]

Here, \(g^{(t)},h\) are functions on the domain of multisets and \(f^{(t)}\) is a function on the domain of tuples. For each \(t\), the colorings \(c^{(t)}\) are graph invariants. When the subsets of nodes with the same colors cannot be further split into different color groups, the algorithm terminates; the stable coloring after convergence is denoted by \(c(G)\).

Choosing injective functions for all \(f^{(t)}\) and setting \(g^{(t)}\) and \(h\) as the identity function results in 1-WL (Weisfeiler et al., 1968). If \(f^{(t)},g^{(t)},h\) are chosen as suitable neural networks, one obtains a Message Passing Neural Network (MPNN). Xu et al. (2019) proved that MPNNs are as powerful as \(1\)-WL if the functions \(f^{(t)},g^{(t)}\), and \(h\) are injective on their respective domains. The \(k\)-WL algorithms uplift the expressive power of \(1\)-WL by considering interactions between \(k\)-tuples of nodes. This results in a hierarchy of strictly more powerful graph invariants (see Appendix B.1 for a formal definition).

### Homomorphism and Subgraph Counting Expressivity

A more nuanced graph invariant can be built by considering the occurrences of a motif \(F\).

**Definition 4**.: _Let \(F\in\mathcal{G}\). A graph invariant \(\zeta\) can homomorphism-count\(F\) if for all pairs \(G,H\in\mathcal{G}\)\(\zeta(G)=\zeta(H)\) implies \(\hom(F,G)=\hom(F,H)\). By analogy, \(\zeta\) can subgraph-count\(F\) if for all pairs \(G,H\in\mathcal{G}\), \(\zeta(G)=\zeta(H)\) implies \(\sub(F,G)=\sub(F,H)\)._

If \(\mathcal{F}\) is a family of graphs, we say that \(\zeta\) can homomorphism-count\(\mathcal{F}\) if \(\zeta\) can homomorphism-count every \(F\in\mathcal{F}\); we denote the vector of homomorphism-count by \(\hom(\mathcal{F},G)\coloneqq(\hom(F,G))_{F\in\mathcal{F}}\). Interpreting \(\hom(\mathcal{F},\cdot)\) as a graph invariant, given by \(G\mapsto\hom(\mathcal{F},G)\), another graph invariant \(\zeta\) can homomorphism-count\(\mathcal{F}\) if and only if \(\zeta\sqsubseteq\hom(\mathcal{F},\cdot)\).

The ability of a graph invariant to count homomorphisms is highly relevant because \(\hom(\mathcal{G},\cdot)\) is a complete graph invariant. Conversely, if \(\zeta\) is a complete graph invariant, then \(\zeta\) can homomorphism-count all graphs (Lovasz, 1967). Additionally, homomorphism-counting serves as a quantitative expressivity measure to compare different WL variants and GNNs, such as \(k\)-WL, Subgraph GNNs, and other methods (Lanzinger et al., 2024; Zhang et al., 2024), and allows for relating them to our proposed \(r\)-\(\ell\)WL variant, as detailed in Corollary 2.

## 4 Loopy Weisfeiler-Leman Algorithm

In this section, we introduce a new graph invariant by enhancing the direct neighborhood of nodes with _simple paths_ between neighbors.

**Definition 5**.: _Let \(G\in\mathcal{G}\). A simple path of length \(r\) is a collection \(\mathbf{p}=\{p_{i}\}_{i=1}^{r+1}\) of \(r+1\) nodes such that \(\{p_{i},p_{i+1}\}\in E(G)\) and \(i\neq j\implies p_{i}\neq p_{j}\) for every \(i,j\in\{1,\ldots,r\}\),._

Simple paths are the building blocks of \(r\)-neighborhoods, which in turn are the backbone of our \(r\)-\(\ell\)WL algorithm. The following definition is inspired by (Cantwell et al., 2019; Kirkley et al., 2021).

**Definition 6**.: _Let \(G\in\mathcal{G}\) and \(r\in\mathbb{N}\setminus\{0\}\), we define the \(r\)-neighborhood\(\mathcal{N}_{r}(v)\) of \(v\in V(G)\) as_

\[\mathcal{N}_{r}(v)\coloneqq\{\mathbf{p}\mid\mathbf{p}\text{ simple path of length }r,\,p_{r+1}\in\mathcal{N}(v),v\notin\mathbf{p}\}\,.\]

For consistency, we set \(\mathcal{N}_{0}(v)\coloneqq\mathcal{N}(v)\). An example of the construction of \(r\)-neighborhood is shown in Figure 2, where different \(r\)-neighborhoods of node \(v\) are represented with different colors.

We generalize \(1\)-WL in (1) as follows.

**Definition 7**.: _We define the \(r\)-loop Weisfeiler-Leman (\(r\)-\(\ell\)WL) test by the following color update:_

\[c_{r}^{(t+1)}(v)\leftarrow\mathrm{HASH}_{r}\left(c_{r}^{(t)}(v),\left\{\left\{c _{r}^{(t)}(\mathbf{p})\mid\mathbf{p}\in\mathcal{N}_{0}(v)\right\}\right\} \right\},\ldots,\left\{\left\{c_{r}^{(t)}(\mathbf{p})\mid\mathbf{p}\in \mathcal{N}_{r}(v)\right\}\right\}\right), \tag{2}\]

_where \(c_{r}^{(t)}(\mathbf{p})\coloneqq\left(c_{r}^{(t)}(p_{1}),c_{r}^We denote by \(c_{r}^{(t)}(G)\) the final graph output after \(t\) iterations of \(r\)-\(\ell\)WL, i.e.,

\[c_{r}^{(t)}(G)=\operatorname{HASH}_{r}\left(\left\{\left\{c_{r}^{(t)}(v)\mid v \in V(G)\right\}\right\}\right),\]

and by \(c_{r}(G)\) the stable coloring after convergence. The stable coloring \(c_{r}\) serves as graph invariant and will be referred to as \(r\)-\(\ell\)WL.

## 5 Expressivity of \(r\)-\(\ell\)WL

We analyze the expressivity of \(r\)-\(\ell\)WL in terms of its ability to distinguish non-isomorphic graphs, subgraph-count, and homomorphism-count motifs. The proofs for all statements are in Appendix D.

### Isomorphism Expressivity

It is straightforward to check that \(0\)-\(\ell\)WL corresponds to \(1\)-WL, since \(\mathcal{N}_{0}(v)=\mathcal{N}(v)\) for all nodes \(v\). However, increasing \(r\) leads to a strict increase in expressivity.

**Proposition 1**.: _Let \(0\leq q<r\). Then, \(r\)-\(\ell\)WL is strictly more powerful than \(q\)-\(\ell\)WL. In particular, every \(r\)-\(\ell\)WL is strictly more powerful than \(1\)-WL._

This shows that the number of graphs we can distinguish monotonically increases with \(r\). We empirically verify this fact on several synthetic datasets in Section 7.

### Subgraph Expressivity

Recent studies highlight limitations in the ability of certain graph invariants to subgraph-count cycles. For instance, \(1\)-WL cannot subgraph-count cycles (Chen et al., 2020, Theorem 3.3), while \(3\)-WL can only subgraph-count cycles of length up to \(7\)(Arvind et al., 2020, Theorem 3.5). Similarly, Subgraph GNNs have limited cycle-counting ability (Huang et al., 2022, Proposition 3.1). In contrast, \(r\)-\(\ell\)WL can count cycles of arbitrary length, as shown in the following statement.

**Theorem 1**.: _For any \(r\geq 1\), \(r\)-\(\ell\)WL can subgraph-count all cycles with at most \(r+2\) nodes._

Since \(3\)-WL cannot subgraph-count any cycle with more than \(7\) nodes, Theorem 1 implies that \(6\)-\(\ell\)WL is not less powerful than \(3\)-WL. This observation generalizes to any \(k\)-WL, as shown next.

**Corollary 1**.: _Let \(k\in\mathbb{N}\). There exists \(r\in\mathbb{N}\), such that \(r\)-\(\ell\)WL is not less powerful than \(k\)-WL. Specifically, \(r\in\mathcal{O}(k^{2})\), with \(r\leq\frac{k(k+1)}{2}-2\) for even \(k\) and \(r\leq\frac{(k+1)^{2}}{2}-2\) for odd \(k\)._

The \(r\)-\(\ell\)WL color refinement algorithm surpasses the limits of the \(k\)-WL hierarchy while only using local computation. This is particularly important since already \(3\)-WL is computationally infeasible, whereas our method can scale efficiently to higher orders if the graphs are sparse, which is commonly the case in real-world applications.

### Homomorphism Expressivity

The following section unveils a close connection between the expressivity of \(r\)-\(\ell\)WL and cactus graphs (Harary et al., 1953), a significant class between trees and graphs with tree-width 2.

**Definition 8**.: _A cactus graph is a graph where every edge lies on at most one simple cycle. For \(r\geq 2\), an \(r\)-cactus graph is a cactus where every simple cycle has at most \(r\) vertices. We denote by \(\mathcal{M}\) the set of all cactus graphs, and by \(\mathcal{M}^{r}\) the set of all \(q\)-cactus graphs for \(q\leq r\)._

Figure 6 shows two examples of cactus graphs. From the expressivity perspective, the ability to homomorphism-count cactus graphs establishes a lower bound strictly between the homomorphism-counting capabilities of \(1\)-WL and \(3\)-WL (Neuen, 2024), as cactus graphs are a strict superset of all trees and a strict subset of all graphs of treewidth two. With this in mind, we are now ready to present our significant result on the homomorphism expressivity of our \(r\)-\(\ell\)WL algorithm.

**Theorem 2**.: _Let \(r\geq 0\). Then, \(r\)-\(\ell\)WL can homomorphism-count \(\mathcal{M}^{r+2}\)._We refer to Appendix G for a detailed proof of Theorem 2, which is fairly involved and requires defining canonical tree decompositions of cactus graphs and unfolding trees of \(r\)-\(\ell\)WL. Demonstrating their strong connection, we then follow the approach in (Dell et al., 2018; B. Zhang et al., 2024) to decompose homomorphism counts of cactus graphs. In fact, we prove a more general result, showing that \(r\)-\(\ell\)WL can count all _fan-cactus graphs_, see Appendix G for more details.

The class \(\mathcal{M}^{2}\) contains only forests; hence, Theorem 2 implies the standard results on the ability of \(1\)-WL to count forests. Since forests are the only class of graphs \(1\)-WL can count, Theorem 2 implies that \(r\)-\(\ell\)WL is always strictly more powerful than \(1\)-WL, corroborating the claim in Proposition 1.

The implications of Theorem 2 are profound: it establishes that \(r\)-\(\ell\)WL can homomorphism-count a large class of graphs. Specifically, Theorem 2 provides a quantitative expressivity measure that enables comparison of \(r\)-\(\ell\)WL's expressivity with other WL variants and GNNs. This comparison is achieved by examining the range of graphs that \(r\)-\(\ell\)WL can homomorphism-count against those countable by other models, as detailed in works by Barcelo et al. (2021) and B. Zhang et al. (2024). For instance, B. Zhang et al. (2024) showed that Subgraph GNNs (Bevilacqua et al., 2021; You et al., 2021; Frasca et al., 2022; Huang et al., 2022) are limited to homomorphism-count graphs with end-point shared NED. Hence, Subgraph GNNs can not homomorphism-count \(F=\left\{\begin{array}{c}\includegraphics[width=14.226378pt]{images/cWL1. eps}\end{array}\right\}\), while \(1\)-\(\ell\)WL can. Based on this, we can identify pairs of graphs that \(1\)-\(\ell\)WL can distinguish but Subgraph GNNs cannot. We summarize these and other implications of Theorem 2 in the following corollary.

**Corollary 2**.: _Let \(r\in\mathbb{N}\setminus\{0\}\). Then,_

1. \(r\)_-_\(\ell\)WL is more powerful than_ \(\mathcal{F}\)_-Hom-GNNs, where_ \(\mathcal{F}=\{C_{3},\ldots,C_{r+2}\}\)_._
2. \(1\)_-_\(\ell\)WL is not less powerful than Subgraph GNNs. In particular, any_ \(r\)_-_\(\ell\)WL can separate infinitely many graphs that Subgraph GNNs fail to distinguish._
3. _For any_ \(k>0\)_,_ \(1\)_-_\(\ell\)WL is not less powerful than Subgraph_ \(k\)_-GNNs. In particular, any_ \(r\)_-_\(\ell\)WL can separate infinitely many graphs that Subgraph_ \(k\)_-GNNs fail to distinguish._
4. \(r\)_-_\(\ell\)WL can subgraph-count all graphs_ \(F\) _such that_ \(\operatorname{\mathrm{spasm}}(F)\subset\mathcal{M}^{r+2}\)_, where_ \(\operatorname{\mathrm{spasm}}(F)\coloneqq\left\{H\in\mathcal{G}\mid\exists \text{ surjective }h\in\operatorname{Hom}(F,H)\right\}\)_. In particular, if_ \(1\leq r\leq 4\)_, then_ \(r\)_-_\(\ell\)WL can subgraph-count all paths up to length_ \(r+3\)_._

A detailed explanation of Subgraph (\(k\)-)GNNs, \(\mathcal{F}\)-Hom-GNNs, along with the proofs of Corollary 2, can be found in Appendix H. Finally, we note that Theorem 2 states a loose lower bound on the homomorphism expressivity of \(r\)-\(\ell\)WL. This observation opens the avenue for future research to explore tight lower bounds, or upper bounds, on the homomorphism expressivity of \(r\)-\(\ell\)WL.

## 6 Loopy Message Passing

In this section, we build a GNN emulating \(r\)-\(\ell\)WL.

**Definition 9**.: _For \(t\in\{0,\ldots,T-1\}\) and \(k\in\{0,\ldots,r\}\), \(r\)-\(\ell\)MPNN applies the following message, update and readout functions:_

\[\begin{split} m_{k}^{(t+1)}(v)&=f_{k}^{(t+1)}\left( \left\{\left\{c_{k}^{(t)}(\mathbf{p})\mid\mathbf{p}\in\mathcal{N}_{k}(v) \right\}\right\}\right),\\ c_{r}^{(t+1)}(v)&=g^{(t+1)}\left(c_{r}^{(t)}(v),\, m_{0}^{(t+1)}(v),\ldots,m_{r}^{(t+1)}(v)\right),\end{split} \tag{3}\]

_and final readout layer \(c_{r}^{(T)}(G)=h\left(\left\{\left\{c_{r}^{(T)}(v)\mid v\in V(G)\right\} \right\}\right)\)._

In the following statement, we link the expressive power of \(r\)-\(\ell\)MPNN and \(r\)-\(\ell\)WL.

**Theorem 3**.: _For fixed \(t,r\geq 0\), \(t\) iterations of \(r\)-\(\ell\)WL are more powerful than \(r\)-\(\ell\)MPNN with \(t\) layers. Conversely, \(r\)-\(\ell\)MPNN is more powerful than \(r\)-\(\ell\)WL if the functions \(f^{(t)},g^{(t)}\) in (3) are injective._

The previous result derives conditions under which \(r\)-\(\ell\)MPNN is as expressive as \(r\)-\(\ell\)WL. To implement \(r\)-\(\ell\)MPNN in practice, we choose suitable neural layers for \(f_{k}^{(t)},g^{(t)}\), and \(h\) in Definition 9. As a consequence of (Xu et al., 2019, Lemma 5), the aggregation function in (3) can be written as

\[f_{k}^{(t+1)}\left(\left\{\left\{\left\{c_{k}^{(t)}(\mathbf{p})\mid\mathbf{p} \in\mathcal{N}_{k}(v)\right\}\right\}\right\}\right):=f\left(\sum_{\mathbf{p} \in\mathcal{N}_{k}(v)}g(\mathbf{p})\right),\]for suitable functions \(f,g\). Since 1-WL is injective on forests (Arvind et al., 2015), hence on paths, and since GIN can approximate 1-WL (Xu et al., 2019), we choose \(f=\mathrm{MLP}\) and \(g=\mathrm{GIN}\). Hence, \(r\)-\(\ell\)GIN is defined as an \(r\)-\(\ell\)MPNN that updates node features via

\[x_{r}^{(t+1)}(v):=\mathrm{MLP}\left(x_{r}^{(t)}(v)+(1+\varepsilon_{0})\sum_{u \in\mathcal{N}_{0}(v)}x_{r}^{(t)}(u)+\sum_{k=1}^{r}(1+\varepsilon_{k})\sum_{ \mathbf{p}\in\mathcal{N}_{k}(v)}\mathrm{GIN}_{k}(\mathbf{p})\right). \tag{4}\]

To reduce the number of learnable parameters in (4), the \(\mathrm{GIN}_{k}\) can be shared among all \(k\). Nothing prevents from choosing a different path-processing layer; we opted for GIN because it is simple yet maximally expressive on paths. We refer to Figure 1 for a visual depiction of \(r\)-\(\ell\)GIN.

Computational ComplexityThe complexity of \(r\)-\(\ell\)GIN is \(\mathcal{O}(|E|+\sum_{v\in V(G)}\sum_{k=1}^{r}2k|\mathcal{N}_{k}(v)|)\). The former addend is the standard message complexity, while the latter arises from applying GIN to paths of length \(k\leq r\). This implies that our model's complexity scales linearly with the number of edges, and with the number of paths within \(\mathcal{N}_{k}(v)\). The number of such paths is typically less than the number of edges. For example, ZINC12K has overall \(598\)K edges while only containing 374K paths in \(\mathcal{N}_{r}(v)\) for \(1\leq r\leq 5\). Hence, the runtime overhead is small in practice. Compared to 3-WLGNN (Dwivedi et al., 2022), which has the same cycle-counting expressivity, our model requires ca. 10 seconds/epoch while 3-WLGNN takes ca. \(329.49\) seconds/epoch on ZINC12K. Our runtime is comparable to that of GAT, MoNet, or GatedGCN (see Table 10 for a thorough comparison).

Comparison with (Michel et al., 2023)PathNN updates node features by computing all possible paths starting from each node. In contrast, our approach selects paths between distinct neighbors, potentially resulting in fewer paths. For instance, a tree's \(r\)-neighborhoods (\(r\geq 1\)) are empty, while counts of paths between nodes are quadratic. Notably, Michel et al. (2023) do not explore the impact of increasing the path length on architecture expressiveness, a consideration we address (see, e.g., Proposition 1 and Corollary 1). Another significant contribution of our work, which we assert does not hold (at least not trivially) for PathNN, is the provable ability to subgraph-count cycle graphs (see, e.g., Theorem 1) and homomorphism-count cactus graphs (see, e.g., Theorem 2).

## 7 Experiments

All instructions to reproduce the experiments are available on GitHub (MIT license). Additional information on the training and test details can be found in Appendix C.

Expressive Power.We showcase the expressive power of \(r\)-\(\ell\)GIN on synthetic datasets:

* _GRAPH8C_(Balcilar et al., 2021) comprises \(11\)\(117\) connected non-isomorphic simple graphs on \(8\) nodes; \(312\) pairs are \(1\)-WL equivalent but none is \(3\)-WL equivalent.
* _EXP_ISO_(Abboud et al., 2022) comprises \(600\) pairs of \(1\)-WL equivalent graphs.

Figure 3: Indistinguishable pairs at initialization, symlog scale. For GRAPH8C and EXP_ISO, we report the proportion of indistinguishable pairs: \(2\) graphs are deemed indistinguishable if the L\({}^{1}\) distance of their embeddings is less than \(10^{-3}\). For COSPECTRAL10 and SR16622, we report the L\({}^{1}\) distance between graph embeddings. We report the mean and standard deviation over \(100\) seeds.

* _COSPECTRAL10_(van Dam et al., 2003): the dataset comprises two cospectral \(4\)-regular non-isomorphic graphs on \(10\) nodes which are \(1\)-WL equivalent (see, e.g., Figure (a)a).
* _SR16622_(Michel et al., 2023) comprises two strongly regular graphs on \(16\) nodes, namely the Shrikhande and the \(4{\times}4\) rook graph, which are \(3\)-WL equivalent (see, e.g., Figure (b)b).

The goal is to check whether the model can distinguish non-isomorphic pairs at initialization. The results are shown in Figure 3.

Additionally, Table 1 shows the performance on _BREC_(Wang et al., 2024), which includes \(400\) pairs of non-isomorphic graphs ranging from \(1\)-WL to \(4\)-WL equivalent. The baselines include PPGN, which is 3-WL equivalent and can count up to 7-cycles and homomorphism-count all graphs of tree-width 2; NestedGNN which is between 1-WL and 3-WL; GSN which is more powerful than 1-WL but whose expressive power depends on the chosen pattern.

Finally, Figure 4 reports the performance on synthetic classification tasks:

* _EXP, CEXP_(Abboud et al., 2021) require expressive power beyond 1-WL.
* _CSL_(Murphy et al., 2019) comprises \(150\) cycle graphs with skip links (see, e.g., Figure (c)c). The task is to predict the length of the skip link.

Counting Power.Following (B. Zhang et al., 2024), we use the SUBGARPHCOUNT dataset (Chen et al., 2020) to test the ability to homomorphism- and subgraphs-count exemplary motifs.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{\(\hom(F,G)\)} & \multicolumn{3}{c}{\(\operatorname{sub}(F,G)\)} \\ \cline{2-9} Model & & & & & & & \\ \cline{2-9} MPNN & 0.300 & 0.233 & 0.254 & 0.358 & 0.208 & 0.188 & 0.146 & 0.261 & 0.205 \\ Subgraph GNN & 0.011 & 0.015 & 0.012 & 0.010 & 0.020 & 0.024 & 0.046 & 0.007 & 0.027 \\ Local 2-GNN & 0.008 & 0.008 & 0.010 & 0.008 & 0.011 & 0.017 & 0.034 & 0.007 & 0.016 \\ Local 2-FGNN & 0.003 & 0.005 & 0.004 & 0.003 & 0.004 & 0.010 & 0.020 & 0.003 & 0.010 \\ \cline{2-9} \(r\)-\(\ell\)GIN & 0.001 & 0.006 & 0.009 & 0.0005 & 0.0005 & 0.0003 & 0.0003 & 0.001 & 0.0004 \\  & (r=2) & (r=3) & (r=3) & (r=1) & (r=2) & (r=3) & (r=4) & (r=2) & (r=3) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Test MAE for homomorphism- and subgraph-counts. Results from (B. Zhang et al., 2024).

Figure 4: Test accuracy on synthetic classification task: (left) shared and (right) non-shared weights.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Model & Basic (60) & Regular (140) & Extension (100) & CFI (100) \\ \hline
3-WL & 60 & 50 & 100 & 60 \\ PPGN & 60 & 50 & 100 & 23 \\ NestedGNN & 59 & 48 & 59 & 0 \\ GSN & 60 & 99 & 95 & 0 \\ OSAN & 52 & 41 & 82 & 2 \\ \hline
4-\(\ell\)GIN & 60 & 100 & 95 & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Num. of distinguished pairs (\(\uparrow\)). Results from (Wang et al., 2024).

[MISSING_PAGE_FAIL:9]

On molecular datasets, we observe that \(r\)-\(\ell\)GIN, although designed for subgraph-counting cycles and homomorphism-counting cactus graphs, is highly competitive. Notably, we outperform the baseline \(0\)-\(\ell\)GIN by 226% on ZINC12K and 400% on ZINC250K and surpass domain-agnostic methods such as CIN or GSN. We conjecture that this is attributed to straightforward optimization, driven by the simplicity of the architecture (see, e.g., Figure 1) and its inductive bias towards counting cycles.

LimitationsPath calculations can become infeasible for dense graphs due to \(\mathcal{O}(N\,d^{r})\) complexity, where \(N\) is the number of nodes and \(d\) is the average degree. However, for sparse graphs, the runtime remains reasonably low. For instance, preprocessing ZINC12K for \(r=5\) takes just over a minute.

## 8 Conclusion

In this paper,we introduce a novel hierarchy of color refinement algorithms, denoted as \(r\)-\(\ell\)WL, which incorporates an augmented neighborhood mechanism accounting for nearby paths. We establish connections between \(r\)-\(\ell\)WL and the classical \(k\)-WL. We construct a GNN (\(r\)-\(\ell\)MPNN) designed to emulate and match the expressive poweror \(r\)-\(\ell\)WL. Theoretical and empirical evidence support the claim that \(r\)-\(\ell\)MPNN can effectively subgraph-count cycles and homomorphism-count cactus graphs.

Future research could focus on precisely characterizing the expressivity of \(r\)-\(\ell\)WL tests by identifying the maximal class of graphs that \(r\)-\(\ell\)WL can homomorphism-count. This would facilitate comparisons by constructing pairs of graphs that \(r\)-\(\ell\)WL cannot separate, but other WL variants can. Another promising direction involves exploring the generalization capabilities of GNNs with provable homomorphism-counting properties. The ability to homomorphism-count certain motifs could provide a mathematical framework to support the intuitive notion that the capacity to count relevant features may improve generalization. We observed this improved generalization experimentally in our ablation study on ZINC12K (see, e.g., Table 8).

## Acknowledgements

R.P. is funded by the Munich Center for Machine Learning (MCML).

S.M. is funded by the NSF-Simons Research Collaboration on the Mathematical and Scientific Foundations of Deep Learning (MoDL) (NSF DMS 2031985) and DFG SPP 1798, KU 1446/27-2.

P.W. is funded by the Vienna Science and Technology Fund (WWTF) project StruDL (ICT22-059).

G.K. acknowledges partial support by the Konrad Zuse School of Excellence in Reliable AI (DAAD), the Munich Center for Machine Learning (BMBF) as well as the German Research Foundation under Grants DFG-SPP-2298, KU 1446/31-1 and KU 1446/32-1. Furthermore, G.K. acknowledges support from the Bavarian State Ministry for Science and the Arts as well as by the Hightech Agenda Bavaria.

## References

* Abboud et al. (2021) Abboud, R., Ceylan, I. I., Grohe, M., and Lukasiewicz, T. (2021). "The Surprising Power of Graph Neural Networks with Random Node Initialization". In: _International Joint Conference on Artificial Intelligence (IJCAI)_, pp. 2112-2118.
* Abboud et al. (2022) Abboud, R., Dimitrov, R., and Ceylan, I. I. (2022). "Shortest Path Networks for Graph Property Prediction". In: _Learning on Graphs Conference (LoG)_, 5:1-5:25.
* Arvind et al. (2020) Arvind, V., Fuhlbruck, F., Kobler, J., and Verbitsky, O. (2020). "On Weisfeiler-Leman Invariance: Subgraph Counts and Related Graph Properties". In: _Journal of Computer and System Sciences_ 113, pp. 42-59.
* Arvind et al. (2015) Arvind, V., Kobler, J., Rattan, G., and Verbitsky, O. (2015). "On the Power of Color Refinement". In: _International Symposium Fundamentals of Computation Theory (FCT)_. Vol. 9210. Lecture Notes in Computer Science, pp. 339-350.
* Balcilar et al. (2021) Balcilar, M., Heroux, P., Gauzere, B., Vasseur, P., Adam, S., and Honeine, P. (2021). "Breaking the Limits of Message Passing Graph Neural Networks". In: _International Conference on Machine Learning (ICML)_, pp. 599-608.
* Balcilar et al. (2020)Barcelo, P., Geerts, F., Reutter, J., and Ryschkov, M. (2021). "Graph neural networks with local graph parameters". In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 25280-25293.
* Bevilacqua et al. (2021) Bevilacqua, B., Frasca, F., Lim, D., Srinivasan, B., Cai, C., Balamurugan, G., Bronstein, M. M., and Maron, H. (2021). "Equivariant subgraph Aggregation Networks". In: _International Conference on Learning Representations (ICLR)_.
* Bodnar et al. (2021) Bodnar, C., Frasca, F., Otter, N., Wang, Y., Lio, P., Montufar, G. F., and Bronstein, M. (2021). "Weisfeiler and Lehman Go Cellular: CW Networks". In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 2625-2640.
* Bouritsas et al. (2023) Bouritsas, G., Frasca, F., Zafeiriou, S., and Bronstein, M. M. (2023). "Improving Graph Neural Network Expressivity via Subgraph Isomorphism Counting". In: _IEEE Transactions on Pattern Analysis and Machine Intelligence_ 45.1, pp. 657-668.
* Bresson and Laurent (2017) Bresson, X. and Laurent, T. (2017). "Residual gated graph convnets". In: _arXiv preprint arXiv:1711.07553_.
* Bronstein et al. (2017) Bronstein, M. M., Bruna, J., LeCun, Y., Szlam, A., and Vandergheynst, P. (2017). "Geometric Deep Learning: Going beyond Euclidean Data". In: _IEEE Signal Processing Magazine_ 34.4, pp. 18-42.
* Cantwell and Newman (2019) Cantwell, G. T. and Newman, M. E. J. (2019). "Message Passing on Networks with Loops". In: _Proceedings of the National Academy of Sciences_ 116.47, pp. 23398-23403.
* Chen et al. (2020) Chen, Z., Chen, L., Villar, S., and Bruna, J. (2020). "Can Graph Neural Networks Count Substructures?" In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 10383-10395.
* Chen et al. (2019) Chen, Z., Villar, S., Chen, L., and Bruna, J. (2019). "On the equivalence between graph isomorphism testing and function approximation with gnns". In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 15868-15876.
* Colbourn and Booth (1981) Colbourn, C. J. and Booth, K. S. (1981). "Linear Time Automorphism Algorithms for Trees, Interval Graphs, and Planar Graphs". In: _SIAM Journal on Computing_ 10.1, pp. 203-225.
* Curtticepean et al. (2017) Curtticepean, R., Dell, H., and Marx, D. (2017). "Homomorphisms are a good basis for counting small subgraphs". In: _ACM SIGACT Symposium on Theory of Computing (STOC)_, pp. 210-223.
* Cybenko (1989) Cybenko, G. (1989). "Approximation by superpositions of a sigmoidal function". In: _Mathematics of control, signals and systems_ 2.4, pp. 303-314.
* Dasoulas et al. (2021) Dasoulas, G., Santos, L. D., Scaman, K., and Virmaux, A. (Jan. 7, 2021). "Coloring Graph Neural Networks for Node Disambiguation". In: _International Joint Conference on Artificial Intelligence (IJCAI)_, pp. 2126-2132.
* Dell et al. (2018) Dell, H., Grohe, M., and Rattan, G. (2018). "Lovasz Meets Weisfeiler and Leman". In: _International Colloquium on Automata, Languages, and Programming (ICALP)_. Vol. 107. LIPIcs, 40:1-40:14.
* Deshpande et al. (2002) Deshpande, M., Kuramochi, M., and Karypis, G. (2002). "Automated Approaches for Classifying Structures". In: _ACM SIGKDD Workshop on Data Mining in Bioinformatics (BIOKD)_, pp. 11-18.
* Dwivedi et al. (2022a) Dwivedi, V. P., Joshi, C. K., Luu, A. T., Laurent, T., Bengio, Y., and Bresson, X. (2022a). "Benchmarking Graph Neural Networks". In: _Journal of Machine Learning Research_ 24.43, pp. 1-48.
* Dwivedi et al. (2022b) Dwivedi, V. P., Rampaek, L., Galkin, M., Parviz, A., Wolf, G., Luu, A. T., and Beaini, D. (2022b). "Long Range Graph Benchmark". In: _Advances in Neural Information Processing Systems_. Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., pp. 22326-22340.
* Fan et al. (2019) Fan, W., Ma, Y., Li, Q., He, Y., Zhao, Y. E., Tang, J., and Yin, D. (2019). "Graph Neural Networks for Social Recommendation". In: _The World Wide Web Conference (WWW)_, pp. 417-426.
* Feng et al. (2022) Feng, J., Chen, Y., Li, F., Sarkar, A., and Zhang, M. (2022). "How Powerful are K-hop Message Passing Graph Neural Networks". In: _Advances in Neural Information Processing Systems (NeurIPS)_.
* Fey et al. (2020) Fey, M., Yuen, J. G., and Weichert, F. (2020). "Hierarchical Inter-Message Passing for Learning on Molecular Graphs". In: _ICML Graph Representation Learning and Beyond (GRL+) Workshop_.
* Fey and Lenssen (2019) Fey, M. and Lenssen, J. E. (2019). "Fast Graph Representation Learning with PyTorch Geometric". In: _ICLR Workshop on Representation Learning on Graphs and Manifolds_.
* Frasca et al. (2022) Frasca, F., Bevilacqua, B., Bronstein, M., and Maron, H. (2022). "Understanding and Extending Subgraph GNNs by Rethinking Their Symmetries". In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 31376-31390.
* Furer (2001) Furer, M. (2001). "Weisfeiler-Lehman Refinement Requires at Least a Linear Number of Iterations". In: _Automata, Languages and Programming_. Ed. by F. Orejas, P. G. Spirakis, and J. van Leeuwen, pp. 322-333.
* Geerts and Reutter (2022) Geerts, F. and Reutter, J. L. (2022). "Expressiveness and Approximation Properties of Graph Neural Networks". In: _International Conference on Learning Representations (ICLR)_.
* Geerts et al. (2021)Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., and Dahl, G. E. (2017). "Neural Message Passing for Quantum Chemistry". In: _International Conference on Machine Learning (ICML)_, pp. 1263-1272.
* Graziani et al. (2024) Graziani, C., Drucks, T., Jogl, F., Bianchini, M., scarselli, franco, and Gartner, T. (2024). "The Expressive Power of Path-Based Graph Neural Networks". In: _Forty-first International Conference on Machine Learning_.
* Hagberg et al. (2008) Hagberg, A. A., Schult, D. A., Swart, P., and Hagberg, J. (2008). "Exploring Network Structure, Dynamics, and Function using NetworkX". In: _Python in Science Conference (SciPy)_,
* Harary and Uhlenbeck (1953) Harary, F. and Uhlenbeck, G. E. (1953). "On the Number of Husimi Trees, I". In: _Proceedings of the National Academy of Sciences of the United States of America_ 39.4, pp. 315-322.
* Hornik et al. (1989) Hornik, K., Stinchcombe, M., and White, H. (1989). "Multilayer feedforward networks are universal approximators". In: _Neural networks_ 2.5, pp. 359-366.
* Hu* et al. (2020a) Hu*, W., Liu*, B., Gomes, J., Zitnik, M., Liang, P., Pande, V., and Leskovec, J. (2020a). "Strategies for Pre-training Graph Neural Networks". In: _International Conference on Learning Representations (ICLR)_.
* Huan et al. (2020b) -- (2020b). "Strategies for Pre-training Graph Neural Networks". In: _International Conference on Learning Representations_.
* Huang et al. (2022) Huang, Y., Peng, X., Ma, J., and Zhang, M. (2022). "Boosting the Cycle Counting Power of Graph Neural Networks with I\({}^{2}\)-GNNs". In: _International Conference on Learning Representations (ICLR)_.
* Ioffe and Szegedy (2015) Ioffe, S. and Szegedy, C. (2015). "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift". In: _International Conference on Machine Learning, (ICML)_. Vol. 37, pp. 448-456.
* Irwin et al. (2012) Irwin, J. J., Sterling, T., Mysinger, M. M., Bolstad, E. S., and Coleman, R. G. (2012). "ZINC: A Free Tool to Discover Chemistry for Biology". In: _Journal of Chemical Information and Modeling_ 52, pp. 1757-1768.
* Jin et al. (2024) Jin, E., Bronstein, M., Ceylan, I. I., and Lanzinger, M. (2024). "Homomorphism Counts for Graph Neural Networks: All About That Basis". In: _International Conference on Machine Learning (ICML)_.
* Keriven and Peyre (2019) Keriven, N. and Peyre, G. (2019). "Universal invariant and equivariant graph neural networks". In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 7090-7099.
* Kingma and Ba (2015) Kingma, D. P. and Ba, J. (2015). "Adam: A Method for Stochastic Optimization". In: _International Conference on Learning Representations (ICLR)_.
* Kipf and Welling (2017) Kipf, T. N. and Welling, M. (2017). "Semi-Supervised Classification with Graph Convolutional Networks". In: _International Conference on Learning Representations (ICLR)_.
* Kirkley et al. (2021) Kirkley, A., Cantwell, G. T., and Newman, M. E. J. (2021). "Belief Propagation for Networks with Loops". In: _Science Advances 7.17, eabf1211.
* Korte and Vygen (2018) Korte, B. and Vygen, J. (2018). _Combinatorial Optimization_. Springer.
* Koyuturk et al. (2004) Koyuturk, M., Grama, A. Y., and Szpankowski, W. (2004). "An efficient algorithm for detecting frequent subgraphs in biological networks". In: _Bioinformatics_ 20 Suppl 1, pp. i200-7.
* Lanzinger and Barcelo (2024) Lanzinger, M. and Barcelo, P. (2024). "On the Power of the Weisfeiler-Leman Test for Graph Motif Parameters". In: _International Conference on Learning Representations (ICLR)_.
* Lim et al. (2022) Lim, D., Robinson, J. D., Zhao, L., Smidt, T., Sra, S., Maron, H., and Jegelka, S. (2022). "Sign and Basis Invariant Networks for Spectral Graph Representation Learning". In: _International Conference on Learning Representations (ICLR)_.
* Lovasz (1967) Lovasz, L. M. (1967). "Operations with structures". In: _Acta Mathematica Academiae Scientiarum Hungarica_ 18, pp. 321-328.
* Maron et al. (2019a) Maron, H., Ben-Hamu, H., Serviansky, H., and Lipman, Y. (2019a). "Provably Powerful Graph Networks". In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 2153-2164.
* Maron et al. (2018) Maron, H., Ben-Hamu, H., Shamir, N., and Lipman, Y. (2018). "Invariant and Equivariant Graph Networks". In: _International Conference on Learning Representations (ICLR)_.
* Maron et al. (2019b) Maron, H., Fetaya, E., Segol, N., and Lipman, Y. (2019b). "On the Universality of Invariant Networks". In: _International Conference on Machine Learning (ICML)_, pp. 4363-4371.
* Maskey et al. (2022) Maskey, S., Parviz, A., Thiessen, M., Stark, H., Sadikaj, Y., and Maron, H. (2022). "Generalized Laplacian Positional Encoding for Graph Representation Learning". In: _NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations_.
* Michel et al. (2023) Michel, G., Nikolentzos, G., Lutzeyer, J. F., and Vazirgiannis, M. (2023). "Path Neural Networks: Expressive and Accurate Graph Neural Networks". In: _International Conference on Machine Learning (ICML)_, pp. 24737-24755.
* Michel et al. (2020)Monti, F., Boscaini, D., Masci, J., Rodola, E., Svoboda, J., and Bronstein, M. M. (2017). "Geometric deep learning on graphs and manifolds using mixture model cnns". In: _IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 5115-5124.
* Monti et al. (2019) Monti, F., Frasca, F., Eynard, D., Mannion, D., and Bronstein, M. M. (2019). _Fake News Detection on Social Media using Geometric Deep Learning_. arXiv: 1902.06673.
* Morris et al. (2023) Morris, C., Lipman, Y., Maron, H., Rieck, B., Kriege, N. M., Grohe, M., Fey, M., and Borgwardt, K. (2023). "Weisfeiler and Leman Go Machine Learning: The Story so Far". In: _Journal of Machine Learning Research_ 24, 333:1-333:59.
* Morris et al. (2020) Morris, C., Rattan, G., and Mutzel, P. (2020). "Weisfeiler and Leman go sparse: Towards scalable higher-order graph embeddings". In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 21824-21840.
* Morris et al. (2019) Morris, C., Ritzert, M., Fey, M., Hamilton, W. L., Lenssen, J. E., Rattan, G., and Grohe, M. (2019). "Weisfeiler and Leman Go Neural: Higher-Order Graph Neural Networks". In: _AAAI Conference on Artificial Intelligence (AAAI)_, pp. 4602-4609.
* Murphy et al. (2019) Murphy, R., Srinivasan, B., Rao, V., and Ribeiro, B. (2019). "Relational Pooling for Graph Representations". In: _International Conference on Machine Learning (ICML)_, pp. 4663-4673.
* Neuen (2024) Neuen, D. (2024). "Homomorphism-Distinguishing Closedness for Graphs of Bounded Tree-Width". In: _International Symposium on Theoretical Aspects of Computer Science (STACS)_, 53:1-53:12.
* Nguyen and Maehara (2020) Nguyen, H. and Maehara, T. (2020). "Graph Homomorphism Convolution". In: _International Conference on Machine Learning (ICML)_, pp. 7306-7316.
* Nishi and Chua (1986) Nishi, T. and Chua, L. (1986). "Uniqueness of solution for nonlinear resistive circuits containing CCCS's or VCVS's whose controlling coefficients are finite". In: _IEEE Transactions on Circuits and Systems_ 33.4, pp. 381-397.
* Paszke et al. (2019) Paszke, A. et al. (2019). "PyTorch: An Imperative Style, High-Performance Deep Learning Library". In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 8024-8035.
* Paten et al. (2011) Paten, B., Diekhans, M., Earl, D., John, J. S., Ma, J., Suh, B., and Haussler, D. (2011). "Cactus graphs for genome comparisons". In: _Journal of Computational Biology_ 18.3, pp. 469-481.
* Qian et al. (2022) Qian, C., Rattan, G., Geerts, F., Niepert, M., and Morris, C. (2022). "Ordered subgraph aggregation networks". In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 21030-21045.
* Sato et al. (2021) Sato, R., Yamada, M., and Kashima, H. (2021). "Random Features Strengthen Graph Neural Networks". In: _SIAM International Conference on Data Mining (SDM)_, pp. 333-341.
* Scarselli et al. (2009) Scarselli, F., Gori, M., Tsoi, A. C., Hagenbuchner, M., and Monfardini, G. (2009). "The Graph Neural Network Model". In: _IEEE Transactions on Neural Networks_ 20.1, pp. 61-80.
* Stokes et al. (2020) Stokes, J. M. et al. (2020). "A deep learning approach to antibiotic discovery". In: _Cell_ 180.4, 688-702.e13.
* Tinhofer (1986) Tinhofer, G. (1986). "Graph isomorphism and theorems of Birkhoff type". In: _Computing_ 36, pp. 285-300.
* (1991). "A note on compact graphs". In: _Discrete Applied Mathematics_ 30, pp. 253-264.
* van Dam and Haemers (2003) van Dam, E. R. and Haemers, W. H. (2003). "Which Graphs Are Determined by Their Spectrum?" In: _Linear Algebra and its Applications_. Vol. 373. Combinatorial Matrix Theory Conference, pp. 241-272.
* Velickovic et al. (2018) Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and Bengio, Y. (2018). "Graph Attention Networks". In: _International Conference on Learning Representations (ICLR)_.
* Vignac et al. (2020) Vignac, C., Loukas, A., and Frossard, P. (2020). "Building powerful and equivariant graph neural networks with structural message-passing". In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 14143-14155.
* Wang and Zhang (2024) Wang, Y. and Zhang, M. (2024). "An Empirical Study of Realized GNN Expressiveness". In: _International Conference on Machine Learning (ICML)_.
* Weisfeiler and Lehman (1968) Weisfeiler, B. and Lehman., A. (1968). "The reduction of a graph to canonical form and the algebra which appears therein". In: _Nauchno-Technicheskaya Informatsia_ 9.
* Welke et al. (2023) Welke, P., Thiessen, M., Jogl, F., and Gartner, T. (2023). "Expectation-Complete Graph Representations with Homomorphisms". In: _International Conference on Machine Learning (ICML)_, pp. 36910-36925.
* Wu et al. (2018) Wu, Z., Ramsundar, B., Feinberg, E. N., Gomes, J., Geniesse, C., Pappu, A. S., Leswing, K., and Pande, V. (2018). _MoleculeNet: A Benchmark for Molecular Machine Learning_. arXiv: 1703.00564.
* Xu et al. (2019) Xu, K., Hu, W., Leskovec, J., and Jegelka, S. (2019). "How Powerful Are Graph Neural Networks?" In: _International Conference on Learning Representations (ICLR)_.
* You et al. (2021) You, J., Gomes-Selman, J. M., Ying, R., and Leskovec, J. (2021). "Identity-Aware Graph Neural Networks". In: _AAAI Conference on Artificial Intelligence (AAAI)_, pp. 10737-10745.

Zhang, B., Gai, J., Du, Y., Ye, Q., He, D., and Wang, L. (2024). "Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness". In: _International Conference on Learning Representations (ICLR)_.
* Zhang and Li (2021) Zhang, M. and Li, P. (2021). "Nested Graph Neural Networks". In: _Advances in Neural Information Processing Systems (NeurIPS)_, pp. 15734-15747.
* Zhao et al. (2022) Zhao, L., Jin, W., Akoglu, L., and Shah, N. (2022). "From Stars to Subgraphs: Uplifting Any GNN with Local Structure Awareness". In: _International Conference on Learning Representations (ICLR)_.
* Zhou et al. (2023) Zhou, J., Feng, J., Wang, X., and Zhang, M. (2023). "Distance-Restricted Folklore Weisfeiler-Leman GNNs with Provable Cycle Counting Power". In: _Advances in Neural Information Processing Systems (NeurIPS)_.

## Appendix

### Table of Contents

* A Additional Figures
* B Additional Notions
* B.1 Higher-Order Weisfeiler-Leman Tests
* C Experimental Details
* C.1 Synthetic Datasets
* C.2 Real-World Datasets
* D Preparation for Proofs
* E Appendix for Section 5.1
* F Appendix for Section 5.2
* G Appendix on Homomorphism Counting and Section 5.3
* G.1 Tree Decomposition Preliminaries
* G.2 Cactus Graphs and their Canonical Tree Decomposition
* G.3 Alternative \(r\)-\(\ell\)WL
* G.4 The Unfolding Tree of \(r\)-\(\ell\)WL
* H Implications of Theorem 2
* H.1 Appendix on \(\mathcal{F}\)-Hom-GNNs and Proof of Corollary 2 i)
* H.2 Appendix on Subgraph GNNs and Proof of Corollary 2 ii)
* H.3 Appendix on Subgraph \(k\)-GNNs and Proof of Corollary 2 iii)
* H.4 Proof of Corollary 2 iv)
* I Appendix for Section 6

[MISSING_PAGE_EMPTY:16]

Figure 7: Example of graphs that Subgraph GNNs cannot separate but \(1\)-\(\ell\)WL can: Subgraph GNNs cannot separate \(G(F)\) and \(H(F)\). However, since \(\hom(F,G(F))\neq\hom(F,H(F))\) and \(F\) is a cactus graph, \(1\)-\(\ell\)WL can separate \(G(F)\) and \(H(F)\) by Theorem 2.

## Appendix B Additional Notions

### Higher-Order Weisfeiler-Leman Tests

It is possible to uplift the expressive power of WL by considering higher-order interactions. The simplest higher-order variant of WL is the \(k\)-dimensional Weisfeiler-Leman test, denoted by \(k\)-WL. Given a graph \(G\) with nodes \(V(G)\) and edges \(E(G)\), the algorithm generates a new graph \(H\) where each node is a \(k\)-tuple of elements of \(V(G)\)

\[V(H)=\Big{\{}\mathbf{v}=\left\{v_{i}\right\}_{i=1}^{k}\ |\ v_{i}\in V(G)\Big{\}} =V(G)^{k},\]

and edges \(E(H)\) are built among those \(k\)-tuples that differ in one entry only

\[E(H)=\left\{\left\{\mathbf{v},\mathbf{u}\right\}\ |\ d_{H}(\mathbf{v},\mathbf{u })=1\,,\ \mathbf{u},\mathbf{v}\in V(H)\right\}\]

where \(d_{H}\) is the Hamming distance. The algorithm assigns to each node \(\mathbf{v}\in V(H)\) an initial color depending on the isomorphic type of the induced subgraph \(G[\mathbf{v}]\). The color refinements scheme is

Figure 8: Some synthetic datasets. The dotted lines are the common edges. The orange edges identifies \(\mathcal{N}_{1}(v)\).

exactly (1) applied to \(H\). While \(H\) can be generated by a simple algorithm, the approach quickly becomes impractical as the number of nodes and edges grows exponentially in \(k\).

## Appendix C Experimental Details

Our model is implemented in PyTorch (BSD-3 license) (Paszke et al., 2019), using PyTorch Geometric (MIT license) (Fey et al., 2019). The \(r\)-neighborhoods are computed with NetworkX (Creative Commons Zero v1.0 Universal) (Hagberg et al., 2008) as preprocessing. Hyperparameters

Figure 9: The input graphs cannot be distinguished by \(1\)-WL, since the color distribution after convergence of the algorithm is equal. \(3\)-WL can distinguish them at the cost of creating new dense graphs. Our proposed \(1\)-\(\ell\)WL can distinguish the two graphs heeding the original graph sparsity.

on real-world datasets were tuned using grid search; for synthetic experiments, we fixed one configuration of hyperparameters. All experiments were run on an internal cluster with Intel Xeon CPUs (28 cores, 192GB RAM) and GeForce RTX 3090 Ti GPUs (4 units, 24GB memory each), as well as Intel Xeon CPUs (32 cores, 192GB RAM) and NVIDIA RTX A6000 GPUs (3 units, 48GB memory each). All models are trained with Adam optimizer (Kingma et al., 2015).

### Synthetic Datasets

The SR16622 dataset is retrieved from the official PATHNN repository (MIT license) (Michel et al., 2023). The GRAPH8C dataset is downloaded from Australian National University webpage (Creative Commons Attribution 4.0 International (CC BY 4.0) license). The EXP, EXP_ISO, and CEXP datasets are downloaded from GNN-RNI official repository (GPL-3.0 license) (Abboud et al., 2021), while the corresponding splits are generated via Stratified 5-fold cross-validation. The CSL dataset is provided by torch_geometric, while the corresponding splits are taken from PathNN official repository. The SUGRAPHCOUNT dataset is taken from the official repository (MIT license) of (Zhao et al., 2022). The BREC dataset is downloaded from its official repository (MIT license) (Wang et al., 2024). The configuration of hyperparameters can be found in Table 5. For the synthetic datasets, we fixed one configuration and studied the effect of increasing \(r\) on the expressive and counting power of the architecture.

For the SR16622, GRAPH8C, EXP_ISO, and COSPECTRAL10 datasets, we report the mean and standard deviation over 100 random seeds. For the EXP, CEXP, and CSL datasets, we report the mean and standard deviation of 5-fold cross-validation. For BREC, we follow the original setup and perform an \(\alpha\)-level Hotellings T-square test; see (Wang et al., 2024) for more details. For the SUBGARPHCOUNT dataset, we report the mean and standard deviation over 4 random seeds, using the original splits from (Zhao et al., 2022).

### Real-World Datasets

All real-world datasets are provided by torch_geometric. The splits for both ZINC datasets are also provided by torch_geometric. For QM9, we follow the set-up of (Zhou et al., 2023) and use random \(80/10/10\) splits. Details for the datasets are provided in Table 6.

Hyperparameters were tuned using grid search. For ZINC12K, the grid was defined by Hidden Size \(\in\{64,128\}\) and Num. Layers \(\in\{3,4,5\}\). For ZINC250K, the grid was defined by Hidden Size \(\in\{128,256\}\) and Num. Layers \(\in\{4\}\). For the QM9 tasks, the grid was defined by Hidden Size \(\in\{64,128\}\) and Num. Layers \(\in\{3,4,5\}\). For the QM9 tasks, we followed the training set-up of (Zhou et al., 2023), training for \(400\) epochs with a ReduceLROnPlateau scheduler, reducing the learning rate by a factor of \(0.9\) if the validation metric did not decrease for 10 epochs. The exact hyperparameters are given in Table 7.

\begin{table}
\begin{tabular}{l c c c c c c c c c}  & & & & & & & & & & \\ Epochs & - & - & - & - & \(10^{3}\) & \(10^{3}\) & \(10^{3}\) & \(1.2\,10^{3}\) & 40 \\ Learning Rate & - & - & - & - & \(10^{-3}\) & \(10^{-3}\) & \(10^{-3}\) & \(10^{-3}\) & \(10^{-4}\) \\ Early Stopping & - & - & - & - & \(\text{lr}<10^{-5}\) & lr \(<10^{-5}\) & lr \(<10^{-5}\) & - & lr \(<10^{-5}\) \\ Scheduler & - & - & - & - & \(\{50,0.5\}\) & \(\{50,0.5\}\) & \(\{50,0.5\}\) & \(\{10,0.9\}\) & \(\{50,0.5\}\) \\ Hidden Size & 64 & 64 & 64 & 64 & 64 & 64 & 64 & 64 & 32 \\ Num. Layers & 3 & 3 & 3 & 3 & 3 & 3 & 3 & 5 & 5 \\ Num. Encoder Layers & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 \\ Num. Decoder Layers & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 & 2 \\ Batch Size & 64 & 64 & 64 & 64 & 64 & 64 & 64 & 128 & 64 \\ Dropout & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ Readout & sum & sum & sum & sum & sum & sum & sum & sum & sum \\ \end{tabular}
\end{table}
Table 5: Hyperparameter configuration for synthetic experiments.

All real-world datasets come with edge features. We use an encoder layer, followed by a linear layer to encode node, edge features, and atomic types before passing them to the \(r\)-\(\ell\)GIN. Within the \(r\)-\(\ell\)GIN layers, we process the edge features via a \(2\)-layered learnable MLP, and replace the GIN in (4) by GINE layers (Hu* et al., 2020a). After \(t\) rounds of \(r\)-\(\ell\)GIN layer, we apply a two-layered MLP as decoder layer. In all experiments, BatchNorm1D (Ioffe et al., 2015) is used in the MLP layers. We refer to Figure 1 for a depiction of the architecture.

\begin{table}
\begin{tabular}{l c c c} \hline \hline
**Dataset** & **Number of graphs** & **Average number of nodes** & **Average number of edges** \\ \hline QM9 & \(130\,831\) & 18.0 & 18.7 \\ ZINC12K & \(12\,000\) & 23.2 & 24.9 \\ ZINC250K & \(249\,456\) & 23.2 & 24.9 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Statistics of real-world datasets.

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline  & **ZINC12K** & **ZINC250K** & **QM9** (\(\mu\)) & **QM9** (\(\alpha\)) & **QM9** (\(\varepsilon_{\mathrm{homo}}\)) \\ \hline Epochs & \(1000\) & \(2000\) & 400 & 400 & 400 \\ Learning Rate & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 \\ Early Stopping & \(\mathrm{lr}<10^{-5}\) & \(\mathrm{lr}<10^{-6}\) & \(\mathrm{lr}<10^{-5}\) & \(\mathrm{lr}<10^{-5}\) & \(\mathrm{lr}<10^{-5}\) \\ Scheduler & \(\{50,0.5\}\) & \(\{50,0.5\}\) & \(\{10,0.9\}\) & \(\{10,0.9\}\) & \(\{10,0.9\}\) \\ r & 5 & 5 & 5 & 5 & 5 \\ Hidden Size & 64 & 256 & 64 & 64 & 64 \\ Depth & 3 & 4 & 4 & 5 & 5 \\ Batch Size & 64 & 128 & 64 & 64 & 64 \\ Dropout & 0 & 0 & 0 & 0 & 0 \\ Readout & sum & sum & sum & sum & sum \\ \# Parameters & \(452\,633\) & \(2\,379\,041\) & \(418\,481\) & \(519\,677\) & \(519\,677\) \\ Preprocessing Time [sec] & 77.4 & 1278.5 & 427.5 & 425.9 & 517.4 \\ Run Time per Seed [h] & 2.8 & 45.6 & 13.6 & 15.9 & 21.1 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Hyperparameters configuration for real-world experiments.

\begin{table}
\begin{tabular}{l c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{2}{c}{MAE (\(\downarrow\))} \\ \cline{2-3}  & Train & Test \\ \hline \(0\)-\(\ell\)GIN & \(0.060\pm 0.009\) & \(0.209\pm 0.007\) \\ \(1\)-\(\ell\)GIN & \(0.060\pm 0.012\) & \(0.201\pm 0.004\) \\ \(2\)-\(\ell\)GIN & \(0.068\pm 0.011\) & \(0.198\pm 0.008\) \\ \(3\)-\(\ell\)GIN & \(0.056\pm 0.013\) & \(0.184\pm 0.007\) \\ \(4\)-\(\ell\)GIN & \(0.0203\pm 0.0002\) & \(0.077\pm 0.001\) \\ \(5\)-\(\ell\)GIN & \(0.022\pm 0.004\) & \(0.072\pm 0.002\) \\ \(6\)-\(\ell\)GIN & \(0.028\pm 0.000\) & \(0.077\pm 0.000\) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Ablation study on the effect of \(r\) in \(r\)-\(\ell\)GIN, ZINC12K.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Model & Memory usage [GB] & Preprocessing [sec] & Training [sec/epoch] \\ \hline MPNN & \(2.28\) & \(64\) & \(45.3\) \\ NestedGNN & \(13.72\) & \(2\,354\) & \(107.8\) \\ I2-GNN & \(19.69\) & \(5\,287\) & \(209.9\) \\
2-DRFWL & \(2.31\) & \(430\) & \(141.9\) \\ \hline
0-\(\ell\)GIN & \(0.02\) (\(0.39\)) & \(191\) & \(44.7\) \\
1-\(\ell\)GIN & \(0.03\) (\(0.48\)) & \(370\) & \(66.4\) \\
2-\(\ell\)GIN & \(0.05\) (\(0.57\)) & \(388\) & \(84.0\) \\
3-\(\ell\)GIN & \(0.07\) (\(0.66\)) & \(408\) & \(85.2\) \\
4-\(\ell\)GIN & \(0.10\) (\(0.82\)) & \(427\) & \(96.9\) \\
5-\(\ell\)GIN & \(0.12\) (\(0.91\)) & \(444\) & \(130.6\) \\ \hline \hline \end{tabular}
\end{table}
Table 10: Empirical time complexity for QM9 dataset; results from (Zhou et al., 2023). In parenthesis the size of the dataset after the computation of \(r\)-neighborhoods.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & \multicolumn{2}{c}{PEPTIDES} \\ \cline{2-3} Model & STRUCTURE (MAE \(\downarrow\)) & FUNC (AP \(\uparrow\)) \\ \hline GCN & \(0.3496\pm 0.0013\) & \(59.30\pm 0.23\) \\ GINE & \(0.3496\pm 0.0013\) & \(59.30\pm 0.23\) \\ GatedGCN & \(0.3420\pm 0.0013\) & \(58.64\pm 0.77\) \\ \hline
7-\(\ell\)GIN & \(0.2513\pm 0.0021\) & \(65.70\pm 0.60\) \\ \hline \hline \end{tabular}
\end{table}
Table 9: Test metrics on long-range graph benchmark datasets (Dwivedi et al., 2022b). The baseline results are obtained from (Dwivedi et al., 2022b, Table 4). Our method is able to enhance performance over standard baselines.

Preparation for Proofs

We begin by recalling some core concepts that are relevant for Section 5 and the proofs therein.

**Definition 10**.: _A node invariant \(\zeta_{(\cdot)}\) is a mapping that assigns to each graph \(G\in\mathcal{G}\) a function \(\zeta_{G}:V(G)\to P\), which satisfies_

\[\forall v\in V(G),\zeta_{G}(v)=\zeta_{H}(h(v)),\]

_where \(H\) is any graph isomorphic to \(G\) and \(h\) is the corresponding isomorphism from \(H\) to \(G\)._

The following definition enables us to compare the expressive power of different node invariants.

**Definition 11** (Node Invariant Refinement).: _Given two node invariants \(\gamma\) and \(\zeta\). We say that \(\zeta\) refines \(\gamma\) if for every fixed graph \(G\) and nodes \(u,v\in V(G)\), it holds \(\zeta_{G}(u)=\zeta_{G}(v)\Rightarrow\gamma_{G}(u)=\gamma_{G}(v)\). We write \(\zeta\sqsubseteq\gamma\)._

We emphasize that every node invariant \(\zeta\) induces a graph invariant \(\mathcal{A}[\gamma]\) by collecting the multiset, i.e., \(G\mapsto\left\{\{\zeta_{G}(v)\}\right\}_{v\in V(G)}\). We denote the induced graph invariant of a node invariant \(\gamma\) as \(\mathcal{A}[\gamma]\).

The following lemma establishes a connection between the expressive power of two node invariants (see Definition 11) and that of their induced graph invariants (see Definition 3).

**Lemma 1**.: _Let \(\zeta,\gamma\) be node invariant. If \(\zeta\sqsubseteq\gamma\), then \(\mathcal{A}[\zeta]\) is more powerful than \(\mathcal{A}[\gamma]\)._

Proof.: Let \(G,H\) be two graphs, and let \(P\) be the underlying palette of \(\zeta,\gamma\). Consider the function

\[\phi:P\longrightarrow P,\;\zeta(u)\mapsto\gamma(u)\;\forall u\in V(G)\cup V(H).\]

As a consequence of \(\zeta\sqsubseteq\gamma\), \(\phi\) is well-defined, since

\[\zeta(u)=\zeta(v)\implies(\phi\circ\zeta)\left(u\right)=\gamma(u)=\gamma(v)=( \phi\circ\zeta)\left(v\right).\]

Assume that \(\mathcal{A}[\zeta](G)=\mathcal{A}[\zeta](H)\), i.e,

\[\left\{\{\zeta(u)\mid u\in V(G)\}\right\}=\left\{\{\zeta(v)\mid v\in V(H)\} \right\}.\]

As \(\phi\) is well-defined, we have

\[\left\{\{\phi\circ\zeta(u)\mid u\in V(G)\}\right\}=\left\{\{\phi\circ\zeta(x) \mid v\in V(H)\}\right\},\]

which leads to \(\mathcal{A}[\gamma](G)=\mathcal{A}[\gamma](H)\). 

## Appendix E Appendix for Section 5.1

In this section, we provide the proof of Proposition 1 from the main paper.

**Proposition 1**.: _Let \(0\leq q<r\). Then, \(r\)-\(\ell\)WL is strictly more powerful than \(q\)-\(\ell\)WL. In particular, every \(r\)-\(\ell\)WL is strictly more powerful than \(1\)-WL._

Proof of Proposition 1.: Let \(r\geq 0\). We aim to prove that \((r+1)\)-\(\ell\)WL is strictly more powerful than \(r\)-\(\ell\)WL. We begin by demonstrating that \((r+1)\)-\(\ell\)WL is more powerful than \(r\)-\(\ell\)WL.

To establish this, we rely on Lemma 1. Specifically, we demonstrate that the underlying \((r+1)\)-\(\ell\)WL node invariant \(c_{r+1}\) refines \(c_{r}\). Moreover, we go beyond and show that the node invariant \(c_{r+1}^{(t)}\) refines \(c_{r}^{(t)}\) at every iteration \(t\geq 0\), which shows that \(t\) iterations of \((r+1)\)-\(\ell\)WL are more powerful than \(t\) iterations of \(r\)-\(\ell\)WL.

For this purpose, let \(G\) be a graph with node set \(V(G)\). For \(t=0\), \(c_{r+1}^{(0)}\sqsubseteq c_{r}^{(0)}\) since both algorithms start with the same labels. By induction, we assume that

\[c_{r+1}^{(t)}(u)=c_{r+1}^{(t)}(v)\implies c_{r}^{(t)}(u)=c_{r}^{(t)}(v) \tag{5}\]

holds; we need to prove that (5) implies

\[c_{r+1}^{(t+1)}(u)=c_{r+1}^{(t+1)}(v)\implies c_{r}^{(t+1)}(u)=c_{r}^{(t+1)}(v). \tag{6}\]Since \(\operatorname{HASH}\) in Definition 7 is injective, \(c^{(t)}_{r+1}(u)=c^{(t)}_{r+1}(v)\) in (5) leads to

\[\left\{\left\{c^{(t)}_{r+1}(\mathbf{p})\mid\mathbf{p}\in\mathcal{N}_{q}(u)\right\} \right\}=\left\{\left\{c^{(t)}_{r+1}(\mathbf{p})\mid\mathbf{p}\in\mathcal{N}_{q }(v)\right\}\right\}\]

for all \(q\in\{0,\ldots,r\}\). The assumption \(c^{(t)}_{r}(u^{q}_{l,k})=c^{(t)}_{r}(v^{q}_{l,k})\) in (5) is satisfied for every path \(\mathbf{u}^{q}_{l}=\left\{u^{q}_{l,k}\right\}\in\mathcal{N}_{q}(u)\) and \(\mathbf{v}^{q}_{l}=\left\{v^{q}_{l,k}\right\}\in\mathcal{N}_{q}(v)\) for \(q=0,\ldots,r\), \(l=1,\ldots,|\mathcal{N}_{v}|\) and \(k=1,\ldots,q+1\). Hence,

\[\left\{\left\{c^{(t)}_{r}(\mathbf{p})\mid\mathbf{p}\in\mathcal{N}_{k}(u)\right\} \right\}=\left\{\left\{c^{(t)}_{r}(\mathbf{p})\mid\mathbf{p}\in\mathcal{N}_{k} (v)\right\}\right\}\]

Inputting this into Definition 7, we get (6), i.e, \(c^{(t+1)}_{r+1}\sqsubseteq c^{(t+1)}_{r}\).

The "strictly" can be deduced as follows. The cycle graph on \((2r+6)\) nodes equipped with a chord between nodes \(1\) and \(r+4\) is \(r\)-\(\ell\)WL equivalent to the graph consisting of two \((r+3)\)-cycles connected by one edge; however, they are not \((r+1)\)-\(\ell\)WL equivalent (see, e.g., Figure 6). 

## Appendix F Appendix for Section 5.2

The goal of this subsection is to provide a proof for Theorem 1 and Corollary 1. In fact, we present and prove a more general statement. Specifically, for a graph \(G\) and \(v\in V(G)\), we introduce the node invariant \(\operatorname{sub}(F^{x},G^{v})\), defined as the count of subgraph isomorphisms from \(F\) to \(G\) that are rooted, meaning that \(x\) is mapped to \(v\). Let us denote this node invariant as \(\operatorname{sub}(F^{x},\cdot)\). Our result establishes that \(c^{(t)}_{r}(\cdot)\) refines \(\operatorname{sub}(C^{x},\cdot)\) for every cycle graph \(C\) with at most \(r\) nodes. In simpler terms, \(c^{(1)}_{r}\) can determine how often node \(v\) appears in a cycle \(C\).

**Lemma 2**.: _Let \(r\geq 1\). For every cycle graph \(C\) with at most \(r+2\) nodes and \(x\in V(C)\), it holds \(c^{(1)}_{r}(\cdot)\sqsubseteq\operatorname{sub}(C^{x},\cdot)\)._

Proof of Lemma 2.: Let \(G\) be any graph, \(u,v\in V(G)\), and \(q=1,\ldots,r+2\). Let \(C\) be a cycle graph with \(q\) nodes. It is important to note that for every \(x_{1},x_{2}\in C\), we have \(\operatorname{sub}(C^{x_{1}},G^{v})=\operatorname{sub}(C^{x_{2}},G^{v})\) since every node in \(C\) is automorphic to each other. Therefore, we can arbitrarily choose any \(x\in V(C)\).

We show that

\[\operatorname{sub}\left(C^{x},G^{u}\right)\neq\operatorname{sub}\left(C^{x},G^ {v}\right)\implies c^{(1)}_{r}(u)\neq c^{(1)}_{r}(v)\]

The number of injective homomorphisms from the \(q\)-long cycles \(C^{x}\) to \(G^{v}\), i.e., \(\operatorname{sub}(C^{a}_{q},G^{v})\), is equal to the number of paths of length \((q-2)\) between distinct neighbors of \(v\).

The neighborhood \(\mathcal{N}_{(q-2)}(v)\) comprises exactly all paths of length \((q-2)\) between any two distinct neighbors of \(v\). Therefore,

\[\operatorname{sub}\left(C^{x},G^{v}\right)=\left|\mathcal{N}_{(q-2)}(v)\right|.\]

Thus

\[\operatorname{sub}\left(C^{x},G^{u}\right)\neq\operatorname{sub}\left(C^{x},G ^{v}\right)\implies\left|\mathcal{N}_{(q-2)}(u)\right|\neq\left|\mathcal{N}_ {(q-2)}(v)\right|,\]

which implies

\[\left\{\left\{c^{(0)}_{q-2}(\mathbf{p}):\mathbf{p}\in\mathcal{N}_{(q-2)}(u) \right\}\right\}\neq\left\{\left\{c^{(0)}_{q-2}(\mathbf{p}):\mathbf{p}\in \mathcal{N}_{(q-2)}(v)\right\}\right\}.\]

Finally, as \(\operatorname{HASH}\) in Definition 7 is injective, we get the thesis \(c^{(1)}_{q-2}(u)\neq c^{(1)}_{q-2}(v)\). 

Now, Theorem 1 from the main paper is a simple corollary of Lemma 2.

**Theorem 1**.: _For any \(r\geq 1\), \(r\)-\(\ell\)WL can subgraph-count all cycles with at most \(r+2\) nodes._

Proof of Theorem 1.: Combining Lemma 2 and Lemma 1, we get that \(c^{(1)}_{r}\) (as a graph invariant) is stronger than the induced graph invariant \(\mathcal{A}\left[\operatorname{sub}(C^{x},\cdot)\right]\). Now, consider graphs \(G,H\), and assume without loss of generality that \(|V(G)|=n=|V(H)|\).

If \(c_{r}^{(1)}(G)=c_{r}^{(1)}(H)\), we have \(\mathcal{A}\left[\operatorname{sub}(C^{x},\cdot)\right](G)=\mathcal{A}\left[ \operatorname{sub}(C^{x},\cdot)\right](H)\). Hence, by definition of induced graph invariants,

\[\left\{\left\{\operatorname{sub}(C^{x},G^{v})\,|\,v\in V(G)\right\}\right\}= \left\{\left\{\operatorname{sub}(C^{x},H^{w})\,|\,w\in V(H)\right\}\right\}.\]

Hence,

\[\frac{1}{n}\sum_{v\in V(G)}\operatorname{sub}(C^{x},G^{v})=\frac{1}{n}\sum_{w \in V(H)}\operatorname{sub}(C^{x},H^{w}),\]

which is equivalent to \(\operatorname{sub}(C,G)=\operatorname{sub}(C,H)\). 

We proceed to restate Corollary 1 and provide its proof.

**Corollary 1**.: _Let \(k\in\mathbb{N}\). There exists \(r\in\mathbb{N}\), such that \(r\)-\(\ell\)WL is not less powerful than \(k\)-WL. Specifically, \(r\in\mathcal{O}(k^{2})\), with \(r\leq\frac{k(k+1)}{2}-2\) for even \(k\) and \(r\leq\frac{(k+1)^{2}}{2}-2\) for odd \(k\)._

Proof of Corollary 1.: Let \(k>0\). We need to show that there exist \(r_{k}\in\mathbb{N}\) and a pair of graphs \(G,H\), such that \(k-\operatorname{WL}(G)=k-\operatorname{WL}(H)\) and \(r_{k}-\ell\text{WL}(G)\neq r_{k}-\ell\text{WL}(H)\).

The _hereditary treewidth_\(\operatorname{hdtw}(F)\) of a graph \(F\) is the maximum treewidth of \(\varphi(F)\) where \(\varphi\) is an edge surjective homomorphism. Neuen (2024) has shown that \(k\)-WL can subgraph-count a graph \(F\) if and only if \(\operatorname{hdtw}(F)\leq k\). This directly implies that for \(F\) with hereditary treewidth larger than \(k\), there exist graphs \(G_{F},H_{F}\) with \(k-\operatorname{WL}(G_{F})=k-\operatorname{WL}(H_{F})\) and \(\operatorname{sub}(F,G)\neq\operatorname{sub}(F,H)\).

Since the hereditary tree-width of cycle graphs is not uniformly bounded (Arvind et al., 2020), for every \(k>0\) there exists a cycle \(C_{c_{k}}\) of length \(c_{k}\in\mathbb{N}\) with hereditary treewidth larger than \(k\). Setting \(F=C_{c_{k}}\) concludes the existence proof with \(r_{k}=c_{k}-2\).

To see that \(r_{k}\in O(k^{2})\), note that the complete graph \(K_{n}\) on \(n\) vertices has treewidth \(n-1\) and exactly \(\binom{n}{2}\) edges. For odd \(n\), \(K_{n}\) is Eulerian, i.e., there exists an edge surjective homomorphism from a cycle to \(K_{n}\) which uses each edge exactly once, i.e., from \(C_{\binom{n}{2}}\). If \(n\) is odd, the minimum \(T\)-join which makes \(K_{n}\) Eulerian contains exactly \(\frac{n}{2}\) edges (see, e.g., Korte et al., 2018). As a result, there exists an edge surjective homomorphism from \(C_{\binom{n}{2}}\) to \(K_{n}\) if \(n\) is odd, and an edge surjective homomorphism from \(C_{\binom{n}{2}+\frac{n}{2}}\) to \(K_{n}\) if \(n\) is even. This implies that \(\operatorname{hdtw}(C_{c_{k}})>k\) for \(c_{k}=\binom{k+1}{2}+\lceil\frac{k+1}{2}\rceil\). Hence, \(r_{k}:=c_{k}-2\in O(k^{2})\). 

## Appendix G Appendix on Homomorphism Counting and Section 5.3

In this section, we provide background information and all proofs related to homomorphism counts. We begin by introducing additional definitions and notation.

**Definition 12** (Induced Subgraph).: _Let \(G=(V(G),E(G))\) and \(S\subset V(G)\). The induced subgraph \(G[S]\) of \(G\) over \(S\) is defined as the graph \(G[S]\) with vertices \(V(G[S])=S\) and edges \(E(G[S])=\{\{u,v\}\in E(G)\,|\,u,v\in S\}\)._

The following definition indicates whether a pair of nodes is connected by an edge or not.

**Definition 13** (Atomic Type).: _For a tuple of nodes \((u_{1},u_{2})\), the atomic type\(\operatorname{atp}_{G}\left((u_{1},u_{2})\right)\) of \(G\) over \((u_{1},u_{2})\) indicates where \(\{u_{1},u_{2}\}\in E(G)\), i.e., \(\operatorname{atp}_{G}((u_{1},u_{2}))=1\) if \(\{u_{1},u_{2}\}\in E(G)\) and zero otherwise._

We continue by defining _tree graphs_, an important class of graphs closely related to the \(1\)-WL test.

**Definition 14** (Tree Graph).: _A graph \(T\) is called a tree (graph) if it is connected and does not contain cycles. A rooted tree\(T^{s}=(V(T^{s}),E(T^{s}))\) is a tree in which a node \(s\in V(T^{s})\) is singled out. This node is called the root of the tree. For each vertex \(t\in V(T^{s})\), we define its depth\(\operatorname{dep}_{T^{s}}(t):=\operatorname{dist}_{T^{s}}(t,s)\), where \(\operatorname{dist}\) denotes the shortest path distance between \(t\) and \(s\). The depth of \(T^{s}\) is then the maximum depth among all nodes \(t\in V(T)\). We define \(\operatorname{Desc}_{T^{s}}(t)\) the set of descendants of \(t\), i.e., \(\operatorname{Desc}_{T^{s}}(t)=\{t^{\prime}\in T^{s}\mid\operatorname{dep}_{T^ {s}}(t^{\prime})=\operatorname{dep}_{T^{s}}(t)+\operatorname{dist}_{T^{s}}(t,t ^{\prime})\}\). For each \(t\in V(T^{s})\setminus\{s\}\), we define the parent node \(\operatorname{pa}_{T^{s}}(t)\) of \(t\) as the unique node \(t^{\prime}\in\mathcal{N}(t)\) such that \(\operatorname{dep}_{T^{s}}(t)=\operatorname{dep}_{T^{s}}(t^{\prime})+1\). We define the subtree of \(T^{s}\) rooted at node \(t\) by \(T^{s}[t]\), i.e., \(T^{s}[t]:=T^{s}[\operatorname{Desc}_{T^{s}}(t)]\)._

[MISSING_PAGE_FAIL:26]

1. \(G\cong\operatorname{canon}(G)\)__
2. \(G\cong H\Longleftrightarrow V(\operatorname{canon}(G))=V(\operatorname{canon}(H)) \wedge E(\operatorname{canon}(G))=E(\operatorname{canon}(H))\)_._

_Moreover, given \(G\in\mathcal{M}^{r+2}\), \(\operatorname{canon}(G)\) can be computed in linear time._

For each \(G\in\mathcal{M}^{r+2}\) we denote the isomorphism between \(G\) and \(\operatorname{canon}(G)\) as \(\operatorname{canon}_{G}\). Colbourn et al. (1981) describe a bottom-up algorithm to obtain \(\operatorname{canon}(G)\) of a fan \(r\)-cactus \(G\). We will implicitly use the results of this canonicalization to define a canonical _tree decomposition_ of fan \(r\)-cacti. The crucial point in the algorithm is a simple way to decide which "direction" to use when dealing with a cycle in the underlying cactus graph. Each undirected, rooted cycle allows for a choice between two directions when building a tree decomposition. We will first define a tree decomposition for a rooted cycle which depends on a choice of direction and then define a canonical direction of cycles in \(G\) based on \(\operatorname{canon}_{G}\).

**Definition 19** (Tree Decomposition of Rooted Cycle).: _Let \(C_{n}\) be a cycle graph on \(n\) nodes \(v_{0}\) to \(v_{n-1}\). The path \(T\) on nodes \(w_{1},\ldots,w_{2n-3}\) with bags \(\beta(w_{1})=\{v_{0},v_{1}\}\) and for \(i\geq 2\)_

\[\beta(w_{i})=\begin{cases}\beta(w_{i-1})\cup\left\{v_{i/2+1}\right\}&\text{ if $i$ is even}\\ \beta(w_{i-1})\setminus\left\{v_{(i-1)/2}\right\}&\text{ if $i$ is odd}\end{cases}\]

_is a tree decomposition of \(C_{n}\). We say that \(v_{0}\) and \(v_{1}\) correspond to \(w_{1}\) and \(v_{i}\) corresponds to \(w_{2i-1}\) for \(i\geq 2\)._

A depiction of the tree decomposition \(T^{0}\) (right) of \(C_{6}\) (left) is shown below. Note that we have to choose one of two possible orientations of the undirected cycle to construct \(T^{0}\). We address this choice in the next definition.

**Definition 20** (Canonical Tree Decomposition of Undirected Rooted Cycle).: _Let \(F^{s}\) be a fan \(r\)-cactus and \(C\) be a simple cycle in the underlying cactus \(G\). Let \(v_{C},v_{1},\ldots,v_{n-1}\) and \(v_{C},v_{n-1},\ldots,v_{1}\) be the two directions of \(C\) rooted at \(v_{C}\). We define the canonical tree decomposition of \(C\) in \(G\) as the tree decomposition of the smaller of the two orientations \(\operatorname{canon}_{F}(v_{C}),\operatorname{canon}_{F}(v_{1}),\ldots, \operatorname{canon}_{F}(v_{n-1})\) and \(\operatorname{canon}_{F}(v_{C}),\operatorname{canon}_{F}(v_{n-1}),\ldots, \operatorname{canon}_{F}(v_{1})\)._

The choice of "smaller" does not matter as long as it defines a total order. One can, for example, use a lexicographical order. Based on Definition 20, we now define a canonical tree decomposition of fan cactus graphs, in the sense that any two isomorphic fan cactus graphs will have isomorphic tree decompositions.

**Definition 21** (Canonical Tree Decomposition of Fan \(r\)-Cactus Graphs).: _Let \(F^{s}\) be a fan \(r\)-cactus and \(G^{s}\) its underlying \(r\)-cactus. We define the canonical tree decomposition \(T^{\tilde{s}}\) of \(F\) rooted at \(\tilde{s}\) as follows_

1. _Node Gadget:_ _For all_ \(v\in V(F)\) _add a node_ \(t\) _to_ \(V(T)\) _and set_ \(\beta(t)=\{v\}\)_. We choose_ \(\tilde{s}\) _such that_ \(\beta(\tilde{s})=\{s\}\)_._
2. _Tree Edge Gadget:_ _For all_ \(\{v,w\}\in E(G)\) _that are not on a simple cycle in_ \(F\) _add a node_ \(x_{\{v,w\}}\) _to_ \(V(T)\) _with_ \(\beta(x_{\{v,w\}})=\{v,w\}\) _and edges_ \(\left\{v,x_{\{v,w\}}\right\}\) _and_ \(\left\{w,x_{\{v,w\}}\right\}\) _to_ \(E(T)\)__
3. _Cycle Gadget:_ _For each (undirected) cycle_ \(C\) _in the underlying cactus_ \(G\)_, add a copy of its canonical tree decomposition_ \(T_{C}^{v_{C}}\) _of_ \(C\) _rooted at_ \(v_{C}\) _to_ \(T\) _and connect nodes in it to the corresponding node gadgets._

See Figure 10 for an illustration. For the discussions in subsequent sections, we introduce the following definition.

**Definition 22** (Depth in the Canonical Tree Decomposition of Fan \(r\)-Cactus Graphs).: _Let \((F,T^{s})\) be a canonical tree decomposition of a fan \(r\)-cactus. We define the depth\(\operatorname{dep}(t)\) of \(t\in V(T)\) recursively as follows:_

1. \(\operatorname{dep}(s)=0\)__
2. _For_ \(v\in V(T)\) _with parent node_ \(p\)_:_ \(\operatorname{dep}(v)=\begin{cases}\operatorname{dep}(p)+1&\text{ if }|\beta(v)|=1 \text{ or }|\beta(p)|=1\\ \operatorname{dep}(p)&\text{ otherwise}\end{cases}\)__

_The depth of \((F,T^{s})\) is then the maximum depth of any node \(t\in V(T^{s})\)._

Intuitively, for a given fan \(r\)-cactus graph \(F\) with its canonical tree decomposition \(T^{s}\), Definition 22 captures the depth (see Definition 15) of the tree \(T^{s}\), if cycles in \(F\) and the corresponding bags in \(T^{s}\) were replaced by single edges.

**Lemma 4**.: _Let \(F^{s}\) be a fan \(r\)-cactus. The canonical tree decomposition \((F,T^{\tilde{s}})\) is a tree decomposition of \(F^{s}\)._

Proof.: We need to show that (1) \(T\) is a tree, (2) for every edge \(e\in E(F)\) there exists some bag \(\beta(v)\) with \(e\subseteq\beta(v)\), and (3) \(T[\{t\in V(T):u\in\beta(t)\}]\) is connected.

To see that \(T\) does not contain cycles, note that we replace each cycle with its cycle gadget, which is a path. It is easy to see that \(T\) is connected as \(G\) is connected.

For (2), note that tree edges \(e\in V(F)\) have their own gadget node in \(x_{e}\) with \(\beta(x_{e})=e\). Similarly, each edge \(e\) on a simple cycle \(C\) of the underlying cactus \(F\) of \(G\) is contained in some bag within the cycle gadget of \(C\). Finally, for diagonal edges \(\{v_{C},w\}\in E(F)\setminus E(G)\), \(v_{C}\) is contained in any bag of the cycle gadget of \(C\). As a result, \(\{v_{C},v\}\) is contained in the bag of the corresponding node of \(v\)

Figure 10: Example of a fan \(6\)-cactus \(F^{1}\) (left) and its canonical tree decomposition \((T,1)\). The underlying rooted \(6\)-cactus \(G^{1}\) (on colored, thick edges) of \(F^{1}\) contains three simple cycles \(C_{1},C_{2},C_{3}\). Additional diagonal edges must have \(v_{C_{i}}\) as one endpoint.

For (3), note that in the tree edge gadget, nodes \(t\) with \(v\in\beta(t)\) are connected to the node gadget of \(v\). In the cycle gadget, any node \(t\) with \(w\in\beta(t)\) is either directly or via its neighbor connected to the node gadget of \(w\) if \(w\neq v_{C}\). As the cycle gadget is connected and \(v_{C}\) is in any bag of the gadget, a path to the node gadget of \(v_{C}\) exists where every bag contains \(v_{C}\). 

We conclude this subsection with a formal definition of when two canonical tree decompositions are isomorphic and prove the main result of this section, i.e., that canonical tree decompositions of fan \(r\)-cacti \(G^{*},H^{t}\) are isomorphic whenever \(G^{*},H^{t}\) are isomorphic.

**Definition 23** (Isomorphism between canonical tree-decomposed graphs).: _Given two canonical tree-decomposed graphs \((G,T^{*})\) and \((\tilde{G},\tilde{T}^{\tilde{s}})\), a pair of mappings \((\rho,\tau)\) is called an isomorphism between \((G,T^{*})\) and \((\tilde{G},\tilde{T}^{\tilde{s}})\), denoted by \((G,T^{*})\cong(\tilde{G},\tilde{T}^{\tilde{s}})\), if the following holds:_

* \(\rho\) _is an isomorphism between_ \(G\) _and_ \(\tilde{G}\)_,_
* \(\tau\) _is an isomorphism between_ \(T^{s}\) _and_ \(\tilde{T}^{\tilde{s}}\)_,_
* _For any_ \(t\in T^{s}\)_, we have_ \(\rho(\beta_{T}(t))=\beta_{\tilde{T}}(\tau(t))\)_._

**Lemma 5**.: _Let \(G^{*}\cong H^{t}\) be rooted \(r\)-fan cacti. Then \((G^{*},T[G^{*}])\cong(H^{t},T[H^{t}])\)._

Proof.: Let \(\rho\) be a root preserving isomorphism between \(G^{*}\) and \(H^{t}\). According to Lemma 3 then there exist isomorphisms \(\mathrm{canon}_{G}\) and \(\mathrm{canon}_{H}\) with \(\rho=\mathrm{canon}_{G}\circ\mathrm{canon}_{H}^{-1}\). We construct \(\tau:V(T[G^{*}])\to V(T[H^{t}])\) from \(\rho\) as follows: It is easy to see that \(\rho\) induces a bijective mapping \(\tau\) between the nodes of \(T[G^{*}]\) and \(T[H^{t}]\) that assigns each gadget node \(v\in V(T[G^{*}])\) to the unique gadget node \(\tau(v)\in V(T[H^{t}])\) with \(\beta(\tau(v))=\rho(\beta(v))\). By the same argument, \(\tau\) maps the root of \(T[G^{s}]\) to the root of \(T[H^{t}]\).

Now assume by contradiction that \(\tau\) is not an isomorphism between \(T[G^{*}]\) and \(T[H^{t}]\). That means that w.l.o.g. there exists \(\{v,w\}\in E(T[G^{s}])\) with \(\{\tau(v),\tau(w)\}\notin E(T[H^{t}])\). However, for the bags of \(v,w\) it holds \(\mathrm{canon}_{G}(\beta(v))=\mathrm{canon}_{H}(\beta(\tau(v)))\) and \(\mathrm{canon}_{G}(\beta(w))=\mathrm{canon}_{H}(\beta(\tau(w)))\). This cannot happen, as the addition of edges in Definition 21 depends only on the images of the bags under canon. 

### Alternative \(r\)-\(\ell\)Wl

In this subsection, we define slightly modified versions of \(1\)-WL and \(r\)-\(\ell\)WL that we consider in the subsequent sections.

**Definition 24** (Alternative \(1\)-WL and \(r\)-\(\ell\)Wl).: _The alternative \(1\)-WL test refines vertices' colors as_

\[c^{(t+1)}(v)\leftarrow\mathrm{HASH}\left(c^{(t)}(v),\Big{\{}\Big{\{}\Big{(} \mathrm{atp}(v,u),c^{(t)}(u)\Big{)}\ |\ u\in V(G)\Big{\}}\Big{\}}\right).\]

_Equivalently, we define the alternative \(r\)-\(\ell\)WL via_

\[c^{(t+1)}_{r}(v)\leftarrow\mathrm{HASH}_{r}\left(c^{(t)}_{r}(v),\Big{\{}\Big{\{}\Big{(}\mathrm{atp}(v,u),c^{(t)}(u)\Big{)}\ |\ u\in V(G) \Big{\}}\Big{\}}\right.,\] \[\Big{\{}\Big{c^{(t)}_{r}(\mathbf{p})\ |\ \mathbf{p}\in \mathcal{N}_{1}(v)\Big{\}}\Big{\}}\,,\] \[\qquad\vdots\] \[\Big{\{}\Big{\{}c^{(t)}_{r}(\mathbf{p})\ |\ \mathbf{p}\in \mathcal{N}_{r}(v)\Big{\}}\Big{\}}\,\Bigg{)},\]

It is well-known that both the alternative \(1\)-WL test and the standard \(1\)-WL test are equally powerful (in terms of their expressive power). Similarly, the alternative \(k\)-WL test and the standard \(k\)-WL test are equally powerful. For the sake of simplicity in the subsequent discussion, we will refer to both the alternative \(1\)-WL and \(k\)-WL tests simply as the \(1\)-WL and \(k\)-WL tests, respectively. Although this practice may seem like a slight abuse of notation, it is justified because the expressive power of these tests remains unaffected.

Finally, as noted in Section 6, we alter the \(r\)-\(\ell\)WL algorithm slightly by incorporating atomic types into the path representation. Hence, we update node features according to

\[\begin{split} c_{r}^{(t+1)}(v)\leftarrow\mathrm{HASH}_{r}\left(c_{r} ^{(t)}(v),&\left\{\left\{\left(\mathrm{atp}(v,u),c^{(t)}(u)\right) \mid u\in V(G)\right\}\right\},\right.\\ &\left.\left\{\left\{\left(\mathrm{atp}(v,\mathbf{p}),c_{r}^{(t)}( \mathbf{p})\right)\mid\mathbf{p}\in\mathcal{N}_{1}(v)\right\}\right\},\right.\\ &\qquad\qquad\vdots\\ &\left.\left\{\left\{\left(\mathrm{atp}(v,\mathbf{p}),c_{r}^{(t)}( \mathbf{p})\right)\mid\mathbf{p}\in\mathcal{N}_{r}(v)\right\}\right\}\right\} \right),\end{split} \tag{7}\]

where \(\left(\mathrm{atp}(v,\mathbf{p}),c_{r}^{(t)}(\mathbf{p})\right):=\left(\left( \mathrm{atp}(v,p_{1}),c_{r}^{(t)}(p_{1})\right),\ldots,\left(\mathrm{atp}(v,p_ {q+1}),c_{r}^{(t)}(p_{q+1})\right)\right)\) for \(\mathbf{p}=\left\{p_{i}\right\}_{i=1}^{q+1}\in\mathcal{N}_{q}(v)\). The definition of atomic types \(\mathrm{atp}(\cdot,\cdot)\) is given in Definition 13. Clearly this version of \(r\)-\(\ell\)WL is more powerful than the standard version, according to Definition 3. However, it is unclear whether \(r\)-\(\ell\)WL with atomic types is strictly more powerful than standard \(r\)-\(\ell\)WL.

### The Unfolding Tree of \(r\)-\(\ell\)WL

Given Definition 20, we assume, for the remainder of this appendix, that every fan cactus graph has a unique labeling function, allowing us to select a unique orientation for every cycle in the graph. We call this orientation the _canonical orientation_. If not otherwise mentioned, we consider the canonical orientation of cycle graphs.

We begin this section by introducing a critical concept known as _bag isomorphism_(Dell et al., 2018; B. Zhang et al., 2024).

**Definition 25** (Bag Isomorphism).: _Let \((F,T^{s})\) be a tree-decomposed graph, and \(G\) be a graph. A homomorphism \(f\) from \(F\) to \(G\) is called a bag isomorphism from \((F,T^{s})\) to \(G\) if, for all \(t\in V(T^{s})\), the mapping \(f\) is an isomorphism from \(F[\beta_{T^{s}}(t)]\) to \(G[f(\beta_{T^{s}}(t))]\). We denote by \(\mathrm{Blso}((F,T^{s}),G)\) the set of all bag isomorphisms from \((F,T^{s})\) to \(G\), and set \(\mathrm{blso}((F,T^{s}),G)=|\mathrm{Blso}((F,T^{s}),G)|\)._

Moving forward, we proceed to define \(r\)-\(\ell\)WL unfolding trees, which intuitively construct, for a given graph and a node in the graph, the computational graph of the \(r\)-\(\ell\)WL algorithm and its canonical tree decomposition.

**Definition 26** (Unfolding tree of \(r\)-\(\ell\)WL).: _Given a graph \(G\), vertex \(v\in V(G)\) and a non-negative \(D\in\mathbb{Z}\), the depth-\(2D\)\(r\)-\(\ell\)WL unfolding tree of a graph \(G\in\mathcal{M}^{r+2}\) at node \(v\), denoted as \(\left(F^{(D)}(v),T^{(D)}(v)\right)\), is a tree-decomposition \((F,T^{s})\) constructed in the following way:_

1. _Initialization:_ \(V(F)=\{v\}\) _without edges, and_ \(T^{s}\) _only has a root node_ \(s\) _with_ \(\beta_{T^{s}}(s)=\{v\}\)_. Define a mapping_ \(V(F)\to V(G)\) _as_ \(\pi(v)=v\)_._
2. _Introduce nodes: For each leaf node_ \(t\) _with_ \(|\beta_{T^{s}}(t)|=1\) _in_ \(T^{s}\)_, do the following procedure:_ _Let_ \(\beta_{T}(t)=\{g\}\)_. For each_ \(w\in V(G)\) _do the following:_ 1. _Add a fresh child_ \(t_{w}\) _to_ \(t\) _in_ \(T^{s}\)_._ 2. _Add a fresh vertex_ \(f\) _to_ \(F\) _and extend_ \(\pi\) _with_ \([f\mapsto w]\)_._ 3. _Define the bag of_ \(t_{w}\) _by_ \(\beta_{T^{s}}(t_{w})=\beta_{T^{s}}(t)\cup\{f\}\)_._ 4. _Add an edge between_ \(f\) _and_ \(g\) _if_ \(\{\pi(f),\pi(g)\}\in E(G)\)_._
3. _Introduce paths: For each_ \(q=1,\ldots,r\)_, do:_ _For each length_ \(q\) _path with canonical orientation_ \(\mathbf{p}=\{p_{i}\}_{i=1}^{q+1}\in\mathcal{N}_{q}(g)\)_, do the following:_ 1. _Add a fresh path_ \(\mathbf{t_{p}}=\left\{t_{\{p_{1}\}},t_{\{p_{1},p_{2}\}},t_{\{p_{2}\}},\ldots,t_ {\{p_{q}\}},t_{\{p_{q},p_{q+1}\}},t_{\{p_{q}+1\}}\right\}\) _to_ \(t\) _in_ \(T^{s}\)_._ 2. _Add_ \(q+1\) _fresh vertices_ \(f_{1},\ldots,f_{q+1}\) _to_ \(F\) _and extend_ \(\pi\) _with_ \([f_{i}\mapsto p_{i}]\) _for every_ \(i=1,\ldots,q+1\)_._
3. _For_ \(i=1,\ldots,q\)_, let the bag of_ \(t_{\{p_{i},p_{i+1}\}}\) _be defined via_ \(\beta_{T^{*}}(t_{\{p_{i},p_{i+1}\}})=\beta_{T^{*}}(t)\cup\{f_{i},f_{i+1}\}\)_._ 4. _For_ \(i=1,\ldots,q+1\)_, let the bag of_ \(t_{\{p_{i}\}}\) _be defined via_ \(\beta_{T^{*}}(t_{\{p_{i}\}})=\beta_{T^{*}}(t)\cup\{f_{i}\}\)_._ 5. _For_ \(i=1,\ldots,q+1\)_, add edges between_ \(f_{i}\) _and_ \(f_{i+1}\)_._ 6. _Add edges between_ \(g\) _and_ \(f_{1},\ldots,f_{q+1}\) _such that for every_ \(i=1,\ldots,q\)_, we have_ \(F[\beta_{T^{*}}(t_{\{p_{i},p_{i+1}\}})]=F[\{f_{i},f_{i+1},g\}]\cong G[\pi(\beta_ {T^{*}}(t_{\{p_{i},p_{i+1}\}}))]\)_, i.e., add edges between_ \(g\) _and_ \(f_{i}\) _if and only if there is an edge between_ \(\{\pi(g),\pi(f_{i})\}\in E(G)\)_._
4. _Forget nodes: If_ \(t\) _is a leaf node of_ \(T^{s}\) _with_ \(|\beta_{T^{*}}(t)|=2\) _and parent_ \(t^{\prime}\) _with_ \(|\beta_{T^{*}}(t)|=1\)_, do the following:_ 1. _Add a fresh child_ \(t_{1}\) _of_ \(t\) _to_ \(T^{s}\)_._ 2. _Let_ \(f\) _be that vertex introduced at_ \(t\)_, that is, we have_ \(\beta_{T^{*}}(t)\setminus\beta_{T^{*}}(t^{\prime})=\{f\}\)_._ 3. _We set_ \(\beta_{T^{*}}(t_{1})=\{f\}\)_._
5. _Forget paths: If_ \(\mathbf{t_{p}}=\big{\{}t_{\{p_{1}\}},t_{\{p_{1},p_{2}\}},t_{\{p_{2}\}},\ldots,t_ {\{p_{q}\}},t_{\{p_{q},p_{q+1}\}},t_{\{p_{q+1}\}}\big{\}}\) _is a leaf path of_ \(T^{s}\) _with parent_ \(t^{\prime}\) _of_ \(t_{\{p_{i}\}}\)_, do the following:_ 1. _For_ \(i=2,\ldots,q+1\)_, add a fresh child_ \(\tilde{t}_{\{p_{i}\}}\) _to_ \(t_{\{p_{i}\}}\)_._ 2. _Let_ \(f_{2},\ldots,f_{q+1}\) _be the vertices introduced at_ \(\mathbf{t_{p}}\)_, that is, we have_ \(\beta_{T^{*}}(t_{\{p_{i}\}})\setminus\beta_{T^{*}}(t^{\prime})=\{f_{i}\}\)_._ 3. _For_ \(i=2,\ldots,q+1\)_, we set_ \(\beta_{T^{*}}(\tilde{t}_{\{p_{i}\}})=\{f_{i}\}\)_._

We refer to Figure 11 for the depth-\(2\)\(2\)-\(\ell\)WL unfolding tree of an example graph.

**Theorem 4**.: _Let \(r\geq 1\). For any graph \(G\), any vertex \(v\in V(G)\), and any non-negative integer \(D\), let \(\big{(}F^{(D)}(v),T^{(D)}(v)\big{)}\) be its depth-\(2D\)\(r\)-\(\ell\)WL unfolding tree at node \(v\). Then, \(F^{(D)}(v)\) is a fan \(r\)-cactus graph, and \(T^{(D)}(v)\) is an \(r\)-canonical tree decomposition of \(F^{(D)}(v)\). Moreover, the constructed mapping \(\pi\) in Definition 26 is a bag isomorphism from \(\big{(}F^{(D)}(v),T^{(D)}(v)\big{)}\) to the graph \(G\)._

Proof.: Clear by the definition of the depth-\(2D\) unfolding tree of \(r\)-\(\ell\)WL. 

We present the following results that fully characterize when two graphs and their respective nodes have the same \(r\)-\(\ell\)WL colors in terms of their \(r\)-\(\ell\)WL unfolding trees.

**Theorem 5**.: _Let \(r\in\mathbb{N}\). For any two connected graphs \(G,H\), any vertices \(v\in V(G)\) and \(x\in V(H)\) and any \(D\in\mathbb{N}\), it holds: \(c_{r}^{(D)}(v)=c_{r}^{(D)}(x)\) if and only if there exists a root preserving isomorphism between \(\big{(}F^{(D)}(v),T^{(D)}(v)\big{)}\) and \(\big{(}F^{(D)}(x),T^{(D)}(x)\big{)}\)._

Proof of "\(\Longrightarrow\)".: The proof is based on induction over \(D\). When \(D=0\), the theorem obviously holds. Assume that the theorem holds for \(D\leq d\), and consider \(D=d+1\). We show that if \(c_{r}^{(d+1)}(v)=c_{r}^{(d+1)}(x)\), then there exists an isomorphism \((\rho,\tau)\) from \(\big{(}F^{(d+1)}(v),T^{(d+1)}(v)\big{)}\) to \(\big{(}F^{(d+1)}(x),T^{(d+1)}(x)\big{)}\) such that \(\rho(v)=x\).

If \(c_{r}^{(d+1)}(v)=c_{r}^{(d+1)}(x)\), then

\[\Big{\{}\Big{\{}\Big{(}\mathrm{atp}(x,y),c_{r}^{(d)}(y)\Big{)}\mid u\in V(G) \Big{\}}\Big{\}}=\Big{\{}\Big{\{}\Big{(}\mathrm{atp}(x,y),c_{r}^{(d)}(y) \Big{)}\mid y\in V(H)\Big{\}}\Big{\}}\,,\]

Figure 11: The depth-\(2\) unfolding tree of graph \(G\) at vertex \(1\) for \(2\)-\(\ell\)WL.

i.e., \(|V(G)|=|V(H)|\), and we set \(n=|V(G)|\). We enumerate \(V(G)=\{w_{1},\ldots,w_{n}\}\) and \(V(H)=\{z_{1},\ldots,z_{n}\}\) such that

\[c_{r}^{(d)}(w_{i})=c_{r}^{(d)}(z_{i}) \tag{8}\]

for all \(i=1,\ldots,n\). Also, again since \(c_{r}^{(d+1)}(v)=c_{r}^{(d+1)}(x)\), we have for every \(q=1,\ldots,r\),

\[\left\{\left\{\left(\left(\operatorname{atp}(v,u_{1}),c_{r}^{(d) }(u_{1})\right),\ldots,\left(\operatorname{atp}(v,u_{q+1}),c_{r}^{(d)}(u_{q+1 })\right)\right)\mid\{u_{1},\ldots,u_{q+1}\}=\mathbf{u}\in\mathcal{N}_{q}(v) \right\}\right\}\] \[= \left\{\left\{\left(\left(\operatorname{atp}(x,y_{1}),c_{r}^{(d) }(y_{1})\right),\ldots,\left(\operatorname{atp}(x,y_{q+1}),c_{r}^{(d)}(y_{q+1 })\right)\right)\mid\{y_{1},\ldots,y_{q+1}\}=\mathbf{y}\in\mathcal{N}_{q}(x) \right\}\right\}.\]

In particular, \(|\mathcal{N}_{q}(v)|=|\mathcal{N}_{q}(x)|\) and we can enumerate the paths in \(\mathcal{N}_{q}(v)\) and \(\mathcal{N}_{q}(x)\) such that

\[c_{r}^{(d)}(\mathbf{u}_{l}^{q})=c_{r}^{(d)}(\mathbf{y}_{l}^{q})\;\;\text{and} \;\;\operatorname{atp}(v,\mathbf{u}_{l}^{q})=\operatorname{atp}(v,\mathbf{y}_ {l}^{q}) \tag{9}\]

for every \(l=1,\ldots,|\mathcal{N}_{q}(v)|\).

Now, by definition of the \(r\)-\(\ell\)WL unfolding tree, the graph \(F^{(d+1)}(v)\) is isomorphic to the union of: a) all graphs \(F^{(d)}(w_{i})\) for \(i=1,\ldots,n\), plus additional edges between \(w_{i}\) to \(v\) if \(\{w_{i},v\}\in E(G)\), and b) all graphs \(F^{(d)}(p_{l,k}^{q})\) for \(q=1,\ldots,r\), \(l=1,\ldots,|\mathcal{N}_{q}(v)|\) for any path \(\mathbf{p}_{l}^{q}=\left\{p_{l,1}^{q},\ldots,p_{l,q+1}^{q}\right\}\in\mathcal{N }_{q}(v)\). And adding, for \(k=1,\ldots q\), edges between \(p_{l,k}^{q}\) and \(p_{l,k+1}^{q}\). And adding, for \(k=1,\ldots q+1\), edges \(p_{l,k}^{q}\) and \(v\) if there is one in \(G\), i.e., if \(\left\{p_{l,k}^{q},v\right\}\in E(G)\).

Similarly, the tree \(T^{(d+1)}(v)\) is isomorphic to the disjoint union of all trees \(T^{(d)}(w_{i})\) (for \(i=1,\ldots,n\)) and \(T^{(d)}(p_{l,k}^{q})\) (for \(q=1,\ldots,r\), \(k=1,\ldots,q+1\) and \(l=1,\ldots,|\mathcal{N}(v)|\)). Plus adding the following fresh tree nodes and edges: a root node \(s\), nodes \(t_{w_{i}}\) (for \(i=1,\ldots,n\)) that connects to \(s\) and the root of \(T^{(d)}(w_{i})\). And for \(q=1,\ldots,r\), \(l=1,\ldots,|\mathcal{N}_{q}(v)|\) for any path \(\mathbf{p}_{l}^{q}\in\mathcal{N}_{q}(v)\) a path of length \(2q\), given by, where \(s\) is attached to \(t_{\left\{p_{l,i}^{q}\right\}}\). And finally, connecting the trees \(T^{(d)}(p_{l,k}^{q})\) at root node \(p_{l,k}^{q}\) to \(\left\{p_{l,k}^{q}\right\}\).

By (8) and induction, there exist isomorphisms \((\rho_{i},\tau_{i})\) from \((F^{(d)}(w_{i}),T^{(d)}(w_{i}))\) to \((F^{(d)}(z_{i}),T^{(d)}(z_{i}))\) such that \(\rho_{i}(w_{i})=z_{i}\) for \(i=1,\ldots,n\). By (9) and induction, there exist isomorphisms \((\rho_{l,k}^{q},\tau_{l,k}^{q})\) from \((F^{(d)}(u_{l,k}^{q}),T^{(d)}(u_{l,k}^{q}))\) to \((F^{(d)}(y_{l,k}^{q}),T^{(d)}(y_{l,k}^{q}))\) such \(\rho_{i}(u_{l,k}^{q})=y_{l,k}^{q}\) for \(q=1,\ldots,r\), \(k=1,\ldots,q+1\) and \(l=1,\ldots,|\mathcal{N}_{q}(v)|\).

We now construct \(\rho\) by merging all \(\rho_{i}\) and \(\rho_{l,k}^{q}\), and construct \(\tau\) by merging all \(\tau_{i}\) and \(\tau_{l,k}^{q}\). We finally specify an appropriate mapping for the tree root, its direct children and the paths attached to the tree root. Then, it is easy to see that \((\rho,\tau)\) is well-defined and an isomorphism between \(\left(F^{(d+1)}(v),T^{(d+1)}(v)\right)\) and \(\left(F^{(d+1)}(x),T^{(d+1)}(x)\right)\) such that \(\rho(v)=x\). 

Proof of "\(\Longleftarrow\)".: We now prove the other direction, again via induction over \(D\). When \(D=0\) the assertion obviously holds. Assume that the assertion holds for \(D\leq d\). Now, assume that there exists an isomorphism \((\rho,\tau)\) between \(\left(F^{(d+1)}(v),T^{(d+1)}(v)\right)\) and \(\left(F^{(d+1)}(x),T^{(d+1)}(x)\right)\) such that \(\rho(v)=x\). We show that \(c_{r}^{(d+1)}(v)=c_{r}^{(d+1)}(x)\).

We begin our proof by establishing the equality of two multisets: \(\left\{\left(c_{r}^{(d)}(w),\operatorname{atp}(v,w))|w\in V(G)\right\}\right\}\) and \(\left\{\left(c_{r}^{(d)}(z),\operatorname{atp}(v,z))|z\in V(H)\right\}\right\}\). The proof of this equivalence closely mirrors the argument presented in the proof of B. Zhang et al. (2024, Lemma C.14). Since \(\tau\) is an isomorphism it maps all tree nodes \(T^{(d+1)}(v)\) of depth \(2\) with \(1\) element in their bag to the corresponding tree nodes in \(T^{(d+1)}(x)\). Let \(s_{1},\ldots,s_{n}\) and \(t_{1},\ldots,t_{n}\) be the nodes in \(T^{(d+1)}(v)\) and \(T^{(d+1)}(x)\) of depth \(2\) with \(1\) element in their bag, respectively. For \(i=1,\ldots,n\), let \(s_{i}^{\prime}\) and \(t_{i}^{\prime}\) the parents of \(s_{i}\) and \(t_{i}\), respectively. We then choose the order such that the following holds for all \(i=1,\ldots,n\)

1. Let \(\beta_{T^{(d+1)}(v)}(s_{i}^{\prime})=\{v,\tilde{w}_{i}\}\) and \(\beta_{T^{(d+1)}(x)}(t_{i}^{\prime})=\{x,\tilde{z}_{i}\}\). Then, \(\rho(v)=x\) and \(\rho(\tilde{w}_{i})=\tilde{z}_{i}\) and thus, per assumption, \(\{v,\tilde{w}_{i}\}\in E(F^{(d+1)}(v))\) if and only if \(\{x,\tilde{z}_{i}\}\in E(F^{(d+1)}(x))\).

[MISSING_PAGE_FAIL:33]

We introduce the following definition that provides a similarity measure between a graph and a tree-decomposed graph.

**Definition 27**.: _Given a graph \(G\) and a tree-decomposed graph \((F,T^{s})\), define_

\[\operatorname{cnt}\left((F,T^{s}),G\right)=\left|\left\{v\in V\mid\exists D\in \mathbb{N}\text{ s.t. }(F^{(D)}(v),T^{(D)}(v))\cong(F,T^{s})\right\}\right|,\]

_where \((F^{(D)}(v),T^{(D)}(v))\) is the depth-\(2D\)\(r\)-\(\ell\)WL unfolding tree of \(G\) at \(v\)._

The counting function \(\operatorname{cnt}\left((F,T^{s}),G\right)\) serves as a key metric, allowing us to draw connections between \(r\)-\(\ell\)WL colorings of two different graphs.

**Corollary 3**.: _Let \(r\in\mathbb{N}\). Let \(G\) and \(H\) be two graphs. Then, \(c_{r}(G)=c_{r}(H)\) if and only if \(\operatorname{cnt}\left((F,T^{s}),G\right)=\operatorname{cnt}\left((F,T^{s}),H\right)\) holds for all graphs \((F,T^{s})\in\mathcal{M}^{r+2}\)._

Proof of "\(\Longrightarrow\)".: Let \(c_{r}(G)=c_{r}(H)\), i.e.,

\[\left\{\left\{c_{r}(v)\,|\,v\in V(G)\right\}\right\}=\left\{\left\{c_{r}(x)\, |\,x\in V(H)\right\}\right\}.\]

Assume, by contradiction, that there exists a tuple \((F,T^{s})\in\mathcal{M}^{r+2}\) such that \(\operatorname{cnt}\left((F,T^{s}),G\right)\neq\operatorname{cnt}\left((F,T^{s }),H\right)\). Let \(c_{1},\ldots,c_{k}\) be the final colors of nodes in \(V(G)\) and \(V(H)\). Then, define for \(i=1,\ldots,k\)

\[\operatorname{cnt}\left((F,T^{s}),G[c_{i}]\right):=\left|\left\{v\in V(G)\,|\, c_{r}(v)=c_{i}\text{ and }\exists D\in\mathbb{N}\text{ s.t. }(F^{(D)}(v),T^{(D)}(v))\cong(F,T^{s})\right\}\right|.\]

We have

\[\operatorname{cnt}\left((F,T^{s}),G\right)=\sum_{i=1}^{k}\operatorname{cnt} \left((F,T^{s}),G[c_{i}]\right),\]

and

\[\operatorname{cnt}\left((F,T^{s}),H\right)=\sum_{i=1}^{k}\operatorname{cnt} \left((F,T^{s}),H[c_{i}]\right).\]

Since \(\operatorname{cnt}\left((F,T^{s}),G\right)\neq\operatorname{cnt}\left((F,T^{s }),H\right)\), there exist an index \(i=1,\ldots,k\) such that

\[\operatorname{cnt}\left((F,T^{s}),G[c_{i}]\right)\neq\operatorname{cnt}\left(( F,T^{s}),H[c_{i}]\right). \tag{10}\]

Furthermore, there exists \(i_{n}\in\mathbb{N}\) such that there are exactly \(n\) nodes \(v_{1},\ldots,v_{n}\) and \(x_{1},\ldots,x_{n}\) such that

\[c_{r}(v_{1})=\ldots=c_{r}(v_{i_{n}})=c_{i}\text{ and }c_{r}(x_{1})=\ldots=c_{r}(x_{i_{n}})=c_{i}.\]

Hence, as \(c_{r}\) refines \(c_{r}^{(D)}\), we have

\[c_{r}^{(D)}(v_{1})=\ldots=c_{r}^{(D)}(v_{i_{n}})=c_{r}^{(D)}(x_{1})=\ldots=c_{ r}^{(D)}(x_{i_{n}}).\]

By (10), there exists some \(D\in\mathbb{N}\) such that (without loss of generality) \((F^{(D)}(v_{1}),T^{(D)}(v_{1}))\cong(F,T^{s})\). Then, by Theorem 5, we have

\[(F^{(D)}(v_{1}),T^{(D)}(v_{1}))\cong\ldots\cong(F^{(D)}(v_{i_{n}} ),T^{(D)}(v_{i_{n}})) \cong(F^{(D)}(x_{i_{n}}),T^{(D)}(x_{i_{n}}))\] \[\cong\ldots\cong(F^{(D)}(x_{1}),T^{(D)}(x_{1})).\]

There does not exist any other node \(w\) with \(c_{r}(w)=c_{1}\) such that the corresponding unfolding tree is isomorphic to \((F^{(D)}(v_{1}),T^{(D)}(v_{1}))\). Hence, \(\operatorname{cnt}((F,T^{s}),G[c_{i}])=\operatorname{cnt}((F,T^{s}),H[c_{i}])\), which is a contradiction.

Proof of "\(\Longleftarrow\)".: Suppose that \(\operatorname{cnt}((F,T^{s}),G)=\operatorname{cnt}((F,T^{s}),H)\) for all \((F,T^{s})\in\mathcal{M}^{r+2}\). Let \(c_{1},\ldots,c_{k_{G}}\) with multiplicities \(m_{1},\ldots,m_{k_{G}}\) and \(\tilde{c}_{1},\ldots,\tilde{c}_{k_{H}}\) with multiplicities \(\tilde{m}_{1},\ldots,\tilde{m}_{k_{G}}\) be the final colors of \(r\)-\(\ell\)WL applied to \(G\) and \(H\), respectively. Consider some \(v\in V(G)\) such that \(c_{r}(v)=c_{1}\). Let \(D\) be sufficiently large (any \(D\) after convergence of \(r\)-\(\ell\)WL), then \(\operatorname{cnt}((F^{(D)}(v),T^{(D)}(v)),G)=\operatorname{cnt}((F^{(D)}(v),T^{ (D)}(v)),H)\) since \((F^{(D)}(v),T^{(D)}(v))\in\mathcal{M}^{r+2}\). Hence, without loss of generality, \(c_{1}=\tilde{c}_{1}\) and \(m_{1}=\tilde{m}_{1}\). Repeating this argument for all colors finishes the proof.

#### g.4.1 Proof of Theorem 2

In this section, we employ techniques adapted from the works of Dell et al. (2018) and B. Zhang et al. (2024) to derive a proof for Theorem 2 from the established result in Corollary 3.

**Definition 28** (Definition 20 in (Dell et al., 2018)).: _Let \((F,T^{t})\) and \((\tilde{F},\tilde{T}^{s})\) be two tree-decomposed graphs. A pair of mappings \((\rho,\tau)\) is said to be a bag isomorphism homomorphism from \((F,T^{t})\) to \((\tilde{F},\tilde{T}^{s})\) if it satisfies the following conditions_

1. \(\rho\) _is a homomorphism from_ \(F\) _to_ \(\tilde{F}\)_._
2. \(\tau\) _is a homomorphism from_ \(T^{t}\) _to_ \(\tilde{T}^{s}\)_._
3. \(\tau\) _is depth-surjective, i.e., the image of_ \(T^{t}\) _under_ \(\tau\) _contains vertices at every depth present in_ \(\tilde{T}^{s}\)_._
4. _For all_ \(t^{\prime}\in T^{t}\)_, we have_ \(\mathrm{dep}_{T^{s}}(t^{\prime})=\mathrm{dep}_{\tilde{T}^{s}}(\tau(t^{\prime}))\) _and_ \(F[\beta_{T^{s}}(t^{\prime})]\cong\tilde{F}[\beta_{\tilde{T}^{s}}(\tau(t^{\prime }))]\)_._
5. _For all_ \(t^{\prime}\in T^{t}\)_, the set equality_ \(\rho(\beta_{T^{t}}(t^{\prime}))=\beta_{\tilde{T}^{s}}(\tau(t^{\prime}))\) _holds._
6. _The depth of_ \(T^{t}\) _and_ \(\tilde{T}^{s}\) _is equal._

_We denote the set of bag isomorphism homomorphisms from \((F,T^{t})\) to \((\tilde{F},\tilde{T}^{s})\) by \(\mathrm{BIsoHom}\left((F,T^{t}),(\tilde{F},\tilde{T}^{s})\right)\) and set \(\mathrm{blsoHom}\left((F,T^{t}),(\tilde{F},\tilde{T}^{s})\right)=|\mathrm{BIsoHom }\left((F,T^{t}),(\tilde{F},\tilde{T}^{s})\right)|\)._

We continue with the following lemma that shows a linear relation between the number of bag isomorphisms and the output of the counting function in Definition 27.

**Lemma 6**.: _Let \(r\in\mathbb{N}\). For any tree-decomposed graph \((F,T^{s})\in\mathcal{M}^{r+2}\) and any graph \(G\), it holds_

\[\mathrm{blso}\left((F,T^{s}),G\right)=\sum_{(\tilde{F},\tilde{T}^{s})\in \mathcal{M}^{r+2}}\mathrm{blsoHom}\left((F,T^{s})\,,\left(\tilde{F},\tilde{T} ^{t}\right)\right)\cdot\mathrm{cnt}\left(\left(\tilde{F},\tilde{T}^{t}\right), G\right). \tag{11}\]

Proof.: Let \((F,T^{s})\) be a tree-decomposed graph such that \(T^{s}\) has depth \(2D\). The sum is over all isomorphism types \((\tilde{F},\tilde{T}^{t})\) of tree-decomposed graphs. This sum is finite and thus well-defined as \(\mathrm{blsoHom}\left((F,T^{s}),\left(\tilde{F},\tilde{T}^{t}\right)\right)=0\) holds if \(\tilde{T}^{t}\) has depth unequal to \(2D\) or nodes with at least \((r+1)\cdot(|V(G)|-1)\) children.

Assume that for the root bag of \((F,T^{s})\) it holds \(\beta_{T^{s}}(s)=\{v\}\). Let \(x\in V(G)\) be any vertex in \(G\), and denote by \((F^{(D)}(x),T^{(D)}(x))\) the depth-\(2D\)\(r\)-\(\ell\)WL-unfolding tree at node \(x\). Define the following two sets,

\[S_{1}(x) =\left\{h\in\mathrm{BIso}((F,T^{s}),G)\mid h(v)=x\right\},\] \[S_{2}(x) =\left\{(\rho,\tau)\in\mathrm{BIsoHom}\left((F,T^{s}),\left(F^{(D )}(x),T^{(D)}(x)\right)\right)\mid\rho(v)=x\right\}.\]

We prove that \(|S_{1}(x)|=|S_{2}(x)|\) for every \(x\in V(G)\), which is equivalent to (11). For this, we show for any bag isomorphism \(h\) from \((F,T^{s})\) to \(G\) with \(h(v)=x\), there exists a _unique_ bag isomorphism homomorphism \(\sigma\) from \((F,T^{s})\) to \((F^{(D)}(x),T^{(D)}(x))\) with \(\sigma(v)=x\) such that \(h=\pi\circ\sigma\), where \(\pi\) is the bag isomorphism from \((F^{(D)}(x),T^{(D)}(x))\) to \(G\), defined in Definition 26 and Theorem 4, respectively. To visualize this proof idea, see Figure 12.

First, define \(\rho(v)\coloneqq x\). Let \(v_{1},\ldots,v_{n}\in V(F)\) be nodes that correspond to bags in \(T^{s}\) of depth \(2\) with one element inside the bag and their parents having two elements in their bag, i.e., \(\{v_{i}\}\) are the corresponding bags. Similarly, set \(x_{1},\ldots,x_{m}\in V(F^{(D)}(x))\) nodes that correspond to bags of depth \(2\) in \(T^{(D)}(x)\), with one element inside the bag and their parents having two elements in their bag. Since \(h\) is a bag isomorphism and \(\pi\) as well, for every \(i=1,\ldots,n\) there exists a \(j_{i}\) such that \(h(v_{i})=\tilde{x}_{j_{i}}=\pi(x_{j_{i}})\), where \(\tilde{x}_{j_{i}}\in V(G)\) and \(x_{j_{i}}\in V\left(F^{(D)}(x)\right)\). Since \(\pi\) and \(h\) are bag isomorphisms, we have

\[F[\{\{v,v_{i}\}\}]\cong G[\{\{x,\tilde{x}_{j_{i}}\}\}]\cong F^{(D)}(x)[\{\{x,x _{j_{i}}\}\}]. \tag{12}\]Now, set \(\rho(v_{i})=x_{j_{i}}\) for every \(i=1,\ldots,n\). Based on (12), we can easily define \(\tau\) such that \(\tau\) satisfies Definition 28 with respect to bags that are of depth \(1\) and \(2\).

For \(q=1,\ldots,r\) and \(l=1,\ldots,|\mathcal{N}_{q}(v)|\), let \(\mathbf{p}_{l}^{q}\) be a path of length \(2q\) starting from the root node \(s\) in \(T^{s}\). Every such path \(\mathbf{p}_{l}^{q}\) in \(T^{s}\) corresponds to unique path \(\mathbf{v}_{l}^{q}\), that is in \(\mathcal{N}_{q}(x)\), of length \(q\) in \(F\). We represent the path by \(\left\{v_{l,1}^{q},v_{l,2}^{q},\ldots,v_{l,q+1}^{q}\right\}\), where every consecutive node is connected to each other and for \(k=1,\ldots q+1\), we have \(\left\{v,v_{l,k}^{q}\right\}\in E(F)\) iff \(\left\{h(v),h(v_{l,k}^{q})\right\}\in E(G)\) as \(h\) is a bag isomorphism. Further for every node \(k=1,\ldots,q+1\) there exists a \(j_{l,k}^{q}\) such that \(h(v_{l,k}^{q})=\tilde{x}_{j_{l,k}^{q}}=\pi(x_{j_{l,k}^{q}})\), where \(\tilde{x}_{j_{l,k}^{q}}\in V(G)\), \(\left\{\tilde{x}_{j_{l,1}^{q}},\ldots,\tilde{x}_{j_{l,q+1}^{q}}\right\}\in \mathcal{N}_{q}(x)\) and \(\left\{x_{j_{l,1}^{q}},\ldots,x_{j_{l,q+1}^{q}}\right\}\in\mathcal{N}_{q}(x)\). We set \(\sigma(v_{l,k}^{q})=x_{j_{l,k}^{q}}\) for every \(k=1,\ldots,q+1\). Clearly, we have

\[F\left[\left\{\left\{v,v_{i},v_{i+1}\right\}\right\}\right]\cong G\left[\left\{ \left\{x,\tilde{x}_{j_{i}},\tilde{x}_{j_{i+1}}\right\}\right\}\right]\cong F^{ (D)}(x)\left[\left\{\left\{x,x_{j_{i}},x_{j_{i+1}}\right\}\right\}\right]. \tag{13}\]

Now, based on (13), we can easily define \(\tau\) such that \(\tau\) satisfies Definition 28 with respect to bags that correspond to paths in \(\mathcal{N}_{q}(v)\) for \(q=1,\ldots,r\). Now, following this construction recursively leads to a bag isomorphism \(\rho\) such that \(h=\pi\circ\rho\).

It remains to show that \((\rho,\tau)\) is unique (up to isomorphism). For this, let \((\rho_{1},\tau_{1})\) be another bag isomorphism between \((F,T^{s})\) and \((F^{(D)}(x),T^{(D)}(x))\) such that \(\rho_{1}(v)=x\) and \(h=\pi\circ\rho_{1}\). We show that \(\rho=\rho_{1}\).

We begin by showing that \(\rho(v)=\rho_{1}(v)\) for every \(v\) that is not in a cycle. Adopting the previous notations, consider \(v_{1},\ldots,v_{n}\in V(F)\) and \(x_{1},\ldots,x_{m}\in V(F^{(D)}(x))\). For each \(i=1,\ldots,n\), let \(k_{i}\) and \(l_{i}\) be the indices such that \(\rho(v_{i})=x_{k_{i}}\) and \(\rho_{1}(v_{i})=x_{l_{i}}\). Consequently, \(\pi(x_{k_{i}})=\pi(x_{l_{i}})\). We note that the image of \(h(v_{i})\) is not contained in a cycle in \(G\), as otherwise, \(h\) would not be a bag isomorphism. Similarly, \(x_{k_{i}}\) and \(x_{l_{i}}\) are not contained in a cycle; otherwise, \(\rho\) and \(\rho_{1}\) would not be bag isomorphisms. Now, \(\pi\) is an injective mapping if the domain is restricted to nodes that are of depth \(1\) and \(2\), and not contained in a cycle. Hence, \(x_{k_{i}}=x_{l_{i}}\).

We continue by showing that for every \(w\in V(F)\), that is contained in a cycle, we have \(\rho(w)=\rho_{1}(w)\). This follows a similar argument as the nodes that are not included in any cycle. We summarize the argument shortly: It must hold that \(\rho(w)\) and \(\rho_{1}(w)\) are contained in a cycle, and \(\pi(\rho(w))\) and \(\pi(\rho_{1}(w))\) as well. Now, \(\pi\) is injective if the domain is restricted to nodes that are only contained in cycles. Hence, \(\rho_{1}=\rho\). 

We continue this subsection by introducing the concept of a _bag extension_ in the context of tree-decomposed graphs. This definition formalizes the notion of one tree-decomposed graph being an extension of another.

**Definition 29** (Definition 20 in (Dell et al., 2018)).: _Let \((F,T^{t})\) be a tree-decomposed graph. A bag extension of \((F,T^{t})\) is a graph \((H,T^{t})\) with \(V(H)=V(F)\) such that for every \(t\in V\left(T^{t}\right)\) the induced subgraph \(H[\beta_{T^{t}}(t)]\) is an extension of \(F[\beta_{T^{t}}(T)]\), i.e., if \(e\in E\left(F[\beta_{T^{t}}(T)]\right)\), then \(e\in E\left(H[\beta_{T^{t}}(T)]\right)\). We define \(\mathrm{bExt}\left((F,T^{t}),(\tilde{F},\tilde{T}^{s})\right)\) as the number of bag extensions of \((F,T^{t})\) that are isomorphic to \((\tilde{F},\tilde{T}^{s})\)._

Intuitively, a bag extension of a tree-decomposed graph \((F,T^{s})\) can be achieved by adding an arbitrary number of edges to \(F\). Each added edge must be contained within a bag that corresponds to a node in the tree \(T^{s}\).

**Definition 30** (Definition C.28 in (B. Zhang et al., 2024)).: _Given a tree-decomposed graph \((F,T^{r})\) and a graph \(G\), a bag-strong homomorphism from \((F,T^{s})\) to \(G\) is a homomorphism \(f\) from \(F\) to \(G\) such that, for all \(t\in V(T^{r})\), \(f\) is a strong homomorphism from \(F[\beta_{T^{s}}(t)]\) to \(G[f(\beta_{T^{s}}(t))]\), i.e., \(\left\{u,v\right\}\in E\left(F[\beta_{T^{s}}(t)]\right)\) iff \(\left\{f(u),f(v)\right\}\in E\left(G[f(\beta_{T^{s}}(t))]\right)\). Denote \(\mathrm{BStrHom}((F,T^{s}),G)\) to be the set of all bag-strong homomorphisms from \((F,T^{s})\) to \(G\), and denote \(\mathrm{bStrHom}((F,T^{s}),G)=|\mathrm{BStrHom}((F,T^{s}),G)|\)._

We continue with decomposing the number of homomorphism from a fan cactus graph to any graph.

**Lemma 7**.: _Let \(r\in\mathbb{N}\). For any tree-decomposed graph \((F,T^{s})\in\mathcal{M}^{r+2}\) and any graph \(G\), it holds_

\[\mathrm{hom}\left(F,G\right)=\sum_{(\tilde{F},\tilde{T}^{t})\in\mathcal{M}^{r+2 }}\mathrm{bExt}\left(\left(F,T^{s}\right),\left(\tilde{F},\tilde{T}^{t}\right) \right)\cdot\mathrm{bStrHom}\left(\left(\tilde{F},\tilde{T}^{t}\right),G \right). \tag{14}\]Proof.: The proof follows the lines of Lemma C.29. in (B. Zhang et al., 2024). First, (14) is well-defined as \(T^{s}\) is finite, hence, there can only be finitely many bag extensions of \((F,T^{s})\).

Further, consider the set

\[S=\left\{\left(\left(\left(\tilde{F},\tilde{T}^{t}\right),(\rho, \tau)\,,g\right)\;|\;\left(\tilde{F},\tilde{T}^{t}\right)\in\mathcal{M}^{r+2} \,,(\rho,\tau)\in\mathrm{BExt}\left((F,T^{s})\,,\left(\tilde{F},\tilde{T}^{t} \right)\right)\;,\right.\right.\] \[\left.\left.g\in\mathrm{BstrHom}\left(\left(\tilde{F},\tilde{T}^{t }\right),G\right)\right\}.\right.\]

We consider the mapping \(\sigma\) from \(S\) to \(\hom(F,G)\) via \(\left((\rho,\tau)\,,g\right)\mapsto g\circ\rho\). We show that for every homomorphism \(h\) there exists a unique, up to automorphisms, \(\left(\tilde{F},\tilde{T}^{t}\right)\in\mathcal{M}^{r+2},(\rho,\tau)\) and \(g\) such that \(h=g\circ\rho\).

We begin with the existence part. For \(h\in\hom(F,G)\), we define \(\left(\tilde{F},\tilde{T}^{t}\right)\in\mathcal{M}^{r+2},(\rho,\tau)\) and \(g\) as follows.

* We define \(\tilde{F}\) by adding the edges given by \[\left\{\{u,v\}\;|\;u,v\in V(F),\exists t\in T^{s}\;\text{s.t.}\;\left\{u,v \right\}\in\beta_{T^{s}}(t),\left\{h(u),h(v)\right\}\in E(G)\right\}.\] (15)

Figure 12: Visualization of proof idea of Lemma 6

We define \(\tilde{T}^{t}:=T^{s}\). Clearly, \(\left(\tilde{F},\tilde{T}^{t}\right)\in\mathcal{M}^{r+2}\) and it is a bag extension as only edges are added that are contained within a bag that corresponds to a node in \(T^{s}\).
* We define \(\rho\) and \(\tau\) as the identity mappings on their respective domain, leading to \((\rho,\tau)\in\mathrm{BExt}\left(\left(F,T^{s}\right),\left(\tilde{F},\tilde{T }^{t}\right)\right)\).
* We define \(g=h\). For \(x\in\tilde{T}^{t}\), we show that \(g\) is a strong homomorphism from \(\tilde{F}[\beta_{\tilde{T}^{t}}(x)]\) to \(G[g\left(\beta_{\tilde{T}^{t}}(x)\right)]\). Let \(\{u,v\}\in E\left(\tilde{F}[\beta_{\tilde{T}^{t}}(x)]\right)\), then \(\{g(u),g(v)\}\in E\left(G[g\left(\beta_{\tilde{T}^{t}}(x)\right)]\right)\) as \(h\) is a homomorphism with respect to the edges \(E(F)\) and in (15) only edge \(\{u,v\}\) were added that satisfy \(\{h(u),h(v)\}\in E(G)\). On the other hand \(\{g(u),g(v)\}\in\tilde{E}\left(G[g\left(\beta_{\tilde{T}^{t}}(x)\right)]\right)\), but \(\{u,v\}\not\in E\left(\tilde{F}[\beta_{\tilde{T}^{t}}(x)]\right)\) would contradict (15) as \(u,v\) are contained in the same bag \(\beta_{\tilde{T}^{t}}(x)\). Hence, \(g\in\mathrm{BstrHom}\left(\left(\tilde{F},\tilde{T}^{t}\right),G\right)\).

We finally prove the uniqueness part, i.e., that \(\sigma\left(\left(\tilde{F}_{1},\tilde{T}^{t_{1}}_{1}\right),(\rho_{1},\tau_{1 }),g_{1}\right)=h\) implies that there exists an isomorphism \((\tilde{\rho},\tilde{\tau})\) from \(\left(\tilde{F}_{1},\tilde{T}^{t_{1}}_{1}\right)\) to \(\left(\tilde{F},\tilde{T}^{t}\right)\) such that \(\tilde{\rho}\circ\rho_{1}=\rho\), \(\tilde{\tau}\circ\tau_{1}=\tau\). We first prove that \(\tilde{F}_{1}\cong\tilde{F}\) and \(\tilde{T}^{t_{1}}_{1}\cong\tilde{T}^{t}\).

1. For any \(u,v\in V(F)\), we obviously have \(\rho(u)=\rho(v)\) iff \(u=v\) iff \(\rho_{1}(u)=\rho_{1}(v)\) as \(\rho\) and \(\rho_{1}\) are injective mappings.
2. Let \(u,v\in V(F)\). Consider \(\{\rho_{1}(u),\rho_{1}(v)\}\in E(\tilde{F}_{1})\), we show that \(\{\rho(u),\rho(v)\}\in E(\tilde{F})\). If \(\{u,v\}\in E(F)\), then clearly \(\{\rho(u),\rho(v)\}\in E(\tilde{F})\) as \(\rho\) is a homomorphism. Hence, assume that \(\{u,v\}\not\in E(F)\). Then, \(u,v\) must be contained in the same bag of \(T^{s}\) as \(\rho_{1}\) is a bag extension and only node pairs are added if they are in the same bag. Hence, \(\rho(u)\) and \(\rho(v)\) are contained in the same bag. As \(g_{1}\) is a homomorphism, we have \(\{g_{1}(\rho_{1}(u)),g_{1}(\rho_{1}(v))\}\in E(G)\). But, then also \(\{g(\rho(u)),g(\rho(v))\}\in E(G)\), and as \(g\) is a strong homomorphism (with respect to the bag in which \(\rho(u)\) and \(\rho(v)\) are contained), we have \(\{\rho(u),\rho(v)\}\in E(\tilde{F})\). By symmetry of the argument, we have \(\{\rho_{1}(u),\rho_{1}(v)\}\in E(\tilde{F}_{1})\) iff \(\{\rho(u),\rho(v)\}\in E(\tilde{F})\).
3. Since \(\rho_{1}\) and \(\rho\) are bag extension, they are bijective on their respective domain. Hence, \(\tilde{\rho}=\rho\circ\rho_{1}^{-1}\) defines an isomorphism from \(\tilde{F}_{1}\) to \(\tilde{F}\). On the other hand, \(\tilde{T}^{t_{1}}_{1}\cong\tilde{T}^{t}\) trivially holds, again with \(\tilde{\tau}=\tau\circ\tau_{1}^{-1}\).

We have \(\tilde{\rho}\circ\rho_{1}=\rho\), \(\tilde{\tau}\circ\tau_{1}=\tau\). We show that the tuple \((\tilde{\rho},\tilde{\tau})\) is an isomorphism, i.e., it remains to show that for any \(b\in\tilde{T}^{t_{1}}_{1}\), we have \(\tilde{\rho}(\beta_{\tilde{T}^{t_{1}}_{1}}(b))=\beta_{\tilde{T}^{t}}(\tilde{ \tau}(b))\). Since \(\tau_{1}\) is surjective, we can choose \(a\) such that \(\tau_{1}(a)=b\). Then,

\[\tilde{\rho}(\beta_{\tilde{T}^{t_{1}}_{1}}(\tau_{1}(a)))=\tilde{\rho}(\rho_{1}( \beta_{T^{s}}(a)))=\rho(\beta_{T^{s}}(a))=\beta_{\tilde{T}^{s}}(\tau(a))=\beta _{\tilde{T}^{t}}(\tilde{\tau}\circ\tau_{1}(a))=\beta_{\tilde{T}^{t}}(\tilde{ \tau}(b)).\]

The first and third equalities hold since \((\rho_{1},\tau_{1})\) and \((\rho,\tau)\) are bag extensions. 

**Definition 31** (Definition 30 in (B. Zhang et al., 2024)).: _Given two tree-decomposed graphs \((F,T^{s})\) and \((\tilde{F},\tilde{T}^{t})\), a homomorphism \((\rho,\tau)\) from \((F,T^{s})\) to \((\tilde{F},\tilde{T}^{t})\) is called bag-strong surjective if \(\rho\) is a bag-strong homomorphism from \((F,T^{s})\) to \(\tilde{F}\) and is surjective on both vertices and edges, and \(\tau\) is an isomorphism from \(T^{s}\) to \(\tilde{T}^{t}\) such that for all \(x\in V(T^{s})\), we have \(\rho(\beta_{T^{s}}(x))=\beta_{\tilde{T}^{t}}(\tau(x))\). Denote \(\mathrm{BStrSurj}((F,T^{s}),(\tilde{F},\tilde{T}^{t}))\) to be the set of all bag-strong subjective homomorphisms from \((F,T^{s})\) to \((\tilde{F},\tilde{T}^{t})\), and denote \(\mathrm{bStrSurj}((F,T^{s}),(\tilde{F},\tilde{T}^{t}))=|\mathrm{BStrSurj}((F,T ^{s}),(\tilde{F},\tilde{T}^{t}))|\)._

**Lemma 8**.: _Let \(r\in\mathbb{N}\). For any tree-decomposed graph \((F,T^{s})\in\mathcal{M}^{r+2}\) and any graph \(G\), it holds_

\[\mathrm{bStrHom}\left(\left(F,T^{s}\right),G\right)=\sum_{(\tilde{F},\tilde{T}^{ t})\in\mathcal{M}^{r+2}}\mathrm{bStrSurj}\left(\left(F,T^{s}\right),\left( \tilde{F},\tilde{T}^{t}\right)\right)\frac{\mathrm{bIso}\left(\left(\tilde{F}, \tilde{T}^{t}\right),G\right)}{\mathrm{aut}\left(\tilde{F},\tilde{T}^{t}\right)}, \tag{16}\]

_where \(\mathrm{aut}(\tilde{F},\tilde{T}^{t})\) counts the number of automorphisms of \((\tilde{F},\tilde{T}^{t})\)._Proof.: The proof follows the lines of Lemma C.31. in (B. Zhang et al., 2024).

Consider the set

\[S=\left\{\left(\left(\tilde{F},\tilde{T}^{t}\right),\left(\rho, \tau\right),g\right)\mid\,\left(\tilde{F},\tilde{T}^{t}\right)\in\mathcal{M}^{ +2}\,,\left(\rho,\tau\right)\in\mathrm{BStrSurj}\left(\left(F,T^{s}\right), \left(\tilde{F},\tilde{T}^{t}\right)\right)\,\right.\] \[\left.g\in\mathrm{BIso}\left(\left(\tilde{F},\tilde{T}^{t}\right),G \right)\right\}.\]

We consider the mapping \(\sigma\) from \(S\) to \(\mathrm{BStrHom}\left(\left(F,T^{s}\right),G\right)\) via \(\left(\left(\rho,\tau\right),g\right)\mapsto g\circ\rho\). We show that for every bag-strong homomorphism \(h\) there exists a unique, up to automorphisms, \(\left(\tilde{F},\tilde{T}^{t}\right)\in\mathcal{M}^{r+2}\), bag-strong surjective homomorphism \(\left(\rho,\tau\right)\) and \(g\) such that \(h=g\circ\rho\).

We begin with the existence part. For \(h\in\mathrm{BStrHom}\left(\left(F,T^{s}\right),G\right)\), we define \(\left(\tilde{F},\tilde{T}^{t}\right)\in\mathcal{M}^{r+2},\left(\rho,\tau\right)\) and \(g\) as follows.

We define \(\tilde{F}\) by defining an equivalence relation \(\sim\) on \(V(F)\): \(u\sim v\) if \(h(u)=h(v)\) and there exists a path \(P\) in \(T^{s}\) with endpoints \(t_{1},t_{2}\in V(T^{s})\) such that \(u\in\beta_{T^{s}}(t_{1}),v\in\beta_{T^{s}}(t_{2})\), and all nodes \(t\) on the path \(P\) satisfies that \(h(u)=h(v)\in h(\beta_{T^{s}}(t))\). We then define \(\rho\) as the quotient map with respect to \(\sim\) and set \(\tilde{F}=F/\sim\), i.e.,

\[V(\tilde{F})=\left\{\rho(u)\mid u\in V(F)\right\},E(\tilde{F})=\left\{\left\{ \rho(u),\rho(v)\right\}\mid\left\{u,v\right\}\in E(F)\right\},\]

which is well-defined as \(\left\{u,v\right\}\in E(F)\) imples \(\rho(u)\neq\rho(v)\) since \(h\) is a homomorphism. Then, \(\rho\) is surjective per construction.

We define the mapping \(g:V(\tilde{F})\to V(G)\) such that \(g(\rho(u))=h(u)\) for all \(u\in V(F)\). This mapping \(g\) is well-defined since \(\rho(u)=\rho(v)\) implies \(h(u)=h(v)\), and \(\rho:V(F)\to V(\tilde{F})\) is surjective. This leads to the equality \(h=g\circ\rho\). To demonstrate that \(g\) is a homomorphism, consider any edge \((x,y)\in E(\tilde{F})\). There exists an edge \((u,v)\in E(F)\) such that \(\rho(u)=x\) and \(\rho(v)=y\), which implies \((h(u),h(v))\in E(G)\), since \(h\) is a homomorphism. Consequently, this means \((g(x),g(y))\in E(G)\).

We continue by defining the tree \(\tilde{T}^{t}:=(V(T),E(T),\beta_{\tilde{T}^{t}})\). We set \(t=s\), and define \(\tau\) to be the identity. Furthermore, we have \(\beta_{\tilde{T}^{t}}(x)=\rho(\beta_{T^{s}}(x))\) for all \(x\in V(T)\). It remains to prove that \((\tilde{F},\tilde{T}^{t})\in\mathcal{M}^{r+2}\) is a valid tree decomposition. For this, it suffices to prove that for any vertex \(x\in V(\tilde{F})\) the subgraph \(B_{\tilde{T}^{t}}(x)\) is connected. For this, let \(x\in V(\tilde{F})\) and \(t_{1},t_{2}\in B_{\tilde{T}^{s}}(x)\). Then, there exists \(u\in\beta_{T^{s}}(t_{1}),v\in\beta_{T^{s}}(t_{2})\) such that \(\rho(u)=x,\rho(v)=x\). Therefore, \(u\sim v\). As such, there exists a path \(P\in T^{s}\) such that all nodes \(b\) on \(P\) satisfy \(h(u)\in h(\beta_{T^{s}}(b))\). Hence, for every \(b\in P\) there exists some \(w_{b}\in\beta_{T^{s}}(b)\) such that \(h(w_{b})=h(u)\), and consequently \(w_{b}\sim u\). Finally, \(x=\rho(u)=\rho(w_{b})\in\rho(\beta_{T^{s}}(b))=\beta_{T^{s}(b)}\) for all \(b\) in the path \(P\). Hence, \(\left(\tilde{F},\tilde{T}^{t}\right)\in\mathcal{M}^{r+2}\).

It remains to prove that \(\rho\) is a bag-strong surjective homomorphism and \(g\) is a bag isomorphism. We begin by showing that \(\rho\) is a bag-strong surjective homomorphism. For this, let \(t\in V(T^{s})\) and \(u,v\in\beta_{T^{s}}(t)\). If \(\left\{u,v\right\}\not\in E(F)\), then \(\left\{h(u),h(v)\right\}\not\in E(G)\) (since \(h\) is a bag-strong homomorphism). Therefore, \(\left\{\rho(u),\rho(v)\right\}\not\in E(\tilde{F})\) since \(g\) is a homomorphism. Hence, \(\rho\) is a bag-strong surjective homomorphism.

We show that \(g\) is a bag isomorphism. Let \(x\in V(\tilde{T}^{t})\), and consider \(\tilde{u},\tilde{v}\in\beta_{\tilde{T}^{t}}(x)\). Since \(\rho\) is surjective, there exist \(u,v\in\beta_{T^{s}}(x)\) such that \(\rho(u)=\tilde{u}\) and \(\rho(v)=\tilde{v}\). We have \(\left\{\rho(u),\rho(v)\right\}\not\in E(\tilde{F})\) iff \(\left\{h(u),h(v)\right\}\not\in E(G)\), since both \(\rho\) and \(h\) are bag-strong homomorphisms. Therefore, g is a bag isomorphism.

We finally prove that \(\sigma\left(\left(\tilde{F}_{1},\tilde{T}^{t_{1}}\right),\left(\rho_{1},\tau_{1 }\right),g_{1}\right)=\sigma\left(\left(\tilde{F},\tilde{T}^{t}\right),(\rho,\tau),g\right)\) implies there exists an isomorphism \((\tilde{\rho},\tilde{\tau})\) from \((\tilde{F}_{1},\tilde{T}_{1}^{t_{1}})\) to \((\tilde{F},\tilde{T}^{t})\) such that \(\tilde{\rho}\circ\rho_{1}=\rho,\tilde{\tau}\circ\tau_{1}=\tau,g_{1}=g\circ \tilde{\rho}\). Let \(h=g_{1}\circ\rho_{1}=g\circ\rho\). We will only show that \(\tilde{F}_{1}\cong\tilde{F}\) since the remaining procedure is almost the same as in previous proofs. It suffices to prove that, for all \(u,v\in V(F)\), \(\rho_{1}(u)=\rho_{1}(v)\) iff

* \(h(u)=h(v)\), and
* There exists a path \(P\) in \(T^{s}\) with endpoints \(t_{1},t_{2}\in V(T)\) such that \(u\in\beta_{T^{s}}(t_{1}),v\in\beta_{T^{s}}(t_{2})\), and all node \(x\) on path \(P\) satisfies that \(h(u)\in h(\beta_{T^{s}}(x))\).

We begin by showing the first direction, i.e., \(\rho_{1}(u)=\rho_{1}(v)\) implies items a) and b). If \(\rho_{1}(u)=\rho_{1}(v)\), we clearly have \(h(u)=h(v)\) as \(g_{1}\) is well-defined. Also, there exists \(x_{1}\in B_{T^{*}}(u),x_{2}\in B_{T^{*}}(v)\), i.e., \(u\in\beta_{T^{*}}(x_{1})\) and \(v\in\beta_{T^{*}}(x_{2})\). Hence, \(\rho(u)\in\rho(\beta_{T^{*}}(x_{1}))\subset\beta_{\hat{T}_{1}^{t_{1}}}(\tau_{ 1}(x_{1}))\) and \(\rho_{1}(u)=\rho_{1}(v)\in\rho_{1}(\beta_{T^{*}}(x_{2}))\subset\beta_{\hat{T} _{1}^{t_{1}}}(\tau_{1}(x_{2}))\) since \((\rho_{1},\tau_{1})\) is a homomorphism. Hence, \(\tau_{1}(x_{1}),\tau_{1}(x_{2})\in B_{\hat{T}_{1}^{t_{1}}}(\rho_{1}(u))\). Since \(\hat{T}_{1}^{t_{1}}[B_{\hat{T}_{1}^{t_{1}}}(\rho_{1}(u))]\) is connected, there is a path \(P\) in \(\hat{T}_{1}^{t_{1}}[B_{\hat{T}_{1}^{t_{1}}}(\rho_{1}(u))]\) with endpoints \(\tau_{1}(x_{1}),\tau_{1}(x_{2})\) such that all nodes \(x\) on \(P\) satisfies \(\rho_{1}(u)\in\beta_{\hat{T}_{1}^{t_{1}}}(x)=\beta_{\hat{T}_{1}^{t_{1}}}(\tau \circ\tau^{-1}(x))=\rho_{1}\left(\beta_{T^{*}}(\tau_{1}^{-1}(x))\right)\). We conclude \(h(u)=g_{1}(\rho_{1}(u))\in g_{1}(\rho_{1}(\rho_{T^{*}}(\tau_{1}^{-1}(x))))=h( \beta_{T^{*}}(\tau_{1}^{-1}(x)))\).

We continue by showing the second direction, i.e., \(\rho_{1}(u)=\rho_{1}(v)\) if items a) and b). We prove this by contradiction, i.e., assume \(\rho_{1}(u)\neq\rho_{1}(v)\) but the above items (a) and (b) hold. We consider two cases.First, assume that \(u\) and \(v\) are in the same bag of \(T^{*}\). Then, as \((\rho_{1},\tau_{1})\) is a homomorphism, the nodes \(\rho_{1}(u)\) and \(\rho_{1}(v)\) are in the same bag of \(\hat{T}_{1}^{t_{1}}\). Since \(g_{1}\) is a bag isomorphism, we have \(g_{1}(\rho_{1}(u))\neq g_{1}(\rho_{1}(v))\). This contradicts Item (a) above.

Now, consider the second case. For this, assume that \(u\) and \(v\) are not in the same bag of \(T^{*}\). Then, there exist two adjacent nodes \(x_{1},x_{2}\) on path \(P\) such that \(u\in\beta_{T^{*}}(x_{1}),u\not\in\beta_{T^{*}}(x_{2})\). We have \(\beta_{T^{*}}(x_{2})\subset\beta_{T^{*}}(x_{1})\) as for every pair of nodes \(t_{1},t_{2}\) in a canonical tree decomposition with \(\{t_{1},t_{2}\}\in E(T^{*})\) we have either \(\beta_{T^{*}}(t_{1})\subset\beta_{T^{*}}(t_{2})\) or \(\beta_{T^{*}}(t_{2})\subset\beta_{T^{*}}(t_{1})\). Now, item (b) implies that there exists \(w\in\beta_{T^{*}}(x_{2})\) such that \(w\neq u\) and \(h(w)=h(u)\). Then, \(\rho_{1}(w)\in\rho_{1}\left(\beta_{T^{*}}(x_{2})\right)\subset\rho_{1}\left( \beta_{T^{*}}(x_{1})\right)\subset\beta_{\hat{T}_{1}^{t_{1}}}(\tau_{1}(x_{1}))\). Therefore, \(\rho_{1}(u)\) and \(\rho_{1}(w)\) are two different nodes in \(\beta_{\hat{T}_{1}^{t_{1}}}\left(\tau_{1}(x_{1})\right)\) with \(g_{1}(\rho_{1}(u))=h(u)=h(w)=g_{1}(\rho_{1}(w))\). This contradicts the condition that \(g_{1}\) is a bag isomorphism. This yields the desired result that \(\tilde{F}\cong\tilde{F}_{1}\). 

Finally, we restate Theorem 2, with its proof now being a straightforward corollary of the preceding results in this section.

**Theorem 2**.: _Let \(r\geq 0\). Then, \(r\)-\(\ell\)WL can homomorphism-count \(\mathcal{M}^{r+2}\)._

Proof.: According to Corollary 3, if \(c_{r}(G)=c_{r}(H)\), then \(\operatorname{cnt}(F,G)=\operatorname{cnt}(F,H)\) for every \(F\in\mathcal{M}^{r+2}\). Utilizing Lemma 6, we extend this result to bag isomorphism counts: \(\operatorname{blso}(F,G)=\operatorname{blso}(F,H)\) holds for every \(F\in\mathcal{M}^{r+2}\). Finally, invoking Lemma 7 and Lemma 8, we conclude that \(\hom(F,G)=\hom(F,H)\) for all \(F\in\mathcal{M}^{r+2}\). 

## Appendix H Implications of Theorem 2

In this section, we discuss important implications of Theorem 2 and provide proofs for the results in Corollary 2.

### Appendix on \(\mathcal{F}\)-Hom-GNNs and Proof of Corollary 2 i)

Recent work in the domain of MPNNs has explored enhancing the initial node features by incorporating homomorphism counts (Barcelo et al., 2021). We summarize this approach in this section and compare it to our \(r\)-\(\ell\)WL algorithm.

Define \(\mathcal{F}=\{P_{1}^{s},\ldots,P_{l}^{s}\}\) as a collection of rooted graphs, termed as patterns. In \(\mathcal{F}\)-Hom-MPNNs, the initial feature vector of a vertex \(v\) in a graph \(G\) combines a one-hot encoding of the label \(\chi_{G}(v)\) with homomorphism counts corresponding to each pattern in \(\mathcal{F}\). The feature vector for each vertex \(v\) is recursively defined over rounds of message passing as follows:

\[\begin{split}\mathbf{x}_{\mathcal{F},G,v}^{(0)}&=( \chi_{G}(v),\hom(P_{1}^{s},G^{v}),\ldots,\hom(P_{l}^{s},G^{v}))\\ \mathbf{x}_{\mathcal{F},G,v}^{(t+1)}&=g^{(t+1)} \left(\mathbf{x}_{\mathcal{F},G,v}^{(t)},f^{(t+1)}\left(\mathbf{x}_{\mathcal{ F},G,u}^{(t)}\mid u\in N_{G}(v)\right)\right)\end{split} \tag{17}\]

Here, \(g^{(t)}\) and \(f^{(t)}\) represent the update and aggregation functions at depth \(t\), respectively.

[MISSING_PAGE_FAIL:41]

algorithm, i.e.,

\[\begin{split}\mathbf{x}^{(0)}_{\mathrm{Sub},G^{u}}(v)&= \left(\chi_{G}(v),\,\mathbb{1}_{v=u}(v)\right)\\ \mathbf{x}^{(t+1)}_{\mathrm{Sub},G^{u}}(v)&=g^{(t+1)} \left(\mathbf{x}^{(t)}_{\mathrm{Sub},G^{u}}(v),f^{(t+1)}\left(\mathbf{x}^{(t)}_ {\mathrm{Sub},G^{u}}(w)\mid w\in N_{G}(v)\right)\right).\end{split} \tag{18}\]

Here, \(g^{(t)}\) and \(f^{(t)}\) represent the update and aggregation functions at depth \(t\), respectively. The final node representations after \(t\) rounds are then calculated by

\[\mathbf{x}^{t}_{\mathrm{Sub},G}(u)=h\left(\mathbf{x}^{(t)}_{\mathrm{Sub},G^{u} }(v)\mid v\in V(G)\right).\]

#### h.2.1 Expressivity of Subgraph GNNs and Comparison with \(r\)-\(\ell\)Wl

The expressivity of subgraph GNNs is fully characterized by the class

\[\mathcal{F}^{\mathrm{sub}}:=\left\{F\mid\exists u\in V(F)\text{ s.t. }F \setminus\{u\}\text{ is a forest}\right\},\]

i.e., Subgraph GNNs can separate a pair of graphs \(G,H\) if and only if \(\hom(\mathcal{F}^{\mathrm{sub}},G)\neq\hom(\mathcal{F}^{\mathrm{sub}},H)\). Furthermore, the set \(\mathcal{F}^{\mathrm{sub}}\) is the maximal set that satisfies this property (B. Zhang et al., 2024, Theorem 3.4). We restate Corollary 2 ii) and provide a proof.

**Corollary 5**.: \(1\)_-\(\ell\)WL is not less powerful than Subgraph GNNs. In particular, any \(r\)-\(\ell\)WL can separate infinitely many graphs that Subgraph GNNs fail to distinguish._

Proof.: We show that already \(1\)-\(\ell\)WL can separate infinitely many graphs that Subgraph GNNs fail to distinguish. The other statements then follow as a simple corollary of Proposition 1.

For clarity, we begin by demonstrating that there exists a pair of graphs that \(1\)-\(\ell\)GIN can separate, but Subgraph GNNs cannot distinguish. Consider the graph \(F\) defined as follows: \(F=\left\{\begin{smallmatrix}\circledR\circledR\circledR\circledR\circledR \circledR\circledR\circledR\circledR\circledR\circledR\circledR\circledR \circledR\circledR\circledR\circledR\circledR\circledR\circledR\circledR \circledR\circledR\circledR\circledR\circledR\circledR\circledR\cir\circledR \cir\circledR\circledR\circledR\circledR\cir\circledR\c

**Theorem 7** (B. Zhang et al., 2024).: _The homomorphism-expressivity of Subgraph \(k\)-GNN is given by \(\mathcal{F}^{\mathrm{sub}(k)}=\{F:\exists U\subset V_{F}\text{ s.t. }|U|\leq k\text{ and }F\setminus U\text{ is a forest }\}\)._

Since the set \(\mathcal{F}^{\mathrm{sub}(k)}\) is the _maximal set_ of graphs that Subgraph \(k\)-GNNs can homomorphism-count, we can derive the following corollary, which restates Corollary 2 iii) and provides a proof.

**Corollary 6**.: _For any \(k\geq 1\), \(1\)-WL is not less powerful than Subgraph \(k\)-GNNs. In particular, any \(r\)-\(\ell\)WL can separate infinitely many graphs that Subgraph \(k\)-GNNs fail to distinguish._

Proof.: The proof parallels the proof of Corollary 2. We present it here for completeness.

Let \(k\geq 1\). We will show that there exists a pair of graphs that \(1\)-\(\ell\)GIN can distinguish, but Subgraph \(k\)-GNNs cannot. Consider the graph \(F\), defined as the unique graph with \(k\) triangle graphs, all connected by an edge to a single node.

It holds that \(F\in\mathcal{M}^{3}\setminus\mathcal{F}^{\mathrm{sub}(k)}\), where \(\mathcal{F}^{\mathrm{sub}(k)}\) is the maximal set that Subgraph \(k\)-GNNs can homomorphism-count. Then, by (B. Zhang et al., 2024, Theorem 3.8), there exists a pair of graphs \(G(F)\) and \(H(F)\) such that \(\hom(F,G(F))\neq\hom(F,H(F))\) and \(\hom(\mathcal{F}^{\mathrm{sub}(k)},G(F))=\hom(\mathcal{F}^{\mathrm{sub}(k)},H( F))\). Hence, Subgraph \(k\)-GNNs cannot separate \(G(F)\) and \(H(F)\). Since \(F\in\mathcal{M}^{3}\), by \(\hom(F,G(F))\neq\hom(F,H(F))\) and Theorem 2, \(1\)-\(\ell\)WL can separate \(G(F)\) and \(H(F)\).

This argument can be repeated for every \(F\in\mathcal{M}^{3}\setminus\mathcal{F}^{\mathrm{sub}(k)}\). Since there are infinitely many non-isomorphic graphs in \(\mathcal{M}^{3}\setminus\mathcal{F}^{\mathrm{sub}(k)}\), the corollary follows. 

### Proof of Corollary 2 iv)

Proof of Corollary 2 iv).: Given graphs \(F\) and \(G\), it is well-known (see, e.g., (Neuen, 2024; Curticapean et al., 2017)) that \(\mathrm{sub}(F,G)\) can be decomposed as:

\[\mathrm{sub}(F,G)=\sum_{F^{\prime}\in\mathrm{spasm}(F)/\sim}\alpha(F^{\prime}) \mathrm{hom}(F^{\prime},G). \tag{20}\]

Here, the sum ranges over all non-isomorphic graphs in \(\mathrm{spasm}(F)\). The sum in (20) is finite since the homomorphic image of \(F\) has at most \(|V(F)|\) nodes. Per assumption, we have \(\mathrm{spasm}(F)\subset\mathcal{M}^{r+2}\), i.e., by Theorem 2, \(r\)-\(\ell\)WL can homomorphism-count \(\mathrm{spasm}(F)\). In particular, if \(r\)-\(\ell\)WL cannot separate two graphs \(G\) and \(H\), we have \(\hom(\mathrm{spasm}(F),G)=\hom(\mathrm{spasm}(F),H)\), and hence, \(\mathrm{sub}(F,G)=\mathrm{sub}(F,H)\).

The result on subgraph-counting paths follows directly as the homomorphic image of a path \(P_{r+3}\) of length \(r+3\) lies in \(\mathcal{M}^{r}\). 

## Appendix I Appendix for Section 6

**Theorem 3**.: _For fixed \(t,r\geq 0\), \(t\) iterations of \(r\)-\(\ell\)WL are more powerful than \(r\)-\(\ell\)MPNN with \(t\) layers. Conversely, \(r\)-\(\ell\)MPNN is more powerful than \(r\)-\(\ell\)WL if the functions \(f^{(t)},g^{(t)}\) in (3) are injective._

Proof of Theorem 3.: We begin by proving that \(c_{r}^{(t)}\sqsubseteq h_{r}^{(t)}\). We argue by induction over \(t\) for any fixed \(r\geq 0\).

Initially, \(c_{r}^{(0)}=h_{r}^{(0)}\) as both labeling functions start with the same base labels. Now assume \(c_{r}^{(t+1)}(u)=c_{r}^{(t+1)}(v)\) for some \(u,v\in V(G)\). By definition,

\[\mathrm{HASH}\left(c_{r}^{(t)}(u),\left\{\left\{c_{r}^{(t)}(\mathbf{p})\left| \mathbf{p}\in\mathcal{N}_{0}(u)\right.\right\}\right\},\ldots\right)=\mathrm{ HASH}\left(c_{r}^{(t)}(v),\left\{\left\{c_{r}^{(t)}(\mathbf{p})\left|\mathbf{p}\in \mathcal{N}_{0}(v)\right.\right\}\right\},\ldots\right).\]

This implies \(c_{r}^{(t)}(u)=c_{r}^{(t)}(v)\) and

\[\left\{\left\{c_{r}^{(t)}(\mathbf{p})\left|\mathbf{p}\in\mathcal{N}_{k}(u) \right.\right\}\right\}=\left\{\left\{c_{r}^{(t)}(\mathbf{p})\left|\mathbf{p} \in\mathcal{N}_{k}(v)\right.\right\}\right\},\quad\forall k\in\left\{0,\ldots,r \right\},\]

as \(\mathrm{HASH}\) is an injective function.

By induction hypothesis, we hence have \(h_{r}^{(t)}(u)=h_{r}^{(t)}(v)\) and

\[\left\{\left\{h_{r}^{(t)}(\mathbf{p})\left|\mathbf{p}\in\mathcal{N}_{k}(u)\right. \right\}\right\}=\left\{\left\{h_{r}^{(t)}(\mathbf{p})\left|\mathbf{p}\in \mathcal{N}_{k}(v)\right.\right\}\right\},\quad\forall k\in\left\{0,\ldots,r \right\},\]

which implies that any function, in particular \(f_{k}^{(t+1)}\) and \(g^{(t+1)}\) have to return the same result. Therefore, we have \(h_{r}^{(t+1)}(u)=h_{r}^{(t+1)}(v)\).

We proceed to prove \(h_{r}^{(t)}\sqsubseteq c_{r}^{(t)}\) if all message, update, and readout functions are injective in Definition 9. For this, we show that for each \(t\geq 0\) there exists an injective function \(\phi\) such that \(h_{r}^{(t)}=\phi\circ c_{r}^{(t)}\). For \(t=0\), we can choose \(\phi\) to be the identity function. Assume that for \(t-1\) there exists an injective function \(\phi\) such that \(h_{r}^{(t-1)}(v)=\phi\circ c_{r}^{(t-1)}(v)\). Then, we can write

\[h_{r}^{(t)}(v) =g^{(t)}\left(h^{(t-1)}(v),m_{0}^{(t)}(v),\ldots,m_{r}^{(t)}(v)\right)\] \[=g^{(t)}\left(\phi\circ c_{r}^{(t-1)}(v),\phi\circ m_{0}^{(t)}(v), \ldots,\phi\circ m_{r}^{(t)}(v)\right),\]

where for every \(q=0,\ldots,r\), we set \(\phi\circ m_{q}^{(t)}(v):=\left\{\left\{\left(\phi\circ c_{r}^{(t-1)}(\mathbf{ p})\mid\mathbf{p}\in\mathcal{N}_{q}(v)\right.\right\}\right\}\) and \((\phi\circ c_{r}^{(t-1)}(\mathbf{p}))=(\phi\circ c_{r}^{(t-1)}(p_{1}),\ldots, \phi\circ c_{r}^{(t-1)}(p_{q+1}))\) for \(\mathbf{p}=\{p_{i}\}_{i=1}^{q+1}\in\mathcal{N}_{q}(v)\). By assumption, all message, update, and readout functions are injective in Definition 9. Since the concatenation of injective functions is injective, there exists an injective function \(\psi\) such that

\[h_{r}^{(t)}(v)=\psi\Bigg{(}c_{r}^{(t-1)}(v), \left\{\left\{c_{r}^{(t-1)}(\mathbf{p})\mid\mathbf{p}\in\mathcal{N} _{0}(v)\right\}\right\},\] \[\left\{\left\{c_{r}^{(t-1)}(\mathbf{p})\mid\mathbf{p}\in\mathcal{ N}_{1}(v)\right\}\right\},\] \[\vdots\] \[\left\{\left\{c_{r}^{(t-1)}(\mathbf{p})\mid\mathbf{p}\in\mathcal{ N}_{r}(v)\right\}\right\}\Bigg{)}.\]

As HASH in Definition 7 is injective, the inverse \(\mathrm{HASH}^{-1}\) exists and is also injective. Hence,

\[h_{r}^{(t)}(v)=\psi\circ\mathrm{HASH}^{-1}\circ\mathrm{HASH} \left(c_{r}^{(t-1)}(v), \left\{\left\{c_{r}^{(t-1)}(\mathbf{p})\mid p\in\mathcal{N}_{0}(v) \right\}\right\},\right.\] \[\left\{\left\{\left(c_{r}^{(t-1)}(\mathbf{p})\mid\mathbf{p}\in \mathcal{N}_{1}(v)\right\}\right\}\right.,\] \[\vdots\] \[\left.\left.\left\{\left(c_{r}^{(t-1)}(\mathbf{p})\mid\mathbf{p} \in\mathcal{N}_{r}(v)\right\}\right\}\right)\right.\] \[=\psi\circ\mathrm{HASH}^{-1}\left(c_{r}^{(t)}(v)\right).\]

Choosing \(\phi=\psi\circ\mathrm{HASH}^{-1}\) finishes the proof. 

We conclude this section with the following lemma that justifies our architectural choice in (4).

**Lemma 9**.: _Let \(x\in\mathbb{Q}^{r}\). Then there exist \(\varepsilon\in\mathbb{R}^{r}\) such that_

\[\varphi(x)=\sum_{k=0}^{r}\varepsilon_{k}x_{k} \tag{21}\]

_is an injective function._

Proof.: We prove this claim by induction. For \(r=0\), any \(x\neq 0\in\mathbb{R}\) fulfills the claim. Now, let \(\varepsilon\in\mathbb{R}^{r}\) such that \(\varphi(x):\mathbb{Q}^{r}\to\mathbb{R}\) is injective. The set \(\mathbb{Q}[\varepsilon_{1},\ldots,\varepsilon_{r}]=\left\{\sum_{k=0}^{r} \varepsilon_{k}x_{k}\,|\,x\in\mathbb{Q}^{r}\right\}\) is 

[MISSING_PAGE_EMPTY:45]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We mention our main results in the abstract and introduction (see Section 1), and provide proofs for all claims (see Appendix E ff.) and code for all experiments (see our GitHub repository). We clearly state the scope of our theoretical results and emphasize that our method provides an expressive architecture suitable for sparse real-world graphs. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We describe the computational complexity of our method in a separate paragraph (see Section 6). We specifically address its preprocessing complexity on dense graphs in a separate paragraph (see Section 7). We also mention important future work that would complement our theoretical contributions in the conclusion section (see Section 8). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: All theorems are numbered, and we refer to the formal proofs in the appendix (see Appendix E ff.). Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All hyperparameters, hyperparameter ranges, and used resources are given in Appendix C. Our GitHub repository provides instructions on how to reproduce all experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). *4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All hyperparameters, hyperparameter ranges, and used resources are given in the appendix (see Appendix C). Our GitHub repository contains instructions to reproduce all experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Data splits, hyperparameters, hyperparameter grids (if used), optimizer, etc. are detailed in the main paper (see Section 7) and appendix (see Appendix C), with additional instructions provided in the anonymous GitHub link (see our GitHub repository). Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]Justification: We report error bars and confidence intervals for our experimental results (see Section 7), capturing the variability due to different random seeds and initialization. Detailed information about statistical significance tests is provided in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All information on the compute resources is given in the appendix (see Appendix C). For the real-world experiments, we provide run times, and for the synthetic experiments, the run times are negligible. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We respect the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction). *10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: While our work has numerous potential societal consequences, none of which we feel must be explicitly emphasized here, its impact lies in providing a theoretical foundation for improved graph representation learning methodologies. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our paper does not release data or models with a high risk for misuse. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All used assets are properly cited, and the licenses are mentioned.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We introduce new assets in the form of code. All details about training, datasets, and models are given in the submitted paper and/or in our GitHub repository, including detailed instructions to run the code. The license is provided under the MIT license. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing or research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: Our paper does not involve crowdsourcing or research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.