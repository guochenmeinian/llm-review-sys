# Locality-Aware

Generalizable Implicit Neural Representation

 Doyup Lee

Kakao Brain

doyup.lee@kakakobrain.com

&Chiheon Kim

Kakao Brain

chiheon.kim@kakobrain.com

&Minsu Cho

POSTECH

mscho@postech.ac.kr

&Wook-Shin Han

POSTECH

wshan@dblab.postech.ac.kr

Equal contributionCorresponding authors

###### Abstract

Generalizable implicit neural representation (INR) enables a single continuous function, i.e., a coordinate-based neural network, to represent multiple data instances by modulating its weights or intermediate features using latent codes. However, the expressive power of the state-of-the-art modulation is limited due to its inability to localize and capture fine-grained details of data entities such as specific pixels and rays. To address this issue, we propose a novel framework for generalizable INR that combines a transformer encoder with a locality-aware INR decoder. The transformer encoder predicts a set of latent tokens from a data instance to encode local information into each latent token. The locality-aware INR decoder extracts a modulation vector by selectively aggregating the latent tokens via cross-attention for a coordinate input and then predicts the output by progressively decoding with coarse-to-fine modulation through multiple frequency bandwidths. The selective token aggregation and the multi-band feature modulation enable us to learn locality-aware representation in spatial and spectral aspects, respectively. Our framework significantly outperforms previous generalizable INRs and validates the usefulness of the locality-aware latents for downstream tasks such as image generation.

## 1 Introduction

Recent advances in generalizable implicit neural representation (INR) enable a single coordinate-based multi-layer perceptron (MLP) to represent multiple data instances as a continuous function. Instead of per-sample training of individual coordinate-based MLPs, generalizable INR extracts latent codes of data instances [13, 14, 40] to modulate the weights or intermediate features of the shared MLP model [8, 11, 19, 35]. However, despite the advances in previous approaches, their performance is still insufficient compared with individual training of INRs per sample.

We postulate that the expressive power of generalizable INRs is limited by the ability of _locality-awareness_ to localize relevant entities from a data instance and control

Figure 1: Learning curves of PSNRs during training on ImageNette 178\(\times\)178.

their structure in a fine-grained manner. Primitive entities of a data instance, such as pixels in an image, tend to have a higher correlation with each other if they are closer in space and time. Thus, this locality of data entities has been used as an important inductive bias for learning the representations of complex data [3]. However, previous approaches to generalizable INRs are not properly designed to leverage the locality of data entities. For example, when latent codes modulate intermediate features [11; 12] or weight matrices [8; 19; 35] of an INR decoder, the modulation methods do not exploit a specified coordinates for decoding, which restricts the latent codes to encoding global information over all pixels without capturing local relationships between specific pixels.

To address this issue, we propose a novel encoder-decoder framework for _locality-aware_ generalizable INR to effectively localize and control the fine-grained details of data. In our framework, a Transformer encoder [37] first extracts locally relevant information from a data instance and predicts a set of latent tokens to encode different local information. Then, our locality-aware INR decoder effectively leverages the latent tokens to predict fine-grained details. Specifically, given an input coordinate, our INR decoder uses a cross-attention to selectively aggregate the local information in the latent tokens and extract a modulation vector for the coordinate. In addition, our INR decoder effectively captures the high-frequency details in the modulation vector by decomposing it into multiple bandwidths of frequency features and then progressively composing the intermediate features. We conduct extensive experiments to demonstrate the high performance and efficacy of our locality-aware generalizable INR on benchmarks as shown in Figure 1. In addition, we show the potential of our locality-aware INR latents to be utilized for downstream tasks such as image synthesis.

Our main contributions can be summarized as follows: 1) We propose an effective framework for generalizable INR with a Transformer encoder and locality-aware INR decoder. 2) The proposed INR decoder with selective token aggregation and multi-band feature modulation can effectively capture the local information to predict the fine-grained data details. 3) The extensive experiments validate the efficacy of our framework and show its applications to a downstream image generation task.

## 2 Related Work

Implicit neural representations (INRs).INRs use neural networks to represent complex data such as audio, images, and 3D scenes, as continuous functions. Especially, incorporating Fourier features [24; 36], periodic activations [31], or multi-grid features [25] significantly improves the performance of INRs. Despite its broad applications [1; 6; 10; 32; 34], INRs commonly require separate training of MLPs to represent each data instance. Thus, individual training of INRs per sample does not learn common representations in multiple data instances.

Generalizable INRs.Previous approaches focus on two major components for generalizable INRs; latent feature extraction and modulation methods. Auto-decoding [23; 26] computes a latent vector per data instance and concatenates it with the input of a coordinate-based MLP. Given input data, gradient-based meta-learning [4; 11; 12] adapts a shared latent vector using a few update steps to scale and shift the intermediate activations of the MLP. Learned Init [35] also uses gradient-based meta-learning but adapts whole weights of the shared MLP. Although auto-decoding and gradient-based meta-learning are agnostic to the types of data, their training is unstable on complex and large-scale datasets. TransINR [8] employs the Transformer [37] as a hypernetwork to predict latent vectors to modulate the weights of the shared MLP. In addition, Instance Pattern Composers [19] have demonstrated that modulating the weights of the second MLP layer is enough to achieve high performance of generalizable INRs. Our framework also employs the Transformer encoder, but focuses on extracting locality-aware latent features for the high performance of generalizable INR.

Leveraging Locality of Data for INRsLocal information in data has been utilized for efficient modeling of INRs, since local relationships between data entities are widely used for effective process of complex data [3]. Given an input coordinate, the coordinate-based MLP only uses latent vectors nearby the coordinate, after a CNN encoder extracts a 2D grid feature map of an image for super-resolution [7] and reconstruction [22]. Spatial Functa [4] demonstrates that leveraging the locality of data enables INRs to be utilized for downstream tasks such as image recognition and generation. Local information in 3D coordinates has also been effective for scene modeling as a hybrid approach using 3D feature grids [18] or the part segmentation [17] of a 3D object. However, previous approaches assume explicit grid structures of latents tailored to a specific data type. Sincewe do not predefine a relationship between latents, our framework is flexible to learn and encode the local information of both grid coordinates in images and non-grid coordinates in light fields.

## 3 Methods

We propose a novel framework for _locality-aware generalizable INR_ which consists of a Transformer encoder to localize the information in data into latent tokens and a locality-aware INR decoder to exploit the localized latents and predict outputs. First, we formulate how generalizable INR enables a single coordinate-based neural network to represent multiple data instances as a continuous function by modulating its weights or features. Then, after we introduce the Transformer encoder to extract a set of latent tokens from input data instances, we explain the details of the locality-aware INR decoder, where _selective token selection_ aggregates the spatially local information for an input coordinate via cross-attention; _multi-band feature modulation_ leverages a different range of frequency bandwidths to progressively decode the local information using coarse-to-fine modulation in the spectral domain.

### Generalizable Implicit Neural Representation

Given a set of data instances \(\mathcal{X}=\{\mathbf{x}^{(n)}\}_{n=1}^{N}\), each data instance \(\mathbf{x}^{(n)}=\{(\mathbf{v}^{(n)}_{i},\mathbf{y}^{(n)}_{i})\}_{i=1}^{M_{n}}\) comprises \(M_{n}\) pairs of an input coordinate \(\mathbf{v}^{(n)}_{i}\in\mathbb{R}^{d_{n}}\) and the corresponding output feature \(\mathbf{y}^{(n)}_{i}\in\mathbb{R}^{d_{\text{out}}}\). Conventional approaches [24; 31; 36] adopt individual coordinate-based MLPs to train and memorize each data instance \(\mathbf{x}^{(n)}\). Thus, the coordinate-based MLP cannot be reused and generalized to represent other data instances, requiring per-sample optimization of MLPs for unseen data instances.

A generalizable INR uses a single coordinate-based MLP as a shared INR decoder \(F_{\theta}:\mathbb{R}^{d_{\text{in}}}\rightarrow\mathbb{R}^{d_{\text{out}}}\) to represent multiple data instances as a continuous function. Generalizable INR [8; 11; 12; 19; 26] extracts the \(R\) number of latent codes \(\mathbf{Z}^{(n)}=\{\mathbf{z}^{(n)}_{k}\in\mathbb{R}^{d}\}_{k=1}^{R}\) from a data instance \(\mathbf{x}^{(n)}\). Then, the latents are used for the INR decoder to represent a data instance \(\mathbf{x}^{(n)}\) as \(\mathbf{y}^{(n)}_{i}=F_{\theta}(\mathbf{v}^{(n)}_{i};\mathbf{Z}^{(n)})\), while updating the parameters \(\theta\) and latents \(\mathbf{Z}^{(n)}\) to minimize the errors over \(\mathcal{X}\):

\[\min_{\theta,\mathbf{Z}^{(n)}}\frac{1}{NM_{n}}\sum_{n=1}^{N}\sum_{i=1}^{M_{n}} \left\|\mathbf{y}^{(n)}_{i}-F_{\theta}(\mathbf{v}^{(n)}_{i};\mathbf{Z}^{(n)} )\right\|_{2}^{2}.\] (1)

We remark that each previous approach employs a different number of latent codes to modulate a coordinate-based MLP. For example, a single latent vector (\(R=1\)) is commonly extracted to modulate intermediate features of the MLP [11; 12; 26], while a multitude of latents (\(R>1\)) are used to modulate its weights [8; 19; 35]. While we modulate the features of MLP, we extract a set of latent codes to localize the information of data to leverage the locality-awareness for latent features.

Figure 2: Overview of our framework for locality-aware generalizable INR. Given a data instance, Transformer encoder extracts its localized latents. Then, the locality-aware INR decoder uses selective token aggregation and multi-band feature modulation to predict the output for the input coordinate.

### Transformer Encoder

Our framework employs a Transformer encoder [37] to extract a set of latents \(\mathbf{Z}^{(n)}\) for each data instance \(\mathbf{x}^{(n)}\) as shown in Figure 2. After a data instance, such as an image or multi-view images, is patchified into a sequence of data tokens, we concatenate the patchified tokens into a sequence of \(R\) learnable tokens as the encoder input. Then, the Transformer encoder extracts a set of latent tokens, where each latent token corresponds to an input learnable token. Note that the permutation-equivariance of self-attention in the Transformer encoder enables us not to predefine the local structure of data and the ordering of latent tokens. During training, each latent token learns to capture the local information of data, while covering whole regions to represent a data instance. Thus, whether a data instance is represented on a grid or non-grid coordinate, our framework is flexible to encode various types of data into latent tokens, while learning the local relationships of latent tokens during training.

### Locality-Aware Decoder for Implicit Neural Representations

We propose the locality-aware INR decoder in Figure 2 to leverage the local information of data for effective generalizable INR. Our INR decoder comprises two primary components: i) _Selective token aggregation via cross attention_ extracts a modulation vector for an input coordinate to aggregate spatially local information from latent tokens. ii) _Multi-band feature modulation_ decomposes the modulation vector into multiple bandwidths of frequency features to amplify the high-frequency features and effectively predict the details of outputs.

#### 3.3.1 Selective Token Aggregation via Cross-Attention

We remark that encoding locality-aware latent tokens is not straightforward since the self-attentions in Transformer do not guarantee a specific relationship between tokens. Thus, the properties of the latent tokens are determined by a modulation method for generalizable INR to exploit the extracted latents. For example, given an input coordinate \(\mathbf{v}\) and latent tokens \(\{\mathbf{z}_{1},...,\mathbf{z}_{R}\}\), a straightforward method can use Instance Pattern Composers [19] to construct a modulation weight \(\mathbf{W}_{\text{m}}=[\mathbf{z}_{1},...,\mathbf{z}_{R}]^{\top}\in\mathbb{R }^{R\times d_{\text{n}}}\) and extract a modulation vector \(\mathbf{m}_{\mathbf{v}}=\mathbf{W}_{\text{m}}\mathbf{v}=[\mathbf{z}_{\mathbf{ v}}^{\top}\mathbf{v},...,\mathbf{z}_{R}^{\top}\mathbf{v}]^{\top}\in\mathbb{R }^{R}\). However, the latent tokens cannot encode the local information of data, since each latent token equally influences each channel of the modulation vector regardless of the coordinate locations (see Section 4.3).

Our selective token aggregation employs cross-attention to aggregate the spatially local latents nearby the input coordinate, while guiding the latents to be locality-aware. Given a set of latent tokens \(\mathbf{Z}^{(n)}=\{\mathbf{z}_{k}^{(n)}\}_{k=1}^{R}\) and a coordinate \(\mathbf{v}_{i}^{(n)}\), a modulation feature vector \(\mathbf{m}_{\mathbf{v}_{i}}^{(n)}\in\mathbb{R}^{d}\) shifts the intermediate features of an INR decoder to predict the output, where \(d\) is the dimensionality of hidden layers in the INR decoder. For the brevity of notation, we omit the superscript \(n\) and subscript \(i\).

Frequency featuresWe first transform an input coordinate \(\mathbf{v}=(v_{1},\cdots,v_{d_{\text{n}}})\in\mathbb{R}^{d_{\text{n}}}\) into frequency features using sinusoidal positional encoding [31, 36]. We define the Fourier features \(\gamma_{\sigma}(\mathbf{v})\in\mathbb{R}^{d_{\text{F}}}\) with bandwidth \(\sigma>1\) and feature dimensionality \(d_{\text{F}}\) as

\[\gamma_{\sigma}(\mathbf{v})=[\cos(\pi\omega_{j}v_{i}),\sin(\pi\omega_{j}v_{i} ):i=1,\cdots,d_{\text{in}},j=0,\cdots,n-1]\] (2)

where \(n=\frac{d_{\text{n}}}{2d_{\text{n}}}\). A frequency \(\omega_{j}=\sigma^{j/(n-1)}\) is evenly distributed between 1 and \(\sigma\) on a log-scale. Based on the Fourier features, we define the _frequency feature_ extraction \(\mathbf{h}_{\text{F}}(\cdot)\) as

\[\mathbf{h}_{\text{F}}(\mathbf{v};\sigma,\mathbf{W},\mathbf{b})=\mathrm{ReLU} \left(\mathbf{W}\gamma_{\sigma}(\mathbf{v})+\mathbf{b}\right),\] (3)

where \(\mathbf{W}\in\mathbb{R}^{d\times d_{\text{F}}}\) and \(\mathbf{b}\in\mathbb{R}^{d}\) are trainable parameters for frequency features, \(d\) denotes the dimensionality of hidden layers in the INR decoder.

Selective token selection via cross-attentionTo predict corresponding output \(\mathbf{y}\) to the coordinate \(\mathbf{v}\), we adopt a cross-attention to extract a modulation feature vector \(\mathbf{m}_{\mathbf{v}}\in\mathbb{R}^{d}\) based on the latent tokens \(\mathbf{Z}=\{\mathbf{z}_{k}\}_{k=1}^{R}\). We first extract the frequency features of the coordinate \(\mathbf{v}\) in Eq (3) as the query of the cross-attention as

\[\mathbf{q}_{\mathbf{v}}:=\mathbf{h}_{\text{F}}(\mathbf{v};\sigma_{\text{q}}, \mathbf{W}_{\text{q}},\mathbf{b}_{\text{q}}),\] (4)

where \(\mathbf{W}_{\text{q}}\in\mathbb{R}^{d\times d_{\text{F}}}\) and \(\mathbf{b}_{\text{q}}\in\mathbb{R}^{d}\) are trainable parameters, and \(\sigma_{\text{q}}\) is the bandwidth for query frequency features. The cross-attention in Figure 2 enables the query to select latent tokens, aggregateits local information, and extract the modulation feature vector \(\mathbf{m_{v}}\) for the input coordinate:

\[\mathbf{m_{v}}:=\mathrm{MultiHeadAttention}(\text{Query}=\mathbf{q_{v}},\text{ Key}=\mathbf{Z},\text{Value}=\mathbf{Z}).\] (5)

An intuitive implementation for selective token aggregation can employ hard attention to select only one latent token for each coordinate. However, in our primitive experiment, using hard attention leads to unstable training and a latent collapse problem that selects only few latent tokens. Meanwhile, multi-head attentions encourage each latent token to easily learn the locality in data instances.

#### 3.3.2 Multi-Band Feature Modulation in the Spectral Domain

After the selective token aggregation extracts a modulation vector \(\mathbf{m_{v}}\), we use multi-band feature modulation to effectively predict the details of outputs. Although Fourier features [24, 36] reduce the spectral bias [2, 28] of neural networks, adopting a simple stack of MLPs to INRs still suffers from capturing the high-frequency data details. To address this issue, we use a different range of frequency bandwidths to decompose the modulation vector into multiple frequency features in the spectral domain. Then, our multi-band feature modulation uses the multiple frequency features to progressively decode the intermediate features, while encouraging a deeper MLP path to learn higher frequency features. Note that the coarse-to-fine approach in the spectral domain is analogous to the locally hierarchical approach in the spatial domain [21, 29, 39] to capture the data details.

Extracting multiple modulation features with different frequency bandwidthsWe extract \(L\) level of modulation features \(\mathbf{m}_{\mathbf{v}}^{(1)},\cdots,\mathbf{m}_{\text{L}}^{(L)}\) from \(\mathbf{m_{v}}\) using different bandwidths of frequency features. Given \(L\) frequency bandwidths as \(\sigma_{1}\geq\sigma_{2}\geq\cdots\geq\sigma_{L}\geq\sigma_{\text{q}}\), we use Eq (3) to extract the \(\ell\)-th level of frequency features of an input coordinate \(\mathbf{v}\) as

\[(\mathbf{h}_{\text{F}})_{\mathbf{v}}^{(\ell)}:=\mathbf{h}_{\text{F}}(\mathbf{ v};\sigma_{\ell},\mathbf{W}_{\text{F}}^{(\ell)},\mathbf{b}_{\text{F}}^{(\ell)})= \mathrm{ReLU}\left(\mathbf{W}_{\text{F}}^{(\ell)}\boldsymbol{\gamma}_{\sigma_ {\ell}}(\mathbf{v})+\mathbf{b}_{\text{F}}^{(\ell)}\right),\] (6)

where \(\mathbf{W}_{\text{F}}^{(\ell)}\) and \(\mathbf{b}_{\text{F}}^{(\ell)}\) are trainable parameters and shared across data instances. Then, the \(\ell\)-th modulation vector \(\mathbf{m}_{\mathbf{v}}^{(\ell)}\) is extracted from the modulation vector \(\mathbf{m}_{\mathbf{v}}\) as

\[\mathbf{m}_{\mathbf{v}}^{(\ell)}:=\mathrm{ReLU}\left((\mathbf{h}_{\text{F}})_{ \mathbf{v}}^{(\ell)}+\mathbf{W}_{\text{m}}^{(\ell)}\mathbf{m}_{\mathbf{v}}+ \mathbf{b}_{\text{m}}^{(\ell)}\right),\] (7)

with a trainable weight \(\mathbf{W}_{\text{m}}^{(\ell)}\) and bias \(\mathbf{b}_{\text{m}}^{(\ell)}\). Considering that \(\mathrm{ReLU}\) cutoffs the values below zero, we assume that \(\mathbf{m}_{\mathbf{v}}^{(\ell)}\) filters out the information of \(\mathbf{m}_{\mathbf{v}}\) based on the \(\ell\)-th frequency patterns of \((\mathbf{h}_{\text{F}})_{\mathbf{v}}^{(\ell)}\).

Multi-band feature modulationAfter decomposing a modulation vector into multiple features with different frequency bandwidths, we progressively compose the \(L\) modulation features by applying a stack of nonlinear operations with a linear layer and ReLU activation. Starting with \(\mathbf{h}_{\mathbf{v}}^{(1)}=\mathbf{m}_{\mathbf{v}}^{(1)}\), we compute the \(\ell\)-th hidden features \(\mathbf{h}_{\mathbf{v}}^{(\ell)}\) for \(\ell=2,\cdots,L\) as

\[\widetilde{\mathbf{h}}_{\mathbf{v}}^{(\ell)}:=\mathbf{m}_{\mathbf{v}}^{(\ell) }+\mathbf{h}_{\mathbf{v}}^{(\ell-1)}\quad\text{and}\quad\mathbf{h}_{\mathbf{v }}^{(\ell)}:=\mathrm{ReLU}(\mathbf{W}^{(\ell)}\widetilde{\mathbf{h}}_{ \mathbf{v}}^{(\ell)}+\mathbf{b}^{(\ell)}),\] (8)

where \(\mathbf{W}^{(\ell)}\in\mathbb{R}^{d\times d}\) and \(\mathbf{b}^{(\ell)}\in\mathbb{R}^{d}\) are trainable weights and biases of the INR decoder. \(\widetilde{\mathbf{h}}_{\mathbf{v}}^{(\ell)}\) denotes the \(\ell\)-th pre-activation of INR decoder for coordinate \(\mathbf{v}\). Note that the modulation features with high-frequency bandwidth can be processed by more nonlinear operations than the features with lower frequency bandwidths, considering that high-frequency features contain more complex signals.

Finally, the output \(\hat{\mathbf{y}}\) is predicted using all intermediate hidden features of the INR decoder as

\[\hat{\mathbf{y}}:=\sum_{\ell=1}^{L}f_{\text{out}}^{(\ell)}(\mathbf{h}_{ \mathbf{v}}^{(\ell)}),\] (9)

where \(f_{\text{out}}^{(\ell)}:\mathbb{R}^{d}\rightarrow\mathbb{R}^{d_{\text{out}}}\) are a linear projection into the output space. Although utilizing only \(\mathbf{h}_{\mathbf{v}}^{(L)}\) is also an option to predict outputs, skip connections of all intermediate features into the output layer enhances the robustness of training to the hyperparameter choices.

[MISSING_PAGE_FAIL:6]

high-quality result of reconstructed images. The results demonstrate that leveraging the locality of data is crucial for generalizable INR to model complex and high-resolution data.

### Few-Shot Novel View Synthesis

We evaluate our framework on novel view synthesis with the ShapeNet Chairs, Cars, and Lamps datasets to predict a rendered image of a 3D object under an unseen view. Given few views of an object with known camera poses, we employ a light field [32] for novel view synthesis. A light field does not use computationally intensive volume rendering [24] but directly predicts RGB colors for the input coordinate for rays with \(d_{\text{in}}=6\) using the Plucker coordinate system. Our INR decoder uses \(d=256\) and two levels of feature modulations with \(\sigma_{\text{q}}=2\) and \((\sigma_{1},\sigma_{2})=(8,4)\).

Figure 4(a) shows that our framework outperforms IPC for novel view synthesis. Our framework shows competitive performance with IPC when only one support view is provided. However, the performance of our framework is consistently improved as the number of support views increases, while outperforming the results of IPC. Note that defining a local relationship between rays is not straightforward due to its non-grid property of the Plucker coordinate. Our Transformer encoder can learn the local relationship between rays to extract locality-aware latent tokens during training and achieve high performance. We analyze the learned locality of rays encoded in the extracted latents in Section 4.3. Figure 4(b) shows that our framework correctly predicts the colors and shapes of a novel view corresponding to the support views, although the predicted views are blurry due to the lack of training objectives with generative modeling. We expect that combining our framework with generative models [5; 38] to synthesize a photorealistic novel view is an interesting future work.

### In-Depth Analysis

Learning Curves on ImageNette 178\(\times\)178Figure 1 juxtaposes the learning curves of our framework and previous approaches on ImageNette 178\(\times\)178. Note that TransINR, IPC, and our framework use the same Transformer encoder to extract data latents, while adopting different modulation methods. While the training speed of our framework is about 80% of the speed of IPC, we remark our framework achieves the test PSNR of 38.72 after 400 epochs of training, outperforming the PSNR of 38.46 achieved by IPC trained for 4000 epochs, hence resulting in \(8\times\) speed-up of training time. That is, our locality-aware latents enables generalizable INR to be both efficient and effective.

Selective token aggregation and multi-band feature modulationsWe conduct an ablation study on image reconstruction of with ImageNette 178\(\times\)178 and FFHQ 256\(\times\)256, novel view synthesis with Lamp-3 views to validate the effectiveness of the selective token aggregation and the multi-band feature modulation. We replace the multi-band feature modulations with a simple stack of MLPs (ours w/o multiFM), and the selective token aggregation with the weight modulation of IPC (ours w/o STA). If both two modules are replaced together, the INR decoder becomes the same

Figure 4: (a) PSNRs on novel view synthesis of ShapeNet Chairs, Cars, and Lamps according to the number of support views (1-5 views). (b) Examples of novel view synthesis with 4 support views.

\begin{table}
\begin{tabular}{l|c c c} \hline  & ImageNette & FFHQ & Lamp \\ \hline Ours & **37.46** & **38.01** & **26.00** \\ w/o STA & 34.54 & 34.52 & 25.31 \\ w/o multiFM & 33.90 & 33.65 & 25.78 \\ \hline IPC [19] & 34.11 & 34.68 & 25.09 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation study on ImageNette 178\(\times\)178, FFHQ 256\(\times\)256, and Lamp-3 views.

architecture as IPC. We use single-head cross-attention for the selective token aggregation to focus on the effect of two modules. Table 3 demonstrates that both the selective token aggregation and the multi-band feature modulation are required for the performance improvement, as there is no significant improvement when only one of the modules is used.

Choices of frequency bandwidthsTable 4 shows that the ordering of frequency bandwidths in Eq. (4) and Eq. (6) can affect the performance. We train our framework with two-level feature modulations on ImageNet 178\(\times\)178 during 400 epochs with different settings of the bandwidths \(\sigma_{1},\sigma_{2},\sigma_{\text{q}}\). Although our framework outperforms IPC regardless of the bandwidth settings, the best PSNR is achieved with \(\sigma_{1}\geq\sigma_{2}\geq\sigma_{\text{q}}\). The results imply that selective token aggregation does not require high-frequency features, but the high-frequency features need to be processed by more nonlinear operations than lower-frequency features as discussed in Section 3.3.2.

The role of extracted latent tokensFigure 5 shows that our framework encodes the local information of data into each latent token, while IPC cannot learn the locality in data coordinates. To visualize the information in each latent token, we randomly select a latent token to be replaced with the zero vector. Then, we visualize the difference between the model predictions with or without the replacement. Each latent token of our framework encapsulates the local information in different regions of images and light fields. However, the latent tokens of IPC cannot exploit the local information of data, while encoding the global information over whole coordinates. Note that our framework _learns_ the structure of locality in light fields during training, although the structure of the Plucker coordinate system is not regular as the grid coordinates of images. Thus, our framework can learn the locality-aware latents of data for generalizable INR regardless of the types of coordinate systems.

### Generating INRs for Conditional Image Synthesis

We examine the potentials of the extracted latent tokens to be utilized for a downstream task such as class-conditional image generation of ImageNet [9]. Note that we cannot use the architecture of U-Net in conventional image diffusion models [4; 30], since our framework is not tailored to the 2D grid coordinate. Thus, we adopt a Transformer-based diffusion model [15; 27] to predict a set of latent tokens after corrupting the latents by Gaussian noises.

\begin{table}
\begin{tabular}{c c|c} \hline \((\sigma_{1},\sigma_{2})\) & \(\sigma_{\text{q}}\) & ImageNette \\ \hline (128, 32) & 16 & **37.46** \\ (32, 128) & 16 & 35.00 \\ (128, 128) & 16 & 35.30 \\ (128, 32) & 128 & 35.58 \\ \hline \multicolumn{3}{c}{IPC (\(\sigma=128\))} & 34.11 \\ \hline \end{tabular}
\end{table}
Table 4: PSNRs of reconstructed ImageNette 178\(\times\)178 with various frequency bandwidths.

Figure 5: Visualization of differences between model predictions after replacing a latent token with the zero vector, for IPC [19] and our framework.

We train 458M parameters of Transformers during 400 epochs to generate our locality-aware latent tokens. We attach the detailed setting in Appendix A.3. When we train a diffusion model to generate latent tokens of IPC in Figure 6, the generated images suffer from severe artifacts, because the prediction error of each latent token for IPC leads to the artifacts over all coordinates. Contrastively, the diffusion model for our locality-aware latents generates realistic images. In addition, although we do not conduct exhaustive hyperparamter search, the FID score of generated images achieves 9.3 with classifier-free guidance scale [16] in Table 5. Thus, the results validate the potential applications of the local latents for INRs. Meanwhile, a few generated images may exhibit checkerboard artifacts, particularly in simple backgrounds, but we leave the elaboration of a diffusion process and sampling techniques for generating INR latents as future work.

### Comparison with Overfitted INRs

Figure 7 shows that our generalizable INR efficiently provides meaningful INRs compared with individual training of INRs per sample. To evaluate the efficiency of our framework, we select ten images of FFHQ 256\(\times\)256 and train randomly initialized FFNet [36] per sample using one NVIDIA V100 GPU. The individual training of FFNets requires over 10 seconds of optimization to achieve the same PSNRs of our framework, where our inference time is negligible. Moreover, when we apply the test-time optimization (TTO) only for the extracted latents, it consistently outperforms per-sample FFNets for 30 seconds while maintaining the structure of latents. When we consider the predicted INR as initialization and finetune all parameters of the INR decoder per each sample, our framework consistently outperforms the per-sampling training of INRs from random initialization. Thus, the results imply that leveraging generalizable INR is computationally efficient to model unseen data as INRs regardless of a TTO.

## 5 Conclusion

We have proposed an effective framework for generalizable INR with the Transformer encoder and locality-aware INR decoder. The Transformer encoder capture the locality of data entities and learn to encode the local information into different latent tokens. Our INR decoder selectively aggregates the locality-aware latent tokens to extract a modulation vector for a coordinate input and exploits the multiple bandwidths of frequency features to effectively predict the fine-grained data details. Experimental results demonstrate that our framework significantly outperforms previous generalizable

Figure 6: The examples of generated 256\(\times\)256 images by generating latents of IPC (left) and ours (right), trained on ImageNet.

\begin{table}
\begin{tabular}{l|c|c c} \hline  & Latent Shape & rPSNR & FID \\ \hline Ours & 256\(\times\)256 & 37.7 & 9.3 \\ \hline Spatial & 16\(\times\)16\(\times\)256 & 37.2 & 11.7 \\ Funeta [4] & 32\(\times\)32\(\times\)64 & 37.7 & 8.8 \\ \hline LDM [30] & 64\(\times\)64\(\times\)3 & 27.4 & 3.6 \\ \hline \end{tabular}
\end{table}
Table 5: Reconstructed PSNRs and FID of generated images on ImageNet 256\(\times\)256.

Figure 7: Comparison with individually trained FFNets [36] per sample.

INRs on image reconstruction and few-shot novel view synthesis. In addition, we have conducted the in-depth analysis to validate the effectiveness of our framework and shown that our locality-aware latent tokens for INRs can be utilized for downstream tasks such as image generation to provide realistic images. Considering that our framework can learn the locality in non-grid coordinates, such as the Plucker coordinate for rays, leveraging our generalizable INR to generate 3D objects or scenes is a worth exploration. In addition, extending our framework to support arbitrary resolution will be an interesting future work. Furthermore, since our framework has still room for performance improvement of high-resolution image reconstruction, such as 1024\(\times\)1024, we expect that elaborating on the architecture and techniques for diffusion models to effectively generate INRs is an interesting future work.

## 6 Acknowledgements

This work was supported by Institute of Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government(MSIT) (No.2018-0-01398: Development of a Conversational, Self-tuning DBMS, 35%; No.2022-0-00113: Sustainable Collaborative Multimodal Lifelong Learning, 30%) and the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIT) (No. NRF-2021R1A2B5B03001551, 35%)

## References

* [1]J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla, and P. P. Srinivasan (2021) Mip-nerf: a multiscale representation for anti-aliasing neural radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 5855-5864. Cited by: SS1.
* [2]R. Basri, M. Galun, A. Geifman, D. Jacobs, Y. Kasten, and S. Kritchman (2020) Frequency bias in neural networks for input of non-uniform density. In International Conference on Machine Learning, pp. 685-694. Cited by: SS1.
* [3]P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, et al. (2018) Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261. Cited by: SS1.
* [4]M. Bauer, E. Dupont, A. Brock, D. Rosenbaum, J. Schwarz, and H. Kim (2023) Spatial functa: scaling functa to imagenet classification and generation. arXiv preprint arXiv:2302.03130. Cited by: SS1.
* [5]E. R. Chan, K. Nagano, M. A. Chan, A. W. Bergman, J. J. Park, A. Levy, M. Aittala, S. De Mello, T. Karras, and G. Wetzstein (2023) GeNVS: generative novel view synthesis with 3D-aware diffusion models. In arXiv, Cited by: SS1.
* [6]H. Chen, B. He, H. Wang, Y. Ren, S. Nam Lim, and A. Shrivastava (2021) Nerv: neural representations for videos. Advances in Neural Information Processing Systems34, pp. 21557-21568. Cited by: SS1.
* [7]Y. Chen, S. Liu, and X. Wang (2021) Learning continuous image representation with local implicit image function. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 8628-8638. Cited by: SS1.
* [8]Y. Chen and X. Wang (2022) Transformers as meta-learners for implicit neural representations. In European Conference on Computer Vision, pp. 170-187. Cited by: SS1.
* [9]J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei (2009) Imagenet: a large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248-255. Cited by: SS1.
* Workshop @ ICLR 2021, Cited by: SS1.
* [11]E. Dupont, H. Kim, S. Ali Eslami, D. Jimenez Rezende, and D. Rosenbaum (2022) From data to functa: your data point is a function and you can treat it like one. In International Conference on Machine Learning, pp. 5694-5725. Cited by: SS1.
** Dupont et al. [2022] Emilien Dupont, Hrushikesh Loya, Milad Alizadeh, Adam Golinski, Yee Whye Teh, and Arnaud Doucet. Coin++: Data agnostic neural compression. _arXiv preprint arXiv:2201.12904_, 2022.
* Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _International conference on machine learning_, pages 1126-1135. PMLR, 2017.
* Ha et al. [2017] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks. In _International Conference on Learning Representations_, 2017.
* Ho et al. [2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _arXiv preprint arXiv:2006.11239_, 2020.
* Ho and Salimans [2022] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* Jiang et al. [2020] Chiyu Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias Niessner, Thomas Funkhouser, et al. Local implicit grid representations for 3d scenes. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6001-6010, 2020.
* Karnewar et al. [2022] Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy Mitra. Relu fields: The little non-linearity that could. In _ACM SIGGRAPH 2022 Conference Proceedings_, pages 1-9, 2022.
* Kim et al. [2023] Chiheon Kim, Doyup Lee, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Generalizable implicit neural representations via instance pattern composers. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023.
* Kingma and Ba [2015] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015.
* Lee et al. [2022] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11523-11532, 2022.
* Mehta et al. [2021] Ishit Mehta, Michael Gharbi, Connelly Barnes, Eli Shechtman, Ravi Ramamoorthi, and Mannohan Chandraker. Modulated periodic activations for generalizable local functional representations. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 14214-14223, 2021.
* Mescheder et al. [2019] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4460-4470, 2019.
* Mildenhall et al. [2021] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. _Communications of the ACM_, 65(1):99-106, 2021.
* Muller et al. [2022] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Trans. Graph._, 41(4):102:1-102:15, July 2022.
* Park et al. [2019] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 165-174, 2019.
* Peebles and Xie [2022] William Peebles and Saining Xie. Scalable diffusion models with transformers. _arXiv preprint arXiv:2212.09748_, 2022.
* Rahaman et al. [2019] Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred Hamprecht, Yoshua Bengio, and Aaron Courville. On the spectral bias of neural networks. In _International Conference on Machine Learning_, pages 5301-5310. PMLR, 2019.
* Razavi et al. [2019] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In _Advances in neural information processing systems_, pages 14866-14876, 2019.
* Rombach et al. [2022] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.

* [31] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. _Advances in Neural Information Processing Systems_, 33:7462-7473, 2020.
* [32] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field networks: Neural scene representations with single-evaluation rendering. _Advances in Neural Information Processing Systems_, 34:19313-19325, 2021.
* [33] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2021.
* [34] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8248-8258, 2022.
* [35] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi Schmidt, Pratul P Srinivasan, Jonathan T Barron, and Ren Ng. Learned initializations for optimizing coordinate-based neural representations. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2846-2855, 2021.
* [36] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. _Advances in Neural Information Processing Systems_, 33:7537-7547, 2020.
* [37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Advances in neural information processing systems_, pages 5998-6008, 2017.
* [38] Daniel Watson, William Chan, Ricardo Martin Brualla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi. Novel view synthesis with diffusion models. In _The Eleventh International Conference on Learning Representations_, 2023.
* [39] Tackgeun You, Saehoon Kim, Chiheon Kim, Doyup Lee, and Bohyung Han. Locally hierarchical auto-regressive modeling for image generation. In _Proceedings of the International Conference on Neural Information Processing Systems_, 2022.
* [40] Luisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adaptation via meta-learning. In _International Conference on Machine Learning_, pages 7693-7702. PMLR, 2019.

Implementation Details

We describe the implementation details of our locality-aware generalizable INR with the Transformer encoder and locality-aware INR decoder. We implement our framework based on the official open-sourced implementation of IPC3 for a fair comparison. Our Transformer encoder comprises six blocks of self-attentions with 12 attention heads, where each head uses 64 dimensions of hidden features, and \(R=256\) latent tokens for all experiments. We use the Adam [20] optimizer with \((\beta_{1},\beta_{2})=(0.9,0.999)\) and constant learning rate of 0.0001. The batch size is 16 and 32 for image reconstruction and novel view synthesis, respectively.

Footnote 3: https://github.com/kakaobrain/ginr-ipc

### Image Reconstruction

178\(\times\)178 image reconstructionFor the image reconstruction of CelebA, FFHQ, and ImageNette with 178\(\times\)178 resolution, we use \(L=2\) level of modulation features for multi-band feature modulation of locality-aware INR decoder. The dimensionality of frequency features and hidden layers in the INR decoder is 256, where \((\sigma_{1},\sigma_{2},\sigma_{\text{q}})=(128,32,16)\). We represent a 178\(\times\)178 resolution of the image as 400 tokens, where each token corresponds to a 9\(\times\)9 size of the image patch with zero padding. We use a multi-head attention block with two attention heads for our selective token selection via cross-attention. Following the experimental setting of previous studies [8; 19], we train our framework on CelebA, FFHQ, and ImageNette during 300, 1000, and 4000 epochs, respectively. When we use four NVIDIA V100 GPUs, the training takes 5.5, 6.7, and 4.3 days, respectively.

ImageNet 256\(\times\)256We use \(L=2\) level of feature modulation for the image reconstruction of ImageNet with 256\(\times\)256 resolution. We use eight heads of selective token aggregation, 256 dimensionality of frequency features and hidden layers of the INR decoder, and \((\sigma_{1},\sigma_{2},\sigma_{\text{q}})=(128,32,16)\). An image is represented as 256 tokens, where each token corresponds to a 16\(\times\)16 patch in the image. We use eight NVIDIA A100 GPUs to train our framework on ImageNet during 20 epochs, where the training takes about 2.5 days.

Ffhq 256\(\times\)256, 512\(\times\)512, and 1024\(\times\)1024Our framework for FFHQ 256\(\times\)256 and 512\(\times\)512 uses \(L=2\) level of feature modulation with \((\sigma_{1},\sigma_{2},\sigma_{\text{q}})=(128,32,16)\). The size of each patch is 16 and 32 for 256\(\times\)256 and 512\(\times\)512 resolutions, respectively, the number of latent tokens is \(R=256\), and the dimensionality of the INR decoder is \(\textit{d}=d=256\). Our selective token aggregation uses two and four heads of cross-attention for FFHQ 256\(\times\)256 and 512\(\times\)512, respectively. We randomly sample the 10% of coordinates to be decoded at each training step to increase the efficiency of training. We train our framework during 400 epochs, while the training takes about 1.5 days using four NVIDIA V100 GPUs for FFHQ with 256\(\times\)256 and about 1.4 days using eight V100 GPUs for FFHQ with 512\(\times\)512. For FFHQ 1024\(\times\)1024, we use 48 patch size to represent an image as 484 data tokens and \(L=2\) level of feature modulation with \((\sigma_{1},\sigma_{2},\sigma_{\text{q}})=(256,64,32)\). The training of 400 epochs takes about 3.4 days using eight NVIDIA V100 GPUs.

### Novel View Synthesis

We train our framework for the task of novel view synthesis on ShapeNet Cars, Chairs, and Lamps. Given a few known camera views as support views of a 3D object, our framework predicts a light field of the 3D object to predict unseen camera views. For a fair comparison, we use the same splits of train-valid samples with previous studies of generalizable INR [8; 19; 35]. Given rendering images of support views with 128\(\times\)128 resolution, we first patchify each rendered image into 256 tokens with 8\(\times\)8 size of patches. Then, we concatenate the patches of all support views with learnable tokens for the input of our Transformer. We use the Plucker coordinate to represent a ray for a pixel as an embedding with six dimensions and concatenate the ray embedding into each pixel along the channel dimension. Since our INR decoder estimates a light field of a 3D object, the INR decoder has six input channels \(d_{\text{in}}=6\) for a ray coordinate and three output channels \(d_{\text{out}}=3\) for a RGB pixel. Our INR decoder uses \(L=2\) level of feature modulation with \((\sigma_{1},\sigma_{2},\sigma_{\text{q}})=(8,4,2)\). We use \(d_{\text{F}}=d=256\) dimensionality of the frequency features and hidden features of the INR decoder. We use 1000 training epochs for ShapeNet Cars and Chairs, while using 500 epochs for ShapeNet Lamps.

### Diffusion Model for INR generation

We implement a diffusion model to generate the latent tokens for INRs of ImageNet 256\(\times\)256. Different from the conventional approaches, which use a U-Net architecture to generate an image, we use a vanilla Transformer with a simple stack of self-attentions, since the latent tokens do not predefine 2D grid structure but are permutation-equivariant. The Transformer for the diffusion model has 458M parameters having 24 self-attention blocks with 1024 dimensions of embeddings and 16 heads. We remark that the locality-aware generalizable INR is not updated during the training of diffusion models. For the training of the diffusion model, we follow the formulation of DDPM [15]. The linear schedule with \(T=1000\) is used to randomly corrupt the latent tokens for INRs using isotropic Gaussian noises, and then we train our Transformer to denoise the latent tokens. Instead of the \(\epsilon\)-parameterization that predicts the noises used for the corruption, our Transformer \(\mathbf{x}_{0}\)-parameterization to predict the original latent tokens. We drop 10% of class conditions for our model to support classifier-free guidance following the conventional setting [16]. For the stability of training, we standardize the features of latent tokens, after computing the mean and standard deviation of feature channels of each latent token based on the training data. We use eight NVIDIA A100 GPUs to train the model with 256 batch size during 400 epochs, where the training takes about 7 days. The Adam [20] optimizer with constant learning rate 0.0001 and \((\beta_{1},\beta_{2})=(0.9,0.999)\) is used without learning rate warm-up and any weight decaying. During training, we further compute the exponential moving average (EMA) of model parameters with a decaying rate of 0.9999. During the evaluation, we use the EMA model with 250 DDIM steps [33] and 2.5 scales of classifier-free guidance [16].

## Appendix B Additional Experiments

### Ablation Study on the Number of Levels

Table 6 demonstrates the effect of the number of levels \(L\) on image reconstruction benchmarks of FFHQ images with 256\(\times\)256, 512\(\times\)512, and 1024\(\times\)1024 resolutions. Our INR decoder uses bandwidths \(\sigma_{\text{q}}=16\) and \((\sigma_{\ell})_{\ell=1}^{L}\) equal to \((128)\), \((128,32)\), \((128,64,32)\) and \((128,90,64,32)\) for \(L=1,2,3,4\) respectively in case of 256\(\times\)256 and 512\(\times\)512 resolution, and all bandwidths are doubled for 1024\(\times\)1024 to leverage high-frequency details.

Note that our framework outperforms previous studies [8, 19] even with \(L=1\). Moreover, the results demonstrate that increasing \(L\) improves the performance, while the performance saturates beyond \(L\geq 3\). We postulate that higher resolution requires a larger number of levels, as the performance gap between \(L=3\) and \(L=4\) decreases as the resolution increases.

### Additional Examples of Novel View Synthesis

In Figure 8, we show additional examples of novel view synthesis of ShapeNet Chairs, Cars, and Lamps with one to five support views.

### Additional Examples of High-resolution Image Reconstruction

Figure 9 and 10 shows image reconstruction examples of FFHQ with 256\(\times\)256, 512\(\times\)512, and 1024\(\times\)1024 resolution by previous studies [8, 19] and our locality-aware generalizable INR. Unlike previous studies, our framework can successfully reconstruct fine-grained details in high resolutions.

\begin{table}
\begin{tabular}{l|c c c} \hline  & 256\(\times\)256 & 512\(\times\)512 & 1024\(\times\)1024 \\ \hline TransINR & 30.96 & 29.35 & - \\ IPC [19] & 34.68 & 31.58 & 28.68 \\ \hline Ours (\(L=1\)) & 37.09 & 34.84 & 31.56 \\ Ours (\(L=2\)) & 39.88 & 35.43 & 31.94 \\ Ours (\(L=3\)) & **40.13** & **35.58** & **32.40** \\ Ours (\(L=4\)) & 39.79 & 35.40 & 32.32 \\ \hline \end{tabular}
\end{table}
Table 6: PSNRs on the reconstructed FFHQ with 256\(\times\)256, 512\(\times\)512, and 1024\(\times\)1024 resolutions for different number of levels.

Figure 8: Examples of novel view synthesis of ShapeNet Chairs, Cars and Lamps with one, two, three, and five support views.

Figure 10: Examples of reconstructed images of FFHQ with 1024\(\times\)1024 resolution by IPC (left) and our locality-aware generalizable INR (right).

Figure 9: Examples of reconstructed images of FFHQ with 256\(\times\)256 resolution (top row) and 512\(\times\)512 resolution (bottom row) by TransINR [8] (left), IPC [19] (middle), and our locality-aware generalizable INR (right).

Figure 11: Additional examples of class-conditional image synthesis by generating the locality-aware latents of our framework via a transformer-based diffusion model with 458M parameters. All images are generated with classifier-free guidance at scale 2.5.

Figure 12: Additional visualization of differences between model predictions after replacing a latent token with the zero vector for IPC [19] and our framework.

[MISSING_PAGE_FAIL:19]