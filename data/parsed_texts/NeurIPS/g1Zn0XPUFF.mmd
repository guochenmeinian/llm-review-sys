# UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling

 Haider Al-Tahan\({}^{1}\), Quentin Garrido\({}^{1,2}\), Randall Balestriero\({}^{3}\),

**Diane Bouchacourt\({}^{1}\), Caner Hazirbas\({}^{1}\), Mark Ibrahim\({}^{1}\)**

\({}^{1}\)Meta FAIR, \({}^{2}\)Univ Gustave Eiffel, CNRS, LIGM, \({}^{3}\)Brown University

###### Abstract

Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks, researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress. To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a range of carefully categorized vision-centric capabilities from object recognition to spatial awareness, counting, and much more. We showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. We find that while scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations. Surprisingly, we also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, we find that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, we also offer guidance on selecting a suitable VLM for a given application. Finally, we release an easy-to-run UniBench code-base with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU. _UniBench_ with model evaluations on all benchmarks are provided as a toolbox at: https://github.com/facebookresearch/unibench

## 1 Introduction

The growing investment in vision-language models (VLMs), capable of a range of open-world multimodal tasks, has spurred the development of numerous benchmarks. Although in principle a more thorough set of evaluations is welcome, the ever-growing number of benchmarks has resulted in a complex, fragmented landscape for evaluation. Researchers are tasked with the heavy burden of implementing the protocol for each benchmark and making sense of how all these benchmarks translate into meaningful axes of progress. Of course, running such a large number of benchmarks also carries a non-trivial computational burden. Consequently, many new models are evaluated only on a _subset of available benchmarks_. When benchmarks are omitted, the research community is faced with blind spots in model strengths and weaknesses. Additionally, comparing the performance of one model versus others becomes challenging as the underlying set of benchmarks is not comparable. Ultimately, drawing well-founded conclusions about the best strategies to advance VLMs in this fragmented landscape of benchmarks is a challenge.

To help researchers navigate this overwhelming landscape of benchmarks and ease the burden of systematically evaluating VLMs, we introduce UniBench. In UniBench we implement 53 visionlanguage model benchmarks in a unified, user-friendly code-base. These benchmarks cover a range of vision-centric capabilities from standard object recognition to spatial understanding, counting, geographic robustness, domain-specific medical and satellite imagery, and many others. With such a comprehensive set of benchmarks, we shine a light on the blind spots in the strengths and weaknesses of the model. Next, to ensure that the research community can translate the many resulting metrics into meaningful axes of progress, we categorize these benchmarks into seven types and seventeen finer-grained capabilities, as shown in Figure 1. Researchers can quickly pinpoint model strengths and weaknesses in a comprehensive, apples-to-apples fashion.

We demonstrate the utility of UniBench by evaluating nearly 60 openly available vision-language models spanning a range of architectures, model sizes, training dataset scales, and learning objectives with scales of up to 12.8B samples and 1B parameters. We systematically compare this diverse set of models across the axes of progress in UniBench. We find that scaling, model size, or training data is a powerful lever for many axes of performance, but offers little benefit for visual relations and reasoning. We also find today's best VLMs struggle with simple benchmarks involving numerical comprehension, even with the right training data, on tasks such as character recognition or counting--including decades old benchmarks such as MNIST and SVHN (LeCun et al., 1998; Netzer et al., 2011). Where scale falls short, we find tailored learning objectives and training data quality are promising levers for relations and reasoning. Finally, we provide practical recommendations on which models practitioners should select. For example, we find large open models such as Eva ViT-E/14 to be a good choice for a general-purpose VLM while models such as NegCLIP excel at specialized tasks such as visual relations.

To facilitate systematic, comparable, yet easy-to-run evaluations we distill the many benchmarks into a few representative evaluations. We provide the UniBench codebase including the 50+ benchmarks with comparisons against all 59 VLMs as well as the distilled set of representative benchmarks that can run in less than 5 minutes on a single A100 GPU. We hope our contribution facilitates thorough and practical evaluation of vision-language model capabilities to faithfully gauge research progress and surface promising strategies to advance VLM research.

## 2 Related Works

### Visual Models From Natural Language Supervision

Visual models trained with natural language supervision have revolutionized computer vision by enabling models to learn rich, joint representations of images and text. A seminal work in this area is CLIP (Contrastive Language-Image Pre-training) introduced by Radford et al. (2021), which demonstrated that pre-training on a large dataset of image-caption pairs using a contrastive objective yields models with remarkable zero-shot transfer capabilities to downstream tasks.

Figure 1: **Benchmark Types in UniBench with their respective performance gains from scaling model size and training dataset size.** Scale offers limited benefits for relational understanding and reasoning tasks.

Following CLIP's success, numerous methods have been proposed to enhance visual models through natural language supervision (Bordes et al., 2024; Jia et al., 2021; Yao et al., 2021; Yu et al., 2022; Li et al., 2022; Singh et al., 2022; Gadre et al., 2023). These models vary in their approaches, including differences in backbone architectures, training objectives (contrastive learning, image-text matching, masked language modeling), and the scale and quality of the training data. To assess the capabilities of these models, the community has developed a diverse set of benchmarks that test various aspects of visual and multimodal understanding (Yuksekgounl et al., 2023; Thrush et al., 2022; Hsieh et al., 2024).

However, the proliferation of benchmarks and models has led to a fragmented evaluation landscape, making it challenging to comprehensively assess and compare models. Different benchmarks emphasize different capabilities, and inconsistent evaluation protocols hinder direct comparison. This fragmentation underscores the need for unified evaluation frameworks like UniBench, which aim to provide a cohesive and comprehensive suite of benchmarks covering a wide range of vision-language understanding tasks.

### CLIP-Style versus LLM-style Evaluation

Evaluation of VLMs has been an active area of research in recent years (Li et al., 2023; Yue et al., 2024; Liu et al., 2024; Salin et al., 2023; Bitton et al., 2023). While these benchmarks provide an insightful perspective of VLM capabilities, they primarily focus on LLM-style evaluations, which generate tokens or text as output. These benchmarks are not suitable for evaluating CLIP-like VLMs, which focus on vision-language classification and understanding capabilities. As a result, they do not allow for direct comparisons with CLIP-Style models.

CLIP-Style evaluation is a widely used approach that calculates the similarity between the image representation and text label. This method focuses on vision-language classification and understanding capabilities, making it particularly useful for evaluating models used as backbone/foundation models for image generation and fine-grain visual tasks (Rombach et al., 2021; Ramesh et al., 2021; Saharia et al., 2022). In contrast, LLM-style evaluation asks the model to demonstrate its knowledge via text generation. While this approach is suitable for evaluating models designed for text-based tasks, it may not be as effective for evaluating models focused on visual tasks. The key difference between CLIP-Style and LLM-style evaluation lies in their respective objectives: CLIP-Style aims to assess a model's ability to align visual and textual representations, whereas LLM-style focuses on assessing a model's ability to generate coherent and accurate text.

UniBench focuses on CLIP-Style evaluation, which provides a more comprehensive understanding of a model's visual reasoning capabilities. By concentrating on traditional zero-shot tasks and predefined choices, we enable an apples-to-apples comparison of progress over the past few years, shedding light on promising directions for future research.

## 3 UniBench: A comprehensive unified evaluation framework for VLMs

Here we describe the benchmarks, protocols, and axes of progress that comprise UniBench as well as the VLMs evaluated.

### VLMs Considered in UniBench

We evaluate 59 openly available VLMs across a range of model sizes, pre-training dataset sizes, learning objectives, and architectures (full list in Appendix Table 6). For training dataset size, we include models trained and/or fine-tuned with datasets ranging from \(13\) million to \(12.8\) billion samples; including DataComp (Gadre et al., 2023) (small, medium, large, and extra-large), LIAON (Schuhmann et al., 2022) (400M, 2B, 5B), MetaCLIP (Xu et al., 2023) (400M and 2.5B), Flickr (Young et al., 2014), PMD (Singh et al., 2022), and COCO (Lin et al., 2015). For model size and architecture, we categorize models based on the number of parameters and whether these models are convolutional or transformer-based models, ranging from ResNet50 (He et al., 2016) with 38 million parameters to EVA02 ViT E (Fang et al., 2023) with 4.3 billion parameters.

Evaluation ProcedureWe evaluate performance of zero-shot classification benchmarks similar to (Radford et al., 2021), by contrasting the representations of class labels (averaged across promptsas defined by Cherti et al. (2022)) with the image representations and using the class with the highest probability as the predicted class. For relation benchmarks, we follow the standard protocol of contrasting correct and incorrect captions with image representations.

### Benchmark Types

To better navigate the overwhelming number of VLM benchmarks, we classify benchmarks into seven distinct types (Figure 1 each covering an key aspect of model performance):

1. **Non-Naural Images:** Consisting of PCam(Veeling et al., 2018), Diabetic Retinopathy(Wang and Yang, 2018), ImageNet Sketch(Wang et al., 2019), imagnetr(Hendrycks et al., 2021a), eurosat(Helber et al., 2019, 2018), and resisc45(Cheng et al., 2017), these benchmarks evaluate the models' ability to handle non-natural images, such as computer-generated graphics, medical images, or satellite imagery.
2. **Object Recognition:** These benchmarks focus on the models' ability to accurately identify and classify objects within images. It includes benchmarks with variety of objects and settings, from everyday items to specific categories like animals or vehicles. Consisting of CUB (Wah et al., 2011), iNaturalist (Van Horn et al., 2018), Pets (Parkhi et al., 2012), MNIST (LeCun et al., 1998), Rendered SST2 (Radford et al., 2021), SVHN (Netzer et al., 2011), Caltech 101 (Fei-Fei et al., 2004), Stanford Cars (Krause et al., 2013), Cifar 10 (Krizhevsky et al., 2009), Cifar 100 (Krizhevsky et al., 2009), Country211 (Radford et al., 2021a), Dollar Street (Gaviria Rojas et al., 2022), FGVC Aircraft (Maji et al., 2013), Flowers 102 (Nilsback and Zisserman, 2008), Food 101 (Bossard et al., 2014), GTSB (Stallkamp et al., 2012), STL-10 (Coates et al., 2011), VOC 2007 (Everingham et al., 2012), ImageNet (Deng et al., 2009), Places365 (Zhou et al., 2017), sun397 (Xiao et al., 2010), MNIST Fashion (Xiao et al., 2017), and PUG: ImageNet (Bordes et al., 2023).
3. **Reasoning:** These benchmarks test the models' capacity to understand relationships between objects, spatial reasoning, and logical inference based on visual input. The benchmarks consist of CLEVR (Johnson et al., 2017), dmlab (Zhai et al., 2019), DSPR (Matthey et al., 2017), Kitti (Geiger et al., 2012), smallNORB (LeCun et al., 2004), and CountBench (Paiss et al., 2023).
4. **Robustness:** These benchmarks evaluates the models' resilience to adversarial attacks and variations in image data. It includes tests with perturbed images to see how well the models can maintain performance under challenging conditions. For example, the ObjectNet benchmark introduces changes in object position, scale, and background, while the ImageNet-R benchmark focuses on transformations related to many types of image renditions. This collection inclues ImageNet-E (Li et al., 2023c), ObjectNet (Barbu et al., 2019), ImageNet-A (Hendrycks et al., 2021b), ImageNet-O (Hendrycks et al., 2021b), ImageNet-9 (Xiao et al., 2020), and ImageNet-V2 (Recht et al., 2019).
5. **Relation:** We include relational benchmarks, such as _Visual Genome_(Yuksekgonul et al., 2023), Winoground (Thrush et al., 2022a), and SugarCrepe (Hsieh et al., 2024) designed to evaluate the models' ability to understand and represent relationships between objects within an image, a crucial aspect of visual understanding. For instance, _Visual Genome_ benchmark includes a variety of relationships (denoted VG-Relation) and attributions (denoted VG-Attribution) tasks, such as spatial relationships (_e.g_, "above", "next to"), action relationships (_e.g_, "riding", "holding"), and appropriate attribution (_e.g_, "the brown horse and the orange cat" vs. "the orange horse and the orange brown").
6. **Texture:** We rely on DTD (Cimpoi et al., 2014) a benchmark focusing on the models' capability to recognize and differentiate textures within images, which is crucial for tasks such as material recognition and scene understanding.
7. **Corruption:** Consisting of ImageNet-C benchmark (Hendrycks and Dietterich, 2019) introduces various types of image corruptions, such as noise, blur, and digital artifacts. These corruptions simulate the types of degradation that images may undergo in real-world scenarios, such as poor lighting conditions, low-quality cameras, or transmission errors.

### Benchmark Capabilities

We further breakdown benchmarks into several capabilities:1. **Depth Estimation, Pose Detection, and Spatial Understanding:** Assessing the models' ability to estimate the depth of objects and scenes from images, and detect object poses which is crucial for understanding spatial relationships.
2. **Medical and Satellite:** Testing the models' performance on medical imaging tasks, such as identifying diseases or conditions from medical scans while testing on satellite imagery requires requires recognizing and interpreting land use, terrain, and other geographic features.
3. **Counting and Character Recognition:** Assessing the models' ability to identify digits and count objects within images, a fundamental skill for quantitative understanding.
4. **Geographic Diversity:** Evaluating the models' capability to recognize and interpret images from diverse geographic locations and settings.
5. **Scene Recognition:** Measuring how well models can identify and classify different scenes or environments.
6. **Standard Object Recognition, ImageNet and Challenging ImageNet:** Evaluating performance on the widely used benchmark for object recognition. We also include the ubiquitous ImageNet and more difficult variants of ImageNet to evaluate model robustness and adaptability.
7. **Specific Classification:** Evaluating models on tasks that require classification of specific categories or fine-grained distinctions between similar objects.
8. **Texture Detection:** Assessing the models' ability to recognize and differentiate various textures within images.
9. **Rendition:** Assessing models' performance on tasks involving rendered or synthetic images, which differ from natural photographs.
10. **Corruptions and Natural Transformations:** Evaluating robustness to image corruptions, such as noise, blur, and other artifacts that degrade image quality whereas natural transformations includes common changes in lighting, rotation, or perspective.

### UniBench: a systematic, practical VLM evaluation

UniBench is framework for comprehensive, fast, and easy-to-use evaluation of VLMs. UniBench also has the ability to expand the existing set of benchmarks and VLMs, as shown in (Code Snippet 1).

```
1importunibench
2fromunibench.models_zoo.wrappers.clipimportClipModel
3fromtorchision.datasetsimportFashionMNIST
4
5evaluator=unibench.Evaluator()
6model=partial(
7ClipModel,
8model=model,
9model_name="vitamin_1_comp1b",
10tokenizer=tokenizer,
11input_resolution=model.visual.image_size[0],
12logit_scale=model.logit_scale,
13)
14evaluator.add_model(model=model)
15class_names=["T-shirt/top",...]
16templates=["animageof{}",...]
17benchmark=partial(FashionMNIST,root="./",train=False,download=True)
18handler=partial(ZeroShotBenchmarkHandler,benchmark_name="
19fashion_mnist_new",classes=class_names,templates=templates)
20evaluator.add_benchmark(benchmark,handler,meta_data={"
21benchmark_type":"objectrecognition"})
22evaluator.evaluate() ```

Code Snippet 1: Running UniBench with a custom model and a new benchmark. UniBench accepts any torchvision dataset type.

## 4 Gauging progress in Vision Language Modeling with UniBench

We show the overall median performance of the nearly 60 VLMs we examined on 53 benchmarks in Figure 2 ranked by their zero-shot classification performance. The results suggest that, while VLMs perform remarkably well on many tasks, for others, VLM performance is near or below random chance level. These results highlight the need for a unifying pipeline to systematically surface model limitations.

### Scaling improves many benchmarks, but offers little benefit for reasoning and relations

Scaling training dataset size hardly helps for reasoning and relations.While scaling training dataset size improves performance across many tasks, this trend does not hold for benchmarks assessing relation understanding and reasoning capabilities. To control for other confounding factors, we fix the architecture, learning paradigm, model size (for right panel), and training dataset size (for left) by using the same CLIP ViT-B/32 model and LAION 400M dataset, respectively Figure 3. The results suggest despite increasing the training dataset size by a factor of \(1000\times\), relational and reasoning benchmarks performance is fairly flat compared to the significant boost in performance on other tasks. We observe a similar trend overall when we include all 59 models in Appendix Figure 7. We specifically pinpoint capabilities such as Depth Estimation, Spatial Understanding, Counting, Scene and Text Recognition, as the underlying capabilities where scale does not lead to improvements as shown in Figure 4.

Scaling model size also offers little to no benefit for reasoning or relations.When we scale models' size from 86 million parameters to 1 billion parameters, we also find that models struggle to scale on similar proportions on relation and reasoning tasks as shown in Figure 3. While for other benchmark types including object recognition, robustness, etc. performance improves by 17.4% as model size scales by \(11\times\), relations and reasoning improve by a modest 3.41% with a fairly flat scaling curve. Similar to scaling training dataset size, scaling model size also offers little benefit for capabilities such as Depth Estimation, Spatial Understanding, Counting, Scene and Text Recognition as shown in Figure 4.

Figure 2: **Median performance of all 59 VLMs on 53 benchmarks, illustrating that despite advancements, VLMs still struggle on several benchmarks. Benchmarks that barely exceed chance-level performance include Winoground, iNaturalist, DSPR, Small Norb, dmlab, Clevr, PCam, Renderedssst2, and Kitti. Blue bars represent the median zero-shot performance of the models, while grey bars indicate the chance-level for each benchmark.**Figure 4: **The effect scaling of training dataset (left) and model size (right) across capabilities for all models.** Accuracy is the difference in performance between the most scaled and the least scaled model across capabilities relative to ImageNet performance.

Figure 3: **The effect of scaling model and training dataset size using a fixed architecture and learning paradigm.** Zero-shot performance of models on various benchmark types. We investigate the impact of training dataset size (left), and model size on various benchmark types (right). To isolate the effect of scale, we fix the architecture, learning paradigm, model size (for right panel), and training dataset size (for left) by using the same CLIP ViT-B/32 model and LAION 400M dataset, respectively. We observe a similar trend when measured across all 59 models as shown in Appendix Figure 7

A Case Study: Digit Recognition and Counting are notable limitations for VLMs even with the right training data

A surprising aspect of VLMs is their poor performance on benchmarks that are traditionally considered straightforward, such as MNIST, CIFAR-10, and CIFAR-100, as shown in Figure 2. For example, a simple 2-layer MLP achieves 99% accuracy on MNIST [22] significantly outperforming all 59 VLMs we studied. To delve deeper into this unexpected result, we controlled for several variables:

1. **VLM confusions go beyond top-1:** To further understand the performance results on MNIST, we compute more generous top-2,-3,-4, and -5 accuracy measures to understand whether models confuse similar digits. We show in Appendix Figure 10 that even when we compute top-5 accuracy (with 50% being chance), VLMs barely reach 90% accuracy suggesting poor performance is not due to minor confusions among digits.
2. **Prompt engineering isn't enough for good performance:** To ensure that the poor performance was not an artifact of the prompts used, we tested multiple hand-crafted prompts that included detailed descriptions of the image characteristics Appendix Figure 9. Despite these tailored prompts, which explicitly described the black-and-white nature and content of the images, the performance still lagged significantly and simpler models.
3. **Training data contains ample samples with digit concept:** We investigated whether the subpar performance could be attributed to a lack of training images containing digit concepts by analyzing the popular LAION 400M dataset. Our findings reveal a substantial number of captions with both word digits (100k-2M) and integer digits (15M-48M) in the training captions, suggesting that the poor performance is not merely due to insufficient training data (see Figure 11 for exact counts by digit).
4. **VLMs struggle on other digit benchmarks:** To further explore whether the poor performance on MNIST is indicative of broader issues in number comprehension, we extend our investigation to other benchmarks such as SVHN, CountBench, and ClevrCount (Appendix Figure 6). We find across all benchmarks VLMs struggled with number recognition and counting tasks.

TakeawayDespite training on vast datasets, even leading VLMs can struggle with simple tasks solved trivially by much smaller models, including tasks involving basic number comprehension. These findings highlight the need for a comprehensive evaluation pipeline that includes so called, simpler benchmarks, to uncover VLM limitations.

Figure 5: **Performance of 59 VLMs on MNIST, showing despite progress, VLMs still struggle on MNIST.** Blue bars represent zero-shot performance of models, grey bars represent the chance-level for MNIST, and green bar shows performance for a 2-Layer MLP.

[MISSING_PAGE_FAIL:9]

## 5 UniBench: A Practical Way Forward for Faster Comprehensive VLM Evaluations

While ideally, evaluating VLMs across all 53 benchmarks would provide the most comprehensive insights, the computational demands and complexity of parsing such extensive data can be overwhelming (6 million images to evaluate; 2+ hours for one model on an A100 GPU). While ImageNet maybe a tempting candidate as it correlates with many benchmarks, for many others, specifically 18 of the 53 benchmarks, ImageNet performance is poorly or negatively correlated Appendix Figure 12. This suggests that success on ImageNet does not universally translate to proficiency in all tasks.

Comprehensive VLM evaluation with UniBench in 5 minutes.To streamline evaluation, we distill the full set of benchmarks in UniBench into seven benchmarks most representative of each axis of progress (via correlations in Appendix A.6). Fortunately, in UniBench this comprehensive set of benchmarks runs in 5 minutes on a single A100 GPU (for ViT-B/32), offering a fast, yet comprehensive evaluation pipeline.

## 6 Discussion

LimitationsWhile we invested a considerable effort to include as comprehensive set of models and benchmarks as possible, there of course will always be new ones we do not cover. We focus especially on vision-centric benchmarks to track progress since the early contrastive vision-language models. To mitigate that, we provide a flexible interfaces to extend UniBench with additional models or benchmarks (see code 1). Our analysis is also limited to the standard zero-shot evaluation protocol.

ImpactTo guide the research community in navigating the overwhelming and fragmented landscape of VLM benchmarks, we introduced UniBench. UniBench provides a unified implementation of 50+ benchmarks, out-of-the-box comparisons across nearly 60 open VLMs, and a distilled fast-to-run set of representative benchmarks that can run on in 5 minutes a single GPU. In doing so, we uncover the limits of scale for reasoning and relations, the promise of data quality and tailored learning objectives, as well as offer recommendations for which VLMs practitioners should use. We hope UniBench is a step towards avoiding the blindspots in VLM evaluations, enabling researchers to comprehensively, yet efficiently evaluate progress.

## References

* Barbu et al. (2019) Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. In _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019. URL https://papers.nips.cc/paper_files/paper/2019/hash/97af07a14cacba681feacf3012730892-Abstract.html.
* Barbu et al. (2019)

\begin{table}
\begin{tabular}{l c c c|c c|c c} \hline \hline \multirow{2}{*}{Benchmark Type} & \multicolumn{2}{c}{Mean} & \multicolumn{2}{c}{**Top**} & \multicolumn{2}{c}{**Top vs Worst Scale**} & \multicolumn{2}{c}{**Worst**} \\ \cline{3-8}  & \multicolumn{1}{c|}{Performance} & \multicolumn{1}{c|}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \\  & & \multicolumn{1}{c|}{Model} & \multicolumn{1}{c|}{Performance} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \\  & & \multicolumn{1}{c|}{Model} & \multicolumn{1}{c|}{Performance} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \\ \hline Corruption & 46.2 & EVA02 ViT E 14 & 74.3 & 153\(\times\) & 50\(\times\) & 2.4 & DataComp ViT B 32 \\ Non-Natural Images & 54.1 & EVA02 ViT E 14 & 74.6 & 153\(\times\) & 50\(\times\) & 16.1 & DataComp ViT B 32 \\ Object Recognition & 55.0 & CLAP ViT G 14 & 71.1 & 98\(\times\) & 21\(\times\) & 12.1 & DataComp ViT W 32 \\ Reasoning & 14.9 & OpenCLIP ViT P 14 & 19.0 & 133\(\times\) & 18\(\times\) & 10.6 & OpenCLIP ResNet101 \\ Relation & 46.7 & NegCLIP ViT B 32 & 66.8 & 30\(\times\) & 1\(\times\) & 33.2 & DataComp ViT B 32 \\ Robustness & 52.1 & EVA02 ViT E 16 & 72.8 & 153\(\times\) & 50\(\times\) & 3.8 & DataComp ViT B 32 \\ Texture & 53.5 & MetaCLIP ViT H 14 & 72.5 & 192\(\times\) & 7\(\times\) & 5.4 & DataComp ViT B 32 \\ \hline Overall & 46.1 & EVA02 ViT E 14 & 61.2 & 153\(\times\) & 50\(\times\) & 12.1 & DataComp ViT B 32 \\ \hline \hline \end{tabular}
\end{table}
Table 1: List of all evaluated benchmark types with their corresponding mean performance across models, the best and worst performing models. The Top vs. Worst Scale shows the proportion difference between the worst and best model on the training dataset size and the model size.

Yonatan Bitton, Hriuk Bansal, Jack Hessel, Rulin Shao, Wanrong Zhu, Anas Awadalla, Josh Gardner, Rohan Taori, and Ludwig Schmidt. Visit-bench: A benchmark for vision-language instruction following inspired by real-world use, 2023. URL https://arxiv.org/abs/2308.06595.
* Bordes et al. (2023) Florian Bordes, Shashank Shekhar, Mark Ibrahim, Diane Bouchacourt, Pascal Vincent, and Ari S. Morcos. Pug: Photorealistic and semantically controllable synthetic data for representation learning, 2023.
* Bordes et al. (2024) Florian Bordes, Richard Yuanzhe Pang, Anurag Ajay, Alexander C Li, Adrien Bardes, Suzanne Petryk, Oscar Manas, Zhiqiu Lin, Anas Mahmoud, Bargav Jayaraman, et al. An introduction to vision-language modeling. _arXiv preprint arXiv:2405.17247_, 2024.
* mining discriminative components with random forests. In _European Conference on Computer Vision_, 2014.
* Cheng et al. (2017) Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. _Proceedings of the IEEE_, 105(10):1865-1883, 2017.
* Cherti et al. (2022) Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning, December 2022. URL http://arxiv.org/abs/2212.07143. arXiv:2212.07143 [cs].
* Cimpoi et al. (2014) M. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed,, and A. Vedaldi. Describing textures in the wild. In _Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)_, 2014.
* Coates et al. (2011) Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 215-223. JMLR Workshop and Conference Proceedings, 2011.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pages 248-255, June 2009. doi: 10.1109/CVPR.2009.5206848. ISSN: 1063-6919.
* Everingham et al. (2007) M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html.
* Fang et al. (2022a) Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks, 2023a.
* Fang et al. (2022) Yuxin Fang, Wen Wang, Binhui Xie, Quan-Sen Sun, Ledell Yu Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. 2023 ieee. In _CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 19358-19369, 2022.
* Fang et al. (2023b) Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual representation for neon genesis. _arXiv preprint arXiv:2303.11331_, 2023b.
* Fei-Fei et al. (2004) Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In _2004 conference on computer vision and pattern recognition workshop_, pages 178-178. IEEE, 2004.
* Gadre et al. (2017) Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets, 2023a. URL https://arxiv.org/abs/2304.14108.
* Ganin et al. (2017)Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets, 2023b.
* Gadre et al. (2024) Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. _Advances in Neural Information Processing Systems_, 36, 2024.
* Rojas et al. (2022) William Gaviria Rojas, Sudnya Diamos, Keertan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. _Advances in Neural Information Processing Systems_, 35:12979-12990, 2022.
* Geiger et al. (2012) Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In _2012 IEEE conference on computer vision and pattern recognition_, pages 3354-3361. IEEE, 2012.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Helber et al. (2018) Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Introducing eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. In _IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium_, pages 204-207. IEEE, 2018.
* Helber et al. (2019) Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. _IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing_, 2019.
* Hendrycks and Dietterich (2019) Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations, 2019.
* Hendrycks et al. (2021a) Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: A critical analysis of out-of-distribution generalization, 2021a.
* Hendrycks et al. (2021b) Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 15262-15271, 2021b.
* Hsieh et al. (2024) Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. _Advances in Neural Information Processing Systems_, 36, 2024.
* Ilharco et al. (2021) Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773. If you use this software, please cite it as below.
* Jia et al. (2021) Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision, 2021. URL https://arxiv.org/abs/2102.05918.
* Johnson et al. (2017) Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2901-2910, 2017.
* Johnson et al. (2018)Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In _Proceedings of the IEEE international conference on computer vision workshops_, pages 554-561, 2013.
* Krizhevsky et al. (2009) Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* LeCun et al. (1998) Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* LeCun et al. (2004) Yann LeCun, Fu Jie Huang, and Leon Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In _Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004._, volume 2, pages II-104. IEEE, 2004.
* Li et al. (2023a) Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension, 2023a. URL https://arxiv.org/abs/2307.16125.
* Li et al. (2022a) Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _ICML_, 2022a.
* Li et al. (2022b) Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation, 2022b. URL https://arxiv.org/abs/2201.12086.
* Li et al. (2023b) Xianhang Li, Zeyu Wang, and Cihang Xie. An inverse scaling law for clip training, 2023b.
* Li et al. (2023c) Xiaodan Li, Yuefeng Chen, Yao Zhu, Shuhui Wang, Rong Zhang, and Hui Xue. ImageNet-E: Benchmarking Neural Network Robustness via Attribute Editing, March 2023c. URL http://arxiv.org/abs/2303.17096. arXiv:2303.17096 [cs].
* Lin et al. (2015) Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015.
* Liu et al. (2023) Haotiian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. URL https://arxiv.org/abs/2304.08485.
* Liu et al. (2024) Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2024. URL https://arxiv.org/abs/2307.06281.
* Maji et al. (2013) S. Maji, J. Kannala, E. Rahtu, M. Blaschko, and A. Vedaldi. Fine-grained visual classification of aircraft. Technical report, 2013.
* Matsuura et al. (2023) Misaki Matsuura, Young Kyun Jung, and Ser Nam Lim. Visual-llm zero-shot classification. 2023.
* Matthey et al. (2017) Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
* Netzer et al. (2011) Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al. Reading digits in natural images with unsupervised feature learning. In _NIPS workshop on deep learning and unsupervised feature learning_, volume 2011, page 7. Granada, Spain, 2011.
* Nilsback and Zisserman (2008) Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In _Indian Conference on Computer Vision, Graphics and Image Processing_, Dec 2008.
* Paiss et al. (2023) Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count to ten. _arXiv preprint arXiv:2302.12066_, 2023.
* Parkhi et al. (2012) Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In _2012 IEEE conference on computer vision and pattern recognition_, pages 3498-3505. IEEE, 2012.
* Parkhi et al. (2013)Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning Transferable Visual Models From Natural Language Supervision, February 2021a. URL http://arxiv.org/abs/2103.00020. arXiv:2103.00020 [cs].
* Radford et al. (2021b) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021b.
* Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021. URL https://arxiv.org/abs/2102.12092.
* Recht et al. (2019) Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet?, 2019.
* Rombach et al. (2021) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.
* Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022. URL https://arxiv.org/abs/2205.11487.
* Salin et al. (2023) Emmanuelle Salin, Stephane Ayache, and Benoit Favre. Towards an exhaustive evaluation of vision-language foundation models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 339-352, 2023.
* Schuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022.
* Singh et al. (2022a) Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model, 2022a.
* Singh et al. (2022b) Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. Flava: A foundational language and vision alignment model, 2022b. URL https://arxiv.org/abs/2112.04482.
* Stallkamp et al. (2012) Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. _Neural networks_, 32:323-332, 2012.
* Thrush et al. (2022a) Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5238-5248, 2022a.
* Thrush et al. (2022b) Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing Vision and Language Models for Visio-Linguistic Compositionality, April 2022b. URL http://arxiv.org/abs/2204.03162. arXiv:2204.03162 [cs].
* Van Horn et al. (2018) Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The naturalist species classification and detection dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 8769-8778, 2018.
* Van den Berg et al. (2015)Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant cnns for digital pathology. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2018: 21st International Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part II 11_, pages 210-218. Springer, 2018.
* Wah et al. [2011] Catherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. _The Caltech-UCSD Birds-200-2011 Dataset_. Jul 2011.
* Wan et al. [2013] Li Wan, Matthew Zeiler, Sixin Zhang, Yann Le Cun, and Rob Fergus. Regularization of neural networks using dropconnect. In Sanjoy Dasgupta and David McAllester, editors, _Proceedings of the 30th International Conference on Machine Learning_, volume 28 of _Proceedings of Machine Learning Research_, pages 1058-1066, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL https://proceedings.mlr.press/v28/wan13.html.
* Wang et al. [2019] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. In _Advances in Neural Information Processing Systems_, pages 10506-10518, 2019.
* Wang and Yang [2018] Zhiguang Wang and Jianbo Yang. Diabetic retinopathy detection via deep convolutional networks for discriminative localization and visual explanation. In _Workshops at the thirty-second AAAI conference on artificial intelligence_, 2018.
* Xiao et al. [2017] Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms, 2017.
* Xiao et al. [2010] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In _2010 IEEE computer society conference on computer vision and pattern recognition_, pages 3485-3492. IEEE, 2010.
* Xiao et al. [2020] Kai Xiao, Logan Engstrom, Andrew Ilyas, and Aleksander Madry. Noise or signal: The role of image backgrounds in object recognition. _ArXiv preprint arXiv:2006.09994_, 2020.
* Xu et al. [2023] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data, 2023.
* Yao et al. [2021] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training, 2021. URL https://arxiv.org/abs/2111.07783.
* Young et al. [2014] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_, 2:67-78, 2014.
* Yu et al. [2022a] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models, 2022a.
* Yu et al. [2022b] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models, 2022b. URL https://arxiv.org/abs/2205.01917.
* Yue et al. [2024] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, 2024. URL https://arxiv.org/abs/2311.16502.
* Yuksekgonul et al. [2022] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it?, October 2022. URL https://arxiv.org/abs/2210.01936v2.
* Yuksekgonul et al. [2023] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why vision-language models behave like bags-of-words, and what to do about it?, March 2023. URL http://arxiv.org/abs/2210.01936. arXiv:2210.01936 [cs].
* Yu et al. [2020]Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with visual concepts, 2022.
* Zhai et al. (2019) Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A large-scale study of representation learning with the visual task adaptation benchmark. _arXiv preprint arXiv:1910.04867_, 2019.
* Zhai et al. (2023) Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training, 2023.
* Zhou et al. (2017) Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2017.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? Please refer to Section 6 3. Did you discuss any potential negative societal impacts of your work? We believe our work is contributing to reliable benchmarking to improve the transparency of model limitations in applications, which view as a positive impact to society. We also reduce computational costs to a minimum (5 minutes on a single GPU) as described in the paper. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them?
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? Did you include complete proofs of all theoretical results? Did you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? The code to run evaluations on 59 VLMs for 53 benchmarks is included in the supplemental material, which includes all evaluation scores with instructions on how to re-generate figures used in the paper. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? Our analysis investigates performance of 59 VLMs for 53 benchmarks, we report the standard error of the mean as error-bar in our figures. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? To compute our evaluations, we have utilized one NVIDIA A100 GPU for each model.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? All benchmarks and VLMs used in the current work has been cited in Section 3.2 and Table 6. 2. Did you mention the license of the assets? We reference the original works for each benchmark used and confirmed each contains a permissive license for research use. 3. Did you include any new assets either in the supplemental material or as a URL? We have not used new assets, we primarily used existing assets from previously published work. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? In
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation?Appendix

### UniBench Implementation Details

We have developed UniBench to be easy-to-run library to allow researchers to systematically compare and contrast existing (n=59 ) and new VLMs on 53 benchmarks. To evaluate new VLMs that expand beyond the already implemented 59 VLMs, users need to follow Code Snippet 2. Users would need to create a class that inherent from ClipModel from uni_bench.models_zoo with get_image_embeddings and get_text_embeddings methods implemented. get_image_embeddings and get_text_embeddings methods takes images and captions as input, respectively, and returns a tensor of encoded representations.

### Natural Language Output Models on UniBench

As described in Section 2.2, LLM-style models defined as models that generate tokens/text as output. Thereby, making them hard to compare with CLIP-style VLMs. In UniBench, we also incorporated LLM-style models in a control experiments. While, LLM-style benchmarks are not suitable for evaluating CLIP-like VLMs, benchmarks in UniBench are capable of testing both LLM and CLIP style models. Following Matsuura et al. (2023) methodology, we evaluated Llava 1.5 (Liu et al., 2023) - a LLM-style VLM - on various benchmark types in UniBench (Table 2). In Table 2, we evaluated 7 and 13 billion scales of Llava.

### Gauging progress in Vision Language Models

Scaling improves many benchmarks, but offers little benefit for reasoning and relation.Appendix Figure 7 shows that despite increasing the training dataset size by a factor of \(1000\times\) and model size by a factor of \(11\times\), relational and reasoning benchmarks performance is fairly flat compared to the significant boost in performance on other tasks. We further pinpoint capabilities such as Depth Estimation, Spatial Understanding, Counting, Scene and Text Recognition, as the underlying capabilities where scale does not lead to improvements as shown in Figure 8.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline Model Name & Corruption & Non-natural Images & Object Recognition & Reasoning & Relation & Robustness & Texture \\ \hline Llava 1.5 13B (Liu et al., 2023) & 31 & 50 & 36 & 11 & 41 & 24 & 34 \\ Llava 1.5 7B (Liu et al., 2023) & 29 & 51 & 32 & 12 & 42 & 23 & 28 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performance (%) of Llava 1.5 on different Benchmark types.

Figure 8: **Benchmark capabilities performance does not scale with dataset and model size** Median zero-shot performance of models on various benchmark capabilities. We investigate the impact of dataset size (left), and model size on various benchmark capabilities (right). We isolate the effect of training data size keeping other factors such as architecture, learning objective, and model size fixed only using ViT B32 (left). For right panel subfigure, we isolate the effect of model size keeping other factors such as architecture, learning objective, and training data size fixed only using LIAON 400M (right).

Figure 7: **The effect of scaling model and training dataset size on all models.** Median zero-shot performance of models on various benchmark types. We investigate the impact of training dataset size (left), and model size on various benchmark types (right).

### Impact of Prompts on MNIST Performance

The MNIST benchmark, featuring handwritten digits, was subjected to various prompting strategies to evaluate their impact on model performance. Our findings reveal a distinct hierarchy in performance based on the type of prompts used. The benchmark was tested with both numeral formats ("zero-nine" and "0-9") and different prompt styles (specialized word prompts, specialized digit prompts, and a basic prompt) (Figure 9).

#### a.4.1 Hierarchy of Prompt Performance

The performance of the MNIST model varied significantly across different prompt types and formats, arranged here from best to worst performing setups: 1. Word digits ("zero-nine") with specialized word prompts 2. Word digits ("zero-nine") with specialized digit prompts 4. Digits ("0-9") with specialized digit prompts 5. Digits ("0-9") with basic prompt 6. Digits ("0-9") with specialized word prompts

#### a.4.2 Specialized Word Prompts

These prompts provided detailed descriptions and contexts, significantly enhancing the model's ability to recognize and interpret the digits accurately. Examples include:

* "showcasing the digit {}, is this image."
* "this number {} is represented in a handwritten form."
* "the numeral {} is captured in this snapshot."
* "the digit {} is depicted visually in this image."
* "this image is a graphical representation of the number {}."
* "this is an illustration of the digit {}."
* "this image represents the digit {} in a handwritten form."
* "the number {} is sketched as a digit in this image."
* "this is a photograph of the digit {}."
* "the number {} is drawn as a digit in this image."

#### a.4.3 Specialized Digit Prompts

These prompts explicitly mention the format or style of the digit, aiding in recognition but to a lesser extent compared to specialized word prompts. Examples include:

* "A photo of the number: '{}'."
* "A digit drawing of the number: '{}'."
* "A digit sketch of the number: '{}'."
* "A handwritten digit image of: '{}'."
* "A digit illustration of: '{}'."
* "A graphical representation of the number: '{}'."
* "A visual depiction of the digit: '{}'."
* "A snapshot of the numeral: '{}'."
* "A handwritten representation of the number: '{}'."
* "An image showcasing the digit: '{}'."

#### a.4.4 Basic Prompt

The basic prompt used:

* "a photo of the number: '{}'."This structured analysis clearly demonstrates how the specificity and relevance of the prompt significantly influence the performance of VLMs. We investigated whether the subpar performance could be attributed to a lack of training images containing digit concepts by analyzing the popular LAION 400M dataset. Our findings reveal a substantial number of captions with both word digits (100k-2M) and integer digits (15M-48M) in the training captions, suggesting that the poor performance is not merely due to insufficient training data (see Figure 11 for exact counts by digit). To further understand the performance results on MNIST, we compute more generous top-2,-3,-4, and -5 accuracy measures to understand whether models confuse similar digits. We show in Appendix Figure 10 that even when we compute top-5 accuracy (with 50% being chance), VLMs barely reach 90% accuracy suggesting poor performance is not due to minor confusions among digits.

Figure 10: **Median performance of 59 VLMs on MNIST while varying accuracy measure from top-1 to top-5**. The following further shows that VLMs performance on MNIST is not due mismatch between top-1 and top-5 guesses. Blue bars represent the median zero-shot performance of models and red bars represents the chance-level for benchmarks.

Figure 9: **Median performance of 59 VLMs on MNIST while varying prompts and labels**. Blue bars represent the median zero-shot performance of models and dashed-grey line represents the chance-level for MNIST.

### Correlation of ImageNet with Other Benchmarks

ImageNet, often considered a cornerstone in the field of computer vision, has been widely used as a benchmark to evaluate the performance of image recognition models. Its extensive dataset and challenging classification tasks have set a standard for algorithm development and comparison. However, while ImageNet correlates well with many benchmarks, it does not exhibit a universal correlation across all tasks. Our analysis reveals that for a significant number of benchmarks, specifically 18 out of the 53 benchmarks analyzed, the performance on ImageNet is poorly or negatively correlated. This is illustrated in Appendix Figure 12, which provides a detailed comparison of benchmark performances. This finding suggests that success on ImageNet does not necessarily translate to proficiency in all visual tasks.

### A Practical Subset of Benchmarks

While ideally, evaluating VLMs across all 53 benchmarks would provide the most comprehensive insights, the computational demands and complexity of parsing such extensive data can be overwhelming (6 million images to evaluate; 2+ hours for one model on an A100 GPU). To streamline evaluation, we distill the full set of benchmarks in UniBench into seven benchmark types and 17 capabilities. These categorizations are based on benchmarks that correlate strongly with other benchmarks within each benchmark type and capability (Tables 3 and 4).

### Weighted Average Performance

To account for the varying difficulties across tasks, we compute the weighted average performance of each model by normalizing their scores relative to the performance of CLIP B/32. We use CLIP B/32 as a baseline because its performance effectively captures the inherent complexity of each task, serving as a proxy for task difficulty.

Figure 13 illustrates the normalization results in lower overall performance scores for all models. However, it does not affect the relative rankings among them. This consistency suggests that while task difficulty impacts absolute performance metrics, the comparative effectiveness of the models remains stable across different levels of task complexity.

Figure 11: **Frequency of different digits in LAION-400M, showing substantial frequency of digits in visual diet of VLMs. Left panel counts the number of words of the digits i.e. [zero-nine] and right panel counts the number of digits in LAION-400M.**Figure 12: **Correlation matrix of models performance across all benchmarks.**

Figure 13: **Weighted Average Performance** for each model using CLIP B/32 as the baseline model performance (as a proxy for task difficulty

\begin{table}
\begin{tabular}{l c c} \hline \hline \multirow{2}{*}{Capabilities} & Most Correlated & \multirow{2}{*}{Correlation Value} \\  & Benchmark & \\ \hline standard object recognition & food101 & 0.85 \\ counting & countbench & 0.76 \\ spatial understanding & dspr y position & 0.29 \\ relations & vg attribution & 0.57 \\ geographic diversity & dollar street & 0.89 \\ specifies classification & flowers102 & 0.7 \\ depth estimation & dmlab & 0.42 \\ pose detection & smallnorb azimuth & 0.57 \\ texture detection & dtd & 1 \\ satellite & eurosat & 0.95 \\ character recognition & mnist & 0.88 \\ imagenet & imagenet1k & 1 \\ natural transformations & imagenet9 & 0.99 \\ rendition & imagenetr & 0.97 \\ challenging imagenet & imagenetv2 & 0.65 \\ corruption & imagenetc & 1 \\ medical & retinopathy & 0.64 \\ scene recognition & sun397 & 0.99 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Evaluate on a curated list of capabilities, rather than the full set, to save time.** The list includes benchmarks that correlate strongly with other benchmarks for each capability.

\begin{table}
\begin{tabular}{l c c} \hline \hline \multirow{2}{*}{Benchmark Type} & Most Correlated & \multirow{2}{*}{Correlation Value} \\  & Benchmark & \\ \hline Object recognition & ImageNet-1k & 0.82 \\ Reasoning (Counting) & CountBench & 0.76 \\ Reasoning (Spatial) & DSPR Position & 0.29 \\ Relation & VG Attribution & 0.57 \\ Texture & DTD & 1 \\ Non-Natural Images & Resisc45 & 0.72 \\ Robustness & ImageNet-v2 & 0.81 \\ Corruption & ImageNet-c & 1 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Evaluate on a curated list of benchmark types, rather than the full set, to save time.** The list includes benchmarks that correlate strongly with other benchmarks for each benchmark type.

\begin{table}
\begin{tabular}{l c c c|c c|c c} \hline \hline  & \multicolumn{2}{c}{Mean} & \multicolumn{2}{c}{**Top**} & \multicolumn{2}{c}{**Top vs Worst Scale**} & \multicolumn{2}{c}{**Worst**} \\ Benchmark Type & Performance & & & & & & & \\  & & & & & & & & \\  & & & & & & & & \\  & & & & & & & & \\  & & & & & & & & \\  & & & & & & & & \\ \hline Challenging Imagenet & 47.8 & EVA02 ViT E 14 & 64.4 & 153 & 50 & 5.0 & DataComp ViT B 32 \\ Character Recognition & 54.8 & CLIPA ViT G 14 & 74.3 & 85 & 48 & 20.5 & OpenCLIP ResNet50 \\ Corruption & 46.1 & EVA02 ViT E 14 & 74.3 & 153 & 50 & 2.3 & DataComp ViT B 32 \\ Counting & 31.4 & OpenCOCA ViT L 14 & 53.1 & 153 & 3 & 11.5 & DataComp ViT B 32 \\ Depth Estimation & 20.4 & DataComp ViT B 16 & 27.6 & 0.6 & 0.1 & 12.4 & OpenCLIP ViT H 14 \\ Geographic Diversity & 33.8 & CLIPA ViT G 14 & 46.8 & 98 & 21 & 5.3 & DataComp ViT B 32 \\ Imagenet & 65.7 & OpenCLIP ViT H 14 & 83.1 & 384 & 7 & 3.9 & DataComp ViT B 32 \\ Medical & 43.3 & MetaCLIP ViT L 14 & 68.6 & 0.3 & 3 & 26.8 & DataComp ViT B 16 \\ Natural Transformations & 56.2 & CLIPA ViT G 14 & 81.7 & 98 & 21 & 2.5 & DataComp ViT B 32 \\ Pose Detection & 3.9 & OpenCLIP ViT B 32 & 4.7 & 5 & 0.9 & 3.3 & OpenCLIP ConvNet \\ Relations & 46.7 & NegCLIP ViT B 32 & 66.7 & 30 & 1 & 33.2 & DataComp ViT B 32 \\ Rendition & 63.7 & CLIPA ViT G 14 & 84.2 & 98 & 21 & 3.8 & DataComp ViT B 32 \\ Satellite & 55.2 & EVA02 ViT E 14 & 75.7 & 153 & 50 & 12.3 & DataComp ViT B 32 \\ Scene Recognition & 53.0 & OpenCLIP ViT H 14 & 61.7 & 384 & 7 & 6.3 & DataComp ViT B 32 \\ Spatial Understanding & 9.1 & MetaCLIP ViT L 14 & 11.3 & 1 & 3 & 6.3 & CLIP ResNet50x4 \\ Specifics Classification & 51.7 & OpenCLIP ViT H 14 & 68.9 & 384 & 7 & 2.8 & DataComp ViT B 32 \\ Standard Object Recognition & 60.0 & CLIPA ViT G 14 & 77.1 & 98 & 21 & 13.8 & DataComp ViT B 32 \\ Texture Detection & 53.4 & MetaCLIP ViT H 14 & 72.4 & 192 & 7 & 5.3 & DataComp ViT B 32 \\ \hline Overall & 44.2 & EVA02 ViT E 14 & 58.0 & 153 & 50 & 11.3 & DataComp ViT B 32 \\ \hline \hline \end{tabular}
\end{table}
Table 5: List of all evaluated capabilities with their corresponding mean performance across models, the best and the worst performing models. The Top vs. Worst Scale shows the proportion difference between the worst and best model on the training dataset size and the model size.

[MISSING_PAGE_FAIL:26]

\begin{table}
\begin{tabular}{l l l l l r r} \hline \hline Benchmark & Measure & Benchmark Type & Capability & Curated & \multirow{2}{*}{Object Centric} & \multicolumn{2}{c}{Number of Classes} \\ \cline{4-5}  & & & & & & \\ \hline caltech101 [Fei-Fei et al., 2004] & zero-shot & object recognition & standard object recognition & False & True & 102 \\ cars [Krause et al., 2013] & zero-shot & object recognition & standard object recognition & False & True & 196 \\ cifar10 [Krizhevsky et al., 2009] & zero-shot & object recognition & standard object recognition & False & True & 100 \\ cifar100 [Krizhevsky et al., 2009] & zero-shot & object recognition & standard object recognition & False & True & 100 \\ clev count [Johnson et al., 2017] & zero-shot & reasoning & counting & True & False & 8 \\ clever distance [Johnson et al., 2017] & zero-shot & reasoning & spatial understanding & True & False & 6 \\ coro order [Yuksekgouni et al., 2023] & relation & relation & relations & False & False & 5 \\ counbench [Paiss et al., 2023] & zero-shot & reasoning & counting & False & False & 10 \\ country211 [Radford et al., 2021a] & zero-shot & object recognition & geographic diversity & False & False & 211 \\ cub [Wah et al., 2011] & zero-shot & object recognition & specifics classification & False & False & 200 \\ dmlab [Zhai et al., 2019] & zero-shot & reasoning & depth estimation & True & False & 6 \\ dollar street [Gavia Rojas et al., 2022] & zero-shot & object recognition & geographic diversity & False & True & 60 \\ dspr orientation [Matthey et al., 2017] & zero-shot & reasoning & pose detection & True & False & 40 \\ dspr x position [Matthey et al., 2017] & zero-shot & reasoning & spatial understanding & True & False & 32 \\ ddp x position [Matthey et al., 2017] & zero-shot & reasoning & spatial understanding & True & False & 32 \\ ddl [Kimpi et al., 2014] & zero-shot & texture & texture detection & True & False & 47 \\ eurosat [Helber et al., 2019, 2018] & zero-shot & non-natural images & satellite & False & False & 10 \\ fashion must [Xiao et al., 2017] & zero-shot & object recognition & character recognition & True & True & 10 \\ fyfe aircraft [Maji et al., 2013] & zero-shot & object recognition & standard object recognition & False & True & 100 \\ flickick30k order [Yuksekgouni et al., 2023] & relation & relation & relations & False & False & 5 \\ flowers102 [Nilsback and Zisserman, 2008] & zero-shot & object recognition & specifics classification & False & True & 102 \\ food101 [Bossard et al., 2014] & zero-shot & object recognition & standard object recognition & False & True & 101 \\ gistb [Sulkamp et al., 2012] & zero-shot & object recognition & standard object recognition & False & True & 43 \\ magnetic1k [Deng et al., 2009] & zero-shot & object recognition & imagenet & False & True & 1000 \\ imagenet9[Xiao et al., 2020] & zero-shot & robustness & natural transformations & True & True & 1000 \\ imagenet sketch [Wang et al., 2019] & zero-shot & non-natural images & rendition & True & True & 1000 \\ imageneten [Hendrycks and Dietterich, 2019] & zero-shot & robustness & challenging imagenet & True & True & 200 \\ imagenet [Helberds et al., 2019] & zero-shot & corruption & corruption & True & True & 1000 \\ imagenet [Li et al., 2023c] & zero-shot & robustness & natural transformations & True & True & 1000 \\ imagenet [Hendrycks et al., 2021b] & zero-shot & robustness & challenging imagenet & True & True & 200 \\ imagenet [Hendrycks et al., 2021a] & zero-shot & non-natural images & rendition & True & True & 200 \\ imagenetv2 [Recht et al., 2019] & zero-shot & robustness & challenging imagenet & True & True & 1000 \\ iintarulst [Van Horn et al., 2018] & zero-shot & object recognition & specifics classification & False & True & 5089 \\ kitti distance [Geiger et al., 2012] & zero-shot & reasoning & depth estimation & False & False & 4 \\ mnist[LeCun et al., 1998] & zero-shot & object recognition & character recognition & True & True & 10 \\ objectnet [Barbu et al., 2019] & zero-shot & robustness & natural transformations & False & True & 113 \\ pcam [Veeing et al., 2018] & zero-shot & non-natural images & medical & True & False & 2 \\ pets [Parkhi et al., 2012] & zero-shot & object recognition & specifics classification & False & True & 37 \\ places365 [Zhou et al., 2017] & zero-shot & object recognition & scene recognition & False & False & 365 \\ pug imagenet [Bordes et al., 2023] & zero-shot & object recognition & standard object recognition & False & True & 151 \\ renderedest2 [Radford et al., 2021a] & zero-shot & object recognition & character recognition & True & True & 2 \\ resics45[Cheng et al., 2017] & zero-shot & non-natural images & satellite & False & False & 45 \\ retinopathy [Wang and Yang, 2018] & zero-shot & non-natural images & medical & False & False & 5 \\ smallhorb azimuth [LeCun et al., 2004] & zero-shot & reasoning & pose detection & True & False & 18 \\ smallhorb elevation [LeCun et al., 2004] & zero-shot & reasoning & spatial understanding & True & False & 9 \\ sall10 [Coates et al., 2011] & zero-shot & object recognition & standard object recognition & False & True & 10 \\ sugarcepe [Hsieh et al., 2024] & relation & relation & relations & False & False & 2 \\ sun397 [Xiao et al., 2010] & zero-shot & object recognition & scene recognition & False & False & 397 \\ swhn [Netzer et al., 2011] & zero-shot & object recognition & character recognition & False & True & 10 \\ cy attribution [Yuksekgouni et al., 2023] & relation & relation & relations & False & False & 2 \\ vg relation [Yuksekgouni et al., 2023] & relation & relation & relations & False & False & 2 \\ voz2007 [Everingham et al.] & zero-shot & object recognition & standard object recognition & False & True & 20 \\ winoground [Thursh et al., 2022a] & relation & relation & relations & False & False & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 7: List of all the benchmarks used in evaluations with their corresponding dataset type, capability, number of classes, whether they are curated and whether they are curated object centric.