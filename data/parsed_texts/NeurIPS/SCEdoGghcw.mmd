# Measuring Progress in Dictionary Learning

for Language Model Interpretability

with Board Game Models

 Adam Karvonen

Independent

Benjamin Wright1

MIT

Can Rager

Independent

Rico Angell

UMass, Amherst

Jannik Brinkmann

University of Mannheim

Logan Smith

Independent

Claudio Mayrink Verdun

Harvard University

David Bau

Northeastern University

&Samael Marks

Northeastern University

Equal contribution. Correspondence to: Adam Karvonen <adam.karvonen@gmail.com>.

###### Abstract

What latent features are encoded in language model (LM) representations? Recent work on training sparse autoencoders (SAEs) to disentangle interpretable features in LM representations has shown significant promise. However, evaluating the quality of these SAEs is difficult because we lack a ground-truth collection of interpretable features that we expect good SAEs to recover. We thus propose to measure progress in interpretable dictionary learning by working in the setting of LMs trained on chess and Othello transcripts. These settings carry natural collections of interpretable features--for example, "there is a knight on F3"--which we leverage into _supervised_ metrics for SAE quality. To guide progress in interpretable dictionary learning, we introduce a new SAE training technique, _\(p\)-annealing_, which improves performance on prior unsupervised metrics as well as our new metrics.2

## 1 Introduction

Mechanistic interpretability aims to reverse engineer neural networks into human-understandable components. What, however, should these components be? Recent work has applied Sparse Autoencoders (SAEs) [9; 16], a scalable unsupervised learning method inspired by sparse dictionary learning to find a disentangled representation of language model (LM) internals. However, measuring progress in training SAEs is challenging because we do not know what a gold-standard dictionary would look like, as it is difficult to anticipate which ground-truth features underlie model cognition. Prior work has either attempted to measure SAE quality in toy synthetic settings [57] or relied on various proxies such as sparsity, fidelity of the reconstruction, and LM-assisted autointerpretability [6].

In this work, we explore a setting that lies between toy synthetic data (where all ground-truth features are known; cf. Elhage et al. [24]) and natural language: LMs trained on board game transcripts. This setting allows us to formally specify natural categories of interpretable features, e.g., "there is a knight on e3" or "the bishop on f5 is pinned." We leverage this to introduce two novel metrics for how much of a model's knowledge an SAEs has captured:* **Board reconstruction.** Can we reconstruct the state of the game board by interpreting each feature as a classifier for some board configuration?
* **Coverage.** Out of a catalog of researcher-specified candidate features, how many of these candidate features actually appear in the SAE?

These metrics carry the limitation that they are sensitive to researcher preconceptions. Nevertheless, we show that they provide a useful new signal of SAE quality.

Additionally, we introduce \(p\)_-annealing_, a novel technique for training SAEs. When training an SAE with \(p\)-annealing, we use an \(L_{p}\)-norm-based sparsity penalty with \(p\) ranging from \(p=1\) at the beginning of training (corresponding to a convex minimization problem) to some \(p<1\) (a non-convex objective) by the end of training. We demonstrate that p-annealing improves over prior methods, giving performance on par with the more compute-intensive Gated SAEs from Rajamanoharan et al. [54], as measured both by prior metrics and our new metrics.

Overall, our main contributions are as follows:

1. We train and open-source over 500 SAEs trained on chess and Othello models each.
2. We introduce two new metrics for measuring the quality of SAEs.
3. We introduce \(p\)-annealing, a novel technique for training SAEs that improves on prior techniques.

## 2 Background

### Language models for Othello and chess

In this work, we make use of LMs trained to autoregressively predict transcripts of chess and Othello games. We emphasize that these transcripts only give lists of moves in a standard notation and do not directly expose the board state. Based on behavioral evidence (the high accuracy of the LMs for predicting legal moves) and prior studies of LM representations [36; 47; 33] we infer that the LMs internally model the board state, making them a good testbed setting for studying LM representations.

Othello.Othello is a two-player strategy board game played on an 8x8 grid, with players using black and white discs. Players take turns placing discs on the board, capturing their opponent's discs by bracketing them between their own, causing the captured discs to turn their color. The goal is to have more discs turned to display your color at the end of the game. The game ends if every square on the board is covered or either player cannot make a move.

In our experiments, we use an 8-layer GPT model with 8 attention heads and a \(n=512\) dimensional hidden space, as provided by Li et al. [36]. This model had no prior knowledge of the game or its rules and was trained from scratch on 20 million game transcripts, where each token in the corpus represents a tile on which players place their discs. The game transcripts were synthetically generated by uniformly sampling from the Othello game tree. Thus, the data distribution captures valid move sequences rather than strategic depth. For this model, Li et al. [36] demonstrated the emergence of a world model--an internal representation of the correct board state allowing it to predict the next move--that can be extracted from the model activations using a non-linear probe. Nanda et al. [47] extended this finding, showing that a similar internal representation could be extracted using linear probes, supporting the linear representation hypothesis [46].

Chess.Othello makes a natural testbed for studying emergent internal representations since the game tree is far too large to memorize. However, the rules and state are not particularly complex. Therefore, we also consider a language model trained on chess game transcripts with identical architecture, provided by Karvonen [33]. The model again had no prior knowledge of chess and was trained from scratch on 16 million human games from the Lichess chess game database [38]. The input to the model is a string in the Portable Game Notation (PGN) format (e.g., "1. e4 e5 2. Nf3 \(\cdots\)"). The model predicts a legal move in 99.8 % of cases and, similar to Othello, it has an internal representation of the board state that can be extracted from the internal activations using a linear probe [33].

### Sparse autoencoders

Given a dataset \(\mathcal{D}\) of vectors \(\mathbf{x}\in\mathbb{R}^{d}\), a sparse autoencoder (SAE) is trained to produce an approximation

\[\mathbf{x}\approx\sum_{i}^{d_{\text{SAE}}}f_{i}(\mathbf{x})\mathbf{d}_{i}+ \mathbf{b}\] (1)

as a sparse linear combination of _features_. Here, the _feature vectors_\(\mathbf{d}_{i}\in\mathbb{R}^{d}\) are unit vectors, the _feature activations_\(f_{i}(\mathbf{x})\geq 0\) are a sparse set of coefficients, and \(\mathbf{b}\in\mathbb{R}^{d}\) is a bias term. Concretely, an SAE is a neural network with an encoder-decoder architecture, where the encoder maps \(\mathbf{x}\) to the vector \(\mathbf{f}=[f_{1}(\mathbf{x})\quad\dots\quad f_{d_{\text{SAE}}}(\mathbf{x})]\) of feature activations, and the decoder maps \(\mathbf{f}\) to an approximate reconstruction of \(\mathbf{x}\).

In this paper, we train SAEs on datasets consisting of activations extracted from the residual stream after the sixth layer for both the chess and Othello models. At these layers, linear probes trained with logistic regression were accurate for classifying a variety of properties of the game board [33; 47]. For training SAEs, we employ a variety of SAE architectures and training algorithms, as detailed in Section 4.

## 3 Measuring autoencoder quality for chess and Othello models

Many of the features learned by our SAEs reflect uninteresting, surface-level properties of the input, such as the presence of certain tokens. However, upon inspection, we additionally find many SAE features which seem to reflect a latent model of the board state, e.g., features that reflect the presence of certain pieces on particular squares, squares that are legal to play on, and strategy-relevant properties like the presence of a pin in chess (Figures 1 and 6).

Fortunately, in the setting of board games, we can formally specify certain classes of these interesting features, allowing us to more rigorously detect them and use them to understand our SAEs. In Section 3.1, we specify certain classes of interesting game board properties. Then, in Section 3.2, we leverage these classes into two metrics of SAE quality.

### Board state properties in chess and Othello models

We formalize a _board state property_ (BSP) to be a function \(g:\{\text{game board}\}\to\{0,1\}\). In this work, we will consider the following interpretable classes of BSPs:

* \(\mathcal{G}_{\text{board state}}\) contains BSPs which classify the presence of a piece at a specific board square, where the board consists of \(8\times 8\) squares in both games. For chess, we consider the full board for the twelve

Figure 1: We find SAE features that detect interpretable board state properties (BSP) with high precision (i.e., above 0.95). This figure illustrates three distinct chessboard states, each an example of a BSP associated with a high activation of a particular SAE feature. Left: A **board state detector** identifies a knight on square f3, owned by the player to move. Middle: A **rook threat detector** indicates an immediate threat posed by a rook to a queen regardless of location and piece threatened. Right: A **pin detector** recognizes moves that resolve a check on a diagonal by creating a pin, again, regardless of location and piece pinned.

distinct piece types (white king, white queen,..., black king), giving a total of \(8\times 8\times 12\) BSPs. For Othello, we consider the full board for the two distinct piece types (black and white), yielding \(8\times 8\times 2\) BSPs.
* \(\mathcal{G}_{\text{strategy}}\) consists of BSPs relevant for predicting legal moves and playing strategically in chess, such as a pin detector. They were selected by the authors based on domain knowledge and prior interpretability work on the chess model AlphaZero [45]. We provide a full list of strategy BSPs in Table 3 in the Appendix. Because our Othello model was trained to play random legal moves, we do not consider strategy BSPs for Othello.

### Measuring SAE quality with board state properties

In this section, we introduce two metrics of SAE quality: coverage and board reconstruction.

Coverage.Given a collection \(\mathcal{G}\) of BSPs, our coverage metric quantifies the extent to which an SAE has identified features that coincide with the BSPs in \(\mathcal{G}\). In more detail, suppose that \(f_{i}\) is an SAE feature and \(t\in[0,1]\) is a threshold, we define the function

\[\phi_{f_{i},t}(\mathbf{x})=\mathbb{I}\left[f_{i}(\mathbf{x})>t\cdot f_{i}^{ \text{max}}\right]\] (2)

where \(f_{i}^{\text{max}}\) is (an empirical estimate of) \(\max_{\mathbf{x}\sim\mathcal{D}}f_{i}(\mathbf{x})\), the maximum value that \(f_{i}\) takes over the dataset \(\mathcal{D}\) of activations extracted from our model, and \(\mathbb{I}\) is the indicator function. We interpret \(\phi_{f_{i},t}\) as a binary classifier; intuitively, it corresponds to binarizing the activations of \(f_{i}\) into "on" vs. "off" at some fraction \(t\) of the maximum value of \(f_{i}\) on \(\mathcal{D}\). Given some BSP \(g\in\mathcal{G}\), let \(F_{1}(\phi_{f_{i},t};g)\in[0,1]\) denote the F1-score for \(\phi_{f_{i},t}\) classifying \(g\). Then we define the _coverage_ of an SAE with features \(\{f_{i}\}\) relative to a set of BSPs \(\mathcal{G}\) to be

\[\text{Cov}(\{f_{i}\},\mathcal{G}):=\frac{1}{|\mathcal{G}|}\sum_{g\in\mathcal{ G}}\max_{t}\max_{f_{i}}F_{1}(\phi_{f_{i},t};g).\] (3)

In other words, we take, for each \(g\in\mathcal{G}\), the \(F_{1}\)-score of the feature that best serves as a classifier for \(g\), and then take the mean of these maximal \(F_{1}\)-scores. An SAE receives a coverage score of \(1\) if, for each BSP \(g\in\mathcal{G}\), it has some feature that is a perfect classifier for \(g\). Since Cov depends on the choice of threshold \(t\), we sweep over \(t\in\{0,0.1,0.2,\dots,0.9\}\) and take the best coverage score; typically this best \(t\) is in \(\{0,0.1,0.2\}\).

Board reconstruction.Again, let \(\mathcal{G}\) be a set of BSPs. Intuitively, the idea of our board reconstruction metric is that, for a sufficiently good SAE, there should be a simple, human-interpretable way to recover the state of the board from the profile of feature activations \(\{f_{i}(\mathbf{x})\}\) on an activation \(\mathbf{x}\in\mathbb{R}^{d}\). Here, the activation \(\mathbf{x}\) was extracted after the post-MLP residual connection in layer 6.

We will base our board reconstruction metric around the following human-interpretable way of recovering a board state from a feature activation profile; we emphasize that different ways of recovering boards from feature activations may lead to qualitatively different results. This recovery rule is based on the assumption that interpretable SAE features tend to be _high precision_ for some subset of BSPs, in line with Templeton et al. [58]. For example, features that classify common configurations of pieces are high precision (but not necessarily high recall) for multiple BSPs. We use a consistent dataset of 1000 games as our training set \(\mathcal{D}_{\text{train}}\) for identifying high-precision features across all Board State Properties (BSPs). An additional, separate set of 1000 games serves as our test set \(\mathcal{D}_{\text{test}}\). Using the training set \(\mathcal{D}_{\text{train}}\), we identify, for each SAE feature \(f_{i}\), all of the BSPs \(g\in\mathcal{G}\) for which \(\phi_{f_{i},t}\) is a _high precision_ (of at least \(0.95\)) classifier. Then for each \(g\in\mathcal{G}\) our prediction rule is

\[\mathcal{P}_{g}(\{f_{i}(\mathbf{x})\})=\begin{cases}1,&\text{if $\phi_{f_{i},t}( \mathbf{x})=1$ for any $f_{i}$ which}\\ &\text{is high precision for $g$ on $\mathcal{D}_{\text{train}}$}\\ 0,&\text{otherwise.}\end{cases}\] (4)

Let \(F_{1}(\boldsymbol{\mathcal{P}}(\{f_{i}(\mathbf{x})\});\mathbf{b})\) denote the \(F_{1}\)-score for a given board state \(\mathbf{b}\), where \(\boldsymbol{\mathcal{P}}(\{f_{i}(\mathbf{x})\})=\{\mathcal{P}_{g}(\{f_{i}( \mathbf{x})\})\}_{g\in\mathcal{G}}\) represents the full predicted board (containing predictions for all 64 squares) obtained from the SAE activations.3Then, the average \(F_{1}\)-score over all board states in the test dataset \(\mathcal{D}_{\text{test}}\) can be calculated as

\[\text{Rec}(\{x_{i}\},\mathcal{D}_{\text{test}})=\frac{1}{|\mathcal{D}_{\text{ test}}|}\sum_{\mathbf{x}\in\mathcal{D}_{\text{test}}}\max_{t}F_{1}(\bm{ \mathcal{P}}(\{f_{i}(\mathbf{x})\});\mathbf{b}).\] (5)

## 4 Training methodologies for SAEs

In our experiments, we investigate four methods for training SAEs, as explained in this section. These are given by two autoencoder architectures and two training methodologies--one with \(p\)-annealing and one without \(p\)-annealing--for each architecture. Our SAEs are available at https://huggingface.co/adamkarvonen/chess_saes/tree/main (chess) and https://huggingface.co/adamkarvonen/othello_saes/tree/main (Othello).

### Standard SAEs

Let \(n\) be the dimension of the model's residual stream activations that are input to the autoencoder, \(m\) the autoencoder hidden dimension, and \(s\) the dataset size. Our baseline "standard" SAE architecture, as introduced in Bricken et al. [9] is defined by encoder weights \(W_{e}\in\mathbb{R}^{m\times n}\), decoder weights \(W_{d}\in\mathbb{R}^{n\times m}\) with columns constrained to have a \(L_{2}\)-norm of 1, and biases \(b_{e}\in\mathbb{R}^{m}\), \(b_{d}\in\mathbb{R}^{n}\). Given an input \(\mathbf{x}\in\mathbb{R}^{n}\), the SAE computes

\[\mathbf{f}(\mathbf{x}) =\text{ReLU}(W_{e}(\mathbf{x}-\mathbf{b}_{d})+\mathbf{b}_{e})\] (6) \[\mathbf{\hat{x}} =W_{d}\,\mathbf{f}(\mathbf{x})+\mathbf{b}_{d}\] (7)

where \(\mathbf{f}(\mathbf{x})\) is the vector of feature activations, and \(\mathbf{\hat{x}}\) is the reconstruction. For a standard SAE, our baseline training method is as implemented in the open-source dictionary_learning repository [43], optimizing the loss

\[\mathcal{L}_{\text{standard}}=\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{\text{ train}}}\Big{[}\|\mathbf{x}-\mathbf{\hat{x}}\|_{2}+\lambda\|\mathbf{f}( \mathbf{x})\|_{1}\Big{]}.\] (8)

for some hyperparameter \(\lambda>0\) controlling sparsity.

### Gated SAEs

The \(L_{1}\) penalty used in the original training method encourages feature activations to be smaller than they would be for optimal reconstruction [62]. To address this, Rajamanoharan et al. [54] introduced a modification to the original SAE architecture that separates the selection of dictionary elements to use in a reconstruction and estimating the coefficients of these dictionary elements. This results in the following gated architecture:

\[\pi_{\text{gate}}(\mathbf{x}):=W_{\text{gate}}(\mathbf{x}-\mathbf{b}_{d})+ \mathbf{b}_{\text{gate}}\] (9)

\[\mathbf{\tilde{f}}(\mathbf{x}):=\mathbb{I}\left[\pi_{\text{gate}}(\mathbf{x}) >0\right]\odot\text{ReLU}(W_{\text{mag}}(\mathbf{x}-\mathbf{b}_{d})+\mathbf{ b}_{\text{mag}})\] (10)

\[\hat{x}(\mathbf{\tilde{f}}(\mathbf{x}))=W_{d}\mathbf{\tilde{f}}(\mathbf{x})+ \mathbf{b}_{d}\] (11)

where \(\mathbb{I}[:>0]\) is the Heaviside step function and \(\odot\) denotes elementwise multiplication. Then, the loss function uses \(\hat{x}_{\text{frozen}}\), a frozen copy of the decoder:

\[\mathcal{L}_{\text{gated}}:=\mathbb{E}_{\mathbf{x}\sim\mathcal{D}_{ \text{train}}}\Big{[} \|\mathbf{x}-\hat{x}(\mathbf{\tilde{f}}(\mathbf{x}))\|_{2}^{2}\] (12) \[+\lambda\|\text{ReLU}(\pi_{\text{gate}}(\mathbf{x}))\|_{1}\] \[+\|\mathbf{x}-\hat{x}_{\text{frozen}}(\text{ReLU}(\pi_{\text{ gate}}(\mathbf{x})))\|_{2}^{2}\Big{]}.\]

### \(p\)-Annealing

Fundamentally, an \(L_{1}\) penalty has been used to induce sparsity in SAE features because it serves as a convex relaxation of the true sparsity measure, the \(L_{0}\)-norm. The \(L_{1}\)-norm is the convex hull of the \(L_{0}\)-norm, making it a tractable alternative for promoting sparsity [63]. However, the proxy loss function is not the same as directly optimizing for sparsity, leading to issues such as feature _shrinkage_[62] and potentially less sparse learned features. Unfortunately, the \(L_{0}\)-norm is non-differentiable and directly minimizing it is an NP-hard problem [48; 18], rendering it impractical for training.

In this work, we propose the use of nonconvex \(L_{p}^{p}\)-minimization, with \(p<1\), as an alternative to the standard \(L_{1}\) minimization in sparse autoencoders (SAEs). This approach has been successfully employed in compressive sensing and sparse recovery to achieve even sparser representations [11, 61, 64, 60]. To perform this optimization, we introduce a method called \(p\)_-annealing_ for training SAEs, based on the compressive sensing technique called \(p\)-continuation [66]. The key idea is to start with convex \(L_{1}\)-minimization through setting \(p=1\) and progressively decrease the value of \(p\) during training, resulting in closer approximations of the true sparsity measure, \(L_{0}\), as \(p\) approaches \(0\). We define the sparsity penalty for each batch \(x\) as a function of the current training step \(s\):

\[\mathcal{L}_{\text{sparse}}(\mathbf{x},s)=\lambda_{s}\|\mathbf{f}(\mathbf{x })\|_{p_{s}}^{p_{s}}=\lambda_{s}\sum_{i}f_{i}(\mathbf{x})^{p_{s}}\] (13)

In other words, the sparsity penalty will be a scaled \(L_{p}^{p}\) norm of the SAE feature activations, with \(p\) decreasing over time. At \(p=1\), the \(L_{p}^{p}\) norm is equal to the \(L_{1}\) norm, and as \(p\to 0\), the \(L_{p}^{p}\) norm limits to the \(L_{0}\)-norm, as \(\lim_{p\to 0}\sum_{i}f_{i}(\mathbf{x})^{p}=\sum_{i}f_{i}(\mathbf{x})^{0}\).

The purpose of annealing \(p\) from \(1\to 0\) instead of starting from a fixed, low value for \(p\) is that the lower the \(p\), the more concave (non-convex) the \(L_{p}^{p}\) norm is, increasing the likelihood of the training process getting stuck in local optima, which we have observed in initial experiments. Therefore, we aim to first arrive at a region of an optimum using the easier-to-train \(L_{1}\) penalty and then gradually shift the loss function. This manifests as keeping \(p=1\) for a certain number of steps and then starting decreasing \(p\) linearly down to \(p_{\text{end}}>0\) at the end of training. We set \(p_{\text{end}}=0.2\).

Coefficient Annealing.Changing the value of \(p\) changes the scale of the \(L_{p}^{p}\) norm. Without also adapting the coefficient \(\lambda\), the strength of the sparsity penalty would vary too wildly across training. Empirically, we found that keeping a constant \(\lambda\) would lead to far too weak of a sparsity penalty for

Figure 2: Comparison of the coverage and board reconstruction metrics for chess SAE quality on \(\mathcal{G}_{\text{board state}}\). The coverage score reports the mean F1 scores over BSPs. The top row corresponds to coverage, and the bottom row corresponds to board reconstruction. The left column contains a scatterplot of loss recovered vs. \(L_{0}\), with the scheme color corresponding to the coverage score and each point representing different hyperparameters. We differentiate between SAE training methods with shapes.

the larger \(p\)'s at the start of training, making the process worse than simply training with a constant \(p\) from the beginning. Consequently, we aim to adapt the coefficient \(\lambda\) such that the strength of the sparsity penalty is not changed significantly due to \(p\) updates. Formally, the update step is:

\[\lambda_{s+1}\leftarrow\lambda_{s}\frac{\sum_{j=s-q+1}^{s}\sum_{i}f_{i}(\mathbf{ x_{j}})^{p_{s}}}{\sum_{j=s-q+1}^{s}\sum_{i}f_{i}(\mathbf{x_{j}})^{p_{s+1}}}.\] (14)

We keep a queue of the most recent \(q\) batches of feature activations mid-training and use them to calibrate the \(\lambda_{s}\) updates. Therefore, the strength of the sparsity penalty is kept _locally_ constant.

Combining \(p\)-annealing with other SAEs.Since the \(p\)-annealing method only modifies the \(L_{1}\) terms in the loss function without affecting the SAE architecture, it is simple to combine \(p\)-annealing with other SAE modifications. This allows us to create the Gated-Annealed SAE method by combining the Gated SAE architecture and \(p\)-annealing. Concretely, we modify \(\mathcal{L}_{\text{gated}}\) (Equation 12) by replacing the sparsity term \(\lambda\|\text{ReLU}(\pi_{\text{gate}}(\mathbf{x}))\|_{1}\) in with \(\lambda_{s}\|\text{ReLU}(\pi_{\text{gate}}(\mathbf{x}))\|_{P_{s}}^{p_{s}}\). Our experiments showed that the optimum values for coefficients \(\lambda\) and \(\lambda_{s}\) differ.

## 5 Results

In this section, we explore the performance of SAEs applied to language models trained on Othello and chess. Consistent with Nanda et al. [47], we find that interpretable SAE features typically track properties relative to the player whose turn it is (e.g. "my king is pinned" rather than "the white king

Figure 3: Comparison of the coverage and board reconstruction metrics for chess SAE quality on \(\mathcal{G}_{\text{strategy}}\). The metrics represent the average coverage and board reconstruction obtained across all BSPs in \(\mathcal{G}_{\text{strategy}}\). The coverage score reports the mean of maximal F1 scores over BSPs. The absolute coverage scores vary significantly between strategy BSPs, as discussed in Appendix D. The top row corresponds to coverage, and the bottom row corresponds to board reconstruction. The left column contains a scatterplot of loss recovered vs. \(L_{0}\), with the color scheme corresponding to the coverage score and each point representing different hyperparameters. We differentiate between SAE training methods with shapes.

[MISSING_PAGE_FAIL:8]

Coverage and board reconstruction are consistent with existing metrics.Figures 2, 4, and 3 demonstrate that both coverage and board reconstruction metrics are optimal in the elbow region of the Pareto frontier. This region, where SAEs reconstruct internal activations efficiently with minimal features, also yielded the most coherent interpretations during our manual inspections. This provides precise, empirical validation to the common wisdom that SAEs in this region of the Pareto frontier are the best.

## 6 Limitations

The proposed metrics for board reconstruction and coverage provide a more objective evaluation of SAE quality than previous subjective methods. Nevertheless, these metrics exhibit several limitations. Primarily, their applicability is confined to the chess and Othello domains, raising concerns about their generalizability to other domains or different models. Additionally, the set of BSPs that underpin these metrics is determined by researchers based on their domain knowledge. This approach may not encompass all pertinent features or strategic concepts, thus potentially overlooking essential aspects of model evaluation. Developing comparable objective metrics for other domains, such as natural language processing, remains a significant challenge. Moreover, our current focus is on evaluating the quality of SAEs in terms of their ability to capture internal representations of the model. However, this does not directly address how these learned features could be utilized for downstream interpretability tasks.

Figure 4: Comparison of the coverage and board reconstruction metrics for Othello SAE quality on \(\mathcal{G}_{\text{board state}}\). The coverage score reports the mean of maximal F1 scores over BSPs. The top row corresponds to coverage, and the bottom row corresponds to board reconstruction. The left column contains a scatterplot of loss recovered vs. \(L_{0}\), with the color scheme corresponding to the coverage score and each point representing different hyperparameters. We differentiate between SAE training methods with shapes.

Related work

Sparse dictionary learning.Since the nineties, dictionary learning [22; 20], sparse regression [26], and later, sparse autoencoders [49] have been extensively studied in the machine learning and signal processing literature. The seminal work of Olshausen and Field [51] introduced the concept of sparse coding in neuroscience (see also [52], building upon the earlier concept of sparse representations [19] and matching pursuit [42]. Subsequently, a series of works established the theoretical and algorithmic foundations of sparse dictionary learning [23; 30; 21; 1; 65; 32; 59; 2; 5; 7; 10]. Notably, Gregor and LeCun [28] introduced LISTA, an unrolled version of ISTA [17] that learns the dictionary instead of having it fixed.

In parallel, autoencoders were introduced in machine learning to automatically learn data features and perform dimensionality reduction [29; 37]. Inspired by sparse dictionary learning, sparse autoencoders [49; 14; 13; 41; 35] were proposed as an unsupervised learning model to build deep sparse hierarchical models of data, assuming a certain degree of sparsity in the hidden layer activations. Later, Luo et al. [39] generalized sparse autoencoders (SAEs) to convolutional SAEs. Although the theory for SAEs is less developed than that of dictionary learning with a fixed dictionary, some progress has been made in quantifying whether autoencoders can, indeed, do sparse coding, e.g., Arpit et al. [4], Rangamani et al. [55], Nguyen et al. [50].

Feature disentanglement using sparse autoencoders.The individual computational units of neural networks are often polysemantic, i.e., they respond to multiple seemingly unrelated inputs [3]. Elhage et al. [24] investigated this phenomenon and suggested that neural networks represent features in linear superposition, which allows them to represent more features than they have dimensions. Thus, in an internal representation of dimension \(n\), a model can encode \(m\gg n\) concepts as linear directions [53], such that only a sparse subset of concepts are active across all inputs - a concept deeply related to the coherence of vectors [26] and to frame theory in general [12]. To identify these concepts, Sharkey et al. [57] used SAEs to perform dictionary learning on a one-layer transformer, identifying a large (overcomplete) basis of features. Cunningham et al. [16] applied SAEs to language models and demonstrated that dictionary features can be used to localize and edit model behavior. Marks et al. [44] proposed a scalable method to discover sparse feature circuits, as opposed to circuits consisting of polysemantic model components, and demonstrated that a human could change the generalization of a classifier by editing its feature circuit. Recently, Kissane et al. [34] explored autoencoders for attention layer outputs. These works have benefited from a variety of open-source libraries for training SAEs for LLM interpretability [43; 8; 15].

## 8 Conclusion

Most SAE research has relied on proxy metrics such as loss recovered and \(L_{0}\), or subjective manual evaluation of interpretability by examining top activations. However, proxy measures only serve as an estimate of interpretability, monosemantic nature, and comprehensiveness of the learned features, while manual evaluations depend on the researcher's domain knowledge and tend to be inconsistent.

Our work provides a new, more objective paradigm for evaluating the quality of an SAE methodology; coverage serves as a quantifiable measure of monosemanticity and quality of feature extraction, while board reconstruction serves as a quantifiable measure of the extent to which an SAE is exhaustively representing the information contained within the language model. Therefore, the optimal SAE methodology can be judged by whether it yields both high coverage and high board reconstruction.

Finally, we propose the \(p\)-annealing method, a modification to the SAE training paradigm that can be combined with other SAE methodologies and results in an improvement in both coverage and board reconstruction over the Standard SAE architecture.

## Author Contributions

A.K. built and maintained our infrastructure for working with board-game models. A.K., S.M., C.R., J.B., and L.S. designed the proposed metrics. B.W. performed initial experiments demonstrating the benefits of training SAEs with \(p<1\). B.W., C.M.V., and S.M. then proposed \(p\)-annealing, with B.W. leading the implementation and developing coefficient annealing. The basic framework for our dictionary learning work was built and maintained by S.M. and C.R. The training algorithms studied were implemented by S.M., C.R., B.W., R.A., and J.B. R.A. trained the SAEs used in our experiments. A.K., C.R., and J.B. selected and implemented the BSPs. A.K. and J.B. trained the linear probes. Many of the authors (including L.S., J.B., R.A.) did experiments applying traditional dictionary learning methods and exploring both toy problems and natural language settings, which helped build valuable intuition. The manuscript was primarily drafted by A.K., B.W., C.R., R.A., J.B., C.M.V., and S.M., with extensive feedback and editing from all authors. D.B. suggested the original project idea.

## Acknowledgments

C.R. is supported by Manifund Regrants and AISST. L.R. is supported by the Long Term Future Fund. S.M. is supported by an Open Philanthropy alignment grant.

The work reported here was performed in part by the University of Massachusetts Amherst Center for Data Science and the Center for Intelligent Information Retrieval, and in part using high performance computing equipment obtained under a grant from the Collaborative R&D Fund managed by the Massachusetts Technology Collaborative.

## References

* Aharon et al. [2006] Michal Aharon, Michael Elad, and Alfred Bruckstein. K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation. _IEEE Transactions on signal processing_, 54(11):4311-4322, 2006.
* Arora et al. [2015] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, efficient, and neural algorithms for sparse coding. In _Conference on learning theory_, pages 113-149. PMLR, 2015.
* Arora et al. [2018] Sanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma, and Andrej Risteski. Linear algebraic structure of word senses, with applications to polysemy. _Transactions of the Association for Computational Linguistics_, 6:483-495, 2018. doi: 10.1162/tacl_a_00034. URL https://aclanthology.org/Q18-1034.
* Arpit et al. [2016] Devansh Arpit, Yingbo Zhou, Hung Ngo, and Venu Govindaraju. Why regularized auto-encoders learn sparse representation? In _International Conference on Machine Learning_, pages 136-144. PMLR, 2016.
* Bao et al. [2015] Chenglong Bao, Hui Ji, Yuhui Quan, and Zuowei Shen. Dictionary learning for sparse coding: Algorithms and convergence analysis. _IEEE transactions on pattern analysis and machine intelligence_, 38(7):1356-1369, 2015.
* Bills et al. [2023] Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language models. _URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023)_, 2023.
* Blasiok and Nelson [2016] Jaroslaw Blasiok and Jelani Nelson. An improved analysis of the er-spud dictionary learning algorithm. In _43rd International Colloquium on Automata, Languages, and Programming (ICALP 2016)_. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2016.
* Bloom and Chanin [2024] Joseph Bloom and David Chanin. Sae lens. https://github.com/jbloomAus/SAELens, 2024.
* Bricken et al. [2016] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity:Decomposing language models with dictionary learning. _Transformer Circuits Thread_, page 2, 2023.
* [10] Matthew Chalk, Olivier Marre, and Gasper Tkacik. Toward a unified theory of efficient, predictive, and sparse coding. _Proceedings of the National Academy of Sciences_, 115(1):186-191, 2018.
* [11] Rick Chartrand. Exact reconstruction of sparse signals via nonconvex minimization. _IEEE Signal Processing Letters_, 14(10):707-710, 2007.
* [12] Ole Christensen et al. _An introduction to frames and Riesz bases_, volume 7. Springer, 2003.
* [13] Adam Coates and Andrew Y Ng. The importance of encoding versus training with sparse coding and vector quantization. In _Proceedings of the 28th international conference on machine learning (ICML-11)_, pages 921-928, 2011.
* [14] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In _Proceedings of the fourteenth international conference on artificial intelligence and statistics_, pages 215-223. JMLR Workshop and Conference Proceedings, 2011.
* [15] Alan Cooney. Sparse autoencoder library. https://github.com/ai-safety-foundation/sparse_autoencoder, 2023.
* [16] Hoagy Cunningham, Aidan Ewart, Logan Riggs Smith, Robert Huben, and Lee Sharkey. Sparse autoencoders find highly interpretable features in language models. In _The Twelfth International Conference on Learning Representations_, 2023.
* [17] Ingrid Daubechies, Michel Defrise, and Christine De Mol. An iterative thresholding algorithm for linear inverse problems with a sparsity constraint. _Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences_, 57(11):1413-1457, 2004.
* [18] Geoff Davis, Stephane Mallat, and Marco Avellaneda. Adaptive greedy approximations. _Constructive approximation_, 13:57-98, 1997.
* [19] David L Donoho. Superresolution via sparsity constraints. _SIAM journal on mathematical analysis_, 23(5):1309-1331, 1992.
* [20] Bogdan Dumitrescu and Paul Irofti. _Dictionary learning algorithms and applications_. Springer, 2018.
* [21] Julian Eggert and Edgar Korner. Sparse coding and nmf. In _2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No. 04CH37541)_, volume 4, pages 2529-2533. IEEE, 2004.
* [22] Michael Elad. _Sparse and redundant representations: from theory to applications in signal and image processing_. Springer Science & Business Media, 2010.
* [23] Michael Elad and Alfred M Bruckstein. A generalized uncertainty principle and sparse representation in pairs of bases. _IEEE Transactions on Information Theory_, 48(9):2558-2567, 2002.
* [24] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of superposition. _arXiv preprint arXiv:2209.10652_, 2022.
* [25] Javier Ferrando, Gabriele Sarti, Arianna Bisazza, and Marta R Costa-jussa. A primer on the inner workings of transformer-based language models. _arXiv preprint arXiv:2405.00208_, 2024.
* [26] Simon Foucart and Holger Rauhut. _A Mathematical Introduction to Compressive Sensing_. Springer New York, New York, NY, 2013.
* [27] Leo Gao, Tom Dupre la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. _arXiv preprint arXiv:2406.04093_, 2024.

* Gregor and LeCun [2010] Karol Gregor and Yann LeCun. Learning fast approximations of sparse coding. In _Proceedings of the 27th international conference on international conference on machine learning_, pages 399-406, 2010.
* Hinton and Zemel [1993] Geoffrey E Hinton and Richard Zemel. Autoencoders, minimum description length and helmholtz free energy. _Advances in neural information processing systems_, 6, 1993.
* Hoyer [2002] Patrik O Hoyer. Non-negative sparse coding. In _Proceedings of the 12th IEEE workshop on neural networks for signal processing_, pages 557-565. IEEE, 2002.
* Jermyn et al. [2024] Adam Jermyn, Adly Templeton, Joshua Batson, and Trenton Bricken. Tanh penalty in dictionary learning, 2024. URL https://transformer-circuits.pub/2024/feb-update/index.html. Accessed: 2024-05-20.
* Jung et al. [2014] Alexander Jung, Yonina C Eldar, and Norbert Gortz. Performance limits of dictionary learning for sparse coding. In _2014 22nd European Signal Processing Conference (EUSIPCO)_, pages 765-769. IEEE, 2014.
* Karvonen [2024] Adam Karvonen. Emergent world models and latent variable estimation in chess-playing language models, 2024.
* Kissane et al. [2024] Connor Kissane, Robert Krzyzanowski, Joseph Isaac Bloom, Arthur Conmy, and Neel Nanda. Interpreting attention layer outputs with sparse autoencoders. In _ICML 2024 Workshop on Mechanistic Interpretability_, 2024.
* Li et al. [2016] Jun Li, Tong Zhang, Wei Luo, Jian Yang, Xiao-Tong Yuan, and Jian Zhang. Sparseness analysis in the pretraining of deep neural networks. _IEEE transactions on neural networks and learning systems_, 28(6):1425-1438, 2016.
* Li et al. [2023] Kenneth Li, Aspen K Hopkins, David Bau, Fernanda Viegas, Hanspeter Pfister, and Martin Wattenberg. Emergent world representations: Exploring a sequence model trained on a synthetic task. In _The Eleventh International Conference on Learning Representations_, 2023.
* Li et al. [2023] Pengzhi Li, Yan Pei, and Jianqiang Li. A comprehensive survey on design and application of autoencoder in deep learning. _Applied Soft Computing_, 138:110176, 2023.
* Lichess [2024] Lichess. lichess.org open database, 2024. URL https://database.lichess.org.
* Luo et al. [2017] Wei Luo, Jun Li, Jian Yang, Wei Xu, and Jian Zhang. Convolutional sparse autoencoders for image classification. _IEEE transactions on neural networks and learning systems_, 29(7):3289-3294, 2017.
* Makelov et al. [2024] Aleksandar Makelov, George Lange, and Neel Nanda. Towards principled evaluations of sparse autoencoders for interpretability and control. _arXiv preprint arXiv:2405.08366_, 2024.
* Makhzani and Frey [2013] Alireza Makhzani and Brendan Frey. K-sparse autoencoders. _arXiv preprint arXiv:1312.5663_, 2013.
* Mallat and Zhang [1993] Stephane G Mallat and Zhifeng Zhang. Matching pursuits with time-frequency dictionaries. _IEEE Transactions on signal processing_, 41(12):3397-3415, 1993.
* Marks and Mueller [2024] Samuel Marks and Aaron Mueller. dictionary_learning. https://github.com/saprmarks/dictionary_learning, 2024.
* Marks et al. [2024] Samuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. _arXiv preprint arXiv:2403.19647_, 2024.
* McGrath et al. [2022] Thomas McGrath, Andrei Kapishnikov, Nenad Tomasev, Adam Pearce, Martin Wattenberg, Demis Hassabis, Been Kim, Ulrich Paquet, and Vladimir Kramnik. Acquisition of chess knowledge in alphazero. _Proceedings of the National Academy of Sciences_, 119(47), November 2022. ISSN 1091-6490. doi: 10.1073/pnas.2206625119. URL http://dx.doi.org/10.1073/pnas.2206625119.

* Mikolov et al. [2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. _Advances in neural information processing systems_, 26, 2013.
* Nanda et al. [2023] Neel Nanda, Andrew Lee, and Martin Wattenberg. Emergent linear representations in world models of self-supervised sequence models. In Yonatan Belinkov, Sophie Hao, Jaap Jumelet, Najoung Kim, Arya McCarthy, and Hosein Mohebbi, editors, _Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP_, pages 16-30, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.blackboxnlp-1.2. URL https://aclanthology.org/2023.blackboxnlp-1.2.
* Natarajan [1995] Balas Kausik Natarajan. Sparse approximate solutions to linear systems. _SIAM journal on computing_, 24(2):227-234, 1995.
* Ng et al. [2011] Andrew Ng et al. Sparse autoencoder. _CS294A Lecture notes_, 72(2011):1-19, 2011.
* Nguyen et al. [2019] Thanh V Nguyen, Raymond KW Wong, and Chinmay Hegde. On the dynamics of gradient descent for autoencoders. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 2858-2867. PMLR, 2019.
* Olshausen and Field [1996] Bruno A Olshausen and David J Field. Emergence of simple-cell receptive field properties by learning a sparse code for natural images. _Nature_, 381(6583):607-609, 1996.
* Olshausen and Field [2004] Bruno A Olshausen and David J Field. Sparse coding of sensory inputs. _Current opinion in neurobiology_, 14(4):481-487, 2004.
* Park et al. [2023] Kiho Park, Yo Joong Choe, and Victor Veitch. The linear representation hypothesis and the geometry of large language models. _arXiv preprint arXiv:2311.03658_, 2023.
* Rajamonaharan et al. [2024] Senthooran Rajamonaharan, Arthur Conmy, Lewis Smith, Tom Lieberum, Vikrant Varma, Janos Kramar, Rohin Shah, and Neel Nanda. Improving dictionary learning with gated sparse autoencoders. _arXiv preprint arXiv:2404.16014_, 2024.
* Rangamani et al. [2018] Akshay Rangamani, Anirbit Mukherjee, Amitabh Basu, Ashish Arora, Tejaswini Ganapathi, Sang Chin, and Trac D Tran. Sparse coding and autoencoders. In _2018 IEEE International Symposium on Information Theory (ISIT)_, pages 36-40. IEEE, 2018.
* Riggs and Brinkmann [2024] Logan Riggs and Jannik Brinkmann. Improving sparse autoencoders by square-rooting l1 and removing lowest activation features, 2024. URL https://www.lesswrong.com/posts/YiGs8qJ8aNBSwt2YN/improving-sae-s-by-sqrt-ing-l1-and-removing-lowest. Accessed: 2024-05-20.
* Sharkey et al. [2023] Lee Sharkey, Dan Braun, and Beren Millidge. Taking features out of superposition with sparse autoencoders, 2023. URL https://www.alignmentforum.org/posts/26QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition. Accessed: 2023-05-10.
* Templeton et al. [2024] Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. _Transformer Circuits Thread_, 2024. URL https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html.
* Tillmann [2014] Andreas M Tillmann. On the computational intractability of exact and approximate dictionary learning. _IEEE Signal Processing Letters_, 22(1):45-49, 2014.
* Wang et al. [2011] Meng Wang, Weiyu Xu, and Ao Tang. On the performance of sparse recovery via \(\ell_{p}\)-minimization \((0\leq p\leq 1)\). _IEEE Transactions on Information Theory_, 57(11):7255-7278, 2011.

* [61] Jinming Wen, Dongfang Li, and Fumin Zhu. Stable recovery of sparse signals via lp-minimization. _Applied and Computational Harmonic Analysis_, 38(1):161-176, 2015.
* [62] Benjamin Wright and Lee Sharkey. Addressing feature suppression in sparse autoencoders, 2024. URL https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes. Accessed: 2024-05-20.
* [63] John Wright and Yi Ma. _High-dimensional data analysis with low-dimensional models: Principles, computation, and applications_. Cambridge University Press, 2022.
* [64] Chengzhu Yang, Xinyue Shen, Hongbing Ma, Yuantao Gu, and Hing Cheung So. Sparse recovery conditions and performance bounds for \(\ell_{p}\)-minimization. _IEEE Transactions on Signal Processing_, 66(19):5014-5028, 2018.
* [65] Jianchao Yang, Kai Yu, Yihong Gong, and Thomas Huang. Linear spatial pyramid matching using sparse coding for image classification. In _2009 IEEE Conference on computer vision and pattern recognition_, pages 1794-1801. IEEE, 2009.
* [66] Le Zheng, Arian Maleki, Haolei Weng, Xiaodong Wang, and Teng Long. Does \(\ell_{p}\)-minimization outperform \(\ell_{1}\)-minimization? _IEEE Transactions on Information Theory_, 63(11):6896-6935, 2017.

Improving and evaluating sparse autoencoders

Despite the success of SAEs at extracting human-interpretable features, they fail to perfectly reconstruct the activations [16]. One challenge in the training of SAEs with an \(L_{1}\) penalty is shrinkage (or 'feature suppression'); in addition to encouraging sparsity, an \(L_{1}\) penalty encourages feature activations to be smaller than they would be otherwise. Wright and Sharkey [62] approached this problem by fine-tuning the sparse autoencoder without a sparsity penalty. Appendix E further quantifies shrinkage across a suite of SAEs trained on chess and Othello models. Jermyn et al. [31] and Riggs and Brinkmann [56] explored alternative sparsity penalties to reduce feature suppression during training. Rajamanoharan et al. [54] introduced Gated SAEs, an architectural variation for the encoder which both addresses shrinkage and improves on the Pareto frontier of \(L_{0}\) vs reconstruction error. Recently, Gao et al. [27] systematically evaluated the scaling laws with respect to sparsity, autoencoder size, and language model size.

The goal of dictionary learning in machine learning is to produce human-interpretable features and capture the underlying model's computations [9]. However, quantitatively measuring interpretability is difficult and often involves manual inspection. Therefore, most existing work assesses the quality of SAEs along different proxy metrics: (1) The cross-entropy loss recovered, which reflects the degree to which the original loss of the language model can be recovered when replacing activations with the autoencoder predictions. (2) The \(L_{0}\)-norm of feature activations \(\mathbb{E}_{z\sim\mathcal{D}}\left\|h(z)\right\|_{0}\), measuring the number of activate features given an input [25]. Makelov et al. [40] proposed to compare SAEs against supervised feature dictionaries in a natural language setting. However, this requires a significant understanding of the model's internal computations and is thus not scalable.

## Appendix B Sparse Autoencoder Training Parameters

We used a single NVIDIA A100 GPU for training SAEs and experiments. It takes much less than 24 hours to train a single SAE on 300 million tokens. Given a trained SAE, our evaluation requires less than 5 minutes of computing time.

\begin{table}
\begin{tabular}{l|c} \hline
**Parameter** & **Value** \\ \hline Number of tokens & 300M \\ Optimizer & Adam \\ Adam betas & (0.9, 0.999) \\ Linear warmup steps & 1,000 \\ Batch size & 8,192 \\ Learning rate & 3e-4 \\ Expansion factor & \{8, 16\} \\ Annealing start & 10,000 \\ \(p_{\text{end}}\) & 0.2 \\ \(\lambda_{\text{init}}\) & [0.02, 2.0] \\ \hline \end{tabular}
\end{table}
Table 2: Training parameters of our sparse autoencoders.

List of Board State Properties

Table 3 summarizes the high-level board state properties considered in \(\mathcal{G}_{\text{strategy}}\). The selection of concepts was inspired by McGrath et al. [45]. The column indicated by \(\#\) denotes the number of individual BSPs per concept. A single BSP per concept indicates we match this condition globally for any corresponding piece.

## Appendix D

\begin{table}
\begin{tabular}{p{113.8pt}|p{113.8pt}|p{113.8pt}} \hline
**Concept** & **\#** & **Description** \\ \hline check & 1 & Indicates whether the player to move is checked by the opponent. \\ \hline can\_check & 1 & Indicates whether the player to move could check the opponent with the next move. \\ \hline queen & 1 & Indicates whether the player to move has a queen on the board. \\ \hline can\_capture\_queen & 1 & Indicates whether the player to move can capture the queen of the opponent. \\ \hline bishop\_pair & 1 & Indicates whether the player to move still has both bishops on the board. \\ \hline castling\_rights & 1 & Indicates whether the player to move is still allowed to castle, contingent on the king and the rooks not having moved. \\ \hline kingside\_castling\_rights & 1 & Indicates whether the player to move is still allowed to kingside castle, contingent on the king and the kingside rook not having moved. \\ \hline queenside\_castling\_rights & 1 & Indicates whether the player to move is still allowed to queenside castle, contingent on the king and the queenside rook not having moved. \\ \hline fork & 1 & Indicates whether the player to move attacks has a fork on major pieces of the opponent. \\ \hline pin & 1 & Indicates whether there is a pin on the board, such that a players piece cannot move without exposing the king behind it to capture. \\ \hline legal\_en\_passant & 1 & Indicates whether the player to move has a legal en passant: a special pawn capture that can only occur immediately after an opponent moves a pawn two squares from its starting position and it lands beside the players pawn. \\ \hline ambiguous\_moves & 1 & Indicates whether there are moves that would require further specification as more than one piece of the same type can move to the same square. \\ \hline threatened\_squares & 64 & Indicates which squares are threatened by the opponent. \\ \hline legal\_moves & 64 & Indicates which squares can be legally moved to by the current player. \\ \hline \end{tabular}
\end{table}
Table 3: List of strategic Board State Properties.

[MISSING_PAGE_FAIL:18]

## Appendix E Relative Reconstruction Bias

Training Standard SAEs with an \(L_{1}\) penalty, as described in Section 4, causes a systematic underestimation of feature activations. Wright and Sharkey [62] term this phenomenon _shrinkage._ Following Rajamanoharan et al. [54], we measure the relative reconstruction bias \(\gamma\) of our SAEs, defined as:

\[\gamma:=\arg\min_{\gamma^{\prime}}\mathbb{E}_{\mathbf{x}\sim\mathcal{D}}\left[ \|\hat{x}_{\text{SAE}}(x)/\gamma^{\prime}-x\|_{2}^{2}\right]\] (15)

Here, \(\mathcal{D}\) denotes a large dataset of model internal activations. Intuitively, \(\gamma<1\) indicates shrinkage. A perfectly unbiased SAE would have \(\gamma=1\).

Our experiments show that p-annealing achieves similar relative reconstruction bias improvements to gated SAEs, both outperforming the baseline architecture. Figure 5 shows that improvements manifest differently across domains: Chess SAEs show a narrower range of bias (\(\gamma\approx 0.98\)) compared to Othello (\(\gamma\approx 0.80\)). This domain-dependent variation may reflect differences in the underlying models or data distributions. We observe unstable \(\gamma\) values for SAEs with L0 near zero, which represent degenerate cases outside the typical operating range of these models.

\begin{table}
\begin{tabular}{l|c c c} \hline
**Concept** & **Linear Probe** & **Best SAE** & **Best SAE Coverage** \\  & \(F_{1}\)**-score** & **Reconstruction** & **score** \\ \hline check & 0.00 & 0.00 & 0.13 \\ \hline can\_check & 0.19 & 0.03 & 0.52 \\ \hline can\_capture\_queen & 0.00 & 0.00 & 0.09 \\ \hline queen & 0.85 & 0.95 & 0.93 \\ \hline bishop\_pair & 0.82 & 0.74 & 0.81 \\ \hline castling\_rights & 0.89 & 0.75 & 0.65 \\ \hline kingside\_castling & 0.89 & 0.75 & 0.65 \\ \hline queen\_castling & 0.89 & 0.75 & 0.64 \\ \hline fork & 0.01 & 0.00 & 0.07 \\ \hline pin & 0.00 & 0.00 & 0.25 \\ \hline legal\_en\_passant & 0.00 & 0.00 & 0.06 \\ \hline ambiguous\_moves & 0.13 & 0.00 & 0.52 \\ \hline threatened\_squares & 0.82 & 0.73 & 0.60 \\ \hline legal\_moves & 0.65 & 0.36 & 0.45 \\ \hline board\_state & 0.26 & 0.01 & 0.11 \\ \hline \end{tabular}
\end{table}
Table 5: Comparison of performance of linear probes trained to predict high-level board state properties given residual stream activations with SAEs, both trained on a model with the same architecture as ChessGPT but randomly initialized. Performance on metrics can be high when the metric is correlated with move number or syntax level patterns (such as castling, which corresponds to 0-0).

## Appendix F Model Internal Board State Representation

### Othello Models

Previous research of Othello-playing language models found that the model learned a nonlinear model of the board state [36]. Further investigation found a closely related linear representation of the board when probing for "my color" vs. "opponent's color" rather than white vs. black [47]. Based on these findings, when measuring the state of the board in Othello, we represent squares as (mine, yours) rather than (white, black).

### Chess Models

Similar to Othello models, prior studies of chess-playing language models found the same property, where linear probes were only successful on the objective of the (mine, yours) representation and were unsuccessful on the (white, black) representation [33]. They measured board state at the location of every period in the Portable Game Notation (PGN) string, which indicates that it is white's turn to move and maintain the (mine, yours) objective. Some characters in the PGN string contain little board state information as measured by linear probes, and there is not a clear ground truth board state part way through a move (e.g., the "f" in "hf3"). We follow these findings and measure the board state at every period in the PGN string.

When measuring chess piece locations, we do not measure pieces on their initial starting location, as this correlates with position in the PGN string. An SAE trained on residual stream activations after the first layer of the chess model (which contains very little board state information as measured by linear probes) obtains a board reconstruction \(F_{1}\)-score of 0.01 in this setting. If we also measure pieces on their initial starting location, the layer 1 SAE's \(F_{1}\)-score increases to 0.52, as the board can be mostly reconstructed in early game positions purely from the token's location in the PGN string. Masking the initial board state and blank squares decreases the \(F_{1}\)-score of the linear probe from 0.99 to 0.98.

## Appendix G Additional examples of learned SAE features

We present two additional examples of learned SAE features that we (subjectively) match to board state properties based on their maximally activating input PGN-strings in Figure 6.

Figure 5: Comparison of the relative reconstruction bias metric \(\gamma\) quantifying feature activation shrinkage across a suite of SAEs. \(\gamma<1\) indicates shrinkage. A perfectly unbiased SAE would have \(\gamma=1\).

Figure 6: Additional examples of learned SAE features. We show the full board state of a chosen game in which the SAE latent has a high activation. The PGN-string (model input) which represents the game history is shown below the board. Tokens that activate SAE features are marked in blue, where darker shades correspond to higher feature activations. Moves that create the considered a board state are highlighted in yellow.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims presented in the abstract and introduction are a faithful representation of the contributions and scope of the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss our main limitation, the sensitivity of our metrics to human preconceptions, both in the introduction and in a dedicated limitations section. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA]  Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We disclose all information about the sources of base models and datasets, as well as information about the sparse autoencoder achitecture and hyperparameters used during training. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide the relevant code to reproduce the main experimental results of the paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We discuss the training process in the main section and provide additional information in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We present results across a large sweep of training parameters to provide information about the significance of the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We discuss our compute resources in Appendix A. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted does conform with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work focuses on evaluating current methods to extract features from neural networks, and proposes a method to improve this process. We do not believe that this work has a direct impact on society. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not introduce new datasets or models that pose such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The creators of original assets (models and data) are properly cited in the paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide a Readme as well as in-code documentations alongside the code. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.