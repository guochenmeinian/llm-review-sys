Knowledge-based in silico models and dataset for the comparative evaluation of mammography AI for a range of breast characteristics, lesion conspicuities and doses

E. Sizikova, N. Saharkhiz, D. Sharma, M. Lago, B. Sahiner, J. G. Delfino, A. Badano

Office of Science and Engineering Laboratories

Center for Devices and Radiological Health

U.S. Food and Drug Administration

Silver Spring, MD 20993 USA

Code and data links available at: https://github.com/DIDSR/msynth-release/

###### Abstract

To generate evidence regarding the safety and efficacy of artificial intelligence (AI) enabled medical devices, AI models need to be evaluated on a diverse population of patient cases, some of which may not be readily available. We propose an evaluation approach for testing medical imaging AI models that relies on in silico imaging pipelines in which stochastic digital models of human anatomy (in object space) with and without pathology are imaged using a digital replica imaging acquisition system to generate realistic synthetic image datasets. Here, we release M-SYNTH1, a dataset of cohorts with four breast fibroglandular density distributions imaged at different exposure levels using Monte Carlo x-ray simulations with the publicly available Virtual Imaging Clinical Trial for Regulatory Evaluation (VICTRE) toolkit. We utilize the synthetic dataset to analyze AI model performance and find that model performance decreases with increasing breast density and increases with higher mass density, as expected. As exposure levels decrease, AI model performance drops with the highest performance achieved at exposure levels lower than the nominal recommended dose for the breast type.

Footnote 1: Code and data links available at: https://github.com/DIDSR/msynth-release/

## 1 Introduction

The goal of this work is to demonstrate that AI models for medical imaging can be evaluated using simulations, specifically, using an in silico (also known as synthetic) imaging pipeline equipped with a stochastic model for human anatomy and disease [1]. We show that in silico methods can constitute rich sources of data with realistic physical variability for performing comparative analysis of AI device performance.

Figure 1: Overview of the computational pipeline components for generating the M-SYNTH in silico dataset for medical imaging AI evaluation.

To date, computational models have been applied to some extent for the analysis of nearly all medical imaging modalities and for a wide variety of clinical tasks [2]. Since it is critical to ensure patient safety and system effectiveness in healthcare applications, rigorous and thorough testing procedures must be performed in order to study performance in the intended population including subpopulations of interest. To prevent estimates that might be biased by overfitting, model testing is typically performed on a previously unseen dataset. However, datasets consisting of patient images may present a limited distribution of the variability in human anatomy and may not always capture rare, but life-critical cases, and may be biased towards specific populations or parameters of image acquisition devices dominant at specific clinical sites. In addition, patient data and associated health records may not be available due to patient privacy, cost, or additional risk associated with additional imaging procedures. Precise mass location and extent (e.g., mass boundaries) are typically not available in the patient's records, and it is burdensome, error-prone, and sometimes impossible to collect this information retrospectively. In many medical imaging applications, these limitations pose a significant barrier to development and evaluation of novel computational techniques in medical imaging products.

We propose evaluating AI models using physics-based simulations. We create realistic test cases by imaging digital objects using digital image acquisition systems. Our in silico testing pipeline offers the ability to control both object and acquisition parameters, and generate highly realistic test cases (see Figure 1). We show that digital objects and computer simulated replicas of image acquisition devices offer a rich source of realistic data capturing a variety of patient and imaging conditions for evaluation purposes. In particular, our approach (and associated dataset) allows for performing _comparative_ analysis of AI performance across physical breast properties (e.g., mass size) and imaging characteristics (e.g., radiation dose). Such testing typically cannot be performed with patient data, as the data may be too costly to collect or unsafe to acquire (e.g., one cannot ethically re-image the same patient multiple times using ionizing radiation). Our contributions in this work can be summarized as follows:

* We demonstrate that, using this approach, we can detect differences in AI model performance based on selected image acquisition device or physical object model parameters. Specifically, we evaluate the effect of image acquisition (radiation dose) and object model (breast and mass densities, mass size) parameters on the performance of the AI model.
* We release a dataset, M-SYNTH, to facilitate testing with pre-computed data using the proposed pipeline. The dataset consists of 1,200 stochastic knowledge-based models and their associated digital mammography (DM) images with varying physical (breast density, mass size and density) and imaging (dose) characteristics.

## 2 Background

First, we introduce the concepts of knowledge-based models and physics-based imaging simulation that form the _in silico imaging pipeline_, the foundation of our work.

**Object Models.**  Knowledge-based (KB) models incorporate information about the physical world into the data generation process to create realistic virtual representations of human parts or organs [3]. As discussed in [1], large cohorts of digital stochastic human models can be represented by:

\[\{\mathbf{f}_{s}\}_{s=1}^{S}=\sum_{n}\theta_{n}^{s}\phi_{n}(\bm{r}),\] (1)

where \(s\) denotes a particular state or random realization of a digital human in a cohort of size \(S\), \(\bm{r}\) denotes a spatial variable, \(\phi_{n}\) denote expansion (basis) functions, and \(\theta_{n}\) denote expansion coefficients. Knowledge-based models specifically are constructed by sampling a set of \(\theta_{n}\) in Eq. 1 from distributions representing the relevant model characteristics, given a specific \(\phi_{n}\) based on the application. The characteristics of the distributions are often derived from physical or biological measurements. In the case of breast, knowledge-based models allow us to vary physical patient characteristics including breast shape, breast shape, mass size and mass density (see Figures 2, 3 and 4).

Specifically, the object (breast) is a model \(D\), parameterized by a vector \(x\) characterizing a fixed, user-defined set of physiological properties (e.g., breast density, mass presence, mass size, glandularity). Given a sample \(x_{s}\), we can generate a realistic, high-resolution object \(f_{s}=D(x_{s})\). We rely on Graff's breast model [3] as the KB model for this project and describe its properties in Section 3.

Digital Mammography (DM) image generation.Once created, KB models are imaged using simulations of x-ray transport through the materials present in each KB model. The image acquisition device \(I\) is a parametric model that receives the object \(d_{i}\) as well as user-defined choices for control parameters \(y\) (e.g., detector type, radiation dose) and outputs an image \(r_{i,j}=I(d_{i},y_{j})\) given a sample choice of parameters \(y_{j}\) and an input object \(d_{i}\). Parameters of such a system (e.g., geometry, source characteristics, detector technology, anti-scatter grid, etc.) can emulate system geometries and x-ray acquisition parameters found in commercially available imaging device (e.g., mammography) specifications. In our work, we used MC-GPU [4], a Monte Carlo x-ray simulation software implemented on GPUs that generates mammography images. Additional details for this component of the pipeline can be found in Section 3.

Related work in generative image models.The in silico imaging pipeline described above is highly related to medical imaging generation using generative models. One popular type of generative model is a generative adversarial network (GAN) [5], which learns a mapping from a low-dimensional representation to images at resolution. Generative models have been applied to a variety of medical image generation tasks [6]. For example, Guan [7] showed that GAN-generated synthetic images can be used to augment a smaller patient breast image dataset for breast image classification. [8] introduced image-based GAN to generate high resolution images conditioned on pixel-level mask constraints. GANs may not correctly capture the link between input parameters and outputs, and thus, are prone to generating unrealistic examples [9]. A number of alternative types of generative models [10; 11; 12; 13; 14] have been developed that address its limitations, such as training instabilities and unrealistic output images. A key advantage of generative models is that their run time can be faster than fully-detailed, object-space simulations, and it remains important to explore and compare both techniques. Their key limitation is that they require large training datasets and typically learn noise and artifacts from the imaging system [15]. In particular, all image acquisition systems have a null space, i.e., the set of object-space details that are not observed in the acquired images due to imaging system limitations (e.g., finite spatial and temporal resolution). Null space constraints limit the ability of generative models to describe certain components of patient anatomy and pathology. Simulation-based testing has been proposed in other fields, such as autonomous vehicle navigation [16], and is related to the concept of generating adversarial perturbations in the image [17; 18; 19] and the physical property space [20; 21; 22]. For example, [23] introduced 3DB, a photo-realistic simulation framework to debug and improve computer vision models. Inspired by these works, we propose to evaluate medical imaging AI using images generated using KB models and physics simulations and release a dataset to facilitate such exploration.

## 3 Dataset Generation

The use of in silico imaging allows for the generation of large object and image datasets without the need of human clinical trials. Here, we take advantage of the benefits of the in silico approach to perform comparative analysis of AI model performance across different physical properties of the case population of breast models. We rely on the VICTRE pipeline 2 for generating breast models and their corresponding DM images. Previous work [24] has shown that the VICTRE pipeline replicated the results of a clinical study comparing DM and digital breast tomosynthesis (DBT) involving hundreds of enrolled women. An overview of the data generation process can be seen in Figure 1.

Footnote 2: See VICTRE Github Page and FDA Regulatory Science Tools (RST) Catalog.

**Breast Model Synthesis.** In silico breast models [3] (also known as breast imaging phantoms) were generated using a procedural analytic model which allows for adjusting various patient characteristics including breast shape, size and glandular density. The models are compressed in the craniocaudal direction using FeBio [25], an open source finite-element software. We simplified the breast materials in non-glandular (as fat) or glandular tissue with Young's modulus and Poisson ratio of \(E=5Pa\), \(\nu=0.49\) and \(E=15Pa\), \(\nu=0.49\), respectively. Lesions were inserted in a subset to create the signal-present cohort. These models were then imaged using a state-of-the-art Monte Carlo x-ray transport code (MC-GPU) [4].

We studied breast densities of extremely dense (referred to as "dense"), heterogeneously dense (referred to as "hetero"), scattered, and fatty, matching the distributions from [24]. For each breast density, a different breast size is used to correspond with population statistics. Therefore, the dense breast is the smallest, followed by heterogeneously dense, then scattered, and then fatty. Each breast 

[MISSING_PAGE_FAIL:4]

materials (air, adipose tissue, fibroplandular tissue, and skin). The second dataset is the VICTRE [24] collection that consists of about 3,000 digital patients with breast sizes and densities representative of a screening population. Digital microcalcification clusters and spiculated masses were inserted in the voxelized phantoms to create the positive cohort. The phantoms were imaged in silico to produce digital mammogram projections and digital breast tomosynthesis volumes. In comparison to both of these datasets, our work contains more significant variability in breast and mass characteristics, as well as a range of applied dose levels for image acquisition, in order to facilitate comparative evaluations of AI across characteristic changes.

Figure 4: _Cohort variability_. Varying breast density: (L to R) Fatty, Scattered, Heterogeneously dense, and Dense with mass size 7 mm and mass density 1.1. Note that dose changes with breast density. (e) shows artistic renderings of models for each composition (details in Kim et al. [27].

Figure 3: Effect of varying mass density (1.0 to 1.1 times glandular tissue density) in a fatty breast. Two models are shown, first: (a)-(c), and second: (d)-(f). Dose (# of hist.) \(2.22\times 10^{10}\) and mass size 7 mm remain constant. Bounding boxes are placed here to indicate the location of the masses.

Figure 2: Effect of varying mass size (5 mm to 9 mm radius) in a fatty breast. Two breast models are shown, first: (a)-(c), and second: (d)-(f). Dose (# of hist.) \(2.22\times 10^{10}\) and mass density 1.1 remain constant. Bounding boxes are placed here to indicate the location of the masses.

[MISSING_PAGE_FAIL:6]

### Implementation Details

**Evaluation Metrics** We evaluate performance using the area under curve (AUC) metric for a mass detection task. Specifically, we treat evaluation as a multiple reader multiple case study, where an AI model is a single reader. Multiple readers are obtained by re-training the model with different random seeds. We rely on the iMRMC software [37; 38] to identify associated confidence intervals.

**Network Training** We represent the AI-enabled device as a neural network with an efficientnet_b0 architecture, receiving an image with one channel and dimensions of 224 by 224, and outputting a binary mass presence label. The network is trained with batch size 64 using binary cross entropy loss (BCE) and optimized using RMSProp optimizer (with learning rate 0.0001). We rely on the timm library [39] and fine-tune the model pre-trained with ImageNet [40]. We also compared performance with alternative architectures (vit_small_patch16_224 and vgg_16), but results were very similar (see supplementary material).

For each specific breast density, radiation dose level, and mass size and density, the 300 images in the M-SYNTH dataset were divided into 200 for training, 50 for validation, and 50 for test. For comparison, we also train the AI device on 410 patient DM images from the INBreast dataset [32], where images were obtained using MammoNovation Siemens full-field digital mammography system with a solid-state amorphous selenium detector. We use the same pre-processing and training regimes on this dataset and learn a network to predict mass presence. The trained models on the real patient dataset were then tested on 50 examples of M-SYNTH dataset for each specific breast density, dose level, and mass size and density. The full experimental setup is implemented in Python and C over a cluster with 50 Tesla V100-SXM2 GPUs.

### Experimental Results

We identify two tasks that can be performed using our method. In the _subgroup analysis_ task, we train and test an AI model using the released synthetic (M-SYNTH) dataset to identify performance changes on specified subgroups. In the _patient data evaluation_ task, we study how an AI model trained on patient data (InBreast) performs on the proposed M-SYNTH dataset. This task can help identify where the trained model may show variable performance for different subgroups belonging to the target population.

**Subgroup Analysis.** In Figures 8 and 9, we report the results of the AI model performance at detecting masses, when the model is trained and tested on the our dataset (see Section 5.1 for details of splits). We find that masses with larger sizes or higher densities (Figures 7(a)-b) are more easily detected. Although models trained on all sizes or mass densities have the highest performance, when the models are trained on smaller masses or lower densities, they generalize better to other masses (more difficult cases).The performance of the models are highest when they are tested and trained on the same breast density and decrease as the density of the test breast phantom differs from the train phantom (Figures 7(c)). The dose levels applied in this study have minimal impact on the performance

Figure 8: _Subgroup analysis._ Performance change across (a) mass size, (b) mass density, (c) breast density, and (d) radiation dose, for models trained and tested on our M-SYNTH dataset. These parameters remained constant for the set of experiments performed during both training and test: (a) Fatty breast phantom, mass density of 1.06, and relative dose of 100%. (b) Fatty breast phantom, mass size of 7 mm, and relative dose of 100%. (c) Mass density of 1.06, mass size of 7 mm, and relative dose of 100%. (d) Fatty breast phantom, mass density of 1.06, and mass size of 7 mm.

of the models and resulted in similar AUC values (Figures (d)d). Evaluation of the performance change across all the breast densities (Figures (a)a-b) reveals that the AUC improves with larger mass density and mass size, yet is impacted by the breast density, where mass detection performance is lowest in high-density breasts (dense) and highest in low-density breasts (fatty) in most of the cases, consistent with findings from clinical practice.

Patient Data Evaluation.In Figure 10, we report experiments where the AI model is trained on INBreast data and evaluated on the M-SYNTH data. Although the performance results for all experiments are lower in general, we find a similar set of trends as when the model is trained on M-SYNTH data. Note that we have made no attempt to match the radiation dose levels or the image acquisition parameters for these comparisons using patient images. Even though the simulated pipeline is designed to replicate a specific DM system with a particular detector technology and technique factors, the comparison suggests similarity between the datasets. The images are qualitatively different but overall have similar glandular patterns which is an important consideration for the realism of the task of detecting masses in a noisy background. We also assessed similarity between INBreast and M-SYNTH datasets in terms of low-level pixel distributions using first five statistical moments: mean, variance, skewness, kurtosis, and hyperskewness. We found that there is a reasonably good alignment in terms of moments, especially when the synthetic images were included at all four breast densities (see supplementary material). Future work should develop a more detailed comparison including radiomics features for the training and testing datasets used in the study to complement the validation of our approach.

Limitations.There are a number of limitations to our work. First, simulations may require long runtimes and demand large computational resources, thus somewhat limiting the amounts of data that can be generated. This limitation needs to be considered with respect to the difficulty of obtaining large patient image datasets with known mass locations. In addition, data can be pre-generated offline (as we do with the M-SYNTH dataset), therefore, removing the large runtime limit and computational burden off the user. Second, testing with simulations is constrained to the variability captured by the parameter space of the object models for anatomy and pathology and the acquisition system. Thus, the complexity of the object model and acquisition system may need to be adjusted depending on the complexity of the questions to be investigated with simulated testing. In particular, a potential risk of testing using simulated data is missing the variability observed in patient populations. Finally, there is a risk of misjudging model performance due to a domain gap between real and synthetic

Figure 9: _Subgroup analysis._ Performance changes for models trained and tested on our M-SYNTH dataset. For each data point, the model is trained on 250 images with masses of radii of 7 mm and mass densities of 1.06, and tested on 50 images with mass characteristics shown in plots for each specific breast density. The radiation dose level remains constant at 100% of the clinically recommended dose for each breast density during training and test.

examples. However, the realism and sophistication of object-based modeling of the imaging pipeline is improving rapidly and may soon compete with other approaches, making approaches based on synthetic data useful and practical for regulatory evaluation of AI-enabled medical devices.

## 6 Conclusion and Future Work

We introduce and discuss an approach for validating AI models using physics-based simulations of digital humans from the object space to the image data, specifically for the task of breast cancer mass detection. The simulated images are highly realistic and offer a challenging test case for AI model evaluation. Our findings are consistent with expected performance and show that the AI model performance increases with mass size and mass density as expected. Finally, we show that our approach can be used to validate a model trained on independent patient data. This finding suggests that the proposed simulation setup can be used as a framework for more general evaluation of medical AI devices. The goal of this study is to demonstrate as proof-of-concept the feasibility of using simulated data to evaluate the comparative performance of AI models. In future work, it would be important to assess the evaluation approach for additional parameters in terms of the distribution of the population of digital humans in the object space, and for a range of image acquisition systems (e.g., by considering alternative simulators). By imaging a more diverse population of breast models, we hope to identify additional insights regarding AI evaluation. Finally, it is important to note that the testing is limited to the variability captured in the digital representations and may not fully indicate absolute real-world performance or trends. This study illustrates that physics-based simulation of mammography images can represent a less burdensome and cost-efficient approach for the evaluation of AI model performance across a wide range of scenarios, including a variety of image acquisition parameters and diverse populations that may not be available or are hard to obtain from human studies. Moreover, this approach offers a complementary evaluation paradigm that does not depend on the availability of patient data.

## 7 Acknowledgements

We thank Andreu Badal (OSEL/CDRH/FDA) and anonymous reviewers for helpful suggestions, Kenny Cha, Mike Mikailov and the OpenHPC team (OSEL/CDRH/FDA) for providing help with experiments, Akhonda, Mohammad (OSEL/CDRH/FDA) for help with data release, and Andrea Kim

Figure 10: _Model Evaluation_. Performance changes for a model trained on 410 real patient images (INBreast dataset) and tested on our M-SYNTH dataset. The test sets consist of 50 images using parameters shown in the plots. The test radiation dose is set to 100% of the clinically recommended dose for each breast density.

(OSEL/CDRH/FDA) for rendering visualizations of the 3D breast model. This is a contribution of the US Food and Drug Administration and is not subject to copyright. The mention of commercial products herein is not to be construed as either an actual or implied endorsement of such products by the Department of Health and Human Services.

## References

* [1] A Badano, M Lago, E Sizikova, JG Delfino, S Guan, MA Anastasio, and B Sahiner. The stochastic digital human is now enrolling for in silico imaging trials-methods and tools for generating digital cohorts. _arXiv preprint arXiv:2301.08719_, 2023.
* [2] Ana Barragan-Montero, Umair Javaid, Gilmer Valdes, Dan Nguyen, Paul Desbordes, Benoit Macq, Siri Willems, Liesbeth Vandewincke, Mats Holmstrom, Fredrik Lofman, et al. Artificial intelligence and machine learning for medical imaging: A technology review. _Physica Medica_, 83:242-256, 2021.
* [3] Christian G Graff. A new, open-source, multi-modality digital breast phantom. In _Medical Imaging 2016: Physics of Medical Imaging_, volume 9783, pages 72-81. SPIE, 2016.
* [4] Andreu Badal, Diksha Sharma, Christian G Graff, Rongping Zeng, and Aldo Badano. Mammography and breast tomosynthesis simulator for virtual clinical trials. _Computer Physics Communications_, 261:107779, 2021.
* [5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.
* [6] Youssef Skandarani, Pierre-Marc Jodoin, and Alain Lalande. Gans for medical image synthesis: An empirical study. _arXiv preprint arXiv:2105.05318_, 2021.
* [7] Shuyue Guan and Murray Loew. Breast cancer detection using synthetic mammograms from generative adversarial networks in convolutional neural networks. _Journal of Medical Imaging_, 6(3):031411-031411, 2019.
* [8] Yinhao Ren, Zhe Zhu, Yingzhou Li, Dehan Kong, Rui Hou, Lars J Grimm, Jeffery R Marks, and Joseph Y Lo. Mask embedding for realistic high-resolution medical image synthesis. In _MICCAI_, pages 422-430. Springer, 2019.
* [9] Elena Sizikova, Xu Cao, Ashia Lewis, Kenny Moise, and Megan Coffee. Improving computed tomography (ct) reconstruction via 3d shape induction. _arXiv preprint arXiv:2208.10937_, 2022.
* [10] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [11] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _International conference on machine learning_, pages 1530-1538. PMLR, 2015.
* [12] Piotr Bojanowski, Armand Joulin, David Lopez-Paz, and Arthur Szlam. Optimizing the latent space of generative networks. _arXiv preprint arXiv:1707.05776_, 2017.
* [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [14] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. _arXiv preprint arXiv:2209.04747_, 2022.
* [15] Weimin Zhou, Sayantan Bhadra, Frank J Brooks, Hua Li, and Mark A Anastasio. Learning stochastic object models from medical imaging measurements by use of advanced ambient generative adversarial networks. _Journal of Medical Imaging_, 9(1):015503, 2022.
* [16] Jingkang Wang, Ava Pun, James Tu, Sivabalan Manivasagam, Abbas Sadat, Sergio Casas, Mengye Ren, and Raquel Urtasun. Adysim: Generating safety-critical scenarios for self-driving vehicles. In _CVPR_, pages 9909-9918, 2021.
* [17] Naveed Akhtar and Ajmal Mian. Threat of adversarial attacks on deep learning in computer vision: A survey. _Ieee Access_, 6:14410-14430, 2018.
* [18] Samuel G Finlayson, John D Bowers, Joichi Ito, Jonathan L Zittrain, Andrew L Beam, and Isaac S Kohane. Adversarial attacks on medical machine learning. _Science_, 363(6433):1287-1289, 2019.

* [19] Hokuto Hirano, Akinori Minagi, and Kazuhiro Takemoto. Universal adversarial attacks on deep neural networks for medical image classification. _BMC medical imaging_, 21:1-13, 2021.
* [20] Chaowei Xiao, Dawei Yang, Bo Li, Jia Deng, and Mingyan Liu. Meshadv: Adversarial meshes for visual recognition. In _CVPR_, pages 6898-6907, 2019.
* [21] Xiaohui Zeng, Chenxi Liu, Yu-Siang Wang, Weichao Qiu, Lingxi Xie, Yu-Wing Tai, Chi-Keung Tang, and Alan L Yuille. Adversarial attacks beyond the image space. In _CVPR_, pages 4302-4311, 2019.
* [22] Hsueh-Ti Derek Liu, Michael Tao, Chun-Liang Li, Derek Nowrouzezahrai, and Alec Jacobson. Beyond pixel norm-balls: Parametric adversaries using an analytically differentiable renderer. _arXiv preprint arXiv:1808.02651_, 2018.
* [23] Guillaume Leclerc, Hadi Salman, Andrew Ilyas, Sai Vemprala, Logan Engstrom, Vibhav Vineet, Kai Xiao, Pengchuan Zhang, Shibani Santurkar, Greg Yang, et al. 3db: A framework for debugging computer vision models. _arXiv preprint arXiv:2106.03805_, 2021.
* [24] Aldo Badano, Christian G Graff, Andreu Badal, Diksha Sharma, Rongping Zeng, Frank W Samuelson, Stephen J Glick, and Kyle J Myers. Evaluation of digital breast tomosynthesis as replacement of full-field digital mammography using an in silico imaging trial. _JAMA network open_, 1(7):e185474-e185474, 11 2018.
* [25] Steve A. Maas, Benjamin J. Ellis, Gerard A. Ateshian, and Jeffrey A. Weiss. FEBio: Finite Elements for Biomechanics. _Journal of Biomechanical Engineering_, 134(1):011005, 02 2012.
* [26] Luis de Sisternes, Jovan G Brankov, Adam M Zysk, Robert A Schmidt, Robert M Nishikawa, and Miles N Wernick. A computational model to generate simulated three-dimensional breast masses. _Medical physics_, 42(2):1098-1118, 2015.
* [27] Andrea Kim, Aumnasha Sengupta, and Aldo Badano. Automated animation pipeline for visualizing in silico tumor growth models. _Physics of Medical Img_, 12463, 2023.
* [28] Aumnasha Sengupta, Andreu Badal, Andrey Makeev, and Aldo Badano. Computational models of direct and indirect x-ray breast imaging detectors for in silico trials. _Medical Physics_, 49(11):6856-6870, 2022.
* [29] Helen ML Frazer, Jennifer SN Tang, Michael S Elliott, Katrina M Kunicki, Brendan Hill, Ravishankar Karthik, Chun Fung Kwok, Carlos A Pena-Solorzano, Yuanhong Chen, Chong Wang, et al. Admani: Annotated digital mammograms and associated non-image datasets. _Radiology: Artificial Intelligence_, 5(2):e220072, 2022.
* [30] Chunyan Cui, Li Li, Hongmin Cai, Zhihao Fan, Ling Zhang, Tingting Dan, Jiao Li, and Jinghua Wang. The chinese mammography database (cmmd): An online mammography database with biopsy confirmed types for machine diagnosis of breast. _Data Cancer Imaging Arch_, 2021.
* [31] Mark D. Halling-Brown, Lucy M. Warren, Dominic Ward, Emma Lewis, Alistair Mackenzie, Matthew G. Wallis, Louise S. Wilkinson, Rosalind M. Given-Wilson, Rita McAvinchey, and Kenneth C. Young. Optimum mammography image database: A large-scale resource of mammography images and clinical data. _Radiology: Artificial Intelligence_, 3(1), 2020.
* [32] Ines C Moreira, Igor Amaral, Ines Domingues, Antonio Cardoso, Maria Joao Cardoso, and Jaime S Cardoso. Inbreast: toward a full-field digital mammographic database. _Academic radiology_, 19(2):236-248, 2012.
* [33] Mateusz Buda, Ashirbani Saha, Ruth Walsh, Sujata Ghate, Nianyi Li, Albert Swiecicki, Joseph Y Lo, and Maciej A Mazurowski. Dete cation of masses and architectural distortions in digital breast tomosynthesis: a publicly available dataset of 5,060 patients and a deep learning model. _arXiv preprint arXiv:2011.07995_, 2020.
* [34] Jiwoong J Jeong, Brianna L Vey, Ananth Bhimireddy, Thomas Kim, Thiago Santos, Ramon Correa, Raman Dutt, Marina Mosunjac, Gabriela Oprea-Ilies, Geoffrey Smith, et al. The emory breast imaging dataset (embed): A racially diverse, granular dataset of 3.4 million screening and diagnostic mammographic images. _Radiology: Artificial Intelligence_, 5(1):e220047, 2023.
* [35] Berkman Sahiner. Digital mammography dream challenge overview (conference presentation). In _Medical Imaging 2017: Computer-Aided Diagnosis_, volume 10134, pages 1159-1159. SPIE, 2017.
* [36] Antonio Sarno, Giovanni Mettivier, Francesca di Franco, Antonio Varallo, Kristina Bliznakova, Andrew M. Hernandez, John M. Boone, and Paolo Russo. Dataset of patient-derived digital breast phantoms for in silico studies in breast computed tomography, digital breast tomosynthesis, and digital mammography. _Medical Physics_, 48(5):2682-2693, 2021.

- Theory and Methods_, 38(15):2586-2603, 2009.
* [38] RST Catalog. iMRMC: Software for the Statistical Analysis of multi-reader multi-case studies, June 2022.
* [39] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.
* [40] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _CVPR_, pages 248-255. Ieee, 2009.