# PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition

George Tsoukalas

UT Austin

Jasper Lee

UT Austin

John Jennings

UT Austin

Jimmy Xin

UT Austin

Michelle Ding

UT Austin

Michael Jennings

UT Austin

Amitayush Thakur

UT Austin

Swarat Chaudhuri

UT Austin

###### Abstract

We present PutnamBench, a new multi-language benchmark for evaluating the ability of neural theorem-provers to solve competition mathematics problems. PutnamBench consists of 1692 hand-constructed formalizations of 640 theorems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. All the problems have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq formalizations. PutnamBench requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We use PutnamBench to evaluate several established neural and symbolic theorem-provers. These approaches can only solve a handful of the PutnamBench problems, establishing the benchmark as a difficult open challenge for research on neural theorem-proving. PutnamBench is available at [https://github.com/trishullab/PutnamBench](https://github.com/trishullab/PutnamBench).

## 1 Introduction

Automating mathematical reasoning is a longstanding goal in artificial intelligence (Newell et al., 1957). A prominent line of work on the problem (Li et al., 2024) uses neural models to direct theorem-proving in formal frameworks like Lean 4 (Moura & Ullrich, 2021), Isabelle (Wenzel et al., 2008), and Coq (The Coq Development Team, 2023). These frameworks can "execute" proofs like code and offer execution feedback, which simplifies the search for correct proofs.

The design of quality benchmarks is a key challenge in this research area. The two most prominent competition-based benchmarks for neural theorem-proving are MiniF2F (Zheng et al., 2021) and Fimo(Liu et al., 2023). The former formalizes a mix of problems from high-school level courses and mathematics competitions such as AIME, AMC, and IMO; the latter consists of a collection of IMO problems. Both benchmarks have limitations. For example, MiniF2F contains many problems that can be immediately solved using an SMT solver, and Fimo only targets the Lean 3 framework, which is no longer actively maintained.

More generally, as large language models (LLMs) grow in importance as a tool for neural theorem-proving (Li et al., 2024), preventing leakage between pretraining sets and evaluation sets is more important than ever. This makes the continued supply of new benchmarks an important goal.

In this paper, we respond to this challenge with PutnamBench, a new hand-curated, multi-language benchmark for neural theorem-provers. PutnamBench includes 1692 formalizations of 640 problems from the William Lowell Putnam Mathematical Competition, the premier college-levelmathematics competition in North America.* All our problems have Lean 4 (Moura & Ullrich, 2021) and Isabelle (Wenzel et al., 2008) formalizations; a substantial fraction have formalizations in Coq (The Coq Development Team, 2023) as well. The formalizations are all manually constructed and have been carefully debugged. The benchmark also includes the original English-language problem statements with permission from the Mathematical Association of America, which organizes the Putnam competition.

Footnote *: PutnamBench is available at [https://github.com/trishullab/PutnamBench](https://github.com/trishullab/PutnamBench).

One key benefit of PutnamBench is that Putnam competition problems require a broad base of mathematical knowledge and skills. Because they target undergraduate students, they cover topics such as analysis and abstract algebra that do not appear in the International Mathematical Olympiad (IMO). At the same time, success in the two competitions is correlated -- top performers on the Putnam competition are often former IMO medalists as well. Hence, PutnamBench is well-aligned with the IMO Grand Challenge (Challenge, 2019) and the AI Mathematical Olympiad (Prize, 2023), the latter of which offers a $10M prize fund for developing a system that can win a gold medal at the IMO.

Another advantage is that PutnamBench supports multiple proof assistants. Lean 4, Coq, and Isabelle are currently the three most popular formal proof languages. However, theorem-proving benchmarks typically only contain problems in a strict subset of these languages -- for example, MiniF2FZheng et al. (2021) does not include Coq problems, and Fimo(Liu et al., 2023) only targets Lean. PutnamBench is the first mathematics-competition benchmark to include problems in all three languages.

We use PutnamBench to evaluate several neural and symbolic approaches: Draft-Sketch-Prove (Jiang et al., 2022), Copra (Thakur et al., 2024), GPT-4, Sledgehammer (Paulson & Blanchette, 2015), and Coqhammer (Czajka & Kaliszyk, 2018). Collectively, these methods can only solve a handful of the PutnamBench problems, establishing PutnamBench as a hard open challenge for the neural theorem-proving community.

## 2 Background

**Formal Theorem-Proving.** Formal proof frameworks like Lean 4 (Moura & Ullrich, 2021), Coq (The Coq Development Team, 2023), and Isabelle (Wenzel et al., 2008) allow users to write machine-verifiable proofs of mathematical theorems. To create such a proof, one first uses a framework-specific language to formally state the target theorem. The mathematical objects referenced in the theorem can be imported from an existing repository or defined by the user. During the proof process, the proof framework maintains a _state_ that includes information about the parts of the proof that remain to be completed. One can change this state by executing a _proof step_. The user's goal is to write a sequence of proof steps (in the framework's language) that changes the proof state to a special state "QED" in which there are no unmet proof obligations.

Figure 1 illustrates a theorem and proof in the Lean 4 framework.

The Putnam Competition.The William Lowell Putnam Mathematical (Competition, 2024), organized by the Mathematical Association of America (MAA), is the premier collegiate mathematics competition in North America. Thousands of undergraduate students from universities across the United States and Canada take the exam each year. The competition comprises two 3-hour-long sessions of six problems each, presented in approximately ascending order of difficulty within each

Figure 1: A formalization of Putnam 1988 B1 in Lean 4, which asserts that for all integers \(a,b\geq 2\), there are positive integers \(x,y,z\) such that \(ab=xy+xz+yz+1\). The formal proof begins by introducing all relevant variables and hypotheses with intro, then indicating the choice of \(x,y,z\) with use, and afterwards proving all goals using the automated tactics linarith and ring. This proof was discovered through a few-shot invocation of GPT-4.

session. While some problems require competitors to furnish a concrete solution (such as a number, a set, or the truth value of a given statement), all problems require a natural-language proof of correctness. The contest draws from a wide variety of topics in the undergraduate curriculum, often using instances of ideas from research-level mathematics.

## 3 PutnamBench

PutnamBench is a multi-language evaluation benchmark consisting of formalized problems from the Putnam competition. PutnamBench is a manually produced benchmark, including 640 formalizations in Lean 4 and Isabelle, and 412 formalizations in Coq. In aggregate, PutnamBench contains 1692 formalizations of Putnam competition problems. We also incorporate the informal statements and numerical solutions where applicable.

Now we elaborate on the main features of PutnamBench.

**Diversity and Breadth.** Compared to MinF2F (Zheng et al., 2021) and Fimo (Liu et al., 2023), which generally rely on high-school mathematics, PutnamBench incorporates a wider variety of problems which require definitions of the standard undergraduate mathematics curriculum. The ProofNet benchmark (Azerbayev et al., 2023) also sources problems from the undergraduate curriculum, but these problems are generally from standard textbooks as opposed to mathematical competitions. Putnam problems often require definitions from multiple fields, which standard textbooks do not necessarily target. Formalizations in PutnamBench include concepts from a wide range of mathematical fields, including: (i) _Analysis_: Limits, integrals, derivatives, continuity; (ii) _Linear Algebra_: Matrices, determinants, fields; (iii) _Abstract Algebra_: Rings, groups, magmas, permutations; (iv) _Algebra_: Polynomials, inequalities, algebraic expressions; (v) _Number Theory_: Primes, irrationality, base representations, divisors, palindromes; (vi) _Geometry_: Polygons, point sets, line intersections, Euclidean distance; (vii) _Set Theory & Combinatorics_: Countability, power sets, discrete structures, games.

**Multiple Languages.**PutnamBench contains formalizations of Putnam problems in Lean 4, Isabelle, and Coq. The formalizations also include concepts defined in each proof assistant's mathematical repositories -- notably, Mathlib, the HOL standard library, and Coquelicot (among various Coq repositories). To the best of our knowledge, PutnamBench is the first undergraduate-level competition benchmark for each of these languages. Furthermore, we are the first to produce a human mathematics competition-style evaluation benchmark for Coq.

We hope that this contribution can enable Coq practitioners access to the rapidly-growing field of machine learning for mathematics.

Generally, the formalizations of the problems are aligned in their structure, including hypothesis naming and framing. Differences may arise according to the underlying foundations of each language.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline
**Benchmark** & \(\#\) & **Natural Language** & **Lean** & **Isabelle** & **Coq** & **Factored Solution** \\ \hline minF2F & 488 & ✓ & ✓\({}^{\dagger}\) & ✓ & & & \\ ProofNet & 371 & ✓ & ✓\({}^{\dagger}\) & & & & N/A \\ Fimo & 149 & ✓ & ✓\({}^{\dagger}\) & & & & \\ PutnamBench & 640 & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of existing formal theorem proving evaluation benchmarks. PutnamBench exceeds prior benchmarks by providing support for all of Lean 4, Isabelle, and Coq, on a set of difficult competition problems using undergraduate-level mathematics. For problems requiring a numerical solution in addition to a proof, we factor the solution out of the theorem statement.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Category** & **Total Quantity** \\ \hline Algebra & 253 \\ Analysis & 226 \\ Number Theory & 107 \\ Geometry & 68 \\ Linear Algebra & 51 \\ Abstract Algebra & 28 \\ Combinatorics & 26 \\ Probability & 9 \\ Set Theory & 8 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Quantity by domain of PutnamBench problems. Our formalizations generally reflect the variety of Putnam problems, though we can only formalize few geometry and probability problems due to limited support for these topics in the respective mathematical libraries.

We also note that the pre-defined mathematical theory in each language differs, which can sometimes lead to difficulties formalizing certain problems.

Compared to the prior benchmarks MiniF2F, Fimo, and ProofNet, PutnamBench is the first to support Lean 4 on initial release +.

Footnote †: MiniF2F, Fimo, and ProofNet were originally released using Lean 3, and MiniF2F and Fimo have been updated to include Lean 4 formalizations following community efforts. (Azerbayev et al., 2023; Vishwakarma et al., 2024). To the best of our knowledge, no open-sourced Lean 4 version of FIMO currently exists.

**Factored Solutions.** Roughly 60% of Putnam problems, in their natural language form, require exhibiting a (closed-form) solution along with a proof of its correctness. Such problems do not assert propositions, and hence are not immediately formalizable as they are not directly the statement of a theorem. Prior benchmarks such as MiniF2F (Zheng et al., 2021) sidestep this issue by rewording the problem statement to ask for a proof that the solution satisfies the constraints of the problem. However, this reduction diminishes the overall difficulty of the problem, as producing a solution can constitute the majority of the difficulty. To address this issue, we factor out solutions of such problems from the formalized theorem statement. We include an example in Figure 2. In this way, we provide two tasks for neural theorem proving:

* **Task 1:** Given the theorem statement, first identify the (closed-form) solution, and then provide a proof of correctness by rewriting the solution into the theorem statement.
* **Task 2:** Given the theorem statement and solution, produce a proof of its correctness. This task aligns with the current benchmarks.

We note that the process of producing the numerical solution may be highly correlated with the proof of its correctness. In this way, our formalizations can reflect the true difficulty of the informal problem statement.

**Formalization effort and challenges.** We hand-crafted our benchmark over the course of several months as a team of two doctoral and five undergraduate students with prior experience in university mathematics, computer science, and formal proof assistants. We found that the average time-to-formalize a single problem in one language was roughly 25 minutes. Each formalization was verified by a second person at least once, and we measured that the verification of a single formalization took between 10 minutes, on average. We acknowledge that the time-to-formalize we report is higher than that of MiniF2F; we believe this is largely due to the increased complexity of the Putnam problems, which oftentimes require definitions we must locate in each language's respective mathematical libraries.

We first produced formalizations in Lean 4, and then proceeded with our formalization effort in Isabelle and then Coq. Due to differences in the underlying foundations of each language, we found that formalizations in one language sometimes do not directly transfer to another; for example, Isabelle does not have a subtyping mechanism, which we made extensive use of in Lean 4. Formalizations in Coq rely on a number of mathematics repositories. Predominantly, we rely

Figure 2: A formalization of Putnam 2008 B5 in Lean 4. As the problem requires exhibiting the set of functions \(f\) satisfying the specified conditions, it is not directly the statement of a theorem. We formalize the problem by instantiating a variable “solution” outside of the theorem statement. In this way, a model can either provide its own candidate, or use the correct solution we provide and attempt to produce a proof of correctness. Benchmarks such as MiniF2F and Fimo only include formalizations with the solution written into the theorem statement.

on MathComp and MathComp-Analysis (Mathcomp, 2015; mathcomp-analysis), but also make us of Stdlib, Stdpp, Coquelicot, GeoCqoq, and Coqtail (Coquelicot, 2015; GeoCqoq, 2015; Allais et al.).

Some problems are not naturally amenable to formalization -- for example, we found that while formalizing problems involving probabilities is possible, such formalizations often require heavy probability theory. Similarly, support for problems involving Euclidean geometry varies across languages; in particular, Lean 4 does not yet have a sufficiently extensive library to make most geometry problems formalizable. By contrast, Coq has an extensive geometry repository called GeoCqoq, which we utilize for our Coq formalizations.

**Dataset Contamination.** Our benchmark is unique compared to informal benchmarks such as MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021) in the sense that the target output _has never been produced_, hence avoiding direct contamination. To the best of our knowledge, we are the first to provide formalizations of a large collection of Putnam problems in any of Lean, Isabelle, and Coq. Since writing a formal proof requires the formal theorem statement, it is highly unlikely any possible formal proof has been written for any of our problems. We performed a thorough investigation of formal mathematics repositories for each language for confirmation, finding no aligned theorems and proofs from the Putnam Competition. We do not include any of the formal proofs in our benchmark.

Furthermore, any proofs found by automated methods in our evaluations are not included and are only mentioned in this article. Indirect contamination can occur through transfer from training on the informal proofs, though producing proofs in formal proof environments still presents a major difficulty for all current neural methods, as we find in Section 4.

**Licensing and Rules of Engagement.** PutnamBench is available under an Apache 2.0 license for Lean 4 and Isabelle, and under an MIT license for Coq. We align the licenses with those of the repositories we use for each language. With permission from the MAA, we include the informal statements as sourced from the competition (Alexanderson et al., 1985; Kedlaya et al., 2002, 2020). We host a public leaderboard at [https://trishullab.github.io/PutnamBench/](https://trishullab.github.io/PutnamBench/) and will readily accept evaluation results from future works.

## 4 Experimental Evaluation

To understand the challenges that PutnamBench poses for state-of-the-art theorem-proving approaches, we attempt to solve its problems using a suite of such approaches. Given the relative lack of tailored systems for multi-language theorem-proving, we run evaluations for each language separately. Any method that is evaluated on multiple languages is based on off-the-shelf foundation models.

Figure 3: Formalizations of Putnam 2006 B2 in (a) Lean 4, (b) Isabelle, (c) Coq. Putnam 2006 B2 asserts that given a finite subset \(X\subseteq\mathbb{R}\) with \(|X|=n>0\), there is a nonempty subset \(S\subseteq X\) and an \(m\in\mathbb{Z}\) such that \(|m+\sum_{s\in S}s|\leq\frac{1}{n+1}\).

[MISSING_PAGE_FAIL:6]

queries to GPT-4 can yield more successful proofs, though it is not yet feasible to perform such an experiment due to the cost of queries to GPT-4.

We found that, by default, GPT-4 produces proofs using Lean 3 syntax, which is not compatible with Lean 4. Even when directed to produce outputs in Lean 4, GPT-4 typically continues to produce outputs in Lean 3. Our prompt, which we include in Figure 16, elucidates some design differences in Lean 4 to better enforce compliance with the Lean 4 syntax. However, we noticed many examples where GPT-4 continues to output terms in Lean 3 syntax. One such example is given in Figure 17.

We run ReProver using the standard search parameters used in LeanDojo (Yang et al., 2023). Our evaluation yields no successfully proven problems, with and without the inclusion of the retrieval module. We believe that Putnam 1988 B1, which the other methods solve, is not solved by ReProver as it requires an understanding that the choice of \(x,y,z=1,a-1,b-1\) will eventually satisfy the conditions of the goal after simplification. Smaller models, like the one driving ReProver's search, may not be as readily capable of such understanding.

Isabelle.We run GPT-4 using the same configuration, with modified prompts for Isabelle, on our Isabelle formalizations. We find that GPT-4 can produce a single successful proof to Putnam 1986 B1, a geometric problem stated algebraically. We include the statement and its proof as generated by GPT-4 in Figure 19.

DSP represents a neurosymbolic methodology which has seen significant application for theorem-proving in MiniF2F. We run DSP with \(pass@10\), using temperature \(T=0.1\) and GPT-4 as the underlying language model. Our evaluation yields four successful proofs: of Putnam 2001 A1 and 1971 B1, two problems involving magmas (sets with a binary operation), one of Putnam 1995 A1, a problem involving a closed-under-multiplication subset of the reals, and Putnam 1986 B1. In particular, Putnam 1995 A1 and 1986 B1 cannot be solved by Sledgehammer alone. The generated proof of Putnam 1995 A1 is included in Figure 4.

We run a baseline using Sledgehammer, a powerful automation tool in Isabelle which makes calls to external SMT solvers to prove a given goal. With a set timeout of \(t=120\) seconds, we run Sledgehammer on each Isabelle formalization. The result of this evaluation is 3 successfully proven problems: Putnam 1971 B1, 2001 A1, and 2012 A2. Notably, all of these problems are statements about sets with binary operations. We include the statements of 1971 B1 and 2012 A2 in Figure 22.

Coq.We run GPT-4 with a Coq-based prompt on our Coq formalizations using the same configuration as in Lean and Isabelle. The result of the experiment is 1 solved problem, namely Putnam 1988 B1, which was also solved in Lean 4. The proof, which we include in Figure 14, generally follows the same structure as the proof in Lean.

An evaluation with COPRA, in a \(pass@1\)-with-\(60\)-queries and \(T=0.0\) also yields a successful proof only for Putnam 1988 B1 which we include in Figure 14. In this case, backtracking was crucial for proof search on this problem. The crucial step in 1988 B1 is the choice of \(x,y,z\) once \(a\) and \(b\) have been introduced. Initially, COPRA predicts the erroneous choice \(x,y,z=1,1,ab-1\) and eventually reverts this choice using backtracking. Afterwards, COPRA predicts a correct choice \(x,y,z=1,a-1,b-1\) and proceeds with the proof.

Figure 4: A formalization of Putnam 2001 A1 in Isabelle and the corresponding proof discovered by our evaluation with DSP. Sledgehammer alone can also produce a successful proof to this theorem.

We run Tactician using the locality sensitive hashing model with a timeout of \(t=600s\) per problem. Our evaluation yields no successfully proven problems. While showing favorable performance on theorems drawn from Coq's standard library (Zhang et al., 2021), such methodologies do not as of yet scale to challenging olympiad-style problems.

We run CoqHammer with 8 parallel threads using an ATP timeout of 100 seconds, proof reconstruction timeout of 15 seconds, and sauto timeout of 5 seconds, for a total of 120 seconds allocated for each formalization. The evaluation yields no successful proofs -- indicating that symbolic tools in Coq are not yet capable of handling PutnamBench problems. It is not surprising that CoqHammer does not match the performance of Sledgehammer even though they rely on the same external solvers. The underlying logical system of Coq is more complex than that of Isabelle and is hence less amenable to automation.

### General Analysis

Aggregating over all experiments performed in all languages, we find that a total of 6 problems in PutnamBench are successfully proven. A majority of these come from evaluations in Isabelle, particularly with strong contributions from Sledgehammer. Sledgehammer can solve all three problems involving magmas which appear in our benchmark but fails to produce successful proofs for any other formalization. DSP solves an additional two problems and relies heavily on Sledgehammer to fill in the proofs of intermediate steps. The single problem solved in Lean and Coq also makes use of automated tactics like linarith and lia, and requires only a single crucial step.

Hence, we find that a few PutnamBench problems are not entirely intractable using current methods. However, anecdotally, these problems are among the easiest ever included in the Putnam competition. All admit a very short natural language proof and do not require reasoning about particularly complicated objects. We believe that significant advancements in automated mathematical reasoning are required to make progress on PutnamBench.

## 5 Related Work

**Formal Benchmarks.** Several evaluation benchmarks for formal mathematics have been developed in recent years. MiniF2F (Zheng et al., 2021) is a formal-to-formal benchmark of competition problems, sourced from high school competitions such as the AMC, AIME, and IMO. MiniF2F is a multi-language benchmark, comprising of 488 problems each formalized in Lean 3, Metamath, Isabelle and HOL Light. We chose not to include formalizations in Metamath and HOL Light as they have not been the focus of attention for neural theorem-proving. A similar competition-style benchmark is FIMO (Liu et al., 2023), which contains 149 Lean 3 formalizations of IMO shortlist problems produced using a back-translation procedure with GPT-4. The automatically-generated formalizations are then manually verified. Both benchmarks are designed to measure _certifying_ the solution to the informal problem statement when one exists. Compfiles (2024) is a collection of 171 Lean 4 formalizations of competition problems, predominantly from the IMO and USAMO, often accompanied by a formal proof, which has not seen use in benchmarking automated theorem-provers. ProofNet (Azerbayev et al., 2023) introduced a benchmark of 371 exercises, formalized in Lean 3, from standard textbooks in the undergraduate mathematics curriculum. While largely not competition-based, problems in ProofNet draw from a broader library of concepts than miniF2F and FIMO, which rely only on high-school mathematics. LeanDojo (Yang et al., 2023) introduces a dataset of formal mathematics and proofs derived from Lean's mathlib library (mathlib Community, 2020), and trains a retrieval-augmented model towards generating proofs on their held-out test set. ProverBot9001 (Sanchez-Stern et al., 2020) introduced a dataset for theorems and proofs written in Coq derived from CompCert (Leroy, 2009), a formally verified C compiler. PISA (Jiang et al., 2021) is a dataset derived from Isabelle's Archive of Formal Proofs (AFP), which contains theorems and proofs from general mathematics as opposed to specifically competition problems.

**Informal Benchmarks.** There are also several popular benchmarks for informal (natural-language) mathematical reasoning. MATH (Hendrycks et al., 2021) is a collection of 12,500 mathematics problems, in natural language only, sourced from various high school competitions additionally supplied with step-by-step informal proofs. GSM8K (Cobbe et al., 2021) is a collection of 8,500 grade school mathematics problems, intended to benchmark natural language reasoning for mathematics-style problems. While benefiting from the abundance of natural language data, these benchmarks fall short, since in natural language, there is no automatic mechanism for certifiable verification of the reasoning path which yielded the numerical answer. For this reason, metrics for success on these benchmarks usually rely on exact-answer match, because verifying reasoning paths is imprecise and is best done by human experts. By contrast, theorem proving in formal proof assistants comes with a high-confidence signal for correctness of the reasoning path, or _proof_, of a theorem.

**Methods for Formal Theorem-Proving.** Significant effort has been spent on developing automatic theorem-provers for formal mathematics (Li et al., 2024). Most recent efforts train a neural module to perform proof-step prediction, which is then wrapped in a search mechanism to locate a valid proof. GPT-\(f\)(Polu and Sutskever, 2020) trains a transformer-based architecture on data derived from the Metamath library (Megill and Wheeler, 2019) for proof synthesis. PACT expands on GPT-\(f\) by incorporating auxiliary training tasks for the neural module towards theorem-proving in Lean 3. FMSCL (Polu et al., 2022) alternates proof-search and training to finetune their neural model based on proofs found during search. HTPS (Lample et al., 2022) uses a transformer-based neural module in an online, MCTS-inspired proof search in Lean 3 and Metamath. COPRA (Thakur et al., 2024) uses GPT-4 supplied with error feedback from the environment and lemmas from a retrieval mechanism for an agentic proof-search in Lean 3 and Coq. LLEMMA (Azerbayev et al., 2024) continues pretraining of Code Llama on a mathematics-based corpus dubbed Proof-Pile-2, and uses their learned model for formal proof search in Lean 4. DeepSeek-Prover Xin et al. (2024) produces synthetic Lean data en-masse for training their prover model. AlphaGeometry (Trinh et al., 2024) targets IMO problems in a geometry-specific proof assistant language using an interleaving search, where a neural module synthesizes auxiliary constructions and a symbolic engine produces deductive closures.

The Isabelle proof assistant (Paulson, 1994), given its declarative nature and powerful symbolic automation, has too been the focus of much attention for neural theorem proving. Isabelle features Sledgehammer (Paulson and Blanchette, 2015), an automated reasoning tool which calls external automated theorem provers (ATPs) for proof synthesis. Draft, Sketch, Prove (DSP) (Jiang et al., 2022b) uses a high-caliber LLM to generate natural language proofs and converts them into formal _sketches_ in Isabelle, whose gaps are then filled using Sledgehammer. Zhao et al. (2023) employed a diffusion model to predict an optimal ordering of the few-shot examples provided to the LLM in the DSP pipeline. Lyra (Zheng et al., 2023) utilized error-feedback from Isabelle's execution to modify holes in the sketch which were too difficult for the symbolic prover. POETRY (Wang et al., 2024) leverages recursion for theorem-proving and trains a neural module to produce proof sketches, as opposed to using in-context learning with an LLM. LEGO-Prover (Wang et al., 2023) extends the pipeline by incorporating a skill library which grows throughout the proof search task. Separate from approaches utilizing natural language proofs, Thor (Jiang et al., 2022a) trains a transformer-based architecture to predict successful invocations of Sledgehammer, along with the usual proof-step objective. Baldur (First et al., 2023) explored repairing erroneous proofs in Isabelle through the use of LLMs.

The Coq interactive theorem prover has seen use in both software verification and general mathematics. Famously, mechanized proofs of the Four Colour Theorem (Robertson et al., 1997) and the Feit-Thompson theorem (Gonthier et al., 2013) were produced in Coq. Similarly, numerous software verification projects have been undertaken in Coq, such as CompCert (a formally verified C compiler) and Verdi (Wilcox et al., 2015) (a framework for verifying distributed systems protocols). ASTactic (Yang and Deng, 2019) trained a neural module involving recurrent networks and attention on data collected from various Coq repositories. Proverbot9001 (Sanchez-Stern et al., 2020) targeted proof synthesis on a set of held-out theorems from the CompCert project. COPRA (Thakur et al., 2024) also evaluates on this CompCert-based task using their multi-language approach. Tactician (Blaauwbroek et al., 2020) develops a platform for proof automation for the Coq practitioner, with support for experimenting with new machine learning techniques for tactic prediction and proof search. (Zhang et al., 2021) explores several online learning techniques inside Tactician, including an approximate \(k\)-nearest neighbors method via locality sensitive hashing which we use for our evaluation. Graph2Tac (Blaauwbroek et al., 2024) uses graph neural networks for learning online hierarchical representations of new theorems and definitions, and is used for proof search within Tactician.

Conclusion

We presented PutnamBench, a benchmark for neural theorem-proving consisting of formalizations of Putnam competition problems. A distinctive feature of PutnamBench is that it spans a broad range of undergraduate-level mathematical topics, including algebra, analysis, and number theory. Another unique benefit is that it includes problems in Lean 4, Isabelle, and Coq, the three most popular formal proof frameworks.

As our experiments show, PutnamBench is a challenging benchmark: all current theorem-proving approaches fail to solve more than a handful of its problems. We believe that these failures include two root causes: (i) While current theorem-provers can effectively stitch together standard proof steps well-represented in the training corpus, they often fail at synthesizing new lemmas and orchestrating these lemmas into intricate proofs. (ii) Current methods often fail to leverage the deep knowledge available in mathematics repositories. Developing a new generation of neural theorem-provers in which these weaknesses are at least partly addressed is an exciting direction of future research.

Acknowledgements.This work was supported by NSF awards CCF-2212559 and CCF-2403211, the NSF Institute for Foundations of Machine Learning, and a gift by the Aziz Family Foundation. We thank Oliver Nash, Eric Wieser, Edward Lockhart, Fabian Gloeckle, Karl Palmskog, Lasse Blaauwbroek, Jason Rute, and Kaiyu Yang for useful discussions, aiding in benchmark maintenance, and support with setting up experiments.

## References

* A. Archive of Formal Proofs -- isa-afp.org. [https://www.isa-afp.org/](https://www.isa-afp.org/), 2004. [Accessed 25-05-2024].
* A. L. K. Klosinski, and L.C. Larson (1985)The _William Lowell Putnam Mathematical Competition: Problems and Solutions, 1965-1984_. MAA problem books series. Mathematical Association of America. External Links: ISBN 9780883854419, Link, Document Cited by: SS1.
* G. Allais, S. Dailler, H. Feree, J. Madiot, P. Pedrot, and A. Pouly (2023)Coqtail-math. External Links: Link Cited by: SS1.
* Z. Azerbayev, B. Piotrowski, H. Schoelkopf, E. W. Ayers, D. Radev, and J. Avigad (2023)Proofnet: autoformalizing and formally proving undergraduate-level mathematics. External Links: Link Cited by: SS1.
* Z. Azerbayev, H. Schoelkopf, K. Paster, M. Dos Santos, S. McAleer, A. Q. Jiang, J. Deng, S. Biderman, and S. Welleck (2024)Llemma: an open language model for mathematics. Mathematics. External Links: Link Cited by: SS1.
* L. Blaauwbroek, J. Urban, and H. Geuvers (2020)The tacticalar: a seamless, interactive tacticarner and prover for coq. pp. 271-277. External Links: Link Cited by: SS1.
* I. Grand Challenge (2019)IMO Grand Challenge -- imo-grand-challenge.github.io. External Links: Link Cited by: SS1.
* K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman (2021)Training verifiers to solve math word problems. External Links: Link Cited by: SS1.
* W. M. Putnam Mathematical Competition (2024)William Lowell Putnam Mathematical Competition -- Mathematical Association of America -- maa.org. [https://maa.org/putnam-2/](https://maa.org/putnam-2/), 2024. [Accessed 08-07-2024].

- dwrensha/compfiles: Catalog Of Math Problems Formalized In Lean -
- github.com. [https://github.com/dwrensha/compfiles](https://github.com/dwrensha/compfiles), 2024. [Accessed 25-05-2024].
* Coquelicot (2015) Coquelicot. Coquelicot. [https://gitlab.inria.fr/coquelicot/coquelicot](https://gitlab.inria.fr/coquelicot/coquelicot), 2015. [Accessed 01-06-2024].
* Czajka and Kaliszyk (2018) Lukasz Czajka and Cezary Kaliszyk. Hammer for coq: Automation for dependent type theory. _Journal of automated reasoning_, 61:423-453, 2018.
* First et al. (2023) Emily First, Markus N Rabe, Talia Ringer, and Yuriy Brun. Baldur: whole-proof generation and repair with large language models. _arXiv preprint arXiv:2303.04910_, 2023.
* GeoCoq/GeoCoq: A formalization of geometry in Coq based on Tarski's axiom system -
- github.com. [https://github.com/GeoCoq/GeoCoq](https://github.com/GeoCoq/GeoCoq), 2015. [Accessed 01-06-2024].
* Gonthier et al. (2013) Georges Gonthier, Andrea Asperti, Jeremy Avigad, Yves Bertot, Cyril Cohen, Francois Garillot, Stephane Le Roux, Assia Mahboubi, Russell O'Connor, Sidi Ould Biha, Ioana Pasca, Laurence Rideau, Alexey Solovyev, Enrico Tassi, and Laurent Thery. A machine-checked proof of the odd order theorem. In Sandrine Blazy, Christine Paulin-Mohring, and David Pichardie (eds.), _Interactive Theorem Proving_, pp. 163-179, Berlin, Heidelberg, 2013. Springer Berlin Heidelberg. ISBN 978-3-642-39634-2.
* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021.
* Jiang et al. (2022a) Albert Q. Jiang, Wenda Li, Szymon Tworkowski, Konrad Czechowski, Tomasz Odrzygozdz, Piotr Milos, Yuhuai Wu, and Mateja Jamnik. Thor: Wielding hammers to integrate language models and automated theorem provers, 2022a.
* Jiang et al. (2022b) Albert Q Jiang, Sean Welleck, Jin Peng Zhou, Wenda Li, Jiacheng Liu, Mateja Jamnik, Timothee Lacroix, Yuhuai Wu, and Guillaume Lample. Draft, sketch, and prove: Guiding formal theorem provers with informal proofs. _arXiv preprint arXiv:2210.12283_, 2022b.
* Jiang et al. (2021) Albert Qiaochu Jiang, Wenda Li, Jesse Michael Han, and Yuhuai Wu. Lisa: Language models of isabelle proofs, 2021.
* Kedlaya et al. (2002) K.S. Kedlaya, B. Poonen, R. Vakil, and Mathematical Association of America. _The William Lowell Putnam Mathematical Competition 1985-2000: Problems, Solutions and Commentary_. MAA Problem Book Series. Mathematical Association of America, 2002. ISBN 9780883858073. URL [https://books.google.com/books?id=AA-10A1nPDcC](https://books.google.com/books?id=AA-10A1nPDcC).
* Kedlaya et al. (2020) K.S. Kedlaya, D.M. Kane, J.M. Kane, and E.M. O'Dorney. _The William Lowell Putnam Mathematical Competition 2001-2016: Problems, Solutions, and Commentary_. Problem Books. American Mathematical Society, 2020. ISBN 9781470454272. URL [https://books.google.com/books?id=QwGMzQEACAAJ](https://books.google.com/books?id=QwGMzQEACAAJ).
* Lample et al. (2022) Guillaume Lample, Timothee Lacroix, Marie-Anne Lachaux, Aurelien Rodriguez, Amaury Hayat, Thibaut Lavril, Gabriel Ebner, and Xavier Martinet. Hypertree proof search for neural theorem proving. _Advances in Neural Information Processing Systems_, 35:26337-26349, 2022.
* Leroy (2009) Xavier Leroy. Formal verification of a realistic compiler. _Commun. ACM_, 52(7):107-115, jul 2009. ISSN 0001-0782. doi: 10.1145/1538788.1538814. URL [https://doi.org/10.1145/1538788.1538814](https://doi.org/10.1145/1538788.1538814).
* Li et al. (2024) Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie Si. A survey on deep learning for theorem proving, 2024.
* Liu et al. (2023) Chengwu Liu, Jianhao Shen, Huajian Xin, Zhengying Liu, Ye Yuan, Haiming Wang, Wei Ju, Chuanyang Zheng, Yichun Yin, Lin Li, Ming Zhang, and Qun Liu. Fimo: A challenge formal dataset for automated theorem proving, 2023.
* math-comp/math-comp: Mathematical Components -
- github.com. [https://github.com/math-comp/math-comp](https://github.com/math-comp/math-comp), 2015. [Accessed 01-06-2024].
* Liu et al. (2021)mathcomp-analysis. GitHub - math-comp/analysis: Mathematical Components compliant Analysis Library -- github.com. [https://github.com/math-comp/analysis](https://github.com/math-comp/analysis), 2017. [Accessed 05-06-2024].
* The mathlib Community [2020] The mathlib Community. The lean mathematical library. In _Proceedings of the 9th ACM SIGPLAN International Conference on Certified Programs and Proofs_, POPL '20. ACM, January 2020. doi: 10.1145/3372885.3373824. URL [http://dx.doi.org/10.1145/3372885.3373824](http://dx.doi.org/10.1145/3372885.3373824).
* Megill and Wheeler [2019] Norman D. Megill and David A. Wheeler. _Metamath: A Computer Language for Pure Mathematics_, 2019. URL [http://us.metamath.org/downloads/metamath.pdf](http://us.metamath.org/downloads/metamath.pdf). [http://us.metamath.org/downloads/metamath.pdf](http://us.metamath.org/downloads/metamath.pdf).
* de Moura and Ullrich [2021] Leonardo de Moura and Sebastian Ullrich. The lean 4 theorem prover and programming language. In _Automated Deduction-CADE 28: 28th International Conference on Automated Deduction, Virtual Event, July 12-15, 2021, Proceedings 28_, pp. 625-635. Springer, 2021.
* Newell et al. [1957] Allen Newell, John Clifford Shaw, and Herbert A Simon. Empirical explorations of the logic theory machine: a case study in heuristic. In _Papers presented at the February 26-28, 1957, western joint computer conference: Techniques for reliability_, pp. 218-230, 1957.
* OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
* Paulson and Blanchette [2015] Lawrence Paulson and Jasmin Blanchette. Three years of experience with sledgehammer, a practical link between automatic and interactive theorem provers, 02 2015.
* Paulson [1994] Lawrence C Paulson. _Isabelle: A generic theorem prover_. Springer, 1994.
* Polu and Sutskever [2020] Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. _arXiv preprint arXiv:2009.03393_, 2020.
* Polu et al. [2022] Stanislas Polu, Jesse Michael Han, Kunhao Zheng, Mantas Baksys, Igor Babuschkin, and Ilya Sutskever. Formal mathematics statement curriculum learning, 2022.
* Prize [2023] Prize. AIMO Prize -- aimoprize.com. [https://aimoprize.com/](https://aimoprize.com/), 2023. [Accessed 01-06-2024].
* Robertson et al. [1997] Neil Robertson, Daniel Sanders, Paul Seymour, and Robin Thomas. The four-colour theorem. _Journal of Combinatorial Theory, Series B_, 70(1):2-44, 1997. ISSN 0095-8956. doi: [https://doi.org/10.1006/jctb.1997.1750](https://doi.org/10.1006/jctb.1997.1750). URL [https://www.sciencedirect.com/science/article/pii/S0095895697917500](https://www.sciencedirect.com/science/article/pii/S0095895697917500).
* Sanchez-Stern et al. [2020] Alex Sanchez-Stern, Yousef Alhessi, Lawrence Saul, and Sorin Lerner. Generating correctness proofs with neural networks. In _Proceedings of the 4th ACM SIGPLAN International Workshop on Machine Learning and Programming Languages_, pp. 1-10, 2020.
* Thakur et al. [2024] Amitayush Thakur, George Tsoukalas, Yeming Wen, Jimmy Xin, and Swarat Chaudhuri. An InContext Learning Agent for Formal Theorem-Proving. In _First Conference on Language Modeling_, 2024.
* The Coq Development Team [2023] The Coq Development Team. The Coq Proof Assistant, September 2023.
* Trinh et al. [2024] Trieu H Trinh, Yuhuai Wu, Quoc V Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. _Nature_, 625(7995):476-482, 2024.
* rahul3613/ProofNet-lean4: ProofNet dataset ported into Lean 4 -
- github.com. [https://github.com/rahul3613/ProofNet-lean4](https://github.com/rahul3613/ProofNet-lean4), 2024. [Accessed 01-06-2024].
* Wang et al. [2023] Haiming Wang, Huajian Xin, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, Heng Liao, and Xiaodan Liang. Lego-prover: Neural theorem proving with growing libraries, 2023.
* Wang et al. [2024] Haiming Wang, Huajian Xin, Zhengying Liu, Wenda Li, Yinya Huang, Jianqiao Lu, Zhicheng Yang, Jing Tang, Jian Yin, Zhenguo Li, and Xiaodan Liang. Proving theorems recursively, 2024.
* Wang et al. [2020]* Wenzel et al. (2008) Makarius Wenzel, Lawrence C Paulson, and Tobias Nipkow. The isabelle framework. In _Theorem Proving in Higher Order Logics: 21st International Conference, TPHOLs 2008, Montreal, Canada, August 18-21, 2008. Proceedings 21_, pp. 33-38. Springer, 2008.
* Wilcox et al. (2015) James R. Wilcox, Doug Woos, Pavel Panchekha, Zachary Tatlock, Xi Wang, Michael D. Ernst, and Thomas Anderson. Verdi: a framework for implementing and formally verifying distributed systems. In _Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation_, PLDI '15, pp. 357-368, New York, NY, USA, 2015. Association for Computing Machinery. ISBN 9781450334686. doi: 10.1145/2737924.2737958. URL [https://doi.org/10.1145/2737924.2737958](https://doi.org/10.1145/2737924.2737958).
* Xin et al. (2024) Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in l1ms through large-scale synthetic data, 2024.
* Yang and Deng (2019) Kaiyu Yang and Jia Deng. Learning to prove theorems via interacting with proof assistants. In _International Conference on Machine Learning_, pp. 6984-6994. PMLR, 2019.
* Yang et al. (2023) Kaiyu Yang, Aidan M Swope, Alex Gu, Rahul Chalamala, Peiyang Song, Shixing Yu, Saad Godil, Ryan Prenger, and Anima Anandkumar. Leandojo: Theorem proving with retrieval-augmented language models. _arXiv preprint arXiv:2306.15626_, 2023.
* Zhang et al. (2021) Liao Zhang, Lasse Blaauwbroek, Bartosz Piotrowski, Prokop Cerny, Cezary Kaliszyk, and Josef Urban. Online machine learning techniques for coq: A comparison, 2021. URL [https://arxiv.org/abs/2104.05207](https://arxiv.org/abs/2104.05207).
* Zhao et al. (2023) Xueliang Zhao, Wenda Li, and Lingpeng Kong. Decomposing the enigma: Subgoal-based demonstration learning for formal theorem proving, 2023.
* Zheng et al. (2023) Chuanyang Zheng, Haiming Wang, Enze Xie, Zhengying Liu, Jiankai Sun, Huajian Xin, Jianhao Shen, Zhenguo Li, and Yu Li. Lyra: Orchestrating dual correction in automated theorem proving, 2023.
* Zheng et al. (2021) Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. Minif2f: a cross-system benchmark for formal olympiad-level mathematics. _arXiv preprint arXiv:2109.00110_, 2021.

## Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] We support our main claims in Section 3 and Section 4. 2. Did you describe the limitations of your work? [Yes] We discussed in Section 3 the challenges of formalizing certain problem categories such as geometry and probability due to the nature of support for such mathematical theory in each language. 3. Did you discuss any potential negative societal impacts of your work? [N/A] We do not anticipate any negative societal impact of our work. 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We have read the ethics review guidelines and ensured our paper conforms to them.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] We do not include any theoretical results. 2. Did you include complete proofs of all theoretical results? [N/A] We do not include any theoretical results.
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] We disclosed all information related to the experiments, which use open-sourced methods. We have also included the URL to our dataset: [https://github.com/trishullab/PUTNAM/](https://github.com/trishullab/PUTNAM/). 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A] We did not perform any training. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [No] We evaluate our selected methodologies using established metrics accepted by the neural theorem-proving community. See Section 4. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Most of our experiments rely on calls to GPT-4, we include sampling details. We also mention the hyperparameters to calls to symbolic methods in Section 4.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] We did cite the creators of any existing assets we used. 2. Did you mention the license of the assets? [Yes] We aligned the license of our benchmark with the license of those assets. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] We included our dataset by sharing the following URL: [https://github.com/trishullab/PUTNAM/](https://github.com/trishullab/PUTNAM/). 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] We obtained permission from the MAA. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] Our data does not contain such content.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] We did not conduct research with human subjects nor crowdsource. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] We did not conduct research with human subjects nor crowdsource. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A] We did not conduct research with human subjects nor crowdsource.

Appendix

We include further examples of formalizations from PutnamBench below.

**Putnam 2001 B4.** Let \(S\) denote the set of rational numbers different from \(\{-1,0,1\}\). Define \(f:S\to S\) by \(f(x)=x-1/x\). Prove or disprove that

\[\bigcap_{n=1}^{\infty}f^{(n)}(S)=\varnothing,\]

where \(f^{(n)}\) denotes \(f\) composed with itself \(n\) times.

Figure 5: A formalization of Putnam 2009 B1 in Coq relying on the MathComp repository.

Figure 6: A formalization of Putnam 2001 B4 in Lean 4. As the problem requires deciding whether the infinite intersection is empty, it is not directly the statement of a theorem. We consider the associated “solution” of this problem to be a boolean value, and factor it out from the theorem statement. sorry is the placeholder keyword for Lean.

**Putnam 2020 A3.** Let \(a_{0}=\pi/2\), and let \(a_{n}=\sin(a_{n-1})\) for \(n\geq 1\). Determine whether

\[\sum_{n=1}^{\infty}a_{n}^{2}\]

converges.

**Putnam 1997 A4.** Let \(G\) be a group with identity \(e\) and \(\phi:G\to G\) a function such that

\[\phi(g_{1})\phi(g_{2})\phi(g_{3})=\phi(h_{1})\phi(h_{2})\phi(h_{3})\]

whenever \(g_{1}g_{2}g_{3}=e=h_{1}h_{2}h_{3}\). Prove that there exists an element \(a\in G\) such that \(\psi(x)=a\phi(x)\) is a homomorphism.

theorem putnam_1997_a4 (G : Type*)

[Group G]

[\(\varphi:G\to G\)]

[\(\eta_{\varphi}:\forall g1\ g2\ g3\ h1\ h2\ h3:G,\ (g1\ *g2\ *g3=1\ \wedge\ h1\ *h2\ *h3=1)\)

\(\rightarrow\varphi\ g1\ *\ \varphi\ g2\ *\ \varphi\ g3=\varphi\ h1\ *\ \varphi\ h2\ *\ \varphi\ h3)

: \(\exists\ a:G,\ let\ \psi:=fun\ g\Rightarrow a\ *\ \varphi\ g;\ \forall\ x\ y:G,\ \psi\ (x\ *y)=\psi\ x\ *\ \psi\)

:= sorry

Figure 8: A formalization of Putnam 1997 A4, which requires knowledge of group theory, in Lean 4. The informal statement is slightly underspecified - \(g_{1},g_{2},g_{3},h_{1},h_{2},h_{3}\) are not explicitly defined to be in \(G\). To produce the formalization, we must be specific about the type of \(g_{i},h_{i}\).

Figure 7: A formalization of Putnam 2020 A3 in Lean 4. As the problem requires deciding whether the series converges, it is not directly the statement of a theorem. We consider the associated “solution” of this problem to be a boolean value, and factor it out from the theorem statement.

**Putnam 2018 B1.** Let \(\mathcal{P}\) be the set of vectors defined by

\[\mathcal{P}=\left\{\begin{pmatrix}a\\ b\end{pmatrix}\middle|\,0\leq a\leq 2,0\leq b\leq 100,\text{ and }a,b\in\mathbb{Z}\right\}\]

Find all \(\mathbf{v}\in\mathcal{P}\) such that the set \(\mathcal{P}\setminus\{\mathbf{v}\}\) obtained by omitting vector \(\mathbf{v}\) from \(\mathcal{P}\) can be partitioned into two sets of equal size and equal sum.

**Putnam 1992 B6.** Let \(\mathcal{M}\) be a set of real \(n\times n\) matrices such that

1. \(I\in\mathcal{M}\), where \(I\) is the \(n\times n\) identity matrix;
2. if \(A\in\mathcal{M}\) and \(B\in\mathcal{M}\), then exactly one of \(AB\in\mathcal{M}\) and \(-AB\in\mathcal{M}\) holds;
3. if \(A\in\mathcal{M}\) and \(B\in\mathcal{M}\), then either \(AB=BA\) or \(AB=-BA\);
4. if \(A\in\mathcal{M}\) and \(A\neq I\), there is at least one \(B\in\mathcal{M}\) such that \(AB=-BA\).

Prove that \(\mathcal{M}\) contains at most \(n^{2}\) matrices.

**Putnam 1992 B6.**

**Putnam 1992 B6.**

**Fixes n :: nat and M :: "(real^'n^n'n) set"**

**assumes npos: "n > 0"**

**and pncard: "CARD('n) = n"**

**and h1: "mat 1 \(\in\) M"**

**and h2: "vACM. Vb\(\in\)M. (A*B \(\in\) M) \(\neq\) (-A*+B \(\in\) M)"**

**and h3: "vACM. Vb\(\in\)M. (A*+B = B*+A) \(\vee\) (A*+B = -B*+A)"**

**and h4: "vACM. (A \(\neq\) mat 1 \(\rightarrow\) (\(\exists\)B\(\in\)M. A*+B = -B*+A))"**

**shows "card M \(\leq\) n^2"**

**sorry**

Figure 10: An Isabelle formalization of Putnam 1992 B6.

Figure 9: A formalization of Putnam 2018 B1, which requires the Vector class from mathlib4.

**Putnam 2012 A3.** Let \(f:[-1,1]\rightarrow\mathbb{R}\) be a continuous function such that

1. \(f(x)=\frac{2-x^{2}}{2}f(\frac{x^{2}}{2-x^{2}})\) for every \(x\) in \([-1,1]\),
2. \(f(0)=1\), and
3. \(\lim_{x\to 1^{-}}\frac{f(x)}{\sqrt{1-x}}\) exists and is finite.

Prove that \(f\) is unique, and express \(f(x)\) in closed form.

definition putnam_2012_a3_solution :: "real \(\Rightarrow\) real" where

"putnam_2012_a3_solution \(\equiv\) (\(\lambda\)x::real. sqrt (1 - x^2))"

theorem putnam_2012_a3:

fixes 5 :: "real set"

and hf :: "(real \(\Rightarrow\) real) \(\Rightarrow\) bool"

defines "S \(\equiv\) (-1..1)"

and "hf \(\equiv\) (\(\lambda\)f::real=real. continuous_on S f \(\wedge\)

(Vx\(\in\)S. fx x = ((2 - x^2))2)*f (x^2/(2 - x^2))) \(\wedge\) f 0 = 1 \(\wedge\)

(\(\exists\)y::real. filterlin (\(\lambda\)x::real. (fx)/sqrt (1 - x)) (nhds y) (at_left 1)))"

shows "hf putnam_2012_a3_solution \(\langle\)

(vf::real=real. hf f \(\rightarrow\) (Vx\(\in\)S. fx = putnam_2012_a3_solution x))"

sorry

Figure 11: An Isabelle formalization of Putnam 2012 A3. The mechanism for factoring the solution out of the theorem statement is similar to that of Lean.

Figure 12: A Coq formalization of Putnam 1980 A5. This formalization is done using Coquelicot, a Coq repository outside of the standard library. The Coq equivalent of sorry is Admitted.

**Putnam 2017 B2.** Suppose that a positive integer \(N\) can be expressed as the sum of \(k\) consecutive positive integers

\[N=a+(a+1)+(a+2)+\cdots+(a+k-1)\]

for \(k=2017\) but for no other values of \(k>1\). Considering all positive integers \(N\) with this property, what is the smallest positive integer \(a\) that occurs in any of these expressions?

From mathcomp Require Import all_ssreflect all_algebra.

Set Implicit Arguments.

Unset Strict Implicit.

Unset Printing Implicit Defensive.

Local Open Scope ring_scope.

Definition putnam_2017_b2_solution : nat := 16.

Theorem putnam_2017_b2 :

let seq (a : int) (K : nat) := \sum_(0 <= i < k) (a + iX:Z) in

let valid (a : int) := a > 0 /\ (forall (b : int) (K : nat), b > 0 -> gt k 1 ->

seq a 2017Xnat = seq b k -> k = 2017Xnat) in

valid putnam_2017_b2_solution /\ (forall a, valid a -> a >=

putnam_2017_b2_solution%:Z).

Proof. Admitted.

**Putnam 1988 B1.** A _composite_ is a product \(ab\) with \(a\) and \(b\) not necessarily distinct integers

\(\{2,3,4,\dots\}\). Show that every composite is expressible as \(xy+xz+yz+1\) with \(x,y,z\) positive integers.

Require Import ZArith Zmutheory Lia.

Open Scope Z.

Theorem putnam_1988_b1:

forall (a : Z), a >= 2 ->

forall (b : Z), b >= 2 ->

exists (x y z: Z), x > 0 /\ y > 0 /\ z > 0 /\

a * b = x * y + y * z + z * x + 1.

Proof.

intros a Ha b Hb.

exists 1, (a - 1), (b - 1).

split.

- lia.

- split.

+ lia.

+ split.

* lia.

Qed.

Figure 14: A Coq proof of Putnam 1988 B1 generated through a few-shot invocation of GPT-4. The proof is similar to that of the Lean version, also discovered by GPT-4. The main difficulty of the problem is to choose the values of \(x,y,z\) given \(a,b\). Once correctly supplied, the remainder of the proof is routine and can be done with automated methods like lia which handles linear arithmetic.

Figure 13: A Coq formalization of Putnam 2017 B2. As the problem requires a numerical witness, we factor that out using Coq’s syntax for making definitions.

Figure 16: Parts of the “system prompt” used by GPT-4 for Lean 4 evaluations. Due to GPT-4’s tendency towards producing outputs in Lean 3 syntax, our prompt places special attention towards preventing such syntactic mistakes. A similar modification is made to COPRA’s system prompt for Lean 3.

Figure 15: Examples of formalizations of easy problems in MiniF2F. While useful for benchmarking straightforward mathematical reasoning in a formal setting, these problems are quite simple compared to the competition problems present in PutnamBench. We note that MiniF2F does include some formalizations of problems sourced directly from high school competitions, but these are fewer in number.

Goals to prove:

[GOALS]

[GOAL] 1

DifferentiableAt \(\mathbb{R}\) (fun x => g x / hg0 x) @ + DifferentiableAt \(\mathbb{R}\) g 0

[HYPOTHESES] 1

[HYPOTHESIS] case mpr

[HYPOTHESIS] f : True

[HYPOTHESIS] g hg@ : R + R

[HYPOTHESIS] hcg : hg0 @ \(\neq\) @

[HYPOTHESIS] hfg : ContinuousAt hg@ 0

[HYPOTHESIS] hfg_div : DifferentiableAt \(\mathbb{R}\) (fun x => g x * hg@ x) @

[STEPS]

[STEP] constructor

[STEP] intro h

[STEP] trivial

[STEP] intros f g hg@ hcg hfg hfg_div

[INCORRECT STEPS]

[STEP] apply differentiable_at_of_mul

[LAST STEP]

apply differentiable_at.div

[ERROR MESSAGE]

error: unknown identifier 'differentiable_at.div'

[END]

theorem putnam_2001_a1

(S : Type*)

[Mul S]

(hS : V a b : S, (a * b) * a = b)

: V a b : S, a * (b * a) = b := by

intro a b

 have h1 : (a * (b * a)) * a = b * a := by

rw +[mul_assoc]

rw [HS]

apply h1

Figure 17: An example of a failed tactic prediction during proof search for Putnam 2011 B2 using COPRA in Lean 4. GPT-4 predicts a tactic involving the premise “differentiable_at.div,” which exists in Lean 3, but not Lean 4. Even with the system prompt asserting outputs should involve Lean 4 syntax alone, GPT-4 is not always capable of making the distinction.

Figure 18: A failed proof generated by few-shot invocation of GPT-4. GPT-4 misunderstands that the hypothesis [Mul S], which gives an operation \(\star\) and asserts it is a binary operation on \(S\), also asserts associativity of the operation. The tactic \(\mathtt{rw}\) [+mul_assoc], which performs a rewrite using the associativity of \(\star\), is hence not applicable in this setting.

**Putnam 1986 B1.** Inscribe a rectangle of base \(b\) and height \(h\) in a circle of radius one, and inscribe an isosceles triangle in the region of the circle cut off by one base of the rectangle. For what values of \(h\) do the rectangle and triangle have the same area?

theorem putnam_1986_b1: fixes b h :: "real" assumes hbh :: "b > @ \(\wedge\) h > @ \(\wedge\) b^2 + h^2 = 2^2" and areaeq : "b*h = @.5 * b * (1 - h/2)" shows "h = ( 2 / 5 )" proof - from assms have eq1: "b^2 + h^2 = 4" by simp from areaeq have eq2: "b * h = @.5 * b * (1 - h/2)" by simp then have eq3: "b * h = @.5 * b - @.25 * b * h" by algebra then have eq4: "1.25 * b * h = @.5 * b" by algebra then have eq5: "h = @.5 / 1.25" using assms(1) by simp then show?thesis by simp qed

Figure 19: A successful proof in Isabelle discovered using GPT-4. While the theorem statement differs from the formalization, which is algebraic, we note that the official solution is also algebraic and is similar to the generated Isabelle proof.

[MISSING_PAGE_FAIL:23]

theorem putnam.1971_b1:  fixes Smul ::"'S > 'S > 'S" (infxl "*" 70)  assumes heslef: "Vx::'S. x * x = x"  and h2: "Vx y z::'S. (x * y) * z = (y * z) * x"  shows"Vx y z::'S. (x * y) * z = x * (y * z) \(\wedge\) x * y = y * x"  proof -  have comm: "Vx y::'S. x * y * x"  proof  fix x y ::'S  have "(x * y) * x = (y * x) * x" using h2 by blast (* sledgehammer *)  also have "... = y * x" using hself by (metis h2) (* sledgehammer *)  finally have "(x * y) * x = y * x" by simp (* sledgehammer *)  then have "x * y = y * x" using hself by (metis h2) (* sledgehammer *)  thus "x * y = y * x" by simp  qed  have assoc: "Vx y z::'S. (x * y) * z = x * (y * z)"  proof  fix x y z ::'S  have "(x * y) * z = (y * z) * x" using h2 sledgehammer  also have "... = x * (y * z)" using comm sledgehammer  finally show "(x * y) * z = x * (y * z)" by simp  qed  show "Vx y z::'S. (x * y) * z = x * (y * z) \(\wedge\) x * y = y * x"  using assoc comm by simp  qed

Figure 21: An erroneous generation produced by DSP for Putnam 1971 B1. While a single invocation of Sledgehammer can prove this problem, the formal sketch generated in the pipeline is much more complex and is erroneous, leading to a failed proof attempt.

Figure 22: The other two Isabelle formalizations solved with invocations of Sledgehammer. We note that the problems Sledgehammer was capable of solving are all problems involving binary operations on sets. It is not surprising that SMT solvers are capable of solving such problems, which do not require reasoning about complicated objects.

Goals to prove:

[GOALS]

[GOAL] 1

1 > 0 /\

1 > 0 /\

a * b - 1 > 0 /\ a * b = 1 * 1 + 1 * (a * b - 1) + (a * b - 1) * 1 + 1

[HYPOTHESES] 1

[HYPOTHESIS] a : Z

[HYPOTHESIS] Ha : a >= 2

[HYPOTHESIS] b : Z

[HYPOTHESIS] Hb : b >= 2

[STEPS]

[STEP] intros a Ha b Hb.

[LAST STEP]

exists 1, 1, (a * b - 1).

[SUCCESS]

[END]

...

Goals to prove: # Step 32 of Search

[GOALS]

[GOAL] 1

a - 1 > 0 /\

1 > 0 /\

b - 1 > 0 /\ a * b = (a - 1) * 1 + 1 * (b - 1) + (b - 1) * (a - 1) + 1

[HYPOTHESES] 1

[HYPOTHESIS] a : Z

[HYPOTHESIS] Ha : a >= 2

[HYPOTHESIS] b : Z

[HYPOTHESIS] Hb : b >= 2

[STEPS]

[STEP] intros a Ha b Hb.

[LAST STEP]

exists (a - 1), 1, (b - 1).

[SUCCESS]

[END]

Figure 23: Early in COPRA’s attempt on Putnam 1988 B1, an incorrect prediction of \(x,y,z\) given \(a,b\) is made, which dooms that path of search as the most crucial step is the correct choice. Later, at step 32 of search, COPRA backtracks and then successfully predicts a correct choice for \(x,y,z\). Once this step is generated, the remainder of the proof is straightforward.