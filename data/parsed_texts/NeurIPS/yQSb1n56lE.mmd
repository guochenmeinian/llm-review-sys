# RFold: RNA Secondary Structure Prediction with Decoupled Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The secondary structure of ribonuclecic acid (RNA) is more stable and accessible in the cell than its tertiary structure, making it essential for functional prediction. Although deep learning has shown promising results in this field, current methods suffer from poor generalization and high complexity. In this work, we present RFold, a simple yet effective RNA secondary structure prediction in an end-to-end manner. RFold introduces a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Moreover, RFold adopts attention maps as informative representations instead of designing hand-crafted features. Extensive experiments demonstrate that RFold achieves competitive performance and about eight times faster inference efficiency than the state-of-the-art method.

## 1 Introduction

Ribonucleic acid is essential in structural biology for its diverse functional classes [8; 45; 49; 18]. The functions of RNA molecules are determined by their structure [57]. The secondary structure, which contains the nucleotide base pairing information, as shown in Fig. 1, is crucial for the correct functions of RNA molecules [13; 11; 68]. Although experimental assays such as X-ray crystallography [6], nuclear magnetic resonance (NMR) [15], and cryogenic electron microscopy [12] can be implemented to determine RNA secondary structure, they suffer from low throughput and expensive cost.

Computational RNA secondary structure prediction methods have become increasingly popular due to their high efficiency [31]. Currently, these methods can be broadly classified into two categories [50; 14; 55; 60]: (i) comparative sequence analysis and (ii) single sequence folding algorithm. Comparative sequence analysis determines the secondary structure conserved among homologous sequences but the limited known RNA families hinder its development [35; 36; 28; 22; 21; 16; 43]. Researchers thus resort to single RNA sequence folding algorithms that do not need multiple sequence alignment information. A classical category of computational RNA folding algorithms is to use dynamic programming (DP) that assumes the secondary structure is a result of energy minimization [3; 44; 39; 73; 42; 10]. However, energy-based

Figure 1: The graph and matrix representation of an RNA secondary structure example.

approaches usually require a nested structure, which ignores some biologically essential structures such as pseudoknots, i.e., non-nested base pairs [5; 54; 70], as shown in Fig. 2. Since predicting secondary structures with pseudoknots under the energy minimization framework has shown to be hard and NP-complete [66; 14], deep learning techniques are introduced as an alternative approach.

Attempts to overcome the limitations of energy-based methods have motivated deep learning methods in the absence of DP. SPOT-RNA [55] is a seminal work that ensembles ResNet [24] and LSTM [25] to identify molecular features. SPOT-RNA does not constrain the output space into valid RNA secondary structures, which degrades its generalization ability [32]. E2Efold [5] employs an unrolled algorithm for constrained programming that post-processes the network output to satisfy the constraints. E2Efold introduces a convex relaxation to make the optimization tractable, leading to possible constraint violations and poor generalization ability [53; 14]. Developing an appropriate optimization that forces the output to be valid becomes an important issue. Apart from the optimization problem, state-of-the-art approaches require hand-crafted features and introduce the pre-processing step for such features, which is inefficient and needs expert knowledge. CDPfold [72] develops a matrix representation based on sequence pairing that reflects the implicit matching between bases. UFold [14] follows the exact post-process mechanism as E2Efold and uses hand-crafted features from CDPfold with U-Net [51] model architecture to improve the performance.

Although promising, current deep learning methods on RNA secondary structure prediction have been distressed by: (1) _the optimization process that is complicated and poor in generalization_ and (2) _the data pre-processing that requires expensive complexity and expert knowledge_. In this paper, we present RFold, a simple yet effective RNA secondary structure prediction method in an end-to-end manner. Specifically, we introduce a decoupled optimization process that decomposes the vanilla constraint satisfaction problem into row-wise and column-wise optimization, simplifying the solving process while guaranteeing the validity of the output. Besides, we adopt attention maps as informative representations to automatically learn the pair-wise interactions of the nucleotide bases instead of using hand-crafted features to perform data pre-processing. We conduct extensive experiments to compare RFold with state-of-the-art methods on several benchmark datasets and show the superior performance of our proposed method. Moreover, RFold has faster inference efficiency than those methods due to its simplicity.

## 2 Related work

### Comparative Sequence Analysis

Comparative sequence analysis determines base pairs conserved among homologous sequences [55; 17; 28; 36; 35; 19; 20]. ILM [52] combines thermodynamic and mutual information content scores. Sankoff [27] merges the sequence alignment and maximal-pairing folding methods [46]. Dynalign [41] and Carnac [62; 47] are the subsequent variants of Sankoff algorithms. RNA forester [26] introduces a tree alignment model for global and local alignments. However, the limited number of known RNA families [21; 16; 43] impedes the development of comparative methods.

### Energy-based Folding Algorithms

When the secondary structure consists only of nested base pairing, dynamic programming can efficiently predict the structure by minimizing energy. Early works in this category include Vienna RNAfold [39], Mfold [73], RNAstructure [42], and CONTRAfold [10]. Faster implementations that speed up dynamic programming have been proposed, such as Vienna RNAplfold [4], LocalFold [37], and LinearFold [30]. However, these methods cannot accurately predict secondary structures with pseudoknots, as predicting the lowest free energy structures with pseudoknots is NP-complete [40], making it difficult to improve performance.

Figure 2: Examples of nested and non-nested secondary structures.

### Learning-based Folding Algorithms

SPOT-RNA [55] is a seminal work that employs deep learning for RNA secondary structure prediction. SPOT-RNA2 [56] improves its predecessor by using evolution-derived sequence profiles and mutational coupling. Inspired by Raptor-X [65] and SPOT-Contact [23], SPOT-RNA uses ResNet and bidirectional LSTM with a sigmoid function to output the secondary structures. MXfold [1] is also an early work that combines support vector machines and thermodynamic models. CDPfold [72], DMFGold [64], and MXFold2 [53] integrate deep learning techniques with energy-based methods. E2Efold [5] takes a remarkable step in constraining the output to be valid by learning unrolled algorithms. However, its relaxation for making the optimization tractable may violate the structural constraints. UFold [14] further introduces U-Net model architecture to improve performance.

## 3 Preliminaries and Backgrounds

### Preliminaries

The primary structure of RNA is the ordered linear sequence of bases, which is typically represented as a string of letters. Formally, an RNA sequence can be represented as \(\bm{X}=(x_{1},...,x_{L})\), where \(x_{i}\in\{\mathrm{A},\mathrm{U},\mathrm{C},\mathrm{G}\}\) denotes one of the four bases, i.e., _Adenine_ (A), _Uracil_ (U), _Ctyosine_ (C), and _Guanine_ (G). The secondary structure of RNA is a contact map represented as a matrix \(\bm{M}\in\{0,1\}^{L\times L}\), where \(\bm{M}_{ij}=1\) if the \(i\)-th and \(j\)-th bases are paired. In the RNA secondary structure prediction problem, we aim to obtain a model with learnable parameters \(\Theta\) that learns a mapping \(\mathcal{F}_{\Theta}:\bm{X}\mapsto\bm{M}\) by exploring the interactions between bases. Here, we decompose the mapping \(\mathcal{F}_{\Theta}\) into two sub-mappings as:

\[\mathcal{F}_{\Theta}:=\mathcal{H}_{\theta_{h}}\circ\mathcal{G}_{\theta_{g}},\] (1)

where \(\mathcal{H}_{\theta_{h}}:\bm{X}\mapsto\bm{H}\), \(\mathcal{G}_{\theta_{g}}:\bm{H}\mapsto\bm{M}\) are mappings parameterized by \(\theta_{h}\) and \(\theta_{g}\), respectively. \(\bm{H}\in\mathbb{R}^{L\times L}\) is regarded as the unconstrained output of neural networks.

### Backgrounds

It is worth noting that there are hard constraints on the formation of RNA secondary structure, meaning that certain types of pairing are not available [59]. Such constraints [5] can be formally described as follows:

* (a) Only three types of nucleotide combinations can form base pairs: \(\mathcal{B}:=\{\mathrm{AU},\mathrm{UA}\}\cup\{\mathrm{GC},\mathrm{CG}\}\cup \{\mathrm{GU},\mathrm{UG}\}\). For any base pair \(x_{i}x_{j}\) where \(x_{i}x_{j}\notin\mathcal{B}\), \(\bm{M}_{ij}=0\).
* (b) No sharp loops within three bases. For any adjacent bases, there can be no pairing between them, i.e., \(\forall|i-j|\leqslant 3,\bm{M}_{ij}=0\).
* (c) There can be at most one pair for each base, i.e., \(\forall i,\sum_{j=1}^{L}\bm{M}_{ij}\leqslant 1\).

The available space of valid secondary structures is all _symmetric_ matrices \(\in\{0,1\}^{L\times L}\) that satisfy the above three constraints. The first two constraints can be satisfied easily. We define a constraint matrix \(\bm{\bar{M}}\) as: \(\bm{\bar{M}}_{ij}:=1\) if \(x_{i}x_{j}\in\mathcal{B}\) and \(|i-j|\geqslant 4\), and \(\bm{\bar{M}}_{ij}:=0\) otherwise. By element-wise multiplication of the network output and the constraint matrix \(\bm{\bar{M}}\), invalid pairs are masked.

The critical issue in obtaining a valid RNA secondary structure is the third constraint, i.e., _processing the network output to create a symmetric binary matrix that only allows a single "1" to exist in each row and column_. There are different strategies for dealing with this issue.

Spot-Rnais a typical kind of method that imposes minor constraints. It takes the original output of neural networks \(\bm{H}\) and directly applies the \(\mathrm{Sigmoid}\) function, assigning a value of 1 to those greater than 0.5 and 0 to those less than 0.5. This process can be represented as:

\[\mathcal{G}(\bm{H})=\mathbbm{1}_{\left[\mathrm{Sigmoid}(\bm{H})>0.5\right]} \odot\bm{H}.\] (2)

Here, the offset term \(s\) has been set to 0.5. No explicit constraints are imposed, and no additional parameters \(\theta_{g}\) are required.

E2Efoldformulates the problem with constrained optimization and introduces an intermediate variable \(\widehat{\bm{M}}\in\mathbb{R}^{L\times L}\). It aims to maximize the predefined score function:

\[\mathcal{S}(\widehat{\bm{M}},\bm{H})=\frac{1}{2}\left\langle\bm{H}-s,\mathcal{T} (\widehat{\bm{M}})\right\rangle-\rho\|\widehat{\bm{M}}\|_{1},\] (3)

where \(\mathcal{T}(\widehat{\bm{M}})=\frac{1}{2}(\widehat{\bm{M}}\odot\widehat{\bm{M }}+(\widehat{\bm{M}}\odot\widehat{\bm{M}})^{T})\odot\widehat{\bm{M}}\) ensures the output is a symmetric matrix that satisfies the constraints (a-b), \(s\) is an offset term that is set as \(\log(9.0)\) here, \(\left\langle\cdot,\cdot\right\rangle\) denotes matrix inner product and \(\rho\|\widehat{\bm{M}}\|_{1}\) is a \(\ell_{1}\) penalty term to make the matrix to be sparse.

The constraint (c) is imposed by requiring Eq. 3 to satisfy \(\mathcal{T}(\widehat{\bm{M}})\mathbb{1}\leqslant\mathbb{1}\). Thus, Eq. 3 is rewritten as:

\[\mathcal{S}(\widehat{\bm{M}},\bm{H})=\min_{\bm{\lambda}\geqslant 0}\frac{1}{ 2}\left\langle\bm{H}-s,\mathcal{T}(\widehat{\bm{M}})\right\rangle-\rho\| \widehat{\bm{M}}\|_{1}\quad-\left\langle\bm{\lambda},\mathrm{ReLU}(\mathcal{T }(\widehat{\bm{M}})\mathbb{1}-\mathbb{1})\right\rangle,\] (4)

where \(\bm{\lambda}\in\mathbb{R}^{L}_{+}\) is a Lagrange multiplier.

Formally, this process can be represented as:

\[\mathcal{G}_{\theta_{g}}(\bm{H})=\mathcal{T}(\arg\max_{\widehat{\bm{M}}\in \mathbb{R}^{L\times L}}\mathcal{S}(\widehat{\bm{M}},\bm{H})).\] (5)

Though three constraints are explicitly imposed in E2Efold, this method requires iterative steps to approximate the valid solutions and cannot guarantee that the results are entirely valid. Moreover, it needs a set of parameters \(\theta_{g}\) in this processing, making tuning the model complex.

## 4 RFold

### Decoupled Optimization

We propose the following formulation for the constrained optimization problem in RNA secondary structure problem:

\[\min_{\bm{M}}-\mathrm{tr}(\bm{M}^{T}\widehat{\bm{H}})\] \[\mathrm{s.t.}\sum_{j=1}^{L}\bm{M}_{ij}\leqslant 1,\forall i;\ \sum_{i=1}^{L}\bm{M}_{ij}\leqslant 1, \forall j,\] (6)

where \(\mathrm{tr}(\bm{M}^{T}\widehat{\bm{H}})=\sum_{i=1}^{L}\sum_{j=1}^{L}\bm{M}_{ij }\widehat{\bm{H}}_{ij}\) represents the trace operation. The matrix \(\widehat{\bm{H}}\) is symmetrized based on the original network output \(\bm{H}\) while satisfying the constraints (a-b) in Sec. 3.2 by multiplying the constraint matrix \(\widetilde{\bm{M}}\), i.e., \(\widehat{\bm{H}}=(\bm{H}\odot\bm{H}^{T})\odot\widetilde{\bm{M}}\).

We then propose to decouple the optimization process into row-wise and column-wise optimizations, and define the corresponding selection schemes as \(S_{r}\) and \(S_{c}\) respectively:

\[S_{r}=\{S_{r}^{1},S_{r}^{2},...,S_{r}^{L}\},\ S_{c}=\{S_{c}^{1},S_{c}^{2},...,S_ {c}^{L}\},\] (7)

where \(S_{r}^{i}\in\{0,1\}^{L}\) signifies the selection scheme on the \(i\)th row, and \(S_{c}^{j}\in\{0,1\}^{L}\) represents the selection scheme on the \(j\)th column. The score function is defined as:

\[\mathcal{S}(S_{r},S_{c},\widehat{\bm{H}})=-\mathrm{tr}(\bm{M}^{T}\widehat{ \bm{H}}),\] (8)

where \(S_{r},S_{c}\) constitute the decomposition of \(\bm{M}\). The goal of the score function is to maximize the dot product of \(\bm{M}\) and \(\widehat{\bm{H}}\) in order to select the maximum value in \(\widehat{\bm{H}}\). Our proposed decoupled optimization reformulates the original constrained optimization problem in Equation 6 as follows:

\[\min_{S_{r},S_{c}}\ \mathcal{S}(S_{r},S_{c})\] (9) \[\mathrm{s.t.}\ \sum_{i=1}^{L}S_{r}^{i}\leqslant 1,\forall i;\ \sum_{j=1}^{L}S_{c}^{j}\leqslant 1, \forall j.\]

If the corresponding \(\widehat{\bm{H}}_{ij}\) have the highest score in its row \(\{\widehat{\bm{H}}_{ik}\}_{k=1}^{L}\) and its column \(\{\widehat{\bm{H}}_{kj}\}_{k=1}^{L}\), then \(\bm{M}_{ij}=1\). By exploring the optimal \(S_{r}\) and \(S_{c}\), the chosen base pairs can be obtained by the optimal scheme \(S=S_{r}\otimes S_{c}\).

### Row-Col Argmax

With the proposed decoupled optimization, the optimal matrix can be easily obtained using the variant Argmax function:

\[\mathrm{Row-Col-Argmax}(\widehat{\bm{H}})=\mathrm{Row-Argmax}(\widehat{\bm{H}}) \odot\mathrm{Col-Argmax}(\widehat{\bm{H}})\] (10)

where \(\mathrm{Row-Argmax}\) and \(\mathrm{Col-Argmax}\) are row-wise and column-wise Argmax functions respectively:

\[\mathrm{Row-Argmax}_{ij}(\widehat{\bm{H}}) =\begin{cases}1,\;\mathrm{if}\;\max\{\widehat{\bm{H}}_{ik}\}_{k=1 }^{L}=\widehat{\bm{H}}_{ij},\\ 0,\;\mathrm{otherwise}.\end{cases}\] (11) \[\mathrm{Col-Argmax}_{ij}(\widehat{\bm{H}}) =\begin{cases}1,\;\mathrm{if}\;\max\{\widehat{\bm{H}}_{kj}\}_{k=1 }^{L}=\widehat{\bm{H}}_{ij},\\ 0,\;\mathrm{otherwise}.\end{cases}\]

**Theorem 1**.: Given a symmetric matrix \(\widehat{\bm{H}}\in\mathbb{R}^{L\times L}\), the matrix \(\mathrm{Row-Col-Argmax}(\widehat{\bm{H}})\) is also a symmetric matrix.

Proof.: See Appendix C.1.

As shown in Fig. 3, taking a random symmetric \(6\times 6\) matrix as an example, we show the output matrics of \(\mathrm{Row-Argmax}\), \(\mathrm{Col-Argmax}\), and \(\mathrm{Row-Col-Argmax}\) functions, respectively. The Row-Col Argmax selects the value that has the maximum value on both its row and column while keeping the output matrix symmetric.

From Theorem 1, we can observe that \(\mathrm{Row-Col-Argmax}(\widehat{\bm{H}})\) is a symmetric matrix that satisfies the constraint (c). Since \(\widehat{\bm{H}}\) already satisfies constraints (a-b), the optimized output is:

\[\mathcal{G}(\bm{H})=S_{r}\otimes S_{c}=\mathrm{Row-Col-Argmax}(\widehat{\bm{H }}),\] (12)

where \(S_{r},S_{c}=\arg\min_{S_{r},S_{c}}-\mathrm{tr}(S_{r},S_{c})\).

### Row-Col Softmax

Though the Row-Col Argmax function can obtain the optimal matrix \(\mathcal{G}(\bm{H})\), it is not differentiable and thus cannot be directly used in the training process. In the training phase, we need to use a differentiable function to approximate the optimal results. Therefore, we propose using a Row-Col Softmax function to approximate the Row-Col Argmax function for training. To achieve this, we perform row-wise Softmax and column-wise Softmax on the symmetric matrix \(\widehat{\bm{H}}\) separately, as shown below:

\[\mathrm{Row-Softmax}_{ij}(\widehat{\bm{H}}) =\frac{\exp(\widehat{\bm{H}}_{ij})}{\sum_{k=1}^{L}\exp(\widehat{ \bm{H}}_{ik})},\] (13) \[\mathrm{Col-Softmax}_{ij}(\widehat{\bm{H}}) =\frac{\exp(\widehat{\bm{H}}_{ij})}{\sum_{k=1}^{L}\exp(\widehat{ \bm{H}}_{kj})}.\]

The Row-Col Softmax function is then defined as follows:

\[\mathrm{Row-Col-Softmax}(\widehat{\bm{H}})=\frac{1}{2}(\mathrm{Row-Softmax}( \widehat{\bm{H}})+\mathrm{Col-Softmax}(\widehat{\bm{H}})),\] (14)

Note that we use the average of \(\mathrm{Row-Softmax}(\widehat{\bm{H}})\) and \(\mathrm{Col-Softmax}(\widehat{\bm{H}})\) instead of the element product as shown in Equ. 10 for the convenience of optimization.

Figure 3: The visualization of the \(\mathrm{Row-Col-Argmax}\) function.

**Theorem 2**.: Given a symmetric matrix \(\widehat{\bm{H}}\in\mathbb{R}^{L\times L}\), the matrix \(\mathrm{Row}\)-\(\mathrm{Col}\)-\(\mathrm{Softmax}(\widehat{\bm{H}})\) is also a symmetric matrix.

Proof.: See Appendix C.2.

As shown in Fig. 4, taking a random symmetric \(6\times 6\) matrix as an example, we show the output matrics of \(\mathrm{Row}\)-\(\mathrm{Softmax}\), \(\mathrm{Col}\)-\(\mathrm{Softmax}\), and \(\mathrm{Row}\)-\(\mathrm{Col}\)-\(\mathrm{Softmax}\) functions, respectively. It can be seen that the output matrix of \(\mathrm{Row}\)-\(\mathrm{Col}\)-\(\mathrm{Softmax}\) is still symmetric. Leveraging the differentiable property of \(\mathrm{Row}\)-\(\mathrm{Col}\)-\(\mathrm{Softmax}\), the model can be easily optimized.

In the training phase, we apply the differentiable Row-\(\mathrm{Col}\) Softmax activation and optimize the mean square error (MSE) loss function between \(\mathcal{G}(\bm{H})\) and \(\bm{M}\):

\[\mathcal{L}(\mathcal{G}(\bm{H}),\bm{M})=\frac{1}{L^{2}}\|\mathrm{Row}\text{- Col}\text{-Softmax}(\widehat{\bm{H}})-\bm{M}\|^{2}.\] (15)

### Seq2map Attention

To simplify the pre-processing step that constructs hand-crafted features based on RNA sequences, we propose a Seq2map attention module that can automatically produce informative representations. We start with a sequence in the one-hot form \(\bm{X}\in\mathbb{R}^{L\times 4}\) and obtain the sum of the token embedding and positional embedding as the input for the Seq2map attention. For convenience, we denote the input as \(\bm{Z}\in\mathbb{R}^{L\times D}\), where \(D\) is the hidden layer size of the token and positional embeddings.

Motivated by the recent progress in attention mechanisms [63, 9, 34, 7, 38, 29, 69, 38], we aim to develop a highly effective sequence-to-map transformation based on pair-wise attention. We obtain the query \(\bm{Q}\in\mathbb{R}^{L\times D}\) and key \(\bm{K}\in\mathbb{R}^{L\times D}\) by applying per-dim scalars and offsets to \(\bm{Z}\):

\[\bm{Q}=\gamma_{Q}\bm{Z}+\beta_{Q},\ \bm{K}=\gamma_{K}\bm{Z}+\beta_{K},\] (16)

where \(\gamma_{Q},\gamma_{K},\beta_{Q},\beta_{K}\in\mathbb{R}^{L\times D}\) are learnable parameters.

Then, the pair-wise attention map is obtained by:

\[\widehat{\bm{Z}}=\mathrm{ReLU}^{2}(\bm{Q}\bm{K}^{T}/L),\] (17)

where \(\mathrm{ReLU}^{2}\) is an activation function that can be recognized as a simplified Softmax function in vanilla Transformers [58]. The output of Seq2map is the gated representation of \(\widehat{\bm{Z}}\):

\[\widehat{\bm{Z}}=\widehat{\bm{Z}}\ \odot\ \sigma(\widehat{\bm{Z}}),\] (18)

where \(\sigma(\cdot)\) is the \(\mathrm{Sigmoid}\) function that performs as a gate operation.

As shown in Fig. 5, we identify the problem of predicting \(\bm{H}\in\mathbb{R}^{L\times L}\) from the given sequence attention map \(\widehat{\bm{Z}}\in\mathbb{R}^{L\times L}\) as an image-to-image segmentation problem and apply the U-Net model architecture to extract pair-wise information.

Figure 4: The visualization of the \(\mathrm{Row}\)-\(\mathrm{Col}\)-\(\mathrm{Softmax}\) function.

Figure 5: The overview model of RFold.

Experiments

We conduct experiments to compare our proposed RFold with state-of-the-art and commonly used methods in the field of RNA secondary structure prediction. Multiple experimental settings are taken into account, including standard RNA secondary structure prediction, generalization evaluation, large-scale benchmark evaluation, and inference time comparison. Ablation studies are also presented.

DatasetsWe use three benchmark datasets: (i) RNAStralign [61], one of the most comprehensive collections of RNA structures, is composed of 37,149 structures from 8 RNA types; (ii) ArchiveII [57], a widely used benchmark dataset in classical RNA folding methods, containing 3,975 RNA structures from 10 RNA types; (iii) bpRNA [55], is a large scale benchmark dataset, containing 102,318 structures from 2,588 RNA types.

BaselinesWe compare our proposed RFold with baselines including energy-based folding methods such as Mfold [73], RNAsoft [2], RNAfold [39], RNAstructure [42], CONTRAfold [10], Contextfold [71], and LinearFold [30]; learning-based folding methods such as SPOT-RNA [55], Externafold [67], E2Efold [5], MXfold2 [53], and UFold [14].

MetricsWe evaluate the performance by precision, recall, and F1 score, which are defined as:

\[\mathrm{Precision}=\frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}},\ \mathrm{Recall}=\frac{ \mathrm{TP}}{\mathrm{TP}+\mathrm{FN}},\ \mathrm{F1}=2\ \frac{\mathrm{Precision}\cdot\mathrm{Recall}}{\mathrm{ Precision}+\mathrm{Recall}},\] (19)

where \(\mathrm{TP},\mathrm{FP}\), and \(\mathrm{FN}\) denote true positive, false positive and false negative, respectively.

Implementation detailsFollowing [14], we train the model for 100 epochs with the Adam optimizer. The learning rate is 0.001, and the batch size is 1 for sequences with different lengths.

### Standard RNA Secondary Structure Prediction

Following [5], we split the RNAStralign dataset into training, validation, and testing sets by stratified sampling to ensure every set has all RNA types. We report the experimental results in Table 1. It can be seen that energy-based methods achieve relatively weak F1 scores ranging from 0.420 to 0.633. Learning-based folding algorithms like E2Efold and UFold can significantly improve performance by large margins, while RFold obtain even better performance among all the metrics. Moreover, RFold obtains about 8% higher precision than the state-of-the-art method. This phenomenon suggests that our proposed decoupled optimization is strict to satisfy all the hard constraints for predicting valid structures.

### Generalization Evaluation

To verify the generalization ability of our proposed RFold, we directly evaluate the performance on another benchmark dataset ArchiveII using the pre-trained model on the RNAStralign training dataset. Following [5], we exclude RNA sequences in ArchiveII that have overlapping RNA types with the RNAStralign dataset for a fair comparison. The results are reported in Table 2.

It can be seen that traditional methods achieve F1 scores in the range of 0.545 to 0.842. Among the state-of-the-art methods, RFold attains the highest F1 score. It is noteworthy that RFold has a relatively lower recall metric and significantly higher precision metric. This phenomenon may be due to the strict constraints imposed by RFold. Although none of the current learning-based methods can meet all the constraints presented in Sec. 3.2, the predictions made by RFold are guaranteed to be valid. Therefore, RFold may cover fewer pairwise interactions, resulting in a lower recall metric. Nonetheless, the highest F1 score indicates the excellent generalization ability of RFold.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Method & Precision & Recall & F1 \\ \hline Mfold & 0.450 & 0.398 & 0.420 \\ RNAStrald & 0.516 & 0.568 & 0.540 \\ RNAstructure & 0.537 & 0.568 & 0.550 \\ CONTRAfold & 0.608 & 0.663 & 0.633 \\ LinearFold & 0.620 & 0.606 & 0.609 \\ CDPfold & 0.633 & 0.597 & 0.614 \\ E2Efold & 0.866 & 0.788 & 0.821 \\ UFold & 0.905 & 0.927 & 0.915 \\ \hline RFold & **0.981** & **0.973** & **0.977** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results on RNAStralign test set. Results in bold and underlined are the top-1 and top-2 performances, respectively.

### Large-scale Benchmark Evaluation

The large-scale benchmark dataset bpRNA has a fixed training set (TR0), evaluation set (VL0), and testing set (TS0). Following [55, 53, 14], we train the model in bpRNA-TR0 and evaluate the performance on bpRNA-TS0 by using the best model learned from bpRNA-VL0. We summarize the evaluation results in Table 3. It can be seen that RFold significantly improves the previous state-of-the-art method SPOT-RNA by 4.0% in the F1 score.

Following [14], we conduct an experiment on long-range interactions. The bpRNA-TS0 dataset contains more versatile RNA sequences of different lengths and various types, which can be a reliable evaluation. Given a sequence of length \(L\), the long-range base pairing is defined as the paired and unpaired bases with intervals longer than \(L/2\). As shown in Table 4, RFold performs unexpectedly well on these long-range base pairing predictions. We can also find that UFold performs better in long-range cases than the complete cases. The possible reason may come from the U-Net model architecture that learns multi-scale features. RFold significantly improves UFold in all the metrics by large margins, demonstrating its strong predictive ability.

### Inference Time Comparison

We compared the running time of various methods for predicting RNA secondary structures using the RNAStarlign testing set with the same experimental setting as in [14]. The results are presented in Table 5, which shows the average inference time per sequence. The fastest energy-based method is LinearFold, which takes an average of about 0.43s for each sequence. The previous learning-based baseline, UFold, takes about 0.16s. RFold has the highest inference speed, costing only about 0.02s per sequence. In particular, RFold is about eight times faster than UFold and sixteen times faster than MXfold2. The fast inference time of RFold is due to its simple sequence-to-map transformation.

\begin{table}
\begin{tabular}{c c c c} \hline \hline Method & Precision & Recall & F1 \\ \hline Mfold & 0.668 & 0.590 & 0.621 \\ RNAfold & 0.663 & 0.613 & 0.631 \\ RNAstructure & 0.664 & 0.606 & 0.628 \\ CONTRAfold & 0.696 & 0.651 & 0.665 \\ LinearFold & 0.724 & 0.605 & 0.647 \\ RNAsoft & 0.665 & 0.594 & 0.622 \\ Eternalfold & 0.667 & 0.622 & 0.636 \\ E2Efold & 0.734 & 0.660 & 0.686 \\ SPOT-RNA & 0.743 & 0.726 & 0.711 \\ MXfold2 & 0.788 & 0.760 & 0.768 \\ Contextfold & 0.873 & 0.821 & 0.842 \\ UFold & 0.887 & **0.928** & 0.905 \\ \hline RFold & **0.938** & 0.910 & **0.921** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results on ArchiveF1 dataset.

\begin{table}
\begin{tabular}{c c c} \hline \hline Method & Precision & Recall & F1 \\ \hline Mfold & 0.315 & 0.450 & 0.356 \\ RNAfold & 0.304 & 0.448 & 0.350 \\ RNAstructure & 0.299 & 0.428 & 0.339 \\ CONTRAfold & 0.306 & 0.439 & 0.349 \\ LinearFold & 0.281 & 0.355 & 0.305 \\ RNAsoft & 0.310 & 0.448 & 0.353 \\ Externafold & 0.308 & 0.458 & 0.355 \\ SPOT-RNA & 0.361 & 0.492 & 0.403 \\ MXfold2 & 0.318 & 0.450 & 0.360 \\ Contextfold & 0.332 & 0.432 & 0.363 \\ UFold & 0.543 & 0.631 & 0.584 \\ \hline RFold & **0.803** & **0.765** & **0.701** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Results on long-range bpRNA-TS0 set.

\begin{table}
\begin{tabular}{c c} \hline \hline Method & Time \\ \hline CDPfold (Tensorflow) & 300.11 s \\ RNAstructure (C) & 142.02 s \\ CONTRAfold (C++) & 30.58 s \\ Mfold (C) & 7.65 s \\ Eternalfold (C++) & 6.42 s \\ RNAsoft (C+) & 4.58 s \\ RNAfold (C) & 0.55 s \\ LinearFold (C++) & 0.43 s \\ SPOT-RNA(Pytorch) & 77.80 s (GPU) \\ E2Efold (Pytorch) & 0.40 s (GPU) \\ MXfold2 (Pytorch) & 0.31 s (GPU) \\ UFold (Pytorch) & 0.16 s (GPU) \\ \hline RFold & **0.02 s** (GPU) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Inference time on the RNAStarlign.

\begin{table}
\begin{tabular}{c c c} \hline \hline Method & Precision & Recall & F1 \\ \hline Mfold & 0.501 & 0.627 & 0.538 \\ E2Efold & 0.140 & 0.129 & 0.130 \\ RNAstructure & 0.494 & 0.622 & 0.533 \\ RNAsoft & 0.497 & 0.626 & 0.535 \\ RNAfold & 0.494 & 0.631 & 0.536 \\ Contextfold & 0.529 & 0.607 & 0.546 \\ LinearFold & 0.561 & 0.581 & 0.550 \\ MXfold2 & 0.519 & 0.646 & 0.558 \\ Externafold & 0.516 & 0.666 & 0.563 \\ CONTRAfold & 0.528 & 0.655 & 0.567 \\ SPOT-RNA & 0.594 & **0.693** & 0.619 \\ UFold & 0.521 & 0.588 & 0.553 \\ RFold & **0.692** & 0.635 & **0.644** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Results on bpRNA-TS0 set.

### Ablation Study

Decoupled OptimizationTo validate the effectiveness of our proposed decoupled optimization, we conduct an experiment that replaces them with other strategies. The results are summarized in Table 6, where RFold-E and RFold-S denote our model with the strategies of E2Efold and SPOT-RNA, respectively. We ignore the recent UFold because it follows exactly the same strategy as E2Efold. We also report the validity which is a sample-level metric evaluating whether all the constraints are satisfied. Though RFold-E has comparable performance in the first three metrics with ours, many of its predicted structures are invalid. The strategy of SPOT-RNA has incorporated no constraint that results in its low validity. Moreover, its strategy seems to not fit our model well, which may be caused by the simplicity of our RFold model.

Seq2map AttentionWe also conduct an experiment to evaluate the proposed Seq2map attention. We replace the Seq2map attention with the hand-crafted features from UFold and the outer concatenation from SPOT-RNA, which are denoted as RFold-U and RFold-SS, respectively. In addition to performance metrics, we also report the average inference time for each RNA sequence to evaluate the model complexity. We summarize the result in Table 7. It can be seen that RFold-U takes much more inference time than our RFold and RFold-SS due to the heavy computational cost when loading and learning from hand-crafted features. Moreover, it is surprising to find that RFold-SS has a little better performance than RFold-U, with the least inference time for its simple outer concatenation operation. However, neither RFold-U nor RFold-SS can provide informative representations.

### Visualization

We visualize two examples predicted by RFold and UFold in Fig. 6. The corresponding F1 scores are denoted at the bottom of each plot. The first secondary structures is a simple example of a nested structure. It can be seen that UFold may fail in such a case. The second secondary structures is much more difficult that contains over 300 bases of the non-nested structure. While UFold fails in such a complex case, RFold can predict the structure accurately. Due to the limited space, we provide more visualization comparisons in Appendix D.

## 6 Conclusion

In this study, we present RFold, a simple yet effective learning-based model for RNA secondary structure prediction. We propose decoupled optimization to replace the complicated post-processing strategies while incorporating constraints for the output. Seq2map attention is proposed for sequence-to-map transformation, which can automatically learn informative representations from a single sequence without extensive pre-processing operations. Comprehensive experiments demonstrate that RFold achieves competitive performance with faster inference speed. We hope RFold can provide a new perspective for efficient RNA secondary structure prediction.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Method & Precision & Recall & F1 & Validity \\ \hline RFold & **0.981** & **0.973** & **0.977** & **100.00\%** \\ \hline RFold-E & 0.888 & 0.906 & 0.896 & 50.31\% \\ RFold-S & 0.223 & 0.988 & 0.353 & 0.00\% \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation study on optimzation strategies (RNAStralign testing set).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Method & Precision & Recall & F1 & Time \\ \hline RFold & **0.981** & **0.973** & **0.977** & 0.0167 \\ \hline RFold-U & 0.875 & 0.941 & 0.906 & 0.0507 \\ RFold-SS & 0.886 & 0.945 & 0.913 & **0.0158** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Ablation study on pre-processing strategies (RNAStralign testing set).

Figure 6: Visualization of the true and predicted structures.

## References

* [1] M. Akiyama, K. Sato, and Y. Sakakibara. A max-margin training of rna secondary structure prediction integrated with the thermodynamic model. _Journal of bioinformatics and computational biology_, 16(06):1840025, 2018.
* [2] M. Andronescu, R. Aguirre-Hernandez, A. Condon, and H. H. Hoos. Rnasoft: a suite of rna secondary structure prediction and design software tools. _Nucleic acids research_, 31(13):3416-3422, 2003.
* [3] S. Bellaousov, J. S. Reuter, M. G. Seetin, and D. H. Mathews. Rnastructure: web servers for rna secondary structure prediction and analysis. _Nucleic acids research_, 41(W1):W471-W474, 2013.
* [4] S. H. Bernhart, I. L. Hofacker, and P. F. Stadler. Local rna base pairing probabilities in large sequences. _Bioinformatics_, 22(5):614-615, 2006.
* [5] X. Chen, Y. Li, R. Umarov, X. Gao, and L. Song. Rna secondary structure prediction by learning unrolled algorithms. In _International Conference on Learning Representations_, 2019.
* [6] H.-K. Cheong, E. Hwang, C. Lee, B.-S. Choi, and C. Cheong. Rapid preparation of rna samples for nmr spectroscopy and x-ray crystallography. _Nucleic acids research_, 32(10):e84-e84, 2004.
* [7] K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin, L. Kaiser, et al. Rethinking attention with performers. In _International Conference on Learning Representations_, 2020.
* [8] F. Crick. Central dogma of molecular biology. _Nature_, 227(5258):561-563, 1970.
* [9] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier. Language modeling with gated convolutional networks. In _International conference on machine learning_, pages 933-941. PMLR, 2017.
* [10] C. B. Do, D. A. Woods, and S. Batzoglou. Contrafold: Rna secondary structure prediction without physics-based models. _Bioinformatics_, 22(14):e90-e98, 2006.
* [11] J. Fallmann, S. Will, J. Engelhardt, B. Gruning, R. Backofen, and P. F. Stadler. Recent advances in rna folding. _Journal of biotechnology_, 261:97-104, 2017.
* [12] S. M. Fica and K. Nagai. Cryo-electron microscopy snapshots of the spliceosome: structural insights into a dynamic ribonucleoprotein machine. _Nature structural & molecular biology_, 24(10):791-799, 2017.
* [13] G. E. Fox and C. R. Woese. 5s rna secondary structure. _Nature_, 256(5517):505-507, 1975.
* [14] L. Fu, Y. Cao, J. Wu, Q. Peng, Q. Nie, and X. Xie. Ufold: fast and accurate rna secondary structure prediction with deep learning. _Nucleic acids research_, 50(3):e14-e14, 2022.
* [15] B. Furgig, C. Richter, J. Wohnert, and H. Schwalbe. Nmr spectroscopy of rna. _ChemBioChem_, 4(10):936-962, 2003.
* [16] P. P. Gardner, J. Daub, J. G. Tate, E. P. Nawrocki, D. L. Kolbe, S. Lindgreen, A. C. Wilkinson, R. D. Finn, S. Griffiths-Jones, S. R. Eddy, et al. Rfam: updates to the rna families database. _Nucleic acids research_, 37(suppl_1):D136-D140, 2009.
* [17] P. P. Gardner and R. Giegerich. A comprehensive comparison of comparative rna structure prediction approaches. _BMC bioinformatics_, 5(1):1-18, 2004.
* [18] S. Geisler and J. Coller. Rna in unexpected places: long non-coding rna functions in diverse cellular contexts. _Nature reviews Molecular cell biology_, 14(11):699-712, 2013.
* [19] J. Gorodkin, L. J. Heyer, and G. D. Stormo. Finding the most significant common sequence and structure motifs in a set of rna sequences. _Nucleic acids research_, 25(18):3724-3732, 1997.
* [20] J. Gorodkin, S. L. Stricklin, and G. D. Stormo. Discovering common stem-loop motifs in unaligned rna sequences. _Nucleic Acids Research_, 29(10):2135-2144, 2001.

* [21] S. Griffiths-Jones, A. Bateman, M. Marshall, A. Khanna, and S. R. Eddy. Rfam: an rna family database. _Nucleic acids research_, 31(1):439-441, 2003.
* [22] R. R. Gutell, J. C. Lee, and J. J. Cannone. The accuracy of ribosomal rna comparative structure models. _Current opinion in structural biology_, 12(3):301-310, 2002.
* [23] J. Hanson, K. Paliwal, T. Litfin, Y. Yang, and Y. Zhou. Accurate prediction of protein contact maps by coupling residual two-dimensional bidirectional long short-term memory with convolutional neural networks. _Bioinformatics_, 34(23):4039-4045, 2018.
* [24] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* [25] S. Hochreiter and J. Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* [26] M. Hochsmann, T. Toller, R. Giegerich, and S. Kurtz. Local similarity in rna secondary structures. In _Computational Systems Bioinformatics. CSB2003. Proceedings of the 2003 IEEE Bioinformatics Conference. CSB2003_, pages 159-168. IEEE, 2003.
* [27] I. L. Hofacker, S. H. Bernhart, and P. F. Stadler. Alignment of rna base pairing probability matrices. _Bioinformatics_, 20(14):2222-2227, 2004.
* [28] I. L. Hofacker, M. Fekete, and P. F. Stadler. Secondary structure prediction for aligned rna sequences. _Journal of molecular biology_, 319(5):1059-1066, 2002.
* [29] W. Hua, Z. Dai, H. Liu, and Q. Le. Transformer quality in linear time. In _International Conference on Machine Learning_, pages 9099-9117. PMLR, 2022.
* [30] L. Huang, H. Zhang, D. Deng, K. Zhao, K. Liu, D. A. Hendrix, and D. H. Mathews. Linear-fold: linear-time approximate rna folding by 5'-to-3' dynamic programming and beam search. _Bioinformatics_, 35(14):i295-i304, 2019.
* [31] E. Iorns, C. J. Lord, N. Turner, and A. Ashworth. Utilizing rna interference to enhance cancer drug discovery. _Nature reviews Drug discovery_, 6(7):556-568, 2007.
* [32] A. J. Jung, L. J. Lee, A. J. Gao, and B. J. Frey. Rffold: Rna secondary structure prediction using deep learning with domain inductive bias.
* [33] A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In _International Conference on Machine Learning_, pages 5156-5165. PMLR, 2020.
* [34] N. Kitaev, L. Kaiser, and A. Levskaya. Reformer: The efficient transformer. In _International Conference on Learning Representations_, 2019.
* [35] B. Knudsen and J. Hein. Rna secondary structure prediction using stochastic context-free grammars and evolutionary history. _Bioinformatics (Oxford, England)_, 15(6):446-454, 1999.
* [36] B. Knudsen and J. Hein. Pfold: Rna secondary structure prediction using stochastic context-free grammars. _Nucleic acids research_, 31(13):3423-3428, 2003.
* [37] S. J. Lange, D. Maticzka, M. Mohl, J. N. Gagnon, C. M. Brown, and R. Backofen. Global or local? predicting secondary structure and accessibility in mrnas. _Nucleic acids research_, 40(12):5215-5226, 2012.
* [38] S. Li, Z. Wang, Z. Liu, C. Tan, H. Lin, D. Wu, Z. Chen, J. Zheng, and S. Z. Li. Efficient multi-order gated aggregation network. _arXiv preprint arXiv:2211.03295_, 2022.
* [39] R. Lorenz, S. H. Bernhart, C. Honer zu Siederdissen, H. Tafer, C. Flamm, P. F. Stadler, and I. L. Hofacker. Viennarna package 2.0. _Algorithms for molecular biology_, 6(1):1-14, 2011.
* [40] R. B. Lyngso and C. N. Pedersen. Rna pseudoknot prediction in energy-based models. _Journal of computational biology_, 7(3-4):409-427, 2000.

* [41] D. H. Mathews and D. H. Turner. Dynalign: an algorithm for finding the secondary structure common to two rna sequences. _Journal of molecular biology_, 317(2):191-203, 2002.
* [42] D. H. Mathews and D. H. Turner. Prediction of rna secondary structure by free energy minimization. _Current opinion in structural biology_, 16(3):270-278, 2006.
* [43] E. P. Nawrocki, S. W. Burge, A. Bateman, J. Daub, R. Y. Eberhardt, S. R. Eddy, E. W. Floden, P. P. Gardner, T. A. Jones, J. Tate, et al. Rfam 12.0: updates to the rna families database. _Nucleic acids research_, 43(D1):D130-D137, 2015.
* [44] R. Nicholas and M. Zuker. Unafold: Software for nucleic acid folding and hybridization. _Bioinformatics_, 453:3-31, 2008.
* [45] H. F. Noller. Structure of ribosomal rna. _Annual review of biochemistry_, 53(1):119-162, 1984.
* [46] R. Nussinov, G. Pieczenik, J. R. Griggs, and D. J. Kleitman. Algorithms for loop matchings. _SIAM Journal on Applied mathematics_, 35(1):68-82, 1978.
* [47] O. Perriquet, H. Touzet, and M. Dauchet. Finding the common structure shared by two homologous rnas. _Bioinformatics_, 19(1):108-116, 2003.
* [48] Z. Qin, W. Sun, H. Deng, D. Li, Y. Wei, B. Lv, J. Yan, L. Kong, and Y. Zhong. cosformer: Rethinking softmax in attention. In _International Conference on Learning Representations_, 2021.
* [49] A. Rich and U. RajBhandary. Transfer rna: molecular structure, sequence, and properties. _Annual review of biochemistry_, 45(1):805-860, 1976.
* [50] E. Rivas. The four ingredients of single-sequence rna secondary structure prediction. a unifying perspective. _RNA biology_, 10(7):1185-1196, 2013.
* [51] O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In _International Conference on Medical image computing and computer-assisted intervention_, pages 234-241. Springer, 2015.
* [52] J. Ruan, G. D. Stormo, and W. Zhang. An iterated loop matching approach to the prediction of rna secondary structures with pseudoknots. _Bioinformatics_, 20(1):58-66, 2004.
* [53] K. Sato, M. Akiyama, and Y. Sakakibara. Rna secondary structure prediction using deep learning with thermodynamic integration. _Nature communications_, 12(1):1-9, 2021.
* [54] M. G. Seetin and D. H. Mathews. Rna structure prediction: an overview of methods. _Bacterial regulatory RNA_, pages 99-122, 2012.
* [55] J. Singh, J. Hanson, K. Paliwal, and Y. Zhou. Rna secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning. _Nature communications_, 10(1):1-13, 2019.
* [56] J. Singh, K. Paliwal, T. Zhang, J. Singh, T. Litfin, and Y. Zhou. Improved rna secondary structure and tertiary base-pairing prediction using evolutionary profile, mutational coupling and two-dimensional transfer learning. _Bioinformatics_, 37(17):2589-2600, 2021.
* [57] M. F. Sloma and D. H. Mathews. Exact calculation of loop formation probability identifies folding motifs in rna secondary structures. _RNA_, 22(12):1808-1818, 2016.
* [58] D. So, W. Manke, H. Liu, Z. Dai, N. Shazeer, and Q. V. Le. Searching for efficient transformers for language modeling. _Advances in Neural Information Processing Systems_, 34:6010-6022, 2021.
* [59] E. W. Steeg. Neural networks, adaptive optimization, and rna secondary structure prediction. _Artificial intelligence and molecular biology_, pages 121-160, 1993.
* [60] M. Szikszai, M. J. Wise, A. Datta, M. Ward, and D. Mathews. Deep learning models for rna secondary structure prediction (probably) do not generalise across families. _bioRxiv_, 2022.

* [61] Z. Tan, Y. Fu, G. Sharma, and D. H. Mathews. Turbofold ii: Rna structural alignment and secondary structure prediction informed by multiple homologs. _Nucleic acids research_, 45(20):11570-11581, 2017.
* [62] H. Touzet and O. Perriquet. Carnac: folding families of related rnas. _Nucleic acids research_, 32(suppl_2):W142-W145, 2004.
* [63] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [64] L. Wang, Y. Liu, X. Zhong, H. Liu, C. Lu, C. Li, and H. Zhang. Dmfold: A novel method to predict rna secondary structure with pseudoknots based on deep learning and improved base pair maximization principle. _Frontiers in genetics_, 10:143, 2019.
* [65] S. Wang, S. Sun, Z. Li, R. Zhang, and J. Xu. Accurate de novo prediction of protein contact map by ultra-deep learning model. _PLoS computational biology_, 13(1):e1005324, 2017.
* [66] X. Wang and J. Tian. Dynamic programming for np-hard problems. _Procedia Engineering_, 15:3396-3400, 2011.
* [67] H. K. Wayment-Steele, W. Kladwang, A. I. Strom, J. Lee, A. Treuille, E. Participants, and R. Das. Rna secondary structure packages evaluated and improved by high-throughput experiments. _BioRxiv_, pages 2020-05, 2021.
* [68] E. Westhof and V. Fritsch. Rna folding: beyond watson-crick pairs. _Structure_, 8(3):R55-R65, 2000.
* [69] H. Wu, J. Wu, J. Xu, J. Wang, and M. Long. Flowformer: Linearizing transformers with conservation flows. _arXiv preprint arXiv:2202.06258_, 2022.
* [70] X. Xu and S.-J. Chen. Physics-based rna structure prediction. _Biophysics reports_, 1(1):2-13, 2015.
* [71] S. Zakov, Y. Goldberg, M. Elhadad, and M. Ziv-Ukelson. Rich parameterization improves rna structure prediction. _Journal of Computational Biology_, 18(11):1525-1542, 2011.
* [72] H. Zhang, C. Zhang, Z. Li, C. Li, X. Wei, B. Zhang, and Y. Liu. A new method of rna secondary structure prediction based on convolutional neural network and dynamic programming. _Frontiers in genetics_, 10:467, 2019.
* [73] M. Zuker. Mfold web server for nucleic acid folding and hybridization prediction. _Nucleic acids research_, 31(13):3406-3415, 2003.