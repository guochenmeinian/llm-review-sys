# MixFormerV2: Efficient Fully Transformer Tracking

 Yutao Cui\({}^{\dagger}\)   Tianhui Song\({}^{\dagger}\)   Gangshan Wu   Limin Wang\({}^{*}\)

State Key Laboratory for Novel Software Technology, Nanjing University, China

https://github.com/MCG-NJU/MixFormerV2

###### Abstract

Transformer-based trackers have achieved high accuracy on standard benchmarks. However, their efficiency remains an obstacle to practical deployment on both GPU and CPU platforms. In this paper, to mitigate this issue, we propose a fully transformer tracking framework based on the successful MixFormer tracker [14], coined as _MixFormerV2_, without any dense convolutional operation or complex score prediction module. We introduce four special prediction tokens and concatenate them with those from target template and search area. Then, we apply a unified transformer backbone on these mixed token sequence. These prediction tokens are able to capture the complex correlation between target template and search area via mixed attentions. Based on them, we can easily predict the tracking box and estimate its confidence score through simple MLP heads. To further improve the efficiency of MixFormerV2, we present a new distillation-based model reduction paradigm, including dense-to-sparse distillation and deep-to-shallow distillation. The former one aims to transfer knowledge from the dense-head based MixViT to our fully transformer tracker, while the latter one is for pruning the backbone layers. We instantiate two MixForemrV2 trackers, where the **MixFormerV2-B** achieves an AUC of 70.6% on LaSOT and AUC of 56.7% on TNL2k with a high GPU speed of 165 FPS, and the **MixFormerV2-S** surpasses FEAR-L by 2.7% AUC on LaSOT with a real-time CPU speed.

+
Footnote †: \({\dagger}\) Equal contribution. \(*\) Corresponding author (lmwang@nju.edu.cn).

## 1 Introduction

Visual object tracking has been a fundamental and long-standing task in computer vision, which aims to locate the object in a video sequence, given its initial bounding box. It has a wide range of practical applications, which often require for low computational latency. So it is important to _design a more efficient tracking architecture while maintaining high accuracy_.

Recently, the transformer-based one-stream trackers [7, 14, 55] attain excellent tracking accuracy than the previous Siamese-based ones [2, 10, 11], due to the unified modeling of feature extraction and target integration within a transformer block, which allows both components to benefit from the transformer development (e.g. ViT [18], self-supervised pre-training [24] or contrastive pre-training [43]). However for these trackers, _inference efficiency_, especially on CPU, is still the main obstacle to practical deployment. Taking the state-of-the-art tracker MixViT [15] as an instance, its pipeline contains i) **transformer backbone** on the token sequence from target template and search area, ii) **dense corner head** on the 2D search region for regression and iii) **extra complex score prediction module** for classification (i.e., estimating the box quality for reliable online samples selection). To achieve a high-efficiency tracker, there are still several issues on the design of MixViT. First, the dense convolutional corner head still exhibits a time-consuming design, as implied in Tab 1. This is because it densely estimates the probability distribution of the box corners through a total often convolutional layers on the high-resolution 2D feature maps. Second, to deal with online template updating, an extra complex score prediction module composed of precise RoI pooling layer, two attention blocks, and a three-layer MLP is required for improving online samples quality, which largely hinders its efficiency and simplicity of MixViT.

To avoid the dense corner head and complicated score prediction module, we propose a new _fully transformer tracking framework--MixFormerV2_ without any dense convolutional operation. Our MixFormerV2 yields a very simple and efficient architecture, which is composed of a transformer backbone on the mixed token sequence and two simple MLP heads on the learnable prediction tokens. Specifically, we introduce four special learnable prediction tokens and concate them with the original tokens from target template and search area. Like the CLS token in standard ViT, these prediction tokens are able to capture the complex relation between target template and search area, serving as a compact representation for subsequent regression and classification. Based on them, we can easily predict the target box and confidence score through simple MLP heads, which results in an efficient fully transformer tracker. Our MLP heads directly regress the _probability distribution_ of four box coordinates, which improves the regression accuracy without increasing overhead.

To further improve efficiency of MixFormerV2, we present a new model reduction paradigm based on distillation, including _dense-to-sparse distillation_ and _deep-to-shallow distillation_. The dense-to-sparse distillation aims to transfer knowledge from the dense-head based MixViT, to our fully transformer tracker. Thanks to the distribution-based regression design in our MLP head, we can easily adopt logits mimicking strategy for distilling MixViT trackers to our MixFormerV2. Based on the observation in Tab. 2, we also exploit the deep-to-shallow distillation to prune our MixFormerV2. We devise a new progressive depth pruning strategy by following a critical principle that constraining the initial distribution of student and teacher trackers to be as similar as possible, which can augment the capacity of transferring knowledge. Specifically, instructed by the frozen teacher model, some certain layers of a copied teacher model are progressively dropped and we use the pruned model as our student initialization. For CPU-realtime tracking, we further introduce an intermediate teacher model to bridge the gap between the large teacher and small student, and prune hidden dim of MLP based on the proposed distillation paradigm.

Based on the proposed model reduction paradigm, we instantiate two types of MixFormerV2 trackers, MixFormerV2-B and MixFormerV2-S. As shown in Fig. 1, MixFormerV2 achieves better trade-off between tracking accuracy and inference speed than previous trackers. Especially, MixFormerV2-B achieves an AUC of 70.6% on LaSOT with a high GPU speed of 165 FPS, and MixFormerV2-S outperforms FEAR-L by 2.7% AUC on LaSOT with a real-time CPU speed. Our contributions are two-fold: 1) We propose the first fully transformer tracking framework without any convolution operation, dubbed as **MixFormerV2**, yielding a more unified and efficient tracker. 2) We present a

\begin{table}
\begin{tabular}{c|c|c|c|c}
**Layer** & **Head** & **Score** & **GPU FPS** & **GFLOPs** \\ \hline
8 & Pyramid. Corner & ✓ & 90 & 27.2 \\
8 & Pyramid. Corner & - & 120\(\uparrow\) 33.3\(\%\) & 26.2 \\
8 & Token-based & - & 166\(\uparrow\) 84.4\(\%\) & 22.5 \\ \hline \end{tabular}
\end{table}
Table 1: Efficiency analysis on MixViT-B with different heads. ‘Pyram. Corner’ represents for the pyramidal corner head [15].

\begin{table}
\begin{tabular}{c|c|c|c|c}
**Layer** & **MLP Ratio** & **Image Size** & **CPU FPS** & **GPU FPS** \\ \hline \multirow{3}{*}{4} & 1 & 288 & 21 & 262 \\ \cline{2-5}  & & 224 & 30 & 280 \\ \cline{2-5}  & 4 & 288 & 12 & 255 \\ \cline{2-5}  & & 224 & 15 & 275 \\ \hline \hline \multirow{3}{*}{8} & 1 & 288 & 12 & 180 \\ \cline{2-5}  & & 224 & 16 & 190 \\ \cline{2-5}  & 4 & 288 & 7 & 150 \\ \cline{2-5}  & & 224 & 8 & 190 \\ \hline \hline \multirow{3}{*}{12} & 1 & 288 & 8 & 130 \\ \cline{2-5}  & 1 & 224 & 12 & 145 \\ \cline{1-1} \cline{2-5}  & 4 & 288 & 4 & 100 \\ \cline{1-1} \cline{2-5}  & & 224 & 6 & 140 \\ \hline \end{tabular}
\end{table}
Table 2: Efficiency analysis on MixViT-B with different backbone settings. The employed prediction head is plain corner head [14] for the analysis.

Figure 1: **Comparison with state-of-the-art trackers** in terms of AUC performance, model Flops and GPU Speed on LaSOT. The circle diameter is in proportion to model flops. MixFormerV2-B surpasses existing trackers by a large margin in terms of both accuracy and inference speed. MixFormerV2-S achieves extremely high tracking speed of over 300 FPS while obtaining competitive accuracy compared with other efficient trackers [4, 5].

new distillation-based model reduction paradigm to make MixFormerV2 more effective and efficient, which can achieve high-performance tracking on platforms with GPUs or CPUs.

## 2 Related Work

Efficient Visual Object Tracking.In recent decades, the visual object tracking task has witnessed rapid development due to the emergence of new benchmark datasets[20; 28; 41; 42; 48] and better trackers [2; 10; 12; 13; 32; 55; 14]. Researchers have tried to explore efficient and effective tracking architectures for practical applications, such as siamese-based trackers [2; 31; 32; 50], online trackers [3; 16] and transformer-based trackers [10; 37; 52]. Benefiting from transformer structure and attention mechanism, recent works [7; 14; 55] on visual tracking are gradually abandoning traditional three-stage model paradigm, i.e., feature extraction, information interaction and location head. They adopted a more unified one-stream model structure to jointly perform feature extraction and interaction, which turned out to be effective for modeling visual object tracking task. However, some modern tracking architectures are too heavy and computational expensive, making it hard to deploy in practical applications. LightTrack [53] employed NAS to search the a light Siamese network, but its speed was not extremely fast on powerful GPUs. FEAR [5], HCAT [9], E.T.Track [4] designed more efficient framework, however were not suitable for one-stream trackers. We are the first to design efficient one-stream tracker so as to achieve good accuracy and speed trade-off.

Knowledge Distillation.Knowledge Distillation [27] was proposed to learn more effective student models with teacher model's supervision. In the beginning, KD is applied in classification problem, where KL divergence is used for measuring the similarity of teacher's and student's predicted probability distribution. For regression problem like object detection, feature mimicking [1; 23; 33] is frequently employed. LD [57] operate logits distillation on bounding box location by converting Dirac delta distribution representation to probability distribution representation of bounding box, which well unifies logits distillation and location distillation. In this work, we exploit some customized strategies to make knowledge distillation more suitable for our tracking framework.

Vision Transformer Compression.There exist many general techniques for the purpose of speeding up model inference, including model quantization [22; 47], knowledge distillation [27; 36], pruning [25], and neural architecture search [19]. Recently many works also focus on compressing vision transformer models. For example, Dynamic ViT [44], Evo-ViT[51] tried to prune tokens in attention mechanism. Autoformer [8], NASViT [21], SlimmingViT [6] employed NAS technique to explore delicate ViT architecture. ViTKD [54] provided several ViT feature distillation guidelines but it focused on compressing the feature dimension instead of model depth. MiniViT [56] applied weights sharing and multiplexing to reduce model parameters. Since one-stream trackers highly rely on training-resource-consuming pre-training, we resort to directly prune the layers of our tracker.

## 3 Method

In this section, we first present the MixFormerV2, which is a more efficient and unified fully transformer tracking framework. Then we describe the proposed distillation-based model reduction, including dense-to-sparse distillation and deep-to-shallow distillation.

### Fully Transformer Tracking: MixFormerV2

The proposed MixFormerV2 is a fully transformer tracking framework without any convolutional operation and complex score prediction module. Its backbone is a plain transformer on the mixed token sequence of three types: target template token, search area token, and learnable prediction token. Then, simple MLP heads are placed on top for predicting probability distribution of the box coordinates and corresponding target quality score. Compared with other transformer-based trackers (e.g. TransT [10], STARK [52], MixFormer [14], OSTrack [55] and SimTrack [7]), our MixFormerV2 streamlines the tracking pipeline by effectively removing the customized convolutional classification and regression heads for the first time, which yields a more unified, efficient and flexible tracker. The overall architecture is depicted in Fig. 2. With inputting the template tokens, the search area tokens and learnable prediction tokens, MixFormerV2 predicts the target bounding boxes and quality score in an end-to-end manner.

Prediction-Token-Involved Mixed Attention.Compared to original slimming mixed attention [15] in MixViT, the key difference lies in the introduction of the special learnable prediction tokens, which are used to capture the correlation between the target template and search area. These prediction tokens can progressively compress the target information and used as a compact representations for subsequent regression and classification. Specifically, given the concatenated tokens of multiple templates, search and four learnable prediction tokens, we pass them into \(N\) layers of prediction-token-involved mixed attention modules (**P-MAM**). We use \(q_{t}\), \(k_{t}\) and \(v_{t}\) to represent template elements (i.e. _query_, _key_ and _value_) of attention, \(q_{s}\), \(k_{s}\) and \(v_{s}\) to represent search region, \(q_{e}\), \(k_{e}\) and \(v_{e}\) to represent learnable prediction tokens. The P-MAM can be defined as:

\[\begin{split} k_{tse}=\mathrm{Concat}(k_{t},k_{s},k_{e}),& v_{tse}=\mathrm{Concat}(v_{t},v_{s},v_{e}),\\ \mathrm{Atten}_{t}=\mathrm{Softmax}(\frac{q_{t}k_{t}^{T}}{\sqrt{ d}})v_{t},\mathrm{Atten}_{s}=\mathrm{Softmax}(\frac{q_{s}k_{tse}^{T}}{\sqrt{d}})v_{tse}, \mathrm{Atten}_{e}=\mathrm{Softmax}(\frac{q_{e}k_{tse}^{T}}{\sqrt{d}})v_{tse} \end{split}\] (1)

where \(d\) represents the dimension of each elements, \(\mathrm{Atten}_{t}\), \(\mathrm{Attent}_{s}\) and \(\mathrm{Atten}_{e}\) are the attention output of the template, search and the learnable prediction tokens respectively. Similar to the original MixFormer, we use the asymmetric mixed attention scheme for efficient online inference. Like the CLS tokens in standard ViT, the learnable prediction tokens automatically learn on the tracking dataset to compress both the template and search information.

Direct Prediction Based on Tokens.After the transformer backbone, we directly use the prediction tokens to regress the target location and estimate its reliable score. Specifically, we exploit the distribution-based regression based on the four special learnable prediction tokens. In this sense, we regress the probability distribution of the four bounding box coordinates rather than their absolute positions. Experimental results in Section 4.2 also validate the effectiveness of this design. As the prediction tokens can compress target-aware information via the prediction-token-involved mixed attention modules, we can simply predict the four box coordinates with a same MLP head as follows:

\[\hat{P}_{X}(x)=\mathrm{MLP}(\mathrm{token}_{X}),X\in\{\mathcal{T},\mathcal{L}, \mathcal{B},\mathcal{R}\}.\] (2)

In implementation, we share the MLP weights among four prediction tokens. For predicted target quality assessment, the Score Head is a simple MLP composed of two linear layers. Specifically, firstly we average these four prediction tokens to gather the target information, and then feed the token into the MLP-based Score Head to directly predict the confidence score \(s\) which is a real number. Formally, we can represent it as:

\[s=\mathrm{MLP}\left(\mathrm{mean}\left(\mathrm{token}_{X}\right)\right),X\in \{\mathcal{T},\mathcal{L},\mathcal{B},\mathcal{R}\}.\]

These token-based heads largely reduces the complexity for both the box estimation and quality score estimation, which leads to a more simple and unified tracking architecture.

Figure 2: **MixFormerV2 Framework.** MixFormerV2 is a fully transformer tracking framework, composed of a transformer backbone and two simple MLP heads on the learnable prediction tokens.

### Distillation-Based Model Reduction

To further improve the efficiency and effectiveness of MixFormerV2, we present a distillation-based model reduction paradigm as shown in Fig. 4, which first perform dense-to-sparse distillation for better token-based prediction and then deep-to-shallow distillation for the model pruning.

#### 3.2.1 Dense-to-Sparse Distillation

In MixFormerV2, we directly regress the target bounding box based on the prediction tokens to the distribution of four random variables \(\mathcal{T},\mathcal{L},\mathcal{B},\mathcal{R}\in\mathbb{R}\), which represents the box's top, left, bottom and right coordinate respectively. In detail, we predict the probability density function of each coordinate: \(X\sim\hat{P}_{X}(x)\), where \(X\in\{\mathcal{T},\mathcal{L},\mathcal{B},\mathcal{R}\}\). The final bounding box coordinates \(B\) can be derived from the expectation over the regressed probability distribution:

\[B_{X}=\mathbb{E}_{\hat{P}_{X}}[X]=\int_{\mathbb{R}}x\hat{P}_{X}(x)\mathrm{d}x.\] (3)

Since the original MixViT's dense convolutional corner heads predict two-dimensional probability maps, i.e. the joint distribution \(P_{\mathcal{T}\mathcal{L}}(x,y)\) and \(P_{\mathcal{B}\mathcal{R}}(x,y)\) for top-left and bottom-right corners, the one-dimensional version of box coordinates distribution can be deduced easily through marginal distribution:

\[\begin{split} P_{\mathcal{T}}(x)&=\int_{\mathbb{R}} P_{\mathcal{T}\mathcal{L}}(x,y)\mathrm{d}y,\quad\ P_{\mathcal{L}}(y)=\int_{ \mathbb{R}}P_{\mathcal{T}\mathcal{L}}(x,y)\mathrm{d}x\\ P_{\mathcal{B}}(x)&=\int_{\mathbb{R}}P_{\mathcal{ B}\mathcal{R}}(x,y)\mathrm{d}y,\quad P_{\mathcal{R}}(y)=\int_{\mathbb{R}}P_{ \mathcal{B}\mathcal{R}}(x,y)\mathrm{d}x.\end{split}\] (4)

Herein, this modeling approach can bridge the gap between the dense corner prediction and our sparse token-based prediction, i.e., and the regression outputs of original MixViT can be regarded as soft labels for dense-to-sparse distillation. Specifically, we use MixViT's outputs \(P_{X}\) as in Equation 4 for supervising the four coordinates estimation \(\hat{P}_{X}\) of MixFormerV2, applying KL-Divergence loss as follows:

\[L_{\text{loc}}=\sum_{X\in\{\mathcal{T},\mathcal{L},\mathcal{B},\mathcal{R}\}} L_{\text{KL}}(\hat{P}_{X},P_{X}).\] (5)

In this way, the localization knowledge is transferred from the dense corner head of MixViT to the sparse token-based head of MixFormerV2.

#### 3.2.2 Deep-to-Shallow Distillation

For further improving efficiency, we focus on pruning the transformer backbone. However, designing a new light-weight backbone is not suitable for fast one-stream tracking. A new backbone of one-stream trackers often highly relies on large-scale pre-training to achieve good performance, which requires for huge amounts of computation. Therefore, we resort to directly cut down some layersof MixFormerV2 backbone based on both the feature mimicking and logits distillation, as can be seen in Fig. 3:Stage2. Let \(F_{i}^{S},F_{j}^{T}\in\mathbb{R}^{h\times w\times c}\) denote the feature map from student and teacher, the subscript represents the index of layers. For logits distillation, we use KL-Divergence loss. For feature imitation, we apply \(L_{2}\) loss:

\[L_{\text{feat}}=\sum_{(i,j)\in\mathcal{M}}L_{2}(F_{i}^{S},F_{j}^{T}),\] (6)

where \(\mathcal{M}\) is the set of matched layer pairs need to be supervised. Specifically, we design a progressive model depth pruning strategy for distillation.

Progressive Model Depth Pruning.Progressive Model Depth Pruning aims to compress MixFormerV2 backbone through reducing the number of transformer layers. Since directly removing some layers could lead to inconsistency and discontinuity, we explore a progressive method for model depth pruning based on the feature and logits distillation. Specifically, instead of letting teacher to supervise a smaller student model from scratch, we make the original student model a complete copy of the teacher model. Then, we will progressively eliminate certain layers of student and make the remaining layers to mimic teacher's representation during training with supervision of teacher. This design allows the initial representation of student and teacher to keep as consistent as possible, providing a smooth transition scheme and reducing the difficulty of feature mimicking.

Formally, let \(x_{i}\) denote output of the \(i\)-th layer of MixFormerV2 backbone, the calculation of attention block can be represented as below (Layer-Normalization operation is omitted in equation):

\[\begin{split} x_{i}^{\prime}&=\text{ATTN}(x_{i-1}) +x_{i-1},\\ x_{i}&=\text{FFN}(x_{i}^{\prime})+x_{i}^{\prime}\\ &=\text{FFN}(\text{ATTN}(x_{i-1})+x_{i-1})+\text{ATTN}(x_{i-1}) +x_{i-1},\end{split}\] (7)

Let \(\mathcal{E}\) be the set of layers to be eliminated in our student network, we apply a decay rate \(\gamma\) on the weights of these layers:

\[x_{i}=\gamma(\text{FFN}(\text{ATTN}(x_{i-1})+x_{i-1})+\text{ATTN}(x_{i-1}))+x_ {i-1},i\in\mathcal{E}.\] (8)

During the first \(m\) epochs of student network training, \(\gamma\) will gradually decrease from \(1\) to \(0\) in the manner of cosine function:

\[\gamma(t)=\begin{cases}0.5\times\left(1+\cos\left(\frac{t}{m}\pi\right) \right),&t\leq m,\\ 0,&t>m.\end{cases}\] (9)

This means these layers in student network are gradually eliminated and finally turn into identity transformation, as depicted in Figure 4. The pruned student model can be obtained by simply removing layers in \(\mathcal{E}\) and keeping the remaining blocks.

Intermediate Teacher.For distillation of an extremely shallow model (4-layers MixFormerV2), we introduce an intermediate teacher (8-layers MixFormerV2) for bridging the deep teacher (12-layers MixFormerV2) and the shallow one. Typically, the knowledge of teacher may be too complex for a small student model to learn. So we introduce an intermediate role serving as teaching assistant to relieve the difficulty of the extreme knowledge distillation. In this sense, we divide the problem of knowledge distillation between teacher and small student into several distillation sub-problems.

MLP Reduction.As shown in Table 2, one key factor affecting the inference latency of tracker on CPU device is the hidden feature dim of MLP in Transformer block. In other words, it becomes the bottleneck that limits the real-time speed on CPU device. In order to leverage this issue, we further prune the hidden dim of MLP based on the proposed distillation paradigm, i.e., feature mimicking and logits distillation. Specifically, let the shape of linear weights in the original model is \(w\in\mathbb{R}^{d_{1}\times d_{2}}\), and the corresponding shape in the pruning student model is \(w^{\prime}\in\mathbb{R}^{d_{1}^{\prime}\times d_{2}^{\prime}}\), in which \(d_{1}^{\prime}\leq d_{1},d_{2}^{\prime}\leq d_{2}\), we will initialize weights for student model as: \(w^{\prime}=w[\colon d_{1}^{\prime},\colon d_{2}^{\prime}]\). Then we apply distillation supervision for training, letting the pruned MLP to simulate original heavy MLP.

### Training of MixFormerV2

The overall training pipeline is demonstrated in Fig. 3, performing dense-to-sparse distillation and then deep-to-shallow distillation to yield our final efficient MixFormerV2 tracker. Then, we train the MLP based score head for 50 epochs. Particularly, for CPU real-time tracking, we employ the intermediate teacher to generate a shallower model (4-layer MixFormerV2) based on the proposed distillation. Besides, we also use the designed MLP reduction strategy for further pruning the CPU real-time tracker. The total loss of distillation training with student \(S\) and teacher \(T\) is calculated as:

\[L=\lambda_{1}L_{1}(B^{S},B^{gt})+\lambda_{2}L_{ciou}(B^{S},B^{gt})+\lambda_{3} L_{dist}(S,T),\] (10)

where the first two terms are exactly the same as original MixFormer's location loss supervised by ground truth bounding box labels, and the rest term is for aforementioned distillation.

## 4 Experiments

### Implemented Details

Training and Inference.Our trackers are implemented using Python 3.6 and PyTorch 1.7. The distillation training is conducted on 8 NVidia Quadro RTX 8000 GPUs. The inference process runs on one NVidia Quadro RTX 8000 GPU and Intel(R) Xeon(R) Gold 6230R CPU @ 2.10GHz. The training datasets includes TrackingNet [42], LaSOT [20], GOT-10k [28] and COCO [35] training splits., which are the same as MixFormer [14]. Each distillation training stage takes 500 epochs, where the first \(m=40\) epochs are for progressively eliminating layers. We train the score prediction MLP for additional 50 epochs. The batch size is 256, each GPU holding 32 samples. We use AdamW optimizer with weight decay of \(10^{-4}\). The initial learning rate is \(10^{-4}\) and will be decreased to \(10^{-5}\) after \(400\) epochs. We use horizontal flip and brightness jittering for data augmentation. We instantiate two types of MixFormerV2, including MixFormerV2-B of 8 P-MAM layers for high-speed tracking on GPU platform and MixFormerV2-S of 4 P-MAM layers with MLP ratio of 1.0 for real-time tracking on CPU platform. Their numbers of parameters are 58.8M and 16.2M respectively. The resolutions of search and template images for MixFormerV2-B are \(288\times 288\) and \(128\times 128\) respectively. While for MixFormerV2-S, the resolutions of search and template images are \(224\times 224\) and \(112\times 112\) for real-time tracking on CPU platform. The inference pipeline is the same as MixFormer [14]. We use the first template together with the current search region as input of MixFormerV2. The dynamic templates are updated when the update interval of 200 is reached by default, where the template with the highest score is selected as an online sample.

Distillation-Based Reduction.For dense-to-sparse distillation, we use MixViT-L as teacher for training MixFormerV2-B by default. We also try to use MixViT-B as the teacher in Tab 5. Particularly, we employ a customized MixViT-B of plain corner head and with search input size of \(224\times 224\) as the teacher for MixFormer-S. For deep-to-shallow distillation, we use the progressive model depth pruning strategy to produce the 8-layer MixFormerV2-B from a 12-layer one. For MixFormerV2-S, we additionally employs intermediate teacher and MLP reduction strategies, and the process is '12-layers MixFormerV2 to 8-layers MixFormerV2, then 8-layers MixFormerV2 to 4-layers MixFormerV2, finally 4-layers MLP-ratio-4.0 MixFormerV2 to 4-layers MLP-ratio-1.0 MixFormerV2-S'.

### Exploration Studies

To verify the effectiveness of our proposed framework and training paradigm, we analyze different components of MixFormerV2 and perform detailed exploration studies on LaSOT [20] dataset.

#### 4.2.1 Analysis on MixFormerV2 Framework

Token-based Distribution Regression.The design of distribution-based regression with special learnable prediction tokens is the core of our MixFormerV2. We conduct experiments on different regression methods in Tab. 2(a). All models employ ViT-B as backbone and are deployed without distillation and online score prediction. Although the pyramidal corner head obtains the best performance, the running speed is largely decreased compared with our token-based regression head in MixFormerV2. MixFormerV2 with four prediction tokens achieves good trade-off between performance and inference latency. Besides, compared to the direct box prediction with one token on thefirst line of Tab. 2(a), which estimates the absolute target position instead of the probability distribution of four coordinates, the proposed distribution-based regression obtains better accuracy. Besides, this design allows to perform dense-to-sparse distillation so as to further boost performance.

Token-based Quality Score Prediction.The design of the prediction tokens also allows to perform more efficient quality score prediction via a simple MLP head. As shown in Tab. 2(b), the token-based score prediction component improves the baseline MixFormerV2-B by 1.7% with increasing quite little inference latency. Compared to ours, the score prediction module in MixViT-B further decreases the running speed by 13.0%, which is inefficient. Besides, the SPM in MixViT requires precise RoI pooling, which hinders the migration to various platforms.

#### 4.2.2 Analysis on Dense-to-Sparse Distillation

We verify the effectiveness of dense-to-sparse distillation in Tab. 2(c). When use MixViT-B without its SPM (69.0% AUC) as the teacher model, the MixFormerV2 of 12 P-MAM layers achieves an AUC score of 68.9%, increasing the baseline by 1.4%. This further demonstrate the significance of the design of four special prediction tokens, which allows to perform dense-to-sparse distillation. The setting of using MixViT-L (71.5% AUC) as the teacher model increases the baseline by an AUC score of 2.2%, which implies the good distillation capacity of the large model.

\begin{table}

\end{table}
Table 3: **Ablation studies on LaSOT.The default choice for our model is colored in gray**.

#### 4.2.3 Analysis on Deep-to-Shallow Distillation

In the following analysis on deep-to-shallow distillation, we use the MixViT-B of 12 layers with plain corner head as the teacher, and MixViT of 4 layers with the same corner head as the student. The models are deployed without score prediction module.

Feature Mimicking & Logits Distillation.To give detailed analysis on different distillation methods for tracking, we conduct experiments in Tab. 2(d). The models are all initialized with the first 4-layers MAE pre-trained ViT-B weights. It can be seen that logits distillation can increase the baseline by 1.7% AUC, and adding feature mimicking further improves by 0.4% AUC, which indicates the effectiveness of both feature mimicking and logits distillation for tracking.

Progressive Model Depth Pruning.We study the effectiveness of the progressive model depth pruning (PMDP) for the student initialization in Tab. 2(b). It can be observed that the PMDP improves the traditional initialization method of using MAE pre-trained first 4-layers ViT-B by 1.9%. This demonstrates that it is critical for constraining the initial distribution of student and teacher trackers to be as similar as possible, which can make the feature mimicking easier. Surprisingly, we find that even the initial weights of the four layers are not continuous, i.e., using the skipped layers (the 3,6,9,12-th) of the teacher for initialization, the performance is better than the baseline (62.9% vs. 64.4%), which further verifies the importance of representation similarity between the two ones.

Determination of Eliminating Epochs.We conduct experiments as shown in the Table 2(g) to choose the best number of epochs \(m\) in the progressive eliminating period. We find that when the epoch \(m\) greater than 40, the choice of \(m\) seems hardly affect the performance. Accordingly we determine the epoch to be 40.

Intermediate Teacher.Intermediate teacher is introduced to promote the transferring capacity from a deep model to a shallow one. We conduct experiment as in Table 2(f). We can observe that the intermediate teacher can bring a gain of 0.7% AUC score which can verify that.

#### 4.2.4 Model Pruning Route

We present the model pruning route from the teacher model to MixFormeV2-B\({}^{*}\) and MixFormervV2-S in Tab. 2(h) and Tab. 2(i) respectively. The models on the first line are corresponding teacher models. We can see that, through the dense-to-sparse distillation, our token-based MixFormervV2-B obtains comparable accuracy with the dense-corner-based MixViT-B with higher running speed. Through the progressive model depth pruning based on the feature and logits distillation, MixFormerv2-B with 8 layers only decreases little accuracy compared to the 12-layers one.

### Comparison with the Previous Methods

Comparison with State-of-the-art Trackers.We evaluate the performance of our proposed trackers on 6 benchmark datasets: including the large-scale LaSOT [20], LaSOT\({}_{ext}\)[20], TrackingNet [42], UAV123 [41], TNL2K [48] and VOT2022 [30]. LaSOT is a large-scale dataset with 1400 long videos in total and its test set contains 280 sequences. TrackingNet provides over 30K videos with more than 14 million dense bounding box annotations. UAV123 is a large dataset containing 123 aerial videos which is captured from low-altitude UAVs. VOT2022 benchmark has 60 sequences, which measures the Expected Average Overlap (EAO), Accuracy (A) and Robustness (R) metrics. Among them, LaSOT\({}_{ext}\) and TNL2K are two relatively recent benchmarks. LaSOT\({}_{ext}\) is a released extension of LaSOT, which consists of 150 extra videos from 15 object classes. TNL2K consists of 2000 sequences, with natural language description for each. We evaluate our MixFormerv2 on the test set with 700 videos. The results are presented in Tab. 4 and Tab. 5. More results on other datasets will be present in supplementary materials. Only the trackers of similar complexity are

\begin{table}
\begin{tabular}{l|c c c c c c c c c|c c}  & KCF & SiamFC & ATOM & D3Sv2 & DiMP & ToMP & TransT & SBT & SwinTrack & **Ours-S** & **Ours-B** \\  & [26] & [2] & [16] & [8] & [3] & [39] & [10] & [49] & [34] & & \\ \hline
**EAO** & 0.239 & 0.255 & 0.386 & 0.356 & 0.430 & 0.511 & 0.512 & 0.522 & 0.524 & 0.431 & **0.556** \\
**Accuracy** & 0.542 & 0.562 & 0.668 & 0.521 & 0.689 & 0.752 & 0.781 & 0.791 & 0.788 & 0.715 & **0.795** \\
**Robustness** & 0.532 & 0.543 & 0.716 & 0.811 & 0.760 & 0.818 & 0.800 & 0.813 & 0.803 & 0.757 & **0.851** \\ \hline \hline \end{tabular}
\end{table}
Table 4: State-of-the-art comparison on VOT2022 [30]. The best results are shown in **bold** font.

included, i.e., the trackers with large-scale backbone or large input resolution are excluded. Our **MixFormerV2-B** achieves state-of-the-art performance among these trackers with a very fast speed, especially compared to transformer-based one-stream tracker. For example, MixFormerV2-B without post-processing strategies surpasses OSTrack by 1.5% AUC on LaSOT and 2.4% AUC on TNL2k, running with _quite faster_ speed (165 FPS vs. 105 FPS). Even the MixFormerV2-B with MixViT-B as the teacher model obtains better performance than existing SOTA trackers, such as MixFormer, OSTrack, ToMP101 and SimTrack, with much faster running speed on GPU.

Comparison with Efficient Trackers.For real-time running requirements on limited computing resources such as CPU, we explore a lightweight model, i.e. **MixFormerV2-S**, which still reaches strong performance. And it is worth noting that this is the first time that transformer-based one-stream tracker is able to run on CPU device with a real-time speed. As demonstrated in Figure 6, MixFormerV2-S surpasses all other architectures of CPU-real-time trackers by a large margin. We take a comparison with other prevailing efficient trackers on multiple datasets, including LaSOT, TrackingNet, UAV123 and TNL2k, in Tab 6. We can see that our MixFormerV2-S outperforms FEAR-L by a an AUC score of 2.7% and STARK-Lightning by an AUC score of 2.0% on LaSOT.

## 5 Conclusion

In this paper, we have proposed a fully transformer tracking framework MixFormerV2, composed of standard ViT backbones on the mixed token sequence and simple MLP heads for box regression and quality score estimation. Our MixFormerV2 streamlines the tracking pipeline by removing the dense convolutional head and the complex score prediction modules. We also present a distillation based model reduction paradigm for MixFormerV2 to further improve its efficiency. Our MixFormerV2 obtains a good trade-off between tracking accuracy and speed on both GPU and CPU platforms. We hope our MixFormerV2 can facilitate the development of efficient transformer trackers in the future.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c|c c|c c|c c} \multirow{2}{*}{**Method**} & \multicolumn{2}{c|}{**LaSOT**} & \multicolumn{2}{c|}{**LaSOT\({}_{ext}\)**} & \multicolumn{2}{c|}{**TNL2k**} & \multicolumn{2}{c|}{**TrackingNet**} & \multicolumn{2}{c|}{**LAV123**} & \multicolumn{2}{c}{**Speed**} \\  & AUC & \(P_{Norm}\) & p & AUC & P & AUC & P & AUC & P\({}_{Norm}\) & p & AUC & P & GPU & CPU \\ \hline
**MixFormerV2-S** & **60.6** & **69.9** & 60.4 & **43.6** & **46.2** & **48.3** & **43** & 75.8 & 81.1 & 70.4 & **65.8** & **86.8** & **325** & 30 \\ FEAR-L [5] & 57.9 & 68.6 & **60.9** & - & - & - & - & - & - & - & - & - & - & - \\ FEAR-XS [5] & 53.5 & 64.1 & 54.5 & - & - & - & - & - & - & - & - & - & 80 & 26 \\ HCAT[9] & 59.0 & 68.3 & 60.5 & - & - & - & **76.6** & **82.6** & **72.9** & 63.6 & - & 195 & **45** \\ E.T.Track [4] & 59.1 & - & - & - & - & - & - & 74.5 & 80.3 & 70.6 & 62.3 & - & 150 & 42 \\ LightTrack-LargeA [53] & 55.5 & - & 56.1 & - & - & - & - & 73.6 & 78.8 & 70.0 & - & - & - & - \\ LightTrack-Mobile [53] & 53.8 & - & 53.7 & - & - & - & - & 72.5 & 77.9 & 69.5 & - & - & 120 & 36 \\ STARK-Lightning & 58.6 & 69.0 & 57.9 & - & - & - & - & - & - & - & - & - & 200 & 42 \\ DMP [3] & 56.9 & 65.0 & 56.7 & - & - & - & - & 74.0 & 80.1 & 68.7 & 65.4 & - & 77 & 15 \\ SiamFC++ [50] & 54.4 & 62.3 & 54.7 & - & - & - & - & 75.4 & 80.0 & 70.5 & - & - & 90 & 20 \\ \hline \end{tabular}
\end{table}
Table 6: Comparison with CPU-realtime trackers on TrackingNet [42], LaSOT [20], LaSOT\({}_{ext}\)[20], UAV123 [41] and TNL2k [48]. The best results are shown in **bold** forns.

\begin{table}
\begin{tabular}{l|c c c|c c|c c|c c|c c|c c|c} \multirow{2}{*}{**Method**} & \multicolumn{2}{c|}{**LaSOT**} & \multicolumn{2}{c|}{**LaSOT\({}_{ext}\)**} & \multicolumn{2}{c|}{**TNL2k**} & \multicolumn{2}{c|}{**TrackingNet**} & \multicolumn{2}{c|}{**UAV123**} & \multicolumn{2}{c}{**Speed**} \\  & AUC & \(P_{Norm}\) & p & AUC & P & AUC & P & AUC & P\({}_{Norm}\) & P & AUC & P & GPU \\ \hline
**MixFormerV2-B** & **70.6** & **80.8** & **76.2** & **50.6** & **56.9** & **57.4** & **58.4** & **83.4** & **88.1** & 81.6 & 69.9 & **92.1** & **165** \\
**MixFormerV2-B*** & 69.5 & 79.1 & 75.0 & - & - & - & 56.6 & 57.1 & 82.9 & 87.6 & 81.0 & **70.5** & 91.9 & **165** \\ MixFormer [14] & 69.2 & 78.7 & 74.7 & - & - & - & - & 83.1 & 88.1 & 81.6 & 70.4 & 91.8 & 25 \\ CTTrack-B [45] & 67.8 & 77.8 & 74.0 & - & - & - & - & 82.5 & 87.1 & 80.3 & 68.8 & 89.5 & 40 \\ OSTrack-256 [55] & 69.1 & 78.7 & 75.2 & 47.4 & 53.3 & 54.3 & - & 83.1 & 87.8 & **82.0** & 68.3 & - & 105 \\ SimTrack-B [7] & 69.3 & 78.5 & - & - & - & - & 54.8 & 53.8 & 82.3 & 86.5 & - & 69.8 & 89.6 & 40 \\ CSWuIT [16] & 66.2 & 75.2 & 70.9 & - & - & - & - & 81.9 & 86.7 & 79.5 & 70.5 & 90.3 & 12 \\ SBT-Base [49] & 65.9 & - & 70.0 & - & - & - & - & - & - & - & - & - & 37 \\ SwinTrack-T [34] & 67.2 & - & 70.8 & 47.6 & 53.9 & 53.0 & 53.2 & 81.1 & - & 78.4 & - & - & 98 \\ TMP101 [39] & 68.5 & 79.2 & 68.5 & - & - & - & - & 81.5 & 86.4 & 78.9 & 66.9 & - & 20 \\ STARK-ST50 [52] & 66.4 & - & - & - & - & - & - & 81.3 & 86.1 & - & - & - & 42 \\ KeepTrack [40] & 67.1 & 77.2 & 70.2 & 48.2 & - & - & - & - & - & - & 69.7 & - & 19 \\ TransT [10] & 64.9 & 73.3 & 69.0 & - & - & 50.7 & 51.7 & 81.4 & 86.7 & 80.3 & 69.1 & - & 50 \\ PHOMP [17] & 59.8 & 68.8 & 60.8 & - & - & - & - & 75.8 & 81.6 & 70.4 & 68.0 & - & 47 \\ ATOM [16] & 51.5 & 57.6 & 50.5 & - & - & - & - & 70.3 & 77.1 & 64.8 & 64.3 & - & 83 \\ \hline \end{tabular}
\end{table}
Table 5: State-of-the-art comparison on TrackingNet [42], LaSOT [20], LaSOT\({}_{ext}\)[20], UAV123 [41] and TNL2K [48]. The best two results are shown in **bold** and underline fonts. **’*’ denotes tracker with MixViT-B as the teacher during the dense-to-sparse distillation process. The default teacher is MixViT-L. Only trackers of similar complexity are included.

[MISSING_PAGE_FAIL:11]

* [24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR_, 2022.
* [25] Yihu He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. In _Proceedings of the IEEE international conference on computer vision_, pages 1389-1397, 2017.
* [26] Joao F. Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed tracking with kernelized correlation filters. _IEEE Trans. Pattern Anal. Mach. Intell._, 37(3):583-596, 2015.
* [27] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network (2015). _arXiv preprint arXiv:1503.02531_, 2, 2015.
* [28] Lianghua Huang, Xin Zhao, and Kaiqi Huang. Got-10k: A large high-diversity benchmark for generic object tracking in the wild. _IEEE Trans. Pattern Anal. Mach. Intell._, 43(5):1562-1577, 2021.
* [29] Matej Kristan, Ales Leonardis, and et. al. The eighth visual object tracking VOT2020 challenge results. In Adrien Bartoli and Andrea Fusiello, editors, _Proceedings of the European Conference on Computer Vision, ECCV Workshops_, 2020.
* [30] Matej Kristan, Ales Leonardis, Jiri Matas, Michael Felsberg, Roman Pflugfelder, Joni-Kristian Kamarainen, Hyung Jin Chang, Martin Danelljan, Luka Cehovin Zajc, Alan Lukezic, et al. The tenth visual object tracking vot2022 challenge results. In _ECCV 2022 Workshops_, pages 431-460, 2023.
* [31] Bo Li, Wei Wu, Qiang Wang, Fangjiang Zhang, Junliang Xing, and Junjie Yan. Siamrpn++: Evolution of siamese visual tracking with very deep networks. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR_, 2019.
* [32] Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual tracking with siamese region proposal network. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR_, 2018.
* [33] Quanquan Li, Shengying Jin, and Junjie Yan. Mimicking very efficient network for object detection. In _Proceedings of the ieee conference on computer vision and pattern recognition_, pages 6356-6364, 2017.
* [34] Liting Lin, Heng Fan, Yong Xu, and Haibin Ling. Swintrack: A simple and strong baseline for transformer tracking. _Neural Information Processing Systems, NIPS_, 2022.
* [35] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In _Proceedings of the European Conference on Computer Vision, ECCV_, 2014.
* [36] Benlin Liu, Yongming Rao, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Metadistiller: Network self-boosting via meta-learned top-down distillation. In _European Conference on Computer Vision_, pages 694-709. Springer, 2020.
* [37] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF International Conference on Computer Vision, ICCV_, 2021.
* A discriminative single shot segmentation tracker. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR_, 2020.
* [39] Christoph Mayer, Martin Danelljan, Goutam Bhat, Matthieu Paul, Danda Pani Paudel, Fisher Yu, and Luc Van Gool. Transforming model prediction for tracking. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR_, 2022.
* [40] Christoph Mayer, Martin Danelljan, Danda Pani Paudel, and Luc Van Gool. Learning target candidate association to keep track of what not to track. In _Proceedings of the IEEE/CVF International Conference on Computer Vision, ICCV_, 2021.
* [41] Matthias Mueller, Neil Smith, and Bernard Ghanem. A benchmark and simulator for UAV tracking. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, _Proceedings of the European Conference on Computer Vision, ECCV_, 2016.
* [42] Matthias Muller, Adel Bibi, Silvio Giancola, Salman Al-Subaihi, and Bernard Ghanem. Trackingnet: A large-scale dataset and benchmark for object tracking in the wild. In _Proceedings of the European Conference on Computer Vision, ECCV_, 2018.
* [43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [44] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. _Advances in neural information processing systems_, 34:13937-13949, 2021.
* [45] Zikai Song, Run Luo, Junqing Yu, Yi-Ping Phoebe Chen, and Wei Yang. Compact transformer tracker with correlative masked modeling. In _Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)_, February 2023.
* [46] Zikai Song, Junqing Yu, Yi-Ping Phoebe Chen, and Wei Yang. Transformer tracking with cyclic shifting window attention. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR_, 2022.
* [47] Kuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quantization with mixed precision. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8612-8620, 2019.
* [48] Xiao Wang, Xiujun Shu, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong Tian, and Feng Wu. Towards more flexible and accurate object tracking with natural language: Algorithms and benchmark. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13763-13773,2021.
* [49] Fei Xie, Chunyu Wang, Guangting Wang, Yue Cao, Wankou Yang, and Wenjun Zeng. Correlation-aware deep tracking. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR_, 2022.
* [50] Yinda Xu, Zeyu Wang, Zuoxin Li, Ye Yuan, and Gang Yu. Siamfcf++: Towards robust and accurate visual tracking with target estimation guidelines. In _Proceedings of the AAAI Conference on Artificial Intelligence, AAAI_, 2020.
* [51] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai Sheng, Ke Li, Weiming Dong, Lioing Zhang, Changsheng Xu, and Xing Sun. Evo-vit: Slow-fast token evolution for dynamic vision transformer. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 2964-2972, 2022.
* [52] Bin Yan, Houwen Peng, Jianlong Fu, Dong Wang, and Huchuan Lu. Learning spatio-temporal transformer for visual tracking. In _Proceedings of the IEEE/CVF International Conference on Computer Vision, ICCV_, 2021.
* [53] Bin Yan, Houwen Peng, Kan Wu, Dong Wang, Jianlong Fu, and Huchuan Lu. Lighttrack: Finding lightweight neural networks for object tracking via one-shot architecture search. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15180-15189, 2021.
* [54] Zhendong Yang, Zhe Li, Aling Zeng, Zexian Li, Chun Yuan, and Yu Li. Vitkd: Practical guidelines for vit feature knowledge distillation. _arXiv preprint arXiv:2209.02432_, 2022.
* [55] Botao Ye, Hong Chang, Bingpeng Ma, and Shiguang Shan. Joint feature learning and relation modeling for tracking: A one-stream framework. _Proceedings of the European Conference on Computer Vision, ECCV_, 2022.
* [56] Jinnian Zhang, Houwen Peng, Kan Wu, Mengchen Liu, Bin Xiao, Jianlong Fu, and Lu Yuan. Minivit: Compressing vision transformers with weight multiplexing. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 12145-12154, 2022.
* [57] Zhaohui Zheng, Rongguang Ye, Qibin Hou, Dongwei Ren, Ping Wang, Wangmeng Zuo, and Ming-Ming Cheng. Localization distillation for object detection. _arXiv preprint arXiv:2204.05957_, 2022.

## Appendix

### Broader Impact

In this paper, we introduce MixFormerV2, a fully transformer tracking approach for efficiently and effectively estimating the state of an arbitrary target in a video. Generic object tracking is one of the fundamental computer vision problems with numerous applications. For example, object tracking (and hence MixFormerV2) could be applied to human-machine interaction, visual surveillance and unmanned vehicles. Our research could be used to improve the tracking performance while maintaining a high running speed. Of particular concern is the use of the tracker by those wishing to position and surveil others illegally. Besides, if the tracker is used in unmanned vehicles, it may be a challenge when facing the complex real-world scenarios. To mitigate the risks associated with using MixFormerV2, we encourage researchers to understand the impacts of using the trackers in particular real-world scenarios.

### Limitations

The main limitation lies in the training overhead of MixFormerV2-S, which performs _multiple_ model pruning based on the dense-to-sparse distillation and deep-to-shallow distillation. In detail, we first perform distillation from MixViT with 12 layers and plain corner head to MixFormerV2 of 12 layers. The 12-layers MixFormerV2 is pruned to 8-layers and then to 4-layers MixFormerV2 based on the deep-to-shallow distillation. Finally, the MLP-ratio-4.0 4-layers MixFormerV2 is pruned to the MLP-ratio-4.0 4-layers MixFormerV2-S for real-time tracking on CPU. For each step, it requires training for 500 epochs which is time-consuming.

### Details of Training Time

The models are trained on 8 Nvidia RTX8000 GPUs. The dense-to-sparse stage takes about 43 hours. The deep-to-shallow stage1 (12-to-8 layers) takes about 42 hours, and stage2 (8-to-4 layers) takes about 35 hours.

### More Results on VOT2020 and GOT10k

VOT2020.We evaluate our tracker on VOT2020 [29] benchmark, which consists of 60 videos with several challenges including fast motion, occlusion, etc. The results is reported in Table 7, with metrics Expected Average Overlap(EAO) considering both Accuracy(A) and Robustness. Our MixFormerV2-B obtains an EAO score of 0.322 surpassing CSWinTT by 1.8%. Besides, our MixFormerV2-S achieves an EAO score of 0.258, which is higher than the efficient tracker LightTrack, with a real-time running speed on CPU.

GOT10k.GOT10k [28] is a large-scale dataset with over 10000 video segments and has 180 segments for the test set. Apart from generic classes of moving objects and motion patterns, the object classes in the train and test set are zero-overlapped. We evaluate MixFormerV2 trained with the four datasets of LaSOT, TrackingNet, COCO and GOT10k-train on GOT10k-test. We compare it with MixFormer and TransT with the same training datasets for fair comparison. MixFormerV2-B improves MixFormer and TransT by 0.7% and 1.6% on AO respectively with a high running speed of 165 FPS.

### More Ablation Studies

Design of Prediction Tokens.We practice three different designs of prediction tokens for the target localization in Tab. 7(a). All the three methods use the formulation of estimating the probability

\begin{table}
\begin{tabular}{l|c c c c c c c c|c c c}  & KCF & SiamFC & ATOM & LightTrack & DMP & STARK & TransT & CSWinTT & MixFormer & **Ours-S** & **Ours-B** \\  & [26] & [2] & [16] & [53] & [3] & [52] & [10] & [46] & [14] & \\ \hline \hline
**VOT20\({}_{\text{EAO}}\)** & 0.154 & 0.179 & 0.271 & 0.242 & 0.274 & 0.280 & - & 0.304 & - & 0.258 & **0.322** \\
**GOT10k\({}_{AO}\)** & 0.203 & 0.348 & 0.556 & - & 0.611 & 0.688 & 0.723\({}^{*}\) & 0.694 & 0.726\({}^{*}\) & 0.621\({}^{*}\) & **0.739\({}^{*}\)** \\ \hline \end{tabular}
\end{table}
Table 7: State-of-the-art comparison on VOT2020 [29] and GOT10k [28]. \(*\) denotes training with four datasets including LaSOT [20], TrackingNet [42], GOT10k [28] and COCO [35]. The best results are shown in **bold** font.

distribution of the four coordinates of the bounding box. The model on the first line denotes using one prediction token and then predicting coordinates distribution with four independent MLP heads. It can be observed that adopting separate prediction tokens for the four coordinates and a same MLP head retains the best accuracy.

More Exploration of PMDPTea-skip4 is a special initialization method, which chooses the skiped four layers (layer-3/6/9/12) of the teacher (MixViT-B) for initialization. In other words, Tea-skip4 is an extreme case of ours PMDP when the eliminating epoch \(m\) equal to \(0\). So it is reasonable that Tea-skip4 performs better than the baseline Tea-fit4, which employs the first four layers of the teacher (MixViT-B) to initialize the student backbone. In Table 7(b), we further evaluate the performance on more benchmarks. It can be seen that ours PMDP surpasses Tea-skip4 by 1.0% on LaSOT_ext, which demonstrate its effectiveness.

Computation Loads of Different Localization HeadWe showcase the FLOPs of different heads as follows. Formally, we denote \(C_{in}\) as input feature dimension, \(C_{out}\) as output feature dimension, \(H_{in},W_{in}\) as input feature map shape of convolution layer, \(H_{out},W_{out}\) as output feature map shape, and \(K\) as the convolution kernel size. The computational complexity of one linear layer is \(O(C_{in}C_{out})\), and that of one convolutional layer is \(O(C_{in}C_{out}H_{out}W_{out}K^{2})\).

In our situation, for T4, the Localization Head contains four MLP to predict four coordinates. Each MLP contains two linear layer, whose input and output dimensions are all 768. The loads can be calculated as:

\[Load_{T4}=4\times(768\times 768+768\times 72)=2580480\sim 2.5M\]

For Py-Corner, totally 24 convolution layers are used. The loads can be calculated as:

\[Load_{Py-Corner}=2*(768*384*18*18*3*3+\] \[384*192*18*18*3*3+\] \[384*192*18*18*3*3+\] \[192*96*36*36*3*3+\] \[384*96*18*18*3*3+\] \[96*48*72*72*3*3+\] \[48*1*72*72*3*3+\] \[192*96*18*18*3*3+\] \[96*48*18*18*3*3+\] \[48*1*18*18*3*3+\] \[48*1*18*18*3*3+\] \[96*48*36*36*3*3+\] \[48*1*36*36*3*3+\] \[= 3902587776\sim 3.9B\]

For simplicity, we do not include some operations such as bias terms and Layer/Batch-Normalization, which does not affect the overall calculation load level. Besides, the Pyramid Corner Head utilize additional ten interpolation operations. Obviously the calculation load of Py-Corner is still hundreds of times of T4.

\begin{table}
\begin{tabular}{c c c} \hline \hline token num. & MLP num. & AUC \\ \hline
1 & 4 & 67.1\% \\
4 & 4 & 67.3 \\
4 & 1 & 67.5\% \\ \hline \hline \end{tabular}
\end{table}
Table 8: **More ablation studies**. The default choice for our model is colored in gray.

### S.4 Visualization Results

Visualization of Attention MapTo explore how the introduced learnable prediction tokens work within the P-MAM, we visualize the attention maps of prediction-token-to-search and prediction-token-to-template in Fig. 6 and Fig. 6, where the prediction tokens are served as _query_ and the others as _keyval_ of the attention operation. From the visualization results, we can arrive that the four prediction tokens are sensitive to corresponding part of the targets and thus yielding a compact object bounding box. We suspect that the performance gap between the dense corner head based MixViT-B and our fully transformer MixFormerV2-B without distillation lies in the lack of holistic target modeling capability. Besides, the prediction tokens tend to extract partial target information in both the template and the search so as to relate the two ones.

Visualization of Predicted Probability DistributionWe show two good cases and bad cases in Figure 7. In Figure 6(a) MixFormerV2 deals with occlusion well and locate the bottom edge correctly. As show in Figure 6(b), the probability distribution of box representation can effectively alleviate issue of ambiguous boundaries. There still exist problems like strong occlusion and similar objects which will lead distribution shift, as demonstrated in Figure 6(c) and 6(d).

Figure 5: Visualization of prediction-token-to-search attention maps, where the prediction tokens are served as _query_ of attention operation. Figure 6: Visualization of prediction-token-to-template attention maps, where the prediction tokens are served as _query_ of attention operation.