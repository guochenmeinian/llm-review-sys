# Low Degree Hardness for Broadcasting on Trees

 Han Huang

Department of Mathematics

University of Missouri

Columbia, MO 65203

hhuang@missouri.edu &Elchanan Mossel

Department of Mathematics

MIT

Cambridge, MA 02139

elmos@mit.edu

###### Abstract

We study the low-degree hardness of broadcasting on trees. Broadcasting on trees has been extensively studied in statistical physics, in computational biology in relation to phylogenetic reconstruction and in statistics and computer science in the context of block model inference, and as a simple data model for algorithms that may require depth for inference.

The inference of the root can be carried by celebrated Belief Propagation (BP) algorithm which achieves Bayes-optimal performance. Recent works indicated that this algorithm in fact requires high level of complexity. Moitra, Mossel and Sandon constructed a chain for which estimating the root better than random (for a typical input) is \(NC1\) complete. Kohler and Mossel constructed chains such that for trees with \(N\) leaves, recovering the root better than random requires a polynomial of degree \(N^{\Omega(1)}\). Both works above asked if such complexity bounds hold in general below the celebrated _Kesten-Stigum_ bound.

In this work, we prove that this is indeed the case for low degree polynomials. We show that for the broadcast problem using any Markov chain on trees with \(N\) leaves, below the Kesten Stigum bound, any \(O(\log N)\) degree polynomial has vanishing correlation with the root.

Our result is one of the first low-degree lower bound that is proved in a setting that is not based or easily reduced to a product measure.

## 1 Introduction

Understanding the computational complexity inference problems of random instances has been extensively studies in different research areas including statistics, cryptography, computational complexity, computational learning theory and statistical physics. The emerging field of research is mainly devoted to the study of computational-to-statistical gaps. ([3, 41, 24])

Recently, low-degree polynomials have emerged as a popular tool for predicting computational-to-statistical gaps, especially in the context of the Bayesian framework. Our work follows [23] in studying the polynomial hardness of broadcasting on trees.

A very exciting line of work, including [19, 18, 25, 4, 15, 26, 17, 9, 40] recently showed that the "low-degree heuristic" can be used to predict computational-statistical gaps for a variety of problems such as recovery in general stochastic block models, sparse PCA, tensor PCA, the planted clique problem, certification in the zero-temperature Sherrington-Kirkpatrick model, the planted sparse vector problem, and for finding solutions in random \(k\)-SAT problems.

Interestingly, it was observed that the predictions from this method often agree with predictions from statistical physics heuristics based on the replica and cavity methods which are closely related to the analysis of BP/AMP fixed points, see e.g. [12, 13, 28]).

It is often argued that low-degree polynomials algorithms are relatively easy to use (e.g. compared to proving SOS lower bounds), and that low degree polynomials capture the power of the "local algorithms" framework used in e.g. [16, 10] as well as algorithms which incorporate global information, such as spectral methods or a constant number of iterations of Approximate Message Passing [29].

In this work, we continue to study the power of low-degree polynomials for the (average case) broadcast on trees problem. In broadcasting on trees the goal is to estimate the value of the Markov process at the root given its value at the leaves and the goal is to do so for arbitrarily deep trees. Two key parameters of the model are the arity of the tree \(d\) and the magnitude of the second eigenvalue \(\lambda\) of the broadcast chain.

A fundamental result in this area [22] is that when \(d|\lambda|^{2}>1\) nontrivial reconstruction of the root is possible by counting the number of the leaves of different types, an algorithm that could be described as a linear function of the leave values. In contrast, when \(d|\lambda|^{2}<1\), such linear estimators have no mutual information with the root (but more complex statistics of the leaves may) [30, 36].

This threshold \(d|\lambda|^{2}=1\) is known as the _Kesten-Stigum threshold_. A series of works showed that the KS threshold is the information theory threshold for non-trivial root inference for some specific channels, including the binary symmetric channel [6, 14, 21, 20] and binary channels that are close to symmetric [8], as well as \(3\times 3\) symmetric channels for large \(d\)[39].

While the Kesten-Stigum bound is easy to compute, it turns out that in many cases, it is _not_ the information-theoretic threshold for root recovery. This was first established in [30] for symmetric channels with sufficiently many states, specifically when \(q\geq C\) for some large constant \(C\). Later it was shown for symmetric channels with \(q\geq 5\) states in [39]. Recently, the results [37] provide more information about the case of \(q=3\) and \(q=4\). Many of the finer results in this area prove predictions from statistical physics. The connection between the broadcast problems and phase transitions in statistical physics was made in [27], and more recent predictions include [5, 2, 38]. Moreover, the information-theoretic threshold may depend on the specific structure of the channel rather than solely on \(d\) and \(\lambda\). Notably, [30] also showed that there exists channels where non-trivial predictions of the root are achievable even when \(|\lambda|=0\).

Much of the interest in Kesten-Stigum threshold comes from the fundamental role it plays in problems, such as algorithmic recovery in the stochastic block model [11, 33, 7, 35, 1] and phylogenetic reconstruction [31]. Count statistics can be viewed as degree 1 polynomials of the leaves, which begs the question of what information more general polynomials can extract from the leaves. See [32, 34] for surveys on the topic.

In [23] it was shown that \(\lambda=0\) even polynomials of degree \(N^{c}\), where \(N=d^{\ell}\) is the number of leaves of for a \(d\)-ary tree of depth \(\ell\), for a small \(c>0\) are not able to correlate with the root label (as \(\ell\) tends to \(\infty\)), whereas computationally efficient reconstruction is generally possible as long as \(d\) is a sufficiently large constant [30].

The main motivation of [23] was to prove that low degree polynomials fail below the Kesten Stigum bound: "It is natural to wonder if the Kesten-Stigum threshold \(d|\lambda|^{2}=1\) is sharp for low-degree polynomial reconstruction, analogous to how it is sharp for robust reconstruction." However the main result of [23] only established this in the very special case of \(\lambda=0\). This problem is also stated in the ICM 2022 paper and talk on the broadcast process [34]: " The authors of [23] ask if a similar phenomenon holds through the non-linear regime. For example, is it true that polynomials of bounded degree have vanishing correlation with \(X_{0}\) in the regime where \(d\lambda^{2}<1\)? " The main results of this paper prove that this is indeed the case. We proceed with formal definitions and statement of the main result.

### Definitions and Main Result

#### Rooted Tree

Recall that every rooted tree \(T\) inherently defines a partial order relation among its vertices: For a pair of distinct vertices, \(u\) is said to be an ancestor of \(v\) (and \(v\) a descendant of \(u\)), denoted as \(v<u\) in this paper, if \(u\) is contained in the unique path from \(v\) to the root \(\rho\). Specifically, if \(v\) and \(u\) are directly connected by an edge, we also refer to \(v\) as a child vertex of \(u\) (and \(u\) as the parent vertex of \(v\)). By \(v\leq u\) we mean that \(v\) is either a descendant of \(u\) or \(v=u\). In general, if \(v<u\) and the path distance between them is \(k\), \(v\) is referred to as a \(k\)th-descendant of \(u\) (and \(u\) as the \(k\)th ancestor of \(v\)).

For a nonnegative integer \(k\), the \(k\)th layer of the tree refers to the set of \(k\)th descendants of the root \(\rho\). (The root here is considered at the 0th layer of the tree.) The depth of the tree is defined as the largest non-negative integer \(\ell\) for which the \(\ell\)th layer is not empty, and we denote the \(\ell\)th layer by \(L\).

The height of a vertex \(u\) is defined as

\[\mathrm{h}(u)=\ell-\text{ the layer of }u.\] (1)

In particular, when \(L\) is the set of leaves, then \(\mathrm{h}(u)\) is simply the distance from \(u\) to \(L\). In this paper, we may abuse the notation by writing \(x\in T\) or \(S\subseteq T\) to mean that \(x\) is a vertex or \(S\) is a subset of vertices in the tree \(T\).

The standard rooted \(d\)-ary tree with depth \(\ell\) is a tree where each vertex \(u\notin L\) has exactly \(d\) children vertices. Let us start by defining the type of trees we will be investigating in this paper, which is a slight generalization of \(d\)-ary tree.

**Definition 1.1**.: _A rooted tree \(T\) with root \(\rho\) has degree dominated by \(d\geq 1\) with parameter \(R\geq 1\) if for every vertex \(u\) and positive integer \(k\), the number of \(k\)th descendants of \(u\) is at most \(Rd^{k}\)._

With the above definition, a \(d\)-ary rooted tree has degree dominated by \(d\geq 1\) with parameter \(R=1\). Further, a typical realization of a Galton-Watson tree of Poisson type with average degree \(d\) and of depth \(\ell\) (a random tree in which each vertex \(u\notin L\) has on average \(d\) children vertices) has a degree dominated by \(d\geq 1\) with parameter \(R\simeq\log(\ell)\).

#### Broadcasting Process on Rooted Trees

Next, we will define the broadcasting process on a rooted tree \(T\) with root \(\rho\). Consider a \(q\times q\) ergodic transition matrix \(M\), where \(q\geq 2\). Recall that every eigenvalue of a transition matrix \(M\) has an absolute value at most 1. Let \(0\leq\lambda\leq 1\) represent the second largest absolute value among the eigenvalues of \(M\). Additionally, we define the stationary distribution of \(M\) as \(\pi\).

The broadcasting process \(X=(X_{v})_{v\in T}\), with state space \([q]:=\{1,2,\ldots,q\}\) and transition matrix \(M\), can be formally described as follows: We initialize the value of \(X_{\rho}\) according to some initial distribution \(\nu\). As we reveal the values layer by layer, when the value \(X_{u}\) is revealed, the value \(X_{v}\) for any child vertex \(v\) of \(u\) is independently distributed according to a specific row of \(M\) depending on the value of \(X_{u}\):

\[\mathbb{P}\{X_{v}=t\,|\,X_{u}=s\}=M_{st}.\]

In other words, each vertex's value depends only on its parent vertex's value. The definition of the process is given below:

**Definition 1.2** (Broadcasting Process on Tree).: _Let \(q\geq 2\) be a positive integer. For any rooted tree \(T\) with root \(\rho\) and a \(q\times q\) ergodic transition matrix \(M\), the broadcasting process \(X=(X_{v})_{v\in T}\) with state space \([q]\), according to transition matrix \(M\) with an initial distribution \(\nu\) for \(X_{\rho}\), is a random process with joint distribution given by:_

\[\forall x=(x_{v})_{v\in T}\in[q]^{T},\ \ \mathbb{P}\{X=x\}=\nu(x_{\rho}) \prod_{(v,u)}M_{x_{u},x_{v}},\]

Figure 1: An example of a binary rooted tree of depth 5 is shown. The vertex \(u\) is at the 3rd layer and \(\mathrm{h}(u)=2\). Further, the following relationships hold: \(v<u\) and \(v\) is a child of \(u\), \(s\in L\) is a 2nd descendant of \(u\), \(w\) is the parent of \(u\), and \(t\) is the 2nd ancestor of \(u\).

_where the product is taken over all pairs \((v,u)\) such that \(v\) is a child vertex of \(u\)._

_For a subset of vertices \(A\subset T\), let us denote_

\[X_{A}=(X_{v})_{v\in A}\,.\]

If the tree is just a path, then the process reduces to a Markov chain. If we assume \(\nu=\pi\), then \(X_{v}\sim\pi\) for every \(v\in T\), as \((X_{v})_{v\in P}\) for every downward path of \(T\) forms a Markov Chain with transition matrix \(M\). Further, let us make a remark about the Markov property of the process.

**Remark 1.3** (Markov Property).: The broadcasting process establishes a **Markov Random Field** on tree \(T\): Given any three disjoint subsets \(A,B,\) and \(C\) of \(T\), if every path from a vertex in \(A\) to a vertex in \(C\) passes through a vertex in \(B\), then the random variables \(X_{A}\) and \(X_{C}\) are conditionally independent given \(X_{B}\).

Polynomials of \(x_{L}\) and the Main Result

**Definition 1.4**.: _Let \(x\in[q]^{T}\). For \(u\in T\), let \(x_{\leq u}=(x_{v})_{v\leq u}\). For subset \(U\subseteq T\), let \(x_{U}=(x_{u})_{u\in U}\)._

The next definition is about the notion of degrees for functions with variables \(x_{L}=(x_{v})_{v\in L}\). This is the generalization of degree of a polynomial.

**Definition 1.5** (Efron-Stein Degree).: _A function \(f\) with variables \(x_{L}\) has Efron-Stein degree at most \(k\) if it can be expressed as_

\[f=\sum_{\alpha}f_{\alpha},\]

_where the summation is over a finite set of indices \(\alpha\), and each \(f_{\alpha}\) is a function of the variable \(x_{S}\) for some \(S\subseteq L\) with \(|S|\leq k\)._

Now, we could properly formulate the main result of the paper:

**Theorem 1.6**.: _Let \(T\) be a rooted tree with root \(\rho\) of depth \(\ell\) and has degree dominated by \(d\geq 1\) with parameter \(R\geq 1\). Consider the broadcasting process on \(T\) with a \(q\times q\) transition matrix \(M\) and \(X_{\rho}\sim\pi\). If \(M\) is ergodic and \(d\lambda^{2}<1\), then there exists a constant \(c>0\) which depends on \(M\) and \(d\lambda^{2}\) such that the following holds: For any function \(f(x_{L})\) of Efron-Stein degree \(\leq c\frac{\ell}{1+\log(R)}\), we have_

\[\mathrm{Var}(\mathbb{E}\big{[}f(X_{L})\,\big{|}\,X_{\rho}\big{]})\leq(\max\{d \lambda^{2},\lambda\})^{\ell/4}\mathrm{Var}(f(X_{L})).\]

**Remark 1.7**.: Given that \(d\lambda^{2}<1\) implies \(\lambda^{2}<1\), and \(\lambda<1\) if and only if \(\lambda^{2}<1\), we can infer that \(\lambda<1\) from the given conditions. Consequently, the term \(\max\{d\lambda^{2},\lambda\}<1\) follows from the assumptions of the theorem. Therefore, the R.H.S. of the inequality decays exponentially with the depth of the tree.

Follows from the theorem and properties of conditonal expectation, we have the following corollary.

**Corollary 1.8**.: _With the same setting as in Theorem 1.6, for any function \(f(x_{L})\) of Efron-Stein degree \(\leq c\frac{\ell}{1+\log(R)}\), and any function \(g(x_{\rho})\) of the root value, their correlation satisfies_

\[\mathrm{Corr}(f(X_{L}),g(X_{\rho})):=\frac{\mathbb{E}\big{[}(f(X_{L})-\mathbb{ E}f(X_{L}))(g(X_{\rho})-\mathbb{E}g(X_{\rho}))\big{]}}{\sqrt{\mathrm{Var}(f(X_{L}) }\sqrt{\mathrm{Var}(g(X_{\rho}))}}\leq(\max\{d\lambda^{2},\lambda\})^{\ell/8}.\]

The proof of the theorem is based on recursion on a notion of _fractal capacity_ of functions. Indeed, the main result is optimal in the fractal sense (Theorem 1.16), as we will later demonstrate that all functions with fractal capacity up to a level proportional to \(\ell\) exhibit vanishing correlation with the root, whereas all functions of the leaves have fractal capacity at most \(\ell+1\). Let us introduce the necessary definitions and notations to introduce both the fractal capacity and the proof overview.

### Fractal Capacity and Proof Overview

To provide a clearer illustration, we establish a correspondence between the vertices of \(T\) and words of varying lengths from \(0\) to \(\ell\), with vertices at the \(k\)th layer represented as words of length \(k\). Wedenote the root \(\rho\) as the empty word \(()\). For each vertex \(u\), represented by the word \((b_{1},b_{2},\ldots,b_{k})\), we define \(d_{u}\) as the number of children vertices of \(u\), and we identify these children vertices as \((b_{1},b_{2},\ldots,b_{k},i)\) with \(i\in[d_{u}]:=\{1,2,\ldots,d_{u}\}\). Notice that \(v\) is a descendant of \(u\) is equivalent to \(u\) is a prefix of \(v\). For brevity, for each \(u=(b_{1},\ldots,b_{k})\in T\) and \(i\in[d_{u}]\), let

\[u_{i}:=(u,i)=(b_{1},\ldots,b_{k},i).\]

For \(I\subseteq[d_{u}]\), let

\[u_{I}=\{u_{i}\}_{i\in I}.\]

Furthermore, we denote the parent vertex of \(u\) as \(\mathfrak{p}(u)=(b_{1},\ldots,b_{k-1})\) and the set of children vertices of \(u\) as \(\mathfrak{c}(u)=u_{[d_{u}]}\).

**Definition 1.9**.: _For a non-empty subset \(S\subseteq L\), we introduce the notation \(\rho(S)\) to represent the nearest common ancestor of the vertices in \(S\), meaning that \(\rho(S)\) is the vertex with smallest height that is an ancestor of all vertices in \(S\)._

_Here we consider the case when \(|S|>1\). Notice that \(\rho(S)\) is not at the \(\ell\)th layer \(L\). For each child \(\rho(S)_{i}\) of \(\rho(S)\) for \(i\in[d_{\rho(S)}]\), we define the set \(S_{i}\) as_

\[S_{i}=S\cap\{v\in L\,:\,v\leq\rho(S)_{i}\},\]

_which is the collection of vertices in \(S\) that are descendants of \(\rho(S)_{i}\). Let_

\[I(S)=\{i\in[d_{\rho(S)}]\,:\,S_{i}\neq\emptyset\}.\]

_Then, \(S\) can be expressed as the disjoint union of the sets \(S_{i}\) for \(i\in I(S)\):_

\[S=\sqcup_{i\in I(S)}S_{i}.\]

_We call the above disjoint union the_ **branch decomposition** _of \(S\), and each \(S_{i}\) for \(i\in I(S)\) a_ **branch part** _of \(S\)._

The branch decomposition is a key concept in the proof, and we define the _fractal capacity_ according to the number of iterations to decompose \(S\) into singletons.

**Definition 1.10**.: _Let_

\[\mathcal{A}_{1}:=\Big{\{}\{u\}\,:\,u\in L\Big{\}}\subseteq\mathbf{2}^{L} \backslash\{\emptyset\},\]

_be the collection of singletons of \(L\). We say a subcollection \(\mathcal{A}\subseteq\mathbf{2}^{L}\backslash\{\emptyset\}\) is_ **closed under decomposition** _with base \(\mathcal{A}_{1}\) if for every \(S\in\mathcal{A}\backslash\mathcal{A}_{1}\), we have \(S_{i}\in\mathcal{A}\) for \(i\in I(S)\)._

**Definition 1.11**.: _For any \(\mathcal{A}\subseteq\mathbf{2}^{L}\backslash\{\emptyset\}\), let_

\[\mathcal{B}(\mathcal{A})\subseteq\mathbf{2}^{L}\backslash\{\emptyset\}\]

_be a new subcollection defined according to the following rules:_

_For any \(S\in\mathbf{2}^{L}\backslash\{\emptyset\}\), \(S\in\mathcal{B}\) if and only if one of the following two conditions holds_

Figure 2: In the these figures, we present the vertices as words and adapt the notations \(u_{1},u_{2}\),etc., for the descendants of \(u\). For the right figure, if \(S=\{u_{21},u_{22},u_{32}\}\), then \(\rho(S)=u\), \(I(S)=\{2,3\}\), and \(S_{2}=\{u_{21},u_{22}\}\), \(S_{3}=\{u_{32}\}\). Further, \(S_{1}\in\mathcal{A}_{3}\), \(S_{2}\in\mathcal{A}_{2}\), and \(S_{3}\in\mathcal{A}_{1}\).

1. \(S\in\mathcal{A}_{1}\)_._
2. \(S\notin\mathcal{A}_{1}\) _and_ \(S_{i}\in\mathcal{A}\) _for_ \(i\in I(S)\)_._

For example, \(\mathcal{B}(\mathcal{A})\) contains sets of size \(\leq 2\).

**Lemma 1.12**.: _If \(\mathcal{A}_{1}\subseteq\mathcal{A}\subseteq\mathbf{2}^{L}\backslash\{\emptyset\}\) is a subcollection closed under decomposition with base \(\mathcal{A}_{1}\), then the collection \(\mathcal{B}=\mathcal{B}(\mathcal{A})\) contains \(\mathcal{A}\) and it is also closed under decomposition with base \(\mathcal{A}_{1}\)._

Proof.: To show \(\mathcal{A}\subseteq\mathcal{B}\), it is sufficient to show \(\mathcal{A}\backslash\mathcal{A}_{1}\subseteq\mathcal{B}\). For any \(S\in\mathcal{A}\backslash\mathcal{A}_{1}\), because \(\mathcal{A}\) is closed under decomposition, \(S_{i}\in\mathcal{A}\) for \(i\in I(S)\). Hence, \(S\in\mathcal{B}\) follows from the definition of \(\mathcal{B}\). Now, for \(S\in\mathcal{B}\backslash\mathcal{A}_{1}\), each \(S_{i}\) with \(i\in I(S)\) is contained in \(\mathcal{A}\subseteq\mathcal{B}\), which in turn implies \(\mathcal{B}\) is closed under decomposition. 

Now, we define recursively that

\[\mathcal{A}_{k}=\mathcal{B}(\mathcal{A}_{k-1}),\] (2)

for positive integer \(k\geq 2\). Observe the following two facts: Consider any non-singleton set \(S\subseteq L\) and \(S_{i}\) with \(i\in I(S)\). First, \(S\in\mathcal{A}_{k}\Rightarrow S_{i}\in\mathcal{A}_{k-1}\) by the definition of \(\mathcal{A}_{k}\). Second, \(\rho(S)>\rho(S_{i})\) by the definition of branch decomposition. Given these two facts, we can prove inductively that

\[\mathrm{h}(\rho(S))=k\Rightarrow S\in\mathcal{A}_{k+1}.\]

Since that there are only \(\ell\) layers of the tree, we conclude that every non-emptyset of \(S\subseteq L\) is in \(\mathcal{A}_{\ell+1}\). Therefore, together with Lemma 1.12, we have the following chain of subcollections:

\[\{\{u\}\,:\,u\in L\}=\mathcal{A}_{1}\subseteq\mathcal{A}_{2}\subseteq\cdots \subseteq\mathcal{A}_{\ell+1}=\mathbf{2}^{L}\backslash\{\emptyset\}.\]

**Definition 1.13** (Fractal Capacity).: _For any non-empty subset \(S\subseteq L\), we define the_ **fractal capacity** _of \(S\) as the smallest \(k\) such that \(S\in\mathcal{A}_{k}\)._

We introduce the notion of fractal capacity, borrowing terminology from fractal geometry. The recursive nature of our definition on subsets of trees mirrors the self-similar complexity found in fractal structures. This recursive and inherently intricate structure motivates our choice of the term fractal capacity, capturing the fractal-like properties that emerge in the collections \((\mathcal{A}_{k})_{k\in[l+1]}\).

**Definition 1.14**.: _Given a collection \(\mathcal{A}\subseteq\mathbf{2}^{L}\backslash\{\emptyset\}\). A function \(f:[q]^{T}\to\mathbb{R}\) is called an \(\mathcal{A}\)-**polynomial** if we can express_

\[f(x)=\sum_{S\in\mathcal{A}}f_{S}(x_{S})\]

_where each \(f_{S}\) is a function of \(x_{S}=(x_{v})_{v\in S}\). A function \(f:[q]^{T}\to\mathbb{R}\) has_ **fractal capacity** _\(\leq k\) if it is a \(\mathcal{A}_{k}\)-polynomial._

**Remark 1.15**.: It is not hard to verify that \(\mathcal{A}_{k}\) contains all non-empty subsets \(S\subseteq L\) with \(|S|\leq k\). Thus, for any function \(f\) with variables \(x_{L}\),

\[\text{Efron-Stein degree of $f\leq k\Rightarrow f$ is an $\mathcal{A}_{k}$-polynomial}.\]

On the other hand, it is worth to remark that for the \(d\)-ary tree of depth \(\ell\geq k\), there exists \(S\in\mathcal{A}_{k}\) with \(|S|=d^{k-1}\). (Namely, taking \(S=\{v\in L\,:\,v<u\}\) for some \(u\) with \(\mathrm{h}(u)=k-1\).) Thus, an \(\mathcal{A}_{k}\)-polynomial could have an Efron-Stein degree exponential in \(k\).

The main result of the paper in terms of the fractal capacity is the following:

**Theorem 1.16**.: _With the same setting as in Theorem 1.6, there exists a constant \(c>0\) which depends on \(M\) and \(d\lambda^{2}\) such that the following holds: For any function \(f(x_{L})\) with_ **fractal capacity**__\(\leq c\frac{\ell}{1+\log(R)}\), we have_

\[\mathrm{Var}(\mathbb{E}\big{[}f(X_{L})\,\big{|}\,X_{\rho}\big{]})\leq(\max\{d \lambda^{2},\lambda\})^{\ell/4}\mathrm{Var}(f(X_{L})).\]

Indeed, Theorem 1.6 is a direct consequence of Theorem 1.16, as \(\mathcal{A}_{k}\)-polynomials contains all polynomials of Efron-Stein degree \(\leq k\). Further, in terms of fractal capacity, the theorem is optimal because the correlation decay persists up to an order proportional to \(\ell\), while a fractal capacity \(\leq\ell\) includes all functions of the leaves.

**Overview of the Proof Idea:** For illustration, let us consider the case where \(T\) is a binary tree of depth \(\ell\) with \(M=\begin{bmatrix}\frac{1+\lambda}{2}&\frac{1-\lambda}{2}\\ \frac{1-\lambda}{2}&\frac{1+\lambda}{2}\end{bmatrix}\), such matrix has eigenvalues \(\lambda\) and \(1\). Here we assume \(2\lambda^{2}<1\).

We recall that for this binary symmetric broadcasting process, it is information-theoretically impossible to recover the root label from the leaves below the KS bound. This implies that all polynomials of \(X_{L}\) have vanishing correlation with \(X_{\rho}\). Still, we use this simple process to illustrate the proof idea as our arguments for low-degree polynomials generalize to general broadcasting processes below the Kesten-Stigum threshold, including cases where it is information theoretically possible to estimate the root from the leaves non-trivially (in such cases there exist functions \(f\) so that \(\lim\inf_{\ell\rightarrow\infty}\operatorname{Corr}(f(X_{L}),X_{\rho})>0\)).

Now, let us consider degree-\(1\) polynomials. Suppose \(f\) is a \(\mathcal{A}_{1}\)-polynomial (equivalently, of Efron-Stein degree 1), we can express it in the form

\[f(x_{L})=\sum_{u\in L}f_{u}(x_{u}),\]

where each \(f_{u}\) is a function of \(x_{u}\). Given our focus on the variance, we may assume \(\mathbb{E}f_{u}(X_{u})=0\) for each \(u\in L\). Then, our goal is to prove \(\mathbb{E}\big{[}\big{(}\mathbb{E}[f(X_{L})\,|\,X_{\rho}]\big{)}^{2}\big{]}\) is negligible comparing to \(\mathbb{E}\big{[}(f(X_{L}))^{2}\big{]}\). Following from the Cauchy-Schwarz inequality that

\[(\sum_{i\in[m]}a_{i})^{2}=(\sum_{i\in[m]}1\cdot a_{i})^{2}\leq m \sum_{i\in[k]}a_{i}^{2},\]

we have

\[\mathbb{E}\big{[}\big{(}\mathbb{E}[f(X_{L})\,|\,X_{\rho}]\big{)}^ {2}\big{]}\leq |L|\sum_{u\in L}\mathbb{E}\big{[}\big{(}\mathbb{E}[f_{u}(X_{u})\,| \,X_{\rho}]\big{)}^{2}\big{]}\] (3) \[\lesssim 2^{\ell}\sum_{u\in L}\lambda^{2\ell}\mathbb{E}\big{[}(f_ {u}(X_{u}))^{2}\big{]}=(2\lambda^{2})^{\ell}\sum_{u\in L}\mathbb{E}\big{[}(f_ {u}(X_{u}))^{2}\big{]},\]

where the second inequality is derived from the variance decay property of in a Markov Chain. Thus, if we can establish \(\sum_{u\in L}\mathbb{E}\big{[}(f_{u}(X_{u}))^{2}\big{]}\) is at the same order as \(\mathbb{E}\big{[}(f(X_{L}))^{2}\big{]}\), the proof is complete. Notice that

\[\mathbb{E}\big{[}(f(X_{L}))^{2}\big{]}=\sum_{u,v\in L}\mathbb{E}[f_{u}(X_{u}) f_{v}(X_{v})]=\sum_{u\in L}\mathbb{E}\big{[}(f_{u}(X_{u}))^{2}\big{]}+\sum_{u\neq v \in L}\mathbb{E}[f_{u}(X_{u})f_{v}(X_{v})].\]

Thus, the goal here is to show

\[\Big{|}\sum_{u\neq v\in L}\mathbb{E}[f_{u}(X_{u})f_{v}(X_{v})] \Big{|}<c\sum_{u\in L}\mathbb{E}\big{[}(f_{u}(X_{u}))^{2}\big{]},\] (4)

for some constant \(c\in(0,1)\), which in turn implies the desired result:

\[\mathbb{E}\big{[}\big{(}\mathbb{E}[f(X_{L})\,|\,X_{\rho}]\big{)}^{2}\big{]} \lesssim(2\lambda^{2})^{\ell}\sum_{u\in L}\mathbb{E}\big{[}(f_{u}(X_{u}))^{2} \big{]}\lesssim(2\lambda^{2})^{\ell}\frac{1}{1-c}\mathbb{E}\big{[}(f(X_{L}))^ {2}\big{]}.\]

Roughly speaking, (4) holds if for most pairs \(u\) and \(v\) within \(L\), the correlation between \(f_{u}(X_{u})\) and \(f_{v}(X_{v})\) is sufficiently small, which is the case for degree-1 polynomials.

Now, let us take a closer look. Fix any two vertices \(u\) and \(v\) in \(L\), with \(w\) as their nearest common ancestor. Suppose \(u\leq w_{1}\) and \(v\leq w_{2}\). Let \(X_{\not\in w_{1}}=(X_{u^{\prime}})_{u^{\prime}\not\in w_{1}}\). We have

\[\big{|}\mathbb{E}[f_{u}(X_{u})f_{v}(X_{v})]\big{|}= |\mathbb{E}[\mathbb{E}[f_{u}(X_{u})\,|\,X_{\not\in w_{1}}]f_{v}(X_{v })]|\] (5) \[\leq \sqrt{\mathbb{E}[(\mathbb{E}[f_{u}(X_{u})\,|\,X_{\not\in w_{1}}]) ^{2}]}\cdot\sqrt{\mathbb{E}[(f_{v}(X_{v}))^{2}]}\] \[\lesssim \lambda^{h(w)}\sqrt{\mathbb{E}[(f_{u}(X_{u}))^{2}]}\sqrt{\mathbb{ E}[(f_{v}(X_{v}))^{2}]},\]

where the last inequality follows from the variance decay of the Markov Chain for length \(\mathrm{h}(w)\). The above inequality implies that the correlation between \(f_{u}(X_{u})\) and \(f_{v}(X_{v})\) is at most of order \(\lambda^{\mathrm{h}(w)}\).

The above bound can be improved to \(\lambda^{2k}\) by also taking the conditional expectation \(\mathbb{E}[f_{v}(X_{v})\,|\,X_{\not\in w_{2}}]\) into account, which requires the Markov Property that \(X_{u}\) and \(X_{v}\) are independent conditioned on \(X_{w}\). From here, one can properly arrange the terms and apply Cauchy-Schwarz inequality to show (4) holds.

Our proof of the main theorem tries to generalize the argument above to low degree polynomials. Let us summarize it by the following five pieces of descriptions.

**I. Bounding Covariance:** First, we generalized the idea on how (5) works for degree-1 polynomials. Suppose \(f_{\alpha}\) and \(f_{\beta}\) are two functions so that the following holds.

1. \(f_{\alpha}(x_{S})\) is a function of \(x_{S}\) for some set \(S\subseteq L\) such that \[\mathbb{E}[(\mathbb{E}[f_{\alpha}(X_{S})\,|\,X_{\not\in w^{\prime}}])^{2}] \ll(\mathbb{E}(f_{\alpha}(X_{S}))^{2},\] where we use \(a\ll b\) to indicate \(a\) is much smaller than \(b\). We keep this not precise to avoid technical details, but expect that the ratio is exponentially small in \(\mathrm{h}(w^{\prime})\).
2. \(f_{\beta}(x_{S^{\prime}})\) is a function of \(x_{S^{\prime}}\) with \(S^{\prime}\subseteq L\) satisfying \(S^{\prime}\cap\{v^{\prime}\,:\,v^{\prime}\leq w^{\prime}\}=\emptyset\).

Then, following the same derivation as shown in (5) we have

\[\big{|}\mathbb{E}[f_{\alpha}(X_{S})f_{\beta}(X_{S^{\prime}})]\big{|}=\big{|} \mathbb{E}\big{[}\mathbb{E}[f_{\alpha}(X_{S})\,|\,X_{\not\in w^{\prime}}]f_{ \beta}(X_{S^{\prime}})\big{]}\big{|}\ll\sqrt{\mathbb{E}[(f_{\alpha}(X_{S}))^{ 2}]}\sqrt{\mathbb{E}[(f_{\beta}(X_{S^{\prime}}))^{2}]}.\]

(See Figure 3 for an illustration.)

**II. Choosing a good decomposition of the function:** In essence, our proof strategy for any given function \(f(x_{L})\) with \(\mathbb{E}f(X_{L})=0\) revolves around decomposing \(f(x_{L})\) into a sum of functions \(f_{\alpha}(x_{L})\) for \(\alpha\) in some index set \(\mathcal{I}\), such that

1. \(|\mathcal{I}|\lesssim 2^{\ell}\),
2. For each \(\alpha\), \(\mathbb{E}f_{\alpha}(X_{L})=0\) and \(\mathbb{E}[(\mathbb{E}[f_{\alpha}(X_{L})\,|\,X_{\rho}]^{2}]\ll\mathbb{E}[(f_{ \alpha}(X_{L}))^{2}]\),
3. Whenever \(\alpha\neq\beta\), we can find \(w\in T\) so that \(f_{\alpha}\) and \(f_{\beta}\) satisfy the covariance bound in **I**. (Possibly with a switch of the roles of \(\alpha\) and \(\beta\)).

Let us remark that while we are writing \(f_{\alpha}(x_{L})\), it does not mean that \(f_{\alpha}\) depends on every leave variables.

If this is the case, then we could follow the argument in the degree 1 case to show that desired result holds.

**III. From \(\mathcal{A}_{k}\) polynomials to \(\mathcal{A}_{k+1}\) polynomials:** The proof of the main theorem builds on **I** and **II** and advancing through a recursion on the fractal capacity of the function. This recursive approach relies on the following property:

Suppose we have shown the second moment decay \(\mathcal{A}_{k}\)-polynomials with mean \(0\) in the following sense: For every \(\mathcal{A}_{k}\)-polynomial \(f(x_{S})\) with mean \(0\) and variable \(x_{S}\) where \(S\subseteq\{v\in L\,:\,v\leq\rho^{\prime}\}\) for some vertex \(\rho^{\prime}\),

\[\mathbb{E}[(\mathbb{E}[f(X_{L})\,|\,X_{\rho^{\prime}}])^{2}]\leq(2\lambda^{2} )^{\mathrm{h}(\rho^{\prime})-\mathrm{h}_{\mathcal{A}_{k}}}\mathbb{E}[(f(X_{L}) )^{2}],\] (6)

Figure 3: In the left figure, the purple dots represent the corresponding input variables for \(f_{\alpha}\), and the yellow dots represent the corresponding input variables for \(f_{\beta}\). In the right figure, the purple dots represent the corresponding input variables for \(\mathbb{E}[f_{\alpha}(X)\,|\,X_{\not\in w_{1}}]\) and the variables do not involve \(x_{v}\) for vertex \(v\) not illustrated due to the Markov property.

where \({\rm h}_{{\cal A}_{k}}\) is some penalty constant depending on \({\cal A}_{k}\). (The bigger the value \({\rm h}_{{\cal A}_{k}}\), the weaker the second moment decay.)

Consider a specific type of \({\cal A}_{k+1}\)-polynomials. Fix a pivot vertex \(w\) with \({\rm h}(w)\) large enough, let \(x_{\leq w_{i}}=(x_{v})_{v\leq w_{i}}\) for \(i\in[2]\). Let \(g\) be a function of the form

\[g(x)=g(x_{\leq w_{1}},x_{\leq w_{2}})=\sum_{\alpha\in{\cal J}}g_{\alpha,1}(x_{ \leq w_{1}})\cdot g_{\alpha,2}(x_{\leq w_{2}}),\]

where \({\cal J}\) is a finite index set and every \(g_{\alpha,i}\) is an \({\cal A}_{k}\)-polynomial whose variables are \(x_{\leq w_{i}}\) (more precisely, its variables are \(x_{S^{\prime}}\) where \(S^{\prime}\subseteq\{v\in L\,:\,v\leq w_{i}\}\)) and \({\mathbb{E}}g_{\alpha,i}(X_{\leq w_{i}})=0\). Then, the function \(g(x)\) is a \({\cal A}_{k+1}\)-polynomial with variable \(x_{S}\) for \(S\subseteq\{v\in L\,:\,v\leq w\}\).

Observe that, if we fix \(x_{\leq w_{2}}\), then \(x_{\leq w_{1}}\mapsto g(x_{\leq w_{1}},x_{\leq w_{2}})\) is an \({\cal A}_{k}\)-polynomial with variable \(x_{S^{\prime}}\) for \(S^{\prime}\subseteq\{v\in L\,:\,v\leq w_{1}\}\) and mean \(0\). This allows us to apply the assumption for \({\cal A}_{k}\) polynomials to show

\[{\mathbb{E}}[({\mathbb{E}}[g(X_{\leq w_{1}},x_{\leq w_{2}})\,|\,X_{\not\leq w _{1}}])^{2}]\lesssim(d\lambda^{2})^{{\rm h}(w)-{\rm h}_{{\cal A}_{k}}}{ \mathbb{E}}[(g(X_{\leq w_{1}},x_{\leq w_{2}}))^{2}].\]

Clearly, the same inequality holds with the roles of \(x_{\leq w_{1}}\) and \(x_{\leq w_{2}}\) switched.

Base on this, one key step in the paper is to show \(g\) satisfies

\[{\mathbb{E}}[({\mathbb{E}}[g(X_{\leq w_{1}},X_{\leq w_{2}})\,|\,X_{\not\leq w _{i}}])^{2}]\lesssim(d\lambda^{2})^{{\rm h}(w)-{\rm h}_{{\cal A}_{k}}}{ \mathbb{E}}[(g(X_{\leq w_{1}},X_{\leq w_{2}}))^{2}]\mbox{ for }i\in[2].\] (7)

This inequality is immediate if \(X_{\leq w_{1}}\) and \(X_{\leq w_{2}}\) are independent, which is not the case in the broadcasting process. One of the main technical challenges in the proof is to show that the inequality holds when \(X_{\leq w_{1}}\) and \(X_{\leq w_{2}}\) are conditionally independent given \(X_{w}\) by the Markov Property. It turns out to impose a significant technical challenge when some entries of \(M\) can be \(0\).

Observe that (7) not only implies \(g\) satisfies the desired second moment decay for \({\bf II}(2)\), but also \({\bf I}(1)\) with \(w^{\prime}\) to be either \(w_{1}\) or \(w_{2}\). Indeed, these two properties will also hold for \(\tilde{g}(x):=g(x)-{\mathbb{E}}g(X)\), the normalized \(g\) with mean \(0\), due to \(({\mathbb{E}}g(X))^{2}\) is negligible comparing to \({\mathbb{E}}g(X)^{2}\).

**IV. A closer look at the decomposition:**

Consider any \({\cal A}_{k+1}\)-polynomial \(f\) of variable \(x_{S}\) for \(S\subseteq\{v\in L\,:\,v\leq\rho^{\prime}\}\) for some vertex \(\rho^{\prime}\). Before we proceed to the discussion of the decomposition, we remark that one cannot simply express \(f\) in the form of \(\tilde{g}\) described above with any pivot vertex \(w\).

Let us give an example to illustrate why: Consider two functions \(f_{1},f_{2}\), where \(f_{i}\) is a function of \(x_{S^{\prime}}\) with \(S^{\prime}\subseteq\{v\in L\,:\,v\leq\rho^{\prime}_{i}\}\) and \(S^{\prime}\in{\cal A}_{k+1}\backslash{\cal A}_{k}\). Let \(f=f_{1}+f_{2}\). Then one can justify that \(f\) cannot be expressed in the form of \(\tilde{g}\) with any pivot \(w\) by using the property "if we fix \(x_{\leq w_{2}}\), then \(x_{x_{\leq w_{1}}}\mapsto g(x_{\leq w_{1}},x_{\leq w_{2}})\) is a \({\cal A}_{k}\)-polynomial" discussed in **III** to derive a contradiction.

The way we decompose \(f\) is to express it as a sum of functions of the form \(\tilde{g}(x)\) in **III**. While the actual decomposition requires a bit more adjustment, it follows from the idea to decompose \(f\) in the form

\[f=\sum_{w\in{\cal I}}f_{w},\]

where the index set \({\cal I}\) is the set of vertices of \(T\) with height slightly greater than \({\rm h}_{{\cal A}_{k}}\) (to ensure correlation decay). Each \(f_{w}\) is a function with variable \(x_{S}\) for \(S\subseteq\{u\in L\,:\,u\leq w\}\) satisfies the description of \(\tilde{g}(x)\) in **III**.

Observe that this decomposition satisfies the description in **II**:

* The size of \({\cal I}\) is bounded by \(2^{\ell}+2^{\ell-1}+\cdots 1\leq 2^{\ell+1}\).
* The second moment decay property of \(f_{w}\) follows from **III**.
* Finally, consider \(u,v\in{\cal I}\). If \(v<u\), say \(v\leq u_{1}\), then \(f_{u}\) and \(f_{v}\) satisfies the covariance bound condition stated in **I** with \(f_{\alpha}=f_{u}\), \(f_{\beta}=f_{2}\), and \(w^{\prime}=u_{2}\). The case when \(u<v\) is similar. When \(u\) and \(v\) are not comparable, then following the Markov Property, \({\mathbb{E}}[f_{u}(X)f_{v}(X)]={\mathbb{E}}{\mathbb{E}}[f_{u}(X)\,|\,X_{\not\leq w }]{\mathbb{E}}[f_{v}(X)\,|\,X_{\not\leq w}]\) with \(w\) being the nearest common ancestor of \(u\) and \(v\) and \(X_{\not\leq w}=(X_{w^{\prime}})_{w^{\prime}\not\leq w}\), which makes it easier to show the covariance bound. (See Figure 4 for an illustration.)With this desirable decomposition, one could try to apply some argument similar to the degree-1 case to show the second moment decay (6) for mean \(0\)\(\mathcal{A}_{k+1}\)-polynomials with a slightly bigger penalty constant \(\mathrm{h}_{\mathcal{A}_{k+1}}\) than \(\mathrm{h}_{\mathcal{A}_{k}}\).

**V. Overview on the induction** Given the decomposition of \(f\) as described in **IV**, together with the second moment assumption (6) on \(\mathcal{A}_{k}\)-polynomials described in **III**, the proof of the main theorem proceeds by induction. The goal is to show that the penalty constant \(\mathrm{h}_{\mathcal{A}_{k}}\) associated with \(\mathcal{A}_{k}\), which appeared (6), satisfies the following recursive inequality:

\[\mathrm{h}_{\mathcal{A}_{k+1}}\leq\mathrm{h}_{\mathcal{A}_{k}}+C,\;\mathrm{ and}\;\mathrm{h}_{\mathcal{A}_{1}}\leq C\]

for some constant \(C\) depending on \(M\) and \(d\lambda^{2}\). If true, by taking \(k\) to be proportional to \(\ell/C\), then the theorem follows. The proof of the theorem requires a careful analysis of the covariance and variance decay to demonstrate that it resembles the behavior observed in the degree-1 case, in order to capture the Kesten-Stigum bound.

**Comparison to other work in low-degree polynomials** While some high-level ideas align with previous work in low degree polynomials mentioned fore, our approach focuses on establishing low-degree lower bounds in a setting where direct comparisons are challenging due to structural differences. Specifically, our work addresses the broadcasting process on trees, where the underlying structures are highly correlated and do not naturally lend themselves to a product measure representation, presenting unique technical challenges not encountered in more independent setups.

Figure 4: In both figures, the purple dots represent the corresponding input variables for \(f_{u}\), and the yellow dots represent the corresponding input variables for \(f_{v}\). In the left figure, we have \(v\leq u_{1}<u\). In the right figure, we have \(u\) and \(v\) are incomparable and \(w\) is the nearest common ancestor.

## Acknowledgments

Han Huang was supported by Elchanan Mossel's Vannevar Bush Faculty Fellowship ONR-N00014-20-1-2826 and by Elchanan Mossel's Simons Investigator award (622132). Elchanan Mossel was partially supported by Bush Faculty Fellowship ONR-N00014-20-1-2826, Simons Investigator award (622132), ARO MURI W911NF1910217 and NSF award CCF 1918421.

## References

* [1] Emmanuel Abbe. Community detection and stochastic block models: recent developments. _The Journal of Machine Learning Research_, 18(1):6446-6531, 2017.
* [2] Emmanuel Abbe and Colin Sandon. Proof of the achievability conjectures for the general stochastic block model. _Communications on Pure and Applied Mathematics_, 71(7):1334-1406, 2018.
* [3] Afonso Bandeira, Amelia Perry, and Alexander S. Wein. Notes on computational-to-statistical gaps: predictions using statistical physics. _Port. Math._, 75(2), 2018.
* [4] Afonso S Bandeira, Dmitriy Kunisky, and Alexander S Wein. Computational hardness of certifying bounds on constrained pca problems. In _ITCS_, 2020.
* [5] Jess Banks, Cristopher Moore, Joe Neeman, and Praneeth Netrapalli. Information-theoretic thresholds for community detection in sparse networks. In _Conference on Learning Theory_, pages 383-416. PMLR, 2016.
* [6] P. M. Bleher, J. Ruiz, and V. A. Zagrebnov. On the purity of the limiting Gibbs state for the Ising model on the Bethe lattice. _J. Statist. Phys._, 79(1-2):473-482, 1995.
* [7] Charles Bordenave, Marc Lelarge, and Laurent Massoulie. Non-backtracking spectrum of random graphs: community detection and non-regular ramanujan graphs. In _Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on_, pages 1347-1357. IEEE, 2015.
* [8] C. Borgs, J. Chayes, E. Mossel, and S. Roch. The kesten-stigum reconstruction bound is tight for roughly symmetric binary channels. In _Proceedings of IEEE FOCS 2006_, pages 518-530, 2006.
* [9] Guy Bresler and Brice Huang. The algorithmic phase transition of random \(k\)-sat for low degree polynomials. In _FOCS_, 2021.
* [10] Wei-Kuo Chen, David Gamarnik, Dmitry Panchenko, and Mustazee Rahman. Suboptimality of local algorithms for a class of max-cut problems. _The Annals of Probability_, 47(3):1587-1618, 2019.
* [11] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborova. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. _Physics Review E_, 84:066106, Dec 2011.
* [12] Aurelien Decelle, Florent Krzakala, Cristopher Moore, and Lenka Zdeborova. Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications. _Physical Review E_, 84(6):066106, 2011.
* [13] Yash Deshpande and Andrea Montanari. Finding hidden cliques of size \(\sqrt{N/e}\) in nearly linear time. _Foundations of Computational Mathematics_, 15(4):1069-1128, 2015.
* [14] W. S. Evans, C. Kenyon, Yuval Y. Peres, and L. J. Schulman. Broadcasting on trees and the Ising model. _Ann. Appl. Probab._, 10(2):410-433, 2000.
* [15] David Gamarnik, Aukosh Jagannath, and Alexander S Wein. Low-degree hardness of random optimization problems. In _2020 IEEE 61st Annual Symposium on Foundations of Computer Science (FOCS)_, pages 131-140. IEEE, 2020.

* [16] David Gamarnik and Madhu Sudan. Limits of local algorithms over sparse random graphs. In _Proceedings of the 5th conference on Innovations in theoretical computer science_, pages 369-376, 2014.
* [17] Justin Holmgren and Alexander S Wein. Counterexamples to the low-degree conjecture. In _ITCS_, 2020.
* [18] Samuel Hopkins. _Statistical inference and the sum of squares method_. PhD thesis, Cornell University, 2018.
* [19] Samuel Hopkins and David Steurer. Efficient bayesian estimation from few samples: community detection and related problems. In _2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 379-390. IEEE, 2017.
* [20] D. Ioffe. Extremality of the disordered state for the Ising model on general trees. In _Trees (Versailles, 1995)_, volume 40 of _Progr. Probab._, pages 3-14. Birkhauser, Basel, 1996.
* [21] D. Ioffe. On the extremality of the disordered state for the Ising model on the Bethe lattice. _Lett. Math. Phys._, 37(2):137-143, 1996.
* [22] Harry Kesten and Bernt P Stigum. Additional limit theorems for indecomposable multidimensional galton-watson processes. _The Annals of Mathematical Statistics_, 37(6):1463-1481, 1966.
* [23] Frederic Koehler and Elchanan Mossel. Reconstruction on trees and low-degree polynomials. _Advances in Neural Information Processing Systems_, 35:18942-18954, 2022.
* [24] D. Kunisky, A.S. Wein, and A.S. Bandeira. Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood ratio. _Mathematical Analysis, its Applications and Computation. ISAAC 2019. Springer Proceedings in Mathematics and Statistics_, 385, 2022.
* [25] Dmitriy Kunisky, Alexander S Wein, and Afonso S Bandeira. Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood ratio. _arXiv preprint arXiv:1907.11636_, 2019.
* [26] Cheng Mao and Alexander S Wein. Optimal spectral recovery of a planted vector in a subspace. _arXiv preprint arXiv:2105.15081_, 2021.
* [27] M. Mezard and A. Montanari. Reconstruction on trees and the spin glass transition. _Journal of Statistical Physics_, 124:1317-1350, 2006.
* [28] Sidhanth Mohanty, Siqi Liu, and Prasad Raghavendra. On statistical inference when fixed points of belief propagation are unstable. In _IEEE 62st Annual Symposium on Foundations of Computer Science (FOCS)_, 2021.
* [29] Andrea Montanari and Alexander S. Wein. Equivalence of approximate message passing and low-degree polynomials in rank-one matrix estimation. _Probability Theory and Related Fields_, 2024.
* [30] E. Mossel. Reconstruction on trees: beating the second eigenvalue. _Ann. Appl. Probab._, 11(1):285-300, 2001.
* [31] E. Mossel. Phase transitions in phylogeny. _Trans. Amer. Math. Soc._, 356(6):2379-2404 (electronic), 2004.
* [32] E. Mossel. Survey: Information flow on trees. In J. Nestril and P. Winkler, editors, _Graphs, Morphisms and Statistical Physics. DIMACS series in discrete mathematics and theoretical computer science_, pages 155-170. 2004.
* [33] E. Mossel, J. Neeman, and A. Sly. Reconstruction and estimation in the planted partition model. _Probability Theory and Related Fields_, (3-4):431-461, 2015. The Arxiv version of this paper is titled Stochastic Block Models and Reconstruction.
* [34] Elchanan Mossel. Combinatorial statistics and the sciences. In _International Congress of Mathematicians: 2022 July 6-14_, volume 6, chapter 5553, pages 1-20. 2023.

* [35] Elchanan Mossel, Joe Neeman, and Allan Sly. A proof of the block model threshold conjecture. _Combinatorica_, 38(3):665-708, 2018.
* [36] Elchanan Mossel and Yuval Peres. Information flow on trees. _The Annals of Applied Probability_, 13(3):817-844, 2003.
* [37] Elchanan Mossel, Allan Sly, and Youngtak Sohn. Exact phase transitions for stochastic block models and reconstruction on trees. In _STOC 2023: Proceedings of the 55th Annual ACM Symposium on Theory of Computing_, pages 96-102, 2023.
* [38] Federico Ricci-Tersenghi, Guilhem Semerjian, and Lenka Zdeborova. Topology of phase transitions in bayesian inference problems. _Physical Review E_, 99(4):042109, 2019.
* [39] A. Sly. Reconstruction of random colourings. _Comm. Math. Phys._, 288, 2009.
* [40] Alexander S Wein. Optimal low-degree hardness of maximum independent set. _arXiv preprint arXiv:2010.06563_, 2020.
* [41] Lenka Zdeborova and Florent Krzakala. Statistical physics of inference: Thresholds and algorithms. _Advances in Physics_, 65(5), 2016.

## Appendix

**Overview**

* In Section A, we give additional notations and basic tools.
* In Section B, we formulate the main theorem we want to prove as an induction statement.
* In Section C, we discuss the case for degree 1 polynomial, which prove the base case of the induction in the theorem, and the results for degree 1 polynomial will be used in the inductive step.
* In Section D, we give a procedure to decompose \(\mathcal{B}\)-polynomial \(f\) for a given collection \(\mathcal{B}\).
* In Section E and F, we derive the proof of Theorem B.6, the inductive step for proving Theorem B.1.
* In Section G and H, we derive the main result in the general case.
* In Section I, we provide a proof of Proposition C.3, which is one technical obstacle for getting our main result from Theorem B.1 to the general setting (Theorem 1.6). It is postponed to this section due to the proof is essentially a result about Markov Chain.
* In Section J, we provided some standard result for decay of Markov Chain.

## Appendix A Additional Notations and Tools

Let us begin with the proof of Remark 1.15 which shows \(\mathcal{A}_{k}\) contains all non-empty subsets of \(L\) of size \(\leq k\):

Proof of Remark 1.15.: The proof will be carried out by induction on \(k\). The base case with \(k=1\) follows from the definition \(\mathcal{A}_{1}:=\{\{v\}\,:\,v\in L\}\). Suppose the claim holds up to some positive integer \(k\). Let \(\emptyset\neq S\subseteq L\) of size \(|S|\leq k+1\). If \(|S|\leq k\), then \(S\in\mathcal{A}_{k}\subseteq\mathcal{B}(\mathcal{A}_{k})=\mathcal{A}_{k+1}\). In the case \(|S|=k+1\geq 2\), we first observe that \(\rho(S)\) is not a leave. Consider the branch decomposition of \(S\) (See Definition 1.9):

\[S=\sqcup_{i\in I(S)}S_{i}.\]

Because \(|I(S)|>1\), for each \(i\in I(S)\) we have \(|S_{i}|<|S|=k+1\). Therefore, \(S_{i}\in\mathcal{A}_{k}\) for \(i\in I(S)\), which in turn implies \(S\in\mathcal{B}(\mathcal{A}_{k})=\mathcal{A}_{k+1}\). Therefore, the claim follows.

Now, to show the second statement. For every node \(w\), let \(S_{w}=\{v\in L\,:\,v\leq w\}\). Observe that if \(w\) is \(k-1\) layers above \(L\), then \(S_{w}\) are the \((k-1)\)th descendants of \(w\), which has size \(|S_{w}|=d^{k-1}\).

We **claim** that for \(w\) which are \(k-1\) layers above \(L\), then \(S_{w}\in\mathcal{A}_{k}\). Let us prove the claim by induction. First, it is clear that for \(w\in L\), \(S_{w}=\{w\}\in\mathcal{A}_{1}\). Suppose the statement holds up to \(k\). Take any \(w\) which is \(k\) layer above \(L\). Then, the branch decomposition of \(S_{w}=\sqcup_{i\in[d]}S_{w_{i}}\). With each \(w_{i}\) is \(k-1\) layer above \(L\), we have \(S_{w_{i}}\in\mathcal{A}_{k}\). Hence, \(S_{w}\in\mathcal{A}_{k+1}\) due to \(\mathcal{A}_{k+1}=\mathcal{B}(\mathcal{A}_{k})\).

### Additional notations

For any positive integer \(n\), let \([n]\) denote the set of positive integers from 1 to \(n\), inclusive: \([n]=1,2,\ldots,n\). For integers \(a\) and \(\hat{b}\) where \(a<b\), let \([a,\hat{b}]=\{a,a+1,\ldots,b\}\).

**Additional Notation for Trees**

For \(u\in T\), we define

\[L_{k}(u)\]

to be the set of \(k\)th descendants of \(u\). For brevity, let \(L_{k}:=L_{k}(\rho)\). Further, for \(h\in[0,\mathrm{h}(u)]\), let

\[D_{h}(u)=\{v\in T\,:\,v\leq u\text{ and }\mathrm{h}(v)=h\},\]

namely the set of descendants of \(u\) which has height \(h\). Observe that

\[D_{k}(u)=L_{\mathrm{h}(u)-k}(u).\]\[T_{u}\]

be the induced subgraph of \(T\) with vertex set \(\{v\in T\,:\,v\leq u\}\) and \(L_{u}=T_{u}\cap L\). It is worth noting that \(T_{u}\) can be seen as a rooted tree with root \(u\) and \(\mathrm{h}(u)\) layers, and the \(\mathrm{h}(u)\)th layer is \(L_{u}\).

**Additional Notation for collection \(\mathcal{A}\subseteq\mathbf{2}^{L}\setminus\{\emptyset\}\)**

**Definition A.1**.: _For a given collection of of subsets \(\mathcal{A}\subseteq\mathbf{2}^{L}\setminus\{\emptyset\}\), we define the following subcollections: For each \(u\in T\), let \(\mathcal{A}_{u}:=\{S\in\mathcal{A}\,:\,\rho(S)=u\}\), \(\mathcal{A}_{\leq u}:=\{S\in\mathcal{A}\,:\,\rho(S)\leq u\}\), and \(\mathcal{A}_{<u}:=\{S\in\mathcal{A}\,:\,\rho(S)<u\}\)._

**Notation for Conditional Expectation**

**Definition A.2**.: _Recall that an antichain \(U\subseteq T\) is a collection of vertices such that no two vertices in \(U\) are comparable under the \(\leq\) relation. For \(x\in[q]^{T}\), we can decompose \(x\) in the form_

\[x=(x_{<U},x_{U},x_{\not\in U}),\]

_where_

\[x_{<U}=(x_{v}\,:\,\exists u\in U\text{ s.t. }v\leq u),\text{ and }x_{\not\in U}=(x_{v}\,:\,\forall u\in U,v\not \leq u).\]

_(See Figure 5 for an illustration.)_

**Definition A.3**.: _[Conditional Expectation] For each antichain \(U\subseteq T\) and \(f:[q]^{T}\to\mathbb{R}\) let_

\[(\mathbb{E}_{U}f)(x):=\mathbb{E}\Big{[}f(X)\,\Big{|}\,X_{U}=x_{U},\,X_{\not \in U}=x_{\not\in U}\Big{]}.\]

To rephrase it, the function \((\mathbb{E}_{U}f)(x)\) represents the expected value of \(f(X)\) condition on \(X_{v}=x_{v}\) for all vertices \(v\) that are not descendents of any \(u\in U\). In the case when \(U=\{u\}\), we will abuse the notation and denote \(\mathbb{E}_{u}\) as \(\mathbb{E}_{\{u\}}\) for \(u\in T\). Futher, for any \(\mathrm{h}\in[0,\ell]\), we set

\[(\mathbb{E}_{\mathrm{h}}f)(x):=\mathbb{E}\Big{[}f(X)\,\Big{|}\,\forall v\in T \,\text{ with }\mathrm{h}(v)\geq\mathrm{h},\,X_{v}=x_{v}\Big{]}.\]

**Remark A.4** (Conditional Expectation and the Markov Property).: Suppose \(f\) is a function of \(x_{L}\) and \(U\) an antichain of \(T\). Let

\[\tilde{L}=\{v\in L\,:\,\exists u\in U\text{ s.t. }v\leq u\}\text{ and }L^{ \prime}=L\setminus\tilde{L}.\]

We remark that by the Markov Property,

\[\mathbb{E}_{U}f(x)=\mathbb{E}_{U}f(x_{U},x_{L^{\prime}})\]

is a function of \(x_{U}\) and \(x_{L^{\prime}}\). To see that, by the Markov Property, \(x_{<U}\) and \(x_{\not\in U}\) are independent conditioned on \(x_{U}\). Thus,

\[(\mathbb{E}_{U}f)(x_{U},x_{\not\in U})= \mathbb{E}\Big{[}f(X_{\tilde{L}},X_{L^{\prime}})\,\Big{|}\,X_{U}=x _{U},X_{\not\in U}=x_{\not\in U}\Big{]}\] \[= \mathbb{E}\Big{[}f(X_{\tilde{L}},x_{L^{\prime}})\,\Big{|}\,X_{U}= x_{U}\Big{]},\]

which implies \(\mathbb{E}_{U}f\) is a function of \(x_{U}\) and \(x_{L^{\prime}}\). (See Figure 6 for an illustration.)

Figure 5: In this figure, the purple dots represent the set \(U\). The yellow dots represent the vertices corresponding to the variables \(x_{<U}\), and the green dots represent the vertices corresponding to the variables \(x_{\not\in U}\).

### Basis Functions on \([q]^{L}\) and some decay properties

The following lemma is a well-known statement from Markov-Chain. Let us formulate it using the Broadcasting Process. (For completeness, we include a proof of this lemma in Section J.)

**Lemma A.5**.: _Suppose \(M\) is irreducible and aperiodic, then there exists \(C=C(M)>1\) so that the following hold: For any \(u\in T\) and \(k\in\mathbb{N}\) so that \(\mathfrak{p}^{k}(u)=\underset{k}{\underbrace{\mathfrak{p}\circ\mathfrak{p} \circ\ldots\mathfrak{p}}}(u)\), the \(k\)th ancester of \(u\), exists. For every function \(a\) with variable \(x_{u}\),_

\[\mathrm{Var}\Big{[}(\mathbb{E}_{\mathfrak{p}^{k}(u)}a)(X_{\mathfrak{p}^{k}(u)} )\Big{]}\leq Ck^{2q}\lambda^{2k}\mathrm{Var}\big{[}a(X_{u})\big{]}\] (8)

_and_

\[C^{-1}\left(\max_{\theta\in[q]}\big{|}a(\theta)-\mathbb{E}a(X_{u})\big{|} \right)^{2}\leq\mathrm{Var}_{Y\sim\pi}a(Y)\leq C\left(\max_{\theta\in[q]} \big{|}a(\theta)-\mathbb{E}a(X_{u})\big{|}\right)^{2}.\] (9)

_And from the above two inequalities, adjusting the constant \(C\) if necessary, we also have_

\[\max_{\theta\in[q]}\big{|}(\mathbb{E}_{\mathfrak{p}^{k}(u)}a)(\theta)- \mathbb{E}a(X_{u})\big{|}\leq Ck^{q}\lambda^{k}\max_{\theta\in[q]}\big{|}a( \theta)-\mathbb{E}a(X_{u})\big{|}.\] (10)

In this paper, we will fix a basis for the space of functions from \([q]\) to \(\mathbb{R}\) for a Markov chain \(M\).

**Definition A.6**.: _For a given \(q\times q\) ergodic and irreducible transition matrix \(M\), we_ **fix** _a basis \(\{\phi_{i}\}_{i\in[0,q-1]}\) for the space of functions from \([q]\) to \(\mathbb{R}\) such that \(\phi_{0}\) is the constant function \(1\) and \(\phi_{i}\) for \(i\in[q-1]\) are functions such that_

\[\mathbb{E}_{Y\sim\pi}\phi_{i}(Y)=0\quad\text{and}\quad\mathbb{E}_{Y\sim\pi} \phi_{1}^{2}(Y)=1.\]

**Definition A.7**.: _Suppose a given \(\ell\) layer rooted tree \(T\) and \(q\times q\) transition matrix described in Lemma A.5 have been given. For \(\sigma\in[0,q-1]^{L}\), let_

\[S(\sigma)=\{v:\sigma(v)\neq 0\}\subseteq L,\]

_the set of vertices in \(L\) in which \(\sigma\) is non-zero. Let \(|\sigma|=|S(\sigma)|\). When \(|\sigma|\geq 1\), we define_

\[\rho(\sigma)=\rho(S(\sigma)).\]

_And when \(|\sigma|\geq 2\), we define_

\[I(\sigma):=I(S(\sigma)).\]

_Further, let_

\[\phi_{\sigma}(x):=\prod_{v\in L}\phi_{\sigma(v)}(x_{v})=\prod_{v\in S(\sigma)} \phi_{\sigma(v)}(x_{v}).\]

_and_

\[\tilde{\phi}_{\sigma}(x):=\phi_{\sigma}(x)-\mathbb{E}\phi_{\sigma}(X).\]

**Remark A.8**.: We remark that \(\phi_{\sigma}(x)\) is a function with variables \(x_{S(\sigma)}\).

The fact that \(\phi_{0},\phi_{1},\ldots,\phi_{q-1}\) forms a basis implies that:

Figure 6: The purple dots represent the set \(U\). In the left figure, the yellow dots represent the vertices corresponding to the variables of \(f\). In the right figure, the yellow dots represent the vertices corresponding to the variable of \(\mathbb{E}_{U}f\).

**Fact A.9**.: _Every function \(f:[q]^{U}\to\mathbb{R}\) can be expressed uniquely in the form_

\[f(x)=\sum_{\sigma\,:\,S(\sigma)\subseteq U}c_{\sigma}\phi_{\sigma}(x).\] (11)

**Remark A.10**.: With the above representation, the **Efron-Stein degree** of function \(f\) equals to the largest magnitude of \(|\sigma|\) among those \(\sigma\) such that \(c_{\sigma}\neq 0\).

**Definition A.11**.: _Given a tree \(T\) and a \(q\times q\) ergodic transition matrix \(M\), let \(\{\phi_{i}\}_{i\in[q-1]}\) be the functions described in Lemma A.5. For a collection of subsets \(\mathcal{A}\subseteq\mathbf{2}^{L}\backslash\{\emptyset\}\), let_

\[\mathcal{F}(\mathcal{A}):=\{\sigma\in[0,q-1]^{L}\,:\,S(\sigma)\in\mathcal{A}\}.\] (12)

_For any \(\sigma\in\mathcal{F}(\mathcal{A})\) with \(|\sigma|>1\), let_

\[\psi_{\sigma}(x):=\prod_{i\in I(\sigma)}\tilde{\phi}_{P_{i}\sigma}(x),\] (13)

_where, for each \(i\in I(\sigma)\), \(P_{i}\sigma\) is the restriction of \(\sigma\) to \(S(\sigma)_{i}\):_

\[(P_{i}\sigma)(v)=\sigma(v)\mathbf{1}(v\in S(\sigma)_{i})\text{ for }v\in L.\]

## Appendix B The overall inductive argument

Here we present the version of the theorem with additional assumption on the transition matrix \(M\) that

\[c_{M}:=\min_{i,j\in[q]}M_{ij}>0.\] (14)

**Theorem B.1**.: _Given the rooted tree \(T\) and the transition matrix \(M\) described in Theorem 1.6, and under the additional assumption that \(c_{M}=\min_{i,j\in[q]}M_{ij}>0\), there exists \(c>0\) dependent on \(M\) and \(d\lambda^{2}\) (and implicitly on \(c_{M}\) as well) so that the following holds: For any function \(f\) of the leaves with_ **fractal capacity**_\(\leq c\frac{\log(dR)}{\log(dR)}\),_

\[\mathrm{Var}(\mathbb{E}\big{[}f(X_{L})\,\big{|}\,X_{\rho}\big{]})\leq(\max\{d \lambda^{2},\lambda\})^{\ell/4}\mathrm{Var}(f(X_{L})).\]

We will first derive the version mentioned above, as it substantially reduces the technical complexity without compromising the structural integrity of the proof in the general setting where \(c_{M}\) might be \(0\).

The proof of Theorem 1.6 will be carried out by induction on \(\mathcal{A}_{k}\)-polynomials. Let us introduce the necessary notations to outline this induction process.

**Definition B.2**.: _Let \(\varepsilon>0\) be the constant such that_

\[\max\{d\lambda^{2},\lambda\}=\exp(-1.1\varepsilon).\]

The constant \(\varepsilon\) is introduced to improve the readability of the paper. Intuitively, we aim to define \(d\lambda^{2}=\exp(-\varepsilon)\), but we relax this definition slightly so that inequalities like the following hold when \(\ell\) is sufficiently large:

\[\text{poly}(\ell)(d\lambda^{2})^{\ell}\leq\exp(-\varepsilon\ell).\]

**Assumption B.3**.: We say that \(\mathcal{A}\) satisfies assumption B.3 with parameters \((\mathrm{h}^{*},c^{*})\) where \(\mathrm{h}^{*}>0\) and \(0<c^{*}<1\), if

\[\mathcal{A}_{1}\subseteq\mathcal{A}\subseteq\mathbf{2}^{L}\backslash\{\emptyset\}\]

is **closed under decomposition**, and moreover,

1. For any \(v\in T\) with \(\mathrm{h}(v)\geq\mathrm{h}^{*}\) and a \(\mathcal{A}_{\leq v}\)-polynomial \(f\), \[\mathrm{Var}\big{[}(\mathbb{E}_{v}f)(X)\big{]}\leq\exp\big{(}-\varepsilon( \mathrm{h}(v)-\mathrm{h}^{*}\big{)}\big{)}\mathrm{Var}\big{[}f(X)\big{]}.\] (15)2. For any \(v\in T\setminus\{\rho\}\) with \(\mathrm{h}(v)\geq\mathrm{h}^{*}\) and a \(\mathcal{A}_{\leq v}\)-polynomial \(f\) with \(\mathbb{E}f(X)=0\), \[c^{*}\mathbb{E}\left[(\mathbb{E}_{v}f^{2})(X_{v})\right]\leq\mathbb{E}[( \mathbb{E}_{v}f^{2})(X_{v})\,|\,X_{\mathfrak{p}(v)}=\theta]\leq\frac{1}{c^{*}} \mathbb{E}\left[(\mathbb{E}_{v}f^{2})(X_{v})\right],\] (16) for all \(\theta\in[q]\).

The inequality (15) bears a resemblance to the inequality we aim to prove in Theorem B.1. The second inequality, (16), will later be seen as a crucial step proving the inductive phase of our proof. Indeed, in the case where \(c_{M}>0\), the condition (16) can be easily satisfied by appropriately choosing \(c^{*}\).

**Lemma B.4**.: _For any given \(\mathcal{A}_{1}\subseteq\mathcal{A}\subseteq\mathbf{2}^{L}\setminus\{\emptyset\}\) which is closed under decomposition. If it satisfies (15) with a given parameter \(\mathrm{h}^{*}\) and \(c_{M}>0\), then \(\mathcal{A}\) satisfies Assumption B.3 with parameter \(\mathrm{h}^{*}\) and \(c^{*}:=\min\left\{c_{M},\frac{1}{\min_{j}\pi(j)}\right\}>0\)._

_In other words, we can choose \(c^{*}\) with_ **no dependence on either \(\mathrm{h}^{*}\) or \(\mathcal{A}\)**_._

Proof.: Consider an arbitrary function \(f\) with variables \((x_{u}\,:\,u\leq v)\) for some \(v\in T\backslash\{\rho\}\).

Let \(g(x):=(\mathbb{E}_{v}f^{2})(x)\). By the Markov Property, \((\mathbb{E}_{v}f^{2})(x)\) is a function of \(x_{v}\), which in turn implies \(g(x)=g(x_{v})\). Now, for any \(\theta\in[q]\), fix an index \(j_{0}\in[q]\) such that \(g(j_{0})\geq\mathbb{E}g(X_{v})\). Relying on \(g\) is a non-negative function,

\[\mathbb{E}[g(X_{v})\,|\,X_{\mathfrak{p}(v)}=\theta]=\sum_{j\in[q]}M_{\theta j }g(j)\geq M_{\theta j_{0}}g(j_{0})\geq c_{M}\mathbb{E}g(X_{j}).\]

By unraveling the definition of \(g\), we can satisfy the first inequality of (16) as long as \(c^{*}<c_{M}\). The proof for the second inequality follows a similar logic, using the condition \(c^{*}\leq\frac{1}{\min_{j}\pi(j)}\) and the trival inequality \(\max_{i,j}M_{ij}\leq 1\). 

Given this notation, the proof of Theorem B.1 proceeds by induction, with the base case and inductive articulated in the subsequent two statements.

**Proposition B.5**.: _Given the rooted tree \(T\) and the transition matrix \(M\) described in Theorem 1.6, and under the additional assumption that \(c_{M}=\min_{i,j\in[q]}M_{ij}>0\). There exists \(C=C(M,\varepsilon)\geq 1\) so that the following holds:_

_Fix \(\rho^{\prime}\in T\) and \(0\leq m\leq\mathrm{h}(\rho^{\prime})\), if \(f(x)\) is a degree 1 polynomials of variables \((x_{v}\,:\,v\in D_{m}(\rho^{\prime}))\), then_

\[\mathrm{Var}\big{[}(\mathbb{E}_{\rho^{\prime}}f)(X)\big{]}\leq\exp\Big{(}- \varepsilon\big{(}\mathrm{h}(\rho^{\prime})-m-C(\log(R)+1)\big{)}\Big{)} \mathrm{Var}\big{[}f(X)\big{]}.\] (17)

**Theorem B.6**.: _Given the rooted tree \(T\) and the transition matrix \(M\) described in Theorem 1.6, and under the additional assumption that \(c_{M}=\min_{i,j\in[q]}M_{ij}>0\). Suppose \(\mathcal{A}\) is a collection of subsets satisfying Assumption B.3 with parameters \((\mathrm{h}^{*},\,c^{*})\). Then, there exists \(C=C(M,d,c^{*})\geq 1\) such that \(\mathcal{B}=\mathcal{B}(\mathcal{A})\) satisfies Assumption B.3 with parameters \(\big{(}\mathrm{h}^{*}+C(\log(R)+1),\,c^{*}\big{)}\)._

Let us derive the proof of Theorem B.1 based on the above two statements.

Proof of Theorem b.1.: We apply Proposition B.5 and Lemma B.4 to get \(\mathcal{A}_{1}\) satisfies Assumption B.3 with parameter \(\mathrm{h}^{*}=C_{B.5}(\log(R)+1)\), where \(C_{B.5}=C(M,d)\) is the constant introduced in the Proposition and

\[c^{*}=\min\Big{\{}c_{M},\frac{1}{\min_{j}\pi(j)}\Big{\}}>0.\]

Then, by applying Theorem B.6 inductively on the chain \(\mathcal{A}_{k}\), we can conclude that \(\mathcal{A}_{k}\) satisfies Assumption B.3 with parameter \(\mathrm{h}^{*}=C(\log(R)+1)k\) and the same \(c^{*}\) described above, provided that \(C=C(M,d,c^{*})\) is the maximum of the constants \(C\) described in Proposition B.5 and Theorem B.6. In other words, for any \(\mathcal{A}_{k}\)-polynomial \(f\),

\[\mathrm{Var}\big{[}(\mathbb{E}_{\rho}f)(X)\big{]}\leq\exp\big{(}-\varepsilon( \ell-C(\log(R)+1)k)\big{)}\mathrm{Var}\big{[}f(X)\big{]}.\]

The theorem follows by choosing \(k=\frac{1}{2C(\log(R)+1)}\ell\).

Variance Decomposition and Variance Estimate for degree 1 polynomials

To describe the goal of this section, let us begin with the variance decomposition of degree 1 polynomials in a slight generalized form. Essentially, the following statement is a direct consequence of the conditional variance formula. We will state the main results first and provide the proof later in this section.

**Lemma C.1**.: _Fix \(\rho^{\prime}\in T\) and \(0\leq k\leq\operatorname{h}(\rho^{\prime})\), consider a function \(g:[q]^{T}\to\mathbb{R}\) of the form_

\[g(x)=\sum_{v\in D_{k}(\rho^{\prime})}g_{v}(x)\quad\text{where}\quad g_{v}(x)=g _{v}(x_{\leq v}).\]

_For \(w\in T_{\rho^{\prime}}\backslash\{\rho^{\prime}\}\) with \(\operatorname{h}(w)\geq k+1\), let_

\[g_{w}(x):=\sum_{v\in D_{k}(w)}g_{v}(x).\]

_Then,_

\[\operatorname{Var}[g(X)]= \operatorname{Var}\bigl{[}(\mathbb{E}_{\rho^{\prime}}g)(X_{\rho^ {\prime}})\bigr{]}+\sum_{w\in T_{\rho^{\prime}}\backslash\{\rho^{\prime}\}: \operatorname{h}(w)\geq k}\mathbb{E}\text{Var}\bigl{[}(\mathbb{E}_{w}g_{w})(X_ {w})\,\big{|}\,X_{\mathfrak{p}(w)}\bigr{]}\] \[+\sum_{v\in D_{k}(\rho^{\prime})}\mathbb{E}\text{Var}\bigl{[}g_{ v}(X)\,\big{|}\,X_{v}\bigr{]}.\]

Our goal is to show that when \(d\lambda^{2}<1\), \(\operatorname{Var}[g(X)]\) is of the same order as \(\sum_{v\in D_{k}(\rho^{\prime})}\operatorname{Var}[g_{v}(X)]\), based on the above lemma.

**Lemma C.2**.: _Suppose the transition matrix \(M\) satisfies \(d\lambda^{2}<1\) and the tree \(T\) has growth factor \(R\). Then, there exists a constant \(C=C(M,\varepsilon)\geq 1\) so that the following holds. Let \(\rho^{\prime}\in T\), \(l^{\prime}:=\operatorname{h}(\rho^{\prime})\), and \(k\in[0,l^{\prime}]\). Consider a function of the form \(g(x)=\sum_{v\in D_{k}(\rho^{\prime})}g_{v}(x_{v})\). Then,_

\[\operatorname{Var}[g(X)]\leq CR\sum_{v\in D_{k}(\rho^{\prime})}\operatorname{ Var}[g_{v}(X_{v})].\] (18)

The opposite bound does not depend on \(d\lambda^{2}\leq 1\). However, the proof in the general case where \(c_{M}=0\) is not straight-forward. We state it in full generality but will defer the general proof and prove it here in the simpler case where \(c_{M}>0\).

**Proposition C.3**.: _There exists a constant \(C=C(M,d)\geq 1\) so that the following holds. Let \(\rho^{\prime}\in T\), and \(k\in[0,\operatorname{h}(\rho^{\prime})]\). For any degree-1 function \(g\) with variables \((x_{v}\,:\,v\in D_{k}(\rho^{\prime}))\). There exists functions \(g_{v}(x)=g_{v}(x_{v})\) for \(v\in D_{k}(\rho^{\prime})\) so that the following holds:_

1. \(g(X)=\sum_{v\in D_{k}(\rho^{\prime})}g_{v}(X_{u})\) _almost surely. (They may not agree as functions from_ \([q]^{T}\) _to_ \(\mathbb{R}\)_.)_
2. _For any_ \(u\in T_{\rho^{\prime}}\) _with_ \(\operatorname{h}(u)\geq k\)_,_ \[\sum_{v\in D_{k}(u)}\operatorname{Var}[g_{v}(X_{v})]\leq CR^{3}\text{Var} \bigl{[}\sum_{v\in D_{k}(u)}g_{v}(X_{v})\bigr{]}.\] (19) _In particular, taking_ \(u=\rho^{\prime}\) _we have_ \[\sum_{v\in D_{k}(\rho^{\prime})}\operatorname{Var}[g_{v}(X_{v})]\leq CR^{3} \text{Var}[g(X)].\] (20)

We postpone the proof of Proposition in full generality in Appendix I, due to the technical complexity of the proof and the fact that the proof is about properties of a Markov Chain. Instead, a statement of the proposition and its proof in the case where \(c_{M}>0\) is provided in this section.

Now, the purpose of this section is twofold.

* First, it is the derivation of the variance related estimates: Lemma C.1, Lemma C.2, and Proposition C.3 with the additional assumption that \(c_{M}>0\). Additionally, we summarise the estimates into a single statements, as stated in Lemma C.7.
* Second, it is the derivation of the base case of the induction, Proposition B.5.

### Variance Decomposition and Estimates

Before we proceed to the proof of Lemma C.1, let us remark on the following consequence of the lemma.

**Remark C.4**.: For any \(g\) described in Lemma C.1, if we define \(h(x):=(\mathbb{E}_{k}g)(x)=\sum_{v\in D_{k}(\rho^{\prime})}(\mathbb{E}_{v}g_{v})( x_{v})\) and \(h_{v}(x):=(\mathbb{E}_{v}g_{v})(x_{v})\), then by applying the lemma to both \(g\) and \(h\), we conclude that

\[\mathrm{Var}[g(X)]=\mathrm{Var}[(\mathbb{E}_{k}g)(X)]+\sum_{v\in D_{k}(\rho^{ \prime})}\mathbb{E}\mathrm{Var}\big{[}g_{v}(X)\,\big{|}\,X_{v}\big{]}.\]

Proof of Lemma C.1.: First, for \(v\in T_{\rho^{\prime}}\backslash\{\rho^{\prime}\}\) with \(\mathrm{h}(v)\geq k\),

\[\tilde{g}_{v}(x):=g_{v}(x)-\mathbb{E}g_{v}(X).\]

Notice the following holds:

\[\tilde{g}(x):=g(x)-\mathbb{E}g(X)=\sum_{v\in D_{k}(\rho^{\prime})}\tilde{g}_{v }(x).\]

Let us start decomposing the variance of \(g\).

\[\mathrm{Var}[g(X)]=\mathrm{Var}[\tilde{g}(X)] =\sum_{v,v^{\prime}\in D_{k}(\rho^{\prime})}\mathbb{E}\big{[} \tilde{g}_{v}(X)\tilde{g}_{v^{\prime}}(X)\big{]}\] \[=\sum_{w\in T_{\rho^{\prime}}\colon\mathrm{h}(w)>k}\sum_{v,v^{ \prime}\in D_{k}(\rho^{\prime})\,:\,\rho(v,v^{\prime})=w}\mathbb{E}\big{[} \tilde{g}_{v}(X)\tilde{g}_{v^{\prime}}(X)\big{]}+\sum_{v\in D_{k}(\rho^{ \prime})}\mathbb{E}\big{[}\tilde{g}_{v}^{2}(X)\big{]}\]

By the Markov Property and rearrangement of the terms, for each \(w\in T_{\rho^{\prime}}\) with \(\mathrm{h}(w)>k\),

\[\sum_{v,v^{\prime}\in D_{k}(\rho^{\prime})\,:\,\rho(v,v^{\prime}) =w}\mathbb{E}\big{[}\tilde{g}_{v}(X)\tilde{g}_{v^{\prime}}(X)\big{]}\] \[= \sum_{v,v^{\prime}\in D_{k}(\rho^{\prime})\,:\,\rho(v,v^{\prime} )=w}\mathbb{E}\big{[}(\mathbb{E}_{w}\tilde{g}_{v})(X)(\mathbb{E}_{w}\tilde{g} _{v^{\prime}})(X)\big{]}\] \[= \sum_{v,v^{\prime}\in D_{k}(w)}\mathbb{E}\big{[}(\mathbb{E}_{w} \tilde{g}_{v})(X)(\mathbb{E}_{w}\tilde{g}_{v^{\prime}})(X)\big{]}-\sum_{v,v^{ \prime}\in D_{k}(w)\,:\,\rho(v,v^{\prime})<w}\mathbb{E}\big{[}(\mathbb{E}_{w} \tilde{g}_{v})(X)(\mathbb{E}_{w}\tilde{g}_{v^{\prime}})(X)\big{]}\] \[= \mathbb{E}\big{[}(\mathbb{E}_{w}\tilde{g}_{w})^{2}(X)\big{]}-\sum _{w^{\prime}\in\mathfrak{c}(w)}\mathbb{E}\big{[}(\mathbb{E}_{w}\tilde{g}_{w^{ \prime}})^{2}(X)\big{]}\]

Hence,

\[\mathrm{Var}[g(X)]\] \[= \sum_{w\in T_{\rho^{\prime}}\colon\mathrm{h}(w)>k}\Big{(} \mathbb{E}\big{[}(\mathbb{E}_{w}\tilde{g}_{w})^{2}(X)\big{]}-\sum_{w^{\prime} \in\mathfrak{c}(w)}\mathbb{E}\big{[}(\mathbb{E}_{w}\tilde{g}_{w^{\prime}})^{ 2}(X)\big{]}\Big{)}+\sum_{v\in D_{k}(\rho^{\prime})}\mathbb{E}\big{[}\tilde{g }_{v}^{2}(X)\big{]}\] \[= \mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}\tilde{g}_{\rho^{ \prime}})^{2}(X)\big{]}+\sum_{w\in T_{\rho^{\prime}}\backslash\{\rho^{\prime} \}\colon\mathrm{h}(w)>k}\mathbb{E}\big{[}(\mathbb{E}_{w}\tilde{g}_{w})^{2}(X)- (\mathbb{E}_{\mathbf{p}(w)}\tilde{g}_{w})^{2}(X)\big{]}\] \[+\sum_{v\in D_{k}(\rho^{\prime})}\Big{(}\mathbb{E}\big{[}\tilde{g }_{v}^{2}(X)\big{]}\underbrace{-\mathbb{E}\big{[}(\mathbb{E}_{v}\tilde{g}_{v}) ^{2}(X)\big{]}+\mathbb{E}\big{[}(\mathbb{E}_{v}\tilde{g}_{v})^{2}(X)\big{]}} -\mathbb{E}(\mathbb{E}_{\mathbf{p}(v)}\tilde{g}_{v})^{2}(X)\Big{)}\] \[= \mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}\tilde{g}_{\rho^{ \prime}})^{2}(X)\big{]}+\sum_{w\in T_{\rho^{\prime}}\backslash\{\rho^{\prime} \}\colon\mathrm{h}(w)\geq k}\mathbb{E}\mathrm{Var}\big{[}(\mathbb{E}_{w}g_{w})( X_{w})\,\big{|}\,X_{\mathbf{p}(w)}\big{]}.+\sum_{v\in D_{k}(\rho^{\prime})} \mathbb{E}\mathrm{Var}\big{[}g_{v}(X)\,\big{|}\,X_{v}\big{]}\]Next, let us show the proof of Lemma C.2.

Proof of Lemma c.2.: Let \(C_{0}=C_{0}(M,d)\) denote the constant introduced in the statement of the Lemma. Its precise value will be determined along the proof. Without lose of generality, we may assume both \(\mathbb{E}g(X)=0\) and \(\mathbb{E}g_{v}(X_{v})=0\) for \(v\in D_{k}(\rho^{\prime})\), and the variance of each function is simply its the second moment. For brevity, let \(\tau_{u}:=(\mathbb{E}(g_{u}(X))^{2})^{1/2}\) for \(u\in D_{k}(\rho^{\prime})\) and \(\ell^{\prime}:=\mathrm{h}(\rho^{\prime})\).

By (8) from Lemma A.5, for \(s\in[l^{\prime}-k]\),

\[\mathbb{E}\big{[}(\mathbb{E}_{\mathfrak{p}^{s}(u)}g_{u})^{2}(X_{ \mathfrak{p}^{s}(u)})\big{]}\leq C_{A.5}s^{2q}\lambda^{2s}\tau_{u}^{2},\] (21)

where \(C_{A.5}\geq 1\) is the constant introduced in the Lemma. In particular, if \(\rho(u,u^{\prime})=\mathfrak{p}^{s}(u)\) for \(u,u^{\prime}\in D_{k}(\rho^{\prime})\), then

\[|\mathbb{E}g_{u}(X)g_{u^{\prime}}(X)|= \big{|}\mathbb{E}\big{[}(\mathbb{E}_{\mathfrak{p}^{s}(u)}g_{u})( X_{\mathfrak{p}^{s}(u)})(\mathbb{E}_{\mathfrak{p}^{s}(u)}g_{u^{\prime}})(X_{ \mathfrak{p}^{s}(u)})\big{]}\big{|}\] \[\leq \big{(}\mathbb{E}\big{[}(\mathbb{E}_{\mathfrak{p}^{s}(u)}g_{u})^ {2}(X_{\mathfrak{p}^{s}(u)})\big{]}\big{)}^{1/2}\cdot\big{(}\mathbb{E}\big{[}( \mathbb{E}_{\mathfrak{p}^{s}(u)}g_{u^{\prime}})^{2}(X_{\mathfrak{p}^{s}(u)}) \big{]}\big{)}^{1/2}\] \[\leq C_{A.5}s^{2q}\lambda^{2s}\tau_{u}\tau_{u^{\prime}}.\]

Then,

\[\mathbb{E}(g(X))^{2} \leq\sum_{u,u^{\prime}\in D_{k}(\rho^{\prime})}|\mathbb{E}g_{u}( X)g_{u^{\prime}}(X)|\] \[=\sum_{s\in[l^{\prime}-k]}\sum_{v\in D_{k+s}(\rho^{\prime})}\sum _{u,u^{\prime}}C_{A.5}s^{2q}\lambda^{2s}\tau_{u}\tau_{u^{\prime}},\]

where the sum \(\sum_{u,u^{\prime}}\) is taken over all ordered pairs \((u,u^{\prime})\) with \(u,u^{\prime}\in D_{k}(v)\) with \(\rho(u,u^{\prime})=v\). By relaxing the constraint of the summation we have

\[\mathbb{E}(g(X))^{2} \leq\sum_{s\in[l^{\prime}-k]}\sum_{v\in D_{k+s}(\rho^{\prime})} \sum_{u,u^{\prime}\in D_{k}(v)}C_{A.5}s^{2q}\lambda^{2s}\tau_{u}\tau_{u^{ \prime}}\] \[=\sum_{s\in[l^{\prime}-k]}\sum_{v\in D_{k+s}(\rho^{\prime})}C_{A. 5}s^{2q}\lambda^{2s}\Big{(}\sum_{u\in D_{k}(v)}\tau_{u}\Big{)}^{2}\] \[\leq\sum_{s\in[l^{\prime}-k]}\sum_{v\in D_{k+s}(\rho^{\prime})}C_ {A.5}s^{2q}\lambda^{2s}Rd^{s}\sum_{u\in D_{k}(v)}\tau_{u}^{2}\] \[= \Big{(}\sum_{s\in[l^{\prime}-k]}C_{A.5}s^{2q}\lambda^{2s}Rd^{s} \Big{)}\sum_{u\in D_{k}(\rho^{\prime})}\tau_{u}^{2}.\]

Next,

\[\sum_{s=1}^{\infty}C_{A.5}s^{2q}\lambda^{2s}Rd^{s}\leq\sum_{s=1}^{\infty}RC_{ A.5}s^{2q}\exp(-1.1\varepsilon s):=C_{0}R.\]

Hence, \(C_{0}\) depends on \(\varepsilon\), \(q\), and \(C_{A.5}\). It is a constant which is determined by \(M\) and \(\varepsilon\).

Let us formulate Proposition C.3 under the additional assumption that \(c_{M}=\min_{i,j\in[q]}M_{ij}>0\). Indeed, in this case, the bound does not depend on \(R\).

**Proposition C.5**.: _Suppose the transition matrix \(M\) satisfies \(c_{M}>0\). There exists a constant \(C=C(M,\varepsilon)\geq 1\) so that the following holds: Let \(\rho^{\prime}\in T\), \(l^{\prime}:=\mathrm{h}(\rho^{\prime})\), and \(k\in[0,l^{\prime}]\). For any function \(g=[q]^{T}\to\mathbb{R}\) of the form_

\[g(x)=\sum_{v\in D_{k}(\rho^{\prime})}g_{v}(x_{v}).\]_The following holds: For any \(u\in T_{\rho^{\prime}}\) with \(\mathrm{h}(u)\geq k\),_

\[\sum_{v\in D_{k}(u)}\mathrm{Var}[g_{v}(X_{v})]\leq C\mathrm{Var}\big{[}\sum_{v\in D _{k}(u)}g_{v}(X_{v})\big{]}.\] (22)

_In particular, taking \(u=\rho^{\prime}\) we have_

\[\sum_{v\in D_{k}(\rho^{\prime})}\mathrm{Var}[g_{v}(X_{v})]\leq C\mathrm{Var}[g (X)].\] (23)

The proof of the Proposition relies on the following immediate consequence of \(c_{M}>0\):

**Lemma C.6**.: _Suppose \(M\) is a \(q\times q\) ergodic transition matrix with \(c_{M}=\min_{i,j\in[q]}M_{ij}>0\). There exists \(C=C(M)\geq 1\) so that the following holds. For any \(u\in T\backslash\{\rho\}\) and a function \(h(x)=h(x_{u})\),_

\[\mathbb{E}\mathrm{Var}[h(X_{u})\,|\,X_{\mathfrak{p}(u)}]\geq\frac{1}{C(M)} \mathrm{Var}[h(X_{u})].\]

Proof.: Let \(\theta_{1}=\mathrm{argmin}_{\theta\in[q]}h(\theta)\) and \(\theta_{2}=\mathrm{argmax}_{\theta\in[q]}h(\theta)\). (In the case of a tie, we may choose any of the minimizers or maximizers.) First, we have

\[\mathrm{Var}[h(X_{u})]\leq(h(\theta_{2})-h(\theta_{1}))^{2}.\]

Next, for any \(\beta\in[q]\), we have

\[\max\big{\{}|\mathbb{E}[h(X_{u})\,|\,X_{\mathfrak{p}(u)}=\beta]-h(\theta_{1}) |,\,|\mathbb{E}[h(X_{u})\,|\,X_{\mathfrak{p}(u)}=\beta]-h(\theta_{2})|\big{\}} \geq\frac{1}{2}|h(\theta_{2})-h(\theta_{1})|.\]

Let \(i\in\{1,2\}\) be the index such that \(|\mathbb{E}[h(X_{u})\,|\,X_{\mathfrak{p}(u)}=\beta]-h(\theta_{i})|\geq\frac{1 }{2}|h(\theta_{2})-h(\theta_{1})|\), and we will use this together with \(c_{M}>0\) to give a lower bound on the conditional variance:

\[\mathrm{Var}[h(X_{u})\,|\,X_{\mathfrak{p}(u)}=\beta]\geq \big{(}\mathbb{E}[h(X_{u})\,|\,X_{\mathfrak{p}(u)}=\beta]-h( \theta_{i})\big{)}^{2}\mathbb{P}\{X_{u}=\theta_{i}\,|\,X_{\mathfrak{p}(u)}=\beta\}\] \[\geq \frac{1}{4}(h(\theta_{2})-h(\theta_{1}))^{2}c_{M}.\]

Since it holds for every \(\beta\in[q]\), we conclude that

\[\mathbb{E}\mathrm{Var}[h(X_{u})\,|\,X_{\mathfrak{p}(u)}]\geq\frac{1}{4}(h( \theta_{2})-h(\theta_{1}))^{2}c_{M}\geq\frac{c_{M}}{4}\mathrm{Var}[h(X_{u})].\]

Proof of Proposition c.5.: We adapt the notation from Lemma C.1. For \(w\leq\rho^{\prime}\) with \(\mathrm{h}(w)>k\), let

\[g_{w}(x):=\sum_{u\in D_{k}(w)}g_{u}(x).\]

Now, we apply Lemma C.1 and Lemma C.6 to get

\[\mathrm{Var}[g(X)]= \mathrm{Var}\big{[}(\mathbb{E}_{\rho^{\prime}}g_{\rho^{\prime}}) (X_{\rho^{\prime}})\big{]}+\sum_{w\in T_{\rho^{\prime}}\backslash\{\rho^{ \prime}\}:\mathrm{h}(w)\geq k}\mathbb{E}\mathrm{Var}\big{[}(\mathbb{E}_{w}g_{w })(X_{w})\,\big{|}\,X_{\mathfrak{p}(w)}\big{]}\] \[+\sum_{v\in D_{k}(\rho^{\prime})}\mathbb{E}\mathrm{Var}\big{[}g_{v }(X)\,\big{|}\,X_{v}\big{]}\] \[\geq \sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\mathrm{Var}\big{[}( \mathbb{E}_{u}g_{u})(X_{u})\,\big{|}\,X_{\mathfrak{p}(u)}\big{]}\] \[\geq \frac{1}{C_{C.6}}\sum_{u\in D_{k}(\rho^{\prime})}\mathrm{Var}[g_{ u}(X_{u})],\]

where we used the fact that all the terms are non-negative, the first inequality is obtained by looking at the second terms for the summands with \(h(w)=k\) and \(C_{C.6}\) is the \(M\)-dependent constant introduced in Lemma C.6.

**Lemma C.7**.: _Suppose \(d\lambda^{2}<1\) and the growth factor is at most \(R\). There exists a constant \(C=C(M,d)\geq 1\) so that the following holds. Fix \(\rho^{\prime}\in T\) and \(0\leq m\leq\mathrm{h}(\rho^{\prime})\), if \(f_{m}(x)\) is a function in the form_

\[f_{m}(x)=\sum_{v\in D_{m}(\rho^{\prime})}f_{v}(x_{\leq v}).\]

_with_

\[\mathbb{E}f_{m}(X)=0.\]

_Then, there exists \(\bar{f}_{v}(x_{\leq v})\) for \(v\in D_{m}(\rho^{\prime})\) such that their sum \(\bar{f}_{m}(x)=\sum_{v\in D_{m}(\rho^{\prime})}\bar{f}_{v}(x_{\leq v})\) satisfies_

\[\bar{f}_{m}(X)=f_{m}(X)\]

_almost surely,_

_and for \(u\leq\rho^{\prime}\) with \(\mathrm{h}(u)\geq m\),_

\[\frac{1}{CR^{3}}\sum_{v\in D_{m}(u)}\mathbb{E}\bar{f}_{v}^{2}(X)\leq\mathbb{E }\Big{(}\sum_{v\in D_{m}(u)}\bar{f}_{v}(X)\Big{)}^{2}\leq CR\sum_{v\in D_{m}(u )}\mathbb{E}\bar{f}_{v}^{2}(X).\] (24)

The statement of the lemma using \(\bar{f}_{m}\) and \(\bar{f}_{b}\) covers also the case \(c_{M}=0\). We will prove the Lemma by using either Proposition C.3 or Proposition C.5 with the assumption \(c_{M}>0\). In the later case, it suffice to simply take \(f_{v}(x_{\leq v})=\bar{f}_{v}(x_{\leq v})\).

**Remark C.8**.: Note that the lemma implies the following: For any \(u\leq\rho^{\prime}\) with \(\mathrm{h}(u)\geq m\), let

\[\bar{f}_{m,u}(x):=\sum_{v\in D_{m}(u)}\bar{f}_{v}(x).\] (25)

Then, for any given \(m\leq k<k^{\prime}\leq\mathrm{h}(\rho^{\prime})\) and \(u\in D_{k^{\prime}}(\rho^{\prime})\), we have

\[\mathbb{E}\bar{f}_{m,u}^{2}(X)\leq CR\sum_{v\in D_{m}(u)}\mathbb{E}\bar{f}_{v} ^{2}(X)=CR\sum_{w\in D_{k}(u)}\sum_{v\in D_{m}(w)}\mathbb{E}\bar{f}_{v}^{2}(X) \leq C^{2}R^{4}\sum_{w\in D_{k}(u)}\bar{f}_{m,w}^{2}(X),\]

where the first inequality follows from the second inequality of (24) and the second inequality follows from the first inequality of (24).

Proof of Lemma c.7.: Let

\[h(x):=(\mathbb{E}_{m}f_{m})(x)=\sum_{v\in D_{m}(\rho^{\prime})}(\mathbb{E}_{v }f_{v})(x_{v}).\]

Note that \(h\) is a degree one function of the variables \((x_{v}\,:\,v\in D_{m}(\rho^{\prime}))\). Thus, we could apply Proposition C.3 to show the existence of 1-variable functions \(h_{v}(x_{v})\) for \(v\in D_{m}(\rho^{\prime})\) such that

\[h(X)=\sum_{v\in D_{m}(\rho^{\prime})}h_{v}(X_{v})\quad\text{almost surely}\] (26)

and for any \(u\in T(\rho^{\prime})\) with \(\mathrm{h}(u)\geq m\),

\[\sum_{v\in D_{m}(u)}\mathrm{Var}[h_{v}(X_{v})]\leq C_{C.3}R^{3}\mathrm{Var} \big{[}\sum_{v\in D_{m}(u)}h_{v}(X_{v})\big{]},\] (27)

where \(C_{C.3}\geq 1\) is the constant introduced in Proposition C.3.

Since \(\mathbb{E}h(X)=\mathbb{E}(\mathbb{E}_{m}f_{m})(X)=0\), we may also assume that

\[\mathbb{E}h_{v}(X)=0\]

for \(v\in D_{m}(\rho^{\prime})\), as a constant shift of the functions will not affect (26) and (27). Now, consider the following functions: For \(v\in D_{m}(\rho^{\prime})\), let

\[\bar{f}_{v}(x_{\leq v})=f_{v}(x_{\leq v})-\mathbb{E}f_{v}(X_{\leq v})+h_{v}(x_ {v})\]

and

\[\bar{f}_{m}(x)=\sum_{v\in D_{m}(\rho^{\prime})}\bar{f}_{v}(x).\]First, since \(\bar{f}_{v}(x_{\leq v})\) is defined as the sum of three terms with mean \(0\), we have \(\mathbb{E}\bar{f}_{v}(X_{\leq v})=0\).

Second,

\[\bar{f}_{m}(X)= \sum_{v\in D_{m}(\rho^{\prime})}\Big{(}f_{v}(X_{\leq v})-(\mathbb{ E}_{v}f_{v})(X_{v})+h_{v}(X_{v})\Big{)}\] \[= f_{m}(X)-h(X)+\sum_{v\in D_{m}(\rho^{\prime})}h_{v}(X_{v})\] \[\underset{\text{\small a.s.}}{=}f_{m}(X).\]

By Remark C.4,

\[\mathrm{Var}\big{[}\sum_{v\in D_{m}(u)}\bar{f}_{v}(X)\big{]}= \mathrm{Var}\Big{[}\Big{(}\mathbb{E}_{m}\sum_{v\in D_{m}(u)}\bar{ f}_{v}\big{)}(X)\Big{]}+\sum_{v\in D_{m}(u)}\mathbb{E}\mathrm{Var}[\bar{f}_{v}(X) \,|\,X_{v}]\] \[= \mathrm{Var}\Big{[}\sum_{v\in D_{m}(u)}(\mathbb{E}_{v}\bar{f}_{v })(X)\Big{]}+\sum_{v\in D_{m}(u)}\mathbb{E}\mathrm{Var}[f_{v}(X)-(\mathbb{E}_ {v}f_{v})(X_{v})+h_{v}(X_{v})\,|\,X_{v}]\] \[= \mathrm{Var}\Big{[}\sum_{v\in D_{m}(u)}h_{v}(X)\Big{]}+\sum_{v\in D _{m}(u)}\mathbb{E}\mathrm{Var}[f_{v}(X)\,|\,X_{v}]\] (28)

To estimate the lower bound, we rely on own choice of \(h_{v}\). By (27) we have

(28) \[\geq \frac{1}{C_{C.3}R^{3}}\sum_{v\in D_{m}(u)}\mathrm{Var}\big{[}h_{v} (X)\big{]}+\sum_{v\in D_{m}(u)}\mathbb{E}\mathrm{Var}[f_{v}(X)\,|\,X_{v}]\] \[= \frac{1}{C_{C.3}R^{3}}\sum_{v\in D_{m}(u)}\Big{(}\mathrm{Var} \big{[}(\mathbb{E}_{v}\bar{f}_{v})(X_{v})\big{]}+\mathbb{E}\mathrm{Var}[\bar{ f}_{v}(X)\,|\,X_{v}]\Big{)}\] \[= \frac{1}{C_{C.3}R^{3}}\sum_{v\in D_{m}(u)}\mathrm{Var}[\bar{f}_{v }(X)].\]

As for the upper bound, we can apply Lemma C.2 and repeat the same derivation as above to get

(28) \[\leq C_{C.2}R\sum_{v\in D_{m}(u)}\mathrm{Var}\big{[}h_{v}(X)\big{]}+ \sum_{v\in D_{m}(u)}\mathbb{E}\mathrm{Var}[f_{v}(X)\,|\,X_{v}]\] \[\leq C_{C.2}R\sum_{v\in D_{m}(u)}\mathrm{Var}[\bar{f}_{v}(X)].\]

Therefore, by taking the constant \(C\) stated in the lemma to be the maximum of \(C_{C.3}\) and \(C_{C.2}\), the proof follows.

### Proof of the base Case of Proposition B.5

We now prove the base case of Proposition B.5:

**Lemma C.9**.: _There exists a constant \(C=C(M,d)\geq 1\) so that the following holds. For any degree \(1\) function \(f\) with variables \((x_{u}:u\in D_{k}(\rho^{\prime}))\) for some \(\rho^{\prime}\in T\) with \(k\leq\mathrm{h}(\rho^{\prime})\),_

\[\mathrm{Var}\big{[}\mathbb{E}\big{[}f(X)\,\big{|}\,X_{\rho^{\prime}}\big{]} \big{]}\leq CR^{4}(\mathrm{h}^{\prime})^{2q}(d\lambda^{2})^{\mathrm{h}^{ \prime}}\mathrm{Var}[f(X)].\] (29)

_where \(\mathrm{h}^{\prime}=\mathrm{h}(\rho^{\prime})-k\)._

Proof.: Let \(f_{u}\) for \(u\in D_{k}(\rho^{\prime})\) be the functions from Proposition C.3 so that

\[f(X)=\sum_{u\in D_{k}(\rho^{\prime})}f_{u}(X_{u})\text{ almost surely}.\] (30)We can assume \(\mathbb{E}f(X)=0\) and \(\mathbb{E}f_{u}(X)=0\) for every \(u\in D_{k}(\rho^{\prime})\) without affecting (30). From Proposition C.3, we have

\[\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}[f_{u}^{2}(X)]\leq C_{C.3}R^{3} \mathbb{E}[f^{2}(X)],\]

where \(C_{C.3}\) denotes the constant \(C\) introduced in the Proposition.

We could apply Lemma A.5 to get

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f)^{2}(X_{\rho^{ \prime}})\big{]}\leq |D_{k}(\rho^{\prime})|\sum_{v\in D_{k}(\rho^{\prime})}\big{[}( \mathbb{E}_{\rho^{\prime}}f_{v})^{2}(X_{\rho^{\prime}})\big{]}\] \[\leq |D_{k}(\rho^{\prime})|C_{A.5}\mathrm{h}^{2q}\lambda^{2\mathrm{h}^ {\prime}}\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}[f_{u}^{2}(X)]\leq C_{C.3} C_{A.5}R^{4}(\mathrm{h}^{\prime})^{2q}(d\lambda^{2})^{\mathrm{h}^{\prime}} \mathbb{E}[f^{2}(X)]\]

where \(C_{A.5}\) denotes the constant \(C\) stated in the Lemma. \(\square\)

Proof of the Base Case Proposition b.5.: Given \(\rho^{\prime}\in T\) and \(0\leq m\leq\mathrm{h}(\rho^{\prime})\) described in the Proposition. Let \(\mathrm{h}^{\prime}=\mathrm{h}(\rho^{\prime})-m\). By Lemma C.9, any function \(f(x)=\sum_{v\in D_{m}(\rho^{\prime})}f_{v}(x_{v})\) satisfies

\[\mathrm{Var}\big{[}(\mathbb{E}_{\rho^{\prime}}f)(X)\big{]}\leq C_{C.9}R^{4}( \mathrm{h}^{\prime})^{q}(d\lambda^{2})^{\mathrm{h}^{\prime}}\mathrm{Var}[f(X)],\]

where \(C_{C.9}\) denotes the \(M\)-dependent constant introduced in the Lemma. With

\[C_{C.9}R^{4}(\mathrm{h}^{\prime})^{q}(d\lambda^{2})^{\mathrm{h}^{\prime}}\leq C _{C.9}R^{4}(\mathrm{h}^{\prime})^{q}\exp(-1.1\varepsilon\mathrm{h}^{\prime}) \leq\exp\big{(}-\varepsilon(\mathrm{h}^{\prime}-C_{1}(\log(R)+1))\big{)},\]

for some \(C_{1}\geq 1\) which depends on \(M,d\). \(\square\)

## Appendix D Decomposition of Polynomials

In this section we study the representation of functions in terms of \(\phi_{\sigma}\) and \(\psi_{\sigma}\). Roughly speaking \(\psi_{\sigma}\) are "more orthogonal" than the \(\phi_{\sigma}\). More formally we will show that under appropriate conditioning expections of \(\psi_{\sigma}\) factorize. Thus some of the effort in the proof and particularly in this section is devoted to relating the \(\phi\) and \(\psi\) representations and bounding moments of such representations.

**Lemma D.1**.: _Assuming \(d\lambda^{2}<1\) and growth factor of \(R\), there exists \(C=C(M,d)\geq 1\) so that the following holds. Let \(\mathcal{A}_{1}\subseteq\mathcal{B}\subseteq\mathbf{2}^{L}\backslash\{\emptyset\}\) be a collection of subsets which is closed under decomposition. Fix a positive integer \(k_{1}\) and \(\rho^{\prime}\in T\) with \(l^{\prime}:=\mathrm{h}(\rho^{\prime})>k_{1}\). For every function \(f\) of the form_

\[f(x)=\sum_{\sigma:\sigma\neq 0\in\mathcal{F}(\mathcal{B}_{\leq\rho^{\prime}})}c_{ \sigma}\phi_{\sigma}(x)\]

_with_

\[\mathbb{E}f(X)=0,\]

_here exists a decomposition of \(f\)_

\[f(X)=\sum_{u\leq\rho^{\prime}\,:\,\mathrm{h}(u)\geq k}\tilde{f}_{u}(X)\text{ almost surely,}\]

_where, for each \(u\leq\rho^{\prime}\) with \(\mathrm{h}(u)\geq k_{1}\), we have a function \(f_{u}(x)=f_{u}(x_{\leq u})\) and_

1. _For_ \(u\in T_{\rho^{\prime}}\) _with_ \(\mathrm{h}(u)>k_{1}\)_,_ \(f_{u}(x)\) _is a linear combination of_ \(\psi_{\sigma}(x)\) _with_ \(\sigma\in\mathcal{F}(\mathcal{B}_{u})\) _and_ \(\tilde{f}_{u}(x)=f_{u}(x)-\mathbb{E}f_{u}(X)\)_._
2. _For_ \(w\leq\rho^{\prime}\) _with_ \(\mathrm{h}(w)\geq k_{1}\)_, we have_ \[\frac{1}{CR^{3}}\mathbb{E}\Big{[}\sum_{u\in D_{k_{1}}(w)}f_{u}^{2}(X)\Big{]} \leq\mathbb{E}(\sum_{u\in D_{k_{1}}(w)}f_{u}(X))^{2}\leq CR\mathbb{E}\Big{[} \sum_{v\in D_{k_{1}}(w)}f_{u}^{2}(X)\Big{]}.\] (31)_We may group the \(f_{u}\) according to \(\mathrm{h}(u)\) and define for \(k_{1}\leq k\leq\mathrm{h}(\rho^{\prime})\),_

\[f_{k}(x):=\sum_{u\in D_{k}(\rho^{\prime})}\tilde{f}_{u}(x).\]

_In other words,_

\[f(X)=\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}f_{k}(X)\text{ almost surely.}\]

To prove the main lemma, let us begin by comparing \(\phi_{\sigma}(x)\) and \(\psi_{\sigma}(x)\) (See Definition A.11).

**Lemma D.2**.: _For \(\sigma\in\mathcal{F}(\mathcal{B}_{u})\), \(\phi_{\sigma}(x)\) can be expressed in the form_

\[\phi_{\sigma}(x)=\prod_{i\in I(\sigma)}\psi_{P_{i}\sigma}(x)-a_{\subset,\sigma }(x)-a_{<,\sigma}(x)-a_{c,\sigma},\] (32)

_where:_

* \(a_{\subset,\sigma}(x)\) _is a linear combination of_ \(\phi_{\sigma^{\prime}}(x)\) _for_ \(\sigma^{\prime}\in\mathcal{F}(\mathcal{B}_{u})\) _such that_ \(I(\sigma^{\prime})\) _is a proper subset of_ \(I(\sigma)\)_._
* \(a_{<,\sigma}(x)\) _is a linear combination of_ \(\phi_{\sigma^{\prime}}(x)\) _for_ \(\sigma^{\prime}\in\mathcal{F}(\mathcal{B}_{<u})\) _(recall that_ \(\mathcal{B}_{<u}=\{S\in\mathcal{B}\,:\,\rho(S)<u\}\)_), and_
* \(a_{c,\sigma}\) _is a constant._

Proof.: Fix \(\sigma\in\mathcal{F}(\mathcal{B})\) and let \(u=\rho(S)\) and \(S=S(\sigma)\). Recall that \(P_{i}\sigma\in[0,q-1]^{T}\) is the projection of \(\sigma\) to \(S_{i}\). We can also decompose the function \(\phi_{\sigma}\) according to \(\{P_{i}\sigma\}_{i\in I(\sigma)}\):

\[\phi_{\sigma}(x)=\prod_{i\in I(S)}\phi_{P_{i}\sigma}(x).\] (33)

Before we proceed, let us note that by Lemma 1.12 and the definition of \(\mathcal{B}\), we have \(P_{i}\sigma\in\mathcal{F}(\mathcal{B}_{\leq u_{i}})\). Now, let us expand the function \(\psi_{\sigma}\) according to its definition:

\[\prod_{i\in I(S)}\psi_{P_{i}\sigma}(x)=\prod_{i\in I(S)}\Big{(}\phi_{P_{i} \sigma}(x)-\mathbb{E}\phi_{P_{i}\sigma}(X)\Big{)}=\sum_{I_{1},I_{2}}\Big{(} \prod_{i\in I_{1}}\phi_{P_{i}\sigma}(x)\Big{)}\Big{(}\prod_{i\in I_{2}}(- \mathbb{E}\phi_{P_{i}\sigma}(X))\Big{)},\]

where the summation is taken over all possible partition \(I_{1}\sqcup I_{2}=I(\sigma)\). Next, we can group the summands into four types based on \(|I_{1}|\) and \(|I_{2}|\):

**Type 1**: \(|I_{1}|=|I(\sigma)|\). The summand is simply \(\phi_{\sigma}(x)\).
**Type 2**: \(2\leq|I_{1}|\leq|I(\sigma)|-1\).

Each summand is a constant multiple of \(\phi_{\sigma^{\prime}}(x)\) where \(\sigma^{\prime}\) is the projection of \(\sigma\) to the indices \(\sqcup_{i\in I_{1}}S_{i}\). Clearly, \(S(\sigma^{\prime})=\sqcup_{i\in I_{1}}S_{i}\). With \(|I_{1}|\geq 2\), we have \(\rho(\sigma^{\prime})=u\). Further, each \(S_{i}\in\mathcal{A}_{\leq u_{i}}\) for \(i\in I(\sigma)\), it follows that \(S(\sigma^{\prime})\in\mathcal{B}_{u}\), which in turn implies \(\sigma^{\prime}\in\mathcal{F}(\mathcal{B}_{u})\).

We denote the sum of summands of this type by \(a_{<,\sigma}(x)\).
**Type 3**: \(|I_{1}|=1\). Each summand is a constant multiple of \(\phi_{P_{i}\sigma}(x)\), where \(i\) is the element in \(I_{1}\). Notice that \(P_{i}\sigma\in\mathcal{F}(\mathcal{A}_{<u})\subset\mathcal{F}(\mathcal{B}_{<u})\) where the inclusion follows from Lemma 1.12. We denote the sum of summands of this type as \(a_{<,\sigma}(x)\).
**Type 4**: \(|I_{1}|=0\) There is only one summand, which is a constant. We denote this constant by \(a_{c,P_{i}\sigma}\).

With this decomposition, (32) follows. 

Given the expressions for \(\psi_{\sigma}(x)\) in terms \(\phi_{\sigma}(x)\) and vice-versa, for any given \(u\in T\backslash L\), we can convert a linear combination of \(\phi_{\sigma}(x)\) with \(\sigma\in\mathcal{F}(\mathcal{B}_{u})\) to that of \(\psi_{\sigma}(x)\) with \(\sigma\in\mathcal{F}(\mathcal{B}_{u})\).

**Lemma D.3**.: _For \(u\in T\backslash L\), consider any function of the form_

\[p_{u}(x)=\sum_{\sigma\in\mathcal{F}(\mathcal{B}_{u})}c_{\sigma}\phi_{\sigma}(x).\]

_Then there exists a decomposition_

\[p_{u}(x)= \tilde{f}_{u}(x)+p_{<,u}(x)+c_{u},\]

_where:_

* \(\tilde{f}_{u}(x)=f_{u}(x)-\mathbb{E}f_{u}(X)\) _and_ \(f_{u}(x)\) _is a linear combination of_ \(\psi_{\sigma}(x)\) _for_ \(\sigma\in\mathcal{F}(\mathcal{B}_{u})\)_,_
* \(p_{<,u}(x)\) _is a linear combination of_ \(\phi_{\sigma}(x)\) _with_ \(\sigma\in\mathcal{F}(\mathcal{B}_{<u})\)_, and_
* \(c_{u}\) _is a constant._

Proof.: The decomposition is constructed through recursion on the following expression:

\[r(p_{u}):=\operatorname{argmax}\bigl{\{}|I(\sigma)|\,:\,\sigma\in\mathcal{F}( \mathcal{B}_{u}),\,c_{\sigma}\neq 0\bigr{\}}.\]

Suppose \(r(p_{u})=2\). Then the statement of simply follows from Lemma D.2.

Suppose the statement of the lemma holds whenever \(r(p_{u})\leq r\) for \(2\leq r<Rd\). Consider any function \(p_{u}\) with \(r(p_{u})=r+1\):

\[p_{u}(x)=\sum_{\sigma\in\mathcal{F}(\mathcal{B}_{u})\,:\,|I(\sigma)|\leq r+1} c_{\sigma}\phi_{\sigma}(x)=\underbrace{\sum_{\sigma\in\mathcal{F}(\mathcal{B}_{u })\,:\,|I(\sigma)|=r+1}c_{\sigma}\phi_{\sigma}(x)}_{:=p_{u,\leq r}(x)}+ \underbrace{\sum_{\sigma\in\mathcal{F}(\mathcal{B}_{u})\,:\,|I(\sigma)|\leq r }c_{\sigma}\phi_{\sigma}(x)}_{:=p_{u,\leq r}(x)}.\]

According to the decomposition of \(\phi_{\sigma}(x)\) in Lemma D.2, let

\[f_{u,r+1}(x) :=\sum_{\sigma\in\mathcal{F}(\mathcal{B}_{u})\,:\,|I(\sigma)|=r+1 }c_{\sigma}\psi_{\sigma}(x)\] \[p_{*,u,r+1}(x) :=\sum_{\sigma\in\mathcal{F}(\mathcal{B}_{u})\,:\,|I(\sigma)|=r+1 }c_{\sigma}a_{*,\sigma}(x)\]

where \(*\) can be \(\subset\), \(<\), or \(c\). Then,

\[p_{u,r+1}(x)=f_{u,r+1}(x)+p_{\subset,u,r+1}(x)+p_{<,u,r+1}(x)+p_{c,u,r+1}(x).\] (34)

Observe that \(p_{u,\leq r}(x)+p_{\subset,u,r+1}(x)\) is a linear combination of \(\phi_{\sigma}(x)\) with \(\sigma\in\mathcal{F}(\mathcal{B}_{u})\) and \(|I(\sigma)|\leq r\). Thus, by the inductive assumption, the summation can be expressed in the form

\[p_{u,\leq r}(x)+p_{\subset,u,r+1}(x)=\tilde{f}_{u}^{\prime}(x)+p_{<,u}^{ \prime}(x)+c_{u}^{\prime}.\]

Finally, let

\[f_{u}(x)= f_{u}^{\prime}(x)+f_{u,r+1}(x),\] \[p_{<,u}(x)= p_{<,u}^{\prime}(x)+p_{<,u,r+1}(x),\] \[c_{u}= c_{u}^{\prime}+p_{c,u}^{\prime}+\mathbb{E}\bigl{[}f_{u,r+1}(X) \bigr{]},\]

and we have

\[p_{u}(x)= f_{u,r+1}(x)+p_{<,u,r+1}(x)+p_{c,u,r+1}(x)+\tilde{f}_{u}^{\prime}(x )+p_{<,u}^{\prime}(x)+c_{u}^{\prime}\] \[= \tilde{f}_{u}(x)+p_{<,u}(x)+c_{u}.\]Proof of Lemma D.1.: We will construct \(f_{u}(x)\) for \(u\) starting from top layer (\(u=\rho^{\prime}\)) to bottom layer. For \(k\in[k_{1},l^{\prime}-1]\), when \(f_{u}(x)\) is constructed for \(u\in T_{\rho^{\prime}}\) with \(\operatorname{h}(u)>k\), we define

\[f_{\leq k}(x)=f(x)-\sum_{u\,:\,\operatorname{h}(u)>k+1}\tilde{f}_{u}(x),\] (35)

where \(\tilde{f}_{u}(x)=f_{u}(x)-\operatorname{\mathbb{E}}f_{u}(X)\). Without lose of generality, let \(f_{\leq l^{\prime}}(x)=f(x)\).

Fix \(k\in[k_{1}+1,l^{\prime}]\). For the induction step, suppose \(\{f_{u}(x)\}_{u\in T_{\rho^{\prime}}\,:\,\operatorname{h}(u)>k}\)\(\{f_{\leq s}(x)\}_{s\in[k,l^{\prime}]}\) have been constructed such that \(f_{\leq k}(x)\) can be expressed in the form

\[f_{\leq k}(x)=c^{\prime}+\sum_{\sigma\in\mathcal{F}(\mathcal{B})\,:\,k_{1}< \operatorname{h}(\rho(\sigma))\leq k}c^{\prime}_{\sigma}\phi_{\sigma}(x)+\sum _{\sigma\in\mathcal{F}(\mathcal{I}^{L})\,:\,\operatorname{h}(\rho(\sigma)) \leq k_{1}}c^{\prime}_{\sigma}\phi_{\sigma}(x).\] (36)

Clearly, this holds when \(k=l^{\prime}\).

For each \(u\) with \(\operatorname{h}(u)=k\), let \(p_{u}(x)=\sum_{\sigma\in\mathcal{F}(\mathcal{B}_{u})}c^{\prime}_{\sigma}\phi_ {\sigma}(x)\), and define \(\tilde{f}_{u}(x),p_{<,u}(x)\), and \(c_{u}\) according to Lemma D.3. Then,

\[f_{\leq k-1}(x)= f_{\leq k}(x)-\sum_{u\,:\,\operatorname{h}(u)=k_{1}}\tilde{f}_{u}(x)\] \[= c^{\prime}+\sum_{\sigma\in\mathcal{F}(\mathcal{B})\,:\,k_{1}< \operatorname{h}(\rho(\sigma))\leq k-1}c^{\prime}_{\sigma}\phi_{\sigma}(x)+ \sum_{\sigma\in\mathcal{F}(\mathcal{I}^{L})\,:\,\operatorname{h}(\rho(\sigma) )\leq k_{1}}c^{\prime}_{\sigma}\phi_{\sigma}(x)\] \[+\sum_{u\,:\,\operatorname{h}(u)=k}(p_{<,u}(x)+c_{u}).\]

Recall from Lemma D.3 that \(p_{<,u}(x)\) is a linear combination of \(\phi_{\sigma}(x)\) with \(\sigma\in\mathcal{F}(\mathcal{B}_{<u})\), the function \(f_{\leq k-1}(x)\) satisfies (36) as well (with \(k\) been replaced by \(k-1\)).

Once the induction terminated at layer \(k_{1}\), we obtain

\[f_{k_{1}}(x)=c+\sum_{u\in D_{k_{1}}(\rho^{\prime})}\sum_{\sigma\in\mathcal{F}( \mathcal{I}^{L_{u}}\setminus\{\emptyset\})}c_{\sigma}\phi_{\sigma}(x).\]

Now, observe that for \(k_{1}<k\leq\operatorname{h}(\rho^{\prime})\), we have \(f_{k}(x)\) is defined as the sum of \(\tilde{f}_{u}\) for \(u\in D_{k}(\rho^{\prime})\), which are functions of mean \(0\). Together with the assumption that \(\operatorname{\mathbb{E}}f(X)=0\), we have

\[\operatorname{\mathbb{E}}f_{k_{1}}(X)=\operatorname{\mathbb{E}}f(X)-\sum_{k=k_ {1}+1}^{\operatorname{h}(\rho^{\prime})}\operatorname{\mathbb{E}}f_{k}(X)=0.\]

Notice that \(f_{k_{1}}\) satisfies the assumption of the function stated in Lemma C.7 with \(m=k_{1}\). By replacing \(f_{k_{1}}\) by \(\bar{f}_{k_{1}}\) and \(f_{u}\) by \(\bar{f}_{u}\) for each \(u\in D_{k_{1}}(\rho^{\prime})\), the second statement follows while the third statement of the Lemma remains true. Hence, the proof is completed. 

## Appendix E Induction Step 1: Decay of \(f_{u}\)

The goal this section and next section is to prove Theorem B.6. Let us restate the theorem here.

**Theorem**.: _Given the rooted tree \(T\) and the transition matrix \(M\) described in Theorem 1.6, and under the additional assumption that \(c_{M}=\min_{i,j\in[q]}M_{ij}>0\). Suppose \(\mathcal{A}\) is a collection of subsets satisfying Assumption B.3 with parameters \(\operatorname{h}^{*}\) and \(c^{*}\). Then, there exists \(C=C(M,d,c^{*})\geq 1\) such that \(\mathcal{B}=\mathcal{B}(\mathcal{A})\) satisfies Assumption B.3 with parameters \(\operatorname{h}^{*}+C(\log(R)+1)\) and \(c^{*}\)._

**In this and the following section**, we will fix a collection \(\mathcal{A}\) that meets Assumption B.3 with some parameters \(l^{*}\) and \(c^{*}\). Additionally, we abbreviate

\[\mathcal{B}=\mathcal{B}(\mathcal{A}).\]Further, we will fix \(\rho^{\prime}\in T\) and a function \(f\) described in the Assumption B.3, and assume

\[\mathbb{E}f(X)=0.\]

The proof is grounded in the decomposition of \(f\) as described in Lemma D.1, which splits \(f\) into summation of \(f_{k}\) and subsequently into summations of \(\tilde{f}_{u}\). Accordingly, this section is devoted to derive the variance decay properties of \(f_{u}\) stated as Proposition E.1 below. The proposition will be used to derive variance decay properties of \(f_{k}\), and toward the proof of Theorem B.6 in next section.

**Proposition E.1**.: _There exists \(C=C(M,c^{*})\geq 1\) so that the following holds. For any \(u\in T\), consider a function \(f_{u}\) of the form_

\[f_{u}(x)=\sum_{0\neq\sigma\in\mathcal{F}(\mathcal{B}_{u})}c_{\sigma}\psi_{ \sigma}(x).\]

_Then, for \(\theta\in[q]\), we have the following bounds on \((\mathbb{E}_{u}f_{u})(x)\) (recall that that by the Markov Property, \((\mathbb{E}_{u}f_{u})(x)\) is a function of \(x_{u}\)):_

\[(\mathbb{E}_{u}f_{u})^{2}(\theta)\leq\exp(-2\varepsilon(\mathrm{h}(u)-C(\log( R)+1)-\mathrm{h}^{*}))(\mathbb{E}_{u}f_{u}^{2})(\theta)\] (37)

_and_

\[\mathbb{E}\big{[}(\mathbb{E}_{u}\tilde{f}_{u})^{2}(X_{u})\big{]}\leq\exp(-2 \varepsilon(\mathrm{h}(u)-C(\log(R)+1)-\mathrm{h}^{*}))\mathbb{E}\tilde{f}_{u }^{2}(X).\] (38)

_Additionally, for any function \(a(x)\) having inputs involving only \((x_{v}\,:\,v\in T_{u_{i}})\) for some \(i\in[d_{u}]\):_

\[\mathbb{E}\big{[}|\tilde{f}_{u}(X)a(X)|\big{]}\leq\exp\Big{(}-\frac{ \varepsilon}{2}(\mathrm{h}(u)-C(\log(R)+1)-\mathrm{h}^{*})\Big{)}(\mathbb{E} \tilde{f}_{u}^{2}(X))^{1/2}(\mathbb{E}a^{2}(X))^{1/2}.\] (39)

**Remark E.2**.: The statement of the Proposition E.1 is exactly the statement of Theorem B.6 restricted to functions all of whose non-zero \(c_{\sigma}\) have \(\rho(\sigma)=\rho^{\prime}\). Thus in some sense in this section we prove the Theorem for the most complex terms. And in the next section we will control the correlations between different terms.

This is an analogue in our setting to the classical fact in Fourier analysis that high amplitude functions have sharp decay under noise.

**Remark E.3**.: We remark that the proposition holds immediately whenever \(|d_{u}|\leq 1\), since \(\mathcal{B}_{u}=\emptyset\).

Before we proceed further, we need to decompose \(f_{u}(x)\).

**Definition E.4**.: _For \(u\in T\backslash L\) and \(f_{u}(x)=\sum_{\sigma\in\mathcal{F}(\mathcal{B}_{u})}c_{\sigma}\psi_{\sigma}(x)\), let_

\[f_{u,I}(x):= \sum_{\sigma\in\mathcal{F}(\mathcal{B}_{u})\,:\,I(\sigma)=I}c_{ \sigma}\psi_{\sigma}(x),\quad\text{and}\] (40) \[\tilde{f}_{u,I}(x):= f_{u,I}(x)-\mathbb{E}f_{u,I}(X)\] (41)

_for each \(I\subseteq[d_{u}]\) with \(|I|\geq 2\)._

Given the above definition, we have

\[f_{u}(x)=\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}f_{u,I}(x).\]

**Proposition E.5**.: _There exists \(C=C(M,c^{*})\geq 1\) so that the following holds. For any \(u\in T\setminus L\) and \(I\subseteq[d_{u}]\) with \(|I|\geq 2\). Consider a function of the form_

\[a(x)=\sum_{\sigma\in\mathcal{F}(\mathcal{B}_{u})\,:\,I(\sigma)=I}c_{\sigma} \psi_{\sigma}(x).\]

_Then, for \(I^{\prime}\subseteq I\), let_

\[U=T\setminus\big{(}\bigcup_{i\in I^{\prime}}T_{u_{i}}\big{)},\]

_and we have_

\[\big{(}(\mathbb{E}_{U}a)(x)\big{)}^{2}\leq \exp\big{(}-\varepsilon|I^{\prime}|(\mathrm{h}(u)-C-\mathrm{h}^{*} )\big{)}(\mathbb{E}_{U}a^{2})(x).\] (42)Roughly speaking the proposition states that under the decay of correlation in Assumption B.3, for functions all of whose coefficient \(c_{\sigma}\) have \(S(\sigma)=I\) for some large set \(I\) we get a variance decay of the form \(\exp(-\epsilon|I|\mathrm{h}(u))\). For later applications the statement is more general allowing to condition on some of the subtrees. This is an analogue in our setting to the classical fact in Fourier analysis that high amplitude functions have sharp decay under noise.

Proof.: We fix \(u\in T\backslash L\) and \(I\subseteq[d_{u}]\). Without lose of generality, we assume \(I^{\prime}=[s]\).

Let \(C_{0}=C_{0}(M,c^{*})\) denote the constant described in the statement of the Proposition. The precise value of \(C_{0}\) will be determined during the proof.

For brevity, we introduce some notations that are only used in this proof.

1. Decomposition of \(x\in[q]^{T}\): Consider the representation of \(x\) as \[x=(x_{u},x_{0},x_{1},\ldots,x_{s}),\] where, \(\forall k\in[s]\), \(x_{k}:=(x_{v}\,:\,v\leq u_{k})\), and \(x_{0}=(x_{v}\,:\,v\in U\setminus\{u\})\).

Further, let

\[x_{\leq k}=(x_{0},x_{1},\ldots,x_{k}).\]

For \(k\in[0,s]\),

\[a_{\leq k}(x_{\leq k}):=\mathbb{E}\Big{[}a(X)\,\Big{|}\,X_{u}=x_{u}\text{ and }X_{\leq k}=x_{\leq k}\Big{]}.\]

Before we proceed to the proof, observe that applying Jenson's inequality on conditional expectation, we can form a chain of inequalities

\[(\mathbb{E}_{U}a)^{2}(x)=(\mathbb{E}_{U}a_{\leq 0}^{2})(x)\leq(\mathbb{E}_{U}a_ {\leq 1}^{2})(x)\leq(\mathbb{E}_{U}a_{\leq 2}^{2})(x)\leq\ldots\leq(\mathbb{E}_{U}a_ {\leq s}^{2})(x)=(\mathbb{E}_{U}a^{2})(x).\]

If \(\mathrm{h}(u)\leq C_{0}+\mathrm{h}^{*}\), then the statement of the Proposition is weaker than the inequality \((\mathbb{E}_{U}a)^{2}(x)\leq(\mathbb{E}_{U}a^{2})(x)\) stated above. So the lemma follows immediately in that case. From now on we assume

\[\mathrm{h}(u)>C_{0}+\mathrm{h}^{*}.\] (43)

We will improve each inequality in the above chain by leveraging the assumption (15).

Given the definition of \(a(x)\),

\[a(x)=\sum_{\sigma}c_{\sigma}\prod_{i\in I\backslash[s]}\tilde{\phi}_{P_{i} \sigma}(x_{0})\prod_{i\in[s]}\tilde{\phi}_{P_{i}\sigma}(x_{i})\]

By the Markov Property, the random variables \((X_{i}|X_{u}=x_{u})_{i\in[0,s]}\) are independent. This gives rise to:

\[a_{\leq k}(x)= \mathbb{E}\Big{[}\sum_{\sigma}c_{\sigma}\prod_{i\in I\backslash[s ]}\tilde{\phi}_{P_{i}\sigma}(X_{0})\prod_{i\in[s]}\tilde{\phi}_{P_{i}\sigma}( X_{i})\,\Big{|}\,X_{u}=x_{u}\text{ and }X_{\leq k}=x_{\leq k}\Big{]}\] \[= \sum_{\sigma}c_{\sigma}\underbrace{\prod_{i\in I\backslash[s]} \tilde{\phi}_{P_{i}\sigma}(x_{0})\prod_{i\in[k]}\tilde{\phi}_{P_{i}\sigma}(x_{ k})}_{\text{This part is freezed.}}\underbrace{\prod_{i\in[k+1,s]}(\mathbb{E}_{u}\tilde{\phi}_{P_{i} \sigma})(x_{u})}_{\text{This part is a function of }x_{u}}.\]

Now, fix \(k\in[s]\) and express \(a_{\leq k}(x)=a_{\leq k}(x_{u},x_{\leq k-1},x_{k})\). An essence of this proof is that the mapping:

\[y_{k}\mapsto a_{\leq k}(x_{u},x_{\leq k-1},y_{k})\]

is a linear combination of \(\tilde{\phi}_{\sigma}(y_{k})\) with \(\sigma\in\mathcal{F}(\mathcal{A}_{u_{k}})\) and the coefficients are functions of \((x_{u},x_{\leq k-1})\), which gives us room to apply the inductive assumption, or (15) from Assumption B.3.

To aid our analysis, we introduce \(Y_{k}\), an independent copy of \(X_{k}\). By (15), we have

\[\mathbb{E}\Big{[}\big{(}\mathbb{E}[a_{\leq k}(x_{u},x_{\leq k-1},Y_{k})|Y_{u}] \big{)}^{2}\Big{]}\leq \exp(-\varepsilon(\mathrm{h}(u)-\mathrm{h}^{*}))\mathbb{E}_{Y_{k}} \big{[}a_{\leq k}^{2}(x_{u},x_{\leq k-1},Y_{k})\big{]}.\] (44)

The reason we introduce \(Y_{k}\) is that the L.H.S. and R.H.S. of the above inequality are not related to (any moments of) conditional expectation of \(a(X)\). However, it can still be used with some adjustment, relying on (16) from Assumption B.3.

Given the assumption on \(C_{0}\) being greater than or equal to \(1\), we have

\[\mathrm{h}(u_{k})=\mathrm{h}(u)-1\stackrel{{\eqref{eq:c-1}}}{{ \geq}}\mathrm{h}^{*}+C_{0}-1\geq\mathrm{h}^{*}.\]

Applying (16) to our function \(y_{k}\mapsto a_{\leq k}(x_{u},x_{\leq k-1},y_{k})\) we get

\[\mathbb{E}_{Y_{k}}\big{[}a_{\leq k}^{2}(x_{u},x_{\leq k-1},Y_{k}) \big{]}\leq \frac{1}{c^{*}}\min_{\theta\in[q]}\mathbb{E}_{Y_{k}}\big{[}a_{ \leq k}^{2}(x_{u},x_{\leq k-1},Y_{k})\big{|}Y_{u}=\theta\big{]}\] \[\leq \frac{1}{c^{*}}\mathbb{E}_{Y_{k}}\big{[}a_{\leq k}^{2}(x_{u},x_{ \leq k-1},Y_{k})\big{|}Y_{u}=x_{u}\big{]}.\] (45)

On the other hand,

\[\pi(x_{u})\big{(}\mathbb{E}_{Y_{k}}[a_{\leq k}(x_{u},x_{\leq k-1},Y_{k})|Y_{u }=x_{u}]\big{)}^{2}\leq\mathbb{E}\Big{[}\big{(}\mathbb{E}_{Y_{k}}[a_{\leq k}( x_{u},x_{\leq k-1},Y_{k})|Y_{u}]\big{)}^{2}\Big{]}.\]

Combining the above expression, (44), and (45), we conclude that

\[\big{(}\mathbb{E}\big{[}a_{\leq k}(x_{u},x_{\leq k-1},Y_{k})\, \big{|}\,Y_{u}=x_{u}\big{]}\big{)}^{2}\leq\frac{1}{c^{*}\pi(x_{u})}\exp(- \varepsilon(\mathrm{h}(u)-\mathrm{h}^{*}))\mathbb{E}_{Y_{k}}\big{[}a_{\leq k} ^{2}(x_{u},x_{\leq k-1},Y_{k})\,\big{|}\,Y_{u}=x_{u}\big{]}.\] (46)

Notice that the expression inside the square in L.H.S. is

\[\mathbb{E}\big{[}a_{\leq k}(x_{u},x_{\leq k-1},Y_{k})\,\big{|}\, Y_{u}=x_{u}\big{]}\] \[= \mathbb{E}\big{[}a_{\leq k}(x_{u},x_{\leq k-1},X_{k})\,\big{|}\, X_{u}=x_{u}\big{]}\] \[= \mathbb{E}\big{[}a_{\leq k}(X_{u},X_{\leq k-1},X_{k})\,\big{|}\, X_{u}=x_{u},X_{\leq k-1}=x_{\leq k-1}\big{]}\] \[= a_{\leq k-1}(x).\]

Similarly,

\[\mathbb{E}_{Y_{k}}\big{[}a_{\leq k}^{2}(x_{u},x_{\leq k-1},Y_{k}) \,\big{|}\,Y_{u}=x_{u}\big{]}= \mathbb{E}\big{[}a_{\leq k}^{2}(x_{u},x_{\leq k-1},X_{k})\, \big{|}\,X_{u}=x_{u}\big{]}\] \[= \mathbb{E}\big{[}a_{\leq k}^{2}(X_{u},X_{\leq k-1},X_{k})\,\big{|} \,X_{u}=x_{u},X_{\leq k-1}=x_{\leq k-1}\big{]}\] \[= a_{\leq k-1}^{2}(x).\]

By imposing the **first assumption** on \(C_{0}\) that

\[C_{0}\geq\frac{1}{\varepsilon}\log\Big{(}\frac{1}{c^{*}\min_{j\in[q]}\pi(j)} \Big{)},\]

it follows from (46) that

\[a_{\leq k-1}^{2}(x)\leq\exp(-\varepsilon(\mathrm{h}(u)-C_{0}-\mathrm{h}^{*})) \mathbb{E}\big{[}a_{\leq k}^{2}(X)\,\big{|}\,X_{u}=x_{u}\text{ and }X_{\leq k-1}=x_{\leq k-1}\big{]}\]

By taking Conditional Expectation on both sides,

\[(\mathbb{E}_{U}a_{\leq k-1}^{2})(x)\leq\exp(-\varepsilon(\mathrm{h}(u)-C_{0}- \mathrm{h}^{*}))\big{(}\mathbb{E}_{U}a_{\leq k}^{2})(x).\]

Finally, we apply this inequality consecutively for \(k\in[s]\) we obtain

\[(\mathbb{E}_{U}a)^{2}(x)\leq\exp(-\varepsilon|I^{\prime}|(\mathrm{h}(u)-C_{0}- \mathrm{h}^{*}))(\mathbb{E}_{U}a^{2})(x).\]

**Corollary E.6**.: _Fix \(u\in T\backslash L\) and a function \(f_{u}(x)\) following the form described in Definition E.4. If \(I,J\subseteq[d_{u}]\) are subsets of \([d_{u}]\) of size at least \(2\), then for every \(\theta\in[q]\),_

\[|(\mathbb{E}_{u}f_{u,I})(\theta)|\leq \exp\Big{(}-\frac{\varepsilon|I|}{2}(\mathrm{h}(u)-C-\mathrm{h}^{ *})\sqrt{(\mathbb{E}_{u}f_{u,I}^{2})(\theta)}\] (47) \[|(\mathbb{E}_{u}f_{u,I}\cdot f_{u,J})(\theta)|\leq \exp\Big{(}-\frac{\varepsilon|I\Delta J|}{2}(\mathrm{h}(u)-C- \mathrm{h}^{*})\Big{)}\sqrt{(\mathbb{E}_{u}f_{u,I}^{2})(\theta)}\cdot\sqrt{( \mathbb{E}_{u}f_{u,J}^{2})(\theta)},\] (48)

_where \(C=C(M,c^{*})\) is the constant introduced in Proposition E.5, and \(I\Delta J:=(I\setminus J)\cup(J\setminus I)\)._

Proof.: For the first statement, it follows from Proposition E.5 with \(a(x)=f_{u,I}(x)\) and \(I=I^{\prime}\).

To prove the second statement, we begin by noting that the inputs of \(f_{u,I}(x)\) and \(f_{u,J}(x)\) do not include \(\big{(}x_{v}\,:\,v\in\bigcup_{i\in J\setminus I}T_{u_{i}}\big{)}\) and \(\big{(}x_{v}\,:\,v\in\bigcup_{i\in I\setminus J}T_{u_{i}}\big{)}\), respectively. Thus, we can apply the Markov Property and that fact that if \(Y,Z,W\) are ind pendent then:

\[\mathbb{E}[g(Y,Z)h(Z,W)]=\mathbb{E}[\mathbb{E}[g(Y,Z)h(Z,W)|Z]]=\mathbb{E}[ \mathbb{E}[g(Y,Z)|Z]h(Z,W)]\]

and this in turn becomes:

\[\mathbb{E}[\mathbb{E}[g(Y,Z)|Z]h(Z,W)|W]=\mathbb{E}[\mathbb{E}[g(Y,Z)|Z] \mathbb{E}[h(Z,W)|W]],\]

to obtain

\[\big{(}\mathbb{E}_{u}f_{u,I}f_{u,J}\big{)}(x)\] \[= \mathbb{E}\bigg{[}\mathbb{E}\Big{[}f_{u,I}(X)\,\Big{|}\,X_{v}\,: \,v\notin\bigcup_{i\in I\setminus J}T_{u_{i}}\Big{]}\cdot\mathbb{E}\Big{[}f _{u,J}(X)\,\Big{|}\,X_{v}:\,v\notin\bigcup_{i\in J\setminus I}T_{u_{i}}\Big{]} \,\bigg{|}\,X_{v}=x_{v}:v\not<u\bigg{]}.\]

In terms of absolute value, by Proposition E.5 we have

\[\big{|}(\mathbb{E}_{u}f_{u,I}f_{u,J})(x)\big{|}\] \[\leq \mathbb{E}\bigg{[}\bigg{|}\mathbb{E}\Big{[}f_{u,I}(X)f_{u,J}(X) \,\Big{|}\,X_{v}\,:\,v\notin\bigcup_{i\in I\Delta J}T_{u_{i}}\Big{]}\bigg{|} \,\bigg{|}\,X_{v}=x_{v}:v\not<u\bigg{]}\] \[= \mathbb{E}\bigg{[}\bigg{|}\mathbb{E}\Big{[}f_{u,I}(X)\,\Big{|}\, X_{v}\,:\,v\notin\bigcup_{i\in I\setminus J}T_{u_{i}}\Big{]}\bigg{|}\,\bigg{|} \,\Big{|}\,\Big{|}\,X_{v}=x_{v}:v\not<u\bigg{]}\] \[\leq \mathbb{E}\bigg{[}\sqrt{\exp(-\varepsilon|I\backslash J|( \mathrm{h}(u)-C-\mathrm{h}^{*}))\cdot\mathbb{E}\Big{[}f_{u,I}^{2}(X)\,\Big{|} \,X_{v}\,:\,v\notin\bigcup_{i\in I\setminus J}T_{u_{i}}\Big{]}}\bigg{|}\] \[\qquad\qquad\cdot\sqrt{\exp(-\varepsilon|J\backslash I|(\mathrm{h }(u)-C-\mathrm{h}^{*}))\cdot\mathbb{E}\Big{[}f_{u,J}^{2}(X)\,\Big{|}\,X_{v}\,: \,v\notin\bigcup_{i\in J\setminus I}T_{u_{i}}\Big{]}}\,\bigg{|}\,X_{v}=x_{v}:v \not<u\bigg{]}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}|I\Delta J|(\mathrm{h}(u)-C- \mathrm{h}^{*})\Big{)}\sqrt{\mathbb{E}\bigg{[}\mathbb{E}\Big{[}f_{u,I}^{2}(X)\, \Big{|}\,X_{v}\,:\,v\notin\bigcup_{i\in I\setminus J}T_{u_{i}}\Big{]}\, \bigg{|}\,X_{v}=x_{v}\,:\,v\not<u\bigg{]}}\] \[= \exp\Big{(}-\frac{\varepsilon}{2}|I\Delta J|(\mathrm{h}(u)-C- \mathrm{h}^{*})\Big{)}\sqrt{(\mathbb{E}_{u}f_{u,I}^{2})(x)\cdot(\mathbb{E}_{u} f_{u,J}^{2})(x)},\]

where the second to last inequality follows from Holder's inequality. 

**Corollary E.7**.: _There exists \(C=C(M,d,c^{*})\geq 1\) so that the following holds. If \(u\in T\backslash L\) with \(\mathrm{h}(u)\geq\mathrm{h}^{*}+C(1+\log(R))\), then for any \(f_{u}(x)\) in the form as described in Definition E.4,_

\[\forall\theta\in[q],\ \frac{1}{2}\cdot\sum_{I\subset[d_{u}]\,:\,|I|\geq 2}( \mathbb{E}_{u}f_{u,I}^{2})(\theta)\leq(\mathbb{E}_{u}f_{u}^{2})(\theta).\] (49)

[MISSING_PAGE_FAIL:33]

since otherwise the statements follow from either Cauchy-Schwarz or Jenson's inequality.

**Part I: Derivation of (37) and (38).**

First, by (47),

\[(\mathbb{E}_{u}f_{u})^{2}(\theta))= \Big{(}\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}(\mathbb{E}_{u}f_{u,I}) (\theta)\Big{)}^{2}\] (54) \[\leq \bigg{(}\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\exp\Big{(}-\frac{ \varepsilon|I|}{2}(\mathrm{h}(u)-C_{E.5}-\mathrm{h}^{*})\Big{)}\cdot\sqrt{( \mathbb{E}_{u}f_{u,I}^{2})(\theta)}\bigg{)}^{2}\] \[\leq \Big{(}\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\exp\Big{(}- \varepsilon|I|(\mathrm{h}(u)-C_{E.5}-\mathrm{h}^{*})\Big{)}\Big{)}\cdot\Big{(} \sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}(\mathbb{E}_{u}f_{u,I}^{2})(\theta) \Big{)},\]

where we applied Cauchy-Schwarz inequality in the last inequality; the constant \(C_{E.5}\) is the constant \(C\) introduced in Proposition E.5.

With the coarse estimate

\[\big{|}\big{\{}I\subseteq[d_{u}]\,:\,|I|=t\big{\}}\big{|}=\binom{d_{u}}{i} \leq d_{u}^{t}\leq(Rd)^{t},\]

we have

\[\Big{(}\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\exp\Big{(}- \varepsilon|I|(\mathrm{h}(u)-C_{E.5}-\mathrm{h}^{*})\big{)}\Big{)}\] \[\leq \sum_{t=2}^{\infty}\exp\bigg{(}-\varepsilon t\Big{(}\mathrm{h}( u)-C_{E.5}-\frac{\log(R)+\log(d)}{\varepsilon}-\mathrm{h}^{*}\Big{)} \bigg{)}.\] (55)

The geometric series above is finite if \(\mathrm{h}(u)\) is large enough, and this can be achieved by imposing assumption of \(C_{0}\) and relying on (53). Now, let us impose the **first assumption** on \(C_{0}\):

\[C_{0}\geq C_{E.5}+(2+2\log(d)+100)/\varepsilon.\] (56)

Then, by (53) we have

\[\mathrm{h}(u)\geq\mathrm{h}^{*}+C_{0}(\log(R)+1)\geq\mathrm{h}^{*}+C_{E.5}+2 \frac{\log(R)+\log(d)}{\varepsilon}+\frac{100}{\varepsilon},\]

which in term implies the R.H.S. of (55) is

\[\frac{\exp\bigg{(}-2\varepsilon\Big{(}\mathrm{h}(u)-C_{E.5}-\frac {\log(R)+\log(d)}{\varepsilon}-\mathrm{h}^{*}\Big{)}\bigg{)}}{1-\exp\bigg{(}- \varepsilon\Big{(}\mathrm{h}(u)-C_{E.5}-\frac{\log(R)+\log(d)}{\varepsilon}- \mathrm{h}^{*}\Big{)}\bigg{)}}\] \[\leq \frac{\exp\bigg{(}-2\varepsilon\Big{(}\mathrm{h}(u)-C_{E.5}- \frac{\log(R)+\log(d)}{\varepsilon}-\mathrm{h}^{*}\Big{)}\bigg{)}}{1-e^{-100}}\] \[\leq 2\exp\bigg{(}-2\varepsilon\Big{(}\mathrm{h}(u)-C_{E.5}-\frac{ \log(R)+\log(d)}{\varepsilon}-\mathrm{h}^{*}\Big{)}\bigg{)}\] \[= \frac{1}{4}\exp\bigg{(}-2\varepsilon\Big{(}\mathrm{h}(u)-C_{E.5}- \frac{\log(R)+\log(d)}{\varepsilon}-\mathrm{h}^{*}-\frac{\log(8)}{2\varepsilon }\Big{)}\bigg{)}\] \[\leq \frac{1}{4}\exp\Big{(}-2\varepsilon\big{(}\mathrm{h}(u)-C_{0}( \log(R)+1)-\mathrm{h}^{*}\big{)}\Big{)}\]

Substituting the above estimate into (54), together with (49) we have

\[(\mathbb{E}_{u}f_{u})^{2}(\theta)\leq \frac{1}{4}\exp\Big{(}-2\varepsilon\big{(}\mathrm{h}(u)-C_{0}( \log(R)+1)-\mathrm{h}^{*}\big{)}\Big{)}\cdot\Big{(}\sum_{I\subseteq[d_{u}]\,: \,|I|\geq 2}(\mathbb{E}_{u}f_{u,I}^{2})(\theta)\Big{)}\] \[\leq \frac{1}{2}\exp\Big{(}-2\varepsilon\big{(}\mathrm{h}(u)-C_{0}( \log(R)+1)-\mathrm{h}^{*}\big{)}\Big{)}(\mathbb{E}_{u}f_{u}^{2})(\theta).\] (57)Therefore, we have derived an inequality which is slightly stronger than (37).

To derive (38), let us first show \(\mathbb{E}f_{u}(X)\) is relatively small using (57) and Jesnon's inequality:

\[\big{(}\mathbb{E}[f_{u}(X)]\big{)}^{2}\leq\mathbb{E}\big{[}(\mathbb{ E}_{u}f_{u})^{2}(X)\big{]}\leq\frac{1}{2}\exp\Big{(}-2\varepsilon\big{(}\mathrm{h}(u)-C_ {0}(\log(R)+1)-\mathrm{h}^{*}\big{)}\Big{)}\mathbb{E}\big{[}f_{u}^{2}(X)\big{]}\] \[\leq \frac{1}{2}\mathbb{E}\big{[}f_{u}^{2}(X)\big{]}.\]

Thus, the variance and the second moment of \(f_{u}(X)\) are the same up to a factor of \(2\):

\[\mathbb{E}\big{[}\tilde{f}_{u}^{2}(X)\big{]}=\mathbb{E}\big{[}f_{u}^{2}(X) \big{]}-\big{(}\mathbb{E}[f_{u}(X)]\big{)}^{2}\geq\frac{1}{2}\mathbb{E}\big{[} f_{u}^{2}(X)\big{]}.\] (58)

We conclude that

\[\mathbb{E}\big{[}(\mathbb{E}_{u}\tilde{f}_{u})^{2}(X)\big{]}\leq \mathbb{E}\big{[}(\mathbb{E}_{u}f_{u})^{2}(X)\big{]}\] \[\leq \frac{1}{2}\exp\Big{(}-2\varepsilon\big{(}\mathrm{h}(u)-C_{0}( \log(R)+1)-\mathrm{h}^{*}\big{)}\Big{)}\mathbb{E}\big{[}f_{u}^{2}(X)\big{]}\] \[\leq \exp(-2\varepsilon(\mathrm{h}(u)-C_{0}(1+\log(R))-\mathrm{h}^{*} ))\mathbb{E}\big{[}\tilde{f}_{u}^{2}(X)\big{]}.\]

Therefore, we complete the proof of (38).

#### Part II: Derivation of (39).

It remains to show (39) and the proof is similar. Fix \(I\subset[d_{u}]\) with \(|I|\geq 2\), let \(I^{\prime}=I\backslash\{i\}\) and we represent \(x\in[q]^{T}\) as \((x_{0},x_{1})\), where

\[x_{0}:= \big{(}x_{v}\,:\,v\notin\bigcup_{j\in I^{\prime}}T_{u_{j}}\big{)} \text{ and }\qquad\qquad x_{1}:= \big{(}x_{v}\,:\,v\in\bigcup_{j\in I^{\prime}}T_{u_{j}}\big{)}.\]

With this notation, we have \(a(x)=a(x_{0})\). Thus,

\[\mathbb{E}\big{[}|\tilde{f}_{u,I}(X)a(X)|\big{]}= \mathbb{E}\Big{[}|\mathbb{E}\big{[}\tilde{f}_{u,I}(X)\,\big{|}\,X _{0}\big{]}\cdot a(X_{0})|\Big{]}\] \[\leq \sqrt{\mathbb{E}\Big{[}\big{(}\mathbb{E}\big{[}\tilde{f}_{u,I}(X )\,\big{|}\,X_{0}\big{]}\big{)}^{2}\Big{]}}\cdot\sqrt{\mathbb{E}\big{[}a^{2}( X_{0})\big{]}}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}|I\setminus\{i\}|(\mathrm{h}(u) -C_{E.5}-\mathrm{h}^{*})\Big{)}\sqrt{\mathbb{E}\big{[}f_{u,I}^{2}(X)\big{]}} \cdot\sqrt{\mathbb{E}\big{[}a^{2}(X)\big{]}},\]

where the last inequality follows from Proposition E.5. Hence,

\[\mathbb{E}\big{[}|\tilde{f}_{u}(X)a(X)|\big{]}\] \[\leq \sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\exp\Big{(}-\frac{ \varepsilon}{2}|I\setminus\{i\}|(\mathrm{h}(u)-C_{E.5}-\mathrm{h}^{*})\Big{)} \sqrt{\mathbb{E}\big{[}f_{u,I}^{2}(X)\big{]}}\cdot\sqrt{\mathbb{E}\big{[}a^{2} (X)\big{]}}\] \[\leq \bigg{(}\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\exp\big{(}- \varepsilon|I\setminus\{i\}|(\mathrm{h}(u)-C_{E.5}-\mathrm{h}^{*})\big{)} \bigg{)}^{1/2}\cdot\sqrt{\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\mathbb{E}\big{[}f_{u,I}^{2}(X) \big{]}}\cdot\sqrt{\mathbb{E}\big{[}a^{2}(X)\big{]}}.\] (59)

Next, we impose the **second assumption** on \(C_{0}\) that

\[C_{0}\geq C_{E.7},\]

where \(C_{E.7}\) is the constant introduced in Corollary E.7. Together our assumption \(\mathrm{h}(u)\geq\mathrm{h}^{*}+C_{0}(\log(R)+1)\) at the beginning of the proof, we can apply the Corollary and (58) to get

\[\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\mathbb{E}f_{u,I}^{2}(X)\leq 2\mathbb{E}( f_{u}(X))^{2}\leq 4\mathbb{E}(\tilde{f}_{u}(X))^{2}.\] (60)Repeating the same argument as in the proof of (38) and relying on the assumption (56) of \(C_{0}\),

\[\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\exp(-\varepsilon|I\backslash\{i \}|(\mathrm{h}(u)-C_{E.5}-\mathrm{h}^{*}))\leq \sum_{t=1}^{\infty}\exp\bigg{(}-\varepsilon t\Big{(}\mathrm{h}(u) -C_{E.5}-\mathrm{h}^{*}-2\frac{\log(Rd)}{\varepsilon}\Big{)}\bigg{)}\] \[\leq \frac{1}{4}\exp\bigg{(}-\varepsilon\Big{(}\mathrm{h}(u)-C_{0}( \log(R)+1)-\mathrm{h}^{*}\Big{)}\bigg{)}.\] (61)

Therefore, combining (60), (61), and (59) we get

\[\mathbb{E}\big{[}|\tilde{f}_{u}(X)a(X)|\big{]}\leq\exp\Big{(}-\frac{ \varepsilon}{2}\big{(}\mathrm{h}(u)-C_{0}(\log(R)+1)-\mathrm{h}^{*}\big{)} \Big{)}\cdot\sqrt{\mathbb{E}\big{[}\tilde{f}_{u}^{2}(X)\big{]}}\cdot\sqrt{ \mathbb{E}\big{[}a^{2}(X)\big{]}}.\]

## Appendix F Induction Step 2: Decay of \(f_{k}\) and the proof of Theorem b.6

As a continuation of the inductive step, we adapt the notation introduced in the previous section. Building on the properties of an single component \(f_{u}\) from Proposition E.1, our objective is to deduce variance and covariance decay of \(f_{k}\), which is stated in Proposition F.1 below. Once it is established, we will be ready to prove Theorem B.6.

### Properties of \(f_{k}\)

The main goal of this subsection is to derive the following Proposition.

**Proposition F.1**.: _There exists \(C=C(M,d,c^{*})\geq 1\) so that the following holds. For any \(\rho^{\prime}\in T\) satisfying_

\[\mathrm{h}(\rho^{\prime})\geq\mathrm{h}^{*}+C(1+\log(R)).\]

_Fix a positive integer \(k_{1}\) such that_

\[\mathrm{h}(\rho^{\prime})\geq k_{1}\geq\mathrm{h}^{*}+C(1+\log(R)).\]

_Consider a function_

\[f(x)= c+\sum_{\sigma\in\mathcal{F}(\mathbb{B}_{\leq\rho^{\prime}})}c_{ \sigma}\phi_{\sigma}(x)\]

_with \(\mathbb{E}f(X)=0\). We decompose \(f\) according to Lemma D.1 with the given \(k_{1}\). Then, the following holds:_

* _for_ \(k\in[k_{1}+1,\mathrm{h}(\rho^{\prime})]\)_,_ \[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f_{k})^{2}(X_{\rho^{ \prime}})\big{]}\leq\exp\Big{(}-\varepsilon\big{(}\mathrm{h}(\rho^{\prime})+ k-C(\log(R)+1)-2\mathrm{h}^{*}\big{)}\Big{)}\mathbb{E}f_{k}^{2}(X),\] (62)
* _for_ \(k=k_{1}\)_,_ \[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f_{k_{1}})^{2}(X_{\rho^{\prime}}) \big{]}\leq\exp\Big{(}-\varepsilon\big{(}\mathrm{h}(\rho^{\prime})-k-C(\log( R)+1)\big{)}\Big{)}\mathbb{E}f_{k_{1}}^{2}(X),\text{ and }\] (63)
* _for_ \(k_{1}\leq m<k\leq\mathrm{h}(\rho^{\prime})\)_,_ \[\big{|}\mathbb{E}\big{[}f_{k}(X)f_{m}(X)\big{]}\big{|}\leq \exp\Big{(}-\frac{\varepsilon}{2}\big{(}k-C(\log(R)+1)-\mathrm{h}^{ *}\big{)}\Big{)}\sqrt{\mathbb{E}\big{[}f_{k}^{2}(X)\big{]}\mathbb{E}\big{[}f_ {m}^{2}(X)\big{]}},\] (64)Before we prove the Proposition, let us prove the following second moment bounds for the partial sums of \(\tilde{f}_{u}\).

**Lemma F.2**.: _There exists a constant \(C=C(M,d,c^{*})\geq 1\) so that the following holds. Consider the same description as stated in Proposition F.1 and \(k_{1}\geq\mathrm{h}^{*}+C(\log(R)+1)\). Let \((h,k)\) be a pair of integers satisfying \(k_{1}<h\leq k\leq l^{\prime}\). For \(u\in D_{k}(\rho^{\prime})\), let_

\[f_{h,u}(x)=\sum_{v\in D_{h}(u)}\tilde{f}_{v}(x).\]

_In other words,_

\[f_{h}(x)=\sum_{u\in D_{k}(\rho^{\prime})}f_{h,u}(x).\]

_The following holds: First, for \(u\in D_{k}(\rho^{\prime})\),_

\[\frac{1}{2}\sum_{v\in D_{h}(u)}\mathbb{E}\tilde{f}_{v}^{2}(X)\leq\mathbb{E}f_{ h,u}^{2}(X)\leq 2\sum_{v\in D_{h}(u)}\mathbb{E}\tilde{f}_{v}^{2}(X).\]

_Second,_

\[\frac{1}{4}\sum_{u\in L_{k}(\rho^{\prime})}\mathbb{E}f_{h,u}^{2}(X)\leq \mathbb{E}f_{h}^{2}(X)\leq 4\sum_{u\in L_{k}(\rho^{\prime})}\mathbb{E}f_{h,u}^{2 }(X).\]

Proof.: Let \(C_{0}=C_{0}(M,d,\varepsilon^{\prime})\) denote the constant introduced in the statement of the Lemma. Its precise value will be determined along the proof.

Let us fix \(u\in D_{k}(\rho^{\prime})\). Consider the following conditional expectation of \(f_{h,u}(x)\).

\[(\mathbb{E}_{h}f_{h,u})(x)=\mathbb{E}\big{[}f_{h,u}(X)\,\big{|}\,X_{v}=x_{v}\, :\,\mathrm{h}(v)\geq h\big{]}=\sum_{v\in D_{h}(\rho^{\prime})}(\mathbb{E}_{v} \tilde{f}_{v})(x_{v}).\]

Comparing the second moments of \(f_{h,u}(x)=\sum_{v\in D_{h}(u)}\tilde{f}_{v}(x)\) and \(\sum_{v\in D_{h}(u)}(\mathbb{E}_{v}\tilde{f}_{v})(x_{v})\) we get

\[\mathbb{E}\Big{[}\Big{(}\sum_{v\in D_{h}(u)}\tilde{f}_{v}(X)\Big{)} ^{2}\Big{]}= \sum_{v\in D_{h}(u)}\mathbb{E}\big{[}\tilde{f}_{v}^{2}(X)\big{]}+ \sum_{v,v^{\prime}\in D_{h}(u)\,:\,v\neq v^{\prime}}\mathbb{E}\Big{[}\tilde{f }_{v}(X)\tilde{f}_{v^{\prime}}(X)\Big{]}\] \[= \sum_{v\in D_{h}(u)}\mathbb{E}\big{[}\tilde{f}_{v}^{2}(X)\big{]} +\sum_{(v,v^{\prime})\in(D_{h}(u))^{2}\,:\,v\neq v^{\prime}}\mathbb{E}\Big{[} \mathbb{E}\Big{[}(\mathbb{E}_{v}\tilde{f}_{v})(X)(\mathbb{E}_{v^{\prime}} \tilde{f}_{v^{\prime}})(X)\,\Big{|}\,X_{\mathfrak{p}(v,v^{\prime})}\Big{]} \Big{]}\] \[= \mathbb{E}\Big{[}\Big{(}\sum_{v\in D_{h}(u)}(\mathbb{E}_{v} \tilde{f}_{v})(X)\Big{)}^{2}\Big{]}+\sum_{v\in D_{h}(u)}\Big{(}\mathbb{E}\big{[} \tilde{f}_{v}^{2}(X)\big{]}-\mathbb{E}\big{[}(\mathbb{E}_{v}\tilde{f}_{v})^{2 }(X)\big{]}\Big{)}\] \[\geq \sum_{v\in D_{h}(u)}\Big{(}1-\exp\Big{(}-\varepsilon\big{(}h-C_{E.1}(1+\log(R))-\mathrm{h}^{*}\big{)}\Big{)}\Big{)}\mathbb{E}\big{[}\tilde{f}_{v} ^{2}(X)\big{]},\] (65)

where the last inequality follow from Proposition E.1 and \(C_{E.1}\) is the constant \(C\) introduced in Proposition E.1.

Here we impose the **first assumption** on \(C_{0}\):

\[C_{0}>10\max\{\varepsilon^{-1},C_{E.1}\}.\]

Then, due to \(k_{1}\geq\mathrm{h}^{*}+C_{0}(\log(R)+1)\), we have

\[\exp(-\varepsilon(h-C_{E.1}(1+\log(R))-\mathrm{h}^{*})))\leq\exp(-\varepsilon(k _{1}-C_{E.1}(1+\log(R))-\mathrm{h}^{*})))\leq\exp(-\varepsilon\cdot 0.9C_{0})\leq 1/2,\]

and thus (65) can be simplified to

\[\mathbb{E}\big{[}f_{h,u}^{2}(X)\big{]}\geq \frac{1}{2}\sum_{v\in D_{h}(u)}\mathbb{E}\big{[}\tilde{f}_{v}^{2 }(X)\big{]}.\] (66)With the lower bound been established, the upper bound can also be derived in the same fashion. Let us first recycle the first three lines of (65):

\[\mathbb{E}\Big{[}\Big{(}\sum_{v\in D_{h}(u)}\tilde{f}_{v}(X)\Big{)}^ {2}\Big{]}= \mathbb{E}\Big{(}\sum_{v\in D_{h}(u)}(\mathbb{E}_{v}\tilde{f}_{v})(X) \Big{)}^{2}+\sum_{v\in D_{h}(u)}\mathbb{E}(\tilde{f}_{v}(X))^{2}-\mathbb{E}( \mathbb{E}_{v}\tilde{f}_{v})^{2}(X)\] \[\leq \mathbb{E}\Big{(}\sum_{v\in D_{h}(u)}(\mathbb{E}_{v}\tilde{f}_{v} )(X)\Big{)}^{2}+\sum_{v\in D_{h}(u)}\mathbb{E}(\tilde{f}_{v}(X))^{2}.\]

Notice that we can apply Lemma C.2 for the first summand in the above expression.

\[\mathbb{E}\Big{(}\sum_{v\in D_{h}(u)}(\mathbb{E}_{v}\tilde{f}_{v} )(X)\Big{)}^{2}=\mathbb{E}\Big{(}\sum_{v\in D_{h}(u)}(\mathbb{E}_{v}\tilde{f} _{v})(X_{v})\Big{)}^{2}\leq C_{C.2}R\sum_{v\in D_{h}(u)}(\mathbb{E}_{v}\tilde{f} _{v})^{2}(X_{v})\]

where \(C_{C.2}\) is the constant introduced in Lemma C.2. Again, applying the estimate from Proposition E.1 we have

\[C_{C.2}R\sum_{v\in D_{h}(u)}(\mathbb{E}_{v}\tilde{f}_{v})^{2}(X_{v})\leq C_{ C.2}R\exp\Big{(}-2\varepsilon\big{(}h-C_{E.1}(1+\log(R))-\mathrm{h}^{*}\big{)} \Big{)}\sum_{v\in D_{h}(u)}\mathbb{E}\tilde{f}_{v}^{2}(X).\]

Here we impose the **second assumption** on \(C_{0}\) that

\[C_{0}\geq C_{E.1}+\frac{1+\log(C_{C.2})}{2\varepsilon}.\] (67)

Then, relying on \(h>k_{1}\geq\mathrm{h}^{*}+C_{0}(\log(R)+1)\),

\[C_{C.2}R\exp\Big{(}-2\varepsilon\big{(}h-C_{E.1}(1+\log(R))- \mathrm{h}^{*}\big{)}\Big{)}\] \[\leq \exp\Big{(}-2\varepsilon\Big{(}h-C_{E.1}(1+\log(R))-\mathrm{h}^{ *}-\frac{\log(C_{C.2})+\log(R)}{2\varepsilon}\Big{)}\Big{)}\] \[\leq 1,\]

which in turn implies

\[\mathbb{E}(f_{h,u}(X))^{2}\leq 2\sum_{v\in D_{h}(u)}\mathbb{E}(\tilde{f}_{v}(X))^{2}.\]

Now it remains to show the second statement. Notice that \(f_{h}=f_{h,\rho^{\prime}}\), we immediately have

\[\frac{1}{2}\sum_{v\in D_{h}(\rho^{\prime})}\mathbb{E}\tilde{f}_{v}^{2}(X)\leq \mathbb{E}f_{h}^{2}(X)\leq 2\sum_{v\in D_{h}(\rho^{\prime})}\mathbb{E}\tilde{f}_{v}^{2}(X)\]

Together with

\[\frac{1}{2}\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}f_{k,u}^{2}(X)\leq\sum_{ v\in D_{h}(\rho^{\prime})}\mathbb{E}\tilde{f}_{v}^{2}(X)\leq 2\sum_{u\in D_{k}(\rho^{ \prime})}\mathbb{E}f_{k,u}^{2}(X),\]

the second statement of the lemma follows.

Proof of Proposition e.1.: Let \(C_{0}=C_{0}(M,d,c^{*})\) denote the constant introduced in the statement of the Lemma. Its precise value will be determined along the proof. Let us make the **first assumption** on \(C_{0}\) that

\[C_{0}\geq C_{F.2},\]

where \(C_{F.2}\) is the constant introduced in Lemma F.2. Now, we could apply the statements of the Lemma.

**Part 1: Derivation of \((\ref{eq:1})\).**Fix \(k\in[k_{1}+1,l^{\prime}]\). Applying Lemma F.2 with the parameters \(h\) and \(k\) in the Lemma setting to be \(k\),

\[\mathbb{E}(f_{k}(X))^{2}\geq \frac{1}{2}\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}(\tilde{f}_{u }(X))^{2}.\] (68)

The next step is to compare the sum of \(\mathbb{E}\tilde{f}_{u}^{2}(X)\) with \(\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f_{k})^{2}(X_{\rho^{\prime}})\big{]}\). By Jenson's inequality,

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f_{k})^{2}(X_{\rho^{ \prime}})\big{]}= \mathbb{E}\Big{[}\Big{(}\sum_{u\in D_{k}(\rho^{\prime})}(\mathbb{ E}_{\rho^{\prime}}\tilde{f}_{u})(X_{\rho^{\prime}})\Big{)}^{2}\Big{]}\] \[\leq \mathbb{E}\Big{[}|D_{k}(\rho^{\prime})|\sum_{u\in D_{k}(\rho^{ \prime})}(\mathbb{E}_{\rho^{\prime}}\tilde{f}_{u})^{2}(X_{\rho^{\prime}}) \Big{]}\] \[= |D_{k}(\rho^{\prime})|\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E} \big{[}(\mathbb{E}_{\rho^{\prime}}\tilde{f}_{u})^{2}(X_{\rho^{\prime}})\big{]}.\]

For each summand, we can apply (8) from Lemma A.5 to get the following estimate.

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}\tilde{f}_{u})^{2}(X_{\rho^{ \prime}})\big{]}\leq C_{A.5}(l^{\prime}-k)^{2q}\lambda^{2(l^{\prime}-k)}\mathbb{E} \big{[}(\mathbb{E}_{u}\tilde{f}_{u})^{2}(X_{u})\big{]}\]

where \(C_{A.5}\) is the constant introduced in the Lemma. Together with \(|D_{k}(\rho^{\prime})|\leq Rd^{l^{\prime}-k}\) from the assumption on \(T\) and \(d\lambda^{2}\leq\exp(-1.1\varepsilon)\) from the definiton of \(\varepsilon\),

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f_{k})^{2}(X_{\rho^{ \prime}})\big{]}\leq C_{A.5}(l^{\prime}-k)^{2q}R\exp(-1.1\varepsilon(l^{\prime}-k)) \sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\big{[}(\mathbb{E}_{u}\tilde{f}_{u })^{2}(X_{u})\big{]}\] \[\leq \frac{1}{2}\exp\Big{(}-\varepsilon\Big{(}l^{\prime}-C^{\prime}(1 +\log(R))-k\Big{)}\Big{)}\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\big{[}( \mathbb{E}_{u}\tilde{f}_{u})^{2}(X_{u})\big{]}\] (69)

where we set

\[C^{\prime}=1+\log\big{(}1+2C_{A.5}\max_{n\in\mathbb{N}}n^{2q}\exp(-0.1 \varepsilon n)\big{)}<+\infty.\]

By Proposition E.1 we have

\[\mathbb{E}\big{[}(\mathbb{E}_{u}\tilde{f}_{u})^{2}(X)\big{]}\leq\exp(-2 \varepsilon(k-C_{E.1}(1+\log(R))-\mathrm{h}^{*}))\mathbb{E}(\tilde{f}_{u}(X))^ {2},\] (70)

where \(C_{E.1}\) is the constant \(C\) introduced in the Proposition. Substituting this inequality into (69), together with (68) from first step,

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f_{k})^{2}(X_{\rho^{ \prime}})\big{]}\leq \frac{1}{2}\exp\Big{(}-\varepsilon\Big{(}l^{\prime}+k-(C^{\prime }+C_{E.1})(\log(R)+1)-2\mathrm{h}^{*}\Big{)}\Big{)}\sum_{u\in D_{k}(\rho^{\prime })}\mathbb{E}(\tilde{f}_{u}(X))^{2}\] \[\leq \exp(-\varepsilon(l^{\prime}+k-(C^{\prime}+C_{E.1})(\log(R)+1)-2 \mathrm{h}^{*}))\mathbb{E}(f_{k}(X))^{2}.\]

Now, we impose the **second assumption** on \(C_{0}\) that

\[C_{0}\geq(C^{\prime}+C_{E.1}),\]

we finished the proof of (62).

#### Part 2: Derivation of (63).

Let us consider

\[h_{k_{1}}(x):=(\mathbb{E}_{k_{1}}f_{k_{1}})(x)=\mathbb{E}\big{[}f_{k_{1}}(X) \,\big{|}\,X_{u}=x_{u}\,:\,u\in D_{k_{1}}(\rho^{\prime})\big{]}.\]

In other words, we may view \(h_{k_{1}}(x)\) as a linear function with variables \(x_{u}\) for \(u\in D_{k_{1}}(\rho^{\prime})\) with \(\mathbb{E}h_{k_{1}}(X)=\mathbb{E}f_{k_{1}}(X)=0\). Then,

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f_{k_{1}})^{2}(X_{ \rho^{\prime}})\big{]}=\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}h_{k_{1}})^ {2}(X_{\rho^{\prime}})\big{]}\leq\exp\big{(}-\varepsilon(\mathrm{h}(\rho^{ \prime})-k_{1}-C_{B.5})\big{)}\mathbb{E}h_{k_{1}}^{2}(X)\\ \leq\exp\big{(}-\varepsilon(\mathrm{h}(\rho^{\prime})-k_{1}-C_{B.5 })\big{)}\mathbb{E}f_{k_{1}}^{2}(X)\]The first inequality follows from Proposition B.5. The second inequality follows from Jensen's inequality. Here we impose the **third assumption** on \(C_{0}\) that

\[C_{0}\geq C_{B.5},\]

the derivation of (63) follows.

**Part 3: Derivation of (64)**

For \(w\leq\rho^{\prime}\) with \(m\leq\mathrm{h}(w)\leq k\), let

\[f_{m,w}(x)=\sum_{v\in D_{k}(w)}\tilde{f}_{v}(x).\]

Let us make a remark that either by second property of \(f\) from Lemma D.1 when \(m=k_{1}\) or by Lemma F.2 in the case when \(m>k_{1}\), we have the following: For \(w\leq\rho^{\prime}\) and \(m\leq k^{\prime}\leq\mathrm{h}(w)\),

\[\Big{(}\sum_{u\in D_{k^{\prime}}(w)}\mathbb{E}f_{m,u}^{2}(X)\Big{)}^{1/2}\leq C _{D.1}R^{2}\big{(}\mathbb{E}f_{m,w}^{2}(X)\big{)}^{1/2}.\] (71)

With this notation,

\[\mathbb{E}f_{k}(X)f_{m}(X)\] \[= \sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\tilde{f}_{u}(X)f_{m,u} (X)+\sum_{u,u^{\prime}\in D_{k}(\rho^{\prime})\,:\,u\neq u^{\prime}}\mathbb{E} \tilde{f}_{u}(X)f_{m,u^{\prime}}(X)\] \[= \mathbb{E}\Big{[}\Big{(}\sum_{u\in D_{k}(\rho^{\prime})}( \mathbb{E}_{u}\tilde{f}_{u})(X)\Big{)}\Big{(}\sum_{u\in D_{k}(\rho^{\prime})}( \mathbb{E}_{u}f_{m,u})(X)\Big{)}\Big{]}+\sum_{u\in D_{k}(\rho^{\prime})} \mathbb{E}\tilde{f}_{u}(X)f_{m,u}(X)\] (72) \[-\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\Big{[}(\mathbb{E}_{u }\tilde{f}_{u})(X)(\mathbb{E}_{u}f_{m,u})(X)\Big{]}.\]

We will estimate the three summands individually.

**Part 3.1: Estimating first summand of (72)** First, we apply Cauchy-Schwarz inequality,

\[\bigg{|}\mathbb{E}\Big{[}(\sum_{u\in D_{k}(\rho^{\prime})}( \mathbb{E}_{u}\tilde{f}_{u})(X))(\sum_{u\in D_{k}(\rho^{\prime})}(\mathbb{E}_ {u}f_{m,u})(X))\Big{]}\bigg{|}= \Big{|}\mathbb{E}\Big{[}(\mathbb{E}_{k}f_{k})(X)(\mathbb{E}_{k} f_{m})(X)\Big{]}\Big{|}\] \[\leq \sqrt{\mathbb{E}\big{[}(\mathbb{E}_{k}f_{k})^{2}(X)\big{]}}\sqrt {\mathbb{E}\big{[}(\mathbb{E}_{k}f_{m})^{2}(X)\big{]}}.\] (73)

Now, combining (70) and (68), we have

\[\sqrt{\mathbb{E}\big{[}(\mathbb{E}_{k}f_{k})^{2}(X)\big{]}}\leq\sqrt{2}\exp \big{(}-\varepsilon\big{(}k-C_{E.1}(1+\log(R))-\mathrm{h}^{*}\big{)}\big{)}( \mathbb{E}f_{k}^{2}(X))^{1/2}.\] (74)

By setting

\[C_{1}=C_{E.1}+\frac{1}{\varepsilon}\Big{(}\frac{1}{2}\log(2)+\log(2C_{D.1})+2 \Big{)},\]

we can conclude that

\[\bigg{|}\mathbb{E}\Big{[}(\sum_{u\in D_{k}(\rho^{\prime})}( \mathbb{E}_{u}\tilde{f}_{u})(X))(\sum_{u\in D_{k}(\rho^{\prime})}(\mathbb{E}_ {u}f_{m,u})(X))\Big{]}\bigg{|}\] \[\leq \exp(-\varepsilon(k-C_{1}(\log(R)+1)-\mathrm{h}^{*}))\sqrt{ \mathbb{E}f_{k}^{2}(X)\mathbb{E}f_{m}^{2}(X)},\] (75)

**Part 3.2: Estimating second summand of (72)** For the second summand of (72), we begin with the estimate for each \(u\in D_{k}(\rho^{\prime})\):

\[\mathbb{E}|\tilde{f}_{u}(X)f_{m,u}(X)|\leq\sum_{i\in[d_{u}]}\mathbb{E}|\tilde {f}_{u}(X)f_{m,u_{i}}(X)|.\]Since for each \(i\in[d_{u}]\) we have \(f_{m,u_{i}}(x)=f_{m,u_{i}}(x_{\leq u_{i}})\), we apply (39) from Proposition E.1 to \(\tilde{f}_{u}\) and \(a(x)=f_{m,u_{i}}(x)\) to get

\[\sum_{i\in[d_{u}]}\mathbb{E}|\tilde{f}_{u}(X)f_{m,u_{i}}(X)|\leq\sum_{i\in[d_{u }]}\exp\Big{(}-\frac{\varepsilon}{2}\big{(}k-C_{E.1}(\log(R)+1)-\mathrm{h}^{*} \big{)}\Big{)}(\mathbb{E}\tilde{f}_{u}^{2}(X))^{1/2}(\mathbb{E}f_{m,u_{i}}^{2}( X))^{1/2},\]

where \(C_{E.1}\) is the constant introduced in the Proposition. Applying Jensen's inequality and (71) with \(w=u\) and \(k^{\prime}=k-1\),

\[\sum_{i\in[d_{u}]}(\mathbb{E}f_{m,u_{i}}^{2}(X))^{1/2}\leq d_{u}^{1/2}\Big{(} \sum_{i\in[d_{u}]}\mathbb{E}f_{m,u_{i}}^{2}(X)\Big{)}^{1/2}\leq(Rd)^{1/2}C_{D. 1}R^{2}\big{(}\mathbb{E}f_{m,u}^{2}(X)\big{)}^{1/2}.\]

Hence,

\[\mathbb{E}|\tilde{f}_{u}(X)f_{m,u}(X)|\leq (Rd)^{1/2}C_{D.1}R^{2}\exp\Big{(}-\frac{\varepsilon}{2}\big{(}k-C_ {E.1}(\log(R)+1)-\mathrm{h}^{*}\big{)}\Big{)}(\mathbb{E}\tilde{f}_{u}^{2}(X)) ^{1/2}\big{(}\mathbb{E}f_{m,u}^{2}(X)\big{)}^{1/2}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}\big{(}k-C_{2}(\log(R)+1)- \mathrm{h}^{*}\big{)}\Big{)}(\mathbb{E}\tilde{f}_{u}^{2}(X))^{1/2}(\mathbb{E} f_{m,u}^{2}(X))^{1/2},\] (76)

where

\[C_{2}=\frac{2}{\varepsilon}\Big{(}\frac{3}{2}+\frac{1}{2}\log(d)+\log(C_{D.1} )\Big{)}+C_{E.1}.\]

Now, returning to the summation, we apply (76) and Cauchy-Schwarz inequality to get

\[\Big{|}\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\tilde{f}_{u}(X )f_{m,u}(X)\Big{|}\] \[\leq \sum_{u\in D_{k}(\rho^{\prime})}\exp\Big{(}-\frac{\varepsilon}{2 }(k-C_{2}(\log(R)+1)-\mathrm{h}^{*})\Big{)}(\mathbb{E}\tilde{f}_{u}^{2}(X))^{ 1/2}(\mathbb{E}f_{m,u}^{2}(X))^{1/2}\] \[\leq 2C_{D.1}R^{2}\exp\Big{(}-\frac{\varepsilon}{2}(k-C_{2}(\log(R)+1 )-\mathrm{h}^{*})\Big{)}\Big{(}\mathbb{E}f_{k}^{2}(X)\big{)}^{1/2}\big{(} \mathbb{E}f_{m}^{2}(X)\big{)}^{1/2}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(k-C_{3}(\log(R)+1)-\mathrm{h}^{ *})\Big{)}\sqrt{\mathbb{E}f_{k}^{2}(X)\mathbb{E}f_{m}^{2}(X)}.\] (77)

In the derivation above, we applied Lemma F.2 for the term \(\Big{(}\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\tilde{f}_{u}^{2}(X)\Big{)}^ {1/2}\) and (71) for the term \(\Big{(}\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}f_{m,u}^{2}(X)\Big{)}^{1/2}\) in the second to last inequality. The constant \(C_{2}\) in the last inequality is defined as

\[C_{3}=\frac{2}{\varepsilon}(\log(2C_{D.1})+2)+C_{2}.\]

**Part 3.3: Estimating third summand of (72)** It remains to bound the third summand, it can be reduced to the upper bound for first summand. Applying the Cauchy-Schwarz inequality and Holder's inequality we have

\[\Big{|}\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\Big{[}(\mathbb{E }_{u}\tilde{f}_{u})(X)(\mathbb{E}_{u}f_{m,u})(X)\Big{]}\Big{|}\] \[\leq \mathbb{E}\Big{[}\Big{|}\sum_{u\in D_{k}(\rho^{\prime})}(\mathbb{ E}_{u}\tilde{f}_{u})(X)(\mathbb{E}_{u}f_{m,u})(X)\Big{]}\Big{]}\] \[\leq \mathbb{E}\Big{[}\Big{(}\sum_{u\in D_{k}(\rho^{\prime})}(\mathbb{ E}_{u}\tilde{f}_{u})^{2}(X)\Big{)}^{1/2}\Big{(}\sum_{u\in D_{k}(\rho^{\prime})}( \mathbb{E}_{u}f_{m,u})^{2}(X)\Big{)}^{1/2}\Big{]}\] \[\leq \sqrt{2}\exp\big{(}-\varepsilon\big{(}k-C_{E.1}(1+\log(R))- \mathrm{h}^{*}\big{)}\big{)}\cdot C_{D.1}R^{2}\sqrt{\mathbb{E}f_{k}^{2}(X) \mathbb{E}f_{m}^{2}(X)}.\]where in the last inequality we applied (74) and (71).

By setting

\[C_{4}=\frac{1}{\varepsilon}\Big{(}\frac{1}{2}\log(2)+\log(C_{D.1})+2\Big{)},\]

we conclude that

\[\Big{|}\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\Big{[}(\mathbb{E}_{u}\tilde{f} _{u})(X)(\mathbb{E}_{u}f_{m,u})(X)\Big{]}\Big{|}\leq \exp\big{(}-\varepsilon\big{(}k-C_{4}(1+\log(R))-\mathrm{h}^{*} \big{)}\big{)}\sqrt{\mathbb{E}f_{k}^{2}(X)\mathbb{E}f_{m}^{2}(X)}.\] (78)

Now, combining the three estimates of the summands (75), (77), and (78) for (72) we conclude that

\[|\mathbb{E}f_{k}(X)f_{m}(X)|\leq \exp\Big{(}-\frac{\varepsilon}{2}\big{(}k-C_{5}(\log(R)+1)-\mathrm{ h}^{*}\big{)}\Big{)}\sqrt{\mathbb{E}f_{k}^{2}(X)\mathbb{E}f_{m}^{2}(X)},\]

where

\[C_{5}:=\frac{2}{\varepsilon}\log(3)+\max\{C_{1},C_{3},C_{4}\}.\]

Now we impose the **forth assumption** on \(C_{0}\) that

\[C_{0}\geq C_{5},\]

and (64) follows.

### Proof of Theorem b.6

Let \(C_{0}=C_{0}(M,d,c^{*})\) denote the constant introduced in the statement of the Lemma. Its precise value will be determined along the proof.

Let \(k_{1}\) be a positive integer with the precise value to be determined later. Here we impose our **first assumption** on \(k_{1}\) that

\[k_{1}\geq C_{F.1}(\log(R)+1)+\mathrm{h}^{*}\]

where \(C_{F.1}\) is a constant that appears in Proposition F.1.

Now, let us consider a function \(f\) described in the Theorem. Without lose of generality, we may assume \(\mathbb{E}f(X)=0\). Then, it is equivalent to estimate the second moments.

Further, let us assume \(\mathrm{h}(\rho^{\prime})\geq k_{1}\) and the decomposition of \(f\) according to Lemma D.1:

\[f(x)=\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}f_{k}(X).\] (79)

Our first goal is to show

\[\mathbb{E}f(X)^{2}\simeq\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}f_{k}^{2} (X),\]

by showing \(\mathbb{E}f_{k}(X)f_{m}(X)\) is insignificant whenever \(k\neq m\).

For \(k_{1}\leq m<k\), by Propostion F.1,

\[2|\mathbb{E}f_{k}(X)f_{m}(X)|\leq 2\exp\Big{(}-\frac{\varepsilon}{2}\big{(}k-C_{F.1}(\log(R)+1)- \mathrm{h}^{*}\big{)}\Big{)}(\mathbb{E}f_{k}^{2}(X))^{1/2}(\mathbb{E}f_{m}^{2 }(X))^{1/2}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}\big{(}k-C_{F.1}(\log(R)+1)- \mathrm{h}^{*}\big{)}\Big{)}\mathbb{E}f_{k}^{2}(X)\] \[+\exp\Big{(}-\frac{\varepsilon}{2}\big{(}k-C_{F.1}(\log(R)+1)- \mathrm{h}^{*}\big{)}\Big{)}\mathbb{E}f_{m}^{2}(X).\]

Applying the above inequality to bound the second moment of \(f(X)\) we get

\[\mathbb{E}f^{2}(X)= \mathbb{E}\sum_{k,m\in[k_{1},\mathrm{h}(\rho^{\prime})]}\mathbb{E }f_{k}(X)f_{m}(X)\] \[\geq \sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}\mathbb{E}f_{k}^{2}( X)\cdot\Big{(}1-\sum_{s\in[k_{1},\mathrm{h}(\rho^{\prime})]}\exp\Big{(}-\frac{ \varepsilon}{2}\big{(}s-C_{F.1}(\log(R)+1)-\mathrm{h}^{*}\big{)}\Big{)}\Big{)}.\]Notice that there exists \(t_{0}\) which depends on \(\varepsilon\) so that

\[\sum_{t=t_{0}}^{\infty}\exp\Big{(}-\frac{\varepsilon}{2}t\Big{)}\leq\frac{1}{2}.\]

By setting

\[k_{1}:=\lceil\mathrm{h}^{*}+C_{F.1}(\log(R)+1)+t_{0}\rceil,\]

we get

\[\mathbb{E}f^{2}(X)\geq\frac{1}{2}\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]} \mathbb{E}f_{k}^{2}(X).\] (80)

Our second goal is comparing \(\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f_{k})^{2}(X_{\rho^{\prime}})\big{]}\) and \(\sum_{k\in[k_{1},\infty]}\mathbb{E}f_{k}^{2}(X)\). Starting with the variance and \(\ell_{\infty}\) norm comparison from Lemma A.5,

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f)^{2}(X_{\rho^{\prime }})\big{]}\leq \mathbb{E}\Big{[}\Big{(}\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime })]}\max_{\theta_{k}\in[q]}\big{|}(\mathbb{E}_{\rho^{\prime}}f_{k})(\theta_{k} )\big{|}\Big{)}^{2}\Big{]}\] \[= \Big{(}\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}\max_{\theta_ {k}\in[q]}\big{|}(\mathbb{E}_{\rho^{\prime}}f_{k})(\theta_{k})\big{|}\Big{)} ^{2}\] \[\leq C_{A.5}\Big{(}\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}\sqrt{ \mathbb{E}(\mathbb{E}_{\rho^{\prime}}f_{k})^{2}(X_{\rho})}\Big{)}^{2},\]

where \(C_{A.5}\) is the constant introduced in the Lemma.

By (62), from Proposition F.1, for \(k\in[k_{1}+1,\mathrm{h}(\rho^{\prime})]\),

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f_{k})^{2}(X_{\rho^{\prime}}) \big{]}\leq\exp\big{(}-\varepsilon(\mathrm{h}-\mathrm{h}^{*})\big{)}\cdot\exp \Big{(}-\varepsilon(\mathrm{h}(\rho^{\prime})-C_{F.1}(\log(R)+1)-\mathrm{h}^{* })\Big{)}\mathbb{E}f_{k}^{2}(X)\]

And for \(k=k_{1}=\lceil\mathrm{h}^{*}+C_{F.1}(\log(R)+1)+t_{0}\rceil\), we apply (63) to get

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f_{k_{1}})^{2}(X_{ \rho^{\prime}})\big{]}\leq \exp\big{(}-\varepsilon(\mathrm{h}(\rho^{\prime})-k_{1}-C_{F.1}( \log(R)+1))\big{)}\mathbb{E}f_{k_{1}}^{2}(X)\] \[\leq \exp\big{(}-\varepsilon(-t_{0}-1)\big{)}\] \[\cdot\exp\Big{(}-\varepsilon(\mathrm{h}(\rho^{\prime})-2C_{F.1}( \log(R)+1)-\mathrm{h}^{*})\Big{)}\mathbb{E}f_{k_{1}}^{2}(X).\]

Substituting these estimate and by Cauchy-Schwarz inequality we have

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f)^{2}(X_{\rho^{\prime }})\big{]}\leq C_{A.5}\exp\Big{(}-\varepsilon(\mathrm{h}(\rho^{\prime})-2C_{F.1}( \log(R)+1)-\mathrm{h}^{*})\Big{)}\] \[\cdot\underbrace{\Big{(}\exp(\varepsilon(t_{0}+1))+\sum_{t=0}^{ \infty}\exp(-\varepsilon t)\Big{)}}_{:=C_{1}}\cdot\sum_{k\in[k_{1},\mathrm{h}( \rho^{\prime})]}\mathbb{E}f_{k}^{2}(X)\] \[\leq C_{A.5}C_{1}2\exp\Big{(}-\varepsilon(\mathrm{h}(\rho^{\prime})-2C _{F.1}(\log(R)+1)-\mathrm{h}^{*})\Big{)}\mathbb{E}f^{2}(X).\]

Now, by taking

\[C_{0}\geq\max\Big{\{}2C_{F.1}+\frac{1}{\varepsilon}\log(C_{A.5}C_{1}2)\,,\,C_ {F.1}+t_{0}+1\Big{\}},\]

we conclude that

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f)^{2}(X_{\rho^{\prime}})]\leq\exp \Big{(}-\varepsilon(\mathrm{h}(\rho^{\prime})-C_{0}(\log(R)+1)-\mathrm{h}^{*} )\Big{)}\mathbb{E}f^{2}(X).\]

It remains to show the case when \(\mathrm{h}(\rho^{\prime})\leq k_{1}\). From the assumption that \(C_{0}\geq C_{F.1}+t_{0}+1\) and \(k_{1}\leq\mathrm{h}^{*}+C_{F.1}(\log(R)+1)+t_{0}+1\), we have

\[\exp\Big{(}-\varepsilon(\mathrm{h}(\rho^{\prime})-C_{0}(\log(R)+1)-\mathrm{h}^{ *})\geq 1.\]

Hence, the statement follows directly from Jensen's inequality.

General Case: Base Case

Now, we want to establish Theorem 1.6, which does not rely on the assumption \(c_{M}>0\). Let us first establish analogues of Assumption B.3 (the inductive assumption), Proposition B.5 (the base case), and Theorem B.1 (the inductive step) in the general case.

**Assumption G.1**.: By stating that \(\mathcal{A}\) satisfies this assumption with given parameter \(\mathrm{h}^{\circ}\), we mean \(\mathcal{A}_{1}\subseteq\mathcal{A}\subseteq\mathbf{2}^{L}\backslash\{\emptyset\}\) is closed under decomposition, and the following holds:

For every \(u\in T\) and any \(\mathcal{A}_{\leq u}\)-polynomials functions \(f\) and \(g\), we have

\[\mathrm{Var}\big{[}(\mathbb{E}_{u}f)(X)\big{]}\leq\exp(-\varepsilon(\mathrm{h}( u)-\mathrm{h}^{\circ}))\mathrm{Var}\big{[}f(X)\big{]}.\] (81)

Further, suppose \(\mathbb{E}f=\mathbb{E}g=0\) and \(\mathrm{h}(u)\geq\mathrm{h}^{\circ}\). Notice that by the Markov Property, \((\mathbb{E}_{u}fg)(x)\), \((\mathbb{E}_{u}f^{2})(x)\), and \((\mathbb{E}_{u}g^{2})(x)\) are functions of \(x_{u}\). Then,

\[\max_{\theta\in[q]}|(\mathbb{E}_{u}fg)(\theta)-\mathbb{E}fg|\leq\exp\Big{(}- \frac{\varepsilon}{2}(\mathrm{h}(u)-\mathrm{h}^{\circ})\Big{)}\sqrt{\min_{ \theta}(\mathbb{E}_{u}f^{2})(\theta)\min_{\theta^{\prime}}(\mathbb{E}_{u}g^{ 2})(\theta)}.\] (82)

The main difference of this assumption and Assumption B.3 is the difference of (82) and (16).

**Proposition G.2**.: _Consider the rooted tree \(T\) and transition matrix \(M\) described in Theorem 1.6. There exists \(C=C(M,d)\geq 1\) such that \(\mathcal{A}_{1}\) satisfies Assumption G.1 with some parameter \(\mathrm{h}^{\circ}\) satisfying_

\[\mathrm{h}^{\circ}\leq C(\log(R)+1).\] (83)

**Theorem G.3**.: _Consider the rooted tree \(T\) and transition matrix \(M\) described in Theorem 1.6. There exists \(C=C(M,d)>1\) so that the following holds. Suppose \(\mathcal{A}\) satisfies Assumption G.1 with some parameter \(\mathrm{h}^{\circ}\). Let \(\mathcal{B}=\mathcal{B}(\mathcal{A})\) (see Definition 1.11). Then, \(\mathcal{B}\) satisfies Assumption G.1 with parameter \(\mathrm{h}^{\circ}+C(\log(R)+1)\)._

Proof of Theorem 1.6.: The proof of Theorem 1.6 is analogous to that of Theorem B.1, employing a similar strategy by leveraging Proposition G.2 and Theorem G.3 in the former, and Proposition B.5 and Theorem B.6 in the latter.

In this section we will prove the Base Case Proposition G.2.

**Lemma G.4**.: _There exists a constant \(C=C(M,\varepsilon)\geq 1\) so such that for any \(\rho^{\prime}\in T\) and \(0\leq m\leq\mathrm{h}(\rho^{\prime})\):_

_Consider two degree 1 polynomials \(f\) and \(g\) with variables \((x_{u}\,:\,u\in D_{m}(\rho^{\prime}))\). Suppose_

\[f(X)=\sum_{u\in D_{m}(\rho^{\prime})}f_{u}(X)\text{ almost surely,}\]

_where \(f_{u}(x)=f_{u}(x_{u})\) and \(\mathbb{E}[f_{u}(X)]=0\), and we assume the same conditions for the polynomial \(g\) and \(g_{u}\). Then,_

\[\max_{\theta\in[q]}\big{|}(\mathbb{E}_{\rho^{\prime}}fg)(\theta)-\mathbb{E}fg \big{|}\leq CR\exp(-\varepsilon(\mathrm{h}(\rho^{\prime})-m))\sqrt{\sum_{u\in D _{m}(\rho^{\prime})}\mathbb{E}f_{u}^{2}(X)}\sqrt{\sum_{u\in D_{m}(\rho^{ \prime})}\mathbb{E}g_{u}^{2}(X)}.\]

Proof.: Let \(C_{0}=C_{0}(M,d)\) denote the constant introduced in the statement of the Lemma. Its value will be determined along the proof.

First of all,

\[\max_{\theta\in[q]}\big{|}(\mathbb{E}_{\rho^{\prime}}fg)(\theta)- \mathbb{E}fg\big{|}= \max_{\theta\in[q]}\Big{|}\sum_{u,v\in D_{m}(\rho^{\prime})}\Big{(}( \mathbb{E}_{\rho^{\prime}}f_{u}g_{v})(\theta)-\mathbb{E}f_{u}g_{v}\Big{|} \Big{)}\] \[\leq \sum_{u,v\in D_{m}(\rho^{\prime})}\max_{\theta\in[q]}\big{|}( \mathbb{E}_{\rho^{\prime}}f_{u}g_{v})(\theta)-\mathbb{E}f_{u}g_{v}\big{|}.\]Our proof will be carried out by estimating each summand. Fix any pair \(u,v\in D_{m}(\rho^{\prime})\) and consider

\[\|(\mathbb{E}_{\rho^{\prime}}f_{u}g_{v})-\mathbb{E}f_{u}g_{v}\|_{\infty}=\max_{ \theta}\big{|}\mathbb{E}_{\rho^{\prime}}\big{[}f_{u}(X)g_{v}(X)-\mathbb{E}f_{u} g_{v}\big{|}\,X_{\rho^{\prime}}=\theta\big{]}\big{|}.\]

Let \(w=\rho(u,v)\). Since \(f_{u}\) and \(g_{v}\) are functions of \(x_{\leq w}\), relying on the Markov Property we know the function \((\mathbb{E}_{w}f_{u}\cdot g_{v})(x)\) is a function of \(x_{w}\) with expected value \(\mathbb{E}f_{u}(X)g_{v}(X)\). With

\[(\mathbb{E}_{\rho^{\prime}}f_{u}g_{v})(x_{\rho^{\prime}})=\mathbb{E}\big{[}( \mathbb{E}_{w}f_{u}g_{v})(X_{w})\,\big{|}\,X_{\rho^{\prime}}=x_{\rho^{\prime}} \big{]},\]

applying (10) from Lemma A.5,

\[\|(\mathbb{E}_{\rho^{\prime}}f_{u}g_{v})-\mathbb{E}f_{u}g_{v}\|_{\infty}\leq C_{A.5}(\mathrm{h}(\rho^{\prime})-\mathrm{h}(w))^{q}\lambda^{\mathrm{h}( \rho^{\prime})-\mathrm{h}(w)}\big{\|}(\mathbb{E}_{w}f_{u}g_{v})(\theta)- \mathbb{E}f_{u}g_{v}\big{\|}_{\infty},\]

where \(C_{A.5}\) is the \(M\)-dependent constant introduced in the Lemma.

Next, we will estimate \(\big{\|}(\mathbb{E}_{w}f_{u}g_{v})(\theta)-\mathbb{E}f_{u}g_{v}\big{\|}_{\infty}\). In the case \(u\neq v\), there exists \(i\neq j\) such that \(u\leq w_{i}\) and \(v\leq w_{j}\), which in turn implies that \((X_{\leq u}\,|\,X_{w}=x_{w})\) and \((X_{\leq v}\,|\,X_{w}=x_{w})\) are jointly independent by the Markov Property. Thus,

\[(\mathbb{E}_{w}f_{u}g_{v})(\theta)=(\mathbb{E}_{w}f_{u})(\theta)(\mathbb{E}_{ w}g_{v})(\theta),\]

which implies

\[\max_{\theta\in[q]}|(\mathbb{E}_{w}f_{u}g_{v})(\theta)|\leq \max_{\theta\in[q]}|(\mathbb{E}_{w}f_{u})(\theta)|\cdot\max_{ \theta\in[q]}|(\mathbb{E}_{w}g_{v})(\theta)|\] \[\leq C_{A.5}^{3}(\mathrm{h}(w)-m)^{2q}\lambda^{2(\mathrm{h}(w)-m)} \sqrt{\mathbb{E}f_{u}^{2}(X)\mathbb{E}g_{v}^{2}(X)},\]

where we applied (10) and (9) from Lemma A.5 in the last inequality. If \(u=v\), then the same estimate follows immediately without relying on (10).

Now, we convert the above estimate to that of \(\|(\mathbb{E}_{w}f_{u}g_{v})(\theta)-\mathbb{E}f_{u}g_{v}\|_{\infty}\), which relies on the simple bound that \(|\mathbb{E}f_{u}(X)g_{v}(X)|\leq\max_{\theta\in[q]}|(\mathbb{E}_{w}f_{u}g_{v}) (\theta)|\). Thus,

\[\big{\|}(\mathbb{E}_{w}f_{u}g_{v})(\theta)-\mathbb{E}f_{u}g_{v} \big{\|}_{\infty}\leq 2\max_{\theta\in[q]}|(\mathbb{E}_{w}f_{u}g_{v})(\theta)|\] \[\leq 2C_{A.5}^{3}(\mathrm{h}(w)-m)^{2q}\lambda^{2(\mathrm{h}(w)-m)} \sqrt{\mathbb{E}f_{u}^{2}(X)\mathbb{E}g_{v}^{2}(X)}.\]

Together we conclude that for a pair \(u,v\in D_{m}(\rho^{\prime})\) with \(w=\rho(u,v)\),

\[\|(\mathbb{E}_{\rho^{\prime}}f_{u}g_{v})-\mathbb{E}f_{u}g_{v}\|_{\infty}\leq 2C_{A.5}^{4}(\mathrm{h}(w)-m)^{2q}(\mathrm{h}(\rho^{\prime})- \mathrm{h}(w))^{q}\lambda^{\mathrm{h}(\rho^{\prime})+\mathrm{h}(w)-2m}\sqrt{ \mathbb{E}f_{u}^{2}(X)g_{v}^{2}(X)}\] \[\leq 2C_{A.5}^{4}(\mathrm{h}(\rho^{\prime})-m)^{3q}\lambda^{\mathrm{h} (\rho^{\prime})+\mathrm{h}(w)-2m}\sqrt{\mathbb{E}f_{u}^{2}(X)g_{v}^{2}(X)}.\]

Relying on this estimate, we are ready to bound the \(l_{\infty}\)-norm of \((\mathbb{E}_{\rho^{\prime}}fg)(x_{\rho^{\prime}})-\mathbb{E}fg\).

\[\max_{\theta\in[q]}\big{|}(\mathbb{E}_{\rho^{\prime}}fg)(\theta)- \mathbb{E}fg\big{|}\] \[\leq \sum_{u,v\in D_{m}(\rho^{\prime})}\max_{\theta\in[q]}\big{|}( \mathbb{E}_{\rho^{\prime}}f_{u}g_{v})(\theta)-\mathbb{E}f_{u}g_{v}\big{|}\] \[= \sum_{k\in[m,\mathrm{h}(\rho^{\prime})]}\sum_{w\in D_{k}(\rho^{ \prime})}\sum_{u,v:\,\rho(u,v)=w}\max_{\theta\in[q]}\big{|}(\mathbb{E}_{\rho }f_{u}g_{v})(\theta)-\mathbb{E}f_{u}g_{v}\big{|}\] \[\leq \sum_{k\in[m,\mathrm{h}(\rho^{\prime})]}\sum_{w\in D_{k}(\rho^{ \prime})}\sum_{u,v:\,\rho(u,v)=w}2C_{A.5}^{4}(\mathrm{h}(\rho^{\prime})-m)^{3 q}\lambda^{\mathrm{h}(\rho^{\prime})+k-2m}\sqrt{\mathbb{E}f_{u}^{2}(X)g_{v}^{2}(X)}.\] (84)

Next, relaxing the condition \(\rho(u,v)=w\) in the summation,

\[(*) \leq\sum_{k\in[m,\mathrm{h}(\rho^{\prime})]}\sum_{w\in D_{k}( \rho^{\prime})}\sum_{u,v\in D_{m}(w)}2C_{A.5}^{4}(\mathrm{h}(\rho^{\prime})-m)^{ 3q}\lambda^{\mathrm{h}(\rho^{\prime})+k-2m}\sqrt{\mathbb{E}f_{u}^{2}(X)g_{v}^{2} (X)}\] \[= \sum_{k\in[m,\mathrm{h}(\rho^{\prime})]}\sum_{w\in D_{k}(\rho^{ \prime})}2C_{A.5}^{4}(\mathrm{h}(\rho^{\prime})-m)^{3q}\lambda^{\mathrm{h}(\rho^ {\prime})+k-2m}\big{(}\sum_{u\in D_{m}(w)}\sqrt{\mathbb{E}f_{u}^{2}(X)}\big{)} \big{(}\sum_{u\in D_{m}(w)}\sqrt{\mathbb{E}g_{u}^{2}(X)}\big{)}.\]Notice the inequality \(\sum_{i\in[n]}\frac{|t_{i}|}{n}\leq\sqrt{\sum_{i\in[n]}\frac{|t_{i}|^{2}}{n}}\) follows from Jenson's inequality applying to the function \(t\mapsto t^{2}\) and the uniform measure on \([n]\). Now apply this inequality to the collection \(\{\sqrt{\mathbb{E}f_{u}^{2}(X)}\}\) and \(\{\sqrt{\mathbb{E}g_{u}^{2}(X)}\}\) respectively, together with \(|D_{m}(w)|\leq Rd^{\mathrm{h}(w)-m},\) from our tree asssumption, we have

\[(*)\leq \sum_{k\in[m,\mathrm{h}(\rho^{\prime})]}\sum_{w\leq\rho^{\prime}: \,w\in D_{k}(\rho^{\prime})}2C_{A.5}^{4}(\mathrm{h}(\rho^{\prime})-m)^{3q} \lambda^{\mathrm{h}(\rho^{\prime})+k-2m}Rd^{k-m}\] \[\cdot\sqrt{\sum_{u\in D_{m}(w)}\mathbb{E}f_{u}^{2}(X)}\sqrt{\sum _{u\in D_{m}(w)}\mathbb{E}g_{u}^{2}(X)}\] \[\leq \sum_{k\in[m,\mathrm{h}(\rho^{\prime})]}2C_{A.5}^{4}(\mathrm{h}( \rho^{\prime})-m)^{3q}\lambda^{\mathrm{h}(\rho^{\prime})+k-2m}Rd^{k-m}\] \[\cdot\sqrt{\sum_{w\leq\rho^{\prime}:\,\mathrm{h}(w)=k}\sum_{u\in D _{m}(w)}\mathbb{E}f_{u}^{2}(X)}\cdot\sqrt{\sum_{w\leq\rho^{\prime}:\,\mathrm{ h}(w)=k}\sum_{u\in D_{m}(w)}\mathbb{E}g_{u}^{2}(X)}\] \[= \sum_{k\in[m,\mathrm{h}(\rho^{\prime})]}2C_{A.5}^{4}(\mathrm{h}( \rho^{\prime})-m)^{3q}\lambda^{\mathrm{h}(\rho^{\prime})+k-2m}Rd^{k-m}\sqrt{ \sum_{u\in D_{m}(\rho^{\prime})}\mathbb{E}f_{u}^{2}(X)}\sqrt{\sum_{u\in D_{m}( \rho^{\prime})}\mathbb{E}g_{u}^{2}(X)},\] (85)

where the last inequality follows from Cauchy-Schwarz inequality. Finally,

\[\sum_{k\in[m,\mathrm{h}(\rho^{\prime})]}2C_{A.5}^{4}(\mathrm{h}( \rho^{\prime})-m)^{3q}\lambda^{\mathrm{h}(\rho^{\prime})+k-2m}Rd^{k-m}\] \[\leq 2C_{A.5}^{4}R(\mathrm{h}(\rho^{\prime})-m)^{3q}\cdot(\mathrm{h} (\rho^{\prime})-m)\lambda^{\mathrm{h}(\rho^{\prime})-m}\max_{k\in[m,h(\rho^{ \prime})]}\lambda^{k-m}d^{k-m}\] \[= 2C_{A.5}^{4}R(\mathrm{h}(\rho^{\prime})-m)^{3q}\cdot(\mathrm{h} (\rho^{\prime})-m)\big{(}\max\{d\lambda^{2},\,\lambda\}\big{)}^{\mathrm{h}( \rho^{\prime})-m}\] \[= 2C_{A.5}^{4}R(\mathrm{h}(\rho^{\prime})-m)^{3q+1}\exp(-1.1\varepsilon (\mathrm{h}(\rho^{\prime})-m))\] \[\leq C_{0}R\exp(-\varepsilon(\mathrm{h}(\rho^{\prime})-m)),\]

where

\[C_{0}=2C_{A.5}^{4}\max_{n\in\mathbb{N}}n^{3q+1}\exp(-0.1\varepsilon n)<+\infty\]

is a constant depending on \(M\) and \(\varepsilon\). Combining the above estimate with (85) we conclude that

\[\max_{\theta\in[q]}\big{|}(\mathbb{E}_{\rho^{\prime}}fg)(\theta)-\mathbb{E}fg \big{|}\leq C_{0}R\exp\big{(}-\varepsilon(\mathrm{h}(\rho^{\prime})-m)\big{)} \sqrt{\sum_{u\in D_{m}(\rho^{\prime})}\mathbb{E}f_{u}^{2}(X)}\sqrt{\sum_{u\in D _{m}(\rho^{\prime})}\mathbb{E}g_{u}^{2}(X)},\]

and the lemma follows. 

The statement of Lemma G.4 together with Proposition C.3 implies the following:

**Corollary G.5**.: _There exists a constant \(C=C(M,\varepsilon)\geq 1\) so that the following holds. For \(\rho^{\prime}\in T\) and \(0\leq m\leq\mathrm{h}(\rho^{\prime})\), consider two degree 1 polynomials \(f\) and \(g\) with variables \((x_{u}\,:\,u\in D_{m}(\rho^{\prime}))\) with \(\mathbb{E}f(X)=\mathbb{E}g(X)=0\). Notice that by the Markov Property, \((\mathbb{E}_{\rho^{\prime}}fg)(x)\) is a function of \(x_{\rho^{\prime}}\). Then,_

\[\max_{\theta\in[q]}\big{|}(\mathbb{E}_{\rho^{\prime}}fg)(\theta)-\mathbb{E}fg \big{|}\leq CR^{4}\exp(-\varepsilon(\mathrm{h}(\rho^{\prime})-m))\sqrt{ \mathbb{E}f^{2}(X)}\sqrt{\mathbb{E}g^{2}(X)}.\]

**Remark G.6**.: By taking the degree 1 polynomial \(f=g\) with the assumption that \(\mathbb{E}f(X)=0\), we get

\[\mathbb{E}\big{[}(\mathbb{E}_{\rho^{\prime}}f^{2})(X)-\mathbb{E}f^{2}(X)\big{]} ^{2}\leq\Big{(}\max_{\theta\in[q]}\big{|}(\mathbb{E}_{\rho^{\prime}}fg)( \theta)-\mathbb{E}fg\big{|}\Big{)}^{2}\leq C^{2}R^{6}\exp(-2\varepsilon\mathrm{ h}(\rho^{\prime}))(\mathbb{E}f^{2}(X))^{2}.\] (86)

In other words, if \(\mathrm{h}(\rho^{\prime})\) is sufficiently large, \((\mathbb{E}_{\rho^{\prime}}f^{2})(X_{\rho^{\prime}})\) is almost the same as \(\mathbb{E}f^{2}(X)\) with a small fluctuation. Let us state this as a seperate lemma.

**Lemma G.7**.: _There exists \(C=C(M,d)\) so that the following holds. For \(\rho^{\prime}\in T\) with_

\[\mathrm{h}(\rho^{\prime})\geq\frac{C(\log(R)+1)}{\varepsilon},\]

_any degree 1 polynomial \(f\) of variables \((x_{u}\,:\,u\in L_{\rho^{\prime}})\) with \(\mathbb{E}f(X)=0\) satisfies_

\[\max_{\theta\in[q]}(\mathbb{E}_{\rho^{\prime}}f^{2})(\theta)\leq 2\min_{ \theta\in[q]}(\mathbb{E}_{\rho^{\prime}}f^{2})(\theta).\]

Proof.: By Corollary G.5, for every \(\theta\in[q]\),

\[\big{|}(\mathbb{E}_{\rho^{\prime}}f^{2})(\theta)-\mathbb{E}f^{2}(X)\big{|} \leq C_{G.5}R^{4}\exp(-\varepsilon\mathrm{h}(\rho^{\prime}))\mathbb{E}f^{2}(X).\]

where \(C_{G.5}\) is the constant introduced in Lemma G.5. Now, we set the constant described in the lemma as

\[C=\frac{1}{\varepsilon}\Big{(}\log(4C_{G.5})+4\Big{)},\]

which implies

\[C_{G.4}R\exp(-\varepsilon\mathrm{h}(\rho^{\prime}))\leq\frac{1}{4}\exp\big{(} -\varepsilon(\mathrm{h}(\rho^{\prime})-C(\log(R)+1))\big{)}.\]

Then, with \(\mathrm{h}(\rho^{\prime})\geq C(\log(R)+1)\)

\[|(\mathbb{E}_{\rho^{\prime}}f^{2})(\theta)-\mathbb{E}f^{2}(X)|\leq\frac{1}{4} \mathbb{E}f^{2}(X),\]

which in term implies

\[\frac{\max_{\theta\in[q]}(\mathbb{E}_{\rho^{\prime}}f^{2})(\theta)}{\min_{ \theta\in[q]}(\mathbb{E}_{\rho^{\prime}}f^{2})(\theta)}\leq\frac{\frac{5}{4} \mathbb{E}f^{2}(X)}{\frac{3}{4}\mathbb{E}f^{2}(X)}<2.\]

Proof of Proposition g.2.: Let \(C_{0}\) denote the constant introduced in the statement of the Proposition. Its precise value will be determined along the proof.

Let \(\rho^{\prime}\in T\) with \(\mathrm{h}^{\prime}:=\mathrm{h}(\rho^{\prime})\). By Lemma C.9, any degree-1 polynomial \(f(x)\) with variables \((x_{u}\,:\,u\in L_{\rho^{\prime}})\) satisfies

\[\mathrm{Var}\big{[}(\mathbb{E}_{\rho^{\prime}}f)(X)\big{]}\leq C_{C.9}R^{4}( \mathrm{h}^{\prime})^{2q}(d\lambda^{2})^{\mathrm{h}^{\prime}}\mathrm{Var}[f(X)],\]

where \(C_{C.9}\) denotes the \(M\)-dependent constant introduced in the Lemma. For the term in front of \(\mathrm{Var}[f(X)]\),

\[C_{C.9}R^{4}(\mathrm{h}^{\prime})^{2q}(d\lambda^{2})^{\mathrm{h}^{\prime}} \leq C_{C.9}R^{4}(\mathrm{h}^{\prime})^{2q}\exp(-1.1\varepsilon\mathrm{h}^{ \prime})\leq\exp\big{(}-\varepsilon\big{(}\mathrm{h}^{\prime}-C_{1}(\log(R)+1 )\big{)}\big{)},\]

where

\[C_{1}:=\frac{1}{\varepsilon}\Big{(}\log(C_{C.9})+4+\max_{n\in\mathbb{N}}n^{2q }\exp(-0.1\varepsilon q)\Big{)}.\]

Thus, if we impose the **first assumption** on \(C_{0}\) that

\[C_{0}\geq C_{1},\]

then the first condition (81) in Assumption G.1 holds for \(\mathcal{A}_{1}\) if we take \(\mathrm{h}^{\circ}\geq C_{0}(1+\log(R))\).

It remains to establish (82). Let \(f,g\) be two degree-1 polynomials in the variables \((x_{u}\,:\,u\in L_{\rho^{\prime}})\) satisfying \(\mathbb{E}f(X)=\mathbb{E}g(X)=0\). First, by Corollary G.5,

\[\max_{\theta\in[q]}|(\mathbb{E}_{\rho^{\prime}}fg)(\theta)-\mathbb{E}fg|\leq C _{G.5}R^{4}\exp(-\varepsilon\mathrm{h}(\rho^{\prime}))\sqrt{\mathbb{E}f^{2}( X)}\sqrt{\mathbb{E}g^{2}(X)},\]

where \(C_{G.5}\) is the constant introduced in Corollary G.5. Next, we would like to apply Lemma G.7. Assuming

\[\mathrm{h}(u)\geq\frac{C_{G.7}(\log(R)+1)}{\varepsilon}\]where \(C_{G.7}\) is the constant introduced in the Lemma, we can apply the lemma to get

\[\mathbb{E}f^{2}(X)\leq 2\min_{\theta}(\mathbb{E}_{\rho^{\prime}}f^{2})(\theta)\]

and the same holds for \(g\). Together we may conclude that

\[\max_{\theta\in[q]}|(\mathbb{E}_{\rho^{\prime}}fg)(\theta)-\mathbb{ E}fg|\leq 2C_{G.5}R^{4}\exp(-\varepsilon\mathrm{h}(\rho^{\prime}))\sqrt{ \min_{\theta}(\mathbb{E}_{\rho^{\prime}}f^{2})(\theta)\min_{\theta}(\mathbb{E} _{\rho^{\prime}}g^{2})(\theta)}\]

Now, we impose the **second assumption** on \(C_{0}\) that

\[C_{0}\geq\max\Big{\{}\frac{1}{\varepsilon}\Big{(}\log(2C_{G.5})+4\Big{)},\, \frac{C_{G.7}}{\varepsilon}\Big{\}}.\]

Then, we conclude that

\[\max_{\theta\in[q]}|(\mathbb{E}_{u}fg)(\theta)-\mathbb{E}fg|\leq \exp\Big{(}-\varepsilon\big{(}\mathrm{h}(\rho^{\prime})-C_{0}( \log(R)+1)\big{)}\Big{)}\sqrt{\min_{\theta}(\mathbb{E}_{u}f^{2})(\theta)\min_ {\theta}(\mathbb{E}_{u}g^{2})(\theta)}\]

provided that

\[\mathrm{h}(\rho^{\prime})\geq C_{0}(\log(R)+1).\]

Therefore, we can conclude that \(\mathcal{A}_{1}\) satisfies Assumption G.1 with

\[\mathrm{h}^{\circ}=C_{0}(\log(R)+1).\]

## Appendix H Inductive Step in General Case

The goal in this section is to prove Theorem G.3. Let us restate the theorem here:

**Theorem**.: _Consider the rooted tree \(T\) and transition matrix \(M\) described in Theorem 1.6. There exists \(C=C(M,d)>1\) so that the following holds. Suppose \(\mathcal{A}\) satisfies Assumption G.1 with some parameter \(\mathrm{h}^{\circ}\). Let \(\mathcal{B}=\mathcal{B}(\mathcal{A})\) (see Definition 1.11). Then, \(\mathcal{B}\) satisfies Assumption G.1 with parameter \(\mathrm{h}^{\circ}+C(\log(R)+1).\)_

\(\mathrm{h}^{\circ}+C(\log(R)+1).\)__

**In this section, we fix a subcollection \(\mathcal{A}\) satisfying Assumption G.1 with a given parameter \(\mathrm{h}^{\circ}\) and let \(\mathcal{B}=\mathcal{B}(\mathcal{A})\).**

We begin with the following lemma, which allows us to recycle some of the results from the case \(c_{M}>0\).

**Lemma H.1**.: _Suppose \(\mathcal{A}\) satisfies Assumption G.1 with parameter \(\mathrm{h}^{\circ}\). Then, then \(\mathcal{A}\) satisfies Assumption B.3 with \(\mathrm{h}^{*}=\mathrm{h}^{\circ}+\frac{2}{\varepsilon}\log(2)\) and \(c^{*}=\frac{1}{2}\)._

Proof.: Let \(f\) be a \(\mathcal{A}_{\leq v}\)-polynomial. If we set \(\mathrm{h}^{*}\geq\mathrm{h}^{\circ}\), then (15) follows immediately from (81).

Now, we assume that \(\mathbb{E}f(X)=0\) and \(\mathrm{h}(v)\geq\mathrm{h}^{\circ}\). We could apply (82) with \(g=f\) to get

\[\max_{\theta\in[q]}\big{|}(\mathbb{E}_{v}f^{2})(\theta)-\mathbb{E}f^{2}(X) \big{|}\leq\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(v)-\mathrm{h}^{\circ })\Big{)}\mathbb{E}f^{2}(X).\]

If \(\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(v)-\mathrm{h}^{\circ})\Big{)} \leq\frac{1}{2}\), or equivalently,

\[\mathrm{h}(v)\geq\mathrm{h}^{\circ}+\frac{2}{\varepsilon}\log(2),\]

then, for every \(\theta\in[q]\),

\[\frac{1}{2}\mathbb{E}h^{2}(X)\leq(\mathbb{E}_{v}h^{2})(\theta)\leq\frac{3}{2} \mathbb{E}h^{2}(X).\]

Therefore, if we set \(\mathrm{h}^{*}\geq\mathrm{h}^{\circ}+\frac{2}{\varepsilon}\log(2)\) and \(c^{*}=\frac{1}{2}\), both (15) and (16) hold.

In the remainning of this section, we set

\[\mathrm{h}^{*}=\mathrm{h}^{\circ}+\frac{2}{\varepsilon}\log(2)\text{ and }c^{*}=\frac{1}{2},\] (87)

and we will rely on the fact that \(\mathcal{A}\) satisfies Assumption B.3 with these two parameters. In particular, we could apply Theorem B.6 to show the existence of \(C_{B.6}=C(M,\varepsilon,1/2)\) such that for any \(\mathcal{B}_{\leq v}\)-polynomial \(f\),

\[\mathrm{Var}\big{[}(\mathbb{E}_{v}f)(X)\big{]}\leq\exp\Big{(}-\varepsilon \big{(}\mathrm{h}(v)-\mathrm{h}^{\circ}+C_{B.6}(\log(R)+1)\big{)}\Big{)} \mathrm{Var}\big{[}f(X)\big{]}.\]

Therefore, to establish Theorem G.3, it remains to show the existence of \(C=C(M,d)\) so that any \(\mathcal{B}_{\leq v}\)-polynomials \(f\) and \(g\) with \(\mathrm{h}(v)\geq\mathrm{h}^{\circ}+C(\log(R)+1)\) and \(\mathbb{E}f(X)=\mathbb{E}g(X)=0\) satisfy

\[\max_{\theta\in[q]}|(\mathbb{E}_{v}fg)(\theta)-\mathbb{E}fg|\leq\exp\Big{(}- \frac{\varepsilon}{2}(\mathrm{h}(v)-\mathrm{h}^{\circ}-C(\log(R)+1))\Big{)} \sqrt{\min_{\theta}(\mathbb{E}_{v}f^{2})(\theta)\min_{\theta^{\prime}}( \mathbb{E}_{v}g^{2})(\theta)}.\]

To establish the above inequality, the higher level structure is essentially the same as that for deriving Theorem B.6. We again decompose \(f\) and \(g\) according to Lemma D.1. To the proof of the theorem, similarly it contains three steps:

1. Establish properties of \(\tilde{f}_{u}\) and \(\tilde{g}_{u}\), see Proposition H.2.
2. Establish properties of \(f_{k}\) and \(g_{k}\), see Proposition H.6.
3. Establish Theorem G.3.

### Properties of \(f_{u}\)

The main goal we want to prove in this subsection is the following Proposition.

**Proposition H.2**.: _There exsits \(C=C(M,d)\geq 1\) so that the following holds. For a given \(u\in T\backslash L\) with_

\[\mathrm{h}(u)\geq\mathrm{h}^{\circ}+C(\log(R)+1),\]

_suppose \(f_{u}\) and \(g_{u}\) are two functions which are linear combination of \(\psi_{\sigma}(x)\) with \(\sigma\in\mathcal{F}(\mathcal{B}_{u})\). Then, for any \(\theta,\theta^{\prime}\in[q]\),_

\[\big{|}(\mathbb{E}_{u}f_{u}g_{u})(\theta)-(\mathbb{E}_{u}f_{u}g_ {u})(\theta^{\prime})\big{|}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-C(\log(R)+1)- \mathrm{h}^{\circ})\Big{)}\sqrt{\min_{\theta}(\mathbb{E}_{u}f_{u}^{2})(\theta )\min_{\theta}(\mathbb{E}_{u}g_{u}^{2})(\theta)}.\]

With a minor modification to our approach, we are able to obtain an analogous result wherein \(f_{u}\) and \(g_{u}\) are substituted by \(\tilde{f}_{u}\) and \(\tilde{g}_{u}\), respectively:

**Corollary H.3**.: _There exsits \(C=C(M,d)\geq 1\) so that the following holds. For a given \(u\in T\backslash L\) with_

\[\mathrm{h}(u)\geq\mathrm{h}^{\circ}+C(\log(R)+1),\]

_suppose \(f_{u}\) and \(g_{u}\) are two functions which are linear combination of \(\psi_{\mathbf{S}}(x)\) with \(\mathbf{S}\in\mathcal{F}(\mathcal{B}_{u})\). Then, for any \(\theta,\theta^{\prime}\in[q]\),_

\[\big{|}(\mathbb{E}_{u}\tilde{f}_{u}\tilde{g}_{u})(\theta)-( \mathbb{E}_{u}\tilde{f}_{u}\tilde{g}_{u})(\theta^{\prime})\big{|}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-C(\log(R)+1)- \mathrm{h}^{\circ})\Big{)}\sqrt{\min_{\theta}(\mathbb{E}_{u}\tilde{f}_{u}^{2}) (\theta)\min_{\theta}(\mathbb{E}_{u}\tilde{g}_{u}^{2})(\theta)}.\]

Let us prove Corollary first.

Proof.: Let \(C_{0}\) denote the constant introduced in the Corollary. Its value will be dervied during the proof.

From the identity

\[\tilde{f}_{u}(x)\tilde{g}_{u}(x)=f_{u}(x)g_{u}(x)-f_{u}(x)\mathbb{E}g_{u}(X)- \mathbb{E}f_{u}(X)g_{u}(x)+\mathbb{E}f_{u}(X)\mathbb{E}g_{u}(X),\]it follows that

\[\big{|}(\mathbb{E}_{u}\tilde{f}_{u}\tilde{g}_{u})(\theta)-(\mathbb{E} _{u}\tilde{f}_{u}\tilde{g}_{u})(\theta^{\prime})\big{|}\leq \big{|}(\mathbb{E}_{u}f_{u}g_{u})(\theta)-(\mathbb{E}_{u}f_{u}g_{u}) (\theta^{\prime})\big{|}+|\mathbb{E}g_{u}(X)|\big{|}(\mathbb{E}_{u}f_{u})( \theta)-(\mathbb{E}_{u}f_{u})(\theta^{\prime})\big{|}\] \[+|\mathbb{E}f_{u}(X)|\big{|}(\mathbb{E}_{u}g_{u})(\theta)-( \mathbb{E}_{u}g_{u})(\theta^{\prime})\big{|}\] \[\leq \big{|}(\mathbb{E}_{u}f_{u}g_{u})(\theta)-(\mathbb{E}_{u}f_{u}g_{ u})(\theta^{\prime})\big{|}+4\max_{\theta^{\prime}}|\mathbb{E}_{u}f_{u}( \theta)|\max_{\theta^{\prime}}|\mathbb{E}_{u}g_{u}(\theta)|.\]

First, we apply Proposion E.1 with the fact that \(\mathcal{A}\) satisfies Assumption G.1 with parameter \(\mathrm{h}^{\circ}=\mathrm{h}^{\circ}+\frac{2}{\varepsilon}\log(2)\) and \(c^{*}=\frac{1}{2}\),

\[\max_{\theta^{\prime}}(\mathbb{E}_{u}f_{u})^{2}(\theta)\leq\exp(-2\varepsilon (\mathrm{h}(u)-C_{E.1}(\log(R)+1)-\mathrm{h}^{\circ}))\max_{\theta^{\prime}}( \mathbb{E}_{u}f_{u}^{2})(\theta^{\prime})\]

where \(C_{E.1}=C(M,d,\frac{1}{2})\) is the constant introduced in the Proposition.

Second, applying Proposition H.2 with \(f_{u}=g_{u}\) we have

\[\big{|}\max_{\theta}(\mathbb{E}_{u}f_{u}^{2})(\theta)-\min_{ \theta^{\prime}}(\mathbb{E}_{u}f_{u}^{2})(\theta^{\prime})\big{|}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-C_{H.2}(\log(R)+ 1)-\mathrm{h}^{\circ})\Big{)}\min_{\theta}(\mathbb{E}_{u}f_{u}^{2})(\theta)\]

where \(C_{H.2}\) is the constant introduced in Proposition H.2.

Let us impose the **first assumption** that \(C_{0}\geq C_{H.2}\). Then, with \(\mathrm{h}(u)\geq\mathrm{h}^{\circ}+C_{0}(\log(R)+1)\), we can conclude that

\[\max_{\theta^{\prime}}(\mathbb{E}_{u}f_{u}^{2})(\theta^{\prime})\leq 2\min_{ \theta^{\prime}}(\mathbb{E}_{u}f_{u}^{2})(\theta^{\prime}).\]

Clearly, the same derivation also holds for \(g_{u}\).

Therefore, we conclude that

\[\big{|}(\mathbb{E}_{u}\tilde{f}_{u}\tilde{g}_{u})(\theta)-( \mathbb{E}_{u}\tilde{f}_{u}\tilde{g}_{u})(\theta^{\prime})\big{|}\] \[\leq \big{|}(\mathbb{E}_{u}f_{u}g_{u})(\theta)-(\mathbb{E}_{u}f_{u}g_{ u})(\theta^{\prime})\big{|}\] \[+8\exp(-2\varepsilon(\mathrm{h}(u)-C_{1}(\log(R)+1)-\mathrm{h}^{ \circ}))\sqrt{\min_{\theta}(\mathbb{E}_{u}f_{u}^{2})(\theta)\min_{\theta}( \mathbb{E}_{u}g_{u}^{2})(\theta)}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-C_{H.2}(\log(R) +1)-\mathrm{h}^{\circ})\Big{)}\sqrt{\min_{\theta}(\mathbb{E}_{u}f_{u}^{2})( \theta)\min_{\theta}(\mathbb{E}_{u}g_{u}^{2})(\theta)}\] \[+8\exp\Big{(}-2\varepsilon\Big{(}\mathrm{h}(u)-C_{E.1}(\log(R)+ 1)-\mathrm{h}^{\circ}-\frac{2}{\varepsilon}\log(2)\Big{)}\Big{)}\sqrt{\min_{ \theta}(\mathbb{E}_{u}f_{u}^{2})(\theta)\min_{\theta}(\mathbb{E}_{u}g_{u}^{2 })(\theta)}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-C_{0}(\log(R)+1) -\mathrm{h}^{\circ})\Big{)}\sqrt{\min_{\theta}(\mathbb{E}_{u}f_{u}^{2})( \theta)\min_{\theta}(\mathbb{E}_{u}g_{u}^{2})(\theta)},\]

where the last inequality follows by imposing the **second assumption** on \(C_{0}\) that

\[C_{0}\geq\frac{2}{\varepsilon}\log(2)+\max\Big{\{}C_{H.2},C_{E.1}+\frac{2}{ \varepsilon}\log(2)+\frac{1}{2\varepsilon}\log(8)\Big{\}}.\]

This completes the proof of the Corollary. 

The main technical part for proving Proposition H.2 is the following:

**Lemma H.4**.: _For any \(u\in T\) with \(\exp\big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-\mathrm{h}^{\circ})\big{)}\leq \frac{1}{4Rd}\), the following holds: Let \(I\subset[d_{u}]\) be a subset of size at least 2. For any \(a(x)\) and \(b(x)\) which are linear combinations of \(\psi_{\sigma}(x)\) with \(\sigma\in\mathcal{B}_{u}\) satisfying \(I(\sigma)=I\), we have_

\[\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{u}ab)(\theta)-( \mathbb{E}_{u}ab)(\theta^{\prime})\big{|}\leq 4dR\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-\mathrm{h}^{ \circ})\Big{)}\sqrt{\min_{\theta}(\mathbb{E}_{u}a^{2})(\theta)\cdot\min_{\theta}( \mathbb{E}_{u}b^{2})(\theta)}.\]

**Remark H.5**.: From the assumption that \(\mathrm{h}(u)\) satisfies

\[4dR\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-\mathrm{h}^{\circ})\Big{)} \leq 1\Leftrightarrow\mathrm{h}(u)\geq\mathrm{h}^{\circ}+\frac{2}{ \varepsilon}\log(4dR).\]

By taking \(a(x)=b(x)\) we have

\[\max_{\theta}(\mathbb{E}_{u}a^{2})(\theta)\leq 2\min_{\theta}(\mathbb{E}_{u}a^{2})( \theta).\] (88)Proof.: Let \(u\), \(a(x)\), and \(b(x)\) be the vertex and functions described in the Lemma. Let us introduce some notations for the ease of expressing the calculation later. For brevity, let

\[\delta=\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-\mathrm{h}^{\circ}) \Big{)}.\]

For \(x\in[q]^{T}\), let

\[x_{u,I}=(x_{u_{i}})_{i\in I}.\]

For any given function \(h(x)\) with variables in \((x_{v}\,:\,v\in\bigcup_{i\in I}T_{u_{i}})\), we define

\[(\mathbb{E}_{u,I}h)(x):=\mathbb{E}\Big{[}h(X)\,\Big{|}\,\forall v \notin\bigcup_{i\in I}\{w<u_{i}\},\,X_{v}=x_{v}\Big{]}.\]

Observe that

\[(\mathbb{E}_{u,I}a)(x),\,(\mathbb{E}_{u,I}b)(x),\,\text{and}\, (\mathbb{E}_{u,I}ab)(x)\]

are functions with input \(x_{u,I}\). This is due to the fact that \(a\) and \(b\) -and consequently \(ab\)- are functions of variables \((x_{v}\,:\,x_{v}\in\bigcup_{i\in I}L_{u_{i}})\) and Markov Property.

**Claim:** The function \(x_{u,I}\mapsto(\mathbb{E}_{u,I}ab)(x_{u,I})\) is Lipschitz continuous with respect to the Hamming Distance with Lipschitz constant

\[2\delta\sqrt{\max_{x_{u,I}}(\mathbb{E}_{u,I}a^{2})(x_{u,I})} \sqrt{\max_{x_{u,I}}(\mathbb{E}_{u,I}b^{2})(x_{u,I})}.\] (89)

We begin with the proof of the claim. Fix an index \(i_{0}\in I\). Without lose of generality, we assume \(I=[k]\) and \(i_{0}=1\). For \(x\in[q]^{T}\), let

\[x_{i}=x_{\leq u_{i}}\]

for \(i\leq[d_{u}]\), and set

\[x_{0}= (x_{2},\ldots,x_{k}).\]

With this notation above, we can express

\[a(x)= a(x_{0},x_{1})\] and \[b(x)= b(x_{0},x_{1}).\]

Fix any value of \(x_{0}\), the function

\[x_{1}\mapsto a(x_{0},x_{1})\]

is a linear combination of \(\tilde{\phi}_{\sigma_{1}}(x_{1})\) with \(\sigma_{1}\in\mathcal{F}(\mathcal{A}_{\leq u})\). Notably, this implies that \(\mathbb{E}a(x_{0},X_{1})=0\). The same properties hold for the function \(x_{1}\mapsto b(x_{0},x_{1})\).

Now, given the assumption \(\exp\big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-\mathrm{h}^{\circ})\big{)} \leq\frac{1}{4Rd}\) implies \(\mathrm{h}(u)\geq\mathrm{h}^{\circ}\), we can apply (82) from Assumption G.1 to get that

\[\max_{\theta,\theta^{\prime}\in[q]}\Big{|}\mathbb{E}\big{[}a(x_{0 },X_{1})b(x_{0},X_{1})\,\big{|}\,X_{u_{1}}=\theta_{1}\big{]}-\mathbb{E}\big{[} a(x_{0},X_{1})b(x_{0},X_{1})\,\big{|}\,X_{u_{1}}=\theta_{2}\big{]}\Big{|}\] \[\leq 2\delta\sqrt{\min_{\theta}\mathbb{E}\big{[}a^{2}(x_{0},X_{1}) \,\big{|}\,X_{u_{1}}=\theta\big{]}\min_{\theta}\mathbb{E}\big{[}b^{2}(x_{0}, X_{1})\,\big{|}\,X_{u_{1}}=\theta\big{]}}.\]

For any \(x\in[q]^{T}\), let \(x_{u_{0}}=(x_{u_{2}},x_{u_{3}},\ldots,x_{u_{d_{u}}})\). By the Markov Property, for any \(\theta\in[q]\),

\[(X_{0}\,|\,X_{u_{0}}=x_{u_{0}},X_{u_{1}}=\theta)= (X_{0}\,|\,X_{u_{0}}=x_{u_{0}})\text{ and}\] \[(X_{1}\,|\,X_{u_{0}}=x_{u_{0}},X_{u_{1}}=\theta)= (X_{u_{1}}\,|\,X_{u_{1}}=\theta)\]

are jointly independent. Hence,

\[\mathbb{E}\big{[}a(X_{0},X_{1})b(X_{0},X_{1})\,\big{|}\,X_{u_{0}} =x_{u_{0}},X_{u_{1}}=\theta\big{]}\] \[= \mathbb{E}\big{[}a(Y_{0},X_{1})b(Y_{0},X_{1})\,\big{|}\,X_{u_{1}} =\theta\big{]}\]where \(Y_{0}\) is an independent copy of \((X_{0}\,|\,X_{u_{0}}=x_{u_{0}})\). We have

\[\big{|}\mathbb{E}\big{[}a(X_{0},X_{1})b(X_{0},X_{1})\,\big{|}\,X_{u_ {0}}=x_{u_{0}},X_{u_{1}}=\theta\big{]}-\mathbb{E}\big{[}a(X_{0},X_{1})b(X_{0},X_{ 1})\,\big{|}\,X_{u_{0}}=x_{u_{0}},X_{u_{1}}=\theta^{\prime}\big{]}\] \[= \Big{|}\mathbb{E}_{Y_{0}}\Big{[}\mathbb{E}_{X_{1}}[a(Y_{0},X_{1}) b(Y_{0},X_{1})\,|\,X_{u_{1}}=\theta]-\mathbb{E}_{X_{1}}[a(Y_{0},X_{1})b(Y_{0},X_{1}) \,|\,X_{u_{1}}=\theta^{\prime}]\big{]}\Big{|}\] \[\leq \mathbb{E}_{Y_{0}}\Big{[}\big{[}\mathbb{E}_{X_{1}}[a(Y_{0},X_{1} )b(Y_{0},X_{1})\,|\,X_{u_{1}}=\theta]-\mathbb{E}_{X_{1}}[a(Y_{0},X_{1})b(Y_{0}, X_{1})\,|\,X_{u_{1}}=\theta^{\prime}]\big{]}\Big{]}\] \[\leq 2\delta\mathbb{E}_{Y_{0}}\Big{[}\big{(}\min_{\theta}\mathbb{E}_ {X_{1}}[a^{2}(Y_{0},X_{1})\,|\,X_{u_{1}}=\theta]\big{)}^{1/2}\cdot\big{(}\min_ {\theta^{\prime}}\mathbb{E}_{X_{1}}[b^{2}(Y_{0},X_{1})\,|\,X_{u_{1}}=\theta^{ \prime}]\big{)}^{1/2}\Big{]}\] \[\leq 2\delta\big{(}\mathbb{E}_{Y_{0}}\big{[}\min_{\theta}\mathbb{E}_ {X_{1}}[a^{2}(Y_{0},X_{1})\,|\,X_{u_{1}}=\theta]\big{]}\big{)}^{1/2}\cdot\big{(} \mathbb{E}_{Y_{0}}\big{[}\min_{\theta^{\prime}}\mathbb{E}_{X_{1}}[b^{2}(Y_{0}, X_{1})\,|\,X_{u_{1}}=\theta^{\prime}]\big{]}\big{)}^{1/2},\]

where the last inequality follows from Holder's inequality. Further,

\[\mathbb{E}_{Y_{0}}\big{[}\min_{\theta}\mathbb{E}_{X_{1}}[a^{2}(Y_ {0},X_{1})\,|\,X_{u_{1}}=\theta]\big{]}\leq \min_{\theta}\mathbb{E}_{Y_{0}}\big{[}\mathbb{E}_{X_{1}}[a^{2}(Y_ {0},X_{1})\,|\,X_{u_{1}}=\theta]\big{]}\] \[= \min_{\theta}\mathbb{E}\big{[}a^{2}(X)\,\big{|}\,X_{u_{0}}=x_{u_{ 0}},X_{u_{1}}=\theta\big{]}\] \[\leq \max_{x_{u,I}}(\mathbb{E}_{u,I}a^{2})(x_{u,I}).\]

Applying the same derivation to \(b\) we get

\[\mathbb{E}_{Y_{0}}\big{[}\min_{\theta}\mathbb{E}_{X_{1}}[b^{2}(Y_ {0},X_{1})\,|\,X_{u_{1}}=\theta]\big{]}\leq\max_{x_{u,I}}(\mathbb{E}_{u,I}b^{ 2})(x_{u,I}).\]

Therefore, our claim (89) follows: For any \(\theta,\theta^{\prime}\in[q]\),

\[\big{|}\mathbb{E}\big{[}a(X)b(X)\,\big{|}\,X_{u_{0}}=x_{u_{0}},x_ {u_{1}}=\theta\big{]}-\mathbb{E}\big{[}a(X)b(X)\,\big{|}\,X_{u_{0}}=x_{u_{0}}, x_{u_{1}}=\theta^{\prime}\big{]}\] \[\leq 2\delta\sqrt{\max_{x_{u,I}}(\mathbb{E}a^{2})(x_{u,I})\max_{x_{u,I}}(\mathbb{E}b^{2})(x_{u,I})}.\]

With the Lipschitz continuity been established, essentially the lemma follows when \(\delta\) is sufficiently small. Let us proceed with the remaining argument. Let

\[x_{u,I}^{\prime}= \mathrm{argmin}_{x_{u,I}}(\mathbb{E}_{u,I}a^{2})(x_{u,I})\qquad \text{ and }\qquad x_{u,I}^{\prime\prime}= \mathrm{argmax}_{x_{u,I}}(\mathbb{E}_{u,I}a^{2})(x_{u,I}).\]

Applying (89) with the assumption \(a(x)=b(x)\) and the fact \(|I|\leq d_{u}\),

\[(\mathbb{E}_{u,I}a^{2})(x_{u,I}^{\prime\prime})-(\mathbb{E}_{u,I}a^{2})(x_{u,I}^{\prime})\leq 2d_{u}\delta(\mathbb{E}_{u,I}a^{2})(a_{u,I}^{\prime\prime}),\]

and hence

\[\max_{x_{u,I}}(\mathbb{E}_{u,I}a^{2})(x_{u,I})\leq \frac{1}{1-2d_{u}\delta}\min_{x_{u,I}}(\mathbb{E}_{u,I}a^{2})(x_ {u,I})\leq\frac{1}{1-2d_{u}\delta}\min_{s}(\mathbb{E}_{u}a^{2})(s),\] (90)

provided that \(2d_{u}\delta<1\).

Again, the same derivation also holds for \(b\). Combining (89) and (90) we conclude that for any \(\theta,\theta^{\prime}\in[q]\),

\[|(\mathbb{E}_{u}ab)(\theta)-(\mathbb{E}_{u}ab)(\theta^{\prime})|\leq |\max_{x_{u,I}}(\mathbb{E}_{u}ab)(x_{u,I})-\min_{x_{u,I}^{\prime }}(\mathbb{E}_{u}ab)(x_{u,I}^{\prime})|\] \[\leq \frac{2d_{u}\delta}{1-2d_{u}\delta}\sqrt{\min_{\theta}(\mathbb{E} _{u}a^{2})(\theta)\min_{\theta^{\prime}}(\mathbb{E}_{u}b^{2})(\theta^{\prime})}.\]

With our assumption on the tree \(T\) that \(d_{u}\leq Rd\), our assumption

\[\delta=\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-\mathrm{h}^{\circ}) \Big{)}\leq\frac{1}{4Rd},\]

implies that

\[\frac{2d_{u}\delta}{1-2d_{u}\delta}\leq 4Rd\delta.\]

We conclude that

\[|(\mathbb{E}_{u}ab)(\theta)-(\mathbb{E}_{u}ab)(\theta^{\prime})|\leq 4Rd\exp\Big{(}\frac{\varepsilon}{2}(\mathrm{h}(u)-\mathrm{h}^{\circ}) \Big{)}\sqrt{\min_{\theta}(\mathbb{E}_{u}a^{2})(\theta)\min_{\theta^{\prime}}( \mathbb{E}_{u}b^{2})(\theta^{\prime})}.\]Proof of Proposition H.2.: Let \(C_{0}=C_{0}(M,d)\) denote the constant introduced in the statement of the Proposition. Recall the decomposition of \(f_{u}\) into \(f_{u,I}\) from Definition E.4, consider the decomposition

\[f_{u}(x)=\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}f_{u,I}(x)\text{ and }g_{u}(x)=\sum_{I \subseteq[d_{u}]\,:\,|I|\geq 2}g_{u,I}(x).\]

The proof of the Proposition will proceed by bounding summands in the formula below:

\[\big{|}(\mathbb{E}_{u}f_{u}g_{u})(\theta)-(\mathbb{E}_{u}f_{u}g_{u})(\theta^{ \prime})\big{|}\leq\sum_{I,J\subseteq[d_{u}]\,:\,|I|,J|\geq 2}\big{|}( \mathbb{E}_{u}f_{u,I}g_{u,J})(\theta)-(\mathbb{E}_{u}f_{u,I}g_{u,J})(\theta^{ \prime})\big{|}.\] (91)

Estimate of summands in (91): For any \(I,J\subseteq[d_{u}]\) with \(|I|,|J|\geq 2\), we have two cases to consider: First, we consider the case \(I\neq J\). Notice that, by Lemma H.1, \(\mathcal{A}\) satisfies Assumption B.3 with parameters \((\mathrm{h}^{\circ}+\frac{2}{\varepsilon}\log(2),\,\frac{1}{2})\). This allows us to invoke Corollary E.6, yielding

\[\big{|}(\mathbb{E}_{u}f_{u,I}g_{u,J})(\theta)-(\mathbb{E}_{u}f_{u,I}g_{u,J})(\theta^{\prime})\big{|}\] \[\leq 2\max_{\theta\in[q]}|(\mathbb{E}_{u}f_{u,I}g_{u,J})(\theta)|\] \[\leq 2\exp\Big{(}-\frac{\varepsilon|I\Delta J|}{2}(\mathrm{h}(u)-C_{E.6}-\mathrm{h}^{\circ}-\frac{2}{\varepsilon}\log(2))\Big{)}\big{(}\max_{ \theta\in[q]}(\mathbb{E}_{u}f_{u,I}^{2})(\theta)\big{)}^{1/2}\cdot\big{(}\max_ {\theta\in[q]}(\mathbb{E}_{u}g_{u,J}^{2})(\theta)\big{)}^{1/2},\]

where \(C_{E.6}=C_{E.6}(M,d,\frac{1}{2})\) is the constant introduced in the Corollary.

Let us impose the **first assumption** on \(C_{0}\) that

\[C_{0}\geq\frac{2}{\varepsilon}(1+\log(4d)),\]

which implies that \(\mathrm{h}(u)\geq\mathrm{h}^{\circ}+C_{0}(\log(R)+1)\geq\mathrm{h}^{\circ}+ \frac{2}{\varepsilon}\log(4dR)\). With this assumption, we could apply the remark (88) of Lemma H.4 to get

\[\big{(}\max_{\theta\in[q]}(\mathbb{E}_{u}f_{u,I}^{2})(\theta)\big{)}^{1/2}\leq 2 \big{(}\min_{\theta\in[q]}(\mathbb{E}_{u}f_{u,I}^{2})(\theta)\big{)}^{1/2}\]

and the same holds for \(g_{u,J}\). Hence, for \(I\neq J\) we have

\[\big{|}(\mathbb{E}_{u}f_{u,I}g_{u,J})(\theta)-(\mathbb{E}_{u}f_{u,I}g_{u,J})(\theta^{\prime})\big{|}\] \[\leq 4\exp\Big{(}-\frac{\varepsilon|I\Delta J|}{2}(\mathrm{h}(u)-C_{E.6}-\mathrm{h}^{\circ}+\frac{2}{\varepsilon}\log(2))\Big{)}\big{(}\min_{ \theta\in[q]}(\mathbb{E}_{u}f_{u,I}^{2})(\theta)\big{)}^{1/2}\cdot\big{(}\min_ {\theta\in[q]}(\mathbb{E}_{u}g_{u,J}^{2})(\theta)\big{)}^{1/2}.\]

Second, we consider the case \(I=J\). Here we simply apply Lemma H.4, yielding

\[\big{|}(\mathbb{E}_{u}f_{u,I}g_{u,I})(\theta)-(\mathbb{E}_{u}f_{u,I}g_{u,I})(\theta^{\prime})\big{|}\] \[\leq 4Rd\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-\mathrm{h}^{ \circ})\Big{)}\big{(}\min_{\theta\in[q]}(\mathbb{E}_{u}f_{u,I}^{2})(\theta) \big{)}^{1/2}\cdot\big{(}\min_{\theta\in[q]}(\mathbb{E}_{u}g_{u,J}^{2})(\theta )\big{)}^{1/2}.\]

Let us unify the above two estimates by introducing

\[C_{1}=\max\Big{\{}C_{E.6}+\frac{2}{\varepsilon}\log(2)+\frac{2}{\varepsilon} \log(8),\,\frac{2}{\varepsilon}(1+\log(24d))\Big{\}}.\]

Then,

\[\big{|}(\mathbb{E}_{u}f_{u,I}g_{u,J})(\theta)-(\mathbb{E}_{u}f_{u,I}g_{u,J})(\theta^{\prime})\big{|}\] \[\leq \underbrace{\frac{1}{6}\exp\Big{(}-\frac{\varepsilon}{2}\max\{| I\Delta J|,1\}(\mathrm{h}(u)-C_{1}(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}}_{:=a_{I,J}} \underbrace{\big{(}\min_{\theta\in[q]}(\mathbb{E}_{u}f_{u,I}^{2})(\theta) \big{)}^{1/2}}_{:=\alpha_{I}}\cdot\underbrace{\big{(}\min_{\theta\in[q]}( \mathbb{E}_{u}g_{u,J}^{2})(\theta)\big{)}^{1/2}}_{:=\beta_{J}},\] (93)

for every pair \(I,J\subseteq[d_{u}]\) with \(|I|\geq 2\) and \(|J|\geq 2\).

Using this inequality, (91) becomes

\[\big{|}(\mathbb{E}_{u}f_{u}g_{u})(\theta)-(\mathbb{E}_{u}f_{u}g_{u})(\theta^{ \prime})\big{|}\leq\sum_{I,J\subseteq[d_{u}]\,:|I|,|J|\geq 2}a_{I,J}\alpha_{I} \beta_{J}=\tilde{\alpha}^{\top}A\vec{\beta}\leq\|\vec{\alpha}\|\cdot\|A\|\cdot\| \vec{\beta}\|\] (94)

where \(\vec{\alpha}=(\alpha_{I})_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\), \(\vec{\beta}=(\beta)_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\), and \(A=(a_{I,J})_{I,J\subseteq[d_{u}]\,:\,|I|,|J|\geq 2}\). Further, \(\|\vec{\alpha}\|\) and \(\|\vec{\beta}\|\) are the \(\ell_{2}\) norms of \(\vec{\alpha}\) and \(\vec{\beta}\), respectively, and \(\|A\|\) is the operator norm of \(A\).

Estimate of operator norm of \(A\): Notice that \(A\) is a symmetric matrix. Thus, we can fix a unit vector \(\vec{\gamma}\) satisfying \(\|A\|=\vec{\gamma}^{\top}A\vec{\gamma}\). For each pair \(I,J\subseteq[d_{u}]\) with \(|I|\geq 2\) and \(|J|\geq 2\), since \(a_{I,J}\geq 0\),

\[\gamma_{I}a_{I,J}\gamma_{J}\leq\frac{a_{I,J}}{2}\gamma_{I}^{2}+\frac{a_{I,J}}{ 2}\gamma_{J}^{2},\]

and thus,

\[\|A\|=\sum_{I,J\subseteq[d_{u}]\,:\,|I|,|J|\geq 2}a_{I,J}\gamma_{I}\gamma_{J} \leq\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\gamma_{I}^{2}\Big{(}\sum_{J\subseteq[d_{ u}]\,:\,|J|\geq 2}a_{I,J}\Big{)}.\] (95)

For each \(I\subseteq[d_{u}]\) with \(|I|\geq 2\), the number of \(J\subseteq[d_{u}]\) with \(|I\Delta J|=k\) is bounded above by \(d_{u}^{k-1}\leq(Rd)^{k}\). Then, with the given estimate of \(a_{I,J}\) in (92),

\[\sum_{J\subseteq[d_{u}]\,:\,|J|\geq 2}a_{I,J}\leq \frac{1}{6}\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-C_{1} (\log(R)+1)-\mathrm{h}^{\circ})\Big{)}\] \[+\sum_{t\geq 1}\frac{1}{6}(Rd)^{t}\exp\Big{(}-\frac{\varepsilon}{2}t (\mathrm{h}(u)-C_{1}(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}.\] (96)

Now, we impose the **second assumption** on \(C_{0}\) that

\[C_{0}\geq C_{1}+\frac{2}{\varepsilon}\Big{(}1+\log(2d)\Big{)}.\]

With the assumption that \(\mathrm{h}(u)\geq\mathrm{h}^{\circ}+C_{0}(\log(R)+1)\), the geometric sum in (96) has a decay rate smaller than \(1/2\). Therefore,

\[\sum_{J\subseteq[d_{u}]\,:\,|J|\geq 2}a_{I,J}\leq \frac{1}{2}\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-C_{2} (\log(R)+1)-\mathrm{h}^{\circ})\Big{)},\]

where

\[C_{2}=C_{1}+\frac{2}{\varepsilon}\Big{(}1+\log(d)\Big{)}.\]

Now applying the above estimate, together with \(\sum_{I\subseteq[d_{u}]\,:\,|I|\geq 2}\gamma_{I}^{2}=1\), to (95), we obtain the following bound:

\[\|A\|\leq\frac{1}{2}\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-C_{1}( \log(R)+1)-\mathrm{h}^{\circ})\Big{)}.\]

Comparison of \(\sum_{I}\min_{\theta\in[q]}(\mathbb{E}_{u}f_{u,I}^{2})(\theta)\) and \(\min_{\theta\in[q]}(\mathbb{E}_{u}f_{u}^{2})(\theta)\) (and the same for \(g\)): Here is the last step toward the proof of the Proposition. Returning to (94), we have

\[\big{|}(\mathbb{E}_{u}f_{u}g_{u})(\theta)-(\mathbb{E}_{u}f_{u}g_{u })(\theta^{\prime})\big{|}\] \[\leq \frac{1}{2}\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-C_{1} (\log(R)+1)-\mathrm{h}^{\circ})\Big{)}\cdot\sqrt{\sum_{I}\min_{\theta\in[q]}( \mathbb{E}_{u}f_{u,I}^{2})(\theta)}\cdot\sqrt{\sum_{I}\min_{\theta\in[q]}( \mathbb{E}_{u}g_{u,I}^{2})(\theta)}.\]

Let us impose the **third assumption** on \(C_{0}\) that

\[C_{0}\geq C_{E.7}+\frac{2}{\varepsilon}\log(2)\]where \(C_{E.7}\) introduced in Corollary E.7. Recall that we have \(\mathrm{h}^{*}=\mathrm{h}^{\circ}+\frac{2}{\varepsilon}\log(2)\) from (87). We can invoke this Corollary to yield:

\[\forall\theta\in[q],\,\sqrt{\sum_{I}(\mathbb{E}_{u}f_{u,I}^{2})(\theta)}\leq \sqrt{2\mathbb{E}f_{u}^{2}(\theta)}.\]

Let \(\theta_{0}\in[q]\) be the value minimizing \(\theta\mapsto\sqrt{\mathbb{E}f_{u}^{2}(\theta)}\). Then,

\[\min_{\theta\in[q]}\sqrt{2\mathbb{E}f_{u}^{2}(\theta)}=\sqrt{2\mathbb{E}f_{u} ^{2}(\theta_{0})}\geq\sqrt{\sum_{I}(\mathbb{E}_{u}f_{u,I}^{2})(\theta_{0})} \geq\sqrt{\sum_{I}\min_{\theta\in[q]}(\mathbb{E}_{u}f_{u,I}^{2})(\theta)}.\]

Clearly, the same derivation also holds for \(g_{u}\). Together we conclude that

\[\big{|}(\mathbb{E}_{u}f_{u}g_{u})(\theta)-(\mathbb{E}_{u}f_{u}g_{ u})(\theta^{\prime})\big{|}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(u)-C_{2}(\log(R)+1) -\mathrm{h}^{\circ})\Big{)}\big{(}\min_{\theta\in[q]}(\mathbb{E}_{u}f_{u}^{2} )(\theta)\big{)}^{1/2}\big{(}\min_{\theta\in[q]}(\mathbb{E}_{u}g_{u}^{2})( \theta)\big{)}^{1/2}.\]

Finally, if we impose the **forth assumption** on \(C_{0}\) that

\[C_{0}\geq C_{2},\]

then the Proposition follows.

### Properties of \(f_{k}\): Products

The goal of this subsection is to establish the following.

**Proposition H.6**.: _There exists \(C=C(M,d)\geq 1\) so that the following holds. For any \(\rho^{\prime}\in T\) satisfying_

\[\mathrm{h}(\rho^{\prime})\geq\mathrm{h}^{\circ}+C(\log(R)+1)\]

_and a positive integer \(\mathrm{h}^{\circ}+C(\log(R)+1)\leq k_{1}\leq\mathrm{h}(\rho^{\prime})\). Consider a function \(f\) and \(g\) are \(\mathcal{B}_{\leq\rho^{\prime}}\) polynomials with \(\mathbb{E}f(X)=\mathbb{E}g(X)=0\). We decompose \(f\) and \(g\) according to Lemma D.1 with the given \(k_{1}\). Then, the following holds: For \(k_{1}\leq m,k\leq\mathrm{h}(\rho^{\prime})\),_

* _If_ \(\max\{m,k\}>k_{1}\)_,_ \[\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{\rho^{ \prime}}f_{k}g_{m})(\theta)-(\mathbb{E}_{\rho^{\prime}}f_{k}g_{m})(\theta^{ \prime})\big{|}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})- \max\{k,m\}-C(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}(\mathbb{E}f_{k}^{2}(X))^{1 /2}(\mathbb{E}g_{m}^{2}(X))^{1/2}.\]
* _If_ \(k=m=k_{1}\)_,_ \[\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{\rho^{ \prime}}f_{k}g_{m})(\theta)-(\mathbb{E}_{\rho^{\prime}}f_{k}g_{m})(\theta^{ \prime})\big{|}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})-2k_ {1}-C(\log(R)+1))\Big{)}(\mathbb{E}f_{k}^{2}(X))^{1/2}(\mathbb{E}g_{m}^{2}(X ))^{1/2}.\]

The proof mirrors the structure used in Proposition F.1. In this case, we rely on both Proposition E.1 and Proposition H.2. Through this subsection, let

\[C^{\circ}=C^{\circ}(M,d)\]

be the constant described in the Proposition. The functions \(f\), \(g\), and \(k_{1}\) are as introduced in the Proposition.

Assuming without lose of generality that \(m\leq k\), we apply the reasoning from (72) in Proposition F.1, yielding

\[(\mathbb{E}_{\rho^{\prime}}f_{k}g_{m})(\theta)\] \[= \mathbb{E}\Big{[}\Big{(}\sum_{u\in D_{k}(\rho^{\prime})}(\mathbb{E} _{u}\tilde{f}_{u})(X)\Big{)}\Big{(}\sum_{u\in D_{k}(\rho^{\prime})}(\mathbb{E} _{u}g_{m,u})(X)\Big{)}\,\Big{|}\,X_{\rho^{\prime}}=\theta\Big{]}+\sum_{u\in D_ {k}(\rho^{\prime})}(\mathbb{E}_{\rho^{\prime}}\tilde{f}_{u}g_{m,u})(\theta)\] \[-\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\Big{[}(\mathbb{E}_{u} \tilde{f}_{u})(X_{u})(\mathbb{E}_{u}g_{m,u})(X_{u})\,\Big{|}\,X_{\rho^{\prime} }=\theta\Big{]},\]

and hence,

\[(\mathbb{E}_{\rho^{\prime}}f_{k}g_{m})(\theta)-(\mathbb{E}_{\rho^ {\prime}}f_{k}g_{m})(\theta^{\prime})\] \[= \big{(}\mathbb{E}_{\rho^{\prime}}(\mathbb{E}_{k}f_{k})(\mathbb{E} _{k}g_{m})\big{)}(\theta)-\big{(}\mathbb{E}_{\rho^{\prime}}(\mathbb{E}_{k}f_{ k})(\mathbb{E}_{k}g_{m})\big{)}(\theta^{\prime})\] \[+\sum_{u\in D_{k}(\rho^{\prime})}\big{(}(\mathbb{E}_{\rho^{\prime }}\tilde{f}_{u}g_{m,u})(\theta)-(\mathbb{E}_{\rho^{\prime}}\tilde{f}_{u}g_{m,u} )(\theta^{\prime})\big{)}\] \[-\Big{(}\sum_{u\in D_{k}(\rho^{\prime})}\big{(}(\mathbb{E}_{\rho ^{\prime}}\big{(}\mathbb{E}_{u}\tilde{f}_{u})(\mathbb{E}_{u}g_{m,u})\big{)}( \theta)-\big{(}(\mathbb{E}_{\rho^{\prime}}\big{(}\mathbb{E}_{u}\tilde{f}_{u}) (\mathbb{E}_{u}g_{m,u})\big{)}(\theta^{\prime})\Big{)}.\] (97)

Similar to the derivation of (64) from Proposition F.1. The proof is dedicated into estimating the above three summands.

We begin with the following estimate:

**Lemma H.7**.: _There exists a constant \(C=C(M,d)\) so that the following holds. Suppose \(C^{\circ}\geq C_{H,2}\), where \(C_{H.2}\) is the constant introduced in Proposition H.2. Then, the following holds: For \(u\in D_{k}(\rho^{\prime})\),_

1. _if_ \(k>k_{1}\)_, then_ \[\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{\rho^{ \prime}}\tilde{f}_{u}g_{m,u})(\theta)-\mathbb{E}_{\rho^{\prime}}\tilde{f}_{u}g _{m,u})(\theta^{\prime})\big{|}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})-k-C( \log(R)+1)-\mathrm{h}^{\circ})\Big{)}(\mathbb{E}\tilde{f}_{u}^{2}(X))^{1/2}( \mathbb{E}g_{m,u}^{2}(X))^{1/2};\]
2. _if_ \(k=m=k_{1}\)_, then_ \[\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{\rho^{ \prime}}\tilde{f}_{u}g_{m,u})(\theta)-\mathbb{E}_{\rho^{\prime}}\tilde{f}_{u}g _{m,u})(\theta^{\prime})\big{|}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})-2k- C)\Big{)}(\mathbb{E}\tilde{f}_{u}^{2}(X))^{1/2}(\mathbb{E}g_{m,u}^{2}(X))^{1/2}.\]

Proof.: Step 1. Bound \(\mathbb{E}|\tilde{f}_{u}(X)g_{m,u}(X)-\mathbb{E}\tilde{f}_{u}g_{m,u}|\) from above: By Holder's inequality,

\[\underbrace{\mathrm{Var}\big{[}(\mathbb{E}_{u}\tilde{f}_{u}g_{m, u})(X_{u})\big{]}}_{\ell_{2}\text{-norm}}\leq \underbrace{\max_{\theta\in[q]}\big{|}(\mathbb{E}_{u}\tilde{f}_{u}g _{m,u})(\theta)-\mathbb{E}\tilde{f}_{u}g_{m,u}\big{|}}_{\ell_{\infty}\text{-norm}} \cdot\underbrace{\mathbb{E}\big{[}(\mathbb{E}_{u}\tilde{f}_{u}g_{m,u})(X_{u}) -\mathbb{E}\tilde{f}_{u}g_{m,u}\big{|}}_{\ell_{1}\text{-norm}}\] \[\leq \sqrt{C_{A.5}\mathrm{Var}\big{[}(\mathbb{E}_{u}\tilde{f}_{u}g_{m, u})(X_{u})\big{]}}\cdot\mathbb{E}\big{|}\tilde{f}_{u}(X)g_{m,u}(X)-\mathbb{E} \tilde{f}_{u}g_{m,u}|\] \[\Leftrightarrow \sqrt{\mathrm{Var}\big{[}(\mathbb{E}_{u}\tilde{f}_{u}g_{m,u})(X_{ u})\big{]}}\leq \sqrt{C_{A.5}\mathbb{E}}|\tilde{f}_{u}(X)g_{m,u}(X)-\mathbb{E}\tilde{f}_{u}g_{m,u }|,\] (98)

where we applied (9) from Lemma A.5 with \(C_{A.5}\) is the constant introduced in the Lemma. Further, relying on (98), together with (9) and (8) from the Lemma A.5, we have

\[\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{\rho^{ \prime}}\tilde{f}_{u}g_{m,u})(\theta)-\mathbb{E}_{\rho^{\prime}}\tilde{f}_{u}g _{m,u})(\theta^{\prime})\big{|}\] \[\leq 2C_{A.5}(\mathrm{h}(\rho^{\prime})-k)^{q}\mathrm{h}^{(\rho^{ \prime})-k}\max_{\theta\in[q]}\big{|}(\mathbb{E}_{u}\tilde{f}_{u}g_{m,u})( \theta)-\mathbb{E}\tilde{f}_{u}g_{m,u}|\] \[\leq 2C_{A.5}^{2}(\mathrm{h}(\rho^{\prime})-k)^{q}\mathrm{h}^{(\rho^{ \prime})-k}\sqrt{C_{A.5}}\mathbb{E}|\tilde{f}_{u}(X)g_{m,u}(X)-\mathbb{E} \tilde{f}_{u}g_{m,u}|\] \[= C_{1}\exp(-\varepsilon(\mathrm{h}(\rho^{\prime})-k))\mathbb{E}| \tilde{f}_{u}(X)g_{m,u}(X)-\mathbb{E}\tilde{f}_{u}g_{m,u}|,\] (99)where

\[C_{1}=2C_{A.5}^{5/2}\cdot\max_{n\in\mathbb{N}}n^{q}\exp(-0.1\varepsilon n).\]

Case 1: \(m<k.\) Here we can simply recycle the estimate from (76):

\[\mathbb{E}|\tilde{f}_{u}(X)g_{m,u}(X)-\mathbb{E}\tilde{f}_{u}g_{m,u }|\leq 2\mathbb{E}|\tilde{f}_{u}(X)g_{m,u}(X)|\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}\big{(}k-C_{2}(\log(R)+1)-\mathrm{ h}^{\circ}\big{)}\Big{)}(\mathbb{E}\tilde{f}_{u}^{2}(X))^{1/2}(\mathbb{E}g_{m,u}^{2}(X ))^{1/2},\] (100)

where

\[C_{2}=\frac{2}{\varepsilon}\Big{(}\frac{3}{2}+\frac{1}{2}\log(d)+\log(C_{D.1} )\Big{)}+C_{E.1}+2\cdot\frac{2}{\varepsilon}\log(2),\]

where \(C_{D.1}\) is the constant introduced in Lemma D.1 and \(C_{E.1}\) is the constant introduced in Proposition E.1.

Case 2: \(k_{1}<m=k.\)

This is the case where we need Proposition H.2. With the assumption that \(C^{\circ}\geq C_{H.2},\) where \(C_{H.2}\geq 1\) is the constant introduced in the Proposition, we have

\[m=k>k_{1}\geq\mathrm{h}^{\circ}+C^{\circ}(\log(R)+1)\geq\mathrm{h}^{\circ}+C_{ H.2}(\log(R)+1),\]

so that we could apply the Proposition to get

\[\mathbb{E}|\tilde{f}_{u}(X)g_{m,u}(X)-\mathbb{E}\tilde{f}_{u}g_{m,u}|\] \[\leq \max_{\theta,\theta^{\prime}}\big{|}(\mathbb{E}_{u}f_{u}g_{u})( \theta)-(\mathbb{E}_{u}f_{u}g_{u})(\theta^{\prime})\big{|}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(k-C_{H.2}(\log(R)+1)-\mathrm{ h}^{\circ})\Big{)}(\mathbb{E}\tilde{f}_{u}^{2}(X))^{1/2}(\mathbb{E}g_{m,u}^{2}(X ))^{1/2}.\]

Case 3: \(k_{1}=m=k\) The last case is straightforward:

\[\mathbb{E}|\tilde{f}_{u}(X)g_{m,u}(X)-\mathbb{E}\tilde{f}_{u}g_{m,u}|\leq 2\mathbb{E}|\tilde{f}_{u}(X)g_{m,u}(X)|\leq 2\sqrt{\mathbb{E} \tilde{f}_{u}^{2}(X)}\sqrt{\mathbb{E}g_{m,u}^{2}(X)}.\]

By taking \(C_{3}=\max\{C_{2},C_{H.2}\}+\frac{2}{\varepsilon}\log(C_{1}),\) the statement of the Lemma follows with \(C=C_{3}.\) 

As an analogue of the above Lemma, we also have

**Lemma H.8**.: _There exists a constant \(C=C(M,d)\) so that the following holds. Suppose \(C^{\circ}\geq C_{H.2}\) is the constant introduced in Proposition H.2. Then, the following holds: For \(u\in D_{k}(\rho^{\prime})\),_

1. _if_ \(k>k_{1},\) _then_ \[\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{\rho^{ \prime}}\big{(}\mathbb{E}_{u}\tilde{f}_{u})(\mathbb{E}_{u}g_{m,u})\big{)}( \theta)-\big{(}(\mathbb{E}_{\rho^{\prime}}\big{(}\mathbb{E}_{u}\tilde{f}_{u})( \mathbb{E}_{u}g_{m,u})\big{)}(\theta^{\prime})\big{|}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})-k-C (\log(R)+1)-\mathrm{h}^{\circ})\Big{)}(\mathbb{E}\tilde{f}_{u}^{2}(X))^{1/2}( \mathbb{E}g_{m,u}^{2}(X))^{1/2}.\]
2. _if_ \(k=m=k_{1},\) _then_ \[\big{|}(\mathbb{E}_{\rho^{\prime}}\big{(}\mathbb{E}_{u}\tilde{f}_ {u})(\mathbb{E}_{u}g_{m,u})\big{)}(\theta)-\big{(}(\mathbb{E}_{\rho^{\prime}} \big{(}\mathbb{E}_{u}\tilde{f}_{u})(\mathbb{E}_{u}g_{m,u})\big{)}(\theta^{ \prime})\big{|}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})-2k- C)\Big{)}(\mathbb{E}\tilde{f}_{u}^{2}(X))^{1/2}(\mathbb{E}g_{m,u}^{2}(X))^{1/2}.\]

Since the proof is simpler and the structure is the same as that for Lemma H.7, we will outline a sketch proof in this case.

Proof.: Let \(a_{u}(x_{u})=(\mathbb{E}_{u}\tilde{f}_{u})(x_{u})\) and \(b_{u}=(\mathbb{E}_{u}g_{m,u})(x_{u})\). Repeating the first step of the proof of Lemma H.7, we have

\[\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{\rho^{\prime}}a_{u}b_{u}) (\theta)-\mathbb{E}_{\rho^{\prime}}a_{u}b_{u})(\theta^{\prime})\big{|}\leq C_{1}\exp(-\varepsilon(\mathrm{h}(\rho^{\prime})-k)) \mathbb{E}|a_{u}(X)b_{u}(X)-\mathbb{E}a_{u}b_{u}|,\]

with

\[C_{1}:=2C_{A.5}^{5/2}\cdot\max_{n\in\mathbb{N}}n^{q}\exp(-0.1\varepsilon n),\]

which is exactly the same constant stated in Lemma H.7. Next,

\[\mathbb{E}|a_{u}(X)b_{u}(X)-\mathbb{E}a_{u}b_{u}|\leq 2\mathbb{E}|a_{u}(X)b_{u} (X)|\leq 2\sqrt{\mathbb{E}a_{u}^{2}(X)}\sqrt{\mathbb{E}b_{u}^{2}(X)}\leq 2 \sqrt{\mathbb{E}a_{u}^{2}(X)}\sqrt{\mathbb{E}g_{m,u}^{2}(X)}.\]

If \(k>k_{1}\), we could apply (38) from Proposition E.1 to \(\tilde{f}_{u}\), and get

\[\sqrt{\mathbb{E}a_{u}^{2}(X)}\leq \exp\Big{(}-2\varepsilon(k-C_{E.1}(\log(R)+1)\underbrace{-\mathrm{ h}^{\circ}-\frac{2}{\varepsilon}\log(2))}_{-\mathrm{h}^{*}}\Big{)}\sqrt{ \mathbb{E}\tilde{f}_{u}^{2}(X)}.\]

Indeed, this tail bound is stronger than what we got from Lemma H.7. The remainning part involves combining these estimates with a suitable constant \(C\) so that the lemma holds. Given the argument was already presented in the proof of Lemma H.7, we will omit these details. 

Before bounding the summands in (97), let us bound \(\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\tilde{f}_{u}^{2}(X)\) and \(\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}g_{m,u}^{2}(X)\) from above by \(\mathbb{E}f_{k}^{2}(X)\) and \(\mathbb{E}g_{m}^{2}(X)\), respectively.

**Lemma H.9**.: _Suppose_

\[C^{\circ}\geq C_{F.2}+\frac{2}{\varepsilon}\log(2),\]

_where \(C_{F.2}\) is the constant introduced in Lemma F.2. Then,_

\[\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}g_{m,u}^{2}(X)\leq\max\left\{4,C_{ D.1}^{2}R^{4}\right\}\cdot\mathbb{E}g_{m}^{2}(X),\]

_and_

\[\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\tilde{f}_{u}^{2}(X)\leq\max\left\{4,C_{D.1}^{2}R^{4}\right\}\cdot\mathbb{E}f_{k}^{2}(X).\]

Proof.: Given that \(C^{\circ}\geq C_{F.2}+\frac{2}{\varepsilon}\log(2)\), we have

\[k_{1}\geq\mathrm{h}^{\circ}+C^{\circ}(\log(R)+1)>\mathrm{h}^{\circ}+\frac{2}{ \varepsilon}\log(2)+C_{F.2}(\log(R)+1)=\mathrm{h}^{*}+C_{F.2}(\log(R)+1),\]

and thus we could apply Lemma F.2. When \(m>k_{1}\), the lemma yields

\[\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}g_{m,u}^{2}(X)=\sum_{u\in D_{k}( \rho^{\prime})}\mathbb{E}\Big{[}\Big{(}\sum_{v\in D_{m}(u)}\tilde{g}_{v}(X) \Big{)}^{2}\Big{]}\leq\sum_{u\in D_{k}(\rho^{\prime})}\sum_{v\in D_{m}(u)}2 \mathbb{E}\tilde{g}_{v}^{2}(X)\leq 4\mathbb{E}g_{m}^{2}(X).\]

And in the case when \(m=k_{1}\), we use the same derivation with Lemma F.2 been replaced by (31) in Lemma D.1 to get

\[\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}g_{m,u}^{2}(X)\leq C_{D.1}R^{3}\cdot C _{D.1}R\mathbb{E}g_{m}^{2}(X).\]

Clearly, the same derivation also holds for the comparison of \(\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\tilde{f}_{u}^{2}(X)\) and \(\mathbb{E}f_{k}^{2}(X)\). 

Now, relying on the above two lemmas, we will estimate the second and third summand of (97):

**Corollary H.10**.: _There exists a constant \(C=C(M,d)\geq 1\) so that the following holds. Suppose_

\[C^{\circ}\geq\max\Big{\{}C_{H.2},C_{F.2}+\frac{2}{\varepsilon}\log(2)\Big{\}},\]

_where the constants are introduced in Proposition H.2 and Lemma F.2, respectively. Then,_1. _if_ \(k>k_{1}\)_, then_ \[\bigg{|}\sum_{u\in D_{k}(\rho^{\prime})}\big{(}(\mathbb{E}_{\rho^{ \prime}}\tilde{f}_{u}g_{m,u})(\theta)-(\mathbb{E}_{\rho^{\prime}}\tilde{f}_{u}g_ {m,u})(\theta^{\prime})\big{)}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})-k-C( \log(R)+1)-\mathrm{h}^{\circ})\Big{)}\big{(}\mathbb{E}f_{k}^{2}(X))^{1/2}( \mathbb{E}g_{m}^{2}(X))^{1/2}.\]
2. _if_ \(k=m=k_{1}\)_, then the above term above can be bounded by_ \[\exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})-2k-C)\Big{)}( \mathbb{E}f_{k}^{2}(X))^{1/2}(\mathbb{E}g_{m}^{2}(X))^{1/2}.\]

Proof.: Let \(C_{1}\) be the maximum of the two constants introduced in Lemma H.7 and Lemma H.8. For convenience, let

\[U:= \bigg{|}\sum_{u\in D_{k}(\rho^{\prime})}\big{(}(\mathbb{E}_{\rho ^{\prime}}\tilde{f}_{u}g_{m,u})(\theta)-(\mathbb{E}_{\rho^{\prime}}\tilde{f}_ {u}g_{m,u})(\theta^{\prime})\big{)}\] \[-\Big{(}\sum_{u\in D_{k}(\rho^{\prime})}\big{(}(\mathbb{E}_{\rho ^{\prime}}\big{(}\mathbb{E}_{u}\tilde{f}_{u})(\mathbb{E}_{u}g_{m,u})\big{)}( \theta)-\big{(}(\mathbb{E}_{\rho^{\prime}}\big{(}\mathbb{E}_{u}\tilde{f}_{u}) (\mathbb{E}_{u}g_{m,u})\big{)}(\theta^{\prime})\big{)}\bigg{|}.\]

By the two lemmas together with the triangle inequality, in the case when \(k>k_{1}\), we have

\[U\leq \sum_{u\in D_{k}(\rho^{\prime})}2\exp\Big{(}-\frac{\varepsilon}{2 }(2\mathrm{h}(\rho^{\prime})-k-C_{1}(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}( \mathbb{E}\tilde{f}_{u}^{2}(X))^{1/2}(\mathbb{E}g_{m,u}^{2}(X))^{1/2}\] \[\leq 2\exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})-k -C_{1}(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}\sqrt{\sum_{u\in D_{k}(\rho^{ \prime})}\mathbb{E}\tilde{f}_{u}^{2}(X)}\sqrt{\sum_{u\in D_{k}(\rho^{\prime})} \mathbb{E}g_{m,u}^{2}(X)}\] \[\leq 2\max\Big{\{}4,C_{D.1}^{2}R^{4}\Big{\}}\sqrt{\mathbb{E}f_{k}^{2} (X)}\sqrt{\mathbb{E}g_{m}^{2}(X)},\] (101)

where the last inequality follows from Lemma H.9. Similarly, when \(k=m=k_{1}\), we have

\[U\leq 2\exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})-2k-C_{1}) \Big{)}\sqrt{\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\tilde{f}_{u}^{2}(X)} \sqrt{\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}g_{m,u}^{2}(X)}.\] (102)

By setting

\[C=\frac{2}{\varepsilon}\left(C_{1}+\log(4)+\log(C_{D.1}^{2})\right),\]

the corollary follows. 

It remains to estimate the first summand of (97):

**Lemma H.11**.: _There exists a constant \(C=C(M,d)\geq 1\) so that the following holds. Suppose_

\[C^{\circ}\geq C_{F.2}+\frac{2}{\varepsilon}\log(2)\]

_where \(C_{F.2}\) is the constant introduced in Lemma F.2. Then,_

1. _if_ \(k>k_{1}\)_, then_ \[\max_{\theta,\theta^{\prime}\in[q]}\Big{|}\big{(}\mathbb{E}_{\rho ^{\prime}}(\mathbb{E}_{k}f_{k})(\mathbb{E}_{k}g_{m})\big{)}(\theta)-\big{(} \mathbb{E}_{\rho^{\prime}}(\mathbb{E}_{k}f_{k})(\mathbb{E}_{k}g_{m})\big{)}( \theta^{\prime})\Big{|}\] \[\leq \exp\Big{(}-\varepsilon(\mathrm{h}(\rho^{\prime})-C(\log(R)+1)- \mathrm{h}^{\circ})\Big{)}\sqrt{\mathbb{E}f_{k}^{2}(X)}\sqrt{\mathbb{E}g_{m}^{2 }(X)}.\]
2. _if_ \(k=m=k_{1}\)_, then the above term is bounded by_ \[\exp(-\varepsilon(\mathrm{h}(\rho^{\prime})-k_{1}-C(\log(R)+1)))\sqrt{ \mathbb{E}f_{k}^{2}(X)}\sqrt{\mathbb{E}g_{m}^{2}(X)}.\]Proof.: Observe that both \((\mathbb{E}_{k}f_{k})(x)=\sum_{u\in D_{k}(\rho^{\prime})}(\mathbb{E}_{u}\tilde{f}_{ u})(x_{u})\) and \((\mathbb{E}_{k}g_{m})(x)=\sum_{u\in D_{k}(\rho^{\prime})}(\mathbb{E}_{k}g_{m,u})(x_ {u})\) are both degree-1 polynomials with variables \((x_{u}\ :\ u\in D_{k}(\rho^{\prime}))\) satisfying

\[\mathbb{E}(\mathbb{E}_{k}g_{m})(X)=\mathbb{E}(\mathbb{E}_{k}f_{k})=0.\]

This allows us to apply Lemma G.4, yielding

\[\max_{\theta,\theta^{\prime}\in[q]}\Big{|}\big{(}\mathbb{E}_{ \rho^{\prime}}(\mathbb{E}_{k}f_{k})(\mathbb{E}_{k}g_{m})\big{)}(\theta)-\big{(} \mathbb{E}_{\rho^{\prime}}(\mathbb{E}_{k}f_{k})(\mathbb{E}_{k}g_{m})\big{)}( \theta^{\prime})\Big{|}\] \[\leq 2\max_{\theta}\Big{|}\big{(}\mathbb{E}_{\rho^{\prime}}(\mathbb{ E}_{k}f_{k})(\mathbb{E}_{k}g_{m})\big{)}(\theta)-\mathbb{E}\big{(}\mathbb{E}_{k}f_{ k})(\mathbb{E}_{k}g_{m})\big{)}\Big{|}\] \[\leq 2C_{G.4}R\exp(-\varepsilon(\mathrm{h}(\rho^{\prime})-k))\sqrt{ \sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\big{[}(\mathbb{E}_{u}\tilde{f}_{u} )^{2}(X)\big{]}}\sqrt{\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\big{[}( \mathbb{E}_{u}g_{m,u})^{2}(X)\big{]}},\]

where \(C_{G.4}\geq 1\) is the constant introduced in Lemma G.4. Next, we apply Lemma H.9 (which is why we need the assumption on \(C^{\circ}\)) to get

\[\sqrt{\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\big{[}(\mathbb{E}_{u}g_{m,u}) ^{2}(X)\big{]}}\leq\sqrt{\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}g_{m,u}^{2 }(X)}\leq\sqrt{\max\big{\{}4,C_{D.1}^{2}R^{4}\big{\}}\cdot\mathbb{E}g_{m}^{2} (X)}.\]

As for \(\sqrt{\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\big{[}(\mathbb{E}_{u}\tilde{f} _{u})^{2}(X)\big{]}}\), if \(k=m=k_{1}\), then we can apply the same derivation to get

\[\sqrt{\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\big{[}(\mathbb{E}_{u}\tilde{ f}_{u})^{2}(X)\big{]}}\leq\sqrt{\max\big{\{}4,C_{D.1}^{2}R^{4}\big{\}}\cdot \mathbb{E}f_{k}^{2}(X)}.\]

This leads to

\[\max_{\theta,\theta^{\prime}\in[q]}\Big{|}\big{(}\mathbb{E}_{\rho ^{\prime}}(\mathbb{E}_{k}f_{k})(\mathbb{E}_{k}g_{m})\big{)}(\theta)-\big{(} \mathbb{E}_{\rho^{\prime}}(\mathbb{E}_{k}f_{k})(\mathbb{E}_{k}g_{m})\big{)}( \theta^{\prime})\Big{|}\] \[\leq 2C_{G.4}\max\big{\{}4,C_{D.1}^{2}R^{4}\big{\}}\exp(-\varepsilon( \mathrm{h}(\rho^{\prime})-k))\sqrt{\mathbb{E}f_{k}^{2}(X)}\sqrt{\mathbb{E}g_{m} ^{2}(X)}.\]

If \(k>k_{1}\), then we can apply Proposition E.1 and Lemma H.9 to get

\[\sqrt{\sum_{u\in D_{k}(\rho^{\prime})}\mathbb{E}\big{[}(\mathbb{E }_{u}\tilde{f}_{u})^{2}(X)\big{]}}\] \[\leq \exp\Big{(}-\varepsilon(k-C_{E.1}(\log(R)+1)-\mathrm{h}^{\circ}- \frac{2}{\varepsilon}\log(2))\Big{)}\sqrt{\sum_{u\in D_{k}(\rho^{\prime})} \mathbb{E}\tilde{f}_{u}^{2}(X)}\] \[\leq \sqrt{\max\big{\{}4,C_{D.1}^{2}R^{4}\big{\}}}\exp\Big{(}- \varepsilon(k-C_{E.1}(\log(R)+1)-\mathrm{h}^{\circ}-\frac{2}{\varepsilon}\log( 2))\Big{)}\sqrt{\mathbb{E}f_{k}^{2}(X)}.\]

In this case, we have

\[\max_{\theta,\theta^{\prime}\in[q]}\Big{|}\big{(}\mathbb{E}_{\rho ^{\prime}}(\mathbb{E}_{k}f_{k})(\mathbb{E}_{k}g_{m})\big{)}(\theta)-\big{(} \mathbb{E}_{\rho^{\prime}}(\mathbb{E}_{k}f_{k})(\mathbb{E}_{k}g_{m})\big{)}( \theta^{\prime})\Big{|}\] \[\leq 2C_{G.4}\max\big{\{}4,C_{D.1}^{2}R^{4}\big{\}}\exp\Big{(}- \varepsilon(\mathrm{h}(\rho^{\prime})-C_{E.1}(\log(R)+1)-\mathrm{h}^{\circ}- \frac{2}{\varepsilon}\log(2))\Big{)}\sqrt{\mathbb{E}f_{k}^{2}(X)}\sqrt{\mathbb{ E}g_{m}^{2}(X)}.\]

By taking

\[C=C_{E.1}+\frac{2}{\varepsilon}\log(2)+\frac{1}{\varepsilon}\Big{(}\log(2C_{G.4 })+\log(C_{D.1}^{2})+4\Big{)},\]

both statements of the lemma follows. 

Proof of Proposition h.6.: Without lose of generality, it is sufficient to prove the case when \(m\leq k\).

First, we impose the **first assumption** that

\[C^{\circ}\geq\max\left\{C_{H.2},\,C_{F.2}+\frac{2}{\varepsilon}\log(2)\right\},\]where the constants are introduced in Proposition H.2 and Lemma F.2, respectively. This allows us to apply Corollary H.10 and Lemma H.11. For simplicity, let

\[C_{1}:=\max\{C_{H.10},\,C_{H.11}\}.\]

Then, combining the Corollary and the Lemma to the estimate (97) we can conclude that: For \(k_{1}\leq m\leq k\) with \(k>k_{1}\),

\[\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{\rho^{ \prime}}f_{k}g_{m})(\theta)-(\mathbb{E}_{\rho^{\prime}}f_{k}g_{m})(\theta^{ \prime})\big{|}\] \[\leq 2\exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})-k -C_{1}(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}(\mathbb{E}f_{k}^{2}(X))^{1/2}( \mathbb{E}g_{m}^{2}(X))^{1/2},\]

and in the case where \(k=m=k_{1}\), the above term is bounded by

\[2\exp\Big{(}-\frac{\varepsilon}{2}(2\mathrm{h}(\rho^{\prime})-2k-C_{1}(\log( R)+1))\Big{)}(\mathbb{E}f_{k}^{2}(X))^{1/2}(\mathbb{E}g_{m}^{2}(X))^{1/2}.\]

Then, the proof of the proposition follows by making the **second assumption** on \(C^{\circ}\) that

\[C^{\circ}\geq C_{1}+\frac{2}{\varepsilon}\log(2).\]

### Proof of Theorem G.3

Proof.: Now we are ready to establish the main theorem. As usual, let \(C_{0}=C_{0}(M,d)\) denote the constant introduced in the statement of the Theorem. The value of \(C_{0}\) will be determined as the proof proceeds.

Applying Theorem B.6 with \(\mathcal{A}\) and \(\mathrm{h}^{*}=\mathrm{h}^{\circ}+\frac{2}{\varepsilon}\log(2)\) and \(c^{*}=1/2\), we conclude that

\[\mathrm{Var}\big{[}(\mathbb{E}_{\rho^{\prime}}f)(X)\big{]}\leq\exp\big{(}- \varepsilon(\mathrm{h}(\rho^{\prime})-C_{B.6}(\log(R)+1)-\mathrm{h}^{\circ} \big{)}\mathrm{Var}\big{[}f(X)\big{]}.\]

for any \(\mathcal{B}_{\leq\rho^{\prime}}\)-polynomial \(f\), where \(C_{B.6}=C(M,d,1/2)\) is the constant introduced by the theorem.

We impose the **first assumption** on \(C_{0}\) that

\[C_{0}\geq C_{B.6}+\frac{2}{\varepsilon}\log(2),\]

and conclude that

\[\mathrm{Var}\big{[}(\mathbb{E}_{\rho^{\prime}}f)(X)\big{]}\leq\exp\big{(}- \varepsilon(\mathrm{h}(\rho^{\prime})-C_{0}(\log(R)+1)-\mathrm{h}^{\circ}) \mathrm{Var}\big{[}f(X)\big{]}.\]

Now, it remains to show that with the suitable choice of \(C_{0}\), for any \(\rho^{\prime}\) with \(\mathrm{h}(\rho^{\prime})\geq\mathrm{h}^{\circ}+C_{0}(\log(R)+1)\) and any two \(\mathcal{B}_{\leq\rho^{\prime}}\)-polynomials \(f\) and \(g\), we have

\[\max_{\theta\in[q]}|(\mathbb{E}_{\rho^{\prime}}fg)(\theta)-\mathbb{E}fg|\leq \exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{\prime})-\mathrm{h}^{\circ }-C_{0}(\log(R)+1))\Big{)}\sqrt{\min_{\theta}(\mathbb{E}_{\rho^{\prime}}f^{2}) (\theta)\min_{\theta^{\prime}}(\mathbb{E}_{\rho^{\prime}}g^{2})(\theta)}.\]

Let

\[C_{1}:=\max\Big{\{}C_{H.6},\frac{2}{\varepsilon}\log(2)+C_{F.1}+t_{0}\Big{\}},\]

where

* \(t_{0}\) is the constant such that \(\sum_{t=t_{0}}^{\infty}\exp\Big{(}-\frac{\varepsilon}{2}t\Big{)}\leq\frac{1}{2}\),
* \(C_{H.6}\) is the constant introduced in Proposition H.6, and
* \(C_{F.1}=C(M,d,1/2)\) is the constant introduced in Proposition F.1.

Next, let

\[k_{1}=\left\lceil\mathrm{h}^{\circ}+C_{1}(\log(R)+1)\right\rceil.\]

The choice of \(C_{1}\) and \(k_{1}\) allow us to apply Proposition H.6 and Proposition F.1 toward both \(f\) and \(g\).

Next, we impose the **second assumption** on \(C_{0}\) that

\[C_{0}\geq 2C_{1}+2.\]

This assumption implies that there is a **gap** between \(h(\rho^{\prime})\) and \(k_{1}\), which is necessary for the proof.

Now, we fix such \(\rho^{\prime}\) and consider two \(\mathcal{B}_{\leq\rho^{\prime}}\)-polynomial \(f\) and \(g\) with \(\mathbb{E}f(X)=\mathbb{E}g(X)=0\). Further, consider the decomposition of \(f\) and \(g\) according to Lemma D.1 with the above chosen \(k_{1}\).

First, by our choice of \(C_{1}\), we have

\[k_{1}\geq\underbrace{\left\lceil\mathrm{h}^{\circ}+\frac{2}{\varepsilon}\log (2)\right\rceil}_{=\mathrm{h}^{*}}+C_{F.1}(\log(R)+1)+t_{0}\rceil.\]

This assumption allow us to recycle the partial step in the proof of Theorem B.6 to obtain (80):

\[\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}\mathbb{E}f_{k}^{2}(X)\leq 2 \mathbb{E}f^{2}(X)\quad\text{ and }\quad\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]} \mathbb{E}g_{k}^{2}(X)\leq 2\mathbb{E}g^{2}(X).\] (103)

Second, with our assumption that \(k_{1}\geq\left\lceil\mathrm{h}^{\circ}+C_{H.6}(\log(R)+1)\right\rceil\), we can apply Proposition H.6 to get

\[\max_{\theta,\theta^{\prime}\in[q]}\left|(\mathbb{E}_{\rho^{ \prime}}fg)(\theta)-(\mathbb{E}_{\rho^{\prime}}fg)(\theta^{\prime})\right| \leq\sum_{m,k\in[k_{1},\mathrm{h}(\rho^{\prime})]}\max_{\theta, \theta^{\prime}\in[q]}\left|(\mathbb{E}_{\rho^{\prime}}f_{k}g_{m})(\theta)-( \mathbb{E}_{\rho^{\prime}}f_{k}g_{m})(\theta^{\prime})\right|\] \[\leq\sum_{m,k\in[k_{1},\mathrm{h}(\rho^{\prime})]}a_{km}\alpha_{k }\beta_{m}=\vec{\alpha}^{\top}A\vec{\beta}\leq\|\vec{\alpha}\|\|A\|\|\vec{ \beta}\|,\]

where \(\vec{\alpha}=(\alpha_{k_{1}},\alpha_{k_{2}},\ldots,\alpha_{\mathrm{h}(\rho^{ \prime})})\) with \(\alpha_{k}=\sqrt{\mathbb{E}f_{k}^{2}(X)}\), \(\vec{\beta}=(\beta_{k_{1}},\beta_{k_{2}},\ldots,\beta_{\mathrm{h}(\rho^{ \prime})})\) with \(\beta_{m}=\sqrt{\mathbb{E}g_{m}^{2}(X)}\), and \(A=(a_{km})_{k,m\in[k_{1},\mathrm{h}(\rho^{\prime})]}\) with

\[a_{km}:=\begin{cases}\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{ \prime})+\mathrm{h}(\rho^{\prime})-\max\{k,m\}-C_{H.6}(\log(R)+1)-\mathrm{h}^{ \circ})\Big{)}&\max\{k,m\}>k_{1}\\ \exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{\prime})+\mathrm{h}(\rho^{ \prime})-2k_{1}-C_{H.6}(\log(R)+1))\Big{)}&k=m=k_{1}.\end{cases}\]

Together with (103), we have

\[\|\vec{\alpha}\|\|A\|\|\vec{\beta}\|\leq 2\|A\|\sqrt{\mathbb{E}f^{2}(X)} \sqrt{\mathbb{E}g^{2}(X)}.\]

The next goal is to bound \(\|A\|\) from above. Notice the fact that the matrix \(A\) is symmetric implies there exists a unit vector \(\vec{\gamma}\) such that \(\|A\|=\vec{\gamma}^{\top}A\vec{\gamma}\). Now we fix such vector \(\vec{\gamma}\). Relying on the fact that \(a_{km}\geq 0\),

\[\|A\|=\sum_{k,m\in[k_{1},\mathrm{h}(\rho^{\prime})]}a_{km}\gamma_{k}\gamma_{m }\leq\sum_{k,m\in[k_{1},\mathrm{h}(\rho^{\prime})]}\frac{a_{km}}{2}(\gamma_{k} ^{2}+\gamma_{m}^{2})=\sum_{m\in[k_{1},\mathrm{h}(\rho^{\prime})]}\gamma_{m}^{2 }\Big{(}\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}a_{km}\Big{)}.\]

Clearly, from the definition of \(a_{km}\), the term \(\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}a_{km}\) is maximized when \(m=k_{1}\).

\[\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}a_{kk_{1}}= \exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{\prime})+ \mathrm{h}(\rho^{\prime})-2k_{1}-C_{H.6}(\log(R)+1))\Big{)}\] \[+\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}\exp\Big{(}-\frac{ \varepsilon}{2}(\mathrm{h}(\rho^{\prime})+\mathrm{h}(\rho^{\prime})-k-C_{H.6 }(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}\] \[= \exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{\prime})-C_{H.6 }(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}.\] \[\cdot\bigg{(}\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{ \prime})-2k_{1}+\mathrm{h}^{\circ})\Big{)}+\sum_{k\in[k_{1},\mathrm{h}(\rho^{ \prime})]}\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{\prime})-k)\Big{)} \bigg{)}.\]First,

\[\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}\exp\Big{(}-\frac{\varepsilon}{2}( \mathrm{h}(\rho^{\prime})-k)\Big{)}\leq\frac{1}{1-\exp(-\varepsilon/2)}\leq \frac{4}{\varepsilon}.\]

Second,

\[\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{\prime})-2k_{1} +\mathrm{h}^{\circ})\Big{)}\leq \exp\Big{(}-\frac{\varepsilon}{2}\Big{(}C_{0}(\log(R)+1)+\mathrm{ h}^{\circ}-2(\mathrm{h}^{\circ}+C_{1}(\log(R)+1)+1)+\mathrm{h}^{\circ}\Big{)}\Big{)}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(C_{0}-2C_{1}-2)(\log(R)+1) \Big{)}\leq 1,\]

which in turn implies that

\[\left(\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{\prime})-2k_{1}+ \mathrm{h}^{\circ})\Big{)}+\sum_{k\in[k_{1},\mathrm{h}(\rho^{\prime})]}\exp \Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{\prime})-k)\Big{)}\right) \leq\frac{5}{\varepsilon}.\]

Hence, we conclude that

\[\|A\|\leq\frac{5}{\varepsilon}\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}( \rho^{\prime})-C_{H.6}(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}.\]

Together we conclude that when \(\mathrm{h}(\rho)\geq\mathrm{h}^{\circ}+C_{1}(\log(R)+1)\), any two \(\mathcal{B}_{\leq\rho^{\prime}}\)-polynomials \(f\) and \(g\) with \(\mathbb{E}f(X)=\mathbb{E}g(X)=0\) satisfies

\[\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{\rho^{ \prime}}fg)(\theta)-(\mathbb{E}_{\rho^{\prime}}fg)(\theta^{\prime})\big{|}\] (104) \[\leq \frac{10}{\varepsilon}\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{ h}(\rho^{\prime})-C_{H.6}(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}\sqrt{\mathbb{E}f^{2}(X)} \sqrt{\mathbb{E}g^{2}(X)}.\]

Now, we impose the **third assumption** on \(C_{0}\) that

\[C_{0}\geq C_{H.6}+\frac{2}{\varepsilon}\log(20/\varepsilon),\]

then

\[\frac{10}{\varepsilon}\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{ \prime})-C_{H.6}(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}\leq\frac{10}{ \varepsilon}\exp\Big{(}-\frac{\varepsilon}{2}(C_{0}-C_{H.6})(\log(R)+1)\Big{)} \leq 1/2.\]

Next, we apply (104) to the special case that \(f=g\):

\[\mathbb{E}f^{2}(X)-\min_{\theta\in[q]}(\mathbb{E}_{\rho^{\prime} }f^{2})(\theta)\leq\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{ \rho^{\prime}}fg)(\theta)-(\mathbb{E}_{\rho^{\prime}}fg)(\theta^{\prime}) \big{|}\leq \frac{1}{2}\mathbb{E}f^{2}(X)\] \[\Rightarrow\mathbb{E}f^{2}(X)\leq 2\min_{\theta\in[q]}(\mathbb{E}_{ \rho^{\prime}}f^{2})(\theta).\]

Clearly, the same statemnet holds for \(g\) as well. Substituting these estimates back to (104), we can conclude that when \(\mathrm{h}(\rho)\geq\mathrm{h}^{\circ}+C_{0}(\log(R)+1)\), any two \(\mathcal{B}_{\leq\rho^{\prime}}\)-polynomials \(f\) and \(g\) with \(\mathbb{E}f(X)=\mathbb{E}g(X)=0\) satisfies

\[\max_{\theta,\theta^{\prime}\in[q]}\big{|}(\mathbb{E}_{\rho^{ \prime}}fg)(\theta)-(\mathbb{E}_{\rho^{\prime}}fg)(\theta^{\prime})\big{|}\] \[\leq \frac{20}{\varepsilon}\exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h} (\rho^{\prime})-C_{H.6}(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}\sqrt{\min_{ \theta\in[q]}(\mathbb{E}_{\rho^{\prime}}f^{2})(\theta)}\sqrt{\min_{\theta\in[q ]}(\mathbb{E}_{\rho^{\prime}}g^{2})(\theta)}\] \[\leq \exp\Big{(}-\frac{\varepsilon}{2}(\mathrm{h}(\rho^{\prime})-C_{0 }(\log(R)+1)-\mathrm{h}^{\circ})\Big{)}\sqrt{\min_{\theta\in[q]}(\mathbb{E}_{ \rho^{\prime}}f^{2})(\theta)}\sqrt{\min_{\theta\in[q]}(\mathbb{E}_{\rho^{ \prime}}g^{2})(\theta)}.\]

Therefore, the theorem follows.

## Appendix I Variance Estimate for degree 1 polynomial

This section is dedicated to prove Proposition C.3. Let us restate it here:

**Proposition I.1**.: _There exists a constant \(C=C(M,d)\geq 1\) so that the following holds: Fix \(\rho^{\prime}\in T\), and \(0\leq k\leq\mathrm{h}(\rho^{\prime})\), then for any degree 1 function \(f\) with variables \((x_{u}\,:\,u\in D_{k}(\rho^{\prime}))\). There exists functions \(f_{u}(x)=f_{u}(x_{u})\) for \(u\in D_{k}(\rho^{\prime})\) so that the following holds:_

1. \(f(X)=\sum_{u\in D_{k}(\rho^{\prime})}f_{u}(X_{u})\) _almost surely. (They may not agree as functions from_ \([q]^{T}\) _to_ \(\mathbb{R}\)_.)_
2. _For any_ \(v\in T_{\rho^{\prime}}\) _with_ \(\mathrm{h}(u)\geq k\)_,_ \[\sum_{u\in D_{k}(v)}\mathrm{Var}[f_{u}(X_{u})]\leq CR^{3}\mathrm{Var}\big{[} \sum_{u\in D_{k}(v)}f_{u}(X_{u})\big{]}.\]

**Example I.2**.: Suppose \(u,v\in\mathfrak{c}(\rho^{\prime})\) for \(u,v,\rho^{\prime}\in T\) and consider

\[M=\frac{1}{2}\begin{bmatrix}1&0&1&0\\ 0&1&0&1\\ 1&0&1&0\\ 0&1&0&1\end{bmatrix}.\]

Let us consider the function \(f(x)=f_{u}(x)+f_{v}(x)\) where

\[f_{u}(x)=\mathbf{1}_{1,3}(x_{u})=\begin{cases}1&\text{if $x_{u}\in\{1,3\}$},\\ 0&\text{otherwise}.\end{cases}\]

and \(f_{v}(x)=-\mathbf{1}_{1,3}(x_{v})\).

Condition on \(X_{\rho^{\prime}}\in\{1,3\}\), \(f(X_{u})+f(X_{v})=1-1=0\) condition on \(X_{\rho^{\prime}}\in\{1,3\}\) and condition on \(X_{\rho^{\prime}}\in\{2,4\}\), \(f(X_{u})+f(X_{v})=0-0=0\). Put it differntly, \(\mathrm{Var}[f(X)]=0\) since \(f(X)=0\) almost surely. However, observe that \(\pi\) is the uniform measure on \([4]\), which implies

\[\mathrm{Var}[f_{u}(X_{u})]=\mathrm{Var}[f_{v}(X_{v})]=\frac{1}{4}>0.\]

Therefore, it is not true that (23) holds for the standard (Efron-Stein) decomposition of \(f(x)=\sum_{v\in D_{k}(\rho^{\prime})}f_{v}(x_{v})\).

Let us make a simple observation to give the insight for the construction. If \(f(X_{u})\) is a function of \(X_{\mathfrak{p}(u)}\), then for each \(i\in[q]\), the function \(f\) must take the same value for all possible outcomes of \(X_{u}\) conditioned on \(X_{\mathfrak{p}(u)}=i\). In other words, the values of \(f\) are constant on the set

\[S_{i}=\mathrm{supp}(\mathrm{row}_{i}(M))\] (105)

for every \(i\in[q]\). Now, let us consider the case where \(f(X_{u})\) is a function of \(X_{\mathfrak{p}^{v}(u)}\). This can be reformulated as follows: for \(k\in[0,r-1]\), \(\mathbb{E}\big{[}f(X_{u})\,\big{|}\,X_{\mathfrak{p}^{k}(u)}\big{]}\) is a function of \(X_{\mathfrak{p}^{k+1}(u)}\). Equivalently, the values of \(M^{k}f\) are constant on the set \(S_{i}\) for every \(i\in[q]\).

Therefore, it is evident that the construction of the basis should primarily revolve around the sets \(\{S_{i}\}_{i\in[q]}\) and their interaction with \(M\).

Following from this discussion, the proof of the Proposition I.1 is divided into the following steps:

Step 1 (Section I.1): We try to give a precise description of when \(f(X_{u})\) is a function of \(X_{\mathfrak{p}^{k}(u)}\) for some \(k\in\mathbb{N}\). To this end, we introduce the following notation.

**Definition I.3**.: _We define the following partial order relation \(\leq\) on the collection of all partitions of \([q]\): Specifically, for two partitions \(\mathbf{P}\) and \(\mathbf{P}^{\prime}\), we say that \(\mathbf{P}\leq\mathbf{P}^{\prime}\) if \(\mathbf{P}^{\prime}\) is finer than or equal to \(\mathbf{P}\)._

Further, there exists \(r\in\mathbb{N}\) such that \(\mathbf{P}^{t,0}\) for \(t\geq r\) is the trivial partition.

**Lemma I.4**.: _There exists a chain of partitions_

\[\mathbf{P}^{0,0}\geq\mathbf{P}^{1,0}\geq\mathbf{P}^{2,0}\dots\geq\mathbf{P}^{r,0}\geq\dots\]

_A function \(f:[q]\mapsto\mathbb{R}\) satisfies that \(f(X_{u})\) is a function of \(X_{\mathfrak{p}^{r}(u)}\) for some \(r\in\mathbb{N}\) if and only if \(f\) is a linear combination of \(\mathbf{1}_{P}\) for \(P\in\mathbf{P}^{r,0}\)._(The double index for the partitions is due to a technical reason, which will be clear in the construction of the partitions.)

Step 2 (Section I.2): Next, we try to extract a basis of functions according to the partitions fro the previous step, along with suitable quantitative estimates:

**Proposition I.5**.: _Let \(M\) be an ergodic and irreducible transition matrix defined on the state space \([q]\). We can construct_

* _a basis of functions from_ \([q]\) _to_ \(\mathbb{R}\)_, denoted as_ \[\{\xi_{\mathsf{w}}\}_{\mathsf{w}\in\mathsf{W}},\] _where_ \(\mathsf{W}\) _is a set of size_ \(q\)_,_
* _a function_ \[r:\mathsf{W}\to\mathbb{N}\cup\{0\},\] _and a constant_ \(C>1\) _(which depends on_ \(M\)_)_

_so that the following holds:_

1. _Let_ \[r_{0}:=\max_{\mathsf{w}\in\mathsf{W}}r(\mathsf{w}).\] _There exists unique_ \(\mathsf{w}_{0}\in\mathsf{W}\) _such that_ \(r(\mathsf{w}_{0})=r_{0}\)_. Moreover,_ \(\xi_{\mathsf{w}_{0}}\equiv 1\)_._
2. _For each_ \(\mathsf{w}\neq\mathsf{w}_{0}\)_,_ \(\xi_{\mathsf{w}}(X_{u})\) _is a function of_ \(X_{v}\) _where_ \(v=\mathfrak{p}^{r(\mathsf{w})}(u)\) _and_ \(\mathbb{E}\xi_{\mathsf{w}}(X_{u})=0\)_._
3. \[\mathrm{Var}\big{[}\sum_{\mathsf{w}}c_{\mathsf{w}}\xi_{\mathsf{w}}(X_{u}) \big{]}\leq C(\max_{\mathsf{w}\,:\,r(\mathsf{w})\neq r_{0}}|c_{\mathsf{w}}|)^{ 2}.\]
4. _For any_ \(0\leq r^{\prime}<r_{0}\) _such that_ \(\{\mathsf{w}\in\mathsf{W}\,:\,r(\mathsf{w})=r^{\prime}\}\) _is not empty,_ \[\mathbb{E}\mathrm{Var}\Big{[}\mathbb{E}\big{[}\sum_{\mathsf{w}\,:\,r(\mathsf{w} )=r^{\prime}}c_{\mathsf{w}}\xi_{\mathsf{w}}(X_{u})\,|X_{v}\big{]}\,\Big{|}\,X _{\mathfrak{p}(v)}\Big{]}\geq\frac{1}{C}(\max_{\mathsf{w}\,:\,r(\mathsf{w})=r^ {\prime}}|c_{\mathsf{w}}|)^{2}.\]
5. _For any_ \(0\leq r^{\prime}<r_{0}\) _such that_ \(\{\mathsf{w}\in\mathsf{W}\,:\,r(\mathsf{w})<r^{\prime}\}\) _is not empty,_ \[\mathbb{E}\mathrm{Var}\Big{[}\mathbb{E}\big{[}\sum_{\mathsf{w}\,:\,r(\mathsf{w} )<r^{\prime}}c_{\mathsf{w}}\xi_{\mathsf{w}}(X_{u})\,|X_{v}\big{]}\,\Big{|}\,X _{\mathfrak{p}(v)}\Big{]}\leq C(\max_{\mathsf{w}\,:\,r(\mathsf{w})<r^{\prime} }|c_{\mathsf{w}}|)^{2}.\]

**Remark I.6**.: For \(\mathsf{w}\in\mathsf{W}\) and \(l\in[r(\mathsf{w})]\), let

\[\xi_{\mathsf{w}}^{(l)}:=M^{l}\xi_{\mathsf{w}},\] (106)

where we treated \(\xi\) as an vector in \(\mathbb{R}^{[q]}\). Equivalently,

\[\xi^{(l)}(\theta)=\mathbb{E}\big{[}\xi(X_{u})\,|\,X_{v}=\theta\big{]}\]

where \(u,v\in T\) are vertices such that \(v=\mathfrak{p}^{l}(u)\).

Step 3 (Section I.3): Finally, we will use the basis from the previous step to decompose degree-1 polynomials to prove Proposition I.1.

### Partitions of \([q]\)

Let us begin with the following observation.

**Lemma I.7**.: _Suppose \(\{O_{\alpha}\}_{\alpha\in I}\) is a collection of non-empty subsets of \([q]\). Then, there exists a unique partition \(\mathbf{P}\) of \([q]\) that satisfies the following 2 conditions:_

1. _For each_ \(\alpha\in I\) _and_ \(P\in\mathbf{P}\)_, either_ \(O_{\alpha}\in P\) _or_ \(O_{\alpha}\cap P=\emptyset\)2. _For any other partition_ \(\mathbf{P}^{\prime}\) _that also satisfies the above property,_ \(\mathbf{P}^{\prime}\leq\mathbf{P}\)_._

Proof.: The proof can be carried out by constructing the partition \(\mathbf{P}\).

Without lose of generality, we may assume the collection \(\{O_{\alpha}\}_{\alpha\in I}\) contains \(\big{\{}\{\theta\}\big{\}}_{\theta\in[q]}\), since for a singleton \(\{\theta\}\) and a set \(P\), it is always true that either \(\{\theta\}\subseteq P\) or \(\{\theta\}\cap P=\emptyset\). Consequently, we may assume

\[\bigcup_{\alpha\in I}O_{\alpha}=[q].\] (107)

First, we define an equivalence relation \(\simeq\) on \(\{O_{\alpha}\}_{\alpha\in I}\) as follows: For any \(\alpha,\alpha^{\prime}\in I\), we denote \(O_{\alpha}\simeq O_{\alpha^{\prime}}\) if there exists a chain \((\alpha_{1},\alpha_{2},\ldots,\alpha_{l})\) such that \(O_{\alpha_{i-1}}\cap O_{\alpha_{i}}\neq\emptyset\) for \(i\in[l]\). Let \(I_{1},\ldots,I_{k_{0}}\subseteq I\) be the partition of \(I\) such that \(\{O_{\alpha}\}_{\alpha\in I_{k}}\) for \(k\in[k_{0}]\) form the equivalence classes of the relation. Now, let \(\mathbf{P}:=\{P_{1},\ldots,P_{k_{0}}\}\), where

\[P_{k}:=\cup_{\alpha\in I_{k}}O_{k}.\]

**Claim 1:** For every \(\alpha\in I\) and \(k\in[k_{0}]\), either \(O_{\alpha}\subseteq P_{k}\) or \(O_{\alpha}\cap P_{k}=\emptyset\).

To prove this claim, consider any \(\alpha\) and \(k\) described above. Suppose \(O_{\alpha}\cap P_{k}\neq\emptyset\). Let \(\theta\in O_{\alpha}\cap P_{k}\) and pick an index \(\alpha^{\prime}\in I_{k}\) such that \(\theta\in O_{\alpha^{\prime}}\). Such an index exists because \(P_{k}=\bigcup_{\alpha^{\prime\prime}\in I_{k}}O_{\alpha^{\prime\prime}}\). Then, we have \(O_{\alpha}\cap O_{\alpha^{\prime}}\neq\emptyset\), implying \(\alpha\in I_{k}\). Consequently, \(O_{\alpha}\subseteq P_{k}\). Therefore, the claim is proven.

**Claim 2:**\(\mathbf{P}\) is a partition of \([q]\).

We need to verify three properties:

1. \(\bigcup_{k\in[k_{0}]}P_{k}=[q]\),
2. \(\forall k\in[k_{0}]\), \(P_{k}\neq\emptyset\), and
3. \(P_{k}\cap P_{k^{\prime}}=\emptyset\) whenever \(k\neq k^{\prime}\).

First, for each \(\theta\in[q]\), by (107), there exists \(\alpha\in I\) such that \(\theta\in O_{\alpha}\). Then, \(\theta\in O_{\alpha}\in P_{k}\) where \(k\) is the index such that \(\alpha\in I_{k}\). Hence, we conclude that \(\bigcup_{k\in[k_{0}]}P_{k}=[q]\).

Second, for each \(k\in[k_{0}]\), let \(\alpha\in I_{k}\). We have \(\emptyset\neq O_{\alpha}\subseteq P_{k}\). Thus, \(P_{k}\) is not an empty set.

Finally, for any distinct \(k,k^{\prime}\in[k_{0}]\), suppose \(\theta\in P_{k}\cap P_{k^{\prime}}\). By (107), let \(\alpha\in I\) be the index so that \(\theta\in O_{\alpha}\). Hence, both \(O_{\alpha}\cap P_{k}\) and \(O_{\alpha}\cap P_{k^{\prime}}\). In particular, it is necessary that \(\alpha\in I_{k}\) and \(\alpha\in I_{k^{\prime}}\), which forces \(k=k^{\prime}\), leading to a contradiction. Therefore, \(P_{k}\cap P_{k^{\prime}}=\emptyset\) whenever \(k\neq k^{\prime}\). Hence, the claim follows.

**Claim 3:**\(\mathbf{P}^{\prime}\leq\mathbf{P}\) for any \(\mathbf{P}^{\prime}\) described in the statement.

To prove the claim, it suffices to show that for any \(P^{\prime}\in\mathbf{P}^{\prime}\) and \(P_{k}\in\mathbf{P}\) with \(k\in[k_{0}]\), if \(P^{\prime}\cap P_{k}\neq\emptyset\), then \(P_{k}\subseteq P^{\prime}\).

Let us consider an arbitrary pair of \(P^{\prime}\in\mathbf{P}^{\prime}\) and \(P_{k}\in\mathbf{P}\) and assume that \(P^{\prime}\cap P_{k}\neq\emptyset\). There exists an index \(\alpha\) such that \(O_{\alpha}\cap P^{\prime}\cap P_{k}\neq\emptyset\). Based on the assumptions regarding \(\mathbf{P}\) and \(\mathbf{P}^{\prime}\), we have \(\alpha\in I_{k}\) and \(O_{\alpha}\subseteq P^{\prime}\).

For every other \(\alpha^{\prime}\in I_{k}\), there exists a chain \((\alpha=\alpha_{0},\alpha_{1},\ldots,\alpha_{l_{0}}=\alpha^{\prime})\) such that \(O_{\alpha_{l-1}}\cap O_{\alpha_{l}}\neq\emptyset\) for \(l\in[l_{0}]\). Observe that if \(O_{\alpha_{l-1}}\subseteq P^{\prime}\), then \(O_{\alpha_{l}}\subseteq P^{\prime}\), due to \(O_{\alpha_{l}}\cap P^{\prime}\supseteq O_{\alpha_{l}}\cap O_{\alpha_{l-1}}\neq\emptyset\). With \(O_{0}\subseteq P^{\prime}\) as our starting point, we can apply this observation repeatedly to conclude that \(O_{\alpha^{\prime}}\subset P^{\prime}\). Since the argument works for every \(\alpha^{\prime}\in I_{k}\), we conclude that \(P_{k}=\bigcup_{\alpha^{\prime\prime}\in I_{k}}O_{\alpha^{\prime\prime}}\subseteq P ^{\prime}\).

**Definition I.8**.: _For any given collection of subsets \(\{O_{\alpha}\}_{\alpha\in I}\) of [q], let \(\mathbf{P}(\{O_{\alpha}\}_{\alpha\in I})\) denote the partition \(\mathbf{P}\) defined in Lemma I.7.__For any given partition \(\mathbf{Q}\) of \([q]\), let_

\[\mathbf{P}_{\mathrm{SC}}(\mathbf{Q}):=\mathbf{P}\big{(}\{Q\}_{Q\in\mathbf{Q}} \cup\{S_{i}\}_{i\in[q]}\big{)}.\]

**Remark I.9**.: Clearly, \(\mathbf{P}_{\mathrm{SC}}(\mathbf{Q})\leq\mathbf{Q}\).

**Definition I.10**.: _Let_

\[\mathbf{P}^{0,0}=\big{\{}\{1\},\{2\},\ldots,\{q\}\big{\}}\]

_and_

\[\mathbf{P}^{1,0}=\mathbf{P}_{SC}(\mathbf{P}^{0,0}).\]

Let us remark that \(\mathbf{P}^{1,0}\) is the finest partition of \([q]\) so that each part \(P\in\mathbf{P}^{1,0}\) either contains \(S_{i}\) or disjoint from \(S_{i}\) for \(i\in[q]\).

We use double indices for indexing the partitions because constructing such a chain of partitions requires the creation of multiple partitions along the way, as we will illustrate shortly.

To proceed, let us begin with a simple observation.

**Lemma I.11**.: _If \(P\in\mathbf{P}^{1,0}\), then_

\[M\mathbf{1}_{P}=\mathbf{1}_{Q}\]

_where_

\[Q=\{i\in[q]\,:\,S_{i}\subseteq P\}.\]

_Suppose \(\mathbf{P}^{1,0}=\{P_{1},P_{2},\ldots,P_{k_{0}}\}\). Then, the collection \(\mathbf{Q}:=\{Q_{1},Q_{2},\ldots,Q_{k_{0}}\}\) where_

\[M\mathbf{1}_{P_{i}}=\mathbf{1}_{Q_{i}}\]

_is also a partition provided that \(M\) is irreducible._

Proof.: For \(i\) with \(S_{i}\cap P=\emptyset\), it is immediate that \((M\mathbf{1}_{P})_{i}=0\). Conversely, when \(S_{i}\cap P\neq\emptyset\), it is necessary that \(S_{i}\subseteq P\). Consequently, \((M\mathbf{1}_{P})_{i}=\sum_{j\in[q]}M_{ij}=1\).

To establish that \(\mathbf{Q}\) is a partition, we need to demonstrate the following three conditions:

1. \(Q_{k}\cap Q_{k^{\prime}}=\emptyset\) for all distinct \(k,k^{\prime}\in[k_{0}]\).
2. \(\bigcup_{k\in[k]}Q_{k}=[q]\).
3. \(Q_{k}\neq\emptyset\) for all \(k\in[k_{0}]\).

For the first condition, suppose there exists \(i\in Q_{k}\cap Q_{k^{\prime}}\) for some distinct \(k\) and \(k^{\prime}\). By definition, \(S_{i}\subseteq P_{k}\) and \(S_{i}\subseteq P_{k^{\prime}}\), which is a contradiction. Hence, \(Q_{k}\cap Q_{k^{\prime}}=\emptyset\).

For the second condition, for every \(i\in[q]\), we know that \(S_{i}\subseteq P_{k}\) for some \(k\). Consequently, \(i\in Q_{k}\), ensuring \(\bigcup_{\alpha\in[k]}Q_{k}=[q]\).

For the third condition, if we assume \(Q_{k}=\emptyset\), implying that no \(i\in[q]\) satisfies \(S_{i}\subseteq P_{k}\), then \(M\) is not irreducible, since the states in \(P_{k}\) cannot be reached. 

**Definition I.12**.: _Let \(\mathbf{P}^{1,1}=\mathbf{Q}\) where \(\mathbf{Q}\) is the partition described in Lemma I.11._

**Lemma I.13**.: _If \(P\) is a finite union of parts in \(\mathbf{P}^{1,0}\), then_

\[M\mathbf{1}_{P}=\mathbf{1}_{Q}\] (108)

_where \(Q\) is a finite union of parts in \(\mathbf{P}^{1,1}\). The above map induces a bijection between subsets of \([q]\) that are finite union of parts of \(\mathbf{P}^{1,0}\) and subsets of \([q]\) that are finite union of parts of \(\mathbf{P}^{1,1}\), in which preserve the inclusion relation is preserved._

Proof.: Let us express \(\mathbf{P}^{1,0}=\{P_{1},P_{2},\ldots,P_{[k_{0}]}\}\) and \(\mathbf{P}^{1,1}=\{Q_{1},Q_{2},\ldots,Q_{k_{0}}\}\) where \(\mathbf{1}_{Q_{k}}=M\mathbf{1}_{P_{k}}\).

For each \(I\subseteq[k_{0}]\), let \(P_{I}=\bigcup_{k\in I}P_{k}\) and \(Q_{I}=\bigcup_{k\in I}Q_{k}\). Since \(\mathbf{1}_{P_{I}}=\sum_{k\in I}\mathbf{1}_{P_{k}}\) and \(\mathbf{1}_{Q_{I}}=\sum_{k\in I}\mathbf{1}_{Q_{k}}\), clearly we have

\[\mathbf{1}_{Q_{I}}=M\mathbf{1}_{P_{I}}.\]

Since naturally both finite union of parts of \(P\) and of \(Q\) are identified with a subset \(I\subset[k_{0}]\) in the above way, the statement of the lemma follows.

An immediate consequence is the following.

**Corollary I.14**.: _The transition matrix \(M\) induces a bijection between partitions that are \(\leq\mathbf{P}^{1,0}\) and partitions that are \(\leq\mathbf{P}^{1,1}\). For convenience, we adopt the following definitions:_

1. _For any partition_ \(\mathbf{P}\) _such that_ \(\mathbf{P}\leq\mathbf{P}^{1,0}\)_, define_ \[M\mathbf{P}:=\{Q\,:\,\exists P\in\mathbf{P}\text{ such that }\mathbf{1}_{Q}=M\mathbf{1}_{P}\}\leq\mathbf{P}^{1,1}.\]
2. _Given any_ \(\mathbf{P}\leq\mathbf{P}^{1,0}\) _and for each_ \(P\in\mathbf{P}\)_, let_ \(MP\) _represent a part in_ \(M\mathbf{P}\) _where_ \[\mathbf{1}_{MP}=M\mathbf{1}_{P}.\]

Next, we will build a collection of partitions \(\mathbf{P}^{r,s}\) for \(r\geq 0\) and \(0\leq s\leq r\) starting with \(\mathbf{P}^{0,0}=\left\{\{1\},\{2\},\ldots,\{q\}\right\}\) and establishing the relationship illustrated by the diagram below.

\[\begin{array}{ccccccccc}\mathbf{P}^{0,0}&\underset{SC}{\geq}&\mathbf{P}^{1, 0}&\geq&\mathbf{P}^{2,0}&\geq&\mathbf{P}^{3,0}&\geq&\mathbf{P}^{4,0}&\ldots\\ &&\downarrow&&\downarrow&&\downarrow&&\downarrow&&\\ &&\mathbf{P}^{1,1}&\underset{SC}{\geq}&\mathbf{P}^{2,1}&\geq&\mathbf{P}^{3,1} &\geq&\mathbf{P}^{4,1}&\ldots\\ &&\downarrow&&\downarrow&&\downarrow&&\\ &&\mathbf{P}^{2,2}&\underset{SC}{\geq}&\mathbf{P}^{3,2}&\geq&\mathbf{P}^{4,2} &\ldots\\ &&&&&\downarrow&&\downarrow&&\\ &&&&&\mathbf{P}^{3,3}&\underset{SC}{\geq}&\mathbf{P}^{4,3}&\ldots\\ &&&&&&\downarrow&&\\ &&&&&&\mathbf{P}^{4,4}&\ldots\\ &&&&&&\ddots&\end{array}\]

( In the above diagram, \(\mathbf{P}\to\mathbf{Q}\) indicates that \(\mathbf{Q}=M\mathbf{P}\); \(\mathbf{Q}\underset{SC}{\geq}\mathbf{P}\) indicates \(\mathbf{P}=\mathbf{P}_{\mathrm{SC}}(\mathbf{Q})\).)

Indeed, the initial definition of \(\mathbf{P}^{0,0}\) and the relation diagram determine the collection of partitions completely. Let us summarise it as a statement:

**Lemma I.15**.: _There exists a unique collection of partitions \(\{\mathbf{P}^{r,s}\}_{r\geq s\geq 0}\) that satisfies the following properties: For \(0\leq s<r\),_

1. \(\mathbf{P}^{0,0}=\left\{\{1\},\{2\},\ldots,\{q\}\right\}\)_._
2. \(\mathbf{P}^{r,s}\leq\mathbf{P}^{1,0}\)_._
3. \(\mathbf{P}^{r,s+1}=M\mathbf{P}^{r,s}\)_._
4. \(\mathbf{P}^{r+1,s}\leq\mathbf{P}^{r,s}\)_._
5. \(\mathbf{P}^{r+1,r}=\mathbf{P}_{\mathrm{SC}}(\mathbf{P}^{r,r})\)_._

Proof of Lemma I.15.: The proof is proceeded by induction. We assume that \(\mathbf{P}^{r,s}\) is constructed and uniquely determined for \(0\leq r<r_{0}\) and \(0\leq s\leq r\) for some \(r_{0}\geq 0\) so that it satisfies the properties described in the lemma.

We will define the partitions in the next column \(\{\mathbf{P}^{r_{0},s}\}_{s\in[0,r_{0}]}\) by starting with \(\mathbf{P}^{r_{0},r_{0}-1}=\mathbf{P}_{\mathrm{SC}}(\mathbf{P}^{r_{0}-1,r_{0} -1})\).

Besides constructing the rest of partitions, we also need to show that these partitions satisfy the following **list of conditions** ( let us denote it as **List A**): For \(s\in[0,r_{0},-1]\),

1. \(\mathbf{P}^{r_{0},s}\leq\mathbf{P}^{1,0}\).
2. \(\mathbf{P}^{r_{0},s}\leq\mathbf{P}^{r_{0}-1,s}\) for \(s\in[0,r_{0}-1]\).
3. \(\mathbf{P}^{r_{0},s+1}=M\mathbf{P}^{r_{0},s}\).

By definition of the map \(\mathbf{P}_{\mathrm{SC}}\), the first and second condition in the list are satisfied for \(s=r_{0}-1\). Relying on \(\mathbf{P}^{r_{0},r_{0}-1}\leq\mathbf{P}^{1,0}\), we can define \(\mathbf{P}^{r_{0},r_{0}}=M\mathbf{P}^{r_{0},r_{0}-1}\). Hence, the third condition in the list is also satisfied for \(s=r_{0}-1\).

It remains to construct \(\mathbf{P}^{r_{0},s}\) for \(s\in[0,r_{0}-2]\) and they satisfy those 3 conditions in the list. This can be proceeded inductively starting from \(s=r_{0}-2\).

**Claim**: For \(s\in[0,r_{0}-2]\), if \(\mathbf{P}^{r_{0},s+1}\leq\mathbf{P}^{r_{0}-1,s+1}\), then there exists a unique partition \(\mathbf{P}^{r_{0},s}\) which satisfies the conditions in **List A** for \(s\).

Suppose the Claim holds. With \(\mathbf{P}^{r_{0},r_{0}-1}\leq\mathbf{P}^{r_{0}-1,r_{0}-1}\), we could apply the claim repeatedly and the lemma follows. The rest of the proof is to show the claim holds.

Let us assume \(\mathbf{P}^{r_{0},s+1}\leq\mathbf{P}^{r_{0}-1,s+1}\) for some \(s\in[0,r_{0}-2]\). First, from our assumption on \(\{\mathbf{P}^{r,s}\}\) for \(0\leq s\leq r_{0}-1\), \(\mathbf{P}^{r_{0}-1,s+1}=M\mathbf{P}^{r_{0}-1,s}\). By Corollary I.14, \(\mathbf{P}^{r_{0}-1,s+1}\leq\mathbf{P}^{1,1}\). Since \(\mathbf{P}^{r_{0},s+1}\leq\mathbf{P}^{r_{0}-1,s+1}\), we conclude that \(\mathbf{P}^{r_{0},s+1}\leq\mathbf{P}^{1,1}\).

Applying Corollary I.14 again, we know there exists an unique partition \(\mathbf{P}\leq\mathbf{P}^{1,0}\) so that \(\mathbf{P}^{r_{0},s+1}=M\mathbf{P}\). We set \(\mathbf{P}^{r_{0},s}:=\mathbf{P}\). In particular, the choice of \(\mathbf{P}^{r_{0},s}\) is unique in order to satisfy the first and third condition from the list.

It remains to show that \(\mathbf{P}^{r_{0},s}\) also satisfies the second condition in **List A**. Notice that from Corollary I.14, the induced map of \(M\) on partitions preserves \(\leq\) relation. Hence, \(\mathbf{P}^{r_{0},s+1}\leq\mathbf{P}^{r_{0}-1,s+1}\) implies \(\mathbf{P}^{r_{0}-1,s}\leq\mathbf{P}^{r_{0}-1,s}\). Therefore, the claim holds.

Proof of Lemma I.4.: We start with the proof on the \(\Rightarrow\) implication. Suppose \(f\) is a function satisfied the first condition described in the lemma.

Since \(f(X_{u})\) is a function of \(X_{\mathbf{p}^{r}(u)}\), this is equivalent to

\[0= \mathbb{E}\Big{[}\mathrm{Var}\big{[}f(X_{u})\,\Big{|}\,X_{\mathbf{p}^{r}(u )}\big{]}\Big{]}.\]

Relying on the identity \(\mathrm{Var}[Y]=\mathbb{E}\mathrm{Var}[Y\,|\,Z]+\mathrm{Var}\big{[}\mathbb{E}[ Y\,|\,Z]\big{]}\) and \((X_{\mathbf{p}^{r}(u)},X_{\mathbf{p}^{r-1}(u)},\,\ldots,X_{u})\) is a Markov Chain,

\[\mathbb{E}\Big{[}\mathrm{Var}\big{[}f(X_{u})\,\Big{|}\,X_{\mathbf{p}^{r}(u)} \big{]}\Big{]}= \sum_{s=1}^{r}\mathbb{E}\Big{[}\mathrm{Var}\big{[}f(X_{u})\,\big{|}\,X_{ \mathbf{p}^{s}(u)}\big{]}\Big{]}.\]

Hence, \(\mathbb{E}\Big{[}\mathrm{Var}\big{[}f(X_{u})\,\big{|}\,X_{\mathbf{p}^{s}(u)} \big{]}\Big{]}=0\) for \(s\in[r]\), which in turn implies \(\mathbb{E}\big{[}f(X_{u})\,\big{|}\,X_{\mathbf{p}^{s-1}(u)}\big{]}\) conditioned on \(X_{\mathbf{p}^{s}(u)}\) is a constant function for each \(s\in[r]\). Equivalently, \(M^{s-1}f\) takes the same value for all elements in each \(S_{i}\) for \(i\in[q]\).

**Claim:** For \(s\in[r]\), if \(f\) can expressed in the form \(f=\sum_{P\in\mathbf{P}^{s-1,0}}c_{s-1,P}\mathbf{1}_{P}\), then it can be expressed in the form \(f=\sum_{P\in\mathbf{P}^{s,0}}c_{s,P}\mathbf{1}_{P}\).

Clearly, if the claim holds, then we can apply it repeatedly to draw the conclusion that \(f\) is a linear combination of \(\mathbf{1}_{P}\) for \(P\in\mathbf{P}^{r,0}\).

Now, we fix \(s\in[r]\) and assume \(f=\sum_{P\in\mathbf{P}^{s-1,0}}c_{s-1,P}\mathbf{1}_{P}\). Then,

\[\mathbb{E}\big{[}f(X_{u})\,\big{|}\,X_{\mathbf{p}^{s-1}(u)}=a\big{]}=(M^{s-1}f) (a)=\sum_{P\in\mathbf{P}^{s-1,0}}c_{s-1,P}M^{s-1}\mathbf{1}_{P}=\sum_{P\in \mathbf{P}^{s-1,0}}c_{s-1,P}\mathbf{1}_{P^{s-1}},\]

where for each \(P\in\mathbf{P}^{s-1,0}\), \(P^{s-1}\in\mathbf{P}^{s-1,s-1}\) is the corresponding part such that \(M^{s-1}\mathbf{1}_{P}=\mathbf{1}_{P^{s-1}}\). In other words, \(M^{s-1}f\) is a linear combination of \(\mathbf{1}_{P}\) for \(P\in\mathbf{P}^{s-1,s-1}\).

Because \(M^{s-1}f\) takes the same value not only for all elements in each \(S_{i}\) for \(i\in[q]\), but also for all elements in each \(P\) for \(P\in\mathbf{P}^{s-1,s-1}\), it implies \(M^{s-1}f\) takes the same value for all elements in each \(P^{\prime}\in\mathbf{P}_{\mathrm{SC}}(\mathbf{P}^{s-1,s-1})=\mathbf{P}^{s,s-1}\).

Together with the fact that the induced map of \(M\) on partitions preserves \(\leq\) relation, we conclude that \(c_{s-1,P_{1}}=c_{s-1,P_{2}}\) for \(P_{1},P_{2}\in P^{s-1,0}\) whenever \(P_{1}\) and \(P_{2}\) are both contained in some \(P\in\mathbf{P}^{s,0}\)Equivalently, within each \(P\in\mathbf{P}^{s,0}\), \(f\) is a constant function. Hence, we can express \(f\) as a linear combination of \(\mathbf{1}_{P}\) for \(P\in\mathbf{P}^{s,0}\).

For the \(\Leftarrow\) implication, suppose \(f\) is a linear combination of \(\mathbf{1}_{P}\) with \(P\in\mathbf{P}^{r,0}\).

What we need to show is for \(s\in[0,r-1]\), \(M^{s}f\) takes the same values for all elements in each \(S_{i}\) for \(i\in[q]\). From the chain \(\mathbf{P}^{r,0}\rightarrow\mathbf{P}^{r,1}\rightarrow\cdots\rightarrow \mathbf{P}^{r,r}\) and by (108), for \(s\in[0,r-1]\), \(M^{s}f\) is a linear combination of \(\mathbf{1}_{P}\) with \(P\in\mathbf{P}^{r,s}\).

Since \(\mathbf{P}^{s+1,s}=\mathbf{P}_{\mathrm{SC}}(bP^{s,s})\leq\mathbf{P}^{1,0}\) and \(\mathbf{P}^{s+1,s}\geq\cdots\geq\mathbf{P}^{r,s}\), we have \(\mathbf{P}^{r,s}\leq\mathbf{P}^{1,0}\), which implies \(M^{s}f\) takes the same values for all elements in each \(S_{i}\) for \(i\in[q]\). Therefore, the proof is completed.

Now, it remains to prove the second statement of the lemma.

First, if there exists \(r\in\mathbb{N}\) such that \(\mathbf{P}^{r,0}\) is trivial. Then \(\mathbf{P}^{t,0}\) is also trivial for \(t>r\) since \(\mathbf{P}^{t,0}\leq\mathbf{P}^{r,0}\). Hence, it is enough to show the existence of \(r\) such that \(\mathbf{P}^{r,0}\) is trivial.

From the assumption on \(M\), we knew that the stationary distribution \(\pi\) of \(M\) satisfies \(\min_{i\in[q]}\pi(i)>0\) and \(M^{r}\) converges entry-wise to the matrix whose row is identically \(\pi\). Therefore, for sufficiently large \(r\), \(\min_{i,j\in[q]}(M^{r})_{ij}>0\).

Now, let us fix such \(r\) and assume \(\mathbf{P}^{r,0}\) is not trivial. Let us express \(\mathbf{P}^{r,s}=\{P_{1}^{r,s},\ldots,P_{k_{r}}^{r,s}\}\) with for \(s\in[0,r]\) and \(k_{r}\geq 2\) where the index is assigned so that \(P_{k}^{r,s}=MP_{k}^{r,s-1}\) for \(s\in[r]\) and \(k\in[k_{r}]\). First,

\[\mathbf{1}_{P_{1}^{r,r}}=M^{r}\mathbf{1}_{P_{1}^{r,0}}.\]

With \(\mathbf{1}_{P_{1}^{r,0}}\) is non-negative and not zero, every component of \(M^{r}\mathbf{1}_{P_{1}^{r,0}}\) is non-zero. This forces \(P_{1}^{r,r}=[q]\), which contradicts to the assumption that \(\mathbf{P}^{r,r}\) is non-trivial.

### A basis of functions from \([q]\mapsto\mathbb{R}\) according to the partition

From now on, let \(r_{0}\) be the smallest non-negative integer such that \(\mathbf{P}^{r,0}\) is trivial. Consider the collection

\[\left\{(P,s)\,:\,s\in[0,r_{0}],P\in\mathbf{P}^{s,0}\right\}\]

We will establish an identification between elements of the set described above and words whose alphabet consists of non-negative integers. This identification is constructed through induction, following these steps:

* First, we identify \(([q],r_{0})\) with the word \((1)\).
* Assuming that elements in \(\{(P,s+1)\,:\,P\in\mathbf{P}^{s+1,0}\}\) have already been identified with unique words, we proceed as follows: For each \((P,s+1)\), suppose there are \(k\) pairs of \((P^{\prime},s)\) such that \(P^{\prime}\subseteq P\). We identify these \(k\) pairs with the words \((\mathsf{w},\mathrm{i})\) for \(\mathrm{i}\in[0,k-1]\), in any order of preference. For each \((P^{\prime},s)\), due to \(\mathbf{P}^{s,0}\) is a finer than or equal to \(\mathbf{P}^{s+1,0}\), there exists an unique pair \((P,s+1)\) so that \(P^{\prime}\subseteq P\). This guarantees the above procedure assigns each \((P^{\prime},s)\) a unique word.

We denote the set of words described above as \(\widetilde{\mathsf{W}}\), and we adopt the notation \(\mathsf{w}\sim(P,s)\) to indicate that \((P,s)\) is associated with the word \(\mathsf{w}\). For a given \(\mathsf{w}\in\widetilde{\mathsf{W}}\), we represent the corresponding pair as \((P_{\mathsf{w}},r(\mathsf{w}))\), where \(r(\mathsf{w})=r_{0}+1-\mathrm{len}(\mathsf{w})\).

Now, let us make the following observations

1. If \(\mathsf{w}\in\widetilde{\mathsf{W}}\) is a word with \(\mathrm{len}(\mathsf{w})<r_{0}+1\), then \((\mathsf{w},0)\in\widetilde{\mathsf{W}}\).
2. Each \((P,s)\) corresponds to a word of length \(r_{0}+1-s\).
3. Suppose \(\mathsf{w},\mathsf{w}^{\prime}\in\widetilde{\mathsf{W}}\) such that \(\mathsf{w}\) is a prefix of \(\mathsf{w}^{\prime}\). Then, \(P_{\mathsf{w}^{\prime}}\subseteq P_{\mathsf{w}}\).

[MISSING_PAGE_FAIL:71]

1. _If_ \(\mathsf{w}=(1)\)_,_ \(\xi_{\mathsf{w}}=\mathbf{1}_{P_{\mathsf{w}}}=1\)_._
2. _If_ \(\mathsf{w}\neq(1)\)_,_ \[\xi_{\mathsf{w}}(\theta):=\mathbf{1}_{P_{\mathsf{w}}}(\theta)-\mathbb{E}_{Y \sim\pi}\mathbf{1}_{P_{\mathsf{w}}}(Y).\]

**Remark I.19**.: The remaining goal in this subsection is to show that \(\mathfrak{B}\) is the desired basis described in Proposition I.5. We also remark that the first two properties stated in Proposition I.5 are already satisfied with this construction: \(\operatorname*{argmax}_{\mathsf{w}\in\mathsf{W}}r(\mathsf{w})=(1)\) with \(\xi_{(1)}=\mathbf{1}_{[q]}=1\); \(\xi_{\mathsf{w}}(X_{u})\) is a function of \(X_{v}\) where \(v=\mathfrak{p}^{r(\mathsf{w})}(u)\).

**Lemma I.20**.: _The collection \(\mathfrak{B}\) forms a linear basis for functions from \([q]\) to \(\mathbb{R}\)._

Proof.: Since there are exactly \(q\) functions, our goal is to show

\[\mathbb{R}^{[q]}=\operatorname*{span}(\{\xi_{\mathsf{w}}\}_{\mathsf{w}\in \mathsf{W}}),\]

and the R.H.S. is the same as \(\operatorname*{span}(\{\mathbf{1}_{P_{\mathsf{w}}}\}_{\mathsf{w}\in\mathsf{W}})\). It suffices to show for each \(i\in[q]\), \(\mathbf{1}_{\{i\}}\) can be expressed as a linear combination of \(\mathbf{1}_{P_{\mathsf{w}}}\) with \(\mathsf{w}\in\mathsf{W}\).

To prove this statement, we will use induction, showing that for \(s\) from \(r_{0}\) to \(0\), each \(\mathbf{1}_{P}\) with \(P\in\mathbf{P}^{s,0}\) can be expressed as a linear combination of of \(\mathbf{1}_{P_{\mathsf{w}}}\) with \(\mathsf{w}\in\mathsf{W}\). Since \(\mathbf{P}^{0,0}=\big{\{}\{1\},\ldots,\{q\}\big{\}}\), the proof follows once we establish this inductive statement.

First, when \(s=r\), since \(\mathbf{1}_{[q]}\) is the only part in \(\mathbf{P}^{r_{0},0}\) and \([q]=P_{(1)}\), the statement holds for \(s=r_{0}\).

Now, suppose the inductive hypothesis holds for \(s+1\) with \(s<r_{0}\). Pick any \(P\in\mathbf{P}^{s,0}\), let \(\mathsf{w}=(\mathsf{w}^{\prime},\mathsf{t})\) be the word associate with \((P,s)\). If \(\mathsf{t}=0\), then

\[\mathbf{1}_{P}=\mathbf{1}_{P_{\mathsf{w}^{\prime}}}-\sum_{P^{\prime\prime}} \mathbf{1}_{P^{\prime\prime}}\]

where the sum is taken over all parts \(P^{\prime\prime}\in\mathbf{P}^{s,0}\backslash\{P\}\) contained in \(P_{\mathsf{w}^{\prime}}\). Each \(P^{\prime\prime}\) in the summation (if it exists) must corresponds to a word of the form \((\mathsf{w}^{\prime},\mathsf{t}^{\prime\prime})\) with \(\mathsf{t}^{\prime\prime}>0\), or equivalently \((\mathsf{w}^{\prime},\mathsf{t}^{\prime\prime})\in\mathsf{W}\). From the induction hypothesis, \(\mathbf{1}_{P_{\mathsf{w}^{\prime}}}\) is a linear combination of \(\mathbf{1}_{P_{\mathsf{w}}}\) with \(\mathsf{w}\in\mathsf{W}\). Therefore, we conclude that \(\mathbf{1}_{P}\) is also a linear combination of \(\mathbf{1}_{P_{\mathsf{w}}}\) with \(\mathsf{w}\in\mathsf{W}\). If \(\mathsf{t}>0\), then \(\mathsf{w}\in\mathsf{W}\), and the same conclusion follows immediately. With no restriction on the choice of \(P\), the induction hypothesis holds for \(s\) as well.

Therefore, the lemma follows from induction. 

**Lemma I.21**.: _For any given \(0\leq r^{\prime}<r\), suppose \(\mathsf{W}_{r^{\prime}}:=\{\mathsf{w}\in\mathsf{W}\,:\,r(\mathsf{w})=r^{ \prime}\}\) is non-empty. Then, there exists a constant \(C\geq 1\) (which could depends on \(M\)) such that the following holds: Let \(u,v\in T\) be two vertices such that \(v=\mathfrak{p}^{r^{\prime}}(u)\). We have_

\[\mathbb{E}\mathrm{Var}\Big{[}\mathbb{E}\big{[}\sum_{\mathsf{w}\in\mathsf{W}_{ r^{\prime}}}c_{\mathsf{w}}\xi_{\mathsf{w}}(X_{u})\,\big{|}\,X_{v}\big{]}\, \Big{|}\,X_{\mathfrak{p}(v)}\Big{]}\geq\frac{1}{C}(\max_{\mathsf{w}\in\mathsf{ W}_{r^{\prime}}}|c_{\mathsf{w}}|)^{2}.\] (109)

Proof.: First, both sides of (109) scale by a factor \(h^{2}\) if every term \(c_{\mathsf{w}}\) is multiplied by \(h\in\mathbb{R}\). Hence, it suffices to establish the inequality in the case

\[\max_{\mathsf{w}\in\mathsf{W}_{r^{\prime}}}|c_{\mathsf{w}}|=1.\]

Given this, consider the set \(\big{\{}(c_{\mathsf{w}})_{\mathsf{w}\in\mathsf{W}_{r^{\prime}}}\,:\,\max_{ \mathsf{w}\in\mathsf{W}_{r^{\prime}}}|c_{\mathsf{w}}|=1\big{\}}\subseteq \mathbb{R}^{\mathsf{W}_{r^{\prime}}}\). It is compact set and

\[\mathbb{E}\mathrm{Var}\Big{[}\mathbb{E}\big{[}\sum_{\mathsf{w}\in\mathsf{W}_{ r^{\prime}}}c_{\mathsf{w}}\xi_{\mathsf{w}}(X_{u})\,\big{|}\,X_{v}\big{]}\, \Big{|}\,X_{\mathfrak{p}(v)}\Big{]}\] (110)

is continuous in \((c_{\mathsf{w}})_{\mathsf{w}\in\mathsf{W}_{r^{\prime}}}\) (it is a polynomial of \(c_{\mathsf{w}}\)). By a compact argument one can establish the existence of \(C\geq 1\) described in the lemma if for every \((c_{\mathsf{w}})_{\mathsf{w}\in\mathsf{W}_{r^{\prime}}}\) with \(\max_{\mathsf{w}\in\mathsf{W}_{r^{\prime}}}|c_{\mathsf{w}}|=1\),

\[\mathbb{E}\mathrm{Var}\Big{[}\mathbb{E}\big{[}\sum_{\mathsf{w}\in\mathsf{W}_{ r^{\prime}}}c_{\mathsf{w}}\xi_{\mathsf{w}}(X_{u})\,\big{|}\,X_{v}\big{]}\, \Big{|}\,X_{\mathfrak{p}(v)}\Big{]}>0.\]

[MISSING_PAGE_EMPTY:73]

### Proof of Proposition I.1

In this subsection, we consider soley degree \(1\) polynomial of the leave values.

**Definition I.23**.: _For any given \(\rho^{\prime}\in T\) and a degree-1 polynomial \(f\) of \(\{x_{u}\}_{u\in L_{\rho^{\prime}}}\), the function can be expressed uniquely in the form_

\[f(x)=\sum_{\mathsf{w}\in\mathsf{W},\;u\in L_{\rho^{\prime}}}c_{\mathsf{w},u} \xi_{\mathsf{w}}(x_{u})\] (113)

_where \(\{\xi_{\mathsf{w}}\}_{\mathsf{w}\in\mathsf{W}}\) is the basis introduced in Proposition I.5._

_For \(u\in T_{\rho^{\prime}}\), let \(f_{u}(x):=\sum_{\mathsf{w}\in\mathsf{W},\;v\in L_{u}}c_{\mathsf{w},v}\xi_{ \mathsf{w}}(x_{v})\). Observe that from this definition, for each \(0\leq l\leq r\),_

\[f(x)=\sum_{u\in T_{\rho^{\prime}}\;\operatorname{h}(u)=l}f_{u}(x).\]

_Further, for \(u\in T_{\rho^{\prime}}\backslash L_{\rho^{\prime}}\), let_

\[c_{\mathsf{w},u}:=\sum_{v\in L_{u}}c_{\mathsf{w},v}.\]

**Remark I.24**.: From the definition above, for each \(\rho^{\prime}\in T\) and degree-1 polynomial \(f\) of variables \(\{x_{u}\}_{u\in L_{\rho^{\prime}}}\), we have

\[\forall u\in T_{\rho^{\prime}},\,\forall x\in\mathbb{R}^{q},\,\,(\mathbb{E}_{ u}f_{u})(x)=\sum_{\mathsf{w}}c_{\mathsf{w},u}\xi_{\mathsf{w}}^{(l)}(x_{u}),\]

where \(\xi_{\mathsf{w}}^{(l)}(\theta)\) is introduced in Remark I.6.

**Proposition I.25**.: _There exists a constant \(C=C(M,d)\geq 1\) so that the following holds: Suppose_

\[f(x)=\sum_{u\in L_{\rho^{\prime}},\,\mathsf{w}\in\mathsf{W}}c_{\mathsf{w},u} \xi(x_{u})\]

_where \(\rho^{\prime}\in T\) is a node satisfying \(\operatorname{h}(\rho^{\prime})\leq r_{0}\) and_

\[c_{\mathsf{w},u}=c_{\mathsf{w},v}\]

_for \(u,v\in L_{\rho^{\prime}}\) satisfying \(\operatorname{h}(\rho(u,v))\leq r(\mathsf{w})\), where \(\rho(u,v)\) is the lowest common ancestor of \(u\) and \(v\). Then,_

\[\sum_{u\in L_{\rho^{\prime}}}\operatorname{Var}[f_{u}(X)]\leq CR^{3}\mathbb{ E}\mathrm{Var}\big{[}f(X)\,|\,X_{\rho^{\prime}}\big{]}\]

If Proposition I.25 is proven, then Proposition I.1 follows as a corollary:

_Proof of Proposition I.1._ Reduction to \(\operatorname{h}(\rho^{\prime})\leq r_{0}\): Without loss of generality, it is sufficient to consider degree \(1\) functions of \(L\), rather than degree \(1\) functions of variables in \(D_{k}(u)\) for some \(u\) in the tree and \(0\leq k\leq\operatorname{h}(u)\).

Recall that

\[D_{r_{0}}(\rho)=\{w\in T\,:\,\operatorname{h}(w)=r_{0}\}.\]

We know that we can express \(f(x)=\sum_{w\in D_{r_{0}}(\rho)}f_{w}(x)\) so that each of them is a degree-1 polynomial with variables \(\{x_{u}\}_{u\in L_{w}}\).

Together with the variance decomposition for degree-1 polynomials (See Lemma C.1)

\[\operatorname{Var}[f(X)]\geq\sum_{w\in D_{r_{0}}(\rho)}\mathbb{E}\mathrm{Var }[f_{w}(X_{w})\,|\,X_{w}],\]

it suffices to prove the same statement for degree-1 polynomials of \(x_{u}\) with \(u\in L_{\rho^{\prime}}\) for \(\rho^{\prime}\) satisfying \(\operatorname{h}(\rho^{\prime})\leq r_{0}\).

Now, we fix such \(\rho^{\prime}\) and consider

\[f(x)=\sum_{\mathsf{w},u\in L_{\rho^{\prime}}}c_{\mathsf{w},u}\xi_{\mathsf{w}}(x_{ u}).\]

Averaging the Coefficients: For each \(\mathsf{w}\in\mathsf{W}\) and for each \(u\in D_{r(\mathsf{w})}(\rho^{\prime})\), we know that for any \(v_{1},v_{2}\in L_{u}\),

\[\xi_{\mathsf{w}}(X_{v_{1}})=\xi_{\mathsf{w}}(X_{v_{2}})\]

almost surely. As a consequcune, we have

\[\sum_{v\in L_{u}}c_{\mathsf{w},v}\xi_{\mathsf{w}}(X_{v})=\sum_{v\in L_{u}}\frac {\sum_{v\in L_{u}}c_{\mathsf{w},v}}{|L_{u}|}\xi_{\mathsf{w}}(X_{v})\]

almost surely. Now, we repeat this averaging process for each \(\mathsf{w}\in\mathsf{W}\) and for each \(u\in D_{r(\mathsf{w})}(\rho^{\prime})\). We denote the resulting function by \(\tilde{f}\). While \(\tilde{f}\) and \(f\) may not be the same function, \(\tilde{f}(X)=f(X)\) almost surely. On the other hand, \(\tilde{f}\) is a function which satisfies the condition in Proposition I.25. Following from the proposition, we have

\[\sum_{u\in L}\mathbb{E}\mathrm{Var}[\tilde{f}_{u}(X)]\leq CR^{3}\mathbb{E} \mathrm{Var}[\tilde{f}(X)\,|\,X_{\rho^{\prime}}]=CR^{3}\mathrm{Var}[f(X)\,|\,X _{\rho^{\prime}}].\]

The proof is complete. 

Let us begin with an intermediate step toward the proof of the Proposition I.25.

**Lemma I.26**.: _Suppose \(f\) is a function described in Definition I.23. For any given \(1\leq l<r\) such that \(\mathsf{W}_{l}:=\{\mathsf{w}\in\mathsf{W}\,:\,r(\mathsf{w})=l\}\) is non-empty. Let \(u\in T_{\rho^{\prime}}\) with \(\mathrm{h}(u)=r(\mathsf{w})\), suppose_

\[t=\max_{\mathsf{w}\in\mathsf{W}_{l}}|c_{\mathsf{w},u}|>0.\]

_Then one of the following statement holds:_

* _Either_ \(\mathbb{E}\mathrm{Var}\big{[}(\mathbb{E}_{u}f_{u})(X_{u})\,|\,X_{\mathsf{p}(u )}\big{]}\geq\frac{\pi_{\min}}{2C_{0}}t^{2}\)_, or_
* \(\max_{\mathsf{w}\in\mathsf{W}_{<l}}|c_{\mathsf{w},u}|\geq\frac{\sqrt{\pi_{ \min}}}{2C_{0}}t\)_._

_Here, \(C_{0}\geq 1\) is the constant \(C\) described in Proposition I.5 and \(\pi_{\min}:=\min_{\theta\in[q]}\pi(\theta)\). Further, in the case when \(l=0\), then we simply have \(\mathbb{E}\mathrm{Var}\big{[}(\mathbb{E}_{u}f_{u})(X_{u})\,|\,X_{\mathsf{p}(u )}\big{]}\geq\frac{1}{C_{0}}t^{2}\)._

Proof.: We decompose \((\mathbb{E}_{u}f_{u})(x)\) into three components:

\[(\mathbb{E}_{u}f_{u})(x)=\sum_{\mathsf{w}\,:r(\mathsf{w})<l}c_{w,u}\xi_{w}^{(l )}(x_{u})+\sum_{\mathsf{w}\,:\,r(\mathsf{w})=l}c_{w,u}\xi_{w}^{(l)}(x_{u})+ \sum_{\mathsf{w}\,:\,r(\mathsf{w})>l}c_{w,u}\xi_{w}^{(l)}(x_{u}),\]

where \(\xi_{w}^{(l)}\) is introduced in Remark I.6.

For each \(\mathsf{w}\) with \(r(\mathsf{w})>l\), \(\xi_{w}^{(l)}(X_{u})\) is a function of \(X_{v}\) with \(v=\mathsf{p}^{r(\mathsf{w})-l}(u)\). Hence, the last component \(\sum_{\mathsf{w}\,:\,r(\mathsf{w})>l}c_{w,u}\xi_{w}^{(l)}(x_{u})\) is a constant function whenever we condition on \(X_{\mathsf{p}(u)}\). Consequently,

\[\mathbb{E}\mathrm{Var}\big{[}(\mathbb{E}_{u}f_{u})(X_{u})\,|\,X_{\mathsf{p}(u )}\big{]}=\mathbb{E}\mathrm{Var}\Big{[}\sum_{\mathsf{w}\,:\,r(\mathsf{w})<l}c_ {w,u}\xi_{w}^{(l)}(X_{u})+\sum_{\mathsf{w}\,:\,r(\mathsf{w})=l}c_{w,u}\xi_{w}^ {(l)}(X_{u})\,\Big{|}\,X_{\mathsf{p}(u)}\Big{]}.\] (114)

From Proposition I.5, we know that

\[\mathbb{E}\mathrm{Var}\Big{[}\sum_{\mathsf{w}\,:\,r(\mathsf{w})=l}c_{w,u}\xi_{ w}^{(l)}(X_{u})\,\Big{|}\,X_{\mathsf{p}(u)}\Big{]}\geq\frac{t^{2}}{C_{0}},\] (115)where the constant \(C_{0}\) is the constant \(C\) stated in the Proposition. Intuitively, from (115) it should be clear that if the R.H.S. of (114) is small, then \(\mathbb{E}\mathrm{Var}\Big{[}\sum_{\mathsf{w}\,:r(\mathsf{w})<l}c_{w,u}\xi_{w}^{( l)}(X_{u})\,\Big{|}\,X_{\mathsf{p}(u)}\Big{]}\) cannot be small. Let us derive this with a coarse estimate.

By (115), we know there exists \(\theta\in[q]\) such that

\[\mathrm{Var}\Big{[}\sum_{\mathsf{w}\,:r(\mathsf{w})=l}c_{w,u}\xi_{w}^{(l)}(X_{ u})\,\Big{|}\,X_{\mathsf{p}(u)}=\theta\Big{]}\geq\frac{t^{2}}{C_{0}}.\]

Now, suppose \(\mathrm{Var}\Big{[}\sum_{\mathsf{w}\,:r(\mathsf{w})<l}c_{w,u}\xi_{w}^{(l)}(X_{ u})\,\Big{|}\,X_{\mathsf{p}(u)}=\theta\Big{]}<\frac{t^{2}}{4C_{0}}\). We could apply triangle inequality to get

\[\geq \sqrt{\mathrm{Var}\Big{[}\sum_{\mathsf{w}\,:r(\mathsf{w})=l}c_{w,u}\xi_{w}^{(l)}(X_{u})\,\Big{|}\,X_{\mathsf{p}(u)}=\theta\Big{]}}-\,\sqrt{ \mathrm{Var}\Big{[}\sum_{\mathsf{w}\,:r(\mathsf{w})<l}c_{w,u}\xi_{w}^{(l)}(X_ {u})\,\Big{|}\,X_{\mathsf{p}(u)}=\theta\Big{]}}\] \[\geq \frac{t}{2C_{0}^{1/2}},\]

and together with (114),

\[\mathbb{E}\mathrm{Var}\big{[}(\mathbb{E}_{u}f_{u})(X_{u})\,|\,X_{ \mathsf{p}(u)}\big{]}\geq \pi(\theta)\mathrm{Var}\Big{[}\sum_{\mathsf{w}\,:r(\mathsf{w})<l }c_{w,u}\xi_{w}^{(l)}(X_{u})+\sum_{\mathsf{w}\,:r(\mathsf{w})=l}c_{w,u}\xi_{w }^{(l)}(X_{u})\,\Big{|}\,X_{\mathsf{p}(u)}\Big{]}\] \[\geq \frac{\pi_{\min}}{2C_{0}}t^{2}.\]

Consider the opposite case where \(\mathrm{Var}\Big{[}\sum_{\mathsf{w}\,:r(\mathsf{w})<l}c_{w,u}\xi_{w}^{(l)}(X_ {u})\,\Big{|}\,X_{\mathsf{p}(u)}=\theta\Big{]}\geq\frac{t^{2}}{4C_{0}}\). First,

\[\mathbb{E}\mathrm{Var}\Big{[}\sum_{\mathsf{w}\,:r(\mathsf{w})<l}c_{w,u}\xi_{w }^{(l)}(X_{u})\,\Big{|}\,X_{\mathsf{p}(u)}=\theta\Big{]}\geq\frac{\pi_{\min}} {4C_{0}}t^{2}.\]

By applying the 4th property stated in Proposition I.5, we conclude that

\[\max_{\mathsf{w}\,:r(\mathsf{w})<l}|c_{\mathsf{w},u}|\geq\frac{\sqrt{\pi_{\min }}}{2C_{0}}t.\]

In the case when \(l=0\). The argument is simpler, which follows directly from (114) and the Proposition I.5.

Proof of Proposition I.25.: Let \(t_{0}=\max_{\mathsf{w},u\in L_{\rho^{\prime}}}|c_{\mathsf{w},u}|\) and let \(\mathsf{w}^{\prime}\in\mathsf{W}\) and \(u^{\prime}\in L_{\rho^{\prime}}\) be the pair such that \(t_{0}=|c_{\mathsf{w}^{\prime},u^{\prime}}|\). Further, let \(l_{0}=r(\mathsf{w}^{\prime})\) and \(u_{0}=\mathsf{p}^{l}(u^{\prime})\).

If \(l_{0}>0\), then we have

\[|c_{\mathsf{w}^{\prime},u_{0}}|=\sum_{v\in L_{u}}|c_{\mathsf{w}^{\prime},v}| \geq|c_{\mathsf{w}^{\prime},u^{\prime}}|=t_{0},\]

where the first equality follows from the assumptions of the coefficients. We will try to construct a sequence of triples \((l_{k},t_{k},u_{k})\) indexed by \(k\) such that \((l_{k})_{k\geq 0}\) is strictly decreasing such that \(\mathsf{W}_{l_{k}}\neq\emptyset\), \(\mathrm{h}(u_{k})=l_{k}\), and \(t_{k}=\max_{\mathsf{w}\in\mathsf{W}_{l_{k}}}|c_{\mathsf{w},u_{k}}|\).

Suppose we have a triple \((l_{k},t_{k},u_{k})\) such that \(l_{k}\geq 0\), \(\mathrm{h}(u_{k})=l_{k}\), \(\mathsf{W}_{l_{k}}\neq\emptyset\), and \(t_{k}=\max_{\mathsf{w}\in\mathsf{W}_{l_{k}}}|c_{\mathsf{w},u_{k}}|\) for some index \(k\geq 0\).

We apply Lemma I.26 to get

1. Either \(\mathbb{E}\mathrm{Var}\big{[}(\mathbb{E}_{u_{k}}f_{u_{k}})(X_{u_{k}})\,\big{|} \,X_{\mathsf{p}(u_{k})}\big{]}\geq\frac{\pi_{\min}}{2C_{0}}t_{k}^{2}\), or 2. \(\max_{\mathsf{w}\in\mathsf{W}_{<\ell_{k}}}|c_{\mathsf{w},u_{k}}|\geq\frac{\sqrt{ \pi_{\min}}}{2C_{0}}t_{k}\). (This case cannot happen if \(\ell_{k}=0\).)

If the first case is true, then we terminate the process of finding next triple \((\ell_{k+1},t_{k+1},u_{k+1})\).

If the second case is true, let \(\mathsf{w}^{\prime\prime}\in\mathsf{W}\) be the vertex such that \(|c_{\mathsf{w}^{\prime\prime},u_{k}}|=\max_{\mathsf{w}\in\mathsf{W}_{<\ell_{ k}}}|c_{\mathsf{w},u_{k}}|\) and set \(\ell_{k+1}=r(w^{\prime\prime})\). Since

\[c_{\mathsf{w},u_{k}}=\sum_{u\in D_{t_{k+1}}(u_{k})}c_{\mathsf{w},u},\]

we have

\[t_{k+1}:=\max_{u\in T_{u_{k}}\,:\,\mathrm{h}(u)=\ell_{k+1}}|c_{\mathsf{w},u}| \geq\frac{1}{Rd^{\ell_{k}-\ell_{k+1}}}|c_{\mathsf{w},u_{k}}|=\frac{1}{Rd^{ \ell_{k}-\ell_{k+1}}}t_{k}.\] (116)

Further, let \(u_{k+1}=\operatorname{argmax}_{u\in T_{u_{k}}\,:\,\mathrm{h}(u)=\ell_{k+1}}|c _{\mathsf{w},u}|\).

In this way, we produce a new triple satisfying the same assumption as \((l_{k},t_{k},u_{k})\) described above.

Since \(l_{0}>l_{1}>l_{2}\dots\) is a monotone decreasing chain of non-negative number, it means this argument must terminated in \(r_{0}\) steps. Now, suppose it terminates at the \(k\)-th step, resulting a triple \((l_{k},t_{k},u_{k})\), and

\[\mathbb{E}\mathrm{Var}\big{[}(\mathbb{E}_{u_{k}}f_{u_{k}})(X_{u_{k}})\,\big{|} \,X_{\mathfrak{p}(u_{k})}\big{]}\geq\frac{\pi_{\min}}{2C_{0}}t_{k}^{2}\stackrel{{ (\ref{eq:l16})}}{{\geq}}\frac{\pi_{\min}}{2C_{0}}\big{(}\frac{\sqrt{\pi_{ \min}}}{2C_{0}}Rd\big{)}^{-2r_{0}}t_{0}^{2}.\]

On the other hand, from Proposition I.5,

\[\sum_{u\in L_{\rho^{\prime}}}\mathrm{Var}[f_{u}(X_{u})]\leq CRd^{r_{0}}t_{0}^ {2}.\]

Therefore, we conclude that

\[\sum_{u\in L_{\rho^{\prime}}}\mathrm{Var}[f_{u}(X_{u})]\leq C(M,d)R^{2r_{0}+1 }\mathbb{E}\mathrm{Var}\big{[}f(X)\,|\,X_{\rho^{\prime}}\big{]}.\]

## Appendix J Properties of Markov Chains and Galton-Watson Tree

#### j.0.1 Proof of Lemma a.5

Recall that real Jordon Canonical form of \(M\) is a \(q\times q\) diagonal block matrix \(\mathbf{J}=\operatorname{diag}(\mathbf{J}_{0},\mathbf{J}_{1},\dots,\mathbf{J }_{s_{1}})\) for some \(s_{1}\leq q\).

Since \(M\) is ergodic, the eigenspace corresponds to eigenvalue \(1\) is 1-dimensional. Thus, We may assume \(\mathbf{J}_{0}=[1]\) is the unique Jordan block corresponds to eigenvalue \(1\).

For each \(s\in[1,s_{1}]\), \(\mathbf{J}_{s}\) is either a \(m_{s}\times m_{s}\) matrix of the form \(\mathbf{J}_{s}=\begin{bmatrix}\lambda_{s}&1&&\\ &\lambda_{s}&\ddots&\\ &&\ddots&1\\ &&&\lambda_{s}\end{bmatrix}\) for some \(\lambda_{s}\in\mathbb{R}\) satisfying \(|\lambda_{s}|\leq\lambda\); or a \(J_{s}\) is a \(2m_{s}\times 2m_{s}\) matrix of the form \(\mathbf{J}_{s}=\begin{bmatrix}\lambda_{s}R_{s}&I_{2}&&\\ &\lambda_{s}R_{s}&\ddots&\\ &&\ddots&I_{2}\\ &&&\lambda_{s}R_{s}\end{bmatrix}\), where \(|\lambda_{s}|\leq\lambda\), and \(R_{s}=\begin{bmatrix}\cos(\theta_{s})&\sin(\theta_{s})\\ -\cos(\theta_{s})&\sin(\theta_{s})\end{bmatrix}\) is a rotation matrix in \(\mathbb{R}^{2}\) with parameter \(\theta_{s}\in(0,2\pi)\). In the later case, it corresponds to the conjugate pair of eigenvalues \(\lambda_{s}(\cos(\theta_{s})\pm\mathbf{i}\sin(\theta_{s}))\)According to Jordon Decomposition, there exists an invertible matrix \(P\) such that \(M=P\mathbf{J}P^{-1}\).

For \(i\in[1,q-1]\), let \(\phi_{i}\) be the \(i+1\)th column of \(P\). Because \(P\) is invertible, \(\{\phi_{i}\}_{i\in[q]}\) form a linear basis of functions from \([q]\) to \(\mathbb{R}\).

Since \(\pi\) is a left-eigenvector of \(M\) with eigenvalue \(1\), we have

\[\mathbb{E}_{Y\sim\pi}\phi_{i}(Y)=\pi^{\top}\phi_{i}=0,\]

because \(\phi_{i}\) is a sum of up to two generalized eigenvectors with eigenvalues not equal to \(1\).

(A generalized eigenvector \(v\) with eigenvalue \(\lambda^{\prime}\) of \(M\) is a vector which satisfies \((M-\lambda^{\prime})^{k}v=\vec{0}\) for some positive integer \(k\). Whenever \(\lambda^{\prime}\neq 1\),

\[\pi^{\top}v=(\frac{1}{(1-\lambda^{\prime})^{k}}\pi^{\top}(M-\lambda^{\prime}) ^{k})v=\frac{1}{(1-\lambda^{\prime})^{k}}\pi^{\top}\cdot\vec{0}=0.\]

If index \(i\) corresponds to \(\mathbf{J}_{s}\) which associated with a real eigenvalue, then \(\phi_{i}\) is a generalized eigenvector with eigenvalue \(\lambda_{s}\); And if \(\mathbf{J}_{s}\) associates with a complex conjugate pair or eigenvalues, then \(\phi_{i}\) is a sum of two generalized eigenvectors with eigenvalues \(\lambda_{s}(\cos(\theta_{s})+\mathbf{i}\sin(\theta_{s}))\) and \(\lambda_{s}(\cos(\theta_{s})-\mathbf{i}\sin(\theta_{s}))\), respectively. ) As a consequence, every function \(f:[q]\mapsto\mathbb{R}\) can be uniquely decomposed in the form

\[f=\mathbb{E}f+\sum_{i\in[q-1]}\delta_{i}\phi_{i}.\] (117)

With this unique decomposition, let us define a semi-norm

\[\|f\|_{M}=\max_{i\in[q-1]}|\delta_{i}|.\]

**Lemma J.1**.: _There exists \(C>0\) so that for every \(f:[q]\rightarrow\mathbb{R}\),_

\[C^{-1}\|f\|_{M}^{2}\leq\mathrm{Var}_{Y\sim\pi}(f(Y))\leq C\|f\|_{M}^{2}.\] (118)

Proof.: Without lose of generality, let \(f=\sum_{i\in[2,q]}\delta_{i}\phi_{i}\), since both \(\|f\|_{M}\) and \(\mathrm{Var}_{Y\sim\pi}(f(Y))\) are invariant under a constant shift.

Let \(D_{\pi}=\mathrm{diag}(\pi_{1},\dots,\pi_{q})\). Also, let \(\vec{\delta}=(0,\delta_{2},\dots,\delta_{q})\). Then,

\[\|f\|_{M}= \|\vec{\delta}\|_{\infty} \text{and} \mathrm{Var}_{Y\sim\pi}(f(Y))= \vec{\delta}^{\top}P^{\top}D_{\pi}P\vec{\delta}.\]

Let \(s_{\max}\) and \(s_{\min}\) be the maximum and minimum singular value of \(P^{\top}D_{\pi}P\), respectively. Together with \(q^{-1/2}\|\vec{\delta}\|_{2}\leq\|\vec{\delta}\|_{\infty}\leq\|\vec{\delta}\|_ {2}\), we have

\[s_{\min}^{2}q^{-1}\|f\|_{M}^{2}\leq s_{\min}^{2}q^{-1}\|\vec{\delta}\|_{2}^{2 }\leq\mathrm{Var}_{Y\sim\pi}(f(Y))\leq s_{\max}^{2}\|\vec{\delta}\|_{2}^{2} \leq s_{\max}^{2}\|f\|_{M}^{2}.\] (119)

If \(s_{\min}>0\), then we can complete the proof by taking \(C=\max\{s_{\max}^{2},q/s_{\min}^{2}\}\). It remains to show that \(s_{\min}>0\), or equvialently \(P^{\top}D_{\pi}P\) is invertible. Because \(M\) is ergodic, each entry of \(\pi\) is positive, and thus \(D_{\pi}\) is invertible. Hence, \(P^{\top}D_{\pi}P\) is invertible because it is a product of three invertible matrices.

**Lemma J.2**.: _There exists \(C>0\) so that for every \(f:[q]\rightarrow\mathbb{R}\),_

\[C^{-1}\|f\|_{M}\leq\|f-\mathbb{E}_{Y\sim\pi}f(Y)\|_{\infty}\leq C\|f\|_{M}.\] (120)

Proof.: This simply follows from both \(\|f\|_{M}\) and \(\|f-\mathbb{E}f\|_{\infty}\) are both norms on the finite dimensional space \(\{f:[q]\rightarrow\mathbb{R}\,:\,\mathbb{E}_{Y\sim\pi}f(Y)=0\}\). 

**Lemma J.3**.: _There exists \(C\geq 1\) depending on \(M\) such that For any function \(f:[q]\mapsto\mathbb{R}\) and \(k\in\mathbb{N}\),_

\[\|M^{k}f\|_{M}\leq Ck^{q}\lambda^{k}\|f\|_{M}.\] (121)

**Remark J.4**.: Notice that \(M^{k}f\) can be interpreted as

\[\mathbb{E}\big{[}f(X_{u})\,\big{|}\,X_{\mathfrak{p}^{k}(u)}=i\big{]}=(M^{k}f)(i),\]

for every \(u\in T\) where \(\mathfrak{p}^{k}(u)\) is well-defined.

Proof.: \[\|M^{k}f\|_{M}= \|P\mathbf{J}^{k}(\sum_{i\in[q]}\delta_{i}e_{i})\|_{M}=\|\mathbf{ J}^{k}(\sum_{i\in[2,q]}\delta_{i}e_{i})\|_{\infty}.\leq q\max_{i\in[2,q]}\| \delta_{i}\|\max_{i,j\in[2,q]}|\mathbf{J}^{k}_{ij}|.\] (122)

Notice that \(\mathbf{J}^{k}\) is the diagonal block matrix whose blocks \(\mathbf{J}^{k}_{s}\) for \(s\in[s_{1}]\). The block \(\mathbf{J}^{k}_{s}\) can be computed directly: In the case when \(\mathbf{J}_{s}\) corresponds to a complex conjugate pair of eigenvalues,

\[\mathbf{J}^{k}_{s}=\begin{bmatrix}\lambda^{k}_{s}R^{k}_{s}&\binom{k}{1}\lambda ^{k-1}_{s}R^{k-1}_{s}&\dots&\binom{k}{m_{s}-1}\lambda^{k-m_{s}+1}_{s}R^{k-m_{ s}+1}_{s}\\ &\lambda^{k}_{s}R^{k}_{s}&\ddots&\vdots\\ &&\ddots&\binom{k}{1}\lambda^{k-1}_{s}R^{k-1}_{s}\\ &&\lambda^{k}_{s}R^{k}_{s},\end{bmatrix}\] (123)

where we treat \(\binom{k}{r}=0\) if \(r>k\). It can be verified directly by induction, relying on the identity \(\binom{k}{r-1}+\binom{k}{r}=\binom{k+1}{r}\). Further, removing the \(R_{s}\) terms in the above equation we obtain the formula for \(\mathbf{J}^{k}_{s}\) when \(\mathbf{J}_{s}\) corresponds to a real eigenvalue.

Therefore, with \(\binom{k}{q}\leq k^{q}\), \(|\lambda^{r}_{s}|\leq\lambda^{r}\), and \(\max_{i,j}R^{r}_{s\,ij}\ <1\) for \(r\geq 1\), we obtain the bound

\[\max_{s\in[2,q]}\max_{i,j\in[q]}|(\mathbf{J}_{s}{}^{k})_{ij}|\leq C^{\prime}k ^{q}\lambda^{k},\] (124)

where \(C^{\prime}\) is a constant which depends on \(q\) and \(\lambda\).

Now we substitute the above bound into (122) to get

\[\|M^{k}f\|_{M}\leq qC^{\prime}k^{q}\lambda^{k}\|f\|_{M}.\]

The proof is completed by taking \(C=qC^{\prime}\).

Proof of Lemma a.5.: The proof of Lemma a.5 follows from the \(\|\cdot\|_{M}\) decay from Lemma J.3 and that both \(\operatorname{Var}[f]\) and \(\|f-\mathbb{E}f\|_{\infty}\) are comparable to \(\|f\|_{M}\) within a constant multiplicative factor (Lemma J.1 and Lemma J.2).

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA]  means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes]  to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claim in the abstract and introduction is Theorem 1.6. The appendix is dedicated to the proof of the theorem.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The result is a theoretical result which justifies low degree hardness for the tree reconstruction model. This is a first indication that Kesten-Stigum bound appears to be the computational-statistical barrier for the problem. In terms of the limitation, the paper does not recover the same result as shown in the paper of Kohler and Moseel [23], where they only deal with the case \(\lambda=0\).
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The paper provides a full set of assumptions and the appendix is dedicated to the complete proof for Theorem 1.6.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The result in the paper is a theoretical result. The paper does not include any experimental results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The result in the paper is a theoretical result. The paper does not include any experimental results.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The result in the paper is a theoretical result. The paper does not include any experimental results.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The result in the paper is a theoretical result. The paper does not include any experimental results.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The result in the paper is a theoretical result. The paper does not include any experimental results.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The result of the paper is a theoretical result, which suggests a previous known bound (Kesten-Stigum) is likely a statistical-computational bound for the tree reconstruction model. It is an generic result that suggest it is hard to design an efficient algorithm for the problem below the Kesten-Stigum bound. On the other hand, the result does not have any direct societal impact.

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The result in the paper is a theoretical result. The paper does not process and release any data or models.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.