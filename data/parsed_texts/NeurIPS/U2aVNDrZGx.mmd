# Benchmarking Complex Instruction-Following with Multiple Constraints Composition

 Bosi Wen\({}^{1,}\)

Equal contributionWork done when these authors interned at Zhipu AI.

Pei Ke\({}^{3,}\)

Equal contributionWork done when these authors interned at Zhipu AI.

Xiaotao Gu\({}^{2}\)

Lindong Wu\({}^{2}\)

Hao Huang\({}^{2}\)

Jinfeng Zhou\({}^{1}\)

Wenchuang Li\({}^{4,}\)

Binxin Hu\({}^{5,}\)

Work done when these authors interned at Zhipu AI.

Wendy Gao\({}^{2}\)

Jiaxin Xu\({}^{1}\)

Yiming Liu\({}^{1}\)

Jie Tang\({}^{1}\)

Hongning Wang\({}^{1}\)

Minlie Huang\({}^{1,}\)

\({}^{1}\)Tsinghua University \({}^{2}\)Zhipu AI \({}^{3}\)University of Electronic Science and Technology of China

\({}^{4}\)China University of Geosciences \({}^{5}\)Central China Normal University

wbs23@mails.tsinghua.edu.cn, aihuang@tsinghua.edu.cn

###### Abstract

Instruction following is one of the fundamental capabilities of large language models (LLMs). As the ability of LLMs is constantly improving, they have been increasingly applied to deal with complex human instructions in real-world scenarios. Therefore, how to evaluate the ability of complex instruction-following of LLMs has become a critical research problem. Existing benchmarks mainly focus on modeling different types of constraints in human instructions while neglecting the composition of different constraints, which is an indispensable constituent in complex instructions. To this end, we propose ComplexBench, a benchmark for comprehensively evaluating the ability of LLMs to follow complex instructions composed of multiple constraints. We propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types, and manually collect a high-quality dataset accordingly. To make the evaluation reliable, we augment LLM-based evaluators with rules to effectively verify whether generated texts can satisfy each constraint and composition. Furthermore, we obtain the final evaluation score based on the dependency structure determined by different composition types. ComplexBench identifies significant deficiencies in existing LLMs when dealing with complex instructions with multiple constraints composition1.

Footnote 1: Our dataset and codes are available at https://github.com/thu-coai/ComplexBench.

## 1 Introduction

Large language models (LLMs) have proven their remarkable abilities in addressing various NLP tasks [1]. Among these, instruction following is one of the most crucial requirements for LLM applications as it determines how well LLMs align with human intents [2]. In real-world use of LLMs, almost all the tasks are formulated as instruction following, where human instructions impose different constraints on the model output to specify the requirement of specific tasks [3].

Hence, how to accurately measure the quality of instruction following has become an essential problem. While early works focused on simple and direct human instructions in traditional NLP tasks, such as translation and text classification [4; 5; 6], recent works have resorted to complex instructions consisting of multiple constraints [3; 7; 8; 9], which are important constituents of LLM's real-world use including role-play [10] and LLMs as agents [11]. These complex instructionfollowing benchmarks aim to measure whether the generated text can meet every constraint in the input instruction.

However, we argue that existing complex instruction-following benchmarks neglect to model the composition of constraints, causing insufficient evaluation of the LLMs' ability to follow complex instructions. Since composition is a natural phenomenon in language use and a long-standing research problem in the NLP community [12, 13, 14, 15], it is a necessary ingredient in complex instructions to specify structural combinations of different constraints. In addition, the ignorance of composition leads to issues in both dataset construction and evaluation method design. On dataset construction, existing benchmarks are currently limited to simple composition types such as _And_ which represents coordination between different constraints [3]. As shown in Figure 1, in addition to _And_, complex instructions can also include more intricate composition types of constraints, such as _Chain_ (for sequential completion of constraints) and _Selection_ (for conditional selection of constraints). Regarding evaluation method design, incorporating more complex composition types brings challenges in both constraint / composition evaluation and final score aggregation. First, complex instructions with structural combinations of constraints make it hard to evaluate each constraint / composition type independently with LLMs / rules due to their coupling. Then, simple aggregation methods for each constraint result, such as direct averaging, which is commonly adopted by existing benchmarks neglect the dependency among constraints brought by composition, causing potential biases in evaluation results.

In this paper, we propose ComplexBench, a novel benchmark to comprehensively evaluate the ability of LLMs to follow complex instructions. ComplexBench is manually constructed based on a hierarchical taxonomy of complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types, which provide a broad perspective to assess the performance of LLMs in dealing with complex instructions. To precisely measure whether LLMs' generated texts satisfy all these constraints and composition types, we design a yes / no question to verify each constraint and composition type respectively, inspired by the existing works on QA-based evaluation [16, 17, 7]. Then, we propose a new evaluation method for complex instruction-following called rule-augmented LLM-based evaluation. This method first extracts evaluation segments from generated responses for each yes / no question and then solves each question with LLMs or rules. Finally, the answers to each question are aggregated via the dependency structure among these questions, which is built based on the composition types. ComplexBench accompanied by our proposed evaluation method is expected to systematically reveal the deficiencies of existing LLMs on complex instructions and provide insights on the improvement of LLMs when dealing with various constraints and compositions. Our main contributions are as follows:

* We propose a comprehensive hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types. We manually collect

Figure 1: An example instruction of ComplexBench. All constraint dimensions contained in the instruction are marked with underlines and colors, which are categorized into three constraint types in our taxonomy: Format, Semantic, and Utility. Below is the composition structure of the instruction, where these constraint dimensions are combined through three composition types: _And_, _Chain_, and _Selection_.

a high-quality benchmark dataset for complex-instruction following, covering all types of constraints and compositions in our taxonomy.
* We accompany the benchmark with a new automated evaluation method to accurately evaluate the ability of LLMs to follow complex instructions, which integrates the advantages of LLM-based and rule-based methods to verify each constraint and composition type and aggregates the final score via the dependency structure brought by composition types.
* We conduct experiments on the proposed benchmark for a wide range of established LLMs, systematically revealing their deficiencies on various constraints and compositions.

## 2 Related Work

**Evaluation of Instruction-Following.** Instruction following remains one of the most important factors determining the practicality of LLMs [21]. Therefore, numerous studies have attempted to evaluate it from various aspects. Earlier works used to focus on simple human instructions formed with mostly a single constraint, such as semantic [5; 4; 6] and format [19; 22; 23] constraints. Since LLMs have been gradually applied to address complex real-world tasks, users have to form complex instructions, which naturally call for the evaluation of the LLMs' ability in complex instruction following [3; 7]. WizardLM [18] employs two strategies, _In-Breadth Evolving_ and _In-depth Evolving_, to form complex instructions from simple ones. CELLO [8] defines complex instructions from task descriptions and input text, and evaluates LLMs with real-world scenarios data. Unlike our work, which includes subjective and objective constraints and combines LLM-based and rule-based evaluations, CELLO focuses only on objective, rule-verifiable constraints and uses rule-based scoring functions for evaluation. Nonetheless, we argue that these benchmarks neglect to model the composition of constraints, which is an important character in complex instructions and brings non-negligible structural complexity that is crucial to assessing LLMs' abilities.

**Compositionality in NLP.** Previous studies have explored compositionality across traditional NLP tasks, including semantic parsing [24; 25; 26], machine translation [26; 27], style transfer [28], and data-to-text generation [29]. However, in the task of instruction-following, how the LLMs deal with the compositionality in instructions is still under-explored. CompMCTG [30] investigates the compositionality of multiple control attributes for LLMs, which is a topic neighboring ours. Nevertheless, our work studies more complex composition types beyond simple coordination between different constraints, such as _Chain_ and _Selection_ and their nested structures, which form the basis of many real-world complex tasks for LLMs.

## 3 ComplexBench Framework

### Overview

To comprehensively evaluate the ability of LLMs to follow complex instructions, we propose a hierarchical taxonomy to define constraints and composition types. For constraints, we extend common constraints in controlled text generation tasks to the instruction-following tasks and consider a two-level structure including coarse-grained types and fine-grained dimensions (Section 3.2). As for compositions that indicate structural combinations of constraints, we consider the characteristics of instruction-following tasks to define the composition types according to existing works on compositionality in traditional NLP tasks (Section 3.3).

\begin{table}
\begin{tabular}{l c c c c c c c c} \hline \hline Benchmark & Data Size & \begin{tabular}{c} Constraint \\ Taxonomy \\ \end{tabular} & \begin{tabular}{c} \\ _And_ \\ \end{tabular} & \begin{tabular}{c} _China_ \\ \end{tabular} & \begin{tabular}{c} _Selection_ \\ \end{tabular} & \begin{tabular}{c} _Nested_ \\ \end{tabular} & \begin{tabular}{c} LLM-based \\ \end{tabular} & \begin{tabular}{c} Rule-based \\ \end{tabular} & 
\begin{tabular}{c} Aggregation Function \\ \end{tabular} \\ \hline WizardLM Testset [18] & 218 & - & ✓ & - & - & - & ✓ & - & - \\ CELLO [8] & 523 & 4 & ✓ & ✓ & - & - & ✓ & Average \\ FollowBench [3] & 820 & 5 & ✓ & - & - & - & ✓ & Average \\ IFEval [19] & 541 & 25 & ✓ & - & - & - & ✓ & Average \\ InfBench [7] & 500 & 5 & ✓ & - & - & - & ✓ & Average \\ Col Testset [20] & 1,068 & - & ✓ & ✓ & - & - & ✓ & - \\ \hline
**ContainsBrecon (ours)** & **1,150** & **4-19** & ✓ & ✓ & ✓ & ✓ & ✓ & Dependency-based Aggregation \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparisons between ComplexBench and other benchmarks, illustrating the features including dataset sizes, constraint taxonomies, composition types, and evaluation methods. - in Aggregation Function means there is no step to evaluate each constraint and aggregate the final score.

### Constraint

Following existing works on controlled text generation and instruction following [31, 32, 33, 34, 19, 10], we propose a two-level structure for constraints including 4 constraint types (i.e., Lexical, Format, Semantic, and Utility) and 19 specific constraint dimensions which are further divided from the above types. The distribution of these constraint types and dimensions within ComplexBench is shown in Figure 2. We present the definitions of constraint types in the following and describe the details of the constraint dimensions in Appendix D.

**Lexical Constraint** requires to output specific keywords or phrases or precisely generate texts that are related to specific keywords mentioned in the instructions [35, 36, 34].

**Format Constraint** specifies the requirements on the output structure (such as JSON, Markdown, and bullet points), length, and patterns of the output, where the patterns include punctuation, content at the beginning or end, and the output templates. Format constraints require LLMs to possess a precise understanding and planning of the output content, which remain challenging for current LLMs [19, 23].

**Semantic Constraint** specifies the topic [37], language style [32], personality [10], and sentiment [38] of the output, which are common constraints in the existing works on controlled text generation.

**Utility Constraint** measures the language, helpfulness, supportiveness, consistency, and factuality of generated texts, which are holistic properties. Among these, helpfulness indicates whether the generated text can complete the basic task included in the instruction (such as _Please introduce the following painting_. in Figure 1) regardless of satisfaction of other constraints, while supportiveness means whether the generated text is faithful to the instruction.

### Composition

As shown in Figure 3, we propose 4 composition types that indicate typical structural combinations of constraints.

Figure 3: Composition types in ComplexBench. Each node is a part of an instruction. The purple node may contain other composition types, while the blue node does not. In addition to 4 basic types, the last row also shows a nested selection type.

Figure 2: Constraint distribution of ComplexBench. The Utility constraints helpfulness and factuality possess a high proportion due to their prevalence in various instructions, which are basic requirements for high-quality outputs.

**Single.** The output is required to satisfy a single constraint, with no composition involved.

**And.** The output needs to satisfy multiple constraints simultaneously. This simple composition type commonly appears in most of the existing benchmarks on complex instruction-following [3, 19, 7].

**Chain.** The output is required to complete multiple tasks in the instruction sequentially, each of which may contain several constraints. Formally, _Chain_ contains \(n\) tasks \(\{T_{1},T_{2},\ldots,T_{n}\}\), which need to be completed sequentially. The output of \(T_{k+1}\) may depends on that of \(T_{k}\) (\(k=1,2,\cdots,n-1\)).

**Selection.** The output is required to select different branches according to certain conditions, fulfilling the constraints of the corresponding branch. Formally, _Selection_ contains \(m\) branches \(\{B_{1},B_{2},\ldots,B_{m}\}\), each of which is a task with expected outputs \(Y_{1},Y_{2},\ldots,Y_{m}\) respectively. We denote a selection function as \(S\) with a range \(\{1,2,\cdots,m\}\), taking the selection condition _cond_ as input. Finally, the expected output of the instruction is \(Y_{S(\textit{cond})}\).

It's worth noting that the above composition types can be nested to construct more complex structures. Each task in _Chain_ and each branch in _Selection_ may also contain other composition types. As shown in the last row of Figure 3, a branch of _Selection_ can also contain _Selection_, thus forming a nested selection composition type.

To verify the necessity and comprehensiveness of the composition types considered in ComplexBench, we analyze the distribution of composition types in real-world scenarios. We collect instructions with high demand and representativeness from an online LLM-based chat service platform that serves more than a million users daily including general and professional instructions. General instructions refer to the instructions used by individual users in routine scenarios, while professional instructions refer to those used by enterprise-level users in business and research scenarios. For each category of instructions, we randomly sample 300 instructions and manually count the number of instructions containing each composition type. We found that the taxonomy of ComplexBench fully covers present composition types. As shown in Figure 4, although the composition types of general instructions are relatively simple and have already been covered by current benchmarks, professional instructions include more complex composition types, such as _Selection_ and nested structures of multiple composition types, which have rarely been considered by current benchmarks. As LLMs have been gradually applied to deal with complex instructions in professional scenarios, it is necessary to evaluate their ability to follow instructions with multiple constraints composition.

## 4 ComplexBench Construction

### Data Collection

We manually construct ComplexBench based on the taxonomy described in Section 3. The detailed construction pipeline consists of four steps, i.e., **Reference Instructions Collection**, **Task Allocation**, **Data Annotation and Validation**, and **Selection Branch Expansion**. We initially used our proposed method to construct Chinese data, while also providing an English version of ComplexBench. More details are in Appendix F, G, and H.

**Reference Instruction Collection.** Considering the difficulty of constructing complex instructions from scratch, annotators are required to create new complex instructions based on provided reference instructions. We collect reference instructions from real-world application scenarios and open-source instruction following benchmarks [19, 3, 7]. We conduct strict desensitization of privacy and carefully filter these instructions using category and quality classifiers.

**Task Allocation.** To ensure comprehensive coverage of each constraint and composition type, we partition the entire dataset construction into multiple annotation tasks. Each annotation task has different requirements for the minimal number of constraint dimensions in each constraint type and composition type. Annotators are required to modify reference instructions to meet the requirements of corresponding tasks. To alleviate the annotation cost, especially when the constraint

Figure 4: Composition type distribution of general and professional instructions.

dimensions in the reference instructions and task requirements are different, we leverage GPT-4 [39] to automatically acquire the constraint dimensions in reference instructions and assign them to corresponding annotation tasks according to minimal editing distance.

**Data Annotation and Validation.** Given reference instructions and corresponding annotation task requirements, annotators are expected to construct new complex instructions and annotate the constraint dimensions and composition types. After the data annotation, newly constructed instructions are cross-validated by other annotators. The process of validation continues until constructed instructions meet the following criteria: (1) Clarity & Reasonableness: The instruction should be easy to understand, unambiguous, and realistic, with at least one reasonable answer. (2) Validity of Constraints: Every constraint within the instruction should substantially influence the output. (3) Complexity & Difficulty: The instruction should be challenging for most LLMs and be capable of distinguishing the complex instruction-following abilities of different LLMs.

**Selection Branch Expansion.** When evaluating the ability of LLMs to follow instructions containing _Selection_, the predisposition toward random selection by LLMs may bring potential bias because most instructions cover only one correct selection branch. But the probability of the correct branch appearing at each position in _Selection_ of data constructed by annotators is unequal 2. To address this issue, in the final stage of instruction construction, we manually modify the selection condition based on the selection function to construct multiple instructions that cover different correct branches, ensuring an equal probability of the correct branch appearing at each position.

Footnote 2: For instance, after manual inspection we find that in all the selection compositions with two branches, annotators have about a 70% probability of selecting the first branch as the correct one.

### Evaluation Protocol

To conduct a detailed evaluation of how well each constraint and composition type is satisfied, we draw inspiration from previous works that transform text evaluation into multiple question-answering tasks [16; 17; 7]. For each constraint and composition type specified in an instruction, we manually craft a scoring question that can be succinctly answered with either "yes" or "no."

Current mainstream evaluation methods contain LLM-based [18; 3; 7] and rule-based methods [8; 19; 20]. In our preliminary experiments, we find that LLM-based methods are effective at answering open-ended scoring questions, but they demonstrate a significant deficiency in those involving numerical computation, counting, and other objective rule-defined areas, such as keyword inclusion and text length. Simultaneously, rule-based methods perform well in rule-defined areas but are powerless against open-ended scoring problems. To address their limitations, we design a Rule-Augmented LLM-based (RAL) evaluation method to equip LLM evaluators with rules to answer

Figure 5: An exemplar evaluation process of ComplexBench. Given an instruction and its scoring questions, ComplexBench integrates the rule and LLM evaluator to verify each of them and aggregates the final score based on the dependency structure of composition types in the instruction.

scoring questions in both rule-defined and open-ended areas. For the instruction \(I\), the generated response to be evaluated \(o\), and the scoring problem \(q\), if \(q\) is verifiable by rules, we first use the LLM to automatically extract segments \(e\) of \(o\), which is related to scoring question \(q\). Subsequently, we use the rule \(R_{q}\) written for \(q\) to obtain the evaluation result \(r_{q}\in\{0,1\}\), that is:

\[e=\mathcal{M}_{ex}(I,q,o)\] (1)

\[r_{q}=R_{q}(e)\] (2)

where \(\mathcal{M}_{ex}\) indicates the LLM with the prompt used for extraction. Otherwise, if \(q\) cannot be verified by rules, we directly use the LLM to measure the quality of \(o\):

\[r_{q}=\mathcal{M}_{eva}(I,q,o)\] (3)

where \(\mathcal{M}_{eva}\) denotes the LLM with the prompt used for evaluation. For composition types, considering that their satisfaction is a prerequisite for satisfying some constraints, we model the dependencies of its scoring questions. Specifically, for _Chain_, all the scoring questions of the subsequent task depend on the answers to those of the preceding task. And for _Selection_, all the scoring questions of the selection branch depend on whether the correct selection branch is selected. If a scoring question is judged as "no", all the scoring questions depending on it will also be directly judged as "no". Formally, we denote the set of scoring questions that \(q\) depends on as \(Dep(q)\). After all scoring questions have been independently verified, _Dependency Aggregation_ will be performed, and the result of \(q\) will be calculated as follows:

\[r_{q}^{{}^{\prime}}=r_{q}\bigwedge_{p\in Dep(q)}r_{p}\] (4)

Finally, following InfoBench [7], we calculate Decomposed Requirements Following Ratio (DRFR) as the final score during _Score Aggregation_. Considering a benchmark dataset has \(N\) instructions, the instruction \(i\) has \(m_{i}\) scoring questions, and the result of the \(j\)-th scoring question is \(r_{ij}^{{}^{\prime}}\), the metric is calculated as: \(DRFR=\sum_{i,j}r_{ij}^{{}^{\prime}}/\sum_{i}m_{i}\). Figure 5 shows a framework of our evaluation protocol.

### Benchmark Statistics

ComplexBench contains 1,150 instructions and 5,306 scoring questions, as shown in Table 2. Nesting depth means the maximum depth of composition types. In addition to three basic composition types including _And_, _Chain_, and _Selection_, we adopt a separate category whose instructions simultaneously contain _Chain_ and _Selection_, aiming to use these two challenging types to explore the boundary of LLMs' ability in complex instruction-following3. We also present the task distribution of ComplexBench in Appendix C.

Footnote 3: Since _And_ commonly appears in various instructions, we simply categorize instructions containing both _Chain_ / _Selection_ and _And_ together with those only containing _Chain_ / _Selection_ into one category.

## 5 Experiments

### Agreement Evaluation

To measure the agreement between our evaluation method and manual evaluation, we randomly sample 200 instructions from ComplexBench to construct a meta-evaluation dataset. Five LLMs are involved in this evaluation as generation models. We employ GPT-4-1106 [39] as our primary judge and adopt two metrics to confirm the reliability of our method: (1) **Overall Pairwise Agreement**: Given an instruction, two model responses (denoted as A and B), the human annotators are instructed to compare the quality and choose from 3 options, namely A better than B, tie, B better than A. Subsequently, the automatic evaluation scores for two model responses are converted into pairwise

\begin{table}
\begin{tabular}{c c|c c c c} \hline \hline \multirow{2}{*}{Category} & \multirow{2}{*}{
\begin{tabular}{c} Nesting \\ Depth \\ \end{tabular} } & \multirow{2}{*}{**\#Inst.**} & \multirow{2}{*}{**\#Len.**} & \multirow{2}{*}{**\#Ques.**} & \multirow{2}{*}{**\#Con.**} \\ \cline{1-1} \cline{5-6}  & & & & & \\ \hline And & 1 & 475 & 279.39 & 4.09 & 4.14 \\ \hline Chain & 1 & 70 & 352.11 & 4.83 & 4.94 \\  & 2 & 170 & 486.84 & 6.24 & 6.32 \\ \hline Selection & 1 & 80 & 753.15 & 2.91 & 2.06 \\  & 2 & 224 & 664.13 & 4.40 & 3.09 \\  & 2 & 34 & 1409.93 & 5.76 & 3.78 \\ \hline Selection \& 2 & 30 & 440.37 & 4.37 & 3.63 \\ Chain & \(\geq\) 3 & 55 & 398.82 & 6.18 & 5.27 \\ \hline Overall & - & 1150 & 477.51 & 4.61 & 4.19 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Statistics of ComplexBench including the number of instructions (**#Inst.**), the average number of characters (**#Len.**), scoring questions (**#Ques.**), and constraints (**#Con.**) per instruction.

comparisons to measure agreement with human annotators. (2) **Question-level Agreement**: Given an instruction and a model response, human annotators are instructed to judge whether each scoring question is satisfied respectively. Then, we calculate the agreement between automatic evaluation results and human-annotated ones.

For the Overall Pairwise Agreement, we sample 500 pairs from the outputs of 5 LLMs. Direct Scoring serves as a baseline, which adopts a scoring prompt [5] to assign a score to the response with a scale of 1-10. As shown in Table 3, our method can improve the agreement with manual evaluations compared to Direct Scoring with a large margin. _Dependency Aggregation_ also shows its important contribution to our method

For the Question-level Agreement, the scoring questions in the meta-evaluation dataset are categorized into two types: (1) Rule-defined, which can be verified by rules and constitutes 17% of the total, and (2) Open-ended, which is not verifiable by rules. We compare our method with Direct Scoring, which considers a response with a score above 5 to satisfy all scoring questions of an instruction. We also remove rule arguments (w/o rule) to verify its effectiveness. As shown in Table 4, RAL outperforms all the baselines and exhibits an impressive 87.82% agreement with humans at the overall level. The LLM-based evaluator (i.e., RAL w/o rule in Table 4) shows its weakness in rule-defined areas that rule arguments mainly contribute to, supporting our motivation.

### Automatic Evaluation

#### 5.2.1 Setup

We use GPT-4-1106 [39] as our judge to evaluate 15 LLMs: (1) **Closed-source LLMs**: GPT-4-1106, Claude-3-Opus [40], GLM-4 [41], ERNIE Bot-4, GPT-3.5-Turbo-1106. (2) **Open-source LLMs**: Qwen1.5-Chat [42], Llama3-Instruct [43], InternLM2-Chat [44], Baichuan2-Chat [45], Mistral-Instruct [46], InternLM2-Chat [44], ChatGLM3-Chat [47]. The sizes of these models vary from 6B to 72B. We use greedy search for reproducibility, and the maximum generation length is 8,192.

#### 5.2.2 Main Results

The main results are shown in Table 5. **Firstly**, the widely recognized powerful GPT-4 still fails to complete 20% of complex instructions, highlighting the necessity of complex instruction evaluation. **Secondly**, as the complexity of composition types within instruction increases, the performance of all LLMs significantly drops, especially on _Selection_ and _Chain_. This aligns with our motivation for constructing complex composition types. **Thirdly**, the performance of most open-source LLMs falls short compared to closed-source LLMs especially on complex composition types, indicating that open-source LLMs still have a large room for improvement in chasing the capabilities of closed-source LLMs.

To dissect the ability of LLMs to follow specific constraint and composition types, we calculate the average accuracy of scoring questions for

\begin{table}
\begin{tabular}{c|c|c} \hline \hline
**Subset** & **Evaluator** & **Agreement between human** \\ \hline \multirow{3}{*}{Rule-defined} & RAL & **95.36\%** \\  & RAL w/o rule & 82.02\% \\  & Direct Scoring & 62.02\% \\ \hline \multirow{3}{*}{Open-ended} & RAL & **86.28\%** \\  & RAL w/o rule & **86.28\%** \\  & Direct Scoring & 77.83\% \\ \hline \multirow{3}{*}{Overall} & RAL & **87.82\%** \\  & RAL w/o rule & 85.56\% \\ \cline{1-1}  & Direct Scoring & 75.18\% \\ \hline \hline \end{tabular}
\end{table}
Table 4: Question-level Agreement with human.

Figure 6: The performance of LLMs on different constraint and composition types.

\begin{table}
\begin{tabular}{c c} \hline \hline
**Evaluation Method** & **Pairwise Agreement** \\ \hline Ours & **0.614** \\ Ours w/o _Dep._ & 0.574 \\ Direct Scoring & 0.512 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Overall Pairwise Agreement with human. _Dep._ means _Dependency Aggregation_.

each type. The results are shown in Figure 6. **Firstly**, for constraints, LLMs generally perform better on Semantic and Utility constraints but struggle with the Format and Lexical constraints that have explicit evaluation standards. **Secondly**, for compositions, _Chain_ presents severe challenges while _Selection_ come second. We speculate that the main difficulty in _Selection_ lies not only in choosing the correct branch but in executing it without interference from irrelevant branches. More results and analyses are in Appendix I.

#### 5.2.3 Analysis

**Decomposition of instructions with composition types.** To explore whether decomposing complex instructions and executing them through multi-round interactions can improve the performance of LLMs, we manually decompose ComplexBench instructions based on composition types (e.g., _Chain_ into sequential tasks, _Selection_ into selection and execution branches, while _And_ remains intact) and compare the performance of LLMs between executing decomposed instructions step-by-step and original instructions in one step. The scoring questions of original instructions are split into corresponding decomposed ones with the same dependencies to ensure a fair comparison.

Table 6 shows that GPT-3.5-Turbo-1106 generally performs worse in decomposed instructions, especially as the complexity of composition types within instructions increases. We conjecture that this is due to cumulative errors in multi-round interactions, highlighting that our benchmark is challenging and cannot be simply solved via instruction decomposition.

**The Coherent Test for Selection.** To comprehensively measure the performance of LLMs on different conditions of _Selection_, we merge the instructions with the same branches and selection functions but different conditions into the same task group. For example, the instruction about the Mona Lisa shown in Table 1 and another instruction where everything else remains the same except the final condition "Painting: Mona Lisa" is changed to "Painting: Galloping horse" are merged into the same task group. The two instructions need to execute two different selection branches. We calculate the proportion of instructions with all scoring questions correct (_Original Test_) and group tasks with all scoring questions correct (_Coherent Test_). Formally, considering that there are \(N\) instructions containing Selection, they are divided into \(K\) task groups. Each instruction \(i\) has \(m_{i}\) scoring ques

\begin{table}
\begin{tabular}{c c|c c c} \hline \hline \multirow{2}{*}{**Category**} & \multirow{2}{*}{
\begin{tabular}{c} **Nesting** \\ **Depth** \\ \end{tabular} } & \multirow{2}{*}{**Origin**} & \multirow{2}{*}{**Decomposition**} & \multirow{2}{*}{\(\Delta\)} \\ \hline
**And** & 1 & 0.845 & 0.845 & 0.000 \\ \hline \multirow{2}{*}{**Chain**} & 1 & 0.686 & 0.655 & -0.031 \\  & 2 & 0.630 & 0.583 & **-0.047** \\ \hline \multirow{2}{*}{**Selection**} & 1 & 0.661 & 0.631 & -0.030 \\  & 2 & 0.561 & 0.520 & -0.041 \\  & \(\geq 3\) & 0.475 & 0.411 & **-0.064** \\ \hline \multirow{2}{*}{**Selection \&**} & 2 & 0.565 & 0.504 & -0.061 \\  & \(\geq 3\) & 0.482 & 0.415 & **-0.067** \\ \hline
**Overall** & - & 0.682 & 0.652 & -0.030 \\ \hline \hline \end{tabular}
\end{table}
Table 6: The performance of GPT-3.5-Turbo-1106 on original and decomposed instructions.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c} \hline \hline
**Category** & **And** & \multicolumn{3}{c|}{**Chain**} & \multicolumn{3}{c|}{**Selection**} & \multicolumn{3}{c|}{**Selection \& Chain**} & \multicolumn{1}{c}{**All**} \\ \hline
**Nesting Depth** & 1 & 1 & 2 & Avg. & 1 & 2 & \(\geq 3\) & Avg. & 2 & \(\geq 3\) & Avg. & Avg. \\ \hline \multicolumn{11}{c}{_Closed-Source Language Models_} \\ \hline GPT-4-1106 & 0.881 & **0.787** & 0.759 & 0.766 & **0.815** & **0.772** & **0.694** & **0.765** & 0.802 & 0.626 & 0.675 & **0.800** \\ Claude-3-Opus & **0.886** & 0.784 & **0.779** & **0.780** & 0.764 & 0.749 & 0.592 & 0.724 & 0.695 & 0.576 & 0.609 & 0.788 \\ GLM-4 & 0.868 & 0.763 & 0.739 & 0.745 & 0.768 & 0.739 & 0.626 & 0.724 & **0.809** & **0.647** & **0.692** & 0.779 \\ ERNIEBot-4 & 0.866 & 0.749 & 0.735 & 0.738 & 0.725 & 0.696 & 0.649 & 0.692 & 0.756 & 0.600 & 0.643 & 0.764 \\ GPT-3.5-Turbo-1106 & 0.845 & 0.686 & 0.630 & 0.644 & 0.661 & 0.561 & 0.475 & 0.561 & 0.565 & 0.482 & 0.505 & 0.682 \\ \hline \multicolumn{11}{c}{_Open-Source Language Models_} \\ \hline Qwen1.5-72B-Chat & 0.873 & 0.749 & 0.730 & 0.735 & 0.751 & 0.698 & 0.521 & 0.675 & 0.611 & 0.521 & 0.546 & 0.752 \\ Llama-3-70B-Instruct & 0.858 & 0.769 & 0.722 & 0.733 & 0.747 & 0.704 & 0.675 & 0.706 & 0.573 & 0.571 & 0.571 & 0.757 \\ \hline InterLM2-20B-Chat & 0.796 & 0.666 & 0.648 & 0.652 & 0.648 & 0.599 & 0.543 & 0.597 & 0.611 & 0.488 & 0.522 & 0.678 \\ Qwen1.5-14B-Chat & 0.817 & 0.657 & 0.636 & 0.641 & 0.622 & 0.621 & 0.536 & 0.606 & 0.550 & 0.435 & 0.467 & 0.680 \\ Baichuan2-13B-Chat & 0.760 & 0.583 & 0.517 & 0.533 & 0.571 & 0.479 & 0.404 & 0.480 & 0.443 & 0.409 & 0.418 & 0.591 \\ \hline Llama-38B-Instruct & 0.778 & 0.669 & 0.568 & 0.592 & 0.597 & 0.552 & 0.483 & 0.546 & 0.626 & 0.429 & 0.484 & 0.638 \\ Mistral-P-Instruct & 0.737 & 0.574 & 0.556 & 0.560 & 0.554 & 0.493 & 0.411 & 0.488 & 0.534 & 0.374 & 0.418 & 0.592 \\ Qwen1.5-P8B-Chat & 0.802 & 0.598 & 0.611 & 0.608 & 0.519 & 0.564 & 0.570 & 0.558 & 0.634 & 0.491 & 0.531 & 0.658 \\ InterLM2-7B-Chat & 0.755 & 0.633 & 0.598 & 0.607 & 0.532 & 0.568 & 0.525 & 0.555 & 0.432 & 0.465 & 0.634 \\ ChatGLM3-6B-Chat & 0.701 & 0.556 & 0.490 & 0.506 & 0.455 & 0.430 & 0.411 & 0.431 & 0.573 & 0.312 & 0.384 & 0.546 \\ \hline \hline \end{tabular}
\end{table}
Table 5: DRFR of LLMs computed by our proposed RAL method. The highest performance among open-source models is underlined, while the highest performance overall is **bold**.

of the \(j\)-th scoring question is \(r^{{}^{\prime}}_{ij}\) (the same definition as Section 4.2). The results of _Original Test_ will be calculated as \(\frac{1}{N}\sum_{i=1}^{N}\bigwedge_{j=1}^{m}r^{{}^{\prime}}_{ij}\), and the results of _Coherent Test_ will be calculated as \(\frac{1}{k}\sum_{k=1}^{K}\bigwedge_{i\in Group(k)}(\bigwedge_{j=1}^{m}r^{{}^{ \prime}}_{ij})\). Instructions containing Selection are categorized as either single-layer or multi-layer nested, respectively. As shown in Figure 7, for single-layer Selection instructions, LLMs with stronger instruction-following abilities show a smaller performance drop in the coherent test, which better understands the selection structure. For more complex multi-layer nested Selection instructions, even the state-of-the-art LLM, GPT-4, achieves only 14.9% accuracy in the coherent test. At the same time, smaller-scale LLMs can't perfectly follow any group of instructions. The results highlight current LLMs' weaknesses in following multi-layer tree-structured instructions.

Comparisons between Other Capabilities.We compare the performance of representative LLMs across 3 prominent LLM evaluation benchmarks in addition to ComplexBench: IFEval[19], HumanEval[48], and MATH[49], focusing on instruction-following, coding, and mathematical ability, respectively. As shown in Table 7, although the performance of various LLMs on ComplexBench is well correlated with their performance on other benchmarks, the rankings of LLMs on ComplexBench do not entirely correspond with those on the other three benchmarks. For instance, ChatGLM3-6B-Chat demonstrates outstanding coding and mathematical abilities among LLMs of similar scale, but it notably struggles with complex instruction-following. On the other hand, while Llama-3-70B-Instruct surpasses GPT-4-1106 on IFEval and ranks first, it still shows a performance gap with GPT-4-1106 on ComplexBench. This discrepancy is primarily in the areas of instructions with complex constraints composition, which are not covered by IFEval, indicating that ComplexBench can provide a complementary perspective for LLM evaluation.

## 6 Conclusion

In this work, we propose ComplexBench, a systematical benchmark for complex instruction-following. We first propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types. Furthermore, we manually collect a high-quality dataset accordingly. Along with the dataset, we propose a structure-aware automatic evaluation method for complex instruction-following with constraints composition and further enhance the evaluation accuracy by equipping LLM-based evaluators with rules. Finally, we conduct extensive experiments to evaluate the performance of current representative LLMs on complex instruction-following and uncover their significant deficiencies in dealing with complex composition types. In summary, we posit that ComplexBench can serve as a valuable tool for benchmarking the complex instruction-following ability of LLMs, providing a complementary perspective for LLM evaluation and useful insights for further work to improve this ability of LLMs.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Model** & **ComplexBench** & **IFEval** & **HumanEval** & **MATH** \\ \hline GPT-4-1106 & 0.800 & 75.4 & 84.6 & 64.3 \\ GLM-4 & 0.779 & 66.7 & 72.0 & 47.9 \\ Queni-5.72B-Chat & 0.752 & 55.8 & 71.3 & 42.5 \\ Llama-3-70B-Instruct & 0.757 & 78.9 & 81.7 & 50.4 \\ Llama-3-8B-Instruct & 0.638 & 68.6 & 62.2 & 30.0 \\ Mistral-7B-Instruct & 0.592 & 40.5 & 30.5 & 13.1 \\ Queni-5.7B-Chat & 0.658 & 38.8 & 46.3 & 23.2 \\ InternlM2-7B-Chat & 0.634 & 46.5 & 59.8 & 23.0 \\ ChatGLM3-6B-Chat & 0.546 & 28.1 & 64.0 & 25.7 \\ \hline Correlation with ComplexBench & - & 0.814 & 0.715 & 0.895 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Model comparison on different abilities. The last row shows the Pearson correlation between the performance of LLMs in ComplexBench and other benchmarks.

Figure 7: The performance variance under the coherent test for _Selection_. The left side represents single-layer _Selection_ instructions, and the right side corresponds to multi-layer _Selection_ instructions.

## Acknowledgements

This work was supported by the National Science Foundation for Distinguished Young Scholars (No. 62125604), the NSFC projects (No. 62306160), and the Tsinghua University Initiative Scientific Research Program. We would also like to thank Zhipu AI for sponsoring the computation resources and annotation costs used in this work.

## References

* [1] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models. _arXiv preprint arXiv:2303.18223_, 2023.
* [2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.
* [3] Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. Followbench: A multi-level fine-grained constraints following benchmark for large language models. _arXiv preprint arXiv:2310.20410_, 2023.
* [4] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsumori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 2023.
* [5] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In _Advances in Neural Information Processing Systems_, volume 36, pages 46595-46623, 2023.
* [6] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. Alignbench: Benchmarking chinese alignment of large language models. _arXiv preprint arXiv:2311.18743_, 2023.
* [7] Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in large language models. _arXiv preprint arXiv:2401.03601_, 2024.
* [8] Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Kunzhe Zhou, Jiaqing Liang, and Yanghua Xiao. Can large language models understand real-world complex instructions? In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 18188-18196, 2024.
* [9] Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, and Zhendong Mao. Benchmarking large language models on controllable generation under diversified instructions. _arXiv preprint arXiv:2401.00690_, 2024.
* [10] Jinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, et al. CharactergIm: Customizing chinese conversational ai characters with large language models. _arXiv preprint arXiv:2311.16832_, 2023.
* [11] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. Agentbench: Evaluating llms as agents. _arXiv preprint arXiv:2308.03688_, 2023.
* [12] Laura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. Abstract meaning representation for sembanking. In _Proceedings of the 7th linguistic annotation workshop and interoperability with discourse_, pages 178-186, 2013.
* [13] Ioannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and Luke Zettlemoyer. Neural amr: Sequence-to-sequence models for parsing and generation. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 146-157, 2017.
* [14] Jacob Andreas. Measuring compositionality in representation learning. In _7th International Conference on Learning Representations_, 2019.
* [15] Sanket Vaibhav Mehta, Jinfeng Rao, Yi Tay, Mihir Kale, Ankur Parikh, and Emma Strubell. Improving compositional generalization with self-training for data-to-text generation. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 4205-4219, 2022.

* [16] Daniel Deutsch, Tania Bedrax-Weiss, and Dan Roth. Towards question-answering as an automatic metric for evaluating the content quality of a summary. _Transactions of the Association for Computational Linguistics_, 9:774-789, 2021.
* [17] Pei Ke, Fei Huang, Fei Mi, Yasheng Wang, Qun Liu, Xiaoyan Zhu, and Minlie Huang. DecompEval: Evaluating generated texts as unsupervised decomposed question answering. In _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 9676-9691, 2023.
* [18] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. _arXiv preprint arXiv:2304.12244_, 2023.
* [19] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. _arXiv preprint arXiv:2311.07911_, 2023.
* [20] Shirley Anugrah Hayati, Taehee Jung, Tristan Bodding-Long, Sudipta Kar, Abhinav Sethy, Joo-Kyung Kim, and Dongyeop Kang. Chain-of-instructions: Compositional instruction tuning on large language models. _arXiv preprint arXiv:2402.11532_, 2024.
* [21] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: a survey and guideline for evaluating large language models' alignment. _arXiv preprint arXiv:2308.05374_, 2023.
* [22] Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, and Caiming Xiong. Fofo: A benchmark to evaluate lms' format-following capability. _arXiv preprint arXiv:2402.18667_, 2024.
* [23] Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, and Mark Gerstein. Struc-bench: Are large language models good at generating complex structured tabular data? In _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 2: Short Papers)_, pages 12-34, 2024.
* [24] Najoung Kim and Tal Linzen. Cogs: A compositional generalization challenge based on semantic interpretation. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 9087-9105, 2020.
* [25] Jonathan Herzig and Jonathan Berant. Span-based semantic parsing for compositional generalization. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 908-921, 2021.
* [26] Yafu Li, Yongjing Yin, Yulong Chen, and Yue Zhang. On compositional generalization of neural machine translation. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021_, pages 4767-4780, 2021.
* [27] Hao Zheng and Mirella Lapata. Disentangled sequence to sequence learning for compositional generalization. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 4256-4268, 2022.
* [28] Yiwei Lyu, Paul Pu Liang, Hai Pham, Eduard Hovy, Barnabas Poczos, Ruslan Salakhutdinov, and Louis-Philippe Morency. StylePTB: A compositional benchmark for fine-grained controllable text style transfer. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2116-2138, June 2021.
* [29] Xinnuo Xu, Ivan Titov, and Mirella Lapata. Compositional generalization for data-to-text generation. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 9299-9317, 2023.
* [30] Tianqi Zhong, Zhaoyi Li, Quan Wang, Linqi Song, Ying Wei, Defu Lian, and Zhendong Mao. Benchmarking and improving compositional generalization of multi-aspect controllable text generation. _arXiv preprint arXiv:2404.04232_, 2024.
* [31] Xianda Zhou and William Yang Wang. MojiTalk: Generating emotional responses at scale. In _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1128-1137, 2018.

* [32] Sudha Rao and Joel Tetreault. Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 129-140, 2018.
* [33] Kalpesh Krishna, John Wieting, and Mohit Iyyer. Reformulating unsupervised style transfer as paraphrase generation. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 737-762, 2020.
* [34] Cristina Garbacea and Qiaozhu Mei. Why is constrained neural language generation particularly challenging? _arXiv preprint arXiv:2206.05395_, 2022.
* [35] Lili Mou, Yiping Song, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin. Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation. In _COLING 2016, 26th International Conference on Computational Linguistics_, pages 3349-3358, 2016.
* [36] Yizhe Zhang, Guoyin Wang, Chunyuan Li, Zhe Gan, Chris Brockett, and Bill Dolan. POINTER: constrained progressive text generation via insertion-based generative pre-training. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing_, pages 8649-8670, 2020.
* [37] Junbo Jake Zhao, Yoon Kim, Kelly Zhang, Alexander M. Rush, and Yann LeCun. Adversarially regularized autoencoders. In _Proceedings of the 35th International Conference on Machine Learning_, pages 5897-5906, 2018.
* [38] Hao Zhou, Minlie Huang, Tianyang Zhang, Xiaoyan Zhu, and Bing Liu. Emotional chatting machine: Emotional conversation generation with internal and external memory. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 32, 2018.
* [39] OpenAI. GPT-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [40] Anthropic. Introducing the next generation of claude, 2024. URL https://www.anthropic.com/news/claude-3-family.
* [41] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. GLM-130B: an open bilingual pre-trained model. In _The Eleventh International Conference on Learning Representations, ICLR 2023_, 2023.
* [42] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023.
* [43] AI@Meta. Llama 3 model card. 2024. URL https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md.
* [44] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. Intermlm2 technical report. _arXiv preprint arXiv:2403.17297_, 2024.
* [45] Baichuan-Inc. Baichuan 2. Online, August 1 2023. URL https://github.com/baichuan-inc/Baichuan2.
* [46] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* [47] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. GIm: General language model pretraining with autoregressive blank infilling. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 320-335, 2022.
* [48] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. _arXiv preprint arXiv:2107.03374_, 2021.
* [49] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. _NeurIPS_, 2021.

## Appendix A Limitations

The limitations of our work are summarized as follows:

**Monolingual Capability**. ComplexBench is primarily constructed based on Chinese reference instructions, which may neglect some elements in other languages and cultures that can influence the complexity of instructions. Recognizing this constraint, we plan to expand ComplexBench by incorporating multiple languages to investigate the disparities in complex instruction-following ability of LLMs across different linguistic environments in future iterations.

**LLM-based Evaluation**. The evaluation method based on LLM is widely used in the automatic evaluation process of ComplexBench. Although experiments show that our evaluation method achieves satisfactory agreement with human judgment generally, the potential biases of LLM-as-Judge, such as verbosity and self-enhancement [5], may affect the overall evaluation correctness. Additionally, we utilize GPT-4-1106 commercial APIs for evaluation, which presents challenges such as high costs and potential data leakage. We leave the development of more accurate and efficient methods for evaluating complex instruction-following as important future work.

## Appendix B Author Statement and License

ComplexBench is distributed under CC BY 4.0. The evaluation code of ComplexBench is distributed under the MIT license. We will bear all responsibility in case of violation of rights, etc.

## Appendix C Task Distribution of ComplexBench

We refer to the taxonomy of AlignBench [6] to categorize the task types of instructions in the ComplexBench. Taking into account that instructions about mathematics have relatively fixed answers and are difficult to construct complex instructions, as well as the coarse granularity of the writing ability category. We remove mathematical and use 4 subcategories of writing ability in AlignBench: practical writing, creative writing, professional writing, and custom writing. When annotators construct instructions, they also provide task category labels simultaneously, the results are shown in Table 8.

## Appendix D Details of Constraint Dimensions

### Lexical Constraint

1. **Word Matching.** The response should accurately find the corresponding content of certain keywords in the given instruction.
2. **Keywords.** The response should (not) include certain keywords, or include several words from a keyword list.

### Format Constraint

1. **JSON Format.** The entire response should be wrapped in JSON format.

\begin{table}
\begin{tabular}{l|c} \hline \hline
**Category** & **\#Samples** \\ \hline Fundamental Language Ability & 159 \\ Advanced Chinese Understanding & 62 \\ Open-ended Questions & 115 \\ Practical Writing & 195 \\ Creative Writing & 105 \\ Professional Writing & 183 \\ Custom Writing & 73 \\ Logical Reasoning & 107 \\ Task-oriented Role Play & 95 \\ Professional Knowledge & 56 \\ \hline Total & 1150 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Task distribution of ComplexBench dataset.

2. **Markdown Format.** The response should follow specific Markdown formats, such as equations, headings, and tables.
3. **Bullets Format.** The response should (not) contain bullet points.
4. **Length.** Control the length of the response, including the number of words, sentences, paragraphs, etc. This constraint can be used in combination with others, such as controlling the number of bullet points or the number of keywords included.
5. **Start with.** Control the content at the beginning of the response.
6. **End with.** Control the content at the end of the response.
7. **Punctuation.** Control the punctuation that appears in the response.
8. **Template.** The response should mimic the format of the given output template.

### Semantic Constraint

1. **Language Style.** The response should adhere to a specific language style. We use the taxonomy of CharacterGLM [10], which defines language style from multiple aspects such as formality, imitation of celebrities, context-specific scenes, and discourse features (like using style from a certain website, emoji, etc.).
2. **Personalization.** The response should align with certain character attributes.
3. **Topic.** The response should focus on a specific topic.
4. **Sentiment.** The response should contain specific emotions. We refer to the six fine-grained categories of ECM [38] for sentiment, named as _Like_, _Happy_, _Sad_, _Disgust_, _Angry_, _Other_.

### Utility Constraint

1. **Helpfulness.** The response should follow task descriptions.
2. **Target Language.** The response should be in a specific language, such as simplified Chinese, traditional Chinese or English.
3. **Supportiveness.** The response should be faithful to input texts, answering based on the information provided in the text completely.
4. **Consistency.** The content of the response should be consistent and free of contradictions.
5. **Factuality.** The response should correspond with facts, which primarily applies to instructions with definitive answers such as mathematical and logical reasoning.

## Appendix E Prompts in Rule-Augmented LLM-based Evaluation

### Prompts for Extractor in Rule-Augmented LLM-based Evaluation

Table 9 provides the prompt template we used for the LLM extractor in Rule-Augmented LLM-based evaluation. And Table 10 provides an example of scoring object extraction and their English translation. To improve performance, we use 6 manually constructed in-context examples in the prompt. Considering that the extraction of content differs significantly when there are multiple scoring objects (e.g., scoring question _"Does each shot's dialogue in the model output start with an interrogative sentence?"_), compared to when there is only one scoring object (e.g., scoring question _"Does the title of the speech given by the model have no more than 10 characters?"_). We use different sets of in-context examples for these two situations.

### Prompts for Evaluator in Rule-Augmented LLM-based Evaluation

Table 11 provides the prompt template we used for the LLM evaluator in Rule-Augmented LLM-based evaluation. And Table 12 provides an example of automatic evaluation and their English translation. We have also explored different settings where all scoring questions from the instruction are presented to the evaluation model simultaneously, or asking the evaluation model to choose "YES" or "NO" without analysis. Ultimately, we found that the current settings achieve the highest level of agreement with humans.

You are an information extraction expert. Below, you will be provided with an [Input Instruction] and its corresponding [Model Response]. Additionally, you will be given a [Scoring Question], which is designed to assess whether [Model Response] satisfies some of requirements within [Input Instruction]. Your task is to extract scoring object in [Model Response] for [Scoring Question].

For example, if [Model Response] contains two essays and [Scoring Question] is "Does the first essay have at least \(500\) words?", then you should only output the first essay from [Model Response]. If [Scoring Question] is "Does the second essay use a vivid language style?", then you should only output the second essay from [Model Response]. And if [Scoring Question] is "Does the output end with 'Reporter from Bloomberg'?", then you should only output the last sentence of [Model Response].

**## Note**

(1) You should copy continuous segments from [Model Response] exactly as it is, without any modification, addition, deletion, or splicing.

(2) Your task is not to extract the part of [Model Response] that satisfies [Scoring Question], but to extract scoring object in [Model Response] for [Scoring Question], even if it does not satisfy corresponding requirements. You do not need to pay attention to what the specific requirements of [Scoring Question] are, nor do you need to evaluate whether [Model Response] satisfies [Scoring Question] requirements.

(4) If there are multiple scoring objects in [Model Response], please use "\(\|\)" to separate each other. If the scoring object is the entire [Model Response], please directly output "All". If the scoring object does not exist in [Model Response], please directly output "None".

(5) Generally, "beginning" refers to the first sentence of [Model Response], and "ending" refers to the last sentence of [Model Response].

Please first give your analysis and explanation of the task, then output the result of the evaluation object you extracted.

**## Output Format**

[Explanation]

xxx

**Evaluation Object for Scoring Question**

Scoring Object: xxx

**(In-Context Examples)**

Please refer to the above examples, and extract the scoring object in [Model Response] for [Scoring Question]. Again, note that you should copy the continuous segments from [Model Response] exactly as it is, without any modification, addition, deletion, or splicing. When giving your scoring object, you must ensure that every character is a real existence in [Model Response].

**[Input Instruction]**

**[Model Response]**

**[Model Response]**

**[Scoring Question]**

\begin{table}
\begin{tabular}{|l|} \hline
**Input Instruction** \\ \{Input Instruction\} \\ \{**Model Response**\} \\ \{**Scoring Question**\} \\ \hline \hline \end{tabular}
\end{table}
Table 9: Prompt template for extraction in Rule-Augmented LLM-based Evaluation.

\begin{table}
\begin{tabular}{p{28.5pt} p{28.5pt}} \hline \hline
**Scoring Question** & **\#1** \\
**Question** & Is the example generated by the model response within 200 words? **(Length)** \\ \hline
**Extractor Output** & **\#1** \\ \hline \hline \end{tabular}
\end{table}
Table 10: An example of segments extraction and their English translation.

Please act as a fair judge, analyze the content of the Model Response, and choose "YES" or "NO" to answer whether the requirement of the Question is satisfied. You should follow the following judgment rules.

- The Question can be seen as the scoring points of the Instruction in steps, judging whether a part of it is satisfied. Therefore, you only need to consider the requirement within the Question, without focusing on whether the entire Instruction is fully satisfied.

- YES: Check whether the Model Response completes the requirement of the Question thoroughly. You should fully understand the meaning of the Question and not miss any small details, only focus on the Question and not pay attention to other requirements in the Instruction. It must be perfectly and sufficiently completed to be evaluated as "YES", without any slight errors or ambiguities. There should not be situations such as "basically correct", "mostly correct", or "correct under certain conditions". These situations should all be evaluated as "NO".

- NO: If the Model Response does not satisfy the requirement of the Question or provides relevant information about the Question, choose "NO".

**Example:** If the Question asks "Is the second sentence of the generated text a complex sentence?" but the Model Response only has one sentence. It does not provide relevant information about the Question. Therefore, you should choose "NO".

**## Detailed Scoring Rules**

(1) When you evaluate whether the Model Response contains bullet points, it must have clear bullet points or numbers to be evaluated as "YES". Merely using conjunctions like "firstly", "then", "next", and "finally" cannot be considered bullet points, and should be evaluated as "NO".

(2) When you evaluate whether the Model Response is in a specific language (such as Chinese/English) unless the Instruction mentions the need to use multiple languages, it must use only that language to be evaluated as "YES", the appearance of other languages (i.e., words from other languages) should be evaluated as "NO".

(3) When you evaluate whether the Model Response selects the correct branch, it is necessary to judge whether the Model Response completes the sub-task of the corresponding branch based on the selection branch in the Instruction.

(4) If the Question includes descriptions like "every", "all", etc., you should consider every object in Model Response about the Question, only if all objects satisfy the requirement of the Question, it can be evaluated as "YES".

## Output Format

Analysis: xxx

Answer: Yes / No

## Evaluation Information

**Instruction**

{Input Instruction}

**Model Response**

{Model Response}

**Question**

{Scoring Question}

Please analyze and answer whether the Model Response satisfies the requirement of Question:

\begin{table}
\begin{tabular}{p{284.5pt}} \hline \hline Please act as a fair judge, analyze the content of the Model Response, and choose ”YES” or ”NO” to answer whether the requirement of the Question is satisfied. You should follow the following judgment rules. \\ - The Question can be seen as the scoring points of the Instruction in steps, judging whether a part of it is satisfied. Therefore, you only need to consider the requirement within the Question, without focusing on whether the entire Instruction is fully satisfied. \\ - YES: Check whether the Model Response completes the requirement of the Question thoroughly. You should fully understand the meaning of the Question and not miss any small details, only focus on the Question and not pay attention to other requirements in the Instruction. It must be perfectly and sufficiently completed to be evaluated as ”YES”, without any slight errors or ambiguities. There should not be situations such as ”basically correct”, ”mostly correct”, or “correct under certain conditions”. These situations should all be evaluated as ”NO”. \\ - NO: If the Model Response does not satisfy the requirement of the Question or provides relevant information about the Question, choose ”NO”. \\ \hline \hline \end{tabular}
\end{table}
Table 11: Prompt template for LLM-based evaluation in Rule-Augmented LLM-based Evaluation.

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_FAIL:21]

Below, a reference instruction, the requirements of the minimum number of constraint dimensions in each constraint type, and the minimum number of composition types will be provided. Please complete the following annotation tasks in order.

1. Construct a new complex instruction based on the reference instruction, ensuring that the number of constraint dimensions in each constraint type within the instruction is greater than or equal to the requirements, and the number of composition types within the instruction is greater than or equal to the requirements. You may also create the new complex instruction from scratch without referencing the provided reference instruction.

2. Annotate the task type of the newly constructed complex instruction, choosing the closest type from the ten options provided.

3. Annotate all the constraint dimensions and composition types within the newly constructed complex instruction.

4. Annotate the scoring questions for the newly constructed complex instruction. Please design a 'yes/no' question for each constraint dimension and composition type to verify if it is satisfied.

5. Annotate the dependencies of scoring questions. Specifically, for _Chain_, all the scoring questions of the subsequent task depend on the answers to those of the preceding task. And for _Selection_, all the scoring questions of the selection branch depend on whether the correct selection branch is selected. When annotating each scoring question, please also annotate which scoring questions it depends on (if any).

When constructing complex instructions, you should adhere to the following three general principles:

1. Clarity & Reasonableness: The instruction should be easy to understand, unambiguous, and realistic, with at least one reasonable answer.

2. Validity of Constraints: Every constraint within the instruction should substantially influence the output.

3. Complexity & Difficulty: The instruction should be challenging for most LLMs and be capable of distinguishing the complex instruction-following abilities of different LLMs.

**[Reference Instruction]**

[reference_instruction]**

**[Task Requirements]**

The minimum number of lexical constraints: [number_of_lexical_constraints]

The minimum number of format constraints: [number_of_format_constraints]

The minimum number of semantic constraints: [number_of_semantic_constraints]

The minimum number of utility constraints: [number_of_utility_constraints]

The minimum number of _And:_ [number_of_And]

The minimum number of _Chain:_ [number_of_Chain]

The minimum number of _Selection:_ [number_of_Selection]

Please construct a new complex instruction based on the reference instruction: [newly_constructed_instruction]

Please choose the task category for the constructed instruction from the following ten options: [task_type_of_newly_constructed_instruction]

A. Fundamental Language Ability & Advanced Chinese Understanding & Open-ended Questions & D. Practical Writing & C. Creative Writing & F. Professional Writing & G. Custom Writing & H. Logical Reasoning & I. Task-oriented Role Play & J. Professional Knowledge

Please choose all lexical constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: [lexical_constraints_in_newly_constructed_instruction]

A. Word Matching & B. Keywords

Please choose all format constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: [format_constraints_in_newly_constructed_instruction]

A. Json Format & B. Markdown Format & C. Bullets Format & D. Punctuation & E. Length & F. Start with & G. End with & H. Template

Please choose all semantic constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: [semantic_constraints_in_newly_constructed_instruction]

A. Language Style & B. Personalization & C. Topic & D. Sentiment

Please choose all utility constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: [utility_constraints_in_newly_constructed_instruction]

A. Target Language & B. Suppressiveness & C. Consistency & D. Factuality & E. Helpfulness

Please choose all composition types within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: [composition_types_in_newly_constructed_instruction]

A. _And & B. Chain & C. Selection_

Please annotate all scoring questions for the constructed instruction, and indicate the constraint dimensions/composition types they evaluate. Each scoring question should be formatted as follows:

**1. Is the language of the article generated by the model in English? (Target language)**

{scoring_questions_for_newly_constructed_instruction}

Please annotate the dependencies of scoring questions. Each scoring question should be formatted as follows (no need to write if there is no dependency):

**4. Is the number of words in the model's response more than 300? (Length, depends on **1**)**

{dependencies_of_scoring_questions}

\begin{table}
\begin{tabular}{l} Below, a reference instruction, the requirements of the minimum number of constraint dimensions in each constraint type, and the minimum number of composition types will be provided. Please complete the following annotation tasks in order.

1. Construct a new complex instruction based on the reference instruction, ensuring that the number of constraint dimensions in each constraint type within the instruction is greater than or equal to the requirements, and the number of composition types within the instruction is greater than or equal to the requirements. You may also create the new complex instruction from scratch without referencing the provided reference instruction.

2. Annotate the task type of the newly constructed complex instruction, choosing the closest type from the ten options provided.

3. Annotate all the constraint dimensions and composition types within the newly constructed complex instruction.

4. Annotate the scoring questions for the newly constructed complex instruction. Please design a 'yes/no' question for each constraint dimension and composition type to verify if it is satisfied.

5. Annotate the dependencies of scoring questions. Specifically, for _Chain_, all the scoring questions of the subsequent task depend on the answers to those of the preceding task. And for _Selection_, all the scoring questions of the selection branch depend on whether the correct selection branch is selected. When annotating each scoring question, please also annotate which scoring questions it depends on (if any).

When constructing complex instructions, you should adhere to the following three general principles:

1. Clarity & Reasonableness: The instruction should be easy to understand, unambiguous, and realistic, with at least one reasonable answer.

2. Validity of Constraints: Every constraint within the instruction should substantially influence the output.

3. Complexity & Difficulty: The instruction should be challenging for most LLMs and be capable of distinguishing the complex instruction-following abilities of different LLMs.

[Reference Instruction]**

[reference_instruction]**

[reference_instruction]**

[Task Requirements]

The minimum number of lexical constraints: [number_of_lexical_constraints]

The minimum number of format constraints: [number_of_format_constraints]

The minimum number of semantic constraints: [number_of_semantic_constraints]

The minimum number of utility constraints: [number_of_utility_constraints]

The minimum number of _And:_ [number_of_And]

The minimum number of _Chain:_ [number_of_Chain]

The minimum number of _Selection:_ [number_of_Selection]

Please construct a new complex instruction based on the reference instruction: [newly_constructed_instruction]

Please choose the task category for the constructed instruction from the following ten options: [task_type_of_newly_constructed_instruction]

A. Fundamental Language Ability & B. Advanced Chinese Understanding & Open-ended Questions & D. Practical Writing & C. Creative Writing & F. Professional Writing & G. Custom Writing & H. Logical Reasoning & I. Task-oriented Role Play & J. Professional Knowledge

Please choose all lexical constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: [lexical_constraints_in_newly_constructed_instruction]

A. Word Matching & B. Keywords

Please choose all format constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: [format_constraints_in_newly_constructed_instruction]

A. Json Format & B. Markdown Format & C. Bullets Format & D. Punctuation & E. Length & F. Start with & G. End with & H. Template

Please choose all semantic constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: [semantic_constraints_in_newly_constructed_instruction]

A. Language Style & B. Personalization & C. Topic & D. Sentiment

Please choose all utility constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: [utility_constraints_in_newly_constructed_instruction]

A. Target Language & B. Suppressiveness & C. Consistency & D. Factuality & E. Helpfulness

Please choose all composition types within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: [composition_types_in_newly_constructed_instruction]

A. _And & B. Chain & C. Selection_

Please annotate all scoring questions for the constructed instruction, and indicate the constraint dimensions/composition types they evaluate. Each scoring question should be formatted as follows:

**1. Is the language of the article generated by the model in English? (Target language)**

{scoring_questions_for_newly_constructed_instruction}

Please annotate the dependencies of scoring questions. Each scoring question should be formatted as follows (no need to write if there is no dependency):

**4. Is the number of words in the model’s response more than 300? (Length, depends on **1**)**

{dependencies_of_scoring_questions}

\end{table}
Table 14: Guidelines for data annotation (translated into English). The blue part is the information provided to the annotators, and the red part is content that requires the annotators to make annotations.

[MISSING_PAGE_EMPTY:23]

Below, an instruction containing _Selection_ will be provided. Please keep all selection branches unchanged, and only modify the selection condition based on the selection function to construct multiple new instructions, covering all selection branches different from the original instruction. For example, for single-layer _Selection_, if the selection function has M different values, you should construct M-1 new instructions, changing the selection conditions to cover all values different from the original instruction.

After constructing the new instructions, similar to the data annotation task, you need to annotate the task types of the instructions and all constraint dimensions and composition types within the instructions. Furthermore, you should annotate the scoring questions for the newly constructed instructions and their dependencies.

[Original Instruction] {instruction} Please annotate the first new instruction by modifying the selection conditions of the original instruction: {newly_constructed_instruction_1} Please choose the task category for the constructed instruction from the following ten options: {task_type_of_newly_constructed_instruction_1} A. Fundamental Language Ability B. Advanced Chinese Understanding C. Open-ended Questions D. Practical Writing E. Creative Writing F. Professional Writing G. Custom Writing H. Logical Reasoning I. Task-oriented Role Play J. Professional Knowledge Please choose all lexical constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: {lexical_constraints_in_newly_constructed_instruction_1} A. Word Matching B. Keywords Please choose all format constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: {format_constraints_in_newly_constructed_instruction_1} A. Json Format B. Markdown Format C. Bullets Format D. Punctuation E. Length F. Start with G. End with H. Template Please choose all semantic constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: {semantic_constraints_in_newly_constructed_instruction_1} A. Language Style B. Personalization C. Topic D. Sentiment Please choose all utility constraints within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: {utility_constraints_in_newly_constructed_instruction_1} A. Target Language B. Supportiveness C. Consistency D. Factuality E. Helpfulness Please choose all composition types within the constructed instruction, multiple selections are allowed, and an option can be chosen more than once: {composition_types_in_newly_constructed_instruction_1} A. _And B. Chain C. Selection Please annotate all scoring questions for the constructed instruction, and indicate the constraint dimensions/composition types they evaluate. Each scoring question should be formatted as follows: **1**. Is the language of the article generated by the model in English? (Target language) {scoring_questions_for_newly_constructed_instruction_1} Please annotate the dependencies of scoring questions. Each scoring question should be formatted as follows (no need to write if there is no dependency): **4**. Is the number of words in the model's response more than 300? (Length, depends on **1**) {dependencies_of_scoring_questions}...... In the same format as above, please annotate the nth new instruction constructed by modifying the selection conditions of the original instruction: {newly_constructed_instruction_n}......

\begin{table}
\begin{tabular}{l l} \hline \hline Below, an instruction containing _Selection_ will be provided. Please keep all selection branches unchanged, and only modify the selection condition based on the selection function to construct multiple new instructions, covering all selection branches different from the original instruction. For example, for single-layer _Selection_, if the selection function has M different values, you should construct M-1 new instructions, changing the selection conditions to cover all values different from the original instruction. \\ \hline \hline \end{tabular}
\end{table}
Table 16: Guidelines for selection branch expansion (translated into English). The blue part is the information provided to the annotators, and the red part is content that requires the annotators to make annotations.

[MISSING_PAGE_EMPTY:25]

\begin{table}
\begin{tabular}{p{284.5pt}} \hline \hline
**Task Details** \\
1. A “Yes” must indicate that the scoring point has been fully and sufficiently satisfied. Any response that contains errors, ambiguity, or cannot be judged should be labeled as “No”. There is no such thing as “basically correct” or “correct under certain conditions”. These should all be labeled as “No”. If the model response does not contain the object to be evaluated by the scoring question, it should also be labeled as “No”. 2. Please only consider whether the scoring point has been fully satisfied by the model response, without the need to consider whether the entire instruction has been satisfied by the model response. \\ \hline
**[in-context examples]** \\
**[Instruction]** \\
**[Model Response]** \\
**[model\_response]** \\
**[Scoring Question]** \\
**[scoring\_question]** \\
**Your choice is: \{option\}** \\
**A. Yes** \\
**B. No** \\ \hline \hline \end{tabular}
\end{table}
Table 18: Guidelines for scoring questions verification and their English translation. The blue part is the information provided to the annotators, and the red part is content that requires the annotators to make annotations.

[MISSING_PAGE_EMPTY:27]

\begin{table}
\begin{tabular}{l l} \hline \hline
**Reference Instruction** & Write lyrics for a song about innovation that appeals to teenagers. Please place your entire song’s lyrics in double quotation marks. \\ \hline
**Task Requirements** & The minimum number of lexical constraints: 0 \\  & The minimum number of format constraints: 2 \\  & The minimum number of semantic constraints: 1 \\  & The minimum number of utility constraints: 0 \\  & The minimum number of _And_: 0 \\  & The minimum number of _Chain_: 1 \\  & The minimum number of _Selection_: 0 \\ \hline \hline \end{tabular} _The Content that Requires the Annotators to Make Annotations_

\begin{tabular}{l l} \hline
**Nevly Constructed** & Write lyrics for a song with the topic of \({}^{*}\)Beijing tourism.\({}^{*}\)Please place your entire song’s lyrics in double quotation marks. Then write two comments for the song, strongly conveying appreciation for it, each comment should be around 20 words. \\ \hline
**Task Type** & Creative Writing \\ \hline
**Constraint Dimensions** & Punctuation, Length, Topic, Sentiment, Helpfulness \\ \hline
**Composition Type** & _And_, _Chain_ \\ \hline
**Scoring Questions and** & 1. Does the model generate lyrics for a song? (_Chain_, Helpfulness) \\
**Their Dependencies** & 2. Do the lyrics for the song generated by the model with the topic of \({}^{*}\)Beijing tourism\({}^{*}\)? (Topic) \\  & 3. Are all the lyrics of the song generated by the model placed in double quotation marks? (Punctuation) \\  & 4. After generating the lyrics of the song, does the model generate two comments for the song? (Helpfulness, dependent on 1) \\  & 5. Do the comments generated by the model strongly convey appreciation for the song? (Sentiment, dependent on 1) \\  & 6. Are the comments generated by the model around 20 words each? (Length, dependent on 1) \\ \hline \hline \end{tabular}
\end{table}
Table 20: An example of data annotation.

More Experimental Results and Analysis

### The Influence of Composition Types Nested Methods

Table 21 presents DRFR of GPT-3.5-Turbo-1106 on instructions with different numbers of each composition type. Nested multiple _Selection_ seems to be significantly more difficult than other composition type nested methods. And the addition of _And_ has a limited impact on the overall complexity of instructions. The result reveals the weakness in the ability of LLMs to follow complex instructions with multi-layer tree structures, highlighting the importance of further efforts to improve LLMs in these areas.

### Detailed Results of Each Constraint and Composition Type

Table 22 presents the average accuracy of LLMs on diverse constraint dimensions and composition types. Topic, Markdown Format, Consistency, Sentiment, and Personalization seem to be the easiest constraint dimensions for LLMs overall, while Length, Punctuation, Keywords, End with, and Factuality pose the greatest challenges. It is worth noting that the performance of all LLMs on Length is far from satisfactory, with even the strongest model achieving only an accuracy rate of 0.532. This result indicates that there is still significant room for improvement in the ability of current LLMs to precisely control and plan the output content.

\begin{table}
\begin{tabular}{c|c c c|c} \hline \hline \multicolumn{3}{c|}{**Composition Type**} & \multicolumn{1}{c}{**DRFR**} \\ \hline
**Number** & **And** & **Chain** & **Selection** & \\ \hline \multirow{3}{*}{1} & 1 & 0 & 0 & 0.845 \\  & 0 & 1 & 0 & 0.686 \\  & 0 & 0 & 1 & **0.682** \\ \hline \multirow{3}{*}{2} & 1 & 1 & 0 & 0.630 \\  & 1 & 0 & 1 & 0.651 \\  & 0 & 1 & 1 & 0.570 \\  & 0 & 0 & 2 & **0.377** \\ \hline \multirow{3}{*}{3} & 1 & 1 & 1 & 0.529 \\  & 1 & 0 & 2 & 0.515 \\  & 0 & 1 & 2 & **0.308** \\ \hline \hline \multicolumn{3}{c}{4} & 1 & 1 & 2 & 0.083 \\ \hline \hline \end{tabular}
\end{table}
Table 21: DRFR of GPT3.5-Turbo-1106 on instructions with different numbers of each composition type.

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c c c} \hline \hline \multicolumn{1}{c}{**Length Language Module**} & (\(\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{\text{    }}}}}}}} }}}}}}}}}\)) & \multicolumn{1}{c}{**DRFR**} \\ \hline
**Number** & **And** & **Chain** & **Selection** & & & & & & & & & \\ \hline \multirow{3}{*}{1} & 1 & 0 & 0 & 0 & 0.845 \\  & 0 & 1 & 0 & 1 & 0 & 0.686 \\  & 0 & 0 & 0 & 1 & **0.682** \\ \hline \multirow{3}{*}{2} & 1 & 1 & 1 & 0 & 0.630 \\  & 1 & 0 & 1 & 1 & 0.651 \\  & 0 & 1 & 1 & 0.570 \\  & 0 & 0 & 2 & **0.377** \\ \hline \multirow{3}{*}{3} & 1 & 1 & 1 & 1 & 0.529 \\  & 1 & 0 & 2 & 0.515 \\  & 0 & 1 & 2 & **0.308** \\ \hline \hline \multicolumn{1}{c}{4} & 1 & 1 & 2 & 0.083 \\ \hline \hline \end{tabular}
\end{table}
Table 22: Detailed results of LLMs on diverse constraint dimensions and composition types. The highest performance overall is **bold**.

### Detailed Results of Each Task Type

Table 23 presents the DRFR of the selected LLMs for each task type. We find that the performance of LLMs across tasks is balanced overall. Relatively, LLMs perform better on tasks related to writing and role-playing, while they have shortcomings in Logical Reasoning, Advanced Chinese Understanding, and Open-ended Questions. All LLMs exhibit significant weaknesses in Logical Reasoning, which is consistent with the Reasoning Drawbacks found in AlignBench [6].

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c} \hline \hline
**Task Type** & **Fund.** & **Chi.** & **Open.** & **Prac.** & **Crea.** & **Pro. Writing** & **Cust.** & **Role.** & **Pro. Knowledge** & **Logic.** & **Overall** \\ \hline \multicolumn{10}{c}{_Closed-Source Language Models_} \\ \hline GPT-4-1106 & **0.783** & **0.751** & **0.761** & 0.810 & **0.845** & 0.808 & **0.870** & 0.856 & **0.838** & 0.681 & **0.800** \\ Claude-3-Opus & 0.752 & 0.729 & 0.722 & 0.805 & **0.845** & **0.816** & 0.864 & **0.874** & 0.722 & 0.698 & 0.788 \\ GLM-4 & 0.738 & 0.717 & 0.735 & **0.821** & 0.798 & 0.800 & 0.843 & 0.858 & 0.745 & 0.683 & 0.779 \\ ERNIERbot-4 & 0.732 & 0.721 & 0.680 & 0.802 & 0.804 & 0.759 & 0.828 & 0.824 & 0.757 & **0.718** & 0.764 \\ GPT-3.5-Turbo-1106 & 0.675 & 0.584 & 0.578 & 0.743 & 0.737 & 0.710 & 0.743 & 0.779 & 0.645 & 0.517 & 0.682 \\ \hline \multicolumn{10}{c}{_Open-Source Language Models_} \\ \hline Question.5-72B-Chat & 0.713 & 0.695 & 0.653 & 0.798 & 0.810 & 0.772 & 0.831 & 0.840 & 0.749 & 0.619 & 0.752 \\ Llama-3-70B-Instruct & 0.732 & 0.617 & 0.676 & 0.771 & 0.833 & 0.767 & 0.855 & 0.853 & 0.741 & 0.678 & 0.757 \\ \hline InternalM2-20B-Chat & 0.641 & 0.595 & 0.619 & 0.713 & 0.751 & 0.676 & 0.778 & 0.792 & 0.691 & 0.512 & 0.678 \\ Qwen1.5-14B-Chat & 0.617 & 0.621 & 0.600 & 0.715 & 0.724 & 0.703 & 0.790 & 0.819 & 0.695 & 0.506 & 0.680 \\ Biachuan2-13B-Chat & 0.549 & 0.528 & 0.515 & 0.646 & 0.665 & 0.608 & 0.660 & 0.713 & 0.548 & 0.410 & 0.591 \\ \hline Llama-38-Bnn-Instruct & 0.610 & 0.558 & 0.580 & 0.690 & 0.702 & 0.673 & 0.719 & 0.670 & 0.622 & 0.468 & 0.638 \\ Mistral-71B-Instruct & 0.530 & 0.394 & 0.578 & 0.647 & 0.686 & 0.604 & 0.713 & 0.686 & 0.494 & 0.457 & 0.592 \\ Qwen1.5-78B-Chat & 0.601 & 0.517 & 0.619 & 0.715 & 0.720 & 0.660 & 0.749 & 0.790 & 0.641 & 0.503 & 0.658 \\ InternalM2-7B-Chat & 0.628 & 0.517 & 0.553 & 0.712 & 0.622 & 0.662 & 0.692 & 0.743 & 0.598 & 0.479 & 0.634 \\ ChatGLM3-6B-Chat & 0.510 & 0.439 & 0.464 & 0.586 & 0.606 & 0.606 & 0.636 & 0.605 & 0.537 & 0.368 & 0.546 \\ \hline \hline \end{tabular}
\end{table}
Table 23: Automated DRFR of LLMs on different task types. The highest performance among open-source models is underlined, while the highest performance overall is **bold**. “_Fund.” denotes Fundamental Language Ability, “_Chi._” denotes Advanced Chinese Understanding, “_Open._” denotes Open-ended Questions, “_Prac._” denotes Practical Writing, “_Crea._” denotes Creative Writing, “_Pro. Writing_” denotes Professional Writing, “_Cust._” denotes Custom Writing, “_Role._” denotes Task-oriented Role Play, “_Pro. Knowledge_” denotes Professional Knowledge and “_Logic._” denotes Logical Reasoning.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Please refer to Section 1. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to Appendix A. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [N/A]Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All the information needed to reproduce the main experimental results is provided in Section 4 and 5. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: We have submitted the dataset we constructed and the code for running all experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: We follow existing works on complex instruction-following [3, 7, 19] to not adopt statistical significance in our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The supplementary materials have elaborated on this. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conforms with the NeurIPS Code of Ethics in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have provided the analysis on broader impacts in the supplementary materials. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We have described the safeguards of our paper in the supplementary materials. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Please refer to Appendix B. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have provided the documentation of new assets in the supplementary materials. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: Please refer to Appendix F. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [No] Justification: Our work is only to construct a benchmark on complex instruction-following to test the corresponding ability of LLMs. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.