# Sample-efficient Bayesian Optimisation

Using Known Invariances

 Theodore Brown\({}^{*}\)\({}^{1,2}\) Alexandru Cioba\({}^{*}\)\({}^{3}\) Ilija Bogunovic\({}^{1}\)

\({}^{*}\)Equal contribution

\({}^{1}\)University College London \({}^{2}\)United Kingdom Atomic Energy Authority \({}^{3}\)MediaTek Research

{theodore.brown.23, i.bogunovic}@ucl.ac.uk

alexandru.cioba@mtkresearch.com

###### Abstract

Bayesian optimisation (BO) is a powerful framework for global optimisation of costly functions, using predictions from Gaussian process models (GPs). In this work, we apply BO to functions that exhibit _invariance_ to a known group of transformations. We show that vanilla and constrained BO algorithms are inefficient when optimising such invariant objectives, and provide a method for incorporating group invariances into the kernel of the GP to produce _invariance-aware_ algorithms that achieve significant improvements in sample efficiency. We derive a bound on the maximum information gain of these invariant kernels, and provide novel upper and lower bounds on the number of observations required for invariance-aware BO algorithms to achieve \(\epsilon\)-optimality. We demonstrate our method's improved performance on a range of synthetic invariant and quasi-invariant functions. We also apply our method in the case where only some of the invariance is incorporated into the kernel, and find that these kernels achieve similar gains in sample efficiency at significantly reduced computational cost. Finally, we use invariant BO to design a current drive system for a nuclear fusion reactor, finding a high-performance solution where non-invariant methods failed.

## 1 Introduction

Bayesian optimisation (BO) has been applied to many global optimisation tasks in science and engineering, such as improving the performance of web servers [30], reducing the losses in particle accelerators [26], or maximising the potency of a drug [1]. The most challenging real-world tasks tend to have large or continuous input domains, as well as target functions that are costly to evaluate.

One particular example of interest is the design and operation of nuclear fusion reactors, which promise to deliver huge amounts of energy with zero greenhouse gas emissions. However, the development of high-performance fusion power plant scenarios is a difficult optimisation task, involving expensive simulations and physical experiments [39, 11]. For tasks like this, it is desirable to develop _sample efficient_ algorithms that only require a small number of evaluations to find the optimal solution.

Crucially, the physical world is characterized by underlying structures that can be leveraged to significantly improve sample efficiency. The concept of _invariance_, which describes properties that remain constant under various transformations, is one of the most fundamental structures in nature. A prime example of invariance is found in image classification tasks: a classifier should consistently identify a cat as a cat, regardless of its orientation, position, or size. The central question we answer is the extent to which the sample efficiency of BO can be improved by exploiting these invariances.

We study a general approach for incorporating invariances into any BO algorithm. We represent invariant objective functions as elements of a reproducing kernel Hilbert space (RKHS), and produce invariance-aware versions of existing Gaussian process (GP) bandit optimisation algorithms by using the kernel of this RKHS. We apply these algorithms to synthetic and nuclear fusion optimisationtasks, and show that incorporating the underlying invariance into the kernel significantly improves the sample efficiency. We also investigate the behaviour of the algorithms when the full invariant kernel is not known, and provide an empirical study of the performance of invariance-aware BO on target functions that are 'quasi-invariant' (modelled as a sum of invariant and non-invariant components). Our main contribution is the derivation of novel upper and lower bounds on the regret achieved by invariance-aware BO algorithms.

### Related work

**Invariance in deep learning.** The role of invariance in deep learning has been extensively explored in the literature. Invariance, particularly to geometric transformations like rotation, scaling, and translation, is crucial for the robust performance of deep learning models. Many deep learning methods incorporate invariance via _data augmentation_, in which additional training examples are generated by transforming the input data [28; 29]. However, the cost of kernelised learning methods often scales poorly with the number of training points \(n\) - Gaussian process inference, for example, incurs \(\mathcal{O}(n^{3})\) time and \(\mathcal{O}(n^{2})\) memory cost [47] - which means that data augmentation is prohibitively expensive. In this work, we adopt an alternative approach by directly embedding invariance properties into the model's structure, which effectively reduces computational complexity [44; 15].

**Structure-informed BO.** There is an extensive body of empirical BO literature that exploits known patterns present in the behaviour of a system. In [24], BO is applied after a variety of physics-informed feature transformations to the state; in [48] the authors employ BO to handle optimization over crystal configurations with symmetries found using [40]. However, in these works, only empirical results are given and asymptotic rates on sample complexity are not estimated.

**Invariant kernel methods.** The theory of group invariant kernels was introduced in [18; 27]. Using these ideas, [46] developed efficient variational methods for learning hyperparameters in invariant Gaussian processes. Invariant kernel ridge regression (KRR) for finite groups has been studied for translation- and permutation-invariant kernels [33; 4]. [38] quantify the gain in sample complexity for kernels invariant to the actions of Lie groups on arbitrary manifolds and tie the sample complexity bounds to the dimensions of the group. Motivated by the invariance properties of convolutional neural networks, [27; 33; 4] focus on kernelised _regression_ with the Neural Tangent Kernel; we instead consider the kernelised _bandit optimisation_ setting with the more general Matern family of kernels. These methods can be readily extended to less strict notions of so-called _quasi_-invariance [18; 46; 4].

**Matern kernels on manifolds.** The previous studies of invariant kernel algorithms rely on the spectral properties of kernels on the hypersphere [33; 4]; the relevant spectral theory for Matern kernels on Riemannian manifolds (including the hypersphere) was developed in [9]. These geometry-aware kernels were applied to optimisation tasks in [20]; however, the structure they include is restricted to the geometry of the domain only, rather than invariance to a set of transformations.

**Regret bounds for Bayesian optimisation.** The techniques for upper-bounding the regret of Bayesian optimisation algorithms using the kernel-dependent quantity known as maximum information gain were developed in [37]. Subsequent studies [43; 8; 14; 36; 19; 41] have focused on improving regret bounds in both cumulative and simple regret settings. [22] provided an upper bound for regret in BO using the convolutional NTK, which exhibits invariance only to the cyclic

Figure 1: Examples of group-invariant functions, sampled from a Gaussian process with the corresponding invariant kernel. Note that observing a point \(x\) (red) provides information about transformed locations \(\{\sigma(x):\sigma\in G\}\) (white).

group. [23] explored graph neural network bandits, offering maximum information gain rates by embedding permutation invariance using additive kernels. In [25], a permutation-invariant kernel is constructed in the same way as ours, and a regret analysis provided. Our work expands upon these approaches by considering more general (non-additive) kernels and broader transformation groups, demonstrating that the regret bounds from [22] can be shown to be a specialisation of ours. Lower bounds for kernelised bandits, including their robust variants ([7; 6]), have been examined in [35] and [12]. In this paper, we derive novel lower bounds on regret for the setting with invariant kernels.

### Contributions

Our main contributions are:

* We develop new upper bounds on sample complexity in Bayesian optimisation with invariant kernels, which explicitly quantify the gain in sample complexity due to the number of symmetries. We extend several results from the literature, as our statement applies to a wide class of groups. This is, to our knowledge, the first time such bounds are given in this degree of generality.
* We show how results from the literature (e.g. [12]) can be extended to obtain lower bounds on the sample complexity of invariant BO, using a novel construction and quantification of members of the RKHS of invariant functions. We extend the methods in the literature to the hyperspherical domain, giving a blueprint for similar constructions on other Riemannian manifolds.
* We conduct experiments that support our theoretical findings. We demonstrate that incorporating invariance to some _subgroup_ of transformations is sufficient to achieve good performance, a result of key importance when the full invariance is not known or is too expensive to compute. We also demonstrate the robustness of the invariant kernel method, in the case where the target function deviates from true invariance.
* We apply these methods to solve a real-world design problem from nuclear fusion engineering that is known to exhibit invariance; our invariance-aware method finds better solutions in fewer samples compared to standard approaches.

## 2 Problem statement

We begin by providing an overview of invariant function spaces. Consider a finite subgroup of isometries, \(G\), on a compact manifold, \(\mathcal{X}\), embedded in \(\mathbb{R}^{d}\). A function \(f:\mathcal{X}\to\mathbb{R}\) is _invariant_ to the action of \(G\) if, for all \(\sigma\in G\),

\[f\circ\sigma=f\quad\text{i.e.}\quad f(x)=f(\sigma(x))\quad\forall x\in\mathcal{ X}.\] (1)

We provide some examples of invariant functions for different groups \(G\) in Figure 1.

In this paper, we study the optimisation of invariant functions that belong to a reproducing kernel Hilbert space (RKHS). The invariance of the RKHS elements translates to invariance properties of the kernel. For \(k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\), we say that \(k\) is _simultaneously invariant_ to the action of \(G\) if, for all \(\sigma\in G\) and \(x,y\in\mathcal{X}\),

\[k(\sigma(x),\sigma(y))=k(x,y),\] (2)

and is _totally invariant_ to the action of \(G\) if, for all \(\sigma,\tau\in G\) and \(x,y\in\mathcal{X}\),

\[k(\sigma(x),\tau(y))=k(x,y),\] (3)

as introduced in [18]. Note that both inner product kernels \(k(x,y)=\kappa\left(\left\langle x,y\right\rangle\right)\) and stationary kernels \(k(x,y)=\kappa(\left\|x-y\right\|)\) satisfy simultaneous invariance, as \(G\) consists of isometries (which preserve distances and inner products).

Next, we outline the relationship between the invariance of the kernel and the invariance of the functions in the associated RKHS. In particular, the following proposition (Proposition 1) identifies the space of invariant functions as a subspace of the RKHS of a simultaneously invariant kernel. Moreover, it turns out that this subspace is an RKHS in its own right, with a corresponding kernel that is _totally invariant_. For a proof of these properties, see Appendix A.1.

**Proposition 1** (RKHS of invariant functions).: _Let \(\mathcal{X}\) and \(G\) be as defined above. Consider a positive definite and simultaneously invariant kernel \(k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\). The kernel defines a reproducing kernel Hilbert space \(\mathcal{H}_{k}\), whose elements are functions \(f:\mathcal{X}\to\mathbb{R}\), and a corresponding Hilbert space norm \(\|\cdot\|_{\mathcal{H}_{k}}\). Define the symmetrization operator \(S_{G}:\mathcal{H}_{k}\to\mathcal{H}_{k}\), such that_

\[S_{G}(f)=\frac{1}{|G|}\sum_{\sigma\in G}f\circ\sigma.\] (4)_Then, \(S_{G}\) is a self-adjoint projection operator, whose image \(\mathrm{Im}[S_{G}]\) is the subspace of \(\mathcal{H}_{k}\) that contains \(G\)-invariant functions. Moreoever, \(\mathrm{Im}[S_{G}]\) is an RKHS in its own right, \(\mathcal{H}_{k_{G}}\). For \(x,y\in\mathcal{X}\), the reproducing kernel of \(\mathcal{H}_{k_{G}}\) is_

\[k_{G}(x,y)=\frac{1}{|G|}\sum_{\sigma\in G}k(\sigma(x),y).\] (5)

We consider a kernelised optimisation setup with bandit feedback, in which the goal is to find the maximum of an unknown function \(f:\mathcal{X}\rightarrow\mathbb{R}\) that is invariant to a known group of transformations \(G\). We model \(f\) as belonging to the invariant RKHS \(\mathcal{H}_{k_{G}}\) from Proposition 1. Without further smoothness assumptions, this task is intractable; similarly to the common non-invariant setting, we make the regularity assumption that \(f\) has bounded RKHS norm, \(\|f\|_{\mathcal{H}_{k_{G}}}<B\). The learner is given \(k\) and \(G\). In Section 3.2, we explore setting \(k\) in \(k_{G}\) to common kernels, such as those from the Matern family.

At each round \(t\), the learner selects a point \(x_{t}\in\mathcal{X}\) and receives a noisy function observation

\[y_{t}=f(x_{t})+\eta_{t},\] (6)

where \(\eta_{t}\) is drawn independently from a \(\sigma_{n}\)-sub-Gaussian noise distribution. The agent reports a point \(x_{t}^{*}\) and incurs a corresponding _simple regret_, defined as

\[r_{t}=\operatorname*{arg\,max}_{x\in\mathcal{X}}f(x)-f(x_{t}^{*}).\] (7)

After \(T\) rounds, the _cumulative regret_ is given by

\[R_{T}=\sum_{t=1}^{T}\left(\operatorname*{arg\,max}_{x\in\mathcal{X}}f(x)-f(x_ {t}^{*})\right).\] (8)

In the next section, we quantify the impact of invariance on the number of function samples required to achieve a given regret, known as the _sample complexity_.

## 3 Sample complexity bounds for invariance-aware Bayesian optimisation

The bandit problem outlined in the previous section can be tackled using _Bayesian optimisation_ (BO) [17]. Intuitively, we expect BO algorithms that utilize the \(G\)-invariance of the target function to achieve improvements in sample complexity, as each observed \(x\) simultaneously provides information about additional transformed locations \(\sigma(x)\) for each \(\sigma\in G\) (see Figure 1).

In this section, we quantify the performance improvements by deriving bounds on the sample complexity of BO with totally invariant kernels (Equation (3)). We begin with an outline of the BO framework.

### Bayesian optimisation of invariant functions

In Bayesian optimisation, predictions from a Gaussian process (GP) are used in conjunction with an acquisition function to select points to query [17]. After collecting a sequence of input-observation pairs, \(\mathcal{D}=\{(x_{1},y_{1}),\ldots,(x_{t},y_{t})\}\), the learner models the invariant target function \(f\in\mathcal{H}_{k_{G}}\) using a GP with a totally invariant kernel, \(\mathcal{GP}(0,k_{G})\). The posterior predictive distribution of the function value at a specified input location \(x\) will be Gaussian with mean and variance given by

\[\mu_{t}(x) =\bm{k}_{G}^{T}(\bm{K}_{G}+\lambda\bm{I})^{-1}\bm{y}\] (9) \[\sigma_{t}^{2}(x) =k_{G}(x,x)-\bm{k}_{G}^{T}(\bm{K}_{G}+\lambda\bm{I})^{-1}\bm{k}_{ G},\] (10)

Figure 2: UCB run with standard \(k\) and invariant kernel \(k_{G}\) applied to an invariant objective; queried points in white. UCB with the invariant kernel requires significantly fewer samples to find the optimum.

where \(\bm{k}_{G}=[k_{G}(x,x_{1}),\ldots,k_{G}(x,x_{t})]^{T}\in\mathbb{R}^{t\times 1}\), \(\bm{K}_{G}\) is the kernel matrix with elements \([\bm{K}_{G}]_{i,j}=k_{G}(x_{i},x_{j})\ \forall i,j\in 1,\ldots,t\), \(\bm{y}=[y_{1},\ldots,y_{t}]^{T}\), and \(\lambda\) is a regularising term. Note that as this GP has a totally \(G\)-invariant kernel, \(k_{G}\), it can be interpreted as a distribution over \(G\)-invariant functions.

We consider two common acquisition functions. The tightest simple regret bounds in the literature are for the _Maximum Variance Reduction_ algorithm (MVR), in which the learner iteratively queries the points with highest uncertainty before reporting the point with highest posterior mean [42]. Another algorithm that has been widely studied is _Upper Confidence Bound_ (UCB), which balances exploration and exploitation by behaving optimistically in the face of uncertainty [37]. Pseudocode for MVR and UCB is provided in Algorithm 1 and Algorithm 2, respectively. Our analysis is framed in terms of MVR and simple regret, although in Section 4.1 we also investigate empirically the cumulative regret performance of UCB.

A popular family of kernels used in BO is the Matern kernel. On \(\mathbb{R}^{d}\), the Matern kernel with parameters \(l,\nu\) is given by

\[k(x,x^{\prime})=\frac{2^{1-\nu}}{\Gamma(\nu)}\left(\frac{\sqrt{2\nu}}{l}\|x-x ^{\prime}\|\right)^{\nu}K_{\nu}\left(\frac{\sqrt{2\nu}}{l}\|x-x^{\prime}\| \right),\] (11)

where \(K_{\nu}\) is the modified Bessel function of the second kind. The parameter \(\nu\) controls the differentiability of the corresponding GP, which enables Matern kernels to define a diverse range of function spaces. As the Matern kernel \(k\) is simultaneously invariant (Equation (2)), we can construct a totally invariant \(k_{G}\) and corresponding RKHS following Proposition 1. The bounds that we derive both hold for the Matern family; our upper bound also extends to _any_ kernel that exhibits polynomial eigendecay.

```
0:\(k_{G}\), \(\mathcal{X}\)
1:initialize\(\mathcal{D}=\{\}\)
2:for\(t=1,\ldots,T\)do
3: Select \(x_{t}=\arg\max_{x\in\mathcal{X}}\mu_{t-1}(x)+\beta\sigma_{t-1}(x)\)
4: Observe \(y_{t}=f(x_{t})+\eta_{t}\)
5: Append \((x_{t},y_{t})\) to \(\mathcal{D}\)
6: Update \(\mu_{t},\sigma_{t}\) from Eqs. 9, 10
7:endfor
8:return\(\hat{x}^{\mathrm{UCB}}_{\mathrm{opt}}=x_{T}\) ```

**Algorithm 1** Max. variance reduction [42]

In Figure 2, we provide a visual example of the impact of using \(k_{G}\) in Bayesian optimisation. We run UCB on a group-invariant objective with the standard and invariant kernels. With the non-invariant kernel \(k\), the algorithm spends many samples querying suboptimal points with similar objective values. In contrast, using the invariant kernel \(k_{G}\) allows the algorithm to identify suboptimal regions across the parameter space with only a handful of samples, resulting in significantly faster convergence.

### Upper bounds on sample complexity

In this section, let \(\mathcal{X}=\mathbb{S}^{d-1}\subset\mathbb{R}^{d}\) and let \(G\) be a finite subgroup of the orthogonal group, \(O(d)\), equipped with its natural action on \(\mathcal{X}\). Let \(k\) be a simultaneously invariant kernel on \(\mathbb{S}^{d-1}\), and assume that the eigenvalues of the associated integral operator decay at a polynomial rate \(\mu_{k}=\mathcal{O}(k^{-\beta_{p}^{*}})\), for some constant \(\beta_{p}^{*}>1\). Many common kernels exhibit this property, such as the standard Matern kernel on \(\mathbb{R}^{d}\). For GPs defined intrinsically on Riemannian manifolds [45], the corresponding Matern kernel also exhibits polynomial eigendecay [9]. We impose the following assumption on the spectra of elements in the group \(G\), which is in line with the assumptions and spectral decay rates in [4].

**Assumption 1**.: _All elements \(\sigma\in G\) satisfy the following spectral property:_

\[m_{\lambda}<d-4\quad if\quad\lambda\neq\pm 1,\] (12)

_for all \(\lambda\in\mathrm{Spec}(\sigma)\), where \(m_{\lambda}\) is the multiplicity of \(\lambda\)._

This assumption excludes certain groups in low dimensions, but we expect it can be relaxed through tighter analysis. In fact, our experiments for \(d=2,3\) show it is essential as a theoretical artefact only.

Nevertheless, Equation (12) holds for all finite orthogonal groups in dimension \(d\geq 10\) (see Lemma 1) as well as for many groups of lower dimension; for example, groups for which \(m_{\lambda}=1\), so that only distinct eigenvalues are present in the spectra. Under this assumption, we have the following upper bound on sample complexity.

**Theorem 1** (Upper bound on sample complexity of invariance-aware BO).: _Let \(k\) be a polynomial eigendecay kernel on \(\mathbb{S}^{d-1}\) which is simultaneously invariant w.r.t. the orthogonal group \(O(d)\), and let \(G\leq O(d)\) be a subgroup of \(O(d)\) satisfying Assumption 1. Let \(k_{G}\) be the totally invariant kernel, as defined in Proposition 1. Consider noisy bandit optimisation of \(f\in\mathcal{H}_{k_{G}}\), where \(\|f\|_{\mathcal{H}_{k_{G}}}<B\) and \(f:\mathbb{S}^{d-1}\to\mathbb{R}\). Then, the maximum information gain (MIG) for the totally invariant kernel after \(T\) rounds, \(\gamma_{T}^{G}\), is bounded by_

\[\gamma_{T}^{G}=\tilde{\mathcal{O}}\Big{(}\tfrac{1}{|G|}\Gamma^{ \frac{d-1}{\beta_{T}}}\Big{)},\] (13)

_where \(\tilde{\mathcal{O}}(\cdot)\) hides logarithmic factors._

_Moreover, for the MVR algorithm with invariant kernel \(k_{G}\) to achieve expected simple regret \(\mathbb{E}[r_{T}]<\epsilon\), it must hold that_

\[T=\tilde{\mathcal{O}}\left(\left(\tfrac{1}{|G|}\right)^{\frac{ \partial_{T}^{\star}}{\partial_{T}^{\star}-d+1}}\epsilon^{-\frac{2\beta_{T}^{ \star}}{\beta_{T}^{\star}-d+1}}\right).\] (14)

_For the Matern kernel, this corresponds to_

\[T=\tilde{\mathcal{O}}\left(\left(\tfrac{1}{|G|}\right)^{\frac{ 2\nu+d-1}{2\nu}}\epsilon^{-\frac{2\nu+d-1}{\nu}}\right).\] (15)

Proof.: See Appendix A.2. 

Our upper bound demonstrates that incorporating the group invariance into the kernel is guaranteed to result in sample efficiency improvements that increase with the size of the group, due to the \(\frac{1}{|G|}\) factor. Our result for the MIG \(\gamma\) of invariant kernels is of independent interest, as it is applicable to a wide range of scenarios. Notably, the linear \(\frac{1}{|G|}\) improvement in the information gain from Equation (13) resembles the linear \(\frac{1}{|G|}\) improvement in generalisation error in the invariant kernel ridge regression setting [3]. Intuitively, this reflects how the MIG criterion is determined solely by the ability of the kernel to approximate the objective function, and is agnostic to the algorithm used in optimisation.

In our proof, we begin with kernels on \(\mathbb{S}^{d-1}\) that are simultaneously invariant w.r.t. the full orthogonal group \(O(d)\), and use Proposition 1 to construct kernels that totally invariant kernels w.r.t finite subgroups \(G\leq O(d)\). Simultaneously \(O(d)\)-invariant kernels on \(\mathbb{S}^{d-1}\) are in fact dot-product kernels, which allows us to use standard versions of Mercer's theorem in terms of known spectral decompositions. If an alternative decomposition is known for a particular \(k\), then it is sufficient that \(k\) instead satisfies the weaker condition of simultaneous invariance w.r.t. \(G\) only.

### Lower bounds on sample complexity

In the next theorem, we present a lower bound on sample complexity which nearly matches the upper bound in its dependence on \(|G|\).

**Theorem 2** (Lower bound on sample complexity of invariance-aware BO with Matern kernels).: _Let \(\mathcal{X}\) be one of \([0,1]^{d}\) or \(\mathbb{S}^{d}\). Let \(k\) be the standard Matern kernel on \(\mathbb{R}^{d}\) with smoothness \(\nu\), and \(G\) be a finite group of isometries acting on \(\mathcal{X}\). Let \(k_{G}\) be the totally invariant kernel constructed from \(k\) and \(G\) according to Proposition 1. Consider noisy bandit optimisation of \(f\in\mathcal{H}_{k_{G}}\), \(f:\mathcal{X}\to\mathbb{R}\), where the observations are corrupted by \(\sigma\)-sub-Gaussian noise. Then, for sufficiently small \(\frac{\epsilon}{B}\),there exists a function family \(\bar{\mathcal{F}}_{\mathcal{X}}(\epsilon,B)\) such that the number of samples required for an algorithm to achieve simple regret less than \(\epsilon\) for all functions in \(\bar{\mathcal{F}}_{\mathcal{X}}\) with probability at least \(1-\delta\) is bounded by:_

\[T=\Omega\left(\left(\tfrac{1}{|G|}\right)^{\frac{\nu+d-1}{\nu} }\epsilon^{-\frac{2\nu+d-1}{\nu}}\sigma^{2}B^{\frac{d-1}{\nu}}\log\frac{1}{ \delta}\right),\] (16)

Proof.: See Appendix A.3.

Our lower bound guarantees that algorithms that are run for a number of sample less than \(T\) fail to be \(\epsilon\)-optimal with high probability. At fixed \(T\), there is a trade-off between this probability and the order of the group. However, when combined with the upper bound our results imply that as the order of the group increases, the probability that an algorithm is \(\epsilon\)-optimal after a fixed number of iterations also increases.

We recover the previously known lower and upper bounds on sample complexity for non-invariant optimisation from [12] and [42] respectively by setting \(|G|=1\). We also note that there is a gap between our bounds, as the exponent of \(\frac{1}{|G|}\) in Equation (15) is \(1+\frac{d-1}{\nu}\) but is \(1+\frac{d-1}{2\nu}\) in Equation (16). A likely reason for this gap is in the lower bound, which uses a packing argument for the support sets of functions in the RKHS. While in the absence of the group action, the lower bound and upper bound agree, it is not clear a priori how to achieve the tightest packing of invariant functions.

To achieve the largest improvement in sample complexity, one would like to choose the largest possible \(G\) when constructing \(k_{G}\). However, as we will see in our experimental section, there is a computational tradeoff between the sample complexity and the computation of \(S_{G}\), as in the case of very large groups the sums over the group elements becomes a computational bottleneck.

## 4 Experiments

We demonstrate the performance gains from incorporating known invariances in a number of synthetic and real-world optimisation tasks. Previous work has demonstrated the sample efficiency improvements from applying invariance-aware algorithms to regression tasks [3; 33; 38]; our experiments complement the literature by observing the corresponding improvements in optimisation tasks. Code for our experiments and implementations of invariant kernels can be found on GitHub1.

Footnote 1: github.com/theo-brown/invariantkernels and github.com/theo-brown/bayesopt_with_invariances

### Synthetic experiments

For each of our synthetic experiments, our goal is to find the maximum of a function drawn from an RKHS with a group-invariant isotropic Matern-\(\nicefrac{{5}}{{2}}\) kernel. The groups we consider act by permutation of the coordinate axes, and therefore include reflections and discrete rotations. For PermInv-2D (Figure 0(a)) and PermInv-6D, the kernel of the GP is invariant to the full permutation group which acts on \(\mathbb{R}^{2}\) and \(\mathbb{R}^{6}\) respectively by permuting the coordinates. For CyclInv-3D it is invariant to the cyclic group acting on \(\mathbb{R}^{3}\) by permuting the axes cyclically (Figure 0(b)). We provide the learner with the true kernel and observation noise variance to eliminate the effect of online hyperparameter learning. For more detail on our experiments, see Appendix B.

In Figure 3, we show that the algorithm's performance on invariant functions is improved by using an invariant kernel. In all cases, the performance of the invariance-aware algorithm significantly outperforms the baselines, converging to the optimum with many fewer samples. Notably, in the PermInv-6D task, the baseline algorithms fail to find the true optimum, whereas the permutation-invariant versions do so without difficulty. We also note that the performance improvement increases with increasing dimension and group size, as predicted by our regret bounds.

For the PermInv tasks, we provide an additional baseline: constrained Bayesian optimisation (CBO, [16]). In this case, we constrain the search space of the acquisition function optimiser to the fundamental region of the group's action. For arbitrary groups, finding an analytical formula for this region can be difficult, particularly when the dimension of the domain is high and the group is complicated; however, it is straightforward in the case of the full permutation group. We find that constrained BO performs worse than our method, and in high dimensions is identical to standard (non-invariant) BO. Intuitively, this matches our expectations, as in CBO the kernel is not aware of all of the structure that the RKHS elements possess - that is, while the _domain of exploration_ is restricted, the _function class_ is not. Observations in CBO contribute no information across the boundaries of the region, unlike in the invariant kernel case, and so the performance improvement of CBO is upper bounded by \(\frac{1}{|G|}\).

For the PermInv-6D task, we also consider the case where the full invariance is not fully known to the learner; that is, the kernel is not totally invariant w.r.t. the full group, but only a _subgroup_. Our results show that incorporating any amount of the true invariance results in performance improvements. In this case, the true function is invariant to the full permutation group in \(6\)D (\(720\) elements). We evaluate the performance of the full invariant kernel, the non-invariant kernel, and two subgroup invariant kernels. The subgroups we consider consist of 'block' permutations, which involve reordering the coordinate indices in chunks of length \(2\) and \(3\) (leading to groups with \(6\) and \(2\) elements respectively). For both algorithms, we see that all kernels that incorporate invariance outperform the non-invariant kernel, even when the difference in the group size considered is large.

### Extension: quasi-invariance using additive kernels

In real-world scenarios, it is possible that the underlying function only exhibits _approximate_ symmetry. For example, in the fusion application from Section 4.3, the launchers may have slightly different properties and are no longer interchangeable. A 2D example of this 'quasi-invariance' is given in the first column of Figure 4. Although the large-scale features (such as the layout of the peaks) remain approximately symmetric, invariance is not maintained in the detail (the values and shapes of the peaks).

We model quasi-invariance by introducing an additive non-invariant disturbance to the target function. The function can then be considered as belonging to the RKHS \(\mathcal{H}_{(1-\varepsilon)k_{G}+\varepsilon k^{\prime}}\), where \(k_{G}\) is a \(G\)-invariant kernel, \(k^{\prime}\) is a non-invariant kernel, and \(\varepsilon\) sets the degree of deviation from invariance. In our experiments, we observe that performing BO with the fully invariant kernel still results in significant performance improvements over the non-invariant kernel when \(\varepsilon\) is small, achieving performance comparable to BO with the kernel of the function's RKHS. As this is a kind of model misspecification, we refer the reader to [5] for a theoretical discussion on the performance of BO with misspecified kernels.

### Real-world application: nuclear fusion scenario design

In this section, we use invariance-aware BO to efficiently solve a design task from fusion engineering.

The leading concept for a controlled fusion power plant is the _tokamak_, a device that confines a high-temperature plasma using powerful magnetic fields. Finding steady-state configurations for tokamak plasma that are simultaneously robustly stable and high-performance is a challenging task. Moreover, evaluating the plasma operating point requires high-fidelity multi-physics simulations, which take several hours to evaluate one configuration due to the complexity and characteristic timescales of the physics involved [34]. Iterating on a design requires many such simulations, which has led to an increased focus on highly sample-efficient algorithms for tokamak design optimisation [13; 32; 10; 31].

We consider a scalarised version of the multi-objective optimisation task presented in [10], in which the goal is to shape the output of a plasma current actuator to maximise various performance and ro

Figure 3: Regret performance of invariant UCB and MVR algorithms across 3 different tasks; lower is better. Non-invariant kernels (blue) are outperformed by the full group invariant kernel (red) as well as partially specified (subgroup-invariant) kernels (green and yellow). For the permutation invariant function, the search space of standard BO can be constrained by the fundamental domain of the group (purple), but this performs worse than the invariant kernel. Mean \(\pm\) s.d., 32 seeds.

bustness criteria based on the so-called _safety factor_ profile. Using a weighted sum, we reduce the vector objective to \([0,1]\), with component weights selected to represent the proposed SPR45 design of the STEP tokamak [39, 31]. As the objective components are in direct competition, the highest achievable scores are around 0.7 - 0.8, while the lowest-scoring converged solutions achieve around 0.4 - 0.5.

In previous work, the actuator output was represented by arbitrary 1D functions (piecewise linear [31] and Bezier curves [10]). However, in practice the actuator will have a finite number of 'launchers' that drive current in a localised region, and so will be unable to accurately recreate arbitrary profiles. In contrast to previous work, we directly optimise a parameterisation that reflects the actuator's real capabilities: a sum of \(12\) Gaussians, where each models a launcher targeted at a different point in the plasma cross-section (Figure 4(a)). In this setting, the objective function is \(f:\mathbb{R}^{12}\rightarrow\mathbb{R}\), where the input is the normalised radial coordinate of the 12 launchers. As all of the launchers have identical behaviour, the objective is invariant to permutations. However, the full invariant kernel is too costly to compute (\(|G|=12!=479\times 10^{6}\)), so we instead use a partially specified kernel (3-block invariant, \(|G|=4!=24\)).

An additional challenge of this task is that large regions of parameter space are _infeasible_, corresponding to simulations that will not converge to a steady-state solution and will therefore be

Figure 4: Performance of MVR and UCB on _quasi-invariant_ functions. Regret shown for the noninvariant kernel \(k^{\prime}\) (standard, blue), the invariant kernel \(k_{G}\) (invariant, green), and the quasi-invariant kernel \(k_{G}+\varepsilon k^{\prime}\) (additive, red). In all cases, the invariant kernel performs almost as well as the true quasi-invariant kernel.

Figure 5: Nuclear fusion application: optimising safety factor by adjusting current drive actuator. In (a), observe that the order of launchers can be permuted without changing the total profile. Incorporating this invariance into the kernel of the UCB algorithm achieves improved performance (b).

impossible to observe. Selecting an appropriate method of dealing with this situation can impact the optimiser's performance, and is an open area of research. In this work, we choose to assign a fixed low score to failed observations for simplicity. As MVR is purely exploratory it has no mechanism that discourages querying points from infeasible regions, leading to many failed observations. UCB is a more natural choice in this setting, as the \(\beta\) parameter introduces a balance between querying unknown regions (that could be infeasible) and exploiting known high-scoring regions.

In Figure 4(b), we see that the invariant version of UCB achieves significantly improved performance compared to the non-invariant version. In fact, the non-invariant algorithm totally fails to find the small region of high-performance solutions, and instead queries many suboptimal profiles that are similar under the action of permutations. The high sample efficiency of the invariant kernel method will enable tokamak designers to iterate on designs faster, with fewer calls to expensive simulations.

## 5 Limitations

Although the number of terms involved in computing the totally invariant kernel only scales linearly with the group size (Equation (5)), this cost can still become a bottleneck for very large groups; for example, the size of the permutation group scales with the factorial of the dimension, leading to \(|G|=24\) for \(d=4\), but \(|G|=479\times 10^{6}\) for \(d=12\). In Figure 6 we compare the compute and memory costs of performing a marginal log likelihood fit of the GP hyperparameters for each of the kernels in the previous section, given 100 observations from the PermInv-6D function. The standard, \(3\)-block invariant, and \(2\)-block invariant kernel are faster than the fully invariant kernel by a factor of \(2\). As we observed that the \(2\)- and \(3\)-block invariant kernels still achieve significantly improved sample efficiency over the standard kernel (Figure 3), we propose that subgroup approximations of the full invariance group can be used as a low-cost alternative when the group size is large, reducing computational cost while maintaining improved performance. Finally, we note that our invariant kernel method scales significantly more favourably than data augmentation, both in terms of memory and compute.

## 6 Conclusion

We have developed new upper bounds on the sample complexity in Bayesian optimization with invariant kernels, highlighting the advantages gained from symmetries that hold for a broad range of practical groups. We also derived novel lower bounds by constructing and quantifying members of the RKHS of invariant functions, extending this approach to the hyperspherical domain and providing a framework for other Riemannian manifolds. Empirically, our experiments show that even partial invariance can effectively improve performance, which is crucial when full invariance is unavailable or computationally intensive. Additionally, we applied these methods to a real-world problem in nuclear fusion engineering, achieving superior outcomes with fewer samples compared to standard methods.

**Broader Impact.** The incorporation of symmetries in Bayesian optimization can have a profound impact on science and engineering. By exploiting symmetrical properties in various applications, such as nuclear engineering, material and drug design, and robotic control, it can reduce the number of samples needed to achieve (near) optimal solutions, thus accelerating research and development processes.

Figure 6: Effect of group size on cost of data augmentation and invariant kernel methods. Benchmark task is to fit a 6D GP with the given kernel to 100 random datapoints from PermInv-6D. Shown are results from 100 seeds, 64 repeats per seed, performed on one NVIDIA V100-32GB GPU using BoTorch. Invariant kernels are (a) more memory efficient than data augmentation, and (b) can be computed faster. Incorporating full invariance via data augmentation exceeds the GPU memory.

[MISSING_PAGE_FAIL:11]

* [22] Parnian Kassraie and Andreas Krause. "Neural contextual bandits without regret". In: _Proceedings of the 25th International Conference on Artificial Intelligence and Statistics_. Vol. 151. 2022, pp. 240-278.
* [23] Parnian Kassraie, Andreas Krause, and Ilija Bogunovic. "Graph neural network bandits". In: _Advances in Neural Information Processing Systems_. Vol. 35. 2022, pp. 34519-34531.
* [24] Danial Khatamsaz et al. "A physics-informed Bayesian optimization approach for material design: application to NiTi shape memory alloys". In: _npj Computational Materials_ 9.1 (2023), p. 221.
* [25] Jungtaek Kim et al. "Bayesian optimization with approximate set kernels". In: _Machine Learning_ 110 (2021), pp. 857-879.
* [26] Johannes Kirschner et al. "Tuning particle accelerators with safety constraints using Bayesian optimization". In: _Physical Review Accelerators and Beams_ 25 (6 2022), p. 062802.
* [27] Imre Risi Kondor. "Group theoretical methods in machine learning". PhD thesis. Columbia University, 2008.
* [28] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. "ImageNet Classification with deep convolutional neural networks". In: _Advances in Neural Information Processing Systems_. Ed. by F. Pereira et al. Vol. 25. 2012.
* [29] Yann Lecun et al. "Learning algorithms for classification: a comparison on handwritten digit recognition". In: _Neural networks: The statistical mechanics perspective_. World Scientific, 1995, pp. 261-276.
* [30] Benjamin Letham et al. "Constrained Bayesian Optimization with noisy experiments". In: _Bayesian Analysis_ 14.2 (2019), pp. 495-519.
* [31] S. Marsden et al. "Using genetic algorithms to optimise current drive in STEP". In: _Europhysics Conference Abstracts_. Vol. 46A. 2022, P2b.123.
* [32] Viraj Mehta et al. "Exploration via planning for information about the optimal trajectory". In: _Advances in Neural Information Processing Systems_. Vol. 35. 2022, pp. 28761-28775.
* [33] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. "Learning with invariances in random features and kernel models". In: _Conference on Learning Theory_. 2021, pp. 3351-3418.
* [34] Michele Romanelli et al. "JINTRAC: a system of codes for integrated simulation of tokamak scenarios". In: _Plasma and Fusion Research_ 9 (2014), pp. 3403023-3403023.
* [35] Jonathan Scarlett, Ilija Bogunovic, and Volkan Cevher. "Lower bounds on regret for noisy Gaussian process bandit optimization". In: _Proceedings of the 2017 Conference on Learning Theory_. Vol. 65. 2017.
* [36] Shubhanshu Shekhar and Tara Javidi. "Gaussian process bandits with adaptive discretization". In: _Electronic Journal of Statistics_ 12 (2018), pp. 3829-3874.
* [37] Niranjan Srinivas et al. "Gaussian process optimization in the bandit setting: no regret and experimental design". In: _Proceedings of the 27th International Conference on Machine Learning_. 2010, pp. 1015-1022.
* [38] Behrooz Tahmasebi and Stefanie Jegelka. "The exact sample complexity gain from invariances for kernel tegression". In: _Advances in Neural Information Processing Systems_. Vol. 36. 2023, pp. 55616-55646.
* [39] Emmi Tholerus et al. "Flat-top plasma operational space of the STEP power plant". In: _Nuclear Fusion_ 64 (Aug. 2024).
* [40] Atsushi Togo and Isao Tanaka. _Spglib: a software library for crystal symmetry search_. https://github.com/spglib/spglib. 2018. eprint: arXiv:1808.01590.
* [41] Sattar Vakili, Kia Khezeli, and Victor Picheny. "On information gain and regret bounds in Gaussian process bandits". In: _International Conference on Artificial Intelligence and Statistics_. Vol. 130. 2021, pp. 82-90.
* [42] Sattar Vakili et al. "Optimal order simple regret for Gaussian process bandits". In: _Advances in Neural Information Processing Systems_ 34 (2021), pp. 21202-21215.
* [43] Michal Valko et al. "Finite-time analysis of kernelised contextual bandits". In: _arXiv:1309.6869_ (2013).
* [44] Elise Van der Pol et al. "MDP homomorphic networks: group symmetries in reinforcement learning". In: _Advances in Neural Information Processing Systems_. Vol. 33. 2020, pp. 4199-4210.

* [45] Peter Whittle. "Stochastic processes in several dimensions". In: _Bulletin of the International Statistical Institute_ 40.2 (1963), pp. 974-994.
* [46] Mark van der Wilk et al. "Learning invariances using the marginal likelihood". In: _Advances in Neural Information Processing Systems_. Vol. 31. 2018, pp. 9960-9970.
* [47] Christopher KI Williams and Carl Edward Rasmussen. _Gaussian processes for machine learning_. 3. MIT Press Cambridge, MA, 2006.
* [48] Yunxing Zuo et al. "Accelerating materials discovery with Bayesian optimization and graph deep learning". In: _Materials Today_ 51 (2021), pp. 126-135.

Proofs

### Proof of Proposition 1 (RKHS of invariant functions)

The kernel \(k\) defines a reproducing kernel Hilbert space, \(\mathcal{H}_{k}\), with inner product \(\langle\cdot,\cdot\rangle_{\mathcal{H}_{k}}\) and norm \(\|f\|_{\mathcal{H}_{k}}=\sqrt{\langle f,f\rangle_{\mathcal{H}_{k}}}\) for \(f\in\mathcal{H}_{k}\).

Recall from Equation (4) that \(S_{G}(f)=\frac{1}{|G|}\sum_{\sigma\in G}f\circ\sigma\).

We begin by showing that \(S_{G}\) is continuous and bounded. Observe that simultaneous invariance determines the following property of the representer elements \(k_{x}(\cdot)=k(x,\cdot)=k(\cdot,x)\):

\[k_{x}\circ\sigma=k_{\sigma^{-1}(x)}\quad\forall\sigma\in G.\] (17)

Now observe that:

\[||k_{x}\circ\sigma||^{2}=\langle k_{x}\circ\sigma,k_{x}\circ\sigma\rangle= \langle k_{\sigma^{-1}(x)},k_{\sigma^{-1}(x)}\rangle=k(\sigma^{-1}(x),\sigma^ {-1}(x))=k(x,x)=||k_{x}||^{2}\] (18)

If \(f=\sum_{i=1}^{n}k_{x_{i}}\), we have, similarly:

\[||f\circ\sigma||^{2}=||\sum_{i=1}^{n}k_{\sigma^{-1}(x_{i})}||^{2}=\sum_{i=1}^ {n}\sum_{j=1}^{n}k(\sigma^{-1}(x_{i}),\sigma^{-1}(x_{j}))=\sum_{i=1}^{n}\sum_{ j=1}^{n}k(x_{i},x_{j})=||f||^{2}\] (19)

Now, let \(f_{n}\to f\) be a Cauchy sequence, where each \(f_{n}\) lies in the span of the \(k_{x}\) elements. We have

\[||f\circ\sigma||=||\lim f_{n}\circ\sigma||=\lim||f_{n}\circ\sigma||=\lim||f_{n }||=||f||\] (20)

Therefore,

\[||S_{G}f||\leq\frac{1}{|G|}\sum_{\sigma\in G}||f\circ\sigma||\leq||f||\] (21)

so \(S_{G}\) is continuous, with \(||S_{G}||\leq 1\).

We now verify that the image of \(S_{G}\), denoted as \(\mathrm{Im}(S_{G})\), contains precisely the \(G\)-invariant functions (Equation (1)). For \(f\in\mathcal{H}_{k},\sigma\in G\), we have

\[S_{G}(f)\circ\sigma =\frac{1}{|G|}\sum_{\tau\in G}(f\circ\tau)\circ\sigma\] (22) \[=\frac{1}{|G|}\sum_{\tau\in G}f\circ(\tau\circ\sigma)\] (23) \[=\frac{1}{|G|}\sum_{\sigma^{\prime}\in G}f\circ\sigma^{\prime}\] (24) \[=S_{G}(f),\] (25)

where Equation (23) follows by associativity of \(G\), and Equation (24) follows by letting \(\sigma^{\prime}:=\tau\circ\sigma\) and using the fact that \(G\) is closed under the \(\circ\) operator. Hence, \(S_{G}(f)\) is invariant to \(\sigma\in G\) (Equation (1)).

Next, we show that the image of \(S_{G}\) is a reproducing kernel Hilbert space with kernel \(k_{G}\) (Equation (5)). First note that \(S_{G}\) is a projection, as applying \(S_{G}\) twice gives

\[S_{G}(S_{G}(f)) =\frac{1}{|G|}\sum_{\sigma\in G}S_{G}(f)\circ\sigma\] (26) \[=\frac{1}{|G|}\sum_{\sigma\in G}S_{G}(f)\] (27) \[=S_{G}(f),\] (28)

where Equation (27) follows by applying Equation (25).

Moreover, \(S_{G}\) is self-adjoint. Let \(f\in\mathcal{H}_{k}\) be an element in the span of the representer elements \(k_{x}\), such that \(f(\cdot)=\sum_{i=1}^{n}\alpha_{i}k_{x_{i}}(\cdot)\), and let \(g\in\mathcal{H}_{k}\). Then,

\[\langle S_{G}(f),g\rangle_{\mathcal{H}_{k}} =\frac{1}{|G|}\left\langle\sum_{\sigma\in G}f\circ\sigma,g\right\rangle _{\mathcal{H}_{k}}\] (29) \[=\frac{1}{|G|}\sum_{i=1}^{n}\sum_{\sigma\in G}\langle\alpha_{i}k _{\sigma^{-1}(x_{i})},g\rangle_{\mathcal{H}_{k}}\] (30) \[=\frac{1}{|G|}\sum_{i=1}^{n}\sum_{\sigma\in G}\alpha_{i}g(\sigma^ {-1}(x_{i}))\] (31) \[=\frac{1}{|G|}\sum_{i}\sum_{\sigma}\langle\alpha_{i}k_{x_{i}},g \circ\sigma^{-1}\rangle_{\mathcal{H}_{k}}\] (32) \[=\frac{1}{|G|}\sum_{\sigma}\langle f,g\circ\sigma\rangle_{ \mathcal{H}_{k}}\] (33) \[=\langle f,S_{G}(g)\rangle_{\mathcal{H}_{k}},\] (34)

where Equation (30) follows by expanding \(f\) and recalling that for simultaneously invariant kernels \(k_{x}\circ\sigma=k(\sigma^{-1}(x),\cdot)\), Equation (31) follows by using the reproducing property of \(k_{\sigma(x)}\), Equation (32) uses the decomposition of \(g\circ\sigma\) in terms of the reproducing element \(k_{x_{i}}\), and Equation (33) uses the definition of \(f\).

Now, as before, let \(f_{n}\to f\) be a Cauchy sequence, where \(f_{n}\) lies in the span of the \(k_{x}\) elements. We have

\[\langle\lim f_{n},S_{G}(g)\rangle_{\mathcal{H}_{k}}=\lim\langle f_{n},S_{G}(g )\rangle=\lim\langle S_{G}(f_{n}),g\rangle=\langle\lim S_{G}(f_{n}),g\rangle= \langle S_{G}(\lim f_{n}),g\rangle_{\mathcal{H}_{k}},\] (35)

by continuity of the inner product. Hence, \(S_{G}\) is self-adjoint for all \(f,g\in\mathcal{H}_{k}\).

As \(S_{G}\) is a projection (Equation (28)) and is self-adjoint \(\mathrm{Im}(S_{G})\) is a closed subspace of \(\mathcal{H}_{k}\). Now, denoting the evaluation functional \(L_{x}:\mathcal{H}_{k}\to\mathbb{R}\), \(L_{x}(f)=f(x)\), note that since \(\mathrm{Im}(S_{G})\) is closed, then if \(L_{x}\) was bounded on \(\mathcal{H}_{k}\), it remains bounded on \(\mathrm{Im}(S_{G})\). Hence \(\mathrm{Im}(S_{G})\) is itself a RKHS, which we'll denote as \(\mathcal{H}^{\prime}\).

We will now relate \(k_{\mathcal{H}^{\prime}}\), the kernel associated with \(\mathcal{H}^{\prime}\), to \(k\) and \(G\). Note that, for \(f\in\mathcal{H}^{\prime}\),

\[f(x)=\langle f,k_{x}\rangle=\langle S_{G}(f),k_{x}\rangle=\langle f,S_{G}(k_{ x})\rangle.\] (36)

So \(S_{G}(k_{x})\) is the reproducing element for \(\mathcal{H}^{\prime}\), giving us

\[k_{\mathcal{H}^{\prime}}(x,y)=\langle S_{G}(k_{y}),S_{G}(k_{x})\rangle=\langle S _{G}(k_{y}),k_{x}\rangle=\frac{1}{|G|}\sum_{\sigma\in G}k(\sigma(x),y),\] (37)

which recovers the formula for the kernel in Equation 5.

Finally, note that \(\langle S_{G}(k_{y}),S_{G}(k_{x})\rangle\) recovers the more general "double sum" formula for producing a totally invariant kernel out of a kernel which is not necessarily simultaneously invariant [18, 33]:

\[k_{G}(x,y)=\frac{1}{|G|^{2}}\sum_{\sigma,\tau\in G}k(\sigma(x),\tau(y)).\] (38)

### Proof of Theorem 1: upper bound on sample complexity

In this section we derive Theorem 1, the upper bound on regret for Bayesian optimisation algorithms that incorporate invariance. The main elements of this proof were introduced in [41]. Our derivation is similar to the one in section C.1 of [22], but extends that work to more general groups and kernels.

In Appendix A.2.1, we present a concise summary of already known bounds on the information gain of dot-product kernels with polynomial eigendecay on the hypersphere. In Appendix A.2.2, we will adapt this argument for information gain of _totally invariant_ versions of these kernels. We then use the bounds on information gain to bound the regret in Appendix A.2.3.

We begin with the following remark which we use throughout this section. Namely, with the assumptions of Theorem 1, a kernel \(k\) on \(\mathbb{S}^{d-1}\) which is simultaneously invariant with respect to the whole of \(O(d)\) is a dot-product kernel. To see this, choose \(r\in[-1,1]\) and pick \(x_{r},y_{r}\) such that \(\langle x_{r},y_{r}\rangle=r\) and define \(\kappa(r)=k(x_{r},y_{r})\). Observe that \(\kappa\) is well defined, as for any other choice of \((x_{r}^{\prime},y_{r}^{\prime})\) there is a (non-unique) isometry \(M_{(x_{r},y_{r})}^{(x_{r}^{\prime},y_{r}^{\prime})}\) such that

\[x_{r}^{\prime}=M_{(x_{r},y_{r})}^{(x_{r}^{\prime},y_{r}^{\prime })}x_{r}\] (39) \[y_{r}^{\prime}=M_{(x_{r},y_{r})}^{(x_{r}^{\prime},y_{r}^{\prime })}y_{r}\] (40)

and therefore \(k(x_{r},y_{r})=k(x_{r}^{\prime},y_{r}^{\prime})\). Hence, throughout this section we can consider dot-product kernels only.

#### a.2.1 Information gain for kernels on the hypersphere

Consider the hyperspherical domain \(\mathbb{S}^{d-1}=\{x\in\mathbb{R}^{d}\;\mathrm{s.t.}\;\|x\|=1\}\). The Mercer decomposition of a dot-product kernel on the hypersphere is

\[k(x^{T}x^{\prime})=\sum_{k=0}^{\infty}\mu_{k}\sum_{i=1}^{N(d,k)}Y_{i,k}(x)Y_{i,k}(x^{\prime}),\] (42)

where \(Y_{i,p}\) is the \(i\)-th spherical harmonic polynomial of degree \(k\) and \(N(d,k)\) is the number of spherical harmonics of degree \(k\) on \(\mathbb{S}^{d-1}\), given by

\[N(d,k)=\frac{2k+d-2}{k}\binom{k+d-3}{d-2},\] (43)

Asymptotically in terms of \(k\) we have

\[N(d,k)=\Theta\left(k^{d-2}\right).\] (44)

In this representation, \(\mu_{k}\) can be interpreted as an eigenvalue that is repeated \(N(d,k)\) times, with corresponding eigenfunctions \(Y_{1,k},\ldots,Y_{N(d,k),k}\). We let \(\psi\) be the maximum absolute value of the eigenfunctions, such that \(|Y_{i,k}(x)|\leq\psi\,\forall x\in\mathbb{S}^{d-1}\).

We project onto a subspace of eigenfunctions corresponding to the first \(D\) eigenvalues. Let \(K\) be the number of distinct values in the first \(D\) eigenvalues. It follows that

\[D =\sum_{k=0}^{K}N(d,k)\] (45) \[c_{1}\sum_{k=0}^{K}k^{d-2}\leq D \leq c_{2}\sum_{k=0}^{K}k^{d-2}\] (46) \[C_{1}\frac{K^{d-1}}{d-1}\leq D \leq C_{2}\frac{K^{d-1}}{d-1}\] (47) \[D =\Theta\left(K^{d-1}\right)\] (48)where Equation (46) follows by substituting Equation (44), and Equation (47) follows by bounding the sum with an integral, and \(c_{i},C_{i}\) are constants that ensure the asymptotic equalities hold.

Truncating the eigenvalues at \(D\) leaves a tail with mass

\[\delta_{D}\leq\sum_{i=D+1}^{\infty}\lambda_{i}\psi^{2} =\psi^{2}\sum_{k=K}^{\infty}\mu_{k}N(d,k)\] (49) \[\leq\psi^{2}C_{1}\sum_{k=K}^{\infty}\mu_{k}k^{d-2}\] (50) \[\leq\psi^{2}C_{2}\sum_{k=K}^{\infty}k^{-\beta_{p}^{*}+d-2}\] (51) \[\leq\psi^{2}C_{3}K^{-\beta_{p}^{*}+d-1}\] (52) \[\leq\psi^{2}C_{4}D^{\frac{-\beta_{p}^{*}+d-1}{d-1}}\] (53)

where Equation (50) follows by substituting Equation (44), Equation (51) follows if the _distinct_ eigenvalues have polynomial decay rate, \(\mu_{k}=\mathcal{O}(k^{-\beta_{p}^{*}})\);2, Equation (52) follows by bounding the sum with an integral; Equation (53) follows because \(D=\Theta(K^{d-1})\), and hence \(K=\Theta(D^{\frac{1}{d-1}})\).

Footnote 2: Matérn kernels on \(\mathbb{S}^{d-1}\) satisfy this property with \(\beta_{p}^{*}=2\nu+d-1\)[9].

Theorem 3 in [41] states that

\[\gamma_{T}\leq\frac{D}{2}\log\left(1+\frac{\bar{k}T}{\tau D}\right)+\frac{T \delta_{D}}{2\tau},\] (54)

where \(|k(x,x^{\prime})|\leq\bar{k}\;\forall x\) and \(\tau\) is the observation noise variance. Substituting in Equation (53) gives

\[\gamma_{T}\leq\frac{D}{2}\log\left(1+\frac{\bar{k}T}{\tau D} \right)+\frac{T\psi^{2}C_{4}}{2\tau}D^{\frac{-\beta_{p}^{*}+d-1}{d-1}}\] (55)

We have freedom in setting \(D\). We choose it so that the first term dominates, by setting

\[\frac{D}{2}\log\left(1+\frac{\bar{k}T}{\tau}\right)\geq\frac{T \psi^{2}C_{4}}{2\tau}D^{\frac{-\beta_{p}^{*}+d-1}{d-1}}\] (56)

This is satisfied by

\[D=\left[\left(\frac{T\psi^{2}C_{4}}{\tau\log\left(1+\frac{\bar{ k}T}{\tau}\right)}\right)^{\frac{d-1}{\beta_{p}^{*}}}\right]\] (57)

which gives:

\[\gamma_{T}\leq\left(\left(\frac{T\psi^{2}C_{4}}{\tau\log\left(1+ \frac{\bar{k}T}{\tau}\right)}\right)^{\frac{d-1}{\beta_{p}^{*}}}+1\right)\log \left(1+\frac{\bar{k}T}{\tau}\right)\] (58)

where we have used the fact that \(\lceil x\rceil\leq x+1\). Hence,

\[\gamma_{T}=\mathcal{O}\left(T^{\frac{d-1}{\beta_{p}}}\log^{1- \frac{d-1}{\beta_{p}^{*}}}T\right).\] (59)

#### a.2.2 Information gain for symmetrised kernels on the hypersphere

Our upper bound involves comparing the corresponding reduction in maximal information across the invariant vs the standard kernel. In this section we will use overbars to refer to properties of the symmetrised (invariant) kernel. We remind the reader that \(G\leq O(d)\) is a finite subgroup of isometries of the sphere \(\mathbb{S}^{d-1}\). Our aim will be to provide bounds on the asymptotics of the eigenspace dimensions \(\frac{\bar{N}(d,k)}{N(d,k)}\) where \(N(d,k)\) is defined as in A.2.1 and \(\bar{N}(d,k)\) is the corresponding eigenspace dimension for the symmetrized kernel \(k_{G}\).

We will follow the exposition from [4], where asymptotic bounds on \(\gamma(d,k):=\frac{\bar{N}(d,k)}{N(d,k)}\) are given with two minor differences: 1. our treatment will handle any finite group of isometries and 2. we pay the price for generalizing to arbitrary groups by obtaining less strict decay rates, which will not trouble us for our analysis.

We will show the following:

**Lemma 1**.: _With the notations described in this section, assume \(G\leq O(d)\) for \(d\geq 10\) is a finite group. Then, if \(-I\in G\):_

\[0\leq\gamma(d,k)\leq\frac{C}{k},\quad\text{for k odd,}\] (60) \[\frac{2}{|G|}\leq\gamma(d,k)\leq\frac{2}{|G|}+\frac{C}{k},\quad \text{for k even,}\] (61)

_and, otherwise:_

\[\frac{1}{|G|}\leq\gamma(d,k)\leq\frac{1}{|G|}+\frac{C}{k}\] (63)

_where \(C\) is a constant that depends on \(k,d\) and the spectra of elements in \(G\)._

Proof.: Recall from [4], the formula:

\[\gamma(d,k)=\frac{1}{|G|}\sum_{\sigma\in G}\gamma(d,k,\sigma)=\sum_{\sigma\in G }\mathbb{E}_{\mu}\left[P_{d,k}\left(\langle\sigma x,x\rangle\right)\right],\] (64)

where \(\mu\) is the uniform measure on the sphere \(S^{d-1}\), and \(P_{d,k}\) is the corresponding Gegenbauer polynomial. To demystify the conditional statement in the Lemma, notice first that \(P_{d,k}\left(\langle x,x\rangle\right)=P_{d,k}(1)=1\), and \(P_{d,k}\left(\langle-x,x\rangle\right)=P_{d,k}(-1)=1\) if \(k\) is even and -1 if \(k\) is odd. For all other \(\sigma\in G\), we'll follow the argument in [4] to show that \(\gamma(d,k,\sigma)\to 0\) with asymptotics at least \(\mathcal{O}\left(\frac{1}{k}\right)\).

We don't make significant contributions to the method in [4], here, other than to remark on the fact that many of the arguments they present follow through for finite subgroups of \(O(d)\).

First remark that for \(\sigma\in G\leq O(d)\) there is a matrix representation \(A_{\sigma}\), which admits a canonical basis in which the matrix elements are:

\[\begin{bmatrix}R_{1}&&&&\\ &\ddots&&&0\\ &&R_{k}&&&\\ &&&\pm 1&&\\ &0&&&\ddots&\\ &&&&\pm 1\end{bmatrix}\] (65)

where each \(R_{\lambda}\) is a \(2\times 2\) matrix of the form \(\left(\begin{smallmatrix}a&b\\ -b&a\end{smallmatrix}\right)\) with eigenvalues \(\lambda\) and \(\bar{\lambda}\).

Moreover, notice that since \(A_{\sigma}^{|G|}=\text{I}\), all eigenvalues are roots of unity \(\lambda=e^{\frac{2\pi i\bar{\jmath}}{\bar{\jmath}}}\) with \(q||G|\). The the eigenvalues of such matrices are the same as the allowable eigenvalues of permutation matrices considered in [4], from which Proposition 6 gives:

\[\gamma(d,k,\sigma)\leq Ck^{-d+s}+o(k^{-d+s})\] (66)

where \(s=\max_{\lambda\in Spec(A_{\sigma})}\{m_{\lambda}+4\cdot\mathbf{1}(|\lambda| <1)\}\). Now assume \(\sigma\neq\pm\text{I}\). We have \(m_{1}\leq d-2\), \(m_{-1}\leq d-2\) and \(m_{\lambda}\leq\frac{d}{2}\) if \(\lambda\neq\pm 1\). Collectively this implies \(-d+s\leq-1\) if \(d\geq 10\)In the following assume that \(-1\notin G\). The corresponding argument for \(-1\in G\) is similar. In view of Lemma 1

\[\frac{1}{|G|}\leq\frac{\bar{N}(d,k)}{N(d,k)}\leq\frac{1}{|G|}+\frac{C}{k}\] (67)

We take the same number of distinct eigenvalues for the invariant and noninvariant kernels, \(K\). However, there will be different multiplicities for the eigenvalues, leading to a different number of eigenvalues \(\bar{D}\). The corresponding tail has mass

\[\bar{\delta}_{\bar{D}}\leq\psi^{2}\sum_{i=\bar{D}+1}^{\infty} \lambda_{i} =\psi^{2}\sum_{k=K}^{\infty}\mu_{k}\bar{N}(d,k)\] (68) \[\leq\psi^{2}\sum_{k=K}^{\infty}\mu_{k}N(d,k)\left(\frac{1}{|G|}+ \frac{C}{k}\right)\] (69) \[\leq\left(\frac{1}{|G|}+\frac{C^{\prime}}{K}\right)\psi^{2}\sum_ {k=K}^{\infty}\mu_{k}N(d,k)\] (70) \[=\left(\frac{1}{|G|}+\frac{C^{\prime}}{K}\right)\delta_{D}\] (71) \[\leq\left(\frac{1}{|G|}+\frac{C^{\prime\prime}}{D^{\frac{1}{d-1} }}\right)\delta_{D}\] (72) \[\leq\frac{1+\epsilon}{|G|}\delta_{D}\] (73)

where Equation (69) follows by substituting Equation (67), Equation (70) follows because \(\frac{1}{k}<\frac{1}{K}\) for \(k>K\), Equation (72) follows because \(K=\Theta(D^{\frac{1}{d-1}})\), and for \(D>D_{\epsilon}\) where \(D_{\epsilon}\) satisfies \(d!\frac{C^{\prime\prime}}{D_{\epsilon}^{\frac{1}{d-1}}}<\epsilon\).

Now, Theorem 3 in [41] gives:

\[\bar{\gamma}_{T}\leq\frac{\bar{D}}{2}\log\left(1+\frac{\bar{k}T}{\tau\bar{D}} \right)+\frac{T\delta_{\bar{D}}}{2\tau},\] (74)

**Proposition 2**.: _If \(a_{n}>0\), \(b_{n}>0\), with \(\lim a_{n}=\lim b_{n}=\infty\), and \(\lim b_{n}/a_{n}=\lambda>0\), then \(\forall\epsilon>0\), \(\exists N\) such that \(\forall N^{\prime}>N\) we have \(\sum_{n<N^{\prime}}b_{n}\geq(1-\epsilon)\lambda\sum_{n<N^{\prime}}a_{n}\)._

Proof.: From the definition of the limit, noting that the \(1-\epsilon\) gives slack to bound \(\lambda\sum_{n<N}a_{n}\) above by \(\epsilon\sum_{N<n<N^{\prime}}b_{n}\). 

Using this proposition on the sequences \(N(d,k)\) and \(\bar{N}(d,k)\) we have further:

\[\bar{\gamma}_{T} \leq\frac{D(1+\epsilon)}{2|G|}\log\left(1+\frac{\bar{k}T|G|}{ \tau D(1+\epsilon)}\right)+\frac{1+\epsilon}{|G|}\frac{T\delta_{D}}{2\tau}\] (75) \[\leq\frac{1+\epsilon}{|G|}\left(\frac{D}{2}\log\left(1+\frac{\bar {k}T|G|}{\tau D(1+\epsilon)}\right)+\frac{T\delta_{D}}{2\tau}\right)\] (76) \[\leq\frac{1+\epsilon}{|G|}\left(\frac{D}{2}\log\left(1+\frac{ \bar{k}T|G|}{\tau D(1+\epsilon)}\right)+\frac{T}{2\tau}\psi^{2}C_{4}D^{\frac{ -\beta_{p}^{*}+d-1}{d-1}}\right)\] (77)

As previously we'll choose \(D_{T}\) as a function of \(T\) such that the logarithmic term dominates (notice that since the first term is increasing in \(D\) any choice of \(D_{T}\) greater than this will also satisfy), i.e.

\[D_{T}\log\left(1+\frac{\bar{k}T|G|}{\tau(1+\epsilon)}\right)\geq\frac{T}{\tau }\psi^{2}C_{4}D_{T}^{\frac{-\beta_{p}^{*}+d-1}{d-1}}\] (78)\[D_{T}\geq\left(\frac{\frac{T}{\tau}\psi^{2}C_{4}}{\log\left(1+\frac{\bar{k}T|G|}{ \tau(1+\epsilon)}\right)}\right)^{\frac{d-1}{\beta_{p}^{*}}}\] (79)

With this choice:

\[\bar{\gamma}_{T}\leq\frac{1+\epsilon}{|G|}\left(\frac{T}{\tau}\psi^{2}C_{4} \right)^{\frac{d-1}{\beta_{p}^{*}}}\log\left(1+\frac{\bar{k}T|G|}{\tau(1+ \epsilon)}\right)^{\frac{\beta_{p}^{*}-d+1}{\beta_{p}^{*}}}\] (80)

Hence,

\[\bar{\gamma}_{T}=\tilde{\mathcal{O}}\left(\frac{1}{|G|}T^{\frac{d-1}{\beta_{p }^{*}}}\right).\] (81)

#### a.2.3 From information gain to sample complexity

Under the assumption of sub-Gaussian noise, it is shown in [42, Remark 2] that the maximum variance reduction algorithm presented in Algorithm 1 incurs simple regret satisfying

\[r_{T}=\mathcal{O}\Bigg{(}\sqrt{\frac{\gamma_{T}\log(T^{d}/\delta)}{T}}\Bigg{)}\] (82)

with probability at least \(1-\delta\), where \(\delta\in(0,1)\).

Note that we are able to use this result without modification, as our approach only incorporates \(G\)-invariance through the kernel; hence, any changes to the regret due to the \(G\)-invariance will only appear in the \(\gamma_{T}\) term. Substituting \(\gamma_{T}\) from Equation (80) into Equation (82) gives

\[r_{T} =\mathcal{O}\left(\frac{1}{|G|^{\frac{1}{2}}}T^{-\frac{\beta_{p}^ {*}-d+1}{2\beta_{p}^{*}}}\log^{\frac{\beta_{p}^{*}-d+1}{2\beta_{p}^{*}}}(T|G| )\log^{\frac{1}{2}}T/\delta\right)\] (83) \[=\mathcal{O}\left(\frac{1}{|G|^{\frac{1}{2}}}T^{-\alpha}\log^{ \alpha}(T|G|)\log^{\frac{1}{2}}T/\delta\right),\] (84)

where we have absorbed factors that do not depend on \(T\) or \(G\), and let \(\alpha=\frac{\beta_{p}^{*}-d+1}{2\beta_{p}^{*}}\).

Our goal is for \(r_{T}=\mathcal{O}(\epsilon)\). This can be achieved by setting

\[T\propto\frac{1}{|G|^{\frac{1}{2\alpha}}}\frac{1}{\epsilon^{\frac{1}{\alpha}} }\log\left(\frac{1}{|G|^{\frac{1}{2\alpha}-1}}\frac{1}{\epsilon^{\frac{1}{ \alpha}}}\right)\log^{\frac{1}{2\alpha}}\left(\frac{1}{|G|^{\frac{1}{2\alpha} }}\frac{1}{\epsilon^{\frac{1}{\alpha}}}\right)\] (85)

with appropriate implied constants. Substitution of \(T\) from Equation (85) into Equation (84) makes it evident that for this \(T\) we have

\[r_{T}=\mathcal{O}(\epsilon)+\mathcal{O}(\log\log T+\log\log|G|).\] (86)

Substituting \(\alpha\) into Equation (85) gives the final upper bound on sample complexity, which is

\[T=\mathcal{O}\left(\left(\frac{1}{|G|}\right)^{\frac{\beta_{p}^{*}-d+1}{ \beta_{p}^{*}-d+1}}\epsilon^{-\frac{2\beta_{p}^{*}}{\beta_{p}^{*}-d+1}}\log \left(\frac{\epsilon^{-\frac{2\beta_{p}^{*}}{\beta_{p}^{*}-d+1}}}{|G|^{\frac{ d-1}{\beta_{p}^{*}-d+1}}}\right)\log^{\frac{\beta_{p}^{*}}{\beta_{p}^{*}-d+1}} \left(\frac{\epsilon^{-\frac{2\beta_{p}^{*}}{\beta_{p}^{*}-d+1}}}{|G|^{\frac{ \beta_{p}^{*}}{\beta_{p}^{*}-d+1}}}\right)\right).\] (87)

For the Matern kernel on the hypersphere, [9] showed that

\[\beta_{p}^{*}=2\nu+d-1,\] (88)

and hence, the MVR algorithm with an invariant Matern GP defined on \(\mathbb{S}^{d-1}\) will achieve simple regret \(\epsilon\) with probability \(1-\delta\) if

\[T=\tilde{\mathcal{O}}\left(\left(\frac{1}{|G|}\right)^{\frac{2\nu+d-1}{2\nu}} \epsilon^{-\frac{2\nu+d-1}{\nu}}\right),\] (89)

where \(\tilde{\mathcal{O}}\) hides logarithmic factors in \(\epsilon\) and \(|G|\).

### Proof of Theorem 2: lower bound on sample complexity

In this section, we derive Theorem 2, the lower bound on regret for the invariant Matern kernel under permutation groups. The main elements of this proof were introduced in [12], which extended the method in [35]. This constructive method serves to demonstrate the tightness of the upper bound. As in [12] and [42], our method involves building up a set of candidate optimization targets in \(\mathcal{H}_{k_{G}}\). However, unlike these papers, it is no longer clear in this setting that such constructions are optimal in terms of the resulting dependence of the lower bound on \(|G|\) (the current gap to the upper bound potentially measures this failure rate). Our methods will produce elements in \(S_{G}(\mathcal{F})\subseteq\mathcal{H}_{k_{G}}\), for a particular set of functions \(\mathcal{F}\), and it's the object of further research if these constructions can be improved.In Figure 6(a), we have an example of the types of functions we construct, by applying the operator \(S_{G}\) to a simple bump function. In this example, \(G\) is a group of rotations along the vertical axis. In Figure 6(b) is an example of a function we cannot retrieve through our method. This function is similarly invariant (also to all other rotations along the axis). Our method relies on a packing argument for the supports of such functions and it is a priori unclear how the tightest packing can be obtained.

We identify a bounded subset of the RKHS of invariant functions that contains functions with the property that each point in \(\mathcal{X}\) is \(\epsilon\)-optimal for at most one function. We then construct a set of noisy bandit problems using functions from this class, before applying [12, Lemma 1] to lower-bound the number of samples that a learner requires in order to distinguish between them, and hence the best-case regret for this configuration.

#### a.3.1 An explicit construction

The proof for our lower bound will be constructive. We will exhibit a family of functions for which there is a computable lower bound of samples needed to distinguish their maxima. The challenge comes from constructing these functions _natively_ in the RKHS of invariant functions. The strategy we employ here is to construct functions by symmetrization of simple bump functions. For the benefit of the reader, we'll show an explicit version of this construction first, on the hypercube \(\mathcal{X}=[0,1]^{d}\) equipped with the action of \(G\leq S_{d}\) and subsequently extend to general actions of subgroups of \(O(d)\) on the sphere \(\mathbb{S}^{d-1}\).

**Lemma 2** (Bounded support function construction [12, Lemma 4]).: _Let \(h(x)=\exp\left(\frac{-1}{1-\|x\|^{2}}\right)\mathds{1}\{\|x\|_{2}<1\}\) be the \(d\)-dimensional bump function, and define \(g(x,w,\epsilon)=\frac{2\epsilon}{h(0)}h(\frac{x}{w})\) for some \(w>0\) and \(\epsilon>0\). Then, \(g\) satisfies the following properties:_

1. \(g(x,w,\epsilon)=0\) _for all x outside the_ \(\ell_{2}\)_-ball of radius_ \(w\) _centred at the origin;_
2. \(g(x,w,\epsilon)\in[0,2\epsilon]\) _for all x, and_ \(g(0)=2\epsilon\)_;_
3. \(\|g\|_{k}\leq c_{1}\frac{2\epsilon}{h(0)}\left(\frac{1}{w}\right)^{\nu}\|h\|_ {k}\) _when_ \(k\) _is the Matern-_\(\nu\) _kernel on_ \(\mathbb{R}^{d}\)_, where_ \(c_{1}\) _is constant. In particular, we have_ \(\|g\|_{k}\leq B\) _when_ \(w=\left(\frac{2\epsilon c_{1}\|h\|_{k}}{h(0)B}\right)^{\frac{1}{\nu}}\)_._

Let \(e_{1},\ldots,e_{d}\) be the canonical basis vectors in \(\mathbb{R}^{d}\). Then, the set

\[L(\alpha)=\left\{\frac{2\lambda_{1}-1}{2}\alpha e_{1}+\cdots+\frac{2\lambda_{ d}-1}{2}\alpha e_{d}\;:\;\lambda_{1},\ldots,\lambda_{d}\in\mathbb{Z}\right\}\] (90)

Figure 7: Examples of invariant functions on the sphere. The construction for our lower bound consists of functions invariant to finite rotations, as in Figure 6(a), but does not include functions like Figure 6(b) (which produce different packings).

forms a lattice of points on \(\mathbb{R}^{d}\) with spacing \(\alpha\). Equivalently, \(L(\alpha)\) defines a partitioning of \(\mathbb{R}^{d}\) into disjoint regions or 'lattice cells' \(\{\mathcal{R}(x_{0})\ :\ x_{0}\in L(\alpha)\}\), where \(\mathcal{R}(x_{0})\) is a \(d\)-dimensional hypercube of side length \(\alpha\) centred on \(x_{0}\).

Define the function class \(\mathcal{F}(w,\epsilon)=\{g(x-x_{0},w,\epsilon)\ :\ x_{0}\in L(w)\}\). By property (1) of Lemma 2, \(g(x-x_{0},w,\epsilon)\) is zero outside of \(\mathcal{R}(x_{0})\). Hence, the support of functions \(f\in\mathcal{F}(w,\epsilon)\) are pairwise disjoint. In addition, as \(\mathcal{F}(w,\epsilon)\) consists only of translates of \(g(x,w,\epsilon)\), we retain \(\max f=2\epsilon\) and \(\|f\|_{k}\leq B\ \forall f\in\mathcal{F}(w,\epsilon)\) for appropriate \(w\) from properties 2 and 3 of Lemma 2 respectively.

The hypercube \([0,1]^{d}\) contains \(N\) of these functions, where

\[N=\left\lfloor\frac{1}{w}\right\rfloor^{d}=\left\lfloor\frac{h(0)B}{2\epsilon c _{1}\|h\|_{k}}\right\rfloor^{d/\nu}.\] (91)

Now, we construct a set of \(G\)_-invariant_ bump functions with disjoint support on the hypercube \([0,1]^{d}\), centred on the lattice cells for a class of groups \(G\) equipped with an action which we define below.

**Lemma 3** (Set of invariant functions).: _Let \(G\) be a subset of \(S_{d}\), the permutation group on \(d\) elements. \(G\) acts on \(\mathbb{R}^{d}\) (and on any invariant subset \(\mathcal{X}\)) by permutation of the coordinate axes. Let \(\mathcal{F}(w,\epsilon)=\{g(x-x_{0},w,\epsilon)\ :\ x_{0}\in L(w)\}\). Then, \(\bar{\mathcal{F}}(w,\epsilon,G)=\{S_{G}(f):f\in\mathcal{F}(|G|^{1/\nu}w,|G|\epsilon)\}\), where \(S_{G}\) is defined as in Proposition 1, satisfies the following properties:_

1. \(\bar{\mathcal{F}}(w,\epsilon,G)\) _is a set of_ \(G\)_-invariant functions on_ \(\mathbb{R}^{d}\) _with pairwise disjoint support;_
2. \(\max_{x}\bar{f}_{i}(x)=\frac{2|G|\epsilon}{|G\cdot x_{i}|}\)_, where_ \(\bar{f}_{i}(x)=S_{G}(g(x-x_{i},|G|^{1/\nu}w,|G|\epsilon))\in\bar{\mathcal{F}} (w,\epsilon,G)\)_;_
3. \(\|\bar{f}\|\leq B\quad\forall\bar{f}\in\bar{\mathcal{F}}(w,\epsilon,G)\)_._

Proof.: First, we show that \(\bar{\mathcal{F}}(w,\epsilon,G)\) is indeed a \(S_{G}\)-invariant set of \(G\)-invariant functions with disjoint support. As all translates \(g(x-x_{0})\) are symmetric around \(x_{0}\), it holds that \(g(\sigma(x)-x_{0})=g(x-\sigma^{-1}(x_{0}))\). From Equation (90), it is evident that the lattice \(L(\alpha)\) is invariant to permutations; that is, \(\sigma(x_{0})\in L(w)\) for all \(x_{0}\in L(w),\sigma\in G\). Hence, \(\mathcal{F}(w,\epsilon)\) itself is invariant under \(S_{G}\), and a fortiori \(\bar{\mathcal{F}}(w,\epsilon,G)=S_{G}\left(\mathcal{F}(|G|^{1/\nu}w,|G| \epsilon)\right)\).

Next, we show that functions in \(\bar{\mathcal{F}}(w,\epsilon,G)\) have maximum value \(\frac{2|G|\epsilon}{|G\cdot x_{i}|}\). Let \(f_{i}\in\mathcal{F}(|G|^{1/\nu}w,|G|\epsilon)\) be the function centred on \(x_{i}\). From Lemma 2, property 2, \(f_{i}\) has maximum value \(2|G|\epsilon\). Let \(G_{x_{i}}=\{\sigma\ :\ \sigma(x_{i})=x_{i}\}\) denote the stabilizer group of \(G\) at \(x_{i}\).

Since the lattice itself is invariant, the image \(G\cdot x_{i}\) of the centre will have size

\[|G\cdot x_{i}|=\frac{|G|}{|G_{x_{i}}|}.\] (92)

The preimage of any element in \(G\cdot x_{i}\) consists of exactly \(|G_{x_{i}}|\) elements. Therefore,

\[S_{G}(f_{i}) =\frac{1}{|G|}\sum_{H\in G/G_{x_{i}}}\sum_{\sigma\in H}f_{i}\circ\sigma\] (93) \[=\frac{|G_{x_{i}}|}{|G|}\sum_{H\in G/G_{x_{i}}}f_{i}\circ\sigma_{ H},\] (94)

where \(\sigma_{H}\) is a representative of the coset \(H\) and \(f_{i}\circ\sigma_{H}\) is the unique value of \(f_{i}\circ\sigma\) on this coset, regardless of its representative.

Note that for any cosets \(H\) and \(K\), \(f_{i}\circ\sigma_{H}\) and \(f_{i}\circ\sigma_{K}\) have disjoint supports from the same arguments as before, and thus,

\[\max S_{G}(f_{i})=\frac{|G_{x_{i}}|}{|G|}\max f_{i}\circ\sigma=2|G|\epsilon\frac {|G_{x_{i}}|}{|G|}=\frac{2|G|\epsilon}{|G\cdot x_{i}|}.\] (96)Finally, we show that functions in \(\bar{\mathcal{F}}(w,\epsilon,G)\) have norm bounded by \(B\). Let \(\bar{f}\in\bar{\mathcal{F}}(w,\epsilon,G)\) and \(f\in\mathcal{F}(|G|^{1/\nu}w,|G|\epsilon)\). From property (3) in Lemma 2,

\[\|f\|_{k} \leq\frac{c_{1}2|G|\epsilon}{h(0)}\left(\frac{1}{|G|^{1/\nu}w} \right)^{\nu}\|h\|_{k}\] \[=\|g\|_{k}\leq B,\]

where \(w\) is the same as before. From Proposition 1, recall that \(S_{G}\) is a projection; hence, \(\|f\|_{k}\leq B\) implies \(\|S_{G}(f)\|_{k_{G}}\leq B\), and so \(\|\bar{f}\|_{k_{G}}\leq\|f\|_{k}\leq B\). 

We now restrict the function set to the unit hypercube. The number of functions from \(\mathcal{F}(|G|^{1/\nu}w,|G|\epsilon)\) (the set of rescaled non-invariant functions) that fit in the unit hypercube is given by \(N^{\prime}\), where

\[N^{\prime} =\left\lfloor\frac{1}{|G|^{1/\nu}w}\right\rfloor^{d}\] (97) \[=\left\lfloor\frac{1}{|G|^{1/\nu}}\right\rfloor^{d}N,\] (98)

where \(N\) is defined in Equation (91).

Clearly, the number of orbits in \(\mathcal{F}(|G|^{1/\nu}w,|G|\epsilon)\), i.e. the number of elements in \(\bar{\mathcal{F}}(w,\epsilon,G)\) is equal to the number of orbits of \(G\) on \(L(w)\), by Lemma 3. We would like to count the number of orbits, or at least bound it from below. For specific groups \(G\) one can do this explicitly, however, in general it is impossible to give a count of the orbits, without further assumption. What we can, however say is that the number of orbits of size exactly \(|G|\) represents almost the total number of orbits when \(w\) is small.

**Proposition 3**.: _Let \(M=|\bar{\mathcal{F}}(w,\epsilon,G)|=|\left(L(|G|^{1/\nu}w)\bigcap[0,1]^{d} \right)/G|\). Let \(K:=\lfloor\frac{1}{|G|^{1/\nu}w}\rfloor\). Then \(M\geq\frac{K^{d}}{|G|}\), and this bound is asymptotically tight in the limit of \(w\to 0\)._

Proof.: Let \(\mathcal{W}(s)\) be the set of points in \(L(|G|^{1/\nu}w)\bigcap[0,1]^{d}\) with exactly \(s\leq d\) repeated indices. Assume \(w\) is such that \(K:=\lfloor\frac{1}{|G|^{1/\nu}w}\rfloor>d+1\). Then, \(\mathcal{W}(0)\neq\emptyset\) under the action of \(G\), such points in \(L(w)\) will have \(|G\cdot x|=|G|\).

Points in \(\bigcup_{s=1}^{d}\mathcal{W}(s)\), i.e. points with repeated indices lie in a union of manifolds of dimension \(d-1\) or below within the hypercube \([0,1]^{d}\). As such, \(w\) decreases,

\[\frac{|\mathcal{W}(0)|}{N^{\prime}}\to 1.\] (99)

This is easy to see as

\[|\mathcal{W}(0)|=K\cdot(K-1)\cdot...(K-d).\] (100)

Hence, we have that \(|\bar{\mathcal{F}}(w,\epsilon,G)|\) approaches \(\frac{|\mathcal{F}(|G|^{1/\nu}w,|G|\epsilon)|}{|G|}\) from above as the width \(w\) tends to \(0\):

\[\lim_{w\to 0}\frac{|\bar{\mathcal{F}}(w,\epsilon,G)|}{|\mathcal{F}(|G|^{1/ \nu}w,|G|\epsilon)|}=\frac{1}{|G|}.\] (101)

Note that \(M\) is also the number of functions from \(\bar{\mathcal{F}}(w,\epsilon,G)\) that are supported in \([0,1]^{d}\). Then, we can rewrite the above inequality as:

\[M \geq\frac{N^{\prime}}{|G|}\] (102) \[\geq\frac{1}{|G|^{1+d/\nu}}N,\] (103)where \(N\) is the number of functions from \(\mathcal{F}(w,\epsilon)\) that fit in \([0,1]^{d}\), as defined in Equation (91). Equation (103) gives a direct relationship between the packing of invariant and non-invariant bump functions with bounded norm that fit in the unit hypercube.

Note that the above analysis shows that this bound is tight for small enough \(w\) which depends only on \(d\) and \(\nu\) since \(|G|<d!\). If one is interested only in Equation (103), this is easy to obtain as \(M\), the number of orbits is greater than \(N^{\prime}\) divided by the size of the largest orbit.

Note also that in the case of noiseless observations any optimisation algorithm will have to query at least \(M\) (resp. \(N\)) points to be able to distinguish between functions in \(\bar{\mathcal{F}}(w,\epsilon,G)\) (resp. \(\mathcal{F}(w,\epsilon)\)) defined on \([0,1]^{d}\); hence, Equation (103) (resp. Equation (91)) can also be interpreted as a lower bound on the sample complexity of finding the maximum of these functions.

#### a.3.2 Distinguishing between bandit instances

The subsequent section of the proof mirrors the approach used to derive a simple regret lower bound in the standard (non-invariant) setting, as detailed in [12, Section 4.2]. The primary distinction lies in the construction of the representative functions that exhibit group invariance, as outlined in Equation (104) and Equation (105).

Recall the definition of a region \(\mathcal{R}(\cdot)\) from the discussion following Equation (90). Define \(\bar{\mathcal{R}}_{i}\) as \(\bar{\mathcal{R}}_{i}=\bigcup_{\sigma\in G}\mathcal{R}(\sigma(x_{i}))\), where \(x_{i}\in L(|G|^{1/\nu}w)\) is the center of the region. Note that these regions are self-contained under \(G\), so that \(x\in\bar{\mathcal{R}}_{i}\implies\sigma(x)\in\bar{\mathcal{R}}_{i}\), and are pairwise disjoint. Therefore, we define a set of \(M\) such regions \(\{\bar{\mathcal{R}}_{i}\}_{i=1}^{M}\), which partition our domain. The value of \(M\) is lower bounded in Equation (103).

We replace \(B\) with \(B/3\) in the construction of \(\mathcal{F}(|G|^{1/\nu}w,|G|\epsilon)\) (see Lemma 3).3 Let \(f_{i}\) and \(f_{j}\) (\(i\neq j\)) be shifted bump functions from the set \(\mathcal{F}(|G|^{1/\nu}w,|G|\epsilon)\), centered at some fixed \(x_{i}\) and \(x_{j}\) respectively, where \(x_{i},x_{j}\in L(|G|^{1/\nu}w)\). Furthermore, these are selected such that the sizes of the orbits, \(|G\cdot x_{i}|\) and \(|G\cdot x_{j}|\), are both equal to \(|G|\). Then, we construct the pair

Footnote 3: We note that this modification affects only the constant factors (also in the bound on \(M\) from Equation (103)), which are not important for our analysis.

\[\bar{f} =S_{G}(f_{i}),\] (104) \[\bar{f}^{\prime} =S_{G}(f_{i})+2S_{G}(f_{j}).\] (105)

The following holds \(\|\bar{f}\|_{k_{G}}\leq B/3\) and \(\|\bar{f}^{\prime}\|_{k_{G}}\leq B\) (from Lemma 3 and triangle inequality). We observe that \(\bar{f}\) and \(\bar{f}^{\prime}\) coincide outside \(\bar{\mathcal{R}}_{j}\) i.e., \(\bar{f}^{\prime}-\bar{f}\) is only non-zero in region \(\bar{\mathcal{R}}_{j}\). Moreover, we have \(\max_{x\in\bar{\mathcal{R}}_{j}}\bar{f}(x)=2\epsilon\) and \(\max_{x\in\bar{\mathcal{R}}_{j}}\bar{f}^{\prime}(x)=4\epsilon\). It follows that only points from \(\bar{\mathcal{R}}_{j}\) and \(\bar{\mathcal{R}}_{i}\) can be \(\epsilon\)-optimal for \(\bar{f}^{\prime}\) and \(\bar{f}\), respectively.

In the subsequent steps, let \(P_{f}[\cdot]\) and \(\mathbb{E}_{f}[\cdot]\) denote probabilities and expectations with respect to the random noise, given a suitably chosen underlying function \(f\). Additionally, we define \(P_{f}(y|x)\) as the conditional distribution \(N(f(x),\sigma^{2})\), consistent with the Gaussian noise model.

We fix \(T\in\mathbb{Z}^{+}\), \(\delta\in(0,1/3)\), \(B>0\) and \(\epsilon\in(0,1/2)\), and assume the existence of an algorithm such that for any \(f\in\mathcal{F}_{k_{G}}(B)\), it achieves an average simple regret \(r(x^{(T)})\leq\epsilon\) with a probability of at least \(1-\delta\). Here, \(x^{(T)}\) represents the final point reported by the algorithm. Note that an algorithm that achieves simple regret less than \(\epsilon\) for all \(f\in\mathcal{F}_{k_{G}}(B)\) automatically solves the decision problem of distinguishing between \(f\) and \(f^{\prime}\) as above. If no such algorithm exists, the sample complexity necessary to achieve this bound on regret must be higher than \(T\). Next, we establish a lower bound on the number of samples required for such an algorithm to differentiate between two bandit environments (with reward functions \(\bar{f}\) and \(\bar{f}^{\prime}\) and the same Gaussian noise model) that share the same input domain but have distinct optimal regions.

Let \(A\) denote the event that the returned point \(x^{(T)}\) falls within the region \(\bar{\mathcal{R}}_{i}\). Assume that an algorithm achieves a simple regret of at most \(\epsilon\) for both functions \(\bar{f}\) and \(\bar{f}^{\prime}\) (both \(\bar{f},\bar{f}^{\prime}\in\mathcal{F}_{k_{G}}(B)\) as established before) each with a probability of at least \(1-\delta\). Under our assumption on simple regret, this implies \(P_{f}[A]\geq 1-\delta\) and \(P_{f^{\prime}}[A]\leq\delta\) since only points within \(\bar{\mathcal{R}}_{i}\) can be \(\epsilon\)-optimal under \(\bar{f}\), and only points within \(\bar{\mathcal{R}}_{j}\) can be \(\epsilon\)-optimal under \(\bar{f}^{\prime}\).

We are now in position to apply [12, Lemma 1]. We apply this lemma using the previously defined \(\bar{f}\). \(\bar{f}^{\prime}\), and \(A\).4 It therefore holds that, for \(\delta\in(0,\frac{1}{3})\):

Footnote 4: Following the approach in [12], we deterministically set the stopping time in Lemma 1 to \(T\), corresponding to the fixed-length setting in which the time horizon is pre-specified. For an adaptation to scenarios where the algorithm may determine its stopping time, refer to Remark 1 in [12].

\[\sum_{m=1}^{M}\mathbb{E}_{\bar{f}}\left[C_{m}\right]\max_{x\in \mathcal{R}_{m}}\mathrm{KL}\left[\mathbb{P}_{\bar{f}}(\cdot|x)\;||\;\mathbb{P} _{\bar{f}^{\prime}}(\cdot|x)\right]\geq\log\frac{1}{2.4\delta},\] (106)

where \(C_{m}\) is the number of queried points in \(\bar{\mathcal{R}}_{m}\) up to time \(T\).

For the two bandit instances and any input \(x\), the observation distributions are \(\mathcal{N}(\bar{f}(x),\sigma^{2})\) and \(\mathcal{N}(\bar{f}^{\prime}(x),\sigma^{2})\). Utilising the standard result for the KL divergence between two Gaussian distributions with the same variance, we obtain:

\[\max_{x\in\mathcal{R}_{m}}\mathrm{KL}\left[\mathbb{P}_{\bar{f}}( \cdot|x)\;||\;\mathbb{P}_{\bar{f}^{\prime}}(\cdot|x)\right] =\frac{(\max_{x\in\bar{\mathcal{R}}_{m}}\bar{f}(x)-\max_{x\in \bar{\mathcal{R}}_{m}}\bar{f}^{\prime}(x))^{2}}{2\sigma^{2}}\] (107) \[=\begin{cases}\frac{8\epsilon^{2}}{\sigma^{2}}&m=j\\ 0&\text{otherwise},\end{cases}\] (108)

as \(\bar{f}(x)=\bar{f}^{\prime}(x)\) for \(x\notin\bar{\mathcal{R}}_{j}\), and for \(x\in\bar{\mathcal{R}}_{j}\) we have that \(\bar{f}(x)=0\) and \(\max_{x\in\bar{\mathcal{R}}_{j}}\bar{f}^{\prime}(x)=4\epsilon\).

Hence, Equation (106) becomes

\[\frac{8\epsilon^{2}}{\sigma^{2}}\mathbb{E}_{\bar{f}}\left[C_{j} \right]\geq\log\frac{1}{2.4\delta}.\] (109)

Let \(J=\{j\in I|x_{j}\in\mathcal{W}(0)\}\) be the set of indices \(j\) such that \(f_{j}\) is centred around a point with no repeated indices such as in Proposition 3. Let \(\rho_{w}=\frac{|J|}{|G|M}\) be the fraction of all orbits of size exactly \(|G|\). By Proposition 3, for any small \(\delta_{w}\), there is a correspondingly small enough \(w\) such that \(\rho_{w}\geq 1-\delta_{w}\). Since the previous inequality holds for an arbitrary \(j\), we sum over over \(j\in\left(J\setminus G\{i\}\right)/G\) to arrive at:

\[\frac{1}{\frac{|J|}{|G|}-1}\sum_{j\neq i}\frac{8\epsilon^{2}}{ \sigma^{2}}\mathbb{E}_{\bar{f}}\left[C_{j}\right]\geq\log\frac{1}{2.4\delta}.\] (110)

By rearranging and noting that \(\sum_{j=1}^{M}\mathbb{E}_{\bar{f}}\left[C_{j}\right]=T\) we obtain

\[T\geq(M\rho_{w}-1)\frac{\sigma^{2}}{8\epsilon^{2}}\log\frac{1}{ 2.4\delta}.\] (111)

Substituting in \(M\) from Equation (103) gives

\[T\geq\frac{1-\delta_{w}}{|G|^{\frac{\nu+d}{\nu}}}N\frac{\sigma^{2}}{8\epsilon^ {2}}\log\frac{1}{2.4\delta}-\frac{\sigma^{2}}{8\epsilon^{2}}\log\frac{1}{2.4 \delta},\] (112)

and using \(N\) from Equation (91) gives the final bound

\[T=\Omega\Bigg{(}\frac{1}{|G|^{\frac{\nu+d}{\nu}}}\frac{\sigma^{2}}{\epsilon^ {2}}\left(\frac{B}{\epsilon}\right)^{d/\nu}\log\frac{1}{\delta}\Bigg{)},\] (113)

where the implied constants can depend on \(d\), \(l\), and \(\nu\).

### Lower bounds on different underlying spaces.

In this section we'll investigate how to raise the lower bounds on sample complexity to embedded submanifolds \(\mathcal{X}\hookrightarrow\mathbb{R}^{d}\). The full treatment below is valid only for the hypersphere, \(\mathcal{X}=\mathbb{S}^{d}\) but many of the ingredients required are valid on other manifolds as well. When specific statements about the sphere are made in the treatment below, we will explicitly refer to \(\mathbb{S}^{d}\) instead of \(\mathcal{X}\).

In the proof in A.3, we used the existence of a \(G\)-invariant lattice to underpin our bump function construction. Moreover, we've used the fact that the RKHS norm \(B:=\|g\|_{k}\sim\frac{\epsilon}{w^{\nu}}\). We will replicate the underlying function class construction for \(\bar{\mathcal{F}}(w,\epsilon,G)\) here.

We will recover the same property for bump functions on \(\mathcal{X}\) but under a different notion of rescaling. Pick a point \(p\in U\subset\mathcal{X}\) and consider \((y^{1},...,y^{m})\) a system of coordinates on \(U\) so that \(x_{i}(y^{1},...,y^{m})\) represent the standard coordinates on the ambient space \(\mathbb{R}^{d}\). Let \(f_{p,w,\epsilon}:\mathbb{R}^{d}\rightarrow\mathbb{R}_{+}\) be a bump function around \(p\) which is 0 outside \(B_{w}(p;\mathbb{R}^{d})\) and supported in \(B_{w/2}(p;\mathbb{R}^{d})\) with height \(\epsilon\), as in section A.3. Let \(\phi_{p,w,\epsilon}=f_{p,w,\epsilon}\big{|}_{\mathcal{X}}\).

If \(\mathcal{X}=\mathbb{S}^{d}\), then the intersection \(B_{r_{w}}(p;\mathcal{X}):=B_{w}(p;\mathbb{R}^{d})\bigcap\mathcal{X}\) is a geodesic neighbourhood with geodesic radius \(r_{w}=\arccos\left(\frac{2-w^{2}}{2}\right)\). For small enough \(w\),

\[w\leq r_{w}\leq w+w^{2},\] (114)

by computing the Taylor expansion.

We will lso need to compute the RKHS norm for \(\phi_{p,w,\epsilon}\) for some suitably defined kernel. Let \(k_{\nu}:\mathbb{R}^{d}\times\mathbb{R}^{d}\) be the standard Matern kernel, and consider \(\bar{k}_{\nu}\) its restriction to \(\mathcal{X}\).

Next, we recall the existence of a fundamental domain for the action of finite groups \(G\) on \(\mathcal{X}\). Following the definition of fundamental domain from [21], we say \(F\subset\mathcal{X}\) is a fundamental domain for the action of \(G\) if:

* \(F\) is a domain.
* \(G\cdot\bar{F}=\mathcal{X}\)
* \(gF\bigcap F\neq\emptyset\implies g=e\), the identity.
* For any compact \(K\subset\mathcal{X}\), the _transporter set_\((\bar{F}|K)_{G}\) is finite.

Note that the last condition is trivial if \(G\) is finite. The construction of \(F\) is simple given that \(G\) acts by isometries of the underlying space \(\mathbb{R}^{d}\) and \(g_{\mathcal{X}}=\iota^{*}g_{\mathbb{R}^{d}}\).

Indeed, let \(x\) be a point such that \(|Gx|=|G|\) (such a point trivially exists, as each member of \(G\) other than the identity fixes a subspace of dimension at most \(d-1\)) on the \(\mathcal{X}=\mathbb{S}^{d-1}\). Then, \(|Gx|=|G|\), and the Dirichlet domain:

\[F_{x}=\{y\in\mathcal{X}|d_{\mathcal{X}}(x,y)<d_{\mathcal{X}}(gx,y),\quad \forall g\in G\setminus\{e\}\}\] (115)

is a fundamental domain for the action (see [21] for proof). One can see this set of points as the interior of the points in \(\mathcal{X}\) which are closest to \(x\) rather than any of its other translates under \(G\). For example, for the action of \(S_{d}\) on the positive orthant \(\mathbb{S}^{d-1}\), it is any of the simplices in the barycentric subdivision.

Equipped with this definition, we construct the lattice of bump function centres on \(\mathcal{X}\) as follows. First construct a \(2r_{w}\)-net \(L(r_{w};F_{x})\) on \(F_{x}\) as the set of centres of an optimal geodesic ball packing of \(F_{x}\). Then define

\[L(r_{w}):=L(r_{w};\mathcal{X})=G\cdot L(r_{w};f_{x})\] (116)

We will need to compare the cardinalities of \(L(r_{w})\) and \(L(tr_{w})\) for some scaling factor \(t>0\). Let \(\Pi(r_{w})\) be the packing number of \(F\subseteq\mathbb{S}^{d-1}\) by geodesic balls of radius \(r_{w}\).

We have trivially that

\[\frac{Vol(F)}{Vol(B_{2r_{w}}(p;\mathcal{X}))}\leq\Pi(r_{w})\leq\frac{Vol(F)}{ Vol(B_{r_{w}}(p;\mathcal{X}))}\] (117)

where the first inequality comes trivially from noticing that a maximal \(r_{w}\) packing is a \(2r_{w}\) covering. Moreover, since the exponential map is a diffeomorphism on small enough neighbourhoods (with derivative I at the origin), we have that:

\[K_{1}Vol(B_{r_{w}}(p;\mathbb{R}^{d-1}))\leq Vol(B_{r_{w}}(p;\mathcal{X}))\leq K _{2}Vol(B_{r_{w}}(p;\mathbb{R}^{d-1}))\] (118)for some \(0<K_{1}<K_{2}\). Combining equations 117 and 118 gives:

\[c_{1}t^{d-1}\Pi(r_{w})\leq\Pi(tr_{w})\leq c_{2}t^{d-1}\Pi(r_{w})\] (119)

Following this discussion, we have the following lemma which is an extension to Lemma 3. Denote by \(w_{r}\) the inverse mapping of \(w\to r_{w}\), defined on small enough geodesic balls on \(\mathbb{S}^{d-1}\). We write \(\mathcal{F}(w,\epsilon)=\{f_{p,w,\epsilon}\,:\,p\in L(r_{w})\}\) for a set of functions on the ambient space and \(\mathcal{F}_{\mathcal{X}}(r_{w},\epsilon)=\{\phi_{p,w,\epsilon}:=f_{p,w, \epsilon}|_{\mathcal{X}}\,:\,p\in L(r_{w})\}\) for the restriction \(\mathcal{F}(w,\epsilon)\big{|}_{\mathcal{X}}\). Similarly, we write \(\bar{\mathcal{F}}(w,\epsilon,G)=\{S_{G}(f):f\in\mathcal{F}(|G|^{1/\nu}w,|G| \epsilon)\}\) and \(\bar{\mathcal{F}}_{\mathcal{X}}(w,\epsilon,G)=\bar{\mathcal{F}}(w,\epsilon,G )\big{|}_{\mathcal{X}}\)

**Lemma 4** (Invariant functions on the sphere).: _The sets \(\bar{\mathcal{F}}_{\mathcal{X}}(r_{w},\epsilon,G)\) and \(\bar{\mathcal{F}}(w,\epsilon,G)\) satisfy the following properties._

1. \(\bar{\mathcal{F}}(w,\epsilon,G)\) _is a set of_ \(G\)_-invariant functions on_ \(\mathbb{S}^{d-1}\) _with pairwise disjoint support;_
2. \(\max_{x}S_{G}(f_{p,w,\epsilon})(x)=\frac{2|G|\epsilon}{|G\cdot p|}\)_._
3. \(\|\bar{f}\|\leq B\quad\forall\bar{f}\in\bar{\mathcal{F}}_{\mathcal{X}}(w, \epsilon,G)\)_._

Proof.: The key ingredient for 1. is to note that each \(f_{p,w,\epsilon}(x)=g(\frac{x-p}{w})\), and thus, each \(f\) is \(G\)-invariant around the point \(p\), namely, for \(\sigma\in G\), \(f_{p,w,\epsilon}\circ\sigma=f_{\sigma^{-1}(p),w,\epsilon}\). Then, since \(L(r_{w})\) is \(G\)-invariant by construction, 1. follows. For 2. the proof is identical to Lemma 3 2. For 3. notice that

\[\|\phi_{p,tw,\epsilon}\|_{\tilde{k}_{\nu}}\leq\inf_{\phi^{\epsilon}}\|\phi_{p,tw,\epsilon}^{e}\|_{k_{\nu}}\leq\|f_{p,tw,\epsilon}\|_{k_{\nu}}=t^{\nu}\|f_{ p,w,\epsilon}\|_{k_{\nu}}\] (120)

where the second inequality follows from the restriction and \(\phi^{\epsilon}\) is any extension of \(\phi\) to the underlying space, since the norm of a restriction is less than that of all extensions.

Let \(N=|\mathcal{F}(w,\epsilon)|\), \(M=\bar{\mathcal{F}}(w,\epsilon,G)\). Recall that these functions are defined in terms of the packing numbers of \(F\), so that \(N=|G|\Pi(r_{w})\) and \(M=\Pi(r_{|G|^{1/\nu}w})\). Pick \(\delta_{w}>0\). For all small enough \(w\), we have that:

\[M\geq\Pi(|G|^{1/\nu}(1+\delta_{w})r_{w})\geq c_{1}|G|^{\frac{d-1}{\nu}}(1+ \delta_{w})^{d-1}\Pi(r_{w})=c_{3}|G|^{1+\frac{d-1}{\nu}}N\] (121)

where the first inequality comes from equation 114 and the fact that the packing number is a decreasing function of the radius, the second inequality comes from equation 119 and \(c_{3}\) is a constant which depends on the ambient dimension and the curvature of the space \(\mathcal{X}\).

Following the arguments of section A.3.2 we see, that by construction, \(L(r_{w})\) admits only orbits of size \(|G|\), end hence in Equation 110, the sum is over all elements in \(\bar{\mathcal{F}}_{\mathcal{X}}(r_{w},\epsilon,G)\). Hence we recover the same lower bound on sample complexity:

\[T=\Omega\Bigg{(}\frac{1}{|G|^{\frac{d-1}{\nu}}}\frac{\sigma^{2}}{\epsilon^{2 }}\left(\frac{B}{\epsilon}\right)^{\frac{d-1}{\nu}}\log\frac{1}{\delta}\Bigg{)},\] (122)

This concludes the proof of Theorem 2.

The key element is the original construction of the functions \(f_{p,w,\epsilon}(x)\), which are of the form \(g(x-p)\) for some bump function constructed around the origin which _is itself symmetric_ with respect to the action of \(O(d)\) (and hence its subgroups).

Experimental details

### Synthetic experiments

We use an isotropic Matern-\(\mathbb{y}_{2}\) kernel as the base kernel \(k\), and compute \(k_{G}\) according to Equation (5). To generate the objective functions, we first generate a large number \(n\) of sample pairs \(\langle x_{i},f(x_{i})\rangle\) from a zero-mean GP prior with kernel lengthscale \(l=0.12\). We scale \(n\) with the dimensionality of the problem \(d\), so that \(n=64\) for \(d=2\), \(n=256\) for \(d=3\), and \(n=512\) for \(d=6\), to mitigate sparsity. We then use BoTorch to fit the _noise parameter_ of a noisy invariant GP with fixed lengthscale \(l=0.12\) to that data, and treat the mean of the trained GP as our objective function. In this way, we ensure that the objective function belongs to the desired RKHS.

During the BO loop, the learner was provided the true lengthscale, magnitude, and noise variance of the trained kernel, to avoid the impact of online hyperparameter learning. For UCB, we use \(\beta=2.0\), except in the quasi-invariant case with \(\varepsilon=0.05\) where we found that increasing \(\beta\) was necessary to achieve good performance (\(\beta=3.0\)). MVR has no algorithm hyperparameters.

For the constrained BO baseline, we replaced BoTorch's default initialisation heuristic with rejection sampling in order to generate multiple random restarts that lie within the fundamental domain. We found that this improved the diversity of the initial conditions, and hence boosted the performance of the acquisition function optimiser.

In all other cases BoTorch defaults were used.

### Fusion experiment

We consider the 3-block permutation group, with \(|G|=4!=24\). The kernel hyperparameters are learned offline, based on 256 runs of the fusion simulator with parameters generated by low-discrepancy Sobol sampling. Due to the cost of evaluating the objective function we use a batched version of the acquisition function as implemented by BoTorch, querying points in batches of 10. We use the multi-start optimisation routine from BoTorch for finding the maximum acquisition function value.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our theoretical results have been listed with all accompanying conditions and assumptions. Equally, our empirical results have the appropriate limitations and special settings described. Both match what is listed in the abstract. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In our 'Limitations' section (Section 5), we outline the computational cost of our method; this is the main drawback of our approach. Importantly, we provide an alternative low-cost approximation for use when the full technique is too costly. In our theoretical results, we clearly state the group actions, kernels, and other assumptions for which our theorems hold (Proposition 1, Theorem 2, Theorem 1). Following the literature, we do not include the impact of online hyperparameter learning in our theory or experiments. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.

3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide formal proofs of all theorems and propositions in the main text in the appendix: Proposition 1 has proof in Appendix A.1, Theorem 2 has proof in Appendix A.3, Theorem 1 has proof in Appendix A.2. These are correct and complete to the best of our knowledge. Where our work depends on lemmas and theorems from other papers they are clearly cited and their use in our case explained in words (e.g. Lemma 2). Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: All resources needed to reproduce our results have been provided in the code itself. The paper merely summarizes and describes the experimental settings, and all results are available directly from our code. Aside from the code, the experimental section clearly describes how the synthetic test functions are generated, and the tools used to perform BO. A complete explanation of the fusion setting is provided, and additional resources cited for more detailed information on the experiental configuration. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully.

3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide a running codebase with scripted experiment files. All will be made available online upon acceptance. We provide access to the relevant simulation environments where needed, otherwise, as our algorithms fundamentally deal with data acquisition, open access to data is not applicable. In particular, as the fusion experiment depends on proprietary code and models that belong to the United Kingdom Atomic Energy Authority, the full fusion experiment cannot be made public. We do, however, provide the optimisation script, with the calls to the proprietary models censored, so that reviewers can view our exact methodology. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All the special experimental settings needed to understand, appraise and judge the usefulness of our results have been provided, and connections to our theoretical setting have been drawn. Details needed for the implementation only are part of the code we supply; this includes exact hyperparameters and seeds used to generate each of the plots. Guidelines: * The answer NA means that the paper does not include experiments.

* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In all our experiments we provide adequate statistics except where not applicable. This includes sample sizes for experimental runs, sources of randomness and the appropriate error bars where applicable. In the fusion experiment, providing estimates of the standard deviation is not meaningful due to the presence of failed simulations, as discussed in the text. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We make special mention of the hardware we use, the computational trade-offs in various applications of our method, with quantitative data summarized in a graph. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?Answer: [Yes] Justification: We conform in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: In our conclusion we include a separate set of comments on broader impact. Our work more broadly falls into the category of foundational and theoretical research. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work poses no such risk. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.

* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite the papers for the code we use ([2, 10]), which are both available under open-source licenses. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We provide commented Python code with informative docstrings. Upon publication, we will share the full code repository under a permissive open-source license. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: [NA] Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.