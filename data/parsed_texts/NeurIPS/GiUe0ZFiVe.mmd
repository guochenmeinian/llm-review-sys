# Bi-Level Offline Policy Optimization with Limited Exploration

Wenzhuo Zhou

Department of Statistics

University of California Irvine

wenzhuz3@uci.edu

###### Abstract

We study offline reinforcement learning (RL) which seeks to learn a good policy based on a fixed, pre-collected dataset. A fundamental challenge behind this task is the distributional shift due to the dataset lacking sufficient exploration, especially under function approximation. To tackle this issue, we propose a bi-level structured policy optimization algorithm that models a hierarchical interaction between the policy (upper-level) and the value function (lower-level). The lower level focuses on constructing a confidence set of value estimates that maintain sufficiently small weighted average Bellman errors, while controlling uncertainty arising from distribution mismatch. Subsequently, at the upper level, the policy aims to maximize a conservative value estimate from the confidence set formed at the lower level. This novel formulation preserves the maximum flexibility of the implicitly induced exploratory data distribution, enabling the power of model extrapolation. In practice, it can be solved through a computationally efficient, penalized adversarial estimation procedure. Our theoretical regret guarantees do not rely on any data-coverage and completeness-type assumptions, only requiring realizability. These guarantees also demonstrate that the learned policy represents the "best effort" among all policies, as no other policies can outperform it. We evaluate our model using a blend of synthetic, benchmark, and real-world datasets for offline RL, showing that it performs competitively with state-of-the-art methods.

## 1 Introduction

Offline reinforcement learning (RL) is a task to learn a good policy using only a pre-collected, fixed dataset, without further exploration with the environment. This distinctive characteristic positions offline RL as a promising approach for solving real-world sequential decision-making problems in healthcare [35, 61], financial marketing [46], robotics [47] and education [32], as acquiring diverse or expert-quality data in these fields can be costly or practically unattainable.

Arguably, two of the biggest challenges in offline RL are the distributional shift between the data-generating distribution and those induced by candidate policies, and the stringent requirements on the properties of function approximation [28]. It has been observed that, in practice, the distributional mismatch often results in unsatisfactory performance of many existing algorithms, and even amplifying with function approximation [18, 27]. Many prior works [39, 13, 2, 14] crucially rely on a global data-coverage assumption and completeness-type function approximation condition in a technical sense. The former necessitates that the dataset to contain any state-action pair with a lower bounded probability so that the distributional shift can be well calibrated. The latter requires the function class to be closed under Bellman updates. Both assumptions are particularly strong and are likely to be violated in practice [58]. Consequently, algorithms that depend on these assumptions may experience performance degradation and instability [52]. Therefore, it is crucial to develop novel algorithms that relax these assumptions, offering robust and widely applicable solutions for real-world scenarios.

[MISSING_PAGE_FAIL:2]

effectively \(\mu\) covers the visitation induced by \(\pi\). The primary objective of offline policy optimization is to learn an optimal policy that maximizes the return, \(J(\pi)\), using the offline dataset. Under the function approximation setting, we assume access to two function classes \(\mathcal{Q}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) and \(\Omega:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\), which are utilized to capture \(q^{\pi}\) and \(\tau_{d_{\pi}/\mu}\), respectively.

**Exploration and coverage.** In general, when saying an offline dataset is well-explored, it means that a well-designed behavior policy has been executed, allowing for comprehensive exploration of the MDP environment. As a result, the dataset is likely to contain possibly all state-action pairs. This implicitly requires \(\mu\) has the global coverage [18; 49]. In this context, the global coverage means that the density ratio-based concentrability coefficient, \(\sup_{s,a}\{d_{\pi}(s,a)/\mu(s,a)\}\), is upper-bounded by a constant \(c\in\mathbb{R}^{+}\) for all policies \(\pi\in\Pi\), where \(\Pi\) is some policy class. This condition is frequently employed in offline RL [2; 8; 12]. However, in practice, this assumption may not hold true, as devising an exploratory policy is a challenging task for large-scale RL problems. Instead, our goal is to learn a good policy with strong theoretical guarantees that can compete against any arbitrarily covered comparator policy under much weaker conditions than the global coverage.

## 3 Bi-Level Offline Policy Optimization Algorithm

In this section, we introduce our bi-level offline policy optimization framework. The development of the framework consists of three major steps.

**Step 1: robust interval learning.** In this step, we aim to provide a robust off-policy interval evaluation. The major advantage of this interval formulation is its robustness to the model-misspecification of the importance-weight class \(\Omega\), and the encoding of distributional-shift information in the policy evaluation process. First, we define a detection function \(\mathbb{D}(\cdot)\), which is used to measure the degree of the distributional-shift in terms of density ratio.

**Definition 3.1**.: _For \(x,c_{1},c_{2},C\in\mathbb{R}^{+}\) and \(C\geq 1\), the detection function \(\mathbb{D}(\cdot)\) satisfies the following conditions: (1) 1-minimum: \(\mathbb{D}(1)=0\). (2) Non-negativity: \(\mathbb{D}(x)\geq 0\). (3) Boundedness on first-order derivative: \(|\mathbb{D}^{\prime}(x)|\leq c_{2}\) if \(x\in[0,C]\). (4) Boundedness on value: \(|\mathbb{D}(x)|\leq c_{1}\) for \(x\in[0,C]\). (5) Strong convexity: \(\mathbb{D}(x)\) is \(M\)-strongly convex with respect to \(x\)._

The family of Renyi entropy [42], Bhattacharyya distance [11], and simple quadratic form functions [60]all satisfy the conditions outlined in Definition 3.1. Under this definition, it can easily observe that \(\mathbb{D}\) has a convex conjugate function [6], \(\mathbb{D}_{*}\) with \(\mathbb{D}_{*}\left(x_{*}\right)=\sup_{x}\left\{x\cdot x_{*}-\mathbb{D}(x)\right\}\), that satisfies \(\mathbb{D}_{*}(0)=0\). It follows from Bellman equation \(\mathcal{B}^{\pi}q^{\pi}(s,a)=q^{\pi}(s,a)\) for any \(s,a\), then \(J(\pi)=q^{\pi}(s^{0},\pi)+\mathbb{E}_{\mu}[\lambda\mathbb{D}_{*}((\mathcal{B}^ {\pi}q^{\pi}(s,a)-q^{\pi}(s,a)/\lambda))/(1-\gamma)]\) for \(\lambda\geq 0\). Applying Fenchel-Legendre transformation [36; 21], and model \(x\) in a restricted importance weight class \(\Omega\) for any \(s,a\), we obtain

\[J(\pi)= q^{\pi}(s^{0},\pi)+\mathbb{E}_{\mu}[\sup_{x}x\cdot(\mathcal{B}^{ \pi}q^{\pi}(s,a)-q^{\pi}(s,a))-\lambda\mathbb{D}(x)]/(1-\gamma) \tag{1}\] \[\geq q^{\pi}(s^{0},\pi)+\mathbb{E}_{\mu}[\tau(s,a)(r(s,a)+\gamma q^{ \pi}(s^{\prime},\pi)-q^{\pi}(s,a))-\lambda\mathbb{D}(\tau(s,a))]/(1-\gamma). \tag{2}\]

Suppose \(q^{\pi}\) is well-specified, i.e., \(q^{\pi}\in\mathcal{Q}\), we can find a lower bound of (2), which is valid for any \(\tau\in\Omega\), via replacing \(q^{\pi}\) with \(\inf_{q\in\mathcal{Q}}\) as follows:

\[J(\pi)\geq\inf_{q\in\mathcal{Q}} \Big{\{}\underbrace{\big{(}\mathbb{E}_{\mu}\left[\tau(s,a)(r(s,a )+\gamma q\left(s^{\prime},\pi\right)-q(s,a))\right]+q(s^{0},\pi)\big{)}/(1- \gamma)}_{:=H(\tau,q,\pi)}\] \[-\underbrace{\lambda/(1-\gamma)\mathbb{E}_{\mu}[\mathbb{D}(\tau (s,a))]}_{:=\lambda\xi(\mathbb{D},\tau)}\Big{\}}.\]

After following a similar derivation, we can establish an upper bound for \(J(\pi)\) as well, and thus construct a value interval for \(J(\pi)\). This interval holds for any \(\tau\) and is therefore robust against model-misspecification of \(\Omega\). In order to obtain a tighter interval, we can shrink the interval width by maximizing the lower bound and minimizing the upper bound, both with respect to \(\tau\). This procedure can be interpreted as searching for some good \(\tau\in\Omega\) to minimize the function approximation error.

\[J(\pi)\in\left[\sup_{\tau\in\Omega}\inf_{q\in\mathcal{Q}}H(\tau,q,\pi)-\lambda \xi(\mathbb{D},\tau),\ \inf_{\tau\in\Omega}\sup_{q\in\mathcal{Q}}H(\tau,q,\pi)+ \lambda\xi(\mathbb{D},\tau)\right], \tag{3}\]

While the interval offers a robust method for dealing with the bias introduced by function approximation when estimating \(J(\pi)\), it lacks a crucial and non-trivial step for handling statistical uncertainty.

**Step 2: uncertainty quantification.** In this step, we quantify the uncertainty of the interval (3), and establish a non-asymptotic confidence interval (CI) for \(J(\pi)\) which integrates bias and uncertainty quantifications in a single interval inspired by [60]. Given offline data \(\mathcal{D}_{1:n}\), our formal result for quantifying sampling uncertainty in order to establish the CI for \(J(\pi)\).

**Theorem 3.1** (Non-asymptotic confidence interval).: _For a target policy \(\pi\), the return \(J(\pi)\) is within a CI for any \(\tau\in\Omega\) with probability at least \(1-\delta\), i.e., \(J(\pi)\in[\widehat{J}_{n}^{-}(\pi;\tau),\widehat{J}_{n}^{+}(\pi;\tau)]\) for_

\[\widehat{J}_{n}^{-}(\pi;\tau):= \frac{1}{n}\sum_{i=1}^{n}\frac{r_{i}\tau(s_{i},a_{i})}{1-\gamma}- \sup_{q\in\mathcal{Q}}\widehat{M}_{n}(-q,\tau)-\lambda\xi_{n}(\mathbb{D},\tau )-\sigma_{n},\] \[\widehat{J}_{n}^{+}(\pi;\tau):= \frac{1}{n}\sum_{i=1}^{n}\frac{r_{i}\tau(s_{i},a_{i})}{1-\gamma} +\sup_{q\in\mathcal{Q}}\widehat{M}_{n}(q,\tau)+\lambda\xi_{n}(\mathbb{D},\tau )+\sigma_{n}, \tag{4}\]

_if the uncertainty deviation \(\sigma_{n}\) satisfies_

\[P\bigg{(}\sup_{\tau\in\Omega}\Big{|}\frac{1}{n(1-\gamma)}\sum_{i=1}^{n}\tau(s_ {i},a_{i})\left(r_{i}+\gamma q^{\pi}(s_{i}^{\prime},\pi)-q^{\pi}(s_{i},a_{i}) \right)-\lambda\xi_{n}(\mathbb{D},\tau)\Big{|}\leq\sigma_{n}\bigg{)}\geq 1-\delta,\]

_where \(\widehat{M}_{n}(q,\tau):=\sum_{i=1}^{n}\tau(s_{i},a_{i})(\gamma q(s_{i}^{\prime },\pi)-q(s_{i},a_{i}))/(1-\gamma)n+q(s^{0},\pi)\)._

Similar to the value interval, the CI \([\widehat{J}_{n}^{-}(\pi;\tau),\widehat{J}_{n}^{+}(\pi;\tau)]\) also holds for any \(\tau\in\Omega\). Therefore, we can optimize the confidence lower and upper bounds in (4) over \(\tau\in\Omega\) to tighten the CI, and obtain:

\[P\Big{(}J(\pi)\in[\sup_{\tau\in\Omega}\widehat{J}_{n}^{-}(\pi;\tau),\inf_{\tau \in\Omega}\widehat{J}_{n}^{+}(\pi;\tau)]\subseteq[\widehat{J}_{n}^{-}(\pi;\tau ),\widehat{J}_{n}^{+}(\pi;\tau)\Big{)}\geq 1-\delta.\]

**Step 3: bridge policy evaluation to policy optimization.** In this step, we aim to formulate a policy optimization based on the derived high-confidence policy evaluation from the previous steps. Given the consistent CI estimation of \(J(\pi)\), we can naturally incorporate the pessimism principle, i.e., using the CI lower bounds of \(J(\pi)\) as the value estimate of the policy evaluation of \(\pi\)[22]. With such a procedure, our objective is to maximize these lower bounds over some family \(\Pi\) of policies:

\[\max_{\pi\in\Pi}\left\{\sup_{\tau\in\Omega}\widehat{J}_{n}^{-}(\pi;\tau) \right\}. \tag{5}\]

Although (5) is algorithmically feasible for obtaining a policy solver \(\widehat{\pi}\), it lacks direct interpretation without taking advantage of the bi-level optimization structure in hindsight. Therefore, we propose to reformulate (5) via a _dual-to-prime conversion_ (shown in Theorem 3.2), which naturally lends itself to lower-upper optimization with guaranteed convergence. Specifically, we formulate (5) as a bi-level framework problem:

\[(\text{Upper Level}) \min_{\pi\in\Pi}-q^{\pi}(s^{0},\pi), \tag{6}\] \[(\text{Lower Level}) s.t.\ \underline{q^{\pi}}\in\operatorname*{arg\,min}_{q\in \mathcal{Q}_{\pi_{n}}}q(s^{0},\pi),\] (7) \[\underline{\textbf{Consistency}}: \mathcal{Q}_{\varepsilon_{n}}=\big{\{}q\in\mathcal{Q}:\sup_{\tau \in\widehat{\Omega}_{\widetilde{\sigma}_{n}}}\big{|}n^{-1}\sum_{i=1}^{n}\tau( s_{i},a_{i})(r_{i}+\gamma q(s_{i}^{\prime},\pi)-q(s_{i},a_{i}))\big{|}\leq \varepsilon_{n}\big{\}},\] \[\underline{\textbf{Uncertainty Control}}: \widetilde{\Omega}_{\widetilde{\sigma}_{n}}=\left\{\tau_{\circ}/ \sup_{\tau_{\circ}\in\Omega}\|\tau_{\circ}\|_{\Omega}\text{ for }\tau_{\circ}\in\Omega:\xi_{n}( \mathbb{D},\tau_{\circ})\right)\leq\widetilde{\sigma}_{n}\right\}.\]

At the upper level, the learned policy \(\widehat{\pi}\) attempts to maximize the value estimate of \(q^{\pi}\) over some policy class \(\Pi\), while at the lower level, \(q^{\pi}\) is to seek the \(q\)-function with the pessimistic policy evaluation value from the confidence set \(\overline{\mathcal{Q}_{\varepsilon_{n}}}\) with consistency guarantee and uncertainty control. For _consistency_, whenever \(q^{\pi}\) or its good approximator is included in \(\mathcal{Q}\) (realizability for \(\mathcal{Q}\) class is satisfied), the set \(\mathcal{Q}_{\varepsilon_{n}}\) ensures the estimation consistency of \(q^{\pi}\) in terms of "sufficently small" weighted average Bellman error. For _uncertainty control_, the constrained set \(\widetilde{\Omega}_{\widetilde{\sigma}_{n}}\) attempts to control the uncertainty arising from distributional shift via a user-specific thresholding hyperparameter \(\widetilde{\sigma}_{n}\). The feasible (uncertainty controllable) candidates \(\tau\in\widetilde{\sigma}_{n}\) are used as weights for the average Bellman error, helping to construct the consistent set \(\mathcal{Q}_{\varepsilon_{n}}\). Risk-averse users can specify a lower value for the thresholding hyperparameter or consider a higher \(\widetilde{\sigma}_{n}\) to tolerate a larger distribution shift. In other words, the chosen value of \(\widetilde{\sigma}_{n}\) depends on the degree of pessimism users want to incorporate in the policy optimization.

[MISSING_PAGE_FAIL:5]

_Here \(\Delta_{\overline{q}^{\pi}-q^{\pi}}(s,a)=\overline{q^{\pi}}(s,a)=\underline{q^{\pi} }(s,a)\) for \(\overline{q^{\pi}}:=\arg\max_{q\in\mathcal{Q}_{\pi_{a}}}q(s^{0},\pi)\) and \(\underline{q^{\pi}}:=\arg\min_{q\in\mathcal{Q}_{\pi_{a}}}q(s^{0},\pi)\). For Pollard's pseudo-dimensions \(D_{\Omega},D_{\mathcal{Q}},D_{\Pi}\), \(\operatorname{Vol}(\Theta)=(e^{D}\max\{D_{\Omega},D_{\mathcal{Q}},D_{\Pi}\}+1 )^{3}((1\lor L)\mathcal{U}_{2}^{*})^{2D}\) with the effective pseudo dimension \(D=D_{\Omega}+D_{\mathcal{Q}}+D_{\Pi}\), where \(L\) is Lipschitz constant of \(M\)-strongly convex function \(\mathbb{D}(\cdot)\). Moreover, \(\mathfrak{C}_{x}\) and \(\widetilde{\mathcal{O}}\) denote constant terms depending on \(x\), and big-Oh notation ignoring high-order terms, respectively._

In the upper bound of Theorem 4.1, we split the regret into four different parts: the on-support intrinsic uncertainty \(\epsilon_{\sigma}\), the on-support bias \(\epsilon_{b}\), the violation of realizability \(\epsilon_{\text{mis}}\), and the off-support extrapolation error \(\epsilon_{\text{off}}\). Recall that we require \(q^{\pi}\in\mathcal{Q}\) as in Assumption 1, in fact, we can further relax the condition to requiring \(q^{\pi}\) to be in the linear hull of \(\mathcal{Q}\)[48], which is more robust to the realizability error \(\epsilon_{\text{mis}}\). In the following, we focus on investigating the roles of the on-support and off-support error terms in the regret bound.

**On-support errors: bias and uncertainty tradeoff.** The on-support error consists of two terms: \(\epsilon_{b}\) and \(\epsilon_{\sigma}\). The on-support uncertainty deviation, \(\epsilon_{\sigma}\), is scaled by a weighted \(L_{2}\)-based concentrability coefficient \(\mathcal{U}_{2}^{*}:=\left\lVert\rho/\mu\right\rVert_{L_{2}(\mu)}\), which measures the distribution mismatch between the implicit exploratory data distribution and the baseline data distribution \(\mu\). Meanwhile, \(\epsilon_{b}\) depends on the probability mass of \((d_{\pi}-\rho)^{+}\mathds{1}_{\mu>0}\), and represents the bias weighted by the probability mass difference between \(d_{\pi}\) and \(\rho\) in the support region of \(\mu\). In general, a small value of \(\mathcal{U}_{2}^{*}\) necessitates choice of the distribution \(\rho\) to be closer to \(\mu\) which reduces \(\epsilon_{\sigma}\), reducing \(\epsilon_{\sigma}\) but potentially increasing the on-support bias \(\epsilon_{b}\) due to the possible mismatch between \(d_{\pi}\) and \(\rho\). Consequently, within the on-support region, there is a trade-off between \(\epsilon_{\sigma}\) and \(\epsilon_{b}\), which is adjusted through \(\mathcal{U}_{2}^{*}\).

**Off-support error: enhanced model extrapolation.** One of our main algorithmic contributions is that the off-support extrapolation error \(\epsilon_{\text{off}}\) can be minimized by selecting the best possible \(\rho\)_without_ worrying about balancing the error trade-off, unlike the on-support scenario. This desirable property is essential for allowing the model to harness its extrapolation capabilities to minimize \(\epsilon_{\text{off}}\), while simultaneously achieving a good on-support estimation error. As a result, the model attains a small regret. Recall the bi-level formulation; at the lower level, (7) addresses uncertainty arising from the distributional shift using \(L_{2}(\mu)\) control rather than \(L_{\infty}\) control. This plays an important role in enhancing the power of the model extrapolation. In particular, Specifically, there exists an implicit exploratory data distribution \(\rho\) with on-support behavior (\(\rho\mathds{1}_{\mu>0}\)) close to \(\mu\), such that \(\left\lVert\rho/\mu\right\rVert_{L_{2}(\mu)}\) is small. On the other hand, its off-support behavior (\(\rho\mathds{1}_{\mu=0}\)) can be arbitrarily flexible, ensuring that \(d_{\pi}\mathds{1}_{\mu=0}\) is close to \(\rho\mathds{1}_{\mu=0}\). Consequently, \((d_{\pi}-\rho)^{+}\mathds{1}_{\mu=0}\) is small, as is \(\epsilon_{\text{off}}\).

When a dataset with partial coverage, as indicated in [49], it is necessary to provide a guarantee: learn the policy with "best efforts" which is competitive to any policy as long as it is covered. Before we state the near-optimal regret guarantee of our algorithm, we formally define a notion of covered policies according to a newly-defined concentrability coefficient.

**Definition 4.1** (\(\mathcal{U}_{2}^{*}\)-covered policy class).: _Let \(\Pi(\mathcal{U}_{2}^{*})\) denote the \(\mathcal{U}_{2}^{*}\)-covered policy class of \(\mu\) for \(\mathcal{U}_{2}^{*}\geq 1\), defined as_

\[\Pi(\mathcal{U}_{2}^{*}):=\left\{\pi\in\Pi:\left\lVert\frac{d_{\pi}(s,a) \mathds{1}_{\mu(s,a)>0}}{\mu(s,a)}\right\rVert_{L_{2}(\mu)}\leq\mathcal{U}_{2}^ {*}\text{ and }\sup_{s,a}\frac{d_{\pi}(s,a)\mathds{1}_{\mu(s,a)=0}}{\mu(s,a)}<+ \infty\right\}.\]

Note that this mixture density ratio concentrability coefficient is always bounded by the \(L_{\infty}\)-based concentrability coefficient. Thus such single-policy concentrability assumption in terms of the mixture density ratio is weaker than the standard \(L_{\infty}\) density ratio-based assumption.

**Corollary 4.1** (Near-optimal regret).: _Under Assumptions 1-3 with \(\varepsilon_{\mathcal{Q}}\in[0,1)\), and we set \(\varepsilon_{n},\widetilde{\sigma}_{n}\) as in Theorem 4.1, then for any good comparator policy \(\pi^{\circ}\in\Pi(\mathcal{U}_{2}^{*})\) (not necessary the optimal policy \(\pi^{*}\)), w.p. \(\geq 1-\delta\), the output policy \(\widetilde{\pi}\) of (6) satisfies_

\[J(\pi^{\circ})-J(\widetilde{\pi})\leq\frac{1}{1-\gamma}\widetilde{\mathcal{O}} \Bigg{(}\mathcal{U}_{2}^{*}(\bar{V}+L)\sqrt{\frac{\ln\{\operatorname{Vol}( \Theta)/\delta\}}{nM}}+\sqrt{(1+\mathcal{U}_{\infty}^{*}+\mathcal{U}_{\infty}^ {*}/M)\,\varepsilon_{\mathcal{Q}}}\Bigg{)}.\]

A close prior result to Corollary 4.1 is that of [9], which develops a pessimistic algorithm based on a nontrivial performance gap condition. Their regret guarantees only hold if the data covers the optimal policy \(\pi^{*}\), in particular, requiring a bounded \(L_{\infty}\) single-policy concentrability with respect to \(\pi^{*}\). In comparison, our guarantee can still provide a meaningful guarantee even when \(\pi^{*}\) is not covered by data. In the following, we include the sample complexity of our algorithm when \(\varepsilon_{\mathcal{Q}}=0\).

**Corollary 4.2** (Polynomial sample complexity).: _Under the conditions in Corollary 4.1, the output policy \(\widehat{\pi}\) of solving (6) satisfies \(J(\pi^{\circ})-J(\widehat{\pi})\leq\varepsilon\) w.p. \(\geq 1-\delta\), if_

\[n=\mathcal{O}\bigg{(}\bigg{(}\frac{(\mathcal{U}_{2}^{\star}(\bar{V}+L)/\sqrt{M} )^{2}}{\varepsilon^{2}(1-\gamma)^{2}}+\frac{(\mathcal{U}_{2}^{\star}\bar{V}^{ 2}(\bar{V}+L)/M)^{0.67}}{\varepsilon^{1.33}(1-\gamma)^{1.33}}+\frac{\mathcal{ U}_{\infty}^{\star}(\bar{V}+L)}{\varepsilon(1-\gamma)}\bigg{)}\ln\frac{\mathrm{ Vol}(\Theta)}{\delta}\bigg{)}.\]

The sample complexity consists of three terms corresponding to the slow rate \(\mathcal{O}(n^{-1/2})\) and the two faster rate \(\mathcal{O}(n^{-1})\) and \(\mathcal{O}(n^{-3/4})\) terms in Corollary 4.1. When \(\mathcal{U}_{2}^{\star}\) and \(\mathcal{U}_{\infty}^{\star}\) are not too much larger than \(\mathcal{U}_{2}^{\star}\), the fast rate terms are dominated, and the sample complexity is of order \(\mathcal{O}(1/\varepsilon^{2})\), which is much faster than \(\mathcal{O}(1/\varepsilon^{6})\) in the close work of [59]. It is worth noting that even in exploratory settings where the global coverage assumption holds, our sample complexity rate matches the fast rate in popular offline RL frameworks with general function approximation [8; 54; 12].

In addition to the near-optimal regret guarantee, in safety-critical applications, an offline RL algorithm should consistently improve upon the baseline (behavior) policies that collected the data [19; 26]. Our algorithm also achieves this improvement guarantee with respect to the baseline policy.

**Theorem 4.2** (Baseline policy improvement).: _Under Assumptions 1-3 with \(\varepsilon_{\mathcal{Q}}=0\) and set \(\varepsilon_{n},\widetilde{\sigma}_{n}\) as in Theorem 4.1. Suppose \(1\in\Omega\) and the baseline policy \(\pi_{b}\in\Pi\) such that \(d_{\pi_{b}}=\mu\), then the regret \((1-\gamma)(J(\pi_{b})-J(\widehat{\pi}))\) for the output policy \(\widehat{\pi}\) of solving (6), w.p. \(\geq 1-\delta\), is upper bounded by_

\[\mathcal{O}\bigg{(}\sqrt{\frac{(\bar{V}+L)^{2}\ln\{\mathrm{Vol}(\Theta)/ \delta\}}{nM}}+\sqrt{\frac{(\bar{V}^{3}+\bar{V}^{2}L)}{M}}\bigg{(}\frac{\ln\{ \mathrm{Vol}(\Theta)/\delta\}}{n}\bigg{)}^{\frac{3}{4}}+\frac{(\bar{V}+L)\ln \{\mathrm{Vol}(\Theta)/\delta\}}{n}\bigg{)}.\]

The aforementioned information-theoretic results enhance the understanding of the developed algorithm, in terms of the function approximation and coverage conditions, sample complexity, horizon dependency, and bound tightness. In practice, although the information-theoretic algorithm offers a feasible solution to the problem, it is not yet tractable and computationally efficient due to the need to solve constrained optimization. In the following section, we develop a practical algorithm as a computationally efficient counterpart for the information-theoretic algorithm.

## 5 Penalized Adversarial Estimation Algorithm

Although the information-theoretic algorithm offers a feasible solution to the problem, it is not yet tractable and computationally efficient due to the need to solve constrained optimization. In this section, we develop an adversarial estimation proximal-mapping algorithm that still adheres to the pessimism principle, but through penalization. Specifically, the adversarial estimation loss is constructed as follows: \(\underset{\tau}{\mathrm{maxim}}\mathcal{L}(q,\tau,\pi,c^{\star},\lambda)\) for solving

\[q(s^{0},\pi)+\frac{1}{(1-\gamma)n}\left\{c^{\star}\Big{|}\sum_{i=1}^{n}\tau(s_ {i},a_{i})\left(q(s_{i},a_{i})-r_{i}-\gamma q(s^{\prime}_{i},\pi)\right)\Big{|} -\lambda\sum_{i=1}^{n}\mathbb{D}(\tau(s_{i},a_{i}))\right\}.\]

We observe that the inner minimization for solving \(q\) is relatively straightforward, as we can obtain a closed-form global solver using the maximum mean discrepancy principle [20; 44]. In contrast, optimizing \(\tau_{\psi}\) is more involved, often requiring a sufficiently expressive non-linear function approximation class, e.g., neural networks. However, concavity typically does not hold for such a class of functions [21]. From this perspective, our problem can be viewed as solving a non-concave maximization problem, conditional on the solved global optimizer \(\bar{q}:=\arg\min_{g}\mathcal{L}(q,\tau,\pi,c^{\star},\lambda)\). At each iteration, we propose to update \(\tau\) by solving the proximal mapping [37] using the Euclidean distance to reduce the computational burden. As a result, the pre-iteration computation is quite low.

```
1:Input observed data \(\mathcal{D}_{1:n}=\{(s_{i},a_{i},r_{i},s^{\prime}_{i})\}_{i=1}^{n}\) and parameters \(q^{0},\tau^{0},\pi^{0},c^{\star}\), \(\lambda\) and \(\zeta\).
2:For\(k=1\) to \(\bar{K}\):
3: Update \(\tau^{k}\) and \(q^{k}\) by solving \(\underset{\tau}{\mathrm{maxim}}\underset{q}{\mathrm{maxim}}\mathcal{L}(q,\tau, \pi^{k-1},c^{\star},\lambda)\)
4: Update \(\pi^{k}\) by solving \(\pi^{k}(\cdot|s)=\underset{\pi\in\Pi}{\mathrm{argmax}}\zeta\left\langle q^{k}( \cdot,s),\pi(\cdot|s)\right\rangle-D_{\mathrm{NegEntropy}}\left(\pi(\cdot|s), \pi^{k}(\cdot|s)\right).\)
5:Return the policy \(\widehat{\pi}\), which randomly selects a policy from the set \(\{\pi^{k}\}_{k=1}^{\bar{K}}\).
```

**Algorithm 1** Adversarial proximal-mapping algorithmOnce \(q\) and \(\tau\) are solved, we apply mirror descent in terms of the negative entropy \(D_{\text{NegEntropy}}\)[5]. That is, given a stochastic gradient direction of \(\pi\) we solve the prox-mapping in each iteration as outlined in step 4 of Algorithm 1. A detailed version of Algorithm 1 with extended discussions on convergence and complexity is provided in Appendix. In the following, we establish the regret guarantee for the policy output by Algorithm 1.

**Theorem 5.1**.: _Under Assumptions 1-3 with \(\varepsilon_{\mathcal{Q}}=0\), we properly choose \(\lambda=\lambda(\mathcal{U}_{2}^{\tau})\), i.e., \(\lambda\) well depends on \(\mathcal{U}_{2}^{\tau}\), and \(c^{*}=\widetilde{\mathcal{O}}\big{(}\sqrt{nV/(\lambda\mathcal{U}\mathcal{U}_{ 2}^{\tau}\ln\{\operatorname{Vol}(\Theta^{\dagger})/\delta\}}\big{)}\big{)}\). After running \(\bar{K}\geq\log|\mathcal{A}|\) rounds of Algorithm 1 with the stepsize \(\zeta=\sqrt{\log|\mathcal{A}|/(2V\bar{K})}\), for any policy \(\pi\in\Pi\), the output policy \(\widehat{\pi}\) of the algorithm, w.p \(\geq 1-\delta\), satisfies,_

\[J(\pi)-J(\widehat{\pi})\leq\ \frac{1}{1-\gamma}\widetilde{ \mathcal{O}}\bigg{(}\sqrt{\frac{d\big{(}\mathcal{U}_{2}^{*}\big{)}^{2}\mathfrak{ C}_{V,\lambda,L}^{1}\ln\{\operatorname{Vol}(\Theta^{\dagger})/\delta\}}{n}}+\sqrt{ \frac{\bar{V}\log|\mathcal{A}|}{\bar{K}}}\\ +\frac{1}{K}\sum_{k=1}^{\bar{K}}\min_{\rho_{k}\in\Delta_{\mathcal{ U}_{2}}}\mathbb{E}_{(d_{\pi}-\rho_{k})^{+}}\bigg{[}\mathds{1}_{\mu=0}\left( \mathcal{B}^{\pi^{k}}q^{k}(s,a)-q^{k}(s,a)\right)+\mathds{1}_{\mu>0}\sqrt{ \frac{\mathfrak{C}_{V,\lambda,L}^{2}\ln\{\operatorname{Vol}(\Theta^{\dagger})/ \delta\}}{n}}\bigg{]}\bigg{)},\]

_where \(\Delta_{\mathcal{U}_{2}^{*}}:=\{\rho_{k}:\|\frac{\rho_{k}}{\mu}\|_{L_{2}(\mu)}< \mathcal{U}_{2}^{*}\}\), \(\mathfrak{C}_{V,\lambda,L}^{1},\mathfrak{C}_{V,\lambda,L}^{2}\) are some constant terms, and the function class complexity \(\operatorname{Vol}(\Theta^{\dagger})=(e^{D}\max\{D_{\Omega},D_{\mathcal{Q}},D_ {\Pi}\}+1)^{3}(\{1\lor L\}\mathcal{U}_{2}^{\tau})^{2D}\) for \(D=D_{\Omega}+D_{\mathcal{Q}}+D_{\Pi}\)._

**Trajectory-adaptive exploratory data distribution.** Similar to Theorem 4.1, the penalized algorithm also exhibits a desirable extrapolation property for minimizing extrapolation error while simultaneously preserving small on-support estimation errors. This is achieved through adaptations of the implicit exploratory data distributions, \(\rho_{k}\) for \(k\in[\bar{K}]\). In contrast to the information-theoretic algorithm, the automatic splitting by \(\rho_{k}\) now depends on the optimization trajectory. At each iteration \(k\), the penalized algorithm allows each implicit exploratory data distribution \(\rho_{k}\) to adapt to the comparator policy \(\pi\). This results in a more flexible adaptation than the one in the information-theoretic algorithm, either for balancing the trade-off between on-support bias and uncertainty incurred by the distributional mismatch between \(d_{\pi}\) and \(\rho_{k}\), or for selecting the best implicit exploratory to minimize model extrapolation error.

**Optimization error.** Blessed by the reparametrization in the proximal-mapping policy update, which projects the mixture policies into the parametric space \(\Pi_{\omega}\), the complexity of the restricted policy class is independent of the class of \(\mathcal{Q}\) and the horizon optimization trajectory \(\bar{K}\). As a result, the optimization error \(\mathcal{O}(\sqrt{V\log|\mathcal{A}|/\bar{K}})\) can be reduced arbitrarily by increasing the maximum number of iterations, \(\bar{K}\), without sacrificing overall regret to balance statistical error and optimization error. This allows for the construction of tight regret bounds. This distinguishes our algorithm from API-style algorithms, which do not possess a policy class that is independent of \(\mathcal{Q}\)[2, 43, 53].

### An Application to Linear MDPs with Refined Concentrability Coefficient

In this section, we conduct a case study in linear MDPs with insufficient data coverage. The concept of the linear MDP is initially developed in the fully exploratory setting [55]. Let \(\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\) be a \(d\)-dimensional feature mapping. We assume throughout that these feature mappings are normalized, such that \(\|\phi(s,a)\|_{L_{2}}\leq 1\) uniformly for all \((s,a)\in\mathcal{S}\times\mathcal{A}\). We focus on action-value functions that are linear in \(\phi\) and consider families of the following form: \(\mathcal{Q}_{\theta}:=\{(s,a)\mapsto\langle\phi(s,a),\theta\ |\ \|\theta\|_{L_{2}} \leq c_{\theta}\}\), where \(c_{\theta}\in[0,\bar{V}]\). For stochastic policies, we consider the soft-max policy class \(\Pi_{\omega}:=\{\pi_{\omega}(a|s)\propto e^{\langle\phi(s,a),\omega\rangle}\ | \ \|\omega\|_{L_{2}}\leq c_{\omega}\},\) where \(c_{\omega}\in(0,\infty)\). Note that the softmax policy class is consistent with the implicit policy class produced by the mirror descent updates with negative entropy in Algorithm 1, where the exponentiated gradient update rule is applied in each iteration. For the importance-weight class, we also consider the following form: \(\Omega_{\psi}:=\{(s,a)\mapsto\langle\phi(s,a),\psi\rangle\ |\ \|\psi\|_{L_{2}} \leq c_{\psi}\}\) where \(c_{\psi}\in(0,\infty)\). To simplify the analysis, we assume the realizability condition for \(\mathcal{Q}_{\theta}\) is exactly met. In this linear MDP setting, we further refine the density ratio to a relative condition number to characterize partial coverage. This concept is recently introduced in the policy gradient literature [1] and is consistently upper-bounded by the \(L_{\infty}\)-based density ratio concentrability coefficient.

**Definition 5.1** (Relative condition number).: _For any policy \(\pi\in\Pi_{\omega}\) and behavior policy \(\pi_{b}\) such that \(d_{\pi_{b}}=\mu\), the relative condition number is defined as \(\iota(d_{\pi},\mu)=\sup_{x\in\mathbb{R}^{d}}\frac{x^{T}\mathbb{E}_{d_{\pi}}[ \phi(s,a)\phi(s,a)^{\top}]x}{x^{\top}\mathbb{E}_{\sigma}[\phi(s,a)\phi(s,a)^{ \top}]x}\)._

**Assumption 4** (Bounded relative condition number).: _For any \(\pi\in\Pi_{\omega}\), \(\iota(d_{\pi},\mu)<\infty\)._Intuitively, this implies that as long as a high-quality comparator policy exists, which only visits the subspace defined by the feature mapping \(\phi\) and is covered by the offline data, our algorithm can effectively compete against it [49]. This partial coverage assumption, in terms of the relative condition number, is considerably weaker than density ratio-based assumptions. In the following, we present our main near-optimal guarantee in linear MDPs. In addition, we design and conduct numerical experiments to empirically validate Theorem 5.2 in terms of the regret rate of convergence.

**Theorem 5.2**.: _Under Assumption 4, if we set property choose \(\lambda=\lambda(c_{\psi})\) and \(c^{*}=\widetilde{\mathcal{O}}\big{(}\sqrt[4]{n/d}\ln\{(1+e\sqrt{n}(1\lor L)Vc_ {\psi}c_{\omega})/\delta\}\big{)}\), and suppose \(\widehat{\pi}^{\text{tr}}\) is returned by Algorithm 1 with linear function approxiamiton after running \(\tilde{K}\gg\log|\mathcal{A}|\) rounds, then for any policy in \(\pi\in\Pi_{\omega}(\mathcal{U}_{2}^{tr})\) for \(\mathcal{U}_{2}^{tr}\geq 1\), w.p. \(\geq 1-\delta\), \(J(\pi)-J(\widehat{\pi}^{\text{tr}})\) is bounded by_

\[\widetilde{\mathcal{O}}\bigg{(}\frac{\sqrt{\min\{\kappa^{2}c_{\psi}^{2}\{ \mathcal{U}_{2}^{tr}\}d^{2},\iota(d_{\pi},\mu)d\}}}{1-\gamma}\sqrt[4]{\frac{ \mathfrak{C}_{\tilde{V},\lambda,L}d\ln\{(1+e\sqrt{n}(1\lor L)\tilde{V}c_{\psi} c_{\omega})/\delta\}}{n}}\bigg{)},\]

_where \(\kappa=\text{trace}(\mathbb{E}_{\mu}[\phi(s,a)\phi(s,a)^{\top}])\) and \(c_{\psi}\{\mathcal{U}_{2}^{tr}\}=\sup_{\{\psi:\|\phi(s,a)^{\top}\psi\|_{L^{2}( \mu)}=\mathcal{U}_{2}^{tr}\}}\|\psi\|_{L_{\infty}}\)._

To the best of our knowledge, this is the first result PAC guarantees for an offline model-free RL algorithm in linear MDPs, requiring only realizability and single-policy concentrability. The regret bound we obtain is at least linear and, at best, sub-linear with respect to the feature dimension \(d\). Our approach demonstrates a sample complexity improvement in terms of feature dimension compared to prior work by [22], with a complexity of \(\mathcal{O}(d^{1/2})\) versus \(\mathcal{O}(d)\). It is worth noting that [22] only establishes results that compete with the optimal policy, and when specialized to linear MDPs, assumes the offline data has global coverage. Another previous study by [53] achieves a similar sub-linear rate in \(d\) as our approach; however, their algorithm is computationally intractable, relying on a much stronger Bellman-completeness assumption and requiring a small action space.

## 6 Experiments

In this section, we evaluate the performance of our practical algorithm by comparing to the model-free offline RL baselines including CQL [25], BEAR [24], BCQ [18], OptiDICE [27], ATAC [10], IQL [23], and TD3+BC [17]. We also compete with a popular model-based approach COMBO [57].

**Synthetic data.** We consider two synthetic environments: a synthetic CartPole environment from the OpenAI Gym [7] and a simulated environment. Detailed discussions on the experimental designs are deferred to the Appendix. In both settings, following [48], we first learn a sub-optimal policy using DQN [34] and then apply softmax to its \(q\)-function, divided by a temperature parameter \(\alpha\) to set the action probabilities to define a behavior policy \(\pi_{b}\). A smaller \(\alpha\) implies \(\pi_{b}\) is less explored, and thus the support of \(\mu=d_{\pi_{b}}\) is relatively small. We vary different values of \(\alpha\) for evaluating the algorithm performance in "low", "medium" and "relatively high" offline data exploration scenarios. We use \(\gamma=0.95\) with the sample-size \(n=1500\) in all experiments. Tuning parameter selection is an open problem in offline policy optimization. Fortunately, Theorem 5.2 suggests an offline selection rule for hyperparameters \(\lambda\) and \(c^{*}\). In the following experiments, we set the hyper-parameters satisfying the condition \(\mathcal{O}(\frac{n^{1/4}}{d\log(\sqrt{n})})\). Figure 1 shows that our algorithm almost consistently outperforms competing methods in different settings. This performance mainly benefits from the advantages exposed in our theoretical analysis, such as model extrapolation enhancement, relaxation of completeness-type assumptions on function approximation, etc. The only exception is the slightly poorer performance compared to COMBO in a high exploration setting, where COMBO may learn a good dynamic model with relatively sufficient exploration. We provide the experiment details in Appendix due to page limit.

**Benchmark data.** We evaluate our proposed approach on the D4RL benchmark of OpenAI Gym locomotion (walker2d, hopper, halfcheetah) and Maze2D tasks [15], which encompasses a variety

Figure 1: The boxplot of the discounted return over \(50\) repeated experiments.

of dataset settings and domains and positions our algorithm within the existing baselines. We take the results of COMBO, OptiDICE and ATAC from their original papers for Gym locomotion, and run COMBO and ATAC using author-provided implementations for Maze2D. The results of BCQ, BEAR methods from the D4RL original paper. In addition, CQL, IQL and TD3+BC are re-run to ensure a fair evaluation process for all tasks. As shown in Table 1, the proposed algorithm achieves the best performance in 7 tasks and is comparable to the baselines in the remaining tasks. In addition to the evaluation of the policy performance, we also conduct sensitivity analyses on the hyperparameter-tuning and study the regret rate of convergence.

**Real-world application.** The Ohio Type 1 Diabetes (OhioT1DM) dataset [33] comprises a cohort of patients with Type-1 diabetes, where each patient exhibits different dynamics and 8 weeks of life-event data, including health status measurements and insulin injection dosages. Clinicians aim to adjust insulin injection dose levels [33, 4] based on a patient's health status in order to maintain glucose levels within a specific range for safe dose recommendations. The state variables consist of health status measurements, and the action space is a bounded insulin dose range. The glycemic index serves as a reward function to assess the quality of dose suggestions. Since the data-generating process is unknown, we follow [31, 29] to utilize the Monte Carlo approximation of the estimated value function on the initial state of each trajectory to evaluate the performance of each method. The mean and standard deviation of the improvements on the Monto Carlo discounted returns are presented in Table 2. As a result, our algorithm achieves the best performance for almost all patients, except for Patient \(552\). The main reason for the desired performance in real data is from the enhanced model extrapolation and relaxed function approximation requirements and outperforms the competing methods. This finding is consistent with the results in the synthetic and benchmark datasets, demonstrating the potential applicability of the proposed algorithm in real-world environments.

## 7 Conclusion

We study offline RL with limited exploration in function approximation settings. We propose a bi-level policy optimization framework, which can be further solved by a computationally practical penalized adversarial estimation algorithm, offering strong theoretical and empirical guarantees. Regarding limitations and future work, while the penalized adversarial estimation is more computationally efficient than the previously constrained problem, it may still be more challenging to solve than single-stage optimization problems. Another future direction is to explore environments with unobservable confounders. It will be interesting to address these limitations in future works.

\begin{table}
\begin{tabular}{c|c c c c c c c c c} \hline \hline Tasks & Proposed & COMBO & BCQ & BEAR & OptiDICE & ATAC & CQL & IQL & TD3+BC \\ \hline walker2d-med & \(80.8\pm 5.1\) & \(81.9\pm 2.8\) & \(53.1\) & \(59.1\) & \(21.8\pm 7.1\) & \(\mathbf{89.6}\) & \(77.2\pm 4.2\) & \(78.3\pm 4.3\) & \(81.7\pm 2.3\) \\ hopper-med & \(94.9\pm 4.3\) & \(97.2\pm 2.2\) & \(54.5\) & \(52.1\) & \(94.1\pm 3.7\) & \(85.6\) & \(74.3\pm 5.8\) & \(66.3\pm 6.4\) & \(\mathbf{98.4}\pm 1.6\) \\ halfcheetah-med & \(\mathbf{58.1\pm 1.4}\) & \(54.2\pm 1.5\) & \(40.7\) & \(41.7\) & \(38.2\pm 0.1\) & \(53.3\) & \(37.2\pm 0.3\) & \(47.4\pm 1.1\) & \(27.8\pm 0.7\) \\ \hline walker2d-med & \(\mathbf{99.6\pm 2.9}\) & \(56.0\pm 8.6\) & \(15.0\) & \(19.2\) & \(21.6\pm 2.1\) & \(92.5\) & \(20.8\pm 1.6\) & \(73.9\pm 2.8\) & \(34.4\pm 4.2\) \\ hopper-med-rep & \(\mathbf{113.0\pm 2.1}\) & \(89.5\pm 1.8\) & \(33.1\) & \(33.7\) & \(33.7\) & \(1.1\) & \(102.5\) & \(32.6\pm 1.9\) & \(94.7\pm 1.5\) & \(44.4\pm 3.7\) \\ halfcheetah-med-rep & \(49.3\pm 2.1\) & \(\mathbf{55.1\pm 1.0}\) & \(38.2\) & \(38.6\) & \(39.8\pm 0.3\) & \(48.0\) & \(41.9\pm 1.1\) & \(44.2\pm 2.5\) & \(48.3\pm 0.7\) \\ \hline walker2d-med & \(\mathbf{103.2\pm 7.4}\) & \(103.3\pm 5.6\) & \(57.5\pm 0.1\) & \(74.8\pm 9.2\) & \(114.2\) & \(103.8\pm 6.90\) & \(109.6\pm 7.0\) & \(100.5\pm 8.9\) \\ hopper-med-exp & \(117.8\pm 1.9\) & \(111.1\pm 2.1\) & \(110.9\) & \(96.3\) & \(111.5\pm 0.6\) & \(\mathbf{119.2}\) & \(111.4\pm 12.9\) & \(91.5\pm 2.2\) & \(112.4\pm 0.3\) \\ halfcheetah-med-exp & \(\mathbf{98.5\pm 3.8}\) & \(90.0\pm 5.6\) & \(64.7\) & \(53.4\) & \(91.1\pm 3.7\) & \(94.8\) & \(66.7\pm 8.9\) & \(86.7\pm 3.6\) & \(95.9\pm 3.9\) \\ \hline walker2d-random & \(\mathbf{11.2\pm 3.8}\) & \(7.0\pm 3.6\) & \(4.9\) & \(7.3\) & \(9

## 8 Acknowledgments

The author is grateful to the five anonymous reviewers and the area chair for their valuable comments and suggestions.

## References

* Agarwal et al. [2020] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation with policy gradient methods in markov decision processes. In _Conference on Learning Theory_, pages 64-66. PMLR, 2020.
* Antos et al. [2008] Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path. _Machine Learning_, 71(1):89-129, 2008.
* Baird [1995] Leemon Baird. Residual algorithms: Reinforcement learning with function approximation. In _Machine Learning Proceedings 1995_, pages 30-37. Elsevier, 1995.
* Bao et al. [2011] Jiansong Bao, Heather R Gilbertson, Robyn Gray, Diane Munns, Gabrielle Howard, Peter Petocz, Stephen Colagiuri, and Jennie C Brand-Miller. Improving the estimation of mealtime insulin dose in adults with type 1 diabetes: the normal insulin demand for dose adjustment (nidda) study. _Diabetes Care_, 34(10):2146-2151, 2011.
* Beck [2017] Amir Beck. _First-order methods in optimization_. SIAM, 2017.
* Boyd et al. [2004] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. _Convex Optimization_. Cambridge University Press, 2004.
* Brockman et al. [2016] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym. _arXiv preprint arXiv:1606.01540_, 2016.
* Chen and Jiang [2019] Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In _International Conference on Machine Learning_, pages 1042-1051. PMLR, 2019.
* Chen and Jiang [2022] Jinglin Chen and Nan Jiang. Offline reinforcement learning under value and density-ratio realizability: the power of gaps. In _Uncertainty in Artificial Intelligence_, pages 378-388. PMLR, 2022.
* Cheng et al. [2022] Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal. Adversarially trained actor critic for offline reinforcement learning. In _International Conference on Machine Learning_, pages 3852-3878. PMLR, 2022.
* Choi and Lee [2003] Euisun Choi and Chulhee Lee. Feature extraction based on bhattacharyya distance. _Pattern Recognition_, 36(8):1703-1709, 2003.
* Duan et al. [2020] Yaqi Duan, Zeyu Jia, and Mengdi Wang. Minimax-optimal off-policy evaluation with linear function approximation. In _International Conference on Machine Learning_, pages 2701-2709. PMLR, 2020.
* Ernst et al. [2005] Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. _Journal of Machine Learning Research_, 6:503-556, 2005.
* Farahmand et al. [2016] Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesvari, and Shie Mannor. Regularized policy iteration with nonparametric function spaces. _The Journal of Machine Learning Research_, 17(1):4809-4874, 2016.
* Fu et al. [2020] Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* Fudenberg and Tirole [1991] Drew Fudenberg and Jean Tirole. _Game theory_. MIT press, 1991.
* Fujimoto and Gu [2021] Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. _Advances in neural information processing systems_, 34:20132-20145, 2021.

* Fujimoto et al. [2019] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. In _International Conference on Machine Learning_, pages 2052-2062. PMLR, 2019.
* Ghavamzadeh et al. [2016] Mohammad Ghavamzadeh, Marek Petrik, and Yinlam Chow. Safe policy improvement by minimizing robust baseline regret. _Advances in Neural Information Processing Systems_, 29, 2016.
* Gretton et al. [2012] Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A kernel two-sample test. _The Journal of Machine Learning Research_, 13(1):723-773, 2012.
* Jiang and Huang [2020] Nan Jiang and Jiawei Huang. Minimax value interval for off-policy evaluation and policy optimization. _Advances in Neural Information Processing Systems_, 33:2747-2758, 2020.
* Jin et al. [2021] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In _International Conference on Machine Learning_, pages 5084-5096. PMLR, 2021.
* Kostrikov et al. [2021] Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. _arXiv preprint arXiv:2110.06169_, 2021.
* Kumar et al. [2019] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. _Advances in Neural Information Processing Systems_, 32, 2019.
* Kumar et al. [2020] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* Laroche et al. [2019] Romain Laroche, Paul Trichelair, and Remi Tachet Des Combes. Safe policy improvement with baseline bootstrapping. In _International conference on machine learning_, pages 3652-3661. PMLR, 2019.
* Lee et al. [2021] Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim. Optidice: Offline policy optimization via stationary distribution correction estimation. In _International Conference on Machine Learning_, pages 6120-6130. PMLR, 2021.
* Levine et al. [2020] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Li et al. [2023] Yuhan Li, Wenzhuo Zhou, and Ruoqing Zhu. Quasi-optimal reinforcement learning with continuous actions. In _The Eleventh International Conference on Learning Representations_, 2023.
* Liu et al. [2020] Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch off-policy reinforcement learning without great exploration. _Advances in neural information processing systems_, 33:1264-1274, 2020.
* Luckett et al. [2020] Daniel J Luckett, Eric B Laber, Anna R Kahkoska, David M Maahs, Elizabeth Mayer-Davis, and Michael R Kosorok. Estimating dynamic treatment regimes in mobile health using v-learning. _Journal of the American Statistical Association_, 115(530):692-706, 2020.
* Mandel et al. [2014] Travis Mandel, Yun-En Liu, Sergey Levine, Emma Brunskill, and Zoran Popovic. Offline policy evaluation across representations with applications to educational games. In _AAMAS_, volume 1077, 2014.
* Marling and Bunescu [2020] Cindy Marling and Razvan Bunescu. The ohiot1dm dataset for blood glucose level prediction: Update 2020. _KHD@ IJCAI_, 2020.
* Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. _nature_, 518(7540):529-533, 2015.

* [35] Susan A Murphy, Mark J van der Laan, James M Robins, and Conduct Problems Prevention Research Group. Marginal mean models for dynamic regimes. _Journal of the American Statistical Association_, 96(456):1410-1423, 2001.
* [36] Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. _arXiv preprint arXiv:2001.01866_, 2021.
* [37] Neal Parikh, Stephen Boyd, et al. Proximal algorithms. _Foundations and trends(r) in Optimization_, 1(3):127-239, 2014.
* [38] David Pollard. Empirical processes: theory and applications. Ims, 1990.
* [39] Doina Precup. Eligibility traces for off-policy policy evaluation. _Computer Science Department Faculty Publication Series_, page 80, 2000.
* [40] Martin L Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* [41] Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement learning and imitation learning: A tale of pessimism. _Advances in Neural Information Processing Systems_, 34:11702-11716, 2021.
* [42] Alfred Renyi. On measures of entropy and information. In _Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics_, volume 4, pages 547-562. University of California Press, 1961.
* [43] Bruno Scherrer and Boris Lesner. On the use of non-stationary policies for stationary infinite-horizon markov decision processes. _Advances in Neural Information Processing Systems_, 25, 2012.
* [44] Chengchun Shi, Masatoshi Uehara, Jiawei Huang, and Nan Jiang. A minimax learning approach to off-policy evaluation in confounded partially observable markov decision processes. In _International Conference on Machine Learning_, pages 20057-20094. PMLR, 2022.
* [45] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* [46] Georgios Theocharous, Yash Chandak, Philip S Thomas, and Frits de Nijs. Reinforcement learning for strategic recommendations. _arXiv preprint arXiv:2009.07346_, 2020.
* [47] Philip Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-confidence off-policy evaluation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 29, 2015.
* [48] Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and q-function learning for off-policy evaluation. In _International Conference on Machine Learning_, pages 9659-9668. PMLR, 2020.
* [49] Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial coverage. In _International Conference on Learning Representations_, 2022.
* [50] Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. _Measures of Complexity: Festschrift for Alexey Chervonenkis_, pages 11-30, 2015.
* [51] Heinrich Von Stackelberg. _Market structure and equilibrium_. Springer Science & Business Media, 2010.
* [52] Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham Kakade. Instabilities of offline rl with pre-trained neural representation. In _International Conference on Machine Learning_, pages 10948-10960. PMLR, 2021.
* [53] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_, 34, 2021.

* Xie and Jiang [2020] Tengyang Xie and Nan Jiang. Q* approximation schemes for batch reinforcement learning: A theoretical comparison. In _Conference on Uncertainty in Artificial Intelligence_, pages 550-559. PMLR, 2020.
* Yang and Wang [2020] Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. In _International Conference on Machine Learning_, pages 10746-10756. PMLR, 2020.
* Yin et al. [2021] Ming Yin, Yu Bai, and Yu-Xiang Wang. Near-optimal offline reinforcement learning via double variance reduction. _Advances in neural information processing systems_, 34:7677-7688, 2021.
* Yu et al. [2021] Tianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn. Combo: Conservative offline model-based policy optimization. _Advances in neural information processing systems_, 34:28954-28967, 2021.
* Zanette et al. [2021] Andrea Zanette, Martin J Wainwright, and Emma Brunskill. Provable benefits of actor-critic methods for offline reinforcement learning. _Advances in neural information processing systems_, 34:13626-13640, 2021.
* Zhan et al. [2022] Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with realizability and single-policy concentrability. In _Conference on Learning Theory_, pages 2730-2775. PMLR, 2022.
* Zhou et al. [2023] Wenzhuo Zhou, Yuhan Li, Ruoqing Zhu, and Annie Qu. Distributional shift-aware off-policy interval estimation: A unified error quantification framework. _arXiv preprint arXiv:2309.13278_, 2023.
* Zhou et al. [2022] Wenzhuo Zhou, Ruoqing Zhu, and Annie Qu. Estimating optimal infinite horizon dynamic treatment regimes via pt-learning. _Journal of the American Statistical Association_, pages 1-14, 2022.