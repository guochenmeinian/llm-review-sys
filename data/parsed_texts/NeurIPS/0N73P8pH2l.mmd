# ScaleLong: Towards More Stable Training of Diffusion Model via Scaling Network Long Skip Connection

 Zhongzhan Huang\({}^{1,2}\)  Pan Zhou\({}^{2}\)1  Shuicheng Yan\({}^{2}\)  Liang Lin\({}^{1}\)

\({}^{1}\)Sun Yat-Sen University, \({}^{2}\)Sea AI Lab

huangzhzh23@mail2.sysu.edu.cn; zhoupan@sea.com; shuicheng.yan@gmail.com; linliang@ieee.org

Footnote 1: Corresponding author.

###### Abstract

In diffusion models, UNet is the most popular network backbone, since its long skip connects (LSCs) to connect distant network blocks can aggregate long-distant information and alleviate vanishing gradient. Unfortunately, UNet often suffers from unstable training in diffusion models which can be alleviated by scaling its LSC coefficients smaller. However, theoretical understandings of the instability of UNet in diffusion models and also the performance improvement of LSC scaling remain absent yet. To solve this issue, we theoretically show that the coefficients of LSCs in UNet have big effects on the stableness of the forward and backward propagation and robustness of UNet. Specifically, the hidden feature and gradient of UNet at any layer can oscillate and their oscillation ranges are actually large which explains the instability of UNet training. Moreover, UNet is also provably sensitive to perturbed input, and predicts an output distant from the desired output, yielding oscillatory loss and thus oscillatory gradient. Besides, we also observe the theoretical benefits of the LSC coefficient scaling of UNet in the stableness of hidden features and gradient and also robustness. Finally, inspired by our theory, we propose an effective coefficient scaling framework ScaleLong that scales the coefficients of LSC in UNet and better improve the training stability of UNet. Experimental results on four famous datasets show that our methods are superior to stabilize training, and yield about 1.5\(\times\) training acceleration on different diffusion models with UNet or UViT backbones. Click here for Code.

## 1 Introduction

Recently, diffusion models (DMs) [1, 2, 3, 4, 5, 6, 7, 8, 9] have become the most successful generative models because of their superiority on modeling realistic data distributions. The methodology of DMs includes a forward diffusion process and a corresponding reverse diffusion process. For forward process, it progressively injects Gaussian noise into a realistic sample until the sample becomes a Gaussian noise, while the reverse process trains a neural network to denoise the injected noise at each step for gradually mapping the Gaussian noise into the vanilla sample. By decomposing the complex generative task into a sequential application of denoising small noise, DMs achieve much better synthesis performance than other generative models, e.g., generative adversarial networks [10] and variational autoencoders [11], on image [12, 13, 14, 15], 3D [16, 17] and video [18, 19, 20, 21] data and beyond.

Figure 1: The feature oscillation in UNet.

**Motivation.** Since the reverse diffusion process in DMs essentially addresses a denoising task, most DMs follow previous image denoising and restoration works [22, 23, 24, 25, 26] to employ UNet as their de-facto backbone. This is because UNet uses long skip connects (LSCs) to connect the distant and symmetrical network blocks in a residual network, which helps long-distant information aggregation and alleviates the vanishing gradient issue, yielding superior image denoising and restoration performance. However, in DMs, its reverse process uses a shared UNet to predict the injected noise at any step, even though the input noisy data have time-varied distribution due to the progressively injected noise. This inevitably leads to the training difficulty and instability of UNet in DMs. Indeed, as shown in Fig. 1, the output of hidden layers in UNet, especially for deep UNet, oscillates rapidly along with training iterations. This feature oscillation also indicates that there are some layers whose parameters suffer from oscillations which often impairs the parameter learning speed. Moreover, the unstable hidden features also result in an unstable input for subsequent layers and greatly increase their learning difficulty. Some works [27, 28, 29, 30, 31] empirically find that \(1/\sqrt{2}\)-constant scaling (\(1/\sqrt{2}\)-CS), that scales the coefficients of LSC (i.e. \(\{\kappa_{i}\}\) in Fig. 2) from one used in UNet to \(1/\sqrt{2}\), alleviates the oscillations to some extent as shown in Fig. 1, which, however, lacks intuitive and theoretical explanation and still suffers from feature and gradient oscillations. So it is unclear yet 1) why UNet in DMs is unstable and also 2) why scaling the coefficients of LSC in UNet helps stabilize training, which hinders the designing new and more advanced DMs in a principle way.

**Contribution.** In this work, we address the above two fundamental questions and contribute to deriving some new results and alternatives for UNet. Particularly, we theoretically analyze the above training instability of UNet in DMs, and identify the key role of the LSC coefficients in UNet for its unstable training, interpreting the stabilizing effect and limitations of the \(1/\sqrt{2}\)-scaling technique. Inspired by our theory, we propose the framework ScalLeLong including two novel-yet-effective coefficient scaling methods which scale the coefficients of LSC in UNet and better improve the training stability of UNet. Our main contributions are highlighted below.

Our first contribution is proving that the coefficients of LSCs in UNet have big effects on the stableness of the forward and backward propagation and robustness of UNet. Formally, _for forward propagation_, we show that the norm of the hidden feature at any layer can be lower- and also upper-bounded, and the oscillation range between lower and upper bounds is of the order \(\mathcal{O}\big{(}m\sum_{i=1}^{N}\kappa_{j}^{2}\big{)}\), where \(\{\kappa_{i}\}\) denotes the scaling coefficients of \(N\) LSCs and the input dimension \(m\) of UNet is often of hundreds. Since standard UNet sets \(\kappa_{i}=1\) (\(\forall i\)), the above oscillation range becomes \(\mathcal{O}(mN)\) and is large, which partly explains the oscillation and instability of hidden features as observed in Fig. 1. The \(1/\sqrt{2}\)-CS technique can slightly reduce the oscillation range, and thus helps stabilize UNet. Similarly, _for backward propagation_, the gradient magnitude in UNet is upper bounded by \(\mathcal{O}\big{(}m\sum_{i=1}^{N}\kappa_{i}^{2}\big{)}\). In standard UNet, this bound becomes a large bound \(\mathcal{O}\big{(}mN\big{)}\), and yields the possible big incorrect parameter update, impairing the parameter learning speed during training. This also explains the big oscillation of hidden features, and reveals the stabilizing effects of the \(1/\sqrt{2}\)-CS technique. Furthermore, _for robustness_ that measures the prediction change when adding a small perturbation into an input, we show robustness bound of UNet is \(\mathcal{O}(\sum_{i=1}^{N}\kappa_{i}M_{0}^{i})\) with a model-related constant \(M_{0}>1\). This result also implies a big robustness bound of standard UNet, and thus shows the sensitivity of UNet to the noise which can give a big incorrect gradient and explain the unstable training. It also shows that \(1/\sqrt{2}\)-CS technique improves the robustness.

Inspired by our theory, we further propose a novel framework ScaleLong including two scaling methods to adjust the coefficients of the LSCs in UNet for stabilizing the training: 1) constant scaling method (**CS** for short) and 2) learnable scaling method (**LS**). CS sets the coefficients \(\{\kappa_{i}\}\) as a serir of exponentially-decaying constants, i.e., \(\kappa_{i}=\kappa^{i-1}\) with a contant \(\kappa\in(0,1]\). As a result, CS greatly stabilizes the UNettraining by largely reducing the robustness error bound from \(\mathcal{O}(M_{0}^{N})\) to \(\mathcal{O}\big{(}M_{0}(\kappa M_{0})^{N-1}\big{)}\), which is also \(\mathcal{O}(\kappa^{N-2})\) times smaller than the bound \(\mathcal{O}(\kappa M_{0}^{N})\) of the universal scaling method, i.e., \(\kappa_{i}=\kappa\), like \(1/\sqrt{2}\)-CS technique. Meanwhile, CS can ensure the information transmission of LSC without degrading into a feedforward network. Similarly, the oscillation range

Figure 2: The diagram of UNet.

of hidden features and also gradient magnitude can also be controlled by our CS. For LS, it uses a tiny shared network for all LSCs to predict the scaling coefficients for each LSC. In this way, LS is more flexible and adaptive than the CS method, as it can learn the scaling coefficients according to the training data, and can also adjust the scaling coefficients along with training iterations which could benefit the training in terms of stability and convergence speed. Fig. 1 shows the effects of CS and LS in stabilizing the UNet training in DMs.

Extensive experiments on CIFAR10 [32], CelebA [33], ImageNet [34], and COCO [35], show the effectiveness of our CS and LS methods in enhancing training stability and thus accelerating the learning speed by at least 1.5\(\times\) in most cases across several DMs, e.g. UNet and UViT networks under the unconditional [3; 4; 5], class-conditional [36; 37] and text-to-image [38; 39; 40; 41; 42] settings.

## 2 Preliminaries and other related works

**Diffusion Model (DM)**. DDPM-alike DMs [1; 2; 3; 4; 5; 6; 8; 9; 43] generates a sequence of noisy samples \(\{\mathbf{x}_{i}\}_{i=1}^{T}\) by repeatedly adding Gaussian noise to a sample \(\mathbf{x}_{0}\) until attaining \(\mathbf{x}_{T}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). This noise injection process, a.k.a. the forward process, can be formalized as a Markov chain \(q\left(\mathbf{x}_{1:T}|\mathbf{x}_{0}\right)=\prod_{t=1}^{T}q\left(\mathbf{x }_{t}|\mathbf{x}_{t-1}\right)\), where \(q(\mathbf{x}_{t}|\mathbf{x}_{t-1})=\mathcal{N}(\mathbf{x}_{t}|\sqrt{\alpha_{ t}}\mathbf{x}_{t-1},\beta_{t}\mathbf{I})\), \(\alpha_{t}\) and \(\beta_{t}\) depend on \(t\) and satisfy \(\alpha_{t}+\beta_{t}=1\). By using the properties of Gaussian distribution, the forward process can be written as

\[q(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathcal{N}(\mathbf{x}_{t};\sqrt{\bar{\alpha }_{t}}\mathbf{x}_{0},(1-\bar{\alpha}_{t})\mathbf{I}),\] (1)

where \(\bar{\alpha}_{t}=\prod_{i=1}^{t}\alpha_{i}\). Next, one can sample \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t} }\epsilon_{t}\), where \(\epsilon_{t}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). Then DM adopts a neural network \(\hat{\epsilon}_{\theta}(\cdot,t)\) to invert the forward process and predict the noise \(\epsilon_{t}\) added at each time step. This process is to recover data \(\mathbf{x}_{0}\) from a Gaussian noise by minimizing the loss

\[\ell_{\text{simple}}^{t}(\theta)=\mathbb{E}\|\epsilon_{t}-\hat{\epsilon}_{ \theta}(\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon_{t},t)\|_{2}^{2}.\] (2)

**UNet-alike Network**. Since the network \(\hat{\epsilon}_{\theta}(\cdot,t)\) in DMs predicts the noise to denoise, it plays a similar role of UNet-alike network widely in image restoration tasks, e.g. image de-raining, image denoising [22; 23; 24; 25; 26; 44; 45]. This inspires DMs to use UNet-alike network in (3) that uses LSCs to connect distant parts of the network for long-range information transmission and aggregation

\[\mathbf{UNet}(x)=f_{0}(x),\quad f_{i}(x)=b_{i+1}\circ[\kappa_{i+1}\cdot a_{i+ 1}\circ x+f_{i+1}\left(a_{i+1}\circ x\right)],\ \ 0\leq i\leq N-1\] (3)

where \(x\in\mathbb{R}^{m}\) denotes the input, \(a_{i}\) and \(b_{i}\)\((i\geq 1)\) are the trainable parameter of the \(i\)-th block, \(\kappa_{i}>0\)\((i\geq 1)\) are the scaling coefficients and are set to 1 in standard UNet. \(f_{N}\) is the middle block of UNet. For the vector operation \(\circ\), it can be designed to implement different networks.

W.l.o.g, in this paper, we consider \(\circ\) as matrix multiplication, and set the \(i\)-th block as a stacked network [46; 47] which is implemented as \(a_{i}\circ x=\mathbf{W}_{l}^{a_{i}}\phi(\mathbf{W}_{l-1}^{a_{i}}...\phi(\mathbf{ W}_{1}^{a_{i}}x))\) with a ReLU activation function \(\phi\) and learnable matrices \(\mathbf{W}_{j}^{a_{i}}\in\mathbb{R}^{m\times m}\)\((j\geq 1)\). Moreover, let \(f_{N}\) also have the same architecture, i.e. \(f_{N}(x)=\mathbf{W}_{l}^{f_{l}}\phi(\mathbf{W}_{l-1}^{f_{l-1}}...\phi(\mathbf{ W}_{1}^{f_{l}}x))\). Following [36], the above UNet has absorbed the fusion operation of the feature \(a_{i}\circ x\) and \(f_{i}\left(a_{i}\circ x\right)\) into UNet backbone. So for simplicity, we analyze the UNet in (3). Moreover, as there are many variants of UNet, e.g. transformer UNet, it is hard to provide a unified analysis for all of them. Thus, here we only analyze the most widely used UNet in (3), and leave the analysis of other UNet variants in future work.

**Other related works.** Previous works [46; 48; 49; 50; 51; 52; 53; 54; 55] mainly studied classification task and observed performance improvement of scaling block output instead of skip connections in ResNet and Transformer, i.e. \(\mathbf{x}_{i+1}=\mathbf{x}_{i}+\kappa_{i}f_{i}(\mathbf{x}_{i})\) where \(f_{i}(\mathbf{x}_{i})\) is the \(i\)-th block. In contrast, we study UNet for DMs which uses long skip connections (LSC) instead of short skip connections, and stabilize UNet training via scaling LSC via Eq. (3) which is superior to scaling block output as shown in Section 5.

## 3 Stability Analysis on UNet

As shown in Section 1, standard U-Net (\(\kappa_{i}=1,\forall i\)) often suffers from the unstable training issue in diffusion models (DMs), while scaling the long skip connection (LSC) coefficients \(\kappa_{i}(\forall i)\) in UNet can help to stabilize the training. However, theoretical understandings of the instability of U-Net and also the effects of scaling LSC coeffeicents remain absent yet, hindering the development of new and more advanced DMs in a principle way. To address this problem, in this section, we will theoretically and comprehensively analyze 1) why UNet in DMs is unstable and also 2) why scaling the coeffeicents of LSC in UNet helps stablize training. To this end, in the following, we will analyze the stableness of the forward and backward propagation of UNet, and investigate the robustness of UNet to the noisy input. All these analyses not only deepen the understanding of the instability of UNet, but also inspire an effective solution for a more stable UNet as introduced in Section 4.

### Stability of Forward Propagation

Here we study the stableness of the hidden features of UNet. This is very important, since for a certain layer, if its features vary significantly along with the training iterations, it means there is at least a layer whose parameters suffer from oscillations which often impairs the parameter learning speed. Moreover, the unstable hidden features also result in an unstable input for subsequent layers and greatly increase their learning difficulty, since a network cannot easily learn an unstable distribution, e.g. Internal Covariate Shift [56, 57], which is also shown in many works [58, 59, 60]. In the following, we theoretically characterize the hidden feature output by bounding its norm.

**Theorem 3.1**.: _Assume that all learnable matrices of UNet in Eq. (3) are independently initialized as Kaiming's initialization, i.e., \(\mathcal{N}(0,\frac{2}{m})\). Then for any \(\rho\in(0,1]\), by minimizing the training loss Eq. (2) of DMs, with probability at least \(1-\mathcal{O}(N)\exp[-\Omega(m\rho^{2})]\), we have_

\[(1-\rho)^{2}\left[c_{1}\|\mathbf{x}_{t}\|_{2}^{2}\cdot\sum\nolimits_{j=i}^{N} \kappa_{j}^{2}+c_{2}\right]\lesssim\|h_{i}\|_{2}^{2}\lesssim(1+\rho)^{2}\left[ c_{1}\|\mathbf{x}_{t}\|_{2}^{2}\cdot\sum\nolimits_{j=i}^{N}\kappa_{j}^{2}+c_{2} \right],\] (4)

_where the hidden feature \(h_{i}\) is the output of \(f_{i}\) defined in Eq. (3), \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon_{t}\) is the input of UNet; \(c_{1}\) and \(c_{2}\) are two constants; \(\kappa_{i}\) is the scaling coefficient of the \(i\)-th LSC in UNet._

Theorem 3.1 reveals that with high probability, the norm of hidden features \(h_{i}\) of UNet can be both lower- and also upper-bounded, and its biggest oscillation range is of the order \(\mathcal{O}\big{(}\|\mathbf{x}_{t}\|_{2}^{2}\sum\nolimits_{j=i}^{N}\kappa_{j} ^{2}\big{)}\) which is decided by the scaling coefficients \(\{\kappa_{i}\}\) and the UNet input \(\mathbf{x}_{t}\). So when \(\|\mathbf{x}_{t}\|_{2}\) or \(\sum\nolimits_{i=1}^{N}\kappa_{i}^{2}\) is large, the above oscillation range is large which allows hidden feature \(h_{i}\) to oscillate largely as shown in Fig. 1, leading to the instability of the forward processing in UNet.

In fact, the following Lemma 3.2 shows that \(\|\mathbf{x}_{t}\|_{2}^{2}\) is around feature dimension \(m\) in DDPM.

**Lemma 3.2**.: _For \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t} }\epsilon_{t}\) defined in Eq. (1) as a input of UNet, \(\epsilon_{t}\sim\mathcal{N}(0,\mathbf{I})\), if \(\mathbf{x}_{0}\) follow the uniform distribution \(U[-1,1]\), then we have_

\[\mathbb{E}\|\mathbf{x}_{t}\|_{2}^{2}=(1-2\mathbb{E}_{t}\bar{\alpha}_{t}/3)m= \mathcal{O}(m).\] (5)

Indeed, the conditions and conclusions of Lemma 3.2 hold universally in commonly used datasets, such as CIFAR10, CelebA, and ImageNet, which can be empirically valid in Fig. 3 (a). In this way, in standard UNet whose scaling coefficients satisfy \(\kappa_{i}=1\), the oscillation range of hidden feature \(\|h_{i}\|_{2}^{2}\) becomes a very large bound \(\mathcal{O}(Nm)\), where \(N\) denotes the number of LSCs. Accordingly, UNet suffers from the unstable forward process caused by the possible big oscillation of the hidden feature \(h_{i}\) which accords with the observations in Fig. 1. To relieve the instability issue, one possible solution is to scale the coefficients \(\{\kappa_{i}\}\) of LSCs, which could reduce the oscillation range of hidden feature \(\|h_{i}\|_{2}^{2}\) and yield more stable hidden features \(h_{i}\) to benefit the subsequent layers. This is also one reason why \(1/\sqrt{2}\)-scaling technique [27; 28; 29; 30; 31] can help stabilize the UNet. However, it is also important to note that these coefficients cannot be too small, as they could degenerate a network into a feedforward network and negatively affect the network representation learning ability.

### Stability of Backward Propagation

As mentioned in Section 3.1, the reason behind the instability of UNet forward process in DMs is the parameter oscillation. Here we go a step further to study why these UNet parameters oscillate. Since parameters are updated by the gradient given in the backward propagation process, we analyze the gradient of UNet in DMs. A stable gradient with proper magnitude can update model parameters smoothly and also effectively, and yields more stable training. On the contrary, if gradients oscillate largely as shown in Fig. 1, they greatly affect the parameter learning and lead to the unstable hidden features which further increases the learning difficulty of subsequent layers. In the following, we analyze the gradients of UNet, and investigate the effects of LSC coefficient scaling on gradients.

For brevity, we use \(\mathbf{W}\) to denote all the learnable matrices of the \(i\)-th block (see Eq. (3)), i.e., \(\mathbf{W}=[\mathbf{W}_{1}^{a_{1}},\mathbf{W}_{1-1}^{a_{1}},...,\mathbf{W}_{ 1}^{a_{1}},\mathbf{W}_{1}^{a_{2}}...,\mathbf{W}_{2}^{b_{1}},\mathbf{W}_{1}^{ b_{1}}]\). Assume the training loss is \(\frac{1}{n}\sum_{s=1}^{n}\ell_{s}(\mathbf{W})\), where \(\ell_{s}(\mathbf{W})\) denotes the training loss of the \(s\)-th sample among the \(n\) training samples. Next, we analyze the gradient \(\nabla\ell(\mathbf{W})\) of each training sample, and summarize the main results in Theorem 3.3, in which we omit the subscript \(s\) of \(\nabla\ell_{s}(\mathbf{W})\) for brevity. See the proof in Appendix.

**Theorem 3.3**.: _Assume that all learnable matrices of UNet in Eq. (3) are independently initialized as Kaiming's initialization, i.e., \(\mathcal{N}(0,\frac{2}{m})\). Then for any \(\rho\in(0,1]\), with probability at least \(1-\mathcal{O}(nN)\exp[-\Omega(m)]\), for a sample \(\mathbf{x}_{t}\) in training set, we have_

\[\|\nabla_{\mathbf{W}_{p}^{q}}\ell_{s}(\mathbf{W})\|_{2}^{2}\lesssim\mathcal{O }\left(\ell(\mathbf{W})\cdot\|\mathbf{x}_{t}\|_{2}^{2}\cdot\sum\nolimits_{j= i}^{N}\kappa_{j}^{2}+c_{3}\right),\quad(p\in\{1,2,...,l\},q\in\{a_{i},b_{i}\}),\] (6)

_where \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t }}\epsilon_{t}\) denotes the noisy sample of the \(s\)-th sample, \(\epsilon_{t}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\), \(N\) is the number of LSCs, \(c_{3}\) is a small constant._

Theorem 3.3 reveals that for any training sample in the training dataset, the gradient of any trainable parameter \(\mathbf{W}_{p}^{q}\) in UNet can be bounded by \(\mathcal{O}(\ell(\mathbf{W})\cdot\|\mathbf{x}_{t}\|_{2}^{2}\sum_{j=i}^{N} \kappa_{j}^{2})\) (up to the constant term). Since Lemma 3.2 shows \(\|\mathbf{x}_{t}\|_{2}^{2}\) is at the order of \(\mathcal{O}(m)\) in expectation, the bound of gradient norm becomes \(\mathcal{O}(m\ell_{s}(\mathbf{W})\sum_{j=i}^{N}\kappa_{j}^{2})\). Consider the feature dimension \(m\) is often hundreds and the loss \(\ell(\mathbf{W})\) would be large at the beginning of the training phase, if the number \(N\) of LSCs is relatively large, the gradient bound in standard UNet (\(\kappa_{i}=1\ \forall i\)) would become \(\mathcal{O}(mN\ell_{s}(\mathbf{W}))\) and is large. This means that UNet has the risk of instability gradient during training which is actually observed in Fig. 1. To address this issue, similar to the analysis of Theorem 3.3, we can appropriately scale the coefficients \(\{\kappa_{i}\}\) to ensure that the model has enough representational power while preventing too big a gradient and avoid unstable UNet parameter updates. This also explains why the \(1/\sqrt{2}\)-scaling technique can improve the stability of UNet in practice.

### Robustness On Noisy Input

In Section 3.1 and 3.2, we have revealed the reasons of instability of UNet and also the benign effects of scaling coefficients of LSCs in UNet during DM training. Here we will further discuss the stability of UNet in terms of the robustness on noisy input. A network is said to be robust if its output does not change much when adding a small noise perturbation to its input. This robustness is important for stable training of UNet in DMs. As shown in Section 2, DMs aim at using UNet to predict the noise \(\epsilon_{t}\sim\mathcal{N}(0,\mathbf{I})\) in the noisy sample for denoising at each time step \(t\). However, in practice, additional unknown noise caused by random minibatch, data collection, and preprocessing strategies is inevitably introduced into noise \(\epsilon_{t}\) during the training process, and greatly affects the training. It is because if a network is sensitive to noisy input, its output would be distant from the desired output, and yields oscillatory loss which leads to oscillatory gradient and parameters. In contrast, a robust network would have stable output, loss and gradient which boosts the parameter learning speed.

Here we use a practical example to show the importance of the robustness of UNet in DMs. According to Eq. (2), DM aims to predict noise \(\epsilon_{t}\) from the UNet input \(\mathbf{x}_{t}\sim\mathcal{N}(\mathbf{x}_{t};\sqrt{\bar{\alpha}_{t}}\mathbf{x}_ {0},(1-\bar{\alpha}_{t})\mathbf{I})\).

Assume an extra Gaussian noise \(\epsilon_{\delta}\sim\mathcal{N}(0,\sigma_{\delta}^{2}\mathbf{I})\) is injected into \(\mathbf{x}_{t}\) which yields a new input \(\mathbf{x}_{t}^{\prime}\sim\mathcal{N}(\mathbf{x}_{t};\sqrt{\alpha_{t}} \mathbf{x}_{0},[(1-\bar{\alpha}_{t})+\sigma_{\delta}^{2}]\mathbf{I})\). Accordingly, if the variance \(\sigma_{\delta}^{2}\) of extra noise is large, it can dominate the noise \(\epsilon_{t}\) in \(\mathbf{x}_{t}^{\prime}\), and thus hinders predicting the desired noise \(\epsilon_{t}\). In practice, the variance of this extra noise could vary along training iterations, further exacerbating the instability of UNet training. Indeed, Fig. 3 (c) shows that the extra noise \(\epsilon_{\delta}\) with different \(\sigma_{\delta}^{2}\) can significantly affect the performance of standard UNet for DMs, and the scaled LSCs can alleviate the impact of extra noise to some extent. We then present the following theorem to quantitatively analyze these observations.

**Theorem 3.4**.: _For UNet in Eq. (3), assume \(M_{0}=\max\{\lvert\lvert b_{i}\circ a_{i}\rvert\rvert_{2},1\leq i\leq N\}\) and \(f_{N}\) is \(L_{0}\)-Lipschitz continuous. \(c_{0}\) is a constant related to \(M_{0}\) and \(L_{0}\). Suppose \(\mathbf{x}_{t}^{\epsilon_{\delta}}\) is an perturbated input of the vanilla input \(\mathbf{x}_{t}\) with a small perturbation \(\epsilon_{\delta}=\lVert\mathbf{x}_{t}^{\epsilon}-\mathbf{x}_{t}\rVert_{2}\). Then we have_

\[\lVert\mathbf{UNet}(\mathbf{x}_{t}^{\epsilon_{\delta}})-\mathbf{UNet}( \mathbf{x}_{t})\rVert_{2}\leq\epsilon_{\delta}\left[\sum\nolimits_{i=1}^{N} \kappa_{i}M_{0}^{i}+c_{0}\right],\] (7)

_where \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t} }\epsilon_{t}\), \(\epsilon_{t}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\), \(N\) is the number of the long skip connections._

Theorem 3.4 shows that for a perturbation magnitude \(\epsilon_{\delta}\), the robustness error bound of UNet in DMs is \(\mathcal{O}(\epsilon_{\delta}(\sum_{i=1}^{N}\kappa_{i}M_{0}^{i}))\). For standard UNet (\(\kappa_{i}=1\forall i\)), this bound becomes a very large bound \(\mathcal{O}(NM_{0}^{N})\). This implies that standard UNet is sensitive to extra noise in the input, especially when LSC number \(N\) is large (\(M_{0}\geq 1\) in practice, see Appendix). Hence, it is necessary to control the coefficients \(\kappa_{i}\) of LSCs which accords with the observations in Fig. 3 (c). Therefore, setting appropriate scaling coefficients \(\{\kappa_{i}\}\) can not only control the oscillation of hidden features and gradients during the forward and backward processes as described in Section 3.1 and Section 3.2, but also enhance the robustness of the model to input perturbations, thereby improving better stability for DMs as discussed at the beginning of Section 3.3.

## 4 Theory-inspired Scaling Methods For Long Skip Connections

In Section 3, we have theoretically shown that scaling the coefficients of LSCs in UNet can help to improve the training stability of UNet on DM generation tasks by analyzing the stability of hidden feature output and gradient, and the robustness of UNet on noisy input. These theoretical results can directly be used to explain the effectiveness of \(1/\sqrt{2}\)-scaling technique in [27, 28, 29, 30, 31] which scales all LSC coefficients \(\kappa_{i}\) from one in UNet to a constant, and is called \(1/\sqrt{2}\) (\(1/\sqrt{2}\)-CS for short). Here we will use these theoretical results to design a novel framework ScaleLong including two more effective scaling methods for DM stable training. 1) constant scaling method (CS for short) and 2) learnable scaling method (LS for short).

### Constant Scaling Method

In Theorem 3.1\(\sim\) 3.4, we already show that adjusting the scaling coefficients \(\{\kappa_{i}\}\) to smaller ones can effectively decrease the oscillation of hidden features, avoids too large gradient and also improves the robustness of U-Net to noisy input. Motivated by this, we propose an effective constant scaling method (CS) that exponentially scales the coefficients \(\{\kappa_{i}\}\) of LSCs in UNet:

\[\text{CS:}\quad\kappa_{i}=\kappa^{i-1},\qquad(i=1,2,..,N,\quad\forall\kappa \in(0,1]).\] (8)

When \(i=1\), the scaling coefficient of the first long skip connection is 1, and thus ensures that at least one \(\kappa_{i}\) is not too small to cause network degeneration.

This exponential scaling in CS method is more effective than the universal scaling methods which universally scales \(\kappa_{i}=\kappa\), e.g. \(\kappa=1/\sqrt{2}\) in the \(1/\sqrt{2}\)-CS method, in terms of reducing the training oscillation of UNet. We first look at the robustness error bound in Theorem 3.4. For our CS, its bound is at the order of \(\mathcal{O}\big{(}M_{0}(\kappa M_{0})^{N-1}\big{)}\) which is \(\mathcal{O}(\kappa^{N-2})\) times smaller than the bound \(\mathcal{O}(\kappa M_{0}^{N})\) of the universal scaling method. This shows the superiority of our CS method. Moreover, from the analysis in Theorem 3.1, our CS method can effectively compress the oscillation range of the hidden feature \(h_{i}\) of the \(i\)-th layer to \(\mathcal{O}\big{(}\kappa^{2i}m\big{)}\). Compared with the oscillation range \(\mathcal{O}\big{(}Nm\big{)}\) of standard UNet, CS also greatly alleviates the oscillation since \(\kappa^{2i}\ll N\). A similar analysis on Theorem 3.3 also shows that CS can effectively control the gradient magnitude, helping stabilize UNet training.

Now we discuss how to set the value of \(\kappa\) in Eq. (8). To this end, we use the widely used DDPM framework on the benchmarks (CIFAR10, CelebA and ImageNet) to estimate \(\kappa\). Firstly, Theorem3.1 reveals that the norm of UNet output is of order \(\mathcal{O}(\|\mathbf{x}_{t}\|_{2}^{2}\sum_{j=1}^{N}\kappa_{j}^{2})\). Next, from the loss function of DMs, i.e., Eq. (2), the target output is a noise \(\epsilon_{t}\sim\mathcal{N}(0,\mathbf{I})\). Therefore, we expect that \(\mathcal{O}(\|\mathbf{x}_{t}\|_{2}^{2}\sum_{j=1}^{N}\kappa_{j}^{2})\approx\| \epsilon_{t}\|_{2}^{2}\), and we have \(\sum_{j=1}^{N}\kappa_{j}^{2}\approx\mathcal{O}(\|\epsilon_{t}\|_{2}^{2}/\| \mathbf{x}_{t}\|_{2}^{2})\). Moreover, Fig. 3 (b) shows the distribution of \(\|\epsilon_{t}\|_{2}^{2}/\|\mathbf{x}_{t}\|_{2}^{2}\) for the dataset considered, which is long-tailed and more than 97% of the values are smaller than 5. For this distribution, during the training, these long-tail samples result in a larger value of \(\|\epsilon_{t}\|_{2}^{2}/\|\mathbf{x}_{t}\|_{2}^{2}\). They may cause the output order \(\mathcal{O}(\|\mathbf{x}_{t}\|_{2}^{2}\sum_{j=1}^{N}\kappa_{j}^{2})\) to deviate more from the \(\|\epsilon_{t}\|_{2}^{2}\), leading to unstable loss and impacting training. Therefore, it is advisable to appropriately increase \(\sum_{j=1}^{N}\kappa_{j}^{2}\) to address the influence of long-tail samples. [61, 62, 63, 64, 65]. So the value of \(\sum_{j=1}^{N}\kappa_{j}^{2}\) should be between the mean values of \(\|\epsilon_{t}\|_{2}^{2}/\|\mathbf{x}_{t}\|_{2}^{2}\), i.e., about 1.22, and a rough upper bound of 5. Combining Eq. (8) and the settings of \(N\) in UViT and UNet, we can estimate \(\kappa\in[0.5,0.95]\). In practice, the hyperparameter \(\kappa\) can be adjusted around this range.

### Learnable Scaling Method

In Section 4.1, an effective constant scaling (CS) method is derived from theory, and can improve the training stability of UNet in practice as shown in Section 5. Here, we provide an alternative solution, namely, the learnable scaling method (LS), which uses a tiny network to predict the scaling coefficients for each long skip connection. Accordingly, LS is more flexible and adaptive than the constant scaling method, since it can learn the coefficients according to the training data, and can also adjust the coefficients along with training iterations which could benefit the training in terms of stability and convergence speed, which will be empirically demonstrated in Section 5.

As shown in Fig. 4, LS designs a calibration network usually shared by all long skip connections to predict scaling coefficients \(\{\kappa_{i}\}\). Specifically, for the \(i\)-th long skip connection, its input feature is \(x_{i}\in\mathbb{R}^{B\times N\times D}\), where \(B\) denotes the batch size. For convolution-based UNet, \(N\) and \(D\) respectively denote the channel number and spatial dimension (\(H\times W\)) of the feature map; for transformer-based UViT [36], \(N\) and \(D\) respectively represent the token number and token dimension. See more discussions on input features of other network architectures in the Appendix. Accordingly, LS feeds \(x_{i}\in\mathbb{R}^{B\times N\times D}\) into the tiny calibration network \(\zeta_{\phi}\) parameterized by \(\phi\) for prediction:

\[\mathbf{LS}:\quad\kappa_{i}=\sigma(\zeta_{\phi}[\text{GAP}(x_{i})])\in\mathbb{ R}^{B\times N\times 1},1\leq i\leq N,\] (9)

where GAP denotes the global average pooling that averages \(x_{i}\) along the last dimension to obtain a feature map of size \(B\times N\times 1\); \(\sigma\) is a sigmoid activation function. After learning \(\{\kappa_{i}\}\), LS uses \(\kappa_{i}\) to scale the input \(x_{i}\) via element-wise multiplication. In practice, network \(\zeta_{\phi}\) is very small, and has only about 0.01M parameters in our all experiments which brings an ignorable cost compared with the cost of UNet but greatly improves the training stability and generation performance of UNet. In fact, since the parameter count of \(\zeta_{\phi}\) itself is not substantial, to make LS more versatile for different network architecture, we can individually set a scaling module into each long skip connection. This configuration typically does not lead to performance degradation and can even result in improved performance, while maintaining the same inference speed as the original LS.

## 5 Experiment

In this section, we evaluate our methods by using UNet [3, 4] and also UViT [36, 66, 67] under the unconditional [3, 4, 5], class-conditional [36, 37] and text-to-image [38, 39, 40, 41, 42] settings on several commonly used datasets, including CIFAR10 [32], CelebA [33], ImageNet [34], and MS-COCO [35]. We follow the settings of [36] and defer the specific implementation details to the Appendix.

**Training Stability.** Fig. 1 shows that our CS and LS methods can significantly alleviate the oscillation in UNet training under different DM settings, which is consistent with the analysis for controlling hidden features and gradients in Section 3. Though the \(1/\sqrt{2}\)-CS method can stabilize training to some extent, there are relatively large oscillations yet during training, particularly for deep networks. Additionally, Fig. 3 (c) shows that CS and LS can resist extra noise interference to some extent, further demonstrating the effectiveness of our proposed methods in improving training stability.

**Convergence Speed**. Fig. 5 shows the training efficiency of our CS and LS methods under different settings which is a direct result of stabilizing effects of our methods. Specifically, during training, for

Figure 4: The diagram of LS.

most cases, they consistently accelerate the learning speed by at least 1.5\(\times\) faster in terms of training steps across different datasets, batch sizes, network depths, and architectures. This greatly saves the actual training time of DMs. For example, when using a 32-sized batch, LS trained for 6.7 hours (200 k steps) achieves superior performance than standard UViT trained for 15.6 hours (500 k steps). All these results reveal the effectiveness of the long skip connection scaling for faster training of DMs.

**Performance Comparison**. Tables 1 show that our CS and LS methods can consistently enhance the widely used UNet and UViT backbones under the unconditional [3; 4; 5], class-conditional [36; 37] and text-to-image [38; 39; 40; 41; 42] settings with almost no additional computational cost. Moreover, on both UNet and UViT, our CS and LS also achieve much better performance than \(1/\sqrt{2}\)-CS. All these results are consistent with the analysis in Section 4.1.

\begin{table}
\begin{tabular}{l c c c c c} \hline
**Model (ClelebA)** & **Type** & **\#Param** & **FID\(\downarrow\)** & **Model (ImageNet 64)** & **Type** & **\#Param** & **FID\(\downarrow\)** \\ \hline DDIM [73] & Uncondition & 56M & 1.97 & Glow [69] & - & - & 3.81 \\ IDDPM [3] & Uncondition & 53M & 2.90 & IDDPM (small) [3] & Class-condition & 100M & 6.92 \\ DDPM+ cont. [27] & Uncondition & 62M & 2.55 & IDDPM (large) & Class-condition & 270M & 2.92 \\ GenViT [70] & Uncondition & 11M & 20.20 & ADM [71] & Class-condition & 296M & 2.07 \\ UNet [4] & Uncondition & 36M & 3.17 & EDM [68] & Class-condition & 296M & 1.36 \\ UViT-S2 [36] & Uncondition & 44M & 3.11 & UViT-M/4 [36] & Class-condition & 131M & 5.85 \\ UNet+1/\(\sqrt{2}\)-CS & Uncondition & 36M & 3.14 & UViT-M/4-\(1/\sqrt{2}\)-CS & Class-condition & 287M & 4.26 \\ UViT-S/2/-\(1/\sqrt{2}\)-CS & Uncondition & 44M & 3.11 & UViT-M/4-\(1/\sqrt{2}\)-CS & Class-condition & 131M & 5.80 \\ \hline UNet+CS (ours) & Uncondition & 36M & 3.05 & UViT-L/4-\(1/\sqrt{2}\)-CS & Class-condition & 287M & 4.20 \\ UNet [3] & Uncondition & 36M & 3.07 & UViT-S/2+CS (ours) & Uncondition & 144M & 3.77 \\ UViT-S/2+CS (ours) & Uncondition & 44M & 2.98 & UNet-\(1/\sqrt{2}\)-CS & Class-condition & 144M & 3.66 \\ UViT-S/2+LS (ours) & Uncondition & 44M & 3.01 & UViT-M/4-CS (ours) & Class-condition & 143M & 3.48 \\ \hline
**Model (MS-COCO)** & **Type** & **\#Param** & **FID\(\downarrow\)** & **UViT-M/4-\(1/\sqrt{2}\)-CS** & Class-condition & 131M & 5.68 \\ \hline VQ-Diffusion [41] & Text-to-Image & 370M & 19.75 & UViT-M/4-\(1/\sqrt{2}\)-CS & Class-condition & 131M & 5.75 \\ Frito [72] & Text-to-Image & 766M & 8.97 & UViT-L/4-CS (ours) & Class-condition & 287M & 3.83 \\ UNet [36] & Text-to-Image & 252M & 5.88 & UViT-S/2+\(1/\sqrt{2}\)-CS & Class-condition & 287M & 4.08 \\ UViT-S/2 [36] & Text-to-Image & 252M & 5.95 & UViT-S/2 (Deep) [36] & Text-to-Image & 252M & 5.95 \\ \hline UNet+CS (ours) & Text-to-Image & 260M & 7.04 & UViT-S/2+\(1/\sqrt{2}\)-CS & Class-condition & 260M & 6.89 \\ UViT-S/2+CS (ours) & Text-to-Image & 252M & 5.77 & UViT-S/2+LS (ours) & Uncondition & 44M & 2.78 \\ UViT-S/2+LS (ours) & Text-to-Image & 252M & 5.60 & UViT-S/2+\(1/\sqrt{2}\)-CS & Class-condition & 287M & 4.08 \\ \hline \end{tabular}
\end{table}
Table 1: Synthesis performance comparison under different diffusion model settings.

Figure 5: Training curve comparison under different training settings in which all experiments here adopt 12-layered UViT and a batch size 128 on CIFAR10 unless specified in the figures.

**Robustness to Batch Size and Network Depth**. UViT is often sensitive to batch size and network depth [36], since small batch sizes yield more noisy stochastic gradients that further aggravate gradient instability. Additionally, Section 3 shows that increasing network depth impairs the stability of both forward and backward propagation and its robustness to noisy inputs. So in Fig. 6, we evaluate our methods on standard UViT with different batch sizes and network depths. The results show that our CS and LS methods can greatly help UViT on CIFAR10 by showing big improvements on standard UViT and \(1/\sqrt{2}\)-CS-based UViT under different training batch sizes and network depth.

**Robustness of LS to Network Design**. For the calibration module in LS, here we investigate the robustness of LS to the module designs. Table 2 shows that using advanced modules, e.g. IE [77; 80] and SE [79], can improve LS performance compared with simply treating \(\kappa_{i}\) as learnable coefficients. So we use SE module [79; 81; 82] in our experiments (see Appendix). Furthermore, we observe that LS is robust to different activation functions and compression ratios \(r\) in the SE module. Hence, our LS does not require elaborated model crafting and adopts default \(r=16\) and ReLU throughout this work.

**Other scaling methods**. Our LS and CS methods scale LSC, unlike previous scaling methods [46; 48; 49; 50; 51; 52; 53] that primarily scale block output in classification tasks. Table 3 reveals that these block-output scaling methods have worse performance than LS (FID: 3.01) and CS (FID: 2.98) under diffusion model setting with UViT as backbone.

## 6 Discussion

**Relationship between LS and CS**. Taking UViT on CIFAR10, MSCOCO, ImageNet and CelebA as baselines, we randomly sample 30 Gaussian noises from \(\mathcal{N}(0,\mathbf{I})\) and measure the scaling coefficients, i.e., \(\kappa_{i}\leq 1\), learned by LS for each LSC. Fig. 7 shows that LS and CS share similar characteristics. Firstly, the predicted coefficients of LS fall within the orange area which is the estimated coefficient range given by CS in Section 4.1. Moreover, these learned coefficients \(\kappa_{i}\) share the almost the same exponential curve with Eq. (8) in CS. These shared characteristics have the potential to serve as crucial starting points for further optimization of DM network architectures in the future. Moreover, we try to preliminary analyze these shared characteristics.

First, for CS, how is the direction of applying the exponentially decayed scale determined? In fact, if we use the reverse direction, namely, \(\kappa_{i}=\kappa^{N-i+1}\)\((\kappa<1)\), the stability bound in Theorem 3.4 is extremely large. The main term of stability bound in Theorem 3.4 can be written as \(\mathcal{S}_{\text{r}}=\sum_{i=1}^{N}\kappa_{i}M_{0}^{i}=\kappa^{N}M_{0}+ \kappa^{N-1}M_{0}^{2}+...+\kappa M_{0}^{N}\), and could be very large, since \(M_{0}^{N}>1\) is large when \(N\) is large and scaling it by a factor \(\kappa\) could not sufficiently control its magnitude. In contrast, our default setting \(\kappa_{i}=\kappa^{i-1}\) of CS can well control the main term in stability bound: \(\mathcal{S}=\sum_{i=1}^{N}\kappa_{i}M_{0}^{i}=M_{0}+\kappa^{1}M_{0}^{2}+...+ \kappa^{N-1}M_{0}^{N}\), where the larger terms \(M_{0}^{i+1}\) are weighted by smaller coefficients \(\kappa^{i}\). In this way, \(\mathcal{S}\) is much smaller than \(\mathcal{S}_{\text{r}}\), which shows the advantages of our default setting. Besides, the following Table 4 also compares the above two settings by using UViT on Cifar10 (batch size = 64), and shows that our default setting exhibits significant advantages.

Next, for LS, there are two possible reasons for why LS discovers a decaying scaling curve similar to the CS. On the one hand, from a theoretical view, as discussed for the direction of scaling, for the \(i\)-th long skip connection \((1\leq i\leq N)\), the learnable \(\kappa_{i}\) should be smaller to better control the

\begin{table}
\begin{tabular}{c c|c|c|c} \hline
**Ratio \(r\)** & **FID\(\downarrow\)** & **Activation** & **FID\(\downarrow\)** & **Modules** & **FID\(\downarrow\)** \\ \hline
4 & 3.06 & ELU [76] & 3.08 & Learnable \(\kappa_{i}\) & 3.46 \\
8 & 3.10 & ReLU [76] (ours) & **3.01** & IE [77] & 3.09 \\
16 (ours) & **3.01** & Mish [78] & 3.08 & SE [79] (ours) & **3.01** \\ \hline \end{tabular}
\end{table}
Table 2: Robustness of LS to network design

Figure 6: Synthesis performance of proposed methods under small batch size and deep architecture.

Figure 7: The \(\kappa_{i}\) from LS.

magnitude of \(M_{0}^{i}\) so that the stability bound, e.g. in Theorem 3.4, is small. This directly yields the decaying scaling strategy which is also learnt by the scaling network. On the other hand, we can also analyze this observation in a more intuitive manner. Specifically, considering the UNet architecture, the gradient that travels through the \(i\)-th long skip connection during the backpropagation process influences the updates of both the first \(i\) blocks in the encoder and the last \(i\) blocks in the UNet decoder. As a result, to ensure stable network training, it's advisable to slightly reduce the gradients on the long skip connections involving more blocks (i.e., those with larger \(i\) values) to prevent any potential issues with gradient explosion.

**Limitations**. CS does not require any additional parameters or extra computational cost. But it can only estimate a rough range of the scaling coefficient \(\kappa\) as shown in Section 4.1. This means that one still needs to manually select \(\kappa\) from the range. In the future, we may be able to design better methods to assist DMs in selecting better values of \(\kappa\), or to have better estimates of the range of \(\kappa\). Another effective solution is to use our LS method which can automatically learn qualified scaling coefficient \(\kappa_{i}\) for each LSC. But LS inevitably introduces additional parameters and computational costs. Luckily, the prediction network in LS is very tiny and has only about 0.01M parameters, working very well in all our experiments.

Moreover, our CS and LS mainly focus on stabilizing the training of DMs, and indeed well achieve their target: greatly improving the stability of UNet and UViT whose effects is learning speed boosting of them by more than 1.5\(\times\) as shown in Fig. 5. But our CS and LS do not bring a very big improvement in terms of the final FID performance in Table 1, although they consistently improve model performance. Nonetheless, considering their simplicity, versatility, stabilizing ability to DMs, and faster learning speed, LS and CS should be greatly valuable in DMs.

**Conclusion**. We theoretically analyze the instability risks from widely used standard UNet for diffusion models (DMs). These risks are about the stability of forward and backward propagation, as well as the robustness of the network to extra noisy inputs. Based on these theoretical results, we propose a novel framework ScaleLong including two effective scaling methods, namely LS and CS, which can stabilize the training of DMs in different settings and lead to faster training speed and better generation performance.

**Acknowledgments**. This work was supported in part by National Key R&D Program of China under Grant No.2021ZD0111601, National Natural Science Foundation of China (NSFC) under Grant No.61836012, 62325605, U21A20470, GuangDong Basic and Applied Basic Research Foundation under Grant No. 2023A1515011374, GuangDong Province Key Laboratory of Information Security Technology.

## References

* [1] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11461-11471, 2022.
* [2] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffusion training via min-snr weighting strategy. _arXiv preprint arXiv:2303.09556_, 2023.
* [3] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In _International Conference on Machine Learning_, pages 8162-8171. PMLR, 2021.
* [4] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [5] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured denoising diffusion models in discrete state-spaces. _Advances in Neural Information Processing Systems_, 34:17981-17993, 2021.
* [6] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline Training step & 5k & 10k & 15k & 20k & 25k & 30k & 35k & 40k & 45k \\ \hline \(\kappa_{i}=\kappa^{N-i+1}\) & 67.26 & 33.93 & 22.78 & 16.91 & 15.01 & 14.01 & 12.96 & 12.34 & 12.26 \\ \(\kappa_{i}=\kappa^{i-1}\) (ours) & 85.19 & 23.74 & 15.36 & 11.38 & 10.02 & 8.61 & 7.92 & 7.27 & 6.65 \\ \hline \end{tabular}
\end{table}
Table 4: The FID-10k\(\downarrow\) result of different direction of scaling.

* [7] Nithin Gopalakrishnan Nair, Kangfu Mei, and Vishal M. Patel. At-ddpm: Restoring faces degraded by atmospheric turbulence using denoising diffusion probabilistic models. _2023 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pages 3423-3432, 2022.
* [8] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 14347-14356, 2021.
* [9] Julian Wyatt, Adam Leach, Sebastian M. Schmon, and Chris G. Willcocks. Anoddpm: Anomaly detection with denoising diffusion probabilistic models using simplex noise. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 649-655, 2022.
* [10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.
* [11] Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Balancing learning and inference in variational autoencoders. In _Proceedings of the aaai conference on artificial intelligence_, volume 33, pages 5885-5892, 2019.
* [12] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2426-2435, 2022.
* [13] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18208-18218, 2022.
* [14] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _International Conference on Machine Learning_, pages 16784-16804. PMLR, 2022.
* [15] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthesizer. _ArXiv_, 2023.
* [16] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. _arXiv preprint arXiv:2209.14988_, 2022.
* [17] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, et al. Rodin: A generative model for sculpting 3d digital avatars using diffusion. _arXiv preprint arXiv:2212.06135_, 2022.
* [18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv preprint arXiv:2204.03458_, 2022.
* [19] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors. _arXiv preprint arXiv:2302.01329_, 2023.
* [20] Patrick Esser, Johnathan Chiu, Parmida Atighechhian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. _arXiv preprint arXiv:2302.03011_, 2023.
* [21] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. _arXiv preprint arXiv:2304.08818_, 2023.
* [22] Yi Zhang, Dasong Li, Xiaoyu Shi, Dailan He, Kangning Song, Xiaogang Wang, Hongwei Qin, and Hongsheng Li. Khnet: Kernel basis network for image restoration. _arXiv preprint arXiv:2303.02881_, 2023.
* [23] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 14821-14831, 2021.
* [24] Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general u-shaped transformer for image restoration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 17683-17693, 2022.

* [25] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 5728-5739, 2022.
* [26] Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image restoration. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part VII_, pages 17-33. Springer, 2022.
* [27] Yang Song, Jascha Narain Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. _ArXiv_, abs/2011.13456, 2020.
* [28] Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45:4713-4726, 2021.
* [29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. _ArXiv_, abs/2205.11487, 2022.
* [30] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion probabilistic models. _ArXiv_, abs/2006.11239, 2020.
* [31] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4396-4405, 2018.
* [32] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
* [33] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_, December 2015.
* [34] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. _International Journal of Computer Vision_, 115:211-252, 2014.
* [35] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In _European Conference on Computer Vision_, 2014.
* [36] Fan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth words: a vit backbone for score-based diffusion models. _arXiv preprint arXiv:2209.12152_, 2022.
* [37] Jonathan Ho. Classifier-free diffusion guidance. _ArXiv_, abs/2207.12598, 2022.
* [38] Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, and Liang Lin. Sur-adapter: Enhancing text-to-image pre-trained diffusion models with large language models. _arXiv preprint arXiv:2305.05189_, 2023.
* [39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-booth: Fine tuning text-to-image diffusion models for subject-driven generation. _ArXiv_, abs/2208.12242, 2022.
* [40] Nupur Kumari, Bin Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. _ArXiv_, abs/2212.04488, 2022.
* [41] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 10686-10696, 2021.
* [42] Jiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and Shenghua Gao. Dream3d: Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion models. _ArXiv_, abs/2212.14704, 2022.
* [43] Firas Khader, Gustav Mueller-Franzes, Soroosh Tayebi Arasteh, Tianyu Han, Christoph Haarburger, Maximilian Franz Schulze-Hagen, Philipp Schad, Sandy Engelhardt, Bettina Baessler, Sebastian Foersch, J. Stegmaier, Christiane Kuhl, Sven Nebelung, Jakob Nikolas Kather, and Daniel Truhn. Denoising diffusion probabilistic models for 3d medical image generation. _Scientific Reports_, 13, 2022.

* [44] Liangyu Chen, Xin Lu, J. Zhang, Xiaojie Chu, and Chengpeng Chen. Hinet: Half instance normalization network for image restoration. _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pages 182-192, 2021.
* [45] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Memnet: A persistent memory network for image restoration. _2017 IEEE International Conference on Computer Vision (ICCV)_, pages 4549-4557, 2017.
* [46] Huishuai Zhang, Da Yu, Mingyang Yi, Wei Chen, and Tie-Yan Liu. Stabilize deep resnet with a sharp scaling factor tau. 2019.
* [47] Huishuai Zhang, Da Yu, Wei Chen, and Tie-Yan Liu. On the stability of multi-branch network, 2021.
* [48] Thomas C. Bachlechner, Bodhisattwa Prasad Majumder, Huanru Henry Mao, G. Cottrell, and Julian McAuley. Rezero is all you need: Fast convergence at large depth. In _Conference on Uncertainty in Artificial Intelligence_, 2020.
* [49] Hongyi Zhang, Yann Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without normalization. _ArXiv_, abs/1901.09321, 2019.
* [50] Soham De and Samuel L. Smith. Batch normalization biases residual blocks towards the identity function in deep networks. _arXiv: Learning_, 2020.
* [51] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture, 2020.
* [52] Bobby He, James Martens, Guodong Zhang, Aleksandar Botev, Andy Brock, Samuel L. Smith, and Yee Whye Teh. Deep transformers without shortcuts: Modifying self-attention for faithful signal propagation. _ArXiv_, abs/2302.10322, 2023.
* [53] Soufiane Hayou, Eugenio Clerico, Bo He, George Deligiannidis, A. Doucet, and Judith Rousseau. Stable resnet. In _International Conference on Artificial Intelligence and Statistics_, 2020.
* [54] Boris Hanin and David Rolnick. How to start training: The effect of initialization and architecture. _Advances in Neural Information Processing Systems_, 31, 2018.
* [55] David Balduzzi, Marcus Frean, Lennox Leary, JP Lewis, Kurt Wan-Duo Ma, and Brian McWilliams. The shattered gradients problem: If resnets are the answer, then what is the question? In _International Conference on Machine Learning_, pages 342-350. PMLR, 2017.
* [56] Muhammad Awais, Md. Tauhid Bin Iqbal, and Sung-Ho Bae. Revisiting internal covariate shift for batch normalization. _IEEE Transactions on Neural Networks and Learning Systems_, 32:5082-5092, 2020.
* [57] You Huang and Yuanlong Yu. An internal covariate shift bounding algorithm for deep neural networks by unitizing layers' outputs. _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8462-8470, 2020.
* [58] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In _International Conference on Machine Learning_, 2015.
* [59] Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normalization help optimization? (no, it is not about internal covariate shift). _ArXiv_, abs/1805.11604, 2018.
* [60] Tim Cooijmans, Nicolas Ballas, Cesar Laurent, and Aaron C. Courville. Recurrent batch normalization. _ArXiv_, abs/1603.09025, 2016.
* [61] Tong Wu, Ziwei Liu, Qingqiu Huang, Yu Wang, and Dahua Lin. Adversarial robustness under long-tailed distribution. _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 8655-8664, 2021.
* [62] Zhuo Chen, Jinglong Chen, Yong Feng, Shen Liu, Tianci Zhang, Kaiyu Zhang, and Wenrong Xiao. Imbalance fault diagnosis under long-tailed distribution: Challenges, solutions and prospects. _Knowl. Based Syst._, 258:110008, 2022.
* [63] Xiyu He and Xiang Qian. A real-time surface defect detection system for industrial products with long-tailed distribution. _2021 IEEE Industrial Electronics and Applications Conference (IEACon)_, pages 313-317, 2021.
* [64] Xudong Wang, Long Lian, Zhongqi Miao, Ziwei Liu, and Stella X. Yu. Long-tailed recognition by routing diverse distribution-aware experts. _ArXiv_, abs/2010.01809, 2020.

* [65] Shaden Alshammari, Yu-Xiong Wang, Deva Ramanan, and Shu Kong. Long- tailed recognition via weight balancing. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6887-6897, 2022.
* [66] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shiliang Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu. One transformer fits all distributions in multi-modal diffusion at scale. _ArXiv_, abs/2303.06555, 2023.
* [67] Zebin You, Yong Zhong, Fan Bao, Jiacheng Sun, Chongxuan Li, and Jun Zhu. Diffusion models and semi-supervised learners benefit mutually with few labels. _ArXiv_, abs/2302.10586, 2023.
* [68] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _ArXiv_, abs/2206.00364, 2022.
* [69] Diederik P. Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. _ArXiv_, abs/1807.03039, 2018.
* [70] Xiulong Yang, Sheng-Min Shih, Yinlin Fu, Xiaoting Zhao, and Shihao Ji. Your vit is secretly a hybrid discriminative-generative diffusion model. _arXiv preprint arXiv:2208.07791_, 2022.
* [71] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in Neural Information Processing Systems_, 34:8780-8794, 2021.
* [72] Wanshu Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank Wang. Frido: Feature pyramid diffusion for complex scene image synthesis. _ArXiv_, abs/2208.13753, 2022.
* [73] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. _arXiv preprint arXiv:2010.02502_, 2020.
* [74] Dongjun Kim, Seung-Jae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation. In _International Conference on Machine Learning_, 2021.
* [75] Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). _arXiv: Learning_, 2015.
* [76] Abien Fred Agarap. Deep learning using rectified linear units (relu). _ArXiv_, abs/1803.08375, 2018.
* [77] Senwei Liang, Zhongzhan Huang, Mingfu Liang, and Haizhao Yang. Instance enhancement batch normalization: an adaptive regulator of batch noise. In _AAAI Conference on Artificial Intelligence_, 2019.
* [78] Diganta Misra. Mish: A self regularized non-monotonic activation function. In _British Machine Vision Conference_, 2020.
* [79] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu. Squeeze-and-excitation networks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 42:2011-2023, 2017.
* [80] Shan Zhong, Wushao Wen, and Jinghui Qin. Switchable self-attention module. _ArXiv_, abs/2209.05680, 2022.
* [81] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In-So Kweon. Cbam: Convolutional block attention module. In _European Conference on Computer Vision_, 2018.
* [82] Zhongzhan Huang, Senwei Liang, Mingfu Liang, and Haizhao Yang. Dianet: Dense-and-implicit attention network. In _AAAI Conference on Artificial Intelligence_, 2019.
* [83] Gavin E Crooks. Survey of simple, continuous, univariate probability distributions. Technical report, Technical report, Lawrence Berkeley National Lab, 2013., 2012.
* [84] Robert B Davies. Algorithm as 155: The distribution of a linear combination of \(\chi\) 2 random variables. _Applied Statistics_, pages 323-333, 1980.
* [85] Robert B Davies. Numerical inversion of a characteristic function. _Biometrika_, 60(2):415-417, 1973.
* [86] Zhongzhan Huang, Wenqi Shao, Xinjiang Wang, Liang Lin, and Ping Luo. Rethinking the pruning criteria for convolutional neural network. _Advances in Neural Information Processing Systems_, 34:16305-16318, 2021.
* [87] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International Conference on Machine Learning_, pages 242-252. PMLR, 2019.

The appendix is structured as follows. In Appendix A, we provide the training details and network architecture design for all the experiments conducted in this paper. Additionally, we present the specific core code used in our study. Appendix B presents the proofs for all the theorems and lemmas introduced in this paper. Finally, in Appendix C, we showcase additional examples of feature oscillation to support our argument about the training instability of existing UNet models when training diffusion models. Furthermore, we investigate the case of scaling coefficient \(\kappa\) greater than 1 in CS, and the experimental results are consistent with our theoretical analysis.

## Appendix A Implementation details

### Experimental setup

In this paper, we consider four commonly used datasets for our experiments: CIFAR10 [32], CelebA [33], ImageNet [34], and MS-COCO [35]. For the unconditional setting, we use CIFAR10, which consists of 50,000 training images, and CelebA 64, which contains 162,770 training images of human faces. For the class-conditional settings, we utilize the well-known ImageNet dataset at 64 \(\times\) 64 resolutions, which contains 1,281,167 training images from 1,000 different classes. For the text-to-image setting, we consider the MS-COCO dataset at 256 \(\times\) 256 resolution. Unless otherwise specified, all of our experiments are conducted on A100 GPU and follow the experimental settings in [36].

### Network architecture details.

The scaling strategies we proposed on long skip connections, namely CS and LS, are straightforward to implement and can be easily transferred to neural network architectures of different diffusion models. Fig. 9 and Fig. 10 compare the original UViT with the code incorporating CS or LS, demonstrating that our proposed methods require minimal code modifications.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline
**Dataset** & **\#class** & **\#training** & **\#type** & **\#Image size** \\ \hline CIFAR10 & 10 & 50,000 & Uncondition & 32 x 32 \\ CelebA & 100 & 162,770 & Uncondition & 64 x 64 \\ ImageNet 64 & 1000 & 1,281,167 & Class-condition & 64 x 64 \\ MS-COCO & - & 82,783 & Text-to-Image & 256 x 256 \\ \hline \hline \end{tabular}
\end{table}
Table 5: The summary of the datasets for the experiments.

Figure 8: The examples of the generated images by UViT with our proposed LS.

CS requires no additional parameters and can rapidly adapt to any UNet-alike architecture and dataset. In our experiments, we have chosen values of \(\kappa\) as 0.5, 0.5, 0.7, and 0.8 for the four datasets illustrated in Table 5, respectively. In various training environments, we recommend setting \(\kappa\) within the range of 0.5 to 0.95. LS designs a calibration network shared by all long skip connections to predict scaling coefficients \(\{\kappa_{i}\}\). Specifically, for the \(i\)-th long skip connection, its input feature is \(x_{i}\in\mathbb{R}^{B\times N\times D}\), where \(B\) denotes the batch size. For convolution-based UNet, \(N\) and \(D\) respectively denote the channel number and spatial dimension (\(H\times W\)) of the feature map; for transformer-based UViT [36], \(N\) and \(D\) respectively represent the token number and token dimension.

In the main text, we define the LS as

\[\kappa_{i}=\sigma(\zeta_{\phi}[\text{GAP}(x_{i})])\in\mathbb{R}^{B\times N \times 1},1\leq i\leq N,\] (10)

For \(\zeta_{\phi}\), we employ a neural network structure based on SENet [79], given by:

\[\zeta_{\phi}(x)=\mathbf{W}_{1}\circ\psi(\mathbf{W}_{2}\circ x),\] (11)

where \(x\in\mathbb{R}^{N}\), \(\circ\) denotes matrix multiplication, and \(\psi\) represents the ReLU activation function. The matrices \(\mathbf{W}_{1}\in\mathbb{R}^{N\times N/r}\) and \(\mathbf{W}1\in\mathbb{R}^{N/r\times N}\), where \(r\) is the compression ratio, are set to \(r=16\) in this paper. In the ablation study of the main text, we have demonstrated that the choice of \(r\) has a minimal impact on the performance of the diffusion model. It is worth noting that if LS is applied to long skip connections where the feature dimensions remain unchanged, such as in UViT, then the calibration network is only a shared \(\zeta_{\phi}(x)\).

However, when LS is applied to neural networks where the dimension of features in certain long skip connections may change, such as in UNet with downsample and upsample operations, we need to provide a set of encoders and decoders in the calibration network to ensure dimension alignment for different feature dimensions.

Assuming there are \(d\) different feature dimensions in the long skip connections, where the dimension of the \(i\)-th feature is (\(B\times N_{i}\times D\)), \(1\leq i\leq d\), the input dimension of \(\zeta_{\phi}(x)\) is set to \(N_{\min}=\min N_{i}\). Additionally, we set up \(d\) sets of encoders and decoders, where features with the same dimension

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Dataset** & **CIFAR10 64\&64 CelebA 64\&6 ImageNet 64\&63-COCO** \\ \hline Latent shape & - & - & - & 32\(\times\)32\(\times\)4 \\ Batch size & 128/64/32 & 128 & 1024 & 256 \\ Training iterations & 500k & 500k & 300k & 1M \\ Warm-up steps & 2.5k & 4k & 5k & 5k \\ Optimizer & AdamW & AdamW & AdamW & AdamW \\ Learning rate & 2.00E-04 & 2.00E-04 & 3.00E-04 & 2.00E-04 \\ Weight decay & 0.03 & 0.03 & 0.03 & 0.03 \\ Beta & (0.99,0.999) & (0.99,0.99) & (0.99,0.99) & (0.9,0.9) \\ \hline \hline \end{tabular}
\end{table}
Table 6: The training settings for different dataset.

Figure 9: The comparison between the original UViT and our proposed CS.

share an encoder and a decoder. The \(i\)-th set of encoder \(\zeta_{\text{en}}^{(i)}\) and decoder \(\zeta_{\text{de}}^{(i)}\) also adopt the SENet structure, given by:

\[\zeta_{\text{en}}^{(i)}(x)=\mathbf{W}_{N_{\min}\times N_{i}/r_{i}}\circ\psi( \mathbf{W}_{N_{i}/r_{i}\times N_{i}}\circ x),\quad x\in\mathbb{R}^{N_{i}}\] (12)

where \(r_{i}\) is the compression ratio, typically set to \(\lfloor N_{i}/4\rfloor\), and

\[\zeta_{\text{de}}^{(i)}(x)=\mathbf{W}_{N_{i}\times N_{i}/r_{i}}\circ\psi( \mathbf{W}_{N_{i}/r_{i}\times N_{\min}}\circ x),\quad x\in\mathbb{R}^{N_{\min}}.\] (13)

In fact, since the parameter count of \(\zeta_{\phi}(x)\) itself is not substantial, to make LS more versatile, we can also individually set a scaling module into each long skip connection. This configuration typically does not lead to significant performance degradation and can even result in improved performance, while maintaining the same inference speed as the original LS.

Figure 10: The comparison between the original UViT and our proposed LS.

The proof of the Theorems and Lemma

### Preliminaries

**Diffusion Model (DM).** DDPM-alike DMs [1, 2, 3, 43, 5, 6, 8, 9, 4] generates a sequence of noisy samples \(\{\mathbf{x}_{i}\}_{i=1}^{T}\) by repeatedly adding Gaussian noise to a sample \(\mathbf{x}_{0}\) until attutaining \(\mathbf{x}_{T}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). This noise injection process, a.k.a. the forward process, can be formalized as a Markov chain \(q\left(\mathbf{x}_{1:T}|\mathbf{x}_{0}\right)=\prod_{t=1}^{T}q\left(\mathbf{x }_{t}|\mathbf{x}_{t-1}\right)\), where \(q(\mathbf{x}_{t}|\mathbf{x}_{t-1})=\mathcal{N}(\mathbf{x}_{t}|\sqrt{\alpha_{ t}}\mathbf{x}_{t-1},\beta_{t}\mathbf{I})\), \(\alpha_{t}\) and \(\beta_{t}\) depend on \(t\) and satisfy \(\alpha_{t}+\beta_{t}=1\). By using the properties of Gaussian distribution, the forward process can be written as

\[q(\mathbf{x}_{t}|\mathbf{x}_{0})=\mathcal{N}(\mathbf{x}_{t};\sqrt{\bar{ \alpha}_{t}}\mathbf{x}_{0},(1-\bar{\alpha}_{t})\mathbf{I}),\] (14)

where \(\bar{\alpha}_{t}=\prod_{i=1}^{t}\alpha_{i}\). Next, one can sample \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t }}\epsilon_{t}\), where \(\epsilon_{t}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\). Then DM adopts a neural network \(\hat{\epsilon}_{\theta}(\cdot,t)\) to invert the forward process and predict the noise \(\epsilon_{t}\) added at each time step. This process is to recover data \(\mathbf{x}_{0}\) from a Gaussian noise by minimizing the loss

\[\ell_{\text{simple}}^{t}(\theta)=\mathbb{E}||\epsilon_{t}-\hat{\epsilon}_{ \theta}(\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon_{t},t)||_{2}^{2}.\] (15)

**UNet-alike Network.** Since the network \(\hat{\epsilon}_{\theta}(\cdot,t)\) in DMs predicts the noise to denoise, it plays a similar role of UNet-alike network widely in image restoration tasks, e.g. image de-raining, image denoising [22, 23, 24, 25, 26, 44, 45]. This inspires DMs to use UNet-alike network in (16) that uses LSCs to connect distant parts of the network for long-range information transmission and aggregation

\[\mathbf{UNet}(x)=f_{0}(x),\quad f_{i}(x)=b_{i+1}\circ\left[\kappa_{i+1}\cdot a _{i+1}\circ x+f_{i+1}\left(a_{i+1}\circ x\right)\right],\ \ 0\leq i\leq N-1\] (16)

where \(x\in\mathbb{R}^{m}\) denotes the input, \(a_{i}\) and \(b_{i}\) (\(i\geq 1\)) are the trainable parameter of the \(i\)-th block, \(\kappa_{i}>0\) (\(i\geq 1\)) are the scaling coefficients and are set to 1 in standard UNet. \(f_{N}\) is the middle block of UNet. For the vector operation \(\circ\), it can be designed to implement different networks.

W.l.o.g, in this paper, we consider \(\circ\) as matrix multiplication, and set the block \(a_{i}\) and \(b_{i}\) as the stacked networks [46, 47] which is implemented as

\[a_{i}\circ x=\mathbf{W}_{l}^{a_{i}}\phi(\mathbf{W}_{l-1}^{a_{i}}...\phi( \mathbf{W}_{1}^{a_{i}}x)),\quad b_{i}\circ x=\mathbf{W}_{l}^{b_{i}}\phi( \mathbf{W}_{l-1}^{b_{i}}...\phi(\mathbf{W}_{1}^{b_{i}}x))\] (17)

with a ReLU activation function \(\phi\) and learnable matrices \(\mathbf{W}_{j}^{a_{i}},\mathbf{W}_{j}^{b_{i}}\in\mathbb{R}^{m\times m}\) (\(j\geq 1\)). Moreover, let \(f_{N}\) also have the same architecture, i.e. \(f_{N}(x)=\mathbf{W}_{l}^{f_{i}}\phi(\mathbf{W}_{l-1}^{f_{i}}...\phi(\mathbf{ W}_{1}^{f_{i}}x))\).

**Proposition B.1** (Amoroso distribution).: _The Amoroso distribution is a four parameter, continuous, univariate, unimodal probability density, with semi-infinite range [83]. And its probability density function is_

\[\mathbf{Amoroso}(X|a,\theta,\alpha,\beta)=\frac{1}{\Gamma(\alpha)}|\frac{ \beta}{\theta}|(\frac{X-a}{\theta})^{\alpha\beta-1}\exp\left\{-(\frac{X-a}{ \theta})^{\beta}\right\},\] (18)

_for \(x,a,\theta,\alpha,\beta\in\mathbb{R},\alpha>0\) and range \(x\geq a\) if \(\theta>0\), \(x\leq a\) if \(\theta<0\). The mean and variance of Amoroso distribution are_

\[\mathbb{E}_{X\sim\mathbf{Amoroso}(X|a,\theta,\alpha,\beta)}X=a+\theta\cdot \frac{\Gamma(\alpha+\frac{1}{\beta})}{\Gamma(\alpha)},\] (19)

_and_

\[\mathbf{Var}_{X\sim\mathbf{Amoroso}(X|a,\theta,\alpha,\beta)}X=\theta^{2} \left[\frac{\Gamma(\alpha+\frac{2}{\beta})}{\Gamma(\alpha)}-\frac{\Gamma( \alpha+\frac{1}{\beta})^{2}}{\Gamma(\alpha)^{2}}\right].\] (20)

**Proposition B.2** (Stirling's formula).: _For big enough \(x\) and \(x\in\mathbb{R}^{+}\), we have an approximation of Gamma function:_

\[\Gamma(x+1)\approx\sqrt{2\pi x}\left(\frac{x}{e}\right)^{x}.\] (21)

**Proposition B.3** (Scaled Chi distribution).: _Let \(X=(x_{1},x_{2},...x_{k})\) and \(x_{i},i=1,...,k\) are \(k\) independent, normally distributed random variables with mean 0 and standard deviation \(\sigma\). The statistic \(\ell_{2}(X)=\sqrt{\sum_{i=1}^{k}x_{i}^{2}}\) follows Scaled Chi distribution [83]. Moreover, \(\ell_{2}(X)\) also follows \(\mathbf{Amoroso}(x|0,\sqrt{2},\frac{k}{2},2)\). By Eq. (19) and Eq. (20), the mean and variance of Scaled Chi distribution are_

\[\mathbb{E}_{X\sim N(\mathbf{0},\sigma^{2}\cdot\mathbf{I}_{\mathbf{k}})}[\ell_ {2}(X)]^{j}=2^{j/2}\sigma^{j}\cdot\frac{\Gamma(\frac{k+j}{2})}{\Gamma(\frac{k }{2})},\] (22)

_and_

\[\mathbf{Var}_{X\sim N(\mathbf{0},\sigma^{2}\cdot\mathbf{I}_{\mathbf{k}})}\ell _{2}(X)=2\sigma^{2}\left[\frac{\Gamma(\frac{k}{2}+1)}{\Gamma(\frac{k}{2})}- \frac{\Gamma(\frac{k+1}{2})^{2}}{\Gamma(\frac{k}{2})^{2}}\right].\] (23)

**Lemma B.4**.: _For big enough \(x\) and \(x\in\mathbb{R}^{+}\), we have_

\[\lim_{x\rightarrow+\infty}\left[\frac{\Gamma(\frac{x+1}{2})}{\Gamma(\frac{x}{ 2})}\right]^{2}\cdot\frac{1}{x}=\frac{1}{2}.\] (24)

_And_

\[\lim_{x\rightarrow+\infty}\frac{\Gamma(\frac{x}{2}+1)}{\Gamma(\frac{x}{2})}- \left[\frac{\Gamma(\frac{x+1}{2})}{\Gamma(\frac{x}{2})}\right]^{2}=\frac{1}{4}.\] (25)

Proof.: \[\lim_{x\rightarrow+\infty}\left[\frac{\Gamma(\frac{x+1}{2})}{ \Gamma(\frac{x}{2})}\right]^{2}\cdot\frac{1}{x} \approx\lim_{x\rightarrow+\infty}\left(\frac{\sqrt{2\pi(\frac{x-1 }{2})}\cdot(\frac{x-1}{2e})^{\frac{x-1}{2}}}{\sqrt{2\pi(\frac{x-2}{2})}\cdot( \frac{x-2}{2e})^{\frac{x-2}{2}}}\right)^{2}\cdot\frac{1}{x}\qquad\text{from Proposition. B.2}\] \[=\lim_{x\rightarrow+\infty}\left(\frac{x-1}{x-2}\right)\cdot \frac{(\frac{x-1}{2e})^{x-2}}{(\frac{x-2}{2e})^{x-2}}\cdot\left(\frac{x-1}{2e }\right)\cdot\frac{1}{x}\] \[=\lim_{x\rightarrow+\infty}\left(1+\frac{1}{x-2}\right)^{x-2} \cdot\frac{x-1}{x-2}\cdot\frac{x-1}{2e}\cdot\frac{1}{x}\] \[=\frac{1}{2}\]

on the other hand, we have

\[\lim_{x\rightarrow+\infty}\frac{\Gamma(\frac{x}{2}+1)}{\Gamma( \frac{x}{2})}-\left[\frac{\Gamma(\frac{x+1}{2})}{\Gamma(\frac{x}{2})}\right]^ {2} =\lim_{x\rightarrow+\infty}\frac{x}{2}-\left(1+\frac{1}{x-2} \right)^{x-2}\cdot\frac{x-1}{x-2}\cdot\frac{x-1}{2e}\] \[=\lim_{x\rightarrow+\infty}\frac{x}{2e}\left(e-(1+\frac{1}{x})^{x }\right)\] \[=\frac{1}{2}\left(-\frac{\frac{1}{e}(-e)}{2}\right)\] \[=\frac{1}{4}\]

**Lemma B.5**.: _[_84_, 85_]_ _Let \(\mathbf{x}=(x_{1},x_{2},...,x_{N})\), where \(x_{i},1\leq i\leq N\), are independently and identically distributed random variables with \(x_{i}\sim\mathcal{N}(\mu,\sigma^{2})\). In this case, the mathematical expectation \(\mathbb{E}(\sum_{i=1}^{N}x_{i}^{2})\) can be calculated as \(\sigma^{2}N+\mu^{2}N\)._

**Lemma B.6**.: _[_86_]_ _Let \(F_{ij}\in\mathbb{R}^{N_{i}\times k\times k}\) be the \(j\)-th filter of the \(i\)-th convolutional layer. If all filters are initialized as Kaiming's initialization, i.e., \(\mathcal{N}(0,\frac{2}{N_{i}\times k\times k})\), in \(i\)-th layer, \(F_{ij}\)\((j=1,2,...,N_{i+1})\) are i.i.d and approximately follow such a distribution during training:_

\[F_{ij}\sim\mathcal{N}(\mathbf{0},\mathcal{O}[(k^{2}N_{i})^{-1}]\cdot\mathbf{I} _{N_{i}\times k\times k}).\] (26)

**Lemma B.7**.: _[_47, 87_]_ _For the feedforward neural network \(\mathbf{W}_{l}\phi(\mathbf{W}_{l-1}...\phi(\mathbf{W}_{1}x))\) with initialization \(\mathcal{N}(0,\frac{2}{m_{i}})\), and \(W_{i},1\leq i\leq l,\in\mathbb{R}^{m\times m}\). \(\forall\rho\in(0,1)\), the following inequality holds with at least \(1-\mathcal{O}(l)\exp[-\Omega(m\rho^{2}/l)]\):_

\[(1-\rho)\|h_{0}\|_{2}\leq\|h_{a}\|_{2}\leq(1+\rho)\|h_{0}\|_{2},\quad a\in\{1,2,...,l-1\},\] (27)

_where \(h_{a}\) is \(\phi(\mathbf{W}_{a}h_{a-1})\) for \(a=1,2,...,l-1\) with \(h_{0}=x\)._

**Lemma B.8**.: _For a matrix \(\mathbf{W}\in\mathbb{R}^{m\times m}\) and its elements are independently follow the distribution \(\mathcal{N}(0,c^{2})\), we have the following estimation while \(m\) is large enough_

\[\|\mathbf{W}s\|_{2}\approx\mathcal{O}(\|s\|_{2}),\] (28)

_where \(s\in\mathbb{R}^{m}\)._

Proof.: Let \(v_{i},1\leq i\leq m\) be the \(i\)-th column of matrix \(\mathbf{W}\), and we have \(v_{i}\sim\mathcal{N}(\mathbf{0},c^{2}\mathbf{I}_{m})\). We first prove the following two facts while \(m\rightarrow\infty\):

(1) \(||v_{i}||_{2}\approx||v_{j}||_{2}\rightarrow\sqrt{2}c\cdot\frac{\Gamma((m+1)/ 2)}{\Gamma(m/2)}\),\(1\leq i<j\leq m\);

(2) \(\langle v_{i},v_{j}\rangle\to 0\), \(1\leq i<j\leq m\);

For the fact (1), since Chebyshev inequality, for \(1\leq i\leq m\) and a given \(M\), we have

\[P\left\{||v_{i}||_{2}-\mathbb{E}(||v_{i}||_{2})|\geq\sqrt{M\mathbf{Var}(||v_{i }||_{2})}\right\}\leq\frac{1}{M}.\] (29)

from Proposition B.3 and Lemma. B.4, we can rewrite Eq. (29) when \(m\rightarrow\infty\):

\[P\left\{||v_{i}||_{2}\in\left[\sqrt{2}c\cdot\frac{\Gamma((m+1)/2)}{\Gamma(m/2 )}-\sqrt{\frac{M}{2}}c,\sqrt{2}c\cdot\frac{\Gamma((m+1)/2)}{\Gamma(m/2)}+ \sqrt{\frac{M}{2}}c\right]\right\}\geq 1-\frac{1}{M}.\] (30)

For a small enough \(\epsilon>0\), let \(M=1/\epsilon\). Note that \(\sqrt{\frac{M}{2}}c=c/\sqrt{2\epsilon}\) is a constant. When \(m\rightarrow\infty\), \(\sqrt{2}c\cdot\frac{\Gamma((m+1)/2)}{\Gamma(m/2)}\gg\sqrt{\frac{M}{2}}c\). Hence, for any \(i\in[1,m]\) and any small enough \(\epsilon\), we have

\[P\left\{||v_{i}||_{2}\approx\sqrt{2}c\cdot\frac{\Gamma((m+1)/2)}{\Gamma(m/2 )}\right\}\geq 1-\epsilon.\] (31)

So the fact (1) holds. Moreover, we consider the \(\langle v_{i},v_{j}\rangle\), \(1\leq i<j\leq m\).

Let \(v_{i}=(v_{i1},v_{i2},...,v_{im})\) and \(v_{j}=(v_{j1},v_{j2},...,v_{jm})\). So \(\langle v_{i},v_{j}\rangle=\sum_{p=1}^{m}v_{ip}v_{jp}\). Note that, \(v_{i}\) and \(v_{j}\) are independent, hence

\[\mathbb{E}(v_{ip}v_{jp})=0,\] (32) \[\mathbf{Var}(v_{ip}v_{jp})=\mathbf{Var}(v_{ip})\mathbf{Var}(v_{jp} )+(\mathbb{E}(v_{ip}))^{2}\mathbf{Var}(v_{jp})+(\mathbb{E}(v_{jp}))^{2}\mathbf{ Var}(v_{ip})=c^{4},\] (33)

since central limit theorem, we have

\[\sqrt{m}\cdot c^{-2}\cdot\left(\frac{1}{m}\sum_{p=1}^{m}v_{ip}v_{jp}-0\right) \sim N(0,1),\] (34)

According to Eq. (22), Lemma B.4 and Eq. (34), when \(m\rightarrow\infty\), we have

\[\frac{\langle v_{i},v_{j}\rangle}{||v_{i}||_{2}\cdot||v_{j}||_{2}}\rightarrow \frac{1}{\sqrt{m}}\cdot\frac{\langle v_{i},v_{j}\rangle}{\sqrt{m}}\sim N(0, \mathcal{O}(\frac{1}{m}))\to N(0,0).\] (35)Therefore, the fact (2) holds and we have \(\langle v_{i},v_{j}\rangle\to 0\), \(1\leq i<j\leq m\).

Next, we consider \(\|\mathbf{W}s\|_{2}\) for any \(s\in\mathbb{R}^{m}\).

\[\begin{split}\|\mathbf{W}s\|_{2}^{2}&=s^{\mathrm{T} }[v_{1},v_{2},...,v_{m}]^{\mathrm{T}}[v_{1},v_{2},...,v_{m}]s\\ &=s^{\mathrm{T}}\begin{pmatrix}\|v_{1}\|_{2}^{2}&v_{1}^{\mathrm{ T}}v_{2}&\cdots&v_{1}^{\mathrm{T}}v_{m}\\ v_{2}^{\mathrm{T}}v_{1}&\|v_{2}\|_{2}^{2}&&\vdots\\ \vdots&&\ddots&\\ v_{m}^{\mathrm{T}}v_{1}&\cdots&&\|v_{m}\|_{2}^{2}\end{pmatrix}s\end{split}\] (36)

From the facts (1) and (2), when \(m\) is large enough, we have \(v_{i}^{\mathrm{T}}v_{j}\to 0,i\neq j\), and \(\|v_{1}\|_{2}^{2}\approx\|v_{2}\|_{2}^{2}\approx\cdots\approx\|v_{m}\|_{2}^{2} :=c_{v}\). Therefore, we have

\[\|\mathbf{W}s\|_{2}^{2}=s^{\mathrm{T}}c_{v}\mathbf{I}_{\mathbf{m}}s=\mathcal{ O}(\|s\|_{2}^{2}).\] (37)

Let \(k=1\) in Lemma B.6, and the convolution kernel degenerates into a learnable matrix. In this case, we can observe that if \(W\in\mathbb{R}^{m\times m}\) is initialized using Kaiming initialization, i.e., \(\mathcal{N}(0,2/m)\), during the training process, the elements of \(W\) will approximately follow a Gaussian distribution \(\mathcal{N}(0,\mathcal{O}(m^{-1}))\). In this case, we can easily rewrite Lemma B.7 as the following Corollary B.9:

**Corollary B.9**.: _For the feedforward neural network \(\mathbf{W}_{l}\phi(\mathbf{W}_{l-1}...\phi(\mathbf{W}_{1}x))\) with kaiming's initialization during training. \(\forall\rho\in(0,1)\), the following inequality holds with at least \(1-\mathcal{O}(l)\exp[-\Omega(m\rho^{2}/l)]\):_

\[\mathcal{O}((1-\rho)\|h_{0}\|_{2})\leq\|h_{a}\|_{2}\leq\mathcal{O}((1+\rho)\| h_{0}\|_{2}),\quad a\in\{1,2,...,l-1\},\] (38)

_where \(h_{a}\) is \(\phi(\mathbf{W}_{a}h_{a-1})\) for \(a=1,2,...,l-1\) with \(h_{0}=x\)._

Moreover, considering Lemma B.6 and Lemma B.8, we can conclude that the matrix \(W_{l}\) in Corollary B.9 will not change the magnitude of its corresponding input \(\phi(\mathbf{W}_{l-1}...\phi(\mathbf{W}_{1}x))\). In other words, we have the following relationship:

\[\mathcal{O}((1-\rho)\|h_{0}\|_{2})\leq\|\mathbf{W}_{l}h_{l-1}\|_{2}\leq \mathcal{O}((1+\rho)\|h_{0}\|_{2}),\] (39)

Therefore, for \(a_{i}\) and \(b_{i}\) defined in Eq. (17), we have following corollary:

**Corollary B.10**.: _For \(a_{i}\) and \(b_{i}\) defined in Eq. (17), \(1\leq i\leq N\), and their corresponding input \(x_{a}\) and \(x_{b}\). \(\forall\rho\in(0,1)\), the following each inequality holds with at least \(1-\mathcal{O}(l)\exp[-\Omega(m\rho^{2}/l)]\):_

\[\|a_{i}\circ x_{a}\|_{2}=\mathcal{O}(\|x_{a}\|_{2}),\quad\|b_{i}\circ x_{b}\| _{2}=\mathcal{O}(\|x_{b}\|_{2}),\] (40)

**Lemma B.11**.: _[_47_]_ _Let \(\mathbf{D}\) be a diagonal matrix representing the ReLU activation layer. Assume the training loss is \(\frac{1}{n}\sum_{s=1}^{n}\ell_{s}(\mathbf{W})\), where \(\ell_{s}(\mathbf{W})\) denotes the training loss of the \(s\)-th sample among the \(n\) training samples. For the \(\mathbf{W}_{p}^{q}\) defined in Eq. (17) and \(f_{0}(\mathbf{x}_{t})\) defined in Eq. (16), where \(p\in\{1,2,...,l\},q\in\{a_{i},b_{i}\}\), the following inequalities hold with at least 1-\(\mathcal{O}(nN)\exp[-\Omega(m)]\):_

\[\|\mathbf{D}(\mathbf{W}_{p+1}^{a_{i}})^{\mathrm{T}}\cdots\mathbf{D}(\mathbf{W }_{l}^{a_{i}})^{\mathrm{T}}\cdots\mathbf{D}(\mathbf{W}_{l}^{b_{1}})^{\mathrm{T }}v\|_{2}\leq\mathcal{O}(\|v\|_{2}),\] (41)

_and_

\[\|\mathbf{D}(\mathbf{W}_{p+1}^{b_{i}})^{\mathrm{T}}\cdots\mathbf{D}(\mathbf{W }_{l}^{b_{1}})^{\mathrm{T}}v\|_{2}\leq\mathcal{O}(\|v\|_{2}),\] (42)

_where \(v\in\mathbb{R}^{m}\)._

**Theorem 3.1**.: Assume that all learnable matrices of UNet in Eq. (16) are independently initialized as Kaiming's initialization, i.e., \(\mathcal{N}(0,\frac{2}{m})\). Then for any \(\rho\in(0,1]\), by minimizing the training loss Eq. (15) of DMs, with probability at least \(1-\mathcal{O}(N)\exp[-\Omega(m\rho^{2})]\), we have

\[(1-\rho)^{2}\left[c_{1}\|\mathbf{x}_{t}\|_{2}^{2}\cdot\sum_{j=i+1}^{N}\kappa_{j }^{2}+c_{2}\right]\lesssim\|h_{i}\|_{2}^{2}\lesssim(1+\rho)^{2}\left[c_{1}\| \mathbf{x}_{t}\|_{2}^{2}\cdot\sum_{j=i+1}^{N}\kappa_{j}^{2}+c_{2}\right],\] (43)

where the hidden feature \(h_{i}\) is the output of \(f_{i}\) defined in Eq. (16), \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t} }\epsilon_{t}\) is the input of UNet; \(c_{1}\) and \(c_{2}\) are two constants; \(\kappa_{i}\) is the scaling coefficient of the \(i\)-th LSC in UNet.

Proof.: Let \(s\) represent the input of \(f_{i}\) in Eq. (16), i.e., \(s=a_{i}\circ a_{i-1}\circ\cdots a_{1}\circ\mathbf{x}_{t}\), and let \(h_{i}=f_{i}(s)\). By expanding Eq. (16), we can observe that \(h_{i}=f_{i}(s)\) is mainly composed of the following \(N-i\) terms in the long skip connection:

\[\left\{\begin{array}{l}\kappa_{i+1}\cdot b_{i+1}\circ a_{i+1}\circ s;\\ \kappa_{i+2}\cdot b_{i+1}\circ b_{i+2}\circ a_{i+2}\circ a_{i+1}\circ s;\\ \cdots\\ \kappa_{N}\cdot b_{i+1}\circ b_{i+2}\cdots\circ b_{N}\circ a_{N}\cdots a_{i+ 1}\circ s;\end{array}\right.\] (44)

When \(N\) is sufficiently large, \(h_{i}\) is predominantly dominated by \(N\) long skip connection terms, while the information on the residual term is relatively minor. Hence, we can approximate Eq. (16) as follows:

\[h_{i}=f_{i}(s)\approx\sum_{j=i+1}^{N}\kappa_{j}\cdot\pi_{b}^{j}\circ\pi_{a}^{ j}\circ s,\quad 0\leq i\leq N-1,\] (45)

where \(\pi_{b}^{j}:=b_{i+1}\circ b_{i+2}\cdots\circ b_{j}\), \(\pi_{a}^{j}:=a_{j}\circ a_{j-1}\cdots\circ a_{i+1}\).

In practice, the depth \(l\) of block \(a_{i}\) or \(b_{i}\) is a universal constant, therefore, from Corollary B.9, \(\forall\rho\in(0,1)\) and \(j\in\{i+1,i+2,...,N\}\), with probability at least \(1-\mathcal{O}(N)\exp[-\Omega(m\rho^{2})]\), we have

\[\|h_{\kappa_{j}}\|_{2}\in\mathcal{O}((1\pm\rho)\|\mathbf{x}_{t}\|_{2}),\] (46)

where

\[h_{\kappa_{j}}=\phi(\mathbf{W}_{l-1}^{b_{i+1}}...\phi(\mathbf{W}_{1}^{b_{i+1} }r)),\quad r=b_{i+2}\cdots\circ b_{j}\circ\pi_{a}^{j}\circ s,\] (47)

s.t. \(h_{i}=f_{i}(s)\approx\sum_{j=i+1}^{N}\kappa_{j}\mathbf{W}_{l}^{b_{i+1}}\circ h _{\kappa_{j}}\). From Lemma B.6, the elements of \(\mathbf{W}_{l}^{b_{i+1}}\) will approximately follow a Gaussian distribution \(\mathcal{N}(0,\mathcal{O}(m^{-1}))\), therefore, \(\mathbb{E}(\mathbf{W}_{l}^{b_{i+1}}\circ h_{\kappa_{j}})=\mathbf{0}\). Moreover, from Lemma B.8,

\[\mathbf{Var}(\mathbf{W}_{l}^{b_{i+1}}\circ h_{\kappa_{j}})\approx h_{\kappa_{ j}}^{\mathrm{T}}\mathcal{O}(m^{-1})\cdot\mathbf{I}_{m\times m}h_{\kappa_{j}}= \mathcal{O}(m^{-1})\|h_{\kappa_{j}}\|_{2}^{2}\cdot\mathbf{I}_{m\times m}.\] (48)

Therefore [47], \(h_{i}\approx\sum_{j=i+1}^{N}\kappa_{j}\mathbf{W}_{l}^{b_{i+1}}\circ h_{\kappa _{j}}\) can be approximately regarded as a Gaussian variable with zero mean and covariance matrix \(\sum_{j=i+1}^{N}\kappa_{j}^{2}\mathcal{O}(m^{-1})\|h_{\kappa_{j}}\|_{2}^{2} \cdot\mathbf{I}_{m\times m}\). According to Lemma B.5, we have

\[\|h_{i}\|_{2}^{2}\approx\mathbb{E}(\sum_{j=i+1}^{N}\kappa_{j}\mathbf{W}_{l}^{ b_{i+1}}\circ h_{\kappa_{j}})=m\cdot\sum_{j=i+1}^{N}\kappa_{j}^{2}\mathcal{O}(m^{-1})\|h_{ \kappa_{j}}\|_{2}^{2}=\sum_{j=i+1}^{N}\kappa_{j}^{2}\mathcal{O}(1)\|h_{\kappa_ {j}}\|_{2}^{2}.\] (49)

From Eq. (46) and Eq. (49), we have

\[(1-\rho)^{2}\left[c_{1}\|\mathbf{x}_{t}\|_{2}^{2}\cdot\sum_{j=i+1}^{N}\kappa_{ j}^{2}+c_{2}\right]\lesssim\|h_{i}\|_{2}^{2}\lesssim(1+\rho)^{2}\left[c_{1}\| \mathbf{x}_{t}\|_{2}^{2}\cdot\sum_{j=i+1}^{N}\kappa_{j}^{2}+c_{2}\right],\] (50)

where we use \(c_{1}\) to denote \(\mathcal{O}(1)\) in Eq. (49) and we use the constant \(c_{2}\) to compensate for the estimation loss caused by all the approximation calculations in the derivation process.

### The proof of Lemma 3.2.

``` Lemma 3.2. For \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon_{t}\) defined in Eq. (14) as a input of UNet, \(\epsilon_{t}\sim\mathcal{N}(0,\mathbf{I})\), if \(\mathbf{x}_{0}\) follow the uniform distribution \(U[-1,1]\), then we have \[\mathbb{E}\|\mathbf{x}_{t}\|_{2}^{2}=(1-2\mathbb{E}_{t}\bar{\alpha}_{t}/3)m= \mathcal{O}(m).\] (51) ```

Proof.: For \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon_{t}\), we have

\[\mathbb{E}\|\mathbf{x}_{t}\|_{2}^{2}=\mathbb{E}_{t,\mathbf{x}_{0}}(\bar{\alpha }_{t}\sum_{j=1}^{m}\mathbf{x}_{0}^{2}(j)+(1-\bar{\alpha}_{t})m),\] (52)

where \(\mathbf{x}_{0}(j)\) represents the \(j\)-th element of \(\mathbf{x}_{0}\). The equality in Eq. (52) follows from the fact that, from Eq. (14) and Lemma B.5, we know that \(\mathbf{x}_{t}\) follows \(\mathcal{N}(\mathbf{x}_{t};\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0},(1-\bar{ \alpha}_{t})\mathbf{I})\). Therefore, \(\|\mathbf{x}_{t}\|_{2}^{2}\) follows a chi-square distribution, whose mathematical expectation is \(\mathbb{E}_{t,\mathbf{x}_{0}}(\bar{\alpha}_{t}\sum_{j=1}^{m}\mathbf{x}_{0}^{2 }(j)+(1-\bar{\alpha}_{t})m)\).

Moreover, since \(\mathbf{x}_{0}\) follow the uniform distribution \(U[-1,1]\), we have

\[\mathbb{E}\|\mathbf{x}_{t}\|_{2}^{2} =\mathbb{E}_{t}\bar{\alpha}_{t}\cdot\sum_{j=1}^{m}\int_{-\infty} ^{\infty}x^{2}P_{x\sim U[-1,1]}(x)dx+\mathbb{E}_{t}(1-\bar{\alpha}_{t})m\] (53) \[=\mathbb{E}_{t}\bar{\alpha}_{t}\cdot m/3+\mathbb{E}_{t}(1-\bar{ \alpha}_{t})m=(1-2\mathbb{E}_{t}\bar{\alpha}_{t}/3)m=\mathcal{O}(m).\]

where \(P_{x\sim U[-1,1]}(x)\) is density function of the elements in \(\mathbf{x}_{0}\). The last equality in Eq. (53) follows the fact that, according to the setup of the diffusion model, it is generally observed that \(\bar{\alpha}_{t}\) monotonically decreases to 0 as \(t\) increases, with \(\bar{\alpha}_{t}\leq 1\). Consequently, we can easily deduce that \(\mathbb{E}_{t}\bar{\alpha}_{t}\) is of the order \(\mathcal{O}(1)\). For instance, in the setting of DDPM, we have \(\mathbb{E}_{t}\bar{\alpha}_{t}=\mathbb{E}_{t}\left(\prod_{i=1}^{t}\sqrt{1- \frac{0.02i}{1000}}\right)\approx 0.27=\mathcal{O}(1)\).

**Theorem 3.3**. Assume that all learnable matrices of UNet in Eq. (16) are independently initialized as Kaiming's initialization, i.e., \(\mathcal{N}(0,\frac{2}{m})\). Then for any \(\rho\in(0,1]\), with probability at least \(1-\mathcal{O}(nN)\exp[-\Omega(m)]\), for a sample \(\mathbf{x}_{t}\) in training set, we have

\[\|\nabla_{\mathbf{W}_{p}^{q}}\ell_{s}(\mathbf{W})\|_{2}^{2}\lesssim\mathcal{O }\left(\ell_{s}(\mathbf{W})\cdot\|\mathbf{x}_{t}\|_{2}^{2}\cdot\sum\nolimits_ {j=i}^{N}\kappa_{j}^{2}+c_{3}\right),\quad(p\in\{1,2,...,l\},q\in\{a_{i},b_{i} \}),\] (54)

where \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon_{t}\) denotes the noisy sample of the \(s\)-th sample, \(\epsilon_{t}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\), \(N\) is the number of LSCs, \(c_{3}\) is a small constant. \(n\) is the size of the training set. ```

**Theorem 3.3**. Assume that all learnable matrices of UNet in Eq. (16) are independently initialized as Kaiming's initialization, i.e., \(\mathcal{N}(0,\frac{2}{m})\). Then for any \(\rho\in(0,1]\), with probability at least \(1-\mathcal{O}(nN)\exp[-\Omega(m)]\), for a sample \(\mathbf{x}_{t}\) in training set, we have

\[\|\nabla_{\mathbf{W}_{p}^{q}}\ell_{s}(\mathbf{W})\|_{2}^{2}\lesssim\mathcal{O} \left(\ell_{s}(\mathbf{W})\cdot\|\mathbf{x}_{t}\|_{2}^{2}\cdot\sum\nolimits_{j =i}^{N}\kappa_{j}^{2}+c_{3}\right),\quad(p\in\{1,2,...,l\},q\in\{a_{i},b_{i} \}),\] (55)

where \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon_{t}\) denotes the noisy sample of the \(s\)-th sample, \(\epsilon_{t}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\), \(N\) is the number of LSCs, \(c_{3}\) is a small constant. \(n\) is the size of the training set. ```

**Algorithm 1**DDPM

Proof.: From Eq. (16) and Eq. (44), we can approximate the output of UNet \(f_{0}(\mathbf{x}_{t})\) by weight sum of \(N\) forward path \(g^{j},1\leq j\leq N\), i.e.,

\[f_{0}(\mathbf{x}_{t})\approx\sum_{j=1}^{N}\kappa_{i}\cdot g^{j}.\] (56)

where \(g^{j}\) denote \(b_{1}\circ b_{2}\circ\cdots b_{j}\circ a_{j}\circ\cdots a_{1}\circ\mathbf{x}_{t}\), and \(1\leq j\leq N\).

Moreover, we use \(g_{p}^{j,q}\) represent the feature map after \(\mathbf{W}_{p}^{q}\) in forward path \(g^{j}\), i.e., \(g_{p}^{j,q}=\mathbf{W}_{p}^{q}h_{p-1}^{j,q}\) and \(h_{p}^{j,q}=\phi(g_{p}^{j,q})\). Next, we first estimate \(\|\nabla_{\mathbf{W}_{p}^{q}}\ell_{s}(\mathbf{W})\|_{2}^{2}\),\[\|\nabla_{\mathbf{W}_{p}^{a_{i}}}\ell_{s}(\mathbf{W})\|_{2}^{2} =\|\sum_{j=i}^{N}\partial\ell_{s}(\mathbf{W})/\partial g^{j}\cdot \partial g^{j}/\partial g_{p}^{j,a_{i}}\cdot\partial g_{p}^{j,a_{i}}/\partial \mathbf{W}_{p}^{a_{i}}\|_{2}^{2}\] \[\leq\sum_{j=i}^{N}\|\partial\ell_{s}(\mathbf{W})/\partial g^{j} \cdot\partial g^{j}/\partial g_{p}^{j,a_{i}}\cdot\partial g_{p}^{j,a_{i}}/ \partial\mathbf{W}_{p}^{a_{i}}\|_{2}^{2}\] \[\leq\sum_{j=i}^{N}\|\mathbf{D}(\mathbf{W}_{p+1}^{a_{i}})^{\mathrm{ T}}\cdots\mathbf{D}(\mathbf{W}_{l}^{a_{i}})^{\mathrm{T}}\cdots\mathbf{D}( \mathbf{W}_{l}^{b_{1}})^{\mathrm{T}}\cdot\partial\ell_{s}(\mathbf{W})/\partial g ^{j}\|_{2}^{2}\cdot\|h_{p-1}^{j,a_{i}}\|_{2}^{2}\] (56)

From Lemma B.11 and Corollary B.9, with probability at least 1-\(\mathcal{O}(nN)\exp[-\Omega(m)]\), we have

\[\|\nabla_{\mathbf{W}_{p}^{a_{i}}}\ell_{s}(\mathbf{W})\|_{2}^{2} \leq\sum_{j=i}^{N}\mathcal{O}(\|\partial\ell_{s}(\mathbf{W})/ \partial g^{j}\|_{2}^{2})\cdot\mathcal{O}(1)\cdot\|\mathbf{x}_{t}\|_{2}^{2}\] \[=\sum_{j=i}^{N}\left(\mathcal{O}(\kappa_{j}^{2}\ell_{s}(\mathbf{ W})\right)\cdot\mathcal{O}(1)\cdot\|\mathbf{x}_{t}\|_{2}^{2}\] (57) \[=\mathcal{O}\left(\ell_{s}(\mathbf{W})\cdot\|\mathbf{x}_{t}\|_{2 }^{2}\cdot\sum_{j=i}^{N}\kappa_{j}^{2}+c_{3}\right)\]

where the constant \(c_{3}\) can compensate for the estimation loss caused by all the approximation calculations in Eq. (55). Similarly, by Eq. (42), we can prove the situation while \(q=b_{i}\), i.e., \(\|\nabla_{\mathbf{W}_{p}^{b_{i}}}\ell_{s}(\mathbf{W})\|_{2}^{2}\lesssim \mathcal{O}\left(\ell_{s}(\mathbf{W})\cdot\|\mathbf{x}_{t}\|_{2}^{2}\cdot \sum_{j=i}^{N}\kappa_{j}^{2}+c_{3}\right)\).

### The proof of Theorem 3.4.

For UNet in Eq. (16), assume \(M_{0}=\max\{\|b_{i}\circ a_{i}\|_{2},1\leq i\leq N\}\) and \(f_{N}\) is \(L_{0}\)-Lipschitz continuous. \(c_{0}\) is a constant related to \(M_{0}\) and \(L_{0}\). Suppose \(\mathbf{x}_{t}^{\epsilon_{\delta}}\) is an perturbated input of the vanilla input \(\mathbf{x}_{t}\) with a small perturbation \(\epsilon_{\delta}=\|\mathbf{x}_{t}^{\epsilon_{\delta}}-\mathbf{x}_{t}\|_{2}\). Then we have

\[\|\mathbf{UNet}(\mathbf{x}_{t}^{\epsilon_{\delta}})-\mathbf{UNet}(\mathbf{x}_{ t})\|_{2}\leq\epsilon_{\delta}\left[\sum\nolimits_{i=1}^{N}\kappa_{i}M_{0}^{i}+c_{ 0}\right],\] (58)

where \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t}} \epsilon_{t}\), \(\epsilon_{t}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\), \(N\) is the number of the long skip connections.

Proof.: From Eq. (16), we have

\[\mathbf{UNet}(\mathbf{x}_{t})=f_{0}(\mathbf{x}_{t})=b_{1}\circ\left[\kappa_{1 }\cdot a_{1}\circ\mathbf{x}_{t}+f_{1}\left(a_{1}\circ\mathbf{x}_{t}\right) \right],\] (59)

and

\[\mathbf{UNet}(\mathbf{x}_{t}^{\epsilon_{\delta}})=f_{0}(\mathbf{x}_{t}^{ \epsilon_{\delta}})=b_{1}\circ\left[\kappa_{1}\cdot a_{1}\circ\mathbf{x}_{t}^{ \epsilon_{\delta}}+f_{1}\left(a_{1}\circ\mathbf{x}_{t}^{\epsilon_{\delta}} \right)\right].\] (60)

Using Taylor expansion, we have

\[\begin{split}\mathbf{UNet}(\mathbf{x}_{t}^{\epsilon_{\delta}})- \mathbf{UNet}(\mathbf{x}_{t})&=b_{1}\circ\left[\kappa_{1}\cdot a_{1 }\circ(\mathbf{x}_{t}^{\epsilon_{\delta}}-\mathbf{x}_{t})+f_{1}\left(a_{1} \circ\mathbf{x}_{t}^{\epsilon_{\delta}}\right)-f_{1}\left(a_{1}\circ\mathbf{x}_ {t}\right)\right]\\ &=b_{1}\circ a_{1}\circ\left[\kappa_{1}\cdot(\mathbf{x}_{t}^{ \epsilon_{\delta}}-\mathbf{x}_{t})+(\mathbf{x}_{t}^{\epsilon_{\delta}}- \mathbf{x}_{t})^{T}\nabla_{z}f_{1}\left(z\right)|_{z=a_{1}\circ\mathbf{x}_{t}} \right].\end{split}\] (61)

Therefore,

\[\|\mathbf{UNet}(\mathbf{x}_{t}^{\epsilon_{\delta}})-\mathbf{UNet}( \mathbf{x}_{t})\|_{2} =\|b_{1}\circ a_{1}\circ\left[\kappa_{1}\cdot(\mathbf{x}_{t}^{ \epsilon_{\delta}}-\mathbf{x}_{t})+(\mathbf{x}_{t}^{\epsilon_{\delta}}- \mathbf{x}_{t})^{T}\nabla_{z}f_{1}\left(z\right)|_{z=a_{1}\circ\mathbf{x}_{t}} \right]\|_{2}\] \[\leq\epsilon_{\delta}\cdot\|b_{1}\circ a_{1}\|_{2}\cdot\left( \kappa_{1}+\|\nabla_{z}f_{1}\left(z\right)|_{z=a_{1}\circ\mathbf{x}_{t}}\|_{2}\right)\] (62)

Next, we estimate the \(\|\nabla_{z}f_{1}\left(z\right)|_{z=a_{1}\circ\mathbf{x}_{t}}\|_{2}\), From Eq. (16), we have \[\nabla_{z}f_{1}(z) =\nabla_{z}b_{2}\circ\left[\kappa_{2}\cdot a_{2}\circ z+f_{2}\left(a_ {2}\circ z\right)\right]\] (63) \[=\kappa_{2}\cdot b_{2}\circ a_{2}\circ\mathbf{I}+b_{2}\circ a_{2} \circ\nabla_{u}f_{2}\left(u\right)|_{u=a_{2}\circ z},\]

Therefore, we have

\[\|\nabla_{z}f_{1}\left(z\right)|_{z=a_{1}\circ\mathbf{x}_{t}}\|_{2} =\|\big{(}\kappa_{2}\cdot b_{2}\circ a_{2}\circ\mathbf{I}+b_{2} \circ a_{2}\circ\nabla_{u}f_{2}\left(u\right)|_{u=a_{2}\circ z}\big{)}|_{z=a_ {1}\circ\mathbf{x}_{t}}\|_{2}\] (64) \[=\|\kappa_{2}\cdot b_{2}\circ a_{2}\circ\mathbf{I}+b_{2}\circ a_ {2}\circ\nabla_{u}f_{2}\left(u\right)|_{u=a_{2}\circ a_{1}\circ\mathbf{x}_{t} }\|_{2}\] \[\leq\|b_{2}\circ a_{2}\|_{2}\cdot\big{(}\kappa_{2}+\|\nabla_{u}f_ {2}\left(u\right)|_{u=a_{2}\circ a_{1}\circ\mathbf{x}_{t}}\|_{2}\big{)}.\]

From Eq. (62) and Eq. (64), let \(\Delta=\|\mathbf{UNet}(\mathbf{x}_{t}^{\varepsilon_{\delta}})-\mathbf{UNet}( \mathbf{x}_{t})\|_{2}\), we have

\[\Delta \leq\epsilon_{\delta}\cdot\|b_{1}\circ a_{1}\|_{2}\cdot\big{(} \kappa_{1}+\|\nabla_{z}f_{1}\left(z\right)|_{z=a_{1}\circ\mathbf{x}_{t}}\|_{2} \big{)}\] (65) \[\leq\epsilon_{\delta}\cdot\|b_{1}\circ a_{1}\|_{2}\cdot\big{[} \kappa_{1}+\|b_{2}\circ a_{2}\|_{2}\cdot\big{(}\kappa_{2}+\|\nabla_{u}f_{2} \left(u\right)|_{u=a_{2}\circ a_{1}\circ\mathbf{x}_{t}}\|_{2}\big{)}\big{]}\] \[\leq\epsilon_{\delta}\cdot(\kappa_{1}\|b_{1}\circ a_{1}\|_{2}+ \kappa_{2}\|b_{1}\circ a_{1}\|_{2}\cdot\|b_{2}\circ a_{2}\|_{2}+\|b_{1}\circ a _{1}\|_{2}\cdot\|b_{2}\circ a_{2}\|_{2}\cdot\|\nabla_{u}f_{2}\left(u\right)|_ {u=a_{2}\circ a_{1}\circ\mathbf{x}_{t}}\|_{2})\] \[\leq\epsilon_{\delta}\big{[}\sum\nolimits_{i=1}^{N}\left(\kappa _{i}\prod\nolimits_{j=1}^{i}\|b_{j}\circ a_{j}\|_{2}\right)+\prod\nolimits_{ j=1}^{N}\|b_{j}\circ a_{j}\|_{2}\cdot L_{0}\big{]}\] \[\leq\epsilon_{\delta}\cdot\left(\kappa_{1}M_{0}+\kappa_{2}M_{0}^ {2}\ldots+\kappa_{N}M_{0}^{N}+c_{0}\right),\]

where \(c_{0}=M_{0}^{N}L_{0}\), and hence we have \(\|\mathbf{UNet}(\mathbf{x}_{t}^{\varepsilon_{\delta}})-\mathbf{UNet}(\mathbf{ x}_{t})\|_{2}\leq\epsilon_{\delta}\left[\sum\nolimits_{i=1}^{N}\kappa_{i}M_{0}^{i }+c_{0}\right]\) for UNet in Eq. (16) and \(\mathbf{x}_{t}=\sqrt{\bar{\alpha}_{t}}\mathbf{x}_{0}+\sqrt{1-\bar{\alpha}_{t }}\epsilon_{t}\), \(\epsilon_{t}\sim\mathcal{N}(\mathbf{0},\mathbf{I})\), with small perturbation \(\epsilon_{\delta}\).

Next, we demonstrate through empirical experiments that \(M_{0}\) in Theorem 3.4 is generally greater than 1. As shown in Fig. 11, we directly computed \(\|b_{i}\circ a_{i}\|_{2}\) for the Eq. (16) on the CIFAR10 and CelebA datasets. We observe that \(\|b_{i}\circ a_{i}\|_{2}\) exhibits a monotonically increasing trend with training iterations and is always greater than 1 at any iteration. Since \(M_{0}=\max\|b_{i}\circ a_{i}\|_{2},1\leq i\leq N\), we conclude that \(M_{0}\geq 1\).

Figure 11: The norm of \(\|b_{i}\circ a_{i}\|_{2}\) on different dataset.

Other experimental observations

Fig. 12 is a supplement to Fig. 1 in the main text. We investigate the case of UNet with a depth of 12 on the CIFAR10 dataset, where each image represents the visualization results obtained with different random seeds. We observe that the original UNet consistently exhibits feature oscillation, while our proposed method effectively reduces the issue of oscillation, thereby stabilizing the training of the diffusion model.

In the main text, we specify that the domain of \(\kappa\) in CS should be (0,1). This ensures stable forward and backward propagation of the model and enhances the model's robustness to input noise, as discussed in Section 3. Otherwise, it would exacerbate the instability of the model training.

In Fig. 13, we illustrate the impact of having \(\kappa>1\) on the performance of the diffusion model. We observe that generally, having \(\kappa>1\) leads to a certain degree of performance degradation, especially in unstable training settings, such as when the batch size is small. For example, in the case of ImageNet64, when the batch size is reduced to 256, the model's training rapidly collapses when \(\kappa>1\). These experimental results regarding \(\kappa\) align with the analysis presented in Section 3.

Figure 12: More examples for feature oscillation under different random seeds (UNets depth = 12).

Figure 13: The situation that the \(\kappa_{i}=\kappa>1\) under different settings.