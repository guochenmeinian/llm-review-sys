# A Universal Growth Rate for

Learning with Smooth Surrogate Losses

 Anqi Mao

Courant Institute

New York, NY 10012

aqmao@cims.nyu.edu &Mehryar Mohri

Google Research & CIMS

New York, NY 10011

mohri@google.com &Yutao Zhong

Courant Institute

New York, NY 10012

yutao@cims.nyu.edu

###### Abstract

This paper presents a comprehensive analysis of the growth rate of \(\mathcal{H}\)-consistency bounds (and excess error bounds) for various surrogate losses used in classification. We prove a square-root growth rate near zero for smooth margin-based surrogate losses in binary classification, providing both upper and lower bounds under mild assumptions. This result also translates to excess error bounds. Our lower bound requires weaker conditions than those in previous work for excess error bounds, and our upper bound is entirely novel. Moreover, we extend this analysis to multi-class classification with a series of novel results, demonstrating a universal square-root growth rate for smooth _comp-sum_ and _constrained losses_, covering common choices for training neural networks in multi-class classification. Given this universal rate, we turn to the question of choosing among different surrogate losses. We first examine how \(\mathcal{H}\)-consistency bounds vary across surrogates based on the number of classes. Next, ignoring constants and focusing on behavior near zero, we identify _minimizability gaps_ as the key differentiating factor in these bounds. Thus, we thoroughly analyze these gaps, to guide surrogate loss selection, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps. Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and \(\mathcal{H}\)-consistency bounds.

## 1 Introduction

Learning algorithms frequently optimize surrogate loss functions like the logistic loss, in lieu of the task's true objective, commonly the zero-one loss. This is necessary when the original loss function is computationally intractable to optimize or lacks essential mathematical properties such as differentiability. But, what guarantees can we rely on when minimizing a surrogate loss? This is a fundamental question with significant implications for learning.

The related property of Bayes-consistency of surrogate losses has been extensively studied in the context of binary classification. Zhang (2004); Bartlett et al. (2006) and Steinwart (2007) established Bayes-consistency for various convex loss functions, including margin-based surrogates. They also introduced excess error bounds (or surrogate regret bounds) for margin-based surrogates. Reid and Williamson (2009) extended these results to proper losses in binary classification.

The Bayes-consistency of several surrogate loss function families in the context of multi-class classification has also been studied by Zhang (2004) and Tewari and Bartlett (2007). Zhang (2004) established a series of results for various multi-class classification formulations, including negative results for multi-class hinge loss functions (Crammer and Singer, 2001), as well as positive results for the sum exponential loss (Weston and Watkins, 1999; Awasthi et al., 2022), the (multinomial) logistic loss (Verhulst, 1838; 1845; Berkson, 1944, 1951), and the constrained losses (Lee et al., 2004).

Later, Tewari and Bartlett (2007) adopted a different geometric method to analyze Bayes-consistency, yielding similar results for these loss function families. Steinwart (2007) developed general tools to characterize Bayes consistency for both binary and multi-class classification. Additionally, excess error bounds have been derived by Pires et al. (2013) for a family of constrained losses and by Duchi et al. (2018) for loss functions related to generalized entropies.

For a surrogate loss \(\ell\), an excess error bound holds for any predictor \(h\) and has the form \(\mathcal{E}_{\ell_{0-1}}(h)\) - \(\mathcal{E}_{\ell_{0-1}}^{*}\leq\Psi(\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^ {*})\), where \(\mathcal{E}_{\ell_{0-1}}(h)\) and \(\mathcal{E}_{\ell}(h)\) represent the expected losses of \(h\) for the zero-one loss and surrogate loss respectively, \(\mathcal{E}_{\ell_{0-1}}^{*}\) and \(\mathcal{E}_{\ell}^{*}\) the Bayes errors for the zero-one and surrogate loss respectively, and \(\Psi\) a non-decreasing function. The _growth rate_ of excess error bounds, that is the behavior of function \(\Psi\) near zero, has gained attention in recent research (Mahdavi et al., 2014; Zhang et al., 2021; Frongillo and Waggoner, 2021; Bao, 2023). Mahdavi et al. (2014) examined the growth rate for _smoothed hinge losses_ in binary classification, demonstrating that smoother losses result in worse growth rates. The optimal rate is achieved with the standard hinge loss, which exhibits linear growth. Zhang et al. (2021) tied the growth rate of excess error bounds in binary classification to two properties of the surrogate loss function: consistency intensity and conductivity. These metrics enable comparisons of growth rates across different surrogates. This prompts a natural question: can we establish rigorous lower and upper bounds for excess error growth rates under specific regularity conditions?

Frongillo and Waggoner (2021) pioneered research on this question in binary classification settings. They established a critical square-root lower bound for excess error bounds when a surrogate loss is locally strongly convex and has a locally Lipschitz gradient. Additionally, they demonstrated a linear excess error bound for Bayes-consistent polyhedral loss functions (convex and piecewise-linear) (Finocchiaro et al., 2019) (see also (Lapin et al., 2016; Ramaswamy et al., 2018; Yu and Blaschko, 2018; Yang and Koyejo, 2020)). More recently, Bao (2023) complemented these results by showing that proper losses associated with Shannon entropy, exponential entropy, spherical entropy, squared \(\alpha\)-norm entropies and \(\alpha\)-polynomial entropies, with \(\alpha>1\), also exhibit a square-root lower bound for excess error bounds relative to the \(\ell_{1}\)-distance.

However, while Bayes-consistency and excess error bounds are valuable, they are not sufficiently informative, as they are established for the family of all measurable functions and disregard the crucial role played by restricted hypothesis sets in learning. As pointed out by Long and Servedio (2013), in some cases, minimizing Bayes-consistent losses can result in constant expected error, while minimizing inconsistent losses can yield an expected loss approaching zero. To address this limitation, the authors introduced the concept of _realizable \(\mathcal{H}\)-consistency_, further explored by Kuznetsov et al. (2014) and Zhang and Agarwal (2020). Nonetheless, these guarantees are only asymptotic and rely on a strong realizability assumption that typically does not hold in practice.

Recent research by Awasthi, Mao, Mohri, and Zhong (2022b, a) and Mao, Mohri, and Zhong (2023f, c, e, b) has instead introduced and analyzed \(\mathcal{H}\)-_consistency bounds_. These bounds are more informative than Bayes-consistency since they are hypothesis set-specific and non-asymptotic. Their work covers broad families of surrogate losses in binary classification, multi-class classification, structured prediction, and abstention (Mao, Mohri, Mohri, and Zhong, 2023a). Crucially, they provide upper bounds on the _estimation error_ of the target loss, for example, the zero-one loss in classification, that hold for any predictor \(h\) within a hypothesis set \(\mathcal{H}\). These bounds relate this estimation error to the surrogate loss estimation error.

Their general form is: \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq\Gamma(\mathcal{E}_{\ell}(h)-\mathcal{ E}_{\ell}^{*}(\mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))\), where \(\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})\) and \(\mathcal{E}_{\ell}^{*}(\mathcal{H})\) represent the best-in-class expected losses for the zero-one and surrogate loss respectively, \(\Gamma\) is a non-negative concave function and \(\mathcal{M}_{\ell_{0-1}}(\mathcal{H})\) and \(\mathcal{M}_{\ell}(\mathcal{H})\) are _minimizability gaps_. The exact definition of these gaps will be detailed later. For now, let us mention that they are non-negative quantities, upper-bounded by the approximation error of their respective loss functions. \(\mathcal{H}\)-consistency bounds subsume excess error bounds as a special case when the hypothesis set is expanded to include all measurable functions, in which case the minimizability gaps vanish. More generally, an \(\mathcal{H}\)-consistency bound with a \(\Gamma\) function implies \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq\Gamma(\mathcal{E}_{\ell}(h)-\mathcal{ E}_{\ell}^{*}(\mathcal{H}))+\Gamma(\mathcal{M}_{\ell}(\mathcal{H}))\) since a concave function \(\Gamma\) with \(\Gamma(0)\geq 0\) is sub-additive over \(\mathbb{R}_{+}\). Thus, when the surrogate estimation loss \(\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}(\mathcal{H})\) is minimized to \(\epsilon\), the zero-one estimation error \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})\) is bounded by \(\Gamma(\epsilon)+\Gamma(\mathcal{M}_{\ell}(\mathcal{H}))-\mathcal{M}_{\ell_{0 -1}}(\mathcal{H})\). Can we characterize the growth rate of \(\mathcal{H}\)-consistency bounds, that is how quickly the functions \(\Gamma\) increase near zero?

**Our results**. This paper presents a comprehensive analysis of the growth rate of \(\mathcal{H}\)-consistency bounds for all margin-based surrogate losses in binary classification, as well as for _comp-sum losses_ and _constrained losses_ in multi-class classification. We establish a square-root growth rate near zero for margin-based surrogate losses \(\ell\) defined by \(\ell(h,x,y)=\Phi(-yh(x))\), assuming only that \(\Phi\) is convex and twice continuously differentiable with \(\Phi^{\prime}(0)\!>\!0\) and \(\Phi^{\prime\prime}(0)\!>\!0\) (Section 4). This includes both upper and lower bounds (Theorem 4.2). These results directly apply to excess error bounds as well. Importantly, our lower bound requires weaker conditions than (Frongillo and Waggoner, 2021, Theorem 4), and our upper bound is entirely novel. This work demonstrates that the \(\mathcal{H}\)-consistency bound growth rate for these loss functions is precisely square-root, refining the "at least square-root" finding of these authors (for excess error bounds). It is known that polyhedral losses admit a linear grow rate (Frongillo and Waggoner, 2021). Thus, a striking dichotomy emerges that reflects previous observations by these authors: \(\mathcal{H}\)-consistency bounds for polyhedral losses exhibit a linear growth rate in binary classification, while they follow a square-root rate for smooth loss functions.

Moreover, we significantly extend our findings to key multi-class surrogate loss families, including _comp-sum losses_(Mao et al., 2023f) (e.g., logistic loss or cross-entropy with softmax (Berkson, 1944), sum-losses (Weston and Watkins, 1999), generalized cross entropy loss (Zhang and Sabuncu, 2018)), and _constrained losses_(Lee et al., 2004; Awasthi et al., 2022a) (Section 5). In Section 5.1, we prove that the growth rate of \(\mathcal{H}\)-consistency bounds for comp-sum losses is exactly square-root. This applies when the auxiliary function \(\Phi\) they are based upon is convex and twice continuously differentiable with \(\Phi^{\prime}(u)\!<\!0\) and \(\Phi^{\prime\prime}(u)\!>\!0\) for all \(u\) in \((0,\frac{1}{2}]\). These conditions hold for all common loss functions used in practice. Further, in Section 5.2, we demonstrate that the square-root growth rate also extends to \(\mathcal{H}\)-consistency bounds for constrained losses. This requires the auxiliary function \(\Phi\) to be convex and twice continuously differentiable with \(\Phi^{\prime}(u)\!>\!0\) and \(\Phi^{\prime\prime}(u)\!>\!0\) for any \(u\geq 0\), alongside an additional technical condition. These are satisfied by all constrained losses typically encountered in practice.

These results reveal a universal square-root growth rate for smooth surrogate losses, the predominant choice in neural network training (over polyhedral losses) for both binary and multi-class classification in applications. Given this universal growth rate, how do we choose between different surrogate losses? Section 6 addresses this question in detail. To start, we examine how \(\mathcal{H}\)-consistency bounds vary across surrogates based on the number of classes. Then, focusing on behavior near zero (ignoring constants), we isolate minimizability gaps as the key differentiating factor in these bounds. These gaps depend solely on the chosen surrogate loss and hypothesis set. We provide a detailed analysis of minimizability gaps, covering: comparisons across different comp-sum losses, conditions where gaps become zero, and general conditions leading to small gaps. These findings help guide surrogate loss selection. Additionally, we demonstrate the key role of minimizability gaps in comparing excess error bounds and \(\mathcal{H}\)-consistency bounds (Appendix F). Importantly, combining \(\mathcal{H}\)-consistency bounds with surrogate loss Rademacher complexity bounds allows us to derive zero-one loss (estimation) learning bounds for surrogate loss minimizers (Appendix O).

For a more comprehensive discussion of related work, please refer to Appendix A. We start with the introduction of necessary concepts and definitions.

## 2 Preliminaries

**Notation and definitions.** We denote the input space by \(\mathcal{X}\) and the label space by \(\mathcal{Y}\), a finite set of cardinality \(n\) with elements \(\{1,\ldots,n\}\). \(\mathcal{D}\) denotes a distribution over \(\mathcal{X}\times\mathcal{Y}\).

We write \(\mathcal{H}_{\mathrm{all}}\) to denote the family of all real-valued measurable functions defined over \(\mathcal{X}\times\mathcal{Y}\) and denote by \(\mathcal{H}\) a subset, \(\mathcal{H}\in\mathcal{H}_{\mathrm{all}}\). The label assigned by \(h\in\mathcal{H}\) to an input \(x\in\mathcal{X}\) is denoted by \(\mathsf{h}(x)\) and defined by \(\mathsf{h}(x)=\operatorname*{argmax}_{y\neq y}h(x,y)\), with an arbitrary but fixed deterministic strategy used for breaking the ties. For simplicity, we fix that strategy to be the one selecting the label with the highest index under the natural ordering of labels.

We will consider general loss functions \(\ell\!:\!\mathcal{H}\times\mathcal{X}\times\mathcal{Y}\to\mathbb{R}_{+}\). For many loss functions used in practice, the loss value at \((x,y)\), \(\ell(h,x,y)\), only depends on the value \(h\) takes at \(x\) and not on its values on other points. That is, there exists a measurable function \(\hat{\ell}\!:\!\mathbb{R}^{n}\times\mathcal{Y}\to\mathbb{R}_{+}\) such that \(\ell(h,x,y)=\hat{\ell}(h(x),y)\), where \(h(x)=[h(x,1),\ldots,h(x,n)]\) is the score vector of the predictor \(h\). We will then say that \(\ell\) is a _pointwise loss function_. We denote by \(\mathcal{E}_{\ell}(h)\) the generalization error or expected loss of a hypothesis \(h\in\mathcal{H}\) and by \(\mathcal{E}_{\ell}^{*}(\mathcal{H})\) the _best-in class error_:\(\mathbb{E}_{(x,y)\sim\mathcal{D}}[\ell(h,x,y)]\), \(\mathcal{E}_{\ell}^{*}(\mathcal{H})=\inf_{h\in\mathcal{H}^{\mathcal{G}}}\mathcal{ E}_{\ell}(h)\). \(\mathcal{E}_{\ell}^{*}(\mathcal{H}_{\rm all})\) is also known as the _Bayes error_. We write \(\mathsf{p}(y\mid x)=\mathcal{D}(Y=y\mid X=x)\) to denote the conditional probability of \(Y=y\) given \(X=x\) and \(p(x)=(\mathsf{p}(1\!\mid\!x),\ldots,\mathsf{p}(n\!\mid\!x))\) for the conditional probability vector for any \(x\in\mathcal{X}\). We denote by \(\mathcal{C}_{\ell}(h,x)\) the _conditional error_ of \(h\in\mathcal{H}\) at a point \(x\in\mathcal{X}\) and by \(\mathcal{C}_{\ell}^{*}(\mathcal{H},x)\) the _best-in-class conditional error_: \(\mathcal{C}_{\ell}(h,x)=\mathbb{E}_{y}[\ell(h,x,y)\mid x]=\mathbb{E}_{y\in \mathcal{Y}}\mathsf{p}(y\!\mid\!x)\,\ell(h,x,y),\mathcal{C}_{\ell}^{*}( \mathcal{H},x)=\inf_{h\in\mathcal{H}}\mathcal{C}_{\ell}(h,x)\), and use the shorthand \(\Delta\mathcal{C}_{\ell,\mathcal{H}}(h,x)=\mathcal{C}_{\ell}(h,x)-\mathcal{C}_ {\ell}^{*}(\mathcal{H},x)\) for the _calibration gap_ or _conditional regret_ of \(\ell\). The generalization error of \(h\) can be written as \(\mathcal{E}_{\ell}(h)=\mathbb{E}_{x}[\mathcal{C}_{\ell}(h,x)]\). For convenience, we also define, for any vector \(p=(p_{1},\ldots,p_{n})\in\Delta^{n}\), where \(\Delta^{n}\) is the probability simplex of \(\mathbb{R}^{n}\), \(\mathcal{C}_{\ell}(h,x,p)=\sum_{y\in\mathcal{Y}}p_{y}\,\ell(h,x,y)\), \(\mathcal{C}_{\ell,\mathcal{H}}^{*}(x,p)=\inf_{h\in\mathcal{H}}\mathcal{C}_{ \ell}(h,x,p)\) and \(\Delta\mathcal{C}_{\ell,\mathcal{H}}(h,x,p)=\mathcal{C}_{\ell}(h,x,p)- \mathcal{C}_{\ell,\mathcal{H}}^{*}(x,p)\). Thus, we have \(\Delta\mathcal{C}_{\ell,\mathcal{H}}(h,x,p(x))=\Delta\mathcal{C}_{\ell, \mathcal{H}}(h,x)\).

We will study the properties of a surrogate loss function \(\ell_{1}\) for a target loss function \(\ell_{2}\). In multi-class classification, \(\ell_{2}\) is typically the zero-one multi-class classification loss function \(\ell_{0-1}\) defined by \(\ell_{0-1}(h,x,y)=1_{h(x)\in y}\). Some surrogate loss functions \(\ell_{1}\) include the max losses (Crammer and Singer, 2001), comp-sum losses (Mao et al., 2023f) and constrained losses (Lee et al., 2004).

**Binary classification.** The definitions just presented were given for the general multi-class classification setting. In the special case of binary classification (two classes), the standard formulation and definitions are slightly different. For convenience, the label space is typically defined as \(\mathcal{Y}=\{-1,+1\}\). Instead of two scoring functions, one for each label, a single real-valued function is used whose sign determines the predicted class. Thus, here, a hypothesis set \(\mathcal{H}\) is a family of measurable real-valued functions defined over \(\mathcal{X}\) and \(\mathcal{H}_{\rm all}\) is the family of all such functions. \(\ell\) is _pointwise_ if there exists a measurable function \(\hat{\ell}\colon\mathbb{R}\times\mathcal{Y}\to\mathbb{R}_{+}\) such that \(\ell(h,x,y)=\hat{\ell}(h(x),y)\). The target loss function is typically the binary loss \(\ell_{0-1}\), defined by \(\ell_{0-1}(h,x,y)=1_{\rm sign(h(x))\neq y}\), where \({\rm sign}(h(x))=1_{h(x)\geq 0}-1_{h(x)<0}\). Some widely used surrogate losses \(\ell_{1}\) for \(\ell_{0-1}\) are margin-based losses, which are defined by \(\ell_{1}(h,x,y)=\Phi(-yh(x))\), for some non-decreasing convex function \(\Phi\colon\mathbb{R}\to\mathbb{R}_{+}\). Instead of two conditional probabilities, one for each label, a single conditional probability corresponding to the positive class \(+1\) is used. That is, let \(\eta(x)=\mathcal{D}(Y=+1\mid X=x)\) denote the conditional probability of \(Y=+1\) given \(X=x\). The conditional error can then be expressed as:

\[\mathcal{C}_{\ell}(h,x)=\mathop{\mathbb{E}}_{y}[\ell(h,x,y)\mid x]=\eta(x)\ell (h,x,+1)+(1-\eta(x))\ell(h,x,-1).\]

For convenience, we also define, for any \(p\in[0,1]\), \(\mathcal{C}_{\ell}(h,x,p)=p\ell(h,x,+1)+(1-p)\ell(h,x,-1)\), \(\mathcal{C}_{\ell,\mathcal{H}}^{*}(x,p)=\inf_{h\in\mathcal{H}}\mathcal{C}_{ \ell}(h,x,p)\) and \(\Delta\mathcal{C}_{\ell,\mathcal{H}}(h,x,p)=\mathcal{C}_{\ell}(h,x,p)-\inf_{h \in\mathcal{H}}\mathcal{C}_{\ell}(h,x,p)\). Thus, we have \(\Delta\mathcal{C}_{\ell,\mathcal{H}}(h,x,\eta(x))=\Delta\mathcal{C}_{\ell, \mathcal{H}}(h,x)\).

To simplify matters, we will use the same notation for binary and multi-class classification, such as \(\mathcal{Y}\) for the label space or \(\mathcal{H}\) for a hypothesis set. We rely on the reader to adapt to the appropriate definitions based on the context.

**Estimation, approximation, and excess errors.** For a hypothesis \(h\), the difference \(\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}(\mathcal{H}_{\rm all})\) is known as the _excess error_. It can be decomposed into the sum of two terms, the _estimation error_, \((\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}(\mathcal{H}))\) and the _approximation error_\(\mathcal{A}_{\ell}(\mathcal{H})=(\mathcal{E}_{\ell}^{*}(\mathcal{H})- \mathcal{E}_{\ell}^{*}(\mathcal{H}_{\rm all}))\):

\[\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}(\mathcal{H}_{\rm all})=(\mathcal{ E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}(\mathcal{H}))+(\mathcal{E}_{\ell}^{*}( \mathcal{H})-\mathcal{E}_{\ell}^{*}(\mathcal{H}_{\rm all})).\] (1)

A fundamental result for a pointwise loss function \(\ell\) is that the Bayes error and the approximation error admit the following simpler expressions. We give a concise proof of this lemma in Appendix B, where we establish the measurability of the function \(x\mapsto\mathcal{C}_{\ell}^{*}(\mathcal{H}_{\rm all},x)\).

**Lemma 2.1**.: _Let \(\ell\) be a pointwise loss function. Then, the Bayes error and the approximation error can be expressed as follows: \(\mathcal{E}_{\ell}^{*}(\mathcal{H}_{\rm all})=\mathbb{E}_{x}[\mathcal{C}_{\ell}^ {*}(\mathcal{H}_{\rm all},x)]\) and \(\mathcal{A}_{\ell}(\mathcal{H})=\mathcal{E}_{\ell}^{*}(\mathcal{H})-\mathbb{E}_ {x}[\mathcal{C}_{\ell}^{*}(\mathcal{H}_{\rm all},x)]\)._

For restricted hypothesis sets \((\mathcal{H}\neq\mathcal{H}_{\rm all})\), the infimum's super-additivity implies that \(\mathcal{E}_{\ell}^{*}(\mathcal{H})\geq\mathbb{E}_{x}[\mathcal{C}_{\ell}^{*}( \mathcal{H},x)]\). This inequality is generally strict, and the difference, \(\mathcal{E}_{\ell}^{*}(\mathcal{H})-\mathbb{E}_{x}[\mathcal{C}_{\ell}^{*}( \mathcal{H},x)]\), plays a crucial role in our analysis.

## 3 \(\mathcal{H}\)-consistency bounds

A widely used notion of consistency is that of _Bayes-consistency_ given below (Steinwart, 2007).

**Definition 3.1** (**Bayes-consistency)**.: A loss function \(\ell_{1}\) is _Bayes-consistent_ with respect to a loss function \(\ell_{2}\), if for any distribution \(\mathcal{D}\) and any sequence \(\{h_{n}\}_{n\in\mathbb{N}}\subset\mathcal{H}_{\rm all}\), \(\lim_{n\to+\infty}\mathcal{E}_{\ell_{1}}(h_{n})-\mathcal{E}_{\ell_{1}}^{*}( \mathcal{H}_{\rm all})=0\) implies \(\lim_{n\to+\infty}\mathcal{E}_{\ell_{2}}(h_{n})-\mathcal{E}_{\ell_{2}}^{*}( \mathcal{H}_{\rm all})=0\).

Thus, when this property holds, asymptotically, a nearly optimal minimizer of \(\ell_{1}\) over the family of all measurable functions is also a nearly optimal optimizer of \(\ell_{2}\). But, Bayes-consistency does not supply any information about a hypothesis set \(\mathcal{H}\) not containing the full family \(\mathcal{H}_{\mathrm{all}}\), that is a typical hypothesis set used for learning. Furthermore, it is only an asymptotic property and provides no convergence guarantee. In particular, it does not give any guarantee for approximate minimizers. Instead, we will consider upper bounds on the target estimation error expressed in terms of the surrogate estimation error, \(\mathcal{H}\)_-consistency bounds_(Awasthi et al., 2022b, a; Mao et al., 2023f), which account for the hypothesis set \(\mathcal{H}\) adopted.

**Definition 3.2** (\(\mathcal{H}\)**-consistency bounds)**.: Given a hypothesis set \(\mathcal{H}\), an \(\mathcal{H}\)_-consistency bound_ relating the loss function \(\ell_{1}\) to the loss function \(\ell_{2}\) for a hypothesis set \(\mathcal{H}\) is an inequality of the form

\[\forall h\in\mathcal{H},\quad\mathcal{E}_{\ell_{2}}(h)-\mathcal{E}_{\ell_{2}}^ {*}(\mathcal{H})+\mathcal{M}_{\ell_{2}}(\mathcal{H})\leq\Gamma\big{(} \mathcal{E}_{\ell_{1}}(h)-\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H})+\mathcal{M} _{\ell_{1}}(\mathcal{H})\big{)},\] (2)

that holds for any distribution \(\mathcal{D}\), where \(\Gamma\colon\mathbb{R}_{+}\to\mathbb{R}_{+}\) is a non-decreasing concave function with \(\Gamma\geq 0\)(Awasthi et al., 2022b, a). Here, \(\mathcal{M}_{\ell_{1}}(\mathcal{H})\) and \(\mathcal{M}_{\ell_{2}}(\mathcal{H})\) are _minimizability gaps_ for the respective loss functions. The minimizability gap for a hypothesis set \(\mathcal{H}\) and loss function \(\ell\) is denoted by \(\mathcal{M}_{\ell}(\mathcal{H})\) and defined as: \(\mathcal{M}_{\ell}(\mathcal{H})=\mathcal{E}_{\ell}^{*}(\mathcal{H})-\mathbb{E }_{\pi}\big{[}\mathcal{E}_{\ell}^{*}(\mathcal{H},x)\big{]}\). It quantifies the discrepancy between the best possible expected loss within a hypothesis class and the expected infimum of pointwise expected losses. This gap is always non-negative: \(\mathcal{M}_{\ell}(\mathcal{H})=\inf_{h\in\mathcal{H}}\mathbb{E}_{\pi}\big{[} \mathcal{C}_{\ell}(h,x)\big{]}-\mathbb{E}_{\pi}[\inf_{h\in\mathcal{H}} \mathcal{C}_{\ell}(\mathcal{H},x)\big{]}\geq 0\), by the infimum's super-addiity, and is bounded above by the approximation error \(\mathcal{A}_{\ell}(\mathcal{H})=\inf_{h\in\mathcal{H}}\mathbb{E}_{\pi}\big{[} \mathcal{C}_{\ell}(h,x)\big{]}-\mathbb{E}_{\pi}[\inf_{h\in\mathcal{H}_{\mathrm{ all}}}\mathcal{C}_{\ell}(\mathcal{H},x)\big{]}\). We further study the key role of minimizability gaps in \(\mathcal{H}\)-consistency bounds and their properties in Section 6 and Appendix D. As shown in Appendix C, under general assumptions, minimizability gaps are essential quantities required in any bound that relates the estimation errors of two loss functions with an arbitrary hypothesis set \(\mathcal{H}\).

Thus, an \(\mathcal{H}\)-consistency bound provides the guarantee that when the surrogate estimation loss \(\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}(\mathcal{H})\) is minimized to \(\epsilon\), the following upper bound holds for the zero-one estimation error:

\[\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}(\mathcal{H})\leq\Gamma(\epsilon+ \mathcal{M}_{\ell}(\mathcal{H}))-\mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq \Gamma(\epsilon)+\Gamma(\mathcal{M}_{\ell}(\mathcal{H}))-\mathcal{M}_{\ell_{0-1 }}(\mathcal{H}),\]

where the second inequality follows from the sub-additivity of a concave function \(\Gamma\) over \(\mathbb{R}_{+}\). We will demonstrate that, for smooth surrogate losses, \(\Gamma(\epsilon)\) scales as \(\sqrt{\epsilon}\). Note, however, that, while \(\Gamma(\epsilon)\) tends to zero when \(\epsilon\to 0\) for functions \(\Gamma\) derived in \(\mathcal{H}\)-consistency bounds, the remaining terms in the bound are constant. This is not surprising as, in general, minimizing the surrogate estimation error to zero _cannot_ guarantee that the zero-one estimation error will also converge to zero. This is well-known, for example, in the case of linear models (Ben-David et al., 2012). Instead, an \(\mathcal{H}\)-consistency bound provides the tightest possible upper bound on the estimation error for the zero-one loss when the surrogate estimation error is minimized.

The upper bound simplifies to \(\Gamma(\epsilon)\) when the minimizability gaps are zero, which occurs when either \(\mathcal{H}=\mathcal{H}_{\mathrm{all}}\) (the set of all measurable functions) or in realizable cases, which are particularly relevant to the practical use of complex neural networks in applications. In Appendix I, we examine more general cases of small minimizability gaps, taking into account the complexity of \(\mathcal{H}\) and the distribution.

Our results cover in particular the special case of excess bounds (\(\mathcal{H}=\mathcal{H}_{\mathrm{all}}\)). Let us emphasize that, for \(\mathcal{H}\neq\mathcal{H}_{\mathrm{all}}\), \(\mathcal{H}\)-consistency bounds offer tighter and more favorable guarantees on the estimation error compared to those derived from excess bounds analysis alone (see Appendix F).

When \(\ell_{2}=\ell_{0-1}\), the zero-one loss, we say that \(\mathcal{T}\) is the \(\mathcal{H}\)_-estimation error transformation function of a surrogate loss \(\ell\)_ if the following holds:

\[\forall h\in\mathcal{H},\quad\mathcal{T}\big{(}\mathcal{E}_{\ell_{0-1}}(h)- \mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H} )\big{)}\leq\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}(\mathcal{H})+ \mathcal{M}_{\ell}(\mathcal{H}),\]

and the bound is _tight_. That is, for any \(t\in[0,1]\), there exists a hypothesis \(h\in\mathcal{H}\) and a distribution such that \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{M }_{\ell_{0-1}}(\mathcal{H})=t\) and \(\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}(\mathcal{H})+\mathcal{M}_{\ell}( \mathcal{H})=\mathcal{T}(t)\). An explicit form of \(\mathcal{T}\) has been characterized for binary margin-based losses (Awasthi et al., 2022b), as well as compsum losses and constrained losses in multi-class classification (Mao et al., 2023b). In the following sections, we will prove the property \(\mathcal{T}(t)=\Theta(t^{2})\) (under mild assumptions), demonstrating a square-root growth rate for \(\mathcal{H}\)-consistency bounds. Appendix E provides examples of \(\mathcal{H}\)-consistency bounds for both binary and multi-class classification. Our analysis also suggests choosing appropriately \(\mathcal{H}\) and the function \(\Gamma\) to ensure a small minimizability gap and to take into account the number of classes and other properties, as discussed in Section 6.

## 4 Binary classification

We consider the broad family of margin-based loss functions \(\ell\) defined for any \(h\in\mathcal{H}\), and \((x,y)\in\mathcal{X}\times\mathcal{Y}\) by \(\ell(h,x,y)=\Phi(-yh(x))\), where \(\Phi\) is a non-decreasing convex function upper-bounding the zero-one loss. Margin-based loss functions include most loss functions used in binary classification. As an example, \(\Phi(u)=\log(1+e^{u})\) for the logistic loss or \(\Phi(u)=\exp(u)\) for the exponential loss. We say that a hypothesis set \(\mathcal{H}\) is _complete_, if for all \(x\in\mathcal{X}\), we have \(\{h(x)\colon h\in\mathcal{H}\}=\mathbb{R}\). As shown by Awasthi et al. (2022b), the transformation \(\mathcal{T}\) has the following form for complete hypothesis sets:

\[\mathcal{T}(t)\colon=\inf_{u\leq 0}f_{t}(u)-\inf_{u\in\mathbb{R}}f_{t}(u).\]

Here, for any \(t\in[0,1]\), \(f_{t}\) is defined by: \(\forall\,u\in\mathbb{R}\), \(f_{t}(u)=\frac{1-t}{2}\Phi(u)+\frac{1+t}{2}\Phi(-u)\). The following result is useful for proving the growth rate in binary classification.

**Theorem 4.1**.: _Let \(\mathcal{H}\) be a complete hypothesis set. Assume that \(\Phi\) is convex and differentiable at zero and satisfies the inequality \(\Phi^{\prime}(0)>0\). Then, the transformation \(\mathcal{T}\) can be expressed as follows:_

\[\forall t\in[0,1],\quad\mathcal{T}(t)=f_{t}(0)-\inf_{u\in\mathbb{R}}f_{t}(u).\]

Proof.: By the convexity of \(\Phi\), for any \(t\in[0,1]\) and \(u\in\mathbb{R}_{-}\), we have

\[f_{t}(u)=\frac{1-t}{2}\Phi(u)+\frac{1+t}{2}\Phi(-u)\geq\Phi(0)-tu\Phi^{\prime }(0)\geq\Phi(0).\]

Thus, we can write \(\mathcal{T}(t)=\inf_{u\leq 0}f_{t}(u)-\inf_{u\in\mathbb{R}}f_{t}(u)\geq\Phi(0)- \inf_{u\in\mathbb{R}}f_{t}(u)=f_{t}(0)-\inf_{u\in\mathbb{R}}f_{t}(u)\), where equality is achieved when \(u=0\). 

**Theorem 4.2** (Upper and lower bound for binary margin-based losses).: _Let \(\mathcal{H}\) be a complete hypothesis set. Assume that \(\Phi\) is convex, twice continuously differentiable, and satisfies the inequalities \(\Phi^{\prime}(0)>0\) and \(\Phi^{\prime\prime}(0)>0\). Then, the following property holds: \(\mathcal{T}(t)=\Theta(t^{2})\); that is, there exist positive constants \(C>0\), \(c>0\), and \(T>0\) such that \(Ct^{2}\geq\mathcal{T}(t)\geq ct^{2}\), for all \(0<t\leq T\)._

**Proof sketch** First, we demonstrate that, by applying the implicit function theorem, \(\inf_{u\in\mathbb{R}}f_{t}(u)\) is attained uniquely by \(a_{t}^{*}\), and that \(a_{t}^{*}\) is continuously differentiable over \([0,\epsilon]\) for some \(\epsilon>0\). The minimizer \(a_{t}^{*}\) satisfies the following condition: \(f_{t}^{\prime}(a_{t}^{*})=\frac{1-t}{2}\Phi^{\prime}(a_{t}^{*})-\frac{1+t}{2} \Phi^{\prime}(-a_{t}^{*})=0\). Specifically, at \(t=0\), we have \(\Phi^{\prime}(a_{0}^{*})=\Phi^{\prime}(-a_{0}^{*})\). Then, by the convexity of \(\Phi\) and monotonicity of the derivative \(\Phi^{\prime}\), we must have \(a_{0}^{*}=0\) and since \(\Phi^{\prime}\) is non-decreasing and \(\Phi^{\prime\prime}(0)>0\), we have \(a_{t}^{*}>0\) for all \(t\in(0,\epsilon]\). Furthermore, since \(a_{t}^{*}\) is a function of class \(C^{\dagger}\), we can differentiate this condition with respect to \(t\) and take the limit \(t\to 0\), which gives the following equality: \(\frac{da_{t}^{*}}{dt}(0)=\frac{\Phi^{\prime}(0)}{\Phi^{\prime\prime}(0)}>0\). Since \(\lim_{t\to 0}\frac{a_{t}^{*}}{t}=\frac{da_{t}^{*}}{dt}\big{(}0\big{)}=\frac{ \Phi^{\prime}(0)}{\Phi^{\prime\prime}(0)}>0\), we have \(a_{t}^{*}=\Theta(t)\). By Theorem 4.1 and Taylor's theorem with an integral remainder, \(\mathcal{T}\) can be expressed as follows: for any \(t\in[0,\epsilon]\), \(\mathcal{T}(t)=f_{t}(0)-\inf_{u\in\mathbb{R}}f_{t}(u)=\int_{0}^{a_{t}^{*}}uf_{ t}^{\prime\prime}(u)\,du=\int_{0}^{a_{t}^{*}}u\big{[}\frac{1-t}{2}\Phi^{ \prime\prime}(u)+\frac{1+t}{2}\Phi^{\prime\prime}(-u)\big{]}du\). Since \(\Phi^{\prime\prime}(0)>0\) and \(\Phi^{\prime\prime}\) is continuous, there is a non-empty interval \([-\alpha,+\alpha]\) over which \(\Phi^{\prime\prime}\) is positive. Since \(a_{0}^{*}=0\) and \(a_{t}^{*}\) is continuous, there exists a sub-interval \([0,\epsilon^{\prime}]\subseteq[0,\epsilon]\) over which \(a_{t}^{*}\leq\alpha\). Since \(\Phi^{\prime\prime}\) is continuous, it admits a minimum and a maximum over any compact set and we can define \(c=\min_{u\in[-\alpha,\alpha]}\Phi^{\prime\prime}(u)\) and \(C=\max_{u\in[-\alpha,\alpha]}\Phi^{\prime\prime}(u)\). \(c\) and \(C\) are both positive since we have \(\Phi^{\prime\prime}(0)>0\). Thus, for \(t\) in \([0,\epsilon^{\prime}]\), the following inequality holds: \(C\frac{(a_{t}^{*})^{2}}{2}=\int_{0}^{a_{t}^{*}}uC\,du\geq\mathcal{T}(t)=\int_{0} ^{a_{t}^{*}}u\big{[}\frac{1-t}{2}\Phi^{\prime\prime}(u)+\frac{1+t}{2}\Phi^{ \prime\prime}(-u)\big{]}du\geq\int_{0}^{a_{t}^{*}}uc\,du=c\frac{(a_{t}^{*})^{2}}{2}\). This implies that \(\mathcal{T}(t)=\Theta(t^{2})\). The full proof is included in Appendix K.

Theorem 4.2 directly applies to excess error bounds as well, when \(\mathcal{H}=\mathcal{H}_{\mathrm{all}}\). Importantly, our lower bound requires weaker conditions than (Frongillo and Waggoner, 2021, Theorem 4), and our upper bound is entirely novel. This result demonstrates that the growth rate for these loss functions is precisely square-root, refining the "at least square-root" finding of these authors. It is known that polyhedral losses admit a linear grow rate (Frongillo and Waggoner, 2021). Thus, a striking dichotomy emerges: \(\mathcal{H}\)-consistency bounds for polyhedral losses exhibit a linear growth rate, while they follow a square-root rate for smooth loss functions (see Appendix G for a detailed comparison).

Multi-class classification

In this section, we will study two families of surrogate losses in multi-class classification: comp-sum losses and constrained losses, defined in Section 5.1 and Section 5.2 respectively. Comp-sum losses and constrained losses are general and cover all loss functions commonly used in practice. We will consider any hypothesis set \(\mathcal{H}\) that is _symmetric_ and _complete_. We say that a hypothesis set is _symmetric_ when it does not depend on a specific ordering of the classes, that is, when there exists a family \(\mathcal{T}\) of functions \(f\) mapping from \(\mathcal{X}\) to \(\mathbb{R}\) such that \(\{[h(x,1),\ldots,h(x,n)]\colon h\in\mathcal{H}\}=\{[f_{1}(x),\ldots,f_{n}(x)] \colon f_{1},\ldots,f_{n}\in\mathcal{T}\}\), for any \(x\in\mathcal{X}\). We say that a hypothesis set \(\mathcal{H}\) is _complete_ if the set of scores it generates spans \(\mathbb{R}\), that is, \(\{h(x,y)\colon h\in\mathcal{H}\}=\mathbb{R}\), for any \((x,y)\in\mathcal{X}\times\mathcal{Y}\).

### Comp-sum losses

Here, we consider comp-sum losses (Mao et al., 2023), defined as

\[\forall h\in\mathcal{H},\forall(x,y)\times\mathcal{X}\times\mathcal{Y},\quad \ell^{\mathrm{comp}}(h,x,y)=\Phi\bigg{(}\frac{e^{h(x,y)}}{\sum_{y^{\prime}\in \mathcal{Y}}e^{h(x,y^{\prime})}}\bigg{)},\]

where \(\Phi\colon\mathbb{R}\to\mathbb{R}_{+}\) is a non-increasing function. For example, \(\Phi\) can be chosen as the negative log function \(u\mapsto-\log(u)\) for the comp-sum losses, which leads to the multinomial logistic loss. As shown by Mao et al. (2023), for symmetric and complete hypothesis sets, the transformation \(\mathcal{T}\) for the family of comp-sum losses can be characterized as follows.

**Theorem 5.1** (Mao et al. (2023, Theorem 3)).: _Let \(\mathcal{H}\) be a symmetric and complete hypothesis set. Assume that \(\Phi\) is convex, differentiable at \(\frac{1}{2}\) and satisfies the inequality \(\Phi^{\prime}(\frac{1}{2})<0\). Then, the transformation \(\mathcal{T}\) can be expressed as_

\[\mathcal{T}(t)=\inf_{\tau\in\big{\{}\frac{1}{2},\frac{1}{2}\big{\}}}\sup_{ \|u\|\leq\tau}\bigg{\{}\Phi(\tau)-\frac{1-t}{2}\Phi(\tau+u)-\frac{1+t}{2}\Phi (\tau-u)\bigg{\}}.\]

Next, we will show that as with the binary case, for the comp-sum losses, the properties \(\mathcal{T}(t)=\Omega(t^{2})\) and \(\mathcal{T}(t)=O(t^{2})\) hold. We first introduce a generalization of the classical implicit function theorem where the function takes the value zero over a set of points parameterized by a compact set. We treat the special case of a function \(F\) defined over \(\mathbb{R}^{3}\) and denote by \((t,a,\tau)\in\mathbb{R}^{3}\) its arguments. The theorem holds more generally for the arguments being in \(\mathbb{R}^{n_{1}}\times\mathbb{R}^{n_{2}}\times\mathbb{R}^{n_{3}}\) and with the condition on the partial derivative being non-zero replaced with a partial Jacobian being non-singular.

**Theorem 5.2** (Implicit function theorem with a compact set).: _Let \(F\colon\mathbb{R}\times\mathbb{R}\times\mathbb{R}\to\mathbb{R}\) be a continuously differentiable function in a neighborhood of \((0,0,\tau)\), for any \(\tau\) in a non-empty compact set \(\mathcal{C}\), with \(F(0,0,\tau)=0\). Then, if \(\frac{\partial F}{\partial a}(0,0,\tau)\) is non-zero for all \(\tau\) in \(\mathcal{C}\), then, there exist a neighborhood \(\mathcal{O}\) of \(0\) and a unique function \(\bar{a}\) defined over \(\mathcal{O}\times\mathcal{C}\) that is continuously differentiable and satisfies_

\[\forall(t,\tau)\in\mathcal{O}\times\mathcal{C},\quad F(t,\bar{a}(t,\tau),\tau )=0.\]

Proof.: By the implicit function theorem (see for example (Dontchev and Rockafellar, 2009)), for any \(\tau\in\mathcal{C}\), there exists an open set \(\mathcal{U}_{\tau}=(-t_{\tau},+t_{\tau})\times(\tau-\epsilon_{\tau},\tau+ \epsilon_{\tau})\), \((t_{\tau}>0\) and \(\epsilon_{\tau}>0)\), and a unique function \(\bar{a}_{\tau}\colon\mathcal{U}_{\tau}\to\mathbb{R}\) that is in \(C^{1}\) and such that for all \((t,\tau)\in\mathcal{U}_{\tau}\), \(F(t,\bar{a}_{\tau}(t),\tau)=0\).

By the uniqueness of \(\bar{a}_{\tau}\), for any \(\tau\neq\tau^{\prime}\) and \((t_{1},\tau_{1})\in\mathcal{U}_{\tau}\cap\mathcal{U}_{\tau^{\prime}}\), we have \(\bar{a}_{\tau}(t_{1},\tau_{1})=\bar{a}_{\tau^{\prime}}(t_{1},\tau_{1})\). Thus, we can define a function \(\bar{a}\) over \(\mathcal{U}=\bigcup_{\tau\in\mathcal{C}}\mathcal{U}_{\tau}\) that is of class \(C^{1}\) and such that for any \((t,\tau)\in\mathcal{U}\), \(F(t,\bar{a}(t,\tau),\tau)=0\).

Now, \(\bigcup_{\tau\in\mathcal{C}}(\tau-\epsilon_{\tau},\tau+\epsilon_{\tau})\) is a cover of the compact set \(\mathcal{C}\) via open sets. Thus, we can extract from it a finite cover \(\bigcup_{\tau\in I}\bigl{(}\tau-\epsilon_{\tau},\tau+\epsilon_{\tau}\bigr{)}\), for some finite cardinality set \(I\). Define \((-t_{0},+t_{0})=\bigcap_{\tau\in I}(-t_{\tau},+t_{\tau})\), which is a non-empty open interval as an intersection of (embedded) open intervals containing zero. Then, \(\bar{a}\) is continously differentiable over \((-t_{0},+t_{0})\times\mathcal{C}\) and for any \((t,\tau)\in(-t_{0},+t_{0})\times\mathcal{C}\), we have \(F(t,\bar{a}(t,\tau),\tau)=0\). 

**Theorem 5.3** (Upper and lower bound for comp-sum losses).: _Assume that \(\Phi\) is convex, twice continuously differentiable, and satisfies the properties \(\Phi^{\prime}(u)<0\) and \(\Phi^{\prime\prime}(u)>0\) for any \(u\in(0,\frac{1}{2}]\). Then, the following property holds: \(\mathcal{T}(t)=\Theta(t^{2})\)._

**Proof sketch** For any \(\tau\in\left[\frac{1}{n},\frac{1}{2}\right]\), define the function \(\mathcal{T}_{\tau}\) by \(\mathcal{T}_{\tau}(t)=f_{t,\tau}(0)-\inf_{|u|\leq\tau}f_{t,\tau}(u)\), where \(f_{t,\tau}(u)=\frac{1-t}{2}\Phi_{\tau}(u)+\frac{1+t}{2}\Phi_{\tau}(-u)\), \(t\in[0,1]\) and \(\Phi_{\tau}(u)=\Phi(\tau+u)\).

We aim to establish a lower and upper bound for \(\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\mathcal{T}_{\tau}(t)\). For any fixed \(\tau\in\left[\frac{1}{n},\frac{1}{2}\right]\), this situation is parallel to that of binary classification (Theorem 4.1 and Theorem 4.2), since we have \(\Phi_{\tau}^{\prime}(0)=\Phi^{\prime}(\tau)<0\) and \(\Phi_{\tau}^{\prime\prime}(0)=\Phi^{\prime\prime}(\tau)>0\). By Theorem 5.2 and the proof of Theorem 4.2, adopting a similar notation, while incorporating the \(\tau\) subscript to distinguish different functions \(\Phi_{\tau}\) and \(f_{t,\tau}\), we can write \(\forall t\in[0,t_{0}]\), \(\mathcal{T}_{\tau}(t)=\int_{0}^{-\Phi_{t,\tau}^{\prime}}u\big{[}\frac{1-t}{2} \Phi_{\tau}^{\prime\prime}(-u)+\frac{1+t}{2}\Phi_{\tau}^{\prime\prime}(u)\big{]}du\), where \(a_{t,\tau}^{*}\) verifies \(a_{0,\tau}^{*}=0\) and \(\frac{\partial a_{t,\tau}^{*}}{\partial t}(0)=\frac{\Phi_{\tau}^{\prime}(0)}{ \Phi_{\tau}^{\prime\prime}(0)}=c_{\tau}<0\). Then, by further analyzing this equality, we can show the lower bound \(\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-a_{t,\tau}^{*}=\Omega(t)\) and the upper bound \(\sup_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-a_{t,\tau}^{*}=O(t)\) for some \(t\in[0,t_{1}]\), \(t_{1}>0\). Finally, using the fact that \(\Phi^{\prime\prime}\) reaches its maximum and minimum over a compact set, we obtain that \(\mathcal{T}(t)=\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\mathcal{T}_ {\tau}(t)=\Theta(t^{2})\). The full proof is included in Appendix L.

Theorem 5.3 significantly extends Theorem 4.2 to multi-class comp-sum losses, which include the logistic loss or cross-entropy used with a softmax activation function. It shows that the growth rate of \(\mathcal{H}\)-consistency bounds for comp-sum losses is exactly square-root, provided that the auxiliary function \(\Phi\) they are based upon is convex, twice continuously differentiable, and satisfies \(\Phi^{\prime}(u)<0\) and \(\Phi^{\prime\prime}(u)>0\) for any \(u\in(0,\frac{1}{2}]\), which holds for most loss functions used in practice.

### Constrained losses

Here, we consider constrained losses (see (Lee et al., 2004)), defined as

\[\forall h\in\mathcal{H},\forall(x,y)\times\mathcal{X}\times\mathcal{Y},\quad \ell^{\mathrm{c}\mathrm{t}\mathrm{a}\mathrm{t}}(h,x,y)=\sum_{y^{\prime}\preceq y }\Phi(h(x,y^{\prime}))\text{ subject to }\sum_{y\in\mathcal{Y}}h(x,y)=0,\]

where \(\Phi\colon\mathbb{R}\to\mathbb{R}_{+}\) is a non-decreasing function. On possible choice for \(\Phi\) is the exponential function. As shown by Mao et al. (2023), for symmetric and complete hypothesis sets, the transformation \(\mathcal{T}\) for the family of constrained losses can be characterized as follows.

**Theorem 5.4** (Mao et al. (2023, Theorem 11)).: _Let \(\mathcal{H}\) be a symmetric and complete hypothesis set. Assume that \(\Phi\) is convex, differentiable at zero and satisfies the inequality \(\Phi^{\prime}(0)>0\). Then, the transformation \(\mathcal{T}\) can be expressed as_

\[\mathcal{T}(t)=\inf_{\tau\geq 0}\sup_{u\in\mathbb{R}}\Biggl{\{}\Bigl{(}2-\frac{1 }{n-1}\Bigr{)}\Phi(\tau)-\frac{2-\frac{1}{n-1}-t}{2}\Phi(\tau+u)-\frac{2-\frac{ 1}{n-1}+t}{2}\Phi(\tau-u)\Biggr{\}}.\]

Next, we will show that for the constrained losses, the properties \(\mathcal{T}(t)=\Omega(t)\) and \(\mathcal{T}(t)=O(t)\) hold as well. Note that by Theorem 5.4, we have

\[\mathcal{T}\biggl{(}\Bigl{(}2-\frac{1}{n-1}\Bigr{)}t\biggr{)}=\biggl{(}2-\frac {1}{n-1}\biggr{)}\inf_{\tau\geq 0}\sup_{u\in\mathbb{R}}\biggl{\{}\Phi(\tau)-\frac{1-t}{2} \Phi(\tau+u)-\frac{1+t}{2}\Phi(\tau-u)\biggr{\}}.\]

Therefore, to prove \(\mathcal{T}(t)=\Theta(t^{2})\), we only need to show

\[\inf_{\tau\geq 0}\sup_{u\in\mathbb{R}}\biggl{\{}\Phi(\tau)-\frac{1-t}{2}\Phi( \tau+u)-\frac{1+t}{2}\Phi(\tau-u)\biggr{\}}=\Theta(t^{2}).\]

For simplicity, we assume that the infimum over \(\tau\geq 0\) can be reached within some finite interval \([0,A]\), \(A>0\). This assumption holds for common choices of \(\Phi\), as discussed in (Mao et al., 2023). Furthermore, as demonstrated in Appendix N, under certain conditions on \(\Phi^{\prime\prime}\), the infimum over \(\tau\in[0,A]\) is reached at zero for sufficiently small values of \(t\). For specific examples, see (Mao et al., 2023, Appendix D.3), where \(\Phi(t)=e^{t}\) is considered.

**Theorem 5.5** (Upper and lower bound for constrained losses).: _Assume that \(\Phi\) is convex, twice continuously differentiable, and satisfies the properties \(\Phi^{\prime}(u)>0\) and \(\Phi^{\prime\prime}(u)>0\) for any \(u\geq 0\). Then, for any \(A>0\), the following property holds:_

\[\inf_{\tau\in[0,A]}\sup_{u\in\mathbb{R}}\biggl{\{}\Phi(\tau)-\frac{1-t}{2}\Phi( \tau+u)-\frac{1+t}{2}\Phi(\tau-u)\biggr{\}}=\Theta(t^{2}).\]

**Proof sketch** For any \(\tau\in[0,A]\), define the function \(\mathcal{T}_{\tau}\) by \(\mathcal{T}_{\tau}(t)=f_{t,\tau}(0)-\inf_{u\in\mathcal{R}}f_{t,\tau}(u)\), where \(f_{t,\tau}(u)=\frac{1-t}{2}\Phi_{\tau}(u)+\frac{1+t}{2}\Phi_{\tau}(-u)\), \(t\in[0,1]\) and \(\Phi_{\tau}(u)=\Phi(\tau+u)\). We aim to establish a lower and upper bound for \(\inf_{\tau\in[0,A]}\mathcal{T}_{\tau}(t)\). For any fixed \(\tau\in[0,A]\), this situation is parallel to that of binary classification (Theorem 4.1 and Theorem 4.2), since we also have \(\Phi_{\tau}^{\prime}(0)=\Phi^{\prime}(\tau)>0\) and \(\Phi_{\tau}^{\prime\prime}(0)=\Phi^{\prime\prime}(\tau)>0\). By applying Theorem 5.2 and leveraging the proof of Theorem 4.2, adopting a similar notation, while incorporating the \(\tau\) subscript to distinguish different functions \(\Phi_{\tau}\) and \(f_{t,\tau}\), we can write \(\forall t\in[0,t_{0}]\), \(\mathcal{T}_{\tau}(t)=\int_{0}^{a_{t,\tau}^{\prime}}u\big{[}\frac{1-t}{2} \Phi_{\tau}^{\prime\prime}(u)+\frac{1+t}{2}\Phi_{\tau}^{\prime\prime}(-u)\big{]}du\), where \(a_{t,\tau}^{*}\) verifies \(a_{0,\tau}^{*}=0\) and \(\frac{\partial a_{t,\tau}^{*}}{\partial t}(0)=\frac{\Phi_{\tau}^{\prime}(0)}{ \Phi_{\tau}^{\prime\prime}(0)}=c_{\tau}>0\). Then, by further analyzing this equality, we can show the lower bound \(\inf_{\tau\in[0,A]}a_{t,\tau}^{*}=\Omega(t)\) and the upper bound \(\sup_{\tau\in[0,A]}a_{t,\tau}^{*}=O(t)\) for some \(t\in[0,t_{1}]\), \(t_{1}>0\). Finally, using the fact that \(\Phi^{\prime\prime}\) reaches its maximum and minimum over some compact set, we obtain that \(\mathcal{T}(t)=\inf_{\tau\in[0,A]}\mathcal{T}_{\tau}(t)=\Theta(t^{2})\). The full proof is included in Appendix M.

Theorem 5.5 significantly expands our findings to multi-class constrained losses. It demonstrates that, under some assumptions, which are commonly satisfied by smooth constrained losses used in practice, constrained loss \(\mathcal{H}\)-consistency bounds also exhibit a square-root growth rate.

## 6 Minimizability gaps

As shown in Sections 4 and 5, \(\mathcal{H}\)-consistency bounds for smooth loss functions in both binary and multi-class classification all admit a square-root growth rate near zero. In this section, we start by examining how the number of classes impacts these bounds. We then turn our attention to the minimizability gaps, which are the only distinguishing factors between the bounds.

### Dependency on number of classes

Even with identical growth rates, surrogate losses can vary in their \(\mathcal{H}\)-consistency bounds due to the number of classes. This factor becomes crucial to consider when the class count is large. Consider the family of comp-sum loss functions \(\ell_{\tau}^{\mathrm{comp}}\) with \(\tau\in[0,2)\), defined as

\[\ell_{\tau}^{\mathrm{comp}}(h,x,y)=\Phi^{\tau}\Bigg{(}\frac{e^{h(x,y)}}{\sum_{ y^{\prime}\in\mathcal{H}}e^{h(x,y^{\prime})}}=\begin{cases}\frac{1}{1-\tau} \Big{(}\Big{[}\sum_{y^{\prime}\in\mathcal{H}}e^{h(x,y^{\prime})-h(x,y)}\Big{]} ^{1-\tau}-1\Big{)}&\tau\neq 1,\tau\in[0,2)\\ \log\Big{(}\sum_{y^{\prime}\in\mathcal{H}}e^{h(x,y^{\prime})-h(x,y)}\Big{)} &\tau=1,\end{cases}\]

where \(\Phi^{\tau}(u)=-\log(u)1_{\tau=1}+\frac{1}{1-\tau}\big{(}u^{\tau-1}-1\big{)}1 _{\tau\neq 1}\), for any \(\tau\in[0,2)\). Mao et al. (2023f, Eq. (7) & Theorem 3.1), established the following bound for any \(h\in\mathcal{H}\) and \(\tau\in[1,2)\),

\[\mathcal{R}_{\ell_{0-1}}(h)-\mathcal{R}_{\ell_{0-1}}^{*}(\mathcal{H})\leq \widetilde{\Gamma}_{\tau}\big{(}\mathcal{R}_{\ell_{\tau}^{\mathrm{comp}}}(h)- \mathcal{R}_{\ell_{\tau}^{\mathrm{comp}}}^{*}(\mathcal{H})+\mathcal{M}_{\ell_{ \tau}^{\mathrm{comp}}}(\mathcal{H})\big{)}-\mathcal{M}_{\ell_{0-1}}(\mathcal{H }),\]

where \(\widetilde{\Gamma}_{\tau}(t)=\sqrt{2n^{\tau-1}t}\). Thus, while all these loss functions show square-root growth, the number of classes acts as a critical scaling factor.

### Comparison across comp-sum losses

In Appendix H, we compare minimizability gaps cross comp-sum losses. We will see that minimizability gaps decrease as \(\tau\) increases. This might suggest favoring \(\tau\) close to \(2\). But when accounting for \(n\), \(\ell_{\tau}^{\mathrm{comp}}\) with \(\tau=1\) (logistic loss) is optimal since \(n\) then vanishes. Thus, both class count and minimizability gaps are essential in loss selection. In Appendix J, we will show that the minimizability gaps can become zero or relatively small under certain conditions in multi-class classification. In such scenarios, the logistic loss is favored, which can partly explain its widespread practical application.

### Small surrogate minimizability gaps

While minimizability gaps vanish in special scenarios (e.g., unrestricted hypothesis sets, best-in-class error matching Bayes error), we now seek broader conditions for zero or small surrogate minimizability gaps to make our bounds more meaningful.

Due to space constraints, we focus on binary classification here, with multi-class results given in Appendix J. We address pointwise surrogate losses which take the form \(\ell(h(x),y)\) for a labeled point(\(x,y\)). We write \(A=\{h(x)\colon h\in\mathcal{H}\}\) to denote the set of predictor values at \(x\), which we assume to be independent of \(x\). All proofs for this section are presented in Appendix I.

**Deterministic scenario**. We first consider the deterministic scenario, where the conditional probability \(\mathsf{p}(y\!\mid\!x)\) is either zero or one. For a deterministic distribution, we denote by \(\mathcal{X}_{+}\) the subset of \(\mathcal{X}\) over which the label is \(+1\) and by \(\mathcal{X}_{-}\) the subset of \(\mathcal{X}\) over which the label is \(-1\). For convenience, let \(\ell_{+}=\inf_{\alpha\in\mathcal{A}}\ell(\alpha,+1)\) and \(\ell_{-}=\inf_{\alpha\in\mathcal{A}}\ell(\alpha,-1)\).

**Theorem 6.1**.: _Assume that \(\mathcal{D}\) is deterministic and that the best-in-class error is achieved by some \(h^{*}\in\mathcal{H}\). Then, the minimizability gap is null, \(\mathcal{M}(\mathcal{H})=0\), iff_

\[\ell(h^{*}(x),+1)=\ell_{+}\text{ a.s. over }\mathcal{X}_{+},\quad\ell(h^{*}(x ),-1)=\ell_{-}\text{ a.s. over }\mathcal{X}_{-}.\]

_If further \(\alpha\mapsto\ell(\alpha,+1)\) and \(\alpha\mapsto\ell(\alpha,-1)\) are injective and \(\ell_{+}=\ell(\alpha_{+},+1)\), \(\ell_{-}=\ell(\alpha_{-},-1)\), then, the condition is equivalent to \(h^{*}(x)=\alpha_{+}1_{x\in\mathcal{X}_{+}}+\alpha_{-}1_{x\in\mathcal{X}_{-}}\). Furthermore, the minimizability gap is bounded by \(\epsilon\) iff \(p(\mathbb{E}[\ell(h^{*}(x),+1)\mid y=+1]-\ell_{+})+(1-p)(\mathbb{E}[\ell(h^{* }(x),-1)\mid y=-1]-\ell_{-})\leq\epsilon\). In particular, the condition implies:_

\[\mathbb{E}[\ell(h^{*}(x),+1)\mid y=+1]-\ell_{+}\leq\frac{\epsilon}{p}\quad \text{and}\quad\mathbb{E}[\ell(h^{*}(x),-1)\mid y=-1]-\ell_{-}\leq\frac{ \epsilon}{1-p}.\]

The theorem suggests that, under those assumptions, for the surrogate minimizability gap to be zero, the best-in-class hypothesis must be piecewise constant with specific values on \(\mathcal{X}_{+}\) and \(\mathcal{X}_{-}\). The existence of such a hypothesis in \(\mathcal{H}\) depends both on the complexity of the decision surface separating \(\mathcal{X}_{+}\) and \(\mathcal{X}_{-}\) and on that of the hypothesis set \(\mathcal{H}\). More generally, when the best-in-class classifier \(\epsilon\)-approximates \(\alpha_{+}\) over \(\mathcal{X}_{+}\) and \(\alpha_{-}\) over \(\mathcal{X}_{-}\), then the minimizability gap is bounded by \(\epsilon\). As an example, when the decision surface is a hyperplane, a hypothesis set of linear functions combined with a sigmoid activation function can provide such a good approximation (see Figure 1 for an illustration in a simple case).

**Stochastic scenario**. Here, we present a general result that is a direct extension of that of the deterministic scenario. We show that the minimizability gap is zero when there exists \(h^{*}\in\mathcal{H}\) that matches \(\alpha^{*}(x)\) for all \(x\), where \(\alpha^{*}(x)\) is the minimizer of the conditional error. We also show that the minimizability gap is bounded by \(\epsilon\) when there exists \(h^{*}\in\mathcal{H}\) whose conditional error \(\epsilon\)-approximates best-in-class conditional error for all \(x\).

**Theorem 6.2**.: _The best-in-class error is achieved by some \(h^{*}\in\mathcal{H}\) and the minimizability gap is null, \(\mathcal{M}(\mathcal{H})=0\), iff there exists \(h^{*}\in\mathcal{H}\) such that for all \(x\),_

\[\operatorname*{\mathbb{E}}_{y}[\ell(h^{*}(x),y)\mid x]=\inf_{\alpha\in A} \operatorname*{\mathbb{E}}_{y}[\ell(\alpha,y)\mid x]\text{ a.s. over }\mathcal{X}.\] (3)

_If further \(\alpha\mapsto\operatorname*{\mathbb{E}}_{y}[\ell(\alpha,y)\mid x]\) is injective and \(\inf_{\alpha\in\mathcal{A}}\operatorname*{\mathbb{E}}_{y}[\ell(\alpha,y)\mid x ]=\operatorname*{\mathbb{E}}_{y}[\ell(\alpha^{*}(x),y)\mid x]\), then, the condition is equivalent to \(h^{*}(x)=\alpha^{*}(x)\) a.s. for \(x\in\mathcal{X}\). Furthermore, the minimizability gap is bounded by \(\epsilon\), \(\mathcal{M}(\mathcal{H})\leq\epsilon\), iff there exists \(h^{*}\in\mathcal{H}\) such that_

\[\operatorname*{\mathbb{E}}_{x}\!\!\left[\operatorname*{\mathbb{E}}_{y}[\ell(h^ {*}(x),y)\mid x]-\inf_{\alpha\in A}\operatorname*{\mathbb{E}}_{y}[\ell(\alpha, y)\mid x]\right]\leq\epsilon.\] (4)

In deterministic settings, condition (4) coincides with that of Theorem 6.1. However, in stochastic scenarios, the existence of such a hypothesis depends on both decision surface complexity and the conditional distribution's properties. For illustration, see Appendix J.3 where we analyze the exponential, logistic (binary), and multi-class logistic losses.

We thoroughly analyzed minimizability gaps, comparing them across comp-sum losses, and identifying conditions for zero or small gaps, which help inform surrogate loss selection. In Appendix F, we show the crucial role of minimizability gaps in comparing excess bounds with \(\mathcal{H}\)-consistency bounds. Importantly, combining \(\mathcal{H}\)-consistency bounds with surrogate loss Rademacher complexity bounds yields zero-one loss (estimation) learning bounds for surrogate loss minimizers (see Appendix O).

## 7 Conclusion

We established a universal square-root growth rate for the widely-used class of smooth surrogate losses in both binary and multi-class classification. This underscores the minimizability gap as a crucial discriminator among surrogate losses. Our detailed analysis of these gaps can provide guidance for loss selection.

Figure 1: Approximation provided by sigmoid activation function.

## References

* Agarwal (2014) S. Agarwal. Surrogate regret bounds for bipartite ranking via strongly proper losses. _The Journal of Machine Learning Research_, 15(1):1653-1674, 2014.
* Awasthi et al. (2021a) P. Awasthi, N. Frank, A. Mao, M. Mohri, and Y. Zhong. Calibration and consistency of adversarial surrogate losses. _Advances in Neural Information Processing Systems_, pages 9804-9815, 2021a.
* Awasthi et al. (2021b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. A finer calibration analysis for adversarial robustness. _arXiv preprint arXiv:2105.01550_, 2021b.
* Awasthi et al. (2022a) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Multi-class \(H\)-consistency bounds. In _Advances in neural information processing systems_, pages 782-795, 2022a.
* Awasthi et al. (2022b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. \(H\)-consistency bounds for surrogate loss minimizers. In _International Conference on Machine Learning_, pages 1117-1174, 2022b.
* Awasthi et al. (2023a) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. Theoretically grounded loss functions and algorithms for adversarial robustness. In _International Conference on Artificial Intelligence and Statistics_, pages 10077-10094, 2023a.
* Awasthi et al. (2023b) P. Awasthi, A. Mao, M. Mohri, and Y. Zhong. DC-programming for neural network optimizations. _Journal of Global Optimization_, 2023b.
* Bao (2023) H. Bao. Proper losses, moduli of convexity, and surrogate regret bounds. In _Conference on Learning Theory_, pages 525-547, 2023.
* Bartlett and Wegkamp (2008) P. L. Bartlett and M. H. Wegkamp. Classification with a reject option using a hinge loss. _Journal of Machine Learning Research_, 9(8), 2008.
* Bartlett et al. (2006) P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classification, and risk bounds. _Journal of the American Statistical Association_, 101(473):138-156, 2006.
* Ben-David et al. (2012) S. Ben-David, D. Loker, N. Srebro, and K. Sridharan. Minimizing the misclassification error rate using a surrogate convex loss. In _International Coference on International Conference on Machine Learning_, pages 83-90, 2012.
* Berkson (1944) J. Berkson. Application of the logistic function to bio-assay. _Journal of the American Statistical Association_, 39:357---365, 1944.
* Berkson (1951) J. Berkson. Why I prefer logits to probits. _Biometrics_, 7(4):327---339, 1951.
* Blondel (2019) M. Blondel. Structured prediction with projection oracles. In _Advances in neural information processing systems_, 2019.
* Cao et al. (2022) Y. Cao, T. Cai, L. Feng, L. Gu, J. Gu, B. An, G. Niu, and M. Sugiyama. Generalizing consistent multi-class classification with rejection to be compatible with arbitrary losses. In _Advances in neural information processing systems_, 2022.
* Charoenphakdee et al. (2021) N. Charoenphakdee, Z. Cui, Y. Zhang, and M. Sugiyama. Classification with rejection based on cost-sensitive classification. In _International Conference on Machine Learning_, pages 1507-1517, 2021.
* Chen et al. (2004) D.-R. Chen, Q. Wu, Y. Ying, and D.-X. Zhou. Support vector machine soft margin classifiers: error analysis. _The Journal of Machine Learning Research_, 5:1143-1175, 2004.
* Ciliberto et al. (2016) C. Ciliberto, L. Rosasco, and A. Rudi. A consistent regularization approach for structured prediction. In _Advances in neural information processing systems_, 2016.
* Ciliberto et al. (2020) C. Ciliberto, L. Rosasco, and A. Rudi. A general framework for consistent structured prediction with implicit loss embeddings. _The Journal of Machine Learning Research_, 21(1):3852-3918, 2020.
* Cortes et al. (2016a) C. Cortes, G. DeSalvo, and M. Mohri. Learning with rejection. In _International Conference on Algorithmic Learning Theory_, pages 67-82, 2016a.
* Ciliberto et al. (2016b)C. Cortes, G. DeSalvo, and M. Mohri. Boosting with abstention. In _Advances in Neural Information Processing Systems_, pages 1660-1668, 2016b.
* Cortes et al. [2023] C. Cortes, G. DeSalvo, and M. Mohri. Theory and algorithms for learning with rejection in binary classification. _Annals of Mathematics and Artificial Intelligence_, 2023.
* Cortes et al. [2024] C. Cortes, A. Mao, C. Mohri, M. Mohri, and Y. Zhong. Cardinality-aware set prediction and top-\(k\) classification. In _Advances in neural information processing systems_, 2024.
* Crammer and Singer [2001] K. Crammer and Y. Singer. On the algorithmic implementation of multiclass kernel-based vector machines. _Journal of machine learning research_, 2(Dec):265-292, 2001.
* Dontchev and Rockafellar [2009] A. L. Dontchev and R. T. Rockafellar. Robinson's implicit function theorem and its extensions. _Math. Program._, 117(1-2):129-147, 2009.
* Duchi et al. [2018] J. Duchi, K. Khosravi, and F. Ruan. Multiclass classification, information, divergence and surrogate risk. _The Annals of Statistics_, 46(6B):3246-3275, 2018.
* Finocchiaro et al. [2019] J. Finocchiaro, R. Frongillo, and B. Waggoner. An embedding framework for consistent polyhedral surrogates. In _Advances in neural information processing systems_, 2019.
* Frongillo and Waggoner [2021] R. Frongillo and B. Waggoner. Surrogate regret bounds for polyhedral losses. In _Advances in Neural Information Processing Systems_, volume 34, pages 21569-21580, 2021.
* Gao and Zhou [2015] W. Gao and Z.-H. Zhou. On the consistency of AUC pairwise optimization. In _International Joint Conference on Artificial Intelligence_, 2015.
* Kotlowski et al. [2011] W. Kotlowski, K. J. Dembczynski, and E. Huellermeier. Bipartite ranking through minimization of univariate loss. In _International Conference on Machine Learning_, pages 1113-1120, 2011.
* Kuratowski and Ryll-Nardzewski [1965] K. Kuratowski and C. Ryll-Nardzewski. A general theorem on selectors. _Bull. Acad. Pol. Sci., Ser. Sci. Math. Astron. Phys._, 13(8):397-403, 1965.
* Kuznetsov et al. [2014] V. Kuznetsov, M. Mohri, and U. Syed. Multi-class deep boosting. In _Advances in Neural Information Processing Systems_, pages 2501-2509, 2014.
* Lapin et al. [2016] M. Lapin, M. Hein, and B. Schiele. Loss functions for top-k error: Analysis and insights. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1468-1477, 2016.
* Lee et al. [2004] Y. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines: Theory and application to the classification of microarray data and satellite radiance data. _Journal of the American Statistical Association_, 99(465):67-81, 2004.
* Long and Servedio [2013] P. Long and R. Servedio. Consistency versus realizable H-consistency for multiclass classification. In _International Conference on Machine Learning_, pages 801-809, 2013.
* Mahdavi et al. [2014] M. Mahdavi, L. Zhang, and R. Jin. Binary excess risk for smooth convex surrogates. _arXiv preprint arXiv:1402.1792_, 2014.
* Mao et al. [2023a] A. Mao, C. Mohri, M. Mohri, and Y. Zhong. Two-stage learning to defer with multiple experts. In _Advances in neural information processing systems_, 2023a.
* Mao et al. [2023b] A. Mao, M. Mohri, and Y. Zhong. H-consistency bounds: Characterization and extensions. In _Advances in Neural Information Processing Systems_, 2023b.
* Mao et al. [2023c] A. Mao, M. Mohri, and Y. Zhong. H-consistency bounds for pairwise misranking loss surrogates. In _International conference on Machine learning_, 2023c.
* Mao et al. [2023d] A. Mao, M. Mohri, and Y. Zhong. Ranking with abstention. In _ICML 2023 Workshop The Many Facets of Preference-Based Learning_, 2023d.
* Mao et al. [2023e] A. Mao, M. Mohri, and Y. Zhong. Structured prediction with stronger consistency guarantees. In _Advances in Neural Information Processing Systems_, 2023e.

A. Mao, M. Mohri, and Y. Zhong. Cross-entropy loss functions: Theoretical analysis and applications. In _International Conference on Machine Learning_, 2023f.
* Mao et al. (2024a) A. Mao, M. Mohri, and Y. Zhong. Principled approaches for learning to defer with multiple experts. In _International Symposium on Artificial Intelligence and Mathematics_, 2024a.
* Mao et al. (2024b) A. Mao, M. Mohri, and Y. Zhong. Predictor-rejector multi-class abstention: Theoretical analysis and algorithms. In _International Conference on Algorithmic Learning Theory_, 2024b.
* Mao et al. (2024c) A. Mao, M. Mohri, and Y. Zhong. Theoretically grounded loss functions and algorithms for score-based multi-class abstention. In _International Conference on Artificial Intelligence and Statistics_, 2024c.
* Mao et al. (2024d) A. Mao, M. Mohri, and Y. Zhong. Enhanced \(H\)-consistency bounds. _arXiv preprint arXiv:2407.13722_, 2024d.
* Mao et al. (2024e) A. Mao, M. Mohri, and Y. Zhong. \(H\)-consistency guarantees for regression. In _International Conference on Machine Learning_, pages 34712-34737, 2024e.
* Mao et al. (2024f) A. Mao, M. Mohri, and Y. Zhong. Multi-label learning with stronger consistency guarantees. In _Advances in neural information processing systems_, 2024f.
* Mao et al. (2024g) A. Mao, M. Mohri, and Y. Zhong. Realizable \(H\)-consistent and Bayes-consistent loss functions for learning to defer. In _Advances in neural information processing systems_, 2024g.
* Mao et al. (2024h) A. Mao, M. Mohri, and Y. Zhong. Regression with multi-expert deferral. In _International Conference on Machine Learning_, pages 34738-34759, 2024h.
* Menon and Williamson (2014) A. K. Menon and R. C. Williamson. Bayes-optimal scorers for bipartite ranking. In _Conference on Learning Theory_, pages 68-106, 2014.
* Mohri et al. (2024) C. Mohri, D. Andor, E. Choi, M. Collins, A. Mao, and Y. Zhong. Learning to reject with a fixed predictor: Application to decontextualization. In _International Conference on Learning Representations_, 2024.
* Mohri et al. (2018) M. Mohri, A. Rostamizadeh, and A. Talwalkar. _Foundations of Machine Learning_. MIT Press, second edition, 2018.
* Mozannar and Sontag (2020) H. Mozannar and D. Sontag. Consistent estimators for learning to defer to an expert. In _International Conference on Machine Learning_, pages 7076-7087, 2020.
* Mozannar et al. (2023) H. Mozannar, H. Lang, D. Wei, P. Sattigeri, S. Das, and D. Sontag. Who should predict? exact algorithms for learning to defer to humans. In _International Conference on Artificial Intelligence and Statistics_, pages 10520-10545, 2023.
* Ni et al. (2019) C. Ni, N. Charoenphakdee, J. Honda, and M. Sugiyama. On the calibration of multiclass classification with rejection. In _Advances in Neural Information Processing Systems_, pages 2582-2592, 2019.
* Nowak et al. (2019) A. Nowak, F. Bach, and A. Rudi. Sharp analysis of learning with discrete losses. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1920-1929, 2019.
* Nowak et al. (2020) A. Nowak, F. Bach, and A. Rudi. Consistent structured prediction with max-min margin markov networks. In _International Conference on Machine Learning_, pages 7381-7391, 2020.
* Nowak et al. (2022) A. Nowak, A. Rudi, and F. Bach. On the consistency of max-margin losses. In _International Conference on Artificial Intelligence and Statistics_, pages 4612-4633, 2022.
* Nowak-Vila et al. (2019) A. Nowak-Vila, F. Bach, and A. Rudi. A general theory for structured prediction with smooth convex surrogates. _arXiv preprint arXiv:1902.01958_, 2019.
* Osokin et al. (2017) A. Osokin, F. Bach, and S. Lacoste-Julien. On structured prediction theory with calibrated convex surrogate losses. In _Advances in Neural Information Processing Systems_, 2017.
* Pires et al. (2013) B. A. Pires, C. Szepesvari, and M. Ghavamzadeh. Cost-sensitive multiclass classification risk bounds. In _International Conference on Machine Learning_, pages 1391-1399, 2013.
* Pires et al. (2019)H. G. Ramaswamy, A. Tewari, and S. Agarwal. Consistent algorithms for multiclass classification with an abstain option. _Electronic Journal of Statistics_, 12(1):530-554, 2018.
* Reid and Williamson (2009) M. D. Reid and R. C. Williamson. Surrogate regret bounds for proper losses. In _International Conference on Machine Learning_, pages 897-904, 2009.
* Steinwart (2007) I. Steinwart. How to compare different loss functions and their risks. _Constructive Approximation_, 26(2):225-287, 2007.
* Tewari and Bartlett (2007) A. Tewari and P. L. Bartlett. On the consistency of multiclass classification methods. _Journal of Machine Learning Research_, 8(36):1007-1025, 2007.
* Uematsu and Lee (2017) K. Uematsu and Y. Lee. On theoretically optimal ranking functions in bipartite ranking. _Journal of the American Statistical Association_, 112(519):1311-1322, 2017.
* Verhulst (1838) P. F. Verhulst. Notice sur la loi que la population suit dans son accroissement. _Correspondance mathematique et physique_, 10:113---121, 1838.
* Verhulst (1845) P. F. Verhulst. Recherches mathematiques sur la loi d'accroissement de la population. _Nouveaux Memoires de l'Academie Royale des Sciences et Belles-Lettres de Bruxelles_, 18:1---42, 1845.
* Verma and Nalisnick (2022) R. Verma and E. Nalisnick. Calibrated learning to defer with one-vs-all classifiers. In _International Conference on Machine Learning_, pages 22184-22202, 2022.
* Verma et al. (2023) R. Verma, D. Barrejon, and E. Nalisnick. Learning to defer to multiple experts: Consistent surrogate losses, confidence calibration, and conformal ensembles. In _International Conference on Artificial Intelligence and Statistics_, pages 11415-11434, 2023.
* Weston and Watkins (1999) J. Weston and C. Watkins. Support vector machines for multi-class pattern recognition. _European Symposium on Artificial Neural Networks_, 4(6), 1999.
* Yang and Koyejo (2020) F. Yang and S. Koyejo. On the consistency of top-k surrogate losses. In _International Conference on Machine Learning_, pages 10727-10735, 2020.
* Yu and Blaschko (2018) J. Yu and M. B. Blaschko. The lovasz hinge: A novel convex surrogate for submodular losses. _IEEE transactions on pattern analysis and machine intelligence_, 42(3):735-748, 2018.
* Yuan and Wegkamp (2010) M. Yuan and M. Wegkamp. Classification methods with reject option based on convex risk minimization. _Journal of Machine Learning Research_, 11(1), 2010.
* Zhang et al. (2021) J. Zhang, T. Liu, and D. Tao. On the rates of convergence from surrogate risk minimizers to the Bayes optimal classifier. _IEEE Transactions on Neural Networks and Learning Systems_, 33(10):5766-5774, 2021.
* Zhang and Agarwal (2020) M. Zhang and S. Agarwal. Bayes consistency vs. H-consistency: The interplay between surrogate loss functions and the scoring function class. In _Advances in Neural Information Processing Systems_, pages 16927-16936, 2020.
* Zhang (2004a) T. Zhang. Statistical behavior and consistency of classification methods based on convex risk minimization. _The Annals of Statistics_, 32(1):56-85, 2004a.
* Zhang (2004b) T. Zhang. Statistical analysis of some multi-category large margin classification methods. _Journal of Machine Learning Research_, 5(Oct):1225-1251, 2004b.
* Zhang and Sabuncu (2018) Z. Zhang and M. Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. In _Advances in neural information processing systems_, 2018.

###### Contents of Appendix

* A Related work
* Proof of Lemma 2.1
* C General form of \(\mathcal{H}\)-consistency bounds
* D Properties of minimizability gaps
* E Examples of \(\mathcal{H}\)-consistency bounds
* F Comparison with excess error bounds
* G Polyhedral losses versus smooth losses
* H Comparison of minimizability gaps across comp-sum losses
* I Small surrogate minimizability gaps: proof for binary classification
* J Small surrogate minimizability gaps: multi-class classification
* J.1 Deterministic scenario
* J.2 Stochastic scenario
* J.3 Examples
* K Proof for binary margin-based losses (Theorem 4.2)
* L Proof for comp-sum losses (Theorem 5.3)
* M Proof for constrained losses (Theorem 5.5)
* N Analysis of the function of \(\tau\)
* O Generalization bounds
* P Future workRelated work

The Bayes-consistency of surrogate losses has been extensively studied in the context of binary classification. Zhang (2004); Bartlett et al. (2006) and Steinwart (2007) established Bayes-consistency for various convex loss functions, including margin-based surrogates. They also introduced excess error bounds (or surrogate regret bounds) for margin-based surrogates. Other works include Chen et al. (2004), which specifically studied the SVM \(q\)-norm soft margin loss and established a square-root excess error bound with an optimal constant for that specific family, and Reid and Williamson (2009), which established tight excess error bounds for proper losses in binary classification.

The Bayes-consistency of several surrogate loss function families in the context of multi-class classification has also been studied by Zhang (2004) and Tewari and Bartlett (2007). Zhang (2004) established a series of results for various multi-class classification formulations, including negative results for multi-class hinge loss functions (Crammer and Singer, 2001), as well as positive results for the sum exponential loss (Weston and Watkins, 1999; Awasthi, Mao, Mohri, and Zhong, 2022), the (multinomial) logistic loss (Verhulst, 1838, 1845; Berkson, 1944, 1951), and the constrained losses (Lee et al., 2004). Later, Tewari and Bartlett (2007) adopted a different geometric method to analyze Bayes-consistency, yielding similar results for these loss function families. Steinwart (2007) developed general tools to characterize Bayes consistency for both binary and multi-class classification. Additionally, excess error bounds have been derived by Pires et al. (2013) for a family of constrained losses and by Duchi et al. (2018) for loss functions related to generalized entropies.

For a surrogate loss \(\ell\), an excess error bound holds for any predictor \(h\) and has the form \(\mathcal{E}_{\ell_{0-1}}(h)\) - \(\mathcal{E}^{*}_{\ell_{0-1}}\xi\Psi(\mathcal{E}_{\ell}(h)-\mathcal{E}^{*}_{\ell})\), where \(\mathcal{E}_{\ell_{0-1}}(h)\) and \(\mathcal{E}_{\ell}(h)\) represent the expected losses of \(h\) for the zero-one loss and surrogate loss respectively, \(\mathcal{E}^{*}_{\ell_{0-1}}\) and \(\mathcal{E}^{*}_{\ell}\) the Bayes errors for the zero-one and surrogate loss respectively, and \(\Psi\) a non-decreasing function.

The _growth rate_ of excess error bounds, that is the behavior of function \(\Psi\) near zero, has gained attention in recent research (Mahdavi et al., 2014; Zhang et al., 2021; Frongillo and Waggoner, 2021; Bao, 2023). Mahdavi et al. (2014) examined the growth rate for _smoothed hinge losses_ in binary classification, demonstrating that smoother losses result in worse growth rates. The optimal rate is achieved with the standard hinge loss, which exhibits linear growth. Zhang et al. (2021) tied the growth rate of excess error bounds in binary classification to two properties of the surrogate loss function: consistency intensity and conductivity. These metrics enable comparisons of growth rates across different surrogates. But, can we establish lower and upper bounds for the growth rate of excess error bounds under specific regularity conditions?

Frongillo and Waggoner (2021) pioneered research on this question in binary classification settings. They established a critical square-root lower bound for excess error bounds when a surrogate loss is locally strongly convex and has a locally Lipschitz gradient. Additionally, they demonstrated a linear excess error bound for Bayes-consistent polyhedral loss functions (convex and piecewise-linear) (Finocchiaro et al., 2019) (see also (Lapin et al., 2016; Ramaswamy et al., 2018; Yu and Blaschko, 2018; Yang and Koyejo, 2020)). More recently, Bao (2023) complemented these results by showing that proper losses associated with Shannon entropy, exponential entropy, spherical entropy, squared \(\alpha\)-norm entropies and \(\alpha\)-polynomial entropies, with \(\alpha>1\), also exhibit a square-root lower bound for excess error bounds relative to the \(\ell_{1}\)-distance.

However, while Bayes-consistency and excess error bounds are valuable, they are not sufficiently informative, as they are established for the family of all measurable functions and disregard the crucial role played by restricted hypothesis sets in learning. As pointed out by Long and Servedio (2013), in some cases, minimizing Bayes-consistent losses can result in constant expected error, while minimizing inconsistent losses can yield an expected loss approaching zero. To address this limitation, the authors introduced the concept of _realizable \(\mathcal{H}\)-consistency_, further explored by Kuznetsov et al. (2014) and Zhang and Agarwal (2020). Nonetheless, these guarantees are only asymptotic and rely on a strong realizability assumption that typically does not hold in practice.

Recent research by Awasthi, Mao, Mohri, and Zhong (2022, 2022) and Mao, Mohri, and Zhong (2023, 2023) has instead introduced and analyzed \(\mathcal{H}\)-_consistency bounds_. These bounds are more informative than Bayes-consistency since they are hypothesis set-specific and non-asymptotic. Their work covers broad families of surrogate losses in binary classification, multi-class classification, structured prediction, and abstention (Mao, Mohri, Mohri, and Zhong, 2023). Crucially, they provide upper bounds on the _estimation error_ of the target loss, for example, the zero-one loss in classification,that hold for any predictor \(h\) within a hypothesis set \(\mathcal{H}\). These bounds relate this estimation error to the surrogate loss estimation error. Their general form is: \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})\leq f( \mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}(\mathcal{H}))\), where \(\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})\) and \(\mathcal{E}_{\ell}^{*}(\mathcal{H})\) represent the best-in-class expected losses for the zero-one and surrogate loss respectively, and \(f\) is a non-decreasing function continuous at zero. \(\mathcal{H}\)-consistency bounds imply in particular excess error bounds, when the hypothesis set is taken to be the family of all measurable functions.

The authors have further analyzed \(\mathcal{H}\)-consistency bounds in structured prediction, ranking, and abstention.

Mao, Mohri, and Zhong (2023) revealed limitations of existing structured prediction loss functions, demonstrating they lack Bayes-consistency. They introduced new surrogate loss families proven to benefit form \(\mathcal{H}\)-consistency bounds, thus also establishing Bayes-consistency. This complements earlier negative finding about the Bayes-consistency of Struct-SVM and positive results for quadratic surrogate (QS) losses or some non-smooth polyhedral-type loss functions (Osokin et al., 2017, Ciliberto et al., 2016, Blondel, 2019, Nowak-Vila et al., 2019, Nowak et al., 2019, 2020, Ciliberto et al., 2020, Nowak et al., 2022).

Mao et al. (2023) showed that there are no meaningful \(\mathcal{H}\)-consistency bounds for general pairwise ranking and bipartite ranking surrogate losses with equicontinuous hypothesis sets, including linear models and neural networks. They proposed ranking with abstention as a solution. These results demonstrated that although these surrogate loss functions have been shown to be Bayes-consistent in various studies (Kotlowski et al., 2011, Menon and Williamson, 2014, Agarwal, 2014, Gao and Zhou, 2015, Uematsu and Lee, 2017), they are, in fact, not \(\mathcal{H}\)-consistent.

Mao et al. (2023) applied \(\mathcal{H}\)-consistency bounds to two-stage learning to defer scenarios, designing new surrogate losses. This complemented the Bayes-consistent surrogate losses in the single-stage scenario of learning to defer (Mozannar and Sontag, 2020, Verma and Nalisnick, 2022, Verma et al., 2023, Mozannar et al., 2023) or learning with abstention (Bartlett and Wegkamp, 2008, Yuan and Wegkamp, 2010, Cortes et al., 2016, 2016, Ramaswamy et al., 2018, Ni et al., 2019, Charoenphakdee et al., 2021, Cao et al., 2022, Cortes et al., 2023).

Mao, Mohri, and Zhong (2024) recently established enhanced \(\mathcal{H}\)-consistency bounds based on more general inequalities relating conditional regrets. They derived more favorable distribution- and predictor-dependent bounds in various scenarios including standard multi-class classification, binary and multi-class classification under Tsybakov noise conditions, and bipartite ranking. \(\mathcal{H}\)-consistency bounds have also been studied in the scenario of adversarial robustness (Awasthi et al., 2021, 2023, 2023, 2023), bounded regression (Mao et al., 2024), regression with multi-expert deferral (Mao et al., 2024), top-\(k\) classification (Cortes et al., 2024), multi-label learning (Mao et al., 2024), score-based abstention (Mao et al., 2024), predictor-rejector abstention (Mao et al., 2024), learning to abstain with a fixed predictor with application in decontextualization (Mohri et al., 2024), ranking with abstention (Mao et al., 2023), realizable learning to defer (Mao et al., 2024), and learning to defer with multiple experts (Mao et al., 2024).

This papers presents a characterization of the growth rate of \(\mathcal{H}\)-consistency bounds, that is how quickly the functions \(f\) increase near zero, both in binary and multi-class classification.

## Appendix B Pointwise loss functions - Proof of Lemma 2.1

**Lemma 2.1**.: _Let \(\ell\) be a pointwise loss function. Then, the Bayes error and the approximation error can be expressed as follows: \(\mathcal{E}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}})=\mathbb{E}_{x}[\mathcal{C}_ {\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)]\) and \(\mathcal{A}_{\ell}(\mathcal{H})=\mathcal{E}_{\ell}^{*}(\mathcal{H})-\mathbb{E }_{x}[\mathcal{C}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)]\)._

Proof.: By definition, for a pointwise loss function \(\ell\), there exists a measurable function \(\hat{\ell}\colon\mathbb{R}^{n}\times\mathcal{Y}\to\mathbb{R}_{+}\) such that \(\ell(h,x,y)=\hat{\ell}(h(x),y)\), where \(h(x)=[h(x,1),\ldots,h(x,n)]\) is the score vector of the predictor \(h\). Thus, the following inequality holds:

\[\mathcal{C}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)=\inf_{h\in\mathcal{H} \mathcal{H}}\mathbb{E}_{y}[\ell(h,x,y)\mid x]=\inf_{\alpha\in\mathbb{R}^{n}} \mathbb{E}_{y}\big{[}\hat{\ell}(\alpha,y)\mid x\big{]}.\]

Since \(\hat{\ell}\colon(\alpha,y)\mapsto\mathbb{R}_{+}\) is measurable, the function \((\alpha,x)\mapsto\mathbb{E}_{y}\big{[}\hat{\ell}(\alpha,y)\mid x\big{]}\) is also measurable. We now show that the function is also measurable.

To do this, we consider for any \(\beta>0\), the set \(\left\{x\colon\inf_{\alpha\in\mathbb{R}^{n}}\mathbb{E}_{y}\big{[}\hat{\ell}(\alpha, y)\mid x\big{]}<\beta\right\}\) which can be expressed as

\[\left\{x\colon\inf_{\alpha\in\mathbb{R}^{n}}\mathbb{E}_{y}\big{[} \hat{\ell}(\alpha,y)\mid x\big{]}<\beta\right\} =\left\{x\colon\exists\alpha\in\mathbb{R}^{n}\text{ such that }\mathbb{E}_{y}\big{[}\hat{\ell}(\alpha,y)\mid x\big{]}< \beta\right\}\] \[=\Pi_{\chi}\Big{\{}(\alpha,x)\colon\mathbb{E}_{y}\big{[}\hat{\ell }(\alpha,y)\mid x\big{]}<\beta\Big{\}},\]

where \(\Pi_{\chi}\) is the projection onto \(\mathcal{X}\). By the measurable projection theorem, \(x\mapsto\inf_{\alpha\in\mathbb{R}^{n}}\mathbb{E}_{y}\big{[}\hat{\ell}(\alpha, y)\mid x\big{]}\) is measurable. Then, since a pointwise difference of measurable functions is measurable, for all \(n\in\mathbb{N}\), the set \(\big{\{}(\alpha,x)\colon\mathbb{E}_{y}\big{[}\hat{\ell}(\alpha,y)\mid x\big{]} <\inf_{\alpha\in\mathbb{R}^{n}}\mathbb{E}_{y}\big{[}\hat{\ell}(\alpha,y)\mid x \big{]}+\frac{1}{n}\big{\}}\) is measurable. Thus, by Kuratowski and Ryll-Nardzewski (1965)'s measurable selection theorem, for all \(n\in\mathbb{N}\), there exists a measurable function \(h_{n}\colon x\mapsto\alpha\in\mathbb{R}^{n}\) such that the following holds:

\[\mathcal{C}_{\ell}(h_{n},x)=\mathbb{E}_{y}\big{[}\hat{\ell}(\alpha,y)\mid x \big{]}<\inf_{\alpha\in\mathbb{R}^{n}}\mathbb{E}_{y}\big{[}\hat{\ell}(\alpha, y)\mid x\big{]}+\frac{1}{n}=\mathcal{C}_{\ell}^{*}\big{(}\mathcal{H}_{\mathrm{all}},x \big{)}+\frac{1}{n}.\]

Therefore, we have

\[\mathcal{E}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}})\leq\mathbb{E}_{x}\big{[} \mathcal{C}_{\ell}(h_{n},x)\big{]}\leq\mathbb{E}_{x}\big{[}\mathcal{C}_{\ell }^{*}(\mathcal{H}_{\mathrm{all}},x)\big{]}+\frac{1}{n}\leq\mathcal{E}_{\ell}^ {*}(\mathcal{H}_{\mathrm{all}})+\frac{1}{n}.\]

By taking the limit \(n\to+\infty\), we obtain \(\mathcal{E}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}})=\mathbb{E}_{x}\big{[} \mathcal{C}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)\big{]}\). By definition, \(\mathcal{A}_{\ell}(\mathcal{H})=\mathcal{E}_{\ell}^{*}(\mathcal{H})-\mathcal{E }_{\ell}^{*}(\mathcal{H}_{\mathrm{all}})=\mathcal{E}_{\ell}^{*}(\mathcal{H})- \mathbb{E}_{x}\big{[}\mathcal{C}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x) \big{]}\). This completes the proof. 

## Appendix C General form of \(\mathcal{H}\)-consistency bounds

Fix a target loss function \(\ell_{2}\) and a surrogate loss \(\ell_{1}\). Given a hypothesis set \(\mathcal{H}\), a bound relating the estimation errors of these loss functions admits the following form:

\[\forall h\in\mathcal{H},\quad\mathcal{E}_{\ell_{2}}(h)-\mathcal{E}_{\ell_{2}}^ {*}(\mathcal{H})\leq\Gamma_{\mathcal{D}}\big{(}\mathcal{E}_{\ell_{1}}(h)- \mathcal{E}_{\ell_{1}}^{*}(\mathcal{H})\big{)},\] (5)

where, for any distribution \(\mathcal{D}\), \(\Gamma_{\mathcal{D}}\colon\mathbb{R}_{+}\to\mathbb{R}_{+}\) is a non-decreasing function on \(\mathbb{R}_{+}\). We will assume that \(\Gamma_{\mathcal{D}}\) is concave. In particular, the bound should hold for any point mass distribution \(\delta_{x}\), \(x\in\mathcal{X}\). We will operate under the assumption that the same bound holds uniformly over \(\mathcal{X}\) and thus that there exists a fixed concave function \(\Gamma\) such that \(\Gamma_{\delta_{x}}=\Gamma\) for all \(x\).

Observe that for any point mass distribution \(\delta_{x}\), the conditional loss and the expected loss coincide and therefore that we have \(\mathcal{E}_{\ell_{2}}(h)-\mathcal{E}_{\ell_{2}}^{*}(\mathcal{H})=\Delta \mathcal{E}_{\ell_{2},\mathcal{H}}(\mathcal{H},x)\), and similarly with \(\ell_{1}\). Thus, we can write:

\[\forall h\in\mathcal{H},\forall x\in\mathcal{X},\quad\Delta\mathcal{E}_{\ell_{ 2},\mathcal{H}}(h,x)\leq\Gamma(\Delta\mathcal{E}_{\ell_{1},\mathcal{H}}(h,x)).\]

Therefore, by Jensen's inequality, for any distribution \(\mathcal{D}\), we have

\[\forall h\in\mathcal{H},\forall x\in\mathcal{X},\quad\mathbb{E}_{x}\big{[} \Delta\mathcal{E}_{\ell_{2},\mathcal{H}}(h,x)\big{]}\leq\mathbb{E}_{x}\big{[} \Gamma(\Delta\mathcal{E}_{\ell_{1},\mathcal{H}}(h,x))\big{]}\leq\Gamma\Big{(} \mathbb{E}_{x}\big{[}\Delta\mathcal{E}_{\ell_{1},\mathcal{H}}(h,x)\big{]} \Big{)}.\]

Since \(\mathbb{E}_{x}\big{[}\Delta\mathcal{E}_{\ell_{2},\mathcal{H}}(h,x)\big{]}= \mathcal{E}_{\ell_{2}}(h)-\mathcal{E}_{\ell_{2}}^{*}(\mathcal{H})+\mathcal{M} _{\ell_{2}}(\mathcal{H})\) and similarly with \(\ell_{1}\), we obtain the following bound for all distributions \(\mathcal{D}\):

\[\forall h\in\mathcal{H},\quad\mathcal{E}_{\ell_{2}}(h)-\mathcal{E}_{\ell_{2}}^ {*}(\mathcal{H})+\mathcal{M}_{\ell_{2}}(\mathcal{H})\leq\Gamma\big{(}\mathcal{E }_{\ell_{1}}(h)-\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H})+\mathcal{M}_{\ell_{1}}( \mathcal{H})\big{)}.\] (6)

This leads to the general form of \(\mathcal{H}\)-consistency bounds that we will be considering, which includes the key role of the minimizability gaps.

## Appendix D Properties of minimizability gaps

By Lemma 2.1, for a pointwise loss function, we have \(\mathcal{E}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}})=\mathbb{E}_{x}\big{[} \mathcal{C}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)\big{]}\), thus the minimizability gap vanishes for the family of all measurable functions.

**Lemma D.1**.: _Let \(\ell\) be a pointwise loss function. Then, we have \(\mathcal{M}_{\ell}(\mathcal{H}_{\mathrm{all}})=0\)._Thus, in that case, (6) takes the following simpler form:

\[\forall h\in\mathcal{H},\quad\mathcal{E}_{\ell_{2}}(h)-\mathcal{E}_{\ell_{2}}^{*} (\mathcal{H}_{\mathrm{all}})\leq\Gamma\big{(}\mathcal{E}_{\ell_{1}}(h)- \mathcal{E}_{\ell_{1}}^{*}(\mathcal{H}_{\mathrm{all}})\big{)}.\] (7)

In general, however, the minimizabiliy gap is non-zero for a restricted hypothesis set \(\mathcal{H}\) and is therefore important to analyze. Let \(\mathcal{J}_{\ell}(\mathcal{H})\) be the difference of pointwise infima \(\mathcal{J}_{\ell}(\mathcal{H})=\mathbb{E}_{x}\big{[}\mathcal{C}_{\ell}^{*}( \mathcal{H},x)-\mathcal{C}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)\big{]}\), which is non-negative. Note that, for a pointwise loss function, the minimizability gap can be decomposed as follows in terms of the approximation error and the difference of pointwise infima:

\[\mathcal{M}_{\ell}(\mathcal{H}) =\mathcal{E}_{\ell}^{*}(\mathcal{H})-\mathcal{E}_{\ell}^{*}( \mathcal{H}_{\mathrm{all}})+\mathcal{E}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}} )-\underset{x}{\mathbb{E}}[\mathcal{C}_{\ell}^{*}(\mathcal{H},x)]\] \[=\mathcal{A}_{\ell}(\mathcal{H})+\mathcal{E}_{\ell}^{*}( \mathcal{H}_{\mathrm{all}})-\underset{x}{\mathbb{E}}[\mathcal{C}_{\ell}^{*}( \mathcal{H},x)]\] \[=\mathcal{A}_{\ell}(\mathcal{H})-\mathcal{J}_{\ell}(\mathcal{H}) \leq\mathcal{A}_{\ell}(\mathcal{H}).\]

Thus, the minimizabiliy gap can be upper bounded by the approximation error. It is however a finer quantity than the approximation error and can lead to more favorable guarantees. When the difference of pointwise infima can be evaluated or bounded, this decomposition can provide a convenient way to analyze the minimizability gap in terms of the approximation error.

Note that \(\mathcal{J}_{\ell}(\mathcal{H})\) can be non-zero for families of bounded functions. Let \(\mathcal{Y}=\{-1,+1\}\) and \(\mathcal{H}\) be a family of functions \(h\) with \(|h(x)|\leq\Lambda\) for all \(x\in\mathcal{X}\) and such that all values in \([-\Lambda,+\Lambda]\) can be reached. Consider for example the exponential-based margin loss: \(\ell(h,x,y)=e^{-yh(x)}\). Let \(\eta(x)=\mathsf{p}(+1\,|\,x)=\mathcal{D}(Y=+1\,|\,X=x)\). Thus, \(\mathcal{E}_{\ell}(h,x)=\eta(x)e^{-h(x)}+(1-\eta(x))e^{h(x)}\). Then, it is not hard to see that \(\mathcal{C}_{\ell}^{*}(\mathcal{H}_{\mathrm{all}},x)=2\sqrt{\eta(x)(1-\eta(x))}\) for all \(x\) but \(\mathcal{C}_{\ell}^{*}(\mathcal{H},x)\) depends on \(\Lambda\) with the minimizing value for \(h(x)\) being: \(\min\Bigl{\{}\frac{1}{2}\log\frac{\eta(x)}{1-\eta(x)},\Lambda\Bigr{\}}\) if \(\eta(x)\geq 1/2\), \(\max\Bigl{\{}\frac{1}{2}\log\frac{\eta(x)}{1-\eta(x)},-\Lambda\Bigr{\}}\) otherwise. Thus, in the deterministic case, \(\mathcal{J}_{\ell}(\mathcal{H})=e^{-\Lambda}\).

When the best-in-class error coincides with the Bayes error, \(\mathcal{E}_{\ell}^{*}(\mathcal{H})=\mathcal{E}_{\ell}^{*}(\mathcal{H}_{ \mathrm{all}})\), both the approximation error and minimizability gaps vanish.

**Lemma D.2**.: _For any loss function \(\ell\) such that \(\mathcal{E}_{\ell}^{*}(\mathcal{H})=\mathcal{E}_{\ell}^{*}(\mathcal{H}_{ \mathrm{all}})=\mathbb{E}_{x}\big{[}\mathcal{C}_{\ell}^{*}(\mathcal{H}_{ \mathrm{all}},x)\big{]}\), we have \(\mathcal{M}_{\ell}(\mathcal{H})=\mathcal{A}_{\ell}(\mathcal{H})=0\)._

Proof.: By definition, \(\mathcal{A}_{\ell}(\mathcal{H})=\mathcal{E}_{\ell}^{*}(\mathcal{H})-\mathcal{E }_{\ell}^{*}(\mathcal{H}_{\mathrm{all}})=0\). Since we have \(\mathcal{M}_{\ell}(\mathcal{H})\leq\mathcal{A}_{\ell}(\mathcal{H})\), this implies \(\mathcal{M}_{\ell}(\mathcal{H})=0\). 

## Appendix E Examples of \(\mathcal{H}\)-consistency bounds

Here, we compile some common examples of \(\mathcal{H}\)-consistency bounds for both binary and multi-class classification. Table 1, 2 and 3 include the examples of \(\mathcal{H}\)-consistency bounds for binary margin-based losses, comp-sum losses and constrained losses, respectively.

These bounds are due to previous work by Awasthi et al. (2022) for binary margin-based losses, by Mao et al. (2023) for multi-class comp-sum losses, and by Awasthi et al. (2022) and Mao et al. (2023) for multi-class constrained losses, respectively. We consider complete hypothesis sets for binary classification (see Section 4), and symmetric and complete hypothesis sets for multi-class classification (see Section 5).

\begin{table}
\begin{tabular}{l l l} \hline \hline \(\Phi(u)\) & margin-based losses \(\ell\) & \(\mathcal{H}\)-Consistency bounds \\ \hline \(e^{u}\) & \(e^{-yh(x)}\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq\sqrt{2}(\mathcal{E}_{\ell}(h)- \mathcal{E}_{\ell}^{*}(\mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))^{\frac{1}{2}}\) \\ \(\log(1+e^{u})\) & \(\log(1+e^{-yh(x)})\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq\sqrt{2}(\mathcal{E}_{\ell}(h)- \mathcal{E}_{\ell}^{*}(\mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))^{\frac{1}{2}}\) \\ \(\max\bigl{\{}0,1+u\bigr{\}}^{2}\) & \(\max\bigl{\{}0,1-yh(x)\bigr{\}}^{2}\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq(\mathcal{E}_{\ell}(h)-\mathcal{E}_{ \ell}^{*}(\mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))^{\frac{1}{2}}\) \\ \(\max\bigl{\{}0,1+u\bigr{\}}\) & \(\max\bigl{\{}0,1-yh(x)\bigr{\}}\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq\mathcal{E}_{\ell}(h)-\mathcal{E}_{ \ell}^{*}(\mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Examples of \(\mathcal{H}\)-consistency bounds for binary margin-based losses.

## Appendix F Comparison with excess error bounds

Excess error bounds can be used to derive bounds for a hypothesis set \(\mathcal{H}\) expressed in terms of the approximation error. Here, we show, however, that, the resulting bounds are looser than \(\mathcal{H}\)-consistency bounds.

Fix a target loss function \(\ell_{2}\) and a surrogate loss \(\ell_{1}\). Excess error bounds, also known as _surrogate regret bounds_, are bounds relating the excess errors of these loss functions of the following form:

\[\forall h\in\mathcal{H}_{\mathrm{all}},\quad\psi\big{(}\mathcal{E}_{\ell_{2} }(h)-\mathcal{E}_{\ell_{2}}^{*}(\mathcal{H}_{\mathrm{all}})\big{)}\leq \mathcal{E}_{\ell_{1}}(h)-\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H}_{\mathrm{all }}),\] (8)

where \(\psi\colon\mathbb{R}_{+}\to\mathbb{R}_{+}\) is a non-decreasing and convex function on \(\mathbb{R}_{+}\). Recall that as shown in (1), the excess error can be written as the sum of the estimation error and the approximation error. Thus, the excess error bound can be equivalently expressed as follows:

\[\forall h\in\mathcal{H}_{\mathrm{all}},\quad\psi\big{(}\mathcal{E}_{\ell_{2} }(h)-\mathcal{E}_{\ell_{2}}^{*}(\mathcal{H})+\mathcal{A}_{\ell_{2}}(\mathcal{ H})\big{)}\leq\mathcal{E}_{\ell_{1}}(h)-\mathcal{E}_{\ell_{1}}^{*}(\mathcal{H})+ \mathcal{A}_{\ell_{1}}(\mathcal{H}).\] (9)

In Section 3, we have shown that the minimizabiliy gap can be upper bounded by the approximation error \(\mathcal{M}_{\ell}(\mathcal{H})\leq\mathcal{A}(\mathcal{H})\) and is in general a finer quantity for a surrogate loss \(\ell_{1}\). However, we will show that for a target loss \(\ell_{2}\) that is _discrete_, the minimizabiliy gap in general coincides with the approximation error.

**Definition F.1**.: We say that a target loss \(\ell_{2}\) is _discrete_ if we can write \(\ell_{2}(h,x,y)=\mathsf{L}(\mathsf{h}(x),y)\) for some binary function \(\mathsf{L}\colon\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}_{+}\).

In other words, a discrete target loss \(\ell_{2}\) is explicitly a function of both the prediction \(\mathsf{h}(x)\) and the true label \(y\), where both belong to the label space \(\mathcal{Y}\). Consequently, it can assume at most \(n^{2}\) distinct discrete values.

Next, we demonstrate that for such discrete target loss functions, if for any instance, the set of predictions generated by the hypothesis set completely spans the label space, then the minimizability gap is precisely equal to the approximation error. For convenience, we denote by \(\mathsf{H}(x)\) the set of predictions generated by the hypothesis set on input \(x\in\mathcal{X}\), defined as \(\mathsf{H}(x)=\{\mathsf{h}(x)\colon h\in\mathcal{H}\}\)

**Theorem F.2**.: _Given a discrete target loss function \(\ell_{2}\). Assume that the hypothesis set \(\mathcal{H}\) satisfies, for any \(x\in\mathcal{X}\), \(\mathsf{H}(x)=\mathcal{Y}\). Then, we have \(\mathcal{I}_{\ell_{2}}(\mathcal{H})=0\) and \(\mathcal{M}_{\ell_{2}}(\mathcal{H})=\mathcal{A}_{\ell_{2}}(\mathcal{H})\)._

Proof.: As shown in Section 3, the minimizability gap can be decomposed in terms of the approximation error and the difference of pointwise infima:

\[\mathcal{M}_{\ell_{2}}(\mathcal{H}) =\mathcal{A}_{\ell_{2}}(\mathcal{H})-\mathcal{I}_{\ell_{2}}( \mathcal{H})\] \[=\mathcal{A}_{\ell_{2}}(\mathcal{H})-\mathbb{E}\Big{[}\mathcal{C} _{\ell_{2}}^{*}(\mathcal{H},x)-\mathcal{C}_{\ell_{2}}^{*}(\mathcal{H}_{ \mathrm{all}},x)\Big{]}.\]

\begin{table}
\begin{tabular}{l l l} \hline \(\Phi(u)\) & Comp-sum losses \(\ell\) & \(\mathcal{H}\)-Consistency bounds \\ \hline \(\frac{1-u}{u}\) & \(\sum_{y^{\prime}\neq y}e^{h(x,y^{\prime})-h(x,y)}\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq\sqrt{2}(\mathcal{E}_{\ell}(h)- \mathcal{E}_{\ell}^{*}(\mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))^{\frac{1}{2}}\) \\ \(-\log(u)\) & \(-\log\Big{(}\frac{e^{h(x,y)}}{\sum_{y^{\prime}\neq y}e^{h(x,y^{\prime})}}\Big{)}\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+ \mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq\sqrt{2}(\mathcal{E}_{\ell}(h)- \mathcal{E}_{\ell}^{*}(\mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))^{\frac{1}{2}}\) \\ \(\frac{1}{\alpha}\big{[}1-u^{\alpha}\big{]}\) & \(\frac{1}{\alpha}\Big{[}1-\Big{[}\frac{e^{h(x,y)}}{\sum_{y^{\prime}\neq y}e^{h(x, y^{\prime})}}\Big{]}^{\alpha}\Big{]}\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{M}_ {\ell_{0-1}}(\mathcal{H})\leq\sqrt{2n^{\alpha}}(\mathcal{E}_{\ell}(h)- \mathcal{E}_{\ell}^{*}(\mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))^{\frac{1}{2}}\) \\ \(1-u\) & \(1-\frac{e^{h(x,y)}}{\sum_{y^{\prime}\neq y}e^{h(x,y^{\prime})}}\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{M}_ {\ell_{0-1}}(\mathcal{H})\leq n(\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}( \mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))\) \\ \hline \end{tabular}
\end{table}
Table 2: Examples of \(\mathcal{H}\)-consistency bounds for comp-sum losses.

\begin{table}
\begin{tabular}{l l l} \hline \(\Phi(u)\) & Constrained losses \(\ell\) & \(\mathcal{H}\)-Consistency bounds \\ \hline \(e^{u}\) & \(\sum_{y^{\prime}\neq y}e^{h(x,y^{\prime})}\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{M}_ {\ell_{0-1}}(\mathcal{H})\leq\sqrt{2}(\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}( \mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))^{\frac{1}{2}}\) \\ \(\max\{0,1+u\}^{2}\) & \(\sum_{y^{\prime}\neq\max\{0,1+h(x,y^{\prime})\}^{2}}\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{M}_ {\ell_{0-1}}(\mathcal{H})\leq(\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}( \mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))^{\frac{1}{2}}\) \\ \((1+u)^{2}\) & \(\sum_{y^{\prime}\neq y}(1+h(x,y^{\prime}))^{2}\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{M}_ {\ell_{0-1}}(\mathcal{H})\leq(\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}( \mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H}))^{\frac{1}{2}}\) \\ \(\max\{0,1+u\}\) & \(\sum_{y^{\prime}\neq y}\max\{0,1+h(x,y^{\prime})\}\) & \(\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}_{\ell_{0-1}}^{*}(\mathcal{H})+\mathcal{M}_ {\ell_{0-1}}(\mathcal{H})\leq\mathcal{E}_{\ell}(h)-\mathcal{E}_{\ell}^{*}( \mathcal{H})+\mathcal{M}_{\ell}(\mathcal{H})\) \\ \hline \end{tabular}
\end{table}
Table 3: Examples of \(\mathcal{H}\)-consistency bounds for constrained losses with \(\sum_{y\neq y}h(x,y)=0\).

By definition and the fact that \(\ell_{2}\) is discrete, the conditional error can be written as

\[\mathcal{C}_{\ell_{2}}(h,x)=\sum_{y\in\sharp}\mathsf{p}(y\!\mid\!x)\ell_{2}(h,x,y )=\sum_{y\in\sharp}\mathsf{p}(y\!\mid\!x)\mathsf{L}(\mathsf{h}(x),y).\]

Thus, for any \(x\in\mathcal{X}\), the best-in-class conditional error can be expressed as

\[\mathcal{C}^{*}_{\ell_{2}}(\mathcal{H},x)=\inf_{h\in\mathcal{H}}\sum_{y\in \sharp}\mathsf{p}(y\!\mid\!x)\mathsf{L}(\mathsf{h}(x),y)=\inf_{y^{\prime}\in \mathsf{H}(x)}\sum_{y\in\sharp}\mathsf{p}(y\!\mid\!x)\mathsf{L}(y^{\prime},y).\]

By the assumption that \(\mathsf{H}(x)=\mathcal{Y}\), we obtain

\[\forall\,x\in\mathcal{X},\quad\mathcal{C}^{*}_{\ell_{2}}(\mathcal{H},x)=\inf_ {y^{\prime}\in\mathsf{H}(x)}\sum_{y\in\sharp}\mathsf{p}(y\!\mid\!x)\mathsf{L} (y^{\prime},y)=\inf_{y^{\prime}\in\sharp}\sum_{y\in\sharp}\mathsf{p}(y\!\mid\! x)\mathsf{L}(y^{\prime},y)=\mathcal{C}^{*}_{\ell_{2}}(\mathcal{H}_{\mathrm{all}},x).\]

Therefore, \(\mathcal{I}_{\ell_{2}}(\mathcal{H})=\mathbb{E}_{x}\!\left[\mathcal{C}^{*}_{ \ell_{2}}(\mathcal{H},x)-\mathcal{C}^{*}_{\ell_{2}}(\mathcal{H}_{\mathrm{all}},x)\right]=0\) and \(\mathcal{M}_{\ell_{2}}(\mathcal{H})=\mathcal{A}_{\ell_{2}}(\mathcal{H})\). 

By Theorem F.2, for a target loss \(\ell_{2}\) that is discrete and hypothesis sets \(\mathcal{H}\) modulo mild assumptions, the minimizability gap coincides with the approximation error. In such cases, by comparing an excess error bound (9) with the \(\mathcal{H}\)-consistency bound (6):

\[\text{Excess error bound:}\quad\psi\!\left(\mathcal{E}_{\ell_{2}}(h) -\mathcal{E}^{*}_{\ell_{2}}(\mathcal{H})+\mathcal{A}_{\ell_{2}}(\mathcal{H}) \right)\leq\mathcal{E}_{\ell_{1}}(h)-\mathcal{E}^{*}_{\ell_{1}}(\mathcal{H}) +\mathcal{A}_{\ell_{1}}(\mathcal{H})\] \[\mathcal{H}\text{-consistency bound:}\quad\psi\!\left(\mathcal{E}_{ \ell_{2}}(h)-\mathcal{E}^{*}_{\ell_{2}}(\mathcal{H})+\mathcal{M}_{\ell_{2}}( \mathcal{H})\right)\leq\mathcal{E}_{\ell_{1}}(h)-\mathcal{E}^{*}_{\ell_{1}}( \mathcal{H})+\mathcal{M}_{\ell_{1}}(\mathcal{H}),\]

we obtain that the left-hand side of both bounds are equal (since \(\mathcal{M}_{\ell_{2}}(\mathcal{H})=\mathcal{A}_{\ell_{2}}(\mathcal{H})\)), while the right-hand side of the \(\mathcal{H}\)-consistency bound is always upper bounded by and can be finer than the right-hand side of the excess error bound (since \(\mathcal{M}_{\ell_{1}}(\mathcal{H})\leq\mathcal{A}_{\ell_{1}}(\mathcal{H})\)), which implies that excess error bounds (or surrogate regret bounds) are in general inferior to \(\mathcal{H}\)-consistency bounds.

## Appendix G Polyhedral losses versus smooth losses

Since \(\mathcal{H}\)-consistency bounds subsume excess error bounds as a special case (Appendix F), the linear growth rate of polyhedral loss excess error bounds (Finocchiaro et al. (2019)) also dictates a linear growth rate for polyhedral \(\mathcal{H}\)-consistency bounds, if they exist. This is illustrated by the hinge loss or \(\rho\)-margin loss which have been shown to benefit from \(\mathcal{H}\)-consistency bounds (Awasthi et al., 2022).

Here, we compare in more detail polyhedral losses and the smooth losses. Assume that a hypothesis set \(\mathcal{H}\) is complete and thus \(\mathsf{H}(x)=\mathcal{Y}\) for any \(x\in\mathcal{X}\). By Theorem F.2, we have \(\mathcal{A}_{\ell_{0-1}}(\mathcal{H})=\mathcal{M}_{\ell_{0-1}}(\mathcal{H})\). As shown by Frongillo and Waggner (2021, Theorem 3), a Bayes-consistent polyhedral loss \(\Phi_{\mathrm{poly}}\) admits the following linear excess error bound, for some \(\beta_{1}>0\),

\[\forall h\in\mathcal{H},\,\beta_{1}\!\left(\mathcal{E}_{\ell_{0-1}}(h)- \mathcal{E}^{*}_{\ell_{0-1}}(\mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H} )\right)\leq\mathcal{E}_{\Phi_{\mathrm{poly}}}(h)-\mathcal{E}^{*}_{\Phi_{ \mathrm{poly}}}(\mathcal{H})+\mathcal{A}_{\Phi_{\mathrm{poly}}}(\mathcal{H}).\] (10)

However, for a smooth loss \(\Phi_{\mathrm{smooth}}\), if it satisfies the condition of Theorem 4.2, \(\Phi_{\mathrm{smooth}}\) admits the following \(\mathcal{H}\)-consistency bound:

\[\forall h\in\mathcal{H},\,\mathcal{T}\!\left(\mathcal{E}_{\ell_{0-1}}(h)- \mathcal{E}^{*}_{\ell_{0-1}}(\mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H} )\right)\leq\mathcal{E}_{\Phi_{\mathrm{smooth}}}(h)-\mathcal{E}^{*}_{\Phi_{ \mathrm{smooth}}}(\mathcal{H})+\mathcal{M}_{\Phi_{\mathrm{smooth}}}(\mathcal{H}).\] (11)

where \(\mathcal{T}(t)=\Theta(t^{2})\). Therefore, our theory offers a principled basis for comparing polyhedral losses (10) and smooth losses (11), which depends on the following factors:

* The growth rate: linear for polyhedral losses, while square-root for smooth losses.
* The optimization property: smooth losses are more favorable for optimization compared to polyhedral losses, in particular with deep neural networks.
* The approximation theory: the approximation error \(\mathcal{A}_{\Phi_{\mathrm{poly}}}(\mathcal{H})\) appears on the right-hand side of the bound for polyhedral losses, whereas a finer quantity, the minimizability gap \(\mathcal{M}_{\Phi_{\mathrm{smooth}}}(\mathcal{H})\), is present on the right-hand side of the bound for smooth losses.

## Appendix H Comparison of minimizability gaps across comp-sum losses

For \(\ell_{\tau}^{\mathrm{compp}}\) loss functions, \(\tau\in[0,2)\), we can characterize minimizability gaps as follows.

**Theorem H.1**.: _Assume that for any \(x\in\mathcal{X}\), we have \(\{(h(x,1),\ldots,h(x,n))\colon h\in\mathcal{H}\}=[-\Lambda,+\Lambda]^{n}\). Then, for comp-sum losses \(\ell_{\tau}^{\mathrm{compp}}\) and any deterministic distribution, the minimizability gaps can be expressed as follows:_

\[\mathcal{M}_{\ell_{\tau}^{\mathrm{compp}}}(\mathcal{H})\leq\widehat{\mathcal{ M}}_{\ell_{\tau}^{\mathrm{compp}}}(\mathcal{H})=f_{\tau}\Big{(}\mathcal{R}_{ \ell_{\tau=0}^{\mathrm{compp}}}^{*}(\mathcal{H})\Big{)}-f_{\tau}\Big{(} \mathcal{C}_{\ell_{\tau=0}^{\mathrm{compp}}}^{*}(\mathcal{H},x)\Big{)},\] (12)

_where \(f_{\tau}(u)=\log(1+u)1_{\tau=1}+\frac{1}{1-\tau}\big{(}(1+u)^{1-\tau}-1)1_{ \tau=1}\) and \(\mathcal{C}_{\ell_{\tau=0}^{\mathrm{compp}}}^{*}(\mathcal{H},x)=e^{-2\Lambda} (n-1)\). Moreover, \(\widehat{\mathcal{M}}_{\ell_{\tau}^{\mathrm{compp}}}(\mathcal{H})\) is a non-increasing function of \(\tau\)._

Proof.: Since \(f_{\tau}\) is concave and non-decreasing, and the equality \(\ell_{\tau}=f_{\tau}(\ell_{\tau=0})\) holds, the minimizability gaps can be upper bounded as follows, for any \(\tau\geq\)0,

\[\mathcal{M}_{\ell_{\tau}^{\mathrm{compp}}}(\mathcal{H})\leq f_{\tau}\Big{(} \mathcal{R}_{\ell_{\tau=0}^{\mathrm{compp}}}^{*}(\mathcal{H})\Big{)}-\mathbb{ E}_{x}[\mathcal{C}_{\ell_{\tau}^{\mathrm{compp}}}^{*}(\mathcal{H},x)].\]

Since the distribution is deterministic, the conditional error can be expressed as follows:

\[\mathcal{C}_{\ell_{\tau}^{\mathrm{compp}}}(h,x)= f_{\tau}\Bigg{(}\sum_{y^{\prime}\neq y_{\max}}\exp(h(x,y^{\prime})-h(x,y_{ \max}))\Bigg{)}\] (13)

where \(y_{\max}=\operatorname*{argmax}\operatorname{\mathsf{p}}(y\,|\,x)\). Using the fact that \(f_{\tau}\) is increasing for any \(\tau>0\), the hypothesis \(h^{*}\colon(x,y)\mapsto\Lambda 1_{y=y_{\max}}-\Lambda 1_{y\neq y_{\max}}\) achieves the best-in-class conditional error. Thus,

\[\mathcal{C}_{\ell_{\tau}^{\mathrm{compp}}}^{*}(\mathcal{H},x)=\mathcal{C}_{ \ell_{\tau}^{\mathrm{compp}}}(h^{*},x)=f_{\tau}\Big{(}\mathcal{C}_{\ell_{\tau =0}^{\mathrm{compp}}}^{*}(\mathcal{H},x)\Big{)}\]

where \(\mathcal{C}_{\ell_{\tau=0}^{\mathrm{compp}}}^{*}(\mathcal{H},x)=e^{-2\Lambda} (n-1)\). Therefore,

\[\mathcal{M}_{\ell_{\tau}^{\mathrm{compp}}}^{*}(\mathcal{H})\leq f_{\tau}\Big{(} \mathcal{R}_{\ell_{\tau=0}^{\mathrm{compp}}}^{*}(\mathcal{H})\Big{)}-f_{\tau} \Big{(}\mathcal{C}_{\ell_{\tau=0}^{\mathrm{compp}}}^{*}(\mathcal{H},x)\Big{)}.\]

This completes the first part of the proof. Using the fact that \(\tau\mapsto f_{\tau}(u_{1})-f_{\tau}(u_{2})\) is a non-increasing function of \(\tau\) for any \(u_{1}\geq u_{2}\geq 0\), the second proof is completed as well. 

The theorem shows that for comp-sum loss functions \(\ell_{\tau}^{\mathrm{comp}}\), the minimizability gaps are non-increasing with respect to \(\tau\). Note that \(\Phi^{\tau}\) satisfies the conditions of Theorem 5.3 for any \(\tau\in[0,2)\). Therefore, focusing on behavior near zero (ignoring constants), the theorem provides a principled comparison of minimizability gaps and \(\mathcal{H}\)-consistency bounds across different comp-sum losses.

## Appendix I Small surrogate minimizability gaps: proof for binary classification

**Theorem 6.1**.: _Assume that \(\mathcal{D}\) is deterministic and that the best-in-class error is achieved by some \(h^{*}\in\mathcal{H}\). Then, the minimizability gap is null, \(\mathcal{M}(\mathcal{H})=0\), iff_

\[\ell(h^{*}(x),+1)=\ell_{+}\text{ a.s. over }\mathcal{X}_{+},\quad\ell(h^{*}(x),-1)= \ell_{-}\text{ a.s. over }\mathcal{X}_{-}.\]

_If further \(\alpha\mapsto\ell(\alpha,+1)\) and \(\alpha\mapsto\ell(\alpha,-1)\) are injective and \(\ell_{+}=\ell(\alpha_{+},+1)\), \(\ell_{-}=\ell(\alpha_{-},-1)\), then, the condition is equivalent to \(h^{*}(x)=\alpha_{+}1_{x\in\mathcal{X}_{+}}+\alpha_{-}1_{x\in\mathcal{X}}\). Furthermore, the minimizability gap is bounded by \(\epsilon\) iff \(p(\mathbb{E}[\ell(h^{*}(x),+1)\mid y=+1]-\ell_{+})+(1-p)\big{(}\mathbb{E}[\ell (h^{*}(x),-1)\mid y=-1]-\ell_{-}\big{)}\leq\epsilon\). In particular, the condition implies:_

\[\mathbb{E}[\ell(h^{*}(x),+1)\mid y=+1]-\ell_{+}\leq\frac{\epsilon}{p}\quad \text{and}\quad\mathbb{E}[\ell(h^{*}(x),-1)\mid y=-1]-\ell_{-}\leq\frac{ \epsilon}{1-p}.\]

Proof.: By definition of \(h^{*}\), using the shorthand \(p=\mathbb{P}[y=+1]\), we can write

\[\inf_{h\in\mathcal{H}}\mathbb{E}[\ell(h(x),y)] =\mathbb{E}[\ell(h^{*}(x),y)]\] \[=p\,\mathbb{E}[\ell(h^{*}(x),+1)\mid y=+1]+(1-p)\,\mathbb{E}[\ell (h^{*}(x),-1)\mid y=-1].\]Since the distribution is deterministic, the expected pointwise infimum can be rewritten as follows:

\[\operatorname*{\mathbb{E}}_{x}\!\left[\inf_{h\in\mathcal{H}}\operatorname* {\mathbb{E}}_{y}\!\left[\ell(h(x),y)\mid x\right]\right]=\operatorname*{ \mathbb{E}}_{x}\!\left[\inf_{\alpha\in A}\operatorname*{\mathbb{E}}_{y}\!\left[ \ell(\alpha,y)\mid x\right]\right] =p\inf_{\alpha\in A}\ell(\alpha,+1)+(1-p)\inf_{\alpha\in A}\ell( \alpha,-1)\] \[=p\ell_{+}+(1-p)\ell_{-},\]

where \(\ell_{+}=\inf_{\alpha\in A}\ell(\alpha,+1)\) and \(\ell_{-}=\inf_{\alpha\in A}\ell(\alpha,-1)\). Thus, we have

\[\mathcal{M}(\mathcal{H})=p\operatorname*{\mathbb{E}}\!\left[\ell(h^{*}(x),+1)- \ell_{+}\mid y=+1\right]+(1-p)\operatorname*{\mathbb{E}}\!\left[\ell(h^{*}(x), -1)-\ell_{-}\mid y=-1\right].\]

In view of that, since, by definition of \(\ell_{+}\) and \(\ell_{-}\), the expressions within the conditional expectations are non-negative, the equality \(\mathcal{M}(\mathcal{H})=0\) holds iff \(\ell(h^{*}(x),+1)-\ell_{+}=0\) almost surely for any \(x\) in \(\mathcal{X}_{+}\) and \(\ell(h^{*}(x),-1)-\ell_{-}=0\) almost surely for any \(x\) in \(\mathcal{X}_{-}\). This completes the first part of the proof. Furthermore, \(\mathcal{M}(\mathcal{H})\leq\epsilon\) is equivalent to

\[p\operatorname*{\mathbb{E}}\!\left[\ell(h^{*}(x),+1)-\ell_{+}\mid y=+1\right] +(1-p)\operatorname*{\mathbb{E}}\!\left[\ell(h^{*}(x),-1)-\ell_{-}\mid y=-1 \right]\leq\epsilon\]

that is

\[p(\operatorname*{\mathbb{E}}\!\left[\ell(h^{*}(x),+1)\mid y=+1\right]-\ell_{ +})+(1-p)(\operatorname*{\mathbb{E}}\!\left[\ell(h^{*}(x),-1)\mid y=-1\right] -\ell_{-})\leq\epsilon.\]

In light of the non-negativity of the expressions, this implies in particular:

\[\operatorname*{\mathbb{E}}\!\left[\ell(h^{*}(x),+1)\mid y=+1\right]-\ell_{+} \leq\frac{\epsilon}{p}\quad\text{and}\quad\operatorname*{\mathbb{E}}\!\left[ \ell(h^{*}(x),-1)\mid y=-1\right]-\ell_{-}\leq\frac{\epsilon}{1-p}.\]

This completes the second part of the proof. 

**Theorem 6.2**.: _The best-in-class error is achieved by some \(h^{*}\in\mathcal{H}\) and the minimizability gap is null, \(\mathcal{M}(\mathcal{H})=0\), iff there exists \(h^{*}\in\mathcal{H}\) such that for all \(x\),_

\[\operatorname*{\mathbb{E}}_{y}\!\left[\ell(h^{*}(x),y)\mid x\right]=\inf_{ \alpha\in A}\operatorname*{\mathbb{E}}_{y}\!\left[\ell(\alpha,y)\mid x\right]\text {a.s. over }\mathcal{X}.\] (3)

_If further \(\alpha\mapsto\operatorname*{\mathbb{E}}_{y}\!\left[\ell(\alpha,y)\mid x\right]\) is injective and \(\inf_{\alpha\in A}\operatorname*{\mathbb{E}}_{y}\!\left[\ell(\alpha,y)\mid x \right]=\operatorname*{\mathbb{E}}_{y}\!\left[\ell(\alpha^{*}(x),y)\mid x\right]\), then, the condition is equivalent to \(h^{*}(x)=\alpha^{*}(x)\) a.s. for \(x\in\mathcal{X}\). Furthermore, the minimizability gap is bounded by \(\epsilon\), \(\mathcal{M}(\mathcal{H})\leq\epsilon\), iff there exists \(h^{*}\in\mathcal{H}\) such that_

\[\operatorname*{\mathbb{E}}_{x}\!\left[\operatorname*{\mathbb{E}}_{y}\!\left[ \ell(h^{*}(x),y)\mid x\right]-\inf_{\alpha\in A}\operatorname*{\mathbb{E}}_{y} \!\left[\ell(\alpha,y)\mid x\right]\right]\leq\epsilon.\] (4)

Proof.: Assume that the best-in-class error is achieved by some \(h^{*}\in\mathcal{H}\). Then, we can write

\[\inf_{h\in\mathcal{H}}\operatorname*{\mathbb{E}}\!\left[\ell(h(x),y)\right]= \operatorname*{\mathbb{E}}\!\left[\ell(h^{*}(x),y)\right]=\operatorname*{ \mathbb{E}}_{x}\!\left[\operatorname*{\mathbb{E}}_{y}\!\left[\ell(h^{*}(x),y) \mid x\right]\right]\!.\]

The expected pointwise infimum can be rewritten as follows:

\[\operatorname*{\mathbb{E}}_{x}\!\left[\inf_{h\in\mathcal{H}}\operatorname*{ \mathbb{E}}_{y}\!\left[\ell(h(x),y)\mid x\right]\right]=\operatorname*{\mathbb{ E}}_{x}\!\left[\inf_{\alpha\in A}\operatorname*{\mathbb{E}}_{y}\!\left[\ell(\alpha,y) \mid x\right]\right]\!.\]

Thus, we have

\[\mathcal{M}(\mathcal{H})=\operatorname*{\mathbb{E}}_{x}\!\left[\operatorname*{ \mathbb{E}}_{y}\!\left[\ell(h^{*}(x),y)\mid x\right]-\inf_{\alpha\in A} \operatorname*{\mathbb{E}}_{y}\!\left[\ell(\alpha,y)\mid x\right]\right]\!.\]

In view of that, since, by the definition of infimum, the expressions within the marginal expectations are non-negative, the condition that \(\mathcal{M}(\mathcal{H})=0\) implies that

\[\operatorname*{\mathbb{E}}_{y}\!\left[\ell(h^{*}(x),y)\mid x\right]=\inf_{\alpha \in A}\operatorname*{\mathbb{E}}_{y}\!\left[\ell(\alpha,y)\mid x\right]\text {a.s. over }\mathcal{X}.\]

On the other hand, if there exists \(h^{*}\in\mathcal{H}\) such that the condition (3) holds, then,

\[\mathcal{M}(\mathcal{H})=\inf_{h\in\mathcal{H}}\operatorname*{\mathbb{E}}\!\left[ \ell(h(x),y)\right]-\operatorname*{\mathbb{E}}_{x}\!\left[\inf_{\alpha\in A} \operatorname*{\mathbb{E}}_{y}\!\left[\ell(\alpha,y)\mid x\right]\right]\leq \operatorname*{\mathbb{E}}_{x}\!\left[\operatorname*{\mathbb{E}}_{y}\!\left[ \ell(h^{*}(x),y)\mid x\right]-\inf_{\alpha\in A}\operatorname*{\mathbb{E}}_{y} \!\left[\ell(\alpha,y)\mid x\right]\right]=0.\]

Since \(\mathcal{M}(\mathcal{H})\) is non-negative, the inequality is achieved. Thus, we have

\[\mathcal{M}(\mathcal{H})=0\text{ and }\inf_{h\in\mathcal{H}}\operatorname*{\mathbb{E}}\!\left[ \ell(h(x),y)\right]=\operatorname*{\mathbb{E}}\!\left[\ell(h^{*}(x),y)\right]\!.\]If there exists \(h^{*}\in\mathcal{H}\) such that the condition (4) holds, then,

\[\mathcal{M}(\mathcal{H})=\inf_{h\in\mathcal{H}}\mathbb{E}[\ell(h(x),y)]-\mathbb{E} \!\!\inf_{x}\!\!\inf_{\alpha\in A}\!\!\mathbb{E}[\ell(\alpha,y)\mid x]\!\leq \!\!\mathbb{E}_{x}\!\!\left[\mathbb{E}[\ell(h^{*}(x),y)\mid x]-\inf_{\alpha\in A }\mathbb{E}[\ell(\alpha,y)\mid x]\right]=\epsilon.\]

On the other hand, since we have

\[\mathcal{M}(\mathcal{H})=\mathbb{E}_{x}\!\!\left[\mathbb{E}[\ell(h^{*}(x),y) \mid x]-\inf_{\alpha\in A}\mathbb{E}[\ell(\alpha,y)\mid x]\right],\]

\(\mathcal{M}(\mathcal{H})\leq\epsilon\) implies that

\[\mathbb{E}_{x}\!\!\left[\mathbb{E}[\ell(h^{*}(x),y)\mid x]-\inf_{\alpha\in A }\mathbb{E}[\ell(\alpha,y)\mid x]\right]\leq\epsilon.\]

This completes the proof. 

## Appendix J Small surrogate minimizability gaps: multi-class classification

We consider the multi-class setting with label space \([n]=\{1,2,\ldots,n\}\). In this setting, the surrogate loss incurred by a predictor \(h\) at a labeled point \((x,y)\) can be expressed by \(\ell(h(x),y)\), where \(h(x)=[h(x,1),\ldots,h(x,n)]\) is the score vector of the predictor \(h\). We denote by \(A\) the set of values in \(\mathbb{R}^{n}\) taken by the score vector of predictors in \(\mathcal{H}\) at \(x\), which we assume to be independent of \(x\): \(A=\{h(x)\colon h\in\mathcal{H}\}\), for all \(x\in\mathcal{X}\).

### Deterministic scenario

We first consider the deterministic scenario, where the conditional probability \(\mathsf{p}(y\!\mid\!x)\) is either zero or one. For a deterministic distribution, we denote by \(\mathcal{X}_{k}\) the subset of \(\mathcal{X}\) over which the label is \(k\). For convenience, let \(\ell_{k}=\inf_{\alpha\in A}\ell(\alpha,k)\), for any \(k\in[n]\).

**Theorem J.1**.: _Assume that \(\mathcal{D}\) is deterministic and that the best-in-class error is achieved by some \(h^{*}\in\mathcal{H}\). Then, the minimizability gap is null, \(\mathcal{M}(\mathcal{H})=0\), iff_

\[\forall\,k\in[n],\,\ell(h^{*}(x),k)=\ell_{k}\text{ a.s. over }\mathcal{X}_{k}.\]

_If further \(\alpha\mapsto\ell(\alpha,k)\) is injective and \(\ell_{k}=\ell(\alpha_{k},k)\) for all \(k\in[n]\), then, the condition is equivalent to \(\forall\,k\in[n]\), \(h^{*}(x)=\alpha_{k}\text{ a.s. for }x\in\mathcal{X}_{k}\). Furthermore, the minimizability gap is bounded by \(\epsilon\), \(\mathcal{M}(\mathcal{H})\leq\epsilon\), iff_

\[\sum_{k\in[n]}p_{k}(\mathbb{E}[\ell(h^{*}(x),k)\mid y=k]-\ell_{k})\leq\epsilon.\]

_In particular, the condition implies:_

\[\mathbb{E}[\ell(h^{*}(x),k)\mid y=k]-\ell_{k}\leq\frac{\epsilon}{p_{k}},\; \forall\,k\in[n],\]

Proof.: By definition of \(h^{*}\), using the shorthand \(p_{k}=\mathbb{P}[y=k]\) for any \(k\in[n]\), we can write

\[\inf_{h\in\mathcal{H}}\mathbb{E}[\ell(h(x),y)]=\mathbb{E}[\ell(h^{*}(x),y)]= \sum_{k\in[n]}p_{k}\,\mathbb{E}[\ell(h^{*}(x),k)\mid y=k].\]

Since the distribution is deterministic, the expected pointwise infimum can be rewritten as follows:

\[\mathbb{E}\!\!\left[\inf_{x}\!\!\left[\inf_{h\in\mathcal{H}}\mathbb{E}[\ell(h( x),y)\mid x]\right]\right]=\mathbb{E}\!\!\left[\inf_{\alpha\in A}\mathbb{E}[\ell( \alpha,y)\mid x]\right]=\sum_{k\in[n]}p_{k}\inf_{\alpha\in A}\ell(\alpha,k)= \sum_{k\in[n]}p_{k}\ell_{k},\]

where \(\ell_{k}=\inf_{\alpha\in A}\ell(\alpha,k)\), for any \(k\in[n]\). Thus, we have

\[\mathcal{M}(\mathcal{H})=\sum_{k\in[n]}p_{k}\,\mathbb{E}[\ell(h^{*}(x),k)-\ell _{k}\mid y=k].\]

In view of that, since, by definition of \(\ell_{k}\), the expressions within the conditional expectations are non-negative, the equality \(\mathcal{M}(\mathcal{H})=0\) holds iff \(\ell(h^{*}(x),k)-\ell_{k}=0\) almost surely for any \(x\) in \(\mathcal{X}_{k}\), \(\forall\,k\in[n]\). Furthermore, \(\mathcal{M}(\mathcal{H})\leq\epsilon\) is equivalent to

\[\sum_{k\in[n]}p_{k}\,\mathbb{E}[\ell(h^{*}(x),k)-\ell_{k}\mid y=k]\leq\epsilon\]

[MISSING_PAGE_FAIL:25]

[MISSING_PAGE_EMPTY:26]

\(|h(x,\cdot)|\leq\Lambda\) for all \(x\in\mathcal{X}\) and such that all values in \([-\Lambda,+\Lambda]\) can be reached. Thus, \(A=[-\Lambda,\Lambda]^{n}\) for any \(x\in\mathcal{X}\). Consider the multi-class logistic loss: \(\ell(h,x,y)=-\log\!\left[\frac{e^{h(x,y)}}{\sum_{y^{\prime}\in\mathcal{X}}e^{h( x,y^{\prime})}}\right]\). For any \(\alpha=\left[\alpha^{1},\ldots,\alpha^{n}\right]\in A\), we denote by \(S_{k}=\frac{e^{\alpha^{k}}}{\sum_{h^{\prime}\in[h]}e^{\alpha^{k^{\prime}}}}\). Then, for any \(x\in\mathcal{X}\) and \(\alpha\in A\),

\[\operatorname*{\mathbb{E}}_{y}[\ell(\alpha,y)\mid x]=-(1-\epsilon)\log(S_{k}) -\frac{\epsilon}{n-1}\sum_{k^{\prime}\neq k}\log(S_{k^{\prime}})\text{ if }x\in \mathcal{X}_{k}.\]

Thus, it is not hard to see that for any \(\epsilon\leq\frac{n-1}{e^{2\Lambda}+n-1}\), the infimum \(\inf_{\alpha\epsilon A}\operatorname*{\mathbb{E}}_{y}[\ell(\alpha,y)\mid x]\) can be achieved by \(\alpha^{*}(x)=[-\Lambda,\ldots,\Lambda,\ldots,-\Lambda]\), where \(\Lambda\) occupies the \(k\)-th position for \(x\in\mathcal{X}_{k}\). Therefore, by Theorem 6.2, for these distributions and loss functions, when the best-in-class classifier \(h^{*}\)\(\epsilon\)-approximates \(\alpha_{k}=\left[-\Lambda,\ldots,\underset{k\text{-th}}{\Lambda},\ldots,-\Lambda\right]\) over \(\mathcal{X}_{k}\), then the minimizability gap is bounded by \(\epsilon\). The existence of such a hypothesis in \(\mathcal{H}\) depends on the complexity of the decision surface.

## Appendix K Proof for binary margin-based losses (Theorem 4.2)

**Theorem 4.2** (Upper and lower bound for binary margin-based losses).: _Let \(\mathcal{H}\) be a complete hypothesis set. Assume that \(\Phi\) is convex, twice continuously differentiable, and satisfies the inequalities \(\Phi^{\prime}(0)>0\) and \(\Phi^{\prime\prime}(0)>0\). Then, the following property holds: \(\mathcal{T}(t)=\Theta(t^{2})\); that is, there exist positive constants \(C>0\), \(c>0\), and \(T>0\) such that \(Ct^{2}\geq\mathcal{T}(t)\geq ct^{2}\), for all \(0<t\leq T\)._

Proof.: Since \(\Phi\) is convex and in \(C^{2}\), \(f_{t}\) is also convex and differentiable with respect to \(u\). For any \(t\in[0,1]\), differentiate \(f_{t}\) with respect to \(u\), we have

\[f_{t}^{\prime}(u)=\frac{1-t}{2}\Phi^{\prime}(u)-\frac{1+t}{2}\Phi^{\prime}(-u).\]

Consider the function \(F\) defined over \(\mathbb{R}^{2}\) by \(F(t,a)=\frac{1-t}{2}\Phi^{\prime}(a)-\frac{1+t}{2}\Phi^{\prime}(-a)\). Observe that \(F(0,0)=0\) and that the partial derivative of \(F\) with respect to \(a\) at \((0,0)\) is \(\Phi^{\prime\prime}(0)>0\):

\[\frac{\partial F}{\partial a}(t,a)=\frac{1-t}{2}\Phi^{\prime\prime}(a)+\frac{ 1+t}{2}\Phi^{\prime\prime}(-a),\quad\frac{\partial F}{\partial a}(0,0)=\Phi^{ \prime\prime}(0)>0.\]

Consequently, by the implicit function theorem, there exists a continuously differentiable function \(\overline{a}\) such that \(F(t,\overline{a}(t))=0\) in a neighborhood \([-\epsilon,\epsilon]\) around zero. Thus, by the convexity of \(f_{t}\) and the definition of \(F\), for \(t\in[0,\epsilon]\), \(\inf_{u\in\mathbb{R}}f_{t}(u)\) is reached by \(\overline{a}(t)\) and we can denote it by \(a_{t}^{*}\). Then, \(a_{t}^{*}\) is continuously differentiable over \([0,\epsilon]\). The minimizer \(a_{t}^{*}\) satisfies the following equality:

\[f_{t}^{\prime}(a_{t}^{*})=\frac{1-t}{2}\Phi^{\prime}(a_{t}^{*})-\frac{1+t}{2} \Phi^{\prime}(-a_{t}^{*})=0.\] (16)

Specifically, at \(t=0\), we have \(\Phi^{\prime}(a_{0}^{*})=\Phi^{\prime}(-a_{0}^{*})\). Since \(\Phi\) is convex, its derivative \(\Phi^{\prime}\) is non-decreasing. Therefore, if \(a_{0}^{*}\) were non-zero, then \(\Phi^{\prime}\) would be constant over the segment \([-|a_{0}^{*}|,|a_{0}^{*}|]\). This would contradict the condition \(\Phi^{\prime\prime}(0)>0\), as a constant function cannot have a positive second derivative at any point. Thus, we must have \(a_{0}^{*}=0\) and since \(\Phi^{\prime}\) is non-decreasing and \(\Phi^{\prime\prime}(0)>0\), we have \(a_{t}^{*}>0\) for all \(t\in(0,\epsilon]\). By Theorem 4.1 and Taylor's theorem with an integral remainder, \(\mathcal{T}\) can be expressed as follows: for any \(t\in[0,\epsilon]\),

\[\mathcal{T}(t) =f_{t}(0)-\inf_{u\in\mathbb{R}}f_{t}(u)\] \[=f_{t}(0)-f_{t}(a_{t}^{*})\] \[=f_{t}^{\prime}(a_{t}^{*})(0-a_{t}^{*})+\int_{a_{t}^{*}}^{0}(0-u)f _{t}^{\prime\prime}(u)\,du\] ( \[f_{t}^{\prime}(a_{t}^{*})=0\] ) \[=\int_{0}^{a_{t}^{*}}uf_{t}^{\prime\prime}(u)\,du\] \[=\int_{0}^{a_{t}^{*}}u\!\left[\frac{1-t}{2}\Phi^{\prime\prime}(u )+\frac{1+t}{2}\Phi^{\prime\prime}(-u)\right]\!du.\] (17)Since \(a_{t}^{*}\) is a function of class \(C^{1}\), we can differentiate (16) with respect to \(t\), which gives the following equality for any \(t\) in \((0,\epsilon]\):

\[-\frac{1}{2}\Phi^{\prime}(a_{t}^{*})+\frac{1-t}{2}\Phi^{\prime\prime}(a_{t}^{*} )\frac{da_{t}^{*}}{dt}(t)-\frac{1}{2}\Phi^{\prime}(-a_{t}^{*})+\frac{1+t}{2} \Phi^{\prime\prime}(-a_{t}^{*})\frac{da_{t}^{*}}{dt}(t)=0.\]

Taking the limit \(t\to 0\) yields

\[-\frac{1}{2}\Phi^{\prime}(0)+\frac{1}{2}\Phi^{\prime\prime}(0)\frac{da_{t}^{* }}{dt}(0)-\frac{1}{2}\Phi^{\prime}(0)+\frac{1}{2}\Phi^{\prime\prime}(0)\frac{ da_{t}^{*}}{dt}(0)=0.\]

This implies that

\[\frac{da_{t}^{*}}{dt}(0)=\frac{\Phi^{\prime}(0)}{\Phi^{\prime\prime}(0)}>0.\]

Since \(\lim_{t\to 0}\frac{a_{t}^{*}}{t}=\frac{da_{t}^{*}}{dt}(0)=\frac{\Phi^{\prime}(0) }{\Phi^{\prime\prime}(0)}>0\), we have \(a_{t}^{*}=\Theta(t)\).

Since \(\Phi^{\prime\prime}(0)>0\) and \(\Phi^{\prime\prime}\) is continuous, there is a non-empty interval \([-\alpha,+\alpha]\) over which \(\Phi^{\prime\prime}\) is positive. Since \(a_{0}^{*}=0\) and \(a_{t}^{*}\) is continuous, there exists a sub-interval \([0,\epsilon^{\prime}]\subseteq[0,\epsilon]\) over which \(a_{t}^{*}\leq\alpha\). Since \(\Phi^{\prime\prime}\) is continuous, it admits a minimum and a maximum over any compact set and we can define \(c=\min_{u\in[-\alpha,\alpha]}\Phi^{\prime\prime}(u)\) and \(C=\max_{u\in[-\alpha,\alpha]}\Phi^{\prime\prime}(u)\). \(c\) and \(C\) are both positive since we have \(\Phi^{\prime\prime}(0)>0\). Thus, for \(t\) in \([0,\epsilon^{\prime}]\), by (17), the following inequality holds:

\[C\frac{(a_{t}^{*})^{2}}{2}=\int_{0}^{a_{t}^{*}}uC\,du\geq\mathcal{T}(t)=\int_ {0}^{a_{t}^{*}}u\bigg{[}\frac{1-t}{2}\Phi^{\prime\prime}(u)+\frac{1+t}{2}\Phi ^{\prime\prime}(-u)\bigg{]}du\geq\int_{0}^{a_{t}^{*}}uC\,du=c\frac{(a_{t}^{*} )^{2}}{2}.\]

This implies that \(\mathcal{T}(t)=\Theta(t^{2})\). 

## Appendix L Proof for comp-sum losses (Theorem 5.3)

**Theorem 5.3** (Upper and lower bound for comp-sum losses).: _Assume that \(\Phi\) is convex, twice continuously differentiable, and satisfies the properties \(\Phi^{\prime}(u)<0\) and \(\Phi^{\prime\prime}(u)>0\) for any \(u\in(0,\frac{1}{2}]\). Then, the following property holds: \(\mathcal{T}(t)=\Theta(t^{2})\)._

Proof.: For any \(\tau\in\left[\frac{1}{n},\frac{1}{2}\right]\), define the function \(\mathcal{T}_{\tau}\) by

\[\forall t\in[0,1],\quad\mathcal{T}_{\tau}(t) =\sup_{|u|\leq\tau}\bigg{\{}\Phi(\tau)-\frac{1-t}{2}\Phi(\tau+u)- \frac{1+t}{2}\Phi(\tau-u)\bigg{\}}\] \[=f_{t,\tau}(0)-\inf_{|u|\leq\tau}f_{t,\tau}(u),\]

where

\[f_{t,\tau}(u)=\frac{1-t}{2}\Phi_{\tau}(u)+\frac{1+t}{2}\Phi_{\tau}(-u)\quad \text{and}\quad\Phi_{\tau}(u)=\Phi(\tau+u).\]

We aim to establish a lower bound for \(\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\mathcal{T}_{\tau}(t)\). For any fixed \(\tau\in\left[\frac{1}{n},\frac{1}{2}\right]\), this situation is parallel to that of binary classification (Theorem 4.1 and Theorem 4.2), since we have \(\Phi_{\tau}^{\prime}(0)=\Phi^{\prime}(\tau)<0\) and \(\Phi_{\tau}^{\prime\prime}(0)=\Phi^{\prime\prime}(\tau)>0\). Let \(a_{t,\tau}^{*}\) denotes the minimizer of \(f_{t,\tau}\) over \(\mathbb{R}\). By applying Theorem 5.2 to the function \(F\colon(t,u,\tau)\mapsto f_{t,\tau}^{\prime}(u)=\frac{1-t}{2}\Phi_{\tau}^{ \prime}(u)-\frac{1+t}{2}\Phi_{\tau}^{\prime}(-u)\) and the convexity of \(f_{t,\tau}\) with respect to \(u\), \(a_{t,\tau}^{*}\) exists, is unique and is continuously differentiable over \([0,t_{0}^{\prime}]\times\left[\frac{1}{n},\frac{1}{2}\right]\), for some \(t_{0}^{\prime}>0\). Moreover, by using the fact that \(f_{0,\tau}^{\prime}(\tau)>0\) and \(f_{0,\tau}^{\prime}(-\tau)<0\), and the convexity of \(f_{0,\tau}\) with respect to \(u\), we have \(\big{|}a_{0,\tau}^{*}\big{|}\leq\tau\), \(\forall\tau\in\left[\frac{1}{n},\frac{1}{2}\right]\). By the continuity of \(a_{t,\tau}^{*}\), we have \(\big{|}a_{t,\tau}^{*}\big{|}\leq\tau\) over \([0,t_{0}]\times\left[\frac{1}{n},\frac{1}{2}\right]\), for some \(t_{0}>0\) and \(t_{0}\leq t_{0}^{\prime}\).

Next, we will leverage the proof of Theorem 4.2. Adopting a similar notation, while incorporating the \(\tau\) subscript to distinguish different functions \(\Phi_{\tau}\) and \(f_{t,\tau}\), we can write

\[\forall t\in[0,t_{0}],\quad\mathcal{T}_{\tau}(t)=\int_{0}^{-a_{t,\tau}^{*}}u \bigg{[}\frac{1-t}{2}\Phi_{\tau}^{\prime\prime}(-u)+\frac{1+t}{2}\Phi_{\tau}^{ \prime\prime}(u)\bigg{]}\,du.\]

where \(a_{t,\tau}^{*}\) verifies

\[a_{0,\tau}^{*}=0\quad\text{and}\quad\frac{\partial a_{t,\tau}^{*}}{\partial t}(0)= \frac{\Phi_{\tau}^{\prime}(0)}{\Phi_{\tau}^{\prime\prime}(0)}=c_{\tau}<0.\] (18)We first show the lower bound \(\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-a^{*}_{t,\tau}=\Omega(t)\). Given the equalities (18), it follows that for any \(\tau\), the following holds: \(\lim_{t\to 0}(-a^{*}_{t,\tau}+c_{\tau}t)=0\). For any \(\tau\in\left[\frac{1}{n},\frac{1}{2}\right]\), \(t\mapsto\left(-a^{*}_{t,\tau}+c_{\tau}t\right)\) is a continuous function over \([0,t_{0}]\) since \(a^{*}_{t,\tau}\) is a function of class \(C^{1}\). Since the infimum over a fixed compact set of a family of continuous functions is continuous, \(t\mapsto\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\left\{-a^{*}_{t, \tau}+c_{\tau}t\right\}\) is continuous. Thus, for any \(\epsilon>0\), there exists \(t_{1}>0\), \(t_{1}\leq t_{0}\), such that for any \(t\in[0,t_{1}]\),

\[\left|\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\left\{-a^{*}_{t, \tau}+c_{\tau}t\right\}\right|\leq\epsilon,\]

which implies

\[\forall\tau\in\left[\frac{1}{n},\frac{1}{2}\right],\quad-a^{*}_{t,\tau}\geq-c _{\tau}t-\epsilon\geq ct-\epsilon,\]

where \(c=\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-c_{\tau}\). Since \(\Phi^{\prime}_{\tau}(0)\) and \(\Phi^{\prime\prime}_{\tau}(0)\) are positive and continuous functions of \(\tau\), this infimum is attained over the compact set \(\left[\frac{1}{n},\frac{1}{2}\right]\), leading to \(c>0\). Since the lower bound holds uniformly over \(\tau\), this shows that for \(t\in[0,t_{1}]\), we have \(\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-a^{*}_{t,\tau}=\Omega(t)\).

Now, since for any \(\tau\in\left[\frac{1}{n},\frac{1}{2}\right]\), \(-a^{*}_{t,\tau}\) is a function of class \(C^{1}\) and thus continuous, its supremum over a compact set, \(\sup_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-a^{*}_{t,\tau}\), is also continuous and is bounded over \([0,t_{1}]\) by some \(a>0\). For \(|u|\leq a\) and \(\tau\in\left[\frac{1}{n},\frac{1}{2}\right]\), we have \(\frac{1}{2}-a\leq\tau+u\leq\frac{1}{2}+a\) and \(\frac{1}{2}-a\leq\tau-u\leq\frac{1}{2}+a\). Since \(\Phi^{\prime\prime}\) is positive and continuous, it reaches its minimum \(C>0\) over the compact set \(\left[\frac{1}{2}-a,\frac{1}{2}+a\right]\). Thus, we can write

\[\forall t\in[0,t_{1}],\forall\tau\in\left[\frac{1}{n},\frac{1}{2 }\right],\quad\mathcal{T}_{\tau}(t) =\int_{0}^{-a^{*}_{t,\tau}}u\left[\frac{1-t}{2}\Phi^{\prime\prime }_{\tau}(-u)+\frac{1+t}{2}\Phi^{\prime\prime}_{\tau}(u)\right]du\] \[\geq\int_{0}^{-a^{*}_{t,\tau}}u\left[\frac{1-t}{2}C+\frac{1+t}{2} C\right]du\] \[=\int_{0}^{-a^{*}_{t,\tau}}Cu\,du=C\frac{(-a^{*}_{t,\tau})^{2}}{2}.\]

Thus, for \(t\leq t_{1}\), we have

\[\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\mathcal{T}_{\tau}(t)\geq C \frac{(\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-a^{*}_{t,\tau})^{2} }{2}\geq\Omega(t^{2}).\]

Similarly, we aim to establish an upper bound for \(\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\mathcal{T}_{\tau}(t)\). We first show the upper bound \(\sup_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-a^{*}_{t,\tau}=O(t)\). Given the equalities (18), it follows that for any \(\tau\), the following holds: \(\lim_{t\to 0}(-a^{*}_{t,\tau}+c_{\tau}t)=0\). For any \(\tau\in\left[\frac{1}{n},\frac{1}{2}\right]\), \(t\mapsto\left(-a^{*}_{t,\tau}+c_{\tau}t\right)\) is a continuous function over \([0,t_{0}]\) since \(a^{*}_{t,\tau}\) is a function of class \(C^{1}\). Since the supremum over a fixed compact set of a family of continuous functions is continuous, \(t\mapsto\sup_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\left\{-a^{*}_{t,\tau }+c_{\tau}t\right\}\) is continuous. Thus, for any \(\epsilon>0\), there exists \(t_{1}>0\), \(t_{1}\leq t_{0}\), such that for any \(t\in[0,t_{1}]\),

\[\left|\sup_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\left\{-a^{*}_{t,\tau}+c _{\tau}t\right\}\right|\leq\epsilon,\]

which implies

\[\forall\tau\in\left[\frac{1}{n},\frac{1}{2}\right],\quad-a^{*}_{t,\tau}\leq-c _{\tau}t+\epsilon\leq ct+\epsilon,\]

where \(c=\sup_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-c_{\tau}\). Since \(\Phi^{\prime}_{\tau}(0)\) and \(\Phi^{\prime\prime}_{\tau}(0)\) are positive and continuous functions of \(\tau\), this supremum is attained over the compact set \(\left[\frac{1}{n},\frac{1}{2}\right]\), leading to \(c>0\). Since the upper bound holds uniformly over \(\tau\), this shows that for \(t\in[0,t_{1}]\), we have \(\sup_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-a^{*}_{t,\tau}=O(t)\).

Now, since for any \(\tau\in\left[\frac{1}{n},\frac{1}{2}\right]\), \(-a^{*}_{t,\tau}\) is a function of class \(C^{1}\) and thus continuous, its supremum over a compact set, \(\sup_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-a^{*}_{t,\tau}\), is also continuous and is bounded over \([0,t_{1}]\) by some \(a>0\). For \(|u|\leq a\) and \(\tau\in\left[\frac{1}{n},\frac{1}{2}\right]\), we have \(\frac{1}{2}-a\leq\tau+u\leq\frac{1}{2}+a\) and \(\frac{1}{2}-a\leq\tau-u\leq\frac{1}{2}+a\). Since \(\Phi^{\prime\prime}\) is positive and continuous, it reaches its maximum \(C>0\) over the compact set \(\left[\frac{1}{2}-a,\frac{1}{2}+a\right]\). Thus, we can write

\[\forall t\in[0,t_{1}],\forall\tau\in\left[\frac{1}{n},\frac{1}{2} \right],\quad\mathcal{T}_{\tau}(t) =\int_{0}^{-a_{t,\tau}^{*}}u\bigg{[}\frac{1-t}{2}\Phi_{\tau}^{ \prime\prime}(-u)+\frac{1+t}{2}\Phi_{\tau}^{\prime\prime}(u)\bigg{]}du\] \[\leq\int_{0}^{-a_{t,\tau}^{*}}u\bigg{[}\frac{1-t}{2}C+\frac{1+t}{2 }C\bigg{]}du\] \[=\int_{0}^{-a_{t,\tau}^{*}}Cu\,du=C\frac{(-a_{t,\tau}^{*})^{2}}{2}.\]

Thus, for \(t\leq t_{1}\), we have

\[\inf_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}\mathcal{T}_{\tau}(t)\leq C \frac{(\sup_{\tau\in\left[\frac{1}{n},\frac{1}{2}\right]}-a_{t,\tau}^{*})^{2} }{2}\leq O(t^{2}).\]

This completes the proof. 

## Appendix M Proof for constrained losses (Theorem 5.5)

**Theorem 5.5** (Upper and lower bound for constrained losses).: _Assume that \(\Phi\) is convex, twice continuously differentiable, and satisfies the properties \(\Phi^{\prime}(u)>0\) and \(\Phi^{\prime\prime}(u)>0\) for any \(u\geq 0\). Then, for any \(A>0\), the following property holds:_

\[\inf_{\tau\in[0,A]}\sup_{u\in\mathbb{R}}\biggl{\{}\Phi(\tau)-\frac{1-t}{2}\Phi (\tau+u)-\frac{1+t}{2}\Phi(\tau-u)\biggr{\}}=\Theta(t^{2}).\]

Proof.: For any \(\tau\in[0,A]\), define the function \(\mathcal{T}_{\tau}\) by

\[\forall t\in[0,1],\quad\mathcal{T}_{\tau}(t) =\sup_{u\in\mathbb{R}}\biggl{\{}\Phi(\tau)-\frac{1-t}{2}\Phi( \tau+u)-\frac{1+t}{2}\Phi(\tau-u)\biggr{\}}\] \[=f_{t,\tau}(0)-\inf_{u\in\mathbb{R}}f_{t,\tau}(u),\]

where

\[f_{t,\tau}(u)=\frac{1-t}{2}\Phi_{\tau}(u)+\frac{1+t}{2}\Phi_{\tau}(-u)\quad \text{and}\quad\Phi_{\tau}(u)=\Phi(\tau+u).\]

We aim to establish a lower bound for \(\inf_{\tau\in[0,A]}\mathcal{T}_{\tau}(t)\). For any fixed \(\tau\in[0,A]\), this situation is parallel to that of binary classification (Theorem 4.1 and Theorem 4.2), since we also have \(\Phi_{\tau}^{\prime}(0)=\Phi^{\prime}(\tau)>0\) and \(\Phi_{\tau}^{\prime\prime}(0)=\Phi^{\prime\prime}(\tau)>0\). Let \(a_{t,\tau}^{*}\) denotes the minimizer of \(f_{t,\tau}\) over \(\mathbb{R}\). By applying Theorem 5.2 to the function \(F\colon(t,u,\tau)\mapsto f_{t,\tau}^{\prime}(u)=\frac{1-t}{2}\Phi_{\tau}^{ \prime}(u)-\frac{1+t}{2}\Phi_{\tau}^{\prime}(-u)\) and the convexity of \(f_{t,\tau}\) with respect to \(u\), \(a_{t,\tau}^{*}\) exists, is unique and is continuously differentiable over \([0,t_{0}]\times[0,A]\), for some \(t_{0}>0\).

Next, we will leverage the proof of Theorem 4.2. Adopting a similar notation, while incorporating the \(\tau\) subscript to distinguish different functions \(\Phi_{\tau}\) and \(f_{t,\tau}\), we can write

\[\forall t\in[0,t_{0}],\quad\mathcal{T}_{\tau}(t)=\int_{0}^{a_{t,\tau}^{*}}u \bigg{[}\frac{1-t}{2}\Phi_{\tau}^{\prime\prime}(u)+\frac{1+t}{2}\Phi_{\tau}^{ \prime\prime}(-u)\bigg{]}du.\]

where \(a_{t,\tau}^{*}\) verifies

\[a_{0,\tau}^{*}=0\quad\text{and}\quad\frac{\partial a_{t,\tau}^{*}}{\partial t }(0)=\frac{\Phi_{\tau}^{\prime}(0)}{\Phi_{\tau}^{\prime\prime}(0)}=c_{\tau}>0.\] (19)

We first show the lower bound \(\inf_{\tau\in[0,A]}a_{t,\tau}^{*}=\Omega(t)\). Given the equalities (19), it follows that for any \(\tau\), the following holds: \(\lim_{t\to 0}\bigl{(}a_{t,\tau}^{*}-c_{\tau}t\bigr{)}=0\). For any \(\tau\in[0,A]\), \(t\mapsto\bigl{(}a_{t,\tau}^{*}-c_{\tau}t\bigr{)}\) is a continuous function over \([0,t_{0}]\) since \(a_{t,\tau}^{*}\) is a function of class \(C^{1}\). Since the infimum over a fixed compact set of a family of continuous functions is continuous, \(t\mapsto\inf_{\tau\in[0,A]}\bigl{\{}a_{t,\tau}^{*}-c_{\tau}t\bigr{\}}\) is continuous. Thus, for any \(\epsilon>0\), there exists \(t_{1}>0\), \(t_{1}\leq t_{0}\), such that for any \(t\in[0,t_{1}]\),

\[\Bigl{|}\inf_{\tau\in[0,A]}\bigl{\{}a_{t,\tau}^{*}-c_{\tau}t\bigr{\}}\Bigr{|} \leq\epsilon,\]

[MISSING_PAGE_FAIL:31]

Analysis of the function of \(\tau\)

Let \(F\) be the function defined by

\[\forall t\in\big{[}0,\tfrac{1}{2}\big{]},\tau\in\mathbb{R},\quad F(t,\tau)= \sup_{u\in\mathbb{R}}\biggl{\{}\Phi(\tau)-\frac{1-t}{2}\Phi(\tau+u)-\frac{1+t}{2 }\Phi(\tau-u)\biggr{\}},\]

where \(\Phi\) is a convex function in \(C^{2}\) with \(\Phi^{\prime},\Phi^{\prime\prime}>0\). In light of the analysis of the previous sections, for any \((\tau,t)\), there exists a unique function \(a_{t,\tau}\) solution of the maximization (supremum in \(F\)), a \(C^{1}\) function over a neighborhood \(U\) of \((\tau,0)\) with \(a_{0,\tau}=0\), \(a_{t,\tau}>0\) for \(t>0\), and \(\frac{\partial a_{t,\tau}}{\partial t}(0,\tau)=\frac{\Phi^{\prime}(\tau)}{ \Phi^{\prime\prime}(\tau)}=c_{\tau}\). Thus, we have \(\lim_{t\to 0}\frac{a_{t,\tau}}{tc_{\tau}}=1\). The optimality of \(a_{t,\tau}\) implies

\[\frac{1-t}{2}\Phi^{\prime}(\tau+a_{t,\tau})=\frac{1+t}{2}\Phi^{\prime}(\tau-a_ {t,\tau}).\]

Thus, the partial derivative of \(F\) over the appropriate neighborhood \(U\) is given by

\[\frac{\partial F}{\partial\tau}(t,\tau) =\Phi^{\prime}(\tau)-\frac{1-t}{2}\Phi^{\prime}(\tau+a_{t,\tau}) \biggl{(}\frac{\partial a_{t,\tau}}{\partial\tau}(t,\tau)+1\biggr{)}-\frac{1 +t}{2}\Phi^{\prime}(\tau-a_{t,\tau})\biggl{(}-\frac{\partial a_{t,\tau}}{ \partial\tau}(t,\tau)+1\biggr{)}\] \[=\Phi^{\prime}(\tau)-\frac{1-t}{2}\Phi^{\prime}(\tau+a_{t,\tau}) \biggl{(}\frac{\partial a_{t,\tau}}{\partial\tau}(t,\tau)+1-\frac{\partial a _{t,\tau}}{\partial\tau}(t,\tau)+1\biggr{)}\] \[=\Phi^{\prime}(\tau)-(1-t)\Phi^{\prime}(\tau+a_{t,\tau}).\]

Since \(\Phi^{\prime}\) is continuous, by the mean value theorem, there exists \(\xi\in(\tau,\tau+a_{t,\tau})\) such that \(\Phi^{\prime}(\tau+a_{t,\tau})-\Phi^{\prime}(\tau)=a_{t,\tau}\Phi^{\prime \prime}(\xi)\). Thus, we can write

\[\frac{\partial F}{\partial\tau}(t,\tau) =\Phi^{\prime}(\tau)-(1-t)\Phi^{\prime}(\tau)-(1-t)a_{t,\tau} \Phi^{\prime\prime}(\xi)\] \[=t\Phi^{\prime}(\tau)-(1-t)a_{t,\tau}\Phi^{\prime\prime}(\xi)\] \[=t\Phi^{\prime}(\tau)\biggl{[}1-(1-t)\frac{a_{t,\tau}}{tc_{\tau}} \frac{\Phi^{\prime\prime}(\xi)}{\Phi^{\prime\prime}(\tau)}\biggr{]}.\]

Note that if \(\Phi^{\prime\prime}\) is locally non-increasing, then we have \(\Phi^{\prime\prime}(\xi)\leq\Phi^{\prime\prime}(\tau)\) and for \(t\) sufficiently small, since \(\Phi^{\prime}\) is increasing and \(\frac{a_{t,\tau}}{tc_{\tau}}\sim 1\):

\[\frac{\partial F}{\partial\tau}(t,\tau)\geq t\Phi^{\prime}(\tau)\biggl{[}1-(1 -t)\frac{a_{t,\tau}}{tc_{\tau}}\biggr{]}\geq 0.\] (20)

In that case, for any \(A>0\), we can find a neighborhood \(\mathcal{O}\) of \(t\) around zero over which \(\frac{\partial F}{\partial\tau}(t,\tau)\) is defined for all \((t,\tau)\in\mathcal{O}\times[0,A]\) and \(\frac{\partial F}{\partial\tau}(t,\tau)\geq 0\). From this, we can conclude that the infimum of \(F\) over \(\tau\in[0,A]\) is reached at zero for \(t\) sufficiently small (\(t\in\mathcal{O}\)).

## Appendix O Generalization bounds

Let \(S=\big{(}(x_{1},y_{1}),\ldots,(x_{m},y_{m})\big{)}\) be a sample drawn from \(\mathcal{D}^{m}\). Denote by \(\widehat{h}_{S}\) an empirical minimizer within \(\mathcal{H}\) for the surrogate loss \(\ell\): \(\widehat{h}_{S}\in\operatorname*{argmin}_{h\in\mathcal{H}}\frac{1}{m}\sum_{i= 1}^{m}\ell(h,x_{i},y_{i})\). Let \(\mathcal{H}_{\ell}\) denote the hypothesis set \(\{(x,y)\mapsto\ell(h,x,y)\colon h\in\mathcal{H}\}\) and \(\mathfrak{R}^{\ell}_{m}(\mathcal{H})\) its Rademacher complexity. We also write \(B_{\ell}\) to denote an upper bound for \(\ell\). Then, given the following \(\mathcal{H}\)-consistency bound:

\[\forall h\in\mathcal{H},\quad\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}^{*}_{ \ell_{0-1}}(\mathcal{H})+\mathcal{M}_{\ell_{0-1}}(\mathcal{H})\leq\Gamma( \mathcal{E}_{\ell}(h)-\mathcal{E}^{*}_{\ell}(\mathcal{H})+\mathcal{M}_{\ell}( \mathcal{H})),\] (21)

for any \(\delta>0\), with probability at least \(1-\delta\) over the draw of an i.i.d. sample \(S\) of size \(m\), the following estimation bound holds for \(\widehat{h}_{S}\):

\[\forall h\in\mathcal{H},\quad\mathcal{E}_{\ell_{0-1}}(h)-\mathcal{E}^{*}_{ \ell_{0-1}}(\mathcal{H})\leq\Gamma\biggl{(}4\mathfrak{R}^{\mathrm{L}}_{m}( \mathcal{H})+2B_{\mathrm{L}}\sqrt{\tfrac{\log\frac{2}{\delta}}{2m}}+\mathcal{M} _{\ell}(\mathcal{H})\biggr{)}-\mathcal{M}_{\ell_{0-1}}(\mathcal{H}).\]

Proof.: By the standard Rademacher complexity bounds (Mohri et al., 2018), for any \(\delta>0\), with probability at least \(1-\delta\), the following holds for all \(h\in\mathcal{H}\):

\[\bigl{|}\mathcal{E}_{\ell}(h)-\widehat{\mathcal{E}}_{\ell,S}(h)\bigr{|}\leq 2 \mathfrak{R}^{\ell}_{m}(\mathcal{H})+B_{\ell}\sqrt{\tfrac{\log(2/\delta)}{2m}}.\]For any \(\epsilon>0\), by definition of the infimum, there exists \(h^{*}\in\mathcal{H}\) such that \(\mathcal{E}_{\ell}(h^{*})\leq\mathcal{E}_{\ell}^{*}(\mathcal{H})+\epsilon\). By the definition of \(\widehat{h}_{S}\), we obtain

\[\mathcal{E}_{\ell}(\widehat{h}_{S})-\mathcal{E}_{\ell}^{*}( \mathcal{H}) =\mathcal{E}_{\ell}(\widehat{h}_{S})-\widehat{\mathcal{E}}_{\ell, S}(\widehat{h}_{S})+\widehat{\mathcal{E}}_{\ell,S}(\widehat{h}_{S})-\mathcal{E}_{ \ell}^{*}(\mathcal{H})\] \[\leq\mathcal{E}_{\ell}(\widehat{h}_{S})-\widehat{\mathcal{E}}_{ \ell,S}(\widehat{h}_{S})+\widehat{\mathcal{E}}_{\ell,S}(h^{*})-\mathcal{E}_{ \ell}^{*}(\mathcal{H})\] \[\leq\mathcal{E}_{\ell}(\widehat{h}_{S})-\widehat{\mathcal{E}}_{ \ell,S}(\widehat{h}_{S})+\widehat{\mathcal{E}}_{\ell,S}(h^{*})-\mathcal{E}_{ \ell}^{*}(h^{*})+\epsilon\] \[\leq 2\bigg{[}2\mathfrak{R}_{m}^{\ell}(\mathcal{H})+B_{\ell} \sqrt{\frac{\log(2/\delta)}{2m}}\bigg{]}+\epsilon.\]

Since the inequality holds for all \(\epsilon>0\), it implies the following:

\[\mathcal{E}_{\ell}(\widehat{h}_{S})-\mathcal{E}_{\ell}^{*}( \mathcal{H})\leq 4\mathfrak{R}_{m}^{\ell}(\mathcal{H})+2B_{\ell}\sqrt{ \frac{\log(2/\delta)}{2m}}.\]

Plugging in this inequality in the \(\mathcal{H}\)-consistency bound (21) completes the proof. 

These bounds for surrogate loss minimizers, expressed in terms of minimizability gaps, offer more detailed and informative insights compared to existing bounds based solely on approximation errors. Our analysis of growth rates suggests that for commonly used smooth loss functions, \(\Gamma\) varies near zero with a square-root dependency. Furthermore, this dependency cannot be generally improved for arbitrary distributions.

## Appendix P Future work

We demonstrated a universal square-root growth rate for smooth surrogate losses commonly used in binary and multi-class classification. This result holds across all data distributions. A promising direction for future research would be to further investigate how incorporating specific distributional assumptions could refine these results.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See the section "Our results" in the introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Appendix P. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: See Section 3, Section 4, Section 5, Section 6, and Appendix.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [NA] Justification: The paper does not include experiments requiring code. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The authors have reviewed the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work is theoretical in nature and we do not anticipate any immediate negative societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.