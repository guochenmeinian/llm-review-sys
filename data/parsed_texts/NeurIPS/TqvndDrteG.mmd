# Computation-Aware Robust Gaussian Processes

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Gaussian Processes (GPs) are flexible nonparametric statistical models equipped with principled uncertainty quantification for both noise and model uncertainty. However, their cubic inference complexity requires them to be combined with approximation techniques when applied to large datasets. Recent work demonstrated that such approximations introduce an additional source of uncertainty, _computational uncertainty_, and that the latter could be quantified, leading to the _computation-aware_ GP, also known as IterGP. In this short communication, we demonstrate that IterGP is not "robust", in the sense that a quantity of interest, the posterior influence function, is not bounded. Subsequently, drawing inspiration from recent work on Robust Conjugate GPs, we introduce a novel class of GPs: IterRCGPs. We carry out a number of theoretical analyses, demonstrating the robustness of IterRCGPs among other things.

## 1 Introduction

Gaussian Processes (GPs, Rasmussen and Williams (2006)) are a class of probabilistic models enjoying many properties such as universal approximation or closed-form computations. Due to their principled uncertainty quantification, they are becoming increasingly popular when applied in high-stakes domains like medical datasets (Cheng _et al._, 2019; Chen _et al._, 2023) or used as a surrogate model in Bayesian Optimization (Garnett, 2023). This being said, GPs suffer from a cubic inference complexity, hindering their use on large datasets. As a remedy, approximation techniques like Sparse Variational Gaussian Processes (Titsias, 2009) or the Nystrom approximation are often used (Williams and Seeger, 2000; Wild _et al._, 2023).

These approximations introduce bias in uncertainty quantification, which, as recently demonstrated, can be quantified and combined with mathematical uncertainty, leading to the development of _computation-aware_ GPs (Wenger _et al._, 2022), also known as IterGPs. This combined uncertainty is shown to be the correct measure for capturing overall uncertainty, as limited computation introduces computational error. While this analysis applies to standard GPs, many practical applications require variations, e.g., to deal with heteroscedasticity or outliers.

Recent work by Altamirano _et al._ (2024) introduced the robust conjugate GP (RCGP), which unifies three classes of GPs. RCGP retains conjugacy, enabling a closed-form posterior while exhibiting a robustness property. However, like standard GPs, RCGP faces significant inference complexity, necessitating approximation methods such as sparse variational RCGP, and therefore suggesting the use of the framework developed by Wenger _et al._ (2022).

Contributions.Our work can be seen as bridging the gap between computation-aware GPs and Robust Conjugate GPs. As such, our contributions are mainly theoretical and can be summarized as follows:* We present IterRCGP, a novel computation-aware Gaussian Process (GP) framework that extends IterGPs by accommodating a broader range of observation noise models.
* We demonstrate that IterRCGP inherits the robustness properties characteristic of RCGP.
* We establish that IterRCGP exhibits convergence behavior and worst-case errors analogous to IterGP.

## 2 Preliminaries

We first introduce notations for GP regression Rasmussen and Williams (2006). Let \(\mathcal{D}=\{(\mathbf{x}_{1},y_{1}),\dots,(\mathbf{x}_{n},y_{n})\}\) be a dataset, with \((\mathbf{x}_{j},y_{j})\in\mathbb{R}^{d}\times\mathbb{R}\) such that \(y_{j}=f(\mathbf{x}_{j})+\epsilon\) and \(\epsilon\sim\mathcal{N}(0,\sigma_{\text{noise}}^{2})\) for all \(j\). The latent function \(f\) is modeled with a GP prior:

\[f(\mathbf{x})\sim\mathcal{GP}(m(\mathbf{x}),k(\mathbf{x},\mathbf{x}^{\prime})).\] (1)

This defines a distribution over functions \(f\) whose mean is \(\mathbb{E}[f(\mathbf{x})]=m(\mathbf{x})\) and covariance \(\text{cov}[f(\mathbf{x}),f(\mathbf{x}^{\prime})]=k(\mathbf{x},\mathbf{x}^{ \prime})\). \(k\) is a kernel function measuring the similarity between inputs. For any finite-dimensional collection of inputs \(\{\mathbf{x}_{1},\dots,\mathbf{x}_{n}\}\), the function values \(\mathbf{f}=[f(\mathbf{x}_{1}),\dots,f(\mathbf{x}_{n})]^{\top}\in\mathbb{R}^{n}\) follow a multivariate normal distribution \(\mathbf{f}\sim\mathcal{N}(\mathbf{m},\mathbf{K})\), where \(\mathbf{m}=[m(\mathbf{x}_{1}),\dots,m(\mathbf{x}_{n})]^{\top}\) and \(\mathbf{K}\in\mathbb{R}^{n\times n}=[k(\mathbf{x}_{j},\mathbf{x}_{l})]_{1\leq j,l\leq n}\) is the kernel matrix.

Given \(\mathcal{D}\), the posterior predictive distribution \(p(f(\mathbf{x})\mid\mathcal{D})\) is Gaussian for all \(\mathbf{x}\) with mean \(\mu_{*}(\mathbf{x})\) and variance \(k_{*}(\mathbf{x},\mathbf{x})\), such that

\[\mu_{*}(\mathbf{x}) =m(\mathbf{x})+\mathbf{k}_{\mathbf{x}}^{\top}(\mathbf{K}+\sigma_ {\text{noise}}^{2}\mathbf{I})^{-1}(\mathbf{y}-\mathbf{m}),\] \[k_{*}(\mathbf{x},\mathbf{x}) =k(\mathbf{x},\mathbf{x})-\mathbf{k}_{\mathbf{x}}^{\top}(\mathbf{ K}+\sigma_{\text{noise}}^{2}\mathbf{I})^{-1}\mathbf{k}_{\mathbf{x}},\]

where \(\mathbf{y}=[y_{1},\dots,y_{n}]\in\mathbb{R}^{n}\) and \(\mathbf{k}_{\mathbf{x}}=[k(\mathbf{x},\mathbf{x}_{1}),\cdots,k(\mathbf{x}, \mathbf{x}_{n})]^{\top}\in\mathbb{R}^{n}\).

Next, we introduce an extension of GPs: Robust Conjugate Gaussian Processes (RCGPs).

**Robust conjugate Gaussian process.** RCGP follows the generalized Bayesian inference framework, substituting the classical likelihood with the loss function \(L_{n}^{w}\)Altamirano _et al._ (2024) defined as

\[L_{n}^{w}(\mathbf{f},\mathbf{x},\mathbf{y})=\frac{1}{n}\left(\sum_{j=1}^{n}w^ {2}(\mathbf{x}_{j},y_{j})s_{\text{model}}^{2}(\mathbf{x}_{j},y_{j})+2\nabla_{ y}[w^{2}(\mathbf{x}_{j},y_{j})s_{\text{model}}(\mathbf{x}_{j},y_{j})]\right),\] (2)

where \(s_{\text{model}}(\mathbf{x},y)=\sigma_{\text{noise}}^{-2}(f(\mathbf{x})-y), \sigma_{\text{noise}}^{2}>0\). The core component of \(L_{n}^{w}\) is the weighting function \(w\), which depends on \(\mathbf{x}\) and \(y\). Altamirano _et al._ (2024)[Table 1] provides three weighting functions corresponding to homoscedastic, heteroscedastic, and outliers-robust GPs. Building on \(L_{n}^{w}\), the authors further define the RCGP's predictive posterior distribution \(p^{w}(f(\mathbf{x})|\mathcal{D})\) as follows:

\[\hat{\mu}_{*}(\mathbf{x})=m(\mathbf{x})+\mathbf{k}_{\mathbf{x}}^{\top} \overbrace{(\mathbf{K}+\sigma_{\text{noise}}^{2}\mathbf{J}_{\mathbf{w}})^{-1} (\mathbf{y}-\mathbf{m}_{\mathbf{w}})}^{\hat{\mathbf{y}}}\] (3) \[\hat{k}_{*}(\mathbf{x},\mathbf{x})=k(\mathbf{x},\mathbf{x})- \mathbf{k}_{\mathbf{x}}^{\top}\hat{\mathbf{K}}^{-1}\mathbf{k}_{\mathbf{x}}\] (4)

for \(\mathbf{w}=(w(\mathbf{x}_{1},y_{1}),\dots,w(\mathbf{x}_{n},y_{n}))^{\top}\), \(\tilde{\mathbf{K}}=\mathbf{K}+\sigma_{\text{noise}}^{2}\mathbf{J}_{\mathbf{w}}\), \(\mathbf{m}_{\mathbf{w}}=\mathbf{m}+\sigma_{\text{noise}}^{2}\nabla_{y}\log( \mathbf{w}^{2})\), and \(\mathbf{J}_{\mathbf{w}}=\text{diag}(\frac{\sigma_{\text{noise}}^{2}}{\mathbf{ w}}^{-2})\). A key advantage of RCGP is its robustness to outliers and non-Gaussian errors. While vanilla GPs exhibit an unbounded posterior influence function, RCGP, under certain conditions, maintains a bounded posterior influence function Altamirano _et al._ (2024)[Proposition 3.2].

## 3 Computation-aware RCGPs

In the same spirit of Wenger _et al._ (2022), we treat the representer weights \(\hat{\mathbf{v}}\) introduced in Equation 3 as a random variable with the prior \(p(\hat{\mathbf{v}})=\mathcal{N}(\hat{\mathbf{v}};\mathbf{0},\tilde{\mathbf{K}} ^{-1})\). We then update \(p(\hat{\mathbf{v}})\) by iteratively applying the tractable matrix-vector multiplication. For a particular iteration \(i\in\{0,\ldots,n\}\), we have the current belief distribution \(p(\hat{\mathbf{v}})=\mathcal{N}(\hat{\mathbf{v}};\tilde{\mathbf{v}}_{i},\tilde{ \boldsymbol{\Sigma}}_{i})\) where

\[\tilde{\mathbf{v}}_{i} =\tilde{\mathbf{v}}_{i-1}+\tilde{\boldsymbol{\Sigma}}_{i-1} \tilde{\mathbf{K}}\mathbf{s}_{i}(\mathbf{s}_{i}^{\top}\tilde{\mathbf{K}} \tilde{\boldsymbol{\Sigma}}_{i-1}\tilde{\mathbf{K}}\mathbf{s}_{i})^{-1}\tilde{ \alpha}_{i}=\tilde{\mathbf{C}}_{i}(\mathbf{y}-\mathbf{m}_{\mathbf{w}})\] (5) \[\tilde{\boldsymbol{\Sigma}}_{i} =\tilde{\boldsymbol{\Sigma}}_{i-1}-\tilde{\boldsymbol{\Sigma}}_{ i-1}\tilde{\mathbf{K}}\mathbf{s}_{i}(\mathbf{s}_{i}^{\top}\tilde{\mathbf{K}} \tilde{\boldsymbol{\Sigma}}_{i-1}\tilde{\mathbf{K}}\mathbf{s}_{i})^{-1}\mathbf{ s}_{i}^{\top}\tilde{\mathbf{K}}\tilde{\boldsymbol{\Sigma}}_{i-1}\] (6) \[\tilde{\alpha}_{i} =\mathbf{s}_{i}^{\top}\underbrace{\tilde{\mathbf{K}}(\hat{ \mathbf{v}}-\tilde{\mathbf{v}}_{i-1})}_{\mathbf{r}_{i-1}}\] (7) \[\tilde{\mathbf{C}}_{i} =\tilde{\mathbf{K}}^{-1}-\tilde{\boldsymbol{\Sigma}}_{i}\] (8)

Here, \(\mathbf{s}_{i}\) denotes the policy corresponding to a specific approximation method (Wenger _et al._, 2022)[Table 1]. This policy serves as the projection of the residual \(\mathbf{r}_{i-1}\) results in \(\alpha_{i}\). The belief regarding the representer weights encodes the computational error as an added source of uncertainty, which can be integrated with the inherent uncertainty of the mathematical posterior.

We obtain the predictive posterior of IterRCGP by integrating out the representer weights: \(p(f(\mathbf{x})|\mathcal{D})=\int p(f(\mathbf{x})|\hat{\mathbf{v}})p(\hat{ \mathbf{v}})d\hat{\mathbf{v}}=\mathcal{N}(\mathbf{f};\hat{\mu}_{i}(\mathbf{x} ),\hat{k}_{i}(\mathbf{x},\mathbf{x}))\) where

\[\hat{\mu}_{i}(\mathbf{x})=m(\mathbf{x})+\mathbf{k}_{\mathbf{x}}^ {\top}\tilde{\mathbf{v}}_{i}\] (9) \[\hat{k}_{i}(\mathbf{x},\mathbf{x})=k(\mathbf{x},\mathbf{x})- \mathbf{k}_{\mathbf{x}}^{\top}\tilde{\mathbf{K}}^{-1}\mathbf{k}_{\mathbf{x}}+ \underbrace{\mathbf{k}_{\mathbf{x}}^{\top}\tilde{\boldsymbol{\Sigma}}_{i} \mathbf{k}_{\mathbf{x}}}_{k_{i}^{\text{temp}}(\mathbf{x},\mathbf{x})}=k( \mathbf{x},\mathbf{x})-\underbrace{\mathbf{k}_{\mathbf{x}}^{\top}\tilde{ \mathbf{C}}_{i}\mathbf{k}_{\mathbf{x}}}_{\text{combined uncertainty}}\] (10)

IterRCGP follows (Algortijn, 1981) from Wenger _et al._ (2022) to compute an estimate weights \(\tilde{\mathbf{v}}_{i}\) and the rank-\(i\) precision matrix approximation \(\tilde{\mathbf{C}}_{i}\).

## 4 Theoretical results

In this section, we present the theoretical properties of IterRCGP, building upon the IterGP framework and the RCGP class. Our theoretical analysis primarily aims to establish the following key results:

* Robustness property of IterGP and IterRCGP (Proposition 1).
* Convergence of IterRCGP's posterior mean in reproducing kernel Hilbert space (RKHS) norm (Proposition 2) and pointwise (Corollary 4).
* Combined uncertainty of IterRCGP is a tight worst-case bound on the relative distance to all potential latent functions shifted by the function \(\mathbf{m}_{\mathbf{w}}\) consistent with computational observations, similar to its IterGP counterpart (Proposition 3).

We establish the robustness properties of IterGP and IterRCGP using the Posterior Influence Function (PIF) as the robustness criterion. Appendix 1 provides a detailed definition of PIF. The proposition presented below is closely related to Altamirano _et al._ (2024)[Proposition 3.2].

**Proposition 1**.: _(Robustness property) Suppose \(f\sim\mathcal{GP}(m,k)\), \(\varepsilon\sim\mathcal{N}(\mathbf{0},\sigma_{\mathrm{noise}}^{2}\mathbf{I})\) and let \(C_{k}^{\prime}\in\mathbb{R};k=1,2,3\) be constants independent of \(y_{m}^{c}\). For any given iteration \(i\in\{0,\ldots,n\}\), IterGP regression has the PIF_

\[\mathrm{PIF}_{\mathrm{IterGP}}(y_{m}^{c},\mathcal{D},i)=C_{1}^{\prime}(y_{m}- y_{m}^{c})^{2}\] (11)

_which is not robust: \(\mathrm{PIF}_{\mathrm{IterGP}}(y_{m}^{c},\mathcal{D},i)\to\infty\) as \(|y_{m}^{c}|\to\infty\). In contrast, for the IterRCGP with \(\sup_{\mathbf{x},y}w(\mathbf{x},y)<\infty\),_

\[\mathrm{PIF}_{\mathrm{IterRCGP}}(y_{m}^{c},\mathcal{D},i)=C_{2}^{\prime}(w(x_{ n},y_{n}^{c})^{2}y_{n}^{c})^{2}+C_{3}^{\prime}\] (12)

_Therefore, if \(\sup_{\mathbf{x},y}w(\mathbf{x},y)^{2}<\infty\), IterRCGP regression is robust since \(\sup_{y_{m}^{c}}|\mathrm{PIF}_{\mathrm{IterRCGP}}(y_{m}^{c},\mathcal{D},i)|<\infty\)._

The proposition demonstrates that IterGP and IterRCGP inherit the same robustness properties as their respective counterparts, GP and RCGP. Specifically, the condition \(\sup_{\mathbf{x},y}w(\mathbf{x},y)<\infty\) ensures each observation has a finite weight, which is the key factor underpinning robustness.

The following proposition is analogous to (Theorem 1) in Wenger _et al._ (2022).

**Proposition 2**.: _(Convergence in RKHS norm of the robust posterior mean approximation) Let \(\mathcal{H}_{k}\) be the RKHS w.r.t. kernel \(k\), \(\sigma^{2}_{\mathrm{noise}}>0\) and let \(\hat{\bm{\mu}}_{*}-\mathbf{m}\in\mathcal{H}_{k}\) be the unique solution to following minimization problem_

\[\operatorname{argmin}_{f\in\mathcal{H}_{k}}L_{n}^{w}(\mathbf{f}, \mathbf{x},\mathbf{y})+\frac{1}{2n}\|\mathbf{f}\|_{\mathcal{H}_{k}}^{2}\] (13)

_which is equivalent to the mathematical RCGP mean posterior shifted by prior mean \(\mathbf{m}\). Then for \(i\in\{0,\dots,n\}\) the IterRCGP posterior mean \(\hat{\bm{\mu}}_{i}\) satisfies:_

\[\|\hat{\bm{\mu}}_{*}-\hat{\bm{\mu}}_{i}\|_{\mathcal{H}_{k}}\leq \hat{\rho}(i)c(\mathbf{J}_{\mathbf{w}})\|\hat{\bm{\mu}}_{*}-\mathbf{m}\|_{ \mathcal{H}_{k}}\] (14)

_where \(\hat{\rho}\) is the relative bound errors corresponding to the number of iterations \(i\) and the constant \(c(\mathbf{J}_{\mathbf{w}})=\sqrt{1+\frac{\lambda_{\max}(\mathbf{J}_{\mathbf{ w}})}{\lambda_{\min}(\mathbf{K})}}\to 1\) as \(\lambda_{\max}(\mathbf{J}_{\mathbf{w}})\to 0\)._

Appendix B provides more details about the relative bound errors. Proposition 2 provides a bound on the RKHS-norm error between the posterior mean of IterRCGP and the mathematical posterior mean of RCGP.

The final proposition parallels [Theorem 2] in Wenger _et al._ (2022), demonstrating that the combined uncertainty is a tight bound for all functions \(g\) that could have yielded the same computational outcomes.

**Proposition 3**.: _(Combined and computational uncertainty as worst-case errors) Let \(\sigma^{2}_{\mathrm{noise}}\geq 0\) and \(\hat{k}_{i}(\cdot,\cdot)=\hat{k}_{*}(\cdot,\cdot)+k_{i}^{\text{comp.}}(\cdot,\cdot)\) be the combined uncertainty of IterRCGP. Furthermore, let \(\mathbf{g}=[g(\mathbf{x}_{1}),\cdots,g(\mathbf{x}_{n})]\in\mathbb{R}^{n}\). Then, for any new \(\mathbf{x}\in\mathcal{X}\) we have_

\[\sup_{\|g-m_{w}\|_{\mathcal{H}_{k}\sigma_{w}}\leq 1} \frac{g(\mathbf{x})-\hat{\mu}^{g}(\mathbf{x})}{\text{math. err.}}+ \underbrace{\hat{\mu}^{g}(\mathbf{x})-\hat{\mu}^{g}_{i}(\mathbf{x})}_{\text{ comp. err.}}=\sqrt{\hat{k}_{i}(\mathbf{x},\mathbf{x})+\sigma^{2}_{\mathrm{noise}}}\] (15) \[\sup_{\|g-m_{w}\|_{\mathcal{H}_{k}\sigma_{w}}\leq 1} \underbrace{\hat{\mu}^{g}(\mathbf{x})-\hat{\mu}^{g}_{i}(\mathbf{x})}_ {\text{comp. err.}}=\sqrt{k_{i}^{\text{comp.}}(\mathbf{x},\mathbf{x})}\] (16)

_where \(\hat{\mu}^{g}(\cdot)=k(\cdot,\mathbf{X})\tilde{\mathbf{K}}^{-1}(\mathbf{g}- \mathbf{m}_{\mathbf{w}})\) is the RCGP's posterior and \(\hat{\mu}^{g}_{i}(\cdot)=k(\cdot,\mathbf{X})\tilde{\mathbf{C}}_{i}(\mathbf{ g}-\mathbf{m}_{\mathbf{w}})\) IterRCGP's posterior mean for the latent function \(g\) and the function \(m_{w}\) lies in \(\mathcal{H}_{k^{\sigma_{w}}}\)._

The consequence of Proposition 3 is then formalized through the following corollary:

**Corollary 4**.: _(Pointwise convergence of robust posterior mean) Assume the conditions of Proposition 3 hold and assume the latent function \(g\in\mathcal{H}_{k^{\sigma_{w}}}\). Let \(\hat{\bm{\mu}}\) be the corresponding mathematical RCGP posterior mean and \(\hat{\bm{\mu}}_{i}\) the IterRCGP posterior mean. It holds that_

\[\frac{|g(\mathbf{x})-\hat{\mu}_{i}(\mathbf{x})|}{\|g\|_{\mathcal{ H}_{k^{\sigma_{w}}}}}\leq\sqrt{\hat{k}_{i}(\mathbf{x},\mathbf{x})+\sigma^{2}_{ \mathrm{noise}}}\] (17) \[\frac{\hat{\mu}(\mathbf{x})-\hat{\mu}_{i}(\mathbf{x})}{\|g\|_{ \mathcal{H}_{k^{\sigma_{w}}}}}\leq\sqrt{k_{i}^{\text{comp.}}(\mathbf{x}, \mathbf{x})}\] (18)

## 5 Conclusion

In this paper, we demonstrated that computation-aware GPs as presented by Wenger _et al._ (2022) lack robustness in the PIF sense. Subsequently, we introduced Iter RCGPs, a novel class of provably robust computation-aware GPs. Since our work mainly involves theoretical analyses, our immediate perspective is to run numerical experiments using synthetic and real-world datasets. Next, one interesting avenue for applying Iter RCGPs is that of Bayesian Optimization (BO), a domain where uncertainty quantification is key to coming up with good exploration policies.

Indeed, the issue of refined uncertainty quantification has recently gained attention in BO. One approach addresses this by jointly optimizing the selection of the optimal data point along with the SVGP parameters and the locations of the inducing points (Maus _et al._, 2024). Another study incorporates conformal prediction into BO by leveraging the conformal Bayes posterior and proposing generalized versions of the corresponding BO acquisition functions (Stanton _et al._, 2023).

## References

* Altamirano et al. (2024) Altamirano, M., Briol, F.-X., and Knoblauch, J. (2024). Robust and conjugate gaussian process regression. In _The 41st International Conference on Machine Learning_.
* Chen et al. (2023) Chen, Y., Prati, A., Montgomery, J., and Garnett, R. (2023). A multi-task gaussian process model for inferring time-varying treatment effects in panel data. In _Proceedings of The 26th International Conference on Artificial Intelligence and Statistics_.
* Cheng et al. (2019) Cheng, L., Ramchandran, S., Vatanen, T., Lietzen, N., Lahesmaa, R., Vehtari, A., and Lahdesmaki, H. (2019). An additive gaussian process regression model for interpretable non-parametric analysis of longitudinal data. _Nature Communications_.
* Garnett (2023) Garnett, R. (2023). _Bayesian Optimization_. Cambridge University Press.
* Kanagawa et al. (2018) Kanagawa, M., Hennig, P., Sejdinovic, D., and Sriperumbudur, B. K. (2018). Gaussian processes and kernel methods: A review on connections and equivalences. _arXiv preprint arXiv:1807.02582_.
* Maus et al. (2024) Maus, N., Kim, K., Pleiss, G., Eriksson, D., Cunningham, J. P., and Gardner, J. R. (2024). Approximation-aware bayesian optimization.
* Rasmussen and Williams (2006) Rasmussen, C. and Williams, C. (2006). _Gaussian Processes for Machine Learning_. MIT Press.
* Scholkopf et al. (2001) Scholkopf, B., Herbrich, R., and Smola, A. J. (2001). A generalized representer theorem. In _International conference on computational learning theory_, pages 416-426. Springer.
* Stanton et al. (2023) Stanton, S., Maddox, W., and Wilson, A. G. (2023). Bayesian optimization with conformal prediction sets. In _International Conference on Artificial Intelligence and Statistics_.
* Titsias (2009) Titsias, M. (2009). Variational learning of inducing variables in sparse gaussian processes. In _Artificial intelligence and statistics_.
* Wenger et al. (2022) Wenger, J., Pleiss, G., Pfortner, M., Hennig, P., and Cunningham, J. P. (2022). Posterior and computational uncertainty in gaussian processes. In _Advances in Neural Information Processing Systems_.
* Wild et al. (2023) Wild, V., Kanagawa, M., and Sejdinovic, D. (2023). Connections and equivalences between the nystrom method and sparse variational gaussian processes.
* Williams and Seeger (2000) Williams, C. and Seeger, M. (2000). Using the nystrom method to speed up kernel machines. _Advances in neural information processing systems_.

## Appendix A Proof of Proposition 1

**Posterior influence function.** Given the dataset \(\mathcal{D}=\{(\mathbf{x}_{j},y_{j})\}_{j=1}^{n}\), we define the contamination of \(\mathcal{D}\) indexed by \(m\in\{1,\dots,n\}\) as \(\mathcal{D}_{m}^{c}=(\mathcal{D}\setminus(\mathbf{x}_{m},y_{m}))\cup(\mathbf{ x}_{m},y_{m}^{c})\). PIF in general, aims to measure the impact of \(y_{m}^{c}\) on inference through the divergence between the contaminated and uncontaminated posteriors \(p(\mathbf{f}|\mathcal{D}_{m}^{c})\) and \(p(\mathbf{f}|\mathcal{D})\):

\[\mathrm{PIF}(y_{m}^{c},\mathcal{D})=\mathrm{KL}(p(\mathbf{f}|\mathcal{D})\|p( \mathbf{f}|\mathcal{D}_{m}^{c}))\] (S1)

where we call a posterior robust if \(\sup_{y\in\mathcal{V}}|\mathrm{PIF}(y_{m}^{c},\mathcal{D})|<\infty\).

We then establish the following lemma to prove Proposition 1.

**Lemma 5**.: _For an arbitrary matrice \(\hat{\mathbf{S}}\in\mathbb{R}^{m\times n}\) and positive semidefinite matrice \(\hat{\mathbf{B}}\in\mathbb{R}^{n\times n}\), we have that_

\[\mathrm{Tr}((\hat{\mathbf{S}}\hat{\mathbf{B}}\hat{\mathbf{S}}^{\top})^{-1})= \hat{\mathbf{S}}^{+\top}\hat{\mathbf{B}}^{-1/2}\hat{\mathbf{G}}\hat{\mathbf{B }}^{-1/2}\hat{\mathbf{S}}^{+}\] (S2)

_where we define \(\hat{\mathbf{G}}=\mathbf{I}-\hat{\mathbf{B}}^{-1/2}(\mathbf{I}-\hat{\mathbf{S }}^{+}\hat{\mathbf{S}})(\hat{\mathbf{B}}^{-1/2}(\mathbf{I}-\hat{\mathbf{S}}^{+ }\hat{\mathbf{S}}))^{+}\) and \({}^{+}\) denotes the Moore-Penrose inverse._

Proof.: The whole proof is derived from an answer to a question posted on the Mathematics Stack Exchange Forums, which we write here for conciseness.

Denote \(\hat{\mathbf{O}}=\mathbf{I}-\hat{\mathbf{S}}^{+}\hat{\mathbf{S}}\) and \(\mathbf{H}(\alpha)=(\hat{\mathbf{S}}(\alpha\mathbf{I}+\hat{\mathbf{B}}^{-1})^ {-1}\hat{\mathbf{S}}^{\top})^{-1}\). Note that

\[(\hat{\mathbf{S}}\hat{\mathbf{B}}\hat{\mathbf{S}}^{\top})^{-1}=\lim_{\alpha \to 0}\mathbf{H}(\alpha)\] (S3)

By applying Woodbury matrix identity, we can rewrite \(\mathbf{H}(\alpha)\) as follows:

\[\mathbf{H}(\alpha)=\left(\frac{1}{\alpha}\hat{\mathbf{S}}\hat{\mathbf{S}}^{ \top}-\frac{1}{\alpha}\hat{\mathbf{S}}\hat{\mathbf{B}}^{-1/2}\left(\mathbf{I} +\frac{1}{\alpha}\hat{\mathbf{B}}^{-1}\right)^{-1}\frac{1}{\alpha}\hat{ \mathbf{B}}^{-1/2}\hat{\mathbf{S}}^{\top}\right)^{-1}\] (S4)

Since \(\hat{\mathbf{S}}\hat{\mathbf{S}}^{\top}\) is invertible, we can apply the Woodbury matrix identity for the second time to obtain

\[\mathbf{H}(\alpha)=\alpha(\hat{\mathbf{S}}\hat{\mathbf{S}}^{\top })^{-1}-(\hat{\mathbf{S}}\hat{\mathbf{S}}^{\top})^{-1}\hat{\mathbf{S}}\hat{ \mathbf{B}}^{-1/2}\] \[\quad(-(\mathbf{I}+\frac{1}{\alpha}\hat{\mathbf{B}}^{-1})+\frac{ 1}{\alpha}\hat{\mathbf{B}}^{-1/2}\hat{\mathbf{S}}^{\top}(\hat{\mathbf{S}}\hat {\mathbf{S}}^{\top})^{-1}\hat{\mathbf{S}}\hat{\mathbf{B}}^{-1/2})^{-1}\hat{ \mathbf{B}}^{-1/2}\hat{\mathbf{S}}^{\top}(\hat{\mathbf{S}}\hat{\mathbf{S}}^{ \top})^{-1}\] (S5) \[=\alpha(\hat{\mathbf{S}}\hat{\mathbf{S}}^{\top})^{-1}+(\hat{ \mathbf{S}}\hat{\mathbf{S}}^{\top})^{-1}\hat{\mathbf{S}}\hat{\mathbf{B}}^{-1/2 }(\mathbf{I}+\frac{1}{\alpha}\hat{\mathbf{B}}^{-1/2}(\mathbf{I}-\hat{\mathbf{ S}}^{\top}(\hat{\mathbf{S}}\hat{\mathbf{S}}^{\top})^{-1}\hat{\mathbf{S}})\hat{ \mathbf{B}}^{-1/2})^{-1}\] \[\quad\hat{\mathbf{B}}^{-1/2}\hat{\mathbf{S}}^{\top}(\hat{ \mathbf{S}}\hat{\mathbf{S}}^{\top})^{-1}\] (S6)

We note that

\[\hat{\mathbf{S}}^{\top}(\hat{\mathbf{S}}\hat{\mathbf{S}}^{\top})^{- 1}=\hat{\mathbf{S}}^{+}\] (S7) \[\mathbf{I}-\hat{\mathbf{S}}^{\top}(\hat{\mathbf{S}}\hat{\mathbf{S} }^{\top})^{-1}\hat{\mathbf{S}}=\hat{\mathbf{O}}\] (S8)

Then, we rewrite \(\mathbf{H}(\alpha)\) as follows:\[\mathbf{H}(\alpha)=\alpha(\hat{\mathbf{S}}\hat{\mathbf{S}}^{\top})^{-1}+\hat{ \mathbf{S}}^{+\top}\hat{\mathbf{B}}^{-1/2}\left(\mathbf{I}+\frac{1}{\alpha}\hat{ \mathbf{B}}^{-1/2}\hat{\mathbf{O}}\hat{\mathbf{O}}\hat{\mathbf{B}}^{-1/2} \right)^{-1}\hat{\mathbf{B}}^{-1/2}\hat{\mathbf{S}}^{+}\] (S9)

Applying the Woodbury matrix identity for the third time provides

\[\mathbf{H}(\alpha)=\alpha(\hat{\mathbf{S}}\hat{\mathbf{S}}^{\top})^{-1}+\hat{ \mathbf{S}}^{+\top}\hat{\mathbf{B}}^{-1/2}(\mathbf{I}-\hat{\mathbf{B}}^{-1/2} \hat{\mathbf{O}}(\alpha\mathbf{I}+\hat{\mathbf{O}}\hat{\mathbf{B}}^{-1}\hat{ \mathbf{O}})^{-1}\hat{\mathbf{O}}\hat{\mathbf{B}}^{-1/2})\hat{\mathbf{B}}^{-1 /2}\hat{\mathbf{S}}^{+}\] (S10)

Since the Moore-Penrose inverse of a matrice \(\mathbf{A}\) is a limit:

\[\mathbf{A}^{+}=\lim_{\alpha\to 0}(\mathbf{A}^{\top}\mathbf{A}+\alpha \mathbf{I})^{-1}\mathbf{A}^{\top}=\lim_{\alpha\to 0}\mathbf{A}^{\top}( \mathbf{A}\mathbf{A}^{\top}+\alpha\mathbf{I})^{-1}\] (S11)

We can take the limit of \(\mathbf{H}(\alpha)\) as \(\alpha\to 0\) and apply the limit relation above to obtain the following result:

\[(\hat{\mathbf{S}}\hat{\mathbf{B}}\hat{\mathbf{S}}^{\top})^{-1}=\hat{\mathbf{ S}}^{+\top}\hat{\mathbf{B}}^{-1/2}\underbrace{(\mathbf{I}-\hat{\mathbf{B}}^{-1/2} \hat{\mathbf{O}}(\hat{\mathbf{B}}^{-1/2}\hat{\mathbf{O}})^{+})}_{\hat{\mathbf{ G}}}\hat{\mathbf{B}}^{-1/2}\hat{\mathbf{S}}^{+}\] (S12)

**PIF for the IterGP.** IterGP regression has the PIF for some constant \(C_{1}^{\prime}\in\mathbb{R}\).

\[\mathrm{PIF}_{\mathrm{IterGP}}(y_{m}^{c},\mathcal{D},i)=C_{1}^{\prime}(y_{m}-y _{m}^{c})^{2}\] (S13)

and is not robust: \(\mathrm{PIF}_{\mathrm{IterGP}}(y_{m}^{c},\mathcal{D},i)\rightarrow\infty\) as \(|y_{m}^{c}|\rightarrow\infty\).

Proof.: Let \(p(\mathbf{f}|\mathcal{D})=\mathcal{N}(\mathbf{f};\boldsymbol{\mu}_{i},\mathbf{ K}_{i})\) and \(p(\mathbf{f}|\mathcal{D}_{m}^{c})=\mathcal{N}(\mathbf{f};\boldsymbol{\mu}_{i}^{c}, \mathbf{K}_{i}^{c})\) be the uncontaminated and contaminated computation-aware GP, respectively. Here,

\[\boldsymbol{\mu}_{i} =\mathbf{m}+\mathbf{K}\mathbf{v}_{i}\] (S14) \[\mathbf{K}_{i} =\mathbf{K}\mathbf{C}_{i}\sigma_{\mathrm{noise}}^{2}\mathbf{I}_{n}\] (S15) \[\boldsymbol{\mu}_{i}^{c} =\mathbf{m}+\mathbf{K}\mathbf{v}_{i}^{c}\] (S16) \[\mathbf{K}_{i}^{c} =\mathbf{K}\mathbf{C}_{i}\sigma_{\mathrm{noise}}^{2}\mathbf{I}_{n}\] (S17)

Note that both \(\mathbf{K}_{i}\) and \(\mathbf{K}_{i}^{c}\) share the same matrice \(\mathbf{C}_{i}\). Then, the PIF has the following form:

\[\mathrm{PIF}_{\mathrm{IterGP}}(y_{m}^{c},\mathcal{D},i)=\frac{1}{2}(\mathrm{ Tr}(\mathbf{K}_{i}^{c}\mathbf{K}_{i})-n+(\boldsymbol{\mu}_{i}^{c}-\boldsymbol{\mu}_{i} )^{\top}(\mathbf{K}_{i}^{c})^{-1}(\boldsymbol{\mu}_{i}^{c}-\boldsymbol{\mu}_{i} )+\ln\left(\frac{\det(\mathbf{K}_{i}^{c})}{\det(\mathbf{K}_{i})}\right)\] (S18)

Based on Altamirano _et al._ (2024), the PIF leads to the following form:

\[\mathrm{PIF}_{\mathrm{IterGP}}(y_{m}^{c},\mathcal{D},i)=\frac{1}{2}\left(( \boldsymbol{\mu}_{i}^{c}-\boldsymbol{\mu}_{i})^{\top}(\mathbf{K}_{i}^{c})^{-1}( \boldsymbol{\mu}_{i}^{c}-\boldsymbol{\mu}_{i})\right)\] (S19)

Notice that the term \(\boldsymbol{\mu}_{i}^{c}-\boldsymbol{\mu}_{i}\) can be written as

\[\boldsymbol{\mu}_{i}^{c}-\boldsymbol{\mu}_{i} =(\mathbf{m}+\mathbf{K}\mathbf{v}_{i}^{c})-(\mathbf{m}+\mathbf{K} \mathbf{v}_{i})\] (S20) \[=\mathbf{K}(\mathbf{v}_{i}^{c}-\mathbf{v}_{i})\] (S21) \[=\mathbf{K}(\mathbf{C}_{i}(\mathbf{y}^{c}-\mathbf{m})-\mathbf{C}_{ i}(\mathbf{y}-\mathbf{m}))\] (S22) \[=\mathbf{K}(\mathbf{C}_{i}(\mathbf{y}^{c}-\mathbf{y}))\] (S23)Substituting the RHS of Eq. (S23) to \(\bm{\mu}_{i}^{c}-\bm{\mu}_{i}\) in Eq. (S19), we obtain

\[\mathrm{PIF}_{\mathrm{IterGP}}(y_{m}^{c},\mathcal{D},i) =\frac{1}{2}(\mathbf{C}_{i}(\mathbf{y}^{c}-\mathbf{y}))^{\top} \mathbf{K}\left(\mathbf{K}\mathbf{C}_{i}\sigma^{2}\mathbf{I}\right)^{-1} \mathbf{K}(\mathbf{C}_{i}(\mathbf{y}^{c}-\mathbf{y}))\] (S24) \[=\frac{1}{2}(\mathbf{y}^{c}-\mathbf{y})^{\top}\mathbf{C}_{i}^{ \top}\mathbf{K}\sigma_{\mathrm{noise}}^{-2}\mathbf{I}(\mathbf{y}^{c}-\mathbf{ y})\] (S25)

Note that \(\mathbf{y}\) and \(\mathbf{y}^{c}\) have only one exception for the \(m-\)th element. Thus, we have

\[\mathrm{PIF}_{\mathrm{IterGP}}(y_{m}^{c},\mathcal{D},i)=\frac{1}{2}[\mathbf{C }_{i}^{\top}\mathbf{K}\sigma^{-2}\mathbf{I}]_{mm}(y_{m}^{c}-y_{m})^{2}\] (S26)

**PIF for the IterRCGP.** For the IterRCGP with \(\sup_{\mathbf{x},y}w(\mathbf{x},y)<\infty\), the following holds

\[\mathrm{PIF}_{\mathrm{IterRCGP}}(y_{m}^{c},\mathcal{D},i)\leq C_{2}^{\prime}(w (\mathbf{x}_{m},y_{m}^{c})^{2}y_{m}^{c})^{2}+C_{3}^{\prime}\] (S27)

for some constants \(C_{2}^{\prime},C_{3}^{\prime}\in\mathbb{R}\). Therefore, if \(\sup_{\mathbf{x},y}y\,w(\mathbf{x},y)^{2}<\infty\), the computation-aware RCGP is robust since \(|\mathrm{PIF}_{\mathrm{IterRCGP}}(y_{m}^{c},\mathcal{D},i)|<\infty\).

Proof.: Without loss of generality, we aim to prove the bound for \(m=n\). We can extend the proof for an arbitrary \(m\in\{1,\ldots,n\}\). Let \(p^{w}(\mathbf{f}|\mathcal{D})=\mathcal{N}(\mathbf{f};\hat{\bm{\mu}}_{i},\hat {\mathbf{K}}_{i})\) and \(p^{w}(\mathbf{f}|\mathcal{D}_{m}^{c})=\mathcal{N}(\mathbf{f};\hat{\bm{\mu}}_{ i}^{c},\hat{\mathbf{K}}_{i}^{c})\) be the uncontaminated and contaminated computation-aware RCGP, respectively. Here,

\[\hat{\bm{\mu}}_{i} =\mathbf{m}+\mathbf{K}\tilde{\mathbf{C}}_{i}\tilde{\mathbf{v}}_{i}\] (S28) \[\hat{\mathbf{K}}_{i} =\mathbf{K}\tilde{\mathbf{C}}_{i}\sigma_{\mathrm{noise}}^{2} \mathbf{J}_{\mathbf{w}}\] (S29) \[\hat{\bm{\mu}}_{i}^{c} =\mathbf{m}+\mathbf{K}\tilde{\mathbf{C}}_{i}^{c}\tilde{\mathbf{v} }_{i}^{c}\] (S30) \[\hat{\mathbf{K}}_{i}^{c} =\mathbf{K}\tilde{\mathbf{C}}_{i}^{c}\sigma_{\mathrm{noise}}^{2} \mathbf{J}_{\mathbf{w}^{c}}\] (S31)

where \(\mathbf{w}^{c}=(w(\mathbf{x}_{1},y_{1}),\ldots,w(\mathbf{x}_{n},y_{n}^{c}))^{\top}\). The PIF has the following form

\[\mathrm{PIF}_{\mathrm{IterRCGP}}(y_{m}^{c},\mathcal{D},i)=\frac{1}{2}\left( \underbrace{\mathrm{Tr}((\hat{\mathbf{K}}_{i}^{c})^{-1}\hat{\mathbf{K}}_{i})- n}_{(1)}+\underbrace{(\hat{\bm{\mu}}_{i}^{c}-\hat{\bm{\mu}}_{i})^{\top}(\hat{ \mathbf{K}}_{i}^{c})^{-1}(\hat{\bm{\mu}}_{i}^{c}-\hat{\bm{\mu}}_{i})}_{(2)}+ \underbrace{\ln\left(\frac{\det(\hat{\mathbf{K}}_{i}^{c})}{\det(\hat{\mathbf{ K}}_{i})}\right)}_{(3)}\right)\] (S32)

We first derive the bound for \((1)\):

\[(1) =\mathrm{Tr}((\hat{\mathbf{K}}_{i}^{c})^{-1}\hat{\mathbf{K}}_{i})-n\] (S33) \[=\mathrm{Tr}\left((\mathbf{K}\tilde{\mathbf{C}}_{i}^{c}\sigma_{ \mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}^{c}})^{-1}\mathbf{K}\tilde{ \mathbf{C}}_{i}\sigma_{\mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}}\right)-n\] (S34) \[=\mathrm{Tr}(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{w}^ {c}}^{-1}(\tilde{\mathbf{C}}_{i}^{c})^{-1}\tilde{\mathbf{C}}_{i}\sigma_{ \mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}})-n\] (S35) \[\leq\mathrm{Tr}(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{w} ^{c}}^{-1}(\tilde{\mathbf{C}}_{i}^{c})^{-1})\mathrm{Tr}(\tilde{\mathbf{C}}_{i} \sigma_{\mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}})-n\] (S36) \[\leq\mathrm{Tr}(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{w} ^{c}}^{-1})\mathrm{Tr}(\tilde{\mathbf{C}}_{i}^{c})^{-1})\mathrm{Tr}(\tilde{ \mathbf{C}}_{i}\sigma_{\mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}})-n\] (S37)

The first and second inequality come from the fact that \(\mathrm{Tr}(\mathbf{A}\mathbf{F})\leq\mathrm{Tr}(\mathbf{A})\mathrm{Tr}( \mathbf{F})\) for two positive semidefinite matrices \(\mathbf{A}\) and \(\mathbf{F}\). Since \(\mathrm{Tr}(\tilde{\mathbf{C}}_{i}\sigma_{\mathrm{noise}}^{2}\mathbf{J}_{ \mathbf{w}})\) does not contain the contamination term, we can write \(\bar{C}_{1}=\mathrm{Tr}(\bar{\mathbf{C}}_{i}\sigma_{\mathrm{noise}}^{2}\mathbf{J} _{\mathbf{w}})\). Let \(\mathbf{B}=(\mathbf{S}_{i}^{\top}\mathbf{K}^{c}\mathbf{S}_{i})^{-1}\) such that \(\mathbf{C}_{i}^{c}=\mathbf{S}_{i}^{\top}\mathbf{B}\mathbf{S}_{i}^{\top}\). Observe that matrice \(\mathbf{B}\) is positive semidefinite. Thus, we can apply Lemma 5 to obtain the bound of \(\mathrm{Tr}((\tilde{\mathbf{C}}_{i}^{c})^{-1})\):

\[\mathrm{Tr}((\tilde{\mathbf{C}}_{i}^{c})^{-1}) =\mathrm{Tr}((\mathbf{S}_{i}^{\top}\mathbf{B}\mathbf{S}_{i}^{\top })^{-1})\] (S38) \[=\mathrm{Tr}(\mathbf{S}_{i}^{+\top}\mathbf{B}^{-1/2}\mathbf{G} \mathbf{B}^{-1/2}\mathbf{S}_{i}^{+})\] (S39) \[\leq\mathrm{Tr}(\mathbf{S}_{i}^{+}\mathbf{S}_{i}^{+\top})\mathrm{ Tr}(\mathbf{B}^{-1/2}\mathbf{B}^{-1/2})\mathrm{Tr}(\mathbf{G})\] (S40)

where

\[\mathrm{Tr}(\mathbf{G}) =\mathrm{Tr}(\mathbf{I}-\mathbf{B}^{-1/2}(\mathbf{I}-\mathbf{S}_ {i}^{+}\mathbf{S}_{i})(\mathbf{B}^{-1/2}(\mathbf{I}-\mathbf{S}_{i}^{+} \mathbf{S}_{i}))^{+})\] (S41) \[=n-\mathrm{Tr}(\mathbf{B}^{-1/2}(\mathbf{I}-\mathbf{S}_{i}^{+} \mathbf{S}_{i})(\mathbf{I}-\mathbf{S}_{i}^{+}\mathbf{S}_{i})^{+}\mathbf{B}^{- 1/2+})\] (S42) \[\leq n-\mathrm{Tr}(\mathbf{B}^{-1/2+}\mathbf{B}^{-1/2})\mathrm{ Tr}((\mathbf{I}-\mathbf{S}_{i}^{+}\mathbf{S}_{i})(\mathbf{I}-\mathbf{S}_{i}^{+} \mathbf{S}_{i})^{+})\] (S43)

The inequality S40 stems from the trace circular property and the inequality of the product of two semidefinite matrices. Note that \(\mathrm{Tr}(\mathbf{G})\leq n\) since \(\mathbf{B}^{-1/2+}\mathbf{B}^{-1/2}\) and \((\mathbf{I}-\mathbf{S}_{i}^{+}\mathbf{S}_{i})(\mathbf{I}-\mathbf{S}_{i}^{+} \mathbf{S}_{i})^{+}\) in S43 are positive semidefinite matrice; thus both have non-negative trace value. Therefore, we find that

\[\mathrm{Tr}((\tilde{\mathbf{C}}_{i}^{c})^{-1}) \leq n\mathrm{Tr}(\mathbf{S}_{i}^{+}\mathbf{S}_{i}^{+\top}) \mathrm{Tr}(\mathbf{B}^{-1})\] (S44) \[\leq n\mathrm{Tr}(\mathbf{S}_{i}^{+}\mathbf{S}_{i}^{+\top}) \mathrm{Tr}(\mathbf{S}_{i}\mathbf{S}_{i}^{\top})\mathrm{Tr}(\tilde{\mathbf{K }}^{c})\] (S45) \[=\bar{C}_{2}\mathrm{Tr}(\mathbf{K}+\sigma_{\mathrm{noise}}^{2} \mathbf{J}_{\mathbf{w}^{c}})\] (S46)

where we define \(\bar{C}_{2}=n\mathrm{Tr}(\mathbf{S}_{i}^{+}\mathbf{S}_{i}^{+\top})\mathrm{Tr} (\mathbf{S}_{i}\mathbf{S}_{i}^{\top})\). We then plug S46 into S37 to obtain

\[(1) \leq\mathrm{Tr}(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{ w}^{c}}^{-1})\mathrm{Tr}(\mathbf{K}+\sigma_{\mathrm{noise}}^{2}\mathbf{J}_{ \mathbf{w}^{c}})\bar{C}_{1}\bar{C}_{2}-n\] (S47) \[=\left(\sum_{j=1}^{n}\left(\sigma_{\mathrm{noise}}^{-2}w^{2}( \mathbf{x}_{j},y_{j})\right)\sum_{k=1}^{n}\left(\mathbf{K}_{kk}+\sigma_{ \mathrm{noise}}^{2}w^{-2}(\mathbf{x}_{k},y_{k})\right)\right)\bar{C}_{1}\bar{C} _{2}-n\] (S48) \[\leq\left(n^{2}\sup_{\mathbf{x},y}w^{2}(\mathbf{x},y)\sup_{\hat{ \mathbf{x}},\hat{y}}w^{-2}(\hat{\mathbf{x}},\hat{y})\right)\bar{C}_{1}\bar{C} _{2}-n=\bar{C}_{3}\] (S49)

Next, we derive the bound for (2). Following Altamirano _et al._ (2024), we have that

\[(2)\leq\lambda_{\max}((\hat{\mathbf{K}}_{i}^{c})^{-1})\|\hat{\boldsymbol{\mu}} _{i}^{c}-\hat{\boldsymbol{\mu}}_{i}\|_{1}^{2}\] (S50)

We expand \(\lambda_{\max}((\hat{\mathbf{K}}_{i}^{c})^{-1})\) and derive the following bound:

\[\lambda_{\max}((\hat{\mathbf{K}}_{i}^{c})^{-1}) =\lambda_{\max}(\sigma^{-2}\mathbf{J}_{\mathbf{w}^{c}}^{-1}( \tilde{\mathbf{C}}_{i}^{c})^{-1}\mathbf{K}^{-1})\] (S51) \[\leq\lambda_{\max}(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{ w}^{c}}^{-1})\lambda_{\max}((\tilde{\mathbf{C}}_{i}^{c})^{-1})\lambda_{\max}( \mathbf{K}^{-1})\] (S52) \[=\lambda_{\max}(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{ w}^{c}}^{-1})\lambda_{\min}(\tilde{\mathbf{C}}_{i}^{c})\lambda_{\max}(\mathbf{K}^{-1})\] (S53) \[\leq\lambda_{\max}(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{ w}^{c}}^{-1})\left(\lambda_{\min}((\tilde{\mathbf{K}}^{c})^{-1})\right)\lambda_{\max}( \mathbf{K}^{-1})\] (S54) \[\leq\lambda_{\max}(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{ w}^{c}}^{-1})\lambda_{\min}((\tilde{\mathbf{K}}^{c})^{-1})\lambda_{\max}(\mathbf{K}^{-1})\] (S55) \[\leq\lambda_{\max}(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{ w}^{c}}^{-1})(\lambda_{\max}(\mathbf{K})+\lambda_{\max}(\sigma_{\mathrm{noise}}^{2} \mathbf{J}_{\mathbf{w}^{c}}))\lambda_{\max}(\mathbf{K}^{-1})\] (S56)The first inequality follows from the maximum eigenvalue of the product of two positive semidefinite matrices. The fact that the maximum eigenvalue of a matrice is equal to the minimum eigenvalue of the inverse leads to the second equality. Recall that \(\tilde{\mathbf{C}}_{i}^{c}=(\tilde{\mathbf{K}}^{c})^{-1}-\mathbf{\Sigma}_{i}\). Since \(\tilde{\mathbf{C}}_{i}^{c},(\tilde{\mathbf{K}}^{c})^{-1}\) and \(\mathbf{\Sigma}_{i}\) are positive semidefinite matrices, the third inequality holds. The fourth inequality stems from the equivalence of the maximum eigenvalue and the addition property of the maximum eigenvalue of two positive semidefinite matrices.

Since \(\mathbf{J}_{\mathbf{w}^{c}}^{-1}=\mathrm{diag}((\mathbf{w}^{c})^{2})\), and \(\sup_{\mathbf{x},y}w(\mathbf{x},y)<\infty\), it holds that \(\lambda_{\max}(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{w}^{c}}^{-1})= \bar{C}_{4}<+\infty\) and \(\lambda_{\max}(\sigma_{\mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}^{c}})=\bar{ C}_{5}<+\infty\), such that

\[\lambda_{\max}((\tilde{\mathbf{K}}_{i}^{c})^{-1})\leq\bar{C}_{4}(\lambda_{ \max}(\mathbf{K})+\bar{C}_{5})\lambda_{\max}(\mathbf{K}^{-1})=\bar{C}_{6}\] (S57)

We substitute \(\bar{C}_{6}\) into (2) to obtain

\[(2)\leq\bar{C}_{6}\|\hat{\bm{\mu}}_{i}^{c}-\hat{\bm{\mu}}_{i}\|_ {1}^{2}\] (S58) \[=\bar{C}_{6}\|(\mathbf{m}+\mathbf{K}\tilde{\mathbf{v}}_{i}^{c})- (\mathbf{m}+\mathbf{K}\tilde{\mathbf{v}}_{i})\|_{1}^{2}\] (S59) \[=\bar{C}_{6}\|\mathbf{K}(\tilde{\mathbf{C}}_{i}^{c}(\mathbf{y}- \mathbf{m}_{\mathbf{w}^{c}})-\tilde{\mathbf{C}}_{i}(\mathbf{y}-\mathbf{m}_{ \mathbf{w}}))\|_{1}^{2}\] (S60) \[\leq\bar{C}_{6}\|\mathbf{K}\|_{F}\|\tilde{\mathbf{C}}_{i}^{c}( \mathbf{y}-\mathbf{m}_{\mathbf{w}^{c}})-\tilde{\mathbf{C}}_{i}(\mathbf{y}- \mathbf{m}_{\mathbf{w}})\|_{1}^{2}\] (S61) \[\leq\bar{C}_{6}\|\mathbf{K}\|_{F}(\|(\tilde{\mathbf{K}}^{c})^{-1} (\mathbf{y}-\mathbf{m}_{\mathbf{w}^{c}})-(\tilde{\mathbf{K}})^{-1}(\mathbf{y} -\mathbf{m}_{\mathbf{w}})\|_{1}^{2}+\|\tilde{\mathbf{\Sigma}}_{i}^{c}(\mathbf{ y}-\mathbf{m}_{\mathbf{w}^{c}})-\tilde{\mathbf{\Sigma}}_{i}(\mathbf{y}-\mathbf{m}_{ \mathbf{w}})\|_{1}^{2})\] (S62) \[\leq q\bar{C}_{6}\|\mathbf{K}\|_{F}(\|(\tilde{\mathbf{K}}^{c})^{- 1}(\mathbf{y}-\mathbf{m}_{\mathbf{w}^{c}})-(\tilde{\mathbf{K}})^{-1}(\mathbf{ y}-\mathbf{m}_{\mathbf{w}})\|_{1}^{2}\] (S63) \[=q\bar{C}_{6}\|\mathbf{K}\|_{F}((\mathbf{K}+\sigma_{\mathrm{noise }}^{2}\mathbf{J}_{\mathbf{w}^{c}})^{-1}(\mathbf{y}-\mathbf{m}_{\mathbf{w}^{c}} )-(\mathbf{K}+\sigma_{\mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}})(\mathbf{y} -\mathbf{m}_{\mathbf{w}})\|_{1}^{2}\] (S64)

for a constant \(q>0\). The second equality follows from Wenger _et al._ (2022)[Eq. (S45)]. The first inequality follows the Cauchy-Schwarz inequality. The second inequality stems from the definition of \(\tilde{\mathbf{C}}_{i}\), \(\tilde{\mathbf{C}}_{i}^{c}\), and the triangle inequality. Finally, the last inequality holds since \((\tilde{\mathbf{K}}_{i}^{-1}-\tilde{\mathbf{\Sigma}}_{i}),\tilde{\mathbf{K}}_{ i}^{-1},\tilde{\mathbf{\Sigma}}_{i}\succeq 0\).

Applying results from Altamirano _et al._ (2024), we obtain

\[(2) \leq q\bar{C}_{6}\|\mathbf{K}\|_{F}((\mathbf{K}+\sigma_{\mathrm{ noise}}^{2}\mathbf{J}_{\mathbf{w}^{c}})^{-1}(\mathbf{y}-\mathbf{m}_{\mathbf{w}^{c}})-( \mathbf{K}+\sigma_{\mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}})(\mathbf{y}- \mathbf{m}_{\mathbf{w}})\|_{1}^{2}\] (S65) \[\leq q\bar{C}_{6}\|\mathbf{K}\|_{F}2((\bar{C}_{7}+\bar{C}_{8})^{ 2}+(\bar{C}_{9}+\bar{C}_{10})^{2}(w(x_{n},y_{n}^{c})^{2}y_{n}^{c})^{2})\] (S66) \[\leq\bar{C}_{11}+\bar{C}_{12}(w(x_{n},y_{n}^{c})^{2}y_{n}^{c})^{2}\] (S67)

where \(\bar{C}_{11}=q\bar{C}_{6}\|\mathbf{K}\|_{F}2(\bar{C}_{7}+\bar{C}_{8})^{2}\) and \(\bar{C}_{12}=q\bar{C}_{6}\|\mathbf{K}\|_{F}2(\bar{C}_{9}+\bar{C}_{10})^{2}\). The terms \(\bar{C}_{7},\bar{C}_{8},\bar{C}_{9},\bar{C}_{10}\) equal to \(\bar{C}_{6},\bar{C}_{8},\bar{C}_{7},\bar{C}_{9}\) in Altamirano _et al._ (2024).

The term (3) can be written as follows:

\[(3) =\ln\left(\frac{\det(\tilde{\mathbf{K}}_{i}^{c})}{\det(\tilde{ \mathbf{K}}_{i})}\right)\] (S68) \[=\ln\left(\frac{\det(\tilde{\mathbf{C}}_{i}^{c}\sigma_{\mathrm{ noise}}^{2}\mathbf{J}_{\mathbf{w}^{c}})}{\det(\tilde{\mathbf{C}}_{i}\sigma_{\mathrm{noise }}^{2}\mathbf{J}_{\mathbf{w}})}\right)\] (S69) \[=\ln(\det(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{w}}^{-1 }\tilde{\mathbf{C}}_{i}^{-1})\mathrm{det}(\tilde{\mathbf{C}}_{i}^{c})\mathrm{ det}(\sigma_{\mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}^{c}}))\] (S70)

Observe that we can write \(\bar{C}_{13}=\ln(\det(\sigma_{\mathrm{noise}}^{-2}\mathbf{J}_{\mathbf{w}}^{-1} \tilde{\mathbf{C}}_{i}^{-1})\) since it does not contain the contimation term. Furthermore, we obtain \[(3) =\ln(\bar{C}_{13}\mathrm{det}(\tilde{\mathbf{C}}_{i}^{c})\mathrm{det} (\sigma_{\mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}^{c}}))\] (S71) \[\leq\ln(\bar{C}_{13}\mathrm{det}((\tilde{\mathbf{K}}^{c})^{-1}) \det(\sigma_{\mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}^{c}}))\] (S72) \[=\ln\left(\bar{C}_{13}\frac{\mathrm{det}(\sigma_{\mathrm{noise}}^ {2}\mathbf{J}_{\mathbf{w}^{c}})}{\mathrm{det}(\mathbf{K}+\sigma_{\mathrm{ noise}}^{2}\mathbf{J}_{\mathbf{w}^{c}})}\right)\] (S73) \[\leq\ln\left(\bar{C}_{13}\frac{\mathrm{det}(\sigma_{\mathrm{noise }}^{2}\mathbf{J}_{\mathbf{w}^{c}})}{\mathrm{det}(\mathbf{K})+\mathrm{det}( \sigma_{\mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w}^{c}})}\right)\] (S74)

The first inequality holds since \(((\tilde{\mathbf{K}}_{i}^{c})^{-1}-\tilde{\mathbf{\Sigma}}_{i}^{c}),(\tilde{ \mathbf{K}}_{i}^{c})^{-1},\tilde{\mathbf{\Sigma}}_{i}^{c}\succeq 0\), so \(\mathrm{det}((\tilde{\mathbf{K}}_{i}^{c})^{-1})\geq\mathrm{det}(\tilde{ \mathbf{\Sigma}}_{i}^{c})\). The last inequality leverages the fact that \(\mathrm{det}(\mathbf{A}+\mathbf{F})\geq\mathrm{det}(\mathbf{A})+\mathrm{det}( \mathbf{F})\) for \(\mathbf{A}\) and \(\mathbf{F}\) are positive semidefinite matrices. Since \(\mathrm{det}(\mathbf{K}),\mathrm{det}(\sigma_{\mathrm{noise}}^{2}\mathbf{J}_ {\mathbf{w}^{c}})\geq 0\), we find that

\[\ln\left(\frac{\mathrm{det}(\sigma_{\mathrm{noise}}^{2}\mathbf{J}_{\mathbf{w} ^{c}})}{\mathrm{det}(\mathbf{K})+\mathrm{det}(\sigma_{\mathrm{noise}}^{2} \mathbf{J}_{\mathbf{w}^{c}})}\right)\leq 1\] (S75)

Leading to the following inequality:

\[(3)\leq\ln(\bar{C}_{13})=\bar{C}_{14}\] (S76)

Finally, putting the three terms together, we obtain the following bound:

\[\mathrm{PIF}_{\mathrm{IterRGCP}}(y_{m}^{c},\mathcal{D},i) \leq\bar{C}_{3}+\bar{C}_{11}+\bar{C}_{12}(w(x_{n},y_{n}^{c})^{2} y_{n}^{c})^{2}+\bar{C}_{14}\] (S77) \[=C_{2}^{\prime}(w(x_{n},y_{n}^{c})^{2}y_{n}^{c})^{2}+C_{3}^{\prime}\] (S78)

where \(C_{2}^{\prime}=\bar{C}_{12}\) and \(C_{3}^{\prime}=\bar{C}_{3}+\bar{C}_{11}+\bar{C}_{14}\).

## Appendix B Proof of Proposition 2

**Unique solution of the empirical-risk minimization problem.** We first show the existence of a unique solution to the empirical risk minimization problem corresponding to RCGP. For this purpose, we set \(\mathbf{m}=\mathbf{0}\). Following Altamirano _et al._ (2024) (proof of [Proposition 3.1]), we can rewrite \(L_{n}^{w}\) and formulate the RCGP objective as the following empirical-risk minimization problem:

\[\hat{\mathbf{f}}=\mathrm{argmin}_{\mathbf{f}\in\mathcal{H}_{k}}\frac{1}{2n} \left(\underbrace{\mathbf{f}^{\top}\lambda^{-1}\mathbf{J}_{\mathbf{w}}^{-1} \mathbf{f}-2\mathbf{f}^{\top}\lambda^{-1}\mathbf{J}_{\mathbf{w}}^{-1}( \mathbf{y}-\mathbf{m}_{\mathbf{w}})+Q(\mathbf{x},\mathbf{y},\lambda)}_{L_{n}^ {w}}+\|\mathbf{f}\|_{\mathcal{H}_{k}}^{2}\right)\] (S79)

where

\[Q(\mathbf{x},\mathbf{y},\lambda)=\mathbf{y}^{\top}\lambda^{-1}\mathrm{diag} (2\lambda^{-1}\mathbf{w}^{2})\mathbf{y}-4\lambda\nabla_{y}\mathbf{y}^{\top} \mathbf{w}^{2}\] (S80)

for \(\lambda>0\). We then show the unique solution to S79 through the following lemma:

**Lemma 6**.: _If \(\lambda>0\) and the kernel \(k\) is invertible, the solution to S79 is a unique, and is given by_

\[\hat{f}(\mathbf{x})=\mathbf{k}_{\mathbf{x}}(\mathbf{K}+\lambda\mathbf{J}_{ \mathbf{w}})^{-1}(\mathbf{y}-\mathbf{m}_{\mathbf{w}})=\sum_{j=1}^{n}\alpha_{j}k (\mathbf{x},\mathbf{x}_{j}),\mathbf{x}\in\mathcal{X}\] (S81)_where_

\[(\alpha_{i},\ldots,\alpha_{n})=(\mathbf{K}+\lambda\mathbf{J}_{\mathbf{w}})^{-1}( \mathbf{y}-\mathbf{m}_{\mathbf{w}})\in\mathbb{R}^{n}\] (S82)

_Proof:_

The optimization problem in S79 allows us to apply the representer theorem (Scholkopf _et al._, 2001). It implies that the solution of S79 can be written as a weighted sum, i.e.,

\[\hat{\mathbf{f}}=\sum_{j=1}^{n}\alpha_{j}k(.,\mathbf{x}_{j})\] (S83)

for \(\alpha_{1},\ldots,\alpha_{n}\in\mathbb{R}\). Let \(\boldsymbol{\alpha}=[\alpha_{1},\ldots,\alpha_{n}]^{\top}\in\mathbb{R}^{n}\). Substituting S83 into S79 provides

\[\operatorname*{argmin}_{\alpha\in\mathbb{R}^{n}}\frac{1}{2n}(\lambda^{-1} \boldsymbol{\alpha}^{\top}\mathbf{K}\mathbf{J}_{\mathbf{w}}^{-1}\mathbf{K} \boldsymbol{\alpha}-2\lambda^{-1}\boldsymbol{\alpha}^{\top}\mathbf{K} \mathbf{J}_{\mathbf{w}}^{-1}(\mathbf{y}-\mathbf{m}_{\mathbf{w}})+Q(\mathbf{x},\mathbf{y},\lambda)+\|\hat{\mathbf{f}}\|_{\mathcal{H}_{k}}^{2})\] (S84)

where \(\|\hat{\mathbf{f}}\|_{\mathcal{H}_{k}}^{2}=\boldsymbol{\alpha}^{\top}\mathbf{ K}\boldsymbol{\alpha}\), following the reproducing property. Taking the differentiation of the objective w.r.t. \(\boldsymbol{\alpha}\), setting it equal to zero, and arranging the result yields the following equation:

\[\mathbf{K}(\mathbf{K}+\lambda\mathbf{J}_{\mathbf{w}})\boldsymbol{\alpha}= \mathbf{K}(\mathbf{y}-\mathbf{m}_{\mathbf{w}})\] (S85)

Since the objective in S84 is a convex function of \(\boldsymbol{\alpha}\), we find that \(\boldsymbol{\alpha}=(\mathbf{K}+\lambda\mathbf{J}_{\mathbf{w}})^{-1}(\mathbf{ y}-\mathbf{m}_{\mathbf{w}})\) provides the minimum of the objective (S79 and S84). Furthermore, we can verify that \(L_{n}^{w}\) is a convex function w.r.t. \(\mathbf{f}\). Therefore, we conclude that \(\boldsymbol{\alpha}=(\mathbf{K}+\lambda\mathbf{J}_{\mathbf{w}})^{-1}(\mathbf{ y}-\mathbf{m}_{\mathbf{w}})\) provides the unique solution to S79. As a remark, Proposition 6 closely connects with [Theorem 3.4] in Kanagawa _et al._ (2018).

**Relative bound errors.** We also provide the equivalence of Proposition 2 in Wenger _et al._ (2022):

**Proposition 7**.: _For any choice of actions a relative bound error \(\hat{\rho}(i)\) s.t. \(\|\hat{\mathbf{v}}-\tilde{\mathbf{v}}_{i}\|_{\tilde{\mathbf{K}}}\leq\hat{ \rho}(i)\|\hat{\mathbf{v}}\|_{\tilde{\mathbf{K}}}\) is given by_

\[\hat{\rho}(i)=(\tilde{\mathbf{v}}^{\top}(\mathbf{I}-\tilde{\mathbf{C}}_{i} \tilde{\mathbf{K}})\tilde{\mathbf{v}})^{1/2}\leq\lambda_{\max}(\mathbf{I}- \tilde{\mathbf{C}}_{i}\tilde{\mathbf{K}})\leq 1\] (S86)

_where \(\tilde{\mathbf{v}}=\hat{\mathbf{v}}/\|\tilde{\mathbf{v}}\|_{\tilde{\mathbf{K}}}\)._

The proof is direct since we only need to substitute \(\mathbf{C}_{i},\tilde{\mathbf{K}},\mathbf{v}_{*}\) in Wenger _et al._ (2022) with \(\tilde{\mathbf{C}}_{i},\tilde{\mathbf{K}},\hat{\mathbf{v}}\), respectively.

**Proof of Proposition 2.** Lemma 6 implies there exists a unique solution to the corresponding RCGP risk minimization problem. Choosing \(\hat{\rho}(i)\) as described in Proposition 7, we have that \(\|\hat{\mathbf{v}}-\tilde{\mathbf{v}}_{i}\|_{\tilde{\mathbf{K}}}^{2}\leq\hat{ \rho}(i)\|\hat{\mathbf{v}}-\tilde{\mathbf{v}}_{0}\|_{\tilde{\mathbf{K}}}\), where \(\tilde{\mathbf{v}}_{0}=\mathbf{0}\). Then, for \(i\in\{0,\ldots,n\}\) we find that

\[\|\hat{\mathbf{v}}-\tilde{\mathbf{v}}_{i}\|_{\tilde{\mathbf{K}}}^ {2} \leq\|\hat{\mathbf{v}}-\tilde{\mathbf{v}}_{i}\|_{\tilde{\mathbf{K} }}^{2}\leq\hat{\rho}^{2}(i)\|\hat{\mathbf{v}}-\tilde{\mathbf{v}}_{0}\|_{ \tilde{\mathbf{K}}}^{2}\] (S87) \[\leq\hat{\rho}(i)^{2}\left(\|\hat{\mathbf{v}}-\tilde{\mathbf{v}}_ {0}\|_{\tilde{\mathbf{K}}}^{2}+\frac{\lambda_{\max}(\mathbf{J}_{\mathbf{w}})}{ \lambda_{\min}(\mathbf{K})}\lambda_{\min}(\mathbf{K})\|\hat{\mathbf{v}}- \tilde{\mathbf{v}}_{0}\|_{\tilde{\mathbf{K}}}^{2}\right)\] (S88) \[\leq\hat{\rho}(i)^{2}\left(\|\hat{\mathbf{v}}-\tilde{\mathbf{v}}_ {0}\|_{\tilde{\mathbf{K}}}^{2}+\frac{\lambda_{\max}(\mathbf{J}_{\mathbf{w}})}{ \lambda_{\min}(\mathbf{K})}\|\hat{\mathbf{v}}-\tilde{\mathbf{v}}_{0}\|_{\tilde {\mathbf{K}}}^{2}\right)\] (S89) \[\leq\hat{\rho}(i)^{2}\left(1+\frac{\lambda_{\max}(\mathbf{J}_{ \mathbf{w}})}{\lambda_{\min}(\mathbf{K})}\right)\|\hat{\mathbf{v}}-\tilde{ \mathbf{v}}_{0}\|_{\tilde{\mathbf{K}}}^{2}\] (S90)The third inequality stems from the definition of \(\mathbf{J_{w}}\) and the fact that the maximum eigenvalue of a diagonal matrice is the largest component of its diagonal. Applying result from Wenger _et al._ (2022), we have that

\[\|\hat{\mathbf{v}}-\tilde{\mathbf{v}}_{i}\|_{\mathbf{K}}^{2}=\|\hat{\bm{\mu}}_{ \ast}-\hat{\bm{\mu}}_{i}\|_{\mathcal{H}_{k}}^{2}\] (S91)

Combining both results and defining \(c(\mathbf{J_{w}})=\left(1+\frac{\lambda_{\max}(\mathbf{J_{w}})}{\lambda_{\min} (\mathbf{K})}\right)\), we obtain

\[\|\hat{\bm{\mu}}_{\ast}-\hat{\bm{\mu}}_{i}\|_{\mathcal{H}_{k}}=\|\hat{\mathbf{ v}}-\tilde{\mathbf{v}}_{i}\|_{\mathbf{K}}\leq\hat{\rho}(i)c(\mathbf{J_{w}})\| \hat{\mathbf{v}}-\tilde{\mathbf{v}}_{0}\|_{\mathbf{K}}=\hat{\rho}(i)c(\mathbf{ J_{w}})\|\hat{\bm{\mu}}_{\ast}-\mathbf{m}\|_{\mathcal{H}_{k}}\] (S92)

## Appendix C Proof of Proposition 3

Here, we refer to \(\sigma_{\text{noise}}^{2}\) as \(\sigma^{2}\) to simplify the notation. Let \(c_{j}=(\tilde{\mathbf{C}}_{i}k^{\sigma w}(\mathbf{X},\mathbf{x}))_{j}\) for \(j=1,\ldots,n\), where we define \(k^{\sigma w}(.,.)=k(.,.)+\frac{\sigma^{2}}{2}\delta_{w}(.,.)\), where

\[\delta_{w}(\mathbf{x},\mathbf{x}^{\prime})=\begin{array}{ccc}&w^{-2}( \mathbf{x},y)&\mathbf{x}=\mathbf{x}^{\prime}\,\text{and}\,\mathbf{x}\in \mathcal{D}\\ &2&\mathbf{x}=\mathbf{x}^{\prime}\,\text{and}\,\mathbf{x}\notin\mathcal{D}\\ &0&\mathbf{x}\neq\mathbf{x}^{\prime}\end{array}\] (S93)

Since \(g,m\in\mathcal{H}_{k^{\sigma w}}\), it implies that \(g-m\in\mathcal{H}_{k^{\sigma w}}\). Then, applying [Lemma 3.9] in Kanagawa _et al._ (2018) provides

\[\left(\sup_{\|g-m_{w}\|_{\mathcal{H}_{k^{\sigma w}}}\leq 1}g( \mathbf{x})-\hat{\mu}_{i}^{g}(\mathbf{x})\right)^{2}=\left(\sup_{\|g-m_{w}\|_{ \mathcal{H}_{k^{\sigma w}}}\leq 1}g(\mathbf{x})-\sum_{j=1}^{n}c_{j}(g( \mathbf{x}_{j})-m_{w}(\mathbf{x}_{j}))\right)^{2}\] (S94) \[=\|k^{\sigma w}(.,\mathbf{x})-k(\mathbf{x},\mathbf{X})\tilde{ \mathbf{C}}_{i}k^{\sigma w}(\mathbf{X},.)\|_{\mathcal{H}_{k^{\sigma w}}}^{2}\] (S95) \[=\langle k^{\sigma w}(.,\mathbf{x}),k^{\sigma w}(.,\mathbf{x}) \rangle_{\mathcal{H}_{k^{\sigma w}}^{\sigma w}}-2\langle k^{\sigma w}(., \mathbf{x}),k(\mathbf{x},\mathbf{X})\tilde{\mathbf{C}}_{i}k^{\sigma w}( \mathbf{X},.)\rangle_{\mathcal{H}_{k}^{\sigma w}}+\] \[\langle k(\mathbf{x},\mathbf{X})\tilde{\mathbf{C}}_{i}k^{\sigma w }(\mathbf{X},.),k(\mathbf{x},\mathbf{X})\tilde{\mathbf{C}}_{i}k^{\sigma w}( \mathbf{X},.)\rangle_{\mathcal{H}_{k}^{\sigma w}}\] (S96)

By reproducing property, we have

\[=k^{\sigma w}(\mathbf{x},\mathbf{x})-2k^{\sigma w}(\mathbf{x}, \mathbf{X})\tilde{\mathbf{C}}_{i}k^{\sigma w}(\mathbf{X},\mathbf{x})+k( \mathbf{x},\mathbf{X})\tilde{\mathbf{C}}_{i}k^{\sigma w}(\mathbf{X},\mathbf{X })\tilde{\mathbf{C}}_{i}k^{\sigma w}(\mathbf{X},\mathbf{x})\] (S97)

if \(\mathbf{x}\neq\mathbf{x}_{j}\) or \(\sigma^{2}=0\), it holds that \(k^{\sigma w}(\mathbf{x},\mathbf{X})=k(\mathbf{x},\mathbf{X})\). By definition, we have \(k^{\sigma w}(\mathbf{X},\mathbf{X})=\tilde{\mathbf{K}}\) and by Wenger _et al._ (2022)[Eq. (S42)], it holds that \(\tilde{\mathbf{C}}_{i}\tilde{\mathbf{K}}\tilde{\mathbf{C}}_{i}=\tilde{ \mathbf{C}}_{i}\). Therefore, we obtain

\[=k(\mathbf{x},\mathbf{x})+\sigma^{2}-2k(\mathbf{x},\mathbf{X}) \tilde{\mathbf{C}}_{i}k(\mathbf{X},\mathbf{x})+k(\mathbf{x},\mathbf{X}) \tilde{\mathbf{C}}_{i}\tilde{\mathbf{K}}\tilde{\mathbf{C}}_{i}k(\mathbf{X}, \mathbf{x})\] (S98) \[=k(\mathbf{x},\mathbf{x})+\sigma^{2}-k(\mathbf{x},\mathbf{X}) \tilde{\mathbf{C}}_{i}k(\mathbf{X},\mathbf{x})\] (S99) \[=\hat{k}_{i}(\mathbf{x},\mathbf{x})+\sigma^{2}\] (S100)

For the last result, we analogously choose \(c_{j}=((\tilde{\mathbf{K}}^{-1}-\tilde{\mathbf{C}}_{i})k^{\sigma w}(\mathbf{X},\mathbf{x}))_{j}\). Then, we obtain \[\left(\sup_{\|g-m_{w}\|_{\mathcal{H}_{k}\sigma^{ww}}\leq 1}\hat{ \mu}^{g}(\mathbf{x})-\hat{\mu}^{g}_{i}(\mathbf{x})\right)^{2}=\left(\sup_{\|g-m_ {w}\|_{\mathcal{H}_{k}\sigma^{ww}}\leq 1}\sum_{j=0}^{n}c_{j}g(\mathbf{x}_{j}) \right)^{2}\] (S101) \[=\|k(\mathbf{x},\mathbf{X})(\tilde{\mathbf{K}}^{-1}-\tilde{ \mathbf{C}}_{i})k^{\sigma w}(\mathbf{X},.)\|_{\mathcal{H}_{k}\sigma^{w}}^{2}\] (S102) \[=k^{\sigma w}(\mathbf{x},\mathbf{X})\tilde{\mathbf{K}}^{-1}\tilde {\mathbf{K}}\tilde{\mathbf{K}}^{-1}k^{\sigma w}(\mathbf{X},\mathbf{x})-2k^{ \sigma w}(\mathbf{X},\mathbf{X})\tilde{\mathbf{K}}^{-1}\tilde{\mathbf{K}} \tilde{\mathbf{C}}_{i}k^{\sigma w}(\mathbf{X},\mathbf{x})+\] \[\quad k^{\sigma w}(\mathbf{x},\mathbf{X})\tilde{\mathbf{C}}_{i} \tilde{\mathbf{K}}\tilde{\mathbf{C}}_{i}k^{\sigma w}(\mathbf{X},\mathbf{x})\] (S103) \[=k(\mathbf{x},\mathbf{X})(\tilde{\mathbf{K}}^{-1}-\tilde{ \mathbf{C}}_{i})k(\mathbf{X},\mathbf{x})\] (S104) \[=k^{\text{comp.}}_{i}(\mathbf{x},\mathbf{x})\] (S105)