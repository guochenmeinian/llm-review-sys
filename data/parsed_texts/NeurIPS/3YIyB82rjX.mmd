# Handling Learnwares from Heterogeneous Feature Spaces with Explicit Label Exploitation

 Peng Tan, Hai-Tian Liu, Zhi-Hao Tan, Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{tanp,liuht,tanzh,zhouzh}@lamda.nju.edu.cn

###### Abstract

The learnware paradigm aims to help users leverage numerous existing high-performing models instead of starting from scratch, where a learnware consists of a well-trained model and the specification describing its capability. Numerous learners are accommodated by a learnware dock system. When users solve tasks with the system, models that fully match the task feature space are often rare or even unavailable. However, models with heterogeneous feature space can still be helpful. This paper finds that _label information_, particularly model outputs, is helpful yet previously less exploited in the accommodation of heterogeneous learnwares. We extend the specification to better leverage model pseudo-labels and subsequently enrich the unified embedding space for better specification evolvement. With label information, the learnware identification can also be improved by additionally comparing conditional distributions. Experiments demonstrate that, even without a model explicitly tailored to user tasks, the system can effectively handle tasks by leveraging models from diverse feature spaces.

## 1 Introduction

The current machine learning paradigm has achieved remarkable success across various domains. This success, however, hinges on several critical factors: access to abundant high-quality labeled data, expensive computational resources, and deep expertise in feature engineering and algorithm design. These requirements pose a significant challenge for ordinary individuals aiming to build high-quality models from scratch. Moreover, issues such as data privacy, the difficulty of model adaptation, and catastrophic forgetting complicate the reuse or adaptation of trained models across different users.

Indeed, most efforts have focused on these issues separately, paying less attention to the fact that these problems are entangled. To address these challenges simultaneously, the _learnware_ paradigm was proposed by Zhou (2016). The learnware paradigm (Zhou and Tan, 2024) aims to assist users in solving their tasks by leveraging existing high-performing models, through the establishment of a _learnware dock system_. One important purpose of learnware paradigm is to enable high-performing models, submitted by developers, to be used "beyond-what-was-submitted." This means that models can be repurposed to assist with tasks not originally targeted by developers. To achieve this, learnware is designed as a high-performing model with a _specification_ describing its capability and utility. The specification, a central component for learnware management and identification, can be implemented by sketching the data distribution in which the model is proficient (Zhou and Tan, 2024). Recently, to facilitate research on the learnware paradigm, the learnware dock system, Beimingwu, has been released (Tan et al., 2024).

Previous research (Liu et al., 2024; Xie et al., 2023; Zhang et al., 2021) focuses on the homogeneous case where models and user tasks share the same feature space. However, in real-world scenarios, the feature spaces of models often differ due to varied feature engineering. As an example, weconsider the widely used clinical database, the OMOP Common Data Model (Biedermann et al., 2021), as illustrated in Figure 1. This model manages healthcare data from various sources through several standardized tables, such as demographic information, diagnoses, laboratory results, and medications. Experts across different hospitals often use different tables for feature engineering, even when working on the same clinical task, leading to the development of heterogeneous models.

In order to manage and exploit models developed from heterogeneous feature spaces, it is essential to build connections between these different spaces. Existing related techniques for exploiting relationships between feature spaces either rely on raw data (Wang and Sun, 2022; Zhu et al., 2023) of the model or utilize additional co-occurrence data (Xu et al., 2013; Huang et al., 2023). However, with model specifications, the learnware dock system can determine the relationships through subspace learning without the need for raw data or extra auxiliary data (Tan et al., 2023). To effectively accommodate heterogeneous learnwares, a unified subspace is constructed based on specifications of all submitted models, which helps to evolve the specification to have the capabilities of meeting requirements across different feature spaces. This paper finds that, without label information, subspace learning tends to yield suboptimal results, causing embeddings with entangled class representations in the subspace, or even rendering them meaningless when feature spaces are only weakly correlated. Additionally, without exploiting label information, the system can only identify models with marginal distributions similar to the user's task, ignoring models' capabilities.

This paper explicitly leverages label information for managing and utilizing heterogeneous models. We extend the specification to better incorporate model pseudo-labels, enabling the transition from unsupervised to supervised subspace learning for better specification evolvement. The extended specification also allows for additional comparison of conditional distributions using label information, thereby improving the learnware identification. The contributions are as follows:

* This paper proposes to exploit the model outputs to evolve specifications into a unified space during heterogeneous learnware accommodation. Specifically, the unified space is constructed based on the specifications of all models. By exploiting model outputs encoded in the specification, the resulting subspace exhibits improved properties, with less entangled class representations and more coherent embeddings.
* This paper extends the specification implementation to more effectively leverage label information by encoding both marginal and conditional distributions. This extended specification provides more accurate label information during subspace learning to better evolve specifications. Additionally, it also allows for additional comparison of conditional distributions, thereby improving the learnware identification.
* Experiments demonstrate that, even without a model explicitly tailored to the user's task, the system can effectively handle the task by leveraging models from diverse feature spaces.

## 2 Preliminary

Specification is the central part of the learnware, capturing the model ability. This section briefly introduces the Reduced Kernel Mean Embedding (RKME) specification (Zhou and Tan, 2024), which sketches the joint distribution of task features and model outputs with kernel methods.

We start by introducing the Kernel Mean Embedding (KME) (Scholkopf and Smola, 2002), which offers a novel representation for distributions. KME transforms a distribution into a reproducing kernel Hilbert space (RKHS). Given a distribution \(\mathcal{D}\) defined over a space \(\mathcal{X}\), the KME is defined as \(\mu_{k}(\mathcal{D}):=\int_{\mathcal{X}}k(\bm{x},\cdot)\mathrm{d}\mathcal{D}( \bm{x})\), where \(k:\mathcal{X}\times\mathcal{X}\rightarrow\mathbb{R}\) is a symmetric and positive definite kernel function, and its associated RKHS is \(\mathcal{H}\). For a data set \(\left\{\bm{x}_{i}\right\}_{i=1}^{m}\) sampled from \(\mathcal{D}\), the empirical estimate of KME is given by \(\hat{\mu}_{k}(\mathcal{D}):=\frac{1}{m}\sum_{i=1}^{m}k\left(\bm{x}_{i},\cdot\right)\).

KME is considered as a potential specification due to several favorable properties. Accessing the raw data, however, compromises the necessary privacy concerns of the specification. Based

Figure 1: Heterogeneous feature space models in real-world scenario.

on KME, the RKME specification is proposed to use a reduced set of minor weighted samples \(\{(\beta_{j},\bm{t}_{j})\}_{j=1}^{n},n\ll m\) to approximate the empirical KME of the original dataset with model pseudo-outputs \(\{\bm{q}_{i}\}_{i=1}^{m}=\{(\bm{x}_{i},\hat{y}_{i})\}_{i=1}^{m}\), where \(\hat{y}_{i}=f(\bm{x}_{i})\) is the model prediction. The reduced set is generated by:

\[\min_{\bm{\beta},\bm{t}}\left\|\frac{1}{m}\sum_{i=1}^{m}k\left(\bm{q}_{i}, \cdot\right)-\sum_{j=1}^{n}\beta_{j}k\left(\bm{t}_{j},\cdot\right)\right\|_{ \mathcal{H}}^{2},\] (1)

with the non-negative coefficients \(\{\beta_{j}\}_{j=1}^{n}\). The RKME \(\Phi(\cdot)=\sum_{j=1}^{n}\beta_{j}k\left(\bm{t}_{j},\cdot\right)\in\mathcal{H}\) acts as the specification, and the RKHS \(\mathcal{H}\) is referred to as the specification space. This specification effectively captures the major information of the distribution \(\mathcal{D}\) without exposing raw data and explicitly encodes the model capability based on its outputs. Notably, in simple cases where the features are sufficient to represent the model capability, the sketch solely on the features \(\{\bm{x}_{i}\}_{i=1}^{m}\) can also be used as the model specification (Wu et al., 2023). In this paper, we further extend the specification generation process to more effectively encode the model's outputs.

## 3 Problem setup

This paper addresses the challenge of constructing a heterogeneous learnware dock system and leveraging it to assist users who have only limited labeled data such that training a model by themselves will lead to poor performance. Without loss of generality, we consider the underlying full feature space, denoted as \(\mathcal{X}_{\text{all}}\), as a composite of \(Q\) distinct blocks, i.e., \(\mathcal{X}_{\text{all}}=\mathcal{X}_{1}\times\cdots\times\mathcal{X}_{Q}\). The feature spaces for developers, \(\mathcal{X}^{\text{dev}}\), and for users, \(\mathcal{X}^{\text{user}}\), are represented as Cartesian products of specific blocks \(\times_{i\in C}\mathcal{X}_{i}\), where \(C\) refers to block indices.

The overall procedure consists of two stages: the submission stage and the deployment stage. In the submission stage, the developer trains a well-performing model \(f_{i}\) on the dataset \(D_{i}:=\{(\bm{x}_{ij},y_{ij})\}_{j=1}^{n_{i}}\) and generates a _developer-level_ specification \(\bm{s}_{i}^{\text{dev}}\), which captures the model's performance without exposing raw data. After receiving all heterogeneous models and their developer-level specifications, the learnware dock system assigns a _system-level_ specification \(\bm{s}_{i}\) to each model \(f_{i}\), based on all submitted specifications \(\{\bm{s}_{i}^{\text{dev}}\}_{i=1}^{N}\). The heterogeneous learnware dock system is then constructed as \(\{(f_{i},\bm{s}_{i})\}_{i=1}^{N}\). In the deployment stage, the user has unlabeled data \(D_{0}^{u}=\{\bm{x}_{0i}\}_{i=1}^{n_{u}}\) and a limited amount of labeled data \(D_{0}^{l}=\{(\tilde{\bm{x}}_{0i},y_{0i})\}_{i=1}^{n_{l}}\) (the unlabeled data cover labeled data features, i.e., \(\{\tilde{\bm{x}}_{i}\}_{i}\subseteq\{\bm{x}_{i}\}_{i}\)). The user generates a user-level task requirement \(\bm{s}_{0}^{\text{user}}\) and submits it to the learnware dock system. The system then identifies the most helpful model(s) for reuse to tackle the user task.

## 4 Methodology

This section outlines our methodology for accommodating heterogeneous models under the learnware paradigm and assisting user tasks, emphasizing the importance and utilization of label information, which remains _unexplored_ in learnware paradigm when dealing with heterogeneous feature spaces.

### Improve managing heterogeneous models with label information

To handle learnwares with heterogeneous feature spaces, it is helpful to exploit the relationships between these spaces. A common approach is to learn a unified subspace. However, without label information, the resulting subspace may produce entangled embeddings of samples from different

Figure 2: An illustration of the learnware paradigm with heterogeneous feature spaces

classes, and when feature blocks are weakly correlated, the subspace may become meaningless (see Section B.2 for detailed discussion). Since subspace learning is based on all learnware specifications, incorporating label information into the model specification is highly beneficial.

We rewrite the RKME specification represented by \(R=(\bm{\beta},T)=\{(\beta_{j},\bm{t}_{j})\}_{j=1}^{m}\) to RKMEL represented by \(R_{L}=(\bm{\beta},Z,Y)=\{(\beta_{j},\bm{z}_{j},y_{j})\}_{j=1}^{m}\) by splitting the sample \(\bm{t}_{j}\) into the feature \(\bm{z}_{j}\) and the pseudo label \(y_{j}\), emphasizing label information. Given existing model specifications \(\{s_{i}^{\text{dev}}:=\{(\beta_{ij},\bm{z}_{j},y_{j})\}_{j=1}^{m_{i}}\}_{i=1}^ {N}\), the learnware dock system can learn a unified subspace \(\mathcal{X}_{\text{sub}}\) with encoding functions \(\{h_{k}:\mathcal{X}_{k}\mapsto\mathcal{X}_{\text{sub}}\}_{k=1}^{Q}\) and decoding functions \(\{g_{k}:\mathcal{X}_{\text{sub}}\mapsto\mathcal{X}_{k}\}_{k=1}^{Q}\) by optimizing \(L=\alpha_{1}L_{\text{reconstruction}}+\alpha_{2}L_{\text{similar}}+\alpha_{3}L_{ \text{supervised}}\) over mapping functions \(\{h_{k},g_{k}\}_{k=1}^{Q}\). The objective function has three components: the reconstruction loss, which trains mapping functions \((h_{k},g_{k})\) to map and reconstruct data in \(\mathcal{X}_{k}\); the similarity loss, which makes embeddings of different slices of \(\bm{z}_{j}\) similar; and the supervised loss, which uses label information to improve subspace learning by making class embeddings more separable or aligning samples within the same class. After subspace learning, the mapping functions \(\{h_{k},g_{k}\}_{k=1}^{Q}\) can project data from any combination of feature space blocks to the subspace. When reusing heterogeneous models, these functions can also fill in missing parts needed for model predictions. Such a framework can be implemented by existing subspace learning methods, such as self-supervised learning (Ucar et al., 2021; Bahri et al., 2022), matrix factorization (Xu and Gong, 2004; Wang et al., 2016). When the system receives learnwares from unseen feature spaces after subspace generation, the system can update the subspace during idle time.

### Improve matching model and user task with label information

Matching with only marginal distribution \(P_{X}\) is not enough.We first review previous methodologies for matching a user's task with a model in the homogeneous case, where all models and user tasks share the same feature space (Wu et al., 2023; Zhang et al., 2021). These methods recommend the model with the most similar marginal distribution \(P_{X}\). To avoid exposing raw data, they use RKME to sketch the marginal distributions of the model task and user task, serving as the model specifications and user requirements. To illustrate the deficiency, we refer to Figure 3, which presents five tasks with uniform distributions. Among these tasks, four have circular support sets and one has a square support set. The two problems are: 1) Models with the same \(P_{X}\) but different \(P_{X|Y}\) are indistinguishable. In Figure 3, Models 1, 2 and 4 are all recommended, but model 2 is unsuitable for the user task. 2) Models with different \(P_{X}\) are rarely considered, despite their potential usefulness. Model 3 in Figure 3, though suitable, are excluded because its \(P_{X}\) is square instead of circle.

Enhance matching by incorporating the conditional distribution \(P_{X|Y}\).Matching solely on the marginal distribution is insufficient for model identification. To better recommend models to user tasks, we propose additionally considering the conditional distribution \(P_{X|Y}\), which helps exclude the model with dissimilar conditional distributions (Model 2) and include the model with similar ones (Model 3). While the user's task can estimate the conditional distribution from labeled data, a key question arises for the model task: _should we use true labels or model-generated pseudo labels?_ Using True labels results in comparing the user's task distribution \(P(X,Y)\) with the model's original task distribution, while pseudo labels results in comparing the model-generated joint distribution \(P(X,\hat{Y})\) with new tasks, allowing the model to be reused beyond its original purpose. As shown in Figure 3, Model 4 would be recommended using pseudo labels but not using true labels. In conclusion, considering both marginal and conditional distributions improves model identification, with model-generated pseudo labels being helpful for encoding model capabilities.

### Summary

To accommodate and identify models developed from heterogeneous feature spaces, it is advantageous to utilize pseudo-label information generated by the models. To better incorporate this information,

Figure 3: Label information is beneficial for matching.

we propose to integrate both marginal and conditional distributions into the model specification and user requirements, represented as \(\{(\beta_{j},\bm{z}_{j},y_{j})\}_{j=1}^{m}\). By comparing these distributions, we improve learnware identification. The inclusion of label information enhances subspace learning, and the framework can be applied across various subspace learning methods.

## 5 Detailed solution

In this section, we provide the detailed procedure for the heterogeneous learnware problem based on the aforementioned methodology, which consists of model specification generation, heterogeneous learnwares accommodation by the system, and system exploitation for solving new user tasks.

### The developer generates the model specification

The model specification sketches task distribution and model capabilities with a reduced set. Instead of sketching the joint distribution of task features and outputs (Eq. (1)), we propose to generate feature and label part separately to balance label and feature information. This includes a unified mechanism for classification and regression, and a specialized mechanism for classification.

Unified mechanism for classification and regression tasks.Given a dataset \(D=\{(\bm{x}_{i},y_{i})\}_{i=1}^{n}\) and a model \(f\) trained on it, we first generate a reduced set \(\{(\beta_{j},\bm{z}_{j})\}_{j=1}^{m}\) solely on \(\{\bm{x}_{i}\}_{i=1}^{n}\) based on RXME via Eq. (1) with \(\bm{q}_{i}=\bm{x}_{i}\), which sketches the marginal distribution of the task feature. To encode the model's ability, pseudo labels can be assigned to the reduced set using \(y_{j}=f(\bm{z}_{j})\), resulting in the labeled reduced set \(R_{L}=\left(\bm{\beta},Z,Y\right)=\left\{(\beta_{j},\bm{z}_{j},y_{j})\right\}_ {j=1}^{m}\), serving as the developer-level model specification \(\bm{s}^{\text{dev}}\).

Specialized mechanism for classification tasks.For classification problems, given the finite label space \(\mathcal{Y}\), we propose the mechanism to directly sketch the model's capacity, characterized by the conditional distribution \(P(X|Y)\) of the model \(f\). We first obtain the model predictions \(\{\hat{y}_{i}\}_{i=1}^{n}\) on its "skilled" marginal distribution, i.e., its training data \(\{\bm{x}_{i}\}_{i=1}^{n}\). Then, the pseudo-labeled dataset \(\{(\bm{x}_{i},\hat{y}_{i})\}_{i=1}^{n}\), which encodes the model's conditional distribution, can be sketched by a labeled reduced set \(R_{L}=\left(\bm{\beta},Z,Y\right)=\left\{(\beta_{j},\bm{z}_{j},y_{j})\right\}_ {j=1}^{m}\) using the following objective:

\[\left\|\sum_{i=1}^{n}\frac{1}{n}k\left(\bm{x}_{i},\cdot\right)-\sum_{j=1}^{m} \beta_{j}k\left(\bm{z}_{j},\cdot\right)\right\|_{\mathcal{H}_{k}}^{2}+\theta \sum_{c=1}^{C}\left\|\sum_{i\in\mathcal{I}_{c}}\frac{1}{n}k\left(\bm{x}_{i}, \cdot\right)-\sum_{j\in\mathcal{I}_{c}^{\prime}}\beta_{j}k\left(\bm{z}_{j}, \cdot\right)\right\|_{\mathcal{H}_{k}}^{2},\] (2)

where \(\mathcal{I}_{c}\) and \(\mathcal{I}_{c}^{\prime}\) represent the indices of samples \(\bm{x}_{i}\) and \(\bm{z}_{j}\) belonging to class \(c\), respectively. \(\theta\) is the parameter used to balance the marginal distribution distance and conditional distribution distance. The labeled reduced set \(R_{L}\) should approximate both the marginal distribution \(\sum_{i=1}^{n}\frac{1}{n}\delta_{\bm{x}_{i}}\) with \(\sum_{j=1}^{m}\beta_{j}\delta_{\bm{z}_{j}}\) and the conditional distribution given the \(c\) class \(\sum_{i\in\mathcal{I}_{c}}\frac{1}{n}\delta_{\bm{x}_{i}}\) with \(\sum_{j\in\mathcal{I}_{c}^{\prime}}\beta_{j}\delta_{\bm{z}_{j}}\) simultaneously. Here, \(\delta(\cdot)\) is the Dirichlet function, which describes the probability mass at a single point. The objective Eq. (2) can be optimized by alternating optimization, the details are showed in E.1. The optimized reduced set \(R_{L}\) is served as the developer-level model specification \(\bm{s}^{\text{dev}}\).

The first unified mechanism sketches the marginal distribution and then encodes the model information, while the second specialized mechanism directly sketches the model's conditional distribution.

### The system accommodates heterogeneous learnwares

After the developer-level specification \(\bm{s}^{\text{dev}}\) is generated, the developer submits the model \(f\) with specification \(\bm{s}^{\text{dev}}\) to the learnware dock system. The system exploits the relationship of different feature spaces and manages heterogeneous models by assigning system-level specification \(\bm{s}^{\text{sys}}\).

Subspace learningAfter the learnware dock system receives heterogeneous models with their developer-level specifications, it generates a unified subspace \(\mathcal{X}_{\text{sub}}\) to connect different feature blocks \(\{\mathcal{X}_{i}\}_{i=1}^{Q}\) based on all developer-level specifications \(\{\bm{s}_{i}^{\text{dev}}:=\{(\beta_{ij},\bm{z}_{ij},y_{ij})\}_{j=1}^{m_{i}} \}_{i=1}^{N}\). During subspace learning, the learnware dock system learns \(2Q\) mapping functions:and \(\{g_{k}:\mathcal{X}_{\text{sub}}\mapsto\mathcal{X}_{k}\}_{k=1}^{Q}\). For a particular sample \(\bm{z}_{ij}\), it can be split into several blocks \(\{\bm{z}_{ij}^{(k)}\}_{k\in C_{i}}\) according to the feature split \(\mathcal{X}_{\text{alt}}=\mathcal{X}_{1}\times\cdots\times\mathcal{X}_{Q}\). The encoding function \(h_{k}\) produces the embedding of sample slice \(\bm{z}_{ij}^{(k)}\) as \(\bm{v}_{ij}^{(k)}\), and the decoding function \(g_{k}\) reconstructs it to \(\mathcal{X}_{k}\) as \(\tilde{\bm{z}}_{ij}^{(k)}\).

The loss for subspace learning is implemented as follows: **1)** The reconstruction loss, \(\mathcal{L}_{\text{reconstruct}}=\sum_{i=1}^{N}\sum_{j=1}^{m_{i}}\beta_{ij}\| \tilde{\bm{z}}_{ij}^{(k)}-\bm{z}_{ij}^{(k)}\|_{\mathcal{F}}^{2}\), penalizes the difference between the original sample \(\bm{z}_{ij}^{(k)}\) and the reconstructed sample \(\tilde{\bm{z}}_{ij}^{(k)}\), weighted by the sample importance \(\beta_{ij}\). **2)** The supervised loss involves building a simple classifier on \(\{\{(\bm{v}_{ij},y_{ij})\}_{j=1}^{m_{i}}\}_{i=1}^{N}\), where \(\bm{v}_{ij}=\text{mean}(\{\bm{v}_{ij}^{(k)}\}_{k\in C_{i}})\), and calculating the prediction loss to make the embeddings of different classes more separable. **3)** The contrastive loss aims to make the embeddings \(\{\bm{v}_{ij}^{(k)}\}_{k\in C_{i}}\) of a single sample \(\bm{z}_{ij}\) similar, while ensuring that embeddings of different samples are dissimilar. The contrastive loss \(\mathcal{L}_{\text{contrastive}}=\sum_{i=1}^{N}l_{i}\) includes \(N\) terms, each term being a weighted loss extended from the Self-VPCL loss [20], calculated on the embeddings \(\{\{\bm{v}_{ij}^{(k)}\}_{k\in C_{i}}\}_{i=1}^{m_{i}}\) of one specification \(\bm{s}_{i}^{\text{dev}}\): \(l_{i}=\sum_{j=1}^{m_{i}}\beta_{ij}\sum_{k\in C_{i}}\sum_{k^{\prime}\in C_{i}, k^{\prime}\neq k}\log\frac{\exp\psi\left(\bm{v}_{ij}^{(k)},\bm{v}_{ij}^{(k^{ \prime})}\right)}{\sum_{t=1}^{m_{i}}\sum_{t\in C_{i}}\exp\psi\left(\bm{v}_{ij}^ {(k)},\bm{v}_{it}^{(k)}\right)}\), where \(\psi\) is the cosine similarity function. In the logarithm term, the numerator represents the similarity of the positive pair, while the denominator is the sum of all pairs. The loss \(\mathcal{L}\) is optimized with gradient descent.

Heterogeneous learnware accommodation.After subspace learning, the learnware dock system builds a unified subspace and corresponding mapping functions \(\{g_{i},h_{i}\}_{i=1}^{Q}\). The dock system then assigns a system-level specification \(\bm{s}_{i}=\{(\beta_{ij},\bm{v}_{ij},y_{ij})\}_{j=1}^{m_{i}}\) for each model based on its developer-level specification \(\bm{s}_{i}^{\text{dev}}:=\{(\beta_{ij},\bm{z}_{ij},y_{ij})\}_{j=1}^{m_{i}}\). During the system-level specification generation, the sample \(\bm{z}_{ij}\) is projected to the unified subspace as \(\bm{v}_{ij}\), while the coefficient \(\beta_{ij}\) and the label \(y_{ij}\) remain unchanged. The projection \(\bm{v}_{ij}\) is calculated as follows: \(\bm{v}_{ij}=\frac{1}{|C_{i}|}\sum_{k\in C_{i}}h_{k}(\bm{z}_{ij}^{(k)})\). The whole procedure of learnware dock system construction is described in Algorithm 3.

### The user exploits the learnware dock system

After the learnware dock system accommodates heterogeneous learnwares, users can submit their task requirements to receive recommended models and the toolkit used for feature transformation. They can then directly reuse the model or combine it with a self-training model.

User requirement generation.As described in Section 4.2, comparing both the marginal distribution \(P_{X}\) and conditional distribution \(P_{X|Y}\) based on the model specification and user requirement helps better learnware identification. Similar to \(\texttt{RXME}_{\text{L}}\) specification encoding both distributions, the \(\texttt{RXME}_{\text{L}}\) requirement of the user is generated similarly to reflect both. In details, given the unlabeled data \(D^{u}=\{\bm{x}_{i}\}_{i=1}^{n_{u}}\) and some labeled data \(D^{l}=\{(\tilde{\bm{x}}_{i},y_{i})\}_{i=1}^{n_{l}}\) (the unlabeled data cover labeled data features, i.e., \(\{\tilde{\bm{x}}_{i}\}_{i}\subseteq\{\bm{x}_{i}\}_{i}\)), the user can generate \(\texttt{RXME}_{\text{L}}\) requirement presented by labeled reduced set \(R_{L}=\{(\beta_{j},\bm{z}_{j},y_{j})\}_{j=1}^{m}\) to sketch the task distribution.

For the classification case, the reduced set \(R_{L}\) can be generated by minimizing the following distance:

\[\left\|\sum_{i=1}^{n_{u}}\frac{1}{n_{u}}k\left(\bm{x}_{i},\cdot\right)-\sum_{j =1}^{m}\beta_{j}k\left(\bm{z}_{j},\cdot\right)\right\|_{\mathcal{H}_{k}}^{2}+ \theta\sum_{c=1}^{C}\left\|\sum_{i\in\mathcal{I}_{c}}\frac{1}{n_{l}}k\left( \tilde{\bm{x}}_{i},\cdot\right)-\sum_{j\in\mathcal{I}_{c}^{\prime}}\beta_{j}k \left(\bm{z}_{j},\cdot\right)\right\|_{\mathcal{H}_{k}}^{2}\] (3)

This equation is similar to specification generation in Eq. (2), where the specification sketches the pseudo-labeled dataset, while the requirement sketches semi-supervised data. The first term calculates the distance between the marginal distributions of the unlabeled dataset \(\sum_{i=1}^{n_{u}}\frac{1}{n_{u}}\delta_{\bm{x}_{i}}\) and the reduced set \(\sum_{j=1}^{m}\beta_{j}\delta_{\bm{z}_{j}}\). The second term calculates the distance between the conditional distributions of the labeled dataset \(\sum_{i\in\mathcal{I}_{c}}\frac{1}{n_{l}}\delta_{\tilde{\bm{x}}_{i}}\) and the reduced set \(\sum_{j\in\mathcal{I}_{c}^{\prime}}\beta_{j}\delta_{\bm{z}_{j}}\), where \(\mathcal{I}_{c}\) and \(\mathcal{I}_{c}^{\prime}\) denote the sample indices of the labeled dataset and the reduced set with label \(c\), respectively. The optimized reduced set \(R_{L}\) becomes the user-level requirement \(\bm{s}_{0}^{\text{user}}\). The optimization is described in E.2

For regression, the requirement is generated by sketching the marginal distribution and applying a self-trained model for pseudo-labeling, similar to unified specification generation in Section 5.1.

Learnware identification.After the user submits the user-level task requirement \(\bm{s}_{0}^{\text{user}}=\{(\beta_{0k},\bm{z}_{0k},y_{0k})\}_{k=1}^{m_{0}}\), to the dock system, the dock system transforms it into the system-level task requirement \(\bm{s}_{0}=\{(\beta_{0k},\bm{v}_{0k},y_{0k})\}_{k=1}^{m_{0}}\) by projecting \(\bm{z}_{0k}\) into the subspace as \(\bm{v}_{0k}\). The dock system then calculates the distance between the system-level specification \(\bm{s}_{i}=\{(\beta_{ij},\bm{v}_{ij},y_{ij})\}_{j=1}^{m_{i}}\) and the system-level task requirement \(\bm{s}_{0}\) as follows:

\[\left\|\sum_{k}^{m_{0}}\beta_{0k}k(\bm{v}_{0k},\cdot)-\sum_{j}^{m_{i}}\beta_{ ij}k(\bm{v}_{ij},\cdot)\right\|+\alpha\sum_{C}\left\|\sum_{k\in\mathcal{I}_{0,C} }\beta_{0k}k(\bm{v}_{0k},\cdot)-\sum_{j\in\mathcal{I}_{i,C}}\beta_{ij}k(\bm{v }_{ij},\cdot)\right\|,\] (4)

which measures both the conditional distribution distances and marginal distribution distances. Where \(\mathcal{I}_{i,C}\) represents the indices of \(\bm{v}_{ij}\) with the class c. The learnware dock system then recommends the learnware with a minimal distance.

Learnware reuse.Once the user receives the recommended model \(f_{i}\) and the dock system toolkit \(\{h_{k},g_{k}\}_{k=1}^{Q}\), they can apply the model to their task. The toolkit helps bridge the gap between different feature spaces. For example, if the user's task is on \(\mathcal{X}_{1}\times\mathcal{X}_{2}\) and the model is on \(\mathcal{X}_{2}\times\mathcal{X}_{3}\times\mathcal{X}_{4}\), the user can project their data using \(h_{1}\) and \(h_{2}\), then decode it to \(\mathcal{X}_{3}\) and \(\mathcal{X}_{4}\) with \(g_{3}\) and \(g_{4}\). The user can use the recommended model's predictions directly or ensemble them with a self-trained model.

### Overall procedure

In the submission stage, the dock system receives models with developer-level specifications that sketch model capabilities and assigns system-level specifications by a learned unified subspace. In the deployment stage, users submit task requirements detailing marginal and conditional distributions to receive recommended learnware. This learnware can be integrated with their self-trained models to significantly enhance performance. The overall process is summarized in Appendix D.1.

## 6 Experiments

### Experiment setup

Datasets.We tested our methods on 30 datasets from the Tabzilla benchmark (McElfresh et al., 2023), excluding tiny datasets. These include 23 classification tasks and 7 regression tasks. For classification tasks, the sample sizes range from 1,000 to 58,310, feature space dimensions from 7 to 7,200, and the number of classes from 2 to 10. For regression tasks, the sample sizes range from 418 to 108,000, and feature space dimensions from 8 to 128.

Compared methods.As the heterogeneous learnware problem, where the user has some labeled data, is a new problem, we first compare our approach with two basic methods that train models from scratch: lightgbm (Ke et al., 2017), a widely used tree-based method for tabular datasets, and TabPFN(Hollmann et al., 2023), a recently proposed prior-data fitted network capable of training and inference on small classification datasets in less than one second. When seeking assistance from the model repository, one simple but inefficient approach is to fetch all models, conduct heterogeneous transfer learning, and select the best one. Align_unlabel(Tan et al., 2024a) aligns the feature space and uses the aligned model directly, while Align_label(Tan et al., 2024a) goes a step further by fine-tuning through training a new model with augmented features that include aligned model predictions. Another method for reusing knowledge from heterogeneous tasks involves pre-training a unified tabular network on different tables and fine-tuning on the downstream user tasks: Transtab(Wang and Sun, 2022) and Xtab(Zhu et al., 2023). However, these methods require access to raw task data, whereas our method protects user privacy. Next, we compare with Hetero(Tan et al., 2023), an initial attempt to address the heterogeneous learnware problem without using label information. Finally, we substitute the specification in our method with the RKME specification from (Zhou and Tan, 2024) as Our_basic_ and conduct a comparison with the proposed method.

Experiment configuration.The feature space is randomly divided into four equal blocks, creating feature spaces from three-block combinations for developer tasks and two-block combinations for user tasks. For user tasks, 100 labeled data points are sampled from the training set. All experiments are repeated five times. For more details, please see Appendix F.1.

### Performance on user tasks

Tables 1 and 2 compare the performance of our proposed methods with other contenders on classification and regression tasks. Our\({}_{\texttt{unify}}\) refers to the performance of the overall procedure with the unified specification, while Our\({}_{\texttt{cls}}\) refers to the specialized specification designed for classification tasks. Our approach, Our\({}_{\texttt{unify}}\), outperforms the competitors in most cases. While Lightgbm and TabPFN use self-training, their performance is limited by the small amount of labeled data. It is showed that TabPFN performs better than Lightgbm under these conditions. This highlights the importance of leveraging well-trained models, even with heterogeneous feature spaces, to improve performance.

Examining Align\({}_{\texttt{unlabel}}\) shows that heterogeneous transfer learning with only aligning feature spaces without labels is less effective than self-training. However, further fine-tuning enables Align\({}_{\texttt{label}}\) to outperform self-training methods. Nevertheless, without leveraging knowledge across different tasks, Align\({}_{\texttt{label}}\) still performs worse than our approach. Transtab and Xtab attempt to create a unified backbone across different tables to leverage cross-task knowledge, but they fail to reuse the high-performing model on each developer task, leading to worse performance than ours. These methods also require training on raw developer data, whereas our method only accesses model specifications without exposing raw data.

Hetero performs worse than our methods due to its lack of modeling the conditional distribution of submitted models and its reliance on unsupervised subspace learning. Compared to Our\({}_{\texttt{basic}}\), our proposed specification outperforms the RKME specification, as it alleviates the issue of label information being overshadowed by feature information during specification generation and comparison. Notably, for classification tasks, our specialized model Our\({}_{\texttt{cls}}\) outperforms Our\({}_{\texttt{unify}}\) due to its ability to encode conditional distribution of the model more effectively.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c} \hline \hline
**Dataset name** & **Lightgbm** & **TabPFN** & **Align\({}_{\texttt{unlabel}}\)** & **Align\({}_{\texttt{label}}\)** & **Transstab** & **Xtab** & **Hetero** & **Our\({}_{\texttt{basic}}\)** & **Our\({}_{\texttt{unify}}\)** & **Our\({}_{\texttt{ch}}\)** \\ \hline credit-g & \(67.4\pm 1.1\) & \(70.5\pm 0.44\) & \(58.3\pm 2.46\) & \(69.1\pm 1.68\) & \(69.6\pm 1.02\) & \(70.3\pm 6.48\) & \(67.5\pm 0.00\) & \(71.2\pm 0.55\) & \(71.0\pm 0.48\) & \(71.4\pm 0.87\) \\ semeno & \(61.9\pm 4.09\) & \(63.4\pm 1.6\) & \(68.1\pm 5.9\) & \(59.6\pm 5.21\) & \(25.4\pm 4.27\) & \(27.1\pm 0.44\) & \(62.0\pm 5.32\) & \(59.2\pm 0.54\) & \(59.9\pm 1.42\) & \(63.7\pm 0.89\) \\ mfet-kaanthren & \(66.1\pm 1.02\) & \(62.6\pm 1.78\) & \(71.2\pm 1.32\) & \(73.3\pm 0.17\) & \(67.2\pm 1.04\) & \(36.0\pm 1.95\) & \(59.4\pm 7.10\) & \(71.2\pm 1.35\) & \(73.5\pm 0.76\) \\ splice & \(64.9\pm 1.9\) & \(63.2\pm 0.48\) & \(42.5\pm 3.11\) & \(56.2\pm 5.46\) & \(72.8\pm 1.46\) & \(56.0\pm 6.84\) & \(60.7\pm 22.12\) & \(73.5\pm 0.49\) & \(73.3\pm 0.89\) \\ gina agnostic & \(75.5\pm 15.6\) & \(76.6\pm 1.51\) & \(45.1\pm 7.45\) & \(74.8\pm 1.42\) & \(57.8\pm 1.23\) & \(62.0\pm 8.00\) & \(88.9\pm 8.73\) & \(87.3\pm 0.89\) & \(89.9\pm 0.39\) \\ Bioresponse & \(64.2\pm 1.57\) & \(57.2\pm 1.57\) & \(57.2\pm 4.28\) & \(64.2\pm 1.30\) & \(51.6\pm 5.16\) & \(56.6\pm 5.64\) & \(66.7\pm 1.67\) & \(61.7\pm 0.71\) & \(61.7\pm 0.79\) & \(62.9\pm 1.57\) & \(71.5\pm 0.89\) \\ sylvine & \(68.6\pm 1.9\) & \(71.4\pm 4.73\) & \(47.2\pm 4.52\) & \(69.2\pm 1.92\) & \(69.6\pm 0.90\) & \(66.6\pm 1.72\) & \(75.1\pm 0.00\) & \(74.8\pm 1.73\) & \(74.5\pm 1.15\) & \(76.9\pm 0.88\) \\ charitine & \(61.7\pm 6.5\) & \(65.4\pm 1.35\) & \(46.6\pm 6.76\) & \(66.9\pm 4.56\) & \(61.8\pm 5.09\) & \(61.8\pm 1.12\) & \(72.2\pm 0.70\) & \(72.6\pm 2.00\) & \(72.6\pm 2.00\) & \(72.7\pm 0.00\) \\ theorem-proving & \(42.1\pm 1.8\) & \(42.5\pm 1.5\) & \(27.0\pm 1.6\) & \(39.5\pm 1.22\) & \(42.0\pm 40.8\) & \(40.8\pm 4.34\) & \(34.1\pm 0.30\) & \(51.2\pm 0.00\) & \(50.4\pm 0.51\) & \(51.0\pm 0.00\) \\ satimage & \(80.5\pm 1.5\) & \(83.3\pm 6.3\) & \(43.1\pm 4.18\) & \(83.3\pm 5.5\) & \(75.4\pm 1.43\) & \(58.8\pm 5.08\) & \(52.8\pm 3.69\) & \(86.9\pm 8.71\) & \(87.6\pm 0.89\) \\ fabert & \(23.4\pm 1.0\) & \(20.4\pm 1.04\) & \(13.5\pm 20.15\) & \(32.0\pm 2.84\) & \(22.9\pm 2.99\) & \(34.2\pm 1.02\) & \(41.2\pm 1.13\) & \(41.0\pm 0.00\) & \(46.0\pm 1.32\) \\ gesture segmentation & \(40.7\pm 1.4\) & \(43.1\pm 2.3\) & \(14.2\pm 1.6\) & \(37.7\pm 3.4\) & \(39.7\pm 3.64\) & \(38.6\pm 1.7\) & \(37.6\pm 0.50\) & \(53.6\pm 0.00\) & \(52.8\pm 4.52\) & \(52.1\pm 0.00\) \\ robert & \(22.5\pm 1.5\) & \(24.3\pm 1.0\) & \(43.0\pm 1.4\) & \(26.4\pm 1.6\) & \(12.0\pm 1.0\) & \(21.0\pm 1.0\) & \(13.4\pm 4.5\) & \(45.5\pm 4.08\) & \(44.8\pm 4.53\) & \(45.3\pm 0.42\) \\ artificial-characters & \(25.0\pm 1.3\) & \(23.0\pm 1.6\) & \(8.5\pm 1.0\)

### Evaluation on users with different size of labeled data

In the previous section, we showed that using a single learnware with heterogeneous feature spaces outperforms training models from scratch when labeled data is limited. Now, we analyze how performance changes as users train models and ensemble their predictions with learnware across different amounts of labeled data. Figures 4 and 5 display these trends for classification and regression tasks. These figures indicate that ensemble methods consistently outperform self-training with 100 labeled data points. With 500 labeled data points, the ensemble method still performs better in nearly 80% of cases. Additionally, learnware continues to enhance performance even with 5000 labeled samples, improving 21% of classification cases and 50% of regression cases. For certain datasets like kin8nm in regression tasks, even when users use their entire training dataset, the recommended heterogeneous learnware can still significantly boost performance.

## 7 Conclusion

This paper evolves specifications to a unified subspace with explicit exploitation of model outputs under the heterogeneous learnware scenario. The specification is extended by additionally encoding conditional distribution to better encode the model capability, which can be further evolved by more effective subspace learning enriched by label information. The extended specification also improves learnware identification by additionally matching conditional distributions. We present the complete workflow of the learnware dock system accommodating heterogeneous learnwares and validate the effectiveness of the proposed methods through extensive experiments.

Figure 4: User performance curve for classification tasks.

Figure 5: User performance curve for regression tasks.

## Acknowledgments

This research was supported by NSFC (62250069) and the Program for Outstanding PhD Candidates of Nanjing University (202401B07). The authors would like to thank Jian-Dong Liu and Jia-Wei Shan for helpful discussions. We are also grateful for the anonymous reviewers for their valuable comments.

## References

* Zhou (2016) Zhi-Hua Zhou. Learnware: on the future of machine learning. _Frontiers of Computer Science_, 10:589-590, 2016.
* Zhou and Tan (2024) Zhi-Hua Zhou and Zhi-Hao Tan. Learnware: small models do big. _Science China Information Sciences_, 67(1):112102, 2024.
* Tan et al. (2024a) Zhi-Hao Tan, Jian-Dong Liu, Xiao-Dong Bi, Peng Tan, Qin-Cheng Zheng, Hai-Tian Liu, Yi Xie, Xiao-Chuan Zou, Yang Yu, and Zhi-Hua Zhou. Beimingwu: A learnware dock system. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 5773-5782, 2024a.
* Liu et al. (2024) Jian-Dong Liu, Zhi-Hao Tan, and Zhi-Hua Zhou. Towards making learnware specification and market evolvable. In _Proceedings of the 38th AAAI Conference on Artificial Intelligence_, pages 13909-13917, 2024.
* Xie et al. (2023) Yi Xie, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Identifying helpful learnwares without examining the whole market. In _Proceedings of the 26th European Conference on Artificial Intelligence_, pages 2752-2759, 2023.
* Zhang et al. (2021) Yu-Jie Zhang, Yu-Hu Yan, Peng Zhao, and Zhi-Hua Zhou. Towards enabling learnware to handle unseen jobs. In _Proceedings of the 35th AAAI Conference on Artificial Intelligence_, pages 10964-10972, 2021.
* Biedermann et al. (2021) Patricia Biedermann, Rose Ong, Alexander Davydov, Alexandra Orlova, Philip Solovyev, Hong Sun, Graham Wetherill, Monika Brand, and Eva-Maria Didden. Standardizing registry data to the omp common data model: experience from three pulmonary hypertension databases. _BMC Medical Research Methodology_, 21:1-16, 2021.
* Wang and Sun (2022) Zifeng Wang and Jimeng Sun. Transtab: Learning transferable tabular transformers across tables. In _Advances in Neural Information Processing Systems 35_, pages 2902-2915, 2022.
* Zhu et al. (2023) Bingzhao Zhu, Xingjian Shi, Nick Erickson, Mu Li, George Karypis, and Mahsa Shoaran. Xtab: Cross-table pretraining for tabular transformers. In _Proceedings of 40th International Conference on Machine Learning_, pages 43181-43204, 2023.
* Xu et al. (2013) Chang Xu, Dacheng Tao, and Chao Xu. A survey on multi-view learning. _arXiv:1304.5634_, 2013.
* Huang et al. (2023) Weitian Huang, Sirui Yang, and Hongmin Cai. Generalized information-theoretic multi-view clustering. In _Advances in Neural Information Processing Systems 36_, pages 58752-58764, 2023.
* Tan et al. (2023) Peng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Handling learnwares developed from heterogeneous feature spaces without auxiliary data. In _Proceedings of the 32nd International Joint Conference on Artificial Intelligence_, pages 4235-4243, 2023.
* Scholkopf and Smola (2002) Bernhard Scholkopf and Alexander Johannes Smola. _Learning with kernels: Support vector machines, regularization, optimization, and beyond_. MIT press, 2002.
* Wu et al. (2023) Xi-Zhu Wu, Wenkai Xu, Song Liu, and Zhi-Hua Zhou. Model reuse with reduced kernel mean embedding specification. _IEEE Transactions on Knowledge and Data Engineering_, 35(1):699-710, 2023.
* Ucar et al. (2021) Talip Ucar, Ehsan Hajiramezanali, and Lindsay Edwards. Subtab: Subsetting features of tabular data for self-supervised representation learning. In _Advances in Neural Information Processing Systems 34_, pages 18853-18865, 2021.
* Wu et al. (2021)Dara Bahri, Heinrich Jiang, Yi Tay, and Donald Metzler. Scarf: Self-supervised contrastive learning using random feature corruption. In _The 10th International Conference on Learning Representations_, 2022.
* Xu and Gong (2004) Wei Xu and Yihong Gong. Document clustering by concept factorization. In _Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 202-209, 2004.
* Wang et al. (2016) Hao Wang, Yan Yang, and Tianrui Li. Multi-view clustering via concept factorization with local manifold regularization. In _Proceedings of the 16th International Conference on Data Mining_, pages 1245-1250, 2016.
* McElfresh et al. (2023) Duncan McElfresh, Sujay Khandagale, Jonathan Valverde, Vishak Prasad C, Ganesh Ramakrishnan, Micah Goldblum, and Colin White. When do neural nets outperform boosted trees on tabular data? In _Advances in Neural Information Processing Systems 36_, pages 76336-76369, 2023.
* Ke et al. (2017) Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu. Lightgbm: A highly efficient gradient boosting decision tree. In _Advances in Neural Information Processing Systems 30_, pages 3146-3154, 2017.
* Hollmann et al. (2023) Noah Hollmann, Samuel Muller, Katharina Eggensperger, and Frank Hutter. Tabpfn: A transformer that solves small tabular classification problems in a second. In _The 11th International Conference on Learning Representations_, 2023.
* Yang et al. (2023) Yazheng Yang, Yuqi Wang, Guang Liu, Ledell Wu, and Qi Liu. Unitabe: Pretraining a unified tabular encoder for heterogeneous tabular data. _arXiv:2307.09249_, 2023.
* Guo et al. (2024) Siyuan Guo, Jonas Bernhard Wildberger, and Bernhard Scholkopf. Out-of-variable generalisation for discriminative models. In _The 12th International Conference on Learning Representations_, 2024.
* Tan et al. (2024) Peng Tan, Zhi-Hao Tan, Yuan Jiang, and Zhi-Hua Zhou. Towards enabling learnware to handle heterogeneous feature spaces. _Machine Learning_, 113(4):1839-1860, 2024b.
* Guo et al. (2023) Lan-Zhe Guo, Zhi Zhou, Yu-Feng Li, and Zhi-Hua Zhou. Identifying useful learnwares for heterogeneous label spaces. In _Proceedings of the 40th International Conference on Machine Learning_, pages 12122-12131, 2023.
* Alvarez-Melis and Fusi (2020) David Alvarez-Melis and Nicolo Fusi. Geometric dataset distances via optimal transport. In _Advances in Neural Information Processing Systems 33_, 2020.
* Memoli (2011) Facundo Memoli. Gromov-wasserstein distances and the metric approach to object matching. _Foundations of Computational Mathematics_, 11(4):417-487, 2011.
* Duan et al. (2012) Lixin Duan, Dong Xu, and Ivor W. Tsang. Learning with augmented features for heterogeneous domain adaptation. In _Proceedings of the 29th International Conference on Machine Learning_, 2012.
* Wang and Mahadevan (2011) Chang Wang and Sridhar Mahadevan. Heterogeneous domain adaptation using manifold alignment. In _Proceedings of the 22th International Joint Conference on Artificial Intelligence_, 2011.
* Day and Khoshgoftaar (2017) Oscar Day and Taghi M Khoshgoftaar. A survey on heterogeneous transfer learning. _Journal of Big Data_, 4:1-42, 2017.
* Ye et al. (2018) Han-Jia Ye, De-Chuan Zhan, Yuan Jiang, and Zhi-Hua Zhou. Rectify heterogeneous models with semantic mapping. In _Proceedings of the 37th International Conference on Machine Learning_, pages 5630-5639, 2018.
* Ye et al. (2020) Han-Jia Ye, De-Chuan Zhan, Yuan Jiang, and Zhi-Hua Zhou. Heterogeneous few-shot model rectification with semantic mapping. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 43(11):3878-3891, 2020.
* Ding and Zhou (2020) Yao-Xiang Ding and Zhi-Hua Zhou. Boosting-based reliable model reuse. In _Proceedings of the 12th Asian Conference on Machine Learning_, pages 145-160, 2020.
* Ding et al. (2018)Yao-Xiang Ding, Xi-Zhu Wu, Kun Zhou, and Zhi-Hua Zhou. Pre-trained model reusability evaluation for small-data transfer learning. In _Advances in Neural Information Processing Systems 35_, pages 37389-37400, 2022.
* Zhang et al. (2023) Yi-Kai Zhang, Ting-Ji Huang, Yao-Xiang Ding, De-Chuan Zhan, and Han-Jia Ye. Model spider: Learning to rank pre-trained models efficiently. In _Advances in Neural Information Processing Systems 36_, pages 13692-13719, 2023.
* Yi et al. (2024) Chao Yi, De-Chuan Zhan, and Han-Jia Ye. Bridge the modality and capacity gaps in vision-language model selection. _arXiv:2403.13797_, 2024.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 770-778, 2016.
* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun, editors, _The 3rd International Conference on Learning Representations_, 2015.

## Table of contents

* Section A presents the key notations used throughout this work.
* Section B offers an in-depth discussion on the proposed methods.
* Section C reviews the related literature and techniques.
* Section D describes the detailed algorithm of the overall procedure.
* Section E explains the optimization during specification generation and user requirement generation.
* Section F provides additional information on the experiments.

## Appendix A Notations

The major notations of this paper are summarized in Table 3.

## Appendix B More discussion

### Superiority of the learnware paradigm for handling heterogeneous models

Difficulties of managing models with heterogeneous feature spaces.To manage models developed from heterogeneous feature spaces, it is essential to exploit the relationship among the corresponding feature spaces \(\{\mathcal{X}_{i}^{\text{dev}}\}_{i=1}^{N}\). While multi-view learning [20] can be beneficial if co-occurrence data across the entire feature space \(\mathcal{X}_{\text{all}}\) is available, obtaining such data is nearly impossible in real-world scenarios. Alternatively, if the raw data of the model task is accessible,

\begin{table}
\begin{tabular}{c|c|l} \hline \hline
**Category** & **Notations** & **Description** \\ \hline \multirow{6}{*}{**basic**} & \(\mathcal{X}_{\text{all}}=\mathcal{X}_{1}\times\cdots\times\mathcal{X}_{Q}\) & the overall feature space and its \(Q\) blocks, the corresponding dimensions are \(d,d_{1},\cdots,d_{Q}\). \\  & \(\mathcal{X}_{1}^{\text{dev}},\cdots,\mathcal{X}_{Q}^{\text{dev}}\) & \(T\) kinds of feature spaces for developers’ task, each of them is the Cartesian product of several feature blocks. The index set of blocks that \(\mathcal{X}_{k}^{\text{dev}}\) has is \(C_{k}\), i.e., \(\mathcal{X}_{k}^{\text{dev}}=\times_{i\in C_{k}}\mathcal{X}_{i}\). \\  & \(\mathcal{X}^{\text{user}}\) & the feature space of the user’s task, it is the Cartesian product of several feature blocks, the corresponding index set of blocks is \(C_{0}\), i.e., \(\mathcal{X}^{\text{user}}=\times_{i\in C_{0}}\mathcal{X}_{i}\). \\  & \(\mathcal{Y}\) & the label space. \\ \hline \multirow{6}{*}{**developer**} & \(D_{i}:=\{(\bm{x}_{ij},y_{ij})\}_{j=1}^{n_{i}}\) & the labeled dataset of the \(i\)-th developer defined on \(\mathcal{X}_{\phi_{i}}^{\text{dev}}\times\mathcal{Y}\) where \(\phi_{i}\in[1,\cdots,T]\) represents the developer feature space index. \\  & \(f_{i}:\mathcal{X}_{\phi_{i}}^{\text{dev}}\rightarrow\mathcal{Y}\) & the model of the \(i\)-th developer trained on \(D_{i}\). \\  & \(\bm{s}_{i}^{\text{dev}}:=\{(\beta_{ij},\bm{z}_{ij})\}_{j=1}^{m_{i}}\) & the _developer-level_ specification of the \(i\)-th model generated from \(D_{i}\) via RKME\({}_{\text{c}}\). \\ \hline \multirow{2}{*}{**user**} & \(D_{i}^{\text{dev}}=\{\bm{x}_{\phi_{i}}\}_{i=1}^{n_{\text{su}}}\) & the unlabeled dataset of the user. \\  & \(D_{0}^{\text{dev}}=\{(\bm{x}_{\phi_{0}},y_{0})\}_{j=1}^{n_{\text{vel}}}\) & the user’s labeled dataset, which is of _limited size_ \\  & \(\bm{s}_{0}^{\text{dev}}:=\{(\beta_{ij},\bm{z}_{0})\}_{j=1}^{m_{\text{vel}}}\) & the _user-level_ requirement of the user generated from \(D_{0}\) via RKME\({}_{\text{c}}\). \\ \hline \multirow{6}{*}{**learnware**} & \(\bm{s}_{i}:=\{(\beta_{ij},\bm{v}_{ij})\}_{j=1}^{m_{i}}\) & the _system-level_ specification of the \(i\)-th model assigned by the learner- \\  & \(\bm{l}_{i}:=(f_{i},\bm{s}_{i})\) & the \(i\)-th learnware accommodated by the learnware dock system. \\  & \(\bm{s}_{0}:=\{(\beta_{ij},\bm{v}_{0j})\}_{j=1}^{m_{0}}\) & the heterogeneous learnware dock system. \\ \hline \multirow{6}{*}{**subspace**} & \(\mathcal{X}_{\text{sub}}\) & the learned subspace with dimension \(d_{\text{sub}}\). \\  & \(h_{k}:\mathcal{X}_{k}\mapsto\mathcal{X}_{\text{sub}}\) & the mapping function which projects the data on the \(k\)-th feature block \(\mathcal{X}_{\text{sub}}\) to the subspace \(\mathcal{X}_{\text{sub}}\). \\ \cline{1-1}  & \(g_{k}:\mathcal{X}_{\text{sub}}\mapsto\mathcal{X}_{k}\) & the mapping function which reconstructs the data on the subspace \(\mathcal{X}_{\text{sub}}\) to the \(k\)-th feature block \(\mathcal{X}_{k}\). \\ \hline \hline \end{tabular}
\end{table}
Table 3: Major notations of this work.

training a unified tabular network on heterogeneous tables (Wang and Sun, 2022; Zhu et al., 2023; Yang et al., 2023) offers another approach to reusing knowledge from heterogeneous tasks. However, a unified model often struggles to perform well across all source tasks due to complex and sometimes conflicting internal patterns. Additionally, in sensitive areas like medicine, data sharing is restricted, and privacy concerns prevent access to raw data. _In our problem, raw data is inaccessible to protect the model provider's privacy, and we do not use hard-to-collect auxiliary data._

Exploit feature space relationship with model specifications.Under the learnware paradigm, each model is submitted with a specification that describes its abilities. This specification can be naturally used to explore the relationships between feature spaces. (Tan et al., 2023) generates a unified subspace \(\mathcal{X}_{\text{sub}}\) and linear projection functions linking it to all feature blocks \(\{\mathcal{X}_{i}\}_{i=1}^{Q}\) by leveraging model RKME specifications generated solely on features. However, the specification lack of label information often leads to unsatisfactory performance of subspace learning, like entangled classes of embeddings, it also performs poorly when feature blocks are weakly dependent. This paper better incorporates label information into the specification to improve subspace learning for better heterogeneous learnware accommodation.

### Exploit label information to handle models with heterogeneous feature spaces

To handle learnwares with heterogeneous feature spaces, it's crucial to exploit the relationship between different feature spaces. When the overall feature space is divided into disjoint blocks and each task data is an arbitrary Cartesian product of several blocks, this exploitation can be divided into two parts: learning relationships between co-occurring feature spaces using data of specific specification, and learning relationships between non-co-occurring feature spaces across all specifications. The first part involves subspace learning to identify a unified subspace, while the second ensures that embeddings of heterogeneous data with intersecting features are closely aligned in the subspace. The first part lays foundation for the second, which is key to managing models with heterogeneous feature spaces.

For the foundational step of subspace learning, if label information is not available, the embeddings of different slices of the same data may not align correctly within the subspace. This misalignment can result in embeddings from different slices having entangled or mixed classes. Furthermore, in extreme cases where feature blocks are jointly independent, subspace learning becomes meaningless due to the irrelevance of features. However, when label information is available, the feature blocks are no longer independent, as all information is used to generate the labels. More discussion that the label information is useful for building connections between independent feature blocks can be found in (Guo et al., 2024).

In summary, label information is essential for subspace learning, as it mitigate entangled classes of embeddings, which critically affects the learnware identification and reuse. It also help ensure performance even when feature blocks are weakly dependent.

## Appendix C Related work

The learnware paradigm.The learnware paradigm (Zhou, 2016; Zhou and Tan, 2024) offers a systematic approach to managing well-trained models and leveraging their capabilities to assist users in solving their tasks, rather than training a model from scratch. A learnware consists of a well-trained model accompanied by a specification that describes its capabilities, with this specification being the central component of the learnware. Wu et al. (2023) proposed the RKME specification, which uses a reduced set to sketch the distribution of the task data. Based on the RKME specification, Wu et al. (2023) proposed to match the data distribution for learnware identification, while Zhang et al. (2021) extended it to handle user tasks with unseen parts. To efficiently recommend learnwares among numerous learnwares, Liu et al. (2024) suggested evolving the specification with other learnwares for more accurate identifications and construct the specification index for managing learnwares for efficient learnware search, Xie et al. (2023) proposed using minor representative learnwares as anchors to speed up learnware identification without traversing the whole system.

Previous research has primarily focused on the homogeneous case, where all models and user tasks share the same feature space. However, in real-world applications, the feature spaces of developer models and user tasks often differ. Tan et al. (2024) was the first to consider the heterogeneous feature space scenario, but it assumes that the original training data is accessible, and auxiliary data across the entire feature space is collected. To relax this strong assumption of data accessibility, Tan et al. (2023) investigated the organization and utilization of a heterogeneous learnware dock system without requiring access to the original data or auxiliary data across the feature space. While this approach is more realistic, its lack of effective use of label information leads to unsatisfactory performance. This paper examines the importance of label information and integrates it throughout the entire process of the heterogeneous learnware dock system. As a broader impact, the detailed implementation of incorporating label information into the learnware specification can help enhance various aspects of the learnware paradigm. In addition to research on heterogeneous feature spaces, Guo et al. (2023) considered scenarios involving heterogeneous labels.

Based on above research, the first learnware docking system, Beimingwu (Tan et al., 2024), was recently released. The system streamlines the entire learnware process and provides a highly scalable architecture, facilitating future algorithm implementation and experimental research.

Related techniques.To measure the distance between two _labeled_ datasets in the same feature space, Alvarez-Melis and Fusi (2020) proposed the Optimal Transport Dataset Distance (OTDD). This approach separately calculates feature and label distances using optimal transport and then combines them. The label distance is derived from the feature distances of partial samples with specific labels. This method aligns with the proposed loss for sketching the labeled dataset to generate specifications and requirements, where the loss consists of feature and label components based on MMD, with the label loss defined by the conditional distributions \(P(X|Y)\). Comparing simply concatenating feature and label, separately tackling feature and label can better measure the distance without label information overwhelmed by the longer feature information. For measuring the distance between two distributions in _different feature spaces_, Memoli (2011) proposed the Gromov-Wasserstein distance, which aggregates all distances of tetrads to measure the distances between two points. In our work, we introduce a method to measure the distance between two labeled datasets in heterogeneous feature spaces using subspace learning and maximum mean discrepancy (MMD) over marginal and conditional distributions.

Existing studies on heterogeneous feature spaces, such as heterogeneous domain adaptation (Duan et al., 2012; Wang and Mahadevan, 2011), heterogeneous transfer learning (Day and Khoshgoftaar, 2017), and heterogeneous model reuse (Ye et al., 2018, 2020), generally map different feature spaces to an intermediate subspace. This process typically requires original data from both domains or co-occurrence data to establish the relationship between different spaces. However, in the learnware paradigm, managing models developed from different feature spaces without auxiliary data becomes feasible due to the existence of RKME specifications associated with each model. Based on this paradigm, we can accommodate, identify, and reuse heterogeneous models of any type without accessing original data or additional co-occurrence auxiliary data. Recently, Guo et al. (2024) explored the relationship between two intersecting feature spaces from a causal perspective, showing that residuals from model predictions can provide information into unobserved variables, specifically, the partial derivative of the true generating function with respect to these unobserved variables. This finding aligns with our approach, where we leverage label information to explore the relationship of different feature spaces.

Recently, some works have focused on identifying and reusing models from a model hub. Ding and Zhou (2020) select models based on an anomaly detector associated with each model, which helps determine whether a feature sample is appropriate for prediction with that model. Ding et al. (2022) propose selecting models using a task-model metric that requires only minimal interaction with data providers. Zhang et al. (2023) propose selecting pre-trained models by calculating the similarity between learned model embeddings and task embeddings, both of which are obtained through the ranking loss. Yi et al. (2024) explore the model selection specifically for visual language models. However, it is important to note that none of these approaches can directly address scenarios involving heterogeneous feature spaces.

## Appendix D Algorithm details

### Summary of overall procedure

The overall procedure of the heterogeneous learnware dock system consists of two stages. In the submission stage, the dock system receives models with developer-level specifications sketching model capabilities and assigns system-level specifications using a learned unified subspace. In the deployment stage, users submit task requirements detailing marginal and conditional distributions to receive the recommended learnware. This learnware can be integrated with their self-trained models to significantly enhance performance. The detailed procedures for each stage are outlined in Algorithm 1 and 2, respectively.

```
1: Each developer trains a model \(f_{i}\) and generates the developer-level specification \(\bm{s}_{i}^{\text{dev}}\) on the dataset \(D_{i}\) defined on \(\mathcal{X}_{i}^{\text{dev}}\times\mathcal{Y}\).
2: Each developer submits both the model and the developer-level specification \((f_{i},\bm{s}_{i}^{\text{dev}})\) to the learnware dock system.
3: The learnware dock system generates the projection function \(h_{i}:\mathcal{X}_{i}\mapsto\mathcal{X}_{\text{sub}}\), reconstruction function \(g_{i}:\mathcal{X}_{\text{sub}}\mapsto\mathcal{X}_{i}\)_for each feature block \(\mathcal{X}_{i}\) and the system-level specification \(\bm{s}_{i}\) for each model \(f_{i}\) based on all developer-level specifications \(\{\bm{s}_{i}^{\text{dev}}\}_{i=1}^{N}\).
4: The model \(f_{i}\) is accommodated by the learnware dock system with system-level specifications as learnware \(\bm{l}_{i}:=\{f_{i},\bm{s}_{i}\}\).
5: The heterogeneous learnware dock system is established as \(\{\bm{l}_{i}\}_{i=1}^{N}\). ```

**Algorithm 1** Submitting stage (learnware accommodation by the system)

### Subspace learning and system-level specification generation

After the developer-level specification \(\bm{s}^{\text{dev}}\) generation, the developer submits the model \(f\) with specification \(\bm{s}^{\text{dev}}\) to the learnware dock system. The system exploits the relationship of different feature spaces and manages heterogeneous models by assigning system-level specification \(\bm{s}^{\text{sys}}\). The details of subspace learning and system-level specification generation are described in Algorithm 3.

## Appendix E Optimization

### Optimization of specification generation specialized for classification tasks

The \(\mathtt{R}\mathtt{R}\mathtt{R}\mathtt{L}\) specification represented by \(R_{L}=(\bm{\beta},Z,Y)=\{(\beta_{m},z_{m},y_{m})\}_{m=1}^{M}\) sketches both the marginal distribution \(P_{X}\) of the training data and the conditional distribution \(P_{X|Y}\) of the model prediction, which is obtained through minimizing the following objective over \(R_{L}\):

\[\left\|\sum_{n=1}^{N}\frac{1}{N}k\left(\bm{x}_{n},\cdot\right)- \sum_{m=1}^{M}\beta_{m}k\left(\bm{z}_{m},\cdot\right)\right\|_{\mathcal{H}_{k} }^{2}+\theta\sum_{c=1}^{C}\left\|\sum_{n\in\mathcal{I}_{c}}\frac{1}{N}k\left( \bm{x}_{n},\cdot\right)-\sum_{m\in\mathcal{I}_{c}^{\prime}}\beta_{m}k\left( \bm{z}_{m},\cdot\right)\right\|_{\mathcal{H}_{k}}^{2},\] (5)

This objective consists of two parts: the first term sketches the marginal distribution \(P_{X}\) of the training data, while the second term sketches the conditional distribution \(P_{X|Y}\) of the model prediction. This objective can be optimized by iterative optimization, which is detailed in the following.

Denote \(\bm{\beta}=(\beta_{1},\cdots,\beta_{M})\), \(Z=\{\bm{z}_{1},\cdots,\bm{z}_{M}\}\) and \(Y=\{y_{1},\cdots,y_{M}\}\), expanding Eq. (5) gives

\[F(\bm{\beta},Z,Y)=\sum_{n,m=1}^{N}\frac{1}{N^{2}}k\left(\bm{x}_{n},\bm{x}_{m}\right)+\sum_{n,m=1}^{M}\beta_{n}\beta_{m}k\left(\bm{z}_{n},\bm{z}_{ m}\right)-2\sum_{n=1}^{N}\sum_{m=1}^{M}\frac{\beta_{m}}{N}k\left(\bm{x}_{n},\bm{z}_{ m}\right)\] \[\quad+\theta\sum_{c=1}^{C}\left(\sum_{n,m\in\mathcal{I}_{c}}\frac {1}{N^{2}}k\left(\bm{x}_{n},\bm{x}_{m}\right)+\sum_{n,m\in\mathcal{I}_{c}^{ \prime}}\beta_{n}\beta_{m}k\left(\bm{z}_{n},\bm{z}_{m}\right)-2\sum_{n\in \mathcal{I}_{c}}\sum_{m\in\mathcal{I}_{c}^{\prime}}\frac{\beta_{m}}{N}k\left( \bm{x}_{n},\bm{z}_{m}\right)\right).\]

The distance \(F(\bm{\beta},Z,Y)\) can also be rewritten as

\[\alpha^{T}K_{xx}\alpha+\beta^{T}K_{zz}\beta-2\alpha^{T}K_{xz} \beta+\theta\sum_{c=1}^{C}\left(\alpha_{c}^{T}K_{x_{c}x_{c}}\alpha_{c}+\beta_{ c}^{T}K_{z_{c}z_{c}}\beta_{c}-2\alpha_{c}^{T}K_{x_{c}z_{c}}\beta_{c}\right)\] \[=\alpha^{T}(K_{xx}+\theta K_{xx}^{b})\alpha+\beta^{T}(K_{zz}+ \theta K_{zz}^{b})\beta-2\alpha^{T}(K_{xz}+\theta K_{xz}^{b})\beta\] \[=\alpha^{T}K_{xx}^{{}^{\prime}}\alpha+\beta^{T}K_{zz}^{{}^{ \prime}}\beta-2\alpha^{T}K_{xz}^{{}^{\prime}}\beta,\]

where \(\alpha\) is the vector containing \(N\) elements of 1.

Next, we address the optimization of Eq. (5). First, we generate \(Y\) while preserving the class ratio of the original labels \(\{y_{n}\}_{n=1}^{N}\). Then, we proceed to optimize \(\bm{\beta}\) and \(Z\).

Fix \(Z\) and update \(\beta\).Suppose vectors in \(Z\) are fixed, setting \(\frac{\partial F(\bm{\beta},Z)}{\partial\bm{\beta}}=0\) obtains the closed-form solution of \(\bm{\beta}\) using pseudo-inverse of \(K_{zz}^{{}^{\prime}}\) :

\[\bm{\beta}=(K_{zz}^{{}^{\prime}})^{\dagger}K_{zx}^{{}^{\prime}}\alpha.\]

Fix \(\beta\) and update \(Z\).When \(\bm{\beta}\) is fixed, \(\{z_{1},\cdots,z_{M}\}\) in \(Z\) are independent in Eq. (5), therefore we can iteratively run gradient descent on each \(z_{m}\) as

\[\bm{z}_{m}^{(t)}=\bm{z}_{m}^{(t-1)}-\eta\frac{\partial F(\bm{\beta},Z)}{ \partial\bm{z}_{m}}.\]

We first review the gradient \(\frac{\partial G(\bm{\beta},Z)}{\partial\bm{z}_{m}}\):

\[\frac{\partial G(\bm{\beta},Z)}{\partial\bm{z}_{m}} =2\sum_{n=1}^{M}\beta_{n}\beta_{m}\frac{\partial k(\bm{z}_{n},\bm {z}_{m})}{\partial\bm{z}_{m}}-2\sum_{n=1}^{N}\frac{\beta_{m}}{N}\frac{ \partial k(\bm{x}_{n},\bm{z}_{m})}{\partial\bm{z}_{m}}\] \[=2\sum_{n=1}^{M}\beta_{n}\beta_{m}(-2\gamma k(\bm{z}_{n},\bm{z}_ {m})(\bm{z}_{m}-\bm{z}_{n}))-2\sum_{n=1}^{N}\frac{\beta_{m}}{N}(-2\gamma k(\bm {x}_{n},\bm{z}_{m})(\bm{z}_{m}-\bm{x}_{n}))\] \[=-4\gamma\beta_{m}(\sum_{n=1}^{M}\beta_{n}k(\bm{z}_{n},\bm{z}_{m}) (\bm{z}_{m}-\bm{z}_{n})-\frac{1}{N}\sum_{n=1}^{N}k(\bm{x}_{n},\bm{z}_{m})(\bm {z}_{m}-\bm{x}_{n}))\]

then, we consider the gradient in our problem \(\frac{\partial F(\bm{\beta},Z)}{\partial z_{m}}\), which is calculated as follows:

\[2\sum_{n=1}^{M}\beta_{n}\beta_{m}\frac{\partial k(\bm{z}_{n},\bm {z}_{m})}{\partial z_{m}}-2\sum_{n=1}^{N}\frac{\beta_{m}}{N}\frac{\partial k( \bm{x}_{n},\bm{z}_{m})}{\partial\bm{z}_{m}}\] \[\quad+\theta\left(2\sum_{n\in\mathcal{I}_{c}^{\prime}}\beta_{n} \beta_{m}\frac{\partial k(\bm{z}_{n},\bm{z}_{m})}{\partial\bm{z}_{m}}-2\sum_{ n\in\mathcal{I}_{c}}\frac{\beta_{m}}{N}\frac{\partial k(\bm{x}_{n},\bm{z}_{m})}{ \partial\bm{z}_{m}}\right)\] \[=2\sum_{n=1}^{M}\beta_{n}\beta_{m}(-2\gamma k(\bm{z}_{n},\bm{z}_ {m})(\bm{z}_{m}-\bm{z}_{n}))-2\sum_{n=1}^{N}\frac{\beta_{m}}{N}(-2\gamma k( \bm{x}_{n},\bm{z}_{m})(\bm{z}_{m}-\bm{x}_{n}))\] \[\quad+\theta\left(2\sum_{n\in\mathcal{I}_{c}^{\prime}}\beta_{n} \beta_{m}(-2\gamma k(\bm{z}_{n},\bm{z}_{m})(\bm{z}_{m}-\bm{z}_{n}))-2\sum_{n \in\mathcal{I}_{c}}\frac{\beta_{m}}{N}(-2\gamma k(\bm{x}_{n},\bm{z}_{m})(\bm {z}_{m}-\bm{x}_{n}))\right)\] \[= -4\gamma\beta_{m}\left(\sum_{n=1}^{M}\beta_{n}k(\bm{z}_{n},\bm{z }_{m})(\bm{z}_{m}-\bm{z}_{n})-\frac{1}{N}\sum_{n=1}^{N}k(\bm{x}_{n},\bm{z}_{m}) (\bm{z}_{m}-\bm{x}_{n})\right.\] \[\quad+\left.\theta\left(\sum_{n\in\mathcal{I}_{c}^{\prime}}\beta _{n}k(z_{n},z_{m})(z_{m}-z_{n})-\frac{1}{N}\sum_{n\in\mathcal{I}_{c}}k(x_{n},z _{m})(z_{m}-x_{n})\right)\right)\] \[= -4\gamma\beta_{m}\left(\sum_{n=1}^{M}\beta_{n}(1+\theta\mathbb{I} (y_{z_{n}}=c))k(\bm{z}_{n},\bm{z}_{m})(\bm{z}_{m}-\bm{z}_{n})\right.\] \[\quad\left.-\frac{1+\theta\mathbb{I}(y_{\bm{x}_{n}}=c)}{N}\sum_{n =1}^{N}k(\bm{x}_{n},\bm{z}_{m})(\bm{z}_{m}-\bm{x}_{n})\right),\]

where \(\mathcal{I}_{c}\) is the sample indices of class c of sample \(\bm{z}_{m}\).

### Optimization of user requirement generation

When the user has major unlabeled data \(D^{u}=\{\bm{x}_{i}\}_{i=1}^{N_{l}}\) and limited labeled data \(D^{l}=\{(\tilde{\bm{x}}_{i},y_{i})\}_{i=1}^{N_{u}}\) (We adjust the notation slightly for a clearer description of the optimization process), we can also 

[MISSING_PAGE_FAIL:19]

## Appendix F Experiments

### More details for basic information

Dataset details.The basic information of used datasets is summarized in Table 4 and Table 5.

Experiment configuration.Each dataset is split into training and test sets with a 4:1 ratio [McElfresh et al., 2023]. The output of regression tasks is scaled to [0,1]. The feature space is randomly divided into four equal blocks. We create four feature spaces for developer tasks from all three-block combinations and six feature spaces for user tasks from all two-block combinations. Our encoder, decoder, and system classifier are two-layer ResNets [He et al., 2016] for tabular data, with subspace and hidden layer dimensions set to 16 and 32, respectively. We optimize using Adam [Kingma and Ba, 2015]. For user tasks, we sample 100 labeled data points from the training set, using stratified

\begin{table}
\begin{tabular}{l c c c} \hline \hline \multicolumn{1}{c}{**Dataset name**} & **\#classes** & **\#features** & **\#instances** \\ \hline openml\_credit\_g\_31 & 1 & 20 & 1000 \\ \hline openml\_semicoin\_9964 & 10 & 256 & 1593 \\ \hline openml\_mfeat\_kathunen\_16 & 10 & 64 & 2000 \\ \hline openml\_splice\_45 & 3 & 60 & 3190 \\ \hline openml\_gina\_agnostic\_3891 & 1 & 970 & 3468 \\ \hline openml\_Bioresponse\_9910 & 1 & 1776 & 3751 \\ \hline openml\_sytvine\_168912 & 1 & 20 & 5124 \\ \hline openml\_christine\_168908 & 1 & 1636 & 5418 \\ \hline openml\_first-order-theorem-proving\_9985 & 6 & 51 & 6118 \\ \hline openml\_satimage\_2074 & 6 & 36 & 6430 \\ \hline openml\_fabert\_168910 & 7 & 800 & 8237 \\ \hline openml\_GesturePhaseSegmentationProcessed\_14969 & 5 & 32 & 9873 \\ \hline openml\_robert\_168332 & 10 & 7200 & 10000 \\ \hline openml\_artificial-characters\_14964 & 10 & 7 & 10218 \\ \hline openml\_ey\_movements\_3897 & 3 & 27 & 10936 \\ \hline openml\_unrsurvey\_9892 & 4 & 8 & 12958 \\ \hline openml\_eeg-eye-state\_14951 & 1 & 14 & 14980 \\ \hline openml\_maagic\_146206 & 1 & 10 & 19020 \\ \hline openml\_riccardo\_168338 & 1 & 4296 & 20000 \\ \hline openml\_guilermo\_168337 & 1 & 4296 & 20000 \\ \hline openml\_nomao\_9977 & 1 & 118 & 34465 \\ \hline openml\_Click\_prediction\_small\_190408 & 1 & 11 & 39948 \\ \hline openml\_volker\_168331 & 10 & 180 & 58310 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Details for classification tasks

Figure 6: summarized mechanisms for \(\mathtt{RKME_{L}}\) generation.

[MISSING_PAGE_FAIL:21]

the performance of decoding functions is matched with the encoding functions. Despite the initial effectiveness of unsupervised subspace learning, further enhancements are achieved by incorporating label information through our proposed specification. With the addition of supervised loss, we ultimately attain optimal performance for both classification and regression tasks.

### Code availability

The code can be found at https://github.com/LAMDA-TP/Hetero-Learnware-Label-Info.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & contrastive & + reconstruction & + supervised \\ \hline acc(\%) & 61.1 & 66.2 & **67.4** \\ \hline \hline \end{tabular} 
\begin{tabular}{c c c c} \hline \hline  & contrastive & + reconstruction & + supervised \\ \hline rmse & 0.208 & 0.200 & **0.194** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Ablation study for subspace learning loss by _adding loss in sequence_ for classification tasks.

Figure 7: All user performance curve for classification tasks.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions supported by the remaining sections. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: This paper assumes that all models with heterogeneous feature spaces share the same label space. However, this assumption can be further extended to include heterogeneous label spaces as well. For example, a basic way is to recommend multiple learners by distribution matching and reuse learnwares by dynamic classifier selection proposed in previous work [Wu et al., 2023]. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide a detailed description of the experimental setup in Appendix F.1. The specifics of our algorithms are thoroughly explained in Appendix D. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The link to the code can be found in Appendix F.4. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: A detailed description of the experimental setup is provided in Appendix F.1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the standard deviations for performance comparison in our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We report our compute resources in Appendix F.1. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: This paper clearly cites the original paper that produced the code package or dataset. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.