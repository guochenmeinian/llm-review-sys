# A Causal Framework for

Decomposing Spurious Variations

 Drago Plecko

 Elias Bareinboim

Department of Computer Science

Columbia University

dp3144@columbia.edu, eb@cs.columbia.edu

###### Abstract

One of the fundamental challenges found throughout the data sciences is to explain why things happen in specific ways, or through which mechanisms a certain variable \(X\) exerts influences over another variable \(Y\). In statistics and machine learning, significant efforts have been put into developing machinery to estimate correlations across variables efficiently. In causal inference, a large body of literature is concerned with the decomposition of causal effects under the rubric of mediation analysis. However, many variations are spurious in nature, including different phenomena throughout the applied sciences. Despite the statistical power to estimate correlations and the identification power to decompose causal effects, there is still little understanding of the properties of spurious associations and how they can be decomposed in terms of the underlying causal mechanisms. In this manuscript, we develop formal tools for decomposing spurious variations in both Markovian and Semi-Markovian models. We prove the first results that allow a non-parametric decomposition of spurious effects and provide sufficient conditions for the identification of such decompositions. The described approach has several applications, ranging from explainable and fair AI to questions in epidemiology and medicine, and we empirically demonstrate its use.

## 1 Introduction

Understanding the relationships of cause and effect is one of the core tenets of scientific inquiry and the human ability to explain why events occurred in the way they did. Hypotheses on possible causal relations in the sciences are often generated based on observing correlations in the world, after which a rigorous process using either observational or experimental data is employed to ascertain whether the observed relationships are indeed causal. One common way of articulating questions of causation is through the average treatment effect (ATE), also known as the total effect (TE), given by

\[\mathbb{E}[y\mid do(x_{1})]-\mathbb{E}[y\mid do(x_{0})],\] (1)

where \(do(\cdot)\) symbolizes the do-operator [9], and \(x_{0},x_{1}\) are two distinct values attained by the variable \(X\). Instead of just quantifying the causal effect, researchers are more broadly interested in determining which causal mechanisms transmit the change from \(X\) to \(Y\). Such questions have received much attention and have been investigated under the rubric of causal mediation analysis [3, 12, 10, 14].

Often, however, the causal relationship may be entirely absent or account only for a part of the initially observed correlation. In these cases, the spurious (or confounded) variations between \(X\) and \(Y\) play a central role in explaining the phenomenon at hand. Interestingly, though, tools for decomposing spurious variations are almost entirely missing from the literature in causal inference 1.

Phenomena in which spurious variations are of central importance are abundant throughout the sciences. For instance, in medicine, the phenomenon called the _obesity paradox_ signifies the counter-intuitive association of increased body fat with better survival chances in the intensive care unit (ICU) [6]. While the full explanation is still unclear, evidence in the literature suggests that the relationship is not causal [5], i.e., it is explained by spurious variations. Spurious variations also play a central role in many epidemiological investigations [13]. In occupational epidemiology, for example, the relationship of exposure to hazardous materials with cancer is confounded by other hazardous working conditions and lifestyle characteristics [4], and such spurious variations themselves may be the target of scientific inquiry. Quantities that measure such spurious variations (or a subset thereof) are called spurious effects in this paper.

Spurious variations are key in applications of fair and explainable AI as well. For instance, consider the widely recognized phenomenon in the literature known as _redlining_[15; 7], in which the location where loan applicants live may correlate with their race. Applications might be rejected based on the zip code, disproportionately affecting certain minority groups. Furthermore, in the context of criminal justice [8], the association of race with increased probability of being classified as high-risk for recidivism may in part be explained by the spurious association of race with other demographic characteristics (we take a closer look at this issue in Sec. 5). Understanding which confounders affect the relationship, and how strongly, is an important step of explaining the phenomenon, and also determining whether the underlying classifier is deemed as unfair and discriminatory.

These examples suggest that a principled approach for decomposing spurious variations may be a useful addition to the general toolkit of causal inference, and may find its applications in a wide range of settings from medicine and public health all the way to fair and explainable AI. For concreteness, in this paper we will consider the quantity

\[P(y\mid x)-P(y\mid do(x)),\]

which we will call the _experimental spurious effect_ (Exp-SE, for short). This quantity, shown graphically in Fig. 1, captures the difference in variations when observing \(X=x\) vs. intervening that \(X=x\), which can be seen as the spurious counterpart of the total effect. Interestingly, the Exp-SE quantity is sometimes evoked in the causal inference literature, i.e.,

\[P(y\mid x)-P(y\mid do(x))=0\] (2)

is known as the _zero-bias_ condition [2; 9, Ch. 6]. This condition allows one to test for the existence of confounding between the variables \(X\) and \(Y\). A crucial observation is that, in many cases, the quantity itself may be of interest (instead of only its _null_), as it underpins the spurious variations.

Against this background, we note that tools that allow for decomposing the Exp-SE quantity currently do not exist in the literature. Our goal in this manuscript is to fill in this gap, and provide a formalism that allows for non-parametric decompositions of spurious variations. Specifically, our contributions are the following:

1. We introduce the notion of a partially abducted submodel (Def. 1), which underpins the inference procedure called Partial Abduction and Prediction (Alg. 2) (akin to Balke & Pearl 3-step procedure [9, Ch. 7]). Building on this new primitive, we prove the first non-parametric decomposition result for spurious effects in Markovian models (Thm. 1),
2. Building on the insights coming from the new procedure, we prove the decomposition result for settings when unobserved confounding is present (Semi-Markovian models) (Thm. 3).
3. We develop sufficient conditions for identification of spurious decompositions (Thm 2, 4).

## 2 Preliminaries

We use the language of structural causal models (SCMs) as our basic semantical framework [9]. A structural causal model (SCM) is a tuple \(\mathcal{M}:=\langle V,U,\mathcal{F},P(u)\rangle\), where \(V\), \(U\) are sets of endogenous (observables) and exogenous (latent) variables respectively, \(\mathcal{F}\) is a set of functions \(f_{V_{i}}\), one for each \(V_{i}\in V\), where \(V_{i}\gets f_{V_{i}}(\mathrm{pa}(V_{i}),U_{V_{i}})\) for some \(\mathrm{pa}(V_{i})\subseteq V\) and \(U_{V_{i}}\subseteq U\). \(P(u)\) is a strictly positive probability measure over \(U\). Each SCM \(\mathcal{M}\) is associated to a causal diagram \(\mathcal{G}\)[9] over

Figure 1: Exp-SE representation.

the node set \(V\) where \(V_{i}\to V_{j}\) if \(V_{i}\) is an argument of \(f_{V_{j}}\), and \(V_{i}\xleftarrow{}V_{j}\) if the corresponding \(U_{V_{i}},U_{V_{j}}\) are not independent [2]. A model with no bidirected edges is called _Markovian_, while a model with bidirected edges is called Semi-Markovian. An instantiation of the exogenous variables \(U=u\) is called a _unit_. By \(Y_{x}(u)\) we denote the potential response of \(Y\) when setting \(X=x\) for the unit \(u\), which is the solution for \(Y(u)\) to the set of equations obtained by evaluating the unit \(u\) in the submodel \(\mathcal{M}_{x}\), in which all equations in \(\mathcal{F}\) associated with \(X\) are replaced by \(X=x\). In a slight abuse of notation, we also replace \(Y=y\) with just \(y\) whenever the former is clear from the context. We next introduce an important inferential procedure for solving different tasks in causal inference.

### Abduction, Action and Prediction

The steps of the _abduction-action-prediction_ method can be summarized as follows:

**Algorithm 1** (Abduction, Action and Prediction [9]).: _Given an SCM \(\langle\mathcal{F},P(u)\rangle\), the conditional probability \(P(Y_{C}\mid E=e)\) of a counterfactual sentence "if it were \(C\) then \(Y\)", upon observing the evidence \(E=e\), can be evaluated using the following three steps:_

1. _Abduction - update_ \(P(u)\) _by the evidence_ \(e\) _to obtain_ \(P(u\mid e)\)_,_
2. _Action - modify_ \(\mathcal{F}\) _by the action_ \(do(C)\)_, where_ \(C\) _is an antecedent of_ \(Y\)_, to obtain_ \(\mathcal{F}_{C}\)_,_
3. _Prediction - use the model_ \(\langle\mathcal{F}_{C},P(u\mid e)\rangle\) _to compute the probability of_ \(Y_{C}\)_._

In the first step, the probabilities of the exogenous variables \(U\) are updated according to the observed evidence \(E=e\). Next, the model \(\mathcal{M}\) is modified to a submodel \(\mathcal{M}_{C}\). The action step allows one to consider queries related to interventions or imaginative, counterfactual operations. In the final step, the updated model \(\langle\mathcal{F}_{C},P(u\mid e)\rangle\) is used to compute the conditional probability \(P(y_{C}\mid e)\). There are two important special cases of the procedure. Whenever the action step is empty, the procedure handles queries in the first, associational layer of the Pearl's Causal Hierarchy (PCH, [2]). Whenever the abduction step is empty, but the action step is not, the procedure handles _interventional_ queries in the second layer of the PCH. The combination of the two steps, more generally, allows one to consider queries in all layers of the PCH, including the third, _counterfactual_ layer. In the following example, we look at the usage of the procedure on some queries.

**Example 1** (Abduction, Action, Prediction).: _Consider the following SCM:_

\[\mathcal{F}:\left\{\begin{aligned} \begin{aligned} \text{X} \leftarrow& f_{X}(U_{X},U_{XZ})\\ Z\leftarrow& f_{Z}(U_{Z},U_{XZ})\\ Y\leftarrow& f_{Y}(X,Z,U_{Y}),\end{aligned}\end{aligned}\right.\] (3) (4) \[\mathcal{F}:\left\{\begin{aligned} \text{X} \leftarrow& f_{X}(U_{X},U_{XZ})\\ Z\leftarrow& f_{Z}(U_{Z},U_{XZ})\\ Y\leftarrow& f_{Y}(X,Z,U_{Y}),\end{aligned}\right.\] (5)

_with \(P(U_{X},U_{XZ},U_{Z},U_{Y})\) the distribution over the exogenous variables. The causal diagram of the model is shown in Fig. 1(a), with an explicit representation of the exogenous variables in Fig. 1(b)._

_We are first interested in the query \(P(y\mid x)\) in the given model. Based on the abduction-prediction procedure, we can simply compute that:_

\[P(y\mid x)=\sum_{u}\mathbb{1}(Y(u)=y)P(u\mid x)=\sum_{u}\mathbb{1}(Y(u)=y)P(u _{z},u_{y})P(u_{x},u_{xz}\mid x).\] (6)

_where the first step follows from the definition of the observational distribution, and the second step follows from noting the independence \(U_{Z},U_{Y}\bot U_{X},U_{XZ},X\). In the abduction step, we can compute the probabilities \(P(u_{x},u_{xz}\mid x)\). In the prediction step, query \(P(y\mid x)\) is computed based on Eq. 6._

Figure 2: Graphical representations of the SCM in Ex. 1.

_Based on the procedure, we can also compute the query \(P(y_{x})\) (see Fig. 2c):_

\[P(y_{x})=\sum_{u}\mathbb{1}(Y_{x}(u)=y)P(u)=\sum_{u}\mathbb{1}(Y(x,u_{xz},u_{z},u_ {y})=y)P(u).\] (7)

_where the first step follows from the definition of an interventional distribution, and the second step follows from noting that \(Y_{x}\) does not depend on \(u_{x}\). In this case, the abduction step is void, since we are not considering any specific evidence \(E=e\). The value of \(Y(x,u_{xz},u_{z},u_{y})\) can be computed from the submodel \(\mathcal{M}_{x}\). Finally, using Eq. 7 we can perform the prediction step. We remark that_

\[\mathbb{1}(Y(x,u_{xz},u_{z},u_{y})=y)=\sum_{u_{x}}\mathbb{1}(Y(u_{x},u_{xz},u_ {z},u_{y})=y)P(u_{x}\mid x,u_{xz},u_{z},u_{y}),\] (8)

_by the law of total probability and noting that \(X\) is a deterministic function of \(u_{x},u_{xz}\). Thus, \(P(y_{x})\) also admits an alternative representation_

\[P(y_{x}) =\sum_{u}\mathbb{1}(Y(u_{x},u_{xz},u_{z},u_{y})=y)P(u_{x}\mid x, u_{xz},u_{z},u_{y})P(u_{xz},u_{z},u_{y})\] (9) \[=\sum_{u}\mathbb{1}(Y(u)=y)P(u_{x}\mid x,u_{xz})P(u_{xz},u_{z},u_ {y}),\] (10)

_where Eq. 10 follows from using the independencies among \(U\) and \(X\) in the graph in Fig. 2b. We revisit the representation in Eq. 10 in Ex. 2._

## 3 Foundations of Decomposing Spurious Variations

After getting familiar with the abduction-action-prediction procedure, our next task is to introduce a new procedure that allows us to decompose spurious effects. First, we define the concept of a _partially abducted submodel_:

**Definition 1** (Partially Abducted Submodel).: _Let \(U_{1},U_{2}\subseteq U\) be a partition of the exogenous variables. Let the partially abducted (PA, for short) submodel with respect to the exogenous variables \(U_{1}\) and evidence \(E=e\) be defined as:_

\[\mathcal{M}^{U_{1},E=e}:=\langle\mathcal{F},P(u_{1})P(u_{2}\mid u_{1},E)\rangle.\] (11)

In words, in the PA submodel, the typically obtained posterior distribution \(P(u\mid e)\) is replaced by the distribution \(P(u_{2}\mid u_{1},e)\). Effectively, the exogenous variables \(U_{1}\) are _not updated according to evidence_. The main motivation for introducing the PA model is that spurious variations arise whenever we are comparing units of the population that are different, a realization dating back to Pearson in the 19th century [11]. To give a formal discussion on what became known as _Pearson's shock_, consider two sets of differing evidence \(E=e\) and \(E=e^{\prime}\). After performing the abduction step, the variations between posterior distributions \(P(u\mid e)\) and \(P(u\mid e^{\prime})\) will be explained by _all the exogenous variables that precede the evidence \(E\)_. In a PA submodel, however, the posterior distribution \(P(u_{1})P(u_{2}\mid u_{1},e)\) will differ from \(P(u_{1})P(u_{2}\mid u_{1},e^{\prime})\) only in variables that are in \(U_{2}\), while the variables in \(U_{1}\) will induce no spurious variations. Note that if \(U_{1}=U\), then the PA submodel will introduce no spurious variations, a point to which we return in the sequel.

We now demonstrate how the definition of a PA submodel can be used to obtain partially abducted conditional probabilities:

**Proposition 1** (PA Conditional Probabilities).: _Let \(P(Y=y\mid E=e^{U_{1}})\) denote the conditional probability of the event \(Y=y\) conditional on evidence \(E=e\), defined as the probability of \(Y=y\) in the PA submodel \(\mathcal{M}^{U_{1},E=e}\) (i.e., the exogenous variables \(U_{1}\) are not updated according to the evidence). Then, we have that:_

\[P(Y=y\mid E=e^{U_{1}})=\sum_{u_{1}}P(U_{1}=u_{1})P(Y=y\mid E=e,U_{1}=u_{1}).\] (12)

### Partial Abduction and Prediction

Based on the notion of a PA submodel, we can introduce the partial-abduction and prediction procedure:

**Algorithm 2** (Partial Abduction and Prediction).: _Given an SCM \(\langle\mathcal{F},P(u)\rangle\), the conditional probability \(P(Y=y\mid E=e^{U_{1}})\) of an event \(Y=y\) upon observing the evidence \(e\), in a world where variables \(U_{1}\) are unresponsive to evidence, can be evaluated using the following two steps:_

1. _Partial Abduction_ _- update_ \(P(u)\) _by the evidence_ \(e\) _to obtain_ \(P(u_{1})P(u_{2}\mid u_{1},e)\)_, where_ \((u_{1},u_{2})\) _is a partition of the exogenous variables_ \(u\)_,_
2. _Prediction_ _- use the model_ \(\langle\mathcal{F},P(u_{1})P(u_{2}\mid u_{1},e)\rangle\) _to compute the probability of_ \(Y=y\)_._

In the first step of the algorithm, we only perform _partial abduction_. The exogenous variables \(U_{2}\) are updated according to the available evidence \(E=e\), while the variables \(U_{1}\) retain their original distribution \(P(u_{1})\) and remain unresponsive to evidence. This procedure allows us to consider queries in which only a subset of the exogenous variables respond to the available evidence. We next explain what kind of queries fall within this scope, beginning with an example:

**Example 2** (Partial Abduction and Prediction).: _Consider the model in Eq. 3-5. We are interested in computing the query:_

\[P(y\mid x^{U_{xz},U_{z}}) =\sum_{u}\mathbb{1}(Y(u)=y)P(u_{xz},u_{z})P(u_{x},u_{y}\mid u_{xz },u_{x},x)\] (13) \[=\sum_{u}\mathbb{1}(Y(u)=y)P(u_{xz},u_{z})P(u_{y})P(u_{x}\mid u_{ xz},u_{x},x)\] (14) \[=\sum_{u}\mathbb{1}(Y(u)=y)P(u_{xz},u_{z},u_{y})P(u_{x}\mid u_{xz },u_{x},x),\] (15)

_where the first step follows from Prop. 1, and the remaining steps from conditional independencies between the \(U\) variables and \(X\). Crucially, the query yields the same expression as in Eq. 10 that we obtained for \(P(y_{x})\) in Ex. 1. Therefore, the conditional probability \(P(y\mid x^{U_{xz},U_{z}})\) in a world where \(U_{XZ},U_{Z}\) are unresponsive to evidence is equal to the interventional probability \(P(y_{x})\)._

As the example illustrates, we have managed to find another procedure that mimics the behavior of the interventional (\(do(X=x)\)) operator in the given example. Interestingly, however, in this procedure, we have not made use of the submodel \(\mathcal{M}_{x}\) that was used in the abduction-action-prediction procedure. We next introduce an additional example that shows how the new procedure allows one to decompose spurious variations in causal models:

**Example 3** (Spurious Decomposition).: _Consider an SCM compatible with the graphical representation in Fig. 2(b) (with exogenous variables \(U\) shown explicitly in red), and the corresponding Semi-Markovian causal diagram in Fig. 2(a). We note that, based on the partial abduction-prediction procedure, the following two equalities hold:_

\[P(y\mid x) =P(y\mid x^{\emptyset})\] (16) \[P(y_{x}) =P(y\mid x^{U_{xz_{1}},U_{xz_{2}}}),\] (17)

_which shows that_

\[\text{Exp-SE}_{x}(y)=P(y\mid x^{\emptyset})-P(y\mid x^{U_{xz_{1}},U_{xz_{2}}}).\] (18)

_The experimental spurious effect can be written as a difference of conditional probabilities \(y\mid x\) in a world where all variables \(U\) are responsive to evidence vs. a world in which \(U_{XZ_{1}},U_{XZ_{2}}\) are

Figure 3: Graphical representations of the SCM in Ex. 1.

unresponsive to evidence. Furthermore, we can also consider a refinement that decomposes the effect_

\[\text{Exp-SE}_{x}(y)=\underbrace{P(y\mid x^{\emptyset})-P(y\mid x^{U_{xz_{1}}})} _{\text{variations of }U_{xz_{1}}}+\underbrace{P(y\mid x^{U_{xz_{1}}})-P(y\mid x^{U_{xz_{1}}},U_{ xz_{2}})}_{\text{variations of }U_{xz_{2}}},\] (19)

_allowing for an additive, non-parametric decomposition of the experimental spurious effect._

The first term in Eq. 19, shown in Fig. 7(a), encompasses spurious variations explained by the variable \(U_{XZ_{1}}\). The second term, in Fig. 3(b), encompasses spurious variations explained by \(U_{XZ_{2}}\).

For an overview, in Tab. 1 we summarize the different inferential procedures discussed so far, indicating the structural causal models associated with them.

## 4 Non-parametric Spurious Decompositions

We now move on to deriving general decomposition results for the spurious effects. Before doing so, we first derive a new decomposition result for the TV measure, not yet appearing in the literature (due to space constraints, all proofs are given in Appendix A):

**Proposition 2**.: _Define the total variation (TV) measure as \(\text{TV}_{x_{0},x_{1}}(y)=P(y\mid x_{1})-P(y\mid x_{0})\), and the total effect TE as \(\text{TE}_{x_{0},x_{1}}(y)=P(y_{x_{1}})-P(y_{x_{0}})\). The total variation measure can be decomposed as:_

\[\text{TV}_{x_{0},x_{1}}(y)=\text{TE}_{x_{0},x_{1}}(y)+(\text{Exp-SE}_{x_{1}}( y)-\text{Exp-SE}_{x_{0}}(y)).\] (20)

The above result clearly separates out the causal variations (measured by the TE) and the spurious variations (measured by Exp-SE terms) within the TV measure. The seminal result from [10] can be used to further decompose the TE measure. In the sequel, we show how the Exp-SE terms can be further decomposed, thereby reaching a full non-parametric decomposition of the TV measure.

### Spurious Decompositions for the Markovian case

When using the definition of a PA submodel, the common variations between \(X,Y\) can be attributed to (or explained by) the unobserved confounders \(U_{1},\ldots,U_{k}\). In order to do so, we first define the notion of an experimental spurious effect for a set of latent variables:

**Definition 2** (Spurious effects for Markovian models).: _Let \(\mathcal{M}\) be a Markovian model. Let \(Z_{1},\ldots,Z_{k}\) be the confounders between variables \(X\) and \(Y\) sorted in any valid topological order, and denote the corresponding exogenous variables as \(U_{1},\ldots,U_{k}\), respectively. Let \(Z_{[i]}=\{Z_{1},\ldots,Z_{i}\}\) and

\begin{table}
\begin{tabular}{|c|c|c|} \hline Procedure & SCM & Queries \\ \hline Abduction-Prediction & \(\langle\mathcal{F},P(u\mid E)\rangle\) & Layer 1 \\ \hline Action-Prediction & \(\langle\mathcal{F}_{x},P(u)\rangle\) & Layer 2 \\ \hline Abduction-Action-Prediction & \(\langle\mathcal{F}_{x},P(u\mid E)\rangle\) & Layers 1, 2, 3 \\ \hline Partial Abduction-Prediction & \(\langle\mathcal{F},P(u_{1})P(u_{2}\mid E)\rangle\) & Layers 1, 2, 3 \\ \hline \end{tabular}
\end{table}
Table 1: Summary of the different procedures and the corresponding probabilistic causal models.

Figure 4: Graphical representation of how the Exp-SE effect is decomposed in Ex. 3.

\(Z_{-[i]}=\{Z_{i+1},\ldots,Z_{k}\}\). \(U_{[i]}\) and \(U_{-[i]}\) are defined analogously. Define the experimental spurious effect associated with variable \(U_{i+1}\) as_

\[\text{Exp-SE}_{x}^{U_{[i]},U_{[i+1]}}(y)=P(y\mid x^{U_{[i]}})-P(y\mid x^{U_{[i+1 ]}}).\] (21)

The intuition behind the quantity \(\text{Exp-SE}_{x}^{U_{[i]},U_{[i+1]}}(y)\) can be explained as follows. The quantity \(P(y\mid x^{U_{[i]}})\) captures all the variations in \(Y\) induced by observing that \(X=x\) apart from those explained by the latent variables \(U_{1},\ldots,U_{i}\), which are fixed a priori and not updated. Similarly, the quantity \(P(y\mid x^{U_{[i+1]}})\) captures the variations in \(Y\) induced by observing that \(X=x\), apart from those explained by \(U_{1},\ldots,U_{i},U_{i+1}\). Therefore, taking the difference of the two quantities measures the variation in \(Y\) induced by observing that \(X=x\) that is explained by the latent variable \(U_{i+1}\).

Based on this definition, we can derive the first key non-parametric decomposition of the experimental spurious effect that allows the attribution of the spurious variations to the latent variables \(U_{i}\):

**Theorem 1** (Latent spurious decomposition for Markovian models).: _The experimental spurious effect \(\text{Exp-SE}_{x}(y)\) can be decomposed into latent variable-specific contributions as follows:_

\[\text{Exp-SE}_{x}(y)=\sum_{i=0}^{k-1}\text{Exp-SE}_{x}^{U_{[i]},U_{[i+1]}}(y) =\sum_{i=0}^{k-1}P(y\mid x^{U_{[i]}})-P(y\mid x^{U_{[i+1]}}).\] (22)

An illustrative example of applying the theorem is shown in Appendix B.1. Thm. 1 allows one to attribute spurious variations to latent variables influencing both \(X\) and \(Y\). The key question is when such an attribution, as shown in Eq. 22, can be computed from observational data in practice (known as an _identifiability_ problem [9]). In fact, when variables are added to the PA submodel in topological order, the attribution of variations to the latents \(U_{i}\) is identifiable, as we prove next:

**Theorem 2** (Spurious decomposition identification in topological ordering).: _The quantity \(P(y\mid x^{U_{[i]}})\) can be computed from observational data using the expression_

\[P(y\mid x^{U_{[i]}})=\sum_{z}P(y\mid z,x)P(z_{-[i]}\mid z_{[i]},x)P(z_{[i]}),\] (23)

_rendering each term of decomposition in Eq. 22 identifiable from the observational distribution \(P(v)\)._

We discuss in Appendix B.2 why a decomposition that does not follow a topological order of the variables \(U_{i}\) is not identifiable.

### Spurious Decompositions in Semi-Markovian Models

In the Markovian case, considered until now, there was a one-to-one correspondence between the observed confounders \(Z_{i}\) and their latent variables \(U_{i}\). This, however, is no longer the case in Semi-Markovian models. In particular, it can happen that there exist exogenous variables \(U_{j}\) that induce common variations between \(X,Y\), but affect more than one confounder \(Z_{i}\). We are interested in \(U_{j}\subseteq U\) that have causal (directed) paths to both \(X,Y\), described by the following definition:

**Definition 3** (Trek).: _Let \(\mathcal{M}\) be an SCM corresponding to a Semi-Markovian model. Let \(\mathcal{G}\) be the causal diagram of \(\mathcal{M}\). A trek \(\tau\) in \(\mathcal{G}\) (from \(X\) to \(Y\)) is an ordered pair of causal paths (\(g_{l}\), \(g_{r}\)) with a common exogenous source \(U_{i}\in U\). That is, \(g_{l}\) is a causal path \(U_{i}\to\cdots\to X\) and \(g_{r}\) is a causal path \(U_{i}\to\cdots\to Y\). The common source \(U_{i}\) is called the top of the trek (ToT for short), denoted \(top(g_{l},g_{r})\). A trek is called spurious if \(g_{r}\) is a causal path from \(U_{i}\) to \(Y\) that is not intercepted by \(X\)._

When decomposing spurious effects, we are in fact interested in all the exogenous variables \(U_{i}\) that lie on top of a spurious trek between \(X\) and \(Y\). It is precisely these exogenous variables that induce common variations between \(X\) and \(Y\). Using any subset of the variables that are top of spurious treks, we define a set-specific notion of a spurious effect:

**Definition 4** (Exogenous set-specific spurious effect).: _Let \(U_{sToT}\subseteq U\) be the subset of exogenous variables that lie on top of a spurious trek between \(X\) and \(Y\). Suppose \(A,B\subseteq U_{sToT}\) are two nested subsets of \(U_{sToT}\), that is \(A\subseteq B\). We then define the exogenous experimental spurious effect with respect to sets \(A,B\) as_

\[\text{Exp-SE}_{x}^{A,B}(y)=P(y\mid x^{A})-P(y\mid x^{B}).\] (24)he above definition is analogous to Def. 2, but we are now fixing different subsets of the tops of spurious treks. Def. 2 supports partial abduction of exogenous variables that are not on top of a spurious trek, but we are seldom interested in these since they do not induce covariations of \(X,Y\). The quantity \(\text{Exp-SE}_{x}^{A,B}(y)\) is presented as a graphical contrast in Fig. 5. In particular, the set of tops of spurious treks \(U_{sToT}\) is partitioned into three parts \((U_{A},U_{B\setminus A},U_{B^{C}})\). The causal diagram in the figure is informal, and the dots \((\cdots)\) represent arbitrary possible observed confounders that lie along indicated pathways. On the l.h.s. of the figure, the set \(U_{A}\) does not respond to the conditioning \(X=x\), whereas \(U_{B\setminus A},U_{B^{C}}\) do. This is contrasted with the r.h.s., in which neither \(U_{A}\) nor \(U_{B\setminus A}\) respond to \(X=x\), whereas \(U_{B^{C}}\) still does respond to the \(X=x\) conditioning. The described contrast thus captures the spurious effect explained by the tops of spurious treks in \(U_{B\setminus A}\).

Analogous to Thm. 1, we next state a variable-specific decomposition of the spurious effect, which is now with respect to exogenous variables that are top of spurious treks:

**Theorem 3** (Semi-Markovian spurious decomposition).: _Let \(U_{sToT}=\{U_{1},\ldots,U_{m}\}\subseteq U\) be the subset of exogenous variables that lie on top of a spurious trek between \(X\) and \(Y\). Let \(U_{[i]}\) denote the variables \(U_{1},\ldots,U_{i}\) (\(U_{[0]}\) denotes the empty set \(\emptyset\)). The experimental spurious effect \(\text{Exp-SE}_{x}(y)\) can be decomposed into variable-specific contributions as follows:_

\[\text{Exp-SE}_{x}(y)=\sum_{i=0}^{m-1}\text{Exp-SE}_{x}^{U_{[i]},U_{[i+1]}}(y) =\sum_{i=0}^{k-1}P(y\mid x^{U_{[i]}})-P(y\mid x^{U_{[i+1]}}).\] (25)

An example demonstrating the Semi-Markovian decomposition is given in Appendix B.3. We next discuss the question of identification. We begin by discussing how to annotate the exogenous variables given a Semi-Markovian causal diagram:

**Definition 5** (Top of trek from the causal diagram).: _Let \(\mathcal{M}\) be a Semi-Markovian model and let \(\mathcal{G}\) be the associated causal diagram. A set of nodes fully connected with bidirected edges is called a clique. A maximal clique \(C_{i}\) is such that there is no clique \(C_{i}^{\prime}\) such that \(C_{i}\subsetneq C_{i}^{\prime}\). The set of variables \(U_{sToT}\) can be constructed from the causal diagram in the following way:_

1. _initialize_ \(U_{sToT}=\emptyset\)_,_
2. _for each maximal clique_ \(C_{i}\)_, consider the associated exogenous variable_ \(U_{C_{i}}\) _pointing to each node in the clique; if there exists a spurious trek between_ \(X\) _and_ \(Y\) _with a top in_ \(U_{C_{i}}\)_, add_ \(U_{C_{i}}\) _to_ \(U_{sToT}\)_._

After defining the explicit construction of the set \(U_{sToT}\), we define the notion of the anchor set:

**Definition 6** (Anchor Set).: _Let \(U_{1},\ldots U_{l}\subseteq U\) be a subset of the exogenous variables. We define the anchor set \(\text{AS}(U_{1},\ldots,U_{l})\) of \((U_{1},\ldots,U_{l})\) as the subset of observables \(V\) that are directly influenced by any of the \(U_{i}\)s,_

\[\text{AS}(U_{1},\ldots,U_{l})=\bigcup_{i=1}^{l}\mathrm{ch}(U_{i}).\] (26)

Another important definition is that of anchor set exogenous ancestral closure:

Figure 5: Quantity \(\text{Exp-SE}_{x}^{A,B}(y)\) as a graphical contrast. Dots \(\cdots\) indicate arbitrary observed confounders along the indicated pathway.

**Definition 7** (Anchor Set Exogenous Ancestral Closure).: _Let \(U_{s}\subseteq U\) be a subset of the exogenous variables. Let \(\text{AS}(U_{s})\) denote the anchor set of \(U_{s}\), and let \(\operatorname{an}^{\text{ex}}(AS(U_{s}))\) denote all exogenous variables that have a causal path to any variable in \(\text{AS}(U_{s})\). \(U_{s}\) is said to satisfy anchor set exogenous ancestral closure (ASEAC) if_

\[U_{s}=\operatorname{an}^{\alpha}(AS(U_{s})).\] (27)

Based on the above, we provide a sufficient condition for identification in the Semi-Markovian case:

**Theorem 4** (ID of variable spurious effects in Semi-Markovian models).: _Let \(U_{s}\subseteq U_{sToT}\). The quantity \(P(y\mid x^{U_{s}})\) is identifiable from observational data \(P(V)\) if the following hold:_

1. \(X\notin\text{AS}(U_{s})\)_,_ \(Y\notin\text{AS}(U_{s})\)__
2. \(U_{s}\) _satisfies anchor set exogenous ancestral closure,_ \(U_{s}=\operatorname{an}^{\alpha}(AS(U_{s}))\)_._

Some instructive examples grounding Defs. 5-7 and Thm. 4 can be found in Appendix B.4. In words, the conditional expectation of \(Y\) given \(X\) in the partially abducted submodel w.r.t. a set \(U_{s}\) is identifiable whenever (i) neither \(X\) nor \(Y\) are elements of the anchor set of \(U_{s}\) and (ii) the set \(U_{s}\) satisfies the anchor set exogenous ancestral closure. Thm. 4 provides a sufficient, but not a necessary condition for identification. An additional discussion of the conditions is given in Appendix C. We hope to address in future work an algorithmic way for identifying spurious effects in full generality.

## 5 Experiments

We now apply our framework to a synthetic example (called Synthetic A) with a known ground truth, summarized in Tab. 2 where the SCM \(\mathcal{M}\) and the causal diagram \(\mathcal{G}\) are given. The source code for the experiment can be found in our repository. For this example, we set the parameters \(\lambda_{1}=\lambda_{2}=\lambda_{3}=0.2\). We then vary each parameter \(\lambda_{i}\in[0,0.2]\) (while keeping the other two parameters fixed), which changes the value of the effect associated with latent variable \(U_{i}\). The effects associated with each \(U_{i},i\in\{1,2,3\}\) are computed based on the decomposition in Thm. 1:

\[\text{Exp-SE}^{U_{i}}_{x}(y) :=\text{Exp-SE}^{\emptyset,U_{i}}_{x}(y)=P(y\mid x^{\emptyset})-P (y\mid x^{U_{1}})\] (28) \[\text{Exp-SE}^{U_{2}}_{x}(y) :=\text{Exp-SE}^{U_{1},\{U_{1},U_{2}\}}_{x}(y)=P(y\mid x^{U_{1}})- P(y\mid x^{U_{1},U_{2}})\] (29) \[\text{Exp-SE}^{U_{3}}_{x}(y) :=\text{Exp-SE}^{\{U_{1},U_{2}\},\{U_{1},U_{2},U_{3}\}}_{x}(y)=P(y \mid x^{U_{1},U_{2}})-P(y\mid x^{U_{1},U_{2},U_{3}}).\] (30)

The key task is to compute the ground truth values of \(P(y\mid x^{U_{[i]}})\) for different values of \(i\). According to Def. 1, we want to obtain the conditional distribution of \(Y\) given \(X=x\) but subject to not updating \(U_{[i]}\) according to the evidence \(X=x\). Based on the true SCM, this can be done efficiently using rejection sampling as follows:

1. Take \(N\) samples from the SCM \(\mathcal{M}\) in Tab. 2,
2. For all samples \(k\in\{1,\dots,N\}\) with \(u^{(k)}\) such that \[X(u^{(k)})\neq x,\] (31) re-sample the part of the unit \(u^{(k)}\) that is not included in \(U_{[i]}\) (e.g., if \(U_{[i]}=\{U_{1},U_{2}\}\), latent \(u^{(k)}_{1},u^{(k)}_{2}\) are not re-sampled but \(u^{(k)}_{3}\) is) and replace \(u^{(k)}\) with this new sample,

\begin{table}
\begin{tabular}{l|c} SCM \(\mathcal{M}\) & Causal Diagram \(\mathcal{G}\) \\ \hline \(Z_{1}\gets B(0.5)\) & \\ \(Z_{2}\gets B(0.4+0.2Z_{1})\) & \\ \(Z_{3}\gets B(0.3+0.3Z_{1}Z_{2})\) & \\ \(X\gets B(0.2+\lambda_{1}Z_{1}+\lambda_{2}Z_{2}+\lambda_{3}Z_{3})\) & \\ \(Y\gets B(0.1+0.2X+\lambda_{1}Z_{1}+\lambda_{2}Z_{2}+\lambda_{3}Z_{3})\) & \\ \end{tabular}
\end{table}
Table 2: SCM and causal diagram for the Synthetic A example.

3. Evaluate the mechanisms \(\mathcal{F}\) of \(\mathcal{M}\) for all units \(u^{(k)}\),
4. If there exists a sample \(k\) with \(X(u^{(k)})\neq x\) go back to Step (2),
5. Return the mean of the \(Y\) variables \(\frac{1}{N}\sum_{k=1}^{N}Y^{(k)}\).

Notice that the described procedure gives us samples from the distribution \(P(y\mid x^{U_{[i]}})\). The values of \(U_{[i]}\) are sampled only once and are not updated after the initial sampling. Other values in \(U\), however, are sampled anew until their values are such that they are compatible with the evidence \(X=x\). Therefore, the procedure guarantees that \(U_{[i]}\) do not respond to the evidence, whereas the complement of \(U_{[i]}\) does, allowing us to compute \(P(y\mid x^{U_{[i]}})\) and in turn the expressions in Eqs. 28-30. The effects are also estimated from observational data based on the identification expressions in Thm. 2. Fig. 6 demonstrates that the SCM-based ground truth matches the estimates based on Thm. 2.

## 6 Conclusions

In this paper, we introduced a general toolkit for decomposing spurious variations in causal models. In particular, we introduced a new primitive called _partially abducted submodel_ (Def. 1), together with the procedure of partial abduction and prediction (Alg. 2). This procedure allows for new machinery for decomposing spurious variations in Markovian (Thm. 1) and Semi-Markovian (Thm. 3) models. Finally, we also developed sufficient conditions for identification of such spurious decompositions (Thms. 2, 4), and demonstrated the approach empirically (Sec. 5). The main limitation of our approach is the need for a fully-specified causal diagram, which may be challenging in practice. However, from a fully specified graph and the data, our tools for decomposing spurious effects give a fine-grained quantification of what the main confounders are. As is common in causal inference, the granularity of the obtained knowledge needs to be matched with the strength of the causal assumptions (in this case, specifying the causal diagram). Conversely, in the absence of such assumptions, fine-grained quantitative knowledge about these effects cannot be obtained in general [2], and we hypothesize that precise quantification of spurious effects is not attainable in the absence of a causal diagram. Finally, we discuss another technical solution that may alleviate some of the difficulty of causal modeling. Recently, cluster diagrams have been proposed [1], in which one can consider groups of confounders (instead of considering each confounder separately), and thus the specification of causal assumptions becomes less demanding (due to clustering, the number of nodes in the graph is smaller). However, causal decompositions as described in this paper can still be applied to cluster diagrams. This offers a way to choose a different level of granularity for settings where domain knowledge may not be specific enough to elicit a full causal diagram.

Figure 6: Experimental results on the Synthetic A example. Lines indicate the estimated values, dots the ground truth obtained from the SCM using rejection sampling, and the 95% confidence intervals are indicated with color. As expected, increasing the \(\lambda_{i}\) coefficient increases the spurious effect associated with the latent variable \(U_{i}\).

## References

* [1] T. V. Anand, A. H. Ribeiro, J. Tian, and E. Bareinboim. Causal effect identification in cluster dags. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37(10), pages 12172-12179, 2023.
* [2] E. Bareinboim, J. D. Correa, D. Ibeling, and T. Icard. On pearl's hierarchy and the foundations of causal inference. In _Probabilistic and Causal Inference: The Works of Judea Pearl_, page 507-556. Association for Computing Machinery, New York, NY, USA, 1st edition, 2022.
* [3] R. M. Baron and D. A. Kenny. The moderator-mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations. _Journal of personality and social psychology_, 51(6):1173, 1986.
* [4] H. Checkoway, N. Pearce, and D. Kriebel. _Research methods in occupational epidemiology_, volume 34. Monographs in Epidemiology and, 2004.
* [5] A. Decruyenaere, J. Steen, K. Colpaert, D. D. Benoit, J. Decruyenaere, and S. Vansteelandt. The obesity paradox in critically ill patients: a causal learning approach to a casual finding. _Critical Care_, 24(1):1-11, 2020.
* [6] V. Hainer and I. Aldhoon-Hainerova. Obesity paradox does exist. _Diabetes care_, 36 (Supplement_2):S276-S281, 2013.
* [7] J. Hernandez. Redlining revisited: mortgage lending patterns in sacramento 1930-2004. _International Journal of Urban and Regional Research_, 33(2):291-313, 2009.
* [8] J. Larson, S. Mattu, L. Kirchner, and J. Angwin. How we analyzed the compas recidivism algorithm. _ProPublica (5 2016)_, 9, 2016.
* [9] J. Pearl. _Causality: Models, Reasoning, and Inference_. Cambridge University Press, New York, 2000. 2nd edition, 2009.
* [10] J. Pearl. Direct and indirect effects. In _Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence_, page 411-420, San Francisco, CA, USA, 2001. Morgan Kaufmann Publishers Inc.
* [11] K. Pearson. Iv. mathematical contributions to the theory of evolution.--v. on the reconstruction of the stature of prehistoric races. _Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character_, 1(192):169-244, 1899.
* [12] J. M. Robins and S. Greenland. Identifiability and exchangeability for direct and indirect effects. _Epidemiology_, pages 143-155, 1992.
* [13] K. J. Rothman, S. Greenland, T. L. Lash, et al. _Modern epidemiology_, volume 3. Wolters Kluwer Health/Lippincott Williams & Wilkins Philadelphia, 2008.
* [14] T. VanderWeele. _Explanation in causal inference: methods for mediation and interaction_. Oxford University Press, 2015.
* [15] Y. Zenou and N. Boccard. Racial discrimination and redlining in cities. _Journal of Urban economics_, 48(2):260-285, 2000.
* [16] J. Zhang and E. Bareinboim. Non-parametric path analysis in structural causal models. In _Proceedings of the 34th Conference on Uncertainty in Artificial Intelligence_, 2018.

AcknowledgementsThis research was supported in part by the NSF, ONR, AFOSR, DoE, Amazon, JP Morgan, and The Alfred P. Sloan Foundation. We would like to thank Inwoo Hwang for providing very useful comments on the paper during the NeurIPS conference.

**Supplementary Material for _A Causal Framework for Decomposing Spurious Variations_**

The source code for reproducing the experiments can be found in our code repository.

## Appendix A Theorem and Proposition Proofs

### Proof of Prop. 2

Proof.: Note that TV and TE are defined as:

\[\text{TV}_{x_{0},x_{1}}(y) =P(y\mid x_{1})-P(y\mid x_{0})\] (32) \[\text{TE}_{x_{0},x_{1}}(y) =P(y_{x_{1}})-P(y_{x_{0}}).\] (33)

We can expand the TV measure in the following way:

\[\text{TV}_{x_{0},x_{1}}(y) =P(y\mid x_{1})-P(y\mid x_{0})\] (34) \[=P(y\mid x_{1})-P(y_{x_{1}})+P(y_{x_{1}})-P(y\mid x_{0})\] (35) \[=\underbrace{P(y\mid x_{1})-P(y_{x_{1}})}_{\text{Exp-SE}_{x_{1}}( y)}+\underbrace{P(y_{x_{1}})-P(y_{x_{0}})}_{\text{TE}_{x_{0},x_{1}}(y)}+ \underbrace{P(y_{x_{0}})-P(y\mid x_{0})}_{-\text{Exp-SE}_{x_{0}}(y)}\] (36) \[=\text{TE}_{x_{0},x_{1}}(y)+\text{Exp-SE}_{x_{1}}(y)-\text{Exp- SE}_{x_{0}}(y),\] (37)

showing the required result. 

### Proof of Thm. 1

Proof.: Note that

\[\sum_{i=0}^{k-1}\text{Exp-SE}_{x}^{U_{[i]},U_{[i+1]}}(y) =\sum_{i=0}^{k-1}P(y\mid x^{U_{[i]}})-P(y\mid x^{U_{[i+1]}})\] (38)

is a telescoping sum, and thus we have that

\[\sum_{i=0}^{k-1}\text{Exp-SE}_{x}^{U_{[i]},U_{[i+1]}}(y) =\sum_{i=0}^{k-1}P(y\mid x^{U_{[i]}})-P(y\mid x^{U_{[i+1]}})\] (39) \[=P(y\mid x^{9})-P(y\mid x^{U_{[k]}})\] (40) \[=P(y\mid x)-P(y_{x})\] (41) \[=\text{Exp-SE}_{x}(y),\] (42)

completing the proof of the theorem. 

### Proof of Thm. 2

Proof.: Notice that fixing a specific value for the variables \((U_{1},\dots,U_{k})=(u_{1},\dots,u_{k})\) also gives a unique value for the variables \((Z_{1},\dots,Z_{k})=(z_{1},\dots,z_{k})\). Therefore, we can write

\[P(y\mid x^{U_{[i]}}) =\sum_{u_{[i]}}P(u_{[i]})P(y\mid x,u_{[i]})\] (43) \[=\sum_{u_{[i]}}P(u_{[i]})P(y\mid x,u_{[i]},z_{[i]}(u_{[i]}))\] (44) \[=\sum_{z_{[i]}}\sum_{u_{[i]}}P(u_{[i]})\mathbbm{1}(Z_{[i]}(u_{[i ]})=z_{[i]})P(y\mid x,z_{[i]})\] (45) \[=\sum_{z_{[i]}}P(z_{[i]})P(y\mid x,z_{[i]})\] (46) \[=\sum_{z}P(y\mid x,z)P(z_{-[i]}\mid x,z_{[i]})P(z_{[i]}).\] (47)The above proof makes use of the fact that the exogenous variables \(U_{i}\) are considered in the topological ordering in the decomposition in Eq. 22, since in this case a fixed value of \(u_{[i]}\) implies a fixed value of \(z_{[i]}\). However, when considering decompositions that do not follow a topological ordering, this is not the case, and we lose the identifiability property of the corresponding effects, as shown in the example in Appendix B.2.

### Proof of Thm. 3

Proof.: The proof is analogous to the proof of Thm. 1, the only difference being that there is no longer a 1-to-1 of the latent variables \(U_{i}\) with the observed confounders \(Z_{i}\). Rather, each \(U_{i}\) may correspond to one or more \(Z_{i}\) variables. However, we still have that

\[\sum_{i=0}^{k-1}\text{Exp-SE}_{x}^{U_{[i]},U_{[i+1]}}(y)=\sum_{i=0}^{k-1}P(y\mid x ^{U_{[i]}})-P(y\mid x^{U_{[i+1]}})\] (48)

is a telescoping sum, and thus we have that

\[\sum_{i=0}^{k-1}\text{Exp-SE}_{x}^{U_{[i]},U_{[i+1]}}(y) =\sum_{i=0}^{k-1}P(y\mid x^{U_{[i]}})-P(y\mid x^{U_{[i+1]}})\] (49) \[=P(y\mid x^{9})-P(y\mid x^{U_{[k]}})\] (50) \[=P(y\mid x)-P(y_{x})\] (51) \[=\text{Exp-SE}_{x}(y),\] (52)

completing the proof of the theorem. 

### Proof of Thm. 4

Proof.: Let \(U_{PA}\) be the set of exogenous variables not updated according to evidence, and suppose that (i) \(X,Y\notin\text{AS}(U_{PA})\); (ii) \(U_{PA}=\operatorname{an}^{\text{ex}}(AS(U_{PA}))\). Note that

\[P(y\mid x^{U_{PA}}) \overset{\text{(def)}}{=}\sum_{u_{PA}}P(u_{PA})P(y\mid x,u_{PA})\] (53) \[=\sum_{u_{PA},z_{AS}}P(u_{PA})P(y\mid x,u_{PA},z_{AS})P(z_{AS}\mid x,u_{PA}),\] (54)

where \(Z_{AS}\) is the anchor set of \(U_{PA}\) and the second line follows from the law of total probability. Consider any exogenous ancestor of \(Z_{AS}\), denoted by \(U_{z}\). By condition (ii) of ancestral closure, \(U_{z}\) must be in \(U_{PA}\). Therefore, \(U_{PA}\) contains all exogenous ancestors of \(Z_{AS}\). Consequently, a fixed value of \(u_{PA}\) also implies a value of \(Z_{AS}\), labeled \(z_{AS}\). This means that

\[P(z_{AS}\mid x,u_{PA})=\mathbb{1}(Z_{AS}(u_{PA})=z_{AS}).\] (55)

Next, suppose there is an open path from \(U_{PA}\) to \(Y\) when conditioning on \(X,Z_{AS}\), labeled \(U_{PA,i}\to Z_{s}\overset{\text{}}{=}Z_{s^{\prime}}\to\dots\to Y\). By definition of the anchor set, \(Z_{AS}\) must contain the first variable on this path, \(Z_{s}\), and \(Z_{s}\) is different from \(X,Y\). Consider first the case with the arrow from \(Z_{s}\) outgoing, \(U_{PA,i}\to Z_{s}\to\dots\to Y\). When conditioning on \(Z_{AS}\), this path is closed since \(Z_{s}\in Z_{AS}\), yielding a contradiction. Consider then the second case with the arrow incoming into \(Z_{s}\), \(U_{PA,i}\to Z_{s}\gets Z_{s^{\prime}}\to\dots\to Y\). Since \(Z_{s^{\prime}}\) points to \(Z_{s}\), \(Z_{s^{\prime}}\) differs from \(X,Y\). Furthermore, by anchor set exogenous ancestral closure, the exogenous variable of \(Z_{s^{\prime}}\), labeled \(U_{s^{\prime}}\), must also be in \(U_{PA}\). Hence, \(Z_{AS}\) contains \(Z_{s^{\prime}}\), and \(Z_{s^{\prime}}\) cannot be a collider on this path, so conditioning on \(Z_{AS}\) blocks the path, again yielding a contradiction. We conclude that no open path between \(U_{PA}\) and \(Y\) exists when conditioning on \(Z_{AS},X\). Therefore, it holds that

\[P(y\mid x,u_{PA},z_{AS})=P(y\mid x,z_{AS}).\] (56)

Finally, by plugging in Eqs. 55-56 into Eq. 54 we obtain that

\[P(y\mid x^{U_{PA}})=\sum_{u_{PA},z_{AS}}P(u_{PA})P(y\mid x,z_{AS})\mathbb{1}(Z _{AS}(u_{PA})=z_{AS})\] (57)\[=\sum_{z_{AS}}P(y\mid x,z_{AS})\underbrace{\sum_{u_{PA}}P(u_{PA}) \mathbb{1}(Z_{AS}(u_{PA})=z_{AS})}_{P(z_{AS})\text{ by definition}}\] (58) \[=\sum_{z_{AS}}P(y\mid x,z_{AS})P(z_{AS}),\] (59)

therefore witnessing identifiability of \(P(y\mid x^{U_{PA}})\) and completing the proof. 

## Appendix B Examples

### Markovian Decomposition Example

**Example 4** (Latent variable attribution in a Markovian model).: _Consider the following SCM \(\mathcal{M}^{*}\):_

\[\mathcal{M}^{*}:\begin{cases}Z_{1}\gets B(0.5)&\text{(\ref{eq:sys})}\\ Z_{2}\gets B(0.4+0.2Z_{1})&\text{(\ref{eq:sys})}\\ X\gets B(0.3+0.2Z_{1}+0.2Z_{2})&\text{(\ref{eq:sys})}\\ Y\gets X+Z_{1}+Z_{2},&\text{(\ref{eq:sys})}\end{cases}\] (60)

_and the causal diagram in Fig. 7. We wish to decompose the quantity Exp-SE\({}_{x}(y)\) into the variations attributed to the latent variables \(U_{1},U_{2}\). Following the decomposition from Thm. 1 we can write_

\[\text{Exp-SE}_{x}(y\mid x_{1})= \underbrace{\mathbb{E}(y\mid x_{1})-\mathbb{E}(y\mid x_{1}^{U_{1 }})}_{U_{1}\text{ contribution}}\] (61) \[+ \underbrace{\mathbb{E}(y\mid x_{1}^{U_{1}})-\mathbb{E}(y\mid x_{ 1}^{U_{1},U_{2}})}_{U_{2}\text{ contribution}}.\]

_We now need to compute the terms appearing in Eq. 61. In particular, we know that_

\[\mathbb{E}(y\mid x_{1}^{U_{1},U_{2}}) =\mathbb{E}(y\mid do(x_{1}))\] (62) \[=1+\mathbb{E}(Z_{1}\mid do(x_{1}))+\mathbb{E}(Z_{2}\mid do(x_{1}))\] (63) \[=1+\mathbb{E}(Z_{1})+\mathbb{E}(Z_{2})=1+0.5+0.5=2.\] (64)

_Similarly, we can also compute_

\[\mathbb{E}(y\mid x_{1})=1+P(Z_{1}=1\mid x_{1})+P(Z_{2}=1\mid x_{1}),\] (65)

_where \(P(Z_{1}=1\mid x_{1})\) can be expanded as_

\[P(Z_{1}=1\mid x_{1}) =\frac{P(Z_{1}=1,X=1)}{P(X=1)}\] (66) \[=\frac{P(Z_{1}=1,X=1,Z_{2}=1)+P(Z_{1}=1,X=1,Z_{2}=0)}{P(X=1)}\] (67) \[=\frac{0.5*0.6*0.7+0.5*0.4*0.5}{0.5}=0.62.\] (68)

_The value of \(P(Z_{2}=1\mid x_{1})\) is computed analogously and also equals \(0.62\), implying that \(\mathbb{E}(y\mid x_{1})=1+0.62+0.62=2.24\). Finally, we want to compute \(\mathbb{E}(y\mid x_{1}^{U_{1}})\), which equals_

\[\mathbb{E}(y\mid x_{1}^{U_{1}})=1+P(Z_{1}=1\mid x_{1}^{U_{1}})+P(Z_{2}=1\mid x _{1}^{U_{1}}).\] (69)

Figure 7: Markovian causal diagram used in Ex. 4 with explicitly drawn latent variables \(U_{1},U_{2}\).

[MISSING_PAGE_FAIL:15]

when computing \(\mathbb{E}^{\mathcal{M}}(y\mid x_{0}^{U_{2}})\) we have that_

\[\mathbb{E}^{\mathcal{M}_{1}}(y\mid x_{0}^{U_{2}}) =1\] (85) \[\mathbb{E}^{\mathcal{M}_{2}}(y\mid x_{0}^{U_{2}}) =0.93,\] (86)

_showing that the quantity \(\mathbb{E}^{\mathcal{M}}(y\mid x_{0}^{U_{2}})\) is non-identifiable._

The example illustrates that even in the Markovian case, when the variables are not considered in a topological order (in the example above, the variable \(U_{2}\) was considered without the variable \(U_{1}\) being added first), we might not be able to identify the decomposition of the spurious effects.

### Semi-Markovian Decomposition Example

**Example 6** (Semi-Markovian spurious decomposition).: _Consider the following SCM \(\mathcal{M}\):_

\[\mathcal{F},P(U):\begin{cases}Z_{1}\gets U_{1}\wedge U_{1X} \\ Z_{2}\gets U_{2}\lor U_{2X}\\ X\gets U_{X}\wedge(U_{1X}\lor U_{2X})\\ Y\gets X+Z_{1}+Z_{2}\\ \\ U_{1},U_{2},U_{1X},U_{2X},U_{X}\stackrel{{ i.i.d}}{{\sim}} \text{Bernoulli}(0.5).\end{cases}\] (87)

_The causal diagram \(\mathcal{G}\) associated with \(\mathcal{M}\) is given in Fig. 9. The exogenous variables that lie on top of a spurious trek are \(U_{1X},U_{2X}\). Therefore, following the decomposition from Thm. 3, we can attribute spurious variations to these two variables:_

\[\text{Exp-SE}_{x}(y\mid x_{1})= \underbrace{\mathbb{E}(y\mid x_{1})-\mathbb{E}(y\mid x_{1}^{U_{1X }})}_{U_{1X}\text{ contribution}}\] (92) \[+\underbrace{\mathbb{E}(y\mid x_{1}^{U_{1X}})-\mathbb{E}(y\mid x _{1}^{U_{1X},U_{2X}})}_{U_{2X}\text{ contribution}}.\]

_We now compute the terms appearing in Eq. 92. In particular, we know that_

\[\mathbb{E}(y\mid x_{1}^{U_{1X},U_{2X}}) =\mathbb{E}(y\mid do(x_{1}))=1+\mathbb{E}(Z_{1}\mid do(x_{1}))+ \mathbb{E}(Z_{1}\mid do(x_{1}))\] (93) \[=1+\mathbb{E}(Z_{1})+\mathbb{E}(Z_{2})=1+0.25+0.75=2.\] (94)

_Similarly, we can also compute_

\[\mathbb{E}(y\mid x_{1})=1+P(Z_{1}=1\mid x_{1})+P(Z_{2}=1\mid x_{1}),\] (95)

_Now, \(P(Z_{1}=1\mid x_{1})=\frac{P(Z_{1}=1,x_{1})}{P(x_{1})}\), and we know that \(X=1\) if and only if \(U_{X}=1\) and \(U_{1X}\lor U_{2X}=1\), which happen independently with probabilities \(\frac{1}{2}\) and \(\frac{3}{4}\), respectively. Next, \(Z_{1}=1,X=1\) happens if and only if \(U_{X}=1,U_{1X}=1\) and \(U_{1}=1\), which happens with probability \(\frac{1}{8}\). Therefore, we can compute_

\[P(Z_{1}=1\mid x_{1})=\frac{\frac{1}{8}}{\frac{1}{2}*\frac{3}{4}}=\frac{1}{3}.\] (96)

_Furthermore, we similarly compute that \(Z_{2}=1,X=1\) happens if either \(U_{X}=1,U_{2X}=1\) or \(U_{X}=1,U_{2X}=0,U_{2}=1,U_{1X}=1\) which happens disjointly with probabilities \(\frac{1}{4}\). \(\frac{1}{16}\), respectively. Therefore,_

\[P(Z_{2}=1\mid x_{1})=\frac{\frac{1}{4}+\frac{1}{16}}{\frac{1}{2}*\frac{3}{4}} =\frac{5}{6}.\] (97)

Figure 9: Causal diagram appearing in Exs. 6-7.

_Putting everything together we obtain that_

\[\mathbb{E}(y\mid x_{1})=1+\frac{1}{3}+\frac{5}{6}=\frac{13}{6}.\] (98)

_Finally, we want to compute \(\mathbb{E}(y\mid x_{1}^{U_{1X}})\), which equals_

\[\mathbb{E}(y\mid x_{1}^{U_{1X}})=1+P(Z_{1}=1\mid x_{1}^{U_{1X}})+P(Z_{2}=1 \mid x_{1}^{U_{1X}}).\] (99)

_Now, to evaluate these expressions, we distinguish two cases, namely (i) \(U_{1X}=1\) and (ii) \(U_{1X}=0\). In the first case, \(P(Z_{1}\mid x_{1})=\frac{1}{2}\) and \(P(Z_{2}=1\mid x_{1})=\frac{3}{4}\). In the second case, \(P(Z_{1}\mid x_{1})=0\) and \(P(Z_{2}=1\mid x_{1})=1\). Therefore, we can compute_

\[P(Z_{1}=1\mid x_{1}^{U_{1X}})=\frac{1}{2}P_{U_{1X}=1}(Z_{1}\mid x _{1})+\frac{1}{2}P_{U_{1X}=0}(Z_{1}\mid x_{1})=\frac{1}{4}\] (100) \[P(Z_{2}=1\mid x_{1}^{U_{1X}})=\frac{1}{2}P_{U_{1X}=1}(Z_{2}\mid x _{1})+\frac{1}{2}P_{U_{1X}=0}(Z_{2}\mid x_{1})=\frac{7}{8},\] (101)

_which implies that \(\mathbb{E}(y\mid x_{1}^{U_{1X}})=\frac{17}{8}\). Finally, this implies that_

\[\underbrace{\text{Exp-SE}_{x}(y\mid x_{1})}_{-\frac{1}{6}}=\underbrace{\text {Exp-SE}^{\emptyset,U_{1X}}_{=\frac{1}{4}\text{ from }U_{1X}}}_{=\frac{1}{8}\text{ from }U_{2X}}+\underbrace{\text{Exp-SE}^{U_{1X};\{U_{1X},U_{ 2X}\}}_{=\frac{1}{8}\text{ from }U_{2X}}}_{=\frac{1}{8}\text{ from }U_{2X}}.\] (102)

The terms appearing on the r.h.s. of Eq. 102 are shown as graphical contrasts in Fig. 4. On the left side of Fig. (a)a, \(U_{1X},U_{2X}\) are responding to the conditioning \(X=x\), compared against the right side where only \(U_{2X}\) is responding to the conditioning \(X=x\). In the second term, in Fig. (b)b, on the left only \(U_{2X}\) responds to \(X=x\), compared against the right side in which neither \(U_{1X}\) nor \(U_{2X}\) respond to \(X=x\) conditioning.

### Semi Markovian Identification Examples

**Example 7** (Spurious Treks).: _Consider the causal diagram in Fig. 7. In the diagram, latent variables \(U_{1},U_{2}\) both lie on top of a spurious trek because:_

\[X\gets Z_{1}\gets U_{1}\to Z_{1}\to Y\text{ is a spurious trek with top }U_{1}\] \[X\gets Z_{2}\gets U_{2}\to Z_{2}\to Y\text{ is a spurious trek with top }U_{2}.\]

_There are also other spurious treks with \(U_{1}\) on top, such as \(X\gets Z_{1}\gets U_{1}\to Z_{1}\to Z_{2}\to Y\)._

**Example 7** (continued - \(U_{sToT}\) construction).: _We continue with Ex. 6 and the causal graph in Fig. 9 and perform the steps as follows:_

1. _initialize_ \(U_{sToT}=\emptyset\)_,_
2. _note that_ \(\{X,Z_{1}\}\) _create a maximal clique, since:_ 1. _they are connected with a bidirected edge and thus form a clique,_ 2. \(\{X,Z_{1},Z_{2}\}\) _do not form a clique, due to the bidirected edge_ \(Z_{1}\)__\(\leftarrow\)__\(\rightarrow\)__\(Z_{2}\) _not being present,_ 3. \(\{X,Z_{1},Y\}\)_,_ \(\{X,Z_{1},Z_{2},Y\}\) _do not form a clique, since_ \(Y\) _is not incident to any bidirected edges,_ 4. _thus, the clique_ \(\{X,Z_{1}\}\) _is also maximal._ _Let the variable_ \(U_{1X}\) _be associated with this clique, pointing to_ \(X,Z_{1}\)_, and note that_ \(U_{1X}\) _lies on top of a spurious trek between_ \(X,Y\)_,_
3. _similarly,_ \(\{X,Z_{2}\}\) _also create a maximal clique, associated with the variable_ \(U_{X2}\)_, pointing to_ \(X,Z_{2}\)_, that lies on top of a spurious trek between_ \(X,Y\)_,_
4. _the node_ \(Y\) _also forms a maximal clique and is associated with the variable_ \(U_{Y}\) _that does not lie on top of a spurious trek (it does not have a path to_ \(X\)_)._

_Therefore, we have constructed the set \(U_{sToT}=\{U_{1X},U_{2X}\}\)._

**Example 7** (continued - anchor set).: _For the set \(U_{{\mathit{s}ToT}}=\{U_{1X},U_{2X}\}\) associated with the causal diagram in Fig. 9, the anchor sets can be computed as follows:_

\[\mathit{AS}(U_{1X}) =\{X,Z_{1}\},\] (103) \[\mathit{AS}(U_{2X}) =\{X,Z_{2}\},\] (104) \[\mathit{AS}(U_{1X},U_{2X}) =\{X,Z_{1},Z_{2}\}.\] (105)

**Example 7** (continued - anchor set exogenous ancestral closure).: _Consider the causal diagram in Fig. 9. With respect to the diagram, we have that_

\[\mathrm{an}^{\mathrm{ex}}(\mathit{AS}(U_{1X})) =\mathrm{an}^{\mathrm{ex}}(X,Z_{1})=\{U_{1X},U_{2X}\},\] (106) \[\mathrm{an}^{\mathrm{ex}}(\mathit{AS}(U_{2X})) =\mathrm{an}^{\mathrm{ex}}(X,Z_{2})=\{U_{1X},U_{2X}\},\] (107) \[\mathrm{an}^{\mathrm{ex}}(\mathit{AS}(\{U_{1X},U_{2X}\})) =\mathrm{an}^{\mathrm{ex}}(X,Z_{1},Z_{2})=\{U_{1X},U_{2X}\}.\] (108)

_Therefore, \(\{U_{1X},U_{2X}\}\) satisfies anchor set exogenous ancestral closure, whereas \(U_{1X}\) and \(U_{2X}\) do not, since for instance \(U_{1X}\) has \(X\) in its anchor set, but \(X\) has \(U_{2X}\) as its ancestor._

We now consider an example of effect identification based on Thm. 4:

**Example 8** (Thm. 4 Application).: _Consider the causal diagram in Fig. 9(a). Consider the query \(\mathbb{E}(y\mid x_{1}^{U_{12}})\) associated with a partially abducted submodel in which the noise variable \(U_{12}\) determining the values of \(Z_{1},Z_{2}\) is not updated according to evidence. Based on Thm. 4, we verify that_

1. \(X,Y\) _are not in the anchor set_ \(AS(U_{12})=\{Z_{1},Z_{2}\}\)_;_
2. \(\mathrm{an}^{\mathrm{ex}}(AS(U_{12}))=\mathrm{an}^{\alpha}(Z_{1},Z_{2})=U_{12}\) _meaning that_ \(U_{12}\) _satisfies anchor set exogenous ancestral closure (ASEAC)._

_Therefore, the query \(\mathbb{E}(y\mid x_{1}^{U_{12}})\) is identifiable from observational data. To witness, we expand the query as:_

\[\mathbb{E}(y\mid x_{1}^{U_{12}}) =\sum_{u_{12}}P(u_{12})\mathbb{E}(y\mid x_{1},u_{12})\] (109) \[=\sum_{u_{12},z_{1},z_{2}}P(u_{12})\mathbb{E}(y\mid x_{1},u_{12}, z_{1},z_{2})\mathbb{1}(Z_{1}(u_{12})=z_{1},Z_{2}(u_{12})=z_{2})\] (110) \[=\sum_{u_{12},z_{1},z_{2}}P(u_{12})\mathbb{E}(y\mid x_{1},z_{1}, z_{2})\mathbb{1}(Z_{1}(u_{12})=z_{1},Z_{2}(u_{12})=z_{2})\] (111) \[=\sum_{z_{1},z_{2}}\mathbb{E}(y\mid x_{1},z_{1},z_{2})\sum_{u_{1 2}}P(u_{12})\mathbb{1}(Z_{1}(u_{12})=z_{1},Z_{2}(u_{12})=z_{2})\] (112) \[=\sum_{z_{1},z_{2}}\mathbb{E}(y\mid x_{1},z_{1},z_{2})P(z_{1},z_{ 2})\] (113) \[=\sum_{z_{1},z_{2},z_{3}}\mathbb{E}(y\mid x_{1},z_{1},z_{2},z_{3} )P(z_{1},z_{2})P(z_{3}\mid x_{1},z_{1},z_{2}),\] (114)

_providing an identification expression from observational data._

## Appendix C Discussion of Thm. 4

Thm. 4 introduces a sufficient condition for the identification of quantities under partial abduction (Def. 2). Here, we discuss why some of the conditions in the theorem are necessary, and provide an example that further elucidates the theorem's scope.

Figure 10: Causal diagrams in Exs. 8-9.

[MISSING_PAGE_FAIL:19]

#### SCMs yields

\[\mathbb{E}^{\mathcal{M}_{1}}(y\mid x_{1}^{U_{1X}}) =\frac{31}{40}\] (126) \[\mathbb{E}^{\mathcal{M}_{2}}(y\mid x_{1}^{U_{1X}}) =\frac{3}{4},\] (127)

_demonstrating that the quantity \(\mathbb{E}(y\mid x_{1}^{U_{1X}})\) is not identifiable for the diagram in Fig. 10b._

The above example provides some intuition about why the variable \(X\) cannot be in the anchor set of the variables \(U_{PA}\) that are not updated according to evidence. Very similarly, an analogous example demonstrating that the variable \(Y\) cannot be in the anchor set of \(U_{PA}\) can also be constructed.