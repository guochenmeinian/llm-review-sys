# Strategic Classification under

Unknown Personalized Manipulation

 Han Shao

Toyota Technological Institute Chicago

Chicago, 60637

han@ttic.edu

&Avrim Blum

Toyota Technological Institute Chicago

Chicago, 60637

avrim@ttic.edu

&Omar Montasser

Toyota Technological Institute Chicago

Chicago, 60637

omar@ttic.edu

###### Abstract

We study the fundamental mistake bound and sample complexity in the strategic classification, where agents can strategically manipulate their feature vector up to an extent in order to be predicted as positive. For example, given a classifier determining college admission, student candidates may try to take easier classes to improve their GPA, retake SAT and change schools in an effort to fool the classifier. _Ball manipulations_ are a widely studied class of manipulations in the literature, where agents can modify their feature vector within a bounded radius ball. Unlike most prior work, our work considers manipulations to be _personalized_, meaning that agents can have different levels of manipulation abilities (e.g., varying radii for ball manipulations), and _unknown_ to the learner.

We formalize the learning problem in an interaction model where the learner first deploys a classifier and the agent manipulates the feature vector within their manipulation set to game the deployed classifier. We investigate various scenarios in terms of the information available to the learner during the interaction, such as observing the original feature vector before or after deployment, observing the manipulated feature vector, or not seeing either the original or the manipulated feature vector. We begin by providing online mistake bounds and PAC sample complexity in these scenarios for ball manipulations. We also explore non-ball manipulations and show that, even in the simplest scenario where both the original and the manipulated feature vectors are revealed, the mistake bounds and sample complexity are lower bounded by \(\Omega(|\mathcal{H}|)\) when the target function belongs to a known class \(\mathcal{H}\).

## 1 Introduction

Strategic classification addresses the problem of learning a classifier robust to manipulation and gaming by self-interested agents (Hardt et al., 2016). For example, given a classifier determining loan approval based on credit scores, applicants could open or close credit cards and bank accounts to increase their credit scores. In the case of a college admission classifier, students may try to take easier classes to improve their GPA, retake the SAT or change schools in an effort to be admitted. In both cases, such manipulations do not change their true qualifications. Recently, a collection of papers has studied strategic classification in both the online setting where examples are chosen by an adversary in a sequential manner (Dong et al., 2018; Chen et al., 2020; Ahmadi et al., 2021, 2023), and thedistributional setting where the examples are drawn from an underlying data distribution (Hardt et al., 2016; Zhang and Conitzer, 2021; Sundaram et al., 2021; Lechner and Urner, 2022). Most existing works assume that manipulation ability is uniform across all agents or is known to the learner. However, in reality, this may not always be the case. For instance, low-income students may have a lower ability to manipulate the system compared to their wealthier peers due to factors such as the high costs of retaking the SAT or enrolling in additional classes, as well as facing more barriers to accessing information about college (Milli et al., 2019) and it is impossible for the learner to know the highest achievable GPA or the maximum number of times a student may retake the SAT due to external factors such as socio-economic background and personal circumstances.

We characterize the manipulation of an agent by a set of alternative feature vectors that she can modify her original feature vector to, which we refer to as the _manipulation set_. _Ball manipulations_ are a widely studied class of manipulations in the literature, where agents can modify their feature vector within a bounded radius ball. For example, Dong et al. (2018); Chen et al. (2020); Sundaram et al. (2021) studied ball manipulations with distance function being some norm and Zhang and Conitzer (2021); Lechner and Urner (2022); Ahmadi et al. (2023) studied a manipulation graph setting, which can be viewed as ball manipulation w.r.t. the graph distance on a predefined known graph.

In the online learning setting, the strategic agents come sequentially and try to game the current classifier. Following previous work, we model the learning process as a repeated Stackelberg game over \(T\) time steps. In round \(t\), the learner proposes a classifier \(f_{t}\) and then the agent, with a manipulation set (unknown to the learner), manipulates her feature in an effort to receive positive prediction from \(f_{t}\). There are several settings based on what and when the information is revealed about the original feature vector and the manipulated feature vector in the game. The simplest setting for the learner is observing the original feature vector before choosing \(f_{t}\) and the manipulated vector after. In a slightly harder setting, the learner observes both the original and manipulated vectors after selecting \(f_{t}\). An even harder setting involves observing only the manipulated feature vector after selecting \(f_{t}\). The hardest and least informative scenario occurs when neither the original nor the manipulated feature vectors are observed.

In the distributional setting, the agents are sampled from an underlying data distribution. Previous work assumes that the learner has full knowledge of the original feature vector and the manipulation set, and then views learning as a one-shot game and solves it by computing the Stackelberg equilibria of it. However, when manipulations are personalized and unknown, we cannot compute an equilibrium and study learning as a one-shot game. In this work, we extend the iterative online interaction model from the online setting to the distributional setting, where the sequence of agents is sampled i.i.d. from the data distribution. After repeated learning for \(T\) (which is equal to the sample size) rounds, the learner has to output a strategy-robust predictor for future use.

In both online and distributional settings, examples are viewed through the lens of the current predictor and the learner does not have the ability to inquire about the strategies the previous examples would have adopted under a different predictor.

Related workOur work is primarily related to strategic classification in online and distributional settings. Strategic classification was first studied in a distributional model by Hardt et al. (2016) and subsequently by Dong et al. (2018) in an online model. Hardt et al. (2016) assumed that agents manipulate by best response with respect to a uniform cost function known to the learner. Building on the framework of (Hardt et al., 2016), Lechner and Urner (2022); Sundaram et al. (2021); Zhang and Conitzer (2021); Hu et al. (2019); Milli et al. (2019) studied the distributional learning problem, and all of them assumed that the manipulations are predefined and known to the learner, either by a cost function or a predefined manipulation graph. For online learning, Dong et al. (2018) considered a similar manipulation setting as in this work, where manipulations are personalized and unknown. However, they studied linear classification with ball manipulations in the online setting and focused on finding appropriate conditions of the cost function to achieve sub-linear Stackelberg regret. Chen et al. (2020) also studied Stackelberg regret in linear classification with uniform ball manipulations. Ahmadi et al. (2021) studied the mistake bound under uniform (possibly unknown) ball manipulations, and Ahmadi et al. (2023) studied regret under a pre-defined and known manipulation. The most relevant work is a recent concurrent study by Lechner et al. (2023), which also explores strategic classification involving unknown personalized manipulations but with a different loss function. In their work, a predictor incurs a loss of \(0\) if and only if the agent refrains from manipulation and the predictor correctly predicts at the unmanipulated feature vector. In our work, the predictor's loss is \(0\)if it correctly predicts at the manipulated feature, even when the agent manipulates. As a result, their loss function serves as an upper bound of our loss function.

There has been a lot of research on various other issues and models in strategic classification. Beyond sample complexity, Hu et al. (2019); Milli et al. (2019) focused on other social objectives, such as social burden and fairness. Recent works also explored different models of agent behavior, including proactive agents Zrnic et al. (2021), non-myopic agents (Haghtalab et al., 2022) and noisy agents (Jagadeesan et al., 2021). Ahmadi et al. (2023) considers two agent models of randomized learners: a randomized algorithm model where the agents respond to the realization, and a fractional classifier model where agents respond to the expectation, and our model corresponds to the randomized algorithm model. Additionally, there is also a line of research on agents interested in improving their qualifications instead of gaming (Kleinberg and Raghavan, 2020; Haghtalab et al., 2020; Ahmadi et al., 2022). Strategic interactions in the regression setting have also been studied (e.g., Bechavod et al. (2021)).

Beyond strategic classification, there is a more general research area of learning using data from strategic sources, such as a single data generation player who manipulates the data distribution (Bruckner and Scheffer, 2011; Dalvi et al., 2004). Adversarial perturbations can be viewed as another type of strategic source (Montasser et al., 2019).

## 2 Model

Strategic classificationThroughout this work, we consider the binary classification task. Let \(\mathcal{X}\) denote the feature vector space, \(\mathcal{Y}=\{+1,-1\}\) denote the label space, and \(\mathcal{H}\subseteq\mathcal{Y}^{\mathcal{X}}\) denote the hypothesis class. In the strategic setting, instead of an example being a pair \((x,y)\), an example, or _agent_, is a triple \((x,u,y)\) where \(x\in\mathcal{X}\) is the original feature vector, \(y\in\mathcal{Y}\) is the label, and \(u\subseteq\mathcal{X}\) is the manipulation set, which is a set of feature vectors that the agent can modify their original feature vector \(x\) to. In particular, given a hypothesis \(h\in\mathcal{Y}^{\mathcal{X}}\), the agent will try to manipulate her feature vector \(x\) to another feature vector \(x^{\prime}\) within \(u\) in order to receive a positive prediction from \(h\). The manipulation set \(u\) is _unknown_ to the learner. In this work, we will be considering several settings based on what the information is revealed to the learner, including both the original/manipulated feature vectors, the manipulated feature vector only, or neither, and when the information is revealed.

More formally, for agent \((x,u,y)\), given a predictor \(h\), if \(h(x)=-1\) and her manipulation set overlaps the positive region by \(h\), i.e., \(u\cap\mathcal{X}_{h,+}\neq\emptyset\) with \(\mathcal{X}_{h,+}:=\{x\in\mathcal{X}|h(x)=+1\}\), the agent will manipulate \(x\) to \(\Delta(x,h,u)\in u\cap\mathcal{X}_{h,+}\)1 to receive positive prediction by \(h\). Otherwise, the agent will do nothing and maintain her feature vector at \(x\), i.e., \(\Delta(x,h,u)=x\). We call \(\Delta(x,h,u)\) the manipulated feature vector of agent \((x,u,y)\) under predictor \(h\).

Footnote 1: For ball manipulations, agents break ties by selecting the closest vector. When there are multiple closest vectors, agents break ties arbitrarily. For non-ball manipulations, agents break ties in any fixed way.

A general and fundamental type of manipulations is _ball manipulations_, where agents can manipulate their feature within a ball of _personalized_ radius. More specifically, given a metric \(d\) over \(\mathcal{X}\), the manipulation set is a ball \(\mathcal{B}(x;r)=\{x^{\prime}|d(x,x^{\prime})\leq r\}\) centered at \(x\) with radius \(r\) for some \(r\in\mathbb{R}_{\geq 0}\). Note that we allow different agents to have different manipulation power and the radius can vary over agents. Let \(Q\) denote the set of allowed pairs \((x,u)\), which we refer to as the feature-manipulation set space. For ball manipulations, we have \(\mathcal{Q}=\{(x,\mathcal{B}(x;r))|x\in\mathcal{X},r\in\mathbb{R}_{\geq 0}\}\) for some known metric \(d\) over \(\mathcal{X}\). In the context of ball manipulations, we use \((x,r,y)\) to represent \((x,\mathcal{B}(x;r),y)\) and \(\Delta(x,h,r)\) to represent \(\Delta(x,h,\mathcal{B}(x;r))\) for notation simplicity.

For any hypothesis \(h\), let the strategic loss \(\ell^{\text{str}}(h,(x,u,y))\) of \(h\) be defined as the loss at the manipulated feature, i.e., \(\ell^{\text{str}}(h,(x,u,y)):=\mathds{1}(h(\Delta(x,h,u))\neq y)\). According to our definition of \(\Delta(\cdot)\), we can write down the strategic loss explicitly as

\[\ell^{\text{str}}(h,(x,u,y))=\begin{cases}1&\text{if }y=-1,h(x)=+1\\ 1&\text{if }y=-1,h(x)=-1\text{ and }u\cap\mathcal{X}_{h,+}\neq\emptyset\,,\\ 1&\text{if }y=+1,h(x)=-1\text{ and }u\cap\mathcal{X}_{h,+}=\emptyset\,,\\ 0&\text{otherwise.}\end{cases} \tag{1}\]

For any randomized predictor \(p\) (a distribution over hypotheses), the strategic behavior depends on the realization of the predictor and the strategic loss of \(p\) is \(\ell^{\text{str}}(p,(x,u,y)):=\mathbb{E}_{h\sim p}\left[\ell^{\text{str}}(h,(x,u,y))\right]\).

Online learningWe consider the task of sequential classification where the learner aims to classify a sequence of agents \((x_{1},u_{1},y_{1}),(x_{2},u_{2},y_{2}),\ldots,(x_{T},u_{T},y_{T})\in\mathcal{Q} \times\mathcal{Y}\) that arrives in an online manner. At each round, the learner feeds a predictor to the environment and then observes his prediction \(\widehat{y}_{t}\), the true label \(y_{t}\) and possibly along with some additional information about the original/manipulated feature vectors. We say the learner makes a mistake at round \(t\) if \(\widehat{y}_{t}\neq y_{t}\) and the learner's goal is to minimize the number of mistakes on the sequence. The interaction protocol (which repeats for \(t=1,\ldots,T\)) is described in the following.

```
1:The environment picks an agent \((x_{t},u_{t},y_{t})\) and reveals some context \(C(x_{t})\). In the online setting, the agent is chosen adversarially, while in the distributional setting, the agent is sampled i.i.d.
2:The learner \(\mathcal{A}\) observes \(C(x_{t})\) and picks a hypothesis \(f_{t}\in\mathcal{Y}^{\mathcal{X}}\).
3:The learner \(\mathcal{A}\) observes the true label \(y_{t}\), the prediction \(\widehat{y}_{t}=f_{t}(\Delta_{t})\), and some feedback \(F(x_{t},\Delta_{t})\), where \(\Delta_{t}=\Delta(x_{t},f_{t},u_{t})\) is the manipulated feature vector.
```

**Protocol 1** Learner-Agent Interaction at round \(t\)

The context function \(C(\cdot)\) and feedback function \(F(\cdot)\) reveals information about the original feature vector \(x_{t}\) and the manipulated feature vector \(\Delta_{t}\). \(C(\cdot)\) reveals the information before the learner picks \(f_{t}\) while \(F(\cdot)\) does after. We study several different settings based on what and when information is revealed.

* The simplest setting for the learner is observing the original feature vector \(x_{t}\) before choosing \(f_{t}\) and the manipulated vector \(\Delta_{t}\) after. Consider a teacher giving students a writing assignment or take-home exam. The teacher might have a good knowledge of the students' abilities (which correspond to the original feature vector \(x_{t}\)) based on their performance in class, but the grade has to be based on how well they do the assignment. The students might manipulate by using the help of ChatGPT / Google / WolframAlpha / their parents, etc. The teacher wants to create an assignment that will work well even in the presence of these manipulation tools. In addition, If we think of each example as representing a subpopulation (e.g., an organization is thinking of offering loans to a certain group), then there might be known statistics about that population, even though the individual classification (loan) decisions have to be made based on responses to the classifier. This setting corresponds to \(C(x_{t})=x_{t}\) and \(F(x_{t},\Delta_{t})=\Delta_{t}\). We denote a setting by their values of \(C,F\) and thus, we denote this setting by \((x,\Delta)\).
* In a slightly harder setting, the learner observes both the original and manipulated vectors after selecting \(f_{t}\) and thus, \(f_{t}\) cannot depend on the original feature vector in this case. For example, if a high-school student takes the SAT test multiple times, most colleges promise to only consider the highest one (or even to "superscore" the test by considering the highest score separately in each section) but they do require the student to submit all of them. Then \(C(x_{t})=\perp\) and \(F(x_{t},\Delta_{t})=(x_{t},\Delta_{t})\), where \(\perp\) is a token for "no information", and this setting is denoted by \((\perp,(x,\Delta))\).
* An even harder setting involves observing only the manipulated feature vector after selecting \(f_{t}\) (which can only be revealed after \(f_{t}\) since \(\Delta_{t}\) depends on \(f_{t}\)). Then \(C(x_{t})=\perp\) and \(F(x_{t},\Delta_{t})=\Delta_{t}\) and this setting is denoted by \((\perp,\Delta)\).
* The hardest and least informative scenario occurs when neither the original nor the manipulated feature vectors are observed. Then \(C(x_{t})=\perp\) and \(F(x_{t},\Delta_{t})=\perp\) and it is denoted by \((\perp,\perp)\).

Throughout this work, we focus on the _realizable_ setting, where there exists a perfect classifier in \(\mathcal{H}\) that never makes any mistake at the sequence of strategic agents. More specifically, there exists a hypothesis \(h^{*}\in\mathcal{H}\) such that for any \(t\in[T]\), we have \(y_{t}=h^{*}(\Delta(x_{t},h^{*},u_{t}))\)2. Then we define the mistake bound as follows.

Footnote 2: It is possible that there is no hypothesis \(\overline{h}\in\mathcal{Y}^{\mathcal{X}}\) s.t. \(y_{t}=\overline{h}(x_{t})\) for all \(t\in[T]\).

**Definition 1**.: _For any choice of \((C,F)\), let \(\mathcal{A}\) be an online learning algorithm under Protocol 1 in the setting of \((C,F)\). Given any realizable sequence \(S=((x_{1},u_{1},h^{*}(\Delta(x_{1},h^{*},u_{1}))),\ldots,(x_{T},u_{T},h^{*}( \Delta(x_{T},h^{*},u_{T})))\in(\mathcal{Q}\times\mathcal{Y})^{T}\), where \(T\) is any integer and \(h^{*}\in\mathcal{H}\), let \(\mathcal{M}_{\mathcal{A}}(S)\) be the number of mistakes \(\mathcal{A}\) makes on the sequence \(S\). The mistake bound of \((\mathcal{H},\mathcal{Q})\), denoted \(\mathrm{MB}_{C,F}\), is the smallest number \(B\in\mathbb{N}\) such that there exists an algorithm \(\mathcal{A}\) such that \(\mathcal{M}_{\mathcal{A}}(S)\leq B\) over all realizable sequences \(S\) of the above form._According the rank of difficulty of the four settings with different choices of \((C,F)\), the mistake bounds are ranked in the order of \(\mathrm{MB}_{x,\Delta}\leq\mathrm{MB}_{\perp,(x,\Delta)}\leq\mathrm{MB}_{\perp, \Delta}\leq\mathrm{MB}_{\perp,\perp}\).

PAC learningIn the distributional setting, the agents are sampled from an underlying distribution \(\mathcal{D}\) over \(\mathcal{Q}\times\mathcal{Y}\). The learner's goal is to find a hypothesis \(h\) with low population loss \(\mathcal{L}_{\mathcal{D}}^{\text{st}}(h):=\mathbb{E}_{(x,u,y)\sim\mathcal{D}} \left[\text{$\ell^{\text{str}}(h,(x,u,y))$}\right]\). One may think of running empirical risk minimizer (ERM) over samples drawn from the underlying data distribution, i.e., returning \(\arg\min_{h\in\mathcal{H}}\frac{1}{m}\sum_{i=1}^{m}\ell^{\text{str}}(h,(x_{i}, u_{i},y_{i}))\), where \((x_{1},u_{1},y_{1}),\ldots,(x_{m},u_{m},y_{m})\) are i.i.d. sampled from \(\mathcal{D}\). However, ERM is unimplementable because the manipulation sets \(u_{i}\)'s are never revealed to the algorithm, and only the partial feedback in response to the implemented classifier is provided. In particular, in this work we consider using the same interaction protocol as in the online setting, i.e., Protocol 1, with agents \((x_{t},u_{t},y_{t})\) i.i.d. sampled from the data distribution \(\mathcal{D}\). After \(T\) rounds of interaction (i.e., \(T\) i.i.d. agents), the learner has to output a predictor \(f_{\mathrm{out}}\) for future use.

Again, we focus on the _realizable_ setting, where the sequence of sampled agents (with manipulation) can be perfectly classified by a target function in \(\mathcal{H}\). Alternatively, there exists a classifier with zero population loss, i.e., there exists a hypothesis \(h^{*}\in\mathcal{H}\) such that \(\mathcal{L}_{\mathcal{D}}^{\text{str}}(h^{*})=0\). Then we formalize the notion of PAC sample complexity under strategic behavior as follows.

**Definition 2**.: _For any choice of \((C,F)\), let \(\mathcal{A}\) be a learning algorithm that interacts with agents using Protocol 1 in the setting of \((C,F)\) and outputs a predictor \(f_{\mathrm{out}}\) in the end. For any \(\varepsilon,\delta\in(0,1)\), the sample complexity of realizable \((\varepsilon,\delta)\)-PAC learning of \((\mathcal{H},\mathcal{Q})\), denoted \(\mathrm{SC}_{C,F}(\varepsilon,\delta)\), is defined as the smallest \(m\in\mathbb{N}\) for which there exists a learning algorithm \(\mathcal{A}\) in the above form such that for any distribution \(\mathcal{D}\) over \(\mathcal{Q}\times\mathcal{Y}\) where there exists a predictor \(h^{*}\in\mathcal{H}\) with zero loss, \(\mathcal{L}_{\mathcal{D}}^{\text{str}}(h)=0\), with probability at least \(1-\delta\) over \((x_{1},u_{1},y_{1}),\ldots,(x_{m},u_{m},y_{m})\overset{i.i.d}{\sim}\mathcal{D}\), \(\mathcal{L}_{\mathcal{D}}^{\text{str}}(f_{\mathrm{out}})\leq\varepsilon\)._

Similar to mistake bounds, the sample complexities are ranked in the same order \(\mathrm{SC}_{x,\Delta}\leq\mathrm{SC}_{\perp,(x,\Delta)}\leq\mathrm{SC}_{\perp,\Delta}\leq\mathrm{SC}_{\perp,\perp}\) according to the rank of difficulty of the four settings.

## 3 Overview of Results

In classic (non-strategic) online learning, the Halving algorithm achieves a mistake bound of \(\log(|\mathcal{H}|)\) by employing the majority vote and eliminating inconsistent hypotheses at each round. In classic PAC learning, the sample complexity of \(\mathcal{O}(\frac{\log(|\mathcal{H}|)}{\varepsilon})\) is achievable via ERM. Both mistake bound and sample complexity exhibit logarithmic dependency on \(|\mathcal{H}|\). This logarithmic dependency on \(|\mathcal{H}|\) (when there is no further structural assumptions) is tight in both settings, i.e., there exist examples of \(\mathcal{H}\) with mistake bound of \(\Omega(\log(|\mathcal{H}|))\) and with sample complexity of \(\Omega(\frac{\log(|\mathcal{H}|)}{\varepsilon})\). In the setting where manipulation is known beforehand and only \(\Delta_{t}\) is observed, Ahmadi et al. (2023) proved a lower bound of \(\Omega(|\mathcal{H}|)\) for the mistake bound. Since in the strategic setting we can achieve a linear dependency on \(|\mathcal{H}|\) by trying each hypothesis in \(\mathcal{H}\) one by one and discarding it once it makes a mistake, the question arises:

_Can we achieve a logarithmic dependency on \(|\mathcal{H}|\) in strategic classification?_

In this work, we show that the dependency on \(|\mathcal{H}|\) varies across different settings and that in some settings mistake bound and PAC sample complexity can exhibit different dependencies on \(|\mathcal{H}|\). We start by presenting our results for ball manipulations in the four settings.

* Setting of \((x,\Delta)\) (observing \(x_{t}\) before choosing \(f_{t}\) and observing \(\Delta_{t}\) after) : For online learning, we propose an variant of the Halving algorithm, called Strategic Halving (Algorithm 1), which can eliminate half of the remaining hypotheses when making a mistake. The algorithm depends on observing \(x_{t}\) before choosing the predictor \(f_{t}\). Then by applying the standard technique of converting mistake bound to PAC bound, we are able to achieve sample complexity of \(\mathcal{O}(\frac{\log(|\mathcal{H}|)\log(|\mathcal{H}|)}{\varepsilon})\).
* Setting of \((\perp,(x,\Delta))\) (observing both \(x_{t}\) and \(\Delta_{t}\) after selecting \(f_{t}\)) : We prove that, there exists an example of \((\mathcal{H},\mathcal{Q})\) s.t. the mistake bound is lower bounded by \(\Omega(|\mathcal{H}|)\). This implies that no algorithm can perform significantly better than sequentially trying each hypothesis, which would make at most \(|\mathcal{H}|\) mistakes before finding the correct hypothesis. However, unlike the construction of mistake lower bounds in classic online learning, where all mistakes can be forced to occur in the initial rounds, we demonstrate that we require \(\Theta(|\mathcal{H}|^{2})\) rounds to ensure that all mistakes occur. Inthe PAC setting, we first show that, any learning algorithm with proper output \(f_{\text{out}}\), i.e., \(f_{\text{out}}\in\mathcal{H}\), needs a sample size of \(\Omega(\frac{|\mathcal{H}|}{\varepsilon})\). We can achieve a sample complexity of \(O(\frac{\log^{2}(|\mathcal{H}|)}{\varepsilon})\) by executing Algorithm 2, which is a randomized algorithm with improper output.
* Setting of \((\bot,\Delta)\) (observing only \(\Delta_{t}\) after selecting \(f_{t}\)) : The mistake bound of \(\Omega(|\mathcal{H}|)\) also holds in this setting, as it is known to be harder than the previous setting. For the PAC learning, we show that any conservative algorithm, which only depends on the information from the mistake rounds, requires \(\Omega(\frac{|\mathcal{H}|}{\varepsilon})\) samples. The optimal sample complexity is left as an open problem.
* Setting of \((\bot,\bot)\) (observing neither \(x_{t}\) nor \(\Delta_{t}\)) : Similarly, the mistake bound of \(\Omega(|\mathcal{H}|)\) still holds. For the PAC learning, we show that the sample complexity is \(\Omega(\frac{|\mathcal{H}|}{\varepsilon})\) by reducing the problem to a stochastic linear bandit problem.

Then we move on to non-ball manipulations. However, we show that even in the simplest setting of observing \(x_{t}\) before choosing \(f_{t}\) and observing \(\Delta_{t}\) after, there is an example of \((\mathcal{H},\mathcal{Q})\) such that the sample complexity is \(\widetilde{\Omega}(\frac{|\mathcal{H}|}{\varepsilon})\). This implies that in all four settings of different revealed information, we will have sample complexity of \(\widetilde{\Omega}(\frac{|\mathcal{H}|}{\varepsilon})\) and mistake bound of \(\widetilde{\Omega}(|\mathcal{H}|)\). We summarize our results in Table 1.

## 4 Ball manipulations

In ball manipulations, when \(\mathcal{B}(x;r)\cap\mathcal{X}_{h,+}\) has multiple elements, the agent will always break ties by selecting the one closest to \(x\), i.e., \(\Delta(x,h,r)=\arg\min_{x^{\prime}\in\mathcal{B}(x;r)\cap\mathcal{X}_{h,+}}d( x,x^{\prime})\). In round \(t\), the learner deploys predictor \(f_{t}\), and once he knows \(x_{t}\) and \(\widetilde{y}_{t}\), he can calculate \(\Delta_{t}\) himself without needing knowledge of \(r_{t}\) by

\[\Delta_{t}=\begin{cases}\arg\min_{x^{\prime}\in\mathcal{X}_{f_{t},+}}d(x_{t},x ^{\prime})&\text{ if }\widetilde{y}_{t}=+1\,,\\ x_{t}&\text{ if }\widetilde{y}_{t}=-1\,.\end{cases}\]

Thus, for ball manipulations, knowing \(x_{t}\) is equivalent to knowing both \(x_{t}\) and \(\Delta_{t}\).

### Setting \((x,\Delta)\): Observing \(x_{t}\) Before Choosing \(f_{t}\)

Online learningWe propose a new algorithm with mistake bound of \(\log(|\mathcal{H}|)\) in setting \((x,\Delta)\). To achieve a logarithmic mistake bound, we must construct a predictor \(f_{t}\) such that if it makes a mistake, we can reduce a constant fraction of the remaining hypotheses. The primary challenge is that we do not have access to the full information, and predictions of other hypotheses are hidden. To extract the information of predictions of other hypotheses, we take advantage of ball manipulations, which

\begin{table}
\begin{tabular}{c|c|c|c}  & setting & mistake bound & sample complexity \\ \hline \multirow{4}{*}{ball} & \((x,\Delta)\) & \(\Theta(\log(|\mathcal{H}|))\) (Thm 1) & \(\widetilde{\mathcal{O}}(\frac{\log(|\mathcal{H}|)}{\varepsilon})^{a}\) (Thm 2), \(\Omega(\frac{\log(|\mathcal{H}|)}{\varepsilon})\) \\ \cline{2-4}  & \((\bot,(x,\Delta))\) & \(\begin{array}{l}\mathcal{O}(\min(\sqrt{\log(|\mathcal{H}|)T},|\mathcal{H}|)) \text{ (Thm 4)}\\ \Omega(\min(\frac{T}{|\mathcal{H}|\log(|\mathcal{H}|)},|\mathcal{H}|))\text{ (Thm 3)} \\ \end{array}\) & \(\mathcal{O}(\frac{\log(|\mathcal{H}|)}{\varepsilon})\) (Thm 6), \(\Omega(\frac{\log(|\mathcal{H}|)}{\varepsilon})\) \\ \cline{2-4}  & \((\bot,\Delta)\) & \(\Theta(|\mathcal{H}|)\) (implied by Thm 3) & \(\text{SC}^{\text{csv}}=\widetilde{\Omega}(\frac{|\mathcal{H}|}{\varepsilon})\) (Thm 7) \\ \cline{2-4}  & \((\bot,\bot)\) & \(\Theta(|\mathcal{H}|)\) (implied by Thm 3) & \(\widetilde{\mathcal{O}}(\frac{|\mathcal{H}|}{\varepsilon})\), \(\widetilde{\Omega}(\frac{|\mathcal{H}|}{\varepsilon})\) (Thm 8) \\ \hline nonball & all & \(\widetilde{\Omega}(|\mathcal{H}|)\)(Cor 1), \(\mathcal{O}(|\mathcal{H}|)\) & \(\widetilde{\mathcal{O}}(\frac{|\mathcal{H}|}{\varepsilon})\), \(\widetilde{\Omega}(\frac{|\mathcal{H}|}{\varepsilon})\) (Cor 1) \\ \hline \end{tabular} \({}^{a}\) A factor of \(\log\)log\((|\mathcal{H}|)\) is neglected.

\end{table}
Table 1: The summary of results. \(\widetilde{\mathcal{O}}\) and \(\widetilde{\Omega}\) ignore logarithmic factors on \(|\mathcal{H}|\) and \(\frac{1}{\varepsilon}\). The superscripts prop stands for proper learning algorithms and csv stands for conservative learning algorithms. All lower bounds in the non-strategic setting also apply to the strategic setting, implying that \(\operatorname{MB}_{C,F}\geq\Omega(\log(|\mathcal{H}|))\) and \(\text{SC}_{C,F}\geq\Omega(\frac{\log(|\mathcal{H}|)}{\varepsilon})\) for all settings of \((C,F)\). In all four settings, a mistake bound of \(\mathcal{O}(|\mathcal{H}|)\) can be achieved by simply trying each hypothesis in \(\mathcal{H}\) while the sample complexity can be achieved as \(\widetilde{\mathcal{O}}(\frac{|\mathcal{H}|}{\varepsilon})\) by converting the mistake bound of \(\mathcal{O}(|\mathcal{H}|)\) to a PAC bound using standard techniques.

induces an ordering over all hypotheses. Specifically, for any hypothesis \(h\) and feature vector \(x\), we define the distance between \(x\) and \(h\) by the distance between \(x\) and the positive region by \(h\), \(\mathcal{X}_{h,+}\), i.e.,

\[d(x,h):=\min\{d(x,x^{\prime})|x^{\prime}\in\mathcal{X}_{h,+}\}\,. \tag{2}\]

At each round \(t\), given \(x_{t}\), the learner calculates the distance \(d(x_{t},h)\) for all \(h\) in the version space (meaning hypotheses consistent with history) and selects a hypothesis \(f_{t}\) such that \(d(x_{t},f_{t})\) is the median among all distances \(d(x_{t},h)\) for \(h\) in the version space. We can show that by selecting \(f_{t}\) in this way, the learner can eliminate half of the version space if \(f_{t}\) makes a mistake. We refer to this algorithm as Strategic Halving, and provide a detailed description of it in Algorithm 1.

**Theorem 1**.: _For any feature-ball manipulation set space \(\mathcal{Q}\) and hypothesis class \(\mathcal{H}\), Strategic Halving achieves mistake bound \(\mathrm{MB}_{x,\Delta}\leq\log(|\mathcal{H}|)\)._

```
1:Initialize the version space \(\mathrm{VS}=\mathcal{H}\).
2:for\(t=1,\ldots,T\)do
3: pick an \(f_{t}\in\mathrm{VS}\) such that \(d(x_{t},f_{t})\) is the median of \(\{d(x_{t},h)|h\in\mathrm{VS}\}\).
4:if\(\widehat{y}_{t}\neq y_{t}\) and \(y_{t}=+\)then\(\mathrm{VS}\leftarrow\mathrm{VS}\setminus\{h\in\mathrm{VS}|d(x_{t},h)\geq d (x_{t},f_{t})\}\);
5:elseif\(\widehat{y}_{t}\neq y_{t}\) and \(y_{t}=-\)then\(\mathrm{VS}\leftarrow\mathrm{VS}\setminus\{h\in\mathrm{VS}|d(x_{t},h)\leq d (x_{t},f_{t})\}\).
6:endfor
```

**Algorithm 1** Strategic Halving

To prove Theorem 1, we only need to show that each mistake reduces the version space by half. Supposing that \(f_{t}\) misclassifies a true positive example \((x_{t},r_{t},+1)\) by negative, then we know that \(d(x_{t},f_{t})>r_{t}\) while the target hypothesis \(h^{*}\) must satisfy that \(d(x_{t},h^{*})\leq r_{t}\). Hence any \(h\) with \(d(x_{t},h)\geq d(x_{t},f_{t})\) cannot be \(h^{*}\) and should be eliminated. Since \(d(x_{t},f_{t})\) is the median of \(\{d(x_{t},h)|h\in\mathrm{VS}\}\), we can elimate half of the version space. It is similar when \(f_{t}\) misclassifies a true negative. The detailed proof is deferred to Appendix B.

PAC learningWe can convert Strategic Halving to a PAC learner by the standard technique of converting a mistake bound to a PAC bound (Gallant, 1986). Specifically, the learner runs Strategic Halving until it produces a hypothesis \(f_{t}\) that survives for \(\frac{1}{\varepsilon}\log(\frac{\log(|\mathcal{H}|)}{\delta})\) rounds and outputs this \(f_{t}\). Then we have Theorem 2, and the proof is included in Appendix C.

**Theorem 2**.: _For any feature-ball manipulation set space \(\mathcal{Q}\) and hypothesis class \(\mathcal{H}\), we can achieve \(\mathrm{SC}_{x,\Delta}(\varepsilon,\delta)=\mathcal{O}(\frac{\log(|\mathcal{ H}|)}{\varepsilon}\log(\frac{\log(|\mathcal{H}|)}{\delta}))\) by combining Strategic Halving and the standard technique of converting a mistake bound to a PAC bound._

### Setting \((\bot,(x,\Delta))\): Observing \(x_{t}\) After Choosing \(f_{t}\)

When \(x_{t}\) is not revealed before the learner choosing \(f_{t}\), the algorithm of Strategic Halving does not work anymore. We demonstrate that it is impossible to reduce constant fraction of version space when making a mistake, and prove that the mistake bound is lower bounded by \(\Omega(|\mathcal{H}|)\) by constructing a negative example of \((\mathcal{H},\mathcal{Q})\). However, we can still achieve sample complexity with poly-logarithmic dependency on \(|\mathcal{H}|\) in the distributional setting.

#### 4.2.1 Results in the Online Learning Model

To offer readers an intuitive understanding of the distinctions between the strategic setting and standard online learning, we commence by presenting an example in which no deterministic learners, including the Halving algorithm, can make fewer than \(|\mathcal{H}|-1\) mistakes.

**Example 1**.: _Consider a star shape metric space \((\mathcal{X},d)\), where \(\mathcal{X}=\{0,1,\ldots,n\}\), \(d(i,j)=2\) and \(d(0,i)=1\) for all \(i,j\in[n]\) with \(i\neq j\). The hypothesis class is composed of singletons over \([n]\), i.e., \(\mathcal{H}=\{2\mathds{1}_{\{i\}}-1|i\in[n]\}\). When the learner is deterministic, the environment can pick an agent \((x_{t},r_{t},y_{t})\) dependent on \(f_{t}\). If \(f_{t}\) is all-negative, then the environment picks \((x_{t},r_{t},y_{t})=(0,1,+1)\), and then the learner makes a mistake but no hypothesis can be eliminated. If \(f_{t}\) predicts \(0\) by positive, the environment will pick \((x_{t},r_{t},y_{t})=(0,0,-1)\), and then the learner makes a mistake but no hypothesis can be eliminated. If \(f_{t}\) predicts some \(i\in[n]\) by positive, the environment will pick \((x_{t},r_{t},y_{t})=(i,0,-1)\), and then the learner makes a mistake with only one hypothesis \(2\mathds{1}_{\{i\}}-1\) eliminated. Therefore, the learner will make \(n-1\) mistakes._In this work, we allow the learner to be randomized. When an \((x_{t},r_{t},y_{t})\) is generated by the environment, the learner can randomly pick an \(f_{t}\), and the environment does not know the realization of \(f_{t}\) but knows the distribution where \(f_{t}\) comes from. It turns out that randomization does not help much. We prove that there exists an example in which any (possibly randomized) learner will incur \(\Omega(|\mathcal{H}|)\) mistakes.

**Theorem 3**.: _There exists a feature-ball manipulation set space \(\mathcal{Q}\) and hypothesis class \(\mathcal{H}\) s.t. the mistake bound \(\mathrm{MB}_{\perp,(x,\Delta)}\geq|\mathcal{H}|-1\). For any (randomized) algorithm \(\mathcal{A}\) and any \(T\in\mathbb{N}\), there exists a realizable sequence of \((x_{t},r_{t},y_{t})_{1:T}\) such that with probability at least \(1-\delta\) (over randomness of \(\mathcal{A}\)), \(\mathcal{A}\) makes at least \(\min(\frac{T}{5|\mathcal{H}|\log(|\mathcal{H}|/\delta)},|\mathcal{H}|-1)\) mistakes._

Essentially, we design an adversarial environment such that the learner has a probability of \(\frac{1}{|\mathcal{H}|}\) of making a mistake at each round before identifying the target function \(h^{*}\). The learner only gains information about the target function when a mistake is made. The detailed proof is deferred to Appendix D. Theorem 3 establishes a lower bound on the mistake bound, which is \(|\mathcal{H}|-1\). However, achieving this bound requires a sufficiently large number of rounds, specifically \(T=\widetilde{\Omega}(|\mathcal{H}|^{2})\). This raises the question of whether there exists a learning algorithm that can make \(o(T)\) mistakes for any \(T\leq|\mathcal{H}|^{2}\). In Example 1, we observed that the adversary can force any deterministic learner to make \(|\mathcal{H}|-1\) mistakes in \(|\mathcal{H}|-1\) rounds. Consequently, no deterministic algorithm can achieve \(o(T)\) mistakes.

To address this, we propose a randomized algorithm that closely resembles Algorithm 1, with a modification in the selection of \(f_{t}\). Instead of using line 3, we choose \(f_{t}\) randomly from VS since we lack prior knowledge of \(x_{t}\). This algorithm can be viewed as a variation of the well-known multiplicative weights method, applied exclusively during mistake rounds. For improved clarity, we present this algorithm as Algorithm 3 in Appendix E due to space limitations.

**Theorem 4**.: _For any \(T\in\mathbb{N}\), Algorithm 3 will make at most \(\min(\sqrt{4\log(|\mathcal{H}|)T},|\mathcal{H}|-1)\) mistakes in expectation in \(T\) rounds._

Note that the \(T\)-dependent upper bound in Theorem 4 matches the lower bound in Theorem 3 up to a logarithmic factor when \(T=|\mathcal{H}|^{2}\). This implies that approximately \(|\mathcal{H}|^{2}\) rounds are needed to achieve \(|\mathcal{H}|-1\) mistakes, which is a tight bound up to a logarithmic factor. Proof of Theorem 4 is included in Appendix E.

#### 4.2.2 Results in the PAC Learning Model

In the PAC setting, the goal of the learner is to output a predictor \(f_{\mathrm{out}}\) after the repeated interactions. A common class of learning algorithms, which outputs a hypothesis \(f_{\mathrm{out}}\in\mathcal{H}\), is called proper. Proper learning algorithms are a common starting point when designing algorithms for new learning problems due to their natural appeal and ability to achieve good performance, such as ERM in classic PAC learning. However, in the current setting, we show that proper learning algorithms do not work well and require a sample size linear in \(|\mathcal{H}|\). The formal theorem is stated as follows and the proof is deferred to Appendix F.

**Theorem 5**.: _There exists a feature-ball manipulation set space \(\mathcal{Q}\) and hypothesis class \(\mathcal{H}\) s.t. \(\mathrm{SC}^{\prime\textit{mpp}}_{\perp,\Delta}(\varepsilon,\frac{7}{8})= \Omega(\frac{|\mathcal{H}|}{\varepsilon})\), where \(\mathrm{SC}^{\prime\textit{mpp}}_{\perp,\Delta}(\varepsilon,\delta)\) is the \((\varepsilon,\delta)\)-PAC sample complexity achievable by proper algorithms._

Theorem 5 implies that any algorithm capable of achieving sample complexity sub-linear in \(|\mathcal{H}|\) must be improper. As a result, we are inspired to devise an improper learning algorithm. Before presenting the algorithm, we introduce some notations. For two hypotheses \(h_{1},h_{2}\), let \(h_{1}\lor h_{2}\) denote the union of them, i.e., \((h_{1}\lor h_{2})(x)=+1\) iff. \(h_{1}(x)=+1\) or \(h_{2}(x)=+1\). Similarly, we can define the union of more than two hypotheses. Then for any union of \(k\) hypotheses, \(f=\vee_{i=1}^{k}h_{i}\), the positive region of \(f\) is the union of positive regions of the \(k\) hypotheses and thus, we have \(d(x,f)=\min_{i\in[k]}d(x,h_{i})\). Therefore, we can decrease the distance between \(f\) and any feature vector \(x\) by increasing \(k\). Based on this, we devise a new randomized algorithm with improper output, described in Algorithm 2.

**Theorem 6**.: _For any feature-ball manipulation set space \(\mathcal{Q}\) and hypothesis class \(\mathcal{H}\), we can achieve \(\mathrm{SC}_{\perp,(x,\Delta)}(\varepsilon,\delta)=\mathcal{O}(\frac{\log^{2} (|\mathcal{H}|)+\log(1/\delta)}{\varepsilon}\log(\frac{1}{\delta}))\) by combining Algorithm 2 with a standard confidence boosting technique. Note that the algorithm is improper._Now we outline the high-level ideas behind Algorithm 2. In correct rounds where \(f_{t}\) makes no mistake, the predictions of all hypotheses are either correct or unknown, and thus, it is hard to determine how to make updates. In mistake rounds, we can always update the version space similar to what was done in Strategic Halving. To achieve a poly-logarithmic dependency on \(|\mathcal{H}|\), we aim to reduce a significant number of misclassifying hypotheses in mistake rounds. The maximum number we can hope to reduce is a constant fraction of the misclassifying hypotheses. We achieve this by randomly sampling a \(f_{t}\) (lines 3-5) s.t. \(f_{t}\) makes a mistake, and \(d(x_{t},f_{t})\) is greater (smaller) than the median of \(d(x_{t},h)\) for all misclassifying hypotheses \(h\) for true negative (positive) examples. However, due to the asymmetric nature of manipulation, which aims to be predicted as positive, the rate of decreasing misclassifications over true positives is slower than over true negatives. To compensate for this asymmetry, we output a \(f_{\mathrm{out}}=h_{1}\lor h_{2}\) with two selected hypotheses \(h_{1},h_{2}\) (lines 10-11) instead of a single one to increase the chance of positive prediction.

We prove that Algorithm 2 can achieve small strategic loss in expectation as described in Lemma 1. Then we can achieve the sample complexity in Theorem 6 by boosting Algorithm 2 to a strong learner. This is accomplished by running Algorithm 2 multiple times until we obtain a good predictor. The proofs of Lemma 1 and Theorem 6 are deferred to Appendix G.

**Lemma 1**.: _Let \(S=(x_{t},r_{t},y_{t})_{t=1}^{T}\sim\mathcal{D}^{T}\) denote the i.i.d. sampled agents in \(T\) rounds and let \(\mathcal{A}(S)\) denote the output of Algorithm 2 interacting with \(S\). For any feature-ball manipulation set space \(\mathcal{Q}\) and hypothesis class \(\mathcal{H}\), when \(T\geq\frac{320\log^{2}(|\mathcal{H}|)}{\varepsilon}\), we have \(\mathbb{E}_{\mathcal{A},S}\left[\mathcal{L}^{\mathit{str}}(\mathcal{A}(S)) \right]\leq\varepsilon\)._

```
1:Initialize the version space \(\mathrm{VS}_{0}=\mathcal{H}\).
2:for\(t=1,\ldots,T\)do
3: randomly pick \(k_{t}\sim\mathrm{Unif}(\{1,2,2^{2},\ldots,2^{\lfloor\log_{2}(n_{t})-1\rfloor}\})\) where \(n_{t}=|\mathrm{VS}_{t-1}|\);
4: sample \(k_{t}\) hypotheses \(h_{1},\ldots,h_{k_{t}}\) independently and uniformly at random from \(\mathrm{VS}_{t-1}\);
5: let \(f_{t}=\bigvee_{i=1}^{k_{t}}h_{i}\).
6:if\(\widehat{y}_{t}\neq y_{t}\) and \(y_{t}=+\)then\(\mathrm{VS}_{t}=\mathrm{VS}_{t-1}\setminus\{h\in\mathrm{VS}_{t-1}|d(x_{t},h) \geq d(x_{t},f_{t})\}\);
7:elseif\(\widehat{y}_{t}\neq y_{t}\) and \(y_{t}=-\)then\(\mathrm{VS}_{t}=\mathrm{VS}_{t-1}\setminus\{h\in\mathrm{VS}_{t-1}|d(x_{t},h) \leq d(x_{t},f_{t})\}\);
8:else\(\mathrm{VS}_{t}=\mathrm{VS}_{t-1}\).
9:endfor
10: randomly pick \(\tau\) from \([T]\) and randomly sample \(h_{1},h_{2}\) from \(\mathrm{VS}_{\tau-1}\) with replacement.
11:output\(h_{1}\lor h_{2}\)
```

**Algorithm 2**

### Settings \((\bot,\Delta)\) and \((\bot,\bot)\)

Online learningAs mentioned in Section 2, both the settings of \((\bot,\Delta)\) and \((\bot,\bot)\) are harder than the setting of \((\bot,(x,\Delta))\), all lower bounds in the setting of \((\bot,(x,\Delta))\) also hold in the former two settings. Therefore, by Theorem 3, we have \(\mathrm{MB}_{\bot,\bot}\geq\mathrm{MB}_{\bot,\Delta}\geq\mathrm{MB}_{\bot,(x, \Delta)}=|\mathcal{H}|-1\).

PAC learningIn the setting of \((\bot,\Delta)\), Algorithm 2 is not applicable anymore since the learner lacks observation of \(x_{t}\), making it impossible to replicate the version space update steps in lines 6-7. It is worth noting that both PAC learning algorithms we have discussed so far fall under a general category called conservative algorithms, depend only on information from the mistake rounds. Specifically, an algorithm is said to be conservative if for any \(t\), the predictor \(f_{t}\) only depends on the history of mistake rounds up to \(t\), i.e., \(\tau<t\) with \(\widehat{y}_{\tau}\neq y_{\tau}\), and the output \(f_{\mathrm{out}}\) only depends on the history of mistake rounds, i.e., \((f_{t},\widehat{y}_{t},y_{t},\Delta_{t})_{t:\widehat{y}_{t}\neq y_{t}}\). Any algorithm that goes beyond this category would need to utilize the information in correct rounds. As mentioned earlier, in correct rounds, the predictions of all hypotheses are either correct or unknown, which makes it challenging to determine how to make updates. For conservative algorithms, we present a lower bound on the sample complexity in the following theorem, which is \(\widetilde{\Omega}(\frac{|\mathcal{H}|}{\varepsilon})\), and its proof is included in Appendix H. The optimal sample complexity in the setting \((\bot,\Delta)\) is left as an open problem.

**Theorem 7**.: _There exists a feature-ball manipulation set space \(\mathcal{Q}\) and hypothesis class \(\mathcal{H}\) s.t. \(\mathrm{SC}^{\mathrm{cur}}_{\bot,\Delta}(\varepsilon,\frac{7}{8})=\widetilde{ \Omega}(\frac{|\mathcal{H}|}{\varepsilon})\), where \(\mathrm{SC}^{\mathrm{cur}}_{\bot,\Delta}(\varepsilon,\delta)\) is \((\varepsilon,\delta)\)-PAC the sample complexity achievable by conservative algorithms._In the setting of \((\bot,\bot)\), our problem reduces to a best arm identification problem in stochastic bandits. We prove a lower bound on the sample complexity of \(\widetilde{\Omega}(\frac{|\mathcal{H}|}{\varepsilon})\) in Theorem 8 by reduction to stochastic linear bandits and applying the tools from information theory. The proof is deferred to Appendix I.

**Theorem 8**.: _There exists a feature-ball manipulation set space \(\mathcal{Q}\) and hypothesis class \(\mathcal{H}\) s.t. \(\mathrm{SC}_{\bot,\bot}(\varepsilon,\frac{7}{8})=\widetilde{\Omega}(\frac{| \mathcal{H}|}{\varepsilon})\)._

## 5 Non-ball Manipulations

In this section, we move on to non-ball manipulations. In ball manipulations, for any feature vector \(x\), we have an ordering of hypotheses according to their distances to \(x\), which helps to infer the predictions of some hypotheses without implementing them. However, in non-ball manipulations, we don't have such structure anymore. Therefore, even in the simplest setting of observing \(x_{t}\) before \(f_{t}\) and \(\Delta_{t}\), we have the PAC sample complexity lower bounded by \(\widetilde{\Omega}(\frac{|\mathcal{H}|}{\varepsilon})\).

**Theorem 9**.: _There exists a feature-manipulation set space \(\mathcal{Q}\) and hypothesis class \(\mathcal{H}\) s.t. \(\mathrm{SC}_{x,\Delta}(\varepsilon,\frac{7}{8})=\widetilde{\Omega}(\frac{| \mathcal{H}|}{\varepsilon})\)._

The proof is deferred to Appendix J. It is worth noting that in the construction of the proof, we let all agents to have their original feature vector \(x_{t}=\mathbf{0}\) such that \(x_{t}\) does not provide any information. Since \((x,\Delta)\) is the simplest setting and any mistake bound can be converted to a PAC bound via standard techniques (see Section A.2 for more details), we have the following corollary.

**Corollary 1**.: _There exists a feature-manipulation set space \(\mathcal{Q}\) and hypothesis class \(\mathcal{H}\) s.t. for all choices of \((C,F)\), \(\mathrm{SC}_{C,F}(\varepsilon,\frac{7}{8})=\widetilde{\Omega}(\frac{|\mathcal{ H}|}{\varepsilon})\) and \(\mathrm{MB}_{C,F}=\widetilde{\Omega}(|\mathcal{H}|)\)._

## 6 Discussion and Open Problems

In this work, we investigate the mistake bound and sample complexity of strategic classification across multiple settings. Unlike prior work, we assume that the manipulation is personalized and unknown to the learner, which makes the strategic classification problem more challenging. In the case of ball manipulations, when the original feature vector \(x_{t}\) is revealed prior to choosing \(f_{t}\), the problem exhibits a similar level of difficulty as the non-strategic setting (see Table 1 for details). However, when the original feature vector \(x_{t}\) is not revealed beforehand, the problem becomes significantly more challenging. Specifically, any learner will experience a mistake bound that scales linearly with \(|\mathcal{H}|\), and any proper learner will face sample complexity that also scales linearly with \(|\mathcal{H}|\). In the case of non-ball manipulations, the situation worsens. Even in the simplest setting, where the original feature is observed before choosing \(f_{t}\) and the manipulated feature is observed afterward, any learner will encounter a linear mistake bound and sample complexity.

Besides the question of optimal sample complexity in the setting of \((\bot,\Delta)\) as mentioned in Sec 4.3, there are some other fundamental open questions.

Combinatorial measureThroughout this work, our main focus is on analyzing the dependency on the size of the hypothesis class \(|\mathcal{H}|\) without assuming any specific structure of \(\mathcal{H}\). Just as VC dimension provides tight characterization for PAC learnability and Littlestone dimension characterizes online learnability, we are curious if there exists a combinatorial measure that captures the essence of strategic classification in this context. In the proofs of the most lower bounds in this work, we consider hypothesis class to be singletons, in which both the VC dimension and Littlestone dimension are \(1\). Therefore, they cannot be candidates to characterize learnability in the strategic setting.

Agnostic settingWe primarily concentrate on the realizable setting in this work. However, investigating the sample complexity and regret bounds in the agnostic setting would be an interesting avenue for future research.

## Acknowledgements

This work was supported in part by the National Science Foundation under grant CCF-2212968, by the Simons Foundation under the Simons Collaboration on the Theory of Algorithmic Fairness, by the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003. The views expressed in this work do not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. Approved for public release; distribution is unlimited.

## References

* Ahmadi et al. (2021) Ahmadi, S., Beyhaghi, H., Blum, A., and Naggita, K. (2021). The strategic perceptron. In _Proceedings of the 22nd ACM Conference on Economics and Computation_, pages 6-25.
* Ahmadi et al. (2022) Ahmadi, S., Beyhaghi, H., Blum, A., and Naggita, K. (2022). On classification of strategic agents who can both game and improve. _arXiv preprint arXiv:2203.00124_.
* Ahmadi et al. (2023) Ahmadi, S., Blum, A., and Yang, K. (2023). Fundamental bounds on online strategic classification. _arXiv preprint arXiv:2302.12355_.
* Bechavod et al. (2021) Bechavod, Y., Ligett, K., Wu, S., and Ziani, J. (2021). Gaming helps! learning from strategic interactions in natural dynamics. In _International Conference on Artificial Intelligence and Statistics_, pages 1234-1242. PMLR.
* Bruckner and Scheffer (2011) Bruckner, M. and Scheffer, T. (2011). Stackelberg games for adversarial prediction problems. In _Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 547-555.
* Chen et al. (2020) Chen, Y., Liu, Y., and Podimata, C. (2020). Learning strategy-aware linear classifiers. _Advances in Neural Information Processing Systems_, 33:15265-15276.
* Dalvi et al. (2004) Dalvi, N., Domingos, P., Sanghai, S., and Verma, D. (2004). Adversarial classification. In _Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 99-108.
* Dong et al. (2018) Dong, J., Roth, A., Schutzman, Z., Waggoner, B., and Wu, Z. S. (2018). Strategic classification from revealed preferences. In _Proceedings of the 2018 ACM Conference on Economics and Computation_, pages 55-70.
* Gallant (1986) Gallant, S. I. (1986). Optimal linear discriminants. _Eighth International Conference on Pattern Recognition_, pages 849-852.
* Haghtalab et al. (2020) Haghtalab, N., Immorlica, N., Lucier, B., and Wang, J. Z. (2020). Maximizing welfare with incentive-aware evaluation mechanisms. _arXiv preprint arXiv:2011.01956_.
* Haghtalab et al. (2022) Haghtalab, N., Lykouris, T., Nietert, S., and Wei, A. (2022). Learning in stackelberg games with non-myopic agents. In _Proceedings of the 23rd ACM Conference on Economics and Computation_, pages 917-918.
* Hardt et al. (2016) Hardt, M., Megiddo, N., Papadimitriou, C., and Wootters, M. (2016). Strategic classification. In _Proceedings of the 2016 ACM conference on innovations in theoretical computer science_, pages 111-122.
* Hu et al. (2019) Hu, L., Immorlica, N., and Vaughan, J. W. (2019). The disparate effects of strategic manipulation. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_, pages 259-268.
* Jagadeesan et al. (2021) Jagadeesan, M., Mendler-Dunner, C., and Hardt, M. (2021). Alternative microfoundations for strategic classification. In _International Conference on Machine Learning_, pages 4687-4697. PMLR.
* Kleinberg and Raghavan (2020) Kleinberg, J. and Raghavan, M. (2020). How do classifiers induce agents to invest effort strategically? _ACM Transactions on Economics and Computation (TEAC)_, 8(4):1-23.
* Lechner and Urner (2022) Lechner, T. and Urner, R. (2022). Learning losses for strategic classification. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 7337-7344.
* Lechner et al. (2023) Lechner, T., Urner, R., and Ben-David, S. (2023). Strategic classification with unknown user manipulations.
* Lechner et al. (2020)Milli, S., Miller, J., Dragan, A. D., and Hardt, M. (2019). The social cost of strategic classification. In _Proceedings of the Conference on Fairness, Accountability, and Transparency_, pages 230-239.
* Montasser et al. (2019) Montasser, O., Hanneke, S., and Srebro, N. (2019). Vc classes are adversarially robustly learnable, but only improperly. In _Conference on Learning Theory_, pages 2512-2530. PMLR.
* Rajaraman et al. (2023) Rajaraman, N., Han, Y., Jiao, J., and Ramchandran, K. (2023). Beyond ucb: Statistical complexity and optimal algorithms for non-linear ridge bandits. _arXiv preprint arXiv:2302.06025_.
* Sundaram et al. (2021) Sundaram, R., Vullikanti, A., Xu, H., and Yao, F. (2021). Pac-learning for strategic classification. In _International Conference on Machine Learning_, pages 9978-9988. PMLR.
* Zhang and Conitzer (2021) Zhang, H. and Conitzer, V. (2021). Incentive-aware pac learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 5797-5804.
* Zrnic et al. (2021) Zrnic, T., Mazumdar, E., Sastry, S., and Jordan, M. (2021). Who leads and who follows in strategic classification? _Advances in Neural Information Processing Systems_, 34:15257-15269.

Technical Lemmas

### Boosting expected guarantee to high probability guarantee

Consider any (possibly randomized) PAC learning algorithm \(\mathcal{A}\) in strategic setting, which can output a predictor \(\mathcal{A}(S)\) after \(T\) steps of interaction with i.i.d. agents \(S\sim\mathcal{D}^{T}\) s.t. \(\mathbb{E}\left[\mathcal{L}^{\text{str}}(\mathcal{A}(S))\right]\leq\varepsilon\), where the expectation is taken over both the randomness of \(S\) and the randomness of algorithm. One standard way in classic PAC learning of boosting the expected loss guarantee to high probability loss guarantee is: running \(\mathcal{A}\) on new data \(S\) and verifying the loss of \(\mathcal{A}(S)\) on a validation data set; if the validation loss is low, outputting the current \(\mathcal{A}(S)\), and repeating this process otherwise.

We will adopt this method to boost the confidence as well. The only difference in our strategic setting is that we can not re-use validation data set as we are only allowed to interact with the data through the interaction protocol. Our boosting scheme is described in the following.

* For round \(r=1,\ldots,R\),
* Run \(\mathcal{A}\) for \(T\) steps of interactions to obtain a predictor \(h_{r}\).
* Apply \(h_{r}\) for the following \(m_{0}\) rounds to obtain the empirical strategic loss on \(m_{0}\), denoted as \(\widehat{l}_{r}=\frac{1}{m_{0}}\sum_{t=t_{r}+1}^{t_{r}+m_{0}}\ell^{\text{str} }(h_{r},(x_{t},r_{t},y_{t}))\), where \(t_{r}+1\) is the starting time of these \(m_{0}\) rounds.
* Break and output \(h_{r}\) if \(\widehat{l}_{r}\leq 4\varepsilon\).
* If for all \(r\in[R]\), \(\widehat{l}_{r}>4\varepsilon\), output an arbitrary hypothesis.

**Lemma 2**.: _Given an algorithm \(\mathcal{A}\), which can output a predictor \(\mathcal{A}(S)\) after \(T\) steps of interaction with i.i.d. agents \(S\sim\mathcal{D}^{T}\) s.t. the expected loss satisfies \(\mathbb{E}\left[\mathcal{L}^{\text{str}}(\mathcal{A}(S))\right]\leq\varepsilon\). Let \(h_{\mathcal{A}}\) denote the output of the above boosting scheme given algorithm \(\mathcal{A}\) as input. By setting \(R=\log\frac{2}{4}\) and \(m_{0}=\frac{3\log(4R/\delta)}{2e}\), we have \(\mathcal{L}^{\text{str}}(h_{\mathcal{A}})\leq 8\varepsilon\) with probability \(1-\delta\). The total sample size is \(R(T+m_{0})=\mathcal{O}(\log(\frac{1}{\delta})(T+\frac{\log(1/\delta)}{ \varepsilon}))\)._

Proof.: For all \(r=1,\ldots,R\), we have \(\mathbb{E}\left[\mathcal{L}^{\text{str}}(h_{r})\right]\leq\varepsilon\). By Markov's inequality, we have

\[\Pr(\mathcal{L}^{\text{str}}(h_{r})>2\varepsilon)\leq\frac{1}{2}\,.\]

For any fixed \(h_{r}\), if \(\mathcal{L}^{\text{str}}(h_{r})\geq 8\varepsilon\), we will have \(\widehat{l}_{r}\leq 4\varepsilon\) with probability \(\leq e^{-m_{0}\varepsilon}\); if \(\mathcal{L}^{\text{str}}(h_{r})\leq 2\varepsilon\), we will have \(\widehat{l}_{r}\leq 4\varepsilon\) with probability \(\geq 1-e^{-2m_{0}\varepsilon/3}\) by Chernoff bound.

Let \(E\) denote the event of \(\{\exists r\in[R],\mathcal{L}^{\text{str}}(h_{r})\leq 2\varepsilon\}\) and \(F\) denote the event of \(\{\widehat{l}_{r}>4\varepsilon\) for all \(r\in[R]\}\). When \(F\) does not hold, our boosting will output \(h_{r}\) for some \(r\in[R]\).

\[\Pr(\mathcal{L}^{\text{str}}(h_{\mathcal{A}})>8\varepsilon)\] \[\leq \Pr(E,\neg F)\Pr(\mathcal{L}^{\text{str}}(h_{\mathcal{A}})>8 \varepsilon|E,\neg F)+\Pr(E,F)+\Pr(\neg E)\] \[\leq \sum_{r=1}^{R}\Pr(h_{\mathcal{A}}=h_{r},\mathcal{L}^{\text{str} }(h_{r})>8\varepsilon|E,\neg F)+\Pr(E,F)+\Pr(\neg E)\] \[\leq Re^{-m_{0}\varepsilon}+e^{-2m_{0}\varepsilon/3}+\frac{1}{2^{R}}\] \[\leq \delta\,,\]

by setting \(R=\log\frac{2}{\delta}\) and \(m_{0}=\frac{3\log(4R/\delta)}{2\varepsilon}\). 

### Converting mistake bound to PAC bound

In any setting of \((C,F)\), if there is an algorithm \(\mathcal{A}\) that can achieve the mistake bound of \(B\), then we can convert \(\mathcal{A}\) to a conservative algorithm by not updating at correct rounds. The new algorithm can still achieve mistake bound of \(B\) as \(\mathcal{A}\) still sees a legal sequence of examples. Given any conservative online algorithm, we can convert it to a PAC learning algorithm using the standard longest survivor technique (Gallant, 1986).

**Lemma 3**.: _In any setting of \((C,F)\), given any conservative algorithm \(\mathcal{A}\) with mistake bound \(B\), let algorithm \(\mathcal{A}^{\prime}\) run \(\mathcal{A}\) and output the first \(f_{t}\) which survives over \(\frac{1}{\varepsilon}\log(\frac{B}{\delta})\) examples. \(\mathcal{A}^{\prime}\) can achieve sample complexity of \(\mathcal{O}(\frac{B}{\varepsilon}\log(\frac{B}{\delta}))\)._

Proof of Lemma 3.: When the sample size \(m\geq\frac{B}{\varepsilon}\log(\frac{B}{\delta})\), the algorithm \(\mathcal{A}\) will produce at most \(B\) different hypotheses and there must exist one surviving for \(\frac{1}{\varepsilon}\log(\frac{B}{\delta})\) rounds since \(\mathcal{A}\) is a conservative algorithm with at most \(B\) mistakes. Let \(h_{1},\ldots,h_{B}\) denote these hypotheses and let \(t_{1},\ldots,t_{B}\) denote the time step they are produced. Then we have

\[\Pr(f_{\mathrm{out}}=h_{i}\text{ and }\mathcal{L}^{\text{str}}(h_{i})> \varepsilon)=\mathbb{E}\left[\Pr(f_{\mathrm{out}}=h_{i}\text{ and }\mathcal{L}^{\text{str}}(h_{i})> \varepsilon|t_{i},z_{1:t_{i}-1})\right]\] \[< \mathbb{E}\left[(1-\varepsilon)^{\frac{1}{\varepsilon}\log(\frac{ B}{\delta})}\right]=\frac{\delta}{B}\,.\]

By union bound, we have

\[\Pr(\mathcal{L}^{\text{str}}(f_{\mathrm{out}})>\varepsilon)\leq\sum_{i=1}^{B} \Pr_{z_{1:T}}(f_{\mathrm{out}}=h_{i}\text{ and }\mathcal{L}^{\text{str}}(h_{i})> \varepsilon)<\delta.\]

We are done. 

### Smooth the distribution

**Lemma 4**.: _For any two data distribution \(\mathcal{D}_{1}\) and \(\mathcal{D}_{2}\), let \(\mathcal{D}_{3}=(1-p)\mathcal{D}_{1}+p\mathcal{D}_{2}\) be the mixture of them. For any setting of \((C,F)\) and any algorithm, let \(\mathbf{P}_{\mathcal{D}}\) be the dynamics of \((C(x_{1}),f_{1},y_{1},F(x_{1},\Delta_{1}),\ldots,C(x_{T}),f_{T},y_{T},\widehat{ y}_{T},F(x_{T},\Delta_{T}))\) under the data distribution \(\mathcal{D}\). Then for any event \(A\), we have \(|\mathbf{P}_{\mathcal{D}_{3}}(A)-\mathbf{P}_{\mathcal{D}_{1}}(A)|\leq 2pT\)._

Proof.: Let \(B\) denote the event of all \((x_{t},u_{t},y_{t})_{t=1}^{T}\) being sampled from \(\mathcal{D}_{1}\). Then \(\mathbf{P}_{\mathcal{D}_{3}}(\neg B)\leq pT\). Then

\[\mathbf{P}_{\mathcal{D}_{3}}(A) =\mathbf{P}_{\mathcal{D}_{3}}(A|B)\mathbf{P}_{\mathcal{D}_{3}}(B) +\mathbf{P}_{\mathcal{D}_{3}}(A|\neg B)\mathbf{P}_{\mathcal{D}_{3}}(\neg B)\] \[=\mathbf{P}_{\mathcal{D}_{1}}(A)\mathbf{P}_{\mathcal{D}_{3}}(B) +\mathbf{P}_{\mathcal{D}_{3}}(A|\neg B)\mathbf{P}_{\mathcal{D}_{3}}(\neg B)\] \[=\mathbf{P}_{\mathcal{D}_{1}}(A)(1-\mathbf{P}_{\mathcal{D}_{3}}( \neg B))+\mathbf{P}_{\mathcal{D}_{3}}(A|\neg B)\mathbf{P}_{\mathcal{D}_{3}}( \neg B)\,.\]

By re-arranging terms, we have

\[|\mathbf{P}_{\mathcal{D}_{1}}(A)-\mathbf{P}_{\mathcal{D}_{3}}(A)|=|\mathbf{P} _{\mathcal{D}_{1}}(A)\mathbf{P}_{\mathcal{D}_{3}}(\neg B)-\mathbf{P}_{ \mathcal{D}_{3}}(A|\neg B)\mathbf{P}_{\mathcal{D}_{3}}(\neg B)|\leq 2pT\,.\]

## Appendix B Proof of Theorem 1

Proof.: When a mistake occurs, there are two cases.

* If \(f_{t}\) misclassifies a true positive example \((x_{t},r_{t},+1)\) by negative, we know that \(d(x_{t},f_{t})>r_{t}\) while the target hypothesis \(h^{*}\) must satisfy that \(d(x_{t},h^{*})\leq r_{t}\). Then any \(h\in\mathrm{VS}\) with \(d(x_{t},h)\geq d(x_{t},f_{t})\) cannot be \(h^{*}\) and are eliminated. Since \(d(x_{t},f_{t})\) is the median of \(\{d(x_{t},h)|h\in\mathrm{VS}\}\), we can eliminate half of the version space.
* If \(f_{t}\) misclassifies a true negative example \((x_{t},r_{t},-1)\) by positive, we know that \(d(x_{t},f_{t})\leq r_{t}\) while the target hypothesis \(h^{*}\) must satisfy that \(d(x_{t},h^{*})>r_{t}\). Then any \(h\in\mathrm{VS}\) with \(d(x_{t},h)\leq d(x_{t},f_{t})\) cannot be \(h^{*}\) and are eliminated. Since \(d(x_{t},f_{t})\) is the median of \(\{d(x_{t},h)|h\in\mathrm{VS}\}\), we can eliminate half of the version space.

Each mistake reduces the version space by half and thus, the algorithm of Strategic Halving suffers at most \(\log_{2}(|\mathcal{H}|)\) mistakes.

Proof of Theorem 2

Proof.: In online learning setting, an algorithm is conservative if it updates it's current predictor only when making a mistake. It is straightforward to check that Strategic Halving is conservative. Combined with the technique of converting mistake bound to PAC bound in Lemma 3, we prove Theorem 2. 

## Appendix D Proof of Theorem 3

Proof.: Consider the feature space \(\mathcal{X}=\{\mathbf{0},\mathbf{e}_{1},\ldots,\mathbf{e}_{n},0.9\mathbf{e}_{1}, \ldots,0.9\mathbf{e}_{n}\}\), where \(\mathbf{e}_{i}\)'s are standard basis vectors in \(\mathbb{R}^{n}\) and metric \(d(x,x^{\prime})=\left\|x-x^{\prime}\right\|_{2}\) for all \(x,x^{\prime}\in\mathcal{X}\). Let the hypothesis class be a set of singletons over \(\{\mathbf{e}_{i}|i\in[n]\}\), i.e., \(\mathcal{H}=\left\{2\mathds{1}_{\{\mathbf{e}_{i}\}}-1|i\in[n]\right\}\). We divide all possible hypotheses (not necessarily in \(\mathcal{H}\)) into three categories:

* The hypothesis \(2\mathds{1}_{\emptyset}-1\), which predicts all negative.
* For each \(x\in\{\mathbf{0},0.9\mathbf{e}_{1},\ldots,0.9\mathbf{e}_{n}\}\), let \(F_{x,+}\) denote the class of hypotheses \(h\) predicting \(x\) as positive.
* For each \(i\in[n]\), let \(F_{i}\) denote the class of hypotheses \(h\) satisfying \(h(x)=-1\) for all \(x\in\{\mathbf{0},0.9\mathbf{e}_{1},\ldots,0.9\mathbf{e}_{n}\}\) and \(h(\mathbf{e}_{i})=+1\). And let \(F_{*}=\cup_{i\in[n]}F_{i}\) denote the union of them.

Note that all hypotheses over \(\mathcal{X}\) fall into one of the three categories.

Now we consider a set of adversaries \(E_{1},\ldots,E_{n}\), such that the target function in the adversarial environment \(E_{i}\) is \(2\mathds{1}_{\{\mathbf{e}_{i}\}}-1\). We allow the learners to be randomized and thus, at round \(t\), the learner draws an \(f_{t}\) from a distribution \(D(f_{t})\) over hypotheses. The adversary, who only knows the distribution \(D(f_{t})\) but not the realization \(f_{t}\), picks an agent \((x_{t},r_{t},y_{t})\) in the following way.

* Case 1: If there exists \(x\in\{\mathbf{0},0.9\mathbf{e}_{1},\ldots,0.9\mathbf{e}_{n}\}\) such that \(\Pr_{f_{t}\sim D(f_{t})}(f_{t}\in F_{x,+})\geq c\) for some \(c>0\), then for all \(j\in[n]\), the adversary \(E_{j}\) picks \((x_{t},r_{t},y_{t})=(x,0,-1)\). Let \(B^{t}_{1,x}\) denote the event of \(f_{t}\in F_{x,+}\).
* In this case, the learner will make a mistake with probability \(c\). Since for all \(h\in\mathcal{H}\), \(h(\Delta(x,h,0))=h(x)=-1\), they are all consistent with \((x,0,-1)\).
* Case 2: If \(\Pr_{f_{t}\sim D(f_{t})}(f_{t}=2\mathds{1}_{\emptyset}-1)\geq c\), then for all \(j\in[n]\), the adversary \(E_{j}\) picks \((x_{t},r_{t},y_{t})=(\mathbf{0},1,+1)\). Let \(B^{t}_{2}\) denote the event of \(f_{t}=2\mathds{1}_{\emptyset}-1\).
* In this case, with probability \(c\), the learner will sample a \(f_{t}=2\mathds{1}_{\emptyset}-1\) and misclassify \((\mathbf{0},1,+1)\). Since for all \(h\in\mathcal{H}\), \(h(\Delta(\mathbf{0},h,1))=+1\), they are all consistent with \((\mathbf{0},1,+1)\).
* Case 3: If the above two cases do not hold, let \(i_{t}=\arg\max_{i\in[n]}\Pr(f_{t}(\mathbf{e}_{i})=1|f_{t}\in F_{*})\), \(x_{t}=0.9\mathbf{e}_{i_{t}}\). For radius and label, different adversaries set them differently. Adversary \(E_{i_{t}}\) will set \((r_{t},y_{t})=(0,-1)\) while other \(E_{j}\) for \(j\neq i_{t}\) will set \((r_{t},y_{t})=(0.1,-1)\). Since Cases 1 and 2 do not hold, we have \(\Pr_{f_{t}\sim D(f_{t})}(f_{t}\in F_{*})\geq 1-(n+2)c\). Let \(B^{t}_{3}\) denote the event of \(f_{t}\in F_{*}\) and \(B^{t}_{3,i}\) denote the event of \(f_{t}\in F_{i}\).
* With probability \(\Pr(B^{t}_{3,i_{t}})\geq\frac{1}{n}\Pr(B^{t}_{3})\geq\frac{1-(n+2)c}{n}\), the learner samples a \(f_{t}\in F_{i_{t}}\), and thus misclassifies \((0.9\mathbf{e}_{i_{t}},0.1,-1)\) in \(E_{j}\) for \(j\neq i_{t}\) but correctly classifies \((0.9\mathbf{e}_{i_{t}},0,-1)\). In this case, the learner observes the same feedback in all \(E_{j}\) for \(j\neq i_{t}\) and identifies the target function \(2\mathds{1}_{\{\mathbf{e}_{i_{t}}\}}-1\) in \(E_{i_{t}}\).
* If the learner samples a \(f_{t}\) with \(f_{t}(\mathbf{e}_{i_{t}})=f_{t}(0.9\mathbf{e}_{i_{t}})=-1\), then the learner observes \(x_{t}=0.9\mathbf{e}_{i_{t}}\), \(y_{t}=-1\) and \(\widehat{y}_{t}=-1\) in all \(E_{j}\) for \(j\in[n]\). Therefore the learner cannot distinguish between adversaries in this case.
* If the learner samples a \(f_{t}\) with \(f_{t}(0.9\mathbf{e}_{i_{t}})=+1\), then the learner observes \(x_{t}=0.9\mathbf{e}_{i_{t}}\), \(y_{t}=-1\) and \(\widehat{y}_{t}=+1\) in all \(E_{j}\) for \(j\in[n]\). Again, since the feedback are identical in all \(E_{j}\) and the learner cannot distinguish between adversaries in this case.
For any learning algorithm \(\mathcal{A}\), his predictions are identical in all of adversarial environments \(\{E_{j}|j\in[n]\}\) before he makes a mistake in Case 3(a) in one environment \(E_{i_{t}}\). His predictions in the following rounds are identical in all of adversarial environments \(\{E_{j}|j\in[n]\}\setminus\{E_{i_{t}}\}\) before he makes another mistake in Case 3(a). Suppose that we run \(\mathcal{A}\) in all adversarial environment of \(\{E_{j}|j\in[n]\}\) simultaneously. Note that once we make a mistake, the mistake must occur simultaneously in at least \(n-1\) environments. Specifically, if we make a mistake in Case 1, 2 or 3(c), such a mistake simultaneously occur in all \(n\) environments. If we make a mistake in Case 3(a), such a mistake simultaneously occur in all \(n\) environments except \(E_{i_{t}}\). Since we will make a mistake with probability at least \(\min(c,\frac{1-(n+2)c}{n})\) at each round, there exists one environment in \(\{E_{j}|j\in[n]\}\) in which \(\mathcal{A}\) will make \(n-1\) mistakes.

Now we lower bound the number of mistakes dependent on \(T\). Let \(t_{1},t_{2},\ldots\) denote the time steps in which we makes a mistake. Let \(t_{0}=0\) for convenience. Now we prove that

\[\Pr(t_{i}>t_{i-1}+k|t_{i-1})=\prod_{\tau=t_{i-1}+1}^{t_{i-1}+k} \Pr(\text{we don't make a mistake in round $\tau$})\] \[\leq \prod_{\tau=t_{i-1}+1}^{t_{i-1}+k}\left(\mathds{1}(\text{Case 3 at round $\tau$})(1-\frac{1-(n+2)c}{n})+\mathds{1}(\text{Case 1 or 2 at round $\tau$})(1-c)\right)\] \[\leq (1-\min(\frac{1-(n+2)c}{n},c))^{k}\leq(1-\frac{1}{2(n+2)})^{k}\,,\]

by setting \(c=\frac{1}{2(n+2)}\). Then by letting \(k=2(n+2)\ln(n/\delta)\), we have

\[\Pr(t_{i}>t_{i-1}+k|t_{i-1})\leq\delta/n\,.\]

For any \(T\),

\[\Pr(\text{\# of mistakes $<$\min(\frac{T}{k+1},n-1)$})\] \[= \leq \Pr(\exists i\in[n-1],t_{i}-t_{i-1}>k)\] \[\leq \sum_{i=1}^{n-1}\Pr(t_{i}-t_{i-1}>k)\leq\delta\,.\]

Therefore, we have proved that for any \(T\), with probability at least \(1-\delta\), we will make at least \(\min(\frac{T}{2(n+2)\ln(n/\delta)+1},n-1)\) mistakes. 

## Appendix E Proof of Theorem 4

```
1: Initialize the version space \(\mathrm{VS}=\mathcal{H}\).
2:for t=1,...,T do
3: Pick one hypotheses \(f_{t}\) from \(\mathrm{VS}\) uniformly at random.
4:if\(\hat{y}_{t}\neq y_{t}\) and \(y_{t}=+\)then
5:\(\mathrm{VS}\leftarrow\mathrm{VS}\setminus\{h\in\mathrm{VS}|d(x_{t},h)\geq d(x_{t},f_{t})\}\).
6:elseif\(\hat{y}_{t}\neq y_{t}\) and \(y_{t}=-\)then
7:\(\mathrm{VS}\leftarrow\mathrm{VS}\setminus\{h\in\mathrm{VS}|d(x_{t},h)\leq d(x_{t},f_{t})\}\).
8:endif
9:endfor
```

**Algorithm 3** MWMR (Multiplicative Weights on Mistake Rounds)

Proof.: First, when the algorithm makes a mistake at round \(t\), he can at least eliminate \(f_{t}\). Therefore, the total number of mistakes will be upper bounded by \(|\mathcal{H}|-1\).

Let \(p_{t}\) denote the fraction of hypotheses misclassifying \(x_{t}\). We say a hypothesis \(h\) is inconsistent with \((x_{t},f_{t},y_{t},\hat{y}_{t})\) iff \((d(x_{t},h)\geq d(x_{t},f_{t})\wedge\hat{y}_{t}=-\wedge y_{t}=+)\) or \((d(x_{t},h)\leq d(x_{t},f_{t})\wedge\hat{y}_{t}=+\wedge y_{t}=-)\). Then we define the following events.

* \(E_{t}\) denotes the event that MWMR makes a mistake at round \(t\). We have \(\Pr(E_{t})=p_{t}\).
* \(B_{t}\) denotes the event that at least \(\frac{p_{t}}{2}\) fraction of hypotheses are inconsistent with \((x_{t},f_{t},y_{t},\widehat{y}_{t})\). We have \(\Pr(B_{t}|E_{t})\geq\frac{1}{2}\).

Let \(n=|\mathcal{H}|\) denote the cardinality of hypothesis class and \(n_{t}\) denote the number of hypotheses in VS after round \(t\). Then we have

\[1\leq n_{T}=n\cdot\prod_{t=1}^{T}(1-\mathds{1}(E_{t})\mathds{1}(B_{t})\frac{p_ {t}}{2})\,.\]

By taking logarithm of both sides, we have

\[0\leq\ln(n_{T})=\ln(n)+\sum_{t=1}^{T}\ln(1-\mathds{1}(E_{t})\mathds{1}(B_{t}) \frac{p_{t}}{2})\leq\ln(n)-\sum_{t=1}^{T}\mathds{1}(E_{t})\mathds{1}(B_{t}) \frac{p_{t}}{2}\,,\]

where the last inequality adopts \(\ln(1-x)\leq-x\) for \(x\in[0,1)\). Then by taking expectation of both sides, we have

\[0\leq\ln(n)-\sum_{t=1}^{T}\Pr(E_{t}\wedge B_{t})\frac{p_{t}}{2}\,.\]

Since \(\Pr(E_{t})=p_{t}\) and \(\Pr(B_{t}|E_{t})\geq\frac{1}{2}\), then we have

\[\frac{1}{4}\sum_{t=1}^{T}p_{t}^{2}\leq\ln(n)\,.\]

Then we have the expected number of mistakes \(\mathbb{E}\left[\mathcal{M}_{\mathrm{MWMR}}(T)\right]\) as

\[\mathbb{E}\left[\mathcal{M}_{\mathrm{MWMR}}(T)\right]=\sum_{t=1}^{T}p_{t}\leq \sqrt{\sum_{t=1}^{T}p_{t}^{2}}\cdot\sqrt{T}\leq\sqrt{4\ln(n)T}\,,\]

where the first inequality applies Cauchy-Schwarz inequality. 

## Appendix F Proof of Theorem 5

Proof.: **Construction of \(\mathcal{Q},\mathcal{H}\) and a set of realizable distributions**

* Let feature space \(\mathcal{X}=\{\mathbf{0},\mathbf{e}_{1},\ldots,\mathbf{e}_{n}\}\cup X_{0}\), where \(X_{0}=\{\frac{\sigma(0,1,\ldots,n-1)}{z}|\sigma\in\mathcal{S}_{n}\}\) with \(z=\frac{\sqrt{1^{2}+\ldots+(n-1)^{2}}}{\alpha}\) for some small \(\alpha=0.1\). Here \(\mathcal{S}_{n}\) is the set of all permutations over \(n\) elements. So \(X_{0}\) is the set of points whose coordinates are a permutation of \(\{0,1/z,\ldots,(n-1)/z\}\) and all points in \(X_{0}\) have the \(\ell_{2}\) norm equal to \(\alpha\). Define a metric \(d\) by letting \(d(x_{1},x_{2})=\left\|x_{1}-x_{2}\right\|_{2}\) for all \(x_{1},x_{2}\in\mathcal{X}\). Then for any \(x\in X_{0}\) and \(i\in[n]\), \(d(x,\mathbf{e}_{i})=\left\|x-\mathbf{e}_{i}\right\|_{2}=\sqrt{(x_{i}-1)^{2}+ \sum_{j\neq i}x_{j}^{2}}=\sqrt{1+\sum_{j=1}^{n}x_{j}^{2}-2x_{i}}=\sqrt{1+ \alpha^{2}-2x_{i}}\). Note that we consider space \((\mathcal{X},d)\) rather than \((\mathbb{R}^{n},\left\|\cdot\right\|_{2})\).
* Let the hypothesis class be a set of singletons over \(\{\mathbf{e}_{i}|i\in[n]\}\), i.e., \(\mathcal{H}=\{2\mathds{1}_{\{\mathbf{e}_{i}\}}-1|i\in[n]\}\).
* We now define a collection of distributions \(\{\mathcal{D}_{i}|i\in[n]\}\) in which \(\mathcal{D}_{i}\) is realized by \(2\mathds{1}_{\{\mathbf{e}_{i}\}}-1\). For any \(i\in[n]\), \(\mathcal{D}_{i}\) puts probability mass \(1-3n\varepsilon\) on \((\mathbf{0},0,-1)\). For the remaining \(3n\varepsilon\) probability mass, \(\mathcal{D}_{i}\) picks \(x\) uniformly at random from \(X_{0}\) and label it as positive. If \(x_{i}=0\), set radius \(r(x)=r_{u}:=\sqrt{1+\alpha^{2}}\); otherwise, set radius \(r(x)=r_{l}:=\sqrt{1+\alpha^{2}-2\cdot\frac{1}{z}}\). Hence, \(X_{0}\) are all labeled as positive. For \(j\neq i\), \(h_{j}=2\mathds{1}_{\{\mathbf{e}_{j}\}}-1\) labels \(\{x\in X_{0}|x_{j}=0\}\) negative since \(r(x)=r_{l}\) and \(d(x,h_{j})=r_{u}>r(x)\). Therefore, \(\mathcal{L}^{\text{sr}}(h_{j})=\frac{1}{n}\cdot 3n\varepsilon=3\varepsilon\). To output \(f_{\mathrm{out}}\in\mathcal{H}\), we must identify the true target function.

**Information gain from different choices of \(f_{t}\)** Let \(h^{*}=2\mathds{1}_{\{\mathbf{e}_{i^{*}}\}}-1\) denote the target function. Since \((\mathbf{0},0,-1)\) is realized by all hypotheses, we can only gain information about the target function when \(x_{t}\in X_{0}\). For any \(x_{t}\in X_{0}\), if \(d(x_{t},f_{t})\leq r_{l}\) or \(d(x_{t},f_{t})>r_{u}\), we cannot learn anything about the target function. In particular, if \(d(x_{t},f_{t})\leq r_{l}\), the learner will observe \(x_{t}\sim\operatorname{Unif}(X_{0})\), \(y_{t}=+1\), \(\widehat{y}_{t}=+1\) in all \(\{\mathcal{D}_{i}|i\in[n]\}\). If \(d(x_{t},f_{t})>r_{u}\), the learner will observe \(x_{t}\sim\operatorname{Unif}(X_{0})\), \(y_{t}=+1\), \(\widehat{y}_{t}=-1\) in all \(\{\mathcal{D}_{i}|i\in[n]\}\). Therefore, we cannot obtain any information about the target function.

Now for any \(x_{t}\in X_{0}\), with the \(i_{t}\)-th coordinate being \(0\), we enumerate the distance between \(x\) and \(x^{\prime}\) for all \(x^{\prime}\in\mathcal{X}\).

* For all \(x^{\prime}\in X_{0}\), \(d(x,x^{\prime})\leq\|x\|+\|x^{\prime}\|\leq 2\alpha<r_{l}\);
* For all \(j\neq i_{t}\), \(d(x,\mathbf{e}_{j})=\sqrt{1+\alpha^{2}-2x_{j}}\leq r_{l}\);
* \(d(x,\mathbf{e}_{i_{t}})=r_{u}\);
* \(d(x,\mathbf{0})=\alpha<r_{l}\).

Only \(f_{t}=2\mathds{1}_{\{\mathbf{e}_{i_{t}}\}}-1\) satisfies that \(r_{l}<d(x_{t},f_{t})\leq r_{u}\) and thus, we can only obtain information when \(f_{t}=2\mathds{1}_{\{\mathbf{e}_{i_{t}}\}}-1\). And the only information we learn is whether \(i_{t}=i^{*}\) because if \(i_{t}\neq i^{*}\), no matter which \(i^{*}\) is, our observation is identical. If \(i_{t}\neq i^{*}\), we can eliminate \(2\mathds{1}_{\{\mathbf{e}_{i_{t}}\}}-1\).

**Sample size analysis** For any algorithm \(\mathcal{A}\), his predictions are identical in all environments \(\{\mathcal{D}_{i}|i\in[n]\}\) before a round \(t\) in which \(\hat{f}_{t}=2\mathds{1}_{\{\mathbf{e}_{i_{t}}\}}-1\). Then either he learns \(i_{t}\) in \(\mathcal{D}_{i_{t}}\) or he eliminates \(2\mathds{1}_{\{\mathbf{e}_{i_{t}}\}}-1\) and continues to perform the same in the other environments \(\{\mathcal{D}_{i}|i\neq i_{t}\}\). Suppose that we run \(\mathcal{A}\) in all stochastic environments \(\{\mathcal{D}_{i}|i\in[n]\}\) simultaneously. When we identify \(i_{t}\) in environment \(\mathcal{D}_{i_{t}}\), we terminate \(\mathcal{A}\) in \(\mathcal{D}_{i_{t}}\). Consider a good algorithm \(\mathcal{A}\) which can identify \(i\) in \(\mathcal{D}_{i}\) with probability \(\frac{1}{8}\) after \(T\) rounds of interaction for each \(i\in[n]\), that is,

\[\Pr_{\mathcal{D}_{i},\mathcal{A}}(i_{\mathrm{out}}\neq i)\leq\frac{1}{8}, \forall i\in[n]\,. \tag{3}\]

Therefore, we have

\[\sum_{i\in[n]}\Pr_{\mathcal{D}_{i},\mathcal{A}}(i_{\mathrm{out}} \neq i)\leq\frac{n}{8}\,. \tag{4}\]

Let \(n_{T}\) denote the number of environments that have been terminated by the end of round \(T\). Let \(B_{t}\) denote the event of \(x_{t}\) being in \(X_{0}\) and \(C_{t}\) denote the event of \(f_{t}=2\mathds{1}_{\{\mathbf{e}_{i_{t}}\}}-1\). Then we have \(\Pr(B_{t})=3n\varepsilon\) and \(\Pr(C_{t}|B_{t})=\frac{1}{n}\), and thus \(\Pr(B_{t}\wedge C_{t})=3n\varepsilon\cdot\frac{1}{n}\). Since at each round, we can eliminate one environment only when \(B_{t}\wedge C_{t}\) is true, then we have

\[\mathbb{E}\left[n_{T}\right]\leq\mathbb{E}\left[\sum_{t=1}^{T} \mathds{1}(B_{t}\wedge C_{t})\right]=T\cdot 3n\varepsilon\cdot\frac{1}{n}=3 \varepsilon T\,.\]

Therefore, by setting \(T=\frac{\left\lfloor\frac{n}{2}\right\rfloor-1}{6\varepsilon}\) and Markov's inequality, we have

\[\Pr(n_{T}\geq\left\lfloor\frac{n}{2}\right\rfloor-1)\leq\frac{3 \varepsilon T}{\left\lfloor\frac{n}{2}\right\rfloor-1}=\frac{1}{2}\,.\]

When there are \(\left\lceil\frac{n}{2}\right\rceil+1\) environments remaining, the algorithm has to pick one \(i_{\mathrm{out}}\), which fails in at least \(\left\lceil\frac{n}{2}\right\rceil\) of the environments. Then we have

\[\sum_{i\in[n]}\Pr_{\mathcal{D}_{i},\mathcal{A}}(i_{\mathrm{out}} \neq i)\geq\left\lceil\frac{n}{2}\right\rceil\Pr(n_{T}\leq\left\lfloor\frac{n }{2}\right\rfloor-1)\geq\frac{n}{4}\,,\]

which conflicts with Eq (4). Therefore, for any algorithm \(\mathcal{A}\), to achieve Eq (3), it requires \(T\geq\left\lfloor\frac{n}{2}\right\rfloor-1\).

Proof of Theorem 6

Given Lemma 1, we can upper bound the expected strategic loss, then we can boost the confidence of the algorithm through the scheme in Section A.1. Theorem 6 follows by combining Lemma 1 and Lemma 2. Now we only need to prove Lemma 1.

Proof of Lemma 1.: For any set of hypotheses \(H\), for every \(z=(x,r,y)\), we define

\[\kappa_{p}(H,z):=\begin{cases}\left|\{h\in H|h(\Delta(x,h,r))=-\}\right|&\text {if }y=+\,,\\ 0&\text{otherwise.}\end{cases}\]

So \(\kappa_{p}(H,z)\) is the number of hypotheses mislabeling \(z\) for positive \(z\)'s and \(0\) for negative \(z\)'s. Similarly, we define \(\kappa_{n}\) as follows,

\[\kappa_{n}(H,z):=\begin{cases}\left|\{h\in H|h(\Delta(x,h,r))=+\}\right|&\text {if }y=-\,,\\ 0&\text{otherwise.}\end{cases}\]

So \(\kappa_{n}(H,z)\) is the number of hypotheses mislabeling \(z\) for negative \(z\)'s and \(0\) for positive \(z\)'s.

In the following, we divide the proof into two parts. First, recall that in Algorithm 2, the output is constructed by randomly sampling two hypotheses with replacement and taking the union of them. We represent the loss of such a random predictor using \(\kappa_{p}(H,z)\) and \(\kappa_{n}(H,z)\) defined above. Then we show that whenever the algorithm makes a mistake, with some probability, we can reduce \(\frac{\kappa_{p}(\mathrm{VS}_{t-1},z_{t})}{2}\) or \(\frac{\kappa_{n}(\mathrm{VS}_{t-1},z_{t})}{2}\) hypotheses and utilize this to provide a guarantee on the loss of the final output.

Upper bounds on the strategic lossFor any hypothesis \(h\), let \(\mathrm{fpr}(h)\) and \(\mathrm{fnr}(h)\) denote the false positive rate and false negative rate of \(h\) respectively. Let \(p_{+}\) denote the probability of drawing a positive sample from \(\mathcal{D}\), i.e., \(\Pr_{(x,r,y)\sim\mathcal{D}}(y=+)\) and \(p_{-}\) denote the probability of drawing a negative sample from \(\mathcal{D}\). Let \(\mathcal{D}_{+}\) and \(\mathcal{D}_{-}\) denote the data distribution conditional on that the label is positive and that the label is negative respectively. Given any set of hypotheses \(H\), we define a random predictor \(R2(H)=h_{1}\lor h_{2}\) with \(h_{1},h_{2}\) randomly picked from \(H\) with replacement. For a true positive \(z\), \(R2(H)\) will misclassify it with probability \(\frac{\kappa_{p}(H,z)^{2}}{|H|^{2}}\). Then we can find that the false negative rate of \(R2(H)\) is

\[\mathrm{fnr}(R2(H))=\mathbb{E}_{z=(x,r,+)\sim\mathcal{D}_{+}}\left[\Pr(R2(H)(x )=-)\right]=\mathbb{E}_{z=(x,r,+)\sim\mathcal{D}_{+}}\left[\frac{\kappa_{p}(H,z)^{2}}{\left|H\right|^{2}}\right]\,.\]

Similarly, for a true negative \(z\), \(R2(H)\) will misclassify it with probability \(1-(1-\frac{\kappa_{n}(H,z)}{|H|})^{2}\leq\frac{2\kappa_{n}(H,z)}{|H|}\). Then the false positive rate of \(R2(H)\) is

\[\mathrm{fpr}(R2(H))=\mathbb{E}_{z=(x,r,-)\sim\mathcal{D}_{-}}\left[\Pr(R2(H)(x )=+)\right]\leq\mathbb{E}_{z=(x,r,-)\sim\mathcal{D}_{+}}\left[\frac{2\kappa_{ n}(H,z)}{\left|H\right|}\right]\,.\]

Hence the loss of \(R2(H)\) is

\[\mathcal{L}^{\text{sr}}(R2(H)) \leq p_{+}\mathbb{E}_{z\sim\mathcal{D}_{+}}\left[\frac{\kappa_{p} (H,z)^{2}}{\left|H\right|^{2}}\right]+p_{-}\mathbb{E}_{z\sim\mathcal{D}_{+}} \left[\frac{2\kappa_{n}(H,z)}{\left|H\right|}\right]\] \[=\mathbb{E}_{z\sim\mathcal{D}}\left[\frac{\kappa_{p}(H,z)^{2}}{ \left|H\right|^{2}}+2\frac{\kappa_{n}(H,z)}{\left|H\right|}\right]\,, \tag{5}\]

where the last equality holds since \(\kappa_{p}(H,z)=0\) for true negatives and \(\kappa_{n}(H,z)=0\) for true positives.

Loss analysisIn each round, the data \(z_{t}=(x_{t},r_{t},y_{t})\) is sampled from \(\mathcal{D}\). When the label \(y_{t}\) is positive, if the drawn \(f_{t}\) satisfying that 1) \(f_{t}(\Delta(x_{t},f_{t},r_{t}))=-\) and 2) \(d(x_{t},f_{t})\leq\mathrm{median}(\{d(x_{t},h)|h\in\mathrm{VS}_{t-1},h(\Delta(x _{t},h,r_{t}))=-\})\), then we are able to remove \(\frac{\kappa_{p}(\mathrm{VS}_{t-1},z_{t})}{2}\) hypotheses from the version space. Let \(E_{p,t}\) denote the event of \(f_{t}\) satisfying the conditions 1) and 2). With probability \(\frac{1}{[\log_{2}(n_{t})]}\), we sample \(k_{t}=1\). Then we sample an \(f_{t}\sim\operatorname{Unif}(\operatorname{VS}_{t-1})\). With probability \(\frac{\kappa_{p}(\operatorname{VS}_{t-1},z_{t})}{2n_{t}}\), the sampled \(f_{t}\) satisfies the two conditions. So we have

\[\Pr(E_{p,t}|z_{t},\operatorname{VS}_{t-1})\geq\frac{1}{\log_{2}(n_{t})}\frac{ \kappa_{p}(\operatorname{VS}_{t-1},z_{t})}{2n_{t}}\,. \tag{6}\]

The case of \(y_{t}\) being negative is similar to the positive case. Let \(E_{n,t}\) denote the event of \(f_{t}\) satisfying that 1) \(f_{t}(\Delta(x_{t},f_{t},r_{t}))=+\) and 2) \(d(x_{t},f_{t})\geq\operatorname{median}(\{d(x_{t},h)|h\in\operatorname{VS}_{t- 1},h(\Delta(x_{t},h,r_{t}))=+\})\). If \(\kappa_{n}(\operatorname{VS}_{t-1},z_{t})\geq\frac{n_{t}}{2}\), then with probability \(\frac{1}{[\log_{2}(n_{t})]}\), we sample \(k_{t}=1\). Then with probability greater than \(\frac{1}{4}\) we will sample an \(f_{t}\) satisfying that 1) \(f_{t}(\Delta(x_{t},f_{t},r_{t}))=+\) and 2) \(d(x_{t},f_{t})\geq\operatorname{median}(\{d(x_{t},h)|h\in\operatorname{VS}_{t- 1},h(\Delta(x_{t},h,r_{t}))=+\})\). If \(\kappa_{n}(\operatorname{VS}_{t-1},z_{t})<\frac{n_{t}}{2}\), then with probability \(\frac{1}{[\log_{2}(n_{t})]}\), we sampled a \(k_{t}\) satisfying

\[\frac{n_{t}}{4\kappa_{n}(\operatorname{VS}_{t-1},z_{t})}<k_{t}\leq\frac{n_{t}} {2\kappa_{n}(\operatorname{VS}_{t-1},z_{t})}\,.\]

Then we randomly sample \(k_{t}\) hypotheses and the expected number of sampled hypotheses which mislabel \(z_{t}\) is \(k_{t}\cdot\frac{\kappa_{n}(\operatorname{VS}_{t-1},z_{t})}{n_{t}}\in(\frac{1}{ 4},\frac{1}{2}]\). Let \(g_{t}\) (given the above fixed \(k_{t}\)) denote the number of sampled hypotheses which mislabel \(x_{t}\) and we have \(\mathbb{E}\left[g_{t}\right]\in(\frac{1}{4},\frac{1}{2}]\). When \(g_{t}>0\), \(f_{t}\) will misclassify \(z_{t}\) by positive. We have

\[\Pr(g_{t}=0)=(1-\frac{\kappa_{n}(\operatorname{VS}_{t-1},z_{t})}{n_{t}})^{k_{t }}<(1-\frac{\kappa_{n}(\operatorname{VS}_{t-1},z_{t})}{n_{t}})^{\frac{n_{t}}{ \kappa_{n}(\operatorname{VS}_{t-1},z_{t})}}\leq e^{-1/4}\leq 0.78\]

and by Markov's inequality, we have

\[\Pr(g_{t}\geq 3)\leq\frac{\mathbb{E}\left[g_{t}\right]}{3}\leq\frac{1}{6}\leq 0.17\,.\]

Thus \(\Pr(g_{t}\in\{1,2\})\geq 0.05\). Conditional on \(g_{t}\) is either \(1\) or \(2\), with probability \(\geq\frac{1}{4}\), all of these \(g_{t}\) hypotheses \(h^{\prime}\) satisfies \(d(x_{t},h^{\prime})\geq\operatorname{median}(\{d(x_{t},h)|h\in\operatorname{ VS}_{t-1},h(\Delta(x_{t},h,r_{t}))=+\})\), which implies that \(d(x_{t},f_{t})\geq\operatorname{median}(\{d(x_{t},h)|h\in\operatorname{VS}_{t- 1},h(\Delta(x_{t},h,r_{t}))=+\})\). Therefore, we have

\[\Pr(E_{n,t}|z_{t},,\operatorname{VS}_{t-1})\geq\frac{1}{80\log_{2}(n_{t})}\,. \tag{7}\]

Let \(v_{t}\) denote the fraction of hypotheses we eliminated at round \(t\), i.e., \(v_{t}=1-\frac{n_{t+1}}{n_{t}}\). Then we have

\[v_{t}\geq\mathds{1}(E_{p,t})\frac{\kappa_{p}(\operatorname{VS}_{t-1},z_{t})}{ 2n_{t}}+\mathds{1}(E_{n,t})\frac{\kappa_{n}(\operatorname{VS}_{t-1},z_{t})}{2 n_{t}}\,. \tag{8}\]

Since \(n_{t+1}=n_{t}(1-v_{t})\), we have

\[1\leq n_{T+1}=n\prod_{t=1}^{T}(1-v_{t})\,.\]

By taking logarithm of both sides, we have

\[0\leq\ln n_{T+1}=\ln n+\sum_{t=1}^{T}\ln(1-v_{t})\leq\ln n-\sum_{t=1}^{T}v_{t}\,,\]

where we use \(\ln(1-x)\leq-x\) for \(x\in[0,1)\) in the last inequality. By re-arranging terms, we have

\[\sum_{t=1}^{T}v_{t}\leq\ln n\,.\]

Combined with Eq (8), we have

\[\sum_{t=1}^{T}\mathds{1}(E_{p,t})\frac{\kappa_{p}(\operatorname{VS}_{t-1},z_{t })}{2n_{t}}+\mathds{1}(E_{n,t})\frac{\kappa_{n}(\operatorname{VS}_{t-1},z_{t}) }{2n_{t}}\leq\ln n\,.\]By taking expectation w.r.t. the randomness of \(f_{1:T}\) and dataset \(S=z_{1:T}\) on both sides, we have

\[\sum_{t=1}^{T}\mathbb{E}_{f_{1:T},z_{1:T}}\left[\mathds{1}(E_{p,t})\frac{\kappa_{ p}(\mathrm{VS}_{t-1},z_{t})}{2n_{t}}+\mathds{1}(E_{n,t})\frac{\kappa_{n}( \mathrm{VS}_{t-1},z_{t})}{2n_{t}}\right]\leq\ln n\,.\]

Since the \(t\)-th term does not depend on \(f_{t+1:T},z_{t+1:T}\) and \(\mathrm{VS}_{t-1}\) is determined by \(z_{1:t-1}\) and \(f_{1:t-1}\), the \(t\)-th term becomes

\[\mathbb{E}_{f_{1:t},z_{1:t}}\left[\mathds{1}(E_{p,t})\frac{\kappa _{p}(\mathrm{VS}_{t-1},z_{t})}{2n_{t}}+\mathds{1}(E_{n,t})\frac{\kappa_{n}( \mathrm{VS}_{t-1},z_{t})}{2n_{t}}\right]\] \[= \mathbb{E}_{f_{1:t-1},z_{1:t}}\left[\mathbb{E}_{f_{t}}\left[ \mathds{1}(E_{p,t})\frac{\kappa_{p}(\mathrm{VS}_{t-1},z_{t})}{2n_{t}}+ \mathds{1}(E_{n,t})\frac{\kappa_{n}(\mathrm{VS}_{t-1},z_{t})}{2n_{t}}|f_{1:t-1 },z_{1:t}\right]\right]\] \[= \mathbb{E}_{f_{1:t-1},z_{1:t}}\left[\mathbb{E}_{f_{t}}\left[ \mathds{1}(E_{p,t})|f_{1:t-1},z_{1:t}\right]\frac{\kappa_{p}(\mathrm{VS}_{t-1},z_{t})}{2n_{t}}+\mathbb{E}_{f_{t}}\left[\mathds{1}(E_{n,t})|f_{1:t-1},z_{1:t} \right]\frac{\kappa_{n}(\mathrm{VS}_{t-1},z_{t})}{2n_{t}}\right] \tag{9}\] \[\geq \mathbb{E}_{f_{1:t-1},z_{1:t}}\left[\frac{1}{\log_{2}(n_{t})} \frac{\kappa_{p}^{2}(\mathrm{VS}_{t-1},z_{t})}{4n_{t}^{2}}+\frac{1}{80\log_{2} (n_{t})}\frac{\kappa_{n}(\mathrm{VS}_{t-1},z_{t})}{2n_{t}}\right]\,, \tag{10}\]

where Eq (9) holds due to that \(\mathrm{VS}_{t-1}\) is determined by \(f_{1:t-1},z_{1:t-1}\) and does not depend on \(f_{t}\) and Eq (10) holds since \(\Pr_{f_{t}}(E_{p,t}|f_{1:t-1},z_{1:t})=\Pr_{f_{t}}(E_{p,t}|\mathrm{VS}_{t-1}, z_{t})\geq\frac{1}{\log_{2}(n_{t})}\frac{\kappa_{p}(\mathrm{VS}_{t-1},z_{t})}{2n_{t}}\) by Eq (6) and \(\Pr_{f_{t}}(E_{n,t}|f_{1:t-1},z_{1:t})=\Pr_{f_{t}}(E_{n,t}|\mathrm{VS}_{t-1}, z_{t})\geq\frac{1}{80\log_{2}(n_{t})}\) by Eq (7). Thus, we have

\[\sum_{t=1}^{T}\mathbb{E}_{f_{1:t-1},z_{1:t}}\left[\frac{1}{\log_{2}(n_{t})} \frac{\kappa_{p}^{2}(\mathrm{VS}_{t-1},z_{t})}{4n_{t}^{2}}+\frac{1}{80\log_{2}( n_{t})}\frac{\kappa_{n}(\mathrm{VS}_{t-1},z_{t})}{2n_{t}}\right]\leq\ln n\,.\]

Since \(z_{t}\sim\mathcal{D}\) and \(z_{t}\) is independent of \(z_{1:t-1}\) and \(f_{1:t-1}\), thus, we have the \(t\)-th term on the LHS being

\[\mathbb{E}_{f_{1:t-1},z_{1:t}}\left[\frac{1}{\log_{2}(n_{t})}\frac {\kappa_{p}^{2}(\mathrm{VS}_{t-1},z_{t})}{4n_{t}^{2}}+\frac{1}{80\log_{2}(n_{t} )}\frac{\kappa_{n}(\mathrm{VS}_{t-1},z_{t})}{2n_{t}}\right]\] \[= \mathbb{E}_{f_{1:t-1},z_{1:t-1}}\left[\mathbb{E}_{z_{t}\sim \mathcal{D}}\left[\frac{1}{\log_{2}(n_{t})}\frac{\kappa_{p}^{2}(\mathrm{VS}_{t -1},z_{t})}{4n_{t}^{2}}+\frac{1}{80\log_{2}(n_{t})}\frac{\kappa_{n}(\mathrm{VS} _{t-1},z_{t})}{2n_{t}}\right]\right]\] \[\geq \frac{1}{320\log_{2}(n)}\mathbb{E}_{f_{1:t-1},z_{1:t-1}}\left[ \mathbb{E}_{z\sim\mathcal{D}}\left[\frac{\kappa_{p}^{2}(\mathrm{VS}_{t-1},z)}{ n_{t}^{2}}+\frac{2\kappa_{n}(\mathrm{VS}_{t-1},z)}{n_{t}}\right]\right]\] \[\geq \frac{1}{320\log_{2}(n)}\mathbb{E}_{f_{1:t-1},z_{1:t-1}}\left[ \mathcal{L}^{\text{str}}(R2(\mathrm{VS}_{t-1}))\right]\,,\]

where the last inequality adopts Eq (5). By summing them up and re-arranging terms, we have

\[\mathbb{E}_{f_{1:T},z_{1:T}}\left[\frac{1}{T}\sum_{t=1}^{T}\mathcal{L}^{\text{ str}}(R2(\mathrm{VS}_{t-1}))\right]=\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}_{f_{1:t-1},z_{1:t-1}} \left[\mathcal{L}^{\text{str}}(R2(\mathrm{VS}_{t-1}))\right]\leq\frac{320\log_{2 }(n)\ln(n)}{T}\,.\]

For the output of Algorithm 2, which randomly picks \(\tau\) from \([T]\), randomly samples \(h_{1},h_{2}\) from \(\mathrm{VS}_{\tau-1}\) with replacement and outputs \(h_{1}\lor h_{2}\), the expected loss is

\[\mathbb{E}\left[\mathcal{L}^{\text{str}}(\mathcal{A}(S))\right]= \mathbb{E}_{S,f_{1:T}}\left[\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}_{h _{1},h_{2}\sim\mathrm{Unif}(\mathrm{VS}_{t-1})}\left[\mathcal{L}^{\text{str}}(h _{1}\lor h_{2})\right]\right]\] \[= \mathbb{E}_{S,f_{1:T}}\left[\frac{1}{T}\sum_{t=1}^{T}\mathcal{L}^ {\text{str}}(R2(\mathrm{VS}_{t-1}))\right]\] \[\leq \frac{320\log_{2}(n)\ln(n)}{T}\leq\varepsilon\,,\]

when \(T\geq\frac{320\log_{2}(n)\ln(n)}{\varepsilon}\). 

#### Post proof discussion of Lemma 1

* Upon first inspection, readers might perceive a resemblance between the proof of the loss analysis section and the standard proof of converting regret bound to error bound.This standard proof converts a regret guarantee on \(f_{1:T}\) to an error guarantee of \(\frac{1}{T}\sum_{t=1}^{T}f_{t}\). However, in this proof, the predictor employed in each round is \(f_{t}\), while the output is an average over \(R2(\mathrm{VS}_{t-1})\) for all \(t\in[T]\). Our algorithm does not provide a regret guarantee on \(f_{1:T}\).
* Please note that our analysis exhibits asymmetry regarding losses on true positives and true negatives. Specifically, the probability of identifying and reducing half of the misclassifying hypotheses on true positives, denoted as \(\Pr(E_{p,t}|z_{t},\mathrm{VS}_{t-1})\) (Eq (6)), is lower than the corresponding probability for true negatives, \(\Pr(E_{n,t}|z_{t},\mathrm{VS}_{t-1})\) (Eq (7)). This discrepancy arises due to the different levels of difficulty in detecting misclassifying hypotheses. For example, if there is exactly one hypothesis \(h\) misclassifying a true positive \(z_{t}=(x_{t},r_{t},y_{t})\), it is very hard to detect this \(h\). We must select an \(f_{t}\) satisfying that \(d(x_{t},f_{t})>d(x_{t},h^{\prime})\) for all \(h^{\prime}\in\mathcal{H}\setminus\{h\}\) (hence \(f_{t}\) will make a mistake), and that \(d(x_{t},f_{t})\leq d(x_{t},h)\) (so that we will know \(h\) misclassifies \(z_{t}\)). Algorithm 2 controls the distance \(d(x_{t},f_{t})\) through \(k_{t}\), which is the number of hypotheses in the union. In this case, we can only detect \(h\) when \(k_{t}=1\) and \(f_{t}=h\), which occurs with probability \(\frac{1}{n_{t}\log(n_{t})}\). However, if there is exactly one hypothesis \(h\) misclassifying a true negative \(z_{t}=(x_{t},r_{t},y_{t})\), we have that \(d(x_{t},h)=\min_{h^{\prime}\in\mathcal{H}}d(x_{t},h^{\prime})\). Then by setting \(f_{t}=\vee_{h\in\mathcal{H}}h\), which will makes a mistake and tells us \(h\) is a misclassifying hypothesis. Our algorithm will pick such an \(f_{t}\) with probability \(\frac{1}{\log(n_{t})}\).

## Appendix H Proof of Theorem 7

Proof.: We will prove Theorem 7 by constructing an instance of \(\mathcal{Q}\) and \(\mathcal{H}\) and showing that for any conservative learning algorithm, there exists a realizable data distribution s.t. achieving \(\varepsilon\) loss requires at least \(\widetilde{\Omega}(\frac{|\mathcal{H}|}{\varepsilon})\) samples.

#### Construction of \(\mathcal{Q}\), \(\mathcal{H}\) and a set of realizable distributions

* Let the input metric space \((\mathcal{X},d)\) be constructed in the following way. Consider the feature space \(\mathcal{X}=\{\mathbf{e}_{1},\ldots,\mathbf{e}_{n}\}\cup X_{0}\), where \(X_{0}=\{\frac{\sigma(0,1,\ldots,n-1)}{z}|\sigma\in\mathcal{S}_{n}\}\) with \(z=\frac{\sqrt{1^{2}+\ldots+(n-1)^{2}}}{\alpha}\) for some small \(\alpha=0.1\). Here \(\mathcal{S}_{n}\) is the set of all permutations over \(n\) elements. So \(X_{0}\) is the set of points whose coordinates are a permutation of \(\{0,1/z,\ldots,(n-1)/z\}\) and all points in \(X_{0}\) have the \(\ell_{2}\) norm equal to \(\alpha\). We define the metric \(d\) by restricting \(\ell_{2}\) distance to \(\mathcal{X}\), i.e., \(d(x_{1},x_{2})=\left\|x_{1}-x_{2}\right\|_{2}\) for all \(x_{1},x_{2}\in\mathcal{X}\). Then we have that for any \(x\in X_{0}\) and \(i\in[n]\), the distance between \(x\) and \(\mathbf{e}_{i}\) is \[d(x,\mathbf{e}_{i})=\left\|x-\mathbf{e}_{i}\right\|_{2}=\sqrt{(x_{i}-1)^{2}+ \sum_{j\neq i}x_{j}^{2}}=\sqrt{1+\sum_{j=1}^{n}x_{j}^{2}-2x_{i}}=\sqrt{1+ \alpha^{2}-2x_{i}}\,,\] which is greater than \(\sqrt{1+\alpha^{2}-2\alpha}>0.8>2\alpha\). For any two points \(x,x^{\prime}\in X_{0}\), \(d(x,x^{\prime})\leq 2\alpha\) by triangle inequality.
* Let the hypothesis class be a set of singletons over \(\{\mathbf{e}_{i}|i\in[n]\}\), i.e., \(\mathcal{H}=\{2\mathds{1}_{\{\mathbf{e}_{i}\}}-1|i\in[n]\}\).
* We now define a collection of distributions \(\{\mathcal{D}_{i}|i\in[n]\}\) in which \(\mathcal{D}_{i}\) is realized by \(2\mathds{1}_{\{\mathbf{e}_{i}\}}-1\). For any \(i\in[n]\), we define \(\mathcal{D}_{i}\) in the following way. Let the marginal distribution \(\mathcal{D}_{\mathcal{X}}\) over \(\mathcal{X}\) be uniform over \(X_{0}\). For any \(x\), the label \(y\) is \(+\) with probability \(1-6\varepsilon\) and \(-\) with probability \(6\varepsilon\), i.e., \(\mathcal{D}(y|x)=\mathrm{Rad}(1-6\varepsilon)\). Note that the marginal distribution \(\mathcal{D}_{\mathcal{X}\times\mathcal{Y}}=\mathrm{Unif}(X_{0})\times\mathrm{ Rad}(1-6\varepsilon)\) is identical for any distribution in \(\{\mathcal{D}_{i}|i\in[n]\}\) and does not depend on \(i\).

If the label is positive \(y=+\), then let the radius \(r=2\). If the label is negative \(y=-\), then let \(r=\sqrt{1+\alpha^{2}-2(x_{i}+\frac{1}{z})}\), which guarantees that \(x\) can be manipulated to \(\mathbf{e}_{j}\) iff \(d(x,\mathbf{e}_{j})<d(x,\mathbf{e}_{i})\) for all \(j\in[n]\). Since \(x_{i}\leq\alpha\) and \(\frac{1}{z}\leq\alpha\), we have \(\sqrt{1+\alpha^{2}-2(x_{i}+\frac{1}{z})}\geq\sqrt{1-4\alpha}>2\alpha\). Therefore, for both positive and negative examples, we have radius \(r\) strictly greater than \(2\alpha\) in both cases.

Randomization and improperness of the output \(f_{\mathrm{out}}\) do not helpNote that algorithms are allowed to output a randomized \(f_{\mathrm{out}}\) and to output \(f_{\mathrm{out}}\notin\mathcal{H}\). We will show that randomization and improperness of \(f_{\mathrm{out}}\) don't make the problem easier. That is, supposing that the data distribution is \(\mathcal{D}_{i^{*}}\) for some \(i^{*}\in[n]\), finding a (possibly randomized and improper) \(f_{\mathrm{out}}\) is not easier than identifying \(i^{*}\). Since our feature space \(\mathcal{X}\) is finite, we can enumerate all hypotheses not equal to \(2\mathds{1}_{\{\mathbf{e}_{i^{*}}\}}-1\) and calculate their strategic population loss as follows.

* \(2\mathds{1}_{\emptyset}-1\) predicts all negative and thus \(\mathcal{L}^{\text{str}}(2\mathds{1}_{\emptyset}-1)=1-6\varepsilon\);
* For any \(a\subset\mathcal{X}\) s.t. \(a\cap X_{0}\neq\emptyset\), \(2\mathds{1}_{a}-1\) will predict any point drawn from \(\mathcal{D}_{i^{*}}\) as positive (since all points have radius greater than \(2\alpha\) and the distance between any two points in \(X_{0}\) is smaller than \(2\alpha\)) and thus \(\mathcal{L}^{\text{str}}(2\mathds{1}_{a}-1)=6\varepsilon\);
* For any \(a\subset\{\mathbf{e}_{1},\ldots,\mathbf{e}_{n}\}\) satisfying that \(\exists i\neq i^{*},\mathbf{e}_{i}\in a\), we have \(\mathcal{L}^{\text{str}}(2\mathds{1}_{a}-1)\geq 3\varepsilon\). This is due to that when \(y=-\), \(x\) is chosen from \(\operatorname{Unif}(X_{0})\) and the probability of \(d(x,\mathbf{e}_{i})<d(x,\mathbf{e}_{i^{*}})\) is \(\frac{1}{2}\). When \(d(x,\mathbf{e}_{i})<d(x,\mathbf{e}_{i^{*}})\), \(2\mathds{1}_{a}-1\) will predict \(x\) as positive.

Under distribution \(\mathcal{D}_{i^{*}}\), if we are able to find a (possibly randomized) \(f_{\mathrm{out}}\) with strategic loss of \(\mathcal{L}^{\text{str}}(f_{\mathrm{out}})\leq\varepsilon\), then we have \(\mathcal{L}^{\text{str}}(f_{\mathrm{out}})=\mathbb{E}_{h\sim f_{\mathrm{out}}} \left[\mathcal{L}^{\text{str}}(h)\right]\geq\Pr_{h\sim f_{\mathrm{out}}}(h \neq 2\mathds{1}_{\{\mathbf{e}_{i^{*}}\}}-1)\cdot 3\varepsilon\). Thus, \(\Pr_{h\sim f_{\mathrm{out}}}(h=2\mathds{1}_{\{\mathbf{e}_{i^{*}}\}}-1)\geq \frac{2}{3}\). Hence, if we are able to find a (possibly randomized) \(f_{\mathrm{out}}\) with \(\varepsilon\) error, then we are able to identify \(i^{*}\) by checking which realization of \(f_{\mathrm{out}}\) has probability greater than \(\frac{2}{3}\). In the following, we will focus on the sample complexity to identify \(i^{*}\). Let \(i_{\mathrm{out}}\) denote the algorithm's answer to question "what is \(i^{*}\)?".

Conservative algorithmsWhen running a conservative algorithm, the rule of choosing \(f_{t}\) at round \(t\) and choosing the final output \(f_{\mathrm{out}}\) does not depend on the correct rounds, i.e. \(\{\tau\in[T]|\widehat{y}_{\tau}=y_{\tau}\}\). Let's define

\[\Delta^{\prime}_{t}=\begin{cases}\Delta_{t}&\text{if }\widehat{y}_{t}\neq y_{t}\\ \bot&\text{if }\widehat{y}_{t}=y_{t}\,,\end{cases} \tag{11}\]

where \(\bot\) is just a symbol representing "no information". Then for any conservative algorithm, the selected predictor \(f_{t}\) is determined by \((f_{\tau},\widehat{y}_{\tau},y_{\tau},\Delta^{\prime}_{\tau})\) for \(\tau<t\) and the final output \(f_{\mathrm{out}}\) is determined by \((f_{t},\widehat{y}_{t},y_{t},\Delta^{\prime}_{t})_{t=1}^{T}\). From now on, we consider \(\Delta^{\prime}_{t}\) as the feedback in the learning process of a conservative algorithm since it make no difference from running the same algorithm with feedback \(\Delta_{t}\).

Smooth the data distributionFor technical reasons (appearing later in the analysis), we don't want to analyze distribution \(\{\mathcal{D}_{i}|i\in[n]\}\) directly as the probability of \(\Delta_{t}=\mathbf{e}_{i}\) is \(0\) when \(f_{t}(\mathbf{e}_{i})=+1\) under distribution \(\mathcal{D}_{i}\). Instead, we consider the mixture of \(\mathcal{D}_{i}\) and another distribution \(\mathcal{D}^{\prime\prime}\), which is identical to \(\mathcal{D}_{i}\) except that \(r(x)=d(x,\mathbf{e}_{i})\) when \(y=-\). More specifically, let \(\mathcal{D}^{\prime}_{i}=(1-p)\mathcal{D}_{i}+p\mathcal{D}^{\prime\prime}_{i}\) with some extremely small \(p\), where \(\mathcal{D}^{\prime\prime}_{i}\)'s marginal distribution over \(\mathcal{X}\times\mathcal{Y}\) is still \(\operatorname{Unif}(X_{0})\times\operatorname{Rad}(1-6\varepsilon)\); the radius is \(r=2\) when \(y=+\), ; and the radius is \(r=d(x,\mathbf{e}_{i})\) when \(y=-\). For any data distribution \(\mathcal{D}\), let \(\mathbf{P}_{\mathcal{D}}\) be the dynamics of \((f_{1},y_{1},\widehat{y}_{1},\Delta^{\prime}_{1},\ldots,f_{T},y_{T},\widehat{y }_{T},\Delta^{\prime}_{T})\) under \(\mathcal{D}\). According to Lemma 4, by setting \(p=\frac{\varepsilon}{16n^{2}}\), when \(T\leq\frac{n}{\varepsilon}\), with high probability we never sample from \(\mathcal{D}^{\prime\prime}_{i}\) and have that for any \(i,j\in[n]\)

\[\big{|}\mathbf{P}_{\mathcal{D}_{i}}(i_{\mathrm{out}}=j)-\mathbf{P}_{\mathcal{D }^{\prime}_{i}}(i_{\mathrm{out}}=j)\big{|}\leq\frac{1}{8}\,. \tag{12}\]

From now on, we only consider distribution \(\mathcal{D}^{\prime}_{i}\) instead of \(\mathcal{D}_{i}\). The readers might have the question that why not using \(\mathcal{D}^{\prime}_{i}\) for construction directly. This is because \(\mathcal{D}^{\prime}_{i}\) does not satisfy realizability and no hypothesis has zero loss under \(\mathcal{D}^{\prime}_{i}\).

Information gain from different choices of \(f_{t}\)In each round of interaction, the learner picks a predictor \(f_{t}\), which can be out of \(\mathcal{H}\). Here we enumerate all choices of \(f_{t}\).

* \(f_{t}(\cdot)=2\mathds{1}_{\emptyset}-1\) predicts all points in \(\mathcal{X}\) by negative. No matter what \(i^{*}\) is, we will observe \((\Delta_{t}=x_{t},y_{t})\sim\operatorname{Unif}(X_{0})\times\operatorname{Rad }(1-6\varepsilon)\) and \(\widehat{y}_{t}=-\). They are identically distributed for all \(i^{*}\in[n]\), and thus, \(\Delta_{t}^{\prime}\) is also identically distributed. We cannot tell any information of \(i^{*}\) from this round.
* \(f_{t}=2\mathds{1}_{a_{t}}-1\) for some \(a_{t}\subset\mathcal{X}\) s.t. \(a\cap X_{0}\neq\emptyset\). Then \(\Delta_{t}=\Delta(x_{t},f_{t},r_{t})=\Delta(x_{t},f_{t},2\alpha)\) since \(r_{t}>2\alpha\) and \(d(x_{t},f_{t})\leq 2\alpha\), \(\widehat{y}_{t}=+\), \(y_{t}\sim\operatorname{Rad}(1-6\varepsilon)\). None of these depends on \(i^{*}\) and again, the distribution of \((\widehat{y}_{t},y_{t},\Delta_{t}^{\prime})\) is identical for all \(i^{*}\) and we cannot tell any information of \(i^{*}\) from this round.
* \(f_{t}=2\mathds{1}_{a_{t}}-1\) for some non-empty \(a_{t}\subset\{\mathbf{e}_{1},\ldots,\mathbf{e}_{n}\}\). For rounds with \(y_{t}=+\), we have \(\widehat{y}_{t}=+\) and \(\Delta_{t}=\Delta(x_{t},f_{t},2)\), which still not depend on \(i^{*}\). Thus we cannot learn any information about \(i^{*}\). But we can learn when \(y_{t}=-\). For rounds with \(y_{t}=-\), if \(\Delta_{t}\in a_{t}\), then we could observe \(\widehat{y}_{t}=+\) and \(\Delta_{t}^{\prime}=\Delta_{t}\), which at least tells that \(2\mathds{1}_{\{\Delta_{t}\}}-1\) is not the target function (with high probability); if \(\Delta_{t}\notin a_{t}\), then \(\widehat{y}_{t}=-\) and we observe \(\Delta_{t}^{\prime}=\perp\).

Therefore, we only need to focus on the rounds with \(f_{t}=2\mathds{1}_{a_{t}}-1\) for some non-empty \(a_{t}\subset\{\mathbf{e}_{1},\ldots,\mathbf{e}_{n}\}\) and \(y_{t}=-\). It is worth noting that drawing an example \(x\) from \(X_{0}\) uniformly, it is equivalent to uniformly drawing a permutation of \(\mathcal{H}\) such that the distances between \(x\) and \(h\) over all \(h\in\mathcal{H}\) are permuted according to it. Then \(\Delta_{t}=\mathbf{e}_{j}\) iff \(\mathbf{e}_{j}\in a_{t}\), \(d(x,\mathbf{e}_{j})\leq d(x,\mathbf{e}_{i^{*}})\) and \(d(x,\mathbf{e}_{j})\leq d(x,\mathbf{e}_{l})\) for all \(\mathbf{e}_{l}\in a_{t}\). Let \(k_{t}=|a_{t}|\) denote the cardinality of \(a_{t}\). In such rounds, under distribution \(\mathcal{D}_{i^{*}}\), the distribution of \(\Delta_{t}^{\prime}\) are described as follows.

1. The case of \(\mathbf{e}_{i^{*}}\in a_{t}\): For all \(j\in a_{t}\setminus\{i^{*}\}\), with probability \(\frac{1}{k_{t}}\), \(d(x_{t},\mathbf{e}_{j})=\min_{\mathbf{e}_{l}\in a_{t}}d(x_{t},\mathbf{e}_{l})\) and thus, \(\Delta_{t}^{\prime}=\Delta_{t}=\mathbf{e}_{j}\) and \(\widehat{y}_{t}=+\) (mistake round). With probability \(\frac{1}{k_{t}}\), we have \(d(x_{t},\mathbf{e}_{i^{*}})=\min_{\mathbf{e}_{l}\in a_{t}}d(x_{t},\mathbf{e}_{ l})\). If the example is drawn from \(\mathcal{D}_{i^{*}}\), we have \(\Delta_{t}=x_{t}\) and \(y_{t}=-\) (correct round), thus \(\Delta_{t}^{\prime}=\perp\). If the example is drawn from \(\mathcal{D}_{i^{*}}^{\prime\prime}\), we have we have \(\Delta_{t}^{\prime}=\Delta_{t}=\mathbf{e}_{i^{*}}\) and \(y_{t}=+\) (mistake round). Therefore, according to the definition of \(\Delta_{t}^{\prime}\) (Eq (11)), we have \[\Delta_{t}^{\prime}=\begin{cases}\mathbf{e}_{j}&\text{w.p. }\frac{1}{k_{t}}\text{ for }\mathbf{e}_{j}\in a_{t},j\neq i^{*}\\ \mathbf{e}_{i^{*}}&\text{w.p. }\frac{1}{k_{t}}p\\ \perp&\text{w.p. }\frac{1}{k_{t}}(1-p)\,.\end{cases}\] We denote this distribution by \(P_{\mathbb{E}}(a_{t},i^{*})\).
2. The case of \(\mathbf{e}_{i^{*}}\notin a_{t}\): For all \(j\in a_{t}\), with probability \(\frac{1}{k_{t}+1}\), then \(d(x_{t},\mathbf{e}_{j})=\min_{\mathbf{e}_{l}\in a_{t}\cup\{\mathbf{e}_{i^{*}}\}}d(x_{t},\mathbf{e}_{l})\) and thus, \(\Delta_{t}=\mathbf{e}_{j}\) and \(\widehat{y}_{t}=+\) (mistake round). With probability \(\frac{1}{k_{t}+1}\), we have \(d(x,\mathbf{e}_{i^{*}})<\min_{\mathbf{e}_{l}\in a_{t}}d(x_{t},\mathbf{e}_{l})\) and thus, \(\Delta_{t}=x_{t}\), \(\widehat{y}_{t}=-\) (correct round), and \(\Delta_{t}^{\prime}=\perp\). Therefore, the distribution of \(\Delta_{t}^{\prime}\) is \[\Delta_{t}^{\prime}=\begin{cases}\mathbf{e}_{j}&\text{w.p. }\frac{1}{k_{t}+1}\text{ for }\mathbf{e}_{j}\in a_{t}\\ \perp&\text{w.p. }\frac{1}{k_{t}+1}\,.\end{cases}\] We denote this distribution by \(P_{\mathbb{E}}(a_{t})\).

To measure the information obtained from \(\Delta_{t}^{\prime}\), we will utilize the KL divergence of the distribution of \(\Delta_{t}^{\prime}\) under the data distribution \(\mathcal{D}_{i^{*}}\) from that under a benchmark distribution. Let \(\overline{\mathcal{D}}=\frac{1}{n}\sum_{i\in n}\mathcal{D}_{i}^{\prime}\) denote the average distribution. The process of sampling from \(\overline{\mathcal{D}}\) is equivalent to sampling \(i^{*}\) uniformly at random from \([n]\) first and drawing a sample from \(\mathcal{D}_{i^{*}}\). Then under \(\overline{\mathcal{D}}\), for any \(\mathbf{e}_{j}\in a_{t}\), we have

\[\Pr(\Delta_{t}^{\prime}=\mathbf{e}_{j})= \Pr(i^{*}=j)\Pr(\Delta_{t}^{\prime}=\mathbf{e}_{j}|i^{*}=j)+\Pr(i^{*} \in a_{t}\setminus\{j\})\Pr(\Delta_{t}^{\prime}=\mathbf{e}_{j}|i^{*}\in a_{t} \setminus\{j\})\] \[+\Pr(i^{*}\notin a_{t})\Pr(\Delta_{t}^{\prime}=\mathbf{e}_{j}|i^{*} \notin a_{t})\]\[= \frac{1}{n}\cdot\frac{p}{k_{t}}+\frac{k_{t}-1}{n}\cdot\frac{1}{k_{t}}+ \frac{n-k_{t}}{n}\cdot\frac{1}{k_{t}+1}=\frac{nk_{t}-1+p(k_{t}+1)}{nk_{t}(k_{t}+ 1)}\,,\]

and

\[\Pr(\Delta^{\prime}_{t}= \bot) =\Pr(i^{*}\in a_{t})\Pr(\Delta^{\prime}_{t}= \bot\ |i^{*}\in a_{t})+\Pr(i^{*}\notin a_{t})\Pr(\Delta^{\prime}_{t}= \bot\ |i^{*}\notin a_{t})\] \[=\frac{k_{t}}{n}\cdot\frac{1-p}{k_{t}}+\frac{n-k_{t}}{n}\cdot \frac{1}{k_{t}+1}=\frac{n+1-p(k_{t}+1)}{n(k_{t}+1)}\,.\]

Thus, the distribution of \(\Delta^{\prime}_{t}\) under \(\overline{\mathcal{D}}\) is

\[\Delta^{\prime}_{t}=\begin{cases}\mathbf{e}_{j}&\text{w.p. }\frac{nk_{t}-1+p(k_{t}+1)}{nk_{t}(k_{t}+1)} \text{ for }\mathbf{e}_{j}\in a_{t}\\ \bot&\text{w.p. }\frac{n+1-p(k_{t}+1)}{n(k_{t}+1)}\,.\end{cases}\]

We denote this distribution by \(\overline{P}(a_{t})\). Next we will compute the KL divergences of \(P_{\in}(a_{t},i^{*})\) and \(P_{\notin}(a_{t})\) from \(\overline{P}(a_{t})\). We will use the inequality \(\log(1+x)\leq x\) for \(x\geq 0\) in the following calculation. For any \(i^{*}\) s.t. \(\mathbf{e}_{i^{*}}\in a_{t}\), we have

\[\mathrm{D}_{\mathrm{KL}}(\overline{P}(a_{t})\|P_{\in}(a_{t},i^{*}))\] \[= (k_{t}-1)\frac{nk_{t}-1+p(k_{t}+1)}{nk_{t}(k_{t}+1)}\log(\frac{nk_ {t}-1+p(k_{t}+1)}{nk_{t}(k_{t}+1)}k_{t})\] \[+\frac{nk_{t}-1+p(k_{t}+1)}{nk_{t}(k_{t}+1)}\log(\frac{nk_{t}-1+p( k_{t}+1)}{nk_{t}(k_{t}+1)}\cdot\frac{k_{t}}{p})\] \[+\frac{n+1-p(k_{t}+1)}{n(k_{t}+1)}\log(\frac{n+1-p(k_{t}+1)}{n(k_ {t}+1)}\cdot\frac{k_{t}}{1-p})\] \[\leq 0+\frac{1}{k_{t}+1}\log(\frac{1}{p})+\frac{2p}{k_{t}+1}=\frac{1} {k_{t}+1}\log(\frac{1}{p})+\frac{2p}{k_{t}+1}\,, \tag{13}\]

and

\[\mathrm{D}_{\mathrm{KL}}(\overline{P}(a_{t})\|P_{\notin}(a_{t}))\] \[= k_{t}\frac{nk_{t}-1+p(k_{t}+1)}{nk_{t}(k_{t}+1)}\log(\frac{nk_{t} -1+p(k_{t}+1)}{nk_{t}(k_{t}+1)}(k_{t}+1))\] \[+\frac{n+1-p(k_{t}+1)}{n(k_{t}+1)}\log(\frac{n+1-p(k_{t}+1)}{n(k_ {t}+1)}(k_{t}+1))\] \[\leq 0+\frac{n+1}{n^{2}(k_{t}+1)}=\frac{n+1}{n^{2}(k_{t}+1)}\,. \tag{14}\]

Lower bound of the informationWe utilize the information theoretical framework of proving lower bounds for linear bandits (Theorem 11 by Rajaraman et al. (2023)) here. For notation simplicity, for all \(i\in[n]\), let \(\mathbf{P}_{i}\) denote the dynamics of \((f_{1},\Delta^{\prime}_{1},y_{1},\widehat{y}_{1},\ldots,f_{T},\Delta^{\prime} _{T},y_{T},\widehat{y}_{T})\) under \(\mathcal{D}^{\prime}_{i}\) and \(\overline{\mathbf{P}}\) denote the dynamics under \(\overline{\mathcal{D}}\). Let \(B_{t}\) denote the event of \(\{f_{t}=2\mathbf{1}_{a_{t}}-1\) for some non-empty \(a_{t}\subset\{\mathbf{e}_{1},\ldots,\mathbf{e}_{n}\}\}\). As discussed before, for any \(a_{t}\), conditional on \(\neg B_{t}\) or \(y_{t}=+1\), \((\Delta^{\prime}_{t},y_{t},\widehat{y}_{t})\) are identical in all \(\{\mathcal{D}^{\prime}_{i}|i\in[n]\}\), and therefore, also identical in \(\overline{\mathcal{D}}\). We can only obtain information at rounds when \(B_{t}\wedge(y_{t}=-1)\) occurs. In such rounds, we know that \(f_{t}\) is fully determined by history (possibly with external randomness, which does not depend on data distribution), \(y_{t}=-1\) and \(\widehat{y}_{t}\) is fully determined by \(\Delta^{\prime}_{t}\) (\(\widehat{y}_{t}=+1\) iff. \(\Delta^{\prime}_{t}\in a_{t}\)).

Therefore, conditional the history \(H_{t-1}=(f_{1},\Delta^{\prime}_{1},y_{1},\widehat{y}_{1},\ldots,f_{t-1},\Delta^ {\prime}_{t-1},y_{t-1},\widehat{y}_{t-1})\) before time \(t\), we have

\[\mathrm{D}_{\mathrm{KL}}(\overline{\mathbf{P}}(f_{t},\Delta^{ \prime}_{t},y_{t},\widehat{y}_{t}|H_{t-1})\|\mathbf{P}_{i}(f_{t},\Delta^{ \prime}_{t},y_{t},\widehat{y}_{t}|H_{t-1}))\] \[= \overline{\mathbf{P}}(B_{t}\wedge(y_{t}=-1))\mathrm{D}_{\mathrm{KL }}(\overline{\mathbf{P}}(\Delta^{\prime}_{t}|H_{t-1},B_{t}\wedge(y_{t}=-1))\| \mathbf{P}_{i}(\Delta^{\prime}_{t}|H_{t-1},B_{t}\wedge(y_{t}=-1)))\] \[= 6\overline{\mathbf{P}}(B_{t})\mathrm{D}_{\mathrm{KL}}(\overline{ \mathbf{P}}(\Delta^{\prime}_{t}|H_{t-1},B_{t}\wedge(y_{t}=-1))\|\mathbf{P}_{i}( \Delta^{\prime}_{t}|H_{t-1},B_{t}\wedge(y_{t}=-1)))\,, \tag{15}\]

where the last equality holds due to that \(y_{t}\sim\mathrm{Rad}(1-6\varepsilon)\) and does not depend on \(B_{t}\).

For any algorithm that can successfully identify \(i\) under the data distribution \(\mathcal{D}_{i}\) with probability \(\frac{3}{4}\) for all \(i\in[n]\), then \(\mathbf{P}_{\mathcal{D}_{i}}(i_{\mathrm{out}}=i)\geq\frac{3}{4}\) and \(\mathbf{P}_{\mathcal{D}_{j}}(i_{\mathrm{out}}=i)\leq\frac{1}{4}\) for all \(j\neq i\). Recall that \(\mathcal{D}_{i}\) and \(\mathcal{D}^{\prime}_{i}\) are very close when the mixture parameter \(p\) is small. Combining with Eq (12), we have

\[\left|\mathbf{P}_{i}(i_{\mathrm{out}}=i)-\mathbf{P}_{j}(i_{\mathrm{ out}}=i)\right|\] \[\geq \left|\mathbf{P}_{\mathcal{D}_{i}}(i_{\mathrm{out}}=i)-\mathbf{P} _{\mathcal{D}_{j}}(i_{\mathrm{out}}=i)\right|-\left|\mathbf{P}_{\mathcal{D}_{ i}}(i_{\mathrm{out}}=i)-\mathbf{P}_{i}(i_{\mathrm{out}}=i)\right|-\left| \mathbf{P}_{\mathcal{D}_{j}}(i_{\mathrm{out}}=i)-\mathbf{P}_{j}(i_{\mathrm{out }}=i)\right|\] \[\geq \frac{1}{2}-\frac{1}{4}=\frac{1}{4}\,.\]

Then we have the total variation distance between \(\mathbf{P}_{i}\) and \(\mathbf{P}_{j}\)

\[\mathrm{TV}(\mathbf{P}_{i},\mathbf{P}_{j})\geq\left|\mathbf{P}_{i}(i_{\mathrm{ out}}=i)-\mathbf{P}_{j}(i_{\mathrm{out}}=i)\right|\geq\frac{1}{4}\,. \tag{16}\]

Then we have

\[\mathbb{E}_{i\sim\mathrm{Unif}([n])}\left[\mathrm{TV}^{2}(\mathbf{ P}_{i},\mathbf{P}_{(i+1)\;\mathrm{mod}\;n})\right]\leq 4\mathbb{E}_{i\sim\mathrm{Unif}([n])}\left[\mathrm{TV}^{2}(\mathbf{P}_{i}, \overline{\mathbf{P}})\right]\] \[\leq 2\mathbb{E}_{i}\left[\mathrm{D}_{\mathrm{KL}}(\overline{ \mathbf{P}}||\mathbf{P}_{i})\right]\] (Pinsker's ineq) \[= 2\mathbb{E}_{i}\left[\sum_{t=1}^{T}\mathrm{D}_{\mathrm{KL}}( \overline{\mathbf{P}}(f_{t},\Delta^{\prime}_{t},y_{t},\widehat{y}_{t}|H_{t-1} )\|\mathbf{P}_{i}(f_{t},\Delta^{\prime}_{t},y_{t},\widehat{y}_{t}|H_{t-1}))\right]\] (Chain rule) \[= 12\varepsilon\mathbb{E}_{i}\left[\sum_{t=1}^{T}\overline{\mathbf{ P}}(B_{t})\mathrm{D}_{\mathrm{KL}}(\overline{\mathbf{P}}(\Delta^{\prime}_{t}|H_{t-1},B_{t} \wedge(y_{t}=-1))\|\mathbf{P}_{i}(\Delta^{\prime}_{t}|H_{t-1},B_{t}\wedge(y_{t }=-1)))\right]\] (Apply Eq (15)) \[= \frac{12\varepsilon}{n}\sum_{t=1}^{T}\overline{\mathbf{P}}(B_{t} )\sum_{i=1}^{n}\mathrm{D}_{\mathrm{KL}}(\overline{\mathbf{P}}(\Delta^{\prime} _{t}|H_{t-1},B_{t}\wedge(y_{t}=-1))\|\mathbf{P}_{i}(\Delta^{\prime}_{t}|H_{t-1 },B_{t}\wedge(y_{t}=-1)))\] \[= \frac{12\varepsilon}{n}\mathbb{E}_{f_{1:T}\sim\overline{\mathbf{ P}}}\left[\sum_{t=1}^{T}\mathds{1}(B_{t})\left(\sum_{i:i\in a_{t}}\mathrm{D}_{ \mathrm{KL}}(\overline{P}(a_{t})\|P_{\in}(a_{t},i))+\sum_{i:i\notin a_{t}} \mathrm{D}_{\mathrm{KL}}(\overline{P}(a_{t})\|P_{\notin}(a_{t}))\right)\right]\] \[\leq \frac{12\varepsilon}{n}\sum_{t=1}^{T}\mathbb{E}_{f_{1:T}\sim \overline{\mathbf{P}}}\left[\sum_{i:i\in a_{t}}\left(\frac{1}{k_{t}+1}\log( \frac{1}{p})+\frac{2p}{k_{t}+1}\right)+\sum_{i:i\notin a_{t}}\frac{n+1}{n^{2}( k_{t}+1)}\right]\] (Apply Eq (13),(14)) \[\leq \frac{12\varepsilon}{n}\sum_{t=1}^{T}(\log(\frac{1}{p})+2p+1)\] \[\leq \frac{12T\varepsilon(\log(16n^{2}/\varepsilon)+2)}{n}\,.\]

Combining with Eq (16), we have that there exists a universal constant \(c\) such that \(T\geq\frac{cn}{\varepsilon(\log(n/\varepsilon)+1)}\). 

## Appendix I Proof of Theorem 8

Proof.: We will prove Theorem 8 by constructing an instance of \(\mathcal{Q}\) and \(\mathcal{H}\) and then reduce it to a linear stochastic bandit problem.

**Construction of \(\mathcal{Q},\mathcal{H}\) and a set of realizable distributions**

* Consider the input metric space in the shape of a star, where \(\mathcal{X}=\{0,1,\ldots,n\}\) and the distance function of \(d(0,i)=1\) and \(d(i,j)=2\) for all \(i\neq j\in[n]\).
* Let the hypothesis class be a set of singletons over \([n]\), i.e., \(\mathcal{H}=\{2\mathds{1}_{\{i\}}-1|i\in[n]\}\).

* We define a collection of distributions \(\{\mathcal{D}_{i}|i\in[n]\}\) in which \(\mathcal{D}_{i}\) is realized by \(2\mathds{1}_{\{i\}}-1\). The data distribution \(\mathcal{D}_{i}\) put \(1-3(n-1)\varepsilon\) on \((0,1,+)\) and \(3\varepsilon\) on \((i,1,-)\) for all \(i\neq i^{*}\). Hence, note that all distributions in \(\{\mathcal{D}_{i}|i\in[n]\}\) share the same distribution support \(\{(0,1,+)\}\cup\{(i,1,-)|i\in[n]\}\), but have different weights.

Randomization and improperness of the output \(f_{\mathrm{out}}\) do not help.Note that algorithms are allowed to output a randomized \(f_{\mathrm{out}}\) and to output \(f_{\mathrm{out}}\notin\mathcal{H}\). We will show that randomization and improperness of \(f_{\mathrm{out}}\) don't make the problem easier. Supposing that the data distribution is \(\mathcal{D}_{i^{*}}\) for some \(i^{*}\in[n]\), finding a (possibly randomized and improper) \(f_{\mathrm{out}}\) is not easier than identifying \(i^{*}\). Since our feature space \(\mathcal{X}\) is finite, we can enumerate all hypotheses not equal to \(2\mathds{1}_{\{i^{*}\}}-1\) and calculate their strategic population loss as follows. The hypothesis \(2\mathds{1}_{\emptyset}-1\) will predict all by negative and thus \(\mathcal{L}^{\mathrm{str}}(2\mathds{1}_{\emptyset}-1)=1-3(n-1)\varepsilon\). For any hypothesis predicting \(0\) by positive, it will predict all points in the distribution support by positive and thus incurs strategic loss \(3(n-1)\varepsilon\). For any hypothesis predicting \(0\) by negative and some \(i\neq i^{*}\) by positive, then it will misclassify \((i,1,-)\) and incur strategic loss \(3\varepsilon\). Therefore, for any hypothesis \(h\neq 2\mathds{1}_{\{i^{*}\}}-1\), we have \(\mathcal{L}^{\mathrm{str}}_{\mathcal{D}_{i^{*}}}(h)\geq 3\varepsilon\).

Similar to the proof of Theorem 7, under distribution \(\mathcal{D}_{i^{*}}\), if we are able to find a (possibly randomized) \(f_{\mathrm{out}}\) with strategic loss \(\mathcal{L}^{\mathrm{str}}(f_{\mathrm{out}})\leq\varepsilon\). Then \(\Pr_{h\sim f_{\mathrm{out}}}(h=2\mathds{1}_{\{i^{*}\}}-1)\geq\frac{2}{3}\). We can identify \(i^{*}\) by checking which realization of \(f_{\mathrm{out}}\) has probability greater than \(\frac{2}{3}\). In the following, we will focus on the sample complexity to identify the target function \(2\mathds{1}_{\{i^{*}\}}-1\) or simply \(i^{*}\). Let \(i_{\mathrm{out}}\) denote the algorithm's answer to question of "what is \(i^{*}\)?".

Smooth the data distributionFor technical reasons (appearing later in the analysis), we don't want to analyze distribution \(\{\mathcal{D}_{i}|i\in[n]\}\) directly as the probability of \((i,1,-)\) is \(0\) under distribution \(\mathcal{D}_{i}\). Instead, for each \(i\in[n]\), let \(\mathcal{D}^{\prime}_{i}=(1-p)\tilde{\mathcal{D}}_{i}+p\mathcal{D}^{\prime \prime}_{i}\) be the mixture of \(\mathcal{D}_{i}\) and \(\mathcal{D}^{\prime\prime}_{i}\) for some small \(p\), where \(\mathcal{D}^{\prime\prime}_{i}=(1-3(n-1)\varepsilon)\mathds{1}_{\{(0,1,+)\}} +3(n-1)\varepsilon\mathds{1}_{\{(i,1,-)\}}\). Specifically,

\[\mathcal{D}^{\prime}_{i}(z)=\begin{cases}1-3(n-1)\varepsilon&\text{for }z=(0,1,+)\\ 3(1-p)\varepsilon&\text{for }z=(j,1,-),\forall j\neq i\\ 3(n-1)p\varepsilon&\text{for }z=(i,1,-)\end{cases}\]

For any data distribution \(\mathcal{D}\), let \(\mathbf{P}_{\mathcal{D}}\) be the dynamics of \((f_{1},y_{1},\widehat{y}_{1},\ldots,f_{T},y_{T},\widehat{y}_{T})\) under \(\mathcal{D}\). According to Lemma 4, by setting \(p=\frac{\varepsilon}{16n^{2}}\), when \(T\leq\frac{n}{\varepsilon}\), we have that for any \(i,j\in[n]\)

\[\big{|}\mathbf{P}_{\mathcal{D}_{i}}(i_{\mathrm{out}}=j)-\mathbf{P}_{\mathcal{ D}^{\prime}_{i}}(i_{\mathrm{out}}=j)\big{|}\leq\frac{1}{8}\,. \tag{17}\]

From now on, we only consider distribution \(\mathcal{D}^{\prime}_{i}\) instead of \(\mathcal{D}_{i}\). The readers might have the question that why not using \(\mathcal{D}^{\prime}_{i}\) for construction directly. This is because no hypothesis has zero loss under \(\mathcal{D}^{\prime}_{i}\), and thus \(\mathcal{D}^{\prime}_{i}\) does not satisfy realizability requirement.

Information gain from different choices of \(f_{t}\)Note that in each round, the learner picks a \(f_{t}\) and then only observes \(\widehat{y}_{t}\) and \(y_{t}\). Here we enumerate choices of \(f_{t}\) as follows.

1. \(f_{t}=2\mathds{1}_{\emptyset}-1\) predicts all points in \(\mathcal{X}\) by negative. No matter what \(i^{*}\) is, we observe \(\widehat{y}_{t}=-\) and \(y_{t}=2\mathds{1}(x_{t}=0)-1\). Hence \((\widehat{y}_{t},y_{t})\) are identically distributed for all \(i^{*}\in[n]\), and thus, we cannot learn anything about \(i^{*}\) from this round.
2. \(f_{t}\) predicts \(0\) by positive. Then no matter what \(i^{*}\) is, we have \(\widehat{y}_{t}=+\) and \(y_{t}=\mathds{1}(x_{t}=0)\). Thus again, we cannot learn anything about \(i^{*}\).
3. \(f_{t}=2\mathds{1}_{a_{t}}-1\) for some non-empty \(a_{t}\subset[n]\). For rounds with \(x_{t}=0\), we have \(\widehat{y}_{t}=y_{t}=+\) no matter what \(i^{*}\) is and thus, we cannot learn anything about \(i^{*}\). For rounds with \(y_{t}=-\), i.e., \(x_{t}\neq 0\), we will observe \(\widehat{y}_{t}=f_{t}(\Delta(x_{t},f_{t},1))=\mathds{1}(x_{t}\in a_{t})\).

Hence, we can only extract information with the third type of \(f_{t}\) at rounds with \(x_{t}\neq 0\).

Reduction to stochastic linear banditsIn rounds with \(f_{t}=2\mathds{1}_{a_{t}}-1\) for some non-empty \(a_{t}\subset[n]\) and \(x_{t}\neq 0\), our problem is identical to a stochastic linear bandit problem. Let us state our problem as Problem 1 and a linear bandit problem as Problem 2. Let \(A=\{0,1\}^{n}\setminus\{\mathbf{0}\}\).

**Problem 1**.: _The environment picks an \(i^{*}\in[n]\). At each round \(t\), the environment picks \(x_{t}\in\{\mathbf{e}_{i}|i\in[n]\}\) with \(P(i)=\frac{1-p}{n-1}\) for \(i\neq i^{*}\) and \(P(i^{*})=p\) and the learner picks an \(a_{t}\in A\) (where we use a \(n\)-bit string to represent \(a_{t}\) and \(a_{t,i}=1\) means that \(a_{t}\) predicts \(i\) by positive). Then the learner observes \(\widehat{y}_{t}=\mathds{1}(a_{t}^{\intercal}x_{t}>0)\) (where we use \(0\) to represent negative label)._

**Problem 2**.: _The environment picks a linear parameter \(w^{*}\in\{w^{i}|i\in[n]\}\) with \(w^{i}=\frac{1-p}{n-1}\mathds{1}-(\frac{1-p}{n-1}-p)\mathbf{e}_{i}\). The arm set is \(A\). For each arm \(a\in A\), the reward is i.i.d. from the following distribution:_

\[r_{w}(a)=\begin{cases}-1,\text{ w.p. }w^{\intercal}a\,,\\ 0\,.\end{cases} \tag{18}\]

_If the linear parameter \(w^{*}=w^{i^{*}}\), the optimal arm is \(\mathbf{e}_{i^{*}}\)._

**Claim 1**.: _For any \(\delta>0\), for any algorithm \(\mathcal{A}\) that identify \(i^{*}\) correctly with probability \(1-\delta\) within \(T\) rounds for any \(i^{*}\in[n]\) in Problem 1, we can construct another algorithm \(\mathcal{A}^{\prime}\) can also identify the optimal arm in any environment with probability \(1-\delta\) within \(T\) rounds in Problem 2._

This claim follows directly from the problem descriptions. Given any algorithm \(\mathcal{A}\) for Problem 1, we can construct another algorithm \(\mathcal{A}^{\prime}\) which simulates \(\mathcal{A}\). At round \(t\), if \(\mathcal{A}\) selects predictor \(a_{t}\), then \(\mathcal{A}^{\prime}\) picks arm the same as \(a_{t}\). Then \(\mathcal{A}^{\prime}\) observes a reward \(r_{w^{*}}\cdot(a_{t})\), which is \(-1\) w. \(w^{i^{*}\intercal}a_{t}\) and feed \(-r_{w^{*}}(a_{t})\) to \(\mathcal{A}\). Since \(\widehat{y}_{t}\) in Problem 1 is \(1\) w.p. \(\sum_{i=1}^{n}a_{t,i}P(i)=w^{i^{*}\intercal}a_{t}\), it is distributed identically as \(-r_{w^{*}}(a_{t})\). Since \(\mathcal{A}\) will be able to identify \(i^{*}\) w.p. \(1-\delta\) in \(T\) rounds, \(\mathcal{A}^{\prime}\) just need to output \(\mathbf{e}_{i^{*}}\) as the optimal arm.

Then any lower bound on \(T\) for Problem 2 also lower bounds Problem 1. Hence, we adopt the information theoretical framework of proving lower bounds for linear bandits (Theorem 11 by Rajaraman et al. (2023)) to prove a lower bound for our problem. In fact, we also apply this framework to prove the lower bounds in other settings of this work, including Theorem 7 and Theorem 9.

Lower bound of the informationFor notation simplicity, for all \(i\in[n]\), let \(\mathbf{P}_{i}\) denote the dynamics of \((f_{1},y_{1},\widehat{y}_{1},\ldots,f_{T},y_{T},\widehat{y}_{T})\) under \(\mathcal{D}^{\prime}_{i}\) and and \(\overline{\mathbf{P}}\) denote the dynamics under \(\overline{\mathcal{D}}=\frac{1}{n}\mathcal{D}^{\prime}_{i}\). Let \(B_{t}\) denote the event of \(\{f_{t}=2\mathds{1}_{a_{t}}-1\) for some non-empty \(a_{t}\subset[n]\}\). As discussed before, for any \(a_{t}\), conditional on \(-B_{t}\) or \(y_{t}=+1\), \((x_{t},y_{t},\widehat{y}_{t})\) are identical in all \(\{\mathcal{D}^{\prime}_{i}|i\in[n]\}\), and therefore, also identical in \(\overline{\mathcal{D}}\). We can only obtain information at rounds when \(B_{t}\wedge y_{t}=-1\) occurs. In such rounds, \(f_{t}\) is fully determined by history (possibly with external randomness, which does not depend on data distribution), \(y_{t}=-1\) and \(\widehat{y}_{t}=-r_{w}(a_{t})\) with \(r_{w}(a_{t})\) sampled from the distribution defined in Eq (18).

For any algorithm that can successfully identify \(i\) under the data distribution \(\mathcal{D}_{i}\) with probability \(\frac{3}{4}\) for all \(i\in[n]\), then \(\mathbf{P}_{\mathcal{D}_{i}}(i_{\mathrm{out}}=i)\geq\frac{3}{4}\) and \(\mathbf{P}_{\mathcal{D}_{j}}(i_{\mathrm{out}}=i)\leq\frac{1}{4}\) for all \(j\neq i\). Recall that \(\mathcal{D}_{i}\) and \(\mathcal{D}^{\prime}_{i}\) are very close when the mixture parameter \(p\) is small. Combining with Eq (17), we have

\[|\mathbf{P}_{i}(i_{\mathrm{out}}=i)-\mathbf{P}_{j}(i_{\mathrm{ out}}=i)|\] \[\geq \left|\mathbf{P}_{\mathcal{D}_{i}}(i_{\mathrm{out}}=i)-\mathbf{ P}_{\mathcal{D}_{j}}(i_{\mathrm{out}}=i)\right|-\left|\mathbf{P}_{\mathcal{D}_{i}}(i_{ \mathrm{out}}=i)-\mathbf{P}_{i}(i_{\mathrm{out}}=i)\right|-\left|\mathbf{P}_{ \mathcal{D}_{j}}(i_{\mathrm{out}}=i)-\mathbf{P}_{j}(i_{\mathrm{out}}=i)\right|\] \[\geq \frac{1}{2}-\frac{1}{4}=\frac{1}{4}\,. \tag{19}\]

Let \(\overline{w}=\frac{1}{n}\mathds{1}\). Let \(\mathrm{kl}(q,q^{\prime})\) denote the KL divergence from \(\mathrm{Ber}(q)\) to \(\mathrm{Ber}(q^{\prime})\). Let \(H_{t-1}=(f_{1},y_{1},\widehat{y}_{1},\ldots,f_{t-1},y_{t-1},\widehat{y}_{t-1})\) denote the history up to time \(t-1\). Then we have

\[\mathbb{E}_{i\sim\mathrm{Unif}([n])}\left[\mathrm{TV}^{2}(\mathbf{ P}_{i},\mathbf{P}_{i+1\bmod n})\right]\leq 4\mathbb{E}_{i\sim\mathrm{Unif}([n])}\left[\mathrm{TV}^{2}(\mathbf{P}_{i}, \overline{\mathbf{P}})\right]\] \[\leq 2\mathbb{E}_{i}\left[\mathrm{D}_{\mathrm{KL}}(\overline{\mathbf{ P}}||\mathbf{P}_{i})\right]\] (Pinsker's ineq) \[= 2\mathbb{E}_{i}\left[\sum_{t=1}^{T}\mathrm{D}_{\mathrm{KL}}( \overline{\mathbf{P}}(f_{t},y_{t},\widehat{y}_{t}|H_{t-1})\|\mathbf{P}_{i}(f_{ t},y_{t},\widehat{y}_{t}|H_{t-1}))\right]\] (Chain rule) \[= 2\mathbb{E}_{i}\left[\sum_{t=1}^{T}\overline{\mathbf{P}}(B_{t} \wedge y_{t}=-1)\mathbb{E}_{a_{1:T}\sim\overline{\mathbf{P}}}\left[\mathrm{D}_{ \mathrm{KL}}(\mathrm{Ber}(\langle\overline{w},a_{t}\rangle)\|\mathrm{Ber}( \langle w^{i},a_{t}\rangle))\right]\right]\]\[= 6(n-1)\varepsilon\mathbb{E}_{i}\left[\sum_{t=1}^{T}\overline{\mathbf{P }}(B_{t})\mathbb{E}_{a_{1:T}\sim\overline{\mathbf{P}}}\left[\mathrm{D}_{\mathrm{ KL}}(\mathrm{Ber}(\langle\overline{w},a_{t}\rangle)\|\mathrm{Ber}(\langle w^{i},a_{t} \rangle))\right]\right]\] \[= \frac{6(n-1)\varepsilon}{n}\sum_{t=1}^{T}\mathbb{E}_{a_{1:T}\sim \overline{\mathbf{P}}}\left[\sum_{i=1}^{n}\mathrm{D}_{\mathrm{KL}}(\mathrm{Ber} (\langle\overline{w},a_{t}\rangle)\|\mathrm{Ber}(\langle w^{i},a_{t}\rangle))\right]\] \[= \frac{6(n-1)\varepsilon}{n}\sum_{t=1}^{T}\mathbb{E}_{a_{1:T}\sim \overline{\mathbf{P}}}\left[\sum_{i:i\in a_{t}}\mathrm{kl}(\frac{k_{t}}{n}, \frac{(k_{t}-1)(1-p)}{n-1}+p)+\sum_{i:i\not=a_{t}}\mathrm{kl}(\frac{k_{t}}{n},\frac{k_{t}(1-p)}{n-1})\right]\] \[= \frac{6(n-1)\varepsilon}{n}\sum_{t=1}^{T}\mathbb{E}_{a_{1:T}\sim \overline{\mathbf{P}}}\left[k_{t}\mathrm{kl}(\frac{k_{t}}{n},\frac{(k_{t}-1)(1 -p)}{n-1}+p)+(n-k_{t})\mathrm{kl}(\frac{k_{t}}{n},\frac{k_{t}(1-p)}{n-1})\right] \tag{20}\]

If \(k_{t}=1\), then

\[k_{t}\cdot\mathrm{kl}(\frac{k_{t}}{n},\frac{(k_{t}-1)(1-p)}{n-1}+p)=\mathrm{kl} (\frac{1}{n},p)\leq\frac{1}{n}\log(\frac{1}{p})\,,\]

and

\[(n-k_{t})\cdot\mathrm{kl}(\frac{k_{t}}{n},\frac{k_{t}(1-p)}{n-1})=(n-1)\cdot \mathrm{kl}(\frac{1}{n},\frac{1-p}{n-1})\leq\frac{1}{(1-p)n(n-2)}\,,\]

where the ineq holds due to \(\mathrm{kl}(q,q^{\prime})\leq\frac{(q-q^{\prime})^{2}}{q^{\prime}(1-q^{\prime})}\). If \(k_{t}=n-1\), it is symmetric to the case of \(k_{t}=1\). We have

\[k_{t}\cdot\mathrm{kl}(\frac{k_{t}}{n},\frac{(k_{t}-1)(1-p)}{n-1} +p)=(n-1)\mathrm{kl}(\frac{n-1}{n},\frac{n-2}{n-1}+\frac{1}{n-1}p)=(n-1) \mathrm{kl}(\frac{1}{n},\frac{1-p}{n-1})\] \[\leq \frac{1}{(1-p)n(n-2)}\,,\]

and

\[(n-k_{t})\cdot\mathrm{kl}(\frac{k_{t}}{n},\frac{k_{t}(1-p)}{n-1})=\mathrm{kl} (\frac{n-1}{n},1-p)=\mathrm{kl}(\frac{1}{n},p)\leq\frac{1}{n}\log(\frac{1}{p} )\,.\]

If \(1<k_{t}<n-1\), then

\[k_{t}\cdot\mathrm{kl}(\frac{k_{t}}{n},\frac{(k_{t}-1)(1-p)}{n-1} +p)= k_{t}\cdot\mathrm{kl}(\frac{k_{t}}{n},\frac{k_{t}-1}{n-1}+\frac{n-k_{t}} {n-1}p)\stackrel{{(a)}}{{\leq}}k_{t}\cdot\mathrm{kl}(\frac{k_{t}} {n},\frac{k_{t}-1}{n-1})\] \[\stackrel{{(b)}}{{\leq}} k_{t}\cdot\frac{(\frac{k_{t}}{n}-\frac{k_{t}-1}{n-1})^{2}}{\frac{k_{t}-1}{n-1}(1- \frac{k_{t}-1}{n-1})}=k_{t}\cdot\frac{n-k_{t}}{n^{2}(k_{t}-1)}\leq\frac{k_{t} \cdot}{n(k_{t}-1)}\leq\frac{2}{n}\,,\]

where inequality (a) holds due to that \(\frac{k_{t}-1}{n-1}+\frac{n-k_{t}}{n-1}p\leq\frac{k_{t}}{n}\) and \(\mathrm{kl}(q,q^{\prime})\) is monotonically decreasing in \(q^{\prime}\) when \(q^{\prime}\leq q\) and inequality (b) adopts \(\mathrm{kl}(q,q^{\prime})\leq\frac{(q-q^{\prime})^{2}}{q^{\prime}(1-q^{\prime})}\), and

\[(n-k_{t})\cdot\mathrm{kl}(\frac{k_{t}}{n},\frac{k_{t}(1-p)}{n-1})\leq(n-k_{t} )\cdot\mathrm{kl}(\frac{k_{t}}{n},\frac{k_{t}}{n-1})\leq\frac{k_{t}(n-k_{t}) }{n^{2}(n-1-k_{t})}\leq\frac{2k_{t}}{n^{2}}\,,\]

where the first inequality hold due to that \(\frac{k_{t}(1-p)}{n-1}\geq\frac{k_{t}}{n}\), and \(\mathrm{kl}(q,q^{\prime})\) is monotonically increasing in \(q^{\prime}\) when \(q^{\prime}\geq q\) and the second inequality adopts \(\mathrm{kl}(q,q^{\prime})\leq\frac{(q-q^{\prime})^{2}}{q^{\prime}(1-q^{\prime})}\). Therefore, we have

\[\mathrm{Eq}\left(\ref{eq:20}\right)\leq\frac{6(n-1)\varepsilon}{n}\sum_{t=1}^ {T}\mathbb{E}_{a_{1:T}\sim\overline{\mathbf{P}}}\left[\frac{2}{n}\log(\frac{1}{p })\right]\leq\frac{12\varepsilon T\log(1/p)}{n}\,.\]

Combining with Eq (19), we have that there exists a universal constant \(c\) such that \(T\geq\frac{cn}{\varepsilon(\log(n/\varepsilon)+1)}\). 

## Appendix J Proof of Theorem 9

Proof.: We will prove Theorem 9 by constructing an instance of \(\mathcal{Q}\) and \(\mathcal{H}\) and showing that for any learning algorithm, there exists a realizable data distribution s.t. achieving \(\varepsilon\) loss requires at least \(\widetilde{\Omega}(\frac{|\mathcal{H}|}{\varepsilon})\) samples.

* Let feature vector space \(\mathcal{X}=\{0,1,\ldots,n\}\) and let the space of feature-manipulation set pairs \(\mathcal{Q}=\{(0,\{0\}\cup s)|s\subset[n]\}\). That is to say, every agent has the same original feature vector \(x=0\) but has different manipulation ability according to \(s\).
* Let the hypothesis class be a set of singletons over \([n]\), i.e., \(\mathcal{H}=\{2\mathds{1}_{\{i\}}-1|i\in[n]\}\).
* We now define a collection of distributions \(\{\mathcal{D}_{i}|i\in[n]\}\) in which \(\mathcal{D}_{i}\) is realized by \(2\mathds{1}_{\{i\}}-1\). For any \(i\in[n]\), let \(\mathcal{D}_{i}\) put probability mass \(1-6\varepsilon\) on \((0,\mathcal{X},+1)\) and \(6\varepsilon\) uniformly over \(\{(0,\{0\}\cup s_{\sigma,i},-1)|\sigma\in\mathcal{S}_{n}\}\), where \(\mathcal{S}_{n}\) is the set of all permutations over \(n\) elements and \(s_{\sigma,i}:=\{j|\sigma^{-1}(j)<\sigma^{-1}(i)\}\) is the set of elements appearing before \(i\) in the permutation \((\sigma(1),\ldots,\sigma(n))\). In other words, with probability \(1-6\varepsilon\), we will sample \((0,\mathcal{X},+1)\) and with \(\varepsilon\), we will randomly draw a permutation \(\sigma\sim\operatorname{Unif}(\mathcal{S}_{n})\) and return \((0,\{0\}\cup s_{\sigma,i},-1)\). The data distribution \(\mathcal{D}_{i}\) is realized by \(2\mathds{1}_{\{i\}}-1\) since for negative examples \((0,\{0\}\cup s_{\sigma,i},-1)\), we have \(i\notin s\) and for positive examples \((0,\mathcal{X},+1)\), we have \(i\in\mathcal{X}\).

Randomization and improperness of the output \(f_{\mathrm{out}}\) do not helpNote that algorithms are allowed to output a randomized \(f_{\mathrm{out}}\) and to output \(f_{\mathrm{out}}\notin\mathcal{H}\). We will show that randomization and improperness of \(f_{\mathrm{out}}\) don't make the problem easier. That is, supposing that the data distribution is \(\mathcal{D}_{i^{*}}\) for some \(i^{*}\in[n]\), finding a (possibly randomized and improper) \(f_{\mathrm{out}}\) is not easier than identifying \(i^{*}\). Since our feature space \(\mathcal{X}\) is finite, we can enumerate all hypotheses not equal to \(2\mathds{1}_{\{i^{*}\}}-1\) and calculate their strategic population loss as follows.

* \(2\mathds{1}_{\emptyset}-1\) predicts all points in \(\mathcal{X}\) by negative and thus \(\mathcal{L}^{\text{str}}(2\mathds{1}_{\emptyset}-1)=1-6\varepsilon\);
* For any \(a\subset\mathcal{X}\) s.t. \(0\in a\), \(2\mathds{1}_{a}-1\) will predict \(0\) as positive and thus will predict any point drawn from \(\mathcal{D}_{i^{*}}\) as positive. Hence \(\mathcal{L}^{\text{str}}(2\mathds{1}_{a}-1)=6\varepsilon\);
* For any \(a\subset[n]\) s.t. \(\exists i\neq i^{*}\), \(i\in a\), we have \(\mathcal{L}^{\text{str}}(2\mathds{1}_{a}-1)\geq 3\varepsilon\). This is due to that when \(y=-1\), the probability of drawing a permutation \(\sigma\) with \(\sigma^{-1}(i)<\sigma^{-1}(i^{*})\) is \(\frac{1}{2}\). In this case, we have \(i\in s_{\sigma,i^{*}}\) and the prediction of \(2\mathds{1}_{a}-1\) is \(+1\).

Under distribution \(\mathcal{D}_{i^{*}}\), if we are able to find a (possibly randomized) \(f_{\mathrm{out}}\) with strategic loss \(\mathcal{L}^{\text{str}}(f_{\mathrm{out}})\leq\varepsilon\), then we have \(\mathcal{L}^{\text{str}}(f_{\mathrm{out}})=\mathbb{E}_{h\sim f_{\mathrm{out}} }\left[\mathcal{L}^{\text{str}}(h)\right]\geq\Pr_{h\sim f_{\mathrm{out}}}(h \neq 2\mathds{1}_{\{i^{*}\}}-1)\cdot 3\varepsilon\). Thus, \(\Pr_{h\sim f_{\mathrm{out}}}(h=2\mathds{1}_{\{i^{*}\}}-1)\geq\frac{2}{3}\) and then, we can identify \(i^{*}\) by checking which realization of \(f_{\mathrm{out}}\) has probability greater than \(\frac{2}{3}\). In the following, we will focus on the sample complexity to identify the target function \(2\mathds{1}_{\{i^{*}\}}-1\) or simply \(i^{*}\). Let \(i_{\mathrm{out}}\) denote the algorithm's answer to question of "what is \(i^{*}\)?".

Smoothing the data distributionFor technical reasons (appearing later in the analysis), we don't want to analyze distribution \(\{\mathcal{D}_{i}|i\in[n]\}\) directly as the probability of \(\Delta_{t}=i^{*}\) is \(0\) when \(f_{t}(i^{*})=+1\). Instead, we consider the mixture of \(\mathcal{D}_{i}\) and another distribution \(\mathcal{D}^{\prime\prime}_{i}\) to make the probability of \(\Delta_{t}=i^{*}\) be a small positive number. More specifically, let \(\mathcal{D}^{\prime}_{i}=(1-p)\mathcal{D}_{i}+p\mathcal{D}^{\prime\prime}_{i}\), where \(\mathcal{D}^{\prime\prime}_{i}\) is defined by drawing \((0,\mathcal{X},+1)\) with probability \(1-6\varepsilon\) and \((0,\{0,i\},-1)\) with probability \(6\varepsilon\). When \(p\) is extremely small, we will never sample from \(\mathcal{D}^{\prime\prime}_{i}\) when time horizon \(T\) is not too large and therefore, the algorithm behaves the same under \(\mathcal{D}^{\prime}_{i}\) and \(\mathcal{D}_{i}\). For any data distribution \(\mathcal{D}\), let \(\mathbf{P}_{\mathcal{D}}\) be the dynamics of \((x_{1},f_{1},\Delta_{1},y_{1},\widehat{y}_{1},\ldots,x_{T},f_{T},\Delta_{T},y_{T },\widehat{y}_{T})\) under \(\mathcal{D}\). According to Lemma 4, by setting \(p=\frac{\varepsilon}{16n^{2}}\), when \(T\leq\frac{n}{\varepsilon}\), we have that for any \(i,j\in[n]\)

\[\left|\mathbf{P}_{\mathcal{D}_{i}}(i_{\mathrm{out}}=j)-\mathbf{P}_{\mathcal{D} ^{\prime}_{i}}(i_{\mathrm{out}}=j)\right|\leq\frac{1}{8}\,. \tag{21}\]

From now on, we only consider distribution \(\mathcal{D}^{\prime}_{i}\) instead of \(\mathcal{D}_{i}\). The readers might have the question that why not using \(\mathcal{D}^{\prime}_{i}\) for construction directly. This is because no hypothesis has zero loss under \(\mathcal{D}^{\prime}_{i}\), and thus \(\mathcal{D}^{\prime}_{i}\) does not satisfy realizability requirement.

Information gain from different choices of \(f_{t}\)In each round of interaction, the learner picks a predictor \(f_{t}\), which can be out of \(\mathcal{H}\). Suppose that the target function is \(2\mathds{1}_{\{i^{*}\}}-1\). Here we enumerate all choices of \(f_{t}\) and discuss how much we can learn from each choice.

* \(f_{t}=2\mathds{1}_{\emptyset}-1\) predicts all points in \(\mathcal{X}\) by negative. No matter what \(i^{*}\) is, we will observe \(\Delta_{t}=x_{t}=0\), \(y_{t}\sim\operatorname{Rad}(1-6\varepsilon)\), \(\widehat{y}_{t}=-1\). They are identically distributed for any \(i^{*}\in[n]\) and thus we cannot tell any information of \(i^{*}\) from this round.
* \(f_{t}=2\mathds{1}_{a_{t}}-1\) for some \(a_{t}\subset\mathcal{X}\) s.t. \(0\in a_{t}\). Then no matter what \(i^{*}\) is, we will observe \(\Delta_{t}=x_{t}=0\), \(y_{t}\sim\operatorname{Rad}(1-6\varepsilon)\), \(\widehat{y}_{t}=+1\). Again, we cannot tell any information of \(i^{*}\) from this round.
* \(f_{t}=2\mathds{1}_{a_{t}}-1\) for some some non-empty \(a_{t}\subset[n]\). For rounds with \(y_{t}=+1\), we have \(x_{t}=0\), \(\widehat{y}_{t}=+1\) and \(\Delta_{t}=\Delta(0,f_{t},\mathcal{X})\sim\operatorname{Unif}(a_{t})\), which still do not depend on \(i^{*}\). For rounds with \(y_{t}=-1\), if the drawn example \((0,\{0\}\cup s,-1)\) satisfies that \(s\cap a_{t}\neq\emptyset\), the we would observe \(\Delta_{t}\in a_{t}\) and \(\widehat{y}_{t}=+1\). At least we could tell that \(\mathds{1}_{\{\Delta_{t}\}}\) is not the target function. Otherwise, we would observe \(\Delta_{t}=x_{t}=0\) and \(\widehat{y}_{t}=-1\).

Therefore, we can only gain some information about \(i^{*}\) at rounds in which \(f_{t}=2\mathds{1}_{a_{t}}-1\) for some non-empty \(a_{t}\subset[n]\) and \(y_{t}=-1\). In such rounds, under distribution \(\mathcal{D}^{\prime}_{i^{*}}\), the distribution of \(\Delta_{t}\) is described as follows. Let \(k_{t}=|a_{t}|\) denote the cardinality of \(a_{t}\). Recall that agent \((0,\{0\}\cup s,-1)\) breaks ties randomly when choosing \(\Delta_{t}\) if there are multiple elements in \(a_{t}\cap s\). Here are two cases: \(i^{*}\in a_{t}\) and \(i^{*}\notin a_{t}\).

1. The case of \(i^{*}\in a_{t}\): With probability \(p\), we are sampling from \(\mathcal{D}^{\prime\prime}_{i^{*}}\) and then \(\Delta_{t}=i^{*}\). With probability \(1-p\), we are sampling from \(\mathcal{D}_{i^{*}}\). Conditional on this, with probability \(\frac{1}{k_{t}}\), we sample an agent \((0,\{0\}\cup s_{\sigma,i^{*}},-1)\) with the permutation \(\sigma\) satisfying that \(\sigma^{-1}(i^{*})<\sigma^{-1}(j)\) for all \(j\in a_{t}\setminus\{i^{*}\}\) and thus, \(\Delta_{t}=0\). With probability \(1-\frac{1}{k_{t}}\), there exists \(j\in a_{t}\setminus\{i^{*}\}\) s.t. \(\sigma^{-1}(j)<\sigma^{-1}(i^{*})\) and \(\Delta_{t}\neq 0\). Since all \(j\in a_{t}\setminus\{i^{*}\}\) are symmetric, we have \(\Pr(\Delta_{t}=j)=(1-p)(1-\frac{1}{k_{t}})\cdot\frac{1}{k_{t}-1}=\frac{1-p}{k _{t}}\). Hence, the distribution of \(\Delta_{t}\) is \[\Delta_{t}=\begin{cases}j&\text{w.p. }\frac{1-p}{k_{t}}\text{ for }j\in a_{t},j \neq i^{*}\\ i^{*}&\text{w.p. }p\\ 0&\text{w.p. }\frac{1-p}{k_{t}}\,.\end{cases}\] We denote this distribution by \(P_{\in}(a_{t},i^{*})\).
2. The case of \(i^{*}\notin a_{t}\): With probability \(p\), we are sampling from \(\mathcal{D}^{\prime\prime}_{i^{*}}\), we have \(\Delta_{t}=x_{t}=0\). With probability \(1-p\), we are sampling from \(\mathcal{D}_{i^{*}}\). Conditional on this, with probability of \(\frac{1}{k_{t}+1}\), \(\sigma^{-1}(i^{*})<\sigma^{-1}(j)\) for all \(j\in a_{t}\) and thus, \(\Delta_{t}=x_{t}=0\). With probability \(1-\frac{1}{k_{t}+1}\) there exists \(j\in a_{t}\) s.t. \(\sigma^{-1}(j)<\sigma^{-1}(i^{*})\) and \(\Delta_{t}\in a_{t}\). Since all \(j\in a_{t}\) are symmetric, we have \(\Pr(\Delta_{t}=j)=(1-p)(1-\frac{1}{k_{t}+1})\cdot\frac{1}{k_{t}}=\frac{1-p}{k _{t}+1}\). Hence the distribution of \(\Delta_{t}\) is \[\Delta_{t}=\begin{cases}j&\text{w.p. }\frac{1-p}{k_{t}+1}\text{ for }j\in a_{t}\\ 0&\text{w.p. }p+\frac{1-p}{k_{t}+1}\,.\end{cases}\] We denote this distribution by \(P_{\notin}(a_{t})\).

To measure the information obtained from \(\Delta_{t}\), we will use the KL divergence of the distribution of \(\Delta_{t}\) under the data distribution \(\mathcal{D}^{\prime}_{i^{*}}\) from that under a benchmark data distribution. We use the average distribution over \(\{\mathcal{D}^{\prime}_{i}|i\in[n]\}\), which is denoted by \(\overline{\mathcal{D}}=\frac{1}{n}\sum_{i\in n}\mathcal{D}^{\prime}_{i}\). The sampling process is equivalent to drawing \(i^{*}\sim\operatorname{Unif}([n])\) first and then sampling from \(\mathcal{D}^{\prime}_{i^{*}}\). Under \(\overline{\mathcal{D}}\), for any \(j\in a_{t}\), we have

\[\Pr(\Delta_{t}=j)= \Pr(i^{*}\in a_{t}\setminus\{j\})\Pr(\Delta_{t}=j|i^{*}\in a_{t} \setminus\{j\})+\Pr(i^{*}=j)\Pr(\Delta_{t}=j|i^{*}=j)\] \[+\Pr(i^{*}\notin a_{t})\Pr(\Delta_{t}=\mathbf{e}_{j}|i^{*}\notin a _{t})\] \[=\frac{k_{t}-1}{n}\cdot\frac{1-p}{k_{t}}+\frac{1}{n}\cdot p+\frac{ n-k_{t}}{n}\cdot\frac{1-p}{k_{t}+1}=\frac{(nk_{t}-1)(1-p)}{nk_{t}(k_{t}+1)}+\frac{p}{n}\,,\]\[\Pr(\Delta_{t}=0) =\Pr(i^{*}\in a_{t})\Pr(\Delta_{t}=0|i^{*}\in a_{t})+\Pr(i^{*}\notin a _{t})\Pr(\Delta_{t}=0|i^{*}\notin a_{t})\] \[=\frac{k_{t}}{n}\cdot\frac{1-p}{k_{t}}+\frac{n-k_{t}}{n}\cdot(p+ \frac{1-p}{k_{t}+1})=\frac{(n+1)(1-p)}{n(k_{t}+1)}+\frac{(n-k_{t})p}{n}\,.\]

Thus, the distribution of \(\Delta_{t}\) under \(\overline{\mathcal{D}}\) is

\[\Delta_{t}=\begin{cases}j&\text{w.p. }\frac{(nk_{t}-1)(1-p)}{nk_{t}(k_{t}+1)} +\frac{p}{n}\text{ for }j\in a_{t}\\ 0&\text{w.p. }\frac{(n+1)(1-p)}{n(k_{t}+1)}+\frac{(n-k_{t})p}{n}\,.\end{cases}\]

We denote this distribution by \(\overline{P}(a_{t})\). Next we will compute the KL divergence of \(P_{\notin}(a_{t})\) and \(P_{\in}(a_{t})\) from \(\overline{P}(a_{t})\). Since \(p=\frac{e}{16n^{2}}\leq\frac{1}{16n^{2}}\), we have \(\frac{(nk_{t}-1)(1-p)}{nk_{t}(k_{t}+1)}+\frac{p}{n}\leq\frac{1-p}{k_{t}+1}\) and \(\frac{(n+1)(1-p)}{n(k_{t}+1)}+\frac{(n-k_{t})p}{n}\leq\frac{1}{k_{t}}+p\). We will also use \(\log(1+x)\leq x\) for \(x\geq 0\) in the following calculation. For any \(i^{*}\in a_{t}\), we have

\[\operatorname{D_{KL}}(\overline{P}(a_{t})\|P_{\in}(a_{t},i^{*}))\] \[= (k_{t}-1)\left(\frac{(nk_{t}-1)(1-p)}{nk_{t}(k_{t}+1)}+\frac{p}{n }\right)\log\left((\frac{(nk_{t}-1)(1-p)}{nk_{t}(k_{t}+1)}+\frac{p}{n})\cdot \frac{k_{t}}{1-p}\right)\] \[+\left(\frac{(nk_{t}-1)(1-p)}{nk_{t}(k_{t}+1)}+\frac{p}{n}\right) \log\left((\frac{(nk_{t}-1)(1-p)}{nk_{t}(k_{t}+1)}+\frac{p}{n})\cdot\frac{1}{p }\right)\] \[+\left(\frac{(n+1)(1-p)}{n(k_{t}+1)}+\frac{(n-k_{t})p}{n}\right) \log\left(\left(\frac{(n+1)(1-p)}{n(k_{t}+1)}+\frac{(n-k_{t})p}{n}\right)\cdot \frac{k_{t}}{1-p}\right)\] \[\leq (k_{t}-1)\left(\frac{(nk_{t}-1)(1-p)}{nk_{t}(k_{t}+1)}+\frac{p}{n }\right)\log(\frac{1-p}{k_{t}+1}\cdot\frac{k_{t}}{1-p})+\frac{1-p}{k_{t}+1} \log(1\cdot\frac{1}{p})\] \[+(\frac{1}{k_{t}}+p)\cdot\log\left(1+pk_{t}\right)\] \[\leq 0+\frac{1}{k_{t}+1}\log(\frac{1}{p})+\frac{2}{k_{t}}\cdot pk_{t} =\frac{1}{k_{t}+1}\log(\frac{1}{p})+2p\,. \tag{22}\]

For \(P_{\notin}(a_{t})\), we have

\[\operatorname{D_{KL}}(\overline{P}(a_{t})\|P_{\notin}(a_{t}))\] \[= k_{t}\left(\frac{(nk_{t}-1)(1-p)}{nk_{t}(k_{t}+1)}+\frac{p}{n} \right)\log\left((\frac{(nk_{t}-1)(1-p)}{nk_{t}(k_{t}+1)}+\frac{p}{n})\cdot \frac{k_{t}+1}{1-p}\right)\] \[+\left(\frac{(n+1)(1-p)}{n(k_{t}+1)}+\frac{(n-k_{t})p}{n}\right) \log\left(\left(\frac{(n+1)(1-p)}{n(k_{t}+1)}+\frac{(n-k_{t})p}{n}\right) \cdot\frac{1}{p+\frac{1-p}{k_{t}+1}}\right)\] \[\leq k_{t}\left(\frac{(nk_{t}-1)(1-p)}{nk_{t}(k_{t}+1)}+\frac{p}{n} \right)\log(\frac{1-p}{k_{t}+1}\cdot\frac{k_{t}+1}{1-p})\] \[+(\frac{1}{k_{t}}+p)\log\left(\left(\frac{(n+1)(1-p)}{n(k_{t}+1) }+\frac{(n-k_{t})p}{n}\right)\cdot\frac{1}{p+\frac{1-p}{k_{t}+1}}\right)\] \[= 0+(\frac{1}{k_{t}}+p)\log(1+\frac{1-p(k_{t}^{2}+k_{t}+1)}{n(1+k_ {t}p)})\] \[\leq (\frac{1}{k_{t}}+p)\frac{1}{n(1+k_{t}p)}=\frac{1}{nk_{t}}\,. \tag{23}\]

Lower bound of the informationNow we adopt the similar framework used in the proofs of Theorem 7 and 8. For notation simplicity, for all \(i\in[n]\), let \(\mathbf{P}_{i}\) denote the dynamics of \((x_{1},f_{1},\Delta_{1},y_{1},\widehat{y}_{1},\ldots,x_{T},f_{T},\Delta_{T},y_{T },\widehat{y}_{T})\) under \(\mathcal{D}^{\prime}_{i}\) and and \(\overline{\mathbf{P}}\) denote the dynamics under \(\overline{\mathcal{D}}\). Let \(B_{t}\) denote the event of \(\{f_{t}=2\mathds{1}_{a_{t}}-1\) for some non-empty \(a_{t}\subset[n]\}\). As discussed before, for any \(a_{t}\), conditional on \(\neg B_{t}\) or \(y_{t}=+1\), \((x_{t},\Delta_{t},y_{t},\widehat{y}_{t})\) are identical in all \(\{\mathcal{D}^{\prime}_{i}|i\in[n]\}\), and therefore, also identical in \(\overline{\mathcal{D}}\). We can only obtain information at rounds when \(B_{t}\wedge(y_{t}=-1)\) occurs.

[MISSING_PAGE_EMPTY:33]