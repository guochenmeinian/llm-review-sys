# The Limits of Transfer Reinforcement Learning with Latent Low-rank Structure

 Tyler Sam

Cornell University

tjs355@cornell.edu &Yudong Chen

University of Wisconsin-Madison

yudong.chen@wisc.edu &Christina Lee Yu

Cornell University

cleeyu@cornell.edu

###### Abstract

Many reinforcement learning (RL) algorithms are too costly to use in practice due to the large sizes \(S,A\) of the problem's state and action space. To resolve this issue, we study transfer RL with latent low rank structure. We consider the problem of transferring a latent low rank representation when the source and target MDPs have transition kernels with Tucker rank \((S,d,A)\), \((S,S,d),(d,S,A)\), or \((d,d,d)\). In each setting, we introduce the transfer-ability coefficient \(\alpha\) that measures the difficulty of representational transfer. Our algorithm learns latent representations in each source MDP and then exploits the linear structure to remove the dependence on \(S,A\), or \(SA\) in the target MDP regret bound. We complement our positive results with information theoretic lower bounds that show our algorithms (excluding the (\(d,d,d\)) setting) are minimax-optimal with respect to \(\alpha\).

## 1 Introduction

Recently, reinforcement learning (RL) algorithms have exhibited great success on a variety of problems, e.g., video games [31], robotics [21], etc, that require a series of decisions over time. These algorithms can be used on any problem that can be modeled as a Markov decision process (MDP) with state space cardinality \(S\), action space cardinality \(A\), and horizon \(H\). However, RL algorithms' practical application is constrained due to the large amounts of data and computational resources required to train models that outperform heuristic approaches. This limitation arises due to the inefficient scaling of RL algorithms with respect to the size of the state and action spaces. In an online setting where rewards are incurred during the process of learning, we may want to minimize the regret, or the difference between the reward of an optimal policy and the reward collected by the learner. In the finite-horizon online episodic MDP setting with \(K\) episodes, any algorithm must incur regret of at least \(\tilde{\Omega}(\sqrt{SAH^{3}K})\)[18]. This regret bound is often too large in practice as many problems modeled as MDPs have large state and action spaces. For example, in the Atari games [25], the state space is the set of all images as a vector of pixel values.

One way to remove the dependence on the state or action space in RL algorithms is to leverage existing data or models from similar problems through transfer learning. In the transfer learning setting for MDPs, the learner can interact with or query from multiple source MDPs, which the learner can utilize at low-cost to improve their performance in the target MDP [27]. The efficacy of the transfer learning approach depends both on the similarity between the source and the target MDPs, and how information is utilized when transferring knowledge between the source and target.

As feature learning in MDPs is difficult and expensive, reusing learned low rank representations can greatly improve the performance on new problems. Thus, in this work, we consider the setting when the source and target MDPs exhibit latent low rank structure, and the transfer RL task is to learn latent low rank representation of the state, action, or state-action pair from the source MDPs, and subsequently use it to improve learning on the target MDP. If the learned representations were perfect, then one could remove all dependence on the state space or action space in the learning task on thetarget MDP. This approach relies on a critical assumption that the transition kernels of the source and target MDPs have low Tucker rank when viewed as an \(S\)-by-\(S\)-by-\(A\) tensor.

The work [4] studied this problem when the transition kernel is low rank along the first mode, i.e., with Tucker rank \((d,S,A)\) where \(d<\min\{S,A\}\); this is also referred to as the Low Rank MDP model. We also consider this setting, as well as the complementary settings in which the transition kernel has low Tucker rank along the second or third mode, i.e., Tucker rank \((S,d,A)\) or \((S,S,d)\). In addition, we study the \((d,d,d)\) Tucker rank setting, where we can further exploit the low rank structure to improve performance in the target problem. (As we elaborate later, up to a factor of \(d\), the \((d,d,d)\) setting subsumes the remaining settings with Tucker ranks \((d,d,A)\), \((S,d,d)\) and \((d,S,d)\).)

The different modes are not interchangeable algorithmically as there is a directionality due to the time of the transition between the current and future state. Additionally, the algorithm in [4] is not computationally tractable as it utilizes an optimization oracle which may not be practical. They assume that the latent representations are within a known finite function class. Our work proposes a computationally efficient algorithm that can handle latent low rank structure on the transition kernel along any of the three different modes.

As the success of a transfer learning approach depends on the similarity between the source and target problems, we introduce the _transfer-ability coefficient_\(\alpha\), which quantifies the similarity between the source and target problems under our low rank assumptions. This coefficient, generalizing the task-relatedness in [4], captures the unique challenge in the transfer reinforcement learning setting.

Our Contributions:We propose new computationally efficient transfer RL algorithms that admit the desired efficient regret bounds under all low Tucker rank settings \((S,d,A)\), \((S,S,d),(d,S,A)\), and \((d,d,d)\). With enough samples from the source MDPs, we remove the dependence on \(S,A,\) or \(SA\) in the regret bound on the target MDP. In addition, we introduce the transfer-ability coefficient \(\alpha\) that measures the ease of transferring a latent representation from the source MDPs to target MDP. We establish information theoretic lower bounds showing that our algorithms are optimal with respect to \(\alpha\) (excluding the \((d,d,d)\) Tucker rank setting). In particular, in the \((d,S,A)\) Tucker rank setting, we achieve the optimal dependence of \(\alpha^{2}\) while [4] is sub-optimal with a dependence on \(\alpha^{5}\).

Table 1 summarizes our main theoretical results, suppressing the dependence on common matrix estimation terms, and compares them with the results from [4].1 In the appendix, we further consider the setting where one has access to generative models in both the source and target problems, and show that the benefits of transfer learning go beyond alleviating exploration burden.

Footnote 1: Since \(\bar{\alpha}\geq\alpha\), moving \(\bar{\alpha}\) into the source sample complexity yields a sample complexity that scales with \(\alpha^{5}\). \(\Phi\) and \(\Gamma\) refer to the function classes that contain the true \(\phi\) and \(\mu\), respectively. As we make no assumptions on the function class of \(\phi\), we replace \(\log(|\Phi||\Gamma|)\) with \(|S||A|\).

## 2 Related Work

**RL in the \((d,S,A)\) Tucker rank Setting:** There exists an extensive literature on Linear MDPs and Low-rank MDPs, which correspond to the \((d,S,A)\) Tucker rank setting. Linear MDPs assume

\begin{table}
\begin{tabular}{c|c c c} \hline \hline  & Tucker Rank & Source Sample Complexity & Target Regret Bound \\ \hline Theorem 2 & \((S,S,d)\) & \(\tilde{O}\left(d^{4}(1+A/S)M^{2}H^{4}\alpha^{2}T\right)\) & \(\tilde{O}\left(\sqrt{(dMH)^{3}ST}\right)\) \\ Theorem 10 & \((S,d,A)\) & \(\tilde{O}\left(d^{4}(S/A+1)M^{2}H^{4}\alpha^{2}T\right)\) & \(\tilde{O}\left(\sqrt{(dMH)^{3}AT}\right)\) \\ Theorem 14 & \((d,S,A)\) & \(\tilde{O}\left(\alpha^{2}dM^{2} TSA\right)\) & \(\tilde{O}\left(\sqrt{(dMH)^{3}T}\right)\) \\ Theorem 15 & \((d,d,d)\) & \(\tilde{O}\left(d^{10}(S+A)M^{2}H^{4}\alpha^{4}T\right)\) & \(\tilde{O}\left(\sqrt{d^{6}M^{6}H^{3}T}\right)\) \\ \hline Theorem 3.1 [4] & \((d,S,A)\) & \(\tilde{O}(\alpha^{5}SA^{5}H^{7}d^{5}M^{2}T)\) & \(\tilde{O}(\sqrt{d^{3}H^{4}T})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Theoretical guarantees of our algorithms alongside results from literature in the different Tucker rank settings. See Table 3 for the regret bounds of algorithms in each Tucker rank setting with known latent representations.

that the dynamics are linear with respect to a known feature mapping \(\phi\). This allows one to remove all dependence on the size of the state and action spaces and replace it with the rank \(d\) of the latent space [17; 37; 36; 34]. The work [13] proposes an algorithm that admits a regret bound of \(\tilde{O}(\sqrt{d^{2}H^{2}T})\), matching the known lower bounds. Our algorithm in the target phase modifies existing linear MDP algorithms to take advantage of the knowledge learned in the source phase. Low Rank MDPs impose the same low-rank structure as linear MDPs but assume that \(\phi\) is unknown. To avoid dependence on the size of the state space, many works in this setting construct algorithms that utilize a computationally inefficient oracle to admit sample complexity or regret bounds that scale with the size of the action space and the log of the size of the function class the true low rank representation [3; 33; 26; 40]. [32] incurs sample complexity on \(SA\) instead of the size of the function class to learn near-optimal policies in reward free low rank MDPs.

**Transfer RL:** As feature learning in low rank MDPs is difficult, [4; 10; 23; 15] study representational transfer in low rank MDPs. The work most closely related to ours is [4], which performs reward-free exploration in the source problem to learn a sufficient representation to use in the target problem in the \((d,S,A)\) Tucker rank setting. They propose a task relatedness coefficient that captures the existing representational transfer RL settings [10]. This coefficient is similar to our transfer-ability coefficient but differs as our low rank assumption is imposed on different modes of the probability transition tensor. Their algorithm admits a source sample complexity bound of \(\tilde{O}(A^{4}\alpha^{3}d^{5}H^{7}K^{2}T\log(|\Phi||\Gamma|))\) with a target regret bound of \(\tilde{O}(\sqrt{d^{3}\alpha^{2}H^{4}T})\). Other transfer RL works [10; 23] require reach-ability assumptions to learn the latent representations in the source phase. Similar to transfer RL, reward free RL studies the problem of giving the learner access to only the transition kernel in the first phase, and in the second phase the learner must output a good policy when given multiple reward functions [19; 39; 11; 32]. While reward free RL is similar to our setting, enabling the dynamics to differ between the source and target phase greatly increases the difficulty and changes the objective in the source problem, i.e., learning a good latent representation instead of collecting trajectories that sufficiently explore the state space.

**RL in the \((S,d,A)\) or \((S,S,d)\) Tucker Rank Setting:** The work [30] introduces the RL setting in which the transition kernel has Tucker rank \((S,d,A)\) or \((S,S,d)\). Assuming access to a generative model, their algorithms learn near-optimal polices with sample complexities that scale with \(S+A\), thereby circumventing the \(SA\) lower bound for tabular reinforcement learning [6]. The work [35] consider the same low Tucker rank assumption on the dynamics but in the offline RL setting. The works [38; 29; 28] empirically show the benefits of combining matrix/tensor estimation with RL algorithms on common stochastic control tasks. [16] consider the low-rank bandits setting and use a similar algorithm to ours by estimating the singular subspaces of the reward matrix to improve the regret bound's dependence on the dimension of the problem.

## 3 Preliminaries

We consider the transfer reinforcement learning setting introduced by [4], in which the learner interacts with \(M\) finite-horizon source MDPs and one finite horizon episodic target MDP. All MDPs share the same discrete state space \(\mathcal{S}\) with cardinality \(S\), discrete action space \(\mathcal{A}\) with cardinality \(A\), and finite horizon \(H\in\mathbb{Z}_{+}\).2 Let \(r=\{r_{h}\}_{h\in[H]}\) be the deterministic unknown reward function of the target MDP, where \(r_{h}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\). For \(m\in[M]\), let \(r_{m}=\{r_{m,h}\}_{h\in[H]}\) be the deterministic unknown reward function for the \(m\)th source MDP, where \(r_{m,h}:\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\).3 Let \(\mathcal{P}=\{P_{h}\}\) be the transition kernel for the target MDP, where \(P_{h}(s^{\prime}|s,a)\) is the probability of transitioning to state \(s^{\prime}\) from state-action pair \((s,a)\) at time step \(h\). Similarly, for \(m\in[M]\), \(\mathcal{P}_{m}\) is the transition kernel for \(m\)th source MDP and defined in the same way as \(\mathcal{P}\). An agent's policy has the form \(\pi=\{\pi_{h}\}_{h\in[H]},\pi_{h}:\mathcal{S}\rightarrow\Delta(\mathcal{A})\), where \(\pi_{h}(a|s)\) is the probability that the agent chooses action \(a\) at state \(s\) and time step \(h\).

Footnote 2: It is possible to relax the assumption of sharing the same state/action spaces. In particular, depending on the Tucker rank of the transition kernel, we only require the source and target MDPs share only one of the spaces.

Footnote 3: Our results can easily be extended to stochastic reward functions.

The value function and action-value (\(Q\)) function of a policy \(\pi\) are the expected reward of following \(\pi\) given a starting state and starting action, respectively, beginning at time step \(h\). We define these as

\[V_{h}^{\pi}(s)\coloneqq\mathbb{E}\Big{[}\sum_{t=h}^{H}r_{t}(s_{t},\pi_{t}(s_{ t}))|s_{h}=s\Big{]},\quad Q_{h}^{\pi}(s,a)\coloneqq\mathbb{E}\Big{[}\sum_{t=h}^{H}r_{t}(s_{ t},a_{t})|s_{h}=s,a_{h}=a\Big{]}\]where \(a_{t}\sim\pi_{t}(s_{t}),s_{t+1}\sim P_{t}(\cdot|s_{t},a_{t})\). The optimal value function and action value function are \(V^{*}_{h}(s)\coloneqq\sup_{\pi}V^{n}_{h}(s)\) and \(Q^{*}_{h}(s,a)\coloneqq\sup_{\pi}Q^{n}_{h}(s,a)\) for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\). They satisfy the Bellman equations \(V^{*}_{h}(s)=\max_{a\in\mathcal{A}}Q^{*}_{h}(s,a),Q^{*}_{h}(s,a)=r_{h}(s,a)+E_{s ^{\prime}\sim P_{h}(\cdot|s,a)}[V^{*}_{h+1}(s^{\prime})]\) for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\). We define an \(\epsilon\)-optimal policy \(\pi\) for \(\epsilon>0\) as any policy that satisfies \(V^{*}_{h}(s)-V^{*}_{h}(s)\leq\epsilon\) for all \(s\in\mathcal{S},h\in[H]\). Similarly, \(Q\) is \(\epsilon\)-optimal if \(Q^{*}_{h}(s,a)-Q_{h}(s,a)<\epsilon\) for all \((s,a,h)\in\mathcal{S}\times\mathcal{A}\times[H]\). Also, we use the shorthand \(P_{h}V(s,a)=\mathbb{E}_{s^{\prime}\sim P_{h}(\cdot|s,a)}[V(s^{\prime})]\).

In the transfer RL problem, the learner first interacts with the \(M\) source MDPs without access to the target MDP. In this phase, called the source phase, the learner is given access to a generative model in each of the source MDPs. The generative model takes in a state-action pair \((s,a)\) and time step \(h\) and outputs \(r_{h}(s,a)\) and an independent sample from \(P_{h}(\cdot|s,a)\)[20]. Access to a generative model in the source phase is necessary; in particular, it has been shown in [4, Theorem 4.1] that without access to a generative model in the source phase, one cannot learn a near-optimal policy in the target phase using the learned representation. Then, in the next phase, called the target phase, the learner loses access to the source MDPs and interacts only with the target MDP in a standard online model, without access to a generative model. The performance of the learner is evaluated by two metrics: i) the regret in the target MDP, i.e., \(Regret(T)=\sum_{k\in[K]}V^{*}_{1}(s)-V^{\pi^{k}}_{1}(s)\), where \(\pi^{k}\) is the learner's policy in episode \(k\), and ii) the number of samples taken in the source phase.

### The Tucker Rank of a Tensor

For the reader's convenience, we first recall the standard definition of the Tucker rank of a tensor.

**Definition 1** (Tucker Rank [24]).: _A tensor \(M\in\mathbb{R}^{n_{1}\times n_{2}\times n_{3}}\) has Tucker rank \((d_{1},d_{2},d_{3})\) if \((d_{1},d_{2},d_{3})\) is the smallest such that there exists a core tensor \(X\in\mathbb{R}^{d_{1}\times d_{2}\times d_{3}}\) and matrices \(G_{i}\in\mathbb{R}^{n_{i}\times d_{i}}\) for \(i\in[3]\) with orthonormal columns such that_

\[M(a,b,c)=\sum_{i\in[d_{1}],j\in[d_{2}],k\in[d_{3}]}X(i,j,k)G_{1}(a,i)G_{2}(b,j )G_{3}(c,k).\]

We note in passing that all our sample complexity and regret bounds in Table 1 remain valid when \(d\) is an upper bound on the exact rank. In standard MDPs, the transition kernel can have Tucker rank \((S,S,A)\) as there is no low rank structure imposed on the tensor. In this work, we investigate the benefits of assuming that the transition kernel is low rank along each mode/dimension of the transition kernel, which corresponds to the \((S,d,A),(S,S,d)\), and \((d,S,A)\) Tucker rank settings. To illustrate the difference in these three settings, we consider \(d=1\) and present the corresponding structure of the transition kernel in Equations (1), (2), and (3), respectively:

\[\text{Tucker rank }(S,1,A)\text{: }\qquad P(s^{\prime}|s,a)=\mu(s) \phi(s^{\prime},a),\] (1) \[\text{Tucker rank }(S,S,1)\text{: }\qquad P(s^{\prime}|s,a)=\mu(a) \phi(s^{\prime},s),\] (2) \[\text{Tucker rank }(1,S,A)\text{: }\qquad P(s^{\prime}|s,a)=\mu(s^{\prime}) \phi(s,a),\] (3)

for some functions \(\mu,\phi\) defined on the appropriate domains. Figure 1 pictorially displays the \((S,S,d)\) Tucker rank assumption on the transition kernel. While the three settings may seem similar, the \((S,d,A)\) and \((S,S,d)\) Tucker rank settings differ significantly from the \((d,S,A)\) Tucker rank setting (i.e., Low Rank MDP). In particular, the \((d,S,A)\) setting assumes low rank structure across time, i.e., the current state and action vs. the next state, while the other Tucker rank assumptions impose low rank structure between the current state and action. These differences are significant as each setting requires different algorithms in the target problem and construction for the lower bounds.

Additionally, we analyze the \((d,d,d)\) Tucker rank setting, where the transition kernel fully factorizes, in which case the next state, current state, and action all have separate low rank representations. This assumption allows us to further exploit the low rank structure to improve the regret bound in the target phase when compared to the \((S,d,A)\) and \((S,S,d)\) Tucker rank settings. Note that this \((d,d,d)\) Tucker rank assumption is equivalent to imposing low rank along any two modes of the transition kernel, up to factors of \(d\) in the sample complexity and regret bounds. For example, for an \((S,d,d)\) Tucker rank transition kernel \(P\), one can factor the \(S\times d\times d\) core tensor into a \(S\times d\) orthonormal matrix and \(d^{2}\times d\times d\) core tensor, which implies \(P\) has Tucker rank \((d^{2},d,d)\).

While the sample complexity and regret bound of each algorithm differ in each Tucker rank setting, one should choose which algorithm to use based on the low rank structure inherent in their problem. For example, in the \((S,S,d)\) Tucker rank setting, one assumes that each action has a low rank representation. One potential application of this setting is a personal movie recommender system. Specifically, the states are user's different states of mind while each action chooses a movie from a large collection. As movies in the same genre evoke similar responses, each action can be mapped to a lower dimensional space with the axis being each genre. In contrast, each state of mind is distinct. In the \((S,d,A)\) Tucker rank setting, one assumes that the state one transitions from has a low rank representation. The \((d,d,d)\) Tucker rank setting combines the two previous assumptions and asserts that current states and actions have separate low dimensional representations while low rank MDPs with \((d,S,A)\) Tucker rank assume that each state-action pair has a joint low rank representation. For sake of brevity, we will only present our assumption and algorithm for the \((S,S,d)\) Tucker rank setting in the main body of this work and defer the other settings to the appendix.

### Low Tucker Rank MDPs

We first define the incoherence \(\mu\) of a matrix, which is used in the regularity conditions of our Tucker rank assumption.

**Definition 2** (Incoherence).: _Let \(X\in\mathbb{R}^{m\times n}\) have rank \(d\) with singular value decomposition \(X=U\Sigma V^{\top}\) with \(U\in\mathbb{R}^{m\times d},V\in\mathbb{R}^{n\times d}\). Then, \(X\) is \(\mu\)-incoherent if for all \(i\in[m],j\in[n]\), \(\|U(i,:)\|_{2}\leq\sqrt{d\mu/m}\) and \(\|V(j,:)\|_{2}\leq\sqrt{d\mu/n}\)._

A small \(\mu\) guarantees that the signal in \(U\) and \(V\) is sufficiently spread out among the rows in those matrices. The notion of incoherence is pervasive in the low-rank estimation literature [7; 8].

Introduced in [30], we assume that reward functions are low rank and the transition kernels have low Tucker rank in each of the source and target MDPs. The main benefit of Assumption 1 is that for any value function \(V\), the Bellman update of any value function, \(Q_{h}=r_{h}+P_{h}V\), is a low rank matrix due to Proposition 4[30].

**Assumption 1**.: _In each of the \(M\) source MDPs, the reward functions have rank \(d\), and the transition kernels have Tucker rank \((S,S,d)\). The target MDP has reward function with rank \(d^{\prime}\) and transition kernels with Tucker rank \((S,S,d^{\prime})\) where \(d^{\prime}\leq dM\). Thus, there exists \(S\times S\times d\) tensors \(U_{m,h},\)\(S\times S\times d^{\prime}\) tensors \(U_{h}\)s, \(A\times d\)\(\mu\)-incoherent matrices \(G_{m,h}\) with orthonormal columns, \(A\times d^{\prime}\)\(\mu\)-incoherent matrices \(G_{h}\) with orthonormal columns, and \(S\times d\) matrices \(W_{m,h},S\times d^{\prime}\) matrices \(W_{h}\) such that_

\[P_{m,h}(s^{\prime}|s,a)=\sum_{i\in[d]}G_{m,h}(a,i)U_{m,h}(s^{ \prime},s,i), r_{m,h}(s,a)=\sum_{i\in[d]}G_{m,h}(a,i)W_{m,h}(s,i),\] \[P_{h}(s^{\prime}|s,a)=\sum_{i\in[d^{\prime}]}G_{h}(a,i)U_{h}(s^{ \prime},s,i), r_{h}(s,a)=\sum_{i\in[d^{\prime}]}G_{h}(a,i)W_{h}(s,i)\]

_where \(\|\sum_{s^{\prime}\in S}g(s^{\prime})U_{m,h}(s^{\prime},s,:)\|_{2}.\|\sum_{s ^{\prime}\in S}g(s^{\prime})U_{h}(s^{\prime},s,:)\|_{2},\|W_{h}(s)\|_{2},\|W_ {m,h}(s)\|_{2}\leq\sqrt{A/(d\mu)}\) for all \(s\in\mathcal{S}\), \(a\in\mathcal{A}\), \(h\in[H]\), and \(m\in[M]\) for any function \(g:S\rightarrow[0,1]\)._

The latent representations of the reward function and transition kernel are not unique under our assumption. Figure 2 visualizes the low rank structure Assumption 1 imposes on the transition kernel. While Assumption 1 states that the source MDPs and target MDPs have latent low rank structure, the assumption does not guarantee that low rank representations are similar. Additionally, we allow target MDP to be more complex than a single source MDP (at most Tucker rank \((S,S,dM)\) compared to Tucker rank \((S,S,d)\), respectively). Thus, to allow for transfer learning, we assume that for each step \(h\in[H]\), the space spanned by the target latent factors is a subset of the space spanned by the latent factors from all the source MDPs.

**Assumption 2**.: _Suppose Assumption 1 holds. The target MDP latent factors \(G_{h}\) and source MDP latent factors \(G_{m,h}\) satisfy for all \(h\in[H]\), \(\mathrm{Span}(\{G_{h}(:,i)\}_{i\in[d^{\prime}]})\subseteq\mathrm{Span}(\{G_{m,h }(:,i)\}_{i\in[d],m\in[M]})\)._

Under Assumption 2, it is possible that no single source MDP captures the target MDP, but the union of all source MDP does. It is also possible that the source MDPs span a strictly larger subspace than the target MDP.

Thanks to the above assumptions, by estimating the latent features \(\{G_{m,h}\}\) of the source MDPs, we can construct an approximation of the target features \(\{G_{h}\}\)**without interacting** with the target MDP, which is essential to transfer learning. Assumption 1 only upper bounds the rank of \(Q^{*}_{m,h}\) by \(d\). However, to learn each source latent factor, one needs to obtain a good estimate of a rank \(d\) matrix with at least one large entry. Therefore, we assume that the optimal \(Q\)-functions in each source MDP are full rank with the largest entry lower bounded by a constant.

**Assumption 3**.: _For each \(m\in[M],h\in[H]\), the optimal \(Q\)-function of the \(m\)-th source MDP at step \(h\) satisfies \(rank(Q^{*}_{m,h})=d\) and \(\|Q^{*}_{m,h}\|_{\infty}\geq C\) for some \(C\in\mathbb{R}^{+}\)._

This assumption allows one to learn the near-optimal \(Q\)-functions from noisy samples of the source MDPs. Assumption 3 and its variants are common in the literature of noisy low-rank estimation [2; 9]. Without the above assumption (the rank of \(Q^{*}\) is strictly less than \(d\)), then one cannot learn every latent factor \(G_{m,h}(:,i)\) since it may have a zero singular value, and thus using the approximate feature mapping would result in regret linear in \(T\). Similarly, \(\|Q^{*}\|_{\infty}\geq C\) ensures that some entries of \(Q^{*}\) are sufficiently large and prevents the noise from dominating the low rank signal that one learns in the source phase.

## 4 Transfer-ability

Our transfer-ability coefficient is motivated by the following observation. Under Assumption 2, we can construct each target latent factor \(G_{h}\) with a linear combination of the source latent factors \(G_{m,h}\). The magnitudes of the coefficients correspond to the difficulty in estimating the target latent factor because the estimation error of \(G_{m,h}\) is amplified by its coefficient. Thus, when these coefficients are too large, transfer learning is essentially impossible; one example is when the source latent factors are almost identical or parallel to each other while the target latent factor is orthogonal to the source latent factors (see Figure 2).

However, these coefficients are not unique and are a function of the source and target latent factors. Furthermore, as the source MDPs latent factors of \(Q^{*}_{m,h}\) and target latent factors \(\{G_{h}\}\) are not unique, each choice of latent factor results in a different set of coefficients. Thus, we introduce \(\mathcal{B}_{h}\) as the set containing all such coefficients given the subspaces from the target and source MDPs. Let \(\mathcal{G}_{S,h}\) be the set of all \(\mathbb{R}^{S\times d}\) orthonormal latent factor matrices with columns that span the column space of \(Q^{*}_{m,h}\) for all \(m\in[M]\), and let \(\mathcal{G}_{T,h}\) be the set of all \(\mathbb{R}^{S\times d^{\prime}}\) orthonormal latent factor matrices with columns that span the column space of \(P_{h}(s^{\prime})\) for all \(s^{\prime}\in\mathcal{S}\). Then, we define the set \(\mathcal{B}_{h}(\mathcal{G}_{T,h},\mathcal{G}_{S,h})\) to contain all such coefficients, i.e., \(B_{h}\in\mathbb{R}^{d^{\prime},d,M}\) and \(B_{h}\in\mathcal{B}_{h}(\mathcal{G}_{T,h},\mathcal{G}_{S,h})\) if there exists \(G_{h}\in\mathcal{G}_{T,h},G_{m,h}\in\mathcal{G}_{S,h}\) such that for all \(i\in[d^{\prime}],h\in[H]\), \(G_{h}(\cdot,i)=\sum_{j\in[d],m\in[M]}B_{h}(i,j,m)G_{m,h}(\cdot,j)\). Now, we define \(\alpha\), which precisely measures the challenge involved in transferring the latent representation.

**Definition 3** (Transfer-ability Coefficient).: _Given a transfer RL problem that satisfies Assumptions 1, 2, and 3, with the definition of \(\mathcal{B}_{h}\) above, we define \(\alpha\) as_

\[\alpha\coloneqq\max_{h\in[H]}\min_{B\in\mathcal{B}_{h}(\mathcal{G}_{T,h}, \mathcal{G}_{S,h})}\max_{i\in[d^{\prime}],j\in[d],m\in[M]}|B(i,j,m)|.\]

Note that the minimum is taken over all latent factor matrices. When there is only one source MDP, \(\alpha\) is small, i.e., less than or equal to one, because the columns of \(G\) form an orthonormal basis of the space containing the target latent factor. In the best case, \(\alpha\) is \(1/(dM)\) (see Appendix D). On the other hand, when there are multiple source MDPs, \(\alpha\) can be arbitrarily large as in Figure 2; in this case, the estimation error of \(G_{1}\) and \(G_{2}\) are amplified by \(\alpha\) when estimating \(G_{T}\) using \(G_{1}\) and \(G_{2}\). We remark that adding more source MDPs or increasing the rank of \(Q^{*}_{m,h}\) will not increase \(\alpha\) as one can use the same set of coefficients without the additional source MDP or larger subspace of \(Q^{*}_{m,h}\). See Appendix D for more discussion on \(\alpha\).

Our transfer-ability coefficient is similar to the quantity \(\alpha_{\max}\) defined in [4]. While these terms both quantify the similarity between the source and target MDPs, in our setting, we allow for the source MDPs' state latent factors, \(\{G_{m,h}\}\), to differ from target MDP's state latent factors while [4] assumes that all MDPs share the same latent representation \(\phi\). Our setting is more general than the one in [4] even when assuming the same Tucker rank assumption.

### Information Theoretic Lower Bound

To formalize the importance of \(\alpha\), we prove a lower bound that shows that a dependence on \(\alpha\) in the sample complexity of the source phase is necessary to benefit from transfer learning. Before proving our main result, we first present an intermediate lower bound to provide intuition on how \(\alpha\) measures the difficulty in transferring a learned representation.

**Theorem 1**.: _There exist two transfer RL instances such that (i) they satisfy Assumptions 1 and 2, (ii) they cannot be distinguished without observing \(\Omega(\alpha^{2})\) samples in the source phase, and (3) they have target action latent features that are orthogonal to each other._

Learning a latent representation \(G\) that is orthogonal to the true representation is no better than randomly guessing a representation \(G^{\prime}\) in the source phase; using either \(G\) or \(G^{\prime}\) in the target phase incurs regret linear in \(T\). Thus, Theorem 1 states that without observing \(\Omega(\alpha^{2})\) samples in the source MDPs, one cannot learn a good policy in the target phase with \(G\) or \(G^{\prime}\), and one should disregard the information from the source MDPs and use a non-transfer learning RL algorithm to achieve regret bound that scales with \(\sqrt{T}\). Therefore, \(\Omega(\alpha^{2})\) samples are required to benefit from transfer learning.

To prove Theorem 1, we construct two \((S,S,1)\) transfer RL problems that satisfies Assumption 2 with similar source action latent representations \(G^{i}_{m,h}\in\mathcal{G}\) but orthogonal target action latent representations \(G^{i}_{T}\) for \(i\in\{1,2\}\). To avoid learning the incorrect representation, one must identify the transfer RL problem by differentiating the \(Q^{*}\)s. By construction, the \(Q^{*}\)s for the two transfer RL problems differ by at most \(1/\alpha\) entrywise, so one must observe \(\Omega(\alpha^{2})\) samples in the source problem from standard hypothesis testing lower bounds [5]. Observing less samples in the source problem results in an estimate orthogonal to the true target representation with constant probability. See Appendix E for the formal proof. One can easily prove a similar result in the \((S,d,A)\) and \((d,S,A)\) Tucker rank setting with similar constructions by switching the states and actions (see Appendices F and G).

## 5 Algorithm

We now present our algorithm that achieves the optimal dependence on \(\alpha\). In the source phase, our algorithm can use any algorithm that learns \(\epsilon\)-optimal \(Q\) functions on each source MDP. As the error bound on the estimated feature representation depends on the incoherence of \(Q^{*}_{m,h}\), we use LR-EVI, which combines empirical value iteration with low rank matrix estimation [30]. After obtaining estimates \(\bar{Q}_{m,h}\) of \(Q^{*}_{m,h}\), our algorithm computes the singular value decompositions (SVDs) of each \(\bar{Q}_{m,h}\) to construct the latent feature representation \(\tilde{G}\) by scaling and concatenating the singular vectors. Then, in the target phase, our algorithm deploys a modification of LSVI-UCB [17] adapted to our Tucker rank setting called LSVI-UCB-(S, S, d) using the \(\tilde{G}\). For ease of notation, we define \(T^{s}_{k,h}=\{k^{\prime}\in[k]|s^{k}_{h}=s\}\), which is the set of episodes before \(k\), in which one was at \(s\) at time step \(h\).

```
1:\(\{N_{h}\}_{h\in[H]}\)
2:for\(m\in[M]\)do
3: Run LR-EVI(\(\{N_{h}\}_{h\in[H]}\)) on source MDP \(m\) to obtain \(\bar{Q}_{m,h}\).
4: Compute the singular value decomposition of \(\bar{Q}_{m,h}=\hat{F}_{m,h}\hat{\Sigma}_{m,h}\hat{G}_{m,h}^{\top}\).
5: Compute feature mapping \(\hat{\tilde{G}}=\{\hat{\tilde{G}}_{h}\}_{h\in[H]}\) with \(\hat{\tilde{G}}_{h}=\sqrt{\frac{A}{d\mu}}\left[\hat{G}_{1,h}\quad\ldots\quad \hat{G}_{M,h}\right]\). ```

**Algorithm 1** Source Phase

LSVI-UCB-(S, S, d) uses the same mechanisms as LSVI-UCB, but we compute coefficients and Gram matrices for each action. Specifically, we update the weights \(w^{s}_{h}\) with the solution to the regularized least squares problem,

\[w^{s}_{h}\leftarrow\arg\min_{w\in\mathbb{R}^{d}}\sum_{t\in T^{s}_{k,h}}(r_{h}(s,a ^{t}_{h})+\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}(s^{t}_{h+1},a^{\prime})-w^{\top} \tilde{G}_{h}(a^{t}_{h}))^{2}+\lambda\|w\|_{2}^{2},\]

using the reward when \(s\) was observed and add an exploration bonus \(\beta^{s}_{k,h}\sqrt{\hat{\tilde{G}}_{h}(a)^{\top}(\Lambda^{s}_{h})^{-1}\hat{ \tilde{G}}_{h}(a)}\) to our estimate of \(Q^{*}\), which is common in linear MDP/bandits algorithms [17; 1]. Multiplying the latent factors by \(\sqrt{A/(d\mu)}\) to construct \(\hat{\tilde{G}}\) ensures that the feature mapping and coefficients are of similar magnitude, which is crucial in adapting the mechanisms of LSVI-UCB to our setting.

While one may wonder why our modification of LSVI-UCB is necessary, Algorithm 2 improves the regret bound by a factor \(\sqrt{S}\) compared to using any linear MDP algorithm off the shelf; reducing our setting to a linear MDP results in using a feature mapping with dimension \(dS\). Thus, using any linear MDP algorithm results in a target regret bound of \(\tilde{O}(\sqrt{d^{2}S^{2}H^{2}T})\)[41]. However, our algorithm shaves off a factor of \(\sqrt{S}\) as our algorithm computes \(S\) copies of \(d\times d\) dimensional gram matrices in contrast to the \(d^{2}S^{2}\) dimensional gram matrices used in linear MDP algorithms.

```
1:\(\lambda,\beta^{s}_{k,h},\hat{\tilde{G}}\)
2:for\(k\in[K]\)do
3: Receive initial state \(s^{k}_{1}\).
4:for\(h=H,\ldots,1\)do
5:for\(s\in S\)do
6: /* Compute Gram matrix \(\Lambda^{s}_{h}\) for each state \(s\) */
7: Set \(\Lambda^{s}_{h}\leftarrow\sum_{t\in T^{s}_{k-1,h}}\hat{\tilde{G}}_{h}(a^{t}_{h}) \hat{\tilde{G}}_{h}(a^{t}_{h})^{\top}+\lambda\mathbf{I}\).
8: /* Estimate \(w^{s}_{h}\) via regularized least squares */
9: Set \(w^{s}_{h}\leftarrow(\Lambda^{s}_{h})^{-1}\sum_{t\in T^{s}_{k-1,h}}\hat{\tilde {G}}_{h}(a^{t}_{h})\left[r_{h}(s,a^{t}_{h})+\max_{a^{\prime}\in A}Q_{h+1}(s^{ t}_{h+1},a^{\prime})\right]\).
10: /* Estimate \(Q^{*}\) via the low rank structure with optimism */
11: Set \(Q_{h}(\cdot,\cdot)\leftarrow\min\Big{(}H,\langle w^{i}_{h},\hat{G}_{h}(\cdot) \rangle+\beta^{i}_{k,h}\sqrt{\hat{G}_{h}(\cdot)^{\top}(\Lambda^{i}_{h})^{-1} \hat{\tilde{G}}_{h}(\cdot)}\Big{)}\).
12:for\(h\in[H]\)do
13: Take action \(a^{k}_{h}\leftarrow\max_{a\in A}Q_{h}(s^{k}_{h},a)\), and observe \(s^{k}_{h+1}\). ```

**Algorithm 2** Target Phase: LSVI-UCB-(S, S, d)

## 6 Theoretical Results

In this section, we prove our main theoretical results that show with enough samples in the source problem, our algorithms admit regret bounds that are independent of \(S,A,\) or both \(S\) and \(A\) in the target problem. We first state our main result in the \((S,S,d)\) Tucker rank setting.

**Theorem 2**.: _Suppose Assumptions 1, 2, and 3 hold, and set \(\delta\in(0,1)\). Furthermore, assume that, for any \(\epsilon\in(0,\sqrt{SH/T})\), \(Q_{m,h}=r_{m,h}+P_{m,h}V_{m,h+1}\) has rank \(d\) and is \(\mu\)-incoherent with condition number \(\kappa\) for all \(\epsilon\)-optimal value functions \(V_{h+1}\). Let \(\lambda=1\) and \(\beta^{s}_{k,h}\) be a function of \(d,H,|T^{s}_{k,h}|,|S|,M,T\). Then, for \(T\geq\frac{S}{\alpha^{2}}\), using at most \(\tilde{O}(d^{4}\mu^{5}\kappa^{4}(1+A/S)M^{2}H^{4}\alpha^{2}T)\) samples in the source problems, our algorithm has regret \(\tilde{O}(\sqrt{(dMH)^{3}ST})\) with probability at least \(1-\delta\)._

Theorem 2 states that using transfer learning improves the performance on the target problem by removing the regret bound's dependence on the action space. Thus, with enough samples from the source MDPs one can recover a regret bound in the target problem that matches the best regret bound for any algorithm in the \((S,S,d)\) Tucker rank setting with **known** latent feature representation \(G\) concerning \(S\) and \(T\). See Appendix E for the proof of Theorem 2.

For sake of brevity, we present our results in our other Tucker rank settings in Table 1 (see Appendices F, G, and H for the formal statements and proofs). In each of our Tucker rank settings, our algorithms use transfer learning to remove the dependence on \(S,A,\) or \(SA\) in the target regret bound, which matches the dependence on \(S\) and \(A\) of the best regret bounds of algorithms with **known** latent representation. While the source sample complexities of Theorems 2 and 10 may seem to not scale linearly in \(A\) and \(S\), respectively, we require the horizon in the target phase to be large enough, i.e., \(T\alpha^{2}(S/A+1)\geq S+A\) and \(T\alpha^{2}(1+A/S)\geq S+A\), respectively, so that one observes at least \(\Omega(S+A)\) samples in the source phase.

While our dependence on \(M\) in the target regret bound may seem sub optimal, our algorithm cannot distinguish which of the \(d\) dimensions (out of \(dM\) possible dimensions) match the ones in the target MDP without interacting with it. Thus, our algorithm must include all of them, which results in using at worst a \(dM\)-dimensional feature representation. However, in the case when the feature representations from source MDPs lie in subspaces that intersect, we can apply a singular value thresholding procedure to remove the unneeded dimensions used in the feature representation at the cost of an increased sample complexity (by a factor of \(A\)) in the source phase (see Appendix C).

**Comparison with [4]:** In comparison to Theorem 3.1 from [4], our result, Theorem 14, has similar dependence on \(A,d,H,\) and \(T\) while our dependence on \(\alpha\) is optimal compared to their \(\alpha^{5}\) dependence 4. While our dependence on \(M\) in the regret bound is worse, this is due to our transfer learning setting being more general; we only require that the space spanned by the target features be a subset of the space spanned by the source features instead of having the target features being a linear combination of the source features. Furthermore, our source sample complexity scales with the size of the state space instead of scaling logarithmically in the size of the given function class.

Footnote 4: Since \(\bar{\alpha}\geq\alpha\), moving \(\bar{\alpha}\) into the source sample complexity yields a sample complexity that scales with \(\alpha^{5}\).

The proofs of 2, Theorem 10, 15, and 14 synthesize the theoretical guarantees of LR-EVI, singular vector perturbation bounds, and LSVI-UCB or our modification of LSVI-UCB. Given \(N\) samples from the source MDPs, we first bound the error between the singular subspaces up to a rotation by \(\sqrt{d^{3}M(S+A)/N}\) (suppressing the dependence on common matrix estimation terms) to construct our feature mapping. Thus, with enough samples in the source MDPs, LR-EVI returns \(Q\) functions with singular vectors that can be used to construct a sufficient feature representation; the additional regret incurred from using the approximate feature representation in LSVI-UCB or Algorithm 2 is dominated by the original regret of using the true representation.

### Discussion of Optimality

In the \((S,d,A),(S,S,d),(d,S,A),\) and \((d,d,d)\) Tucker rank settings, the source sample complexity's scalings concerning \(S,A,H,\) and \(T\) are reasonable when learning \(\sqrt{A/T},\sqrt{S/T},\sqrt{1/T},\) and \(\sqrt{1/T}\)-optimal \(Q\) functions, respectively, given the \(Q\) learning lower bounds in [30]. Similarly, the dependence on \(\mu\) and \(\kappa\) are standard for RL algorithms incorporating matrix estimation.

In the \((S,d,A),(S,S,d),\) and \((d,S,A)\) Tucker rank settings, our \(\alpha^{2}\) dependence in the source sample complexity is optimal due to our lower bounds. In the \((d,d,d)\) Tucker rank setting, our \(\alpha^{4}\) dependence is likely optimal when eliminating the dependence on \(S\) and \(A\) in the target problem; one must combine both sets of singular vectors together, \(F:\mathcal{S}\rightarrow\mathbb{R}^{dM}\) and \(G:\mathcal{A}\rightarrow\mathbb{R}^{dM}\), to create a feature mapping \(\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{d^{2}M^{2}}\), which causes the \(\alpha^{4}\) dependence in our upper bound. However, if \(\alpha^{4}\) is too large, one can use the algorithms from the \((S,d,A)\) or \((S,S,d)\) Tucker rank settings to benefit from transfer learning. Proving an \(\alpha^{4}\) lower bound is an interesting open question. Additionally, the dependence on \(d\) and \(M\) in this setting is worse as our low rank latent representation of each state-action pair lies in a \(d^{2}M^{2}\) dimensional space in contrast to a space with dimension \(dM\).

The regret bounds of our algorithms used in the target phase, i.e., Algorithm 2, Algorithm 4, and LSVI-UCB [17], in each Tucker rank setting are optimal with respect to \(S,A,\) and \(T\). Since each Tucker rank setting captures tabular MDPs, we can construct lower bounds with a reduction from the bound in [18], e.g., in the \((S,S,d)\) setting, the regret is at least \(\tilde{\Omega}(\sqrt{dSH^{2}T})\). However, our dependence on \(d\) and \(H\) are sub-optimal. The extra factor of \(\sqrt{H}\) is likely due to our use of Hoeffding's inequality instead of tracking the variance of the estimates to use Bernstein's inequality. Our dependence on \(d\) is sub-optimal as LSVI-UCB is sub-optimal with respect to \(d\); for linear MDPs, LSVI-UCB++ [13] admits a regret bound of \(\tilde{O}(\sqrt{d^{2}H^{2}T})\), which matches the lower bound from [41]. Thus, to obtain the optimal dependence on \(d\) and \(H\), we would need to adapt LSVI-UCB++ to our Tucker rank setting.

Conclusion

We study transfer RL, in which one transfers a latent representation between source MDPs and target MDP with Tucker rank \((S,S,d),(S,d,A),(d,S,A)\) or \((d,d,d)\) transition kernels. As [4] studied transfer RL in the \((d,S,A)\) Tucker rank setting, our study completes the analysis of representational transfer in RL assuming that one or more modes of the transition kernel is low rank. Furthermore, we propose the transfer-ability coefficient that quantifies the difficulty of transferring latent representations between the source and target problems with information theoretic lower bounds. We propose computationally simple algorithms that admit regret bounds that are independent of \(S,A\), or \(SA\) depending on the Tucker rank setting given enough samples from the source MDPs. Furthermore, these regret bounds match the bounds of algorithms that are given the true latent representation. Our dependence on \(d\) and \(H\) is not optimal and can be improved by adapting the methods in [13] to our setting, which is an interesting open problem. Furthermore, proving theoretical guarantees of other forms of transfer RL is worthwhile and an interesting future direction.

Acknowledgements:Y. Chen is partially supported by NSF CCF-1704828 and NSF CCF-2233152.

## References

* [1] Yasin Abbasi-yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 24. Curran Associates, Inc., 2011.
* [2] Emmanuel Abbe, Jianqing Fan, Kaizheng Wang, and Yiqiao Zhong. Entrywise eigenvector analysis of random matrices with low expected rank. _https://arxiv.org/abs/1709.09565_, 48, 09 2017.
* [3] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. Flambe: Structural complexity and representation learning of low rank mdps. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 20095-20107. Curran Associates, Inc., 2020.
* [4] Alekh Agarwal, Yuda Song, Wen Sun, Kaiwen Wang, Mengdi Wang, and Xuezhou Zhang. Provable benefits of representational transfer in reinforcement learning, 2023.
* [5] Martin Anthony and Peter L. Bartlett. _Neural Network Learning: Theoretical Foundations_. Cambridge University Press, USA, 1st edition, 2009.
* [6] Mohammad Gheshlaghi Azar, Remi Munos, and Hilbert J. Kappen. On the sample complexity of reinforcement learning with a generative model. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, ICML'12, page 1707-1714, Madison, WI, USA, 2012. Omnipress.
* [7] Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization. _Communications of the ACM_, 55(6):111-119, 2012.
* [8] Emmanuel Candes and Justin Romberg. Sparsity and incoherence in compressive sampling. _Inverse problems_, 23(3):969, 2007.
* [9] Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, and Yuling Yan. Noisy matrix completion: Understanding statistical guarantees for convex relaxation via nonconvex optimization. _SIAM Journal on Optimization_, 30:3098-3121, 01 2020.
* [10] Yuan Cheng, Songtao Feng, Jing Yang, Hong Zhang, and Yingbin Liang. Provable benefit of multitask representation learning in reinforcement learning, 2022.
* [11] Yuan Cheng, Ruiquan Huang, Jing Yang, and Yingbin Liang. Improved sample complexity for reward-free reinforcement learning under low-rank mdps, 2023.
* [12] Jianqing Fan, Weichen Wang, and Yiqiao Zhong. An \(\ell_{\infty}\) eigenvector perturbation bound and its application to robust covariance estimation, 2016.

* [13] Jiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu. Nearly minimax optimal reinforcement learning for linear markov decision processes, 2023.
* [14] Roger A. Horn and Charles R. Johnson. _Matrix Analysis_. Cambridge University Press, 1985.
* [15] Haque Ishfaq, Thanh Nguyen-Tang, Songtao Feng, Raman Arora, Mengdi Wang, Ming Yin, and Doina Precup. Offline multitask representation learning for reinforcement learning, 2024.
* [16] Yassir Jedra, William Reveillard, Stefan Stojanovic, and Alexandre Proutiere. Low-rank bandits via tight two-to-infinity singular subspace recovery, 2024.
* [17] Chi Jin. Provably efficient reinforcement learning with linear function approximation. 2020.
* [18] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I. Jordan. Is q-learning provably efficient?, 2018.
* [19] Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu. Reward-free exploration for reinforcement learning. In _ICML_, 2020.
* [20] Michael Kearns and Satinder Singh. Finite-sample convergence rates for q-learning and indirect algorithms. In M. Kearns, S. Solla, and D. Cohn, editors, _Advances in Neural Information Processing Systems_, volume 11. MIT Press, 1999.
* [21] Jens Kober, J. Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_, 32(11):1238-1274, 2013.
* [22] Tor Lattimore, Csaba Szepesvari, and Gellert Weisz. Learning with good feature representations in bandits and in rl with a generative model, 2020.
* [23] Rui Lu, Gao Huang, and Simon S. Du. On the power of multitask representation learning in linear mdp, 2021.
* [24] Osman Asif Malik and Stephen Becker. Low-rank tucker decomposition of large tensors using tensorsketch. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning, 2013.
* [26] Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal. Model-free representation learning and exploration in low-rank mdps. _CoRR_, abs/2102.07035, 2021.
* [27] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. _IEEE Transactions on Knowledge and Data Engineering_, 22(10):1345-1359, 2010.
* [28] Sergio Rozada and Antonio G. Marques. Tensor and matrix low-rank value-function approximation in reinforcement learning, 2022.
* [29] Sergio Rozada, Victor Tenorio, and Antonio G. Marques. Low-rank state-action value-function approximation. In _2021 29th European Signal Processing Conference (EUSIPCO)_, pages 1471-1475, 2021.
* [30] Tyler Sam, Yudong Chen, and Christina Lee Yu. Overcoming the long horizon barrier for sample-efficient reinforcement learning with latent low-rank structure, 2023.
* [31] Kun Shao, Zhentao Tang, Yuanheng Zhu, Nannan Li, and Dongbin Zhao. A survey of deep reinforcement learning in video games, 2019.
* [32] Stefan Stojanovic, Yassir Jedra, and Alexandre Proutiere. Spectral entry-wise matrix estimation for low-rank reinforcement learning. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 77056-77070. Curran Associates, Inc., 2023.

* [33] Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. Representation learning for online and offline rl in low-rank mdps, 2021.
* [34] Bingyan Wang, Yuling Yan, and Jianqing Fan. Sample-efficient reinforcement learning for linearly-parameterized mdps with a generative model, 2021.
* [35] Xumei Xi, Christina Lee Yu, and Yudong Chen. Matrix estimation for offline reinforcement learning with low-rank structure, 2023.
* [36] Lin Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and regret bound. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 10746-10756. PMLR, 13-18 Jul 2020.
* [37] Lin F. Yang and Mengdi Wang. Sample-optimal parametric q-learning using linearly additive features. In _ICML_, 2019.
* [38] Yuzhe Yang, Guo Zhang, Zhi Xu, and Dina Katabi. Harnessing structures for value-based planning and reinforcement learning. In _International Conference on Learning Representations_, 2020.
* [39] Xuezhou Zhang, Yuzhe ma, and Adish Singla. Task-agnostic exploration in reinforcement learning, 2020.
* [40] Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen Sun. Efficient reinforcement learning in block mdps: A model-free representation learning approach, 2022.
* [41] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. Nearly minimax optimal reinforcement learning for linear mixture markov decision processes, 2021.

###### Contents

* 1 Introduction
* 2 Related Work
* 3 Preliminaries
	* 3.1 The Tucker Rank of a Tensor
	* 3.2 Low Tucker Rank MDPs
* 4 Transfer-ability
	* 4.1 Information Theoretic Lower Bound
* 5 Algorithm
* 6 Theoretical Results
	* 6.1 Discussion of Optimality
* 7 Conclusion
* A Notation Table
* B Table of Regret Bounds with Known Latent Representations
* C Thresholding Procedure in the \((S,S,d)\) Tucker Rank Setting
* C.1 Thresholding Procedure without Knowledge of \(d^{\prime\prime}\)
* D Properties of the Transfer-ability Coefficient \(\alpha\)
* E Omitted Proofs in the \((S,S,d)\) Tucker Rank Setting
* E.1 Proof of Theorem 1
* E.2 Proof of Theorem 2
* E.3 Proof of Theorem 3
* F \((S,d,A)\) Tucker Rank Setting
* F.1 Information Theoretic Lower Bound
* F.2 Simulator to Simulator Transfer RL
* G \((d,S,A)\) Tucker Rank Setting
* H \((d,d,d)\) Tucker Rank Setting
* I \(\ell_{\infty}\) eigen perturbation bound with non-uniform noise: rank-\(r\) case
* J Proofs of LSVI-UCB-(S, d, A) and LSVI-UCB-(S, S, d)
* J.1 LSVI-UCB-(S, d, A)

## Appendix A Notation Table

\begin{table}
\begin{tabular}{c|l} \hline Symbol & Definition \\ \hline \(\mathcal{S},S\) & State space, Cardinality of \(\mathcal{S}\) \\ \(\mathcal{A},A\) & Action space, Cardinality of \(\mathcal{A}\) \\ \(H,K\) & Finite Horizon, Number of Episodes \((T=KH)\) \\ \(M\) & Number of Source MDPs \\ \(r_{m,h},P_{m,h}\) & Reward Function, Transition Kernel for Source MDP \(m\) at step \(h\) \\ \(r_{h},P_{h}\) & Reward Function, Transition Kernel for the Target MDP at step \(h\) \\ \(Q^{\pi}_{m,h},Q^{\pi}_{h}\) & Action-value Function at Step \(h\) Following \(\pi\) for Source MDP \(m\) and the Target MDP \\ \(Q^{*}_{m,h},Q^{*}_{h}\) & Optimal Action-value Function at Step \(h\) for Source MDP \(m\) and the Target MDP \\ \(V^{\pi}_{m,h},V^{*}_{h}\) & Value Function at Step \(h\) Following \(\pi\) for Source MDP \(m\) and the Target MDP \\ \(d\) & Rank of the Low-dimensional Mode of \(P_{m,h}\) \\ \(d^{\prime}\) & Rank of the Low-dimensional Mode of \(P_{h}\) \\ \(\alpha\) & Transfer-ability Coefficient \\ \(G_{m,h}(a),G_{h}(a)\) & True Latent Representation of Each Action in the Source and \\  & Target MDPs in the \((S,S,d)\) Tucker Rank Setting \\ \(\tilde{G}_{h}\) & Concatenation of \(G_{m,h}\) for \(m\in[M]\) \\ \(\tilde{G}_{h}\) & Latent Representation Estimate Computed in the Source Phase \\ LR-EVI(\(\cdot\)) & Low Rank Empirical Value Iteration [30] \\ \(\Lambda^{s}_{h}\) & Gram Matrix for State \(s\) at Step \(h\) \\ \(w^{s}_{h}\) & Regularized Least Squares Solution for State \(s\) at Step \(h\) \\ \(T^{s}_{k,h}\) & Episodes before Episode \(k\), in which one was at State \(s\) at Step \(h\) \\ \(\mu(X)\) & Incoherence of Matrix \(X\) \\ \(\kappa(X)\) & Condition Number of Matrix \(X\) \\ \(\sigma_{i}(X)\) & \(i\)-th Singular Value of \(X\) \\ \(\|X\|_{op}\) & Operator Norm of Matrix \(X\) \\ \(\|X\|_{\infty}\) & Maximum of the Absolute Value of Each Entry of \(X\) \\ \hline \end{tabular}
\end{table}
Table 2: List of notation

## Appendix B Table of Regret Bounds with Known Latent Representations

Since each Tucker rank setting captures tabular MDPs, we can construct lower bounds with a reduction from the bound in [18]; in the \((S,d,A)\) Tucker rank setting, the regret is at least \(\tilde{\Omega}(\sqrt{dAH^{2}T})\), and in the \((S,S,d)\) setting, the regret is at least \(\tilde{\Omega}(\sqrt{dAH^{2}T})\). As mentioned, our dependence on \(d\) and \(H\) are sub optimal because we modify LSVI-UCB [17], which has a sub-optimal dependence on \(d\) and \(H\). Modifying the algorithm from [13] to achieve the optimal dependence on \(d\) and \(H\) in the \((S,S,d)\) and \((S,d,A)\) Tucker rank settings is an interesting problem for future research.

## Appendix C Thresholding Procedure in the \((S,s,d)\) Tucker Rank Setting

When the feature representations of each source MDP lies in orthogonal \(d\)-dimensional subspaces, we cannot discard any unique dimension of the \(M\) subspaces as we cannot tell which of the \(d\)-dimensions match the ones in the target MDP without interacting with it. However, when the feature mappings from the source MDPs lie in intersecting subspaces, i.e.,

\[\text{rank}\left(\tilde{G}_{h}\right)=\text{rank}\left(\sqrt{\frac{A}{d\mu} }\left[G_{1,h}\quad\ldots\quad G_{M,h}\right]\right)=d^{\prime\prime}\]

for \(d^{\prime}\leq d^{\prime\prime}\leq dM\) (recall the target MDP has Tucker rank \((S,S,d^{\prime})\)), we use the following procedure: after computing \(\tilde{G}_{h}\) via concatenation of \(\hat{G}_{m,h}\) for all \(m\in[M]\), we perform a singular value decomposition of \(\tilde{G}_{h}\) and threshold the singular values to keep only \(d^{\prime\prime}\) dimensions. With this procedure, our regret bound now depends on \(d^{\prime\prime}\) instead of \(dM\) at the cost of an increased source sample complexity. We only present the theorem and proof in the \((S,S,d)\) Tucker rank setting, but our results and methods extend to the other Tucker rank settings.

**Theorem 3**.: _Suppose Assumptions 1, 2, and 3 hold, and assume the source MDP latent factors span a \(d^{\prime\prime}\)-dimensional space. Let \(\delta\in(0,1)\). Furthermore, assume that, for any \(\epsilon\in(0,\sqrt{SH/T})\), \(Q_{m,h}=r_{m,h}+P_{m,h}V_{m,h+1}\) has rank \(d\) and is \(\mu\)-incoherent with condition number \(\kappa\) for all \(\epsilon\)-optimal value functions \(V_{h+1}\). Let \(\lambda=1\) and \(\beta^{*}_{k,h}\) be a function of \(d,H,|T^{s}_{k,h}|,|S|,M,T\). Then, for \(T\geq\frac{S}{\alpha^{2}}\), using at most \(\tilde{O}(d^{\prime}\mu^{5}\kappa^{4}(A+A^{2})M^{3}H^{4}\alpha^{2}T/(Sd^{ \prime\prime}))\) samples in the source problems, our algorithm has regret \(\tilde{O}(\sqrt{(d^{\prime\prime}H)^{3}ST})\) with probability at least \(1-\delta\)._

The proof of Theorem 3 follows the proof of Theorem 2, except we require a larger source sample complexity to guarantee with high probability that the \(d^{\prime\prime}+1\)-th singular value of the concatenated feature representation is sufficiently small. With this procedure, we ensure that our feature representation removes the unneeded dimensions. See Appendix E for proof of the above theorem.

### Thresholding Procedure without Knowledge of \(d^{\prime\prime}\)

While the above procedure requires knowledge of the dimension of the subspace spanned by the latent features of the source MDPs, one can also threshold small singular values of the concatenated feature mapping as the misspecification error will still be small. The specific procedure is as follows:

\begin{table}
\begin{tabular}{c|c c} \hline \hline  & Tucker Rank & Regret Bound with Known Latent Representation \\ \hline Theorem 5 & \((S,S,d)\) & \(\tilde{O}(\sqrt{d^{3}H^{3}ST})\) \\ Theorem 8 & \((S,d,A)\) & \(\tilde{O}(\sqrt{d^{3}H^{3}AT})\) \\ Theorem 3.1 [17] & \((d,S,A)\) & \(\tilde{O}(\sqrt{d^{3}H^{3}T})\) \\ Theorem 5.1 [13], Theorem 5.6 [41] & \((d,S,A)\) & \(\tilde{\Theta}\left(\sqrt{d^{2}H^{2}T}\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Theoretical guarantees of our algorithms alongside results from literature in the different Tucker rank settings. To the best of our knowledge, there are no algorithms tailored to the \((d,d,d)\) Tucker rank setting. Instead, one should use a linear MDP algorithm.

after performing the singular value decomposition on the concatenation of \(\tilde{G}_{m,h}\), we first find the smallest value of \(t\in[dM]\) that satisfies

\[\sigma_{t+1}\leq\sqrt{\frac{tHSd\mu}{\alpha^{2}T(dM-t)M^{2}A}},\]

and threshold \(\sigma_{t+1},\ldots,\sigma_{dM}\). Let

\[\mathcal{T}=\left\{t\in[dM]|\sigma_{t+1}\leq\sqrt{\frac{tHSd\mu}{\alpha^{2}T( dM-t)M^{2}A}}\right\}\]

be the set of \(t\) that satisfies the inequality on the singular values. Then, with the specified procedure, our algorithm admits a target regret bound that depends on \(t\) instead of \(dM\). Note that \(t\) is minimally \(1\) and at most \(dM\).

**Theorem 4**.: _Suppose Assumptions 1, 2, and 3 hold. Let \(\delta\in(0,1)\) and \(t=\min_{t^{\prime}\in\mathcal{T}}t^{\prime}\). Furthermore, assume that, for any \(\epsilon\in(0,\sqrt{SH/T})\), \(Q_{m,h}=r_{m,h}+P_{m,h}V_{m,h+1}\) has rank \(d\) and is \(\mu\)-incoherent with condition number \(\kappa\) for all \(\epsilon\)-optimal value functions \(V_{h+1}\). Let \(\lambda=1\) and \(\beta^{*}_{k,h}\) be a function of \(d,H,|T^{s}_{k,h}|,|S|,M,T\). Then, for \(T\geq\frac{S}{\alpha^{2}}\), using at most \(\tilde{O}(d^{5}\mu^{5}\kappa^{4}(1+A/S)M^{3}H^{4}\alpha^{2}T)\) samples in the source problems, our algorithm has regret \(\tilde{O}(\sqrt{(tH)^{3}ST})\) with probability at least \(1-\delta\)._

With an increase in a factor of \(dM\) to the source sample complexity, we can potentially improve the regret bound of the target by decreasing the dimension of the feature representation from \(dM\) to \(t\). See Appendix E for the proof of the above theorem.

## Appendix D Properties of the Transfer-ability Coefficient \(\alpha\)

In this section, we prove properties of \(\alpha\) and provide easy and hard instances. We first bound the values of \(\alpha\).

**Lemma 1**.: _Let \(\alpha\) be defined according to Definition 4. Then, we have that \(\frac{1}{dM}\leq\alpha<\infty\)._

Proof.: Since Assumption 6 holds and that the latent factors are normalized, it follows that for all \(i\in[d^{\prime}]\) and any latent factor matrix,

\[\|F_{h}(:,i)\|_{2} =\|\sum_{j\in[d],m\in[M]}B_{h}(i,j,m)F_{m,h}(:,j)\|_{2}\] \[1 \leq\sum_{j\in[d],m\in[M]}\|B_{h}(i,j,m)F_{m,h}(:,j)\|_{2}\] \[\leq\sum_{j\in[d],m\in[M]}|B_{h}(i,j,m)|\|F_{m,h}(:,j)\|_{2}\] \[=\sum_{j\in[d],m\in[M]}|B_{h}(i,j,m)|.\]

Given that \(1\leq\sum_{j\in[d],m\in[M]}|B_{h}(i,j,m)|\), it follows that the minimum of \(\max_{j\in[d],m\in[M]}|B_{h}(i,j,m)|\) is attained when \(|B_{h}(i,j,m)|=\frac{1}{dM}\). Thus, \(\alpha\geq\frac{1}{dM}\)

Let \(\gamma\in(0,1)\). Consider the construction where the target latent factor is \(F_{T}=[1,0]\) and the source latent factors from source MDPs one and two are

\[F_{1}=[1-\gamma,\gamma\sqrt{2/\gamma-1}],\quad F_{2}=[1-\gamma,-\gamma\sqrt{2 /\gamma-1}].\]

Clearly, the latent factors are orthonormal. Then, it follows that the only linear combination of \(F_{1}\) and \(F_{2}\) that results in \(F\) is

\[F_{T}=cF_{1}+cF_{2},\]

which implies that \(c=\frac{1}{2-2\gamma}\). Thus, \(\alpha=\frac{1}{2-2\gamma}\), and it follows that we can make \(\alpha\) arbitrarily large because for any positive constant \(C\), we can choose \(\gamma\) so that \(\alpha>C\)As the proof of Lemma 1 uses a construction in which \(\alpha\) can be arbitrarily large, we next provide simple constructions where \(\alpha\) is small. In the rank one case where all source latent factors equal the target latent factor, \(\alpha\) attains its minimum of \(\alpha=\frac{1}{M}\). Next, we consider the construction where

\[F_{T}=\left[\frac{1}{\sqrt{d}},\ldots,\frac{1}{\sqrt{d}}\right],\]

and there are \(M\) source MDPs with the standard basis vectors in \(\mathbb{R}^{d}\) as the latent factors equally distributed among the \(M\) rank one source MDPs. Thus,

\[F_{T}=\sum_{m\in M}\frac{\sqrt{d}}{M}F_{m},\]

and \(\alpha=\sqrt{d}/M\).

We next present a transfer RL instance with large \(\alpha\). Consider the following construction with the goal of trying to transfer the state latent factors. Without loss of generality, let the size of the state space and action space be \(4n\) for some \(n\in\mathbb{N}_{+}\). Furthermore, we refer to the \(i\)-th quarter of the states as \(s_{i}\) for \(i\in[4]\) and the \(i\)-th half of the actions as \(a_{i}\) for \(i\in[2]\) because our construction is a block MDP with four latent states and two latent actions. Let \(H=2\). In each MDP, the transition kernel does not vary with \(h\) while the reward function is time dependent. Assume that the transition kernels have Tucker rank \((S,d,A)\) (Assumption 5). For ease of notation, we refer to the \(i\)-th row as any state in \(s_{i}\) and the \(i\)-th column as any state in \(a_{i}\). The target MDP's latent factors are

\[F_{h}=\left[\begin{array}{cc}\sqrt{1/(2n)}&0\\ \sqrt{1/(2n)}&0\\ 0&\sqrt{1/(2n)}\\ 0&\sqrt{1/(2n)}\end{array}\right],\quad W_{1}=\left[\begin{array}{cc}\sqrt{ 1/(2n)}&\sqrt{1/(8n)}\\ \sqrt{1/(2n)}&\sqrt{1/(8n)}\end{array}\right],\quad W_{2}=\left[\begin{array}[ ]{cc}\sqrt{1/(8n)}&\sqrt{1/(2n)}\\ \sqrt{1/(8n)}&\sqrt{1/(2n)}\end{array}\right]\]

for all \(h\in[H]\), and

\[U_{h}(s_{1}|a_{1}) =U_{h}(s_{2}|a_{1})=[3\sqrt{2}/(8\sqrt{n}),\sqrt{2}/(8\sqrt{n})]\] \[U_{h}(s_{1}|a_{2}) =U_{h}(s_{2}|a_{2})=[\sqrt{2}/(8\sqrt{n}),3\sqrt{2}/(8\sqrt{n})]\] \[U_{h}(s_{3}|a_{1}) =U_{h}(s_{4}|a_{1})=[3\sqrt{2}/(8\sqrt{n}),\sqrt{2}/(8\sqrt{n})]\] \[U_{h}(s_{3}|a_{2}) =U_{h}(s_{4}|a_{2})=[\sqrt{2}/(8\sqrt{n}),3\sqrt{2}/(8\sqrt{n})].\]

Thus, the reward functions and transition kernels for the target MDP are

\[r_{1}=\left[\begin{array}{cc}\frac{1}{2t_{1}}&\frac{1}{2t_{1}}\\ \frac{2t_{1}}{2t_{1}}&\frac{2t_{1}}{2t_{1}}\\ \frac{4t_{1}}{4t_{1}}&\frac{4t_{1}}{4n}\end{array}\right]\quad r_{2}=\left[ \begin{array}{cc}\frac{1}{4t_{1}}&\frac{1}{4t_{1}}\\ \frac{1}{4t_{1}}&\frac{1}{4t_{1}}\\ \frac{2t_{1}}{2n}&\frac{2t_{1}}{2n}\end{array}\right]\]

and the transition kernels are

\[P_{h}(s_{1}|\cdot,\cdot)=P_{h}(s_{2}|\cdot,\cdot)=\left[\begin{array}{cc} \frac{3}{8n}&\frac{1}{8n}\\ \frac{8}{8n}&\frac{1}{8n}\\ \frac{8}{8n}&\frac{8n}{8n}\end{array}\right]\quad P_{h}(s_{3}|\cdot,\cdot)=P_{ h}(s_{4}|\cdot,\cdot)=\left[\begin{array}{cc}\frac{1}{8n}&\frac{3}{8n}\\ \frac{8}{8n}&\frac{8}{8n}\\ \frac{8}{8n}&\frac{8}{8n}\end{array}\right]\]

for all \(h\in[H]\). Using the Bellman equations, it follows that the optimal \(Q\) function for the above MDP is

\[Q_{1}^{*}=\left[\begin{array}{cc}\frac{1}{16n}&\frac{15}{16n}\\ \frac{16n}{16n}&\frac{16n}{16n}\\ \frac{16n}{16n}&\frac{16n}{16n}\end{array}\right],\quad Q_{2}^{*}=\left[ \begin{array}{cc}\frac{1}{4t_{1}}&\frac{1}{4t_{1}}\\ \frac{4t_{1}}{4t_{1}}&\frac{4t_{1}}{4t_{1}}\\ \frac{2t_{1}}{2n}&\frac{2t_{1}}{2n}\end{array}\right].\]

and both \(Q^{*}\)s have \(F_{h}\) as orthonormal latent factors. Next, we present the orthonormal latent factors for the first source MDP

\[F_{1,h}=\left[\begin{array}{cc}1/\sqrt{2n}&1/\sqrt{4n}\\ -1/\sqrt{2n}&1/\sqrt{4n}\\ 0&1/\sqrt{4n}\\ 0&1/\sqrt{4n}\end{array}\right],W_{1,1}=\left[\begin{array}{cc}3/\sqrt{4n} &3/\sqrt{4n}\\ 1/\sqrt{4n}&1/\sqrt{4n}\end{array}\right],W_{1,2}=\left[\begin{array}{cc}1/ \sqrt{4n}&1/\sqrt{4n}\\ 2/\sqrt{4n}&2/\sqrt{4n}\end{array}\right].\]It follows that the reward functions are

\[r_{1,1}=\left[\begin{array}{ccc}(3+3\sqrt{2})/(4n)&(1+\sqrt{2})/(4n)\\ (3-3\sqrt{2})/(4n)&(1-\sqrt{2})/(4n)\\ 3/(4n)&1/(4n)\\ 3/(4n)&1/(4n)\end{array}\right],r_{1,2}=\left[\begin{array}{ccc}(1+\sqrt{2}) /(4n)&(1+\sqrt{2})/(2n)\\ (1-\sqrt{2})/(4n)&(1-\sqrt{2})/(2n)\\ 1/(4n)&1/(2n)\\ 1/(4n)&1/(2n)\end{array}\right].\]

with \(P_{1,h}(\cdot|\cdot,\cdot)=1/(4n)\). Since \(P_{1,h}\) is the uniform transition kernel (which has the second column of \(F_{1,h}\) as its latent factor), it follows that \(PV^{*}_{1,2}(\cdot,\cdot)\) is a rank one matrix with every entry being the same positive constant. Thus, \(Q^{*}_{1,1}=r_{1,1}+P_{1,1}V^{*}_{1,2}\) and \(Q^{*}_{1,2}=r_{1,2}\) are rank two matrices with \(F_{1,h}\) as latent factors.

The second source MDP has the following latent factors

\[F_{2,h}=\left[\begin{array}{ccc}\frac{1}{c\sqrt{2n}}+\frac{1}{c\beta\sqrt{ 4n}}&1/\sqrt{4n}\\ -\frac{1}{c\sqrt{2n}}+\frac{1}{c\beta\sqrt{4n}}&1/\sqrt{4n}\\ -1/(c\beta\sqrt{4n})&1/\sqrt{4n}\\ -1/(c\beta\sqrt{4n})&1/\sqrt{4n}\end{array}\right],W_{2,1}=\left[\begin{array} []{ccc}1/\sqrt{2n}&0\\ 0&1/\sqrt{2n}\\ 1/\sqrt{2n}&0\end{array}\right],W_{2,2}=\left[\begin{array}{ccc}0&1/\sqrt{ 2n}\\ 1/\sqrt{2n}&0\end{array}\right]\]

where \(c=\sqrt{1+1/\beta^{2}}\) for all \(h\in[H]\) with \(P_{2,h}(\cdot|\cdot,\cdot)=1/(4n)\). Since \(P_{2,h}\) is the uniform transition kernel (which has the second column of \(F_{2,h}\) as its latent factor), it follows that \(PV^{*}_{2,2}(\cdot,\cdot)\) is a rank one matrix with every entry being the same positive constant. Thus, \(Q^{*}_{2,1}=r_{2,1}+P_{2,1}V^{*}_{2,2}\) and \(Q^{*}_{2,2}=r_{2,2}\) are rank two matrices with \(F_{2,h}\) as latent factors. Now, we prove that \(\alpha\in\Omega(\beta)\).

**Corollary 1**.: _Consider the above construction that satisfies Assumptions 5 and 6. Then, \(\alpha\) defined in Definition 4 satisfies \(\alpha\in\Omega(\beta)\)._

Proof.: Consider the above construction with \(\beta>0\). First, we show that when expressing the first column of \(F_{h}\) as a linear combination of the columns of \(F_{1,h}\) and \(F_{2,h}\), one coefficient is \(\Omega(\beta)\).

We first note that one must use the first column in \(F_{2,h}\) or else the system of linear equations is inconsistent. Thus, we solve

\[a\left[\begin{array}{c}1/\sqrt{2n}\\ -1\sqrt{2n}\\ 0\\ 0\end{array}\right]+b\left[\begin{array}{c}1/\sqrt{4n}\\ 1/\sqrt{4n}\\ 1/\sqrt{4n}\\ 1/\sqrt{4n}\end{array}\right]+d\left[\begin{array}{c}\frac{1}{c\sqrt{2n}}+ \frac{1}{c\beta\sqrt{4n}}\\ -\frac{1}{c\sqrt{2n}}+\frac{1}{c\beta\sqrt{4n}}\\ -1/(c\beta\sqrt{4n})\\ -1/(c\beta\sqrt{4n})\end{array}\right]=\left[\begin{array}{c}1/\sqrt{22n}\\ 1/\sqrt{n}\\ 0\\ 0\end{array}\right]\]

It follows that \(d=bc\beta\). Thus, \(b=1/\sqrt{2}\), and \(d\in\Omega(\beta)\) because \(c\geq 1\).

To show that \(\alpha\in\Omega(\beta)\), we assume for sake of contradiction that there exists some set of basis vectors \(F^{\prime}_{1,h}\) and \(F_{2,h^{\prime}}\) that span the subspace defined by \(F_{1,h}\) and \(F_{2,h}\), respectively, such that we can express the any rotation of the first column of \(F_{h}\) as a linear combination of the columns of \(F^{\prime}_{1,h}\) and \(F_{2,h^{\prime}}\) using coefficients \(a,b,c,d\) such that \(|a|,|b|,|c|,|d|\in o(\beta)\). Since \(F_{h},F_{1,h}\), and \(F_{2,h}\) are orthonormal latent factors, it follows that there exist rotation matrices \(R,R_{1},R_{2}\) with \(\|R\|_{\infty},\|R_{1}\|_{\infty},\|R_{2}\|_{\infty}\in O(1)\) such that \(F^{\prime}_{1,h}=R_{1}F_{1,h}\) and \(F^{\prime}_{2,h}=R_{2}F_{2,h}\). It follows that \(R^{-1}\) exists and has entries with constant magnitude. By construction we have

\[F_{h}(:,0)=aR^{-1}R_{1}F_{1,h}(:,0)+bR^{-1}R_{1}F_{1,h}(:,1)+cR^{-1}R_{2}F_{2,h }(:,0)+dR^{-1}R_{2}F_{2,h}(:,1).\]

Since the entries of the rotation matrices and their inverse are bounded by a constant, the above equation states that we can represent the first column of \(F_{h}\) as a linear combination of the columns of \(F_{1,h}\) and \(F_{2,h}\) using only constant coefficients. Thus, we reach a contradiction because we've shown that one entry of \(dR^{-1}R_{2}\in\Omega(\beta)\). Therefore, \(\alpha\in\Omega(\beta)\). 

## Appendix E Omitted Proofs in the \((S,s,d)\) Tucker Rank Setting

We first prove our lower bound.

### Proof of Theorem 1

Proof.: Suppose all source and target MDPs \((S,A,P,H,r)\) share the same state space, action space, and horizon \(H=1\) and assume that \(\mathcal{S}=\mathcal{A}=[2n]\) for some \(n\in\mathbb{N}_{+}\). For ease of notation, we let \(s_{1}\) and \(a_{1}\) refer to any state and action in \([n]\), respectively, and \(s_{2}\) and \(a_{2}\) refer to any state and action in \(\{n+1,\dots 2n\}\), respectively. We will present the latent factors and optimal Q functions as block vectors and matrices with \(s_{1},s_{2},a_{1},a_{2}\) as the blocks containing \(n\) entries. The initial state distribution in the target MDP is uniform over \(\mathcal{S}\). We now present two transfer RL problems with similar \(Q\) functions (with rows \(s_{1},s_{2}\) and columns \(a_{1},a_{2}\)) that satisfy Assumptions 1 and 2 but have orthogonal target state latent factors. For ease of notation, the superscript \(i\) of \(Q^{*,i}_{m,h},Q^{*,i}_{h}\) denotes transfer RL problem for \(i\in\{1,2\}\). Then, the optimal \(Q\) functions for transfer RL problem one are

\[Q^{*,1}_{1,1} =n\begin{bmatrix}\sqrt{1/n}\\ 0\end{bmatrix}\begin{bmatrix}\sqrt{1/(2n)}&\sqrt{1/(2n)}\end{bmatrix}= \begin{bmatrix}1/\sqrt{2}&1/\sqrt{2}\\ 0&0\end{bmatrix},\] \[Q^{*,1}_{2,1} =n\begin{bmatrix}0\\ \sqrt{1/n}\end{bmatrix}\begin{bmatrix}\sqrt{1/(2n)}&\sqrt{1/(2n)}\end{bmatrix}= \begin{bmatrix}0&0\\ 1/\sqrt{2}&1/\sqrt{2}\end{bmatrix},\] \[Q^{*,1}_{1} =n\begin{bmatrix}\sqrt{1/(2n)}\\ -\sqrt{1/(2n)}\end{bmatrix}\begin{bmatrix}\sqrt{1/(2n)}&\sqrt{1/(2n)}\end{bmatrix}= \begin{bmatrix}1/2&1/2\\ -1/2&-1/2\end{bmatrix},\]

and the optimal \(Q\) functions for transfer RL problem two are

\[Q^{*,2}_{1,1} =n\begin{bmatrix}\sqrt{1/n}\\ 0\end{bmatrix}\begin{bmatrix}\sqrt{1/(2n)}&\sqrt{1/(2n)}\end{bmatrix}= \begin{bmatrix}1/\sqrt{2}&1/\sqrt{2}\\ 0&0\end{bmatrix},\] \[Q^{*,2}_{2,1} =n\begin{bmatrix}0\\ 0/1/n\end{bmatrix}G^{\prime}=\begin{bmatrix}0&0\\ (1+1/\alpha)/(c\sqrt{2})&(1-1/\alpha)/(c\sqrt{2})\end{bmatrix},\] \[Q^{*,2}_{1} =n\begin{bmatrix}\sqrt{1/(2n)}\\ -\sqrt{1/(2n)}\end{bmatrix}\begin{bmatrix}\sqrt{1}&-\sqrt{\frac{1}{(2n)}} \end{bmatrix}=\begin{bmatrix}1/2&-1/2\\ -1/2&1/2\end{bmatrix},\]

where

\[G^{\prime}=\begin{bmatrix}(\sqrt{1/(2n)}+\sqrt{1/(2n\alpha^{2})})/\sqrt{1+1/ \alpha^{2}}&(\sqrt{1/(2n)}-\sqrt{1/(2n\alpha^{2})}/\sqrt{1+1/\alpha^{2}} \end{bmatrix},\]

and \(c=\sqrt{1+1/\alpha^{2}}\). Note that \(c\in(1,2dM)\) because \(\alpha\geq 1/(dM)\) from Lemma 1 for \(\alpha>0\). The incoherence \(\mu\), rank \(d\), and condition number \(\kappa\) of the above \(Q\) functions are \(O(1)\). Furthermore, the above construction satisfies Assumption 5 with Tucker rank \((S,S,1)\) and Assumption 6 because the source and target MDPs share the same action latent factor \(\begin{bmatrix}\sqrt{1/(2n)}&\sqrt{1/(2n)}\end{bmatrix}\) in the first transfer RL problem while in the second transfer RL problem,

\[\begin{bmatrix}\sqrt{1/(2n)}&-\sqrt{1/(2n)}\end{bmatrix}=-\alpha\begin{bmatrix} \sqrt{1/(2n)}&\sqrt{1/(2n)}\end{bmatrix}+\alpha cG^{{}^{\prime}}.\]

Note that \(G^{1}=\begin{bmatrix}\sqrt{1/(2n)}&\sqrt{1/(2n)}\end{bmatrix}\) and \(G^{2}=\begin{bmatrix}\sqrt{1/(2n)}&-\sqrt{1/(2n)}\end{bmatrix}\), so the target latent factors of the different transfer RL problems are orthogonal to each other.

The learner is given either transfer RL problem one or two with equal probability with a generative model in the source problem. When interacting with the source MDPs, the learner specifies a state-action pair \((s,a)\) and source MDP \(m\) and observes a realization of a shifted and scaled Bernoulli random variable \(X\). The distribution of \(X\) is that \(X=1\) with probability \((Q^{*,i}_{m,1}(s,a)+1)/2\) and \(X=-1\) with probability \((1-Q^{*,i}_{m,1}(s,a))/2)\). Furthermore, the learner is given the knowledge that the state latent features lie in the function class \(\mathcal{G}=\{G^{1},G^{2}\}\) and must use the knowledge obtained from interacting with the source MDP to choose one to use in the target MDP, which is no harder than receiving no information about the function class.

To distinguish between the two transfer RL problems, one needs to differentiate between \(Q^{*,1}_{2,1}\) and \(Q^{*,2}_{2,1}\). By construction, the magnitude of the largest entrywise difference between \(Q^{*,1}_{2,1}\) and \(Q^{*,2}_{2,1}\) is lower bounded by \(\Omega(1/\alpha)\).

If the learner observes \(Z\) samples in the source MDPs, where \(Z\leq O(\alpha^{2})\) for some constant \(C>0\), then the probability of correctly identifying \(G\) is upper bounded by \(0.76\) (Lemma 5.1 with \(\delta=0.24\)[5]). Thus, if the learner does not observe \(\Omega(\alpha^{2})\) samples in the source phase, then the learner returns the incorrect feature mapping that is orthogonal to the true feature mapping with probability at least \(0.24\)

### Proof of Theorem 2

We first present the theoretical guarantees of LSVI-UCB-(S, S, d) and defer their proofs to Appendix J.

**Theorem 5**.: _Assume that Assumption 1 holds and the learner is given the true latent action representation \(G=\{G_{h}\}_{h\in[H]}\). Then, there exists a constant \(c>0\) such that for any \(\delta\in(0,1)\), if we set \(\lambda=1,\beta_{k,h}^{s}=cdH\sqrt{\iota},\iota=\log(2dTS/\delta)\), then with probability at least \(1-\delta\), the total regret of Algorithm 2 is \(\tilde{O}(\sqrt{d^{3}SH^{3}T})\)._

Theorem 5 states that LSVI-UCB-(S, S, d) completely removes the regret bound's dependence on the size of the state space by utilizing the latent action representation \(G\). Similarly, the algorithm is robust to misspecification error.

**Assumption 4** (\(\xi\)-approximate \((S,S,d)\) Tucker rank MDP).: _Assume \(\xi\in[0,1]\). Then, an MDP \((\mathcal{S},\mathcal{A},P,r,H)\) is a \(\xi\)-approximate \((S,S,d)\) Tucker rank MDP with given feature map \(G\) if there exist \(H\) unknown \(S\)-by-\(d\) matrices \(W=\{W_{h}\}_{h\in[H]}\) and \(S\)-by-\(S\)-by-\(d\) tensors \(U=\{U_{h}\}_{h\in[H]}\) such that_

\[|r_{h}(s,a)-W_{h}(s)^{\top}G_{h}(a)|\leq\xi,\]

\[\left|\sum_{s^{\prime}\in S}(P_{h}(s^{\prime}|s,a)-U_{h}(s^{\prime},s)G_{h}(a )^{\top})\right|\leq\xi\]

_for all \(h\in[H]\) where \(\|W_{h}\|_{2},\|\sum_{s^{\prime}\in S}g(s^{\prime})U_{h}(s^{\prime},a)\|_{2} \leq\sqrt{A/(d\mu)}\) for any function \(g:\mathcal{S}\to[0,1]\)._

**Theorem 6**.: _Under the Assumption 4, for any \(\delta\in(0,1)\), for \(\lambda=1,\beta_{k,h}^{s}=O(Hd\sqrt{\iota}+\xi\sqrt{kd}H)\) for \(\iota=\log(2dTS/\delta)\), the regret of Algorithm 2 is \(\tilde{O}(\sqrt{d^{3}H^{3}ST}+\xi dHT)\) with probability at least \(1-\delta\)._

We now prove Theorem 2.

Proof.: Suppose the assumption stated in Theorem 2 hold. Then, from [30], running LR-EVI with a sample complexity of \(C^{d}d^{\mu}\iota^{5}\kappa^{4}(S+A)MH^{4}\alpha^{2}T\log(2SAMH/\delta)/S\) on each source MDP results in \(\tilde{Q}_{m,h}\) functions with singular value decomposition \(\tilde{Q}_{m,h}=\hat{F}_{m,h}\hat{\Sigma}_{m,h}\hat{G}_{m,h}\) that are \(\sqrt{HS}/(16\alpha\mu\kappa\sqrt{dMT})\)-optimal with probability at least \(1-\delta/2\).

Since our estimates are \(\sqrt{HS}/(16\alpha\kappa\sqrt{dMT})\)-optimal, it follows that \(\bar{\gamma}<1/2\) in the singular vector perturbation bound, Corollary 3. Since \(\|Q_{m,h}^{*}\|_{\infty}\geq C\), from Corollary 3, it follows that there exists rotation matrices \(R_{m,h}\) such that

\[\|(\hat{G}_{m,h}R_{m,h}-G_{m,h})(a)\|_{2}\leq\frac{d\sqrt{H\mu S}}{\alpha\sqrt{ AMT}}\]

for all \(a\in A\) where the optimal \(Q\) function have singular value decomposition \(Q_{m,h}^{*}=F_{m,h}\Sigma_{m,h}G_{m,h}^{\top}\).

Let \(\tilde{G}\) be the \(A\times dM\) matrix from joining all the source action latent factor matrices, and \(\hat{\tilde{G}}_{h}\) be the \(A\times dM\) matrix from joining all \(\tilde{G}_{m,h}\). Let \(R_{h}\) be the \(d\times dM\) matrix that joins \(R_{m,h}\) for all \(m\in[M]\). Then, from Assumption 2 and Definition 3, it follows that for all \((s^{\prime},s,a)\in\mathcal{S}\times\mathcal{S}\times A\), that

\[r_{h}(s,a) =\sum_{i\in[d]}W_{h}(s,i)G_{h}(a,i)\] \[=\sum_{i\in[d]}\left(\sum_{j\in[d],m\in[M]}B(i,j,m)G_{h,m}(a,j) \right)W_{h}(s,i)\] \[=\sum_{i,j\in[d],m\in[M]}B(i,j,m)G_{m,h}(a,j)W_{h}(s,i)\] \[=\tilde{W}_{h}^{\prime}(s)G_{h}(a)^{\top}\]and \(P(s^{\prime}|\cdot,\cdot)=\hat{G}_{h}U^{\prime}_{h}(s^{\prime})^{\top}\) (with the same argument) for some \(S\times dM\) matrices \(W^{\prime}_{h},U^{\prime}_{h}(s^{\prime})\) with entries bounded by \(\alpha\). Note that \(W^{\prime}_{h}\) and \(U^{\prime}_{h}(s^{\prime})\) are matrices that join \(W_{m,h}\) and \(U_{m,h}(s^{\prime})\), respectively, with entries that at most \(\alpha\) times larger than the original entry.

Therefore, it follows for all \((s^{\prime},s,a,h)\in\mathcal{S}\times\mathcal{S}\times\mathcal{A}\times[H]\),

\[|r_{h}(s,a)-\hat{\hat{G}}_{h}(a)R_{h}W^{\prime}_{h}(s)^{\top}| =|\tilde{G}_{h}(a)W^{\prime}_{h}(s)^{\top}-\hat{\hat{G}}_{h}(a)R_ {h}W^{\prime}_{h}(s)^{\top}|\] \[=|(\tilde{G}_{h}(a)-\hat{\hat{G}}_{h}(a)R_{h})W^{\prime}_{h}(s)|\] \[\leq\alpha|\sum_{m\in M}(G_{m,h}(a)-\hat{G}_{m,h}(a)R_{m,h})W_{m, h}(s)|\] \[\leq\alpha\sum_{m\in M}\|G_{m,h}(a)-\hat{G}_{m,h}(a)R_{m,h}\|_{2} \|W_{m,h}(s)\|_{2}\] \[\leq\alpha\sqrt{\frac{A}{d\mu}}\sum_{m\in M}\|G_{m,h}(a)-\hat{G}_ {m,h}(a)R_{m,h}\|_{2}\] \[\leq\sqrt{\frac{dHMS}{T}}\]

from the Cauchy-Schwarz inequality and our singular vector perturbation bound. From the same logic,

\[\left|\sum_{s^{\prime}\in S}(P_{h}(s^{\prime}|s,a)-U_{h}(s^{\prime },s)G_{h}(a)^{\top})\right| =|\sum_{s^{\prime}\in S}\tilde{G}_{h}(a)U^{\prime}_{h}(s^{\prime},s)-\hat{\hat{G}}_{h}(a)R_{h}U^{\prime}_{h}(s^{\prime},s)^{\top}|\] \[=|(\tilde{G}_{h}(a)-\hat{\hat{G}}_{h}(a)R_{h})\sum_{s^{\prime} \in S}U^{\prime}_{h}(s^{\prime},s)^{\top}|\] \[\leq\alpha|\sum_{m\in M}(G_{m,h}(a)-\hat{G}_{m,h}(a)R_{m,h})\sum_ {s^{\prime}\in S}U_{m,h}(s^{\prime},s)|\] \[\leq\alpha\sum_{m\in M}\|(G_{m,h}(a)-\hat{G}_{m,h}(a)R_{m,h})\|_{ 2}\|\sum_{s^{\prime}\in S}U_{m,h}(s^{\prime},s)^{\top}\|_{2}\] \[\leq\sqrt{\frac{dHMS}{T}}.\]

Therefore, \(\hat{\hat{G}}_{h}(a)\) satisfies Assumption 4 with \(\xi=\sqrt{\frac{dHHS}{T}}\). Then it follows that running LSVI-UCB-TR using \(\hat{\hat{G}}_{h}(a)\) with \(\delta^{\prime}=\delta/2\) during the target phase of the algorithm admits a regret bound of

\[Regret(T)\leq C"(\sqrt{d^{3}M^{3}H^{3}ST}+dMHT\sqrt{\frac{dMHS}{T}})\in\tilde {O}(\sqrt{d^{3}M^{3}H^{3}ST})\]

with probability at least \(1-\delta\) from a final union bound. Furthermore, the sample complexity in the source phase is

\[\tilde{O}\left(\frac{d^{4}\mu^{5}\kappa^{4}(S+A)M^{2}H^{4}\alpha^{2}T}{S} \right),\]

which proves our result. 

### Proof of Theorem 3

Proof.: Let the assumptions of Theorem 3 hold. Then, from [30], running LR-EVI with a sample complexity of \(C^{\prime}d^{7}\mu^{5}\kappa^{4}A(S+A)M^{2}H^{4}\alpha^{2}T\log(2SAdMH/\delta) /(Sd^{\prime\prime})\) on each source MDP results in \(\bar{Q}_{m,h}\) functions with singular value decomposition \(\bar{Q}_{m,h}=\hat{F}_{m,h}\hat{\Sigma}_{m,h}\hat{G}_{m,h}\) that are \(\sqrt{Hd^{\prime\prime}S}/(16\alpha\mu M\kappa d^{2}\sqrt{AT})\)-optimal with probability at least \(1-\delta/2\).

Since our estimates are \(\sqrt{Hd^{\prime\prime}S}/(16\alpha\mu M\kappa d^{2}\sqrt{AT})\)-optimal, it follows that \(\bar{\gamma}<1/2\) in the singular vector perturbation bound, Corollary 3. Since \(\|Q^{*}_{m,h}\|_{\infty}\geq C\), from Corollary 3, it follows that there exists rotation matrices \(R_{m,h}\) such that

\[\|(\hat{G}_{m,h}R_{m,h}-G_{m,h})(a)\|_{2}\leq\frac{\sqrt{Hd^{\prime \prime}\mu S}}{\alpha AM\sqrt{dT}}\]

for all \(a\in A\) where the optimal \(Q\) function have singular value decomposition \(Q_{m,h}^{*}=F_{m,h}\Sigma_{m,h}G_{m,h}^{\top}\).

Let \(\tilde{G}\) be the \(A\times dM\) matrix from joining all the source action latent factor matrices, and \(\hat{\tilde{G}}_{h}\) be the \(A\times dM\) matrix from joining all \(\bar{G}_{m,h}\). Let \(R_{h}\) be the \(d\times dM\) matrix that joins \(R_{m,h}\) for all \(m\in[M]\). Then, from Assumption 2 and Definition 3, it follows that for all \((s^{\prime},s,a)\in\mathcal{S}\times\mathcal{S}\times A\), that

\[r_{h}(s,a) =\sum_{i\in[d]}W_{h}(s,i)G_{h}(a,i)\] \[=\sum_{i\in[d]}\left(\sum_{j\in[d],m\in[M]}B(i,j,m)G_{h,m}(a,j) \right)W_{h}(s,i)\] \[=\sum_{i,j\in[d],m\in[M]}B(i,j,m)G_{m,h}(a,j)W_{h}(s,i)\] \[=\tilde{W}_{h}^{\prime}(s)G_{h}(a)^{\top}\]

and \(P(s^{\prime}|\cdot,\cdot)=\tilde{G}_{h}U_{h}^{\prime}(s^{\prime})^{\top}\) (with the same argument) for some \(S\times dM\) matrices \(W_{h}^{\prime},U_{h}^{\prime}(s^{\prime})\) with entries bounded by \(\alpha\). Note that \(W_{h}^{\prime}\) and \(U_{h}^{\prime}(s^{\prime})\) are matrices that join \(W_{m,h}\) and \(U_{m,h}(s^{\prime})\), respectively, with entries that at most \(\alpha\) times larger than the original entry.

Let \(E=\hat{\tilde{G}}_{h}-\tilde{G}_{h}\), where \(E=[E_{1}\ldots E_{M}]\,.\) We next bound the \(d^{\prime\prime}+1\) smallest singular value of \(\hat{\tilde{G}}_{h}\) with our singular vector perturbation bound ( \(\|E\|_{\infty}\leq\|E\|_{2,\infty}\)).

\[\sigma_{d^{\prime\prime}+1}(\hat{G}_{h})=\sigma_{d^{\prime\prime }+1}(\tilde{G}_{h})+\|E\|_{op}\leq\sqrt{AMM}\|E\|_{\infty}\leq\frac{\sqrt{Hd^ {\prime\prime}\mu S}}{\alpha\sqrt{AMT}}\]

With singular value decomposition \(\hat{\tilde{G}}_{h}=X\Sigma Y^{\top}\), let \(G_{h}^{d^{\prime\prime}}\) be the thresholded feature representation estimate, i.e., \(G_{h}^{d^{\prime\prime}}=X(:,:\,d^{\prime\prime})\Sigma(:\,d^{\prime\prime},: \,d^{\prime\prime})Y(:,d^{\prime\prime})^{\top}\). Therefore, it follows for all \((s^{\prime},s,a,h)\in\mathcal{S}\times\mathcal{S}\times\mathcal{A}\times[H]\),

\[|r_{h}(s,a) -G_{h}^{d^{\prime\prime}}(a)R_{h}W_{h}^{\prime}(s,:\,d^{\prime \prime})^{\top}|\] \[=|\tilde{G}_{h}(a)W_{h}^{\prime}(s)^{\top}-\hat{G}_{h}^{d^{ \prime\prime}}(a)R_{h}W_{h}^{\prime}(s,:\,d^{\prime\prime})^{\top}|\] \[\leq|\tilde{G}_{h}(a)W_{h}^{\prime}(s)^{\top}-\hat{\tilde{G}}_{h }(a)R_{h}W_{h}^{\prime}(s)^{\top}+|\hat{\tilde{G}}_{h}(a)R_{h}W_{h}^{\prime}(s )^{\top}-G_{h}^{d^{\prime\prime}}(a)R_{h}W_{h}^{\prime}(s,:\,d^{\prime\prime} )^{\top}|\] \[\leq\alpha|\sum_{m\in M}(G_{m,h}(a)-\hat{G}_{m,h}(a)R_{m,h})W_{m,h }(s)|+|\hat{\tilde{G}}_{h}(a,d^{\prime\prime}+1:)W_{h}^{\prime}(s,d^{\prime \prime}+1:)^{\top}|\] \[\leq\alpha\sum_{m\in M}\|G_{m,h}(a)-\hat{G}_{m,h}(a)R_{m,h}\|_{2} \|W_{m,h}(s)\|_{2}+\alpha\sigma_{d^{\prime\prime}+1}(\hat{\tilde{G}}_{h}) \sqrt{dM}\sum_{m\in[M]}\|W_{m,h}\|_{2}\] \[\leq\alpha\sqrt{\frac{A}{d\mu}}\left(\sum_{m\in M}\|G_{m,h}(a)- \hat{G}_{m,h}(a)R_{m,h}\|_{2}+\sigma_{d^{\prime\prime}+1}(\hat{\tilde{G}}_{h}) \sqrt{dM}\right)\] \[\leq\sqrt{\frac{d^{\prime\prime}HS}{T}}\]

from the Cauchy-Schwarz inequality and our singular vector perturbation bound. From the same logic,

\[\left|\sum_{s^{\prime}\in S}(P_{h}(s^{\prime}|s,a)-U_{h}(s^{\prime},s)G_{h}^{d ^{\prime\prime}}(a)^{\top})\right|\leq\sqrt{\frac{d^{\prime\prime}HS}{T}}.\]Therefore, \(G_{h}^{d^{\prime\prime}}(a)\) satisfies Assumption 4 with \(\xi=\sqrt{\frac{d^{\prime\prime}HS}{T}}\). Then it follows that running LSVICUB-TR using \(G_{h}^{d^{\prime\prime}}(a)\) with \(\delta^{\prime}=\delta/2\) during the target phase of the algorithm admits a regret bound of

\[Regret(T)\leq C"(\sqrt{(d^{\prime\prime})^{3}H^{3}ST}+d^{\prime\prime}HT\sqrt{ \frac{d^{\prime\prime}HS}{T}})\in\tilde{O}(\sqrt{(d^{\prime\prime})^{3}H^{3} ST})\]

with probability at least \(1-\delta\) from a final union bound. Furthermore, the sample complexity in the source phase is

\[\tilde{O}\left(\frac{d^{7}\mu^{5}\kappa^{4}(A+A^{2})M^{3}H^{4}\alpha^{2}T}{Sd^ {\prime\prime}}\right),\]

which proves our result.

We now prove Theorem 4.

Proof.: Suppose the assumption stated in Theorem 4 hold. Then, from [30], running LR-EVI with a sample complexity of \(C^{d}\delta^{\mu}\kappa^{4}(S+A)M^{2}H^{4}\alpha^{2}T\log(2SAdMH/\delta)/S\) on each source MDP results in \(\tilde{Q}_{m,h}\) functions with singular value decomposition \(\tilde{Q}_{m,h}=\tilde{F}_{m,h}\hat{\Sigma}_{m,h}\tilde{G}_{m,h}\) that are \(\sqrt{HS}/(16\alpha\mu\kappa\sqrt{d^{2}M^{2}T})\)-optimal with probability at least \(1-\delta/2\).

Since our estimates are \(\sqrt{HS}/(16\alpha\kappa\sqrt{d^{2}M^{2}T})\)-optimal, it follows that \(\bar{\gamma}<1/2\) in the singular vector perturbation bound, Corollary 3. Since \(\|Q_{m,h}^{*}\|_{\infty}\geq C\), from Corollary 3, it follows that there exists rotation matrices \(R_{m,h}\) such that

\[\|(\hat{G}_{m,h}R_{m,h}-G_{m,h})(a)\|_{2}\leq\frac{\sqrt{Hd\mu S}}{\alpha\sqrt {AM^{2}T}}\]

for all \(a\in A\) where the optimal \(Q\) function have singular value decomposition \(Q_{m,h}^{*}=F_{m,h}\Sigma_{m,h}G_{m,h}^{\top}\).

Let \(\tilde{G}\) be the \(A\times dM\) matrix from joining all the source action latent factor matrices, and \(\hat{\tilde{G}}_{h}\) be the \(A\times dM\) matrix from joining all \(\tilde{G}_{m,h}\). Let \(R_{h}\) be the \(d\times dM\) matrix that joins \(R_{m,h}\) for all \(m\in[M]\). Then, from Assumption 2 and Definition 3, it follows that for all \((s^{\prime},s,a)\in\mathcal{S}\times\mathcal{S}\times A\), that

\[r_{h}(s,a) =\sum_{i\in[d]}W_{h}(s,i)G_{h}(a,i)\] \[=\sum_{i\in[d]}\left(\sum_{j\in[d],m\in[M]}B(i,j,m)G_{h,m}(a,j) \right)W_{h}(s,i)\] \[=\sum_{i,j\in[d],m\in[M]}B(i,j,m)G_{m,h}(a,j)W_{h}(s,i)\] \[=\tilde{W}_{h}^{\prime}(s)G_{h}(a)^{\top}\]

and \(P(s^{\prime}|\cdot,\cdot)=\tilde{G}_{h}{U}_{h}^{\prime}{(s^{\prime})}^{\top}\) (with the same argument) for some \(S\times dM\) matrices \(W_{h}^{\prime},U_{h}^{\prime}(s^{\prime})\) with entries bounded by \(\alpha\). Note that \(W_{h}^{\prime}\) and \(U_{h}^{\prime}(s^{\prime})\) are matrices that join \(W_{m,h}\) and \(U_{m,h}(s^{\prime})\), respectively, with entries that at most \(\alpha\) times larger than the original entry. Let \(\tilde{G}^{t}\) be the thresholded feature mapping, where \(t\) is defined as \(t=\min_{t^{\prime}\in\mathcal{T}}t^{\prime}\) for

\[\mathcal{T}=\left\{t\in[dM]|\sigma_{t+1}\leq\sqrt{\frac{tHSd\mu}{\alpha^{2}T (dM-t)M^{2}A}}\right\}.\]Therefore, with the same logic as the previous proof, it follows for all \((s^{\prime},s,a,h)\in\mathcal{S}\times\mathcal{S}\times\mathcal{A}\times[H]\),

\[|r_{h}(s,a)-\hat{\hat{G}}^{t}(a)R_{h}W_{h}^{\prime}(s)^{\top}| =|\tilde{G}_{h}(a)W_{h}^{\prime}(s)^{\top}-\hat{\hat{G}}^{t}R_{h}W_ {h}^{\prime}(s)^{\top}|\] \[\leq|(\tilde{G}_{h}(a)-\hat{\hat{G}}_{h}(a)R_{h})W_{h}^{\prime}(s )|+|\hat{\hat{G}}_{h}(a)R_{h}W_{h}^{\prime}(s)^{\top}-\hat{\hat{G}}_{h}^{t}(a)( R_{h}W_{h}^{\prime}(s))^{t,\top}|\] \[\leq\sqrt{\frac{HS}{T}}+|\hat{\hat{G}}_{h}(a,t+1:)(R_{h}W_{h}^{ \prime})(s,t+1:)^{\top}|\] \[\leq\sqrt{\frac{HS}{T}}+\sigma_{t+1}\alpha\sqrt{dM-t}\|W_{h}^{ \prime}\|_{2}\] \[\leq\sqrt{\frac{HS}{T}}+\sqrt{\frac{tHS}{T}}\] \[\leq 2\sqrt{\frac{tHS}{T}}\]

where the second and third inequalities comes from the same logic used in the proofs of Theorems 2 and 4, and the fourth inequality comes from the definition of \(t\). From the same logic,

\[\left|\sum_{s^{\prime}\in S}(P_{h}(s^{\prime}|s,a)-U_{h}(s^{\prime},s)\hat{ \hat{G}}^{t}(a)^{\top})\right|\leq 2\sqrt{\frac{tHS}{T}}.\]

Therefore, \(\hat{\tilde{G}}_{h}(a)\) satisfies Assumption 4 with \(\xi=\sqrt{\frac{tHS}{T}}\). Then it follows that running LSVI-UCB-TR using \(\hat{\hat{G}}_{h}^{t}(a)\) with \(\delta^{\prime}=\delta/2\) during the target phase of the algorithm admits a regret bound of

\[Regret(T)\leq C"(\sqrt{t^{3}H^{3}ST}+2tHT\sqrt{\frac{tHS}{T}})\in\tilde{O}( \sqrt{t^{3}H^{3}ST})\]

with probability at least \(1-\delta\) from a final union bound. Furthermore, the sample complexity in the source phase is

\[\tilde{O}\left(\frac{d^{5}\mu^{5}\kappa^{4}(S+A)M^{3}H^{4}\alpha^{2}T}{S} \right),\]

which proves our result. 

## Appendix F \((S,d,A)\) Tucker Rank Setting

In this section, we present the assumptions, algorithm, and results for the \((S,d,A)\) Tucker rank setting. In this setting, we assume the following structure on the source and target MDPs.

**Assumption 5**.: _In each of the \(M\) source MDPs, the reward functions have rank \(d\), and the transition kernels have Tucker rank \((S,d,A)\). Let the target MDP's reward functions have rank \(d^{\prime}\) and transition kernels have Tucker rank \((S,d^{\prime},A)\) where \(d^{\prime}\leq dM\). Thus, there exists \(S\times d\times A\) tensors \(U_{m,h},\)\(S\times d^{\prime}\times A\) tensors \(U_{h}\), \(S\times d\)\(\mu\)-incoherent matrices \(F_{m,h},S\times d^{\prime}\)\(\mu\)-incoherent matrices \(F_{h}\) with orthonormal columns, and \(A\times d\) matrices \(W_{m,h},\)\(A\times d^{\prime}\) matrices \(W_{h}\) such that_

\[P_{m,h}(s^{\prime}|s,a)=\sum_{i\in[d]}F_{m,h}(s,i)U_{m,h}(s^{\prime},a,i), \quad r_{m,h}(s,a)=\sum_{i\in[d]}F_{m,h}(s,i)W_{m,h}(a,i)\]

_and_

\[P_{h}(s^{\prime}|s,a)=\sum_{i\in[d^{\prime}]}F_{h}(s,i)U_{h}(s^{\prime},a,i), \quad r_{h}(s,a)=\sum_{i\in[d^{\prime}]}F_{h}(s,i)W_{h}(a,i)\]

_where \(\|\sum_{s^{\prime}\in\mathcal{S}}g(s^{\prime})U_{m,h}(s^{\prime},:,a)\|_{2}, \quad\|\sum_{s^{\prime}\in\mathcal{S}}g(s^{\prime})U_{h}(s^{\prime},:,a)\|_{2},\|W_{h}(a)\|_{2},\|W_{m,h}(a)\|_{2}\quad\leq\sqrt{S/(d\mu)}\) for all \(s\in\mathcal{S}\), \(a\in\mathcal{A}\), \(h\in[H]\), and \(m\in[M]\) for any function \(g:\mathcal{S}\rightarrow[0,1]\)._

Similarly, to allow transfer of a latent representation between the source and target MDPs, we assume that the subspace spanned by the set of all source latent factors contains the space spanned by the set of target latent factors.

**Assumption 6**.: _Suppose Assumption 5 holds. The target MDP latent factors \(F_{h}\) and source MDP latent factors \(F_{m,h}\) satisfy for all \(h\in[H]\), \(\operatorname{Span}(\{F_{h}(:,i)\}_{i\in[d^{\prime}]})\subseteq\operatorname{ Span}(\{F_{m,h}(:,i)\}_{i\in[d],m\in[M]})\)._Before defining our transfer-ability coefficient, we first introduce the following notation. We define the set \(\mathcal{B}_{h}(F_{h},\{F_{h,m}\}_{m\in M})\) to contain all coefficients of the linear combinations, i.e., \(B_{h}\in\mathbb{R}^{d^{\prime},d,M}\) and \(B_{h}\in\mathcal{B}_{h}(F_{h},\{F_{h,m}\}_{m\in M})\) if for all \(i\in[d^{\prime}],h\in[H]\), \(F_{h}(\cdot,i)=\sum_{j\in[d],m\in[M]}B_{h}(i,j,m)F_{m,h}(\cdot,j)\). Furthermore, as the source MDPs latent factors of \(Q^{*}_{m,h}\) are not unique, we define \(\alpha\) to measure the difficulty given the best set of latent factors from the source MDPs. Thus, \(\alpha\) precisely measures the challenge involved in transferring the latent representation.

**Definition 4** (Transfer-ability Coefficient).: _Given a transfer RL problem that satisfies Assumptions 5, 6, and 3, let \(\mathcal{F}\) be the set of all \(\mathbb{R}^{S\times d}\) matrices with orthonormal columns that span the column space of \(Q^{*}_{m,h}\) for all \(m\in[M],h\in[H]\). Then, we define \(\alpha\) as_

\[\alpha\coloneqq\max_{h\in[H]}\min_{F\in\mathcal{F}}\min_{B\in\mathcal{B}_{h}(F _{h},F)}\max_{i\in[d^{\prime}],j\in[d],m\in[M]}|B(i,j,m)|.\]

We now prove our lower bound in this Tucker rank setting.

### Information Theoretic Lower Bound

To formalize the importance of \(\alpha\) in transfer learning, we prove a lower bound that shows that a dependence on \(\alpha\) in the sample complexity of the source phase is necessary to benefit from transfer learning.

**Theorem 7**.: _There exist two transfer RL instances such that (i) they satisfy Assumptions 5 and 6, (ii) they cannot be distinguished without observing \(\Omega(\alpha^{2})\) samples in the source phase, and (3) they have target state latent features that are orthogonal to each other._

Proof.: Suppose all source and target MDPs \((S,A,P,H,r)\) share the same state space, action space, and horizon \(H=1\) and assume that \(\mathcal{S}=\mathcal{A}=[2n]\) for some \(n\in\mathbb{N}_{+}\). For ease of notation, we let \(s_{1}\) and \(a_{1}\) refer to any state and action in \([n]\), respectively, and \(s_{2}\) and \(a_{2}\) refer to any state and action in \(\{n+1,\ldots 2n\}\), respectively. We will present the latent factors and optimal Q functions as block vectors and matrices with \(s_{1},s_{2},a_{1},a_{2}\) as the blocks containing \(n\) entries. The initial state distribution in the target MDP is uniform over \(\mathcal{S}\). We now present two transfer RL problems with similar \(Q\) functions (with rows \(s_{1},s_{2}\) and columns \(a_{1},a_{2}\)) that satisfy Assumptions 5 and 6 but have orthogonal target state latent factors. For ease of notation, the superscript \(i\) of \(Q^{*,i}_{m,h},Q^{*,i}_{h}\) denotes transfer RL problem. Then, the optimal \(Q\) functions for transfer RL problem one are

\[Q^{*,1}_{1,1} =n\begin{bmatrix}\sqrt{1/(2n)}\\ \sqrt{1/(2n)}\end{bmatrix}\begin{bmatrix}\sqrt{1/n}&0\end{bmatrix}=\begin{bmatrix} 1/\sqrt{2}&0\\ 1/\sqrt{2}&0\end{bmatrix},\quad Q^{*,1}_{2,1}=n\begin{bmatrix}\sqrt{1/(2n)}\\ \sqrt{1/(2n)}\end{bmatrix}\begin{bmatrix}0&\sqrt{1/n}\end{bmatrix}=\begin{bmatrix} 0&1/\sqrt{2}\\ 0&1/\sqrt{2}\end{bmatrix},\] \[Q^{*,1}_{1} =n\begin{bmatrix}\sqrt{1/(2n)}\\ \sqrt{1/(2n)}\end{bmatrix}\begin{bmatrix}\sqrt{\frac{1}{n}}&-\sqrt{\frac{1}{n} }\end{bmatrix}=\begin{bmatrix}1/2&-1/2\\ 1/2&-1/2\end{bmatrix},\]

and the optimal \(Q\) functions for transfer RL problem two are

\[Q^{*,2}_{1,1} =n\begin{bmatrix}\sqrt{1/(2n)}\\ \sqrt{1/(2n)}\end{bmatrix}\begin{bmatrix}\sqrt{1/n}&0\end{bmatrix}=\begin{bmatrix} 1/\sqrt{2}&0\\ 1/\sqrt{2}&0\end{bmatrix}\quad Q^{*,2}_{2,1}=nF^{\prime}\begin{bmatrix}0&\sqrt{ 1/n}\end{bmatrix}=\begin{bmatrix}0&(1+1/\alpha)/(c\sqrt{2})\\ 0&(1-1/\alpha)/(c\sqrt{2})\end{bmatrix},\] \[Q^{*,2}_{1} =n\begin{bmatrix}\sqrt{1/(2n)}\\ -\sqrt{1/(2n)}\end{bmatrix}\begin{bmatrix}\sqrt{\frac{1}{n}}&-\sqrt{\frac{1}{n} }\end{bmatrix}=\begin{bmatrix}1/2&-1/2\\ -1/2&1/2\end{bmatrix},\]

where

\[F^{\prime}=\begin{bmatrix}(\sqrt{1/(2n)}+\sqrt{1/(2n\alpha^{2})}/\sqrt{1+1/ \alpha^{2}}&(\sqrt{1/(2n)}-\sqrt{1/(2n\alpha^{2})}/\sqrt{1+1/\alpha^{2}} \end{bmatrix}\]

and \(c\in(1,2dM)\) because \(\alpha\geq 1/(dM)\) from Lemma 1 for \(\alpha>0\). The incoherence \(\mu\), rank \(d\), and condition number \(\kappa\) of the above \(Q\) functions are \(O(1)\). Furthermore, the above construction satisfies Assumption 5 with Tucker rank \((S,1,A)\) and Assumption 6 because the source and target MDPs share the same state latent factor \([\sqrt{1/(2n)},\sqrt{1/(2n)}]\) in the first transfer RL problem while in the second transfer RL problem,

\[\begin{bmatrix}\sqrt{1/(2n)}&-\sqrt{1/(2n)}\end{bmatrix}=-\alpha\begin{bmatrix} \sqrt{1/(2n)}&\sqrt{1/(2n)}\end{bmatrix}+\alpha cF^{{}^{\prime}\top}.\]

Furthermore, note that the target latent factors,

\[F^{1}=\begin{bmatrix}\sqrt{1/(2n)}\\ \sqrt{1/(2n)}\end{bmatrix}\text{ and }F^{2}=\begin{bmatrix}\sqrt{1/(2n)}\\ -\sqrt{1/(2n)}\end{bmatrix}\]are orthogonal to each other.

The learner is given either transfer RL problem one or two with equal probability with a generative model in the source problem. When interacting with the source MDPs, the learner specifies a state-action pair \((s,a)\) and source MDP \(m\) and observes a realization of a shifted and scaled Bernoulli random variable \(X\). The distribution of \(X\) is that \(X=1\) with probability \((Q_{m,1}^{*,i}(s,a)+1)/2\) and \(X=-1\) with probability \((1-Q_{m,1}^{*,i}(s,a))/2)\). Furthermore, the learner is given the knowledge that the state latent features lie in the function class \(\mathcal{F}=\{F^{1},F^{2}\}\) and must use the knowledge obtained from interacting with the source MDP to choose one to use in the target MDP, which is no harder than receiving no information about the function class.

To distinguish between the two transfer RL problems, one needs to differentiate between \(Q_{2,1}^{*,1}\) and \(Q_{2,1}^{*,2}\). By construction, the magnitude of the largest entrywise difference between \(Q_{2,1}^{*,1}\) and \(Q_{2,1}^{*,2}\) is lower bounded by \(\Omega(1/\alpha)\).

If the learner observes \(Z\) samples in the source MDPs, where \(Z\leq O(\alpha^{2})\) for some constant \(C>0\), then the probability of correctly identifying \(F\) is upper bounded by \(0.76\) (Lemma 5.1 with \(\delta=0.24\)[5]). Thus, if the learner does not observe \(\Omega(\alpha^{2})\) samples in the source phase, then the learner returns the incorrect feature mapping that is orthogonal to the true feature mapping with probability at least \(0.24\). 

Theorem 7 states that unless one incurs a sample complexity of \(\Omega(\alpha^{2})\) in the source phase, the latent representation \(F\) learned in the source phase is useless in the target phase as one would incur linear regret using \(F\).

Our algorithm in this setting is essentially the same as the one used in the \((S,S,d)\) Tucker rank setting except we construct our latent factors using the other set of singular vectors and compute Gram matrices and coefficients for each action instead of each state.

```
0:\(\{N_{h}\}_{h\in[H]}\)
1:for\(m\in[M]\)do
2: Run LR-EVI(\(\{N_{h}\}_{h\in[H]}\)) on source MDP \(m\) to obtain \(\bar{Q}_{m,h}\).
3: Compute the singular value decomposition of \(\bar{Q}_{m,h}=\hat{F}_{m,h}\hat{\Sigma}_{m,h}\hat{G}_{m,h}^{\top}\).
4: Compute feature mapping \(\hat{\hat{F}}=\hat{\hat{F}}_{h}\}_{h\in[H]}\) with \(\hat{\hat{F}}_{h}=\left\{\sqrt{\frac{S}{d\mu}}\hat{F}_{m,h}(\cdot;i)|i\in[d],m \in[M]\right\}\). ```

**Algorithm 3** Source Phase

```
0:\(\lambda,\beta_{k,h}^{a},\hat{\hat{F}}\)
1:for\(k\in[K]\)do
2: Receive initial state \(s_{1}^{k}\).
3:for\(h=H,\ldots,1\)do
4:for\(a\in\mathcal{A}\)do
5: Set \(\Lambda_{h}^{a}\leftarrow\sum_{t\in T_{k-1,h}^{a}}\hat{\hat{F}}_{h}(s_{h}^{t}) \hat{\hat{F}}_{h}(s_{h}^{t})^{\top}+\lambda\mathbf{I}\).
6:/* Compute Gram matrix \(\Lambda_{h}^{a}\) */
7: Set \(w_{h}^{a}\leftarrow(\Lambda_{h}^{a})^{-1}\sum_{t\in T_{k-1,h}^{a}}\hat{\hat{F}}_ {h}(s_{h}^{t})\left[r_{h}(s_{h}^{t},a)+\max_{a^{\prime}\in\mathcal{A}}Q_{h+1 }(s_{h+1}^{t},a^{\prime})\right]\).
8:/* Estimate \(w_{h}^{a}\) via regularized least squares */
9: Set \(Q_{h}(\cdot,\cdot)\leftarrow\min\Big{(}H,\langle\dot{w_{h}},\hat{\hat{F}}_{h}( \cdot)\rangle+\beta_{\hat{k},h}\sqrt{\hat{\hat{F}}_{h}(\cdot)^{\top}(\Lambda_ {h})^{-1}\hat{\hat{F}}_{h}(\cdot)}\Big{)}\).
10:/* Estimate \(w_{h}^{s}\) via regularized least squares */
11:for\(h\in[H]\)do
12: Take action \(a_{h}^{k}\leftarrow\max_{a\in\mathcal{A}}Q_{h}(s_{h}^{k},a)\), and observe \(s_{h+1}^{k}\). ```

**Algorithm 4** Target Phase: LSVI-UCB-(S, d, A)

We first present the theoretical guarantees of LSVI-UCB-(S, d, A) and defer their proofs to AppendixBefore proving our main positive result in the \((S,d,A)\) Tucker rank setting, we first present the definitions and theorems of LSVI-UCB-(S, d, A) and defer their proofs to Appendix J. If one is given the true latent low-rank representation of each state, then LSVI-UCB-(S, d, A) admits the following regret bound.

**Theorem 8**.: _Assume that Assumption 5 holds and the learner is given the true latent state representation \(F=\{F_{h}\}_{h\in[H]}\). Then, there exists a constant \(c>0\) such that for any \(\delta\in(0,1)\), if we set \(\lambda=1,\beta_{k,h}^{a}=cdH\sqrt{\iota},\iota=\log(2dTA/\delta)\), then with probability at least \(1-\delta\), the total regret of LSVI-UCB-TR is \(\tilde{O}(\sqrt{d^{3}AH^{3}T})\)._

Theorem 8 states that LSVI-UCB-TR completely removes the regret bound's dependence on the size of the state space by utilizing the latent state representation \(F\). To measure the algorithm's robustness with respect to misspecification error of the low-rank representation, we first define \(\xi\)-approximate \((S,d,A)\) Tucker rank MDPs (similarly to Assumption B from [17]).

**Assumption 7** (\(\xi\)-approximate \((S,d,A)\) Tucker rank MDP).: _Let \(\xi\in[0,1]\). Then, an MDP \((\mathcal{S},\mathcal{A},P,r,H)\) is a \(\xi\)-approximate \((S,d,A)\) Tucker rank MDP with given feature map \(F\) if there exist \(H\) unknown \(A\)-by-\(d\) matrices \(W=\{W_{h}\}_{h\in[H]}\) and \(S\)-by-\(d\)-by-\(A\) tensors \(U=\{U_{h}\}_{h\in[H]}\) such that_

\[|r_{h}(s,a)-F_{h}(s)^{\top}W_{h}(a)|\leq\xi,\]

\[\left|\sum_{s^{\prime}\in S}(P_{h}(s^{\prime}|s,a)-F_{h}(s)^{\top}U_{h}(s^{ \prime},a))\right|\leq\xi\]

_for all \(h\in[H]\) where \(\|W_{h}\|_{2},\|\sum_{s^{\prime}\in\mathcal{S}}g(s^{\prime})U_{h}(s^{\prime}, a)\|_{2}\leq\sqrt{S/(d\mu)}\) for any function \(g:\mathcal{S}\to[0,1]\)._

The following result shows that similar to LSVI-UCB, our algorithm is robust to the misspecified setting.

**Theorem 9**.: _Suppose that the learner is given a feature mapping \(F^{\prime}\) that satisfies Assumption 7. Then, using \(F=\sqrt{S/(d\mu)}F^{\prime}\) as the feature mapping for any \(\delta\in(0,1)\) and \(\lambda=1,\beta_{k,h}^{a}=O(Hd\sqrt{\iota}+\xi\sqrt{d|T_{k,h}^{a}|}H)\) for \(\iota=\log(2dTA/\delta)\), the regret of LSVI-UCB-TR is \(\tilde{O}(\sqrt{d^{3}H^{3}AT}+\xi dHT)\) with probability at least \(1-\delta\)._

The above theorem states that the misspecification error adds a term that is linear in \(T\) to the regret bound of LSVI-UCB-(S, d, A). Thus, if one's latent state representation is close enough, i.e., \(\xi\) is so small that there exists some positive constant \(c\) such that \(\xi dHT\leq c\sqrt{d^{3}H^{3}AT}\), then LSVI-UCB-(S, d, A) admits a regret bound that is independent of the state space, which is the motivation for our main result. We now present our main theorem.

**Theorem 10**.: _Suppose Assumptions 5, 6, and 3 hold, and set \(\delta\in(0,1)\). Furthermore, assume that, for any \(\epsilon\in(0,\sqrt{AH/T})\), \(Q_{m,h}=r_{m,h}+P_{m,h}V_{m,h+1}\) has rank \(d\) and is \(\mu\)-incoherent with condition number \(\kappa\) for all \(\epsilon\)-optimal value functions \(V_{h+1}\). Also, let \(\lambda=1\) and \(\beta_{k,h}^{a}\) be a function of \(d,H,|T_{k,h}^{a}|,|A|,M,T\). Then, for \(T\geq\frac{A}{\alpha^{2}}\), using at most \(\tilde{O}\left(d^{4}\iota^{5}\kappa^{4}(S/A+1)M^{2}H^{4}\alpha^{2}T\right)\) samples in the source problems, our algorithm has regret at most \(\tilde{O}\left(\sqrt{(dMH)^{3}AT}\right)\) with probability at least \(1-\delta\)._

Theorem 10 states that using transfer learning improves the performance on the target problem by removing the regret bound's dependence on the state space. Thus, with enough samples from the source MDPs one can recover a regret bound in the target problem that matches the best regret bound for any algorithm in the \((S,d,A)\) Tucker rank setting with **known** latent feature representation \(F\) concerning \(A\) and \(T\). We now prove Theorem 10.

Proof.: Suppose the assumption stated in Theorem 10 hold. Then, from [30], running LR-EVI with a sample complexity of \(C^{\prime}d^{4}\mu^{5}\kappa^{4}(S+A)MH^{4}\alpha^{2}T\log(2SAdMH/\delta)/A\) on each source MDP results in \(\tilde{Q}_{m,h}\) functions with singular value decomposition \(\tilde{Q}_{m,h}=\hat{F}_{m,h}\hat{\Sigma}_{m,h}\hat{G}_{m,h}\) that are \(\sqrt{HA}/(16\alpha\mu\kappa\sqrt{dMT})\)-optimal with probability at least \(1-\delta/2\).

Since our estimates are \(\sqrt{HA}/(16\alpha\sqrt{dMT})\)-optimal, it follows that \(\bar{\gamma}<1/2\) in the singular vector perturbation bound, Corollary 3. Since \(\|Q^{*}_{m,h}\|_{\infty}\geq C\), from Corollary 3, it follows that there exists rotation matrices \(R_{m,h}\) such that

\[\|(\hat{F}_{m,h}R_{m,h}-F_{m,h})(s)\|_{2}\leq\frac{d\sqrt{H\mu A}}{\alpha\sqrt{ SMT}}\]

for all \(s\in\mathcal{S}\) where the optimal \(Q\) function have singular value decomposition \(Q^{*}_{m,h}=F_{m,h}\Sigma_{m,h}G_{m,h}^{\top}\).

Let \(\tilde{F}\) be the \(S\times dM\) matrix from joining all the source state latent factor matrices, and \(\hat{\tilde{F}}_{h}\) be the \(S\times dM\) matrix from joining all \(\bar{F}_{m,h}\). Let \(R_{h}\) be the \(d\times dM\) matrix that joins \(R_{m,h}\) for all \(m\in[M]\). Then, from Assumption 6 and Definition 4, it follows that for all \((s^{\prime},s,a)\in\mathcal{S}\times\mathcal{S}\times\mathcal{A}\), that

\[r_{h}(s,a) =\sum_{i\in[d]}F_{h}(s,i)W_{h}(a,i)\] \[=\sum_{i\in[d]}\left(\sum_{j\in[d],m\in[M]}B(i,j,m)F_{h,m}(s,j) \right)W_{h}(a,i)\] \[=\sum_{i,j\in[d],m\in[M]}B(i,j,m)F_{m,h}(s,j)W_{h}(a,i)\] \[=\tilde{F}_{h}(s)W_{h}^{\prime}(a)^{\top}\]

and \(P(s^{\prime}|\cdot,\cdot)=\tilde{F}_{h}U_{h}^{\prime}(s^{\prime})^{\top}\) (with the same argument) for some \(A\times dM\) matrices \(W_{h}^{\prime},U_{h}^{\prime}(s^{\prime})\). Note that \(W_{h}^{\prime}\) and \(U_{h}^{\prime}(s^{\prime})\) are matrices that join \(W_{m,h}\) and \(U_{m,h}(s^{\prime})\), respectively, with entries that at most \(\alpha\) times larger than the original entry.

Therefore, it follows for all \((s^{\prime},s,a,h)\in\mathcal{S}\times\mathcal{S}\times\mathcal{A}\times[H]\),

\[|r_{h}(s,a)-\hat{\tilde{F}}_{h}(s)R_{h}W_{h}^{\prime}(a)^{\top}| =|\tilde{F}_{h}(s)W_{h}^{\prime}(a)^{\top}-\hat{\tilde{F}}_{h}(s) R_{h}W_{h}^{\prime}(a)^{\top}|\] \[=|(\tilde{F}_{h}(s)-\hat{\tilde{F}}_{h}(s)R_{h})W_{h}^{\prime}(a)|\] \[\leq\alpha|\sum_{m\in M}(F_{m,h}(s)-\hat{F}_{m,h}(s)R_{m,h})W_{m, h}(a)|\] \[\leq\alpha\sum_{m\in M}\|F_{m,h}(s)-\hat{F}_{m,h}(s)R_{m,h}\|_{2} \|W_{m,h}(a)\|_{2}\] \[\leq\alpha\sqrt{\frac{S}{d\mu}}\sum_{m\in M}\|F_{m,h}(s)-\hat{F}_ {m,h}(s)R_{m,h}\|_{2}\] \[\leq\sqrt{\frac{dHMA}{T}}\]

from the Cauchy-Schwarz inequality and our singular vector perturbation bound. From the same logic,

\[\left|\sum_{s^{\prime}\in S}(P_{h}(s^{\prime}|s,a)-F_{h}(s)^{ \top}U_{h}(s^{\prime},a))\right| =|\tilde{F}_{h}(s)U_{h}^{\prime}(s^{\prime},a)-\hat{\tilde{F}}_{h }(s)R_{h}U_{h}^{\prime}(s^{\prime},a)^{\top}|\] \[=|(\tilde{F}_{h}(s)-\hat{\tilde{F}}_{h}(s)R_{h})\sum_{s^{\prime} \in\mathcal{S}}U_{h}^{\prime}(s^{\prime},a)^{\top}|\] \[\leq\alpha|\sum_{m\in M}(F_{m,h}(s)-\hat{F}_{m,h}(s)R_{m,h})\sum_ {s^{\prime}\in\mathcal{S}}U_{m,h}(s^{\prime},a)|\] \[\leq\alpha\sum_{m\in M}\|F_{m,h}(s)-\hat{F}_{m,h}(s)R_{m,h}\|_{2} \|\sum_{s^{\prime}\in\mathcal{S}}U_{m,h}(s^{\prime},a)^{\top}\|_{2}\] \[\leq\sqrt{\frac{dHMA}{T}}.\]Therefore, \(\hat{\hat{F}}_{h}(s)\) satisfies Assumption 7 with \(\xi=\sqrt{\frac{dMHA}{T}}\). Then it follows that running LSVICUB-TR using \(\hat{\hat{F}}_{h}(s)\) with \(\delta^{\prime}=\delta/2\) during the target phase of the algorithm admits a regret bound of

\[Regret(T)\leq C"(\sqrt{d^{3}M^{3}H^{3}AT}+dMH\sqrt{\frac{dMHA}{T}})\in\tilde{O }(\sqrt{d^{3}M^{3}H^{3}AT})\]

with probability at least \(1-\delta\) from a final union bound. Furthermore, the sample complexity in the source phase is

\[\tilde{O}\left(\frac{d^{4}\mu^{5}\kappa^{4}(S+A)M^{2}H^{4}\alpha^{2}T}{A} \right),\]

which proves our result. 

### Simulator to Simulator Transfer RL

In this section, we consider the same transfer reinforcement learning setting with the low Tucker rank assumptions except the learner is given access to the generative model in both the source and target problems. In this setting, we isolate the statistical difficulty of the transfer RL problem by removing the challenge of exploration. Thus, instead of regret, the goal in this setting is to learn an \(\epsilon\)-optimal \(Q\) function in the target phase using as few samples in the source and target phases as possible. The main benefit of having access to a generative model in the target phase is that the learner can choose the best states to observe covariates from when performing the regression step at each time step. In this setting, our algorithm still uses LR-EVI in the source phase (Algorithm 3 and adapts the algorithm from [22] for the finite horizon setting in the target phase. Learning \(Q^{*}\) over the support of an approximately optimal design \(\rho\) on our latent factors \(\hat{\hat{F}}\) allows us to construct a good estimate of \(Q^{*}\) over all state-action pairs. The optimal design problem that we solve in this algorithm is

\[G(\rho)=\sum_{s\in S}\rho(s)\hat{\hat{F}}(s)\hat{\hat{F}}(s)^{\top},\quad g( \rho)=\max_{s\in S}\|\hat{\hat{F}}\|_{G(\rho)^{-1}}\]

where \(\rho\) is a probability distribution over \(S\). From Theorems 4.3 and 4.4 [22], one can compute a \(\rho\) such that \(g(\rho)\leq 2d\), with the core set of states, or anchor states, \(\mathcal{S}^{\#}=\{s\in S|\rho(s)>0\}\) having size at most \(4d\log(\log(d))+16\) in a polynomial number of computations. This ensures that both the error amplification from using the low-rank structure and the number of states we need to observe \(Q^{*}\) is not too large. Our algorithm is tailored to the \((S,d,A)\) Tucker rank setting but can easily be modified for the other Tucker rank settings.

```
1:\(\{N_{h}^{t}\}_{h\in[H]},\hat{\hat{F}}\)
2:for\(h=H,\ldots,1\)do
3: Compute \(\mathcal{S}_{h}^{\#},\rho_{h}\) according to the above optimal design problem with \(\hat{\hat{F}}_{h}\).
4:for\((s,a)\in\mathcal{S}_{h}^{\#}\times\mathcal{A}\)do
5: Collect \(N_{h}^{t}\) samples of the reward function and one-step transition to estimate \(\hat{Q}_{h}(s,a)\) with \[\hat{Q}_{h}(s,a)=\hat{r}_{h}(s,a)+\mathbb{E}_{s^{\prime}\sim\hat{P}_{h}(\cdot |s,a)}[\hat{V}_{h+1}(s^{\prime})],\] where \(\hat{r}_{h}(s,a)\) denotes the empirical average of the \(N_{h}^{t}\) samples of the reward function, and \(\hat{P}_{h}(\cdot|s,a)\) denotes the empirical distribution over the \(N_{h}^{t}\) samples.
6: Use the linear structure to estimate \(\bar{Q}\) over all state-action pairs with \[\hat{\theta}_{a,h}=G(\rho)^{-1}\sum_{s\in\mathcal{S}_{h}^{\#}}\rho(s)\hat{ \hat{F}}_{h}(s)\hat{Q}_{h}(s,a),\quad\bar{Q}_{h}(s,a)=\hat{\hat{F}}_{h}(s) \theta_{a,h}.\]
7: Compute the value function and policy using the \(Q\) function, \[\hat{V}_{h}(s)=\max_{a\in\mathcal{A}}\bar{Q}_{h}(s,a),\quad\hat{\pi}_{h}(s)= \arg\max_{a\in\mathcal{A}}\bar{Q}_{h}(s,a).\] ```

**Algorithm 5** Target PhaseIn the source phase, our algorithm learns estimates of \(Q_{m,h}^{*}\) and then computes the singular value decomposition to construct approximate feature mappings using the singular subspaces with respect to the state. In step \((a)\) of the target phase, one first approximately solves the optimal design problem to compute the anchor states. After obtaining estimates of the \(Q\) function on the anchor states, one uses the linear structure in step \((c)\) to estimate the full \(Q\) function over all states with the least squares solution. The above algorithm admits the following sample complexities for learning an \(\epsilon\)-optimal \(Q\) function in the target phase.

**Theorem 11**.: _Suppose Assumptions 5, 6, and 3 hold, and set \(\delta\in(0,1)\). Furthermore, assume that for any \(\epsilon>0,\)\(Q_{m,h}=r_{m,h}+P_{m,h}V_{m,h+1}\) has rank \(d\) and is \(\mu\)-incoherent with condition number \(\kappa\) for all \(\epsilon\)-optimal value functions \(V_{h+1}\). Then, for \(\epsilon\leq\alpha d^{2}\), using at most_

\[\tilde{O}\left(\frac{d^{7}\mu^{3}\kappa^{4}(S+A)H^{9}\alpha^{2}M^{3}}{\epsilon ^{2}}\right)\]

_samples in the source problems, Algorithm 5 learns an \(\epsilon\)-optimal policy in the target MDP using at most_

\[\tilde{O}\left(\frac{d^{2}M^{2}H^{5}A}{\epsilon^{2}}\right)\]

_samples with probability at least \(1-\delta\)._

Theorem 11 states that with enough samples in the source phase, one can remove the dependence on the size of the state space in the target phase; in tabular finite-horizon MDPs with \((S,d,A)\) Tucker rank transition kernels, one needs to observe at least \(\tilde{\Omega}(d(S+A)H^{3}/\epsilon^{2})\) samples from a generative model to learn an \(\epsilon\)-optimal policy. Furthermore, the source sample complexity's dependence on \(S,A,\) and \(H\) are reasonable while the dependence on \(\alpha\) is optimal due to our lower bound. We can easily construct similar algorithms that admit the corresponding theoretical guarantees in the other Tucker rank settings. We now prove Theorem 11.

Proof.: In the source phase, using

\[\tilde{O}\left(\frac{d^{6}\mu^{3}\kappa^{4}(S+A)H^{9}\alpha^{2}M^{4}}{ \epsilon^{2}}\right)\]

samples guarantees that we learn \(\epsilon/(128H^{2}\alpha M^{3/2}\kappa d^{2})\)-optimal \(\tilde{Q}_{m,h}=\hat{F}_{m,h}\hat{\Sigma}_{m,h}\hat{G}_{m,h}^{\top}\) functions for each source MDP with probability at least \(1-\delta/2\). Since \(\alpha d^{2}\geq\epsilon\) by assumption, we have \(\bar{\gamma}<1/2\) in the singular vector perturbation bound, Corollary 3. Since \(\|Q_{m,h}^{*}\|_{\infty}\geq C\), it follows from Corollary 3 that there exists rotation matrices \(R_{m,h}\) such that

\[\|F_{m,h}-\hat{F}_{m,h}R\|_{2,\infty}\leq\frac{\epsilon\sqrt{\mu}}{8H^{2} \alpha M^{3/2}\sqrt{S}}\]

holds for all \(m\in[M],h\in[H]\) with probability at least \(1-\delta/2\). Let \(\tilde{F}\) be the \(S\times dM\) matrix from joining all the source state latent factor matrices, and \(\hat{\tilde{F}}_{h}\) be the \(S\times dM\) matrix from joining all \(\hat{F}_{m,h}\). Let \(R_{h}\) be the \(d\times dM\) matrix that joins \(R_{m,h}\) for all \(m\in[M]\). Then, from Assumption 6 and Definition 4, it follows that for all \((s^{\prime},s,a)\in\mathcal{S}\times\mathcal{S}\times\mathcal{A}\), that

\[r_{h}(s,a) =\sum_{i\in[d]}F_{h}(s,i)W_{h}(a,i)\] \[=\sum_{i\in[d]}\left(\sum_{j\in[d],m\in[M]}B(i,j,m)F_{h,m}(s,j) \right)W_{h}(a,i)\] \[=\sum_{i,j\in[d],m\in[M]}B(i,j,m)F_{m,h}(s,j)W_{h}(a,i)\] \[=\tilde{F}_{h}(s)W_{h}^{\prime}(a)^{\top}\]

and \(P(s^{\prime}|\cdot,\cdot)=\tilde{F}_{h}U_{h}^{\prime}(s^{\prime})^{\top}\) (with the same argument) for some \(A\times dM\) matrices \(W_{h}^{\prime},U_{h}^{\prime}(s^{\prime})\). Note that \(W_{h}^{\prime}\) and \(U_{h}^{\prime}(s^{\prime})\) are matrices that join \(W_{m,h}\) and \(U_{m,h}(s^{\prime})\), respectively, with entries that at most \(\alpha\) times larger than the original entry. Therefore, it follows for all \((s^{\prime},s,a,h)\in\mathcal{S}\times\mathcal{S}\times\mathcal{A}\times[H]\),

\[|r_{h}(s,a)-\hat{\tilde{F}}_{h}(s)R_{h}W^{\prime}_{h}(a)^{\top}| =|\tilde{F}_{h}(s)W^{\prime}_{h}(a)^{\top}-\hat{\tilde{F}}_{h}(s)R _{h}W^{\prime}_{h}(a)^{\top}|\] \[=|(\tilde{F}_{h}(s)-\hat{\tilde{F}}_{h}(s)R_{h})W^{\prime}_{h}(a)|\] \[\leq\alpha|\sum_{m\in M}(F_{m,h}(s)-\hat{F}_{m,h}(s)R_{m,h})W_{m,h }(a)|\] \[\leq\alpha\sum_{m\in M}\|F_{m,h}(s)-\hat{F}_{m,h}(s)R_{m,h}\|_{2} \|W_{m,h}(a)\|_{2}\] \[\leq\alpha\sqrt{\frac{S}{d\mu}}\sum_{m\in M}\|F_{m,h}(s)-\hat{F}_ {m,h}(s)R_{m,h}\|_{2}\] \[\leq\frac{\epsilon}{8\sqrt{dM}H^{2}}\]

from the Cauchy-Schwarz inequality and our singular vector perturbation bound. From the same logic,

\[|PV(s,a)-[\hat{\tilde{F}}_{h}R_{h}U^{\prime}_{h}V](s,a)| =|\sum_{s^{\prime}\in S}P_{h}(s^{\prime}|s,a)V(s^{\prime})-\hat{ \tilde{F}}_{h}(s)R_{h}U^{\prime}_{h}(s^{\prime},a)^{\top}V(s^{\prime})|\] \[=|\sum_{s^{\prime}\in S}\tilde{F}_{h}(s)U^{\prime}_{h}(s^{\prime},a)^{\top}V(s^{\prime})-\hat{\tilde{F}}_{h}(s)R_{h}U^{\prime}_{h}(s^{\prime},a)^{\top}V(s^{\prime})|\] \[\leq H|(\tilde{F}_{h}(s)-\hat{\tilde{F}}_{h}(s)R)\sum_{s^{\prime }\in S}U^{\prime}_{h}(s^{\prime},a)|\] \[\leq H\alpha\sum_{m\in M}\|F_{m,h}(s)-\hat{F}_{m,h}(s)R\|_{2}\| \sum_{s^{\prime}\in S}U_{m,h}(s^{\prime},a)\|_{2}\] \[\leq\frac{H\alpha\sqrt{S}}{\sqrt{d\mu}}\sum_{m\in M}\|F_{m,h}(s)- \hat{F}_{m,h}(s)R\|_{2}\] \[\leq\frac{\epsilon}{8\sqrt{dM}H}.\]

From the above result, it follows that for any value function \(V\), the misspecification error of using the approximate feature mapping of the corresponding \(Q\) function \(Q^{\prime}=r+PV\) satisfies

\[|Q^{\prime}_{h}(s,a)-Q^{\prime}_{h,d}(s,a)| \leq|r_{h}(s,a)-\hat{F}(s)RW(a)^{\top}|+|PV(s,a)-[\hat{\tilde{F}} _{h}R_{h}U^{\prime}_{h}V](s,a)|\] \[\leq\frac{\epsilon}{8\sqrt{dM}H^{2}}+\frac{\epsilon}{8\sqrt{dM}H }\leq\frac{\epsilon}{4H\sqrt{dM}}.\]

To prove our main theorem, we first present the following helper lemma and defer the proof to later in this section.

**Lemma 2**.: _Suppose the setting of Theorem 11 holds and that the misspecification error of the bellman update for any value function using an approximate feature mapping satisfies_

\[|Q^{\prime}_{h}(s,a)-Q^{\prime}_{h,d}(s,a)|\leq\frac{\epsilon}{4H\sqrt{dM}}.\]

_Then, the learned \(Q\) function from our algorithm at time step \(h\) satisfies_

\[\|\tilde{Q}_{h}-Q^{*}_{h}\|_{\infty},\|\tilde{Q}_{h}-Q^{\tilde{*}}_{h}\|_{ \infty}\leq\frac{\epsilon(H-h+1)}{H}\]

_for \(h\in[H]\) for \(N_{h}=H^{4}dMC^{2}\log(SAH/\delta)/\epsilon^{2}\) with probability at least \(1-(H-h+1)\delta/(2H)\)._

From a union bound and applying Lemma 2, it follows that Linear \(Q\)-Learning learns an \(\epsilon\)-optimal policy with probability at least \(1-\delta\). Therefore, the sample complexity of the algorithm is

\[\sum_{h\in H}N_{h}|\mathcal{S}^{\#}_{h}|A\leq\tilde{O}\left(\frac{d^{2}M^{2} H^{5}A}{\epsilon^{2}}\right).\]It follows from the triangle inequality that the learned policy is \(2\epsilon\)-optimal by replacing \(2\epsilon\) with \(\epsilon^{\prime}\).

We now prove Lemma 2.

Proof.: We proceed via induction. At time step \(H\) (the base case), it follows from Hoeffding's inequality for \(N_{H}=H^{4}dMC^{2}\log(SAH/\delta)/\epsilon^{2}\) for some constant \(C>0\),

\[|\hat{Q}_{H}(s,a)-Q_{H}^{*}(s,a)|\leq\frac{\epsilon}{4\sqrt{dM}H}\]

for \((s,a)\in\mathcal{S}_{h}^{\#}\times\mathcal{A}\). From Proposition 4.5 from [22] and our assumption on the misspecification error of the approximate feature mapping, it follows that

\[|\bar{Q}_{H}(s,a)-Q_{H}^{*}(s,a)|\leq\frac{\epsilon}{H}.\]

Since \(Q_{H}^{\sharp}=Q_{H}^{*}\), it follows that the base case holds.

Next, assume that

\[\|\bar{Q}_{h+1}-Q_{h+1}^{*}\|_{\infty},\|\bar{Q}_{h+1}-Q_{h+1}^{ \sharp}\|_{\infty}\leq\frac{\epsilon(H-h)}{H}\]

holds with probability at least \(1-(H-h)\delta/(2H)\). We define \(Q_{h}^{\prime}\) as \(Q_{h}^{\prime}=r_{h}+P_{h}\hat{V}_{h+1}\). Let \(Q_{h,d}^{\prime}=F_{h}(W_{h}+\sum_{s^{\prime}\in S}\hat{V}_{h+1}(s^{\prime})U _{h}(s^{\prime}))\). Then, it follows that from Hoeffding's inequality for \(N_{h}=H^{4}C^{2}dM\log(SAH/\delta)/\epsilon^{2}\) for some constant \(C>0\), for all \((s,a)\in\mathcal{S}_{h}^{\#}\times\mathcal{A}\), in step (b) of our algorithm,

\[|\hat{Q}_{h}(s,a)-Q_{h}^{\prime}(s,a)|\leq\frac{\epsilon}{4H\sqrt{dM}}.\]

From our assumption on the misspecification error of the approximate feature mapping, it follows that \(\|Q_{h}^{\prime}-Q_{h,d}^{\prime}\|_{\infty}\leq\frac{\epsilon}{4H\sqrt{dM}}\). Then, from Proposition 4.5 [22], we have

\[|\bar{Q}_{h}(s,a)-Q_{h,d}^{\prime}(s,a)|\leq\frac{\epsilon}{2H}+ \frac{\epsilon}{2H}=\frac{\epsilon}{H}\]

for all \((s,a)\in S\times\mathcal{A}\). Since

\[|Q_{h}^{*}(s,a)-Q_{h}^{\prime}(s,a)|\leq|\sum_{s^{\prime}\in S}( V_{h+1}^{*}(s^{\prime})-\hat{V}_{h+1}(s^{\prime}))P_{h}(s^{\prime}|s,a)|\leq \frac{\epsilon(H-h)}{H}\]

and

\[|Q_{h}^{\sharp}(s,a)-Q_{h}^{\prime}(s,a)|\leq|\sum_{s^{\prime}\in S }(V_{h+1}^{*}(s^{\prime})-\hat{V}_{h+1}(s^{\prime}))P_{h}(s^{\prime}|s,a)| \leq\frac{\epsilon(H-h)}{H},\]

holds from the inductive hypothesis for all \((s,a)\in S\times\mathcal{A}\), it follows that

\[|\bar{Q}_{h}(s,a)-Q_{h}^{*}(s,a)| \leq|\bar{Q}_{h}(s,a)-Q_{h}^{\prime}(s,a)|+|Q_{h}^{\prime}(s,a)- Q_{h}^{*}(s,a)|\] \[\leq\frac{\epsilon}{H}+\frac{\epsilon(H-h)}{H}=\frac{\epsilon(H- h+1)}{H},\]

and similarly,

\[|Q_{h}^{\sharp}(s,a)-\bar{Q}_{h}(s,a)|\leq\frac{\epsilon(H-h+1)}{H}.\]

Thus, the inductive hypothesis holds as a union bound asserts that the above result holds with probability at least \(1-(H-h+1)\delta/(2H)\)

## Appendix G (\(d,s,a\)) Tucker Rank Setting

In the low-rank MDP transfer RL setting, [4] provide an algorithm that admits a source sample complexity of \(\tilde{O}(A\alpha^{3}MT\log(\Phi))\) and target regret of \(\tilde{O}(\bar{\alpha}H^{2}d^{3/2}\sqrt{T})\) with high probability in the transfer RL setting with low-rank MDPs. We first present the transfer learning assumptions in [4] needed to prove a lower bound in their setting. Low rank MDPs assume the following stucutre with shared latent representation \(\phi\),

\[P_{m,h}(s^{\prime}|s,a) =\mu_{m,h}(s^{\prime})^{\top}\phi_{h}(s,a),\] \[P_{h}(s^{\prime}|s,a) =\mu_{h}(s^{\prime})^{\top}\phi_{h}(s,a),\]

where \(\|\phi_{h}(s,a)\|_{2}\leq 1\) and \(\|\int\mu(s)g(s)dx\|_{2}\leq\sqrt{d}\) for any function \(g:S\rightarrow[0,1]\)[3] for all \(m\in[M],h\in[H]\). To allow transfer learning to occur, [4] assume the following:

**Assumption 8** (Assumption 2.2 [4]).: _For any \(h\in[H]\) and \(s^{\prime}\in S\), there exists \(\alpha_{m,h}(s^{\prime})\in\mathbb{R}\) such that \(\mu_{h}(s^{\prime})=\sum_{m\in[M]}\alpha_{m,h}(s^{\prime})\mu_{m,h}(s^{\prime})\),_

and define the task-relatedness coefficient as \(\alpha=\max_{m\in[M],h\in[H],s\in S}\alpha_{m,h}(s)\). They leave determining the optimal dependence on \(\alpha\) as an interesting open question. Similarly to our lower bound in Theorem 1, we prove that one must incur a source sample complexity of \(\Omega(\alpha^{2})\) to benefit from using transfer learning. Note that in this version of transfer RL, there is no reward function in the source tasks.

**Theorem 12**.: _There exist two transfer RL instances such that (i) they satisfy Assumptions 9 and 8, (ii) they cannot be distinguished without observing \(\Omega(\alpha^{2})\) samples in the source phase, and (3) they have target state-action latent features that are orthogonal to each other._

Proof.: We consider two low-rank transfer RL problems, in which the source MDPs have similar transition kernels but differing \(\phi\). All source and target MDPs \((\mathcal{S},\mathcal{A},P,H,r)\) share the same state space, action space, and horizon \(H=2\) with \(M=2\). The learner is given access to a generative model in the source problems. To introduce the MDPs (without loss of generality assume that \(|S|,|A|\) are even), we first define the feature representations: the feature mapping \(\phi_{i}\) for transfer RL problem \(i\) is

\[\phi_{1}(s_{1},a_{1})=\phi_{1}(s_{2},a_{2})=[1,0]^{\top},\phi_{1}(s_{1},a_{2}) =\phi_{2}(s_{2},a_{1})=[0,1]^{\top},\]

\[\phi_{2}(s_{1},a_{1})=\phi_{2}(s_{2},a_{2})=[0,1]^{\top},\phi_{2}(s_{1},a_{2}) =\phi_{2}(s_{2},a_{1})=[1,0]^{\top}\]

where \(s_{1}\) refers to states \(1,\ldots,|S|/2\), \(s_{2}\) refers to states \(|S|/2+1,\ldots,|S|\), and similarly for actions. We will present the transition kernels matrices with \(s_{1},s_{2},a_{1},a_{2}\) as the blocks containing \(|S|/2\) or \(|A|/2\) entries. Clearly, the two feature representations are orthogonal to each other. Next, thetransition kernels \(P^{i}_{m,1}\) for transfer RL problem \(i\) and source MDP \(m\) are

\[P^{1}_{1,1}(s_{1}|\cdot,\cdot) =[1/2,1/2-1/\alpha]\phi_{1}(\cdot,\cdot)\] \[=\begin{bmatrix}1/2&1/2-1/\alpha\\ 1/2-1/\alpha&1/2\end{bmatrix}\] \[P^{1}_{1,1}(s_{2}|\cdot,\cdot) =[1/2,1/2+1/\alpha]\phi_{1}(\cdot,\cdot)\] \[=\begin{bmatrix}1/2&1/2+1/\alpha\\ 1/2+1/\alpha&1/2\end{bmatrix}\] \[P^{2}_{1,1}(s_{1}|\cdot,\cdot) =[1/2,1/2-1/\alpha]\phi_{2}(\cdot,\cdot)\] \[=\begin{bmatrix}1/2-1/\alpha&1/2\\ 1/2&1/2-1/\alpha\end{bmatrix}\] \[P^{2}_{1,1}(s_{2}|\cdot,\cdot) =[1/2,1/2+1/\alpha]\phi_{2}(\cdot,\cdot)\] \[=\begin{bmatrix}1/2+1/\alpha&1/2\\ 1/2&1/2+1/\alpha\end{bmatrix}\] \[P^{1}_{2,1}(s_{1}|\cdot,\cdot) =P^{1}_{2,1}(s_{2}|\cdot,\cdot)=P^{2}_{2,1}(s_{2}|\cdot,\cdot)=P^{ 2}_{2,1}(s_{2}|\cdot,\cdot)\] \[=[1/2,1/2]\phi_{1}(\cdot,\cdot)=[1/2,1/2]\phi_{2}(\cdot,\cdot)\] \[=\begin{bmatrix}1/2&1/2\\ 1/2&1/2\end{bmatrix}\] \[P^{i}_{T}(\cdot|\cdot,\cdot) =\mu_{T}(\cdot)\phi_{i}(\cdot,\cdot),\]

where \(\mu_{T}(\cdot)=[0,1]\) in both transfer RL problems. It follows that

\[\mu_{T}(s_{1})=\alpha(\mu_{1}^{2}(s_{1})-\mu_{1}^{1}(s_{1}))=\alpha(\mu_{2}^{2 }(s_{1})-\mu_{2}^{1}(s_{1}))\]

and similar results hold for \(s_{2}\), so \(\alpha\) satisfies the definition of the task-relatedness coefficient. It follows that in both settings, \(\alpha_{\max}=\alpha\). The target phase reward functions are defined as

\[r^{1}_{1}(\cdot,\cdot) =r^{1}_{2}(\cdot,\cdot)=[1,0]\phi_{1}(\cdot,\cdot)\] \[r^{2}_{1}(\cdot,\cdot) =r^{2}_{2}(\cdot,\cdot)=[1,0]\phi_{2}(\cdot,\cdot),\]

and the initial state distribution is uniform across all states. By construction, in transfer RL problem 1, the only state action pairs that receive reward are \((s_{1},a_{1}),(s_{2},a_{2})\). In contrast, in transfer RL problem 2, the only state action pairs that receive reward are \((s_{1},a_{1}),(s_{2},a_{1})\).

The learner is given either transfer RL problem one or two with equal probability with a generative model in the source problem. When interacting with the source MDPs, the learner specifies a state-action pair \((s,a)\) and source MDP \(m\) and observes a transition from \(P^{i}_{1,m}\). Furthermore, the learner is given the knowledge that the state latent features lie in the function class \(\Phi=\{\phi_{1},\phi_{2}\}\) and must use the knowledge obtained from interacting with the source MDP to choose one to use in the target MDP, which is no harder than receiving no information about the function class.

To distinguish between the two transfer RL problems, one needs to differentiate between \(P^{1}_{1,1}\) and \(P^{2}_{1,1}\). By construction, the magnitude of the largest entrywise difference between \(P^{1}_{1,1}\) and \(P^{2}_{1,1}\) is lower bounded by \(\Omega(1/\alpha^{2})\). If the learner observes \(Z\) transitions in the source MDPs, where \(Z\leq O(\alpha^{2})\) for some constant \(C>0\), then the probability of correctly identifying \(\phi\) is upper bounded by \(0.76\)[5]. Thus, if the learner does not observe \(\Omega(\alpha^{2})\) samples in the source phase, then the learner returns the incorrect feature mapping that is orthogonal to the true feature mapping with probability at least \(0.24\). 

Theorem 12 states that one must incur dependence on \(\alpha\) in the source sample complexity to benefit from transfer learning as using a feature mapping that is orthogonal to the true one results in regret linear in \(T\).

As the low rank MDP setting is analogous to our \((d,S,A)\) Tucker rank setting, we next present our assumptions in this setting,

**Assumption 9**.: _In each of the \(M\) source MDPs, the reward functions have rank \(d\), and the transition kernels have Tucker rank \((d,S,A)\). Let the target MDP's reward functions have rank \(d^{\prime}\) and transition kernels have Tucker rank \((d^{\prime},S,A)\) where \(d^{\prime}\leq dM\). Thus, there exists \(S\times d\) tensors \(U_{m,h},\,S\times d^{\prime}\) tensors \(U_{h}\), \(SA\times d\) matrices with orthonormal columns \(\phi_{m,h},\,SA\times d^{\prime}\) matrices with orthonormal columns \(\phi_{h}\), and \(d\)-dimensional vectors \(W_{m,h},\) and \(d^{\prime}\)-dimensional vectors \(W_{h}\) such that_

\[P_{m,h}(s^{\prime}|s,a) =\sum_{i\in[d]}\phi_{m,h}(s,a,i)U_{m,h}(s^{\prime},i),\] \[r_{m,h}(s,a) =\sum_{i\in[d]}\phi_{m,h}(s,a,i)W_{m,h}(i)\]

_and_

\[P_{h}(s^{\prime}|s,a) =\sum_{i\in[d^{\prime}]}\phi_{h}(s,a,i)U_{h}(s^{\prime},i),\] \[r_{h}(s,a) =\sum_{i\in[d^{\prime}]}\phi_{h}(s,a,i)W_{h}(i)\]

_where \(\|\sum_{s^{\prime}\in\mathcal{S}}g(s^{\prime})U_{m,h}(s^{\prime})\|_{2,}\| \sum_{s^{\prime}\in\mathcal{S}}g(s^{\prime})U_{h}(s^{\prime})\|_{2}\leq\sqrt{ d},\|W_{h}\|_{2},\) and \(\|W_{m,h}(s)\|_{2}\leq\sqrt{S/\mu}\) for all \(s\in\mathcal{S}\), \(a\in\mathcal{A}\), \(h\in[H]\), and \(m\in[M]\) for any function \(g:\mathcal{S}\rightarrow[0,1]\)._

While [4] assume that the target latent representation is a linear combination of the source latent representation, we allow for a more general setting to enable transfer learning. Specifically, we assume that the space spanned by the target latent representations is a subset of the space spanned by the source latent representations.

**Assumption 10**.: _Suppose Assumption 9 holds. For latent factors \(\phi_{h}\) from the target MDP and \(\phi_{m,h}\) for the source MDPs, let \(\phi_{h}=\{\phi_{h},\phi_{h,m}\}_{m\in[M]}\). Then, there exists a non-empty set \(\mathcal{B}(\phi_{h})\) such that the elements in \(\mathcal{B}(\phi_{h})\) are \(B_{h}\in\mathbb{R}^{d^{\prime},d,M}\) such that_

\[\phi_{h}(s,i)=\sum_{j\in[d],m\in[M]}B_{h}(i,j,m)\phi_{m,h}(s,j)\]

_for all \(i\in[d^{\prime}],s\in S,h\in[H]\)._

As in the previous Tucker rank settings, we present the transfer-ability coefficient to quantify the difficulty transfer learning.

**Definition 5**.: _Given a transfer RL problem that satisfies Assumptions 9 and 10. Let \(\Phi\) be the set of all \(\mathbb{R}^{SA\times d}\) latent factor matrices with orthonormal columns that span the column space of \(P_{m,h}\) for all \(m\in[M],h\in[H]\). Then, we define \(\alpha\) as_

\[\alpha:=\min_{\phi\in\Phi}\min_{B\in\mathcal{B}(\phi)}\max_{i\in[d^{\prime}], j\in[d],m\in[M]}|B(i,j,m)|.\]

[4] provide an alternate definition of \(\alpha\), which also measures the difficulty of transfer RL, and leave determining the optimal dependence on \(\alpha\) as an interesting open question. Similarly to our lower bound in Theorem 1, we prove that one must incur a source sample complexity of \(\Omega(\alpha^{2})\) to benefit from using transfer learning. Note that in this Tucker rank setting, there is no reward function in the source tasks.

**Theorem 13**.: _There exist two transfer RL instances such that (i) they satisfy Assumptions 9 and 10, (ii) they cannot be distinguished without observing \(\Omega(\alpha^{2})\) samples in the source phase, and (3) they have target state-action latent features that are orthogonal to each other._

Proof.: Consider the construction where all source and target MDPs \((\mathcal{S},\mathcal{A},P,1,r)\) share the same state space, action space, and horizon with \(H=1\). As the horizon is one, we use the construction used to prove Theorem 7. We consider two transfer RL problems \(i,i^{\prime}\), in which the source MDPs have similar \(Q_{1}^{*}\), but with orthogonal target \(\phi_{T}\). For ease of notation, we let \(a_{1}\) refer to any action in \([n]\), and \(a_{2}\) refer to any action in \(\{n+1,\dots 2n\}\), respectively. The initial state distribution in the target MDP is uniform over \(\mathcal{S}\). We now present two transfer RL problems with similar \(Q\) functions that satisfy Assumptions 5 and 6 but have orthogonal target state latent factors. For ease of notation, the superscript \(i\) of \(Q_{m,h}^{*,i},Q_{h}^{*,i}\) denotes transfer RL problem. Then, the optimal \(Q\) functions (withcolumns representing \(a_{1}\) and \(a_{2}\) and the row being any state) for transfer RL problem one are

\[Q_{1,1}^{*,1} =W_{1,h}^{1}\phi_{1,1}^{1}\left[\sqrt{n}\over\sqrt{n}\right]\left[ \sqrt{1/(2n)}\quad\sqrt{1/(2n)}\right]=\left[1/\sqrt{2}\quad 1/\sqrt{2}\right],\] \[Q_{2,1}^{*,1} =W_{2,h}^{1}\phi_{1,1}^{1}\left[\sqrt{n}\over\sqrt{n}\right] \left[\sqrt{1/(2n)}\quad\sqrt{1/(2n)}\right]=\left[1/\sqrt{2}\quad 1/\sqrt{2} \right],\] \[Q_{1}^{*,1} =W_{h}\phi_{1}^{1}=\left[\sqrt{n}\over\sqrt{n}\right]\left[\sqrt{ 1/(2n)}\quad\sqrt{1/(2n)}\right]=\left[1/\sqrt{2}\quad 1/\sqrt{2}\right]\]

and for transfer RL problem two are

\[Q_{1,1}^{*,2} =W_{1,h}^{2}\phi_{1,1}^{2}\left[\sqrt{n}\over\sqrt{n}\right] \left[\sqrt{1/(2n)}\quad\sqrt{1/(2n)}\right]=\left[1/\sqrt{2}\quad 1/\sqrt{2} \right],\] \[Q_{2,1}^{*,2} =W_{2,h}^{2}\phi_{2,1}^{2}\left[\sqrt{n}\over\sqrt{n}\right]G^{ \prime}=\left[(1+1/\alpha)/(c\sqrt{2})\quad(1-1/\alpha)/(c\sqrt{2})\right],\] \[Q_{2}^{*,1} =W_{h}\phi_{1}^{2}=\left[\sqrt{n}\over\sqrt{n}\right]\left[\sqrt{ 1/(2n)}\quad-\sqrt{1/(2n)}\right]=\left[1/\sqrt{2}\quad-1/\sqrt{2}\right]\]

and \(c=\sqrt{1+1/\alpha^{2}}\). Note that \(c\in(1,2dM)\) because \(\alpha\geq 1/(dM)\) from Lemma 1 for \(\alpha>0\). The incoherence \(\mu\), rank \(d\), and condition number \(\kappa\) of the above \(Q\) functions are \(O(1)\). Furthermore, the above construction satisfies Assumption 9 with Tucker rank \((1,S,A)\) and Assumption 10 because the source and target MDPs share the same feature representation \(\phi_{1}^{1}=\phi_{1,1}^{1}\) in the first transfer RL problem while in the second transfer RL problem,

\[\phi_{1}^{2}=-\alpha\phi_{2,1}^{2}+\alpha\phi_{2,1}^{2}.\]

Note that \(\phi_{1}^{1}\) and \(\phi_{2}^{2}\) are orthogonal to each other by construction.

The learner is given either transfer RL problem one or two with equal probability with a generative model in the source problem. When interacting with the source MDPs, the learner specifies a state-action pair \((s,a)\) and source MDP \(m\) and observes a realization of a shifted and scaled Bernoulli random variable \(X\). The distribution of \(X\) is that \(X=1\) with probability \((Q_{m,1}^{*,i}(s,a)+1)/2\) and \(X=-1\) with probability \((1-Q_{m,1}^{*,i}(s,a))/2)\). Furthermore, the learner is given the knowledge that the state latent features lie in the function class \(\Phi=\{\phi_{1},\phi_{2}\}\) and must use the knowledge obtained from interacting with the source MDP to choose one to use in the target MDP, which is no harder than receiving no information about the function class.

To distinguish between the two transfer RL problems, one needs to differentiate between \(Q_{2,1}^{*,1}\) and \(Q_{2,1}^{*,2}\). By construction, the magnitude of the largest entrywise difference between \(Q_{2,1}^{*,1}\) and \(Q_{2,1}^{*,2}\) is lower bounded by \(\Omega(1/\alpha)\).

If the learner observes \(Z\) samples in the source MDPs, where \(Z\leq O(\alpha^{2})\) for some constant \(C>0\), then the probability of correctly identifying \(G\) is upper bounded by \(0.76\) (Lemma 5.1 with \(\delta=0.24\)[5]). Thus, if the learner does not observe \(\Omega(\alpha^{2})\) samples in the source phase, then the learner returns the incorrect feature mapping that is orthogonal to the true feature mapping with probability at least \(0.24\). 

Theorem 13 states that one must incur dependence on \(\alpha\) in the source sample complexity to benefit from transfer learning. With this lower bound, we've shown in all three Tucker rank settings that \(\alpha\) determines the effectiveness of transfer learning under our transfer learning assumptions. We now present our algorithm that admits an efficient source sample complexity and target regret bound.

In contrast to our approaches in the other Tucker rank settings, one learns the latent representation in the source MDPs through the transition kernels. One potential issue arises when all entries of \(P(\cdot|\cdot,a)\) are small; for example, each entry of the uniform transition kernel is \(1/S\), which is not lower bounded by a constant. Thus, we let \(D=\min_{m\in[M],h\in[H],a\in A}\|P_{m,h}(\cdot|\cdot,a)\|_{\infty}\), and the dependence on \(D\) is unsurprising as one needs to ensure the noise is sufficiently small to learn the latent representations from \(P_{m,h}\). We now present and prove the main result in this Tucker rank setting.

**Theorem 14**.: _Suppose Assumptions 5, 6, and 3 hold in the \((d,S,A)\) Tucker Rank setting, and set \(\delta\in(0,1)\). Furthermore, assume that for any \(\epsilon\in(0,\sqrt{H/T})\), \(P_{m,h}(\cdot|\cdot,a)\) has rank \(d\) and is \(\mu\)-incoherent with condition number \(\kappa\) for all \(a\in A\). Let \(D=\min_{m\in[M],h\in[H],a\in\mathcal{A}}\|P_{m,h}(\cdot|\cdot,a)\|_{\infty}\) Then, for \(T\geq H/\alpha^{2}\), using at most_

\[\tilde{O}\left(\frac{\alpha^{2}d\mu^{2}\kappa^{2}M^{2}TSA}{D^{2}}\right),\]

_samples in the source problems, our algorithm has regret at most_

\[\tilde{O}\left(\sqrt{(dMH)^{3}T}\right)\]

_with probability at least \(1-\delta\) for \(\lambda=1\) and \(\beta_{h,k}\), which is a function of \(d,H,T,M\)._

Proof.: Suppose the setting of Theorem 14 holds. Recall that we let \(D=\min_{m\in[M],h\in[H],a\in\mathcal{A}}\|P_{m,h}(\cdot|\cdot,a)\|_{\infty}\). Then, observing \(N\) transitions from each state-action pairs ensures that for each \((s^{\prime},s,a)\in\mathcal{S}\times\mathcal{S}\times A\) and for each source MDP at each time step,

\[|P_{m,h}(s^{\prime}|s,a)-\hat{P}_{m,h}(s^{\prime}|s,a)|\leq\frac{\sqrt{H}}{ \alpha\mu\kappa D\sqrt{dMT}}\]

with probability at least \(1-\delta/(2S^{2}AMH)\) where \(\hat{P}_{m,h}(s^{\prime}|s,a)=\frac{1}{N}\sum_{i\in[N]}\mathbf{1}_{X_{i}(s,a)=s ^{\prime}}\) and \(X_{i}\sim P(\cdot|s,a)\) for

\[N=\frac{\alpha^{2}d\mu^{2}\kappa^{2}MT\log(4S^{2}AMH/\delta)}{2HD^{2}}.\]

Let \(\hat{P}_{m,h}(\cdot|\cdot,a)=\hat{F}_{m,h}^{a}\hat{\Sigma}_{m,h}\hat{G}_{m,h}\top\) be the singular value decomposition of our estimate where \(\tilde{F}\) corresponds to the state one transitions from. Since \(T\geq\alpha^{2}/H\), it follows that \(\bar{\gamma}\leq 1/2\). Thus, 3 states that there exists rotation matrices \(R_{m,h,a}\) such that

\[\|(\hat{F}_{m,h}^{a}R_{m,h,a}-F_{m,h}^{a})(s)\|_{2}\leq\frac{d\sqrt{H\mu}}{ \alpha\sqrt{MTS}},\]

for all \(s\in\mathcal{S}\) where the the true transition kernel \(P_{m,h}(\cdot|\cdot,a)\) has singular value decomposition \(P_{m,h}(\cdot|\cdot,a)=F_{m,h}^{a}\Sigma_{m,h}G_{m,h}^{\top}\). We can re-express our terms in the more common Low rank MDP setting with \(\phi_{m,h}(s,a)=F_{m,h}^{a}(s)\) and \(\mu_{m,h}(s^{\prime})=\Sigma_{m,h}G_{m,h}^{\top}\).

Let \(\tilde{F}\) be the \(SA\times dM\) matrix from joining all the source state latent factor matrices from \(P(\cdot|\cdot,a)\) for each \(a\in A\), and \(\hat{\tilde{F}}_{h}\) be the \(SA\times dM\) matrix from joining all \(\tilde{F}_{m,h}\). Let \(R_{h}\) be the \(d\times dM\) matrix that joins \(R_{m,h}\) for all \(m\in[M]\). Then, from Assumption 6 and Definition 4, it follows that for all \((s^{\prime},s,a)\in\mathcal{S}\times\mathcal{S}\times A\), that

\[r_{h}(s,a) =\sum_{i\in[d]}F_{h}^{a}(s,i)W_{h}(i)\] \[=\sum_{i\in[d]}\left(\sum_{j\in[d],m\in[M]}B(i,j,m)F_{h,m}^{a}(s, j)\right)W_{h}(i)\] \[=\sum_{i,j\in[d],m\in[M]}B(i,j,m)F_{m,h}^{a}(s,j)W_{h}(i)\] \[=\tilde{F}_{h}^{a}(s)W_{h}^{{}^{\prime}\top}\]

and \(P(s^{\prime}|s,a)=\tilde{F}_{h}^{a}(s)U_{h}^{\prime}(s^{\prime})^{\top}\) (with the same argument) for some \(dM\)-dimension vector \(W_{h}^{\prime},U_{h}^{\prime}(s^{\prime})\). Note that \(W_{h}^{\prime}\) and \(U_{h}^{\prime}(s^{\prime})\) are vectors that join \(W_{m,h}\) and \(U_{m,h}(s^{\prime})\), respectively, with entries that at most \(\alpha\) times larger than the original entry.

Therefore, it follows for all \((s^{\prime},s,a,h)\in\mathcal{S}\times\mathcal{S}\times\mathcal{A}\times[H]\),

\[|r_{h}(s,a)-\hat{\tilde{F}}_{h}^{a}(s)R_{h}W_{h}^{{}^{\prime}\top}| =|\tilde{F}_{h}^{a}(s)W_{h}^{{}^{\prime}\top}-\hat{\tilde{F}}_{h} ^{a}(s)R_{h}W_{h}^{{}^{\prime}\top}|\] \[=|(\tilde{F}_{h}^{a}(s)-\hat{\tilde{F}}_{h}^{a}(s)R_{h})W_{h}^{{} ^{\prime}\top}|\] \[\leq\alpha|\sum_{m\in M}(F_{m,h}^{a}(s)-\hat{F}_{m,h}^{a}(s)R_{m, h})W_{m,h}|\] \[\leq\alpha\sum_{m\in M}\|F_{m,h}^{a}(s)-\hat{F}_{m,h}^{a}(s)R_{m, h}\|_{2}\|W_{m,h}\|_{2}\] \[\leq\alpha\sqrt{\frac{S}{d\mu}}\sum_{m\in M}\|F_{m,h}^{a}(s)-\hat {F}_{m,h}^{a}(s)R_{m,h}\|_{2}\] \[\leq\sqrt{\frac{dHM}{T}}\]

from the Cauchy-Schwarz inequality and our singular vector perturbation bound. From the same logic,

\[\left|\sum_{s^{\prime}\in S}(P_{h}(s^{\prime}|s,a)-\hat{\tilde{F} }_{h}^{a}(s)R_{h}U_{h}^{\prime}(s^{\prime})^{\top}\right| \leq|\sum_{s^{\prime}\in\mathcal{S}}P_{h}(s^{\prime}|s,a)-\hat{ \tilde{F}}_{h}^{a}(s)R_{h}U_{h}^{\prime}(s^{\prime})^{\top}|\] \[=|\sum_{s^{\prime}\in\mathcal{S}}\tilde{F}_{h}^{a}(s)U_{h}^{ \prime}(s^{\prime})-\hat{\tilde{F}}_{h}^{a}(s)R_{h}U_{h}^{\prime}(s^{\prime} )^{\top}|\] \[\leq\alpha|\sum_{m\in M}(F_{m,h}^{a}(s)-\hat{F}_{m,h}^{a}(s)R_{m, h})\sum_{s^{\prime}\in\mathcal{S}}U_{m,h}(s^{\prime})|\] \[\leq\alpha\sum_{m\in M}\|F_{m,h}^{a}(s)-\hat{F}_{m,h}^{a}(s)R_{m,h}\|_{2}\|\sum_{s^{\prime}\in\mathcal{S}}U_{m,h}(s^{\prime})^{\top}\|_{2}\] \[\leq\sqrt{\frac{dHM}{T}}.\]

Therefore, \(\hat{\tilde{F}}_{h}^{a}(s)\) satisfies Assumption B in [17] with \(\xi=\sqrt{\frac{dMH}{T}}\)5. Then it follows that running LSVI-UCB using \(\hat{\tilde{F}}_{h}^{a}(s)\) with \(\delta^{\prime}=\delta/2\) during the target phase of the algorithm admits a regret bound \[Regret(T)\leq C"(\sqrt{d^{3}M^{3}H^{3}T}+dMH\sqrt{\frac{dMH}{T}})\in\tilde{O}(\sqrt{d ^{3}M^{3}H^{3}T})\]

with probability at least \(1-\delta\) from a final union bound. Furthermore, the sample complexity in the source phase is

\[\tilde{O}\left(\frac{\alpha^{2}d\mu^{2}\kappa^{2}M^{2}TSA}{D^{2}}\right),\]

which proves our result. 

## Appendix H \((d,d,d)\) Tucker Rank Setting

In this section, we present the assumptions, algorithm, and proof for Theorem 15. In the \((d,d,d)\) Tucker rank setting, we assume the following low-rank structure on the MDPs.

**Assumption 11**.: _In each of the \(M\) source MDPs, the reward functions have rank \(d\), and the transition kernels have Tucker rank \((d,d,d)\). The target MDP's reward functions have rank \(d^{\prime}\) and transition kernels have Tucker rank \((d,d^{\prime},d^{\prime})\) where \(d^{\prime}\leq dM\). Thus, there exists \(d\times d\times d\) tensors \(U_{m,h}\), \(d\times d^{\prime}\times d^{\prime}\) tensors \(U_{h}\), \(S\times d\) matrices \(V_{m,h}\), \(S\times d^{\prime}\) matrices \(V_{h}\), \(S\times d\) matrices \(F_{m,h}\), \(S\times d^{\prime}\) matrices \(F_{h}\), \(A\times d\) matrices \(G_{m,h}\), and \(A\times d^{\prime}\) matrices \(G_{h}\) all with orthonormal columns, and sequences of non-increasing singular values \(\{\sigma_{m,h}(i)\geq\mathbb{R}_{+}\}_{i\in[d]},\{\sigma_{h}(i)\geq\mathbb{R}_ {+}\}_{i\in[d^{\prime}]}\) such that_

\[P_{m,h}(s^{\prime}|s,a) =\sum_{i\in[d],j\in[d^{\prime}],k\in[d]}U_{m,h}(i,j,k)V_{m,h}(s^{ \prime},i)F_{m,h}(s,j)G_{m,h}(a,k),\] \[r_{m,h}(s,a) =\sum_{i\in[d]}\sigma_{m,h}(i)F_{m,h}(s,i)G_{m,h}(a,i)\]

_and_

\[P_{h}(s^{\prime}|s,a) =\sum_{i\in[d],j\in[d^{\prime}],k\in[d^{\prime}]}U_{h}(i,j,k)V_{h }(s^{\prime},i)F_{h}(s,j)G_{h}(a,k),\] \[r_{h}(s,a) =\sum_{i\in[d^{\prime}]}\sigma_{h}(i)F_{h}(s,i)G_{h}(a,i)\]

_where_

\[\|\sum_{i\in[d],s^{\prime}\in\mathcal{S}}g(s^{\prime})U_{m,h}(i,..;)V_{m,h}(s^{\prime},i)\|_{F^{\prime}}\|\sum_{s^{\prime}\in\mathcal{S},i\in[d ]}g(s^{\prime})U_{h}(i,.;.:)V_{h}(s^{\prime},i)\|_{F},\sigma_{m,h}(i),\sigma_{ h}(i)\leq\frac{\sqrt{SA}}{d\mu}\]

_for all \(s\in\mathcal{S}\), \(a\in\mathcal{A}\), \(h\in[H]\), \(m\in[M]\), and \(i\in[d]\) for any function \(g:\mathcal{S}\rightarrow[0,1]\)._

To allow transfer learning to occur, we assume that the latent factors from the source MDPs span the space spanned by the target latent factors.

**Assumption 12**.: _For latent factors \(F_{h}\) from the target MDP and \(F_{m,h}\) for the source MDPs, let \(F=\{F_{h},F_{h,m}\}_{h\in[H],m\in[M]}\). For latent factors \(G_{h}\) from the target MDP and \(G_{m,h}\) for the source MDPs, let \(G=\{G_{h},G_{h,m}\}_{h\in[H],m\in[M]}\). Then, there exists a non-empty set \(\mathcal{B}(F),\mathcal{C}(G)\) such that the elements in \(\mathcal{B}(F)\) are \(B\in\mathbb{R}^{d^{\prime},d,M}\) and the elements in \(\mathcal{C}(G)\) are \(C\in\mathbb{R}^{d^{\prime},d,M}\) such that_

\[F_{h}(s,i) =\sum_{j\in[d],m\in[M]}B(i,j,m)F_{m,h}(s,j)\]

_and_

\[G_{h}(a,i) =\sum_{j\in[d],m\in[M]}C(i,j,m)G_{m,h}(a,j)\]

_for all \(i\in[d^{\prime}],s\in\mathcal{S},a\in\mathcal{A}\)._

Similarly to the \((S,d,A)\) and \((S,S,d)\) Tucker rank settings, we introduce the transfer-ability coefficient that measures the difficulty of applying transfer learning.

**Definition 6** (Transfer-ability Coefficient).: _Given a transfer RL problem that satisfies Assumptions 11, 12, and 3, let \(\mathcal{F}\) be the set of all \(\mathbb{R}^{S\times d}\) latent factor matrices with orthonormal columns that span the column space of \(Q^{*}_{m,h}\) for all \(m\in[M],h\in[H]\). Let \(\mathcal{G}\) be the set of all \(\mathbb{R}^{A\times d}\) latent factor matrices with orthonormal columns that span the row space of \(Q^{*}_{m,h}\) for all \(m\in[M],h\in[H]\). Then, we define \(\alpha\) as_

\[\alpha\coloneqq\max\left(\min_{F\in\mathcal{F}}\min_{B\in\mathcal{B}(F)}\max_ {i\in[d^{\prime}],j\in[d],m\in[M]}|B(i,j,m)|,\min_{G\in\mathcal{G}}\min_{C\in \mathcal{C}(F)}\max_{i\in[d^{\prime}],j\in[d],m\in[M]}|C(i,j,m)|\right).\]In contrast to the definition in the other Tucker rank settings, \(\alpha\) bounds the largest coefficient used in the linear combination of \(F_{h}\) and \(G_{h}\). The algorithm that we use in this setting runs LR-EVI on each of the source MDPs, constructs a feature mapping \(\hat{\phi}\) using the singular vectors from the singular value decomposition of \(\tilde{Q}_{m,h}\), and runs LSVI-UCB with \(\hat{\phi}\). Since we use both sets of singular vectors to construct \(\hat{\phi}\), our feature mapping has latent dimension \(d^{2}M^{2}\).

```
0:\(\{N_{h}\}_{h\in[H]}\)
1:for\(m\in[M]\)do
2: Run LR-EVI(\(\{N_{h}\}_{h\in[H]}\)) on source MDP \(m\) to obtain \(\tilde{Q}_{m,h}\).
3: Compute the singular value decomposition of \(\tilde{Q}_{m,h}=\hat{F}_{m,h}\hat{\Sigma}_{m,h}\hat{G}_{m,h}^{\top}\).
4: Compute the feature mapping \(\hat{\phi}=\{\hat{\phi}_{h}\}_{h\in[H]}\) with \[\hat{\phi}_{h}=\left\{\frac{\sqrt{SA}}{d\mu}\hat{F}_{m,h}(:,i)\hat{G}_{n,h}(:,j)|i,j\in[d],m,n\in[M]\right\}.\] ```

**Algorithm 8** Source Phase

The above algorithm admits the following result.

**Theorem 15**.: _Suppose Assumptions 11, 12, and 3 hold in the \((d,d,d)\) Tucker Rank setting, and set \(\delta\in(0,1)\). Furthermore, assume that for any \(\epsilon\in(0,\sqrt{H/T})\), \(Q_{m,h}=r_{m,h}+P_{m,h}V_{m,h+1}\) has rank \(d\) and is \(\mu\)-incoherent with condition number \(\kappa\) for all \(\epsilon\)-optimal value functions \(V_{h+1}\). Then, assuming \(T\geq H/\alpha^{2}\), using at most_

\[\tilde{O}\left(d^{10}\mu^{5}\kappa^{4}(S+A)M^{2}H^{4}\alpha^{4}T\right)\]

_samples in the source problems, our algorithm has regret at most_

\[\tilde{O}\left(\sqrt{d^{6}M^{6}H^{3}T}\right)\]

_with probability at least \(1-\delta\) for \(\lambda=1\) and \(\beta_{h,k}\), which is a function of \(d,H,T,M\)._

We now prove Theorem 15 using a similar argument to the one used in the proof of Theorem 10 except we show that the target MDP with our feature mapping satisfies Assumption B in [17] and apply LSVI-UCB.

Proof.: Suppose the assumptions stated in Theorem 15 hold. Then, from Theorem 9[30], it follows that LR-EVI returns near-optimal \(Q\) functions \(\tilde{Q}_{m,h}\) functions with singular value decomposition \(\tilde{Q}_{m,h}=\hat{F}_{m,h}\hat{\Sigma}_{m,h}\hat{G}_{m,h}\) that are \(\sqrt{H}/(\kappa\alpha^{2}Md^{4}\mu\sqrt{T})\)-optimal with probability at least \(1-\delta/2\) using

\[\tilde{O}\left(d^{10}\mu^{5}\kappa^{4}(S+A)MH^{4}\alpha^{4}T\right)\]

samples on each source MDP. Since \(T\geq H/\alpha^{4}\), from \(\bar{\gamma}\leq\frac{1}{2}\) in Proposition 2. Thus, 3 states that there exists rotation matrices \(R_{m,h,f},R_{m,h,g}\) such that

\[\|(\hat{F}_{m,h}R_{m,h,f}-F_{m,h})(s)\|_{2}\leq\frac{\sqrt{H}}{\alpha^{4}Md^{ 2}\sqrt{TS\mu}},\quad\|(\hat{G}_{m,h}R_{m,h,g}-G_{m,h})(s)\|_{2}\leq\frac{ \sqrt{H}}{\alpha^{4}Md^{2}\sqrt{TA\mu}}\]

for all \(s\in\mathcal{S}\) where the optimal \(Q\) function have singular value decomposition \(Q_{m,h}^{*}=F_{m,h}\Sigma_{m,h}G_{m,h}^{\top}\).

Then, from Assumption 6 and Definition 4, it follows that for all \((s^{\prime},s,a)\in\mathcal{S}\times\mathcal{S}\times\mathcal{A}\), that

\[r_{h}(s,a) =\sum_{i\in[d]}\sigma_{h}(i)F_{h}(s,i)G_{h}(a,i)\] \[=\sum_{i\in[d]}\sigma_{h}(i)\left(\sum_{j\in[d],m\in[M]}B(i,j,m)F_ {h,m}(s,j)\right)\left(\sum_{l\in[d],n\in[M]}C(i,l,n)G_{h,n}(s,l)\right)\] \[=\sum_{j,l\in[d],m,n\in[M]}F_{m,h}(s,j)G_{n,h}(a,l)\left(\sum_{i \in[d]}\sigma_{h}(i)B(i,j,m)C(i,l,n)\right)\] \[=\theta_{h}\phi(s,a)_{h}^{\top}\]

where \(\theta_{h}=\{\sum_{i\in[d]}\sigma_{h}(i)B(i,j,m)C(i,l,n)\}_{j,l\in[d],m,n\in[M]}\) and \(\phi_{h}=\{F_{m,h}(s,j)G_{n,h}(a,l)\}_{j,l\in[d],m,n\in[M]}\) and \(P(s^{\prime}|,\cdot)=\mu_{h}(s^{\prime})\phi_{h}^{\top}\) (with the same argument) for \(\mu_{h}(s^{\prime})=\{\sum_{i,j,k\in[d]}U_{h}(i,j,k)V_{h}(s^{\prime},i)B(j,l,m )C(k,x,n)\}_{l,x\in[d],m,n\in[M]}\). It follows that we can express \(r_{h}=\tilde{F}_{h}\tilde{\Sigma}_{h}\tilde{G}_{h}^{\top}\) where \(\tilde{F}_{h}\) be the \(S\times d^{2}M^{2}\) matrix from joining all \(F_{m,h}\)\(dM\) times, \(\tilde{G}_{h}\) be the \(A\times d^{2}M^{2}\) matrix from joining all \(G_{m,h}\)\(dM\) times, and \(\tilde{\Sigma}_{h}\) is the diagonal matrix with entries \(\theta_{h}(j,l,m,n)\) for all \(j,l\in[d],m,n\in[M]\). Note that the entries of \(\tilde{\Sigma}_{h}\) and \(\mu_{h}\) are at most \(d\alpha^{2}\) larger than their original value.

Let \(\hat{\tilde{F}}_{h}\) be the \(S\times d^{2}M^{2}\) matrix from joining all \(\tilde{F}_{m,h}R_{m,h,f}\)\(dM\) times, and \(\hat{\tilde{G}}_{h}\) be the \(A\times d^{2}M^{2}\) matrix from joining all \(\tilde{G}_{m,h}R_{m,h,g}\)\(dM\) times. Let \(\tilde{\theta}_{h}\) satisfy

\[\tilde{\theta}_{h}\hat{\phi}_{h}^{\top}=\hat{\tilde{F}}_{h}\tilde{\Sigma}_{h} \hat{\tilde{G}}_{h}^{\top}.\]

Therefore, it follows for all \((s^{\prime},s,a,h)\in\mathcal{S}\times\mathcal{S}\times\mathcal{A}\times[H]\),

\[|r_{h}(s,a)-\tilde{\theta}_{h}\hat{\phi}_{h}^{\top}(s,a)| =|\tilde{F}_{h}\tilde{\Sigma}_{h}\tilde{G}_{h}^{\top}(s,a)-\hat{ \tilde{F}}_{h}\tilde{\Sigma}_{h}\hat{\tilde{G}}_{h}^{\top}(s,a)|\] \[\leq|\tilde{F}_{h}\tilde{\Sigma}_{h}\tilde{G}_{h}^{\top}(s,a)- \hat{\tilde{F}}_{h}\tilde{\Sigma}_{h}\tilde{G}_{h}^{\top}(s,a)|+|\hat{\tilde {F}}_{h}\tilde{\Sigma}_{h}\tilde{G}_{h}^{\top}(s,a)-\hat{\tilde{F}}_{h}\tilde {\Sigma}_{h}\hat{\tilde{G}}_{h}^{\top}(s,a)|\] \[\leq dM\sum_{m\in M}\|F_{m,h}(s)-\hat{F}_{m,h}(s)R_{m,h,f}\|_{2}\| \tilde{\Sigma}_{h}\|_{op}\max_{m\in M}\sqrt{d}\|G_{m,h}\|_{\infty}\] \[+\sqrt{d}\max_{m\in M}\|\hat{F}_{m,h}\|_{\infty}\|\tilde{\Sigma}_{ h}\|_{op}\|G_{m,h}(a)-\hat{G}_{m,h}(s)R_{m,h,g}\|_{2}\] \[\leq d^{3/2}M\sum_{m\in M}\|F_{m,h}(s)-\hat{F}_{m,h}(s)R_{m,h,f}\| _{2}\|\tilde{\Sigma}_{h}\|_{op}\max_{m\in M}\|G_{m,h}\|_{\infty}\] \[+\max_{m\in M}(\|F_{m,h}\|+\|F_{m,h}R-\hat{F}_{m,h}\|_{\infty})\| \tilde{\Sigma}_{h}\|_{op}\|G_{m,h}(a)-\hat{G}_{m,h}(s)R_{m,h,g}\|_{2}\] \[\leq Cd^{3}M\frac{\sqrt{S}}{\sqrt{\mu}}\sum_{m\in M}\|F_{m,h}(s)- \hat{F}_{m,h}(s)R_{m,h,f}\|_{2}\] \[\leq C\frac{dM\sqrt{H}}{\sqrt{T}},\]

where the second inequality comes from the triangle inequality and Cauchy-Schwarz inequality, the third inequality comes from the triangle inequality, and the fourth inequality comes from the definition of incoherence, our bound on the entries of \(\tilde{\Sigma}\) and the left term dominating the right term, and the final inequality is due to our singular vector perturbation bounds. Similarly,

\[\|P_{h}(\cdot|s,a)-\mu_{h}\tilde{(}s^{\prime})\hat{\phi}_{h}(s,a)^{\top}\|_{TV} \leq C\frac{dM\sqrt{H}}{\sqrt{T}}\]

for some absolute constant \(C>0\) where \(\mu_{h}\tilde{(}s^{\prime})\) satisfies \(\hat{\tilde{F}}_{h}\tilde{\Sigma}_{h}(s^{\prime})\hat{\tilde{G}}_{h}^{\top}\) and \(\tilde{\Sigma}_{h}(s^{\prime})\) is the \(d^{2}M^{2}\times d^{2}M^{2}\) diagonal matrix with entries \(\mu_{h}(s^{\prime},l,k,m,n)\) with the same argument using the regularity condition on \(\mu_{h}\).

Therefore, \(\hat{\phi}\) satisfies Assumption B in [17] with \(\xi=O(dM\sqrt{H/T})\). Running LSVI-UCB with \(\hat{\phi}\) admits a regret bound of

\[Regret(T)\leq C\left(\sqrt{d^{6}M^{6}H^{3}{T_{l}}^{2}}+\frac{d^{3}M^{3}H^{3/2}T \sqrt{\iota}}{\sqrt{T}}\right)\in\tilde{O}\left(d^{6}M^{6}H^{3}T\right)\]

for some absolute constant \(C>0\) with probability at least \(1-\delta\) from a final union bound. The sample complexity in the source phase is

\[\tilde{O}\left(d^{10}\mu^{5}\kappa^{4}(S+A)MH^{4}\alpha^{4}T\right).\]

which completes the proof. 

## Appendix I \(\ell_{\infty}\) eigen perturbation bound with non-uniform noise: rank-\(r\) case

Suppose \(A=A^{*}+D\in\mathbb{R}^{m\times n}\), where \(A^{*}\) is the true rank-\(r\) matrix and \(D\) is the noise matrix. Suppose \(A^{*}\) has SVD \(A^{*}=U^{*}{\Sigma^{*}}{V^{*}}^{\top}\), where \(U^{*}\in^{m\times r},V^{*}\in^{n\times r}\) are orthonormal matrices containing the left and right singular vectors, respectively, and \(\Sigma^{*}=\operatorname{diag}(\sigma_{1}^{*},\ldots,\sigma_{r}^{*})\) is a diagonal matrix of the singular values. Similarly, suppose \(A\) has SVD \(A=U\Sigma V^{\top}+U^{\prime}{\Sigma^{\prime}}{V^{\prime}}^{\top}\), where \(U\in^{m\times r},V\in^{n\times r}\) and \(\Sigma=\operatorname{diag}(\sigma_{1},\ldots,\sigma_{r})\) correspond to the top-\(r\) singular vectors/values, and \(U^{\prime}\in^{m\times(m-r)},V^{\prime}\in^{m\times(n-r)}\) and \(\Sigma\in^{(m-r)\times(n-r)}\) correspond to the bottom singular vectors/values.

The following proposition is a generalization of the perturbation bound in [12] to the setting with non-uniform noise.

**Proposition 2**.: _Suppose \(\overline{\gamma}:=\frac{\left\|D\right\|_{\text{op}}}{\sigma_{r}^{*}}<\frac {1}{2}\). There exist two orthonormal matrices \(\overline{R}\),\(\underline{R}\in^{r\times r}\) such that_

\[\left\|\left(U-U^{*}\overline{R}\right)_{i}.\right\|_{2} \leq\left\|U_{i}^{*}.\right\|_{2}.\,8\overline{\gamma}+\left\|D_{ i}.\right\|_{2}.\,\frac{1}{\sigma_{r}^{*}(1-\overline{\gamma})},\qquad\forall i \in[m],\] \[\left\|\left(V-V^{*}\underline{R}\right)_{j}.\right\|_{2} \leq\left\|V_{j}^{*}.\right\|_{2}.\,8\overline{\gamma}+\left\|D_{ \cdot j}.\right\|_{2}.\,\frac{1}{\sigma_{r}^{*}(1-\overline{\gamma})},\qquad \forall j\in[n].\]

To prove Proposition 2, we need some notations. Let \(\overline{H}:=U^{\top}U^{*}\in^{r\times r}\) and its SVD be \(\overline{H}=\overline{A}\overline{A}\overline{B}^{\top}\), where \(\overline{A},\overline{B}\in^{r\times r}\) are orthonormal, and \(\overline{\Lambda}=\operatorname{diag}(\overline{\sigma}_{1},\ldots,\overline {\sigma}_{r})\in^{r\times r}\) is a diagonal matrix. Similarly, let \(\underline{H}:=V^{\top}V^{*}\)\(\in^{r\times r}\) and its SVD be \(\underline{H}=\underline{A}\underline{\Lambda}\underline{B}^{\top}\). Finally, define the orthonormal matrices \(\operatorname{sgn}(\overline{H}):=\overline{A}\overline{B}^{\top}\) and \(\operatorname{sgn}(\underline{H}):=\underline{A}\underline{B}^{\top}\). (The function \(\operatorname{sgn}(\cdot)\) is called the matrix sign function.)

We need a technical lemma, which generalizes [2, Lemma 3] to the asymmetric case.

**Lemma 3** (Properties of matrix sign function).: _We have \(\left\|\overline{H}\right\|_{\text{op}}\leq 1\) and \(\left\|\underline{H}\right\|_{\text{op}}\leq 1\). When \(\overline{\gamma}:=\frac{\left\|D\right\|_{\text{op}}}{\sigma_{r}^{*}}<\frac{ 1}{2}\), we have_

\[\sqrt{\left\|\overline{H}-\operatorname{sgn}(\overline{H})\right\|_{\text{op}}} \leq\left\|UU^{\top}-{V^{*}}{V^{*}}^{\top}\right\|_{\text{op}} \leq\frac{\max\left\{\left\|U^{*}D\right\|_{\text{op}},\left\|DV^{*} \right\|_{\text{op}}\right\}}{(1-\overline{\gamma})\sigma_{r}^{*}}\leq\frac{ \overline{\gamma}}{1-\overline{\gamma}},\]

\[\sqrt{\left\|\underline{H}-\operatorname{sgn}(\underline{H})\right\|_{\text{op}}} \leq\left\|VV^{\top}-{V^{*}}{V^{*}}^{\top}\right\|_{\text{op}} \leq\frac{\max\left\{\left\|U^{*}D\right\|_{\text{op}},\left\|DV^{*} \right\|_{\text{op}}\right\}}{(1-\overline{\gamma})\sigma_{r}^{*}}\leq\frac{ \overline{\gamma}}{1-\overline{\gamma}},\]

_and_

\[\left\|\Sigma\underline{H}-\overline{H}\Sigma\right\|_{\text{op}}\leq 2\left\|D \right\|_{\text{op}},\qquad\left\|\overline{H}^{\top}\Sigma-\Sigma\underline{H }^{\top}\right\|_{\text{op}}\leq 2\left\|D\right\|_{\text{op}},\]

_and_

\[\left\|\overline{H}^{-1}\right\|_{\text{op}}\leq\frac{(1-\overline{\gamma})^{2 }}{(1-2\overline{\gamma})},\qquad\left\|\underline{H}^{-1}\right\|_{\text{op}} \leq\frac{(1-\overline{\gamma})^{2}}{(1-2\overline{\gamma})}.\]

Proof.: We prove the bounds for \(\overline{H}\). The proof for \(\underline{H}\) is identical. Clearly \(\left\|\overline{H}\right\|_{\text{op}}\leq\left\|U\right\|_{\text{op}}\left\| U^{*}\right\|_{\text{op}}=1\).

By Weyl's inequality, we have \(\left|\sigma_{i}^{*}-\sigma_{i}\right|\leq\left\|D\right\|_{\mathsf{op}},\forall i \in[m]\), hence

\[\sigma_{r}^{*}-\sigma_{r+1}\geq\sigma_{r}^{*}-\left\|D\right\|_{\mathsf{op}}=(1- \overline{\gamma})\sigma_{r}^{*}>\frac{1}{2}\sigma_{r}^{*}>0\]

by assumption. On the other hand, by standard perturbation theory, \(1\geq\overline{\sigma}_{1}\geq\cdots\geq\overline{\sigma}_{r}\geq 0\) are the cosines of the principal angles \(0\leq\overline{\theta}_{1}\leq\cdots\leq\overline{\theta}_{r}\leq\pi/2\) between the column spaces of \(U\) and \(U^{*}\), and \(\sin\overline{\theta}_{r}=\left\|UU^{\top}-U^{*}U^{*\top}\right\|_{\mathsf{op}}\). Hence \(\left\|\mathrm{sgn}(\overline{H})-\overline{H}\right\|_{\mathsf{op}}=1-\cos \overline{\theta}_{r}\). By Wedin's \(\sin\Theta\) Theorem, we have

\[\sin\overline{\theta}_{r}\leq\frac{\max\left\{\left\|U^{*}D\right\|_{\mathsf{ op}},\left\|DV^{*}\right\|_{\mathsf{op}}\right\}}{\sigma_{r}^{*}-\sigma_{r+1}} \leq\frac{\max\left\{\left\|U^{*}D\right\|_{\mathsf{op}},\left\|DV^{*}\right\| _{\mathsf{op}}\right\}}{(1-\overline{\gamma})\sigma_{r}^{*}}.\]

Since \(\cos\overline{\theta}_{r}\geq\cos^{2}\overline{\theta}_{r}=1-\sin^{2} \overline{\theta}_{r}\), we obtain

\[\sqrt{\left\|\mathrm{sgn}(\overline{H})-\overline{H}\right\|_{\mathsf{op}}}= \sqrt{1-\cos\overline{\theta}_{r}}\leq\sin\overline{\theta}_{r}\leq\frac{ \max\left\{\left\|U^{*}D\right\|_{\mathsf{op}},\left\|DV^{*}\right\|_{\mathsf{ op}}\right\}}{(1-\overline{\gamma})\sigma_{r}^{*}}\leq\frac{\left\|D\right\|_{ \mathsf{op}}}{(1-\overline{\gamma})\sigma_{r}^{*}}=\frac{\overline{\gamma}}{1- \overline{\gamma}}.\]

Note that

\[U^{\top}DV^{*}=U^{\top}(A-A^{*})V^{*}=\Sigma V^{\top}V^{*}-U^{\top}U^{*}\Sigma ^{*}=\Sigma\underline{H}-\overline{H}\Sigma^{*},\qquad\text{and}\]

Hence

\[\left\|\Sigma\underline{H}-\overline{H}\Sigma\right\|_{\mathsf{op}} \leq\left\|\Sigma\underline{H}-\overline{H}\Sigma^{*}\right\|_{ \mathsf{op}}+\left\|\overline{H}(\Sigma^{*}-\Sigma)\right\|_{\mathsf{op}}\] \[=\left\|U^{\top}DV^{*}\right\|_{\mathsf{op}}+\left\|\overline{H} (\Sigma^{*}-\Sigma)\right\|_{\mathsf{op}}\] \[\leq\left\|D\right\|_{\mathsf{op}}+\left\|\overline{H}\right\|_{ \mathsf{op}}\left\|\Sigma^{*}-\Sigma\right\|_{\mathsf{op}}\] \[\leq 2\left\|D\right\|_{\mathsf{op}},\]

where we use \(\left\|\overline{H}\right\|_{\mathsf{op}}\leq 1\) and \(\left\|\Sigma^{*}-\Sigma\right\|_{\mathsf{op}}\leq\left\|D\right\|_{\mathsf{op}}\). Similarly,

\[U^{*\top}DV=U^{*\top}\left(A-A^{*}\right)V=U^{*\top}U\Sigma-\Sigma^{*}V^{* \top}V^{\top}=\overline{H}^{\top}\Sigma-\Sigma^{*}\underline{H}^{\top},\]

so a similar argument as above gives \(\left\|\overline{H}^{\top}\Sigma-\Sigma\underline{H}^{\top}\right\|_{\mathsf{ op}}\leq 2\left\|D\right\|_{\mathsf{op}}.\)

Finally, recall that \(\left\|\overline{H}-\mathrm{sgn}(\overline{H})\right\|_{\mathsf{op}}\leq \left(\frac{\overline{\gamma}}{1-\overline{\gamma}}\right)^{2}\) and note the simple identity \(X^{-1}-Y^{-1}=X^{-1}(Y-X)Y^{-1}\). It follows that

\[\left\|\overline{H}^{-1}-\mathrm{sgn}(\overline{H})^{-1}\right\|_ {\mathsf{op}}\leq \left\|\overline{H}^{-1}\right\|_{\mathsf{op}}\left\|\overline{H} -\mathrm{sgn}(\overline{H})\right\|_{\mathsf{op}}\left\|\mathrm{sgn}( \overline{H})^{-1}\right\|_{\mathsf{op}}\] \[=\] \[\leq\]

It follows that

\[\left\|H^{-1}\right\|_{\mathsf{op}}\leq\left\|\mathrm{sgn}(\overline{H})^{-1} \right\|_{\mathsf{op}}+\left\|\overline{H}^{-1}-\mathrm{sgn}(\overline{H})^{- 1}\right\|_{\mathsf{op}}\leq 1+\frac{\overline{\gamma}^{2}}{1-2\overline{ \gamma}}=\frac{(1-\overline{\gamma})^{2}}{1-2\overline{\gamma}}.\]

This completes the proof of Lemma 3. 

We are now ready to prove Proposition 2. We have

\[U-U^{*}\,\mathrm{sgn}(\overline{H})^{\top} =AV\Sigma^{-1}-U^{*}\,\mathrm{sgn}(\overline{H})^{\top} A=U\Sigma V^{\top}\] \[=(A^{*}+D)V\Sigma^{-1}-U^{*}\,\mathrm{sgn}(\overline{H})^{\top}\] \[=U^{*}\Sigma^{*}V^{*\top}V\Sigma^{-1}-U^{*}\,\mathrm{sgn}(\overline {H})^{\top}+DV\Sigma^{-1} A^{*}=U^{*}\Sigma^{*}V^{*\top}\] \[=U^{*}\Sigma^{*}\underline{H}^{\top}\Sigma^{-1}-U^{*}\,\mathrm{ sgn}(\overline{H})^{\top}+DV\Sigma^{-1}, \underline{H}=V^{\top}V^{*}\]But \(\Sigma^{*}\underline{H}^{\top}=(\Sigma+\Sigma^{*}-\Sigma)\underline{H}^{\top}= \overline{H}^{\top}\Sigma+\left(\Sigma\underline{H}^{\top}-\overline{H}^{\top }\Sigma\right)+(\Sigma^{*}-\Sigma)\,\underline{H}^{\top}\). Hence

\[U-U^{*}\operatorname{sgn}(\overline{H})^{\top} =U^{*}\left[\overline{H}^{\top}\Sigma+\left(\Sigma\underline{H} ^{\top}-\overline{H}^{\top}\Sigma\right)+(\Sigma^{*}-\Sigma)\,\underline{H}^{ \top}\right]\Sigma^{-1}-U^{*}\operatorname{sgn}(\overline{H})^{\top}+DV\Sigma^ {-1}\] \[=U^{*}\overline{H}^{\top}+U^{*}\left(\Sigma\underline{H}^{\top}- \overline{H}^{\top}\Sigma\right)\Sigma^{-1}+U^{*}\left(\Sigma^{*}-\Sigma \right)\underline{H}^{\top}\Sigma^{-1}-U^{*}\operatorname{sgn}(\overline{H}) ^{\top}+DV\Sigma^{-1}\] \[=U^{*}\left(\overline{H}-\operatorname{sgn}(\overline{H}) \right)^{\top}+U^{*}\left(\Sigma\underline{H}^{\top}-\overline{H}^{\top} \Sigma\right)\Sigma^{-1}+U^{*}\left(\Sigma^{*}-\Sigma\right)\underline{H}^{ \top}\Sigma^{-1}+DV\Sigma^{-1}.\]

For each \(i\in[m]\), we can bound the \(i\)-th row of \(U-U^{*}\operatorname{sgn}(\overline{H})^{\top}\) as

\[\left\|\left(U-U^{*}\operatorname{sgn}(\overline{H})\right)_{i} \right\|_{2}\] \[\qquad\qquad+\left\|D_{i}\right\|_{2}\left\|V\right\|_{\mathsf{op }}\left\|\Sigma^{-1}\right\|_{\mathsf{op}}\] \[\overset{\text{(i)}}{\leq}\left\|U_{i}^{*}\right\|_{2}\left( \left(\frac{\overline{\gamma}}{1-\overline{\gamma}}\right)^{2}+2\left\|D \right\|_{\mathsf{op}}\cdot\frac{1}{\sigma_{r}^{*}(1-\overline{\gamma})}+ \left\|D\right\|_{\mathsf{op}}\cdot 1\cdot\frac{1}{\sigma_{r}^{*}(1-\overline{ \gamma})}\right)+\left\|D_{i}\right\|_{2}\cdot\frac{1}{\sigma_{r}^{*}(1- \overline{\gamma})}\] \[=\left\|U_{i}^{*}\right\|_{2}\left(\left(\frac{\overline{\gamma} }{1-\overline{\gamma}}\right)^{2}+\frac{3\overline{\gamma}}{1-\overline{ \gamma}}\right)+\left\|D_{i}\right\|_{2}\cdot\frac{1}{\sigma_{r}^{*}(1- \overline{\gamma})}\] \[\overset{\text{(ii)}}{\leq}\left\|U_{i}^{*}\right\|_{2}\cdot 8 \overline{\gamma}+\left\|D_{i}\right\|_{2}\cdot\frac{1}{\sigma_{r}^{*}(1- \overline{\gamma})},\]

where step (i) follows from Lemma 3 and the bound

\[\left\|\Sigma^{-1}\right\|_{\mathsf{op}}=\frac{1}{\sigma_{r}}\leq\frac{1}{ \sigma_{r}^{*}-\left\|D\right\|_{\mathsf{op}}}=\frac{1}{\sigma_{r}^{*}(1- \overline{\gamma})},\]

and step (ii) holds since \(\overline{\gamma}\leq\frac{1}{2}\). Setting \(\overline{R}=\operatorname{sgn}(\overline{H})^{\top}\) proves the first inequality in Proposition 2.

The second inequality in Proposition 2 can be established by a similar argument.

**Corollary 3**.: _Suppose the setting of Proposition 2 holds. Furthermore, assume that \(A^{*}\) is \(\mu\)-incoherent with condition number \(\kappa\) and \(\|A^{*}\|_{\infty}\geq C\) for some constant \(C>0\). Then, there exist two orthonormal matrices \(\overline{R},\underline{R}\in^{r\times r}\) such that_

\[\left\|\left(U-U^{*}\overline{R}\right)_{i}\right\|_{2} \leq\frac{16\kappa(\mu d)^{3/2}\|D\|_{\infty}}{C\sqrt{m}},\qquad \forall i\in[m],\] \[\left\|\left(V-V^{*}\underline{R}\right)_{j}\right\|_{2} \leq\frac{16\kappa(\mu d)^{3/2}\|D\|_{\infty}}{C\sqrt{n}}, \qquad\forall j\in[n].\]

Proof.: From the definition of the \(\ell_{\infty}\) norm, there exists \((i,j)\in[m]\times[m]\) such that \(\|A^{*}\|_{\infty}=|A_{i,j}|\). Then, it follows that

\[\|A^{*}\|_{\infty} =|A_{i,j}|=|(U^{*}\Sigma^{*}{V^{*}}^{\top})_{i,j}|\] \[\leq\|\Sigma^{*}\|_{\mathsf{op}}\|U_{i}^{*}\|_{2}\|V_{j}^{*}\|_{2}\] \[\leq\frac{\kappa\sigma_{d}\mu d}{\sqrt{mn}}.\]

Furthermore, we note that \(\|D\|_{op}\leq\|D\|_{\infty}\sqrt{mn}\). Thus, the result follows from Proposition 2 and using the definition of incoherence and the above inequalities. 

## Appendix J Proofs of LSVI-UCB-(S, d, A) and LSVI-UCB-(S, S, d)

In this section, we present the proofs of Theorem 8 and 9 and the related lemmas. The proofs, theorems, and lemmas are modifications of the analogous result/proof in [17] tailored to our Tucker rank settings.

### Lsvi-Ucb-(s, d, A)

To prove Theorem 8, we first introduce notation used throughout this section and then present auxiliary lemmas. We reiterate that the steps and analysis in this subsection are essentially the same as the proofs in [17], except we have \(A\) copies of the weight vectors and gram matrices. We let \(A^{a}_{h,k},w^{a}_{h,k},Q^{k}_{h},V^{k}_{h},\pi^{k}\) as the parameters/estimates at episode \(k\). Furthermore, let \(V^{k}_{h}=\max_{a\in\mathcal{A}}Q^{k}_{h}(s,a)\) and \(\pi^{k}\) be the greedy policy w.r.t. \(Q^{k}_{h}\). We let \(F^{k}_{h}:=F_{h}(s^{k}_{h})\).

To prove the desired regret bound, we first need to prove that \(\mathbb{P}V-\hat{P}V\) concentrates about 0, which differs from typical concentration inequalities for self normalized processes due to the value function not being constant. Thus, we first bound the log of the covering number of our function class for the value function to control the error of our algorithm's estimate.

**Lemma 4**.: _Let \(\mathcal{V}\) denote a function class of mappings from \(\mathcal{S}\) to \(\mathbb{R}\) with the following parametric form_

\[V(\cdot)=\min\left\{\max_{a\in\mathcal{A}}w^{\top}_{a}F(\cdot)+\beta\sqrt{F( \cdot)^{\top}\Lambda_{a}^{-1}F(\cdot)},H\right\}\]

_where the parameters \(w_{a},\beta,\Lambda_{a}\) satisfy \(\|w_{a}\|\leq L\) for all \(a\in\mathcal{A}\), \(\beta\in[0,B]\), and the minimum eigenvalue of \(\Lambda_{a}\) is greater than \(\lambda\). Assume that \(\|F(s)\|\leq 1\) for all \(s\in\mathcal{S}\), and let \(\mathcal{N}_{\epsilon}\) be the \(\epsilon\)-covering number of \(\mathcal{V}\) with respect to the distance \(dist(V,V^{\prime})=\sup_{s}|V(s)-V^{\prime}(s)|\). Then,_

\[\log(\mathcal{N}_{\epsilon})\leq d\log(1+4L/\epsilon)+d^{2}\log(1+8d^{1/2}B^ {2}/(\lambda\epsilon^{2}))\]

Proof.: Equivalently, we reparameterize the function class by \(D_{a}=\beta^{2}\Lambda_{a}^{-1}\),

\[V(\cdot)=\min\left\{\max_{a\in\mathcal{A}}w^{\top}_{a}F(\cdot)+\sqrt{F(\cdot) ^{\top}D_{a}F(\cdot)},H\right\}\]

where \(\|w_{a}\|\leq L\) and \(\|D_{a}\|\leq B^{2}/\lambda\) for all \(a\in\mathcal{A}\). For any \(V_{1},V_{2}\in\mathcal{V}\), let them take the form in the above equation with parameters \((w_{a},D_{a})\) and \((w^{\prime}_{a},D^{\prime}_{a})\). Since \(\min(\cdot,H)\) and \(\max_{a\in\mathcal{A}}\) are contraction maps,

\[dist(V_{1}-V_{2}) \leq\sup_{s,a}|(w^{\top}_{a}F(s)+\sqrt{F(s)^{\top}D_{a}F(s)})-(w ^{\prime\top}_{a}F(s)+\sqrt{F(s)^{\top}D^{\prime}_{a}F(s)})|\] \[\leq\sup_{a,F:\|F\|\leq 1}|(w^{\top}_{a}F+\sqrt{F^{\top}D_{a}F}) -(w^{\prime\top}_{a}F+\sqrt{F^{\top}D^{\prime}_{a}F})|\] \[\leq\max_{a\in\mathcal{A}}\left[\sup_{F:\|F\|\leq 1}|(w_{a}-w^{ \prime}_{a})^{\top}F|+\sup_{F:\|F\|\leq 1}|\sqrt{F^{\top}(D_{a}-D^{\prime}_{a})F}|\right]\] \[\leq\max_{a\in\mathcal{A}}\left[\|w_{a}-w^{\prime}_{a}\|+\sqrt{ \|D_{a}-D^{\prime}_{a}\|}\right]\leq\max_{a\in\mathcal{A}}\left[\|w_{a}-w^{ \prime}_{a}\|+\sqrt{\|D_{a}-D^{\prime}_{a}\|_{F}}\right]\]

where the holds because \(|\sqrt{x}-\sqrt{y}|\leq\sqrt{|x-y|}\) for any \(x,y\geq 0\). Let \(C_{w}\) be an \(\epsilon/2\)-cover of \(\{w\in\mathbb{R}^{d}|\|w\|\leq L\}\) with respect to the 2-norm, and \(C_{D}\) be an \(\epsilon^{2}/4\)-cover of \(\{D\in\mathbb{R}^{d\times d}|\|D\|_{F}\leq d^{1/2}B^{2}/\lambda\}\) with respect to the Frobenius norm. By Lemma D.5 from [17], we know that,

\[|C_{w}|\leq(1+4L/\epsilon)^{d},\quad|C_{D}|\leq(1+8d^{1/2}B^{2}/(\lambda \epsilon^{2}))^{d^{2}}.\]

Hence, for any \(V_{1}\), there exists a \(w^{\prime}_{a}\in C_{w}\) and \(D^{\prime}_{a}\in C_{D}\) for all \(a\in\mathcal{A}\) such that \(V_{2}\) parameterized by \((w^{\prime}_{a},D^{\prime}_{a})\) satisfies \(dist(V_{1},V_{2})\leq\epsilon\). Hence, \(\mathcal{N}_{\epsilon}\leq|C_{w}||C_{D}|\), which gives:

\[\log(\mathcal{N}_{\epsilon})\leq\log(|C_{w}|)+\log(|C_{D}|)\leq d\log(1+4 \epsilon L/\epsilon)+d^{2}\log(1+8d^{1/2}B^{2}/(\lambda\epsilon^{2})).\]

We next bound the \(\ell_{2}\)-norm of the weight vector of any policy.

**Lemma 5**.: _Let \(\{w^{a}_{h}\}\) be the weight vector of some policy \(\pi\), i.e., \(Q^{\pi}_{h}(s,a)=F_{h}(s)^{\top}w^{a}_{h}\). Then,_

\[\|w^{a}_{h}\|\leq 2H\sqrt{d}.\]Proof.: Note that \(Q_{h}^{\pi}(s,a)=r_{h}(s,a)+\sum_{s^{\prime}}V_{h+1}^{\pi}(s^{\prime})P_{h}(s^{ \prime}|s,a)\), so, using the low-rank representations of \(r_{h},P_{h}\), it follows that

\[\|w_{h}^{a}\|_{2}=\|W_{h}(a,\cdot)+\sum_{s^{\prime}}V^{\pi}(s^{\prime})U_{h}(s^{ \prime},a,\cdot)\|_{2}\leq\|W_{h}(a,\cdot)\|_{2}+\|\sum_{s^{\prime}}V^{\pi}(s^{ \prime})U_{h}(s^{\prime},a,\cdot)\|_{2}\leq\sqrt{d}+H\sqrt{d}.\]

Recall that since the feature mapping was scaled up (\(F_{h}=\sqrt{\frac{S}{d\mu}}F^{\prime}\)) to ensure the max entries of \(F\) and \(W,U\) are on the same scale, it follows that \(\|W_{h}(a)\|_{2}\leq 1\) and \(\|\sum_{s^{\prime}\in\mathcal{S}}V_{h+1}^{\pi}(s^{\prime})U_{h}(s^{\prime},a )\|_{2}\leq H\). Therefore, it follows that \(\|w_{h}^{a}\|_{2}\leq 2H\sqrt{d}\). 

We next bound the 2-norm of the weight vector from the algorithm.

**Lemma 6**.: _For any \(k,h,a\), the weight vector from the above algorithm satisfies \(\|w_{h,k}^{a}\|\leq 2H\sqrt{dk/\lambda}\)._

Proof.: For any \(F_{h}(s)=f\), we have for any \(a\in\mathcal{A}\)

\[|f^{\top}w_{h,k}^{a}| =|f^{\top}(\Lambda_{h}^{a})^{-1}\sum_{t\in T_{h,k-1}^{a}}F_{h}(s_ {h}^{t})\left[r_{h}(s_{h}^{t},a)+\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}(s_{h+ 1}^{t},a^{\prime})\right]|\] \[\leq|\sum_{t\in T_{h,k-1}^{a}}f^{\top}(\Lambda_{h}^{a})^{-1}F_{h} (s_{h}^{t})|2H\] \[\leq 2H\sqrt{(\sum_{t\in T_{h,k-1}^{a}}f^{\top}(\Lambda_{h}^{a})^{ -1}f^{\top})(\sum_{t\in T_{h,k-1}^{a}}F_{h}(s_{h}^{t})^{\top}(\Lambda_{h}^{a}) ^{-1}F_{h}(s_{h}^{t}))}\] \[\leq 2H\|f\|_{2}\sqrt{d|T_{h,k-1}^{a}|/\lambda}\leq 2H\sqrt{dk/ \lambda},\]

where the second to last inequality is due to Lemma D.1 from [17] and that Rayleigh quotients are upper bounded by the largest eigenvalue of the matrix (Theorem 4.2.2 from [14]). The last inequality is due to \(\|F_{h}(s)\|_{2}\leq 1\) and \(|T_{h,k-1}^{a}|\leq k\). 

The above two lemmas are necessary in ensuring that the estimation error is small as they depend multiplicatively on the norm of the weight vectors. We next prove the main concentration lemma that controls the estimate.

**Lemma 7**.: _Let \(c^{\prime}\) be the constant in the definition of \(\beta_{k,h}^{a}=c^{\prime}dH\sqrt{\iota}\). Then, there exists a constant \(C>0\) that is independent of \(c^{\prime}\) such that for any \(\delta\in(0,1)\), if we let the event \(\mathcal{X}\) be_

\[\forall(a,k,h)\in\mathcal{A}\times[K]\times[H]:\qquad\left\|\sum_{t\in T_{h,k- 1}^{a}}F_{h}(s_{h}^{t})(V_{h+1}^{t}(s_{h+1}^{t})-P_{h}V_{h+1}^{t}(s_{h}^{t},a )\right\|_{(\Lambda_{h,t}^{a})^{-1}}\leq CdH\sqrt{\chi}\]

_where \(\chi=\log(2(c^{\prime}+1)dTA/\delta)\), then \(\mathbb{P}(\mathcal{X})\geq 1-\delta/2\)._

Proof.: From Lemma 6, we have \(\|w_{h,k}^{a}\|_{2}\leq 2H\sqrt{dk/\lambda}\). By construction, the minimum eigenvalue of \(\Lambda_{h,t}^{a}\) is lower bounded by \(\lambda\). Then, from Lemma D.4 in [17] (with \(\delta^{\prime}=\delta/A\)), Lemma 4, and taking a union bound over actions, we have that with probability at least \(1-\delta^{\prime}/2\), for all \(a\in\mathcal{A}\) and \(\epsilon>0\),

\[\left\|\sum_{t\in T_{h,k-1}^{a}}F_{h}(s_{h}^{t})(V_{h+1}^{t}(s_{h+1}^{t})-P_{h }V_{h+1}^{t}(s_{h}^{t},a))\right\|_{(\Lambda_{h,t}^{a})^{-1}}^{2}\leq 4H^{2} \left[\frac{d}{2}\log((k+\lambda)/\lambda))+\log(2A\mathcal{N}_{\epsilon}/ \delta)\right]+8k^{2}\epsilon^{2}/\lambda.\]

Then, from Lemma 4, it follows that with probability at least \(1-\delta^{\prime}/2\), for all \(a\in A\) and \(\epsilon>0\),

\[\left\|\sum_{t\in T_{h,k-1}^{a}}F_{h}(s_{h}^{t})(V_{h+1}^{t}(s_{h +1}^{t})-P_{h}V_{h+1}^{t}(s_{h}^{t},a)\right\|_{(\Lambda_{h,t}^{a})^{-1}}^{2}\] \[\leq 4H^{2}\left[\frac{d}{2}\log((k+\lambda)/\lambda))+\log(2A/ \delta)+d\log(1+4L/\epsilon)+d^{2}\log(1+8d^{1/2}\beta^{2}/(\lambda\epsilon^{2} ))\right]+8k^{2}\epsilon^{2}/\lambda.\]Choosing \(\lambda=1,\beta^{a}_{k,h}=CdH\iota,\epsilon=dH/k\) gives that there exists some constant \(C^{\prime}\) such that

\[\left\|\sum_{t\in T^{a}_{k,h-1}}F_{h}(s^{t}_{h})(V^{t}_{h+1}(s^{t}_{h+1})-P_{h}V ^{t}_{h+1}(s^{t}_{h},a)\right\|^{2}_{(\Lambda^{a}_{h,t})^{-1}}\leq C^{\prime}d^ {2}H^{2}\log(2(c_{\beta}+1)dTA/\delta).\]

We next recursively bound the difference between the value function maintained in the algorithm and the true value function of any policy \(\pi\). We upperbound this with their expected difference and an additional error term, which is bounded with high probability.

**Lemma 8**.: _There exists a constant \(c_{\beta}\) such that \(\beta^{a}_{k,h}=c_{\beta}dH\sqrt{\iota}\) where \(\iota=\log(2dAT/p)\) and for any fixed policy \(\pi\), on the event \(\mathcal{X}\) defined in Lemma 7, we have for all \((s,a,h,k)\in S\times A\times[H]\times[K]\):_

\[F_{h}(s)^{\top}w^{a}_{h,k}-Q^{\pi}_{h}(s,a)=P_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})(s,a)+\Delta^{k}_{h}(s,a),\]

_where \(|\Delta^{k}_{h}(s,a)|\leq\beta^{a}_{k,h}\sqrt{F_{h}(s)^{\top}(\Lambda^{a}_{h, k})^{-1}F_{h}(s)}\)._

Proof.: From our low-rank assumption, it follows that

\[Q^{\pi}_{h}(s,a)=F_{h}(s)^{\top}w^{a}_{h}=(r_{h}+P_{h}V^{\pi}_{h+1})(s,a),\]

which following the steps from the proof of Lemma B.4 in [17] gives

\[w^{a}_{h,k}-w^{a,\pi}_{h} =-\lambda(\Lambda^{a}_{h,k})^{-1}w^{a,\pi}_{h}\] \[+(\Lambda^{a}_{h,k})^{-1}\sum_{t\in T^{a}_{k-1,h}}F_{h}(s^{t}_{h} )(V^{k}_{h+1}(s^{t}_{h+1})-P_{h}V^{k}_{h+1}(s^{t}_{h},a)\] \[+(\Lambda^{a}_{h,k})^{-1})\sum_{t\in T^{a}_{k-1,h}}F_{h}(s^{t}_{h })P_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})(s^{t}_{h},a)\]

Now, we bound each term \((q^{a}_{1},q^{a}_{2},q^{a}_{3})\) individually, for the first term, note for all \(a\in\mathcal{A}\)

\[|F_{h}(s)^{\top}q^{a}_{1}|=|\lambda F_{h}(s)^{\top}(\Lambda^{a}_{h,k})^{-1}w^ {a,\pi}_{h}|\leq\sqrt{\lambda}\|w^{a,\pi}_{h}\|\sqrt{F_{h}(s)^{\top}(\Lambda^ {a}_{h,k})^{-1}F_{h}(s)}\]

For the second term, given the event \(\mathcal{X}\), we have from Cauchy Schwarz

\[|F_{h}(s)^{\top}q^{a}_{2}|\leq CdH\sqrt{\chi}\sqrt{F_{h}(s)^{\top}(\Lambda^{a }_{h,k})^{-1}F_{h}(s)}\]

where \(\chi=\log(2(c_{\beta}+1)dTA/p)\). For the third term,

\[F_{h}(s)^{\top}q^{a}_{3} =F_{h}(s)^{\top}\left((\Lambda^{a}_{h,k})^{-1})\sum_{t\in T^{a}_{ k-1,h}}F_{h}(s^{t}_{h})P_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})(s^{t}_{h},a)\right)\] \[=F_{h}(s)^{\top}\left((\Lambda^{a}_{h,k})^{-1})\sum_{t\in T^{a}_{ k-1,h}}F_{h}(s^{t}_{h})F_{h}(s^{t}_{h})^{\top}\sum_{s^{\prime}\in\mathcal{S}}(V^{ k}_{h+1}-V^{\pi}_{h+1})(s^{\prime})U_{h}(s^{\prime},a)\right)\] \[=F_{h}(s)^{\top}\left(\sum_{s^{\prime}\in\mathcal{S}}(V^{k}_{h+1} -V^{\pi}_{h+1})(s^{\prime})U_{h}(s^{\prime},a)\right)\] \[-\lambda F_{h}(s)^{\top}\left((\Lambda^{a}_{h,k})^{-1})\sum_{s^{ \prime}\in\mathcal{S}}(V^{k}_{h+1}-V^{\pi}_{h+1})(s^{\prime})U_{h}(s^{\prime}, a)\right)\]

We note that

\[F_{h}(s)^{\top}\left(\sum_{s^{\prime}\in\mathcal{S}}(V^{k}_{h+1}-V^{\pi}_{h+1} )(s^{\prime})U_{h}(s^{\prime},a)\right)=P_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})(s,a)\]and

\[|\lambda F_{h}(s)^{\top}\left((\Lambda_{h,k}^{a})^{-1})\sum_{s^{\prime}\in\mathcal{ S}}(V_{h+1}^{k}-V_{h+1}^{k})(s^{\prime})U_{h}(s^{\prime},a)\right)|\leq 2H\sqrt{d \lambda}\sqrt{F_{h}(s)^{\top}(\Lambda_{h,k}^{a})^{-1}F_{h}(s)}\]

Since

\[|F_{h}(s)^{\top}w_{h,k}^{a}-Q_{h}^{\pi}(s,a)|=F_{h}(s)^{\top}(w_{h,k}^{a}-w_{h }^{a,\pi})=F_{h}(s)^{\top}(q_{1}^{a}+q_{2}^{a}+q_{3}^{a}),\]

it follows that

\[|F_{h}(s)^{\top}w_{h,k}^{a}-Q_{h}^{\pi}(s,a)-P_{h}(V_{h+1}^{k}-V_{h+1}^{\pi})( s,a)|\leq C"dH\sqrt{\chi}\sqrt{F_{h}(s)^{\top}(\Lambda_{h,k}^{a})^{-1}F_{h}(s)}.\]

Similarly as in [17], we choose \(c_{\beta}\) that satisfies \(C"\sqrt{\log(2)+\log(c_{\beta}+1)}\leq c_{\beta}\sqrt{\log(2)}\). 

This lemma implies that by adding the appropriate bonus, \(Q_{h}^{k}\) is always an upperbound of \(Q_{h}^{*}\) with high probability, i.e., achieving optimism.

**Lemma 9**.: _On the event \(\mathcal{X}\) defined in lemma 7, we have \(Q_{h}^{k}(s,a)\geq Q_{h}^{*}(s,a)\) for all \((s,a,h,k)\in\mathcal{S}\times\mathcal{A}\times[H]\times[K]\)._

Proof.: We prove this with induction. The base case holds because at step \(H+1\), the value function is zero, so from Lemma 8,

\[|F_{H}(s)^{\top}w_{H,k}^{a}-Q_{H}^{*}(s,a)|\leq\beta_{k,h}^{a}\sqrt{F_{H}(s)^{ \top}(\Lambda_{H,k}^{a})^{-1})F_{H}(s)}\]

and thus,

\[Q_{H}^{*}(s,a)\leq\min\Big{(}H,F_{H}(s)^{\top}w_{H,k}^{a}+\beta_{k,h}^{a}\sqrt {F_{H}(s)^{\top}(\Lambda_{H,k}^{a})^{-1})F_{H}(s)}\Big{)}=Q_{H}^{k}(s,a).\]

From the inductive hypothesis (assuming that \(Q_{h+1}^{k}(s,a)\geq Q_{h+1}^{*}(s,a)\)), it follows that \(P_{h}(V_{h+1}^{k}-V_{h+1}^{*})(s,a)\geq 0\). From Lemma 8, \(F_{h}(s)^{\top}w_{h,k}^{a}-Q_{h}^{*}(s,a)\leq\beta_{k,h}^{a}\sqrt{F_{h}(s)^{ \top}(\Lambda_{h,k}^{a})^{-1})F_{h}(s)}\). It follows that

\[Q_{h}^{*}(s,a)\leq\min\Big{(}H,F_{h}(s)^{\top}w_{h,k}^{a}+\beta_{k,h}^{a}\sqrt {F_{h}(s)^{\top}(\Lambda_{h,k}^{a})^{-1})F_{h}(s)}\Big{)}=Q_{h}^{k}(s,a).\]

We next show that Lemma 8 transforms to the recursive formula \(\delta_{h}^{k}=V_{h}^{k}(s_{h}^{k})-V_{h}^{\pi_{k}}(s_{h}^{k})\), which is used in proving our regret bound.

**Lemma 10**.: _Let \(\delta_{h}^{k}=V_{h}^{k}(s_{h}^{k})-V_{h}^{\pi_{k}}(s_{h}^{k})\) and \(\delta_{h+1}^{k}=\mathbb{E}[\delta_{h+1}^{k}|s_{h}^{k},a_{h}^{k}]-\delta_{h+1}^ {k}\). Then, on the event \(\mathcal{X}\) defined in Lemma 7, we have for any \((h,k)\):_

\[\delta_{h}^{k}\leq\delta_{h+1}^{k}+\xi_{h+1}^{k}+\beta_{k,h}^{a}\sqrt{F_{h}(s _{h}^{k})^{\top}(\Lambda_{h,k}^{a_{h}^{k}})^{-1}F_{h}(s_{h}^{k})}\]

Proof.: From Lemma 8, we have for any \((s,a,h,k)\) that

\[Q_{h}^{k}(s,a)-Q_{h}^{\pi_{k}}(s,a)\leq P_{h}(V_{h+1}^{k}-V_{h+1}^{\pi_{k}})( s,a)+\beta_{k,h}^{a}\sqrt{F_{h}(s)^{\top}(\Lambda_{h,k}^{a})^{-1}F_{h}(s)}.\]

From the definition of \(V^{\pi_{k}}\), we have

\[\delta_{h}^{k}=Q_{h}^{k}(s_{h}^{k},a_{h}^{k})-Q_{h}^{\pi_{k}}(s_{h}^{k},a_{h}^ {k}).\]

Finally, we prove the main theorem, Theorem 8, which asserts that the regret bound of our algorithm is efficient with respect to \(A\) and \(T\).

Proof.: Suppose that the Assumptions required in Theorem 8 hold. We condition on the event \(\mathcal{X}\) from Lemma 7 with \(p=\delta/2\) and use the notation for \(\delta_{h}^{k},\xi_{h}^{k}\) as in Lemma 10. From Lemmas 9 and 10, we have

\[Regret(K)=\sum_{k=1}^{K}(V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi_{k}}(s_{1}^{k}))\leq \sum_{k=1}^{K}\delta_{1}^{k}\leq\sum_{k\in[K]}\sum_{h\in[H]}\xi_{h}^{k}+\beta_{ k,h}^{a}\sum_{k\in[K]}\sum_{h\in[H]}\sqrt{F_{h}(s_{h}^{k})^{\top}(\Lambda_{h,k}^{a _{h}^{k}})^{-1}F_{h}(s_{h}^{k})}\]

Since the observations at episode \(k\) are independent of the computed value function (this uses the trajectories from episodes 1 to \(k-1\), it follows \(\{\xi_{h}^{k}\}\) is a martingale difference sequence with \(|\xi_{h}^{k}|\leq 2H\) for all \((k,h)\). Thus, from the Azuma Hoeffding inequality, we have

\[\sum_{k\in[K],h\in H}\xi_{h}^{k}\leq\sqrt{2TH^{2}\log(2/p)}\leq 2H\sqrt{T_{ \iota}}\]

with probability at least \(1-\delta/2\). For the second term, we note that the minimum eigenvalue of \(\Lambda_{h,k}^{a_{h}^{k}}\) is at least one by construction and \(\|F_{h}(s)\|\leq 1\). From the Elliptical Potential Lemma (Lemma D.2 [17]), we have for all \(a\in\mathcal{A}\) and \(h\in[H]\),

\[\sum_{k=1}^{K}F_{h}(s_{k})^{\top}(\Lambda_{h,k}^{a})^{-1}F_{h}(s_{k})\leq 2 \log(det(\Lambda_{h,k+1}^{a})/det(\Lambda_{h,0}^{a}))\]

Furthermore, we have \(\|\Lambda_{h,k+1}^{a}\|=\|\sum_{t\in T_{K,h}^{a}}F_{h}(s_{h}^{t})F_{h}(s_{h}^ {t})^{\top}+\lambda I\|\leq\lambda+|T_{K,h}^{a}|\leq\lambda+K\). It follows that

\[\sum_{k=1}^{K}F_{h}(s_{k})^{\top}(\Lambda_{h,k}^{a})^{-1}F_{h}(s_{k})\leq 2d \log(1+K/\lambda)\leq 2d\iota\]

Next, by Cauchy-Schwarz and grouping the regret by each action, it follows that

\[\sum_{k\in[K]}\sum_{h\in[H]}\sqrt{F_{h}(s_{h}^{k})^{\top}(\Lambda _{h,k}^{a_{h}^{k}})^{-1}F_{h}(s_{h}^{k})} \leq\sum_{h\in[H]}\sqrt{K}\left[\sum_{k\in[K]}F_{h}(s_{h}^{k})^{ \top}(\Lambda_{h,k}^{a_{h}^{k}})^{-1}F_{h}(s_{h}^{k})\right]^{1/2}\] \[=\sum_{h\in[H]}\sqrt{K}\left[\sum_{a\in\mathcal{A}}\sum_{t\in T_ {K,h}^{a}}F_{h}(s_{h}^{t})^{\top}(\Lambda_{h,t}^{a})^{-1}F_{h}(s_{h}^{t}) \right]^{1/2}\] \[\leq\sum_{h\in[H]}\sqrt{K}\left[\sum_{a\in\mathcal{A}}2d\iota \right]^{1/2}\] \[\leq H\sqrt{2KAd\iota}.\]

Since \(\beta_{k,h}^{a}=cdH\sqrt{\iota}\) for some constant \(c\), from a union bound and the previous bounds, it follows that

\[Regret(K)\leq 2H\sqrt{T_{\iota}}+\beta_{k,h}^{a}H\sqrt{2KAd\iota}\in\tilde{O}( \sqrt{d^{3}H^{3}AT}).\]

with probability at least \(1-\delta\). 

To use LSVI-UCB-\((S,d,A)\) in our transfer RL setting, we next show that it is robust to misspecification error. Similarly, we first prove the helper lemmas needed in the misspecified setting and follow the proof structure and techniques used in [17].

**Lemma 11**.: _For a \(\xi\)-approximate \((S,d,A)\) Tucker rank MDP (Assumption 7), then for any policy \(\pi\) there exists corresponding weight vectors \(\{w_{h}^{a}\}\) where \(w_{h}^{a}=W_{h}(a)+\sum_{s^{\prime}\in\mathcal{S}}V_{h+1}^{\pi}(s^{\prime})U_{ h}(s^{\prime},a)\) such that for all \((s,a)\in\mathcal{S}\times\mathcal{A}\)_

\[|Q_{h}^{\pi}(s,a)-F_{h}(s)w_{h}^{\pi\top}|\leq 3H\xi.\]Proof.: Note that \(Q_{h}^{\pi}(s,a)=r_{h}(s,a)+\sum_{s^{\prime}}V_{h+1}^{\pi}(s^{\prime})P_{h}(s^{ \prime}|s,a)\), so, using the low-rank representations of \(r_{h},P_{h}\), it follows that

\[|Q_{h}^{\pi}(s,a)-F_{h}(s)w_{h}^{a\top}| \leq|r_{h}(s,a)-F_{h}(s)W_{h}(a)^{\top}|\] \[\qquad+\left|\sum_{s^{\prime}}V_{h+1}^{\pi}(s^{\prime})P_{h}(s^{ \prime}|s,a)-F_{h}(s)\sum_{s^{\prime}\in\mathcal{S}}V_{h+1}^{\pi}(s^{\prime})U _{h}(s^{\prime},a)\right|\] \[\leq\xi+2H\xi\] \[\leq 3H\xi\]

**Lemma 12**.: _Suppose Assumption 7 holds. Let \(\{w_{h}^{a}\}\) be the weight vector of some policy \(\pi\), i.e., \(Q_{h}^{\pi}(s,a)=F_{h}(s)^{\top}w_{h}^{a}\). Then,_

\[\|w_{h}^{a}\|\leq 2Hd.\]

Proof.: Recall that \(w_{h}^{a}=W_{h}(a)+\sum_{s^{\prime}\in\mathcal{S}}V_{h+1}^{\pi}(s^{\prime})U _{h}(s^{\prime},a)\). Since the feature mapping was scaled up (\(F_{h}=\sqrt{\frac{S}{d\mu}}F^{\prime}\)) to ensure the max entries of \(F\) and \(W,U\) are on the same scale, it follows that \(\|W_{h}(a)\|_{2}\leq 1\) and \(\|\sum_{s^{\prime}\in\mathcal{S}}V_{h+1}^{\pi}(s^{\prime})U_{h}(s^{\prime},a) \|_{2}\leq H\). Thus, \(\|w_{h}^{a}\|_{2}\leq 2Hd\). 

Similarly, we bound the stochastic noise but account for the misspecification error.

**Lemma 13**.: _Suppose Assumption 7 holds. Let \(c_{m}^{\prime}\) be the constant in the definition of \(\beta_{k,h}^{a}=c_{m}^{\prime}H(d\sqrt{t}+\xi\sqrt{|T_{k,h}^{a}|d})\). Then, there exists a constant \(C>0\) that is independent of \(c_{m}^{\prime}\) such that for any \(\delta\in(0,1)\), if we let the event \(\mathcal{X}\) be_

\[\forall(a,k,h)\in\mathcal{A}\times[K]\times[H]:\qquad\left\|\sum_{t\in T_{h,k- 1}^{a}}F_{h}(s_{h}^{t})(V_{h+1}^{t}(s_{h+1}^{t})-P_{h}V_{h+1}^{t}(s_{h}^{t},a) \right\|_{(\Lambda_{h,t}^{a})^{-1}}\leq CdH\sqrt{\chi}\]

_where \(\chi=\log(2(c_{m}^{\prime}+1)dTA/\delta)\), then \(\mathbb{P}(\mathcal{X})\geq 1-\delta/2\)._

Proof.: The proof is the same as the proof of Lemma 7 except we increase \(\beta_{k,h}^{a}\) to account for the misspecification error. Since \(\xi\leq 1\) by assumption, the modification to \(\beta_{k,h}^{a}\) only affects \(C\). 

We next account for the noise being adversarial due to the misspecification error.

**Lemma 14**.: _Let \(\{\epsilon_{t}\}\) be any sequence such that \(|\epsilon_{\tau}|\leq B\) for any \(t\). Then, for any \((h,k,a)\in[H]\times[K]\times\mathcal{A}\) and \(F\in\mathbb{R}^{d}\), we have_

\[|F^{\top}(\Lambda_{k,h}^{a})^{-1}\sum_{t\in T_{k-1,h}^{a}}F_{h}(s_{h}^{t}) \epsilon_{t}|\leq B\sqrt{d|T_{k-1,h}^{a}|F^{\top}(\Lambda_{k,h}^{a})^{-1}F}.\]

Proof.: From the Cauchy-Schwarz inequality and Elliptical Potential Lemma (Lemma D.1 [17]), we have

\[|F^{\top}(\Lambda_{k,h}^{a})^{-1}\sum_{t\in T_{k-1,h}^{a}}F_{h}(s _{h}^{t})\epsilon_{t}| \leq B\sqrt{\left(\sum_{t\in T_{k-1,h}^{a}}F^{\top}(\Lambda_{k,h}^ {a})^{-1}F\right)\left(\sum_{t\in T_{k-1,h}^{a}}F_{h}(s_{h}^{t})^{\top}( \Lambda_{k,h}^{a})^{-1}F_{h}(s_{h}^{t})\right)}\] \[\leq B\sqrt{d|T_{k-1,h}^{a}|F^{\top}(\Lambda_{k,h}^{a})^{-1}F}.\]

Next, we bound the error between a policies \(Q\) function and our low-rank estimate.

[MISSING_PAGE_EMPTY:51]

Since \(\|\tilde{P}_{h}(\cdot|s,a)-P_{h}(\cdot|s,a)\|_{\infty}\leq 1\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), it follows that

\[|\tilde{P}_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})(s,a)-P_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})( s,a)|\leq|(\tilde{P}_{h}-P_{h})(V^{k}_{h+1}-V^{\pi}_{h+1})(s,a)|\leq 2H\xi.\]

From Lemma 14, we have \(|F_{h}(s),q^{a}_{4}|\leq 2H\xi\sqrt{d|T^{a}_{k,h}|F_{h}(s)^{\top}(\Lambda^{-1}_{h,k})^{-1}F_{h}(s)}\). Since

\[|F_{h}(s)^{\top}w^{a}_{h,k}-Q^{\pi}_{h}(s,a)|=F_{h}(s)^{\top}(w^{a}_{h,k}-w^{a, \pi}_{h})=F_{h}(s)^{\top}(q^{a}_{1}+q^{a}_{2}+q^{a}_{3}+q^{a}_{4}),\]

it follows that

\[|F_{h}(s)^{\top}w^{a}_{h,k}-Q^{\pi}_{h}(s,a)-P_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})( s,a)|\leq\sqrt{\chi}(C^{\top}d\sqrt{\chi}+2\xi\sqrt{|T^{a}_{k,h}|}d)H\sqrt{F_{h} (s)^{\top}(\Lambda^{a}_{h,k})^{-1}F_{h}(s)}+4H\xi.\]

Similarly as in [17], we choose \(c_{\beta}\) that satisfies \(C^{\top}\sqrt{\log(2)+\log(c_{\beta}+1)}\leq c_{\beta}\sqrt{\log(2)}\). 

Now, we prove that \(Q^{k}_{h}\) is an upperbound of \(Q^{*}_{h}\) conditioned on the event in Lemma 13.

**Lemma 16**.: _Suppose Assumption 7 holds. On the event \(\mathcal{X}\) defined in lemma 13, we have \(Q^{k}_{h}(s,a)\geq Q^{*}_{h}(s,a)-4H(H+1-h)\xi\) for all \((s,a,h,k)\in\mathcal{S}\times\mathcal{A}\times[H]\times[K]\)._

Proof.: We prove this with induction. The base case holds because at step \(H+1\), the value function is zero, so from Lemma 15,

\[|F_{H}(s)^{\top}w^{a}_{H,k}-Q^{*}_{H}(s,a)|\leq\beta^{a}_{k,h}\sqrt{F_{H}(s)^ {\top}(\Lambda^{a}_{H,k})^{-1})F_{H}(s)}+4H\xi\]

and thus,

\[Q^{*}_{H}(s,a)-4H\xi\leq\min\Big{(}H,F_{H}(s)^{\top}w^{a}_{H,k}+\beta^{a}_{k, h}\sqrt{F_{H}(s)^{\top}(\Lambda^{a}_{H,k})^{-1})F_{H}(s)}\Big{)}=Q^{k}_{H}(s,a).\]

From the inductive hypothesis (assuming that \(Q^{k}_{h+1}(s,a)\geq Q^{*}_{h+1}(s,a)-4H(H-h)\xi\)), it follows that \(P_{h}(V^{k}_{h+1}-V^{*}_{h+1})(s,a)\geq-4\xi\). From Lemma 15, \(F_{h}(s)^{\top}w^{a}_{h,k}-Q^{*}_{h}(s,a)\leq\beta^{a}_{k,h}\sqrt{F_{h}(s)^{ \top}(\Lambda^{a}_{h,k})^{-1})F_{h}(s)}+4H\xi\). It follows that

\[Q^{*}_{h}(s,a)-4H(H-h+1)\xi\leq\min\Big{(}H,F_{h}(s)^{\top}w^{a}_{h,k}+\beta^ {a}_{k,h}\sqrt{F_{h}(s)^{\top}(\Lambda^{a}_{h,k})^{-1})F_{h}(s)}\Big{)}=Q^{k}_ {h}(s,a).\]

This completes the proof. 

Similarly, to the regular case, the gap in the misspecified setting has a recursive formula.

**Lemma 17**.: _Let \(\delta^{k}_{h}=V^{k}_{h}(s^{k}_{h})-V^{\pi_{k}}_{h}(s^{k}_{h})\) and \(\xi^{k}_{h+1}=\mathbb{E}[\delta^{k}_{h+1}|s^{k}_{h},a^{k}_{h}]-\delta^{k}_{h+1}\). Then, on the event \(\mathcal{X}\) defined in Lemma 13, we have for any \((h,k)\):_

\[\delta^{k}_{h}\leq\delta^{k}_{h+1}+\xi^{k}_{h+1}+\beta^{a}_{k,h}\sqrt{F_{h}(s ^{k}_{h})^{\top}(\Lambda^{a}_{h,k})^{-1}F_{h}(s^{k}_{h})}+4H\xi\]

Proof.: From Lemma 15, we have for any \((s,a,h,k)\) that

\[Q^{k}_{h}(s,a)-Q^{\pi_{k}}_{h}(s,a)\leq P_{h}(V^{k}_{h+1}-V^{\pi_{k}}_{h+1})( s,a)+\beta^{a}_{k,h}\sqrt{F_{h}(s)^{\top}(\Lambda^{a}_{h,k})^{-1}F_{h}(s)}+4H\xi.\]

From the definition of \(V^{\pi_{k}}\), we have

\[\delta^{k}_{h}=Q^{k}_{h}(s^{k}_{h},a^{k}_{h})-Q^{\pi_{k}}_{h}(s^{k}_{h},a^{k}_ {h}),\]

and substituting this into the first equation finishes the proof. 

Finally, we prove the main result in the misspecified setting Theorem 9.

Proof.: Suppose that the Assumptions required in Theorem 9 hold. We condition on the event \(\mathcal{X}\) from Lemma 13 with \(p=\delta/2\) and use the notation for \(\delta_{h}^{k},\xi_{h}^{k}\) as in Lemma 17. From Lemma 16, we have \(Q_{1}^{k}(s,a)\geq Q_{1}^{*}(s,a)-4H^{2}\xi\), which implies \(V_{1}^{*}(s)-V_{1}^{\pi_{k}}(s)\leq\delta_{1}^{k}+4H^{2}\xi\). Thus, from Lemma 15, on the event \(\mathcal{X}\), it follows that

\[Regret(K) =\sum_{k=1}^{K}(V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi_{k}}(s_{1}^{k})) \leq\sum_{k=1}^{K}\delta_{1}^{k}+4H^{2}\xi\] \[\leq\sum_{k\in[K]}\sum_{h\in[H]}\xi_{h}^{k}+\sum_{k\in[K]}\beta_{ k,h}^{a}\sum_{h\in[H]}\sqrt{F_{h}(s_{h}^{k})^{\top}(\Lambda_{h,k}^{a_{h}^{k}})^{ -1}F_{h}(s_{h}^{k})}+4HT\xi\]

since \(HK=T\). Since the observations at episode \(k\) are independent of the computed value function (this uses the trajectories from episodes 1 to \(k-1\), it follows \(\{\xi_{h}^{k}\}\) is a martingale difference sequence with \(|\xi_{h}^{k}|\leq 2H\) for all \((k,h)\). Thus, from the Azuma Hoeffding inequality, we have

\[\sum_{k\in[K],h\in H}\xi_{h}^{k}\leq\sqrt{2TH^{2}\log(2/p)}\leq 2H\sqrt{Tt}\]

with probability at least \(1-\delta/2\). By construction of \(\beta_{k,h}^{a}\), we have

\[\sum_{k\in[K]}\beta_{k,h}^{a}\sqrt{F_{h}(s_{h}^{k})^{\top}( \Lambda_{h,k}^{a_{h}^{k}})^{-1}F_{h}(s_{h}^{k})} \leq CH(\sum_{k\in[K]}d\sqrt{t}\sqrt{F_{h}(s_{h}^{k})^{\top}( \Lambda_{h,k}^{a_{h}^{k}})^{-1}F_{h}(s_{h}^{k})}\] \[+\sum_{k\in[K]}\xi\sqrt{|T_{k,h}^{a}|d}\sqrt{F_{h}(s_{h}^{k})^{ \top}(\Lambda_{h,k}^{a_{h}^{k}})^{-1}F_{h}(s_{h}^{k})}).\]

From the Cauchy-Schwarz inequality, it follows that

\[\sum_{k\in[K]}\sqrt{F_{h}(s_{h}^{k})^{\top}(\Lambda_{h,k}^{a_{h}^{k}})^{-1}F_ {h}(s_{h}^{k})}\leq\left(\sqrt{K}\right)\left(\sqrt{\sum_{k\in[K]}F_{h}(s_{h} ^{k})^{\top}(\Lambda_{h,k}^{a_{h}^{k}})^{-1}F_{h}(s_{h}^{k})}\right).\]

Since the minimum eigenvalue of \(\Lambda_{h,k}^{a_{h}^{k}}\) is at least one by construction and \(\|F_{h}(s)\|\leq 1\), from the Elliptical Potential Lemma (Lemma D.2 [17]), we have for all \(a\in\mathcal{A}\) and \(h\in[H]\),

\[\sum_{k=1}^{K}F_{h}(s_{k})^{\top}(\Lambda_{h,k}^{a})^{-1}F_{h}(s_{k})\leq 2 \log(det(\Lambda_{h,k+1}^{a})/det(\Lambda_{h,0}^{a}))\]

Furthermore, we have \(\|\Lambda_{h,k+1}^{a}\|=\|\sum_{t\in T_{K,h}^{a}}F_{h}(s_{h}^{t})F_{h}(s_{h}^{ t})^{\top}+\lambda I\|\leq\lambda+|T_{K,h}^{a}|\leq\lambda+K\). It follows that

\[\sum_{k=1}^{K}F_{h}(s_{k})^{\top}(\Lambda_{h,k}^{a})^{-1}F_{h}(s_{k})\leq 2 d\log(1+K/\lambda)\leq 2d\iota,\]

so grouping the episodes by actions gives

\[\left[\sum_{k\in[K]}F_{h}(s_{h}^{k})^{\top}(\Lambda_{h,k}^{a_{h}^{k}})^{-1}F_ {h}(s_{h}^{k})\right]^{1/2}=\left[\sum_{a\in\mathcal{A}}\sum_{t\in T_{K,h,a}}F _{h}(s_{h}^{t})^{\top}(\Lambda_{h,t}^{a})^{-1}F_{h}(s_{h}^{t})\right]^{1/2} \leq\sqrt{2dA\iota}.\]For the next term, let \(k_{a,i}\) refer to the episode in which action \(a\) was taken at time step \(h\) for the \(i\)th time. Then, we first re-index the summation and then use the Cauchy-Schwarz inequality to get

\[\sum_{k\in[K]}\sqrt{|T^{a}_{k,h}|}\sqrt{F_{h}(s^{k}_{h})^{\top}( \Lambda^{a^{k}_{h}}_{h,k})^{-1}F_{h}(s^{k}_{h}))} =\sum_{a\in\mathcal{A}}\sum_{i=1}^{|T^{a}_{k,h}|}\sqrt{i}\sqrt{F ^{\top}_{k_{a,i},h}(\Lambda^{a}_{k_{a,i},h})^{-1}F_{k_{a,i},h}}\] \[\leq\sum_{a\in\mathcal{A}}\left(\sum_{i=1}^{|T^{a}_{k,h}|}i \right)^{1/2}\left(\sum_{t\in T^{a}_{k,h}}F^{\top}_{t,h}(\Lambda^{a}_{t,h})^{- 1}F_{t,h}\right)^{1/2}\] \[\leq C^{\prime}\sum_{a\in\mathcal{A}}|T^{a}_{K,h}|\left(\sum_{t \in T^{a}_{K,h}}F^{\top}_{t,h}(\Lambda^{a}_{t,h})^{-1}F_{t,h}\right)^{1/2}\] \[\leq C^{\prime}\sum_{a\in\mathcal{A}}|T^{a}_{K,h}|\sqrt{2d},\]

where the last inequality comes from holds from the above elliptical potential lemma. Applying the results to the initial inequality, we have

\[\sum_{k\in[K]}\beta^{a}_{k,h}\sqrt{F_{h}(s^{k}_{h})^{\top}(\Lambda^{a^{k}_{h}} _{h,k})^{-1}F_{h}(s^{k}_{h})}\leq CH(\sqrt{2d^{3}AK\iota^{2}}+\xi\sqrt{d}C^{ \prime}\sum_{a\in\mathcal{A}}|T^{a}_{K,h}|\sqrt{2d\iota})\leq C(\sqrt{2d^{3} AHT\iota^{2}}+\xi\sqrt{2}dT.\]

Substituting these results into the original regret bound gives

\[Regret(K)\leq C^{\prime\prime\prime}(\sqrt{d^{3}H^{3}AT\iota^{2}}+dTH\xi\sqrt{ \iota}).\]

for some constant \(C^{\prime\prime\prime}>0\) with probability at least \(1-\delta\). 

### Lsvi-Ucb-(s, S, d)

We reiterate that the steps and analysis in this subsection are essentially the same as the proofs in [17], except we have \(S\) copies of the weight vectors and gram matrices. The results and proofs follow the same structure and logic as in The proof of all the theorems and lemmas follow the same steps as the proofs for the analogous result in the \((S,d,A)\) setting except we replace \(F(s)\) with \(G(a)\) and estimate the Gram matrix and weight vectors for each state instead of each action.

To prove Theorem 5, we first introduce notation used throughout this section and then present auxiliary lemmas. We let \(\Lambda^{s}_{h,k},w^{s}_{h,k},Q^{k}_{h},V^{k}_{h},\pi^{k}\) as the parameters/estimates at episode \(k\). Furthermore, let \(V^{k}_{h}=\max_{s\in\mathcal{S}}Q^{k}_{h}(s,a)\) and \(\pi^{k}\) be the greedy policy w.r.t. \(Q^{k}_{h}\). We let \(G^{k}_{h}:=G_{h}(a^{k}_{h})\).

We first bound the log of the covering number of our function class for the value function to control the error of our algorithm's estimate, which allows us to prove our concentration result as the value function estimate is not constant.

**Lemma 18**.: _Let \(\mathcal{V}\) denote a function class of mappings from \(\mathcal{S}\) to \(\mathbb{R}\) with the following parametric form for all \(s\in\mathcal{S}\)_

\[V(s)=\min\left\{\max_{a\in\mathcal{A}}w^{\top}_{s}G(a)+\beta\sqrt{G(a)^{\top }\Lambda^{-1}_{s}G(a)},H\right\}\]

_where the parameters \(w_{s},\beta,\Lambda_{s}\) satisfy \(\|w_{s}\|\leq L\) for all \(s\in\mathcal{S}\), \(\beta\in[0,B]\), and the minimum eigenvalue of \(\Lambda_{s}\) is greater than \(\lambda\). Assume that \(\|G(a)\|\leq 1\) for all \(s\in\mathcal{S}\), and let \(\mathcal{N}_{\epsilon}\) be the \(\epsilon\)-covering number of \(\mathcal{V}\) with respect to the distance \(dist(V,V^{\prime})=\sup_{s}|V(s)-V^{\prime}(s)|\). Then,_

\[\log(\mathcal{N}_{\epsilon})\leq d\log(1+4L/\epsilon)+d^{2}\log(1+8d^{1/2}B^{2 }/(\lambda\epsilon^{2}))\]

Proof.: Equivalently, we reparameterize the function class by \(D_{s}=\beta^{2}\Lambda^{-1}_{s}\), for all \(s\in\mathcal{S}\)

\[V(s)=\min\left\{\max_{a\in\mathcal{A}}w^{\top}_{s}G(a)+\sqrt{G(a)^{\top}D_{s}G (a)},H\right\}\]where \(\|w_{s}\|\leq L\) and \(\|D_{s}\|\leq B^{2}/\lambda\) for all \(s\in\mathcal{S}\). For any \(V_{1},V_{2}\in\mathcal{V}\), let them take the form in the above equation with parameters \((w_{s},D_{s})\) and \((w_{s^{\prime}},D_{s^{\prime}})\). Since \(\min(\cdot,H)\) and \(\max_{s\in\mathcal{S}}\) are contraction maps,

\[dist(V_{1}-V_{2}) \leq\sup_{s,a}|(w_{s}^{\top}G(a)+\sqrt{G(a)^{\top}D_{s}G(a)})-(w_ {s^{\prime}}^{\top}G(a)+\sqrt{G(a)^{\top}D_{s^{\prime}}G(a)})|\] \[\leq\sup_{s,G:\|G\|\leq 1}|(w_{s}^{\top}G+\sqrt{G^{\top}D_{s}G})-(w _{s^{\prime}}^{\top}G+\sqrt{G^{\top}D_{s^{\prime}}G})|\] \[\leq\max_{s\in\mathcal{S}}\left[\sup_{G:\|G\|\leq 1}|(w_{s}-w_{s^{ \prime}})^{\top}G|+\sup_{G:\|G\|\leq 1}|\sqrt{G^{\top}(D_{s}-D_{s^{\prime}})G}|\right]\] \[\leq\max_{s\in\mathcal{S}}\left[\|w_{s}-w_{s^{\prime}}\|+\sqrt{ \|D_{s}-D_{s^{\prime}}\|}\right]\leq\max_{s\in\mathcal{S}}\left[\|w_{s}-w_{s^ {\prime}}\|+\sqrt{\|D_{s}-D_{s^{\prime}}\|_{F}}\right]\]

where the third inequality holds because \(|\sqrt{x}-\sqrt{y}|\leq\sqrt{|x-y|}\) for any \(x,y\geq 0\). Let \(C_{w}\) be an \(\epsilon/2\)-cover of \(\{w\in\mathbb{R}^{d}|\|w\|\leq L\}\) with respect to the 2-norm, and \(C_{D}\) be an \(\epsilon^{2}/4\)-cover of \(\{D\in\mathbb{R}^{d\times d}|\|D\|_{F}\leq d^{1/2}B^{2}/\lambda\}\) with respect to the Frobenius norm. By Lemma D.5 from [17], we know that,

\[|C_{w}|\leq(1+4L/\epsilon)^{d},\quad|C_{D}|\leq(1+8d^{1/2}B^{2}/(\lambda \epsilon^{2}))^{d^{2}}.\]

Hence, for any \(V_{1}\), there exists a \(w_{s^{\prime}}\in C_{w}\) and \(D_{s^{\prime}}\in C_{D}\) for all \(a\in\mathcal{A}\) such that \(V_{2}\) parameterized by \((w_{s^{\prime}},D_{s^{\prime}})\) satisfies \(dist(V_{1},V_{2})\leq\epsilon\). Hence, \(\mathcal{N}_{\epsilon}\leq|C_{w}||C_{D}|\), which gives:

\[\log(\mathcal{N}_{\epsilon})\leq\log(|C_{w}|)+\log(|C_{D}|)\leq d\log(1+4 \epsilon L/\epsilon)+d^{2}\log(1+8d^{1/2}B^{2}/(\lambda\epsilon^{2})).\]

We next bound the \(\ell_{2}\)-norm of the weight vector of any policy.

**Lemma 19**.: _Let \(\{w_{h}^{s}\}\) be the weight vector of some policy \(\pi\), i.e., \(Q_{h}^{\pi}(s,a)=G_{h}(a)^{\top}w_{h}^{s}\). Then,_

\[\|w_{h}^{s}\|\leq 2H\sqrt{d}.\]

Proof.: Note that \(Q_{h}^{\pi}(s,a)=r_{h}(s,a)+\sum_{s^{\prime}}V_{h+1}^{\pi}(s^{\prime})P_{h}(s^ {\prime}|s,a)\), so, using the low-rank representations of \(r_{h},P_{h}\), it follows that

\[\|w_{h}^{s}\|_{2}=\|W_{h}(s,\cdot)+\sum_{s^{\prime}}V^{\pi}(s^{\prime})U_{h}(s ^{\prime},s,\cdot)\|_{2}\leq\|W_{h}(s,\cdot)\|_{2}+\|\sum_{s^{\prime}}V^{\pi}( s^{\prime})U_{h}(s^{\prime},s,\cdot)\|_{2}.\]

Recall that since the feature mapping was scaled up (\(G_{h}=\sqrt{\frac{A}{d\mu}}G^{\prime}\)) to ensure the max entries of \(G\) and \(W,U\) are on the same scale, it follows that \(\|W_{h}(s)\|_{2}\leq 1\) and \(\|\sum_{s^{\prime}\in\mathcal{S}}V_{h+1}^{\pi}(s^{\prime})U_{h}(s^{\prime},s) \|_{2}\leq H\). Therefore, it follows that \(\|w_{h}^{s}\|_{2}\leq 2H\sqrt{d}\). 

We next bound the 2-norm of the weight vector from the algorithm.

**Lemma 20**.: _For any \(k,h,s\), the weight vector from the above algorithm satisfies \(\|w_{h,k}^{s}\|\leq 2H\sqrt{dk/\lambda}\)._

Proof.: For any \(G_{h}(a)=g\), we have for all \(s\in\mathcal{S}\),

\[|g^{\top}w_{h,k}^{s}| =|g^{\top}(\Lambda_{h}^{s})^{-1}\sum_{t\in T_{k-1,h}^{s}}G_{h}(a_ {h}^{t})\left[r_{h}(s,a_{h}^{t})+\max_{a^{\prime}\in\mathcal{A}}Q_{h+1}(s_{h+1} ^{t},a^{\prime})\right]|\] \[\leq|\sum_{t\in T_{k-1,h}^{s}}g^{\top}(\Lambda_{h}^{s})^{-1}G_{ h}(a_{h}^{t})|2H\] \[\leq 2H\sqrt{(\sum_{t\in T_{k-1,h}^{s}}g^{\top}(\Lambda_{h}^{s})^{ -1}g)(\sum_{t\in T_{k-1,h}^{s}}G_{h}(a_{h}^{t})^{\top}(\Lambda_{h}^{s})^{-1}G_{ h}(a_{h}^{t}))}\] \[\leq 2H\|g\|_{2}\sqrt{d|T_{k-1,h}^{s}|/\lambda}\leq 2H\sqrt{dk/\lambda}\]where the second to last inequality is due to Lemma D.1 from [17] and that Rayleigh quotients are upper bounded by the largest eigenvalue of the matrix (Theorem 4.2.2 from [14]). The last inequality is due to \(\|g\|_{2}\leq 1\) and \(|T_{k-1,h}^{s}|\leq k\). 

We next prove the main concentration lemma that controls the estimate.

**Lemma 21**.: _Let \(c^{\prime}\) be the constant in the definition of \(\beta_{k,h}^{s}=c^{\prime}dH\sqrt{\iota}\). Then, there exists a constant \(C>0\) that is independent of \(c^{\prime}\) such that for any \(\delta\in(0,1)\), if we let the event \(\mathcal{X}\) be_

\[\forall(s,k,h)\in\mathcal{S}\times[K]\times[H]:\qquad\left\|\sum_{t\in T_{k-1,h}^{s}}G_{h}(a_{h}^{t})(V_{h+1}^{t}(s_{h+1}^{t})-P_{h}V_{h+1}^{t}(s,a_{h}^{t} )\right\|_{(\Lambda_{h,t}^{s})^{-1}}\leq CdH\sqrt{\chi}\]

_where \(\chi=\log(2(c^{\prime}+1)dTS/\delta)\), then \(\mathbb{P}(\mathcal{X})\geq 1-\delta/2\)._

Proof.: From Lemma 20, we have \(\|w_{h,k}^{s}\|_{2}\leq 2H\sqrt{dk/\lambda}\). By construction, the minimum eigenvalue of \(\Lambda_{h,t}^{s}\) is lower bounded by \(\lambda\). Then, from Lemma D.4 in [17] (with \(\delta^{\prime}=\delta/S\)), Lemma 18, and taking a union bound over all states, we have that with probability at least \(1-\delta^{\prime}/2\), for all \(s\in\mathcal{S}\) and \(\epsilon>0\),

\[\left\|\sum_{t\in T_{k-1,h}^{s}}G_{h}(a_{h}^{t})(V_{h+1}^{t}(s_{h+1}^{t})-P_{h }V_{h+1}^{t}(s,a_{h}^{t})\right\|_{(\Lambda_{h,t}^{s})^{-1}}^{2}\leq 4H^{2} \left[\frac{d}{2}\log((k+\lambda)/\lambda))+\log(2S\mathcal{N}_{\epsilon}/ \delta)\right]+8k^{2}\epsilon^{2}/\lambda.\]

Then, from Lemma 18, it follows that with probability at least \(1-\delta^{\prime}/2\), for all \(a\in S\) and \(\epsilon>0\),

\[\left\|\sum_{t\in T_{k-1,h}^{s}}G_{h}(a_{h}^{t})(V_{h+1}^{t}(s_{h +1}^{t})-P_{h}V_{h+1}^{t}(s,a_{h}^{t})\right\|_{(\Lambda_{h,t}^{s})^{-1}}^{2}\] \[\leq 4H^{2}\left[\frac{d}{2}\log((k+\lambda)/\lambda))+\log(2S/ \delta)+d\log(1+4L/\epsilon)+d^{2}\log(1+8d^{1/2}\beta^{2}/(\lambda\epsilon^{ 2}))\right]+8k^{2}\epsilon^{2}/\lambda.\]

Choosing \(\lambda=1,\beta_{k,h}^{s}=CdH\iota,\epsilon=dH/k\) gives that there exists some constant \(C^{\prime}\) such that

\[\left\|\sum_{t\in T_{k-1,h}^{s}}G_{h}(a_{h}^{t})(V_{h+1}^{t}(s_{h+1}^{t})-P_{ h}V_{h+1}^{t}(s,a_{h}^{t})\right\|_{(\Lambda_{h,t}^{s})^{-1}}^{2}\leq C^{\prime}d^{2} H^{2}\log(2(c_{\beta}+1)dTS/\delta).\]

We next recursively bound the difference between the value function maintained in the algorithm and the true value function of any policy \(\pi\). We upperbound this with their expected difference and an additional error term, which is bounded with high probability.

**Lemma 22**.: _There exists a constant \(c_{\beta}\) such that \(\beta_{k,h}^{s}=c_{\beta}dH\sqrt{\iota}\) where \(\iota=\log(2dST/p)\) and for any fixed policy \(\pi\), on the event \(\mathcal{X}\) defined in Lemma 21, we have for all \((s,a,h,k)\in S\times S\times[H]\times[K]\):_

\[G_{h}(a)^{\top}w_{h,k}^{s}-Q_{h}^{\pi}(s,a)=P_{h}(V_{h+1}^{k}-V_{h+1}^{\pi})( s,a)+\delta_{h}^{k}(s,a),\]

_where \(|\delta_{h}^{k}(s,a)|\leq\beta_{k,h}^{s}\sqrt{G_{h}(a)^{\top}(\Lambda_{h,k}^{s })^{-1}G_{h}(a)}\)._

Proof.: From our low-rank assumption, it follows that

\[Q_{h}^{\pi}(s,a)=G_{h}(a)^{\top}w_{h}^{s}=(r_{h}+P_{h}V_{h+1}^{\pi})(s,a),\]which following the steps from the proof of Lemma B.4 in [17] gives

\[w^{s}_{h,k}-w^{s,\pi}_{h} =-\lambda(\Lambda^{s}_{h,k})^{-1}w^{s,\pi}_{h}\] \[+(\Lambda^{s}_{h,k})^{-1}\sum_{t\in T^{s}_{k-1,h}}G_{h}(a^{t}_{h}) (V^{k}_{h+1}(s^{t}_{h+1})-P_{h}V^{k}_{h+1}(s,a^{t}_{h})\] \[+(\Lambda^{s}_{h,k})^{-1})\sum_{t\in T^{s}_{k-1,h}}G_{h}(a^{t}_{h} )P_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})(s,a^{t}_{h})\]

Now, we bound each term (\(q^{s}_{1},q^{s}_{2},q^{s}_{3}\)) individually, for the first term, note for all \(s\in S,a\in\mathcal{A}\)

\[|G_{h}(a)^{\top}q^{s}_{1}|=|\lambda G_{h}(a)^{\top}(\Lambda^{s}_{h,k})^{-1}w^{s,\pi}_{h}|\leq\sqrt{\lambda}\|w^{s,\pi}_{h}\|\sqrt{G_{h}(a)^{\top}(\Lambda^{s}_ {h,k})^{-1}G_{h}(a)}\]

For the second term, given the event \(\mathcal{X}\), we have from Cauchy Schwarz

\[|G_{h}(a)^{\top}q^{s}_{2}|\leq CdH\sqrt{\chi}\sqrt{G_{h}(a)^{\top}(\Lambda^{s} _{h,k})^{-1}G_{h}(a)}\]

where \(\chi=\log(2(c_{\beta}+1)dTS/p)\). For the third term,

\[G_{h}(a)^{\top}q^{s}_{3} =G_{h}(a)^{\top}\left((\Lambda^{s}_{h,k})^{-1}\right)\sum_{t\in T^ {s}_{k-1,h}}G_{h}(a^{t}_{h})P_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})(s,a^{t}_{h})\] \[=G_{h}(a)^{\top}\left((\Lambda^{s}_{h,k})^{-1}\right)\sum_{t\in T ^{s}_{k-1,h}}G_{h}(a^{t}_{h})G_{h}(a^{t}_{h})^{\top}\sum_{s^{\prime}\in \mathcal{S}}(V^{k}_{h+1}-V^{\pi}_{h+1})(s^{\prime})U_{h}(s^{\prime},s)\Bigg{)}\] \[=G_{h}(a)^{\top}\left(\sum_{s^{\prime}\in\mathcal{S}}(V^{k}_{h+1 }-V^{\pi}_{h+1})(s^{\prime})U_{h}(s^{\prime},s)\right)\] \[-\lambda G_{h}(a)^{\top}\left((\Lambda^{s}_{h,k})^{-1}\right) \sum_{s^{\prime}\in\mathcal{S}}(V^{k}_{h+1}-V^{\pi}_{h+1})(s^{\prime})U_{h}(s ^{\prime},s)\Bigg{)}\]

We note that

\[G_{h}(a)^{\top}\left(\sum_{s^{\prime}\in\mathcal{S}}(V^{k}_{h+1}-V^{\pi}_{h+1 })(s^{\prime})U_{h}(s^{\prime},s)\right)=P_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})(s,a)\]

and

\[|\lambda G_{h}(a)^{\top}\left((\Lambda^{s}_{h,k})^{-1}\right)\sum_{s^{\prime} \in\mathcal{S}}(V^{k}_{h+1}-V^{k}_{h+1})(s^{\prime})U_{h}(s^{\prime},s)\Bigg{)} \mid\leq 2H\sqrt{d\lambda}\sqrt{G_{h}(a)^{\top}(\Lambda^{s}_{h,k})^{-1}G_{h} (a)}\]

Since

\[|G_{h}(a)^{\top}w^{s}_{h,k}-Q^{\pi}_{h}(s,a)|=G_{h}(a)^{\top}(w^{s}_{h,k}-w^{ s,\pi}_{h})=G_{h}(a)^{\top}(q^{s}_{1}+q^{s}_{2}+q^{s}_{3}),\]

it follows that

\[|G_{h}(a)^{\top}w^{s}_{h,k}-Q^{\pi}_{h}(s,a)-P_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})( s,a)|\leq C^{\top}dH\sqrt{\chi}\sqrt{G_{h}(a)^{\top}(\Lambda^{s}_{h,k})^{-1}G_{h} (a)}.\]

Similarly as in [17], we choose \(c_{\beta}\) that satisfies \(C"\sqrt{\log(2)+\log(c_{\beta}+1)}\leq c_{\beta}\sqrt{\log(2)}\). 

This lemma implies that by adding the appropriate bonus, \(Q^{k}_{h}\) is always an upperbound of \(Q^{*}_{h}\) with high probability

**Lemma 23**.: _On the event \(\mathcal{X}\) defined in lemma 21, we have \(Q^{k}_{h}(s,a)\geq Q^{*}_{h}(s,a)\) for all \((s,a,h,k)\in\mathcal{S}\times\mathcal{A}\times[H]\times[K]\)._Proof.: We prove this with induction. The base case holds because at step \(H+1\), the value function is zero, so from Lemma 22,

\[|G_{h}(a)^{\top}w^{s}_{h,k}-Q^{*}_{H}(s,a)|\leq\beta^{s}_{k,h}\sqrt{G_{h}(a)^{ \top}(\Lambda^{s}_{h,k})^{-1})G_{h}(a)}\]

and thus,

\[Q^{*}_{H}(s,a)\leq\min\left(H,G_{h}(a)^{\top}w^{s}_{h,k}+\beta^{s}_{k,h}\sqrt{ G_{h}(a)^{\top}(\Lambda^{s}_{h,k})^{-1})G_{h}(a)}\right)=Q^{k}_{H}(s,a).\]

From the inductive hypothesis (assuming that \(Q^{k}_{h+1}(s,a)\geq Q^{*}_{h+1}(s,a)\)), it follows that \(P_{h}(V^{k}_{h+1}-V^{*}_{h+1})(s,a)\geq 0\). From Lemma 22, \(G_{h}(a)^{\top}w^{s}_{h,k}-Q^{*}_{h}(s,a)\leq\beta^{s}_{k,h}\sqrt{G_{h}(a)^{ \top}(\Lambda^{s}_{h,k})^{-1}G_{h}(a)}\). It follows that

\[Q^{*}_{h}(s,a)\leq\min\left(H,G_{h}(a)^{\top}w^{s}_{h,k}+\beta^{s}_{k,h}\sqrt{ G_{h}(a)^{\top}(\Lambda^{s}_{h,k})^{-1})G_{h}(a)}\right)=Q^{k}_{h}(s,a).\]

We next show that Lemma 22 transforms to the recursive formula \(\delta^{k}_{h}=V^{k}_{h}(s^{k}_{k})-V^{\pi_{k}}_{h}(s^{k}_{h})\).

**Lemma 24**.: _Let \(\delta^{k}_{h}=V^{k}_{h}(s^{k}_{h})-V^{\pi_{k}}_{h}(s^{k}_{h})\) and \(\xi^{k}_{h+1}=\mathbb{E}[\delta^{k}_{h+1}|s^{k}_{h},s^{k}_{h}]-\delta^{k}_{h+1}\). Then, on the event \(\mathcal{X}\) defined in Lemma 21, we have for any \((h,k)\):_

\[\delta^{k}_{h}\leq\delta^{k}_{h+1}+\xi^{k}_{h+1}+\beta^{s}_{k,h}\sqrt{G_{h}(a ^{k}_{h})^{\top}(\Lambda^{s^{k}_{h}}_{h,k})^{-1}G_{h}(a^{k}_{h})}\]

Proof.: From Lemma 22, we have for any \((s,a,h,k)\) that

\[Q^{k}_{h}(s,a)-Q^{\pi_{k}}_{h}(s,a)\leq P_{h}(V^{k}_{h+1}-V^{\pi_{k}}_{h+1})(s,a)+\beta^{s}_{k,h}\sqrt{G_{h}(a)^{\top}(\Lambda^{s}_{h,k})^{-1}G_{h}(a)}.\]

From the definition of \(V^{\pi_{k}}\), we have

\[\delta^{k}_{h}=Q^{k}_{h}(s^{k}_{h},a^{k}_{h})-Q^{\pi_{k}}_{h}(s^{k}_{h},a^{k}_ {h}).\]

Finally, we prove the main theorem, Theorem 5.

Proof.: Suppose that the Assumptions required in Theorem 5 hold. We condition on the event \(\mathcal{X}\) from Lemma 21 with \(p=\delta/2\) and use the notation for \(\delta^{k}_{h},\xi^{k}_{h}\) as in Lemma 24. From Lemmas 9 and 24, we have

\[Regret(K)=\sum_{k=1}^{K}(V^{*}_{1}(s^{k}_{1})-V^{\pi_{k}}_{1}(s^{k}_{1}))\leq \sum_{k=1}^{K}\delta^{k}_{1}\leq\sum_{k\in[K]}\sum_{h\in[H]}\xi^{k}_{h}+\beta^{ s}_{k,h}\sum_{k\in[K]}\sum_{h\in[H]}\sqrt{G_{h}(a^{k}_{h})^{\top}(\Lambda^{s^{k} _{h}}_{h,k})^{-1}G_{h}(a^{k}_{h})}\]

Since the observations at episode \(k\) are independent of the computed value function (this uses the trajectories from episodes 1 to \(k-1\), it follows \(\{\xi^{k}_{h}\}\) is a martingale difference sequence with \(|\xi^{k}_{h}|\leq 2H\) for all \((k,h)\). Thus, from the Azuma Hoeffding inequality, we have

\[\sum_{k\in[K],h\in H}\xi^{k}_{h}\leq\sqrt{2TH^{2}\log(2/p)}\leq 2H\sqrt{T \iota}\]

with probability at least \(1-\delta/2\). For the second term, we note that the minimum eigenvalue of \(\Lambda^{s^{k}_{h}}_{h,k}\) is at least one by construction and \(\|G_{h}(a)\|\leq 1\). From the Elliptical Potential Lemma (Lemma D.2 [17]), we have for all \(s\in\mathcal{S}\) and \(h\in[H]\),

\[\sum_{k=1}^{K}G_{h}(a^{k}_{h})^{\top}(\Lambda^{s}_{h,k})^{-1}G_{h}(a^{k}_{h}) \leq 2\log(det(\Lambda^{s}_{h,k+1})/\det(\Lambda^{s}_{h,0}))\]Furthermore, we have \(\|\Lambda^{s}_{h,k+1}\|=\|\sum_{t\in T^{s}_{K,h}}G_{h}(a^{t}_{h})G_{h}(a^{t}_{h})^ {\top}+\lambda I\|\leq\lambda+|T^{s}_{K,h}|\leq\lambda+K\). It follows that

\[\sum_{k=1}^{K}G_{h}(a^{k}_{h})^{\top}(\Lambda^{s}_{h,k})^{-1}G_{h}(a^{k}_{h}) \leq 2d\log(1+K/\lambda)\leq 2d\iota\]

Next, by Cauchy-Schwarz and grouping the regret by each state, it follows that

\[\sum_{k\in[K]}\sum_{h\in[H]}\sqrt{G_{h}(a^{k}_{h})^{\top}(\Lambda^ {s^{s}_{h,k}}_{h,k})^{-1}G_{h}(a^{k}_{h})} \leq\sum_{h\in[H]}\sqrt{K}\left[\sum_{k\in[K]}G_{h}(a^{k}_{h})^{ \top}(\Lambda^{s^{s}_{h,k}}_{h,k})^{-1}G_{h}(a^{k}_{h})\right]^{1/2}\] \[=\sum_{h\in[H]}\sqrt{K}\left[\sum_{s\in\mathcal{S}}\sum_{t\in T^{ s}_{K,h}}G_{h}(a^{t}_{h})^{\top}(\Lambda^{s}_{h,t})^{-1}G_{h}(a^{t}_{h}) \right]^{1/2}\] \[\leq\sum_{h\in[H]}\sqrt{K}\left[\sum_{s\in\mathcal{S}}2d\iota \right]^{1/2}\] \[\leq H\sqrt{2KSd}.\]

Since \(\beta^{s}_{k,h}=cdH\sqrt{\iota}\) for some constant \(c\), from a union bound and the previous bounds, it follows that

\[Regret(K)\leq 2H\sqrt{T\iota}+\beta^{s}_{k,h}H\sqrt{2KSd}\iota\in\tilde{O}( \sqrt{d^{3}H^{3}ST}).\]

with probability at least \(1-\delta\). 

Next, we prove the helper lemmas needed in the misspecified setting and follow the proof structure and techniques used in [17].

**Lemma 25**.: _For a \(\xi\)-approximate \((S,S,d)\) Tucker rank MDP (Assumption 4), then for any policy \(\pi\) there exists corresponding weight vectors \(\{w^{s}_{h}\}\) where \(w^{s}_{h}=\Sigma_{h}W_{h}(s)+\sum_{s^{\prime}\in\mathcal{S}}V^{\pi}_{h+1}(s^{ \prime})U_{h}(s^{\prime},s)\) such that for all \((s,a)\in\mathcal{S}\times\mathcal{A}\)_

\[|Q^{\pi}_{h}(s,a)-G_{h}(a)w^{s\top}_{h}|\leq 3H\xi.\]

Proof.: Note that \(Q^{\pi}_{h}(s,a)=r_{h}(s,a)+\sum_{s^{\prime}}V^{\pi}_{h+1}(s^{\prime})P_{h}(s ^{\prime}|s,a)\), so, using the low-rank representations of \(r_{h},P_{h}\), it follows that

\[|Q^{\pi}_{h}(s,a)-G_{h}(a)w^{s\top}_{h}|\] \[\leq|r_{h}(s,a)-G_{h}(a)W_{h}(s)^{\top}|+\left|\sum_{s^{\prime}} V^{\pi}_{h+1}(s^{\prime})P_{h}(s^{\prime}|s,a)-G_{h}(a)\sum_{s^{\prime}\in \mathcal{S}}V^{\pi}_{h+1}(s^{\prime})U_{h}(s^{\prime},s)\right|\] \[\leq\xi+2H\xi\] \[\leq 3H\xi.\]

**Lemma 26**.: _Suppose Assumption 7 holds. Let \(\{w^{s}_{h}\}\) be the weight vector of some policy \(\pi\), i.e., \(Q^{\pi}_{h}(s,a)=G_{h}(a)^{\top}w^{s}_{h}\). Then,_

\[\|w^{s}_{h}\|\leq 2H\sqrt{d}.\]

Proof.: Recall that \(w^{s}_{h}=\Sigma_{h}W_{h}(s)+\sum_{s^{\prime}\in\mathcal{S}}V^{\pi}_{h+1}(s^{ \prime})U_{h}(s^{\prime},s)\). From the same argument used to prove Lemma 19, it follows that \(\|w^{s}_{h}\|_{2}\leq 2H\sqrt{d}\) 

Similarly, we bound the stochastic noise but account for the misspecification error.

**Lemma 27**.: _Suppose Assumption 7 holds. Let \(c^{\prime}_{m}\) be the constant in the definition of \(\beta^{s}_{k,h}=c^{\prime}_{m}H(d\sqrt{\iota}+\xi\sqrt{|T^{s}_{k,h}|}d)\). Then, there exists a constant \(C>0\) that is independent of \(c^{\prime}_{m}\) such that for any \(\delta\in(0,1)\), if we let the event \(\mathcal{X}\) be_

\[\forall(a,k,h)\in\mathcal{A}\times[K]\times[H]:\qquad\left\|\sum_{t\in T^{s}_ {k-1,h}}G_{h}(a^{t}_{h})(V^{t}_{h+1}(s^{t}_{h+1})-P_{h}V^{t}_{h+1}(s,a^{t}_{h} )\right\|_{(\Lambda^{s}_{h,t})^{-1}}\leq CdH\sqrt{\chi}\]

_where \(\chi=\log(2(c^{\prime}_{m}+1)dTS/\delta)\), then \(\mathbb{P}(\mathcal{X})\geq 1-\delta/2\)._

Proof.: The proof is the same as the proof of Lemma 21 except we increase \(\beta^{s}_{k,h}\) to account for the misspecification error. Since \(\xi\leq 1\) by assumption, the modification to \(\beta^{s}_{k,h}\) only affects \(C\). 

Next, we account for the adversarial misspecification error.

**Lemma 28**.: _Let \(\{\epsilon_{t}\}\) be any sequence such that \(|\epsilon_{\tau}|\leq B\) for any \(t\). Then, for any \((h,k,s)\in[H]\times[K]\times\mathcal{S}\) and \(G\in\mathbb{R}^{d}\), we have_

\[|G^{\top}(\Lambda^{s}_{k,h})^{-1}\sum_{t\in T^{s}_{k-1,h}}G_{h}(a^{t}_{h}) \epsilon_{t}|\leq B\sqrt{d|T^{s}_{k-1,h}|G^{\top}(\Lambda^{s}_{k,h})^{-1}G}.\]

Proof.: From the Cauchy-Schwarz inequality and Elliptical Potential Lemma (Lemma D.1 [17]), we have

\[|G^{\top}(\Lambda^{s}_{k,h})^{-1}\sum_{t\in T^{s}_{k-1,h}}G_{h}(a ^{t}_{h})\epsilon_{t}| \leq B\sqrt{\left(\sum_{t\in T^{s}_{k-1,h}}G^{\top}(\Lambda^{s}_{ k,h})^{-1}G\right)\left(\sum_{t\in T^{s}_{k-1,h}}G_{h}(a^{t}_{h})^{\top}( \Lambda^{s}_{k,h})^{-1}G_{h}(a^{t}_{h})\right)}\] \[\leq B\sqrt{d|T^{s}_{k-1,h}|G^{\top}(\Lambda^{s}_{k,h})^{-1}G}.\]

Next, we bound the error between a policies \(Q\) function and our low-rank estimate.

**Lemma 29**.: _There exists a constant \(c^{\prime}_{m}\) such that \(\beta^{s}_{k,h}=c^{\prime}_{m}H(d\sqrt{\iota}+\xi\sqrt{|T^{s}_{k,h}|}d)\) where \(\iota=\log(2dST/p)\) and for any fixed policy \(\pi\), on the event \(\mathcal{X}\) defined in Lemma 27, we have for all \((s,a,h,k)\in S\times S\times[H]\times[K]\):_

\[G_{h}(a)^{\top}w^{s}_{h,k}-Q^{\pi}_{h}(s,a)=P_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})( s,a)+\delta^{k}_{h}(s,a),\]

_where \(|\delta^{k}_{h}(s,a)|\leq\beta^{s}_{k,h}\sqrt{G_{h}(a)^{\top}(\Lambda^{s}_{h,k})^{-1}G_{h}(a)}+4H\xi\)._

Proof.: From Lemma 25, it follows that there exists a weight vector \(w^{s}_{h}=W_{h}(a)+\sum_{s^{\prime}\in\mathcal{S}}V^{\pi}_{h+1}(s^{\prime})U _{h}(s^{\prime},s)\), such that for all \((s,a)\in\mathcal{S}\times\mathcal{A}\),

\[|Q^{\pi}_{h}(s,a)-G_{h}(a)^{\top}w^{s}_{h}|\leq 2H\xi.\]

Let \(\tilde{P}(\cdot|s,a)=G_{h}(a)^{\top}U_{h}(\cdot,s)\), so we have for any \((s,a)\in\mathcal{S}\times\mathcal{A}\), \(G_{h}(a)^{\top}w^{s}_{h}=G_{h}(a)^{\top}W_{h}(a)+\tilde{P}V^{\pi}_{h+1}(s,a)\). Therefore,

\[w^{s}_{h,k}-w^{s,\pi}_{h} =-\lambda(\Lambda^{s}_{h,k})^{-1}w^{s,\pi}_{h}\] \[+(\Lambda^{s}_{h,k})^{-1}\sum_{t\in T^{s}_{k-1,h}}G_{h}(a^{t}_{h })(V^{k}_{h+1}(s^{t}_{h+1})-P_{h}V^{k}_{h+1}(s,a^{t}_{h})\] \[+(\Lambda^{s}_{h,k})^{-1}\sum_{t\in T^{s}_{k-1,h}}G_{h}(a^{t}_{h })\tilde{P}_{h}(V^{k}_{h+1}-V^{\pi}_{h+1})(s,a^{t}_{h})\] \[+(\Lambda^{s}_{h,k})^{-1}\sum_{t\in T^{s}_{k-1,h}}G_{h}(a^{t}_{h })\left(r_{h}(s,a^{t}_{h})-G_{h}(a^{t}_{h})W_{h}(a)+(P_{h}-\tilde{P}_{h})V^{k}_ {h+1}(s,a^{t}_{h})\right)\]Now, we bound each of the four terms above \((q_{1}^{s},q_{2}^{s},q_{3}^{s},q_{4}^{s})\) individually, for the first term, note for all \(a\in\mathcal{A}\)

\[|G_{h}(a)^{\top}q_{1}^{s}|=|\lambda G_{h}(a)^{\top}(\Lambda_{h,k}^{s})^{-1}w_{h }^{s,\pi}|\leq\sqrt{\lambda}\|w_{h}^{s,\pi}\|\sqrt{G_{h}(a)^{\top}(\Lambda_{h, k}^{s})^{-1}G_{h}(a)}\]

For the second term, given the event \(\mathcal{X}\), we have from Cauchy Schwarz

\[|G_{h}(a)^{\top}q_{2}^{s}|\leq CdH\sqrt{\chi}\sqrt{G_{h}(a)^{\top}(\Lambda_{h, k}^{s})^{-1}G_{h}(a)}\]

where \(\chi=\log(2(c_{\beta}+1)dTS/p)\). For the third term,

\[G_{h}(a)^{\top}q_{3}^{s} =G_{h}(a)^{\top}\left((\Lambda_{h,k}^{s})^{-1}\sum_{t\in T_{k-1,h} ^{s}}G_{h}(a_{h}^{t})\tilde{P}_{h}(V_{h+1}^{k}-V_{h+1}^{\pi})(s,a_{h}^{t})\right)\] \[=G_{h}(a)^{\top}\left((\Lambda_{h,k}^{s})^{-1}\sum_{t\in T_{k-1,h }^{s}}G_{h}(a_{h}^{t})G_{h}(a_{h}^{t})^{\top}\sum_{s^{\prime}\in\mathcal{S}}( V_{h+1}^{k}-V_{h+1}^{\pi})(s^{\prime})U_{h}(s^{\prime},s)\right)\] \[=G_{h}(a)^{\top}\left(\sum_{s^{\prime}\in\mathcal{S}}(V_{h+1}^{k} -V_{h+1}^{\pi})(s^{\prime})U_{h}(s^{\prime},s)\right)\] \[-\lambda G_{h}(a)^{\top}\left((\Lambda_{h,k}^{s})^{-1}\sum_{s^{ \prime}\in\mathcal{S}}(V_{h+1}^{k}-V_{h+1}^{\pi})(s^{\prime})U_{h}(s^{\prime},s)\right),\]

and note that

\[G_{h}(a)^{\top}\left(\sum_{s^{\prime}\in\mathcal{S}}(V_{h+1}^{k}-V_{h+1}^{\pi })(s^{\prime})U_{h}(s^{\prime},s)\right)=\tilde{P}_{h}(V_{h+1}^{k}-V_{h+1}^{ \pi})(s,a)\]

and

\[|\lambda G_{h}(a)^{\top}\left((\Lambda_{h,k}^{s})^{-1}\sum_{s^{\prime}\in \mathcal{S}}(V_{h+1}^{k}-V_{h+1}^{k})(s^{\prime})U_{h}(s^{\prime},s)\right)| \leq 2H\sqrt{d\lambda}\sqrt{G_{h}(a)^{\top}(\Lambda_{h,k}^{s})^{-1}G_{h}(a)}.\]

Since \(\|\tilde{P}_{h}(\cdot|s,a)-P_{h}(\cdot|s,a)\|_{\infty}\leq 1\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\), it follows that

\[|\tilde{P}_{h}(V_{h+1}^{k}-V_{h+1}^{\pi})(s,a)-P_{h}(V_{h+1}^{k}-V_{h+1}^{\pi })(s,a)|\leq|(\tilde{P}_{h}-P_{h})(V_{h+1}^{k}-V_{h+1}^{\pi})(s,a)|\leq 2H\xi.\]

From Lemma 28, we have \(|G_{h}(a),q_{4}^{s}|\leq 2H\xi\sqrt{d|T_{k,h}^{s}|G_{h}(a)^{\top}(\Lambda_{h,k}^{- 1})^{-1}G_{h}(a)}\). Since

\[|G_{h}(a)^{\top}w_{h,k}^{s}-Q_{h}^{\pi}(s,a)|=G_{h}(a)^{\top}(w_{h,k}^{s}-w_{h }^{s,\pi})=G_{h}(a)^{\top}(q_{1}^{s}+q_{2}^{s}+q_{3}^{s}+q_{4}^{s}),\]

it follows that

\[|G_{h}(a)^{\top}w_{h,k}^{s}-Q_{h}^{\pi}(s,a)-P_{h}(V_{h+1}^{k}-V_{h+1}^{\pi})( s,a)|\leq\sqrt{\chi}(C^{\top}d\sqrt{\chi}+2\xi\sqrt{|T_{k,h}^{s}|d})H\sqrt{G_{h}(a)^{ \top}(\Lambda_{h,k}^{s})^{-1}G_{h}(a)}+4H\xi.\]

Similarly as in [17], we choose \(c_{\beta}\) that satisfies \(C"\sqrt{\log(2)+\log(c_{\beta}+1)}\leq c_{\beta}\sqrt{\log(2)}\). 

Now, we prove that \(Q_{h}^{k}\) is an upperbound of \(Q_{h}^{*}\) conditioned on the event in Lemma 27.

**Lemma 30**.: _Suppose Assumption 7 holds. On the event \(\mathcal{X}\) defined in lemma 27, we have \(Q_{h}^{k}(s,a)\geq Q_{h}^{*}(s,a)-4H(H+1-h)\xi\) for all \((s,a,h,k)\in\mathcal{S}\times\mathcal{A}\times[H]\times[K]\)._

Proof.: We prove this with induction. The base case holds because at step \(H+1\), the value function is zero, so from Lemma 29,

\[|G_{h}(a)^{\top}w_{h,k}^{s}-Q_{H}^{*}(s,a)|\leq\beta_{k,h}^{s}\sqrt{G_{h}(a)^{ \top}(\Lambda_{h,k}^{s})^{-1})G_{h}(a)}+4H\xi\]and thus,

\[Q_{H}^{*}(s,a)-4H\xi\leq\min\left(H,G_{h}(a)^{\top}w_{h,k}^{*}+\beta_{k,h}^{s} \sqrt{G_{h}(a)^{\top}(\Lambda_{h,k}^{s})^{-1})G_{h}(a)}\right)=Q_{H}^{k}(s,a).\]

From the inductive hypothesis (assuming that \(Q_{h+1}^{k}(s,a)\geq Q_{h+1}^{*}(s,a)-4H(H-h)\xi\)), it follows that \(P_{h}(V_{h+1}^{k}-V_{h+1}^{*})(s,a)\geq-4\xi\). From Lemma 29, \(G_{h}(a)^{\top}w_{h,k}^{s}-Q_{h}^{*}(s,a)\leq\beta_{k,h}^{s}\sqrt{G_{h}(a)^{ \top}(\Lambda_{h,k}^{s})^{-1})G_{h}(a)}+4H\xi\). It follows that

\[Q_{h}^{*}(s,a)-4H(H-h+1)\xi\leq\min\left(H,G_{h}(a)^{\top}w_{h,k}^{s}+\beta_{k,h}^{s}\sqrt{G_{h}(a)^{\top}(\Lambda_{h,k}^{s})^{-1})G_{h}(a)}\right)=Q_{h}^{k }(s,a).\]

Similarly, to the regular case, the gap in the misspecified setting has a recursive formula.

**Lemma 31**.: _Let \(\delta_{h}^{k}=V_{h}^{k}(s_{h}^{k})-V_{h}^{\pi_{k}}(s_{h}^{k})\) and \(\xi_{h+1}^{k}=\mathbb{E}[\delta_{h+1}^{k}|s_{h}^{k},a_{h}^{k}]-\delta_{h+1}^{k}\). Then, on the event \(\mathcal{X}\) defined in Lemma 27, we have for any \((h,k)\):_

\[\delta_{h}^{k}\leq\delta_{h+1}^{k}+\xi_{h+1}^{k}+\beta_{k,h}^{s}\sqrt{G_{h}(a _{h}^{k})^{\top}(\Lambda_{h,k}^{s_{h}^{k}})^{-1}G_{h}(a_{h}^{k})}+4H\xi\]

Proof.: From Lemma 29, we have for any \((s,a,h,k)\) that

\[Q_{h}^{k}(s,a)-Q_{h}^{\pi_{k}}(s,a)\leq P_{h}(V_{h+1}^{k}-V_{h+1}^{\pi_{k}})( s,a)+\beta_{k,h}^{s}\sqrt{G_{h}(a)^{\top}(\Lambda_{h,k}^{s})^{-1}G_{h}(a)}+4H\xi.\]

From the definition of \(V^{\pi_{k}}\), we have

\[\delta_{h}^{k}=Q_{h}^{k}(s_{h}^{k},a_{h}^{k})-Q_{h}^{\pi_{k}}(s_{h}^{k},a_{h}^ {k}),\]

and substituting this into the first equation finishes the proof. 

Finally, we prove the main result in the misspecified setting Theorem 9.

Proof.: Suppose that the Assumptions required in Theorem 6 hold. We condition on the event \(\mathcal{X}\) from Lemma 27 with \(p=\delta/2\) and use the notation for \(\delta_{h}^{k},\delta_{h}^{k}\) as in Lemma 31. From Lemma 16, we have \(Q_{1}^{k}(s,a)\geq Q_{1}^{*}(s,a)-4H^{2}\xi\), which implies \(V_{1}^{*}(s)-V_{1}^{\pi_{k}}(s)\leq\delta_{1}^{k}+4H^{2}\xi\). Thus, from Lemma 29, on the event \(\mathcal{X}\), it follows that

\[Regret(K) =\sum_{k=1}^{K}(V_{1}^{*}(s_{1}^{k})-V_{1}^{\pi_{k}}(s_{1}^{k})) \leq\sum_{k=1}^{K}\delta_{1}^{k}+4H^{2}\xi\] \[\leq\sum_{k\in[K]}\sum_{h\in[H]}\xi_{h}^{k}+\sum_{k\in[K]}\beta_{ k,h}^{s}\sum_{h\in[H]}\sqrt{G_{h}(a_{h}^{k})^{\top}(\Lambda_{h,k}^{s_{h}^{k}})^{-1 }G_{h}(a_{h}^{k})}+4HT\xi\]

since \(HK=T\). Since the observations at episode \(k\) are independent of the computed value function (this uses the trajectories from episodes 1 to \(k-1\), it follows \(\{\xi_{h}^{k}\}\) is a martingale difference sequence with \(|\xi_{h}^{k}|\leq 2H\) for all \((k,h)\). Thus, from the Azuma Hoeffding inequality, we have

\[\sum_{k\in[K],h\in H}\xi_{h}^{k}\leq\sqrt{2TH^{2}\log(2/p)}\leq 2H\sqrt{T_{l}}\]

with probability at least \(1-\delta/2\).Note that

\[\sum_{k\in[K]}\beta_{k,h}^{s}\sqrt{G_{h}(a_{h}^{k})^{\top}( \Lambda_{h,k}^{s_{h}^{k}})^{-1}G_{h}(a_{h}^{k})}\] \[\leq CH(\sum_{k\in[K]}d\sqrt{\iota}\sqrt{G_{h}(a_{h}^{k})^{\top}( \Lambda_{h,k}^{s_{h}^{k}})^{-1}G_{h}(a_{h}^{k})}+\sum_{k\in[K]}\xi\sqrt{|T_{k,h }^{s}|d}\sqrt{G_{h}(a_{h}^{k})^{\top}(\Lambda_{h,k}^{s_{h}^{k}})^{-1}G_{h}(s_{h} ^{k})}).\]From the Cauchy-Schwarz inequality, it follows that

\[\sum_{k\in[K]}\sqrt{G_{h}(a_{h}^{k})^{\top}(\Lambda_{h,k}^{s^{k}})^{-1}G_{h}(a_{h} ^{k})}\leq\left(\sqrt{K}\right)\left(\sqrt{\sum_{k\in[K]}G_{h}(a_{h}^{k})^{\top} (\Lambda_{h,k}^{s^{k}})^{-1}G_{h}(a_{h}^{k})}\right).\]

Let \(k_{i}\) refers to the episode in which state \(s\) was seen at time step \(h\) for the \(i\)th time. Then, for the other term, we first re-index the summation and then use the Cauchy-Schwarz inequality to get

\[\sum_{k\in[K]}\sqrt{|T_{k,h}^{s}|}\sqrt{G_{h}(a_{h}^{k})^{\top}( \Lambda_{h,k}^{s^{k}})^{-1}G_{h}(a_{h}^{k})}) =\sum_{s\in\mathcal{S}}\sum_{i=1}^{|T_{k,h}^{s}|}\sqrt{i}\sqrt{G _{k_{i},h}^{\top}(\Lambda_{k_{i},h}^{a})^{-1}G_{k_{i},h}}\] \[\leq\left(\sum_{i=1}^{|T_{k,h}^{s}|}i\right)^{1/2}\left(\sum_{t \in T_{k,h}^{s^{k}}}G_{k,h}^{\top}(\Lambda_{k,h}^{s^{k}})^{-1}G_{k,h}\right)^ {1/2}\] \[\leq C^{\prime}|T_{K,h}^{s}|\left(\sum_{t\in T_{K,h}^{s}}G_{k,h}^ {\top}(\Lambda_{k,h}^{s^{k}})^{-1}G_{k,h}\right)^{1/2}\]

for some absolute constant \(C^{\prime}>0\). Since the minimum eigenvalue of \(\Lambda_{h,k}^{s^{k}}\) is at least one by construction and \(\|G_{h}(s)\|\leq 1\), from the Elliptical Potential Lemma (Lemma D.2 [17]), we have for all \(a\in\mathcal{A}\) and \(h\in[H]\),

\[\sum_{k=1}^{K}G_{h}(s_{k})^{\top}(\Lambda_{h,k}^{a})^{-1}G_{h}(s_{k})\leq 2 \log(det(\Lambda_{h,k+1}^{a})/det(\Lambda_{h,0}^{a}))\]

Furthermore, we have \(\|\Lambda_{h,k+1}^{a}\|=\|\sum_{t\in T_{k,h}^{s}}G_{h}(s_{h}^{t})G_{h}(s_{h}^ {t})^{\top}+\lambda I\|\leq\lambda+|T_{k,h}^{s}|\leq\lambda+K\). It follows that

\[\sum_{k=1}^{K}G_{h}(s_{k})^{\top}(\Lambda_{h,k}^{a})^{-1}G_{h}(s_{k})\leq 2d \log(1+K/\lambda)\leq 2d\iota,\]

so grouping the episodes by actions gives

\[\left[\sum_{k\in[K]}G_{h}(a_{h}^{k})^{\top}(\Lambda_{h,k}^{s^{k}})^{-1}G_{h}( a_{h}^{k})\right]^{1/2}=\left[\sum_{s\in\mathcal{S}}\sum_{t\in T_{K,h,a}}G_{h}(a_{h}^ {t})^{\top}(\Lambda_{h,t}^{a})^{-1}G_{h}(a_{h}^{t})\right]^{1/2}\leq\sqrt{2dS \iota}\]

Thus,

\[\sum_{k\in[K]}\beta_{k,h}^{s}\sqrt{G_{h}(a_{h}^{k})^{\top}( \Lambda_{h,k}^{s^{k}})^{-1}G_{h}(a_{h}^{k})} \leq CH(\sqrt{2d^{3}SK\iota^{2}}+\xi\sqrt{d}C^{\prime}\sum_{s\in \mathcal{S}}|T_{k,h}^{s}|\sqrt{2d\iota})\] \[\leq C(\sqrt{2d^{3}SHT\iota^{2}}+\xi\sqrt{2\iota}dT.\]

Substituting these results into the original regret bound gives

\[Regret(K)\leq C^{\prime\prime\prime}(\sqrt{d^{3}H^{3}ST\iota^{2}}+dTH\xi\sqrt{ \iota}).\]

for some constant \(C^{\prime\prime\prime}>0\) with probability at least \(1-\delta\). 

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our claims in the abstract and introduction reflect our theoretical results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the optimality of our theoretical guarantees with respect to the parameters of the problem. We highlight that our algorithm is sub-optimal with respect to the rank and horizon of the MDP. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]. Justification: We include all assumptions and proofs for each theorem in this work. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: There are no experiments in this paper. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: There is no data or code used in this paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: There are no experiments or data used in this paper. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA]. Justification:There are no experiments or data used in this paper. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA]. Justification: There are no experiments, so there were no computational resources needed. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes]. Justification: The contributions of this paper are theoretical results, and we believe that they will not lead to societal harm. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes]. Justification: We mention potential scenarios in which one benefits from our methodology. As this paper focuses on foundational research by proving theoretical bounds on transfer reinforcement learning, we do not believe that there are any direct paths to negative applications. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: Our paper poses no such risks as are contributions are strictly theoretical. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA]. Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]. Justification: Our paper's contribution is theoretical and does not realease new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]. Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.