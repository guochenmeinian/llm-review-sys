# Bayesian Optimization with Cost-varying Variable Subsets

 Sebastian Shenghong Tay12, Chuan Sheng Foo23, Daisuke Urano4,

Richalynn Chiu Xian Leong4, Bryan Kian Hsiang Low1

1Department of Computer Science, National University of Singapore

2Institute for Infocomm Research (I2R), A*STAR, Singapore

3Centre for Frontier AI Research (CFAR), A*STAR, Singapore

4Temasek Life Sciences Laboratory, Singapore

sebastian.tay@u.nus.edu, foo_chuan_sheng@i2r.a-star.edu.sg, daisuke@tll.org.sg, richalynn@tll.org.sg, lowkh@comp.nus.edu.sg

###### Abstract

We introduce the problem of _Bayesian optimization with cost-varying variable subsets_ (BOCVS) where in each iteration, the learner chooses a subset of query variables and specifies their values while the rest are randomly sampled. Each chosen subset has an associated cost. This presents the learner with the novel challenge of balancing between choosing more informative subsets for more directed learning versus leaving some variables to be randomly sampled to reduce incurred costs. This paper presents a novel Gaussian process upper confidence bound-based algorithm for solving the BOCVS problem that is provably no-regret. We analyze how the availability of cheaper control sets helps in exploration and reduces overall regret. We empirically show that our proposed algorithm can find significantly better solutions than comparable baselines with the same budget.

## 1 Introduction

_Bayesian optimization_ (BO) is a powerful framework for the sample-efficient optimization of costly-to-evaluate black-box objective functions [11] and has been successfully applied to many experimental design problems of significance such as hyperparameter optimization [6; 39], chemical synthesis [30], and particle accelerator control [29], among others. Conventional BO assumes that the learner has full control over all query variables (i.e., all variables in the input to the objective function). However, in many real-world optimization problems, some of the query variables may be subject to randomness affecting their values. In some cases, the randomness affecting a specific variable can be eliminated (by allowing the learner to select its value), but at a cost. We illustrate with a few concrete scenarios: In precision agriculture, consider a farm aiming to find the optimal conditions for largest crop yield where the query variables are a set of soil nutrient concentrations (e.g., Ca, B, NH\({}_{3}\), K) and pH. The farm may rely on the naturally-occurring quantities of these nutrients in the available soil, but these quantities will be randomly sampled. Alternatively, they may control some subset of these quantities (via manufactured soil and fertilizers) at a higher cost. In advanced manufacturing where random variation occurs in every operation [34], certain specifications of a product may be left unspecified by the manufacturer and randomly determined, or specified but at a higher cost. In ad revenue maximization or crowdsourcing where information is gathered from a large number of individuals via ad platforms or crowdsourcing platforms such as Amazon Mechanical Turk, suppose that the query variables describe the demographics of the individual, such as country of origin or income level. The learner may allow the platform to randomly assign the task to any individuals, or the learner may demand a specific subgroup of individuals at a higher cost. In all these practical scenarios, the goal is to find the maximizer with as little incurred cost as possible. At each query iteration, the learner isfaced with the non-trivial problem of deciding which variables to specify (for more directed learning) vs. which variables to allow to be randomly sampled (to reduce incurred costs), in addition to the usual BO problem of deciding the specified variables' values.

To the best of our knowledge, there are no existing works that tackle this problem precisely. The work of Hayashi et al. [13] introduced the problem of _BO with partially specified queries_ (BOPSQ) in which the subset of deterministically selected variables (_control set_) and randomly sampled variables (_random set_) can also be chosen by the learner, but it does not consider the costs incurred by such choices. This is a non-trivial limitation as the presence of costs can significantly alter the learner's decisions. Under such a formulation, if a control set is a strict subset of another, then the former will never be chosen as there is no benefit to having variable values we randomly sampled instead of chosen by the learner. Consequently, if there exists a control set that includes all the variables in a query, then all other control sets will not be used and the problem reduces to conventional BO. In practice, however, the availability of other control sets confers an advantage if these other control sets are cheaper. Having access to cheaper but more random control sets allows the learner to explore the query space cheaply and then use costlier but more deterministic control sets to exploit high-value regions. BOPSQ in its current formulation excludes the analysis of such strategies and is akin to multi-fidelity BO [15] but without modeling the costs of the different information sources: In this case, the learner would simply choose the highest-fidelity information source all the time, thus making the problem setting trivial.

This paper introduces the problem of _BO with cost-varying variable subsets_ (BOCVS) that explicitly models the cost of each control set and is more useful in practical scenarios. Our work generalizes BOPSQ and argues that BOCVS problems are much richer when analyzed from a similar perspective as multi-fidelity BO, and the various control sets are treated as information sources with different levels of usefulness and costs. By using cheap control sets for exploration and expensive control sets for exploitation, we show that with an appropriately designed algorithm, a learner can find significantly better solutions with a lower cost expenditure. To achieve this, we leverage the _Gaussian process upper confidence bound_ (GP-UCB) acquisition function [7; 32] to design a novel _no-regret_ algorithm, i.e., its incurred simple regret tends to \(0\) as the number of iterations tends to infinity, and the algorithm's best chosen query converges to the optimal solution. We additionally analyze the impact of the availability of cheaper control sets on the regret incurred by the most expensive control set. We observe that our algorithm generally outperforms the non-cost-aware baselines, while simple extensions based on Thompson sampling, maximizing UCB or expected improvement-based acquisition scores per unit cost [31, Sec. \(3.2\)] either fail to converge or fail to utilize cheap control sets effectively. Concretely, the contributions of our work in this paper include the following:

* We introduce the BOCVS problem (Sec. 4) and solve it by designing a novel UCB-based algorithm (Sec. 4.1) with a theoretical analysis of its properties, including the conditions under which it is provably no-regret and the impact of the availability of cheaper control sets on the regret incurred by the most expensive control set, and discuss the practical considerations (Sec. 4.2);
* We empirically evaluate the performance of our proposed algorithm against the baselines under several experimental settings with synthetic and real-world datasets (Sec. 5), including a plant growth dataset and an airfoil self-noise dataset corresponding, respectively, to the precision agriculture and advanced manufacturing use cases motivated earlier in this section.

## 2 Related Work

The work of Hayashi et al. [13] introduced _BO with partially specified queries_ (BOPSQ) and tackled the problem with Thompson sampling. However, it fails to consider the relative costs of control sets, which hinders the learner's ability to take advantage of all control sets even in the presence of more deterministic control sets. The work of Oliveira et al. [25] proposed BO with uncertain inputs in which the executed query is sampled from a probability distribution depending on the proposed query. Though related, its problem setting is motivated more by uncertainty in the input query even post-observation and does not involve variable subset selection. These two works are part of a line of research investigating BO in situations where the learner may not have full control over all variables in a query, which includes BO for expected values [38], risk-averse BO [5; 21; 22], and distributionally robust BO [17; 24; 37]. These works also do not consider variable subset selection. Our treatment of the BOCVS problem is inspired by multi-fidelity BO in which the learner has access to cheap, low-fidelity surrogates of the true objective function [15; 27; 35; 36]. In such works (and in ours), modeling costs is crucial as the learner would simply choose the highest-fidelity information source (in ours, the maximally deterministic control set) otherwise. While the general idea of paying less for potentially less informative queries is similar, our problem setting is fundamentally different: The lack of informativeness comes from the uncertainty of the executed query as opposed to a bias in the observed function values.

The BOCVS setting may be viewed as a special case of causal BO as formulated by Aglietti et al. [1] and continued in several works [2; 4]. Specifically, our setting is a case in which there are no 'non-manipulative' variables and the causal DAG is such that all input variables have no parents and are parents of the output variable. Nevertheless, we believe our focus on this special case has value as it allows us to derive useful theoretical results such as algorithm regret bounds that, to the best of our knowledge, do not exist for the completely general causal BO setting at the time of writing. The work of Sussex et al. [33] includes a regret bound, but is also a special case of [1], and has little overlap with our work as it does not consider costs of control sets or explicit probability distributions over input variables. We believe that our work is sufficiently general to be useful for practical scenarios (where the full causal BO apparatus may be unnecessary), and is also a stepping stone towards theory for the general case.

## 3 BO and Gaussian Processes

We will first give a brief review of conventional BO [11]. Given a query set \(\mathcal{X}\) and an objective function \(f:\mathcal{X}\to\mathbb{R}\), a learner wishes to find the maximizing query \(\mathbf{x}^{*}\coloneqq\operatorname*{argmax}_{\mathbf{x}\in\mathcal{X}}f( \mathbf{x})\). However, \(f\) is black-box (i.e., not available in closed form) and can only be learned by submitting a query \(\mathbf{x}_{t}\in\mathcal{X}\) in each iteration \(t\) for function evaluation and receiving a noisy observation \(y_{t}\coloneqq f(\mathbf{x}_{t})+\xi_{t}\) where each \(\xi_{t}\) is i.i.d. \(\sigma\)-sub-Gaussian noise with zero mean. Each function evaluation is assumed to be expensive in some way, such as in terms of money or time spent. So, the learner must be sample-efficient and find \(\mathbf{x}^{*}\) in as few iterations as possible. BO achieves sample efficiency by leveraging a Bayesian model to represent a probabilistic belief of the function values at unobserved regions of \(\mathcal{X}\) in a principled manner. While any Bayesian model may be used for BO, _Gaussian processes_ (GPs) [42] are a common choice as they enable exact posterior inference: The GP posterior belief of \(f\) at any query \(\mathbf{x}\in\mathcal{X}\) after \(t\) iterations is a Gaussian with posterior mean and variance given by

\[\mu_{t}(\mathbf{x})\coloneqq\mathbf{k}_{t}(\mathbf{x})^{\top}(\mathbf{K}_{t} +\lambda\mathbf{I})^{-1}\mathbf{y}_{t}\;,\quad\sigma_{t}^{2}(\mathbf{x}) \coloneqq k(\mathbf{x},\mathbf{x})-\mathbf{k}_{t}(\mathbf{x})^{\top}( \mathbf{K}_{t}+\lambda\mathbf{I})^{-1}\mathbf{k}_{t}(\mathbf{x})\] (1)

where \(\mathbf{y}_{t}\coloneqq(y_{j})_{j=1}^{t}\in\mathbb{R}^{t}\), \(k\) is a positive semidefinite _kernel_ (covariance function), \(\mathbf{k}_{t}(\mathbf{x})\coloneqq(k(\mathbf{x},\mathbf{x}_{j}))_{j=1}^{t}\in \mathbb{R}^{t}\), \(\mathbf{K}_{t}\coloneqq(k(\mathbf{x}_{j},\mathbf{x}_{j^{\prime}}))_{j,j^{ \prime}=1}^{t}\in\mathbb{R}^{t\times t}\), and \(\lambda\) is an algorithm parameter; if the noise is a Gaussian with variance \(\sigma^{2}\), then the true posterior is recovered with \(\lambda=\sigma^{2}\). The kernel \(k\) is an important modeling choice as the GP posterior mean will reside in the _reproducing kernel Hilbert space_ (RKHS) associated with \(k\). For simplicity, we assume w.l.o.g. that \(k(\mathbf{x},\mathbf{x}^{\prime})\leq 1\) for any pair of queries \(\mathbf{x},\mathbf{x}^{\prime}\in\mathcal{X}\). Kernel \(k\) affects the _maximum information gain_ (MIG) defined as

\[\gamma_{T}(\mathcal{X})\coloneqq\max_{\{\mathbf{x}_{t}\}_{t=1}^{T}\subseteq \mathcal{X}}0.5\log\left|\mathbf{I}+\lambda^{-1}\mathbf{K}_{T}\right|.\]

The MIG characterizes the statistical complexity of a problem and plays an integral role in the theoretical analysis. For the commonly used squared exponential kernel, \(\gamma_{T}(\mathcal{X})=\mathcal{O}((\log T)^{d+1})\), while for the Matern kernel with \(\nu>1\), \(\gamma_{T}(\mathcal{X})=\mathcal{O}(T^{d(d+1)/(2v+d(d+1))}(\log T))\)[32]. Importantly, \(\gamma_{T}(\mathcal{X})\) is increasing in the volume of \(\mathcal{X}\)[32, Theorem 8].

## 4 BO with Cost-varying Variable Subsets (BOCVS)

The BOCVS problem consists of a compact query set \(\mathcal{X}\subset\mathbb{R}^{d}\) and an objective function \(f:\mathcal{X}\to\mathbb{R}\) in the RKHS of \(k\) with the RKHS norm upper bounded by \(B\). For simplicity, assume w.l.o.g. that \(\mathcal{X}=[0,1]^{d}\). Let \([d]\coloneqq\{1,2,...,d\}\). The learner is given a collection \(\mathcal{I}\subseteq 2^{[d]}\) of _control sets_ indexed by \(1,2,\ldots,m:=|\mathcal{I}|\). Each control set \(i\in[m]\), denoted by \(\mathcal{I}_{i}\subseteq[d]\), indicates the variables in a query with values that can be chosen by the learner. The complement \(\overline{\mathcal{I}}_{i}\coloneqq[d]\setminus\mathcal{I}_{i}\) of \(\mathcal{I}_{i}\) is the corresponding _random set_ indicating the variables in a query with values that will be randomly sampled from some distribution. A query \(\mathbf{x}\in\mathcal{X}\) can be represented by a combination of _partial queries_\([\mathbf{x}^{i},\mathbf{x}^{-i}]\) comprising the _control partial query_\(\mathbf{x}^{i}\coloneqq(x_{\ell})_{\ell\in\mathcal{I}_{i}}\) (i.e., \(\mathbf{x}^{i}\) collects the variablesindexed by \(\mathcal{I}_{i}\)) and the _random partial query_\(\mathbf{x}^{-i}\coloneqq(x_{\ell})_{\ell\in\overline{\mathcal{I}}_{i}}\) where \(x_{\ell}\) denotes the \(\ell\)-th variable in the query vector \(\mathbf{x}\). Note that \([\mathbf{x}^{i},\mathbf{x}^{-i}]\) is not a simple vector concatenation as the variables may need to be reordered according to their indices. Furthermore, let \(\mathcal{X}^{i}\coloneqq\{\mathbf{x}^{i}\mid\mathbf{x}\in\mathcal{X}\}\).

In iteration \(t\), the learner chooses control set \(i_{t}\in\mathcal{I}\) and specifies the values in control partial query \(\mathbf{x}^{i_{t}}\). The random partial query \(\mathbf{x}^{-i_{t}}\) will then be randomly sampled from the environment. For example, if \(d=4\) and \(\mathcal{I}_{i_{t}}=\{1,3\}\), then \(\overline{\mathcal{I}}_{i_{t}}=\{2,4\}\) and the learner will be able to choose the values in \(\mathbf{x}^{i_{t}}\) (i.e., the \(1^{\text{st}}\) and \(3^{\text{rd}}\) variables) but not those in \(\mathbf{x}^{-i_{t}}\) (i.e., the \(2^{\text{nd}}\) and \(4^{\text{th}}\) variables). The full query in iteration \(t\) is then \(\mathbf{x}_{t}=[\mathbf{x}^{i_{t}},\mathbf{x}^{-i_{t}}]=(x_{t,\ell})_{\ell\in[ \mathcal{I}]}\). Each observed variable \(x_{t,\ell}\) for \(\ell\in\overline{\mathcal{I}}_{i_{t}}\) is a realization of a random variable \(X_{t,\ell}\sim\mathcal{P}_{\ell}\). The observed \(\mathbf{x}^{-i_{t}}\) is then a realization of the random vector \(\mathbf{X}^{-i_{t}}\coloneqq\left(X_{t,\ell}\right)_{\ell\in\overline{ \mathcal{I}}_{i_{t}}}\sim\mathbb{P}^{-i_{t}}\) where \(\mathbb{P}^{-i_{t}}\) is the product measure \(\bigtimes_{\ell\in\overline{\mathcal{I}}_{i_{t}}}\mathcal{P}_{\ell}\). In other words, each variable in a random partial query is independently sampled from a probability distribution that governs that variable. All distributions are assumed to be known. The learner then observes \(y_{t}\coloneqq f(\mathbf{x}_{t})+\xi_{t}\) where each \(\xi_{t}\) is i.i.d. \(\sigma\)-sub-Gaussian noise with a zero mean. Fig. 1 illustrates two iterations in a BOCVS problem setting.

The learner wishes to find the optimal control set \(i^{*}\) and specified values in control partial query \(\mathbf{x}^{i^{*}}\) that maximize the expected value of \(f([\mathbf{x}^{i},\mathbf{X}^{-i}])\) where the expectation is w.r.t. \(\mathbf{X}^{-i}\sim\mathbb{P}^{-i}\):

\[(i^{*},\mathbf{x}^{i^{*}})\coloneqq\operatorname*{argmax}_{(i,\mathbf{x}^{i}) \in[m]\times\mathcal{X}^{i}}\mathbb{E}\big{[}f([\mathbf{x}^{i},\mathbf{X}^{-i} ])\big{]}\,.\]

The learner has an initial budget \(C\in\mathbb{R}^{+}\) and every control set \(\mathcal{I}_{i}\) has an associated cost \(c_{i}>0\) for all \(i\in[m]\). Let the control set indices be defined such that \(c_{1}\leq c_{2}\leq\ldots\leq c_{m}\).1

Footnote 1: While our problem definition does not require that \(c_{i}\leq c_{j}\Leftrightarrow\max_{\mathbf{x}^{i}\in\mathcal{X}^{i}}\mathbb{E} \big{[}f([\mathbf{x}^{i},\mathbf{X}^{-i}])\big{]}\leq\max_{\mathbf{x}^{j}\in \mathcal{X}^{j}}\mathbb{E}\big{[}f([\mathbf{x}^{i},\mathbf{X}^{-j}])\big{]}\), one might reasonably expect this to be the case in real-world problems, i.e., ”better” control sets cost more to specify. This also implies that \(\mathcal{I}_{i}\subseteq\mathcal{I}_{j}\Rightarrow c_{i}\leq c_{j}\).

In every iteration \(t\), the learner pays \(c_{i_{t}}\). The learning procedure ends after \(T\) iterations when \(C-\sum_{t=1}^{T}c_{i_{t}}<c_{i_{T+1}}\), i.e., the learner has not enough budget left to pay for the chosen control set. \(T\) will now be a random variable depending on the algorithm and the random outcomes of the learning procedure. The cost-varying cumulative regret is defined as

\[R_{T}\coloneqq\sum_{t=1}^{T}c_{i_{t}}\left(\mathbb{E}\big{[}f([\mathbf{x}^{i_{ t}},\mathbf{X}^{-i^{*}}])\big{]}-\mathbb{E}\big{[}f([\mathbf{x}^{i_{t}}, \mathbf{X}^{-i_{t}}])\big{]}\right).\]

The regret incurred by choosing a sub-optimal control set and specifying sub-optimal values in the control partial query is weighted by the cost of that control set. This naturally incorporates the notion that the penalty for sub-optimal plays is lower if the play was cheap, while also penalizing using the entire budget on sub-optimal plays, regardless of whether those plays are cheap or expensive. Intuitively, to minimize the cost-varying regret, a learner would attempt to use the cheap control sets (i.e., low \(c_{i}\), low \(\mathbb{E}\big{[}f([\mathbf{x}^{i},\mathbf{X}^{-i}])\big{]}\)) to explore the query space, and use the expensive control sets (i.e., high \(c_{i}\), high \(\mathbb{E}\big{[}f([\mathbf{x}^{i},\mathbf{X}^{-i}])\big{]}\)) to exploit control partial queries with high expected function values.1 When all \(c_{i}=1\), we recover the BOPSQ problem [13], and \(C\) is simply the number of iterations

Figure 1: Two iterations in a BOCVS problem setting. The grey boxes are isometric views of a query set \(\mathcal{X}\subset\mathbb{R}^{3}\). The blue regions depict the probability densities of random vectors \([\mathbf{x}^{i_{t}},\mathbf{X}^{-i_{t}}]\) and \([\mathbf{x}^{i_{t+1}},\mathbf{X}^{-i_{t+1}}]\). In iteration \(t\), the learner chooses the control set \(i_{t}=1\) and specifies the value (of the first variable \(x_{t,1}\)) in control partial query \(\mathbf{x}^{i_{t}}\), while the last two variables \(X_{t,2},X_{t,3}\) in random partial query \(\mathbf{X}^{-i_{t}}\) will be randomly sampled. In iteration \(t+1\), the learner chooses the control set \(i_{t+1}=2\) and specifies the values (of the first two variables \(x_{t,1},x_{t,2}\)) in control partial query \(\mathbf{x}^{i_{t+1}}\), while the last variable \(X_{t,3}\) in random partial query \(\mathbf{X}^{-i_{t+1}}\) will be randomly sampled.

in the learning trajectory. In fact, BOPSQ reduces to a simpler problem if there exists a _full query control set_ that allows the learner to choose the values of all \(d\) variables. If \([d]\in\mathcal{I}\), then \(\mathcal{I}_{i^{*}}=[d]\) and \(\mathbb{E}\big{[}f(\left\{\left.\mathbf{x}^{i^{*}},\mathbf{X}^{-i^{*}}\right. \right\rceil\big{]}\big{]}=\max_{\mathbf{x}\in\mathcal{X}}f(\mathbf{x})\) since expectations of a function are never greater than the maximum value of the function. In other words, the full query control set is guaranteed to be the optimal control set and the BOPSQ problem reduces to one of conventional BO. In general, under BOPSQ, any control set that is a strict subset of another will never be chosen.

### Ucb-Cvs

Alg. 1 describes our UCB-CVS algorithm for solving the BOCVS problem. In iteration \(t\), it uses the GP posterior belief of \(f\) to construct an _upper confidence bound_ (UCB) \(u_{t-1}\) of \(f\):

\[u_{t-1}(\mathbf{x})=\mu_{t-1}(\mathbf{x})+\beta_{t}\sigma_{t-1}(\mathbf{x})\]

where the sequence \((\beta_{t})_{t\geq 1}\) is an algorithm parameter that controls the tradeoff between exploration and exploitation. UCB-based algorithm design is a classic strategy in the stochastic bandits [19, Ch. 7] and BO literature [7, 32] and makes use of the "_optimism in the face of uncertainty_" (OFU) principle [18]: Queries with a large posterior standard deviation (i.e., high uncertainty) are given high acquisition scores as the function values at those queries may be potentially high. UCB-CVS adapts this strategy by taking the expectation of the UCB as part of the acquisition process. Due to the monotonicity of expectation, if \(u_{t-1}\) is an upper bound of \(f\) (i.e., \(u_{t-1}(\mathbf{x})\geq f(\mathbf{x})\) for any \(\mathbf{x}\in\mathcal{X}\)), then \(\mathbb{E}\big{[}u_{t-1}([\mathbf{x}^{i},\mathbf{X}^{-i}])\big{]}\) is also an upper bound of \(\mathbb{E}\big{[}f([\mathbf{x}^{i},\mathbf{X}^{-i}])\big{]}\) for any \(i\in[m],\mathbf{x}^{i}\in\mathcal{X}^{i}\).

UCB-CVS also takes as input an \(\epsilon\)-schedule \((\epsilon_{t})_{t=1}^{\infty}\) where \(\epsilon_{t}\geq 0\) for all \(t\). To choose the control set in iteration \(t\), it first computes \(g_{t}\) which is the expected UCB of the best control set and specified values in the control partial query (Step 3). It then collects every control set \(i\) that fulfills the condition \(\max_{\mathbf{x}^{i}\in\mathcal{X}^{i}}\mathbb{E}\big{[}u_{t-1}([\mathbf{x}^ {i},\mathbf{X}^{-i}])\big{]}+\epsilon_{t}\geq g_{t}\) into a set \(\mathcal{S}_{1}\) (Step 4). It further reduces this set \(\mathcal{S}_{1}\) to \(\mathcal{S}_{2}\) by retaining only the control sets with the lowest cost (Step 5). Finally, it chooses the control set from \(\mathcal{S}_{2}\) with the largest expected UCB value (Step 6). Each \(\epsilon_{t}\) thus serves as a relaxation that enables exploration with cheaper control sets. Choosing many \(\epsilon_{t}\) to be large results in many iterations of choosing cheaper control sets; conversely, choosing \(\epsilon_{t}=0\) for all \(t\) ignores all costs.

Our first result upper bounds the cost-varying cumulative regret incurred by UCB-CVS. Define the _feasible set_\(\widetilde{\mathcal{X}}_{i}\coloneqq\bigtimes_{\ell=1}^{d}[a_{t}^{i},b_{t}^{i}]\) for each control set \(i\) such that \(a_{\ell}^{i}=0\), \(b_{\ell}^{i}=1\) if \(\ell\in\mathcal{I}_{i}\), and \(a_{\ell}^{i}=\sup\{a\in[0,1]\mid F_{\ell}(a)=0\}\), \(b_{\ell}^{i}=\inf\{b\in[0,1]\mid F_{\ell}(b)=1\}\) otherwise, where \(F_{\ell}\) is the CDF of \(X_{\ell}\sim\mathcal{P}_{\ell}\). \(\widetilde{\mathcal{X}}_{i}\) is a subset of \(\mathcal{X}\) in which any query chosen with control set \(i\) must reside. Define \(T_{i}\) as the total number of iterations in which control set \(i\) is chosen.

**Theorem 4.1**.: _With probability at least \(1-\delta\), UCB-CVS (Alg. 1) incurs a cost-varying cumulative regret bounded by_

\[R_{T}\leq\mathcal{O}\Bigg{(}\left(B+\sqrt{\gamma_{T}(\mathcal{X})+\log\frac{m +1}{\delta}}\right)\left(\sum_{i=1}^{m}\!c_{i}\left(\sqrt{T_{i}\gamma_{T_{i}}( \widetilde{\mathcal{X}}_{i})}+\log\frac{m+1}{\delta}\right)\right)\Bigg{)}+c_ {m}{\sum_{t=1}^{T}}\epsilon_{t}\]

_by setting \(\beta_{t}=B+\sigma\sqrt{2\left(\gamma_{t-1}(\mathcal{X})+1+\log((m+1)/\delta )\right)}\)._For any appropriately chosen kernel such that \(\gamma_{T}(\mathcal{X})<\mathcal{O}(\sqrt{T})\) (e.g., commonly used squared exponential kernel, see Sec. 3) and \(\epsilon\)-schedule such that \(\sum_{t=1}^{T}\epsilon_{t}\) is sublinear in \(T\), the cumulative regret incurred will be sublinear in \(T\): \(\lim_{T\to\infty}R_{T}/T=0\). Since the mean of a sequence is no less than the minimum, and all \(c_{i}>0\), this further implies the desired no-regret property: \(\lim_{T\to\infty}\min_{1\leq t\leq T}(\mathbb{E}\big{[}f([\mathbf{x}^{i^{*}}, \mathbf{X}^{-i^{*}}])\big{]}-\mathbb{E}\big{[}f([\mathbf{x}^{i_{t}},\mathbf{X} ^{-i_{t}}])\big{]}\big{)}=0\), i.e., the best control set and specified values in control partial query in the algorithm's choices eventually converge to the optimal solution. The proof of Theorem 4.1 relies on choosing an appropriate sequence of \(\beta_{t}\) such that \(u_{t-1}(\mathbf{x})\geq f(\mathbf{x})\) for any \(\mathbf{x}\in\mathcal{X},t\geq 1\) with high probability [7, Theorem 2]. The cumulative regret is bounded by a sum of expectations of posterior standard deviations, which can then be bounded by a sum of posterior standard deviations plus some additional terms [16, Lemma 3] and in turn bounded in terms of the MIG [7, Lemma 4]. The proofs of all results in this paper are provided in Appendix A.

Since each \(\gamma_{T_{i}}(\widetilde{\mathcal{X}}_{i})\) is increasing in the volume of \(\widetilde{\mathcal{X}}_{i}\), Theorem 4.1 states that control sets with smaller feasible sets will incur less regret. If the size of a feasible set is taken to be a reasonable surrogate for the diffuseness of the probability distributions involved, Theorem 4.1 then suggests that control sets with corresponding random sets whose probability distributions are less diffuse will incur less regret.2 Theorem 4.1 also informs us that one sufficient condition on the \(\epsilon\)-schedule for the cost-varying regret to be sublinear in \(T\) is that \(\sum_{t=1}^{T}\epsilon_{t}\) is sublinear in \(T\). Our next proposition provides an alternative condition (neither is more general than the other):

Footnote 2: The feasible set of control set \(i\) is defined in a worst-case manner, which may be too conservative to be a good surrogate for diffuseness, especially for concentrated probability distributions with non-zero density everywhere. Nevertheless, it facilitates the worst-case analysis of the regret bounds.

**Proposition 4.2**.: _If there exists a \(\tilde{\epsilon}>0\) s.t. for all \(i\neq i^{*}\), \(\epsilon_{t}\leq\mathbb{E}\big{[}f([\mathbf{x}^{i^{*}},\mathbf{X}^{-i^{*}}]) \big{]}-\max_{\mathbf{x}^{i}\in\mathcal{X}^{i}}\mathbb{E}\big{[}f([\mathbf{x }^{i},\mathbf{X}^{-i}])\big{]}-\tilde{\epsilon}\) eventually (i.e., the inequality holds for all \(t\geq q\) for some \(q\geq 1\)), and \(\gamma_{T}(\mathcal{X})<\mathcal{O}(\sqrt{T})\), then, with probability at least \(1-\delta\), \(\lim_{T\to\infty}T_{i}/T=0\) for all \(i\neq i^{*}\) and UCB-CVS incurs a cost-varying cumulative regret that is sublinear in \(T\) by setting \(\beta_{t}=B+\sigma\sqrt{2\left(\gamma_{t-1}(\mathcal{X})+1+\log((m+1)/\delta) \right)}\)._

The above results have shown that with an appropriately chosen \(\epsilon\)-schedule, UCB-CVS satisfies the no-regret property. However, ignoring all costs by setting \(\epsilon_{t}=0\) for all \(t\) also achieves no-regret. This begs the question: _In what way does a good \(\epsilon\)-schedule improve UCB-CVS?_ Supposing the most expensive control set is the full query control set, the presence of queries chosen with cheaper control sets should reduce the cost-varying regret incurred by the full query control set by ruling out low function value regions and directing the full queries towards high function value regions. Additionally, it is reasonable to conjecture that the more diffuse each variable's (indexed by \(\ell\)) probability distribution \(\mathcal{P}_{\ell}\) is, the more the cheaper control sets would explore the query space and thus, the lower the cost-varying regret incurred by the full query control set. To derive such a result, the plan of attack is to relate the variances (i.e., notion of diffuseness) of the probability distributions to the distances between queries chosen with the cheaper control sets, followed by analyzing the effect of these distances and the number of times cheaper control sets were played on the MIG term of the most expensive control set. Our next result relates the distance between pairs of queries chosen with control set \(i\) to the variance \(\mathbb{V}[X_{\ell}]\) of every probability distribution \(\mathcal{P}_{\ell}\) for \(\ell\in\overline{\mathcal{I}}_{i}\):

**Lemma 4.3**.: _Suppose that for each control set \(i\), the random variable \(Y_{i}:=\big{\|}[\mathbf{0},\mathbf{X}_{1}^{-i}]-[\mathbf{0},\mathbf{X}_{2}^{- i}]\big{\|}^{2}\) has a median \(M_{i}\) s.t. \(\mathbb{E}[Y_{i}|Y_{i}>M_{i}]\leq h_{i}M_{i}\) for some \(h_{i}>0\) where \(\mathbf{X}_{1}^{-i},\mathbf{X}_{2}^{-i}\sim\mathbb{P}^{-i}\). With probability at least \(1-\delta\), there will be at least \(N_{i}\) non-overlapping pairs of queries \(\mathbf{x}\) and \(\mathbf{x}^{\prime}\) chosen by UCB-CVS (Alg. 1) with control set \(i\) s.t. \(\|\mathbf{x}-\mathbf{x}^{\prime}\|^{2}\geq M_{i}\) where_

\[N_{i}=\Big{\lfloor}(T_{i}-1)/4-\sqrt{(T_{i}/4)\log(1/\delta)}\Big{\rfloor}\quad \text{and}\quad M_{i}\geq(4/(h_{i}+1))\sum_{\ell\in\overline{\mathcal{I}}_{i} }\mathbb{V}[X_{\ell}]\.\] (2)

From (2), the higher the variances of the distributions that govern the variables in the random set, the larger the lower bound \(M_{i}\) on the squared distance between at least \(N_{i}\) pairs of queries chosen with control set \(i\). As expected, the number \(N_{i}\) of pairs increases with \(T_{i}\) (i.e., the total number of iterations in which control set \(i\) is chosen). The assumption on \(Y_{i}\) is mild: As long as \(Y_{i}\) has at least \(1\) non-zero median, it will hold. The assumption excludes the case in which \(\mathcal{P}_{\ell}\) for all \(\ell\in\overline{\mathcal{I}}_{i}\) are degenerate with all probability mass on a single point. With Lemma 4.3, we now derive an alternative regret bound that depends on the variances of the distributions and the number of plays of cheaper control sets:

**Theorem 4.4**.: _Suppose that the following hold:_

* _Assumption of Lemma_ 4.3 _holds;_
* \(k(\mathbf{x},\mathbf{x}^{\prime})\) _is an isotropic kernel which only depends on distance between_ \(\mathbf{x}\) _&_ \(\mathbf{x}^{\prime}\) _and can be written as_ \(k(\|\mathbf{x}-\mathbf{x}^{\prime}\|)\)_;_
* _There exists an iteration_ \(r\) _s.t. for all_ \(t\leq r,i_{t}\leq m-1\)_, and for all_ \(t>r,i_{t}=m\) _._

_Then, with probability at least \(1-\delta\), UCB-CVS (Alg. 1) incurs a cost-varying cumulative regret bounded by_

\[R_{T}\leq\mathcal{O} \Bigg{(}\left(B+\sqrt{\gamma_{T}(\mathcal{X})+\log\frac{2m}{ \delta}}\right)\left(c_{m}\left(\sqrt{T\gamma_{T}(\mathcal{X})}-\mathcal{L}+ \log\frac{2m}{\delta}\right)\right.\] \[\left.+\sum_{i=1}^{m-1}c_{i}\left(\sqrt{T_{i}\gamma_{T_{i}}( \widetilde{\mathcal{X}}_{i})}+\log\frac{2m}{\delta}\right)\,\right)\Bigg{)}+c _{m}{\sum_{t=1}^{T}}\epsilon_{t}\] \[\mathcal{L}\coloneqq\lambda\left(\sum_{i=1}^{m-1}N_{i}\log\Big{(} V_{i}-2k\big{(}\sqrt{M_{i}}\big{)}-k\big{(}\sqrt{M_{i}}\big{)}^{2}\right)+W\right)\]

_by setting \(\beta_{t}=B+\sigma\sqrt{2\left(\gamma_{t-1}(\mathcal{X})+1+\log((2m)/\delta) \right)}\) where \(N_{i}\) and \(M_{i}\) are previously defined in Lemma 4.3, and \(V_{i}\) and \(W\) are residual terms defined in Appendix A.5._

Theorem 4.4 shows that the MIG term pertaining to the most expensive control set \(m\) is reduced by \(\mathcal{L}\) which increases as \(N_{i}\) increases, which in turn increases as \(T_{i}\) increases. This suggests that an \(\epsilon\)-schedule that increases the number of times cheaper control sets are played can reduce the MIG term. \(\mathcal{L}\) also increases as \(k(\sqrt{M_{i}})\) decreases. For common kernels such as the squared exponential or Matern kernel with \(\nu>1\) (which satisfy the second assumption on isotropic kernel), \(k(\sqrt{M_{i}})\) decreases as \(M_{i}\) increases, from which we may conclude that higher variance probability distributions governing each \(X_{\ell}\) lead to a larger \(\mathcal{L}\) due to (2) and hence a larger decrease on the MIG term. In cases where \(c_{m}\gg c_{i}\) for all \(i\neq m\), a carefully chosen \(\epsilon\)-schedule can thus lead to a large decrease in the regret bound via \(\mathcal{L}\). The third assumption is (informally) approximately true in practice due to the design of UCB-CVS: If a decreasing \(\epsilon\)-schedule is used, the algorithm will choose the cheaper but sub-optimal control sets at the start. After \(\epsilon_{t}\) has decreased past a certain value, the algorithm will only choose the optimal (and likely most expensive) control set. The proof sketch upper bounds the sum of posterior standard deviations of queries chosen with control set \(m\) with the MIG term minus the sum of posterior standard deviations of queries chosen with all other control sets. This latter sum is then lower bounded by a log determinant of the prior covariance matrix which is then decomposed into a sum of log determinants of pairs of queries. The dependence on the distances between the pairs can be made explicit in this form. Neither Theorems 4.1 nor 4.4 is more general than the other.

### Practical Considerations

UCB-CVS is presented with the \(\epsilon\)-schedule formulation for generality and ease of theoretical analysis. In practice, however, the \(\epsilon\)-schedule is a hyperparameter that is difficult to interpret and choose. We propose a simple _explore-then-commit_ (ETC) variant with which the learner only chooses the number of plays of each _cost group_ (i.e., defined as a collection of control sets with the same cost that is not the maximum cost). In each iteration, the algorithm will choose the cost group with the lowest cost and non-zero remaining plays, and then choose the control set within that cost group with the largest expected UCB (similar to Step 6 in Alg. 1). Once all cost groups have zero remaining plays, the algorithm chooses the control set with the largest expected UCB among all control sets. This algorithm is highly interpretable and is equivalent to UCB-CVS with a specific sublinear \(\epsilon\)-schedule (that cannot be known _a priori_). Furthermore, the learner should choose the number of plays adaptively depending on the cost of each cost group. On computational considerations, UCB-CVS may be computationally expensive if the number \(m\) of control sets is large (e.g., if every subset of variables is available as a control set and \(m=2^{d}\)) as each control set requires a maximization of the expected UCB (which can be approximated with Monte Carlo sampling). In such cases, the learner has the option to simply ignore any number of control sets to reduce \(m\), as long as \(i^{*}\) is not ignored.

Figure 2: Mean and standard error (over \(10\) RNG seeds) of the simple regret (lower is better) incurred against cost spent (budget) \(C\) by **TS-PSQ**, **UCB-PSQ**, **ETC-50**, **ETC-100**, and **ETC-Ada** with varying objective functions, cost sets, and variances of distributions. A diamond indicates the average budget after which an algorithm only chooses the optimal control set.

Experiments and Discussion

This section empirically evaluates the performance of the tested algorithms with \(4\) objective functions: (a) function samples from a GP prior (\(3\)-D), (b) the Hartmann synthetic function (\(3\)-D), (c) a plant growth simulator built from real-world data where the variables are nutrients such as NH\({}_{3}\) and pH (\(5\)-D), and (d) a simulator built from the airfoil self-noise dataset (\(5\)-D) from the UCI Machine Learning Repository [9]. For the first \(2\) objective functions, the control sets are all possible subsets of the \(3\) variables except the empty set, which leads to \(7\) control sets. For the plant growth objective function, we pick \(7\) control sets including the full query control set. For the airfoil self-noise objective function, similar to that of [13], we pick \(7\) control sets of \(2\) variables each that are not subsets of each other. We use \(3\) different sets of costs for the \(7\) control sets: **cheap** (\(\{0.01,0.01,0.01,0.1,0.1,0.1,1\}\)), **moderate** (\(\{0.1,0.1,0.1,0.2,0.2,0.2,1\}\)), and **expensive** (\(\{0.6,0.6,0.6,0.8,0.8,0.8,1\}\)). Using these sets of costs, the control sets are ordered such that \(c_{i}<c_{j}\Rightarrow\max_{\mathbf{x}^{i}\in\mathcal{X}^{i}}\mathbb{E}\big{[} f([\mathbf{x}^{i},\mathbf{X}^{-i}])\big{]}\leq\max_{\mathbf{x}^{j}\in\mathcal{X}^{j}} \mathbb{E}\big{[}f([\mathbf{x}^{j},\mathbf{X}^{-j}])\big{]}\). These cost sets have fixed the optimal (i.e., last) control set to have a cost of \(1\). While these cost sets may (at first glance) seem arbitrary, it is the algorithms' _relative performance across these cost sets_ rather than the absolute performance on a single cost set that allows us to understand the conditions under which particular algorithms perform better or worse. Real-world applications (unlike the experiments conducted here) will come with their own cost sets defined by real-world constraints. If the real costs can also be categorized in a similar relative way like the above cheap, moderate, and expensive cost sets, then the results are expected to be similar. Every probability distribution \(\mathcal{P}_{\ell}\) is a truncated normal distribution with mean \(0.5\) and the same variance which is one of \(0.02\), \(0.04\), and \(0.08\) (the uniform distribution on \([0,1]\) has variance \(1/12\)).

We compare the performance of our algorithm against that of the baseline Thompson sampling (**TS-PSQ**) algorithm developed in [13]. We test **UCB-PSQ** (\(\epsilon\)-schedule with \(\epsilon_{t}=0\) for all \(t\)) along with the ETC variant of UCB-CVS (Sec. 4.2) with \(3\) sets of hyperparameters: \(50\) plays per cost group (**ETC-50**), \(100\) plays per cost group (**ETC-100**), and a cost-adaptive version with \(4/c_{j}\) plays per cost group where \(c_{j}\) is the cost of the control sets in that cost group (**ETC-Ada**). We also investigated simple extensions of TS-PSO, UCB-PSQ, and expected improvement (adapted for BOPSQ) for the BOCVS problem by dividing the acquisition score of a control set by its cost in a manner similar to that in [31, Sec. \(3.2\)]. We observed that these naive methods generally do not work well; we defer the results and discussion of these methods to Appendix B. Refer to Appendix C for full descriptions of all experimental settings and algorithm hyperparameters. The code for the experiments may be found at https://github.com/sebtsh/bocvs.

Fig. 2 shows the mean and standard error (over \(10\) RNG seeds) of the simple regret \(\min_{1\leq t\leq T(C)}\mathbb{E}\big{[}f([\mathbf{x}^{i^{*}},\mathbf{X}^{-i^ {*}}])\big{]}-\mathbb{E}\big{[}f([\mathbf{x}^{i_{t}},\mathbf{X}^{-i_{t}}]) \big{]}\) (lower is better) incurred against cost spent (budget) \(C\) by each algorithm with varying objective functions, cost sets, and variances of distributions where \(\mathcal{T}(C)\) denotes the maximum iteration reached after spending \(C\). The simple regret encodes the value of the best solution an algorithm has chosen within a certain budget and is a measure of cost efficiency. We report the salient observations below:

**(1) UCB-CVS variants outperform TS-PSQ and UCB-PSQ under cheap/moderate costs when the full query control set is available.** With the GP sample, Hartmann, and plant growth objective functions, the full query control set is available. TS-PSQ and UCB-PSQ only choose the full query control set in every iteration and are very cost inefficient under cheap and moderate costs, while UCB-CVS variants are able to use the cheaper control sets for exploration, followed by using the full query control set for exploitation, and find much better solutions with the same budget. As expected, their performance advantage reduces as the costs increase and \(c_{m}\) gets closer to \(c_{i}\) for all \(i\neq m\).

**(2) Cost-adaptive UCB-CVS (ETC-Ada) can maintain competitive performance under expensive costs.** The non-cost-adaptive variants, ETC-50 and ETC-100, perform worse than TS-PSQ and UCB-PSQ under expensive costs. In contrast, it can be observed that ETC-Ada generally performs well under all costs by tuning the number of plays of suboptimal cost groups according to their costs. We recommend practitioners to use adaptive algorithms to achieve good performance under any cost set. In particular, the results suggest that an \(\mathcal{O}(c_{i}^{-1})\) threshold is likely to work well across different sets of costs and is a robust choice for practitioners that keeps the number of hyperparameters to a minimum.

**(3) TS-PSQ and UCB-PSQ perform relatively well when the control sets are not subsets of each other.** With the airfoil self-noise objective function, TS-PSQ and UCB-PSQ perform better as the control sets with this objective function are not subsets of each other and thus, they can also use the cheaper control sets during learning, while the UCB-CVS variants suffer worse performance here due to artificially selecting suboptimal control sets and queries with the \(\epsilon\)-relaxations. This worse performance is encoded in Theorems 4.1 and 4.4 as the sum of \(\epsilon_{t}\) terms.

**(4) Increasing the variance of the probability distributions has competing effects on the simple regret.** Of the \(42\) experimental settings (combinations of objective function, cost set, and algorithm) in which the variance makes a difference (excluding TS-PSQ and UCB-PSQ for all objective functions except airfoil), the settings with variance \(0.02\), \(0.04\), and \(0.08\) achieved the lowest mean simple regret by the end \(11\), \(6\), and \(25\) times, respectively. This generally supports Theorem 4.4's prediction that higher variances decrease the upper bound on regret. However, due to the looseness of the bound, this effect is not guaranteed and there are still cases where lower variances lead to a lower regret, as suggested by the argument about feasible sets when discussing Theorem 4.1; note that the same MIGs of the feasible sets for control sets \(1\) to \(m-1\) appear in Theorem 4.4. We observe competing effects and conclude that the effect of increasing variance is problem- and algorithm-dependent. While higher variances may lead to more exploration, they may also result in too much smoothing of function values which may hinder the learner's ability to focus on high-value query regions.

## 6 Conclusion

This paper introduces the BOCVS problem and describes the UCB-CVS algorithm that is provably no-regret in solving this problem. We show that our algorithm performs well across several different experimental settings and achieves the desired goal of finding significantly better solutions within the same budget. This work opens up avenues of future research: In particular, an entropy search-based algorithm [14; 23; 41] that chooses control sets and queries based on expected information gain per unit cost is a non-trivial and promising direction for alternative methods of solving BOCVS.

## Acknowledgements and Disclosure of Funding

This research/project is supported by the Agency for Science, Technology and Research, Singapore (A*STAR), under its RIE\(2020\) Advanced Manufacturing and Engineering (AME) Programmatic Funds (Award A\(20\)H\(6\)b\(0151\)) and its RIE\(2020\) Advanced Manufacturing and Engineering (AME) Industry Alignment Fund - Pre Positioning (IAF-PP) (Award A\(19\)E\(4\)a\(0101\)). Sebastian Shenghong Tay is supported by A*STAR.

## References

* Aglietti et al. [2020] V. Aglietti, X. Lu, A. Paleyes, and J. Gonzalez. Causal Bayesian optimization. In _Proc. AISTATS_, pages 3155-3164. PMLR, 2020.
* Aglietti et al. [2021] V. Aglietti, N. Dhir, J. Gonzalez, and T. Damoulas. Dynamic causal Bayesian optimization. In _Proc. NeurIPS_, volume 34, pages 10549-10560, 2021.
* Balandat et al. [2020] M. Balandat, B. Karrer, D. R. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. BoTorch: A framework for efficient Monte-Carlo Bayesian optimization. In _Proc. NeurIPS_, pages 21524-21538, 2020.
* Branchini et al. [2023] N. Branchini, V. Aglietti, N. Dhir, and T. Damoulas. Causal entropy optimization. In _Proc. AISTATS_, pages 8586-8605. PMLR, 2023.
* Cakmak et al. [2020] S. Cakmak, R. Astudillo Marban, P. Frazier, and E. Zhou. Bayesian optimization of risk measures. In _Proc. NeurIPS_, pages 20130-20141, 2020.
* Chen et al. [2018] Y. Chen, A. Huang, Z. Wang, I. Antonoglou, J. Schrittwieser, D. Silver, and N. de Freitas. Bayesian optimization in AlphaGo. arXiv:1812.06855, 2018.
* Chowdhury and Gopalan [2017] S. R. Chowdhury and A. Gopalan. On kernelized multi-armed bandits. In _Proc. ICML_, pages 844-853, 2017.
* Crabtree and Haynsworth [1969] D. E. Crabtree and E. V. Haynsworth. An identity for the Schur complement of a matrix. _Proceedings of the American Mathematical Society_, 22(2):364-366, 1969.
* Dua and Graff [2017] D. Dua and C. Graff. UCI machine learning repository, 2017. URL http://archive.ics.uci.edu/ml.

* [10] J. Gardner, G. Pleiss, K. Q. Weinberger, D. Bindel, and A. G. Wilson. GPyTorch: Blackbox matrix-matrix Gaussian process inference with GPU acceleration. In _Proc. NeurIPS_, pages 7587-7597, 2018.
* [11] R. Garnett. _Bayesian Optimization_. Cambridge University Press, 2023. To appear.
* [12] C. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg, N. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. F. del Rio, M. Wiebe, P. Peterson, P. Gerard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant. Array programming with NumPy. _Nature_, 585(7825):357-362, 2020.
* [13] S. Hayashi, J. Honda, and H. Kashima. Bayesian optimization with partially specified queries. _Machine Learning_, 111(3):1019-1048, 2022.
* [14] J. M. Hernandez-Lobato, M. W. Hoffman, and Z. Ghahramani. Predictive entropy search for efficient global optimization of black-box functions. In _Proc. NeurIPS_, pages 918-926, 2014.
* [15] K. Kandasamy, G. Dasarathy, J. B. Oliva, J. Schneider, and B. Poczos. Gaussian process bandit optimisation with multi-fidelity evaluations. In _Proc. NeurIPS_, pages 1000-1008, 2016.
* [16] J. Kirschner and A. Krause. Information directed sampling and bandits with heteroscedastic noise. In _Proc. COLT_, pages 358-384, 2018.
* [17] J. Kirschner, I. Bogunovic, S. Jegelka, and A. Krause. Distributionally robust Bayesian optimization. In _Proc. AISTATS_, pages 2174-2184, 2020.
* [18] T. L. Lai, H. Robbins, et al. Asymptotically efficient adaptive allocation rules. _Advances in Applied Mathematics_, 6(1):4-22, 1985.
* [19] T. Lattimore and C. Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020.
* [20] J. Mockus, V. Tiesis, and A. Zilinskas. The application of Bayesian methods for seeking the extremum. In L. C. W. Dixon and G. P. Szego, editors, _Towards Global Optimization 2_, pages 117-129. North-Holland Publishing Company, 1978.
* [21] Q. P. Nguyen, Z. Dai, B. K. H. Low, and P. Jaillet. Optimizing conditional value-at-risk of black-box functions. In _Proc. NeurIPS_, pages 4170-4180, 2021.
* [22] Q. P. Nguyen, Z. Dai, B. K. H. Low, and P. Jaillet. Value-at-risk optimization with Gaussian processes. In _Proc. ICML_, pages 8063-8072, 2021.
* [23] Q. P. Nguyen, Z. Wu, B. K. H. Low, and P. Jaillet. Trusted-maximizers entropy search for efficient Bayesian optimization. In _Proc. UAI_, pages 1486-1495, 2021.
* [24] T. Nguyen, S. Gupta, H. Ha, S. Rana, and S. Venkatesh. Distributionally robust Bayesian quadrature optimization. In _Proc. AISTATS_, pages 1921-1931, 2020.
* [25] R. Oliveira, L. Ott, and F. Ramos. Bayesian optimisation under uncertain inputs. In _Proc. AISTATS_, pages 1177-1184, 2019.
* [26] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. In _Proc. NeurIPS_, pages 8026-8037, 2019.
* [27] M. Poloczek, J. Wang, and P. Frazier. Multi-information source optimization. In _Proc. NeurIPS_, pages 4288-4298, 2017.
* [28] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In _Proc. NeurIPS_, pages 1177-1184, 2007.
* [29] R. Shalloo, S. Dann, J.-N. Gruse, et al. Automation and control of laser wakefield accelerators using Bayesian optimization. _Nature Communications_, 11(1):1-8, 2020.
* [30] B. J. Shields, J. Stevens, J. Li, M. Parasram, F. Damani, J. I. M. Alvarado, J. M. Janey, R. P. Adams, and A. G. Doyle. Bayesian reaction optimization as a tool for chemical synthesis. _Nature_, 590(7844):89-96, 2021.
* [31] J. Snoek, H. Larochelle, and R. P. Adams. Practical Bayesian optimization of machine learning algorithms. In _Proc. NeurIPS_, pages 2951-2959, 2012.
* [32] N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the bandit setting: no regret and experimental design. In _Proc. ICML_, pages 1015-1022, 2010.

* [33] S. Sussex, A. Makarova, and A. Krause. Model-based causal Bayesian optimization. In _Proc. ICLR_, 2023.
* [34] P. M. Swamidass. _Encyclopedia of production and manufacturing management_. Springer Science & Business Media, 2000.
* [35] K. Swersky, J. Snoek, and R. P. Adams. Multi-task Bayesian optimization. In _Proc. NeurIPS_, pages 2004-2012, 2013.
* [36] S. Takeno, H. Fukuoka, Y. Tsukada, T. Koyama, M. Shiga, I. Takeuchi, and M. Karasuyama. Multi-fidelity Bayesian optimization with max-value entropy search and its parallelization. In _Proc. ICML_, pages 9334-9345, 2020.
* [37] S. S. Tay, C. S. Foo, U. Daisuke, R. Leong, and B. K. H. Low. Efficient distributionally robust Bayesian optimization with worst-case sensitivity. In _Proc. ICML_, pages 21180-21204, 2022.
* [38] S. Toscano-Palmerin and P. I. Frazier. Bayesian optimization with expensive integrands. _SIAM Journal on Optimization_, 32(2):417-444, 2022.
* [39] R. Turner, D. Eriksson, M. McCourt, J. Killi, E. Laaksonen, Z. Xu, and I. Guyon. Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020. In _Proc. NeurIPS 2020 Competition and Demonstration Track_, pages 3-26, 2021.
* [40] P. Virtanen, R. Gommers, T. E. Oliphant, M. Haberland, T. Reddy, D. Cournapeau, E. Burovski, P. Peterson, W. Weckesser, J. Bright, S. J. van der Walt, M. Brett, J. Wilson, K. J. Millman, N. Mayorov, A. R. J. Nelson, E. Jones, R. Kern, E. Larson, C. J. Carey, I. Polat, Y. Feng, E. W. Moore, J. VanderPlas, D. Laxalde, J. Perktold, R. Cimrman, I. Henriksen, E. A. Quintero, C. R. Harris, A. M. Archibald, A. H. Ribeiro, F. Pedregosa, P. van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. _Nature Methods_, 17:261-272, 2020.
* [41] Z. Wang and S. Jegelka. Max-value entropy search for efficient Bayesian optimization. In _Proc. ICML_, pages 3627-3635, 2017.
* [42] C. K. Williams and C. E. Rasmussen. _Gaussian Processes for Machine Learning_. MIT Press, Cambridge, MA, 2006.

Proofs

### Proof of Theorem 4.1

_Theorem 4.1_.: With probability at least \(1-\delta\), UCB-CVS (Alg. 1) incurs a cost-varying cumulative regret bounded by

\[R_{T}\leq\mathcal{O}\left(\left(B+\sqrt{\gamma_{T}(\mathcal{X})+\log\frac{m+1}{ \delta}}\right)\left(\sum_{i=1}^{m}c_{i}\left(\sqrt{T_{i}\gamma_{T_{i}}( \widetilde{\mathcal{X}_{i}})}+\log\frac{m+1}{\delta}\right)\right)\right)+c_{m }\sum_{t=1}^{T}\epsilon_{t}.\]

by setting \(\beta_{t}=B+\sigma\sqrt{2\left(\gamma_{t-1}(\mathcal{X})+1+\log((m+1)/\delta) \right)}\).

Proof.: \[R_{T} \coloneqq\sum_{t=1}^{T}c_{i_{t}}\left(\mathbb{E}\big{[}f([\mathbf{ x}^{i^{*}},\mathbf{X}^{-i^{*}}])\big{]}-\mathbb{E}\big{[}f([\mathbf{x}^{i_{t}}, \mathbf{X}^{-i_{t}}])\big{]}\right)\] \[\leq\sum_{t=1}^{T}c_{i_{t}}\left(\mathbb{E}\Big{[}u_{t-1}([ \mathbf{x}^{i_{t}},\mathbf{X}^{-i^{*}}])\Big{]}-\mathbb{E}\big{[}f([\mathbf{x }^{i_{t}},\mathbf{X}^{-i_{t}}])\big{]}\right)\] \[\leq\sum_{t=1}^{T}c_{i_{t}}\left(\mathbb{E}\big{[}u_{t-1}([ \mathbf{x}^{i_{t}},\mathbf{X}^{-i_{t}}])\big{]}-\mathbb{E}\big{[}f([\mathbf{x }^{i_{t}},\mathbf{X}^{-i_{t}}])\big{]}\right)+c_{m}\sum_{t=1}^{T}\epsilon_{t}\] (3) \[=\left(\sum_{i=1}^{m}c_{i}\sum_{t\in\widetilde{T}_{i}}\mathbb{E} \big{[}u_{t-1}([\mathbf{x}^{i_{t}},\mathbf{X}^{-i_{t}}])-f([\mathbf{x}^{i_{t} },\mathbf{X}^{-i_{t}}])\big{]}\right)+c_{m}\sum_{t=1}^{T}\epsilon_{t}\] \[\overset{(ii)}{\leq}\left(\sum_{i=1}^{m}c_{i}(2\beta_{T})\sum_{t \in\widetilde{T}_{i}}\mathbb{E}\big{[}\sigma_{t-1}([\mathbf{x}^{i_{t}}, \mathbf{X}^{-i_{t}}])\big{]}\right)+c_{m}\sum_{t=1}^{T}\epsilon_{t}\] \[\overset{(iii)}{\leq}\left(\sum_{i=1}^{m}c_{i}(2\beta_{T})\left(2 \sum_{t\in\widetilde{T}_{i}}\sigma_{t-1}(\mathbf{x}_{t})+4\log\frac{m+1}{ \delta}+8\log(4)+1\right)\right)+c_{m}\sum_{t=1}^{T}\epsilon_{t}\] (4) \[\overset{(iv)}{\leq}\left(\sum_{i=1}^{m}c_{i}(2\beta_{T})\left(2 \sqrt{4(T_{i}+2)\gamma_{T_{i}}(\widetilde{\mathcal{X}_{i}})}+4\log\frac{m+1}{ \delta}+8\log(4)+1\right)\right)+c_{m}\sum_{t=1}^{T}\epsilon_{t}\] \[=\mathcal{O}\left(\sum_{i=1}^{m}c_{i}\beta_{T}\left(\sqrt{T_{i} \gamma_{T_{i}}(\widetilde{\mathcal{X}_{i}})}+\log\frac{m+1}{\delta}\right) \right)+c_{m}\sum_{t=1}^{T}\epsilon_{t}\] (5) \[=\mathcal{O}\left(\left(B+\sqrt{\gamma_{T}(\mathcal{X})+\log\frac {m+1}{\delta}}\right)\left(\sum_{i=1}^{m}c_{i}\left(\sqrt{T_{i}\gamma_{T_{i}}( \widetilde{\mathcal{X}_{i}})}+\log\frac{m+1}{\delta}\right)\right)\right)+c_{m} \sum_{t=1}^{T}\epsilon_{t}\]

where \(\widetilde{T}_{i}\) is the ordered sequence of iterations at which control set \(i\) is chosen, \((i)\) follows from the algorithm's choice of \(\mathbf{x}^{i_{t}}\), \((ii)\) follows from Lemma A.4 with probability \(\delta/(m+1)\), \((iii)\) follows from Lemma A.1 with probability \(\delta/(m+1)\) applied once for each control set \(i\), and \((iv)\) follows from Lemma A.3 and the definition of \(\widetilde{\mathcal{X}_{i}}\) as the feasible set for control set \(i\). A union bound over the \(m+1\) events comprising the \(m\) applications of Lemma A.1 and single application of Lemma A.4 yields the desired \(1-\delta\) probability bound.

### Proof of Lemma a.1

**Lemma A.1**.: _Let \(k(\mathbf{x},\mathbf{x})=1\) and let \(\widetilde{T}_{i}\) be the ordered sequence of iterations at which control set \(i\) is chosen by UCB-CVS. For any \(i\in[m]\), with probability at least \(1-\delta\),_

\[\sum_{t\in\widetilde{T}_{i}}\mathbb{E}\big{[}\sigma_{t-1}([\mathbf{x}^{i_{t}}, \mathbf{X}^{-i_{t}}])\big{]}\leq 2\sum_{t\in\widetilde{T}_{i}}\sigma_{t-1}( \mathbf{x}_{t})+4\log\frac{1}{\delta}+8\log 4+1.\]

Proof.: For this proof, define a probability space \((\Omega,\mathcal{F},\mathcal{P})\) and a filtration \(\mathbb{F}=\{\mathcal{F}_{t}\}_{t=1}^{\infty}\), where \(\mathcal{F}_{t}\coloneqq\sigma(i_{1},\mathbf{x}^{i_{1}},\mathbf{x}^{-i_{1}}, y_{1},i_{2},\mathbf{x}^{i_{2}},\mathbf{x}^{-i_{2}},y_{2},...,i_{t}, \mathbf{x}^{i_{t}},\mathbf{x}^{-i_{t}},y_{t})\), the sigma-algebra generated by all the random variables in the BO procedure known by the end of iteration \(t\).

In advance of proving this result, it should be clarified that, in the main paper and all proofs excluding that of this lemma, \(\mathbb{E}\big{[}\sigma_{t-1}([\mathbf{x}^{i_{t}},\mathbf{X}^{-i_{t}}])\big{]}\) denotes the quantity obtained by treating \(\sigma_{t-1}\) and \(\mathbf{x}^{i_{t}}\) as deterministic and treating \(\mathbf{X}^{-i_{t}}\) as a random vector distributed according to the probability distribution \(\mathbb{P}^{-i_{t}}\). This is for ease of exposition. In the following proof, however, when using the formalism of random processes, what was previously referred to as \(\mathbb{E}\big{[}\sigma_{t-1}([\mathbf{x}^{i_{t}},\mathbf{X}^{-i_{t}}])\big{]}\) is actually \(\mathbb{E}[\sigma_{t-1}(\mathbf{x}_{t})\,|\,\mathcal{F}_{t-1}]\). The meaning is equivalent since \(\sigma_{t-1}\), \(i_{t}\), and \(\mathbf{x}^{i_{t}}\) are \(\mathbb{F}\)-predictable, and the only uncertainty about \(\sigma_{t-1}(\mathbf{x}_{t})\) arises from the lack of knowledge about \(\mathbf{x}^{-i_{t}}\).

Now we begin the proof proper. Define \(m\) stochastic processes \(\{X_{t}^{(1)}\}_{t=1}^{\infty}\), \(\{X_{t}^{(2)}\}_{t=1}^{\infty}\),..., \(\{X_{t}^{(m)}\}_{t=1}^{\infty}\), where, using \(\mathbbm{1}[A]\) to denote the indicator function that is equal to \(1\) when the event \(A\) is true and \(0\) otherwise,

\[X_{t}^{(i)}\coloneqq\sigma_{t-1}(\mathbf{x}_{t})\cdot\mathbbm{1}[i_{t}=i]\enspace.\]

Since each \(X_{t}^{(i)}\) is \(\mathcal{F}_{t}\)-measurable, each stochastic process is adapted to \(\mathbb{F}\). Next, define

\[m_{t}^{(i)} \coloneqq\mathbb{E}\Big{[}X_{t}^{(i)}\,|\,\mathcal{F}_{t-1}\Big{]}\] \[=\mathbb{E}[\sigma_{t-1}(\mathbf{x}_{t})\cdot\mathbbm{1}[i_{t}=i] \;|\,\mathcal{F}_{t-1}]\] \[=\mathbbm{1}[i_{t}=i]\,\mathbb{E}[\sigma_{t-1}(\mathbf{x}_{t})\,| \,\mathcal{F}_{t-1}]\]

where the last equality uses the pull-through property since \(i_{t}\) is \(\mathcal{F}_{t-1}\) measurable. Using Lemma A.5 with \(b_{t}=1\) since \(k(\mathbf{x},\mathbf{x})=1\), with probability at least \(1-\delta\), for any \(T\geq 1\),

\[\sum_{t=1}^{T}m_{t}^{(i)}\leq 2\sum_{t=1}^{T}X_{t}^{(i)}+4\log\frac{1}{ \delta}+8\log 4+1\] \[\sum_{t=1}^{T}\mathbbm{1}[i_{t}=i]\,\mathbb{E}[\sigma_{t-1}( \mathbf{x}_{t})\,|\,\mathcal{F}_{t-1}]\leq 2\sum_{t=1}^{T}\sigma_{t-1}(\mathbf{x}_{t}) \cdot\mathbbm{1}[i_{t}=i]+4\log\frac{1}{\delta}+8\log 4+1\] \[\sum_{t\in\widetilde{T}_{i}}\mathbb{E}[\sigma_{t-1}(\mathbf{x}_{t })\,|\,\mathcal{F}_{t-1}]\leq 2\sum_{t\in\widetilde{T}_{i}}\sigma_{t-1}( \mathbf{x}_{t})+4\log\frac{1}{\delta}+8\log 4+1\]

which completes the proof. 

### Proof of Proposition 4.2

_Proposition 4.2_.: _If there exists a \(\tilde{\epsilon}>0\) s.t. for all \(i\neq i^{*}\),_

\[\epsilon_{t}\leq\mathbb{E}\Big{[}f([\mathbf{x}^{i^{*}},\mathbf{X}^{-i^{*}}]) \Big{]}-\max_{\mathbf{x}^{i}\in\mathcal{X}^{i}}\mathbb{E}\big{[}f([\mathbf{x }^{i},\mathbf{X}^{-i}])\big{]}-\tilde{\epsilon}\]

_eventually (i.e., the inequality holds for all \(t\geq q\) for some \(q\geq 1\)), and \(\gamma_{T}(\mathcal{X})<\mathcal{O}(\sqrt{T})\), then, with probability at least \(1-\delta\), \(\lim_{T\to\infty}T_{i}/T=0\) for all \(i\neq i^{*}\) and UCB-CVS incurs a cost-varying cumulative regret that is sublinear in \(T\) by setting \(\beta_{t}=B+\sigma\sqrt{2\left(\gamma_{t-1}(\mathcal{X})+1+\log((m+1)/\delta) \right)}\)._

Proof.: Define \(\mathbf{x}_{t}^{i}\coloneqq\operatorname*{argmax}_{\mathbf{x}^{i}\in \mathcal{X}^{i}}\mathbb{E}\big{[}u_{t-1}([\mathbf{x}^{i},\mathbf{X}^{-i}]) \big{]}\), and \(j_{t}\coloneqq\operatorname*{argmax}_{i\in[m]}\max_{\mathbf{x}^{i}\in \mathcal{X}^{i}}\mathbb{E}\big{[}u_{t-1}([\mathbf{x}^{i},\mathbf{X}^{-i}]) \big{]}\). Using \(\mathbbm{1}[A]\) to denote the indicator functionthat is equal to \(1\) when the event \(A\) is true and \(0\) otherwise,

\[T_{i} \stackrel{{(i)}}{{\leq}}\sum_{t=1}^{T}\mathbbm{1}\Big{[} \mathbb{E}\big{[}u_{t-1}([\mathbf{x}_{t}^{i},\mathbf{X}^{-i}])\big{]}+\epsilon_ {t}\geq\mathbb{E}\Big{[}u_{t-1}([\mathbf{x}_{t}^{i},\mathbf{X}^{-j_{t}}]) \Big{]}\Big{]}\] \[\stackrel{{(ii)}}{{\leq}}\sum_{t=1}^{T}\mathbbm{1} \Big{[}\mathbb{E}\big{[}u_{t-1}([\mathbf{x}_{t}^{i},\mathbf{X}^{-i}])\big{]}+ \epsilon_{t}\geq\mathbb{E}\Big{[}u_{t-1}([\mathbf{x}^{i^{*}},\mathbf{X}^{-i^{ *}}])\big{]}\Big{]}\] \[\leq\sum_{t=1}^{T}\mathbbm{1}\Big{[}\mathbb{E}\big{[}u_{t-1}([ \mathbf{x}_{t}^{i},\mathbf{X}^{-i}])\big{]}+\epsilon_{t}\geq\mathbb{E}\Big{[} f([\mathbf{x}^{i^{*}},\mathbf{X}^{-i^{*}}])\Big{]}\Big{]}\] \[=\sum_{t=1}^{T}\mathbbm{1}\Big{[}\mathbb{E}\big{[}u_{t-1}([ \mathbf{x}_{t}^{i},\mathbf{X}^{-i}])\big{]}\geq\mathbb{E}\Big{[}f([\mathbf{x} ^{i^{*}},\mathbf{X}^{-i^{*}}])\Big{]}-\epsilon_{t}\Big{]}\] \[\leq q-1+\sum_{t=q}^{T}\mathbbm{1}\Big{[}\mathbb{E}\big{[}u_{t-1} ([\mathbf{x}_{t}^{i},\mathbf{X}^{-i}])\big{]}\geq\mathbb{E}\Big{[}f([\mathbf{x }^{i^{*}},\mathbf{X}^{-i^{*}}])\Big{]}-\epsilon_{t}\Big{]}\] \[\leq q-1+\sum_{t=q}^{T}\mathbbm{1}\Big{[}\mathbb{E}\big{[}u_{t-1} ([\mathbf{x}_{t}^{i},\mathbf{X}^{-i}])\big{]}\geq\max_{\mathbf{x}^{i}\in \mathcal{X}^{i}}\mathbb{E}\big{[}f([\mathbf{x}^{i},\mathbf{X}^{-i}])\big{]}+ \tilde{\epsilon}\Big{]}\] \[=q-1+\sum_{t=q}^{T}\mathbbm{1}\big{[}\mathbb{E}\big{[}u_{t-1}([ \mathbf{x}_{t}^{i},\mathbf{X}^{-i}])\big{]}-\mathbb{E}\big{[}f([\mathbf{x}_{t} ^{i},\mathbf{X}^{-i}])\big{]}\geq\tilde{\epsilon}\big{]}\] \[\leq q-1+\frac{1}{\tilde{\epsilon}}\sum_{t=q}^{T}\mathbb{E}\big{[} u_{t-1}([\mathbf{x}_{t}^{i},\mathbf{X}^{-i}])\big{]}-\mathbb{E}\big{[}f([ \mathbf{x}_{t}^{i},\mathbf{X}^{-i}])\big{]}\] \[\stackrel{{(iii)}}{{\leq}}q-1+\mathcal{O}^{*}\left( \frac{1}{\tilde{\epsilon}}\sqrt{T-q+1}\left(B\sqrt{\gamma_{T-q+1}(\mathcal{X}) }+\gamma_{T-q+1}(\mathcal{X})\right)\right)\]

where \((i)\) follows since control set \(i\) is only played when the condition on the RHS is true, \((ii)\) follows from the definitions of \(j_{t}\) and \(\mathbf{x}_{t}^{j_{t}}\), and \((iii)\) follows from the steps from (3) to (5) in the proof of Theorem 4.1, and \(\mathcal{O}^{*}\) denotes suppressing logarithmic factors. Now dividing both sides by \(T\) and taking the limit as \(T\) goes to infinity,

\[\lim_{T\to\infty}\frac{T_{i}}{T} \leq\lim_{T\to\infty}\frac{1}{T}\left(q-1+\mathcal{O}^{*}\left( \frac{1}{\tilde{\epsilon}}\sqrt{T-q+1}\left(B\sqrt{\gamma_{T-q+1}(\mathcal{X}) }+\gamma_{T-q+1}(\mathcal{X})\right)\right)\right)\] \[=0\]

which follows from \(\gamma_{T}(\mathcal{X})<\mathcal{O}(\sqrt{T})\) and completes the proof that, if the conditions in the proposition are fulfilled, suboptimal control sets will only be played a number of times that is sublinear in \(T\). The proof that \(R_{T}\) will then also be sublinear in \(T\) is straightforward. Assuming without loss of generality that \(i^{*}=m\),

\[R_{T} \coloneqq\sum_{t=1}^{T}c_{i_{t}}\left(\mathbb{E}\Big{[}f([\mathbf{ x}^{i^{*}},\mathbf{X}^{-i^{*}}])\Big{]}-\mathbb{E}\big{[}f([\mathbf{x}^{i_{t}}, \mathbf{X}^{-i_{t}}])\big{]}\right)\] \[=\sum_{i=1}^{m}\sum_{t\in\widehat{T_{i}}}c_{i}\left(\mathbb{E} \Big{[}f([\mathbf{x}^{i^{*}},\mathbf{X}^{-i^{*}}])\Big{]}-\mathbb{E}\big{[}f([ \mathbf{x}^{i_{t}},\mathbf{X}^{-i_{t}}])\big{]}\right)\] \[\leq\sum_{i=1}^{m-1}T_{i}c_{i}\left(\mathbb{E}\Big{[}f([\mathbf{ x}^{i^{*}},\mathbf{X}^{-i^{*}}])\Big{]}-\min_{\mathbf{x}^{i}\in\mathcal{X}^{i}} \mathbb{E}\big{[}f([\mathbf{x}^{i},\mathbf{X}^{-i}])\big{]}\right)\] \[\qquad+c_{m}\sum_{t\in\widehat{T_{m}}}\left(\mathbb{E}\Big{[}f([ \mathbf{x}^{i^{*}},\mathbf{X}^{-i^{*}}])\Big{]}-\mathbb{E}\big{[}f([\mathbf{x }^{i_{t}},\mathbf{X}^{-i_{t}}])\big{]}\right)\]\[=\sum_{\ell\in\overline{\mathcal{I}}_{i}}\mathbb{E}\Big{[}(X_{\ell}- \overline{X}_{\ell}-(X^{\prime}_{\ell}-\overline{X}_{\ell}))^{2}\Big{]}\] \[=\sum_{\ell\in\overline{\mathcal{I}}_{i}}\mathbb{E}\Big{[}((X_{ \ell}-\overline{X}_{\ell})-(X^{\prime}_{\ell}-\overline{X}^{\prime}_{\ell}))^{ 2}\Big{]}\] \[=\sum_{\ell\in\overline{\mathcal{I}}_{i}}\mathbb{E}\Big{[}(X_{ \ell}-\overline{X}_{\ell})^{2}-2(X_{\ell}-\overline{X}_{\ell})(X^{\prime}_{ \ell}-\overline{X}^{\prime}_{\ell})+(X^{\prime}_{\ell}-\overline{X}^{\prime} _{\ell})^{2}\Big{]}\]\[=\sum_{\ell\in\overline{\mathcal{T}}_{i}}\mathbb{E}\big{[}(X_{\ell}- \overline{X}_{\ell})^{2}\big{]}-\mathbb{E}\Big{[}(X_{\ell}-\overline{X}_{\ell})( X_{\ell}^{\prime}-\overline{X}_{\ell}^{\prime})\Big{]}+\mathbb{E}\Big{[}(X_{\ell}^{ \prime}-\overline{X}_{\ell}^{\prime})^{2}\Big{]}\] \[=\sum_{\ell\in\overline{\mathcal{T}}_{i}}\mathbb{E}\big{[}(X_{ \ell}-\overline{X}_{\ell})^{2}\big{]}+\mathbb{E}\Big{[}(X_{\ell}^{\prime}- \overline{X}_{\ell}^{\prime})^{2}\Big{]}\] \[=\sum_{\ell\in\overline{\mathcal{T}}_{i}}2\mathbb{V}[X_{\ell}].\] (6)

We will now construct a lower bound for a median of \(Y_{i}\) denoted \(M_{i}\).

\[\mathbb{E}[Y_{i}] =\mathbb{E}[Y_{i}|Y_{i}<M_{i}]\cdot P(Y_{i}<M_{i})+\mathbb{E}[Y_{ i}|Y_{i}=M_{i}]\cdot P(Y_{i}=M_{i})+\mathbb{E}[Y_{i}|Y_{i}>M_{i}]\cdot P(Y_{i}>M_{i})\] \[\leq M_{i}\cdot P(Y_{i}\leq M_{i})+\mathbb{E}[Y_{i}|Y_{i}>M_{i}] \cdot P(Y_{i}>M_{i})\] \[\overset{(i)}{\leq}M_{i}\cdot P(Y_{i}\leq M_{i})+h_{i}M_{i}\cdot P (Y_{i}>M_{i})\] \[\overset{(ii)}{\leq}\frac{1}{2}M_{i}+\frac{1}{2}(h_{i}\cdot M_{i})\] \[=\frac{h_{i}+1}{2}M_{i}\]

where \((i)\) follows from our assumption on the median \(M_{i}\) and \((ii)\) follows from the definition of a median: \(P(Y_{i}\leq M_{i})\geq 1/2\). Substituting in (6) completes our construction of the lower bound for \(M_{i}\):

\[M_{i} \geq\frac{2}{h_{i}+1}\mathbb{E}[Y_{i}]\] \[\geq\frac{4}{h_{i}+1}\sum_{\ell\in\overline{\mathcal{T}}_{i}} \mathbb{V}[X_{\ell}].\]

Now consider the \(\lfloor T_{i}/2\rfloor\) non-overlapping pairs of queries chosen with control set \(i\)3. Associate each pair with a random variable \(Y_{ij}\) such that we have \(\lfloor T_{i}/2\rfloor\) i.i.d. random variables \(Y_{i1},Y_{i2},...,Y_{i\lfloor T_{i}/2\rfloor}\). From the definition of a median, \(P(Y_{i}\geq M_{i})\geq 1/2\). Without loss of generality, assume the worst-case such that \(P(Y_{i}\geq M_{i})=1/2\). We can now construct \(\lfloor T_{i}/2\rfloor\) i.i.d. Bernoulli random variables \(Z_{1},Z_{2},...,Z_{n}\), \(n=\lfloor T_{i}/2\rfloor\), with \(p=1/2\) where a success (\(Z_{j}=1\)) corresponds to \(Y_{ij}\geq M_{i}\) and a failure (\(Z_{j}=0\)) corresponds to \(Y_{ij}<M_{i}\). Further define the random variable \(Z:=\sum_{j=1}^{n}Z_{j}\).

Footnote 3: While we technically have \(\binom{T_{i}}{2}\) (overlapping) pairs, the squared distances between each such pair will be identically distributed but not independent. For example, if \(T_{i}\geq 3\) and we knew that \(\binom{T_{i}}{2}-1\) of the squared distances were equal to \(0\) (i.e., all the queries are exactly the same), the last squared distance must also be equal to \(0\).

Applying Hoeffding's inequality,

\[P\left(\frac{1}{n}\sum_{j=1}^{n}(Z_{j}-p)\leq-t\right) \leq\exp{(-2nt^{2})}\] \[P\left(\frac{1}{n}Z-p\leq-t\right) \leq\exp{(-2nt^{2})}\] \[P\left(Z\leq n(p-t)\right) \leq\exp{(-2nt^{2})}.\]

Choosing \(t=p-\alpha/n\) for some constant \(\alpha\),

\[P\left(Z\leq\alpha\right)\leq\exp{\left(-2n\left(p-\frac{\alpha}{n}\right)^{2} \right)}.\]

For \(P\left(Z\leq\alpha\right)\leq\delta\),

\[\exp{\left(-2n\left(p-\frac{\alpha}{n}\right)^{2}\right)}=\delta\]\[\alpha=np-\sqrt{\frac{n}{2}\log\frac{1}{\delta}}\] \[\alpha=\frac{1}{2}\left\lfloor\frac{T_{i}}{2}\right\rfloor-\sqrt{ \frac{1}{2}\left\lfloor\frac{T_{i}}{2}\right\rfloor\log\frac{1}{\delta}}\] \[\alpha\geq\frac{1}{4}(T_{i}-1)-\sqrt{\frac{1}{4}T_{i}\log\frac{1} {\delta}}\] \[\alpha\geq\left\lfloor\frac{1}{4}(T_{i}-1)-\sqrt{\frac{1}{4}T_{i} \log\frac{1}{\delta}}\right\rfloor.\]

Therefore, with probability more than \(1-\delta\), \(Z>N_{i}\coloneqq\left\lfloor\frac{1}{4}(T_{i}-1)-\sqrt{\frac{1}{4}T_{i}\log \frac{1}{\delta}}\right\rfloor\), i.e., the number of non-overlapping pairs with squared distance greater than \(M_{i}\) is at least \(N_{i}\), which completes the proof. 

### Proof of Theorem 4.4

_Theorem 4.4_.: _If the following assumptions hold:_

1. _The assumption of Lemma_ 4.3 _holds;_
2. _The kernel_ \(k(\mathbf{x},\mathbf{x}^{\prime})\) _is an isotropic kernel (which only depends on distance and can be written as_ \(k(\left\|\mathbf{x}-\mathbf{x}^{\prime}\right\|)\)_);_
3. _There exists an iteration_ \(r\) _such that for all_ \(t\leq r,i_{t}\leq m-1\) _and for all_ \(t>r,i_{t}=m\)_;_

_then with probability at least \(1-\delta\), UCB-CVS (Alg. 1) incurs a cost-varying cumulative regret bounded by_

\[R_{T}\leq c_{m} \sum_{t=1}^{T} \epsilon_{t}+\mathcal{O}\Bigg{(}\left(B+\sqrt{\gamma_{T}(\mathcal{ X})+\log\frac{2m}{\delta}}\right)\left(c_{m}\left(\sqrt{T\gamma_{T}(\mathcal{X})}- \mathcal{L}+\log\frac{2m}{\delta}\right)+\right.\] \[\left.\sum_{i=1}^{m-1}c_{i}\left(\sqrt{T_{i}\gamma_{T_{i}}( \widetilde{\mathcal{X}_{i}})}+\log\frac{2m}{\delta}\right)\Bigg{)}\Bigg{)}\] \[\mathcal{L}\coloneqq\lambda\Bigg{(}\sum_{i=1}^{m-1}N_{i}\log\left( V_{i}-2k\left(\sqrt{M_{i}}\right)-k\left(\sqrt{M_{i}}\right)^{2}\right)+W \Bigg{)}\]

_by setting \(\beta_{t}=B+\sigma\sqrt{2\left(\gamma_{t-1}(\mathcal{X})+1+\log((2m)/\delta) \right)}\), where \(N_{i}\) and \(M_{i}\) are defined as in Lemma 4.3, and \(V_{i}\) and \(W\) are residual terms defined in (10)._

Proof.: We first construct a lower bound on the sum of posterior standard deviations of the queries up to iteration \(r\), i.e., the queries that were chosen with any control set except the last.

\[\sum_{t=1}^{r}\sigma_{t-1}(\mathbf{x}_{t}) \stackrel{{(i)}}{{\geq}}\sum_{t=1}^{r}\sigma_{t-1}^{2 }(\mathbf{x}_{t})\] \[=\lambda\sum_{t=1}^{r}\lambda^{-1}\sigma_{t-1}^{2}(\mathbf{x}_{t})\] \[\stackrel{{(ii)}}{{\geq}}\lambda\sum_{t=1}^{r}\log(1+ \lambda^{-1}\sigma_{t-1}^{2}(\mathbf{x}_{t}))\] \[\stackrel{{(iii)}}{{=}} \lambda\log\left|\mathbf{I}+\lambda^{-1}\mathbf{K}_{r}\right|\] \[=\lambda\log\left(\lambda^{-r}\left|\lambda\mathbf{I}+\mathbf{K} _{r}\right|\right)\] \[=\lambda\left(-r\log\lambda+\log\left|\lambda\mathbf{I}+\mathbf{ K}_{r}\right|\right)\] \[\stackrel{{(iv)}}{{\geq}} \lambda(\log\left|\lambda\mathbf{I}+\mathbf{K}_{r}\right|-2)\] (7)where \((i)\) follows from the assumption that \(k(x,x)=1\) which implies \(\sigma_{t-1}(\mathbf{x})\leq 1\) for all \(\mathbf{x}\in\mathcal{X}\) and all \(t\geq 1\), \((ii)\) follows since \(\log(1+x)\leq x\) for all \(x>-1\), \((iii)\) follows from Lemma A.2, and \((iv)\) follows from \(\lambda=1+\frac{2}{7}\) (Lemma A.4), noting that \(T\geq r\), and taking \(\lim_{r\to\infty}-r\log\lambda\).

From Lemma 4.3 with probability \(\delta/(2m)\), there will be at least \(N_{i}\) pairs of queries chosen with control set \(i\) with squared distance at least \(M_{i}\), where

\[N_{i} =\left\lfloor\frac{1}{4}(T_{i}-1)-\sqrt{\frac{1}{4}T_{i}\log \frac{2m}{\delta}}\right\rfloor\] \[M_{i} =\frac{4}{h_{i}+1}\sum_{t\in\mathcal{I}_{i}}\mathbb{V}[X_{\ell}]\]

Gather these \(2\sum_{i=1}^{m-1}N_{i}\) queries in an ordered sequence \(\mathcal{S}\) and keep paired queries adjacent to each other. The sequence should be ordered such that, for any control sets \(i\) and \(j\), if \(i<j\), then queries chosen with \(i\) should appear in the sequence before queries chosen with \(j\). Denote as \(\widetilde{\mathcal{T}}\) the ordered sequence of iterations at which each of these queries were chosen by the learner where the order corresponds to the order in \(\mathcal{S}\). Using row and column swaps on \(\mathbf{K}_{r}\), construct a new Gram matrix \(\mathbf{K}_{s}\) such that, for all \(j,\ell\leq 2\sum_{i=1}^{m-1}N_{i}\),

\[[\mathbf{K}_{s}]_{j\ell}=[\mathbf{K}_{r}]_{\widetilde{\mathcal{T}}_{j} \widetilde{\mathcal{T}}_{\ell}}.\]

In other words, we have simply reordered the underlying queries that result in the Gram matrix \(\mathbf{K}_{r}\) to produce a new Gram matrix \(\mathbf{K}_{s}\) such that the first \(2\sum_{i=1}^{m-1}N_{i}\) rows (and columns) correspond to the \(2N_{i}\) queries, and paired queries (that have at least \(M_{i}\) squared distance between them) are kept in adjacent rows (and columns). Note that

\[[\lambda\mathbf{I}+\mathbf{K}_{r}]_{\widetilde{\mathcal{T}}_{j}\widetilde{ \mathcal{T}}_{\ell}}=[\lambda\mathbf{I}+\mathbf{K}_{s}]_{j\ell}\]

i.e., the same row and column swap operations on \(\lambda\mathbf{I}+\mathbf{K}_{r}\) result in \(\lambda\mathbf{I}+\mathbf{K}_{s}\). Note that swapping the positions of two queries corresponds to a row swap and a column swap in the Gram matrix. We can thus conclude that

\[|\lambda\mathbf{I}+\mathbf{K}_{r}|=|\lambda\mathbf{I}+\mathbf{K}_{s}|\] (8)

since determinants are invariant under an even number of row or column swaps.

Write \(|\lambda\mathbf{I}+\mathbf{K}_{s}|\) as

\[|\lambda\mathbf{I}+\mathbf{K}_{s}|=\begin{bmatrix}\mathbf{A}_{1}&\mathbf{B}_ {1}\\ \mathbf{C}_{1}&\mathbf{D}_{1}\end{bmatrix}\]

where \(\mathbf{A}_{1}\) is a \(2\times 2\) matrix. Since \(\mathbf{A}_{1}\) is invertible,

\[|\lambda\mathbf{I}+\mathbf{K}_{s}|=|\mathbf{A}_{1}|\left|\mathbf{D}_{1}- \mathbf{C}_{1}\mathbf{A}_{1}^{-1}\mathbf{B}_{1}\right|\]

where \(\mathbf{D}_{1}-\mathbf{C}_{1}\mathbf{A}_{1}^{-1}\mathbf{B}_{1}\) is the Schur complement of \(\mathbf{A}_{1}\). Observe that

\[\mathbf{D}_{1}-\mathbf{C}_{1}\mathbf{A}_{1}^{-1}\mathbf{B}_{1} =\lambda\mathbf{I}+\mathbf{K}_{s-2}-\mathbf{k}_{2,s-2}^{\top}( \mathbf{K}_{2}+\lambda\mathbf{I})^{-1}\mathbf{k}_{2,s-2}\] \[=\lambda\mathbf{I}+\hat{\mathbf{K}}_{s-2}\]

where \(\mathbf{K}_{2}\) and \(\mathbf{K}_{s-2}\) are the prior covariance matrices of the first \(2\) queries and last \(r-2\) queries respectively, \(\mathbf{k}_{2,s-2}\) is the prior covariance between the first \(2\) queries and the last \(r-2\) queries, and \(\hat{\mathbf{K}}_{s-2}\) is the posterior covariance matrix of the last \(r-2\) queries conditioned on observations at the first \(2\) queries. We can repeat this decomposition:

\[\lambda\mathbf{I}+\hat{\mathbf{K}}_{s-2}=\begin{bmatrix}\mathbf{A}_{2}& \mathbf{B}_{2}\\ \mathbf{C}_{2}&\mathbf{D}_{2}\end{bmatrix}\]

\[\left|\lambda\mathbf{I}+\hat{\mathbf{K}}_{s-2}\right|=|\mathbf{A}_{2}|\left| \mathbf{D}_{2}-\mathbf{C}_{2}\mathbf{A}_{2}^{-1}\mathbf{B}_{2}\right|\]

\[\mathbf{D}_{2}-\mathbf{C}_{2}\mathbf{A}_{2}^{-1}\mathbf{B}_{2}=\lambda \mathbf{I}+\hat{\mathbf{K}}_{s-4}\]

where \(\hat{\mathbf{K}}_{s-4}\) is the posterior covariance matrix of the last \(r-4\) queries conditioned on observations at the first \(4\) queries, by the quotient property of the Schur complement [8]. Define \(N:=\sum_{i=1}^{m-1}N_{i}\). Performing this decomposition \(N\) times yields

\[|\lambda\mathbf{I}+\mathbf{K}_{s}|=\prod_{j=1}^{N}|\mathbf{A}_{j}|\left| \lambda\mathbf{I}+\hat{\mathbf{K}}_{s-2N}\right|\]where each \(\mathbf{A}_{j}\) is the \(2\times 2\) posterior covariance matrix of a pair of queries chosen with some control set \(i\) that have least \(M_{i}\) squared distance between them conditioned on observations at the first \(2(j-1)\) queries in the sequence, plus \(\lambda\mathbf{I}\). From (7) and (8),

\[\sum_{t=1}^{r}\sigma_{t-1}(\mathbf{x}_{t})\geq\lambda\left(\sum_{j=1}^{N}\log \left|\mathbf{A}_{j}\right|+\log\left|\lambda\mathbf{I}+\hat{\mathbf{K}}_{s-2 N}\right|-2\right).\] (9)

Let \(\hat{\mathbf{x}}_{j}\) and \(\hat{\mathbf{x}}_{j}^{\prime}\) refer to the pair of queries associated with \(\mathbf{A}_{j}\), and \(\tilde{k}_{j}\) to the posterior covariance function conditioned on observations at the first \(2(j-1)\) queries in the sequence. Define \(\mathbf{k}_{j}\) and \(\mathbf{k}_{j}^{\prime}\) as the \(\mathbb{R}^{2(j-1)}\) vectors of the prior covariance between the first \(2(j-1)\) queries in the sequence and \(\hat{\mathbf{x}}_{j}\) and \(\hat{\mathbf{x}}_{j}^{\prime}\) respectively. Further define \(\mathbf{M}_{j}\coloneqq\mathbf{K}_{2(j-1)}+\lambda\mathbf{I}\). Use \(\left\langle\mathbf{u},\mathbf{v}\right\rangle_{\mathbf{M}}\) to denote \(\mathbf{u}^{\top}\mathbf{M}\mathbf{v}\), and \(\left\|\mathbf{u}\right\|_{\mathbf{M}}\) to denote \(\sqrt{\left\langle\mathbf{u},\mathbf{u}\right\rangle_{\mathbf{M}}}\). Each \(\left|\mathbf{A}_{j}\right|\) can be lower bounded as

\[\left|\mathbf{A}_{j}\right| =(\tilde{k}_{j}(\hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j})+ \lambda)(\tilde{k}_{j}(\hat{\mathbf{x}}_{j}^{\prime},\hat{\mathbf{x}}_{j}^{ \prime})+\lambda)-\tilde{k}_{j}(\hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j}^{ \prime})^{2}\] \[=\tilde{k}_{j}(\hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j})\tilde{k }_{j}(\hat{\mathbf{x}}_{j}^{\prime},\hat{\mathbf{x}}_{j}^{\prime})+\lambda \tilde{k}_{j}(\hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j})+\lambda\tilde{k}_{j}( \hat{\mathbf{x}}_{j}^{\prime},\hat{\mathbf{x}}_{j}^{\prime})+\lambda^{2}- \tilde{k}_{j}(\hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j}^{\prime})^{2}\] \[\stackrel{{(i)}}{{\geq}} \left(1-\left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2} \right)\left(1-\left\|\mathbf{k}_{j}^{\prime}\right\|_{\mathbf{M}_{j}^{-1}}^{ 2}\right)+\lambda\left(1-\left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^ {2}\right)+\lambda\left(1-\left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^ {2}\right)+\lambda^{2}\] \[\stackrel{{(ii)}}{{\geq}} \left(1-\left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2} \right)\left(1-\left\|\mathbf{k}_{j}^{\prime}\right\|_{\mathbf{M}_{j}^{-1}}^{ 2}\right)+\lambda\left(1-\left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^ {2}\right)+\lambda^{2}\] \[\stackrel{{(iii)}}{{\geq}} 1-\left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}- \left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}+\lambda\left(1-\left\| \mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}\right)+\lambda\left(1-\left\| \mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}\right)+\lambda^{2}\] \[\stackrel{{(iii)}}{{\geq}} 1-\left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}- \left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}+\lambda\left(1-\left\| \mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}\right)+\lambda\left(1-\left\| \mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}\right)+\lambda^{2}\] \[\stackrel{{(iii)}}{{\geq}} 1-\left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}- \left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}+\lambda\left(1-\left\| \mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}\right)+\lambda\left(1-\left\| \mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}\right)+\lambda^{2}\] \[\stackrel{{(iii)}}{{\geq}} 1-\left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}- \left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}+\lambda\left(1-\left\| \mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}\right)+\lambda\left(1-\left\| \mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}\right)+\lambda^{2}\] \[\stackrel{{(iii)}}{{\geq}} -k(\hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j}^{\prime})^{2}-2k(\hat{ \mathbf{x}}_{j},\hat{\mathbf{x}}_{j}^{\prime})\] \[=\lambda^{2}-1+(\lambda+1)\left(1-\left\|\mathbf{k}_{j}\right\|_ {\mathbf{M}_{j}^{-1}}^{2}\right)+(\lambda+1)\left(1-\left\|\mathbf{k}_{j}^{ \prime}\right\|_{\mathbf{M}_{j}^{-1}}^{2}\right)-2k(\hat{\mathbf{x}}_{j},\hat{ \mathbf{x}}_{j}^{\prime})-k(\hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j}^{\prime})^ {2}\] \[=\lambda^{2}-1+(\lambda+1)\left(\tilde{k}_{j}(\hat{\mathbf{x}}_{j}, \hat{\mathbf{x}}_{j})+\tilde{k}_{j}(\hat{\mathbf{x}}_{j}^{\prime},\hat{\mathbf{x} }_{j}^{\prime})\right)-2k(\hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j}^{\prime})-k( \hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j}^{\prime})^{2}\]

where \((i)\) follows from our assumption that \(k(\mathbf{x},\mathbf{x})=1\), \((ii)\) follows from the Cauchy-Schwarz inequality, and \((iii)\) follows since \(1-\left\|\mathbf{k}_{j}\right\|_{\mathbf{M}_{j}^{-1}}^{2}\leq 1\) and \(1-\left\|\mathbf{k}_{j}^{\prime}\right\|_{\mathbf{M}_{j}^{-1}}^{2}\leq 1\).

Define \(S_{i}\coloneqq\sum_{\ell=1}^{i}N_{i}\) and \(\tilde{v}_{i}\coloneqq\min_{S_{i-1}+1\leq j\leq S_{i}}\frac{1}{2}(\tilde{k}_{j}( \hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j})+\tilde{k}_{j}(\hat{\mathbf{x}}_{j}^{ \prime},\hat{\mathbf{x}}_{j}^{\prime}))\). Substituting this result into (9),

\[\sum_{t=1}^{r}\sigma_{t-1}(\mathbf{x}_{t}) \geq\lambda\Bigg{(}\sum_{j=1}^{N}\log\left(\lambda^{2}-1+( \lambda+1)\left(\tilde{k}_{j}(\hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j})+\tilde{k}_ {j}(\hat{\mathbf{x}}_{j}^{\prime},\hat{\mathbf{x}}_{j}^{\prime})\right)-2k( \hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j}^{\prime})-k(\hat{\mathbf{x}}_{j},\hat{ \mathbf{x}}_{j}^{\prime})^{2}\Big{)}\] \[\qquad+\log\left|\lambda\mathbf{I}+\hat{\mathbf{K}}_{s-2N} \right|-2\Bigg{)}\] \[=\lambda\Bigg{(}\sum_{i=1}^{m-1}\sum_{j=S_{i-1}+1}^{S_{i}}\log \left(\lambda^{2}-1+(\lambda+1)\left(\tilde{k}_{j}(\hat{\mathbf{x}}_{j},\hat{ \mathbf{x}}_{j})+\tilde{k}_\[-k(\hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j}^{\prime})^{2}\Big{)}+ \log\left|\lambda\mathbf{I}+\hat{\mathbf{K}}_{s-2N}\right|-2\Bigg{)}\] \[\geq\lambda\Bigg{(}\sum_{i=1}^{m-1}\sum_{j=S_{i-1}+1}^{S_{i}}\log \left(\lambda^{2}-1+2(\lambda+1)\tilde{v}_{i}-2k(\hat{\mathbf{x}}_{j},\hat{ \mathbf{x}}_{j}^{\prime})-k(\hat{\mathbf{x}}_{j},\hat{\mathbf{x}}_{j}^{ \prime})^{2}\right)\] \[\qquad+\log\left|\lambda\mathbf{I}+\hat{\mathbf{K}}_{s-2N}\right| -2\Bigg{)}\] \[\overset{(i)}{\geq}\lambda\Bigg{(}\sum_{i=1}^{m-1}\sum_{j=S_{i-1 }+1}^{S_{i}}\log\left(\lambda^{2}-1+2(\lambda+1)\tilde{v}_{i}-2k\left(\sqrt{M _{i}}\right)-k\left(\sqrt{M_{i}}\right)^{2}\right)\] \[\qquad+\log\left|\lambda\mathbf{I}+\hat{\mathbf{K}}_{s-2N}\right| -2\Bigg{)}\] \[=\lambda\Bigg{(}\sum_{i=1}^{m-1}N_{i}\log\left(\lambda^{2}-1+2( \lambda+1)\tilde{v}_{i}-2k\left(\sqrt{M_{i}}\right)-k\left(\sqrt{M_{i}}\right) ^{2}\right)\] \[\qquad+\log\left|\lambda\mathbf{I}+\hat{\mathbf{K}}_{s-2N}\right| -2\Bigg{)}\] \[=\lambda\Bigg{(}\sum_{i=1}^{m-1}N_{i}\log\left(V_{i}-2k\left( \sqrt{M_{i}}\right)-k\left(\sqrt{M_{i}}\right)^{2}\right)+W\Bigg{)}\] (10) \[=:\mathcal{L}\] (11)

where \(V_{i}\coloneqq\lambda^{2}-1+2(\lambda+1)\tilde{v}_{i}\) and \(W\coloneqq\log\left|\lambda\mathbf{I}+\hat{\mathbf{K}}_{s-2N}\right|-2\), \((i)\) follows from our assumption that the kernel \(k\) is stationary and can be written in a single argument form as \(k(\|\mathbf{x}-\mathbf{x}^{\prime}\|)=k(\mathbf{x},\mathbf{x}^{\prime})\) and the fact that every pair of queries in \(\mathcal{S}\) chosen with control set \(i\) has squared distance at least \(M_{i}\).

Starting from (4) in the proof of Theorem 4.1 except replacing the probabilities of all events with \(2m/\delta\),

\[R_{T} \leq\left(\sum_{i=1}^{m}c_{i}(2\beta_{T})\left(2\sum_{t\in\widehat {T}_{i}}\sigma_{t-1}(\mathbf{x}_{t})+4\log\frac{2m}{\delta}+8\log(4)+1\right) \right)+c_{m}\sum_{t=1}^{T}\epsilon_{t}\] \[=2\beta_{T}\Bigg{(}c_{m}\Bigg{(}2\sum_{t\in\widehat{T}_{m}} \sigma_{t-1}(\mathbf{x}_{t})+4\log\frac{2m}{\delta}+8\log(4)+1\Bigg{)}\] \[\qquad+\sum_{i=1}^{m-1}c_{i}\Bigg{(}2\sum_{t\in\widehat{T}_{i}} \sigma_{t-1}(\mathbf{x}_{t})+4\log\frac{2m}{\delta}+8\log(4)+1\Bigg{)}\Bigg{)} +c_{m}\sum_{t=1}^{T}\epsilon_{t}\] \[\overset{(i)}{\leq}2\beta_{T}\Bigg{(}c_{m}\Bigg{(}2\sum_{t\in \widehat{T}_{m}}\sigma_{t-1}(\mathbf{x}_{t})+4\log\frac{2m}{\delta}+8\log(4)+1 \Bigg{)}\] \[\qquad+\sum_{i=1}^{m-1}c_{i}\Bigg{(}2\sqrt{4(T_{i}+2)\gamma_{T_{i }}(\widehat{\mathcal{X}}_{i})}+4\log\frac{2m}{\delta}+8\log(4)+1\Bigg{)}\Bigg{)} +c_{m}\sum_{t=1}^{T}\epsilon_{t}\] \[\overset{(iii)}{\leq}2\beta_{T}\Bigg{(}c_{m}\Bigg{(}2\sqrt{4(T+2) \gamma_{T}(\mathcal{X})}-\mathcal{L}+4\log\frac{2m}{\delta}+8\log(4)+1\Bigg{)}\]\[\sum_{t=1}^{T}m_{t}\leq 2\sum_{t=1}^{T}X_{t}+4b_{T}\log\frac{1}{ \delta}+8b_{T}\log(4b_{T})+1\]

## Appendix B Comparison to Naive Baselines

We investigated simple extensions of TS-PSQ, UCB-PSQ, and EI-PSQ (i.e., the classic expected improvement algorithm [20] adapted for BOPSQ, see Appendix C for details) for the cost-varying problem by dividing the acquisition score of a control set by its cost in a manner similar to Snoek et al. [31, Sec. 3.2]. Fig. 3 shows the mean and standard error of the simple regret incurred over \(10\) RNG seeds for one set of experiments. We found that these naive methods generally do not work well. For TS per unit cost and UCB-PSQ per unit cost, if a suboptimal control set is very cheap, its acquisition score may remain artificially high throughout, and the algorithm fails to converge. EI per unit cost was slightly more promising, but suffered from the inverse problem: the suboptimal control sets had \(0\) expected improvement very quickly and dividing by the cost had no effect. This algorithm thus fails to encourage exploration with cheaper control sets. Furthermore, the EI algorithm was computationally expensive due to the double Monte Carlo expectation computation. In general, we see that the UCB-CVS algorithm is able to use the cheaper control sets much more effectively for exploration and hence find better solutions.

## Appendix C Experimental Details

All experiments use a squared exponential kernel with ARD lengthscales that depend on the objective function, \(k(\mathbf{x},\mathbf{x}^{\prime})=1\), Gaussian observation noise with \(\sigma=0.01\), and \(5\) initial query-observation pairs with queries drawn uniformly at random. All expectations are approximated with Monte Carlo sampling with \(1024\) samples. All acquisition maximizations are performed with L-BFGS-B with random restarts. All query sets are \([0,1]^{d}\).

### Objective functions

The control sets described here are given in an order corresponding to their costs given in Sec. 5. For example, for the GP samples objective, under the cheap cost set, control set \(\{1\}\) has cost \(0.01\), control set \(\{1,2\}\) has cost \(0.1\), and control set \(\{1,2,3\}\) has cost \(1\).

**Samples from GP prior (3-D):** We use samples from the same kernel \(k\) used to model the GP posteriors during learning. We use a kernel lengthscale of \(0.1\) and control sets \(\{\{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},\{1,2,3\}\}\).

**Hartmann (3-D):** We use a kernel lengthscale of \(0.1\) and control sets \(\{\{1\},\{2\},\{3\},\{1,2\},\{1,3\},\{2,3\},\{1,2,3\}\}\).

**Plant growth simulator (5-D):** The plant growth simulator is a GP built from private data collected on the maximum leaf area achieved by Marchania plants depending on input variables Ca, B, NH\({}_{3}\), K, and pH. We use min-max feature scaling to scale all input variables to \([0,1]\) and standardize the output values. We use the posterior mean of the GP as the objective function. We use a kernel lengthscale of \(0.2\) and control sets \(\{\{1,2\},\{3,4\},\{4,5\},\{1,2,3\},\{2,3,4\},\{3,4,5\},\{1,2,3,4,5\}\}\).

**Airfoil self-noise (5-D):** We use the airfoil self-noise dataset from the UCI Machine Learning Repository [9]. To scale all input variables to \([0,1]\), we first take the natural logarithm of variables \(1\) and \(5\), then do min-max feature scaling on all input variables. We also standardize the output values. We then feed the data into a default SingleTaskGP from BoTorch and use the posterior mean as the objective function. We use a kernel lengthscale of \(0.2\) and control sets \(\{\{4,5\},\{2,5\},\{1,4\},\{2,3\},\{3,5\},\{1,2\},\{3,4\}\}\).

### Algorithms

**UCB-PSQ and UCB-CVS:** For the experiments, we set \(\beta_{t}=2\) for all \(t\).

**TS-PSQ:** Following [13], we use random Fourier features (RFF) [28] to approximately sample from a GP posterior. We use RFF with \(1024\) features.

Figure 3: Mean and standard error (over \(10\) RNG seeds) of the simple regret (lower is better) incurred against cost spent (budget) \(C\) by all algorithms including TS-PSQ per unit cost, UCB-PSQ per unit cost, and EI per unit cost, with samples from the GP prior as the objective function, moderate cost set, and all variances. A diamond indicates the average budget after which an algorithm only chooses the optimal control set.

**EI-PSQ:** We adapt the BoTorch acquisition NoisyExpectedImprovement to the BOPSQ problem setting. To evaluate the acquisition score of a partial query, we first sample \(32\) fantasy models of \(f\) from the GP posterior. For each fantasy model, we compute the expected value of the partial query and take the best value as the value of the best observation so far (assuming the full query control set is available). We then compute the improvement score as the expected value minus the best value, and then average the improvement score over all fantasy models.

### Implementation

The experiments were implemented in Python. The major libraries used were NumPy [12], SciPy [40], PyTorch [26], GPyTorch [10] and BoTorch [3]. For more details, please refer to the code repository.

### Compute

The following CPU times in seconds were collected on a server running Ubuntu 20.04.4 LTS with \(2\times\) Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz and 256 GB of RAM. We measure the CPU time for \(1\) iteration of TS-PSQ and UCB-CVS with a dataset of \(100\) observations. In general, none of the algorithms in the settings tested in this paper require a significant amount of compute.

\begin{tabular}{l l l l l} \hline \hline  & GP sample & Hartmann & Plant & Airfoil \\ \hline TS-PSQ & 6.27 & 4.14 & 8.96 & 232.27 \\ UCB-CVS & 37.92 & 52.34 & 61.96 & 87.09 \\ \hline \hline \end{tabular}

## Appendix D Limitations

A limitation of our work is that the theoretical guarantees of UCB-CVS rely on a few assumptions that may not hold in practice. For example, the regularity assumption that assumes the objective function \(f\) resides in some RKHS may not be true in some problems. The kernel corresponding to this RKHS may not be known either. The work also assumes that the probability distributions governing each variable are independent and fixed. In practice, these assumptions may be violated, if the probability distributions have some dependence on one another, or may change over time.