# Log-concave Sampling from a Convex Body with a Barrier: a Robust and Unified Dikin Walk

Yuzhou Gu

New York University

yuzhougu@nyu.edu

&Nikki Lijing Kuang

University of California, San Diego

llkuang@ucsd.edu

&Yi-An Ma

University of California, San Diego

yianma@ucsd.edu

&Zhao Song

Simons Institute for the Theory of Computing, UC Berkeley

magic.linuxkde@gmail.com

&Lichen Zhang

MIT CSAIL

lichenz@csail.mit.edu

###### Abstract

We consider the problem of sampling from a \(d\)-dimensional log-concave distribution \(\pi(\theta)\propto\exp(-f(\theta))\) for \(L\)-Lipschitz \(f\), constrained to a convex body with an efficiently computable self-concordant barrier function, contained in a ball of radius \(R\) with a \(w\)-warm start.

We propose a _robust_ sampling framework that computes spectral approximations to the Hessian of the barrier functions in each iteration. We prove that for polytopes that are described by \(n\) hyperplanes, sampling with the Lee-Sidford barrier function mixes within \(\widetilde{O}((d^{2}+dL^{2}R^{2})\log(w/\delta))\) steps with a per step cost of \(\widetilde{O}(nd^{\omega-1})\), where \(\omega\approx 2.37\) is the fast matrix multiplication exponent. Compared to the prior work of Mangoubi and Vishnoi, our approach gives faster mixing time as we are able to design a generalized soft-threshold Dikin walk beyond log-barrier.

We further extend our result to show how to sample from a \(d\)-dimensional spectralhedron, the constrained set of a semidefinite program, specified by the set \(\{x\in\mathbb{R}^{d}:\sum_{i=1}^{d}x_{i}A_{i}\succeq C\}\) where \(A_{1},\ldots,A_{d},C\) are \(n\times n\) real symmetric matrices. We design a walk that mixes in \(\widetilde{O}((nd+dL^{2}R^{2})\log(w/\delta))\) steps with a per iteration cost of \(\widetilde{O}(n^{\omega}+n^{2}d^{3\omega-5})\). We improve the mixing time bound of prior best Dikin walk due to Narayanan and Rakhlin that mixes in \(\widetilde{O}((n^{2}d^{3}+n^{2}dL^{2}R^{2})\log(w/\delta))\) steps.

## 1 Introduction

Given a convex body, generate samples from the body according to structured densities is a fundamental problem in computer science and machine learning. It has extensive applications in constrained convex optimization (Lovasz and Vempala, 2006; Narayanan, 2016), differentially private learning (Wang et al., 2015; Lin et al., 2024) and online optimization (Narayanan and Rakhlin, 2017). A central theme in the theory of sampling is to leverage stochasticity and reduce per iteration costs without having to proportionally increase the number of iterations. This theme has played out in continuous optimization for both first and second order methods. For the first order methods, focus is on reducing the variance of the gradient estimators (Johnson and Zhang, 2013; Shalev-Shwartz and Zhang, 2013; Defazio et al., 2014). For the second order methods, matrix sketching is often used toreduce computation and storage related to the Hessian matrix (Lee et al., 2019; Jiang et al., 2020, 2021; Song and Yu, 2021; Qin et al., 2023).

In Markov chain Monte Carlo (MCMC), much of the progress is on the first order methods. Non-asymptotic analyses are performed for the stochastic gradient Langevin algorithms and their variance reduced extensions (Dubey et al., 2016; Raginsky et al., 2017; Brosse et al., 2018; Chatterji et al., 2018; Zou et al., 2018; Dalalyan and Karagulyan, 2019; Li et al., 2019; Ding and Li, 2021). For second order methods, there has been a paucity when it comes to applying the sketching techniques.

In this paper, we focus on sampling from a log-concave distribution constrained to a \(d\)-dimensional convex body \(\mathcal{K}\) that is described by \(n\) constraints, where second order information is proven essential for capturing the geometry of the convex body and consequently for achieving fast convergence rates (Narayanan, 2016; Narayanan and Rakhlin, 2017; Chen et al., 2018; Laddha et al., 2020; Mangoubi and Vishnoi, 2023, 2024). In particular, we associate the convex body with a _self-concordant barrier function_(Nesterov and Nemirovskii, 1994; Renegar, 1988; Vaidya, 1989) and utilize the Hessian matrix \(H(x)\) of the barrier function in the sampling algorithm. We then use the Hessian matrix to propose samples and compute the probability to accept or reject the proposed samples. We note that this problem involves subtleties beyond the scope of constrained optimization. In continuous optimization, which focuses on finding the descent directions, unbiased estimators with reasonable variance oftentimes suffice (Vaidya, 1989; Lee et al., 2015; Huang et al., 2022). In MCMC, on the other hand, the target probability distribution needs to be maintained along the entire trajectory. This poses significant challenges to speed up the sampling algorithms. In the scenario of uniform sampling over a polytope, Laddha et al. (2020) shows that for a simple logarithmic barrier1, an unbiased estimator in fact suffices. However, more complicated barriers such as the Lee-Sidford barrier (Lee and Sidford, 2014, 2019) can only be approximately computed2, an unbiased estimator is extremely hard to be obtained. Moreover, for sampling from more sophisticated log-concave distributions and convex bodies, a more general approach is needed.

Footnote 1: Logarithmic barriers, or log-barriers have been extensively studied in the context of sampling and convex optimization. For a \(d\)-dimensional polytope with \(n\) constraints, it typically leads to a convergence rate polynomially depends on \(n\).

Footnote 2: Lee-Sidford barriers are the first polynomial-time computable barriers with complexity parameter depends only logarithmic on \(n\) and in extension, the convergence rate.

We therefore propose to obtain a high precision estimator to the desired acceptance rate with an improved per step running time and without sacrificing the rapid mixing time. In particular, we ask the following question:

_Can we significantly reduce per iteration cost, while preserving the convergence rate of the log-concave sampling algorithms over various convex bodies?_

We answer the above question in the affirmative. To this end, we present a slow of results regarding log-concave sampling. For polytopes, we provide a walk that mixes in \(\widetilde{O}(d^{2}+dL^{2}R^{2})\) steps3 with per iteration cost \(\widetilde{O}(nd^{\omega-1})\). Prior state-of-the-art results are due to Mangoubi and Vishnoi (2023, 2024), for which their walks mix in \(\widetilde{O}(nd+dL^{2}R^{2})\) with a per iteration cost4\(\widetilde{O}(\mathrm{nnz}(A)+d^{2})\). Our walk mixes faster whenever \(n\geq d\). Our result partially resolves an open problem in Mangoubi and Vishnoi (2023) as they asked whether it's possible to design a Dikin walk whose mixing time is only depends on \(d\) and independent of \(L,R\). We remove the \(L\) and \(R\) dependence on the dominating term \(d^{2}\), hence for the case of sampling from uniform distribution (\(L=0\)), we recover the state-of-art result of Laddha et al. (2020) which mixes in \(\widetilde{O}(d^{2})\) steps with a per iteration cost \(\widetilde{O}(nd^{\omega-1})\).

Footnote 3: We use \(\widetilde{O}(\cdot)\) to hide polylogarithmic dependence on \(n,d\) and \(\delta\) where \(\delta\) is the TV distance between the target distribution and our Markov chain \(\mu\).

Footnote 4: We use \(\mathrm{nnz}(A)\) to denote the number of nonzero entries in matrix \(A\).

We also consider sampling from convex bodies beyond polytopes. In semidefinite programming (SDP), one often focuses on the dual program of an SDP, where the constraint set is defined as a spectrahedron \(\mathcal{K}=\{x\in\mathbb{R}^{d}:\sum_{i=1}^{d}x_{i}A_{i}\succeq C\}\)5 for \(n\times n\) symmetric matrices \(A_{i}\) and \(C\)(Jiang et al., 2020; Huang et al., 2022). We propose a walk that samples from a log-concave distribution over a spectrahedron \(\mathcal{K}\) using the Hessian information of the log-barrier. The walk mixes in \(\widetilde{O}(nd+dL^{2}R^{2})\) steps. For log-barrier of an SDP, explicitly forming the Hessian matrix takes a prohibitively large \(O(n^{\omega}d+n^{2}d^{\omega-1})\) time. We utilize our robust sampling framework to approximately compute this Hessian via tensor-based sketching techniques, achieving a runtime of \(\widetilde{O}(n^{\omega}+n^{2}d^{3\omega-5})\). As long as \(n\geq d^{\frac{3\omega-4}{\omega-2}}\) (which is a usual setting for SDP, where \(d\ll n\)), this provides a speedup.

### Related works

In this work, we focus on Dikin walk (Kannan and Narayanan, 2012), a refined variant of ball walk (Lovasz and Simonovits, 1993). Roughly speaking, ball walk progresses by moving to a random point in the ball centered at the current point with the obvious downside that when the convex body is flat, ball walk progresses slowly. Dikin walk overcomes this problem by instead moving to a random point in a good ellipse centered at the current point, in an effort to better utilize the local geometry of the convex body.

For sampling over polytopes, a number of Dikin walks use the ellipse induced by the log-barrier functions (Kannan and Narayanan, 2012). The work of Laddha et al. (2020) shows that uniform sampling over a polytope can mix in \(O(\nu d)\) steps, where \(\nu\) is the self-concordant parameter of the barrier function6. Going beyond uniform distributions, Narayanan and Rakhlin (2017) proposes a walk that samples from a log-concave distribution on a polytope in \(\widetilde{O}(\nu^{2}d)\) steps. This bound is later improved by Mangoubi and Vishnoi (2023), in which they show that for logarithmic barrier, a mixing time of \(\widetilde{O}(nd+dL^{2}R^{2})\) is attainable, where \(L\) is the Lipschitz constant of \(f\) and \(R\) is the radius of the ball that contains \(\mathcal{K}\). A more recent work by Kook and Vempala (2024) has obtained better bound when the function \(f\) is \(\alpha\)-relative strongly-convex and the density \(\pi\) is \(\beta\)-Lipschitz. They manage to obtain a mixing bound of \(O(\nu d\beta\log(w/\delta))\). However, their algorithm requires stronger assumptions on \(f\) and hence is incomparable to our result. In most previous works, the focus has been on improving the mixing time, rather than on per iteration costs.

Footnote 6: Technically, what they have shown is that if the Hessian is a \(\overline{\nu}\)-symmetric self-concordant barrier function, then the walk mixes in \(O(\overline{\nu}d)\) steps, and they subsequently prove for various barriers of interest, \(\overline{\nu}=\nu\).

For any convex body, it is well-known that a universal barrier with \(\nu=d\) exists (Nesterov and Nemirovskii, 1989; Lee and Yue, 2021), however it is computationally hard to construct the Hessian of the universal barrier. In short, the universal barrier requires to compute the volume of the polar set of a high dimensional body, which is even hard to approximate deterministically (Furedi and Barany, 1986; Nesterov and Nemirovskii, 1994). The seminal work of Lee and Sidford (Lee and Sidford, 2014) presents a nearly-universal barrier with \(\nu=O(d\log^{5}n)\) and the Hessian can be _approximated_ in \(O(nd^{\omega-1})\) time. The Lee-Sidford barrier was originally designed to solve linear programs in \(O(\sqrt{d})\) iterations, and it has been leveraged lately for Dikin walks with rapid mixing time. For uniform sampling, Chen et al. (2018) gives an analysis with a walk that mixes in \(\widetilde{O}(d^{2.5})\) steps. Subsequently, Laddha et al. (2020) improves the mixing to \(\widetilde{O}(d^{2})\) steps. In a pursuit to better leverage the local geometry, Lee and Vempala (2017) proposes a walk relying on Riemannian metric that mixes in \(\widetilde{O}(nd^{3/4})\) steps. The mixing rate is later improved to \(\widetilde{O}(nd^{2/3})\) via Riemannian Hamiltonian Monte Carlo (RHMC) with log-barrier (Lee and Vempala, 2018) and \(\widetilde{O}(n^{1/3}d^{4/3})\) with a mixed Lee-Sidford and log-barrier (Gatmiry et al., 2024). In this work, we show that log-concave sampling can also leverage Lee-Sidford barrier to obtain a mixing time of \(\widetilde{O}(d^{2}+dL^{2}R^{2})\). In comparison, the hit-and-run algorithm (Lovasz and Vempala, 2007) mixes in \(\widetilde{O}(d^{2}R^{2}/r^{2})\) steps where \(r\) is the radius of the inscribed ball inside \(\mathcal{K}\). For the regime where \(L=O(1)\) and \(r=O(1)\), our walk mixes strictly faster than that of hit-and-run.

We note that for sampling over more general convex bodies, a recent work (Chen and Eldan, 2022) proves that for uniform sampling over an isotropic convex body, the mixing time bound is \(\widetilde{O}(d^{2}/\psi_{d}^{2})\) where \(\psi_{d}\) is the KLS constant (Kannan et al., 1995). However, it is unclear how to generalize their result to non-isotropic convex bodies or log-concave densities.

### Our results

Our results concern log-concave sampling from polytopes and spectrahedra. For polytopes, we state the result in its full generality.

**Theorem 1.1** (Robust sampling for log-concave distribution over polytopes).: _Let \(\delta\in(0,1)\) and \(R\geq 1\). Given a constraint matrix \(A\in\mathbb{R}^{n\times d}\) with a vector \(b\in\mathbb{R}^{n}\), let \(\mathcal{K}:=\{x\in\mathbb{R}^{d}:Ax\leq b\}\) be the corresponding polytope. Suppose \(\mathcal{K}\) is enclosed in a ball of radius \(R\) with non-empty interior. Let \(f:\mathcal{K}\rightarrow\mathbb{R}\) be an \(L\)-Lipschitz, convex function with an evaluation oracle. Finally, let \(\pi\) be the distribution such that \(\pi\propto e^{-f}\)._

_Suppose we are given an initial point from \(\mathcal{K}\) that is sampled from a \(w\)-warm start distribution7 with respect to \(\pi\) for some \(w>0\), then there is an algorithm (Algorithm 1) that outputs a point from a distribution \(\mu\) where \(\mathrm{TV}(\mu,\pi)\leq\delta\)._

Footnote 7: We say a distribution \(\rho\) is a \(w\)-warm start of with respect to a distribution \(\pi\) if \(\sup_{x\in\mathcal{K}}\frac{\rho(x)}{\pi(x)}\leq w\).

_Let \(g:\mathcal{K}\rightarrow\mathbb{R}\) be a \(\nu\)-self-concordant barrier function such that in time \(\mathcal{C}_{g}\), a spectral approximation \(\widetilde{H}_{g}\) of the Hessian of \(g\) denoted by \(H_{g}\) can be computed satisfying_

\[(1-\epsilon)\cdot H_{g}\preceq\widetilde{H}_{g}\preceq(1+\epsilon)\cdot H_{g}\]

_for \(\epsilon=\Theta(1/d)\). Then, Algorithm 1 takes at most_

\[\widetilde{O}((\nu d+dL^{2}R^{2})\log(w/\delta))\]

_Markov chain steps. It uses \(O(1)\) function evaluations and an extra \(\mathcal{C}_{g}+d^{\omega}\) time at each step._

Let us pause and make some remarks regarding Theorem 1.1. As long as the Hessian matrix can be approximately generated with an error inversely depends on \(d\), then our algorithm is guaranteed to converge. Moreover, if the approximate Hessian can be generated more efficiently, then this directly implies an improvement of our algorithm. On the mixing time side, Theorem 1.1 is nearly-optimal up to polylogarithmic factors and the dependence on \(L\) and \(R\). In particular, the \(\nu d\) mixing time bound is also achieved by Laddha et al. (2020) in the case of uniform sampling. We instantiate the meta result of Theorem 1.1 into the following theorem.

**Theorem 1.2** (Robust sampling with nearly-universal barrier).: _Under the conditions of Theorem 1.1, let \(g\) be the Lee-Sidford barrier with \(\nu=O(d\log^{5}n)\). Then, we have_

\[\mathcal{C}_{g}=\widetilde{O}(nd^{\omega-1}).\]

_The algorithm takes at most_

\[\widetilde{O}((d^{2}+dL^{2}R^{2})\log(w/\delta))\]

_Markov chain steps._

The Lee-Sidford barrier (Lee and Sidford, 2014, 2019) is the first polynomial-time computable barrier with a nearly-optimal self-concordance parameter. Several prior works (Chen et al., 2018; Laddha et al., 2020) utilize this barrier for uniform sampling. For log-concave sampling, the walk of Kook and Vempala (2024) requires strong-convexity-like assumption on \(f\) in order to attain an \(\widetilde{O}(d^{2})\) mixing time. Our work is the first to obtain an \(\widetilde{O}(d^{2})\) mixing time for log-concave sampling over polytopes, when \(L\) and \(R\) are small.

\begin{table}
\begin{tabular}{|l|l|l|} \hline
**References** & **Mixing time** & **Per iteration cost** \\ \hline Lovasz and Vempala (2006) & \(d^{2}R^{2}/r^{2}\) & \(nd\) \\ \hline Narayanan and Rakhlin (2017) & \(d^{5}+d^{3}L^{2}R^{2}\) & \(nd^{\omega-1}\) \\ \hline Mangoubi and Vishnoi (2023) & \(nd+dL^{2}R^{2}\) & \(nd^{\omega-1}\) \\ \hline Mangoubi and Vishnoi (2024) & \(nd+dL^{2}R^{2}\) & \(\mathrm{nnz}(A)+d^{2}\) \\ \hline This work & \(d^{2}+dL^{2}R^{2}\) & \(nd^{\omega-1}\) \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of algorithms for sampling from a log-concave density \(e^{-f}\) over a \(d\)-dimensional polytope with \(n\) constraints, where \(f\) is \(L\)-Lipschitz. We omit \(\widetilde{O}(\cdot)\) to mixing time and per iteration cost. We assume the evaluation of \(f\) can be done in unit time. The first row is the hit-and-run walk. Our algorithm has the fastest mixing time among all Dikin walks, and for \(L=O(1)\), \(r=O(1)\), our walk mixes faster than hit-and-run.

Our next result is closely related to semidefinite programming. Given the the set \(\mathcal{K}=\{x\in\mathbb{R}^{d}:\sum_{i=1}^{d}x_{i}A_{i}\succeq C\}\) for symmetric \(n\times n\) matrices \(A_{1},\ldots,A_{d},C\), we consider sampling from a log-concave distribution over \(\mathcal{K}\). We use the popular log-barrier for \(\mathcal{K}\) studied in Nesterov and Nemirovskii (1994); Jiang et al. (2020); Huang et al. (2022). To further utilize our robust sampling framework, we develop a novel approach for generating a spectral approximation of the Hessian. Compared to Jiang et al. (2020); Huang et al. (2022) in which the Hessian is maintained in a very careful way in conjunction with the interior point method, our spectral approximation does not require to control the changes over iterations and is highly efficient when \(n\gg d\), which is a popular regime for semidefinite programs.

**Theorem 1.3** (Robust sampling for log-concave distribution over spectrahedra).: _Let \(\delta\in(0,1)\) and \(R\geq 1\). Given a collection of symmetric matrices \(A_{1},\ldots,A_{d}\in\mathbb{R}^{n\times n}\) and a target symmetric matrix \(C\in\mathbb{R}^{n\times n}\), let \(\mathcal{K}:=\{x\in\mathbb{R}^{d}:\sum_{i=1}^{d}x_{i}A_{i}\succeq C\}\) be the corresponding spectrahedron. Suppose \(\mathcal{K}\) is enclosed in a ball of radius \(R\) with non-empty interior. Let \(f:\mathcal{K}\to\mathbb{R}\) be an \(L\)-Lipschitz, convex function with an evaluation oracle. Finally, let \(\pi\) be the distribution such that \(\pi\propto e^{-f}\)._

_Suppose we are given an initial point from \(\mathcal{K}\) that is sampled from a \(w\)-warm start distribution with respect to \(\pi\) for some \(w>0\), then there is an algorithm (Algorithm 1) that outputs a point from a distribution \(\mu\) where \(\mathrm{TV}(\mu,\pi)\leq\delta\)._

_Let \(g:\mathcal{K}\to\mathbb{R}\) be the logarithmic barrier function with \(\nu=n\). There is an algorithm that uses_

\[\widetilde{O}((nd+dL^{2}R^{2})\log(w/\delta))\]

_Markov chain steps. In each step it uses \(O(1)\) function evaluations and an extra cost of_

\[\widetilde{O}(n^{\omega}+n^{2}d^{3\omega-5}).\]

Prior work due to Narayanan and Rakhlin (2017) provides a walk that mixes in \(\widetilde{O}(n^{2}d^{3}+n^{2}dL^{2}R^{2})\) steps and each step could be implemented in \(O(dn^{\omega}+n^{2}d^{\omega-1})\) time, our algorithm improves upon both mixing and per iteration cost. On the front of per iteration cost, computing the Hessian of the log-barrier of \(\mathcal{K}\) exactly takes time \(O(dn^{\omega}+n^{2}d^{\omega-1})\), for the regime where \(n\gg d\) the dominating term is \(dn^{\omega}\). Note that whenever \(n\gg d^{\frac{3\omega-4}{\omega-2}}\), our algorithm is more efficient than that of exact computation. This is a common regime in semidefinite program in which the number of constraints \(d\) is small compared to the size of the primal solution.

## 2 Technical Overview

Before diving into our techniques, we first illustrate an informal version of our algorithm. The algorithm itself is a variant of the Dikin walk (Kannan and Narayanan, 2012) called the soft-threshold Dikin walk (Mangoubi and Vishnoi, 2023). Starting with a \(w\)-warm start point, the algorithm approximately computes the Hessian of the barrier function, then proposes a new point \(z\) from a Gaussian with mean at the current point \(x\) and variance \(\widetilde{\Phi}(x)^{-1}\), where \(\widetilde{\Phi}\) is a multiple of the approximate Hessian plus a multiple of the identity. The proposal is then accepted via a Metropolis filter. Note that compared to the standard Dikin walk framework, our algorithm crucially allows the Hessian to be approximated with a good precision. This means any improvements on generating spectral approximation leads to runtime improvement of our algorithm in a black-box fashion.

\begin{table}
\begin{tabular}{|l|l|l|} \hline
**References** & **Mixing time** & **Per iteration cost** \\ \hline Lovasz and Vempala (2006) & \(d^{2}R^{2}/r^{2}\) & \(n^{\omega}+n^{2}d\) \\ \hline Narayanan and Rakhlin (2017) & \(n^{2}d^{3}+n^{2}dL^{2}R^{2}\) & \(n^{\omega}d+n^{2}d^{\omega-1}\) \\ \hline This work & \(nd+dL^{2}R^{2}\) & \(n^{\omega}+n^{2}d^{3\omega-5}\) \\ \hline \end{tabular}
\end{table}
Table 2: Comparison of algorithms for sampling from a log-concave density \(e^{-f}\) over a \(d\)-dimensional spectrahedron with \(n\times n\) symmetric matrix constraints, where \(f\) is \(L\)-Lipschitz. We omit \(\widetilde{O}(\cdot)\) to mixing time and per iteration cost. We assume evaluation of \(f\) can be done in unit time. The first row is the hit-and-run walk. Our algorithm mixes faster than Narayanan and Rakhlin (2017) in all parameter regimes, and for \(L=O(1)\), \(r=O(1)\), \(d^{2}R^{2}\geq n\), our walk mixes faster than hit-and-run.

**Algorithm 1** Sampler for log-concave distribution \(\pi\propto\exp(-f)\) over convex body with a barrier function \(g\). \(\mathcal{A}\) is the description of the convex set, function \(f\) is an \(L\)-Lipschitz concave function and \(g\) is a \(\nu\)-self-concordant barrier function, \(x_{0}\in\mathcal{K}\) is a \(w\)-warm start point, and \(\delta\) is the total variance distance between our Markov chain and the target distribution. Let \(\mathcal{K}\subseteq\mathbb{R}^{d}\) enclosed by a ball of radius \(R\).

```
1:procedureSamplingConvexBody(\(\mathcal{A},f,g,x_{0},\delta\))
2: Initialize \(x\) to be a point in \(\mathcal{K}\)
3:\(T\leftarrow\widetilde{O}((\nu d+dL^{2}R^{2})\log(w/\delta))\)
4:for\(t=1\to T\)do
5: Sample a point \(\xi\sim\mathcal{N}(0,I_{d})\)
6: // \(H(x)\) is the exact Hessian matrix at \(x\), we never explicitly form it
7: Compute \(\widetilde{H}(x)\) where \((1-1/d)H(x)\preceq\widetilde{H}(x)\preceq(1+1/d)H(x)\)
8:\(\widetilde{\Phi}(x)=d\cdot\widetilde{H}(x)+dL^{2}\cdot I_{d}\)
9:\(z\gets x+\widetilde{\Phi}(x)^{-1/2}\xi\)
10:if\(z\in\operatorname{Int}(\mathcal{K})\)then
11: // \(H(z)\) is the exact Hessian matrix at \(z\)
12: Compute \(\widehat{H}(z)\) such that \((1-1/d)H(z)\preceq\widehat{H}(z)\preceq(1+1/d)H(z)\)
13:\(\widehat{\Phi}(z)=d\cdot\widehat{H}(z)+dL^{2}\cdot I_{d}\)
14:\(\tau\leftarrow\frac{\exp(-f(z))\cdot(\det(\widetilde{\Phi}(z)))^{1/2}\exp(-0.5 \|x-z\|_{\widetilde{\Phi}(z)}^{2})}{\exp(-f(x))\cdot(\det(\widetilde{\Phi}(x )))^{1/2}\cdot\exp(-0.5\|x-z\|_{\widetilde{\Phi}(z)}^{2})}\)
15: // Our walk is lazy, i.e., it only moves to a new point with half probability
16: accept\(x\gets z\) with probability \(0.5\cdot\min\{\tau,1\}\)
17:else
18: reject\(z\)
19:endif
20:endfor
21:endprocedure ```

**Algorithm 1** Sampler for log-concave distribution \(\pi\propto\exp(-f)\) over convex body with a barrier function \(g\). \(\mathcal{A}\) is the description of the convex set, function \(f\) is an \(L\)-Lipschitz concave function and \(g\) is a \(\nu\)-self-concordant barrier function, \(x_{0}\in\mathcal{K}\) is a \(w\)-warm start point, and \(\delta\) is the total variance distance between our Markov chain and the target distribution. Let \(\mathcal{K}\subseteq\mathbb{R}^{d}\) enclosed by a ball of radius \(R\).

### Soft-threshold Dikin walk for log-concave sampling via a barrier-based argument

For uniform sampling from structured convex bodies such as polytopes, Kannan and Narayanan (2012) introduce a refined ball walk that utilizes a _self-concordant barrier function_ associated with the convex body. To extend the Dikin walk framework for log-concave sampling, Narayanan and Rakhlin (2017) adds an additional coefficient \(\frac{\exp(-f(x))}{\exp(-f(z))}\) to the acceptance probability of Metropolis filter. This achieves a sub-optimal mixing time bound of \(\widetilde{O}(\nu^{2}d)\). By introducing a soft-threshold regularizer to the Hessian, Mangoubi and Vishnoi (2023) proves that for log-barrier function, it is possible to approach the optimality with an \(\widetilde{O}(nd+dL^{2}R^{2})\) mixing time. Unfortunately, the method used by Mangoubi and Vishnoi (2023) is specialized to log-barrier, making it hard to generalize to other barriers. They show that under proper scaling, the true Hessian \(H(x)\) under a sequence of polytopes converges to the regularized Hessian under the given polytope in the limit. They define the \(j\)-th polytope as

\[A_{j}=\begin{bmatrix}A\\ I_{d}\\ \vdots\\ I_{d}\end{bmatrix},b_{j}=\begin{bmatrix}b\\ j\cdot\mathbf{1}_{d}\\ \vdots\\ j\cdot\mathbf{1}_{d}\end{bmatrix}\]

the number of copies of \(I_{d}\) and \(j\cdot\mathbf{1}_{d}\) is a function of \(j\). While this polytope-based method works well for log-barrier, it functions poorly for more intricate barriers, such as volumetric barrier and Lee-Sidford barrier. Take volumetric barrier as an example, recall that \(H_{\mathrm{vol}}(x)=A^{\top}\Sigma(S(x)^{-1}A)S(x)^{-2}A\) where \(\Sigma(S(x)^{-1}A)\) is the statistical leverage score (Drineas et al., 2006; Spielman and Srivastava, 2011). Leverage score is a numerical quantity that measures _how important a row of a matrix is, compared to other rows_. This means that, if one duplicates a row infinitely many times, the row will be assigned a score of 0. More concretely, suppose we are given an \(n\times d\) matrix with \(n\) identical rows, then the leverage score of each row is \(\frac{d}{n}\). Taking \(n\rightarrow\infty\), it's easy to see that all rows have score 0, and the Hessian of volumetric barrier will zero out the rows contributed to theinfinitely many copies of identities. Thus, for volumetric barrier, extra constraints of the polytope sequence vanish. On the other hand, the Mangoubi and Vishnoi (2023) construction heavily relies on the infinitely many occurrences of \(I_{d}\) for it to converge to the regularized Hessian.

To circumvent this issue and provide a simpler argument, we develop a proof strategy that is _barrier-based_, instead of polytope-based. We example a class of popular barrier functions for polytopes that can be classified as _weighted log-barrier functions_(Vaidya, 1989). Roughly speaking, these functions take in the form of \(g(x)=-\sum_{i=1}^{n}w_{i}\log(S(x)_{i})\), with \(w=\mathbf{1}_{n}\) being the log-barrier, \(w=\sigma(S(x)^{-1}A)\) being the volumetric barrier (Vaidya, 1989) and if we choose \(w\) to be a proper power of the Lewis weights, then the barrier is precisely the Lee-Sidford barrier (Lee and Sidford, 2019; Laddha et al., 2020). These weights are stable, meaning that if two points are close in certain local norm induced by proper ellipses given by these weights, then their weights are also close (Lee and Sidford, 2019). This property proves to be particularly useful when proving the mixing rate of our Dikin walk. In addition, we carve out three sufficient conditions for a weighed log-barrier barrier function to work for log-concave distribution, and has \(\widetilde{O}(\nu d)\) mixing rate:

* \(\overline{\nu}\)**-symmetry.** The Hessian \(H(x)\) is \(\overline{\nu}\)-symmetry, that is, for any \(x\in\mathcal{K}\), \(E_{x}(1)\subseteq\mathcal{K}\cap(2x-\mathcal{K})\subseteq E_{x}(\sqrt{\overline {\nu}})\) where \(E_{x}(r)=\{y\in\mathbb{R}^{d}:(y-x)^{\top}H(x)(y-x)\leq r^{2}\}\). We show that \(\overline{\nu}=\nu\) for log-barrier, volumetric barrier and Lee-Sidford barrier for polytopes, and log-barrier for spectrahedra.
* **Bounded local norm.** The variance term \(\|H(x)^{-1/2}\cdot\nabla\log\det(H(x))\|_{2}^{2}\leq d\operatorname{poly}\log n\). We show this condition holds for all barriers of interest in this paper.
* **Convexity of regularized barrier.** The function \(F(x)=\log\det(H(x)+I_{d})\) is convex at \(x\) for any \(x\in\mathcal{K}\). We note that for log-barrier, volumetric barrier and Lee-Sidford barrier, it has been shown that \(\log\det(H(x))\) is indeed convex at \(x\). We further prove that this still holds when a copy of identity is blended in.

We want to highlight that our approach is more robust and generic than that of Mangoubi and Vishnoi (2023), as it solely depends on the structure of the Hessian matrix. Our argument should be treated as a generalization of Sachdeva and Vishnoi (2016), in which they utilize the bounded local norm and convexity of \(\log\det(H(x))\) for log-barrier, together with the Gaussianity to conclude that the difference between \(\log\det\) are small. We go beyond log-barrier for polytopes and uniform sampling.

### Approximation preserves the mixing time

The core premise of our algorithm is we allow the Hessian to be approximated, so that each step can be implemented efficiently. This poses significant challenges in proving mixing, as even if we have a good approximation of one-step in the Markov chain, the approximate chain does not necessarily mixes as fast as the original chain. For example, if every step is approximated within \(\epsilon\)-TV distance to the original Markov chain, then in \(T\) steps, the TV distance between the resulting distribution under approximate walk and the distribution under original walk could be as large as \(T\epsilon\). This means we have to take \(\epsilon\) very small (\(O(T_{\text{mix}}^{-1})\)) to guarantee same convergence property as original chain, which is unacceptable. A related issue is that the stationary distribution under approximate walk may not be the same as the target distribution. Therefore we need a way to control the mixing properties of the approximate walk. We resolve this problem by exactly computing the acceptance probability under approximate walk. That is, after proposing the next step \(z\) from \(x,\widehat{\Phi}(x)\), we sample \(\widehat{\Phi}(z)\), and accept with probability \(\min\{1,\frac{\pi(z)\widetilde{G}_{x}(z)}{\pi(x)\widetilde{G}_{x}(z)}\}\) (where \(\widetilde{G}_{x}(z)\) is the probability of proposing \(z\) starting from \(x\) under the approximate walk). Recall that in the exact walk, we accept with probability \(\min\{1,\frac{\pi(z)G_{x}(z)}{\pi(x)\widetilde{G}_{x}(z)}\}\) (where \(G_{x}(z)\) is the probability of proposing \(z\) stating from \(x\) under the exact walk). If we use this acceptability for the approximate walk, then we only have one-step approximation guarantee, and will suffer from the problem mentioned previously.

By using this modified acceptance probability, we can prove reversibility of the approximate walk, and that stationary distribution of the approximate walk is indeed our target distribution. For the mixing rate, we bound the conductance of the approximate walk, which requires us to prove the following properties:

1. The proposed step is accepted with decent probability.

2. Starting from two points moderately close to each other (i.e., \(\|x-z\|_{\Phi(x)}\leq\epsilon\)), the approximate steps taken \(P_{x}\) and \(P_{z}\) are close in TV distance (i.e., \(\mathrm{TV}(P_{x},P_{z})\leq 1-\epsilon^{\prime}\)) for some absolute constants \(\epsilon,\epsilon^{\prime}>0\).

We are able to prove these properties by comparison with the original chain. That is, assuming the original chain satisfies properties (1)(2), then the approximate chain also satisfies the same properties, as long as the approximation is good enough. This enables us to prove these key properties in a hassle-free way. To prove these comparison results, we need to prove, for example, \(\widetilde{G}_{x}(z)\approx G_{x}(z)\). Because both \(G_{x}(z)\) and \(\widetilde{G}_{x}(z)\) have nice factorizations

\[G_{x}(z) \propto\det(\Phi(x))^{1/2}\exp(-\frac{1}{2}\|x-z\|_{\Phi(x)}^{2}),\] \[\widetilde{G}_{x}(z) \propto\det(\widetilde{\Phi}(x))^{1/2}\exp(-\frac{1}{2}\|x-z\|_{ \widetilde{\Phi}(x)}^{2}),\]

we only need to prove \(\det(\Phi(x))\approx\det(\widetilde{\Phi}(x))\) and \(\exp(-\frac{1}{2}\|x-z\|_{\Phi(x)}^{2})\approx\exp(-\frac{1}{2}\|x-z\|_{ \widetilde{\Phi}(x)}^{2})\) separately. Both of these can be handled via \(\Phi(x)\approx\widetilde{\Phi}(x)\), using our approximation procedure.

### Approximation of Lewis weights for rapid mixing

Given our robust and generic Dikin walk framework, we realize it with the Lee-Sidford barrier whose complexity \(\nu=d\log^{5}n\). This ensures the walk mixes in \(\widetilde{O}(d^{2}+dL^{2}R^{2})\) steps, improving upon the log-barrier-based walk of Mangoubi and Vishnoi (2023, 2024) that mixes in \(\widetilde{O}(nd+dL^{2}R^{2})\) steps, as the log-barrier has complexity \(\nu=n\). To implement the Lee-Sidford barrier, it is imperative to compute the \(\ell_{p}\) (for \(p>0\)) Lewis weights, which is defined as the following convex program:

\[\min_{M\succeq 0}\ -\log\det M,\ \text{subject to}\ \sum_{i=1}^{n}(a_{i}^{ \top}Ma_{i})^{p/2}\leq d,\] (1)

and the \(\ell_{p}\) Lewis weights is a vector \(w_{p}\in\mathbb{R}^{d}\) where \((w_{p})_{i}=a_{i}^{\top}M_{*}a_{i}\) and \(M_{*}\) is the optimum of Program (1). Solving the program exactly is not efficient, and a long line of works (Cohen and Peng, 2015; Lee and Sidford, 2014, 2019; Cohen et al., 2019; Fazel et al., 2022; Jambulapati et al., 2022) provide fast algorithms that approximate all weights up to \((1\pm\epsilon)\)-factor. The Hessian of Lee-Sidford barrier has the form (up to scaling) \(H_{\mathrm{Lewis}}(x)=A^{\top}S(x)^{-1}W_{p}(S(x)^{-1}A)^{1-2/p}S(x)^{-1}A\) where \(W_{p}(S(x)^{-1}A)\) is the diagonal matrix of \(\ell_{p}\) Lewis weights with respect to \(S(x)^{-1}A\). When clear from context, we use \(W_{p}\) as a shorthand for \(W_{p}(S(x)^{-1}A)\). As \(W_{p}\) could be approximated in \(\widetilde{O}(nd^{\omega-1})\) time, we could then perform subsequent operations exactly to form the approximate Hessian. We instead provide a spectral approximation procedure that runs in \(\widetilde{O}(\mathrm{nnz}(A)+\mathcal{T}_{\mathrm{mat}}(d,d^{3},d))\) given the approximate Lewis weights where \(\mathcal{T}_{\mathrm{mat}}(a,b,c)\) is the time of multiplying an \(a\times b\) matrix with a \(b\times c\) matrix and \(\mathcal{T}_{\mathrm{mat}}(d,d^{3},d)\approx d^{4.2}\). This is important as even though it doesn't lead to direct runtime improvement, if approximate Lewis weights can be computed in \(\widetilde{O}(\mathrm{nnz}(A)+\mathrm{poly}(d))\) time, then we obtain a per iteration cost upgrade in a black-box fashion. Our approach is based on spectral sparsification via random sampling: given a matrix \(B\in\mathbb{R}^{n\times d}\), one can construct a matrix \(\widetilde{B}\in\mathbb{R}^{s\times d}\) consists of re-scaled rows of \(B\), such that \((1-\epsilon)\cdot B^{\top}B\preceq\widetilde{B}^{\top}\widetilde{B}\preceq(1+ \epsilon)\cdot B^{\top}B\). If we sample according to the leverage scores (Drinas et al., 2006; Spielman and Srivastava, 2011), one can set \(s=O(\epsilon^{-2}d\log d)\). In our case, we set \(B=W_{p}^{1/2-1/p}S(x)^{-1}A\), and perform the sampling. However, computing leverage score exactly is as expensive as forming the Hessian exactly, which requires \(O(nd^{\omega-1})\) time. To speed up the crucial step, we use randomized sketching technique to reduce the row count from \(n\) to \(\mathrm{poly}(d)\), then approximately estimate all leverage scores with this small matrix. One can either use the sparse embedding matrix (Nelson and Nguyen, 2013) to approximate these scores in time \(O(\mathrm{nnz}(A)\log n+\mathcal{T}_{\mathrm{mat}}(d,\epsilon^{-2}d,d))\), or use the subsampled randomized Hadamard transform (Lu et al., 2013) in time \(O(nd\log n+\mathcal{T}_{\mathrm{mat}}(d,\epsilon^{-2}d,d))\). We remark that approximating leverage scores for subsampling has many applications in numerical linear algebra, such as low rank approximation (Boutsidis et al., 2016; Song et al., 2017, 2019). Utilizing this framework, we provide an algorithm that approximates the Hessian of Lee-Sidford barrier in time \(\widetilde{O}(nd^{\omega-1})+\widetilde{O}(\mathrm{nnz}(A)+\mathcal{T}_{\mathrm{ mat}}(d,d^{3},d))\), as advertised.

### Approximation of log-barrier Hessian over a spectrahedron

For a spectrahedron induced by the dual semidefinite program, it is also natural to define its log-barrier with its corresponding Hessian being

\[H_{\log}(x)=\mathsf{A}(S(x)^{-1}\otimes S(x)^{-1})\mathsf{A}^{\top},\]

where \(\mathsf{A}\in\mathbb{R}^{d\times n^{2}}\) with the \(i\)-th row being \(\operatorname{vec}(A_{i})\) for \(n\times n\) symmetric matrix \(A_{i}\), and \(S(x)=\sum_{i=1}^{d}x_{i}A_{i}-C\) where \(C\) is also an \(n\times n\) symmetric matrix. Nesterov and Nemirovskii (1994) shows that by smartly arranging the organization of \(H_{\log}(x)\), it can be computed exactly in time \(O(dn^{\omega}+d^{\omega-1}n^{2})\). Beyond exact computation, Jiang et al. (2020); Huang et al. (2022) present algorithms that approximately _maintain_ the Hessian matrix under low-rank updates. These maintenance approaches are carried towards solving semidefinite programs in which the trajectory can be carefully controlled. For our soft-threshold Dikin walk, though the proposal generates a point that is relatively close to the starting point, due to its Gaussianity nature, much less structure can be exploited and thus maintained. Instead, we propose a maintenance-free approach that uses randomized sketching to generate a spectral approximation of \(H_{\log}(x)\).

To illustrate the algorithm, let's first define a matrix

\[\mathsf{B}=\mathsf{A}(S(x)^{-1/2}\otimes S(x)^{-1/2}).\]

It is not hard to see that \(H_{\log}(x)=\mathsf{B}\mathsf{B}^{\top}\). Due to the Kronecker product structure of \((S(x)^{-1/2}\otimes S(x)^{-1/2})\), it is natural to consider sketches for Kronecker product of matrices (Diao et al., 2018, 2019; Ahle et al., 2020; Song et al., 2021). Following the standard procedure for using sketch to generate spectral approximation, we can choose a TensorSRHT matrix (Ahle et al., 2020; Song et al., 2021)\(T\) and compute \(R:=(S(x)^{-1/2}\otimes S(x)^{-1/2})T\) then form \(\mathsf{A}R\). This is unfortunately, not efficient enough, as multiplying \(\mathsf{A}\) with \(R\) might be too slow. To further optimize the runtime efficiency, we use a more intrinsic approach based on the matrix \(T\) and \(\mathsf{B}\). It is well-known that the \(i\)-th row of \(\mathsf{B}\) can be first computed as \(S(x)^{-1/2}A_{i}S(x)^{-1/2}\) then vectorize. If we can manage to apply the sketch in a row-by-row fashion, then fast matrix multiplication can be utilized for even faster algorithms. We recall that \(T=P\cdot(HD_{1}\otimes HD_{2})\) where \(H\) is the Hadamard matrix and \(P\) is a row sampling matrix. To compute a row, we can first apply \(HD_{1}\) and \(HD_{2}\) individually to \(S(x)^{-1/2}\) in \(\widetilde{O}(n^{2})\) time since \(H\) is a Hadamard matrix. We can then form two matrices \(X\) and \(Y\) with rows being the corresponding sampled rows from \(HD_{1}\) and \(HD_{2}\). Finally, we compute \(XA_{i}Y^{\top}\) to form one row. Using fast matrix multiplication, we can form each row in \(\mathcal{T}_{\text{mat}}(d^{3},n,n)=O(n^{2}d^{3(\omega-2)})\) time. Applying this procedure to \(d\) rows leads to an overall \(O(n^{2}d^{3\omega-5})\) time, which beats the \(O(dn^{\omega})\) time as long as \(n\gg d\). Our runtime is somewhat slow when \(n\) is not that larger than \(d\), this is due to we have to set \(\epsilon=O(1/d)\). For more popular regimes where it suffices to set \(\epsilon=O(1)\), our algorithm outperforms exact computation as long as \(n\geq d\).

## 3 Applications

There are many applications of our Dikin walk, such as differentially private learning, simulated annealing and regret minimization. For a more comprehensive overview of the applications, we refer readers to Narayanan and Rakhlin (2017).

Differentially private learning.Let \(\mathcal{X}^{m}=\{x_{1},\ldots,x_{m}\}\) denote an \(m\)-point dataset, associate a convex loss function \(\ell_{i}\) to point \(x_{i}\), the learning problem attempts to find an optimal parameter \(\theta_{*}\in\mathcal{K}\) that minimizes \(\ell(\theta)=\sum_{i=1}^{m}\ell_{i}(\theta)\). We could further enforce _privacy_ constraints to learning: we say a randomized algorithm \(\widetilde{\mathcal{M}}:\mathcal{X}^{m}\to\mathbb{R}\) is \(\epsilon\)-DP if for any datasets \(X,X^{\prime}\in\mathcal{X}^{m}\) differ by a single point and for any \(S\subseteq\mathbb{R}\), \(\Pr[\mathcal{M}(X)\subseteq S]\leq e^{\epsilon}\Pr[\mathcal{M}(X^{\prime}) \subseteq S]\). The differentially private learning seeks to solve the learning problem under DP guarantees, and it has been shown that if one allows for _approximate DP_, then it can be achieved via the exponential mechanism and sampling from the distribution \(\pi(\theta)\propto e^{-\ell(\theta)}\)(Wang et al., 2015). If \(\mathcal{K}\) is a polytope or spectrahedron, and in addition \(\ell\) is \(L\)-Lipschitz, one can implement our walk to obtain an \(\widetilde{O}(d^{2}+dL^{2}R^{2})\) mixing for polytopes or \(\widetilde{O}(nd+dL^{2}R^{2})\) mixing for spectrahedra. Via a standard reduction from TV distance bound to infinity distance bound (Mangoubi and Vishnoi, 2022), our walk can also be adapted for a _pure DP_ guarantee (Lin et al., 2024).

Simulated annealing for convex optimization.Let \(f:\mathbb{R}^{d}\rightarrow\mathbb{R}\) be an \(L\)-Lipschitz and convex function, we consider the problem of minimizing \(f\) over \(\mathcal{K}\) when we can only access the function value of \(f\). This is also referred to as zeroth-order convex optimization. One could solve the problem via the _simulated annealing framework_, in which one needs to sample from the distribution \(\pi(x)\propto e^{-f(x)/T}\) where \(T\) is the temperature parameter (Kalai and Vempala, 2006). Our walk can be adapted to the framework for polytopes, it mixes in \(\widetilde{O}(d^{2.5}+d^{1.5}L^{2}R^{2})\) steps and for spectrahedra, it mixes in \(\widetilde{O}(nd^{1.5}+d^{1.5}L^{2}R^{2})\) steps, improving upon prior best algorithms that mix in \(\min\{d^{5.5}+d^{3.5}L^{2}R^{2},nd^{1.5}+d^{1.5}L^{2}R^{2}\}\) steps for polytopes and \(n^{2}d^{3.5}+n^{2}d^{1.5}L^{2}R^{2}\) steps for spectrahedra (Narayanan and Rakhlin, 2017; Mangoubi and Vishnoi, 2023).

Online convex optimization.Consider the following online optimization problem: let \(\mathcal{K}\) be a convex set that we could choose our actions from and let \(\ell_{1},\dots,\ell_{T}\) be a sequence of unknown convex cost functions with \(\ell_{t}:\mathcal{K}\rightarrow\mathbb{R}\). The goal is to design a good strategy that chooses from \(\mathcal{K}\) so that the total cost is small, compared to the offline optimal cost, in which one could choose the strategy after seeing the entire sequence of \(\ell_{t}\)'s. The algorithm works as follows: at round \(t\), we choose a distribution \(\mu_{t-1}\) supported on \(\mathcal{K}\) and play the action \(Y_{t}\sim\mu_{t-1}\). Our goal is to minimize the expected regret, defined as \(\mathrm{Reg}_{T}(U)=\mathbb{E}[\sum_{t=1}^{T}\ell_{t}(Y_{t})-\sum_{t=1}^{T}\ell _{t}(U)]\) with respect to all randomized strategies defined by distribution \(p_{U}\). It turns out if one sets \(s_{t}(x)=\eta\sum_{s=1}^{t}\ell_{t}(x)\) where \(\eta>0\) is a step size parameter and sets \(\mu_{t}\propto e^{-s_{t}}\), then this update rule reflects the multiplicative weights update algorithm (Arora et al., 2012). Moreover, one could prove a nearly-optimal regret bound of \(O(\sqrt{T})\) if the KL divergence between \(\mu_{0}\) and all possible \(p_{U}\)'s are bounded. The algorithmic problem of sampling from \(\mu_{t}\) is equivalent to the log-concave sampling problem we study in this paper, and we could efficiently generate strategies with good regress, similar to Narayanan and Rakhlin (2017). Let \(\ell_{t}\)'s be 1-Lipschitz linear functions, Narayanan and Rakhlin (2017) uses their time-varying Dikin walk to achieve a regret of \(O(d^{2.5}\sqrt{T})\), while we could significantly improve this bound to \(O(d\sqrt{T})\).

## 4 Conclusion

In this paper, we design a class of error-robust Dikin walks for sampling from a log-concave and log-Lipschitz density over a convex body. The key features of our walks are that their mixing time depends _linearly_ on the complexity of self-concordant barrier function of the convex body, and they allow computationally intensive quantities to be _approximated_ rather than computed exactly. For polytopes, our walk mixes in \(\widetilde{O}(d^{2}+dL^{2}R^{2})\) steps with a per iteration cost of \(\widetilde{O}(nd^{\omega-1})\), and for spectrahedra, our walk mixes in \(\widetilde{O}(nd+dL^{2}R^{2})\) steps with a per iteration cost of \(\widetilde{O}(n^{\omega}+n^{2}d^{3\omega-5})\). For polytopes, our walk is the first successful adaptation of the Lee-Sidford barrier for log-concave sampling under the minimal assumption that \(f\) is Lipschitz, improving upon the mixing of prior works (Narayanan and Rakhlin, 2017; Mangoubi and Vishnoi, 2023, 2024). For spectrahedra, we improve the mixing of Narayanan and Rakhlin (2017) from \(n^{2}d^{3}+n^{2}dL^{2}R^{2}\) to \(nd+dL^{2}R^{2}\), for the term that depends on only \(n\) and \(d\), we obtain an improvement of \(nd^{2}\), and for the term that depends on \(L\) and \(R\), we shave off the quadratic dependence on \(n\). Moreover, we adapt our error-robust framework and present a sketching algorithm for approximating the Hessian matrix in \(\widetilde{O}(n^{\omega}+n^{2}d^{3\omega-5})\) time. Our results have deep implications, as it could be leveraged for differentially private learning, convex optimization and regret minimization.

While our framework offers the fastest mixing Dikin walk for log-concave sampling, its mixing time has a rather unsatisfactory dependence on the radius of the bounding box, \(R\). It would be interesting if one could design a walk that _does not_ depend on \(R\). We also note that we require the density of be log-Lipschitz rather than Lipschitz. Kook and Vempala (2024) shows that if \(f\) in addition satisfies the _relative strongly-convex property_, then there exists a walk whose mixing does not depend on \(R\) and one only needs the density of be Lipschitz. Another important direction is to further improve the mixing rate of sampling over a spectrahedron using barrier functions such as volumetric barrier and hybrid barrier (Anstreicher, 2000), while maintaining an small per iteration cost. Finally, extending RHMC to log-concave sampling would be essentially, as it has the potential to give the fastest mixing walk by utilizing Riemannian metrics instead of Euclidean. As our work is theoretical in nature, we don't foresee any potential negative societal impact. It has the potential positive impact to reduce energy consumption and carbon emission when deployed in practice.

## Acknowledgement

We would like to thank Jonathan Kelner, Yin Tat Lee, Santosh Vempala and Yunbum Kook for valuable discussions and anonymous reviewers for helpful comments. The research is partially supported by the NSF awards: SCALE MoDL-2134209, CCF-2112665 (TILOS). It is also supported by the U.S. Department of Energy, the Office of Science, the Facebook Research Award, as well as CDC-RFA-FT-23-0069 from the CDC's Center for Forecasting and Outbreak Analytics. Lichen Zhang is supported by NSF awards CCF-1955217 and DMS-2022448. For more information related to the paper and adjacent topics, see https://www.youtube.com/@zhaosong2031 and https://space.bilibili.com/3546587376650961.

## References

* Ahle et al. (2020) Thomas D Ahle, Michael Kapralov, Jakob BT Knudsen, Rasmus Pagh, Ameya Velingker, David P Woodruff, and Amir Zandieh. Oblivious sketching of high-degree polynomial kernels. In _Proceedings of the Fourteenth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 141-160, 2020.
* Alman and Williams (2021) Josh Alman and Virginia Vassilevska Williams. A refined laser method and faster matrix multiplication. In _Proceedings of the 2021 ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 522-539. SIAM, 2021.
* Anstreicher (2000) Kurt M Anstreicher. The volumetric barrier for semidefinite programming. _Mathematics of Operations Research_, 25(3):365-380, 2000.
* Arora et al. (2012) Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta-algorithm and applications. _Theory of Computing_, 8(1):121-164, 2012.
* Boutsidis et al. (2016) Christos Boutsidis, David P Woodruff, and Peilin Zhong. Optimal principal component analysis in distributed and streaming models. In _Proceedings of the forty-eighth annual ACM symposium on Theory of Computing (STOC)_, pages 236-249, 2016.
* Brosse et al. (2018) Nicolas Brosse, Alain Durmus, and Eric Moulines. The promises and pitfalls of stochastic gradient langevin dynamics. _Advances in Neural Information Processing Systems_, 31, 2018.
* Chatterji et al. (2018) Niladri Chatterji, Nicolas Flammarion, Yian Ma, Peter Bartlett, and Michael Jordan. On the theory of variance reduction for stochastic gradient monte carlo. In _International Conference on Machine Learning_, pages 764-773. PMLR, 2018.
* Chen and Eldan (2022) Yuansi Chen and Ronen Eldan. Hit-and-run mixing via localization schemes, 2022.
* Chen et al. (2018) Yuansi Chen, Raaz Dwivedi, Martin J. Wainwright, and Bin Yu. Fast mcmc sampling algorithms on polytopes. _J. Mach. Learn. Res._, 2018.
* Cohen and Peng (2015) Michael B. Cohen and Richard Peng. Lp row sampling by lewis weights. In _Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing_, STOC '15, 2015.
* Cohen et al. (2019) Michael B Cohen, Ben Cousins, Yin Tat Lee, and Xin Yang. A near-optimal algorithm for approximating the john ellipsoid. In _Conference on Learning Theory_, pages 849-873. PMLR, 2019.
* Dalalyan and Karagulyan (2019) Arnak S Dalalyan and Avetik Karagulyan. User-friendly guarantees for the langevin monte carlo with inaccurate gradient. _Stochastic Processes and their Applications_, 129(12):5278-5311, 2019.
* Defazio et al. (2014) Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. _Advances in neural information processing systems_, 27, 2014.
* Diao et al. (2018) Huaian Diao, Zhao Song, Wen Sun, and David Woodruff. Sketching for kronecker product regression and p-splines. In _International Conference on Artificial Intelligence and Statistics_, pages 1299-1308. PMLR, 2018.
* Diao et al. (2019)Huaian Diao, Rajesh Jayaram, Zhao Song, Wen Sun, and David Woodruff. Optimal sketching for kronecker product regression and low rank approximation. _Advances in neural information processing systems_, 32:4737-4748, 2019.
* Ding and Li (2021) Zhiyan Ding and Qin Li. Langevin monte carlo: random coordinate descent and variance reduction. _J. Mach. Learn. Res._, 22:205-1, 2021.
* Drineas et al. (2006) Petros Drineas, Michael W. Mahoney, and S. Muthukrishnan. Sampling algorithms for l2 regression and applications. In _Proceedings of the Seventeenth Annual ACM-SIAM Symposium on Discrete Algorithm_, SODA '06, page 1127-1136, USA, 2006. Society for Industrial and Applied Mathematics.
* Duan et al. (2023) Ran Duan, Hongxun Wu, and Renfei Zhou. Faster matrix multiplication via asymmetric hashing. In _FOCS_, 2023.
* Dubey et al. (2016) Kumar Avinava Dubey, Sashank J Reddi, Sinead A Williamson, Barnabas Poczos, Alexander J Smola, and Eric P Xing. Variance reduction in stochastic gradient langevin dynamics. _Advances in neural information processing systems_, 29, 2016.
* Fazel et al. (2022) Maryam Fazel, Yin Tat Lee, Swati Padmanabhan, and Aaron Sidford. Computing lewis weights to high precision. In _Proceedings of the 2022 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)_, pages 2723-2742. SIAM, 2022.
* Furedi and Barany (1986) Z Furedi and I Barany. Computing the volume is difficult. In _Proceedings of the Eighteenth Annual ACM Symposium on Theory of Computing_, STOC '86, page 442-447, New York, NY, USA, 1986. Association for Computing Machinery.
* Le Gall and Urrutia (2018) Francois Le Gall and Florent Urrutia. Improved rectangular matrix multiplication using powers of the coppersmith-winograd tensor. In _Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA '18, page 1029-1046, 2018.
* Le Gall (2024) Francois Le Gall. Faster rectangular matrix multiplication by combination loss analysis. In _Proceedings of the Thirty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA'24, 2024.
* Gatmiry et al. (2024) Khashayar Gatmiry, Jonathan Kelner, and Santosh S. Vempala. Sampling polytopes with riemannian hmc: Faster mixing via the lewis weights barrier. In Shipra Agrawal and Aaron Roth, editors, _Proceedings of Thirty Seventh Conference on Learning Theory_, volume 247 of _Proceedings of Machine Learning Research_, pages 1796-1881. PMLR, 30 Jun-03 Jul 2024.
* Horn and Johnson (1990) Roger A. Horn and Charles R. Johnson. _Matrix Analysis_. Cambridge University Press, 1990.
* Huang et al. (2022) Baihe Huang, Shunhua Jiang, Zhao Song, Runzhou Tao, and Ruizhe Zhang. Solving sdp faster: A robust ipm framework and efficient implementation. In _FOCS_, 2022.
* Jambulapati et al. (2022) Arun Jambulapati, Yang P. Liu, and Aaron Sidford. Improved iteration complexities for overconstrained p-norm regression. In _Proceedings of the 54th Annual ACM SIGACT Symposium on Theory of Computing_, STOC 2022, 2022.
* Jiang et al. (2020a) Haotian Jiang, Tarun Kathuria, Yin Tat Lee, Swati Padmanabhan, and Zhao Song. A faster interior point method for semidefinite programming. In _2020 IEEE 61st annual symposium on foundations of computer science (FOCS)_, pages 910-918. IEEE, 2020a.
* Jiang et al. (2020b) Haotian Jiang, Yin Tat Lee, Zhao Song, and Sam Chiu-wai Wong. An improved cutting plane method for convex optimization, convex-concave games, and its applications. In _Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing_, pages 944-953, 2020b.
* Jiang et al. (2021) Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. A faster algorithm for solving general lps. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 823-832, 2021.
* Johnson and Zhang (2013) Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. _Advances in neural information processing systems_, 26, 2013.
* Johnson et al. (2019)William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space. _Contemporary mathematics_, 26(189-206):1, 1984.
* Kalai and Vempala (2006) Adam Tauman Kalai and Santosh Vempala. Simulated annealing for convex optimization. _Mathematics of Operations Research_, 31(2):253-266, 2006.
* Kannan et al. (1995) R. Kannan, L. Lovasz, and M. Simonovits. Isoperimetric problems for convex bodies and a localization lemma. _Discrete Comput. Geom._, 1995.
* Kannan and Narayanan (2012) Ravindran Kannan and Hariharan Narayanan. Random walks on polytopes and an affine interior point method for linear programming. _Mathematics of Operations Research_, 37(1):1-20, 2012.
* Kook and Vempala (2024) Yunbum Kook and Santosh S. Vempala. Gaussian cooling and Dikin walks: The interior-point method for logconcave sampling. In Shipra Agrawal and Aaron Roth, editors, _Proceedings of Thirty Seventh Conference on Learning Theory_, volume 247 of _Proceedings of Machine Learning Research_, pages 3137-3240. PMLR, 30 Jun-03 Jul 2024.
* Laddha et al. (2020) Aditi Laddha, Yin Tat Lee, and Santosh Vempala. Strong self-concordance and sampling. In _Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC)_, pages 1212-1222, 2020.
* Laurent and Massart (2000) Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selection. _Annals of Statistics_, pages 1302-1338, 2000.
* Lee and Sidford (2014) Yin Tat Lee and Aaron Sidford. Path finding methods for linear programming: Solving linear programs in \(\mathfrak{o}(\text{sqrt}(\text{rank}))\) iterations and faster algorithms for maximum flow. In _55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014_, pages 424-433, 2014.
* Lee and Sidford (2019) Yin Tat Lee and Aaron Sidford. Solving linear programs with sqrt (rank) linear system solves. _arXiv preprint arXiv:1910.08033_, 2019.
* Lee and Vempala (2017) Yin Tat Lee and Santosh S. Vempala. Geodesic walks in polytopes. In _Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing_, STOC 2017, 2017.
* Lee and Vempala (2018) Yin Tat Lee and Santosh S Vempala. Convergence rate of riemannian hamiltonian monte carlo and faster polytope volume computation. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing_, pages 1115-1121, 2018.
* Lee and Yue (2021) Yin Tat Lee and Man-Chung Yue. Universal barrier is \(n\)-self-concordant. _Mathematics of Operations Research_, 46(3):1129-1148, 2021.
* Lee et al. (2015) Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its implications for combinatorial and convex optimization. In _Foundations of Computer Science (FOCS), 2015 IEEE 56th Annual Symposium on_, pages 1049-1065. IEEE, 2015.
* Lee et al. (2019) Yin Tat Lee, Zhao Song, and Qiuyi Zhang. Solving empirical risk minimization in the current matrix multiplication time. In _Conference on Learning Theory_, pages 2140-2157. PMLR, 2019.
* Li et al. (2019) Zhize Li, Tianyi Zhang, Shuyu Cheng, Jun Zhu, and Jian Li. Stochastic gradient hamiltonian monte carlo with variance reduction for bayesian inference. _Machine Learning_, 108(8):1701-1727, 2019.
* Lin et al. (2024) Yingyu Lin, Yian Ma, Yu-Xiang Wang, Rachel Emily Redberg, and Zhiqi Bu. Tractable MCMC for private learning with pure and gaussian differential privacy. In _The Twelfth International Conference on Learning Representations_, 2024.
* Lovasz and Simonovits (1993) Laszlo Lovasz and Miklos Simonovits. Random walks in a convex body and an improved volume algorithm. _Random structures & algorithms_, 4(4):359-412, 1993.
* Lovasz and Vempala (2003) Laszlo Lovasz and Santosh Vempala. Hit-and-run is fast and fun. _preprint, Microsoft Research_, 2003.
* Lovasz and Vempala (2006) Laszlo Lovasz and Santosh Vempala. Simulated annealing in convex bodies and an \(O^{*}(n^{4})\) volume algorithm. _Journal of Computer and System Sciences_, 72(2):392-417, 2006.
* Lovasz and Vempala (2018)Laszlo Lovasz and Santosh Vempala. The geometry of logconcave functions and sampling algorithms. _Random Structures & Algorithms_, 30(3):307-358, 2007.
* Lu et al. (2013) Yichao Lu, Paramveer Dhillon, Dean P Foster, and Lyle Ungar. Faster ridge regression via the subsampled randomized hadamard transform. In _Advances in neural information processing systems_, pages 369-377, 2013.
* Mangoubi and Vishnoi (2022) Oren Mangoubi and Nisheeth Vishnoi. Sampling from log-concave distributions with infinity-distance guarantees. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 12633-12646. Curran Associates, Inc., 2022.
* Mangoubi and Vishnoi (2023) Oren Mangoubi and Nisheeth K Vishnoi. Sampling from structured log-concave distributions via a soft-threshold dikin walk. In _Advances in Neural Information Processing Systems_, NeurIPS'23, 2023.
* Mangoubi and Vishnoi (2024) Oren Mangoubi and Nisheeth K. Vishnoi. Faster sampling from log-concave densities over polytopes via efficient linear solvers. In _The Twelfth International Conference on Learning Representations_, 2024.
* Narayanan (2016) Hariharan Narayanan. Randomized interior point methods for sampling and optimization. _Ann. Appl. Probab._, 2016.
* Narayanan and Rakhlin (2017) Hariharan Narayanan and Alexander Rakhlin. Efficient sampling from time-varying log-concave distributions. _The Journal of Machine Learning Research_, 18(1):4017-4045, 2017.
* Nelson and Nguyen (2013) Jelani Nelson and Huy L Nguyen. OSNAP: Faster numerical linear algebra algorithms via sparser subspace embeddings. In _54th Annual IEEE Symposium on Foundations of Computer Science (FOCS)_, pages 117-126. IEEE, 2013.
* Nesterov and Nemirovskii (1989) Yurii Nesterov and Arkadii Nemirovskii. Self-concordant functions and polynomial-time methods in convex programming. _USSR Academy of Sciences_, 1989.
* Nesterov and Nemirovskii (1994) Yurii Nesterov and Arkadii Nemirovskii. _Interior-point polynomial algorithms in convex programming_. SIAM, 1994.
* Qin et al. (2023) Lianke Qin, Zhao Song, Lichen Zhang, and Danyang Zhuo. An online and unified algorithm for projection matrix vector multiplication with application to empirical risk minimization. In _AISTATS_, 2023.
* Raginsky et al. (2017) Maxim Raginsky, Alexander Rakhlin, and Matus Telgarsky. Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis. In _Conference on Learning Theory_, pages 1674-1703. PMLR, 2017.
* Renegar (1988) James Renegar. A polynomial-time algorithm, based on newton's method, for linear programming. _Math. Program._, 1988.
* Sachdeva and Vishnoi (2016) Sushant Sachdeva and Nisheeth K Vishnoi. The mixing time of the dikin walk in a polytope--a simple proof. _Operations Research Letters_, 44(5):630-634, 2016.
* Shalev-Shwartz and Zhang (2013) Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. _Journal of Machine Learning Research_, 14(1), 2013.
* Song and Yu (2021) Zhao Song and Zheng Yu. Oblivious sketching-based central path method for solving linear programming problems. In _38th International Conference on Machine Learning (ICML)_, 2021.
* Song et al. (2017) Zhao Song, David P Woodruff, and Peilin Zhong. Low rank approximation with entrywise l1-norm error. In _Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing_, pages 688-701, 2017.
* Song et al. (2019) Zhao Song, David P Woodruff, and Peilin Zhong. Relative error tensor low rank approximation. In _Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms_, pages 2772-2789. SIAM, 2019.
* Song et al. (2019)* Song et al. (2021) Zhao Song, David Woodruff, Zheng Yu, and Lichen Zhang. Fast sketching of polynomial kernels of polynomial degree. In _International Conference on Machine Learning_, pages 9812-9823. PMLR, 2021.
* Spielman and Srivastava (2011) Daniel A Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. _SIAM Journal on Computing_, 40(6):1913-1926, 2011.
* Tropp (2011) Joel A Tropp. Improved analysis of the subsampled randomized hadamard transform. _Advances in Adaptive Data Analysis_, 3(01n02):115-126, 2011.
* Vaidya (1989) Pravin M Vaidya. A new algorithm for minimizing convex functions over convex sets. In _30th Annual Symposium on Foundations of Computer Science (FOCS)_, pages 338-343. IEEE Computer Society, 1989.
* Vempala (2005) Santosh Vempala. Geometric random walks: a survey. _Combinatorial and computational geometry_, 52(573-612):2, 2005.
* Wang et al. (2015) Yu-Xiang Wang, Stephen Fienberg, and Alex Smola. Privacy for free: Posterior sampling and stochastic gradient monte carlo. In Francis Bach and David Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 2493-2502, Lille, France, 07-09 Jul 2015. PMLR.
* Williams (2012) Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd. In _Proceedings of the forty-fourth annual ACM symposium on Theory of computing (STOC)_, pages 887-898. ACM, 2012.
* Williams et al. (2024) Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. New bounds for matrix multiplication: from alpha to omega. In _Proceedings of the Thirty-Fifth Annual ACM-SIAM Symposium on Discrete Algorithms_, SODA'24, 2024.
* Woodruff (2014) David P. Woodruff. Sketching as a tool for numerical linear algebra. _Foundations and Trends in Theoretical Computer Science_, 10(1-2):1-157, 2014.
* Zou et al. (2018) Difan Zou, Pan Xu, and Quanquan Gu. Stochastic variance-reduced hamilton monte carlo methods. In _International Conference on Machine Learning_, pages 6028-6037. PMLR, 2018.

## Appendix

Roadmap.Since there are many technical details in the appendix, we provide a roadmap. The appendix can be divided into 5 parts: the first part conveys some preliminary information and states the sufficient conditions we require barrier functions to have, in Section A and B. The second part proves the mixing time of the Dikin walk when barrier functions satisfy the conditions, and the proofs are divided into Section C and D. The next part focuses on the runtime complexity of sampling from polytopes, including how to generate a spectral approximation of the Hessian, approximate leverage scores and Lewis weights to high precision, in Section E and how to incorporate these algorithmic prototypes to implement an efficient sampling algorithm in Section F. We dedicate Section G to study the log-barrier for sampling from a spectrahedron, as it is relatively less explored before. Finally, we prove the convexity of the function \(\log\det(H(x)+I_{d})\) for log-barrier, volumetric barrier and in extension Lee-Sidford barrier in Section H and I.

## Appendix A Preliminaries

For any positive integer, we use \([n]\) to denote the set \(\{1,2,\cdots,n\}\). For a vector \(x\in\mathbb{R}^{n}\), we use \(\|x\|_{2}\) to denote its \(\ell_{2}\) norm, i.e., \(\|x\|_{2}:=(\sum_{i=1}^{n}x_{i}^{2})^{1/2}\). We use \(\|x\|_{1}\) to denote its \(\ell_{1}\) norm, \(\|x\|_{1}:=\sum_{i=1}^{n}|x_{i}|\). We use \(\|x\|_{\infty}\) to denote its \(\ell_{\infty}\) norm, i.e., \(\|x\|_{\infty}:=\max_{i\in[n]}|x_{i}|\). For a random variable \(X\), we use \(\mathbb{E}[X]\) to denote its expectation. We use \(\Pr[\cdot]\) to denote probability.

We use \(\mathbf{0}_{d}\) to denote a length-\(d\) vector where every entry is \(0\). We use \(\mathbf{1}_{d}\) to denote a length-\(d\) vector where every entry is \(1\). We use \(I_{d}\) to denote an identity matrix which has size \(d\times d\) or simply \(I\) when dimension is clear from context.

For a matrix \(A\), we use \(A^{\top}\) to denote the transpose of matrix \(A\). For a square and non-singular matrix \(A\), we use \(A^{-1}\) to denote the inverse of matrix \(A\). For a real square matrix \(A\), we say it is positive definite (PD, i.e., \(A\succ 0\)) if for all vectors \(x\in\mathbb{R}^{n}\) (except for \(\mathbf{0}_{n}\)), we have \(x^{\top}Ax>0\). For a real square matrix \(A\), we say it is positive semi-definite(PSD, i.e., \(A\succeq 0\)) if for all vectors \(x\in\mathbb{R}^{n}\), we have \(x^{\top}Ax\geq 0\). For a square matrix, we use \(\det(A)\) to denote the determinant of matrix \(A\). For a matrix \(A\), we use \(\|A\|\) to denote its spectral norm, use \(\|A\|_{F}\) to denote its Frobenius norm, i.e., \(\|A\|_{F}:=(\sum_{i=1}^{n}\sum_{j=1}^{d}A_{i,j}^{2})^{1/2}\) and use \(\operatorname{nnz}(A)\) to denote the number of nonzero entries in \(A\).

Given a function \(f:\mathcal{K}\to\mathbb{R}\), we say it's convex if for any \(x,y\in\mathcal{K}\), \(f(x)\geq f(y)+\nabla f(y)^{\top}(x-y)\). We say it's \(L\)-Lipschitz if \(|f(x)-f(y)|\leq L\cdot\|x-y\|_{2}\) for a fixed parameter \(L>0\).

For two distributions \(P_{1}\) and \(P_{2}\), we use \(\operatorname{TV}(P_{1},P_{2})\) to denote the total variation (TV) distance between \(P_{1}\) and \(P_{2}\).

For a convex body \(\mathcal{K}\subseteq\mathbb{R}^{d}\), we use \(\operatorname{Int}(\mathcal{K})\) to denote the interior of \(\mathcal{K}\).

We use \(\otimes\) to denote Kronecker product. We use \(A\otimes_{S}B\) to denote \(A\otimes I+I\otimes B\). Given a matrix \(A\), we use \(\operatorname{vec}(A)\) to denote its vectorization, since we will always apply \(\operatorname{vec}(\cdot)\) to a symmetric matrix, whether the vectorization is row-major or column-major doesn't matter. We use \(\circ\) to denote Hadamard or element-wise product.

For two vectors \(x,y\in\mathbb{R}^{d}\), we use \(\langle x,y\rangle=x^{\top}y\) to denote the standard Euclidean inner product over \(\mathbb{R}^{d}\), and for two symmetric matrices \(A,B\in\mathbb{R}^{d\times d}\), we use \(\langle A,B\rangle=\operatorname{tr}[A^{\top}B]\) to denote the trace inner product.

In Section A.1, we present several definitions and notations related to fast matrix multiplication. In Section A.2, we provide some backgrounds about convex geometry. In Section A.3, we state several basic probability tools. In Section A.4, we state several basic facts related to trace, and Kronecker product. In Section A.5, we present the standard notion about self-concordance barrier.

### Fast matrix multiplication

**Definition A.1** (Fast matrix multiplication).: _Given three positive integers \(a,b,c\), we use \(\mathcal{T}_{\mathrm{mat}}(a,b,c)\) to denote the time of multiplying an \(a\times b\) matrix with another \(b\times c\) matrix._

For convenience, we also define the \(\omega(\cdot,\cdot,\cdot)\) function as follows:

**Definition A.2** (Fast matrix multiplication, an alternative notation).: _Given \(x,y,z\), we use \(d^{\omega(x,y,z)}\) to denote the time of multiplying a \(d^{x}\times d^{y}\) matrix with another \(d^{y}\times d^{z}\) matrix._

**Lemma A.3** (Gall and Urrutia (2018)).: _We have the following bounds of \(\omega(\cdot,\cdot,\cdot)\):_

* \(\omega(1,1,1)=\omega\)_,_
* \(\omega(1,3,1)=4.199712\) _(see Table_ 3 _in Gall and Urrutia_ _(_2018_)__)._

Here, \(\omega\) denotes the exponent of matrix multiplication, currently \(\omega\approx 2.373\)(Williams, 2012; Gall and Urrutia, 2018; Alman and Williams, 2021; Duan et al., 2023; Williams et al., 2024; Gall, 2024).

### Convex geometry

We define \(B(x,R)\subset\mathbb{R}^{d}\) as

\[B(x,R):=\{y\in\mathbb{R}^{d}\;:\;\|y-x\|_{2}\leq R\}.\]

Define polytope \(\mathcal{K}\) as

\[\mathcal{K}:=\{x\in\mathbb{R}^{d}\;:\;Ax\leq b\}.\]

Define spectrahedron \(\mathcal{K}\) as

\[\mathcal{K}:=\{x\in\mathbb{R}^{d}\;:\;\sum_{i=1}^{d}x_{i}A_{i}\succeq C\},\]

where \(A_{1},\ldots,A_{d},C\in\mathbb{R}^{n\times n}\) are symmetric matrices.

We will often use the notion of a _Dikin ellipsoid_:

**Definition A.4** (Dikin ellipsoid).: _We define the Dikin ellipsoid \(D_{\theta}\subset\mathbb{R}^{d}\) as follows_

\[D_{\theta}:=\{w\in\mathbb{R}^{d}\;:\;w^{\top}H^{-1}(\theta)w\leq 1\}\]

_where \(H(\theta)\in\mathbb{R}^{d\times d}\) is the Hessian of self-concordant barrier at \(\theta\)/_

We define the standard cross-ratio distance (see Definition E.1 in Mangoubi and Vishnoi (2023) for example).

**Definition A.5** (Cross-ratio distance).: _Let \(u,v\in\mathcal{K}\). If \(u\neq v\), let \(p,q\) be two endpoints of the chord in \(\mathcal{K}\) which passes through \(u\) and \(v\) such that the four points lie in the order of \(p,u,v,q\), let_

\[\sigma(u,v):=\frac{\|u-v\|_{2}\cdot\|p-q\|_{2}}{\|p-u\|_{2}\cdot\|v-q\|_{2}}.\]

_We additionally set \(\sigma(u,v)=0\) for \(u=v\)._

_For two subsets \(S_{1},S_{2}\subseteq\mathcal{K}\), we define_

\[\sigma(S_{1},S_{2}):=\min\{\sigma(u,v):u\in S_{1},v\in S_{2}\}.\]

We state the standard isoperimetric inequality for cross-ratio distance.

**Lemma A.6** (Lovasz and Vempala (2003)).: _Let \(\pi:\mathbb{R}^{d}\to\mathbb{R}\) be a log-concave density, with support on a convex body \(\mathcal{K}\). Then for any partition of \(\mathbb{R}^{d}\) into measurable sets \(S_{1},S_{2},S_{3}\), the induced measure \(\pi^{*}\) satisfies_

\[\pi^{*}(S_{3})\geq\sigma(S_{1},S_{2})\cdot\pi^{*}(S_{1})\cdot\pi^{*}(S_{2}).\]

### Probability tools

We state some useful concentration inequalities.

**Lemma A.7** (Lemma 1 on page 1325 of Laurent and Massart (2000)).: _Let \(X\sim\mathcal{X}_{k}^{2}\) be a chi-squared distributed random variable with \(k\) degrees of freedom. Each random variable has zero mean and \(\sigma^{2}\) variance. Then_

\[\Pr[X-k\sigma^{2}\geq(2\sqrt{kt}+2t)\sigma^{2}] \leq\exp(-t),\] \[\Pr[k\sigma^{2}-X\geq 2\sqrt{kt}\sigma^{2}] \leq\exp(-t).\]

**Lemma A.8** (Matrix Chernoff bound (Tropp, 2011)).: _Let \(X_{1},\ldots,X_{s}\) be independent copies of a symmetric random matrix \(X\in\mathbb{R}^{d\times d}\) with \(\mathbb{E}[X]=0\), \(\|X\|\leq\gamma\) almost surely and \(\|\,\mathbb{E}[X^{\top}X]\|\leq\sigma^{2}\). Let \(W=\frac{1}{s}\sum_{i\in[s]}X_{i}\). For any \(\epsilon\in(0,1)\),_

\[\Pr[\|W\|\geq\epsilon]\leq 2d\cdot\exp\left(-\frac{s\epsilon^{2}}{\sigma^{2}+ \gamma\epsilon/3}\right).\]

### Basic algebra facts

**Lemma A.9**.: _Let \(A\) and \(B\) be \(m\times m\) matrices. Then we have_

* \(\mathrm{tr}[AB]=\mathrm{tr}[BA]\)_._
* _If_ \(A\) _is symmetric, then_ \(\mathrm{tr}[AB]=\mathrm{tr}[AB^{\top}]\)_._
* _If_ \(A\) _and_ \(B\) _are PSD, then_ \(\langle A,B\rangle\geq 0\)_, and_ \(\langle A,B\rangle=0\) _if and only if_ \(AB=\mathbf{0}_{m\times m}\)_._
* _If_ \(A\succeq 0\) _and_ \(B\succeq C\)_, then_ \(\langle A,B\rangle\geq\langle A,C\rangle\)_._

**Lemma A.10**.: _Let \(A,B,C,D\) be conforming matrices. Then_

* \((A\otimes B)(C\otimes D)=AC\otimes BD\)_._
* \((A\otimes_{S}B)(C\otimes_{S}D)=\frac{1}{2}(AC\otimes_{S}BD+AD\otimes_{S}BC)\)_._
* \((A\otimes B)^{\top}=A^{\top}\otimes B^{\top}\)_._
* _If_ \(A\) _and_ \(B\) _are non-singular, then_ \(A\otimes B\) _is non-singular, and_ \((A\otimes B)^{-1}=A^{-1}\otimes B^{-1}\)_._
* \(\mathrm{vec}(ABC)=(C^{\top}\otimes A)\cdot\mathrm{vec}(B)\)_._

### Self-concordant barrier

We provide a standard definition about self-concordance barrier,

**Definition A.11** (Self-concordant barrier).: _A real-valued function \(F:\mathrm{Int}(\mathcal{K})\to\mathbb{R}\), is a regular self-concordant barrier if it satisfies the conditions stated below. For convenience, if \(x\not\in\mathrm{Int}(\mathcal{K})\), we define \(F(x)=\infty\)._

1. _(Convex, Smooth)_ \(F\) _is a convex_ _three continuously differentiable function on_ \(\mathrm{Int}(\mathcal{K})\)_._
2. _(Barrier) For every sequence of points_ \(\{x_{i}\}\in\mathrm{Int}(\mathcal{K})\) _converging to a point_ \(x\not\in\mathrm{Int}(\mathcal{K})\)_,_ \(\lim_{i\to\infty}f(x_{i})=\infty\)_._
3. _(Differential Inequalities) For all_ \(h\in\mathbb{R}^{d}\) _and all_ \(x\in\mathrm{Int}(\mathcal{K})\)_, the following inequalities hold._ 1. \(D^{2}F(x)[h,h]\) _is_ \(2\)_-Lipschitz continuous with respect to the local norm, which is equivalent to_ \[D^{3}F(x)[h,h,h]\leq 2(D^{2}F(x)[h,h])^{\frac{3}{2}}.\] 2. \(F(x)\) _is_ \(\nu\)_-Lipschitz continuous with respect to the local norm defined by_ \(F\)_,_ \[\|\nabla_{h}F(x)\|_{2}^{2}\leq\nu\cdot\|h\|_{H(x)}^{2}.\] _We call the smallest positive integer_ \(\nu\) _for which this holds,_ the self-concordance parameter _of the barrier._Sufficient Conditions for Log-concave Sampling

To prove that our algorithm works for log-concave distribution, we state three key assumptions (see Section B.1) which are the sufficient conditions to prove the mixing rate of log-concave distribution, for different barrier functions.

### Conditions: \(\overline{\nu}\)-symmetry, convexity and bounded local norm

We state three key conditions in order to lower bound the conductance for log-concave sampling.

**Assumption B.1**.: _Given a convex body \(\mathcal{K}\), let \(H:\mathcal{K}\to\mathbb{R}^{d\times d}\) be a self-concordant matrix function, we assume that_

1. \(\overline{\nu}\)**-symmetry.** _For any_ \(x\in\mathcal{K}\)_, we have_ \(E_{x}(1)\subseteq\mathcal{K}\cap(2x-\mathcal{K})\subseteq E_{x}(\sqrt{ \overline{\nu}})\) _where_ \(E_{x}(r)=\{y\in\mathbb{R}^{d}:(y-x)^{\top}H(x)(y-x)\leq r^{2}\}\)_._
2. **Convexity.** _Let_ \(F:\mathcal{K}\to\mathbb{R}\) _be defined as_ \(F(x)=\log(\det(H(x)+I_{d}))\)_, then_ \(F\) _is convex in_ \(x\)_._
3. **Bounded local norm.** _Let_ \(\nabla\log(\det(H(x)))\) _denote the gradient of_ \(\log(\det(H(x)))\) _in_ \(x\)_. For any_ \(x\in\mathcal{K}\)_, we have_ \[\|(H(x))^{-1/2}\cdot\nabla\log(\det(H(x)))\|_{2}^{2}\leq\widetilde{O}(d).\]

For the \(\overline{\nu}\)-symmetry assumption, we will prove that for barriers of the concern (log-barrier, Lee-Sidford barrier for polytopes and log-barrier for spectrahedra), \(\overline{\nu}=\nu\). This characteristic is proved in Laddha et al. (2020) for polytopes, and we prove that for spectrahedra in Section G. For convexity, due to the large amount of calculations, we defer to Section H and I. For bounded local norm, the results for polytopes are similarly shown in Laddha et al. (2020), and we will prove for log-barrier over spectrahedra.

### Local PSD approximation for any self-concordant matrix function

In this section, we prove a generalization of the matrix function self-concordance to regularized matrix function. When \(H\) is Hessian of the log barrier, this fact was proved in (Mangoubi and Vishnoi, 2023, Lemma E.3). We generalize this fact to general barriers.

**Lemma B.2**.: _Let \(\alpha\in(0,1)\). Let \(H:\mathcal{K}\to\mathbb{R}\) be a self-concordant matrix function and \(\Phi(u):=\alpha^{-1}H(u)+\eta^{-1}I_{d}\). For any \(u,v\in\mathcal{K}\) such that \(\|u-v\|_{\Phi(u)}\leq\frac{1}{\alpha^{1/2}}\) we have_

\[(1-\alpha^{1/2}\cdot\|u-v\|_{\Phi(u)})^{2}\cdot\Phi(v)\preceq\Phi(u)\preceq( 1+\alpha^{1/2}\cdot\|u-v\|_{\Phi(u)})^{2}\cdot\Phi(v).\]

Proof.: By (Laddha et al., 2020, Lemma 1.1), we have

\[(1-\|u-v\|_{H(u)})^{2}\cdot H(v)\preceq H(u)\preceq(1+\|u-v\|_{H(u)})^{2} \cdot H(v).\]

Because \(H(u)\preceq\alpha\cdot\Phi(u)\), we get

\[(1-\alpha^{1/2}\cdot\|u-v\|_{\Phi(u)})^{2}\cdot H(v)\preceq H(u)\preceq(1+ \alpha^{1/2}\cdot\|u-v\|_{\Phi(u)})^{2}\cdot H(v).\]

Using \(\Phi(u)=\alpha^{-1}H(u)+\eta^{-1}I_{d}\) again we get

\[(1-\alpha^{1/2}\cdot\|u-v\|_{\Phi(u)})^{2}\cdot\Phi(v)\preceq\Phi(u)\preceq( 1+\alpha^{1/2}\cdot\|u-v\|_{\Phi(u)})^{2}\cdot\Phi(v).\]

This completes the proof. 

### Bounded local norm for regularized Hessian function

We note that the bounded local norm condition generally holds for standard barrier functions, but in our core argument, we will instead rely on the bounded local norm of the _regularized Hessian function_. We prove that the bounded local norm condition implies the bounded local norm on the regularized Hessian function.

**Lemma B.3**.: _Let \(H:\mathcal{K}\to\mathbb{R}^{d\times d}\) be a matrix function, define \(F(x)=\log\det(H(x)+L^{2}I_{d})\) for \(L\in\mathbb{R}\), then we have the following inequality:_

\[\|(H(x)+L^{2}I_{d})^{-1/2}\nabla F(x)\|_{2}^{2}\leq\|H(x)^{-1/2}\nabla\log\det( H(x))\|_{2}^{2}.\]

Proof.: Let \(H(x)=U\Lambda U^{\top}\) be the eigendecomposition of the matrix \(H(x)\), let \(y=U^{\top}\nabla H(x)\), then

\[\|(H(x)+L^{2}I_{d})^{-1/2}\nabla F(x)\|_{2}^{2} =\|(H(x)+L^{2}I_{d})^{-3/2}\nabla H(x)\|_{2}^{2}\] \[=\|U(\Lambda+L^{2}I_{d})^{-3/2}U^{\top}\nabla H(x)\|_{2}^{2}\] \[=\|(\Lambda+L^{2}I_{d})^{-3/2}y\|_{2}^{2},\]

the given condition implies

\[\|H(x)^{-1/2}\nabla\log\det(H(x))\|_{2}^{2} =\|H(x)^{-3/2}\nabla H(x)\|_{2}^{2}\] \[=\|U\Lambda^{-3/2}U^{\top}\nabla H(x)\|_{2}^{2}\] \[=\|\Lambda^{-3/2}y\|_{2}^{2}.\]

As \(L^{2}\geq 0\), we can expand the regularized squared \(\ell_{2}\) norm as

\[\|(\Lambda+L^{2}I_{d})^{-3/2}y\|_{2}^{2} =\sum_{i=1}^{d}\frac{1}{(\lambda_{i}+L^{2})^{3}}y_{i}^{2}\] \[\leq\sum_{i=1}^{d}\frac{1}{\lambda_{i}^{3}}y_{i}^{2}\] \[=\|\Lambda^{-3/2}y\|_{2}^{2}.\]

This completes the proof. 

## Appendix C Key Tools for Robust Sampling

In this section, we list several tools which can be shared in the core proofs of all the main theorems. This section is organized as follows. In Section C.1, we provide a tool for Gaussian concentration. In Section C.2, we show that PSD approximation implies the determinant approximation (by losing a factor of \(d\)). One of the major idea in this work is instead of using exact accept probability, we will only need to use approximate accept probability. In Section C.3, we prove several useful robust properties for approximate accept probability. In Section C.4, we present a general lemma for bounding the TV distance between exact process and approximate process. In Section C.5, we provide some specific choices for parameters and then bound the TV distance. In Section C.6, we provide a definition which will be used in proof of lower bound on conductance. In Section C.7, we show how to lower bound the conductance by cross ratio.

### Gaussian concentration lemma

**Lemma C.1**.: _Let \(\xi\sim\mathcal{N}(0,I_{d})\), then for any \(t>\sqrt{2d}\), we have_

\[\Pr[\|\xi\|_{2}\geq t]\leq\,\exp(-(t^{2}-d)/8).\]

Proof.: We consider the squared \(\ell_{2}\) norm of \(\xi\): \(\|\xi\|_{2}^{2}=\sum_{i=1}^{d}\xi_{i}^{2}\), which is a \(\chi^{2}\) distribution with degree of freedom \(d\). By Lemma A.7, we know that

\[\Pr[\|\xi\|_{2}^{2}\geq 2\sqrt{d}k+2k^{2}+d]\leq\,\exp(-k^{2}),\]

set \(k^{2}=(t^{2}-d)/8\), we obtain the desired bound.

### Spectral approximation to determinant approximation

**Lemma C.2**.: _Let \(\epsilon_{\Phi}\in(0,1)\). Given four psd matrices satify the following conditions_

\[(1-\epsilon_{\Phi})\Phi(x) \preceq\widetilde{\Phi}(x)\preceq(1+\epsilon_{\Phi})\Phi(x),\] \[(1-\epsilon_{\Phi})\Phi(z) \preceq\widehat{\Phi}(z)\preceq(1+\epsilon_{\Phi})\Phi(z).\]

_Then we have_

\[(1-10\cdot\epsilon_{\Phi}\cdot d)\cdot\frac{\det(\Phi(x))}{\det( \Phi(z))}\leq\frac{\det(\widetilde{\Phi}(x))}{\det(\widetilde{\Phi}(z))}\leq (1+10\cdot\epsilon_{\Phi}\cdot d)\cdot\frac{\det(\Phi(x))}{\det(\Phi(z))}\]

Proof.: Since \(\widetilde{\Phi}(x)\in(1\pm\epsilon_{\Phi})\Phi(x)\), we have the following spectral bound:

\[(1-\epsilon_{\Phi})I_{d}\preceq\Phi(x)^{-1/2}\widetilde{\Phi}(x)\Phi(x)^{-1/2 }\preceq(1+\epsilon_{\Phi})I_{d},\]

we can take the determinant of the middle term:

\[\det(\Phi(x)^{-1/2}\widetilde{\Phi}(x)\Phi(x)^{-1/2}) =\det(\Phi(x)^{-1/2})\det(\widetilde{\Phi}(x))\det(\Phi(x)^{-1/2})\] \[=\frac{\det(\widetilde{\Phi}(x))}{\det(\Phi(x))}\] \[\in\det((1\pm\epsilon_{\Phi})I_{d})\] \[\in(1\pm\epsilon_{\Phi})^{d}\] \[\in 1+10\cdot\epsilon_{\Phi}\cdot d.\]

Thus, we complete the proof. 

### Approximate accept probability

We start with some definitions:

**Definition C.3**.: _We define_

\[\widehat{G}_{u}(x) :=\det(\widehat{\Phi}(u))^{1/2}\cdot\exp(-0.5\|u-x\|_{\widetilde {\Phi}(u)}^{2})\] \[G_{x}(u) :=\det(\Phi(x))^{1/2}\cdot\exp(-0.5\|u-x\|_{\Phi(x)}^{2})\] \[\widetilde{G}_{x}(u) :=\det(\widetilde{\Phi}(x))^{1/2}\cdot\exp(-0.5\|u-x\|_{\widetilde {\Phi}(x)}^{2})\]

**Definition C.4**.: _We define_

\[P_{x,\widetilde{\Phi}(x)}^{\mathrm{accept}}(u) :=\underset{\widetilde{\Phi}(u)}{\mathbb{E}}\big{[}\min\{1, \frac{\widehat{G}_{u}(x)\exp(-f(u))}{G_{x}(u)\exp(-f(x))}\}\big{]}\] \[P_{x,\widetilde{\Phi}(x)}^{\mathrm{accept}}(u) :=\underset{\widetilde{\Phi}(u)}{\mathbb{E}}\big{[}\min\{1, \frac{\widehat{G}_{u}(x)\exp(-f(u))}{\widehat{G}_{x}(u)\exp(-f(x))}\}\big{]}\]

_We will use \(P_{x}\), \(\widetilde{P}_{x}\) as a shorthand for the above notations, and we call \(P_{x}\) the exact process and \(\widetilde{P}_{x}\) the approximate process._

**Lemma C.5**.: _Fix \(x\) and \(\widetilde{\Phi}(x)\). Suppose \(\widetilde{\Phi}(x)\) is \((\epsilon_{H},\delta_{H})\)-good approximation to \(\Phi(x)\). Then_

\[\epsilon_{p}:=\underset{\omega\sim\mathcal{N}(x,\Phi^{-1}(x))}{\mathbb{E}}[|P_ {x,\Phi(x)}^{\mathrm{accept}}(u)-P_{x,\widetilde{\Phi}(x)}^{\mathrm{accept}} (u)|]\leq 0.001.\]

Proof.: Note that

\[\frac{G_{x}(u)}{\widetilde{G}_{x}(u)}=\frac{(\det\Phi(x))^{1/2}\exp(-\tfrac{1 }{2}\|x-u\|_{\widetilde{\Phi}(x)}^{2})}{(\det\widetilde{\Phi}(x))^{1/2}\exp(- \tfrac{1}{2}\|x-u\|_{\widetilde{\Phi}(x)}^{2})}.\]Because \(\widetilde{\Phi}(x)\) is a good approximation to \(\Phi(x)\), by Lemma C.2 we have

\[(1-\epsilon_{H})^{d/2}\leq\frac{(\det\Phi(x))^{1/2}}{(\det\widetilde{\Phi}(x))^{ 1/2}}\leq(1+\epsilon_{H})^{d/2}\]

and

\[(1-\epsilon_{H})^{2}\leq\frac{\|x-u\|_{\Phi(x)}^{2}}{\|x-u\|_{\widetilde{\Phi} (x)}^{2}}\leq(1+\epsilon_{H})^{2}.\]

By choosing step size sufficiently small, we have \(\|u-x\|_{\Phi(x)}^{2}<d\) with probability \(0.9999\). Therefore

\[\Pr\left[\|\|x-u\|_{\Phi(x)}^{2}-\|x-u\|_{\widetilde{\Phi}(x)}^{2}\right|\leq 3 \epsilon_{H}d\right]\geq 0.9999.\]

Then

\[\Pr\left[1-3\epsilon_{H}d\leq\frac{\exp(-0.5\|x-u\|_{\Phi(x)}^{2})}{\exp(-0.5 \|x-u\|_{\widetilde{\Phi}(x)}^{2})}\leq 1+3\epsilon_{H}d\right]\geq 0.9999.\]

For \(\epsilon_{H}d\) small enough, we have

\[\Pr\left[0.999\leq\frac{P_{x,\Phi(x)}^{\mathrm{accept}}(u)}{P_{x,\widetilde{ \Phi}(x)}^{\mathrm{accept}}(u)}\leq 1.0001\right]\geq 0.9998,\]

and therefore

\[\Pr[|P_{x,\Phi(x)}^{\mathrm{accept}}(u)-P_{x,\widetilde{\Phi}(x)}^{\mathrm{ accept}}(u)|\leq 0.0001]\geq 0.9998.\]

So

\[\mathop{\mathbb{E}}_{u\sim\mathcal{N}(x,\Phi^{-1}(x))}[|P_{x, \Phi(x)}^{\mathrm{accept}}(u)-P_{x,\widetilde{\Phi}(x)}^{\mathrm{accept}}(u)|] \leq 0.9998\cdot 0.0001+0.0002\cdot 1\] \[\leq 0.001.\]

This completes the proof. 

### TV distance between exact process and approximate process

**Lemma C.6** (TV distance between exact process and approximate process).: _Let \(\widetilde{\Phi}\) denote the \((1\pm\epsilon_{H})\) approximation of \(\Phi\). Let \(\delta_{H}\) denote the failure probability. Let \(\epsilon_{p}\) be defined as Lemma C.5. Let \(P_{x}\) denote the exact process. Let \(\widetilde{P}_{x}\) denote the approximate process. If \(x\in\mathcal{K}\), then_

\[\mathrm{TV}(P_{x},\widetilde{P}_{x})\leq\delta_{H}+\epsilon_{p}+\sqrt{d \epsilon_{H}}.\]

Proof.: We say \(\widetilde{\Phi}(x)\) is good if it is an \((\epsilon_{H},\delta_{H})\)-approximation to \(\Phi(x)\), and \(\widetilde{\Phi}(x)\) is bad otherwise. We can upper bound \(\mathrm{TV}(P_{x},\widetilde{P}_{x})\) as follows:

\[\mathrm{TV}(P_{x},\widetilde{P}_{x}) \leq \mathbb{E}[\mathrm{TV}(P_{x,\Phi(x)},P_{x,\widetilde{\Phi}(x)})]\] \[= \mathbb{E}[\mathrm{TV}(P_{x,\Phi(x)},P_{x,\widetilde{\Phi}(x)}) \mid\widetilde{\Phi}(x)\text{ is bad}]\] \[+ \mathbb{E}[\mathrm{TV}(P_{x,\Phi(x)},P_{x,\widetilde{\Phi}(x)}) \mid\widetilde{\Phi}(x)\text{ is good}]\] \[\leq \Pr[\widetilde{\Phi}(x)\text{ is bad}]+\mathbb{E}[\mathrm{TV}(P_{x, \Phi(x)},P_{x,\widetilde{\Phi}(x)})\mid\widetilde{\Phi}(x)\text{ is good}]\] \[\leq \delta_{H}+\mathbb{E}[\mathrm{TV}(P_{x,\Phi(x)},P_{x,\widetilde{ \Phi}(x)})\mid\widetilde{\Phi}(x)\text{ is good}].\]

where the third step follows from \(\mathrm{TV}(\cdot,\cdot)\leq 1\), the last step follows from Lemma E.3.

It remains to compute the second term in the above equation. By Pinsker's inequality, when \(\widetilde{\Phi}(x)\) is good, we have

\[\mathrm{TV}(\mathcal{N}(x,\widetilde{\Phi}^{-1}(x)),\mathcal{N}(x,\Phi^{-1}( x)))^{2}\]\[\leq 0.5\cdot D(\mathcal{N}(x,\widetilde{\Phi}^{-1}(x))||\mathcal{N}(x, \Phi^{-1}(x)))\] \[= 0.25\cdot(\operatorname{tr}[\widetilde{\Phi}(x)^{-1/2}\Phi(x) \widetilde{\Phi}(x)^{-1/2}]-d+\log\frac{\det\widetilde{\Phi}(x)}{\det\Phi(x)})\] \[\leq 0.25\cdot((1\pm\epsilon_{H})d-d+\log(1\pm\epsilon_{H})^{d})\] \[\leq 0.5\cdot d\epsilon_{H}.\] (2)

where the first step follows from Pinsker inequality.

Now, we can upper bound \(\operatorname{TV}(P_{x},\widetilde{P}_{x})\) in the following sense,

\[\operatorname{TV}(P_{x},\widetilde{P}_{x}) \leq\delta_{H}+\mathbb{E}[\operatorname{TV}(P_{x,\Phi(x)},P_{x, \widetilde{\Phi}(x)})\mid\widetilde{\Phi}(x)\text{ is good}]\] \[\leq\delta_{H}+\operatorname*{\mathbb{E}}_{\begin{subarray}{c}u \sim\mathcal{N}(x,\Phi^{-1}(x))\\ u^{\prime}\sim\mathcal{N}(x,\widetilde{\Phi}^{-1}(x))\\ \text{any}\text{\,cop}\end{subarray}}[|P_{x,\Phi(x)}^{\operatorname{accept} }(u)-P_{x,\widetilde{\Phi}(x)}^{\operatorname{accept}}(u)|]+\operatorname{TV }(\mathcal{N}(x,\widetilde{\Phi}^{-1}(x)),\mathcal{N}(x,\Phi^{-1}(x)))\] \[=\delta_{H}+\epsilon_{p}+\operatorname{TV}(\mathcal{N}(x, \widetilde{\Phi}^{-1}(x)),\mathcal{N}(x,\Phi^{-1}(x)))\] \[\leq\delta_{H}+\epsilon_{p}+\sqrt{d\epsilon_{H}}\]

The forth step follows from definition of \(\epsilon_{p}\), the fifth step follows from Eq. (2). 

### TV distance between exact and approximate process: instantiation

**Lemma C.7**.: _If \(d\epsilon_{H}<0.001\) and \(\delta_{H}<0.001\). Then, we have_

\[\operatorname{TV}(P_{x},\widetilde{P}_{x})\leq 0.01\]

Proof.: We can upper bound it as follows

\[\operatorname{TV}(P_{x},\widetilde{P}_{x}) \leq\delta_{H}+\epsilon_{p}+\sqrt{d\epsilon_{H}}\] \[\leq 0.001+\epsilon_{p}+0.001\] \[\leq 0.01\]

where the first step follows from Lemma C.6, the second step follows from choice \(\epsilon_{H},\delta_{H}\), and the last step follows from Lemma C.5. 

### Lower bound on conductance, definition

**Definition C.8**.: _Let \(\beta\in(0,0.1)\) denote some fixed constant. Let \(S_{1}\subset\mathcal{K}\) and \(S_{2}:=\mathcal{K}\backslash S_{1}\) and \(\pi(S_{1})\leq 1/2\). We define \(S_{1}^{\prime}\) and \(S_{2}^{\prime}\) as follows_

\[S_{1}^{\prime} :=\{x\in S_{1}:\widetilde{P}_{x}(S_{2})\leq\beta\},\] \[S_{2}^{\prime} :=\{z\in S_{2}:\widetilde{P}_{z}(S_{1})\leq\beta\}.\]

### Lower bound on conductance, lemma

**Lemma C.9** (Lower bound the conductance by cross ratio).: _Let \(S_{1}^{\prime}\) and \(S_{2}^{\prime}\) be defined as Definition C.8. The conductance \(\phi\) satisfies_

\[\phi\geq\frac{1}{16}\beta\cdot\sigma(S_{1}^{\prime},S_{2}^{\prime}).\]

Proof.: The proof follows the general format for conductance proofs for geometric Markov chains (see e.g. Section 5 of Vempala (2005)).

Let \(S_{1}\subseteq\mathcal{K}\) and let \(S_{2}=\mathcal{K}\backslash S_{1}\). Let \(S_{1}^{\prime}\) and \(S_{2}^{\prime}\) be defined as Definition C.8. We define \(S_{3}^{\prime}\)

\[S_{3}^{\prime}:=(\mathcal{K}\backslash S_{1}^{\prime})\backslash S_{2}^{\prime}.\]By Lemma D.12, we have that

\[\int_{x\in S_{1}}\pi(x)\widetilde{P}_{x}(S_{2})\mathrm{d}x=\int_{x\in S_{2}}\pi(x )\widetilde{P}_{x}(S_{1})\mathrm{d}x.\] (3)

Thus, by Lemma A.6, we have

\[\pi^{\star}(S_{3}^{\prime})\geq\sigma(S_{1}^{\prime},S_{2}^{\prime})\pi^{\star}( S_{1}^{\prime})\pi^{\star}(S_{2}^{\prime}).\] (4)

**Case 1.** First, we assume that both \(\pi^{\star}(S_{1}^{\prime})\geq\frac{1}{4}\pi^{\star}(S_{1})\) and \(\pi^{\star}(S_{2}^{\prime})\geq\frac{1}{4}\pi^{\star}(S_{2})\). In this case we have

\[\int_{S_{1}}\widetilde{P}_{x}(S_{2})\pi(x)\mathrm{d}x =\frac{1}{2}\int_{S_{1}}\widetilde{P}_{x}(S_{2})\pi(x)\mathrm{d}x +\frac{1}{2}\int_{S_{2}}\widetilde{P}_{x}(S_{1})\pi(x)\mathrm{d}x\] \[\geq\frac{\beta}{2}\cdot\pi^{\star}(S_{3}^{\prime})\] \[\geq\frac{\beta}{2}\cdot\sigma(S_{1}^{\prime},S_{2}^{\prime}) \cdot\pi^{\star}(S_{1}^{\prime})\pi^{\star}(S_{2})\] \[\geq\frac{\beta}{4}\cdot\sigma(S_{1}^{\prime},S_{2}^{\prime}) \cdot\min(\pi^{\star}(S_{1}^{\prime}),\pi^{\star}(S_{2}^{\prime}))\] \[\geq\frac{\beta}{16}\cdot\sigma(S_{1}^{\prime},S_{2}^{\prime}) \cdot\min(\pi^{\star}(S_{1}),\pi^{\star}(S_{2})).\] (5)

where the first step follows from Eq. (3), the second step follows from Definition C.8, the third step follows from Eq. (4), the fifth step follows from one of \(\pi^{\star}(S_{1}^{\prime})\) and \(\pi^{\star}(S_{2}^{\prime})\) is at least \(1/2\).

**Case 2.** Now suppose that instead either \(\pi^{\star}(S_{1}^{\prime})<\frac{1}{4}\pi^{\star}(S_{1})\) or \(\pi^{\star}(S_{2}^{\prime})<\frac{1}{4}\pi^{\star}(S_{2})\).

**Case 2a.** If \(\pi^{\star}(S_{1}^{\prime})<\frac{1}{4}\pi^{\star}(S_{1})\) then we have

\[\int_{S_{1}}\widetilde{P}_{x}(S_{2})\pi(x)\mathrm{d}x =\frac{1}{2}\int_{S_{1}}\widetilde{P}_{x}(S_{2})\pi(x)\mathrm{d}x +\frac{1}{2}\int_{S_{2}}\widetilde{P}_{x}(S_{1})\pi(x)\mathrm{d}x\] \[\geq\frac{1}{2}\int_{S_{1}\setminus S_{1}^{\prime}}\widetilde{P} _{x}(S_{2})\pi(x)\mathrm{d}x\] \[\geq\frac{1}{2}\cdot\frac{3}{4}\cdot\beta\pi^{\star}(S_{1})\] \[\geq\frac{3}{8}\beta\min(\pi^{\star}(S_{1}),\pi^{\star}(S_{2})).\] (6)

where the first step follows from Eq. (3), and the third step follows from Definition C.8.

**Case 2b.** Similarly, if \(\pi^{\star}(S_{2}^{\prime})<\frac{1}{4}\pi^{\star}(S_{2})\) we have

\[\int_{S_{1}}\widetilde{P}_{x}(S_{2})\pi(x)\mathrm{d}x =\frac{1}{2}\int_{S_{1}}\widetilde{P}_{x}(S_{2})\pi(x)\mathrm{d}x +\frac{1}{2}\int_{S_{2}}\widetilde{P}_{x}(S_{1})\pi(x)\mathrm{d}x\] \[\geq\frac{1}{2}\int_{S_{2}\setminus S_{2}^{\prime}}\widetilde{P} _{x}(S_{1})\pi(x)\mathrm{d}x\] \[\geq\frac{1}{2}\cdot\frac{3}{4}\cdot\beta\pi^{\star}(S_{2})\] \[\geq\frac{3}{8}\beta\min(\pi^{\star}(S_{1}),\pi^{\star}(S_{2})).\] (7)

where the first step follows from Eq. (3), and the third step follows from Definition C.8.

Therefore, Eq. (5), (6), and (7) together imply that

\[\frac{1}{\min(\pi^{\star}(S_{1}),\pi^{\star}(S_{2}))}\int_{S_{1}}\widetilde{P} _{x}(S_{2})\pi(x)\mathrm{d}x\geq\frac{\beta}{16}\cdot\sigma(S_{1}^{\prime},S_{2} ^{\prime})\] (8)

for every partition \(S_{1}\cup S_{2}=\mathcal{K}\).

Hence, Eq. (8) implies that

\[\phi=\inf_{S\subseteq\mathcal{K}:\pi^{\star}(S)\leq\frac{1}{2}}\frac{1}{\pi^{ \star}(S)}\int_{S}\widetilde{P}_{x}(\mathcal{K}\backslash S)\pi(x)\mathrm{d}x \geq\frac{\beta}{16}\sigma(S_{1}^{\prime},S_{2}^{\prime}).\qed\]Correctness for General Barrier Functions with Regularization

In Section D.1, we show how to bound the density ratios. In Section D.2, we show how to bound the determinant. In Section D.3, we show how to bound the local norm. In Section D.4, we explain how to bound the TV distance between two exact distributions. In Section D.5, we show how to bound the TV distance between two approximate processes. In Section D.6, we show how to bound the TV distance between two Gaussians. In Section D.7, we prove reversibility and stationary distribution. In Section D.8, we prove a lower bound on cross ratio distance. In Section D.9, we relate the cross ratio distance to local norm by utilizing \(\overline{\nu}\)-symmetry.

```
1:procedureMain(\(A\in\mathbb{R}^{n\times d},b\in\mathbb{R}^{n},\delta\in(0,1),x_{0}\in\mathbb{R}^{d}, \alpha>0,\eta>0\))\(\triangleright\) Theorem 1.1
2:\(\triangleright\)\(\mathcal{K}:=\{x\in\mathbb{R}^{d}:Ax\leq b\}\)
3: Let \(g\) be the log-barrier for \(\mathcal{K}\) with \(\nu=n\)
4:\(\alpha\leftarrow\Theta(1/d)\)
5:\(\eta\leftarrow\Theta(1/(dL^{2}))\)
6:\(T\leftarrow(\nu\alpha^{-1}+\eta^{-1}R^{2})\cdot\log(w/\delta)\)
7:\(x\gets x_{0}\)\(\triangleright\) We are given \(x_{0}\in\operatorname{Int}(\mathcal{K})\)
8:\(\epsilon\leftarrow\Theta(1)\)
9:\(\epsilon_{H}\leftarrow\Theta(\epsilon/d)\)
10:for\(t=1\to T\)do
11: Sample a point \(\xi\sim\mathcal{N}(0,I_{d})\)\(\triangleright\) Let \(H\) denote the Hessian of barrier function \(g\)
12:\(\widetilde{H}(x)\leftarrow\textsc{SubSample}(A,b,x,n,d,\epsilon_{H})\)\(\triangleright\) Algorithm 3
13:\(\triangleright\)\(\widetilde{H}(x)\) can be written \(\sum_{i\in S}\widetilde{\sigma}_{i}a_{i}a_{i}^{\top}\) where \(|S|=\epsilon_{H}^{-2}d\log d\)
14:\(\triangleright\) The above step takes \(\operatorname{nnz}(A)\log n+\mathcal{T}_{\operatorname{mat}}(d,\epsilon_{H}^{ -2}d,d)\) time
15:\(\widetilde{\Phi}(x)\leftarrow\alpha^{-1}\widetilde{H}(x)+\eta^{-1}I_{d}\)
16:\(z\gets x+\widetilde{\Phi}(x)^{-1/2}\xi\)
17:if\(z\in\operatorname{Int}(\mathcal{K})\)then
18:\(\widetilde{H}(z)\leftarrow\textsc{SubSample}(A,b,z,n,d,\epsilon_{H})\)\(\triangleright\) Algorithm 3
19:\(\widetilde{\Phi}(z)\leftarrow\alpha^{-1}\widetilde{H}(z)+\eta^{-1}I_{d}\)
20:accept\(x\gets z\) with probability \[\frac{1}{2}\cdot\min\Big{\{}\frac{\exp(-f(z))\cdot(\det(\widehat{\Phi}(z)))^{ 1/2}\cdot\exp(-0.5\|x-z\|_{\widehat{\Phi}(z)}^{2})}{\exp(-f(x))\cdot(\det( \widehat{\Phi}(x)))^{1/2}\cdot\exp(-0.5\|x-z\|_{\widehat{\Phi}(x)}^{2})},1 \Big{\}}\]
21:else
22: reject\(z\)
23:endif
24:endfor
25:endfor
26:return\(x\)
27:endprocedure ```

**Algorithm 2** Our algorithm for sampling from polytope with log-barrier (formal version of Algorithm 1). As the only differences between log-barrier for polytopes and others are the choice of \(\nu\) and how to generate the approximate the Hessian, we only present this algorithm.

### Bounding the density ratio

**Lemma D.1** (Bounding the density ratio).: _Let \(x\in\operatorname{Int}(\mathcal{K})\). Suppose \(f\) is \(L\)-Lipschitz and \(\eta\leq 1/(10dL^{2})\), then_

\[\Pr_{z\sim\mathcal{N}(x,\Phi^{-1}(x))}\Big{[}\frac{\exp(-f(z))}{\exp(-f(x))} \geq\frac{1}{2}\Big{]}\geq 0.999.\]

_If in addition, \(f\) is \(\beta\)-smooth and \(\eta\leq 1/(10d\beta)\), then we further have_

\[\Pr_{z\sim\mathcal{N}(x,\Phi^{-1}(x))}\Big{[}\frac{\exp(-f(x))}{\exp(-f(z))} \geq\frac{1}{2}\Big{]}\geq 0.499.\]Proof.: **Proof of Part 1.** Since \(z\sim\mathcal{N}(x,\Phi(x)^{-1})\), we have that

\[z =x+\Phi(x)^{-1/2}\xi\] \[=x+(\alpha^{-1}\cdot H(x)+\eta^{-1}I_{d})^{-1/2}\]

for some \(\xi\sim\mathcal{N}(0,I_{d})\).

Since \(\alpha^{-1}H(x)+\eta^{-1}I_{d}\succeq\eta^{-1}I_{d}\), and \(H(x)\) and \(I_{d}\) are both positive definite, we have that

\[\eta\cdot I_{d}\succeq(\alpha^{-1}H(x)+\eta^{-1}I_{d})^{-1}\] (9)

Thus, we can upper bound \(\|z-x\|_{2}\) as follows:

\[\|z-x\|_{2} =\|(\alpha^{-1}H(x)+\eta^{-1}I_{d})^{-1/2}\xi\|_{2}\] \[=\sqrt{\xi^{\top}(\alpha^{-1}H(x)+\eta^{-1}I_{d})^{-1}\xi}\] \[\leq\sqrt{\xi^{\top}\eta I_{d}\xi}\] \[=\sqrt{\eta}\cdot\|\xi\|_{2}\]

where the third step follows from Eq. (9),

Recall that \(\xi\) is sampled from \(\mathcal{N}(0,I_{d})\), using Lemma C.1, we can show

\[\Pr[\|\xi\|_{2}>t]\leq\exp(-(t^{2}-d)/8),\ \ \forall t>\sqrt{2d}.\]

Combining the above two equations, we have

\[\Pr[\|z-x\|_{2}>\sqrt{\eta}\sqrt{20d}]\leq\exp(-\frac{19}{8}d)<0.001.\] (10)

Using \(\eta\leq 1/(80dL^{2})\), we have

\[\Pr[\|z-x\|_{2}>1/(2L)]<0.001.\] (11)

Since \(f\) is \(L\)-Lipschitz, then we have

\[\frac{\exp(-f(z))}{\exp(-f(x))}=\exp(-(f(z)-f(x)))\geq\exp(-L\|z-x\|_{2}).\]

Therefore,

\[\Pr_{z\sim N(x,\Phi^{-1}(x))}\Big{[}\frac{\exp(-f(z))}{\exp(-f(x) )}\geq 1/2\Big{]} \geq\Pr_{z\sim N(x,\Phi^{-1}(x))}\Big{[}\exp(-L\|z-x\|_{2})\geq 1/2 \Big{]}\] \[=\Pr_{z\sim N(x,\Phi^{-1}(x))}\Big{[}\|z-x\|_{2}\leq\log(2)/L \Big{]}\] \[\geq\Pr_{z\sim N(x,\Phi^{-1}(x))}\Big{[}\|z-x\|_{2}\leq 1/(2L) \Big{]}\] \[\geq 0.999\]

where the last inequality holds by Eq. (11).

**Proof of Part 2.** Moreover, in the setting where \(f\) is differentiable and \(\beta\)-smooth, we have that, since \(z-x\) is a multivariate Gaussian random variable,

\[\Pr[(z-x)^{\top}\nabla f(x)\leq 0]=\frac{1}{2}.\]

If \((z-x)^{\top}\nabla f(x)\leq 0\), we have that

\[f(z)-f(x) \leq(z-x)^{\top}\nabla f(x)+\beta\|z-x\|_{2}^{2}\] \[\leq\beta\|z-x\|_{2}^{2}.\]Therefore,

\[\Pr_{z\sim N(x,\widetilde{\Phi}^{-1}(x))}[\frac{\pi(z)}{\pi(x)}\geq \frac{1}{2}]\] \[\geq \Pr_{z\sim N(x,\widetilde{\Phi}^{-1}(x))}[\frac{\pi(z)}{\pi(x)}\geq \frac{1}{2}\ \ \text{and}\ \ (z-x)^{\top}\nabla f(x)\leq 0]-\Pr_{z\sim N(x, \widetilde{\Phi}^{-1}(x))}[(z-x)^{\top}\nabla f(x)>0]\] \[\geq \Pr_{z\sim N(x,\widetilde{\Phi}^{-1}(x))}[e^{-\beta\|z-x\|_{2}^{ 2}}\geq\frac{1}{2}]-0.5\] \[= \Pr_{z\sim N(x,\widetilde{\Phi}^{-1}(x))}[\|z-x\|_{2}\leq\frac{ \sqrt{\log(2)}}{\sqrt{\beta}}]-0.5\] \[\geq 0.999-0.5\] \[= 0.499,\]

where the last Inequality holds by Eq. (10) since \(\eta\leq\frac{1}{20d\beta}\). 

**Lemma D.2**.: _Let \(\|x-z\|_{\Phi(x)}<0.001\). Let \(\eta\leq 0.01/L^{2}\). Then we have_

\[|f(x)-f(z)|\leq 0.1\]

Proof.: We have

\[|f(x)-f(z)| \leq L\cdot\|x-z\|_{2}\] \[\leq L\cdot\sqrt{\eta}\cdot\|x-z\|_{\Phi(x)}\] \[\leq 0.1\]

where the second step follows from \(\Phi(x)\succeq\eta^{-1}I_{d}\), the last step follows from \(\eta\leq 0.01/L^{2}\). 

### Bounding the determinant

**Lemma D.3** (Bounding the determinant).: _Consider any \(x\in\operatorname{Int}(\mathcal{K})\), and \(\xi\sim\mathcal{N}(0,I_{d})\). Let \(z=x+(\Phi(x))^{-1/2}\xi\). Then_

\[\Pr_{\xi}\Big{[}\log(\det(\Phi(z)))-\log(\det(\Phi(x)))\geq-0.1\Big{]}\geq 0. 999.\]

Proof.: First, note that

\[\log(\det(\Phi(z)))-\log(\det(\Phi(x))) = \log(\frac{\det(\Phi(z))}{\det(\Phi(x))})\] \[= \log(\frac{\det(d\cdot H(z)+dL^{2}\cdot I_{d})}{\det(d\cdot H(x)+ dL^{2}\cdot I_{d})}))\] \[= \log(\frac{\det(H(z)+L^{2}\cdot I_{d})}{\det(H(x)+L^{2}\cdot I_{ d})}),\]

thus, it suffices to consider the function \(F(x)=\log(\det(H(x)+L^{2}\cdot I_{d}))\) and bound \(F(z)-F(x)\). By Assumption (ii), we know that \(F(x)\) is convex.

Thus,

\[F(z)-F(x)\geq(z-x)^{\top}\nabla F(x).\]

We know that

\[z=x+(\Phi(x))^{-1/2}\xi\]

where \(\xi\sim\mathcal{N}(0,I_{d})\).

Thus,

\[F(z)-F(x)\geq\xi^{\top}(\Phi(x))^{-1/2}\nabla F(x)\]By property of Gaussian distribution, we know that

\[\xi^{\top}(\Phi(x))^{-1/2}\nabla F(x)\]

is a Gaussian with mean \(0\) and variance \(\|(\Phi(x))^{-1/2}\nabla F(x)\|_{2}^{2}\).

By definition of \(\Phi\) and Assumption (iii), we have

\[\|(H(x)+L^{2}\cdot I_{d})^{-1/2}\nabla F(x)\|_{2}^{2}=O(d)\]

and therefore

\[\|\Phi(x)^{-1/2}\nabla F(x)\|_{2}^{2}=O(1).\]

Thus, rescaling the function and applying the standard concentration inequality, we complete the proof. 

**Lemma D.4** (Bounding the determinant, shifted).: _Let \(\alpha<0.001/d\). For fixed \(x,z\in\mathcal{K}\) such that \(\|z-x\|_{\Phi(x)}<0.001\). Then, we have_

\[\log(\det(\Phi(z)))-\log(\det(\Phi(x)))\geq-0.01.\]

Proof.: Let \(F(x)=\log\det(H(x)+L^{2}\cdot I_{d})\). Then

\[\log(\det(\Phi(z)))-\log(\det(\Phi(x)))=F(z)-F(x)\geq(z-x)^{\top} \nabla F(x)\]

where the second step is because \(F(x)\) is convex (by Assumption (ii)).

We have

\[((z-x)^{\top}\nabla F(x))^{2} =((z-x)^{\top}\Phi(x)^{1/2}\cdot\Phi(x)^{-1/2}\nabla F(x))^{2}\] \[\leq\|(z-x)^{\top}\Phi(x)^{1/2}\|_{2}\cdot\|\Phi(x)^{-1/2}\nabla F (x)\|_{2}\] \[\leq 0.001\cdot\|\Phi(x)^{-1/2}\nabla F(x)\|_{2}\] \[\leq 0.001\cdot 0.1.\]

where the second step is by Cauchy-Schwarz, the third step is by assumption \(\|z-x\|_{\Phi(x)}\leq 0.001\), the fourth step is by \(\alpha<0.001/d\) and Assumption (iii). 

### Bounding the local norm

**Lemma D.5** (Bounding the local norm).: _Consider any \(x\in\mathrm{Int}(\mathcal{K})\), and \(\xi\sim\mathcal{N}(0,I_{d})\). Let \(\eta\leq 0.001/d\). Let \(z=x+(\Phi(x))^{-1/2}\xi\). Then_

\[\Pr_{\xi}\Big{[}\|z-x\|_{\Phi(z)}^{2}-\|z-x\|_{\Phi(x)}^{2}|\leq 0.01 \Big{]}\geq 0.999\]

Proof Sketch.: Our proof is a generalization of [2, Proposition 7], where they prove the result for non-regularized log-barrier. Note that their proof also works for regularized barriers because two points are close in the \(\Phi(x)\) norm indicates that they are close in the \(H(x)\) norm. The only difference is that when computing the Gaussian polynomial as in Sachdeva and Vishnoi (2016), we need to handle non-uniform weights instead of the uniform weights of log-barrier. This could also be handled by observing that if two points are close in \(H(x)\) norm, then their weights are close in the sense that \(\|w_{p}(x)^{-1}(w_{p}(x)-w_{p}(z))\|_{\infty}\leq c_{p}\) where \(c_{p}<1\) is some small constant that depends on \(p\) (see (Lee and Sidford, 2019, Lemma 34)). One could then check that the argument of (Sachdeva and Vishnoi, 2016, Proposition 7) is robust under small perturbation to the weights, and conclude similar conclusions. 

**Lemma D.6** (Bounding the local norm, shifted).: _Consider any \(x\in\mathrm{Int}(\mathcal{K})\), and \(\xi\sim\mathcal{N}(0,I_{d})\). Let \(\eta\leq 0.001/d\). Let \(\|z-x\|_{\Phi(x)}<0.001\). Let \(u=x+(\Phi(x))^{-1/2}\xi\). Then_

\[\Pr_{\xi}\Big{[}\|\|u-z\|_{\Phi(u)}^{2}-\|u-x\|_{\Phi(u)}^{2}|\leq 0.1 \Big{]}\geq 0.999\]

[MISSING_PAGE_EMPTY:29]

For convenience, we define

\[a_{1} :=\frac{G_{z}(u)}{G_{x}(u)}\] \[a_{2} :=\frac{\exp(-f(u))G_{u}(x)}{\exp(-f(x))G_{x}(u)}\] \[a_{3} :=\frac{\exp(-f(u))G_{u}(z)}{\exp(-f(z))G_{z}(u)}\cdot\frac{G_{z}(u )}{G_{x}(u)}\]

Let \(\theta\in(0,1)\). We have the following:

\[\mathrm{TV}(P_{x},P_{z}) =1-p_{\min}\] \[=1-\mathbb{E}[\min\{1,a_{1},a_{2},a_{3}\}]\] \[=1-\min\{1,\theta\}\cdot\Pr[\min\{a_{1},a_{2},a_{3}\}\geq\theta] -\min\{a_{1},a_{2},a_{3}\}\cdot\Pr[\min\{a_{1},a_{2},a_{3}\}<\theta]\] \[\leq 1-\min\{1,\theta\}\cdot\Pr[\min\{a_{1},a_{2},a_{3}\}\geq\theta]\] \[\leq 1-\theta\cdot\Pr[\min\{a_{1},a_{2},a_{3}\}\geq\theta]\] \[\leq 1-\theta\cdot\theta_{0}\] \[\leq 1-0.01\cdot 0.95\] \[\leq 0.99\]

where the sixth step follows from Eq. (12), and the last step follows from choice of \(\theta\) and \(\theta_{0}\).

Let \(a_{i}=e^{-b_{i}}\). It suffices to prove that for some \(\theta\in(0,1)\), we have

\[\Pr[\min\{a_{1},a_{2},a_{3}\}\geq\theta]\geq\theta_{0}.\] (12)

Suppose for some \(\tau\in[1,5]\) we can show that

\[\Pr[b_{1}<\tau] \geq\theta_{1}\] \[\Pr[b_{2}<\tau] \geq\theta_{2}\] \[\Pr[b_{3}<\tau] \geq\theta_{3}\]

Then by union bound, we can know that

\[\Pr[\max\{b_{1},b_{2},b_{3}\}<\tau]\] \[= \Pr[\min\{a_{1},a_{2},a_{3}\}>e^{-\tau}]\] \[= \Pr[\min\{a_{1},a_{2},a_{3}\}>\theta]\] \[\geq 1-(1-\theta_{1})-(1-\theta_{2})-(1-\theta_{3})\] \[\geq 0.95\]

where the second step follows from \(e^{-\tau}=\theta\geq 0.01\), and the last step follows from \(\theta_{1}=\theta_{2}=\theta_{3}=0.99\). In the following, we will establish the three bounds of interest.

**Part 1.** By definition, we have

\[a_{1} =\frac{G_{z}(u)}{G_{x}(u)}\] \[=\frac{(\det(\Phi(z)))^{1/2}}{(\det(\Phi(x)))^{1/2}}\cdot\exp(-0. 5\|u-z\|_{\Phi(z)}^{2}+0.5\|u-x\|_{\Phi(x)}^{2})\] \[= \exp(-0.5\|u-z\|_{\Phi(z)}^{2}+0.5\|u-x\|_{\Phi(x)}^{2}+0.5\log \det(\Phi(z))-0.5\log(\det(\Phi(x))))\]

Using Lemma D.7 (on the term \(0.5\|u-z\|_{\Phi(z)}^{2}-0.5\|u-x\|_{\Phi(u)}^{2}\) and Lemma D.5 (on the term \(0.5\|u-x\|_{\Phi(u)}^{2}-0.5\|u-x\|_{\Phi(x)}^{2}\)), we have

\[\Pr_{u}(\underbrace{0.5\|u-z\|_{\Phi(z)}^{2}-0.5\|u-x\|_{\Phi(u)}^{2}}_{ \text{Lemma D.7}}+\underbrace{0.5\|u-x\|_{\Phi(u)}^{2}-0.5\|u-x\|_{\Phi(x)}^ {2}}_{\text{Lemma D.5}}\leq 0.1\tau]\geq 0.999,\]From Lemma D.4, we know

\[0.5\log(\det(\Phi(x)))-0.5\log(\det(\Phi(z)))\leq 0.1\tau.\]

Thus,

\[\Pr[b_{1}\leq\tau]\geq 0.99\]

**Part 2.** By definition, for \(a_{2}\) we have

\[a_{2} =\frac{\exp(-f(u))G_{u}(x)}{\exp(-f(x))G_{x}(u)}\] \[=\frac{\exp(-f(u))}{\exp(-f(x))}\cdot\frac{(\det(\Phi(u)))^{1/2}} {(\det(\Phi(x)))^{1/2}}\cdot\exp(-0.5\|x-u\|_{\Phi(u)}^{2}+0.5\|u-x\|_{\Phi(x)} ^{2})\] \[=\exp(-f(u)+f(x)+0.5\log(\det(\Phi(u)))-0.5\log(\det(\Phi(x)))-0.5 \|x-u\|_{\Phi(u)}^{2}+0.5\|u-x\|_{\Phi(x)}^{2})\]

By Lemma D.1, we have

\[\Pr[f(u)-f(x)\leq 0.1\tau]\geq 0.999\]

By Lemma D.3, we have

\[\Pr_{u}[0.5\log(\det(\Phi(x)))-0.5\log(\det(\Phi(u)))\leq 0.1\tau]\geq 0.999\]

By Lemma D.5, we have

\[\Pr_{u}[0.5\|x-u\|_{\Phi(u)}^{2}-0.5\|u-x\|_{\Phi(x)}^{2}\leq 0.1\tau]\geq 0.999\]

Then, combining the above three equations, we can get the following result:

\[\Pr[b_{2}\leq\tau]\geq 0.99\]

**Part 3.**

\[a_{3} =\frac{\exp(-f(u))G_{u}(z)}{\exp(-f(z))G_{z}(u)}\cdot\frac{G_{z}(u )}{G_{x}(u)}\] \[=\frac{\exp(-f(u))}{\exp(-f(z))}\cdot\frac{(\det(\Phi(u)))^{1/2}} {(\det(\Phi(x)))^{1/2}}\cdot\exp(-0.5\|z-u\|_{\Phi(u)}^{2}+0.5\|u-x\|_{\Phi(x) }^{2})\] \[=\exp(-f(u)+f(z)+0.5\log(\det(\Phi(u)))-0.5\log(\det(\Phi(x)))-0.5 \|z-u\|_{\Phi(u)}^{2}+0.5\|u-x\|_{\Phi(x)}^{2})\]

By Lemma D.1 (on the term \(f(u)-f(x)\)) and Lemma D.2 (on the term \(f(x)-f(z)\)), we have

\[\Pr_{u}[\underbrace{f(u)-f(x)}_{\text{Lemma D.1}}+\underbrace{f(x)-f(z)}_{ \text{Lemma D.2}}\leq 0.1\tau]\geq 0.999.\]

By Lemma D.3, we have

\[\Pr_{u}[0.5\log(\det(\Phi(x)))-0.5\log(\det(\Phi(u)))\leq 0.1\tau]\geq 0.999.\]

By Lemma D.5 (on the term \(0.5\|u-x\|_{\Phi(u)}^{2}-0.5\|u-x\|_{\Phi(x)}^{2}\)) and Lemma D.6 (on the term \(0.5\|z-u\|_{\Phi(u)}^{2}-0.5\|u-x\|_{\Phi(u)}^{2}\)), we have

\[\Pr_{u}[0.5\|z-u\|_{\Phi(u)}^{2}-0.5\|u-x\|_{\Phi(u)}^{2}+\underbrace{0.5\|u- x\|_{\Phi(u)}^{2}-0.5\|u-x\|_{\Phi(x)}^{2}}_{\text{Lemma D.5}}\leq 0.2\tau]\geq 0.999\]

Thus, combining the above three equations, we obtain the following:

\[\Pr[b_{3}\leq\tau]\geq 0.99.\qed\]

### Bounding TV distance between approximate processes

**Lemma D.9** (Robust version of Lemma E.10 in Mangoubi and Vishnoi (2023)).: _If \(\delta_{H}<0.001\), \(d\alpha<0.001\), \(d\epsilon_{H}<0.001\) and for any \(x,z\in\mathcal{K}\), \(\|x-z\|_{\Phi(x)}<0.001\), then we have_

\[\mathrm{TV}(\widetilde{P}_{x},\widetilde{P}_{z})\leq 0.99.\]

Proof.: We can upper bound \(\mathrm{TV}(\widetilde{P}_{x},\widetilde{P}_{y})\) as follows:

\[\mathrm{TV}(\widetilde{P}_{x},\widetilde{P}_{y}) \leq\,\mathrm{TV}(\widetilde{P}_{x},P_{x})+\mathrm{TV}(P_{x},P_ {y})+\mathrm{TV}(P_{y},\widetilde{P}_{y})\] \[\leq\,\mathrm{TV}(\widetilde{P}_{x},P_{x})+0.9+\mathrm{TV}(P_{y },\widetilde{P}_{y})\] \[\leq 0.01+0.9+0.01\] \[\leq 0.99\]

First step is by triangle inequality. Second step is by Lemma D.8. The third step is by Lemma C.7. 

### Bounding TV distance between Gaussians

**Lemma D.10** (Lemma E.6 in Mangoubi and Vishnoi (2023)).: _For any \(x,z\in\mathcal{K}\) such that \(\|x-z\|_{\Phi(x)}\leq 0.001\), we have_

\[\mathrm{TV}(\mathcal{N}(x,\Phi^{-1}(x)),\mathcal{N}(z,\Phi^{-1}(z)))\leq\sqrt {3d\alpha+1/2}\cdot\|x-z\|_{\Phi(x)}.\]

_Further, if \(\alpha d<0.001\), then we have_

\[\mathrm{TV}(\mathcal{N}(x,\Phi^{-1}(x)),\mathcal{N}(z,\Phi^{-1}(z)))\leq 0.001.\]

**Lemma D.11** (Robust version of Lemma D.10).: _For any \(x,z\in\mathcal{K}\) such that \(\|x-z\|_{\Phi(x)}\leq\frac{1}{4\alpha^{1/2}}\), with probability at least \(1-1/1000\), we have_

\[\mathrm{TV}(\mathcal{N}(x,\widetilde{\Phi}^{-1}(x)),\mathcal{N}(z,\widehat{ \Phi}^{-1}(z)))\leq\sqrt{2d\epsilon_{H}}+\sqrt{3d\alpha+1/2}\cdot\|x-z\|_{\Phi (x)}\]

Proof.: By triangle inequality,

\[\mathrm{TV}(\mathcal{N}(x,\widetilde{\Phi}^{-1}(x)),\mathcal{N}( z,\widehat{\Phi}^{-1}(z)))\] \[\leq\,\mathrm{TV}(\mathcal{N}(x,\widetilde{\Phi}^{-1}(x)), \mathcal{N}(x,\Phi^{-1}(x)))\] \[+\,\mathrm{TV}(\mathcal{N}(x,\Phi^{-1}(x)),\mathcal{N}(z,\Phi^{- 1}(z)))\] \[+\,\mathrm{TV}(\mathcal{N}(z,\Phi^{-1}(z)),\mathcal{N}(z,\widehat{ \Phi}^{-1}(z))).\]

Second term is bounded using Lemma D.10. First and third term are bounded using similar ways. We have

\[\mathrm{TV}(\mathcal{N}(x,\widetilde{\Phi}^{-1}(x)),\mathcal{N}( x,\Phi^{-1}(x)))^{2}\] \[\leq\frac{1}{2}D(\mathcal{N}(x,\widetilde{\Phi}^{-1}(x))\| \mathcal{N}(x,\Phi^{-1}(x)))\] \[=\frac{1}{4}(\mathrm{tr}[\widetilde{\Phi}(x)^{-1/2}\Phi(x) \widetilde{\Phi}(x)^{-1/2}]-d+\log\frac{\det\widetilde{\Phi}(x)}{\det\Phi(x)})\] \[\leq\frac{1}{4}((1\pm\epsilon_{H})d-d+\log(1\pm\epsilon_{H})^{d})\] \[\leq\frac{1}{2}d\epsilon_{H}.\]

Thus, we complete the proof.

### Reversibility and stationary distribution

For any \(x\in\mathcal{K}\), we define the random variable \(Z_{x}\) to be the step taken by the Markov chain in Algorithm 2 from the point \(x\), that is, set \(z=x+\widetilde{\Phi}(x)^{-1/2}\xi\) where \(\xi\sim\mathcal{N}(0,I_{d})\). If \(z\in\mathcal{K}\), set \(Z_{x}=z\) with probability

\[\frac{1}{2}\cdot\min\Big{\{}\frac{\exp(-f(z))\cdot(\det(\widehat{\Phi}(z)))^{ 1/2}\cdot\exp(-0.5\|x-z\|_{\widetilde{\Phi}(z)}^{2})}{\exp(-f(x))\cdot(\det( \widetilde{\Phi}(x)))^{1/2}\cdot\exp(-0.5\|x-z\|_{\widetilde{\Phi}(x)}^{2})},1 \Big{\}}.\]

Else, we set \(z=x\).

We provide a modified version of Proposition E.12 in Mangoubi and Vishnoi (2023) where our Markov chain differs from theirs.

**Lemma D.12** (Reversibility and stationary distribution).: _For any \(S_{1},S_{2}\subseteq\mathcal{K}\) we have that_

\[\int_{x\in S_{1}}\pi(x)\widetilde{P}_{x}(S_{2})\mathrm{d}x=\int_{y\in S_{2}} \pi(y)\widetilde{P}_{y}(S_{1})\mathrm{d}y.\]

Proof.: Let \(a,b\) be two i.i.d. random variables (one can view \(a\) and \(b\) as random coins to generate the corresponding sparsifers) such that \(\widetilde{\Phi}_{a,x}\) is a function of \(a,x\) and \(\widehat{\Phi}_{b,y}\) is a function of \(b,y\).

Let \(p_{a,x}(\cdot)\) be pdf of \(\mathcal{N}(0,\widetilde{\Phi}_{a,x}^{-1})\) and \(p_{b,y}(\cdot)\) be the pdf of \(\mathcal{N}(0,\widehat{\Phi}_{b,y}^{-1})\). Because \(a\) and \(b\) are i.i.d, they are interchangeable, i.e., \((a,b)\) has the same distribution as \((b,a)\).

\[\int_{x\in S_{1}}\pi(x)\widetilde{P}_{x}(S_{2})\mathrm{d}x\] \[= \int_{x\in S_{1}}\pi(x)\int_{a}q(a)\int_{y\in S_{2}}p_{a,x}(y) \int_{b}q(b)\cdot\min\{\frac{\pi(y)p_{b,y}(x)}{\pi(x)p_{a,x}(y)},1\}\mathrm{d }b\;\mathrm{d}y\;\mathrm{d}a\;\mathrm{d}x\] \[= \int_{a}q(a)\int_{b}q(b)\int_{x\in S_{1}}\int_{y\in S_{2}}\min\{ \pi(y)p_{b,y}(x),\pi(x)p_{a,x}(y)\}\mathrm{d}y\;\mathrm{d}x\;\mathrm{d}b\; \mathrm{d}a\] \[= \int_{y\in S_{2}}\int_{a}q(a)\int_{x\in S_{1}}\int_{b}q(b)\min\{ \pi(y)p_{a,y}(x),\pi(x)p_{b,x}(y)\}\mathrm{d}b\;\mathrm{d}x\;\mathrm{d}a\; \mathrm{d}y\] \[= \int_{y\in S_{2}}\pi(y)\widetilde{P}_{y}(S_{1})\mathrm{d}y.\]

The first and fifth steps are by definition of \(\widetilde{P}_{x}\). The second and fourth steps are by changing order of integration. The third step is by interchangeability of \(a\) and \(b\).

This proves reversibility and stationary distribution. 

### Lower bound on cross ratio distance

**Definition D.13**.: _We define_

\[\mathsf{F}(n,\alpha,\eta,R):=0.1/\sqrt{n\alpha^{-1}+\eta^{-1}R^{2}}.\]

_For simplicity, we use \(\mathsf{F}\) to denote \(\mathsf{F}(n,\alpha,\beta,R)\)._

**Lemma D.14**.: _Let \(S_{1}^{\prime}\) and \(S_{2}^{\prime}\) be defined as Definition C.8. Le \(\mathsf{F}\) be defined as Definition D.13. We have_

\[\sigma(S_{1}^{\prime},S_{2}^{\prime})>1000\mathsf{F}.\]

Proof.: Using Lemma D.17, we have that for any \(u,v\in\mathcal{K}\),

\[\sigma(u,v)\geq\mathsf{F}\cdot\|u-v\|_{\Phi(u)}.\]We will try to prove \(\sigma(u,v)>1000\text{F}\) in the following. To do that, we will prove it by making a contradiction.

Suppose \(\sigma(u,v)\leq 1000\text{F}\), then we have to require \(\|u-v\|_{\Phi(u)}\leq 0.001\).

Once we had that \(\|u-v\|_{\Phi(u)}\leq 0.001\), using Lemma D.9, for any \(u,v\in\mathcal{K}\) we have that

\[\mathrm{TV}(\widetilde{P}_{u},\widetilde{P}_{v})\leq 0.99\] (13)

On the other hand, Definition C.8 implies that, for any \(u\in S^{\prime}_{1}\), \(v\in S^{\prime}_{2}\) we have that

\[\mathrm{TV}(\widetilde{P}_{u},\widetilde{P}_{v})\geq 1-\widetilde{P}_{u}(S_{ 2})-\widetilde{P}_{v}(S_{1})\geq 1-2\beta>0.99.\] (14)

where the last step follows from \(1-2\beta>0.99\).

Thus, we obtain a contradiction, which means that

\[\sigma(S^{\prime}_{1},S^{\prime}_{2})\geq\sigma(u,v)>1000\text{F}.\qed\]

### Bounding cross ratio distance through \(\overline{\nu}\)-symmetry

In Lemma E.2 of Mangoubi and Vishnoi (2023), they only prove the cross ratio distance bound for log-barrier function. Here, we generalize it to arbitrary barrier function. The key concept our proof relies on is the notion of \(\overline{\nu}\)-symmetry (Assumption (i)).

**Lemma D.15**.: _Let \(H\in\mathcal{K}\to\mathbb{R}^{d\times d}\) be a \(\overline{\nu}\)-symmetric function, and \(u,v\in\mathcal{K}\). Consider the chord that passes through \(u,v\) and intersects with \(\mathcal{K}\). Let \(p,q\) be the two endpoints of the chord, with the order \(p,u,v,q\). Then, we have_

\[\|p-u\|_{H(u)}\leq\sqrt{\overline{\nu}}.\]

Proof.: Suppose we can show that \(p\in\mathcal{K}\cap(2u-\mathcal{K})\), then it naturally follows that \(p\in E_{u}(\sqrt{\overline{\nu}})\) and \(\|p-u\|_{H(u)}\leq\sqrt{\overline{\nu}}\).

\(p\in\mathcal{K}\) trivially follows by the construction of the chord. To see \(p\in 2u-\mathcal{K}\), it is enough to show that for some \(y\in\mathcal{K}\), we have \(p=2u-y\). We claim \(y=2u-p\) is such a choice. To see \(2u-p\in\mathcal{K}\), we need to show that \(2Au-Ap\leq b\). We partition the constraints into two sets.

**Case 1.** Suppose for \(a_{i}\), we have \(a_{i}^{\top}u\leq a_{i}^{\top}p\), then \(2a_{i}^{\top}u\leq 2a_{i}^{\top}p\leq a_{i}^{\top}p+b_{i}\), therefore, \(2a_{i}^{\top}u-a_{i}^{\top}p\leq b_{i}\).

**Case 2.** Suppose otherwise, \(a_{i}^{\top}u>a_{i}^{\top}p\), then consider

\[2a_{i}^{\top}u-a_{i}^{\top}p-b_{i} =(a_{i}^{\top}u-a_{i}^{\top}p)+(a_{i}^{\top}u-b_{i})\] \[=\underbrace{(a_{i}^{\top}u-b_{i})-(a_{i}^{\top}p-b_{i})}_{d_{1}} \underbrace{-(b_{i}-a_{i}^{\top}u)}_{d_{2}}.\]

We shall show that \(d_{1}\leq d_{2}\) via a geometric argument. Consider the projection of both \(p\) and \(u\) onto the hyperplane \(a_{i}x=b_{i}\), and the two chords that pass through \(p\) and \(u\) respectively, orthogonal to the hyperplane. Let us denote them with \(l_{p},l_{u}\) respectively. Observe that \(l_{p}\) and \(l_{u}\) are parallel, and \(\|l_{p}-l_{u}\|_{2}=(a_{i}^{\top}u-b_{i})-(a_{i}^{\top}p-b_{i})=d_{1}\). It should then be obvious that \(d_{1}\leq d_{2}\): let \(\mathrm{proj}(u)\) denote the orthogonal projection of \(u\) onto the hyperplane, then

\[\underbrace{\|l_{u}-\mathrm{proj}(u)\|_{2}}_{d_{2}}=\underbrace{\|l_{u}-l_{p} \|_{2}+\|l_{p}-\mathrm{proj}(p)\|_{2}}_{d_{1}}\]

since \(\|l_{p}-\mathrm{proj}(p)\|_{2}\geq 0\), we have that \(d_{1}\leq d_{2}\), and consequently,

\[2a_{i}^{\top}u-a_{i}^{\top}p\leq b_{i}.\]

This proves that \(2u-p\in\mathcal{K}\), and we can conclude that \(p\in 2u-\mathcal{K}\), which yields our desired result. 

**Remark D.16**.: _The above argument can be extended to spectrahedra and more general convex sets._Let \(u,v\in\mathcal{K}\) be two arbitrary points, and consider the chord that passes through \(u,v\) with two endpoints being \(p,q\). Note that \(p,q\in\partial\mathcal{K}\).

**Lemma D.17**.: _For any \(u,v\in\mathcal{K}\), let \(H\) be any matrix function satisfying Assumption (i), (ii) and (iii). We assume that \(\mathcal{K}\) is contained in ball of radius \(R\) and has nonempty interior._

_For any parameter \(\alpha\in(0,1),\eta\in(0,1)\), we define matrix function \(\Phi(u):=\alpha^{-1}\cdot H(u)+\eta^{-1}I_{d}\). Then, we have_

\[\sigma(u,v)\geq\frac{1}{\sqrt{2\nu\alpha^{-1}+\eta^{-1}R^{2}}}\cdot\|u-v\|_{ \Phi(u)}\]

Proof.: Without loss of generality, we can assume that

\[\|p-u\|_{2}\leq\|u-q\|_{2}.\] (15)

We can lower bound \(\sigma^{2}(u,v)\) as follows:

\[\sigma^{2}(u,v) =(\frac{\|u-v\|_{2}^{2}\cdot\|p-q\|_{2}^{2}}{\|p-u\|_{2}^{2}\cdot \|v-q\|_{2}^{2}})\] \[\geq\max\{\frac{\|u-v\|_{2}^{2}}{\|p-u\|_{2}^{2}},\frac{\|u-v\|_ {2}^{2}}{\|v-q\|_{2}^{2}}\}\] \[\geq\max\{\frac{\|u-v\|_{2}^{2}}{\|p-u\|_{2}^{2}},\frac{\|u-v\|_ {2}^{2}}{\|u-q\|_{2}^{2}}\}\] \[\geq\max\{\frac{\|u-v\|_{2}^{2}}{\|p-u\|_{2}^{2}},\frac{\|u-v\|_ {2}^{2}}{\|p-q\|_{2}^{2}},\frac{\|u-v\|_{2}^{2}}{\|p-q\|_{2}^{2}}\}\] \[=\max\{\frac{\|u-v\|_{2}^{2}}{\|p-u\|_{2}^{2}},\frac{\|u-v\|_{2}^ {2}}{\|p-q\|_{2}^{2}}\}\] \[\geq\frac{1}{2}\frac{\|u-v\|_{2}^{2}}{\|p-u\|_{2}^{2}}+\frac{1}{2 }\frac{\|u-v\|_{2}^{2}}{\|p-q\|_{2}^{2}}\] \[\geq\frac{1}{2}\frac{\|u-v\|_{H}^{2}}{\|p-u\|_{H}^{2}}+\frac{1}{2 }\frac{\|u-v\|_{2}^{2}}{R^{2}}\] \[\geq\frac{1}{2}\frac{\|u-v\|_{H}^{2}}{\nu}+\frac{1}{2}\frac{\|u- v\|_{2}^{2}}{R^{2}}\] \[=(u-v)^{\top}(\frac{1}{2\nu\alpha^{-1}}\times\alpha^{-1}H+\frac{ 1}{2R^{2}\eta^{-1}}\times\eta^{-1}I_{d})(u-v)\] \[\geq\frac{1}{2\nu\alpha^{-1}+2\eta^{-1}R^{2}}\|u-v\|_{\Phi(u)}^{2}\]

where the second step is by \(\|p-q\|_{2}\geq\max\{\|p-u\|_{2},\|v-q\|_{2}\}\), the third step is by \(\|v-q\|_{2}\leq\|u-q\|_{2}\), the fourth step is by \(\|p-q\|_{2}\geq\|p-u\|_{2}\), the fifth step follows from \(\|p-u\|_{2}\leq\|u-q\|_{2}\) (see Eq. (15)), sixth step is by \(\max\{A,B\}\geq 0.5\cdot(A+B)\), where the seventh step follows from \(\|p-q\|_{2}\leq R\) and Eq. (16). The eighth step is due to Lemma D.15.

It remains to show Eq. (16). Due to the fact that \(p,u,v\) are on the same chord, vector \(u-v\) and \(p-u\) are on the same direction. This means we can write \(u-v=c\cdot(p-u)\) for some \(c\in\mathbb{R}\), and

\[\frac{\|u-v\|_{H}}{\|p-u\|_{H}} =\frac{\|c\cdot(p-u)\|_{H}}{\|p-u\|_{H}}\] \[=c\] \[=\frac{\|u-v\|_{2}}{\|p-u\|_{2}}.\] (16)

This completes the proof.

Computing Approximate Lewis Weights and Leverage Scores

In Section E.1, we present an algorithm to approximate the Hessian. In Section E.2, we show how to compute leverage score to low precision. In Section E.3, we show how to compute Lewis weights to high precision.

### Subsampling to approximate the Hessian

```
1:procedureSubsample(\(A\in\mathbb{R}^{n\times d},b\in\mathbb{R}^{n},x,n,d,\epsilon_{H}\in(0,1)\))
2:\(\epsilon_{\sigma}\gets 0.001\)
3:\(m\leftarrow\epsilon_{H}^{-2}d\log d\)
4: Form \(B\in\mathbb{R}^{n\times d}\) where \(i\)-th row of \(B\) is \(a_{i}\cdot(\langle a_{i},x\rangle-b_{i})^{-1}\)
5: Compute the \(O(1\pm\epsilon_{\sigma})\)-approximation to the leverage score of \(B\)
6: Sample a matrix \(\widetilde{B}\in\mathbb{R}^{m\times d}\) such that \((1-\epsilon_{H})B^{\top}B\preceq\widetilde{B}^{\top}\widetilde{B}\preceq(1+ \epsilon_{H})B^{\top}B\)
7:return\(\widetilde{B}^{\top}\widetilde{B}\)
8:endprocedure ```

**Algorithm 3** Subsampling Algorithm

In this section, we provide a sparsification result for \(H(w)\) using leverage score sampling.

To better monitor the whole process, it is useful to write \(H(w)\) as \(A^{\top}S(w)^{-2}A\), where \(A\in\mathbb{R}^{n\times d}\) is the constraint matrix and \(S(w)\) is a diagonal matrix with \(S(w)_{i}=a_{i}^{\top}w-b_{i}\). The sparsification process is then sample the rows from the matrix \(S(w)^{-1}A\).

**Definition E.1**.: _Let \(B\in\mathbb{R}^{n\times d}\) be a full rank matrix. We define the leverage score of the \(i\)-th row of \(B\) as_

\[\sigma_{i}(B):=b_{i}^{\top}(B^{\top}B)^{-1}b_{i},\]

_where \(b_{i}\) is the \(i\)-th row of \(B\)._

**Definition E.2** (Sampling process).: _For any \(w\in K\), let \(H(w)=A^{\top}S(w)^{-2}A\). Let \(p_{i}\geq\frac{\beta\cdot\sigma_{i}(S^{-1}(w)A)}{d}\), suppose we sample with replacement independently for \(s\) rows of matrix \(S(w)^{-1}A\), with probability \(p_{i}\) of sampling row \(i\) for some \(\beta\geq 1\). Let \(i(j)\) denote the index of the row sampled in the \(j\)-th trial. Define the generated sampling matrix as_

\[\widetilde{H}(w):=\frac{1}{s}\sum_{j=1}^{s}\frac{1}{p_{i(j)}}\frac{a_{i(j)}a_{ i(j)}^{\top}}{(a_{i(j)}^{\top}w-b_{i(j)})^{2}}.\]

**Lemma E.3** (Sample using Matrix Chernoff).: _Let \(\epsilon,\delta\in(0,1)\) be precision and failure probability parameters, respectively. Suppose \(\widetilde{H}(w)\) is generated as in Definition E.2, then with probability at least \(1-\delta\), we have_

\[(1-\epsilon)\cdot H(w)\preceq\widetilde{H}(w)\preceq(1+\epsilon)\cdot H(w).\]

_Moreover, the number of rows \(s=\Theta(\beta\cdot\epsilon^{-2}d\log(d/\delta))\)._

Proof.: The proof will be designing a family of random matrices \(X\). Let \(y_{i}=(A^{\top}S(w)^{-2}A)^{-1/2}S(w)_{i,i}^{-1}\cdot a_{i}\) be the \(i\)-th sampled row and set \(Y_{i}=\frac{1}{p_{i}}y_{i}y_{i}^{\top}\). Let \(X_{i}=Y_{i}-I_{d}\). Note that

\[\sum_{i=1}^{n}y_{i}y_{i}^{\top} = \sum_{i=1}^{n}(A^{\top}S(w)^{-2}A)^{-1/2}S(w)_{i,i}^{-2}\cdot a_{i }a_{i}^{\top}(A^{\top}S(w)^{-2}A)^{-1/2}\] (17) \[= (A^{\top}S(w)^{-2}A)^{-1/2}(\sum_{i=1}^{n}S(w)_{i,i}^{-2}a_{i}a_{ i}^{\top})(A^{\top}S(w)^{-2}A)^{-1/2}\] \[= (A^{\top}S(w)^{-2}A)^{-1/2}(A^{\top}S(w)^{-2}A)(A^{\top}S(w)^{-2} A)^{-1/2}\] \[= I_{d}.\]Also, the norm of \(y_{i}\) connects directly to the leverage score:

\[\|y_{i}\|_{2}^{2} =S(w)_{i,i}^{-1}a_{i}^{\top}(A^{\top}S(w)^{-2}A)^{-1}S(w)_{i,i}^{-1}a_ {i}\] \[=\sigma_{i}(S^{-1}A).\] (18)

We use \(i(j)\) to denote the index of row that has been sampled during \(j\)-th trial.

**Unbiased Estimator.** Note that

\[\mathbb{E}[X] =\,\mathbb{E}[Y]-I_{d}\] \[=(\sum_{i=1}^{n}p_{i}\cdot\frac{1}{p_{i}}y_{i}y_{i}^{\top})-I_{d}\] \[=0.\]

**Bound on \(\|X\|\).** To bound \(\|X\|\), we provide a bound for any \(\|X_{i}\|\) as follows:

\[\|X_{i}\| =\|Y_{i}-I_{d}\|\] \[\leq 1+\|Y_{i}\|\] \[=1+\frac{\|y_{i}y_{i}^{\top}\|}{p_{i}}\] \[\leq 1+\frac{\|y_{i}\|_{2}^{2}}{\beta\cdot\sigma_{i}(S^{-1}(w)A)}\] \[=\frac{d}{\beta\cdot\sigma_{i}(S^{-1}(w)A)}\|(S(w)A)_{i}\|_{2}^{2 }+1.\] \[=1+\frac{d}{\beta}.\]

**Bound on \(\|\mathbb{E}[X^{\top}X]\|\).** We compute the spectral norm of the covariance matrix:

\[\mathbb{E}[X_{i(j)}^{\top}X_{i(j)}] =I_{d}+\mathbb{E}[\frac{y_{i(j)}y_{i(j)}^{\top}y_{i(j)}y_{i(j)}^{ \top}}{p_{i}^{2}}]-2\,\mathbb{E}[\frac{y_{i(j)}y_{i(j)}^{\top}}{p_{i}}]\] \[=I_{d}+(\sum_{i=1}^{n}\frac{\sigma_{i}(S(w)^{-1}A)}{p_{i}}y_{i}y_ {i}^{\top})-2I_{d}\] \[\leq\sum_{i=1}^{n}\frac{d}{\beta}y_{i}y_{i}^{\top}-I_{d}\] \[=(\frac{d}{\beta}-1)I_{d},\]

the spectral norm is then

\[\|\,\mathbb{E}[X_{i(j)}^{\top}X_{i(j)}]\|\leq\frac{d}{\beta}-1.\]

**Put things together.** Set \(\gamma=1+\frac{d}{\beta}\) and \(\sigma^{2}=\frac{d}{\beta}-1\), we apply Matrix Chernoff Bound as in Lemma A.8:

\[\Pr[\|W\|\geq\epsilon] \leq 2d\cdot\exp\left(-\frac{s\epsilon^{2}}{d/\beta-1+(1+d/\beta) \epsilon/3}\right)\] \[=2d\cdot\exp(-s\epsilon^{2}\cdot\Theta(\beta/d))\] \[\leq\delta\]

where we choose \(s=\Theta(\beta\cdot\epsilon^{-2}d\log(d/\delta))\). Finally, we notice that

\[W=\frac{1}{s}(\sum_{j=1}^{s}\frac{1}{p_{i(j)}}y_{i(j)}y_{i(j)}^{\top}-I_{d})\]\[=(A^{\top}S(w)^{-2}A)^{-1/2}(\frac{1}{s}\sum_{j=1}^{s}\frac{1}{p_{i(j)}} \frac{a_{i(j)}a_{i(j)}^{\top}}{(a_{i(j)}^{\top}w-b_{i(j)})^{2}})(A^{\top}S(w)^{-2} A)^{-1/2}-I_{d}\] \[=H(w)^{-1/2}\widetilde{H}(w)H(w)^{-1/2}-I_{d}.\]

Therefore, we can conclude the desired result via \(\|W\|\geq\epsilon\). 

### Computing leverage score

In this section, we provide an algorithm that approximates leverage scores in near-input sparsity time. Our algorithm makes use of the sparse embedding matrix of Nelson and Nguyen (2013) that has \(O(\log(n/\epsilon))\) nonzero entries per column.

**Lemma E.4** (Approximate leverage scores).: _Let \(A\in\mathbb{R}^{n\times d}\), there exists an algorithm that runs in time \(\widetilde{O}(\epsilon^{-1}(\operatorname{nnz}(A)+\epsilon^{-1}d^{\omega}))\) that outputs a vector \(\widetilde{w}_{2}(A)\in\mathbb{R}^{n}\), such that \(\widetilde{w}_{2}(A)\approx_{\epsilon}w_{2}(A)\)._

Proof.: We follow an approach of Woodruff (2014), but instead we use a high precision sparse embedding matrix Nelson and Nguyen (2013) and prove a two-sided bound on leverage score.

Let \(S\) be a sparse embedding matrix with \(r=O(\epsilon^{-2}d\operatorname{poly}\log(d/(\epsilon\delta)))\) rows and each column has \(O(\epsilon^{-1}\log(nd/\epsilon))\) nonzero entries. We first compute \(SA\) in time \(\widetilde{O}(\epsilon^{-1}\operatorname{nnz}(A))\), then compute the QR decomposition \(SA=QR\) in time \(\widetilde{O}(\epsilon^{-2}d^{\omega})\). Note that \(R\in\mathbb{R}^{d\times d}\) hence \(R^{-1}\) can be computed in \(O(d^{\omega})\) time.

Now, let \(G\in\mathbb{R}^{d\times t}\) matrix with \(t=O(\epsilon^{-2}\log(n/\delta))\), each entry of \(G\) is i.i.d. \(\mathcal{N}(0,1/t)\) random variables. Set \(q_{i}=\|e_{i}^{\top}AR^{-1}G\|_{2}^{2}\) for all \(i\in[n]\). We argue \(q_{i}\) is a good approximation to \((w_{2}(A))_{i}\).

First, with failure probability at most \(\delta/n\), we have that \(q_{i}\approx_{\epsilon}\|e_{i}^{\top}AR^{-1}\|_{2}^{2}\) via Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984). Now, it suffices to argue that \(\|e_{i}^{\top}AR^{-1}\|_{2}^{2}\) approximates \(\|e_{i}^{\top}U\|_{2}^{2}\) well, where \(U\in\mathbb{R}^{n\times d}\) is the left singular vectors of \(A\). To see this, first observe that for any \(x\in\mathbb{R}^{d}\),

\[\|AR^{-1}x\|_{2}^{2} =(1\pm\epsilon)\cdot\|SAR^{-1}x\|_{2}^{2}\] \[=(1\pm\epsilon)\cdot\|Qx\|_{2}^{2}\] \[=(1\pm\epsilon)\cdot\|x\|_{2}^{2},\]

where the last step is due to \(Q\) has orthonormal columns. This means that all singular values of \(AR^{-1}\) are in the range \([1-\epsilon,1+\epsilon]\). Now, since \(U\) is an orthonormal basis for the column space of \(A\), \(AR^{-1}\) and \(U\) has the same column space (since \(R\) is full rank). This means that there exists a change of basis matrix \(T\in\mathbb{R}^{d\times d}\) with \(AR^{-1}T=U\). Our goal is to provide a bound on all singular values of \(T\). For the upper bound, we claim the largest singular value is at most \(1+2\epsilon\), to see this, suppose for the contradiction that the largest singular is larger than \(1+2\epsilon\) and let \(v\) be its corresponding (unit) singular vector. Since the smallest singular value of \(AR^{-1}\) is at least \(1-\epsilon\), we have

\[\|AR^{-1}Tv\|_{2}^{2} \geq(1-\epsilon)\|Tv\|_{2}^{2}\] \[>(1-\epsilon)(1+2\epsilon)\] \[>1,\]

however, recall \(AR^{-1}T=U\), therefore \(\|AR^{-1}Tv\|_{2}^{2}=\|Uv\|_{2}^{2}=\|v\|_{2}^{2}=1\), a contradiction. One can similarly establish a lower bound of \(1-2\epsilon\). Hence, the singular values of \(T\) are in the range of \([1-2\epsilon,1+2\epsilon]\). This means that

\[\|e_{i}^{\top}AR^{-1}\|_{2}^{2} =\|e_{i}^{\top}UT^{-1}\|_{2}^{2}\] \[=(1\pm 2\epsilon)\|e_{i}^{\top}U\|_{2}^{2}\] \[=(1\pm 2\epsilon)(w_{2}(A))_{i},\]

as desired. Scaling \(\epsilon\) to \(\epsilon/2\) yields the approximation result.

Now, regarding the running time of computing \(q_{i}\), note that we can first multiply \(R^{-1}\) with \(G\) in time \(\widetilde{O}(\epsilon^{-2}d^{2})\), this gives a matrix of size \(d\times t\). Multiplying this matrix with \(A\) takes \(\widetilde{O}(\epsilon^{-1}\operatorname{nnz}(A))\) time. Hence, the overall time for computing \(\widetilde{w}_{2}(A)=q\in\mathbb{R}^{n}\) is \(\widetilde{O}(\epsilon^{-1}\operatorname{nnz}(A)+\epsilon^{-2}d^{\omega})\). 

**Remark E.5**.: _It suffices to choose \(\epsilon=\frac{1}{10}\) for generating approximate leverage scores and subsampling the target matrix \(H(x)\)._

### Computing Lewis weights

Before stating the main technical tool for this section, we want to make several remarks regarding the Lewis weights.

Since its introduction by Cohen and Peng (Cohen and Peng, 2015) as an algorithmic tool for \(\ell_{p}\) row sampling, Lewis weights, as its natural formulation follows from a convex program, is _not known to be solved exactly in polynomial time_. All known algorithms (Cohen and Peng, 2015; Lee and Sidford, 2019; Fazel et al., 2022; Cohen et al., 2019; Jambulapati et al., 2022) can _only compute \(\epsilon\)-approximate Lewis weights_.

**Definition E.6** (\(\ell_{p}\) Lewis weights).: _Let \(A\in\mathbb{R}^{n\times d}\). The \(\ell_{p}\) Lewis weights of \(A\) is a vector \(w_{p}(A)\in\mathbb{R}^{n}\) satisfying_

\[w_{p}(A)=\sigma(W_{p}(A)^{\frac{1}{2}-\frac{1}{p}}A),\]

_where \(W_{p}(A)\in\mathbb{R}^{n\times n}\) is the diagonal matrix that puts \(w_{p}(A)\) on the diagonal, \(\sigma:\mathbb{R}^{n\times d}\to\mathbb{R}^{n}\) is the operation that outputs all leverage scores of input matrix._

We are now posed to state a main tool from Lee and Sidford (2019).

**Theorem E.7** (Theorem 46 in Lee and Sidford (2019)).: _Let \(P=\{x:Ax>b\}\) denote the interior of non-empty polytope for non-degenerate \(A\in\mathbb{R}^{n\times d}\). There is an \(O(d\log^{5}n)\)-self concordant barrier \(\psi\) defined by \(\ell_{p}\) Lewis weight with \(p=\Theta(\log n)\) satisfying_

\[A_{x}^{\top}W_{x}A_{x}\preceq\nabla^{2}\psi(x)\preceq(q+1)A_{x}^{\top}W_{x}A_ {x}\]

_where \(A_{x}=\operatorname{diag}(Ax-b)\) and \(w_{x}\) is the \(\ell_{p}\) Lewis weight of the matrix \(A_{x}\). Furthermore, we can compute or update the \(w_{x}\), \(\nabla\psi(x)\) and \(\nabla^{2}\psi(x)\) as follows:_

* _Initial Weight: For any_ \(x\in\mathbb{R}^{d}\)_, one can compute a vector_ \(\widetilde{w}_{x}\) _such that_ \((1-\epsilon)w_{x}\leq\widetilde{w}_{x}\leq(1+\epsilon)w_{x}\) _in_ \(O(nd^{\omega-1/2}\cdot\log^{3}n\log(n/\epsilon))\) _time._
* _Update Weight and Compute Gradient/Hessian: Given a vector_ \(\widetilde{w}_{x}\) _such that_ \(\widetilde{w}_{x}=(1\pm\frac{1}{100})w_{x}\) _for any_ \(y\) _with_ \(\|x-y\|_{A_{x}^{\top}W_{x}A_{x}}\leq\frac{c}{\log^{2}n}\) _for some small constant_ \(c>0\)_, we can compute_ \(\widetilde{w}_{y}\)_,_ \(v\) _and_ \(H\) _such that_ \(\widetilde{w}_{y}=(1\pm\epsilon)w_{y}\)_,_ \[\|v-\nabla\psi(x)\|_{\nabla^{2}\psi(x)^{-1}}\leq\epsilon,\ \ \ \mathrm{and}\ \ (1-\epsilon)\nabla^{2}\psi(x)\preceq H \preceq(1+\epsilon)\nabla^{2}\psi(x)\] _in_ \(O(nd^{\omega-1}\cdot\log n\cdot\log(n/\epsilon))\) _time._

## Appendix F Fast Approximate Sampling from Polytopes: Complexity

In this section, we provide the runtime analysis for sampling from polytopes via log-barrier and Lee-Sidford barrier. The result for log-barrier is in Section F.1 and for Lee-Sidford barrier is in Section F.2.

### Log-barrier in nearly-linear time

Throughout this section, we let \(\alpha\in(0,1/(10^{5}d))\) and \(\eta\in(0,1/(20dL^{2}))\).

**Lemma F.1**.: _If \(g\) is log-barrier function with \(\nu=n\), then each iteration of Algorithm 2 can be implemented in \(O(\operatorname{nnz}(A)+\mathcal{T}_{\mathrm{mat}}(d,d^{3},d))\) time plus \(O(1)\) calls to the oracle of \(f\)._

Proof.: We go through each step of Algorithm 2 and add up the time and oracle calls for each step:

We first need to sample a \(d\)-dimensional Gaussian random vector \(\xi\sim N(0,I_{d})\), which can be performed in \(O(d)\) time.

At each iteration, by the definition of log-barrier function, we can write the Hessian as follows:

\[H(x):=\sum_{j=1}^{n}\frac{a_{j}a_{j}^{\top}}{(a_{j}^{\top}x-b_{j})^{2}}.\]

We define define matrix \(B\in\mathbb{R}^{n\times d}\) as follows

\[B:=S^{-1}A\]

where \(s_{i}=(\langle a_{i},x\rangle-b_{i})\), \(\forall i\in[n]\). Then we can write \(H(x)=B^{\top}B\in\mathbb{R}^{d\times d}\).

For matrix \(B\), we can compute a constant approximation of its leverage score, then we can sample according to leverage, and generate a diagonal matrix \(D\) such that

\[(1-\epsilon)B^{\top}B\preceq B^{\top}DDB\preceq(1+\epsilon)B^{\top}B.\]

This step takes \(\widetilde{O}(\mathrm{nnz}(A)+d^{\omega(1,3,1)})\) time.

Computing \(\widetilde{\Phi}(x)\) can be done in \(d^{2}\) time and computing the vector \(z\)

\[z=x+\widetilde{\Phi}(x)^{-\frac{1}{2}}\xi\]

requires to invert the matrix \(\Phi(x)\), taking the square root and multiplying with a vector \(\xi\). Inversion and square root can be performed in time \(O(d^{\omega})\) by computing its spectral decomposition, and the matrix-vector product can be done in time \(O(d^{2})\). Hence, the overall time of this step is \(O(d^{\omega})\).

To determine whether \(z\in K\), one can simplify verify \(Az\leq b\), in time \(O(\mathrm{nnz}(A))\).

We also need to compute \(\widehat{H}(z)\) and \(\widehat{\Phi}(z)\). This is similar to computing \(\widetilde{H}(x)\) and \(\widetilde{\Phi}(x)\).

We then need to compute the determinant \(\det(\widetilde{\Phi}(x))\) and \(\det(\widehat{\Phi}(z))\) which can be done in \(O(d^{\omega})\) time via a spectral decomposition.

We then need \(O(1)\) oracle queries to the function values of \(f\).

Therefore, adding up the number of arithmetic operations and oracle calls from all the different steps of Algorithm 2, we get that each iteration of Algorithm 2 can be computed in \(\widetilde{O}(\mathrm{nnz}(A)+\mathcal{T}_{\mathrm{mat}}(d,d^{3},d))\) arithmetic operations plus \(O(1)\) calls to the oracle of \(f\). 

**Remark F.2**.: _We note a recent work Mangoubi and Vishnoi (2024) has provided an algorithm that runs in \(\mathrm{nnz}(A)+d^{2}\) time per iteration, by adapting techniques from Laddha et al. (2020). Their per iteration cost improvement only works for log-barrier, and they could only manage to obtain an \(\widetilde{O}(nd+dL^{2}R^{2})\) mixing time. Our framework on the other hand generalizes to other barriers such as Lee-Sidford barrier, which provides a nearly-optimal mixing rate. We also adapt the framework to sampling from spectrahedra._

### Lee-Sidford barrier via approximation scheme

**Lemma F.3**.: _If \(g\) is the Lee-Sidford barrierfunction with \(\nu=d\log^{5}n\), then, each iteration of Algorithm 2 can be implemented in \(\widetilde{O}(nd^{\omega-1})\) time plus \(O(1)\) calls to the oracle of \(f\)._

Proof.: For Lee-Sidford barrier, it suffices to use \(H(x)=A_{x}^{\top}W_{x}A_{x}\) as where \(W_{x}\) is the \(\ell_{p}\) Lewis weights for \(p=\Theta(\log m)\). However, as Lewis weights cannot be computed exactly, we shall use the algorithm of Lee and Sidford (2019) to compute an \(\epsilon\)-approximation.

To invoke Theorem E.7, we need to make sure that \(z\) and \(x\) satisfy \(\|z-x\|_{A_{x}^{\top}W_{x}A_{x}}\leq\frac{c}{\log^{2}n}\). We can achieve so by scaling down \(\alpha\) by a factor of \(\log^{2}n\). This only blows up the convergence by \(\widetilde{O}(1)\) factor, so it is acceptable.

The runtime analysis is then similar to Lemma F.1. Computing the initial Lewis weights takes \(\widetilde{O}(nd^{\omega-1/2}\log(1/\epsilon))\) time owing to Theorem E.7. As the algorithm requires \(\widetilde{O}(d^{2})\) iterations, this time can be amortized. For each iteration, it needs to query the Lewis weights data structure that outputs an \(\epsilon\)-Lewis weights in \(\widetilde{O}(nd^{\omega-1}\log(1/\epsilon))\) time. As we will choose \(\epsilon\) as \(O(1/d)\), this time is \(\widetilde{O}(nd^{\omega-1})\). We can then use this approximate Hessian to progress the algorithm. All subsequent operations take \(O(d^{\omega})\) time. Thus, the cost per iteration is \(\widetilde{O}(nd^{\omega-1})\)Approximate Sampling from a Spectrahedron

We present an algorithm that efficiently and approximately samples from a spectrahedron, which is a popular convex body utilized by semidefinite programs.

### Definitions

**Definition G.1**.: _Let \(A_{1},\ldots,A_{d}\in\mathbb{R}^{n\times n}\) be a collection of symmetric matrices and \(C\in\mathbb{R}^{n\times n}\) be symmetric. We define the corresponding spectrahedron as_

\[\mathcal{K}=\{x\in\mathbb{R}^{d}:\sum_{i=1}^{d}x_{i}A_{i}\succeq C\}.\]

We define the log-barrier for spectrahedron and its corresponding Hessian.

**Definition G.2**(Nesterov and Nemirovskii [1994]).: _Let \(\mathcal{K}\) be a spectrahedron described by symmetric matrices \(\{A_{1},\ldots,A_{d}\}\subseteq\mathbb{R}^{n\times n}\) and \(C\in\mathbb{R}^{n\times n}\). The log barrier \(\phi_{\log}(x)\) is defined as_

\[\phi_{\log}(x)=\ -\log\det(S(x))\]

_and its corresponding Hessian is defined as_

\[H_{\log}(x)=\mathsf{A}(S(x)^{-1}\otimes S(x)^{-1})\mathsf{A}^{\top},\]

_where \(S(x):=\sum_{i=1}^{d}x_{i}A_{i}-C\in\mathbb{R}^{n\times n}\) and \(\mathsf{A}\in\mathbb{R}^{d\times n^{2}}\) and the \(i\)-th row of \(\mathsf{A}\) is the vectorization of \(A_{i}\in\mathbb{R}^{n\times n}\)._

To compute \(H_{\log}(x)\), it is handy to define a matrix \(\mathsf{B}\in\mathbb{R}^{d\times n^{2}}\).

**Definition G.3**.: _We define a matrix \(\mathsf{B}\in\mathbb{R}^{d\times n^{2}}\) as \(\mathsf{B}=\mathsf{A}(S(x)^{-1/2}\otimes S(x)^{-1/2})\). Consequently, \(H_{\log}(x)=\mathsf{B}\mathsf{B}^{\top}\)._

We state a useful lemma for computing the matrix \(\mathsf{B}\).

**Lemma G.4**.: _Let matrix \(\mathsf{B}\in\mathbb{R}^{d\times n^{2}}\) be defined as in Def. G.3. Then, the \(i\)-th row of \(\mathsf{B}\) can be computed as \(\operatorname{vec}(S(x)^{-1/2}A_{i}S(x)^{-1/2})\)._

Proof.: The proof relies on a simple fact of Kronecker product and vectorization:

\[\operatorname{vec}(S(x)^{-1/2}A_{i}S(x)^{-1/2})=(S(x)^{-1/2} \otimes S(x)^{-1/2})\operatorname{vec}(A_{i}),\]

which is the definition of the \(i\)-th row of \(\mathsf{B}\). 

### \(n\)-symmetry of \(H_{\log}\) for spectrahedra

In this section, we prove that \(H_{\log}\) is \(n\)-symmetry for spectrahedra.

**Lemma G.5**.: \(H_{\log}(x)\) _is \(n\)-symmetry, that is, for any \(x\in\mathcal{K}\),_

\[E_{x}(1)\subseteq\mathcal{K}\cap(2x-\mathcal{K})\subseteq E_{x}( \sqrt{n}).\]

Proof.: Pick a point \(y\in E_{x}(1)\), then we know that \((y-x)^{\top}\mathsf{A}(S(x)^{-1}\otimes S(x)^{-1})\mathsf{A}^{\top}(y-x)\leq 1\). We note that the set \(\mathcal{K}\cap(2x-\mathcal{K})\) can be characterized as the set of all points \(u\in\mathbb{R}^{d}\) such that

\[\sum_{i=1}^{d}u_{i}A_{i} \succeq C,\] \[\sum_{i=1}^{d}(2x-u)_{i}A_{i} \succeq C.\]

These two conditions imply

\[-S(x)\preceq\sum_{i=1}^{d}(u_{i}-x_{i})A_{i}\preceq S(x),\]\[-I_{n}\preceq\sum_{i=1}^{d}(u_{i}-x_{i})S(x)^{-1/2}A_{i}S(x)^{-1/2}\preceq I_{n}.\]

We will prove \(y\) satisfies these conditions. Note that \(y\in E_{x}(1)\) implies that

\[\|\sum_{i=1}^{d}(y_{i}-x_{i})\operatorname{vec}(S(x)^{-1/2}A_{i}S(x)^{-1/2})\| _{2}^{2}\leq 1,\]

\[\|\sum_{i=1}^{d}(y_{i}-x_{i})S(x)^{-1/2}A_{i}S(x)^{-1/2}\|_{F}^{2}\leq 1\]

if we let \(M\) denote \(\sum_{i=1}^{d}(y_{i}-x_{i})S(x)^{-1/2}A_{i}S(x)^{-1/2}\), then the above condition implies that \(M^{\top}M\preceq I_{n}\), meaning all the eigenvalues of \(M\) lie between \([-1,1]\), thus we have shown that \(E_{x}(1)\subseteq\mathcal{K}\cap(2x-\mathcal{K})\).

For the other direction, we know that

\[-I_{n}\preceq\sum_{i=1}^{d}(y_{i}-x_{i})S(x)^{-1/2}A_{i}S(x)^{-1/2}\preceq I_ {n},\]

and henceforth

\[(y-x)^{\top}\mathsf{A}(S(x)^{-1}\otimes S(x)^{-1})\mathsf{A}^{ \top}(y-x)\] \[=\|\sum_{i=1}^{d}(y_{i}-x_{i})S(x)^{-1/2}A_{i}S(x)^{-1/2}\|_{F}^{2}\] \[\leq\|I_{n}\|_{F}^{2}\] \[=n,\]

where we use the fact that \(-I_{n}\preceq M\preceq I_{n}\) hence all eigenvalues of \(M\) lie between \([-1,1]\), and the squared eigenvalues of \(M\) have its magnitude at most 1. We then use the squared Frobenius norm is equivalent to squared \(\ell_{2}\) norm of singular values of \(M\), as desired. 

### Bounded local norm of \(H_{\log}(x)\)

We prove that \(H_{\log}(x)\) has the bounded local norm property.

**Lemma G.6**.: _Let \(H_{\log}(x)\in\mathbb{R}^{d\times d}\) be defined as in Def. G.2. Let \(F(x)=\log\det(H_{\log}(x))\). Then, \(F(x)\) is convex in \(x\)._

Proof.: The proof is by observe that the function \(F(x)\) is actually the volumetric barrier function for the SDP spectrahedron. Thus, the function \(F\) is convex, for more details, see Nesterov and Nemirovskii (1994). 

**Lemma G.7**.: _Let \(F(x)\) be defined as in Lemma G.6. Then, we have_

\[\nabla F(x)=\begin{bmatrix}-\operatorname{tr}[\mathsf{P}((S(x)^{-1/2}A_{1}S(x )^{-1/2})\otimes_{S}(S(x)^{-1/2}A_{1}S(x)^{-1/2}))]\\ -\operatorname{tr}[\mathsf{P}((S(x)^{-1/2}A_{2}S(x)^{-1/2})\otimes_{S}(S(x)^{ -1/2}A_{2}S(x)^{-1/2}))]\\ \vdots\\ -\operatorname{tr}[\mathsf{P}((S(x)^{-1/2}A_{m}S(x)^{-1/2})\otimes_{S}(S(x)^{ -1/2}A_{m}S(x)^{-1/2}))]\end{bmatrix}\]

_where \(\mathsf{P}=\mathsf{B}^{\top}H(x)^{-1}\mathsf{B}\)._

Proof.: For simplicity, we let \(H\) to denote \(H_{\log}\). For each coordinate \(j\in[d]\), we have

\[\frac{\partial F}{\partial x_{j}} = \operatorname{tr}[H(x)^{-1}\frac{\partial H(x)}{\partial x_{j}}]\] \[= \operatorname{tr}[H(x)^{-1}\mathsf{A}\frac{\partial(S(x)^{-1} \otimes S(x)^{-1})}{\partial x_{j}}\mathsf{A}^{\top}]\]\[= \operatorname{tr}[H(x)^{-1}\mathsf{A}((S(x)^{-1}A_{j}S(x)^{-1}) \otimes S(x)^{-1}+S(x)^{-1}\otimes(S(x)^{-1}A_{j}S(x)^{-1}))\mathsf{A}^{\top}]\] \[= \operatorname{tr}[H(x)^{-1}\mathsf{A}((S(x)^{-1}A_{j}S(x)^{-1}) \otimes S(x)^{-1}+S(x)^{-1}\otimes(S(x)^{-1}A_{j}S(x)^{-1}))\mathsf{A}^{\top}],\]

examine the following term:

\[\operatorname{tr}[H(x)^{-1}\mathsf{A}((S(x)^{-1}A_{j}S(x)^{-1}) \otimes S(x)^{-1})\mathsf{A}^{\top}]\] \[= \operatorname{tr}[\mathsf{A}^{\top}H(x)^{-1}\mathsf{A}((S(x)^{-1 }A_{j})\otimes I)(S(x)^{-1}\otimes S(x)^{-1})]\] \[= \operatorname{tr}[(S(x)^{-1/2}\otimes S(x)^{-1/2})\mathsf{A}^{ \top}H(x)^{-1}\mathsf{A}(S(x)^{-1/2}\otimes S(x)^{-1/2})\] \[\cdot((S(x)^{-1/2}A_{j})\otimes S(x)^{1/2})(S(x)^{-1/2}\otimes S (x)^{-1/2})]\] \[= \operatorname{tr}[\mathsf{P}((S(x)^{-1/2}A_{j})\otimes S(x)^{1/ 2})(S(x)^{-1/2}\otimes S(x)^{-1/2})]\] \[= \operatorname{tr}[\mathsf{P}((S(x)^{-1/2}A_{j}S(x)^{-1/2}) \otimes I_{n})],\]

where \(\mathsf{P}\in\mathbb{R}^{n^{2}\times n^{2}}\) is the projection of \(\mathsf{B}^{\top}\), i.e., \(\mathsf{P}=\mathsf{B}^{\top}H(x)^{-1}\mathsf{B}\). Thus,

\[\nabla F(x)=\begin{bmatrix}-\operatorname{tr}[\mathsf{P}((S(x)^{-1/2}A_{1}S( x)^{-1/2})\otimes_{S}(S(x)^{-1/2}A_{1}S(x)^{-1/2}))]\\ -\operatorname{tr}[\mathsf{P}((S(x)^{-1/2}A_{2}S(x)^{-1/2})\otimes_{S}(S(x)^{ -1/2}A_{2}S(x)^{-1/2}))]\\ \vdots\\ -\operatorname{tr}[\mathsf{P}((S(x)^{-1/2}A_{m}S(x)^{-1/2})\otimes_{S}(S(x)^{ -1/2}A_{m}S(x)^{-1/2}))]\end{bmatrix}.\]

This completes the proof. 

**Lemma G.8**.: _The matrix function \(H(x)=\mathsf{A}(S^{-1}\otimes S^{-1})\mathsf{A}^{\top}\in\mathbb{R}^{d\times d}\) has the following bounded norm property: for any direction \(h\in\mathbb{R}^{d}\),_

\[\|H(x)^{-1/2}DH(x)[h]H(x)^{-1/2}\|_{F}^{2}\leq 4d\|h\|_{H(x)}^{2}.\]

Proof.: Let \(x_{t}=x+t\cdot h\) for some fixed vector \(h\in\mathbb{R}^{d}\), \(S_{t}=\sum_{i=1}^{m}x_{t,i}A_{i}-C\in\mathbb{R}^{n\times n}\), \(\mathsf{A}_{t}=\mathsf{A}(S_{t}^{-1/2}\otimes S_{t}^{-1/2})\in\mathbb{R}^{d \times n^{2}}\), \(P_{t}=\mathsf{A}_{t}^{\top}(\mathsf{A}_{t}\mathsf{A}_{t}^{\top})^{-1}\mathsf{ A}_{t}\in\mathbb{R}^{n^{2}\times n^{2}}\) and \(\Sigma_{t}=\sum_{i=1}^{n}(e_{i}^{\top}\otimes I_{n})P_{t}(e_{i}\otimes I_{n}) =\sum_{i=1}^{n}(I_{n}\otimes e_{i}^{\top})P_{t}(I_{n}\otimes e_{i})\). Note that for any matrix \(M\in\mathbb{R}^{n\times n}\), \(\operatorname{tr}[P_{t}\cdot(M\otimes I_{n})]=\operatorname{tr}[P_{t}\cdot(I_{n }\otimes M)]=\operatorname{tr}[\Sigma_{t}\cdot M]\). Also note that \(\operatorname{tr}[\Sigma_{t}]=\operatorname{tr}[P_{t}]=d\).

Let \(H_{t}=\mathsf{A}^{\top}(S_{t}^{-1}\otimes S_{t}^{-1})\mathsf{A}\in\mathbb{R}^{ d\times d}\). We have

\[\|H_{t}^{-1/2}(\frac{\partial}{\partial t}H_{t})H_{t}^{-1/2}\|_{F} ^{2}\] \[= \operatorname{tr}[H_{t}^{-1}\cdot(\frac{\partial}{\partial t}H_{t} )\cdot H_{t}^{-1}\cdot(\frac{\partial}{\partial t}H_{t})]\] \[= \operatorname{tr}[H_{t}^{-1}\cdot\mathsf{A}^{\top}\frac{\partial S _{t}^{-1}\otimes S_{t}^{-1}}{\partial t}\mathsf{A}\cdot H_{t}^{-1}\mathsf{A}^{ \top}\frac{\partial(S_{t}^{-1}\otimes S_{t}^{-1})}{\partial t}\mathsf{A}]\] \[= \operatorname{tr}[\mathsf{A}H_{t}^{-1}\mathsf{A}^{\top}\cdot\frac {\partial S_{t}^{-1}\otimes S_{t}^{-1}}{\partial t}\cdot\mathsf{A}H_{t}^{-1} \mathsf{A}^{\top}\cdot\frac{\partial(S_{t}^{-1}\otimes S_{t}^{-1})}{\partial t }]\] \[= \operatorname{tr}[P_{t}(S_{t}^{-1/2}\otimes S_{t}^{-1/2})^{-1} \frac{\partial S_{t}^{-1}\otimes S_{t}^{-1}}{\partial t}(S_{t}^{-1/2}\otimes S _{t}^{-1/2})^{-1}\] \[\cdot P_{t}(S_{t}^{-1/2}\otimes S_{t}^\[= \operatorname{tr}[P_{t}\cdot((S_{t}^{-1/2}(\sum_{i=1}^{d}h_{i}A_{i})S_ {t}^{-1/2})^{2}\otimes_{S}I_{n}+(S_{t}^{-1/2}(\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1/ 2}\otimes S_{t}^{-1/2}(\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1/2}))]\] \[= 2\operatorname{tr}[S_{t}^{-1/2}(\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{ -1/2}\cdot\Sigma_{t}\cdot S_{t}^{-1/2}(\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1/2}]\] \[+ \operatorname{tr}[P_{t}\cdot(S_{t}^{-1/2}(\sum_{i=1}^{d}h_{i}A_{i })S_{t}^{-1/2}\otimes S_{t}^{-1/2}(\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1/2})]\] \[\leq 4\operatorname{tr}[S_{t}^{-1/2}(\sum_{i=1}^{d}h_{i}A_{i})S_ {t}^{-1/2}\cdot\Sigma_{t}\cdot S_{t}^{-1/2}(\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1/ 2}]\] \[\leq 4\lVert\Sigma_{t}\rVert\operatorname{tr}[(S_{t}^{-1/2}( \sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1/2})^{2}]\] \[\leq 4\operatorname{tr}[\Sigma_{t}]\operatorname{tr}[(S_{t}^{-1/2 }(\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1/2})^{2}]\] \[= 4d\lVert h\rVert_{H_{t}}^{2},\]

the fifth step is by for PSD matrix \(M\), we have \(MPM\preceq M^{2}\) where \(P\) is a projection matrix, the sixth step is by the below derivation, the seventh step is by \(\operatorname{tr}[P_{t}\cdot(M^{2}\otimes_{S}I_{n})]=2\operatorname{tr}[M \Sigma_{t}M]\), the eighth step is by for PSD matrix \(M\), \(2M\otimes_{S}I_{n}\succeq M\otimes M\), the ninth step is by Holder's inequality where \(\langle A,B\rangle\leq\lVert A\rVert\cdot\lVert B\rVert_{s_{1}}\) for \(\lVert B\rVert_{s_{1}}\) be its Schatten-1 norm. If \(B\) is PSD, \(\lVert B\rVert_{1}=\operatorname{tr}[B]\), the tenth step is by for PSD matrix \(A\), \(\lVert A\rVert\leq\operatorname{tr}[A]\).

Note that

\[\frac{\partial S_{t}^{-1}}{\partial t} = -S_{t}^{-1}\frac{\partial S_{t}}{\partial t}S_{t}^{-1}\] \[= -S_{t}^{-1}(\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1}\]

Thus,

\[\frac{\partial S_{t}^{-1}\otimes S_{t}^{-1}}{\partial t} = -S_{t}^{-1}(\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1}\otimes S_{t}^{-1}\] \[-S_{t}^{-1}\otimes S_{t}^{-1}(\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1}\]

and we have

\[(S_{t}^{-1/2}\otimes S_{t}^{-1/2})^{-1}\frac{\partial S_{t}^{-1} \otimes S_{t}^{-1}}{\partial t}(S_{t}^{-1}\otimes S_{t}^{-1})^{-1}\frac{ \partial S_{t}^{-1}\otimes S_{t}^{-1}}{\partial t}(S_{t}^{-1/2}\otimes S_{t}^ {-1/2})^{-1}\] \[= (S_{t}^{-1/2}\otimes S_{t}^{-1/2})^{-1}(S_{t}^{-1}(\sum_{i=1}^{d} h_{i}A_{i})S_{t}^{-1}\otimes S_{t}^{-1}+S_{t}^{-1}\otimes S_{t}^{-1}(\sum_{i=1}^{d} h_{i}A_{i})S_{t}^{-1})(S_{t}^{-1/2}\otimes S_{t}^{-1/2})^{-1}\] \[\cdot(S_{t}^{-1/2}\otimes S_{t}^{-1/2})^{-1}(S_{t}^{-1}(\sum_{i=1} ^{d}h_{i}A_{i})S_{t}^{-1}\otimes S_{t}^{-1}+S_{t}^{-1}\otimes S_{t}^{-1}(\sum_{ i=1}^{d}h_{i}A_{i})S_{t}^{-1})(S_{t}^{-1/2}\otimes S_{t}^{-1/2})^{-1}\] \[= (S_{t}^{-1/2}(\sum_{i=1}^{d}h_{i}A_{i})S_{t}^{-1/2}\otimes_{S}I_{ n})^{2},\]

plug in the above derivation we obtain the desired result. Finally, we send \(t\to 0\) and conclude the proof. 

**Lemma G.9**.: _Let \(H(x)\) be the matrix function \(\mathsf{A}(S^{-1}\otimes S^{-1})\mathsf{A}^{\top}\), then we have_

\[\lVert H(x)^{-1/2}\nabla F(x)\rVert_{2}\leq\widetilde{O}(d).\]Proof.: We have

\[\|H(x)^{-1/2}\nabla F(x)\|_{2} =\max_{v:\|v\|_{2}=1}(H(x)^{-1/2}\nabla F(x))^{\top}v\] \[=\max_{v:\|v\|_{2}=1}\operatorname{tr}[H(x)^{-1}DH(x)[H(x)^{-1/2}v]]\] \[=\max_{u:\|u\|_{H(x)}=1}\operatorname{tr}[H(x)^{-1/2}DH(x)[u]H(x)^ {-1/2}]\] \[\leq\max_{u:\|u\|_{H(x)}=1}\sqrt{d}\|H(x)^{-1/2}DH(x)[u]H(x)^{-1/2} \|_{F}\] \[\leq 2d\cdot\|u\|_{H(x)}\] \[=2d,\]

where the fourth step is by \(\operatorname{tr}[M]\leq\sqrt{d}\cdot\|M\|_{F}\) for \(d\times d\) PSD matrix \(M\), the fifth step follows from Lemma G.8, the last step follows from \(\|u\|_{H(x)}=1\). 

### Fast Hessian approximation

We need a particular type of sketch for Kronecker product of matrices.

**Definition G.10** (TensorSRHT(Song et al., 2021)).: _The TensorSRHT\(\Pi:\mathbb{R}^{n}\times\mathbb{R}^{n}\to\mathbb{R}^{s}\) is defined as \(S:=\frac{1}{\sqrt{s}}P\cdot(HD_{1}\otimes HD_{2})\), where each row of \(P\in\{0,1\}^{s\times n^{2}}\) contains only one \(1\) at a random coordinate and one can view \(P\) as a sampling matrix. \(H\) is a \(n\times n\) Hadamard matrix, and \(D_{1}\), \(D_{2}\) are two \(n\times n\) independent diagonal matrices with diagonals that are each independently set to be a Rademacher random variable (uniform in \(\{-1,1\}\))._

**Lemma G.11** (Lemma 2.12 in Song et al. (2021)).: _Let \(\Pi\) be a TensorSRHT matrix defined in Definition G.10. If \(s=O(\epsilon^{-2}d\log^{3}(nd/(\epsilon\delta)))\), then for any orthonormal basis \(U\in\mathbb{R}^{n^{2}\times d}\), we have that with probability at least \(1-\delta\), the singular values of \(\Pi U\) lie in the range of \([1-\epsilon,1+\epsilon]\)._

The following result provides an efficient embedding for \(\mathsf{B}\).

**Lemma G.12**.: _Let \(\mathsf{B}\in\mathbb{R}^{d\times n^{2}}\) be defined as in Def. G.3, \(\epsilon\in(0,1/10)\) denote an accuracy parameter and \(\delta\in(0,1/10)\) denote a failure probability._

_Let \(\Pi\in\mathbb{R}^{s\times n^{2}}\) be a TensorSRHT matrix with \(s=\Theta(\epsilon^{-2}d\log^{3}(nd/(\epsilon\delta)))\), then we have_

\[\Pr[\|\Pi\mathsf{B}^{\top}x\|_{2}=(1\pm\epsilon)\|\mathsf{B}^{\top}x\|_{2}, \forall x\in\mathbb{R}^{d}]\geq 1-\delta.\]

_Moreover, \(\Pi\mathsf{B}^{\top}\) can be computed in time_

\[O(d\cdot\mathcal{T}_{\mathrm{mat}}(n,n,s)).\]

Proof.: The correctness part follows directly from Lemma G.11. It remains to argue for the running time. We need to unravel the construction of both \(S\) and \(\mathsf{B}\) (see Definition G.2). Recall that \(\Pi=\frac{1}{\sqrt{s}}P\cdot(HD_{1}\otimes HD_{2})\) and

\[\Pi(S(x)^{-1/2}\otimes S(x)^{-1/2})\operatorname{vec}(A_{i})\] \[=\frac{1}{\sqrt{s}}P\cdot(HD_{1}\otimes HD_{2})\cdot(S(x)^{-1/2} \otimes S(x)^{-1/2})\operatorname{vec}(A_{i})\] \[=\frac{1}{\sqrt{s}}P\cdot(HD_{1}S(x)^{-1/2}\otimes HD_{2}S(x)^{- 1/2})\operatorname{vec}(A_{i})\] \[=\frac{1}{\sqrt{s}}P\cdot\operatorname{vec}(HD_{2}S(x)^{-1/2}A_{ i}S(x)^{-1/2}D_{1}H^{\top})\]

since \(P\) is a row sampling matrix, the product can be computed as follows:

* First compute \(HD_{1}S(x)^{-1/2}\) and \(HD_{2}S(x)^{-1/2}\). Since \(H\) is a Hadamard matrix, this step can be carried out in \(O(n^{2}\log n)\) time.

* Applying \(P\) to the vector can be interpreted as sampling \(s\) coordinates from the matrix \(HD_{2}S(x)^{-1/2}A_{i}S(x)^{-1/2}D_{1}H^{\top}\). Let \((i_{1},j_{1}),\ldots,(i_{s},j_{s})\) denote the coordinates sampled by \(P\). We form two matrices \(X,Y\in\mathbb{R}^{s\times n}\) where the \(k\)-th row of \(X\) is \((HD_{2}S(x)^{-1/2})_{i_{k},*}\) and the \(k\)-th row of \(Y\) is \((HD_{1}S(x)^{-1/2})_{j_{k},*}\). It is easy to verify that the \((i_{k},j_{k})\)-th entry of \(XA_{i}Y^{\top}\in\mathbb{R}^{s\times s}\) is the corresponding entry of \(HD_{2}S(x)^{-1/2}A_{i}S(x)^{-1/2}D_{1}H^{\top}\). This step therefore takes \(\mathcal{T}_{\mathrm{mat}}(n,n,s)\) time.

As we need to apply the second step to \(d\) rows, the total runtime is

\[O(d\cdot\mathcal{T}_{\mathrm{mat}}(n,n,s)).\qed\]

**Remark G.13**.: _For comparison, compute \(\mathsf{B}\) straightforwardly using the Kronecker product identity takes \(O(dn^{\omega})\) time. As long as the row count \(s\) is independent of \(n\), Lemma G.12 approximately computes \(\mathsf{B}\) in time \(\widetilde{O}(n^{2}\cdot\mathrm{poly}(d))\). For the regime where \(d\ll n\), our algorithm is much more efficient._

## Appendix H Convexity of Regularized Log-Barrier Function

In this section, we prove the convexity of \(\log\det(H_{\log}(x)+I_{d})\) for both polytopes and spectrahedra.

### Convexity of \(\log\det(H_{\log}(x)+I_{d})\): polytopes

We prove that for \(H(x)=\sum_{i=1}^{n}\frac{a_{i}a_{i}^{\top}}{(a_{i}^{\top}x-b_{i})^{2}}\), the Hessian of the log barrier, we have the convexity.

**Definition H.1** (Ridge leverage score).: _Let \(B\in\mathbb{R}^{n\times d}\), we define the \(\lambda\)-ridge leverage score of \(B\) as_

\[\widetilde{\sigma}_{\lambda,i}(B):=b_{i}^{\top}(B^{\top}B+\lambda I_{d})^{-1 }b_{i}.\]

_If \(\lambda=1\), we abbreviate as \(\widetilde{\sigma}_{i}(B)\)._

_For convenient of the proof, we also define_

\[\widetilde{\sigma}_{\lambda,i,j}(B):=b_{i}^{\top}(B^{\top}B+\lambda I_{d})^{- 1}b_{j}.\]

_We also define \(\widehat{\sigma}_{\lambda,i}(B)\) as follows:_

\[\widehat{\sigma}_{\lambda,i}(B):=b_{i}^{\top}(B^{\top}B+\lambda I_{d})^{-2}b_ {i}.\]

**Fact H.2**.: _Let \(X\) be \(\mathbb{R}^{d\times d}\) be invertible and symmetric. Then we have_

\[\frac{\partial X^{-1}}{\partial t}=-X^{-1}\frac{\partial X}{\partial t}X^{-1}.\]

**Lemma H.3**.: _Let \(X\in\mathbb{R}^{d\times d}\) be invertible and symmetric. Then,_

\[\partial\log\det X=X^{-1}\partial X.\]

Proof.: The proof is by chain rule:

\[\partial\log\det X=\frac{1}{\det X}\partial\det X,\]

to compute \(\partial\det X\), recall the adjugate of \(X\), denoted by \(\mathrm{adj}(X)\) where \(X^{-1}=\frac{1}{\det X}\ \mathrm{adj}(X)\), and it remains to show that \(\partial\det X=\mathrm{adj}(X)\), this can be derived using cofactor expansion of a matrix, which states that fix a row \(i\), we have

\[\det X=\sum_{k=1}^{d}X_{i,k}\cdot\mathrm{adj}(X)_{i,k},\]

by product rule,

\[\frac{\partial\det X}{\partial X_{i,j}}=\sum_{k=1}^{d}\frac{\partial X_{i,k}} {\partial X_{i,j}}\cdot\mathrm{adj}(X)_{i,k}+X_{i,k}\cdot\frac{\partial\ \mathrm{adj}(X)_{i,k}}{\partial X_{i,j}}\]\[=\operatorname{adj}(X)_{i,j},\]

where \(\frac{\partial\operatorname{adj}_{i,k}}{\partial X_{i,j}}=0\) for all \(k\) since the cofactor of \(i,k\) is the principal minor by removing row \(i\) and column \(k\). Therefore,

\[\partial\det X=\operatorname{adj}(X)\]

and consequently,

\[\partial\log\det X =\frac{1}{\det X}\partial\det X\] \[=\frac{1}{\det X}\operatorname{adj}(X)\] \[=X^{-1}.\qed\]

The goal of this section is to prove the \(d\times d\) matrix \(\nabla^{2}F(x)\succ 0\). We start with computing \(\nabla F(x)\in\mathbb{R}^{d}\).

**Lemma H.4**.: _We define \(s_{x,i}=a_{i}^{\top}x-b_{i}\) for each \(i\in[n]\). Let \(S_{x}\) denote a diagonal matrix such that \(S_{x}=\operatorname{diag}(s_{x})\). Let \(H(x)=A^{\top}S_{x}^{-2}A\in\mathbb{R}^{d\times d}\). Let \(F(x)=\log\det(H(x)+I_{d})\), then_

\[\nabla F(x)=\ -2\sum_{i=1}^{n}\widetilde{\sigma}_{i}(S_{x}^{-1}A)\frac{a_{i} }{s_{x,i}}.\]

Proof.: First, we know that

\[\frac{\partial s_{x,i}^{-2}}{\partial x_{j}}=\frac{-2a_{i,j}}{s_{x,i}^{3}}.\] (19)

For each \(j\in[d]\), we can write \(\frac{\partial F}{\partial x_{j}}\) as follows:

\[\frac{\partial F}{\partial x_{j}} = \operatorname{tr}[(H(x)+I_{d})^{-1}\frac{\partial(H(x)+I_{d})}{ \partial x_{j}}]\] \[= \operatorname{tr}[(H(x)+I_{d})^{-1}\frac{\partial H(x)}{x_{j}}]\] \[= \operatorname{tr}[(H(x)+I_{d})^{-1}\sum_{i=1}^{n}a_{i}a_{i}^{ \top}\frac{\partial s_{x,i}^{-2}}{\partial x_{j}}]\] \[= \operatorname{tr}[(H(x)+I_{d})^{-1}\sum_{i=1}^{n}-2a_{i}a_{i}^{ \top}\frac{a_{i,j}}{s_{x,i}^{3}}]\] \[= -2\sum_{i=1}^{n}\operatorname{tr}[\frac{a_{i,j}}{s_{x,i}}\frac{a _{i}^{\top}(H(x)+I_{d})^{-1}a_{i}}{s_{x,i}^{2}}]\] \[= -2\sum_{i=1}^{n}\widetilde{\sigma}_{i}(S_{x}^{-1}A)\frac{a_{i,j} }{s_{x,i}}.\]

where the forth step follows from Eq. (19), the fifth step follows from \(\operatorname{tr}[AB]=\operatorname{tr}[BA]\), and the last step follows from \(\widetilde{\sigma}\).

We use chain rule:

\[\frac{\partial F}{\partial x} = \left[\tfrac{\partial F}{\partial x_{1}}\quad\tfrac{\partial F}{ \partial x_{2}}\quad\cdots\quad\tfrac{\partial F}{\partial x_{d}}\right]^{\top}\] \[= -2\sum_{i=1}^{n}\widetilde{\sigma}_{i}(S_{x}^{-1}A)\frac{a_{i}}{s _{x,i}}.\qed\]

**Fact H.5**.: _We have the following partial derivative:_

\[\frac{\partial(\widetilde{\sigma}_{i}(S_{x}^{-1}A)s_{x,i}^{2})}{\partial x_{ l}}=2\sum_{j=1}^{n}\widetilde{\sigma}_{i,j}^{2}(S_{x}^{-1}A)\frac{a_{j,l}}{s_{x,j}}\]Proof.: We analyze the partial derivative term

\[\frac{\partial}{\partial x_{l}}a_{i}^{\top}(H(x)+I_{d})^{-1}a_{i} =a_{i}^{\top}(\frac{\partial(H(x)+I_{d})^{-1}}{\partial x_{l}})a_{i}\] \[=a_{i}^{\top}(-(H(x)+I_{d})^{-1}\frac{\partial H(x)}{\partial x_{l }}(H(x)+I_{d})^{-1})a_{i}\] \[=a_{i}^{\top}(2(H(x)+I_{d})^{-1}(\sum_{j=1}^{n}a_{j}a_{j}^{\top} \frac{a_{j,l}}{s_{x,j}^{3}})(H(x)+I_{d})^{-1})a_{i}\] \[=2\sum_{j=1}^{n}\frac{(a_{i}^{\top}(H(x)+I_{d})^{-1}a_{j})^{2}}{s _{x,j}^{2}}\frac{a_{j,l}}{s_{x,j}}\] \[=2\sum_{j=1}^{n}\widetilde{\sigma}_{i,j}^{2}(S_{x}^{-1}A)\frac{a_ {j,l}}{s_{x,j}}\] (20)

where the second step follows from Fact H.2. 

**Fact H.6**.: _We have_

\[\widetilde{\sigma}_{i}(S_{x}^{-1}A)=\widehat{\sigma}_{i}(S_{x}^{-1}A)+\sum_{j =1}^{n}\widetilde{\sigma}_{i,j}^{2}(S_{x}^{-1}A).\]

Proof.: Note that

\[(H(x)+I_{d})^{-1} =(H(x)+I_{d})^{-1}(H(x)+I_{d})(H(x)+I_{d})^{-1}\] \[=(H(x)+I_{d})^{-2}+\sum_{j=1}^{n}\frac{(H(x)+I_{d})^{-1}a_{j}a_{j }^{\top}(H(x)+I_{d})^{-1})}{s_{x,j}^{2}},\]

therefore,

\[\widetilde{\sigma}_{i}(S_{x}^{-1}A) =a_{i}^{\top}(H(x)+I_{d})^{-1}a_{i}\frac{1}{s_{x,i}^{2}}\] \[=\frac{a_{i}^{\top}(H(x)+I_{d})^{-2}a_{i}}{s_{x,i}^{2}}+\sum_{j=1 }^{n}\frac{(a_{i}^{\top}(H(x)+I_{d})^{-1}a_{j})^{2}}{s_{x,i}^{2}s_{x,j}^{2}}\] \[=\widehat{\sigma}_{i}(S_{x}^{-1}A)+\sum_{j=1}^{n}\widetilde{ \sigma}_{i,j}^{2}(S_{x}^{-1}A)\]

Thus we complete the proof. 

**Lemma H.7**.: _Let \(F(x)=\log\det(H(x)+I_{d})\), then_

\[\nabla^{2}F(x)\succeq 0.\]

_Thus, \(F(x)\) is convex._

Proof.: Note that

\[\frac{\partial}{\partial x_{l}}\frac{\partial}{\partial x_{k}}F(x) = -\Big{(}\sum_{i=1}^{n}a_{i}^{\top}(H(x)+I_{d})^{-1}a_{i}a_{i} \frac{\partial}{\partial x_{l}}(\frac{a_{i,k}}{s_{x,i}^{3}})+\frac{a_{i,k}}{s_ {x,i}^{3}}\frac{\partial}{\partial x_{l}}a_{i}^{\top}(H(x)+I_{d})^{-1}a_{i} \Big{)}\] (21) \[= 3\sum_{i=1}^{n}\frac{a_{i}^{\top}(H(x)+I_{d})^{-1}a_{i}}{s_{x,i} ^{2}}\frac{a_{i,k}a_{i,l}}{s_{x,i}^{2}}-\frac{a_{i,k}}{s_{x,i}^{3}}\frac{ \partial}{\partial x_{l}}a_{i}^{\top}(H(x)+I_{d})^{-1}a_{i}.\]

Plug in Eq. (20) into Eq. (21), we have

\[3\sum_{i=1}^{n}\widetilde{\sigma}_{i}(S_{x}^{-1}A)\frac{a_{i,k}a_{i,l}}{s_{x, i}^{2}}-2\sum_{i=1}^{n}\sum_{j=1}^{n}\widetilde{\sigma}_{i,j}(S_{x}^{-1}A)^{2} \frac{a_{i,k}a_{j,l}}{s_{x,j}s_{x,i}},\]\[3\sum_{i=1}^{n}\widetilde{\sigma}_{i}(S_{x}^{-1}A)\frac{a_{i}a_{i}^{ \top}}{s_{x,i}^{2}}-2\sum_{i=1}^{n}\sum_{j=1}^{n}\widetilde{\sigma}_{i,j}^{2}(S_ {x}^{-1}A)\frac{a_{i}a_{j}^{\top}}{s_{x,j}s_{x,i}}\] \[=3\sum_{i=1}^{n}(\widehat{\sigma}_{i}(S_{x}^{-1}A)+\sum_{j=1}^{n} \widetilde{\sigma}_{i,j}^{2}(S_{x}^{-1}A))\frac{a_{i}a_{i}^{\top}}{s_{x,i}^{2 }}-\,2\sum_{i=1}^{n}\sum_{j=1}^{n}\widetilde{\sigma}_{i,j}^{2}(S_{x}^{-1}A) \frac{a_{i}a_{j}^{\top}}{s_{x,j}s_{x,i}}\] \[=3\sum_{i=1}^{n}\widehat{\sigma}_{i}(S_{x}^{-1}A)\frac{a_{i}a_{i} ^{\top}}{s_{x,i}^{2}}+3\sum_{i=1}^{n}\sum_{j=1}^{n}\widetilde{\sigma}_{i,j}^{2 }(S_{x}^{-1}A)\frac{a_{i}a_{i}^{\top}}{s_{x,i}^{2}}-\,2\sum_{i=1}^{n}\sum_{j=1 }^{n}\widetilde{\sigma}_{i,j}^{2}(S_{x}^{-1}A)\cdot\frac{a_{i}a_{j}^{\top}}{s _{x,j}s_{x,i}}\] \[=3\sum_{i=1}^{n}\widehat{\sigma}_{i}(S_{x}^{-1}A)\frac{a_{i}a_{i }^{\top}}{s_{x,i}^{2}}+\sum_{i=1}^{n}\widetilde{\sigma}_{i}(S_{x}^{-1}A)\frac{ a_{i}a_{i}^{\top}}{s_{x,i}^{2}}+\,2\sum_{i=1}^{n}\sum_{j=1}^{n}\widetilde{ \sigma}_{i,j}^{2}(S_{x}^{-1}A)\cdot(\frac{a_{i}a_{i}^{\top}}{s_{x,i}^{2}}-\frac {a_{i}a_{j}^{\top}}{s_{x,j}s_{x,i}})\] \[:=B_{1}+B_{2}+B_{3}.\]

where the first step follows from Fact H.6, the last step follows from

\[B_{1} :=3\sum_{i=1}^{n}\widehat{\sigma}_{i}(S_{x}^{-1}A)\frac{a_{i}a_{i }^{\top}}{s_{x,i}^{2}},\] \[B_{2} :=\,\sum_{i=1}^{n}\widetilde{\sigma}_{i}(S_{x}^{-1}A)\frac{a_{i}a _{i}^{\top}}{s_{x,i}^{2}},\] \[B_{3} :=2\sum_{i=1}^{n}\sum_{j=1}^{n}\widetilde{\sigma}_{i,j}^{2}(S_{x} ^{-1}A)\cdot(\frac{a_{i}a_{i}^{\top}}{s_{x,i}^{2}}-\frac{a_{i}a_{j}^{\top}}{s_ {x,j}s_{x,i}}).\]

It is obvious that \(B_{1}\succ 0\) and \(B_{2}\succ 0\).

It is not hard to see that

\[\frac{a_{i}a_{i}^{\top}}{s_{x,i}^{2}}+\frac{a_{j}a_{j}^{\top}}{s_{x,j}^{2}} \succeq 2\frac{a_{i}a_{j}^{\top}}{s_{x,j}s_{x,i}}.\]

Thus we have \(B_{3}\succeq 0\) and we complete the proof. 

### Convexity of \(\log\det(H_{\log}(x)+I_{d})\): spectrahedra

In this section, we prove that \(\log\det(H_{\log}(x)+I_{d})\) is a convex function on \(x\). Combined with the bounded variance property, we provide an algorithm that samples from the SDP spectrahedron.

**Definition H.8**.: _We define the ridge-projection matrix \(P\in\mathbb{R}^{n^{2}\times n^{2}}\) as_

\[P:=(S^{-1/2}\otimes S^{-1/2})\mathsf{A}^{\top}(H+I_{d})^{-1}\mathsf{A}(S^{-1/2 }\otimes S^{-1/2}).\]

_We define projection matrix \(\overline{P}\in\mathbb{R}^{n^{2}\times n^{2}}\) as follows_

\[\overline{P}:=(S^{-1/2}\otimes S^{-1/2})\mathsf{A}^{\top}H^{-1}\mathsf{A}(S^{- 1/2}\otimes S^{-1/2}).\]

**Remark H.9**.: _It is not hard to see that \(P\) is a PSD matrix. Note that the matrix \(H+I_{d}\) is PSD, since \(H\) is PSD, therefore \((H+I_{d})^{-1}\) is also PSD. Thus, the ridge-projection is PSD, as its two "arms" are symmetric._

**Lemma H.10**.: _Let \(\overline{P}\in\mathbb{R}^{n^{2}\times n^{2}}\) be the projection matrix and let \(P\in\mathbb{R}^{n^{2}\times n^{2}}\) be the ridge-projection matrix defined in Def. H.8. Then, we have \(P\preceq\overline{P}\)._

Proof.: Let \(H=U\Sigma U^{\top}\) be its eigendecomposition, we need to show that

\[(H+I_{d})^{-1}\preceq H^{-1}.\]

Let us expand the LHS:

\[(U\Sigma U^{\top}+I_{d})^{-1} =(U(\Sigma+I_{d})U^{\top})^{-1}\] \[=U(\Sigma+I_{d})^{-1}U^{\top}\] \[\preceq U\Sigma^{-1}U^{\top},\]

where the last step follows from for any non-negative number \(\lambda\), \((1+\lambda)^{-1}\leq\lambda^{-1}\)The following lemma is a generalization of Theorem 4.1 of Anstreicher [2000].

**Lemma H.11**.: _Let \(H_{\log}(x)\in\mathbb{R}^{d\times d}\) be defined as Definition G.6. Let \(F(x)=\log\det(H_{\log}(x)+I_{d})\). Then, \(F(x)\) is convex in \(x\)._

Proof.: Using standard algebra and chain rule computations, we have

\[\nabla^{2}F(x)=2Q(x)+R(x)-2T(x).\]

For simplicity, we drop \(x\). The \(Q,R,T\in\mathbb{R}^{d\times d}\) can be defined as follows

\[Q_{i,j} :=\mathsf{A}(H+I_{d})^{-1}\mathsf{A}^{\top}\cdot(S^{-1}A_{i}S^{-1 }A_{j}S^{-1}\oplus S^{-1}),\] \[R_{i,j} :=\mathsf{A}(H+I_{d})^{-1}\mathsf{A}^{\top}\cdot(S^{-1}A_{i}S^{- 1}\oplus S^{-1}A_{j}S^{-1}),\] \[T_{i,j} :=\mathsf{A}(H+I_{d})^{-1}\mathsf{A}^{\top}\cdot(S^{-1}A_{i}S^{- 1}\oplus S^{-1})\cdot\mathsf{A}(H+I_{d})^{-1}\mathsf{A}^{\top}\cdot(S^{-1}A_{j }S^{-1}\oplus S^{-1}).\]

Using Lemma H.12, we know that

\[0\preceq Q(x)\preceq\nabla^{2}F(x)\preceq 3Q(x)\]

Thus, \(F(x)\) is convex. 

**Lemma H.12**.: _For any \(x\), if \(S(x)\succ 0\), then we have_

* _Part 1._ \(Q\succeq 0\)_;_
* _Part 2._ \(T\succeq 0\)_;_
* _Part 3._ \(T\preceq\frac{1}{2}(Q+R)\)_;_
* _Part 4._ \(\nabla^{2}F(x)\succeq 0\)_;_
* _Part 5._ \(R\preceq Q\)_;_
* _Part 6._ \(\nabla^{2}F(x)\preceq 3Q(x)\)_._

Proof.: **Proof of Part 1 and 2.** Let \(\xi\in\mathbb{R}^{d}\) and \(\xi\neq 0\).

Then we have

\[\xi^{\top}Q\xi =\sum_{i,j}Q_{i,j}\xi_{i}\xi_{j}\] \[=\mathsf{A}(H+I_{d})^{-1}\mathsf{A}^{\top}\cdot(S^{-1}BS^{-1}BS^{ -1}\oplus S^{-1})\] \[=P\cdot(\overline{B}^{2}\oplus I_{d})\]

where \(B=B(\xi)=\sum_{i=1}^{d}\xi_{i}A_{i}\), and \(\overline{B}=S^{-1/2}BS^{-1/2}\).

Similarly, we have

\[\xi^{\top}R\xi =\mathsf{A}(H+I_{d})^{-1}\mathsf{A}^{\top}\cdot(S^{-1}BS^{-1} \otimes S^{-1}BS^{-1})\] \[=P\cdot(\overline{B}\otimes\overline{B})\]

We can rewrite \(\xi^{\top}T\xi\) as follows:

\[\xi^{\top}T\xi =\mathsf{A}^{\top}(H+I)^{-1}\mathsf{A}\cdot(S^{-1}BS^{-1}\oplus S ^{-1})\mathsf{A}(H+I)^{-1}\mathsf{A}^{\top}\cdot(S^{-1}BS^{-1}\oplus S^{-1})\] \[=P\cdot(\overline{B}\oplus I_{d})P(\overline{B}\oplus I_{d})\]

It is obvious that \(I_{d},P\) and \(\overline{B}^{2}\) are all PSD matrices.

Using Part 3 of Lemma A.9 and Part 6 of Lemma A.10, we have

\[\xi^{\top}Q\xi \geq 0\] \[\xi^{\top}T\xi \geq 0\]Since \(\xi\) is arbitrary, then we know that

\[Q \succeq 0\] \[T \succeq 0\]

**Proof of Part 3.** Note that \(\overline{P}\) is a projection matrix, and \(P\preceq\overline{P}\) by Lemma H.10.

\[(\overline{B}\oplus I_{d})P(\overline{B}\oplus I_{d}) \preceq(\overline{B}\oplus I_{d})\overline{P}(\overline{B}\oplus I _{d})\] \[\preceq(\overline{B}\oplus I_{d})I_{d}(\overline{B}\oplus I_{d})\] \[=\frac{1}{2}((\overline{B}^{2}\oplus I_{d})+(\overline{B}\otimes \overline{B}))\]

where the last step follows from Part 2 of Lemma A.10.

Applying Part 1 of Lemma A.9, we have

\[\langle P,(\overline{B}\oplus I_{d})P(\overline{B}\oplus I_{d})\rangle\leq \frac{1}{2}\langle P,(\overline{B}^{2}\oplus I_{d})+(\overline{B}\otimes \overline{B})\rangle\]

The above equation implies the following

\[\xi^{\top}T\xi\leq\frac{1}{2}\xi^{\top}(Q+R)\xi\]

Note that, here \(\xi\) is arbitrary, thus we know that

\[T\preceq\frac{1}{2}(Q+R)\]

**Proof of Part 4.** We have

\[\nabla^{2}F(x) =2Q+R-2T\] \[\succeq 2Q+R-2\cdot\frac{1}{2}(Q+T)\] \[=Q\]

**Proof of Part 5.** Let \(v_{i}\) be orthonormal eigenvectors of \(\overline{B}\) with corresponding eigenvalues \(\lambda_{i}\).

Then using [10, Theorem 4.4.5], \(\overline{B}^{2}\oplus I\) has orthonormal eigenvectors \(v_{i}\otimes v_{j}\) with corresponding eigenvalues \(\frac{1}{2}(\lambda_{i}^{2}+\lambda_{j}^{2})\), while (see Horn and Johnson [1990]) \(\overline{B}\otimes\overline{B}\) has the same eigenvectors \(v_{i}\otimes v_{j}\), with corresponding eigenvalues \(\lambda_{i}\lambda_{j}\).

It then follows from \((\lambda_{i}-\lambda_{j})^{2}\) for each \(i,j\) that

\[\overline{B}^{2}\oplus I_{d}\succeq\overline{B}\otimes\overline{B}.\]

Using Part 4 of Lemma A.9, we know

\[\langle P,\overline{B}^{2}\oplus I_{d}\rangle\geq\langle P,\overline{B}\otimes \overline{B}\rangle.\]

Which is

\[\xi^{\top}Q\xi\geq\xi^{\top}R\xi\]

Since it's arbitrary \(\xi\), then we have

\[Q\succeq R\]

**Proof of Part 6.** We have

\[\nabla^{2}F(x) =2Q+R-2T\] \[\preceq 2Q+R\] \[\preceq 3Q\]

where the second step follows from \(T\succeq 0\), the last step follows from Part 5.

### Discussion

It is well-known that for popular barrier functions, such as volumetric barrier and Lee-Sidford barrier (Lee and Sidford, 2014, 2019), the function \(\log\det H(x)\) is convex in \(x\). However, given a PSD matrix function \(H(x)\) for which \(\log\det H(x)\) is convex in \(x\), it is generally not true that \(\log\det(H(x)+I_{d})\) is convex.

**Fact H.13** (Folklore).: _Suppose that \(H(x)\) is a positive semi-definite matrix for \(x\) in the domain. Suppose \(\log\det H(x)\) is convex. It is possible \(\log\det(H(x)+I)\) is not convex._

Proof.: If \(\log\det(H(x)+I)\) is convex in \(x\), then for any fixed \(x\in K\), \(v\in\mathbb{R}^{d},\log\det(H(x+tv)+I_{d})\) is convex in \(t\) (for \(t\) sufficiently close to \(0\)).

For \(t\) sufficiently closely to \(0\), define \(f_{i}(t):=\lambda_{i}(H(x+tv))\). Then

\[\log\det H(x+tv)=\sum_{i\in[d]}\log f_{i}(t)\]

and the condition that \(\log\det H(x)\) is convex is equivalent to

\[\frac{\partial^{2}}{\partial t^{2}}\log\det H(x+tv)=\sum_{i\in[d]}\frac{f_{i} ^{\prime\prime}(t)f_{i}(t)-f_{i}^{\prime}(t)^{2}}{f_{i}(t)^{2}}\geq 0.\]

Now

\[\log\det(H(x+tv)+I_{d})=\sum_{i\in[d]}\log(f_{i}(t)+1).\]

The condition that \(\log\det(H(x)+I_{d})\) is convex is equivalent to

\[\frac{\partial^{2}}{\partial t^{2}}\log\det(H(x+tv)+I_{d})=\sum_{i\in[d]}\frac {f_{i}^{\prime\prime}(t)(f_{i}(t)+1)-f_{i}^{\prime}(t)^{2}}{(f_{i}(t)+1)^{2}} \geq 0.\]

Suppose \(d=2\), \(f_{1}(0)=1,f_{1}^{\prime}(0)=0,f_{1}^{\prime\prime}(0)=1\), \(f_{2}(0)=4,f_{2}^{\prime}(0)=0,f_{2}^{\prime\prime}(0)=-3\). Then

\[\frac{\partial^{2}}{\partial t^{2}}\Big{|}_{t=0}\log\det H(x+tv) =\frac{f_{1}^{\prime\prime}(0)}{f_{1}(0)}+\frac{f_{2}^{\prime \prime}(0)}{f_{2}(0)}>0.\] \[\frac{\partial^{2}}{\partial t^{2}}\Big{|}_{t=0}\log\det(H(x+tv)+ I) =\frac{f_{1}^{\prime\prime}(0)}{f_{1}(0)+1}+\frac{f_{2}^{\prime\prime}(0)}{f_{2}(0 )+1}<0.\]

This gives an example for which \(\log\det H(x)\) is convex in a region but \(\log\det(H(x)+I_{d})\) is not. 

## Appendix I Convexity of Regularized Volumetric Barrier Function

In this section, we prove the convexity of \(\log\det\) of the regularized volumetric barrier function. This is crucial, as the convexity proof for regularized Lee-Sidford barrier is identical up to replacing leverage score by Lewis weights.

### Definitions

**Definition I.1**.: _Let \(A\in\mathbb{R}^{n\times d}\). Let \(b\in\mathbb{R}^{n}\). For each \(i\in[n]\), we define_

\[s_{x,i} :=(a_{i}^{\top}x-b_{i})\] \[s_{x} :=Ax-b\]

_For each \(i\in[n]\), we define_

\[\sigma_{i,i}(A_{x}) :=a_{x,i}^{\top}(A_{x}^{\top}A_{x})^{-1}a_{x,i}\]

_For each \(i\in[n]\), for each \(l\in[n]\)_

\[\sigma_{i,l}(A_{x}) :=a_{x,i}^{\top}(A_{x}^{\top}A_{x})^{-1}a_{x,l}\]

_Let \(\Sigma=\sigma_{*,*}(A_{x})\circ I_{n}\)._

**Definition I.2**.: _We define \(H(x)\in\mathbb{R}^{d\times d}\) as follows_

\[H(x):=\underbrace{A_{x}^{\top}}_{d\times n}\underbrace{\Sigma(A_{x})}_{n\times n }\underbrace{A_{x}}_{n\times d}\]

**Definition I.3**.: _For each \(j\in[d]\), we define \(P_{j}(x)\in\mathbb{R}^{n\times n}\) as follows_

\[P_{j}(x):=\underbrace{\frac{\partial\Sigma(A_{x})}{\partial x_{j}}}_{n\times n}\]

_For each \(j\in[d]\), for each \(k\in[d]\), we define \(P_{j,k}(x)\in\mathbb{R}^{n\times n}\) as follows:_

\[P_{j,k}(x):=\underbrace{\frac{\partial^{2}\Sigma(A_{x})}{\partial x_{k}x_{j} }}_{n\times n}\]

In the previous sections, we mainly use notation \(\sigma_{*,*}(A_{x})\). For simplicity, from this section, we will use notation \(Q(x)\) instead.

**Definition I.4**.: _We define \(Q(x)\in\mathbb{R}^{n\times n}\) as follows_

\[Q(x):=\underbrace{\sigma_{*,*}(A_{x})}_{n\times n}.\]

**Fact I.5**.: _We have_

\[\sigma_{i,i}(A_{x})=\langle\sigma_{*,i}^{2}(A_{x}),\mathbf{1}_{n}\rangle.\]

Proof.: The proof is straightforward from definition. 

We list a number of standard calculus results.

**Fact I.6**.: _We define the following quantities:_

* _Let_ \(s_{x}=Ax-b\in\mathbb{R}^{n}\)_;_
* _Let_ \(S_{x}=\operatorname{diag}(s_{x})\in\mathbb{R}^{n\times n}\) _denote the diagonal matrix by putting_ \(s_{x}\) _on the diagonal;_
* _Let_ \(A_{*,j}\) _denote the_ \(j\)_-th column of matrix_ \(A\in\mathbb{R}^{n\times d}\)_;_
* _Let_ \(A_{x}=S_{x}^{-1}A\)_;_
* _Let_ \(a_{x,i}^{\top}\) _denote the_ \(i\)_-th row of_ \(A_{x}\) _for each_ \(i\in[n]\)_._

_Then, we have for each \(j\in[d]\)_

* **Part 1.** \[\underbrace{\frac{\partial s_{x}}{\partial x_{j}}}_{n\times 1}=\underbrace{A_{*,j}}_ {n\times 1}\]
* **Part 2.** \[\underbrace{\frac{\partial s_{x}^{-1}}{\partial x_{j}}}_{n\times 1}=- \underbrace{s_{x}^{-2}}_{n\times 1}\circ\underbrace{A_{*,j}}_{n\times 1}\]
* **Part 3.** \[\underbrace{\frac{\partial s_{x}^{-2}}{\partial x_{j}}}_{n\times 1}=-2 \underbrace{s_{x}^{-3}}_{n\times 1}\circ\underbrace{A_{*,j}}_{n\times 1}\]* **Part 4.** \[\underbrace{\frac{\partial S_{x}^{-2}}{\partial x_{j}}}_{n\times n}=2\underbrace{ \operatorname{diag}(-s_{x}^{-3}\circ A_{*,j})}_{n\times n}=2\operatorname{ diag}(-S_{x}^{-3}A_{*,j})\]
* **Part 5.** \[\underbrace{\frac{\partial A^{\top}S_{x}^{-2}A}{\partial x_{j}}}_{d\times d}=2 \underbrace{A^{\top}_{d\times n}}_{d\times n}\underbrace{\operatorname{diag}(- S_{x}^{-3}A_{*,j})}_{n\times n}\underbrace{A}_{n\times d}\]
* **Part 6.** \[\frac{\partial A_{x}^{\top}A_{x}}{\partial x_{j}}=2A_{x}^{\top}\operatorname{ diag}(-A_{x,*,j})A_{x}\]
* **Part 7.** \[\frac{\partial(A_{x}^{\top}A_{x})^{-1}}{\partial x_{j}}=2(A_{x}^{\top}A_{x})^{- 1}\cdot A_{x}^{\top}\operatorname{diag}(A_{x,*,j})A_{x}\cdot(A_{x}^{\top}A_{x })^{-1}\]
* **Part 8.** _For each_ \(i\in[n]\)__ \[\frac{\partial a_{x,i}}{\partial x_{j}}=-\underbrace{A_{x,i,j}}_{\text{ scalar}}\cdot\underbrace{a_{x,i}}_{d\times 1}\]
* **Part 9.** _For each_ \(i\in[n]\)__ \[\frac{\partial a_{x,i}a_{x,i}^{\top}}{\partial x_{j}}=-2\cdot\underbrace{A_{x,i,j}}_{\text{scalar}}\cdot\underbrace{a_{x,i}a_{x,i}^{\top}}_{d\times d}\]
* **Part 10.** \[\underbrace{\frac{\partial A_{x,i,j}}{\partial x_{j}}}_{\text{ scalar}}=-\underbrace{A_{x,i,j}^{2}}_{\text{scalar}}\]
* **Part 11.** \[\underbrace{\frac{\partial A_{x,i,j}}{\partial x_{k}}}_{\text{ scalar}}=-\underbrace{A_{x,i,j}}_{\text{scalar}}\frac{A_{x,i,k}}{\text{ scalar}}\]
* **Part 12.** \[\frac{\partial A_{x,*,j}}{\partial x_{j}}=-A_{x,*,j}^{\circ 2}\]
* **Part 13.** \[\frac{\partial A_{x,*,j}}{\partial x_{k}}=-A_{x,*,j}\circ A_{x,*,k}\]
* **Part 14.** \[\underbrace{\frac{\partial A_{x}}{\partial x_{j}}}_{n\times d}=-\underbrace{ \operatorname{diag}(A_{x,*,j})}_{n\times n}\underbrace{A_{x}}_{n\times d}\]

**Definition I.7**.: _We define \(\widetilde{\Sigma}(A_{x})\in\mathbb{R}^{n\times n}\) and \(\widetilde{\sigma}_{*,*}(A_{x})\in\mathbb{R}^{n\times n}\) as_

\[\widetilde{\Sigma}(A_{x}) :=(A_{x}(H(x)+I_{d})^{-1}A_{x}^{\top})\circ I_{n}\] \[\widetilde{\sigma}_{*,*}(A_{x}) :=A_{x}(H(x)+I_{d})^{-1}A_{x}^{\top}\]

**Definition I.8**.: _We define \(F(x)\) as follows:_

\[F(x):=\log(\det(H(x)+I_{d}))\]

**Definition I.9**.: _We define \(\frac{\partial}{\partial x}\frac{\partial F}{\partial x^{\top}}\in\mathbb{R}^{ d\times d}\) to be_

\[\frac{\partial}{\partial x}\frac{\partial F}{\partial x^{\top}}=[\frac{ \partial}{\partial x}\frac{\partial F}{\partial x^{\top}}]_{1}+[\frac{ \partial}{\partial x}\frac{\partial F}{\partial x^{\top}}]_{2}\]

_where_

\[[\frac{\partial}{\partial x}\frac{\partial F}{\partial x^{\top} }]_{1,j,k} =\,+\operatorname{tr}[(H(x)+I_{d})^{-1}\frac{\partial^{2}H(x)}{ \partial x_{j}\partial x_{k}}]\] \[[\frac{\partial}{\partial x}\frac{\partial F}{\partial x^{\top}}]_ {2,j,k} =\,-\operatorname{tr}[(H(x)+I_{d})^{-1}\frac{\partial H(x)}{ \partial x_{k}}(H(x)+I_{d})^{-1}\cdot\frac{\partial H(x)}{\partial x_{j}}]\]

### Gradient of \(\sigma_{i,j}\)

**Fact I.10** (First derivative of leverage score).: _We define the following quantities:_

* _For each_ \(j\in[d]\)_, let_ \(A_{*,j}\in\mathbb{R}^{n}\) _denote_ \(j\)_-th column of_ \(A\)_;_
* _Let_ \(A_{x}:=S_{x}^{-1}A\in\mathbb{R}^{n\times d}\)_;_
* _Let_ \(A_{x,*,j}\in\mathbb{R}^{n}\) _denote the_ \(j\)_-th column of_ \(A_{x}\in\mathbb{R}^{n\times d}\)_;_
* _Let_ \(\Sigma(A_{x})\in\mathbb{R}^{n\times n}\) _denote a diagonal matrix where_ \((i,i)\)_-th entry is_ \(\sigma_{i,i}(A_{x})\)_;_
* _Let_ \(\sigma_{*,*}^{\circ 2}(A_{x})=\sigma_{*,*}(A_{x})\circ\sigma_{*,*}(A_{x})\in \mathbb{R}^{n\times n}\)_;_
* _Let_ \(\sigma_{*,i}(A_{x})\in\mathbb{R}^{n}\) _denote a column vector of_ \(\sigma_{*,*}(A_{x})\in\mathbb{R}^{n\times n}\)_._

_Then we have for each \(j\in[d]\)_

* _Part 1. For each_ \(i\in[n]\)_,_ \[\frac{\partial\sigma_{i,i}(A_{x})}{\partial x_{j}}=2\langle\sigma_{*,i}(A_{x}) \circ\sigma_{*,i}(A_{x}),A_{x,*,j}\rangle-2\sigma_{i,i}(A_{x})\cdot A_{x,i,j}\]
* _Part 2. For each_ \(i\in[n]\)_,_ \(l\in[n]\)__ \[\frac{\partial\sigma_{i,l}(A_{x})}{\partial x_{j}}=2\langle\sigma_{i,*}(A_{x}) \circ\sigma_{l,*}(A_{x}),A_{x,*,j}\rangle-\sigma_{i,l}(A_{x})\cdot(A_{x,i,j}+ A_{x,l,j})\]

Proof.: We know

\[\frac{\partial\sigma_{i,i}(A_{x})}{\partial x_{j}} =\frac{\partial a_{x,i}^{\top}(A_{x}^{\top}A_{x})^{-1}a_{x,i}}{ \partial x_{j}}\] \[=\frac{\partial\langle a_{x,i}a_{x,i}^{\top},(A_{x}^{\top}A_{x})^ {-1}\rangle}{\partial x_{j}}\] \[=\langle\frac{\partial a_{x,i}a_{x,i}^{\top}}{\partial x_{j}},(A_ {x}^{\top}A_{x})^{-1}\rangle+\langle a_{x,i}a_{x,i}^{\top},\frac{\partial(A_{x }^{\top}A_{x})^{-1}}{\partial x_{j}}\rangle\]

For the first term in the above, we have

\[-2A_{x,i,j}\cdot\langle a_{x,i}a_{x,i}^{\top},(A_{x}^{\top}A_{x})^{-1}\rangle =-2A_{x,i,j}\cdot\sigma_{i,i}(A_{x})\]For the second term, we have

\[\langle a_{x,i}a_{x,i}^{\top},\frac{\partial(A_{x}^{\top}A_{x})^{-1} }{\partial x_{j}}\rangle =2\langle a_{x,i}a_{x,i}^{\top},(A_{x}^{\top}A_{x})^{-1}\cdot A_{x }^{\top}\operatorname{diag}(A_{x,*,j})A_{x}\cdot(A_{x}^{\top}A_{x})^{-1}\rangle\] \[=2a_{x,i}^{\top}(A_{x}^{\top}A_{x})^{-1}\cdot A_{x}^{\top} \operatorname{diag}(A_{x,*,j})A_{x}\cdot(A_{x}^{\top}A_{x})^{-1}a_{x,i}\] \[=2a_{x,i}^{\top}(A_{x}^{\top}A_{x})^{-1}\cdot(\sum_{l=1}^{n}a_{x, l}a_{x,l}^{\top}A_{x,l,j})\cdot(A_{x}^{\top}A_{x})^{-1}a_{x,i}\] \[=2\sum_{l=1}^{n}\sigma_{l,i}(A_{x})^{2}A_{x,l,j}\] \[=2\langle\sigma_{*,i}^{2}(A_{x}),A_{x,*,j}\rangle.\]

Similarly, we can prove for \(\frac{\partial\sigma_{*,i}(A_{x})}{\partial x_{j}}\). 

### Gradient of \(\sigma\)

**Lemma I.11**.: _Let \(\sigma\) and \(\Sigma\) be defined as Definition I.1. Then, we have_

* **Part 1** \[\frac{\partial\sigma_{*,i}(A_{x})}{\partial x_{j}}=2\underbrace{\sigma_{*,*}(A _{x})}_{n\times n}\underbrace{(\sigma_{*,i}\circ A_{x,*,j})}_{n\times 1}-2 \underbrace{\sigma_{*,i}(A_{x})}_{n\times 1}\circ\underbrace{A_{x,*,j}}_{n\times 1}\]
* **Part 2.** \[\frac{\partial\sigma_{*,*}(A_{x})}{\partial x_{j}}\] \[=2\underbrace{\sigma_{*,*}(A_{x})}_{n\times n}\operatorname{diag} (A_{x,*,j})\sigma_{*,*}(A_{x})-\operatorname{diag}(A_{x,*,j})\sigma_{*,*}(A_{ x})-\sigma_{*,*}(A_{x})\operatorname{diag}(A_{x,*,j})\]
* **Part 3.** \[\underbrace{\frac{\partial\Sigma(A_{x})}{\partial x_{j}}}_{n\times n} =2\operatorname{diag}(\underbrace{\sigma_{*,*}^{\circ 2}(A_{x})}_{n \times n}\underbrace{A_{x,*,j}}_{n\times 1})-2\operatorname{diag}(\underbrace{ \Sigma(A_{x})}_{n\times n}\underbrace{A_{x,*,j}}_{n\times 1})\] \[=2\operatorname{diag}((\sigma_{*,*}^{\circ 2}(A_{x})-\Sigma(A_{x}))A_{ x,*,j})\]

Proof.: It follows from Fact I.10. 

### Gradient for \(H(x)\)

**Lemma I.12**.: _Recall the definitions of the following quantities:_

* _Let_ \(H(x)\in\mathbb{R}^{d\times d}\) _be defined as Definition I.2._
* _For each_ \(j\in[d]\)_, let_ \(P_{j}(x)\in\mathbb{R}^{n\times n}\) _be defined as Definition I.3._
* _For each_ \(j\in[d]\)_, for each_ \(k\in[d]\)_, let_ \(P_{j,k}(x)\in\mathbb{R}^{n\times n}\) _be defined as Definition_ I.3_._

_Then, we have_

* **Part 1.** _For each_ \(j\in[d]\)__ \[\frac{\partial H(x)}{\partial x_{j}}= -2A_{x}^{\top}\operatorname{diag}(\Sigma(A_{x})A_{x,*,j})A_{x}\] \[+A_{x}^{\top}P_{j}(x)A_{x}\]* **Part 2.**_For each_ \(j\in[d]\)_, for each_ \(k\in[d]\)__ \[\frac{\partial}{\partial x_{k}}(\frac{\partial H(x)}{\partial x_{j }})= +6A_{x}^{\top}\operatorname{diag}(A_{x,*,k})\Sigma(A_{x}) \operatorname{diag}(A_{x,*,j})A_{x}\] \[-2A_{x}^{\top}P_{k}(x)\operatorname{diag}(A_{x,*,j})A_{x}\] \[-2A_{x}^{\top}P_{j}(x)\operatorname{diag}(A_{x,*,k})A_{x}\] \[+A_{x}^{\top}P_{j,k}(x)A_{x}\]

Proof.: **Proof of Part 1.** We have

\[\frac{\partial H(x)}{\partial x_{j}} =\frac{\partial(A_{x}^{\top}\Sigma(A_{x})A_{x})}{\partial x_{j}}\] \[=(\frac{\partial A_{x}^{\top}}{\partial x_{j}})\cdot\Sigma(A_{x} )A_{x}+A_{x}^{\top}\cdot(\frac{\partial\Sigma(A_{x})}{\partial x_{j}})\cdot A _{x}+A_{x}^{\top}\Sigma(A_{x})\cdot(\frac{\partial A_{x}}{\partial x_{j}})\] \[= -A_{x}\operatorname{diag}(A_{x,*,j})\Sigma(A_{x})A_{x}\] \[+A_{x}^{\top}P(x)_{j}A_{x}\] \[-A_{x}\Sigma(A_{x})\operatorname{diag}(A_{x,*,j})A_{x}\] \[= -2A_{x}^{\top}\operatorname{diag}(\Sigma(A_{x})A_{x,*,j})A_{x}\] \[+A_{x}^{\top}P_{j}(x)A_{x}\]

**Proof of Part 2.** Then, we have

\[\frac{\partial}{\partial x_{k}}(\frac{\partial H(x)}{\partial x_{ j}}) =\frac{\partial}{\partial x_{k}}(-2A_{x}^{\top}\Sigma(A_{x}) \operatorname{diag}(A_{x,*,j})A_{x}+A_{x}^{\top}P_{j}(x)A_{x})\] \[=\frac{\partial}{\partial x_{k}}(-2A_{x}^{\top}\Sigma(A_{x}) \operatorname{diag}(A_{x,*,j})A_{x})+\frac{\partial}{\partial x_{k}}(A_{x}^{ \top}P_{j}(x)A_{x}),\] (22)

where the first step follows from **Part 1**, the second step follows from the sum rule.

For the first term of Eq. (22), we have

\[\frac{\partial}{\partial x_{k}}(-2A_{x}^{\top}\Sigma(A_{x}) \operatorname{diag}(A_{x,*,j})A_{x})\] \[= -2\frac{\partial}{\partial x_{k}}(A_{x}^{\top})\Sigma(A_{x}) \operatorname{diag}(A_{x,*,j})A_{x}-2A_{x}^{\top}\frac{\partial}{\partial x_ {k}}(\Sigma(A_{x}))\operatorname{diag}(A_{x,*,j})A_{x}\] \[-2A_{x}^{\top}\Sigma(A_{x})\frac{\partial}{\partial x_{k}}( \operatorname{diag}(A_{x,*,j}))A_{x}-2A_{x}^{\top}\Sigma(A_{x})\operatorname{ diag}(A_{x,*,j})\frac{\partial}{\partial x_{k}}(A_{x})\] \[= -2\frac{\partial}{\partial x_{k}}(A_{x}^{\top})\Sigma(A_{x}) \operatorname{diag}(A_{x,*,j})A_{x}-2A_{x}^{\top}\frac{\partial}{\partial x _{k}}(\Sigma(A_{x}))\operatorname{diag}(A_{x,*,j})A_{x}\] \[-2A_{x}^{\top}\Sigma(A_{x})\operatorname{diag}(\frac{\partial}{ \partial x_{k}}(A_{x,*,j}))A_{x}-2A_{x}^{\top}\Sigma(A_{x})\operatorname{ diag}(A_{x,*,j})\frac{\partial}{\partial x_{k}}(A_{x})\] \[= +2A_{x}^{\top}\operatorname{diag}(A_{x,*,k})\Sigma(A_{x}) \operatorname{diag}(A_{x,*,j})A_{x}\] \[-2A_{x}^{\top}P_{k}(x)\operatorname{diag}(A_{x,*,j})A_{x}\] \[+2A_{x}^{\top}\Sigma(A_{x})\operatorname{diag}(A_{x,*,j}) \operatorname{diag}(A_{x,*,k})A_{x}\] \[+2A_{x}^{\top}\Sigma(A_{x})\operatorname{diag}(A_{x,*,j}) \operatorname{diag}(A_{x,*,k})A_{x},\] (23)

For the second term of Eq. (22), we have

\[\frac{\partial}{\partial x_{k}}(A_{x}^{\top}P_{j}(x)A_{x})\] \[= \frac{\partial}{\partial x_{k}}(A_{x}^{\top})P_{j}(x)A_{x}+A_{x}^{ \top}\frac{\partial}{\partial x_{k}}(P_{j}(x))A_{x}+A_{x}^{\top}P_{j}(x)\frac {\partial}{\partial x_{k}}(A_{x})\] \[= -(A_{x}^{\top})\operatorname{diag}(A_{x,*,k})P_{j}(x)A_{x}+A_{x}^ {\top}P_{j,k}(x)A_{x}-A_{x}^{\top}P_{j}(x)\operatorname{diag}(A_{x,*,k})A_{x}\]\[= -(A_{x}^{\top})\operatorname{diag}(A_{x,*,k})P_{j}(x)A_{x}+A_{x}^{ \top}P_{j,k}(x)A_{x}-A_{x}^{\top}\operatorname{diag}(A_{x,*,k})P_{j}(x)A_{x}\] \[= -2(A_{x}^{\top})\operatorname{diag}(A_{x,*,k})P_{j}(x)A_{x}+A_{x}^ {\top}P_{j,k}(x)A_{x}.\] (24)

By combining Eq. (22), Eq. (23), and Eq. (24), we have

\[\frac{\partial}{\partial x_{k}}(\frac{\partial H(x)}{\partial x_ {j}})= +2A_{x}^{\top}\operatorname{diag}(A_{x,*,k})\Sigma(A_{x}) \operatorname{diag}(A_{x,*,j})A_{x}\] \[-2A_{x}^{\top}P_{k}(x)\operatorname{diag}(A_{x,*,j})A_{x}\] \[+2A_{x}^{\top}\Sigma(A_{x})\operatorname{diag}(A_{x,*,j}) \operatorname{diag}(A_{x,*,k})A_{x}\] \[+2A_{x}^{\top}\Sigma(A_{x})\operatorname{diag}(A_{x,*,j}) \operatorname{diag}(A_{x,*,k})A_{x}\] \[-2A_{x}^{\top}\operatorname{diag}(A_{x,*,k})P_{j}(x)A_{x}\] \[+A_{x}^{\top}P_{j,k}(x)A_{x}\] \[= +6A_{x}^{\top}\operatorname{diag}(A_{x,*,k})\Sigma(A_{x}) \operatorname{diag}(A_{x,*,j})A_{x}\] \[-2A_{x}^{\top}P_{k}(x)\operatorname{diag}(A_{x,*,j})A_{x}\] \[-2A_{x}^{\top}P_{j}(x)\operatorname{diag}(A_{x,*,k})A_{x}\] \[+A_{x}^{\top}P_{j,k}(x)A_{x}.\]

Thus, we complete the proof. 

### Hessian of \(H(x)\)

**Lemma I.13**.: _Recall the definitions of following quantities:_

* _Let_ \(H(x):=A_{x}^{\top}\Sigma(A_{x})A_{x}\in\mathbb{R}^{d\times d}\) _be as definition I.2._
* _Let_ \(Q(x):=\sigma_{*,*}(A_{x})\in\mathbb{R}^{n\times n}\) _be defined as Definition I.4._

_Then, we have_

* **Part 1**.: _For each_ \(j\in[d]\) _and for each_ \(k\in[d]\)__ \[\frac{\partial^{2}H(x)}{\partial x_{j}x_{k}}=C_{1}+C_{2}+C_{3}+C_{4}+C_{5}\] _where we define some local_ \(d\times d\) _size matrix variables_
* \(C_{1}=+20A_{x}^{\top}\operatorname{diag}(A_{x,*,j})\Sigma(A_{x}) \operatorname{diag}(A_{x,*,k})A_{x}\)__
* \(C_{2}=-6A_{x}^{\top}\operatorname{diag}(Q^{\circ 2}(x)(A_{x,*,k}\circ A_{x,*,j}))A_{x}\)__
* \(C_{3}=-8A_{x}^{\top}\operatorname{diag}(A_{x,*,k})\operatorname{diag}(Q^{ \circ 2}(x)A_{x,*,j})A_{x}\)__
* \(C_{4}=-8A_{x}^{\top}\operatorname{diag}(A_{x,*,j})\operatorname{diag}(Q^{ \circ 2}(x)A_{x,*,k})A_{x}\)__
* \(C_{5}=+8A_{x}^{\top}((Q(x)\operatorname{diag}(A_{x,*,k})Q(x)\operatorname{ diag}(A_{x,*,j})Q(x))\circ I_{n})A_{x}\)__

Proof.: By **Part 2** of Lemma I.12, we can show that

\[\frac{\partial^{2}H(x)}{\partial x_{j}x_{k}}=B_{1}+B_{2}+B_{3}+B_{4}\]

where

\[B_{1}= +6A_{x}^{\top}\operatorname{diag}(A_{x,*,k})\Sigma(A_{x}) \operatorname{diag}(A_{x,*,j})A_{x}\] \[B_{2}= -2A_{x}^{\top}P_{k}(x)\operatorname{diag}(A_{x,*,j})A_{x}\] \[B_{3}= -2A_{x}^{\top}P_{j}(x)\operatorname{diag}(A_{x,*,k})A_{x}\] \[B_{4}= +A_{x}^{\top}P_{j,k}(x)A_{x}\]

where \(B_{2}\) can be decomposed further as

\[B_{2}= -2A_{x}^{\top}2\operatorname{diag}((Q^{\circ 2}(x)-\Sigma(A_{x}))A_{x,*,k}) \operatorname{diag}(A_{x,*,j})A_{x}\]

[MISSING_PAGE_EMPTY:59]

### Gradient and Hessian of \(F(x)\)

**Lemma I.14**.: _Let \(F(x)\in\mathbb{R}\) be defined as Definition I.8. Then, we have_

* **Part 1.** \[\frac{\partial F(x)}{\partial x_{j}}=\operatorname{tr}[(H(x)+I_{d})^{-1}\cdot \frac{\partial H(x)}{\partial x_{j}}]\]
* **Part 2** \[\frac{\partial}{\partial x_{k}}\frac{\partial F(x)}{\partial x_{j}} = +\operatorname{tr}[(H(x)+I_{d})^{-1}\frac{\partial^{2}H}{\partial x _{j}\partial x_{k}}]\] \[-\operatorname{tr}[(H(x)+I_{d})^{-1}\frac{\partial H(x)}{ \partial x_{k}}(H(x)+I_{d})^{-1}\cdot\frac{\partial H(x)}{\partial x_{j}}]\]

Proof.: **Proof of Part 1.** We have

\[\frac{\partial F(x)}{\partial x_{j}} =\frac{\partial\log(\det(H(x)+I_{d}))}{\partial x_{j}}\] \[= \operatorname{tr}[(H(x)+I_{d})^{-1}\frac{\partial(H(x)+I_{d})}{ \partial x_{j}}]\] \[= \operatorname{tr}[(H(x)+I+d)^{-1}(\frac{\partial H(x)}{\partial x _{j}}+\frac{\partial I_{d}}{\partial x_{j}})]\] \[= \operatorname{tr}[(H(x)+I_{d})^{-1}\cdot\frac{\partial H(x)}{ \partial x_{j}}],\]

where the first step follows from the definition of \(F(x)\) (see Definition I.8), the third step follows from the basic calculus rule, and the last step follows from \(\frac{\partial I_{d}}{\partial x_{j}}=0\).

**Proof of Part 2.** We have

\[\frac{\partial}{\partial x_{k}}\frac{\partial F(x)}{\partial x_{j}}\] \[= \frac{\partial}{\partial x_{k}}(\operatorname{tr}[(H(x)+I_{d})^{- 1}\cdot\frac{\partial H(x)}{\partial x_{j}}])\] \[= \operatorname{tr}[\frac{\partial}{\partial x_{k}}((H(x)+I_{d})^{ -1})\cdot\frac{\partial H(x)}{\partial x_{j}}]+\operatorname{tr}[(H(x)+I_{d} )^{-1}\cdot\frac{\partial}{\partial x_{k}}(\frac{\partial H(x)}{\partial x_{ j}})]\] \[= \operatorname{tr}[-(H(x)+I_{d})^{-1}\frac{\partial}{\partial x_{k }}(H(x))(H(x)+I_{d})^{-1}\cdot\frac{\partial H(x)}{\partial x_{j}}]+ \operatorname{tr}[(H(x)+I_{d})^{-1}\cdot\frac{\partial}{\partial x_{k}}( \frac{\partial H(x)}{\partial x_{j}})]\] \[= +\operatorname{tr}[(H(x)+I_{d})^{-1}\frac{\partial^{2}H}{ \partial x_{j}\partial x_{k}}]-\operatorname{tr}[(H(x)+I_{d})^{-1}\frac{ \partial H(x)}{\partial x_{k}}(H(x)+I_{d})^{-1}\cdot\frac{\partial H(x)}{ \partial x_{j}}],\]

where the first step follows from **Part 1**, the second step follows from the product rule, and the last step follows from simple algebra. 

Using standard algebraic tools in literature (Lee and Sidford, 2019), we can show that,

**Lemma I.15**.: _We can rewrite it as follows_

\[\frac{\partial}{\partial x}\frac{\partial F}{\partial x} =20H_{1}-6H_{2}-8H_{3}-8H_{4}+8H_{5}\] \[\quad-4G_{1}+8G_{2}+8G_{3}-16G_{4}\]

_and_

\[\frac{\partial}{\partial x}\frac{\partial F}{\partial x}\succ 0.\]

_Thus, the function \(F\) is convex in \(x\)._

### Quantity I for Hessian of \(F\)

**Lemma I.16**.: _We have_

\[[\frac{\partial}{\partial x}\frac{\partial F}{\partial x^{\top}}]_{1}=20H_{1}-6H_{ 2}-8H_{3}-8H_{4}+8H_{5}\]

_where_

* \(H_{1}=A_{x}^{\top}\widetilde{\Sigma}(A_{x})\Sigma(A_{x})A_{x}\)__
* \(H_{2}=A_{x}^{\top}\operatorname{diag}(Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x}) \mathbf{1}_{n})A_{x}\)__
* \(H_{3}=A_{x}^{\top}Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x})A_{x}\)__
* \(H_{4}=A_{x}^{\top}\widetilde{\Sigma}(A_{x})Q^{\circ 2}(x)A_{x}\)__
* \(H_{5}=A_{x}^{\top}(Q(x)\widetilde{\Sigma}(A_{x})Q(x))\circ Q(x))A_{x}\)__

Proof.: It comes from \(P_{i,j}\) and the following lemma. 

**Lemma I.17**.: _Let \(H_{1}\in\mathbb{R}^{d\times d}\) be Hessian where each entry is_

\[(H_{1})_{j,k}=\operatorname{tr}[(H(x)+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(A_{x,*,j})\Sigma(A_{x})\operatorname{diag}(A_{x,*,k})A_{x}]\]

_Then, we have_

\[H_{1}=A_{x}^{\top}\widetilde{\Sigma}(A_{x})\Sigma(A_{x})A_{x}\]

Proof.: We can rewrite \((H_{1})_{j,k}\) as follows

\[(H_{1})_{j,k} = \operatorname{tr}[(H(x)+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(A_{x,*,j})\Sigma(A_{x})\operatorname{diag}(A_{x,*,k})A_{x}]\] \[= \operatorname{tr}[A_{x}(H(x)+I_{d})^{-1}A_{x}^{\top}\operatorname {diag}(A_{x,*,j})\Sigma(A_{x})\operatorname{diag}(A_{x,*,k})]\] \[= \operatorname{tr}[\widetilde{\sigma}_{*,*}(A_{x})\operatorname{ diag}(A_{x,*,j})\Sigma(A_{x})\operatorname{diag}(A_{x,*,k})]\] \[= A_{x,*,j}^{\top}\widetilde{\Sigma}(A_{x})\Sigma(A_{x})A_{x,*,k},\]

where the first step follows from the Lemma statement, the second step follows from the fact that all of \((H(x)+I_{d})^{-1}\), \(\operatorname{diag}(A_{x,*,j})\), \(\Sigma(A_{x})\), and \(\operatorname{diag}(A_{x,*,k})\) are diagonal matrices, the third step follows from the definition of \(\widetilde{\sigma}_{*,*}\) (see Definition I.7), and the last step follows from the definition of \(\widetilde{\Sigma}(A_{x})\) (see Definition I.7).

Thus, we have

\[H_{1}=A_{x}^{\top}\widetilde{\Sigma}(A_{x})\Sigma(A_{x})A_{x}.\qed\]

**Lemma I.18**.: _Let \(H_{2}\) be defined as_

\[(H_{2})_{j,k}=\operatorname{tr}[(H(x)+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(Q^{\circ 2}(x)(A_{x,*,k}\circ A_{x,*,j}))A_{x}]\]

_Then, we have_

\[H_{2}=A_{x}^{\top}\operatorname{diag}(Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x}) \mathbf{1}_{n})A_{x}\]

Proof.: We have

\[(H_{2})_{j,k} = \operatorname{tr}[(H(x)+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(Q^{\circ 2}(x)(A_{x,*,k}\circ A_{x,*,j}))A_{x}]\] \[= \operatorname{tr}[\widetilde{\Sigma}(A_{x})\operatorname{diag}(Q^ {\circ 2}(x)(A_{x,*,k}\circ A_{x,*,j}))]\] \[= \mathbf{1}_{n}^{\top}\widetilde{\Sigma}(A_{x})Q^{\circ 2}(x)(A_{x,*,k }\circ A_{x,*,j})\] \[= A_{x,*,k}^{\top}\operatorname{diag}(Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x} )\mathbf{1}_{n})A_{x,*,j}\]

where the first step follows from the Lemma statement, the second step follows from the definition of \(\widetilde{\Sigma}(A_{x})\) (see Definition I.7), the third step follows from the definition of \(\mathbf{1}_{n}^{\top}\), and the last step follows from the definition of \(\Sigma(A_{x})\).

Thus, we have

\[H_{2}=A_{x}^{\top}\operatorname{diag}(Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x}) \mathbf{1}_{n})A_{x}.\qed\]

**Lemma I.19**.: _Let \(H_{3}\) be defined as_

\[(H_{3})_{j,k}=\operatorname{tr}[(H(x)+I_{d})^{-1}A_{x}^{\top}\operatorname{diag}( A_{x,*,k})\operatorname{diag}(Q^{\circ 2}(x)(A_{x,*,j}))A_{x}]\]

_Then, we_

\[H_{3}=A_{x}^{\top}Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x})A_{x}\]

Proof.: Then, we have

\[(H_{3})_{j,k} = \operatorname{tr}[(H(x)+I_{d})^{-1}A_{x}^{\top}\operatorname{diag} (A_{x,*,k})\operatorname{diag}(Q^{\circ 2}(x)(A_{x,*,j}))A_{x}]\] \[= \operatorname{tr}[\widetilde{\Sigma}(A_{x})\operatorname{diag}( A_{x,*,k})\operatorname{diag}(Q^{\circ 2}(x)(A_{x,*,j}))]\] \[= A_{x,*,k}^{\top}\widetilde{\Sigma}(A_{x})Q^{\circ 2}(x)A_{x,*,j},\]

where the first step follows from the Lemma statement, the second step follows from the definition of \(\widetilde{\Sigma}(A_{x})\) (see Definition I.7), and the last step follows from the property of \(\operatorname{diag}(\cdot)\).

Thus, we have

\[H_{3}=A_{x}^{\top}Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x})A_{x}.\qed\]

**Lemma I.20**.: _Let \(H_{4}\) be defined as_

\[(H_{4})_{j,k}=\operatorname{tr}[(H(x)+I_{d})^{-1}A_{x}^{\top} \operatorname{diag}(A_{x,*,j})\operatorname{diag}(Q^{\circ 2}(x)(A_{x,*,k}))A_{x}]\]

_Then, we have_

\[H_{5}=A_{x}^{\top}(Q(x)\widetilde{\Sigma}(A_{x})Q(x))\circ Q(x))A_{x}\]

Proof.: We have

\[(H_{5})_{j,k} = \operatorname{tr}[(H(x)+I_{d})^{-1}A_{x}^{\top}(Q(x)\operatorname{ diag}(A_{x,*,k})Q(x)\operatorname{diag}(A_{x,*,j})Q(x))\circ I_{n})A_{x}]\] \[= \operatorname{tr}[\widetilde{\Sigma}(A_{x})(Q(x)\operatorname{ diag}(A_{x,*,k})Q(x)\operatorname{diag}(A_{x,*,j})Q(x))\circ I_{n}]\] \[= \sum_{l=1}^{n}\widetilde{\Sigma}_{l,l}(A_{x})Q_{*,l}(x)^{\top} \operatorname{diag}(A_{x,*,k})Q(x)\operatorname{diag}(A_{x,*,j})Q_{*,l}(x)\] \[= A_{x,*,k}^{\top}\sum_{l=1}^{n}\widetilde{\Sigma}_{l,l}(A_{x}) \operatorname{diag}(Q_{*,l}(x))Q(x)\operatorname{diag}(Q_{*,l}(x))A_{x,*,j}\] \[= A_{x,*,k}^{\top}(\sum_{l=1}^{n}\widetilde{\Sigma}_{l,l}(A_{x}) (Q_{*,l}(x)Q_{*,l}(x)^{\top}\circ Q(x)))A_{x,*,j}\] \[= A_{x,*,k}^{\top}((Q(x)\widetilde{\Sigma}(A_{x})Q(x))\circ Q(x))A _{x,*,j},\]

where the first step follows from the Lemma statement, the second step follows from the definition of \(\widetilde{\Sigma}(A_{x})\) (see Definition I.7), the third step follows from the definition of \(\operatorname{tr}[\cdot]\), and the last step follows from the fact that \(\widetilde{\Sigma}(A_{x})\) is a diagonal matrix (see Definition I.7).

Thus, we have

\[H_{5}=A_{x}^{\top}(Q(x)\widetilde{\Sigma}(A_{x})Q(x))\circ Q(x))A_{x}.\qed\]

### Quantity II for Hessian of \(F\)

Recall that

\[\frac{\partial H}{\partial x_{j}}= -2A_{x}^{\top}\operatorname{diag}(\Sigma(A_{x})A_{x,*,j})A_{x}\] \[+A_{x}^{\top}\frac{\partial\Sigma(A_{x})}{\partial x_{j}}A_{x}\] \[= -2A_{x}^{\top}\operatorname{diag}(\Sigma(A_{x})A_{x,*,j})A_{x}\] \[+2A_{x}^{\top}\operatorname{diag}((Q^{\circ 2}(x)-\Sigma(A_{x}))A_{x,*,j} )A_{x}\] \[=A_{x}^{\top}\operatorname{diag}(2Q^{\circ 2}(x)-4\Sigma(A_{x}))A_{x,*,j} )A_{x}\]

**Lemma I.22**.: _For the second item, we have_

\[[\frac{\partial}{\partial x}\frac{\partial F}{\partial x}]_{2}=-4G_{1}+8G_{2}+ 8G_{3}-16G_{4}\]

* \(G_{1}=A_{x}^{\top}Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x})^{2}Q^{\circ 2}(x)A_{x}\)__
* \(G_{2}=A_{x}^{\top}Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x})^{2}\Sigma(A_{x})A_{x}\)__
* \(G_{2}=A_{x}^{\top}\Sigma(A_{x})\widetilde{\Sigma}(A_{x})^{2}Q^{\circ 2}(x)A_{x}\)__
* \(G_{4}=A_{x}^{\top}\Sigma(A_{x})\widetilde{\Sigma}(A_{x})^{2}\Sigma(A_{x})A_{x}\)__

Proof.: The proof follows by combining the following lemmas. 

**Lemma I.23**.: _Let \((G_{1})_{j,k}\)_

\[(G_{1})_{j,k}=\,\operatorname{tr}[(H+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(Q^{\circ 2}(x)A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(Q^{\circ 2}(x)A_{x,*,k})A_{x}]\]

_Then, we have_

\[G_{1}=A_{x}^{\top}Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x})^{2}Q^{\circ 2 }(x)A_{x}\]

Proof.: We can show

\[(G_{1})_{j,k}= \,\operatorname{tr}[(H+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(Q^{\circ 2}(x)A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(Q^{\circ 2}(x)A_{x,*,k})A_{x}]\] \[= \,\operatorname{tr}[\widetilde{\Sigma}(A_{x})\operatorname{ diag}(Q^{\circ 2}A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(Q^{\circ 2}(x)A_{x,*,k})]\] \[= \,\operatorname{tr}[\widetilde{\Sigma}(A_{x})\operatorname{ diag}(Q^{\circ 2}A_{x,*,j})\widetilde{\Sigma}(A_{x})\operatorname{ diag}(Q^{\circ 2}(x)A_{x,*,k})]\] \[= \,A_{x,*,j}^{\top}Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x})^{2}Q^{ \circ 2}(x)A_{x,*,k}\]

Thus, we have

\[G_{1}=A_{x}^{\top}Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x})^{2}Q^{\circ 2 }(x)A_{x}.\qed\]

The proofs of \(G_{2},G_{3},G_{4}\) are similar to \(G_{1}\). So we omit the details here.

**Lemma I.24**.: _Let \((G_{2})_{j,k}\)_

\[(G_{2})_{j,k}=\,\operatorname{tr}[(H+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(Q^{\circ 2}(x)A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(\Sigma(A_{x})A_{x,*,k})A_{x}]\]

_Then, we have_

\[G_{2}=A_{x}^{\top}Q^{\circ 2}(x)\widetilde{\Sigma}(A_{x})^{2}\Sigma(A_{x})A_{x}.\]

**Lemma I.25**.: _Let \((G_{3})_{j,k}\)_

\[(G_{3})_{j,k}=\,\operatorname{tr}[(H+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(\Sigma(A_{x})A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(Q^{\circ 2}(x)A_{x,*,k})A_{x}]\]

_Then, we have_

\[G_{3}=A_{x}^{\top}\Sigma(A_{x})\widetilde{\Sigma}(A_{x})^{2}Q^{\circ 2}(x)A_{x}.\]

**Lemma I.26**.: _Let \((G_{4})_{j,k}\)_

\[(G_{4})_{j,k}=\,\operatorname{tr}[(H+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(\Sigma(A_{x})A_{x,*,j})A_{x}(H+I_{d})^{-1}A_{x}^{\top}\operatorname{ diag}(\Sigma(A_{x})A_{x,*,k})A_{x}]\]

_Then, we have_

\[G_{4}=A_{x}^{\top}\Sigma(A_{x})\widetilde{\Sigma}(A_{x})^{2}\Sigma(A_{x})A_{x}.\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main result of this paper is a class of Dikin walks for log-concave sampling from convex bodies with self-concordant barrier functions. The abstract and introduction reflect the complexity of these walks, and the remainder of the paper is dedicated to prove these assertions. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations are discussed in the last paragraph of Section 4. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The assumptions are clearly stated in the statement of the theorems: convex body contained in a ball of radius \(R\), density is log-Lipschitz with parameter \(L\), the walk starts from a \(w\)-warm starting point. See Theorem 1.1, 1.2 and 1.3. The proofs of Theorem 1.1 and 1.2 results can be found in Section C and D for mixing time, and Section F for per iteration cost. Proofs of Theorem 1.3 can be found in Section G. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The results in this paper are theoretical with no experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The results in this paper are theoretical with no experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The results in this paper are theoretical with no experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The results in this paper are theoretical with no experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The results in this paper are theoretical with no experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have carefully read Code of Ethics and the paper is adhered to those principles. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: The potential societal impact is discussed in the last paragraph of Section 4. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The results in this paper are theoretical with no experiments. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The results in this paper are theoretical with no experiments. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The results in this paper are theoretical with no experiments. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The results in this paper are theoretical with no experiments. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The results in this paper are theoretical with no experiments. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.