# Unveiling Transformer Perception by

Exploring Input Manifolds

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

This paper introduces a general method for the exploration of equivalence classes in the input space of Transformer models. The proposed approach is based on sound mathematical theory which describes the internal layers of a Transformer architecture as sequential deformations of the input manifold. Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model, we are able to reconstruct equivalence classes in the input space and navigate across them. We illustrate how this method can be used as a powerful tool for investigating how a Transformer sees the input space, facilitating local and task-agnostic explainability in Computer Vision and Natural Language Processing tasks.

## 1 Introduction

In this paper, we propose a method for exploring the input space of Transformer models by identifying equivalence classes with respect to their predictions. We define an _equivalence class_ of a Transformer model as the set of vectors in the embedding space whose outcomes under the Transformer process are the same. The study of the input manifold on which the inverse image of models lies provides insights for both explainability and sensitivity analyses. Existing methods aiming at the exploration of the input space of Deep Neural Networks and Transformers either rely on perturbations of input data using heuristic or gradient-based criteria [16; 22; 17; 14], or they analyze specific properties of the embedding space [5].

Our approach is based on sound mathematical theory which describes the internal layers of a Transformer architecture as sequential deformations of the input manifold. Using eigendecomposition of the pullback of the distance metric defined on the output space through the Jacobian of the model, we are able to reconstruct equivalence classes in the input space and navigate across them. In the XAI scenario, our framework can facilitate local and task-agnostic explainability methods applicable to Computer Vision (CV) and Natural Language Processing (NLP) tasks, among others.

In Section 2, we summarise the preliminaries of the mathematical foundations of our approach. In Section 3, we present our method for the exploration of equivalence classes in the input of the Transformer models. In Section 4, we perform a preliminary investigation of some applicability options of our method on textual and visual data. In Section 5, we discuss the relevant literature about embedding space exploration and feature importance. Finally, in Section 6, we give our concluding remarks1.

[FOOTNOPreliminaries

In this Section, we provide the theoretical foundation of the proposed approach, namely the Geometric Deep Learning framework based on Riemannian Geometry [2].

A neural network is considered as a sequence of maps, the layers of the network, between manifolds, and the latter are the spaces where the input and the outputs of the layers belong to.

**Definition 1** (Neural Network).: _A neural network is a sequence of \(\mathcal{C}^{1}\) maps \(\Lambda_{i}\) between manifolds of the form:_

\[M_{0}\xrightarrow{\Lambda_{1}}M_{1}\xrightarrow{\Lambda_{2}}M_{2}\xrightarrow{ \Lambda_{4}}\cdots\xrightarrow{\Lambda_{n-1}}M_{n-1}\xrightarrow{\Lambda_{n}}M _{n}\] (1)

_We call \(M_{0}\) the input manifold and \(M_{n}\) the output manifold. All the other manifolds of the sequence are called representation manifolds. The maps \(\Lambda_{i}\) are the layers of the neural network. We denote with \(\mathcal{N}_{(i)}=\Lambda_{n}\circ\cdots\circ\Lambda_{i}:M_{i}\to M_{n}\) the mapping from the \(i\)-th representation layer to the output layer._

As an example, consider a shallow network with just one layer, the composition of a linear operator \(A\cdot+b\) with a sigmoid function \(\sigma\), where \(A\in\mathbb{R}^{m\times n}\) and \(b\in\mathbb{R}^{m}\): then, the input manifold \(M_{0}\) and the output manifold \(M_{1}\) shall be \(\mathbb{R}^{n}\) and \(\mathbb{R}^{m}\), respectively, and the map \(\Lambda_{1}(\cdot)=\sigma(A\cdot+b)\). We generalize this observation into the following definition.

**Definition 2** (Smooth layer).: _A map \(\Lambda_{i}:M_{i-1}\to M_{i}\) is called a smooth layer if it is the restriction to \(M_{i-1}\) of a function \(\overline{\Lambda}^{(i)}(x):\mathbb{R}^{d_{i-1}}\to\mathbb{R}^{d_{i}}\) of the form_

\[\overline{\Lambda}^{(i)}_{\alpha}(x)=F^{(i)}_{\alpha}\left(\sum_{\beta}A^{(i) }_{\alpha\beta}x_{\beta}+b^{(i)}_{\alpha}\right)\] (2)

_for \(i=1,\cdots,n\), \(x\in\mathbb{R}^{d_{i}}\), \(b^{(i)}\in\mathbb{R}^{d_{i}}\) and \(A^{(i)}\in\mathbb{R}^{d_{i}\times d_{i-1}}\), with \(F^{(i)}:\mathbb{R}^{d_{i}}\to\mathbb{R}^{d_{i}}\) a diffeomorphism._

**Remark 1**.: _Transformers implicitly apply for this framework, since their modules are smooth functions, such as fully connected layers, GeLU and sigmoid activations._

Our aim is to transport the geometric information on the data lying in the output manifold to the input manifold: this allows us to obtain insight on how the network "sees" the input space, how it manipulates it for reaching its final conclusion. For fulfilling this objective, we need several tools from differential geometry. The first key ingredient is the notion of singular Riemannian metric, which has the intuitive meaning of a degenerate scalar product which changes point to point.

**Definition 3** (Singular Riemannian metric).: _Let \(M=\mathbb{R}^{n}\) or an open subset of \(\mathbb{R}^{n}\). A singular Riemannian metric \(g\) over \(M\) is a map \(g:M\to Bil(\mathbb{R}^{n}\times\mathbb{R}^{n})\) that associates to each point \(p\) a positive semidefinite symmetric bilinear form \(g_{p}:\mathbb{R}^{n}\times\mathbb{R}^{n}\to\mathbb{R}\) in a smooth way._

Without loss of generality, we can assume the following hypotheses on the sequence (1): _i)_ The manifolds \(M_{i}\) are open and path-connected sets of dimension \(\dim M_{i}=d_{i}\). _ii)_ The maps \(\Lambda_{i}\) are \(\mathcal{C}^{1}\) submersions. _iii)_\(\Lambda_{i}(M_{i-1})=M_{i}\) for every \(i=1,\cdots,n\). _iv)_ The manifold \(M_{n}\) is equipped with the structure of Riemannian manifold, with metric \(g^{(n)}\). Definition 3 naturally leads to the definition of the pseudolength and of energy of a curve.

**Definition 4** (Pseudolength and energy of a curve).: _Let \(\gamma:[a,b]\to\mathbb{R}^{n}\) a curve defined on the interval \([a,b]\subset\mathbb{R}\) and \(\|v\|_{p}=\sqrt{g_{p}(v,v)}\) the pseudo-norm induced by the pseudo-metric \(g_{p}\) at point \(p\). Then the pseudolength of \(\gamma\) and its energy are defined as_

\[Pl(\gamma)=\int_{a}^{b}\|\dot{\gamma}(s)\|_{\gamma(s)}ds=\int_{a}^{b}\sqrt{g_{ \gamma(s)}(\dot{\gamma}(s),\dot{\gamma}(s))}ds,\qquad E(\gamma)=\int_{a}^{b}\| \dot{\gamma}(s)\|_{\gamma(s)}^{2}ds\] (3)

The notion of pseudolength leads naturally to define the distance between two points.

**Definition 5** (Pseudodistance).: _Let \(x,y\in M=\mathbb{R}^{n}\). The pseudodistance between \(x\) and \(y\) is then_

\[Pd(x,y)=\inf\{Pl(\gamma)\mid\gamma:[0,1]\to M,\,\gamma\in\mathcal{C}^{1}([0,1 ]),\,\gamma(0)=x,\,\gamma(1)=y\}.\] (4)One can observe that endowing the space \(\mathbb{R}^{n}\) with a singular Riemannian metric leads to have non trivial curves whose length is zero. A straightforward consequence is that there are distinct points whose pseudodistance is therefore zero: a natural equivalence relation arises, _i.e._\(x\sim y\Leftrightarrow Pd(x,y)=0\), obtaining thus a metric space \((\mathbb{R}^{n}/\sim,Pd)\).

The second crucial tool is the notion of pullback of a function. Let \(f\) be a function from \(\mathbb{R}^{p}\) to \(\mathbb{R}^{q}\), and fix the coordinate systems \(x=(x_{1},\ldots,x_{p})\) and \(y=(y_{1},\ldots,y_{q})\) on \(\mathbb{R}^{p}\) and on \(\mathbb{R}^{q}\), respectively. Moreover, we endow \(\mathbb{R}^{q}\) with the standard Euclidean metric \(g\), whose associated matrix is the identity. The space \(\mathbb{R}^{p}\) can be equipped with the pullback metric \(f^{*}g\) whose representation matrix reads as

\[(f^{*}g)_{ij}=\sum_{h,k=1}^{q}\left(\frac{\partial f_{h}}{\partial x_{i}} \right)g_{hk}\left(\frac{\partial f_{k}}{\partial x_{j}}\right).\] (5)

The sequence (1) shows that a neural network can be considered simply as a function, a composition of maps: hence, taking \(f=\Lambda_{n}\circ\Lambda_{n-1}\circ\cdots\circ\Lambda_{1}\) and supposing that \(M_{0}=\mathbb{R}^{p},M_{n}=\mathbb{R}^{q}\), the generalization of (5) applied to (1) provides with the pullback of a generic neural network.

Hereafter, we consider in (1) the case \(M_{n}=\mathbb{R}^{q}\), equipped with the trivial metric \(g^{(n)}=I_{q}\), _i.e._, the identity. Each manifold \(M_{i}\) of the sequence (1) is equipped with a Riemannian singular metric, denoted with \(g^{(i)}\), obtained via the pullback of \(\mathcal{N}_{(i)}\). The pseudolength of a curve \(\gamma\) on the \(i\)-th manifold, namely \(Pl_{i}(\gamma)\), is computed via the relative metric \(g^{(i)}\) via (3).

### General results

We depict hereafter the theoretical bases of our approach. We denote with \(\mathcal{N}_{i}\) the submap \(\Lambda_{i}\circ\cdots\circ\Lambda_{n}:M_{i}\to M_{n}\), and with \(\mathcal{N}\equiv\mathcal{N}_{0}\) the map describing the action of the complete network. The starting point is to consider the pair \((M_{i},Pd_{i})\): this is a pseudometric space, which can be turned into a full-fledged metric space \(M_{i}/\sim_{i}\) by the metric identification \(x\sim_{i}y\Leftrightarrow Pd_{i}(x,y)=0\). The first result states that the length of a curve on the \(i\)-th manifold is preserved among the mapping on the subsequent manifolds.

**Proposition 1**.: _Let \(\gamma:[0,1]\to M_{i}\) be a piecewise \(\mathcal{C}^{1}\) curve. Let \(k\in\{i,i+1,\cdots,n\}\) and consider the curve \(\gamma_{k}=\Lambda_{k}\circ\cdots\circ\Lambda_{i}\circ\gamma\) on \(M_{k}\). Then \(Pl_{i}(\gamma)=Pl_{k}(\gamma_{k})\)._

In particular this is true when \(k=n\), _i.e._, the length of a curve is preserved in the last manifold. This result leads naturally to claim that if two points are in the same class of equivalence, then they are mapped into the same point under the action of the neural network.

**Proposition 2**.: _If two points \(p,q\in M_{i}\) are in the same class of equivalence, then \(\mathcal{N}_{i}(p)=\mathcal{N}_{i}(q)\)._

The next step is to prove that the sets \(M_{i}/\sim_{i}\) are actually smooth manifolds: to this aim, we introduce another equivalence relation: \(x\sim_{\mathcal{N}_{i}}y\) if and only if there exists a piecewise \(\gamma:[0,1]\to M_{i}\) such that \(\gamma(0)=x,\gamma(1)=y\) and \(\mathcal{N}_{i}\circ\gamma(s)=\mathcal{N}_{i}(x)\ \forall s\in[0,1]\). The introduction of this equivalence relation allows us to easily state the following proposition.

**Proposition 3**.: _Let \(x,y\in M_{i}\), then \(x\sim_{i}y\) if and only if \(x\sim_{\mathcal{N}_{i}}y\)._

The following corollary contains the natural consequences of the previous result; the second point of the claim below is the counterpart of Proposition 2.

**Corollary 1**.: _Under the hypothesis of Proposition 3, one has that \(M_{i}/\!\!\sim_{i}=M_{i}/\!\!\sim_{\mathcal{N}_{i+1}}\). Moreover, if two points \(p,q\in M_{i}\) are connected by a \(\mathcal{C}^{1}\) curve \(\gamma:[0,1]\to M_{i}\) satisfying \(\mathcal{N}_{i}(p)=\mathcal{N}_{i}\circ\gamma(s)\) for every \(s\in[0,1]\), then they lie in the same class of equivalence._

Making use of the Godement's criterion, we are now able to prove that the set \(M_{i}/\sim_{i}\) is a smooth manifold, together with its dimension.

**Proposition 4**.: \(\frac{M_{i}}{\sim_{i}}\) _is a smooth manifold of dimension \(dim(\mathcal{N}(M_{0}))\)._

This last achievement provides practical insights about the projection \(\pi_{i}\) on the quotient space, that consists the building block of the algorithms used for recovering and exploring the equivalence classes of a neural network.

**Proposition 5**.: \(\pi_{i}:M_{i}\to M_{i}/\sim_{i}\) _is a smooth fiber bundle, with \(Ker(d\pi_{i})=\mathcal{V}M_{i}\), which is therefore an integrable distribution. \(\mathcal{V}M_{i}\) is the vertical bundle of \(M_{i}\). Every class of equivalence \([p]\) is a path-connected submanifold of \(M_{i}\) and coincide with the fiber of the bundle over the point \(p\in M_{i}\)._

## 3 Methodology

The results depicted in Section 2.1 provide powerful tools for investigating how a neural network sees the input space starting from a point \(x\). In particular we point out the following remarks: i) If two points \(x,y\) belonging to the input manifold \(M_{0}\) are are such that \(x\sim_{0}y\), then \(\mathcal{N}(x)=\mathcal{N}(y)\); ii) given a point \(p\in M_{n}\), the counterimage \(\mathcal{N}^{-1}(p)\) is a smooth manifold, whose connected components are classes of equivalences in \(M_{0}\) with respect to \(\sim_{0}\). A necessary condition for two points \(x,y\in M_{0}\) to be in the same class of equivalence is that \(\mathcal{N}(x)=\mathcal{N}(y)\); iii) any class of equivalence \([x]\), \(x\in M_{0}\), is a maximal integral submanifold of \(\mathcal{V}M_{0}\). The above observations directly provide with a strategy to build up the equivalence class of an input point \(x\in M_{0}\). Proposition 5 tells us that \(\mathcal{V}M_{0}\) is an integrable distribution, with dimension equal to the dimension of the kernel of \(g^{(0)}\): we can hence find \(dim(Ker(g^{(0)}))\) vector fields which are a base for the tangent space of \(M_{0}\). This means that we can compute the eigenvalue decomposition of \(g^{(0)}_{x}\) and consider the \(L\) linearly independent eigenvectors, namely \(\{v_{l}\}_{l=1,\ldots,L}\), associated to the null eigenvalue: these eigenvectors depend _smoothly_ on the point, a fact that is not trivial when the matrix associated to the metric depends on several parameters [15]. We can build then all the null curves by randomly selecting one eigenvector \(\tilde{v}\in\{v_{l}\}\) and then reconstruct the curve along the direction \(\tilde{v}\) from the starting point \(x\). From a practical point of view, one is led to solve the Cauchy problem, a first order differential equation, with \(\dot{\gamma}=\tilde{v}\) and initial condition \(\gamma(0)=x\).

### Input Space Exploration

This whole procedure is coded in the Singular Metric Equivalence Class (SiMEC) and the Singular Metric Exploration (SiMExp) algorithms, whose general schemes are depicted in Algorithms 1 and 2. SiMEC reconstructs the class of equivalence of the input via the exploration of the input space by randomly selecting one of the eigenvectors related to the zero eigenvalue. On the opposite, in SiMExp, in order to move from a class of equivalence to another we consider the eigenvectors relative to the nonzero eigenvalues. This requires the slight difference in lines 5 to 7 between Algorithm 1 and Algorithm 2.

```
1: Set the network \(\mathcal{N}\); choose the maximum number of iterations. Choose the input \(p_{0}\).
2:for\(k=0,1,\ldots,K-1\)do
3: Compute \(g^{0}_{\mathcal{N}(p_{k})}\)
4: Compute the pullback metric \(g^{0}_{p_{k}}\)
5: Diagonalize \(g^{0}_{p_{k}}\) and find the eigenvectors \(\{v_{l}\}_{l}\) associated to the zero eigenvalue
6: Randomly select \(\tilde{v}\in\{v_{l}\}_{l}\)
7:\(\delta=1/\sqrt{\max(\text{eigenvalues of }g^{0}_{p_{k}})}\)
8:\(p_{k+1}\gets p_{k}+\delta\tilde{v}\)
9:endfor
10: Optionally: store \(\{p_{k}\}_{k=0,\ldots,K}\) for optimizing future computations
11: Project \(p_{k}\) to the nearest feasible region ```

**Algorithm 1** The Singular Metric Equivalence Class (SiMEC) algorithm.

There are some remarks to point out. From a numerical point of view, the diagonalization of the pullback may lead to have even negative eigenvalues: hence one may use the notion of energy of a curve, related to the pseudolength. The update rule for the new point (line 8) amounts to solve the differential problem via the Euler method: for a reliable solution, we suggest to choose a small step-length \(\delta\). On the other hand, if the value of \(\delta\) is too small more iterations are needed to moveaway from the starting point sensibly. Therefore there is a trade-off between the reliability of the solution and the exploration pace. The proof of the well-posedness theorem for Cauchy problems, cf. [18, Theorem 2.1], yields some insights, suggesting to set \(\delta\) equal to the inverse of the Lipschitz constant of the map \(\mathcal{N}\) - which in practice we can estimate with the inverse of the square root of the largest eigenvalue \(\lambda_{M}\) of the pullback metric \(g^{0}_{p_{k}}\). This is our default choice for Algorithm 1. We also note that Algorithm 1 is more sensitive to the choice of the parameter \(\delta\) compared to Algorithm 2. To build points in the same equivalence class Algorithm 1 needs to follow a null curve closely with as little approximations as possible, namely with a small \(\delta\). In contrast Algorithm 2, whose goal is to change the equivalence class from one iteration to the next, does not have the same problem and larger \(\delta\) are allowed. Out default choice is therefore to set \(\delta=2\lambda_{M}^{-1/2}\) for Algorithm 2. As for the computational complexity of the two algorithms, the most demanding step is the computation of the eigenvalues and eigenvectors, which is \(O(n^{3})\), with \(n\) the dimension of the square matrix \(g^{0}_{p_{k}}\)[20]. Since all the other operations are either \(O(n)\) or \(O(n^{2})\), we conclude that the complexity of both Algorithms 1 and 2 is \(O(n^{3})\).

### Interpretability

Algorithms 1 and 2 allow for the exploration of the equivalence classes in the input space of a Transformer model. However, the points explored by these algorithms may not be directly interpretable by a human perspective. For instance, an image or a piece of text may need to be decoded to be "readable" by a human observer. Furthermore, we present an interpretation of the eigenvalues of the pullback metric which allows us to define a feature importance metric. We present two interpretability methods for Transformers based on input space exploration. Both methods are then demonstrated on a Vision Transformer (ViT) trained for digit classification [8], and two BERT models, one trained for hate speech classification and the other trained for MLM [7; 19].

```
1:Inputs:
2: Transformer model \(T\) with: Tokenizer \(t_{T}\), Embedding layer \(e_{T}\), Intermediate layers \(l_{T}\)
3:Input data \(x\)
4:Retrieve segments \(x^{t}=t_{T}(x)\)
5:Compute embeddings \(x^{e}=e_{T}(x^{t})\)
6:Compute intermediate representations \(g^{n}_{l_{T}(x^{e})}\)
7:Calculate the pullback metric \(g^{0}_{x^{e}}\)
8:Diagonalize \(g^{0}_{x^{e}}\) to extract eigenvalues
9:Identify the maximum eigenvalue for each embedding, indicating its importance
10:Output: Heatmap of embedding importance based on the eigenvalues ```

**Algorithm 4** Exploration of Embedding Space in Transformers

Feature importance.Consider a Transformer model \(T\) whose architecture includes a tokenizer \(t_{T}\) (or patcher for images) that segments the input so that each segment can be converted into a continuous representation by an embedding layer \(e_{T}\). This results in a matrix of dimensions \(n_{s}\times h\), where \(n_{s}\) represents the number of segments, and \(h\) denotes the hidden size of the model's embeddings. The eigenvalues of the pullback metric can be used to deduce the importance of each embedding and, by extension, the significance of the segments they represent, with respect to the final prediction. The process for determining the importance of textual tokens or image patches is outlined in Algorithm 3. The appearance of the resulting heatmaps varies according to the type of input used. An example of experiments with ViT on the MNIST dataset [12] is shown in Figure 1 that depicts heatmaps for two MNIST instances. Figure 2, on the left, illustrates two experiment using Algorithm 3 on both a BERT model for hate speech detection and a BERT model for MLM.

Interpretation of input space exploration.Using SiMEC and SiMExp to explore the embedding space reveals how Transformer models perceive equivalence among different data points. Specifically, these methodologies facilitate the sequential acquisition of embedding matrices \(p_{0}\dots p_{K}\) at each iteration, as detailed in Algorithms 1 and 2. Algorithm 4 implements a practical application of the SiMEC/SiMExp approach with Transformer models. A key feature of this method is its ability to selectively update specific tokens (for text inputs) or patches (for image inputs) during each iteration. This selective updating allows us to explore targeted modifications that prompt the model to either categorize different inputs as the same class or recognize them as distinct. Unlike traditional approaches where modifications are predetermined, this method lets the model itself guide us to understand which data points belong to specific equivalence classes. To interpret embeddings resulted from the exploration process, they must be mapped back into a human-understandable form, such as text or images. The interpretation of an embedding vector depends on the operations performed by the Transformer's embedding module \(e_{T}\). If \(e_{T}\) consists only of invertible operations, it is feasible to construct a layer that performs the inverse operation relative to \(e_{T}\). The output can then be visualized and directly interpreted by humans, allowing for a comparison with the original input to discern how differences in embeddings reflect differences in their representations (e.g., text, images). If the operations in \(e_{T}\) are non-invertible, a trained decoder is required to reconstruct an interpretable output from each embedding matrix \(p_{0}\ldots p_{K}\). When using a BERT model, it is feasible to utilize layers that are specialized for the masked language modeling (MLM) task to map input embeddings back to tokens. This approach is effective whether the BERT model in question is specifically designed for MLM or for sentence classification. In the case of sentence classification models, it is necessary to select a corresponding MLM BERT model that shares the same internal architecture, including the number of layers and embedding size.

Algorithm 5 depicts the process of interpreting Algorithm 4 outputs for both ViT and BERT experiments. After initializing the decoder according to the model type, the embeddings \(p_{0}\ldots p_{K}\) need to be constrained to a feasible region. This region is defined by the distribution of embeddings derived from the original input instances. Next, the embeddings are decoded, and the selected segments for exploration are extracted. These segments are then used to replace the corresponding parts of the original input instance. Figure 3 depicts an example outcome of Algorithm 5 applied on a ViT exploration experiment. Given that the interpretation process includes both a capping step and a decoding step (lines 10 and 11 of Algorithm 5), it's important to note that there isn't a direct 1:1 correspondence between each iteration's update and the interpretation outcomes. Our primary focus is on exploring the input embedding space, rather than the input image or input sentence spaces. For further investigation, we provide a detailed discussion on considering interpretation outputs as alternative prompts in Section 4.

Figure 1: Example output from Algorithm 3 applied to digit classification. These two instances are predicted as 3 (left) and 4 (right). The brightness of the color indicates the eigenvalue’s magnitude. The brighter the color, the more sensitive the patch. This indicates that changes in the values of these sensitive patches are likely to have a greater impact on the prediction probabilities. Each patch in the heatmap corresponds to a \(2\times 2\) square pixel.

Figure 2: Example outputs from Algorithm 3. The darker the color, the higher the token’s eigenvalue. Left: The sentence analysed is classified as “offensive” by the BERT for hate speech detection, with significant contributions from tokens [CLS], politicians, corrupt, and ##it (part of the word _deceeiful_). Right: Example instance processed by a BERT model for masked language modeling. [MASK] is predicted as “ham”, with the most influential tokens being pizza and cheese.

## 4 Experiments

Experiments are conducted on textual and visual data. We aim to perform a preliminary investigation of 3 features of our approach: (i) how the class probability changes on the decoded output of SiMEC/SiMExp, (ii) what is the trade-off between the quantity and the quality of the output, and (iii) how our method can be used to extract feature importance-based explanations.

In the textual case, we experiment with hate speech classification datasets: we use HateXplain2[13], which provides a ground truth for feature importance, plus a sample of 100 hate speech sentences generated by prompting ChatGPT3, which serve purposes (i) and (ii). In the visual case, we perform experiments on MNIST [12] dataset.

Footnote 2: MIT License

Footnote 3: Used prompts are included in the Supplementary Materials.

Using interpretation outputs as alternative promptsAn interesting investigation is to determine if our interpretation algorithm (Algorithm 5) can generate alternative prompts that stay in the same equivalence class as the original input data or move to a different one, based on SiMEC and SiMExp explorations. We test how the probability assigned to the original equivalence class by the Transformer model changes as the SiMEC and SiMExp algorithms explore the input embedding manifold.

For BERT experiments we generate prompts to inspect the probability distribution over the vocabulary for tokens updated by Algorithms 1 and 2. We decode the updated \(p_{0}\dots p_{K}\) using Algorithm 5, focusing on tokens updated through the iterations. For each of these decoded tokens, we extract the top-5 scores to obtain 5 alternative tokens to replace the original ones, creating 5 alternate prompts. We then extract the prediction \(i^{*}=\operatorname*{arg\,max}_{i}y_{i}\) for the original sentence, which represents the output whose equivalence class we aim to explore. Finally, we classify the new prompts, obtaining the corresponding predictions \(Y=\mathbf{y}^{(0)}\dots\mathbf{y}^{(K)}\), where each \(\mathbf{y}^{(k)}\in\mathbb{R}^{N}\), \(N\) being the number of prediction classes. We visualize the prediction trend for the \(i^{*}\)th value in every \(\mathbf{y}^{(0)}\dots\mathbf{y}^{(K)}\) categorizing the images into two subsets: those that lead to a change in prediction \(Y_{c}=\{\mathbf{y}^{(k)}\in Y\mid\operatorname*{arg\,max}_{i}y_{i}^{(k)}\neq i ^{*}\}\) and those that don't \(Y_{s}=\{\mathbf{y}_{i}\in Y\mid\operatorname*{arg\,max}_{i}y_{i}^{(k)}=i^{*}\}\).

Figure 3: Example of SiMEC and SiMExp output interpretation for ViT digit classification. Left: Original MNIST image of an “8”. Center: Interpretation of a \(p_{1000}\) from a SiMEC experiment, where \(p_{1000}\) is predicted as “8”. Right: Interpretation of a \(p_{1000}\) from a SiMExp experiment, where \(p_{1000}\) is predicted as “4”. All patches are subject to SiMEC and SiMExp updates.

Sentence classification experiments4 involved 1000 iterations from both SiMEC and SiMExp, applied to a subset of 8 sentences from the ChatGPT hate speech dataset. The plot on the left side of Figure 4 illustrates that, as the original embeddings are increasingly modified, SiMExp tends to produce alternatives with lower prediction values for \(i^{*}\) compared to SiMEC. Thus, even if predictions change in SiMEC experiments, the equivalence class prediction value remains approximately constant and higher than in SiMExp. Considering the plot on the right side of Figure 4, SiMExp identifies prompts that lower the prediction value for \(i^{*}\). ViT and MLM experiments are detailed in the Supplementary Materials.

Footnote 4: Model used: huggingface.co/ctoraman/hate-speech-bert

Input space explorationWe measure the time required to explore the input space of a ViT with the SiMEC algorithm and compare it with a perturbation-based method. The perturbation-based method mimics a trial-and-error approach as it takes an input image and, at each iteration, perturbs it by a semi-random vector \(\mathbf{v}_{t+1}=a_{t}\mathbf{v}_{t}+\eta\epsilon\), where \(a_{t}=1\) if \(y_{t}=y_{t-1}\), \(a_{t}=-1\) otherwise, \(\epsilon\) is an orthogonal random vector from a standard normal distribution and \(\eta\) is the step length. With the perturbation, we obtain a new image, then check whether the model yields the same label for the new image. The perturbation vector is re-initialized at random from a normal distribution \(20\%\) of the times to allow for exploration. We construct this method to have a direct comparison with ours in the absence of a consolidated literature about the task.

We train a ViT model having 4 layers and 4 heads per layer on the MNIST dataset5. The SiMEC algorithm is run for 1000 iterations, so that it can generate 1000 examples starting from a single image. In a sample of 100 images, the average time is approximately 339 seconds.6 In the same time, the perturbation-based algorithm can produce up to 36000 images. However, we notice that the perturbation-based algorithm ends up producing monochrome (pixel color has zero variance) or totally noisy images, which provide little information about the behavior of the model. Excluding only the images with low color variance (\(<0.01\)), we are left, on average, with \(19\) images (standard deviation \(13.9\)). SiMEC, in contrast, doesn't present this behavior, as all 1000 images have high enough intensity variance and are thus useful for explainability purposes.

Footnote 5: Using Adam optimizer, the model achieved the highest validation accuracy (\(96.25\%\)) in 20 epochs.

Footnote 6: All experiments are based on the current PyTorch implementation of the algorithms and run on a Ubuntu 20.04 machine endowed with one NVIDIA A100 GPU and CUDA 12.4.

As BERT has many more parameters with respect to our ViT model, processing textual data takes longer. Specifically, in a sample of 16 sentences, the average time needed to run 1000 iterations on a sentence is 7089 seconds, taking into account both MLM and classification experiments.

Feature importance-based explanationsWe compare our method against Attention Rollout (AR) [1] and the Relevancy method proposed by Chefer et al. [6]. In the textual case, we provide a quantitative evaluation using the HateXplain dataset, which contains 20147 sentences (of which 1924 in the test set) annotated with _normal_, _offensive_ and _hate speech_ labels as well as the positions of words that support the label decision. We then measure the cosine similarity between the importance assigned by each method to each word in a sentence and the ground truth. Notice that, since the

Figure 4: Analysis involving results SiMEC and SiMExp applied to BERT for hate speech detection. Left: Prediction values for \(i^{*}\) for each \(\mathbf{y}\in Y_{c}\). Right: Prediction values for \(\mathbf{y}\in Y_{s}\).

dataset contains multiple annotations, the ground truth \(y\) for each word \(w\) is obtained as the average of the binary labels assigned by each annotator, and therefore \(y(w)\in[0;1]\). We also normalize all scores in \([0;1]\) so to have them on the same scale. The average similarity achieved by our method is **0.707** (standard deviation \(\sigma\) = 0.302), against **0.7** (\(\sigma\) = 0.315) for Relevancy and **0.583** (\(\sigma\) = 0.318) for AR. This proves our method to be more effective in finding the most sensitive tokens for classification. We provide an example on image classification in the Supplementary Materials.

## 5 Related work

Our work relates to embedding space exploration literature, and has at least one collateral applications in the XAI domain, namely producing feature importance-based explanations.

Embedding space exploration.Works dealing with embedding space exploration mostly focus on the study of specific properties of the embedding space of Transformers, especially in NLP. For instance, Cai et al. [5] challenge the idea that the embedding space is inherently anisotropic [10] discovering local isotropy, and find low-dimensional manifold structures in the embedding space of GPT and BERT. Bis et al. [3] argue that the anisotropy of the embedding space derives from embeddings shifting in common directions during training. In the field of CV, Vilas et al. [21] map internal representations of a ViT onto the output class manifold, enabling the early identification of class-related patches and the computation of saliency maps on the input image for each layer and head. Applying Singular Value Decomposition to the Jacobian matrix of a ViT, Salman et al. [17] treat the input space as the union of two subspaces: one in which image embedding doesn't change, and another one for which it changes. Except for the last one, all the aforementioned approaches rely on data samples. By studying the inverse image of the model, instead, we can do away with data samples.

Feature importance-based explanations.Feature importance is a measure of the contribution of each data feature to a model prediction. In the context of Computer Vision and Natural Language Processing, it amounts to giving a weight to pixels (or patches of pixels) in an image and tokens in a piece of text, respectively. In recent years, much research has focused on Transformers in both CV and NLP. Most approaches are based on the attention mechanism of the Transformer architecture. Abnar and Zuidema [1] quantify the overall attention of the output on the input by computing a linear combination of layer attentions (Attention Rollout) or applying a maximum flow algorithm (Attention Flow). To overcome the limitations [4] of attention-based methods, Hao et al. [11] use the concept of _attribution_, which is obtained by multiplying attention matrices by the integrated gradient of the model with respect to them. Chefer et al. [6] propose the Relevancy metric to generalize attribution to bi-modal and encoder-decoder architectures. Other methods are perturbation-based, where perturbations of input data are used to record any change in the output and draw a saliency map on the input. In order to overcome the main issue with such methods, i.e. the generation of outlier inputs, Englebert et al. [9] apply perturbations after the position encoding of the patches. In contrast with these methods, ours does not need arbitrary perturbations of inputs, and considers all parameters of the model, not only the attention query and key matrices.

## 6 Conclusions

Our exploration of the Transformer architecture through a theoretical framework grounded in Riemannian Geometry led to the application of our two algorithms, SiMEC and SiMExp, for examining equivalence classes in the Transformers' input space. We demonstrated how the results of these exploration methods can be interpreted in a human-readable form and conducted preliminary investigations into their potential applications. Notably, our methods show promise for ranking feature importance and generating alternative prompts within the same or different equivalence classes.

Future research directions include expanding our experimental results and delving deeper into the potential of our framework for controlled input generation within an equivalence class. This application holds significant promise for enhancing the explainability of Transformer models' decisions and for addressing issues related to bias and hallucinations.

## References

* Abnar and Zuidema [2020] S. Abnar and W. Zuidema. Quantifying attention flow in transformers. 2020. arXiv:2005.00928.
* Benfenati and Marta [2023] A. Benfenati and A. Marta. A singular Riemannian geometry approach to Deep Neural Networks I. Theoretical foundations. _Neural Networks_, 158:331-343, 2023.
* Bis et al. [2021] D. Bis, M. Podkorytov, and X. Liu. Too much in common: Shifting of embeddings in transformer language models and its implications. In _Proceedings of the 2021 conference of the North American chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5117-5130, 2021.
* Brunner et al. [2019] G. Brunner, Y. Liu, D. Pascual, O. Richter, M. Ciaramita, and R. Wattenhofer. On identifiability in transformers. _International Conference on Learning Representations_, 2019.
* Cai et al. [2020] X. Cai, J. Huang, Y. Bian, and K. Church. Isotropy in the contextual embedding space: Clusters and manifolds. In _International conference on learning representations_, 2020.
* Chefer et al. [2021] H. Chefer, S. Gur, and L. Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 397-406, 2021.
* Devlin et al. [2019] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In J. Burstein, C. Doran, and T. Solorio, editors, _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423.
* Dosovitskiy et al. [2021] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. arXiv:2010.11929.
* Englebert et al. [2023] A. Englebert, S. Stassin, G. Nanfack, S. A. Mahmoudi, X. Siebert, O. Cornu, and C. De Vleeschouwer. Explaining through transformer input sampling. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 806-815, 2023.
* Gao et al. [2019] J. Gao, D. He, X. Tan, T. Qin, L. Wang, and T.-Y. Liu. Representation degeneration problem in training natural language generation models. In _International conference on learning representations_, volume abs/1907.12009, 2019. URL https://api.semanticscholar.org/CorpusID:59317065.
* Hao et al. [2021] Y. Hao, L. Dong, F. Wei, and K. Xu. Self-attention attribution: Interpreting information interactions inside transformer. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 12963-12971, 2021.
* Lecun et al. [1998] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
* Mathew et al. [2021] B. Mathew, P. Saha, S. M. Yimam, C. Biemann, P. Goyal, and A. Mukherjee. Hatexplain: A benchmark dataset for explainable hate speech detection. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 14867-14875, 2021.
* Papernot et al. [2016] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami. The Limitations of Deep Learning in Adversarial Settings. In _2016 IEEE European Symposium on Security and Privacy (EuroS&P)_, pages 372-387, Mar. 2016. doi: 10.1109/EuroSP.2016.36.
* Rellich and Berkowitz [1969] F. Rellich and J. Berkowitz. _Perturbation Theory of Eigenvalue Problems_. New York University. Institute of Mathematical Sciences. Gordon and Breach, 1969. ISBN 9780677006802.
* Ribeiro et al. [2016] M. T. Ribeiro, S. Singh, and C. Guestrin. " why should i trust you?" explaining the predictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1135-1144, 2016.

* Salman et al. [2024] S. Salman, M. M. B. Shams, and X. Liu. Intriguing equivalence structures of the embedding space of vision transformers, 2024. arXiv:2401.15568.
* Taylor [2023] M. E. Taylor. _Partial differential equations. I: Basic theory_, volume 115 of _Appl. Math. Sci._ Cham: Springer, 3rd corrected and expanded edition edition, 2023. ISBN 978-3-031-33858-8; 978-3-031-33861-8; 978-3-031-33859-5. doi: 10.1007/978-3-031-33859-5.
* Toraman et al. [2022] C. Toraman, F. Sahinuc, and E. H. Yilmaz. Large-scale hate speech detection with cross-domain transfer. In _Proceedings of the Language Resources and Evaluation Conference_, pages 2215-2225, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1.238.
* Trefethen and Bau [2022] L. N. Trefethen and D. I. Bau. _Numerical linear algebra. Twenty-fifth anniversary edition_, volume 181 of _Other Titles Appl. Math._ Philadelphia, PA: Society for Industrial and Applied Mathematics (SIAM), 2022. ISBN 978-1-61197-715-8.
* Vilas et al. [2024] M. G. Vilas, T. Schaumloffel, and G. Roig. Analyzing vision transformers for image classification in class embedding space. _Advances in Neural Information Processing Systems_, 36, 2024.
* Wu et al. [2024] M. Wu, H. Wu, and C. Barrett. Verix: Towards verified explainability of deep neural networks. _Advances in neural information processing systems_, 36, 2024.

NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and introduction we claim that we present a method for the exploration of equivalence classes in the input space of Transformer models, which is analyzed in depth in Section 3. The mathematical theory we refer to is deepened in Section 2. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations and tradeoff given by numeric integration in Subsection 3.1 and theoretical assumptions are enumerated in Section 2. Computational efficiency of our algorithms is discussed in Subsection 3.1. We conducted experiments on 3 datasets only, one of which of small dimensions since our main focus is on the mathematical theory grounding the application of the method to Transformers, as stated at the beginning of Section 4. Other limitations are mentioned throughout Section 4, including the fact that our investigations in the human-readable scenario are at a preliminary stage. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best 

[MISSING_PAGE_FAIL:13]

* If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).
* We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
* **Open access to data and code*
* Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: All experiments are made reproducible through scripts provided as supplementary material. Guidelines:
* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
* **Experimental Setting/Details*
* Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: All details are provided in Section 4: details of the analyzed architectures, number of iterations of the SiMEC/SiMExp algorithms, technical infrastructure on which the experiments were performed, amount of data the experiments were performed on. Guidelines:
* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
* **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments?Answer: [Yes] Justification: The standard deviation is reported for the experiment that supports the claims of the paper, i.e. the one on feature importance-based explanations, in Section 4. Standard deviation is also reported for the number of uninformative images produced by the perturbation-based baseline method in the Input space exploration experiment.

Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All the experiments were performed on the same infrastructure, which is reported in a footnote in Section 4. Time of execution is one of the key indicators reported for the Input space exploration experiments. More computing power would be required for experiments on bigger Transformer models. Guidelines:

* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).

9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We comply with the terms of use of the datasets employed in the experiments, and we deem our work has no potentially harmful effect on people safety, security, discrimination, surveillance, harassment, nor on human rights. Our proposal does not contribute to spread bias and unfairness towards certain groups of people nor to harm the environment.

Guidelines:

* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: Although the impacts of XAI on society is broad and deep, in this paper we focus only on the technical problem of exploring the equivalence classes in the input space of Transformers, which doesn't add any specific impact to the discussion about XAI in general. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The datasets used in the paper are explicitely mentioned in the references, as required by the terms of use. Where applicable, the license is also reported. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work doesn't include crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects**

Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?

Answer: [NA]

Justification: Our work does not involve crowdsourcing nor research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.