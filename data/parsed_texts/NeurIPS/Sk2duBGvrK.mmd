# Understanding Generalizability of Diffusion Models

Requires Rethinking the Hidden Gaussian Structure

 Xiang Li1, Yixiang Dai1, Qing Qu1

1Department of EECS, University of Michigan,

forkobe@umich.edu, yixiang@umich.edu, qingqu@umich.edu

###### Abstract

In this work, we study the generalizability of diffusion models by looking into the hidden properties of the learned score functions, which are essentially a series of deep denoisers trained on various noise levels. We observe that as diffusion models transition from memorization to generalization, their corresponding nonlinear diffusion denoisers exhibit increasing linearity. This discovery leads us to investigate the linear counterparts of the nonlinear diffusion models, which are a series of linear models trained to match the function mappings of the nonlinear diffusion denoisers. Interestingly, these linear denoisers are approximately the optimal denoisers for a multivariate Gaussian distribution characterized by the empirical mean and covariance of the training dataset. This finding implies that diffusion models have the inductive bias towards capturing and utilizing the Gaussian structure (covariance information) of the training dataset for data generation. We empirically demonstrate that this inductive bias is a unique property of diffusion models in the generalization regime, which becomes increasingly evident when the model's capacity is relatively small compared to the training dataset size. In the case where the model is highly overparameterized, this inductive bias emerges during the initial training phases before the model fully memorizes its training data. Our study provides crucial insights into understanding the notable strong generalization phenomenon recently observed in real-world diffusion models.

## 1 Introduction

In recent years, diffusion models [1; 2; 3; 4] have become one of the leading generative models, powering the state-of-the-art image generation systems such as Stable Diffusion [5]. To understand the empirical success of diffusion models, several works [6; 7; 8; 9; 10; 11; 12] have focused on their sampling behavior, showing that the data distribution can be effectively estimated in the reverse sampling process, assuming that the score function is learned accurately. Meanwhile, other works [13; 14; 15; 16; 17; 18] investigate the learning of score functions, showing that effective approximation can be achieved with score matching loss under certain assumptions. However, these theoretical insights, grounded in simplified assumptions about data distribution and neural network architectures, do not fully capture the complex dynamics of diffusion models in practical scenarios. One significant discrepancy between theory and practice is that real-world diffusion models are trained only on a finite number of data points. As argued in [19], theoretically a perfectly learned score function over the empirical data distribution can only replicate the training data. In contrast, diffusion models trained on finite samples exhibit remarkable generalizability, producing high-quality images that significantly differ from the training examples. Therefore, a good understanding of the remarkable generative power of diffusion models is still lacking.

In this work, we aim to deepen the understanding of generalizability in diffusion models by analyzing the inherent properties of the learned score functions. Essentially, the score functions can be interpreted as a series of deep denoisers trained on various noise levels. These denoisers are then chained together to progressively denoise a randomly sampled Gaussian noise into its correspondingclean image, thus, understanding the function mappings of these diffusion denoisers is critical to demystify the working mechanism of diffusion models. Motivated by the linearity observed in the diffusion denoisers of effectively generalized diffusion models, we propose to elucidate their function mappings with a linear distillation approach, where the resulting linear models serve as the linear approximations of their nonlinear counterparts.

Contributions of this work:Our key findings can be highlighted as follows:

* **Inductive bias towards Gaussian structures (Section 3).** Diffusion models in the _generalization regime_ exhibit an inductive bias towards learning diffusion denoisers that are close (but not equal) to the optimal denoisers for a multivariate Gaussian distribution, defined by the empirical mean and covariance of the training data. This implies the diffusion models have the inductive bias towards capturing the Gaussian structure (covariance information) of the training data for image generation.
* **Model Capacity and Training Duration (Section 4)** We show that this inductive bias is most pronounced when the model capacity is relatively small compared to the size of the training data. However, even if the model is highly overparameterized, such inductive bias still emerges during early training phases, before the model memorizes its training data. This implies that early stopping can prompt generalization in overparameterized diffusion models.
* **Connection between Strong Generalization and Gaussian Structure (Section 5).** Lastly, we argue that the recently observed strong generalization [20] results from diffusion models learning certain common low-dimensional structural features shared across non-overlapping datasets. We show that such low-dimensional features can be partially explained through the Gaussian structure.

Relationship with Prior Arts.Recent research [20; 21; 22; 23; 24] demonstrates that diffusion models operate in two distinct regimes: (_i_) a memorization regime, where models primarily reproduce training samples and (_ii_) a generalization regime, where models generate high-quality, novel images that extend beyond the training data. In the generalization regime, a particularly intriguing phenomenon is that diffusion models trained on non-overlapping datasets can generate nearly identical samples [20]. While prior work [20] attributes this "_strong generalization_" effect to the structural inductive bias inherent in diffusion models leading to the optimal denoising basis (geometry-adaptive harmonic basis), our research advances this understanding by demonstrating diffusion models' inductive bias towards capturing the Gaussian structure of the training data. Our findings also corroborate with observations of earlier study [25] that the learned score functions of well-trained diffusion models closely align with the optimal score functions of a multivariate Gaussian approximation of the training data.

## 2 Preliminary

Basics of Diffusion Models.Given a data distribution \(p_{\text{data}}(\bm{x})\), where \(\bm{x}\in\mathbb{R}^{d}\), diffusion models [1; 2; 3; 4] define a series of intermediate states \(p(\bm{x};\sigma(t))\) by adding Gaussian noise sampled from \(\mathcal{N}(\bm{0},\sigma(t)^{2}\bm{I})\) to the data, where \(\sigma(t)\) is a predefined schedule that specifies the noise level at time \(t\in[0,T]\), such that at the end stage the noise mollified distribution \(p(\bm{x};\sigma(T))\) is indistinguishable from the pure Gaussian distribution. Subsequently, a new sample is generated by progressively denoising a random noise \(\bm{x}_{T}\sim\mathcal{N}(\bm{0},\sigma(T)^{2}\bm{I})\) to its corresponding clean image \(\bm{x}_{0}\). Following [4], this forward and backward diffusion process can be expressed with a probabilistic ODE:

\[d\bm{x}=-\dot{\sigma}(t)\sigma(t)\nabla_{\bm{x}}\log p(\bm{x};\sigma(t))dt.\] (1)

In practice the score function \(\nabla_{\bm{x}}\log p(\bm{x};\sigma(t))\) can be approximated by

\[\nabla_{\bm{x}}\log p(\bm{x};\sigma(t))=(\mathcal{D}_{\bm{\theta}}(\bm{x}; \sigma(t))-\bm{x})/\sigma(t)^{2},\] (2)

where \(\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t))\) is parameterized by a deep network with parameters \(\bm{\theta}\) trained with the denoising score matching objective:

\[\min_{\bm{\theta}}\mathbb{E}_{\bm{x}\sim p_{\text{data}}}\mathbb{E}_{\bm{ \epsilon}\sim\mathcal{N}(\bm{0},\sigma(t)^{2}\bm{I})}\left[\|\mathcal{D}_{\bm {\theta}}(\bm{x}+\bm{\epsilon};\sigma(t))-\bm{x}\|_{2}^{2}\right].\] (3)

In the discrete setting, the reverse ODE in (1) takes the following form:

\[\bm{x}_{i+1}\leftarrow(1-(t_{i}-t_{i+1})\frac{\dot{\sigma}(t_{i})}{\sigma(t_{ i})})\bm{x}_{i}+(t_{i}-t_{i+1})\frac{\dot{\sigma}(t_{i})}{\sigma(t_{i})} \mathcal{D}_{\bm{\theta}}(\bm{x}_{i};\sigma(t_{i})),\] (4)where \(\bm{x}_{0}\sim\mathcal{N}(\bm{0},\sigma^{2}(t_{0})\bm{I})\). Notice that at each iteration \(i\), the intermediate sample \(\bm{x}_{i+1}\) is the sum of the scaled \(\bm{x}_{i}\) and the denoising output \(\mathcal{D}_{\bm{\theta}}(\bm{x}_{i};\sigma(t_{i}))\). Obviously, the final sampled image is largely determined by the denoiser \(\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t))\). If we can understand the function mapping of these diffusion denoisers, we can demystify the working mechanism of diffusion models.

Optimal Diffusion Denoisers under Simplified Data Assumptions.Under certain assumptions on the data distribution \(p_{\text{data}}(\bm{x})\), the optimal diffusion denoisers \(\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t))\) that minimize the score matching objective (3) can be derived analytically in closed-forms as we discuss below.

* _Multi-delta distribution of the training data._ Suppose the training dataset contains a finite number of data points \(\{\bm{y}_{1},\bm{y}_{2},...,\bm{y}_{N}\}\), a natural way to model the data distribution is to represent it as a multi-delta distribution: \(p(\bm{x})=\frac{1}{N}\sum_{i=1}^{N}\delta(\bm{x}-\bm{y}_{i})\). In this case, the optimal denoiser is \[\mathcal{D}_{\mathrm{M}}(\bm{x};\sigma(t))=\frac{\sum_{i=1}^{N}\mathcal{N}( \bm{x};\bm{y}_{i},\sigma(t)^{2}\bm{I})\bm{y}_{i}}{\sum_{i=1}^{N}\mathcal{N}( \bm{x};\bm{y}_{i},\sigma(t)^{2}\bm{I})},\] (5) which is essentially a softmax-weighted combination of the finite data points. As proved in [24], such diffusion denoisers \(\mathcal{D}_{\mathrm{M}}(\bm{x};\sigma(t))\) can only generate exact replicas of the training samples, therefore they have no generalizability.
* _Multivariate Gaussian distribution._ Recent work [25] suggests modeling the data distribution \(p_{\text{data}}(\bm{x})\) as a multivariate Gaussian distribution \(p(\bm{x})=\mathcal{N}(\bm{\mu},\bm{\Sigma})\), where the mean \(\bm{\mu}\) and the covariance \(\bm{\Sigma}\) are approximated by the empirical mean \(\bm{\mu}=\frac{1}{N}\sum_{i=1}^{N}\bm{y}_{i}\) and the empirical covariance \(\bm{\Sigma}=\frac{1}{N}\sum_{i=1}^{N}(\bm{y}_{i}-\bm{\mu})(\bm{y}_{i}-\bm{\mu}) ^{T}\) of the training dataset. In this case, the optimal denoiser is: \[\mathcal{D}_{\mathrm{G}}(\bm{x};\sigma(t))=\bm{\mu}+\bm{U}\tilde{\bm{\Lambda} }_{\sigma(t)}\bm{U}^{T}(\bm{x}-\bm{\mu}),\] (6) where \(\bm{\Sigma}=\bm{U}\bm{\Lambda}\bm{U}^{T}\) is the SVD of the empirical covariance matrix, with singular values \(\bm{\Lambda}=\mathrm{diag}\left(\lambda_{1},\cdots,\lambda_{d}\right)\) and \(\tilde{\bm{\Lambda}}_{\sigma(t)}=\mathrm{diag}\left(\frac{\lambda_{1}}{\lambda _{1}+\sigma(t)^{2}}\cdots\frac{\lambda_{d}}{\lambda_{d}+\sigma(t)^{2}}\right)\). With this linear Gaussian denoiser, as proved in [25], the sampling trajectory of the probabilistic ODE (1) has close form: \[\bm{x}_{t}=\bm{\mu}+\sum_{i=1}^{d}\sqrt{\frac{\sigma(t)^{2}+\lambda_{i}}{\sigma (T)^{2}+\lambda_{i}}}\bm{u}_{i}^{T}(\bm{x}_{T}-\bm{\mu})\bm{u}_{i},\] (7) where \(\bm{u}_{i}\) is the \(i^{\text{th}}\) singular vector of the empirical covariance matrix. While [25] demonstrate that the Gaussian scores approximate learned scores at high noise variances, we show that they are nearly the best linear approximations of learned scores across a much wider range of noise variances.

Generalization vs. Memorization of Diffusion Models.As the training dataset size increases, diffusion models transition from the memorization regime--where they can only replicate its training images--to the generalization regime, where the they produce high-quality, novel images [17]. While memorization can be interpreted as an overfitting of diffusion models to the training samples, the mechanisms underlying the generalization regime remain less well understood. This study aims to explore and elucidate the inductive bias that enables effective generalization in diffusion models.

## 3 Hidden Linear and Gaussian Structures in Diffusion Models

In this section, we study the intrinsic structures of the learned score functions of diffusion models in the generalization regime. Through various experiments and theoretical investigation, we show that

_Diffusion models in the generalization regime have inductive bias towards learning the Gaussian structures of the dataset._

Based on the _linearity_ observed in diffusion denoisers trained in the generalization regime, we propose to investigate their intrinsic properties through a _linear distillation_ technique, with which we train a series of linear models to approximate the nonlinear diffusion denoisers (Section 3.1). Interestingly, these linear models closely resemble the optimal denoisers for a multivariate Gaussian distribution characterized by the empirical mean and covariance of the training dataset (Section 3.2). This implies diffusion models have the inductive bias towards learning the Gaussian structure of the training dataset. We theoretically show that the observed Gaussian structure is the optimal solution to the denoising score matching objective under the constraint that the model is linear (Section 3.3). In the subsequent sections, although we mainly demonstrate our results using the FFHQ datasets, our findings are robust and extend to various architectures and datasets, as detailed in Appendix G.

### Diffusion Models Exhibit Linearity in the Generalization Regime

Our study is motivated by the emerging linearity observed in diffusion models in the generalization regime. Specifically, we quantify the linearity of diffusion denoisers at various noise level \(\sigma(t)\) by jointly assessing their "Additivity" and "Homogeneity" with a linearity score (LS) defined by the cosine similarity between \(\mathcal{D}_{\bm{\theta}}(\alpha\bm{x}_{1}+\beta\bm{x}_{2};\sigma(t))\) and \(\alpha\mathcal{D}_{\bm{\theta}}(\bm{x}_{1};\sigma(t))+\beta\mathcal{D}_{\bm{ \theta}}(\bm{x}_{2};\sigma(t))\):

\[\mathsf{LS}(t)\ =\ \mathbb{E}_{\bm{x}_{1},\bm{x}_{2}\sim p(\bm{x};\sigma(t))} \left[\left|\left\langle\frac{\mathcal{D}_{\bm{\theta}}(\alpha\bm{x}_{1}+ \beta\bm{x}_{2};\sigma(t))}{\left\|\mathcal{D}_{\bm{\theta}}(\alpha\bm{x}_{1}+ \beta\bm{x}_{2};\sigma(t))\right\|_{2}},\frac{\alpha\mathcal{D}_{\bm{\theta}}( \bm{x}_{1};\sigma(t))+\beta\mathcal{D}_{\bm{\theta}}(\bm{x}_{2};\sigma(t))}{ \left\|\mathcal{D}_{\bm{\theta}}(\alpha\bm{x}_{1};\sigma(t))+\beta\mathcal{D }_{\bm{\theta}}(\bm{x}_{2};\sigma(t))\right\|_{2}}\right\rangle\right|\right],\]

where \(\bm{x}_{1},\bm{x}_{2}\sim p(\bm{x};\sigma(t))\), and \(\alpha\in\mathbb{R}\) and \(\beta\in\mathbb{R}\) are scalars. In practice, the expectation is approximated with its empirical mean over 100 samples. A more detailed discussion on this choice of measuring linearity is deferred to Appendix A.

Following the EDM training configuration [4], we set the noise levels \(\sigma(t)\) within the continuous range [0.002,80]. As shown in Figure 1, as diffusion models transition from the memorization regime to the generalization regime (increasing the training dataset size), the corresponding diffusion denoisers \(\mathcal{D}_{\bm{\theta}}\) exhibit increasing linearity. This phenomenon persists across diverse datasets1 as well as various training configurations2; see Appendix B for more details. This emerging linearity motivates us to ask the following questions:

Footnote 1: For example, FFHQ [26], CIFAR-10 [27], AFHQ [28] and LSUN-Churches [29].

Footnote 2: For example, EDM-VE, EDM-VP and EDM-ADM.

* _To what extent can a diffusion model be approximated by a linear model?_
* _If diffusion models can be approximated linearly, what are the underlying characteristics of this linear approximation?_

Investigating the Linear Structures via Linear Distillation.To address these questions, we investigate the hidden linear structure of diffusion denoisers through _linear distillation_. Specifically, for a given diffusion denoiser \(\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t))\) at noise level \(\sigma(t)\), we approximate it with a linear function (with a bias term) such that:

\[\mathcal{D}_{\mathrm{L}}(\bm{x};\sigma(t)):=\bm{W}_{\sigma(t)}\bm{x}+\bm{b}_{ \sigma(t)}\ \approx\ \mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t)),\ \forall\bm{x}\sim p(\bm{x};\sigma(t)),\] (8)

where the weight \(\bm{W}_{\sigma(t)}\in\mathbb{R}^{d\times d}\) and bias \(\bm{b}_{\sigma(t)}\in\mathbb{R}^{d}\) are learned by solving the following optimization problem with gradient descent:3

Footnote 3: For the following, the input is the vectorized version of the noisy image and the expectation is approximated using finite samples of input-output pairs \((\bm{x}_{i}+\bm{\epsilon}_{i},\mathcal{D}_{\bm{\theta}}(\bm{x}_{i}+\bm{ \epsilon},\sigma(t)))\) with \(i=1,...,N\) (see distillation details in Appendix C).

\[\min_{\bm{W}_{\sigma(t)},\bm{b}_{\sigma(t)}}\mathbb{E}_{\bm{x}\sim p_{\bm{ \theta}}(\bm{x})}\mathbb{E}_{\bm{\epsilon}\sim\mathcal{N}(\bm{0},\sigma(t)^{2 }\bm{I})}||\bm{W}_{\sigma(t)}(\bm{x}+\bm{\epsilon})+\bm{b}_{\sigma(t)}- \mathcal{D}_{\bm{\theta}}(\bm{x}+\bm{\epsilon};\sigma(t))||_{2}^{2}.\] (9)

If these linear models effectively approximate the nonlinear diffusion denoisers, analyzing their weights can elucidate the generation mechanism.

While diffusion models are trained on continuous noise variance levels within [0.002,80], we examine the 10 discrete sampling steps specified by the EDM schedule [4]: [80.0, 42.415, 21.108, 9.723, 4.06, 1.501, 0.469, 0.116, 0.020, 0.002]. These steps are considered sufficient for studying the diffusion mappings for two reasons: _(i)_ images generated using these 10 steps closely match those generated with more steps, and _(ii)_ recent research [30] demonstrates that the diffusion denoisers trained on similar noise variances exhibit analogous function mappings, implying that denoiser behavior at discrete variances represents their behavior at nearby variances.

Figure 1: **Linearity scores of diffusion denoisers.** Solid and dashed lines depict the linearity scores across noise variances for models in the generalization and memorization regimes, respectively, where \(\alpha=\beta=1/\sqrt{2}\).

After obtaining the linear models \(\mathcal{D}_{\mathrm{L}}\), we evaluate their differences with the actual nonlinear denoisers \(\mathcal{D}_{\bm{\theta}}\) with the score field approximation error, calculated using the expectation over the root mean square error (RMSE):

\[\text{Score-Difference}(t):=\mathbb{E}_{\bm{x}\sim p_{\bm{\theta}}(\bm{x}),\bm{ \epsilon}\sim\mathcal{N}(\bm{0};\sigma(t)^{2}\bm{I})}\underbrace{\sqrt{\frac{ \|\mathcal{D}_{\mathrm{L}}(\bm{x}+\bm{\epsilon};\sigma(t))-\mathcal{D}_{\bm{ \theta}}(\bm{x}+\bm{\epsilon};\sigma(t))\|_{2}^{2}}{d}}}_{\text{RMSE of a pair of randomly sampled $\bm{x}$ and $\bm{\epsilon}$}},\] (10)

where \(d\) represents the data dimension and the expectation is approximated with its empirical mean. While we present RMSE-based results in the main text, our findings remain consistent across alternative metrics, including NMSE, as detailed in Appendix G.

We perform linear distillation on well trained diffusion models operating in the generalization regime. For comprehensive analysis, we also compute the score approximation error between \(\mathcal{D}_{\bm{\theta}}\) and: _(i)_ the optimal denoisers for the multi-delta distribution \(\mathcal{D}_{\mathrm{M}}\) defined as (5), and _(ii)_ the optimal denoisers for the multivariate Gaussian distribution \(\mathcal{D}_{\mathrm{G}}\) defined as (6). As shown in Figure 2, our analysis reveals three distinct regimes:

* _High-noise regime [20,80]_. In this regime, only coarse image structures are generated (Figure 2(right)). Quantitatively, as shown in Figure 2(left), the distilled linear model \(\mathcal{D}_{\mathrm{L}}\) closely approximates its nonlinear counterpart \(\mathcal{D}_{\bm{\theta}}\) with RMSE below 0.05. Both Gaussian score \(\mathcal{D}_{\mathrm{G}}\) and multi-delta score \(\mathcal{D}_{\mathrm{M}}\) also achieve comparable approximation accuracy.
* _Low-noise regime [0.002,0.1]_. In this regime, only subtle, imperceptible details are added to the generated images. Here, both \(\mathcal{D}_{\mathrm{L}}\) and \(\mathcal{D}_{\mathrm{G}}\) effectively approximate \(\mathcal{D}_{\bm{\theta}}\) with RMSE below 0.05.
* _Intermediate-noise regime [0.1,20]_. This crucial regime, where realistic image content is primarily generated, exhibits significant nonlinearity. While \(\mathcal{D}_{\mathrm{M}}\) exhibits high approximation error due to rapid convergence to training samples--a memorization effect theoretically proved in [24], both \(\mathcal{D}_{\mathrm{L}}\) and \(\mathcal{D}_{\mathrm{G}}\) maintain relatively lower approximation errors.

Qualitatively, as shown in Figure 2(right), despite the relatively high score approximation error in the intermediate noise regime, the images generated with \(\mathcal{D}_{\mathrm{L}}\) closely resemble those generated with \(\mathcal{D}_{\bm{\theta}}\) in terms of the overall image structure and certain amount of fine details. This implies _(i)_ the underlying linear structure within the nonlinear diffusion models plays a pivotal role in their generalization capabilities and _(ii)_ such linear structure is effectively captured by our distilled linear models. In the next section, we will explore this linear structure by examining the linear models \(\mathcal{D}_{\mathrm{L}}\).

### Inductive Bias towards Learning the Gaussian Structures

Notably, the Gaussian denoisers \(\mathcal{D}_{\mathrm{G}}\) exhibit behavior strikingly similar to the linear denoisers \(\mathcal{D}_{\mathrm{L}}\). As illustrated in Figure 2(left), they achieve nearly identical score approximation errors, particularly in the critical intermediate variance region. Furthermore, their sampling trajectories are remarkably similar (Figure 2(right)), producing nearly identical generated images that closely match those from the actual diffusion denoisers (Figure 3). These observations suggest that \(\mathcal{D}_{\mathrm{L}}\) and \(\mathcal{D}_{\mathrm{G}}\) share

Figure 2: **Score field approximation error and sampling Trajectory. The left and right figures demonstrate the score field approximation error and the sampling trajectories \(\mathcal{D}(\bm{x}_{t};\sigma(t)\) of actual diffusion model (EDM), Multi-Delta model, linear model and Gaussian model respectively. Notice that the curve corresponding to the Gaussian model almost overlaps with that of the linear model, suggesting they share similar funciton mappings.**

similar function mappings across various noise levels, leading us to hypothesize that the intrinsic linear structure underlying diffusion models corresponds to the Gaussian structure of the training data--specifically, its empirical mean and covariance. We validate this hypothesis by empirically showing that \(\mathcal{D}_{\mathrm{L}}\) is close to \(\mathcal{D}_{\mathrm{G}}\) through the following three complementary experiments:

* _Similarity in weight matrices._ As illustrated in Figure 4(left), \(\bm{W}_{\sigma(t)}\) progressively converge towards \(\bm{U}\tilde{\Lambda}_{\sigma(t)}\bm{U}^{T}\) throughout the linear distillation process, achieving small normalized MSE (less than 0.2) for most of the noise levels. The less satisfactory convergence behavior at \(\sigma(t)=80.0\) is due to inadequate training of the diffusion models at this particular noise level, which is minimally sampled during the training of actual diffusion models (see Appendix G.2 for more details).
* _Similarity in Force functions._ Furthermore, Figure 2(left, gray line) demonstrates that \(\mathcal{D}_{\mathrm{L}}\) and \(\mathcal{D}_{\mathrm{G}}\) maintain small score differences (RMSE less than 0.05) across all noise levels, indicating that these denoisers exhibit similar function mappings throughout the diffusion process.
* _Similarity in principal components._ As shown in Figure 4(right), for a wide noise range (\(\sigma(t)\in[0.116,80.0]\)), the leading singular vectors of the linear weights \(\bm{W}_{\sigma(t)}\) (denoted \(\bm{U}_{Linear}\)) align well with \(\bm{U}\), the singular vectors of the Gaussian weights.4 This implies that \(\bm{U}\), representing the principal components of the training data, is effectively captured by the diffusion models. In the low-noise regime (\(\sigma(t)\in[0.002,0.116]\)), however, \(\mathcal{D}_{\bm{\theta}}\) approximates the identity mapping, leading to ambiguous singular vectors with minimal impact on image generation. Further analysis of \(\mathcal{D}_{\bm{\theta}}\)'s behavior in the low-noise regime is provided in Appendices D and F.1.

Footnote 4: For \(\sigma(t)\in[0.116,80.0]\), the less well recovered singular vectors have singular values close to 0, whereas those corresponding to high singular values are well recovered.

Since the optimization problem (9) is convex w.r.t. \(\bm{W}_{\sigma(t)}\) and \(\bm{b}_{\sigma(t)}\), the optimal solution \(\mathcal{D}_{\mathrm{L}}\) represents the unique optimal linear approximation of \(\mathcal{D}_{\bm{\theta}}\). Our analyses demonstrate that this optimal linear approximation closely aligns with \(\mathcal{D}_{\mathrm{G}}\), leading to our central finding: diffusion models in the generalization regime exhibit an inductive bias (which we term as the Gaussian inductive bias) towards learning the Gaussian structure of training data. This manifests in two main ways: (_i_) In the high-noise variance regime, well-trained diffusion models learn \(\mathcal{D}_{\bm{\theta}}\) that closely approximate the linear Gaussian denoisers \(\mathcal{D}_{\mathrm{G}}\); (_ii_) As noise variance decreases, although \(\mathcal{D}_{\bm{\theta}}\) diverges from \(\mathcal{D}_{\mathrm{G}}\), \(\mathcal{D}_{\mathrm{G}}\) remains nearly identical to the optimal linear approximation \(\mathcal{D}_{\mathrm{L}}\), and images generated by \(\mathcal{D}_{\mathrm{G}}\) retain structural similarity to those generated by \(\mathcal{D}_{\bm{\theta}}\).

Finally, we emphasize that the Gaussian inductive bias only emerges in the generalization regime. By contrast, in the memorization regime, Figure 5 shows that \(\mathcal{D}_{\mathrm{L}}\) significantly diverges from \(\mathcal{D}_{\mathrm{G}}\), and both \(\mathcal{D}_{\mathrm{G}}\) and \(\mathcal{D}_{\mathrm{L}}\) provide considerably poorer approximations of \(\mathcal{D}_{\bm{\theta}}\) compared to the generalization regime.

Figure 4: **Linear model shares similar function mapping with Gaussian model.** The left figure shows the difference between the linear weights and the Gaussian weights w.r.t. 100 training epochs of the linear distillation process for the 10 discrete noise levels. The right figure shows the correlation matrices between the first 100 singular vectors of the linear weights and Gaussian weights.

Figure 3: **Images sampled from various Models.** The figure shows the samples generated using different models starting from the same initial noises.

### Theoretical Analysis

In this section, we demonstrate that imposing linear constraints on diffusion models while minimizing the denoising score matching objective (3) leads to the emergence of Gaussian structure.

**Theorem 1**.: _Consider a diffusion denoiser parameterized as a single-layer linear network, defined as \(\mathcal{D}(\bm{x}_{t};\sigma(t))=\bm{W}_{\sigma(t)}\bm{x}_{t}+\bm{b}_{\sigma(t)}\), where \(\bm{W}_{\sigma(t)}\in\mathbb{R}^{d\times d}\) is a linear weight matrix and \(\bm{b}_{\sigma(t)}\in\mathbb{R}^{d}\) is the bias vector. When the data distribution \(p_{\text{data}}(\bm{x})\) has finite mean \(\bm{\mu}\) and bounded positive semidefinite covariance \(\bm{\Sigma}\), the optimal solution to the score matching objective (3) is exactly the Gaussian denoiser defined in (6):_

\[\mathcal{D}_{\mathrm{G}}(\bm{x}_{t};\sigma(t))\ =\ \bm{U}\tilde{\bm{\Lambda}}_{ \sigma(t)}\bm{U}^{T}(\bm{x}_{t}-\bm{\mu})+\bm{\mu},\]

_with \(\bm{W}_{\sigma(t)}=\bm{U}\tilde{\bm{\Lambda}}_{\sigma(t)}\bm{U}^{T}\) and \(\bm{b}_{\sigma(t)}=\left(\bm{I}-\bm{U}\tilde{\bm{\Lambda}}_{\sigma(t)}\bm{U}^ {T}\right)\bm{\mu}\)._

The detailed proof is postponed to Appendix E. This optimal solution corresponds to the classical Wiener filter [31], revealing that diffusion models naturally learn the Gaussian denoisers when constrained to linear architectures. To understand why highly nonlinear diffusion models operate near this linear regime, it is helpful to model the training data distribution as the multi-delta distribution \(p(\bm{x})=\frac{1}{N}\sum_{i=1}^{N}\delta(\bm{x}-\bm{y}_{i})\), where \(\{\bm{y}_{1},\bm{y}_{2},...,\bm{y}_{N}\}\) is the finite training images. Notice that this formulation better reflects practical scenarios where only a finite number of training samples are available rather than the ground truth data distribution. Importantly, it is proved in [25] that the optimal denoisers \(\mathcal{D}_{\mathrm{M}}\) in this case is approximately equivalent to \(\mathcal{D}_{\mathrm{G}}\) for high noise variance \(\sigma(t)\) and query points far from the finite training data. This equivalence explains the strong similarity between \(\mathcal{D}_{\mathrm{G}}\) and \(D_{\mathrm{M}}\) in the high-noise variance regime, and consequently, why \(\mathcal{D}_{\bm{\theta}}\) and \(\mathcal{D}_{\mathrm{G}}\) exhibit high similarity in this regime--deep networks converge to the optimal denoisers for finite training datasets.

However, this equivalence between \(\mathcal{D}_{\mathrm{G}}\) and \(\mathcal{D}_{\mathrm{M}}\) breaks down at lower \(\sigma(t)\) values. The denoising outputs of \(\mathcal{D}_{\mathrm{M}}\) are convex combinations of training data points, weighted by a softmax function with temperature \(\sigma(t)^{2}\). As \(\sigma(t)^{2}\) decreases, this softmax function increasingly approximates an argmax function, effectively retrieving the training point \(\bm{y}_{i}\) closest to the input \(\bm{x}\). Learning this optimal solution requires not only sufficient model capacity to memorize the entire training dataset but also, as shown in [32], an exponentially large number of training samples. Due to these learning challenges, deep networks instead converge to local minima \(\mathcal{D}_{\bm{\theta}}\) that, while differing from \(\mathcal{D}_{\mathrm{M}}\), exhibit better generalization property. Our experiments reveal that these learned \(\mathcal{D}_{\bm{\theta}}\) share similar function mappings with \(\mathcal{D}_{\mathrm{G}}\). The precise mechanism driving diffusion models trained with gradient descent towards this particular solution remains an open question for future research.

Notably, modeling \(p_{\text{data}}(\bm{x})\) as a multi-delta distribution reveals a key insight: while unconstrained optimal denoisers (5) perfectly capture the scores of the empirical distribution, they have no gen

Figure 5: **Comparison between the diffusion denoisers in memorization and generalization regimes**. Figure(a) demonstrates that in the memorization regime (trained on small datasets of size 1094 and 68), \(\mathcal{D}_{\mathrm{L}}\) significantly diverges from \(\mathcal{D}_{\mathrm{G}}\), and both provide substantially poorer approximations of \(\mathcal{D}_{\bm{\theta}}\) compared to the generalization regime (trained on larger datasets of size 35000 and 1094). Figure(b) qualitatively shows that the denoising outputs of \(\mathcal{D}_{\bm{\theta}}\) closely match those of \(\mathcal{D}_{\mathrm{G}}\) only in the generalization regime—a similarity that persists even when the denoisers process pure noise inputs.

eralizability. In contrast, Gaussian denoisers, despite having higher score approximation errors due to the linear constraint, can generate novel images that closely match those produced by the actual diffusion models. This suggests that the generative power of diffusion models stems from the imperfect learning of the score functions of the empirical distribution.

## 4 Conditions for the Emergence of Gaussian Structures and Generalizability

In Section 3, we demonstrate that diffusion models exhibit an inductive bias towards learning denoisers that are close to the Gaussian denoisers. In this section, we investigate the conditions under which this bias manifests. Our findings reveal that this inductive bias is linked to model generalization and is governed by _(i)_ the model capacity relative to the dataset size and _(ii)_ the training duration. For additional results, including experiments on CIFAR-10 dataset, see Appendix F.

### Gaussian Structures Emerge when Model Capacity is Relatively Small

First, we find that the Gaussian inductive bias and the generalization of diffusion models are heavily influenced by the relative size of the model capacity compared to the training dataset. In particular, we demonstrate that:

_Diffusion models learn the Gaussian structures when the model capacity is relatively small compared to the size of training dataset._

This argument is supported by the following two key observations:

\(\bullet\) _Increasing dataset size prompts the emergence of Gaussian structure at fixed model scale._ We train diffusion models using the EDM configuration [4] with a fixed channel size of 128 on datasets of varying sizes \([68,137,1094,8750,35000,70000]\) until FID convergence. Figure 6(left) demonstrates that the score approximation error between diffusion denoisers \(\mathcal{D}_{\boldsymbol{\theta}}\) and Gaussian denoisers \(\mathcal{D}_{\mathrm{G}}\) decreases as the training dataset size grows, particularly in the crucial intermediate noise variance regime (\(\sigma(t)\in[0.116,20]\)). This increasing similarity between \(\mathcal{D}_{\boldsymbol{\theta}}\) and \(\mathcal{D}_{\mathrm{G}}\) correlates with a transition in the models' behavior: from a memorization regime, where generated images are replicas of training samples, to a generalization regime, where novel images exhibiting Gaussian structure5 are produced, as shown in Figure 6(b). This correlation underscores the critical role of Gaussian structure in the generalization capabilities of diffusion models.

Footnote 5: We use the term ”exhibiting Gaussian structure” to describe images that resemble those generated by Gaussian denoisers.

\(\bullet\) _Decreasing model capacity promotes the emergence of Gaussian structure at fixed dataset sizes._ Next, we investigate the impact of model scale by training diffusion models with varying channel sizes \([4,8,16,32,64,128]\), corresponding to \([64\text{k},251\text{k},992\text{k},4\text{M},16\text{M},64\text{M}]\) parameters, on a fixed training dataset of 1094 images. Figure 7(left) shows that in the intermediate noise variance regime (\(\sigma(t)\in[0.116,20]\)), the discrepancy between \(\mathcal{D}_{\boldsymbol{\theta}}\) and \(\mathcal{D}_{\mathrm{G}}\) decreases with decreasing model scale, indicating that Gaussian structure emerges in low-capacity models. Figure 7(right) demonstrates that

Figure 6: **Diffusion models learn the Gaussian structure when training dataset is large.** Models with a fixed scale (channel size 128) are trained across various dataset sizes. The left and right figures show the score difference and the generated images respectively. ”NN” denotes the nearest neighbor in the training dataset to the images generated by the diffusion models.

this trend corresponds to a transition from data memorization to the generation of images exhibiting Gaussian structure. Here we note that smaller models lead to larger discrepancy between \(\mathcal{D}_{\boldsymbol{\theta}}\) and \(\mathcal{D}_{\mathrm{G}}\) in the high-noise regime. This phenomenon arises because diffusion models employ a bell-shaped noise sampling distribution that prioritizes intermediate noise levels, resulting in insufficient training at high noise variances, especially when model capacity is limited (see more details in Appendix F.2).

These two experiments collectively suggest that the inductive bias of diffusion models is governed by the relative capacity of the model compared to the training dataset size.

### Overparameterized Models Learn Gaussian Structures before Memorization

In the overparameterized regime, where model capacity significantly exceeds training dataset size, diffusion models eventually memorize the training data when trained to convergence. However, examining the learning progression reveals a key insight:

_Diffusion models learn the Gaussian structures with generalizability before they memorize._

Figure 8 demonstrates that during early training epochs (0-841), \(\mathcal{D}_{\boldsymbol{\theta}}\) progressively converge to \(\mathcal{D}_{\mathrm{G}}\) in the intermediate noise variance regime, indicating that the diffusion model is progressively learning the Gaussian structure in the initial stages of training. Notably. By epoch 841, the diffusion model generates images strongly resembling those produced by the Gaussian model, as shown in Figure 8(b). However, continued training beyond this point increases the difference between \(\mathcal{D}_{\boldsymbol{\theta}}\) and \(\mathcal{D}_{\mathrm{G}}\) as the model transitions toward memorization. This observation suggests that early stopping could be an effective strategy for promoting generalization in overparameterized diffusion models.

## 5 Connection between Strong Generalizability and Gaussian Structure

A recent study [20] reveals an intriguing "strong generalization" phenomenon: diffusion models trained on large, non-overlapping image datasets generate nearly identical images from the same initial

Figure 8: **Diffusion model learns the Gaussian structure in early training epochs. Diffusion model with same scale (channel size 128) is trained using 1094 images. The left and right figures shows the score difference and the generated images respectively.**

Figure 7: **Diffusion model learns the Gaussian structure when model scale is small. Models with different scales are trained on a fixed training dataset of 1094 images. The left and right figures show the score difference and the generated images respectively.**

noise. While this phenomenon might be attributed to deep networks' inductive bias towards learning the "true" continuous distribution of photographic images, we propose an alternative explanation: rather than learning the complete distribution, deep networks may capture certain low-dimensional common structural features shared across these datasets and these features can be partially explained by the Gaussian structure.

To validate this hypothesis, we examine two diffusion models with channel size 128, trained on non-overlapping datasets S1 and S2 (35000 images each). Figure 9(a) shows that images generated by these models (bottom) closely match those from their corresponding Gaussian models (top), highlighting the Gaussian structure's role in strong generalization.

Comparing Figure 9(a)(top) and (b)(top), we observe that \(\mathcal{D}_{\mathrm{G}}\) generates nearly identical images whether the Gaussian structure is calculated on a small dataset (1094 images) or a much larger one (35000 images). This similarity emerges because datasets of the same class can exhibit similar Gaussian structure (empirical covariance) with relatively few samples--just hundreds for FFHQ. Given the Gaussian structure's critical role in generalization, small datasets may already contain much of the information needed for generalization, contrasting previous assertions in [20] that strong generalization requires training on datasets of substantial size (more than \(10^{5}\) images). However, smaller datasets increase memorization risk, as shown in Figure 9(b). To mitigate this, as discussed in Section 4, we can either reduce model capacity or implement early stopping (Figure 9(c)). Indeed, models trained on 1094 and 35000 images generate remarkably similar images, though the smaller dataset yields lower perceptual quality. This similarity further demonstrates that small datasets contain substantial generalization-relevant information closely tied to Gaussian structure. Further discussion on the connections and differences between our work and [20] are detailed in Appendix H.

## 6 Discussion

In this study, we empirically demonstrate that diffusion models in the generalization regime have the inductive bias towards learning diffusion denoisers that are close to the corresponding linear Gaussian denoisers. Although real-world image distributions are significantly different from Gaussian, our findings imply that diffusion models have the bias towards learning and utilizing low-dimensional data structures, such as the data covariance, for image generation. However, the underlying mechanism by which the nonlinear diffusion models, trained with gradient descent, exhibit such linearity remains unclear and warrants further investigation.

Moreover, the Gaussian structure only partially explains diffusion models' generalizability. While models exhibit increasing linearity as they transition from memorization to generalization, a substantial gap persists between the linear Gaussian denoisers and the actual nonlinear diffusion models, especially in the intermediate noise regime. As a result, images generated by Gaussian denoisers fall short in perceptual quality compared to those generated by the actual diffusion models especially for complex dataset such as CIFAR-10. This disparity highlights the critical role of nonlinearity in high-quality image generation, a topic we aim to investigate further in future research.

Figure 9: **Diffusion models in the strong generalization regime generate similar images as the Gaussian models. Figure(a) Top: Generated images of Gaussian models; Bottom: Generated images of diffusion models, with model scale 128; S1 and S2 each has 35000 non-overlapping images. Figure(b) Top: Generated images of Gaussian model; Bottom: Generated images of diffusion models in the memorization regime, with model scale 128; S1 and S2 each has 1094 non-overlapping images. Figure(c): Early stopping and reducing model capacity help transition diffusion models from memorization to generalization.**

## Data Availability Statement

The code and instructions for reproducing the experiment results will be made available in the following link: https://github.com/Morefre/Understanding-Generalizability-of-Diffusion-Models-Requires-Rethinking-the-Hidden-Gaussian-Structure.

## Acknowledgment

We acknowledge funding support from NSF CAREER CCF-2143904, NSF CCF-2212066, NSF CCF- 2212326, NSF IIS 2312842, NSF IIS 2402950, ONR N00014-22-1-2529, a gift grant from KLA, an Amazon AWS AI Award, and MICDE Catalyst Grant. We also acknowledge the computing support from NCSA Delta GPU [33]. We thank Prof. Rongrong Wang (MSU) for fruitful discussions and valuable feedbacks.

## References

* [1] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International conference on machine learning_, pages 2256-2265. PMLR, 2015.
* [2] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* [3] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_.
* [4] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. _Advances in Neural Information Processing Systems_, 35:26565-26577, 2022.
* [5] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [6] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. In _International Conference on Learning Representations_, 2023.
* [7] Valentin De Bortoli. Convergence of denoising diffusion models under the manifold hypothesis. _Transactions on Machine Learning Research_, 2022.
* [8] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for score-based generative modeling with polynomial complexity. _Advances in Neural Information Processing Systems_, 35:22870-22882, 2022.
* [9] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In _International Conference on Algorithmic Learning Theory_, pages 946-985. PMLR, 2023.
* [10] Yuchen Wu, Yuxin Chen, and Yuting Wei. Stochastic runge-kutta methods: Provable acceleration of diffusion models. _arXiv preprint arXiv:2410.04760_, 2024.
* [11] Gen Li, Yuting Wei, Yuejie Chi, and Yuxin Chen. A sharp convergence theory for the probability flow odes of diffusion models. _arXiv preprint arXiv:2408.02320_, 2024.
* [12] Zhihan Huang, Yuting Wei, and Yuxin Chen. Denoising diffusion probabilistic models are optimally adaptive to unknown low dimensionality. _arXiv preprint arXiv:2410.18784_, 2024.
* [13] Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang. Score approximation, estimation and distribution recovery of diffusion models on low-dimensional data. In _International Conference on Machine Learning_, pages 4672-4712. PMLR, 2023.

* [14] Kazusato Oko, Shunta Akiyama, and Taiji Suzuki. Diffusion models are minimax optimal distribution estimators. In _International Conference on Machine Learning_, pages 26517-26582. PMLR, 2023.
* [15] Kulin Shah, Sitan Chen, and Adam Klivans. Learning mixtures of gaussians using the ddpm objective. _Advances in Neural Information Processing Systems_, 36:19636-19649, 2023.
* [16] Hugo Cui, Eric Vanden-Eijnden, Florent Krzakala, and Lenka Zdeborova. Analysis of learning a flow-based generative model from limited sample complexity. In _The Twelfth International Conference on Learning Representations_, 2023.
* [17] Huijie Zhang, Jinfan Zhou, Yifu Lu, Minzhe Guo, Liyue Shen, and Qing Qu. The emergence of reproducibility and consistency in diffusion models. In _Forty-first International Conference on Machine Learning_, 2024.
* [18] Peng Wang, Huijie Zhang, Zekai Zhang, Siyi Chen, Yi Ma, and Qing Qu. Diffusion models learn low-dimensional distributions via subspace clustering. _arXiv preprint arXiv:2409.02426_, 2024.
* [19] Sixu Li, Shi Chen, and Qin Li. A good score does not lead to a good generative model. _arXiv preprint arXiv:2401.04856_, 2024.
* [20] Zahra Kadkhodaie, Florentin Guth, Eero P Simoncelli, and Stephane Mallat. Generalization in diffusion models arises from geometry-adaptive harmonic representation. In _The Twelfth International Conference on Learning Representations_, 2023.
* [21] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or digital forgery? investigating data replication in diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6048-6058, 2023.
* [22] Gowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Understanding and mitigating copying in diffusion models. _Advances in Neural Information Processing Systems_, 36:47783-47803, 2023.
* [23] TaeHo Yoon, Joo Young Choi, Sehyun Kwon, and Ernest K Ryu. Diffusion probabilistic models generalize when they fail to memorize. In _ICML 2023 Workshop on Structured Probabilistic Inference \(\{\backslash\&\}\) Generative Modeling_, 2023.
* [24] Xiangming Gu, Chao Du, Tianyu Pang, Chongxuan Li, Min Lin, and Ye Wang. On memorization in diffusion models. _arXiv preprint arXiv:2310.02664_, 2023.
* [25] Binxu Wang and John J Vastola. The hidden linear structure in score-based models and its application. _arXiv preprint arXiv:2311.10892_, 2023.
* [26] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4401-4410, 2019.
* [27] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [28] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8188-8197, 2020.
* [29] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. _arXiv preprint arXiv:1506.03365_, 2015.
* [30] Hyojun Go, Yunsung Lee, Seunghyun Lee, Shinhyeok Oh, Hyeongdon Moon, and Seungtaek Choi. Addressing negative transfer in diffusion models. _Advances in Neural Information Processing Systems_, 36, 2024.

- denoising. In Mallat Stephane, editor, _A Wavelet Tour of Signal Processing (Third Edition)_, pages 535-610. Academic Press, Boston, third edition edition, 2009.
* [32] Chen Zeno, Greg Ongie, Yaniv Blumenfeld, Nir Weinberger, and Daniel Soudry. How do minimum-norm shallow denoisers look in function space? _Advances in Neural Information Processing Systems_, 36, 2024.
* [33] Timothy J Boerner, Stephen Deems, Thomas R Furlani, Shelley L Knuth, and John Towns. Access: Advancing innovation: Nsf's advanced cyberinfrastructure coordination ecosystem: Services & support. In _Practice and Experience in Advanced Research Computing_, pages 173-176. 2023.
* [34] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.
* [35] DP Kingma. Adam: a method for stochastic optimization. In _Int Conf Learn Represent_, 2014.
* [36] Alfred O. Hero. Statistical methods for signal processing. 2005.
* [37] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In _Proceedings of the 25th international conference on Machine learning_, pages 1096-1103, 2008.
* [38] Pascal Vincent. A connection between score matching and denoising autoencoders. _Neural computation_, 23(7):1661-1674, 2011.
* [39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In _Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18_, pages 234-241. Springer, 2015.
* [40] Sergey Ioffe. Batch normalization: Accelerating deep network training by reducing internal covariate shift. _arXiv preprint arXiv:1502.03167_, 2015.
* [41] Andrew L Maas, Awni Y Hannun, Andrew Y Ng, et al. Rectifier nonlinearities improve neural network acoustic models. In _Proc. icml_, volume 30, page 3. Atlanta, GA, 2013.
* [42] William Peebles and Saining Xie. Scalable diffusion models with transformers. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 4195-4205, 2023.
* [43] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [44] Sreyas Mohan, Zahra Kadkhodaie, Eero P Simoncelli, and Carlos Fernandez-Granda. Robust and interpretable blind image denoising via bias-free convolutional neural networks. In _International Conference on Learning Representations_.
* [45] Hila Manor and Tomer Michaeli. On the posterior distribution in denoising: Application to uncertainty quantification. In _The Twelfth International Conference on Learning Representations_.
* [46] Siyi Chen, Huijie Zhang, Minzhe Guo, Yifu Lu, Peng Wang, and Qing Qu. Exploring low-dimensional subspaces in diffusion models for controllable image editing. _arXiv preprint arXiv:2409.02374_, 2024.

## Appendices

### 1 Introduction

* 2 Preliminary
* 3 Hidden Linear and Gaussian Structures in Diffusion Models
	* 3.1 Diffusion Models Exhibit Linearity in the Generalization Regime
	* 3.2 Inductive Bias towards Learning the Gaussian Structures
	* 3.3 Theoretical Analysis
* 4 Conditions for the Emergence of Gaussian Structures and Generalizability
	* 4.1 Gaussian Structures Emerge when Model Capacity is Relatively Small
	* 4.2 Overparameterized Models Learn Gaussian Structures before Memorization
* 5 Connection between Strong Generalizability and Gaussian Structure
* 6 Discussion
* A Measuring the Linearity of Diffusion Denoisers
* B Emerging Linearity of Diffusion Models
* B.1 Generalization and Memorization Regimes of Diffusion Models
* B.2 Diffusion Models Exhibit Linearity in the Generalization Regime
* C Linear Distillation
* D Diffusion Models in Low-noise Regime are Approximately Linear Mapping
* E Theoretical Analysis
* E.1 Proof of Theorem 1
* E.2 Two Extreme Cases
* F More Discussion on Section 4
* F.1 Behaviors in Low-noise Regime
* F.2 Behaviors in High-noise Regime
* F.3 Similarity between Diffusion Denoiers and Gaussian Denoisers
* F.4 CIFAR-10 Results
* G Additional Experiment Results
* G.1 Gaussian Structure Emerges across Various Network Architectures
* G.2 Gaussian Inductive Bias as a General Property of DAEs

* Gaussian Structure Emerges across Various datasets
* G.4 Strong Generalization on CIFAR-10
* G.5 Measuring Score Approximation Error with NMSE
* Discussion on Geometry-Adaptive Harmonic Bases
* H.1 GAHB only Partially Explain the Strong Generalization
* H.2 GAHB Emerge only in Intermediate-Noise Regime

## Appendix A Measuring the Linearity of Diffusion Denoisers

In this section, we provide a detailed discussion on how to measure the linearity of diffusion model. For a diffusion denoiser, \(\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t))\), to be considered approximately linear, it must fulfill the following conditions:

* _Additivity:_ The function should satisfy \(\mathcal{D}_{\bm{\theta}}(\bm{x}_{1}+\bm{x}_{2};\sigma(t))\approx\mathcal{D}_ {\bm{\theta}}(\bm{x}_{1};\sigma(t))+\mathcal{D}_{\bm{\theta}}(\bm{x}_{2};\sigma (t))\).
* _Homogeneity:_ It should also adhere to \(\mathcal{D}_{\bm{\theta}}(\alpha\bm{x};\sigma(t))\approx\alpha\mathcal{D}_{\bm {\theta}}(\bm{x};\sigma(t))\).

To jointly assess these properties, we propose to measure the difference between \(\mathcal{D}_{\bm{\theta}}(\alpha\bm{x}_{1}+\beta\bm{x}_{2};\sigma(t))\) and \(\alpha\mathcal{D}_{\bm{\theta}}(\bm{x}_{1};\sigma(t))+\beta D(\bm{x}_{2}; \sigma(t))\). While the linearity score is introduced as the cosine similarity between \(\mathcal{D}_{\bm{\theta}}(\alpha\bm{x}_{1}+\beta\bm{x}_{2};\sigma(t))\) and \(\alpha\mathcal{D}_{\bm{\theta}}(\bm{x}_{1};\sigma(t))+\beta D(\bm{x}_{2}; \sigma(t))\) in the main text:

\[\mathsf{LS}(t)\ =\ \mathbb{E}_{\bm{x}_{1},\bm{x}_{2}\sim p(\bm{x}; \sigma(t))}\left[\left|\left\langle\frac{\mathcal{D}_{\bm{\theta}}(\alpha\bm{ x}_{1}+\beta\bm{x}_{2};\sigma(t))}{\|\mathcal{D}_{\bm{\theta}}(\alpha\bm{x}_{1}+ \beta\bm{x}_{2};\sigma(t))\|_{2}},\frac{\alpha\mathcal{D}_{\bm{\theta}}(\bm{ x}_{1};\sigma(t))+\beta\mathcal{D}_{\bm{\theta}}(\bm{x}_{1};\sigma(t))}{\| \alpha\mathcal{D}_{\bm{\theta}}(\bm{x}_{1};\sigma(t))+\beta\mathcal{D}_{\bm{ \theta}}(\bm{x}_{1};\sigma(t))\|_{2}}\right\rangle\right|\right],\] (11)

it can also be defined with the normalized mean square difference (NMSE):

\[\mathbb{E}_{\bm{x}_{1},\bm{x}_{2}\sim p(\bm{x};\sigma(t))}\frac{\|\mathcal{D }_{\bm{\theta}}(\alpha\bm{x}_{1}+\beta\bm{x}_{2};\sigma(t))-(\alpha\mathcal{ D}_{\bm{\theta}}(\bm{x}_{1};\sigma(t))+\beta\mathcal{D}_{\bm{\theta}}(\bm{x}_{1}; \sigma(t)))\|_{2}}{||\mathcal{D}_{\bm{\theta}}(\alpha\bm{x}_{1}+\beta\bm{x}_{ 2};\sigma(t))||_{2}},\] (12)

where the expectation is approximated with its empirical mean over 100 randomly sampled pairs of \((\bm{x}_{1},\bm{x}_{2})\). In the next section, we will demonstrate the linearity score with both metrics.

Since the diffusion denoisers are trained solely on inputs \(\bm{x}\sim p(\bm{x};\sigma(t))\), their behaviors on out-of-distribution inputs can be quite irregular. To produce a denoised output with meaningful image structure, it is critical that the noise component in the input \(\bm{x}\) matches the correct variance \(\sigma(t)^{2}\). Therefore, our analysis of linearity is restricted to in-distribution inputs \(\bm{x}_{1}\) and \(\bm{x}_{2}\), which are randomly sampled images with additive Gaussian noises calibrated to noise variance \(\sigma(t)^{2}\). We also need to ensure that the values of \(\alpha\) and \(\beta\) are chosen such that \(\alpha^{2}+\beta^{2}=1\), maintaining the correct variance for the noise term in the combined input \(\alpha\bm{x}_{1}+\beta\bm{x}_{2}\). We present the linearity scores, calculated with varying values of \(\alpha\) and \(\beta\), for diffusion models trained on diverse datasets in Figure 10. These models are trained with the EDM-VE configuration proposed in [4], which ensures the resulting models are in the generalization regime. Typically, setting \(\alpha=\beta=1/\sqrt{2}\) yields the lowest linearity score; however, even in this scenario, the cosine similarity remains impressively high, exceeding 0.96. This high value underscores the presence of significant linearity within diffusion denoisers.

We would like to emphasize that for linearity to manifest in diffusion denoisers, it is crucial that they are well-trained, achieving a low denoising score matching loss as indicated in (3). As shown in Figure 11, the linearity notably reduces in a less well trained diffusion model (Baseline-VE) comapred to its well-trained counterpart (EDM-VE). Although both models utilize the same 'VE' network architecture \(\mathcal{F}_{\bm{\theta}}(\bm{x};\sigma(t))\)[2], they differ in how the diffusion denoisers are parameterized:

\[\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t)):=c_{\text{skip}}(\sigma(t))\bm{x}+c _{\text{out}}(\mathcal{F}_{\bm{\theta}}(\bm{x};\sigma(t))),\] (13)where \(c_{\text{skip}}\) is the skip connection and \(c_{\text{out}}\) modulate the scale of the network output. With carefully tailored \(c_{\text{skip}}\) and \(c_{\text{out}}\), the EDM-VE configuration achieves a lower score matching loss compared to Baseline-VE, resulting in samples with higher quality as illustrated in Figure 11(right).

## Appendix B Emerging Linearity of Diffusion Models

In this section we provide a detailed discussion on the observation that diffusion models exhibit increasing linearity as they transition from memorization to generalization, which is briefly described in Section 3.1.

### Generalization and Memorization Regimes of Diffusion Models

As shown in Figure 12, as the training dataset size increases, diffusion models transition from the memorization regime--where they can only replicate its training images--to the generalization regime, where the they produce high-quality, novel images. To measure the generalization capabilities of diffusion models, it is crucial to assess their ability to generate images that are not mere replications of the training dataset. This can be quantitatively evaluated by generating a large set of images from the diffusion model and measuring the average difference between these generated images and their nearest neighbors in the training set. Specifically, let \(\{\bm{x}_{1},\bm{x}_{2},...,\bm{x}_{k}\}\) represent \(k\) randomly sampled images from the diffusion models (we choose \(k=100\) in our experiments), and let \(Y:=\{\bm{y}_{1},\bm{y}_{2},...,\bm{y}_{N}\}\) denote the training dataset consisting of \(N\) images. We define the generalization score as follows:

\[\text{GL Score}:=\frac{1}{k}\sum_{i=1}^{k}\frac{||\bm{x}_{i}- \text{NN}_{Y}(\bm{x}_{i})||_{2}}{||\bm{x}_{i}||_{2}}\] (14)

where \(\text{NN}_{Y}(\bm{x}_{i})\) represents the nearest neighbor of the sample \(\bm{x}_{k}\) in the training dataset \(Y\), determined by the Euclidean distance on a per-pixel basis. Empirically, a GL score exceeding 0.6 indicates that the diffusion models are effectively generalizing beyond the training dataset.

### Diffusion Models Exhibit Linearity in the Generalization Regime

As demonstrated in Figure 13(a) and (d), diffusion models transition from the memorization regime to the generalization regime as the training dataset size increases. Concurrently, as depicted in Figure 13(b), (c), (e) and (f), the corresponding diffusion denoisers exhibit increasingly linearity. This phenomenon persists across diverse datasets datasets including FFHQ [26], AFHQ [28] and LSUN-Churches [29], as well as various model architectures including EDM-VE [3], EDM-VP [2] and EDM-ADM [34]. This emerging linearity implies that the hidden linear structure plays an important role in the generalizability of diffusion model.

## Appendix C Linear Distillation

As discussed in Section 3.1, we propose to study the hidden linearity observed in diffusion denoisers with linear distillation. Specifically, for a given diffusion denoiser \(\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t))\), we aim to approxi

Figure 10: **Linearity scores for varying \(\alpha\) and \(\beta\). The diffusion models are trained with the edm-ve configuration [4], which ensures the models are in the generalization regime.**

mate it with a linear function (with a bias term for more expressibility):

\[\mathcal{D}_{\mathrm{L}}(\bm{x};\sigma(t)):=\bm{W}_{\sigma(t)}\bm{x}+\bm{b}_{ \sigma(t)}\ \approx\ \mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t)),\]

for \(\bm{x}\sim p(\bm{x};\sigma(t))\). Notice that for three dimensional images with size \((c,h,w)\), \(\bm{x}\in\mathbb{R}^{d}\) represents their vectorized version, where \(d=c\times w\times h\). Let

\[\mathcal{L}(\bm{W},\bm{b})=\frac{1}{n}\sum_{i=1}^{n}\left\|\bm{W}_{\sigma(t)} \{k-1\}(\bm{x}_{i}+\bm{\epsilon}_{i})+\bm{b}_{\sigma(t)}\{k-1\}-\mathcal{D}_ {\bm{\theta}}(\bm{x}_{i}+\bm{\epsilon}_{i};\sigma(t))\right\|_{2}^{2}\]

We train \(10\) independent linear models for each of the selected noise variance level \(\sigma(t)\) with the procedure summarized in Algorithm 1:

In practice, the gradients on \(\bm{W}_{\sigma(t)}\) and \(\bm{b}_{\sigma(t)}\) are obtained through automatic differentiation. Additionally, we employ the Adam optimizer [35] for updates. Additional linear distillation results are provided in Figure 14.

Figure 11: **Linearity scores and sampling trajectory. The left and right figures demonstrate the linearity scores and the sampling trajectories \(\mathcal{D}(\bm{x}_{t};\sigma(t)\) of actual diffusion model (EDM-VE and Baseline-VE), Multi Delta model, linear model, and Gaussian model respectively.**

Figure 12: **Memorization and generalization regimes of diffusion models. Figures(a) to (c) show the images generated by diffusion models trained on 70000, 4375, 1094 FFHQ images and their corresponding nearest neighbors in the training dataset respectively. Figures(d) to (f) show the images generated by diffusion models trained on 50000, 12500, 782 CIFAR-10 images and their corresponding nearest neighbors in the training dataset respectively. Notice that when the training dataset size is small, diffusion model can only generate images in the training dataset.**

## Appendix D Diffusion Models in Low-noise Regime are Approximately Linear Mapping

It should be noted that the low score difference between \(\mathcal{D}_{\mathrm{G}}\) and \(\mathcal{D}_{\bm{\theta}}\) within the low-noise regime (\(\sigma(t)\in[0.002,0.116]\)) does not imply the diffusion denoisers capture the Gaussian structure, instead, the similarity arises since both of them are converging to the identity mapping as \(\sigma(t)\) decreases. As shown in Figure 15, within this regime, the differences between the noisy input \(\bm{x}\) and their corresponding denoised outputs \(\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t))\) quickly approach 0. This indicates that the learned denoisers \(\mathcal{D}_{\bm{\theta}}\) progressively converge to the identity function. Additionally, from (6), it is evident that the difference between the Gaussian weights and the identity matrix diminishes as \(\sigma(t)\) decreases, which explains why \(\mathcal{D}_{\mathrm{G}}\) can well approximate \(\mathcal{D}_{\bm{\theta}}\) in the low noise variance regime.

We hypothesize that \(\mathcal{D}_{\bm{\theta}}\) learns the identity function because of the following two reasons:

_(i)_ within the low-noise regime, since the added noise is negligible compared to the clean image, the identity function already achieves a small denoising error, thus serving as a shortcut which is exploited by the deep network.

_(ii)_ As discussed in Appendix A, diffusion models are typically parameterized as follows:

\[\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t)):=c_{\text{skip}}(\sigma(t))\bm{x} +c_{\text{out}}(\mathcal{F}_{\bm{\theta}}(\bm{x};\sigma(t))),\]

where \(\mathcal{F}_{\bm{\theta}}\) represents the deep network, and \(c_{\text{skip}}(\sigma(t))\) and \(c_{\text{out}}(\sigma(t))\) are adaptive parameters for the skip connection and output scaling, respectively, which adjust according to the noise variance levels. For canonical works on diffusion models [2, 34, 3, 4], as \(\sigma(t)\) approaches zero, \(c_{\text{skip}}\) and \(c_{\text{out}}\) converge to 1 and 0 respectively. Consequently, at low variance levels, the function forms of diffusion denoisers are approximatly identity mapping: \(\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t))\approx\bm{x}\).

This convergence to identity mapping has several implications. First, the weights \(\bm{W}_{\sigma(t)}\) of the distilled linear models \(\mathcal{D}_{\mathrm{L}}\) approach the identity matrix at low variances, leading to ambiguous

Figure 13: **Diffusion model exhibit increasing linearity as they transition from memorization to generalization.** Figure(a) and (d) demonstrate that for both FFHQ and CIFAR-10 datasets, the generalization score increases with the training dataset size, indicating progressive model generalization. Figure(b), (c), (e), and (f) show that this transition towards generalization is accompanied by increasing denoiser linearity. Specifically, Figure(b) and (e) display linearity scores calculated using cosine similarity (11), while Figure(c) and (f) show scores computed using NMSE (12). Both metrics reveal consistent trends.

singular vectors. This explains the poor recovery of singular vectors for \(\sigma(t)\in[0.002,0.116]\) shown in Figure 4. Second, the presence of the bias term in (8) makes it challenging for our linear model to learn the identity function, resulting in large errors at \(\sigma(t)=0.002\) as shown in Figure 4(a).

Finally, from (4), we observe that when \(\mathcal{D}_{\bm{\theta}}\) acts as an identity mapping, \(\bm{x}_{i+1}\) remains unchanged from \(\bm{x}_{i}\). This implies that sampling steps in low-variance regions minimally affect the generated image content, as confirmed in Figure 2, where image content shows negligible variation during these steps.

Figure 14: **Additional linear distillation results.** Figure(a) demonstrates the gradual symmetrization of linear weights during the distillation process. Figure(b) shows that at convergence, the singular values of the linear weights closely match those of the Gaussian weights. Figure(c) and Figure(d) display the leading singular vectors of both linear and Gaussian weights at \(\sigma(t)=4\) for FFHQ and LSUN-Churches datasets, respectively, revealing a strong correlation.

## Appendix E Theoretical Analysis

### Proof of Theorem 1

In this section, we give the proof of Theorem 1 (Section 3.3). Our theorem is based on the following two assumptions:

**Assumption 1**.: _Suppose that the diffusion denoisers are parameterized as single-layer linear networks, defined as \(\mathcal{D}(\bm{x};\sigma(t))=\bm{W}_{\sigma(t)}\bm{x}+\bm{b}_{\sigma(t)}\), where \(\bm{W}_{\sigma(t)}\in\mathbb{R}^{d\times d}\) is the linear weight and \(\bm{b}_{\sigma(t)}\in\mathbb{R}^{d}\) is the bias._

**Assumption 2**.: _The data distribution \(p_{\text{data}}(\bm{x})\) has finite mean \(\bm{\mu}\) and bounded positive semidefinite covariance \(\bm{\Sigma}\)_

**Theorem 1**.: _Under Assumption 1 and Assumption 2, the optimal solution to the denoising score matching objective (3) is exactly the Gaussian denoiser: \(\mathcal{D}_{\mathrm{G}}(\bm{x},\sigma(t))=\bm{\mu}+\bm{U}\hat{\bm{\Lambda}}_{ \sigma(t)}\bm{U}^{T}(\bm{x}-\bm{\mu})\), where \(\bm{\Sigma}=\bm{U}\bm{\Lambda}\bm{U}^{T}\) represents the SVD of the covariance matrix, with singular values \(\lambda_{\{k=1,\dots,d\}}\) and \(\hat{\bm{\Lambda}}_{\sigma(t)}=\mathrm{diag}[\frac{\lambda_{k}}{\lambda_{k}+ \sigma(t)^{2}}]\). Furthermore, this optimal solution can be obtained via gradient descent with a proper learning rate._

To prove **Theorem 1**, we first show that the Gaussian denoiser is the optimal solution to the denoising score matching objective under the linear network constraint. Then we will show that such optimal solution can be obtained via gradient descent with a proper learning rate.

The Global Optimal Solution.Under the constraint that the diffusion denoiser is restricted to a single-layer linear network with bias:

\[\mathcal{D}(\bm{x};\sigma(t))=\bm{W}_{\sigma(t)}\bm{x}+\bm{b}_{ \sigma(t)},\] (15)

We get the following optimizaiton problem from Equation (3):

\[\bm{W}^{\star},\bm{b}^{\star}=\underset{\bm{W},\bm{b}}{\text{arg min}}\;\mathcal{L}(\bm{W},\bm{b};\sigma(t)):=\mathbb{E}_{\bm{x}\sim \mathcal{D}_{\text{train}}}\mathbb{E}_{\bm{\epsilon}\sim\mathcal{N}(\bm{0}, \sigma(t)^{2}\bm{I})}\|\bm{W}(\bm{x}+\bm{\epsilon})+\bm{b}-\bm{x}\|_{2}^{2},\] (16)

where we omit the footnote \(\sigma(t)\) in \(\bm{W}_{\sigma(t)}\) and \(\bm{b}_{\sigma(t)}\) for simplicity. Since expectation preserves convexity, the optimization problem Equation (16) is a convex optimization problem. To find the global optimum, we first eliminate \(\bm{b}\) by requiring the partial derivative \(\nabla_{\bm{b}}\mathcal{L}(\bm{W},\bm{b};\sigma(t))\) to be \(\bm{0}\). Since

\[\nabla_{\bm{b}}\mathcal{L}(\bm{W},\bm{b};\sigma(t)) =2*\mathbb{E}_{\bm{x}\sim p_{\text{data}}}\mathbb{E}_{\bm{ \epsilon}\sim\mathcal{N}(\bm{0},\sigma(t)^{2}\bm{I})}((\bm{W}-\bm{I})\bm{x}+ \bm{W}\bm{\epsilon}+\bm{b})\] (17) \[=2*\mathbb{E}_{\bm{x}\sim p_{\text{data}}}((\bm{W}-\bm{I})\bm{x} +\bm{b})\] (18) \[=2*((\bm{W}-\bm{I})\bm{\mu}+\bm{b}),\] (19)

we have

\[\bm{b}^{\star}=(\bm{I}-\bm{W}^{\star})\bm{\mu}.\] (20)

Figure 15: **Difference between \(\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t))\) and \(\bm{x}\) for various noise variance levels.** Figures(a) and (c) show the differences between \(\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t))\) and \(\bm{x}\) across \(\sigma(t)\in[0.002,80]\), measured by normalized MSE and cosine similarity, respectively. Figures(b) and (d) provide zoomed-in views of (a) and (c). The diffusion models were trained on the FFHQ dataset. Notice that the difference between \(\mathcal{D}_{\bm{\theta}}(\bm{x};\sigma(t))\) and \(\bm{x}\) quickly converges to near zero in the low noise variance regime. The trend is consistent for various model architectures.

[MISSING_PAGE_FAIL:21]

Notice that there exists a \(\eta\), such that every eigen value of \(\bm{A}\) is smaller than 1 and greater than 0. In this case, \(\bm{A}^{t+1}\to\bm{0}\) as \(t\to\infty\). Similarly, by the property of matrix geometric series, we have \(\sum_{i=0}^{t}\bm{A}^{i}\to(\bm{I}-\bm{A})^{-1}\). Therefore we have:

\[\tilde{\bm{W}} \to\begin{bmatrix}\mathbb{E}_{\bm{x}}(\bm{x}^{T}\bm{x})&\bm{\mu} \end{bmatrix}\begin{bmatrix}\mathbb{E}_{\bm{x}}(\bm{x}\bm{x}^{T})+\sigma(t)^{2} \bm{I}&\bm{\mu}\\ \bm{u}^{T}&1\end{bmatrix}^{-1}\] (37) \[=\begin{bmatrix}\mathbb{E}_{\bm{x}}(\bm{x}^{T}\bm{x})&\bm{\mu} \end{bmatrix}\begin{bmatrix}\bm{B}&\bm{\mu}\\ \bm{\mu}^{T}&1\end{bmatrix}^{-1},\] (38)

where we define \(\bm{B}:=\mathbb{E}_{\bm{x}}(\bm{x}\bm{x}^{T})+\sigma(t)^{2}\bm{I}\) for simplicity. By the Sherman-Morrison-Woodbury formula, we have:

\[\begin{bmatrix}\bm{B}&\bm{\mu}\\ \bm{\mu}^{T}&1\end{bmatrix}^{-1}=\begin{bmatrix}(\bm{B}-\bm{\mu}\bm{\mu}^{T})^ {-1}&-(\bm{B}-\bm{\mu}\bm{\mu}^{T})^{-1}\bm{\mu}\\ -(1-\bm{\mu}^{T}\bm{B}^{-1}\bm{\mu})^{-1}\bm{\mu}^{T}\bm{B}^{-1}&(1-\bm{\mu}^{ T}\bm{B}^{-1}\bm{\mu})^{-1}\end{bmatrix}.\] (39)

Therefore, we have:

\[\tilde{\bm{W}} \to\begin{bmatrix}\mathbb{E}_{\bm{x}}[\bm{x}\bm{x}^{T}](\bm{B}- \bm{\mu}\bm{\mu}^{T})^{-1}-\frac{\bm{\mu}\bm{\mu}^{T}\bm{B}^{-1}}{1-\bm{\mu}^{ T}\bm{B}^{-1}\bm{\mu}}&-\mathbb{E}_{\bm{x}}[\bm{x}\bm{x}^{T}](\bm{B}-\bm{\mu} \bm{\mu}^{T})^{-1}\bm{\mu}+\frac{\bm{\mu}}{1-\bm{\mu}^{T}\bm{B}^{-1}\bm{\mu}} \end{bmatrix},\] (40)

from which we have

\[\bm{W} \to\mathbb{E}_{\bm{x}}[\bm{x}\bm{x}^{T}](\bm{B}-\bm{\mu}\bm{\mu} ^{T})^{-1}-\frac{\bm{\mu}\bm{\mu}^{T}\bm{B}^{-1}}{1-\bm{\mu}^{T}\bm{B}^{-1}\bm {\mu}}\] (41) \[\bm{b} \to-\mathbb{E}_{\bm{x}}[\bm{x}\bm{x}^{T}](\bm{B}-\bm{\mu}\bm{\mu} ^{T})^{-1}\bm{\mu}+\frac{\bm{\mu}}{1-\bm{\mu}^{T}\bm{B}^{-1}\bm{\mu}}\] (42)

Since \(\mathbb{E}_{\bm{x}}[\bm{x}\bm{x}^{T}]=\mathbb{E}_{\bm{x}}[(\bm{x}-\bm{\mu})(( \bm{x}-\bm{\mu})^{T}]+\bm{\mu}\bm{\mu}^{T}\), we have:

\[\bm{W}=\bm{\Sigma}(\bm{\Sigma}+\sigma(t)^{2}\bm{I})^{-1}+\bm{\mu} \bm{\mu}^{T}(\bm{B}-\bm{\mu}\bm{\mu}^{T})^{-1}-\frac{\bm{\mu}\bm{\mu}^{T}\bm{ B}^{-1}}{1-\bm{\mu}^{T}\bm{B}^{-1}\bm{\mu}}.\] (43)

Applying Sherman-Morrison Formula, we have:

\[(\bm{B}-\bm{\mu}\bm{\mu}^{T})^{-1}=\bm{B}^{-1}+\frac{\bm{B}^{-1} \bm{\mu}\bm{\mu}^{T}\bm{B}^{-1}}{1-\bm{\mu}^{T}\bm{B}^{-1}\bm{\mu}},\] (44)

therefore

\[\bm{\mu}\bm{\mu}^{T}(\bm{B}-\bm{\mu}\bm{\mu}^{T})^{-1}-\frac{\bm {\mu}\bm{\mu}^{T}\bm{B}^{-1}}{1-\bm{\mu}^{T}\bm{B}^{-1}\bm{\mu}} =\frac{\bm{\mu}\bm{\mu}^{T}\bm{B}^{-1}\bm{\mu}\bm{\mu}^{T}\bm{B} ^{-1}}{1-\bm{\mu}^{T}\bm{B}^{-1}\bm{\mu}}-\frac{\bm{\mu}\bm{\mu* Case 1: \(\mathcal{D}_{\bm{\theta}}(\bm{x}+\bm{\epsilon};\sigma(t))\approx\bm{x}\;\;\forall \bm{x}\sim p_{\text{data}},\bm{\epsilon}\sim\mathcal{N}(\bm{0},\sigma(t)^{2} \bm{I})\).
* Case 2: \(\mathcal{D}_{\bm{\theta}}(\bm{x}+\bm{\epsilon};\sigma(t))\approx\mathcal{D}_{ \mathrm{G}}(\bm{x}+\bm{\epsilon};\sigma(t))\) for \(\;\forall\bm{x}\sim p_{\text{data}},\bm{\epsilon}\sim\mathcal{N}(\bm{0}, \sigma(t)^{2}\bm{I})\).

Case 1 requires \(\mathcal{D}_{\bm{\theta}}(\bm{x}+\bm{\epsilon};\sigma(t))\) to be the oracle denoiser that perfectly recover the ground truth clean image, which never happens in practice except when \(\sigma(t)\) becomes extremely small. Instead, our empirical results suggest diffusion models in the generalization regime bias towards Case 2, where deep networks learn \(\mathcal{D}_{\bm{\theta}}\) that approximate (not equal) to \(D_{\mathrm{G}}\). This is evidenced in Figure 5(b), where diffusion models trained on larger datasets (35000 and 7000 images) produce denoising outputs similar to \(\mathcal{D}_{\mathrm{G}}\). Notice that this similarity holds even when the denoisers take pure Gaussian noise as input. The exact mechanism driving diffusion models trained with gradient descent towards this particular solution remains an open question and we leave it as future work.

## Appendix F More Discussion on Section 4

While in Section 4 we mainly focus on the discussion of the behavior of diffusion denoisers in the intermediate-noise regime, in this section we study the denoiser dynamics in both low and high-noise regime. We also provide additional experiment results on CIFAR-10 dataset.

### Behaviors in Low-noise Regime

We visualize the score differences between \(\mathcal{D}_{\mathrm{G}}\) and \(\mathcal{D}_{\bm{\theta}}\) in low-noise regime in Figure 16. The left figure demonstrates that when the dataset size becomes smaller than a certain threshold, the score difference at \(\sigma=0\) remains persistently non-zero. Moreover, the right figure shows that this difference depends solely on dataset size rather than model capacity. This phenomenon arises from two key factors: _(i)_\(\mathcal{D}_{\bm{\theta}}\) converges to the identity mapping at low noise levels, independent of training dataset size and model capacity, and _(ii)_\(\mathcal{D}_{\mathrm{G}}\) approximates the identity mapping at low noise levels only when the empirical covariance matrix is full-rank, as can be seen from (6).

Since the rank of the covariance matrix is upper-bounded by the training dataset size, \(\mathcal{D}_{\mathrm{G}}\) differs from the identity mapping when the dataset size is smaller than the data dimension. This creates a persistent gap between \(\mathcal{D}_{\mathrm{G}}\) and \(\mathcal{D}_{\bm{\theta}}\), with smaller datasets leading to lower rank and consequently larger score differences. These observations align with our discussion in Appendix D.

### Behaviors in High-noise Regime

As shown in Figure 7(a), while a decreased model scale pushes \(\mathcal{D}_{\bm{\theta}}\) in the intermediate noise region towards \(\mathcal{D}_{\mathrm{G}}\), their differences enlarges in the high noise variance regime. This phenomenon arises because diffusion models employ a bell-shaped noise sampling distribution that prioritizes intermediate noise levels, resulting in insufficient training at high noise variances. A shown in Figure 17, for high \(\sigma(t)\), \(\mathcal{D}_{\bm{\theta}}\) converge to \(\mathcal{D}_{\mathrm{G}}\) when trained with sufficient model capacity (Figure 17(b)) and training time (Figure 17(c)). This behavior is consistent irrespective of the training dataset sizes (Figure 17(a)). Convergence in the high-noise variance regime is less crucial in practice, since diffusion steps in

Figure 16: **Score differences for low-noise variances.** The left and right figures are the zoomed-in views of Figure 6(a) and Figure 7(a) respectively. Notice that when the dataset size is smaller than the dimension of the image, the score differences are always non-zero at \(\sigma=0\).

this regime contribute substantially less than those in the intermediate-noise variance regime--a phenomenon we analyze further in Appendix G.5.

### Similarity between Diffusion Denoisers and Gaussian Denoisers

In Section 4, we demonstrate that the Gaussian inductive bias is most prominent in models with limited capacity and during early training stages, a finding qualitatively validated in Figure 18. Specifically, Figure 18(b) shows that larger models (channel sizes 128 and 64) tend to memorize,

Figure 17: \(\mathcal{D}_{\bm{\theta}}\) **converge to \(\mathcal{D}_{\mathrm{G}}\) with no overfitting for high noise variances.** Figure(a) shows the denoising outputs of \(\mathcal{D}_{\mathrm{M}}\), \(\mathcal{D}_{\mathrm{G}}\) and well-trained (trained with sufficient model capacity till convergence) \(\mathcal{D}_{\bm{\theta}}\). Notice that at high noise variance, the three different denoisers are approximately equivalent despite the training dataset size. Figure(b) shows the denoising outputs of \(\mathcal{D}_{\bm{\theta}}\) with different model scales trained until convergence. Notice that \(\mathcal{D}_{\bm{\theta}}\) converges to \(\mathcal{D}_{\mathrm{G}}\) only when the model capacity is large enough. Figure(c) shows the denoising outputs of \(\mathcal{D}_{\bm{\theta}}\) with sufficient large model capacity at different training epochs. Notice that \(\mathcal{D}_{\bm{\theta}}\) converges to \(\mathcal{D}_{\mathrm{G}}\) only when the training duration is long enough.

Figure 18: **Denoising outputs of \(\mathcal{D}_{\mathrm{G}}\) and \(\mathcal{D}_{\bm{\theta}}\) at \(\sigma=4\). Figure(a) shows the clean image \(\bm{x}\) (from test set), random noise \(\bm{e}\) and the resulting noisy image \(\bm{y}\). Figure(b) compares denoising outputs of \(\mathcal{D}_{\bm{\theta}}\) across different channel sizes [4, 8, 64, 128] with those of \(\mathcal{D}_{\mathrm{G}}\). Figure(c) shows the evolution of \(\mathcal{D}_{\bm{\theta}}\) outputs at training epochs [187, 841, 9173, 64210] alongside \(\mathcal{D}_{\mathrm{G}}\) outputs. All models are trained on a fixed dataset of 1,094 images.**

directly retrieving training data as denoising outputs. In contrast, smaller models (channel sizes 8 and 4) exhibit behavior similar to \(\mathcal{D}_{\mathrm{G}}\), producing comparable denoising outputs. Similarly, Figure 18 reveals that during early training epochs (0-841), \(\mathcal{D}_{\boldsymbol{\theta}}\) outputs progressively align with those of \(\mathcal{D}_{\mathrm{G}}\). However, extended training beyond this point leads to memorization.

### CIFAR-10 Results

The effects of model capacity and training duration on the Gaussian inductive bias, as demonstrated in Figures 19 to 21, extend to the CIFAR-10 dataset. These results confirm our findings from Section 4: the Gaussian inductive bias is most prominent when model scale and training duration are limited.

## Appendix G Additional Experiment Results

While in the main text we mainly demonstrate our findings using EDM-VE diffusion models trained on FFHQ, in this section we show our results are robust and extend to various model architectures and datasets. Furthermore, we demonstrate that the Gaussian inductive bias is not unique to diffusion models, but it is a fundamental property of denoising autoencoders [37]. Lastly, we verify that our

Figure 19: **Large dataset size prompts the Gaussian structure. Models with the same scale (channel size 64) are trained on CIFAR-10 datasets with varying sizes. Figure(a) shows that larger dataset size leads to increased similarity between \(\mathcal{D}_{\mathrm{G}}\) and \(\mathcal{D}_{\boldsymbol{\theta}}\), resulting in structurally similar generated images as shown in Figure(b).**

Figure 20: **Smaller model scale prompts the Gaussian structure. Models with varying scales are trained on a fixed CIFAR-10 datasets with 782 images. Figure(a) shows that smaller model scale leads to increased similarity between \(\mathcal{D}_{\mathrm{G}}\) and \(\mathcal{D}_{\boldsymbol{\theta}}\) in the intermediate noise regime (\(\sigma\in[0.1,10]\)), resulting in structurally similar generated images as shown in figure(b). However, smaller scale leads to larger score differences in high-noise regime due to insufficient training from limited model capacity.**

conclusions remain consistent when using alternative metrics such as NMSE instead of the RMSE used in the main text.

### Gaussian Structure Emerges across Various Network Architectures

We first demonstrate that diffusion models capture the Gaussian structure of the training dataset, irrespective of the deep network architectures used. As shown in Figure 22 (a), (b), and (c), although the actual diffusion models, \(\mathcal{D}_{\bm{\theta}}\), are parameterized with different architectures, for all noise variances except \(\sigma(t)\in\{0.002,80.0\}\), their corresponding linear models, \(\mathcal{D}_{\mathrm{L}}\), consistently converge towards the common Gaussian models, \(\mathcal{D}_{\mathrm{G}}\), determined by the training dataset. Qualitatively, as depicted in Figure 23, despite variations in network architectures, diffusion models generate nearly identical images, matching those generated from the Gaussian models.

### Gaussian Inductive Bias as a General Property of DAEs

In previous sections, we explored the properties of diffusion models by interpreting them as collections of deep denoisers, which are equivalent to the denoising autoencoders (DAEs) [37] trained on various noise variances by minimizing the denoising score matching objective (3). Although diffusion models and DAEs are equivalent in the sense that both of them are trying to learn the score function of the

Figure 21: **Diffusion model learns the Gaussian structure in early training epochs. Models with the same scale (channel size 128) are trained on a fixed CIFAR-10 datasets with 782 images. Figure(a) shows that the similarity between \(\mathcal{D}_{\mathrm{G}}\) and \(\mathcal{D}_{\bm{\theta}}\) progressively increases during early training epochs (0-921) in the intermediate noise regime (\(\sigma\in[0.1,10]\)), resulting in structurally similar generated images as shown in figure(b). However, continue training beyond this point results in diverged \(\mathcal{D}_{\mathrm{G}}\) and \(\mathcal{D}_{\bm{\theta}}\), resulting in memorization.**

Figure 22: **Linear model shares similar function mapping with Gaussian model. The figures demonstrate the evolution of normalized MSE between the linear weights \(\mathcal{D}_{\mathrm{L}}\) and the Gaussian weights \(\mathcal{D}_{\mathrm{G}}\) w.r.t. linear distillation training epochs. Figures(a), (b) and (c) correspond to diffusion models trained on FFHQ, with EDM-VE, EDM-ADM and EDM-VP network architectures specified in [4] respectively.**

noise-mollified data distribution [38], the training objective of diffusion models is more complex [4]:

\[\min_{\bm{\theta}}\mathbb{E}_{\bm{x},\bm{\epsilon},\sigma}[\lambda(\sigma)c_{ \text{out}}(\sigma)^{2}||\mathcal{F}_{\bm{\theta}}(\bm{x}+\bm{\epsilon},\sigma) -\underbrace{\frac{1}{c_{\text{out}}(\sigma)}(\bm{x}-c_{\text{skip}}(\sigma) (\bm{x}+\bm{\epsilon}))}_{\text{linear combination of $\bm{x}$ and $\bm{\epsilon}$}}||_{2}^{2}],\] (52)

where \(\bm{x}\sim p_{\text{data}},\bm{\epsilon}\sim\mathcal{N}(\bm{0},\sigma(t)^{2} \bm{I})\) and \(\sigma\sim p_{\text{train}}\). Notice that the training objective of diffusion models has a few distinct characteristics:

* Diffusion models use a single deep network \(\mathcal{F}_{\bm{\theta}}\) to perform denoising score matching across all noise variances while DAEs are typically trained separately for each noise level.
* Diffusion models are not trained uniformly across all noise variances. Instead, during training the probability of sampling a given noise level \(\sigma\) is controlled by a predefined distribution \(p_{\text{train}}\) and the loss is weighted by \(\lambda(\sigma)\).
* Diffusion models often utilize special parameterizations (13). Therefore, the deep network \(\mathcal{F}_{\bm{\theta}}\) is trained to predict a linear combination of the clean image \(\bm{x}\) and the noise \(\bm{\epsilon}\), whereas DAEs typically predict the clean image directly.

Given these differences, we investigate whether the Gaussian inductive bias is unique to diffusion models or a general characteristic of DAEs. To this end, we train separate DAEs (deep denoisers) using the vanilla denoising score matching objective (3) on each of the 10 discrete noise variances specified by the EDM schedule [80.0, 42.415, 21.108, 9.723, 4.06, 1.501, 0.469, 0.116, 0.020, 0.002], and compare the score differences between them and the corresponding Gaussian denoisers \(\mathcal{D}_{\text{G}}\). We use no special parameterization so that \(\mathcal{D}_{\bm{\theta}}=\mathcal{F}_{\bm{\theta}}\); that is, the deep network directly predicts the clean image. Furthermore, the DAEs for each noise variance are trained till convergence, ensuring all noise levels are trained sufficiently. We consider the following architectural choices:

* _DAE-NCSN_: In this setting, the network \(\mathcal{F}_{\bm{\theta}}\) uses the NCSN architecture [3], the same as that used in the EDM-VE diffusion model.
* _DAE-Skip_: In this setting, \(\mathcal{F}_{\bm{\theta}}\) is a U-Net [39] consisting of convolutional layers, batch normalization [40], leaky ReLU activation [41] and convolutional skip connections. We refer to this network as "Skip-Net". Compared to NCSN, which adapts the state of the art architecture designs, Skip-Net is deliberately constructed to be as simple as possible to test how architectural complexity affects the Gaussian inductive bias.
* _DAE-DiT_: In this setting, \(\mathcal{F}_{\bm{\theta}}\) is a Diffusion Transformer (DiT) introduced in [42]. Vision Transformers are known to lack inductive biases such as locality and translation equivariance that are inherent to convolutional models [43]. Here we are interested in if this affects the Gaussian inductive bias.

Figure 23: **Images sampled from various model.The figure shows the sampled images from diffusion models with different network architectures and those from their corresponding Gaussian models.**

* _DAE-Linear_: In this setting we set \(\mathcal{F}_{\bm{\theta}}\) to be a linear model with a bias term as in (8). According to Theorem 1, these models should converge to Gaussian denoisers.

The quantitative results are shown in Figure 24(a). First, the DAE-linear models well approximate \(\mathcal{D}_{\mathrm{G}}\) across all 10 discrete steps (RMSE smaller than 0.04), consistent with Theorem 1. Second, despite the differences between diffusion models (EDM) and DAEs, they achieve similar score approximation errors relative to \(\mathcal{D}_{\mathrm{G}}\) for most noise variances, meaning that they can be similarly approximated by \(\mathcal{D}_{\mathrm{G}}\). However, diffusion models exhibit significantly larger deviations from \(\mathcal{D}_{\mathrm{G}}\) at higher noise variances (\(\sigma\in\{42.415,80.0\}\)) since they utilize a bell-shaped noise sampling distribution \(p_{\text{train}}\) that emphasizes training on intermediate noise levels, leading to under-training at high noise variances. Lastly, the DAEs with different architectures achieve comparable score approximation errors, and both DAEs and diffusion models generate images matching those from the Gaussian model, as shown in Figure 24(b). These findings demonstrate that the Gaussian inductive bias is not unique to diffusion models or specific architectures but is a fundamental property of DAEs.

### Gaussian Structure Emerges across Various datasets

As illustrated in Figure 25, for diffusion models trained on the CIFAR-10, AFHQ and LSUN-Churches datasets that are in the generalization regime, their generated samples match those produced by the corresponding Gaussian models. Additionally, their linear approximations, \(\mathcal{D}_{\mathrm{L}}\), obtained through linear distillation, align closely with the Gaussian models, \(\mathcal{D}_{\mathrm{G}}\), resulting in nearly identical generated images. These findings confirm that the Gaussian structure is prevalent across various datasets.

### Strong Generalization on CIFAR-10

Figure 26 demonstrates the strong generalization effect on CIFAR-10. Similar to the observations in Section 5, reducing model capacity or early stopping the training process prompts the Gaussian inductive bias, leading to generalization.

### Measuring Score Approximation Error with NMSE

While in Section 3.1 we define the score field approximation error between denoisers \(\mathcal{D}_{1}\) and \(\mathcal{D}_{2}\) with RMSE ( (10)), this error can also be quantified using NMSE:

\[\text{Score-Difference}(t):=\mathbb{E}_{\bm{x}\sim p_{\text{data}}(\bm{x}),\bm{\epsilon}\sim\mathcal{N}(\bm{0};\sigma(t)^{2}\bm{I})}\frac{||\mathcal{D} _{1}(\bm{x}+\bm{\epsilon})-\mathcal{D}_{2}(\bm{x}+\bm{\epsilon})||_{2}}{|| \mathcal{D}_{1}(\bm{x}+\bm{\epsilon})||_{2}}.\] (53)

As shown in Figure 27, while the trend in intermediate-noise and low-noise regimes remains unchanged, NMSE amplifies differences in the high-noise variance regime compared to RMSE. This amplified score difference between \(\mathcal{D}_{\mathrm{G}}\) and \(\mathcal{D}_{\bm{\theta}}\) does not contradict our main finding that diffusion models in the generalization regime exhibit an inductive bias towards learning denoisers

Figure 24: **Comparison between DAEs and diffusion models.** Figure(a) compares the score field approximation error between Gaussian models and both _(i)_ diffusion models (EDM vs. Gaussian) and _(ii)_ DAEs with varying architectures. Figure(b) illustrates the generation trajectories of different models initialized from the same noise input.

approximately equivalent to \(\mathcal{D}_{\mathrm{G}}\) in the high-noise variance regime. As discussed in Section 3.2 and appendices F.2 and G.2, this large score difference stems from inadequate training in this regime.

Figure 27 (Gaussian vs. DAE) demonstrates that when DAEs are sufficiently trained at specific noise variances, they still converge to \(\mathcal{D}_{\mathrm{G}}\). Importantly, the insufficient training in the high-noise variance regime minimally affects final generation quality. Figure 25(f) shows that while the diffusion model (EDM) produces noisy trajectories at early timesteps (\(\sigma\in\{80.0,42.415\}\)), these artifacts quickly disappear in later stages, indicating that the Gaussian inductive bias is most influential in the intermediate-noise variance regime.

Notably, even when \(\mathcal{D}_{\bm{\theta}}\) are inadequately trained in the high-noise variance regime, they remain approximable by linear functions, though these functions no longer match \(\mathcal{D}_{\mathrm{G}}\).

Figure 25: **Final generated images and sampling trajectories for various models.** Figures(a), (c) and (e) demonstrate the images generated using different models starting from the same noises for LSUN-Churches, AFHQ and CIFAR-10 respectively. Figures(b), (d) and (f) demonstrate the corresponding sampling trajectories.

## Appendix H Discussion on Geometry-Adaptive Harmonic Bases

### GAHB only Partially Explain the Strong Generalization

Recent work [20] observes that diffusion models trained on sufficiently large non-overlapping datasets (of the same class) generate nearly identical images. They explain this "strong generalization" phenomenon by analyzing bias-free deep diffusion denoisers with piecewise linear input-output

Figure 27: **Comparison between RMSE and NMSE score differences. Figures(a) and (c) show the score field approximation errors measured with RMSE loss while figures(b) and (d) show these errors measured using NMSE loss. Compared to RMSE, the NMSE metric highlight the score differences in the high-noise regime, where diffusion models receive the least training.**

Figure 26: **Strong generalization on CIFAR-10 dataset. Figure(a) Top: Generated images of Gaussian models; Bottom: Generated images of diffusion models, with model scale 64; S1 and S2 each has 25000 non-overlapping images. Figure(b) Top: Generated images of Gaussian model; Bottom: Generated images of diffusion models in the memorization regime, with model scale 128; S1 and S2 each has 782 non-overlapping images. Figure(c): Early stopping and reducing model capacity help transition diffusion models from memorization to generalization.**mappings:

\[\mathcal{D}(\bm{x}_{t};\sigma(t)) =\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\bm{x}\] (54) \[=\sum_{k}\lambda_{k}(\bm{x}_{t})\bm{u}_{k}(\bm{x}_{t})\bm{v}_{k}^ {T}(\bm{x}_{t})\bm{x}_{t},\] (55)

where \(\lambda_{k}(\bm{x}_{t})\), \(\bm{u}_{k}(\bm{x}_{t})\), and \(\bm{v}_{k}(\bm{x}_{t})\) represent the input-dependent singular values, left and right singular vectors of the network Jacobian \(\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\). Under this framework, strong generalization occurs when two denoisers \(\mathcal{D}_{1}\) and \(\mathcal{D}_{2}\) have similar Jacobians: \(\nabla\mathcal{D}_{1}(\bm{x}_{t};\sigma(t))\approx\nabla\mathcal{D}_{2}(\bm{ x}_{t};\sigma(t))\). The authors conjecture this similarity arises from networks' inductive bias towards learning certain optimal \(\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\) that has sparse singular values and the singular vectors of which are the geometry-adaptive harmonic bases (GAHB)--near-optimal denoising bases that adapt to input \(\bm{x}_{t}\).

While [20] provides valuable insights, their bias-free assumption does not reflect real-world diffusion models, which inherently contain bias terms. For feed forward ReLU networks, the denoisers are piecewise affine:

\[\mathcal{D}(\bm{x}_{t};\sigma(t))=\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\bm{ x}_{t}+\bm{b}_{\bm{x}_{t}},\] (56)

where \(\bm{b}_{\bm{x}_{t}}\) is the network bias that depends on both network parameterization and the noisy input \(\bm{x}_{t}\)[44]. Here, similar Jacobians alone cannot explain strong generalization, as networks may differ significantly in \(\bm{b}_{\bm{x}_{t}}\). For more complex network architectures where even piecewise affinity fails, we consider the local linear expansion of \(\mathcal{D}(\bm{x}_{t};\sigma(t))\):

\[\mathcal{D}(\bm{x}_{t}+\Delta\bm{x};\sigma(t))=\nabla\mathcal{D}(\bm{x}_{t}; \sigma(t))\Delta\bm{x}_{t}+\mathcal{D}(\bm{x}_{t};\sigma(t)),\] (57)

which approximately holds for small perturbation \(\Delta\bm{x}\). Thus, although \(\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\) characterizes \(\mathcal{D}(\bm{x}_{t};\sigma(t))\)'s local behavior around \(\bm{x}_{t}\), it does not provide sufficient information on the global properties.

Our work instead examines global behavior, demonstrating that \(\mathcal{D}(\bm{x}_{t};\sigma(t))\) is close to \(\mathcal{D}_{\mathrm{G}}(\bm{x}_{t};\sigma(t))\)--the optimal linear denoiser under the Gaussian data assumption. This implies that strong generalization partially stems from networks learning similar Gaussian structures across non-overlapping datasets of the same class. Since our linear model captures global properties but not local characteristics, it complements the local analysis in [20].

### GAHB Emerge only in Intermediate-Noise Regime

For completeness, we study the evolution of the Jacobian matrix \(\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\) across various noise levels \(\sigma(t)\). The results are presented in Figures 28 and 29, which reveal three distinct regimes:

* _High-noise regime [10,80]._ In this regime, the leading singular vectors6 of the Jacobian matrix \(\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\) well align with those of the Gaussian weights (the leading principal components of the training dataset), consistent with our finding that diffusion denoisers approximate linear Gaussian denoisers in this regime. Notice that DAEs trained sufficiently on separate noise levels (Figure 29) show stronger alignment compared to vanilla diffusion models (Figure 28), which suffer from insufficient training at high noise levels. Footnote 6: We only care about leading singular vectors since the Jacobians in this regime are highly low-rank. The less well aligned singular vectors have singular values near 0.
* _Intermediate-noise regime [0.1,10]:_ In this regime, GAHB emerge as singular vectors of \(\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\) diverge from the principal components, becoming increasingly adaptive to the geometry of input image.
* _Low-noise regime [0.002,0.1]._ In this regime, the leading singular vectors of \(\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\) show no clear patterns, consistent with our observation that diffusion denoisers approach the identical mapping, which has unconstrained singular vectors.

Notice that the leading singular vectors of \(\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\) are the input directions that lead to the maximum variation in denoised outputs, thus revealing meaningful information on the local properties of \(\mathcal{D}(\bm{x}_{t};\sigma(t))\) at \(\bm{x}_{t}\). As demonstrated in Figure 30, perturbing input \(\bm{x}_{t}\) along these vectors at difference noise regimes leads to distinct effects on the final generated images: _(i)_ in the high-noise regime where the leading singular vectors align with the principal components of the training dataset,perturbing \(\bm{x}_{t}\) along these directions leads to canonical changes such as image class, _(ii)_ in the intermediate-noise regime where the GAHB emerge, perturbing \(\bm{x}_{t}\) along the leading singular vectors modify image details such as colors while preserving overall image structure and _(iii)_ in the low-noise regime where the leading singular vectors have no significant pattern, perturbing \(\bm{x}_{t}\) along these directions yield no meaningful semantic changes.

These results collectively demonstrate that the singular vectors of the network Jacobian \(\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\) have distinct properties at different noise regimes, with GAHB emerging specifically in the intermediate regime. This characterization has significant implications for uncertainty quantification [45] and image editing [46].

Figure 28: **Evolution of \(\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\) across varying noise levels.** Figure(a) shows the generation trajectory. Figure(b) shows the correlation matrix between Jacobian singular vectors \(\bm{U}(\bm{x}_{t})\) and training dataset principal components \(\bm{U}\). Notice that the leading singular vectors of \(\bm{U}(\bm{x}_{t})\) and \(\bm{U}\) well align in early timesteps but diverge in later timesteps. Figure(c) shows the first three principal components of the training dataset while figures(d-f) show the evolution of Jacobian’s first three singular vectors across noise levels. These singular vectors initially match the principal components but progressively adapt to input image geometry, before losing distinct patterns at very low noise levels. While we present only left singular vectors, right singular vectors exhibit nearly identical behavior and yield equivalent results.

## Appendix I Computing Resources

All the diffusion models in the experiments are trained on A100 GPUs provided by NCSA Delta GPU [33].

Figure 30: **Effects of perturbing \(\bm{x}_{t}\) along Jacobian singular vectors.** Figure(a)-(c) demonstrate the effects of perturbing input \(\bm{x}_{t}\) along the first singular vector of the Jacobian matrix (\(\bm{x}_{t}\pm\lambda\bm{u}_{1}(\bm{x}_{t})\)) on the final generated images. Perturbing \(\bm{x}_{t}\) in high-noise regime (Figure (a)) leads to canonical image changes while perturbation in intermediate-noise regime (Figure (b)) leads to change in details but the overall image structure is preserved. At very low noise variances, perturbation has no significant effect (Figure (c)). Similar effects are observed in concurrent work [46].

Figure 29: **Evolution of \(\nabla\mathcal{D}(\bm{x}_{t};\sigma(t))\) across varying noise levels for DAEs.** We repeat the experiments in Figure 28 on DAEs that are sufficiently trained on each discrete noise levels. Notice that with sufficient training, the Jacobian singular vectors \(\bm{U}(\bm{x}_{t})\) show a better alignment with principal components \(\bm{U}\) in early timesteps.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We mention the main contribution in both the abstract and introduction section, listed in the "Contributions". * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations in the "Discussion" section.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We provide the assumption and proof in Section 3.3 and Appendix E. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We include the experiment details in Section 3, Section 4 and Section 5. We will also release our code upon publication. Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes]

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).

* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We include the experiment details in Section 3, Section 4 and Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For various plots we specify that the numbers are calculated over 100 random samples. However, since the standard deviations are quite small, the error bar is unnecessary. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We include the computating resources in Appendix I. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research doesn't have potential harm cause by research process and negative societal impact. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work is an empirical and theoretical work on the inductive bias of the diffusion models. It has more scientific contributions rather than societal impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our work is an empirical and theoretical work on how the diffusion model learns the data distribution. It doesn't have a high risk of misuse.

Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
* **Licenses for existing assets*
* Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We use the public released dataset and models, which properly credited the license. Guidelines:
* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our work is an empirical and theoretical work on how the diffusion model learns the data distribution. We don't release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.