# Delayed Algorithms for Distributed Stochastic Weakly Convex Optimization

Wenzhi Gao

Stanford University

gwz@stanford.edu

&Qi Deng

Shanghai University of Finance and Economics

qideng@sufe.edu.cn

Work done while at SHUFE.The corresponding author.

###### Abstract

This paper studies delayed stochastic algorithms for weakly convex optimization in a distributed network with workers connected to a master node. Recently, Xu et al. 2022 showed that an inertial stochastic subgradient method converges at a rate of \(\mathcal{O}(\tau_{\max}/\sqrt{K})\) which depends on the maximum information delay \(\tau_{\max}\). In this work, we show that the delayed stochastic subgradient method (DSGD) obtains a tighter convergence rate which depends on the expected delay \(\bar{\tau}\). Furthermore, for an important class of composition weakly convex problems, we develop a new delayed stochastic prox-linear (DSPL) method in which the delays only affect the high-order term in the complexity rate and hence, are negligible after a certain number of DSPL iterations. In addition, we demonstrate the robustness of our proposed algorithms against arbitrary delays. By incorporating a simple safeguarding step in both methods, we achieve convergence rates that depend solely on the number of workers, eliminating the effect of the delay. Our numerical experiments further confirm the empirical superiority of our proposed methods.

## 1 Introduction

In this paper, we consider the following stochastic optimization problem

\[\min_{x\in\mathbb{R}^{n}}\psi(x)\coloneqq\mathbb{E}_{\xi\sim}\Xi[f(x,\xi)]+ \omega(x),\] (1)

where \(f(x,\xi)\) is a nonconvex continuous function over \(x\) and \(\xi\) is a random variable sampled from some distribution \(\Xi\); \(\omega(x)\) is lower-semicontinuous and proximal-friendly. We assume that both \(f(x,\xi)\) and \(\omega(x)\) belong to a general class of nonsmooth nonconvex functions that exhibit weak convexity. Here, we say that a function \(g(x)\) is \(\kappa\)-weakly convex if \(g(x)+\frac{\kappa}{2}\|x\|^{2}\) is convex for some \(\kappa\geq 0\). Weakly convex optimization has attracted growing interest in machine learning in recent years, and we are particularly interested in a general type of weakly convex problems with the following composition structure [12]

\[f(x,\xi)=h(c(x,\xi)),\] (2)

where \(h\) is a convex Lipschitz function and \(c(x,\xi)\) is smooth. Optimization in the above composition form is pervasive in applications arising from machine learning and data science, including robust phase retrieval [14], blind deconvolution [8], robust PCA and matrix completion [5], among others.

Stochastic (sub)gradient method and its proximal variants [39; 10; 30; 28; 31] (all referred as SGD in our paper) are arguably the most popular approaches for solving problem (1). Typically, SGD iteratively solves \(x^{k+1}=\arg\min_{x}\ \{\langle f^{\prime}(x^{k},\xi^{k}),x-x^{k}\rangle+ \omega(x)+\frac{\gamma_{k}}{2}\|x-x^{k}\|^{2}\},\) where \(\xi^{k}\) isa random sample and \(f^{\prime}(x^{k},\xi^{k})\) denotes a subgradient of \(f(x^{k},\xi^{k})\). However, despite its wide popularity in machine learning, the sequential and synchronous nature of SGD is not suitable for modern applications that require parallel processing in multi-cores or over multiple machines.

To further improve SGD in parallel and distributed environments, recent work considers a more practical asynchronous setting where the parameter updates allow outdated gradient information. See [1, 32, 38, 29, 21]. In the asynchronous setting, it is crucial to know how the stale updates based on delayed information affects convergence. For smooth convex optimization, Agarwal and Duchi [1] show that delayed stochastic gradient method (DSGD) obtains a rate of \(\mathcal{O}(\sigma/\sqrt{K}+\tau_{\sigma^{2}}/(\sigma^{2}K))\), where \(\tau_{\sigma^{2}}\) bounds of the second moment of random delays. DSGD with adaptive stepsize has also been studied by [25, 32, 38] to achieve better empirical performance and the latest work [2, 33] further improves the rate to \(\mathcal{O}(\sigma/\sqrt{K}+\tau_{\text{max}}/K)\). For general smooth (nonconvex) problems, the work [33] shows that DSGD converges to stationarity at a rate of \(\mathcal{O}(\sigma/\sqrt{K}+\tau_{\text{max}}/K)\). In a follow-up study [7], the authors propose a more robust DSGD whose rate depends on the average delay \(\tau_{\text{avg}}\), rather than the maximum delay. Recently, Koloskova et al. [18] develop a sharp analysis for asynchronous SGD and then a simple delay-adaptive stepsize to achieve the best rate \(\mathcal{O}(\sigma/\sqrt{K}+\tau_{\text{avg}}/K)\). Based on novel delay-adaptive stepsizes and virtual iterate-based analysis, a concurrent work [26] has established new convergence rates that depend on the number of workers rather than the delay of the gradient.

Despite much progress in distributed smooth optimization, it remains unclear how to develop efficient asynchronous algorithms for nonsmooth and even nonconvex problems. For general nonsmooth convex problems, the pioneering work [27] has shown that the asynchronous incremental subgradient method obtains an \(\mathcal{O}(\sqrt{\tau_{\text{max}}/K})\) convergence rate. The aforementioned work [26] shows that DSGD achieves a delay-independent rate of \(\mathcal{O}(\sqrt{m/K})\), where \(m\) is the number of workers. However, it is still unknown whether their key technique by telescoping the virtual iterates can be extended to composite nonconvex optimization.

For distributed weakly convex optimization, Xu et al. [34] shows that Delayed Stochastic Extrapolated Subgradient (DSEGD), an inertial version of DSGD, exhibits a convergence rate of \(\mathcal{O}((1+\tau_{\text{max}})/\sqrt{K}+\tau_{\text{max}}^{2}/K)\), which has an undesired dependence on the maximum delay \(\tau_{\text{max}}\). This issue is further exacerbated in real heterogeneous environments where the maximum delay is dictated by the slowest, or the "straggler" nodes. It remains unclear

_1) whether such delay dependence in DSGD is still improvable or not for weakly convex problems?_

Nevertheless, even if the rate is improvable in terms of \(\tau\), there yet remains a fundamental challenge, as the delay significantly influences the leading term \(\mathcal{O}(1/\sqrt{K})\) of the convergence rate. This contrasts with smooth distributed optimization, where the delay only affects a higher order term \(\mathcal{O}(1/K)\) and hence is negligible in the long run. Such a performance gap highlights a substantial limitation of the prior study when nonsmoothness is present. Hence, it is natural to ask: For distributed weakly convex optimization,

_2) can we design algorithms with a negligible penalty caused by delays?_

_3) Moreover, can we make convergence rates delay independent?_

The goal of this paper is to address the above three questions.

ContributionsIn this paper, we answer the above questions positively (Table 1). Our contributions are as follows.

* For distributed weakly convex optimization, we provide a sharp convergence analysis of DSGD under statistical assumptions on the delays and obtain a rate of \(\mathcal{O}(\bar{\tau}/\sqrt{K}+\tau_{\sigma^{2}}/K)\), where \(\bar{\tau},\tau_{\sigma^{2}}\) are respectively first and second moments of stochastic delays. Our result significantly improves upon the previous \(\mathcal{O}(\tau_{\text{max}}/\sqrt{K}+\tau_{\text{max}}^{2}/K)\) rate for DSGD[34].
* For weakly convex problems with composition structure (2), we propose a new delayed stochastic prox-linear method (DSPL) which can exploit the structure (2) more effectively. Unlike SGD, the stochastic prox-linear algorithm ([9, 11]) partially linearizes the inner function \(c(\cdot,\xi)\) while retaining the outer function \(h(\cdot)\). Then it iteratively solves:\(\langle\nabla c(x^{k},\xi),x-x^{k}\rangle)+\omega(x)+\frac{\gamma_{k}}{2}\|x-x^{k} \|^{2}\big{\}}\). To the best of our knowledge, this is the first study of SPL in the asynchronous distributed setting. We show that the new DSPL method achieves a rate of \(\mathcal{O}(1/\sqrt{K}+\tau_{\sigma^{2}}/K)\), which is consistently better than that of DSGD in terms of the dependence on delay. Interestingly, our result implies that the delay is negligible when \(K\) is sufficiently large.
3. We propose a simple yet effective safeguarding step that skips the iteration when the delay is significantly large. This enhancement ensures that the rate depends only on the number of workers rather than on the delay explicitly. Specifically, in an environment of \(m\) workers, we obtain an \(\mathcal{O}(\sqrt{m}/\sqrt{K}+m/K)\) rate for DSGD and \(\mathcal{O}(1/\sqrt{K}+m^{2}/K)\) for DSPL, making both methods robust to arbitrary delays. As per our knowledge, these are the first delay-independent rates for distributed nonsmooth nonconvex optimization. Prior to our work, delay-independent rates were only known for smooth or convex optimization [26], which were derived through a distinctly different delay-adaptive stepsize strategy.

Structure of the paperSection 2 introduces the notations and problem setup. Section 3 analyzes the delayed stochastic (proximal) subgradient method (DSGD). Section 4 develops the delayed stochastic prox-linear method (DSPL). Section 5 proposes a simple strategy to make the asynchronous algorithms robust to arbitrary delays. Section 6 conducts experiments to verify our theoretical results. We draw conclusions in Section 7 and leave all the proofs and further discussions in the appendix.

## 2 Preliminaries

NotationsWe use \(\|\cdot\|\) and \(\langle\cdot,\cdot\rangle\) to denote the Euclidean norm and inner product. The sub-differential of a function \(f\) is defined by \(\partial f(x)\coloneqq\{v:f(y)\geq f(x)+\langle v,y-x\rangle+o(\|x-y\|),y\to x\}\) and \(f^{\prime}(x)\in\partial f(x)\) denotes a subgradient. If \(0\in\partial f(x)\), then \(x\) is called a stationary point of \(f\). The domain of \(f\) is defined by \(\operatorname{dom}f\coloneqq\{x:f(x)<\infty\}\). At iteration \(k\), we use \(\mathbb{E}_{k}[\cdot]\) to denote the expectation conditioned on past iterations \(\{x^{1},\ldots,x^{k-1}\}\). \(1\!\{\cdot\}\) denotes the 0-1 indicator function of an event. Given a delay sequence \(\{\tau_{k}\}\), we write \(\Delta_{1}\coloneqq\frac{1}{K}\sum_{k=1}^{K}\tau_{k}\) and \(\Delta_{2}\coloneqq\frac{1}{K}\sum_{k=1}^{K}\tau_{k}^{2}\).

Our analysis will adopt the Moreau envelope as the potential function, a technique initially identified in the work [8]. Let \(f\) be a \(\kappa\)-weakly convex function. Given \(\rho>\kappa\), the Moreau envelope and the associated proximal mapping of \(f\) are respectively defined by

\[f_{1/\rho}(x)\coloneqq\min_{y}\left\{f(y)+\tfrac{\rho}{2}\|x-y\|^{2}\right\} \quad\text{and}\quad\text{prox}_{f/\rho}(x)\coloneqq\operatorname*{arg\,min} _{y}\left\{f(y)+\tfrac{\rho}{2}\|x-y\|^{2}\right\}.\]

By the optimality condition and convexity of \(f(y)+\tfrac{\rho}{2}\|x-y\|^{2}\), \(0\in\partial f(\text{prox}_{f/\rho}(x))+\rho(\text{prox}_{f/\rho}(x)-x)\). The Moreau envelope can be nicely interpreted as a smooth approximation of the original function. Specifically, it can be shown that \(f_{1/\rho}(x)\) is differentiable and its gradient is \(\nabla f_{1/\rho}(x)=\rho(x-\text{prox}_{f/\rho}(x))\) (See [8]). Combining the above two relations, we obtain \(\nabla f_{1/\rho}(x)\in\partial f(\text{prox}_{f/\rho}(x))\). Therefore, the Moreau envelope can be used as a measure of approximate stationarity: if \(\|\nabla f_{1/\rho}(x)\|\leq\varepsilon\), then \(x\) is in the proximity (i.e. \(\|x-\text{prox}_{f/\rho}(x)\|\leq\rho^{-1}\varepsilon\)) of a near stationary point \(\text{prox}_{f/\rho}(x)\) (i.e. \(\operatorname{dist}(\partial f(\text{prox}_{f/\rho}(x)),0)\leq\varepsilon\)).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Delay & Work & Setting & Problems / algorithms & Convergence rate \\ \hline \multirow{3}{*}{Non-robust} & [34] & & \(f+\omega\) / DSGD & \(\mathcal{O}(\frac{\gamma_{\text{max}}}{\sqrt{K}}+\frac{\tau_{\text{max}}^{2}}{K})\) \\  & Ours & weakly convex & \(f+\omega\) / DSGD & \(\mathcal{O}(\frac{\gamma_{\text{max}}}{\sqrt{K}}+\frac{\tau_{\text{max}}^{2}}{K})\) \\  & & & \(h\circ e+\omega\) / DSPL & \(\mathcal{O}(\frac{\gamma_{\text{max}}}{\sqrt{K}}+\frac{\tau_{\text{max}}^{2}}{K})\) \\ \hline \multirow{3}{*}{Robust} & [26, 18, 7] & smooth nonconvex & \(f\) / DSGD & \(\mathcal{O}(\frac{1}{\sqrt{K}}+\frac{m}{K})\) or \(\mathcal{O}(\frac{1}{\sqrt{K}}+\frac{\tau_{\text{avg}}}{K})\) \\  & & & \(f+\omega\) / DSGD & \(\mathcal{O}(\frac{\gamma_{\text{max}}}{\sqrt{K}}+\frac{\tau_{\text{avg}}}{K})\) \\ \cline{1-1}  & & & \(h\circ e+\omega\) / DSPL & \(\mathcal{O}(\frac{1}{\sqrt{K}}+\frac{m^{2}}{K})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Rates of delayed algorithms for nonconvex optimization. \(m\): the agent number; \(\tau_{\text{max}}\): the maximum delay; \(\bar{\tau}=\mathbb{E}[\tau_{k}]\); \(\tau_{\sigma^{2}}=\mathbb{E}[\tau_{k}^{2}]\); \(\tau_{\text{avg}}\) average over arbitrary delays.

### Assumptions

Throughout the paper, we make the following assumptions.

1. (i.d. sample) We draw i.i.d. samples \(\{\xi^{k}\}\) from \(\Xi\).
2. (Lipschitz continuity) \(\omega(x)\) is \(L_{\omega}\)-Lipschitz continuous over its domain.
3. (Weak convexity) \(\omega\) is \(\kappa\)-weakly convex.
4. (Bounded moments) The distribution of the independent stochastic delays \(\{\tau_{k}\}\) has bounded first and second moments. i.e., \(\mathbb{E}[\tau_{k}]\leq\bar{\tau}<\infty,\mathbb{E}[\tau_{k}^{2}]\leq\tau_{ \sigma^{2}}<\infty,\forall k\).

_Remark 1_.: Assumptions **A1** to **A3** are typical in stochastic weakly convex optimization [8], while **A4** is common in distributed optimization [3; 32]. Moreover, throughout our analysis, we regard \(\{\tau_{k}\}\) as a pre-defined random sequence independent of data sampling and the algorithm [32; 34].

## 3 Delayed proximal subgradient method

In this section, we analyze the convergence rate of the delayed stochastic proximal subgradient method (DSGD) for weakly convex optimization. DSGD is the workhorse of most applications and is frequently analyzed in the literature on centralized distributed optimization.

**Input:**\(x^{1}\);

**for**\(k\) = 1, 2,... **do**

Let \(g^{k-\tau_{k}}\in\partial f(x^{k-\tau_{k}},\xi^{k-\tau_{k}})\) be computed by a worker with delay \(\tau_{k}\);

In the master node update

\[x^{k+1}=\operatorname*{arg\,min}_{x\in\mathbb{R}^{n}}\big{\{}\langle g^{k- \tau_{k}},x-x^{k}\rangle+\omega(x)+\tfrac{\gamma_{k}}{2}\|x-x^{k}\|^{2}\}.\] (3)

**end**

**Algorithm 1**Delayed stochastic proximal subgradient method

We restrict \(f\) to have a bounded subgradient and make the following extra assumption.

1. \(f(x,\xi)\) is \(\lambda\)-weakly convex, \(\mathbb{E}_{\xi}[\|g\|^{2}]\leq L_{f}^{2},g\in\partial f(x,\xi)\) for all \(x\in\operatorname{dom}\omega\subseteq\text{int}(\operatorname{dom}f)\) and all \(\xi\sim\Xi\).

Now we are ready to present the convergence analysis of DSGD. Our analysis relies on Moreau envelope, denoted as \(\psi_{1/\rho}(x^{k})\), which serves as the potential function. After assessing the descent of the potential function with delays separated as an error term, we derive the convergence result by bounding the stochastic delays. The following lemma provides the descent property.

**Lemma 1**.: _Let \(\tilde{x}^{k}:=\text{prox}_{\psi/\rho}(x^{k})\). Suppose **A1**, **A2**, **A3** and **B1** hold. If \(\rho>2\lambda+\kappa,\gamma_{k}\geq\rho\), then_

\[\tfrac{\rho(\rho-2\lambda-\kappa)}{2(\gamma_{k}-2\lambda-\kappa )}\|\tilde{x}^{k}-x^{k}\|^{2} \leq\psi_{1/\rho}(x^{k})-\mathbb{E}_{k}[\psi_{1/\rho}(x^{k+1})]- \tfrac{\rho(\gamma_{k}-\rho)}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E}_{k}[\|x ^{k+1}-x^{k}\|^{2}]\] \[\quad+\tfrac{\rho\lambda}{\gamma_{k}-2\lambda-\kappa}\mathbb{E}_{ k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]+\tfrac{2\rho L_{f}}{\gamma_{k}-2\lambda- \kappa}\mathbb{E}_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|]\]

_Remark 2_.: **Lemma 1** shows that aside from the noise and delay-related terms, unless we are close to approximate stationarity characterized by \(\|\tilde{x}^{k}-x^{k}\|^{2}\), there is always sufficient decrease in the potential function. Intuitively, if we take sufficiently large regularization \(\gamma\) and bound the delays using **A4**, convergence is almost immediate. Now we show convergence of DSGD in **Theorem 1**.

**Theorem 1**.: _Under the same conditions as_ **Lemma 1** _as well as_ **A4**_, taking \(\gamma_{k}\equiv\gamma=\rho+4\lambda+\kappa+\sqrt{K}/\alpha\) for some \(\alpha>0\), letting \(k^{*}\) be chosen between 1 and \(K\) uniformly at random, then_

\[\mathbb{E}\big{[}\|\nabla\psi_{1/\rho}(x^{k^{*}})\|^{2}\big{]}\leq\tfrac{2 \rho}{\rho-2\lambda-\kappa}\Big{[}\tfrac{(\rho+2\lambda)D}{K}+\tfrac{D}{ \sqrt{K}\alpha}+\tfrac{4\rho L_{f}(L_{f}+L_{\omega})\alpha}{\sqrt{K}}(\Delta_ {1}+1)+\tfrac{8\rho\lambda(L_{f}+L_{\omega})^{2}\alpha^{2}}{K}\Delta_{2}\Big{]},\]

_where \(D=\psi_{1/\rho}(x^{1})-\inf_{x}\psi(x)\) and recall our notation \(\Delta_{1}=\frac{1}{K}{\sum_{k=1}^{K}}\tau_{k},\Delta_{2}=\frac{1}{K}{\sum_{k= 1}^{K}}\tau_{k}^{2}\)._

_Remark 3_.: Note that \(\alpha\) controls the trade-off between noise, delay and optimization. In practice we can set \(\alpha\) as a hyper-parameter and tune it to improve performance.

[MISSING_PAGE_FAIL:5]

From **Proposition 1**, we see \(f(x,\xi)\) is \(L_{h}C(\xi)\)-weakly convex since the error between \(f(x,\xi)\) and a convex function is bounded by a quadratic function. Furthermore, we know that \(f(x)\) is \(L_{h}L_{c}\) Lipschitz-continuous and \(L_{h}C\)-weakly convex after taking expectation. For a unified analysis, we take \(L_{f}=L_{h}L_{c},\lambda=L_{h}C\) and use these constants to present the results. The following lemma characterizes the descent property for our potential function in DSPL.

**Lemma 2**.: _Suppose_ **A1**_,_ **A2**_,_ **A3** _and_ **C1** _hold, if \(\rho>2\lambda+\kappa,\gamma_{k}\geq\rho\), then_

\[\tfrac{\rho(\rho-2\lambda-\kappa)}{2(\gamma_{k}-2\lambda-\kappa)} \|\hat{x}^{k}-x^{k}\|^{2} \leq\psi_{1/\rho}(x^{k})-\mathbb{E}_{k}[\psi_{1/\rho}(x^{k+1})]- \tfrac{\rho(\gamma_{k}-\rho)}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E}_{k}[ \|x^{k+1}-x^{k}\|^{2}]\] \[\quad+\tfrac{2\rho L_{f}^{2}}{(\gamma_{k}-\kappa)(\gamma_{k}-2 \lambda-\kappa)}+\tfrac{3\beta\lambda}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E }_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}].\]

Similar to the analysis of DSGD, we bound the delays to give the convergence result.

**Theorem 2**.: _Under the same conditions as_ **Lemma 2** _as well as_ **A4**_, taking \(\gamma_{k}\equiv\gamma=\rho+6\lambda+\kappa+\sqrt{K}/\alpha\) for some \(\alpha>0\), letting \(k^{*}\) be uniformly chosen between \(1\) and \(K\), then_

\[\mathbb{E}[\|\nabla\psi_{1/\rho}(x^{k^{*}})\|^{2}]\leq\tfrac{2\rho}{\rho-2 \lambda-\kappa}\Big{[}\tfrac{(\rho+4\lambda)D}{K}+\tfrac{D}{\sqrt{K}\alpha}+ \tfrac{2\rho L_{f}^{2}\alpha}{\sqrt{K}}+\tfrac{6\rho\lambda(L_{f}+L_{\omega}) ^{2}\alpha^{2}}{K}\Delta_{2}\Big{]},\]

_where \(D=\psi_{1/\rho}(x^{1})-\inf_{x}\psi(x)\)._

If we take \(\mathbb{E}[\Delta_{2}]=\tau_{\sigma^{2}}\), then Theorem 2 implies that

\[\mathbb{E}[\|\nabla\psi_{1/\rho}(x^{k^{*}})\|^{2}]=\mathcal{O}(\tfrac{1}{ \sqrt{K}}+\tfrac{\lambda\Delta}{K})\;\;\text{or}\;\;\mathbb{E}[\|\nabla\psi_{ 1/\rho}(x^{k^{*}})\|^{2}]=\mathcal{O}(\tfrac{1}{\sqrt{K}}+\tfrac{\lambda\tau _{\sigma^{2}}}{K}).\]

Compared to DSGD, the delays for DSPL appear only in a higher-order term with respect to \(K\). Different from DSGD where \(\lambda\) only characterizes weak convexity, \(\lambda\) for DSPL also represents the quadratic upper-bound on \(f(x,\xi)\) in **P2**, which is not 0 even if the problem is convex.

Dspl vs. DsgWe give some further insights on the behavior of DSPL and DSGD for the composition function (2). Intuitively, as the algorithm converges, we have \(\lim_{k\to 0}\|x^{k}-x^{k+1}\|=0\) a.s. When \(x^{k}\approx x^{k-\tau_{k}}\), DSPL enjoys an increasingly stable estimation of the proximal mapping (4), as the influence of delay is diminishing and the error is mainly driven by stochastic sampling. The same conclusion holds on smooth DSGD due to the Lipschitz continuity of \(\nabla f\). On the other hand, when DSGD is applied for a nonsmooth problem, the master node receives an out-of-date subgradient \(f^{\prime}\) from the worker and solves (3). Since \(f(x,\xi)\) is nonsmooth, the subgradient \(f^{\prime}(x^{k-\tau_{k}},\xi)\) may differ significantly from \(f^{\prime}(x^{k},\xi)\) even when the sequence \(\{x^{k}\}\) converges. Hence, DSGD will constantly suffer from delay during all the updates (3).

Dspl with momentumWe also remark that the momentum technique from DSGD can be extended to DSPL, which gives us the same \(\mathcal{O}(\tau_{\text{max}}/\sqrt{K}+\tau_{\text{max}}^{2}/K)\) convergence rate as in [34]. We refer the interested readers to the Appendix F.

When \(f\) is smooth, the analysis of DSPL can be adapted to yield a comparable convergence rate for the proximal stochastic gradient method for minimizing \(\psi(\cdot)\).

**Theorem 3**.: _Suppose all the assumptions in_ **Lemma 1** _and_ **A4** _hold, and that \(f\) is \(\lambda\)-smooth. Let \(\gamma_{k}\equiv\gamma=\rho+6\lambda+\kappa+\sqrt{K}/\alpha\) for some \(\alpha>0\), then \(\mathbb{E}[\|\nabla\psi_{1/\rho}(x^{k^{*}})\|^{2}]=\mathcal{O}(\tfrac{1}{\sqrt {K}}+\tfrac{\lambda\Delta_{2}}{K})\)._

So far we have spent two sections analyzing the two algorithms so that enough intuition can be established. All these serve for our ultimate goal: making DSGD and DSPL robust to arbitrary delays.

Figure 1: DSGD and DSPL in a master-worker architecture.

Weakly convex optimization robust to arbitrary delays

This section proposes robust variants of DSGD and DSPL, for which the explicit delays are eliminated from the convergence rate. What we will do is reduce the impact of delay to _the number of agents in the network_. Moving forward, we substitute **A4** with **D1**.

**D1:**: The distributed environment has \(m\) workers.

The previously established results have provided sufficient intuition on how delays impact our algorithms. In view of **Theorem 1** and **Theorem 2**, we have isolated delay-dependent \(\mathcal{O}(\Delta_{1}/\sqrt{K})\) and \(\mathcal{O}(\Delta_{2}/K)\) in the proof. Although \(\Delta_{1}/\sqrt{K}\) seems larger, it turns out that \(\Delta_{2}\) stands in our way. The following lemma shows \(\Delta_{1}=\frac{1}{K}\sum_{k=1}^{K}\tau_{k}\) is bounded by \(m\).

**Lemma 3**.: _In a distributed environment of \(m\) workers \(\Delta_{1}=\frac{1}{K}\sum_{k=1}^{K}\tau_{k}\leq m-1\leq m\)._

Given **Lemma 3**, we know \(\Delta_{1}/\sqrt{K}=\mathcal{O}(m/\sqrt{K})\) and we can replace dependency on \(\Delta_{1}\) by \(m\). Now we consider \(\Delta_{2}=\frac{1}{K}\sum_{k=1}^{K}\tau_{k}^{2}\). Even if we have a larger denominator \(K\) neutralizing the effect of \(\Delta_{2}\), the following example shows that in the worst case, \(\Delta_{2}\) can be of \(\mathcal{O}(K)\) and result in \(\mathcal{O}(1)\) error.

**Example 1** (Why \(\Delta_{2}\) hurts performance).: _Given sequence of delays \(\{\tau_{k}\}\) such that \(\tau_{k}=0,k\leq K-1,\tau_{K}=K\). Then \(\Delta_{2}=K\) and \(\Delta_{2}/K=1\)._

The example tells that delays of \(\mathcal{O}(K)\) ruin our convergence. In other words, to recover an overall \(\mathcal{O}(1/\sqrt{K})\) convergence rate, we need \(\Delta_{2}/K=\mathcal{O}(1/\sqrt{K})\Rightarrow\sum_{k=1}^{K}\tau_{k}^{2}= \mathcal{O}(K^{3/2})\). The next lemma provides a hint for our algorithm design.

**Lemma 4**.: _If a sequence of nonnegative integers \(\{\tau_{k}\}\) satisfy \(\sum_{k=1}^{K}\tau_{k}\leq mK\), then given \(T\geq 0\),_

1. _if_ \(\tau_{k}\leq T\) _for all_ \(k\)_, then_ \(\sum_{k=1}^{K}\tau_{k}^{2}\leq mKT\)_;_
2. \(\sum_{k=1}^{K}\mathbb{I}\{\tau_{k}\leq T\}\geq K-mKT^{-1}\)_._

**Lemma 4** tells us two facts about a nonnegative integer sequence of length \(K\) with sum bounded by \(\mathcal{O}(K)\): **1)**. if we restrict the elements to be less than \(\mathcal{O}(T)\), then \(\sum_{k=1}^{K}\tau_{k}^{2}\leq\mathcal{O}(KT)\). **2)**. there will be \(\Omega(K-mKT^{-1})\) elements bounded by \(T\). Back to our context, this implies **1)**. To reduce \(\Delta_{2}/K\) to \(\mathcal{O}(1/\sqrt{K})\), we can discard the iterations of delays greater than \(T=\mathcal{O}(\sqrt{K})\). **2)**. We skip no more than \(\mathcal{O}(\sqrt{K})\) iterations and optimization works with \(\mathcal{O}(K-\sqrt{K})=\mathcal{O}(K)\) iterations left.

``` Input:\(x^{1},T=r^{-1}mK^{\beta}\); for\(k=1\), 2,... do if\(\tau_{k}\leq T\)then  Update with (3) for DSGD or (4) for DSPL else \(x^{k+1}=x^{k}\)  end if end while ```

**Algorithm 3**Safeguarded DSGD/DSPL

Having established the foundational understanding, we outline the main steps in **Algorithm 3**. It is based on the aforementioned intuition and incorporates a "safeguarding" step to discard inaccurate information which could potentially hurt the convergence performance. As the above argument on the accumulated delay is independent of any specific algorithm, the safeguarding strategy can be applied to both DSGD and DSPL. To make our parameter setting more general, in our analysis we consider \(T=r^{-1}mK^{\beta},r>0,\beta\geq 0\).

Intuitively, under worst-case scenarios, **Algorithm 3** will perform after \(K\) iterations in a manner similar to its non-safeguarded counterpart. However, the maximum delay will be capped at \(r^{-1}mK^{\beta}\), and the iteration count will be \(K(1-rK^{-\beta})\).

**Theorem 4** (Safeguarded DSGD).: _Under the same conditions as **Lemma 1** as well as **D1**, taking **1)**\(\beta>0,K>r^{1/\beta}\) or **2)**\(\beta=0,r<1\), then letting \(\gamma_{k}\equiv\gamma=\frac{\sqrt{K}}{\alpha\sqrt{\eta}}+\rho+\kappa+4\lambda, \eta=1+\frac{r}{K^{\beta}-r}\) for some \(\alpha>0\) and \(k^{*}\) be uniformly chosen between iterations where \(\tau_{k}\leq T=r^{-1}mK^{\beta}\),_

\[\mathbb{E}[\|\nabla\psi_{1/\rho}(x^{k^{*}})\|^{2}]\leq\frac{2\rho}{\rho-2\lambda -\kappa}\Big{[}\tfrac{\eta(\rho+2\lambda)D}{K}+\tfrac{\sqrt{\eta}D}{\sqrt{K} \alpha}+\tfrac{4\eta^{3/2}\rho L_{f}(L_{f}+L_{\omega})m\alpha}{\sqrt{K}}+ \tfrac{8\eta^{2}\rho\lambda(L_{f}+L_{\omega})^{2}m^{2}\alpha^{2}}{rK^{1-\beta}} \Big{]},\]

_where \(D=\psi_{1/\rho}(x^{1})-\inf_{x}\psi(x)\)._

**Theorem 5** (Safeguarded DSPL).: _Under the same conditions as_ **Lemma 2** _as well as_ **D1**_, taking **I)**__\(\beta>0,K>r^{1/\beta}\) or **2)**__\(\beta=0,r<1\), then letting \(\gamma_{k}\equiv\gamma=\tfrac{\sqrt{K}}{\alpha\sqrt{\eta}}+\rho+\kappa+6\lambda,\eta=1+\tfrac{r}{K^{\beta}-r}\) for some \(\alpha>0\) and \(k^{*}\) be uniformly chosen between iterations where \(\tau_{k}\leq T=r^{-1}mK^{\beta}\),_

\[\mathbb{E}[\|\nabla\psi_{1/\rho}(x^{k^{*}})\|^{2}]\leq\tfrac{2\rho}{\rho-2 \lambda-\kappa}\Big{[}\tfrac{\eta(\rho+4\lambda)D}{K}+\tfrac{\sqrt{\eta}D}{ \sqrt{K}\alpha}+\tfrac{2\sqrt{\eta}\rho L_{f}\alpha}{\sqrt{K}}+\tfrac{6\eta^{2 }\rho\lambda(L_{f}+L_{\omega})^{2}m^{2}\alpha^{2}}{rK^{1-\beta}}\Big{]}.\]

_where \(D=\psi_{1/\rho}(x^{1})-\inf_{x}\psi(x)\)._

_Remark 5_.: **Theorem 4** and 5 show that by employing a safeguarding step, both DSGD and DSPL can achieve delay-independent rates. It is also interesting to see how the choice of \(\beta\) and \(r\) affects performance. To recover a convergence rate of \(\mathcal{O}(K^{-1/2})\), it is sufficient to have \(\beta\leq 1/2\). If we set \(\beta>0\), then we skip \(rK^{-\beta}\) of all the iterations and incur a penalty of up to \(\eta^{3/2}\) in DSGD and \(\eta^{1/2}\) in DSPL. However, this loss becomes negligible for large \(K\), as \(\eta=1+\tfrac{r}{K^{\beta}-r}\to 1\). Alternatively, if we set \(\beta=0\), DSGD achieves \(\mathcal{O}(\sqrt{m}/\sqrt{K}+m/K)\) rate with \(\alpha=1/\sqrt{m}\) while DSPL yields a rate of \(\mathcal{O}(1/\sqrt{K}+m^{2}/K)\) with \(\alpha=1\). This setting aligns with [7; 18] and achieves optimal rate on \(m\). However, the penalty from \(\eta>1\) is non-negligible and adversely affects the overall convergence rate by a constant factor. Therefore, we can strike a balance in practice by choosing \(\beta\) in \((0,1/2]\) and allow delays of up to \(\mathcal{O}(K^{\beta})\).

## 6 Experiments

This section performs numerical experiments on the robust phase retrieval problem to demonstrate the efficiency of our methods. Given a measuring matrix \(A\in\mathbb{R}^{m\times n}\) and a set of observations \(b_{i}\approx|\langle a_{i},\hat{x}\rangle|^{2},1\leq i\leq m\) (\(m\) in this section represents the number of samples), robust phase retrieval aims to recover the true signal \(\hat{x}\) from

\[\min_{x\in\mathbb{R}^{n}}\frac{1}{m}\sum_{i=1}^{m}|\langle a_{i},x\rangle^{2} -b_{i}|+\delta_{\{x:\|x\|\leq M\}},\]

where \(\delta_{S}\) denotes the set indicator function and the \(\ell_{1}\) loss function improves robustness. Our experiment contains three parts. The first part profiles algorithms in an asynchronous environment simulated via MPI; our second experiment runs sequentially with simulated delays from common distributions; our last experiment also runs in the simulated environment and demonstrates the effectiveness of safeguarding step under adversarially chosen delays.

### Experiment setup

**Synthetic data**. For the synthetic data, we take \(m=300,n=100\) in the experiments of simulated delay and \(m=1500,n=500\) in the asynchronous environment. Data generation follows the setup of [11], where, given some \(\kappa\geq 1\), we compute \(A=QD,Q\in\mathbb{R}^{m\times n},q_{ij}\sim\mathcal{N}(0,1)\) and \(D=\text{diag}(d),d\in\mathbb{R}^{n},d_{i}\in[1/\kappa,1]\) for all \(i\). Then we generate a true signal \(\hat{x}\sim\mathcal{N}(0,I)\) and obtain the measurements \(b\) using formula \(b_{i}=\langle a_{i},\hat{x}\rangle^{2}\). Last we randomly choose \(p_{\text{fail}}\)-fraction of the measurements and add \(\mathcal{N}(0,25)\) to them to simulate data corruption.

**Real-life data**. The real-life data is generated from zipcode dataset, where we vectorize a 16\(\times\)16 hand-written digit from [16] and use it as the signal. The measuring matrix \(A\) comes from a normalized Hadamard matrix \(H\in\mathbb{R}^{256\times 256}\): we generate three diagonal matrices \(S_{j}=\text{diag}(s_{j}),j=1,2,3\); each element of \(s_{j}\in\mathbb{R}^{256}\) is taken from \(\{-1,1\}\) randomly and we let \(A=H[S_{1},S_{2},S_{3}]^{\mathsf{T}}\in\mathbb{R}^{768\times 256}\). Finally \(p_{\text{fail}}\)-fraction of the observations are set \(0\).

**1) Dataset**. In the asynchronous environment, we keep up with [34] setting \(\kappa=1,p_{\text{fail}}=0\) and in the simulated environment, we follow [11] setting \(\kappa\in\{1,10\}\) and \(p_{\text{fail}}\in\{0.2,0.3\}\).

2. **Initial point and radius**. Synthetic data: we generate \(x^{\prime}\sim\mathcal{N}(0,I_{n})\) and start from \(x^{1}=\frac{x^{\prime}}{\|x^{\prime}\|}\); zipcode data: we generate \(x^{\prime}\sim\mathcal{N}(\hat{x},I_{n})\) and take \(x^{1}=10x^{\prime}\). \(M=1000\|x^{1}\|\).
3. **Stopping criterion**. We run algorithms for 400 epochs (\(K=400m\)). In the asynchronous environment, algorithms run until reaching the maximum iteration. In the simulated environment, algorithms stop if \(f\leq 1.5f(\hat{x})\). When \(f\) contains corrupted measurements, \(f(\hat{x})>0\).
4. **Stepsize**. We set \(\gamma=\sqrt{K/\alpha}\), where \(\alpha\in\{0.1,0.5,1.0\}\) in the asynchronous environment, \(\alpha\in[10^{-2},10^{1}]\) for synthetic data and \(\alpha\in[10^{1},10^{2}]\) for the zipcode dataset.
5. **Simulated delay**. In the simulated environment, we generate \(\tau_{k}\) from two common distributions from literature, which are geometric \(\mathcal{G}(p)\) and Poisson \(\mathcal{P}(\lambda)\)[37]. After the delay is generated, it is truncated by twice the mean of the distribution.
6. **Adversarial delay**. We let delay happen at the last iteration of each epoch and use the information of \(x^{1}\) to update. Safeguarding parameter is set to \(T=0.1\sqrt{K}\).
7. **Trade-off between computation and communication**. In the asynchronous environment, the numerical linear algebra on the worker uses a raw implementation (not importing package) to balance the cost of gradient computation and communication.

### Asynchronous environment

Our first experiment runs in an asynchronous environment implemented by MPI Python interface and is profiled on an Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz machine with 10 cores and 20 threads. This experiment runs on a single machine to verify our theoretical finding rather than to test the algorithm's real performance on a specific distributed architecture.

The first figure plots the wall-clock time (in seconds) for DSGD and DSPL to complete 400 epochs when the number of workers increases. It is observed that both algorithms exhibit speed-up with more workers and note that DSPL takes more time than DSGD due to the need to pass the function value to the master and slightly more expensive updates. But as the second and the third figure suggest, this extra cost is justified by the superior convergence: in the first several epochs DSPL reaches a high accuracy of \(10^{-6}\) in both function value and distance to the optimal solution, while DSGD stagnates at a relatively low-accuracy solution of \(10^{-2}\). These observations suggest that DSPL offers better convergence behavior than the methods only based on subgradient. Finally, our experiments suggest both DSGD and DSPL are not sensitive to the increase in the number of workers when there are relatively few workers.

### Simulated environment

The second part of our experiment compares the performance between DSGD and DSPL and is based on the simulated delay, where the algorithm runs sequentially but the gradient information is computed from the previous iterates.

Figure 3 plots the number of iterations for each algorithm to converge under different datasets and delay parameters. We see that in spite of delays, DSPL admits a wider range of stepsize parameters ensuring convergence than DSGD, and the performance slightly deteriorates as delay increases.

Figure 2: First: speedup in time and the number of workers. Second: progress of \(\|x^{k}-\hat{x}\|\) in the first 40 epochs given \(\alpha=0.1\). Third: \(f(x^{k})-f(\hat{x})\) in the first 40 epochs given \(\alpha=0.5\). For more details about the two figures on the right, please refer to Figure 12 in the appendix.

### Adversarial delay

The final experiment verifies the efficacy of our safeguarding step when delays are introduced in an adversarial setting. We evaluate the performance of DSGD, DSPL, both with and without the safeguarding step, under the delay patterns we have generated.

Figure 4 clearly illustrates the superior performance of our method incorporating the safeguarding step. As suggested by our theory, DSGD is notably sensitive to delays of \(\mathcal{O}(K)\). However, upon discarding outdated information, DSGD exhibits much greater robustness. Interestingly, even under our adversarial setup, the performance of DSPL remains acceptable, albeit slightly inferior to its safeguarded version, which aligns well with our theoretical findings.

## 7 Conclusions

We offer a sharp analysis of delayed stochastic algorithms for weakly convex optimization, discussing the widely utilized DSGD method and introducing the novel DSPL method for problems with a composition structure. Through careful examination of delay factors, we propose a straightforward safeguarding approach to eliminate the effect of delays, and instead derive bounds depending on the number of agents in the distributed environment. This makes our algorithms resilient to arbitrary delays. A promising future direction is the application of these prox-linear methods in more diverse distributed settings, such as decentralized networks.

## 8 Acknowledgement

The authors thank the anonymous reviewers for their constructive suggestions. This research is partially supported by National Natural Science Foundation of China (NSFC-72150001, 11831002).

## References

* [1] Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 24. Curran Associates, Inc., 2011.

Figure 4: From left to right: zipcode data, \(p_{\text{fail}}=0.2,\alpha\in\{10,100\}\), \(p_{\text{fail}}=0.3,\alpha\in\{10,100\}\). x-axis: iteration count; y-axis: \(f(x^{k})\).

* [2] Yossi Arjevani, Ohad Shamir, and Nathan Srebro. A tight convergence analysis for stochastic gradient descent with delayed updates. In _Algorithmic Learning Theory_, pages 111-132. PMLR, 2020.
* [3] Karl Backstrom, Marina Papatriantafilou, and Philippas Tsigas. Mindthestep-asyncpsgd: Adaptive asynchronous parallel stochastic gradient descent. In _2019 IEEE International Conference on Big Data (Big Data)_, pages 16-25. IEEE, 2019.
* [4] Dimitri Bertsekas and John Tsitsiklis. _Parallel and distributed computation: numerical methods_. Athena Scientific, 2015.
* [5] Vasileios Charisopoulos, Yudong Chen, Damek Davis, Mateo Diaz, Lijun Ding, and Dmitriy Drusvyatskiy. Low-rank matrix recovery with composite optimization: good conditioning and rapid convergence. _Foundations of Computational Mathematics_, 21(6):1505-1593, 2021.
* [6] Shixiang Chen, Alfredo Garcia, and Shahin Shahrampour. On distributed non-convex optimization: Projected subgradient method for weakly convex problems in networks. _IEEE Transactions on Automatic Control_, 2021.
* [7] Alon Cohen, Amit Daniely, Yoel Drori, Tomer Koren, and Mariano Schain. Asynchronous stochastic optimization robust to arbitrary delays. _Advances in Neural Information Processing Systems_, 34, 2021.
* [8] Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. _SIAM Journal on Optimization_, 29(1):207-239, 2019.
* [9] Damek Davis, Dmitriy Drusvyatskiy, and Kellie J MacPhee. Stochastic model-based minimization under high-order growth. _arXiv preprint arXiv:1807.00255_, 2018.
* [10] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc'aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. _Advances in neural information processing systems_, 25, 2012.
* [11] Qi Deng and Wenzhi Gao. Minibatch and momentum model-based methods for stochastic weakly convex optimization. _Advances in Neural Information Processing Systems_, 34, 2021.
* [12] Dmitriy Drusvyatskiy and Courtney Paquette. Efficiency of minimizing compositions of convex functions and smooth maps. _Mathematical Programming_, pages 1-56, 2018.
* [13] John C Duchi and Feng Ruan. Stochastic methods for composite and weakly convex optimization problems. _SIAM Journal on Optimization_, 28(4):3229-3259, 2018.
* [14] John C Duchi and Feng Ruan. Solving (most) of a set of quadratic equalities: Composite optimization for robust phase retrieval. _Information and Inference: A Journal of the IMA_, 8(3):471-529, 2019.
* [15] R Fletcher. A model algorithm for composite nondifferentiable optimization problems. In _Nondifferential and Variational Techniques in Optimization_, pages 67-76. Springer, 1982.
* [16] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. _The elements of statistical learning: data mining, inference, and prediction_, volume 2. Springer, 2009.
* [17] Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian Stich, and Martin Jaggi. Error feedback fixes signsgd and other gradient compression schemes. In _International Conference on Machine Learning_, pages 3252-3261. PMLR, 2019.
* [18] Anastasiia Koloskova, Sebastian U Stich, and Martin Jaggi. Sharper convergence guarantees for asynchronous sgd for distributed and federated learning. _Advances in Neural Information Processing Systems_, 35:17202-17215, 2022.
* [19] Adrian S Lewis and Stephen J Wright. A proximal method for composite minimization. _Mathematical Programming_, 158(1):501-546, 2016.
* [20] Xiao Li, Zhihui Zhu, Anthony Man-Cho So, and Jason D Lee. Incremental methods for weakly convex optimization. _arXiv preprint arXiv:1907.11687_, 2019.

* [21] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. Asynchronous parallel stochastic gradient for nonconvex optimization. _Advances in Neural Information Processing Systems_, 28, 2015.
* [22] Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent. _Advances in Neural Information Processing Systems_, 30, 2017.
* [23] Xiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu. Asynchronous decentralized parallel stochastic gradient descent. In _International Conference on Machine Learning_, pages 3043-3052. PMLR, 2018.
* [24] Vien Mai and Mikael Johansson. Convergence of a stochastic gradient method with momentum for non-smooth non-convex optimization. In _International Conference on Machine Learning_, pages 6630-6639. PMLR, 2020.
* [25] Brendan McMahan and Matthew Streeter. Delay-tolerant algorithms for asynchronous distributed online learning. _Advances in Neural Information Processing Systems_, 27, 2014.
* [26] Konstantin Mishchenko, Francis Bach, Mathieu Even, and Blake Woodworth. Asynchronous SGD beats minibatch SGD under arbitrary delays. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [27] Angelia Nedic, Dimitri P Bertsekas, and Vivek S Borkar. Distributed asynchronous incremental subgradient methods. _Studies in Computational Mathematics_, 8(C):381-407, 2001.
* [28] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on Optimization_, 19(4):1574-1609, 2009.
* [29] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. Hogwild!: A lock-free approach to parallelizing stochastic gradient descent. volume 24, 2011.
* [30] Herbert Robbins and Sutton Monro. A stochastic approximation method. _The Annals of Mathematical Statistics_, pages 400-407, 1951.
* [31] Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Stochastic convex optimization. In _COLT_, volume 2, page 5, 2009.
* [32] Suvrit Sra, Adams Wei Yu, Mu Li, and Alex Smola. Adadelay: Delay adaptive distributed stochastic optimization. In _Artificial Intelligence and Statistics_, pages 957-965. PMLR, 2016.
* [33] Sebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for sgd with delayed gradients and compressed updates. _Journal of Machine Learning Research_, 21:1-36, 2020.
* [34] Yangyang Xu, Yibo Xu, Yonggui Yan, and Jie Chen. Distributed stochastic inertial-accelerated methods with delayed derivatives for nonconvex problems. _SIAM Journal on Imaging Sciences_, 15(2):550-590, 2022.
* [35] Jinshan Zeng and Wotao Yin. On nonconvex decentralized gradient descent. _IEEE Transactions on signal processing_, 66(11):2834-2848, 2018.
* [36] Junyu Zhang and Lin Xiao. Stochastic variance-reduced prox-linear algorithms for nonconvex composite optimization. _Mathematical Programming_, pages 1-43, 2021.
* [37] Wei Zhang, Suyog Gupta, Xiangru Lian, and Ji Liu. Staleness-aware async-sgd for distributed deep learning. _arXiv preprint arXiv:1511.05950_, 2015.
* [38] Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and Tie-Yan Liu. Asynchronous stochastic gradient descent with delay compensation. In _International Conference on Machine Learning_, pages 4120-4129. PMLR, 2017.
* [39] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex Smola. Parallelized stochastic gradient descent. _Advances in neural information processing systems_, 23, 2010.

## Appendix

Table of Contents

* A More on the related works
* B Bregman context and auxiliary results
* B.1 Context setup
* B.2 Auxiliary lemmas
* C Convergence Analysis of DSGD
* C.1 Proof of Lemma 1
* C.2 Proof of Theorem 1
* D Convergence analysis of DSPL
* D.1 Preliminaries and analysis of Bregman DSPL
* D.2 Proof of Lemma 2 and Theorem 2
* D.3 Proof of Theorem 3
* E Delay-independent analysis
* E.1 Proof of Lemma 3
* E.2 Proof of Lemma 4
* E.3 Proof of Theorem 4
* E.4 Proof of Theorem 5
* F DSPL with momentum
* F.1 Preliminaries of DSEPL
* F.2 Convergence analysis of DSEPL
* G Kernel and subproblems
* G.1 Choosing the divergence kernel
* G.2 Bregman proximal subproblem
* G.3 Euclidean subproblem with nonsmooth regularizer
* H Additional experiments
* H.1 Robust phase retrieval
* H.2 Blind deconvolution problem
* H.3 Proximal sub-problems
* H.4 Separate figures of Section 6

Structure of the appendixIn this paper, we strive to provide a comprehensive treatment of delayed stochastic algorithms for stochastic weakly convex optimization in both theoretical and practical aspects.

**1) Theoretically**, we choose to employ a more general Bregman divergence context in our proof. This broader context covers the primary results for the Euclidean setting discussed in the main section. Specifically, this context will be invoked in our analysis for DSPL method to accommodate real-life applications of prox-linear method and by relaxing the Lipschitz condition **C1**.

**2) Practically**, we provide a detailed discussion on how to implement the DSPL algorithm, both in Euclidean and Bregman context. We specifically address the selection of the Bregman divergence kernel and propose efficient subroutines to solve proximal subproblems. Additionally, in view of the wide usage of momentum techniques in stochastic optimization, we analyze DSEPL, the momentum variant of DSPL. While this is not the central contribution of our paper, we believe it represents an important step for making DSPL applicable to real-world problems.

The appendix is organized as follows.

The first five sections address the theoretical aspects of delayed stochastic algorithms.

In Section A, we first conduct an extended review of other related works. In Section B, we introduce the Bregman context with some auxiliary results. In Section C, we perform convergence analysis for the DSGD method, and Section D presents a more general Bregman DSPL that relaxes the Lipschitzness assumption. Section E presents the safeguarded variant of our algorithms that are robust to arbitrary delays.

**(Optional)** The last three sections are devoted to more practical aspects.

In Section F, we present the convergence analysis of DSEPL, namely, the momentum variant of DSPL using variable extrapolation. Section G discusses how to construct the kernel and solve the Bregman proximal subproblems. Finally, Section H displays additional experiment results.

## Appendix A More on the related works

First, we review the literature on stochastic weakly-convex optimization. The Prox-Linear (PL) method [19; 12; 15] has received more attention lately. The seminal work [8] conducts a novel complexity analysis using the Moreau envelope as the potential function. They show that SGD (and many other stochastic algorithms) achieves an \(\mathcal{O}(\frac{1}{\varepsilon^{2}})\) complexity in terms of convergence to the proximity of approximate stationary points. [20] analyzes an incremental subgradient method for finite-sum problems. [36] extends the prox-linear algorithm to the finite-sum setting and provides analysis based on variance-reduction. SGD with momentum is studied in [24] and [11] reveals that SPL can be accelerated by minibatching even for nonsmooth objective functions. [11] also employs heavy-ball momentum to further improve the model-based optimization.

There is substantial literature on distributed and asynchronous optimization. We refer to the seminal work [4]. In addition to the aforementioned asynchronous issues, another important research direction concerns reducing the network communication cost by gradient compression. For example, see [17]. Besides the centralized setting, there is also growing interest in decentralized algorithms. Recently, [6] presents a decentralized SGD in the network where nodes exchange parameters and sub-gradients locally. However, their method requires all the nodes to update synchronously and only

Figure 5: Structure of the appendixproves asymptotic convergence of SGD. An extensive study of decentralized optimization is beyond our work. Therefore, we refer interested readers to [22, 23, 35] for more recent advances.

## Appendix B Bregman context and auxiliary results

We initiate the theoretical discussion by introducing the Bregman context, which includes the concepts of Bregman divergence, relative Lipschitzian property, and Bregman Moreau envelope. Following the provision of requisite definitions, we present some auxiliary results that will be used frequently in our analysis.

### Context setup

Bregman divergence and relative LipschitznessGiven a Legendre function \(d(\cdot)\)[7] that is proper, closed, strictly convex and essentially smooth, it induces the Bregman divergence defined by

\[V_{d}(x,y):=d(x)-d(y)-\langle\nabla d(y),x-y\rangle\]

and \(d\) is called the kernel of \(V_{d}\). Since \(d\) is strictly convex, we know \(V_{d}(x,y)\geq 0\) and the equality holds if and only if \(x=y\). The notion of Bregman divergence greatly enhances the coverage of the algorithms and has recently been adopted in the context of weakly convex optimization [8, 3] to tackle a broader class of nonsmooth nonconvex optimization problems. In this paper, we mainly leverage the concept of relative Lipschitzian property to extend the analysis of delayed stochastic algorithms. First, we give its formal definition.

**Definition 1** (Relative Lipschitzian property).: _[_3_]_ _A function \(f\) satisfies \(L\)-relative Lipschitzian property to kernel \(d\) if for all \(x,y\in\operatorname{dom}d\),_

\[f(x)-f(y)\leq L\sqrt{2V_{d}(y,x)}.\] (5)

#### Bregman proximal mapping and Bregman envelope

As in conventional weakly convex optimization, we need an approximate measure of stationarity in the non-Euclidean setting. A naturally choice is the Bregman envelope [3]. Assume that \(d\) is 1-strongly convex, then for any \(\rho>0\), the Bregman envelope of \(f\) with respect to kernel \(d\) is

\[\psi_{1/\rho}^{d}(x)\coloneqq\min_{y}\{f(y)+\rho V_{d}(y,x)\}\]

and the Bregman proximal mapping is represented using

\[\operatorname{prox}_{\psi/\rho}^{d}(x)\coloneqq\operatorname*{arg\,min}_{y}\{f (y)+\rho V_{d}(y,x)\}.\]

We sometimes refer to \(\operatorname{prox}_{\psi/\rho}^{d}(x)\) as \(\hat{x}\) and proximal distance \(\mathbb{E}[V_{d}(\hat{x}^{k},x^{k})]\) has been proven a proper measure of the convergence of stochastic algorithms [3] under specific choice of \(d\) and \(\rho\).

Connection with Euclidean geometryOne advantage of using Bregman context is that it subsumes the Euclidean case. If we take \(d(x)=\frac{1}{2}\|x\|^{2}\), then \(V_{d}(x,y)=\frac{1}{2}\|x-y\|^{2}\) and relative Lipschitzian property becomes standard Lipschitz condition. Particularly we have the following relation between \(V_{d}(\hat{x},x)\) and Moreau envelope.

\[V_{d}(\hat{x}^{k},x^{k})=\frac{1}{2}\|\hat{x}^{k}-x^{k}\|^{2}=\frac{1}{2\rho^ {2}}\|\nabla\psi_{1/\rho}^{d}(x)\|^{2}.\]

When the context is clear, we will use \(\psi=\psi^{d}\) directly if \(d(x)=\frac{1}{2}\|x\|^{2}\). In the next section, we present several useful auxiliary results that appear frequently in the proof.

### Auxiliary lemmas

In this section, we present all the auxiliary lemmas that will be used in the proof of our main results. While some of these lemmas are widely recognized in the field, we reproduce them here to ensure that our work remains self-contained. To begin, we introduce the well-known three-point lemma.

**Lemma 5** (Three point lemma).: _Let \(f\) be a closed convex function and define_

\[z=\operatorname*{arg\,min}_{x}\{f(x)+\gamma V_{d}(x,y)\}.\]

_for some \(y\in\operatorname{dom}d\). Then we have_

\[f(z)+\gamma V_{d}(z,y)\leq f(x)+\gamma V_{d}(x,y)-\gamma V_{d}(x,z),\forall x \in\operatorname{dom}d.\]

The following lemma bounds, for a deterministic function, the proximal step by the regularization term and relative Lipschitzian property.

**Lemma 6** (Bounding the proximal step).: _Given the divergence \(V_{d}\) generated by some 1-strongly convex function \(d\). Let the function \(f(x)\) satisfy the relative Lipschitzian property (5). For any \(\gamma>0\) such that \(f+\gamma d\) is strongly convex, if we define_

\[x^{+}=\operatorname*{arg\,min}_{y}\{f(y)+\gamma V_{d}(y,x)\},\]

_then_

\[\sqrt{V_{d}(x^{+},x)}\leq\sqrt{2}\gamma^{-1}L\] (6)

_and_

\[\|x^{+}-x\|\leq 2\gamma^{-1}L.\] (7)

Proof.: By the optimality of \(x^{+}\) and relative Lipschitzian property, we have

\[f(x^{+})+\gamma V_{d}(x^{+},x)\leq f(x)\]

and

\[\gamma V_{d}(x^{+},x)\leq f(x)-f(x^{+})\leq L\sqrt{2V_{d}(x^{+},x)}.\] (8)

Dividing both sides of (8) by \(\sqrt{V_{d}(x^{+},x)}\) gives (9); (10) uses \(V_{d}(x^{+},x)\geq\frac{1}{2}\|x^{+}-x\|^{2}\). 

**Lemma 7** (Bounding the proximal step in expectation).: _Given the divergence \(V_{d}\) generated by some 1-strongly convex function \(d\). Let convex stochastic function \(f(x,\xi)\) satisfy the relative Lipschitzian property (5) with \(L=L(\xi)\) for \(\xi\sim\Xi\). For any \(\gamma>0\), if we define_

\[x^{+}=\operatorname*{arg\,min}_{y}\{f(y,\xi)+\gamma V_{d}(y,x)\},\]

_then_

\[\tfrac{1}{\sqrt{2}}\mathbb{E}_{\xi\sim\Xi}[\|x^{+}-x\|]\leq\mathbb{E}_{\xi \sim\Xi}[\sqrt{V_{d}(x^{+},x)}]\leq\sqrt{2}\gamma^{-1}\mathbb{E}_{\xi\sim\Xi} [L(\xi)]\] (9)

_and_

\[\mathbb{E}_{\xi\sim\Xi}[\|x^{+}-x\|^{2}]\leq 4\gamma^{-2}\mathbb{E}_{\xi\sim\Xi} [L(\xi)^{2}].\] (10)

Proof.: We can apply **Lemma 6**. Then we have, for each \(\xi\sim\Xi\), that

\[\sqrt{V_{d}(x^{+},x)}\leq\sqrt{2}\gamma^{-1}L(\xi)\quad\text{and}\quad V_{d}(x ^{+},x)\leq 2\gamma^{-2}L^{2}(\xi).\]

Taking expectation with respect to \(\xi\), the fact that \(V_{d}(x^{+},x)\geq\frac{1}{2}\|x^{+}-x\|^{2}\) completes the proof 

## Appendix C Convergence Analysis of \(\mathtt{DSGD}\)

In this section, we present the convergence analysis of \(\mathtt{DSGD}\). The proof follows a standard inexact potential reduction scheme and is done in the Euclidean setup, where **Lemma 5** and **Lemma 7** hold for \(d=\frac{1}{2}\|\cdot\|^{2}\).

[MISSING_PAGE_FAIL:17]

Finally, we measure the reduction in potential function \(\psi_{1/\rho}(x^{k})\) and successively deduce that

\[\mathbb{E}_{k}[\psi_{1/\rho}(x^{k+1})]\] \[=\mathbb{E}_{k}[f(\hat{x}^{k+1})+\omega(\hat{x}^{k+1})+\frac{\rho}{ 2}\|\hat{x}^{k+1}-x^{k+1}\|^{2}]\] \[\leq\mathbb{E}_{k}[f(\hat{x}^{k})+\omega(\hat{x}^{k})+\frac{\rho}{ 2}\|\hat{x}^{k}-x^{k+1}\|^{2}]\] \[\leq\mathbb{E}_{k}[f(\hat{x}^{k})+\omega(\hat{x}^{k})+\frac{\rho}{ 2}\|\hat{x}^{k}-x^{k}\|^{2}]-\frac{\rho(\rho-2\lambda-\kappa)}{2(\gamma_{k}-2 \lambda-\kappa)}\|\hat{x}^{k}-x^{k}\|^{2}\] \[\quad+\frac{\rho\lambda}{\gamma_{k}-2\lambda-\kappa}\mathbb{E}_{ k}[\|x^{k+1}-x^{-\tau_{k}}\|^{2}]+\frac{2\rho L_{f}}{\gamma_{k}-2\lambda-\kappa} \mathbb{E}_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|]\] (16) \[\quad-\frac{\rho(\gamma_{k}-\rho)}{2(\gamma_{k}-2\lambda-\kappa )}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]\] \[=\psi_{1/\rho}(x^{k})-\frac{\rho(\rho-2\lambda-\kappa)}{2(\gamma _{k}-2\lambda-\kappa)}\|\hat{x}^{k}-x^{k}\|^{2}+\frac{\rho\lambda}{\gamma_{k}- 2\lambda-\kappa}\mathbb{E}_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]\] \[\quad+\frac{2\rho L_{f}}{\gamma_{k}-2\lambda-\kappa}\mathbb{E}_{ k}[\|x^{k+1}-x^{k-\tau_{k}}\|]-\frac{\rho(\gamma_{k}-\rho)}{2(\gamma_{k}-2 \lambda-\kappa)}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}].\]

where relation (16) plugs (15) in. A simple re-arrangement completes the proof.

### Proof of Theorem 1

By the descent property revealed in **Lemma 1**, we multiply both sides of the inequality by \(\gamma_{k}-2\lambda-\kappa\), telescope over \(k=1,\ldots,K\) and deduce that

\[\frac{\rho(\rho-2\lambda-\kappa)}{2}\mathbb{E}[\|\hat{x}^{k^{*}}- x^{k^{*}}\|^{2}]\] \[=\frac{\rho(\rho-2\lambda-\kappa)}{2K}\sum_{k=1}^{K}\mathbb{E}[\| \hat{x}^{k}-x^{k}\|^{2}]\] \[\leq\frac{\gamma-2\lambda-\kappa}{K}\{\psi_{1/\rho}(x^{1})- \mathbb{E}[\psi_{1/\rho}(x^{K+1})]\}-\frac{\rho(\gamma-\rho)}{2K}\sum_{k=1}^{ K}\mathbb{E}[\|x^{k+1}-x^{k}\|^{2}]\] \[\quad+\frac{\rho}{K}\sum_{k=1}^{K}\mathbb{E}[\lambda\|x^{k+1}-x^{ k-\tau_{k}}\|^{2}+2L_{f}\|x^{k+1}-x^{k-\tau_{k}}\|]\] \[\leq\frac{(\gamma-2\lambda-\kappa)D}{K}-\frac{\rho(\gamma-\rho) }{2K}\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}-x^{k}\|^{2}]\] \[\quad+\frac{\rho}{K}\sum_{k=1}^{K}\mathbb{E}[-\frac{\gamma-\rho} {2}\|x^{k+1}-x^{k}\|^{2}+\lambda\|x^{k+1}-x^{k-\tau_{k}}\|^{2}+2L_{f}\|x^{k+1} -x^{k-\tau_{k}}\|],\] (17)

where (17) is due to \(\psi(x^{*})\leq\mathbb{E}_{k}[\psi_{1/\rho}(x^{K+1})]\). Now it remains to bound the error from the stochastic delays. Let's first consider applying \(\|a+b\|^{2}\leq 2\|a\|^{2}+2\|b\|^{2}\) to get

\[\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]\leq 2\sum_{k=1}^{K} \mathbb{E}[\|x^{k}-x^{k-\tau_{k}}\|^{2}]+2\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}-x^ {k}\|^{2}]\] (18)and we can bound the first term using

\[\sum_{k=1}^{K}\mathbb{E}[\|x^{k}-x^{k-\tau_{k}}\|^{2}] =\,\sum_{k=1}^{K}\mathbb{E}\Big{[}\Big{\|}\sum_{l=1}^{\tau_{k}}x^{k +1-l}-x^{k-l}\Big{\|}^{2}\Big{]}\] \[\leq\,\sum_{k=1}^{K}\tau_{k}\sum_{l=1}^{\tau_{k}}\mathbb{E}[\|x^{ k+1-l}-x^{k-l}\|^{2}]\] (19) \[\leq\frac{4(L_{f}+L_{\omega})^{2}}{\gamma^{2}}\sum_{k=1}^{K}\tau_ {k}^{2},\] (20)

where (19) uses the fact \(\|\sum_{i=1}^{k}a_{i}\|^{2}\leq k\sum_{i=1}^{k}\|a_{i}\|^{2}\), (20) invokes **Lemma 7** since \(\langle g,x\rangle+\omega\) is \((\|g\|+L_{\omega})\) Lipschitz continuous with \(\mathbb{E}_{\xi}[\|g\|^{2}]\leq L_{f}^{2}\), and we let \(x^{k-j}=x^{1}\) if \(k\geq j\). On the other hand, we bound the first-order terms by

\[\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}-x^{k-\tau_{k}}\|] \leq\,\sum_{k=1}^{K}\sum_{l=0}^{\tau_{k}}\mathbb{E}[\|x^{k-l+1}-x^ {k-l}\|]\] \[\leq\,\sum_{k=1}^{K}2\gamma^{-1}(L_{f}+L_{\omega})(\tau_{k}+1)\] (21) \[=2\gamma^{-1}(L_{f}+L_{\omega})\big{(}K+\sum_{k=1}^{K}\tau_{k} \big{)},\]

where (21) again invokes **Lemma 7**. Plugging the bound back, we deduce that

\[\frac{\rho(\rho-2\lambda-\kappa)}{2}\mathbb{E}[\|\hat{x}^{k^{*}}- x^{k^{*}}\|^{2}]\] \[\leq\,\frac{(\gamma-2\lambda-\kappa)D}{K}+\frac{\rho}{K}\sum_{k=1 }^{K}\mathbb{E}[-\frac{\gamma-\rho}{2}\|x^{k+1}-x^{k}\|^{2}+\lambda\|x^{k+1}- x^{k-\tau_{k}}\|^{2}+2L_{f}\|x^{k+1}-x^{k-\tau_{k}}\|]\] \[\leq\,\frac{(\gamma-2\lambda-\kappa)D}{K}+\frac{4\rho L_{f}(L_{f} +L_{\omega})}{\gamma K}\sum_{k=1}^{K}(\tau_{k}+1)+\frac{8\rho\lambda(L_{f}+L_{ \omega})^{2}}{\gamma^{2}K}\sum_{k=1}^{K}\tau_{k}^{2}\] (22) \[\leq\,\frac{(\rho+2\lambda)D}{K}+\frac{D}{\sqrt{K}\alpha}+\frac{ 4\rho L_{f}(L_{f}+L_{\omega})\alpha}{\sqrt{K}}\Big{(}\frac{1}{K}\sum_{k=1}^{K} \tau_{k}+1\Big{)}+\frac{8\rho\lambda(L_{f}+L_{\omega})^{2}\alpha^{2}}{K}\Big{(} \frac{1}{K}\sum_{k=1}^{K}\tau_{k}^{2}\Big{)}\]

where (22) is by \(\gamma=\sqrt{K}/\alpha+\rho+4\lambda+\kappa\) and we cancel the first summation term in (18) with \(-\sum_{k=1}^{K}\frac{\gamma-\rho}{2}\|x^{k+1}-x^{k}\|^{2}\) since \(\gamma\geq\rho+4\lambda\). The proof is complete after dividing both sides by \(\rho(\rho-2\lambda-\kappa)\) and using the fact that \(\frac{1}{2}\|\hat{x}^{k}-x^{k}\|^{2}=\frac{1}{2\rho^{2}}\|\nabla\psi_{1/\rho} (x)\|^{2}\).

## Appendix D Convergence analysis of \(\mathtt{DSPL}\)

In this section, we introduce a more general \(\mathtt{DSPL}\) associated with Bregman divergence and establish its convergence analysis. After proving the result in our Bregman context, the Euclidean results in Section 4 follows immediately as a special case.

Why use Bregman divergence?Recall that in **C1** we assume Lipschitz continuity of both \(c\) and \(\nabla c\), which does not hold without assuming a bounded set. The main issue here is to expect a Lipschitz smooth function to also be Lipschitz continuous. Using the Bregman context, we can relax such Lipschitz continuity to relative Lipschitzian property, so that we do not necessarily need the bounded set assumption.

### Preliminaries and analysis of Bregman \(\mathtt{DSPL}\)

In this section, we present the convergence results of \(\mathtt{DSPL}\). As we mentioned in **Remark 4**, we replace \(\frac{1}{2}\|\cdot\|^{2}\) regularization by divergence \(V_{d}\) and summarize the update in **Algorithm 4**.

Now we overload the assumptions used in Section 4.

1. (i.i.d. sample) It is possible to draw i.i.d. samples \(\{\xi^{k}\}\) from \(\Xi\).
2. (Relative Lipschitzian property) \(\omega(x)\) satisfies relatively Lipschitzian property (5) \[\omega(x)-\omega(y)\leq L_{\omega}\sqrt{2V_{d}(y,x)}\]
3. (Weak convexity) \(\omega(x)\) is \(\kappa\)-weakly convex.
4. (Strongly-convex kernel) Kernel function \(d\) is 1-strongly convex.
5. (Bounded moment) The distribution of the independent stochastic delays \(\{\tau_{k}\}\) has bounded first and second moments. i.e., \(\mathbb{E}[\tau_{k}]\leq\bar{\tau}<\infty,\mathbb{E}[\tau_{k}^{2}]\leq\tau_{ \sigma^{2}}<\infty\), for all \(k\).
6. \(h(x)\) is convex and \(L_{h}\)-Lipschitz continuous; \(c(x,\xi)\) has \(C(\xi)\)-Lipschitz continuous gradient, for all \(\xi\sim\Xi\). Moreover, during the algorithm the stochastic function \(f_{z}(x,\xi),z\in\operatorname*{dom}d\) is \(L_{f}(\xi)\)-relative Lipschitzian to \(d\) for all \(\xi\sim\Xi\). Moreover, we assume \(\mathbb{E}_{\xi}[C(\xi)^{2}]\leq C^{2},\mathbb{E}_{\xi}[L_{c}(\xi)^{2}]\leq L_ {c}^{2}\).

_Remark 6_.: It can be seen that we added extra assumptions on the kernel and relaxed the Lipschitz continuity of \(c\) in \(\mathbf{C1^{\prime}}\) and \(\omega\) in \(\mathbf{A2^{\prime}}\). Specially if \(c\) is \(L_{c}(\xi)\)-Lipschitz continuous, we have \(f_{z}(x,\xi)\) is \(L_{h}L_{c}(\xi)\)-Lipschitz continuous since

\[f_{z}(x,\xi)-f_{z}(y,\xi)\] \[=h(c(z,\xi)+\langle\nabla c(z,\xi),x-z\rangle)-h(c(z,\xi)+ \langle\nabla c(z,\xi),y-z\rangle)\] \[\leq L_{h}|\langle\nabla c(z,\xi),x-y\rangle|\] \[\leq L_{h}L_{c}(\xi)\|x-y\|\]

With the above assumptions, we can further derive a more general version of **Proposition 1**.

**Proposition 2** (**Proposition 1** in the Bregman setting).: _Let \(\{f_{z}(\cdot,\xi)\}\) be the sequence of stochastic functions queried during the DSPL algorithm, then_

1. _[label=_P0_]_
2. _(Convexity)_ \(f_{z}(x,\xi)\) _is convex, for all_ \(x\in\operatorname*{dom}d\) _and_ \(\xi\sim\Xi\)_._
3. _(Two-sided approximation)_ \(|f_{z}(x,\xi)-f(x,\xi)|\leq\frac{L_{h}C(\xi)}{2}\|x-z\|^{2}\)_, for all_ \(x\in\operatorname*{dom}d\) _and all_ \(\xi\sim\Xi\)_._
4. _(Relative Lipschitzian property)_ \(f_{z}(x,\xi)-f_{z}(y,\xi)\leq L_{f}(\xi)\sqrt{2V_{d}(y,x)}\) _for all_ \(x,y\in\operatorname*{dom}d\) _and all_ \(\xi\sim\Xi\)_._

As we did in Section 4, we take \(\lambda=L_{h}C\) and use these constants to present the results. Our analysis adopts the conventional potential reduction framework but a more careful treatment of stochastic noise is needed to improve the convergence result. The next lemma bounds the stochastic noise of the algorithm and is key to our analysis of DSPL.

**Lemma 8** (Stability of the stochastic iteration).: _Assume that the assumptions \(\mathbf{A1^{\prime}}\) to \(\mathbf{A4^{\prime}}\) hold. If \(f_{x}(\cdot,\xi)\) is convex and satisfies **Proposition 2**, then the proximal iteration_

\[x^{k+1}=\operatorname*{arg\,min}_{x}\{f_{x^{k-\tau_{k}}}(x,\xi^{k-\tau_{k}})+ \omega(x)+\gamma_{k}V_{d}(x,z^{k})\}\]_satisfies_

\[\left|\mathbb{E}_{k}\left\{\mathbb{E}_{\xi}\left[f_{x^{k-\tau_{k}}}(x^{k+1},\xi) \right]-f_{x^{k-\tau_{k}}}(x^{k+1},\xi^{k-\tau_{k}})\right\}\right|\leq\frac{2L_ {f}^{2}}{\gamma-\kappa},\]

_for any \(\gamma_{k}>\kappa\)._

Now we can get the lemmas that match our main results in a Bregman context.

**Lemma 9** (**Lemma 2** in the Bregman setting).: _Suppose \(\mathbf{A1^{\prime}}\),\(\mathbf{A2^{\prime}}\),\(\mathbf{A3^{\prime}}\),\(\mathbf{A4^{\prime}}\) and \(\mathbf{C1^{\prime}}\) hold, if \(\rho>2\lambda+\kappa,\gamma_{k}\geq\rho\), then_

\[\frac{\rho(\rho-2\lambda-\kappa)}{\gamma_{k}-2\lambda-\kappa}V_{d }(\hat{x}^{k},x^{k}) \leq\psi_{1/\rho}^{d}(x^{k})-\mathbb{E}_{k}[\psi_{1/\rho}^{d}(x^ {k+1})]-\frac{\rho(\gamma_{k}-\rho)}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E} _{k}[\|x^{k+1}-x^{k}\|^{2}]\] \[\quad+\frac{3\rho\lambda}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E }_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]+\frac{2\rho L_{f}^{2}}{(\gamma_{k}-\kappa )(\gamma_{k}-2\lambda-\kappa)}\]

**Theorem 6** (**Theorem 2** in the Bregman setting).: _Under the same conditions as **Lemma 9**, as well as \(\mathbf{A5^{\prime}}\), taking \(\gamma_{k}\equiv\gamma=\rho+6\lambda+\kappa+\sqrt{K}/\alpha\) for some \(\alpha>0\), letting \(k^{*}\) be uniformly chosen between 1 and \(K\), then_

\[\mathbb{E}[V_{d}(\hat{x}^{k^{*}},x^{k^{*}})]\leq\frac{1}{\rho(\rho-2\lambda- \kappa)}\Bigg{[}\frac{(\rho+4\lambda)D}{K}+\frac{D}{\sqrt{K}\alpha}+\frac{2 \rho L_{f}^{2}\alpha}{\sqrt{K}}+\frac{6\rho\lambda(L_{f}+L_{\omega})^{2} \alpha^{2}}{K}\Delta_{2}\Bigg{]},\]

_where \(D=\psi_{1/\rho}^{d}(x^{1})-\inf_{x}\psi(x)\)._

**Proof Proposition 2**

\(f_{z}(x,\xi)\) inherits convexity from \(h\). The other properties hold by

\[|f(x,\xi)-f_{y}(x,\xi)|\] \[=|h(c(x,\xi))-h(c(y,\xi)+\langle\nabla c(y,\xi),x-y\rangle)|\] \[\leq L_{h}|c(x,\xi)-c(y,\xi)-\langle\nabla c(y,\xi),x-y\rangle|\] \[\leq\frac{L_{h}C}{2}\|x-y\|^{2}\]

and \(\mathbf{P3^{\prime}}\) is by \(\mathbf{C1^{\prime}}\).

**Proof of Lemma 8**

Without loss of generality, we consider the following proximal mapping

\[\mathcal{A}(z,x,\xi)\coloneqq\operatorname*{arg\,min}_{w}\{f_{z}(w,\xi)+ \omega(w)+\gamma V_{d}(w,x)\},\]

where \(\mathcal{A}\) denotes the proximal mapping from two past iterates \(z,x\) and a given sample \(\xi\sim\Xi\). Then we invoke the three-point lemma to get, for any \(u\in\operatorname*{dom}d\) that

\[f_{z}(\mathcal{A}(z,x,\xi),\xi)+\omega(\mathcal{A}(z,x,\xi))+ \gamma V_{d}(\mathcal{A}(z,x,\xi),x)\] \[\leq f_{z}(u,\xi)+\omega(u)+\gamma V_{d}(u,x)-(\gamma-\kappa)V_{d }(u,\mathcal{A}(z,x,\xi))\] (24)

Similarly, given some \(\xi^{\prime}\sim\Xi,v\in\operatorname*{dom}d\), we have

\[f_{z}(\mathcal{A}(z,x,\xi^{\prime}),\xi^{\prime})+\omega(\mathcal{ A}(z,x,\xi^{\prime}))+\gamma V_{d}(\mathcal{A}(z,x,\xi^{\prime}),x)\] \[\leq f_{z}(v,\xi^{\prime})+\omega(v)+\gamma V_{d}(v,x)-(\gamma- \kappa)V_{d}(v,\mathcal{A}(z,x,\xi^{\prime})).\] (25)

Letting \(u=\mathcal{A}(z,x,\xi^{\prime})\) in (24) and \(v=\mathcal{A}(z,x,\xi)\) in (25), we sum the two relations up and get

\[(\gamma-\kappa)[V_{d}(\mathcal{A}(z,x,\xi^{\prime}),\mathcal{A}( z,x,\xi))+V_{d}(\mathcal{A}(z,x,\xi),\mathcal{A}(z,x,\xi^{\prime}))]\] \[\leq f_{z}(\mathcal{A}(z,x,\xi),\xi^{\prime})-f_{z}(\mathcal{A}( z,x,\xi^{\prime}),\xi^{\prime})+f_{z}(\mathcal{A}(z,x,\xi^{\prime}),\xi)-f_{z}( \mathcal{A}(z,x,\xi),\xi)\] \[\leq L_{f}[\sqrt{2V_{d}(\mathcal{A}(z,x,\xi^{\prime}),\mathcal{A}( z,x,\xi))}+\sqrt{2V_{d}(\mathcal{A}(z,x,\xi),\mathcal{A}(z,x,\xi^{\prime}))}]\] (26) \[\leq 2L_{f}[\sqrt{V_{d}(\mathcal{A}(z,x,\xi^{\prime}),\mathcal{A}( z,x,\xi))+V_{d}(\mathcal{A}(z,x,\xi),\mathcal{A}(z,x,\xi^{\prime}))}]\] (27)

[MISSING_PAGE_FAIL:22]

Re-arranging the terms,

\[(\gamma_{k}-\kappa)\mathbb{E}_{k}[V_{d}(\hat{x}^{k},x^{k+1})]+\frac{ \gamma_{k}-\rho}{2}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]\] (35) \[\leq(\gamma_{k}-\rho)V_{d}(\hat{x}^{k},x^{k})+\frac{\lambda}{2} \mathbb{E}_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]+\frac{\lambda}{2}\|x^{k-\tau_{k }}-\hat{x}^{k}\|^{2}+\frac{2L_{f}^{2}}{\gamma_{k}-\kappa}\] \[\leq(\gamma_{k}-\rho)V_{d}(\hat{x}^{k},x^{k})+\frac{3\lambda}{2} \mathbb{E}_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]+\lambda\mathbb{E}_{k}[\|x^{k+1 }-\hat{x}^{k}\|^{2}]+\frac{2L_{f}^{2}}{\gamma_{k}-\kappa}\] (36) \[=(\gamma_{k}-\rho)V_{d}(\hat{x}^{k},x^{k})+\frac{3\lambda}{2} \mathbb{E}_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]+\frac{2L_{f}^{2}}{\gamma_{k}- \kappa}+2\lambda\mathbb{E}_{k}[V_{d}(\hat{x}^{k},x^{k+1})]\] \[\quad+\lambda\mathbb{E}_{k}[\|x^{k+1}-\hat{x}^{k}\|^{2}-2V_{d}( \hat{x}^{k},x^{k+1})]\] \[\leq(\gamma_{k}-\rho)V_{d}(\hat{x}^{k},x^{k})+\frac{3\lambda}{2} \mathbb{E}_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]+\frac{2L_{f}^{2}}{\gamma_{k}- \kappa}+2\lambda\mathbb{E}_{k}[V_{d}(\hat{x}^{k},x^{k+1})]\] (37)

where the (36) uses \(\|a+b\|^{2}\leq 2\|a\|^{2}+2\|b\|^{2}\) and (37) again follows by \(V_{d}(\hat{x}^{k},x^{k+1})\geq\frac{1}{2}\|x^{k+1}-\hat{x}^{k}\|^{2}\). Now we re-arrange the terms and divide both sides by \(\gamma_{k}-2\lambda-\kappa\) to get

\[\mathbb{E}_{k}[V_{d}(\hat{x}^{k},x^{k+1})]\] \[\leq\frac{\gamma_{k}-\rho}{\gamma_{k}-2\lambda-\kappa}V_{d}(\hat {x}^{k},x^{k})-\frac{\gamma_{k}-\rho}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E }_{k}[\|x^{k+1}-x^{k}\|^{2}]\] \[\quad+\frac{2L_{f}^{2}}{(\gamma_{k}-\kappa)(\gamma_{k}-2\lambda -\kappa)}+\frac{3\lambda}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E}_{k}[\|x^{k+ 1}-x^{k-\tau_{k}}\|^{2}]\] \[=V_{d}(\hat{x}^{k},x^{k})-\frac{\rho-2\lambda-\kappa}{\gamma_{k} -2\lambda-\kappa}V_{d}(\hat{x}^{k},x^{k})-\frac{\gamma_{k}-\rho}{2(\gamma_{k} -2\lambda-\kappa)}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]\] \[\quad+\frac{2L_{f}^{2}}{(\gamma_{k}-\kappa)(\gamma_{k}-2\lambda -\kappa)}+\frac{3\lambda}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E}_{k}[\|x^{k +1}-x^{k-\tau_{k}}\|^{2}]\] (38)

Now we are ready to evaluate the decrease in the potential function.

\[\mathbb{E}_{k}[\psi_{1/\rho}^{d}(x^{k+1})]\] \[=\mathbb{E}_{k}[f(\hat{x}^{k+1})+\omega(\hat{x}^{k+1})+\rho V_{d}( \hat{x}^{k+1},x^{k+1})]\] \[\leq\mathbb{E}_{k}[f(\hat{x}^{k})+\omega(\hat{x}^{k})+\rho V_{d}( \hat{x}^{k},x^{k+1})]\] \[\leq f(\hat{x}^{k})+\omega(\hat{x}^{k})+\rho V_{d}(\hat{x}^{k},x^ {k})-\frac{\rho(\rho-2\lambda-\kappa)}{\gamma_{k}-2\lambda-\kappa}V_{d}(\hat{x }^{k},x^{k})\] (39) \[\quad+\frac{3\rho\lambda}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E }_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]+\frac{2\rho L_{f}^{2}}{(\gamma_{k}- \kappa)(\gamma_{k}-2\lambda-\kappa)}\] \[\quad-\frac{\rho(\gamma_{k}-\rho)}{2(\gamma_{k}-2\lambda-\kappa )}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]\] \[=\psi_{1/\rho}^{d}(x^{k})-\frac{\rho(\rho-2\lambda-\kappa)}{ \gamma_{k}-2\lambda-\kappa}V_{d}(\hat{x}^{k},x^{k})-\frac{\rho(\gamma_{k}-\rho )}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]\] \[\quad+\frac{3\rho\lambda}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E }_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]+\frac{2\rho L_{f}^{2}}{(\gamma_{k}- \kappa)(\gamma_{k}-2\lambda-\kappa)},\]

where (39) plugs in the relation from (38). Another re-arrangement completes the proof.

**Proof of Theorem 6**Summing the relation from **Lemma 9** from \(k=1,\ldots,K\) and multiplying both sides by \(\gamma-2\lambda-\kappa\),

\[\rho(\rho-2\lambda-\kappa)\mathbb{E}[V_{d}(\hat{x}^{k^{*}},x^{k^{*}})]\] \[=\frac{\rho(\rho-2\lambda-\kappa)}{K}\sum_{k=1}^{K}\mathbb{E}[V_{d }(\hat{x}^{k},x^{k})]\] (40) \[\leq\frac{\gamma-2\lambda-\kappa}{K}\big{\{}\psi_{1/\rho}^{d}(x^ {1})-\mathbb{E}[\psi_{1/\rho}^{d}(x^{K+1})]\big{\}}+\frac{2\rho L_{f}^{2}}{ \gamma-\kappa}\] \[\quad+\frac{3\rho\lambda}{2K}\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}- x^{k-\tau_{k}}\|^{2}-\frac{\rho(\gamma-\rho)}{2K}\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}- x^{k}\|^{2}]\] \[\leq\frac{(\gamma-2\lambda-\kappa)D}{K}+\frac{2\rho L_{f}^{2}}{ \gamma-\kappa}+\frac{\rho}{K}\sum_{k=1}^{K}\mathbb{E}\Big{[}-\frac{\gamma- \rho}{2}\|x^{k+1}-x^{k}\|^{2}+\frac{3\lambda}{2}\|x^{k+1}-x^{k-\tau_{k}}\|^{2 }\Big{]},\] (41)

where (41) uses the relation \(\psi_{1/\rho}^{d}(x^{K+1})\geq\inf_{x}\psi^{d}(x)\). Now it remains to bound the error of delays and recall that we have

\[\sum_{k=1}^{K}\|x^{k+1}-x^{k-\tau_{k}}\|^{2}\leq 2\sum_{k=1}^{K}\|x^{k+1}-x^{k} \|^{2}+4\gamma^{-2}(L_{f}+L_{\omega})^{2}\sum_{k=1}^{K}\tau_{k}^{2}\] (42)

and plugging the bounds back, we have

\[\rho(\rho-2\lambda-\kappa)\mathbb{E}[V_{d}(\hat{x}^{k^{*}},x^{k^{ *}})]\] \[\leq\frac{(\gamma-2\lambda-\kappa)D}{K}+\frac{2\rho L_{f}^{2}}{ \gamma-\kappa}+\frac{6\rho\lambda(L_{f}+L_{\omega})^{2}}{\gamma^{2}K}\sum_{k=1 }^{K}\tau_{k}^{2}\] \[\leq\frac{(\rho+4\lambda)D}{K}+\frac{D}{\sqrt{K}\alpha}+\frac{2 \rho L_{f}^{2}\alpha}{\sqrt{K}}+\frac{6\rho\lambda(L_{f}+L_{\omega})^{2}\alpha ^{2}}{K}\Delta_{2},\] (43)

where in (43) we cancel the first summation from (42) since \(\gamma-\rho=\sqrt{K}/\alpha+6\lambda+\kappa\geq 6\lambda\). Finally we divide both sides by \(\rho(\rho-2\lambda-\kappa)\) to complete the proof.

### Proof of Lemma 2 and Theorem 2

First \(d(x)=\frac{1}{2}\|x\|^{2}\) satisfies \(\textbf{A4}^{\prime}\). Since **A1**, **A2**, **A3**, **A4** and **C1** are equivalent to \(\textbf{A1}^{\prime}\), \(\textbf{A2}^{\prime}\), \(\textbf{A3}^{\prime}\), \(\textbf{A5}^{\prime}\) and \(\textbf{C1}^{\prime}\), noticing that **Lemma 2** is a special case of **Lemma 9** and **Theorem 2** is a special case of **Theorem 6**, we complete the proof.

### Proof of Theorem 3

Recall that in the proof of DSPL, we actually used **A4** and the properties from **Proposition 1** that are deduced from **A1**, **A2**, **A3** and **C1**. Indeed, it is straightforward to verify that \(f_{z}(x,\xi)=\langle\nabla f(z,\xi),x-z\rangle\) satisfies **Proposition 1** given **A1**, **A2**, **A3** and \(\lambda\)-smoothness of \(f\).

## Appendix E Delay-independent analysis

In this section, we further improve our analysis and show that by adopting a simple safeguarding strategy during the iterations.

### Proof of Lemma 3

We show \(\sum_{k=1}^{K}\tau_{k}\leq mK\) by noticing the following facts: **1)**. at each iteration there are at most \(m-1\) agents accumulating 1 unit of delay; **2)**. \(\tau_{k}\) must come from a machine's previous delay accumulation. **3)**. once the gradient is used, delay on a machine starts count from 0. Assuming that at \(k=1\) there are no previously accumulated delays on each agent, then we have \(\sum_{k=1}^{K}\tau_{k}\leq(m-1)K\leq mK\). We use a loose bound \(mK\) to simplify notation.

### Proof of Lemma 4

We know that \(T\sum_{k=1}^{K}\mathbb{I}\{\tau_{k}\geq T\}\leq\sum_{k=1}^{K}\tau_{k}\leq mK\) and recall that \(T=r^{-1}mK^{\beta}\):

\[\sum_{k=1}^{K}\mathbb{I}\{\tau_{k}\leq T\}\geq K-\sum_{k=1}^{K}\mathbb{I}\{\tau_ {k}\geq T\}\geq K-\frac{mK}{T}=K-rK^{1-\beta}.\]

Also we have \(\sum_{k=1}^{K}\tau_{k}^{2}\leq T\sum_{k=1}^{K}\tau_{k}\leq mKT\).

### Proof of Theorem 4

First we rewrite **Lemma 1** by associating potential reduction with \(I_{k}\coloneqq\mathbb{I}\{\tau_{k}\leq T\}\).

\[\mathbb{E}_{k}[\psi_{1/\rho}(x^{k+1})] \leq\psi_{1/\rho}(x^{k})-\frac{\rho(\rho-2\lambda-\kappa)I_{k}}{ 2(\gamma_{k}-2\lambda-\kappa)}\|\dot{x}^{k}-x^{k}\|^{2}-\frac{\rho(\gamma_{k}- \rho)I_{k}}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}\] \[\quad+\frac{\rho\lambda I_{k}}{\gamma_{k}-2\lambda-\kappa} \mathbb{E}_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]+\frac{2\rho L_{f}I_{k}}{\gamma _{k}-2\lambda-\kappa}\mathbb{E}_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|],\]

where we have \(x^{k+1}=x^{k}\), thus \(\mathbb{E}_{k}[\psi_{1/\rho}(x^{k+1})]=\psi_{1/\rho}(x^{k})\) if \(I_{k}=0\).

From **Lemma 4**, we know that \(\sum_{k=1}^{K}I_{k}\geq K-rK^{1-\beta}=K(1-rK^{-\beta})=\eta^{-1}K\). Then similar telescopic sum over the _un-skipped_ iterations will give

\[\frac{\rho(\rho-2\lambda-\kappa)}{2}\mathbb{E}[\|\dot{x}^{k^{*}}- x^{k^{*}}\|^{2}]\] \[=\frac{\rho(\rho-2\lambda-\kappa)}{2\sum_{k=1}^{K}I_{k}}\sum_{k=1 }^{K}\mathbb{E}[\|\dot{x}^{k}-x^{k}\|^{2}I_{k}]\] \[\leq\frac{\gamma-2\lambda-\kappa}{\sum_{k=1}^{K}I_{k}}\{\psi_{1/ \rho}(x^{1})-\mathbb{E}[\psi_{1/\rho}(x^{K+1})]\}-\frac{\rho(\gamma-\rho)}{2 \sum_{k=1}^{K}I_{k}}\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}-x^{k}\|^{2}]I_{k}\] (44) \[\quad+\frac{\rho\lambda}{\sum_{k=1}^{K}I_{k}}\sum_{k=1}^{K} \mathbb{E}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]I_{k}+\frac{2\rho L_{f}}{\sum_{k=1}^ {K}I_{k}}\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}-x^{k-\tau_{k}}\|]I_{k}\] \[\leq\frac{\rho D}{\sum_{k=1}^{K}I_{k}}+\frac{\sqrt{K}D}{\alpha \sqrt{\eta}\sum_{k=1}^{K}I_{k}}\] (45) \[\quad+\frac{\rho}{\sum_{k=1}^{K}I_{k}}\sum_{k=1}^{K}\mathbb{E} \big{[}-\frac{\gamma-\rho}{2}\|x^{k+1}-x^{k}\|^{2}+\lambda\|x^{k+1}-x^{k-\tau _{k}}\|^{2}+2L_{f}\|x^{k+1}-x^{k-\tau_{k}}\|\big{]}I_{k},\]

where (44) uses the fact that \(\gamma_{k}\equiv\gamma\) is the same for all the iterations; (45) uses the relation \(\gamma=\frac{\sqrt{K}}{\alpha\sqrt{\eta}}+\rho+\kappa+4\lambda\geq\max\{\frac{ \sqrt{K}}{\alpha\sqrt{\eta}},\rho+\kappa+4\lambda\}\). Then we bound the above terms, respectively, by

\[\frac{\rho D}{\sum_{k=1}^{K}I_{k}}\leq \frac{\rho D}{\eta^{-1}K}=\frac{\rho\eta D}{K}\] (46) \[\frac{\sqrt{K}D}{\alpha\sqrt{\eta}\sum_{k=1}^{K}I_{k}}\leq \frac{\sqrt{K}D}{\alpha\eta^{-1/2}K}=\frac{\sqrt{\eta}D}{\sqrt{K} \alpha},\] (47)

where we used the fact that \(\sum_{k=1}^{K}I_{k}\leq K\). Meanwhile,

\[\frac{2\rho L_{f}}{\sum_{k=1}^{K}I_{k}}\sum_{k=1}^{K}\|x^{k+1}-x^ {k-\tau_{k}}\|I_{k} \leq\frac{2\rho L_{f}}{\sum_{k=1}^{K}I_{k}}\sum_{k=1}^{K}\|x^{k+1}- x^{k-\tau_{k}}\|\] \[\leq\frac{4\sqrt{\eta}\rho L_{f}(L_{f}+L_{\omega})\alpha}{\sqrt{K} \eta^{-1}}\sum_{k=1}^{K}(\tau_{k}+1)\] \[\leq\frac{4\eta^{3/2}\rho L_{f}(L_{f}+L_{\omega})m\alpha}{\sqrt{K }},\] (48)where (48) uses a tighter bound \(\sum_{k=1}^{K}\tau_{k}\leq(m-1)K\). Finally, we bound the delays using

\[\frac{2\rho\lambda}{\sum_{k=1}^{K}I_{k}}\sum_{k=1}^{K}\mathbb{E}[\|x^{k}-x^{k- \tau_{k}}\|^{2}]I_{k}\leq\frac{8\eta\rho\lambda(L_{f}+L_{\omega})^{2}\alpha^{2} }{K\sum_{k=1}^{K}I_{k}}\sum_{k=1}^{K}\tau_{k}^{2}I_{k}\leq\frac{8\eta^{2}\rho \lambda(L_{f}+L_{\omega})^{2}\alpha^{2}}{K^{2}}\sum_{k=1}^{K}\tau_{k}^{2}I_{k},\]

and that \(\frac{1}{K^{2}}\sum_{k=1}^{K}\tau_{k}^{2}I_{k}\leq\frac{mKT}{K^{2}}=\frac{m^{2 }}{rK^{1-\beta}}\). Putting the above bounds back, using the same technique as in (18) to cancel the error \(2\lambda\|x^{k+1}-x^{k}\|^{2}\) with \(-\sum_{k=1}^{K}\frac{\gamma-\rho}{2}\|x^{k+1}-x^{k}\|^{2},\gamma-\rho\geq 4\lambda\), we can re-arrange the terms to complete the proof.

### Proof of Theorem 5

We prove **Theorem 5** by showing its Bregman version.

**Theorem 7** (Safeguarded DSPL in Bregman setting).: _Under the same conditions as_ **Lemma 9** _as well as_ **D1**_, taking **1)**_\(\beta>0,K>r^{1/\beta}\) or **2)**_\(\beta=0,r<1\), then letting \(\gamma_{k}\equiv\gamma=\frac{\sqrt{K}}{\alpha\sqrt{\eta}}+\rho+\kappa+6\lambda, \eta=1+\frac{r}{K^{\beta}-r}\) for some \(\alpha>0\) and \(k^{*}\) be uniformly chosen between iterations where \(\tau_{k}\leq\sqrt{K}\), then_

\[\mathbb{E}[V_{d}(\hat{x}^{k^{*}},x^{k^{*}})]\leq\frac{1}{\rho(\rho-2\lambda- \kappa)}\Big{[}\frac{\eta(\rho+4\lambda)D}{K}+\frac{\sqrt{\eta}D}{\sqrt{K} \alpha}+\frac{2\sqrt{\eta}\rho L_{f}\alpha}{\sqrt{K}}+\frac{6\eta^{2}\rho \lambda(L_{f}+L_{\omega})^{2}m^{2}\alpha^{2}}{K^{1/\beta}}\Big{]},\]

_where \(D=\psi_{1/\rho}^{4}(x^{1})-\inf_{x}\psi^{d}(x)\)._

**Proof of Theorem 7**

Similar to the proof of **Theorem 4**, we write the modified potential reduction by

\[\mathbb{E}_{k}[\psi_{1/\rho}^{d}(x^{k+1})] \leq\psi_{1/\rho}^{d}(x^{k})-\frac{\rho(\rho-2\lambda-\kappa)I_{ k}}{2(\gamma_{k}-2\lambda-\kappa)}V_{d}(\hat{x}^{k},x^{k})-\frac{\rho( \gamma_{k}-\rho)I_{k}}{2(\gamma_{k}-2\lambda-\kappa)}\mathbb{E}_{k}[\|x^{k+1} -x^{k}\|^{2}]\] \[\quad+\frac{3\rho\lambda I_{k}}{\gamma_{k}-2\lambda-\kappa} \mathbb{E}_{k}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]+\frac{2\rho L_{f}I_{k}}{( \gamma_{k}-\kappa)(\gamma_{k}-2\lambda-\kappa)}\]

Then telescoping gives

\[\frac{\rho(\rho-2\lambda-\kappa)}{2}\mathbb{E}[V_{d}(\hat{x}^{k^ {*}},x^{k^{*}})]\] \[=\frac{\rho(\rho-2\lambda-\kappa)}{2\sum_{k=1}^{K}I_{k}}\sum_{k=1 }^{K}\mathbb{E}[V_{d}(\hat{x}^{k},x^{k})I_{k}]\] \[\leq\frac{\gamma-2\lambda-\kappa}{\sum_{k=1}^{K}I_{k}}\{\psi_{1/ \rho}(x^{1})-\mathbb{E}[\psi_{1/\rho}(x^{K+1})]\}-\frac{\rho(\gamma-\rho)}{2 \sum_{k=1}^{K}I_{k}}\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}-x^{k}\|^{2}]I_{k}\] \[\quad+\frac{3\rho\lambda}{\sum_{k=1}^{K}I_{k}}\sum_{k=1}^{K} \mathbb{E}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]I_{k}+\frac{2\rho L_{f}}{\gamma-\kappa}\] \[\leq\frac{(\rho+4\lambda)D}{\sum_{k=1}^{K}I_{k}}+\frac{\sqrt{K}D} {\alpha\sqrt{\eta}\sum_{k=1}^{K}I_{k}}+\frac{2\rho L_{f}}{\gamma-\kappa}\] (49) \[\quad+\frac{\rho}{\sum_{k=1}^{K}I_{k}}\sum_{k=1}^{K}\mathbb{E} \Big{[}-\frac{\gamma-\rho}{2}\|x^{k+1}-x^{k}\|^{2}+\frac{3\lambda}{2}\|x^{k+1 }-x^{k-\tau_{k}}\|^{2}\Big{]}I_{k}\] \[\leq\frac{\eta(\rho+4\lambda)D}{K}+\frac{\sqrt{\eta}D}{\sqrt{K} \alpha}+\frac{2\sqrt{\eta}\rho L_{f}\alpha}{\sqrt{K}}+\frac{6\eta^{2}\rho \lambda(L_{f}+L_{\omega})^{2}m^{2}\alpha^{2}}{rK^{1-\beta}},\]

where (49) reuses (46), (47) and (48). A re-arrangement completes the proof of **Theorem 7**. Taking \(d(x)=\frac{1}{2}\|x\|^{2}\) completes the proof of **Theorem 5**.

## Appendix F Dspl with momentum

### Preliminaries of Dsepl

Momentum has been an important ingredient for a stochastic algorithm to be implemented in practice. In this section, we incorporate the extrapolation technique into the delayed prox-linear algorithm. Before the master performs a proximal update, it uses two recent iterates to compute an extrapolated iterate \(y^{k}\). Then a proximal update is done centered around \(y^{k}\) with delayed information. We summarize the procedure in **Algorithm 5**. Throughout this section we take \(\gamma_{k}\equiv\gamma\).

```
0:\(x^{0}\), \(x^{1},\beta\); for\(k\) = 1, 2, \(\ldots\)do  Let \(c(x^{k-\tau_{k}},\xi^{k-\tau_{k}})\) and \(\nabla c(x^{k-\tau_{k}},\xi^{k-\tau_{k}})\) be computed by a worker with delay \(\tau_{k}\);  In the master node update \[y^{k} =x^{k}+\beta(x^{k}-x^{k-1}),\] \[x^{k+1} =\operatorname*{arg\,min}_{x}\big{\{}f_{x^{k-\tau_{k}}}(x,\xi^{k -\tau_{k}})+\omega(x)+\frac{\gamma}{2}\|x-y^{k}\|^{2}\big{\}}.\]

end ```

**Algorithm 5**Extrapolated DSPL

To analyze DSEPL, we extend the framework from [5] to the delayed case and the analysis is based on a virtual iterate

\[z^{k}\coloneqq x^{k}+\beta\theta^{-1}(x^{k}-x^{k-1}),\]

where the extrapolation parameter, also known as momentum, is fixed at some constant \(\beta\in[0,1)\) and \(\theta=1-\beta\). Using the virtual iterate, DSEPL uses a more complicated potential function.

\[\psi_{1/\rho}(z^{k})+\frac{\rho\beta}{(\gamma-\kappa)\theta^{2}}\psi(x^{k})+ \frac{\rho(\gamma\beta+2\rho\beta^{2}\theta^{-2})}{2(\gamma-\lambda)\theta}\| x^{k}-x^{k-1}\|^{2}\] (50)

As extrapolation increases the instability of the iterations, analysis of DSEPL furthers requires boundedness of the delays.

**E1:**: (Bounded delay) Independent delays are bounded by \(\tau<\infty\).

The following lemma presents a similar descent property for the potential function.

**Lemma 10**.: _Suppose_ **A1**_,_ **A2**_,_ **A3** _and_ **C1** _hold. Given \(0\leq\beta<1,\rho>3\lambda+2\kappa\beta+\kappa\) and \(\gamma\geq\rho\),_

\[\frac{(\rho-\kappa\theta)}{2\rho(\gamma-\kappa)\theta}\|\nabla \psi_{1/\rho}(z^{k})\|^{2}\] \[\quad\leq\psi_{1/\rho}(z^{k})-\mathbb{E}_{k}[\psi_{1/\rho}(z^{k+1 })]+\frac{\rho\beta}{(\gamma-\kappa)\theta^{2}}\{\psi(x^{k})-\mathbb{E}_{k}[ \psi(x^{k+1})]\}\] \[\quad\quad+\frac{\rho(2\rho\beta^{2}\theta^{-1}+\gamma\beta\theta )}{2(\gamma-\kappa)\theta^{2}}\{\|x^{k}-x^{k-1}\|^{2}-\mathbb{E}_{k}[\|x^{k+1 }-x^{k}\|^{2}]\}\] \[\quad\quad+\frac{2\rho L_{f}^{2}}{(\gamma-\kappa)^{2}\theta^{2}} -\frac{\rho(\gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta^{2}\theta^{ -1})}{2(\gamma-\kappa)\theta^{2}}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]+\frac{ \rho\mathbb{E}_{k}[\varepsilon_{k}]}{(\gamma-\kappa)\theta^{2}},\]

_where \(\varepsilon_{k}=(\lambda\theta+\frac{\lambda}{2})\|x^{k+1}-x^{k-\tau_{k}}\|^{ 2}+\frac{\lambda(1-\theta)}{2}\|x^{k}-x^{k-\tau_{k}}\|^{2}\) characterizes the error of delay._

We next bound the stochastic delay using **E1** and eliminate the delays.

**Lemma 11**.: _Under the same conditions as_ **Lemma 10** _as well as_ **E1**_, given \(\gamma>\max\big{\{}\rho,2(\rho+\kappa\beta)\theta^{-1}+2\rho\beta^{2}\theta^{- 3}\big{\}}\),_

\[\mathbb{E}[\|\nabla\psi_{1/\rho}(z^{k^{*}})\|^{2}]\leq\frac{2\rho\theta}{\rho- \kappa\theta}\Big{[}\frac{(\gamma-\kappa+\rho\beta\theta^{-2})D}{K}+\frac{2 \rho L_{f}^{2}}{(\gamma-\kappa)\theta^{2}}+\frac{\rho\lambda(3\tau^{2}+2\beta) }{2\theta^{2}K}\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}-x^{k}\|^{2}]\Big{]},\]

_where \(D=\max\{\psi_{1/\rho}(z^{1})-\psi_{1/\rho}(z^{*}),\psi(x^{1})-\psi(x^{*})\}\)._

_Remark 7_.: **Lemma 11** reveals the effect of extrapolation. After simplification \(\theta=1-\beta\) appears in the denominator of the error from delay and will enlarge such an error if \(\beta\to 1\). This is intuitive since when extrapolating using two iterations generated from delayed information, the error is magnified.

By taking less aggressive steps, we bound the term \(\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}-x^{k}\|^{2}]\) and arrive at the final convergence result for DSEPL.

**Theorem 8**.: _Under the same conditions as_ **Lemma 11**_, if we further choose \(\gamma\geq 2\theta^{-1}(\rho+\kappa\beta)+2\rho\theta^{-3}\beta^{2}+2\lambda \theta^{-2}\beta+3\lambda\theta^{-1}\tau^{2}\), then_

\[\mathbb{E}[\|\nabla\psi_{1/\rho}(z^{k^{*}})\|^{2}]\leq\frac{2\rho\theta}{(\rho -\kappa\theta)\sqrt{K}}\Big{(}\frac{\lambda(3\tau^{2}+2\beta)}{\Gamma_{\beta} }+1\Big{)}\left[\frac{(\gamma-\kappa+\rho\beta\theta^{-2})D}{\sqrt{K}}+\frac{2 \rho L_{f}^{2}\sqrt{K}}{(\gamma-\kappa)\theta^{2}}\right],\]

_where \(\Gamma_{\beta}\coloneqq\gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta ^{2}\theta^{-1}-3\lambda\tau^{2}-2\lambda\beta\)._

_Remark 8_.: If \(\gamma=\mathcal{O}(\sqrt{K})\), then \(\frac{(\gamma-\kappa+\rho\beta\theta^{-2})D}{K}+\frac{2\rho L_{f}^{2}}{(\gamma -\kappa)\theta^{2}}=\mathcal{O}(\frac{1}{\sqrt{K}})\), \(\Gamma_{\beta}^{-1}=\mathcal{O}(\frac{1}{\sqrt{K}})\) and

\[\mathbb{E}[\|\nabla\psi_{1/\rho}(z^{k^{*}})\|^{2}]=\mathcal{O}\Big{(}\frac{1} {K}+\frac{1}{\sqrt{K}}+(\frac{1}{K}+\frac{1}{\sqrt{K}})\frac{\tau^{2}}{\sqrt {K}}\Big{)},\]

which implies that the delay is negligible if \(\tau=o(K^{1/4})\).

_Remark 9_.: Although our convergence result is based on the extrapolated sequence, we can leverage **Lemma 10** to show that \(\mathbb{E}[\|x^{k^{*}+1}-x^{k^{*}}\|^{2}]\), and subsequently \(\mathbb{E}[\|z^{k^{*}}-x^{k^{*}}\|^{2}]\) is \(\mathcal{O}(\frac{1}{K})\). Using smoothness of the Moreau envelope, we finally have \(\mathbb{E}[\|\nabla\psi_{1/\rho}(x^{k^{*}})\|^{2}]\) is \(\mathcal{O}(\frac{1}{\sqrt{K}})\).

### Convergence analysis of DSEPL

In this section we present the convergence analysis for DSEPL.

#### f.2.1 Auxiliary results

To show the convergence of DSEPL, we need to define an auxiliary sequence

\[z^{k}\coloneqq x^{k}+\frac{\beta}{1-\beta}(x^{k}-x^{k-1}).\]

Given \(x^{k}\), define \(\bar{x}=\beta x^{k}+(1-\beta)x\) for \(x\in\mathrm{dom}(\omega)\) and \(\theta=1-\beta\). The following identities hold

\[\bar{x}-x^{k} =\theta(x-x^{k})\] \[\bar{x}-y^{k} =\theta(x-z^{k})\] \[\bar{x}-x^{k+1} =\theta(x-z^{k+1}).\]

and will be used frequently in the analysis.

**Proof of Lemma 10**

First by the \((\gamma-\kappa)\)-strong convexity of the proximal sub-problem, we have

\[f_{x^{k-\tau_{k}}}(x^{k+1},\xi^{k-\tau_{k}})+\omega(x^{k+1})+ \frac{\gamma}{2}\|x^{k+1}-y^{k}\|^{2}\] \[\leq f_{x^{k-\tau_{k}}}(\bar{x},\xi^{k-\tau_{k}})+\omega(\bar{x} )+\frac{\gamma}{2}\|\bar{x}-y^{k}\|^{2}-\frac{\gamma-\kappa}{2}\|x^{k+1}-\bar {x}\|^{2}\] \[=f_{x^{k-\tau_{k}}}(\bar{x},\xi^{k-\tau_{k}})+\omega(\bar{x})+ \frac{\gamma\theta^{2}}{2}\|x-z^{k}\|^{2}-\frac{(\gamma-\kappa)\theta^{2}}{2} \|x-z^{k+1}\|^{2}.\] (51)

Also, since \(f_{x^{k-\tau_{k}}}(\cdot,\xi^{k-\tau_{k}})+\omega(\cdot)+\frac{\kappa}{2}\| \cdot-x^{k}\|^{2}\) is convex, we plug \(\bar{x}\) in and apply convexity to get that

\[f_{x^{k-\tau_{k}}}(\bar{x},\xi^{k-\tau_{k}})+\omega(\bar{x})+ \frac{\kappa}{2}\|\bar{x}-x^{k}\|^{2}\] \[\leq(1-\theta)\left[f_{x^{k-\tau_{k}}}(x^{k},\xi^{k-\tau_{k}})+ \omega(x^{k})\right]+\theta\big{[}f_{x^{k-\tau_{k}}}(x,\xi^{k-\tau_{k}})+ \omega(x)+\frac{\kappa}{2}\|x-x^{k}\|^{2}\big{]}\] \[\leq(1-\theta)\big{[}f_{x^{k-\tau_{k}}}(x^{k},\xi^{k-\tau_{k}})+ \omega(x^{k})\big{]}+\theta\big{[}f(x)+\omega(x)+\frac{\kappa}{2}\|x-x^{k}\|^{2 }+\frac{\lambda}{2}\|x-x^{k-\tau_{k}}\|^{2}\big{]},\]where the second inequality leverages two-sided approximation since \(\big{|}f_{x^{k-\tau_{k}}}(x,\xi^{k-\tau_{k}})-f(x)\big{|}\leq\frac{\lambda}{2}\|x -x^{k-\tau_{k}}\|^{2}\). Then a simple re-arrangement gives

\[f_{x^{k-\tau_{k}}}(\bar{x},\xi^{k-\tau_{k}})+\omega(\bar{x})\] \[\leq(1-\theta)\left[f_{x^{k-\tau_{k}}}(x^{k},\xi^{k-\tau_{k}})+ \omega(x^{k})\right]\] \[\quad+\theta\big{[}f(x)+\omega(x)+\frac{\kappa}{2}\|x-x^{k}\|^{2} +\frac{\lambda}{2}\|x-x^{k-\tau_{k}}\|^{2}\big{]}-\frac{\kappa\theta^{2}}{2} \|x-x^{k}\|^{2}.\]

Now combining the above inequality with (51), we get that

\[f_{x^{k-\tau_{k}}}(x^{k+1},\xi^{k-\tau_{k}})+\omega(x^{k+1})+ \frac{\gamma}{2}\|x^{k+1}-y^{k}\|^{2}\] \[\leq(1-\theta)\big{[}f_{x^{k-\tau_{k}}}(x^{k},\xi^{k-\tau_{k}})+ \omega(x^{k})\big{]}+\theta\big{[}f(x)+\omega(x)+\frac{\kappa}{2}\|x-x^{k}\|^{ 2}+\frac{\lambda}{2}\|x-x^{k-\tau_{k}}\|^{2}\big{]}\] \[\quad-\frac{\kappa\theta^{2}}{2}\|x-x^{k}\|^{2}+\frac{\gamma \theta^{2}}{2}\|x-x^{k}\|^{2}-\frac{(\gamma-\kappa)\theta^{2}}{2}\|x-z^{k+1}\| ^{2}.\] (52)

From now on we let \(x=\hat{z}^{k}\) in (52) and successively deduce that

\[f_{x^{k-\tau_{k}}}(x^{k+1},\xi^{k-\tau_{k}})+\omega(x^{k+1})+ \frac{\gamma}{2}\|x^{k+1}-y^{k}\|^{2}\] \[\leq(1-\theta)\left[f_{x^{k-\tau_{k}}}(x^{k},\xi^{k-\tau_{k}})+ \omega(x^{k})\right]+\theta\big{[}f(\hat{z}^{k})+\omega(\hat{z}^{k})+\frac{ \kappa}{2}\|\hat{z}^{k}-x^{k}\|^{2}+\frac{\lambda}{2}\|\hat{z}^{k}-x^{k-\tau_{k }}\|^{2}\big{]}\] \[\quad-\frac{\kappa\theta^{2}}{2}\|\hat{z}^{k}-x^{k}\|^{2}+\frac{ \gamma\theta^{2}}{2}\|\hat{z}^{k}-z^{k}\|^{2}-\frac{(\gamma-\kappa)\theta^{2}} {2}\|\hat{z}^{k}-z^{k+1}\|^{2}.\] \[=(1-\theta)\left[f_{x^{k-\tau_{k}}}(x^{k},\xi^{k-\tau_{k}})+ \omega(x^{k})\right]+\theta[f(\hat{z}^{k})+\omega(\hat{z}^{k})]+\frac{\kappa \theta-\kappa\theta^{2}}{2}\|\hat{z}^{k}-x^{k}\|^{2}+\frac{\lambda\theta}{2}\| \hat{z}^{k}-x^{k-\tau_{k}}\|^{2}\] \[\quad+\frac{\gamma\theta^{2}}{2}\|\hat{z}^{k}-z^{k}\|^{2}-\frac{( \gamma-\kappa)\theta^{2}}{2}\|\hat{z}^{k}-z^{k+1}\|^{2}\] \[\leq(1-\theta)\left[f_{x^{k-\tau_{k}}}(x^{k},\xi^{k-\tau_{k}})+ \omega(x^{k})\right]+\theta[f(\hat{z}^{k})+\omega(\hat{z}^{k})]+\frac{\lambda \theta}{2}\|\hat{z}^{k}-x^{k-\tau_{k}}\|^{2}\] \[\quad+\frac{\gamma\theta^{2}}{2}\|\hat{z}^{k}-z^{k}\|^{2}-\frac{( \gamma-\kappa)\theta^{2}}{2}\|\hat{z}^{k}-z^{k+1}\|^{2}+\theta\kappa\beta[\|x^ {k+1}-x^{k}\|^{2}+\|\hat{z}^{k}-x^{k+1}\|^{2}]\] (53) \[\leq(1-\theta)\left[f_{x^{k-\tau_{k}}}(x^{k},\xi^{k-\tau_{k}})+ \omega(x^{k})\right]+\theta[f(\hat{z}^{k})+\omega(\hat{z}^{k})]+\theta(\kappa \beta+\lambda)\|\hat{z}^{k}-x^{k+1}\|^{2}\] \[\quad+\lambda\theta\|x^{k+1}-x^{k-\tau_{k}}\|^{2}+\frac{\gamma \theta^{2}}{2}\|\hat{z}^{k}-z^{k}\|^{2}-\frac{(\gamma-\kappa)\theta^{2}}{2}\| \hat{z}^{k}-z^{k+1}\|^{2}+\theta\kappa\beta\|x^{k+1}-x^{k}\|^{2},\] (54)

where the second inequality (53) applies \(\|\hat{z}^{k}-x^{k}\|^{2}\leq 2[\|x^{k+1}-x^{k}\|^{2}+\|\hat{z}^{k}-x^{k+1}\|^{2}]\) and the last inequality applies \(\|\hat{z}^{k}-x^{k-\tau_{k}}\|^{2}\leq 2\|\hat{z}^{k}-x^{k+1}\|^{2}+2\|x^{k+1}-x^{k- \tau_{k}}\|^{2}\).

By definition of \(\hat{z}^{k}\), for \(\rho>\lambda+\kappa\), we have \((\rho-\lambda-\kappa)\)-strong convexity of \(\psi(x)+\frac{\rho}{2}\|\cdot-x\|^{2}\) and

\[f(\hat{z}^{k})+\omega(\hat{z}^{k})+\frac{\rho}{2}\|\hat{z}^{k}-z^{k}\|^{2}\] \[\leq f(x^{k+1})+\omega(x^{k+1})+\frac{\rho}{2}\|x^{k+1}-z^{k}\|^{2} -\frac{\rho-\lambda-\kappa}{2}\|x^{k+1}-\hat{z}^{k}\|^{2}.\] (55)

Multiplying (55) by \(\theta\) and adding it to (54), we have

\[\frac{\gamma}{2}\|x^{k+1}-y^{k}\|^{2}\] \[\leq(1-\theta)\big{[}f_{x^{k-\tau_{k}}}(x^{k},\xi^{k-\tau_{k}})+ \omega(x^{k})\big{]}+\theta f(x^{k+1})-f_{x^{k-\tau_{k}}}(x^{k+1},\xi^{k-\tau_{k }})-(1-\theta)\omega(x^{k+1})\] \[\quad+\frac{\rho\theta}{2}\|x^{k+1}-z^{k}\|^{2}-\frac{\theta(\rho-3 \lambda-2\kappa\beta-\kappa)}{2}\|x^{k+1}-\hat{z}^{k}\|^{2}+\lambda\theta\|x^{k +1}-x^{k-\tau_{k}}\|^{2}\] \[\quad+\frac{\gamma\theta^{2}-\rho\theta}{2}\|\hat{z}^{k}-z^{k}\|^{2 }-\frac{(\gamma-\kappa)\theta^{2}}{2}\|\hat{z}^{k}-z^{k+1}\|^{2}+\theta\kappa \beta\|x^{k+1}-x^{k}\|^{2}.\] (56)Then we bound the first line of the right hand side in (56) by

\[(1-\theta)\left[f_{x^{k-\tau_{k}}}(x^{k},\xi^{k-\tau_{k}})+\omega(x^{ k})\right]+\theta f(x^{k+1})-f_{x^{k-\tau_{k}}}(x^{k+1},\xi^{k-\tau_{k}})-(1- \theta)\omega(x^{k+1})\] \[\leq (1-\theta)[f(x^{k},\xi^{k-\tau_{k}})+\omega(x^{k})-f(x^{k+1})- \omega(x^{k+1})]+f(x^{k+1})-\mathbb{E}_{\xi}\left[f_{x^{k-\tau_{k}}}(x^{k+1}, \xi)\right]\] \[+\mathbb{E}_{\xi}\left[f_{x^{k-\tau_{k}}}(x^{k+1},\xi)\right]-f_{x ^{k-\tau_{k}}}(x^{k+1},\xi^{k-\tau_{k}})+\frac{(1-\theta)\lambda}{2}\|x^{k}-x^ {k-\tau_{k}}\|^{2}\] \[\leq (1-\theta)[f(x^{k},\xi^{k-\tau_{k}})+\omega(x^{k})-f(x^{k+1})- \omega(x^{k+1})]+\frac{2L_{f}^{2}}{\gamma-\kappa}\] \[+\frac{\lambda}{2}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}+(1-\theta)\|x^ {k}-x^{k-\tau_{k}}\|^{2}],\] (57)

where the inequalities invoke two-sided approximation \(\left|f_{x^{k-\tau_{k}}}(x^{k},\xi^{k-\tau_{k}})-f(x^{k},\xi^{k-\tau_{k}}) \right|\leq\frac{\lambda}{2}\|x^{k}-x^{k-\tau_{k}}\|^{2}\) and bound \(\mathbb{E}_{\xi}\left[f_{x^{k-\tau_{k}}}(x^{k+1},\xi)\right]-f_{x^{k-\tau_{k}} }(x^{k+1},\xi^{k-\tau_{k}})\) with **Lemma 8**. Now plugging (57) back into (56) gives

\[\frac{\gamma}{2}\|x^{k+1}-y^{k}\|^{2}\] \[\leq (1-\theta)[f(x^{k},\xi^{k-\tau_{k}})+\omega(x^{k})-f(x^{k+1})- \omega(x^{k+1})]+\frac{2L_{f}^{2}}{\gamma-\kappa}\] \[+\frac{\rho\theta}{2}\|x^{k+1}-z^{k}\|^{2}-\frac{\theta(\rho-3 \lambda-2\kappa\beta-\kappa)}{2}\|x^{k+1}-\hat{z}^{k}\|^{2}\] \[+\frac{\gamma\theta^{2}-\rho\theta}{2}\|\hat{z}^{k}-z^{k}\|^{2}- \frac{(\gamma-\kappa)\theta^{2}}{2}\|\hat{z}^{k}-z^{k+1}\|^{2}+\theta\kappa \beta\|x^{k+1}-x^{k}\|^{2}.\] \[+\big{(}\lambda\theta+\frac{\lambda}{2}\big{)}\|x^{k+1}-x^{k-\tau_ {k}}\|^{2}+\frac{\lambda(1-\theta)}{2}\|x^{k}-x^{k-\tau_{k}}\|^{2}.\] (58)

Isolating the delayed error by \(\varepsilon_{k}=\big{(}\lambda\theta+\frac{\lambda}{2}\big{)}\|x^{k+1}-x^{k- \tau_{k}}\|^{2}+\frac{\lambda(1-\theta)}{2}\|x^{k}-x^{k-\tau_{k}}\|^{2}\) and we have, by algebraic manipulations of the momentum terms, that

\[\|x^{k+1}-y^{k}\|^{2} =\|x^{k+1}-x^{k}+x^{k}-y^{k}\|^{2}\] \[\geq\|x^{k+1}-x^{k}\|^{2}+\beta^{2}\|x^{k}-x^{k-1}\|^{2}-\beta\|x ^{k+1}-x^{k}\|^{2}-\beta\|x^{k}-x^{k-1}\|^{2}\] \[=\theta\|x^{k+1}-x^{k}\|^{2}-\beta\theta\|x^{k}-x^{k-1}\|^{2}\] (59)

and that

\[\frac{\rho\theta}{2}\|x^{k+1}-z^{k}\|^{2}\leq\rho\theta\|x^{k+1}-x^{k}\|^{2}+ \rho\beta^{2}\theta^{-1}\|x^{k}-x^{k-1}\|^{2}.\] (60)

Combining (59), (60) with (58) and taking expectation, we have

\[\frac{\gamma\theta}{2}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]- \frac{\gamma\beta\theta}{2}\|x^{k}-x^{k-1}\|^{2}\] \[\leq (1-\theta)\{\psi(x^{k})-\mathbb{E}_{k}[\psi(x^{k+1})]\}+\frac{2L_ {f}^{2}}{\gamma-\kappa}\] \[+(\rho+\kappa\beta)\theta\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]+ \rho\beta^{2}\theta^{-1}\|x^{k}-x^{k-1}\|^{2}-\frac{\theta(\rho-3\lambda-2 \kappa\beta-\kappa)}{2}\mathbb{E}_{k}[\|x^{k+1}-\hat{z}^{k}\|^{2}]\] \[+\frac{\gamma\theta^{2}-\rho\theta}{2}\|\hat{z}^{k}-z^{k}\|^{2}- \frac{(\gamma-\kappa)\theta^{2}}{2}\mathbb{E}_{k}[\|\hat{z}^{k}-z^{k+1}\|^{2}]+ \mathbb{E}_{k}[\varepsilon_{k}].\]

After re-arrangement, we arrive at

\[\frac{(\gamma-\kappa)\theta^{2}}{2}\mathbb{E}_{k}[\|\hat{z}^{k}- z^{k+1}\|^{2}]\] \[\leq (1-\theta)\{\psi(x^{k})-\mathbb{E}_{k}[\psi(x^{k+1})]\}+\frac{ \gamma\theta^{2}-\rho\theta}{2}\|\hat{z}^{k}-z^{k}\|^{2}\] \[+\big{(}\rho\beta^{2}\theta^{-1}+\frac{\gamma\beta\theta}{2} \big{)}\{\|x^{k}-x^{k-1}\|^{2}-\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]\}-\frac{ \theta(\rho-3\lambda-2\kappa\beta-\kappa)}{2}\mathbb{E}_{k}[\|x^{k+1}-\hat{z}^{ k}\|^{2}]\] \[+\frac{2L_{f}^{2}}{\gamma-\kappa}-\frac{\gamma\theta^{2}-2\theta( \rho+\kappa\beta)-2\rho\beta^{2}\theta^{-1}}{2}\mathbb{E}_{k}[\|x^{k+1}-x^{k} \|^{2}]+\mathbb{E}_{k}[\varepsilon_{k}].\] (61)Dividing both sides of (61) by \(\frac{(\gamma-\kappa)\theta^{2}}{2}\), we obtain that

\[\mathbb{E}_{k}[\|\hat{z}^{k}-z^{k+1}\|^{2}]\] \[\leq\frac{2(1-\theta)}{(\gamma-\kappa)\theta^{2}}\{\psi(x^{k})- \mathbb{E}_{k}[\psi(x^{k+1})]\}+\frac{\gamma\theta-\rho}{(\gamma-\kappa) \theta}\|\hat{z}^{k}-z^{k}\|^{2}\] \[\quad+\frac{2\rho\beta^{2}\theta^{-1}+\gamma\beta\theta}{(\gamma- \kappa)\theta^{2}}\{\|x^{k}-x^{k-1}\|^{2}-\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{ 2}]\}-\frac{\rho-3\lambda-2\kappa\beta-\kappa}{(\gamma-\kappa)\theta}\mathbb{E }_{k}[\|x^{k+1}-\hat{z}^{k}\|^{2}]\] \[\quad+\frac{4L_{f}^{2}}{(\gamma-\kappa)^{2}\theta^{2}}-\frac{ \gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta^{2}\theta^{-1}}{(\gamma -\kappa)\theta^{2}}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]+\frac{2\mathbb{E}_{ k}[\varepsilon_{k}]}{(\gamma-\kappa)\theta^{2}}\] \[=\|\hat{z}^{k}-z^{k}\|^{2}-\frac{\rho-\kappa\theta}{(\gamma- \kappa)\theta}\|\hat{z}^{k}-z^{k}\|^{2}+\frac{2\beta}{(\gamma-\kappa)\theta} \{\psi(x^{k})-\mathbb{E}_{k}[\psi(x^{k+1})]\}\] \[\quad+\frac{2\rho\beta^{2}\theta^{-1}+\gamma\beta\theta}{(\gamma- \kappa)\theta^{2}}\{\|x^{k}-x^{k-1}\|^{2}-\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{ 2}]\}-\frac{\rho-3\lambda-2\kappa\beta-\kappa}{(\gamma-\kappa)\theta}\mathbb{ E}_{k}[\|x^{k+1}-\hat{z}^{k}\|^{2}]\] \[\quad+\frac{4L_{f}^{2}}{(\gamma-\kappa)^{2}\theta^{2}}-\frac{ \gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta^{2}\theta^{-1}}{(\gamma -\kappa)\theta^{2}}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]+\frac{2\mathbb{E}_{ k}[\varepsilon_{k}]}{(\gamma-\kappa)\theta^{2}}.\] (62)

Next, we consider the Moreau envelope and

\[\mathbb{E}_{k}[\psi_{1/\rho}(z^{k+1})]\] \[=\mathbb{E}_{k}\left[\psi(\hat{z}^{k+1})+\frac{\rho}{2}\|\hat{z}^ {k+1}-z^{k+1}\|^{2}\right]\] \[\leq\mathbb{E}_{k}\left[f(\hat{z}^{k})+\omega(\hat{z}^{k})+\frac {\rho}{2}\|\hat{z}^{k}-z^{k+1}\|^{2}\right]\] \[\leq f(\hat{z}^{k})+\omega(\hat{z}^{k})+\frac{\rho}{2}\|\hat{z}^{ k}-z^{k}\|^{2}+\frac{\rho\beta}{(\gamma-\kappa)\theta^{2}}\{\psi(x^{k})- \mathbb{E}_{k}[\psi(x^{k+1})]\}\] \[\quad-\frac{\rho(\rho-\kappa\theta)}{2(\gamma-\kappa)\theta}\| \hat{z}^{k}-z^{k}\|^{2}+\frac{\rho(2\rho\beta^{2}\theta^{-1}+\gamma\beta\theta) }{2(\gamma-\kappa)\theta^{2}}\{\|x^{k}-x^{k-1}\|^{2}-\mathbb{E}_{k}[\|x^{k+1} -x^{k}\|^{2}]\}\] \[\quad+\frac{2\rho L_{f}^{2}}{(\gamma-\kappa)^{2}\theta^{2}}- \frac{\rho(\gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta^{2}\theta^{- 1})}{2(\gamma-\kappa)\theta^{2}}\mathbb{E}_{k}[[\|x^{k+1}-x^{k}\|^{2}]+\frac{ \rho\mathbb{E}_{k}[\varepsilon_{k}]}{(\gamma-\kappa)\theta^{2}}\] (63) \[=\psi_{1/\rho}(z^{k})+\frac{\rho\beta}{(\gamma-\kappa)\theta^{2}} \{\psi(x^{k})-\mathbb{E}_{k}[\psi(x^{k+1})]\}\] \[\quad-\frac{\rho(\rho-\kappa\theta)}{2(\gamma-\kappa)\theta}\| \hat{z}^{k}-z^{k}\|^{2}+\frac{\rho(2\rho\beta^{2}\theta^{-1}+\gamma\beta\theta) }{2(\gamma-\kappa)\theta^{2}}\{\|x^{k}-x^{k-1}\|^{2}-\mathbb{E}_{k}[\|x^{k+1} -x^{k}\|^{2}]\}\] \[\quad+\frac{2\rho L_{f}^{2}}{(\gamma-\kappa)^{2}\theta^{2}}- \frac{\rho(\gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta^{2}\theta^{- 1})}{2(\gamma-\kappa)\theta^{2}}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]+\frac{ \rho\mathbb{E}_{k}[\varepsilon_{k}]}{(\gamma-\kappa)\theta^{2}},\] (64)

where (63) uses the relation from (62). Last we re-arrangement the terms in (64) to get

\[\frac{\rho(\rho-\kappa\theta)}{2(\gamma-\kappa)\theta}\|\hat{z}^{ k}-z^{k}\|^{2}\] \[\leq\psi_{1/\rho}(z^{k})-\mathbb{E}_{k}[\psi_{1/\rho}(z^{k+1})]+ \frac{\rho\beta}{(\gamma-\kappa)\theta^{2}}\{\psi(x^{k})-\mathbb{E}_{k}[\psi(x^ {k+1})]\}\] \[\quad+\frac{\rho(2\rho\beta^{2}\theta^{-1}+\gamma\beta\theta)}{2( \gamma-\kappa)\theta^{2}}\{\|x^{k}-x^{k-1}\|^{2}-\mathbb{E}_{k}[\|x^{k+1}-x^{ k}\|^{2}]\}\] \[\quad+\frac{2\rho L_{f}^{2}}{(\gamma-\kappa)^{2}\theta^{2}}- \frac{\rho(\gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta^{2}\theta^{- 1})}{2(\gamma-\kappa)\theta^{2}}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]+\frac{ \rho\mathbb{E}_{k}[\varepsilon_{k}]}{(\gamma-\kappa)\theta^{2}}.\]

Recalling that \(\|\nabla\psi_{1/\rho}(\hat{z}^{k})\|^{2}=\rho^{2}\|\hat{z}^{k}-z^{k}\|^{2}\), we complete the proof.

**Proof of Lemma 11**By **Lemma 10** we have that

\[\frac{\rho-\kappa\theta}{2\rho(\gamma-\kappa)\theta}\|\hat{z}^{k}-z^ {k}\|^{2}\] \[\leq\psi_{1/\rho}(z^{k})-\mathbb{E}_{k}[\psi_{1/\rho}(z^{k+1})]+ \frac{\rho\beta}{(\gamma-\kappa)\theta^{2}}\{\psi(x^{k})-\mathbb{E}_{k}[\psi(x^ {k+1})]\}\] \[\quad+\frac{2\rho\beta^{2}\theta^{-1}+\gamma\beta\theta)}{2( \gamma-\kappa)\theta^{2}}\{\|x^{k}-x^{k-1}\|^{2}-\mathbb{E}_{k}[\|x^{k+1}-x^{ k}\|^{2}]\}\] (65) \[\quad+\frac{2\rho L_{f}^{2}}{(\gamma-\kappa)^{2}\theta^{2}}- \frac{\rho(\gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta^{2}\theta^{-1 })}{2(\gamma-\kappa)\theta^{2}}\mathbb{E}_{k}[\|x^{k+1}-x^{k}\|^{2}]+\frac{ \rho\mathbb{E}_{k}[\varepsilon_{k}]}{(\gamma-\kappa)\theta^{2}}.\]

Summing up the inequality (65) from \(k=1\) to \(K\), multiplying both sides by \(\gamma+\kappa>0\) and taking expectation, we have

\[\frac{\rho-\kappa\theta}{2\rho\theta}\mathbb{E}[\|\nabla\psi_{1/ \rho}(z^{k^{*}})\|^{2}]\] \[\quad\leq\frac{\gamma-\kappa}{K}\{\psi_{1/\rho}(z^{1})-\mathbb{E }_{k}[\psi_{1/\rho}(z^{K+1})]\}+\frac{\rho\beta}{\theta^{2}K}\{\psi(x^{k})- \mathbb{E}_{k}[\psi(x^{k+1})]\}\] \[\quad\quad+\frac{\rho(2\rho\beta^{2}\theta^{-1}+\gamma\beta \theta)}{2\theta^{2}K}\|x^{1}-x^{0}\|^{2}+\frac{2\rho L_{f}^{2}K}{(\gamma- \kappa)\theta^{2}}+\frac{\rho}{\theta^{2}K}\sum_{k=1}^{K}\mathbb{E}[ \varepsilon_{k}]\] \[\quad\quad-\frac{\rho(\gamma\theta^{2}-2\theta(\rho+\kappa\beta) -2\rho\beta^{2}\theta^{-1})}{2\theta^{2}K}\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}- x^{k}\|^{2}]\] \[\quad\leq\frac{(\gamma-\kappa+\rho\beta\theta^{-2})D}{K}+\frac{2 \rho L_{f}^{2}}{(\gamma-\kappa)\theta^{2}}+\frac{\rho}{\theta^{2}K}\sum_{k=1}^ {K}\mathbb{E}[\varepsilon_{k}]\] \[\quad\quad-\frac{\rho(\gamma\theta^{2}-2\theta(\rho+\kappa\beta) -2\rho\beta^{2}\theta^{-1})}{2\theta^{2}K}\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}- x^{k}\|^{2}]\] (66)

where the last inequality uses \(x^{0}=x^{1}\) and \(D=\max\{\psi_{1/\rho}(z^{1})-\psi_{1/\rho}(z^{*}),\psi(x^{1})-\psi(x^{*})\}\). Now we divide both sides of the inequality by \(\frac{\rho-\kappa\theta}{2\rho\theta}\) to get

\[\mathbb{E}[\|\nabla\psi_{1/\rho}(z^{k^{*}})\|^{2}]\leq\frac{2\rho\theta}{\rho -\kappa\theta}\left[\frac{(\gamma-\kappa+\rho\beta\theta^{-2})D}{K}+\frac{2 \rho L_{f}^{2}}{(\gamma-\kappa)\theta^{2}}+\frac{\rho}{\theta^{2}K}\sum_{k=1}^ {K}\mathbb{E}_{k}[\varepsilon_{k}]\right].\]

Last we bound \(\sum_{k=1}^{K}\mathbb{E}[\varepsilon_{k}]\). First, we have the following relations

\[\sum_{k=1}^{K}\|x^{k+1}-x^{k-\tau_{k}}\|^{2}\leq\tau^{2}\sum_{k=1}^{K}\|x^{k}- x^{k-1}\|^{2}\leq\tau^{2}\sum_{k=1}^{K}\|x^{k+1}-x^{k}\|^{2},\]

where the first inequality is by **E1** and the second inequality uses the fact that \(x^{0}=x^{1}\). Similarly

\[\sum_{k=1}^{K}\|x^{k+1}-x^{k-\tau_{k}}\|^{2} \leq\sum_{k=1}^{K}2[\|x^{k}-x^{k-\tau_{k}}\|^{2}+\|x^{k+1}-x^{k}\| ^{2}]\] \[\leq 2(\tau^{2}+1)\sum_{k=1}^{K}\|x^{k+1}-x^{k}\|^{2},\]

where the first inequality uses \(\|a+b\|^{2}\leq 2\|a\|^{2}+2\|b\|^{2}\) and the second inequality re-uses the bound of the first term. Plugging the above bounds back into \(\sum_{k=1}^{K}\mathbb{E}[\varepsilon_{k}]\), we successively deduce that

\[\sum_{k=1}^{K}\mathbb{E}[\varepsilon_{k}] =\sum_{k=1}^{K}\frac{\lambda(1-\theta)}{2}\|x^{k}-x^{k-\tau_{k}}\|^ {2}+\sum_{k=1}^{K}(\theta+1/2)\lambda\mathbb{E}[\|x^{k+1}-x^{k-\tau_{k}}\|^{2}]\] \[\leq\lambda[(1-\theta)(\tau^{2}+1)+(\theta+1/2)\tau^{2}]\sum_{k=1 }^{K}\mathbb{E}[\|x^{k+1}-x^{k}\|^{2}]\] \[=\lambda(3\tau^{2}/2+\beta)\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}-x^{ k}\|^{2}],\]

which implies

\[\mathbb{E}[\|\nabla\psi_{1/\rho}(z^{k^{*}})\|^{2}]\] \[\leq\frac{2\rho\theta}{\rho-\kappa\theta}\left[\frac{(\gamma- \kappa+\rho\beta\theta^{-2})D}{K}+\frac{2\rho L_{f}^{2}}{(\gamma-\kappa) \theta^{2}}+\frac{\rho\lambda(3\tau^{2}+2\beta)}{2\theta^{2}K}\sum_{k=1}^{K} \mathbb{E}[\|x^{k+1}-x^{k}\|^{2}]\right],\] (67)

and this completes the proof.

**Proof of Theorem 8**

By **Lemma 11**, it remains to bound the quantity \(\sum_{k=1}^{K}\mathbb{E}[\|x^{k+1}-x^{k}\|^{2}]\). To this end we consider the relation in (66), where we have

\[\frac{\rho(\gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta^{2}\theta^{- 1}-3\lambda\tau^{2}-2\lambda\beta)}{2\theta^{2}K}\sum_{k=1}^{K}\mathbb{E}[\|x^ {k+1}-x^{k}\|^{2}]\]

\[\leq\frac{(\gamma-\kappa+\rho\beta\theta^{-2})D}{K}+\frac{2\rho L_{f}^{2}}{( \gamma-\kappa)\theta^{2}}.\]

Since we choose \(\beta,\gamma\) such that \(\gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta^{2}\theta^{-1}-3 \lambda\tau^{2}-2\lambda\beta>0\), then

\[\frac{\rho\lambda(3\tau^{2}+2\beta)}{2\theta^{2}K}\sum_{k=1}^{K}\mathbb{E}[\|x ^{k+1}-x^{k}\|^{2}]\leq\frac{\lambda(3\tau^{2}+2\beta)\left\{\frac{(\gamma- \kappa+\rho\beta\theta^{-2})D}{K}+\frac{2\rho L_{f}^{2}}{(\gamma-\kappa) \theta^{2}}\right\}}{\gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta^{2 }\theta^{-1}-3\lambda\tau^{2}-2\lambda\beta}.\]

Plugging the bound back, we have

\[\mathbb{E}[\|\nabla\psi_{1/\rho}(z^{k^{*}})\|^{2}]\] \[\leq\frac{2\rho\theta}{\rho-\kappa\theta}\Bigg{[}\frac{(\gamma- \kappa+\rho\beta\theta^{-2})D}{K}+\frac{2\rho L_{f}^{2}}{(\gamma-\kappa) \theta^{2}}+\frac{\lambda(3\tau^{2}+2\beta)\left\{\frac{(\gamma-\kappa+\rho \beta\theta^{-2})D}{K}+\frac{2\rho L_{f}^{2}}{(\gamma-\kappa)\theta^{2}} \right\}}{\gamma\theta^{2}-2\theta(\rho+\kappa\beta)-2\rho\beta^{2}\theta^{-1 }-3\lambda\tau^{2}-2\lambda\beta}\Bigg{]}.\]

This completes our proof.

## Appendix G Kernel and subproblems

In this section, we delve into the practical considerations related to Bregman proximal methods. In particular, we discuss how to construct suitable divergence kernels for DSPL so that we can accommodate non-Lipschitzness of the objective function. We also propose efficient subroutines for solving Bregman proximal subproblems, which are applicable to a wide range of loss functions typically encountered in machine learning tasks.

### Choosing the divergence kernel

The selection of the kernel function is a critical aspect of the Bregman proximal method. The choice of kernel has been widely discussed in the literature [3, 6], and we incorporate these findings to construct suitable kernels for the DSPL method. Given the assumptions we have made throughout the paper, it suffices to consider kernels \(d(x)=\mathcal{P}_{n}^{p}(\|x\|)\coloneqq\sum_{k=0}^{n}p_{k}\|x\|^{k}\), which are degree-\(n\) polynomials in \(\|x\|\). We recall the following lemma and refer the readers to [3] for its proof.

**Lemma 12**.: _Given \(f(x)\) such that \(\frac{f(x)-f(y)}{\|x-y\|}\leq\mathcal{P}_{n}^{p}(\|x\|)+\mathcal{P}_{n}^{p}(\|y\| \|)=\sum_{k=0}^{n}p_{k}(\|x\|^{k}+\|y\|^{2}),p\geq 0,\forall x\in\operatorname{dom}h\), then \(f\) satisfies \(1\)-relative Lipschitzian property with kernel \(d(x)=\sum_{k=0}^{n}\frac{3k+7}{k+2}p_{k}\|x\|^{k+2}\)._

The above lemma implies that once we establish a polynomial upper bound on the Lipschitzness of \(f_{z}(x,\xi)+\omega(x)\), a kernel \(d\) is immediately available.

We note that the above kernel covers most of the applications [2] of the prox-linear method, including phase retrieval, blind deconvolution, matrix completion, covariance estimation, robust PCA, and so on. In many machine learning applications, \(c\) has Lipschitz continuous gradient, meaning that \(\nabla c\) is bounded by first-order growth. For example, in the phase retrieval problem, we have \(\omega=0,h(x)=|x|,c(x,\xi)=\langle a,x\rangle^{2}-b\). Hence, we get

\[\|\nabla c(z,\xi)\|=2|\langle a,z\rangle|\cdot\|a\|\leq 2\|a\|^{2}\|z\|\text{ and } \frac{f_{z}(x,\xi)-f_{z}(y,\xi)}{\|x-y\|}\leq 2\|a\|^{2}\|z\|.\]

Now assume that \(z=x\), then for any \(p_{0}\geq 0\) and \(p_{1}=2\alpha^{2},\alpha=\sup_{a\sim\Xi}\|a\|\) we know that \(2\|a\|^{2}\|x\|\leq p_{0}+p_{1}\|x\|\). Hence, we can take \(d(x)=\frac{7}{2}\|x\|^{2}+\frac{10\alpha^{2}}{3}\|x\|^{3}\) as our kernel. Recall that in \(\texttt{DSPL}\)\(z=x^{k-\tau_{k}},x=x^{k}\), as long as there exist some \(M,N\) such that \(\|x^{j}\|\leq M\|x^{k}\|+N,k\geq j\) for all \(k\), our assumptions are satisfied.

Now that we have specified ways to construct kernels for DSPL, we discuss how to solve the Bregman proximal subproblems in the next section. The subproblems' solution determines the practical applicability of our methods, and we show that these subproblems can be efficiently solved for a wide range of problems.

### Bregman proximal subproblem

In this section, we discuss the solution of the proximal subproblems for DSPL when \(\omega=0\), where the Bregman proximal subproblem can then be expressed as

\[\min_{x}\quad h(\langle a,x\rangle+b)+V_{d}(x,y).\] (68)

Consider a piece-wise linear \(h(x)=\max\{\alpha_{1}x+\beta_{1},\alpha_{2}x+\beta_{2}\}\). This formulation characterizes common nonsmooth loss functions including \(\ell_{1}\) loss \(|x|=\max\{x,-x\}\) and hinge loss \(\max\{0,x\}\). Substituting \(h\) in and removing terms irrelevant to \(x\), we arrive at the following DSPL subproblem.

\[\min_{x}\quad\max\{\langle a_{1},x\rangle+b_{1},\langle a_{2},x \rangle+b_{2}\}+d(x)\] (69)

The next proposition provides a solution to the above subproblem.

**Proposition 3** (Solving the proximal subproblem).: _The proximal subproblem can be solved by evaluating solutions to the following three problems_

\[\min_{x}\,\langle a_{1},x\rangle+d(x),\] \[\min_{x}\,\langle a_{2},x\rangle+d(x),\]

_and_

\[\min_{x}\quad\quad\quad\quad\langle a_{1},x\rangle+d(x)\] \[\operatorname{subject\ to}\quad\langle a_{1}-a_{2},x\rangle+b_{1}- b_{2}=0,\]

_where the solution to the third problem satisfies \(x=u+\alpha v\), where \(u=-\frac{(a_{1}-a_{2})(b_{1}-b_{2})}{\|a_{1}-a_{2}\|^{2}}\), \(v=\frac{(\|a_{1}\|^{2}-\|a_{2}\|^{2})}{2\|a_{1}-a_{2}\|^{2}}(a_{1}-a_{2})- \frac{a_{1}+a_{2}}{2}\) and \(\alpha\) is a positive root of the following equation_

\[\sum_{k=0}^{n}p_{k}\alpha\|u+\alpha v\|^{k}-1=0.\]

Proof.: Recall that we are solving the following convex optimization problem

\[\min_{x}\quad\max\{\langle a_{1},x\rangle+b_{1},\langle a_{2},x \rangle+b_{2}\}+d(x),\]where \(d\) is strongly convex and thus the problem admits a unique minimizer \(x^{*}\). Then we do case analysis.

**Case 1**. \(\langle a_{1},x^{*}\rangle+b_{1}>\langle a_{2},x^{*}\rangle+b_{2}\). In this case solving \(\min_{x}\langle a_{1},x\rangle+d(x)\) produces \(x^{*}\).

**Case 2**. \(\langle a_{1},x^{*}\rangle+b_{1}<\langle a_{2},x^{*}\rangle+b_{2}\). In this case solving \(\min_{x}\langle a_{2},x\rangle+d(x)\) produces \(x^{*}\).

The above two cases degenerate into the subproblems from stochastic mirror descent and its solution has been discussed in [6]. The last case is a bit more tricky.

**Case 3**. \(\langle a_{1},x^{*}\rangle+b_{1}=\langle a_{2},x^{*}\rangle+b_{2}\).

In this case, the proximal subproblem becomes an equality constrained problem.

\[\min_{x} \langle a_{1},x\rangle+d(x)\] subject to \[\langle a_{1}-a_{2},x\rangle+b_{1}-b_{2}=0\]

Letting \(\lambda\) be the multiplier of the equality constraint and appealing to the optimality condition, we have

\[a_{1}+\nabla d(x)+\lambda(a_{1}-a_{2}) =0\] (70) \[a_{2}+\nabla d(x)+\lambda(a_{1}-a_{2}) =0\] (71) \[\langle a_{1}-a_{2},x\rangle+b_{1}-b_{2} =0.\] (72)

Summing (70) (71) and dividing both sides by 2, we have

\[\tfrac{a_{1}+a_{2}}{2}+\nabla d(x)+\lambda(a_{1}-a_{2})=0.\] (73)

Multiplying both sides of (73) by \(a_{1}-a_{2}\), we arrive at

\[\tfrac{\|a_{1}\|^{2}-\|a_{2}\|^{2}}{2}+\langle a_{1}-a_{2},\nabla d(x)\rangle +\lambda\|a_{1}-a_{2}\|^{2}=0.\] (74)

Since \(d(x)\) is a polynomial in \(\|x\|\), \(\nabla d(x)=\zeta x,\zeta>0\) and using (72), we have

\[\lambda=\tfrac{b_{1}-b_{2}}{\|a_{1}-a_{2}\|^{2}}\zeta-\tfrac{\|a_{1}\|^{2}-\|a _{2}\|^{2}}{2\|a_{1}-a_{2}\|^{2}}.\] (75)

Then substituting \(\lambda\) into (74),

\[0 =\tfrac{a_{1}+a_{2}}{2}+\nabla d(x)+\lambda(a_{1}-a_{2})\] \[=\tfrac{a_{1}+a_{2}}{2}+\zeta x+\zeta\tfrac{(a_{1}-a_{2})(b_{1}-b _{2})}{\|a_{1}-a_{2}\|^{2}}-\tfrac{\|a_{1}\|^{2}-\|a_{2}\|^{2}}{2\|a_{1}-a_{2 }\|^{2}}(a_{1}-a_{2})\] \[=\zeta\left[x+\tfrac{(a_{1}-a_{2})(b_{1}-b_{2})}{\|a_{1}-a_{2}\|^ {2}}\right]-\tfrac{\|a_{1}\|^{2}-\|a_{2}\|^{2}}{2\|a_{1}-a_{2}\|^{2}}(a_{1}-a_ {2})+\tfrac{a_{1}+a_{2}}{2}\]

we get

\[x=\zeta^{-1}\left[\tfrac{(\|a_{1}\|^{2}-\|a_{2}\|^{2})}{2\|a_{1}-a_{2}\|^{2}} (a_{1}-a_{2})-\tfrac{a_{1}+a_{2}}{2}\right]-\tfrac{(a_{1}-a_{2})(b_{1}-b_{2}) }{\|a_{1}-a_{2}\|^{2}}\]

for some \(\zeta>0\). Without loss of generality, we express \(x=u+\alpha v\) for \(u=-\tfrac{(a_{1}-a_{2})(b_{1}-b_{2})}{\|a_{1}-a_{2}\|^{2}}\) and \(v=\tfrac{(\|a_{1}\|^{2}-\|a_{2}\|^{2})}{2\|a_{1}-a_{2}\|^{2}}(a_{1}-a_{2})- \tfrac{a_{1}+a_{2}}{2}\) and \(\alpha>0\). Substituting back gives \(\zeta=\sum_{k=0}^{n}p_{k}\|x\|^{k}\) and

\[\|\zeta(x-u)-v\|=\|\alpha\zeta v-v\|=|\alpha\zeta-1|\cdot\|v\|=0.\]

Therefore, we only need to verify the positive roots of \(\sum_{k=0}^{n}p_{k}\alpha\|u+\alpha v\|^{k}-1=0\), which can be done efficiently if \(n\) is not large. This completes the proof. 

### Euclidean subproblem with nonsmooth regularizer

In this section, we discuss the solution of the proximal subproblems for DSPL when \(d(x)=\tfrac{1}{2}\|x\|^{2}\) and \(\omega\) takes on one of two forms: **1)**. Indicator function of ball \(\delta_{\{\|x\|\leq R\}}\). **2)**. \(\ell_{1}\) regularizer \(\|x\|_{1}\). We still assume \(h(x)=\max\{\alpha_{1}x+\beta_{1},\alpha_{2}x+\beta_{2}\}\), which covers all the popular applications of SPL from [2, 4]. Following the same argument as in the previous section, each DSPL iteration is reduced to

\[\min_{x} \max\{\langle a_{1},x\rangle+b_{1},\langle a_{2},x\rangle+b_{2}\}+ \omega(x)+\tfrac{1}{2}\|x-y\|^{2}\]and after doing case analysis on \(\langle a_{1},x^{*}\rangle+b_{1}\) and \(\langle a_{2},x^{*}\rangle+b_{2}\) as in **Proposition 3**, it suffices to solve

\[\min_{x} \quad\omega(x)+\tfrac{1}{2}\|x-y\|^{2}\] subject to \[\quad\quad\langle a,x\rangle-b=0\]

efficiently.

Case 1.\(\omega(x)=\delta_{\{\|x\|\leq R\}}\). In this case, we need to solve

\[\min_{x} \quad\tfrac{1}{2}\|x-y\|^{2}\] subject to \[\quad\quad\langle a,x\rangle=b\] \[\|x\|^{2}\leq r=R^{2},\]

whose KKT condition is given by

\[\langle a,x\rangle =b\] \[\lambda \geq 0\] \[\lambda(\|x\|^{2}-r) =0\] \[x-y+\nu a+2\lambda x =0\]

After simplification, we arrive at

\[x=\frac{y}{1+2\lambda}-\frac{\langle a,y\rangle a-(1+2\lambda)ba}{(1+2 \lambda)\|a\|^{2}}.\]

Then either \(\lambda=0\) or the solution can be obtained using bisection.

Case 2.\(\omega(x)=\|x\|_{1}\). In this case, we need to solve

\[\min_{x} \quad\|x\|_{1}+\tfrac{1}{2}\|x-y\|^{2}\] subject to \[\quad\quad\langle a,x\rangle-b=0\]

Given the Lagrangian multiplier \(\nu\) associated with the equality constraint, we have that \(x(\nu)=\mathcal{S}(y-\nu a)\), which is given by the soft-thresholding operator. The residual of the equality constraint is given by:

\[f(\nu)=\langle a,x\rangle-b=\langle a,x(\nu)\rangle-b.\]

This function is univariate in \(\nu\) and is semi-smooth. Therefore, semi-smooth Newton method can efficiently find its root. This makes the computation tractable.

## Appendix H Additional experiments

In this section, we present additional experiments incorporating momentum. Our experiments focus on both robust phase retrieval and blind deconvolution problems.

### Robust phase retrieval

We refer readers to Section 6 for the detailed formulation of phase retrieval. Most of the experiment settings in this section are consistent with Section 6.

#### h.1.1 Experiment setup

**1) Initial point and radius**. For the synthetic data, we generate \(x^{\prime}\sim\mathcal{N}(0,I_{n})\) and start from \(x^{0}=x^{1}=\frac{x^{\prime}}{\|x^{\prime}\|}\) and for zipcode data, we generate \(x^{\prime}\sim\mathcal{N}(\hat{x},I_{n})\) and take \(x^{0}=x^{1}=x^{\prime}\). \(M=1000\|x^{0}\|\).

**2) Stepsize**. We tune the stepsize parameter setting \(\gamma=\sqrt{K/\alpha}\), where \(\alpha\in\{0.1,0.5,1.0\}\) in the asynchronous environment, \(\alpha\in[10^{-2},10^{1}]\) for synthetic data in the simulated environment and \(\alpha\in[10^{0},10^{1}]\) for the zipcode dataset.
**3) Momentum parameter**. In the asynchronous environment, we allow \(\beta\in\{0,0.1,0.3,0.6\}\) and in the simulated environment, we test \(\beta\in\{0.6,0.9\}\) for the synthetic and zipcode data respectively.
**4) Others**. Other settings are consistent with Section 6.

#### h.1.2 Asynchronous environment

The first figure plots the effect of different momentum parameters in the distributed environment. It can be seen that both DSEGD and DSEPL perform better with momentum. Moreover, the performance of DSEPL dominate DSEGD as we observed in the previous experiments.

#### h.1.3 Simulated Environment

Figure 7 plots the impact of staleness on the number of iterations each algorithm takes to reach the desired accuracy. It can be seen that with other parameters fixed, DSPL tends to be more robust against delays than the pure subgradient-based methods, which is consistent with the theoretical results. Moreover, we observe that when extrapolation is used, the algorithm converges faster at the cost of less robustness as delay increases.

Our last experiment investigates the robustness of DSPL compared to DSGD and justifies the use of extrapolation in presence of delay. Figure 8 plots the number of iterations for each algorithm to converge with different datasets, extrapolation parameters, and delays. In spite of delays, DSPL and DSEPL still admit a wider range of stepsize parameters ensuring convergence than DSGD. Also, when the stepsize is not large, the use of extrapolation can effectively accelerate the convergence.

### Blind deconvolution problem

In this section, we present the additional experiments on blind deconvolution problem to further illustrate the efficiency of DSPL. The blind deconvolution problem, unlike robust phase retrieval,

Figure 6: First: speedup in time and the number of workers. Second: progress of \(\|x^{k}-\hat{x}\|\) in the first 40 epochs given 16 workers and \(\alpha=0.5\). Third: progress of \(f(x^{k})-f(\hat{x})\) in the first 40 epochs given \(\beta=0.3\) and different number of workers.

Figure 7: Left to right: \((\kappa,p_{\text{fail}})=(10,0.3),\alpha=5.0\), Geometric and Poisson delays; zipcode data of \(p_{\text{fail}}=0.2\),\(\alpha=6.0\), Geometric and Poisson delays. x-axis represents \(\tau_{\text{max}}\) and y-axis shows average iteration number to reach the stopping criterion over 20 tests.

aims to recover two signals from their convolution and its mathematical formulation is given by

\[\min_{x,y\in\mathbb{R}^{n}}\frac{1}{m}\sum_{i=1}^{m}|\langle u_{i},x\rangle\langle v _{i},y\rangle-b_{i}|+\mathbb{I}\{\|x\|,\|y\|\leq\Delta\},\]

where \(\{b_{i}\}\) are the measurements, \(\{(u_{i},v_{i})\}\) are the measuring data and \(x,y\) are the optimization variables corresponding to the signals. Similar to phase retrieval, we first present the detailed setup and then inspect the performance of DSPL in both real and simulated asynchronous environments.

#### h.2.1 Experiment setup

**Data generation.** We use synthetic data for blind deconvolution problems.

**Synthetic data**. We take \(m=300,n=100\) in the experiments of simulated delay and \(m=1500,n=150\) in the asynchronous environment. The data is generated similar to phase retrieval, where, given some conditioning parameter \(\kappa\geq 1\), we compute \(U=Q_{1}D_{2},V=Q_{2}D_{2},Q\in\mathbb{R}^{m\times n},q_{ij}\sim\mathcal{N}(0,1)\) and \(D=\text{diag}(d),d\in\mathbb{R}^{n},d_{i}\in[1/\kappa,1],\forall i\). Then two true signals are generated like \(\hat{x}\) in phase retrieval and we random corruption is applied.

1. **Dataset**. In the asynchronous environment, we set \(\kappa=1,p_{\text{fail}}=0\) and in the simulated environment, we set \(\kappa=1\) and \(p_{\text{fail}}\in\{0.2,0.3\}\).
2. **Initial point and radius**. We generate \(x^{\prime},y^{\prime}\sim\mathcal{N}(0,I_{n})\) and start from \(x^{0}=x^{1}=\frac{x^{\prime}}{\|x^{\prime}\|},y^{0}=y^{1}=\frac{y^{\prime}}{ \|y^{\prime}\|}\). \(\Delta=1000\|(x^{0},y^{0})\|\).
3. **Stepsize**. We tune the stepsize parameter setting \(\gamma=\sqrt{K/\alpha}\), where \(\alpha\in\{0.1,0.5,1.0\}\) in the asynchronous environment, \(\alpha\in[10^{-2},10^{1}]\) for synthetic data in the simulated environment.

The rest of the experiment setup are consistent with in phase retrieval.

#### h.2.2 Asynchronous environment

The experiments for blind deconvolution again justifies the effect of DSPL in its convergence behavior and robustness to the stochastic delays. By retaining the smooth structure of the inner composite function, DSPL gives a more accurate approximation of the original objective function only at cost of transmitting one more scaler (i.e., the inner objective value). Hence DSPL serves as a competitive alternative to DSGD when the problem enjoys composite structure (2) in the distributed setting.

#### h.2.3 Simulated environment

In the simulated experiments for blind deconvolution, we can see similar robustness of DSPL against delay and stepsize selection, which confirms our previous observations.

### Proximal sub-problems

This subsection shows how the sub-problems from DSPL and DSPL are solved for completeness. The results are a direct adaptation from [1] and interested readers can check [1] for the detailed derivations.

Phase retrieval problemIn phase retrieval problem, we denote \(x\) to be the point where the stochastic model function is constructed and \(y\) to be the center of the proximal term. Then the DSPL proximal subproblem is given by

\[\min_{x}\ \big{|}\langle a,z\rangle^{2}+2\langle a,z\rangle\langle a,x-z \rangle-b\big{|}+\frac{\gamma}{2}\|x-y\|^{2}\]

and it admits closed-form solution when away from the boundary

\[x^{+}=y+\text{Proj}_{[-1,1]}(-\frac{\delta}{\|\zeta\|^{2}})\zeta,\]

where \(\delta=\gamma^{-1}(\langle a,z\rangle^{2}+2\langle a,z\rangle\langle a,x-z \rangle-b)\) and \(\zeta=2\gamma^{-1}\langle a,z\rangle a\) and \(\text{Proj}_{[-1,1]}(\cdot)\) denotes projection onto the box \([-1,1]\).

Blind deconvolution problemIn the blind deconvolution problem, we use \(z=(z_{x},z_{y})\) to denote the point of model construction and \(w=(w_{x},w_{y})\) to denote the proximal center. Then the subproblem is given by

\[\min_{\|x\|,\|y\|\leq\Delta}\ |\langle u,z_{x}\rangle\langle v,z_{y}\rangle+ \langle v,z_{y}\rangle\langle u,x-z_{x}\rangle+\langle u,z_{x}\rangle\langle v,y-z_{y}\rangle-b|+\frac{\gamma}{2}[\|x-w_{x}\|^{2}+\|y-w_{y}\|^{2}].\]

First we assume that the constraints are inactive, then the solution is available in closed form

\[w^{+}=w+\text{Proj}_{[-1,1]}(-\frac{\delta}{\|\zeta\|^{2}})\zeta,\]

Figure 10: From left to right: \((\kappa,p_{\text{fail}})=(1,0.2),\alpha=4.6\), Geometric and Poisson delays; \((\kappa,p_{\text{fail}})=(1,0.3)\),\(\alpha=2.1\), Geometric and Poisson delays. The x-axis represents delay \(\tau\) and the y-axis gives the number of iterations to reach the stopping criterion.

Figure 9: First: speedup in time and the number of workers. Second: progress of \(\|(x^{k},y^{k})-(\hat{x},\hat{y})\|\) in the first 40 epochs given 16 workers and \(\alpha=0.5\). Third: progress of \(f(x^{k},y^{k})-f(\hat{x},\hat{y})\) in the first 40 epochs given \(\beta=0.1\) and different number of workers.

where

\[\delta =\gamma^{-1}\left[\langle u,z_{x}\rangle\langle v,z_{y}\rangle+ \langle v,z_{y}\rangle\langle u,w_{x}-z_{x}\rangle+\langle u,z_{x}\rangle\langle v,w_{y}-z_{y}\rangle-b\right]\] \[\zeta =\gamma^{-1}(\langle v,z_{y}\rangle u,\langle u,z_{x}\rangle v).\]

### Separate figures of Section 6

## References in the appendix

* [1] Damek Davis and Dmitriy Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. _SIAM Journal on Optimization_, 29(1):207-239, 2019.
* [2] Damek Davis and Dmitriy Drusvyatskiy. Graphical convergence of subgradients in nonconvex optimization and learning. _Mathematics of Operations Research_, 47(1):209-231, 2022.
* [3] Damek Davis, Dmitriy Drusvyatskiy, and Kellie J MacPhee. Stochastic model-based minimization under high-order growth. _arXiv preprint arXiv:1807.00255_, 2018.
* [4] Damek Davis and Benjamin Grimmer. Proximally guided stochastic subgradient method for nonsmooth, nonconvex problems. _SIAM Journal on Optimization_, 29(3):1908-1930, 2019.
* [5] Qi Deng and Wenzhi Gao. Minibatch and momentum model-based methods for stochastic weakly convex optimization. _Advances in Neural Information Processing Systems_, 34, 2021.
* [6] Haihao Lu. "relative continuity" for non-lipschitz nonsmooth convex optimization using stochastic (or deterministic) mirror descent. _INFORMS Journal on Optimization_, 1(4):288-303, 2019.
* [7] R Tyrrell Rockafellar. Convex analysis princeton university press. _Princeton, NJ_, 1970.
* [8] Siqi Zhang and Niao He. On the convergence rate of stochastic mirror descent for nonsmooth nonconvex optimization. _arXiv preprint arXiv:1806.04781_, 2018.

Figure 11: From left to right: \((\kappa,p_{\text{fail}})=(1,0.2)\), Geometric and Poisson delays with \(\tau=10\); \((\kappa,p_{\text{fail}})=(1,0.3)\), Geometric and Poisson delays with \(\tau=216\).

Figure 12: Figure 2 after splitting DSGD and DSPL. Two plots on the left: \(\|x^{k}-\hat{x}\|\) in the first 40 epochs given \(\alpha=0.1\); two on the right: \(f(x^{k})-f(\hat{x})\) in the first 40 epochs given \(\alpha=0.5\).