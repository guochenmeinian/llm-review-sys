# On kernel-based statistical learning in the mean field limit

Christian Fiedler\({}^{1}\) Michael Herty\({}^{2}\) Sebastian Trimpe\({}^{3}\)

\({}^{1,3}\)Institute for Data Science in Mechanical Engineering (DSME)

RWTH Aachen University, Aachen, Germany

{fiedler,trimpe}@dsme.rwth-aachen.de

\({}^{2}\) Institute for Geometry and Practical Mathematics (IGPM)

RWTH Aachen University, Aachen, Germany

herty@igpm.rwth-aachen.de

###### Abstract

In many applications of machine learning, a large number of variables are considered. Motivated by machine learning of interacting particle systems, we consider the situation when the number of input variables goes to infinity. First, we continue the recent investigation of the mean field limit of kernels and their reproducing kernel Hilbert spaces, completing the existing theory. Next, we provide results relevant for approximation with such kernels in the mean field limit, including a representer theorem. Finally, we use these kernels in the context of statistical learning in the mean field limit, focusing on Support Vector Machines. In particular, we show mean field convergence of empirical and infinite-sample solutions as well as the convergence of the corresponding risks. On the one hand, our results establish rigorous mean field limits in the context of kernel methods, providing new theoretical tools and insights for large-scale problems. On the other hand, our setting corresponds to a new form of limit of learning problems, which seems to have not been investigated yet in the statistical learning theory literature.

## 1 Introduction

Models with many variables play an important role in many fields of mathematical and physical sciences. In this context, going to the limit of infinitely many variables is an important analysis and modeling approach. Interacting particle systems are a classic example; these are usually modeled as dynamical systems describing the temporal evolution of many interacting objects. In physics, such systems were first investigated in the context of gas dynamics, cf. [1]. Since even small volumes of gases typically contain an enormous number of molecules, a microscopic modeling approach quickly becomes infeasible and one considers the evolution of densities instead [2]. In the past decades, interacting particle systems arising from many different domains have been considered, for example, animal movement (inter alia swarms of birds, schools of fish, colonies of microorganisms) [3; 4], social and political dynamics [5; 6], crowd modeling and control (pedestrian movement, gathering at large events like football games or concerts) [7; 8; 9], swarms of robots [10; 11; 12] or vehicular traffic (in particular, traffic jams) [13]. There is now a vast literature on such applications, and we refer to the surveys [14; 15; 16] as starting points. A prototypical example of such a system is given by \(\dot{x}_{i}=\frac{1}{M}\sum_{j=1}^{M}\phi(x_{i},x_{j})(x_{j}-x_{i})\), for \(i=1,\ldots,M\), where \(M\in\mathbb{N}_{+}\) particles or agents are modelled by their state \(x_{i}\in\mathbb{R}^{d}\), \(i=1,\ldots,M\), evolving according to some interaction rule \(\phi:\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\mathbb{R}\). Typical questions then concern the long-term behavior of such systems, in particular, emergent phenomena like consensus or alignment [17]. While first-principles modelinghas been very successful for interacting particle systems in physical domains, using this approach to model the interaction rules in complex domains like social and opinion dynamics, pedestrian and animal movement or vehicular traffic, can be problematic. Therefore, learning interaction rules from data has been recently intensively investigated, for example, in the pioneering works [18, 19]. The data consists typically of (sampled) trajectories of the particle states, potentially with measurement noise, and the goal is to learn a good approximation of the interaction rule \(\phi\).

Our work is motivated by a related problem. Frequently, the state of such a complex multiagent system can be easily measured or estimated, e.g., by video recordings or image snapshots for bird swarms or schools of fish, and microscopy recordings for microorganism colonies; aerial imaging for human crowds (e.g., via quadcopters); and polling and social media analysis for opinion dynamics. However, some interesting features of the whole system might be more difficult to measure. For example, how a swarm of birds or a school of fish will react to an external stimulus (like an approaching predator), given the current state of the population. Such a reaction could be a change of density or spread of the population, or a change in mean velocity. Another example is given by features of a society in opinion dynamics (average happiness, aggression potential, susceptibility to adversarial interventions), given the current "opinion state". Measuring such features can be difficult, for example, due to a required intervention. Formally, such a feature is a functional \(F_{M}:(\mathbb{R}^{d})^{M}\to\mathbb{R}\) of the current state of the system, and since the state is often easy to measure, it would be useful to have an explicit mapping from state to feature of interest. However, since first principles modeling is unlikely to be successful in the domains considered here, it is promising to learn such a mapping from data. We can formalize this as a standard supervised learning task: The data set consists of \(D_{N}^{[M]}=((\vec{x}_{1},y_{1}),\ldots,(\vec{x}_{N},y_{N}))\), where \(\vec{x}_{n}\in(\mathbb{R}^{d})^{M}\) are snapshot measurements of the particle states (corresponding to the input of the functional) and \(y_{n}\in\mathbb{R}\) is the value of the functional of interest, potentially with measurement noise, at snapshot state \(\vec{x}_{n}\). Let us assume an additive noise model, i.e., \(y_{n}=F_{M}(\vec{x}_{n})+\epsilon_{n}\) for \(n=1,\ldots,N\), where \(\epsilon_{1},\ldots,\epsilon_{N}\in\mathbb{R}\) are noise variables. This is now a regression problem that could be solved, for example, using a Support Vector Machine (SVM) [20]. Note that for this we need a kernel \(k_{M}:(\mathbb{R}^{d})^{M}\times(\mathbb{R}^{d})^{\mathbb{H}}\to\mathbb{R}\) on \((\mathbb{R}^{d})^{M}\).

Similarly to classical physical examples like gas dynamics, the case of a large number of particles is also relevant in modern complex interacting particle systems. Since this poses computational and modeling challenges, it can be advantageous to go also here to a kinetic level and model the evolution of the particle distribution instead of every individual particle. It is well-established how to derive a kinetic partial differential equation from ordinary differential equations systems on the particle level, for example, using the Boltzmann equation or via a mean field limit, cf. [17] for an overview in the context of multi-agent systems. Formally, instead of trajectories of particle states of the form \([0,T]\ni t\mapsto\vec{x}(t)\in(\mathbb{R}^{d})^{M}\), we then have trajectories of probability measures \([0,T]\ni t\mapsto\mu(t)\in\mathcal{P}(\mathbb{R}^{d})\). This immediately raises the question of whether the learning setup outlined above also allows a corresponding kinetic limit. More precisely, let \(K\subseteq\mathbb{R}^{d}\) be compact and assume that all particles remain confined to this compactum, i.e., \(x_{i}(t)\in K\) for all \(i=1,\ldots,M\) and all \(t\in[0,T]\) under the microscopic dynamics.1 If the underlying dynamics have a mean field limit, then it is reasonable to assume that the finite-input functionals \(F_{M}:K^{M}\to\mathbb{R}\) converge also in mean field to some \(F:\mathcal{P}(K)\to\mathbb{R}\) for \(M\to\infty\), see Section 2 for a precise definition of this notion. In turn, we can now formulate a corresponding learning problem on the mean field level: A data set is then given by \(D_{N}=((\mu_{1},y_{1}),\ldots,(\mu_{N},y_{N}))\), where \(\mu_{n}\in\mathcal{P}(K)\) are snapshots of the particle state distribution over time and \(y_{n}\in\mathbb{R}\) are again potentially noisy measurements of the functional. Assuming an additive noise model, this corresponds to \(y_{n}=F(\mu_{n})+\epsilon_{n}\), \(n=1,\ldots,N\). If we want to use an SVM on the kinetic level, we need a kernel \(k:\mathcal{P}(K)\times\mathcal{P}(K)\to\mathbb{R}\) on probability distributions. There are several options available for this, see e.g. [21]. However, assuming that all ingredients of the learning problem arise as a mean field limit, this naturally leads to the question of whether a mean field limit of kernels exists, and what this means for the relation of the learning problems on the finite-input and kinetic level. In [22], this reasoning has motivated the introduction and investigation of the mean field limit of kernels. In the present work, we extend the theory of these kernels and investigate them in the context of statistical learning theory. In particular, since in practice one would use the mean field kernels on microscopic data with large, but finite \(M\), we need convergence results of the various objects appearing in statistical learning with kernels. Exactly such results are provided in Section 4.

Footnote 1: This means the dynamics on the level of individual particles.

Finally, we would like to stress that the technical developments here are independent of the motivation outlined above, in that they apply to mean field limits of functions and kernels that do not necessarily arise form the dynamics of interacting particle systems.

ContributionsOur contributions cover three closely related aspects. 1) We extend and complete the theory of mean field limit kernels and their RKHSs (Section 2). In Theorem 2.3, we precisely describe the relationship between the RKHS of the finite-input kernels and the RKHS of the mean field kernel, completing the results from [22]. In particular, this allows us to interpret the latter RKHS as the mean field limit of the former RKHSs. Furthermore, in Lemma 2.4 and 2.5, we provide inequalities for the corresponding RKHS norms, which are necessary for \(\Gamma\)-convergence arguments. 2) We provide results relevant for approximation with mean field limit kernels (Section 3). With Proposition 3.1, we give a first result on the approximation power of mean field limit kernels, and in Theorem 3.3 we can also provide a representer theorem for these kernels. For its proof, we use a \(\Gamma\)-convergence argument, which is to the best of our knowledge the first time this technique has been used in the context of kernel methods. 3) We investigate the mean field limit of kernels in the context of statistical learning theory (Section 4). We first establish an appropriate mean field limit setup for statistical learning problems, based on a slightly stronger mean field limit existence result than available so far, cf. Proposition 2.1. To the best of our knowledge, this is a new form of a limit for learning problems. In this setup, we then provide existence, uniqueness, and representer theorems for empirical and (using an apparently new notion of mean field convergence of probability distributions) infinite-sample solutions of SVMs, cf. Proposition 4.3 and 4.5. Finally, under a uniformity assumption, we can also establish convergence of the minimal risks in Proposition 4.7.

Our developments are relevant from two different perspectives: on the one hand, they constitute a theoretical proof-of-concept that the mean field limit can be "pulled through" the (kernel-based) statistical learning theory setup. In particular, this demonstrates that rigorous theoretical results can be transferred through the mean field limit, similar to works in the context of control of interacting particle systems, see e.g. [23]. On the other hand, our setup appears to be a new variant of a large-number-of-variables limit in the context of machine learning, complementing established settings like infinite-width neural networks [24].

Due to space constraints, all proofs and some additional technical results have been placed in the supplementary material.

## 2 Kernels and their RKHSs in the mean field limit

Setup and preliminariesLet \((X,d_{X})\) be a compact metric space and denote by \(\mathcal{P}(X)\) the set of Borel probability measures on \(X\). We endow \(\mathcal{P}(X)\) with the topology of weak convergence of probability measures. Recall that for \(\mu_{n},\mu\in\mathcal{P}(X)\), we say that \(\mu_{n}\to\mu\) weakly if for all bounded and continuous \(f:X\to\mathbb{R}\) (since \(X\) is compact, this is equivalent to \(f\) continuous) we have \(\lim_{n\to\infty}\int_{X}\phi(x)\mathrm{d}\mu_{n}(x)\to\int_{X}\phi(x) \mathrm{d}\mu(x)\). The topology of weak convergence can be metrized by the Kantorowich-Rubinstein metric \(d_{\text{KR}}\), defined by

\[d_{\text{KR}}(\mu_{1},\mu_{2})=\sup\left\{\int_{X}\phi(x)\mathrm{d}(\mu_{1}- \mu_{2})(x)\mid\phi:X\to\mathbb{R}\text{ is 1-Lipschitz }\right\}.\]

Note that since \(X\) is compact and hence separable, the Kantorowich-Rubinstein metric is equal to the 1-Wasserstein metric here. Furthermore, \(\mathcal{P}(X)\) is compact in this topology. For \(M\in\mathbb{N}_{+}\) and \(\vec{x}\in X^{M}\), denote the \(i\)-th component of \(\vec{x}\) by \(x_{i}\), and define the _empirical measure_ for \(\vec{x}\) by \(\hat{\mu}[\vec{x}]=\frac{1}{M}\sum_{i=1}^{M}\delta_{x_{i}}\), where \(\delta_{x}\) denotes the Dirac measure centered at \(x\in X\). The empirical measures are dense in \(\mathcal{P}(X)\) w.r.t. the Kantorowich-Rubinstein metric. Additionally, define \(d_{\text{KR}}^{2}:\mathcal{P}(X)^{2}\times\mathcal{P}(X)^{2}\to\mathbb{R}_{\geq 0}\) by \(d_{\text{KR}}^{2}((\mu_{1},\mu_{1}^{\prime}),(\mu_{2},\mu_{2}^{\prime}))=d_{ \text{KR}}(\mu_{1},\mu_{2})+d_{\text{KR}}(\mu_{1}^{\prime},\mu_{2}^{\prime})\), and note that \((\mathcal{P}(X)^{2},d_{\text{KR}}^{2})\) is a compact metric space. Moreover, denote the set of permutations on \(\{1,\dots,M\}\) by \(\mathcal{S}_{M}\), and for a tuple \(\vec{x}\in X^{M}\) and permutation \(\sigma\in\mathcal{S}_{M}\) define \(\sigma\vec{x}=(x_{\sigma(1)},\dots,x_{\sigma(M)})\). Finally, we recall some well-known definitions and results from the theory of reproducing kernel Hilbert spaces, following [20, Chapter 4]. For an arbitrary set \(\mathcal{X}\neq\emptyset\) and a Hilbert space \((H,\langle\cdot,\cdot\rangle_{H})\) of functions on \(\mathcal{X}\), we say that a map \(k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) is a _reproducing kernel_ for \(H\) if 1) \(k(\cdot,x)\in H\) for all \(x\in\mathcal{X}\); 2) for all \(x\in\mathcal{X}\) and \(f\in H\) we have \(f(x)=\langle f,k(\cdot,x)\rangle_{H}\). Note that if a reproducing kernel exists, it is unique. If such a Hilbert space has a reproducing kernel, we call \(H\) a reproducing kernel Hilbert space (RKHS) and \(k\) its (reproducing) kernel. It is well-known that a reproducing kernel is symmetric and positive semidefinite, and that every symmetric and positive semidefinite function has a unique RKHS for which it is the reproducing kernel. For brevity, if \(k\) is symmetric and positive semidefinite, or equivalently, if it is the reproducing kernel of an RKHS, we call \(k\) simply a kernel, and denote by \((H_{k},\langle\cdot,\cdot\rangle_{k})\) its unique associated RKHS. Define also \(H_{k}^{\text{pre}}=\operatorname{span}\{k(\cdot,x)\mid x\in\mathcal{X}\}\), then for \(f=\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n})\in H_{k}^{\text{pre}}\) and \(g=\sum_{m=1}^{M}\beta_{m}k(\cdot,y_{m})\in H_{k}^{\text{pre}}\) we have \(\langle f,g\rangle_{k}=\sum_{n=1}^{N}\sum_{m=1}^{M}\alpha_{n}\beta_{m}k(y_{m}, x_{n})\), and \(H_{k}^{\text{pre}}\) is dense in \(H_{k}\).

The mean field limit of functions and kernelsGiven \(f_{M}:X^{M}\to\mathbb{R}\), \(M\in\mathbb{N}_{+}\), and \(f:\mathcal{P}(X)\to\mathbb{R}\), we say that \(f_{M}\)_converges in mean field to_\(f\) and that \(f\) is the (or a) _mean field limit_ of \(f_{M}\), if \(\lim_{M\to\infty}\sup_{\vec{x}\in X^{M}}|f_{M}(\vec{x})-f(\hat{\mu}[\vec{x}]) |=0\). In this case, we write \(f_{M}\xrightarrow{\mathcal{P}_{1}}f\). Let now \((Y,d_{Y})\) be another metric space and \(f_{M}:X^{M}\times Y\to\mathbb{R}\), \(M\in\mathbb{N}_{+}\), and \(f:\mathcal{P}(X)\times Y\to\mathbb{R}\), then we say that \(f_{M}\)_converges in mean field to_\(f\) and that \(f\) is the (or a) _mean field limit_ of \(f_{M}\), if for all compact \(K\subseteq Y\) we have

\[\lim_{M\to\infty}\sup_{\vec{x}\in X^{M},y\in K}|f_{M}(\vec{x},y)-f(\hat{\mu}[ \vec{x}],y)|=0.\] (1)

and also write \(f_{M}\xrightarrow{\mathcal{P}_{1}}f\). The following existence results for mean field limits is slightly more general than what is available in the literature, and it is essentially a direct generalization of [25, Theorem 2.1], in the form of [26, Lemma 1.2].

**Proposition 2.1**.: Let \((X,d_{X})\) be a compact metric space and \((Z,d_{Z})\) a metric space that has a countable basis \((U_{n})_{n}\) such that \(\bar{U}_{n}\) is compact for all \(n\in\mathbb{N}\). Let \(f_{M}:X^{M}\times Z\to\mathbb{R}\), \(M\in\mathbb{N}_{+}\), be a sequence of functions fulfilling the following conditions: 1) _(Symmetry in \(\vec{x}\))2_ For all \(M\in\mathbb{N}_{+}\), \(\vec{x}\in X^{M}\), \(z\in Z\) and permutations \(\sigma\in\mathcal{S}_{M}\), we have \(f_{M}(\sigma\vec{x},z)=f_{M}(\vec{x},z)\); 2) _(Uniform boundedness_) There exists \(B_{f}\in\mathbb{R}_{\geq 0}\) and a function \(b:Z\to\mathbb{R}_{\geq 0}\) such that \(\forall M\in\mathbb{N}_{+},\vec{x}\in X^{M},z\in z:|f_{M}(\vec{x},z)|\leq B_{f }+b(z);3)\) _(Uniform Lipschitz continuity_) There exists some \(L_{f}\in\mathbb{R}_{>0}\) such that for all \(M\in\mathbb{N}_{+}\), \(\vec{x}_{1},\vec{x}_{2}\in X^{M}\), \(z_{1},z_{2}\in Z\) we have \(|f_{M}(\vec{x}_{1},z_{1})-f_{M}(\vec{x}_{2},z_{2})|\leq L_{f}\left(d_{\text{ RK}}(\hat{\mu}[\vec{x}_{1}],\hat{\mu}[\vec{x}_{2}])+d_{Z}(z_{1},z_{2})\right)\).

Footnote 2: As is well-known, cf. [26, Remark 1.1.3], this condition is actually implied by the next condition. However, as usual in the kinetic theory literature, we kept this condition for emphasis.

Then there exists a subsequence \((f_{M_{\ell}})_{\ell}\) and a continuous function \(f:\mathcal{P}(X)\times Z\to\mathbb{R}\) such that \(f_{M_{\ell}}\xrightarrow{\mathcal{P}_{1}}f\) for \(\ell\to\infty\). Furthermore, \(f\) is \(L_{f}\)-Lipschitz continuous and there exists \(B_{F}\in\mathbb{R}_{\geq 0}\) such that for all \(\mu\in\mathcal{P}(X)\), \(z\in Z\) we have \(|f(\mu,z)|\leq B_{F}+b(z)\).

We now turn to the mean field limit of kernels as introduced in [22]: Given \(k_{M}:X^{M}\times X^{M}\to\mathbb{R}\) and \(k:\mathcal{P}(X)\times\mathcal{P}(X)\to\mathbb{R}\), we say that \(k_{M}\)_converges in mean field to_\(k\) and that \(k\) is the (or a) _mean field limit_ of \(k_{M}\), if

\[\lim_{M\to\infty}\sup_{\vec{x},\vec{x}^{\prime}\in X^{M}}|k_{M}(\vec{x},\vec{ x}^{\prime})-k(\hat{\mu}[\vec{x}],\hat{\mu}[\vec{x}^{\prime}])|=0.\] (2)

In this case we write \(k_{M}\xrightarrow{\mathcal{P}_{1}}k\).

For convenience, we recall [22, Theorem 2.1], which ensures the existence of a mean field limit of a sequence of kernels.

**Proposition 2.2**.: Let \(k_{M}:X^{M}\times X^{M}\to\mathbb{R}\) be a sequence of kernels fulfilling the following conditions. 1) _(Symmetry in \(\vec{x}\))_ For all \(M\in\mathbb{N}_{+}\), \(\vec{x},\vec{x}^{\prime}\in X^{M}\) and permutations \(\sigma\in\mathcal{S}_{M}\) we have \(k_{M}(\sigma\vec{x},\vec{x}^{\prime})=k_{M}(\vec{x},\vec{x}^{\prime})\); 2) _(Uniform boundedness_) There exists \(C_{k}\in\mathbb{R}_{\geq 0}\) such that \(\forall M\in\mathbb{N}_{+},\vec{x},\vec{x}^{\prime}\in X^{M}:|k_{M}(\vec{x}, \vec{x}^{\prime})|\leq C_{k}\); 3) _(Uniform Lipschitz continuity)_ There exists some \(L_{k}\in\mathbb{R}_{>0}\) such that for all \(M\in\mathbb{N}_{+}\), \(\vec{x}_{1},\vec{x}_{1}^{\prime},\vec{x}_{2},\vec{x}_{2}^{\prime}\in X^{M}\) we have \(|k_{M}(\vec{x}_{1},\vec{x}_{1}^{\prime})-k_{M}(\vec{x}_{2},\vec{x}_{2}^{\prime})| \leq L_{k}d_{\text{KR}}^{\text{2}}\left[(\hat{\mu}[\vec{x}_{1}],\hat{\mu}[\vec{x}_{ 1}^{\prime}]),(\hat{\mu}[\vec{x}_{2}],\hat{\mu}[\vec{x}_{2}^{\prime}])\right]\).

Then there exists a subsequence \((k_{M_{\ell}})_{\ell}\) and a continuous kernel \(k:\mathcal{P}(X)\times\mathcal{P}(X)\to\mathbb{R}\) such that \(k_{M_{\ell}}\xrightarrow{\mathcal{P}_{1}}k\), and \(k\) is also bounded by \(C_{k}\).

Let \(k_{M}:X^{M}\times X^{M}\to\mathbb{R}\) be a given sequence of kernels fulfilling the conditions of Proposition 2.2. Then there exists a subsequence \((k_{M_{\ell}})_{\ell}\) converging in mean field to a kernel \(k:\mathcal{P}(X)\times\mathcal{P}(X)\to\mathbb{R}\).

_From now on, we only consider this subsequence_ and denote it again by \((k_{M})_{M}\), i.e., \(k_{M}\xrightarrow[\mathcal{P}_{1}]{\mathcal{P}_{1}}k\). Unless noted otherwise, every time we need a further subsequence, we will make this explicit.3

Footnote 3: It is customary in the kinetic theory literature to switch to such a subsequence. However, for some results that are about to follow, it is important that no further switch to a subsequence happens, hence we need to be more explicit in these cases.

The RKHS of the mean field limit kernelDenote by \(H_{M}:=H_{k_{M}}\) the (unique) RKHS corresponding to kernel \(k_{M}\) and denote by \(H_{k}\) the unique RKHS of \(k\). For basic properties of these objects as well as classes of suitable kernels we refer to [22].

We clarify the relation between \(H_{M}\)and \(H_{k}\) in the next result.

**Theorem 2.3**.: 1) For every \(f\in H_{k}\), there exists a sequence \(f_{M}\in H_{M}\), \(M\in\mathbb{N}_{+}\), such that \(f_{M}\xrightarrow[\mathcal{P}_{1}]{\mathcal{P}_{1}}f\). 2) Let \(f_{M}\in H_{M}\) be sequence such that there exists \(B\in\mathbb{R}_{\geq 0}\) with \(\|f_{M}\|_{M}\leq B\) for all \(M\in\mathbb{N}_{+}\). Then there exists a subsequence \((f_{M_{\ell}})_{\ell}\) and \(f\in H_{k}\) with \(f_{M_{\ell}}\xrightarrow[\mathcal{P}_{1}]{\mathcal{P}_{1}}f\) and \(\|f\|_{k}\leq B\).

In other words, on the one hand, every RKHS function from \(H_{k}\)arises as a mean field limit of RKHS functions from \(H_{M}\).On the other hand, every uniformly norm-bounded sequence of RKHS functions \((f_{M})_{M}\) has a mean field limit in \(H_{k}\).

Note that the preceding result is considerably stronger than the corresponding results in [22]: In contrast to [22, Theorem 4.4] we do not need to go to another subsequence in the first item, and we ensure that the mean field limit \(f\) is contained in \(H_{k}\) (and norm-bounded by the same uniform bound), which was missing from Corollary 4.3 in the same reference.

The relation between the kernels \(k_{M}\) and their RKHSs \(H_{M}\), and the mean field limit kernel \(k\) and its RKHS \(H_{k}\) is illustrated as a commutative diagram in Figure 1. In order to arrive at the mean field RKHS \(H_{k}\), on the one hand, we consider the mean field limit \(k\) of the \(k_{M}\), and then form the corresponding RKHS \(H_{k}\). This is essentially the content of Proposition 2.2. On the other hand, we can first go from the kernel \(k_{M}\) to the associated unique RKHS \(H_{M}\) (for each \(M\in\mathbb{N}_{+}\)). Theorem 2.3 then says that \(H_{k}\) can be interpreted as a mean field limit of the RKHSs \(H_{M}\), since every function in \(H_{k}\) arises as a mean field limit of a sequence of functions from the \(H_{M}\), and every uniformly norm-bounded sequence of such functions has a mean field limit that is in \(H_{k}\).

Next, we state two technical results that will play an important role in the following developments, and which might be of independent interest. They describe \(\liminf\) and \(\limsup\) inequalities required for \(\Gamma\)-convergence arguments used later on.

**Lemma 2.4**.: Let \(f_{M}\in H_{M}\), \(M\in\mathbb{N}_{+}\), and \(f\in H_{k}\) such that \(f_{M}\xrightarrow[\mathcal{P}_{1}]{\mathcal{P}_{1}}f\), then

\[\|f\|_{k}\leq\liminf_{M\to\infty}\|f_{M}\|_{M}.\] (3)

**Lemma 2.5**.: Let \(f\in H_{k}\). Then there exist \(f_{M}\in H_{M}\), \(M\in\mathbb{N}_{+}\), such that \(\lim_{M\to\infty}\sup_{\vec{x}\in X^{M}}|f_{M}(\vec{x})-f(\hat{\mu}[\vec{x}])|=0\), and

\[\limsup_{M\to\infty}\|f_{M}\|_{M}\leq\|f\|_{k}.\] (4)

## 3 Approximation with kernels in the mean field limit

Kernel-based machine learning methods use in general an RKHS as the hypothesis space, and learning often reduces to a search or optimization problem over this function space. For this reason, it is

Figure 1: The kernel \(k\) arises as the mean field limit (MFL) of the kernels \(k_{M}\) (Proposition 2.2). Every uniformly norm-bounded sequence \(f_{M}\in H_{M}\), \(M\in\mathbb{N}_{+}\), has an MFL in \(H_{k}\), and every function \(f\in H_{k}\) arises as such an MFL (Theorem 2.3). Based on [22, Figure 1].

important to investigate the approximation properties of a given kernel and its associated RKHS as well as to ensure that the learning problem over an RKHS (which is in general an infinite-dimensional object) can be tackled with finite computations.

The next result asserts that, under a uniformity condition, the approximation power of the finite-input kernels \(k_{M}\) is inherited by the mean field limit kernel.

**Proposition 3.1**.: For \(M\in\mathbb{N}_{+}\), let \(\mathcal{F}_{M}\) be the set of symmetric functions that are continuous w.r.t. \((\vec{x},\vec{x}^{\prime})\mapsto d_{\text{KR}}(\hat{\mu}[\vec{x}],\hat{\mu}[ \vec{x}^{\prime}])\). Let \(\mathcal{F}\subseteq C^{0}(\mathcal{P}(X),\mathbb{R})\) such that for all \(f\in\mathcal{F}\) and \(\epsilon>0\) there exist \(B\in\mathbb{R}_{\geq 0}\) and sequences \(f_{M}\in\mathcal{F}_{M}\), \(\hat{f}_{M}\in H_{M}\), \(M\in\mathbb{N}_{+}\), such that 1) \(f_{M}\xrightarrow{\mathcal{P}_{1}}f\) 2) \(\|f_{M}-\hat{f}_{M}\|_{\infty}\leq\epsilon\) for all \(M\in\mathbb{N}_{+}\) 3) \(\|\hat{f}_{M}\|_{M}\leq B\) for all \(M\in\mathbb{N}_{+}\). Then for all \(f\in\mathcal{F}\) and \(\epsilon>0\), there exists \(\hat{f}\in H_{k}\) with \(\|f-\hat{f}\|_{\infty}\leq\epsilon\).

Intuitively, the set \(\mathcal{F}\) consists of all continuous functions on \(\mathcal{P}(X)\) that arise as a mean field limit of functions which can be uniformly approximated by uniformly norm-bounded RKHS functions. The result then states (to use a somewhat imprecise terminology) that the RKHS \(H_{k}\) is dense in \(\mathcal{F}\). We can interpret this as an appropriate mean field variant of the universality property of kernels: a kernel on a compact metric space is called universal if its associated RKHS is dense w.r.t. the supremum norm in the space of continuous functions, and many common kernels are universal, cf. e.g. [20, Section 4.6]. In our setting, ideally universality of the finite-input kernels \(k_{M}\) is inherited by the mean field limit kernel \(k\). However, since the mean field limit can be interpreted as a form of smoothing limit, some uniformity requirements should be expected. Proposition 3.1 provides exactly such a condition.

**Remark 3.2**.: In Proposition 3.1, the set \(\mathcal{F}\) is a subvectorspace of \(C^{0}(\mathcal{P}(X),\mathbb{R})\). Furthermore, if the \(\mathcal{P}_{1}\)-convergence in the definition of \(\mathcal{F}\) is uniform, then \(\mathcal{F}\) is closed.

Since \(k_{M}\) and \(k\) are kernels, we have the usual representer theorem for their corresponding RKHSs, cf. e.g. [27]. A natural question is then whether we have mean field convergence of the minimizers and their representation. This is clarified by the next result.

**Theorem 3.3**.: Let \(N\in\mathbb{N}_{+}\), \(\mu_{1},\ldots,\mu_{N}\in\mathcal{P}(X)\) and for \(n=1,\ldots,N\) let \(\vec{x}_{n}^{[M]}\in X^{M}\), \(M\in\mathbb{N}_{+}\), such that \(\hat{\mu}[\vec{x}_{n}^{[M]}]\xrightarrow{d_{\text{KR}}}\mu_{n}\) for \(M\to\infty\). Let \(L:\mathbb{R}^{N}\to\mathbb{R}_{\geq 0}\) be continuous and strictly convex and \(\lambda>0\). For each \(M\in\mathbb{N}_{+}\) consider the problem

\[\min_{f\in H_{M}}L(f(\vec{x}_{1}^{[M]}),\ldots,f(\vec{x}_{N}^{[M]}))+\lambda\| f\|_{M},\] (5)

as well as the problem

\[\min_{f\in H_{k}}L(f(\mu_{1}),\ldots,f(\mu_{N}))+\lambda\|f\|_{k}.\] (6)

Then for each \(M\in\mathbb{N}_{+}\) problem (5) has a unique solution \(f_{M}^{*}\), which is of the form \(f_{M}^{*}=\sum_{n=1}^{N}\alpha_{n}^{[M]}k_{M}(\cdot,\vec{x}_{n}^{[M]})\in H_{M}\), with \(\alpha_{1}^{[M]},\ldots,\alpha_{N}^{[M]}\in\mathbb{R}\), and problem (6) has a unique solution \(f^{*}\), which is of the form \(f^{*}=\sum_{n=1}^{N}\alpha_{n}k(\cdot,\mu_{n})\in H_{k}\), with \(\alpha_{1},\ldots,\alpha_{N}\in\mathbb{R}\). Furthermore, there exists a subsequence \((f_{M_{t}}^{*})_{\ell}\) such that \(f_{M_{\ell}}^{*}\xrightarrow{\mathcal{P}_{1}}f^{*}\) and

\[L(f_{M_{t}}^{*}(\vec{x}_{1}^{[M_{\ell}]}),\ldots,f_{M_{t}}^{*}(\vec{x}_{N}^{[ M_{\ell}]}))+\lambda\|f_{M_{t}}^{*}\|_{M_{\ell}}\to L(f^{*}(\mu_{1}), \ldots,f^{*}(\mu_{N}))+\lambda\|f^{*}\|_{k}.\] (7)

for \(\ell\to\infty\).

The main point of this result is the convergence of the minimizers, which we will establish using a \(\Gamma\)-convergence argument. This approach seems to have been introduced by [28, 18, 29] originally in the context of multi-agent systems.

**Remark 3.4**.: An inspection of the proof reveals that in Theorem 3.3 we can replace the term \(\lambda\|\cdot\|_{M}\) and \(\lambda\|\cdot\|_{k}\) by \(\Omega(\|\cdot\|_{M})\) and \(\Omega(\|\cdot\|_{k})\), where \(\Omega:\mathbb{R}_{\geq 0}\to\mathbb{R}_{\geq 0}\) is a nonnegative, strictly increasing and continuous function.

## 4 Support Vector Machines with mean field limit kernels

We now turn to the mean field limit of kernels in the context of statistical learning theory, focusing on SVMs. We first briefly recall the standard setup of statistical learning theory, and formulate an appropriate mean field limit thereof. We then investigate empirical and infinite-sample solutions of SVMs and their mean field limits, as well as the convergence of the corresponding risks.

Statistical learning theory setupWe now introduce the standard setup of statistical learning theory, following mostly [20, Chapters 2 and 5]. Let \(\mathcal{X}\neq\emptyset\) (associated with some \(\sigma\)-algebra) and \(\emptyset\neq Y\subseteq\mathbb{R}\) closed (associated with the corresponding Borel \(\sigma\)-algebra). A _loss function_ is in this setting a measurable function \(\ell:\mathcal{X}\times Y\times\mathbb{R}\to\mathbb{R}_{\geq 0}\). Let \(P\) be a probability distribution on \(\mathcal{X}\times Y\) and \(f:\mathcal{X}\to\mathbb{R}\) a measurable function, then the _risk_ of \(f\)_w.r.t. \(P\) and loss function_\(\ell\) is defined by

\[\mathcal{R}_{\ell,P}(f)=\int_{\mathcal{X}\times Y}\ell(x,y,f(x))\mathrm{d}P.\]

Note that this is always well-defined since \((x,y)\mapsto\ell(x,y,f(x))\) is a measurable and nonnegative function. For a set \(H\subseteq\mathbb{R}^{\mathcal{X}}\) of measurable functions we also define the _minimal risk over_\(H\) by

\[\mathcal{R}^{H*}_{\ell,P}=\inf_{f\in H}\mathcal{R}_{\ell,P}(f).\]

If \(H\) is a normed vector space, we additionally define the _regularized risk_ of \(f\in H\) and the _minimal regularized risk over_\(H\) by

\[\mathcal{R}_{\ell,P,\lambda}(f)=\mathcal{R}_{\ell,P}(f)+\lambda\|f\|_{H}^{2},\qquad\mathcal{R}^{H*}_{\ell,P,\lambda}=\inf_{f\in H}\mathcal{R}_{\ell,P, \lambda}(f),\]

where \(\lambda\in\mathbb{R}_{\geq 0}\) is the _regularization parameter_. A _data set of size \(N\in\mathbb{N}_{+}\)_ is a tuple \(D_{N}=((x_{1},y_{1}),\ldots,(x_{N},y_{N}))\in(\mathcal{X}\times Y)^{N}\) and for a function \(f:\mathcal{X}\to\mathbb{R}\) we define its _empirical risk_ by

\[\mathcal{R}_{\ell,D_{N}}(f)=\frac{1}{N}\sum_{n=1}^{N}\ell(x_{n},y_{n},f(x_{n})).\]

If \(H\) is a normed vector space and \(f\in H\), we define additionally the _regularized empirical risk_ and the _minimal regularized empirical risk over_\(H\) by

\[\mathcal{R}_{\ell,D_{N},\lambda}(f)=\mathcal{R}_{\ell,D_{N}}(f)+\lambda\|f\|_ {H}^{2},\qquad\mathcal{R}^{H*}_{\ell,D_{N},\lambda}=\inf_{f\in H}\mathcal{R} _{\ell,D_{N},\lambda}(f),\]

where \(\lambda\in\mathbb{R}_{>0}\) is again the regularization parameter. Note that the notation for the empirical risks is consistent with the risk w.r.t. a probability distribution \(P\), if we identify a data set \(D_{N}\) by the corresponding empirical distribution \(\frac{1}{N}\sum_{n=1}^{N}\delta_{(x_{n},y_{n})}\).

In the following, \(H\) will be a RKHS and a minimizer (assuming existence and uniqueness) of \(\mathcal{R}^{H*}_{\ell,P,\lambda}\) will be called an _infinite-sample support vector machine (SVM)_. Similarly, \(\mathcal{R}^{H*}_{\ell,D_{N},\lambda}\) will be called the _empirical solution of the SVM w.r.t. the data set_\(D_{N}\). Note that this is the common terminology in statistical learning theory, cf. [20], and corresponds to (empirical) risk minization with Tikhonov regularization.

Statistical learning theory setup in the mean field limitLet now \(\emptyset\neq Y\subseteq\mathbb{R}\) be compact and \(\ell_{M}:X^{M}\times Y\times\mathbb{R}\to\mathbb{R}_{\geq 0}\), \(M\in\mathbb{N}\), such that 1) \(\ell_{M}(\vec{x},y,t)=\ell_{M}(\vec{x},y,t)\) for all \(\vec{x}\in X^{M}\), \(\sigma\in\mathcal{S}_{M}\), \(y\in Y\), \(t\in\mathbb{R}\); 2) there exists \(C_{\ell}\in\mathbb{R}_{\geq 0}\) and a nondecreasing function \(b:\mathbb{R}_{\geq 0}\to\mathbb{R}_{\geq 0}\) with \(|\ell_{M}(\vec{x},y,t)|\leq C_{\ell}+b(|t|)\) for all \(M\in\mathbb{N}\) and \(\vec{x}\in X^{M},y\in Y,t\in\mathbb{R}\); 3) there exists \(L_{\ell}\in\mathbb{R}_{\geq 0}\) with

\[|\ell_{M}(\vec{x}_{1},y_{1},t_{1})-\ell_{M}(\vec{x}_{2},y_{2},t_{2})|\leq L_{ \ell}(d_{\text{KR}}(\hat{\mu}[\vec{x}_{1}],\hat{\mu}[\vec{x}_{2}])+|y_{1}-y_{2 }|+|t_{1}-t_{2}|)\]

for all \(\vec{x}_{1},x_{2}\in X^{M}\), \(y_{1},y_{1}^{\prime}\in Y,t_{1},t_{2}\in\mathbb{R}\). In particular, all \(\ell_{M}\) are measurable (assuming the Borel \(\sigma\)-algebra on \(X^{M}\)) and hence are loss functions on \(X^{M}\times Y\). Proposition 2.1 ensures the existence of a subsequence \((\ell_{M_{m}})_{m}\) and an \(L_{\ell}\)-Lipschitz continuous function \(\ell:\mathcal{P}(X)\times Y\times\mathbb{R}\to\mathbb{R}\) with

\[\lim_{M\to\infty}\sup_{\begin{subarray}{c}\vec{x}\in X^{M_{m}}\\ y\in Y,t\in K\end{subarray}}|\ell_{M_{m}}(\vec{x},y,t)-\ell(\hat{\mu}[\vec{x} ],y,t)|=0\] (8)

for all compact \(K\subseteq\mathbb{R}\), and we write again \(\ell_{M_{m}}\xrightarrow{\mathcal{P}_{1}}\ell\). _For readability, from now on we switch to this subsequence._ Furthermore, we also get from Proposition 2.1 that there exists some \(C_{L}\in\mathbb{R}_{\geq 0}\) such that \(|\ell(\mu,y,t)|\leq C_{L}+b(|t|)\) for all \(\mu\in\mathcal{P}(X),y\in Y,t\in\mathbb{R}\).

**Remark 4.1**.: Note that, for Proposition 2.1 to apply, it is enough to assume in item 2) above the existence of a function \(b:\mathbb{R}\to\mathbb{R}_{\geq 0}\) with \(|\ell_{M}(\vec{x},y,t)|\leq C_{\ell}+b(|t|)\). However, we chose the slightly stronger condition that \(b\) is nondecreasing, since then \(\ell_{M}\) is a _Nemitskii loss_ according to [20, Definition 2.16]. Since the function with constant value \(C_{\ell}\) is actually \(P_{M}\)-integrable, this means that \(\ell_{M}\) is even a \(P_{M}\)_-integrable Nemitskii loss_ according to [20]. A similar remark then applies to \(\ell\).

**Lemma 4.2**.: The function \(\ell\) is nonnegative. Furthermore, if all \(\ell_{M}\) are convex loss functions [20, Definition 2.12], i.e., if for all \(M\in\mathbb{N}_{+}\), \(\vec{x}\in X^{M},y\in Y,t_{1},t_{2}\in\mathbb{R}\) and \(\lambda\in(0,1)\) we have

\[\ell_{M}(\vec{x},y,\lambda t_{1}+(1-\lambda)t_{2})\leq\lambda\ell_{M}(\vec{x},y,t_{1})+(1-\lambda)\ell_{M}(\vec{x},y,t_{2}),\] (9)

then so is \(\ell\).

Empirical SVM solutionsGiven data sets \(D_{N}^{[M]}=\left((\vec{x}_{1}^{[M]},y_{1}^{[M]}),\ldots,(\vec{x}_{N}^{[M]},y_ {N}^{[M]})\right)\) for all \(M\in\mathbb{N}_{+}\) with \(\vec{x}_{n}^{[M]}\in X^{M}\), \(y_{n}^{[M]}\in Y\), and \(D_{N}=((\mu_{1},y_{1}),\ldots,(\mu_{N},y_{N}))\) with \(\mu_{n}\in\mathcal{P}(X)\) and \(y_{n}\in Y\), we write \(D_{N}^{[M]}\xrightarrow[\mathcal{P}_{1}]{}D_{N}\) if \(\hat{\mu}[\vec{x}_{n}^{[M]}]\xrightarrow[\mathcal{d_{N}}]{}\mu_{n}\) and \(y_{n}^{[M]}\to y_{n}\) (where \(M\to\infty\)) for all \(n=1,\ldots,N\). We can interpret this as mean field convergence of the data sets.

Furthermore, consider the empirical risk of hypothesis \(f_{M}\in H_{M}\) (and \(f\in H_{k}\)) on data set \(D_{N}^{[M]}\) (and \(D_{N}\))

\[\mathcal{R}_{\ell_{M},D_{N}^{[M]}}(f_{M})=\frac{1}{N}\sum_{n=1}^{N}\ell_{M}( \vec{x}_{n}^{[M]},y_{n}^{[M]},f_{M}(\vec{x}_{n}^{[M]})),\qquad\mathcal{R}_{ \ell,D_{N}}(f)=\frac{1}{N}\sum_{n=1}^{N}\ell(\mu_{n},y_{n},f(\mu_{n})),\]

and the corresponding regularized risk

\[\mathcal{R}_{\ell_{M},D_{N}^{[M]},\lambda}(f_{M}) =\frac{1}{N}\sum_{n=1}^{N}\ell_{M}(\vec{x}_{n}^{[M]},y_{n}^{[M]},f_{M}(\vec{x}_{n}^{[M]}))+\lambda\|f_{M}\|_{M}^{2}\] \[\mathcal{R}_{\ell,D_{N},\lambda}(f) =\frac{1}{N}\sum_{n=1}^{N}\ell(\mu_{n},y_{n},f(\mu_{n}))+\lambda \|f\|_{k}^{2},\]

where \(\lambda\in\mathbb{R}_{>0}\) is the regularization parameter.

**Proposition 4.3**.: Let \(\lambda>0\), assume that all \(\ell_{M}\) are convex and let \(D_{N}^{[M]}\), \(D_{N}\) be finite data sets with \(D_{N}^{[M]}\xrightarrow[\mathcal{P}_{1}]{}D_{N}\). Then for all \(M\in\mathbb{N}_{+}\), \(H_{M}\ni f_{M}\mapsto\mathcal{R}_{\ell_{M},D_{N}^{[M]},\lambda}(f_{M})\) has a unique minimizer \(f_{M,\lambda}^{*}\in H_{M}\) and \(H_{k}\ni f\mapsto\mathcal{R}_{\ell,D_{N},\lambda}(f)\) has a unique minimizer \(f_{\lambda}^{*}\in H_{k}\). Furthermore, for all \(M\in\mathbb{N}_{+}\) there exist \(\alpha_{n}^{[M]}\in\mathbb{R}\), \(n=1,\ldots,N\), such that \(f_{M,\lambda}^{*}=\sum_{n=1}^{N}\alpha_{n}^{[M]}k_{M}(\cdot,\vec{x}_{n}^{[M]})\), and there exist \(\alpha_{1},\ldots,\alpha_{N}\in\mathbb{R}\) such that \(f_{\lambda}^{*}=\sum_{n=1}^{N}\alpha_{n}k(\cdot,\mu_{n})\). Finally, there exists a subsequence \((f_{M_{m},\lambda}^{*})_{m}\) such that \(f_{M_{m},\lambda}^{*}\xrightarrow[\mathcal{P}_{1}]{}f_{\lambda}^{*}\) and \(\mathcal{R}_{\ell_{M_{m}},D_{N}^{[M_{m}]},\lambda}(f_{M_{m},\lambda}^{*}) \to\mathcal{R}_{\ell,D_{N},\lambda}(f_{\lambda}^{*})\) for \(m\to\infty\).

Convergence of distributions and infinite-sample SVMs in the mean field limitWe now turn to the question of mean field limits of distributions and the associated learning problems and SVM solutions. Let \((P^{[M]})_{M}\) be a sequence of distributions, where \(P^{[M]}\) is a probability distribution on \(X^{M}\times Y\), and let \(P\) be a probability distribution on \(\mathcal{P}(X)\times Y\). We say that \(P^{[M]}\)_converges in mean field to \(P\)_ and write \(P^{[M]}\xrightarrow[\mathcal{P}_{1}]{}P\), if for all continuous (w.r.t. the product topology on \(\mathcal{P}(X)\times Y\)) and bounded 4\(f\) we have

Footnote 4: Of course, since \(Y\) is compact, all continuous \(f\) are bounded in our present setting.

\[\int_{X^{M}\times Y}f(\hat{\mu}[\vec{x}],y)\mathrm{d}P^{[M]}(\vec{x},y)\to \int_{\mathcal{P}(X)\times Y}f(\mu,y)\mathrm{d}P(\mu,y).\] (10)

This convergence notion of probability distributions (on different input spaces) appears to be not standard, but it is a natural concept in the present context. Essentially, it is weak (also called narrow) convergence of probability distributions adapated to our setting.

Consider now data sets \(D_{N}^{[M]}\), \(D_{N}\), with \(D_{N}^{[M]}\xrightarrow[\mathcal{P}_{1}]{}D_{N}\), then we also have convergence in mean field of the datasets, interpreted as empirical distributions: let \(f\in C^{0}(\mathcal{P}(X)\times Y,\mathbb{R})\) be bounded, then

\[\int_{X^{M}\times Y}f(\hat{\mu}[\vec{x}],y)\mathrm{d}D_{N}^{[M]}( \vec{x},y) =\frac{1}{N}\sum_{n=1}^{N}f(\hat{\mu}[\vec{x}_{n}^{[M]}],y_{n}^{[M]})\] \[\xrightarrow[M\to\infty]{} \frac{1}{N}\sum_{n=1}^{N}f(\mu_{n},y_{n})=\int_{\mathcal{P}(X) \times Y}f(\mu,y)\mathrm{d}D_{N}(\mu,y).\]This shows that the mean field convergence of probability distributions as defined here is a direct generalization of the natural notion of mean field convergence of data sets.

Finally, consider the risk of hypothesis \(f_{M}\in H_{M}\) and \(f\in H_{k}\) w.r.t. the distribution \(P^{[M]}\) and \(P\), respectively,

\[\mathcal{R}_{\ell_{M},P^{[M]}}(f_{M}) =\int_{X^{M}\times Y}\ell_{M}(\vec{x},y,f_{M}(\vec{x}))\mathrm{d}P ^{[M]}(\vec{x},y)\] \[\mathcal{R}_{\ell,P}(f) =\int_{\mathcal{P}(X)\times Y}\ell(\mu,y,f(\mu))\mathrm{d}P(\mu,y),\]

as well as the minimal risks

\[\mathcal{R}^{H_{M}*}_{\ell_{M},P^{[M]}}=\inf_{f_{M}\in H_{M}}\mathcal{R}_{\ell _{M},P^{[M]}}(f_{M})\qquad\mathcal{R}^{H_{k}*}_{\ell,P}=\inf_{f\in H_{k}} \mathcal{R}_{\ell,P}(f).\]

Our first result ensures that mean field convergence of distributions \(P^{[M]}\), loss functions \(\ell_{M}\) and data sets \(D^{[M]}_{N}\) ensures the convergence of the corresponding risks of the empirical SVM solutions.

**Lemma 4.4**.: Consider the situation and notation of Proposition 4.3 and assume that \(P^{[M]}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}P\). We then have \(\mathcal{R}_{\ell_{M_{m}},P^{[M_{m}]}}(f^{*}_{M_{m},\lambda})\to\mathcal{R}_{ \ell,P}(f^{*}_{\lambda})\) for \(m\to\infty\).

Next, we investigate the mean field convergence of infinite-sample SVM solutions and their associated risks. Define for \(\lambda\in\mathbb{R}_{\geq 0}\) (and all \(M\in\mathbb{N}_{+}\)) the regularized risk of \(f_{M}\in H_{M}\) and \(f\in H_{k}\), respectively, by

\[\mathcal{R}_{\ell_{M},P^{[M]},\lambda}(f_{M})=\mathcal{R}_{\ell_{M},P^{[M]}}( f_{M})+\lambda\|f_{M}\|_{M}^{2},\qquad\mathcal{R}_{\ell,P,\lambda}(f)=\mathcal{R}_{ \ell,P}(f)+\lambda\|f\|_{k}^{2},\]

and the corresponding minimal risks by

\[\mathcal{R}^{H_{M}*}_{\ell_{M},P^{[M]},\lambda}=\inf_{f_{M}\in H_{M}}\mathcal{ R}_{\ell_{M},P^{[M]},\lambda}(f_{M}),\qquad\mathcal{R}^{H_{k}*}_{\ell,P, \lambda}=\inf_{f\in H_{k}}\mathcal{R}_{\ell,P,\lambda}(f).\]

**Proposition 4.5**.: 5 Let \(\lambda>0\), assume that all \(\ell_{M}\) are convex loss functions and let \(P^{[M]}\) and \(P\) be probability distributions on \(X^{M}\times Y\) and \(\mathcal{P}(X)\times Y\), respectively, with \(P^{[M]}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}P\). Then for all \(M\in\mathbb{N}_{+}\), \(H_{M}\ni f_{M}\mapsto\mathcal{R}_{\ell_{M},P^{[M]},\lambda}(f_{M})\) has a unique minimizer \(f^{*}_{M,\lambda}\in H_{M}\) and \(H_{k}\ni f\mapsto\mathcal{R}_{\ell,P,\lambda}(f)\) has a unique minimizer \(f^{*}_{\lambda}\in H_{k}\). Furthermore, there exists a subsequence \((f^{*}_{M_{m},\lambda})\) such that \(f^{*}_{M_{m},\lambda}\xrightarrow{\mathcal{P}_{1}}f^{*}_{\lambda}\) and \(\mathcal{R}_{\ell_{M_{m}},P^{[M_{m}]},\lambda}(f^{*}_{M_{m},\lambda})\to \mathcal{R}_{\ell,P,\lambda}(f^{*}_{\lambda})\) for \(m\to\infty\). In particular, \(\mathcal{R}^{H_{M_{m}}*}_{\ell_{M_{m}},P^{[M_{m}]},\lambda}\to\mathcal{R}^{H_ {k}*}_{\ell,P,\lambda}\).

Finally, we would like to show that \(\mathcal{R}^{H_{M}*}_{\ell_{M},P^{[M]}}\to\mathcal{R}^{H_{k}*}_{\ell,P}\) for \(P^{[M]}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}P\). Up to a subsequence, this is established under Assumption 4.6. Define the _approximation error functions_, cf. [20, Definition 5.14], by

\[A_{2}^{[M]}(\lambda)=\inf_{f\in H_{M}}\mathcal{R}_{\ell_{M},P^{[M]},\lambda}( f)-\mathcal{R}^{H_{M}*}_{\ell_{M},P^{[M]}}\qquad A_{2}(\lambda)=\inf_{f\in H _{k}}\mathcal{R}_{\ell,P,\lambda}(f)-\mathcal{R}^{H_{k}*}_{\ell,P},\]

where \(M\in\mathbb{N}_{+}\) and \(\lambda\in\mathbb{R}_{\geq 0}\). Note that (for all \(M\in\mathbb{N}_{+}\)) \(A_{2}^{[M]},A_{2}:\mathbb{R}_{\geq 0}\to\mathbb{R}_{\geq 0}\) are increasing, concave and continuous, and \(A_{2}^{[M]},A_{2}(0)=0\), cf. [20, Lemma 5.15]. We need essentially equicontinuity of \((A_{2}^{[M]})_{M}\) in 0, which is formalized in the following assumption.

**Assumption 4.6**.: For all \(\epsilon>0\) there exists \(\lambda_{\epsilon}>0\) such that for all \(0<\lambda\leq\lambda_{\epsilon}\) and \(M\in\mathbb{N}_{+}\) we have \(A_{2}^{[M]}(\lambda)\leq\epsilon\).

**Proposition 4.7**.: Assume that all \(\ell_{M}\) are convex loss functions, let \(P^{[M]}\) and \(P\) be probability distributions on \(X^{M}\times Y\) and \(\mathcal{P}(X)\times Y\), respectively, with \(P^{[M]}\xrightarrow{\mathcal{P}_{1}}P\). If Assumption 4.6 holds, there exists a strictly increasing sequence \((M_{m})_{m}\) with \(\mathcal{R}^{H_{M_{m}}*}_{\ell_{M_{m}},P^{[M_{m}]}}\to\mathcal{R}^{H_{k}*}_{ \ell,P}\) for \(m\to\infty\).

Conclusion

We investigated the mean field limit of kernels and their RKHSs, as well as the mean field limit of statistical learning problems solved with SVMs. In particular, we managed to complete the basic theory of mean field kernels as started in [22]. Additionally, we investigated their approximation capabilities by providing a first approximation result and a variant of the representer theorem for mean field kernels. Finally, we introduced a corresponding mean field limit of statistical learning problems and provided convergence results for SVMs using mean field kernels. In contrast to other settings involving a large number of variables, for example, infinite-width neural networks, here we considered the case of an increasing number of inputs. This work opens many directions for future investigation. For example, it would be interesting to remove or weaken Assumption 4.6 for a result like Proposition 4.7. Another relevant direction is to find approximation results that are stronger than Proposition 3.1. Finally, it would be interesting to investigate whether statistical guarantees, like consistency or learning rates, for the finite-input learning problems can be transferred to the mean field level.

## Acknowledgments and Disclosure of Funding

We would like to thank Noel Brindise, Pierre-Francois Massiani and Alexander von Rohr for very detailed and helpful comments on the manuscript, and the anonymous reviewers for their detailed and helpful comments. This work is funded in part under the Excellence Strategy of the Federal Government and the Lander (G:(DE-82)EXS-SF-SFDdM035), which the authors gratefully acknowledge. The authors further thank the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) for the financial support through 320021702/GRK2326, 333849990/IRTG-2379, B04, B05, and B06 of 442047500/SFB1481, HE5386/18-1,19-2,22-1,23-1,25-1.

## References

* [1] Carlo Cercignani. _Rarefied gas dynamics: from basic concepts to actual calculations_, volume 21 of _Cambridge Texts in Applied Mathematics_. Cambridge university press, 2000.
* [2] Carlo Cercignani, Reinhard Illner, and Mario Pulvirenti. _The mathematical theory of dilute gases_, volume 106 of _Applied Mathematical Sciences_. Springer Science & Business Media, 1994.
* [3] Michele Ballerini, Nicola Cabibbo, Raphael Candelier, Andrea Cavagna, Evaristo Cisbani, Irene Giardina, Vivien Lecomte, Alberto Orlandi, Giorgio Parisi, Andrea Procaccini, et al. Interaction ruling animal collective behavior depends on topological rather than metric distance: Evidence from a field study. _Proceedings of the national academy of sciences_, 105(4):1232-1237, 2008.
* [4] Yael Katz, Kolbjorn Tunstrom, Christos C Ioannou, Cristian Huepe, and Iain D Couzin. Inferring the structure and dynamics of interactions in schooling fish. _Proceedings of the National Academy of Sciences_, 108(46):18720-18725, 2011.
* [5] Giuseppe Toscani. Kinetic models of opinion formation. _Communications in mathematical sciences_, 4(3):481-496, 2006.
* [6] Claudio Castellano, Santo Fortunato, and Vittorio Loreto. Statistical physics of social dynamics. _Reviews of modern physics_, 81(2):591, 2009.
* [7] John RG Dyer, Anders Johansson, Dirk Helbing, Iain D Couzin, and Jens Krause. Leadership, consensus decision making and collective behaviour in humans. _Philosophical Transactions of the Royal Society B: Biological Sciences_, 364(1518):781-789, 2009.
* [8] Emiliano Cristiani, Benedetto Piccoli, and Andrea Tosin. _Multiscale modeling of pedestrian dynamics_, volume 12 of _Modeling, Simulation and Applications_. Springer, 2014.
* [9] Giacomo Albi, Mattia Bongini, Emiliano Cristiani, and Dante Kalise. Invisible control of self-organizing agents leaving unknown environments. _SIAM Journal on Applied Mathematics_, 76(4):1683-1710, 2016.
* [10] Andres A Peters, Richard H Middleton, and Oliver Mason. Leader tracking in homogeneous vehicle platoons with broadcast delays. _Automatica_, 50(1):64-74, 2014.
* [11] Kwang-Kyo Oh, Myoung-Chul Park, and Hyo-Sung Ahn. A survey of multi-agent formation control. _Automatica_, 53:424-440, 2015.

* Choi et al. [2019] Young-Pil Choi, Dante Kalise, Jan Peszek, and Andres A Peters. A collisionless singular Cucker-Smale model with decentralized formation control. _SIAM Journal on Applied Dynamical Systems_, 18(4):1954-1981, 2019.
* Tosin and Zanella [2019] Andrea Tosin and Mattia Zanella. Kinetic-controlled hydrodynamics for traffic models with driver-assist vehicles. _Multiscale Modeling & Simulation_, 17(2):716-749, 2019.
* Naldi et al. [2010] Giovanni Naldi, Lorenzo Pareschi, and Giuseppe Toscani. _Mathematical modeling of collective behavior in socio-economic and life sciences_. Springer Science & Business Media, 2010.
* Vicsek and Zafeiris [2012] Tamas Vicsek and Anna Zafeiris. Collective motion. _Physics reports_, 517(3-4):71-140, 2012.
* Gong et al. [2022] Xiaoqian Gong, Michael Herty, Benedetto Piccoli, and Giuseppe Visconti. Crowd dynamics: Modeling and control of multiagent systems. _Annual Review of Control, Robotics, and Autonomous Systems_, 6, 2022.
* Carrillo et al. [2010] Jose A Carrillo, Massimo Fornasier, Giuseppe Toscani, and Francesco Vecil. Particle, kinetic, and hydrodynamic models of swarming. _Mathematical modeling of collective behavior in socio-economic and life sciences_, pages 297-336, 2010.
* Bongini et al. [2017] Mattia Bongini, Massimo Fornasier, Markus Hansen, and Mauro Maggioni. Inferring interaction rules from observations of evolutive systems i: The variational approach. _Mathematical Models and Methods in Applied Sciences_, 27(05):909-951, 2017.
* Lu et al. [2019] Fei Lu, Ming Zhong, Sui Tang, and Mauro Maggioni. Nonparametric inference of interaction laws in systems of agents from trajectory data. _Proceedings of the National Academy of Sciences_, 116(29):14424-14433, 2019.
* Steinwart and Christmann [2008] Ingo Steinwart and Andreas Christmann. _Support vector machines_. Springer Science & Business Media, 2008.
* Christmann and Steinwart [2010] Andreas Christmann and Ingo Steinwart. Universal kernels on non-standard input spaces. _Advances in neural information processing systems_, 23, 2010.
* Fiedler et al. [2023] Christian Fiedler, Michael Herty, Michael Rom, Chiara Segala, and Sebastian Trimpe. Reproducing kernel Hilbert spaces in the mean field limit. _Kinetic and Related Models_, 16(6):850-870, 2023. ISSN 1937-5093.
* Herty and Zanella [2017] Michael Herty and Mattia Zanella. Performance bounds for the mean-field limit of constrained dynamics. _Discrete & Continuous Dynamical Systems_, 37(4):2023, 2017.
* Arora et al. [2019] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. _Advances in neural information processing systems_, 32, 2019.
* Cardaliaguet [2010] Pierre Cardaliaguet. Notes on mean field games. Technical report, 2010.
* Carmona and Delarue [2018] Rene Carmona and Francois Delarue. _Probabilistic theory of mean field games with applications I-II_. Springer, 2018.
* Scholkopf et al. [2001] Bernhard Scholkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In _International conference on computational learning theory_, pages 416-426. Springer, 2001.
* Fornasier and Solombrino [2014] Massimo Fornasier and Francesco Solombrino. Mean-field optimal control. _ESAIM: Control, Optimisation and Calculus of Variations_, 20(4):1123-1152, 2014.
* Fornasier et al. [2019] Massimo Fornasier, Stefano Lisini, Carlo Orrieri, and Giuseppe Savare. Mean-field optimal control as gamma-limit of finite agent controls. _European Journal of Applied Mathematics_, 30(6):1153-1186, 2019.
* Lang [2012] Serge Lang. _Real and functional analysis_, volume 142. Springer Science & Business Media, 2012.
* Atteia [1992] Marc Atteia. _Hilbertian kernels and spline functions_. Elsevier, 1992.
* Maso [2012] Gianni Dal Maso. _An introduction to \(\Gamma\)-convergence_, volume 8 of _Progress in Nonlinear Differential Equations and Their Applications_. Springer Science & Business Media, 2012.
* Bongini [2016] Mattia Bongini. _Sparse optimal control of multiagent systems_. PhD thesis, Technische Universitat Munchen, 2016.

## Appendix A Proofs

In this section of the supplementary material, we provide detailed proofs for all results in the main text.

### Proofs for Section 2

We start with Proposition 2.1, whose proof is based on [26, Lemma 1.2].

Proof.: _of Proposition 2.1_ For \(M\in\mathbb{N}_{+}\) define the McShane extension \(F_{M}:\mathcal{P}(X)\times Z\to\mathbb{R}\) by

\[F_{M}(\mu,z)=\inf_{\vec{x}\in X^{M}}f_{M}(\vec{x},z)+L_{f}d_{\text{KR}}(\hat{ \mu}[\vec{x}],\mu).\]

Observe that \(F_{M}\) is well-defined (i.e., \(\mathbb{R}\)-valued) since \(f_{M}(\cdot,z)\) and \(L_{f}d_{\text{KR}}(\hat{\mu}[\cdot],\mu)\) are bounded for every \(z\in Z\) (since \(f_{M}\) and \(d_{\text{KR}}(\hat{\mu}[\cdot],\mu)\) are continuous and \(\mathcal{P}(X)\) is compact, hence bounded).

**Step 1**\(F_{M}\) extends \(f_{M}\), i.e., for all \(M\in\mathbb{N}_{+}\), \(\vec{x}\in X^{M}\) and \(z\in Z\) we have \(F_{M}(\hat{\mu}[\vec{x}],z)=f_{M}(\vec{x},z)\). To show this, let \(\vec{x}\in X^{M}\) and \(z\in Z\) be arbitrary and observe that by definition

\[F_{M}(\hat{\mu}[\vec{x}],z)=\inf_{\vec{x}^{\prime}\in X^{M}}f_{M}(\vec{x}^{ \prime},z)+L_{f}d_{\text{KR}}(\hat{\mu}[\vec{x}^{\prime}],\hat{\mu}[\vec{x}]) \leq f_{M}(\vec{x},z)+L_{f}d_{\text{KR}}(\hat{\mu}[\vec{x}],\hat{\mu}[\vec{x}] )=f_{M}(\vec{x},z).\]

If \(F_{M}(\hat{\mu}[\vec{x}],z)<f_{M}(\vec{x},z)\), then there exists some \(\vec{x}^{\prime}\in X^{M}\) such that

\[f_{M}(\vec{x}^{\prime},z)+L_{f}d_{\text{KR}}(\hat{\mu}[\vec{x}^{\prime}],\hat{ \mu}[\vec{x}])<f_{M}(\vec{x},z),\]

but this means that

\[L_{f}d_{\text{KR}}(\hat{\mu}[\vec{x}^{\prime}],\hat{\mu}[\vec{x}])<f_{M}(\vec{ x},z)-f_{M}(\vec{x}^{\prime},z)\leq|f_{M}(\vec{x},z)-f_{M}(\vec{x}^{\prime},z)|,\]

contradicting the \(L_{f}\)-Lipschitz continuity of \(f_{M}\).

**Step 2** All \(F_{M}\) are \(L_{f}\)-continuous: Let \(M\in\mathbb{N}_{+}\), \(\mu_{i}\in\mathcal{P}(X)\) and \(z_{i}\in Z\), \(i=1,2\), be arbitrary. Since \(X^{M}\) is compact and \(f_{M}(\cdot,z)\) and \(L_{f}d_{\text{KR}}(\hat{\mu}[\cdot],\mu_{i})\), \(i=1,2\), are continuous, the infimum in the definition of \(F_{M}\) is actually attained. Let \(\vec{x}_{2}\in X^{M}\) such that \(F_{M}(\mu_{2},z_{2})=f_{M}(\vec{x}_{2},z_{2})+L_{f}d_{\text{KR}}(\hat{\mu}[ \vec{x}_{2}],\mu_{2})\), then we have

\[F_{M}(\mu_{1},z_{1}) \leq f_{M}(\vec{x}_{2},z_{1})+L_{f}d_{\text{KR}}(\hat{\mu}[\vec{x }_{2}],\mu_{1})\] \[=f_{M}(\vec{x}_{2},z_{1})+L_{f}d_{\text{KR}}(\hat{\mu}[\vec{x}_{2 }],\mu_{2})-L_{f}d_{\text{KR}}(\hat{\mu}[\vec{x}_{2}],\mu_{2})+L_{f}d_{\text{ KR}}(\hat{\mu}[\vec{x}_{2}],\mu_{1})\] \[\leq f_{M}(\vec{x}_{2},z_{2})+L_{f}d_{\text{KR}}(\hat{\mu}[\vec{x} _{2}],\mu_{2})+L_{f}d_{Z}(z_{1},z_{2})-L_{f}d_{\text{KR}}(\hat{\mu}[\vec{x}_{2 }],\mu_{2})\] \[\qquad+L_{f}d_{\text{KR}}(\hat{\mu}[\vec{x}_{2}],\mu_{1})\] \[\leq F_{M}(\mu_{2},z_{2})+L_{f}d_{Z}(z_{1},z_{2})-L_{f}d_{\text{KR} }(\hat{\mu}[\vec{x}_{2}],\mu_{2})+L_{f}d_{\text{KR}}(\mu_{1},\mu_{2})\] \[\qquad+L_{f}d_{\text{KR}}(\hat{\mu}[\vec{x}_{2}],\mu_{2})\] \[=F_{M}(\mu_{2},z_{2})+L_{f}(d_{\text{KR}}(\mu_{1},\mu_{2})+d_{Z} (z_{1},z_{2})),\]

where we used the definition of \(F_{M}\) in the first inequality, the Lipschitz continuity of \(f_{M}\) (w.r.t. the second argument) for the second inequality, and then the fact that \(\vec{x}_{2}\) attains the infimum in the definition of \(F_{M}(\mu_{2},z_{2})\) and the triangle inequality for \(d_{\text{KR}}\). Interchanging the roles of \(\mu_{1},z_{1}\) and \(\mu_{2},z_{2}\) then establishes the claim.

**Step 3** There exists \(B_{F}\in\mathbb{R}_{\geq 0}\) such that for all \(M\in\mathbb{N}_{+}\), \(\mu\in\mathcal{P}(X)\) and \(z\in Z\) we have \(|F_{M}(\mu,z)|\leq B_{F}+h(z)\): Let \(D_{\mathcal{P}(X)}\) be the diameter of \(\mathcal{P}(X)\) (which is finite since \(\mathcal{P}(X)\) is compact), then for all \(M\in\mathbb{N}_{+}\) and \(\vec{x}\in X^{M}\), \(z\in Z\), \(\mu\in\mathcal{P}(X)\) we have

\[-(B_{f}+L_{f}D_{\mathcal{P}(X)}+b(z))\leq f_{M}(\vec{x},z)+L_{f}d_{\text{KR}}( \hat{\mu}[\vec{x}],\mu)\leq B_{f}+L_{f}D_{\mathcal{P}(X)}+b(z),\]

therefore \(|F_{M}(\mu,z)|\leq B_{f}+L_{f}D_{\mathcal{P}(X)}+b(z)\), showing the claim with \(B_{F}=B_{f}+L_{f}D_{\mathcal{P}(X)}\).

**Step 4** Summarizing, \((F_{M})_{M}\) is a sequence of \(L_{f}\)-Lipschitz continuous and hence equicontinuous functions such that for all \(\mu\in\mathcal{P}(X)\) and \(z\in Z\), the set \(\{F_{M}(\mu,z)\mid M\in\mathbb{N}_{+}\}\) is relatively compact (since it is a bounded subset of \(\mathbb{R}\)). We can now use a variant of the Arzela-Ascoli theorem, cf. [30,Corollary III.3.3]. From the assumption on \(Z\), we can find a sequence \((V_{n})_{n}\) of open subsets of \(Z\) such that all \(\bar{V}_{n}\) are compact, \(\bar{V}_{n}\subseteq V_{n+1}\) and we have \(\bigcup_{n}V_{n}=Z\). Then \((F_{M}|_{\bar{V}_{n}})_{M}\) is a sequence of functions that fulfills the conditions of the Arzela-Ascoli theorem (since \(\mathcal{P}(X)\times K_{n}\) is compact), so there exists a subsequence \((F_{M_{\ell}^{(n)}|_{\bar{V}_{n}}})_{\ell}\) that converges uniformly to a continuous function on \(\mathcal{P}(X)\times\bar{V}_{n}\). Denote the diagonal subsequence of all these subsequences by \((F_{M_{\ell}})_{\ell}\), then there exists a continuous \(f:\mathcal{P}(X)\times Z\to\mathbb{R}\) such that \((F_{M_{\ell}})_{\ell}\) converges uniformly on compact subsets to \(f\). Since \(\mathcal{P}(X)\) is compact, this means that for all compact \(K\subseteq Z\)

\[\lim_{\ell}\sup_{\begin{subarray}{c}\mu\in\mathcal{P}(X)\\ z\in K\end{subarray}}|F_{M_{\ell}}(\mu,z)-f(\mu,z)|=0.\]

This also implies that for all \(\mu\in\mathcal{P}(X)\) and \(z\in Z\) we have \(|f(\mu,z)|\leq B_{F}+b(z)\).

Furthermore, \(f\) is also \(L_{f}\)-Lipschitz continuous: Let \(\mu_{i}\in\mathcal{P}(X)\), \(z_{i}\in Z\), \(i=1,2\), and \(\epsilon>0\) be arbitrary. Let \(K\subseteq Z\) be compact with \(z_{1},z_{2}\in K\) and choose \(\ell\in\mathbb{N}_{+}\) such that

\[\sup_{\begin{subarray}{c}\mu\in\mathcal{P}(X)\\ z\in K\end{subarray}}|F_{M_{\ell}}(\mu,z)-f(\mu,z)|\leq\frac{\epsilon}{2}.\]

We then have

\[|f(\mu_{1},z_{1})-f(\mu_{2},z_{2})| \leq|f(\mu_{1},z_{1})-F_{M_{\ell}}(\mu_{1},z_{1})|+|F_{M_{\ell}} (\mu_{1},z_{1})-F_{M_{\ell}}(\mu_{2},z_{2})|\] \[\qquad+|F_{M_{\ell}}(\mu_{2},z_{2})-f(\mu_{2},z_{2})|\] \[\leq L_{f}\left(d_{\mathbf{R}\ell}(\mu_{1},\mu_{2})+d_{Z}(z_{1},z _{2})\right)+\epsilon,\]

and since \(\epsilon>0\) was arbitrary, the claim follows.

**Step 5** For \(\ell\in\mathbb{N}_{+}\) and \(\vec{x}\in X^{M_{\ell}}\), \(z\in Z\) we have

\[|f_{M_{\ell}}(\vec{x},z)-f(\hat{\mu}[\vec{x}],z)|=|F_{M_{\ell}}(\hat{\mu}[\vec {x}],z)-f(\hat{\mu}[\vec{x}],z)|\]

since \(F_{M_{\ell}}\) extends \(f_{M_{\ell}}\), and hence

\[\sup_{\begin{subarray}{c}\vec{x}\in X^{M_{\ell}}\\ z\in K\end{subarray}}|f_{M_{\ell}}(\vec{x},z)-f(\hat{\mu}[\vec{x}],z)|\to 0.\]

Next, we provide the proofs for the \(\Gamma\)-\(\liminf\) and \(\Gamma\)-\(\limsup\) results.

Proof.: _of Lemma 2.4_ Assume the statement is not true, i.e., \(\|f\|_{k}>\liminf_{M\to\infty}\|f_{M}\|_{M}\). This means that there exists a subsequence \(M_{\ell}\) and \(C\in\mathbb{R}_{\geq 0}\) such that \(\|f\|_{k}>\lim_{\ell}\|f_{M_{\ell}}\|_{M_{\ell}}=C\). Note that this implies that \(\|f\|_{k}>0\).

Let \(\epsilon_{1},\epsilon_{2}>0\) and \(\alpha>1\), \(\beta\in(0,1)\) be arbitrary. From Theorem B.1, there exists \((\vec{\mu},\vec{\alpha})\in\mathcal{P}(X)^{N}\times\mathbb{R}^{N}\) such that

\[\mathcal{D}(\vec{\mu},\vec{\alpha},f,k)+\epsilon_{1}\geq\|f\|_{k},\]

and w.l.o.g. we can assume that \(\epsilon_{1}>0\) is small enough so that \(\mathcal{D}(\vec{\mu},\vec{\alpha},f,k)>0\). The latter implies that \(\mathcal{E}(\vec{\mu},\vec{\alpha},f),\ \mathcal{W}(\vec{\mu},\vec{\alpha},k)>0\), so defining

\[\epsilon_{\alpha} =\frac{\alpha-1}{\alpha}\mathcal{E}(\vec{\mu},\vec{\alpha},f)\] \[\epsilon_{\beta} =(1/\beta-1)\mathcal{W}(\vec{\mu},\vec{\alpha},k)\]

we get \(\epsilon_{\alpha},\epsilon_{\beta}>0\). For each \(n=1,\ldots,N\), choose \(\vec{x}_{n}^{[M]}\in X^{M}\) such that \(\vec{x}_{n}^{[M]}\xrightarrow{\text{dso}}\mu_{n}\) for \(M\to\infty\). Choose now \(L_{1}\in\mathbb{N}\) such that for all \(\ell\geq L_{1}\) we get

\[|\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})- \mathcal{E}(\vec{\mu},\vec{\alpha},f)| \leq\epsilon_{\alpha}\] \[|\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})- \mathcal{W}(\vec{\mu},\vec{\alpha},k)| \leq\epsilon_{\beta}.\]

(cf. also the proof of Theorem 2.3) and \(\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k^{[M_{\ell}]})>0\). We then get

\[\mathcal{E}(\vec{\mu},\vec{\alpha},f) \leq\alpha\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})\] \[\mathcal{W}(\vec{\mu},\vec{\alpha},k) \geq\beta\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k^{[M_{ \ell}]}),\]so altogether

\[\frac{\mathcal{E}(\vec{\mu},\vec{\alpha},f)}{\mathcal{W}(\vec{\mu},\vec{\alpha},k)} \leq\frac{\alpha\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})}{ \beta\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k^{[M_{\ell}]})}.\]

Using Theorem B.1 again leads to

\[\frac{\alpha\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M})}{\beta \mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k^{[M_{\ell}]})}=\mathcal{D}( \vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}},k^{[M_{\ell}]})\leq\|f_{M_{ \ell}}\|_{M_{\ell}}.\]

Finally, let \(L_{2}\) such that for all \(\ell\geq L_{2}\) we have \(\|f_{M_{\ell}}\|_{M_{\ell}}\leq C+\epsilon_{2}\). For \(\ell\geq L_{1},L_{2}\) we then get

\[C <\|f\|_{k}\leq\mathcal{D}(\vec{\mu},\vec{\alpha},f,k)+\epsilon_{1}\] \[=\frac{\mathcal{E}(\vec{\mu},\vec{\alpha},f)}{\mathcal{W}(\vec{ \mu},\vec{\alpha},k)}+\epsilon_{1}\] \[\leq\frac{\alpha\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_ {M_{\ell}})}{\beta\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k^{[M_{\ell}]} )}+\epsilon_{1}\] \[\leq\frac{\alpha}{\beta}\|f_{M_{\ell}}\|_{M_{\ell}}+\epsilon_{1}\] \[\leq\frac{\alpha}{\beta}C+\frac{\alpha}{\beta}\epsilon_{2}+ \epsilon_{1}.\]

Since \(\epsilon_{1},\epsilon_{2}>0\) and \(\alpha>1\), \(\beta\in(0,1)\) were arbitrary, this implies that

\[C<\|f\|_{k}\leq C,\]

a contradiction. 

Proof.: _of Lemma 2.5_ Let \(f\in H_{k}\) be arbitrary and choose \((\epsilon_{n})_{n}\subseteq\mathbb{R}_{>0}\) with \(\epsilon_{n}\searrow 0\).

**Step 1** For each \(n\in\mathbb{N}\) choose

\[f_{n}^{\text{pre}}=\sum_{\ell=1}^{L_{n}}\alpha_{\ell}^{(n)}k(\cdot,\mu_{\ell} ^{(n)})\in H_{k}^{\text{pre}},\]

where \(\alpha_{1}^{(n)},\ldots,\alpha_{L_{n}}^{(n)}\in\mathbb{R}\) and \(\mu_{1}^{(n)},\ldots,\mu_{L_{n}}^{(n)}\in\mathcal{P}(X)\), with

\[\|f-f_{n}^{\text{pre}}\|_{k}\leq\frac{\epsilon_{n}}{3\sqrt{C_{k}}}\]

and \(\|f_{n}^{\text{pre}}\|_{k}\leq\|f\|_{k}\). To see that such a sequence of functions exists, choose some sequence \((\bar{f}_{n})_{n}\in H_{k}^{\text{pre}}\) with \(\bar{f}_{n}=\sum_{\ell=1}^{L_{n}}\bar{\alpha}_{\ell}^{(n)}k(\cdot,\bar{\mu}_{ \ell}^{(n)})\), where \(\bar{\alpha}_{\ell}^{(n)}\in\mathbb{R}\), \(\bar{\mu}_{\ell}^{(n)}\in\mathcal{P}(X)\), with \(\bar{f}_{n}\stackrel{{\|\cdot\|_{k}}}{{\longrightarrow}}f\) (exists since \(H_{k}^{\text{pre}}\) is dense in \(H_{k}\)). Define now for \(n\in\mathbb{N}\)

\[\bar{H}_{n}=\text{span}\{k(\cdot,\bar{\mu}_{\ell}^{(m)})\mid m=1,\ldots,n,\; \ell=1,\ldots,\bar{L}_{m}\}\]

and \(\hat{f}_{n}=P_{\bar{H}_{n}}f\), where \(P_{\bar{H}_{n}}\) is the orthogonal projection onto \(\bar{H}_{n}\). Then \(\bar{H}_{n}\subseteq H_{k}^{\text{pre}}\), \(\|\hat{f}_{n}\|_{k}=\|P_{\bar{H}_{n}}f\|_{k}\leq\|f\|_{k}\) and \(\|f-\hat{f}_{n}\|_{k}\leq\|f-\bar{f}_{n}\|_{k}\to 0\) (since \(\hat{f}_{n}=P_{\bar{H}_{n}}f\) is the orthogonal projection of \(f\) onto \(\bar{H}_{n}\) and \(\bar{f}_{n}\in\bar{H}_{n}\)), hence \(\hat{f}_{n}\stackrel{{\|\cdot\|_{k}}}{{\longrightarrow}}f\). We can now choose \((f_{n}^{\text{pre}})_{n}\) as a subsequence of \((\hat{f}_{n})_{n}\).

Next, for all \(n\in\mathbb{N}\) and \(\ell=1,\ldots,L_{n}\) choose \(\vec{x}_{M}^{(n,\ell)}\in X^{M}\) with \(\hat{\mu}[\vec{x}_{M}^{(n,\ell)}]\stackrel{{\text{dgs}}}{{ \longrightarrow}}\mu_{\ell}^{(n)}\) for \(M\to\infty\). Furthermore, for all \(n\in\mathbb{N}\) choose \(M_{n}\in\mathbb{N}\) such that for all \(M\geq M_{n}\) and \(\ell=1,\ldots,L_{n}\) we have

\[d_{\text{KR}}(\hat{\mu}[\vec{x}_{M}^{(n,\ell)}],\mu_{\ell}^{(n)})\leq\min\left\{ \frac{\epsilon_{n}}{3\left(1+L_{k}\sum_{\ell^{\prime}=1}^{L_{n}}|\alpha_{\ell^{ \prime}}^{(n)}|\right)},\frac{\epsilon_{n}^{2}}{2\left(1+2L_{k}\sum_{i,j=1}^{L_ {n}}|\alpha_{i}^{(n)}||\alpha_{j}^{(n)}|\right)}\right\}\]

and

\[\sup_{\vec{x},\vec{x}^{\prime}\in X^{M}}|k_{M}(\vec{x},\vec{x}^{\prime})-k( \hat{\mu}[\vec{x}],\hat{\mu}[\vec{x}^{\prime}])|\leq\min\left\{\frac{\epsilon_{ n}}{3\left(1+\sum_{\ell^{\prime}=1}^{L_{n}}|\alpha_{\ell^{\prime}}^{(n)}| \right)},\frac{\epsilon_{n}^{2}}{2\left(1+\sum_{i,j=1}^{L_{n}}|\alpha_{i}^{(n)}| |\alpha_{j}^{(n)}|\right)}\right\}.\]W.l.o.g. we can assume that \((M_{n})_{n}\) is strictly increasing. For \(M\in\mathbb{N}\), let \(n(M)\) be the largest integer such that \(M_{n(M)}\leq M\) and define

\[\hat{f}_{M}^{\text{pre}} =\sum_{\ell=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M))}k(\cdot,\hat{\mu}[ \vec{x}_{M}^{(n(M),\ell)}])\in H_{k}^{\text{pre}}\] \[f_{M} =\sum_{\ell=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M))}k_{M}(\cdot,\vec{x }_{M}^{(n(M),\ell)})\in H_{M}^{\text{pre}}.\]

**Step 2** We now show that \(f_{M}\xrightarrow[]{\mathcal{P}_{1}}f\). For this, let \(\epsilon>0\) be arbitrary and \(n_{\epsilon}\in\mathbb{N}\) such that \(\epsilon_{n}\leq\epsilon\). Let now \(M\geq M_{n_{\epsilon}}\) (note that this implies that \(n(M)\geq n_{\epsilon}\) and hence \(\epsilon_{n(M)}\leq\epsilon_{n}\)) and \(\vec{x}\in X^{M}\), then we have

\[|f(\hat{\mu}[\vec{x}])-f_{M}(\vec{x})|\leq\underbrace{|f(\hat{\mu}[\vec{x}])-f _{n(M)}(\hat{\mu}[\vec{x}])|}_{=I}+\underbrace{|f_{n(M)}(\hat{\mu}[\vec{x}])- \hat{f}_{M}^{\text{pre}}(\hat{\mu}[\vec{x}])|}_{=II}+\underbrace{|\hat{f}_{M}^{ \text{pre}}(\hat{\mu}[\vec{x}])-f_{M}(\vec{x})|}_{=III}\]

We continue with

\[I =|f(\hat{\mu}[\vec{x}])-f_{n(M)}(\hat{\mu}[\vec{x}])|\] \[=|\langle f-f_{n(M)},k(\cdot,\hat{\mu}[\vec{x}])\rangle_{k}|\] \[\leq\|f-f_{n(M)}\|_{k}\|k(\cdot,\hat{\mu}[\vec{x}])\|_{k}\] \[=\|f-f_{n(M)}\|_{k}\sqrt{k(\hat{\mu}[\vec{x}],\hat{\mu}[\vec{x}])}\] \[\leq\frac{\epsilon_{n(M)}}{3\sqrt{C_{k}}}\sqrt{C_{k}}\]

where we first used the reproducing property of \(k\), then Cauchy-Schwarz, again the reproducing property of \(k\), and finally the choice \(f_{n(M)}\) and the boundedness of \(k\).

Next,

\[II =|f_{n(M)}(\hat{\mu}[\vec{x}])-\hat{f}_{M}^{\text{pre}}(\hat{\mu} [\vec{x}])|\] \[=\left|\sum_{\ell=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M))}k(\cdot,\mu _{\ell}^{(n(M))})-\sum_{\ell=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M))}k(\cdot,\hat{ \mu}[\vec{x}_{M}^{(n(M),\ell)}])\right|\] \[\leq\sum_{\ell=1}^{L_{n(M)}}\left|\alpha_{\ell}^{(n(M))}\right| |k(\cdot,\mu_{\ell}^{(n(M))})-k(\cdot,\hat{\mu}[\vec{x}_{M}^{(n(M),\ell)}])|\] \[\leq L_{k}\sum_{\ell=1}^{L_{n(M)}}\left|\alpha_{\ell}^{(n(M))} \right|d_{\text{KR}}(\hat{\mu}[\vec{x}_{M}^{(n(M),\ell)}],\mu_{\ell}^{(n(M))})\] \[\leq\frac{\epsilon_{n(M)}}{3},\]

where we used the triangle inequality, the Lipschitz continuity of \(k\), and then the choice of the sequence \((M_{n})_{n}\).

Finally,

\[III =|\hat{f}_{M}^{\text{pre}}(\hat{\mu}[\vec{x}])-f_{M}(\vec{x})|\] \[=\left|\sum_{\ell=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M))}k(\cdot,\hat {\mu}[\vec{x}_{M}^{(n(M),\ell)}])-\sum_{\ell=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M) )}k_{M}(\cdot,\vec{x}_{M}^{(n(M),\ell)})\right|\] \[\leq\sum_{\ell=1}^{L_{n(M)}}\left|\alpha_{\ell}^{(n(M))}\right| |k(\cdot,\hat{\mu}[\vec{x}_{M}^{(n(M),\ell)}])-k_{M}(\cdot,\vec{x}_{M}^{(n(M),\ell)})|\] \[\leq\frac{\epsilon_{n(M)}}{3},\]where the triangle inequality has been used in the first step and then again the choice of the sequence \((M_{n})_{n}\).

Altogether,

\[|f(\hat{\mu}[\vec{x}])-f_{M}(\vec{x})| \leq I+II+III\] \[\leq\frac{\epsilon_{n(M)}}{3}+\frac{\epsilon_{n(M)}}{3}+\frac{ \epsilon_{n(M)}}{3}\] \[\leq\epsilon,\]

establishing \(f_{M}\xrightarrow{\mathcal{P}_{1}}f\).

**Step 3** We now show \(\limsup_{M\to\infty}\|f_{M}\|_{M}\leq\|f\|_{k}\). Let \(\epsilon>0\) be arbitrary and \(n_{\epsilon}\in\mathbb{N}\) such that \(\epsilon_{n}\leq\epsilon\) and let \(M\geq M_{n_{\epsilon}}\). We have

\[\|f_{M}\|_{M}^{2} =\sum_{\ell,\ell^{\prime}=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M))} \alpha_{\ell^{\prime}}^{(n(M))}k_{M}(\vec{x}_{M}^{(n(M),\ell^{\prime})},\vec{ x}_{M}^{(n(M),\ell^{\prime})})\] \[\leq\sum_{\ell,\ell^{\prime}=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M))} \alpha_{\ell^{\prime}}^{(n(M))}k(\mu_{\ell^{\prime}}^{(n(M))},\mu_{\ell}^{(n( M))})\,+|R_{1}|+|R_{2}|\] \[=\|f_{n(M)}^{\text{pre}}\|_{k}^{2}+R_{1}+R_{2}\] \[\leq\|f\|_{k}^{2}+R_{1}+R_{2}.\]

with remainder terms

\[R_{1} =\sum_{\ell,\ell^{\prime}=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M))} \alpha_{\ell^{\prime}}^{(n(M))}k_{M}(\vec{x}_{M}^{(n(M),\ell^{\prime})},\vec{ x}_{M}^{(n(M),\ell^{\prime})})-\sum_{\ell,\ell^{\prime}=1}^{L_{n(M)}}\alpha_{ \ell}^{(n(M))}\alpha_{\ell^{\prime}}^{(n(M))}k(\hat{\mu}[\vec{x}_{M}^{(n(M), \ell^{\prime})}],\hat{\mu}[\vec{x}_{M}^{(n(M),\ell^{\prime})}])\] \[R_{2} =\sum_{\ell,\ell^{\prime}=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M))} \alpha_{\ell^{\prime}}^{(n(M))}k(\hat{\mu}[\vec{x}_{M}^{(n(M),\ell^{\prime})} ],\hat{\mu}[\vec{x}_{M}^{(n(M),\ell^{\prime})}])-\sum_{\ell,\ell^{\prime}=1}^{ L_{n(M)}}\alpha_{\ell}^{(n(M))}\alpha_{\ell^{\prime}}^{(n(M))}k(\mu_{\ell^{\prime}}^{(n( M))},\mu_{\ell}^{(n(M))})\]

We now bound these terms, so that

\[R_{1} =\left|\sum_{\ell,\ell^{\prime}=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M)) }\alpha_{\ell^{\prime}}^{(n(M))}k_{M}(\vec{x}_{M}^{(n(M),\ell^{\prime})},\vec {x}_{M}^{(n(M),\ell^{\prime})})-\sum_{\ell,\ell^{\prime}=1}^{L_{n(M)}}\alpha_{ \ell}^{(n(M))}\alpha_{\ell^{\prime}}^{(n(M))}k(\hat{\mu}[\vec{x}_{M}^{(n(M), \ell^{\prime})}],\hat{\mu}[\vec{x}_{M}^{(n(M),\ell^{\prime})}])\right.\] \[\leq\sum_{\ell,\ell^{\prime}=1}^{L_{n(M)}}|\alpha_{\ell}^{(n(M)) }||\alpha_{\ell^{\prime}}^{(n(M))}||k_{M}(\vec{x}_{M}^{(n(M),\ell^{\prime})}, \vec{x}_{M}^{(n(M),\ell^{\prime})})-k(\hat{\mu}[\vec{x}_{M}^{(n(M),\ell^{ \prime})}],\hat{\mu}[\vec{x}_{M}^{(n(M),\ell^{\prime})}])|\] \[\leq\frac{\epsilon_{n(M)}^{2}}{2},\]

and

\[R_{2} =\left|\sum_{\ell,\ell^{\prime}=1}^{L_{n(M)}}\alpha_{\ell}^{(n(M)) }\alpha_{\ell^{\prime}}^{(n(M))}k(\hat{\mu}[\vec{x}_{M}^{(n(M),\ell^{\prime})} ],\hat{\mu}[\vec{x}_{M}^{(n(M),\ell^{\prime})}])-\sum_{\ell,\ell^{\prime}=1}^{ L_{n(M)}}\alpha_{\ell}^{(n(M))}\alpha_{\ell^{\prime}}^{(n(M))}k(\mu_{\ell^{\prime}}^{(n(M))},\mu_{\ell}^{(n(M))})\right|\] \[\leq\sum_{\ell,\ell^{\prime}=1}^{L_{n(M)}}|\alpha_{\ell}^{(n(M)) }||\alpha_{\ell^{\prime}}^{(n(M))}||k(\hat{\mu}[\vec{x}_{M}^{(n(M),\ell^{ \prime})}],\hat{\mu}[\vec{x}_{M}^{(n(M),\ell^{\prime})}])-k(\mu_{\ell^{\prime}}^{(n (M))},\mu_{\ell}^{(n(M))})|\] \[\leq L_{k}\sum_{\ell,\ell^{\prime}=1}^{L_{n(M)}}|\alpha_{\ell}^{( n(M))}||\alpha_{\ell^{\prime}}^{(n(M))}|\left(d_{\text{RA}}(\hat{\mu}[\vec{x}_{M}^{(n(M), \ell)}],\mu_{\ell}^{(n(M))})+d_{\text{RA}}(\hat{\mu}[\vec{x}_{M}^{(n(M),\ell^{ \prime})}],\mu_{\ell^{\prime}}^{(n(M))})\right)\] \[\leq\frac{\epsilon_{n(M)}^{2}}{2}.\]

Altogether,

\[\|f_{M}\|_{M}^{2} \leq\|f\|_{k}^{2}+|R_{1}|+|R_{2}|\] \[\leq\|f\|_{k}^{2}+\frac{\epsilon_{n(M)}^{2}}{2}+\frac{\epsilon_{n(M )}^{2}}{2}\] \[\leq\|f\|_{k}^{2}+\epsilon^{2},\]so \(\|f_{M}\|_{M}\leq\|f\|_{k}+\epsilon\) for all \(M\geq M_{n_{\epsilon}}\), and since \(\epsilon>0\) was arbitrary, we finally get \(\limsup_{M\to\infty}\|f_{M}\|_{M}\leq\|f\|_{k}\). 

Finally, we can now provide the proof for the central Theorem 2.3.

Proof.: _of Theorem 2.3_ The first statement is part of Lemma 2.5. Let us turn to the second statement: The existence of the subsequence \((f_{M_{\ell}})_{\ell}\) and the continuous function \(f:\mathcal{P}(X)\to\mathbb{R}\) with \(f_{M_{\epsilon}}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}f\) was shown in [22, Corollary 4.3], so we only have to ensure that \(f\in H_{k}\) with \(\|f\|_{k}\leq B\). For this, we use the characterization of RKHS functions from Theorem B.1. In particular, we will utilize the notation introduced there.

**Step 1** Let \((\vec{\mu},\vec{\alpha})\in\mathcal{P}(X)^{N}\times\mathbb{R}^{N}\). We show that if \(\mathcal{W}(\vec{\mu},\vec{\alpha},k)=0\), then \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)=0\).

Assume that \(\mathcal{W}(\vec{\mu},\vec{\alpha},k)=0\). If \(B=0\), then \(f_{M}\equiv 0\) and \(f_{M_{\ell}}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}f\) implies that \(f\equiv 0\), so the claim is clear in this case. Assume now \(B>0\), let \(\epsilon>0\) be arbitrary and for \(n=1,\ldots,N\), choose sequences \(\vec{x}_{1}^{[M]}\in X^{M}\) such that \(\vec{x}_{n}^{[M]}\stackrel{{ d\mathsf{x}_{\mathsf{x}}}}{{ \longrightarrow}}\mu_{n}\) for \(M\to\infty\). For convenience, define \(\vec{X}^{[M]}=\left(\vec{x}_{1}^{[M]}\quad\cdots\quad\vec{x}_{N}^{[M]}\right)\). Choose now \(\ell_{\epsilon}\in\mathbb{N}\) such that for all \(M\geq M_{\ell_{\epsilon}}\) we get \(\mathcal{W}(\vec{X}^{[M]},\vec{\alpha},k_{M})\leq\epsilon/B\). This is possible since \(k_{M}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}k\) together with the continuity of \(k_{M}\) and \(k\) as well as \(\vec{x}_{n}^{[M]}\stackrel{{ d\mathsf{x}_{\mathsf{x}}}}{{ \longrightarrow}}\mu_{n}\) for \(M\to\infty\) and all \(n=1,\ldots,N\) implies that \(\mathcal{W}(\vec{X}^{[M]},\vec{\alpha},k_{M})\to\mathcal{W}(\vec{\mu},\vec{ \alpha},k)=0\). Let now \(\ell\geq\ell_{\epsilon}\) be arbitrary and observe that \(f_{M}\in H_{M}\) implies \(\mathcal{N}(f_{M},k_{M})<\infty\) according to Theorem B.1, so in particular \(\mathcal{D}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}},k_{M_{\ell}})<\infty\).

If \(\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})=0\), then we get that \(\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})=0\leq\epsilon\) since \(\mathcal{D}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}},k_{M_{\ell}})<\infty\), which implies by definition that \(\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})=0\).

If \(\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})>0\), then we have

\[\frac{\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})}{\mathcal{ W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})}=\mathcal{D}(\vec{X}^{[M_{ \ell}]},\vec{\alpha},f_{M_{\ell}},k_{M_{\ell}})\leq\mathcal{N}(f_{M_{\ell}},k_ {M_{\ell}})=\|f_{M_{\ell}}\|_{M_{\ell}}\leq B,\]

which implies

\[\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})\leq B\mathcal{W}( \vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})\leq\epsilon.\]

Since \(f_{M_{\ell}}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}f\) together with the continuity of \(f_{M}\) and \(f\) as well as \(\vec{x}_{n}^{[M]}\stackrel{{ d\mathsf{x}_{\mathsf{x}}}}{{ \longrightarrow}}\mu_{n}\) implies that \(\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})\to\mathcal{E}( \vec{\mu},\vec{\alpha},f)\), we get that \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)\leq\epsilon\), and since \(\epsilon>0\) was arbitrary we arrive at \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)\leq 0\).

Assume now that \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)<0\). This implies that there exist \(\delta>0\) and \(\ell_{\delta}\in\mathbb{N}\) such that for all \(\ell\geq\ell_{\delta}\) we have \(\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})\leq-\delta<0\), since \(\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})\to\mathcal{E}( \vec{\mu},\vec{\alpha},f)\). Let \(\ell\geq\ell_{\delta}\), then we get that \(\mathcal{E}(\vec{X}^{[M_{\ell}]},-\vec{\alpha},f_{M_{\ell}})\geq\delta>0\) and we have \(\mathcal{W}(\vec{X}^{[M_{\ell}]},-\vec{\alpha},k_{M_{\ell}})=\mathcal{W}(\vec{X} ^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})>0\). We can then continue with

\[\frac{\delta}{\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_ {\ell}})} \leq\frac{\mathcal{E}(\vec{X}^{[M_{\ell}]},-\vec{\alpha},f_{M_{ \ell}})}{\mathcal{W}(\vec{X}^{[M_{\ell}]},-\vec{\alpha},k_{M_{\ell}})}\] \[\leq\mathcal{D}(\vec{X}^{[M_{\ell}]},-\vec{\alpha},f_{M_{\ell}},k_ {M_{\ell}})\] \[\leq\mathcal{N}(f_{M_{\ell}},k_{M_{\ell}})\] \[=\|f_{M_{\ell}}\|_{M_{\ell}}\leq B,\]

which implies that \(\mathcal{W}(\vec{X}^{[M_{\ell}]},-\vec{\alpha},k_{M_{\ell}})=\mathcal{W}(\vec{X} ^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})\geq\delta/B\). But since \(\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})\to\mathcal{W}(\vec{ \mu},\vec{\alpha},k)\), this implies that \(\mathcal{W}(\vec{\mu},\vec{\alpha},k)\geq\delta/B>0\), a contradiction. Altogether, \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)=0\).

**Step 2** Let \((\vec{\mu},\vec{\alpha})\in\mathcal{P}(X)^{N}\times\mathbb{R}^{N}\). If \(\mathcal{W}(\vec{\mu},\vec{\alpha},k)>0\) and \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)>0\), then

\[\frac{\mathcal{E}(\vec{\mu},\vec{\alpha},f)}{\mathcal{W}(\vec{\mu},\vec{\alpha},k )}\leq B.\]To show this, let \(\alpha>1\) and \(\beta\in(0,1)\) be arbitrary. Define

\[\epsilon_{\alpha} =\frac{\alpha-1}{\alpha}\mathcal{E}(\vec{\mu},\vec{\alpha},f)\] \[\epsilon_{\beta} =(1/\beta-1)\mathcal{W}(\vec{\mu},\vec{\alpha},k)\]

and observe that \(\epsilon_{\alpha},\epsilon_{\beta}>0\). Furthermore, for all \(n=1,\ldots,N\) choose a sequence \(\vec{x}_{n}^{[M]}\in X^{M}\) such that \(\vec{x}_{n}^{[M]}\xrightarrow{d_{\mathbf{x}}}\mu_{n}\) for \(M\to\infty\), and define \(\vec{X}^{[M]}=\left(\vec{x}_{1}^{[M]}\quad\cdots\quad\vec{x}_{N}^{[M]}\right)\). Choose \(\ell_{\epsilon}\in\mathbb{N}_{+}\) such that for all \(\ell\geq\ell_{\epsilon}\) we have

\[|\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})- \mathcal{E}(\vec{\mu},\vec{\alpha},f)| \leq\epsilon_{\alpha}\] \[|\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})- \mathcal{W}(\vec{\mu},\vec{\alpha},k)| \leq\epsilon_{\beta}\]

and \(\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})>0\). Such an \(\ell_{\epsilon}\) exists because \(k_{M}\xrightarrow{\mathcal{P}_{1}}k\) together with the continuity of \(k_{M}\) and \(k\) as well as the convergence of \(\vec{x}_{n}^{[M]}\) to \(\mu_{n}\) imply that \(\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})\to\mathcal{W}(\vec {\mu},\vec{\alpha},k)\), and \(f_{M_{\ell}}\xrightarrow{\mathcal{P}_{1}}f\) together with the continuity of \(f_{M}\) and \(f\) imply that \(\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})\to\mathcal{E}(\vec {\mu},\vec{\alpha},f)\).

Let now \(\ell\geq\ell_{\epsilon}\) be arbitrary. By definition of \(\epsilon_{\alpha}\) we get \(\alpha\epsilon_{\alpha}\leq(\alpha-1)\mathcal{E}(\vec{\mu},\vec{\alpha},f)\), which in turn leads to

\[\epsilon_{\alpha} \leq\epsilon_{\alpha}-\alpha\epsilon_{\alpha}+(\alpha-1)\mathcal{ E}(\vec{\mu},\vec{\alpha},f)\] \[=-(\alpha-1)\epsilon_{\alpha}+(\alpha-1)\mathcal{E}(\vec{\mu}, \vec{\alpha},f)\] \[=(\alpha-1)(\mathcal{E}(\vec{\mu},\vec{\alpha},f)-\epsilon_{ \alpha})\] \[\leq(\alpha-1)\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M _{\ell}}),\]

where we used in the last inequality that \(\alpha-1>0\) and by choice of \(\ell_{\epsilon}\) we have \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)\leq\mathcal{E}(\vec{X}^{[M_{\ell}]}, \vec{\alpha},f_{M_{\ell}})+\epsilon_{\alpha}\). We can then continue with

\[\mathcal{E}(\vec{\mu},\vec{\alpha},f) \leq\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})+ \epsilon_{\alpha}\] \[\leq\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})+ (\alpha-1)\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}})\] \[=\alpha\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec{\alpha},f_{M_{\ell}}).\]

Next, by definition of \(\epsilon_{\beta}\) and choice of \(\ell_{\epsilon}\) we find that

\[\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}}) \leq\mathcal{W}(\vec{\mu},\vec{\alpha},k)+\epsilon_{\beta}\] \[=\mathcal{W}(\vec{\mu},\vec{\alpha},k)+(1/\beta-1)\mathcal{W}( \vec{\mu},\vec{\alpha},k)\] \[=(1/\beta)\mathcal{W}(\vec{\mu},\vec{\alpha},k),\]

hence

\[\frac{1}{\mathcal{W}(\vec{\mu},\vec{\alpha},k)}\leq\frac{1}{\beta\mathcal{W}( \vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{\ell}})}.\]

Combining these results, we get that for all \(\ell\geq\ell_{\epsilon}\)

\[\frac{\mathcal{E}(\vec{\mu},\vec{\alpha},f)}{\mathcal{W}(\vec{\mu},\vec{ \alpha},k)}\leq\frac{\alpha}{\beta}\frac{\mathcal{E}(\vec{X}^{[M_{\ell}]},\vec {\alpha},f_{M_{\ell}})}{\mathcal{W}(\vec{X}^{[M_{\ell}]},\vec{\alpha},k_{M_{ \ell}})}\leq\frac{\alpha}{\beta}\mathcal{N}(f_{M_{\ell}},k_{M_{\ell}})=\frac{ \alpha}{\beta}\|f_{M_{\ell}}\|_{M_{\ell}}\leq\frac{\alpha}{\beta}B.\]

Since \(\alpha>1\) and \(\beta\in(0,1)\) were arbitrary, this shows that

\[\frac{\mathcal{E}(\vec{\mu},\vec{\alpha},f)}{\mathcal{W}(\vec{\mu},\vec{ \alpha},k)}\leq B.\]

**Step 3** Let \((\vec{\mu},\vec{\alpha})\in\mathcal{P}(X)^{N}\times\mathbb{R}^{N}\) be arbitrary. If \(\mathcal{W}(\vec{\mu},\vec{\alpha},k)=0\), then we get from Step 1 that \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)=0\leq B\). Assume now \(\mathcal{W}(\vec{\mu},\vec{\alpha},k)>0\). If \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)=0\), then again \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)=0\leq B\). If \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)>0\), then Step 2 ensures that

\[\frac{\mathcal{E}(\vec{\mu},\vec{\alpha},f)}{\mathcal{W}(\vec{\mu},\vec{ \alpha},k)}=\mathcal{D}(\vec{\mu},\vec{\alpha},f,k)\leq B.\]

Finally, if \(\mathcal{E}(\vec{\mu},\vec{\alpha},f)<0\), then again

\[\frac{\mathcal{E}(\vec{\mu},\vec{\alpha},f)}{\mathcal{W}(\vec{\mu},\vec{ \alpha},k)}=\mathcal{D}(\vec{\mu},\vec{\alpha},f,k)<0\leq B.\]

Altogether, we get that \(\mathcal{D}(\vec{\mu},\vec{\alpha},f,k)\leq B\). Since \((\vec{\mu},\vec{\alpha})\) was arbitrary, maximization leads to \(\mathcal{N}(f,k)\leq B<\infty\), hence \(f\in H_{k}\) and \(\|f\|_{k}=\mathcal{N}(f,k)\leq B\).

### Proofs for Section 3

In this section we provide the proofs for the results relating to approximation with kernels in the mean field limit.

Proof.: _of Proposition 3.1_ Let \(f\in\mathcal{F}\) and \(\epsilon>0\) be arbitrary. Let \(B\in\mathbb{R}_{\geq 0}\) and \(f_{M}\in\mathcal{F}_{M}\), \(\hat{f}_{M}\in H_{M}\), \(M\in\mathbb{N}_{+}\), such that \(f_{M}\xrightarrow{\mathcal{P}_{1}}f\), \(\|f_{M}-\hat{f}_{M}\|\leq\frac{\epsilon}{5}\) and \(\|\hat{f}_{M}\|_{M}\leq B\) for all \(M\in\mathbb{N}_{+}\) (exist by definition of \(\mathcal{F}\)). Theorem 2.3 ensures that there exists a subsequence \((f_{M_{\epsilon}})_{\ell}\) and \(\hat{f}\in H_{k}\) with \(\|\hat{f}\|_{k}\leq B\) such that \(\hat{f}_{M_{\epsilon}}\xrightarrow{\mathcal{P}_{1}}\hat{f}\) for \(\ell\to\infty\). Choose now \(L_{1}\in\mathbb{N}_{+}\) such that for all \(\ell\geq L_{1}\) we have

\[\sup_{\vec{x}\in X^{M_{\epsilon}}}|\hat{f}_{M_{\epsilon}}(\vec{x} )-\hat{f}(\hat{\mu}[\vec{x}])| \leq\frac{\epsilon}{5}\] \[\sup_{\vec{x}\in X^{M_{\epsilon}}}|f_{M_{\epsilon}}(\vec{x})-f( \hat{\mu}[\vec{x}])| \leq\frac{\epsilon}{5}.\]

Let now \(\mu\in\mathcal{P}(X)\) be arbitrary and choose a sequence \(\vec{x}_{M}\in X^{M}\) with \(\hat{\mu}[\vec{x}_{M}]\xrightarrow{d_{\text{box}}}\mu\). Finally, let \(L_{2}\in\mathbb{N}_{+}\) such that for all \(\ell\geq L_{2}\) we have

\[|f(\mu)-f(\hat{\mu}[\vec{x}_{M_{\epsilon}}])| \leq\frac{\epsilon}{5}\] \[|\hat{f}(\mu)-\hat{f}(\hat{\mu}[\vec{x}_{M_{\epsilon}}])| \leq\frac{\epsilon}{5}\]

(such an \(L_{2}\) exists due to the continuity of \(f\) and \(\hat{f}\)).

We now have for \(\ell\geq\max\{L_{1},L_{2}\}\) that

\[|f(\mu)-\hat{f}(\mu)| \leq|f(\mu)-f(\hat{\mu}[\vec{x}_{M_{\epsilon}}])|+|f(\hat{\mu}[ \vec{x}_{M_{\epsilon}}])-f_{M_{\epsilon}}(\vec{x}_{M_{\epsilon}})|+|f_{M_{ \epsilon}}(\vec{x}_{M_{\epsilon}})-\hat{f}_{M_{\epsilon}}(\vec{x}_{M_{\epsilon }})|\] \[\qquad+|\hat{f}_{M_{\epsilon}}(\vec{x}_{M_{\epsilon}})-\hat{f}( \hat{\mu}[\vec{x}_{M_{\epsilon}}])|+|\hat{f}(\hat{\mu}[\vec{x}_{M_{\epsilon}}] )-\hat{f}(\mu)|\] \[\leq\frac{\epsilon}{5}+\frac{\epsilon}{5}+\frac{\epsilon}{5}+ \frac{\epsilon}{5}+\frac{\epsilon}{5}=\epsilon.\]

Since \(\mu\) was arbitrary, the result follows. 

Proof.: _of Remark 3.2_ We first show that \(\mathcal{F}\) is a subvectorspace. Let \(f,g\in\mathcal{F}\) and \(\lambda\in\mathbb{R}\), \(\epsilon>0\) be arbitrary. W.l.o.g. we can assume \(\lambda\neq 0\). Choose sequences \(f_{M},g_{M}\in\mathcal{F}_{M}\), \(\hat{f}_{M},\hat{g}_{M}\in H_{M}\), \(M\in\mathbb{N}_{+}\), and constants \(B_{f},B_{g}\in\mathbb{R}_{\geq 0}\) from the definition of \(\mathcal{F}\) for \(f\), \(\frac{\epsilon}{2|\lambda|}\), and \(g\), \(\frac{\epsilon}{2}\), respectively. Let \(M\in\mathbb{N}_{+}\), \(\vec{x}\in X^{M}\) be arbitrary, then

\[|\lambda f_{M}(\vec{x})+g(\vec{x})-(\lambda f(\hat{\mu}[\vec{x}])-g(\hat{\mu} [\vec{x}]))|\leq|\lambda||f_{M}(\vec{x})-f(\hat{\mu}[\vec{x}])|+|g_{M}(\vec{x} )-g(\hat{\mu}[\vec{x}])|\]

together with \(f_{M}\xrightarrow{\mathcal{P}_{1}}f\), \(g_{M}\xrightarrow{\mathcal{P}_{1}}g\) shows that \(\lambda f_{M}+g_{M}\xrightarrow{\mathcal{P}_{1}}\lambda f+g\).

Next, we have for all \(M\in\mathbb{N}_{+}\) that

\[\|(\lambda f_{M}+g_{M})-(\lambda\hat{f}_{M}+\hat{g}_{M})\|_{\infty}\leq| \lambda|\|f_{M}-\hat{f}_{M}\|_{\infty}+\|g_{M}-\hat{g}_{M}\|_{\infty}\leq| \lambda|\frac{\epsilon}{2|\lambda|}+\frac{\epsilon}{2}=\epsilon.\]

Finally,

\[\|\lambda\hat{f}_{M}+\hat{g}_{M}\|_{M}\leq|\lambda|\|\hat{f}_{M}\|_{M}+\|\hat{ g}_{M}\|_{M}\leq|\lambda|B_{f}+B_{g},\]

establishing that \((\lambda\hat{f}_{M}+\hat{g}_{M})_{M}\) is uniformly norm-bounded. Altogether, we have that \(\lambda f+g\in\mathcal{F}\).

We now turn to the second claim. Let \((f^{(n)})_{n}\subseteq\mathcal{F}\) such that \(f^{(n)}\to f\) for some \(f\in C^{0}(\mathcal{P}(X),\mathbb{R})\) and for all \(\bar{\epsilon}>0\) there exist \(f_{M}^{(n)}\in\mathcal{F}_{M}\), \(\hat{f}_{M}^{(n)}\in H_{M}\), \((\rho_{M})_{M}\subseteq\mathbb{R}_{\geq 0}\) and \(B^{(n)}\in\mathbb{R}_{\geq 0}\) with \(\rho_{M}\searrow 0\), \(\|f_{M}^{(n)}-\hat{f}_{M}^{(n)}\|_{\infty}\leq\bar{\epsilon}\) and \(\|\hat{f}_{M}^{(n)}\|_{M}\leq B^{(n)}\) for all \(n,M\in\mathbb{N}_{+}\), and

\[\sup_{\vec{x}\in X^{M}}|f_{M}^{(n)}(\vec{x})-f^{(n)}(\hat{\mu}[\vec{x}])|\leq \rho_{M}\]for all \(n,M\in\mathbb{N}_{+}\). We now show that \(f\in\mathcal{F}\). For this, let \(\epsilon>0\) be arbitrary and choose \(f_{M}^{(n)}\in\mathcal{F}_{M}\), \(\hat{f}_{M}^{(n)}\in H_{M}\), \((\rho_{M})_{M}\subseteq\mathbb{R}_{\geq 0}\) and \(B^{(n)}\in\mathbb{R}_{\geq 0}\) as above with \(\bar{\epsilon}=\frac{\epsilon}{4}\). Let \(N\in\mathbb{N}_{+}\) be such that \(\|f^{(m)}-f^{(n)}\|_{\infty}\leq\frac{\epsilon}{4}\) for all \(m,n\geq N\) (such an \(N\) exists since \((f^{(n)})_{n}\) converges in \(C^{0}(\mathcal{P}(X),\mathbb{R})\) and hence is a Cauchy sequence). Furthermore, let \(M_{\rho}\in\mathbb{N}_{+}\) be such that for all \(M\geq M_{\rho}\) we have \(\rho_{M}\leq\frac{\epsilon}{4}\). Define now \(f_{M}=f_{M}^{(M)}\) and \(\hat{f}_{M}=\hat{f}_{M}^{(M)}\) for \(M=1,\ldots,M_{\rho}-1\), and \(f_{M}=f_{M}^{(M+N)}\), \(\hat{f}_{M}=\hat{f}_{M}^{(N)}\) for \(M\geq M_{\rho}\).

**Step 1** Let \(M\geq M_{\rho}\) and \(\vec{x}\in X^{M}\) be arbitrary. We have

\[|f_{M}(\vec{x})-f(\hat{\mu}[\vec{x}])| =|f_{M}^{(N+M)}(\vec{x})-f(\hat{\mu}[\vec{x}])|\] \[\leq|f_{M}^{(N+M)}(\vec{x})-f^{(N+M)}(\hat{\mu}[\vec{x}])|+|f^{(N +M)}(\hat{\mu}[\vec{x}])-f(\hat{\mu}[\vec{x}])|\] \[\leq\rho_{M}+\|f^{(N+M)}-f\|_{\infty},\]

and since the right hand side (which is independent of \(\vec{x}\)) converges to 0 for \(M\to\infty\), we get \(f_{M}\xrightarrow{\mathcal{P}_{1}}f\).

**Step 2** For \(M=1,\ldots,M_{\rho}\) we get

\[\|f_{M}-\hat{f}_{M}\|_{\infty}=\|f_{M}^{(M)}-\hat{f}_{M}^{(M)}\|_{\infty}\leq \bar{\epsilon}\leq\epsilon.\]

Let now \(M\geq M_{\rho}\) and \(\vec{x}\in X^{M}\) be arbitrary. We have

\[|f_{M}(\vec{x})-\hat{f}_{M}(\vec{x})| =|f_{M}^{(M+N)}(\vec{x})-\hat{f}_{M}^{(N)}(\vec{x})|\] \[\leq|f_{M}^{(M+N)}(\vec{x})-f^{(N+M)}(\hat{\mu}[\vec{x}])|+|f^{(N +M)}(\hat{\mu}[\vec{x}])-f^{(N)}(\hat{\mu}[\vec{x}])|\] \[\qquad+|f^{(N)}(\hat{\mu}[\vec{x}])-f_{M}^{(N)}(\vec{x})|+|f_{M}^ {(N)}(\vec{x})-\hat{f}_{M}^{(N)}(\vec{x})|\] \[\leq\sup_{\vec{x}^{\prime}\in X^{M}}|f_{M}^{(M+N)}(\vec{x}^{\prime })-f^{(M+N)}(\hat{\mu}[\vec{x}^{\prime}])|+\|f^{(M+N)}-f^{(N)}\|_{\infty}\] \[\qquad+\sup_{\vec{x}^{\prime}\in X^{M}}|f^{(N)}(\hat{\mu}[\vec{ x}^{\prime}])-f_{M}^{(N)}(\vec{x}^{\prime})|+\|f_{M}^{(N)}-\hat{f}_{M}^{(N)}\|_{\infty}\] \[\leq\rho_{M}+\frac{\epsilon}{4}+\rho_{M}+\bar{\epsilon}\] \[\leq 4\frac{\epsilon}{4}=\epsilon,\]

and since \(\vec{x}\in X^{M}\) was arbitrary, we get \(\|f_{M}-\hat{f}_{M}\|_{\infty}\leq\epsilon\).

**Step 3** For \(M=1,\ldots,M_{\rho}-1\) we get by construction that \(\|\hat{f}_{M}\|_{M}=\|\hat{f}_{M}^{(M)}\|_{M}\leq B^{(M)}\), and for \(M\geq M_{\rho}\) we find \(\|\hat{f}_{M}\|_{M}=\|\hat{f}_{M}^{(N)}\|_{M}\leq B^{(N)}\). Altogether, we get for \(M\in\mathbb{N}_{+}\) that

\[\|\hat{f}_{M}\|_{M}\leq\max\{B^{(1)},\ldots,B^{(M_{\rho}-1)},B^{(N)}\}.\]

Combining the three steps establishes that \(f\in\mathcal{F}\). 

Finally, here is the proof of the represnter theorem in the mean field limit.

Proof.: _of Theorem 3.3_ The existence and uniqueness of \(f_{M}\) and \(f\) follows from the well-known representer theorem (applied to all \(k_{M}\) and \(k\)).

We now turn to the convergence of the minimizers. For all \(M\in\mathbb{N}_{+}\) we have

\[\lambda\|f_{M}^{*}\|_{M}\leq L(f_{M}^{*}(\vec{x}_{1}^{[M]}),\ldots,f_{M}^{*}( \vec{x}_{N}^{[M]}))+\lambda\|f\|_{M}\leq L(0,\ldots,0),\]

i.e., \(\|f_{M}^{*}\|_{M}\leq L(0,\ldots,0)/\lambda\). Define

\[\mathcal{L}_{M}:H_{M}\to\mathbb{R}_{\geq 0},\;f\mapsto L(f( \vec{x}_{1}^{[M]}),\ldots,f(\vec{x}_{N}^{[M]}))+\lambda\|f\|_{M}\] \[\mathcal{L}:H_{k}\to\mathbb{R}_{\geq 0},\;f\mapsto L(f(\mu_{1}), \ldots,f(\mu_{N}))+\lambda\|f\|_{k},\]

and let \(f_{M}\in H_{M}\) with \(f_{M}\xrightarrow{\mathcal{P}_{1}}f\) for some \(f\in H_{k}\). The continuity of \(f_{M}\), \(f\) and \(L\) as well as \(\vec{x}_{n}^{[M]}\xrightarrow{d_{\mathbb{R}}}\mu_{n}\) for \(M\to\infty\) and all \(n=1,\ldots,N\), imply then that \(\lim_{M\to\infty}L(f_{M}(\vec{x}_{1}^{[M]}),\ldots,f_{M}(\vec{x}_{N}^{[M]}))=L(f(\mu_ {1}),\ldots,f(\mu_{N}))\). Combining this with Lemma 2.4 leads to

\[\mathcal{L}(f)\leq\liminf_{M\to\infty}\mathcal{L}_{M}(f).\]

Let now \(f\in H_{k}\) be arbitrary and let \(f_{M}\in H_{M}\) be the sequence from Lemma 2.5. Using the same arguments as above we find that

\[\limsup_{M\to\infty}\mathcal{L}_{M}(f_{M})\leq\|f\|_{k}.\]

We have shown that \(\mathcal{L}_{M}\stackrel{{\Gamma}}{{\longrightarrow}}\mathcal{L}\) and hence Proposition B.3 ensures that there exists a subsequence \((f_{M_{\ell}}^{*})_{\ell}\) such that \(f_{M_{\ell}}^{*}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}f ^{*}\) and \(\mathcal{L}_{M_{\ell}}(f_{M_{\ell}}^{*})\to\mathcal{L}(f^{*})\). 

### Proofs for Section 4

Proof.: _of Lemma 4.2_ That \(\ell\) is nonnegative is clear from the proof of Proposition 2.1. Let now all \(\ell_{M}\) be convex and let \(\mu\in\mathcal{P}(X)\), \(y\in Y,t_{1},t_{2}\in\mathbb{R}\) and \(\lambda\in(0,1)\) be arbitrary, and define \(I=[\min\{t_{1},t_{2}\},\max\{t_{1},t_{2}\}]\). Furthermore, let \(\vec{x}_{M}\in X^{M}\) with \(\vec{x}_{M}\stackrel{{ d\text{\tiny{Rx}}}}{{\longrightarrow}}\mu\) for \(M\to\infty\) and \(\epsilon>0\) be arbitrary. Choose now \(M\) so large that

\[|\ell(\mu,y,\lambda t_{1}+(1-\lambda)t_{2})-\ell(\hat{\mu}[\vec{ x}_{M}],y,\lambda t_{1}+(1-\lambda)t_{2})| \leq\frac{\epsilon}{6}\sup_{\begin{subarray}{c}\vec{x}\in X^{M} \\ y^{\prime}\in Y,\ell\in I\end{subarray}}|\ell_{M}(\vec{x},y^{\prime},t^{ \prime})-\ell(\hat{\mu}[\vec{x}],y^{\prime},t^{\prime})|\] \[\leq\frac{\epsilon}{6}.\]

This is possible due to the continuity of \(\ell\), as well as \(\ell_{M}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}\ell\). We then have

\[\ell(\mu,y,\lambda t_{1}+(1-\lambda)t_{2}) \leq\ell(\hat{\mu}[\vec{x}],y,\lambda t_{1}+(1-\lambda)t_{2})+ \frac{\epsilon}{6}\] \[\leq\ell_{M}(\vec{x}_{M},y,\lambda t_{1}+(1-\lambda)t_{2})+ \frac{\epsilon}{3}\] \[\leq\lambda\ell_{M}(\vec{x}_{M},y,t_{1})+(1-\lambda)\ell_{M}( \vec{x}_{M},y,t_{2})+\frac{\epsilon}{3}\] \[\leq\lambda\ell(\hat{\mu}[\vec{x}_{M}],y,t_{1})+(1-\lambda)\ell( \hat{\mu}[\vec{x}_{M}],y,t_{2})+\frac{\epsilon}{3}+(\lambda+1-\lambda)\frac{ \epsilon}{6}\] \[\leq\lambda\ell(\mu,y,t_{1})+(1-\lambda)\ell(\mu,y,t_{2})+\epsilon,\]

and since \(\epsilon>0\) was arbitrary, this establishes

\[\ell(\mu,y,\lambda t_{1}+(1-\lambda)t_{2})\leq\lambda\ell(\mu,y,t_{1})+(1- \lambda)\ell(\mu,y,t_{2}),\]

i.e., convexity of \(\ell\). 

Proof.: _of Proposition 4.3_ From Lemma 4.2 we get that \(\ell\) is nonnegative and convex. The existence, uniqueness and the representation formulas follow then from the standard representer theorem, cf. e.g., [20, Theorem 5.5].

Furthermore, for all \(M\in\mathbb{N}_{+}\) we have

\[\lambda\|f_{M,\lambda}^{*}\|_{M}^{2} \leq\frac{1}{N}\sum_{n=1}^{N}\ell_{M}(\vec{x}_{n}^{[M]},y_{n}^{[M ]},f_{M,\lambda}^{*}(\vec{x}_{n}^{[M]}))+\lambda\|f_{M,\lambda}^{*}\|_{M}^{2}\] \[\leq\mathcal{R}_{\ell_{M},D_{N}^{[M]},\lambda}(0)\] \[\leq NC_{\ell},\]

hence \(\|f_{M,\lambda}^{*}\|_{M}\leq\sqrt{\frac{NC_{\ell}}{\lambda}}\).

Let \(f\in H_{k}\) and \((f_{M})_{M}\), \(f_{M}\in H_{M}\), such that \(f_{M}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}f\). From \(D_{N}^{[M]}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}D_{N}\) and the continuity of \(\ell_{M}\), \(\ell\), together with \(\ell_{M}\stackrel{{\mathcal{P}_{1}}}{{\longrightarrow}}\ell\) and the boundedness of \(\{y_{n}^{[M]}\mid M\in\mathbb{N}_{+},n=1,\ldots,N\}\subseteq Y\) and \(\{f_{M}(\vec{x}_{n}^{[M]})\mid M\in\mathbb{N}_{+},N=1,\ldots,N\}\) we find that

\[\lim_{M}\frac{1}{N}\sum_{n=1}^{N}\ell_{M}(\vec{x}_{n}^{[M]},y_{n}^{[M]},f_{M}( \vec{x}_{n}^{[M]}))=\frac{1}{N}\sum_{n=1}^{N}\ell(\mu_{n},y_{n},f(\mu_{n})).\]Combining this with Lemma 2.4 and Lemma 2.5 then establishes that \(\mathcal{R}_{\ell_{M,D_{N}^{[M]},\lambda}}\xrightarrow{\Gamma}\mathcal{R}_{\ell,D_{ N},\lambda}\) and the remaining claims follow from Proposition B.3 and the uniqueness of the minimizers. 

Proof.: _of Lemma 4.4_ Let \(\epsilon>0\) be arbitrary. Recall from the proof of Proposition 4.3 that for all \(M\in\mathbb{N}_{+}\) we have \(\|f_{M,\lambda}^{*}\|_{M}\leq\sqrt{\frac{NC_{\ell}}{\lambda}}\), and hence for all \(\vec{x}\in X^{M}\) we have

\[|f_{M,\lambda}^{*}(\vec{x})| \leq\|f_{M,\lambda}^{*}\|_{k}\|k_{M}(\cdot,\vec{x})\|_{k}\] \[\leq\sqrt{\frac{NC_{\ell}}{\lambda}}\sqrt{C_{k}}.\]

A similar argument applies to \(f_{\lambda}^{*}\in H_{k}\), so we can find a compact set \(K\subseteq\mathbb{R}\) with

\[\{f_{M,\lambda}^{*}(\vec{x}_{\mathrm{i}}^{[M]})\mid M\in\mathbb{N}_{+},n=1, \ldots,N\}\cup\{f_{\lambda}^{*}(\mu_{n})\mid n=1,\ldots,N\}\subseteq K.\]

Choose now \(m_{\epsilon}\in\mathbb{N}_{+}\) such that for all \(m\geq m_{\epsilon}\) we have

\[\sup_{\begin{subarray}{c}\vec{x}\in X^{M_{m}}\\ y\in Y\end{subarray}}|\ell_{M_{m}}(\vec{x},y,f_{M_{m},\lambda}^{*}(\vec{x}))- \ell_{M_{m}}(\vec{x},y,f_{\lambda}^{*}(\hat{\mu}[\vec{x}]))|\leq\frac{\epsilon }{3}\] \[\sup_{\begin{subarray}{c}\vec{x}\in X^{M_{m}}\\ y\in Y,t\in K\end{subarray}}|\ell_{M_{m}}(\vec{x},y,t)-\ell(\hat{\mu}[\vec{x}], y,t)|\leq\frac{\epsilon}{3}\] \[\left|\int_{X^{M_{m}}\times Y}\ell(\hat{\mu}[\vec{x}],y,f_{ \lambda}^{*}(\hat{\mu}[\vec{x}]))\mathrm{d}P^{[M_{m}]}(\vec{x},y)-\int_{ \mathcal{P}(X)\times Y}\ell(\mu,y,f_{\lambda}^{*}(\mu))\mathrm{d}(\mu,y)\right| \leq\frac{\epsilon}{3}.\]

Such a \(m_{\epsilon}\) exists since \(f_{M_{m},\lambda}^{*}\xrightarrow{\mathcal{P}_{1}}f_{\lambda}^{*}\) and all \(\ell_{M_{m}}\) are uniformly Lipschitz continuous (first inequality), \(\ell_{M_{m}}\xrightarrow{\mathcal{P}_{1}}\ell\) and \(Y\) and \(K\) are compact (second inequality), and \(P^{[M]}\xrightarrow{\mathcal{P}_{1}}P\) as well as that \((\mu,y)\mapsto\ell(\mu,y,f_{\lambda}^{*}(\mu))\) is continuous and bounded (third inequality). We now have

\[\left|\mathcal{R}_{\ell_{M_{m}},P^{[M_{m}]}}(f_{M_{m},\lambda}^{* })-\mathcal{R}_{\ell,P}(f_{\lambda}^{*})\right|\] \[\leq\left|\int_{X^{M_{m}}\times Y}\ell_{M_{m}}(\vec{x},y,f_{M_{m}, \lambda}^{*}(\vec{x}))-\ell_{M_{m}}(\vec{x},y,f_{\lambda}^{*}(\hat{\mu}[\vec{ x}]))\mathrm{d}P^{[M_{m}]}(\vec{x},y)\right|\] \[\qquad\qquad+\left|\int_{X^{M_{m}}\times Y}\ell_{M_{m}}(\vec{x},y,f_{\lambda}^{*}(\hat{\mu}[\vec{x}]))-\ell(\hat{\mu}[\vec{x}],y,f_{\lambda}^{* }(\hat{\mu}[\vec{x}]))\mathrm{d}P^{[M_{m}]}(\vec{x},y)\right|\] \[\qquad\qquad+\left|\int_{X^{M_{m}}\times Y}\ell(\hat{\mu}[\vec{x} ],y,f_{\lambda}^{*}(\hat{\mu}[\vec{x}]))\mathrm{d}P^{[M_{m}]}(\vec{x},y)-\int_{ \mathcal{P}(X)\times Y}\ell(\mu,y,f_{\lambda}^{*}(\mu))\mathrm{d}(\mu,y)\right|\] \[\leq\int_{X^{M_{m}}\times Y}|\ell_{M_{m}}(\vec{x},y,f_{M_{m}, \lambda}^{*}(\vec{x}))-\ell_{M_{m}}(\vec{x},y,f_{\lambda}^{*}(\hat{\mu}[\vec {x}]))|\mathrm{d}P^{[M_{m}]}(\vec{x},y)\] \[\qquad\qquad+\int_{X^{M_{m}}\times Y}|\ell_{M_{m}}(\vec{x},y,f_{ \lambda}^{*}(\hat{\mu}[\vec{x}]))-\ell(\hat{\mu}[\vec{x}],y,f_{\lambda}^{*}( \hat{\mu}[\vec{x}]))|\mathrm{d}P^{[M_{m}]}(\vec{x},y)\] \[\qquad\qquad+\frac{\epsilon}{3}\] \[\leq\epsilon,\]

and since \(\epsilon>0\) was arbitrary, the claim follows. 

Proof.: _of Proposition 4.5_ Observe that all \(k_{M}\) are bounded measurable kernels, \(\mathcal{R}_{\ell_{M},P^{[M]}}(f_{M})<\infty\) for all \(f\in H_{M}\), \(\ell_{M}\) is a convex, \(P^{[M]}\)-integrable Nemitskii loss (cf. Remark 4.1) and hence [20, Lemma 5.1, Theorem 5.2] guarantee the existence and uniqueness of \(f_{M,\lambda}^{*}\). A completely analogous argument shows the existence and uniqueness of \(f_{\lambda}^{*}\).

We now show that \(\mathcal{R}_{\ell_{M},P^{[M]},\lambda}\xrightarrow{\Gamma}\mathcal{R}_{\ell,P,\lambda}\). For the \(\Gamma\)-\(\liminf\)-inequality, let \(f_{M}\in H_{M}\), \(f\in H_{k}\) be arbitrary with \(f_{M}\xrightarrow{\mathcal{P}_{1}}f\), and let \(\epsilon>0\). Choose \(M_{\epsilon}\in\mathbb{N}_{+}\) so large that for all \(M\geq M_{\epsilon}\)

\[\left|\int\ell(\hat{\mu}[\vec{x}],y,f(\hat{\mu}[\vec{x}])\mathrm{d}P^{[M]}( \vec{x},y))-\int\ell(\mu,y,f(\mu))\mathrm{d}P(\mu,y)\right|\leq\frac{\epsilon}{2}\](this is possible since \((\mu,y)\mapsto\ell(\mu,y,f(\mu))\) is bounded and continuous and \(P^{[M]}\xrightarrow{\mathcal{P}_{1}}P\)) and

\[|\ell_{M}(\vec{x},y,f_{M}(\vec{x}))-\ell(\hat{\mu}[\vec{x}],y,f(\hat{\mu}[\vec{ x}]))|\leq\frac{\epsilon}{2}\]

for all \(\vec{x}\in X^{M}\), \(y\in Y\) (this is possible due to the same argument used in the proof of Lemma 4.4). For \(M\geq M_{\epsilon}\) we then find

\[\mathcal{R}_{\ell,P,\lambda}(f) =\int\ell(\mu,y,f(\mu))\mathrm{d}P(\mu,y)+\lambda\|f\|_{k}^{2}\] \[\leq\int\ell_{M}(\vec{x},y,f_{M}(\vec{x}))\mathrm{d}P^{[M]}(\vec {x},y)\] \[\qquad+\left|\int\ell(\hat{\mu}[\vec{x}],y,f(\hat{\mu}[\vec{x}]) \mathrm{d}P^{[M]}(\vec{x},y))-\int\ell(\mu,y,f(\mu))\mathrm{d}P(\mu,y)\right|\] \[\qquad+\left|\int\ell_{M}(\vec{x},y,f_{M}(\vec{x}))-\ell(\hat{ \mu}[\vec{x}],y,f(\hat{\mu}[\vec{x}]))\mathrm{d}P^{[M]}(\vec{x},y)\right|+ \lambda\|f\|_{k}^{2}\] \[\leq\int\ell_{M}(\vec{x},y,f_{M}(\vec{x}))\mathrm{d}P^{[M]}(\vec {x},y)+\lambda\liminf_{M}\|f_{M}\|_{M}^{2}+\epsilon,\]

where we used Lemma 2.4 in the last inequality.

For the \(\Gamma\)-\(\limsup\)-inequality, let \(f\in H_{k}\) be arbitrary and let \((f_{M})_{M}\) be the recovery sequence from Lemma 2.5. The desired inequality then follows by repeating the arguments from above.

Finally, using exactly the same argument as in the proof of Proposition 4.3 shows that \(\|f_{M,\lambda}^{*}\|_{M}\leq\sqrt{\frac{NC_{\ell}}{\lambda}}\), so we can apply Proposition B.3 and the result follows. 

Proof.: _of Proposition 4.7_ Let \((\epsilon_{n})_{n}\subseteq\mathbb{R}_{>0}\) with \(\epsilon_{m}\searrow 0\). We construct a strictly increasing sequence \((M_{n})_{n}\) such that

\[\left|\mathcal{R}_{\ell_{M_{n}},P^{[M_{n}]}}^{H_{M_{n}}*}-\mathcal{R}_{\ell,P }^{H_{n}*}\right|\leq\epsilon_{n}\]

for all \(n\in\mathbb{N}_{+}\).

We start with \(n=1\): Since \(A_{2}(0)=0\) and \(A_{2}\) is continuous in 0, cf. [20, Lemma 5.15], there exists \(\lambda_{1}^{\prime}\in\mathbb{R}_{>0}\) such that \(A_{2}(\lambda)\leq\frac{\epsilon_{1}}{3}\) for all \(0<\lambda\leq\lambda_{1}^{\prime}\). From Assumption 4.6 we get \(\lambda_{1}^{\prime\prime}\in\mathbb{R}_{>0}\) such that for all \(M\in\mathbb{N}_{+}\) we have \(A_{2}^{[M]}(\lambda)\leq\frac{\epsilon_{1}}{3}\) for all \(0<\lambda\leq\lambda_{1}^{\prime\prime}\). Define now \(\lambda_{1}=\min\{\lambda_{1}^{\prime},\lambda_{1}^{\prime\prime}\}\), and observe that \(\lambda_{1}>0\). Proposition 4.5 ensures the existence of a strictly increasing sequence \((M_{m}^{(1)})_{m}\subseteq\mathbb{N}_{+}\) with

\[\mathcal{R}_{\ell_{M_{n}^{(1)}},P^{[M_{m}^{(1)}]},\lambda_{1}}^{H_{M_{n}}*} \rightarrow\mathcal{R}_{\ell,P,\lambda_{1}}^{H_{k}*}\]

for \(m\rightarrow\infty\). Choose \(m_{1}\in\mathbb{N}_{+}\) such that for all \(m\geq m_{1}\) we have

\[\left|\mathcal{R}_{\ell_{M_{n}^{(1)}},P^{[M_{m}^{(1)}]},\lambda_{1}}^{H_{M_{n} ^{(1)}},*}-\mathcal{R}_{\ell,P,\lambda_{1}}^{H_{k}*}\right|\leq\frac{\epsilon _{1}}{3}.\]

We now set \(M_{1}=M_{m_{1}}^{(1)}\) and get that

\[\left|\mathcal{R}_{\ell_{M_{1}},P^{[M_{1}]}}^{H_{M_{1}}*}-\mathcal{R }_{\ell,P}^{H_{k}*}\right| \leq\left|\mathcal{R}_{\ell_{M_{1}^{(1)}},P^{[M_{m_{1}}^{(1)}]}}^{H _{M_{1}^{(1)}}*}-\mathcal{R}_{\ell_{M_{n}^{(1)}},P^{[M_{m_{1}}^{(1)}]}, \lambda_{1}}^{H_{M_{n_{1}}^{(1)}}*}\right|+\left|\mathcal{R}_{\ell_{M_{n_{1}}^{ (1)}},P^{[M_{m_{1}}^{(1)}]},\lambda_{1}}^{H_{M_{n_{1}}^{(1)}}*}-\mathcal{R}_{ \ell,P,\lambda_{1}}^{H_{k}*}\right|\] \[\qquad+\left|\mathcal{R}_{\ell,P,\lambda_{1}}^{H_{k}*}-\mathcal{R }_{\ell,P}^{H_{k}*}\right|\] \[\leq A_{2}^{[M_{m}^{(1)}]}(\lambda_{1})+\frac{\epsilon_{1}}{3}+A_ {2}(\lambda_{1})\] \[\leq\epsilon_{1}.\]

We can now repeat the argument from above inductively: Suppose we have constructed our subsequence up to \(n\in\mathbb{N}_{+}\), i.e., \(M_{1},\ldots,M_{n}\). Choose \(\lambda^{\prime}\in\mathbb{R}_{>0}\) such that \(A_{2}(\lambda)\leq\frac{\epsilon_{n+1}}{3}\) for 

[MISSING_PAGE_EMPTY:24]

\[\mathcal{N}(f,k)=\sup_{(\vec{x},\vec{\alpha})\in\mathcal{X}^{N}\times\mathbb{R}^{N}} \mathcal{D}(\vec{x},\vec{\alpha},f,k).\]

We collect now some simple facts that will be used repeatedly.

Let \(\vec{x}\in\mathcal{X}^{N}\), \(\vec{\alpha}\in\mathbb{R}^{N}\), \(N\in\mathbb{N}_{+}\), be arbitrary, and define

\[f=\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n})\in H_{k}^{\text{pre}}.\]

1. By construction, \(\mathcal{W}(\vec{x},\vec{\alpha},k)\in\mathbb{R}_{\geq 0}\) (recall that \(k\) is positive semidefinite).
2. Since \(f\in H_{k}^{\text{pre}}\), its RKHS norm has an explicit form and we find \[\|f\|_{k}=\sqrt{\sum_{i,j=1}^{N}\alpha_{i}\alpha_{j}k(x_{j},x_{i})}=\mathcal{ W}(\vec{x},\vec{\alpha},k).\] This also implies that \(f\equiv 0\) if and only if \(\mathcal{W}(\vec{x},\vec{\alpha},k)=0\).
3. If \(\mathcal{W}(\vec{x},\vec{\alpha},k)>0\), then \[\mathcal{D}(\vec{x},\vec{\alpha},f,k) =\frac{\mathcal{E}(\vec{x},\vec{\alpha},f)}{\mathcal{W}(\vec{x}, \vec{\alpha},k)}\] \[=\frac{\sum_{i=1}^{N}\alpha_{i}f(x_{i})}{\sqrt{\sum_{i,j=1}^{N} \alpha_{i}\alpha_{j}k(x_{j},x_{i})}}\] \[=\frac{\sum_{i,j=1}^{N}\alpha_{i}\alpha_{j}k(x_{j},x_{i})}{\sqrt{ \sum_{i,j=1}^{N}\alpha_{i}\alpha_{j}k(x_{j},x_{i})}}\] \[=\frac{\mathcal{W}(\vec{x},\vec{\alpha},k)^{2}}{\mathcal{W}(\vec{ x},\vec{\alpha},k)}=\mathcal{W}(\vec{x},\vec{\alpha},k).\]

We can now state the characterization result.

**Theorem B.1**.: Let \(k:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) be a kernel and \(f\in\mathbb{R}^{\mathcal{X}}\). Then \(f\in H_{k}\) if and only if \(\mathcal{N}(f,k)<\infty\). If \(f\in H_{k}\), then \(\|f\|_{k}=\mathcal{N}(f,k)\).

For convenience, we provide a full self-contained proof of this result.

Proof.: **Step 1** First, we show that for \(f\in H_{k}\), we have \(\|f\|_{k}=\mathcal{N}(f,k)\).

\(\mathcal{N}(f,k)\leq\|f\|_{k}\)_:_ Let \(N\in\mathbb{N}_{+}\) and \((\vec{x},\vec{\alpha})\in\mathcal{X}^{N}\times\mathbb{R}^{N}\) be arbitrary. Observe that

\[\mathcal{E}(\vec{x},\vec{\alpha},f) =\sum_{n=1}^{N}\alpha_{n}f(x_{n})\] \[=\sum_{n=1}^{N}\alpha_{n}\langle f,k(\cdot,x_{n})\rangle_{k}\] \[=\langle f,\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n})\rangle_{k}\] \[\leq\|f\|_{k}\left\|\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n})\right\| _{k}\] \[=\|f\|_{k}\mathcal{W}(\vec{x},\vec{\alpha},k).\]

If \(\mathcal{W}(\vec{x},\vec{\alpha},k)=\|\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n}) \|_{k}=0\), then \(\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n})=0_{H_{k}}\), hence \(\mathcal{E}(\vec{x},\vec{\alpha},f)=\langle f,0_{H_{k}}\rangle_{k}=0\) and by definition \(\mathcal{D}(\vec{x},\vec{\alpha},f,k)=0\leq\|f\|_{k}\).

If \(\mathcal{W}(\vec{x},\vec{\alpha},k)>0\), we can rearrange to get

\[\frac{\mathcal{E}(\vec{x},\vec{\alpha},f)}{\mathcal{W}(\vec{x},\vec{\alpha},k)}= \mathcal{D}(\vec{x},\vec{\alpha},f,k)\leq\|f\|_{k}.\]

Since \((\vec{x},\vec{\alpha})\) was arbitrary, we find that \(\mathcal{N}(\vec{x},\vec{\alpha},f,k)\leq\|f\|_{k}\).

\(\mathcal{N}(f,k)\geq\|f\|_{k}\): Let \(\epsilon>0\) and choose \(f_{\epsilon}=\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n})\in H_{k}^{\text{pre}}\) such that \(\|f-f_{\epsilon}\|_{k}<\epsilon\). If \(\mathcal{W}(\vec{x},\vec{\alpha},k)=\|f_{\epsilon}\|_{k}=0\), then \(f_{\epsilon}=0_{H_{k}}\) and hence \(\mathcal{E}(\vec{x},\vec{\alpha},f)=\langle f,f_{\epsilon}\rangle_{k}= \langle f,0_{H_{k}}\rangle_{k}=0\). By definition, this then shows

\[\mathcal{D}(\vec{x},\vec{\alpha},f)=0=\|f_{\epsilon}\|_{k}\geq\|f\|_{k}-\epsilon.\]

Before we continue, note that for all \(f_{1},f_{2}\in H_{k}\) we have

\[|\mathcal{E}(\vec{x},\vec{\alpha},f_{1})-\mathcal{E}(\vec{x}, \vec{\alpha},f_{2})| =\left|\sum_{n=1}^{N}\alpha_{n}(f_{1}(x_{n})-f_{2}(x_{n}))\right|\] \[=\left|\sum_{n=1}^{N}\alpha_{n}\langle f_{1}-f_{2},k(\cdot,x_{n}) \rangle_{k}\right|\] \[=\left|\langle f_{1}-f_{2},\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n} )\rangle_{k}\right|\] \[\leq\|f_{1}-f_{2}\|_{k}\|f_{\epsilon}\|_{k}.\]

Assume now that \(\mathcal{W}(\vec{x},\vec{\alpha},k)>0\), then we get

\[\mathcal{D}(\vec{x},\vec{\alpha},f,k) =\frac{\mathcal{E}(\vec{x},\vec{\alpha},f)}{\mathcal{W}(\vec{x}, \vec{\alpha},k)}\] \[\geq\frac{\mathcal{E}(\vec{x},\vec{\alpha},f_{\epsilon})}{ \mathcal{W}(\vec{x},\vec{\alpha},k)}-\frac{\|f-f_{\epsilon}\|_{k}\|f_{ \epsilon}\|_{k}}{\mathcal{W}(\vec{x},\vec{\alpha},k)}\] \[\geq\frac{\mathcal{E}(\vec{x},\vec{\alpha},f_{\epsilon})}{ \mathcal{W}(\vec{x},\vec{\alpha},k)}-\frac{\epsilon\|f_{\epsilon}\|_{k}}{ \mathcal{W}(\vec{x},\vec{\alpha},k)}\] \[=\mathcal{W}(\vec{x},\vec{\alpha},k)-\epsilon\] \[=\|f_{\epsilon}\|_{k}-\epsilon\] \[\geq\|f\|_{k}-2\epsilon\]

Altogether, by definition of \(\mathcal{N}(f,k)\), we get that

\[\mathcal{N}(f,k)\geq\mathcal{D}(\vec{x},\vec{\alpha},f,k)\geq\|f\|_{k}-2\epsilon.\]

Since \(\epsilon>0\) was arbitrary, we find that \(\mathcal{N}(f,k)\geq\|f\|_{k}\).

**Step 2** Let \(f\in\mathbb{R}^{\mathcal{X}}\) be arbitrary. We show that if \(\mathcal{N}(f,k)<\infty\), then

\[\ell_{f}:H_{k}^{\text{pre}}\rightarrow\mathbb{R}\] \[\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n})\mapsto\sum_{n=1}^{N} \alpha_{n}f(x_{n})\]

is a well-defined, linear and continuous (w.r.t. \(\|\cdot\|_{k}\)) map.

To establish the _well-posedness_, let \((\vec{x},\vec{\alpha})\in\mathcal{X}^{N}\times\mathbb{R}^{N}\) and \((\vec{y},\vec{\beta})\in\mathcal{X}^{M}\times\mathbb{R}^{M}\) such that

\[\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n})=\sum_{m=1}^{M}\beta_{m}k(\cdot,y_{m}) \in H_{k}^{\text{pre}}.\]

This implies that

\[\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n})+\sum_{m=1}^{M}(-\beta_{m})k(\cdot,y_{m} )=0_{H_{k}}\]and hence \(\mathcal{W}((\vec{x},\vec{y}),(\vec{\alpha},-\vec{\beta}),k)=\|\sum_{n=1}^{N} \alpha_{n}k(\cdot,x_{n})+\sum_{m=1}^{M}(-\beta_{m})k(\cdot,y_{m})\|_{k}=0\). Assume now that

\[\sum_{n=1}^{N}\alpha_{n}f(x_{n})\neq\sum_{m=1}^{m}\beta_{m}f(x_{m}),\]

then we get that

\[\sum_{n=1}^{N}\alpha_{n}f(x_{n})+\sum_{m=1}^{m}(-\beta_{m})f(x_{m})=\mathcal{E }((\vec{x},\vec{y}),(\vec{\alpha},-\vec{\beta}),f)\neq 0\]

which by definition implies that \(\mathcal{D}((\vec{x},\vec{y}),(\vec{\alpha},-\vec{\beta}),f,k)=\infty\) and therefore \(\mathcal{N}(f,k)=\infty\), a contradiction.

The _linearity_ is then clear. Finally, to show the _continuity_, let \(H_{k}^{\text{pre}}\ni f_{0}=\sum_{n=1}^{N}\alpha_{n}k(\cdot,x_{n})\) be arbitrary and set \(\vec{x}=(x_{1}\quad\cdots\quad x_{N})\), \(\vec{\alpha}=(\alpha_{1}\quad\cdots\quad\alpha_{N})\), then

\[|\ell_{f}(f_{0})| =\left|\sum_{n=1}^{N}\alpha_{n}f(x_{n})\right|\] \[=|\mathcal{E}(\vec{x},\vec{\alpha},f)|\] \[\leq\mathcal{N}(f,k)\mathcal{W}(\vec{x},\vec{\alpha},k)\] \[=\mathcal{N}(f,k)\|f_{0}\|_{k}.\]

Since \(\mathcal{N}(f,k)\) is finite and independent of \(f_{0}\), and \(\ell_{f}\) is a linear map, this shows the continuity of \(\ell_{f}\).

**Step 3** Let \(f\in\mathbb{R}^{\mathcal{X}}\) such that \(\mathcal{N}(f,k)<\infty\). Since according to Step 2\(\ell_{f}\) is a linear and continuous map on \(H_{k}^{\text{pre}}\) and the latter is dense in \(H_{k}\), there exists a unique linear and continuous extension \(\bar{\ell}_{f}:H_{k}\to\mathbb{R}\) of \(\ell_{f}\). Furthermore, from the Riesz Representation Theorem there exists a unique \(\hat{f}\in H_{k}\) with \(\bar{\ell}_{f}=\langle\cdot,\hat{f}\rangle_{k}\). For all \(x\in\mathcal{X}\) we then get

\[\hat{f}(x) =\langle\hat{f},k(\cdot,x)\rangle_{k}\] \[=\langle k(\cdot,x),\hat{f}\rangle_{k}\] \[=\bar{\ell}_{f}(k(\cdot,x))\] \[=\ell_{f}(k(\cdot,x))\] \[=f(x),\]

hence \(f=\hat{f}\in H_{k}\). 

### A \(\Gamma\)-convergence argument

We use repeatedly the concept of \(\Gamma\)-convergence, see for example [32]. For convenience, in this section we summarize the well-known and standard main argument, roughly following [33, Chapter 5].

**Definition B.2**.: Let \(F_{M}:H_{M}\to\mathbb{R}\cup\{\infty\}\) and \(F:H_{k}\to\mathbb{R}\cup\{\infty\}\). We say that \(F_{M}\)\(\Gamma\)-converges to \(F\) and write \(F_{M}\xrightarrow{\Gamma}F\), if

1. For all sequences \((f_{M})_{M}\), \(f_{M}\in H_{M}\), with \(f_{M}\xrightarrow{\mathcal{P}_{1}}f\) for some \(f\in H_{k}\), we have \[F(f)\leq\liminf_{M}F_{M}(f_{M}).\]
2. For all \(f\in H_{k}\) there exists a sequence \((f_{M})_{M}\) with \(f_{M}\in H_{M}\) such that \(f_{M}\xrightarrow{\mathcal{P}_{1}}f\) and \[F(f)\geq\limsup_{M}F_{M}(f_{M}).\]

The sequence in the second item is commonly called a _recovery sequence_ (for \(f\)).

**Proposition B.3**.: Let \(F_{M}\xrightarrow{\Gamma}F\) and \(f_{M}^{*}\in\operatorname*{argmin}_{f\in H_{M}}F_{M}(f)\) for all \(M\in\mathbb{N}\) (in particular, all the minima are attained). If there exists \(B\in\mathbb{R}_{\geq 0}\) such that \(\|f_{M}^{*}\|_{M}\leq B\) for all \(M\in\mathbb{N}\), then there exists a subsequence \((f_{M_{\ell}}^{*})_{\ell}\) and \(f^{*}\in H_{k}\) such that \(f_{M_{\ell}}^{*}\xrightarrow{\mathcal{P}_{1}}f^{*}\). Furthermore, \(F_{M_{\ell}}(f_{M_{\ell}}^{*})\to F(f^{*})\).

[MISSING_PAGE_EMPTY:28]