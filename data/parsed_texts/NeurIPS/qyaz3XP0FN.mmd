# Parametric model reduction of mean-field and stochastic systems via higher-order action matching

Jules Berman

Courant Institute of

Mathematical Sciences

New York University

New York, NY 10012

jmb1174@nyu.edu

&Tobias Blickhan

Courant Institute of

Mathematical Sciences

New York University

New York, NY 10012

tobias.blickhan@nyu.edu

Equal contribution

Benjamin Peherstorfer

Courant Institute of Mathematical Sciences

New York University

New York, NY 10012

pehersto@cims.nyu.edu

###### Abstract

The aim of this work is to learn models of population dynamics of physical systems that feature stochastic and mean-field effects and that depend on physics parameters. The learned models can act as surrogates of classical numerical models to efficiently predict the system behavior over the physics parameters. Building on the Benamou-Brenier formula from optimal transport and action matching, we use a variational problem to infer parameter- and time-dependent gradient fields that represent approximations of the population dynamics. The inferred gradient fields can then be used to rapidly generate sample trajectories that mimic the dynamics of the physical system on a population level over varying physics parameters. We show that combining Monte Carlo sampling with higher-order quadrature rules is critical for accurately estimating the training objective from sample data and for stabilizing the training process. We demonstrate on Vlasov-Poisson instabilities as well as on high-dimensional particle and chaotic systems that our approach accurately predicts population dynamics over a wide range of parameters and outperforms state-of-the-art diffusion-based and flow-based modeling that simply condition on time and physics parameters.

## 1 Introduction

Predicting the behavior of time-dependent processes \(X_{t,\mu}\) over time \(t\) and across varying physics parameters \(\mu\) is a key challenge in computational science and engineering [46, 65]. The dynamics of \(X_{t,\mu}\) typically are described by systems of (stochastic) differential equations, which are derived from physics models and can be computationally expensive to simulate [40, 32]. Thus, it is desirable to learn reduced or surrogate models that can be rapidly evaluated to predict the system behavior across varying physics parameters [72, 10, 11, 45].

Reduced modeling via learning population dynamicsGiven a data set of samples, i.e., realizations of the random variable \(X_{t,\mu}\) on a suitable domain \(\mathcal{X}\subseteq\mathbb{R}^{d}\),

\[\{X_{t_{j},\mu_{k}}^{i}\,|i=1,\ldots,N_{x},\quad j=1,\ldots,N_{t},\quad k=1, \ldots,N_{\mu}\}\subset\mathcal{X},\] (1)

we aim to learn a dynamical-system reduced model to rapidly predict samples that approximately follow the same law \(\rho_{t,\mu}\) as \(X_{t,\mu}\) over time \(t\) and varying physics parameter \(\mu\). We refer to the evolution of \(\rho_{t,\mu}\) in time as population dynamics. Learning the population dynamics instead of learning the dynamics of the individual trajectories \(t\mapsto X_{t,\mu}^{i}\) for all \(i=1,\ldots,N_{x}\) and \(\mu\) can be beneficial: There are cases where \(\rho_{t,\mu}\) does not change in time, yet every sample trajectory \(t\mapsto X_{t,\mu}^{i}\) follows complicated dynamics. For example, consider incompressible fluid dynamics with constant density. Samples corresponding to particles that comprise the fluid can have complicated trajectories, whereas on a distribution level, the density of the fluid is constant and so are the population dynamics. Furthermore, learning population dynamics seamlessly treats deterministic and stochastic systems because the stochastic models that we consider can be expressed as deterministic Fokker-Planck equations on the population level.

Our approach: Learning parametric minimal energy vector fields that represent population dynamicsBuilding on standard literature on optimal transport theory [8] as well as the so-called action-matching loss introduced in [61], we pose a variational problem to learn gradient fields \(\nabla s_{t,\mu}\) so that the continuity equation corresponding to the vector field given by \(\nabla s_{t,\mu}\) approximates the population dynamics \(\rho_{t,\mu}\) of the samples (1). In the spirit of reduced modeling [72, 10, 11, 45], we seek a vector field \(s_{t,\mu}\) that generalizes to different values of the physics parameters \(\mu\). We therefore optimize for \(s_{t,\mu}\) that minimizes the average objective of a variational problem over all parameters \(\mu\sim\nu\), where \(\nu\) describes the distribution of parameters on the domain \(\mathcal{D}\subset\mathbb{R}^{p}\). We parametrize \(s_{t,\mu}\) with a neural network with weight modulation [39, 13] so that it can be evaluated quickly over \(t\) and \(\mu\).

_Rapid sample generation in inference phase_ Predictions at inference time at new physics parameters \(\mu\) are made by sampling based on the vector field \(\nabla s_{t,\mu}\), which means that our approach represents \(\rho_{t,\mu}\) through the application of \(\nabla s_{t,\mu}\) on an initial condition. Importantly, time \(t\) in the inference step corresponds to the time of the physics problem so that in one inference step a whole sample trajectory is obtained, rather than a sample at one specific time point as in regular conditioning-based methods (see literature review). Thus, we can rapidly generate samples that follow the law \(\rho_{t,\mu}\) in the inference phase.

_Stabilizing training with higher-order quadrature_ An important part of our contribution is stabilizing the training procedure by accurately estimating the objective of the variational problems from few data samples. In particular, instead of uniformly sampling over the data (1), we introduce an empirical loss (8) that utilizes higher-order quadrature [27] in the time direction so that the learned \(\nabla s_{t,\mu}\) accurately captures the dynamics over time \(t\). Consequently, we refer to our approach as higher-order action matching (HOAM). Our numerical experiments show that the higher-order quadrature in the empirical loss is key for learning gradient fields \(\nabla s_{t,\mu}\) that accurately capture the evolution in time \(t\) and that generalize across physics parameters \(\mu\).

Literature reviewWe review relevant literature; see Figure 1 for an overview.

_Non-intrusive and data-driven surrogate modeling_ There is a range of surrogate and latent modeling methods that aim to learn or reduce the sample dynamics of the realizations rather than the population dynamics, such as dynamic mode decomposition, Koopman-based methods, and others [71, 76, 86, 12, 46, 58, 92] as well as neural network-based methods such as neural ordinary differential equations [19, 28, 48]. There also are methods for stochastic systems [51, 42, 88, 19, 28, 73, 21]. However, all of these methods ignore physics parameter dependencies and/or aim to learn the sample dynamics, whereas we focus on parametric population dynamics.

_Population dynamics and trajectory inference_ Learning population dynamics has been considered extensively in computational biology in the context of gene expression, where the focus is on learning from independent samples at selected time points rather than from sample trajectories [34, 30, 93, 75, 85, 47]; however, many of these approaches [17, 84] are simulation-based and thus require integrating dynamics during the training or parameterizing the density additionally to the vector field. These works also are not concerned with generalizing over a range of physics parameters in many cases.

_Diffusion- and flow-based modeling_ There is a large body of work on diffusion-based [91, 79, 36, 41, 81, 82] and flow-based modeling [2, 54]; see [1] for a detailed review. These approaches are not taking into account time \(t\) because they learn paths between a reference and a target distribution only. There are works that condition on time \(t\) and a parameter \(\mu\) such as [68, 14, 26, 37, 33, 38, 52], but this requires then generating a path for each time step at inference time, which is computationally expensive. Furthermore, the conditioning on time \(t\) means that the target distribution \(\rho_{t,\mu}\) at each time \(t\) and \(\mu\) is different, and thus a separate hyper-parameter tuning can be required, which is impractical over many time steps and physics parameters as in our physics problems; see our numerical experiments. The works [15, 78, 50] compute transport-based solutions but parametrize different quantities than our approach, require actively sampling data, and ignore physics parameters \(\mu\). We note that there also is work on forecasting with diffusion- and flow-based modeling [68, 62, 18, 20], which is a different task than our task of predicting across varying physics parameters.

_Optimal transport_ Besides the machine learning literature, variational approaches for inferring vector fields are extensively used in optimal transport theory [5, 4]. Of particular importance to us is the formulation by Benamou and Brenier [8]. The Bennamou-Brenier formula describes a joint optimization problem over vector fields and paths in probability space and the action matching loss [61] is the restriction of this optimization problem to the case of a fixed path and the vector field parametrized by a neural network, which are core building blocks for us that we show can be used together with a parameter dependency.

ContributionsWe summarize our contributions:

**(a)** Developing a loss to learn population dynamics that remain valid across varying physics parameters by building on optimal transport literature [8] and action matching [61].

**(b)** Introducing higher-order quadrature schemes for the loss to efficiently couple the gradient fields over time. This leads to lower variance estimators of the loss that critically stabilize training.

**(c)** Demonstrating on a range of physics problems from Vlasov-Poisson instabilities to high-dimensional chaotic systems that our approach leads to (i) accurate predictions of population dynamics and (ii) orders of magnitude speedups in inference/prediction over

Figure 1: Parametric model reduction with our HOAM seeks to learn vector fields that represent population dynamics \(\rho_{t}\) over time \(t\). In contrast, parametric model reduction with score-based diffusion denoising and flow-based modeling requires conditioning on time \(t\), which leads to separate, costly inference steps for each time step of a sample trajectory.

classical methods that numerically solve the underlying partial differential equations as well as standard diffusion- and flow-based models that condition on physical time.

We provide an implementation of our method at https://github.com/julesberman/HOAM.

## 2 Method

### Parameter-dependent population dynamics

Continuity equationLet us consider data (1) corresponding to the probability measure \(\rho_{t,\mu}\), which is absolutely continuous for \(t\in[0,1]\) and \(\mu\in\mathcal{D}\). We use the same notation for the measure and its density. The density \(\nu\) of \(\mu\) is also assumed to be absolutely continuous on \(\mathcal{D}\). We consider population dynamics of \(X_{t,\mu}\sim\rho_{t,\mu}\) that can be described by the continuity equation

\[\partial_{t}\rho_{t,\mu}=-\nabla\cdot\left(\rho_{t,\mu}v_{t,\mu}\right),\qquad \text{for all }t\in[0,1]\,,\mu\in\mathcal{D}\,,\] (2)

with the initial condition \(\rho_{t=0,\mu}=:\rho_{0,\mu}\) and vector field \(v_{t,\mu}\). Notice that in our case the continuity equation (2) depends on the physics parameter \(\mu\sim\nu\). There can be many vector fields \(v_{t,\mu}\) that lead to the same population dynamics (2). For example, if \(v_{t,\mu}\) is a vector field that describes the dynamics of \(\rho_{t,\mu}\) via (2), then another vector field is given by \(v^{\prime}_{t,\mu}=v_{t,\mu}+w/\rho_{t,\mu}\) with any other \(w\) that satisfies \(\nabla\cdot w=0\) as long as \(\rho_{t,\mu}\) is positive.

Uniqueness via gradient fields and the corresponding elliptic problemsBecause we aim to learn a vector field from sample data (1) that describes the population dynamics (2) of the corresponding law \(\rho_{t,\mu}\), it is helpful to remove this non-uniqueness. One way to do so is to restrict the vector field to \(v_{t,\mu}=\nabla s_{t,\mu}\) so that it is a gradient field [4, p. 45]. Plugging \(v_{t,\mu}=\nabla s_{t,\mu}\) into (2), together with the assumptions \(\rho_{t,\mu}>0\) and \(\int_{\mathcal{X}}\partial_{t}\rho_{t,\mu}\mathrm{d}x=0\), leads to parametric elliptic problems in \(s_{t,\mu}\)

\[-\nabla\cdot\left(\rho_{t,\mu}\nabla s_{t,\mu}\right)=\partial_{t}\rho_{t,\mu }\,,\] (3)

with coefficient function \(\rho_{t,\mu}\), right-hand side (source term) \(\partial_{t}\rho_{t,\mu}\), and homogeneous Neumann boundary conditions \(\rho_{t,\mu}\nabla s_{t,\mu}\cdot\hat{n}=0\) on \(\partial\mathcal{X}\) with normal vector \(\hat{n}\) for all \(t\in[0,1]\) and \(\mu\in\mathcal{D}\). The weak forms of the elliptic problems (3) lead to energy minimization problems that can be used to learn the gradient field \(s_{t,\mu}\) via optimization:

\[\min_{s\in H^{1}(\rho_{t,\mu},\mathcal{X})}E_{t,\mu}(s):=\min_{s\in H^{1}( \rho_{t,\mu},\mathcal{X})}\frac{1}{2}\int_{\mathcal{X}}|\nabla s|^{2}\rho_{t, \mu}\mathrm{d}x-\int_{\mathcal{X}}\partial_{t}\rho_{t,\mu}s\mathrm{d}x\] (4)

for each \(t\in[0,1]\) and \(\mu\in\mathcal{D}\). The space \(H^{1}(\rho_{t,\mu},\mathcal{X})\) contains functions \(s\) with \(\int_{\mathcal{X}}|\nabla s|^{2}\rho_{t,\mu}\mathrm{d}x<\infty\), which is the energy (semi-)norm corresponding to the \(\rho_{t,\mu}\)-weighted inner product [29, Sec. 2.3.2].

Optimal transportStandard elliptic theory guarantees unique solutions up to constants of (4) in the Sobolev space \(H^{1}(\mathcal{X})\) under strong assumptions on \(\rho_{t,\mu}\) such as uniform boundedness by a positive constant for all \(t\) and \(\mu\); see [29, Proposition 2.2] and [11, Section 3.2]. The theory of optimal transport allows treating the much more general case when \(\rho_{t,\mu}\) is not uniformly bounded away from zero and possibly atomic; we refer to [8] and [74, Section 5.3.1] for details. Among all vector fields \(v_{t,\mu}\) that are compatible to \(\rho_{t,\mu}\) in the sense of (2), gradient fields \(\nabla s_{t,\mu}\) have the smallest associated kinetic energy \(\frac{1}{2}\int_{\mathcal{X}}|v|^{2}\rho_{t,\mu}\mathrm{d}x\), which is the objective considered in [8]. In the language of optimal transport and in particular the formalism of [63], vector fields with minimal kinetic energy describe tangent vectors to the curve \(t\mapsto\rho_{t,\mu}\). The metric is the inner product of \(L^{2}(\rho_{t,\mu},\mathcal{X},\mathbb{R}^{d})\). This is the weak Riemannian structure of \(\mathcal{P}(\mathcal{X})\) equipped with the Kantorovich-Rubinstein metric and described in detail in [5, Chapter 8]. We give a short description in Appendix E.

Energy functional with entropy termInstead of the energy (4), we can also use other choices of the energy to select gradient fields, as long as energy functions are convex to maintain uniqueness. We consider an energy that is based on a different notion of discrepancy on \(\mathcal{P}(\mathcal{X})\), the entropic optimal transport or Schrodinger bridge problem [77, 56],

\[E^{\epsilon}_{t,\mu}(s)=\frac{1}{2}\int_{\mathcal{X}}|\nabla(s-\frac{\epsilon^ {2}}{2}\log\rho_{t,\mu})|^{2}\rho_{t,\mu}\mathrm{d}x-\int_{\mathcal{X}}\partial _{t}\rho_{t,\mu}s\mathrm{d}x\,,\] (5)which depends on \(\epsilon\geq 0\). The energy \(E^{\epsilon}_{t,\mu}\) is of particular interest for two reasons: One, the Euler-Lagrange equation of (5) in strong form is the Fokker-Planck equation for \(s^{\epsilon}_{t,\mu}\): \(\partial_{t}\rho_{t,\mu}=-\nabla\cdot(\rho_{t,\mu}\nabla s^{\epsilon}_{t,\mu}) +\frac{\epsilon^{2}}{2}\Delta\rho_{t,\mu}\), again with homogeneous Neumann boundary conditions for all \(t\in[0,1]\) and \(\mu\in\mathcal{D}\); see Appendix C. This means we can efficiently generate samples after learning \(s^{\epsilon}_{t,\mu}\) via corresponding stochastic differential equations (SDEs). Two, it can be interpreted as regularizing the field \(s_{t,\mu}\), which we discuss in Appendix C.

### Loss for learning vector fields over time \(t\) and physics parameter \(\mu\)

Variational formulation over \(t\) and \(\mu\)So far we just carried along time \(t\) and physics parameter \(\mu\) but did not address them in the variational problems, i.e., we had separate variational problems (4) for all \(t\in[0,1]\) and \(\mu\sim\nu\). We now propose to consider the average energy over \(t\) and \(\mu\) to infer a map \(s:[0,1]\times\mathcal{D}\to H^{1}(\rho_{t,\mu},\mathcal{X}),(t,\mu)\mapsto s_ {t,\mu}\), which is called a solution map in reduced modeling [72, 10, 11, 45],

\[\min_{s:[0,1]\times\mathcal{D}\to H^{1}(\rho_{t,\mu},\mathcal{X})}E^{ \epsilon}(s):=\min_{s}\int_{\mathcal{D}}\int_{0}^{1}E^{\epsilon}_{t,\mu}(s_{t,\mu})\,\mathrm{d}t\,\mathrm{d}\nu(\mu).\] (6)

Notice that time \(t\) and physics parameter \(\mu\) have two different effects on the gradient field \(\nabla s_{t,\mu}\): Time \(t\) couples the elliptic problems (i.e., (3) for \(\epsilon=0\)) via the time derivative \(\partial_{t}\rho_{t,\mu}\); see Appendix D. In contrast, the elliptic problems are uncoupled over \(\mu\) and can be considered separately. This means that to compute the solution to an elliptic problem for one value of \(\mu\in\mathcal{D}\), one does not need to consider any other \(\mu^{\prime}\in\mathcal{D}\). This will allow us to sample the physics parameters over \(\mathcal{D}\) independently from each other when estimating the corresponding loss, whereas we will use higher-order quadrature to obtain an accurate approximation of the time integral to ensure the coupling between the time points is reflected in \(s_{t,\mu}\); see Section 2.3.

Loss for learning gradient fields from samples over \(t\) and \(\mu\)The energy \(E_{t,\mu}\) defined in (4) as well as the energy \(E^{\epsilon}_{t,\mu}\) defined in (5) leads to a loss that can be estimated from samples (1). The quantity \(\partial_{t}\rho_{t,\mu}\) appears in (4) and (5), which is typically unavailable when we have access to data samples (1) only. Integration by parts of the term involving \(\partial_{t}\rho_{t,\mu}\) eliminates it, see also Appendix D. We arrive at

\[E^{\epsilon}(s)\!=\!\int_{\mathcal{D}}\left[\int_{0}^{1}\int_{ \mathcal{X}}\left(\frac{1}{2}|\nabla s_{t,\mu}|^{2}\!+\!\partial_{t}s_{t,\mu} \!+\!\frac{\epsilon^{2}}{2}\Delta s_{t,\mu}\right)\rho_{t,\mu}\mathrm{d}x \mathrm{d}t\!-\!\int_{\mathcal{X}}s_{t,\mu}\rho_{t,\mu}\mathrm{d}x\bigg{|}_{t =0}^{t=1}\right]\mathrm{d}\nu(\mu)\,.\] (7)

Note that this loss is comprised only of expectation values with respect to \(\rho_{t,\mu}\) and is therefore well-defined also for empirical distributions. The choice \(\varepsilon>0\) assumes that the Fisher information of \(\rho_{t,\mu}\) is finite.

**Remark 1**.: _Loss functions of the form as (7) but without the parameter dependence have been used in [61] and [47, Theorem 2.1]. In fact, the case with \(\epsilon=0\) appears already in [8, Equation 35] and [64, Section 3]. We build on these results but work with population dynamics that depend on physics parameters, which leads to the loss shown in (7)._

### Parameterizing the vector field, estimating the loss from data, sampling

Parametrizing \(s_{t,\mu}\) with weight modulationsWe parametrize the vector field \(s_{t,\mu}\) via a neural network with continuous versions of low-rank adaptation (CoLoRA) layers, which have been successfully used for parametric model reduction of deterministic time-dependent dynamical systems [13]; see also [39]. The layers have the form \(\mathcal{C}(x)=Wx+\phi(t,\mu)ABx+b\), where \(W\) is a weight matrix, \(A,B\) are low-rank matrices, \(b\) is a bias vector, and \(\phi(t,\mu)\in\mathbb{R}\) is a scalar weight modulation; see Appendix B. Only the weight modulations \(\phi(t,\mu)\) depend on time \(t\) and physics parameter \(\mu\). We use a hyper-network \(h:[0,1]\times\mathcal{D}\times\Psi\to\mathbb{R}\) that depends on the weight vector \(\psi\in\Psi\subseteq\mathbb{R}^{q}\) to map \(t\) and \(\mu\) to the modulation weights \(\phi(t,\mu)=h(t,\mu;\psi)\). The weights \(W,A,B,b\), which are independent of \(t\) and \(\mu\), over all layers are collected into the weight vector \(\theta\in\Theta\subseteq\mathbb{R}^{q^{{}^{\prime}}}\). Typically \(q\ll q^{\prime}\). Using the hyper-network encourages continuity of \(s_{t,\mu}\) in time \(t\), which is key for many physics problems [13].

Combining higher-order quadrature and Monte Carlo sampling for estimating the loss from sample dataEstimating the loss (7) from data can be challenging because the three nested integrals (expectations) over the samples \(X^{i}_{t,\mu}\), time \(t\), and physics parameter \(\mu\) can have different properties and correspondingly need different numerical treatment. Our numerical results show that it is critical to accurately estimate the loss to avoid instabilities in the training; see Section 3 and Figure 2.

We propose a combination of higher-order numerical quadrature and Monte Carlo sampling to estimate the loss (7). In particular, we propose to use a higher-order quadrature rule for the time \(t\) integral. Because it is a one-dimensional integral, standard higher-order quadrature rules from numerical analysis are applicable [27]. The time integral needs to be estimated with particular high accuracy to ensure the coupling between the time points as well as the coupling to the boundary terms to match the path from \(\rho_{0,\mu}\) at time \(t=0\) to \(\rho_{1,\mu}\) at time \(t=1\). Our numerical results will show that estimating the time integral to high accuracy is essential for stabilizing the training. In contrast to the one-dimensional integral over time, the integrals over \(\mathcal{X}\) and the parameter domain \(\mathcal{D}\) can be high dimensional and thus we estimate them via Monte Carlo estimation.

We consider two high-order quadrature rules, composite Simpson's quadrature and Gauss-Legendre quadrature [27]; see Appendix A. We refer to our method as HOAM-S and HOAM-G when using either quadrature, respectively. Importantly, these quadrature rules require samples on specifically spaced time points, equidistant in the case of Simpson's and at the Gauss-Legendre nodes in the case of Gauss quadrature. If the data set (1) does not contain samples at these time points then we interpolate the data to the appropriate times. We note that for Simpson's quadrature, interpolation is typically unnecessary as data simulated with numerical methods often come at equispaced points in time.

We denote a Monte Carlo estimate of an expectation value obtained from a mini-batch as \(\hat{\mathbb{E}}^{n}_{x\sim\rho}[f]:=\sum_{i=1}^{n}f(X^{i})\) where \(X^{1},X^{2},\ldots,X^{n}\sim\rho\). Then, the empirical loss with mini-batching of sizes \(n_{x},n_{\mu}\) and \(n_{t}\) quadrature points in time is given by

\[\hat{E}^{\epsilon}(s)\!=\!\hat{\mathbb{E}}^{n_{\mu}}_{\mu\sim\nu}\!\bigg{[} \sum_{n=1}^{n_{t}}w_{n}\,\hat{\mathbb{E}}^{n_{x}}_{x\sim\rho_{t_{n,\mu}}}\, \bigg{[}\frac{1}{2}|\nabla s_{t_{n},\mu}|^{2}\!+\!\partial_{t}s_{t_{n},\mu}\! +\!\frac{\epsilon^{2}}{2}\Delta s_{t_{n},\mu}\bigg{]}\!-\!\hat{\mathbb{E}}^{n_ {x}}_{x\sim\rho_{t,\mu}}\,[s_{t,\mu}]\bigg{|}_{t=0}^{t=1}\bigg{]}\] (8)

where \(w_{n}\) are numerical quadrature weights and \(t_{n}\) are the corresponding nodes; see Appendix A for the Simpson's quadrature and Gauss-Legendre weights and nodes.

Rapid predictions (inference) with learned reduced modelsMaking predictions in the inference step means drawing samples that follow the law represented by the learned gradient field \(\nabla s_{t,\mu}\), which approximates the law \(\rho_{t,\mu}\) of \(X_{t,\mu}\). Because we train with the loss (7), we integrate the SDE \(\mathrm{d}\hat{X}_{t,\mu}=\nabla s_{t,\mu}(\hat{X}_{t,\mu})\mathrm{d}t+ \epsilon\mathrm{d}W_{t}\), where \(W_{t}\) are Wiener processes and \(\epsilon\) is the same \(\epsilon\) that is used in the training loss (7); see Appendix C. As initial condition, we use samples from \(\rho_{0,\mu}\) at time \(t=0\). Of course other sampling schemes can be used [70].

Notice that the time \(t\) in the SDE used for generating samples is the same time as of the physics problem and thus of the sample trajectory. This means that the costs of the inference step of our HOAM for generating a trajectory of length \(K\) scales as \(\mathcal{O}(K)\). In contrast, introducing a conditioning on time and physics parameter in, e.g., noise-conditioned score matching (NCSM) [80] and conditional flow matching (CFM) or stochastic interpolants [2, 54] requires inferring a separate sampling path for each \(t\) and \(\mu\) pair of interest. In particular, the inference costs of CFM scale as \(\mathcal{O}(K\tau)\), where \(\tau\) is the number of steps taken in the differential equation for generating one sample at one time point. For NCSM with annealed Langevin sampling, the inference costs scale as \(\mathcal{O}(K\tau\sigma)\), where \(\sigma\) is the number of annealing steps. Contrasting this to the scaling of \(\mathcal{O}(K)\) of our HOAM approach shows that HOAM is well suited for fast predictions over \(t\) and \(\mu\) as required in parametric model reduction.

## 3 Numerical experiments

ExamplesWe consider the following parametric dynamical systems; details in Appendix B. _1. Harmonic oscillator_: A collection of particles evolves in four-dimensional phase-space

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

dynamics to learn from such a system are the population dynamics \(\rho_{t,\mu}\) rather than the sample dynamics; see Appendix B.2. We observe the particles computed with a particle-in-cell method and learn the gradient field \(\nabla s_{t,\mu}\) with the proposed HOAM approach. For a test physics parameter \(\mu\) that controls the wave number, we then generate samples with \(\nabla s_{t,\mu}\) and plot a histogram in Figure 3 for the bump-on-tail (top) and two-stream (middle top) instability. Our approach approximates well the histogram obtained with the classical particle-in-cell method. Figure 5 (right) shows that HOAM is the only method which provides speedup over the classical particle-in-cell (full) model, as NCSM and CFM lead to 1-2 orders of magnitude longer inference times than HOAM and the full models.

For the strong Landau damping problem in six dimensions (three spatial and three velocity), our HOAM approach achieves about 2 orders of magnitude speedup. This is because the runtime of the full model based on the traditional particle-in-cell method to compute the mean-field dynamics scales poorly with the dimension. In this example, the runtime of the full model increases by almost two orders of magnitude. In contrast, the runtime of our HOAM reduced model increases only from 6 to 8 seconds. This importantly shows that the computational costs of the inference step of reduced models built with HOAM avoid exponential scaling with the dimension in this example.

We now compute the electric energy as a quantity of interest from the generated samples over time \(t\) for the test physics parameters, which we plot in Figure 4 and its relative error averaged over time (e.e.) in Table 1 (see (25)). Our HOAM approximates the electric energy well at later times, whereas NCSM and CFM lead to poorer approximations at later times \(t\). This is relevant because this non-linear regime is where numerical solvers become important; the initial (linear) growth regime can be approximated well by analytical perturbation theory. Also for the six-dimensional strong Landau damping problem, our HOAM approach provides accurate predictions of the electric energy with orders of magnitude speedups; see Table 1 and Figure 3 as well as Figure 7 in the appendix.

Speedups in inference step (predictions)Recall two limitations of introducing a time and physics parameter dependence in NCSM/CFM via conditioning (see page 3 and Section 2.3): (i) For each \(t\) and \(\mu\), a separate sampling path has to be computed, which leads to orders of magnitude higher inference runtimes than in HOAM; see Table 1, Section 2.3. (ii) For each \(t\) and \(\mu\) pair, the target distribution \(\rho_{t,\mu}\) is different, which can require \(t\)- and \(\mu\)-specific tuning of hyper-parameters of the inference step, which is impractical and thus can lead to a deterioration of accuracy compared to our HOAM approach; see Figure 3-4.

Predicting statistics of chaotic and particle dynamics in high dimensionsWe now consider the nine-dimensional dynamical system introduced in [69], which leads to chaotic behavior. We show in Figure 3 (bottom) the sample histogram corresponding to a test physics parameter that represents the Rayleigh number. At time \(t=3.7\) and projecting onto

Figure 4: Electric energy of bump-on-tail (top) and two-stream (bottom) instability. HOAM with Simpson’s and Gauss quadrature accurately predicts the energy growth in the transient regime and oscillations at later times. The ground truth is displayed in blue.

dimension three and nine, the histograms show that the proposed HOAM accurately matches the low probability region that connects the two high probability regions, whereas AM fails to converge. Consider now the example of the particles in an aharmonic trap, which leads to 100-dimensional samples \(X^{i}_{t,\mu}\). For a test physics parameter, Figure 5 shows that HOAM accurately predicts the mean particle positions even for this high dimensional system.

## 4 Conclusions, limitations, and future work

For parametric model reduction, learning population dynamics via minimal-energy vector fields over time \(t\) and physics parameter \(\mu\) with our variational approach helps reduce inference runtime compared to standard diffusion- and flow-based modeling that condition on \(t\) and \(\mu\) and therefore have to solve a separate inference problem for each time step and physics parameter at test time. Because we learn the dynamics over time \(t\), it is critical to accurately capture the coupling over the time steps, for which we propose to use higher-order quadrature schemes when estimating time integrals in the training loss. The higher-order quadrature of the time integrals considerably improves training stability. Our approach achieves comparable errors as state-of-the-art methods while at the same time reducing inference runtime by 1-2 orders of magnitude. Additionally, HOAM provides speedups of up to 2 orders of magnitude to classical numerical full models.

_Limitations_: First, if there are only very few samples in time, even numerical quadrature cannot provide an accurate enough estimation of the loss, which could be a limitation in computational biology [23, 9]. Second, we currently seek a vector field that minimizes the kinetic energy or a variant thereof. Investigating other notions of energy that might lead to vector fields with other desired properties in certain problems remains a challenge.

We do not expect that this work has negative societal impacts.

\begin{table}
\begin{tabular}{l|l l|l l|l l|l l} \hline example: & \multicolumn{2}{c|}{**two-stream**} & \multicolumn{2}{c|}{**bump-on-tail**} & \multicolumn{2}{c|}{**strong Landau**} & \multicolumn{2}{c}{**9D chaos**} \\ \hline metric: & e.e. & r.t. [s] & e.e. & r.t. [s] & e.e. & r.t. [s] & sinkhorn & r.t. [s] \\ \hline CFM [2, 54] & 1.44 & 139 & 5.52 & 141 & 0.629 & 161 & 0.259 & 36 \\ NCSM [80] & 0.245 & 1142 & 0.626 & 1133 & 4.06 & 4531 & 0.869 & 1109 \\ AM [61] & 0.275 & 6 & 0.892 & 6 & NaN & - & 80.1 & 7 \\ HOAM-S (ours) & **0.078** & 6 & **0.427** & 6 & 0.641 & 7 & **0.214** & 7 \\ HOAM-G (ours) & 0.208 & 6 & 0.429 & 6 & **0.447** & 7 & 0.217 & 7 \\ \hline \end{tabular}
\end{table}
Table 1: HOAM with Simpson’s and Gauss quadrature outperforms state-of-the-art methods w.r.t. inference runtime (r.t.) with comparable errors when applied to various physics problems for parametric model reduction. Metrics: e.e. is the relative error in electric energy, see (25); for the Sinkhorn divergence, see Appendix B.5.

Figure 5: **Left**: HOAM accurately predicts the time evolution of the mean position of a 100-dimensional particle system in an aharmonic moving trap (dim 1 vs dim 100). **Right**: HOAM reduced models provide about 2 orders of magnitude speedup over traditional numerical (full) models for the 6 dimensional strong Landau problem. HOAM is also 1–2 orders of magnitude faster than CFM and NCSM, which provide no speedup over the full models in our problems.

## Acknowledgements

Berman and Peherstorfer were partially supported by the Air Force Office of Scientific Research under award FA9550-21-1-0222. Peherstorfer and Blickhan were partially supported by the Office of Naval Research, United States under award N00014-22-1-2728. We thank Stefan Possanner and Dominik Bell (Max Planck Institute for Plasma Physics) for their support with using the high-fidelity code used in the six-dimensional Vlasov experiments.

## References

* [1] M. S. Albergo, N. M. Boffi, and E. Vanden-Eijnden. Stochastic interpolants: A unifying framework for flows and diffusions. _arXiv_, 2303.08797, 2023.
* [2] M. S. Albergo and E. Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In _The Eleventh International Conference on Learning Representations_, 2023.
* [3] L. Ambrosio and W. Gangbo. Hamiltonian ODEs in the Wasserstein space of probability measures. _Communications on Pure and Applied Mathematics_, 61(1):18-53, Jan. 2008.
* [4] L. Ambrosio and N. Gigli. A User's Guide to Optimal Transport. In _Modelling and Optimisation of Flows on Networks_, volume 2062, pages 1-155. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013. Series Title: Lecture Notes in Mathematics.
* [5] L. Ambrosio, N. Gigli, and G. Savare. _Gradient Flows_. Lectures in Mathematics ETH Zurich. Birkhauser-Verlag, Basel, 2005.
* [6] V. Arnold. Sur la geometrie differentielle des groupes de Lie de dimension infinie et ses applications a l'hydrodynamique des fluides parfaits. _Annales de l'institut Fourier_, 16(1):319-361, 1966.
* 2207, 2010.
* [8] J.-D. Benamou and Y. Brenier. A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem. _Numerische Mathematik_, 84(3):375-393, Jan. 2000.
* [9] J.-D. Benamou, T. O. Gallouet, and F.-X. Vialard. Second-Order Models for Optimal Transport and Cubic Splines on the Wasserstein Space. _Foundations of Computational Mathematics_, 19(5):1113-1143, Oct. 2019.
* [10] P. Benner, S. Gugercin, and K. Willcox. A survey of projection-based model reduction methods for parametric dynamical systems. _SIAM Review_, 57(4):483-531, 2015.
* [11] P. Benner, M. Ohlberger, A. Cohen, and K. Willcox, editors. _Model Reduction and Approximation: Theory and Algorithms_. Society for Industrial and Applied Mathematics, Philadelphia, PA, July 2017.
* [12] P. Benner and M. Redmann. Model reduction for stochastic systems. _Stochastic Partial Differential Equations: Analysis and Computations_, 3(3):291-338, Sep 2015.
* [13] J. Berman and B. Peherstorfer. CoLoRA: Continuous low-rank adaptation for reduced implicit neural modeling of parameterized partial differential equations. In R. Salakhutdinov, Z. Kolter, K. Heller, A. Weller, N. Oliver, J. Scarlett, and F. Berkenkamp, editors, _Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine Learning Research_, pages 3565-3583. PMLR, 21-27 Jul 2024.
* [14] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, V. Jampani, and R. Rombach. Stable video diffusion: Scaling latent video diffusion models to large datasets, 2023.

* [15] N. M. Boffi and E. Vanden-Eijnden. Probability flow solution of the fokker-planck equation. _Machine Learning: Science and Technology_, 4(3):035012, jul 2023.
* [16] J. Bruna, B. Peherstorfer, and E. Vanden-Eijnden. Neural Galerkin schemes with active learning for high-dimensional evolution equations. _Journal of Computational Physics_, 496:112588, 2024.
* [17] C. Bunne, L. Papaxanthos, A. Krause, and M. Cuturi. Proximal optimal transport modeling of population dynamics. In G. Camps-Valls, F. J. R. Ruiz, and I. Valera, editors, _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151 of _Proceedings of Machine Learning Research_, pages 6511-6528. PMLR, 28-30 Mar 2022.
* [18] S. R. Cachay, B. Zhao, H. James, and R. Yu. DYffusion: A dynamics-informed diffusion model for spatiotemporal forecasting. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [19] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31. Curran Associates, Inc., 2018.
* [20] Y. Chen, M. Goldstein, M. Hua, M. S. Albergo, N. M. Boffi, and E. Vanden-Eijnden. Probabilistic forecasting with stochastic interpolants and Follmer processes. _arXiv_, 2403.13724, 2024.
* [21] Y. Chen and D. Xiu. Learning stochastic dynamical system via flow map operator. _Journal of Computational Physics_, 508:112984, 2024.
* [22] Y. Cheng, A. J. Christlieb, and X. Zhong. Energy-conserving discontinuous Galerkin methods for the Vlasov-Ampere system. _Journal of Computational Physics_, 256:630-655, 2014.
* [23] S. Chewi, J. Clancy, T. Le Gouic, P. Rigollet, G. Stepaniants, and A. Stromme. Fast and Smooth Interpolation on Wasserstein Space. In A. Banerjee and K. Fukumizu, editors, _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 3061-3069. PMLR, Apr. 2021.
* [24] S.-N. Chow, W. Li, and H. Zhou. Wasserstein Hamiltonian flows. _Journal of Differential Equations_, 268(3):1205-1219, Jan. 2020.
* [25] M. Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013.
* [26] A. Davtyan, S. Sameni, and P. Favaro. Efficient video prediction via sparsely conditioned flow matching. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 23263-23274, October 2023.
* [27] P. Deuflhard and A. Hohmann. _Numerical Analysis in Modern Scientific Computing_. Springer, 2003.
* [28] E. Dupont, A. Doucet, and Y. W. Teh. Augmented neural ODEs. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [29] A. Ern and J.-L. Guermond. _Theory and Practice of Finite Elements_, volume 159 of _Applied Mathematical Sciences_. Springer New York, New York, NY, 2004.
* [30] J. A. Farrell, Y. Wang, S. J. Riesenfeld, K. Shekhar, A. Regev, and A. F. Schier. Single-cell reconstruction of developmental trajectories during zebrafish embryogenesis. _Science_, 360(6392):eaar3131, June 2018.

- A heuristic point of view. _Revista Matematica Iberoamericana_, 36(4):1071-1112, Jan. 2020.
* [32] R. G. Ghanem and P. D. Spanos. _Stochastic Finite Elements: A Spectral Approach_. Springer, 1991.
* [33] W. Harvey, S. Naderiparizi, V. Masrani, C. D. Weilbach, and F. Wood. Flexible diffusion modeling of long videos. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [34] T. Hashimoto, D. Gifford, and T. Jaakkola. Learning population-level diffusions with generative RNNs. In M. F. Balcan and K. Q. Weinberger, editors, _Proceedings of The 33rd International Conference on Machine Learning_, volume 48 of _Proceedings of Machine Learning Research_, pages 2417-2426, New York, New York, USA, 20-22 Jun 2016. PMLR.
* [35] J. Hittinger and J. Banks. Block-structured adaptive mesh refinement algorithms for Vlasov simulation. _Journal of Computational Physics_, 241:118-140, 2013.
* [36] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 6840-6851. Curran Associates, Inc., 2020.
* [37] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video diffusion models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 8633-8646. Curran Associates, Inc., 2022.
* [38] B. Holzschuh, S. Vegetti, and N. Thuerey. Solving inverse physics problems with score matching. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [39] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022.
* [40] T. J. R. Hughes. _The Finite Element Method: Linear Static and Dynamic Finite Element Analysis_. Dover Publications, 2012.
* [41] A. Hyvarinen. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(24):695-709, 2005.
* [42] P. Kidger, J. Foster, X. Li, and T. J. Lyons. Neural SDEs as infinite-dimensional GANs. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 5453-5463. PMLR, 18-24 Jul 2021.
* [43] K. Kormann and A. Yurova. A generalized Fourier-Hermite method for the Vlasov-Poisson system. _BIT Numerical Mathematics_, 61(3):881-909, Sept. 2021.
* [44] T. Koshizuka and I. Sato. Neural Lagrangian Schrodinger Bridge: Diffusion Modeling for Population Dynamics. In _The Eleventh International Conference on Learning Representations_, 2023.
* [45] B. Kramer, B. Peherstorfer, and K. E. Willcox. Learning nonlinear reduced models from data with operator inference. _Annual Review of Fluid Mechanics_, 56(Volume 56, 2024):521-548, 2024.
* [46] J. N. Kutz, S. L. Brunton, B. W. Brunton, and J. L. Proctor. _Dynamic mode decomposition: data-driven modeling of complex systems_. SIAM, 2016.
* 500, 2024.
* [48] K. Lee and E. J. Parish. Parameterized neural ordinary differential equations: applications to computational physics problems. _Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 477(2253):20210162, 2021.
* [49] A. Lenard and I. B. Bernstein. Plasma Oscillations with Diffusion in Velocity Space. _Physical Review_, 112(5):1456-1459, Dec. 1958.
* [50] L. Li, S. Hurault, and J. Solomon. Self-consistent velocity matching of probability flows. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [51] X. Li, T.-K. L. Wong, R. T. Q. Chen, and D. K. Duvenaud. Scalable gradients and variational inference for stochastic differential equations. In C. Zhang, F. Ruiz, T. Bui, A. B. Dieng, and D. Liang, editors, _Proceedings of The 2nd Symposium on Advances in Approximate Bayesian Inference_, volume 118 of _Proceedings of Machine Learning Research_, pages 1-28. PMLR, 08 Dec 2020.
* [52] M. Lienen, D. Ludke, J. Hansen-Palmus, and S. Gunnemann. From zero to turbulence: Generative modeling for 3d flow simulation. In _The Twelfth International Conference on Learning Representations_, 2024.
* Collisionless Plasmas. In E. M. Lifshitz and L. P. Pitaevski, editors, _Physical Kinetics_, volume 10 of _Course of Theoretical Physics_, pages 115-167. Pergamon, Amsterdam, Jan. 1981.
* [54] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le. Flow Matching for Generative Modeling. In _The Eleventh International Conference on Learning Representations_, 2023.
* [55] J. Lott. Some Geometric Calculations on Wasserstein Space. _Communications in Mathematical Physics_, 277(2):423-437, Nov. 2007.
* A_, 34(4):1533-1574, 2014.
* [57] D. B. Melrose. _Instabilities in Space and Laboratory Plasmas_. Cambridge University Press, Aug. 1986. Publication Title: Instabilities in Space and Laboratory Plasmas ADS Bibcode: 1986islp.book.....M.
* [58] I. Mezic. Spectral properties of dynamical systems, model reduction and decompositions. _Nonlinear Dynamics_, 41(1-3):309-325, 2005.
* [59] C. Mouhot and C. Villani. On Landau damping. _Acta Mathematica_, 207(1):29-201, 2011.
* [60] A. Muntean, J. Rademacher, and A. Zagaris, editors. _Macroscopic and Large Scale Phenomena: Coarse Graining, Mean Field Limits and Ergodicity_, volume 3 of _Lecture Notes in Applied Mathematics and Mechanics_. Springer International Publishing, Cham, 2016.
* [61] K. Neklyudov, R. Brekelmans, D. Severo, and A. Makhzani. Action Matching: Learning Stochastic Dynamics from Samples. In A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 25858-25889. PMLR, July 2023.
* [62] F. Noe, A. Tkatchenko, K.-R. Muller, and C. Clementi. Machine learning for molecular simulation. _Annual Review of Physical Chemistry_, 71(Volume 71, 2020):361-390, 2020.
* [63] F. Otto. The geometry of dissipative evolution equations: the porous medium equation. _Communications in Partial Differential Equations_, 26(1-2):101-174, Jan. 2001.
* [64] F. Otto and C. Villani. Generalization of an Inequality by Talagrand and Links with the Logarithmic Sobolev Inequality. _Journal of Functional Analysis_, 173(2):361-400, June 2000.

* [65] B. Peherstorfer, K. Willcox, and M. Gunzburger. Survey of multifidelity methods in uncertainty propagation, inference, and optimization. _SIAM Review_, 60(3):550-591, 2018.
* [66] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. Courville. Film: visual reasoning with a general conditioning layer. In _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence_, AAAI'18/IAAI'18/EAAI'18. AAAI Press, 2018.
* [67] S. Possanner, F. Holderied, Y. Li, B. K. Na, D. Bell, S. Hadjout, and Y. Guclu. High-Order Structure-Preserving Algorithms for Plasma Hybrid Models. In F. Nielsen and F. Barbaresco, editors, _Geometric Science of Information_, volume 14072, pages 263-271. Springer Nature Switzerland, Cham, 2023. Series Title: Lecture Notes in Computer Science.
* [68] K. Rasul, C. Seward, I. Schuster, and R. Vollgraf. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In M. Meila and T. Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 8857-8868. PMLR, 18-24 Jul 2021.
* [69] P. Reiterer, C. Lainscsek, F. Schurrer, C. Letellier, and J. Maquet. A nine-dimensional lorenz system to study high-dimensional chaos. _Journal of Physics A: Mathematical and General_, 31(34):7121, aug 1998.
* [70] C. Robert and G. Casella. _Monte Carlo Statistical Methods_. Springer, 2004.
* [71] C. W. Rowley, I. Mezic, S. Bagheri, P. Schlatter, and D. S. Henningson. Spectral analysis of nonlinear flows. _Journal of Fluid Mechanics_, 641:115-127, 2009.
* [72] G. Rozza, D. Huynh, and A. Patera. Reduced basis approximation and a posteriori error estimation for affinely parametrized elliptic coercive partial differential equations. _Archives of Computational Methods in Engineering_, 15(3):1-47, 2007.
* [73] C. Salvi, M. Lemercier, and A. Gerasimovics. Neural stochastic pdes: Resolution-invariant learning of continuous spatiotemporal dynamics. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 1333-1344. Curran Associates, Inc., 2022.
* [74] F. Santambrogio. _Optimal Transport for Applied Mathematicians_, volume 87 of _Progress in Nonlinear Differential Equations and Their Applications_. Springer International Publishing, Cham, 2015.
* [75] G. Schiebinger, J. Shu, M. Tabaka, B. Cleary, V. Subramanian, A. Solomon, J. Gould, S. Liu, S. Lin, P. Berube, L. Lee, J. Chen, J. Brumbaugh, P. Rigollet, K. Hochedlinger, R. Jaenisch, A. Regev, and E. S. Lander. Optimal-Transport Analysis of Single-Cell Gene Expression Identifies Developmental Trajectories in Reprogramming. _Cell_, 176(4):928-943.e22, Feb. 2019.
* [76] P. J. Schmid. Dynamic mode decomposition of numerical and experimental data. _Journal of Fluid Mechanics_, 656:5-28, 2010.
* [77] E. Schrodinger. Uber die Umkehrung der Naturgesetze. Technical Report 1931 IX, Akademie der Wissenschaften, Berlin, 1931.
* [78] Z. Shen, Z. Wang, S. Kale, A. Ribeiro, A. Karbasi, and H. Hassani. Self-consistency of the fokker planck equation. In P.-L. Loh and M. Raginsky, editors, _Proceedings of Thirty Fifth Conference on Learning Theory_, volume 178 of _Proceedings of Machine Learning Research_, pages 817-841. PMLR, 02-05 Jul 2022.

* [79] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In F. Bach and D. Blei, editors, _Proceedings of the 32nd International Conference on Machine Learning_, volume 37 of _Proceedings of Machine Learning Research_, pages 2256-2265, Lille, France, 07-09 Jul 2015. PMLR.
* [80] Y. Song and S. Ermon. Generative modeling by estimating gradients of the data distribution. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* [81] Y. Song, S. Garg, J. Shi, and S. Ermon. Sliced score matching: A scalable approach to density and score estimation. In _Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019_, page 204, 2019.
* [82] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2021.
* [83] E. Sonnendrucker, A. Wacher, R. Hatzky, and R. Kleiber. A split control variate scheme for PIC simulations with collisions. _Journal of Computational Physics_, 295:402-419, Aug. 2015.
* [84] A. Tong, J. Huang, G. Wolf, D. Van Dijk, and S. Krishnaswamy. TrajectoryNet: A dynamic optimal transport network for modeling cellular dynamics. In H. D. III and A. Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 9526-9536. PMLR, 13-18 Jul 2020.
* [85] C. Trapnell, D. Cacchiarelli, J. Grimsby, P. Pokharel, S. Li, M. Morse, N. J. Lennon, K. J. Livak, T. S. Mikkelsen, and J. L. Rinn. The dynamics and regulators of cell fate decisions are revealed by pseudotemporal ordering of single cells. _Nature Biotechnology_, 32(4):381-386, Apr. 2014.
* [86] J. H. Tu, C. W. Rowley, D. M. Luchtenburg, S. L. Brunton, and J. N. Kutz. On dynamic mode decomposition: Theory and applications. _Journal of Computational Dynamics_, 1(2):391-421, 2014.
* [87] T. M. Tyranowski. Stochastic variational principles for the collisional Vlasov-Maxwell and Vlasov-Poisson equations. _Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences_, 477(2252):20210167, Aug. 2021.
* [88] B. Tzen and M. Raginsky. Neural stochastic differential equations: Deep latent gaussian models in the diffusion limit. _arXiv_, 1905.09883, 2019.
* [89] C. Villani. _Optimal transport: old and new_. Number 338 in Grundlehren der mathematischen Wissenschaften. Springer, Berlin, 2009.
* [90] C. Villani. _Topics in optimal transportation_. Number 58 in Graduate studies in mathematics. American Mathematical Society, Providence, Rhode Island, reprinted with corrections edition, 2016.
* [91] P. Vincent. A connection between score matching and denoising autoencoders. _Neural Computation_, 23(7):1661-1674, 2011.
* [92] M. O. Williams, I. G. Kevrekidis, and C. W. Rowley. A data-driven approximation of the Koopman operator: Extending dynamic mode decomposition. _Journal of Nonlinear Science_, 25(6):1307-1346, 2015.
* [93] F. A. Wolf, F. K. Hamey, M. Plass, J. Solana, J. S. Dahlin, B. Gottgens, N. Rajewsky, L. Simon, and F. J. Theis. PAGA: graph abstraction reconciles clustering with trajectory inference through a topology preserving map of single cells. _Genome Biology_, 20(1):59, Dec. 2019.

Quadrature rules

### Monte Carlo estimation

Monte Carlo estimation approximates an integral by evaluating the integrand at randomly sampled nodes within an interval \([a,b]\),

\[\int_{a}^{b}f(t)\,dt\approx\frac{b-a}{N}\sum_{i=1}^{N}f(t_{i}).\] (9)

We consider only the case where the nodes \(t_{i}\) are uniformly distributed random variables in \([a,b]\). The weights are:

\[w_{i}=\frac{b-a}{N},\quad i=1,2,\ldots,N.\] (10)

The root mean-squared integration error of Monte Carlo estimation decays as \(\mathcal{O}(N^{-1/2})\), for integrands with bounded variance.

### Trapezoidal Rule

The trapezoidal rule approximates the integral of a function \(f\) over an interval \([a,b]\) by dividing it into \(N\) subintervals of equal width \(h=\frac{b-a}{N}\). The approximation is given by:

\[\int_{a}^{b}f(t)\,dt\approx h\left[\frac{1}{2}f(t_{0})+\sum_{i=1}^{N-1}f(t_{i} )+\frac{1}{2}f(t_{N})\right],\] (11)

where the nodes \(t_{i}\) are:

\[t_{i}=a+ih,\quad i=0,1,\ldots,N.\] (12)

The trapezoidal rule is a second-order rule, which means that the integration error decays as \(\mathcal{O}(h^{2})\) for sufficiently smooth functions.

### Composite Simpson's Rule

Composite Simpson's rule approximates the integral by fitting parabolas through intervals. It divides \([a,b]\) into an even number \(N\) of subintervals of width \(h=\frac{b-a}{N}\). The approximation is:

\[\int_{a}^{b}f(t)\,dt\approx\frac{h}{3}\left[f(t_{0})+2\sum_{i=1}^{N/2-1}f(t_{2 i})+4\sum_{i=1}^{N/2}f(t_{2i-1})+f(t_{N})\right],\] (13)

with nodes:

\[t_{i}=a+ih,\quad i=0,1,\ldots,N.\] (14)

Composite Simpson's rule is a fourth-order rule, which means that the integration error decays as \(\mathcal{O}(h^{4})\) for sufficiently smooth functions.

### Gauss-Legendre Quadrature

Gauss-Legendre quadrature approximates the integral over \([-1,1]\) by choosing nodes \(t_{i}\) and weights \(w_{i}\) so that polynomials of the highest possible degree are integrated exactly. A Gauss-Legendre quadrature has the form

\[\int_{-1}^{1}f(t)\,dt\approx\sum_{i=1}^{n}w_{i}f(t_{i}),\] (15)where \(t_{i}\) are the roots of the Legendre polynomial \(P_{n}(t)\), and the weights are:

\[w_{i}=\frac{2}{(1-t_{i}^{2})[P_{n}^{\prime}(t_{i})]^{2}}.\] (16)

For integration over \([a,b]\), a linear transformation maps \([-1,1]\) to \([a,b]\):

\[\tilde{t}_{i}=\frac{b-a}{2}t_{i}+\frac{a+b}{2},\quad\tilde{w}_{i}=\frac{b-a}{2 }w_{i}.\] (17)

The approximation becomes:

\[\int_{a}^{b}f(t)\,dt\approx\sum_{i=1}^{n}\tilde{w}_{i}f(\tilde{t}_{i}).\] (18)

Gauss-Legendre quadrature exactly integrates polynomials of degree \(2n-1\), where \(n\) is the number of nodes.

## Appendix B Details about numerical examples

### Harmonic oscillator with background collisions

The equation of motion in four-dimensional phase-space for \(X=[X_{1},X_{2},V_{1},V_{2}]\) is given by:

\[\frac{\mathrm{d}}{\mathrm{d}t}\begin{bmatrix}X_{1}\\ X_{2}\\ V_{1}\\ V_{2}\end{bmatrix}(t)=\begin{bmatrix}V_{1}\\ V_{2}\\ -\omega^{2}X_{1}\\ -\omega^{2}X_{2}\end{bmatrix}(t)+\begin{bmatrix}0\\ 0\\ \eta\\ \eta\end{bmatrix}\xi(t)\,.\] (19)

Here, \(\eta=5\times 10^{-2}\) and \(\xi\) denotes white noise. The initial configuration is a Gaussian centered at \(m_{0}=[1,1]\) with covariance equal to \(\Sigma_{0}=10^{-2}\times\mathrm{Id}\) in the spatial coordinates \(X_{1},X_{2}\) and Gaussian in the velocity coordinates \(V_{1},V_{2}\) centered at zero and with covariance \(\Sigma_{0}=10^{-2}\times\mathrm{Id}\).

### Vlasov-Poisson problems

Mean field approximationsThe Vlasov-Poisson system describes the interaction of charged particles. Due to the presence of the Coulomb force, the dynamics of a single particle depend on the position of all other particles. Assuming \(N\) particles in the system, this means \(\frac{\mathrm{d}}{\mathrm{d}t}X_{t,\mu}^{i}=v(t,X_{t,\mu}^{i};\mu,X_{t,\mu}^{1 },\ldots,X_{t,\mu}^{N})\). Given the fact that \(N\) is in practice extremely large, it is natural to pass to the _mean-field limit_. Assuming the particles are indistinguishable, the result is a PDE of the form \(\partial_{t}\rho_{t,\mu}+\nabla\cdot(\rho_{t,\mu}v_{\mathrm{mf}}(t,\cdot;\mu, \rho_{t,\mu}))=0\) that describes the evolution of the collection (or population, ensemble) of particles denoted by \(\rho_{t,\mu}\). In the specific case of the Vlasov-Poisson problem, Coulomb interactions in the mean-field limit give rise to a Poisson equation determining an electric field that is generated by the collection of particles and influences its dynamics. For completeness sake, we mention that the singularity of the Coulomb interaction poses a considerable technical challenge when passing to this limit. We refer to [53, 59] for the derivation of the Vlasov-Poisson equation and [60] for more examples of mean-field systems. The theory behind the test-cases we run in this work can be found in [57], Chapter 3.

Governing equationWe slightly change the notation here to be consistent with the references. \(f:\mathcal{X}_{x}\times\mathbb{R}^{d}\times\mathbb{R}\times\mathcal{D}\to \mathbb{R}\), \(d\in\{1,2,3\}\), denotes the distribution function governed by the Vlasov-Poisson system

\[\partial_{t}f(x,v,t;\mu) =-v\cdot\nabla_{x}f(x,v,t;\mu)-\nabla\phi(x,t)\cdot\nabla_{v}f(x, v,t;\mu)=0\,,\] (20) \[-\mu^{2}\Delta\phi(x,t;\mu) =1-\int_{\mathbb{R}^{d}}f(x,v,t;\mu)\mathrm{d}v\,.\] (21)

In the notation of the rest of this work, \(f(\cdot,\cdot,t;\mu)=\rho_{t,\mu}\), \(\mathcal{X}_{x}\times\mathbb{R}^{d}=\mathcal{X}\). The spatial domain \(\mathcal{X}_{x}\) is a subset of \(\mathbb{R}^{d}\), in all our examples it is of the form \([Two-stream instabilityIn this case, \(d=2\), so the particle positions vary in \(\mathcal{X}_{x}=[0,l_{1}]\) with periodic boundary conditions and their velocity evolves in \(\mathbb{R}\). For the two-stream instability, we set the initial distribution to

\[f_{0}(x,v):=\frac{1}{2\sqrt{2\pi}}\left(1+\alpha\cos\left(2\pi\frac{x}{l_{1}} \right)\right)\left(\exp\left(-\frac{(v-v_{0})^{2}}{2}\right)+\exp\left(-\frac{ (v+v_{0})^{2}}{2}\right)\right),\] (22)

with \(\alpha=0.05,l_{1}=50,v_{0}=3\). The parameter \(\mu\) varies as \(\mu_{\text{train}}\in\{1.2,1.3,\ldots,1.9\}\) and \(\mu_{\text{test}}\in\{1.25,1.85\}\). We use a particle-in-cell method for generating the data based on the repository https://github.com/pmocz/pic-python. The number of marker particles is \(N=25000\) and for the sake of computing the electric field, a uniform grid of \(N/8\) cells is used. Integration in time is done via a Stormer-Verlet splitting over \(t\in[0,40]\) with time-step size \(10^{-2}\).

Bump-on tailWe consider the initial distribution

\[f_{0}(x,v)=\frac{1}{\sqrt{2\pi}}\left(1+\alpha\cos\left(2\pi\frac{x}{l_{1}} \right)\right)\left(\frac{\delta}{\sigma_{1}}\exp\left(-\frac{v^{2}}{2\sigma_{1 }^{2}}\right)+\frac{1-\delta}{\sigma_{2}}\exp\left(-\frac{(v-v_{b})^{2}}{2 \sigma_{2}^{2}}\right)\right)\,,\] (23)

with \(\alpha=0.05,l_{1}=50,v_{b}=4,\delta=9/10,\sigma_{1}=1,\sigma_{2}=1/\sqrt{2}\). The parameter \(\mu\) varies as \(\mu_{\text{train}}\in\{1.3,1.4,\ldots,2.0\}\) and \(\mu_{\text{test}}\in\{1.35,1.95\}\). The other parameters are the same as in the two-stream case.

Strong Landau dampingIn this case, \(d=6\) and

\[f_{0}(x,v)=\frac{1}{\sqrt{2\pi}^{3}}\left(1+\alpha\cos\left(2\pi\frac{x_{1}}{ l_{1}}\right)\right)\exp\left(-\frac{|v|^{2}}{2}\right)\,,\] (24)

with \(l_{1}=4\pi\) and \(l_{2}=l_{3}=1\). The data is generated using the Struply package [67], the exact specifications of the simulation are available at https://gitlab.mpcdf.mpg.de/struply as an example problem. The physics parameter we vary is the mass of the charged particles, which has the effect of changing the strength of the inertial term accelerating the particles relative to the advection term \(v\cdot\nabla_{x}f\). This implies \(\mu\in\{0.5,0.6,\ldots,1.5\}\), where \(\mu=1.0\) corresponds to the default settings. This \(\mu=1.0\) is also the test parameter and is excluded from the training set. The timing for the full order method has been obtained on a computing cluster with AMD EPYC Genoa 9554 CPUs using 8 MPI processes, which is a default option of the used code. For a single MPI process, it extends to 27 minutes while for 16, it can be reduced to 4 minutes.

The high-fidelity data we generate is using a control variate approach in order to reduce numerical noise introduced by the finite number of marker particles. Since we require the particles to be identical for our method, we assume they are all weighted equally when re-constructing the electric potential. This biases our reconstructed potential in comparison to the physical one, but we observe in practice that this is only by a multiplicative constant. We save \(10^{5}\) marker particles from the high-order simulations and use \(N=25000\) of them as input data for our method. We integrate in time over \(t\in[0,8.75]\)

### High-dimensional chaos

We consider the dynamical system introduced in [69]. We generate samples by initializing a 9 dimensional Gaussian centered at the origin with width equal to \(2\times 10^{-2}\). We then integrate these samples forward as an SDE whose drift is given by the 9-dimensional system of ODEs described in [69] and whose diffusion term is given as diagonal noise equal to \(5\times 10^{-2}\). We integrate 25000 particles of the system up to \(T=20\) using the Euler-Maruyama scheme with time step size equal to \(10^{-2}\). The parameter \(\mu\) varies as \(\mu_{\text{train}}\in\{13.5,13.6,\ldots,14.2\}\) and \(\mu_{\text{test}}\in\{13.65,14.05\}\).

### Particles in anharmonic trap

We consider the evolution of interacting particles in an anharmonic trap [16]. The two-dimensional particle positions \(Z_{1}(t,\mu),\ldots,Z_{M}(t,\mu)\) are governed by an SDE

\[\mathrm{d}Z_{i}=g(t,Z_{i})\mathrm{d}t+\sum_{j=1}^{M}K(Z_{i},Z_{j}) \mathrm{d}t+\sqrt{2\gamma}\mathrm{d}W_{i}\,,\qquad i=1,\ldots,M\,,\]

where \(\gamma>0\) is the diffusion coefficient and \(W_{i}\) are independent Wiener processes. The function \(g(t,Z)=(a(t)-Z)^{3}\) describes a time-dependent one-body force, where \(a(t)=5/4(\sin(\pi t)+3/2))+\mu\cos(2\pi t)\) is the position of the trap. The function \(K(Z,Z^{\prime})=\frac{\alpha}{M}(Z^{\prime}-Z)\) describes a pairwise interaction term. We set \(\alpha=-1/4\) and \(\gamma=10^{-2}\). The parameter \(\mu\) is in the range \(\mathcal{D}=[0.3,0.9]\) and modifies the position of the trap. A sample \(X^{i}_{t,\mu}\) corresponds to a vector \([Z_{1}(t,\mu),\ldots,Z_{M}(t,\mu)]^{T}\) of dimension \(100\), because we have \(M=50\) particles and each position \(Z_{j}(t,\mu)\) as two dimensions. We generate samples via Monte Carlo by using the Euler-Maruyama scheme. The time step size is \(\delta t=10^{-3}\) and we integrate up to final time \(2\).

### Other details about numerical experiments

In terms of network architecture, we follow [13] closely because we use their network architecture. We use MLPs to parameterize both the main network and the hyper-network with swish activation functions. The main network is depth \(7\) and width \(64\) linear layers while the hyper-network is depth \(3\) with width \(15\) linear layers. The rank of the CoLoRA modulations is set to \(3\). Identical CoLoRA architectures are used for all HOAM experiments as well as the comparisons with AM, NCSM, and CFM. The only difference is the size of the output layer for NCSM and CFM whose outputs must be the same dimensionality as their inputs.

For all experiments we use an Adam optimizer at a \(2\times 10^{-3}\) learning rate with a cosine learning rate scheduler. For all experiments unless otherwise noted, we take a batch size of \(256\) particles over \(256\) time points. We optimize for \(50,000\) Adam iterations for Vlasov examples and for \(25,000\) Adam iterations for all other examples.

The results were computed on NVIDIA Quadro RTX 8000 GPUs. All code was implemented in Python using the JAX library with JIT complication where possible.

Hyper-parameter \(\epsilon\) in the loss (7) searched over \(\{0,1,2,5,7\}\times 10^{-2}\) for both HOAM and AM.

The relative error in the electric energy is computed as

\[\frac{1}{T}\sum_{t=1}^{T}\frac{|e_{\mathrm{true}}(t)-e_{\mathrm{predict}}(t)|}{ |e_{\mathrm{true}}(t)|},\] (25)

where \(e_{\mathrm{true}}(t)\) is the electric energy predicted by the high-fidelity numerical simulations at time \(t\) and \(e_{\mathrm{predict}}(t)\) is the electric energy computed from samples of either HOAM (ours), AM, NCSM, or CFM. The relative error in the mean is

\[\frac{1}{T}\sum_{t=1}^{T}\frac{|\mathbb{E}[\rho_{\mathrm{true}}(t)]-\mathbb{E} [\rho_{\mathrm{predict}}(t)]|}{|\mathbb{E}[\rho_{\mathrm{true}}(t)]|}\,,\] (26)

where the expected values are estimated via Monte Carlo from the generated samples.

The Sinkhorn distance is computed with https://ott-jax.readthedocs.io/en/latest/ with threshold \(10^{-3}\); see also [25].

### Additional numerical results

In Figure 6 we show the various projections at time \(t=3.7\) of the sample distribution corresponding to the nine-dimensional chaotic system [69].

In Figure 7 we show the particle histograms and the electric energy curves for the six-dimensional Vlasov-Poisson problem corresponding to strong Landau damping.

In Figure 8, for the linear oscillator example, we compare CoLoRA to two other modulation schemes: FiLM [66] and MLP. For the MLP the inputs \(x,t,\mu\) are concatenated together and input directly to the model. There is no hyper-network or modulation scheme. For FiLM, we closely follow the original paper. The main network takes \(x\) as input and the hyper-network \(t,\mu\) as input. The hyper-network and main network have the same parameter counts as in the CoLoRA experiments. The output of the hyper-network then directly modulates the activation of each layer of the main network as detailed in the original FiLM paper [66]. Figure 8 shows that parameterizing the vector field \(s_{t,\mu}\) with CoLoRA layers achieves the lowest mean Wasserstein distance, which motivates the use of the CoLoRA modulation scheme [13].

## Appendix C Calculations regarding the entropic loss

In the following, assume that \(\rho\in\mathcal{P}(\mathcal{X})\) is a smooth density bounded away from zero. We begin by showing some calculation rules of the operator \(-\Delta_{\rho}:s\mapsto-\nabla\cdot(\rho\nabla s)\) with

Figure 6: Shows the projections of other dimensions of the nine-dimensional chaotic system [69]; see also Figure 3.

Figure 7: Electric energy and solution field at time \(t=4\) for the 6 dimensional strong Landau example.

homogeneous Neumann boundary conditions. In its weak form, it reads

\[-\int_{\mathcal{X}}f\Delta_{\rho}s\mathrm{d}x=\int_{\mathcal{X}}\nabla f\cdot \nabla s\,\rho\mathrm{d}x\quad\forall f\in\mathcal{C}^{\infty}(\mathcal{X}).\] (27)

With the choice \(f=\log\rho\), we find the useful identity \(\Delta_{\rho}\log\rho=\Delta\rho\). Next, recall the objective \(E^{\epsilon}\) from (5):

\[E^{\varepsilon}(s)=\frac{1}{2}\int_{\mathcal{X}}\left|\nabla\left(s-\frac{ \varepsilon^{2}}{2}\log\rho\right)\right|^{2}\rho\mathrm{d}x-\int_{\mathcal{X }}\partial_{t}\rho s\mathrm{d}x.\]

Now denote by \(\delta s\) an arbitrary element of \(\mathcal{C}^{\infty}(\mathcal{X})\). Then, if \(s^{\epsilon}\) is a minimizer of the (strictly convex) objective, it holds that

\[0=\frac{!}{\mathrm{d}\tau}E^{\epsilon}(s^{\epsilon}+\tau\delta s)\bigg{|}_{\tau =0}=-\int_{\mathcal{X}}\delta s\Delta_{\rho}\left(s^{\epsilon}-\frac{ \varepsilon^{2}}{2}\log\rho\right)\mathrm{d}x-\int_{\mathcal{X}}\partial_{t} \rho\delta s\mathrm{d}x\quad\forall\delta s.\] (28)

Hence,

\[0=\Delta_{\rho}\left(s^{\epsilon}-\frac{\varepsilon^{2}}{2}\log\rho\right)+ \partial_{t}\rho=\nabla\cdot(\rho\nabla s^{\epsilon})-\frac{\varepsilon^{2}}{ 2}\Delta\rho+\partial_{t}\rho.\] (29)

Furthermore, note that (5) is identical to

\[E^{\epsilon}_{t,\mu}(s)=\int_{\mathcal{X}}\left(\frac{1}{2}|\nabla s|^{2}+ \frac{\epsilon^{2}}{2}\Delta s\right)\rho_{t,\mu}\mathrm{d}x-\int_{\mathcal{X }}\partial_{t}\rho_{t,\mu}s\mathrm{d}x+\frac{\epsilon^{2}}{8}\int_{\mathcal{X }}|\nabla\log\rho_{t,\mu}|^{2}\rho_{t,\mu}\mathrm{d}x\] (30)

after integration by parts. The last term is the Fisher information of the data at \(t,\mu\) and plays no role in the optimization.

## Appendix D Motivating the partial integration in time in the loss

Note that the problems from Equation (3) corresponding to different values of \(t\) are coupled through the term \(\partial_{t}\rho_{t,\mu}\). This is most apparent when one discretizes the equation in time. Denote by \(\{t_{i}\}_{i=0}^{n_{t}}\) a strictly increasing sequence with \(t_{0}=0,t_{n_{t}}=1\), and \(t_{i+1}-t_{i}=\delta t_{i}\). Then, for fixed but arbitrary \(\mu\), we obtain \(n_{t}\) coupled problems of the form

\[\min_{s_{t_{i}}\in H^{1}(\rho_{t_{i,\mu}},\mathcal{X})}\frac{1}{2}\int_{ \mathcal{X}}|\nabla s_{t_{i},\mu}|^{2}\rho_{t_{i,\mu}}\mathrm{d}x-\frac{1}{ \delta t}\int_{\mathcal{X}}(\rho_{t_{i+1},\mu}-\rho_{t_{i},\mu})s_{t_{i},\mu} \mathrm{d}x\quad\forall i,\mu.\] (31)

Adding these problems and shifting the indices, one can eliminate \(\rho_{t_{i+1},\mu}\), explicitly coupling \(s_{t_{i},\mu}\) and \(s_{t_{i+1},\mu}\). The continuous equivalent of this of course is an integration over \(t\), followed by an integration by parts.

Figure 8: Comparison of CoLoRA modulation scheme [13] versus FiLM [66] and MLP. CoLoRA layers achieve the lowest mean Wasserstein distance compared to FiLM and MLP. In particular, CoLoRA avoids outliers with larger errors.

Geometric picture of the optimization problem

We omit the dependence on the parameter \(\mu\) here for the sake of simpler notation and write \(\mathrm{d}\rho\) for \(\rho\,\mathrm{d}x\) for brevity. Note that the following considerations are purely formal. They are meant to illustrate a geometric picture of the optimization problems we consider. We claim no originality of these ideas; the exposition is based on Chapter 7 of [89] as well as [24, 55].

Otto calculusBased on the identification of the tangent space of \(P(\mathcal{X})\) with the space of gradients (more rigorously, at point \(\rho_{t}\in P(\mathcal{X})\), the closure of \(\{\nabla f:f\in\mathcal{C}^{\infty}(\mathcal{X})\}\) in \(L^{2}(\mathcal{X},\rho_{t},\mathbb{R}^{d})\), see Definition 8.4.1 in [5]), one can view \(\mathcal{P}(\mathcal{X})\) formally as a Riemannian manifold:

**Definition 1** ([63]).: _Let \(\tau\mapsto\rho_{\tau}^{1}\) and \(\tau\mapsto\rho_{\tau}^{2}\) be two curves valued in \(\mathcal{P}(\mathcal{X})\) for \(\tau\in(t-\epsilon,t+\epsilon)\) such that \(\rho_{\tau}^{1}\big{|}_{\tau=t}=\rho_{\tau}^{2}\big{|}_{\tau=t}=\rho_{t}\). The optimal transport metric on \(T\mathcal{P}(\mathcal{X})\) at \(\rho_{t}\in P(\mathcal{X})\) is given by_

\[g(\rho_{t})(\partial_{\tau}\rho_{\tau}^{1}\big{|}_{\tau=t}, \partial_{\tau}\rho_{\tau}^{2}\big{|}_{\tau=t})=\int_{\mathcal{X}}(\nabla s_{ t}^{1}\cdot\nabla s_{t}^{2})d\rho_{t}:\\ \partial_{\tau}\rho_{\tau}^{1}+\nabla\cdot(\rho_{t}\nabla s_{t}^ {1})=0,\partial_{\tau}\rho_{\tau}^{2}+\nabla\cdot(\rho_{t}\nabla s_{t}^{2})=0.\] (32)

This formalism is commonly named after the author of [63] and is closely linked to Arnold's considerations on geometric hydrodynamics [6]2 As both the identification of \(s_{t}\) from \(\partial_{t}\rho_{t}\) and the metric depend on \(\rho_{t}\), the geometry defined on \(\mathcal{P}(\mathcal{X})\) in this way is non-trivial.

Footnote 2: The derivation of fluid dynamics from variational principles is, of course, much older and goes back as far as Langrange’s Mécanique analytique published in 1789.

Action of a curveThe optimization Equation (3) has an appealing physical interpretation: The vector field we define as tangent to the curve is, among all compatible ones, the one with the smallest integrated kinetic energy. In analogy with the physical literature, we call \(\frac{1}{2}\int_{0}^{1}\int_{\mathcal{X}}|\nabla s_{t}|^{2}\mathrm{d}\rho_{t}\) the _action_ of the curve \(t\mapsto\rho_{t}\) with tangent velocity \(\nabla s_{t}\). We want to stress that while this procedure is reminiscent of physical action principles, in the latter a solution corresponds to a _stationary point_ given boundary conditions at the beginning and end of the curve. The problem we consider in Equation (6) is more narrow and concerned with finding \(\nabla s_{t}\) that matches a _given_ curve \(t\mapsto\rho_{t}\). Determining curves of minimal action in \(\mathcal{P}(\mathcal{X})\), leads to the Benamou-Brenier formula ([4], Proposition 2.30)):

\[\frac{1}{2}W_{2}^{2}(\rho_{0},\rho_{1})=\inf_{\rho,s}\left(\frac{1}{2}\int_{0} ^{1}\int_{\mathcal{X}}|\nabla s_{t}|^{2}d\rho_{t}\,dt:\partial_{t}\rho_{t}+ \nabla\cdot(\rho_{t}\nabla s_{t})=0,\rho_{t=0}=\rho_{0},\rho_{t=1}=\rho_{1} \right),\] (33)

with \(W_{2}\) the Wasserstein (or Kantorochiv-Rubinstein) distance.

Lagrangian functionsThe selection criterion based on kinetic energy alone is not without alternatives. In [24], the relation \(\partial_{t}\rho=-\Delta_{\rho}s\) is interpreted as a form of Legendre transform, hence \(s\) plays the role of a momentum and \(L(\rho_{t},\partial_{t}\rho_{t},t)=\int_{\mathcal{X}}|\nabla\Delta_{\rho}^{ \dagger}\partial_{t}\rho|^{2}\mathrm{d}\rho\) that of a Lagrangian. Here, we introduced the notation \(\Delta_{\rho}^{\dagger}\) to denote the pseudo inverse operator. Note that, formally, it is sensible to consider \(\partial_{t}\rho\) as an element of the tangent space of \(\mathcal{P}(\mathcal{X})\). After all, \(\rho+\tau\partial_{t}\rho\in\mathcal{P}(\mathcal{X})\) for \(\rho\) strictly positive and \(\tau\) small enough. In this picture, \(s\) is an element of the cotangent space. The introduction of [63] addresses the two concepts and how they relate.

Any function \(L:(\rho,\partial_{t}\rho,t)\mapsto L(\rho,\partial_{t}\rho,t)\), strictly convex and superlinear in its second argument, can be chosen to define the minimization objective.3 Details can be found in Chapter 7 of [89], which also features a comprehensive discussion of the history and applications of this problem. In recent years, this formulation has been applied for modeling purposes, e.g. in [44]. To give an example, the choice \(L(\rho,\partial_{t}\rho,t)=\frac{1}{2}\int_{\mathcal{X}}|\nabla\Delta_{\rho}^{ \dagger}\partial_{t}\rho|^{2}\mathrm{d}\rho-\int_{\mathcal{X}}Vd\rho\) for a potential \(V:\mathcal{X}\to\mathbb{R}\) can be used to model obstacles in the path of the samples.

There exist a number of partial differential equations whose solutions \(\rho_{t}\) can be described as curves of stationary action with respect to such Lagrangians, described in [3, 24], as well as [89], Chapter 23, and [90], Chapter 8.

Schrodinger BridgeThe objective defined in Equation (5) corresponds to the choice

\[L^{\epsilon}(\rho,\partial_{t}\rho,t):=\frac{1}{2}\int_{\mathcal{X}}\left| \nabla\left(-\Delta_{\rho}^{\dagger}\partial_{t}\rho+\frac{\epsilon^{2}}{2} \log\rho\right)\right|^{2}\mathrm{d}\rho.\] (34)

The associated momentum \(s^{\epsilon}\) therefore satisfies \(s^{\epsilon}=\frac{\delta L^{\epsilon}}{\delta(\partial_{t}\rho)}\), hence \(-\Delta_{\rho}s^{\epsilon}+\frac{\epsilon^{2}}{2}\Delta\rho=\partial_{t}\rho\), a Fokker-Planck equation. Furthermore, the action of the curve \(t\mapsto\rho_{t}\) is given by

\[\int_{0}^{1}L^{\epsilon}(\rho_{t},\partial_{t}\rho,t)\mathrm{d}t =\int_{0}^{1}\left(\frac{1}{2}\int_{\mathcal{X}}|\nabla\Delta_{\rho}^{\dagger} \partial_{t}\rho|^{2}\mathrm{d}\rho+\frac{\epsilon^{4}}{8}\int_{\mathcal{X}}| \nabla\log\rho_{t}|^{2}\,d\rho_{t}\right)\,dt\\ +\frac{\epsilon^{2}}{2}\left(\int_{\mathcal{X}}\log\rho_{t} \mathrm{d}\rho_{t}\bigg{|}_{t=1}-\int_{\mathcal{X}}\log\rho_{t}\mathrm{d}\rho _{t}\bigg{|}_{t=0}\right).\] (35)

This expression is known as the dual formulation of the Kantorovich-Schrodinger problem ([31], Theorem 36, except for the fact that the \(\epsilon\) therein corresponds to \(\epsilon^{2}/2\) here). While the classical optimal transport problem is concerned with the path connecting \(\rho_{0}\) and \(\rho_{1}\) minimizing the time integral of the kinetic energy (which coincides with the transport cost), the Schrodinger-Bridge problem is concerned with finding the most likely configuration at intermediate times, subject to the information that the configuration is given at times \(0\) and \(1\) and assuming that the particles \(X_{t}\) undergo Brownian motion with diffusivity \(\varepsilon^{2}/2\). Unless \(\rho_{1}\) is the result of a convolution of \(\rho_{0}\) with a Gaussian kernel of width \(\varepsilon\), the evolution of the system towards \(\rho_{1}\) is a rare event and the most likely solution is to be understood conditional on the observation of this event.

Rigorous results can be found in Section 5 of [31]. Another derivation of the loss function from Equation (5), starting from the static formulation and linking to the dynamical picture presented here, can also be found in [47], Theorem 2.1. In their notation, \(\Psi=-s\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Section 3. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Section 4. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes]

Justification: Section 2.1, Appendix C-E.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Section 3, Appendix A-B, code implementation link in Section 1 (retracted for review). Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Section 1 provides link to code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Section 3, Appendix A-B, code publication discussed in Section 1. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Figure 2 shows replicates, results reported in Figure 3-6, Table 1 are based on thousands of samples. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
* **Experiments Compute Resources*
* Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Table 1, Appendix B. Guidelines:
* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
* **Code Of Ethics*
* Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Only data from numerical simulations are used. We do not expect that this work has negative societal impacts. Guidelines:
* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
* **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: Section 4. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Appendix B. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.

* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Section 1 and Appendix B. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.