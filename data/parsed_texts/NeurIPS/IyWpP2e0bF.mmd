# Block Coordinate Plug-and-Play Methods

for Blind Inverse Problems

 Weijie Gan

Washington University in St. Louis

weijie.gan@wustl.edu

&Shirin Shoushtari

Washington University in St. Louis

s.shirin@wustl.edu

&Yuyang Hu

Washington University in St. Louis

h.yuyang@wustl.edu

&Jiaming Liu

Washington University in St. Louis

jiaming.liu@wustl.edu

&Hongyu An

Washington University in St. Louis

hongyuan@wustl.edu

&Ulugbek S. Kamilov

Washington University in St. Louis

kamilov@wustl.edu

###### Abstract

Plug-and-play (PnP) prior is a well-known class of methods for solving imaging inverse problems by computing fixed-points of operators combining physical measurement models and learned image denoisers. While PnP methods have been extensively used for image recovery with known measurement operators, there is little work on PnP for solving blind inverse problems. We address this gap by presenting a new block-coordinate PnP (BC-PnP) method that efficiently solves this joint estimation problem by introducing learned denoisers as priors on both the unknown image and the unknown measurement operator. We present a new convergence theory for BC-PnP compatible with blind inverse problems by considering _nonconvex_ data-fidelity terms and _expansive_ denoisers. Our theory analyzes the convergence of BC-PnP to a stationary point of an _implicit_ function associated with an _approximate_ minimum mean-squared error (MMSE) denoiser. We numerically validate our method on two blind inverse problems: automatic coil sensitivity estimation in magnetic resonance imaging (MRI) and blind image deblurring. Our results show that BC-PnP provides an efficient and principled framework for using denoisers as PnP priors for jointly estimating measurement operators and images.

## 1 Introduction

Many problems in computational imaging, biomedical imaging, and computer vision can be formulated as _inverse problems_ involving the recovery of high-quality images from low-quality observations. Imaging inverse problems are generally ill-posed, which means that multiple plausible clean images could lead to the same observation. It is thus common to introduce prior models on the desired images. While the literature on prior modeling of images is vast, current methods are often based on _deep learning (DL)_, where a deep model is trained to map observations to images [1, 2, 3].

_Plug-and-play (PnP) priors_[4, 5] is one of the most widely-used DL frameworks for solving imaging inverse problems. PnP methods circumvent the need to explicitly describe the full probability density of images by specifying image priors using image denoisers. The integration of state-of-the-art deep denoisers with physical measurement models within PnP has been shown to be effective in a number of inverse problems, including image super-resolution, phase retrieval, microscopy, andmedical imaging [6, 7, 8, 9, 10, 11, 12, 13] (see also recent reviews [14, 15]). Practical success of PnP has also motivated novel extensions, theoretical analyses, statistical interpretations, as well as connections to related approaches such as score matching and diffusion models [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29].

Despite the rich literature on PnP, the existing work on the topic has primarily focused on the problem of image recovery where the measurement operator is known exactly. There is little work on PnP for _blind_ inverse problems, where both the image and the measurement operator are unknown. This form of blind inverse problems are ubiquitous in computational imaging with well-known applications such as blind deblurring [30] and parallel magnetic resonance imaging (MRI) [31]. In this paper, we address this gap by developing a new PnP approach that uses denoisers as priors over both the unknown measurement model and the unknown image, and efficiently solves the joint estimation task as a _block-coordinate PnP (BC-PnP)_ method. While a variant of BC-PnP was proposed in the recent paper [21], it was never used for jointly estimating the images and the measurement operators. Additionally, the convergence theory in [21] is inadequate for blind inverse problems since it assumes convex data-fidelity terms and nonexpansive denoisers. We present a new convergence analysis applicable to _nonconvex_ data-fidelity terms and _expansive_ denoisers. Our theoretical analysis provides explicit error bounds on the convergence of BC-PnP for _approximate_ minimum mean squared error (MMSE) denoisers under a set of clearly specified assumptions. We show the practical relevance of BC-PnP by solving joint estimation problems in blind deblurring and accelerated parallel MRI. Our numerical results show the potential of denoisers to act as PnP priors over the measurement operators as well as images. Our work thus addresses a gap in the current PnP literature by providing a new efficient and principled framework applicable to a wide variety of blind imaging inverse problems.

All proofs and some details that have been omitted for space appear in the supplementary material.

## 2 Background

**Inverse Problems.** Many imaging problems can be formulated as inverse problems where the goal is to estimate an unknown image \(\bm{x}\in\mathbb{R}^{n}\) from its degraded observation \(\bm{y}=\bm{A}\bm{x}+\bm{e}\), where \(\bm{A}\in\mathbb{R}^{m\times n}\) is a measurement operator and \(\bm{e}\in\mathbb{R}^{m}\) is the noise. A common approach for solving inverse problems is based on formulating an optimization problem

\[\widehat{\bm{x}}\in\underset{\bm{x}\in\mathbb{R}^{n}}{\text{arg min}}\,f(\bm{x})\quad\text{with}\quad f(\bm{x})=g(\bm{x})+h(\bm{x})\;,\] (1)

where \(g\) is the data-fidelity term that quantifies consistency with the observation \(\bm{y}\) and \(h\) is the regularizer that infuses a prior on \(\bm{x}\). For example, a widely-used data-fidelity term and regularizer in computational imaging are the least-squares \(g(\bm{x})=\frac{1}{2}\left\|\bm{A}\bm{x}-\bm{y}\right\|_{2}^{2}\) and the total variation (TV) functions \(h(\bm{x})=\tau\left\|\bm{D}\bm{x}\right\|_{1}\), where \(\bm{D}\) is the image gradient, and \(\tau>0\) a regularization parameter.

The traditional inverse problem formulations assume that the measurement operator \(\bm{A}\) is known exactly. However, in many applications, it is more practical to model the measurement operator as \(\bm{A}(\bm{\theta})\), where \(\bm{\theta}\in\mathbb{R}^{p}\) are unknown parameters to be estimated jointly with \(\bm{x}\). This form of inverse problems are often referred to as _blind_ inverse problems and arise in a wide-variety of applications, including prallell MRI [32, 33, 34, 35, 36, 37], blind deblurring [38, 39, 30], and computed tomography [40, 41, 42, 43].

**DL.** There is a growing interest in DL for solving imaging inverse problems [1, 2, 3]. Instead of explicitly defining a regularizer, DL approaches for solving inverse problems learn a mapping from the measurements to the desired image by training a convolutional neural network (CNN) to perform regularized inversion [44, 45, 46, 47, 48]. Model-based DL (MBDL) has emerged as powerful DL framework for inverse problems that combines the knowledge of the measurement operator with an image prior specified by a CNN (see reviews [3, 49]). The literature of MBDL is vast, but some well-known examples include PnP, regularization by denoising (RED), deep unfolding (DU), compressed sensing using generative models (CSGM), and deep equilibrium models (DEQ) [50, 51, 52, 53, 54]. All these approaches come with different trade-offs in terms of imaging performance, computational and memory complexity, flexibility, need for supervision, and theoretical understanding.

The literature on DL approaches for blind inverse problems is broad, with many specialized methods developed for different applications. While an in-depth review would be impractical for this paper, we mention several representative approaches adopted in prior work. The direct application of DL to predict the measurement operator from the observation was explored in [55, 56]. Deep image prior (DIP) was used as a learning-free prior to regularize the image and the measurement operator in [57, 58]. Generative models, including both GANs and diffusion models, have been explored as regularizers for blind inverse problems in [59, 60]. Other work considered the use of a dedicated neural network to predict the parameters of the measurement operator, adoption of model adaptation strategies, and development of autocalibration methods based on optimization [61, 62, 63, 34, 35, 64].

**PnP**. PnP [4, 5] is one of the most popular MBDL approaches based on using deep denoisers as imaging priors (see also recent reviews [14, 15]). For example, the proximal gradient method variant of PnP (referred to as PnP-ISTA in this paper) can be formulated as a fixed-point iteration [65]

\[\bm{x}^{k}\leftarrow\mathsf{D}_{\sigma}\left(\bm{z}^{k}\right)\quad\text{with} \quad\bm{z}^{k}\leftarrow\bm{x}^{k-1}-\gamma\nabla g(\bm{x}^{k-1})\;,\] (2)

where \(\mathsf{D}_{\sigma}\) is a denoiser with a parameter \(\sigma>0\) for controlling its strength and \(\gamma>0\) is a step-size. The theoretical convergence of PnP-ISTA has been explored for convex functions \(g\) using monotone operator theory [20, 22] as well as for nonconvex functions based on interpreting the denoiser as a MMSE estimator [23]. The analysis in this paper builds on the convergence theory in [23] that uses an elegant formulation by Gribonval [66] establishing a direct link between MMSE estimation and regularized inversion. Many variants of PnP have been developed over the past few years [6, 7, 8, 9, 10, 11, 12], which has motivated an extensive research on its theoretical properties [16, 18, 20, 22, 23, 27, 28, 29, 67, 29, 70].

_Block coordinate regularization by denoising (BC-RED)_ is a recent PnP variant for solving large-scale inverse problems by updating along a subset of coordinates at every iteration [21]. BC-RED is based on regularization by denoising (RED), another well-known variant of PnP that seeks to formulate an explicit regularizer for a given image denoiser [17, 19]. BC-RED was applied to several non-blind inverse problems and was theoretically analyzed for convex data-fidelity terms.

PnP was extended to blind deblurring in [71, 72] by considering an additional prior on blur kernels that promotes sparse and nonegative solutions. PnP was also applied to holography with unknown phase errors by using the Gaussian Markov random field model as the prior for the phase errors [73]. _Calibrated RED (Cal-RED)_[43] is a recent related extension of RED that calibrates the measurement operator during RED reconstruction by combining the traditional RED updates over an unknown image with a gradient descent over the unknown parameters of the measurement operator. However, this prior work does not leverage any learned priors for the measurement operator and does not provide any theoretical analysis.

**Our contributions.**_(1)_ Our first contribution is in the use of learned deep denoisers for regularizing the measurement operators within PnP. While the idea of calibration within PnP was introduced in [43], denoisers were not used as priors for measurement operators. **(2)** Our second contribution is the application of BC-PnP as an efficient method for jointly estimating the unknown image and the measurement operator. While BC-RED was introduced in [21] as a block-coordinate variant of PnP, the method was used for solving non-blind inverse problems by using patch-based image denoisers. **(3)** Our third contribution is a new convergence theory for BC-PnP for the _sequential_ and _random_ block-selection strategies under approximate MMSE denoisers. Our analysis does _not_ assume convex data-fidelity terms, which makes it compatible with blind inverse problems. Our analysis can be seen as an extension of [23] to block-coordinate updates and approximate MMSE denoisers. **(4)** Our fourth contribution is the implementation of BC-PnP using learned deep denoisers as priors for two distinct blind inverse problems: blind deblurring and auto-calibrated parallel MRI. Our code--which we share publicly--shows the potential of learning deep denoisers over measurement operators and using them for jointly estimating the uknown image and the uknown measurement operator.

## 3 Block Coordinate Plug-and-Play Method

We propose to efficiently solve blind inverse problems by using a block-coordinate PnP method, where each block represents one group of unknown variables (images, measurement operators, etc). The novelty of our work relative to [21] is in solving blind inverse problems by introducing learned priors on both the unknown image and the uknown measurement operator. Additionally, unlike [21], our work proposes a fully nonconvex formulation that is more applicable to blind inverse problems.

Consider the decomposition of a vector \(\bm{x}\in\mathbb{R}^{n}\) into \(b\geq 1\) blocks

\[\bm{x}=(\bm{x}_{1},\cdots,\bm{x}_{b})\in\mathbb{R}^{n_{1}}\times\cdots\times \mathbb{R}^{n_{b}}\quad\text{with}\quad n=n_{1}+\cdots+n_{b}.\] (3)For each \(i\in\{1,\cdots,b\}\), we define a matrix \(\mathbf{U}_{i}\in\mathbb{R}^{n\times n_{i}}\) that injects a vector in \(\mathbb{R}^{n_{i}}\) into \(\mathbb{R}^{n}\) and its transpose \(\mathbf{U}_{i}^{\mathsf{T}}\) that extracts the \(i\)th block from a vector in \(\mathbb{R}^{n}\). For any \(\bm{x}\in\mathbb{R}^{n}\), we have

\[\bm{x}=\sum_{i=1}^{b}\mathbf{U}_{i}\bm{x}_{i}\quad\text{with}\quad\bm{x}_{i}= \mathbf{U}_{i}^{\mathsf{T}}\bm{x}\in\mathbb{R}^{n_{i}},\quad i=1,\cdots,b\quad \Leftrightarrow\quad\sum_{i=1}^{b}\mathbf{U}_{i}\mathbf{U}_{i}^{\mathsf{T}}= \mathbf{I}.\] (4)

Note that (4) directly implies the norm preservation \(\|\bm{x}\|_{2}^{2}=\|\bm{x}_{1}\|_{2}^{2}+\cdots+\|\bm{x}_{b}\|_{2}^{2}\) for any \(\bm{x}\in\mathbb{R}^{n}\). We are interested in a block-coordinate algorithm that uses only a subset of operator outputs corresponding to coordinates in some block \(i\in\{1,\cdots,b\}\). Hence, for an operator \(\mathsf{G}:\mathbb{R}^{n}\to\mathbb{R}^{n}\), we define the block-coordinate operator \(\mathsf{G}_{i}:\mathbb{R}^{n}\to\mathbb{R}^{n_{i}}\) as

\[\mathsf{G}_{i}(\bm{x})\,\coloneqq\,[\mathsf{G}(\bm{x})]_{i}=\mathbf{U}_{i}^{ \mathsf{T}}\mathsf{G}(\bm{x})\in\mathbb{R}^{n_{i}},\quad\bm{x}\in\mathbb{R}^{ n}.\] (5)

We are in-particular interested in two operators: (a) the gradient \(\nabla g(\bm{x})=(\nabla_{1}g(\bm{x}),\cdots,\nabla_{b}g(\bm{x}))\) of the data-fidelity term \(g\) and (b) the denoiser \(\mathsf{D}_{\bm{\sigma}}(\bm{x})=(\mathsf{D}_{\sigma_{1}}(\bm{x}_{1}),\cdots, \mathsf{D}_{\sigma_{b}}(\bm{x}_{b}))\), where the vector \(\bm{\sigma}=(\sigma_{1},\cdots,\sigma_{b})\in\mathbb{R}^{b}_{+}\) consists of parameters for controlling the strength of each block denoiser. Note how the denoiser acts in a separable fashion across different blocks.

When \(b=1\), we have \(\mathbf{U}_{1}=\mathbf{U}_{1}^{\mathsf{T}}=\mathbf{I}\) and BC-PnP reduces to the conventional PnP-ISTA [23, 65]. When \(b>1\), we have at least two blocks with BC-PnP updating only one block at a time

\[\bm{x}_{j}^{k}=\begin{cases}\bm{x}_{j}^{k-1}&\text{ when }j\neq i_{k}\\ \mathsf{D}_{\sigma_{j}}(\bm{x}_{j}^{k-1}-\gamma\nabla_{j}g(\bm{x}^{k-1}))& \text{ when }j=i_{k}\end{cases},\quad j\in\{1,\cdots,b\}.\] (6)

As with any coordinate descent method (see [74] for a review), BC-PnP can be implemented using different block selection strategies. One common strategy is to simply update blocks sequentially as \(i_{k}=1+\mathsf{mod}(k-1,b)\), where \(\mathsf{mod}(\cdot)\) denotes the modulo operator. An alternative is to proceed in epochs of \(b\) consecutive iterations, where at the start of each epoch the set \(\{1,\cdots,b\}\) is reshuffled, and \(i_{k}\) is then selected consecutively from this ordered set. Finally, one can adopt a fully randomized strategy where indices \(i_{k}\) are selected as i.i.d. random variables distributed uniformly over \(\{1,\cdots,b\}\).

Throughout this work, we will assume that each denoiser \(\mathsf{D}_{\sigma_{i}}\) is an _approximate_ MMSE estimator for the following AWGN denoising problem

\[\bm{z}_{i}=\bm{x}_{i}+\bm{n}_{i}\quad\text{with}\quad\bm{x}_{i}\sim p_{\bm{x} _{i}},\quad\bm{n}_{i}\sim\mathcal{N}(0,\sigma_{i}^{2}\mathbf{I}),\] (7)

where \(i\in\{1,\cdots,b\}\) and \(\bm{z}_{i}\in\mathbb{R}^{n_{i}}\). We rely only on an _approximation_ of the MMSE estimator of \(\bm{x}_{i}\) given \(\bm{z}_{i}\), since the _exact_ MMSE denoiser corresponds to the generally intractable posterior mean

\[\mathsf{D}_{\sigma_{i}}^{*}(\bm{z}_{i})\,\coloneqq\,\mathbb{E}[\bm{x}_{i}|\bm {z}_{i}]=\int_{\mathbb{R}^{n_{i}}}\bm{x}p_{\bm{x}_{i}|\bm{z}_{i}}(\bm{x}|\bm{z }_{i})\,\mathrm{d}\bm{x}.\] (8)

Approximate MMSE denoisers are a useful model for denoisers due to the use of the MSE loss

\[\mathcal{L}(\mathsf{D}_{\sigma_{i}})=\mathbb{E}\left[\|\bm{x}_{i}-\mathsf{D}_ {\sigma_{i}}(\bm{z}_{i})\|_{2}^{2}\right]\] (9)

for training deep denoisers, as well as the optimality of MMSE denoisers with respect to widely used image-quality metrics such as signal-to-noise ratio (SNR).

As a simple illustration of the generality of BC-PnP, consider \(b=2\) with the least-squares objective

\[g(\bm{x})=\frac{1}{2}\|\bm{y}-\bm{A}(\bm{\theta})\bm{v}\|_{2}^{2}\quad\text{ with}\quad\bm{x}\,\coloneqq\,(\bm{v},\bm{\theta}),\] (10)where \(\bm{v}\in\mathbb{R}^{n_{i}}\) denotes the unknown image and \(\bm{\theta}\in\mathbb{R}^{n_{2}}\) denotes the unknown parameters of the measurement operator. BC-PnP can then be implemented by first pre-training a dedicated AWGN denoiser \(\mathsf{D}_{\sigma_{i}}\) for each block \(i\) and using it as a prior within Algorithm 1. It is also worth noting that the functions \(g\) in (10) is nonconvex with respect to the variable \(\bm{x}\in\mathbb{R}^{n}\). In the next section, we present the full convergence analysis of BC-PnP without any convexity assumptions on \(g\) and nonexpansiveness assumptions on the denoiser \(\mathsf{D}_{\bm{\sigma}}\).

## 4 Convergence Analysis of BC-PnP

In this section, we present two new theoretical convergence results for BC-PnP. We first discuss its convergence under the sequential updates and then under fully random updates. It is worth mentioning that BC-RED with fully random updates was theoretically analyzed in [21]. The novelty of our analysis here lies in that it allows for nonconvex functions \(g\) and expansive denoisers \(\mathsf{D}_{\sigma_{i}}\). The nonconvexity of \(g\) is essential since most data-fidelity terms used for blind inverse problems are nonconvex. On the other hand, by allowing expansive \(\mathsf{D}_{\sigma_{i}}\), our analysis avoids the need for the spectral normalization techniques that were previously suggested for PnP methods [21; 22].

In the following, we will denote as \(\mathsf{D}_{\bm{\sigma}}^{*}\coloneqq(\mathsf{D}_{\sigma_{1}}^{*},\cdots, \mathsf{D}_{\sigma_{b}}^{*})\) the exact MMSE denoiser in (8). Our analysis will require five assumptions that will serve as sufficient conditions for our theorems.

**Assumption 1**.: _The blocks \(\bm{x}_{i}\) are independent with non-degenerate priors \(p_{\bm{x}_{i}}\) over \(\mathbb{R}^{n_{i}}\)._

As a reminder, a probability distribution \(p_{\bm{x}_{i}}\) is _degenerate_ over \(\mathbb{R}^{n_{i}}\), if it is supported on a space of lower dimensions than \(n_{i}\). Assumption 1 is required for establishing an explicit link between the MMSE denoiser (8) and the following regularizer (see also [23; 66] for additional background)

\[h(\bm{x})=\sum_{i=1}^{b}h_{i}(\bm{x}_{i}),\quad\bm{x}=(\bm{x}_{1},\cdots,\bm{ x}_{n})\in\mathbb{R}^{n},\] (11)

where each function \(h_{i}\) is defined as (see the derivation in Section D.2 of the supplement)

\[h_{i}(\bm{x}_{i})\,\coloneqq\,\begin{cases}-\frac{1}{2\gamma}\|\bm{x}_{i}-( \mathsf{D}_{\sigma_{i}}^{*})^{-1}(\bm{x}_{i})\|_{2}^{2}+\frac{\sigma_{i}^{2}}{ \gamma}h_{\sigma_{i}}((\mathsf{D}_{\sigma_{i}}^{*})^{-1}(\bm{x}_{i}))&\text{for }\bm{x}_{i}\in\mathsf{Im}(\mathsf{D}_{ \sigma_{i}}^{*})\\ +\infty&\text{for }\bm{x}_{i}\notin\mathsf{Im}(\mathsf{D}_{\sigma_{i}}^{*}), \end{cases}\] (12)

where \(\gamma>0\) is the step size, \((\mathsf{D}_{\sigma_{i}}^{*})^{-1}:\mathsf{Im}(\mathsf{D}_{\sigma_{i}}^{*}) \to\mathbb{R}^{n_{i}}\) is the inverse mapping, which is well defined and smooth over \(\mathsf{Im}(\mathsf{D}_{\sigma_{i}}^{*})\), and \(h_{\sigma_{i}}(\cdot)\,\coloneqq\,-\log(p_{\bm{x}_{i}}(\cdot))\), where \(p_{\bm{x}_{i}}\) is the probability distribution over the AWGN corrupted observations (7). Note that the function \(h_{i}\) is smooth for any \(\bm{x}_{i}\in\mathsf{Im}(\mathsf{D}_{\sigma_{i}}^{*})\), which is the consequence of the smoothness of both \((\mathsf{D}_{\sigma_{i}}^{*})^{-1}\) and \(h_{\sigma_{i}}\).

**Assumption 2**.: _The function \(g\) is continuously differentiable and \(\nabla g\) is Lipschitz continuous with constant \(L>0\). Additionally, each block gradient \(\nabla_{i}g\) is block Lipschitz continuous with constant \(L_{i}>0\). We define the largest block Lipschitz constant as \(L_{\mathsf{max}}\,\coloneqq\,\mathsf{max}\{L_{1},\cdots,L_{b}\}\)._

Lipschitz continuity of the gradient \(\nabla g\) is a standard assumption in the context of imaging inverse problems. Note that we always have the relationship \((L/b)\leq L_{\mathsf{max}}\leq L\) (see Section 3.2 in [74]).

**Assumption 3**.: _The explicit data-fidelity term and the implicit regularizer are bounded from below_

\[\inf_{\bm{x}\in\mathbb{R}^{n}}g(\bm{x})>-\infty,\quad\inf_{\bm{x}\in\mathbb{R} ^{n}}h(\bm{x})>-\infty.\] (13)

Assumption 3 implies that there exists \(f^{*}>-\infty\) such that \(f(\bm{x})\geq f^{*}\) for all \(\bm{x}\in\mathbb{R}^{n}\). Since Assumptions 1-3 correspond to the standard assumptions used in the literature, they are broadly satisfied in the context of inverse problems.

Our analysis assumes that at every iteration, BC-PnP uses inexact MMSE denoisers on each block. While there are several ways to specify the nature of "inexactness," we consider the case where at every iteration \(k\) of BC-PnP the distance of the output of \(\mathsf{D}_{\sigma_{i}}\) to \(\mathsf{D}_{\sigma_{i}}^{*}\) is bounded by a constant \(\varepsilon_{k}\).

**Assumption 4**.: _Each block denoiser \(\mathsf{D}_{\sigma_{i}}\) in \(\mathsf{D}_{\bm{\sigma}}\) satisfies_

\[\|\mathsf{D}_{\sigma_{i}}(\bm{z}_{i}^{k})-\mathsf{D}_{\sigma_{i}}^{*}(\bm{z}_{i} ^{k})\|_{2}\leq\varepsilon_{k},\quad i\in\{1,\cdots,b\},\quad k=1,2,3,\cdots,\]

_where \(\mathsf{D}_{\sigma_{i}}^{*}\) is given in (8) and \(\bm{z}_{i}^{k}=\bm{x}_{i}^{k-1}-\gamma\nabla_{i}g(\bm{x}^{k-1})\)._For convenience, we will define quantities \(\varepsilon^{2}\,\coloneqq\,\mathsf{max}\{\varepsilon_{1}^{2},\varepsilon_{2}^{2}, \cdots\}\) and \(\overline{\varepsilon}_{i}^{2}\,\coloneqq\,(1/t)\left(\varepsilon_{1}^{2}+\cdots+ \varepsilon_{t}^{2}\right)\) that correspond to the largest and the mean squared-distances between the inexact and exact denoisers. Assumption 4 states that the error of the approximate MMSE denoiser used for inference is bounded relative to the exact MMSE denoiser, which is reasonable when the approximate MMSE denoiser is a CNN trained to minimize the MSE.

It has been shown in the prior work [23, 66] that the function \(h\) in (11) is infinitely continuously differentiable over \(\mathsf{Im}(\mathsf{D}_{\bm{\sigma}}^{*})\). Our analysis requires the extension of the region where \(h\) is smooth to include the range of the approximate MMSE denoiser, which is the goal of our next assumption.

**Assumption 5**.: _Each regularizer \(h_{i}\) in (12) associated with the MMSE denoiser (8) is continously differentiable and has a Lipschitz continuous gradient with constant \(M_{i}>0\) over the set_

\[\mathsf{Im}_{\varepsilon}(\mathsf{D}_{\sigma_{i}}^{*})\,\coloneqq\,\{\bm{x} \in\mathbb{R}^{n_{i}}:\|\bm{x}-\mathsf{D}_{\sigma_{i}}^{*}(\bm{z})\|_{2}\leq \varepsilon,\,\bm{z}\in\mathbb{R}^{n_{i}}\},\quad i\in\{1,\cdots,b\}.\]

We will define \(M_{\mathsf{max}}\,\coloneqq\,\mathsf{max}\{M_{1},\cdots,M_{b}\}\) to be the largest Lipschitz constant and \(\mathsf{Im}_{\varepsilon}(\mathsf{D}_{\bm{\sigma}}^{*})\,\coloneqq\,\{\bm{x} \in\mathbb{R}^{n}:\bm{x}_{i}\in\mathsf{Im}_{\varepsilon}(\mathsf{D}_{\sigma_{ i}}^{*}),\,i\in\{1,\ldots,b\}\}\) to be the set over which \(h\) is smooth. Assumption 5 expands the region where the regularizer associated with the exact MMSE denoiser is smooth by including the range of the approximate MMSE denoiser. For example, this assumption is automatically true when the exact and approximate MMSE denoisers have the same range, which is reasonable when the approximate MMSE denoiser is trained to imitate the exact one.

Our first theoretical result considers the sequential updates, where at each iteration, \(i_{k}\) is selected as \(i_{k}=1+\mathsf{mod}(k-1,b)\) with \(\mathsf{mod}(\cdot)\) being the modulo operator. We can then express any iterate \(ib\) produced by BC-PnP for \(i\geq 1\) as

\[\bm{x}^{ib}=(\bm{x}_{1}^{ib},\cdots,\bm{x}_{b}^{ib})=(\bm{x}_{1}^{(i-1)b+1}, \cdots,\bm{x}_{b}^{ib}).\]

Note that \(\bm{x}^{ib}\in\mathsf{Im}_{\varepsilon}(\mathsf{D}_{\bm{\sigma}}^{*})\) since each block is an output of the denoiser. We prove the following result.

**Theorem 1**.: _Run BC-PnP under Assumptions 1-5 using the sequential block selection and the step \(0<\gamma<1/L_{\mathsf{max}}\). Then, we have_

\[\underset{1\leq i\leq t}{\text{min}}\,\|\nabla f(\bm{x}^{ib})\|_{2}^{2}\leq \frac{1}{t}\sum_{i=1}^{t}\|\nabla f(\bm{x}^{ib})\|_{2}^{2}\leq\frac{C_{1}}{t} (f(\bm{x}^{0})-f^{*})+C_{2}\overline{\varepsilon}_{tb}^{2},\]

_where \(C_{1}>0\) and \(C_{2}>0\) are iteration independent constants. If additionally the sequence of error terms \(\{\varepsilon_{i}\}_{i\geq 1}\) is square-summable, we have that \(\nabla f(\bm{x}^{tb})\to\bm{0}\) as \(t\to 0\)._

Our second theoretical result considers fully random updates, where at each iteration, \(i_{k}\) is selected as i.i.d. random variables distributed over \(\{1,\cdots,b\}\). In this setting, we analyze the convergence of BC-PnP in terms of the sequence \(\{\mathsf{G}(\bm{x}^{k})\}_{k\geq 0}\). Note that it is straightforward to verify that \(\mathsf{Zer}(\mathsf{G})=\mathsf{Zer}(\nabla f)\), which makes this analysis meaningful. We prove the following result.

**Theorem 2**.: _Run BC-PnP under Assumptions 1-5 using the random i.i.d. block selection and the step \(0<\gamma<1/L_{\mathsf{max}}\). Then, we have_

\[\underset{1\leq k\leq t}{\text{min}}\,\mathbb{E}\left[\|\mathsf{G}(\bm{x}^{k-1} )\|_{2}^{2}\right]\leq\mathbb{E}\left[\frac{1}{t}\sum_{k=1}^{t}\|\mathsf{G}( \bm{x}^{k-1})\|_{2}^{2}\right]\leq\frac{D_{1}}{t}(f(\bm{x}^{0})-f^{*})+D_{2} \overline{\varepsilon}_{t}^{2},\]

_where \(D_{1}>0\) and \(D_{2}>0\) are iteration independent constants. If additionally the sequence of error terms \(\{\varepsilon_{i}\}_{i\geq 1}\) is square-summable, we have that \(\mathsf{G}(\bm{x}^{t})\overset{\mathsf{a.s.}}{\longrightarrow}\bm{0}\) as \(t\to\infty\)._

The expressions for the constants in Theorems 1 and 2 are given in the proofs. The theorems show that if the sequence of approximation errors is square-summable, BC-PnP asymptotically achieves a stationary point of \(f\). On the other hand, if the sequence of approximation errors is not square-summable, the convergence is only up to an error term that depends on the average of the squared approximation errors. Both theorems can thus be viewed as more flexible alternatives for the convergence analysis in [21]. It is also worth mentioning that the theorems are interesting even when the denoiser errors are not square-summable, since they provide explicit error bounds on convergence. While the analysis in [21] assumes convex \(g\) and nonexpansive \(\mathsf{D}_{\bm{\sigma}}\), the analysis here does not require these two assumptions. It instead views the denoiser \(\mathsf{D}_{\bm{\sigma}}\) as an _approximation_ to the MMSE estimator \(\mathsf{D}_{\bm{\sigma}}^{*}\), where the approximation error is bounded by \(\varepsilon_{k}\) at every iteration of BC-PnP. This view is compatible with denoisers trained to minimize the MSE loss (9).

## 5 Numerical Validation

We numerically validate BC-PnP on two blind inverse problems: (a) _compressed sensing parallel MRI (CS-PARI)_ with automatic coil sensitivity map (CSM) estimation and (b) _blind image deblurring_. We adopt the traditional \(\ell_{2}\)-norm loss in (10) as the data-fidelity term for both problems. We will use \(\bm{x}\) to denote the unknown image and \(\bm{\theta}\) to denote the unknown parameters of the measurement operator. We use the relative root mean squared error (RMSE) and structural similarity index (SSIM) as quantitative metrics to evaluate the performance.

We experimented with several ablated variants of BC-PnP, including PnP, PnP-GD\({}_{\bm{\theta}}\), and PnP-oracle\({}_{\bm{\theta}}\). PnP and PnP-oracle\({}_{\bm{\theta}}\) denote basic variants of PnP that use pre-estimated and ground truth measurement operators, respectively. PnP-GD\({}_{\bm{\theta}}\) is a variant of PnP based on [43], where \(\bm{\theta}\) is estimated without any DL prior. It is worth noting that PnP-oracle\({}_{\bm{\theta}}\) is provided as an idealized reference in our experiment. As discussed in the following subsections, we also compare BC-PnP against several widely-used baseline methods specific to CS-PMRI and blind image deblurring.

### Compressed Sensing Parallel MRI

The measurement operator of CS-PMRI consists of complex measurement operators \(\bm{A}(\bm{\theta})\in\mathbb{C}^{m\times n}\) that depend on unknown CSMs \(\{\bm{\theta}_{i}\}\) in \(\mathbb{C}^{n}\). Each sub-measurement operator can be parameterized as \(\bm{A}_{i}(\bm{\theta}_{i})=\bm{P}\bm{F}\mathsf{diag}(\bm{\theta}_{i})\), where \(\bm{F}\) is the Fourier transform, \(\bm{P}\in\mathbb{R}^{m\times n}\) is the sampling operator, and \(\mathsf{diag}(\bm{\theta}_{i})\) forms a matrix by placing \(\bm{\theta}_{i}\) on its diagonal. We used T2-weighted MR brain acquisitions of 165 subjects obtained from the validation set of the fastMRI dataset [75] as the the fully sampled measurement for simulating measurements. We obtained reference \(\bm{\theta}_{i}\) from the fully sampled measurements using ESPIRiT [76]. These 165 subjects were split into 145, 10, and 10 for training, validation, and testing, respectively. BC-PnP and baseline methods were tested on 10 2D slices, randomly selected from the testing subjects. We followed [75] to retrospectively undersample the fully sampled data using 1D Cartesian equispaced sampling masks with 10% auto-calibration signal (ACS) [76] lines. We conducted our experiments for acceleration factors \(R=6\) and \(8\). We adopted DRUNet [12] as the architectures of \(\mathsf{D}_{\bm{\sigma}}\) for training both the image and CSM denoisers. BC-PnP and its ablated variants are initialized using CSMs \(\bm{\theta}_{0}\) pre-estimated using ESPIRiT [76] and images \(\bm{x}_{0}\leftarrow\bm{A}(\bm{\theta}_{0})^{\mathsf{H}}\bm{y}\), where \(\bm{A}^{\mathsf{H}}\) denotes the Hermitian transpose of \(\bm{A}\).

We considered several baseline methods, including ENLIVE [35], ESPIRiT-TV [76], Unet [77], and ISTANet+ [51]. ENLIVE is an iterative algorithm that jointly estimates images and coil sensitivity profiles. ESPIRiT-TV is an iterative algorithm that applies TV reconstruction method in (1). Unet is trained to map raw measurements to desired ground truth without the knowledge of measurement operator. ISTANet+ denotes a widely-used DU architecture. We tested ESPIRiT-TV and ISTANet+ using CSMs pre-estimated using ESPIRiT.

Figure 1 illustrates the convergence behaviour of BC-PnP on the test set for the acceleration factor \(R=8\). Figure 2 illustrates reconstruction results for the acceleration factor \(R=6\). Table 1 summarizes the quantitative evaluation of BC-PnP relative to other PnP variants and the baseline methods. These results show that joint estimation can lead to significant improvements and that BC-PnP can perform as well as the idealized PnP-oracle\({}_{\bm{\theta}}\) that knowns the true measurement operator.

Figure 1: _Illustration of the BC-PnP convergence using the sequential and random i.i.d. block selection rules on CS-PMRI with the sampling factor \(R=8\). Leftmost two plots: Evolution of the distance between two consecutive image and CSM iterates. Rightmost three plots: Evolution of the RMSE and SSIM metrics relative to the true solutions across BC-PnP iterations. Note how both block selection rules lead to a nearly identical convergence behaviour of BC-PnP in this experiment._

### Blind Image Deblurring

The measurement operator in blind image deblurring can be modeled as \(\bm{A}(\bm{\theta})\bm{x}=\bm{\theta}*\bm{x}\), where \(\bm{\theta}\) is the unknown blur kernel, \(\bm{x}\) is the unknown image, and \(*\) is the convolution. We randomly selected 10 testing ground truth image from CBSD68 [80] dataset. We generated \(25\times 25\) Gaussian kernels with \(\sigma=10^{1}\). We tested the algorithms on 2 Gaussian kernels. We used a pre-trained image denoiser, as in the experimental setting of [12]. The kernel denoiser was trained on 10,000 generated kernels at several noise levels. We adopted DnCNN with 17 layers as the architectures of \(\mathsf{D}_{\sigma}\) for training kernel denoisers. BC-PnP and its ablated variants are initialized with the blur kernels \(\bm{\theta}_{0}\) pre-estimated using Pan-DCP [39] and images \(\bm{x}_{0}\leftarrow\bm{A}(\bm{\theta}_{0})^{\mathsf{T}}\bm{y}\).

We compared BC-PnP against several baseline methods, including Pan-DCP [39], SelfDeblur [58], DeblurGAN [78], USRNet [79]. Pan-DCP is an optimization-based method that jointly estimates image and blur kernel. SelfDeblur trains two deep image priors (DIP) [81] to jointly estimate the blur kernel and the image. DeblurGAN is a supervised learning-based method that lacks the capability for kernel estimation, but can reconstruct images via direct inference. USRNet is a DU baseline that was tested using blur kernel estimated from [39]. The results of DeblurGAN and USRNet are obtained by running the published code with the pre-trained weights.

Figure 3 illustrates the reconstruction results with a Gaussian kernel. Figure 3 demonstrates that BC-PnP can reconstruct the fine details of the snake skin, as highlighted by the white arrows, while both Pan-DCP and PnP produce smoother reconstructions. Additionally, BC-PnP generates a more accurate blur kernel compared to the ground truth kernel, whereas Pan-DCP and SelfDeblur yield blur kernels with artifacts. Table 2 presents the quantitative evaluation of the reconstruction results using

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{_Calibration (Y/N)_} & \multicolumn{3}{c}{\(R=6\)} & \multicolumn{2}{c}{\(R=8\)} \\ \cline{3-8}  & & \(\mathsf{RMSE}_{\bm{x}}\downarrow\mathsf{SSIM}_{\bm{x}}\uparrow\mathsf{RMSE}_{\bm{ \theta}}\downarrow\mathsf{RMSE}_{\bm{x}}\downarrow\mathsf{SSIM}_{\bm{x}} \uparrow\mathsf{RMSE}_{\bm{\theta}}\downarrow\) \\ \hline ENLIVE [35] & ✓ & 0.371 & 0.763 & — & 0.419 & 0.730 & — \\ ESPIRiT-TV [76] & ✓ & 0.218 & 0.884 & 0.256 & 0.361 & 0.818 & 0.356 \\ Unet [77] & ✗ & 0.218 & 0.904 & — & 0.195 & 0.907 & — \\ ISTANet+ [51] & ✗ & 0.110 & 0.946 & — & 0.140 & 0.928 & — \\ \cline{2-8} PnP & ✗ & 0.111 & 0.950 & 0.256 & 0.171 & 0.924 & 0.356 \\ \cline{2-8} PnP-GD\({}_{\bm{\theta}}\)[43] & ✓ & 0.116 & 0.950 & 0.254 & 0.163 & 0.926 & 0.355 \\ \cline{2-8} BC-PnP (Ours) & ✓ & 0.091 & 0.961 & 0.247 & 0.122 & 0.946 & 0.337 \\ \cline{2-8} PnP-oracle\({}_{\bm{\theta}}\)\({}^{\star}\) & ✗ & 0.069 & 0.969 & 0.000 & 0.082 & 0.962 & 0.000 \\ \hline \hline \multicolumn{8}{l}{\({}^{\star}\) not available in practice for blind inverse problems.} \\ \end{tabular}
\end{table}
Table 1: RMSE and SSIM performance of several methods on CS-PMRI. The table highlights the best and second best results. The _Calibration_ column highlights methods specifically designed to solve the blind inverse problem. Note how the use of a DL prior over the measurement operator enables BC-PnP to outperform PnP and PnP-GD\({}_{\bm{\theta}}\) and approach the performance of the oracle algorithm.

Figure 2: _Illustration of results from several well-known methods on CS-PMRI with the sampling factor \(R=6\). The quantities in the top-left corner of each image provide RMSE and SSIM values for each method. The squares at the bottom of each image visualize the error and the corresponding zoomed area in the image. Note how BC-PnP using a deep denoiser on the unknown CSMs outperforms uncalibrated PnP and matches PnP-oracle\({}_{\bm{\theta}}\) that knows the true CSMs._a Gaussian kernel, indicating that BC-PnP outperforms the baseline methods and nearly matches the SSIM and RMSE values of PnP-\(\text{oracle}_{\bm{\theta}}\) that is based on the ground truth blur kernel.

## 6 Conclusion

The work presented in this paper proposes a new BC-PnP method for jointly estimating unknown images and unknown measurement operators in blind inverse problems, presents its theoretical analysis in terms of convergence and accuracy, and applies the method to two well-known blind inverse problems. The proposed method and its theoretical analysis extend the recent work on PnP by introducing a learned prior on the unknown measurement operator, dropping the convexity assumptions on the data-fidelity term, and nonexpansiveness assumptions on the denoiser. The numerical validation of BC-PnP shows the improvements due to the use of learned priors on the measurement operator and the ability of the method to match the performance of the oracle PnP method that knows the true measurement operator. One conclusion of this work is the potential effectiveness of PnP for solving inverse problems where the unknown quantities are not only images.

## Limitations

The work presented in this paper comes with several limitations. The proposed BC-PnP method is based on PnP, which means that its performance is inherently limited by the use of AWGN denoisers as priors. While denoisers provide a convenient, principled, and flexible mechanism to specify priors,

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{_Calibration (Y/N)_} & \multicolumn{5}{c}{\(\bm{\|}\)} & \multicolumn{2}{c}{\(\bm{\|}\)} \\ \cline{3-8}  & & \(\text{RMSE}_{\bm{x}}\downarrow\text{SSIM}_{\bm{x}}\uparrow\text{RMSE}_{\bm{ \theta}}\downarrow\) & \(\text{RMSE}_{\bm{x}}\downarrow\text{SSIM}_{\bm{x}}\uparrow\text{RMSE}_{\bm{ \theta}}\downarrow\) \\ \hline Pan-DCP [39] & ✓ & 0.087 & 0.835 & 0.283 & 0.114 & 0.733 & 0.246 \\ SelfDeblur [58] & ✓ & 0.219 & 0.495 & 0.775 & 0.176 & 0.553 & 0.831 \\ DeblurGAN [78] & � & 0.090 & 0.823 & — & 0.118 & 0.716 & — \\ USRNet [79] & � & 0.106 & 0.855 & — & 0.114 & 0.769 & — \\ \hline PnP & � & 0.082 & 0.857 & 0.283 & 0.106 & 0.763 & 0.246 \\ PnP-GD\({}_{\bm{\theta}}\)[43] & ✓ & 0.082 & 0.857 & 0.283 & 0.108 & 0.767 & 0.246 \\ BC-PnP (Ours) & ✓ & 0.055 & 0.921 & 0.097 & 0.098 & 0.794 & 0.107 \\ \hline PnP-\(\text{oracle}_{\bm{\theta}}\)\({}^{\ast}\) & � & 0.051 & 0.929 & 0.000 & 0.088 & 0.817 & 0.000 \\ \hline \multicolumn{8}{l}{\({}^{\ast}\) not available in practice for blind inverse problems.} \\ \end{tabular}
\end{table}
Table 2: Quantitative evaluation of BC-PnP in blind image deblurring. We highlighted the best and second best results, respectively. The _Calibration_ column highlights methods specifically designed to solve the blind inverse problem. Note how the use of a prior over the measurement operator enables BC-PnP to nearly match the performance of the oracle algorithm.

Figure 3: _Illustration of results from several well-known methods on blind image deblurring with the Gaussian kernel. The squares at the top of each image show the estimated kernels. The quantities in the top-left corner of each image provide RMSE and SSIM values for each method. The squares at the bottom of each image highlight the error and the corresponding zoomed image region. Note how the BC-PnP using a deep denoiser on the unknown kernel significantly outperforms the traditional PnP method and matches the performance of the oracle PnP method that knows the true blur kernel. Note also the effectiveness of BC-PnP for estimating the unknown blur kernel._

they are inherently self-supervised and their empirical performance can thus be suboptimal compared to priors trained in a supervised fashion for a specific inverse problem. PnP running over many iterations can also have higher computational complexity compared to some end-to-end alternatives, such as DU with a small number of steps. Our analysis is based on the assumption that the denoiser used for inference computes an approximation of the true MMSE denoiser. While this assumption is reasonable for deep denoisers trained using the MSE loss, it is not directly applicable to denoisers trained using other common loss functions, such as the \(\ell_{1}\)-norm or SSIM. Finally, as is common with most theoretical work, our analysis only holds when our assumptions are satisfied, which might limit its applicability in practice. Our future work will investigate ways to improve on the results presented here by exploring new PnP strategies for relaxing assumptions for convergence, considering end-to-end trained variants of BC-PnP based on DEQ [53, 54], and exploring BC-PnP using explicit regularizers [26, 27, 28].

## Broader Impact

The expected impact of this work is in the area of imaging inverse problems with potential applications to computational imaging. There is a growing interest in computational imaging to leverage pre-trained deep models for estimating the unknown image as well as the unknown parameters of the imaging system. The ability to better address such problems can lead to new imaging tools for biomedical and scientific studies. While novel DL methods, such as the proposed BC-PnP approach, have the potential to enable new technological capabilities, they also come with a downside of being more complex and requiring higher-levels of technical sophistication. While our aim is to positively contribute to humanity, one can unfortunately envisage nonethical use of imaging technology.

Research presented in this article was supported in part by the NSF CCF-2043134. This work is also supported by the NIH R01EB032713, RF1AG082030, RF1NS116565, and R21NS127425.

## References

* [1] M. T. McCann, K. H. Jin, and M. Unser, "Convolutional neural networks for inverse problems in imaging: A review," _IEEE Signal Process. Mag._, vol. 34, no. 6, pp. 85-95, 2017.
* [2] A. Lucas, M. Iliadis, R. Molina, and A. K. Katsaggelos, "Using deep neural networks for inverse problems in imaging: Beyond analytical methods," _IEEE Signal Process. Mag._, vol. 35, no. 1, pp. 20-36, Jan. 2018.
* [3] G. Ongie, A. Jalal, C. A. Metzler, R. G. Baraniuk, A. G. Dimakis, and R. Willett, "Deep learning techniques for inverse problems in imaging," _IEEE J. Sel. Areas Inf. Theory_, vol. 1, no. 1, pp. 39-56, May 2020.
* [4] S. V. Venkatakrishnan, C. A. Bouman, and B. Wohlberg, "Plug-and-play priors for model based reconstruction," in _Proc. IEEE Global Conf. Signal Process. and Inf. Process._, Austin, TX, USA, Dec. 3-5, 2013, pp. 945-948.
* [5] S. Sreehari, S. V. Venkatakrishnan, B. Wohlberg, G. T. Buzzard, L. F. Drummy, J. P. Simmons, and C. A. Bouman, "Plug-and-play priors for bright field electron tomography and sparse interpolation," _IEEE Trans. Comput. Imaging_, vol. 2, no. 4, pp. 408-423, Dec. 2016.
* [6] C. Metzler, P. Schniter, A. Veeraraghavan, and R. Baraniuk, "prDeep: Robust phase retrieval with a flexible deep network," in _Proc. Int. Conf. Mach. Learn._, Stockholmsmassan, Stockholm Sweden, Jul. 10-15 2018, pp. 3501-3510.
* [7] K. Zhang, W. Zuo, S. Gu, and L. Zhang, "Learning deep CNN denoiser prior for image restoration," in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, Honolulu, USA, July 21-26, 2017, pp. 3929-3938.
* [8] T. Meinhardt, M. Moeller, C. Hazirbas, and D. Cremers, "Learning proximal operators: Using denoising networks for regularizing inverse imaging problems," in _Proc. IEEE Int. Conf. Comp. Vis._, Venice, Italy, Oct. 22-29, 2017, pp. 1799-1808.
* [9] W. Dong, P. Wang, W. Yin, G. Shi, F. Wu, and X. Lu, "Denoising prior driven deep neural network for image restoration," _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 41, no. 10, pp. 2305-2318, Oct 2019.
* [10] K. Zhang, W. Zuo, and L. Zhang, "Deep plug-and-play super-resolution for arbitrary blur kernels," in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, Long Beach, CA, USA, June 16-20, 2019, pp. 1671-1681.
* [11] K. Wei, A. Aviles-Rivero, J. Liang, Y. Fu, C.-B. Schonlieb, and H. Huang, "Tuning-free plug-and-play proximal algorithm for inverse imaging problems," in _Proc. Int. Conf. Mach. Learn._, 2020.

* [12] K. Zhang, Y. Li, W. Zuo, L. Zhang, L. Van Gool, and R. Timofte, "Plug-and-play image restoration with deep denoiser prior," _IEEE Trans. Patt. Anal. and Machine Intell._, 2022.
* [13] R. Liu, Y. Sun, J. Zhu, L. Tian, and U. S. Kamilov, "Recovery of continuous 3D refractive index maps from discrete intensity-only measurements using neural fields," _Nat. Mach. Intell._, vol. 4, pp. 781-791, Sept. 2022.
* [14] R. Ahmad, C. A. Bouman, G. T. Buzzard, S. Chan, S. Liu, E. T. Reehorst, and P. Schniter, "Plug-and-play methods for magnetic resonance imaging: Using denoisers for image recovery," _IEEE Signal Process. Mag._, vol. 37, no. 1, pp. 105-116, 2020.
* [15] U. S. Kamilov, C. A. Bouman, G. T. Buzzard, and B. Wohlberg, "Plug-and-play methods for integrating physical and learned models in computational imaging," _IEEE Signal Process. Mag._, vol. 40, no. 1, pp. 85-97, Jan. 2023.
* [16] S. H. Chan, X. Wang, and O. A. Elgendy, "Plug-and-play ADMM for image restoration: Fixed-point convergence and applications," _IEEE Trans. Comp. Imag._, vol. 3, no. 1, pp. 84-98, Mar. 2017.
* [17] Y. Romano, M. Elad, and P. Milanfar, "The little engine that could: Regularization by denoising (RED)," _SIAM J. Imaging Sci._, vol. 10, no. 4, pp. 1804-1844, 2017.
* [18] G. T. Buzzard, S. H. Chan, S. Sreehari, and C. A. Bouman, "Plug-and-play unplugged: Optimization free reconstruction using consensus equilibrium," _SIAM J. Imaging Sci._, vol. 11, no. 3, pp. 2001-2020, Sep. 2018.
* [19] E. T. Reehorst and P. Schniter, "Regularization by denoising: Clarifications and new interpretations," _IEEE Trans. Comput. Imag._, vol. 5, no. 1, pp. 52-67, Mar. 2019.
* [20] Y. Sun, B. Wohlberg, and U. S. Kamilov, "An online plug-and-play algorithm for regularized image reconstruction," _IEEE Trans. Comput. Imag._, vol. 5, no. 3, pp. 395-408, Sept. 2019.
* [21] Y. Sun, J. Liu, and U. S. Kamilov, "Block coordinate regularization by denoising," in _Proc. Adv. Neural Inf. Process. Syst._, Vancouver, BC, Canada, Dec. 2019, pp. 382-392.
* [22] Ernest K. Ryu, J. Liu, S. Wang, X. Chen, Z. Wang, and W. Yin, "Plug-and-play methods provably converge with properly trained denoisers," in _Proc. Int. Conf. Mach. Learn._, Long Beach, CA, USA, Jun. 09-15 2019, vol. 97, pp. 5546-5557.
* [23] X. Xu, Y. Sun, J. Liu, B. Wohlberg, and U. S. Kamilov, "Provable convergence of plug-and-play priors with mmse denoisers," _IEEE Signal Process. Lett._, vol. 27, pp. 1280-1284, 2020.
* [24] J. Liu, S. Asif, B. Wohlberg, and U. S. Kamilov, "Recovery analysis for plug-and-play priors using the restricted eigenvalue condition," in _Proc. Adv. Neural Inf. Process. Syst._, December 6-14, 2021, pp. 5921-5933.
* [25] Z. Kadkhodaie and E. P. Simoncelli, "Stochastic solutions for linear inverse problems using the prior implicit in a denoiser," in _Proc. Adv. Neural Inf. Process. Syst._, December 6-14, 2021, pp. 13242-13254.
* [26] R. Cohen, Y. Blau, D. Freedman, and E. Rivlin, "It has potential: Gradient-driven denoisers for convergent solutions to inverse problems," in _Proc. Adv. Neural Inf. Process. Syst._, December 6-14, 2021, pp. 18152-18164.
* [27] S. Hurault, A. Leclaire, and N. Papadakis, "Gradient step denoiser for convergent plug-and-play," in _Proc. Int. Conf. Learn. Represent._, 2022.
* [28] S. Hurault, A. Leclaire, and N. Papadakis, "Proximal denoiser for convergent plug-and-play optimization with nonconvex regularization," in _Proc. Int. Conf. Mach. Learn._, 2022, pp. 9483-9505.
* [29] R. Laumont, V. De Bortoli, A. Almansa, J. Delon, A. Durmus, and M. Pereyra, "Bayesian imaging using plug & play priors: When Langevin meets Tweedie," _SIAM J. Imaging Sci._, vol. 15, no. 2, pp. 701-737, 2022.
* [30] P. Campisi and K. Egiazarian, _Blind image deconvolution: theory and applications_, CRC press, 2017.
* [31] J. A. Fessler, "Optimization methods for magnetic resonance image reconstruction," _IEEE Signal Process. Mag._, vol. 1, no. 37, pp. 33-40, Jan. 2020.
* [32] L. Ying and J. Sheng, "Joint image reconstruction and sensitivity estimation in SENSE (JSENSE)," _Magn. Reson. Med._, vol. 57, no. 6, pp. 1196-1202, June 2007.
* [33] M. Uecker, T. Hohage, K. T. Block, and J. Frahm, "Image reconstruction by regularized nonlinear inversion-Joint estimation of coil sensitivities and image content," _Magn. Reson. Med._, vol. 60, no. 3, pp. 674-682, Sept. 2008.
* [34] Y. Jun, H. Shin, T. Eo, and D. Hwang, "Joint deep model-based MR image and coil sensitivity reconstruction network (joint-icnet) for fast MRI," in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2021, pp. 5270-5279.

* [35] H C. M Holme, S. Rosenzweig, F. Ong, R. N Wilke, M. Lustig, and M. Uecker, "ENLIVE: An efficient nonlinear method for calibrationless and robust parallel imaging," _Scientific reports_, vol. 9, no. 1, pp. 1-13, 2019.
* [36] M. Arvinte, S. Vishwanath, A. H. Tewfik, and J. I. Tamir, "Deep J-Sense: Accelerated MRI reconstruction via unrolled alternating optimization," in _Proc. Med. Image. Comput. Comput. Assist. Intervent_, Apr. 2021.
* [37] A. Sriram, J. Zbontar, T. Murrell, A. Defazio, C L. Zitnick, N. Yakubova, F. Knoll, and P. Johnson, "End-to-end variational networks for accelerated MRI reconstruction," in _Proc. Med. Image. Comput. Comput. Assist. Intervent_, 2020, pp. 64-73.
* [38] D. Kundur and D. Hatzinakos, "Blind image deconvolution," _IEEE Signal Process. Mag._, vol. 13, no. 3, pp. 43-64, 1996.
* [39] J. Pan, D. Sun, H. Pfister, and M. Yang, "Deblurring images via dark channel prior," _IEEE Trans. Pattern Anal. Mach. Intell._, vol. 40, no. 10, pp. 2315-2328, 2017.
* [40] E. Malhotra and A. Rajwade, "Tomographic reconstruction from projections with unknown view angles exploiting moment-based relationships," in _Proc. IEEE Int. Conf. Image Proc._, 2016, pp. 1759-1763.
* [41] D. C Lee, P. F Hoffmann, D. L Kopperdahl, and T. M Keaveny, "Phantomless calibration of ct scans for measurement of bmd and bone strength--inter-operator reanalysis precision," _Bone_, vol. 103, pp. 325-333, 2017.
* [42] C. Cheng, Y. Ching, P. Ko, and Y. Hwu, "Correction of center of rotation and projection angle in synchrotron x-ray computed tomography," _Scientific reports_, vol. 8, no. 1, pp. 98-84, 2018.
* [43] M. Xie, J. Liu, Y. Sun, W. Gan, B. Wohlberg, and U. S. Kamilov, "Joint reconstruction and calibration using regularization by denoising with application to computed tomography," in _Proc. IEEE Int. Conf. Comp. Vis. Workshops_, October 2021, pp. 4028-4037.
* [44] S. Wang, Z. Su, L. Ying, X. Peng, S. Zhu, F. Liang, D. Feng, and D. Liang, "Accelerating magnetic resonance imaging via deep learning," in _Proc. Int. Symp. Biomedical Imaging_, April 2016, pp. 514-517.
* [45] K. H. Jin, M. T. McCann, E. Froustey, and M. Unser, "Deep convolutional neural network for inverse problems in imaging," _IEEE Trans. Image Process._, vol. 26, no. 9, pp. 4509-4522, Sep. 2017.
* [46] E. Kang, J. Min, and J. Ye, "A deep convolutional neural network using directional wavelets for low-dose x-ray CT reconstruction," _Medical Physics_, vol. 44, no. 10, pp. e360-e375, 2017.
* [47] H. Chen, Y. Zhang, M. K. Kalra, F. Lin, Y. Chen, P. Liao, J. Zhou, and G. Wang, "Low-dose CT with a residual encoder-decoder convolutional neural network," _IEEE Trans. Med. Imag._, vol. 36, no. 12, pp. 2524-2535, Dec. 2017.
* [48] X. Xu, J. Pan, Y. Zhang, and M. Yang, "Motion Blur Kernel Estimation via Deep Learning," _IEEE Trans. on Image Process._, vol. 27, no. 1, pp. 194-205, Jan. 2018.
* [49] V. Monga, Y. Li, and Y. C. Eldar, "Algorithm unrolling: Interpretable, efficient deep learning for signal and image processing," _IEEE Signal Process. Mag._, vol. 38, no. 2, pp. 18-44, Mar. 2021.
* [50] A. Bora, A. Jalal, E. Price, and A. G. Dimakis, "Compressed sensing using generative priors," in _Proc. Int. Conf. Mach. Learn._, Sydney, Australia, Aug. 2017, pp. 537-546.
* [51] J. Zhang and B. Ghanem, "ISTA-Net: Interpretable optimization-inspired deep network for image compressive sensing," in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2018, pp. 1828-1837.
* [52] A. Hauptmann, F. Lucka, M. Betcke, N. Huynh, J. Adler, B. Cox, P. Beard, S. Ourselin, and S. Arridge, "Model-based learning for accelerated, limited-view 3-d photoacoustic tomography," _IEEE Trans. Med. Imag._, vol. 37, no. 6, pp. 1382-1393, 2018.
* [53] D. Gilton, G. Ongie, and R. Willett, "Deep equilibrium architectures for inverse problems in imaging," _IEEE Trans. Comput. Imag._, vol. 7, pp. 1123-1133, 2021.
* [54] J. Liu, X. Xu, W. Gan, S. Shoushtari, and U. S. Kamilov, "Online deep equilibrium learning for regularization by denoising," in _Proc. Adv. Neural Inf. Process. Syst._, New Orleans, LA, 2022.
* [55] Y. Han and J. C. Ye, "Framing U-Net via deep convolutional framelets: Application to sparse-view CT," _IEEE Trans. Med. Imag._, vol. 37, no. 6, pp. 1418-1429, 2018.
* [56] X. Peng, B. Sutton, F. Lam, and Z. Liang, "Deepsense: Learning coil sensitivity functions for sense reconstruction using deep learning," _Magn. Reson. Med._, vol. 87, no. 4, pp. 1894-1902, 2022.
* [57] E. Bostan, R. Heckel, M. Chen, M. Kellman, and L. Waller, "Deep phase decoder: self-calibrating phase microscopy with an untrained deep neural network," _Optica_, vol. 7, no. 6, pp. 559-562, 2020.
* [58] D. Ren, K. Zhang, Q. Wang, Q. Hu, and W. Zuo, "Neural blind deconvolution using deep priors," in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2020, pp. 3341-3350.
* [59] M. Asim, F. Shamshad, and A. Ahmed, "Blind image deconvolution using deep generative priors," _IEEE Trans. Comput. Imaging_, vol. 6, pp. 1493-1506, 2020.

* [60] H. Chung, J. Kim, S. Kim, and J. Ye, "Parallel Diffusion Models of Operator and Image for Blind Inverse Problems," in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, 2023.
* [61] Y. Hu, W. Gan, C. Ying, T. Wang, C. Eldenz, J. Liu, Y. Chen, H. An, and U. S Kamilov, "SPICE: Self-supervised learning for mri with automatic coil sensitivity estimation," _arXiv:2210.02584_, 2022.
* [62] A. Gossard and P. Weiss, "Training Adaptive Reconstruction Networks for Blind Inverse Problems," _arXiv:2202.11342_, 2022.
* [63] Y. Huang, E. Chouzenoux, and J.-C. Pesquet, "Unrolled variational bayesian algorithm for image blind deconvolution," _IEEE Trans. Image Process._, vol. 32, pp. 430-445, 2022.
* [64] Y. Li, M. Tofighi, J. Geng, V. Monga, and Y. C Eldar, "Efficient and interpretable deep blind image deblurring via algorithm unrolling," _IEEE Trans. Comput. Imaging_, vol. 6, pp. 666-681, 2020.
* [65] U. S. Kamilov, H. Mansour, and B. Wohlberg, "A plug-and-play priors approach for solving nonlinear imaging inverse problems," _IEEE Signal. Proc. Let._, vol. 24, no. 12, pp. 1872-1876, Dec. 2017.
* [66] R. Gribonval, "Should penalized least squares regression be interpreted as maximum a posteriori estimation?," _IEEE Trans. Signal Process._, vol. 59, no. 5, pp. 2405-2410, May 2011.
* [67] T. Tirer and R. Giryes, "Image restoration by iterative denoising and backward projections," _IEEE Trans. Image Process._, vol. 28, no. 3, pp. 1220-1234, Mar. 2019.
* [68] A. M. Teodoro, J. M. Bioucas-Dias, and M. A. T. Figueiredo, "A convergent image fusion algorithm using scene-adapted Gaussian-mixture-based denoising," _IEEE Trans. Image Process._, vol. 28, no. 1, pp. 451-463, Jan. 2019.
* [69] Y. Sun, Z. Wu, B. Wohlberg, and U. S. Kamilov, "Scalable plug-and-play ADMM with convergence guarantees," _IEEE Trans. Comput. Imag._, vol. 7, pp. 849-863, July 2021.
* [70] R. Cohen, M. Elad, and P. Milanfar, "Regularization by denoising via fixed-point projection (red-pro)," _SIAM J. Imaging Sci._, vol. 14, no. 3, pp. 1374-1406, 2021.
* [71] M. Ljubenovic and M. A. Figueiredo, "Blind image deblurring using class-adapted image priors," in _Proc. IEEE Int. Conf. Image Proc._ IEEE, 2017, pp. 490-494.
* [72] M. Ljubenovic and M. A. Figueiredo, "Plug-and-play approach to class-adapted blind image deblurring," _Int. J. Doc. Anal. Recognit._, vol. 22, no. 2, pp. 79-97, 2019.
* [73] C. J Pellizzari, M. F Spencer, and C. A Bouman, "Coherent plug-and-play: digital holographic imaging through atmospheric turbulence using model-based iterative reconstruction and convolutional neural networks," _IEEE Trans. Comput. Imaging_, vol. 6, pp. 1607-1621, 2020.
* [74] S. J. Wright, "Coordinate descent algorithms," _Math. Program._, vol. 151, no. 1, pp. 3-34, June 2015.
* [75] F. Knoll _et al._, "fastMRI: A publicly available raw k-space and DICOM dataset of knee images for accelerated MR image reconstruction using machine learning," _Radiology: Artificial Intelligence_, vol. 2, no. 1, pp. e190007, 2020.
* [76] M. Uecker, P. Lai, M. J. Murphy, P. Virtue, M. Elad, J. M. Pauly, S. S. Vasanawala, and M. Lustig, "ESPIRiT--an eigenvalue approach to autocalibrating parallel mri: where sense meets grappa," _Magn. Reson. Med._, vol. 71, no. 3, pp. 990-1001, 2014.
* [77] O. Ronneberger, P. Fischer, and T. Brox, "U-Net: Convolutional networks for biomedical image segmentation," in _Proc. Med. Image. Comput. Comput. Assist. Intervent._, 2015, pp. 234-241.
* [78] O. Kupyn, T. Martyniuk, J. Wu, and Z. Wang, "Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better," in _Proc. IEEE Int. Conf. Comput. Vis._, 2019, pp. 8878-8887.
* [79] K. Zhang, L. V. Gool, and R. Timofte, "Deep unfolding network for image super-resolution," in _Proc. IEEE Conf. Comput. Vis. Pattern Recognit._, Jun. 2020, pp. 3217-3226.
* [80] D. Martin, C. Fowlkes, D. Tal, and J. Malik, "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics," in _Proc. IEEE Int. Conf. Comp. Vis._, Vancouver, Canada, July 7-14, 2001, pp. 416-423.
* [81] D. Ulyanov, A. Vedaldi, and V. Lempitsky, "Deep image prior," in _Proc. IEEE Conf. Computer Vision and Pattern Recognition_, Salt Lake City, UT, USA, June 18-22, 2018, pp. 9446-9454.
* [82] R. Gribonval and P. Machart, "Reconciling "priors" & "priors" without prejudice?," in _Proc. Adv. Neural Inf. Process. Syst._, Lake Tahoe, NV, USA, December 5-10, 2013, pp. 2193-2201.
* [83] D. P. Bertsekas, "Incremental proximal methods for large scale convex optimization," _Math. Program. Ser. B_, vol. 129, pp. 163-195, 2011.
* [84] J. Bolte, S. Sabach, and M. Teboulle, "Proximal alternating linearized minimization for nonconvex and nonsmooth problems," _Math. Program._, vol. 146, pp. 459-494, 2013.

* [85] J. Mairal, "Incremental majorization-minimization optimization with application to large-scale machine learning," _SIAM J. Optim._, vol. 25, no. 2, pp. 829-855, Jan. 2015.
* [86] E. K. Ryu and W. Yin, _Large-Scale Convex Optimization: Algorithms and Analyses via Monotone Operators_, Cambridge University Press, 2023.
* [87] A. Kazerouni, U. S. Kamilov, E. Bostan, and M. Unser, "Bayesian denoising: From MAP to MMSE using consistent cycle spinning," _IEEE Signal Process. Lett._, vol. 20, no. 3, pp. 249-252, March 2013.
* [88] R. Gribonval and M. Nikolova, "On Bayesian estimation and proximity operators," _Appl. Comput. Harmon. Anal._, vol. 50, pp. 49-72, Jan. 2021.
* [89] D. Kingma and J. Ba, "Adam: A method for stochastic optimization," in _Proc. Int. Conf. on Learn. Represent._, 2015.