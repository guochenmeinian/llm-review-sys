# Faster Margin Maximization Rates for Generic Optimization Methods

 Guanghui Wang\({}^{1}\), Zihao Hu\({}^{1}\), Vidya Muthukumar\({}^{2,3}\), Jacob Abernethy\({}^{1,4}\)

\({}^{1}\)College of Computing, Georgia Institute of Technology

\({}^{2}\)School of Electrical and Computer Engineering, Georgia Institute of Technology

\({}^{3}\)School of Industrial and Systems Engineering, Georgia Institute of Technology

\({}^{4}\)Google Research, Atlanta

{gwang369,zihaohu,vmuthukumar8}@gatech.edu, abernethyj@google.com

###### Abstract

First-order optimization methods tend to inherently favor certain solutions over others when minimizing a given training objective with multiple local optima. This phenomenon, known as _implicit bias_, plays a critical role in understanding the generalization capabilities of optimization algorithms. Recent research has revealed that gradient-descent-based methods exhibit an implicit bias for the \(\ell_{2}\)-maximal margin classifier in the context of separable binary classification. In contrast, generic optimization methods, such as mirror descent and steepest descent, have been shown to converge to maximal margin classifiers defined by alternative geometries. However, while gradient-descent-based algorithms demonstrate fast implicit bias rates, the implicit bias rates of generic optimization methods have been relatively slow. To address this limitation, in this paper, we present a series of state-of-the-art implicit bias rates for mirror descent and steepest descent algorithms. Our primary technique involves transforming a generic optimization algorithm into an online learning dynamic that solves a regularized bilinear game, providing a unified framework for analyzing the implicit bias of various optimization methods. The accelerated rates are derived leveraging the regret bounds of online learning algorithms within this game framework.

## 1 Introduction

The training objective in the optimization of modern over-parametrized ML models typically presents various local optima with low training error. Despite this, empirical studies have demonstrated that first-order optimization methods generally converge to the solution with strong generalization, even without any explicit regularization (Neyshabur et al., 2014; Zhang et al., 2021). This observation has spurred interest in the study of the _implicit bias_ of the algorithm. In other words, _among all potential parameter choices with low training error, which ones are inherently favored by optimization methods?_

For the classical linear classification problem with separable data, the pioneering works (Soudry et al., 2018; Ji and Telgarsky, 2018) reveal that minimizing the (unregularized) empirical risk with exponential loss by the classical gradient descent (GD) automatically _maximizes the \(\|\cdot\|_{2}\)-margin_, meaning its margin converges to that of the best classifier within an \(\ell_{2}\)-norm ball (referred to as the \(\left\|\cdot\right.\|_{2}\)-maximal margin classifier). This finding implies that GD exhibits an _implicit bias_ towards the \(\left\|\cdot\right.\|_{2}\)-maximal margin classifier, which helps account for its favorable generalization performance. However, these works show that GD only maximizes the \(\|\cdot\|_{2}\)-margin at a slow \(O\left(\frac{\log n}{\log T}\right)\) rate, where \(T\) is the time horizon, and \(n\) is the cardinality of the data set. Since then, several _faster_ marginmaximization rates have been reported. Nacson et al. (2019) revealed that for the exponential loss, GD with an aggressive step size attains an \(O\left(\frac{\log n+\log T}{\sqrt{T}}\right)\|\cdot\|_{2}\)-margin maximization rate. This result was later improved to \(O\left(\frac{\log n}{T}\right)\) by Ji & Telgarsky (2021), via an elegant primal-dual analysis. Recent work (Ji et al., 2021; Wang et al., 2022b) further proved that momentum-based GD and Nesterov-accelerated GD can maximize the \(\|\cdot\|_{2}\)-margin at an \(O\left(\frac{\log n}{T^{2}}\right)\) rate.

As the implicit bias of gradient-descent-based methods becomes better understood, it is natural to explore similar characterizations for other optimization methods. Note that, since gradient-descent-based methods are biased towards the \(\|\cdot\|_{2}\)-maximal margin classifier, they might generalize poorly when the data does not adhere to the \(\ell_{2}\)-geometry, as shown in Gentile (2000); Chen et al. (2001). This limitation suggests that it is essential to study the implicit bias of alternative optimization methods that could potentially be biased in different directions. Two such methods include steepest descent with respect to different norms and mirror descent with different potentials. For instance, Gunasekar et al. (2018) demonstrate that for the exponential loss, the steepest descent algorithm with respect to a general norm \(\|\cdot\|\) asymptotically converges to the corresponding \(\|\cdot\|\)-maximal margin classifier. This result implies that the steepest descent algorithm can adapt to different data geometries by changing the norm used in the algorithm. On the other hand, Sun et al. (2022) show that the mirror descent algorithm with the potential \(\|\cdot\|_{q}^{q}\) for \(q>1\) can maximize the \(\|\cdot\|_{q}\)-margin at a rate of \(O\left(\frac{1}{(\log T)^{q-1}}\right)\).

While the asymptotic directional convergence of these generic optimization methods is well-understood, a natural question remains: _can generic optimization methods (e.g., mirror descent and steepest descent) achieve faster margin maximization rates?_ Several papers have contributed partial answers to this question. Li et al. (2021) show that mirror descent with an aggressive step size maximizes the margin in an \(O\left(\frac{\log n}{T^{1/4}}\right)\) rate. However, their analysis assumes the potential function is _both strongly-convex and smooth_ with respect to some norm, and thus is mainly limited to the \(\ell_{2}\)-geometry. For steepest descent with respect to a general norm \(\|\cdot\|\), Nacson et al. (2019) prove that an \(O\left(\frac{\log n+\log T}{\sqrt{T}}\right)\|\cdot\|\)-margin maximization rate can be achieved with an appropriately chosen step size, but it is unclear whether it can be further improved. In this paper, we provide the fastest known rates for margin maximization for generic optimization methods, through the following contributions:

* First, we study a _weighted-average_ version of mirror descent with the squared \(\ell_{q}\)-norm \(\frac{1}{2}\|\cdot\|_{q}^{2}\) as the potential for \(q\in(1,2]\). This potential function is strongly convex but not smooth. We show that, with an appropriately chosen step size, the algorithm achieves a faster \(\|\cdot\|_{q}\)-margin maximization rate on the order of \(O\left(\frac{\log n\log T}{(q-1)T}\right)\). We also further improve the rate to \(O\left(\frac{1}{T(q-1)}+\frac{\log n\log T}{T^{2}}\right)\) with a more aggressive step size. When \(q=2\), the algorithm reduces to average GD, and our rate \(O\left(\frac{1}{T}+\frac{\log n\log T}{T^{2}}\right)\) is a \(\log n\)-factor tighter than the \(O\left(\frac{\log n}{T}\right)\) rate of the last-iterate GD (Ji & Telgarsky, 2021).
* Next, for the steepest descent algorithm with respect to the \(\ell_{q}\)-norm for \(q\in(1,2]\), we show the margin maximization rate can be improved from \(O\left(\frac{\log n+\log T}{\sqrt{T}}\right)\) to \(O\left(\frac{\log n}{T(q-1)}\right)\).
* Finally, we demonstrate that a even faster \(O\left(\frac{\log n}{T^{2}(q-1)}\right)\|\cdot\|_{q}\)-margin maximization rate can be achieved in two ways: a) mirror descent with Nesterov acceleration, or b) steepest descent with extra gradient and momentum.

The essential premise for our approach is that _minimizing empirical risk (ERM) with generic optimization methods can be equivalently viewed as solving a regularized bilinear game with online learning dynamics_. Within this framework, we design new pairs of online learning methods whose outputs (and, by extension, the outputs of the corresponding generic optimization methods) automatically maximize the margin. The convergence rates are determined by the time-averaged regret bounds of these online learning algorithms when played against each other, which turn out to be much faster than the worst-case \(O(1/\sqrt{T})\) rate.

Wang et al. (2022b) were the first to draw parallels between _Nesterov-accelerated GD_ for ERM and solving the bilinear game through an online dynamic. However, it was still open that whether this kind of analysis suits other GD-based methods. Moreover, the non-linearity of the mirror map in generic optimization methods makes analysis particularly challenging. In this paper, we reveal that the game framework can in fact encompass implicit bias analysis for a range of generic optimization methods, and offer a more streamlined and unified analysis. Within this game framework, we derive several other results beyond those mentioned above:

* By selecting suitable online learning algorithms, we obtain a momentum-based data-dependent MD algorithm with an \(O(\frac{\mathcal{V}_{T}}{T^{2}(q-1)}+\frac{\log n\log T}{T^{2}})\parallel \cdot\|_{q}\)-margin maximization rate, where \(\mathcal{V}_{T}=\sum_{t=2}^{T}\|\mathbf{p}_{t}-\mathbf{p}_{t-1}\|_{1}^{2}\) is the path-length of a series of distributions on the training data \(\mathbf{p}_{t}\). In the worst case, this reduces to the margin maximization rate of MD, but this can be much tighter if \(\mathcal{V}_{T}\) is sublinear in \(T\).
* Apart from margin maximization rates, we also demonstrate the corresponding directional error, i.e., the bound on the \(\ell_{q}\)-distance between the maximal margin classifier and the normalized output of the generic methods, which are also controlled by the regret bounds of two-players against each other. This kind of convergence rates are new for most of the generic methods. In general, we prove that the directional errors are typically a square-root factor worse than the margin maximization rates.
* For our steepest descent, by setting the norm to the general norm \(\|\cdot\|\) and the \(\ell_{2}\)-norm respectively, we can recover the algorithms and theoretical guarantees in Nacson et al. (2019); Ji and Telgarsky (2021) under the game framework. This implies that these algorithms can also be viewed as solving a _regularized_ bilinear game using online learning algorithms, offering a deeper understanding of the role of implicit bias in optimization methods.

Additional related workThe strategy of solving a zero-sum game using online learning algorithms playing against each other has been extensively studied, primarily through the lens of _independent learning agents_(e.g., Rakhlin and Sridharan, 2013; Daskalakis et al., 2018; Wang and Abernethy, 2018; Daskalakis and Panageas, 2019; Zhang et al., 2022). In contrast, our central motivation and challenge lies in identifying the exact equivalent forms of generic optimization algorithms under the regularized bilinear game dynamic. Our framework is also motivated by the line of research that employs the _Fenchel-game_ to elucidate commonly used convex optimization methods (Abernethy et al., 2018; Wang and Abernethy, 2018; Wang et al., 2021b). However, our framework diverges significantly from these approaches. These works focus on the convergence of the optimization problem itself, while our framework emphasizes that the choice of optimization algorithm, which solely targets the minimization of empirical risk, has a significant impact on maximizing the margin, which we might view as an "algorithmic externality." Max-margin guarantees can not arise from convergence of the ERM objective alone, as there are typically multiple global minima in ERM minimization. Our analysis also considers an entirely different min-max problem than that of the Fenchel game (Wang et al., 2021b). Consequently, the correspondences we establish between optimization algorithms and online dynamics also differ. Finally, we note that previous work has also analyzed the implicit bias through direct primal optimization analysis (e.g., Nacson et al., 2019; Sun et al., 2022) or using a dual perspective (e.g., Ji and Telgarsky, 2021; Ji et al., 2021). For the former analysis, it is unclear whether and how faster rates can be obtained. For the latter, it remains an open question how to extend the framework beyond \(\ell_{2}\)-geometry, which in some sense was the motivation for the present work. For more related work, we refer the reader to Appendix A.

## 2 Preliminaries

In this section, we present our basic setting along with some standard assumptions and definitions.

NotationWe use lower case bold face letters \(\mathbf{x}\), \(\mathbf{y}\) to denote vectors, lower case letters \(a,b\) to denote scalars, and upper case bold face letters \(\mathbf{A},\mathbf{B}\) to denote matrices. For a vector \(\mathbf{x}\in\mathbb{R}^{d}\), we use \(x_{i}\) to denote the \(i\)-th component of \(\mathbf{x}\). For a matrix \(\mathbf{A}\in\mathbb{R}^{n\times d}\), let \(\mathbf{A}_{(i,:)}\) be its \(i\)-th row, \(\mathbf{A}_{(:,j)}\) the \(j\)-th column, and \(\mathbf{A}_{(i,j)}\) the \(i\)-th element of the \(j\)-th column. \(\forall\mathbf{x}\in\mathbb{R}^{d}\), we use \(\|\cdot\|\) to denote a general norm in \(\mathbb{R}^{d}\), \(\|\cdot\|_{*}\) its dual norm, \(\|\mathbf{x}\|_{q}\) the \(q\)-norm of \(\mathbf{x}\), defined as \(\|\mathbf{x}\|_{q}=(\sum_{i=1}^{d}|x_{i}|^{q})^{1/q}\), and \(q\in(1,2]\). We use \(\|\cdot\|_{p}\) to denote the dual norm of \(q\)-norm, where \(p\in[2,\infty)\), \(\frac{1}{p}+\frac{1}{q}=1\)We denote \(\mathcal{B}_{\|\cdot\|}\) the \(\|\cdot\)\(\|\)-ball, defined as \(\mathcal{B}_{\|\cdot\|}=\{\mathbf{x}\in\mathbb{R}^{d}|\|\mathbf{x}\|\leq 1\}\). \(\forall\mathbf{x},\mathbf{x}^{\prime}\in\mathbb{R}^{d}\), we define the Bregman divergence between \(\mathbf{x}\) and \(\mathbf{x}^{\prime}\) with respect to a strictly convex potential function \(\Phi(\mathbf{x})\) as \(D_{\Phi}(\mathbf{x},\mathbf{x}^{\prime})=\Phi(\mathbf{x})-\Phi(\mathbf{x}^{ \prime})-\nabla\Phi(\mathbf{x}^{\prime})^{\top}(\mathbf{x}-\mathbf{x}^{\prime})\). For a positive integer \(n\), we denote \(\{1,\ldots,n\}\) as \([n]\), and the \((n-1)\)-dimensional simplex as \(\Delta^{n}\). Let \(E:\Delta^{n}\mapsto\mathbb{R}\) be the negative entropy function, defined as \(E(\mathbf{p})=\sum_{i=1}^{n}p_{i}\log p_{i},\forall\mathbf{p}\in\Delta^{n}\).

Basic settingConsider a set of \(n\) data points \(\mathcal{S}=\{(\mathbf{x}^{(i)},y^{(i)})\}\), where \(\mathbf{x}^{(i)}\in\mathbb{R}^{d}\) is the feature vector for the \(i\)-th example, and \(y^{(i)}\in\{-1,+1\}\) the corresponding binary label. We are interested the optimization trajectory of first-order methods for minimizing the following unbounded and unregularized empirical risk:

\[\min_{\mathbf{w}\in\mathbb{R}^{d}}L(\mathbf{w})=\frac{1}{n}\sum_{i=1}^{n}r( \mathbf{w}^{\top}\mathbf{x}^{(i)};y^{(i)}),\] (1)

where \(\mathbf{w}\in\mathbb{R}^{d}\) is a linear classifier, \(r:\mathbb{R}\times\{\pm 1\}\mapsto\mathbb{R}\) is the loss function. Following previous work, we focus on the exponential loss, given by \(r(\mathbf{w}^{\top}\mathbf{x};y)=\exp(-y\mathbf{x}^{\top}\mathbf{w})\). We introduce the following standard assumption and definitions.

**Definition 1** (\(\|\cdot\|\)-margin).: _For a linear classifier \(\mathbf{w}\in\mathbb{R}^{d}\) and a norm \(\|\cdot\|\), we define its \(\|\cdot\|\)-margin as_

\[\widetilde{\gamma}(\mathbf{w})=\frac{\min_{i\in[n]}y^{(i)}\mathbf{w}^{\top} \mathbf{x}^{(i)}}{\|\mathbf{w}\|}=\frac{\min_{\mathbf{p}\in\Delta^{n}} \mathbf{p}^{\top}\mathbf{A}\mathbf{w}}{\|\mathbf{w}\|},\]

_where \(\mathbf{A}=[\ldots;y^{(i)}\mathbf{x}^{(i)\top};\ldots]\in\mathbb{R}^{n\times d}\) is the matrix that contains all data._

**Assumption 1**.: _Assume \(\mathcal{S}\) is linearly separable and bounded with respect to some norm \(\|\cdot\|\). More specifically, we assume \(\exists\mathbf{w}_{\|\cdot\|}^{*}\in\mathcal{B}_{\|\cdot\|}\), s.t., \(\mathbf{w}_{\|\cdot\|}^{*}=\operatorname*{argmax}_{\|\mathbf{w}\|\leq 1}\min_{i \in[n]}y^{(i)}\mathbf{x}^{(i)\top}\mathbf{w},\) whose margin \(\widetilde{\gamma}(\mathbf{w}_{\|\cdot\|}^{*})=\gamma>0\). We refer to \(\mathbf{w}_{\|\cdot\|}^{*}\) as the \(\|\cdot\|\)-maximal margin classifier. Note that, for any \(\mathbf{w}\in\mathbb{R}^{d}\), if \(\widetilde{\gamma}(\mathbf{w})=\gamma\), \(\mathbf{w}\) and \(\mathbf{w}_{\|\cdot\|}^{*}\) are at the same direction. Finally, we also assume \(\mathcal{S}\) is bounded wrt some dual norm \(\|\cdot\|_{*}\), i.e., \(\forall i\in[n]\), \(\|\mathbf{x}^{(i)}\|_{*}\leq 1\)._

**Definition 2** (\(\|\cdot\|\)-Margin maximization rate and \(\|\cdot\|\)-directional error).: _Suppose Assumption 1 is satisfied. We consider a sequence of solutions \(\mathbf{w}_{1},\ldots,\mathbf{w}_{\|\cdot\|}\), and state that \(\mathbf{w}_{t}\) converges to \(\mathbf{w}_{\|\cdot\|}^{*}\) if either \(\lim_{t\to\infty}\widetilde{\gamma}(\mathbf{w}_{t})\to\gamma\), or \(\lim_{t\to\infty}\|\frac{\mathbf{w}_{t}}{\|\mathbf{w}_{t}\|}-\mathbf{w}_{\| \cdot\|}^{*}\|\to 0\). We define the upper bound on \(|\gamma-\widetilde{\gamma}(\mathbf{w}_{t})|\) the \(\|\cdot\|\)-margin maximization rate, and \(\|\frac{\mathbf{w}_{t}}{\|\mathbf{w}_{t}\|}-\mathbf{w}_{\|\cdot\|}^{*}\|\) the \(\|\cdot\|\)-directional error._

## 3 A Game Framework for Maximizing the Margin

In this section, we present a general game framework and demonstrate that solving this game with online learning algorithms can directly maximize the margin and minimize the directional error.

Figure 1: Illustration of the game framework for implicit bias analysis. In Section 3, we show that solving a regularized bilinear game with online learning algorithms (top box) can directly maximize the margin, and the convergence rate is on the same order of the averaged regret \(C_{T}\) (right box); In Sections 4, we prove that minimizing the empirical risk with a series of generic optimization methods (left box) is equivalent to using online learning algorithms to solve the regularized bilinear game. Thus, the implicit bias rates can be directly obtained by plugging in the regret bounds.

Then, in Section 4, we show that many generic optimization methods can be considered as solving this game with different online dynamics. As a result, the margin maximization rate (and also the directional error) of these optimization methods are exactly characterized by the regret bounds of the corresponding online learning algorithms. We illustrate this procedure in Figure 1. The game objective is defined as follows:

\[\max_{\mathbf{w}\in\mathbb{R}^{d}}\min_{\mathbf{p}\in\Delta^{n}}g(\mathbf{p}, \mathbf{w})=\mathbf{p}^{\top}\mathbf{A}\mathbf{w}-\Phi(\mathbf{w}),\] (2)

where \(\Phi(\mathbf{w})=\frac{1}{2}\|\mathbf{w}\|^{2}\) is a regularizer and \(\|\cdot\|\) denotes some _general norm_ in \(\mathbb{R}^{d}\). Following previous work (Wang et al., 2021, 2022b), we apply a weighted no-regret dynamic protocol (summarized in Protocol 1) to solve the game. We first give a brief introduction of Protocol 1, and then present the theorem about the margin of its output.

Description of Protocol 1In Protocol 1, the players of the zero-sum game try to find the equilibrium by applying online learning algorithms. In each round \(t\), the \(\mathbf{p}\)-player first picks a decision \(\mathbf{p}_{t}\), and passes a weighted loss function to the \(\mathbf{w}\)-player, defined as

\[\alpha_{t}h_{t}(\mathbf{w})=-\alpha_{t}(\mathbf{p}_{t}^{\top}\mathbf{A} \mathbf{w}-\Phi(\mathbf{w}))=-\alpha_{t}g(\mathbf{p}_{t},\mathbf{w}).\]

Then, the \(\mathbf{w}\)-player observes the loss, picks a decision \(\mathbf{w}_{t}\), and passes a weighted loss function

\[\alpha_{t}\ell_{t}(\mathbf{p})=\alpha_{t}(\mathbf{p}^{\top}\mathbf{A} \mathbf{w}_{t}-\Phi(\mathbf{w}_{t}))=\alpha_{t}g(\mathbf{p},\mathbf{w}_{t}),\]

to the \(\mathbf{p}\)-player. Note that the order of the two players can also be reversed. After \(T\) iterations, the algorithm outputs the weighted sum of the \(\mathbf{w}\)-player's decisions: \(\widehat{\mathbf{w}}_{T}=\sum_{t=1}^{T}\alpha_{t}\mathbf{w}_{t}\). Under this framework, we define the weighted regret upper bound of both players respectively as

\[\sum_{t=1}^{T}\alpha_{t}\ell_{t}(\mathbf{p}_{t})-\min_{\mathbf{p}\in\Delta^{n }}\sum_{t=1}^{T}\alpha_{t}\ell_{t}(\mathbf{p})\leq\text{Reg}_{T}^{\mathbf{p}},\ \ \text{and}\ \ \sum_{t=1}^{T}\alpha_{t}h_{t}(\mathbf{w})-\min_{\mathbf{w}\in\mathbb{R}^{d}} \sum_{t=1}^{T}\alpha_{t}h_{t}(\mathbf{w})\leq\text{Reg}_{T}^{\mathbf{w}}.\]

Denote the upper bound on the _average_ weighted regret by \(C_{T}=(\text{Reg}_{T}^{\mathbf{p}}+\text{Reg}_{T}^{\mathbf{w}})/\sum_{t=1}^{T }\alpha_{t}\). We have the following conclusion on the margin and directional error of \(\widetilde{\mathbf{w}}_{T}\). The proof of this theorem can be found in Appendix B.

**Theorem 1**.: _Suppose Assumption 1 holds with respect to some general norm \(\|\cdot\|\). Consider solving the two-player zero-sum game defined in (2) by applying Protocol 1. Then \(\widetilde{\mathbf{w}}_{T}\) will have a positive margin on round \(T\) if \(C_{T}\leq\frac{\gamma^{2}}{4}\). Moreover, as long as \(C_{T}\leq\frac{\gamma^{2}}{4}\), we have_

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A} \widetilde{\mathbf{w}}_{T}}{\|\widetilde{\mathbf{w}}_{T}\|}\geq\gamma-\frac{4C _{T}}{\gamma^{2}}.\] (3)

_If \(\Phi(\mathbf{w})\) is \(\lambda\)-strongly convex with wrt \(\|\cdot\|\), we have_

\[\left\|\frac{\widetilde{\mathbf{w}}_{T}}{\|\widetilde{\mathbf{w}}_{T}\|}- \mathbf{w}_{\|\cdot\|}^{*}\right\|\leq\frac{8\sqrt{2}}{\gamma^{2}\sqrt{\lambda }}\sqrt{C_{T}}.\]

Theorem 1 shows that the output of Protocol 1, denoted as \(\widetilde{\mathbf{w}}_{T}\), achieves a positive margin when the average regret \(C_{T}\leq\frac{\gamma^{2}}{4}\). In the following sections, we demonstrate that with appropriately chosen online learning algorithms \(C_{T}\) always decreases with respect to \(T\); in fact \(C_{T}\to 0\) as \(T\to\infty\). Therefore, once the condition \(C_{T}\leq\frac{\gamma^{2}}{4}\) is met for a particular value \(T_{0}\), it will also be met for all \(T\geq T_{0}\). Thereafter, \(\widetilde{\mathbf{w}}_{T}\) continues to increase the \(\|\cdot\|\)-margin and converges to the maximum \(\|\cdot\|\)-margin classifier, and the rate is directly characterized by \(C_{T}\). Since \(C_{T}\) is the average regret of the online learning algorithms, better bounds on \(C_{T}\) lead to a less stringent condition on large enough \(T\). Finally, we note that the condition on sufficiently large \(T\) is also (explicitly or implicitly) required in all previous work on the non-asymptotic margin maximization rates of generic methods (Nacson et al., 2019; Li et al., 2021; Sun et al., 2022). We refer to Appendix A for more details.

## 4 Implicit Bias of Generic Methods

In this section, we show that average mirror descent and steepest descent can find their equivalent online learning forms under Protocol 1. Thus, their margin maximization rates can be directly characterized by the corresponding average regret \(C_{T}\). For clarity, we use \(\mathbf{v}_{t}\) to denote the classifier updates in the original methods, and \(\mathbf{w}_{t}\) the update under the game framework. Note that Theorem 1 clearly implies that the convergence rate of the directional error is always a square-root worse than that of the margin maximization rate. Due to space limitations, we only present the margin maximization rates, while the corresponding rates on directional error are presented in the appendix.

### Mirror-Descent-Type of Methods

First, we consider minimizing (1) by applying the following mirror descent algorithm:

\[\mathbf{v}_{t}=\operatorname*{argmin}_{\mathbf{v}\in\mathbb{R}^{d}}\eta_{t} \left\langle\nabla L(\mathbf{v}_{t-1}),\mathbf{v}\right\rangle+D_{\Phi}( \mathbf{v},\mathbf{v}_{t-1}),\] (4)

where \(D_{\Phi}(\mathbf{v},\mathbf{v}_{t-1})\) is the Bregman divergence between \(\mathbf{v}\) and \(\mathbf{v}_{t-1}\), and \(\Phi(\mathbf{v})\) is a strongly convex potential function that defines the mirror map. Note that since the feasible domain in (4) is unbounded, we can rewrite the algorithm in the following form:

\[\nabla\Phi(\mathbf{v}_{t})=\nabla\Phi(\mathbf{v}_{t-1})-\eta_{t}\nabla L( \mathbf{v}_{t-1}).\]

In this paper, we consider weighted-average mirror descent with the squared \(q\)-norm, i.e., \(\Phi(\mathbf{w})=\frac{1}{2}\|\mathbf{w}\|_{q}^{2}\), where \(q\in(1,2]\), and demonstrate that this optimization algorithm can enable faster \(\|\cdot\|\)-margin maximization rates. The detailed update rule is summarized in the left box of Algorithm 1. It is worth noting that this type of regularizer is \((q-1)\)-strongly convex with respect to \(\|\cdot\|_{q}\), and can be updated efficiently in closed form as below1: for each coordinate \(i\in[d]\), we have

Footnote 1: This expression appears in (Section 6.7, Orabona, 2019) and we reproduce it for completeness.

\[\widehat{v}_{t,i} =\text{sign}(v_{t-1,i})|v_{t-1,i}|^{q-1}\|\mathbf{v}_{t-1}\|_{q}^ {2-q}-\eta_{t}[\nabla L(\mathbf{v}_{t-1})]_{i},\] (5) \[v_{t,i} =\text{sign}(\widehat{v}_{t,i})|\widehat{v}_{t,i}|^{p-1}\| \widehat{\mathbf{v}}_{t}\|_{p}^{2-p}.\]

We make a few final observations about this algorithm: 1) Instead of using the weighted sum \(\widetilde{\mathbf{v}}_{T}\), we could output the weighted average \(\frac{\widetilde{\mathbf{v}}_{t}}{\sum_{s=1}^{q}\alpha_{s}}\) without altering the margin or directional convergence rate. This is attributed to the scale-invariance of the margin, i.e., \(\forall c>0,\mathbf{w}\in\mathbb{R}^{d}\), \(\widetilde{\gamma}(\mathbf{w})=\widetilde{\gamma}(c\mathbf{w})\). The same argument applies to the directional error. 2) The use of the weighted average is standard in the analysis of mirror descent (e.g., Section 4.2 of Bubeck et al. (2015)). This paper shows that using non-uniform weights is advantageous for achieving rapid margin maximization rates; 3) The per-round computational complexity of (5) is \(O(d)\), which is similar to that of \(p\)-mirror-descent of Sun et al. (2022). However, we note that the \(p\)-MD algorithm of Sun et al. (2022) does not need to compute the norm of the decision at each round, which could be more efficient in real-world applications where parallel or distributed computation is desired.

For Algorithm 1, we have the following theorem. We present its proof in Appendix C, along with a more general theorem that allows a general configuration of the parameters \(\eta_{t}\), \(\alpha_{t}\) and \(\beta_{t}\).

**Theorem 2**.: _Suppose Assumption 1 holds wrt \(\|\cdot\|_{q}\)-norm for \(q\in(1,2]\). For the left box of Algorithm 1, let \(\eta_{t}=\frac{1}{L(\mathbf{v}_{t-1})}\). For the right box, let \(\alpha_{t}=1\), and \(\beta_{1}=1\), \(\beta_{t}=\frac{1}{t-1}\). Then the methods in the two boxes of Algorithm 1 are identical, in the sense that \(\widetilde{\mathbf{v}}_{T}=\widetilde{\mathbf{w}}_{T}\). Moreover, we have the average regret upper bound \(C_{T}=\frac{\left(\frac{2}{q-1}+2\log n\right)(\log T+2)}{T}\). Therefore, the algorithm achieves a positive margin when \(T\) is sufficiently large such that \(T\geq\frac{4\left(\frac{2}{q-1}+2\log n\right)(\log T+2)}{\gamma^{2}}\). We have the convergence rate_

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\widetilde{ \mathbf{v}}_{T}}{\left\|\widetilde{\mathbf{v}}_{T}\right\|_{q}}\geq\gamma- \frac{4\left(\frac{2}{q-1}+2\log n\right)(\log T+2)}{\gamma^{2}T}=\gamma-O \left(\frac{\log n\log T}{(q-1)\gamma^{2}T}\right),\] (6)

_and_

\[\left\|\frac{\widetilde{\mathbf{v}}_{T}}{\left\|\widetilde{\mathbf{v}}_{T} \right\|_{q}}-\mathbf{w}_{\|\cdot\|_{q}}^{*}\right\|_{q}\leq\frac{8\sqrt{2}}{ \gamma^{2}\sqrt{(q-1)}}\sqrt{\frac{\left(\frac{2}{q-1}+2\log n\right)(\log T +2)}{T}}=O\left(\frac{\sqrt{\log n\log T}}{\gamma^{2}(q-1)\sqrt{T}}\right).\]

The first part of Theorem 2 indicates that the mirror descent algorithm can be described as two players using certain cleverly designed online learning algorithms to solve the _regularized_ bilinear game in (2). More specifically, for the \(\mathbf{p}\)-player, we propose a new and unusual online learning algorithm, which we call _regularized greedy_.

\[\mathbf{p}_{t}=\operatorname*{argmin}_{\mathbf{p}\in\Delta^{n}}\alpha_{t}\ell _{t-1}(\mathbf{p})+\beta_{t}D_{\text{KL}}\left(\mathbf{p},\frac{\mathbf{1}}{n }\right).\]

Essentially, in round \(t\), the \(\mathbf{p}\)-player minimizes the previous round's loss function, \(\ell_{t-1}\), plus a regularizer at round \(t\), and the two terms are balanced by the parameters \(\alpha_{t}\) and \(\beta_{t}\). On the other hand, we select the _follow-the-leader_\({}^{+}\) algorithm for the \(\mathbf{w}\)-player:

\[\mathbf{w}_{t}=\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R}^{d}}\sum_{j=1} ^{t}\alpha_{j}h_{j}(\mathbf{w}),\]

which returns the solution that minimize the cumulative loss so far. The \(+\) sign in the name is because the algorithm can pick the decision \(\mathbf{w}_{t}\)_after_ seeing its loss function. This is an interesting and unusual design because the regularized greedy algorithm will clearly suffer a worst-case _linear_ regret for the \(\mathbf{p}\)-player. Fortunately, for our specific case we are able to prove a sharper _data-dependent_ regret bound for the \(\mathbf{p}\)-player as below:

\[\text{Reg}_{T}^{\mathbf{p}}=O\left(\sum_{t=2}^{T}\frac{(t-1)(q-1)}{2}\| \mathbf{w}_{t}-\mathbf{w}_{t-1}\|_{q}^{2}+\log n\log T\right),\]

Therefore, the dominating term (i.e., the first term above) of the \(\mathbf{p}\)-player's regret bound can be canceled by the \(\mathbf{w}\)-player's regret bound, given by:

\[\text{Reg}_{T}^{\mathbf{w}}=O\left(-\sum_{t=2}^{T}\frac{(t-1)(q-1)}{2}\| \mathbf{w}_{t}-\mathbf{w}_{t-1}\|_{q}^{2}\right).\]

Note that the \(\mathbf{w}\)-player's regret bound is negative as the corresponding algorithm used is _clairvoyant_, i.e. can see the current loss \(\ell_{t}\) before making a decision at round \(t\). This ensures that sublinear (and more generally fast) rates are possible. Note that \(\beta_{t}\) and \(\alpha_{t}\) will influence both the regret bound and the algorithm equivalence analysis, so finding the right parameter configuration that works for both is a non-trivial task. We make the choice \(\beta_{t}=\frac{\alpha_{t}}{\sum_{i=1}^{t-1}\alpha_{i}}\), which ensures both algorithmic equivalence and sublinear regret bounds.

The second part of Theorem 2 shows that the average regret \(C_{T}\) of Algorithm 1 is on the order of \(O\left(\frac{\log n\log T}{(q-1)\gamma^{2}T}\right)\). Therefore, by plugging in Theorem 1, we observe that the margin shrinks on the order of \(\gamma-O\left(\frac{\log n\log T}{\gamma^{2}(q-1)T}\right)\), and the implicit bias convergence rate is \(O\left(\frac{\log n\log T}{\gamma^{2}(q-1)\sqrt{T}}\right).\) Next, we show an improved rate with a more aggressive step size on the order of \(O\left(\frac{t}{L(\mathbf{v}_{t})}\right)\) instead of \(O\left(\frac{1}{L(\mathbf{v}_{t})}\right)\). The proof of this result is given in Appendix C.

**Theorem 3**.: _Suppose Assumption 1 holds wrt \(\|\cdot\|_{q}\)-norm for \(q\in(1,2]\). For the left box of Algorithm 1, let \(\eta_{t}=\frac{t}{L(\mathbf{v}_{t-1})}\), and let the final output be \(\widetilde{\mathbf{v}}_{T}=\sum_{t=1}^{T}\frac{2}{t+1}\mathbf{v}_{t}\). For the right box, let \(\alpha_{t}=t\), and \(\beta_{1}=1\), \(\beta_{t}=\frac{2}{t-1}\). Then the two algorithms are identical, in the sense that \(\widetilde{\mathbf{v}}_{T}=\widetilde{\mathbf{w}}_{T}\). Moreover, when \(T\geq\sqrt{\frac{8\left[\frac{4T}{4-1}+4\log n\log T+1+2\log n\right]}{\gamma^ {2}}}\), we have_

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\widetilde{ \mathbf{v}}_{T}}{\left\|\widetilde{\mathbf{v}}_{T}\right\|_{q}}\geq\gamma- \frac{32}{\gamma^{2}T(q-1)}-\frac{8(4\log n\log T+1+2\log n)}{\gamma^{2}T^{2}},\] (7)

_and_

\[\left\|\frac{\widetilde{\mathbf{v}}_{T}}{\left\|\widetilde{\mathbf{v}}_{T} \right\|_{q}}-\mathbf{w}_{\|\cdot\|_{q}}^{*}\right\|_{q}\leq\frac{8\sqrt{2}}{ \gamma^{2}\sqrt{q-1}}\sqrt{\frac{8}{(q-1)T}+\frac{4\log n\log T+2\log n+1}{T^ {2}}}.\]

Observe that the margin maximization rate in Theorem 3 is \(O\left(\frac{1}{(q-1)\gamma^{2}T}\right)+O\left(\frac{\log n\log T}{\gamma^{2} T^{2}}\right)\). Compared to (6), it has a better dependence on \(\log n\) and \(\log T\).

Finally, we focus on a momentum-based mirror descent algorithm, which is given in Algorithm 2. For this algorithm, we have the following guarantee.

**Theorem 4**.: _Suppose Assumption 1 holds wrt \(\|\cdot\|_{q}\)-norm for \(q\in(1,2]\). For the left box of Algorithm 2, let \(\eta_{t}=\frac{t}{L(\mathbf{v}_{t-1})}\), and \(\widehat{\eta}_{t}=\frac{t-1}{L(\mathbf{v}_{t-1})}\). For the second box, let \(\alpha_{t}=t\), and \(\beta_{t}=\frac{2}{t+1}\). Then the methods in the two boxes of Algorithm 2 are identical, in the sense that \(\widetilde{\mathbf{v}}_{T}=\widetilde{\mathbf{w}}_{T}\). Moreover, when \(T\geq\frac{\sqrt{8\left(4\log n\log T+\frac{2T}{q-1}\right)}}{\gamma}\), we have_

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\widetilde{ \mathbf{v}}_{T}}{\left\|\widetilde{\mathbf{v}}_{T}\right\|_{q}}\geq\gamma-\frac {\sum_{t=1}^{T}\|\mathbf{p}_{t}-\mathbf{p}_{t-1}\|_{1}^{2}}{\gamma^{2}(q-1)T^ {2}}-\frac{32\log n\log T}{\gamma^{2}T^{2}}\] (8)

_and_

\[\left\|\frac{\widetilde{\mathbf{v}}_{T}}{\left\|\widetilde{\mathbf{v}}_{T} \right\|_{q}}-\mathbf{w}_{\|\cdot\|_{q}}^{*}\right\|_{q}\leq\frac{8\sqrt{2}}{ \gamma^{2}\sqrt{(q-1)}}\sqrt{\frac{\sum_{t=1}^{T}\|\mathbf{p}_{t}-\mathbf{p}_{t -1}\|_{1}^{2}}{(q-1)T^{2}}+\frac{32\log n\log T}{T^{2}}}.\]

The above theorem shows that, for sufficiently large \(T\), the margin maximization rate can be data-dependent. Note that \(\sum_{t=1}^{T}\|\mathbf{p}_{t}-\mathbf{p}_{t-1}\|_{1}^{2}\leq 2T\), so in the worst case, the bound reduces to the results in Theorem 3, but it can become significantly better when \(\sum_{t=1}^{T}\|\mathbf{p}_{t}-\mathbf{p}_{t-1}\|_{1}^{2}\) is small. We expect that when \(T\) is very large, \(\mathbf{p}_{T}\) will change very slowly as we already know that the direction of \(\widetilde{\mathbf{v}}_{T}\) will converge -- however, turning this into a precise faster rate dependent on the original training data geometry, i.e. \(\mathbf{A}\), is an intriguing open question.

[MISSING_PAGE_FAIL:9]

the method maintains a momentum term \(\mathbf{g}_{t}\) with an additional gradient (Step 2), then identifies the steepest direction with respect to \(\mathbf{g}_{t}\) (Step 3), and applies this direction to update the decision (Step 4). At first glance, these two algorithms appear markedly different. However, we show that with appropriately chosen parameters, they are actually equivalent, in the sense that they both correspond to the online dynamic in the bottom box of Algorithm 4. More specifically, we provide the following theoretical guarantee. The proof is given in Appendix E.

**Theorem 6**.: _Suppose Assumption 1 holds wrt a general norm \(\|\cdot\|\), and \(\frac{1}{2}\|\cdot\|^{2}\) is \(\lambda\)-strongly convex wrt \(\|\cdot\|\). For the left box, let \(\beta_{t,1}=\frac{\lambda}{4}\), \(\beta^{\prime}_{t,1}=\frac{\lambda}{2(t-1)}\), \(\beta_{t,2}=1\), \(\beta^{\prime}_{t,2}=\frac{2}{t+1}\), and \(\eta_{t}=\frac{t}{L(\mathbf{v}_{t})}\). For the right box, let \(\beta_{t,3}=\frac{t-1}{t+1}\), \(\beta_{t,4}=\frac{\lambda}{4}\), \(\beta^{\prime}_{t,4}=\frac{\lambda t\|\mathbf{g}_{t-1}\|_{*}}{4}\), \(\beta^{\prime}_{t,3}=\frac{2}{(t+1)L(\beta_{t,4}\mathbf{v}_{t-1}+\beta^{ \prime}_{t,4}\mathbf{g}_{t-1})}\), and \(\eta_{t}=t\|\mathbf{g}_{t}\|_{*}\). For the bottom box, let \(c=\frac{\lambda}{4}\), \(\alpha_{t}=t\). Then all three methods in Algorithm 4 are identical, in the sense that \(\widetilde{\mathbf{v}}_{T}=\mathbf{v}_{T}=\widetilde{\mathbf{w}}_{T}\). Moreover, when \(T\geq\frac{4\sqrt{2\log n}}{\sqrt{\lambda\gamma}}\), we have_

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}A\widetilde{\mathbf{w}}_{ T}}{\|\widetilde{\mathbf{w}}_{T}\|}\geq\gamma-\frac{32\log n}{\gamma^{2}T^{2} \lambda},\]

_and_

\[\left\|\frac{\mathbf{v}_{T}}{\|\mathbf{v}_{T}\|}-\mathbf{w}^{*}_{\|\cdot\|} \right\|\leq\frac{32\sqrt{\log n}}{\gamma^{2}\lambda T}.\]

RemarkTheorem 6 reveals that the two strategies implemented in Algorithm 4 yield an optimal \(O(\log n/[\gamma^{2}T^{2}])\) rate. It is worth noting that a similar online dynamic to the one detailed in the bottom box of Algorithm 4 was also considered by (Algorithm 5, Wang et al., 2022b). Nonetheless, there are some crucial distinctions: 1) Their work only demonstrated that this dynamic could achieve a positive margin, leaving open questions regarding whether the margin can be maximized (i.e., converge to \(\gamma\)), and if so, what the margin maximization rate would be; 2) They only presented the online dynamic, without its equivalent optimization form. Finally, we note that, Theorem 6 requires the norm to be strongly convex, which is satisfied for, e.g. the \(q\)-norm when \(q\in(1,2]\).

## 5 Conclusion and Future Work

This paper examines the implicit bias of generic optimization methods, delivering accelerated margin maximization and directional errors for average mirror descent and steepest descent. Our approach translates _generic optimization methods for ERM_ into _online learning dynamics for a regularized bilinear game_, offering a simpler analysis and a fresh perspective on implicit bias. Despite the effectiveness of the game framework in handling generic methods and accelerated techniques, it presently holds some limitations: 1) the framework is currently operational only for exponential loss, making its extension to handle more general losses a vital area for future research; 2) identifying algorithmic equivalence is nuanced and non-trivial, and it is as yet unresolved whether this framework can elucidate other methods, such as the last-iterate of MD; 3) Finally, it remains to be seen whether more advanced online learning algorithms are beneficial under our framework, such as parameter-free online learning (Orabona & Pal, 2016; Cutkosky & Orabona, 2018) or adaptive online learning methods (van Erven & Koolen, 2016; Wang et al., 2019; Zhang et al., 2019; Wang et al., 2021a).

**Acknowledgments.** GW was supported by a ARC-ACO fellowship provided by Georgia Tech. VM was supported by the NSF (through CAREER award CCF-2239151 and award IIS-2212182), an Adobe Data Science Research Award, an Amazon Research Award and a Google Research Colabs Award. JA was supported by the AI4OPT Institute, as part of NSF Award 2112533, and NSF through Award IIS-1910077.

## References

* Abernethy et al. (2018) Abernethy, J., Lai, K. A., Levy, K. Y., and Wang, J.-K. Faster rates for convex-concave games. In _Proceedings of the 31st Annual Conference on Learning Theory_, pp. 1595-1625, 2018.
* Azizan & Hassibi (2019) Azizan, N. and Hassibi, B. Stochastic gradient/mirror descent: Minimax optimality and implicit regularization. In _the 7th International Conference on Learning Representations_, 2019.
* Boyd & Vandenberghe (2004) Boyd, S. and Vandenberghe, L. _Convex Optimization_. Cambridge University Press, 2004.
* Bubeck et al. (2015) Bubeck, S. et al. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* Chen et al. (2001) Chen, S. S., Donoho, D. L., and Saunders, M. A. Atomic decomposition by basis pursuit. _SIAM review_, 43(1):129-159, 2001.
* Cutkosky & Orabona (2018) Cutkosky, A. and Orabona, F. Black-box reductions for parameter-free online learning in banach spaces. In _Proceedings of the 31st Annual Conference on Learning Theory_, 2018.
* Daskalakis & Panageas (2019) Daskalakis, C. and Panageas, I. Last-iterate convergence: Zero-sum games and constrained min-max optimization. In _10th Innovations in Theoretical Computer Science Conference_, 2019.
* Daskalakis et al. (2018) Daskalakis, C., Ilyas, A., Syrgkanis, V., and Zeng, H. Training gans with optimism. In _the 6th International Conference on Learning Representations_, 2018.
* Gentile (2000) Gentile, C. A new approximate maximal margin classification algorithm. _Advances in Neural Information Processing Systems 13_, 2000.
* Gunasekar et al. (2018a) Gunasekar, S., Lee, J., Soudry, D., and Srebro, N. Characterizing implicit bias in terms of optimization geometry. In _Proceedings of the 35th International Conference on Machine Learning_, pp. 1832-1841, 2018a.
* Gunasekar et al. (2018b) Gunasekar, S., Lee, J. D., Soudry, D., and Srebro, N. Implicit bias of gradient descent on linear convolutional networks. _Advances in neural information processing systems 31_, 2018b.
* Hazan (2016) Hazan, E. Introduction to online convex optimization. _Foundations and Trends in Optimization_, 2(3-4):157-325, 2016.
* Ji & Telgarsky (2018) Ji, Z. and Telgarsky, M. Risk and parameter convergence of logistic regression. _arXiv preprint arXiv:1803.07300_, 2018.
* Ji & Telgarsky (2020a) Ji, Z. and Telgarsky, M. Gradient descent aligns the layers of deep linear networks. In _the 8th International Conference on Learning Representations_, 2020a.
* Ji & Telgarsky (2020b) Ji, Z. and Telgarsky, M. Directional convergence and alignment in deep learning. _Advances in Neural Information Processing Systems 33_, 2020b.
* Ji & Telgarsky (2021) Ji, Z. and Telgarsky, M. Characterizing the implicit bias via a primal-dual analysis. In _Proceedings of the 32nd International Conference on Algorithmic Learning Theory_, pp. 772-804, 2021.
* Ji et al. (2020) Ji, Z., Dudik, M., Schapire, R. E., and Telgarsky, M. Gradient descent follows the regularization path for general losses. In _Proceedings of the 33rd Annual Conference on Learning Theory_, pp. 2109-2136. PMLR, 2020.
* Ji et al. (2021) Ji, Z., Srebro, N., and Telgarsky, M. Fast margin maximization via dual acceleration. In _Proceedings of the 38th International Conference on Machine Learning_, pp. 4860-4869, 2021.
* Lai & Muthukumar (2023) Lai, K.-W. and Muthukumar, V. General loss functions lead to (approximate) interpolation in high dimensions. _arXiv preprint arXiv:2303.07475_, 2023.
* Lai & Toth (2018)Li, Y., Ju, C., Fang, E. X., and Zhao, T. Implicit regularization of bregman proximal point algorithm and mirror descent on separable data. _arXiv preprint arXiv:2108.06808_, 2021.
* Lyu and Li (2020) Lyu, K. and Li, J. Gradient descent maximizes the margin of homogeneous neural networks. In _the 8th International Conference on Learning Representations_, 2020.
* Nacson et al. (2019) Nacson, M. S., Lee, J., Gunasekar, S., Savarese, P. H. P., Srebro, N., and Soudry, D. Convergence of gradient descent on separable data. In _Proceeding of the 22nd International Conference on Artificial Intelligence and Statistics_, pp. 3420-3428, 2019.
* Nesterov (1988) Nesterov, Y. On an approach to the construction of optimal methods of minimization of smooth convex functions. _Ekonomika i Mateaticheskie Metody_, 24(3):509-517, 1988.
* Neyshabur et al. (2014) Neyshabur, B., Tomioka, R., and Srebro, N. In search of the real inductive bias: On the role of implicit regularization in deep learning. _arXiv preprint arXiv:1412.6614_, 2014.
* Orabona (2019) Orabona, F. A modern introduction to online learning. _arXiv preprint arXiv:1912.13213_, 2019.
* Orabona and Pal (2016) Orabona, F. and Pal, D. Coin betting and parameter-free online learning. _Advances in Neural Information Processing Systems 29_, 2016.
* Rakhlin and Sridharan (2013) Rakhlin, S. and Sridharan, K. Optimization, learning, and games with predictable sequences. In _Advances in Neural Information Processing Systems 26_, pp. 3066-3074, 2013.
* Ramdas and Pena (2016) Ramdas, A. and Pena, J. Towards a deeper geometric, analytic and algorithmic understanding of margins. _Optimization Methods and Software_, 31(2):377-391, 2016.
* Soheili and Pena (2012) Soheili, N. and Pena, J. A smooth perceptron algorithm. _SIAM Journal on Optimization_, 22(2):728-737, 2012.
* Soudry et al. (2018) Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., and Srebro, N. The implicit bias of gradient descent on separable data. _The Journal of Machine Learning Research_, 19(1):2822-2878, 2018.
* Sun et al. (2022) Sun, H., Ahn, K., Thrampoulidis, C., and Azizan, N. Mirror descent maximizes generalized margin and can be implemented efficiently. In _Advances in Neural Information Processing Systems 35_, 2022.
* Telgarsky (2013) Telgarsky, M. Margins, shrinkage, and boosting. In _International Conference on Machine Learning_, pp. 307-315. PMLR, 2013.
* Tseng (2008) Tseng, P. On accelerated proximal gradient methods for convex-concave optimization. _submitted to SIAM Journal on Optimization_, 2(3), 2008.
* van Erven and Koolen (2016) van Erven, T. and Koolen, W. M. MetaGrad: Multiple learning rates in online learning. In _Advances in Neural Information Processing Systems 29_, pp. 3666-3674, 2016.
* Vardi (2022) Vardi, G. On the implicit bias in deep-learning algorithms. _arXiv preprint arXiv:2208.12591_, 2022.
* Wang et al. (2022a) Wang, B., Meng, Q., Zhang, H., Sun, R., Chen, W., Ma, Z.-M., and Liu, T.-Y. Does momentum change the implicit regularization on separable data? _Advances in Neural Information Processing Systems 35_, 2022a.
* Wang et al. (2019) Wang, G., Lu, S., and Zhang, L. Adaptivity and optimality: A universal algorithm for online convex optimization. In _Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence_, 2019.
* Wang et al. (2021a) Wang, G., Wan, Y., Yang, T., and Zhang, L. Online convex optimization with continuous switching constraint. _Advances in Neural Information Processing Systems_, 34:28636-28647, 2021a.
* Wang et al. (2022b) Wang, G., Hanashiro, R., Guha, E., and Abernethy, J. On accelerated perceptrons and beyond. _arXiv preprint arXiv:2210.09371_, 2022b.
* Wang and Abernethy (2018) Wang, J.-K. and Abernethy, J. D. Acceleration through optimistic no-regret dynamics. In _Advances in Neural Information Processing Systems 32_, pp. 3824-3834, 2018.
* Wang et al. (2019)Wang, J.-K., Abernethy, J., and Levy, K. Y. No-regret dynamics in the fenchel game: A unified framework for algorithmic convex optimization. _arXiv preprint arXiv:2111.11309_, 2021b.
* Zhang et al. (2021) Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* Zhang et al. (2019) Zhang, L., Wang, G., Tu, W.-W., and Zhou, Z.-H. Dual adaptivity: A universal algorithm for minimizing the adaptive regret of convex functions. _ArXiv e-prints_, arXiv:1906.10851, 2019.
* Zhang et al. (2022) Zhang, M., Zhao, P., Luo, H., and Zhou, Z.-H. No-regret learning in time-varying zero-sum games. In _Proceedings of the 39th International Conference on Machine Learning_, 2022.

Additional Related Work

As discussed in the introduction, the implicit bias of GD-based methods on linear spreadable data with exponentially-tailed loss has been extensively studied (Soudry et al., 2018; Ji and Telgarsky, 2018; Nacson et al., 2019; Ji and Telgarsky, 2021; Ji et al., 2021; Wang et al., 2022b). For \(\ell_{1}\)-norm, Telgarsky (2013) provides an \(O(\frac{1}{\sqrt{T}})\) margin maximization rate, and an \(O(\frac{1}{T})\) convergence rate to a sub-optimal margin. The idea of using the regularized game framework for analyzing the implicit bias of Nesterov-accelerated GD is firstly considered in Wang et al. (2022b). This idea is motivated by the smooth perceptron analysis of Soheili and Pena (2012), and the the Fenchel game framework by Wang et al. (2021b). The idea also inspired by the primal-dual analysis of Ji and Telgarsky (2021), who discovered the relationship between the normalized gradient descent for the exponential loss and a smooth update of a distribution over data. Apart from the setting above, the implicit bias of GD-based optimization methods have also been studied in other cases, such as minimizing more general loss functions (Ji et al., 2020; Ji and Telgarsky, 2021; Lai and Muthukumar, 2023), and deep neural networks (Gunasekar et al., 2018b; Ji and Telgarsky, 2020a,b; Lyu and Li, 2020; Vardi, 2022).

Compared with GD-based methods, the implicit bias of generic optimization methods for linear classification is less understood. Apart from the work summarized in the introduction (Gunasekar et al., 2018a; Nacson et al., 2019; Li et al., 2021; Sun et al., 2022), the implicit bias of mirror descent is also studied in regression problems (Gunasekar et al., 2018a; Azizan and Hassibi, 2019). We note that, as indicated in Gunasekar et al. (2018a) and Sun et al. (2022), the analysis of implicit bias for classification and regression are "fundamentally different". Apart from steepest and mirror descent, the implicit bias of other generic methods, such as Adagrad and Adam are also studied (Gunasekar et al., 2018a; Wang et al., 2022a)

Discussion on large enough \(T\)Our Theorem 1 indicates that to ensure the margin can be maximized, the number of iterations \(T\) has to be large enough such that \(C_{T}\leq\frac{\gamma^{2}}{4}\). We note that, similar conditions are also required (explicitly or implicitly) in other work for analyzing the margin maximization rates for generic optimization methods. For example, in the proof of Theorem 8 of Nacson et al. (2019), in order to combine the upper bound in (53) and lower bound in (54), one need to make sure the LHS of (53), i.e., the margin, is non-negative, which essentially hinge on the condition that \(\sqrt{T}=\Omega\left(\frac{\log n\log T}{\gamma^{2}}\right)\), similar to what is required in our Corollary 1. In Sun et al. (2022), page 21, to make sure the term \(1+\frac{\log n}{m_{t}}\) be a constant, \(T\) has to be large enough such that \(m_{t}\), the margin, goes to \(\infty\). In Li et al. (2021), the requirements for a large enough \(T\) is stated in the main theorem (e.g., Theorem 4.2).

Comparison with Wang et al. (2022b)As discussed in the introduction, our work is motivated by Wang et al. (2022b). Compared with Wang et al. (2022b), we note that: 1) Wang et al. (2022b) draws the connection between Nesterov-accelerated GD for ERM and solving the bilinear game through an online dynamic. However, it was unclear whether this kind of analysis suits other gradient-descent-based methods, and generic optimization methods such as mirror descent/steepest descent was not addressed at all. We observe in this work that the non-linearity of the mirror map in generic optimization methods, such as mirror descent and steepest descent, makes the analysis particularly challenging. In this paper, we reveal that the game framework can in fact encompass implicit bias analysis for a range of generic optimization methods, and offer a more streamlined and unified analysis. 2) Wang et al. (2022b) also proposed an accelerated p-norm perceptron problem. However, they only demonstrated that the algorithm could achieve a non-negative margin, leaving open questions regarding whether the margin can be maximized, and if so, what the margin maximization rate would be; 2) They only presented the online dynamic, without its equivalent optimization form under ERM.

## Appendix B Proof of Theorem 1

Define \(m(\mathbf{w})=\min_{\mathbf{p}\in\Delta^{n}}g(\mathbf{p},\mathbf{w})\), \(\overline{\mathbf{w}}_{T}=\frac{1}{\sum_{t=1}^{T}\alpha_{t}}\sum_{t=1}^{T} \alpha_{t}\mathbf{w}_{t}=\frac{1}{\sum_{t=1}^{T}\alpha_{t}}\widetilde{\mathbf{ w}}_{T}\), and we introduce following lemma, which shows that using online learning for solving the game defined in (2) maximizes \(m(\mathbf{w})\).

**Lemma 1** (Abernethy et al. (2018)).: _Consider solving the game defined in (2) with the online learning dynamic defiend in Protocol 1. We have that \(\forall\mathbf{w}\in\mathbb{R}^{d}\),_

\[m(\mathbf{w})-m(\overline{\mathbf{w}}_{T})\leq\frac{\text{Reg}_{T}^{\mathbf{p} }+\text{Reg}_{T}^{\mathbf{w}}}{\sum_{t=1}^{T}\alpha_{t}}.\]

Based on Lemma 1 and the definition of \(m(\cdot)\), we have

\[\begin{split} m\left(\overline{\mathbf{w}}_{T}\right)=m\left( \frac{\widetilde{\mathbf{w}}_{T}}{\sum_{t=1}^{T}\alpha_{t}}\right)& =\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\frac{ \widetilde{\mathbf{w}}_{T}}{\sum_{t=1}^{T}\alpha_{t}}-\frac{1}{2}\left\|\frac {\widetilde{\mathbf{w}}_{T}}{\sum_{t=1}^{T}\alpha_{t}}\right\|^{2}\\ &\geq m\left(\left\|\frac{\widetilde{\mathbf{w}}_{T}}{\sum_{t=1} ^{T}\alpha_{t}}\right\|\mathbf{w}_{\|\cdot\|}^{*}\right)-\frac{\text{Reg}_{T}^ {\mathbf{p}}+\text{Reg}_{T}^{\mathbf{w}}}{\sum_{t=1}^{T}\alpha_{t}}\\ &=\gamma\left\|\frac{\widetilde{\mathbf{w}}_{T}}{\sum_{t=1}^{T} \alpha_{t}}\right\|-\frac{1}{2}\left\|\frac{\widetilde{\mathbf{w}}_{T}}{\sum_{ t=1}^{T}\alpha_{t}}\right\|^{2}-\frac{\text{Reg}_{T}^{\mathbf{p}}+\text{Reg}_{T}^{ \mathbf{w}}}{\sum_{t=1}^{T}\alpha_{t}},\end{split}\] (9)

which implies that

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\widetilde{ \mathbf{w}}_{T}}{\|\widetilde{\mathbf{w}}_{T}\|}\geq\gamma-\frac{\text{Reg}_{ T}^{\mathbf{p}}+\text{Reg}_{T}^{\mathbf{w}}}{\|\widetilde{\mathbf{w}}_{T}\|}.\] (10)

The above proof follows the main idea in Wang et al. (2022b). Next, we turn to lower bound \(\|\widetilde{\mathbf{w}}_{T}\|\). Note that since \(\mathbf{w}_{T}\) (and also \(\widetilde{\mathbf{w}}_{T}\)) does not have a simple explicit form, the the technique for lower bounding the norm in (Wang et al., 2022b) fails and we need to find a new approach. Let \((\mathbf{x},y)\in\{(\mathbf{x}^{(i)},y^{(i)})\}_{i=1}^{n}\) be a data point. We have

\[\|\widetilde{\mathbf{w}}_{T}\|\geq\|y\mathbf{x}\|_{*}\|\widetilde{\mathbf{w}} _{T}\|\geq y\mathbf{x}^{\top}\widetilde{\mathbf{w}}_{T}\geq\min_{\mathbf{p} \in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\widetilde{\mathbf{w}}_{T},\] (11)

where the first inequality is due to assumption that \(\|\mathbf{x}\|_{*}\leq 1\), the second inequality is derived from the Cauchy-Schwarz inequality. To proceed, we need a lower bound on the unormalized margin of \(\widetilde{\mathbf{w}}_{T}\). We have

\[\begin{split} m\left(\overline{\mathbf{w}}_{T}\right)=m\left( \frac{\widetilde{\mathbf{w}}_{T}}{\sum_{t=1}^{T}\alpha_{t}}\right)& =\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\frac{ \widetilde{\mathbf{w}}_{T}}{\sum_{t=1}^{T}\alpha_{t}}-\frac{1}{2}\left\|\frac {\widetilde{\mathbf{w}}_{T}}{\sum_{t=1}^{T}\alpha_{t}}\right\|^{2}\\ &\geq m\left(\gamma\mathbf{w}_{\|\cdot\|}^{*}\right)-\frac{\text{ Reg}_{T}^{\mathbf{p}}+\text{Reg}_{T}^{\mathbf{w}}}{\sum_{t=1}^{T}\alpha_{t}}\\ &=\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A} \mathbf{w}_{\|\cdot\|}^{*}-\frac{1}{2}\|\gamma\mathbf{w}_{\|\cdot\|}^{*}\|^{2} -\frac{\text{Reg}_{T}^{\mathbf{p}}+\text{Reg}_{T}^{\mathbf{w}}}{\sum_{t=1}^{T} \alpha_{t}}\\ &=\frac{\gamma^{2}}{2}-\frac{\text{Reg}_{T}^{\mathbf{p}}+\text{ Reg}_{T}^{\mathbf{w}}}{\sum_{t=1}^{T}\alpha_{t}},\end{split}\] (12)

where for the first inequality we apply Lemma 1 and compare \(m(\widetilde{\mathbf{w}}_{T})\) with that of \(\gamma\mathbf{w}_{\|\cdot\|}^{*}\), and the last equality is derived based on Assumption 1 (the margin of \(\mathbf{w}_{\|\cdot\|}^{*}\) is \(\gamma\), and \(\|\mathbf{w}_{\|\cdot\|}^{*}\|=1\)).. (12) suggests that

\[\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\widetilde{\mathbf{ w}}_{T}\geq\underbrace{\frac{1}{2}\frac{\|\widetilde{\mathbf{w}}_{T}\|^{2}}{ \sum_{t=1}^{T}\alpha_{t}}}_{\geq 0}+\frac{\gamma^{2}}{2}\sum_{t=1}^{T}\alpha_{t}-\left(\text{Reg}_{T}^{ \mathbf{p}}+\text{Reg}_{T}^{\mathbf{w}}\right)\geq\frac{\gamma^{2}}{2}\sum_{t =1}^{T}\alpha_{t}-\left(\text{Reg}_{T}^{\mathbf{p}}+\text{Reg}_{T}^{\mathbf{w}} \right).\] (13)

Note that, to plug in the lower bound of \(\widetilde{\mathbf{w}}_{T}\), we need to ensure the RHS of (13) is positive. Notice that when \(\frac{\gamma^{2}}{2}\sum_{t=1}^{T}\alpha_{t}\geq 2\left(\text{Reg}_{T}^{ \mathbf{p}}+\text{Reg}_{T}^{\mathbf{p}}\right)\),

\[\|\widetilde{\mathbf{w}}_{T}\|\geq\frac{\gamma^{2}}{2}\sum_{t=1}^{T}\alpha_{t}- \left(\text{Reg}_{T}^{\mathbf{p}}+\text{Reg}_{T}^{\mathbf{w}}\right)\geq\frac{ \gamma^{2}}{4}\sum_{t=1}^{T}\alpha_{t}+\left[\frac{\gamma^{2}}{4}\sum_{t=1}^{T} \alpha_{t}-\left(\text{Reg}_{T}^{\mathbf{Combining (10), (11), and (13), we have

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\widetilde{\mathbf{ w}}_{T}}{\|\widetilde{\mathbf{w}}_{T}\|}\geq\gamma-\frac{4\left(\text{Reg}_{T}^{ \mathbf{p}}+\text{Reg}_{T}^{\mathbf{w}}\right)}{\gamma^{2}\sum_{t=1}^{T}\alpha _{t}}=\frac{4C_{T}}{\gamma^{2}}.\] (14)

Note that to apply (13), we need \(\frac{\gamma^{2}}{2}\sum_{t=1}^{T}\alpha_{t}-2\left(\text{Reg}_{T}^{\mathbf{ p}}+\text{Reg}_{T}^{\mathbf{w}}\right)\geq 0\).

Finally, we focus on the distance between \(\frac{\widetilde{\mathbf{w}}_{T}}{\|\widetilde{\mathbf{w}}_{T}\|}\) and \(\mathbf{w}_{\|\cdot\|}^{*}\) for the case where \(\Phi(\mathbf{w})\) is strongly convex wrt \(\|\cdot\|\). This part of the proof is motivated by Theorem 4 of Ramdas & Pena (2016), who show that a variant of the perceptron algorithm can converge to the \(\ell_{2}\)-maximum margin classifier in an \(O(1/\sqrt{t})\) convergence rate. We have

\[\left\|\frac{\widetilde{\mathbf{w}}_{T}}{\|\widetilde{\mathbf{w} }_{T}\|}-\mathbf{w}_{\|\cdot\|}^{*}\right\|=\left\|\frac{\overline{\mathbf{w} }_{T}}{\|\overline{\mathbf{w}}_{T}\|}-\mathbf{w}_{\|\cdot\|}^{*}\right\| =\frac{\|\overline{\mathbf{w}}_{T}-\|\overline{\mathbf{w}}_{T}\| \mathbf{w}_{\|\cdot\|}^{*}\|}{\|\overline{\mathbf{w}}_{T}\|}\] (15) \[=\frac{\|\overline{\mathbf{w}}_{T}-\gamma\mathbf{w}_{\|\cdot\|}^{ *}+\gamma\mathbf{w}_{\|\cdot\|}^{*}-\|\overline{\mathbf{w}}_{T}\|\mathbf{w}_{ \|\cdot\|}^{*}\|}{\|\mathbf{w}\|}\] \[\leq\frac{\|\overline{\mathbf{w}}_{T}-\gamma\mathbf{w}_{\|\cdot \|}^{*}\|+|\gamma-\|\overline{\mathbf{w}}_{T}\|}{\|\overline{\mathbf{w}}_{T}\|}\] \[=\frac{\|\overline{\mathbf{w}}_{T}-\gamma\mathbf{w}_{\|\cdot\|}^ {*}\|+\|\gamma\mathbf{w}_{\|\cdot\|}^{*}\|-\|\overline{\mathbf{w}}_{T}\|}{\| \overline{\mathbf{w}}_{T}\|}\] \[\leq\frac{2\|\overline{\mathbf{w}}_{T}-\gamma\mathbf{w}_{\|\cdot \|}^{*}\|}{\|\overline{\mathbf{w}}_{T}\|}\]

where the first inequality is based on the Minkowski inequality and the fact that \(\|\mathbf{w}_{\|\cdot\|}^{*}\|=1\). Next, note that \(m(\mathbf{w})\) is \(\lambda\)-strongly concave with respect to \(\|\cdot\|\), and \(\gamma\mathbf{w}_{\|\cdot\|}^{*}\) maximize \(m(\mathbf{w})\). This is because it is easy to see that the optimal solution of \(m(\mathbf{w})\) always lies in the direction of \(\mathbf{w}_{\|\cdot\|}^{*}\), and we only need to decide the norm. Let \(c>0\) be some constant, we have \(m\left(c\mathbf{w}_{\|\cdot\|}^{*}\right)=c\gamma-\frac{1}{2}c^{2}\). The function is maximized when \(c=\gamma\), which implies that the optimal solution is \(\gamma\mathbf{w}_{\|\cdot\|}^{*}\). Combining these facts with Lemma 1, we have

\[\frac{\lambda}{2}\|\overline{\mathbf{w}}_{T}-\gamma\mathbf{w}_{\|\cdot\|}^{*} \|^{2}\leq m(\gamma\mathbf{w}_{\|\cdot\|}^{*})-m(\overline{\mathbf{w}}_{T}) \leq\frac{\text{Reg}_{T}^{\mathbf{p}}+\text{Reg}_{T}^{\mathbf{w}}}{\sum_{t=1}^ {T}\alpha_{t}}.\] (16)

Finally, combining the lower bound of \(\overline{\mathbf{w}}_{T}\) proved in (11), we have when \(\frac{\gamma^{2}}{2}\sum_{t=1}^{T}\alpha_{t}-2\left(\text{Reg}_{T}^{\mathbf{ p}}+\text{Reg}_{T}^{\mathbf{w}}\right)\geq 0\),

\[\|\overline{\mathbf{w}}_{T}\|=\frac{1}{\sum_{t=1}^{T}\alpha_{t}}\|\widetilde{ \mathbf{w}}_{T}\|\geq\frac{1}{\sum_{t=1}^{T}\alpha_{t}}\left[\frac{\gamma^{2}} {4}\sum_{t=1}^{T}\alpha_{t}\right]=\frac{\gamma^{2}}{4},\]

so, combining the equation above with (15) and (16), we get

\[\left\|\frac{\widetilde{\mathbf{w}}_{T}}{\|\widetilde{\mathbf{w}}_{T}\|}- \mathbf{w}_{\|\cdot\|}^{*}\right\|\leq\frac{8\sqrt{2(\text{Reg}_{T}^{\mathbf{ p}}+\text{Reg}_{T}^{\mathbf{w}})}}{\gamma^{2}\sqrt{\lambda\sum_{t=1}^{T}\alpha_{t}}}.\] (17)

## Appendix C Omitted Proof in Section 4.1

In this section, we provide the omitted proof of Section 4.1. Here, we present a more general algorithm framework (given in Algorithm 5) which allows different step sizes. In the following, we first state a general theorem for this algorithm, and Theorems 2 and 3 is then given as Corollaries 2 and 3.

**Theorem 7**.: _Suppose Assumption 1 holds wrt \(\|\cdot\|_{q}\)-norm for \(q\in(1,2]\). For the left box of Algorithm 5, let \(\eta_{t}=\frac{\alpha_{t}}{L(\mathbf{v}_{t-1})}\). For the right box, let \(\beta_{t}\) be \(\frac{\alpha_{t}}{\sum_{i=1}^{t}\alpha_{i}}\) for \(t>1\), \(\beta_{1}=\alpha_{1}\). Then the methods in the two boxes of Algorithm 5 are identical, in the sense that \(\widetilde{\mathbf{v}}_{T}=\widetilde{\mathbf{w}}_{T}\), and \(\mathbf{v}_{t}=\mathbf{w}_{t}\cdot\sum_{i=1}^{t}\alpha_{i}\). Moreover, the regret upper bound_

\[\text{\rm Reg}_{T}^{\mathbf{p}} =2\sum_{t=2}^{T}\frac{\alpha_{t}^{2}}{\sum_{j=1}^{t-1}\alpha_{j}( q-1)}+\sum_{t=2}^{T}\frac{\sum_{j=1}^{t-1}\alpha_{j}(q-1)}{\cdot 2}\|\mathbf{w}_{t}- \mathbf{w}_{t-1}\|_{q}^{2}+2\log n\sum_{t=1}^{T}\beta_{t}+\alpha_{1},\] \[\text{\rm Reg}_{T}^{\mathbf{w}} =\]

_Thus, \(\widetilde{\mathbf{v}}_{T}\) achieves a positive margin (no smaller than \(\gamma^{2}/4\)) for sufficiently large \(T\) such that_

\[\frac{\gamma^{2}}{4}\sum_{t=1}^{T}\alpha_{t}\geq\left(2\sum_{t=2}^{T}\frac{ \alpha_{t}^{2}}{\sum_{j=1}^{t-1}\alpha_{j}(q-1)}+2\log n\sum_{t=2}^{T}\frac{ \alpha_{t}}{\sum_{i=1}^{t-1}\alpha_{i}}\right)+\alpha_{1}(1+2\log n).\] (18)

_After (18) is satisfied, the margin of \(\widetilde{\mathbf{v}}_{T}\) is lower bounded by_

\[\frac{\min_{\mathbf{p}\in\Delta^{\mathbf{n}}}\mathsf{p}^{\top}A \widetilde{\mathbf{v}}_{T}}{\|\widetilde{\mathbf{v}}_{T}\|_{q}}\geq\gamma- \frac{4\left[\left(2\sum_{t=2}^{T}\frac{\alpha_{t}^{2}}{\sum_{j=1}^{t-1}\alpha _{j}(q-1)}+2\log n\sum_{t=2}^{T}\frac{\alpha_{t}}{\sum_{i=1}^{t-1}\alpha_{i}} \right)+\alpha_{1}(1+2\log n)\right]}{\gamma^{2}\sum_{t=1}^{T}\alpha_{t}},\]

_and the directional error is_

\[\left\|\frac{\widetilde{\mathbf{w}}_{T}}{\|\widetilde{\mathbf{w} }_{T}\|_{q}}-\mathbf{w}_{\|\cdot\|}^{*}\right\|_{q}\] \[\leq\frac{8}{\gamma^{2}\sqrt{q-1}}\sqrt{\frac{2\left[\left(2\sum _{t=2}^{T}\frac{\alpha_{t}^{2}}{\sum_{j=1}^{t-1}\alpha_{j}(q-1)}+2\log n\sum_{t =2}^{T}\frac{\alpha_{t}}{\sum_{i=1}^{t-1}\alpha_{i}}\right)+\alpha_{1}(1+2\log n )\right]}{(q-1)\sum_{t=1}^{T}\alpha_{t}}}.\]

Next, we show that different step sizes (decided by \(\alpha_{t}\)) lead to different implicit bias convergence rates. Firstly, we consider setting the step size as \(\eta_{t}=\frac{1}{\sqrt{tL(\mathbf{v}_{t-1})}}\), which leads to a slow \(\widetilde{O}(\frac{1}{\gamma^{2}\sqrt{T}(q-1)})\) bound.

**Corollary 1**.: _Let \(\alpha_{t}=\frac{1}{\sqrt{t}}\). Then the margin is lower bounded by_

\[\frac{\min_{\mathbf{p}\in\Delta^{\mathbf{n}}}\mathsf{p}^{\top}\mathbf{A} \widetilde{\mathbf{v}}_{T}}{\|\widetilde{\mathbf{v}}_{T}\|_{q}}\geq\gamma- \frac{4\left(\left(\frac{2}{q-1}+\log n\right)\log T+4\log n+\frac{4}{q-1} \right)}{\gamma^{2}\sqrt{T}}=\gamma-\widetilde{O}\left(\frac{1}{\gamma^{2} \sqrt{T}(q-1)}\right),\]

_and_

\[\left\|\frac{\widetilde{\mathbf{w}}_{T}}{\|\widetilde{\mathbf{w}}_{T}\|_{q}}- \mathbf{w}_{\|\cdot\|}^{*}\right\|_{q}\leq\frac{8\sqrt{2}}{\gamma^{2}\sqrt{q-1 }}\sqrt{\frac{\left(\frac{2}{q-1}+\log n\right)\log T+4\log n+\frac{4}{q-1}}{ \sqrt{T}}}=\widetilde{O}\left(\frac{1}{(q-1)\gamma^{2}T^{1/4}}\right),\]

_when \(T\) is sufficiently large such that \(\sqrt{T}\geq\frac{4\left(\left(\frac{2}{q-1}+\log n\right)\log T+4\log n+\frac{ 4}{q-1}\right)}{\gamma^{2}}=\widetilde{O}\left(\frac{1}{(q-1)\gamma^{2}}\right)\)._

Next, we show that a faster rate can be obtained with a constant \(\alpha_{t}\), which implies Theorem 2.

[MISSING_PAGE_EMPTY:18]

Thus, \(\forall c>0\), \(c\nabla\Phi(\mathbf{w})=\nabla\Phi(c\mathbf{w})\). Then, multiplying \(c=\sum_{j=1}^{t}\alpha_{j}\) on both sides, we get

\[\nabla\Phi\left(\mathbf{w}_{t}\sum_{j=1}^{t}\alpha_{j}\right)=\nabla\Phi\left( \mathbf{w}_{t-1}\sum_{j=1}^{t-1}\alpha_{j}\right)+\alpha_{t}\mathbf{A}^{\top} \mathbf{p}_{t}.\]

On the other hand, for the \(\mathbf{p}\)-player, we have

\[\mathbf{p}_{t}=\operatorname*{argmin}_{\mathbf{p}\in\Delta^{n}}\alpha_{t} \ell_{t-1}(\mathbf{p})+\beta_{t}D_{\text{KL}}\left(\mathbf{p},\frac{\mathbf{1 }}{n}\right)=\operatorname*{argmin}_{\mathbf{p}\in\Delta^{n}}\frac{\alpha_{t }}{\beta_{t}}\mathbf{p}^{\top}\mathbf{A}\mathbf{w}_{t-1}+\sum_{i=1}^{n}p_{i} \log\frac{p_{i}}{\frac{1}{n}}.\]

Based on a standard argument on the relationship between OMD with the negative entropy regularizer on the simplex (see, e.g., Section 6.6 of Orabona, 2019), it is easy to verify that \(\forall i\in[n],t\in[T]\),

\[p_{t,i}=\frac{\exp(-\frac{\alpha_{t}}{\beta_{t}}y^{(i)}\mathbf{x}^{(i)\top} \mathbf{w}_{t-1})}{\sum_{j=1}^{n}\exp(-\frac{\alpha_{t}}{\beta_{t}}y^{(j)} \mathbf{x}^{(j)\top}\mathbf{w}_{t-1})},\]

where \(p_{t,i}\) is the \(i\)-th element of \(\mathbf{p}_{t}\). Moreover, based on the definition of \(L(\mathbf{w})\), for any \(\mathbf{w}\in\mathbb{R}^{d}\),

\[\frac{\nabla L(\mathbf{w})}{L(\mathbf{w})}=-A^{\top}\left[\ldots,\frac{\exp(- y^{(i)}\mathbf{x}^{(i)\top}\mathbf{w})}{\sum_{j=1}^{n}\exp(-y^{(j)}\mathbf{x}^{(j) \top}\mathbf{w})},\ldots\right]^{\top},\]

which implies that

\[\mathbf{A}^{\top}\mathbf{p}_{t}=-\frac{\nabla L\left(\frac{\alpha_{t}}{\beta _{t}}\mathbf{w}_{t-1}\right)}{L\left(\frac{\alpha_{t}}{\beta_{t}}\mathbf{w}_{ t-1}\right)}.\]

Combining the above equations and the definition of \(\beta_{t}=\frac{\alpha_{t}}{\sum_{i=1}^{t-1}\alpha_{i}}\), we get

\[\nabla\Phi\left(\mathbf{w}_{t}\sum_{j=1}^{t}\alpha_{j}\right)=\nabla\Phi\left( \mathbf{w}_{t-1}\sum_{j=1}^{t-1}\alpha_{j}\right)-\alpha_{t}\frac{\nabla L \left(\mathbf{w}_{t-1}\sum_{j=1}^{t-1}\alpha_{j}\right)}{L\left(\mathbf{w}_{t -1}\sum_{j=1}^{t-1}\alpha_{j}\right)}.\]

Substituting \(\mathbf{v}_{t}=\mathbf{w}_{t}\cdot\sum_{j=1}^{t}\alpha_{j}\), we get

\[\nabla\Phi(\mathbf{v}_{t})=\nabla\Phi(\mathbf{v}_{t-1})-\alpha_{t}\frac{ \nabla L(\mathbf{v}_{t-1})}{L(\mathbf{v}_{t-1})},\]

and \(\widetilde{\mathbf{w}}_{T}=\sum_{t=1}^{T}\alpha_{t}\mathbf{w}_{t}=\sum_{t=1}^ {T}\frac{\alpha_{t}}{\sum_{j=1}^{t}\alpha_{j}}\mathbf{v}_{t}\). The proof is finished by replacing \(\frac{\alpha_{t}}{L(\mathbf{v}_{t-1})}\) with \(\eta_{t}\).

[MISSING_PAGE_EMPTY:20]

For the \(\mathbf{w}\)-player, note that \(h_{t}(\mathbf{w})\) is \((q-1)\)-strongly convex wrt the \(\|\cdot\|_{q}\)-norm. Thus, based on Lemma 3, we have

\[\text{Reg}_{T}^{\mathbf{w}}\leq-\sum_{t=1}^{T}\left(\frac{(q-1)\sum_{s=1}^{t-1} \alpha_{s}}{2}\right)\|\mathbf{w}_{t}-\mathbf{w}_{t-1}\|_{q}^{2}.\] (22)

### Proof of Corollary 1

Note that \(C_{T}\leq\frac{\gamma^{2}}{4}\) is equivalent to \(\frac{\gamma^{2}}{4}\sum_{t=1}^{T}\alpha_{t}\geq(\text{Reg}_{T}^{\mathbf{p}}+ \text{Reg}_{T}^{\mathbf{w}})\). We have \(\frac{\gamma^{2}}{4}\sum_{t=1}^{T}\alpha_{t}=\frac{\gamma^{2}}{4}\sum_{t=1}^{ T}\frac{1}{\sqrt{t}}\geq\frac{\gamma^{2}}{4}\sqrt{T}\), and

\[\begin{split}\text{Reg}_{T}^{\mathbf{p}}+\text{Reg}_{T}^{ \mathbf{w}}&=\,\left(2\sum_{t=2}^{T}\frac{\alpha_{t}^{2}}{\sum_{ j=1}^{t-1}\alpha_{j}(q-1)}+2\log n\sum_{t=2}^{T}\frac{\alpha_{t}}{\sum_{i=1}^{t-1} \alpha_{i}}\right)+\alpha_{1}(1+2\log n)\\ &=2\sum_{t=2}^{T}\frac{1/t}{(q-1)\sum_{j=1}^{t-1}1/\sqrt{j}}+2 \log n\sum_{t=2}^{T}\frac{1/\sqrt{t}}{\sum_{i=1}^{t-1}1/\sqrt{j}}+1+2\log n\\ &\leq 2\sum_{t=2}^{T}\frac{1}{(q-1)t\sqrt{t-1}}+\log n\sum_{t=2}^{T} \frac{1}{\sqrt{t}\sqrt{t-1}}+1+2\log n\\ &\leq\frac{2}{q-1}(\log T+1)+\log n(\log T+1)+1+2\log n\\ &\leq\,\left(\frac{2}{q-1}+\log n\right)\log T+4\log n+\frac{4}{ q-1}.\end{split}\] (23)

Therefore, \(C_{T}\leq\frac{\gamma^{2}}{4}\) when

\[\sqrt{T}\geq\frac{4\left(\left(\frac{2}{q-1}+\log n\right)\log T+4\log n+\frac {4}{q-1}\right)}{\gamma^{2}},\]

and in this case

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\tilde{\mathbf{ v}}_{T}}{\left\|\tilde{\mathbf{v}}_{T}\right\|_{q}}\geq\gamma-\frac{4\left( \left(\frac{2}{q-1}+\log n\right)\log T+4\log n+\frac{4}{q-1}\right)}{\gamma^ {2}\sqrt{T}}.\] (24)

### Proof of Corollary 2

We have \(\frac{\gamma^{2}}{4}\sum_{t=1}^{T}\alpha_{t}=\frac{\gamma^{2}}{4}T\), and

\[\begin{split}\text{Reg}_{T}^{\mathbf{p}}+\text{Reg}_{T}^{ \mathbf{w}}&=\,\left(2\sum_{t=2}^{T}\frac{\alpha_{t}^{2}}{\sum_{j=1 }^{t-1}\alpha_{j}(q-1)}+2\log n\sum_{t=2}^{T}\frac{\alpha_{t}}{\sum_{i=1}^{t-1 }\alpha_{i}}\right)+\alpha_{1}(1+2\log n)\\ &=2\sum_{t=2}^{T}\frac{1}{(t-1)(q-1)}+2\log n\sum_{t=2}^{T}\frac{ 1}{t-1}+1+2\log n\\ &=1+2\log n+\left(\frac{2}{q-1}+2\log n\right)\sum_{t=1}^{T-1} \frac{1}{t}\\ &\leq\,\left(\frac{2}{q-1}+2\log n\right)(\log T+2).\end{split}\] (25)

Therefore, \(C_{T}\leq\frac{\gamma^{2}}{4}\) when

\[T\geq\frac{4\left(\frac{2}{q-1}+2\log n\right)(\log T+2)}{\gamma^{2}},\]and in this case

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\widetilde{\mathbf{ v}}_{T}}{\left\|\tilde{\mathbf{v}}_{T}\right\|_{q}}\geq\gamma-\frac{4\left(\frac{2}{q-1}+2 \log n\right)(\log T+2)}{\gamma^{2}T}.\] (26)

and in this case

### Proof of Corollary 3

We have \(\frac{\gamma^{2}}{4}\sum_{t=1}^{T}\alpha_{t}=\frac{\gamma^{2}}{4}\frac{T(T+1)} {2}\geq\frac{\gamma^{2}}{8}T^{2}\), and

\[\left(2\sum_{t=2}^{T}\frac{\alpha_{t}^{2}}{\sum_{j=1}^{t-1}\alpha_ {j}(q-1)}+2\log n\sum_{t=2}^{T}\frac{\alpha_{t}}{\sum_{i=1}^{t-1}\alpha_{i}} \right)+\alpha_{1}(1+2\log n)\] (27) \[=2\sum_{t=2}^{T}\frac{2t^{2}}{t(t-1)(q-1)}+2\log n\sum_{t=2}^{T} \frac{2t}{t(t-1)}+1+2\log n\] \[\leq\frac{8T}{q-1}+4\log n\log T+1+2\log n.\]

Therefore, \(C_{T}\leq\frac{\gamma^{2}}{4}\) when

\[T\geq\sqrt{\frac{8\left[\frac{8T}{q-1}+4\log n\log T+1+2\log n\right]}{\gamma ^{2}}},\]

and in this case

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A}\widetilde{ \mathbf{v}}_{T}}{\left\|\tilde{\mathbf{v}}_{T}\right\|_{q}}\geq\gamma-\frac{3 2}{\gamma^{2}T(q-1)}-\frac{8(4\log n\log T+1+2\log n)}{\gamma^{2}T^{2}}.\] (28)

### Proof of Theorem 4

We first focus on the algorithm equivalence. We have

\[\begin{split}\mathbf{w}_{t}&=\operatorname*{argmin }_{\mathbf{w}\in\mathbb{R}^{d}}-\sum_{i=1}^{t-1}\alpha_{i}\mathbf{p}_{i}^{\top }\mathbf{A}\mathbf{w}-\alpha_{t}\mathbf{p}_{t-1}^{\top}\mathbf{A}\mathbf{w}+ \sum_{i=1}^{t}\alpha_{i}\Phi(\mathbf{w})\\ &=\left[\nabla\Phi\right]^{-1}\left(\frac{1}{\sum_{i=1}^{t} \alpha_{i}}\left(\sum_{i=1}^{t-1}\alpha_{i}\mathbf{A}^{\top}\mathbf{p}_{i}+ \alpha_{t}\mathbf{A}^{\top}\mathbf{p}_{t-1}\right)\right),\end{split}\] (29)

So we have

\[\begin{split}\nabla\Phi(\mathbf{w}_{t})&=\frac{1}{ \sum_{i=1}^{t}\alpha_{i}}\left(\sum_{i=1}^{t-1}\alpha_{i}\mathbf{A}^{\top} \mathbf{p}_{i}+\alpha_{t}\mathbf{A}^{\top}\mathbf{p}_{t-1}\right)\\ &=\frac{1}{\sum_{i=1}^{t}\alpha_{i}}\left(\frac{\sum_{i=1}^{t-1} \alpha_{i}}{\sum_{i=1}^{t-1}\alpha_{i}}\left(\sum_{i=1}^{t-2}\alpha_{i}\mathbf{ A}^{\top}\mathbf{p}_{i}+\alpha_{t-1}\mathbf{A}^{\top}\mathbf{p}_{t-2}\right)+ \alpha_{t}\mathbf{A}^{\top}\mathbf{p}_{t-1}+\alpha_{t-1}\mathbf{A}^{\top}( \mathbf{p}_{t-1}-\mathbf{p}_{t-2})\right)\\ &=\frac{1}{\sum_{i=1}^{t}\alpha_{i}}\left(\left[\sum_{i=1}^{t-1} \alpha_{i}\right]\nabla\Phi(\mathbf{w}_{t-1})+\alpha_{t}\mathbf{A}^{\top} \mathbf{p}_{t-1}+\alpha_{t-1}\mathbf{A}^{\top}(\mathbf{p}_{t-1}-\mathbf{p}_{t -2})\right).\end{split}\] (30)

Therefore, we have

\[\nabla\Phi\left(\mathbf{w}_{t}\sum_{i=1}^{t}\alpha_{i}\right)=\nabla\Phi\left( \mathbf{w}_{t-1}\sum_{i=1}^{t-1}\alpha_{i}\right)+\alpha_{t}\mathbf{A}^{\top} \mathbf{p}_{t-1}+\alpha_{t-1}\mathbf{A}^{\top}(\mathbf{p}_{t-1}-\mathbf{p}_{t -2}).\] (31)On the other hand, for the \(\mathbf{p}\)-player, we have

\[\mathbf{p}_{t}=\operatorname*{argmin}_{\mathbf{p}\in\Delta^{n}}\alpha_{t}\ell_{t} (\mathbf{p})+\beta_{t}D_{\text{KL}}\left(\mathbf{p},\frac{\mathbf{1}}{n}\right).\]

Based on the relationship between OMD with the negative entropy regularizer on the simplex (Hazan, 2016), it is easy to verify that \(\forall i\in[n],t\in[T]\),

\[p_{t,i}=\frac{\exp(-\frac{\alpha_{t}}{\beta_{t}}y^{(i)}\mathbf{x}^{(i)\top} \mathbf{w}_{t})}{\sum_{j=1}^{n}\exp(-\frac{\alpha_{t}}{\beta_{t}}y^{(j)} \mathbf{x}^{(j)\top}\mathbf{w}_{t})},\]

which implies that

\[\mathbf{A}^{\top}\mathbf{p}_{t}=-\frac{\nabla L\left(\frac{\alpha_{t}}{\beta _{t}}\mathbf{w}_{t}\right)}{L\left(\frac{\alpha_{t}}{\beta_{t}}\mathbf{w}_{t} \right)}.\]

Combining the equations above and replace \(\sum_{k=1}^{t}\alpha_{k}\mathbf{w}_{k}\) with \(\mathbf{v}_{t}\), we get

\[\nabla\Phi(\mathbf{v}_{t})=\nabla\Phi(\mathbf{v}_{t-1})-\alpha_{t}\frac{ \nabla L(\mathbf{v}_{t-1})}{L(\mathbf{v}_{t-1})}-\alpha_{t-1}\left(\frac{ \nabla L(\mathbf{v}_{t-1})}{L(\mathbf{v}_{t-1})}-\frac{\nabla L(\mathbf{v}_{t -2})}{L(\mathbf{v}_{t-2})}\right).\]

The proof is finished by setting \(\alpha_{t}=t\).

Next, we focus on the regret. For the \(\mathbf{w}\)-player, Note that \(h_{t}(\mathbf{w})\) is \((q-1)\)-strongly convex wrt the \(\|\cdot\|\)-norm. Let \(\widehat{\mathbf{w}}_{t}=\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R}^{d}} \sum_{i=1}^{t-1}\alpha_{i}h_{i}(\mathbf{w})\). Then, based on Lemma 4, we have

\[\text{Reg}_{T}^{\mathbf{w}}\leq \,\sum_{t=1}^{T}\alpha_{t}\left(h_{t}(\mathbf{w}_{t})-h_{t}( \widehat{\mathbf{w}}_{t+1})-h_{t-1}(\mathbf{w}_{t})+h_{t-1}(\widehat{\mathbf{ w}}_{t+1})\right)\] (32) \[-\sum_{t=1}^{T}\frac{\sum_{i=1}^{t}\alpha_{i}(q-1)}{2}\|\mathbf{ w}_{t}-\widehat{\mathbf{w}}_{t+1}\|_{q}^{2}\] \[= \,\sum_{t=1}^{T}\alpha_{t}(\mathbf{p}_{t}-\mathbf{p}_{t-1})^{\top }\mathbf{A}(\mathbf{w}_{t}-\widehat{\mathbf{w}}_{t+1})-\sum_{t=1}^{T}\frac{ \sum_{i=1}^{t}\alpha_{i}(q-1)}{2}\|\mathbf{w}_{t}-\widehat{\mathbf{w}}_{t+1} \|_{q}^{2}\] \[\leq \,\sum_{t=1}^{T}\alpha_{t}\|(\mathbf{p}_{t}-\mathbf{p}_{t-1})^{ \top}\mathbf{A}\|_{p}\|\mathbf{w}_{t}-\widehat{\mathbf{w}}_{t+1}\|_{q}-\sum_{t =1}^{T}\frac{\sum_{i=1}^{t}\alpha_{i}(q-1)}{2}\|\mathbf{w}_{t}-\widehat{ \mathbf{w}}_{t+1}\|_{q}^{2}\] \[\leq \,\sum_{t=1}^{T}\frac{\alpha_{t}^{2}}{2\sum_{i=1}^{t}\alpha_{i}(q -1)}\|\mathbf{A}^{\top}(\mathbf{p}_{t}-\mathbf{p}_{t-1})\|_{p}^{2}+\frac{\sum_{ i=1}^{t}\alpha_{i}(q-1)}{2}\|\mathbf{w}_{t}-\widehat{\mathbf{w}}_{t+1}\|_{q}^{2}\] \[-\sum_{t=1}^{T}\frac{\sum_{i=1}^{t}\alpha_{i}(q-1)}{2}\|\mathbf{ w}_{t}-\widehat{\mathbf{w}}_{t+1}\|_{q}^{2}\] \[= \,\frac{1}{2(q-1)}\sum_{t=1}^{T}\frac{\alpha_{t}^{2}}{\sum_{i=1}^{ t}\alpha_{i}}\left\|\sum_{i=1}^{n}(p_{t,i}-p_{t-1,i})y^{(i)}\mathbf{x}^{(i)} \right\|_{p}^{2}\] \[\leq \,\frac{1}{2(q-1)}\sum_{t=1}^{T}\frac{\alpha_{t}^{2}}{\sum_{i=1}^{ t}\alpha_{i}}\left\|\mathbf{p}_{t}-\mathbf{p}_{t-1}\right\|_{1}^{2},\]

where the first inequality is based on Holder's inequality, the second inequality is based on Young's inequality, i.e.,

\[\sum_{t=1}^{T}\alpha_{t}\|(\mathbf{p}_{t}-\mathbf{p}_{t-1})^{\top }\mathbf{A}\|_{p}\|\mathbf{w}_{t}-\widehat{\mathbf{w}}_{t+1}\|_{q}\] (33) \[\leq \,\sum_{t=1}^{T}\frac{\alpha_{t}^{2}}{2\sum_{i=1}^{t}\alpha_{i}(q -1)}\|\mathbf{A}^{\top}(\mathbf{p}_{t}-\mathbf{p}_{t-1})\|_{p}^{2}+\frac{\sum_ {i=1}^{t}\alpha_{i}(q-1)}{2}\|\mathbf{w}_{t}-\widehat{\mathbf{w}}_{t+1}\|_{q} ^{2}\]For the \(\mathbf{p}\)-player, we have

\[\sum_{t=1}^{T}\alpha_{t}\mathbf{p}_{t}^{T}\mathbf{A}\mathbf{w}_{t}- \min_{\mathbf{p}\in\Delta^{n}}\alpha_{t}\mathbf{p}^{\top}\mathbf{A}\mathbf{w}_{t}\] (34) \[= \sum_{t=1}^{T}(\alpha_{t}\mathbf{p}_{t}^{\top}\mathbf{A}\mathbf{w} _{t}+\beta_{t}D_{\text{KL}}\left(\mathbf{p}_{t},\frac{\mathbf{1}}{n}\right)- \min_{\mathbf{p}\in\Delta^{n}}\sum_{t=1}^{T}\alpha_{t}\mathbf{p}^{\top}\mathbf{ A}\mathbf{w}_{t}-\sum_{t=1}^{T}\beta_{t}D_{E}\left(\mathbf{p}_{t},\frac{\mathbf{1}}{n}\right)\] \[\leq 2\sum_{t=1}^{T}\beta_{t}\log n=2\sum_{t=1}^{T}\frac{\alpha_{t }}{\sum_{i=1}^{t}\alpha_{i}}\log n.\]

Finally, we focus on the margin and implicit bias. Since \(\alpha_{t}=t\), for the \(\mathbf{w}\)-player's regret, we have

\[\frac{1}{2(q-1)}\sum_{t=1}^{T}\frac{\alpha_{t}^{2}}{\sum_{i=1}^{t}\alpha_{i}} \left\|\mathbf{p}_{t}-\mathbf{p}_{t-1}\right\|_{1}^{2}\leq\frac{1}{(q-1)}\sum_ {t=1}^{T}\left\|\mathbf{p}_{t}-\mathbf{p}_{t-1}\right\|_{1}^{2},\]

and for the \(\mathbf{p}\)-player, we have

\[\sum_{t=1}^{T}\frac{\alpha_{t}}{\sum_{i=1}^{t}\alpha_{i}}\log n\leq 4\log T\log n.\]

Following Theorem 1, we have the margin and implicit bounds when

\[\sum_{t=1}^{T}\alpha_{t}=\frac{T(T+1)}{2}\geq\frac{T^{2}}{2} \geq\frac{4}{\gamma^{2}}\left(4\log n\log T+\frac{2T}{(q-1)}\right)\] (35) \[\geq\frac{4}{\gamma^{2}}\left(4\log n\log T+\frac{1}{(q-1)}\sum_ {t=1}^{T}\left\|\mathbf{p}_{t}-\mathbf{p}_{t-1}\right\|_{1}^{2}\right),\]

since the RHS is exactly \(\frac{\gamma^{2}}{4}(\text{Reg}_{T}^{\mathbf{p}}+\text{Reg}_{T}^{\mathbf{w}})\).

## Appendix D Omitted Proof in Section 4.2

In this section, we provide the proof related to the steepest descent algorithm. We first restate Algorithm 3, which is presented in Algorithm 6. Here, we provide two online dynamic under the game framework. They both are equivalent to the steepest descent algorithm in the left box, in the sense that \(\mathbf{v}_{T}=\widetilde{\mathbf{w}}_{T}\). The left one is good for recovering the results in Nacson et al. (2019), while the bottom one is more suitable for analysing our accelerated rates.

We first recover the results of Nacson et al. (2019) in Theorem 8, and then proof our Theorem 5.

```
1:for\(t=1,\dots,T\)do
2:\(\mathbf{s}_{t-1}=\operatorname*{argmin}_{\|\mathbf{s}\|\leq 1}\mathbf{s}^{\top} \nabla L(\mathbf{v}_{t-1})\)\(\mathbf{p}\)-player: \(\mathbf{p}_{t}=\operatorname*{argmin}_{\mathbf{p}\in\Delta^{n}}\sum_{i=1}^{t-1} \alpha_{i}\ell_{i}(\mathbf{p})+D_{\text{KL}}\left(\mathbf{p},\frac{\mathbf{1} }{n}\right)\)
3:\(\mathbf{v}_{t}=\mathbf{v}_{t-1}+\eta_{t-1}\mathbf{s}_{t-1}\)\(\mathbf{w}\)-player: \(\mathbf{w}_{t}=\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R}^{d}}\alpha_{t}h_{t}( \mathbf{w})\)
4:endfor
5:Output:\(\widetilde{\mathbf{w}}_{T}=\sum_{t=1}^{T}\alpha_{t}\mathbf{w}_{t}\) ```

**Algorithm 6** Steepest Descent [Recall \(\ell_{t}(\mathbf{p})=g(\mathbf{p},\mathbf{w}_{t}),\) and \(h_{t}(\mathbf{w})=-g(\mathbf{p}_{t},\mathbf{w})\)]

```
1:for\(t=1,\dots,T\)do
2:\(\mathbf{s}_{t-1}=\operatorname*{argmin}_{\|\mathbf{s}\|\leq 1}\mathbf{s}^{\top} \nabla L(\mathbf{v}_{t-1})\)\(\mathbf{p}\)-player: \(\mathbf{p}_{t}=\operatorname*{argmin}_{\mathbf{p}\in\Delta^{n}}\sum_{i=1}^{t-1} \alpha_{i}\ell_{i}(\mathbf{p})+D_{\text{KL}}\left(\mathbf{p},\frac{\mathbf{1} }{n}\right)\)
3:\(\mathbf{v}_{t}=\mathbf{v}_{t-1}+\eta_{t-1}\mathbf{s}_{t-1}\)\(\mathbf{w}\)-player: \(\mathbf{w}_{t}=\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R}^{d}}\alpha_{t}h_{t}( \mathbf{w})\)
4:endfor
5:Output:\(\widetilde{\mathbf{w}}_{T}=\sum_{t=1}^{T}\alpha_{t}\mathbf{w}_{t}\) ```

**Algorithm 7** Steepest Descent [Recall \(\ell_{t}(\mathbf{p})=g(\mathbf{p},\mathbf{w}_{t}),\) and \(h_{t}(\mathbf{w})=-g(\mathbf{p}_{t},\mathbf{w})\)]

**Theorem 8**.: _Suppose Assumption 1 holds wrt a general norm \(\|\cdot\|\). Let \(\eta_{t}=\frac{\alpha_{t}\|\nabla L(\mathbf{w}_{t-1})\|_{*}}{L(\mathbf{w}_{t-1})}\). Then, the methods in the top two boxes of Algorithm 6 are equivalent, in the sense that \(\mathbf{w}_{T}=\widetilde{\mathbf{w}}_{T}\). Moreover, let \(\alpha_{t}=\frac{1}{\sqrt{t}}\). Then \(C_{T}=\frac{\log n+2\log T+2}{\sqrt{T}}\). Therefore, when \(T\) is sufficiently large such that \(\sqrt{T}\geq\frac{4(\log n+2\log T+2)}{\gamma^{2}}\), we have_

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A} \widetilde{\mathbf{w}}_{T}}{\|\widetilde{\mathbf{w}}_{T}\|}\geq\gamma-\frac{4 (2+\log n+2\log T)}{\gamma^{2}\sqrt{T}}.\] (36)

The first part of Theorem 8 shows that the steepest descent algorithm can be seen as an online learning dynamic where the in each round \(\mathbf{p}\)-player performs the standard _follow-the-regularized-leader_ algorithm (Hazan, 2016), while the \(\mathbf{w}\)-player uses _best-response\({}^{+}\)_(Wang et al., 2021b), meaning it picks the decision by directly minimizing the loss of the round. The second part of the theorem shows that the margin convergence rate, which recovers the \(\gamma-O\left(\frac{\log n+\log T}{\sqrt{T}}\right)\) rate of Nacson et al. (2019), while the convergence in terms of distance is new. Note that the strongly convex condition is not required for the margin maximization analysis, but is needed for the distance convergence analysis. Next, we restate Theorem 5, where an improved margin maximization rate when the strong convexity condition is met.

**Theorem 9** (Restate of Theorem 5).: _Suppose Assumption 1 holds wrt a general norm \(\|\cdot\|\), and \(\frac{1}{2}\|\cdot\|^{2}\) is \(\lambda\)-strongly convex wrt \(\|\cdot\|\). Let \(\eta_{t}=\frac{\alpha_{t}\|\nabla L(\mathbf{w}_{t})\|}{L(\mathbf{w}_{t})}\). Then the methods in the first and third boxes of Algorithm 6 are are equivalent, in the sense that \(\mathbf{v}_{T}=\widetilde{\mathbf{w}}_{T}\). Moreover, let \(\alpha_{t}=\frac{\lambda}{2}\). Then \(C_{T}=\frac{\lambda}{2}\cdot\frac{\alpha+\log n}{T\lambda}\). Therefore, when \(T\geq\frac{\lambda+4\log n}{\lambda\gamma^{2}}\), we have_

\[\frac{\min_{\mathbf{p}\in\Delta^{n}}\mathbf{p}^{\top}\mathbf{A} \mathbf{v}_{T}}{\|\mathbf{v}_{T}\|}\geq\gamma-\frac{\lambda+4\log n}{\gamma^{2 }T\lambda}.\ \ \text{and}\ \ \left\|\frac{\mathbf{v}_{T}}{\|\mathbf{v}_{T}\|}-\mathbf{w}_{\|\cdot\|}^{*} \right\|^{2}\leq\frac{8}{\sqrt{\lambda}\gamma^{2}}\sqrt{\frac{\lambda+2\log n }{T\lambda}}.\]

### Proof of Theorem 8

We first focus on the algorithm equivalence between the top two boxes. Based on the relationship between FTRL and EWA (Orabona, 2019), one can verify that

\[p_{t,i}=\frac{\exp(-y^{(i)}\mathbf{x}^{(i)\top}(\sum_{k=1}^{t-1}\alpha_{k} \mathbf{w}_{k}))}{\sum_{j=1}^{n}\exp(-y^{(j)}\mathbf{x}^{(j)\top}(\sum_{k=1}^{ t-1}\alpha_{k}\mathbf{w}_{k}))}.\]

Combining with the definition of \(L\) and \(\widetilde{\mathbf{w}}_{t}\), it implies that

\[\frac{\nabla L(\widetilde{\mathbf{w}}_{t-1})}{L(\widetilde{\mathbf{w}}_{t-1})} =-\mathbf{A}^{\top}\mathbf{p}_{t},\]

That is, \(\nabla L(\widetilde{\mathbf{w}}_{t-1})=-L(\widetilde{\mathbf{w}}_{t-1}) \mathbf{A}^{\top}\mathbf{p}_{t}\). Let \(\widetilde{\mathbf{s}}_{t}=\operatorname*{argmin}_{\|\mathbf{s}\|\leq 1} \mathbf{s}^{\top}\frac{\nabla L(\widetilde{\mathbf{w}}_{t-1})}{L(\widetilde{ \mathbf{w}}_{t-1})}\). Note that, on one hand, we have

\[\widetilde{\mathbf{s}}_{t}=\operatorname*{argmin}_{\|\mathbf{s}\|\leq 1} \mathbf{s}^{\top}\frac{\nabla L(\widetilde{\mathbf{w}}_{t-1})}{L(\widetilde{ \mathbf{w}}_{t-1})}=\operatorname*{argmin}_{\|\mathbf{s}\|\leq 1}\mathbf{s}^{\top} \nabla L(\widetilde{\mathbf{w}}_{t-1}),\] (37)

where the second equality is because because the _argmin_ does not change if we scale the objective functions. On the other hand, we have

\[\widetilde{\mathbf{s}}_{t}=\operatorname*{argmin}_{\|\mathbf{s}\|\leq 1} \mathbf{s}^{\top}\frac{\nabla L(\widetilde{\mathbf{w}}_{t-1})}{L(\widetilde{ \mathbf{w}}_{t-1})}=\operatorname*{argmax}_{\|\mathbf{s}\|\leq 1}\mathbf{s}^{\top} \left(-\frac{\nabla L(\widetilde{\mathbf{w}}_{t-1})}{L(\widetilde{\mathbf{w}}_{ t-1})}\right).\] (38)

To proceed, we introduce the following lemma.

**Lemma 2**.: _Let \(\|\cdot\|\) be any norm in \(\mathbb{R}^{d}\). Let \(\mathbf{a}\in\mathbb{R}^{d}\), and_

\[\mathbf{s}=\operatorname*{argmax}_{\|\mathbf{s}^{\prime}\|\leq 1}\mathbf{s}^{ \prime\top}\mathbf{a}.\] (39)

_Then_

\[\|\mathbf{a}\|_{*}\mathbf{s}=\operatorname*{argmin}_{\mathbf{x}\in\mathbb{R}^ {d}}-\mathbf{a}^{\top}\mathbf{x}+\frac{1}{2}\|\mathbf{x}\|^{2}.\] (40)Proof.: We first focus on (40). Note that the objective has two terms. For the first term, based on Holder's inequality, we have

\[-\mathbf{a}^{\top}\mathbf{x}\geq-\|\mathbf{a}\|_{*}\|\mathbf{x}\|.\]

Let \(\|\mathbf{x}\|=c\), where \(c>0\) is a constant, then the equality is achieved (and thus the first term of the objective function is minimized) when

\[\mathbf{x}=\operatorname*{argmin}_{\|\mathbf{x}^{\prime}\|\leq c}-\mathbf{x} ^{{}^{\prime}\top}\mathbf{a}=\operatorname*{argmax}_{\|\mathbf{x}^{\prime}\| \leq c}\mathbf{x}^{{}^{\prime}\top}\mathbf{a}.\] (41)

In this case, for the objective function of (40), we have \(-\mathbf{a}^{\top}\mathbf{x}+\frac{1}{2}\|\mathbf{x}\|^{2}=-c\|\mathbf{a}\|_{* }+\frac{1}{2}c^{2}\). It's easy to see that the objective function is minimized when \(c=\|\mathbf{a}\|_{*}\). The proof is finished by combining (39) and (41). 

This lemma shows that the best response direction (under our game) is the steepest direction. Thus, we have

\[\begin{split}\mathbf{w}_{t}&=\operatorname*{ argmin}_{\mathbf{w}\in\mathbb{R}^{d}}-\mathbf{p}_{t}^{\top}\mathbf{A}\mathbf{w}+ \frac{1}{2}\|\mathbf{w}\|^{2}=\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R} ^{d}}\mathbf{w}^{\top}\frac{\nabla L(\widetilde{\mathbf{w}}_{t-1})}{L( \widetilde{\mathbf{w}}_{t-1})}+\frac{1}{2}\|\mathbf{w}\|^{2}\\ &=\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R}^{d}}-\mathbf{w }^{\top}\left(-\frac{\nabla L(\widetilde{\mathbf{w}}_{t-1})}{L(\widetilde{ \mathbf{w}}_{t-1})}\right)+\frac{1}{2}\|\mathbf{w}\|^{2}\\ &=\operatorname*{argmax}_{\|\mathbf{x}^{\prime}\|\leq 1} \mathbf{w}^{\top}\left(-\frac{\nabla L(\widetilde{\mathbf{w}}_{t-1})}{L( \widetilde{\mathbf{w}}_{t-1})}\right)=\frac{\|\nabla L(\widetilde{\mathbf{w}}_ {t-1})\|_{*}}{L(\widetilde{\mathbf{w}}_{t-1})}\widetilde{\mathbf{s}}_{t}. \end{split}\] (42)

where the first equality is based on the update rule of \(\mathbf{w}_{t}\) at the right box of Algorithm 6, the final inequality is based on Lemma 2 and (38). Note that, for the first equality, we dropped \(\alpha_{t}\) as it is a scaling parameter and does not have a influence on _argmin_. Thus, combining with (37), we have

\[\mathbf{w}_{t}=\frac{\|\nabla L(\widetilde{\mathbf{w}}_{t-1})\|_{*}}{L( \widetilde{\mathbf{w}}_{t-1})}\widetilde{\mathbf{s}}_{t}=\frac{\|\nabla L( \widetilde{\mathbf{w}}_{t-1})\|_{*}}{(L(\widetilde{\mathbf{w}}_{t-1}))} \operatorname*{argmin}_{\|\mathbf{s}\|\leq 1}\mathbf{s}^{\top}\nabla L( \widetilde{\mathbf{w}}_{t-1}).\]

Finally, we have

\[\widetilde{\mathbf{w}}_{t}=\widetilde{\mathbf{w}}_{t-1}+\alpha_{t}\mathbf{w}_{ t}=\widetilde{\mathbf{w}}_{t}+\alpha_{t}\frac{\|\nabla L(\widetilde{\mathbf{w}}_{t-1}) \|_{*}}{L(\widetilde{\mathbf{w}}_{t-1})}\operatorname*{argmin}_{\|\mathbf{s} \|\leq 1}\mathbf{s}^{\top}\nabla L(\widetilde{\mathbf{w}}_{t-1}).\]

We can finish the first part of the proof by replacing \(\widetilde{\mathbf{w}}_{t}\) with \(\mathbf{v}_{t}\), \(\alpha_{t}\frac{\|\nabla L(\widetilde{\mathbf{w}}_{t-1})\|_{*}}{L(\widetilde{ \mathbf{w}}_{t-1})}\) with \(\eta_{t}\), and noticing \(\operatorname*{argmin}_{\|\mathbf{s}\|\leq 1}\mathbf{s}^{\top}\nabla L( \widetilde{\mathbf{w}}_{t-1})\) is \(\mathbf{s}_{t-1}\).

Next, we study the regret bound. Note that the \(\mathbf{p}\)-player plays the FTRL algorithm on the simplex with a 1-strongly convex regularizer \(D_{\text{KL}}\left(\mathbf{p},\frac{1}{n}\right)\) (wrt \(\ell_{1}\)-norm). Therefore, based on Lemma 5, the regret can be upper bounded by

\[\begin{split}\sum_{t=1}^{T}\alpha_{t}\ell_{t}(\mathbf{p}_{t})- \sum_{t=1}^{T}\alpha_{t}\ell_{t}(\mathbf{p}^{*})&\leq\,\log n+2 \sum_{t=1}^{T}\alpha_{t}^{2}\|\mathbf{A}^{\top}\mathbf{w}_{t}\|_{\infty}^{2} \\ &=\,\log n+2\sum_{t=1}^{T}\frac{1}{t}\left(\max_{i\in n}|y^{(i)} \mathbf{x}^{(i)\top}\mathbf{w}_{t}|\right)\left(\max_{i\in n}|y^{(i)}\mathbf{ x}^{(i)\top}\mathbf{w}_{t}|\right)\\ &\leq\,\log n+2\sum_{t=1}^{T}\frac{1}{t}\|\mathbf{w}_{t}\|^{2}\\ &=\,\log n+2\sum_{t=1}^{T}\frac{1}{t}\|\mathbf{A}^{\top}\mathbf{ p}_{t}\|_{*}^{2}\leq\log n+2\log T+2,\end{split}\] (43)

where the equality is because

\[\|\mathbf{w}_{t}\|=\left\|\left\|\frac{\nabla L(\mathbf{w}_{t})}{L(\mathbf{w}_ {t})}\right\|_{*}\,\widetilde{\mathbf{s}}_{t}\right\|=\left\|\frac{\nabla L( \mathbf{w}_{t})}{L(\mathbf{w}_{t})}\right\|_{*}=\|\mathbf{A}^{\top}\mathbf{p}_ {t}\|_{*}.\]

On the other hand, the \(\mathbf{w}\)-player uses the best response\({}^{+}\) algorithm, and one can easily observe that the regret is upper bounded by 0. Finally, we have \(\sum_{t=1}^{T}\alpha_{t}=\sum_{t=1}^{T}\frac{1}{\sqrt{t}}\geq\sqrt{T}\).

### Proof of Theorem 9

We first focus on the bottom box. For the \(\mathbf{w}\)-player, we show that, if we set \(\delta_{t-1}=\frac{1}{\alpha_{t-1}}\), then this OMD algorithm is equivalent to the best response algorithm.

Specifically, note that \(\Phi(\mathbf{w})=\frac{1}{2}\|\mathbf{w}\|^{2}\) is now \(\lambda\)-strongly convex. Thus the corresponding mirror map is well-defined and unique, and the function \(\nabla\Phi(\cdot)\) is invertible. Therefore, the solution for best response is

\[\widehat{\mathbf{w}}_{t}=\operatorname*{argmin}_{\mathbf{w}}\alpha_{t-1}h_{t-1 }(\mathbf{w})=\operatorname*{argmin}_{\mathbf{w}}h_{t-1}(\mathbf{w})=\nabla \Phi^{-1}(\mathbf{A}^{\top}\mathbf{p}_{t-1}).\] (44)

On the other hand, since the \(\mathbf{w}\)-player uses OMD, and the decision set is unbounded, we have

\[\nabla\Phi(\mathbf{w}_{t})=\nabla\Phi(\mathbf{w}_{t-1})-\delta_{t-1}\alpha_{t -1}\nabla h_{t-1}(\mathbf{w}_{t-1})=\nabla\Phi(\mathbf{w}_{t-1})+\mathbf{A}^{ \top}\mathbf{p}_{t-1}-\nabla\Phi(\mathbf{w}_{t-1})=\mathbf{A}^{\top}\mathbf{p }_{t-1}.\] (45)

Note that \(\delta_{t-1}\alpha_{t-1}=1\). Combining (44) and (45), we can draw the conclusion that \(\mathbf{w}_{t}\) and \(\widehat{\mathbf{w}}_{t}\) are identical, which shows OMD (with \(\delta_{t-1}=\frac{1}{\alpha_{t-1}}\)) and BR (at round \(t-1\)) here are the same. We use the BR form the algorithm equivalence analysis, and OMD form for the regret analysis.

Next, we prove the algorithm equivalence of the left and bottom boxes in Algorithm 6. The arguments are similar to that in Appendix D.1. The difference is that the order of the \(\mathbf{w}\)-player and the \(\mathbf{p}\)-player is switched. Firstly, for the \(\mathbf{p}\)-player, based on the connection between FTRL and EWA, we have

\[p_{t,i}\propto\exp\left(-y^{(i)}\mathbf{x}^{(i)\top}\left(\sum_{j=1}^{t} \alpha_{j}\mathbf{w}_{j}\right)\right)=\exp\left(-y^{(i)}\mathbf{x}^{(i)\top} \widetilde{\mathbf{w}}_{t}\right).\]

Combining with the definition of \(L\), it implies that

\[\frac{\nabla L(\widetilde{\mathbf{w}}_{t})}{L(\widetilde{\mathbf{w}}_{t})}=- \mathbf{A}^{\top}\mathbf{p}_{t},\]

That is, \(\nabla L(\widetilde{\mathbf{w}}_{t})=-L(\widetilde{\mathbf{w}}_{t})\mathbf{A }^{\top}\mathbf{p}_{t}\). Let

\[\widetilde{\mathbf{s}}_{t}=\operatorname*{argmax}_{\|\mathbf{s}\|\leq 1}- \mathbf{s}^{\top}\frac{\nabla L(\widetilde{\mathbf{w}}_{t})}{L(\widetilde{ \mathbf{w}}_{t})}=\operatorname*{argmin}_{\|\mathbf{s}\|\leq 1}\mathbf{s}^{\top} \nabla L(\widetilde{\mathbf{w}}_{t}),\] (46)

Combining the first equality in (46) and Lemma 2, we have

\[\frac{\|\nabla L(\widetilde{\mathbf{w}}_{t})\|_{*}}{L(\widetilde{\mathbf{w}}_{ t})}\widetilde{\mathbf{s}}_{t}=\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R}^{d}} \mathbf{w}^{\top}\frac{\nabla L(\widetilde{\mathbf{w}}_{t})}{L(\widetilde{ \mathbf{w}}_{t})}+\frac{1}{2}\|\mathbf{w}\|^{2}=\operatorname*{argmin}_{ \mathbf{w}\in\mathbb{R}^{d}}-\mathbf{p}_{t}^{\top}\mathbf{A}\mathbf{w}+\frac{1 }{2}\|\mathbf{w}\|^{2}=\mathbf{w}_{t+1}.\]

Thus,

\[\mathbf{w}_{t}=\frac{\|\nabla L(\widetilde{\mathbf{w}}_{t-1})\|_{*}}{L( \widetilde{\mathbf{w}}_{t-1})}\widetilde{\mathbf{s}}_{t-1}=\frac{\|\nabla L( \widetilde{\mathbf{w}}_{t-1})\|_{*}}{(L(\widetilde{\mathbf{w}}_{t-1}))}\operatorname* {argmin}_{\|\mathbf{s}\|\leq 1}\mathbf{s}^{\top}\nabla L(\widetilde{\mathbf{w}}_{t-1}).\]

Finally, we have

\[\widetilde{\mathbf{w}}_{t}=\widetilde{\mathbf{w}}_{t-1}+\alpha_{t}\mathbf{w}_{ t}=\widetilde{\mathbf{w}}_{t}+\alpha_{t}\frac{\|\nabla L(\widetilde{\mathbf{w}}_{t-1})\|_{*} }{L(\widetilde{\mathbf{w}}_{t-1})}\operatorname*{argmin}_{\|\mathbf{s}\|\leq 1 }\mathbf{s}^{\top}\nabla L(\widetilde{\mathbf{w}}_{t-1}).\]

We can finish the first part of the proof by replacing \(\widetilde{\mathbf{w}}_{t}\) with \(\mathbf{v}_{t}\), \(\alpha_{t}\frac{\|\nabla L(\widetilde{\mathbf{w}}_{t-1})\|_{*}}{L(\widetilde{ \mathbf{w}}_{t-1})}\) with \(\eta_{t-1}\), and \(\operatorname*{argmin}_{\|\mathbf{s}\|\leq 1}\mathbf{s}^{\top}\nabla L( \widetilde{\mathbf{w}}_{t-1})\) with \(\mathbf{s}_{t}\).

Next, we focus on regret. For the \(\mathbf{w}\)-player, it uses the OMD algorithm, and we set the initial point \(\mathbf{w}_{0}=\mathbf{0}\). Note that we fixed \(\alpha_{t}=\frac{3}{4}\) for all \(t\), and thus step size \(\delta_{t-1}=\frac{1}{\alpha_{t-1}}=\frac{4}{\lambda}\) is also fixed.

Therefore, Lemma 7 can be applied. Define \(\mathbf{u}=\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R}^{d}}\sum_{t=1}^{T} \alpha_{t}h_{t}(\mathbf{w})\), we have

\[\sum_{t=1}^{T}\alpha_{t}h_{t}(\mathbf{w}_{t})-\min_{\mathbf{w}\in \mathbb{R}^{d}}\sum_{t=1}^{T}\alpha_{t}h_{t}(\mathbf{w}) =\sum_{t=1}^{T}\alpha_{t}h_{t}(\mathbf{w}_{t})-\sum_{t=1}^{T} \alpha_{t}h_{t}(\mathbf{u})\] (47) \[\leq\frac{\Phi(\mathbf{u})}{\delta}+\sum_{t=1}^{T}\frac{\delta \alpha_{t}^{2}}{\lambda}\left\|\nabla h_{t}(\mathbf{w}_{t})\right\|_{*}^{2}\] \[=\frac{\Phi(\mathbf{u})}{\delta}+\sum_{t=1}^{T}\frac{\alpha_{t}^{ 2}\delta}{\lambda}\left\|-\mathbf{A}^{\top}\mathbf{p}_{t}+\nabla\Phi(\mathbf{ w}_{t})\right\|_{*}^{2}\] \[=\frac{\Phi(\mathbf{u})}{\delta}+\sum_{t=1}^{T}\frac{\alpha_{t}^ {2}\delta}{\lambda}\left\|-\mathbf{A}^{\top}\mathbf{p}_{t}+\mathbf{A}^{\top} \mathbf{p}_{t-1}\right\|_{*}^{2}\] \[=\alpha_{T}\Phi(\mathbf{u})+\sum_{t=1}^{T}\frac{\alpha_{t}}{ \lambda}\left\|-\mathbf{A}^{\top}\mathbf{p}_{t}+\mathbf{A}^{\top}\mathbf{p}_{ t-1}\right\|_{*}^{2}\] \[=\alpha_{T}\Phi(\mathbf{u})+\sum_{t=1}^{T}\frac{\alpha_{t}}{ \lambda}\left\|\sum_{i=1}^{n}y^{(i)}\mathbf{x}^{(i)}(p_{t,i}-p_{t-1,i})\right\| _{*}^{2}\] \[\leq\alpha_{T}\Phi(\mathbf{u})+\sum_{t=1}^{T}\frac{\alpha_{t}}{ \lambda}\left(\sum_{i=1}^{n}\left|p_{t,i}-p_{t-1,i}\right|\right)^{2}\] \[\leq\alpha_{T}\Phi(\mathbf{u})+\sum_{t=1}^{T}\frac{\alpha_{t}}{ \lambda}\left\|\mathbf{p}_{t}-\mathbf{p}_{t-1}\right\|_{1}^{2},\]

where the second-to-last inequality is derived using triangle inequality and the assumption that \(\|\mathbf{x}^{(i)}\|_{*}\) is upper bounded by 1. Next, for \(\mathbf{u}\), note that

\[\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R}^{d}}\sum_{t=1}^{ T}\alpha_{t}h_{t}(\mathbf{w}) =\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R}^{d}}-\sum_{t=1}^ {T}\alpha_{t}\mathbf{p}_{t}^{\top}\mathbf{A}\mathbf{w}+\frac{\sum_{t=1}^{T} \alpha_{t}}{2}\|\mathbf{w}\|^{2}\] (48) \[=\operatorname*{argmin}_{\mathbf{w}\in\mathbb{R}^{d}}-\frac{1}{ \sum_{t=1}^{T}\alpha_{t}}\sum_{t=1}^{T}\alpha_{t}\mathbf{p}_{t}^{\top} \mathbf{A}\mathbf{w}+\frac{1}{2}\|\mathbf{w}\|^{2}.\]

Based on Lemma 2, we have

\[\mathbf{u}=\left\|\mathbf{A}^{\top}\left(\frac{1}{\sum_{t=1}^{T}\alpha_{t}} \sum_{t=1}^{T}\alpha_{t}\mathbf{p}_{t}\right)\right\|_{*}\mathbf{s},\]

where

\[\mathbf{s}=-\operatorname*{argmax}_{\|\mathbf{s}\|\leq 1}\mathbf{s}^{\top} \left(\mathbf{A}^{\top}\left(\frac{1}{\sum_{t=1}^{T}\alpha_{t}}\sum_{t=1}^{T} \alpha_{t}\mathbf{p}_{t}\right)\right).\]

Therefore,

\[\Phi(\mathbf{u})=\frac{1}{2}\|\mathbf{u}\|^{2}=\frac{1}{2}\left\|\mathbf{A}^{ \top}\left(\frac{1}{\sum_{t=1}^{T}\alpha_{t}}\sum_{t=1}^{T}\alpha_{t}\mathbf{ p}_{t}\right)\right\|_{*}^{2}\leq\frac{1}{2},\]

where the inequality is based on the triangle-inequality and the assumption that the dual norm of data is upper bounded by 1. Finally, for the \(\mathbf{p}\)-player, since it uses FTRL\({}^{+}\), based on Lemma 6, we have

\[\sum_{t=1}^{T}\alpha_{t}\ell_{t}(\mathbf{p}_{t})-\sum_{t=1}^{T}\ell_{t}( \mathbf{p}^{*})\leq\log n-\sum_{t=1}^{T}\frac{1}{2}\|\mathbf{p}_{t}-\mathbf{p} _{t-1}\|_{1}^{2}.\]

To summarize, and let \(\alpha_{t}=\frac{\lambda}{2}\), we have

\[\frac{\text{Reg}_{T}^{\mathbf{w}}+\text{Reg}_{T}^{\mathbf{p}}}{\sum_{t=1}^{T} \alpha_{t}}=\frac{\frac{\lambda}{4}+\log n}{T\lambda}.\]

[MISSING_PAGE_EMPTY:29]

Next, let \(\mathbf{g}_{t}=-\frac{1}{\sum_{j=1}^{t}\alpha_{j}}\sum_{j=1}^{t}\alpha_{j}\mathbf{ A}^{\top}\mathbf{p}_{j}\), we know

\[\mathbf{g}_{t}=\frac{\sum_{j=1}^{t-1}\alpha_{j}}{\sum_{j=1}^{t}\alpha_{j}} \mathbf{g}_{t}+\left(-\frac{\alpha_{t}}{\sum_{j=1}^{t}\alpha_{j}}\mathbf{A}^{ \top}\mathbf{p}_{t}\right).\]

For the \(\mathbf{p}\)-player, it is clear that due to the optimistic term, we have

\[-\mathbf{A}^{\top}\mathbf{p}_{t}=\frac{\nabla L(c\widetilde{\mathbf{w}}_{t-1} +c\alpha_{t}\mathbf{w}_{t-1})}{L(c\widetilde{\mathbf{w}}_{t-1}+c\alpha_{t} \mathbf{w}_{t-1})}.\]

To summarize, and let \(\alpha_{t}=t\), we can conclude the proof by the following algorithm:

\[\mathbf{g}_{t} =\frac{t-1}{t+1}\mathbf{g}_{t-1}+\frac{2}{t+1}\frac{\nabla L(c \widetilde{\mathbf{w}}_{t-1}+ct\mathbf{w}_{t-1})}{L(c\widetilde{\mathbf{w}}_{ t-1}+ct\mathbf{w}_{t-1})}=\frac{t-1}{t+1}\mathbf{g}_{t-1}+\frac{2}{t+1}\frac{ \nabla L(c\widetilde{\mathbf{w}}_{t-1}+ct\|\mathbf{g}_{t-1}\|_{*}\mathbf{s}_{ t-1})}{L(c\widetilde{\mathbf{w}}_{t-1}+ct\|\mathbf{g}_{t-1}\|_{*}\mathbf{s}_{ t-1})},\] \[\mathbf{s}_{t} =\ =-\operatorname*{argmax}_{\|\mathbf{s}\|\leq 1}-\mathbf{s}^{ \top}\mathbf{g}_{t}=\operatorname*{argmin}_{\|\mathbf{s}\|\leq 1}\mathbf{s}^{\top}\mathbf{g}_{t},\] \[\widetilde{\mathbf{w}}_{t} =\widetilde{\mathbf{w}}_{t-1}+t\|\mathbf{g}_{t}\|_{*}\mathbf{s}_ {t},\]

and let \(\beta_{t,3}=\frac{t-1}{t+1}\), \(\beta_{t,4}=\frac{\lambda}{4}\), \(\beta^{\prime}_{t,4}=\frac{\lambda t\|\mathbf{g}_{t-1}\|_{*}}{4}\), \(\beta^{\prime}_{t,3}=\frac{2}{(t+1)L(\beta_{t,4}\mathbf{v}_{t-1}+\beta^{\prime }_{t,4}\mathbf{s}_{t-1})}\), \(\eta_{t}=t\|\mathbf{g}_{t}\|_{*}\). Finally, we focus on the regret bound. For the \(\mathbf{w}\)-player, note that \(\Phi(\mathbf{w})\) is \(\lambda\)-strongly convex with respect to \(\|\cdot\|\). Thus, based on Lemma 3, we have

\[\sum_{t=1}^{T}\alpha_{t}h_{t}(\mathbf{w}_{t})-\sum_{t=1}^{T}\alpha_{t}h_{t}( \mathbf{w})\leq-\sum_{t=1}^{T}\frac{\lambda(t-1)}{4}\|\mathbf{w}_{t}-\mathbf{ w}_{t-1}\|^{2}.\] (52)

On the other hand, note that \(c=\frac{\lambda}{4}\), so based on Lemma 8, we have

\[\sum_{t=1}^{T}\alpha_{t}\ell_{t}(\mathbf{p}_{t})-\sum_{t=1}^{T} \alpha_{t}\ell_{t}(\mathbf{p}) \leq\frac{4\log n}{\lambda}+\frac{\lambda}{8}\sum_{t=1}^{T}t^{2} \|\mathbf{A}\mathbf{w}_{t}-\mathbf{A}\mathbf{w}_{t-1}\|_{\infty}^{2}\] \[=\frac{4\log n}{\lambda}+\frac{\lambda}{8}\sum_{t=1}^{T}t^{2} \left(\max_{i\in[n]}|y^{(i)}\mathbf{x}^{(i)\top}(\mathbf{w}_{t}-\mathbf{w}_{t -1})|\right)^{2}\] (53) \[\leq\frac{4\log n}{\lambda}+\frac{\lambda}{8}\sum_{t=1}^{T}t^{2} \|\mathbf{w}_{t}-\mathbf{w}_{t-1}\|^{2}.\]

It is easy to verify that \(\frac{t^{2}}{8}\leq\frac{t(t-1)}{4}\) for \(t\geq 2\). So to summarize we get

\[C_{T}=\frac{8\log n}{\lambda T^{2}}.\]

The proof can be finished by plugging in Theorem 1.

## Appendix F Regret Bounds for OCO Algorithms

In this section, we provide standard regret bounds for Follow-The-Leader\({}^{+}\) (FTL\({}^{+}\)), Optimistic Follow-The-Leader (OptimisticFTL), Follow-The-Regularized-Leader (FTRL), Follow-The-Regularized-Leader\({}^{+}\)(FTRL\({}^{+}\)), and Optimistic Follow-The-Regularized-Leader (Optimistic FTRL). Lemmas 3, 4, 5 and 6 are based on Lemma of 3 of Wang et al. (2021b), and Lemmas 7 and 8 comes from Theorems 6.8 and 7.35 of Orabona (2019).

**Lemma 3**.: _Consider a weighted online learning problem with a series of \(\lambda\)-strongly functions \(f_{1},\ldots,f_{T}\), and a series of corresponding parameters \(\alpha_{1},\ldots,\alpha_{T}\). The \(FTL^{+}\) algorithm, given by_

\[\mathbf{z}_{t}=\operatorname*{argmin}_{\mathbf{z}\in\mathbb{R}^{d}}\sum_{i=1}^ {t}\alpha_{i}f_{i}(\mathbf{z}),\]

_achieves the following regret bound:_

\[\forall\mathbf{z}\in\mathbb{R}^{d},\ \sum_{t=1}^{T}\alpha_{t}f_{t}(\mathbf{z} _{t})-\sum_{t=1}^{T}\alpha_{t}f_{t}(\mathbf{z})\leq\ -\sum_{t=1}^{T}\left(\frac{\lambda\sum_{s=1}^{t-1}\alpha_{s}}{2}\right)\| \mathbf{z}_{t}-\mathbf{z}_{t-1}\|^{2}.\]

**Lemma 4**.: _Consider a weighted online learning problem with a series of \(\lambda\)-strongly functions \(f_{1},\ldots,f_{T}\), and a series of corresponding parameters \(\alpha_{1},\ldots,\alpha_{T}\). Let \(\widehat{\mathbf{z}}_{t}=\operatorname*{argmin}_{\mathbf{z}\in\mathbb{R}^{d}} \sum_{i=1}^{t-1}\alpha_{i}h_{i}(\mathbf{z})\). Then, the Optimistic FTL algorithm, given by_

\[\mathbf{z}_{t}=\operatorname*{argmin}_{\mathbf{z}\in\mathbb{R}^{d}}\sum_{i=1}^{ t-1}\alpha_{i}f_{i}(\mathbf{z})+\alpha_{t}f_{t-1}(\mathbf{z}),\]

_achieves the following regret \(\forall\mathbf{z}\in\mathbb{R}^{d}\):_

\[\sum_{t=1}^{T}\alpha_{t}f_{t}(\mathbf{z}_{t})-\sum_{t=1}^{T} \alpha_{t}f_{t}(\mathbf{z})\] \[\leq \sum_{t=1}^{T}\alpha_{t}\left(f_{t}(\mathbf{z}_{t})-f_{t}( \widehat{\mathbf{z}}_{t+1})-f_{t-1}(\mathbf{z}_{t})+f_{t-1}(\widehat{\mathbf{ z}}_{t+1})\right)-\sum_{t=1}^{T}\frac{\lambda(\sum_{i=1}^{t}\alpha_{i})}{2}\| \mathbf{z}_{t}-\widehat{\mathbf{z}}_{t+1}\|^{2}.\]

**Lemma 5**.: _Consider a weighted online learning problem with a series of convex functions \(f_{1},\ldots,f_{T}\), and a series of corresponding parameters \(\alpha_{1},\ldots,\alpha_{T}\). Let \(R(\mathbf{z})\) be a \(1\)-strongly convex regularizer wrt \(\|\cdot\|\). The FTRL algorithm, given by_

\[\mathbf{z}_{t}=\operatorname*{argmin}_{\mathbf{z}\in\mathbb{R}^{d}}\sum_{i=1} ^{t-1}\alpha_{i}f_{i}(\mathbf{z})+R(\mathbf{z}),\]

_achieves the following regret bound:_

\[\forall\mathbf{z}\in\mathbb{R}^{d},\ \sum_{t=1}^{T}\alpha_{t}f_{t}(\mathbf{z}_{t}) -\sum_{t=1}^{T}\alpha_{t}f_{t}(\mathbf{z})\leq R(\mathbf{z})+2\sum_{t=1}^{T} \alpha_{t}^{2}\|\nabla f_{t}(\mathbf{z}_{t})\|_{*}^{2}.\]

**Lemma 6**.: _Consider a weighted online learning problem with a series of convex functions \(f_{1},\ldots,f_{T}\), and a series of corresponding parameters \(\alpha_{1},\ldots,\alpha_{T}\). Let \(R(\mathbf{z})\) be a \(1\)-strongly convex regularizer wrt \(\|\cdot\|\). The FTRL\({}^{+}\) algorithm, given by_

\[\mathbf{z}_{t}=\operatorname*{argmin}_{\mathbf{z}\in\mathbb{R}^{d}}\eta\sum_{ i=1}^{t}\alpha_{i}f_{i}(\mathbf{z})+R(\mathbf{z}),\]

_achieves the following regret bound:_

\[\forall\mathbf{z}\in\mathcal{Z},\ \sum_{t=1}^{T}\alpha_{t}f_{t}(\mathbf{z}_{t}) -\sum_{t=1}^{T}\alpha_{t}f_{t}(\mathbf{z})\leq\frac{R(\mathbf{z})}{\eta}-\sum _{t=1}^{T}\frac{1}{2\eta}\|\mathbf{z}_{t}-\mathbf{z}_{t-1}\|^{2}.\] (54)

**Lemma 7**.: _Consider a weighted online learning problem with a series of convex functions \(f_{1},\ldots,f_{T}\), and a series of corresponding parameters \(\alpha_{1},\ldots,\alpha_{T}\). Let \(\Phi(\mathbf{z})\) be a \(\beta\)-strongly convex regularizer wrt \(\|\cdot\|\). Let \(\mathbf{z}_{0}\in\mathbb{R}^{d}\) be the initial point. The OMD algorithm, given by_

\[\nabla\Phi(\mathbf{z}_{t})=\nabla\Phi(\mathbf{z}_{t-1})-\eta\alpha_{t-1} \nabla f_{t-1}(\mathbf{z}_{t-1}),\]

_achieves the following regret bound:_

\[\forall\mathbf{z}\in\mathbb{R}^{d},\ \sum_{t=1}^{T}\alpha_{t}f_{t}(\mathbf{z}_{t })-\sum_{t=1}^{T}\alpha_{t}f_{t}(\mathbf{z})\leq\frac{D_{\Phi}(\mathbf{z}; \mathbf{z}_{0})}{\eta}+\frac{1}{\beta}\sum_{t=1}^{T}\eta\alpha_{t}^{2}\|\nabla f _{t}(\mathbf{z}_{t})\|_{*}^{2}.\]

**Lemma 8**.: _Consider a weighted online learning problem with a series of convex functions \(f_{1},\ldots,f_{T}\), and a series of corresponding parameters \(\alpha_{1},\ldots,\alpha_{T}\). Let \(R(\mathbf{z})\) be a \(1\)-strongly convex regularizer wrt \(\|\cdot\|\), and \(\psi_{t}(\mathbf{z})\) be the optimism term in round \(t\). The Optimistic FTRL algorithm, given by_

\[\mathbf{z}_{t}=\operatorname*{argmin}_{\mathbf{z}\in\mathbb{R}^{d}}\sum_{i=1}^ {t-1}\alpha_{i}f_{i}(\mathbf{z})+\alpha_{t}\psi_{t}(\mathbf{z})+\frac{1}{\eta }R(\mathbf{z}),\]

_achieves the following regret bound:_

\[\forall\mathbf{z}\in\mathcal{Z},\ \sum_{t=1}^{T}\alpha_{t}f_{t}(\mathbf{z}_{t })-\sum_{t=1}^{T}\alpha_{t}f_{t}(\mathbf{z})\leq\frac{R(\mathbf{z})}{\eta}+ \sum_{t=1}^{T}\bigg{[}\frac{\|\alpha_{t}\nabla f_{t}(\mathbf{z}_{t})-\alpha_{t }\nabla\psi_{t}(\mathbf{z}_{t})\|_{*}^{2}}{2/\eta}.\bigg{]}\] (55)