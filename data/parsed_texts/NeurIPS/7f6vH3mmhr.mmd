# Multi-Agent Learning with Heterogeneous Linear Contextual Bandits

 Anh Do

Johns Hopkins University

ado8@jhu.edu

&Thanh Nguyen-Tang

Johns Hopkins University

nguyent@cs.jhu.edu

&Raman Arora

Johns Hopkins University

arora@cs.jhu.edu

###### Abstract

As trained intelligent systems become increasingly pervasive, multi-agent learning has emerged as a popular framework for studying complex interactions between autonomous agents. Yet, a formal understanding of how and when learners in heterogeneous environments benefit from sharing their respective experiences is still in its infancy. In this paper, we seek answers to these questions in the context of linear contextual bandits. We present a novel distributed learning algorithm based on the upper confidence bound (UCB) algorithm, which we refer to as H-LinUCB, wherein agents cooperatively minimize the group regret under the coordination of a central server. In the setting where the level of heterogeneity or dissimilarity across the environments is known to the agents, we show that H-LinUCB is provably optimal in regimes where the tasks are highly similar or highly dissimilar.

## 1 Introduction

Heterogeneous multi-agent systems enable agents to work together and coordinate their actions to solve complex problems. These systems are inherently scalable, as they can distribute the computational load across multiple agents. This scalability allows the system to handle large and sophisticated tasks beyond the capabilities of a single agent. Despite the potential of multi-agent systems, it poses the following fundamental challenges.

* **Statistical challenge.** Each agent's reward distribution may vary, meaning that different agents receive different rewards for the same action. This heterogeneity in reward distributions introduces complexity and makes coordination among agents more difficult. Furthermore, Wang et al. (2021) point out that an ineffective use of shared data could lead to a significant negative impact on the overall performance. In particular, sharing experiences amongst agents may hinder the system's performance if the tasks are too dissimilar (Rosenstein, 2005; Brunskill and Li, 2013).
* **Computational complexity.** Coordinating the decisions of numerous agents and performing complex computations pose challenges in terms of computational resources, time constraints, and algorithmic scalability.
* **Communication cost.** Efficient communication is also a fundamental challenge for a large-scale multi-agent system. As the number of agents in a system increases, the complexity of interactions between agents grows exponentially. Managing the interaction of numerous agents and making decisions in a timely manner becomes increasingly difficult.

Several works address these challenges for heterogeneous multi-agent systems, including federated linear bandits (Huang et al., 2021; Li and Wang, 2022), clustering bandits (Gentile et al., 2014; Ghosh et al., 2023), multi-task linear bandits (Hu et al., 2021; Yang et al., 2021). However, these works either rely on some special structure of the parameter (Li and Wang, 2022), e.g., a low-rank structure (Huet al., 2021) or make different assumptions, e.g., stochastic contexts, finite decision set (Huang et al., 2021; Ghosh et al., 2023), etc.

In this work, we provide a general notion of heterogeneous multi-agent linear contextual bandits (\(\varepsilon\)-MALCB) and give analytical results under different regimes of heterogeneity. Specifically, we study a model that consists of \(M\) agents. Each agent \(i\in[M]\) plays a \(d\)-dimensional linear contextual bandit, parametrized by \(\theta_{i}\) (see Section 3 for more details), for \(T\) rounds. We capture the heterogeneity by a dissimilarity parameter \(\varepsilon>0\) such that \(\|\theta_{i}-\theta_{j}\|_{2}\leq\varepsilon\), for all \(i,j\in[M]\). Notably, we do not assume any special structure on the linear parameter; and we allow the decision set to be infinite and possibly chosen adversarially.

Motivating application.Consider a personalized recommendation system for online advertisements (Li et al., 2010; Bouneffouf et al., 2020; Ghosh et al., 2023). Here, the platform needs to be adaptive to user preferences and maximize total user clicks based on user-click feedback. Each ad can be represented as a context vector, encoding information such as publisher, topic, ad content, etc. The inner product of the ad's context vector and user preference represents the alignment. A higher inner product value indicates greater relevance of the ad. Furthermore, the recommended ads must be personalized to accommodate user preference differences. One naive approach would involve solving a separate linear contextual bandit problem for each user. However, we can pose the following question: Can we enhance the system's performance by pooling data from other users? If so, to what extent of user heterogeneity can we achieve that?

Contributions.We make the following contributions in this paper.

* First, we formulate the heterogeneous multi-agent linear contextual bandits as \(\varepsilon\)-MALCB problem, building on the classic notion of heterogeneity in multiarmed bandits (MABs) Wang et al. (2021). Our notion of heterogeneity is natural and captures many settings in real-world applications.
* Second, when the level of dissimilarity is known, we propose a distributed algorithm, namely H-LinUCB which achieves a regret of \(\tilde{\mathcal{O}}(d\sqrt{MT}+\min\{\varepsilon dMT,dM\sqrt{T}\})\) under the coordination of a central server. We discuss in detail how to handle the dissimilarity and introduce a criterion for stopping collaboration when the level of dissimilarity is high. We show that under the regime of low dissimilarity, we can still achieve a regret of \(\tilde{\mathcal{O}}(d\sqrt{MT})\), which is the same regret rate as if \(M\) agents collaborate to solve the _same_ task. In this regime, H-LinUCB outperforms independent learners improving by a factor of \(\sqrt{M}\), from \(\tilde{\mathcal{O}}(dM\sqrt{T})\) to \(\tilde{\mathcal{O}}(d\sqrt{MT})\). This is significant when we have a large number of agents.
* Third, we complement the upper bound with a lower bound of \(\Omega(d\sqrt{MT}+\min\{\varepsilon MT,dM\sqrt{T}\})\). This suggests that our theoretical guarantees are tight in settings where tasks are highly similar or highly dissimilar.
* Finally, we validate our theoretical results with numerical simulations on synthetic data. When the level of dissimilarity is small, H-LinUCB outperforms independent learning. When the level of dissimilarity is high, our simulation shows that blindly using shared data can lead to linear regret, emphasizing the importance of the criterion we propose for when to stop the collaboration.

## 2 Related work

The classic linear bandits have a rich literature in both theory and application; see, for example, (Abbasi-Yadkori et al., 2011; Li et al., 2010; Chu et al., 2011; Chatterji et al., 2020; Rusmevichientong and Tsitsiklis, 2010; Bouneffouf et al., 2020; Mahadik et al., 2020), to name a few. With a surge in distributed computing, multi-agent systems have shown their potential and gained more attention in recent years. A large body of works dedicated to studying the homogeneous setting of multiple collaborating agents solve a global linear bandits problem (Wang et al., 2020; Dubey and Pentland, 2020; Moradipari et al., 2022; Mitra et al., 2022; Martinez-Rubio et al., 2019; Chawla et al., 2022).

The problem of multi-agent linear bandits in heterogeneous environments, on the other hand, has received limited attention. Soare et al. (2014) were amongst the first to study heterogeneous linear bandits; however, the focus in that work is not on group regret, and the authors only consider the setting where tasks are similar. More recently, Huang et al. (2021) proposed an algorithm with a novel multi-agent G-Optimal design. They assume that the heterogeneity comes from the contexts associated with each agent, but agents can still collaborate since they share the same arm parameters. Li and Wang (2022) consider an extension where they assume that each agent's parameter has two components - a shared global component and an individual local component. This formalization requires agents to work on their respective tasks (the local component) but still allows agents to collaborate on the common task (the global component). Wang et al. (2021) study heterogeneity of the Bernoulli MABs problem and provide guarantees for both cases when the level of heterogeneity is known and unknown.

A related line of work studies heterogeneous linear bandits through clustering (Gentile et al., 2014; Li et al., 2016, 2019; Korda et al., 2016; Ghosh et al., 2023). These works give a guarantee based on the clustering structure of the different linear bandit problems - agents belonging to the same cluster will likely achieve the highest collaboration gain. We do not make any assumption about the "clusterability" of different bandit problems we may encounter. A yet another approach focuses on multi-task linear bandits, wherein we solve multiple different but closely related linear bandits tasks (Yang et al., 2021, 2022; Hu et al., 2021; Cella et al., 2023; Du et al., 2023). In particular, these works rely on the assumption that all tasks share a common \(k\)-dimensional representation, where \(k\ll d\). Then, pooling data from different bandits helps learn a good representation and reduces the statistical burden of learning by reducing the linear bandit problem in \(d\) dimensions to a \(k\)-dimensional setting. We do not consider multi-task learning here.

Our formulation of heterogeneous contextual linear bandits is similar to that of misspecified and corrupted bandits setting (Remark 3.1 for more details) (Ghosh et al., 2017; Lattimore and Csaba, 2020; Takemura et al., 2021; Foster et al., 2020; He et al., 2022). It is then natural to ask if we can apply the techniques from that part of the literature to deal with the dissimilarity between different bandits in a heterogeneous setting. However, there is a fundamental difference in how the two problems manifest themselves. While misspecification may be typically unavoidable in many settings, in a heterogeneous bandit setting, an agent can always choose to rely solely on its own data if it finds that the data from other agents are too dissimilar.

## 3 Preliminaries

Multi-Agent Linear Contextual Bandits.We consider a multi-agent learning setting with \(M\) agents. At each round \(t\in[T]\), each agent \(m\in[M]\) picks an action (context)1\(x_{m,t}\in\mathcal{D}_{m,t}\), where \(\mathcal{D}_{m,t}\subseteq\mathbb{R}^{d}\) is a given decision set. The agent \(m\) receives reward \(y_{m,t}=x_{m,t}^{\top}\theta_{m}+\eta_{m,t}\), where \(\theta_{m}\in\mathbb{R}^{d}\) is an unknown but fixed parameter and \(\eta_{m,t}\) is sub-Gaussian noise. Let \(\mathcal{F}_{t}\) denote the filtration, i.e., the \(\sigma\)-algebra, induced by \(\sigma(\{x_{m,k}\}_{m\in[M],k\leq t+1},\{\eta_{m,k}\}_{m\in[M],k\leq t})\).

Footnote 1: Throughout the paper, we use the terms _action_ and _context_ interchangeably.

Regret.Our goal is to design algorithms for multi-agent linear contextual bandits that achieve a small group regret defined as

\[\mathcal{R}(M,T)=\sum_{t=1}^{T}\sum_{m=1}^{M}\left(\max_{x\in\mathcal{D}_{m,t }}\langle x,\theta_{m}\rangle-\langle x_{m,t},\theta_{m}\rangle\right).\]

Assumption 3.1.: Without loss of generality, we assume that,

1. **Bounded parameters**: \(\|\theta_{m}\|_{2}\leq 1,\|x\|_{2}\leq 1,\forall x\in\mathcal{D}_{m,t},m\in[M],t \in[T]\).
2. **Sub-Gaussian noise**: \(\eta_{m,t}\) is conditionally zero-mean 1-sub-Gaussian random variable with respect to \(\mathcal{F}_{t-1}\).

We note that the assumptions above are standard in linear bandits literature (Abbasi-Yadkori et al., 2011; Hu et al., 2021; Huang et al., 2021). Further, it is straightforward to let \(\|\theta_{m}\|\leq B\), for some constant \(B\), by appropriately scaling the rewards. We make no additional assumptions on the context. The decision set could be infinite, and given to each agent possibly adversarially.

**Definition 3.1**.: _(\(\varepsilon\)-MALCB Problem) A multi-agent linear contextual bandits problem is said to be an \(\varepsilon\)-MALCB problem instance, if for any two agents \(i,j\in[M]\), \(\|\theta_{i}-\theta_{j}\|_{2}\leq\varepsilon\), for an \(\varepsilon\geq 0\). We call \(\varepsilon\) the dissimilarity parameter._

**Definition 3.2**.: _(Homogeneous setting) A multi-agent linear contextual bandits problem is homogeneous, if it is an \(\varepsilon\)-MALCB with \(\varepsilon=0\), i.e., \(\theta_{i}=\theta_{j}\), for all \(i,j\in[M]\)._

Given the bound on the parameters, we have that \(\|\theta_{i}-\theta_{j}\|_{2}\leq 2\) for any \(i,j\in[M]\). Therefore, it suffices to only consider the case where \(\varepsilon\in[0,2]\).

**Remark 3.1**.: (Misspecified structure) Under the Assumption 3.1, for any two agents \(i,j\) we have that \(\theta_{i}^{\top}x-\varepsilon\leq\theta_{j}^{\top}x\leq\theta_{i}^{\top}x+\varepsilon\). Then, \(\mathbb{E}[y_{j,x}]=\theta_{i}^{\top}x+\Delta(x)\), for \(\Delta(x)\in[-\varepsilon,\varepsilon]\). This represents a misspecified structure wherein agent \(i\) receives the reward \(y_{j,x}\) from agent \(j\neq i\).

**Remark 3.2**.: (Recover the \(\varepsilon\)-MPMAB of Wang et al. (2021)). We note that the \(\varepsilon\)-MPMAB is a special case of \(\varepsilon\)-MALCB. Define the mean reward of \(K\) arms for agent \(m\) as \(\theta_{m}=[\mu_{1}^{m},\ldots,\mu_{K}^{m}]\). Then, the reward for arm \(k\) at round \(t\) is \(y_{t,k}^{m}=\theta_{i}^{\top}e_{k}+\eta_{t}\). The decision set \(\mathcal{D}=\{e_{1},\ldots,e_{K}\}\) are the standard basis vectors. This is a fixed set of arms, given to all agents at each round. The dissimilarity parameter \(\varepsilon\) is defined as: \(\|\theta_{i}-\theta_{j}\|_{\infty}\leq\varepsilon\) for all \(i,j\in[M]\).

Nonetheless, the results in Wang et al. (2021) are not directly comparable to ours since the dissimilarity parameter \(\varepsilon\) hides inside the size of the set of _subpar_ arms \(|\mathcal{I}_{\varepsilon}|\).2 Furthermore, Wang et al. (2021) give guarantees in a full-communication setting, in which each agent has full access to the past data of all other agents at every round.

Footnote 2: RobustAgg\((\varepsilon)\) algorithm achieves \(\tilde{\mathcal{O}}(\sqrt{\mathcal{I}_{\varepsilon}MT}+M\sqrt{(|\mathcal{I}_ {\varepsilon}|-1)T}+M|\mathcal{I}_{\varepsilon}|)\) regret when \(\varepsilon\) is known. The subpar arms \(\mathcal{I}_{\varepsilon}\) is defined in Wang et al. (2021, Section 3.2).

**Remark 3.3**.: There are other formulations that also capture the heterogeneity in multi-agent linear bandits. Huang et al. (2021) consider a multi-agent linear bandits setting with a fixed size decision set, containing \(K\) actions, \(\{\theta_{a}\}_{a=1}^{K}\), which is unknown to the agents. Each agent \(i\) is associated with \(K\) different contexts \(\{x_{i,a}\}_{a=1}^{K}\). At reach round, each agent \(i\) picks an action \(a\in[K]\), and receives reward \(r_{i,a}=x_{i,a}^{\top}\theta_{a}+\eta_{i,a}\). Since \(x_{i,a}\) can vary for different agents, this captures the heterogeneity across agents. It also allows for collaboration across agents since they share the same decision set.

Li and Wang (2022) assume that each agent parameter \(\theta\) has a special structure that consists of a shared global component and a unique local component. The reward of agent \(i\) can be given as,

\[r_{i,x}=\left[\begin{array}{c}\theta^{(g)}\\ \theta^{(i)}\end{array}\right]^{\top}\left[\begin{array}{c}\mathbf{x}^{(g)} \\ \mathbf{x}^{(l)}\end{array}\right]\text{. We do not assume such special structure of the linear parameter.}\]

The notion of \(\varepsilon\)-MALCB is in the worst-case sense. One can imagine an \(\varepsilon\)-MALCB instance of \(M\) agents, such that \(M-1\) agents have identical linear parameter, i.e. \(\theta_{i}=\theta_{j},\forall i,j\in[M-1]\), and the parameter of the last agent \(\|\theta_{M}-\theta_{M-1}\|_{2}=\varepsilon\). With this type of instance, for a large \(M\), we can simply use DisLinUCB of Wang et al. (2021) and achieve nearly optimal regret. Clustering bandits framework could also be used for this \(\varepsilon\)-MALCB instance since it presents a strong cluster structure. Even though our formulation takes a pessimistic approach but it can handle the case that bandits are largely unclustered.

Our goal is to design a system such that its performance is no worse than running \(M\) independent bandit algorithms for each user (zero collaboration). The system should also be adaptive to the heterogeneity in the problem instance, i.e., it should automatically leverage any structure in the problem parameters \(\{\theta_{m}\}_{m=1}^{M}\) to collaboratively solve all bandit problems at a "faster" rate. To benchmark the performance of such a system, we consider the following baseline.

Independent Learners (Ind-OFUL).We establish a baseline algorithm in which each agent independently runs an optimal linear contextual bandits algorithm (OFUL, (Abbasi-Yadkori et al., 2011)) without any communication. Each agent incurs \(\tilde{\mathcal{O}}(d\sqrt{T})\) regret, and \(\tilde{\mathcal{O}}(dM\sqrt{T})\) group regret.

Notation.We denote the weighted norm of vector \(x\) w.r.t. matrix \(A\) (Mahalanobis norm) as \(\|x\|_{A}=\sqrt{x^{\top}Ax}\). We write \(A\succcurlyeq B\) iff \(A-B\) is a positive semi-definite matrix. We use \(\tilde{\mathcal{O}}\left(\cdot\right)\) to hide the polylogarithmic factors in standard Big O notation.

Main Results

In this section, we present H-LinUCB, a UCB-style algorithm for \(\varepsilon\)-MALCB problem, and give guarantees for the case when the dissimilarity \(\varepsilon\) is known to the agents. In Section 4.2, we present a lower bound and discuss the implication of our results in different regimes of dissimilarity. We defer the detailed proof to the Appendix.

```
0: Dissimilarity parameter \(\varepsilon\), number of agents \(M\), number of rounds \(T\), regularization parameter \(\lambda\), dimension \(d\), confidence parameters \(\beta_{t},\forall t\in[T]\), weight threshold parameter \(\alpha\), collaboration budget \(\tau\), synchronization threshold \(D\)
1:\(t_{syn}\gets 1\)\(\triangleright\)\(t_{syn}\) is the index of the last synchronized round
2:\(V_{syn}\gets 0,b_{syn}\gets 0\triangleright V_{syn},b_{syn}\) store the relevant statistics of all agents after synchronization
3:\(V_{epoch,m}\gets 0,b_{epoch,m}\gets 0,\forall m\in[M]\)\(\triangleright\)\(V_{epoch,m},b_{epoch,m}\) store the relevant statistics of agent \(m\) in the current epoch
4:for\(t=1,\cdots,T\)do
5:for Agent \(m=1,\cdots,M\)do
6:if\(t=\tau\)then
7:\(V_{syn}\gets 0,b_{syn}\gets 0\)
8:\(V_{epoch,m}\gets 0,b_{epoch,m}\gets 0\)
9:endif
10:\(V_{m,t}\leftarrow\lambda I+V_{syn}+V_{epoch,m}\)
11:\(\hat{\theta}_{m,t}\gets V_{m,t}^{-1}\left(b_{syn}+b_{epoch,m}\right)\)
12: Construct the confidence ellipsoid \(\mathcal{C}_{m,t}=\left\{\theta\in\mathbb{R}^{d}:\left\|\hat{\theta}_{m,t}- \theta\right\|_{V_{m,t}}\leq\beta_{t}\right\}\)
13:\((x_{m,t},\tilde{\theta}_{m,t})=\operatorname*{arg\,max}_{(x,\theta)\in \mathcal{D}_{m,t}\times\mathcal{C}_{m,t}}\langle\theta,x\rangle\)
14: Play \(x_{m,t}\) and get reward \(y_{m,t}\)
15:\(w_{m,t}\leftarrow\mathds{1}\left[t<\tau\right]\min\left(1,\alpha/\|x_{m,t}\|_ {V_{m,t}^{-1}}\right)+\mathds{1}[t\geq\tau]\)
16:\(V_{epoch,m}\gets V_{epoch,m}+w_{m,t}x_{m,t}x_{m,t}^{\top}\)
17:\(b_{epoch,m}\gets b_{epoch,m}+w_{m,t}x_{m,t}y_{m,t}\)
18:if\(\log\left[\det(V_{m,t}+w_{m,t}x_{m,t}x_{m,t}x_{m,t}^{\top})/\det(\lambda I+V_ {syn})\right]\cdot(t-t_{syn})\geq D\) and \(t<\tau\)then
19: Send a synchronization signal to the server to start a communication round
20:endif
21:if A communication round is started then
22: Agent \(i\) sends \(V_{epoch,i},b_{epoch,i}\) to the server, \(\forall i\in[M]\)
23: Server computes \(V_{syn}\gets V_{syn}+V_{epoch,i},b_{syn}\gets b_{syn}+b_{epoch,i}, \forall i\in[M]\)
24: Server sends \(V_{syn},b_{syn}\) back to all agents
25:\(V_{epoch,i}\gets 0;b_{epoch,i}\gets 0;\forall i\in[M]\)\(\triangleright\) Reset \(V_{epoch,i},b_{epoch,i}\) for the new epoch
26:\(t_{syn}\gets t\)
27:endif
28:endfor
29:endfor ```

**Algorithm 1** H-LinUCB

### H-LinUCB Algorithm

H-LinUCB is a distributed UCB-style algorithm (see Algorithm 1 for pseudocode), in which agents work cooperatively under the coordination of a central server.

H-LinUCB has two learning phases: the _collaboration_ phase (for rounds \(t\in\{1,\ldots,\tau-1\}\)) and the _independent learning_ phase (for rounds \(t\in\{\tau,\ldots,T\}\)), where \(\tau\leq T\) is the collaboration budget. Intuitively, our two-phase learning framework ensures that the agents stop collaboration after \(\tau\) rounds lest they incur a linear regret in bandit environments with large dissimilarity. Naturally, then, the parameter \(\tau\) should depend on the dissimilarity parameter, \(\varepsilon\). We give an optimal choice of \(\tau\) in Theorem 4.1.

At each round \(t<\tau\) (the collaboration phase), each agent's data is weighted to adapt to the dissimilarity across different agents (Line 15). Then, each agent uses the weighted data to construct its Confidence Ellipsoid (Line 12) and makes a decision following the optimism principle (Line 13). When a certain condition is met (Line 18), data is pooled and synchronized across the agents. Starting from round \(\tau\), all collaboration ceases and each agent enters the independent learning mode and runs an independent copy of the OFUL algorithm (Abbasi-Yadkori et al., 2011) locally for the last \(T-\tau+1\) rounds.

We note that H-LinUCB builds upon DisLinUCB of Wang et al. (2020, Protocol 8) with the following modifications:

* We scale each agent's data using the weight \(\min(1,\alpha/\|x_{m,t}\|_{V_{m,t}^{-1}})\), which we adopt from He et al. (2022), to handle the dissimilarity across different agents (Line 15).
* We only allow collaboration until round \(\tau\) (Line 18). The value of \(\tau\) depends on the dissimilarity parameter, which we assume is given.
* We reset the variables \(V_{syn},b_{syn},V_{epoch,m},b_{epoch,m}\) at round \(\tau\) (Lines 6-9), where each agent switches to the independent learning mode. Here, \(epoch\) refers to the time period between two consecutive synchronization rounds.

Each agent uses all of the data available to them at each round to construct the _Confidence Ellipsoid_\(\mathcal{C}_{m,t}\) using the result in Lemma 4.1. Given the confidence ellipsoid, the agent chooses the action optimistically: \((x_{m,t},\tilde{\theta}_{m,t})=\arg\max_{(x,\theta)\in\mathcal{D}_{m,t}\times \mathcal{C}_{m,t}}\langle\theta,x\rangle\). During the collaboration phase, if the variation in the volume of the ellipsoid exceeds a certain synchronization threshold, \(D\), it triggers a synchronization condition (Lines 18-20). Subsequently, the central server commences the synchronization procedure to update \(V_{syn},b_{syn}\) across all participating agents (Lines 21-27. The optimal value of \(D\) depends on the number of agents \(M\), dimension \(d\), and the collaboration budget \(\tau\).

The weight \(\min(1,\alpha/\|x_{m,t}\|_{V_{m,t}^{-1}})\) is a truncation of the inverse bonus, where \(\alpha>0\) is a threshold parameter that shall be optimized later. When \(x_{m,t}\) is not explored much, we have a large exploration bonus \(\|x_{m,t}\|_{V_{m,t}^{-1}}\) (low confidence). Hence, the algorithm will put a small weight on it to avoid a large regret due to stochastic noise and misspecification. When \(\|x_{m,t}\|_{V_{m,t}^{-1}}\) is small (high confidence), H-LinUCB puts a large weight on it, and it can be as large as one (He et al., 2022).3 We note that using this weighting could have a significant negative impact on the performance if we are not careful. Several recent studies show that we can incur a regret of \(\tilde{\mathcal{O}}(d\sqrt{T}+\varepsilon\sqrt{d}T)\) and \(\tilde{\mathcal{O}}(d\sqrt{T}+\varepsilon dT)\) for misspecified and corrupted linear bandits, respectively (Lattimore and Csaba, 2020, Takemura et al., 2021, Foster et al., 2020, He et al., 2022).4 However, a direct application of an algorithm designed for misspecified/corrupted linear bandits to our setting can lead to linear regret when \(\varepsilon=\Theta(1)\). This is significantly worse as compared to naive independent learning, which always achieves sub-linear \(\tilde{\mathcal{O}}(dM\sqrt{T})\) regret.

Footnote 3: The technique of using the exploration bonus to control misspecification is also used in Zhang et al. (2023).

Footnote 4: Takemura et al. (2021), Foster et al. (2020) give guarantees for when the misspecification level is unknown. The CW-OFUL algorithm of He et al. (2022) has \(\tilde{\mathcal{O}}(d\sqrt{T}+dC)\) regret, where \(C\) is the total amount of corruption. Setting \(C=\varepsilon T\), where \(\varepsilon\) is the level of misspecification at each round, gives us \(\tilde{\mathcal{O}}(d\sqrt{T}+\varepsilon dT)\) regret.

We emphasize that the condition \((t<\tau)\) in Line 18 is crucial to avoid linear regret for H-LinUCB in the regime of large \(\varepsilon\). For example, for \(\varepsilon=\Theta(1)\), \(\tau=T\), Algorithm 1 incurs \(\tilde{\mathcal{O}}(dMT)\) regret, which is linear in term of \(MT\). Furthermore, Theorem 4.3 indicates that there exists an instance of \(\varepsilon\)-MALCB such that any algorithm incurs at least \(\Omega(dM\sqrt{T})\) regret. Then, each agent playing OFUL independently would be enough to achieve an optimal \(\tilde{\mathcal{O}}(dM\sqrt{T})\) regret. This suggests that we get a tighter upper bound if we cease collaboration; we discuss the stopping criterion and the choice of \(\tau\) in Theorem 4.2.

Communication protocol.We use a star-shaped communication network where \(M\) agents can interact with a central server (Wang et al., 2020, Dubey and Pentland, 2020). Each agent communicates with the server by uploading and downloading its data but does not communicate directly with each other. The communication will be triggered only if any agent has enough new data since the last synchronization round. Finally, we assume no latency, or error in the communication between the central server and agents.

**Remark 4.1**.: Wang et al. (2020) show that DisLinUCB can rely on old data and produce nearly optimal policy without much communication, only incurring logarithmic factors in the final regret. H-LinUCB has the same communication cost of \(\mathcal{O}(M^{1.5}d^{3})\), as DisLinUCB, which does not depend on the horizon \(T\).

When an agent uses data from other agents, due to the dissimilarity, we need to adjust our confidence bound to make sure the true linear parameter \(\theta_{m}\) lies in the defined ellipsoid with high probability. The next result shows how to construct such a _Confidence Ellipsoid_.

**Lemma 4.1**.: _(Confidence Ellipsoid). With probability at least \(1-M\delta_{1}-M\delta_{2}\), for each agent \(m\in[M]\), \(\theta_{m}\) lies in the confidence set,_

\[\mathcal{C}_{m,t}=\left\{\theta\in\mathbb{R}^{d}:\left\|\hat{\theta}_{m,t}- \theta\right\|_{V_{m,t}}\leq\beta_{t}\right\},\]

_where_

\[\beta_{t}=\begin{cases}\sqrt{\lambda}+\alpha\varepsilon Mt+\sqrt{d\log\left( \frac{1+Mt/(\lambda d)}{\delta_{1}}\right)},\text{ for }t<\tau,\\ \sqrt{\lambda}+\sqrt{d\log\left(\frac{1+t/(\lambda d)}{\delta_{2}}\right)}, \text{ for }t\geq\tau.\end{cases}\]

Note that the result above provides two separate confidence bounds for \(\theta_{m}\), one for the period before round \(\tau\) and the other one for after round \(\tau\). Before round \(\tau\), agent \(m\) will use all the data from other agents to construct \(\mathcal{C}_{m,t}\). The proof follows by first bounding \(\left\|\hat{\theta}_{m,t}-\theta_{m}\right\|_{V_{m,t}(\lambda)}\) as

\[\left\|\hat{\theta}_{m,t}-\theta_{m}\right\|_{V_{m,t}(\lambda)}\leq\underset{ \text{Regularization term}}{\underbrace{I_{1}}}+\underset{\text{Regularization term}}{\underbrace{\left\|\sum_{i\neq m}\sum_{k=1}^{t_{s}}w_{i,k}x_{i,k}x_{i,k}^{\top}( \theta_{i}-\theta_{m})\right\|_{V_{m,t}^{-1}(\lambda)}}}+\underset{\text{Noise term}}{\underbrace{I_{3}}}.\]

Here, \(I_{1}\) is a bounded regularization term, \(I_{3}\) is a noise term that can be bounded by self-normalization lemma (Lemma C.4). Finally, the dissimilarity term \(I_{2}\) is bounded from above by \(\alpha\varepsilon Mt\) using the definition of dissimilarity \(\left\|\theta_{i}-\theta_{m}\right\|\leq\varepsilon\), and applying a similar argument as He et al. (2022) and the choice of the weight \(w_{m,t}=\min(1,\alpha/\|x_{m,t}\|_{V_{m,t}^{-1}})\). For the phase after round \(\tau\), we use the same argument as Abbasi-Yadkori et al. (2011) to construct the confidence bound.

Next, we present the group regret upper bound of Algorithm 1 up to round \(\tau\).

**Theorem 4.1**.: _Given Assumption 3.1, \(T\geq 1\), any \(\tau\leq T\), and \(\delta_{1}>0\), setting \(\lambda=1\) and \(\alpha=\frac{\sqrt{d}}{\varepsilon M\tau}\) in the upper bound of \(\beta_{t}\) on the confidence interval according to Lemma 4.1\(\forall t\in[T]\), and setting the synchronization threshold \(D=\tau\log(M\tau)/(dM)\), we have that with probability at least \(1-M\delta_{1}\), the group regret of Algorithm 1 up to round \(\tau\), is bounded as_

\[\mathcal{R}(M,\tau)\leq 20\sqrt{2}\left(d\sqrt{M\tau}\xi_{\tau}^{2}+ \varepsilon dM\tau\xi_{\tau}^{1.5}\right),\]

_where \(\xi_{t}=\log\left(\frac{1+Mt/(\lambda d)}{\delta_{1}}\right)\)._

Theorem 4.1 shows that Algorithm 1 incurs \(\tilde{\mathcal{O}}(d\sqrt{M\tau})\) regret in the first term (which is the same order as a single agent playing for \(M\tau\) rounds) plus a penalty of using the data from other agents in the order of \(\tilde{\mathcal{O}}(\varepsilon dM\tau)\). The regret of \(\tilde{\mathcal{O}}(d\sqrt{M\tau})\) is unavoidable for any regime of \(\varepsilon\), and this rate is known to be optimal in the case of homogeneous multi-agent. Since we use the technique from He et al. (2022) to handle the dissimilarity, the corruption amount of each round is \(\varepsilon\), and a total corruption of \(\varepsilon dM\tau\) when the central server allows to collaborate up to round \(\tau\). To the best of our knowledge, we are not aware of any UCB-based algorithm for misspecified linear bandits in the setting of infinite arms. We expect that employing a misspecified linear bandits algorithm would achieve a regret of \(\tilde{\mathcal{O}}(d\sqrt{MT}+\varepsilon\sqrt{d}MT)\), which is tighter by a factor of \(\sqrt{d}\). It is worth noting that the CW-OFUL algorithm in He et al. (2022) is designed for handling corruption, whether it can achieve \(\tilde{\mathcal{O}}(d\sqrt{T}+\varepsilon\sqrt{d}T)\) in a misspecified setting remains an open question.

We now present our main result giving an upper bound on the group regret of H-LinUCB.

**Theorem 4.2**.: _Given Assumption 3.1, \(T\geq 1\), let \(\lambda=1,\alpha=\frac{\sqrt{d}}{\varepsilon dM\tau},\delta_{1}=\delta_{2}= \frac{1}{M^{2}T^{2}}\) in the upper bound of \(\beta_{t}\) on the confidence interval (see Lemma 4.1) \(\forall t\in[T]\). Let \(\tau=\min(\lfloor\frac{1}{2e^{2}}\rfloor,T)\), and let the synchronization threshold \(D=\tau\log(M\tau)/(dM)\). Then, the expected group regret of Algorithm 1 is bounded as_

\[\mathbb{E}\left[\mathcal{R}(M,T)\right]\leq 320\sqrt{2}\left(d\sqrt{MT}+2\min \left\{\varepsilon dMT,dM\sqrt{T}\right\}\right)\log^{2}(MT).\]

Here, \(\tau\) is the maximum round that the central server allows communication. After that, all agents switch to independent learning. By choosing \(\tau=\min(\lfloor\frac{1}{2e^{2}}\rfloor,T)\), agents fully cooperate in the regime \(\varepsilon\in[0,\frac{1}{\sqrt{2T}}]\) if \(T\leq\frac{1}{2\varepsilon^{2}}\), and gradually reduce \(\tau\) as \(\varepsilon\) increases from \(\frac{1}{\sqrt{2T}}\) to \(+\infty\). This is important for avoiding a linear regret since when \(\varepsilon\) is large, most of the regret comes from the \(\varepsilon dMT\) term and dominates the \(d\sqrt{MT}\) term. The condition on Line 6 also discards all of the synchronized data. In the extreme case, when \(\varepsilon>1\), there is no collaboration happening due to the condition in Line 18 failing at every round. In other words, H-LinUCB behaves like Ind-OFUL.

In the other extreme case, when \(\varepsilon=0\), all agents solve identical linear bandits. The weight in Line 15 always evaluates to its minimum value of \(1\) for all \(t\in[T]\) since \(\alpha=\frac{\sqrt{d}}{\varepsilon MT}\rightarrow+\infty\). We have \(t<\varepsilon^{2}\) for all rounds. Therefore, the reset condition in Line 6 is never triggered and H-LinUCB behaves exactly like DisLinUCB, achieving a regret of \(\tilde{\mathcal{O}}(d\sqrt{MT})\), which is optimal up to some logarithmic factors.

Theorem 4.2 suggests that the upper bound of H-LinUCB is tighter than Ind-OFUL for all \(\varepsilon\).

**Remark 4.2**.: Ghosh et al. (2023) also propose a personalized algorithm (PMLB) for the heterogeneous multi-agent linear bandits; however, our problem setting is fundamentally different than that of Ghosh et al. (2023). They consider a finite action set and impose a strong distributional assumption on how contexts are generated, i.e., the stochastic context \(x_{i,t}\) for each action \(i\) and at each round \(t\) is zero-mean and forms a positive-definite covariance matrix. In stark contrast, we consider the adversarial setting where the context set is _adversarially_ generated at each round (and thus, the associated action set can be infinite and arbitrary). This renders the algorithm and guarantees of Ghosh et al. (2023) inapplicable in our setting and, thus, requires a completely different treatment. Those assumptions of Ghosh et al. (2023) are crucial for them to obtain the \(\tilde{\mathcal{O}}(T^{1/4})\) bound (for \(\varepsilon=0\)). We note that this bound is not information-theoretically possible in our adversarial setting; the minimax lower bound in such settings is \(\Omega(\sqrt{T})\) (Theorem 4.3).

### Lower bound

In this section, we present a lower bound result for the \(\varepsilon\)-MALCB problem. We denote \(\mathcal{R}_{\mathcal{A},\mathcal{I}}(M,T)\) as a regret of algorithm \(\mathcal{A}\) on a problem instance \(\mathcal{I}\) of \(M\) agents run for \(T\) rounds.

**Theorem 4.3**.: _Let \(\mathcal{I}(\varepsilon)\) denote the class of \(\varepsilon\)-MALCB problem instances that satisfy the Assumption 3.1. Then for any \(d,M,T\in\mathbb{Z}^{+}\) with \(\frac{d}{2}\leq T,\frac{d^{2}}{48}\leq T\), \(\forall\varepsilon\geq 0\), we have the following,_

\[\inf_{\mathcal{A}}\sup_{\mathcal{I}\in\mathcal{I}(\varepsilon)}\mathcal{R}_{ \mathcal{A},\mathcal{I}}(M,T)=\Omega\left(d\sqrt{MT}+\min\left\{\varepsilon MT,dM\sqrt{T}\right\}\right).\]

The first term is a straightforward observation that solving an \(\varepsilon\)-MALCB is at least as hard as solving a single linear bandits for \(MT\) rounds, or \(M\) agents solving identical bandits for \(T\) rounds. The second term suggests that we pay an additional regret of \(\varepsilon MT\) for a _small_\(\varepsilon\in[0,\frac{d}{\sqrt{T}}]\), and \(\Omega(dM\sqrt{T})\) for a _large_\(\varepsilon\geq\frac{d}{\sqrt{T}}\). We note that \(\Omega(dM\sqrt{T})\) is also the lower bound of Ind-OFUL when each agent incurs a regret of at least \(\Omega(d\sqrt{T})\). We believe that the analysis of the lower bound could be tightened by using the arguments from misspecified bandits literature, achieving a lower bound of \(\Omega(d\sqrt{MT}+\min\{\varepsilon\sqrt{d}MT,dM\sqrt{T}\})\).

The lower bound suggests that our upper bound is tight up to logarithmic factors in the following extreme regimes, (i) \(\varepsilon\in[0,\frac{1}{\sqrt{MT}}]\), where \(\mathcal{R}(M,T)=\Theta(d\sqrt{MT})\); (ii) \(\varepsilon\in[\frac{d}{\sqrt{T}},+\infty]\), where \(\mathcal{R}(M,T)=\Theta(dM\sqrt{T})\). In regime (i), all agents solve tasks that are similar to one another, yielding the highest collaborative gain. In regime (ii), tasks are highly dissimilar, H-LinUCB turns off the collaboration and lets agents solve their own tasks individually.

Finally, in the regime that \(\varepsilon\in(\frac{1}{\sqrt{MT}},\frac{d}{\sqrt{T}})\), our results illustrate the interpolation between two extremes. In this regime, our upper bound presents a gap of \(d\) in the dissimilarity term \(\varepsilon dMT\) compared to \(\varepsilon MT\) in the lower bound.

The key idea in the proof of Theorem 4.3 is based on an information-theoretic lower bound of Lemma C.1, wherein we extend the result from (single-agent) linear bandits to _heterogeneous_ multi-agent linear bandits. Here, we give the lower bound result without any constraints on the communication, hence, this is also the lower bound of the H-LinUCB algorithm.

## 5 Numerical Simulations

In this section, we provide some numerical simulations to support our theory. Our goal is to address the following question: how does H-LinUCB perform in three different regimes of dissimilarity: (i) \(\varepsilon\in[0,\frac{1}{\sqrt{MT}}]\), (ii) \(\varepsilon\in(\frac{1}{\sqrt{MT}},\frac{d}{\sqrt{T}})\), (iii) \(\varepsilon\in[\frac{d}{\sqrt{T}},+\infty]\)?

We compare the performance of H-LinUCB with that of the following two algorithms: (a) Independent Learers (Ind-OFUL), wherein each agent independently runs OFUL algorithm of Abbasi-Yadkori et al. (2011), and there is no communication between agents (zero collaboration), and (b) DisLinUCB, for which we use the implementation of Wang et al. (2020) without any modification.

Simulation setup.We generate the \(\varepsilon\)-MALCB problem for \(M=60,d=30,T=10000\) via the following procedure. We first choose a value of \(\varepsilon\) in each of the three dissimilarity regimes. Then we create the linear parameters \(\{\theta_{m}\}_{m=1}^{M}\) as follows. Let \(u,\{v_{m}\}_{m=1}^{M}\) be random vectors with unit norm. We set \(\theta_{m}=c\cdot u+\frac{\varepsilon}{2}v_{m}\), where \(c\) is a constant in the range \([0,1-\varepsilon]\). This guarantees \(\|\theta_{m}\|\leq 1\) and \(\|\theta_{i}-\theta_{j}\|\leq\varepsilon\) for any two agents \(i,j\). At each round, for each agent, we create a new decision set with a size of \(50\), each action is random and normalized to 1. The random noise is sampled from the standard normal distribution, \(\eta\sim\mathcal{N}(0,1)\). We run each experiment 10 times, then report the

Figure 1: Simulation on synthetic data with \(M=60,d=30,T=10000\).

group regret averaged over the runs and the confidence intervals in Figure 1. Our code is available here: https://github.com/anhddo/hlinUCB.

Results and discussions.In regime (i), where the level of dissimilarity is small, plots (a) and (b) show that H-LinUCB retains a regret comparable with DisLinUCB.

In regime (ii), plots (c) and (d) illustrate the interpolation between the two extreme regimes.

In regime (iii), plots (e) and (f), DisLinUCB incurs linear regret, H-LinUCB has the same rate with Ind-OFUL. This illustrates that collaboration brings no benefit when the dissimilarity is high.

## 6 Conclusions

In this paper, we studied the _heterogeneous_ multi-agent linear contextual bandit problem. We formulated the problem under the notion of \(\varepsilon\)-MALCB, and provided the upper and lower bounds when \(\varepsilon\) is known. We showed that our results are provably optimal in the regime where tasks are highly similar or highly dissimilar. Finally, we validated our theoretical results with numerical simulations on synthetic data.

A natural avenue for future work would be to close the gap in the regime \(\varepsilon\in(\frac{1}{\sqrt{MT}},\frac{d}{\sqrt{T}})\). Another research direction pertains to designing an adaptive algorithm when \(\varepsilon\) is unknown. Such an algorithm would be practical and flexible enough to apply to a wide range of heterogeneous multi-agent bandit problems. We are also interested in extending this work to a more challenging setting such as Reinforcement Learning.

## Acknowledgements

This research was supported, in part, by DARPA GARD award HR00112020004, NSF CAREER award IIS-1943251, funding from the Institute for Assured Autonomy (IAA) at JHU, and the Spring'22 workshop on "Learning and Games" at the Simons Institute for the Theory of Computing.

## References

* Abbasi-Yadkori et al. (2011) Yasin Abbasi-Yadkori, David Pal, and Csaba Szepesvari. Improved algorithms for linear stochastic bandits. _Advances in neural information processing systems_, 24, 2011.
* Bouneffouf et al. (2020) Djallel Bouneffouf, Irina Rish, and Charu Aggarwal. Survey on applications of multi-armed and contextual bandits. In _2020 IEEE Congress on Evolutionary Computation (CEC)_, pages 1-8. IEEE, 2020.
* Brunskill and Li (2013) Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning. In _Uncertainty in Artificial Intelligence_, page 122. Citeseer, 2013.
* Cella et al. (2023) Leonardo Cella, Karim Lounici, Gregoire Pacreau, and Massimiliano Pontil. Multi-task representation learning with stochastic linear bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 4822-4847. PMLR, 2023.
* Chatterji et al. (2020) Niladri Chatterji, Vidya Muthukumar, and Peter Bartlett. Osom: A simultaneously optimal algorithm for multi-armed and linear contextual bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 1844-1854. PMLR, 2020.
* Chawla et al. (2022) Ronshee Chawla, Abishek Sankararaman, and Sanjay Shakkottai. Multi-agent low-dimensional linear bandits. _IEEE Transactions on Automatic Control_, 2022.
* Chu et al. (2011) Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff functions. In _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, pages 208-214. JMLR Workshop and Conference Proceedings, 2011.
* Du et al. (2016) Yihan Du, Longbo Huang, and Wen Sun. Multi-task representation learning for pure exploration in linear bandits. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 8511-8564. PMLR, 23-29 Jul 2023.
* Dubey and SandyPentland (2020) Abhimanyu Dubey and Alex'SandyPentland. Differentially-private federated linear bandits. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 6003-6014. Curran Associates, Inc., 2020.
* Foster et al. (2020) Dylan J Foster, Claudio Gentile, Mehryar Mohri, and Julian Zimmert. Adapting to misspecification in contextual bandits. _Advances in Neural Information Processing Systems_, 33:11478-11489, 2020.
* Gentile et al. (2014) Claudio Gentile, Shuai Li, and Giovanni Zappella. Online clustering of bandits. In _International Conference on Machine Learning_, pages 757-765. PMLR, 2014.
* Ghosh et al. (2017) Avishek Ghosh, Sayak Ray Chowdhury, and Aditya Gopalan. Misspecified linear bandits. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 31, 2017.
* Ghosh et al. (2023) Avishek Ghosh, Abishek Sankararaman, and Kannan Ramchandran. Multi-agent heterogeneous stochastic linear bandits. In _Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2022, Grenoble, France, September 19-23, 2022, Proceedings, Part IV_, pages 300-316. Springer, 2023.
* He et al. (2022) Jiafan He, Dongruo Zhou, Tong Zhang, and Quanquan Gu. Nearly optimal algorithms for linear contextual bandits with adversarial corruptions. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Hu et al. (2021) Jiachen Hu, Xiaoyu Chen, Chi Jin, Lihong Li, and Liwei Wang. Near-optimal representation learning for linear bandits and linear rl. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 4349-4358. PMLR, 18-24 Jul 2021.
* Huang et al. (2021) Ruiquan Huang, Weiqiang Wu, Jing Yang, and Cong Shen. Federated linear contextual bandits. _Advances in Neural Information Processing Systems_, 34:27057-27068, 2021.
* Korda et al. (2016) Nathan Korda, Balazs Szorenyi, and Shuai Li. Distributed clustering of linear bandits in peer to peer networks. In _International conference on machine learning_, pages 1301-1309. PMLR, 2016.
* Lattimore and Csaba (2020) Tor Lattimore and Szepesvari Csaba. _Bandit algorithms_. Cambridge University Press, 2020.
* Li and Wang (2022) Chuanhao Li and Hongning Wang. Asynchronous upper confidence bound algorithms for federated linear bandits. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151 of _Proceedings of Machine Learning Research_, pages 6529-6553. PMLR, 28-30 Mar 2022.
* WWW '10_, 2010. doi: 10.1145/1772690.1772758.
* Li et al. (2016) Shuai Li, Claudio Gentile, and Alexandros Karatzoglou. Graph clustering bandits for recommendation. _arXiv preprint arXiv:1605.00596_, 2016.
* Li et al. (2019) Shuai Li, Wei Chen, Shuai Li, and Kwong-Sak Leung. Improved algorithm on online clustering of bandits. In _Proceedings of the 28th International Joint Conference on Artificial Intelligence_, IJCAI'19, page 2923-2929. AAAI Press, 2019. ISBN 9780999241141.
* Mahadik et al. (2020) Kanak Mahadik, Qingyun Wu, Shuai Li, and Amit Sabne. Fast distributed bandits for online recommendation systems. In _Proceedings of the 34th ACM international conference on supercomputing_, pages 1-13, 2020.
* Ma et al. (2017)David Martinez-Rubio, Varun Kanade, and Patrick Rebeschini. Decentralized cooperative stochastic bandits. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.
* Mitra et al. (2022) Aritra Mitra, Arman Adibi, George J. Pappas, and Hamed Hassani. Collaborative linear bandits with adversarial agents: Near-optimal regret bounds. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Moradipari et al. (2022) Ahmadreza Moradipari, Mohammad Ghavamzadeh, and Mahnoosh Alizadeh. Collaborative multi-agent stochastic linear bandits. In _2022 American Control Conference (ACC)_, pages 2761-2766. IEEE, 2022.
* Rosenstein (2005) Michael T. Rosenstein. To transfer or not to transfer. In _NIPS 2005_, 2005.
* Rusmevichientong and Tsitsiklis (2010) Paat Rusmevichientong and John N Tsitsiklis. Linearly parameterized bandits. _Mathematics of Operations Research_, 35(2):395-411, 2010.
* Soare et al. (2014) Marta Soare, Ouais Alsharif, Alessandro Lazaric, and Joelle Pineau. Multi-task linear bandits. In _NIPS2014 workshop on transfer and multi-task learning: theory meets practice_, 2014.
* Takemura et al. (2021) Kei Takemura, Shinji Ito, Daisuke Hatano, Hanna Sumita, Takuro Fukunaga, Naonori Kakimura, and Ken-ichi Kawarabayashi. A parameter-free algorithm for misspecified linear contextual bandits. In _International Conference on Artificial Intelligence and Statistics_, pages 3367-3375. PMLR, 2021.
* Wang et al. (2020) Yuanhao Wang, Jiachen Hu, Xiaoyu Chen, and Liwei Wang. Distributed bandit learning: Near-optimal regret with efficient communication. In _International Conference on Learning Representations_, 2020.
* Wang et al. (2021) Zhi Wang, Chicheng Zhang, Manish Kumar Singh, Laurel Riek, and Kamalika Chaudhuri. Multitask bandit learning through heterogeneous feedback aggregation. In Arindam Banerjee and Kenji Fukumizu, editors, _Proceedings of The 24th International Conference on Artificial Intelligence and Statistics_, volume 130 of _Proceedings of Machine Learning Research_, pages 1531-1539. PMLR, 13-15 Apr 2021.
* Yang et al. (2021) Jiaqi Yang, Wei Hu, Jason D. Lee, and Simon Shaolei Du. Impact of representation learning in linear bandits. In _International Conference on Learning Representations_, 2021.
* Yang et al. (2022) Jiaqi Yang, Qi Lei, Jason D Lee, and Simon S Du. Nearly minimax algorithms for linear bandits with shared representation. _arXiv preprint arXiv:2203.15664_, 2022.
* Zhang et al. (2023) Weitong Zhang, Jiafan He, Zhiyuan Fan, and Quanquan Gu. On the interplay between misspecification and sub-optimality gap in linear contextual bandits. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 41111-41132. PMLR, 23-29 Jul 2023.

Proof of Upper Bound

### Proof of Lemma 4.1

Proof.: We prove that the true parameter of each agent lies in the _Confidence Ellipsoid_ with high probability. In particular, we prove such results for two periods, for round \(t<\tau\) and \(t\geq\tau\). We denote \(\delta_{1},\delta_{2}\) as the probabilities such that true linear parameters lie outside the confidence ellipsoid for the period of \(t<\tau\), and \(t\geq\tau\), respectively.

For the round \(t<\tau\), let \(t_{s}\) be the last round that synchronization occurs, where \(t_{s}<t\). Then each agent's statistics can be divided into two parts, its own data and the pooled data from other agents since round \(t_{s}\). Specifically, for an agent \(m\), at round \(t\), we have the following for \(V_{m,t},b_{m,t}\),

\[V_{m,t} =\lambda I+\sum_{k=1}^{t-1}w_{m,k}x_{m,k}x_{m,k}^{\top}+\sum_{i \neq m}\sum_{k=1}^{t_{s}}w_{i,k}x_{i,k}x_{i,k}^{\top},\] \[b_{m,t} =\sum_{k=1}^{t-1}w_{m,k}x_{m,k}y_{m,k}+\sum_{i\neq m}\sum_{k=1}^{ t_{s}}w_{i,k}x_{i,k}y_{i,k}.\]

We then have the following for \(\hat{\theta}_{m,t}\),

\[\hat{\theta}_{m,t} =V_{m,t}^{-1}b_{m,t}\] \[=V_{m,t}^{-1}\left[\sum_{k=1}^{t-1}w_{m,k}x_{m,k}y_{m,k}+\sum_{i \neq m}\sum_{k=1}^{t_{s}}w_{i,k}x_{i,k}y_{i,k}\right]\] \[=V_{m,t}^{-1}\left[\sum_{k=1}^{t-1}w_{m,k}x_{m,k}\left(x_{m,k}^{ \top}\theta_{m}+\eta_{m,k}\right)+\sum_{i\neq m}\sum_{k=1}^{t_{s}}w_{i,k}x_{i, k}\left(x_{i,k}^{\top}\theta_{i}+\eta_{i,k}\right)\right]\] \[=V_{m,t}^{-1}\left[\left(\sum_{k=1}^{t-1}w_{m,k}x_{m,k}x_{m,k}^{ \top}+\sum_{i\neq m}\sum_{k=1}^{t_{s}}w_{i,k}x_{i,k}x_{i,k}^{\top}\right) \theta_{m}\right]\] \[\quad+V_{m,t}^{-1}\left(\sum_{i\neq m}\sum_{k=1}^{t_{s}}w_{i,k}x _{i,k}x_{i,k}^{\top}\left(\theta_{i}-\theta_{m}\right)\right)\] \[\quad+V_{m,t}^{-1}\left[\sum_{k=1}^{t-1}w_{m,k}x_{m,k}\eta_{m,k} +\sum_{i\neq m}\sum_{k=1}^{t_{s}}w_{i,k}x_{i,k}\eta_{i,k}\right].\]

Notice that the summation in the first term is the covariance matrix \(V_{m,t}\) but missing the regularization term \(\lambda I\); thus, the first term can be simplified as \(\theta_{m}-\lambda V_{m,t}^{-1}\theta_{m}\). Applying triangle inequality, we have the following for \(\left\|\hat{\theta}_{m,t}-\theta_{m}\right\|_{V_{m,t}}\),

\[\left\|\hat{\theta}_{m,t}-\theta_{m}\right\|_{V_{m,t}} \leq\underbrace{\lambda\left\|\theta_{m}\right\|_{V_{m,t}^{-1}}}_{ I_{1}:\text{Regularization term}}+\underbrace{\left\|\sum_{i\neq m}\sum_{k=1}^{t_{s}}w_{i,k}x_{i,k}x_{i,k}^{\top} (\theta_{i}-\theta_{m})\right\|_{V_{m,t}^{-1}}}_{I_{2}:\text{Dissimilarity term}}\] \[\quad+\underbrace{\left\|\sum_{k=1}^{t-1}w_{m,k}x_{m,k}\eta_{m,k} +\sum_{i\neq m}\sum_{k=1}^{t_{s}}w_{i,k}x_{i,k}\eta_{i,k}\right\|_{V_{m,t}^{- 1}}}_{I_{3}:\text{Noise term}}.\]

For the regularization term \(I_{1}\), it is upper bounded by,

\[I_{1}=\lambda\left\|\theta_{m}\right\|_{V_{m,t}^{-1}}\leq\frac{\lambda}{ \lambda_{\min}\left(V_{m,t}^{1/2}\right)}\left\|\theta_{m}\right\|_{2}\leq \sqrt{\lambda}\left\|\theta_{m}\right\|_{2}\leq\sqrt{\lambda}.\]For the dissimilarity term \(I_{2}\), we have,

\[I_{2} =\left\|\sum_{i\neq m}\sum_{k=1}^{t_{s}}w_{i,k}x_{i,k}x_{i,k}^{\top }(\theta_{i}-\theta_{m})\right\|_{V_{m,t}^{-1}}\] \[\leq\sum_{i=1}^{M}\sum_{k=1}^{t_{s}}\left\|w_{i,k}x_{i,k}x_{i,k}^{ \top}(\theta_{i}-\theta_{m})\right\|_{V_{m,t}^{-1}}\] \[=\sum_{i=1}^{M}\sum_{k=1}^{t_{s}}w_{i,k}\left|x_{i,k}^{\top}( \theta_{i}-\theta_{m})\right|\left\|x_{i,k}\right\|_{V_{m,t}^{-1}}\] \[\leq\sum_{i=1}^{M}\sum_{k=1}^{t_{s}}\|x_{i,k}\|\|\theta_{i}- \theta_{m}\|\alpha\] \[\leq\alpha st_{s},\]

where we use triangle inequality in the first inequality, the second inequality holds since \(w_{m,t}\leq\alpha/\|x_{m,t}\|_{V_{m,t}^{-1}}\) by the definition of \(w_{m,t}\), the last inequality holds due to the bounded parameters assumption and the \(\varepsilon\)-MALCB definition (see Assumption 3.1 and Definition 3.1).

To bound term \(I_{3}\), we denote vector \(\bar{x}_{m,t}=\sqrt{w_{m,t}}x_{m,t}\), and the random noise \(\bar{\eta}_{m,t}=\sqrt{w_{m,t}}\eta_{m,t}\). We have \(w_{m,t}\leq 1\), therefore, \(\bar{\eta}_{m,t}\) is 1-subGaussian. Furthermore, we rewrite the covariance matrix as \(V_{m,t}=\lambda I+\sum_{s=1}^{t}\bar{x}_{m,s}\bar{x}_{m,s}^{\top}+\sum_{i\neq m }\sum_{k=1}^{t_{s}}\bar{x}_{i,k}\bar{x}_{i,k}^{\top}\). We then have the following for the noise term \(I_{3}\),

\[I_{3} =\left\|\sum_{k=1}^{t-1}\bar{x}_{m,k}\bar{\eta}_{m,k}+\sum_{i\neq m }\sum_{k=1}^{t_{s}}\bar{x}_{i,k}\bar{\eta}_{i,k}\right\|_{V_{m,t}^{-1}}\] \[\leq\sqrt{2\log\left(\frac{\det\left(V_{m,t}\right)^{1/2}\det(V_ {0})^{-1/2}}{\delta_{1}}\right)}\] \[\leq\sqrt{d\log\left(\frac{1+Mt/(\lambda d)}{\delta_{1}}\right)},\]

where we apply Lemma C.4 in the first inequality. Now, putting these three terms together, we have the following _Confidence Ellipsoid_ bound for round \(t<\tau\),

\[\left\|\hat{\theta}_{m,t}-\theta_{m}\right\|_{V_{m,t}}\leq\left(\sqrt{\lambda }+\alpha\varepsilon Mt+\sqrt{d\log\left(\frac{1+Mt/(\lambda d)}{\delta_{1}} \right)}\right).\]

We now turn our attention to the round \(t\geq\tau\). Since agents switch to _independent learning_, we follow the same argument for confidence ellipsoid as in Theorem 2 of Abbasi-Yadkori et al. (2011), and get

\[\beta_{t}=\sqrt{\lambda}+\sqrt{d\log\left(\frac{1+t/(\lambda d)}{\delta_{2}} \right)}.\]

Finally, we complete the proof by taking a union bound over all agents. 

### Proof of Theorem 4.1

Before proving Theorem 4.1, we give an upper bound on the pseudo-regret, \(r_{m,t}\), in the next proposition. The proof follows standard arguments for linear bandits; most of the arguments can be extracted from Wang et al. (2020), Dubey and Pentland (2020).

**Proposition A.1**.: _The pseudo-regret \(r_{t}\) obtained by any agent \(m\) at round \(t\), is upper bounded as_

\[r_{m,t}\leq 2\beta_{t}\min\Big{(}1,\|x_{m,t}\|_{V_{m,t}^{-1}}\Big{)}.\]

Proof.: Recall that an agent makes decisions optimistically: \((x_{m,t},\tilde{\theta}_{m,t})=\underset{(x,\theta)\in\mathcal{D}_{m,t}\times \mathcal{C}_{m,t}}{\arg\max}x^{\top}\theta\).

Let \(x_{m,t}^{\star}\) denote the optimal action at round \(t\) of agent \(m\), i.e., \(x_{m,t}^{\star}=\underset{x\in\mathcal{D}_{m,t}}{\arg\max}x^{\top}\theta_{m}\).

We then have the following for the pseudo-regret

\[r_{m,t} ={x_{m,t}^{\star}}^{\top}\theta_{m}-{x_{m,t}}^{\top}\theta_{m}\] \[\leq{x_{m,t}}^{\top}\tilde{\theta}_{m,t}-{x_{m,t}}^{\top}\theta_{m} (\text{Since }(x_{m,t},\tilde{\theta}_{m,t})\text{ is optimistic})\] \[={\color[rgb]{0,0,0}\definecolor[named]{pgfstrokecolor}{ rgb}{0,0,0}\pgfsys@color@gray@stroke{0}\pgfsys@color@gray@fill{0}\langle V_{m,t}^{-1/2}x_{m,t},V_{m,t}^{ 1/2}(\tilde{\theta}_{m,t}-\theta_{m})\rangle} (V_{m,t}\succcurlyeq 0)\] \[\leq\|x_{m,t}\|_{V_{m,t}^{-1}}\left\|\tilde{\theta}_{m,t}-\theta_ {m}\right\|_{V_{m,t}} (\text{Cauchy-Schwarz's inequality})\] \[\leq\|x_{m,t}\|_{V_{m,t}^{-1}}\left(\|\tilde{\theta}_{m,t}- \tilde{\theta}_{m,t}\|_{V_{m,t}}+\|\tilde{\theta}_{m,t}-\theta_{m}\|_{V_{m,t}} \right) (\text{Triangle inequality})\] \[\leq 2\beta_{t}\|x_{m,t}\|_{V_{m,t}^{-1}} (\text{Since }\theta_{m},\tilde{\theta}_{m,t}\in\mathcal{C}_{t})\] \[\leq 2\beta_{t}\cdot\min\Big{(}1,\|x_{m,t}\|_{V_{m,t}^{-1}} \Big{)},\]

where the last inequality is due to the fact that \(\beta_{t}\geq 1\) and that suboptimality is no more than 2. 

We now proceed to prove Theorem 4.1. The proof technique follows the analysis of group regret in Wang et al. (2020) and applies the arguments from Theorem 4.2 of He et al. (2022) to handle the dissimilarity.

**Theorem A.1** (Theorem 4.1 restated).: _For a given \(T\), and \(\tau\leq T\), if we set \(D=\tau\log(M\tau)/(dM)\), and \(\beta_{t},\forall t\in[\tau]\) according to Lemma 4.1, then, given Assumption 3.1, with probability at least \(1-M\delta_{1}\), the group regret of Algorithm 1 up to round \(\tau\), is upper bounded as_

\[\mathcal{R}(M,\tau)\leq 4\sqrt{2}\Big{(}\sqrt{\lambda dM\tau}( \xi_{\tau})^{1.5}+\alpha\varepsilon\sqrt{d}(M\tau)^{1.5}\sqrt{\xi_{\tau}}+d \sqrt{M\tau}\xi_{\tau}+\frac{\sqrt{\lambda}d\xi_{\tau}}{\alpha}+\varepsilon dM \tau\xi_{\tau}\\ +\frac{(d\xi_{\tau})^{1.5}}{\alpha}+\alpha\varepsilon\sqrt{d}(M \tau\xi_{\tau})^{1.5}+d\sqrt{M\tau}\xi_{\tau}^{2}\Big{)},\]

_where \(\xi_{t}=\log\Big{(}\frac{1+Mt/(\lambda d)}{\delta_{1}}\Big{)}\). Furthermore, if we choose \(\lambda=1,\alpha=\frac{\sqrt{d}}{\varepsilon M\tau}\), then the group regret is upper bounded as_

\[\mathcal{R}(M,\tau)\leq 20\sqrt{2}\left(d\sqrt{M\tau}\xi_{\tau}^{2}+\varepsilon dM \tau\xi_{\tau}^{1.5}\right).\]

_We refer to this group regret as the "collaboration" regret, and denote it as \(\mathcal{R}_{\mathrm{collab}}(M,T)\)._

Proof.: Let \(P\) be the total number of synchronization rounds, then, we can index the synchronization matrix \(V_{syn}\) for \(P\) epoch as \(\{V_{syn,p}\}_{p=1}^{P}\), and define \(V_{p}=\lambda I+V_{syn,p}\). Observe that \(\det(V_{0})=\det(\lambda I)=\lambda^{d}\) and \(\det(V_{P})\leq(\operatorname{trace}(V_{P})/d)^{d}\leq(\lambda+M\tau/d)^{d}\). Therefore,

\[\log\frac{\det(V_{P})}{\det(V_{0})}\leq d\log\left(1+\frac{M\tau}{\lambda d} \right).\] (1)

By telescoping, we have that \(\log\frac{\det(V_{P})}{\det(V_{0})}=\sum_{p=1}^{P}\log\frac{\det(V_{p})}{\det(V_ {p-1})}\). Therefore, we have at most \(R=\lceil d\log(1+\frac{M\tau}{\lambda d})\rceil\) epochs such that \(\log\frac{\det(V_{P})}{\det(V_{p-1})}\geq 1\); otherwise, it violates the conditionin Equation (1). WLOG, we use logarithm base 2 for the determinant ratio. This implies that for _all_ but \(R\) epochs,

\[1\leq\frac{\det(V_{p})}{\det(V_{p-1})}\leq 2,\] (2)

We call the epochs satisfying Equation (2) as _good_ epochs. We imagine a single agent playing \(M(\tau-1)\) actions \(x_{1,1},x_{2,1},\ldots,x_{M-1,\tau-1},x_{M,\tau-1}\) sequentially. Let \(W_{m,t}=\lambda I+\sum_{i=1}^{M}\sum_{s=1}^{t-1}w_{i,s}x_{i,s}x_{i,s}^{\top}+ \sum_{j=1}^{m-1}w_{j,t}x_{j,t}x_{j,t}^{\top}\)be the covariance matrix of this imaginary agent when it gets to \(x_{m,t}.\) If \(x_{m,t}\) belongs to a good epoch, say the \(j\)-th epoch, we have the following:

\[1\leq\frac{\|x_{m,t}\|_{V_{m,t}^{-1}}}{\|x_{m,t}\|_{W_{m,t}^{-1}}}\leq\sqrt{ \frac{\det(W_{m,t})}{\det(V_{m,t})}}\leq\sqrt{\frac{\det(V_{j})}{\det(V_{j-1}) }}\leq\sqrt{2},\] (3)

where the first inequality is due to the fact that \(W_{m,t}\succcurlyeq V_{m,t}\), and the second inequality follows from Lemma C.2.

Now, applying the Proposition A.1, we bound the pseudo-regret \(r_{m,t}\) of these good epochs as follows:

\[r_{m,t} \leq 2\beta_{t}\min\left(1,\|x_{m,t}\|_{V_{m,t}^{-1}}\right)\] \[\leq 2\beta_{t}\min\left(1,\|x_{m,t}\|_{W_{m,t}^{-1}}\sqrt{\frac{ \det(W_{m,t})}{\det(V_{m,t})}}\right)\] \[\leq 2\sqrt{2}\beta_{t}\min\left(1,\|x_{m,t}\|_{W_{m,t}^{-1}} \right),\]

where we use Lemma C.2 in the second inequality, and Equation (3) in the third inequality.

Let \(\mathcal{R}_{\mathrm{good}}(M,\tau)\) be the group regret of these good epochs up to round \(\tau\). Suppose \(P_{\mathrm{good}}\) contains all the good epochs, and \(\mathcal{B}_{p}\) contains all the pairs \((m,t)\) belonging to epoch \(p\). We have

\[\mathcal{R}_{\mathrm{good}}(M,\tau) =\sum_{p\in P_{\mathrm{good}}}\sum_{(m,t)\in\mathcal{B}_{p}}r_{m,t}\] \[\leq\sum_{p\in P_{\mathrm{good}}}\sum_{(m,t)\in\mathcal{B}_{p}}2 \sqrt{2}\beta_{\tau-1}\min\left(1,\|x_{m,t}\|_{W_{m,t}^{-1}}\right)\] \[=\underbrace{\sum_{p\in P_{\mathrm{good}}}\sum_{(m,t)\in\mathcal{ B}_{p}\wedge w_{m,t}=1}2\sqrt{2}\beta_{\tau-1}\min\left(1,\|x_{m,t}\|_{W_{m,t}^{-1}} \right)}_{I_{1}:\text{Rounds of good epoch with }w_{m,t}=1}\] \[\quad+\underbrace{\sum_{p\in P_{\mathrm{good}}}\sum_{(m,t)\in \mathcal{B}_{p}\wedge w_{m,t}<1}2\sqrt{2}\beta_{\tau-1}\min\left(1,\|x_{m,t} \|_{W_{m,t}^{-1}}\right)}_{I_{2}:\text{Rounds of good epoch with }w_{m,t}<1},\]

where in the last equality we split the summation to two cases: \(w_{m,t}=1\), and \(w_{m,t}<1\).

For term \(I_{1}\), we consider all the pair \((m,t)\) in good epochs up to round \(\tau\) such that \(w_{m,t}=1\); we assume that we have a total of such \(K\) pairs, and these pairs can be _sequentially_ listed as \(\{(\tilde{m}_{1},\tilde{t}_{1}),(\tilde{m}_{2},\tilde{t}_{2}),\cdots,(\tilde{ m}_{K},\tilde{t}_{K})\}\). With this notation, for \(i\leq K\), we construct the auxiliary covariance matrix \(A_{\tilde{m}_{i},\tilde{t}_{i}}=\lambda I+\sum_{k=1}^{i-1}x_{\tilde{m}_{k}, \tilde{t}_{k}}x_{\tilde{m}_{k},\tilde{t}_{k}}^{\top}\). Thus, we have \(W_{\tilde{m}_{i},\tilde{t}_{i}}\succcurlyeq A_{\tilde{m}_{i},\tilde{t}_{i}}\), which implies \(\|x_{\tilde{m}_{i},\tilde{t}_{i}}\|_{W_{\tilde{m}_{i},\tilde{t}_{i}}^{-1}}\leq \|x_{\tilde{m}_{i},\tilde{t}_{i}}\|_{A_{\tilde{m}_{i},\tilde{t}_{i}}^{-1}}\).

We now have the following for term \(I_{1}\):

\[I_{1} =\sum_{p\in P_{\mathrm{good}}}\sum_{(m,t)\in\mathcal{B}_{p}\wedge w_{ m,t}=1}2\sqrt{2}\beta_{\tau-1}\min\left(1,\left\|x_{m,t}\right\|_{W^{-1}_{m,t}}\right)\] \[=\sum_{k=1}^{K}2\sqrt{2}\beta_{\tau-1}\min\left(1,\left\|x_{\tilde {m}_{k},\tilde{t}_{k}}\right\|_{W^{-1}_{m_{k},\tilde{t}_{k}}}\right)\] \[\leq\sum_{k=1}^{K}2\sqrt{2}\beta_{\tau-1}\min\left(1,\left\|x_{ \tilde{m}_{k},\tilde{t}_{k}}\right\|_{A^{-1}_{m_{k},\tilde{t}_{k}}}\right)\] \[\leq 2\sqrt{2}\beta_{\tau-1}\sqrt{M\tau}\sqrt{\sum_{i=1}^{K}\min \left(1,\left\|x_{\tilde{m}_{k},\tilde{t}_{k}}\right\|_{A^{-1}_{m_{k},\tilde{t }_{k}}}^{2}\right)}\] \[\leq 4\beta_{\tau-1}\sqrt{2M\tau\log\left(\frac{\det(V_{P})}{ \det(V_{0})}\right)}\] \[\leq 4\sqrt{2}\beta_{\tau-1}\sqrt{dM\tau\log\left(1+\frac{M \tau}{\lambda d}\right)}\] \[\leq 4\sqrt{2}\beta_{\tau-1}\sqrt{dM\tau\xi_{\tau}},\]

where in the second inequality we use Cauchy Schwarz's inequality and the fact that \(K\leq M\tau\), and use Lemma C.3 in the second inequality.

For term \(I_{2}\), since we consider the case such that \(w_{m,t}<1\), thus, by the definition of \(w_{m,t}\) we have \(\frac{1}{\alpha}w_{m,t}\cdot\left\|x_{m,t}\right\|_{V^{-1}_{m,t}}=1\). Therefore, we have,

\[I_{2} =2\sqrt{2}\sum_{p\in P_{\mathrm{good}}}\sum_{(m,t)\in\mathcal{B}_ {p}\wedge w_{m,t}<1}\min\left(1,\beta_{\tau}\|x_{m,t}\|_{W^{-1}_{m,t}}\right)\] \[=2\sqrt{2}\sum_{p\in P_{\mathrm{good}}}\sum_{(m,t)\in\mathcal{B}_ {p}\wedge w_{m,t}<1}\min\left(1,\beta_{\tau}\frac{w_{m,t}\|x_{m,t}\|_{V^{-1}_{ m,t}}}{\alpha}\left\|x_{m,t}\right\|_{W^{-1}_{m,t}}\right)\] \[\leq 4\sum_{p\in P_{\mathrm{good}}}\sum_{(m,t)\in\mathcal{B}_{p} \wedge w_{m,t}<1}\min\left(1,\beta_{\tau}\frac{w_{m,t}\|x_{m,t}\|_{W^{-1}_{m,t }}^{2}}{\alpha}\right),\]

where the last inequality follows from Equation (3).

Similarly to what we have done with \(I_{1}\), we consider all the pair \((m,t)\) in good epochs up to round \(\tau\) such that \(w_{m,t}<1\); we assume that we have a total of such \(K\) pairs, and these pairs can be _sequentially_ listed as \(\{(\bar{m}_{1},\bar{t}_{1}),(\bar{m}_{2},\bar{t}_{2}),\cdots,(\bar{m}_{K},\bar {t}_{K})\}\). Furthermore, we define \(\bar{x}_{m,t}=\sqrt{w_{m,t}}x_{m,t}\), and construct the auxiliary covariance matrix,

\[\bar{W}_{\bar{m}_{i},\bar{t}_{i}}=\lambda I+\sum_{k=1}^{i-1}w_{\bar{m}_{k}, \bar{t}_{k}}x_{\bar{m}_{k},\bar{t}_{k}}x_{\bar{m}_{k},\bar{t}_{k}}^{\top}= \lambda I+\bar{x}_{\bar{m}_{k},\bar{t}_{k}}\bar{x}_{\bar{m}_{k},\bar{t}_{k}}^{ \top}.\]

Thus, we have \(W_{\bar{m}_{i},\bar{t}_{i}}\succcurlyeq\bar{W}_{\bar{m}_{i},\bar{t}_{i}}\), which implies \(\left\|x_{\bar{m}_{i},\bar{t}_{i}}\right\|_{W^{-1}_{\bar{m}_{i},\bar{t}_{i}}} \leq\left\|x_{\bar{m}_{i},\bar{t}_{i}}\right\|_{\bar{W}^{-1}_{\bar{m}_{i},\bar {t}_{i}}}\).

Therefore,

\[I_{2} \leq 4\sum_{k=1}^{K}\min\left(1,\beta_{\tau-1}w_{\bar{m}_{k},\bar{t}_ {k}}/\alpha\left\|x_{\bar{m}_{k},\bar{t}_{k}}\right\|_{W^{-1}_{\bar{m}_{k},\bar {t}_{k}}}^{2}\right)\] \[\leq 4\sum_{k=1}^{K}\min\left(1+\beta_{\tau-1}/\alpha,(1+\beta_{ \tau-1}/\alpha)\|\sqrt{w_{\bar{m}_{k},\bar{t}_{k}}}x_{\bar{m}_{k},\bar{t}_{k}} \|_{W^{-1}_{\bar{m}_{k},\bar{t}_{k}}}^{2}\right)\] \[\leq 4(1+\beta_{\tau-1}/\alpha)\sum_{k=1}^{K}\min\left(1,\|\bar{x }_{\bar{m}_{k},\bar{t}_{k}}\|_{W^{-1}_{\bar{m}_{k},\bar{t}_{k}}}^{2}\right)\] \[\leq 4(1+\beta_{\tau-1}/\alpha)d\log\left(1+\frac{M\tau}{\lambda d }\right)\] \[\leq 4(1+\beta_{\tau-1}/\alpha)d\xi_{\tau},\]

where we use Lemma C.3 in the fourth inequality. Putting the bounds for \(I_{1}\) and \(I_{2}\) together, we have

\[\mathcal{R}_{\text{good}}(M,\tau)=I_{1}+I_{2} \leq 4\sqrt{2}\beta_{\tau-1}\sqrt{dM\tau\xi_{\tau}}+4\left(1+ \frac{\beta_{\tau-1}}{\alpha}\right)d\xi_{\tau}\] \[=4\sqrt{2}\left(\beta_{\tau-1}\sqrt{dM\tau\xi_{\tau}}+d\xi_{\tau} +\frac{\beta_{\tau-1}}{\alpha}d\xi_{\tau}\right).\]

Plugging \(\beta_{\tau-1}=\sqrt{\lambda}+\alpha\varepsilon M\tau+\sqrt{d\log(\frac{1+M \tau/\lambda}{\delta_{1}})}\), the regret of the good epochs is bounded as

\[\mathcal{R}_{\text{good}}(M,\tau)\leq 4\sqrt{2}\left(\sqrt{\lambda dM\tau\xi_{ \tau}}+\alpha\varepsilon\sqrt{d}(M\tau)^{1.5}\sqrt{\xi_{\tau}}+d\sqrt{M\tau} \xi_{\tau}+\frac{\sqrt{\lambda}d\xi_{\tau}}{\alpha}+\varepsilon dM\tau\xi_{ \tau}+\frac{(d\xi_{\tau})^{1.5}}{\alpha}\right).\]

Now, we focus on the epochs that are _not good_, which do not satisfy Equation (2). Let \(p\) be one such epoch, and let \(t_{0}\) be the first round and \(n\) be the length, respectively, of the epoch \(p\). Recall that at the beginning of each epoch, agents' covariance matrices are synchronized, i.e., \(V_{m,t_{0}}=\lambda I+V_{syn},\forall m\in[M]\). We can then bound the regret of the (bad) epoch \(p\) as follows.

\[\mathcal{R}_{\text{bad}}(M,p) \leq 2\beta_{\tau-1}\sum_{m=1}^{M}\sum_{t=t_{0}}^{t_{0}+n}\min \left(1,\left\|x_{m,t}\right\|_{V^{-1}_{m,t}}\right)\] \[\leq 2\beta_{\tau-1}\sum_{m=1}^{M}\sqrt{n}\sqrt{\sum_{t=t_{0}}^{t _{0}+n}\min\left(1,\left\|x_{m,t}\right\|_{V^{-1}_{m,t}}^{2}\right)}\] \[\leq 2\beta_{\tau-1}\sum_{m=1}^{M}\sqrt{n\log\frac{\det(V_{m,t_{0 }+n})}{\det(V_{m,t_{0}})}},\]

where we use Cauchy-Schwarz's inequality and Lemma C.3 in the second inequality.

Lets say the synchronization is triggered on round \(t_{0}+n+1\), i.e., for one of the agents we have that \((n+1)\log\frac{\det(V_{m,t_{0}+n+1})}{\det(V_{m,t_{0}})}>D\). Since \(n\log\frac{\det(V_{m,t_{0}+n})}{\det(V_{m,t_{0}})}<D\) for all \(m\in[M]\), for this bad epoch (i.e., from round \(t_{0}\) to round \(t_{0}+n\)), we can bound the group regret as

\[\mathcal{R}_{\text{bad}}(M,p)\leq 2\beta_{\tau-1}M\sqrt{D}.\]

As we argued earlier, since \(\det V_{p}\leq(\lambda+\frac{MT}{d})^{d}\), the bad epochs are rare, i.e., there are at most \(R=\lceil d\log(1+\frac{M\tau}{\lambda d})\rceil\) (otherwise, if \(\det(V_{p})/\det(V_{p-1})>2\) for more than \(R\) rounds, then the condition of Equation (1) is violated). Setting \(D=\tau\log(M\tau)/(dM)\) and plugging \(\beta_{\tau-1}\) into the bound above,

\[\mathcal{R}_{\text{bad}}(M,\tau) \leq 2R\beta_{\tau-1}M\sqrt{D}\] \[\leq 2\left(d\xi_{\tau}\left(\sqrt{\lambda}+\alpha\varepsilon M \tau+\sqrt{d\xi_{\tau}}\right)M\sqrt{D}\right)\] \[\leq 2\left(\sqrt{\lambda dM\tau}(\xi_{\tau})^{1.5}+\alpha \varepsilon\sqrt{d}(M\tau\xi_{\tau})^{1.5}+d\sqrt{M\tau}\xi_{\tau}^{2}\right).\]We finish the proof by putting the regret of the good and the bad epochs together,

\[\mathcal{R}(M,\tau) =\mathcal{R}_{\mathrm{good}}(M,\tau)+\mathcal{R}_{\mathrm{bad}}(M,\tau)\] \[\leq 4\sqrt{2}\Big{(}\sqrt{\lambda dM\tau}(\xi_{\tau})^{1.5}+ \alpha\varepsilon\sqrt{d}(M\tau)^{1.5}\sqrt{\xi_{\tau}}+d\sqrt{M\tau}\xi_{\tau }+\frac{\sqrt{\lambda}d\xi_{\tau}}{\alpha}+\varepsilon dM\tau\xi_{\tau}\] \[\qquad\qquad\qquad+\frac{(d\xi_{\tau})^{1.5}}{\alpha}+\alpha \varepsilon\sqrt{d}(M\tau\xi_{\tau})^{1.5}+d\sqrt{M\tau}\xi_{\tau}^{2}\Big{)}.\]

### Proof of Theorem 4.2

Proof.: Setting \(\delta_{1}=\delta_{2}=1/(M^{2}T)\), then the group regret caused by failure event, which does not satisfy Lemma 4.1, is at most \(MT\cdot(M\delta_{1}+M\delta_{2})=\mathcal{O}(1)\). Thus, we mainly consider the group regret when Lemma 4.1 holds.

From round \(\tau\) onward, agents switch to _independent learning_ mode, this is the group regret from round \(\tau\) to the end, we denote it as \(\mathcal{R}_{\mathrm{ind}}(M,T)\). Furthermore, for round \(t\geq\tau\), we have \(V_{m,t}=\lambda I+\sum_{s=\tau}^{t-1}x_{m,s}x_{m,s}^{\top}\) as the gram matrix which only contains agent's data.

Applying Proposition A.1, we have the following

\[\mathcal{R}_{\mathrm{ind}}(M,T) \leq\sum_{m=1}^{M}\sum_{t=\tau}^{T}2\beta_{t}\min\left(1,\|x_{m,t }\|_{V_{m,t}^{-1}}\right)\] \[\leq 2\sum_{m=1}^{M}\beta_{T}\sqrt{T-\tau}\sqrt{\sum_{t=\tau}^{T} \min\left(1,\|x_{m,t}\|_{V_{m,t}^{-1}}^{2}\right)}\] \[\leq 2\sum_{m=1}^{M}\left(\sqrt{\lambda}+\sqrt{d\log\left(\frac{ 1+T/(\lambda d)}{\delta_{2}}\right)}\right)\sqrt{T-\tau}\sqrt{2d\log\left(1+ \frac{T}{\lambda d}\right)}\] \[\leq 4\sqrt{2}Md\sqrt{T-\tau}\xi_{T},\]

where we use Cauchy-Schwarz's inequality in the first inequality, and Lemma 4.1 in the second inequality. Combining with the regret from the beginning up to round \(\tau\) from Theorem 4.1, we have

\[\mathcal{R}(M,T) =\mathcal{R}_{\mathrm{collab}}(M,T)+\mathcal{R}_{\mathrm{ind}}(M,T)\] (4) \[\leq 20\sqrt{2}\left(d\sqrt{M\tau}+\varepsilon dM\tau+dM\sqrt{T- \tau}\right)\xi_{T}^{2}\]

We notice that Equation (4) has the second term that is linear in term of \(M\tau\). To avoid linear regret, the choice of \(\tau\) needs to adapt to the dissimilarity level \(\varepsilon\).

For the last two terms, by Cauchy-Schwarz's inequality, we have \(\varepsilon dM\tau+dM\sqrt{T-\tau}\leq dM\sqrt{2(\varepsilon^{2}\tau^{2}+T- \tau)})\), this term can be optimized by setting \(\tau=\frac{1}{2\varepsilon^{2}}\). We need to restrict \(\tau=\min(\lfloor\frac{1}{2}\varepsilon^{-2}\rfloor,T)\) since \(\tau\) can not exceed \(T\). In other words, all agents can collaborate up to round \(\min\left(\lfloor\frac{1}{2}\varepsilon^{-2}\rfloor,T\right).\)

Next, we consider the following two cases:

Case 1: For \(\frac{1}{2}\varepsilon^{-2}\leq T\), we have that \(dM\sqrt{T-\tau}\leq dM\sqrt{T}\), and \(\varepsilon dM\tau=\frac{dM}{2\varepsilon}\leq dM\sqrt{T}\). Thus, \(\varepsilon dM\tau+dM\sqrt{T-\tau}\leq 2dM\sqrt{T}\).

Case 2: For \(\frac{1}{2}\varepsilon^{-2}>T\), we have \(\tau=T\). This implies that \(\varepsilon dM\tau+dM\sqrt{T-\tau}=\varepsilon dMT\). We also have \(\varepsilon dMT<\frac{1}{\sqrt{2}}dM\sqrt{T}\) due to \(\frac{1}{2}\varepsilon^{2}>T\).

Notice that, in both cases, \(\varepsilon dM\tau+dM\sqrt{T-\tau}\) is evaluated as small as \(\varepsilon dMT\) and always be upper bounded by \(2dM\sqrt{T}\). Therefore, in Equation (4), the summation of the last two terms of the group regret is upper bounded by \(2\min\{\varepsilon dMT,dM\sqrt{T}\}\).

By the choice of \(\delta_{1},\delta_{2},\lambda\), and assume that \(d\geq 1\), we can upper bound \(\xi_{T}\leq 4\log(MT)\). Therefore, we have the following for the expected group regret

\[\mathbb{E}\left[\mathcal{R}(M,T)\right] \leq 20\sqrt{2}\left(d\sqrt{MT}+2\min\{\varepsilon dMT,dM\sqrt{T} \}\right)\xi_{T}^{2}\] \[\leq 320\sqrt{2}\left(d\sqrt{MT}+2\min\{\varepsilon dMT,dM\sqrt{T }\}\right)\log^{2}(MT)\]

## Appendix B Proof of Lower Bound

### Proof of Theorem 4.3

Before proving the theorem, we give a formal definition of \(\varepsilon\)-MALCB problem for \(p\)-norm,

**Definition B.1**.: _Given an instance of multi-agent linear bandits, it belongs to the class of \(p\)-norm \(\varepsilon\)-MALCB, which we denote as \(\mathcal{I}_{p}(\varepsilon)\), if for all \(i,j\in[M]\), \(\|\theta_{i}-\theta_{j}\|_{p}\leq\varepsilon\)._

To prove Theorem 4.3, we use the results of the two following lemmas on _max-norm \(\varepsilon\)_-MALCB instances.

**Lemma B.1**.: _Let \(\mathcal{I}_{\infty}(\varepsilon)\) be the class of max-norm \(\varepsilon\)-MALCB instances that satisfy the Assumption 3.1. For \(\varepsilon\geq 0\), we have the following,_

\[\inf_{\mathcal{A}}\sup_{\mathcal{I}\in\mathcal{I}_{\infty}(\varepsilon)} \mathcal{R}_{\mathcal{A},\mathcal{I}}(M,T)=\Omega(d\sqrt{MT}).\]

**Lemma B.2**.: _Let \(\mathcal{I}_{\infty}(\varepsilon)\) be the class of max-norm \(\varepsilon\)-MALCB instances that satisfy the Assumption 3.1. Assume \(d\leq 2T,\frac{d^{2}}{48}\leq T\). For \(\varepsilon\geq 0\), we have the following,_

\[\inf_{\mathcal{A}}\sup_{\mathcal{I}\in\mathcal{I}_{\infty}(\varepsilon)} \mathcal{R}_{\mathcal{A},\mathcal{I}}(M,T)=\Omega\left(\min\left\{\varepsilon MT \sqrt{d},dM\sqrt{T}\right\}\right).\]

Proof of Theorem 4.3.: Combining the results of Lemma B.1 and Lemma B.2, we have the following results for the class of max-norm \(\varepsilon\)-MALCB, for \(\varepsilon\geq 0\),

\[\inf_{\mathcal{A}}\sup_{\mathcal{I}\in\mathcal{I}_{\infty}(\varepsilon)} \mathcal{R}_{\mathcal{A},\mathcal{I}}(M,T)=\Omega\left(d\sqrt{MT}+\min\left\{ \varepsilon MT\sqrt{d},dM\sqrt{T}\right\}\right).\]

In addition, \(\|x\|_{\infty}\leq\varepsilon\) implies \(\|x\|_{2}\leq\varepsilon\sqrt{d}\), therefore, \(\mathcal{I}_{\infty}(\varepsilon)\subseteq\mathcal{I}_{2}(\varepsilon\sqrt{d})\). In other words, for \(\varepsilon\geq 0\),

\[\inf_{\mathcal{A}}\sup_{\mathcal{I}\in\mathcal{I}_{2}(\varepsilon)}\mathcal{R }_{\mathcal{A},\mathcal{I}}(M,T)\geq\inf_{\mathcal{A}}\sup_{\mathcal{I}\in \mathcal{I}_{\infty}\left(\frac{\varepsilon}{\sqrt{d}}\right)}\mathcal{R}_{ \mathcal{A},\mathcal{I}}(M,T)=\Omega\left(d\sqrt{MT}+\min\left\{\varepsilon MT,dM\sqrt{T}\right\}\right).\]

Proof of Lemma b.1.: In this lemma, we prove that solving any \(\varepsilon\)-MALCB instance for \(T\) rounds is at least as hard as solving a (single-agent) linear bandits for \(MT\) rounds. We prove the lemma by contradiction, which is based on linear bandits lower bound of Lemma C.1. We have that \(\mathcal{I}_{\infty}(\varepsilon^{\prime})\subseteq\mathcal{I}_{\infty}( \varepsilon)\), for \(0\leq\varepsilon^{\prime}\leq\varepsilon\). This implies, for \(\varepsilon\geq 0\),

\[\inf_{\mathcal{A}}\sup_{\mathcal{I}\in\mathcal{I}_{\infty}(\varepsilon)} \mathcal{R}_{\mathcal{A},\mathcal{I}}(M,T)\geq\inf_{\mathcal{A}}\sup_{\mathcal{ I}\in\mathcal{I}_{\infty}(0)}\mathcal{R}_{\mathcal{A},\mathcal{I}}(M,T).\]

We completes the proof by proving \(\inf_{\mathcal{A}}\sup_{\mathcal{I}\in\mathcal{I}_{\infty}(0)}\mathcal{R}_{ \mathcal{A},\mathcal{I}}(M,T)=\Omega\left(d\sqrt{MT}\right)\).

Now, we assume there exists an algorithm \(\mathcal{A}\) which achieves \(\sup_{\mathcal{I}\in\mathcal{I}_{\infty}(0)}\mathcal{R}_{\mathcal{A}, \mathcal{I}}(M,T)<\frac{d\sqrt{MT}}{16\sqrt{3}}\).

We observe that \(\mathcal{I}_{\infty}(0)\) is the class of multi-agent solving exactly the same linear bandits problem. We simulate the algorithm \(\mathcal{B}\) on a single agent linear bandits for \(MT\) rounds by the protocol of \(M\) agents solving an identical linear bandits for \(T\) rounds. Therefore, if we have \(\mathcal{R}_{\mathcal{A},\mathcal{I}}(M,T)<\frac{d\sqrt{MT}}{16\sqrt{3}}\) then we also achieve \(\mathcal{R}_{\mathcal{B},\mathcal{I}}(MT)<\frac{d\sqrt{MT}}{16\sqrt{3}}\), which contradicts Lemma C.1. Thus, we have \(\sup_{\mathcal{I}\in\mathcal{I}_{\infty}(0)}\mathcal{R}_{\mathcal{A},\mathcal{ I}}(M,T)\geq\frac{d\sqrt{MT}}{16\sqrt{3}}\), which completes the proof.

Proof of Lemma b.2.: We extend Lemma C.1 to multi-agent linear contextual bandit by constructing the following _max-norm_\(\varepsilon\)-MALCB instance.

Max-norm \(\varepsilon\)-MALCB instance.The set of linear parameter of \(M\) agents \(\{\theta_{m}\}_{m=1}^{M}\) belong to a \(d\)-dimensional hypercube \(\theta_{m}\in\{\pm\varepsilon\}^{d}\), where \(\varepsilon\in[0,\frac{1}{\sqrt{d}}]\). Let \(\mathcal{D}=\left\{x\in\mathbb{R}^{d}:\|x\|_{2}\leq 1\right\}\) be the action set given to agents at every round. The reward when agent \(m\) picks action \(x\) is defined as \(r_{m,x}=\theta_{m}^{\top}x+\eta_{m,x}\), where the noise samples from a standard normal distribution, \(\eta_{m,x}\sim\mathcal{N}(0,1)\).

We first verify if the instance satisfies the Assumption 3.1. It satisfies the context assumption since we also restrict the context vector to lie in the unit ball. It also satisfies the \(\|\theta_{m}\|\leq 1\) assumption because we restrict \(\varepsilon\in[0,\frac{1}{\sqrt{d}}]\). Finally, \(\eta_{m,x}\sim\mathcal{N}(0,1)\) is 1-subGaussian distribution. This instance belongs to \(\mathcal{I}_{\infty}(2\varepsilon)\) since \(\|\theta_{i}-\theta_{j}\|_{\infty}\leq 2\varepsilon\) for all \(i,j\in[M]\).

Now, we proceed to prove Lemma B.2. Let \(\{\theta_{m}\}_{m=1}^{M}\) be a set of parameters of the max-norm \(\varepsilon\)-MALCB instance. For brevity, we omit the commans in the subscripts when it is clear from the context, e.g., \(\theta_{mi}=\theta_{m,i}\) or \(x_{tmi}=x_{t,m,i},\forall t\in[T],m\in[M],i\in[d]\). Given \(m\in[M],i\in[d]\), we define the stopping time \(\tau_{mi}=T\wedge\min\{t:\sum_{s=1}^{t}x_{smi}^{2}\geq t/d\}\), and the function \(U_{mi}(z)=\sum_{t=1}^{\tau_{mi}}(\frac{1}{\sqrt{d}}-x_{tmi}\cdot z)^{2}\). We then have following result for \(U_{mi}(1)\):

\[U_{mi}(1)=\sum_{t=1}^{\tau_{mi}}\left(\frac{1}{\sqrt{d}}-x_{tmi} \right)^{2}\leq 2\sum_{t=1}^{\tau_{mi}}\frac{1}{d}+2\sum_{t=1}^{\tau_{mi}}x_{ tmi}^{2}\leq\frac{4T}{d}+2.\] (5)

Let \(x_{m}^{\star}\) be the optimal action of agent \(m\). We then have the following for the group regret

\[\mathcal{R}_{\mathcal{A}}(M,T) =\mathbb{E}_{\{\theta_{m}\}_{m=1}^{M}}\left[\sum_{t=1}^{T}\sum_{m =1}^{M}\sum_{i=1}^{d}\left(x_{mi}^{\star}\theta_{mi}-x_{tmi}\theta_{mi}\right)\right]\] \[=\varepsilon\cdot\mathbb{E}_{\{\theta_{m}\}_{m=1}^{M}}\left[\sum_ {t=1}^{T}\sum_{m=1}^{M}\sum_{i=1}^{d}\left(\frac{1}{\sqrt{d}}-x_{tmi}\operatorname {sign}(\theta_{mi})\right)\right]\] \[\geq\frac{\varepsilon\sqrt{d}}{2}\cdot\mathbb{E}_{\{\theta_{m}\} _{m=1}^{M}}\left[\sum_{t=1}^{T}\sum_{m=1}^{M}\sum_{i=1}^{d}\left(\frac{1}{ \sqrt{d}}-x_{tmi}\operatorname{sign}(\theta_{mi})\right)^{2}\right]\] \[\geq\frac{\varepsilon\sqrt{d}}{2}\cdot\sum_{m=1}^{M}\sum_{i=1}^{ d}\mathbb{E}_{\{\theta_{m}\}_{m=1}^{M}}\left[\sum_{t=1}^{\tau_{mi}}\left(\frac{1}{ \sqrt{d}}-x_{tmi}\operatorname{sign}(\theta_{mi})\right)^{2}\right],\]

where we use the fact that optimal action \(x_{m}^{\star}=[\frac{\theta_{mi}}{\|\theta_{m}\|},\cdots,\frac{\theta_{md}}{ \|\theta_{md}\|}]^{\top}\), which is a unit vector and has the same direction with \(\theta_{m}\), the first inequality holds due to \(\|x_{tmi}\|^{2}\leq 1\).

Now, let \(\{\theta_{m}^{\prime}\}_{m=1}^{M}\) be another set of linear parameters that are different at only _one coordinate_ of the linear parameter of only _one agent_ compared to \(\{\theta_{m}\}_{m=1}^{M}\). Specifically, fix \(g\in[M],i\in[d]\), we have \(\theta_{g,i}=-\theta_{g,i}^{\prime}\); otherwise, \(\theta_{k,j}=\theta_{k,j}^{\prime}\), for \((k,j)\neq(g,i),\forall k\in[M],j\in[d]\). To simplify the notion, we define \(\Phi^{\prime}\) as \(Md\)-dimensional vectors, where \(\Phi,\Phi^{\prime}\in\{\pm\varepsilon\}^{Md}\). That is, \(\Phi,\Phi^{\prime}\) represent \(\{\theta_{m}\}_{m=1}^{M},\{\theta_{m}^{\prime}\}_{m=1}^{M}\), respectively. We let \(\mathbb{P}^{\prime}\) and \(\mathbb{P}^{\prime}\) be the law of \(U_{gi}(z)\) w.r.t. the interaction of the multi-agent linear bandits induced by \(\Phi,\Phi^{\prime}\), respectively. We then get

\[\mathbb{E}_{\Phi}\left[U_{gi}(1)\right] \geq\mathbb{E}_{\Phi^{\prime}}\left[U_{gi}(1)\right]-\left(\frac{ 4T}{d}+2\right)\sqrt{\frac{1}{2}D_{KL}\left(\mathbb{P}\|\mathbb{P}^{\prime}\right)}\] \[\geq\mathbb{E}_{\Phi^{\prime}}\left[U_{gi}(1)\right]-\frac{ \varepsilon}{2}\left(\frac{4T}{d}+2\right)\sqrt{\mathbb{E}\left[\sum_{t=1}^{ \tau_{gi}}x_{tgi}^{2}\right]}\] \[\geq\mathbb{E}_{\Phi^{\prime}}\left[U_{gi}(1)\right]-\frac{ \varepsilon}{2}\left(\frac{4T}{d}+2\right)\sqrt{\frac{T}{d}+1}\] \[\geq\mathbb{E}_{\Phi^{\prime}}\left[U_{gi}(1)\right]-4\sqrt{3} \varepsilon\left(\frac{T}{d}\right)^{3/2},\]where the first inequality holds due to Lemma C.5 and Equation (5), the second inequality follows the stopping time arguments from Lattimore and Csaba (2020), the last inequality holds due to the assumption that \(d\leq 2T\). Then,

\[\mathbb{E}_{\Phi}\left[U_{gi}(1)\right]+\mathbb{E}_{\Phi^{\prime}} \left[U_{gi}(-1)\right] \geq\mathbb{E}_{\Phi^{\prime}}\left[U_{gi}(1)+U_{gi}(-1)\right]-4 \sqrt{3}\varepsilon\left(\frac{T}{d}\right)^{3/2}\] \[=2\mathbb{E}_{\Phi^{\prime}}\left[\frac{\tau_{gi}}{d}+\sum_{t=1} ^{\tau_{gi}}x_{tgi}^{2}\right]-4\sqrt{3}\varepsilon\left(\frac{T}{d}\right)^{ 3/2}\] \[\geq\frac{2T}{d}-4\sqrt{3}\varepsilon\left(\frac{T}{d}\right)^{3/ 2}\geq\frac{T}{d},\]

where the last inequality holds for \(\varepsilon\in[0,\frac{1}{4\sqrt{3}}\sqrt{\frac{d}{T}}]\), this satisfies the requirement \(\varepsilon\in[0,\frac{1}{\sqrt{d}}]\) of the instance construction due to the assumption that \(\frac{d^{2}}{48}\leq T\). Furthermore, we denote \(\Phi_{-mi}\) as a \((Md-1)\)-dimensional vector, which is obtained by excluding \(\Phi_{mi}\) from vector \(\Phi\). Let \(\mathcal{R}_{\Phi}(M,T)\) be the regret w.r.t. \(\Phi\), and applying _randomization hammer_, we have the following,

\[\sum_{\Phi\in\{\pm\varepsilon\}^{Md}}\mathcal{R}_{\Phi}(M,T) \geq\frac{\varepsilon\sqrt{d}}{2}\sum_{m=1}^{M}\sum_{i=1}^{d} \sum_{\Phi\in\{\pm\varepsilon\}^{Md}}\mathbb{E}_{\Phi}\left[U_{mi}\left( \operatorname{sign}\left(\varepsilon_{mi}\right)\right)\right]\] \[=\frac{\varepsilon\sqrt{d}}{2}\sum_{m=1}^{M}\sum_{i=1}^{d}\sum_{ \Phi_{-mi}\in\{\pm\varepsilon\}^{Md-1}}\sum_{\Phi_{mi}\in\{\pm\varepsilon\} }\mathbb{E}_{\Phi}\left[U_{mi}\left(\operatorname{sign}\left(\varepsilon_{mi }\right)\right)\right]\] \[\geq\frac{\varepsilon\sqrt{d}}{2}\sum_{m=1}^{M}\sum_{i=1}^{d} \sum_{\Phi_{-mi}\in\{\pm\varepsilon\}^{Md-1}}\frac{T}{d}=2^{Md-2}\varepsilon MT \sqrt{d}.\]

Therefore, we conclude that there exists an instance with the parameter set of \(\left\{\theta_{m}\right\}_{m=1}^{M}\) such that \(\mathcal{R}(M,T)\geq\frac{\varepsilon MT\sqrt{d}}{4}\), for \(\varepsilon\in[0,\frac{1}{4\sqrt{3}}\sqrt{\frac{d}{T}}]\). Notice that this proof holds for \(\mathcal{I}_{\infty}(2\varepsilon)\) class. Scaling down by 2, we have the following for \(\mathcal{I}_{\infty}(\varepsilon)\) class,

\[\mathcal{R}_{\mathcal{I}_{\infty}(\varepsilon)}(M,T)=\mathcal{R}_{\mathcal{I}_ {\infty}(2\cdot\frac{\varepsilon}{2})}(M,T)\geq\frac{\varepsilon MT\sqrt{d}}{ 8},\text{for }\varepsilon\in\left[0,\frac{1}{2\sqrt{3}}\sqrt{\frac{d}{T}} \right].\]

Observe that \(\frac{\varepsilon MT\sqrt{d}}{8}\) is a strictly increasing function w.r.t. \(\varepsilon\). It is at most \(\frac{dM\sqrt{T}}{16\sqrt{3}}\), when \(\varepsilon=\frac{1}{2\sqrt{3}}\sqrt{\frac{d}{T}}\). In other words, \(\mathcal{R}_{\mathcal{I}_{\infty}(\varepsilon)}(M,T)\geq\min(\frac{ \varepsilon MT\sqrt{d}}{8},\frac{dM\sqrt{T}}{16\sqrt{3}})\) for \(\varepsilon\in[0,\frac{1}{2\sqrt{3}}\sqrt{\frac{d}{T}}]\).

Recall that \(\inf_{\mathcal{A}}\sup_{\mathcal{I}\in\mathcal{I}_{\infty}(\varepsilon)} \mathcal{R}(M,T)\geq\inf_{\mathcal{A}}\sup_{\mathcal{I}\in\mathcal{I}_{\infty} (\varepsilon^{\prime})}\mathcal{R}(M,T)\), for any \(\varepsilon\geq\varepsilon^{\prime}\geq 0\). Thus, we conclude, \(\forall\varepsilon\geq 0\), the following holds,

\[\inf_{\mathcal{A}}\sup_{\mathcal{I}\in\mathcal{I}_{\infty}(\varepsilon)} \mathcal{R}_{\mathcal{A},\mathcal{I}}(M,T)\geq\min(\frac{\varepsilon MT\sqrt{d }}{8},\frac{dM\sqrt{T}}{16\sqrt{3}}),\]

which completes the proof. 

## Appendix C Supporting Lemmas

**Lemma C.1**.: _([Lattimore and Csaba, 2020, Theorem 24.2]). Assume \(d\leq 2n\) and let \(\mathcal{D}=\left\{x\in\mathbb{R}^{d}:\|x\|_{2}\leq 1\right\}\). Then there exists a parameter vector \(\theta\in\mathbb{R}^{d}\) with \(\|\theta\|_{2}^{2}=d^{2}/(48n)\) such that \(R_{n}(\mathcal{D},\theta)\geq d\sqrt{n}/(16\sqrt{3})\)._

**Lemma C.2**.: _([Abbasi-Yadkori et al., 2011, Lemma 12]). Let \(A,B\) and \(C\) be positive semi-definite matrices such that \(A=B+C\). Then, we have that_

\[\sup_{x\neq 0}\frac{x^{\top}Ax}{x^{\top}Bx}\leq\frac{\det(A)}{\det(B)}.\]

**Lemma C.3**.: _([Abbasi-Yadkori et al., 2011, Lemma 11]). Let \(\{X_{t}\}_{t=1}^{\infty}\) be a sequence in \(\mathbb{R}^{d},V\) a \(d\times d\) positive definite matrix and define \(\bar{V}_{t}=V+\sum_{s=1}^{t}X_{s}X_{s}^{\top}\). Then, we have that_

\[\log\left(\frac{\det\left(\bar{V}_{n}\right)}{\det(V)}\right)\leq\sum_{t=1}^{n }\|X_{t}\|_{\bar{V}_{t-1}^{-1}}^{2}\,.\]

_Further, if \(\left\|X_{t}\right\|_{2}\leq L\) for all \(t\), then_

\[\sum_{t=1}^{n}\min\left\{1,\left\|X_{t}\right\|_{\bar{V}_{t-1}^{-1}}^{2} \right\}\leq 2\left(\log\det\left(\bar{V}_{n}\right)-\log\det V\right)\leq 2 \left(d\log\left(\left(\operatorname{trace}(V)+nL^{2}\right)/d\right)-\log \det V\right)\]

**Lemma C.4**.: _(Self-Normalized Bound for Vector-Valued Martingales, [Abbasi-Yadkori et al., 2011, Theorem 11]). Let \(\{F_{t}\}_{t=0}^{\infty}\) be a filtration. Let \(\{\eta_{t}\}_{t=1}^{\infty}\) be a real-valued stochastic process such that \(\eta_{t}\) is \(F_{t}\)-measurable and \(\eta_{t}\) is conditionally \(R\)-sub-Gaussian for some \(R\geq 0\) i.e._

\[\forall\lambda\in\mathbb{R}\quad\mathbf{E}\left[e^{\lambda\eta_{t}}\mid F_{t- 1}\right]\leq\exp\left(\frac{\lambda^{2}R^{2}}{2}\right).\]

_Let \(\{X_{t}\}_{t=1}^{\infty}\) be an \(\mathbb{R}^{d}\)-valued stochastic process such that \(X_{t}\) is \(F_{t-1}\)-measurable. Assume that \(V\) is a \(d\times d\) positive definite matrix. For any \(t\geq 0\), define \(\bar{V}_{t}=V+\sum_{s=1}^{t}X_{s}X_{s}^{\top}\). Then, for any \(\delta>0\), with probability at least \(1-\delta\), for all \(t\geq 0\),_

\[\left\|\sum_{s=1}^{t}\eta_{s}X_{s}\right\|_{\bar{V}_{t}^{-1}}^{2}\leq 2R^{2} \log\left(\frac{\det\left(\bar{V}_{t}\right)^{1/2}\det(V)^{-1/2}}{\delta} \right).\]

**Lemma C.5**.: _(Pinsker's inequality [Lattimore and Csaba, 2020, Equation 14.12]) For measures \(P\) and \(Q\) on the same probability space \((\Omega,\mathcal{F})\), we have the following,_

\[\sup_{A\in\mathcal{F}}P(A)-Q(A)\leq\sqrt{\frac{1}{2}D_{KL}(P||Q)}.\]