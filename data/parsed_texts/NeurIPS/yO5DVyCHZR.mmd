# A Simple and Optimal Approach for

Universal Online Learning with Gradient Variations

 Yu-Hu Yan, Peng Zhao, Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{yanyh, zhaop, zhouzh}@lamda.nju.edu.cn

Correspondence: Peng Zhao <zhaop@lamda.nju.edu.cn

###### Abstract

We investigate the problem of universal online learning with gradient-variation regret. Universal online learning aims to achieve regret guarantees without the prior knowledge of the curvature of the online functions. Moreover, we study the problem-dependent gradient-variation regret as it plays a crucial role in bridging stochastic and adversarial optimization as well as game theory. In this work, we design a universal approach with the _optimal_ gradient-variation regret simultaneously for strongly convex, exp-concave, and convex functions, thus addressing an open problem highlighted by Yan et al. (2023). Our approach is _simple_ since it is algorithmically efficient-to-implement with a two-layer online ensemble structure and only \(1\) gradient query per round, and theoretically easy-to-analyze with a novel and alternative analysis to the gradient-variation regret. Concretely, previous works on gradient variations require controlling the algorithmic stability, which is challenging and leads to sub-optimal regret and less efficient algorithm design. Our analysis overcomes this issue by using a Bregman divergence negative term from linearization and a useful smoothness property.

## 1 Introduction

Online convex optimization (OCO) models a sequential \(T\)-round game between an online learner and the environments (Hazan, 2016; Orabona, 2019). In each round \(t\in[T]\), the learner selects a decision \(\mathbf{x}_{t}\) from a convex compact set \(\mathcal{X}\subseteq\mathbb{R}^{d}\). Simultaneously, the environments adversarially choose a convex loss function \(f_{t}:\mathcal{X}\mapsto\mathbb{R}\). Subsequently, the learner suffers a loss of \(f_{t}(\mathbf{x}_{t})\), receives feedback on the function \(f_{t}(\cdot)\), and updates her decision to \(\mathbf{x}_{t+1}\). In OCO, the learner aims to optimize the game-theoretical performance measure known as regret (Cesa-Bianchi and Lugosi, 2006), which is formally defined as

\[\textsc{Reg}_{T}\triangleq\sum_{t=1}^{T}f_{t}(\mathbf{x}_{t})-\min_{\mathbf{x }\in\mathcal{X}}\sum_{t=1}^{T}f_{t}(\mathbf{x}).\] (1.1)

It represents the learner's excess cumulative loss compared with the best fixed comparator in hindsight.

In OCO, _function curvatures_ play an important role in the best attainable regret bounds. Traditional studies examine three types of curvatures: convexity, exp-concavity, and strong convexity. Specifically, for convex functions, online gradient descent (OGD) achieves \(\mathcal{O}(\sqrt{T})\) regret (Zinkevich, 2003); for \(\alpha\)-exp-concave functions, online Newton step assuming \(\alpha\) is known obtains \(\mathcal{O}(\frac{d}{\alpha}\log T)\) regret (Hazan et al., 2007); and for \(\lambda\)-strongly convex functions, OGD with known \(\lambda\) attains \(\mathcal{O}(\frac{1}{\lambda}\log T)\)(Hazan et al., 2007). These results are shown to be minimax optimal (Ordentich and Cover, 1998; Abernethy et al., 2008). Recent studies further strengthen them by introducing _two levels of adaptivity_.

High-level Adaptivity to Unknown Curvatures.Traditional studies require function curvature information in advance to select suitable algorithms for provable bounds. However, the information could be hard to access in real-world applications. To this end, a line of research aims to design a single _universal_ algorithm that does not require the curvature information while achieving the same regret guarantees as if knowing it, achieving the adaptivity to unknown curvatures. The pioneering work of MetaGrad (van Erven and Koolen, 2016) proposed carefully designed surrogate functions and achieved \(\mathcal{O}(\frac{d}{\alpha}\log T)\) for \(\alpha\)-exp-concave functions and \(\mathcal{O}(\sqrt{T})\) for convex functions. Subsequently, Wang et al. (2019) obtained the optimal \(\mathcal{O}(\frac{1}{\lambda}\log T)\) regret for \(\lambda\)-strongly convex functions, while maintaining the optimal rates in the other cases. Another remarkable progress of Zhang et al. (2022) proposed a flexible framework with simplified analyses and further enhanced the minimax results using smoothness. We provide a detailed introduction of this work in Section 2.2.

Low-level Adaptivity to Gradient Variation.Although the regret guarantees based on the time horizon \(T\) are optimal in the minimax sense, in this work we are interested in achieving the _gradient-variation_ regret (Chiang et al., 2012; Yang et al., 2014), which replaces the dependence of the time horizon \(T\) by the gradient variation quantity defined in the following:

\[V_{T}\triangleq\sum_{t=2}^{T}\sup_{\mathbf{x}\in\mathcal{X}}\|\nabla f_{t}( \mathbf{x})-\nabla f_{t-1}(\mathbf{x})\|^{2}.\] (1.2)

Under smoothness assumptions, the minimax regret can be improved to \(\mathcal{O}(\frac{1}{\lambda}\log V_{T})\), \(\mathcal{O}(\frac{d}{\alpha}\log V_{T})\), and \(\mathcal{O}(\sqrt{V_{T}})\) for \(\lambda\)-strongly convex, \(\alpha\)-exp-concave, and convex functions, respectively. In this work, continuing previous gradient-variation online learning results (Zhao et al., 2020, 2024; Zhang et al., 2022; Chen et al., 2023), we focus on the gradient-variation regret for the following reasons: _(i)_ gradient-variation bounds safeguard the minimax guarantees. Besides, as demonstrated by Zhao et al. (2024), the gradient-variation regret is more fundamental than another well-known problem-dependent quantity known as the small loss \(F_{T}\triangleq\min_{\mathbf{x}\in\mathcal{X}}\sum_{t\leq T}f_{t}(\mathbf{x})\)(Srebro et al., 2010; Orabona et al., 2012) since gradient-variation regret can imply small-loss bounds directly in analysis; _(ii)_ the gradient variation plays a crucial role in bridging adversarial and stochastic optimization (Sachs et al., 2022); and _(iii)_ the gradient-variation regret can be used to achieve fast rates in multi-player games (Syrgkanis et al., 2015; Zhang et al., 2022). More detailed explanations of the importance of achieving such adaptivity are provided at the end of this section.

Motivated by the aforementioned two levels of adaptivity, we focus on the problem of achieving _universal gradient-variation_ regret, i.e., designing a single universal approach with gradient-variation regret across different curvature types without the prior knowledge of them. For this problem, Zhang et al. (2022) achieved partial results of \(\mathcal{O}(\frac{1}{\lambda}\log V_{T})\), \(\mathcal{O}(\frac{d}{\alpha}\log V_{T})\), \(\mathcal{O}(\sqrt{T})\) for \(\lambda\)-strongly convex, \(\alpha\)-exp-concave, and convex functions, respectively. Subsequently, Yan et al. (2023) proposed a carefully designed three-layer online ensemble approach to stabilize the algorithm and improved the convex result to \(\mathcal{O}(\sqrt{V_{T}\log V_{T}})\), achieving the first universal gradient-variation guarantee. Although optimal for strongly convex and exp-concave functions, their results still exhibit a gap with the optimal \(\mathcal{O}(\sqrt{V_{T}})\) regret in the convex case. Here "optimal" refers to matching the best known results with curvature information since problem-dependent lower bound cannot be easily obtained. The only lower bound we are aware of is \(\Omega(\sqrt{V_{T}})\) for convex functions (Yang et al., 2014, Remark 5).

\begin{table}
\begin{tabular}{c|c c c|c|c} \hline \hline \multirow{2}{*}{**Works**} & \multicolumn{3}{c|}{**Regret Bounds**} & \multicolumn{2}{c}{**Efficiency**} \\ \cline{2-6}  & Strongly Convex & Exp-concave & Convex & \# Gradient & \# Base \\ \hline van Erven and Koolen (2016) & \(d\log T\) & \(d\log T\) & \(\sqrt{T}\) & \(1\) & \(\log T\) \\ \hline Wang et al. (2019) & \(\log T\) & \(d\log T\) & \(\sqrt{T}\) & \(1\) & \(\log T\) \\ \hline Zhang et al. (2022) & \(\log V_{T}\) & \(d\log V_{T}\) & \(\sqrt{T}\) & \(\log T\) & \(\log T\) \\ \hline Yan et al. (2023) & \(\log V_{T}\) & \(d\log V_{T}\) & \(\sqrt{V_{T}\log V_{T}}\) & \(1\) & \((\log T)^{2}\) \\ \hline \hline
**Ours** & \(\log V_{T}\) & \(d\log V_{T}\) & \(\sqrt{V_{T}}\) & \(1\) & \(\log T\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with existing results. The second column shows the regret bounds for strongly convex, exp-concave, and convex functions, following the \(\mathcal{O}(\cdot)\)-notation. Note that we only list universal guarantees related to the gradient variation \(V_{T}\) or the time horizon \(T\). Each gradient-variation bound can directly apply a corresponding small-loss regret in analysis, which is formally stated in Theorem 2 and omitted here for clarity. We treat the \(\log\log T\) factor as a constant and omit it. “# Gradient” is the number of gradient queries in each round, where “1” represents exactly one gradient query. And “# Base” stands for the number of base learners.

To handle the uncertainty, _online ensemble_ is commonly employed and proven effective in enhancing the robustness (Zhou, 2012; Zhao, 2021), such as adaptive regret minimization (Hazan and Seshadhri, 2007; Daniely et al., 2015; Zhang et al., 2019), dynamic regret minimization (Zhang et al., 2018; Zhao et al., 2020, 2024), and universal online learning (van Erven and Koolen, 2016; Zhang et al., 2022; Yan et al., 2023). Concretely, an online ensemble algorithm contains multiple _base learners_ for exploring the environments and a _meta learner_ for ensemble. In universal online learning, base learners make guesses on the curvature information, and the meta learner tracks the best base learner (i.e., with the most accurate guess) on the fly.

Due to the deployment of the online ensemble framework, _computational efficiency_ has become a point of concern, with two essential factors. The first is the _number of base learners_, because each independently runs an online learning algorithm that involves time-consuming gradient computations and projections. The second factor is the _number of gradient queries_, especially when the gradient evaluation is costly, e.g., in nuclear norm optimization (Ji and Ye, 2009) and mini-batch optimization (Li et al., 2014). In universal online learning, an "efficient" algorithm is expected to adopt only \(\mathcal{O}(\log T)\) base learners, which is inherent to the online ensemble design, and only 1 gradient query per round, matching the gradient query complexity of standard OGD. In terms of this metric, Zhang et al. (2022) employed \(\mathcal{O}(\log T)\) base learners, but required \(\mathcal{O}(\log T)\) gradient queries per round. Yan et al. (2023) used only \(1\) gradient query per round but required \(\mathcal{O}((\log T)^{2})\) base learners (caused by their three-layer algorithm design), resulting in reduced efficiency.

Results.Motivated by the above considerations of optimality and efficiency, in this work, we propose a simple universal approach that achieves the _optimal_\(\mathcal{O}(\frac{1}{\lambda}\log V_{T})\), \(\mathcal{O}(\frac{d}{\alpha}\log V_{T})\), and \(\mathcal{O}(\sqrt{V_{T}})\) regret simultaneously for \(\lambda\)-strongly convex, \(\alpha\)-exp-concave, and convex functions, and is _efficient_ with one gradient query per round and \(\mathcal{O}(\log T)\) base learners, resolving a major open problem highlighted by Yan et al. (2023). We summarize our theoretical results in Theorem 1, and compare our results with existing ones in Table 1. Furthermore, we validate the effectiveness of our approach by: _(i)_ showing that our universal gradient-variation regret directly implies the optimal universal small-loss regret in analysis without any algorithm modifications; and _(ii)_ applying them to the stochastically extended adversarial (SEA) model (Sachs et al., 2022), an intermediate framework between stochastic and adversarial optimization. We achieve the same state-of-the-art guarantees as Chen et al. (2024), but without curvature information. The details are provided in Section 4.

Techniques.Our technical contributions include two simple and novel analyses. First, the key to gradient-variation regret is to analyze its empirical version, formally, \(\|\nabla f_{t}(\mathbf{x}_{t})-\nabla f_{t-1}(\mathbf{x}_{t-1})\|_{2}^{2}\). The previous approach to addressing this term involves controlling the algorithmic stability \(\|\mathbf{x}_{t}-\mathbf{x}_{t-1}\|_{2}^{2}\), which is highly challenging in universal online learning, leading to sub-optimal results and less efficient algorithm design (Yan et al., 2023). In this work, we overcome this issue via a novel analysis by a useful smoothness property and a Bregman divergence negative term from linearization, where the latter is inspired by the recent advance in stochastic optimization (Joulani et al., 2020). Second, we adopt the surrogate functions proposed by Yan et al. (2023) to reduce the gradient query complexity, and provide a novel analysis for the empirical gradient variation based on the surrogates. A technical comparison with previous works is provided in Section 4.

Organization.The rest of the paper is structured as follows. Section 2 introduces preliminaries and a general framework for universal online learning. Section 3 presents our efficient approach with the optimal universal gradient-variation regret. Section 4 presents the implication and application of our results. Finally Section 5 concludes the paper. All the proofs can be found in the appendices.

## 2 Preliminaries

In this section, we introduce some preliminary knowledge, including notations, assumptions, definitions, and a general framework of universal online learning.

### Notations, Assumptions, and Definitions

For simplicity, we use \(\|\cdot\|\) for \(\|\cdot\|_{2}\) by default and use \(a\lesssim b\) or \(a=\mathcal{O}(b)\) if there exists a constant \(C<\infty\) such that \(a/b\leq C\). In the following, we introduce the assumptions used in this work.

**Assumption 1** (Boundedness).: For any \(\mathbf{x},\mathbf{y}\in\mathcal{X}\subseteq\mathbb{R}^{d}\), the domain diameter satisfies \(\|\mathbf{x}-\mathbf{y}\|\leq D\), and for \(t\in[T]\), the gradient norm of the online functions is bounded as \(\|\nabla f_{t}(\mathbf{x})\|\leq G\).

**Assumption 2** (Smoothness).: For each \(t\in[T]\), the online function \(f_{t}(\cdot)\) is \(L\)-smooth, i.e., \(\|\nabla f_{t}(\mathbf{x})-\nabla f_{t}(\mathbf{y})\|\leq L\|\mathbf{x}-\mathbf{ y}\|\) holds for any \(\mathbf{x},\mathbf{y}\in\mathbb{R}^{d}\).

Both assumptions are common in the literature. Specifically, the boundedness assumption is widely used in OCO (Hazan, 2016). And the smoothness assumption is essential for first-order algorithms in achieving the gradient-variation regret (Chiang et al., 2012). Note that here Assumption 2 requires smoothness on the whole \(\mathbb{R}^{d}\) space and can be relaxed to a slightly larger domain than \(\mathcal{X}\), formally, \(\mathcal{X}_{+}\triangleq\{\mathbf{x}+\mathbf{b}\,|\,\mathbf{x}\in\mathcal{X},\mathbf{b}\in\nicefrac{{G}}{{L}}\cdot\mathbb{B}\}\), where \(\mathbb{B}\triangleq\{\mathbf{x}\,|\,\|\mathbf{x}\|\leq 1\}\) is a unit ball. We defer the details of this relaxed smoothness requirement and its derivation to Appendix A. In the following, we provide formal definitions of strong convexity and exp-concavity.

**Definition 1**.: _For any \(\mathbf{x},\mathbf{y}\in\mathcal{X}\), a function \(f(\cdot)\) is \(\lambda\)-strongly convex if \(f(\mathbf{x})-f(\mathbf{y})\leq\langle\nabla f(\mathbf{x}),\mathbf{x}- \mathbf{y}\rangle-\frac{\lambda}{2}\cdot\|\mathbf{x}-\mathbf{y}\|^{2}\); \(f(\cdot)\) is \(\alpha\)-exp-concave if \(f(\mathbf{x})-f(\mathbf{y})\leq\langle\nabla f(\mathbf{x}),\mathbf{x}- \mathbf{y}\rangle-\frac{\alpha}{2}\cdot\langle\nabla f(\mathbf{x}),\mathbf{x} -\mathbf{y}\rangle^{2}\)._

Note that the formal definition of \(\beta\)-exp-concavity is that \(\exp(-\beta f(\cdot))\) is concave. Under Assumption 1, \(\beta\)-exp-concavity implies Definition 1 with \(\alpha=\frac{1}{2}\cdot\min\{1/(4GD),\beta\}\)(Hazan, 2016, Lemma 4.3). Therefore, we adopt Definition 1 as an alternative definition of exp-concavity for clarity.

### A General Framework for Universal Online Learning

In this part, we present a general framework of universal online learning (Zhang et al., 2022; Yan et al., 2023). Formally, we study the problem where the learner lacks the prior knowledge of curvature information, including _(i)_ curvature type: convexity, exp-concavity, or strong convexity; and _(ii)_ curvature coefficient: exp-concavity \(\alpha\) or strong convexity \(\lambda\). Without loss of generality, we focus on the case of \(\alpha,\lambda\in[1/T,1]\). If \(\alpha,\lambda<1/T\), even the optimal minimax results -- \(\mathcal{O}(\frac{d}{\alpha}\log T)\) for exp-concave functions and \(\mathcal{O}(\frac{1}{\lambda}\log T)\) for strongly convex functions (Hazan et al., 2007) - become linear in \(T\), rendering the regret bounds vacuous. On the other hand, if \(\alpha,\lambda>1\), we can simply treat them as \(\alpha,\lambda=1\), only making the regret worsen by an ignorable constant factor. This simplification is also adopted by Zhang et al. (2022); Yan et al. (2023).

To handle the uncertainty of curvatures, an online ensemble structure is usually employed, with multiple base learners exploring the environments and a meta learner tracking the best base learner. More specifically, to deal with the unknown curvature coefficients \(\alpha\) and \(\lambda\), we discretize them into the following candidate pool (Zhang et al., 2022):

\[\mathcal{H}\triangleq\{1/T,2/T,4/T,\ldots,2^{n-1}/T\},\] (2.1)

whose size is \(n=\lceil\log_{2}T\rceil+1=\mathcal{O}(\log T)\). It can be proved that the discretized candidate pool \(\mathcal{H}\) can approximate the continuous value of \(\alpha\) or \(\lambda\) with negligible constant errors. By doing this, it is natural to design three distinct groups of base learners:

1. _strongly convex_ base learners: \(n\) in total, each of which implements the algorithm for strongly convex functions with a guess \(\lambda_{i}\in\mathcal{H}\) of the strong convexity coefficient \(\lambda\);
2. _exp-concave_ base learners: \(n\) in total, each of which implements the algorithm for exp-concave functions with a guess \(\alpha_{i}\in\mathcal{H}\) of the exp-concavity coefficient \(\alpha\);
3. _convex_ base learner: only one, it runs the algorithm for convex functions.

In total, there are \(N\triangleq(2n+1)=\mathcal{O}(\log T)\) base learners with a two-layer structure, which is for now necessary in this problem. The best base learner is the one with the right guess of the curvature type and the closest guess of the curvature coefficient. Taking \(\lambda\)-strongly convex functions as an example, the guessed coefficient of the best base learner (indexed by \(i^{\star}\)) satisfies \(\lambda_{i^{\star}}\leq\lambda\leq 2\lambda_{i^{\star}}\).

Denoting by \(\mathbf{x}_{t,i}\) the decision generated by the \(i\)-th base learner at the \(t\)-th round, \(p_{t,i}\) the weight of the meta learner on the \(i\)-th base learner, an online ensemble method outputs the final decision as \(\mathbf{x}_{t}=\sum_{i\in[N]}p_{t,i}\mathbf{x}_{t,i}\). This forms a general framework for universal online learning and it remains to select suitable algorithms and loss functions for the meta and base learners. We will illuminate our concrete algorithm design in Section 3.1 and present a more detailed description of our meta/base learners configurations in Appendix B.

## 3 Our Approach

In this section, we present our approach for universal online learning with gradient-variation regret. Section 3.1 presents the overall procedure of our proposed algorithm. Subsequently, we outline our two key technical components: Section 3.2 presents a novel analysis to handle the empirical gradient variation, and Section 3.3 introduces surrogate functions to improve efficiency and provides a corresponding analysis for the empirical gradient variation defined on surrogates. We finally provide the optimal universal gradient-variation regret guarantees in Section 3.4.

### Overall Algorithm

In this part, we present our simple approach for universal online learning with gradient variations, summarized in Algorithm 1. Basically, it is a two-layer online ensemble. Base learners are implemented using the preliminary configurations given in Section 2.2 and on carefully designed surrogate functions. The meta learner runs Optimistic-Adapt-ML-Prod(Wei et al., 2016) on linearized losses. We specify the algorithmic details below, and a more detailed procedure in Appendix B.

In Line 3, the learner makes a weighted combination of the base learners' decisions \(\{\mathbf{x}_{t,i}\}_{i\in[N]}\) using the meta learner's weights \(\bm{p}_{t}=(p_{t,1},\dots,p_{t,N})\), submits the final decision \(\mathbf{x}_{t}\), suffers a loss \(f_{t}(\mathbf{x}_{t})\), and receives a single \(\nabla f_{t}(\mathbf{x}_{t})\) as the gradient feedback, using _only_\(1\) gradient query per round.

```
0: Meta learner \(\mathcal{A}\), base learners \(\{\mathcal{B}_{i}\}_{i\in[N]}\)
1:Initialize:\(p_{1,i}=\nicefrac{{1}}{{N}}\) and \(\mathbf{x}_{1,i}\) to be an arbitrary decision inside \(\mathcal{X}\) for all \(i\in[N]\)
2:for\(t=1\)to\(T\)do
3: Submit \(\mathbf{x}_{t}=\sum_{i\in[N]}p_{t,i}\mathbf{x}_{t,i}\), suffer \(f_{t}(\mathbf{x}_{t})\), and observe the gradient \(\nabla f_{t}(\mathbf{x}_{t})\)
# Meta Update:
4:\(\mathcal{A}\) updates to \(\bm{p}_{t+1}\in\Delta_{N}\) using the rule (B.5) with learning rate (B.6)
# Base Update:
5: Construct surrogates \(h^{\text{exp}}_{t,i}(\cdot)\)\(h^{\text{sc}}_{t,i}(\cdot)\), \(h^{\text{c}}_{t,i}(\cdot)\) defined in (3.1) using only \(\nabla f_{t}(\mathbf{x}_{t})\)
6:\(\mathcal{B}_{i}\) updates to \(\mathbf{x}_{t+1,i}\) using surrogates functions (3.1) and update rules (B.1)-(B.3) for \(i\in[N]\)
7:endfor ```

**Algorithm 1** A Simple Approach for Universal Online Learning with Gradient Variations

Meta Algorithm.In Line 4, the meta learner uses Optimistic-Adapt-ML-Prod(Wei et al., 2016) to update the weights by the following rule:

\[p_{t+1,i}\propto\varepsilon_{t,i}\cdot\exp(\varepsilon_{t,i}m_{t+1,i})\cdot W_ {t,i},W_{t,i}=\big{(}W_{t-1,i}\cdot\exp\big{(}\varepsilon_{t-1,i}r_{t,i}- \varepsilon_{t-1,i}^{2}(r_{t,i}-m_{t,i})\big{)}\big{)}^{\frac{\varepsilon_{t,i }}{\varepsilon_{t-1,i}}}\.\]

Specifically, denoting by \(\ell_{t,i}\triangleq\langle\nabla f_{t}(\mathbf{x}_{t}),\mathbf{x}_{t,i}\rangle\) the loss of the \(i\)-th dimension, the meta algorithm inputs: \(r_{t,i}=\langle\bm{\ell}_{t},\bm{p}_{t}\rangle-\ell_{t,i}\), the instantaneous regret; \(\varepsilon_{t,i}\), a time-varying learning rate; and \(m_{t,i}\), an estimation of the true loss of the \(t\)-th round (the choice of optimisms will be shown later). The meta algorithm then outputs the weights \(\bm{p}_{t+1}=(p_{t+1,1},\dots,p_{t+1,N})\) of the next round.

With appropriate learning rates (B.6), Optimistic-Adapt-ML-Prod achieves an optimistic second-order bound of \(\sum_{t\leq T}r_{t,i}\leq\mathcal{O}(\sqrt{\log N\sum_{t}(r_{t,i}-m_{t,i})^{2} }+\log N)\), where the \(\log N\) factor is negligible since the base learner number \(N\) equals \(\mathcal{O}(\log T)\) and we can treat \(\mathcal{O}(\log\log T)\) as a constant (Luo and Schapire, 2015). The formal guarantee of Optimistic-Adapt-ML-Prod is deferred to Lemma 2 in the appendix. In our problem, the instantaneous regret \(r_{t,i}=\langle\bm{\ell}_{t},\bm{p}_{t}\rangle-\ell_{t,i}=\langle\nabla f_{t}( \mathbf{x}_{t}),\mathbf{x}_{t}-\mathbf{x}_{t,i}\rangle\). Thus we choose \(m_{t,i}=\langle\nabla f_{t-1}(\mathbf{x}_{t-1}),\mathbf{x}_{t}-\mathbf{x}_{t,i}\rangle\) for the convex base learner and \(m_{t,i}=0\) otherwise (i.e., for exp-concave and strongly convex base learners).2 By doing this, we can upper-bound \(\sum_{t}\langle\nabla f_{t}(\mathbf{x}_{t}),\mathbf{x}_{t}-\mathbf{x}_{t,i}\rangle\) by \(\mathcal{O}(\sqrt{\sum_{t}\langle\nabla f_{t}(\mathbf{x}_{t})-\nabla f_{t-1}( \mathbf{x}_{t-1}),\mathbf{x}_{t}-\mathbf{x}_{t,i}\rangle^{2}})\) for the convex base learner and by \(\mathcal{O}(\sqrt{\sum_{t}\langle\nabla f_{t}(\mathbf{x}_{t}),\mathbf{x}_{t}- \mathbf{x}_{t,i}\rangle^{2}})\) otherwise. Later in Section 3.3, we illuminate how such results could benefit the final regret guarantees.

Base Algorithm.In Line 5, we adopt carefully designed surrogate functions for different types of base learners to reduce the gradient query complexity (Yan et al., 2023). Specifically, strongly convex, exp-concave, and the convex base learners run on the surrogate functions below respectively:

\[h^{\text{sc}}_{t,i}(\mathbf{x})\triangleq\langle\nabla f_{t}(\mathbf{x}_{t}), \mathbf{x}\rangle+\frac{\lambda_{i}}{4}\|\mathbf{x}-\mathbf{x}_{t}\|^{2},\quad h ^{\text{exp}}_{t,i}(\mathbf{x})\triangleq\langle\nabla f_{t}(\mathbf{x}_{t}), \mathbf{x}\rangle+\frac{\alpha_{i}}{4}\langle\nabla f_{t}(\mathbf{x}_{t}), \mathbf{x}-\mathbf{x}_{t}\rangle^{2},\] (3.1)

and \(h^{\text{c}}_{t,i}(\mathbf{x})\triangleq\langle\nabla f_{t}(\mathbf{x}_{t}), \mathbf{x}\rangle\), where \(\lambda_{i},\alpha_{i}\) are selected from the candidate coefficient pool \(\mathcal{H}\) in (2.1). We emphasize that the surrogate functions require only \(1\) gradient query \(\nabla f_{t}(\mathbf{x}_{t})\) per round. Finally, in Line 6, the \(i\)-th base learner \(\mathcal{B}_{i}\) updates the decision to \(\mathbf{x}_{t+1,i}\) using optimistic online mirror descent (OOMD) (Rakhlin and Sridharan, 2013), which is general and covers many algorithms of interest, such as OGD and online Newton step. For each curvature type (convexity, exp-concavity, or strong convexity), we adopt a correspondingly configured OOMD as the base learner. For detailed update rules of differently configured OOMD, we refer readers to (B.1)-(B.3) in Appendix B.

As for previous works, Zhang et al. (2022) adopted Adapt-ML-Prod (Gaillard et al., 2014) as the meta learner, which does not incorporate optimisms and thus is impossible to achieve gradient-variation regret for convex functions, and operates on the original loss function \(f_{t}(\cdot)\) for base learners, which leads to a less efficient gradient query complexity of \(\mathcal{O}(\log T)\) per round. Yan et al. (2023) used a two-layer meta algorithm MsMwC-Master (Chen et al., 2021) as the meta learner, resulting in a three-layer ensemble structure, which is also not efficient enough. Compared with approaches above, our Algorithm 1 is simpler and more efficient as it requires \(\mathcal{O}(\log T)\) base learners and only \(1\) gradient query in each round. We emphasize that our contributions mainly lie in the technical aspects showing that although simple, our approach can achieve the optimal universal gradient-variation regret, which is accomplished via two novel analytical components.

### Novel Analysis on Empirical Gradient Variations

In this part, we provide a novel analysis of the gradient-variation regret. For clarity, we illustrate from the lowest level -- as we only use one gradient \(\nabla f_{t}(\mathbf{x}_{t})\) in the \(t\)-th round, to obtain the gradient variation \(V_{T}\) defined in (1.2), it is necessary to first attain its _empirical_ version \(\bar{V}_{T}\triangleq\sum_{t\leq T}\|\nabla f_{t}(\mathbf{x}_{t})-\nabla f_{t -1}(\mathbf{x}_{t-1})\|^{2}\). Previous studies decompose this term into two parts:

\[\|\nabla f_{t}(\mathbf{x}_{t})-\nabla f_{t-1}(\mathbf{x}_{t-1})\| ^{2} \lesssim\|\nabla f_{t}(\mathbf{x}_{t})-\nabla f_{t-1}(\mathbf{x}_ {t})\|^{2}+\|\nabla f_{t-1}(\mathbf{x}_{t})-\nabla f_{t-1}(\mathbf{x}_{t-1})\| ^{2}\] \[\leq\sup_{\mathbf{x}\in\mathcal{X}}\|\nabla f_{t}(\mathbf{x})- \nabla f_{t-1}(\mathbf{x})\|^{2}+L^{2}\|\mathbf{x}_{t}-\mathbf{x}_{t-1}\|^{2},\]

using smoothness (i.e., Assumption 2). Aggregating the first term over \(T\) rounds leads to the desired \(V_{T}\) quantity and the remaining challenge is to control the algorithmic stability \(\|\mathbf{x}_{t}-\mathbf{x}_{t-1}\|^{2}\). Consequently, since each decision is a weighted combination of base learners' decisions (i.e., \(\mathbf{x}_{t}=\sum_{i\leq N}p_{t,i}\mathbf{x}_{t,i}\)), the algorithmic stability is difficult to control. To this end, Yan et al. (2023) decomposed the stability term in the following way:

\[\|\mathbf{x}_{t}-\mathbf{x}_{t-1}\|^{2}\lesssim\sum_{i=1}^{N}p_{t,i}\|\mathbf{ x}_{t,i}-\mathbf{x}_{t-1,i}\|^{2}+\|\bm{p}_{t}-\bm{p}_{t-1}\|_{1}^{2}.\] (3.2)

Consequently, for the first term, the authors injected correction terms to the meta learner following Zhao et al. (2024). To cancel the second term, the meta algorithm must include a corresponding negative stability term in its analysis, while achieving an optimistic second-order bound simultaneously. To the best of our knowledge, the only feasible algorithm satisfying both requirements is the two-layer meta algorithm MsMwC-Master(Chen et al., 2021), which leads to a three-layer online ensemble structure and therefore affects the efficiency. Besides, it attains a second-order bound of the form \(\mathcal{O}(\sqrt{Q_{T,i}\log Q_{T,i}})\), where \(Q_{T,i}\triangleq\sum_{t}(\ell_{t,i}-m_{t,i})^{2}\), which causes the sub-optimality of the regret guarantees with an additional logarithmic factor in the results of Yan et al. (2023).

In this work, we handle the empirical gradient variation alternatively via a novel and simple analysis with two key parts: _(i)_ a negative term arising from linearization; and _(ii)_ a useful smoothness property. First, we observe that the instantaneous regret can be transformed as:

\[f_{t}(\mathbf{x}_{t})-f_{t}(\mathbf{x}^{\star})=\langle\nabla f_{t}(\mathbf{x} _{t}),\mathbf{x}_{t}-\mathbf{x}^{\star}\rangle-\mathcal{D}_{f_{t}}(\mathbf{x} ^{\star},\mathbf{x}_{t}),\] (3.3)

where \(\mathbf{x}^{\star}\in\arg\min_{\mathbf{x}\in\mathcal{X}}\sum_{t}f_{t}(\mathbf{x})\) and \(\mathcal{D}_{f}(\mathbf{x},\mathbf{y})\triangleq f(\mathbf{x})-f(\mathbf{y})- \langle\nabla f(\mathbf{y}),\mathbf{x}-\mathbf{y}\rangle\) is the Bregman divergence associated with function \(f(\cdot)\). The last term is a _negative term from linearization_, which 

[MISSING_PAGE_FAIL:7]

where the first step uses (3.3) and the fact that \(\mathcal{D}_{f_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})\geq\frac{\lambda}{2}\| \mathbf{x}_{t}-\mathbf{x}^{\star}\|^{2}\) since \(f_{t}(\cdot)\) is \(\lambda\)-strongly convex. The second step uses the definition of the best base learner (indexed by \(i^{\star}\)): \(\lambda_{i^{\star}}\leq\lambda\leq 2\lambda_{i^{\star}}\) and the definition of surrogate functions defined in (3.1). The first term above (_meta regret_) assesses how well the algorithm tracks the best base learner, and the second term (_base regret_) measures the best base learner's performance. The meta regret contains a linearized regret with a negative term from curvatures. This negative term is useful for exp-concave and strongly convex functions if the linearized regret enjoys a second-order bound:

\[\sum_{t=1}^{T}r_{t,i^{\star}}-\frac{\lambda_{i^{\star}}}{4}\sum_{t=1}^{T}\| \mathbf{x}_{t}-\mathbf{x}_{t,i^{\star}}\|^{2}\lesssim\sqrt{\sum_{t=1}^{T} \langle\nabla f_{t}(\mathbf{x}_{t}),\mathbf{x}_{t}-\mathbf{x}_{t,i^{\star}} \rangle^{2}}-\frac{\lambda_{i^{\star}}}{4}\sum_{t=1}^{T}\|\mathbf{x}_{t}- \mathbf{x}_{t,i^{\star}}\|^{2}\lesssim\frac{1}{\lambda},\]

where the \(1/\lambda\) factor can be absorbed by the final regret \(\mathcal{O}(\frac{1}{\lambda}\log V_{T})\). The base regret is defined on the surrogates, which preserves the curvature properties, but using only \(1\) gradient \(\nabla f_{t}(\mathbf{x}_{t})\).

Below we explain the reason for handling the empirical gradient variation defined on surrogates. Taking the \(i\)-th strongly convex base learner as an example, it updates as \(\mathbf{x}_{t,i}=\Pi_{\mathcal{X}}[\widehat{\mathbf{x}}_{t,i}-\eta_{t}\mathbf{ m}_{t}]\) and \(\widehat{\mathbf{x}}_{t+1,i}=\Pi_{\mathcal{X}}[\widehat{\mathbf{x}}_{t,i}- \eta_{t}\nabla h_{t,i}^{\text{sc}}(\mathbf{x}_{t,i})]\), an initialization of the OOMD algorithm, where \(\eta_{t}\) represents the step size, \(\Pi_{\mathcal{X}}[\mathbf{x}]=\arg\min_{\mathbf{y}\in\mathcal{X}}\|\mathbf{x}- \mathbf{y}\|\) denotes the Euclidean projection onto \(\mathcal{X}\), and \(\widehat{\mathbf{x}}_{t,i}\) is an intermediate variable. With appropriately chosen step sizes, the base learner achieves an optimistic bound of \(\mathcal{O}(\log D_{T})\), where \(D_{T}=\sum_{t\leq T}\|\nabla h_{t,i}^{\text{sc}}(\mathbf{x}_{t,i})-\mathbf{m} _{t}\|^{2}\) (e.g., please refer to Theorem 15 of Chiang et al. (2012)). Therefore, choosing the optimism as \(\mathbf{m}_{t}=\nabla h_{t-1,i}^{\text{sc}}(\mathbf{x}_{t-1,i})\) leads to an empirical gradient-variation bound \(\mathcal{O}(\log D_{T})\)_defined on surrogates_,3 where

Footnote 3: For strongly convex functions, it is possible to choose \(\mathbf{m}_{t}=\nabla f_{t-1}(\mathbf{x}_{t-1})\) to avoid additional surrogate-induced terms, that will be discussed below. We choose the gradient of the last round as the optimism since this is the the only choice at present to achieve an optimistic regret for exp-concave functions (Chiang et al., 2012).

\[D_{T} = \sum_{t=2}^{T}\|\nabla h_{t,i}^{\text{sc}}(\mathbf{x}_{t,i})- \nabla h_{t-1,i}^{\text{sc}}(\mathbf{x}_{t-1,i})\|^{2}\] \[= \sum_{t=2}^{T}\left\|\nabla f_{t}(\mathbf{x}_{t})-\nabla f_{t-1} (\mathbf{x}_{t-1})+\frac{\lambda_{i}}{2}(\mathbf{x}_{t,i}-\mathbf{x}_{t})- \frac{\lambda_{i}}{2}(\mathbf{x}_{t-1,i}-\mathbf{x}_{t-1})\right\|^{2}.\]

The empirical gradient variation defined on the original functions, i.e., \(\|\nabla f_{t}(\mathbf{x}_{t})-\nabla f_{t-1}(\mathbf{x}_{t-1})\|^{2}\), can be handled via the analysis in Section 3.2. The main challenge is to deal with the rest terms caused by the surrogate functions. Yan et al. (2023) overcame this issue by controlling \((\mathbf{x}_{t}-\mathbf{x}_{t-1})\) and \((\mathbf{x}_{t,i}-\mathbf{x}_{t-1,i})\) separately. Again, as we have explained in Section 3.2, since the decision \(\mathbf{x}_{t}\) is a weighted combination of base learners' decisions (i.e., \(\mathbf{x}_{t}=\sum_{i\leq N}p_{t,i}\mathbf{x}_{t,i}\)), handling the algorithmic stability term \(\|\mathbf{x}_{t}-\mathbf{x}_{t-1}\|^{2}\) directly using (3.2) would results in a less efficient three-layer online ensemble structure and the sub-optimality of the regret guarantees, as Yan et al. (2023) did.

In this work, we propose a novel analysis -- while controlling \((\mathbf{x}_{t,i}-\mathbf{x}_{t})-(\mathbf{x}_{t-1,i}-\mathbf{x}_{t-1})\) in each individual round is hard, it can be bounded when _aggregated over the time horizon_. Specifically, we bound it by combining two summations into one:

\[\sum_{t=2}^{T}\|(\mathbf{x}_{t,i}-\mathbf{x}_{t})-(\mathbf{x}_{t-1,i}-\mathbf{x }_{t-1})\|^{2}\lesssim\sum_{t=2}^{T}\|\mathbf{x}_{t,i}-\mathbf{x}_{t}\|^{2}+ \sum_{t=2}^{T}\|\mathbf{x}_{t-1,i}-\mathbf{x}_{t-1}\|^{2}\leq 2\sum_{t=1}^{T}\| \mathbf{x}_{t,i}-\mathbf{x}_{t}\|^{2}.\]

The same idea is also used in the derivation of (3.5). Consequently, this term can be canceled out by the negative term from curvatures in the meta regret. For this cancellation to occur, appropriate coefficients are chosen, which are provided in the detailed proofs (e.g., the 'Regret Analysis' part in the proof of Theorem 1) and are omitted here for clarity.

This simple and novel analysis eliminates the need to control the overall algorithmic stability term of \(\|\mathbf{x}_{t}-\mathbf{x}_{t-1}\|^{2}\) required by previous works, and is essential for achieving the improved computational efficiency and the optimal regret guarantees, as shown in the next part.

### Optimal Universal Gradient-Variation Regret Guarantees

In this part, we present our main theoretical result -- our simple and efficient Algorithm 1 (in Section 3.1) which adopts two novel analyses (in Section 3.2 and Section 3.3) achieves the _optimal_

[MISSING_PAGE_FAIL:9]

**Theorem 3**.: _Under Assumption 1 and smoothness of \(F_{t}(\cdot)\) for any \(t\in[T]\): if \(F_{t}(\cdot)\) is convex, Algorithm 1 achieves \(\mathcal{O}(\sqrt{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}})\); if \(f_{t}(\cdot)\) is exp-concave, it achieves \(\mathcal{O}(d\log(\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}))\); and if \(F_{t}(\cdot)\) is strongly convex, it achieves \(\mathcal{O}((\sigma_{\max}^{2}+\Sigma_{\max}^{2})\log((\sigma_{1:T}^{2}+ \Sigma_{1:T}^{2})/(\sigma_{\max}^{2}+\Sigma_{\max}^{2})))\)._

Theorem 3 requires exp-concavity of the individual function \(f_{t}(\cdot)\) rather than the expected function \(F_{t}(\cdot)\). This assumption is also used by Chen et al. (2023) and common in the studies of stochastic exp-concave optimization Mahdavi et al. (2015); Koren and Levy (2015).

### Discussion on Comparison with Correction-based Approach

In this part, we discuss the technical comparison with the previous correction-based approach Yan et al. (2023). Compared with their approach, ours is simpler and achieves the optimal universal problem-dependent regret Theorem 1 and Theorem 2) and the best known guarantees in the SEA model (Theorem 3). Although not providing guarantees as favorable as ours, Yan et al. (2023) can control the overall algorithmic stability (i.e., \(\|\mathbf{x}_{t}-\mathbf{x}_{t-1}\|^{2}\)) using collaborative online ensemble Zhao et al. (2024), which is necessary in achieving fast rates in multi-player games Syrgkanis et al. (2015). For example, in a min-max game \(\min_{\mathbf{x}\in\mathcal{X}}\max_{\mathbf{y}\in\mathcal{Y}}\mathbf{x}^{ \top}A\mathbf{y}\), where \(A\) is a game matrix, since \(A\) is unknown, the Nash equilibrium is typically computed through repeated play, i.e., player-\(\mathbf{x}\) and player-\(\mathbf{y}\) select \(\{\mathbf{x}_{t}\}_{t=1}^{T}\) and \(\{\mathbf{y}_{t}\}_{t=1}^{T}\) sequentially to approach the Nash equilibrium. For player-\(\mathbf{x}\), in the \(t\)-th round, it suffers a loss \(\mathbf{x}_{t}^{\top}A\mathbf{y}_{t}\) and receives the gradient \(A\mathbf{y}_{t}\). Similarly, player-\(\mathbf{y}\) suffers \(-\mathbf{x}_{t}^{\top}A\mathbf{y}_{t}\) and receives \(-A\mathbf{x}_{t}\). For player-\(\mathbf{x}\), if it updates via OOMD, its gradient-variation regret contains \(\|A\mathbf{y}_{t}-A\mathbf{y}_{t-1}\|^{2}\), which includes the stability of player-\(\mathbf{y}\). In this case, to achieve fast rates, we indeed need to control the algorithm stability like \(\|\mathbf{x}_{t}-\mathbf{x}_{t-1}\|^{2}\) and \(\|\mathbf{y}_{t}-\mathbf{y}_{t-1}\|^{2}\). This can be done by Yan et al. (2023) while this work cannot since we do not directly control the algorithmic stability. Interested readers can refer to Appendix A.2 in Yan et al. (2023) for more details.

## 5 Conclusion

In this work, we investigate universal online learning with gradient-variation regret. We propose a simple two-layer online ensemble approach that not only achieves the optimal \(\mathcal{O}(\frac{1}{\lambda}\log V_{T})\), \(\mathcal{O}(\frac{d}{\alpha}\log V_{T})\), and \(\mathcal{O}(\sqrt{V_{T}})\) regret simultaneously for \(\lambda\)-strongly convex, \(\alpha\)-exp-concave, and convex functions and is efficient with \(\mathcal{O}(\log T)\) base learners and only \(1\) gradient query per round. This is done via the negative Bregman divergence term from linearization and the useful smoothness property of \(\|\nabla f(\mathbf{x})-\nabla f(\mathbf{y})\|^{2}\leq 2L\mathcal{D}_{f}( \mathbf{y},\mathbf{x})\). We further validate the effectiveness of our approach and results by implying the optimal universal small-loss regret directly in analysis and achieving the best known results in the stochastically extended adversarial model.

Two future directions are worth investigating. The first is to reduce the number of projections to only \(1\) in each round Mhammedi et al. (2019); Zhao et al. (2022); Yang et al. (2024), thereby further improving the computational efficiency. The second direction involves extending our algorithm and results to the unconstrained domain using recent advances in parameter-free online learning Orabona and Pal (2016); Cutkosky and Orabona (2018); Jacobsen and Cutkosky (2022), to broaden its applicability across a wider range of scenarios.

\begin{table}
\begin{tabular}{c|c c c|c} \hline \hline \multirow{2}{*}{**Works**} & \multicolumn{4}{c|}{**Regret Bounds**} & \multirow{2}{*}{**Single**} \\ \cline{2-3} \cline{5-5}  & Strongly Convex & & \multirow{2}{*}{Exp-concave} & \multirow{2}{*}{**Convex**} & \multirow{2}{*}{**Algorithm?**} \\ \hline Sachs et al. (2022) & \(\mathcal{O}((\sigma_{\max}^{2}+\Sigma_{\max}^{2})\log T)\) & N/A & \(\mathcal{O}(\sqrt{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}})\) & ✗ \\ \hline Chen et al. (2024) & \(\mathcal{O}\left(\left(\sigma_{\max}^{2}+\Sigma_{\max}^{2}\right)\log\left( \frac{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}}{\sigma_{\max}^{2}+\Sigma_{\max}^{2}}\right)\right)\) & \(\mathcal{O}(d\log(\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}))\) & ✗ \\ \hline \hline Sachs et al. (2023) & \(\mathcal{O}((\sigma_{\max}^{2}+\Sigma_{\max}^{2}+D^{2}L^{2})\log^{2}T)\) & N/A & \(\mathcal{O}(\sqrt{T\log T})\) & ✓ \\ \hline Yan et al. (2023) & \(\mathcal{O}((\sigma_{\max}^{2}+\Sigma_{\max}^{2})\log(\sigma_{1:T}^{2}+\Sigma_{1 :T}^{2}))\) & \(\mathcal{O}(d\log(\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}))\) & \(\mathcal{O}(\sqrt{(\sigma_{1:T}^{2}+\Sigma_{1:T}^{2})}\log(\sigma_{1:T}^{2}+ \Sigma_{1:T}^{2}))\) & ✓ \\ \hline
**Ours** & \(\mathcal{O}\left(\left(\sigma_{\max}^{2}+\Sigma_{\max}^{2}\right)\log\left( \frac{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}}{\sigma_{\max}^{2}+\Sigma_{\max}^{2}}\right)\right)\) & \(\mathcal{O}(d\log(\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}))\) & ✓ \\ \hline \hline \end{tabular}
\end{table}
Table 2: Comparisons of our results with existing ones. The second column presents the regret bounds, where \(\sigma_{1:T}^{2}\) and \(\Sigma_{1:T}^{2}\) represent the stochastic and adversarial statistics of the SEA problem. The last column indicates whether the results can be achieved by a single algorithm (i.e., suitable in the universal setup). We achieve the same state-of-the-art guarantees as Chen et al. (2024) using one single algorithm.

## Acknowledgements

This research was supported by National Science and Technology Major Project (2022ZD0114802), NSFC (62176117), and JiangsuSF (BK20220776). Peng Zhao was supported in part by the Xiaomi Foundation.

## References

* Abernethy et al. (2008) J. Abernethy, P. L. Bartlett, A. Rakhlin, and A. Tewari. Optimal strategies and minimax lower bounds for online convex games. In _Proceedings of the 21st Annual Conference on Learning Theory (COLT)_, pages 414-424, 2008.
* Cesa-Bianchi and Lugosi (2006) N. Cesa-Bianchi and G. Lugosi. _Prediction, Learning, and Games_. Cambridge University Press, 2006.
* Chen et al. (2021) L. Chen, H. Luo, and C. Wei. Impossible tuning made possible: A new expert algorithm and its applications. In _Proceedings of the 34th Annual Conference on Learning Theory (COLT)_, pages 1216-1259, 2021.
* Chen et al. (2023) S. Chen, W.-W. Tu, P. Zhao, and L. Zhang. Optimistic online mirror descent for bridging stochastic and adversarial online convex optimization. In _Proceedings of the 40th International Conference on Machine Learning (ICML)_, pages 5002-5035, 2023.
* 62, 2024.
* Chiang et al. (2012) C. Chiang, T. Yang, C. Lee, M. Mahdavi, C. Lu, R. Jin, and S. Zhu. Online optimization with gradual variations. In _Proceedings of the 25th Annual Conference on Learning Theory (COLT)_, pages 6.1-6.20, 2012.
* Cutkosky and Orabona (2018) A. Cutkosky and F. Orabona. Black-box reductions for parameter-free online learning in Banach spaces. In _Proceedings of the 31st Annual Conference on Learning Theory (COLT)_, pages 1493-1529, 2018.
* Daniely et al. (2015) A. Daniely, A. Gonen, and S. Shalev-Shwartz. Strongly adaptive online learning. In _Proceedings of the 32nd International Conference on Machine Learning (ICML)_, pages 1405-1411, 2015.
* Gaillard et al. (2014) P. Gaillard, G. Stoltz, and T. van Erven. A second-order bound with excess losses. In _Proceedings of the 27th Annual Conference on Learning Theory (COLT)_, pages 176-196, 2014.
* Hazan (2016) E. Hazan. Introduction to Online Convex Optimization. _Foundations and Trends in Optimization_, 2(3-4):157-325, 2016.
* Hazan and Seshadhri (2007) E. Hazan and C. Seshadhri. Adaptive algorithms for online decision problems. _Electronic Colloquium on Computational Complexity (ECCC)_, 14(088), 2007.
* Hazan et al. (2007) E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. _Machine Learning_, 69(2-3):169-192, 2007.
* Jacobsen and Cutkosky (2022) A. Jacobsen and A. Cutkosky. Parameter-free mirror descent. In _Proceedings of the 35th Annual Conference on Learning Theory (COLT)_, pages 4160-4211, 2022.
* Ji and Ye (2009) S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In _Proceedings of the 26th International Conference on Machine Learning (ICML)_, pages 457-464, 2009.
* Joulani et al. (2020) P. Joulani, A. Raj, A. Gyorgy, and C. Szepesvari. A simpler approach to accelerated optimization: Iterative averaging meets optimism. In _Proceedings of the 37th International Conference on Machine Learning (ICML)_, pages 4984-4993, 2020.
* Koren and Levy (2015) T. Koren and K. Y. Levy. Fast rates for exp-concave empirical risk minimization. In _Advances in Neural Information Processing Systems 28 (NIPS)_, pages 1477-1485, 2015.
* Kale et al. (2016)M. Li, T. Zhang, Y. Chen, and A. J. Smola. Efficient mini-batch training for stochastic optimization. In _Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD)_, pages 661-670, 2014.
* Luo and Schapire (2015) H. Luo and R. E. Schapire. Achieving all with no parameters: AdaNormalHedge. In _Proceedings of the 28th Annual Conference on Learning Theory (COLT)_, pages 1286-1304, 2015.
* Mahdavi et al. (2015) M. Mahdavi, L. Zhang, and R. Jin. Lower and upper bounds on the generalization of stochastic exponentially concave optimization. In _Proceedings of the 28th Annual Conference on Learning Theory (COLT)_, pages 1305-1320, 2015.
* Mhammedi et al. (2019) Z. Mhammedi, W. M. Koolen, and T. van Erven. Lipschitz adaptivity with multiple learning rates in online learning. In _Proceedings of the 32nd Annual Conference on Learning Theory (COLT)_, pages 2490-2511, 2019.
* Nesterov (2018) Y. Nesterov. _Lectures on Convex Optimization_, volume 137. Springer, 2018.
* Orabona (2019) F. Orabona. A modern introduction to online learning. _ArXiv preprint_, arXiv:1912.13213, 2019.
* Orabona and Pal (2016) F. Orabona and D. Pal. Coin betting and parameter-free online learning. In _Advances in Neural Information Processing Systems 29 (NIPS)_, pages 577-585, 2016.
* Orabona et al. (2012) F. Orabona, N. Cesa-Bianchi, and C. Gentile. Beyond logarithmic bounds in online learning. In _Proceedings of the 15th International Conference on Artificial Intelligence and Statistics (AISTATS)_, pages 823-831, 2012.
* Ordentlich and Cover (1998) E. Ordentlich and T. M. Cover. The cost of achieving the best portfolio in hindsight. _Mathematics of Operations Research_, 23(4):960-982, 1998.
* Rakhlin and Sridharan (2013) A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In _Proceedings of the 26th Annual Conference on Learning Theory (COLT)_, pages 993-1019, 2013.
* Sachs et al. (2022) S. Sachs, H. Hadjii, T. van Erven, and C. Guzman. Between stochastic and adversarial online convex optimization: Improved regret bounds via smoothness. In _Advances in Neural Information Processing Systems 35 (NeurIPS)_, pages 691-702, 2022.
* Sachs et al. (2023) S. Sachs, H. Hadjii, T. van Erven, and C. Guzman. Accelerated rates between stochastic and adversarial online convex optimization. _ArXiv preprint_, arXiv:2303.03272, 2023.
* Srebro et al. (2010) N. Srebro, K. Sridharan, and A. Tewari. Smoothness, low noise and fast rates. In _Advances in Neural Information Processing Systems 23 (NIPS)_, pages 2199-2207, 2010.
* Syrgkanis et al. (2015) V. Syrgkanis, A. Agarwal, H. Luo, and R. E. Schapire. Fast convergence of regularized learning in games. In _Advances in Neural Information Processing Systems 28 (NIPS)_, pages 2989-2997, 2015.
* van Erven and Koolen (2016) T. van Erven and W. M. Koolen. MetaGrad: Multiple learning rates in online learning. In _Advances in Neural Information Processing Systems 29 (NIPS)_, pages 3666-3674, 2016.
* Wang et al. (2019) G. Wang, S. Lu, and L. Zhang. Adaptivity and optimality: A universal algorithm for online convex optimization. In _Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI)_, pages 659-668, 2019.
* Wei et al. (2016) C. Wei, Y. Hong, and C. Lu. Tracking the best expert in non-stationary stochastic environments. In _Advances in Neural Information Processing Systems 29 (NIPS)_, pages 3972-3980, 2016.
* Yan et al. (2023) Y.-H. Yan, P. Zhao, and Z.-H. Zhou. Universal online learning with gradient variations: A multi-layer online ensemble approach. In _Advances in Neural Information Processing Systems 36 (NeurIPS)_, pages 37682-37715, 2023.
* Yang et al. (2014) T. Yang, M. Mahdavi, R. Jin, and S. Zhu. Regret bounded by gradual variation for online convex optimization. _Machine Learning_, 95(2):183-223, 2014.
* Yang et al. (2024) W. Yang, Y. Wang, P. Zhao, and L. Zhang. Universal online convex optimization with 1 projection per round. _ArXiv preprint_, arXiv:2405.19705, 2024.
* Zhang et al. (2015)L. Zhang, S. Lu, and Z.-H. Zhou. Adaptive online learning in dynamic environments. In _Advances in Neural Information Processing Systems 31 (NeurIPS)_, pages 1330-1340, 2018.
* Zhang et al. (2019) L. Zhang, T.-Y. Liu, and Z.-H. Zhou. Adaptive regret of convex and smooth functions. In _Proceedings of the 36th International Conference on Machine Learning (ICML)_, pages 7414-7423, 2019.
* Zhang et al. (2022a) L. Zhang, G. Wang, J. Yi, and T. Yang. A simple yet universal strategy for online convex optimization. In _Proceedings of the 39th International Conference on Machine Learning (ICML)_, pages 26605-26623, 2022a.
* Zhang et al. (2022b) M. Zhang, P. Zhao, H. Luo, and Z.-H. Zhou. No-regret learning in time-varying zero-sum games. In _Proceedings of the 39th International Conference on Machine Learning (ICML)_, pages 26772-26808, 2022b.
* Zhao (2021) P. Zhao. _Online Ensemble Theories and Methods for Robust Online Learning_. PhD thesis, Nanjing University, Nanjing, China, 2021. Advisor: Zhi-Hua Zhou.
* Zhao et al. (2020) P. Zhao, Y.-J. Zhang, L. Zhang, and Z.-H. Zhou. Dynamic regret of convex and smooth functions. In _Advances in Neural Information Processing Systems 33 (NeurIPS)_, pages 12510-12520, 2020.
* Zhao et al. (2022) P. Zhao, Y.-F. Xie, L. Zhang, and Z.-H. Zhou. Efficient methods for non-stationary online learning. In _Advances in Neural Information Processing Systems 35 (NeurIPS)_, pages 11573-11585, 2022.
* 52, 2024.
* Zhou (2012) Z.-H. Zhou. _Ensemble Methods: Foundations and Algorithms_. Chapman & Hall/CRC Press, 2012.
* Zinkevich (2003) M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _Proceedings of the 20th International Conference on Machine Learning (ICML)_, pages 928-936, 2003.

On Smoothness Assumption

In this section, we propose a relaxation of the smoothness requirement to a slightly larger domain than the feasible domain \(\mathcal{X}\), in contrast to the whole \(\mathbb{R}^{d}\) space as in Assumption 2. The relaxed smoothness assumption is detailed in the following.

**Assumption 3** (Relaxed Smoothness).: Under the condition of \(\|\nabla f_{t}(\mathbf{x})\|\leq G\) for any \(\mathbf{x}\in\mathcal{X}\) and \(t\in[T]\), all online functions are \(L\)-smooth: \(\|\nabla f_{t}(\mathbf{x})-\nabla f_{t}(\mathbf{y})\|\leq L\|\mathbf{x}- \mathbf{y}\|\) for any \(t\in[T]\) and \(\mathbf{x},\mathbf{y}\in\mathcal{X}_{+}\), where \(\mathcal{X}_{+}\triangleq\{\mathbf{x}+\mathbf{b}\mid\mathbf{x}\in\mathcal{X}, \mathbf{b}\in\nicefrac{{G}}{{L}}\cdot\mathbb{B}\}\) and \(\mathbb{B}\triangleq\{\mathbf{x}\mid\|\mathbf{x}\|\leq 1\}\) is a unit ball.

The domain of Assumption 3 is slightly larger than \(\mathcal{X}\) -- for any \(\mathbf{x}_{+}\in\mathcal{X}_{+}\), we can always find an \(\mathbf{x}\in\mathcal{X}\) such that \(\|\mathbf{x}_{+}-\mathbf{x}\|\leq\nicefrac{{G}}{{L}}\). In this work, one of the key technical contributions is to handle the empirical gradient variation via a useful smoothness property (Proposition 1). We show in the following that this condition can be satisfied by requiring only Assumption 3.

**Lemma 1**.: _Under Assumption 1, for any online function \(f(\cdot)\) satisfying Assumption 3, it holds that \(\|\nabla f(\mathbf{x})-\nabla f(\mathbf{y})\|^{2}\leq 2L\mathcal{D}_{f}( \mathbf{y},\mathbf{x})\) for any \(\mathbf{x},\mathbf{y}\in\mathcal{X}\)._

Proof.: To begin with, we present the self-bounding property (Srebro et al., 2010), which is useful in proving our result -- if a function \(f:\mathbb{R}^{d}\mapsto\mathbb{R}\) is \(L\)-smooth and bounded from below, then for any \(\mathbf{x}\in\mathbb{R}^{d}\), it holds that

\[\|\nabla f(\mathbf{x})\|^{2}\leq 2L\left(f(\mathbf{x})-\inf_{\mathbf{y}\in \mathbb{R}^{d}}f(\mathbf{y})\right).\] (A.1)

Next, we aim to prove that if we only need (A.1) on a bounded domain \(\mathcal{X}\), we require smoothness only on a slightly larger domain than \(\mathcal{X}\). To see this, we delve into the proof of the self-bounding property. Specifically, for any \(\mathbf{x},\mathbf{v}\in\mathbb{R}^{d}\), it holds that

\[\langle-\nabla f(\mathbf{x}),\mathbf{v}\rangle-\frac{L}{2}\|\mathbf{v}\|^{2} \leq f(\mathbf{x})-f(\mathbf{x}+\mathbf{v})\leq f(\mathbf{x})-\inf_{\mathbf{ y}\in\mathbb{R}^{d}}f(\mathbf{y}),\]

where the first step requires smoothness on \(\mathbf{x}\) and \(\mathbf{x}+\mathbf{v}\). Consequently, by taking maximization over \(\mathbf{v}\), it holds that

\[f(\mathbf{x})-\inf_{\mathbf{y}\in\mathbb{R}^{d}}f(\mathbf{y})\geq\sup_{ \mathbf{v}\in\mathbb{R}^{d}}\langle-\nabla f(\mathbf{x}),\mathbf{v}\rangle- \frac{L}{2}\|\mathbf{v}\|^{2}=\frac{1}{2L}\|\nabla f(\mathbf{x})\|^{2},\]

which leads to the self-bounding property (A.1) by taking \(\mathbf{v}=-\frac{1}{L}\nabla f(\mathbf{x})\). The above proof is from Theorem 4.23 of Orabona (2019). This means that for the self-bounding property, we only require the smoothness to hold for any \(\mathbf{x}\in\mathcal{X}\) and \(\mathbf{x}-\frac{1}{L}\nabla f(\mathbf{x})\). Under Assumption 1, this can be satisfied by requiring smoothness on a slightly larger domain than \(\mathcal{X}\), namely, \(\mathcal{X}_{+}\triangleq\{\mathbf{x}+\mathbf{b}\mid\mathbf{x}\in\mathcal{X},\mathbf{b}\in\nicefrac{{G}}{{L}}\cdot\mathbb{B}\}\).

Now we are ready to prove the final result. To begin with, we define a surrogate function of \(g(\mathbf{x})\triangleq f(\mathbf{x})-\langle\nabla f(\mathbf{x}_{0}),\mathbf{ x}\rangle\) for any \(\mathbf{x}\in\mathcal{X}\), where \(\mathbf{x}_{0}\in\mathcal{X}\). Due to the above property we have just proved, by requiring smoothness on \(\mathcal{X}_{+}\), we have

\[\|\nabla g(\mathbf{x})\|^{2}\leq 2L\left(g(\mathbf{x})-\inf_{\mathbf{y}\in \mathbb{R}^{d}}g(\mathbf{y})\right).\]

Denoting by \(\mathbf{y}^{\star}\in\arg\min_{\mathbf{y}\in\mathbb{R}^{d}}g(\mathbf{y})\), the above inequality equals to

\[\|\nabla f(\mathbf{x})-\nabla f(\mathbf{x}_{0})\|^{2} \leq 2L\left(f(\mathbf{x})-\langle\nabla f(\mathbf{x}_{0}),\mathbf{ x}\rangle-f(\mathbf{y}^{\star})+\langle\nabla f(\mathbf{x}_{0}),\mathbf{y}^{\star}\rangle\right)\] \[=2L(f(\mathbf{x})-f(\mathbf{y}^{\star})-\langle\nabla f(\mathbf{ x}_{0}),\mathbf{x}-\mathbf{y}^{\star}\rangle),\]

due to the definition of \(g(\cdot)\). The proof using the self-bounding property is from Theorem 2.1.5 of Nesterov (2018). Finally, we note that \(g(\cdot)\) is minimized at \(\mathbf{y}^{\star}=\mathbf{x}_{0}\), leading to \(\|\nabla f(\mathbf{x})-\nabla f(\mathbf{x}_{0})\|^{2}\leq 2L\mathcal{D}_{f}( \mathbf{x}_{0},\mathbf{x})\) for any \(\mathbf{x},\mathbf{x}_{0}\in\mathcal{X}\), which finishes the proof. 

## Appendix B Omitted Details of Algorithm 1

In this section, we provide some omitted details of our Algorithm 1, including the losses and update rules of the base and meta learners.

Base Learners.To begin with, we duplicate the candidate coefficient pool (2.1) for both the exp-concave coefficient \(\alpha\) and the strongly convex coefficient \(\lambda\), denoted by \(\mathcal{H}^{\text{exp}}\triangleq\mathcal{H}\) and \(\mathcal{H}^{\text{sc}}\triangleq\mathcal{H}\). Consequently, denoting by \(N^{\text{exp}}=N^{\text{sc}}\triangleq|\mathcal{H}|\) the size of candidate pool, for each \(\alpha_{i}\in\mathcal{H}^{\text{exp}}\) and \(\lambda_{j}\in\mathcal{H}^{\text{sc}}\), where \(i\in[N^{\text{exp}}]\) and \(j\in[N^{\text{sc}}]\), we define corresponding groups of base learners for optimizing exp-concave and strongly convex functions. Specifically, for _\(\alpha\)-exp-concave_ functions, we define a group of base learners \(\{\mathcal{B}_{t}^{\text{exp}}\}_{i\in[N^{\text{exp}}]}\), where the \(i\)-th base learner runs the algorithm below:

\[\begin{split}\mathbf{x}_{t,i}&=\operatorname*{arg \,min}_{\mathbf{x}\in\mathcal{X}}\left\{\langle\nabla h_{t-1,i}^{\text{exp}}( \mathbf{x}_{t-1,i}),\mathbf{x}\rangle+\mathcal{D}_{\psi_{t,i}}(\mathbf{x}, \widehat{\mathbf{x}}_{t,i})\right\},\\ \widehat{\mathbf{x}}_{t+1,i}&=\operatorname*{arg\, min}_{\mathbf{x}\in\mathcal{X}}\left\{\langle\nabla h_{t,i}^{\text{exp}}( \mathbf{x}_{t,i}),\mathbf{x}\rangle+\mathcal{D}_{\psi_{t,i}}(\mathbf{x}, \widehat{\mathbf{x}}_{t,i})\right\},\end{split}\] (B.1)

where \(\psi_{t,i}(\mathbf{x})\triangleq\frac{1}{2}\mathbf{x}^{\top}U_{t,i}\mathbf{x}\), \(U_{t,i}=(1+\frac{\alpha_{i}G^{2}}{2})I+\frac{\alpha_{i}}{2}\sum_{s=1}^{t-1} \nabla h_{s,i}^{\text{exp}}(\mathbf{x}_{s,i})h_{s,i}^{\text{exp}}(\mathbf{x}_{ s,i})^{\top}\), \(\alpha_{i}\) is the \(i\)-th element in \(\mathcal{H}^{\text{exp}}\), and \(h_{t,i}^{\text{exp}}(\cdot)\) is a surrogate loss function for \(\mathcal{B}_{t}^{\text{exp}}\), defined as

\[h_{t,i}^{\text{exp}}(\mathbf{x})\triangleq\langle\nabla f_{t}(\mathbf{x}_{t}), \mathbf{x}\rangle+\frac{\alpha_{i}}{4}\langle\nabla f_{t}(\mathbf{x}_{t}), \mathbf{x}-\mathbf{x}_{t}\rangle^{2}.\]

Similarly, for _\(\lambda\)-strongly convex_ functions, we define a group of base learners \(\{\mathcal{B}_{t}^{\text{sc}}\}_{i\in[N^{\text{sc}}]}\), where the \(i\)-th base learner runs the algorithm below:

\[\mathbf{x}_{t,i}=\Pi_{\mathcal{X}}[\widehat{\mathbf{x}}_{t,i}-\eta_{t,i} \nabla h_{t-1,i}^{\text{sc}}(\mathbf{x}_{t-1,i})],\quad\widehat{\mathbf{x}}_{ t+1,i}=\Pi_{\mathcal{X}}[\widehat{\mathbf{x}}_{t,i}-\eta_{t,i}\nabla h_{t,i}^{ \text{sc}}(\mathbf{x}_{t,i})],\] (B.2)

where \(\eta_{t,i}=2/(1+\lambda_{i}t)\), \(\lambda_{i}\) is the \(i\)-th element in \(\mathcal{H}^{\text{sc}}\), and \(h_{t,i}^{\text{sc}}(\cdot)\) is a surrogate loss function for \(\mathcal{B}_{i}^{\text{sc}}\), defined as

\[h_{t,i}^{\text{sc}}(\mathbf{x})\triangleq\langle\nabla f_{t}(\mathbf{x}_{t}), \mathbf{x}\rangle+\frac{\lambda_{i}}{4}\|\mathbf{x}-\mathbf{x}_{t}\|^{2}.\]

For _convex_ functions, we only have to define one base learner \(\mathcal{B}^{\text{sc}}\), which updates as

\[\mathbf{x}_{t,i}=\Pi_{\mathcal{X}}[\widehat{\mathbf{x}}_{t,i}-\eta_{t,i} \nabla f_{t-1}(\mathbf{x}_{t-1})],\quad\widehat{\mathbf{x}}_{t+1,i}=\Pi_{ \mathcal{X}}[\widehat{\mathbf{x}}_{t,i}-\eta_{t,i}\nabla f_{t}(\mathbf{x}_{t} )],\] (B.3)

where \(\eta_{t,i}=\min\{D/\sqrt{1+\sum_{s=2}^{t-1}\|\nabla f_{t}(\mathbf{x}_{t})- \nabla f_{t-1}(\mathbf{x}_{t-1})\|^{2}},1\}\). Finally, we conclude the configurations of base learners. Specifically, we deploy

\[\{\mathcal{B}_{i}\}_{i\in[N]}\triangleq\{\mathcal{B}_{i}^{\text{ exp}}\}_{i\in[N^{\text{exp}}]}\cup\{\mathcal{B}_{i}^{\text{sc}}\}_{i\in[N^{ \text{exp}}]}\cup\{\mathcal{B}^{\text{sc}}\},\text{ where }N\triangleq N^{\text{exp}}+N^{\text{sc}}+1,\] (B.4)

as the total set of base learners.

Meta Learner.The meta learner simply runs Optimistic-Adapt-ML-Prod (Wei et al., 2016), which updates as follows:

\[p_{t+1,i}\propto\varepsilon_{t,i}\cdot\exp(\varepsilon_{t,i}m_{t+1,i})\cdot W_{ t,i},\] (B.5)

where \(\ell_{t,i}\triangleq\langle\nabla f_{t}(\mathbf{x}_{t}),\mathbf{x}_{t,i}\rangle\) is the loss of the \(i\)-th dimension, \(r_{t,i}=\langle\bm{\ell}_{t},\bm{p}_{t}\rangle-\ell_{t,i}\) represents the instantaneous regret, \(m_{t,i}=\langle\nabla f_{t-1}(\mathbf{x}_{t-1}),\mathbf{x}_{t}-\mathbf{x}_{t,i}\rangle\) for the index \(i\) indicating \(\mathcal{B}^{\text{sc}}\) and \(m_{t,i}=0\) for indexes indicating \(\mathcal{B}^{\text{exp}}\) and \(\mathcal{B}^{\text{sc}}\). The learning rate \(\varepsilon_{t,i}\) is chosen as

\[\varepsilon_{t,i}=\min\left\{\frac{1}{8},\sqrt{\frac{\ln N}{\sum_{s\in[t]}(r_{ s,i}-m_{s,i})^{2}}}\right\}.\] (B.6)

## Appendix C Proof for Section 3

In this section, we provide the proof of Theorem 1, our main theoretical result for the optimal universal gradient-variation regret.

Proof.: We first give different decompositions of the regret for different curvature types, then analyze the meta and base regret, and finally combine them to achieve the regret bound. For simplicity, we define \(\mathbf{g}_{t}\triangleq\nabla f_{t}(\mathbf{x}_{t})\) and

\[\bar{V}_{T}\triangleq\sum_{t=2}^{T}\|\nabla f_{t}(\mathbf{x}_{t})-\nabla f_{t- 1}(\mathbf{x}_{t-1})\|^{2}\]for short. Using the analysis in Section 3.2, the empirical gradient variation can bounded as

\[\bar{V}_{T}\leq 3\sum_{t=2}^{T}\|\nabla f_{t}(\mathbf{x}_{t})-\nabla f_{t}( \mathbf{x}^{\star})\|^{2}+3\sum_{t=2}^{T}\|\nabla f_{t}(\mathbf{x}^{\star})- \nabla f_{t-1}(\mathbf{x}^{\star})\|^{2}\] \[+3\sum_{t=2}^{T}\|\nabla f_{t-1}(\mathbf{x}^{\star})-\nabla f_{t- 1}(\mathbf{x}_{t-1})\|^{2}\leq 6L\sum_{t=2}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{ \star},\mathbf{x}_{t})+3V_{T}+6L\sum_{t=2}^{T}\mathcal{D}_{f_{t-1}}(\mathbf{x} ^{\star},\mathbf{x}_{t-1})\] \[\leq 3V_{T}+12L\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{\star },\mathbf{x}_{t}).\] (C.1)

Regret Decomposition.Denoting by \(\mathbf{x}^{\star}\in\arg\min_{\mathbf{x}\in\mathcal{X}}\sum_{t\in[T]}f_{t}( \mathbf{x})\), for _convex_ functions, we decompose the regret as

\[\textsc{Reg}_{T} = \sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}^{ \star}\rangle-\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{\star},\mathbf{x}_ {t})\] (by ( 3.3 )) \[= \underbrace{\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}- \mathbf{x}_{t,i^{\star}}\rangle}_{\textsc{Meta-Reg}}+\underbrace{\sum_{t=1}^{ T}h_{t,i^{\star}}^{\mathrm{c}}(\mathbf{x}_{t,i^{\star}})-\sum_{t=1}^{T}h_{t,i^{ \star}}^{\mathrm{c}}(\mathbf{x}^{\star})}_{\textsc{Base-Reg}}-\sum_{t=1}^{T} \mathcal{D}_{f_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t}),\] (C.2)

where \(h_{t,i}^{\mathrm{c}}(\mathbf{x})\triangleq\langle\mathbf{g}_{t},\mathbf{x}\rangle\).

For _\(\alpha\)-exp-concave_ functions, we decompose the regret as

\[\textsc{Reg}_{T} = \sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}^{ \star}\rangle-\frac{1}{2}\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})-\frac{1}{2}\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{ \star},\mathbf{x}_{t})\] (by ( 3.3 )) \[\leq \sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}^{ \star}\rangle-\frac{\alpha}{4}\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t }-\mathbf{x}^{\star}\rangle^{2}-\frac{1}{2}\sum_{t=1}^{T}\mathcal{D}_{f_{t}}( \mathbf{x}^{\star},\mathbf{x}_{t})\] \[\leq \underbrace{\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}- \mathbf{x}_{t,i^{\star}}\rangle-\frac{\alpha_{i^{\star}}}{4}\sum_{t=1}^{T} \langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}_{t,i^{\star}}\rangle^{2}}_{ \textsc{Meta-Reg}}\qquad\qquad\text{(by $\alpha_{i^{\star}}\leq\alpha\leq 2 \alpha_{i^{\star}}$)}\] \[\qquad\qquad\qquad+\underbrace{\sum_{t=1}^{T}h_{t,i^{\star}}^{ \mathrm{exp}}(\mathbf{x}_{t,i^{\star}})-\sum_{t=1}^{T}h_{t,i^{\star}}^{ \mathrm{exp}}(\mathbf{x}^{\star})}_{\textsc{Base-Reg}}-\frac{1}{2}\sum_{t=1}^{ T}\mathcal{D}_{f_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t}),\] (C.3)

where the second step is due to the definitions of exp-concavity and Bregman divergence and the last step is due to the definition of the surrogate function \(h_{t,i}^{\mathrm{exp}}(\mathbf{x})\triangleq\langle\mathbf{g}_{t},\mathbf{x} \rangle+\frac{\alpha_{i}}{4}\langle\nabla f_{t}(\mathbf{x}_{t}),\mathbf{x}- \mathbf{x}_{t}\rangle^{2}\), where \(\alpha_{i}\in\mathcal{H}\), defined in (2.1).

For _\(\lambda\)-strongly convex_ functions, following the similar decomposition in the exp-concavity case,

\[\textsc{Reg}_{T} \leq \underbrace{\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}- \mathbf{x}_{t,i^{\star}}\rangle-\frac{\lambda_{i^{\star}}}{4}\sum_{t=1}^{T} \|\mathbf{x}_{t}-\mathbf{x}_{t,i^{\star}}\|^{2}}_{\textsc{Meta-Reg}}\] (by \[\lambda_{i^{\star}}\leq\lambda\leq 2\lambda_{i^{\star}}\] ) \[\qquad\qquad+\underbrace{\sum_{t=1}^{T}h_{t,i^{\star}}^{\mathrm{ sc}}(\mathbf{x}_{t,i^{\star}})-\sum_{t=1}^{T}h_{t,i^{\star}}^{\mathrm{sc}}( \mathbf{x}^{\star})}_{\textsc{Base-Reg}}-\frac{1}{2}\sum_{t=1}^{T}\mathcal{D} _{f_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t}),\] (C.4)

due to the definition of the surrogate \(h_{t,i}^{\mathrm{sc}}(\mathbf{x})\triangleq\langle\mathbf{g}_{t},\mathbf{x} \rangle+\frac{\lambda_{i}}{4}\|\mathbf{x}-\mathbf{x}_{t}\|^{2}\), where \(\lambda_{i}\in\mathcal{H}\) in (2.1).

Meta Regret Analysis.We adopt Optimistic-Adapt-ML-Prod (Wei et al., 2016) as the meta learner, and present its regret analysis below for self-containedness.

**Lemma 2** (Theorem 3.4 of Wei et al. [2016]).: _Denoting by \(\bm{p}_{t}\) the weights of the algorithm, \(\bm{\ell}_{t}\) the loss vector, and \(m_{t,i}\) the optimism, by choosing the learning rate optimally as (B.6), the regret of Optimistic-Adapt-ML-Prod (B.5) with respect to any expert \(i\in[N]\) satisfies_

\[\sum_{t=1}^{T}\langle\bm{\ell}_{t},\bm{p}_{t}-\bm{e}_{i}\rangle\leq C_{0}\sqrt {1+\sum_{t=1}^{T}(r_{t,i}-m_{t,i})^{2}}+C_{1},\]

_where \(r_{t,i}=\langle\bm{\ell}_{t},\bm{p}_{t}-\bm{e}_{i}\rangle\), \(\bm{e}_{i}\) denotes the \(i\)-th standard basis vector, \(C_{0}=\sqrt{\ln N}+\ln(1+\frac{N}{e}(1+\ln(T+1)))/\sqrt{\ln N}\), and \(C_{1}=\frac{1}{4}(\ln N+\ln(1+\frac{N}{e}(1+\ln(T+1))))+2\sqrt{\ln N}+16\ln N\)._

Here we adopt \(\ell_{t,i}=\langle\bm{g}_{t},\bm{x}_{t,i}\rangle\) such that \(\langle\bm{\ell}_{t},\bm{p}_{t}-\bm{e}_{i}\rangle=\langle\bm{g}_{t},\bm{x}_{t }-\bm{x}_{t,i}\rangle\). Besides, since the number of base learners \(N=\mathcal{O}(\log T)\) as explained in Section 2, the constants \(C_{0}\) and \(C_{1}\) are in the order of \(\mathcal{O}(\log\log T)\) and can be treated as ignorable constants, following previous convention [Luo and Schapire, 2015, Gaillard et al., 2014].

For _convex_ functions, we choose the optimism as \(m_{t,i}=\langle\bm{g}_{t-1},\bm{x}_{t}-\bm{x}_{t,i}\rangle\) for the index \(i\) indicating the convex base learner. As explained in Section 3.1, although \(\bm{x}_{t}\) is unknown for now, we only require the scalar value of \(\langle\bm{g}_{t-1},\bm{x}_{t}\rangle\). Denoting by \(z=\langle\bm{g}_{t-1},\bm{x}_{t}\rangle\), it actually forms a fixed-point problem of \(z=\langle\bm{g}_{t-1},\bm{x}_{t}(z)\rangle\), where \(\bm{x}_{t}\) is a function of \(z\) since \(\bm{x}_{t}\) depends on \(p_{t,i}\), \(p_{t,i}\) relies on \(m_{t,i}\), and \(m_{t,i}\) depends on \(z\). Such a one-dimensional fixed-point problem can be solved with an \(\mathcal{O}(1/T)\) approximation error through \(\mathcal{O}(\log T)\) binary searches, and aggregating the approximate error over the whole time horizon will only incur an additive constant to the final regret. As a result, such an optimism setup is valid. Consequently, the meta regret in (C.2) can be bounded as

\[\text{Meta-Reg}\leq C_{0}\sqrt{1+\sum_{t=1}^{T}\langle\bm{g}_{t} -\bm{g}_{t-1},\bm{x}_{t}-\bm{x}_{t,i^{*}}\rangle^{2}}+C_{1}\] (by Lemma 2 ) \[\leq C_{0}\sqrt{1+D^{2}\bar{V}_{T}}+C_{1}\leq C_{0}\sqrt{1+3D^{2 }V_{T}+12LD^{2}\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\bm{x}^{\star},\bm{x}_{t})}+ C_{1}\] (by (C.1)) \[\leq\mathcal{O}(\sqrt{V_{T}})+C_{0}\sqrt{12LD^{2}\sum_{t=1}^{T} \mathcal{D}_{f_{t}}(\bm{x}^{\star},\bm{x}_{t})}\leq\mathcal{O}(\sqrt{V_{T}})+ \mathcal{O}(C_{2})+\frac{C_{0}}{2C_{2}}\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\bm{ x}^{\star},\bm{x}_{t}),\]

where the second step adopts Assumption 1, the fourth step uses \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b}\) for any \(a,b\geq 0\), the last step uses AM-GM inequality: \(\sqrt{ab}\leq\frac{a\pi}{2}+\frac{b}{2x}\) for any \(a,b,x>0\). Note that \(C_{2}\) is used to ensure the positive Bregman divergence term to be canceled and will be specified in the end.

For _exp-concave_ functions, we choose the optimism as \(m_{t,i}=0\) for indexes \(i\) indicating exp-concave base learners. By Lemma 2, the meta regret in (C.3) can be bounded as

\[\text{Meta-Reg} \leq C_{0}\sqrt{1+\sum_{t=1}^{T}\langle\bm{g}_{t},\bm{x}_{t}-\bm{x}_{t,i^{*}}\rangle^{2}}-\frac{\alpha_{i^{*}}}{4}\sum_{t=1}^{T}\langle\bm{g}_{t}, \bm{x}_{t}-\bm{x}_{t,i^{*}}\rangle^{2}+C_{1}\] \[\leq\mathcal{O}(C_{3})+\left(\frac{C_{0}}{2C_{3}}-\frac{\alpha_{ i^{*}}}{4}\right)\sum_{t=1}^{T}\langle\bm{g}_{t},\bm{x}_{t}-\bm{x}_{t,i^{*}} \rangle^{2},\] (C.5)

where the last step omits the ignorable additive \(C_{0}\) or \(C_{1}\) terms and is due to AM-GM inequality. \(C_{2}\) is a constant to be specified.

For _strongly convex_ functions, we choose the optimism \(m_{t,i}=0\) for indexes \(i\) indicating strongly convex base learners. By Lemma 2, the meta regret in (C.4) can be bounded as

\[\text{Meta-Reg}\leq C_{0}\sqrt{1+\sum_{t=1}^{T}\langle\bm{g}_{t},\bm{x}_{t}-\bm{x}_{t,i^ {*}}\rangle^{2}}-\frac{\lambda_{i^{*}}}{4}\sum_{t=1}^{T}\|\bm{x}_{t}-\bm{x}_{t, i^{*}}\|^{2}+C_{1}\] (by Lemma 2 ) \[\leq C_{0}\sqrt{1+D^{2}\sum_{t=1}^{T}\|\bm{x}_{t}-\bm{x}_{t,i^{*} }\|^{2}}-\frac{\lambda_{i^{*}}}{4}\sum_{t=1}^{T}\|\bm{x}_{t}-\bm{x}_{t,i^{*}}\|^ {2}+C_{1}\] (by Assumption 1 )\[\leq\mathcal{O}(C_{4})+\left(\frac{C_{0}D^{2}}{2C_{4}}-\frac{\lambda_{i^{*}}}{4} \right)\sum_{t=1}^{T}\|\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\|^{2},\] (C.6)

where the last step omits the ignorable additive \(C_{0}\) or \(C_{1}\) terms and is due to AM-GM inequality. \(C_{4}\) is a constant to be specified.

Base Regret Analysis.For _convex_ functions, when using the update rule (B.3), due to the standard analysis of OOMD for convex functions (e.g., Lemma 10 of Yan et al. (2023)), it holds that

\[\text{Base-Reg}\leq 5D\sqrt{1+\sum_{t=2}^{T}\|\nabla h_{t,i^{*}}^{ \mathrm{c}}(\mathbf{x}_{t,i^{*}})-\nabla h_{t-1,i^{*}}^{\mathrm{c}}(\mathbf{x }_{t-1,i^{*}})\|^{2}}+\mathcal{O}(1)\] \[=5D\sqrt{1+\bar{V}_{T}}+\mathcal{O}(1)\leq\mathcal{O}(\sqrt{V_{T }})+5D\sqrt{12L\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{\star},\mathbf{x }_{t})}\] (by (C.1)) \[\leq\mathcal{O}(\sqrt{V_{T}})+\mathcal{O}(C_{5})+\frac{5D}{2C_{5} }\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t}),\]

where the second step is due to the property of the surrogate function: \(\nabla h_{t,i}^{\mathrm{c}}(\mathbf{x}_{t,i})=\mathbf{g}_{t}\), and the last step uses AM-GM inequality. \(C_{5}\) is a constant to be specified.

For _exp-concave_ functions, when using the update rule (B.1), due to the standard analysis of OOMD for exp-concave functions (e.g., Lemma 11 of Yan et al. (2023)), the base regret can be bounded as

\[\text{Base-Reg}\leq\frac{16d}{\alpha_{i^{*}}}\ln\left(1+\frac{\alpha_{i^{*}}} {8d}\sum_{t=2}^{T}\left\|\nabla h_{t,i^{*}}^{\mathrm{exp}}(\mathbf{x}_{t,i^{* }})-\nabla h_{t-1,i^{*}}^{\mathrm{exp}}(\mathbf{x}_{t-1,i^{*}})\right\|^{2} \right)+\mathcal{O}(1).\] (C.7)

Next, we analyze the empirical gradient variation defined on the surrogate function \(h_{t,i}^{\mathrm{exp}}(\cdot)\):

\[\sum_{t=2}^{T}\left\|\nabla h_{t,i^{*}}^{\mathrm{exp}}(\mathbf{x }_{t,i^{*}})-\nabla h_{t-1,i^{*}}^{\mathrm{exp}}(\mathbf{x}_{t-1,i^{*}})\right\| ^{2}\] \[= \sum_{t=2}^{T}\left\|\mathbf{g}_{t}+\frac{\alpha_{i^{*}}}{2} \mathbf{g}_{t}\langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\rangle -\mathbf{g}_{t-1}-\frac{\alpha_{i^{*}}}{2}\mathbf{g}_{t-1}\langle\mathbf{g}_ {t-1},\mathbf{x}_{t-1}-\mathbf{x}_{t-1,i^{*}}\rangle\right\|^{2}\] \[\leq 3\bar{V}_{T}+3\sum_{t=2}^{T}\left\|\frac{\alpha_{i^{*}}}{2} \mathbf{g}_{t}\langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\rangle \right\|^{2}+3\sum_{t=2}^{T}\left\|\frac{\alpha_{i^{*}}}{2}\mathbf{g}_{t-1} \langle\mathbf{g}_{t-1},\mathbf{x}_{t-1}-\mathbf{x}_{t-1,i^{*}}\rangle\right\| ^{2}\] \[\leq 3\bar{V}_{T}+6\sum_{t=1}^{T}\left\|\frac{\alpha_{i^{*}}}{2} \mathbf{g}_{t}\langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\rangle \right\|^{2}\] (C.8) \[\leq 9V_{T}+36L\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})+2\alpha_{i^{*}}^{2}G^{2}\sum_{t=1}^{T}\langle\mathbf{g}_{t}, \mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\rangle^{2},\] (by (C.1) and Assumption 1)

where the first step is due to the property of the surrogate function: \(\nabla h_{t,i}^{\mathrm{exp}}(\mathbf{x}_{t,i})=\mathbf{g}_{t}+\frac{\alpha_{i} }{2}\mathbf{g}_{t}\langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}_{t,i}\rangle\), the second step is by the Cauchy-Schwarz inequality: \((a+b+c)^{2}\leq 3(a^{2}+b^{2}+c^{2})\) for any \(a,b,c\in\mathbb{R}\). Plugging the surrogate's empirical gradient variation back to the base regret, we obtain

\[\text{Base-Reg} \leq\frac{16d}{\alpha_{i^{*}}}\ln\left(1+\frac{9\alpha_{i^{*}}}{8d }V_{T}+\frac{9\alpha_{i^{*}}L}{2d}\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^ {\star},\mathbf{x}_{t})+\frac{\alpha_{i^{*}}^{3}G^{2}}{4d}\sum_{t=1}^{T} \langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\rangle^{2}\right)\] \[\leq\mathcal{O}\left(\frac{d}{\alpha}\ln(C_{6}V_{T})\right)+\frac {16d}{C_{6}\alpha_{i^{*}}}\left(\frac{9\alpha_{i^{*}}L}{2d}\sum_{t=1}^{T} \mathcal{D}_{f_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})+\frac{\alpha_{i^{*}}^{3} G^{2}}{4d}\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}} \rangle^{2}\right)\] \[\leq\mathcal{O}\left(\frac{d}{\alpha}\ln V_{T}\right)+\frac{72L}{ C_{6}}\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})+\frac{4G^{2}}{C_{6}} \sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\rangle^ {2}+\mathcal{O}(\ln C_{6}).\]The second step requires \(C_{6}\geq 1\) by Lemma5 and uses the property of the best base learner, i.e., \(\alpha_{i^{*}}\leq\alpha\leq 2\alpha_{i^{*}}\). The last step is because of \(\alpha_{i}\leq 1\).

For _strongly convex_ functions, when using the update rule (B.2), due to the analysis of OOMD for strongly convex functions (e.g., Lemma12 of Yan et al. (2023)), the base regret can be bounded as

\[\textsc{Base-Reg}\leq\frac{16G^{2}}{\lambda_{i^{*}}}\ln\left(1+ \lambda_{i^{*}}\sum_{t=2}^{T}\left\|\nabla h^{\text{sc}}_{t,i^{*}}(\mathbf{x}_ {t,i^{*}})-\nabla h^{\text{sc}}_{t-1,i^{*}}(\mathbf{x}_{t-1,i^{*}})\right\|^{2 }\right)+\mathcal{O}(1).\] (C.9)

Next, we analyze the empirical gradient variation defined on the surrogate function \(h^{\text{sc}}_{t,i}(\cdot)\):

\[\sum_{t=2}^{T}\left\|\nabla h^{\text{sc}}_{t,i^{*}}(\mathbf{x}_ {t,i^{*}})-\nabla h^{\text{sc}}_{t-1,i^{*}}(\mathbf{x}_{t-1,i^{*}})\right\|^{2}\] \[=\sum_{t=2}^{T}\left\|\mathbf{g}_{t}+\frac{\lambda_{i^{*}}}{2}( \mathbf{x}_{t,i^{*}}-\mathbf{x}_{t})-\mathbf{g}_{t-1}-\frac{\lambda_{i^{*}}}{ 2}(\mathbf{x}_{t-1,i^{*}}-\mathbf{x}_{t-1})\right\|^{2}\] \[\leq 3\bar{V}_{T}+3\sum_{t=2}^{T}\left\|\frac{\lambda_{i^{*}}}{2} (\mathbf{x}_{t,i^{*}}-\mathbf{x}_{t})\right\|^{2}+3\sum_{t=2}^{T}\left\|\frac {\lambda_{i^{*}}}{2}(\mathbf{x}_{t-1,i^{*}}-\mathbf{x}_{t-1})\right\|^{2}\] (C.10) \[\leq 9V_{T}+36L\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{*}, \mathbf{x}_{t})+2\lambda_{i^{*}}^{2}\sum_{t=1}^{T}\left\|\mathbf{x}_{t,i^{*}}- \mathbf{x}_{t}\right\|^{2},\] (by (C.1))

where the first step is due to the property of the surrogate: \(\nabla h^{\text{sc}}_{t,i}(\mathbf{x}_{t,i})=\mathbf{g}_{t}+\frac{\lambda_{i} }{2}(\mathbf{x}_{t,i}-\mathbf{x}_{t})\), and the second step is due to the Cauchy-Schwarz inequality. Plugging the surrogate's empirical gradient variation back to the base regret, we obtain

\[\textsc{Base-Reg} \leq\frac{16G^{2}}{\lambda_{i^{*}}}\ln\left(1+9\lambda_{i^{*}}V_{ T}+36L\lambda_{i^{*}}\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{*}, \mathbf{x}_{t})+2\lambda_{i^{*}}^{3}\sum_{t=1}^{T}\left\|\mathbf{x}_{t,i^{*}}- \mathbf{x}_{t}\right\|^{2}\right)\] \[\leq\mathcal{O}\left(\frac{1}{\lambda}\ln(C_{7}V_{T})\right)+ \frac{16G^{2}}{C_{7}\lambda_{i^{*}}}\left(36L\lambda_{i^{*}}\sum_{t=1}^{T} \mathcal{D}_{f_{t}}(\mathbf{x}^{*},\mathbf{x}_{t})+2\lambda_{i^{*}}^{3}\sum_{t =1}^{T}\left\|\mathbf{x}_{t,i^{*}}-\mathbf{x}_{t}\right\|^{2}\right)\] \[\leq\mathcal{O}\left(\frac{1}{\lambda}\ln V_{T}\right)+\frac{576G ^{2}L}{C_{7}}\sum_{t=1}^{T}\mathcal{D}_{f_{t}}(\mathbf{x}^{*},\mathbf{x}_{t}) +\frac{32G^{2}}{C_{7}}\sum_{t=1}^{T}\left\|\mathbf{x}_{t,i^{*}}-\mathbf{x}_{t} \right\|^{2}+\mathcal{O}(\ln C_{7}),\]

where the second step requires \(C_{7}\geq 1\) by Lemma5 and uses the property of the best base learner, i.e., \(\lambda_{i^{*}}\leq\lambda\leq 2\lambda_{i^{*}}\). The last step is due to \(\lambda_{i}\leq 1\).

Regret Analysis.For _convex_ functions, by combining the meta and base regret, it holds that

\[\textsc{Reg}_{T}\leq\mathcal{O}(\sqrt{V_{T}})+\mathcal{O}(C_{2}+C_{5})+\left( \frac{C_{0}}{2C_{2}}+\frac{5D}{2C_{5}}-1\right)\sum_{t=1}^{T}\mathcal{D}_{f_{t }}(\mathbf{x}^{*},\mathbf{x}_{t})\leq\mathcal{O}(\sqrt{V_{T}}),\]

by choosing \(C_{2}=C_{0}\) and \(C_{5}=5D\).

For _exp-concave_ functions, by combining the meta and base regret, it holds that

\[\textsc{Reg}_{T} \leq\mathcal{O}\left(\frac{d}{\alpha}\ln V_{T}\right)+\mathcal{O }(C_{3}+\ln C_{6})+\left(\frac{C_{0}}{2C_{3}}+\frac{4G^{2}}{C_{6}}-\frac{ \alpha_{i^{*}}}{4}\right)\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}- \mathbf{x}_{t,i^{*}}\rangle^{2}\] \[\quad+\left(\frac{72L}{C_{6}}-\frac{1}{2}\right)\sum_{t=1}^{T} \mathcal{D}_{f_{t}}(\mathbf{x}^{*},\mathbf{x}_{t})\leq\mathcal{O}\left(\frac{d} {\alpha}\ln V_{T}\right),\]

by choosing \(C_{6}=\max\{1,144L,\frac{32G^{2}}{\alpha_{i^{*}}}\}\) and \(C_{3}=\frac{4C_{0}}{\alpha_{i^{*}}}\). Note that such a parameter configuration will only add an \(\mathcal{O}(1/\alpha)\) factor to the final regret bound, which can be absorbed.

For _strongly convex_ functions, by combining the meta and base regret, it holds that

\[\textsc{Reg}_{T}\leq\mathcal{O}\left(\frac{1}{\lambda}\ln V_{T}\right)+ \mathcal{O}(C_{4}+\ln C_{7})+\left(\frac{C_{0}D^{2}}{2C_{4}}+\frac{32G^{2}}{C_{ 7}}-\frac{\lambda_{i^{*}}}{4}\right)\sum_{t=1}^{T}\left\|\mathbf{x}_{t}- \mathbf{x}_{t,i^{*}}\right\|^{2}\]\[\leq C_{0}\sqrt{1+D^{2}\bar{V}_{T}}+C_{1}\leq C_{0}\sqrt{1+16D^{2}L \bar{F}_{T}}+C_{1}.\hskip 28.452756pt\text{(by Assumption \ref{Assumption:1} and (D.1))}\]

For _exp-concave_ functions, we choose the optimism as \(m_{t,i}=0\) for indexes \(i\) indicating the exp-concave base learners. The meta regret is bounded in the same way as (C.5).

For _strongly convex_ functions, we choose the optimism as \(m_{t,i}=0\) for indexes \(i\) indicating the strongly convex base learners. The meta regret is bounded in the same way as (C.6).

Base Regret Analysis.For _convex_ functions, using the same base algorithms as in the proof of Theorem 1, the base regret can be bounded as

\[\text{Base-Reg}\leq 5D\sqrt{1+\bar{V}_{T}}+\mathcal{O}(1)\leq 5D\sqrt{1+16 L\bar{F}_{T}}+\mathcal{O}(1).\]

For _exp-concave_ functions, using the same base algorithms as in the proof of Theorem 1, the base regret can be bounded by (C.7). Following (C.8), the empirical gradient variation defined on the surrogate function \(h_{t,i}^{\text{exp}}(\cdot)\) can be bounded as

\[\leq 48L\bar{F}_{T}+2\alpha_{i}^{2}.G^{2}\sum_{t=1}^{T}\langle \mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\rangle^{2}.\hskip 113.811024pt \text{(by Assumption \ref{Assumption:1} and (D.1))}\]Plugging the surrogate's empirical gradient variation back to the base regret, we obtain

\[\text{Base-Reg}\leq\frac{16d}{\alpha_{i^{*}}}\ln\left(1+\frac{6L \alpha_{i^{*}}}{d}\bar{F}_{T}+\frac{\alpha_{i^{*}}^{3}G^{2}}{4d}\sum_{t=1}^{T} \langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\rangle^{2}\right)+ \mathcal{O}(1)\] \[\leq\frac{16d}{\alpha_{i^{*}}}\ln\left(C_{8}\left(1+\frac{6L \alpha_{i^{*}}}{d}\bar{F}_{T}\right)\right)+\frac{16d}{C_{8}\alpha_{i^{*}}} \left(\frac{\alpha_{i^{*}}^{3}G^{2}}{4d}\sum_{t=1}^{T}\langle\mathbf{g}_{t}, \mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\rangle^{2}\right)\] \[\leq\frac{32d}{\alpha}\ln\left(1+\frac{6L}{d}\bar{F}_{T}\right)+ \frac{4G^{2}}{C_{8}}\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}- \mathbf{x}_{t,i^{*}}\rangle^{2}+\mathcal{O}(\ln C_{8}),\]

where the second step requires \(C_{8}\geq 1\) by Lemma5 and uses the property of the best base learner, i.e., \(\alpha_{i^{*}}\leq\alpha\leq 2\alpha_{i^{*}}\). The last step is due to \(\alpha_{i}\leq 1\).

For _strongly convex_ functions, using the same base algorithms as in the proof of Theorem1, the base regret can be bounded by (C.9). Following (C.10), the empirical gradient variation defined on the surrogate function \(h_{t,i}^{\text{sc}}(\cdot)\) can be bounded as

\[\sum_{t=2}^{T}\left\|\nabla h_{t,i^{*}}^{\text{sc}}(\mathbf{x}_{t,i^{*}})-\nabla h_{t-1,i^{*}}^{\text{sc}}(\mathbf{x}_{t-1,i^{*}})\right\|^{2} \leq 3\bar{V}_{T}+6\sum_{t=1}^{T}\left\|\frac{\lambda_{i^{*}}}{2}(\mathbf{ x}_{t,i^{*}}-\mathbf{x}_{t})\right\|^{2}\] \[\leq 48L\bar{F}_{T}+2\lambda_{i^{*}}^{2}\sum_{t=1}^{T}\|\mathbf{x}_ {t,i^{*}}-\mathbf{x}_{t}\|^{2}.\] (by (D.1))

Plugging the surrogate's empirical gradient variation back to the base regret, we obtain

\[\text{Base-Reg}\leq\frac{16G^{2}}{\lambda_{i^{*}}}\ln\left(1+48 L\lambda_{i^{*}}\bar{F}_{T}+2\lambda_{i^{*}}^{3}\sum_{t=1}^{T}\|\mathbf{x}_{t,i^{*} }-\mathbf{x}_{t}\|^{2}\right)+\mathcal{O}(1)\] \[\leq\frac{16G^{2}}{\lambda_{i^{*}}}\ln\left(C_{9}\left(1+48L \lambda_{i^{*}}\bar{F}_{T}\right)\right)+\frac{16G^{2}}{C_{9}\lambda_{i^{*}}} \left(2\lambda_{i^{*}}^{3}\sum_{t=1}^{T}\|\mathbf{x}_{t,i^{*}}-\mathbf{x}_{t} \|^{2}\right)\] \[\leq\frac{32G^{2}}{\lambda}\ln(1+48L\bar{F}_{T})+\frac{32G^{2}}{C _{9}}\sum_{t=1}^{T}\|\mathbf{x}_{t,i^{*}}-\mathbf{x}_{t}\|^{2}+\mathcal{O}(\ln C _{9}),\]

where the second step requires \(C_{9}\geq 1\) by Lemma5 and uses the property of the best base learner, i.e., \(\lambda_{i^{*}}\leq\lambda\leq 2\lambda_{i^{*}}\). The last step is due to \(\lambda_{i}\leq 1\).

Regret Analysis.For _convex_ functions, by combining the meta and base regret, it holds that

\[\text{Reg}_{T}\leq C_{0}\sqrt{1+16D^{2}L\bar{F}_{T}}+5D\sqrt{1+1 6L\bar{F}_{T}}+C_{1}\leq\mathcal{O}(\sqrt{F_{T}}),\]

where the last step is due to Lemma9 of Zhao et al. (2024), restated below for self-containedness.

**Lemma 3** (Lemma 9 of Zhao et al. (2024)).: _For any \(x,y,a,b>0\) satisfying \(x-y\leq\sqrt{ax}+b\), it holds that \(x-y\leq\sqrt{ay+ab}+a+b\)._

For _exp-concave_ functions, by combining the meta and base regret, it holds that

\[\text{Reg}_{T} \leq\left(\frac{C_{0}}{2C_{3}}+\frac{4G^{2}}{C_{8}}-\frac{\alpha_ {i^{*}}}{4}\right)\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{ x}_{t,i^{*}}\rangle^{2}+\frac{32d}{\alpha}\ln\left(1+\frac{6L}{d}\bar{F}_{T} \right)+\mathcal{O}(C_{3}+\ln C_{8})\] \[\leq\frac{32d}{\alpha}\ln\left(1+\frac{6L}{d}\bar{F}_{T}\right) +\mathcal{O}(1)\leq\mathcal{O}\left(\frac{d}{\alpha}\ln F_{T}\right),\] (by Lemma6)

where the second step chooses \(C_{3}=\frac{4C_{0}}{\alpha_{i^{*}}}\) and \(C_{8}=\max\{1,\frac{32G^{2}}{\alpha_{i^{*}}}\}\). Note that such a parameter configuration will only add an \(\mathcal{O}(1/\alpha)\) factor to the final regret bound, which can be absorbed.

For _strongly convex_ functions, by combining the meta and base regret, it holds that

\[\text{Reg}_{T}\leq\left(\frac{C_{0}D^{2}}{2C_{4}}+\frac{32G^{2}}{C_{9}}-\frac{ \lambda_{i^{*}}}{4}\right)\sum_{t=1}^{T}\|\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}} \|^{2}+\frac{32G^{2}}{\lambda}\ln(1+48L\bar{F}_{T})+\mathcal{O}(C_{4}+\ln C_{9})\]\[\leq\frac{32G^{2}}{\lambda}\ln(1+48L\bar{F}_{T})\leq\mathcal{O}\left( \frac{1}{\lambda}\ln F_{T}\right),\] (by Lemma 6)

where the second step is by choosing \(C_{4}=\frac{4C_{0}D^{2}}{\lambda_{i^{*}}}\) and \(C_{9}=\max\{1,\frac{256G^{2}}{\lambda_{i^{*}}}\}\). Note that such a parameter configuration will only add an \(\mathcal{O}(1/\lambda)\) factor to the final regret bound, which can be absorbed. Also note that the constants \(C_{3},C_{4},C_{8},C_{9}\) only exist in analysis and thus can be chosen arbitrarily, finishing the proof. 

### Proof of Theorem 3

Proof.: To begin with, we give a different analysis of the empirical gradient variation:

\[\mathbb{E}[\bar{V}_{T}]\leq 5\mathbb{E}\left[\sum_{t=2}^{T}\| \nabla f_{t}(\mathbf{x}_{t})-\nabla F_{t}(\mathbf{x}_{t})\|^{2}\right]+5\sum_ {t=2}^{T}\|\nabla F_{t}(\mathbf{x}_{t})-\nabla F_{t}(\mathbf{x}^{*})\|^{2}\] \[+5\mathbb{E}\left[\sum_{t=2}^{T}\|\nabla F_{t}(\mathbf{x}^{*})- \nabla F_{t-1}(\mathbf{x}^{*})\|^{2}\right]+5\sum_{t=2}^{T}\|\nabla F_{t-1}( \mathbf{x}^{*})-\nabla F_{t-1}(\mathbf{x}_{t-1})\|^{2}\] \[+5\mathbb{E}\left[\sum_{t=2}^{T}\|\nabla F_{t-1}(\mathbf{x}_{t-1 })-\nabla f_{t-1}(\mathbf{x}_{t-1})\|^{2}\right]\leq 10\sigma_{1:T}^{2}+5 \Sigma_{1:T}^{2}+20L\sum_{t=1}^{T}\mathcal{D}_{F_{t}}(\mathbf{x}^{*}, \mathbf{x}_{t}),\] (D.2)

where the first step is due to Cauchy-Schwarz inequality and the last step is because of the definitions of \(\sigma_{1:T}^{2}\) and \(\Sigma_{1:T}^{2}\) (given in Section 4) and the analysis proposed in Section 3.2.

In the following, we first give regret decompositions for different curvature types, then we analyze the meta and base regret, and combine them for the final regret guarantees.

Regret Decomposition.Denoting by \(\mathbf{x}^{\star}\in\arg\min_{\mathbf{x}\in\mathcal{X}}\sum_{t\in[T]}f_{t}( \mathbf{x})\), for _convex_ functions, we decompose the regret as

\[\mathbb{E}[\textsc{Reg}_{T}] =\mathbb{E}\left[\sum_{t=1}^{T}F_{t}(\mathbf{x}_{t})-\sum_{t=1}^{ T}F_{t}(\mathbf{x}^{\star})\right]=\mathbb{E}\left[\sum_{t=1}^{T}\langle\nabla F_{t} (\mathbf{x}_{t}),\mathbf{x}_{t}-\mathbf{x}^{\star}\rangle\right]-\sum_{t=1}^{ T}\mathcal{D}_{F_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})\] \[=\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}\langle\mathbf{g}_{t },\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\rangle\right]}_{\textsc{Meta-Reg}}+ \underbrace{\mathbb{E}\left[\sum_{t=1}^{T}h_{t,i^{*}}^{\mathrm{c}}(\mathbf{x}_ {t,i^{*}})-h_{t,i^{*}}^{\mathrm{c}}(\mathbf{x}^{\star})\right]}_{\textsc{ Base-Reg}}-\sum_{t=1}^{T}\mathcal{D}_{F_{t}}(\mathbf{x}^{\star}, \mathbf{x}_{t}),\]

where the first and third step use \(F_{t}(\mathbf{x})=\mathbb{E}[f_{t}(\mathbf{x})]\), the second step uses the definition of Bregman divergence, and the fourth step is due to \(h_{t,i}^{\mathrm{c}}(\mathbf{x})\triangleq\langle\mathbf{g}_{t},\mathbf{x}\rangle\).

For _exp-concave_ functions, following the similar decomposition as in the proof of Theorem 1 in Appendix C, we decompose the regret as

\[\mathbb{E}[\textsc{Reg}_{T}] =\mathbb{E}\left[\sum_{t=1}^{T}\langle\nabla F_{t}(\mathbf{x}_{t }),\mathbf{x}_{t}-\mathbf{x}^{\star}\rangle\right]-\frac{1}{2}\sum_{t=1}^{T} \mathcal{D}_{F_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})-\frac{1}{2}\sum_{t=1}^{ T}\mathcal{D}_{F_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})\] \[=\mathbb{E}\left[\sum_{t=1}^{T}\langle\nabla f_{t}(\mathbf{x}_{t }),\mathbf{x}_{t}-\mathbf{x}^{\star}\rangle\right]-\frac{1}{2}\sum_{t=1}^{T} \mathcal{D}_{F_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})-\frac{1}{2}\sum_{t=1}^{T} \mathcal{D}_{F_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})\] \[\leq\underbrace{\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t }-\mathbf{x}_{t,i^{*}}\rangle-\frac{\alpha_{i^{*}}}{4}\sum_{t=1}^{T}\langle \mathbf{g}_{t},\mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\rangle^{2}}_{\textsc{Meta- Reg}}\]\[+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}h_{t,i^{*}}^{\text{exp}}( \mathbf{x}_{t,i^{*}})-h_{t,i^{*}}^{\text{exp}}(\mathbf{x}^{\star})\right]}_{\text {Basic-Reg}}-\frac{1}{2}\sum_{t=1}^{T}\mathcal{D}_{F_{t}}(\mathbf{x}^{\star}, \mathbf{x}_{t}),\]

where the second step uses the definition of the expected function \(F_{t}(\cdot)\), the third step requires the exp-concavity of \(f_{t}(\cdot)\), and the fourth step is due to \(h_{t,i}^{\text{exp}}(\mathbf{x})\triangleq\langle\mathbf{g}_{t},\mathbf{x} \rangle+\frac{\alpha_{i}}{4}\langle\nabla f_{t}(\mathbf{x}_{t}),\mathbf{x}- \mathbf{x}_{t}\rangle^{2}\), where \(\alpha_{i}\in\mathcal{H}\), defined in (2.1).

For _strongly convex_ functions, following the similar decomposition as in Appendix C, we have

\[\mathbb{E}[\text{Re}_{G}] =\mathbb{E}\left[\sum_{t=1}^{T}\langle\nabla F_{t}(\mathbf{x}_{t }),\mathbf{x}_{t}-\mathbf{x}^{\star}\rangle\right]-\frac{1}{2}\sum_{t=1}^{T} \mathcal{D}_{F_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})-\frac{1}{2}\sum_{t=1}^ {T}\mathcal{D}_{F_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})\] \[\leq\mathbb{E}\left[\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{ x}_{t}-\mathbf{x}^{\star}\rangle\right]-\frac{\lambda}{4}\sum_{t=1}^{T}\| \mathbf{x}_{t}-\mathbf{x}^{\star}\|^{2}-\frac{1}{2}\sum_{t=1}^{T}\mathcal{D}_ {F_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})\] \[\leq\underbrace{\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x}_{t }-\mathbf{x}_{t,i^{*}}\rangle-\frac{\lambda_{i^{*}}}{4}\sum_{t=1}^{T}\| \mathbf{x}_{t}-\mathbf{x}_{t,i^{*}}\|^{2}}_{\text{Meta-Reg}}\] \[\qquad\qquad+\underbrace{\mathbb{E}\left[\sum_{t=1}^{T}h_{t,i^{* }}^{\text{sc}}(\mathbf{x}_{t,i^{*}})-h_{t,i^{*}}^{\text{sc}}(\mathbf{x}^{ \star})\right]}_{\text{Base-Reg}}-\frac{1}{2}\sum_{t=1}^{T}\mathcal{D}_{F_{t}} (\mathbf{x}^{\star},\mathbf{x}_{t}),\]

where the second step, different from the exp-concave case, only requires the strong convexity of \(F_{t}(\cdot)\), and the third step is due to \(h_{t,i}^{\text{sc}}(\mathbf{x})\triangleq\langle\mathbf{g}_{t},\mathbf{x} \rangle+\frac{\lambda_{i}}{4}\|\mathbf{x}-\mathbf{x}_{t}\|^{2}\), where \(\lambda_{i}\in\mathcal{H}\), defined in (2.1).

Meta Regret Analysis.Our Algorithm 1 can be applied to the SEA model without any algorithm modifications. As a result, we directly use the same parameter configurations as in the proof of Theorem 1 (i.e., in Appendix C).

For _convex_ functions, the meta regret can be bounded as

\[\text{Meta-Reg}\leq\mathbb{E}\left[C_{0}\sqrt{1+D^{2}\nabla_{T}} +C_{1}\right]\leq C_{0}\sqrt{1+D^{2}\mathbb{E}[\nabla_{T}]}+C_{1}\] \[\leq C_{0}\sqrt{1+5D^{2}(2\sigma_{1:T}^{2}+\Sigma_{1:T}^{2})+20D ^{2}L\sum_{t=1}^{T}\mathcal{D}_{F_{t}}(\mathbf{x}^{\star},\mathbf{x}_{t})+C_{1}}\] (by (D.2)) \[\leq\mathcal{O}\left(\sqrt{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}} \right)+\mathcal{O}(C_{10})+\frac{C_{0}}{2C_{10}}\sum_{t=1}^{T}\mathcal{D}_{F_{ t}}(\mathbf{x}^{\star},\mathbf{x}_{t}),\]

where the second step is by Jensen's inequality and the last step is due to AM-GM inequality. \(C_{10}\) is a constant to be specified.

For _exp-concave_ and _strongly convex_ functions, the meta regret is bounded in the same way as (C.5) and (C.6), and thus omitted here.

Base Regret Analysis.For _convex_ functions, the base regret can be bounded as

\[\text{Base-Reg}\leq 5D\sqrt{1+\mathbb{E}[\tilde{V}_{T}]}\leq 5D \sqrt{1+10\sigma_{1:T}^{2}+5\Sigma_{1:T}^{2}+20L\sum_{t=1}^{T}\mathcal{D}_{F_{ t}}(\mathbf{x}^{\star},\mathbf{x}_{t})}\] \[\leq\mathcal{O}\left(\sqrt{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}} \right)+\mathcal{O}(C_{11})+\frac{5D}{2C_{11}}\sum_{t=1}^{T}\mathcal{D}_{F_{t}} (\mathbf{x}^{\star},\mathbf{x}_{t}),\]

where the first step is by Jensen's inequality, the second step is due to (D.2), and the last step is because of AM-GM inequality. \(C_{11}\) is a constant to be specified.

[MISSING_PAGE_FAIL:24]

Using Lemma 4, we control the base regret as

\[\text{Base-Reg}\leq\mathcal{O}\left(\frac{1}{\lambda}\left(\sigma_{ \max}^{2}+\Sigma_{\max}^{2}\right)\ln\frac{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}}{ \sigma_{\max}^{2}+\Sigma_{\max}^{2}}\right)\] \[\qquad\quad+\frac{480LGD}{\lambda_{i^{*}}}\ln\left(1+2\lambda_{i^ {*}}\sum_{t=1}^{T}\mathcal{D}_{F_{t}}(\mathbf{x}^{*},\mathbf{x}_{t})\right)+ \frac{8D^{2}}{\lambda_{i^{*}}}\ln\left(1+2\lambda_{i^{*}}^{3}\sum_{t=1}^{T}\| \mathbf{x}_{t,i^{*}}-\mathbf{x}_{t}\|^{2}\right)\] \[\leq\mathcal{O}\left(\frac{1}{\lambda}\left(\sigma_{\max}^{2}+ \Sigma_{\max}^{2}\right)\ln\frac{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}}{\sigma_{ \max}^{2}+\Sigma_{\max}^{2}}\right)+\mathcal{O}(\ln C_{13}+\ln C_{14})\] \[\qquad\quad+\frac{960LGD}{C_{13}}\sum_{t=1}^{T}\mathcal{D}_{F_{t }}(\mathbf{x}^{*},\mathbf{x}_{t})+\frac{16D^{2}}{C_{14}}\sum_{t=2}^{T}\| \mathbf{x}_{t,i^{*}}-\mathbf{x}_{t}\|^{2},\]

where the first term initializes Lemma 4 as \(a_{t}=\sigma_{t}^{2}+\sigma_{t-1}^{2}+\Sigma_{t}^{2}\) (i.e., \(a_{\max}=\mathcal{O}(\sigma_{\max}^{2}+\Sigma_{\max}^{2})\)) and \(b=1/(\sigma_{\max}^{2}+\Sigma_{\max}^{2})\), the second term initializes Lemma 4 as \(a_{t}=\mathcal{D}_{F_{t}}(\mathbf{x}^{*},\mathbf{x}_{t})+\mathcal{D}_{F_{t-1}} (\mathbf{x}^{*},\mathbf{x}_{t-1})\) (i.e., \(a_{\max}=4GD\) due to Assumption 1) and \(b=\lambda_{i^{*}}\), the third term initializes Lemma 4 as \(a_{t}=\lambda_{i^{*}}^{2}\|\mathbf{x}_{t,i^{*}}-\mathbf{x}_{t}\|^{2}+\lambda_{ i^{*}}^{2}\|\mathbf{x}_{t-1,i^{*}}-\mathbf{x}_{t-1}\|^{2}\) (i.e., \(a_{\max}=2D^{2}\) due to \(\lambda_{i}\leq 1\) and Assumption 1) and \(b=\lambda_{i^{*}}\). The \(\mathcal{O}(1)\) term contains ignorable terms like \(\mathcal{O}(1/\lambda)\). The second step requires \(C_{13},C_{14}\geq 1\) by Lemma 5.

Regret Analysis.For _convex_ functions, by combining the meta and base regret, it holds that

\[\text{Reg}_{T} \leq\mathcal{O}\left(\sqrt{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}} \right)+\mathcal{O}(C_{10}+C_{11})+\left(\frac{C_{0}}{2C_{10}}+\frac{5D}{2C_{1 1}}-1\right)\sum_{t=1}^{T}\mathcal{D}_{F_{t}}(\mathbf{x}^{*},\mathbf{x}_{t})\] \[\leq\mathcal{O}\left(\sqrt{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}} \right),\]

by choosing \(C_{10}=C_{0}\) and \(C_{11}=5D\).

For _exp-concave_ functions, by combining the meta and base regret, it holds that

\[\text{Reg}_{T}\leq\mathcal{O}\left(\frac{d}{\alpha}\ln\left(\sigma _{1:T}^{2}+\Sigma_{1:T}^{2}\right)\right)+\mathcal{O}(C_{3}+\ln C_{12})+ \left(\frac{120L}{C_{12}}-\frac{1}{2}\right)\sum_{t=1}^{T}\mathcal{D}_{F_{t}} (\mathbf{x}^{*},\mathbf{x}_{t})\] \[\qquad\qquad+\left(\frac{C_{0}}{2C_{3}}+\frac{4G^{2}}{C_{12}}- \frac{\alpha_{i^{*}}}{4}\right)\sum_{t=1}^{T}\langle\mathbf{g}_{t},\mathbf{x} _{t}-\mathbf{x}_{t,i^{*}}\rangle^{2}\leq\mathcal{O}\left(\frac{d}{\alpha}\ln \left(\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}\right)\right),\]

by choosing \(C_{12}=\max\{1,240L,\frac{32G^{2}}{\alpha_{i^{*}}}\}\) and \(C_{3}=\frac{4C_{0}}{\alpha_{i^{*}}}\). Note that such a parameter configuration will only add an \(\mathcal{O}(1/\alpha)\) factor to the final regret bound, which can be absorbed.

For _strongly convex_ functions, by combining the meta and base regret, it holds that

\[\text{Reg}_{T}\leq\mathcal{O}\left(\frac{1}{\lambda}\left(\sigma _{\max}^{2}+\Sigma_{\max}^{2}\right)\ln\frac{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2} }{\sigma_{\max}^{2}+\Sigma_{\max}^{2}}\right)+\mathcal{O}(C_{4}+\ln C_{13}+ \ln C_{14})\] \[\quad+\left(\frac{C_{0}D^{2}}{2C_{4}}+\frac{16D^{2}}{C_{14}}- \frac{\lambda_{i^{*}}}{4}\right)\sum_{t=1}^{T}\|\mathbf{x}_{t}-\mathbf{x}_{t, i^{*}}\|^{2}+\left(\frac{960LGD}{C_{13}}-\frac{1}{2}\right)\sum_{t=1}^{T} \mathcal{D}_{F_{t}}(\mathbf{x}^{*},\mathbf{x}_{t})\] \[\leq\mathcal{O}\left(\frac{1}{\lambda}\left(\sigma_{\max}^{2}+ \Sigma_{\max}^{2}\right)\ln\frac{\sigma_{1:T}^{2}+\Sigma_{1:T}^{2}}{\sigma_{ \max}^{2}+\Sigma_{\max}^{2}}\right),\]

by choosing \(C_{13}=\max\{1,1920LGD\}\), \(C_{14}=\max\{1,\frac{128D^{2}}{\lambda_{i^{*}}}\}\) and \(C_{4}=\frac{4C_{0}D^{2}}{\lambda_{i^{*}}}\). Note that such a parameter configuration will only add an \(\mathcal{O}(1/\lambda)\) factor to the final bound, which can be absorbed.

Note that the constants \(C_{3},C_{4},C_{10},C_{11},C_{12},C_{13},C_{14}\) only exist in analysis and thus can be chosen arbitrarily, finishing the proof. 

## Appendix E Technical Lemmas

See 5

For any \(a>1,b>0\), it holds that \(\ln(a+b)\leq\ln(Ca)+\frac{b}{C}\) for some \(C\geq 1\).

Proof.: The one-line proof is presented below:

\[\ln(a+b)\leq\ln(Ca+b)\leq\ln(Ca)+\ln\left(1+\frac{b}{Ca}\right)\leq\ln(Ca)+\frac{ b}{C},\]

where the first step is due to \(C\geq 1\), and the last step adopts \(\ln(1+x)\leq x\) for any \(x\geq 0\). 

**Lemma 6** (Corollary 5 of Orabona et al. (2012)).: _If \(a,b,c,d,x>0\) satisfy \(x-d\leq a\ln(bx+c)\), then it holds that_

\[x-d\leq a\ln\left(2ab\ln\frac{2ab}{e}+2bd+2c\right).\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We have claimed the paper's contribution in both the abstract and the introduction part. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: One limitation we could discover is that our approach is not applicable in multi-player games since it does not control the algorithmic stability, which is essential and necessary in achieving fast rates in games. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: The assumptions are provided and justified in Section 2. We provide theoretical guarantees in Section 3 and Section 4, and all the corresponding proofs can be found in Appendix C and Appendix D. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.

**Experimental Result Reproducibility**

Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?

Answer: [NA] Justification: This paper does not include experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

**Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: This paper does not include experiments requiring code. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: This paper does not include experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: This paper does not include experiments. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: This paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This paper is a purely theoretical work, and we do not find specific societal impacts that should be highlighted here. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not include experiments (data or models), and thus poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.