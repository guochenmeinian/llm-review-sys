# Katakomba:

Tools and Benchmarks for Data-Driven NetHack

 Vladislav Kurenkov

Tinkoff

v.kurenkov@tinkoff.ai

&Alexander Nikulin\({}^{*}\)

Tinkoff

a.p.nikulin@tinkoff.ai

&Denis Tarasov

Tinkoff

den.tarasov@tinkoff.ai &Sergey Kolesnikov

Tinkoff

s.s.kolesnikov@tinkoff.ai

Contributed equally.

###### Abstract

NetHack is known as the frontier of reinforcement learning research where learning-based methods still need to catch up to rule-based solutions. One of the promising directions for a breakthrough is using pre-collected datasets similar to recent developments in robotics, recommender systems, and more under the umbrella of offline reinforcement learning (ORL). Recently, a large-scale NetHack dataset was released; while it was a necessary step forward, it has yet to gain wide adoption in the ORL community. In this work, we argue that there are three major obstacles for adoption: resource-wise, implementation-wise, and benchmark-wise. To address them, we develop an open-source library2 that provides workflow fundamentals familiar to the ORL community: pre-defined D4RL-style tasks, uncluttered baseline implementations, and reliable evaluation tools with accompanying configs and logs synced to the cloud.

Footnote 2: Source code: [https://github.com/corl-team/katakomba](https://github.com/corl-team/katakomba)

## 1 Introduction

Reinforcement learning (Arulkumaran et al., 2017; Levine et al., 2020) led to remarkable progress in the decision-making problems in recent years: robotics (Smith et al., 2022; Kumar et al., 2021), recommender systems (Chen et al., 2022), traffic control (Chen et al., 2020), energy management (Yu et al., 2019), combinatorial optimization (Mazyavkina et al., 2021), and videogames (Baker et al., 2022; Hafner et al., 2023). Many of those are considered solved or close to solved problems, and the arguably next untapped frontier for RL algorithms is the NetHack (Kuttler et al., 2020).

NetHack3 is considered one of the most challenging games for humans, even more to learning agents. This game requires strong generalization capabilities, as the levels are procedurally generated with each reset, and extraordinary memory capacity, as the episodes may last for 100k steps requiring to remember what happened a thousand steps before to act optimally. Moreover, the high level of uncertainty and its dependence on the initial character configuration further hardens the problem. Indeed, at the moment, the best agent, AutoAscend (Hambro et al., 2022), is essentially rule-based, and yet reinforcement learning agents cannot even come close in terms of the game score or levels reached. While there were various attempts to advance online reinforcement learning agents, this task seems out of reach for them.

Recently, a promising direction was outlined - re-using large-scale datasets of human/bot demonstrations to bootstrap RL agents, either in the pre-training phase or in the fine-tuning stage (Hambro et al., 2022; Kumar et al., 2022). This research direction holds great promise to advance the community's progress in solving the NetHack and attacking the weak sides of the RL agents. While the proposed tools and datasets are necessary, their implementations, support, and ease of adoption still need to be improved. What is essential, the use of NetHack promises the democratization of research efforts, providing way more efficient tools compared to both StarCraft (Vinyals et al., 2019) and Minecraft (Guss et al., 2019) learning environments. However, while the open-sourced datasets are a significant first step, the provided tools are very far from being readily adopted by other researchers (Section 2).

In this work, we aim to address this gap and provide a set of instruments that are conducive to future research endeavors, smooth the learning curve, and define a precise set of tasks to be evaluated against when training offline RL agents on the NetHack dataset, where contributions are as follows:

* **D4RL-Style Benchmark** A set of small-scale datasets based on the large-scale data source (Hambro et al., 2022) for faster experimentation, including several data loaders for different speed-memory tradeoffs with a familiar interface to the ORL community alike Fu et al. (2020).
* **Clean Recurrent Offline RL Baselines** Straightforward implementations of popular offline RL baselines augmented with recurrence: BC (Michie et al., 1990), CQL (Kumar et al., 2020), AWAC (Nair et al., 2020), REM (Agarwal et al., 2020), IQL (Kostrikov et al., 2021). Each implementation is separated into single files akin to Huang et al. (2021); Tarasov et al. (2022) to smooth the learning curve.
* **Evaluation Guideline** We use reliable evaluation tools for comparing offline RL agents in the natural presence of high variability. For this, we employ recent RLiable tools (Agarwal et al., 2021) that avoid comparing against mean values (which are especially deceptive in the case of the NetHack environment). In order to facilitate the comparisons, we also release the raw results we obtained in our experiments so that future work may re-use them for a fair comparison.
* **Open-Sourced Training Logs and Configs** Moreover, we keep a public track of our experiments: including configs, implementations, and learning dynamics using Weights&Biases (Biewald, 2020), so that it is transparent for inspection and reproduction.

We believe our work is a logical next step for advancing reinforcement learning in the NetHack environment, providing a conducive set of tools, algorithms, and utilities for experimenting with offline RL algorithms. Moreover, as we plan to support the benchmark, we intend to extend it with offline-to-online tools and benchmarks. By no means this work serves as a converged fixed point. We view this as a starting step in building reliable tools for solving NetHack with prior data and commit to further support and development.

## 2 What Hinders the Adoption of Data-Driven NetHack?

In this section, we describe the main obstacles and motivate our work. We outline our experience with the recently released dataset (Hambro et al., 2022), accompanying tools and divide the main issues into three categories: implementational, resources, and benchmark-wise. These are needed to illustrate that the initial release of the dataset is a significant step. However, it is only the first step, and further development is needed, which we will address in further sections.

Implementation-wiseFirst, when trying to reproduce the experiments, we found that installing the released tools is error-prone: where the provided Dockerfile was not operable and required fixing the moolib library with a CMake modification, which is probably undesirable for broader adoption of practitioners. Second, when we attempted to reproduce the scores reported in Hambro et al. (2022), the only configuration file provided was for the IQL algorithm4 and its modification with hyperparameters for other algorithms from the paper required additional effort to assign proper values. Consequently, the reproduction did not result near what was reported in the paper (note that a similar pattern was observed in Piterbarg et al. (2023)). While we do not claim that the reproduction is impossible, we could not find the proper fix in a reasonable amount of time and consider this as another obstacle in adoption - practitioners should either have access to the original training logs or be able to replicate the results reliably.

Beyond the replication frictions, another issue lies in the design of implementations. Offline and offline-to-online settings are interleaved within just one file (1500 lines of code), where the algorithms are realized using the framework for distributed reinforcement learning not typical for the ORL community (Mella et al., 2022). Consequently, understanding the underlying logic is hard, and some performance-related implementation tricks can be hard to spot. For example, q-learning-based algorithms do not use the last element of the sequence, i.e., every \(sequence\_length\) game tuple is not used for training (but only for bootstrapping). While all of these may seem like minor issues, their sum brings a significant burden upon practitioners interested in the data-driven NetHack research.

Resource-wiseThe great addition to the released datasets was an accompanying data loader that operates over compressed game episodes (we refer to it as TTYRec throughout the text). While the Hambro et al. (2022b) demonstrated that the proposed loader could operate at approximately 5 hours per epoch, we observed a slower rate when adapting it for recurrent algorithms. The main issue lies in the underlying format where the data is fetched the following way - \((s_{t},a_{t},r_{t-1})\)5. While this may seem non-significant, it results in the need to fix the tuples sequence to obtain a suitable format \((s_{t},a_{t},s_{t+1},r_{t})\). To better demonstrate the impact of this problem, consider Table 1, where we test the original data loader and the one with the fix, observing a significant increase in the loading time as both batch size and sequence length increase. While this can be avoided with the original loader by simply discarding each \(sequence\_length\) element (as done in the original work), the problem persists as the original implementations do not work without reward shaping with potentials requiring a scan over the sequence, not to mention that such data rejection is not justified beyond performance reasons. As an additional but important issue to resource-constrained practitioners, one must first download the entire 90GB AA dataset, even if they aim to work on a subset of data.

Footnote 5: Notation standard for the RL community, where \(s\) is a state, \(a\) is an action, and \(r\) is a reward.

Benchmark-wiseArguably, one of the driving forces in Deep Learning and Reinforcement Learning, in general, is a well-defined set of tasks. To demonstrate how the proposed large-scale dataset could be utilized for tasks definition, Hambro et al. (2022b) described two settings: learning on the whole dataset for all role-race-alignment-sex combinations and learning on the subset of data for the Monk-Human-Neutral setting. While this is a good entry point and could be of great use to practitioners interested in large-scale data regimes, there was no detailed proposal on how one should further define tasks which is indeed an open question in the NetHack learning community (Kuttler et al., 2020). Moreover, the original raw large-scale dataset requires practitioners to manually define SQL queries for extracting the data of interest, which is flexible but could be an overkill.

More importantly, the proposed comparison metrics were mean and median statistics of average episode returns over training seeds. One may argue that the median is a robust statistic. However, in this case, it is a median of _average_ episode returns over training seeds and not of the whole set of evaluation episodes. It is well known in the RL community (Agarwal et al., 2021) that those are not reliable due to the highly-variable nature of RL agents. As of the NetHack, this further amplifies by the extremely-variable nature of the game itself as both demonstrated in Kuttler et al. (2020); Hambro et al. (2022b). Therefore, to reduce the noise in the progress of RL agents in this domain, one would need a different evaluation toolset that is better suited for it.

\begin{table}
\begin{tabular}{l|c c|c c c} \hline \hline
**Variants** & **TTYRec** & **TTYRec**, **Proper Tugles** & **HDF5 (Mexmap)** & **HDF5 (RAM)** & **HDF5 (Compressed)** \\ \hline batch\_size=64, seq\_len=16 & 15ms & 17ms & 2ms & 1ms & 516ms \\ batch\_size=256, seq\_len=32 & 74ms & 132ms & 12ms & 9ms & 2.39s \\ batch\_size=256, seq\_len=64 & 255ms & 372ms & 18ms & 14ms & 2.42s \\ \hline \hline \end{tabular}
\end{table}
Table 1: Loading times for different storage formats averaged over 500 iterations. The right part of the table, with HDF5 columns, depicts the loading time for datasets repacked within Katakomba. Note that the transitions are in the proper format for them by design.

Katakomba

Given the issues described in the previous section, we are ready to present Katakomba - a set of tools, benchmarks, and memory-based offline RL baselines. In this section, we gradually introduce the components of the library with accompanying code examples to better demonstrate the ease of use. All the numbers and performance metrics discussed in the text were measured on the same set of hardware: 14CPU, 128GB RAM, NVMe, 1xA100; for more details, please, see Appendix D.

### Benchmark

Listing 1: Usage example: Tasks are defined via the character field that is then used by the offline wrapper for dataset loading and score normalization.

```
fromkatakomba.envimportNetHackChallenge fromkatakomba.envimportOfflineNetHackChallengeWrapper
#Thetaskisspecifiedusingthecharacterfield env=NetHackChallenge( character="mon-hum-neu", observation_keys=["ttyt_chars","ttyt_colors","ttyt_cursor"] )
#Aconvenientwrapperthatprovidesinterfacesfor
#datasetloadingandscorenormalization env=OfflineNetHackChallengeWrapper(env)
```

DecompositionIn our benchmark, we re-use the dataset collected by the AutoAscend bot (Hambro et al., 2022b, a). While this bot is highly-capable, it still is considered an early-game contender because it can not descend further than two levels in half of the episodes. As the dataset becomes an early game bridgehead, we divide the tasks based on the character configurations: role, race, and alignment. In the NetHack community, these are known to be the most important (and even having a dramatic effect), requiring to utilize varying abilities of each role or race6. Overall, in opposition to merging all combinations, this decomposition allows more focus on the characters' gameplay and the size reduction one needs to download for both playing-around and committed research, as we will describe further.

Footnote 6: [https://nethackwiki.com/wiki/Player](https://nethackwiki.com/wiki/Player)

CategoriesThis results in 38 datasets in total. However, it may not be possible for researchers to examine each of them as the training times can be an obstacle. To this end, we further divide the tasks into three categories: Base, Extended, and Complete. We separate each category based on the wisdom of the NetHack community, i.e., that roles have a more substantial effect on the gameplay than race, and race has more effect than alignment. The datasets and categories are listed in Table 2. Base tasks consist of all possible roles for the human race; we choose a neutral alignment where possible; otherwise, we pick the alternative one (for humans, if there is no neutral alignment, there is only one alternative). We include all other role-race combinations for Extended tasks. For Complete, we add tasks that were not included in the Base and Extended categories. Note that these three categories combined cover all of the allowed role-race-alignment combinations.

Data SelectionAs demonstrated in the previous section, the original TTYRec dataset is actually slower than expected when it is used for learning agents. Moreover, one may be unable to download the 90GB dataset. Therefore, we take a different path by subsampling and repacking the original dataset task-wise, averaging 680 episodes and 1.3GB per task. The subsampling procedure is stratified by the game score (for more details, please see the script7). This allows for more versatility: one can download the needed datasets on demand as in D4RL (Fu et al., 2020); furthermore, this permits us to address the rolling issue as we repacked the transition tuples in the way suitable to typical ORL pipeline as a part of the release. To ensure the reliable accessibility of the data, we host it on the HuggingFace Hub (Engstrom et al., 2020).

[MISSING_PAGE_FAIL:5]

```
Listing 2: Usage example: Katakomba provides different loaders that can be chosen depending on one's speed-memory tradeoff.
```
#DecompressthedatasetintofRAM dataset=env.get_dataset(mode="in_memory")
#Decompressionon-read dataset=env.get_dataset(mode="compressed")
#TheoriginaldataloaderintroducedinHambroetal,2022 dataset=env.get_dataset(scale="big")
#Decompressthedatasetondisk dataset=env.get_dataset(mode="memmap")
#Ifyouwanttodeletethedecompresseddataset
#Thiswillnotaffectthecompressedversion dataset.close() ```

HDF5, In-Disk (Compressed, Disk)This mode is the most cheap one but slow. Essentially, the dataset is read on from disk and decompressed on-the-fly. We found this useful for debugging purposes where one does not need the whole training process to be run.

TTYRec, In-Disk (Compressed, Disk)In case one finds the original approach to data loading more suitable, we also provide a convenient interface that wraps the source large-scale dataset and loader. One can also set it up in more detail using keyword arguments from the original TTYRec data loader. However, this option comes with the downsides described in Section 2, i.e., slower iteration speed and the need to download the 90GB dataset at least once.

In addition to the dataset interfaces, we also provide an implementation of a replay buffer suitable for sequential offline RL algorithms for bootstrapping practitioners with ready-to-go tools.

### Evaluation Methodology under High Variability

In Hambro et al. (2022), authors used an average episode return across seeds when comparing baselines. While this is a standard practice in the RL and ORL communities, it was recently shown to be unreliable (Agarwal et al., 2021) as the algorithms are known to be highly variable in performance. This problem amplifies even more for NetHack, where the score distribution is typically quite wide for humans and bots (Kuttler et al., 2020).

To address this, we argue that the evaluation toolbox from Agarwal et al. (2021) is more appropriate and suggest using it when comparing NetHack learning agents. We use these tools for two dimensions: in-game score and death level. The first dimension corresponds to what one typically optimizes with ORL agents but is considered a proxy metric (Kuttler et al., 2020). While the latter lower bounds the early-game progress and is more indicative of the game completion.

Furthermore, similar to Fu et al. (2020), we also suggest reporting normalized scores to capture how far one is from the AutoAscend bot. We use mean scores per dataset as a normalization factor and rescale to [0, 100] range after normalization, similar to D4RL (Fu et al., 2020). This functionality is also provided as a part of the offline wrapper for the NetHackChallenge environment. Please refer to Appendix D for precise values.

## 4 Benchmarking Recurrent Offline RL Algorithms

AlgorithmsFor our benchmark, we rely on the following set of algorithms: Behavioral Cloning (BC) (Michie et al., 1990), Implicit Q-Learning (IQL) (Kostrikov et al., 2021), Conservative Q-Learning (CQL) (Kumar et al., 2020), Randomized Ensemble Mixture (REM) (Agarwal et al., 2020), and Advantage-Weighted Actor-Critic (AWAC) (Nair et al., 2020). These are known as either the most competitive in the continuous control setting (Tarasov et al., 2022) or were shown to be competitive in the discrete control (Agarwal et al., 2020; Kumar et al., 2022). Similar to Hambro et al. (2022, 2022), we build upon Chaotic-Dwarven-GPT-5 architecture that converts the try observation into an image and then feeds it into the CNN layers followed by the LSTM (Hochreiter and Schmidhuber, 1997). Ultimately, we test five common ORL algorithms augmented with recurrence. To the best of our knowledge, there are no other open-sourced alternatives beyond the Hambro et al. (2022) that also do not implement the AWAC algorithm.

Experimental SetupWe train every algorithm for 500k gradient steps, resulting in around 20 epochs per dataset. We report the scores of the last trained policy over 50 evaluation episodes as standard in the ORL community. Important to highlight that while this amount may seem small for NetHack, this number is adequate when used in conjunction with stratified datasets and RLiable evaluation tools due to the underlying bootstrapping mechanism. For specific hyperparameters used, please either see Appendix E or the configuration files provided in the code repository.

Replicability and Reproducibility StatementTo ensure that our experiments' results are replicable and can easily be reproduced and inspected, we rely on the Weights&Biases (Biewald, 2020). In the provided code repository, one can find all the logs, including configs, dependencies, code snapshots, hyperparameters, system variables, hardware specifications, and more. Moreover, to reduce the efforts of interested parties required for inspection, we structurize the results using the Weights&Biases public reports.

ResultsThe outcomes are twofold. First, the results achieved are similar to the already observed by the Piterbarg et al. (2023) and Hambro et al. (2022), but on a larger number of offline RL algorithms tested. As shown in Figure 1 and Figure 2, all algorithms were unable to replicate the scores of the AutoAscend bot, reaching normalized scores below 6.0 on average and not progressing beyond the first level on the majority of training runs. Notably, only 5% of the episodes resulted in a normalized score of around 20.0 (Figure 0(b)). Moreover, REM has not been able to achieve even the non-zero normalized score. Second, perhaps surprisingly, the only algorithm that does not rely in any way on policy constraints is also the only algorithm that completely failed. This, and also the high correlation

Figure 1: Normalized performance under the Katakomba benchmark for all 38 datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 5700 points for constructing these graphs. As one can see, there is not much improvement beyond naive behavioral cloning.

in the performance profiles (Figure 0(b)), gives us a hint that all other methods showing non-zero results actually rely primarily on behavioral cloning in one form or another, such as advantage weighted regression in IQL and AWAC or KL-divergence as in CQL. Indeed, the most successful hyperparameters in our experiments proved to be those that significantly increase the weight of losses that encourage similarity to the behavioral policy (see Table 10 in the Appendix E). Furthermore, as shown in the Figure 0(c) and Figure 0(c) Behavioral Cloning algorithm is not worse than all the other more sophisticated offline RL algorithms. Thus, NetHack remains a major challenge for offline RL algorithms, and Katakomba can serve as a starting point and testbed for offline RL research in this direction. For graphs stratified by the Base, Extended, and Complete categories, see Appendix F.

## 5 Related Work

Offline RLIn recent years, there was considerable interest of the reinforcement learning community in the offline setting, which resulted in numerous and diverse algorithms specifically tailored for this setup (Nair et al., 2020; Kumar et al., 2020; Kostrikov et al., 2021; Chen et al., 2021; Fujimoto and Gu, 2021; An et al., 2021; Nikulin et al., 2023; Tarasov et al., 2023). The core idea behind most of them is to ensure the resulting policy stays close to the behavioral one. This could be achieved via different ways: penalizing value function (Kumar et al., 2020; An et al., 2021; Nikulin et al., 2023), constraining the policy outputs (Fujimoto and Gu, 2021; Tarasov et al., 2023), or even training directly in the conservative latent space (Zhou et al., 2020; Chen et al., 2022; Akimov et al., 2022). Due to the benchmark-centricity of the RL field, most of the proposed ORL algorithms are for continuous control with a few exceptions (Agarwal et al., 2020; Kumar et al., 2020, 2022). The de-facto standard benchmark is the D4RL (Fu et al., 2020), which provides a suite of datasets focused on continuous control with proprioceptive states under different regimes, such as sparse-reward or low-data regimes. Also, few benchmarks move the focus from proprioceptive states to images or other more complex entities (Qin et al., 2022; Lu et al., 2022; Agarwal et al., 2020).

RL for NetHackNetHack as a testbed for RL agents was introduced in Kuttler et al. (2020). To further advance the RL agents in this domain, the NetHack Challenge Competition (NHC) was held

Figure 2: Death level under the Katakomba benchmark for all 38 datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 570 points for constructing these graphs. As one can clearly see, there is not much improvement beyond the naive behavioral cloning.

(Hambro et al., 2022) that resulted in two of the most performant agents - learning-based Chaotic-Dwarven-GPT-5, and a rule-based AutoAscend (AA). Notably, the latter outperformed learning-based agents by a wide margin. Consequently, this solution was used to collect the large-scale NetHack Learning Dataset (Hambro et al., 2022). The closest concurrent work is by Piterbarg et al. (2023) - the authors released another AA dataset but accompanied with the hierarchical labels, which arise due to the nature of the AutoAscend bot, demonstrating their usefulness in cloning the AA bot. However, in contrast to our work, Piterbarg et al. (2023) focuses on the large-scale setup similar to Hambro et al. (2022).

## 6 Discussion, Limitations, and Future Work

In this work, we focused on building reliable tools and benchmarks for offline RL setting using the recently released AutoAscend large-scale dataset (Hambro et al., 2022). While this does not cover the whole spectrum of NetHack's community interests, such as offline-to-online regime or learning from human demonstrations, we believe our effort is helpful in establishing reliable and open instruments for data-driven NetHack. Moreover, our contributions could be of interest to the part of the ORL community studying discrete control, memory, and adaptivity (Ghosh et al., 2022).

Given our results and experience with the NetHack Learning Environment, we believe fruitful future research may lie along the following directions: finding a better state-encoder, as the current one presents a bottleneck in both efficiency (rendering is expensive) and locality (only the small part of the terminal is used). Another interesting research direction would be to assess recently appeared recurrence mechanisms such as Linear Recurrent Unit (Orvieto et al., 2023), which might also speed up the training process without hurting the performance. Also, as the interest in generalization properties will appear, it would be a great addition to include more datasets that will provide metadata on the seeds used for data generation, as it will allow to assess trained agents on both seen and unseen seeds to quantify the generalization gap more systematically.

Overall, we firmly believe that NetHack provides a nice playground for investigating how to build a next generation of reinforcement learning agents using prior data that would encompass stronger generalization and memory capabilities. To this end, we plan to continuously maintain the benchmark, accompanying tools, and curate new datasets if considered useful for further advancements.

## References

* Agarwal et al. (2020) Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. In _International Conference on Machine Learning_, pp. 104-114. PMLR, 2020.
* Agarwal et al. (2021) Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. _Advances in neural information processing systems_, 34:29304-29320, 2021.
* Akimov et al. (2022) Dmitriy Akimov, Vladislav Kurenkov, Alexander Nikulin, Denis Tarasov, and Sergey Kolesnikov. Let offline rl flow: Training conservative agents in the latent space of normalizing flows. _arXiv preprint arXiv:2211.11096_, 2022.
* An et al. (2021) Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun Oh Song. Uncertainty-based offline reinforcement learning with diversified q-ensemble. _Advances in neural information processing systems_, 34:7436-7447, 2021.
* Arulkumaran et al. (2017) Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony Bharath. A brief survey of deep reinforcement learning. _arXiv preprint arXiv:1708.05866_, 2017.
* Baker et al. (2022) Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. _Advances in Neural Information Processing Systems_, 35:24639-24654, 2022.
* Biewald (2020) Lukas Biewald. Experiment tracking with weights and biases, 2020. URL [https://www.wandb.com/](https://www.wandb.com/). Software available from wandb.com.
* Berman et al. (2021)Chacha Chen, Hua Wei, Nan Xu, Guanjie Zheng, Ming Yang, Yuanhao Xiong, Kai Xu, and Zhenhui Li. Toward a thousand lights: Decentralized deep reinforcement learning for large-scale traffic signal control. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pp. 3414-3421, 2020.
* Chen et al. (2021) Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* Chen et al. (2022a) Minmin Chen, Can Xu, Vince Gatto, Devanshu Jain, Aviral Kumar, and Ed H. Chi. Off-policy actor-critic for recommender systems. _Proceedings of the 16th ACM Conference on Recommender Systems_, 2022a.
* Chen et al. (2022b) Xi Chen, Ali Ghadirzadeh, Tianhe Yu, Yuan Gao, Jianhao Wang, Wenzhe Li, Bin Liang, Chelsea Finn, and Chongjie Zhang. Latent-variable advantage-weighted policy optimization for offline rl. _arXiv preprint arXiv:2203.08949_, 2022b.
* Engstrom et al. (2020) Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, L. Rudolph, and Aleksander Madry. Implementation matters in deep rl: A case study on ppo and trpo. In _ICLR_, 2020.
* Fu et al. (2020) Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning. _arXiv preprint arXiv:2004.07219_, 2020.
* Fujimoto & Gu (2021) Scott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning. _Advances in neural information processing systems_, 34:20132-20145, 2021.
* Ghosh et al. (2022) Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline rl policies should be trained to be adaptive. In _International Conference on Machine Learning_, pp. 7513-7530. PMLR, 2022.
* Guss et al. (2019) William H Guss, Brandon Houghton, Nicholay Topin, Phillip Wang, Cayden Codel, Manuela Veloso, and Ruslan Salakhutdinov. Minerl: a large-scale dataset of minecraft demonstrations. In _Proceedings of the 28th International Joint Conference on Artificial Intelligence_, pp. 2442-2448, 2019.
* Hafner et al. (2023) Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains through world models. _arXiv preprint arXiv:2301.04104_, 2023.
* Hambro et al. (2022a) Eric Hambro, Sharada Mohanty, Dmitrii Babaev, Minwoo Byeon, Dipam Chakraborty, Edward Grefenstette, Minqi Jiang, Jo Daejin, Aussi Kanervisto, Jongmin Kim, et al. Insights from the neurips 2021 netback challenge. In _NeurIPS 2021 Competitions and Demonstrations Track_, pp. 41-52. PMLR, 2022a.
* Hambro et al. (2022b) Eric Hambro, Roberta Raileanu, Danielle Rothermel, Vegard Mella, Tim Rocktaschel, Heinrich Kuttler, and Naila Murray. Dungeons and data: A large-scale netback dataset. In _Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022b.
* Hochreiter & Schmidhuber (1997) Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9:1735-80, 12 1997. doi: 10.1162/neco.1997.9.8.1735.
* Huang et al. (2021) Shengyi Huang, Rousslan Fernand Julien Dossa, Chang Ye, and Jeff Braga. Cleanrl: High-quality single-file implementations of deep reinforcement learning algorithms. _arXiv preprint arXiv:2111.08819_, 2021.
* Kingma & Ba (2014) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kostrikov et al. (2021) Ilya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit q-learning. _arXiv preprint arXiv:2110.06169_, 2021.
* Kumar et al. (2020) Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 33:1179-1191, 2020.
* Kumar et al. (2020)Aviral Kumar, Anikati Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for offline model-free robotic reinforcement learning. In _5th Annual Conference on Robot Learning_, 2021. URL [https://openreview.net/forum?id=fy4ZBWxYbIo](https://openreview.net/forum?id=fy4ZBWxYbIo).
* Kumar et al. [2022a] Aviral Kumar, Rishabh Agarwal, Xinyang Geng, G. Tucker, and Sergey Levine. Offline q-learning on diverse multi-task data both scales and generalizes. _ArXiv_, abs/2211.15144, 2022a.
* Kumar et al. [2022b] Aviral Kumar, Rishabh Agarwal, Xinyang Geng, George Tucker, and Sergey Levine. Offline q-learning on diverse multi-task data both scales and generalizes. _arXiv preprint arXiv:2211.15144_, 2022b.
* Kuttler et al. [2020] Heinrich Kuttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, and Tim Rocktaschel. The netback learning environment. _Advances in Neural Information Processing Systems_, 33:7671-7684, 2020.
* Levine et al. [2020] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. _arXiv preprint arXiv:2005.01643_, 2020.
* Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* Lu et al. [2022] Cong Lu, Philip J. Ball, Tim G. J. Rudner, Jack Parker-Holder, Michael A. Osborne, and Yee Whye Teh. Challenges and opportunities in offline reinforcement learning from visual observations, 2022.
* Mazyavkina et al. [2021] Nina Mazyavkina, Sergey Sviridov, Sergei Ivanov, and Evgeny Burnaev. Reinforcement learning for combinatorial optimization: A survey. _Computers & Operations Research_, 134:105400, 2021.
* Mella et al. [2022] Vegard Mella, Eric Hambro, Danielle Rothermel, and Heinrich Kuttler. moolib: A Platform for Distributed RL. 2022. URL [https://github.com/facebookresearch/moolib](https://github.com/facebookresearch/moolib).
* Michie et al. [1990] Donald Michie, Michael Bain, and J Hayes-Miches. Cognitive models from subcognitive skills. _IEE control engineering series_, 44:71-99, 1990.
* Nair et al. [2020] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement learning with offline datasets. _arXiv preprint arXiv:2006.09359_, 2020.
* Nikulin et al. [2023] Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, and Sergey Kolesnikov. Anti-exploration by random network distillation. _arXiv preprint arXiv:2301.13616_, 2023.
* Orvieto et al. [2023] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences, 2023.
* Piterbarg et al. [2023] Ulyana Piterbarg, Lerrel Pinto, and Rob Fergus. Netback is hard to hack. _arXiv preprint arXiv:2305.19240_, 2023.
* Qin et al. [2022] Rong-Jun Qin, Xingyuan Zhang, Songyi Gao, Xiong-Hui Chen, Zewen Li, Weinan Zhang, and Yang Yu. Neorl: A near real-world benchmark for offline reinforcement learning. _Advances in Neural Information Processing Systems_, 35:24753-24765, 2022.
* Smith et al. [2022] Laura Smith, Ilya Kostrikov, and Sergey Levine. A Walk in the Park: Learning to Walk in 20 Minutes With Model-Free Reinforcement Learning, August 2022.
* Tarasov et al. [2022] Denis Tarasov, Alexander Nikulin, Dmitry Akimov, Vladislav Kurenkov, and Sergey Kolesnikov. CORL: Research-oriented deep offline reinforcement learning library. In _3rd Offline RL Workshop: Offline RL as a "Launchpad"_, 2022. URL [https://openreview.net/forum?id=SysA49bBcv](https://openreview.net/forum?id=SysA49bBcv).
* Tarasov et al. [2023] Denis Tarasov, Vladislav Kurenkov, Alexander Nikulin, and Sergey Kolesnikov. Revisiting the minimalist approach to offline reinforcement learning. _arXiv preprint arXiv:2305.09836_, 2023.
* Vinyals et al. [2019] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Michael Mathieu, Andrew Dudzik, Junyoung Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 575(7782):350-354, 2019.
* Vinyals et al. [2019]* Yu et al. (2019) Liang Yu, Weiwei Xie, Di Xie, Yulong Zou, Dengyin Zhang, Zhixin Sun, Linghua Zhang, Yue Zhang, and Tao Jiang. Deep reinforcement learning for smart home energy management. _IEEE Internet of Things Journal_, 7(4):2751-2762, 2019.
* Zhou et al. (2020) Wenxuan Zhou, Sujay Bajracharya, and David Held. Plas: Latent action space for offline reinforcement learning. In _Conference on Robot Learning_, 2020.

What Is Inside the Datasets?

Every dataset is repacked into HDF5 files similar to Fu et al. (2020). The data keys are described in Table 3; along to the typical \((s_{t},a_{t},r_{t},d_{t})\) tuples, the metadata is also provided as the datasets' attributes with a comprehensive information about specific trajectories similar to Hambro et al. (2022). The re-packing script is provided at [https://github.com/corl-team/katakomba/tree/main/scripts/generate_small_dataset.py](https://github.com/corl-team/katakomba/tree/main/scripts/generate_small_dataset.py).

## Appendix B License

Our codebase and repacked datasets are released under the NETHACK GENERAL PUBLIC LICENSE. The original NetHack Learning environment (Kuttler et al., 2020) and large-scale datasets (Hambro et al., 2022) are also released under NETHACK GENERAL PUBLIC LICENSE.

## Appendix C General Ethic Conduct and Potential Negative Societal Impact

To the best of our knowledge, our work does not present any direct potential negative societal impact.

As of the general ethic conduct, we believe that the most relevant issue to be discussed is the "Consent to use or share the data". Our work is largely built upon both the NetHack Learning Environment (Kuttler et al., 2020) and the coresponding large-scale dataset (Hambro et al., 2022), and as already described in the Appendix B both are distributed under the NETHACK GENERAL PUBLIC LICENSE that explicitly allows for re-usage and re-distribution.

\begin{table}
\begin{tabular}{l c c l} \hline \hline Name & Type & Shape & Description \\ \hline \hline tty\_chars & np.uint8 & [B, T, H, W] & \(s_{t}\): The on-screen characters (default screen size 80 x 24). \\ tty\_colors & np.int8 & [B, T, H, W] & \(s_{t}\): The on-screen colors for each character. \\ tty\_cursor & np.int16 & [B, T, 2] & \(s_{t}\): The coordinates of the on-screen cursor. \\ actions & np.uint8 & [B, T] & \(a_{t}\): The NLE actions the player made in response to the \(s_{t}\). \\ rewards & np.int32 & [B, T] & \(r_{t}\): The difference between in-game scores at states \(s_{t}\) and \(s_{t-1}\). Note that this was used in all implementations of the algorithms provided in Hambro et al. (2022). We also found that without this reward-shaping, all offline RL algorithms failed completely. \\ dones & np.uint8 & [B, T] & \(d_{t}\): An indicator whether the current state is the last one in the trajectory. \\ \hline \hline \end{tabular}
\end{table}
Table 3: The re-packed datasets constitute of transformed data from Hambro et al. (2022). Dissimilar the the large scale dataset, the repacked data is now in the format familiar to the ORL practitioners. We also save the metadata for each trajectory, for a comprehensive description, please, see Appendix F in Hambro et al. (2022).

Resources and Statistics

We used 64 separated computational nodes with 1xA100, 14CPU, 128GB RAM, and the NVMe as long-term storage for all our experiments. All the values reported in the paper were also obtained under this configuration. One can also find more detailed information inside the Weights&Biases logs in the code repository.

## Appendix D

\begin{table}
\begin{tabular}{l|r r r} \hline \hline
**Tasks** & **Minimum Score** & **Maximum Score** & **Mean Score** \\ \hline \hline
**Base (Role-Centric)** & - & - & - \\ \hline arc-hum-neu & 0.0 & 138103.0 & 6636.44 \\ bar-hum-neu & 0.0 & 292342.0 & 17836.68 \\ cav-hum-neu & 0.0 & 258978.0 & 12113.87 \\ hea-hum-neu & 0.0 & 64337.0 & 4068.27 \\ kni-hum-law & 0.0 & 419154.0 & 14137.06 \\ mon-hum-neu & 0.0 & 171224.0 & 17456.05 \\ pri-hum-neu & 0.0 & 114269.0 & 7732.69 \\ ran-hum-neu & 0.0 & 54874.0 & 8067.99 \\ rog-hum-cha & 0.0 & 68628.0 & 4818.20 \\ sam-hum-law & 0.0 & 155163.0 & 11009.36 \\ tou-hum-neu & 0.0 & 59484.0 & 4211.47 \\ val-hum-neu & 16.0 & 313858.0 & 18624.77 \\ wiz-hum-neu & 0.0 & 71709.0 & 5323.48 \\ \hline \hline
**Extended (Race-Centric)** & - & - & - \\ \hline pri-elf-cha & 0.0 & 83744.0 & 7109.35 \\ ran-elf-cha & 0.0 & 66690.0 & 9014.18 \\ wiz-elf-cha & 0.0 & 71664.0 & 5005.16 \\ arc-dwa-law & 0.0 & 83496.00 & 5445.69 \\ cav-dwa-law & 0.0 & 161682.0 & 11893.48 \\ val-dwa-law & 0.0 & 1136591.0 & 23473.61 \\ arc-gno-neu & 0.0 & 110054.0 & 5316.57 \\ cav-gno-neu & 0.0 & 142460.0 & 10083.06 \\ hea-gno-neu & 0.0 & 69566.0 & 3783.93 \\ ran-gno-neu & 0.0 & 58137.0 & 6965.04 \\ wiz-gno-neu & 0.0 & 37376.0 & 4317.51 \\ bar-orc-cha & 0.0 & 164296.0 & 17594.38 \\ ran-orc-cha & 3.0 & 69244.0 & 7608.48 \\ rog-orc-cha & 0.0 & 54892.0 & 4897.69 \\ wiz-orc-cha & 0.0 & 40871.0 & 5016.74 \\ \hline \hline
**Complete (Alignment-Centric)** & - & - & - \\ \hline arc-hum-law & 2.0 & 84823.0 & 5826.35 \\ cav-hum-law & 0.0 & 156966.0 & 12462.82 \\ mon-hum-law & 7.0 & 190783.0 & 16091.57 \\ pri-hum-law & 0.0 & 99250.0 & 6847.99 \\ val-hum-law & 0.0 & 428274.0 & 26103.03 \\ bar-hum-cha & 0.0 & 164446.0 & 18228.11 \\ mon-hum-cha & 0.0 & 223997.0 & 18353.30 \\ pri-hum-cha & 0.0 & 58367.0 & 8262.56 \\ ran-hum-cha & 3.0 & 62599.0 & 8378.50 \\ wiz-hum-cha & 0.0 & 55185.0 & 5316.82 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Scores used for Normalization. You can also find them at [https://github.com/corl-team/katakomba/blob/main/katakomba/utils/scores.py](https://github.com/corl-team/katakomba/blob/main/katakomba/utils/scores.py). For other statistics, please, see Table 2 in the main text.

Hyperparameters

For all algorithms, hyperparameters have been reused from previous work whenever possible. For BC, CQL, and IQL reference values, see Appendix I.4 in the Hambro et al. (2022). For AWAC, hyperparameters from IQL were reused due to the very similar policy updating scheme. For REM, hyperparameters were taken from the original work (see Agarwal et al. (2020)).

As in Hambro et al. (2022), and in contrast to the original CQL implementation, we multiply the TD loss by the \(\alpha\) coefficient instead of the CQL loss, as we observed better results with such a scheme. We performed a search for \(\alpha\in[0.0001,0.0005,0.001,0.01,0.05,0.1,0.5,1.0]\) with best value \(\alpha=0.0001\).

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Parameter** & **Value** \\ \hline optimizer & AdamW (Kingma \& Ba, 2014; Loshchilov \& Hutter, 2017) \\ training iterations & 500000 \\ batch size & 64 \\ sequence length & 16 \\ learning rate & 3e-4 \\ weight decay & 0.0 \\ state encoder & Chaotic-Dwarven-GPT-5(Hambro et al., 2022,b) \\ LSTM hidden dim & 2048 \\ LSTM layers & 2 \\ LSTM dropout & 0.0 \\ use previous action & True \\ \hline \hline \end{tabular}
\end{table}
Table 6: CQL hyperparameters. Note that in our implementation, the \(\alpha\) coefficient multiplies the TD loss.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Parameter** & **Value** \\ \hline optimizer & AdamW (Kingma \& Ba, 2014; Loshchilov \& Hutter, 2017) \\ training iterations & 500000 \\ batch size & 64 \\ sequence length & 16 \\ learning rate & 3e-4 \\ weight decay & 0.0 \\ state encoder & Chaotic-Dwarven-GPT-5(Hambro et al., 2022,b) \\ LSTM hidden dim & 2048 \\ LSTM layers & 2 \\ LSTM dropout & 0.0 \\ use previous action & True \\ tau (\(\tau\)) & 5e-3 \\ gamma (\(\gamma\)) & 0.999 \\ reward clip range & [-10.0, 10.0] \\ alpha (\(\alpha\)) & 1e-4 \\ \hline \hline \end{tabular}
\end{table}
Table 5: BC hyperparameters.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Parameter** & **Value** \\ \hline optimizer & AdamW (Kingma \& Ba, 2014; Loshchilov \& Hutter, 2017) \\ training iterations & 500000 \\ batch size & 64 \\ sequence length & 16 \\ learning rate & 3e-4 \\ weight decay & 0.0 \\ state encoder & Chaotic-Dwarven-GPT-5(Hambro et al., 2022a,b) \\ LSTM hidden dim & 2048 \\ LSTM layers & 2 \\ LSTM dropout & 0.0 \\ use previous action & True \\ tau (\(\tau\)) & 5e-3 \\ gamma (\(\gamma\)) & 0.999 \\ reward clip range & [-10.0, 10.0] \\ expectile & 0.8 \\ temperature & 1.0 \\ advantage clip max & 100 \\ \hline \hline \end{tabular}
\end{table}
Table 7: IQL hyperparameters.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Parameter** & **Value** \\ \hline optimizer & AdamW (Kingma \& Ba, 2014; Loshchilov \& Hutter, 2017) \\ training iterations & 500000 \\ batch size & 64 \\ sequence length & 16 \\ learning rate & 3e-4 \\ weight decay & 0.0 \\ state encoder & Chaotic-Dwarven-GPT-5(Hambro et al., 2022a,b) \\ LSTM hidden dim & 2048 \\ batch size & 64 \\ sequence length & 16 \\ learning rate & 3e-4 \\ weight decay & 0.0 \\ state encoder & Chaotic-Dwarven-GPT-5(Hambro et al., 2022a,b) \\ LSTM hidden dim & 2048 \\ LSTM layers & 2 \\ LSTM dropout & 0.0 \\ use previous action & True \\ tau (\(\tau\)) & 5e-3 \\ gamma (\(\gamma\)) & 0.999 \\ reward clip range & [-10.0, 10.0] \\ temperature & 1.0 \\ advantage clip max & 100 \\ \hline \hline \end{tabular}
\end{table}
Table 8: AWAC hyperparameters.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Parameter** & **Value** \\ \hline optimizer & AdamW (Kingma \& Ba, 2014; Loshchilov \& Hutter, 2017) \\ training iterations & 500000 \\ batch size & 64 \\ sequence length & 16 \\ learning rate & 3e-4 \\ weight decay & 0.0 \\ state encoder & Chaotic-Dwarven-GPT-5(Hambro et al., 2022a,b) \\ LSTM hidden dim & 2048 \\ LSTM layers & 2 \\ LSTM dropout & 0.0 \\ use previous action & True \\ tau (\(\tau\)) & 5e-3 \\ gamma (\(\gamma\)) & 0.999 \\ reward clip range & [-10.0, 10.0] \\ temperature & 1.0 \\ advantage clip max & 100 \\ \hline \hline \end{tabular}
\end{table}
Table 9: REM hyperparameters.

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Alpha**\(\left(\alpha\right)\) & **Return** \\ \hline
0.0001 & 526.72 \(\pm\) 71.37 \\
0.0005 & 396.50 \(\pm\) 39.35 \\
0.001 & 395.56 \(\pm\) 139.92 \\
0.05 & 226.55 \(\pm\) 196.45 \\
0.01 & 32.42 \(\pm\) 43.69 \\
0.1 & 14.04 \(\pm\) 10.29 \\
0.5 & 0.00 \(\pm\) 0.00 \\
1.0 & 0.59 \(\pm\) 0.45 \\ \hline \hline \end{tabular}
\end{table}
Table 10: The effect of CQL policy constraint strength on the performance. Results, which are averaged across 3 seeds, are sorted based on the final unnormalized game score.

Results per Benchmark Categories

In this section, we report the results stratified by the introduced categories. If one is willing to inspect specific datasets, we organized all training logs into Weights&Biases public reports, found at [https://wandb.ai/tlab/NetHack/reports](https://wandb.ai/tlab/NetHack/reports).

Note that one can find all the evaluation scores (for more than one checkpoint) within the runs and use them for any evaluation tools of interest. Also, we provide convenient scripts for constructing RLiable (Agarwal et al., 2021) graphs based on the provided runs that can be configured for one's purposes as well (see [https://github.com/corl-team/katakomba/tree/main/scripts/rliable_report.py](https://github.com/corl-team/katakomba/tree/main/scripts/rliable_report.py)).

Figure 3: Normalized performance under the Katakomba benchmark for Base datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 1950 points for constructing these graphs.

Figure 4: Normalized performance under the Katakomba benchmark for Extended datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 2250 points for constructing these graphs.

Figure 5: Normalized performance under the Katakomba benchmark for Complete datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 1500 points for constructing these graphs.

Figure 6: Death levels under the Katakomba benchmark for Base datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 1950 points for constructing these graphs.

Figure 7: Death level under the Katakomba benchmark for Extended datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 2250 points for constructing these graphs.

Figure 8: Death levels under the Katakomba benchmark for Complete datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 1500 points for constructing these graphs.

Figure 9: Unnormalized in-game score under the Katakomba benchmark for Base datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 1950 points for constructing these graphs.

Figure 11: Unnormalized in-game score under the Katakomba benchmark for Complete datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 1500 points for constructing these graphs.

Figure 10: Unnormalized in-game score under the Katakomba benchmark for Extended datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 2250 points for constructing these graphs.