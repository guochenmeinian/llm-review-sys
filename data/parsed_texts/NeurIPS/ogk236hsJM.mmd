# One-Step Diffusion Distillation through

Score Implicit Matching

Weijian Luo

Peking University

luoweijian@stu.pku.edu.cn

&Zemin Huang

Westlake University

huangzemin@westlake.edu.cn

&Zhengyang Geng

Carnegie Mellon University

zgeng2@cs.cmu.edu

&J. Zico Kolter

Carnegie Mellon University

zkolter@cs.cmu.edu

&Guo-jun Qi

Westlake University

guojjun@gmail.com

Correspondence to Guo-jun Qi. The project was initiated and supported by the MAPLE lab of Westlake University.

###### Abstract

Despite their strong performances on many generative tasks, diffusion models require a large number of sampling steps in order to generate realistic samples. This has motivated the community to develop effective methods to distill pre-trained diffusion models into more efficient models, but these methods still typically require few-step inference or perform substantially worse than the underlying model. In this paper, we present Score Implicit Matching (SIM) a new approach to distilling pre-trained diffusion models into single-step generator models, while maintaining almost the same sample generation ability as the original model as well as being data-free with no need of training samples for distillation. The method rests upon the fact that, although the traditional score-based loss is intractable to minimize for generator models, under certain conditions we _can_ efficiently compute the _gradients_ for a wide class of score-based divergences between a diffusion model and a generator. SIM shows strong empirical performances for one-step generators: on the CIFAR10 dataset, it achieves an FID of 2.06 for unconditional generation and 1.96 for class-conditional generation. Moreover, by applying SIM to a leading transformer-based diffusion model, we distill a single-step generator for text-to-image (T2I) generation that attains an aesthetic score of 6.42 with no performance decline over the original multi-step counterpart, clearly outperforming the other one-step generators including SDXL-TURBO of 5.33, SDXL-LIGHTNING of 5.34 and HYPER-SDXL of 5.85. We will release this industry-ready one-step transformer-based T2I generator along with this paper.

## 1 Introduction

Over the past years, diffusion models (DMs) [21, 67, 65] have shown significant advancements across a broad spectrum of applications, ranging from data synthesis [25, 26, 51, 52, 22, 56, 23, 31], to density estimation [32, 8], text-to-image generation[54, 60, 2, 80, 7], text-to-3D creation [56, 74, 28, 34], image editing [47, 9, 19, 1, 30, 49], and beyond [83, 79, 5, 85, 18, 59, 14, 73, 89, 72, 42, 78, 44, 84, 13, 11, 46, 16, 71, 55, 10]. From a high level point of view, diffusion models, also framed as score-based diffusion models, use diffusion processes to corrupt the data distribution. They are then trained to approximate the score functions of the noisy data distributions across varying noise levels.

Diffusion models have multiple advantages, such as training flexibility, scalability, and the ability to produce high-quality samples, making them a favored choice for modern AIGC models. After training, the learned score functions can be used to reverse the data corruption process, which can be implemented by numerically solving the associated stochastic differential equation. Such a data generation mechanism usually requires many neural network evaluations, which leads to a significant limitation of DMs: _the generation performance of DMs degrades substantially when the number of sampling steps is reduced_. This shortcoming restricts the practical deployment of DMs, particularly where quick inference is crucial, such as on devices with limited computational capacities like mobile phones and edge devices, or in applications requiring rapid response times.

This challenge has spurred a variety of approaches aimed at expediting the sampling process of diffusion models while preserving their robust generative capabilities. Distillation approaches, in particular, focus on applying distillation algorithms to transition the knowledge from pre-trained, teacher diffusion models to efficient student-generative models which are capable of producing high-quality samples within a few generation steps.

Figure 1: Time for a Human Preference Study! Could you please tell us which one is better? Hint: the rightmost column is the one-step Latent Consistency Model of PixelArt-\(\alpha\); The left two columns are randomly placed, with one generated from our one-step **SIM-DiT-600M** model, and another generated from the 14-step PixelArt-\(\alpha\) teacher diffusion model. We put the answer in Appendix B.1.

Some works have studied the diffusion distillation algorithm through the lens of probability divergence minimization. For instance, Luo et al. [43], Yin et al. [82] have studied the algorithms that minimize the KL divergence between teacher and one-step student models. Zhou et al. [93] have explored distilling with Fisher divergences, resulting in impressive empirical performances. Though these studies have contributed to the community in both theoretical and empirical aspects with applicable single-step generator models, their theories are built upon specific divergences, namely the Kullback-Leibler divergence and the Fisher divergence, which potentially restrict the distillation performances. A more general framework for understanding and improving diffusion distillation is still lacking.

In this work, we introduce Score Implicit Matching (SIM), a novel framework for distilling pre-trained diffusion models into one-step generator networks while maintaining high-quality generations. To do so, we propose a wide and flexible class of score-based divergences between the (intractable) score function of the generator model and that of the original diffusion model, for arbitrary distance functions between the two score functions. The key technical insight of this work is that although such divergences cannot be computed explicitly, the _gradient_ of these divergences _can_ be computed exactly using a result we call the _score-gradient theorem_, leading to an implicit minimization of the divergence. This lets us efficiently train models based on such divergences.

We evaluate the performance of SIM compared to previous approaches, using different choices of distance functions to define the divergence. Most relatedly, we compare SIM with the Diff-Instruct (DI) [43] method, which uses a KL-based divergence term, and the Score Identity Distillation (SiD) method [93], which we show to be a special case of our approach when the distance function is simply chosen to be the squared \(L_{2}\) distance (though derived in an entirely different fashion). We also show empirically that SIM with a specially-designed Pseudo-Huber distance function shows faster convergences and stronger robustness to hyper-parameters than \(L_{2}\) distance, making the resulting method substantially strong than previous approaches.

Finally, we show that SIM obtains very strong empirical performance in absolute terms relative to past work in the field on CIFAR10 image generation and text-to-image generation. On the CIFAR10 dataset, SIM shows a one-step generative performance with a Frechet Inception Distance (FID) of 2.06 for unconditional generation and 1.96 for class-conditional generation. More qualitatively, distilling a leading diffusion-transformer-based [53] text-to-image diffusion model results in an extremely capable one-step text-to-image generator which we show is almost lossless in terms of generative performances as teacher diffusion model. Particularly, by applying SIM to PixelArt-\(\alpha\)[7], a single-step generator is distilled that reaches an outstanding aesthetic score of \(6.42\) with no performance decline over the original multi-step diffusion model. This remarkably outperforms the other one-step text-to-image generators including SDXL-TURBO [64] of 5.33, SDXL-LIGHTNING [35] of 5.34 and HYPER-SDXL [57] of 5.85. Such a result not only marks a new direction for one-step text-to-image generation but also motivates further studies of distilling diffusion-transformer-based AIGC models in other domains such as video generation.

## 2 Diffusion Models

In this section, we introduce preliminary knowledge and notations about diffusion models and diffusion distillation. Assume we observe data from the underlying distribution \(q_{d}(\bm{x})\). The goal of generative modeling is to train models to generate new samples \(\bm{x}\sim q_{d}(\bm{x})\). The forward diffusion process of DM transforms any initial distribution \(q_{0}=q_{d}\) towards some simple noise distribution,

\[\mathrm{d}\bm{x}_{t}=\bm{F}(\bm{x}_{t},t)\mathrm{d}t+G(t)\mathrm{d}\bm{w}_{t},\] (2.1)

where \(\bm{F}\) is a pre-defined drift function, \(G(t)\) is a pre-defined scalar-value diffusion coefficient, and \(\bm{w}_{t}\) denotes an independent Wiener process. A continuous-indexed score network \(\bm{s}_{\varphi}(\bm{x},t)\) is employed to approximate marginal score functions of the forward diffusion process (2.1). The learning of score networks is achieved by minimizing a weighted denoising score matching objective [70; 67],

\[\mathcal{L}_{DSM}(\varphi)=\int_{t=0}^{T}\lambda(t)\mathbb{E}_{\bm{x}_{0} \sim q_{0},\bm{x}_{t}|\bm{x}_{0}\sim q_{t}(\bm{x}_{t}|\bm{x}_{0})}\|\bm{s}_{ \varphi}(\bm{x}_{t},t)-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0})\|_ {2}^{2}\mathrm{d}t.\] (2.2)

Here the weighting function \(\lambda(t)\) controls the importance of the learning at different time levels and \(q_{t}(\bm{x}_{t}|\bm{x}_{0})\) denotes the conditional transition of the forward diffusion (2.1). After training, the score network \(\bm{s}_{\varphi}(\bm{x}_{t},t)\approx\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t})\) is a good approximation of the marginal score function of the diffused data distribution. High-quality samples from a DM can be drawn by simulating SDE which is implemented by the learned score network [67]. However, the simulation of an SDE is significantly slower than that of other models such as one-step generator models.

## 3 Score Implicit Matching

In this section, we introduce Score Implicit Matching which is a general method tailored for the one-step distillation of score-based diffusion models. We first introduce the problem setup and notations, then introduce a general family of score-based probability divergences and show how SIM can be used to minimize the mentioned divergences. We finally discuss specific choices of the method, such as the choice of distance function, and explore the effect this has on the distillation.

Problem setup.Our starting point is a pre-trained diffusion model specified by the score function

\[\bm{s}_{q_{t}}(\bm{x}_{t})\coloneqq\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t})\] (3.1)

where \(q_{t}(\bm{x}_{t})\)'s are the underlying distribution diffused at time \(t\) according to (2.1). We assume that the pre-trained diffusion model provides a sufficiently good approximation of data distribution, and thus will be the only item of consideration for our approach.

The student model of interest is a single-step generator network \(g_{\theta}\), which can transform an initial random noise \(\bm{z}\sim p_{z}\) to obtain a sample \(\bm{x}=g_{\theta}(\bm{z})\); this network is parameterized by network parameters \(\theta\). Let \(p_{\theta,0}\) denote the data distribution of the student model, and \(p_{\theta,t}\) denote the marginal diffused data distribution of the student model with the same diffusion process (2.1). The student distribution implicitly induces a score function

\[\bm{s}_{p_{\theta,t}}(\bm{x}_{t})\coloneqq\nabla_{\bm{x}_{t}}\log p_{\theta,t }(\bm{x}_{t}),\] (3.2)

and evaluating it is generally performed by training an alternative score network as elaborated later.

### General Score-based Divergences

The goal of one-step diffusion distillation is to let the student distribution \(p_{\theta,0}\) match the data distribution \(q_{0}\). To do so, we propose to match the diffused marginal distribution \(p_{\theta,t}\) and \(q_{t}\) at all diffusion time levels. We can define such an objective via the following general score-based divergence. Assume \(\mathbf{d}:\mathbb{R}^{d}\to\mathbb{R}\) is a scalar-valued proper distance function (i.e., a function that obeys \(\forall\bm{x},\mathbf{d}(\bm{x})\geq 0\) and \(\mathbf{d}(\bm{x})=0\) if and only if \(\bm{x}=\mathbf{0}\)). Given a sampling distribution \(\pi_{t}\) that has larger distribution support than \(p_{t}\) and \(q_{t}\), we can formally define a time-integral score divergence as

\[\mathcal{D}^{[0,T]}(p,q)\coloneqq\int_{t=0}^{T}w(t)\mathbb{E}_{\bm{x}_{t} \sim\pi_{t}}\bigg{\{}\mathbf{d}(\bm{s}_{p_{t}}(\bm{x}_{t})-\bm{s}_{q_{t}}(\bm {x}_{t}))\bigg{\}}\mathrm{d}t,\] (3.3)

where \(p_{t}\) and \(q_{t}\) denote the marginal densities of the diffusion process (2.1) at time \(t\) initialized with \(q\) and \(p\) respectively. \(w(t)\) is an integral weighting function. Clearly, we have \(\mathcal{D}^{[0,T]}(p,q)=0\) if and only if all marginal score functions agree, which implies that \(p_{0}(\bm{x}_{t})=q_{0}(\bm{x}_{t}),\ a.s.\ \pi_{0}\).

### Score Implicit Matching

Based upon this motivation, we would like to minimize the integral score-based divergence between \(p_{\theta}\) and \(q\) in order to train the student model, i.e.,

\[\mathcal{L}(\theta)=\mathcal{D}^{[0,T]}(p_{\theta},q)=\int_{t=0}^{T}w(t) \mathbb{E}_{\bm{x}_{t}\sim\pi_{t}}\big{[}\mathbf{d}(\bm{s}_{p_{\theta,t}}(\bm {x}_{t})-\bm{s}_{q_{t}}(\bm{x}_{t}))\big{]}\mathrm{d}t,\] (3.4)

where we assume that the distribution \(\pi_{t}\) has no parameter dependence of \(\theta\), such as \(\psi_{t}(\bm{x}_{t})=p_{\mathrm{sg}[\theta]}(\bm{x}_{t})\). Taking the gradient with respect to \(\theta\), we have

\[\frac{\partial}{\partial\theta}\mathcal{L}(\theta)=\int_{t=0}^{T}w(t)\mathbb{ E}_{\bm{x}_{t}\sim\pi_{t}}\bigg{[}\mathbf{d}^{\prime}(\bm{s}_{p_{\theta,t}}(\bm {x}_{t})-\bm{s}_{q_{t}}(\bm{x}_{t}))\frac{\partial}{\partial\theta}\bm{s}_{p_{ \theta,t}(\bm{x}_{t})}\bigg{]}\mathrm{d}t,\] (3.5)

where \(\mathbf{d}^{\prime}\) denotes the derivative of \(\mathbf{d}\) wrt. its inputs, i.e. \(\nabla_{\bm{y}}\bm{d}(\bm{y})\). Unfortunately, because the score function is not tractable, it is impossible to compute \(\frac{\partial}{\partial\theta}\bm{s}_{p_{\theta,t}(\bm{x}_{t})}\) directly, rendering such a direct approach impractical.

Fortunately, a key finding of our paper is if we choose the sampling distribution to the diffused implicit distribution, i.e. \(\pi_{t}=p_{\mathrm{sg}[\theta],t}\) where the notation \(\mathrm{sg}[\theta]\) denotes the _stop gradient_ operator that cuts off the parameter dependence of \(\theta\), the loss function (3.4) along with its intractable gradient (3.5) can be minimized efficiently via an gradient-equivalent loss. This relies on our Theorem 3.1.

**Theorem 3.1** (Score-divergence gradient Theorem).: If distribution \(p_{\theta,t}\) satisfies some mild regularity conditions, we have for any score function \(\bm{s}_{q_{t}}(.)\), the following equation holds for all parameter \(\theta\):

\[\mathbb{E}_{\bm{x}_{t}\sim p_{\mathrm{sg}[\theta],t}}\bigg{[} \mathbf{d}^{\prime}(\bm{s}_{p_{\theta,t}}(\bm{x}_{t})-\bm{s}_{q_{t}}(\bm{x}_{t }))\frac{\partial}{\partial\theta}\bm{s}_{p_{\theta,t}(\bm{x}_{t})}\bigg{]}\] (3.6) \[=-\frac{\partial}{\partial\theta}\mathbb{E}_{\bm{s}_{q_{0}}\sim p _{\theta,0}\atop\bm{x}_{t}|\bm{x}_{0}\sim q_{t}(\bm{x}_{t}|\bm{x}_{0})}\bigg{[} \bigg{\{}\mathbf{d}^{\prime}(\bm{s}_{p_{\mathrm{sg}[\theta],t}}(\bm{x}_{t})- \bm{s}_{q_{t}}(\bm{x}_{t}))\bigg{\}}^{T}\bigg{\{}\bm{s}_{p_{\mathrm{sg}[ \theta],t}}(\bm{x}_{t})-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0}) \bigg{\}}\bigg{]}.\]

The key observation here is that we replace the intractable _gradient_ of the score function on the left-hand side of (3.6) with a much affordable _evaluation_ of the score function on the right-hand side, the latter of which can be accomplished much more easily using a separate approximation network. This theorem can be proved by using score-projection identity [70, 93] which was first introduced to bridge denoising score matching with denoising auto-encoders. However, the key in proving Theorem 3.1 is a proper choice of \(\theta\)-parameter (in)dependence by appropriately stopping the gradients shown in this theorem. We provide the detailed proof in Appendix A.1.

Now it is ready to reveal the objective we will use to train the implicit generator \(g_{\theta}\). A direct result of (3.6) is the gradient (3.5) can be realized via minimizing a tractable loss function

\[\mathcal{L}_{SIM}(\theta)=\int_{t=0}^{T}w(t)\mathbb{E}_{\bm{s} \sim p_{x},\bm{x}_{0}=q_{\theta}(\bm{x}),\atop\bm{x}_{t}|\bm{x}_{0}\sim q_{t} (\bm{x}_{t}|\bm{x}_{0})}\bigg{\{}-\mathbf{d}^{\prime}(\bm{y}_{t})\bigg{\}}^{ T}\bigg{\{}\bm{s}_{p_{\mathrm{sg}[\theta],t}}(\bm{x}_{t})-\nabla_{\bm{x}_{t}} \log q_{t}(\bm{x}_{t}|\bm{x}_{0})\bigg{\}}\mathrm{d}t\] (3.7)

with \(\bm{y}_{t}\coloneqq\bm{s}_{p_{\mathrm{sg}[\theta],t}}(\bm{x}_{t})-\bm{s}_{q_{t }}(\bm{x}_{t})\). By Theorem 3.1, this alternative loss has an identical gradient to that of the original loss without the need to access the gradient of the score network.

In practice, we can use another online diffusion model \(\bm{s}_{\psi}(\bm{x}_{t},t)\) to approximate the generator model's score function \(\bm{s}_{p_{\mathrm{sg}[\theta],t}}(\bm{x}_{t})\) pointwise, which was also done in previous works such as Luo et al. [43], Zhou et al. [93], and Yin et al. [82]. _We name the distillation method that minimizes the objective \(\mathcal{L}_{SIM}(\theta)\) in (3.7) the Score Implicit Matching (SIM) because the learning process implicitly matches the intractable marginal score function \(\bm{s}_{p_{\mathrm{sg},t}}(.)\) of the implicit student model with the explicit score function of the pre-trained diffusion model \(\bm{s}_{q_{t}}(.)\)._

The complete algorithm for SIM is shown in Algorithm 1, which trains the student model through two alternative phases between learning the marginal score function \(\bm{s}_{\psi}\), and updating the generator model with gradient (3.7). The former phase follows the standard DM learning procedure, i.e., minimizing the denoising score matching loss function (2.2), with a slight change that the sample is generated from the generator. The resulting \(\bm{s}_{\psi}(\bm{x}_{t},t)\) provides a good pointwise estimation of \(\bm{s}_{p_{\mathrm{sg}[\theta],t}}(\bm{x}_{t})\)The latter phase updates the generator's parameter \(\theta\) by minimizing the loss function (3.7), where two needed functions are provided by pretrained DM \(\bm{s}_{q_{t}}(\bm{x}_{t})\) and learned DM \(\bm{s}_{\psi}(\bm{x}_{t},t)\).

### Instances of Score Implicit Matching.

The previous section introduced the SIM algorithm without choosing a specific distance function \(\mathbf{d}(.)\). Here we discuss different choices and their influence on the distillation process. We also show that in the SIM framework, the SiD can be viewed as a special case.

The Design Choice of Distance Function \(\mathbf{d}(.)\).Clearly, various choices of distance function \(\mathbf{d}(.)\) result in different distillation algorithms. Perhaps the most natural choice of the distance function is a simple squared distance, i.e. \(\mathbf{d}(\bm{y}_{t})=\|\bm{y}_{t}\|_{2}^{2}\). The corresponding derivative term writes \(\mathbf{d}^{\prime}(\bm{y}_{t})=2\bm{y}_{t}\). In fact, such a loss function recovers the _delta loss_ studied in SiD [93], in which the authors empirically find that such a loss function works satisfactorily (though through a very different derivation). Thus, SiD is in fact a special case of SIM, though the derivation of SiD there does not suggest how alternative losses may be employed. A direct generalization of the quadratic form is the \(\alpha\)-power of the \(\alpha\)-norm where \(\alpha>1\) and \(\alpha\) is even. In this case, the distance function writes \(\mathbf{d}(\bm{y}_{t})=\alpha\bm{y}_{t}^{(\alpha-1)}\) and the resulting loss function is summarized in Table 4 in Appendix A.3.

The Pseudo-Huber distance function.Different from powered norms, we introduce SIM with the Pseudo-Huber distance function, which is defined with \(\bm{d}(\bm{y})\coloneqq\sqrt{\|\bm{y}_{t}\|_{2}^{2}+c^{2}}-c\), where \(c\) is a pre-defined positive constant. The corresponding distillation objective writes

\[\mathcal{L}_{SIM}(\theta)=-\bigg{\{}\frac{\bm{y}_{t}}{\sqrt{\|\bm{y}_{t}\|_{2 }^{2}+c^{2}}}\bigg{\}}^{T}\bigg{\{}\bm{s}_{\psi}(\bm{x}_{t},t)-\nabla_{\bm{x}_ {t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0})\bigg{\}}.\] (3.8)

_In the rest of this paper, we will use the Pseudo-Huber distance as the default choice of the distance, unless specified otherwise._ Due to the limited space, we summarize different choices of distance function and the corresponding loss functions in Table 4 as well as their derivations, along with more discussions in Appendix A.3.

Particularly, unlike SiD (the \(L^{2}\) case in Table 4), with the Pseudo-Huber distance in the SIM, we observe that the vector \(\bm{y}_{t}\) is naturally normalized adaptively by dividing by a squared root of the vector. Such a normalization can stabilize the training loss, resulting in a robust and fast-converging distillation process. In section 4.1, we conduct empirical experiments to show three advantages: robustness to large-learning rate, fast convergence, and improved performances.

### Related Works

Diffusion distillation [41] is a research area that aims to reduce generation costs using teacher diffusion models. It involves three primary distillation methods: 1) _Trajectory Distillation:_ This method trains a student model to mimic the generation process of diffusion models with fewer steps. Direct distillation ([39; 15]) and progressive distillation ([61; 48]) variants predict less noisy data from noisy inputs. Consistency-based methods ([68; 29; 66; 36; 17]) minimize the self-consistency metric. These require true data samples for training. 2) _Distributional Matching:_ It focuses on aligning the student's generation distribution with that of a teacher diffusion model. Among them are adversarial training methods ([76; 77]) requiring real data for distilling diffusion models. Another important line of methods attempts to minimize divergences like KL ([82]) such as Diff-Instruct (DI) [45; 82] and Fisher divergence such as Score identity Distillation (SiD) ([93]), often without needing real data. Though SIM has gotten inspiration from SiD and DI, the gap between SIM and SiD and DI is significant. SIM not only offers solid mathematical foundations which may lead to a deep understanding of diffusion distillation, but also provides substantial flexibility in using different distance functions, resulting in strong empirical performances when using specific Pseudo-Huber distance. 3) _Other Methods:_ Methods like operator learning ([86]), ReFlow ([37]), and FMM [3] provide alternative insights into distillation. Moreover, many works made outstanding efforts to scale up diffusion distillation to one-step text-to-image generation and beyond[40; 50; 69; 82; 92]

## 4 Experiments

### One-step CIFAR10 Generation

Experiment Settings.In this experiment, we apply SIM to distill the pre-trained EDM [26] diffusion models into one-step generator models on the CIFAR10 [33] dataset. We follow the same setting as DI [43] and SiD [93] to distill the diffusion model into a one-step generator. Details can be found in Appendix B.2. We refer to the high-quality codebase of SiD [93]3 to reproduce its results by closely referring to its configurations on our devices. We also re-implement the DI under the same experiment settings.

Footnote 3: https://github.com/mingyuanzhou/SiD

Performances.We evaluate the performance of the trained generator via Frechet Inception Distance (FID) [20], which is the lower the better. We refer to the evaluation protocols in [43] for comparison 4. Table 1 and 2 summarize the FID of generative models on CIFAR10 datasets. We reproduce the SiD and the DI with the same computing environments and evaluation protocol as SIM for a fair comparison. Models in the upper part of the table have different architectures or diffusion models from the EDM model, while the models in the lower part of the tables share exactly the same architecture and the teacher EDM diffusion models, which thus are directly comparable.

Footnote 4: https://github.com/pkulwj1994/diff_instruct

As shown in Table 1, for the CIFAR10 unconditional generation task, the proposed SIM achieves a decent FID of \(2.06\) with only one generation step, outperforming SiD and DI with the same training compute. It is on par with the CTM and the SiD's official implementation which are trained to fully converge with training costs of hundreds of GPU days. For the class-conditional generation in Table 2, the SIM achieves an FID of 1.96, acting among top-performing models.

The CIFAR-10 generation tasks are much toyish as merely performed with diffusion models of limited capacities on a simple dataset. We will perform experiments to distill from top-performing transformer-based diffusion models for text-to-image generation tasks. We will show that the one-step T2I generator distilled by SIM demonstrates state-of-the-art results over other industry-level models.

\begin{table}
\begin{tabular}{l c c} \hline \hline Method & NFE (\(\downarrow\)) & FID (\(\downarrow\)) \\
**Different Architecture as EDM Model** & \\ \hline \hline BigGAN [4] & 1 & 14.73 \\ BigGAN+Tune[4] & 1 & 8.47 \\ StyleGAN2 [25] & 1 & 6.96 \\ MultiHing[27] & 1 & 6.40 \\ FQ-GAN [87] & 1 & 5.59 \\ StyleGAN2-ADA [24] & 1 & 2.42 \\ StyleGAN2-ADA+DI [43] & 1 & 2.27 \\ StyleGAN2 + SMaRT [75] & 1 & 2.06 \\ StyleGAN-XL [63] & 1 & 1.85 \\ \hline \hline
**Same Architecture as EDM[26]** & 35 & 1.97 \\ EDM [26] & 20 & 2.54 \\ EDM [26] & 10 & 15.56 \\ EDM [26] & 1 & 314.81 \\ GET [15] & 1 & 6.25 \\ DFT-Distruct [43] & 1 & 4.19 \\ DMD (w.o. reg) [82] & 1 & 5.58 \\ DMD (w.o. KL) [82] & 1 & 3.82 \\ DMD [82] & 1 & 2.66 \\ CTM [29] & 1 & 1.73 \\ CTM [29] & 2 & **1.63** \\ SiD (\(\alpha=1.0\)) [93] & 1 & 1.93 \\ SiD (\(\alpha=1.2\))[93] & 1 & 1.71 \\
**SIM (ours)** & 1 & 1.96 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Class-conditional sample quality on CIFAR10 dataset. \(\dagger\) means method we reproduced.

\begin{table}
\begin{tabular}{l c} \hline \hline Method & NFE (\(\downarrow\)) & FID (\(\downarrow\)) \\
**Different Architecture as EDM Model** & \\ \hline \hline BigGAN [4] & 1 & 14.73 \\ BigGAN+Tune[4] & 1 & 8.47 \\ StyleGAN2 [25] & 1 & 6.96 \\ MultiHing[27] & 1 & 6.40 \\ FQ-GAN [87] & 1 & 5.59 \\ StyleGAN2-ADA [24] & 1 & 2.42 \\ StyleGAN2-ADA+DI [43] & 1 & 2.27 \\ StyleGAN2 + SMaRT [75] & 1 & 2.06 \\ StyleGAN-XL [63] & 1 & 1.85 \\ \hline
**Same Architecture as EDM[26]** & **Model** \\ \hline \hline EDM [26] & 35 & 1.82 \\ EDM [26] & 20 & 2.54 \\ EDM [26] & 10 & 15.56 \\ EDM [26] & 1 & 314.81 \\ GET [15] & 1 & 6.25 \\ Diff-Distruct [43] & 1 & 4.19 \\ DMD (w.o. reg) [82] & 1 & 5.58 \\ DMD (w.o. KL) [82] & 1 & 3.82 \\ DMD [82] & 1 & 2.66 \\ CTM [29] & 1 & 1.73 \\ CTM [29] & 2 & **1.63** \\ SiD (\(\alpha=1.0\)) [93] & 1 & 1.93 \\ SiD (\(\alpha=1.2\))[93] & 1 & 1.71 \\
**SIM (ours)** & 1 & 1.96 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Unconditional sample quality on CIFAR-10. \(\dagger\) means method we reproduced.

Before that let us further look into some advantages of SIM - robustness to large learning rate and faster convergences - over SiD and DI on CIFAR-10, which will shed some light on how distillation methods scale up to more complex tasks with much larger neural networks.

Robustness to large learning rate.We apply SIM, SiD, and DI under the same settings to distill from EDM (details in Appendix) on the CIFAR10 unconditional generation task, with a learning rate of \(1e-4\), and plot the Fretchet Inception Distance (FID) [20] and the Inception Score [62] in Figure 2. Both the DI and the SiD are unstable even in the early training phase, while the SIM can steadily converge even with a large learning rate. The potential reason is that SIM naturally normalizes the loss objective to keep its scale from changing abruptly along the training process. _This distinguishes SIM from SiD in practice for training large models, because training modern large models is so expensive that researchers often have few chances to adjust the hyperparameters within budget._

Fast convergence.The second advantage of SIM is its faster convergence than SiD 5. To show this, we follow the same setting as SiD on CIFAR10 unconditional generation. As shown in Figure 2 and Figure 3, under all configurations, the SIM consistently shows better FID and Inception Scores under the same training iterations. Due to page limitations, we put more details in Appendix B.2.

Experiments on CIFAR10 generation show that SIM is a strong, robust, yet fast converging one-step diffusion distillation algorithm. However, the power of SIM is not restricted to a toy CIFAR-10 benchmark. In section 4.2, we apply the SIM to distill a 0.6B DiT [53]) based text-to-image diffusion model and obtain the state-of-the-art transformer-based one-step generator.

Footnote 5: We find that the DI converges fast but suffers from mode-collapse issues. So we do not compare with it.

### Transformer-based One-step Text-to-Image Generator

Experiment Settings.In recent years, transformer-based text-to-X generation models have gained great attention across image generations such as Stable Diffusion V3 [12] and video generation such as Sora [6]. In this section, we apply SIM to distill one of the leading open-sourced DiT-based diffusion models that have gained lots of attention recently: the 0.6B PixelArt-\(\alpha\) model [7], which is built upon with DiT model [53], resulting in the state-of-the-art one-step generator in terms of both quantitative evaluation metric and subjective user studies.

Experiment Settings and Evaluation Metrics.The goal of one-step distillation is to accelerate the diffusion model into one-generation steps while maintaining or even outperforming the teacher diffusion model's performances. To verify the performance gap between our one-step model and the diffusion model, we compare four quantitative values: the aesthetic score, the PickScore, the Image Reward, and our user-studied comparison score. On the SAM-LLaVA-Caption10M, which is one of the datasets the original PixelArt-\(\alpha\) model is trained on, we compare the SIM one-step model, which we called the **SIM-DiT-600M**, with the PixelArt-\(\alpha\) model with a 14-step DPM-Solver[38] to evaluate the in-data performance gap. We also compare the SIM-DiT-600M and PixelArt-\(\alpha\) with other few-step models, such as LCM [40], TCD [91], PeReflow [81], and Hyper-SD [57] series on the widely used COCO-2017 validation dataset. We refer to Hyper-SD's evaluation protocols to compute evaluation metrics. Table 3 summarizes the evaluation performances of all models. For the human preference study against PixArt-\(\alpha\) and SIM-DiT-600M, we randomly select 17 prompts from the SAM Caption dataset and generate images with both PixArt-\(\alpha\) and SIM-DiT-600M, then

Figure 2: **Left Two:** Comparison of distillation methods with a batch size of 256 and a learning rate of \(1e-4\). _(Left):_ the FID value. _(Right)_: the Inception Scores. **Right Two:** Comparison of distillation methods with a batch size of 256 and a learning rate of \(1e-5\). _(Left):_ the FID value. _(Right):_ the Inception Scores. All methods are constrained to the same settings except for the distillation methods.

ask the studied user to choose their preference according to image quality and alignments with the prompts. Figure 1 shows a visualization of our user study cases, in which it is difficult to distinguish the images from PixArt-\(\alpha\) and SIM-DiT-600M.

Almost lossless one-step distillation.It is surprising that SIM-DiT-600M achieves almost no performance loss compared to teacher diffusion models. For instance, on the SAM Caption dataset in Table 3, SIM-DiT-600M recovers \(99.6\%\) aesthetic score of PixArt-\(\alpha\) model and \(100\%\) PickScore. However, the SIM-DiT-600M shows a slightly smaller Image Reward, which can be potentially optimized with more training computes. When compared with leading few-step text-to-image models such as SDXL-Turbo, SDXL-lightning, and Hyper-SDXL, the SIM-DiT-600M shows a dominant aesthetic score with a significant margin, together with a decent Image Reward and Pick Score.

Besides the top performance, the training cost of SIM-DiT-600M is surprisingly cheap. Our best model is trained (data-freely) with 4 A100-80G GPUs for 2 days, while other models in Table 3 require hundreds of A100 GPU days. We summarize the distillation costs in Table 3, marking that SIM is a super efficient distillation method with astonishing scaling ability. We believe such efficiency comes from two properties of SIM. First, the SIM is data-free, making the distillation process not need ground truth image data. Second, the use of the Pseudo-Huber distance function (3.3) adaptively normalizes the loss function, resulting in robustness to hyper-parameters and training stability.

Qualitative comparison.Figure 3 qualitatively compares SIM-DiT-600M against other leading few-step text-to-image generative models. It is obvious that SIM-DiT-600M generates images with higher aesthetic performances than other models. This reflects the quantitative results in Table 3 where the SIM-DiT-600M reaches a high aesthetic score. Both the quantitative and qualitative results showcase the SIM-DiT-600M as the top-performing one-step text-to-image generator. Please check our supplementary materials for more qualitative evaluations.

Figure 4: Visualization of bad generation cases of one-step SIM-DiT model.

Figure 3: Qualitative comparison of SIM-DiT-600M against other few-step text-to-image models. Please zoom in to check details, lighting, and aesthetic performances. Prompts in Appendix B.7.

**Failure Cases of One-step SIM-DiT Model.** Though the SIM-DiT one-step model shows impressive performances, it inevitably has limitations. For instance, we find that the 0.6B SIM-DiT one-step model sometimes fails to generate high-quality tiny human faces and proper human arms and fingers. Besides, the model sometimes generates a wrong number of objects and contents that do not strictly follow the prompts. We believe that scaling up the model size and teacher diffusion models will help to address these issues. Please refer to Figure 4 for visualization of failure cases.

## 5 Conclusion and Future Works

This paper presents a novel diffusion distillation method, the score implicit matching (SIM), which enables to transform pre-trained multi-step diffusion models into one-step generators in a data-free fashion. The theoretical foundations and practical algorithms introduced in this paper can enable more affordable deployment of single-step generators across various domains and applications at scale without compromising the performance of underlying generative models.

Nonetheless, SIM has its limitations that call for further research. First, with the abundance of other powerful pre-trained generative models such as flow-matching models, it is worth exploring to reveal if it is possible to generalize the application of SIM to such a broader family of generative models. Second, even though data-free is an important feature of SIM, incorporating new data in the SIM can further boost the quality of generated images failed by the teacher model. This potential benefit has yet to be explored. We hope this could ease the training of large generative models.

## Acknowledgement

Zhengyang Geng is supported by funding from the Bosch Center for AI. Zico Kolter gratefully acknowledges Bosch's funding for the lab.

We would like to acknowledge constructive suggestions from reviewers and ACs/SACs/PCs of NeurIPS 2024. We acknowledge Dr. Mingyuan Zhou for his constructive suggestions on the representation of our theoretical results. We also acknowledge the authors of Diff-Instruct and Score-identity Distillation for their great contributions to high-quality diffusion distillation Python code. We appreciate the authors of PixelArt-\(\alpha\) for making their DiT-based diffusion model public.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{**Steps**} & \multirow{2}{*}{**Type**} & \multirow{2}{*}{**Params**} & **AS** & **Image** & **Pick** & **User** & **Distill** \\  & & & & & **Score** & **Reward** & **Score** & **Pref** & **Cost** \\ \hline SD15-Base [58] & 25 & UnNet & 860 M & 5.26 & 0.18 & 0.217 & & \\ SD15-LSTM [40] & 4 & UnNet & 860 M & 5.66 & -0.37 & 0.212 & & \\ SD15-TCD [91] & 4 & UnNet & 860 M & 5.45 & -0.15 & 0.214 & & \\ P\({}_{\text{F}}\)RF\({}_{\text{LOW}}\)[81] & 4 & UnNet & 860 M & 5.64 & -0.35 & 0.208 & M GPU\(\times\) N Days \\ Hyper-SD15[57] & 1 & UnNet & 860 M & 5.79 & 0.29 & 0.215 & & \\ \hline SDXL-Loss [58] & 25 & UnNet & 2.6 B & 5.54 & 0.87 & 0.229 & & \\ SDXL-LSTM [40] & 4 & UnNet & 2.6 B & 5.42 & 0.48 & 0.224 & & \\ SDXL-TCD [91] & 4 & UnNet & 2.6 B & 5.42 & 0.67 & 0.226 & & \\ SDXL-Lightworth [35] & 4 & UnNet & 2.6 B & 5.63 & 0.72 & 0.229 & & 64 A100\(\times\) N Days \\ Hyper-SDXL[57] & 4 & UnNet & 2.6 B & 5.74 & 0.93 & 0.232 & & 32 A100\(\times\) N Days \\ SDXL-Turb0 [64] & 1 & UnNet & 2.6 B & 5.33 & 0.78 & 0.228 & & M GPU\(\times\) N Days \\ SDXL-Lightworth [35] & 1 & UnNet & 2.6 B & 5.34 & 0.54 & 0.223 & & 64 A100\(\times\) N Days \\ Hyper-SDXL[57] & 1 & UnNet & 2.6 B & 5.85 & 1.19 & 0.231 & & 32 A100\(\times\) N Days \\ P\({}_{\text{Y}}\)Axnet-\(\alpha\)[7] & 30 & DPT & 610 M & 5.97 & 0.82 & 0.226 & & \\
**SIM-DT-600M** & 1 & DPT & 610 M & 6.42 & 0.67 & 0.223 & & 4 A100\(\times\) 2 days \\ \hline P\({}_{\text{Y}}\)Axnet-\(\alpha\)[7] & 30 & DPT & 610 M & 5.93 & 0.53 & 0.223 & 54.88 & \\
**SIM-DT-600M** & 1 & DPT & 610 M & 5.91 & 0.44 & 0.223 & 45.12\(\times\) & 4 A100\(\times\) 2 days \\ \hline \hline \end{tabular}
\end{table}
Table 3: Quantitative comparisons with frontier text-to-image models on COCO-2017 validation dataset. The user preference is the winning rate of our user study on SIM-DiT-600M against 20-step PixelArt-\(\alpha\). \(*\) means the results evaluated on the SAM-LLaVA-Caption10M dataset, and SIM-DiT-600M means the SIM generator distilled from PixelArt-\(\alpha\)-600M, excluding those in the T5 text encoder. The distillation cost _M GPU\(\times\) N Days_ means the model did not report the cost.

## References

* Avraham et al. [2023] Omri Avraham, Ohad Fried, and Dani Lischinski. Blended latent diffusion. _ACM Transactions on Graphics (TOG)_, 42(4):1-11, 2023.
* Balaji et al. [2022] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. ediff-i: Text-to-image diffusion models with ensemble of expert denoisers. _arXiv preprint arXiv:2211.01324_, 2022.
* Boffi et al. [2024] Nicholas M Boffi, Michael S Albergo, and Eric Vanden-Eijnden. Flow map matching. _arXiv preprint arXiv:2406.07507_, 2024.
* Brock et al. [2019] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In _International Conference on Learning Representations_, 2019. URL https://openreview.net/forum?id=B1xsgj09Fm.
* Brooks et al. [2023] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18392-18402, 2023.
* Brooks et al. [2024] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. URL https://openai.com/research/video-generation-models-as-world-simulators.
* Chen et al. [2023] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-\(\alpha\): Fast training of diffusion transformer for photorealistic text-to-image synthesis. _ArXiv_, abs/2310.00426, 2023. URL https://api.semanticscholar.org/CorpusID:263334265.
* Chen et al. [2019] Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and Jorn-Henrik Jacobsen. Residual flows for invertible generative modeling. In _Advances in Neural Information Processing Systems_, pages 9916-9926, 2019.
* Couairon et al. [2022] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based semantic image editing with mask guidance. _ArXiv_, abs/2210.11427, 2022.
* Deng et al. [2024] Wei Deng, Weijian Luo, Yixin Tan, Marin Bilos, Yu Chen, Yuriy Nevmyvaka, and Ricky TQ Chen. Variational \(\mathrm{schr}\backslash^{\top}\)dinger diffusion models. _arXiv preprint arXiv:2405.04795_, 2024.
* Deng et al. [2024] Wei Deng, Weijian Luo, Yixin Tan, Marin Bilos, Yu Chen, Yuriy Nevmyvaka, and Ricky TQ Chen. Variational schrodinger diffusion models. In _Forty-first International Conference on Machine Learning_, 2024.
* Esser et al. [2024] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. _arXiv preprint arXiv:2403.03206_, 2024.
* Feng et al. [2023] Yasong Feng, Weijian Luo, Yimin Huang, and Tianyu Wang. A lipschitz bandits approach for continuous hyperparameter optimization. _arXiv preprint arXiv:2302.01539_, 2023.
* Gal et al. [2022] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_, 2022.
* Geng et al. [2023] Zhengyang Geng, Ashwini Pokle, and J Zico Kolter. One-step diffusion distillation via deep equilibrium models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=b6KvK2de99.
* Geng et al. [2024] Zhengyang Geng, Ashwini Pokle, William Luo, Justin Lin, and J Zico Kolter. Consistency models made easy. _arXiv preprint arXiv:2406.14548_, 2024.

* [17] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Josh Susskind. Boot: Data-free distillation of denoising diffusion models with bootstrapping. _arXiv preprint arXiv:2306.05544_, 2023.
* [18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. _arXiv preprint arXiv:2208.01626_, 2022.
* [19] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. _arXiv preprint arXiv:2304.07090_, 2023.
* [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In _Advances in Neural Information Processing Systems_, pages 6626-6637, 2017.
* [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [22] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. _arXiv preprint arXiv:2204.03458_, 2022.
* [23] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac, and Max Welling. Equivariant diffusion for molecule generation in 3d. In _International Conference on Machine Learning_, pages 8867-8887. PMLR, 2022.
* [24] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. _Advances in Neural Information Processing Systems_, 33, 2020.
* [25] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 8110-8119, 2020.
* [26] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In _Proc. NeurIPS_, 2022.
* [27] Ilya Kavalerov, Wojciech Czaja, and Rama Chellappa. A multi-class hinge loss for conditional gans. In _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pages 1290-1299, 2021.
* [28] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6007-6017, 2023.
* [29] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. _arXiv preprint arXiv:2310.02279_, 2023.
* [30] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 2426-2435, 2022.
* [31] Heeseung Kim, Sungwon Kim, and Sungroh Yoon. Guided-tts: A diffusion model for text-to-speech via classifier guidance. In _International Conference on Machine Learning_, pages 11119-11133. PMLR, 2022.
* [32] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems 31_, pages 10215-10224. 2018.
* [33] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 Dataset. _online: http://www. cs. toronto. edu/kriz/cifar. html_, 55, 2014.

* [34] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 300-309, 2023.
* [35] Shanchuan Lin, Anran Wang, and Xiao Yang. Sdxl-lightning: Progressive adversarial diffusion distillation. _arXiv preprint arXiv:2402.13929_, 2024.
* [36] Hongjian Liu, Qingsong Xie, Zhijie Deng, Chen Chen, Shixiang Tang, Fueyang Fu, Zhengjun Zha, and Haonan Lu. Scott: Accelerating diffusion models with stochastic consistency distillation. _arXiv preprint arXiv:2403.01505_, 2024.
* [37] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. _arXiv preprint arXiv:2209.03003_, 2022.
* [38] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion probabilistic model sampling in around 10 steps. _arXiv preprint arXiv:2206.00927_, 2022.
* [39] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. _arXiv preprint arXiv:2101.02388_, 2021.
* [40] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high-resolution images with few-step inference. _arXiv preprint arXiv:2310.04378_, 2023.
* [41] Weijian Luo. A comprehensive survey on knowledge distillation of diffusion models. _arXiv preprint arXiv:2304.04262_, 2023.
* [42] Weijian Luo and Zhihua Zhang. Data prediction denoising models: The pupil outdoes the master, 2024. URL https://openreview.net/forum?id=wYmcfur889.
* [43] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=MLIs5iRq4w.
* [44] Weijian Luo, Hao Jiang, Tianyang Hu, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Training energy-based models with diffusion contrastive divergences. _arXiv preprint arXiv:2307.01668_, 2023.
* [45] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models. _Advances in Neural Information Processing Systems_, 36, 2024.
* [46] Weijian Luo, Boya Zhang, and Zhihua Zhang. Entropy-based training methods for scalable neural implicit samplers. _Advances in Neural Information Processing Systems_, 36, 2024.
* [47] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.
* [48] Chenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. _arXiv preprint arXiv:2210.03142_, 2022.
* [49] Ron Mokady, Amir Hertz, Kfir Aherman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6038-6047, 2023.
* [50] Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. _arXiv preprint arXiv:2312.05239_, 2023.
* [51] Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. _arXiv preprint arXiv:2102.09672_, 2021.

* [52] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. _arXiv preprint arXiv:1609.03499_, 2016.
* [53] William Peebles and Saining Xie. Scalable diffusion models with transformers. _arXiv preprint arXiv:2212.09748_, 2022.
* [54] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.
* [55] Ashwini Pokle, Zhengyang Geng, and J Zico Kolter. Deep equilibrium approaches to diffusion models. _Advances in Neural Information Processing Systems_, 35:37975-37990, 2022.
* [56] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In _The Eleventh International Conference on Learning Representations_, 2022.
* [57] Yuxi Ren, Xin Xia, Yanzuo Lu, Jiacheng Zhang, Jie Wu, Pan Xie, Xing Wang, and Xuefeng Xiao. Hyper-sd: Trajectory segmented consistency model for efficient image synthesis. _arXiv preprint arXiv:2404.13686_, 2024.
* [58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [59] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 22500-22510, 2023.
* [60] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasempour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. _arXiv preprint arXiv:2205.11487_, 2022.
* [61] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In _International Conference on Learning Representations_, 2022. URL https://openreview.net/forum?id=TIdXIPzhoI.
* [62] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In _Advances in neural information processing systems_, pages 2234-2242, 2016.
* [63] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. _ACM SIGGRAPH 2022 Conference Proceedings_, 2022.
* [64] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. _arXiv preprint arXiv:2311.17042_, 2023.
* [65] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR, 2015.
* [66] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. _arXiv preprint arXiv:2310.14189_, 2023.
* [67] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In _International Conference on Learning Representations_, 2020.
* [68] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. _arXiv preprint arXiv:2303.01469_, 2023.
* [69] Yuda Song, Zehao Sun, and Xuanwu Yin. Sdxs: Real-time one-step latent diffusion models with image conditions. _arXiv preprint arXiv:2403.16627_, 2024.

* Vincent [2011] Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders. _Neural Computation_, 23(7):1661-1674, 2011.
* Wang et al. [2024] Yifei Wang, Weimin Bai, Weijian Luo, Wenzheng Chen, and He Sun. Integrating amortized inference with diffusion models for learning clean distribution from corrupted images. _arXiv preprint arXiv:2407.11162_, 2024.
* Wang et al. [2022] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. _arXiv preprint arXiv:2206.02262_, 2022.
* Wang et al. [2023] Zhendong Wang, Yifan Jiang, Huangjie Zheng, Peihao Wang, Pengcheng He, Zhangyang "Atlas" Wang, Weizhu Chen, and Mingyuan Zhou. Patch diffusion: Faster and more data-efficient training of diffusion models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, _Advances in Neural Information Processing Systems_, volume 36, pages 72137-72154. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/e4667dd0a5a54b74019b72b677ed8ec1-Paper-Conference.pdf.
* Wang et al. [2023] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Profilicdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. _arXiv preprint arXiv:2305.16213_, 2023.
* Xia et al. [2021] Mengfei Xia, Yujun Shen, Ceyuan Yang, Ran Yi, Wenping Wang, and Yong-jin Liu. Smart: Improving gans with score matching regularity. _arXiv preprint arXiv:2311.18208_, 2023.
* Xiao et al. [2021] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. In _International Conference on Learning Representations_, 2021.
* Xu et al. [2023] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-to-image generation via diffusion gans. _arXiv preprint arXiv:2311.09257_, 2023.
* Xue et al. [2023] Shuchen Xue, Mingyang Yi, Weijian Luo, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhi-Ming Ma. Sa-solver: Stochastic adams solver for fast sampling of diffusion models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=f6a9XVFYIo.
* Xue et al. [2023] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Text-to-image generation via large mixture of diffusion paths. _ArXiv_, abs/2305.18295, 2023. URL https://api.semanticscholar.org/CorpusID:258959002.
* Yan et al. [2024] Hanshu Yan, Xingchao Liu, Jiachun Pan, Jun Hao Liew, Qiang Liu, and Jiashi Feng. Perflow: Piecewise rectified flow as universal plug-and-play accelerator. _arXiv preprint arXiv:2405.07510_, 2024.
* Yin et al. [2023] Tianwei Yin, Michael Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William T Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. _arXiv preprint arXiv:2311.18828_, 2023.
* Zhang et al. [2023] Boya Zhang, Weijian Luo, and Zhihua Zhang. Enhancing adversarial robustness via score-based optimization. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL https://openreview.net/forum?id=MOAHXRzHhm.
* Zhang et al. [2023] Boya Zhang, Weijian Luo, and Zhihua Zhang. Purify++: Improving diffusion-purification with advanced diffusion models and control of randomness. _arXiv preprint arXiv:2310.18762_, 2023.
* Zhang et al. [2023] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.

* [86] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. _arXiv preprint arXiv:2204.13902_, 2022.
* [87] Yang Zhao, Chunyuan Li, Ping Yu, Jianfeng Gao, and Changyou Chen. Feature quantization improves gan training. _arXiv preprint arXiv:2004.02088_, 2020.
* [88] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzadenesheli, and Anima Anandkumar. Fast sampling of diffusion models via operator learning. _arXiv preprint arXiv:2211.13449_, 2022.
* [89] Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Truncated diffusion probabilistic models and diffusion-based adversarial auto-encoders. _arXiv preprint arXiv:2202.09671_, 2022.
* [90] Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Truncated diffusion probabilistic models and diffusion-based adversarial auto-encoders. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=HDxgaKK9561.
* [91] Jianbin Zheng, Minghui Hu, Zhongyi Fan, Chaoyue Wang, Changxing Ding, Dacheng Tao, and Tat-Jen Cham. Trajectory consistency distillation. _arXiv preprint arXiv:2402.19159_, 2024.
* [92] Mingyuan Zhou, Zhendong Wang, Huangjie Zheng, and Hai Huang. Long and short guidance in score identity distillation for one-step text-to-image generation. _ArXiv 2406.01561_, 2024. URL https://arxiv.org/abs/2406.01561.
* [93] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. _arXiv preprint arXiv:2404.04057_, 2024.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our main claim is that we propose a method, Score Implicit Matching (SIM), that can distill a pre-trained diffusion model into a one-step generator with very strong performances compared to the other SoTA models. We have highlighted important contributions of our method in 3.2 and have provided sufficient evidence to support our central claims in the experiments section 4.1 and 4.2. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations**Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have explcitly stated the limitations of our method in conclusion, and have suggested some potential future work to mitigate these limitations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Though this paper is more an empirically strong paper, we also provide clear assumptions or detailed supporting references for the theoretical statements of the paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes]Justification: We have included details of experimental settings and hyperparameters in B.2. We also plan to release our code to ensure transparency and reproducibility of the results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No], Justification: Since our code has a business policy, we can not release the code at this time. But we plan to release the code once the acceptance. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.

* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have included all relevant experimental details, including details of datasets, hyperparameters, optimizer, etc. both in the main paper, as well as in B.2. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We provide the statistical significance test for the scaling results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of computing workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: We have stated GPU requirements for training our models in 3. We have also included other relevant details of computational requirements in B.2. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute worker CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We follow the Code of Ethics from all the perspectives stated. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our paper presents a diffusion distillation approach to strengthen the efficiency of diffusion models. We do not see social impacts within our subject. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards**Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We use the pre-trained PixelArt-\(\alpha\) text-to-image model as a teacher distillation model, along with its open-sourced training datasets. We have described the details in section 4.2. Since currently we don't plan to open source our text-to-image model, so there is no concern for use to release unsafe contents. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Major assets in our work are: * Datasets CIFAR-10 and SAM-recaptioned: we have cited the original paper that proposed these datasets. * Models PixelArt-\(\alpha\): we have cited the original paper that proposed these datasets. * Our codebase is adapted by modifying code available on github by the authors of EDM [26] and Diff-Instruct [43]. The relevant license is Attribution-NonCommercial-ShareAlike 4.0 International. Our code includes and correctly attributes the relevant copyright information. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]Justification: We do not release new assets in the submission phase. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [Yes] Justification: We have one experiment including human preference studies. We describe the detailed instructions for conducting such a user study in section B.4. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [Yes] Justification: We have one experiment including human preference studies. We have acquired the approval of users under study to help fill our forms. No potential threats to human subjects were detected, and all results were anonymized to prevent any potential exposure of human identities. All human subjects were properly paid. B.4. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

Theory Parts

### Proof of Theorem 3.1

The proof of Theorem 3.1 is based on the so-called Score-projection identity which was first found in Vincent [70] to bridge denoising score matching and denoising auto-encoders. Later the identity is applied by Zhou et al. [93] for deriving distillation methods based on Fisher divergences. We appreciate the efforts of Zhou et al. [93] and re-write the score-projection identity here without proof. Readers can check Zhou et al. [93] for a complete proof of score-projection identity.

**Theorem A.1** (Score-projection identity).: Let \(\bm{u}(\cdot,\theta)\) be a vector-valued function, using the notations of Theorem 3.1, under mild conditions, the identity holds:

\[\mathbb{E}_{\begin{subarray}{c}\bm{x}_{0}\sim p_{\theta,0}\\ \bm{x}_{t}\left|\bm{x}_{0}\sim q_{t}(\bm{x}_{t}\left|\bm{x}_{0}\right)\right. \end{subarray}}\bm{u}(\bm{x}_{t},\theta)^{T}\bigg{\{}\bm{s}_{p_{\theta,t}}(\bm{ x}_{t})-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0})\bigg{\}}=0,\ \ \forall\theta.\]

Next, we turn to prove the Theorem 3.1.

Proof.: We prove a more general result. Let \(\bm{u}(\cdot)\) be a vector-valued function, the so-called score-projection identity [93, 70] holds,

\[\mathbb{E}_{\begin{subarray}{c}\bm{x}_{0}\sim p_{\theta,0}\\ \bm{x}_{t}\left|\bm{x}_{0}\sim q_{t}(\bm{x}_{t}\left|\bm{x}_{0}\right)\right. \end{subarray}}\bm{u}(\bm{x}_{t},\theta)^{T}\bigg{\{}\bm{s}_{p_{\theta,t}}( \bm{x}_{t})-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0})\bigg{\}}=0, \ \ \forall\theta.\] (A.1)

Taking \(\theta\) gradient on both sides of identity (A.1), we have

\[0 =\mathbb{E}_{\begin{subarray}{c}\bm{x}_{0}\sim p_{\theta,0}\\ \bm{x}_{t}\left|\bm{x}_{0}\sim q_{t}(\bm{x}_{t}\left|\bm{x}_{0}\right)\right. \end{subarray}}\frac{\partial}{\partial x_{t}}\bigg{\{}\bm{u}(\bm{x}_{t}, \theta)^{T}\big{\{}\bm{s}_{p_{\theta,t}}(\bm{x}_{t})-\nabla_{\bm{x}_{t}}\log q _{t}(\bm{x}_{t}|\bm{x}_{0})\big{\}}\bigg{\}}\frac{\partial\bm{x}_{t}}{ \partial\theta}\] \[+\mathbb{E}_{\begin{subarray}{c}\bm{x}_{0}\sim p_{\theta,0}\\ \bm{x}_{t}\left|\bm{x}_{0}\sim q_{t}(\bm{x}_{t}\left|\bm{x}_{0}\right)\right. \end{subarray}}\frac{\partial}{\partial\bm{x}_{0}}\bigg{\{}\bm{u}(\bm{x}_{t}, \theta)^{T}\big{\{}-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0})\big{\}} \bigg{\}}\frac{\partial\bm{x}_{0}}{\partial\theta}\] (A.2) \[+\mathbb{E}_{\begin{subarray}{c}\bm{x}_{0}\sim p_{\theta,0}\\ \bm{x}_{t}\left|\bm{x}_{0}\sim q_{t}(\bm{x}_{t}\left|\bm{x}_{0}\right)\end{subarray}} \bm{u}(\bm{x}_{t},\theta)^{T}\frac{\partial}{\partial\theta}\bigg{\{}\bm{s}_{ p_{\theta,t}}(\bm{x}_{t})\bigg{\}}+\frac{\partial}{\partial\theta}\bm{u}(\bm{x}_{t}, \theta)^{T}\bm{s}_{\theta}(\bm{x}_{t})\] (A.3) \[=\mathbb{E}_{\begin{subarray}{c}\bm{x}_{t}\sim p_{\theta,0}\\ \bm{x}_{t}\left|\bm{x}_{0}\sim q_{t}(\bm{x}_{t}\left|\bm{x}_{0}\right)\end{subarray}} \bm{u}(\bm{x}_{t},\theta)^{T}\frac{\partial}{\partial\theta}\bigg{\{}\bm{s}_{ p_{\theta,t}}(\bm{x}_{t})\bigg{\}}\] (A.4) \[+\frac{\partial}{\partial\bm{x}_{0}}\bigg{\{}\bm{u}(\bm{x}_{t}, \theta)^{T}\big{\{}-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0})\big{\}} \bigg{\}}\frac{\partial\bm{x}_{0}}{\partial\theta}\] (A.5) \[+\frac{\partial}{\partial\theta}\bm{u}(\bm{x}_{t},\theta)^{T}\bm{ s}_{\theta}(\bm{x}_{t})\bigg{\}}\] (A.6) \[=\mathbb{E}_{\bm{x}_{t}\sim p_{\theta,t}}\bm{u}(\bm{x}_{t}, \theta)^{T}\frac{\partial}{\partial\theta}\bigg{\{}\bm{s}_{p_{\theta,t}}(\bm{ x}_{t})\bigg{\}}\] (A.8) \[+\frac{\partial}{\partial\theta}\mathbb{E}_{\begin{subarray}{c} \bm{x}_{0}\sim p_{\theta,0}\\ \bm{x}_{t}\left|\bm{x}_{0}\sim q_{t}(\bm{x}_{t}\left|\bm{x}_{0}\right)\end{subarray}} \bm{u}(\bm{x}_{t},\theta)^{T}\bigg{\{}\bm{s}_{p_{\left[\theta\right],t}}(\bm{ x}_{t})-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0})\bigg{\}}\] (A.9)

Therefore we have the following identity:

\[\mathbb{E}_{\bm{x}_{t}\sim p_{\theta,t}}\bm{u}(\bm{x}_{t},\theta)^{T}\frac{ \partial}{\partial\theta}\bigg{\{}\bm{s}_{p_{\theta,t}}(\bm{x}_{t})\bigg{\}}=- \frac{\partial}{\partial\theta}\mathbb{E}_{\begin{subarray}{c}\bm{x}_{0}\sim p_ {\theta,0}\\ \bm{x}_{t}\left|\bm{x}_{0}\sim q_{t}(\bm{x}_{t}\left|\bm{x}_{0}\right)\end{subarray}} \bm{u}(\bm{x}_{t},\theta)^{T}\bigg{\{}\bm{s}_{p_{\left[\theta\right],t}}(\bm{x}_{t })-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0})\bigg{\}}\] (A.10)

which holds for arbitrary function \(\bm{u}(\cdot,\theta)\) and parameter \(\theta\). If we set

\[\bm{u}(\bm{x}_{t},\theta)=\bm{d}^{\prime}(\bm{y}_{t})\] \[\bm{y}_{t}=\bm{s}_{p_{\mathrm{g}}\left|\bm{y}\right|,t}(\bm{x}_{t} )-\bm{s}_{q_{t}}(\bm{x}_{t})\]Then we formally have

\[\frac{\partial}{\partial\theta}\mathbb{E}_{\bm{x}_{t}\sim p_{\bm{x}_ {t}\in[\theta],t}}\bigg{\{}\mathbf{d}^{\prime}(\bm{y}_{t})\bigg{\}}^{T}\bigg{\{} \bm{s}_{p_{\theta,t}}(\bm{x}_{t})\bigg{\}}\] \[=\frac{\partial}{\partial\theta}\mathbb{E}_{\bm{x}_{0}\sim p_{ \theta,0}\atop\bm{x}_{t}|_{\bm{x}_{0}\sim p_{\theta}(\bm{x}_{t}|_{\bm{x}_{0}})} }\bigg{\{}-\mathbf{d}^{\prime}(\bm{y}_{t})\bigg{\}}^{T}\bigg{\{}\bm{s}_{p_{ \theta,t}}(\bm{x}_{t})-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0}) \bigg{\}}\] (A.11)

### Pytorch style pseudo-code of Score Implicit Matching

In this section, we give a PyTorch style pseudo-code for algorithm 1, with the Pseudo-Huber distance function. For a detailed algorithm on CIFAR10 with EDM model, please check Algorithm 2.

```
1importtorch
2importtorch.nnasnn
3importtorch.optimasoptim
4
5#InitializegeneratorG
6G=Generator()
7
8##loadteacherDM
9Sd=DiffusionModel().load('/path_to_ckpt').eval().requires_grad_(False)
10Sg=copy.deepcopy(Sd)##initializeonlineDMwithteacherDM
11
12#Defineoptimizers
13opt_G=optim.Adam(G.parameters(),lr=0.001,betas=(0.0,0.999))
14opt_Sg=optim.Adam(Sg.parameters(),lr=0.001,betas=(0.0,0.999))
15
16#Trainingloop
17whileTrue:
18#updateSg
19Sg.train().requires_grad_(True)
20G.eval().requires_grad_(False)
21
22#loopfor2timestoupdatesSg
23for=inrange(2):
24z=torch.randn((2000,2)).to(device)
25withtorch.no_grad():
26fake_x=G(z)
27
28t=torch.from_numpy(np.random.choice(np.arange(1,Sd.T),size=fake_x.shape[0],replace=True)).to(device).long()
29fake_xt,t,noise,sigma_t,g2_t=Sd(fake_x,t=t,return_t=True)
30sigma_t=sigma_t.view(-1,1).to(device)
31g2t=g2t.to(device)
32score=Sg(torch.cat([fake_xt,t.view(-1,1)/Sd.T],-1))/sigma_t
33
34batch_sg_loss=score+noise/sigma_t
35batch_sg_loss={g2_t*batch_sg_loss.square().sum(-1)}.mean()*Sd.T
36
37optimizer_Sg.zero_grad()
38batch_sg_loss.backward()
39optimizer_Sg.step()
40
41
42#updateG
43Sg.eval().requires_grad_(False)
44G.train().requires_grad_(True)
45
46z=torch.randn((2000,2)).to(device)
47fake_x=G(z)
48t = torch.from_numpy(np.random.choice(np.arange(1,diffusion.T),size=fake_x,shape[0],replace=True)).to(device).long()
* fake_xt, t, noise, sigma_t, g2_t = diffusion(fake_x, t=t, return_t=True)
* sigma_t = sigma_t.view(-1,1).to(device)
* g2_t = g2_t.to(device)
* score_true = Sd(torch.cat([fake_xt,t.view(-1,1)/diffusion.T],-1))/sigma_t
* score_fake = Sg(torch.cat([fake_xt,t.view(-1,1)/diffusion.T],-1))/sigma_t

* offset_coeff = denoise_diff / torch.sqrt(denoise_diff.square().sum([1,2,3],keepdims=True) + self.phuber_c**2) weight = 1.0
* batch_g_loss = weight
* (fake_denoise - images) batch_g_loss = batch_g_loss.sum([1,2,3]).mean()
* optimizer_G.zero_grad() batch_g_loss.backward() optimizer_G.step() ```

Listing 1: Pytorch Style Pseudo-code of SIM

### Instances of SIM with different distance functions

In section 3.3, we have discussed the powered normed as distance functions. Other choices, such as the Huber distance, which is defined as

\[\forall 1\leq d\leq D,\ \ L_{\delta}(\bm{y})_{d}\coloneqq\begin{cases}y_{d}^{2}/ 2&\text{for }y_{d}\geq\delta\\ \delta(|y_{d}|-\delta/2)&\text{otherwise}\end{cases}\]

For other choices of distance functions, such as \(L1\) norm and exponential with powered norms, we put them in Table 4.

## Appendix B Empirical Parts

### Answer for the human preference study

The answer to the human preference study in Figure 1 is

* the middle image of the first row is generated by one-step SIM-DiT-600M;

\begin{table}
\begin{tabular}{l c c} \hline \hline Choice of \(\mathbf{d}(.)\) & \(\mathbf{d}^{\prime}(\bm{y}_{t})\) & Loss function \\ \hline \(\|\bm{y}_{t}\|_{2}^{2}\) & \(2\bm{y}_{t}\) & \(-2\bm{y}_{t}^{T}\Big{\{}\bm{s}_{\psi}(\bm{x}_{t},t)-\nabla_{\bm{x}_{t}}\log q_ {t}(\bm{x}_{t}|\bm{x}_{0})\Big{\}}\) \\ \(\|\bm{y}_{t}\|_{2}^{\alpha,\alpha\geq 1}\), \(\alpha\bm{y}_{t}^{(\alpha-1)}\) & \(-\alpha\Big{\{}\bm{y}_{t}^{(\alpha-1)}\Big{\}}^{T}\Big{\{}\bm{s}_{\psi}(\bm{x}_ {t},t)-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0})\Big{\}}\) \\ \(\underset{\alpha\geq 1,\alpha\text{ }even}{\exp(\beta\|\bm{y}_{t}\|_{2}^{\alpha}) \alpha\bm{y}_{t}^{(\alpha-1)}}\), \(\alpha\exp(\beta\|\bm{y}_{t}\|_{\alpha}^{\alpha})\bm{y}_{t}^{(\alpha-1)}\) & \(-\alpha\exp(\beta\|\bm{y}_{t}\|_{\alpha}^{\alpha})\Big{\{}\bm{y}_{t}^{(\alpha-1) }\Big{\}}\Big{\{}\bm{s}_{\psi}(\bm{x}_{t},t)-\nabla_{\bm{x}_{t}}\log q_{t}(\bm {x}_{t}|\bm{x}_{0})\Big{\}}\) \\ \(\|\bm{y}_{t}\|_{1}\) & \(\underset{\alpha\geq 1,\alpha\text{ }even}{\operatorname{sign}(\bm{y}_{t})}\) & \(-\underset{\alpha\geq 1,\alpha\text{ }even}{\operatorname{sign}(\bm{y}_{t})}^{T}\Big{\{}\bm{s}_{\psi}(\bm{x}_ {t},t)-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0})\Big{\}}\) \\ \(\underset{\begin{subarray}{c}L_{\delta}(\bm{y}_{t})\\ L_{\delta}(\cdot)\text{ Heat Loss}\end{subarray}}{\frac{\partial}{\partial\bm{y}_{t}}L_{ \delta}(\bm{y}_{t})}\) & \(-\frac{\partial}{\partial\bm{y}_{t}}L_{\delta}(\bm{y}_{t})^{T}\Big{\{}\bm{s}_{ \psi}(\bm{x}_{t},t)-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}|\bm{x}_{0}) \Big{\}}\) \\ \(\sqrt{\|\bm{y}_{t}\|_{2}^{2}+c^{2}-c}\) & \(2\frac{\bm{y}_{t}}{\sqrt{\|\bm{y}_{t}\|_{2}^{2}+c^{2}}}\) & \(-2\Big{\{}\frac{\bm{y}_{t}}{\sqrt{\|\bm{y}_{t}\|_{2}^{2}+c^{2}}}\Big{\}}^{T} \Big{\{}\bm{s}_{\psi}(\bm{x}_{t},t)-\nabla_{\bm{x}_{t}}\log q_{t}(\bm{x}_{t}| \bm{x}_{0})\Big{\}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Instances of Score Implicit Matching loss with different distance functions. The notations are aligned with the Algorithm 1.

* the leftmost image of the second row is generated by one step SIM-DiT-600M;
* the leftmost image of the third row is generated by one-step SIM-DiT-600M.

### Experiment details on CIFAR10 dataset

We follow the experiment setting of SiD and DI on CIFAR10. We start with a brief introduction to the EDM model [26].

The EDM model depends on the diffusion process

\[\mathrm{d}\bm{x}_{t}=t\mathrm{d}\bm{w}_{t},t\in[0,T].\] (B.1)

Samples from the forward process (B.1) can be generated by adding random noise to the output of the generator function, i.e., \(\bm{x}_{t}=\bm{x}_{0}+t\bm{\epsilon}\) where \(\bm{\epsilon}\sim\mathcal{N}(\bm{0},\bm{I})\) is a Gaussian vector. The EDM model also reformulates the diffusion model's score matching objective as a denoising regression objective, which writes,

\[\mathcal{L}(\psi)=\int_{t=0}^{T}\lambda(t)\mathbb{E}_{\bm{x}_{0} \sim p_{0},\bm{x}_{t}|\bm{x}_{0}\sim p_{t}(\bm{x}_{t}|\bm{x}_{0})}||\bm{d}_{ \psi}(\bm{x}_{t},t)-\bm{x}_{0}\|_{2}^{2}\mathrm{d}t.\] (B.2)

Where \(\bm{d}_{\psi}(\cdot)\) is a denoiser network that tries to predict the clean sample by taking noisy samples as inputs. Minimizing the loss (B.2) leads to a trained denoiser, which has a simple relation to the marginal score functions as:

\[\bm{s}_{\psi}(\bm{x}_{t},t)=\frac{\bm{d}_{\psi}(\bm{x}_{t},t)-\bm{x}_{t}}{t^{ 2}}\] (B.3)

Under such a formulation, we actually have pre-trained denoiser models for experiments. Therefore, we use the EDM notations in later parts.

Construction of the one-step generator.Let \(\bm{d}_{\theta}(\cdot)\) be pretrained EDM denoiser models. Owing to the denoiser formulation of the EDM model, we construct the generator to have the same architecture as the pre-trained EDM denoiser with a pre-selected index \(t^{*}\), which writes

\[\bm{x}_{0}=g_{\theta}(\bm{z})\coloneqq\bm{d}(\bm{z},t^{*}),\ \ \bm{z}\sim\mathcal{N}(\bm{0},(t^{*})^{2}\bm{I}).\] (B.4)

We initialize the generator with the same parameter as the teacher EDM denoiser model.

Time index distribution.When training both the EDM diffusion model and the generator, we need to randomly select a time \(t\) in order to approximate the integral of the loss function (B.2). The EDM model has a default choice of \(t\) distribution as log-normal when training the diffusion (denoiser) model, i.e.

\[t\sim p_{EDM}(t):\ t=\exp(s)\] (B.5) \[s\sim\mathcal{N}(P_{mean},P_{std}^{2}),\ \ P_{mean}=-1.2,P_{std}=1.2.\] (B.6)

And a weighting function

\[\lambda_{EDM}(t)=\frac{(t^{2}+\sigma_{data}^{2})}{(t\times\sigma_{data})^{2}}.\] (B.7)

In our algorithm, we follow the same setting as the EDM model when updating the online diffusion (denoiser) model.

In SiD, they propose to use a special discrete time distribution, which writes

\[\sigma_{k}=(\sigma_{max}^{\frac{1}{2}}\frac{i}{K-1}(\sigma_{min}^{ \frac{1}{\sigma}}-\sigma_{max}^{\frac{1}{\sigma}}))^{\rho},\] \[\sigma_{max}=80.0,\sigma_{min}=0.002,\rho=7.0,K=1000\]

They proposed to choose \(t\) uniformly from

\[t\sim p_{SiD}(t):\ \ k\sim Unif[0,800],t=\sigma_{k};\] (B.8)

We name such a time distribution the \(Karr\) distribution in Figure 2 because such a schedule was originally proposed in Karras' EDM work for sampling.

However, in practice, we find that \(Karr\) distribution (B.8) empirically does not work well. Instead, we find that a modified log-normal time distribution when updating the generation with SIM works better than \(Karr\) distribution. Our SIM time distribution writes:

\[t\sim p_{SIM}(t):\ \ t=\exp(s)\] (B.9)

\[s\sim\mathcal{N}(P_{mean},P_{std}^{2}),\ \ P_{mean}=-3.5,P_{std}=2.5.\] (B.10)

Weighting function.As we have said, we use the same \(\lambda_{EDM}(t)\) (B.7) weighting function as EDM when updating the denoiser model. When updating the generator, SiD uses a specially designed weighting function, which writes:

\[w_{SiD}(t)=\frac{C\times t^{4}}{\|\bm{x}_{0}-\bm{d}_{q_{t}}(\bm{x}_{t})\|_{1, \mathrm{sg}}}\] (B.11)

\[\bm{x}_{t}=\bm{x}_{0}+t\epsilon,\ \ \epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I})\] (B.12)

The notation \(\mathrm{sg}\) means stop-gradient, and \(C\) is the data dimensions. They claim such a weighting function helps to stabilize the training. However, in our experiments, since the SIM itself has normalized the loss (see section 4), we do not use such ad-hoc weighting functions. Instead, we just set the weighting function to be 1 for all time. We call the SiD's weighting function the \(sidwgt\) in Figure 2, and our weighting the \(nowgt\) in Figure 2.

In Figure 2, we compare the SiD and SIM with different time distribution and weighting functions. We find that SIM+nowgt+lognormal time distribution gives the best performances significantly, therefore our final experiment tasks such a configuration. Table 5 records the detailed configurations we use for SIM on CIFAR10 EDM distillation.

With the optimal setting and EDM formulation, we can rewrite our algorithm in an EDM style in Algorithm 2.

### Experiment details on Text-to-Image Distillation

In the Text-to-Image distillation part, in order to align our experiment with that on CIFAR10, we rewrite the PixArt-\(\alpha\) model in EDM formulation:

\[\bm{d}(\bm{x};t)=\bm{x}-tF_{\theta}\] (B.14)

Here, following the iDDPM+DDIM preconditioning in EDM, PixArt-\(\alpha\) is denoted by \(F_{\theta}\), \(\bm{x}\) is the image data plus noise with a standard deviation of \(t\), for the remaining parameters such as \(C_{1}\) and \(C_{2}\), we kept them unchanged to match those defined in EDM. Unlike the original model, we only retained the image channels for the output of this model. Since we employed the preconditioning of iDDPM+DDIM in the EDM, each \(\sigma\) value is rounded to the nearest 1000 bins after being passed into the model. For the actual values used in PixArt-\(\alpha\), beta_start is set to 0.0001, and beta_end is set to 0.02. Therefore, according to the formulation of EDM, the range of our noise distribution is [0.01, 156.6155], which will be used to truncate our sampled \(t\). For our one-step generator, it is formulated as:

\[g_{\theta}(\bm{z})=\bm{d}(\bm{z},t^{*})=\bm{z}-t^{*}F_{\theta}\] (B.15)

Here following SiD \(t^{*}=2.5\) and \(\bm{z}\sim\mathcal{N}(0,(t^{*})^{2}\mathbf{I})\), we observed in practice that larger values of \(t^{*}\) lead to faster convergence of the model, but the difference in convergence speed is negligible for the complete model training process and has minimal impact on the final results.

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline Hyperparameter & \multicolumn{2}{c|}{CIFAR-10 (Uncond)} & \multicolumn{2}{c}{CIFAR-10 (Cond)} \\  & DM \(\bm{s}_{\psi}\) & Generator \(g_{\theta}\) & DM \(\bm{s}_{\psi}\) & Generator \(g_{\theta}\) \\ \hline Learning rate & 1e-5 & 1e-5 & 1e-5 & 1e-5 \\ Batch size & 256 & 256 & 256 & 256 \\ \(\sigma(t^{*})\) & 2.5 & 2.5 & 2.5 & 2.5 \\ \(Adam\)\(\beta_{0}\) & 0.0 & 0.0 & 0.0 & 0.0 \\ \(Adam\)\(\beta_{1}\) & 0.999 & 0.999 & 0.999 & 0.999 \\ Time Distribution & \(p_{EDM}(t)\)(B.5) & \(p_{SIM}(t)\)(B.9) & \(p_{EDM}(t)\)(B.5) & \(p_{SIM}(t)\)(B.9) \\ Weighting & \(\lambda_{EDM}(t)\)(B.7) & 1 & \(\lambda_{EDM}(t)\)(B.7) & 1 \\ Loss function & (B.2) & (B.13) & (B.2) & (B.13) \\ Number of GPUs & 4\(\times\)A100-40G & 4\(\times\)A100-40G & 4\(\times\)A100-40G & 4\(\times\)A100-40G \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameters used for SIM on CIFAR10 EDM DistillationWe utilized the SAM-LLaVA-Caption10M dataset, which comprises prompts generated by the LLaVA model on the SAM dataset. These prompts provide detailed descriptions for the images, thereby offering us a challenging set of samples for our distillation experiments.

All experiments in this section were conducted on 4 A100-40G GPUs with bfloat16 precision, using the PixArt-XL-2-512x512 model version, employing the same hyperparameters. For both optimizers, we utilized Adam with a learning rate of 5e-6 and betas=[0, 0.999]. Additionally, to enable a batch size of 1024, we employed gradient checkpointing and set the gradient accumulation to 8. Finally, regarding the training noise distribution, instead of adhering to the original iDDPM schedule, we sample the \(\sigma\) from a log-normal distribution with a mean of -2.0 and a standard deviation of 2.0, we use the same noise distribution for both optimization steps and set the two loss weighting to constant 1. Our best model was trained on the SAM Caption dataset for approximately 16k iterations, which is equivalent to less than 2 epochs. This training process took about 2 days on 4 A100-40G GPUs.

We also tested the impact of different noise distributions on the distillation process. When the noise distribution is highly concentrated around smaller values, we observed a phenomenon where the generated samples appear excessively dark. On the other hand, when we used slightly larger noise distributions, we found that the structure of the generated samples tended to be unstable.

### Instruction for Human Preference Study

Our user study primarily focuses on comparing the outputs of the distilled model and the teacher model. Each image has undergone rigorous manual review to ensure the safety of survey participants. We conducted the study using questionnaires, where users were presented with two randomly ordered images generated by the distilled model and teacher model and asked to select the sample that best matched the text description and had higher image quality. Finally, we used the collected votes for the distilled model and the teacher model as indicators of user preference. The questionnaire website used for conducting these evaluations are shown in Figure 5.

To be more specific, we randomly selected 17 prompt words and generated images of resolution 512x512 using both the student model and the teacher model. To facilitate comparison, we presented the two images side by side in random order. In the questionnaire, we provided the complete prompt words for reference in addition to the generated images. In the end, we collected approximately 30 survey responses in total.

Figure 5: Demonstration of our human preference user study interface.

Figure 6: One-step SIM model on CIFAR10-conditional. FID=1.96.

### Generated Samples on CIFAR10

### FID Convergence on CIFAR10 Unconditional Generation

### Prompts for Figure 3

* prompt for first row of Figure 3: _A small cactus with a happy face in the Sahara desert._
* prompt for second row of Figure 3: _An image of a jade green and gold coloured Faberge egg, 16k resolution, highly detailed, product photography, trending on artstation, sharp

Figure 8: The comparison of FID convergence between SIM and SiD.

Figure 7: One-step SIM model on CIFAR10-unconditional. FID=2.06.

focus, studio photo, intricate details, fairly dark background, perfect lighting, perfect composition, sharp features, Miki Asai Macro photography, close-up, hyper detailed, trending on artstation, sharp focus, studio photo, intricate details, highly detailed, by greg rutkowski._
* prompt for third row of Figure 3: _Baby playing with toys in the snow._