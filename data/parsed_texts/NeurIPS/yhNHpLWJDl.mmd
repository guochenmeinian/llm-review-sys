Finite-Time Analysis of Whittle Index based Q-Learning for Restless Multi-Armed Bandits with Neural Network Function Approximation

Guojun Xiong, Jian Li

Stony Brook University

{guojun.xiong,jian.li.3}@stonybrook.edu

###### Abstract

Whittle index policy is a heuristic to the intractable restless multi-armed bandits (RMAB) problem. Although it is provably asymptotically optimal, finding Whittle indices remains difficult. In this paper, we present Neural-Q-Whittle, a Whittle index based Q-learning algorithm for RMAB with neural network function approximation, which is an example of nonlinear two-timescale stochastic approximation with Q-function values updated on a faster timescale and Whittle indices on a slower timescale. Despite the empirical success of deep Q-learning, the non-asymptotic convergence rate of Neural-Q-Whittle, which couples neural networks with two-timescale Q-learning largely remains unclear. This paper provides a finite-time analysis of Neural-Q-Whittle, where data are generated from a Markov chain, and Q-function is approximated by a ReLU neural network. Our analysis leverages a Lyapunov drift approach to capture the evolution of two coupled parameters, and the nonlinearity in value function approximation further requires us to characterize the approximation error. Combing these provide Neural-Q-Whittle with \(\mathcal{O}(1/k^{2/3})\) convergence rate, where \(k\) is the number of iterations.

## 1 Introduction

We consider the restless multi-armed bandits (RMAB) problem [56], where the decision maker (DM) repeatedly activates \(K\) out of \(N\) arms at each decision epoch. Each arm is described by a Markov decision process (MDP) [45], and evolves stochastically according to two different transition kernels, depending on whether the arm is activated or not. Rewards are generated with each transition. Although RMAB has been widely used to study constrained sequential decision making problems [5, 38, 12, 64, 29, 36, 35, 25], it is notoriously intractable due to the explosion of state space [44]. A celebrated heuristic is the Whittle index policy [56], which computes the Whittle index for each arm given its current state as the cost to pull the arm. Whittle index policy then activates the \(K\) highest indexed arms at each decision epoch, and is provably asymptotically optimal [54].

However, the computation of Whittle index requires full knowledge of the underlying MDP associated with each arm, which is often unavailable in practice. To this end, many recent efforts have focused on learning Whittle indices for making decisions in an online manner. First, model-free reinforcement learning (RL) solutions have been proposed [10, 23, 53, 8, 28, 57, 59, 61, 58, 3], among which [3] developed a Whittle index based Q-learning algorithm, which we call Q-Whittle for ease of exposition, and provided the first-ever rigorous asymptotic analysis. However, Q-Whittle suffers from slow convergence since it only updates the Whittle index of a specific state when that state is visited. In addition, Q-Whittle needs to store the Q-function values for all state-action pairs, which limits its applicability only to problems with small state space. Second, deep RL methods have been leveraged to predict Whittle indices via training neural networks [40, 41]. Thoughthese methods are capable of dealing with large state space, there is no asymptotic or finite-time performance guarantee. Furthermore, training neural networks requires to tuning hyper-parameters. This introduces an additional layer of complexity to predict Whittle indices. Third, to address aforementioned deficiencies, [60] proposed Q-Whittle-LFA by coupling Q-Whittle with linear function approximation and provided a finite-time convergence analysis. One key limitation of Q-Whittle-LFA is the unrealistic assumption that all data used in Q-Whittle-LFA are sampled i.i.d. from a fixed stationary distribution.

To tackle the aforementioned limitations and inspired by the empirical success of deep Q-learning in numerous applications, we develop Neural-Q-Whittle, a Whittle index based Q-learning algorithm with _neural network function approximation under Markovian observations_. Like [3, 60], the updates of Q-function values and Whittle indices form a two-timescale stochastic approximation (2TSA) with the former operating on a faster timescale and the later on a slower timescale. Unlike [3, 60], our Neural-Q-Whittle uses a deep neural network with the ReLU activation function to approximate the Q-function. However, Q-learning with neural network function approximation can in general diverge [2], and the theoretical convergence of Q-learning with neural network function approximation has been limited to special cases such as fitted Q-iteration with i.i.d. observations [22], which fails to capture the practical setting of Q-learning with neural network function approximation.

In this paper, we study the non-asymptotic convergence of Neural-Q-Whittle with data generated from a Markov decision process. Compared with recent theoretical works for Q-learning with neural network function approximation [13, 22, 62], our Neural-Q-Whittle involves a two-timescale update between two coupled parameters, i.e., Q-function values and Whittle indices. This renders existing finite-time analysis in [13, 22, 62] not applicable to our Neural-Q-Whittle due to the fact that [13, 22, 62] only contains a single-timescale update on Q-function values. Furthermore, [13, 22, 62] required an additional projection step for the update of parameters of neural network function so as to guarantee the boundedness between the unknown parameter at any time step with the initialization. This in some cases is impractical. Hence, a natural question that arises is

_Is it possible to provide a non-asymptotic convergence rate analysis of Neural-Q-Whittle with two coupled parameters updated in two timescales under Markovian observations without the extra projection step?_

The theoretical convergence guarantee of two-timescale Q-learning with neural network function approximation under Markovian observations remains largely an open problem, and in this paper, we provide an affirmative answer to this question. Our main contributions are summarized as follows:

\(\bullet\) We propose Neural-Q-Whittle, a novel Whittle index based Q-learning algorithm with neural network function approximation for RMAB. Inspired by recent work on TD learning [48] and Q-learning [15] with linear function approximation, our Neural-Q-Whittle removes the additional impractical projection step in the neural network function parameter update.

\(\bullet\) We establish the first finite-time analysis of Neural-Q-Whittle under Markovian observations. Due to the two-timescale nature for the updates of two coupled parameters (i.e., Q-function values and Whittle indices) in Neural-Q-Whittle, we focus on the convergence rate of these parameters rather than the convergence rate of approximated Q-functions as in [13, 22, 62]. Our key technique is to view Neural-Q-Whittle as a 2TSA for finding the solution of suitable nonlinear equations. Different from recent works on finite-time analysis of a general 2TSA [20] or with linear function approximation [60], the nonlinear parameterization of Q-function in Neural-Q-Whittle under Markovian observations imposes significant difficulty in finding the global optimum of the corresponding nonlinear equations. To mitigate this, we first approximate the original neural network function with a collection of local linearization and focus on finding a surrogate Q-function in the neural network function class that well approximates the optimum. Our finite-time analysis then requires us to consider two Lyapunov functions that carefully characterize the coupling between iterates of Q-function values and Whittle indices, with one Lyapunov function defined with respect to the true neural network function, and the other defined with respect to the locally linearized neural network function. We then characterize the errors between these two Lyapunov functions. Putting them together, we prove that Neural-Q-Whittle achieves a convergence in expectation at a rate \(\mathcal{O}(1/k^{2/3})\), where \(k\) is the number of iterations.

\(\bullet\) Finally, we conduct experiments to validate the convergence performance of Neural-Q-Whittle, and verify the sufficiency of our proposed condition for the stability of Neural-Q-Whittle.

## 2 Preliminaries

**RMAB.** We consider an infinite-horizon average-reward RMAB with each arm \(n\in\mathcal{N}\) described by a unichain MDP [45]\(\mathcal{M}_{n}:=(\mathcal{S},\mathcal{A},P_{n},r_{n})\), where \(\mathcal{S}\) is the state space with cardinality \(S<\infty\), \(\mathcal{A}\) is the action space with cardinality \(A\), \(P_{n}(s^{\prime}|s,a)\) is the transition probability of reaching state \(s^{\prime}\) by taking action \(a\) in state \(s\), and \(r_{n}(s,a)\) is the reward associated with state-action pair \((s,a)\). At each time slot \(t\), the DM activates \(K\) out of \(N\) arms. Arm \(n\) is "active" at time \(t\) when it is activated, i.e., \(A_{n}(t)=1\); otherwise, arm \(n\) is "passive", i.e., \(A_{n}(t)=0\). Let \(\Pi\) be the set of all possible policies for RMAB, and \(\pi\in\Pi\) is a feasible policy, satisfying \(\pi:\mathcal{F}_{t}\mapsto\mathcal{A}^{N}\), where \(\mathcal{F}_{t}\) is the sigma-algebra generated by random variables \(\{S_{n}(h),A_{n}(h):\forall n\in\mathcal{N},h\leq t\}\). The objective of the DM is to maximize the expected long-term average reward subject to an instantaneous constraint that only \(K\) arms can be activated at each time slot, i.e.,

\[\text{RMAB:}\quad\max_{\pi\in\Pi}\ \liminf_{T\to\infty}\frac{1}{T}\mathbb{E}_{\pi} \left(\sum_{t=0}^{T}\sum_{n=1}^{N}r_{n}(t)\right),\quad\text{s.t.}\ \sum_{n=1}^{N}A_{n}(t)=K,\quad\forall t.\] (1)

**Whittle Index Policy.** It is well known that RMAB (1) suffers from the curse of dimensionality [44]. To address this challenge, Whittle [56] proposed an index policy through decomposition. Specifically, Whittle relaxed the constraint in (1) to be satisfied on average and obtained a unconstrained problem: \(\max_{\pi\in\Pi}\liminf_{T\to\infty}\frac{1}{T}\mathbb{E}_{\pi}\sum_{t=1}^{T} \sum_{n=1}^{N}\{r_{n}(t)+\lambda(1-A_{n}(t))\}\), where \(\lambda\) is the Lagrangian multiplier associated with the constraint. The key observation of Whittle is that this problem can be decomposed and its solution is obtained by combining solutions of \(N\) independent problems via solving the associated dynamic programming (DP): \(V_{n}(s)=\max_{a\in\{0,1\}}Q_{n}(s,a),\forall n\in\mathcal{N},\) where

\[Q_{n}(s,a)+\beta=a\Big{(}r_{n}(s,a)+\sum_{s^{\prime}}p_{n}(s^{\prime}|s,1)V_{n }(s^{\prime})\Big{)}+(1-a)\Big{(}r_{n}(s,a)+\lambda+\sum_{s^{\prime}}p_{n}(s^ {\prime}|s,0)V_{n}(s^{\prime})\Big{)},\] (2)

where \(\beta\) is unique and equals to the maximal long-term average reward of the unichain MDP, and \(V_{n}(s)\) is unique up to an additive constant, both of which depend on the Lagrangian multiplier \(\lambda\). The optimal decision \(a^{*}\) in state \(s\) then is the one which maximizes the right hand side of the above DP. The Whittle index associated with state \(s\) is defined as the value \(\lambda_{n}^{*}(s)\in\mathbb{R}\) such that actions \(0\) and \(1\) are equally favorable in state \(s\) for arm \(n\)[3; 23], satisfying

\[\lambda_{n}^{*}(s):=r_{n}(s,1)+\sum_{s^{\prime}}p_{n}(s^{\prime}|s,1)V_{n}(s^ {\prime})-r_{n}(s,0)-\sum_{s^{\prime}}p_{n}(s^{\prime}|s,0)V_{n}(s^{\prime}).\] (3)

Whittle index policy then activates \(K\) arms with the largest Whittle indices at each time slot. Additional discussions are provided in Section B in supplementary materials.

**Q-Learning for Whittle Index.** Since the underlying MDPs are often unknown, [3] proposed Q-Whittle, a tabular Whittle index based Q-learning algorithm, where the updates of Q-function values and Whittle indices form a 2TSA, with the former operating on a faster timescale for a given \(\lambda_{n}\) and the later on a slower timescale. Specifically, the Q-function values for \(\forall n\in\mathcal{N}\) are updated as

\[Q_{n,k+1}(s,a):=Q_{n,k}(s,a)+\alpha_{n,k}\mathds{1}_{\{S_{n,k}=s,A_{n,k}=a\}}\Big{(}r_{n}(s,a)+(1-a)\lambda_{n,k}(s)\\ +\max_{a}Q_{n,k}(S_{n,k+1},a)-I_{n}(Q_{k})-Q_{n,k}(s,a)\Big{)},\] (4)

where \(I_{n}(Q_{k})=\frac{1}{2S}\sum_{s\in\mathcal{S}}(Q_{n,k}(s,0)+Q_{n,k}(s,1))\) is standard in the relative Q-learning for long-term average MDP setting [1], which differs significantly from the discounted reward setting [45; 1]. \(\{\alpha_{n,k}\}\) is a step-size sequence satisfying \(\sum_{k}\alpha_{n,k}=\infty\) and \(\sum_{k}\alpha_{n,k}^{2}<\infty\).

Accordingly, the Whittle index is updated as

\[\lambda_{n,k+1}(s)=\lambda_{n,k}(s)+\eta_{n,k}(Q_{n,k}(s,1)-Q_{n,k}(s,0)),\] (5)

with the step-size sequence \(\{\eta_{n,k}\}\) satisfying \(\sum_{k}\eta_{n,k}=\infty\), \(\sum_{k}\eta_{n,k}^{2}<\infty\) and \(\eta_{n,k}=o(\alpha_{n,k})\). The coupled iterates (4) and (5) form a 2TSA, and [3] provided an asymptotic convergence analysis.

## 3 Neural Q-Learning for Whittle Index

A closer look at (5) reveals that Q-Whittle only updates the Whittle index of a specific state when that state is visited. This makes Q-Whittle suffers from slow convergence. In addition, Q-Whittleneeds to store the Q-function values for all state-action pairs, which limits its applicability only to problems with small state space. To address this challenge and inspired by the empirical success of deep Q-learning, we develop Neural-Q-Whittle through coupling Q-Whittle with neural network function approximation by using low-dimensional feature mapping and leveraging the strong representation power of neural networks. For ease of presentation, we drop the subscript \(n\) in (4) and (5), and discussions in the rest of the paper apply to any arm \(n\in\mathcal{N}\).

Specifically, given a set of basis functions \(\phi_{\ell}:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R},\forall\ell=1, \cdots,d\) with \(d\ll SA\), the approximation of Q-function \(Q_{\boldsymbol{\theta}}(s,a)\) parameterized by a unknown weight vector \(\boldsymbol{\theta}\in\mathbb{R}^{md}\), is given by \(Q_{\boldsymbol{\theta}}(s,a)=f(\boldsymbol{\theta};\boldsymbol{\phi}(s,a)),\ \forall s \in\mathcal{S},a\in\mathcal{A}\), where \(f\) is a nonlinear neural network function parameterized by \(\boldsymbol{\theta}\) and \(\boldsymbol{\phi}(s,a)\), with \(\boldsymbol{\phi}(s,a)=(\phi_{1}(s,a),\cdots,\phi_{d}(s,a))^{\intercal}\). The feature vectors are assumed to be linearly independent and are normalized so that \(\|\boldsymbol{\phi}(s,a)\|\leq 1,\forall s\in\mathcal{S},a\in\mathcal{A}\). In particular, we parameterize the Q-function by using a two-layer neural network [13, 62]

\[f(\boldsymbol{\theta};\boldsymbol{\phi}(s,a)):=\frac{1}{\sqrt{m}}\sum_{r=1}^{ m}b_{r}\sigma(\mathbf{w}_{r}^{\intercal}\boldsymbol{\phi}(s,a)),\] (6)

where \(\boldsymbol{\theta}=(b_{1},\ldots,b_{m},\mathbf{w}_{1}^{\intercal},\ldots, \mathbf{w}_{m}^{\intercal})^{\intercal}\) with \(b_{r}\in\mathbb{R}\) and \(\mathbf{w}_{r}\in\mathbb{R}^{d\times 1},\forall r\in[1,m]\). \(b_{r},\forall r\) are uniformly initialized in \(\{-1,1\}\) and \(w_{r},\forall r\) are initialized as a zero mean Gaussian distribution according to \(\mathcal{N}(\mathbf{0},\mathbf{I}_{d}/d)\). During training process, only \(\mathbf{w}_{r},\forall r\) are updated while \(b_{r},\forall r\) are fixed as the random initialization. Hence, we use \(\boldsymbol{\theta}\) and \(\mathbf{w}_{r},\forall r\) interchangeably throughout this paper. \(\sigma(x)=\max(0,x)\) is the rectified linear unit (ReLU) activation function1.

Footnote 1: The finite-time analysis of Deep Q-Networks (DQN) [13, 22, 62] and references therein focuses on the ReLU activation function, as it has certain properties that make the analysis tractable. ReLU is piecewise linear and non-saturating, which can simplify the mathematical analysis. Applying the same analysis to other activation functions like the hyperbolic tangent (tanh) could be more complex, which is out of the scope of this work.

Given (6), we can rewrite the Q-function value updates in (4) as

\[\boldsymbol{\theta}_{k+1}=\boldsymbol{\theta}_{k}+\alpha_{k}\Delta_{k}\nabla _{\boldsymbol{\theta}}f(\boldsymbol{\theta}_{k};\boldsymbol{\phi}(S_{k},A_{k} )),\] (7)

with \(\Delta_{k}\) being the temporal difference (TD) error defined as \(\Delta_{k}:=r(S_{k},A_{k})+(1-A_{k})\lambda_{k}(s)-I(\boldsymbol{\theta}_{k})+ \max_{a}f(\boldsymbol{\theta}_{k};\boldsymbol{\phi}(S_{k+1},a))-f(\boldsymbol {\theta}_{k};\boldsymbol{\phi}(S_{k},A_{k}))\), where \(I(\boldsymbol{\theta}_{k})=\frac{1}{2S}\sum_{s\in\mathcal{S}}[f(\boldsymbol{ \theta}_{k};\boldsymbol{\phi}(s,0))+f(\boldsymbol{\theta}_{k};\boldsymbol{\phi }(s,1))]\). Similarly, the Whittle index update (5) can be rewritten as

\[\lambda_{k+1}(s)=\lambda_{k}(s)+\eta_{k}(f(\boldsymbol{\theta}_{k};\boldsymbol {\phi}(s,1))-f(\boldsymbol{\theta}_{k};\boldsymbol{\phi}(s,0))).\] (8)

The coupled iterates in (7) and (8) form Neural-Q-Whittle as summarized in Algorithm 1, which aims to learn the coupled parameters \((\boldsymbol{\theta}^{\star},\lambda^{\star}(s))\) such that \(f(\boldsymbol{\theta}^{\star},\boldsymbol{\phi}(s,1))=f(\boldsymbol{\theta}^{ \star},\boldsymbol{\phi}(s,0)),\forall s\in\mathcal{S}\).

**Remark 1**.: _Unlike recent works for Q-learning with linear [6, 37, 67] or neural network function approximations [13, 22, 62], we do not assume an additional projection step of the updates of unknown parameters \(\boldsymbol{\theta}_{k}\) in (7) to confine \(\boldsymbol{\theta}_{k},\forall k\) into a bounded set. This projection step is often used to stabilize the iterates related to the unknown stationary distribution of the underlying Markov chain, which in some cases is impractical. More recently, [48] removed the extra projection step and established the finite-time convergence of TD learning, which is treated as a linear stochastic approximation algorithm. [15] extended it to the Q-learning with linear function approximation.__However, these state-of-the-art works only contained a single-timescale update on Q-function values, i.e., with the only unknown parameter \(\bm{\theta}\), while our Neural-Q-Whittle involves a two-timescale update between two coupled unknown parameters \(\bm{\theta}\) and \(\lambda\) as in (7) and (8). Our goal in this paper is to expand the frontier by providing a finite-time bound for Neural-Q-Whittle under Markovian noise without requiring an additional projection step. We summarize the differences between our work and existing literature in Table 1._

## 4 Finite-Time Analysis of Neural-Q-Whittle

In this section, we present the finite-time analysis of Neural-Q-Whittle for learning Whittle index \(\lambda(s)\) of any state \(s\in\mathcal{S}\) when data are generated from a MDP. To simplify notation, we abbreviate \(\lambda(s)\) as \(\lambda\) in the rest of the paper. We start by first rewriting the updates of Neural-Q-Whittle in (7) and (8) as a nonlinear two-timescale stochastic approximation (2TSA) in Section 4.1.

### A Nonlinear 2TSA Formulation with Neural Network Function

We first show that Neural-Q-Whittle can be rewritten as a variant of the nonlinear 2TSA. For any fixed policy \(\pi\), since the state of each arm \(\{S_{k}\}\) evolves according to a Markov chain, we can construct a new variable \(X_{k}=(S_{k},A_{k},S_{k+1})\), which also forms a Markov chain with state space \(\mathcal{X}:=\{(s,a,s^{\prime})|s\in\mathcal{S},\pi(a|s)\geq 0,p(s^{\prime}|s,a )>0\}\). Therefore, the coupled updates (7) and (8) of Neural-Q-Whittle can be rewritten in the form of a nonlinear 2TSA [20]:

\[\bm{\theta}_{k+1}=\bm{\theta}_{k}+\alpha_{k}h(X_{k},\bm{\theta}_{k},\lambda_ {k}),\qquad\lambda_{k+1}=\lambda_{k}+\eta_{k}g(X_{k},\bm{\theta}_{k},\lambda_ {k}),\] (9)

where \(\bm{\theta}_{0}\) and \(\lambda_{0}\) being arbitrarily initialized in \(\mathbb{R}^{md}\) and \(\mathbb{R}\), respectively; and \(h(\cdot)\) and \(g(\cdot)\) satisfy

\[h(X_{k},\bm{\theta}_{k},\lambda_{k}) :=\nabla_{\bm{\theta}}f(\bm{\theta}_{k};\bm{\phi}(S_{k},A_{k})) \Delta_{k},\qquad\bm{\theta}_{k}\in\mathbb{R}^{md},\lambda_{k}\in\mathbb{R},\] (10) \[g(X_{k},\bm{\theta}_{k},\lambda_{k}) :=f(\bm{\theta}_{k};\bm{\phi}(s,1))-f(\bm{\theta}_{k};\bm{\phi}(s,0)),\qquad\bm{\theta}_{k}\in\mathbb{R}^{md}.\] (11)

Since \(\eta_{k}\ll\alpha_{k}\), the dynamics of \(\bm{\theta}\) evolves much faster than those of \(\lambda\). We aim to establish the finite-time performance of the nonlinear 2TSA in (9), where \(f(\cdot)\) is the neural network function defined in (6). This is equivalent to find the root2\((\bm{\theta}^{*},\lambda^{*})\) of a system with _two coupled_ nonlinear equations \(h:\mathcal{X}\times\mathbb{R}^{md}\times\mathbb{R}\to\mathbb{R}^{md}\) and \(g:\mathcal{X}\times\mathbb{R}^{md}\times\mathbb{R}\to\mathbb{R}\) such that

Footnote 2: The root \((\bm{\theta}^{*},\lambda^{*})\) of the nonlinear 2TSA (9) can be established by using the ODE method following the solution of suitably defined differential equations [9; 49; 3; 21; 19; 20], i.e., \(\hat{\bm{\theta}}=H(\bm{\theta},\lambda),\hat{\lambda}=\frac{\eta}{\alpha}G( \bm{\theta},\lambda)\), where a fixed stepsize is assumed for ease of expression at this moment.

\[H(\bm{\theta},\lambda):=\mathbb{E}_{\mu}[h(X,\bm{\theta},\lambda)]=0,\qquad G (\bm{\theta},\lambda):=\mathbb{E}_{\mu}[g(X,\bm{\theta},\lambda)]=0,\] (12)

where \(X\) is a random variable in finite state space \(\mathcal{X}\) with unknown distribution \(\mu\). For a fixed \(\bm{\theta}\), to study the stability of \(\lambda\), we assume the condition on the existence of a mapping such that \(\lambda=y(\bm{\theta})\) is the unique solution of \(G(\bm{\theta},\lambda)=0\). In particular, \(y(\bm{\theta})\) is given as

\[y(\bm{\theta})=r(s,1)+\sum_{s^{\prime}}p(s^{\prime}|s,1)\max_{a}f(\bm{\theta} ;\bm{\phi}(s^{\prime},a))-r(s,0)-\sum_{s^{\prime}}p(s^{\prime}|s,0)\max_{a}f( \bm{\theta};\bm{\phi}(s^{\prime},a)).\] (13)

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline
**Algorithm** & **Noise** & **Approximation** & **Timescale** & **Whittle index** \\ \hline \hline Q-Whittle [3] & _i.i.d._ & ✗ & _two-timescale_ & ✓ \\ Q-Whittle-LFA [60; 1; 1] & _i.i.d._ & linear & _two-timescale_ & ✓ \\ Q-Learning-LFA [60; 1; 10; 11] & _Markovian_ & linear & _single-timescale_ & ✗ \\ Q-Learning-NFA [13; 15; 22; 62] & _Markovian_ & _neural network_ & _single-timescale_ & ✗ \\ TD-Learning-LFA [48] & _Markovian_ & linear & _single-timescale_ & ✗ \\
2TSA-IID [19; 21] & _i.i.d._ & ✗ & _two-timescale_ & ✗ \\
2TSA-Markovian [20] & _Markovian_ & ✗ & _two-timescale_ & ✗ \\ \hline \hline Neural-Q-Whittle **(this work)** & _Markovian_ & _neural network_ & _two-timescale_ & ✓ \\ \hline \end{tabular}
\end{table}
Table 1: Comparison of settings in related works.

### Main Results

As inspired by [20], the finite-time analysis of such a nonlinear 2TSA boils down to the choice of two step sizes \(\{\alpha_{k},\eta_{k},\forall k\}\) and a Lyapunov function that couples the two iterates in (9). To this end, we first define the following two error terms:

\[\tilde{\bm{\theta}}_{k}=\bm{\theta}_{k}-\bm{\theta}^{*},\qquad\tilde{\lambda}_{ k}=\lambda_{k}-y(\bm{\theta}_{k}),\] (14)

which characterize the coupling between \(\bm{\theta}_{k}\) and \(\lambda_{k}\). If \(\tilde{\bm{\theta}}_{k}\) and \(\tilde{\lambda}_{k}\) go to zero simultaneously, the convergence of \((\bm{\theta}_{k},\lambda_{k})\) to \((\bm{\theta}^{*},\lambda^{*})\) can be established. Thus, to prove the convergence of \((\bm{\theta}_{k},\lambda_{k})\) of the nonlinear 2TSA in (9) to its true value \((\bm{\theta}^{*},\lambda^{*})\), we can equivalently study the convergence of \((\tilde{\bm{\theta}}_{k},\tilde{\lambda}_{k})\) by providing the finite-time analysis for the mean squared error generated by (9). To couple the fast and slow iterates, we define the following weighted Lyapunov function

\[M(\bm{\theta}_{k},\lambda_{k}):=\frac{\eta_{k}}{\alpha_{k}}\|\tilde{\bm{\theta }}_{k}\|^{2}+\|\tilde{\lambda}_{k}\|^{2}=\frac{\eta_{k}}{\alpha_{k}}\|\bm{ \theta}_{k}-\bm{\theta}^{*}\|^{2}+\|\lambda_{k}-y(\bm{\theta}_{k})\|^{2},\] (15)

where \(\|\cdot\|\) stands for the the Euclidean norm for vectors throughout the paper. It is clear that the Lyapunov function \(M(\bm{\theta}_{k},\lambda_{k})\) combines the updates of \(\bm{\theta}\) and \(\lambda\) with respect to the true neural network function \(f(\bm{\theta};\bm{\phi}(s,a))\) in (6).

To this end, our goal turns to characterize finite-time convergence of \(\mathbb{E}[M(\bm{\theta}_{k},\lambda_{k})]\). However, it is challenging to directly finding the global optimum of the corresponding nonlinear equations due to the nonlinear parameterization of Q-function in Neural-Q-Whittle. In addition, the operators \(h(\cdot),g(\cdot)\) and \(y(\cdot)\) in (10), (11) and (13) directly relate with the convoluted neural network function \(f(\bm{\theta};\bm{\phi}(s,a))\) in (6), which hinders us to characterize the smoothness properties of theses operators. Such properties are often required for the analysis of stochastic approximation [15; 19; 21].

To mitigate this, **(Step 1)** we instead approximate the true neural network function \(f(\bm{\theta},\bm{\phi}(s,a))\) with a collection of local linearization \(f_{0}(\bm{\theta};\bm{\phi}(s,a))\) at the initial point \(\bm{\theta}_{0}\). Based on the surrogate stationary point \(\bm{\theta}_{0}^{*}\) of \(f_{0}(\bm{\theta};\bm{\phi}(s,a))\), we correspondingly define a modified Lyapunov function \(\hat{M}(\bm{\theta}_{k},\lambda_{k})\) combining updates of \(\bm{\theta}\) and \(\lambda\) with respect to such local linearization. Specifically, we have

\[\hat{M}(\bm{\theta}_{k},\lambda_{k}):=\frac{\eta_{k}}{\alpha_{k}}\|\bm{\theta }_{k}-\bm{\theta}_{0}^{*}\|^{2}+\|\lambda_{k}-y_{0}(\bm{\theta}_{k})\|^{2},\] (16)

where \(y_{0}(\cdot)\) is in the same expression as \(y(\cdot)\) in (13) by replacing \(f(\cdot)\) with \(f_{0}(\cdot)\), and we will describe this in details below. **(Step 2)** We then study the convergence rate of the nonlinear 2TSA using this modified Lyapunov function under general conditions. **(Step 3)** Finally, since the two coupled parameters \(\bm{\theta}\) and \(\lambda\) in (9) are updated with respect to the true neural network function \(f(\bm{\theta};\bm{\phi}(s,a))\) in (6) in Neural-Q-Whittle, while we characterize their convergence using the approximated neural network function in Step 2. Hence, this further requires us to characterize the approximation errors. We visualize the above three steps in Figure 1 and provide a proof sketch in Section 4.3. Combing them together gives rise to our main theoretical results on the finite-time performance of Neural-Q-Whittle, which is formally stated in the following theorem.

**Theorem 1**.: _Consider iterates \(\{\bm{\theta}_{k}\}\) and \(\{\lambda_{k}\}\) generated by Neural-Q-Whittle in (7) and (8). Given \(\alpha_{k}=\frac{\alpha_{0}}{(k+1)},\eta_{k}=\frac{\eta_{0}}{(k+1)^{4/3}}\), we have for \(\forall k\geq\tau\)_

\[\mathbb{E}[M(\bm{\theta}_{k+1},\lambda_{k+1})|\mathcal{F}_{k-\tau }]\leq\frac{2\tau^{2}\mathbb{E}[\hat{M}(\bm{\theta}_{\tau},\lambda_{\tau})]}{( k+1)^{2}}+\frac{1200\alpha_{0}^{3}}{\eta_{0}}\frac{(C_{1}+\|\hat{\bm{ \theta}}_{0}\|)^{2}+(2C_{1}+\|\hat{\lambda}_{0}\|)^{2}}{(k+1)^{2/3}}\] \[+\frac{2\eta_{0}c_{0}^{2}}{\alpha_{0}(1-\kappa)^{2}}\|span(\Pi_{ \mathcal{F}}f(\bm{\theta}^{*})-f(\bm{\theta}^{*}))\|^{2}+\bigg{(}\frac{2}{(k+ 1)^{2/3}}+2\bigg{)}\,\mathcal{O}\Big{(}\frac{c_{1}^{3}(\|\bm{\theta}_{0}\|+| \lambda_{0}|+1)^{3}}{m^{1/2}}\Big{)},\] (17)

_where \(C_{1}:=c_{1}(\|\bm{\theta}_{0}\|+\|\lambda_{0}\|+1)\) with \(c_{1}\) being a proper chosen constant, \(c_{0}\) is a constant defined in Assumption 3, \(\tau\) is the mixing time defined in (22), \(span\) denotes for the span semi-norm [47], and \(\Pi_{\mathcal{F}}\) represents the projection to the set of \(\mathcal{F}\) containing all possible \(f_{0}(\bm{\theta};\bm{\phi}(s,a))\) in (18)._

The first term on the right hand side (17) corresponds to the bias compared to the Lyapunov function at the mixing time \(\tau\), which goes to zero at a rate of \(\mathcal{O}(1/k^{2})\). The second term corresponds to the accumulated estimation error of the nonlinear 2TSA due to Markovian noise, which vanishes at the rate \(\mathcal{O}(1/k^{2/3})\). Hence it dominates the overall convergence rate in (17). The third term captures the distance between the optimal solution \((\bm{\theta}^{*},\lambda^{*})\) to the true neural network function \(f(\bm{\theta}_{k};\bm{\phi}(s,a))\) in(6) and the optimal one \((\bm{\theta}_{0}^{*},y_{0}(\bm{\theta}_{0}^{*}))\) with local linearization \(f_{0}(\bm{\theta}_{k};\bm{\phi}(s,a))\) in (18), which quantifies the error when \(f(\bm{\theta}^{*})\) does not fall into the function class \(\mathcal{F}\). The last term characterizes the distance between \(f(\bm{\theta}_{k};\bm{\phi}(s,a))\) and \(f_{0}(\bm{\theta}_{k};\bm{\phi}(s,a))\) with any \(\bm{\theta}_{k}\). Both terms diminish as \(m\to\infty\). Theorem 1 implies the convergence to the optimal value \((\bm{\theta}^{*},\lambda^{*})\) is bounded by the approximation error, which will diminish to zero as representation power of \(f_{0}(\bm{\theta}_{k};\bm{\phi}(s,a))\) increases when \(m\to\infty\). Finally, we note that the right hand side (17) ends up in \(\mathcal{O}(1/k^{2})+\mathcal{O}(1/k^{2/3})+c\), where \(c\) is a constant and its value goes to \(0\) as \(m\to\infty\). This indicates the error bounds of linearization with the original neural network functions are controlled by the overparameterization value of \(m\). Need to mention that a constant step size will result in a non-vanishing accumulated error as in [15].

**Remark 2**.: _A finite-time analysis of nonlinear 2TSA was presented in [39]. However, [39] required a stability condition that \(\lim_{k\to\infty}(\bm{\theta}_{k},\lambda_{k})=(\bm{\theta}^{*},\lambda^{*})\), and both \(h\) and \(g\) are locally approximated as linear functions. [19; 60] relaxed these conditions and provided a finite-time analysis under i.i.d. noise. These results were later extended to Markovian noise [20] under the assumption that \(H\) function is strongly monotone in \(\bm{\theta}\) and \(G\) function is strongly monotone in \(\lambda\). Since [20] leveraged the techniques in [19], it needed to explicitly characterize the covariance between the error caused by Markovian noise and the parameters' residual error in (14), leading to the convergence analysis much more intrinsic. [15] exploited the mixing time to avoid the covariance between the error caused by Markovian noise and the parameters' residual error, however, it only considered the single timescale \(Q\)-learning with linear function approximation. Though our Neural-Q-Whittle can be rewritten as a nonlinear 2TSA, the nonlinear parameterization of \(Q\)-function caused by the neural network function approximation makes the aforementioned analysis not directly applicable to ours and requires additional characterization as highlighted in Figure 1. The explicit characterization of approximation errors further distinguish our work._

### Proof Sketch

In this section, we sketch the proofs of the three steps shown in Figure 1 as required for Theorem 1.

#### 4.3.1 Step 1: Approximated Solution of Neural-Q-Whittle

We first approximate the optimal solution by projecting the Q-function in (6) to some function classes parameterized by \(\bm{\theta}\). The common choice of the projected function classes is the local linearization of \(f(\bm{\theta};\bm{\phi}(s,a))\) at the initial point \(\bm{\theta}_{0}\)[13; 62], i.e., \(\mathcal{F}:=\{f_{0}(\bm{\theta};\bm{\phi}(s,a)),\forall\bm{\theta}\in\Theta\}\), where

\[f_{0}(\bm{\theta};\bm{\phi}(s,a))=\frac{1}{\sqrt{m}}\sum_{r=1}^{m}b_{r}\mathds {1}\{\mathbf{w}_{r,0}^{\mathsf{T}}\bm{\phi}(s,a)>0\}\mathbf{w}_{r}^{\mathsf{T }}\bm{\phi}(s,a).\] (18)

Then, we define the approximate stationary point \(\bm{\theta}_{0}^{*}\) with respect to \(f_{0}(\bm{\theta};\bm{\phi}(s,a))\) as follows.

**Definition 1**.: _[_13; 62_]_ _A point \(\bm{\theta}_{0}^{*}\in\Theta\) is said to be the approximate stationary point of Algorithm 1 if for all feasible \(\bm{\theta}\in\Theta\) it holds that \(\mathbb{E}_{\mu,\pi,\mathcal{P}}[(\Delta_{0}\cdot\nabla_{\bm{\theta}}f_{0}( \bm{\theta};\bm{\phi}(s,a)))^{\mathsf{T}}(\bm{\theta}-\bm{\theta}_{0}^{*})] \geq 0,\forall\bm{\theta}\in\Theta,\) with \(\Delta_{0}:=[r(s,a)+(1-a)\lambda^{*}-I_{0}(\bm{\theta})+\max_{a^{\prime}}f_{0 }(\bm{\theta};\bm{\phi}(s^{\prime},a))-f_{0}(\bm{\theta};\bm{\phi}(s,a))]\), where \(I_{0}(\bm{\theta})=\frac{1}{2S}\sum_{s\in\mathcal{S}}[f_{0}(\bm{\theta};\bm{ \phi}(s,0))+f_{0}(\bm{\theta};\bm{\phi}(s,1))]\)._

Figure 1: Neural-Q-Whittle operates w.r.t. true neural function \(f(\cdot)\) with its finite-time performance given in Theorem 1 (indicated in dashed lines). Our proofs operate in three steps: (i) Step 1: Obtain local linearization \(f_{0}(\cdot)\) and define Lyapunov function \(\hat{M}(\cdot)\) w.r.t. \(f_{0}(\cdot)\). (ii) Step 2: Characterize the finite-time performance w.r.t. \(\hat{M}(\cdot)\) using Lyapunov drift method. Since Neural-Q-Whittle is updated w.r.t. \(f(\cdot)\), we need to characterize the gap between \(f(\cdot)\) and \(f_{0}(\cdot)\). (iii) Step 3: Similarly, we characterize the approximation errors between \(M(\cdot)\) and \(\hat{M}(\cdot)\).

Though there is a gap between the true neural function (6) and the approximated local linearized function (18), the gap diminishes as the width of neural network i.e., \(m\), becomes large [13, 62].

With the approximated stationary point \(\boldsymbol{\theta}_{0}^{*}\), we can redefine the two error terms in (14) as

\[\hat{\boldsymbol{\theta}}_{k}=\boldsymbol{\theta}_{k}-\boldsymbol{\theta}_{0}^ {*},\qquad\hat{\lambda}_{k}=\lambda_{k}-y_{0}(\boldsymbol{\theta}_{k}),\] (19)

using which we correspondingly define a modified Lyapunov function \(\hat{M}(\boldsymbol{\theta}_{k},\lambda_{k})\) in (16), where

\[y_{0}(\boldsymbol{\theta})\!=\!r(s,1)\!+\!\sum_{s^{\prime}}\!p(s^{\prime}|s,1) \max_{a}\!f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s^{\prime},a))\!-\!r( s,0)\!\!\sum_{s^{\prime}}p(s^{\prime}|s,0)\max_{a}\!f_{0}(\boldsymbol{ \theta};\boldsymbol{\phi}(s^{\prime},a)).\] (20)

#### 4.3.2 Step 2: Convergence Rate of \(\hat{M}(\boldsymbol{\theta}_{k},\lambda_{k})\) in (16)

Since we approximate the true neural network function \(f(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))\) in (6) with the local linearized function \(f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))\) in (18), the operators \(h(\cdot)\) and \(g(\cdot)\) in (10)-(11) turn correspondingly to be

\[h_{0}(X_{k},\boldsymbol{\theta}_{k},\lambda_{k})=\nabla_{\boldsymbol{\theta} }f_{0}(\boldsymbol{\theta}_{k};\boldsymbol{\phi}(S_{k},A_{k}))\Delta_{k,0},\ g_{0}( \boldsymbol{\theta}_{k}):=f_{0}(\boldsymbol{\theta}_{k};\boldsymbol{\phi}(s,1 ))-f_{0}(\boldsymbol{\theta}_{k};\boldsymbol{\phi}(s,0)),\] (21)

with \(\Delta_{k,0}:=r(S_{k},A_{k})+(1-A_{k})\lambda_{k}-I_{0}(\boldsymbol{\theta}_{ k})+\max_{a}f_{0}(\boldsymbol{\theta}_{k};\boldsymbol{\phi}(S_{k+1},a))-f_{0}( \boldsymbol{\theta}_{k};\boldsymbol{\phi}(S_{k},A_{k})).\)

Before we present the finite-time error bound of the nonlinear 2TSA (9) under Markovian noise, we first discuss the mixing time of the Markov chain \(\{X_{k}\}\) and our assumptions.

**Definition 2** (Mixing time [15]).: _For any \(\delta>0\), define \(\tau_{\delta}\) as_

\[\tau_{\delta}=\!\min\{k\geq 1:\|\mathbb{E}[h_{0}(X_{k},\boldsymbol{\theta}, \lambda)|X_{0}=x]-H_{0}(\boldsymbol{\theta},\lambda)\|\leq\delta(\|\boldsymbol {\theta}-\boldsymbol{\theta}_{0}^{*}\|+\|\lambda-y_{0}(\boldsymbol{\theta}_{0 }^{*}\|)\|)\}.\] (22)

**Assumption 1**.: _The Markov chain \(\{X_{k}\}\) is irreducible and aperiodic. Hence, there exists a unique stationary distribution \(\mu\)[32], and constants \(C>0\) and \(\rho\in(0,1)\) such that \(d_{TV}(P(X_{k}|X_{0}=x),\mu)\leq C\rho^{k},\forall k\geq 0,x\in\mathcal{X},\) where \(d_{TV}(\cdot,\cdot)\) is the total-variation (TV) distance [32]._

**Remark 3**.: _Assumption 1 is often assumed to study the asymptotic convergence of stochastic approximation under Markovian noise [4, 9, 15]._

**Lemma 1**.: _The function \(h_{0}(X,\boldsymbol{\theta},\lambda)\) defined in (21) is globally Lipschitz continuous w.r.t \(\boldsymbol{\theta}\) and \(\lambda\) uniformly in \(X\), i.e., \(\|h_{0}(X,\boldsymbol{\theta}_{1},\lambda_{1})\!-\!h_{0}(X,\boldsymbol{\theta} _{2},\lambda_{2})\|\leq L_{h,1}\|\boldsymbol{\theta}_{1}-\boldsymbol{\theta}_ {2}\|+L_{h,2}\|\lambda_{1}-\lambda_{2}\|,\forall X\in\mathcal{X}\), and \(L_{h,1}=3,h_{h,2}=1\) are valid Lipschitz constants._

**Lemma 2**.: _The function \(g_{0}(\boldsymbol{\theta})\) defined in (21) is linear and thus Lipschitz continuous in \(\boldsymbol{\theta}\), i.e., \(\|g_{0}(\boldsymbol{\theta}_{1})-g_{0}(\boldsymbol{\theta}_{2})\|\leq L_{g} \|\boldsymbol{\theta}_{1}-\boldsymbol{\theta}_{2}\|\), and \(L_{g}=2\) is a valid Lipschitz constant._

**Lemma 3**.: _The function \(y_{0}(\boldsymbol{\theta})\) defined in (20) is linear and thus Lipschitz continuous in \(\boldsymbol{\theta}\), i.e., \(\|y_{0}(\boldsymbol{\theta}_{1})-y_{0}(\boldsymbol{\theta}_{2})\|\leq L_{y} \|\boldsymbol{\theta}_{1}-\boldsymbol{\theta}_{2}\|\), and \(L_{y}=2\) is a valid Lipschitz constant._

**Remark 4**.: _The Lipschitz continuity of \(h_{0}\) guarantees the existence of a solution \(\boldsymbol{\theta}\) to the ODE \(\dot{\boldsymbol{\theta}}\) for a fixed \(\lambda\), while the Lipschitz continuity of \(g_{0}\) and \(y_{0}\) ensures the existence of a solution \(\lambda\) to the ODE \(\dot{\lambda}\) when \(\boldsymbol{\theta}\) is fixed. These lemmas often serve as assumptions when proving the convergence rate for both linear and nonlinear 2TSA [31, 39, 18, 24, 19, 17, 27]._

**Lemma 4**.: _For a fixed \(\lambda\), there exists a constant \(\mu_{1}>0\) such that \(h_{0}(X,\boldsymbol{\theta},\lambda)\) defined in (10) satisfies_

\[\mathbb{E}[\dot{\boldsymbol{\theta}}^{\mathsf{T}}h_{0}(X,\boldsymbol{\theta}, \lambda)]\leq-\mu_{1}\|\dot{\boldsymbol{\theta}}\|^{2}.\]

_For fixed \(\boldsymbol{\theta}\), there exists a constant \(\mu_{2}>0\) such that \(g_{0}(X,\boldsymbol{\theta},\lambda)\) defined in (11) satisfies_

\[\mathbb{E}[\dot{\lambda}g_{0}(X,\boldsymbol{\theta},\lambda)]\leq-\mu_{2}\| \dot{\lambda}\|^{2}.\]

**Remark 5**.: _Lemma 4 guarantees the stability and uniqueness of the solution \(\boldsymbol{\theta}\) to the ODE \(\dot{\boldsymbol{\theta}}\) for a fixed \(\lambda\), and the uniqueness of the solution \(\lambda\) to the ODE \(\dot{\lambda}\) for a fixed \(\boldsymbol{\theta}\). This assumption can be viewed as a relaxation of the stronger monotone property of nonlinear mappings [19, 15], since it is automatically satisfied if \(h\) and \(g\) are strong monotone as assumed in [19]._

**Lemma 5**.: _Under Assumption 1 and Lemma 1, there exist constants \(C>0\), \(\rho\in(0,1)\) and \(L=\max(3,\max_{X}h_{0}(X,\boldsymbol{\theta}_{0}^{*}),y_{0}(\boldsymbol{ \theta}_{0}^{*}))\) such that_

\[\tau_{\delta}\leq\frac{\log(1/\delta)+\log(2LCmd)}{\log(1/\rho)}.\]

**Remark 6**.: \(\tau_{\delta}\) _is equivalent to the mixing time of the underlying Markov chain satisfying \(\lim_{\delta\to 0}\delta\tau_{\delta}=0\)[15]. For simplicity, we remove the subscript and denote it as \(\tau\)._

We now present the finite-time error bound for the Lyapunov function \(\hat{M}(\boldsymbol{\theta}_{k},\lambda_{k})\) in (16).

**Theorem 2**.: _Consider iterates \(\{\boldsymbol{\theta}_{k}\}\) and \(\{\lambda_{k}\}\) generated by_ Neural-Q-Whittle _in (7) and (8). Given Lemma 1-4, \(\alpha_{k}=\frac{\alpha_{0}}{(k+1)},\eta_{k}=\frac{\eta_{0}}{(k+1)^{4/3}}\), \(\hat{C}_{1}:=c_{1}(\|\boldsymbol{\theta}_{0}\|+\|\lambda_{0}\|+1)\) with a constant \(c_{1}\),_

\[\mathbb{E}[\hat{M}(\boldsymbol{\theta}_{k+1},\lambda_{k+1})| \mathcal{F}_{k-\tau}]\leq\frac{\tau^{2}\mathbb{E}[\hat{M}(\boldsymbol{ \theta}_{\tau},\lambda_{\tau})]}{(k+1)^{2}}+\frac{600\alpha_{0}^{3}}{\eta_{0 }}\frac{(C_{1}+\|\hat{\boldsymbol{\theta}}_{0}\|)^{2}+(2C_{1}+\|\hat{\lambda} _{0}\|)^{2}}{(k+1)^{2/3}}\\ +\frac{\mathcal{O}\Big{(}c_{1}^{3}(\|\boldsymbol{\theta}_{0}\| \!+\!|\lambda_{0}|\!+\!1)^{3}m^{-1/2}\Big{)}}{(k+1)^{2/3}},\quad\forall k\geq\tau.\] (23)

3.3 Step 3: Approximation Error between \(M(\boldsymbol{\theta}_{k},\lambda_{k})\) and \(\hat{M}(\boldsymbol{\theta}_{k},\lambda_{k})\)

Finally, we characterize the approximation error between Lyapunov functions \(M(\boldsymbol{\theta}_{k},\lambda_{k})\) and \(\hat{M}(\boldsymbol{\theta}_{k},\lambda_{k})\). Since we are dealing with long-term average MDP, we assume that the total variation of the MDP is bounded [47].

**Assumption 2**.: _There exists \(0<\kappa<1\) such that \(\sup_{(s,a),(s^{\prime},a^{\prime})}\|p(\cdot|s,a)-p(\cdot|s^{\prime},a^{ \prime})\|_{TV}=2\kappa\)._

Hence, the Bellman operator is a span-contraction operator [47], i.e.,

\[span(\mathcal{T}f_{0}(\boldsymbol{\theta}_{0}^{*})-\mathcal{T}f(\boldsymbol{ \theta}^{*}))\leq\kappa\;span(f_{0}(\boldsymbol{\theta}_{0}^{*})-f(\boldsymbol {\theta}^{*})).\] (24)

**Assumption 3**.: \(\|\boldsymbol{\theta}_{0}^{*}-\boldsymbol{\theta}^{*}\|\leq c_{0}\|span(f_{0} (\boldsymbol{\theta}_{0}^{*})-f(\boldsymbol{\theta}^{*}))\|\)_, with \(c_{0}\) being a positive constant._

**Lemma 6**.: _For \(M(\boldsymbol{\theta}_{k},\lambda_{k})\) in (15) and \(\hat{M}(\boldsymbol{\theta}_{k},\lambda_{k})\) in (16), with constants \(c_{1}\) and \(c_{0}\) (Assumption 3),_

\[M(\boldsymbol{\theta}_{k},\lambda_{k})\leq 2\hat{M}(\boldsymbol{\theta}_{k}, \lambda_{k})+\frac{2\eta_{k}c_{0}^{2}}{\alpha_{k}(1-\kappa)}\|span(\Pi_{ \mathcal{F}}f(\boldsymbol{\theta}^{*})-f(\boldsymbol{\theta}^{*}))\|+2 \mathcal{O}\Big{(}\frac{c_{1}^{3}(\|\boldsymbol{\theta}_{0}\|\!+\!|\lambda_{0} |\!+\!1)^{3}}{m^{1/2}}\Big{)}.\]

## 5 Numerical Experiments

We numerically evaluate the performance of Neural-Q-Whittle using an example of circulant dynamics [23, 3, 8]. The state space is \(\mathcal{S}=\{1,2,3,4\}\). Rewards are \(r(1,a)=-1,r(2,a)=r(3,a)=0\), and \(r(4,a)=1\) for \(a\in\{0,1\}\). The dynamics of states are circulant and defined as

\[P^{1}=\begin{bmatrix}0.5&0.5&0&0\\ 0&0.5&0.5&0\\ 0&0&0.5&0.5\\ 0.5&0&0&0.5\end{bmatrix}\text{ and }P^{0}=\begin{bmatrix}0.5&0&0&0.5\\ 0.5&0.5&0&0\\ 0&0.5&0.5&0\\ 0&0&0.5&0.5\end{bmatrix}.\]

This indicates that the process either remains in its current state or increments if it is active (i.e., \(a=1\)), or it either remains the current state or decrements if it is passive (i.e., \(a=0\)). The exact value of Whittle indices [23] are \(\lambda(1)=-0.5,\lambda(2)=0.5,\lambda(3)=1,\) and \(\lambda(4)=-1\).

In our experiments, we set the learning rates as \(\alpha_{k}=0.5/(k+1)\) and \(\eta_{k}=0.1/(k+1)^{4/3}\). We use \(\epsilon\)-greedy for the exploration and exploitation tradeoff with \(\epsilon=0.5\). We consider a two-layer neural network with the number of neurons in the hidden layer as \(m=200\). As described in Algorithm 1, \(b_{r},\forall r\) are uniformly initialized in \(\{-1,1\}\) and \(w_{r},\forall r\) are initialized as a zero mean Gaussian distribution according to \(\mathcal{N}(\textbf{0},\textbf{I}_{d}/d)\). These results are carried out by Monte Carlo simulations with 100 independent trials.

Figure 2: Convergence of Neural-Q-Whittle.

**Convergence to true Whittle index.** First, we verify that Neural-Q-Whittle convergences to true Whittle indices, and compare to Q-Whittle, the first Whittle index based Q-learning algorithm. As illustrated in Figure 1(a), Neural-Q-Whittle guarantees the convergence to true Whittle indices and outperforms Q-Whittle [3] in the convergence speed. This is due to the fact that Neural-Q-Whittle updates the Whittle index of a specific state even when the current visited state is not that state.

Second, we further compare with other other Whittle index learning algorithms, i.e., Q-Whittle-LFA [60], WIQL [8] and QWIC [23]in Figure 3. As we observe from Figure 3, only Neural-Q-Whittle and Q-Whittle-LFA in [60] can converge to the true Whittle indices for each state, while the other two benchmarks algorithms do not guarantee the convergence of true Whittle indices. Interestingly, the learning Whittle indices converge and maintain a correct relative order of magnitude, which is still be able to be used in real world problems [60]. Moreover, we observe that Neural-Q-Whittle achieves similar convergence performance as Q-Whittle-LFA in the considered example, whereas the latter has been shown to achieve good performance in real world applications in [60]. Though this work focuses on the theoretical convergence analysis of Q-learning based whittle index under the neural network function approximation, it might be promising to implement it in real-world applications to fully leverage the strong representation ability of neural network functions, which serves as future investigation of this work.

**Convergence of the Lyapunov function defined in (15).** We also evaluate the convergence of the proposed Lyapunov function defined in (15), which is presented in Figure 1(b). It depicts \(\mathbb{E}[M(\boldsymbol{\theta}_{k},\lambda_{k})]\) vs. the number of iterations in logarithmic scale. For ease of presentation, we only take state \(s=4\) as an illustrative example. It is clear that \(M(\boldsymbol{\theta}_{k},\lambda_{k})\) converges to zero as the number of iterations increases, which is in alignment with our theoretical results in Theorem 1.

**Verification of Assumption 3.** We now verify Assumption 3 that the gap between \(\boldsymbol{\theta}_{0}^{*}\) and \(\boldsymbol{\theta}^{*}\) can be bounded by the span of \(f_{0}(\boldsymbol{\theta}_{0}^{*})\) and \(f(\boldsymbol{\theta}^{*})\) with a constant \(c_{0}\). In Figure 4, we show \(c_{0}\) as a function of the number of neurons in the hidden layer \(m\). It clearly indicates that constant \(c_{0}\) exists and decreases as the number of neurons grows larger.

## 6 Conclusion

We presented Neural-Q-Whittle, a Whittle index based Q-learning algorithm for RMAB with neural network function approximation. We proved that Neural-Q-Whittle achieves an \(\mathcal{O}(1/k^{2/3})\) convergence rate, where \(k\) is the number of iterations when data are generated from a Markov chain and Q-function is approximated by a ReLU neural network. By viewing Neural-Q-Whittle as 2TSA and leveraging the Lyapunov drift method, we removed the projection step on parameter update of Q-learning with neural network function approximation. Extending the current framework to two-timescale Q-learning (i.e., the coupled iterates between Q-function values and Whittle indices) with general deep neural network approximation is our future work.

Figure 4: Verification of Assumption 3 w.r.t the constant \(c_{0}\).

Figure 3: Convergence comparison between Neural-Q-Whittle and benchmark algorithms.

## Acknowledgements

This work was supported in part by the National Science Foundation (NSF) grant RINGS-2148309, and was supported in part by funds from OUSD R&E, NIST, and industry partners as specified in the Resilient & Intelligent NextG Systems (RINGS) program. This work was also supported in part by the U.S. Army Research Office (ARO) grant W911NF-23-1-0072, and the U.S. Department of Energy (DOE) grant DE-EE0009341. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.

## References

* [1] Jinane Abounadi, Dimitrib Bertsekas, and Vivek S Borkar. Learning algorithms for markov decision processes with average cost. _SIAM Journal on Control and Optimization_, 40(3):681-698, 2001.
* [2] Joshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep q-learning. _arXiv preprint arXiv:1903.08894_, 2019.
* [3] Konstantin E Avrachenkov and Vivek S Borkar. Whittle index based q-learning for restless bandits with average reward. _Automatica_, 139:110186, 2022.
* [4] Dimitri Bertsekas and John N Tsitsiklis. _Neuro-dynamic programming_. Athena Scientific, 1996.
* [5] Dimitris Bertsimas and Jose Nino-Mora. Restless Bandits, Linear Programming Relaxations, and A Primal-Dual Index Heuristic. _Operations Research_, 48(1):80-90, 2000.
* [6] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation. In _Conference on learning theory_, pages 1691-1692. PMLR, 2018.
* [7] Shalabh Bhatnagar, Richard S Sutton, Mohammad Ghavamzadeh, and Mark Lee. Natural Actor-Critic Algorithms. _Automatica_, 45(11):2471-2482, 2009.
* [8] Arpita Biswas, Gaurav Aggarwal, Pradeep Varakantham, and Milind Tambe. Learn to intervene: An adaptive learning policy for restless bandits in application to preventive healthcare. In _Proc. of IJCAI_, 2021.
* [9] Vivek S Borkar. _Stochastic Approximation: A Dynamical Systems Viewpoint_, volume 48. Springer, 2009.
* [10] Vivek S Borkar and Karan Chadha. A reinforcement learning algorithm for restless bandits. In _2018 Indian Control Conference (ICC)_, pages 89-94. IEEE, 2018.
* [11] Vivek S Borkar and Vijaymohan R Konda. The Actor-Critic Algorithm as Multi-Time-Scale Stochastic Approximation. _Sadhana_, 22(4):525-543, 1997.
* [12] Vivek S Borkar, K Ravikumar, and Krishnakant Saboo. An index policy for dynamic pricing in cloud computing under price commitments. _Applicationes Mathematicae_, 44:215-245, 2017.
* [13] Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal difference and q learning provably converge to global optima. _Mathematics of Operations Research_, 2023.
* [14] Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. Finite-Sample Analysis of Stochastic Approximation Using Smooth Convex Envelopes. _arXiv e-prints_, pages arXiv-2002, 2020.
* [15] Zaiwei Chen, Sheng Zhang, Thinh T Doan, Siva Theja Maguluri, and John-Paul Clarke. Performance of Q-learning with Linear Function Approximation: Stability and Finite-Time Analysis. _arXiv preprint arXiv:1905.11425_, 2019.
* [16] Wenhan Dai, Yi Gai, Bhaskar Krishnamachari, and Qing Zhao. The Non-Bayesian Restless Multi-Armed Bandit: A Case of Near-Logarithmic Regret. In _Proc. of IEEE ICASSP_, 2011.

* [17] Gal Dalal, Balazs Szorenyi, and Gugan Thoppe. A tale of two-timescale reinforcement learning with the tightest finite-time bound. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 3701-3708, 2020.
* [18] Gal Dalal, Gugan Thoppe, Balazs Szorenyi, and Shie Mannor. Finite sample analysis of two-timescale stochastic approximation with applications to reinforcement learning. In _Conference On Learning Theory_, pages 1199-1233. PMLR, 2018.
* [19] Thinh T Doan. Nonlinear two-time-scale stochastic approximation: Convergence and finite-time performance. _arXiv preprint arXiv:2011.01868_, 2020.
* [20] Thinh T Doan. Finite-time convergence rates of nonlinear two-time-scale stochastic approximation under markovian noise. _arXiv preprint arXiv:2104.01627_, 2021.
* [21] Thinh T Doan and Justin Romberg. Linear Two-Time-Scale Stochastic Approximation A Finite-Time Analysis. In _Proc. of Allerton_, 2019.
* [22] Jianqing Fan, Zhaoran Wang, Yuchen Xie, and Zhuoran Yang. A theoretical analysis of deep q-learning. In _Learning for Dynamics and Control_, pages 486-489. PMLR, 2020.
* [23] Jing Fu, Yoni Nazarathy, Sarat Moka, and Peter G Taylor. Towards q-learning the whittle index for restless bandits. In _2019 Australian & New Zealand Control Conference (ANZCC)_, pages 249-254. IEEE, 2019.
* [24] Harsh Gupta, Rayadurgam Srikant, and Lei Ying. Finite-time performance bounds and adaptive learning rate selection for two time-scale reinforcement learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* [25] Bowen Jiang, Bo Jiang, Jian Li, Tao Lin, Xinbing Wang, and Chenghu Zhou. Online restless bandits with unobserved states. In _Proc. of ICML_, 2023.
* [26] Young Hun Jung and Ambuj Tewari. Regret Bounds for Thompson Sampling in Episodic Restless Bandit Problems. _Proc. of NeurIPS_, 2019.
* [27] Maxim Kaledin, Eric Moulines, Alexey Naumov, Vladislav Tadic, and Hoi-To Wai. Finite time analysis of linear two-timescale stochastic approximation with markovian noise. In _Conference on Learning Theory_, pages 2144-2203. PMLR, 2020.
* [28] Jackson A Killian, Arpita Biswas, Sanket Shah, and Milind Tambe. Q-learning lagrange policies for multi-action restless bandits. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 871-881, 2021.
* [29] Jackson A Killian, Andrew Perrault, and Milind Tambe. Beyond" To Act or Not to Act": Fast Lagrangian Approaches to General Multi-Action Restless Bandits. In _Proc.of AAMAS_, 2021.
* [30] Vijay R Konda and John N Tsitsiklis. Actor-Critic Algorithms. In _Proc. of NIPS_, 2000.
* [31] Vijay R Konda and John N Tsitsiklis. Convergence rate of linear two-time-scale stochastic approximation. _The Annals of Applied Probability_, 14(2):796-819, 2004.
* [32] David A Levin and Yuval Peres. _Markov chains and mixing times_, volume 107. American Mathematical Soc., 2017.
* [33] Haoyang Liu, Keqin Liu, and Qing Zhao. Logarithmic Weak Regret of Non-Bayesian Restless Multi-Armed Bandit. In _Proc of IEEE ICASSP_, 2011.
* [34] Haoyang Liu, Keqin Liu, and Qing Zhao. Learning in A Changing World: Restless Multi-Armed Bandit with Unknown Dynamics. _IEEE Transactions on Information Theory_, 59(3):1902-1916, 2012.
* [35] Aditya Mate, Jackson Killian, Haifeng Xu, Andrew Perrault, and Milind Tambe. Collapsing bandits and their application to public health intervention. _Advances in Neural Information Processing Systems_, 33:15639-15650, 2020.

* [36] Aditya Mate, Andrew Perrault, and Milind Tambe. Risk-Aware Interventions in Public Health: Planning with Restless Multi-Armed Bandits. In _Proc.of AAMAS_, 2021.
* [37] Francisco S Melo, Sean P Meyn, and M Isabel Ribeiro. An analysis of reinforcement learning with function approximation. In _Proceedings of the 25th international conference on Machine learning_, pages 664-671, 2008.
* [38] Rahul Meshram, Aditya Gopalan, and D Manjunath. Optimal recommendation to users that react: Online learning for a class of pomdps. In _2016 IEEE 55th Conference on Decision and Control (CDC)_, pages 7210-7215. IEEE, 2016.
* [39] Abdelkader Mokkadem and Mariane Pelletier. Convergence rate and averaging of nonlinear two-time-scale stochastic approximation algorithms. _The Annals of Applied Probability_, 16(3):1671-1702, 2006.
* [40] Khaled Nakhleh, Santosh Ganji, Ping-Chun Hsieh, I Hou, Srinivas Shakkottai, et al. Neurwin: Neural whittle index network for restless bandits via deep rl. _Advances in Neural Information Processing Systems_, 34, 2021.
* [41] Khaled Nakhleh, I Hou, et al. Deeptop: Deep threshold-optimal policy for mdps and rnabs. _arXiv preprint arXiv:2209.08646_, 2022.
* [42] Ronald Ortner, Daniil Ryabko, Peter Auer, and Remi Munos. Regret Bounds for Restless Markov Bandits. In _Proc. of Algorithmic Learning Theory_, 2012.
* [43] Tejas Pagare, Vivek Borkar, and Konstantin Avrachenkov. Full gradient deep reinforcement learning for average-reward criterion. _arXiv preprint arXiv:2304.03729_, 2023.
* [44] Christos H Papadimitriou and John N Tsitsiklis. The Complexity of Optimal Queueing Network Control. In _Proc. of IEEE Conference on Structure in Complexity Theory_, 1994.
* [45] Martin L Puterman. _Markov Decision Processes: Discrete Stochastic Dynamic Programming_. John Wiley & Sons, 1994.
* [46] Guannan Qu and Adam Wierman. Finite-Time Analysis of Asynchronous Stochastic Approximation and \(Q\)-Learning. In _Proc. of COLT_, 2020.
* [47] Hiteshi Sharma, Mehdi Jafarnia-Jahromi, and Rahul Jain. Approximate relative value learning for average-reward continuous state mdps. In _Uncertainty in Artificial Intelligence_, pages 956-964. PMLR, 2020.
* [48] Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation andtd learning. In _Conference on Learning Theory_, pages 2803-2830. PMLR, 2019.
* [49] Wesley Suttle, Kaiqing Zhang, Zhuoran Yang, Ji Liu, and David Kraemer. Reinforcement Learning for Cost-Aware Markov Decision Processes. In _Proc. of ICML_, 2021.
* [50] Cem Tekin and Mingyan Liu. Online Learning of Rested and Restless Bandits. _IEEE Transactions on Information Theory_, 58(8):5588-5611, 2012.
* [51] John N Tsitsiklis and Benjamin Van Roy. Average Cost Temporal-Difference Learning. _Automatica_, 35(11):1799-1808, 1999.
* [52] Yi Wan, Abhishek Naik, and Richard S Sutton. Learning and Planning in Average-Reward Markov Decision Processes. In _Proc. of ICML_, 2021.
* [53] Siwei Wang, Longbo Huang, and John Lui. Restless-UCB, an Efficient and Low-complexity Algorithm for Online Restless Bandits. In _Proc. of NeurIPS_, 2020.
* [54] Richard R Weber and Gideon Weiss. On An Index Policy for Restless Bandits. _Journal of applied probability_, pages 637-648, 1990.
* [55] Chen-Yu Wei, Mehdi Jafarnia Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain. Model-free Reinforcement Learning in Infinite-Horizon Average-Reward Markov Decision Processes. In _Proc. of ICML_, 2020.

* [56] Peter Whittle. Restless Bandits: Activity Allocation in A Changing World. _Journal of applied probability_, pages 287-298, 1988.
* [57] Guojun Xiong, Jian Li, and Rahul Singh. Reinforcement Learning Augmented Asymptotically Optimal Index Policies for Finite-Horizon Restless Bandits. In _Proc. of AAAI_, 2022.
* [58] Guojun Xiong, Xudong Qin, Bin Li, Rahul Singh, and Jian Li. Index-aware Reinforcement Learning for Adaptive Video Streaming at the Wireless Edge. In _Proc. of ACM MobiHoc_, 2022.
* [59] Guojun Xiong, Shufan Wang, and Jian Li. Learning infinite-horizon average-reward restless multi-action bandits via index awareness. _Proc. of NeurIPS_, 2022.
* [60] Guojun Xiong, Shufan Wang, Jian Li, and Rahul Singh. Whittle index based q-learning for wireless edge caching with linear function approximation. _arXiv preprint arXiv:2202.13187_, 2022.
* [61] Guojun Xiong, Shufan Wang, Gang Yan, and Jian Li. Reinforcement Learning for Dynamic Dimensioning of Cloud Caches: A Restless Bandit Approach. In _Proc. of IEEE INFOCOM_, 2022.
* [62] Pan Xu and Quanquan Gu. A finite-time analysis of q-learning with neural network function approximation. In _International Conference on Machine Learning_, pages 10555-10565. PMLR, 2020.
* [63] Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang. Provably Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost. In _Proc. of NeurIPS_, 2019.
* [64] Zhe Yu, Yunjian Xu, and Lang Tong. Deadline Scheduling as Restless Bandits. _IEEE Transactions on Automatic Control_, 63(8):2343-2358, 2018.
* [65] Shangtong Zhang, Yi Wan, Richard S Sutton, and Shimon Whiteson. Average-Reward Off-Policy Policy Evaluation with Function Approximation. _arXiv preprint arXiv:2101.02808_, 2021.
* [66] Sheng Zhang, Zhe Zhang, and Siva Theja Maguluri. Finite Sample Analysis of Average-Reward TD Learning and \(Q\)-Learning. _Proc. of NeurIPS_, 2021.
* [67] Shaofeng Zou, Tengyu Xu, and Yingbin Liang. Finite-sample analysis for sarsa with linear function approximation. _Advances in neural information processing systems_, 32, 2019.

Related Work

**Online Restless Bandits.** The online RMAB setting, where the underlying MDPs are unknown, has been gaining attention, e.g., [16; 33; 34; 50; 42; 26]. However, these methods do not exploit the special structure available in the problem and contend directly with an extremely high dimensional state-action space yielding the algorithms to be too slow to be useful. Recently, RL based algorithms have been developed [10; 23; 53; 8; 28; 57; 59; 61; 58; 3], to explore the problem structure through index policies. For instance, [23] proposed a Q-learning algorithm for Whittle index under the discounted setting, which lacks of convergence guarantees. [8] approximated Whittle index using the difference of \(Q(s,1)-Q(s,0)\) for any state \(s\), which is not guaranteed to converge to the true Whittle index in general scenarios. To our best knowledge, the Q-Whittle in (4)-(5) proposed by [3] is the first algorithm with a rigorous asymptotic analysis. Therefore, [23; 3; 8; 28] lacked finite-time performance analysis and multi-timescale stochastic approximation algorithms usually suffer from slow convergence.

[53; 57] designed model-based low-complexity policy but is constrained to either a specific Markovian model or depends on a simulator for a finite-horizon setting which cannot be directly applied here. Latter on, [60] showed the finite-time convergence performance under the Q-Whittle setting of [3] with linear function approximation. However, the underlying assumption in [3; 60] is that data samples are drawn i.i.d per iteration. This is often not the case in practice since data samples of Q-learning are drawn according to the underlying Markov decision process. Till now, the finite-time convergence rate of Q-Whittle under the more challenging Markovian setting remains to be an open problem. Though [43] proposed a novel DQN method and applied it to Whittle index learning, it lacks of theoretical convergence analysis. To our best knowledge, our work is the first to study low-complexity model-free Q-learning for RMAB with neural network function approximation and provide a finite-time performance guarantee.

**Two-Timescale Stochastic Approximation.** The theoretical understanding of average-reward reinforcement learning (RL) methods is limited. Most existing results focus on asymptotic convergence [51; 1; 52; 65], or finite-time performance guarantee for discounted Q-learning [15; 46; 14]. However, the analysis of average-reward RL algorithms is known to be more challenging than their discounted-reward counterparts [66; 55]. In particular, our Neural-Q-Whittle follows the 2TSA scheme [11; 30; 7]. The standard technique for analyzing 2TSA is via the ODE method to prove asymptotic convergence [9]. Building off the importance of asymptotic results, recent years have witnessed a focus shifted to non-asymptotic, finite-time analysis of 2TSA [24; 21; 19; 63]. The closest work is [19], which characterized the convergence rate for a general non-linear 2TSA with i.i.d. noise. We generalize this result to provide a finite-time analysis of our Neural-Q-Whittle with Markovian noise. In addition, existing finite-time analysis, e.g., sample complexity [66] and regret [55] of Q-learning with average reward focus on a single-timescale SA, and hence cannot be directly applied to our Neural-Q-Whittle. Finally, existing Q-learning with linear function approximation [37; 6; 67] and neural network function approximation [13; 62] requires an additional projection step onto a bounded set related to the unknown stationary distribution of the underlying MDPs, or focuses on a single-timescale SA [15].

## Appendix B Review on Whittle Index Policy

Whittle index policy addresses the intractable issue of RMAB through decomposition. In each round \(t\), it first calculates the Whittle index for each arm \(n\) independently only based on its current state \(s_{n}(t)\), and then the Whittle index policy simply selects the \(K\) arms with the highest indices to activate. Following Whittle's approach[56], we can consider a system with only one arm due to the decomposition, and the Lagrangian is expressed as

\[L(\pi,\lambda)=\liminf_{T\rightarrow\infty}\frac{1}{T}\mathbb{E}_{\pi}\sum_{t =1}^{T}\Big{\{}r(t)+\lambda\Big{(}1-a(t)\Big{)}\Big{\}},\] (25)

where \(\lambda\) is the Lagrangian multiplier (or the subsidy for selecting passive action). For a particular \(\lambda\), the optimal activation policy can be expressed by a set of states in which it would activate this arm, which is denoted \(D(\lambda)\).

**Definition 3** (Indexiability).: _We denote \(D(\lambda)\) as the set of states \(S\) for which the optimal action for the arm is to choose a passive action, i.e., \(A=0\). Then the arm is said to be indexable if \(D(\lambda)\) increases with \(\lambda\), i.e., if \(\lambda>\lambda^{\prime}\), then \(D(\lambda)\supseteq D(\lambda^{\prime})\)._

Following the indexability property, the Whittle index in a particular state \(S\) is defined as follows.

**Definition 4** (Whittle Index).: _The Whittle index in state \(S\) for the indexable arm is the smallest value of the Lagrangian multiplier \(\lambda\) such that the optimal policy at state \(S\) is indifferent towards actions \(A=0\) and \(A=1\). We denote such a Whittle index as \(\lambda(S)\) satisfying \(\lambda(S):=\inf_{\lambda\geq 0}\{S\in D(\lambda)\}\)._

**Definition 5** (Whittle index policy).: _Whittle index policy is a controlled policy which activates the \(K\) arms with the highest whittle index \(\lambda_{i}(S_{i}(t))\) at each time slot \(t\)._

Appendix C Proof of Lemmas for "Step 2: Convergence Rate of \(\hat{M}(\boldsymbol{\theta}_{k},\lambda_{k})\) in (16)"

### Proof of Lemma 1

Proof.: Recall that

\[f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))=\frac{1}{\sqrt{m}}\sum_{r=1 }^{m}b_{r}\mathds{1}\{\mathbf{w}_{r,0}^{\intercal}\boldsymbol{\phi}(s,a)>0 \}\mathbf{w}_{r}^{\intercal}\boldsymbol{\phi}(s,a).\]

Thus we denote \(\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))\) as

\[\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s,a)):=\Big{[}\frac{1}{\sqrt{m}}b_{1}\mathds{1}\{\mathbf{w}_ {1,0}^{\intercal}\boldsymbol{\phi}(s,a)>0\}\boldsymbol{\phi}(s,a)^{\intercal},\ldots,\] \[\frac{1}{\sqrt{m}}b_{m}\mathds{1}\{\mathbf{w}_{m,0}^{\intercal} \boldsymbol{\phi}(s,a)>0\}\boldsymbol{\phi}(s,a)^{\intercal}\Big{]}^{\intercal}.\] (26)

Since \(\|\boldsymbol{\phi}(s,a)\|\leq 1,\forall s\in\mathcal{S},a\in\mathcal{A}\) and the fact that \(b_{r},\forall r\in[m]\) is uniformly initialized as \(1\) and \(-1\), we have \(\|\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a ))\|\leq 1\).

Therefore, we have the following inequality for any parameter pairs \((\boldsymbol{\theta}_{1},\lambda_{1})\) and \((\boldsymbol{\theta}_{2},\lambda_{2})\) with \(X=(s,a,s^{\prime})\in\mathcal{X}\),

\[\|h_{0}(X,\boldsymbol{\theta}_{1},\lambda_{1})-h_{0}(X,\boldsymbol {\theta}_{2},\lambda_{2})\|\] \[=\Big{\|}\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}_{ 1};\boldsymbol{\phi}(s,a))\Big{[}r(s,a)+(1-a)\lambda_{1}-I_{0}(\boldsymbol{ \theta}_{1})+\max_{a_{1}}f_{0}(\boldsymbol{\theta}_{1};\boldsymbol{\phi}(s^{ \prime},a_{1}))-f_{0}(\boldsymbol{\theta}_{1};\boldsymbol{\phi}(s,a))\Big{]}\] \[\quad-\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}_{ 2};\boldsymbol{\phi}(s,a))\Big{[}r(s,a)+(1-a)\lambda_{2}-I_{0}(\boldsymbol{ \theta}_{2})+\max_{a_{2}}f_{0}(\boldsymbol{\theta}_{2};\boldsymbol{\phi}(s^{ \prime},a_{2}))-f_{0}(\boldsymbol{\theta}_{2};\boldsymbol{\phi}(s,a))\Big{]} \Big{\|}\] \[\stackrel{{(a_{1})}}{{=}}\Big{\|}\nabla_{\boldsymbol{ \theta}}f_{0}(\boldsymbol{\theta}_{1};\boldsymbol{\phi}(s,a))\Big{[}(1-a)( \lambda_{1}-\lambda_{2})+I_{0}(\boldsymbol{\theta}_{2})-I_{0}(\boldsymbol{ \theta}_{1})+f_{0}(\boldsymbol{\theta}_{2};\boldsymbol{\phi}(s,a))-f_{0}( \boldsymbol{\theta}_{1};\boldsymbol{\phi}(s,a))\] \[\qquad\qquad\qquad\qquad\qquad+\max_{a_{1}}\Big{(}f_{0}( \boldsymbol{\theta}_{1};\boldsymbol{\phi}(s^{\prime},a_{1}))-\max_{a_{2}}f_{ 0}(\boldsymbol{\theta}_{2};\boldsymbol{\phi}(s^{\prime},a_{2}))\Big{]}\Big{\|}\] \[\stackrel{{(a_{2})}}{{\leq}}\|(1-a)(\lambda_{1}- \lambda_{2})\|+\|f_{0}(\boldsymbol{\theta}_{2};\boldsymbol{\phi}(s,a))-f_{0}( \boldsymbol{\theta}_{1};\boldsymbol{\phi}(s,a))\|\] \[\qquad\qquad\qquad\qquad+\bigg{\|}\frac{1}{2S}\sum_{\tilde{s} \in\mathcal{S}}f_{0}(\boldsymbol{\theta}_{2};\boldsymbol{\phi}(\tilde{s},0))-f_ {0}(\boldsymbol{\theta}_{1};\boldsymbol{\phi}(\tilde{s},0))+f_{0}(\boldsymbol {\theta}_{2};\boldsymbol{\phi}(\tilde{s},1))-f_{0}(\boldsymbol{\theta}_{1}; \boldsymbol{\phi}(\tilde{s},1))\bigg{\|}\] \[\qquad\qquad\qquad\qquad+\Big{\|}\max_{a_{1}}\Big{(}f_{0}( \boldsymbol{\theta}_{1};\boldsymbol{\phi}(s^{\prime},a_{1}))-\max_{a_{2}}f_{ 0}(\boldsymbol{\theta}_{2};\boldsymbol{\phi}(s^{\prime},a_{2}))\Big{\|}\] \[\stackrel{{(a_{3})}}{{\leq}}\|(1-a)(\lambda_{1}- \lambda_{2})\|+\|\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}_{1}; \boldsymbol{\phi}(s,a))(\boldsymbol{\theta}_{2}-\boldsymbol{\theta}_{1})\|\] \[\qquad\qquad\qquad\qquad+\bigg{\|}\frac{1}{2S}\sum_{\tilde{s} \in\mathcal{S}}\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}_{1}; \boldsymbol{\phi}(\tilde{s},0))(\boldsymbol{\theta}_{2}-\boldsymbol{\theta}_{1})+ \nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}_{1};\boldsymbol{\phi}( \tilde{s},1))(\boldsymbol{\theta}_{2}-\boldsymbol{\theta}_{1})\bigg{\|}\] \[\qquad\qquad\qquad+\Big{\|}\max_{a_{1}}\Big{(}f_{0}(\boldsymbol{ \theta}_{1};\boldsymbol{\phi}(s^{\prime},a_{1}))-\max_{a_{2}}f_{0}(\boldsymbol {\theta}_{2};\boldsymbol{\phi}(s^{\prime},a_{2}))\Big{\|}\] \[\stackrel{{(a_{4})}}{{\leq}}\|(\lambda_{1}-\lambda_{2}) \|+2\|\boldsymbol{\theta}_{1}-\boldsymbol{\theta}_{2}\|+\Big{\|}\max_{a_{1}} \Big{(}f_{0}(\boldsymbol{\theta}_{1};\boldsymbol{\phi}(s^{\prime},a_{1}))-\max_{a _{2}}f_{0}(\boldsymbol{\theta}_{2};\boldsymbol{\phi}(s^{\prime},a_{2}))\Big{\|}\]\[\leq\Big{\|}\bm{\theta}_{1}-\bm{\theta}_{2}\|,\]

with the last inequality holds due to (27) and (28).

### Proof of Lemma 4

Proof.: \(1)\) We first show that there exists a constant \(\mu_{1}>0\) such that \(\mathbb{E}[\hat{\boldsymbol{\theta}}^{\intercal}h_{0}(X,\boldsymbol{\theta}, \lambda)]\leq-\mu_{1}\|\hat{\boldsymbol{\theta}}\|^{2}\). According to the definition of \(\boldsymbol{\theta}_{0}^{*}\) given in Definition 1, \(\mathbb{E}[h_{0}(X,\boldsymbol{\theta}_{0}^{*},y_{0}(\boldsymbol{\theta}_{0}^ {*}))]=0\). Hence, we have

\[\mathbb{E}\left[\hat{\boldsymbol{\theta}}^{\intercal}(h_{0}(X, \boldsymbol{\theta},\lambda)-h_{0}(X,\boldsymbol{\theta}_{0}^{*},y_{0}( \boldsymbol{\theta}_{0}^{*}))\right]\] \[=\hat{\boldsymbol{\theta}}^{\intercal}\mathbb{E}[h_{0}(X, \boldsymbol{\theta},\lambda)-h_{0}(X,\boldsymbol{\theta}_{0}^{*},y_{0}( \boldsymbol{\theta}_{0}^{*}))]\] \[=\hat{\boldsymbol{\theta}}^{\intercal}\mathbb{E}\Big{[}\nabla_{ \boldsymbol{\theta}}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))\Big{[}r (s,a)+(1-a)\lambda-I_{0}(\boldsymbol{\theta})+\max_{a_{1}}f_{0}(\boldsymbol{ \theta};\boldsymbol{\phi}(s^{\prime},a_{1}))-f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s,a))\Big{]}\] \[\quad-\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}_{0 }^{*};\boldsymbol{\phi}(s,a))\Big{[}r(s,a)+(1-a)y_{0}(\boldsymbol{\theta}_{0}^ {*})-I_{0}(\boldsymbol{\theta}_{0}^{*})+\max_{a_{2}}f_{0}(\boldsymbol{\theta}_ {0}^{*};\boldsymbol{\phi}(s^{\prime},a_{2}))-f_{0}(\boldsymbol{\theta}_{0}^{ *};\boldsymbol{\phi}(s,a))\Big{]}\Big{]}\Big{]}\] \[\overset{(b_{1})}{=}\hat{\boldsymbol{\theta}}^{\intercal} \mathbb{E}\Big{[}\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s,a))\Big{[}(1-a)(\lambda-y_{0}(\boldsymbol{\theta}_{0}^{*} ))+I(\boldsymbol{\theta}_{0}^{*})-I_{0}(\boldsymbol{\theta})+f_{0}(\boldsymbol {\theta}_{0}^{*};\boldsymbol{\phi}(s,a))-f_{0}(\boldsymbol{\theta};\boldsymbol {\phi}(s,a))\] \[\qquad\qquad\qquad\qquad+\max_{a_{1}}f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s^{\prime},a_{1}))-\max_{a_{2}}f_{0}(\boldsymbol{\theta}_{0 }^{*};\boldsymbol{\phi}(s^{\prime},a_{2}))\Big{]}\Big{]}\] \[=\hat{\boldsymbol{\theta}}^{\intercal}\mathbb{E}\Big{[}\nabla_{ \boldsymbol{\theta}}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))\Big{[} \max_{a_{1}}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s^{\prime},a_{1}))- \max_{a_{2}}f_{0}(\boldsymbol{\theta}_{0}^{*};\boldsymbol{\phi}(s^{\prime},a_{ 2}))\Big{]}\Big{]}\] \[\quad-\hat{\boldsymbol{\theta}}^{\intercal}\mathbb{E}\Big{[} \nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))[ I_{0}(\boldsymbol{\theta})-I_{0}(\boldsymbol{\theta}_{0}^{*})]\Big{]}-\hat{ \boldsymbol{\theta}}^{\intercal}\mathbb{E}\Big{[}\nabla_{\boldsymbol{ \theta}}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))[f_{0}(\boldsymbol {\theta};\boldsymbol{\phi}(s,a))-f_{0}(\boldsymbol{\theta}_{0}^{*};\boldsymbol {\phi}(s,a))]\Big{]}\] \[\qquad\qquad\qquad\qquad+\hat{\boldsymbol{\theta}}^{\intercal} \mathbb{E}\Big{[}\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s,a))[(1-a)(\lambda-y_{0}(\boldsymbol{\theta}_{0}^{*}))] \Big{]}\] \[\overset{(b_{2})}{\leq}\hat{\boldsymbol{\theta}}^{\intercal} \mathbb{E}\Big{[}\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s,a))\max_{a^{\prime}}\Big{[}f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s^{\prime},a^{\prime}))-f_{0}(\boldsymbol{\theta}_{0}^{*}; \boldsymbol{\phi}(s^{\prime},a^{\prime}))\Big{]}\Big{]}\] \[\overset{(b_{4})}{=}\|\hat{\boldsymbol{\theta}}\|^{2}\mathbb{E} \Big{[}\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))^{\intercal}\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s,a))\Big{]}\] \[\qquad\qquad\qquad\qquad-\|\hat{\boldsymbol{\theta}}\|^{2} \mathbb{E}\Big{[}\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s,a))^{\intercal}\Big{[}\frac{1}{2S}\sum_{\hat{s}\in \mathcal{S}}\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta};\boldsymbol{ \phi}(\tilde{s},0))+f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(\tilde{s},1)) \Big{]}\Big{]}\] \[\qquad\qquad\qquad\qquad-\|\hat{\boldsymbol{\theta}}\|^{2} \mathbb{E}\Big{[}\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s,a))^{\intercal}\nabla_{\boldsymbol{\theta}}f_{0}( \boldsymbol{\theta};\boldsymbol{\phi}(s,a))\Big{]}\]

where \((b_{1})\) holds since \(\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))= \nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta}_{0}^{*};\boldsymbol{\phi} (s,a))\) as in (26), \((b_{2})\) is due to the fact that \(\max_{a_{1}}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s^{\prime},a_{1}))- \max_{a_{1}}f_{0}(\boldsymbol{\theta}_{0}^{*};\boldsymbol{\phi}(s^{\prime},a_{2} ))\leq\max_{a^{\prime}}\Big{[}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s^{ \prime},a^{\prime}))-f_{0}(\boldsymbol{\theta}_{0}^{*};\boldsymbol{\phi}(s^{ \prime},a^{\prime}))\Big{]}\), and \((b_{3})\) holds due to the fact that \(\hat{\boldsymbol{\theta}}^{\intercal}\mathbb{E}\Big{[}\nabla_{\boldsymbol{\theta}}f_{ 0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))[(1-a)(\lambda-y_{0}(\boldsymbol{ \theta}_{0}^{*}))]\Big{]}\leq 0\) since a larger Whittle index \(\lambda\) will choose the action \(a=1\). Notice that the \(\tilde{a}\) in \((b_{4})\) represents the action \(a^{\prime}\) which maximizes \(f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s^{\prime},a^{\prime}))-f_{0}( \boldsymbol{\theta}_{0}^{*};\boldsymbol{\phi}(s^{\prime},a^{\prime}))\). Due to the definition of \(\nabla_{\boldsymbol{\theta}}f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,a))\) in (26), we show that \(\mathbb{E}[\hat{\boldsymbol{\theta}}^{\intercal}h_{0}(X,\boldsymbol{\theta}, \lambda)]\leq 0\).

\(2)\) Next, we show that there exists a constant \(\mu_{2}>0\) such that \(\mathbb{E}[\hat{\lambda}g_{0}(X,\boldsymbol{\theta},\lambda)]\leq-\mu_{2}\|\hat{ \lambda}\|^{2}\). According to the definition of \(g_{0}(\boldsymbol{\theta})\), i.e., \(g_{0}(\boldsymbol{\theta}):=f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,1))-f_{0 }(\boldsymbol{\theta};\boldsymbol{\phi}(s,0))\). Since \(y_{0}(\boldsymbol{\theta})\) is the solution of \(\lambda\) such that \(f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,1))=f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s,0))\), the signs of \(\hat{\lambda}:=\lambda-y_{0}(\theta)\) and \(f_{0}(\boldsymbol{\theta};\boldsymbol{\phi}(s,1))-f_{0}(\boldsymbol{\theta}; \boldsymbol{\phi}(s,0))\) are always opposite. Hence, we have \(\mathbb{E}[\hat{\lambda}g_{0}(X,\boldsymbol{\theta},\lambda)]\leq 0\), which completes the proof.

### Proof of Lemma 5

Proof.: Under Lemma 1, we have

\[\|h_{0}(X,\bm{\theta},\lambda)-h_{0}(X,\bm{\theta}^{*},\lambda^{*})\|\leq 3\|\bm{ \theta}-\bm{\theta}^{*}\|+\|\bm{\lambda}-\bm{\lambda}^{*}\|.\] (29)

Let \(L=\max(3,\max_{X}h_{0}(X,\bm{\theta}^{*},\lambda^{*}))\), then according to (29), we have

\[\|h_{0}(X,\bm{\theta},\lambda)\|\leq L(\|\bm{\theta}-\bm{\theta}^{*}\|+\|\bm{ \lambda}-\bm{\lambda}^{*}\|+1).\]

Denote \(h_{0}^{i}(X,\bm{\theta},\lambda)\) as the \(i\)-th element of \(h_{0}(X,\bm{\theta},\lambda)\). Following [15], we can show that \(\bm{\theta}\in\mathbb{R}^{md}\), \(\lambda\in\mathbb{R}^{1}\), and \(x\in\mathcal{X}\),

\[\|\mathbb{E}[h_{0}(X_{k},\bm{\theta},\lambda)|X_{0}=x]-\mathbb{E} _{\mu}[h_{0}(X,\bm{\theta},\lambda)]\|\] \[\leq\sum_{i=1}^{md}|\mathbb{E}[h_{i}(X_{k},\bm{\theta},\lambda)|X _{0}=x]-\mathbb{E}_{\mu}[h_{0}^{i}(X,\bm{\theta},\lambda)]|\] \[\leq 2L(\|\bm{\theta}-\bm{\theta}^{*}\|+\|\lambda-\lambda^{*}\|+1 )\sum_{i=1}^{md}\left|\mathbb{E}\left[\frac{h_{0}^{i}(X_{k},\bm{\theta}, \lambda)}{2L(\|\bm{\theta}-\bm{\theta}^{*}\|+\|\lambda-\lambda^{*}\|+1)}\Big{|} X_{0}=x\right]\right.\] \[\left.\qquad\qquad\qquad-\mathbb{E}_{\mu}\left[\frac{h_{0}^{i}(X,\bm{\theta},\lambda)}{2L(\|\bm{\theta}-\bm{\theta}^{*}\|+\|\lambda-\lambda^{* }\|+1)}\right]\right|\] \[\leq 2L(\|\bm{\theta}-\bm{\theta}^{*}\|+\|\lambda-\lambda^{*}\|+1) mdC\rho^{k},\]

where the last inequality holds due to Assumption 1. To guarantee \(2L(\|\bm{\theta}-\bm{\theta}^{*}\|+\|\lambda-\lambda^{*}\|+1)mdC\rho^{k}\leq \delta(\|\bm{\theta}-\bm{\theta}^{*}\|+\|\lambda-\lambda^{*}\|+1)\), we have

\[\tau_{\delta}\leq\frac{\log(1/\delta)+\log(2Lcmd)}{\log(1/\rho)},\]

which completes the proof.

### Proof of Lemma 6

Proof.: Based on the definition of \(M(\bm{\theta}_{k},\lambda_{k})\) in (15), we have

\[M(\bm{\theta}_{k},\lambda_{k}) :=\frac{\eta_{k}}{\alpha_{k}}\|\bm{\theta}_{k}-\bm{\theta}^{*}\|^ {2}+\|\lambda_{k}-y(\bm{\theta}_{k})\|^{2}\] \[=\frac{\eta_{k}}{\alpha_{k}}\|\bm{\theta}_{k}-\bm{\theta}_{0}^{*} +\bm{\theta}_{0}^{*}-\bm{\theta}^{*}\|^{2}+\|\lambda_{k}-y_{0}(\bm{\theta}_{k })+y_{0}(\bm{\theta}_{k})-y(\bm{\theta}_{k})\|^{2}\] \[\leq\frac{2\eta_{k}}{\alpha_{k}}(\|\bm{\theta}_{k}-\bm{\theta}_{0 }^{*}\|^{2}+\|\bm{\theta}_{0}^{*}-\bm{\theta}^{*}\|^{2})+2(\|\lambda_{k}-y_{0} (\bm{\theta}_{k})\|^{2}+\|y_{0}(\bm{\theta}_{k})-y(\bm{\theta}_{k})\|^{2})\] \[=2\hat{M}(\bm{\theta}_{k},\lambda_{k})+\frac{2\eta_{k}}{\alpha_{k }}\|\bm{\theta}_{0}^{*}-\bm{\theta}^{*}\|^{2}+2\|y_{0}(\bm{\theta}_{k})-y( \bm{\theta}_{k})\|^{2}\] \[\leq 2\hat{M}(\bm{\theta}_{k},\lambda_{k})+\frac{2\eta_{k}c_{0}^{2 }}{\alpha_{k}}\|span(f_{0}(\bm{\theta}_{0}^{*})-f(\bm{\theta}^{*}))\|^{2}+2\|y_ {0}(\bm{\theta}_{k})-y(\bm{\theta}_{k})\|^{2},\] (30)

where the first inequality holds based on \(\|\bm{x}+\bm{y}\|^{2}\leq 2\|\bm{x}\|^{2}+2\|\bm{y}\|^{2}\), and the second inequality holds based on Assumption 3. Next, we bound \(\|span(f_{0}(\bm{\theta}_{0}^{*})-f(\bm{\theta}^{*}))\|\) as follows

\[\|span(f_{0}(\bm{\theta}_{0}^{*})-f(\bm{\theta}^{*}))\| =\|span(f_{0}(\bm{\theta}_{0}^{*})-\Pi_{F}f(\bm{\theta}^{*})+\Pi _{F}f(\bm{\theta}^{*})-f(\bm{\theta}^{*}))\|\] \[\leq\|span(f_{0}(\bm{\theta}_{0}^{*})-\Pi_{F}f(\bm{\theta}^{*})) \|+\|span(\Pi_{F}f(\bm{\theta}^{*})-f(\bm{\theta}^{*}))\|\] \[=\|span(\Pi_{F}\mathcal{T}f_{0}(\bm{\theta}_{0}^{*})-\Pi_{F} \mathcal{T}f(\bm{\theta}^{*}))\|+\|span(\Pi_{F}f(\bm{\theta}^{*})-f(\bm{ \theta}^{*}))\|\] \[\leq\kappa\|span(f_{0}(\bm{\theta}_{0}^{*})-f(\bm{\theta}^{*}))\| +\|span(\Pi_{F}f(\bm{\theta}^{*})-f(\bm{\theta}^{*}))\|,\] (31)

where the last inequality follows (24). This indicates that

\[\|span(f_{0}(\bm{\theta}_{0}^{*})-f(\bm{\theta}^{*}))\|^{2}\leq\frac{1}{(1- \kappa)^{2}}\|span(\Pi_{F}f(\bm{\theta}^{*})-f(\bm{\theta}^{*}))\|^{2}.\] (32)We further bound \(\|y_{0}(\bm{\theta}_{k})-y(\bm{\theta}_{k})\|^{2}\) as follows

\[\|y_{0}(\bm{\theta}_{k})-y(\bm{\theta}_{k})\|^{2} =\Big{\|}\sum_{s^{\prime}}p(s^{\prime}|s,\!1)\max_{a_{1}}f_{0}(\bm {\theta}_{k};\bm{\phi}(s^{\prime},a_{1}))-\sum_{s^{\prime}}p(s^{\prime}|s,\!0) \max_{a_{2}}f_{0}(\bm{\theta}_{k};\bm{\phi}(s^{\prime},a_{2}))\] \[\quad-\sum_{s^{\prime}}p(s^{\prime}|s,\!1)\max_{a_{3}}f(\bm{ \theta}_{k};\bm{\phi}(s^{\prime},a_{3}))+\sum_{s^{\prime}}p(s^{\prime}|s,\!0) \max_{a_{4}}f(\bm{\theta}_{k};\bm{\phi}(s^{\prime},a_{4}))\Big{\|}^{2}\] \[=\Big{\|}\sum_{s^{\prime}}p(s^{\prime}|s,\!1)(\max_{a_{1}}f_{0}( \bm{\theta}_{k};\bm{\phi}(s^{\prime},a_{1}))-\max_{a_{3}}f(\bm{\theta}_{k}; \bm{\phi}(s^{\prime},a_{3})))\] \[\qquad-\sum_{s^{\prime}}p(s^{\prime}|s,\!0)(\max_{a_{2}}f_{0}( \bm{\theta}_{k};\bm{\phi}(s^{\prime},a_{2}))-\max_{a_{4}}f(\bm{\theta}_{k}; \bm{\phi}(s^{\prime},a_{4}))\Big{\|}^{2}\] \[\leq 2\|\max_{(s,a)}f_{0}(\bm{\theta}_{k};\bm{\phi}(s,a))-f(\bm{ \theta}_{k};\bm{\phi}(s,a))\|^{2}\] \[\leq 2\mathcal{O}\Big{(}\frac{c_{1}^{3}(\|\bm{\theta}_{0}\|+| \lambda_{0}|+1)^{3}}{m^{1/2}}\Big{)},\] (33)

where the last inequality is due to Lemma 10. Substituting (32) and (33) back to (30) yields the final results.

## Appendix D Proof of the Theorem 2

To prove Theorem 2, we need the following three key lemmas about the error terms defined in (19).

**Lemma 7**.: _Let \(\{\bm{\theta}_{k},\lambda_{k}\}\) be generated by (9). Then under Lemmas 1-4, for any \(k\geq\tau\), we have_

\[\mathbb{E}\left[\left\|\hat{\bm{\theta}}_{k+1}\right\|^{2}|\mathcal{ F}_{k-\tau}\right] \leq(1+150\alpha_{k}^{2}+\eta_{k}/\alpha_{k}-2\alpha_{k}\mu_{1}) \mathbb{E}\Big{[}\left\|\hat{\bm{\theta}}_{k}\right\|^{2}|\mathcal{F}_{k-\tau }\Big{]}+6\alpha_{k}^{3}\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k}\Big{\|}^{2 }|\mathcal{F}_{k-\tau}\Big{]}\] \[+\frac{\alpha_{k}^{3}}{\eta_{k}}\mathcal{O}\Big{(}c_{1}^{3}(\| \bm{\theta}_{0}\|+|\lambda_{0}|+1)^{3}\cdot m^{-1/2}\Big{)}.\] (34)

Proof.: According to (19), we have \(\hat{\bm{\theta}}_{k+1}:=\bm{\theta}_{k+1}-\bm{\theta}_{0}^{*}=\hat{\bm{ \theta}}_{k}+\alpha_{k}h(X_{k},\bm{\theta}_{k},\lambda_{k}),\) which leads to

\[\left\|\hat{\bm{\theta}}_{k+1}\right\|^{2} =\left\|\hat{\bm{\theta}}_{k}\right\|^{2}+2\alpha_{k}\hat{\bm{ \theta}}_{k}^{\intercal}h(X_{k},\bm{\theta}_{k},\lambda_{k})+\left\|\alpha_{k }h(X_{k},\bm{\theta}_{k},\lambda_{k})\right\|^{2}\] \[=\left\|\hat{\bm{\theta}}_{k}\right\|^{2}+2\alpha_{k}\hat{\bm{ \theta}}_{k}^{\intercal}(h(X_{k},\bm{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\bm{ \theta}_{k},\lambda_{k}))+2\alpha_{k}\hat{\bm{\theta}}_{k}^{\intercal}h_{0}(X_ {k},\bm{\theta}_{k},\lambda_{k})\] \[\qquad+\alpha_{k}^{2}\|h(X_{k},\bm{\theta}_{k},\lambda_{k})-h_{0 }(X_{k},\bm{\theta}_{k},\lambda_{k})+h_{0}(X_{k},\bm{\theta}_{k},\lambda_{k}) \|^{2}\] \[\leq\left\|\hat{\bm{\theta}}_{k}\right\|^{2}+2\alpha_{k}\hat{\bm{ \theta}}_{k}^{\intercal}(h(X_{k},\bm{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\bm{ \theta}_{k},\lambda_{k}))+2\alpha_{k}\hat{\bm{\theta}}_{k}^{\intercal}h_{0}(X_ {k},\bm{\theta}_{k},\lambda_{k})\] \[\qquad+2\alpha_{k}^{2}\|h(X_{k},\bm{\theta}_{k},\lambda_{k})-h_{0 }(X_{k},\bm{\theta}_{k},\lambda_{k})\|^{2}+2\alpha_{k}^{2}\|h_{0}(X_{k},\bm{ \theta}_{k},\lambda_{k})\|^{2}.\] (35)

The above inequality holds due to the fact that \(\|\bm{x}+\bm{y}\|^{2}\leq 2\|\bm{x}\|^{2}+2\|\bm{y}\|^{2}\). Taking expectations of \(\|\hat{\bm{\theta}}_{k+1}\|^{2}\) w.r.t \(\mathcal{F}_{k-\tau}\) yields

\[\mathbb{E}\Big{[}\|\hat{\bm{\theta}}_{k+1}\|^{2}|\mathcal{F}_{k- \tau}\Big{]}{\leq}\mathbb{E}\Big{[}\Big{\|}\hat{\bm{\theta}}_{k}\Big{\|}^{2} |\mathcal{F}_{k-\tau}\Big{]}+2\alpha_{k}\mathbb{E}\Big{[}\hat{\bm{\theta}}_{k}^ {\intercal}h_{0}(X_{k},\bm{\theta}_{k},\lambda_{k})|\mathcal{F}_{k-\tau}\Big{]}\] \[\qquad+\underbrace{2\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h_{0}(X_ {k},\bm{\theta}_{k},\lambda_{k})\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}}_{ \text{Term}_{1}}\] \[\qquad+\underbrace{2\alpha_{k}\mathbb{E}\Big{[}\hat{\bm{\theta}}_{k}^ {\intercal}(h(X_{k},\bm{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\bm{\theta}_{k}, \lambda_{k}))|\mathcal{F}_{k-\tau}\Big{]}}_{\text{Term}_{2}}\] \[\qquad+\underbrace{2\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h(X_ {k},\bm{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\bm{\theta}_{k},\lambda_{k}) \Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}}_{\text{Term}_{3}}\]\[\leq \mathbb{E}\Big{[}\Big{\|}\hat{\bm{\theta}}_{k}\Big{\|}^{2}|\mathcal{F} _{k-\tau}\Big{]}-2\alpha_{k}\mu_{1}\mathbb{E}\Big{[}\Big{\|}\tilde{\bm{\theta}} _{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}+\text{Term}_{1}+\text{Term}_{2}+ \text{Term}_{3},\] (36)

where the last inequality is due to Lemma 4. Next, we bound each individual term. Term\({}_{1}\) is bounded as

\[\text{Term}_{1} =2\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h_{0}(X_{k},\bm{\theta} _{k},\lambda_{k})\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}\] \[\overset{(\ref{eq:1})}{=}2\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|} h_{0}(X_{k},\bm{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\bm{\theta}_{k},y_{0}(\bm{ \theta}_{k}))+h_{0}(X_{k},\bm{\theta}_{k},y_{0}(\bm{\theta}_{k}))\] \[\qquad\qquad-h_{0}(X_{k},\bm{\theta}_{0}^{*},y_{0}(\bm{\theta}_{ 0}^{*}))+h_{0}(X_{k},\bm{\theta}_{0}^{*},y_{0}(\bm{\theta}_{0}^{*}))-H_{0}( \bm{\theta}_{0}^{*},y_{0}(\bm{\theta}_{0}^{*}))\Big{\|}^{2}|\mathcal{F}_{k- \tau}\Big{]}\] \[\overset{(\ref{eq:1})}{\leq}6\alpha_{k}^{2}\mathbb{E}\Big{[} \Big{\|}h_{0}(X_{k},\bm{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\bm{\theta}_{k},y_ {0}(\bm{\theta}_{k}))\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}\] \[\qquad\qquad+6\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h_{0}(X_{k}, \bm{\theta}_{k},y_{0}(\bm{\theta}_{k}))-h_{0}(X_{k},\bm{\theta}_{0}^{*},y_{0}( \bm{\theta}_{0}^{*}))\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}\] \[\qquad\qquad+6\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h_{0}(X_{k}, \bm{\theta}_{0}^{*},y_{0}(\bm{\theta}_{0}^{*}))-H_{0}(\bm{\theta}_{0}^{*},y_{ 0}(\bm{\theta}_{0}^{*}))\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}\] \[\overset{(\ref{eq:1})}{\leq}6\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|} \hat{\lambda}_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}+150\alpha_{k}^{2} \mathbb{E}\Big{[}\Big{\|}\hat{\bm{\theta}}_{k}\Big{\|}^{2}|\mathcal{F}_{k- \tau}\Big{]},\] (37)

where \((\ref{eq:1})\) holds due to \(H_{0}(\bm{\theta}_{0},y_{0}(\bm{\theta}_{0}^{*}))=0\), \((\ref{eq:1})\) follows from the triangular inequality, and \((\ref{eq:1})\) follows from the Lipschitz continuity of \(h_{0}(X,\bm{\theta},\lambda)\) in Lemma 1.

\(\text{Term}_{2}\) is bounded as

\[\text{Term}_{2} =2\alpha_{k}\mathbb{E}\Big{[}\hat{\bm{\theta}}_{k}^{\intercal}(h( X_{k},\bm{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\bm{\theta}_{k},\lambda_{k}))| \mathcal{F}_{k-\tau}\Big{]}\] \[\overset{(\ref{eq:1})}{\leq}\frac{\eta_{k}}{\alpha_{k}}\mathbb{E} \Big{[}\Big{\|}\hat{\bm{\theta}}_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}+ \frac{\alpha_{k}^{2}}{\eta_{k}}\mathcal{O}\Big{(}c_{1}^{3}(\|\bm{\theta}_{0}\| +|\lambda_{0}|+1)^{3}\cdot m^{-1/2}\Big{)},\] (38)

where \((\ref{eq:1})\) holds due to the fact that \(2x^{\intercal}\bm{y}\leq\|\bm{x}\|^{2}+\|\bm{y}\|^{2}\) and \((\ref{eq:1})\) is due to Lemma 10.

\(\text{Term}_{3}\) is bounded as

\[\text{Term}_{3} =2\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h(X_{k},\bm{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\bm{\theta}_{k},\lambda_{k})\Big{\|}^{2}|\mathcal{F}_{ k-\tau}\Big{]}\] \[\overset{(\ref{eq:1})}{\leq}2\alpha_{k}^{2}\mathcal{O}\Big{(}c_ {1}^{3}(\|\bm{\theta}_{0}\|+|\lambda_{0}|+1)^{3}\cdot m^{-1/2}\Big{)},\] (39)

where \((\ref{eq:1})\) comes from Lemma 10. Substituting Term\({}_{1}\), Term\({}_{2}\), and Term\({}_{3}\) back into (36) leads to the desired result in (34), which is

\[\mathbb{E}\Big{[}\|\hat{\bm{\theta}}_{k+1}\|^{2}|\mathcal{F}_{k- \tau}\Big{]}\leq \mathbb{E}\Big{[}\Big{\|}\hat{\bm{\theta}}_{k}\Big{\|}^{2}| \mathcal{F}_{k-\tau}\Big{]}-2\alpha_{k}\mu_{1}\mathbb{E}\Big{[}\Big{\|}\hat{\bm{ \theta}}_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}\] \[+6\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k}\Big{\|} ^{2}|\mathcal{F}_{k-\tau}\Big{]}+150\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}\hat{ \bm{\theta}}_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}\] \[+\frac{\eta_{k}}{\alpha_{k}}\mathbb{E}\Big{[}\Big{\|}\hat{\bm{ \theta}}_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}+\frac{\alpha_{k}^{3}}{\eta_ {k}}\mathcal{O}\Big{(}c_{1}^{3}(\|\bm{\theta}_{0}\|+|\lambda_{0}|+1)^{3}\cdot m ^{-1/2}\Big{)}\] \[+2\alpha_{k}^{2}\mathcal{O}\Big{(}c_{1}^{3}(\|\bm{\theta}_{0}\|+| \lambda_{0}|+1)^{3}\cdot m^{-1/2}\Big{)}\] \[=(1+150\alpha_{k}^{2}+\eta_{k}/\alpha_{k}-2\alpha_{k}\mu_{1}) \mathbb{E}\Big{[}\Big{\|}\hat{\bm{\theta}}_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau} \Big{]}+6\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k}\Big{\|}^{2}| \mathcal{F}_{k-\tau}\Big{]}\] \[\qquad+(\alpha_{k}^{3}/\eta_{k}+2\alpha_{k}^{2})\mathcal{O}\Big{(} c_{1}^{3}(\|\bm{\theta}_{0}\|+|\lambda_{0}|+1)^{3}\cdot m^{-1/2}\Big{)}.\]

By neglecting higher order infinitesimal, we have the inequality in (34). This completes the proof.

**Lemma 8**.: _Let \(\{\bm{\theta}_{k},\lambda_{k}\}\) be generated by (9). Then under Lemmas 1-4, for any \(k\geq\tau\), we have_

\[\mathbb{E}\left[\left\|\hat{\lambda}_{k+1}\right\|^{2}\middle| \mathcal{F}_{k-\tau}\right] \leq(1-2\eta_{k}\mu_{2}+\alpha_{k}\eta_{k}+24\alpha_{k}^{2}+ \frac{\eta_{k}}{\alpha_{k}}-\frac{2\eta_{k}^{2}\mu_{k}}{\alpha_{k}}+\eta_{k}^{2 }+\frac{24\alpha_{k}^{3}}{\eta_{k}})\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k }\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}\] \[+(600\alpha_{k}^{2}+8\eta_{k}^{2}+\frac{8\eta_{k}^{3}}{\alpha_{k} }+\frac{600\alpha_{k}^{3}}{\eta_{k}})\mathbb{E}\Big{[}\Big{\|}\hat{\bm{\theta }}_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}\] \[+\frac{\eta_{k}}{\alpha_{k}}\mathcal{O}\Big{(}c_{1}^{3}(\|\bm{ \theta}_{0}\|+|\lambda_{0}|+1)^{3}\cdot m^{-1/2}\Big{)}.\] (40)

Proof.: According to the definition in (14), we have

\[\hat{\lambda}_{k+1} =\lambda_{k+1}-y_{0}(\bm{\theta}_{k+1})\] \[=\hat{\lambda}_{k}+\eta_{k}g(\bm{\theta}_{k})+y_{0}(\bm{\theta}_ {k})-y_{0}(\bm{\theta}_{k+1}),\]

which leads to

\[\Big{\|}\hat{\lambda}_{k+1}\Big{\|}^{2} =\Big{\|}\hat{\lambda}_{k}+\eta_{k}g(\bm{\theta}_{k})+y_{0}(\bm{ \theta}_{k})-y_{0}(\bm{\theta}_{k+1})\Big{\|}^{2}\] \[=\underbrace{\Big{\|}\hat{\lambda}_{k}+\eta_{k}g(\bm{\theta}_{k}) \Big{\|}^{2}}_{\text{Term}_{1}}+\underbrace{\Big{\|}y_{0}(\bm{\theta}_{k})-y_ {0}(\bm{\theta}_{k+1})\Big{\|}^{2}}_{\text{Term}_{2}}\] \[\qquad+\underbrace{2\left(\hat{\lambda}_{k}+\eta_{k}g(\bm{\theta }_{k})\right)\Big{(}y_{0}(\bm{\theta}_{k})-y_{0}(\bm{\theta}_{k+1})\Big{)}}_{ \text{Term}_{3}}.\] (41)

The second equality is due to the fact that \(\|\bm{x}+\bm{y}\|^{2}=\|\bm{x}\|^{2}+\|\bm{y}\|^{2}+2\bm{x}^{\intercal}\bm{y}\). We next analyze the conditional expectation of each term in \(\Big{\|}\hat{\lambda}_{k+1}\Big{\|}^{2}\) on \(\mathcal{F}_{k-\tau}\). We first focus on \(\text{Term}_{1}\).

\[\mathbb{E}\Big{[}\text{Term}_{1}|\mathcal{F}_{k-\tau}\Big{]}\] \[=\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k}\Big{\|}^{2}+2\eta_{k} \hat{\lambda}_{k}g(\bm{\theta}_{k})+2\eta_{k}\hat{\lambda}_{k}(g(\bm{\theta}_ {k})-g_{0}(\bm{\theta}_{k}))+\eta_{k}^{2}\Big{\|}g(\bm{\theta}_{k})-g_{0}( \bm{\theta}_{k})+g_{0}(\bm{\theta}_{k})\Big{\|}^{2}|\mathcal{F}_{k-\tau} \Big{]}\] \[\leq\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k}\Big{\|}^{2}| \mathcal{F}_{k-\tau}\Big{]}+2\eta_{k}\mathbb{E}\Big{[}\hat{\lambda}_{k}g_{0}( \bm{\theta}_{k})|\mathcal{F}_{k-\tau}\Big{]}+2\eta_{k}\mathbb{E}\Big{[}\hat{ \lambda}_{k}(g(\bm{\theta}_{k})-g_{0}(\bm{\theta}_{k}))|\mathcal{F}_{k-\tau} \Big{]}\] \[\qquad\qquad+2\eta_{k}^{2}\mathbb{E}\Big{[}\Big{\|}g(\bm{\theta}_ {k})-g_{0}(\bm{\theta}_{k})\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}+2\eta_{k} ^{2}\mathbb{E}\Big{[}\Big{\|}g_{0}(\bm{\theta}_{k})\Big{\|}^{2}|\mathcal{F}_{ k-\tau}\Big{]}\] \[\overset{(d_{1})}{=}\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k} \Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}+2\eta_{k}\mathbb{E}\Big{[}\hat{ \lambda}_{k}g_{0}(\bm{\theta}_{k})|\mathcal{F}_{k-\tau}\Big{]}+2\eta_{k} \mathbb{E}\Big{[}\hat{\lambda}_{k}(g(\bm{\theta}_{k})-g_{0}(\bm{\theta}_{k}))| \mathcal{F}_{k-\tau}\Big{]}\] \[\overset{(d_{2})}{=}\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k} \Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}-2\eta_{k}\mu_{2}\mathbb{E}\Big{[} \Big{\|}\hat{\lambda}_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}+8\eta_{k}^{2 }\mathbb{E}\Big{[}\Big{\|}\hat{\bm{\theta}}_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau} \Big{]}\] \[\overset{(d_{3})}{\leq}\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k} \Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}-2\eta_{k}\mu_{2}\mathbb{E}\Big{[} \Big{\|}\hat{\lambda}_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}+8\eta_{k}^{2 }\mathbb{E}\Big{[}\Big{\|}\hat{\bm{\theta}}_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau} \Big{]}\] \[\qquad\qquad+\alpha_{k}\eta_{k}\mathbb{E}\Big{[}\Big{\|}\hat{\lambda }_{k}\Big{\|}^{2}|\mathcal{F}_{k-\tau}\Big{]}+(4\eta_{k}/\alpha_{k}+8\eta_{k}^{2 })\mathcal{O}\Big{(}c_{1}^{3}(\|\bm{\theta}_{0}\|+|\lambda_{0}|+1)^{3}\cdot m^{ -1/2}\Big{)},\]

where \((d_{1})\) follows from \(g_{0}(\bm{\theta}_{0}^{*})=0\), \((d_{2})\) holds due to Lemma 4 and the Lipschitz continuity of \(y_{0}\) in Lemma 3, and \((d_{3})\) comes from Lemma 10. For \(\text{Term}_{2}\), we have

\[\mathbb{E}\Big{[}\text{Term}_{2}|\mathcal{F}_{k-\tau}\Big{]}=\mathbb{E}\left[ \Big{\|}y_{0}(\bm{\theta}_{k})-y_{0}(\bm{\theta}_{k+1})\Big{\|}^{2}|\mathcal{F}_{ k-\tau}\right]\]\[=4\mathbb{E}\left[\left\|\boldsymbol{\theta}_{k}-\boldsymbol{\theta}_{k+1 }\right\|^{2}\lvert\mathcal{F}_{k-\tau}\right]\] \[=4\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h(X_{k},\boldsymbol{ \theta}_{k},\lambda_{k})\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}\] \[=4\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h(X_{k},\boldsymbol{ \theta}_{k},\lambda_{k})-h_{0}(X_{k},\boldsymbol{\theta}_{k},\lambda_{k})+h_{ 0}(X_{k},\boldsymbol{\theta}_{k},\lambda_{k})\Big{\|}^{2}\lvert\mathcal{F}_{k- \tau}\Big{]}\] \[=8\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h_{0}(X_{k},\boldsymbol{ \theta}_{k},\lambda_{k})\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}+8\alpha _{k}^{2}\mathbb{E}\Big{[}\Big{\|}h(X_{k},\boldsymbol{\theta}_{k},\lambda_{k}) -h_{0}(X_{k},\boldsymbol{\theta}_{k},\lambda_{k})\Big{\|}^{2}\lvert\mathcal{F }_{k-\tau}\Big{]}\] \[\overset{(d_{4})}{=}8\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h_{0} (X_{k},\boldsymbol{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\boldsymbol{\theta}_{ k},y_{0}(\boldsymbol{\theta}_{k}))+h_{0}(X_{k},\boldsymbol{\theta}_{k},y_{0}( \boldsymbol{\theta}_{k}))\] \[\qquad\qquad-h_{0}(X_{k},\boldsymbol{\theta}_{0}^{*},y_{0}( \boldsymbol{\theta}_{0}^{*}))+h_{0}(X_{k},\boldsymbol{\theta}_{0}^{*},y_{0}( \boldsymbol{\theta}_{0}^{*}))-H_{0}(\boldsymbol{\theta}_{0}^{*},y_{0}( \boldsymbol{\theta}_{0}^{*}))\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}\] \[\qquad\qquad+8\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h(X_{k}, \boldsymbol{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\boldsymbol{\theta}_{k}, \lambda_{k})\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}\] \[\overset{(d_{5})}{\leq}24\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|} h_{0}(X_{k},\boldsymbol{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\boldsymbol{ \theta}_{k},y_{0}(\boldsymbol{\theta}_{k}))\Big{\|}^{2}\lvert\mathcal{F}_{k- \tau}\Big{]}\] \[\qquad\qquad+24\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h_{0}(X_{k}, \boldsymbol{\theta}_{k},y_{0}(\boldsymbol{\theta}_{k}))-h_{0}(X_{k},\boldsymbol {\theta}_{0}^{*},y_{0}(\boldsymbol{\theta}_{0}^{*}))\Big{\|}^{2}\lvert \mathcal{F}_{k-\tau}\Big{]}\] \[\qquad\qquad+8\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h(X_{k}, \boldsymbol{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\boldsymbol{\theta}_{k}, \lambda_{k})\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}\] \[\overset{(d_{6})}{\leq}24\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|} \hat{\lambda}_{k}\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}+600\alpha_{k}^{ 2}\mathbb{E}\Big{[}\Big{\|}\hat{\boldsymbol{\theta}}_{k}\Big{\|}^{2}\lvert \mathcal{F}_{k-\tau}\Big{]}\] \[\qquad\qquad+8\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|}h(X_{k}, \boldsymbol{\theta}_{k},\lambda_{k})-h_{0}(X_{k},\boldsymbol{\theta}_{k}, \lambda_{k})\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}\] \[\overset{(d_{7})}{\leq}24\alpha_{k}^{2}\mathbb{E}\Big{[}\Big{\|} \hat{\lambda}_{k}\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}+600\alpha_{k}^{ 2}\mathbb{E}\Big{[}\Big{\|}\hat{\boldsymbol{\theta}}_{k}\Big{\|}^{2}\lvert \mathcal{F}_{k-\tau}\Big{]}\] \[\qquad\qquad+8\alpha_{k}^{2}\mathcal{O}\Big{(}c_{1}^{3}(\| \boldsymbol{\theta}_{0}\|+\lvert\lambda_{0}\rvert+1)^{3}\cdot m^{-1/2}\Big{)}\] (42)

where \((d_{4})\) is due to the fact that \(H_{0}(\boldsymbol{\theta}_{0}^{*},y_{0}(\boldsymbol{\theta}_{0}^{*}))=0\), \((d_{5})\) holds according to \(\|\boldsymbol{x}+\boldsymbol{y}+\boldsymbol{z}\|^{2}\leq 3\|\boldsymbol{x}\|^{2}+3\| \boldsymbol{y}\|^{2}+3\|\boldsymbol{z}\|^{2}\) since \(g(X_{k},f(\boldsymbol{\lambda}^{*}),\boldsymbol{\lambda}^{*})=\boldsymbol{0}\), \((d_{6})\) holds because of the Lipschitz continuity of \(h_{0}\) and \(y_{0}\) in Lemma 1 and Lemma 3, and \((d_{7})\) comes from Lemma 10. Next, we have the conditional expectation of \(\text{Term}_{3}\) as

\[\mathbb{E}\Big{[}\text{Term}_{3}\lvert\mathcal{F}_{k-\tau}\Big{]} =2\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k}+\eta_{k}g( \boldsymbol{\theta}_{k})\Big{\|}\cdot\Big{\|}y_{0}(\boldsymbol{\theta}_{k})-y_{ 0}(\boldsymbol{\theta}_{k+1})\Big{\|}\lvert\mathcal{F}_{k-\tau}\Big{]}\] \[\overset{(d_{8})}{\leq}\frac{\eta_{k}}{\alpha_{k}}\text{Term}_{1} +\frac{\alpha_{k}}{\eta_{k}}\text{Term}_{2}\] \[=\frac{\eta_{k}}{\alpha_{k}}\mathbb{E}\Big{[}\Big{\|}\hat{ \lambda}_{k}\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}-\frac{2\eta_{k}^{2}\mu_{ 2}}{\alpha_{k}}\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k}\Big{\|}^{2}\lvert \mathcal{F}_{k-\tau}\Big{]}+\frac{8\eta_{k}^{3}}{\alpha_{k}}\mathbb{E}\Big{[} \Big{\|}\hat{\boldsymbol{\theta}}_{k}\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}\] \[\qquad\qquad+\eta_{k}^{2}\mathbb{E}\Big{[}\Big{\|}\hat{\lambda}_{k }\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}+\frac{\eta_{k}}{\alpha_{k}}(4 \eta_{k}/\alpha_{k}+8\eta_{k}^{2})\mathcal{O}\Big{(}c_{1}^{3}(\|\boldsymbol{ \theta}_{0}\|+\lvert\lambda_{0}\rvert+1)^{3}\cdot m^{-1/2}\Big{)}\] \[+\frac{24\alpha_{k}^{3}}{\eta_{k}}\mathbb{E}\Big{[}\Big{\|}\hat{ \lambda}_{k}\Big{\|}^{2}\lvert\mathcal{F}_{k-\tau}\Big{]}+\frac{600\alpha_{k}^{ 3}}{\eta_{k}}\mathbb{E}\Big{[}\Big{\|}\hat{\boldsymbol{\theta}}_{k}\Big{\|}^{2} \lvert\mathcal{F}_{k-\tau}\Big{]}\] \[\qquad\qquad+\frac{8\alpha_{k}^{3}}{\eta_{k}}\mathcal{O}\Big{(}c_ {1}^{3}(\|\boldsymbol{\theta}_{0}\|+\lvert\lambda_{0}\rvert+1)^{3}\cdot m^{-1/2} \Big{)},\]

where \((d_{8})\) holds because \(2\mathbf{x}^{T}\mathbf{y}\leq 1/\beta\|\mathbf{x}\|^{2}+\beta\|\mathbf{y}\|^{2}\), \(\forall\beta>0\). Summing \(\text{Term}_{1}\), \(\text{Term}_{2}\), and \(\text{Term}_{3}\) and neglecting higher order infinitesimal yield the desired result.

Now we are ready to prove the results in Theorem 2. Providing Lemma 8 and Lemma 7, if \(\frac{\eta_{k}}{\alpha_{k}}\) is non-increasing, we have the following inequality

\[\mathbb{E}\left[\hat{M}(\boldsymbol{\theta}_{k+1},\lambda_{k+1}) \Big{|}\mathcal{F}_{k-\tau}\right] =\mathbb{E}\left[\frac{\eta_{k}}{\alpha_{k}}\big{\|}\hat{ \boldsymbol{\theta}}_{k+1}\right\|^{2}+\left\|\hat{\lambda}_{k+1}\right\|^{2} \Big{|}\mathcal{F}_{k-\tau}\right]\] \[\leq\frac{\eta_{k}}{\alpha_{k}}(1-2\alpha_{k}\mu_{1})\mathbb{E} \Big{[}\left\|\hat{\boldsymbol{\theta}}_{k}\right\|^{2}|\mathcal{F}_{k-\tau} \Big{]}+\frac{600\alpha_{k}^{3}}{\eta_{k}}\mathbb{E}\Big{[}\left\|\hat{ \boldsymbol{\theta}}_{k}\right\|^{2}|\mathcal{F}_{k-\tau}\Big{]}\] \[\qquad\qquad+\frac{8\alpha_{k}^{3}}{\eta_{k}}\mathcal{O}\Big{(}c _{1}^{3}(\|\boldsymbol{\theta}_{0}\|+|\lambda_{0}|+1)^{3}\cdot m^{-1/2}\Big{)}\] \[\qquad\qquad+(1-2\eta_{k}\mu_{2})\mathbb{E}\Big{[}\left\|\hat{ \lambda}_{k}\right\|^{2}|\mathcal{F}_{k-\tau}\Big{]}\] \[\qquad\qquad+\frac{600\alpha_{k}^{3}}{\eta_{k}}\mathbb{E}\Big{[} \left\|\hat{\lambda}_{k}\right\|^{2}|\mathcal{F}_{k-\tau}\Big{]}.\] (43)

Since \((k+1)^{2}\cdot\frac{\alpha_{k}^{3}}{\eta}=\frac{\alpha_{0}^{3}}{\eta_{0}}(k+1) ^{1/3}\), multiplying both sides of (43) with \((k+1)^{2}\), we have

\[(k+1)^{2}\mathbb{E}\Big{[}\hat{M}(\boldsymbol{\theta}_{k+1}, \lambda_{k+1})\Big{|}\mathcal{F}_{k-\tau}\Big{]}\] \[\leq k^{2}\mathbb{E}\Big{[}\hat{M}(\boldsymbol{\theta}_{k},\lambda_{k}) \Big{|}\mathcal{F}_{k-\tau}\Big{]}+\frac{600\alpha_{0}^{3}}{\eta_{0}}(k+1)^{1/ 3}\left(\left\|\hat{\boldsymbol{\theta}}_{k}\right\|^{2}+\left\|\hat{\lambda }_{k}\right\|^{2}\right)\] \[\qquad+\frac{8\alpha_{0}^{3}}{\eta_{0}}(k+1)^{1/3}\mathcal{O} \Big{(}c_{1}^{3}(\|\boldsymbol{\theta}_{0}\|+|\lambda_{0}|+1)^{3}\cdot m^{-1/2 }\Big{)}.\] (44)

Summing (44) from time step \(\tau\) to time step \(k\), we have

\[(k+1)^{2}\mathbb{E}\Big{[}\hat{M}(\boldsymbol{\theta}_{k+1}, \lambda_{k+1})\Big{|}\mathcal{F}_{k}\Big{]} \leq\tau^{2}\mathbb{E}\Big{[}\hat{M}(\boldsymbol{\theta}_{\tau}, \lambda_{\tau})\Big{]}+\frac{600\alpha_{0}^{3}}{\eta_{0}}(k+1)^{4/3}\left( \left\|\hat{\boldsymbol{\theta}}_{\tau}\right\|^{2}+\left\|\hat{\lambda}_{ \tau}\right\|^{2}\right)\] \[+\frac{8\alpha_{0}^{3}}{\eta_{0}}(k+1)^{4/3}\mathcal{O}\Big{(}c_{ 1}^{3}(\|\boldsymbol{\theta}_{0}\|+|\lambda_{0}|+1)^{3}\cdot m^{-1/2}\Big{)}\] \[\leq\tau^{2}\mathbb{E}\Big{[}\hat{M}(\boldsymbol{\theta}_{\tau}, \lambda_{\tau})\Big{]}+\frac{600\alpha_{0}^{3}}{\eta_{0}}\frac{(C_{1}+\|\hat{ \boldsymbol{\theta}}_{0}\|)^{2}+(2C_{1}+\|\hat{\lambda}_{0}\|)^{2}}{(k+1)^{-4/3}}\] \[+\frac{8\alpha_{0}^{3}}{\eta_{0}}\frac{\mathcal{O}\Big{(}c_{1}^{ 3}(\|\boldsymbol{\theta}_{0}\|+|\lambda_{0}|+1)^{3}\cdot m^{-1/2}\Big{)}}{(k+1) ^{-4/3}},\] (45)

where the second inequality holds due to Lemma 9. Finally, dividing both sides by \((k+1)^{2}\) and moving the constant term into \(\mathcal{O}(\cdot)\) yields the results in Theorem 2.

## Appendix E Auxiliary Lemmas

In this part, we present several key lemmas which are needed for the major proofs. We first show the parameters update in (9) is bounded in the following lemma.

**Lemma 9**.: _The update of \(\boldsymbol{\theta}_{k}\) and \(\lambda_{k}\) in (9) is bounded with respect to the initial \(\boldsymbol{\theta}_{0}\) and \(\lambda_{0}\), i.e.,_

\[\|\boldsymbol{\theta}_{k}-\boldsymbol{\theta}_{0}\|+|\lambda_{k}-\lambda_{0}| \leq c_{1}(\|\boldsymbol{\theta}_{0}\|+|\lambda_{0}|+1),\]

_with \(c_{1}\) be the constant, i.e., \(c_{1}:=\frac{1}{2}+\frac{3}{2}(L_{h}^{\prime}\alpha_{\tau}+L_{g}^{\prime}\eta _{\tau})(L_{h}^{\prime}\alpha_{\tau}+L_{g}^{\prime}\eta_{\tau}+1)\)._

Proof.: Without loss of generality, we assume that

\[L_{h}^{\prime}\geq\max(3,\max_{X\in\mathcal{X}}\|h_{0}(X,0,0)\|),\;L_{g}^{ \prime}\geq\max(2,\max_{X\in\mathcal{X}}\|g_{0}(X,0,0)\|).\]

Then based on triangular inequality and Lemmas 1-2, we have

\[\|h_{0}(X,\boldsymbol{\theta},\lambda)\|\leq L_{h}^{\prime}(\|\boldsymbol{ \theta}\|+|\lambda|+1),\;\|g_{0}(X,\boldsymbol{\theta},\lambda)\|\leq L_{g}^{ \prime}(\|\boldsymbol{\theta}\|+|\lambda|+1),\forall\boldsymbol{\theta}, \lambda,X\in\mathcal{X}.\] (46)Since we have \(\bm{\theta}_{k+1}=\bm{\theta}_{k}+\alpha_{k}h(X_{k},\bm{\theta}_{k},\lambda_{k})\), we have the following inequality due to Lipschitz continuity of \(h\) in (46)

\[\|\bm{\theta}_{k+1}-\bm{\theta}_{k}\|=\alpha_{k}\|h(X_{k},\bm{\theta}_{k}, \lambda_{k})\|\leq\alpha_{k}L^{\prime}_{h}(\|\bm{\theta}_{k}\|+|\lambda_{k}|+1).\] (47)

Similarly, we have

\[|\lambda_{k+1}-\lambda_{k}|=\eta_{k}|g(X_{k},\bm{\theta}_{k},\lambda_{k})|\leq \eta_{k}L^{\prime}_{g}(\|\bm{\theta}_{k}\|+|\lambda_{k}|+1).\] (48)

Due to triangular inequality, adding (47) and (48) leads to

\[\|\bm{\theta}_{k+1}\|+|\lambda_{k+1}|+1 \leq(L^{\prime}_{h}\alpha_{k}+L^{\prime}_{g}\eta_{k}+1)(\|\bm{ \theta}_{k}\|+|\lambda_{k}|+1)\] \[\leq(L^{\prime}_{h}\alpha_{0}+L^{\prime}_{g}\eta_{0}+1)(\|\bm{ \theta}_{k}\|+|\lambda_{k}|+1),\] (49)

where the second inequality holds due to the non-increasing learning rates \(\{\alpha_{k},\eta_{k}\}\). Rewriting the above inequality in (49) in a recursive manner yields

\[\|\bm{\theta}_{k}\|+\lambda_{k}+1\leq(L^{\prime}_{h}\alpha_{0}+L^{\prime}_{g }\eta_{0}+1)^{k-\tau}(\|\bm{\theta}_{\tau}\|+|\lambda_{\tau}|+1).\] (50)

Hence, we have

\[\|\bm{\theta}_{k}-\bm{\theta}_{k-\tau}\|+|\lambda_{k}-\lambda_{k- \tau}| \leq\sum_{t=k-\tau}^{k-1}\|\bm{\theta}_{t+1}-\bm{\theta}_{t}\|+| \lambda_{t+1}-\lambda_{t}|\] \[\leq(L^{\prime}_{h}\alpha_{0}+L^{\prime}_{g}\eta_{0})\sum_{t=k- \tau}^{k-1}(\|\bm{\theta}_{t}\|+|\lambda_{t}|+1)\] \[\leq(L^{\prime}_{h}\alpha_{0}+L^{\prime}_{g}\eta_{0})(\|\bm{ \theta}_{k-\tau}\|+|\lambda_{k-\tau}|+1)\] \[\leq 2(L^{\prime}_{h}\alpha_{0}+L^{\prime}_{g}\eta_{0})\tau(\|\bm{ \theta}_{k-\tau}\|+|\lambda_{k-\tau}|+1),\]

where the last inequality holds when \((L^{\prime}_{h}\alpha_{0}+L^{\prime}_{g}\eta_{0})\tau\leq 1/4\). This implies when \(k=\tau\), we have

\[\|\bm{\theta}_{\tau}-\bm{\theta}_{0}\|+|\lambda_{\tau}-\lambda_{0}|\leq 2(L^{ \prime}_{h}\alpha_{0}+L^{\prime}_{g}\eta_{0})\tau(\|\bm{\theta}_{0}\|+| \lambda_{0}|+1).\] (51)

Similarly, we also have

\[\|\bm{\theta}_{k}-\bm{\theta}_{\tau}\|+\lambda_{k}-\lambda_{\tau} \leq\sum_{t=\tau}^{k-1}\|\bm{\theta}_{t+1}-\bm{\theta}_{t}\|+ \lambda_{t+1}-\lambda_{t}\] \[\leq\sum_{t=\tau}^{k-1}(L^{\prime}_{h}\alpha_{t}+L^{\prime}_{g} \eta_{t})(\|\bm{\theta}_{t}\|+\lambda_{t}+1)\] \[\leq(\|\bm{\theta}_{\tau}\|+\lambda_{\tau}+1)\sum_{t=\tau}^{k-1} (L^{\prime}_{h}\alpha_{t}+L^{\prime}_{g}\eta_{t})\prod_{i=0}^{t-\tau}(L^{ \prime}_{h}\alpha_{\tau+i}+L^{\prime}_{g}\eta_{\tau+i}+1).\] (52)

Therefore, the following inequality holds

\[\|\bm{\theta}_{k}-\bm{\theta}_{0}\|+|\lambda_{k}-\lambda_{0}|\] \[\leq\|\bm{\theta}_{k}-\bm{\theta}_{\tau}\|+|\lambda_{k}-\lambda_{ \tau}|+\|\bm{\theta}_{\tau}-\bm{\theta}_{0}\|+|\lambda_{\tau}-\lambda_{0}|\] \[\leq 2(L^{\prime}_{h}\alpha_{0}+L^{\prime}_{g}\eta_{0})\tau(\|\bm{ \theta}_{0}\|+|\lambda_{0}|+1)\] \[\qquad+(\|\bm{\theta}_{\tau}\|+\lambda_{\tau}+1)\sum_{t=\tau}^{k- 1}(L^{\prime}_{h}\alpha_{t}+L^{\prime}_{g}\eta_{t})\prod_{i=0}^{t-\tau}(L^{ \prime}_{h}\alpha_{\tau+i}+L^{\prime}_{g}\eta_{\tau+i}+1)\] \[\leq 2(L^{\prime}_{h}\alpha_{0}+L^{\prime}_{g}\eta_{0})\tau(\|\bm{ \theta}_{0}\|+|\lambda_{0}|+1)\]\[\leq\left(\frac{1}{2}+\frac{3}{2}\sum_{t=\tau}^{k-1}(L_{h}^{\prime} \alpha_{t}+L_{g}^{\prime}\eta_{t})\prod_{i=0}^{t-\tau}(L_{h}^{\prime}\alpha_{\tau +i}+L_{g}^{\prime}\eta_{\tau+i}+1)\right)(\|\boldsymbol{\theta}_{0}\|+|\lambda_{0 }|+1),\]

with the last equality holds when \((L_{h}^{\prime}\alpha_{0}+L_{g}^{\prime}\eta_{0})\tau\leq 1/4\). When \(\sum_{t=\tau}^{k-1}(L_{h}^{\prime}\alpha_{t}+L_{g}^{\prime}\eta_{t})\prod_{i=0 }^{t-\tau}(L_{h}^{\prime}\alpha_{\tau+i}+L_{g}^{\prime}\eta_{\tau+i}+1)\) is non-increasing with \(k\), then we can set \(c_{1}\) as \(c_{1}:=\frac{1}{2}+\frac{3}{2}.(L_{h}^{\prime}\alpha_{\tau}+L_{g}^{\prime}\eta _{\tau})(L_{h}^{\prime}\alpha_{\tau}+L_{g}^{\prime}\eta_{\tau}+1)\). This completes the proof. 

Provided Lemma 9, we have the following lemma related with local linearization of Q functions and the original Q functions.

**Lemma 10** (Lemma 5.2 in [13]).: _There exists a constant \(c_{1}\)such that_

\[\mathbb{E}\Big{[}\|h(X_{k},\boldsymbol{\theta}_{k},\lambda_{k})-h_{0}(X_{k}, \boldsymbol{\theta}_{k},\lambda_{k})\|^{2}|\mathcal{F}_{k-\tau}\Big{]}\leq \mathcal{O}\Big{(}c_{1}^{3}(\|\boldsymbol{\theta}_{0}\|+|\lambda_{0}|+1)^{3} \cdot m^{-1/2}\Big{)}.\]

Lemma 10 indicates that if the updated parameter is always bounded in a ball with the initialized one as the center and a fixed radius, the local linearized function \(f_{0}(\cdot)\) in (18) and the original neural network approximated function \(f(\cdot)\) in (6) have bounded gap, which tends to be zero as the width of hidden layer \(m\) grow large. For interested readers, please refer to [13] for detailed proofs of this lemma.