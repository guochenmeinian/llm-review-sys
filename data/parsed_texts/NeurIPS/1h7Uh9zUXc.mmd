# Reliable learning in challenging environments

 Maria-Florina Balcan

Carnegie Mellon University

ninamf@cs.cmu.edu &Steve Hanneke

Purdue University

steve.hanneke@gmail.com &Rattana Pukdee

Carnegie Mellon University

rpukdee@cs.cmu.edu &Dravyansh Sharma

Carnegie Mellon University

dravyans@cs.cmu.edu

###### Abstract

The problem of designing learners that provide guarantees that their predictions are provably correct is of increasing importance in machine learning. However, learning theoretic guarantees have only been considered in very specific settings. In this work, we consider the design and analysis of reliable learners in challenging test-time environments as encountered in modern machine learning problems: namely 'adversarial' test-time attacks (in several variations) and 'natural' distribution shifts. In this work, we provide a reliable learner with provably optimal guarantees in such settings. We discuss computationally feasible implementations of the learner and further show that our algorithm achieves strong positive performance guarantees on several natural examples: for example, linear separators under log-concave distributions or smooth boundary classifiers under smooth probability distributions.

## 1 Introduction

The question of providing reliability guarantees on the output of learned classifiers has been studied previously in the classical learning setting where the training and test data are independent and identically distributed (i.i.d.) draws from the same distribution [10, 11, 1]. Conceptually, a _reliable_ learner outputs a prediction and may output a correctness guarantee. We know that the learner is correct on all points with the guarantee as long as the learning-theoretic assumptions hold, e.g., realizability. While a trivial model that abstains from providing any guarantee is also a reliable learner, we are interested in a reliable learner that provides the guarantee on as many points as possible (_useful_ in the sense of Rivest and Sloan [10]). [11] provides a characterization of optimal reliable learners in this classical learning setting.

However, the assumption that the training and test data are drawn from the same distribution is often violated in practice. The mismatch may take the form of a 'natural distribution shift' when the test distribution is different from the training distribution or 'adversarial attacks' when there is an adversary that can perturb a test data point with the goal of changing the model prediction. This is frequently accompanied by a significant performance drop, as well as the inability to guarantee the usefulness of the algorithm. As a result, there is a significant interest in the study of test-time attacks [12, 13, 14] and distribution shift [10, 10, 11] among the applied machine learning community. Furthermore, recently there has been growing interest in the theoretical machine learning community for designing approaches with provable guarantees under test time attacks [1, 14, 15] as well as renewed interest in distribution shift [1, 13, 12]. All the prior theoretical work in the literature has mainly focused on the effect of attacks or distribution shift on average error rate (e.g. [1, 1]). However,this neglects a major relevant concern for users of machine learning algorithms, namely the ability to provide correctness guarantees for individual predictions: i.e., reliability.

In this work, we advance this line of work by developing a general understanding of how to learn reliably in the presence of corruptions or changes to the test set, specifically under adversarial test-time attacks as well as distribution shift between the training (source) and test (target) data.

**Our results**. We consider algorithms that provide robustly-reliable predictions which are guaranteed to be correct under standard assumptions from statistical learning theory, for both test-time attacks and distribution shift. Our first main set of results tackles the challenging case of adversarial test-time perturbations. For this setting, we introduce a novel compelling reliability criterion on a learner that particularly captures the challenge of reliability under the test-time attacks. Given a test point \(z\), a _robustly-reliable_ classifier either abstains from prediction, or outputs both a prediction \(y\) and a reliability guarantee \(\eta\) with the guarantee that \(y\) is correct unless one of two bad events has occurred: 1) the true target function does not belong to the given hypothesis set \(\mathcal{H}\) or, 2) a test-point \(z\) is perturbed from its original point by adversarial strength of at least \(\eta\) (measured in the relevant metric). In the case of distribution shift, we provide novel analysis and a complexity measure that extend the classical notion of reliable learning to the setting when the test distribution is allowed to be an arbitrary new distribution.

### Summary of contributions

1. We propose robustly-reliable learners for test-time attacks which guarantee reliable learning in the presence of test-time attacks, and characterize the region of instance space where they are simultaneously robust and reliable. Specifically, under the realizable setting, for adversarial perturbations within metric balls around the test points, we use the radius of the metric ball as a natural notion of adversarial strength. We output a reliability radius \(\eta\) with a guarantee that our prediction on a point is correct as long as it was perturbed with a distance less than \(\eta\) (under a given metric). We further show that our proposed robustly-reliable learner achieves pointwise optimal values for this reliability radius: that is, no robustly-reliable learner can output a reliability radius larger than our learner for any point in the instance space (Theorem B.1, B.2).
2. The pointwise optimal algorithm is easy to derive from our definition. We discuss a computationally efficient implementation of the optimal learners. (Section 4).
3. We discuss variants of these algorithms and guarantees appropriate for three different variants of adversarial losses studied in the literature: depending on whether the perturbed point must have the same label as the original point, or in lieu of this, whether the algorithm should predict the true label of the perturbed point, or the same label as the original point (Definition 1).
4. We further introduce a safely-reliable region, which captures the challenge caused by the adversary's ability to perturb a test point to cause a reduction in our reliability radius (Definition 6). As examples, we show that the safely-reliable region can be large for linear separators under log-concave distributions and for classifiers with smooth decision boundaries under nearly-uniform distributions and as a consequence, the robustly-reliable region is large as well (Theorem 3.3).
5. We extend this characterization to abstention-based reliable predictions for arbitrary adversarial perturbation sets, where we no longer restrict ourselves to metric balls. We again get a tight characterization of the robustly-reliable region (Theorem C.1).
6. We also consider reliability in the distribution shift setting where the test data points come from a different distribution. We introduce a novel refinement to the notion of disagreement coefficient [14], to measure the **transferability of reliability guarantees** across distributions. We provide bounds on the probability mass of the reliable region under transfer for several interesting examples including, when learning linear separators, transfer from \(\beta_{1}\) log-concave to \(\beta_{2}\) log-concave and to \(s\)-concave distributions (Theorems G.1, G.2). We additionally bound the probability of the reliable region for learning classifiers with general smooth classification boundaries, for transfer between smooth distributions (Theorem G.3).
7. We further extend our reliability results to the setting of robustness tranfer, where the test data is simultaneously under adversarial perturbations as well as distribution shift (Section J).
8. Finally, we demonstrate that it is possible extend our results into the agnostic setting. (Section 7)

**Conceptual advances over prior work.** Prior works on certified robustness [13, 14, 15] have examined pointwise consistency guarantees. The certified robustness guarantee is only that a prediction does not change with an adversarial perturbation, but it does not guarantee that the prediction is correct (neither for the original point nor the perturbation); in particular, a constant function is always certified robust but it may not be useful. In contrast, our notion of robustly-reliable learner guarantees that, for any test point \(x\) and perturbation \(z\), if \(z\) has a distance less than \(\eta\) to \(x\) (\(\eta\) = reliability radius), then the prediction will be "correct" (robust loss zero) in a sense informed by which robust loss we are addressing; we discuss this idea for several different losses, leading to different interpretations of this guarantee. In particular, for the stability loss, the prediction being "correct" means that it predicts the true label of the original point \(x\); this implies certified robustness, but is even stronger, since it also guarantees the correct label. Prior work [1] introduces the notion of a robustly-reliable learner for poisoning attacks which is different from our definition that is tailored to test-time attacks with a guarantee in terms of a reliability radius. In distribution shifts setting, we are the first to assess the transferability of reliability guarantees which differ from a widely-studied metric of average error rate. For additional related work, we refer to Appendix A.

## 2 Preliminaries and problem formulation

Let \(\mathcal{X}\) denote the instance space and \(\mathcal{Y}=\{0,1\}\) be the label space. Let \(\mathcal{H}\) be a hypothesis class. The learner \(\mathcal{L}\) is given access to a labeled sample \(S=\{(x_{i},y_{i})\}_{i=1}^{m}\) drawn from a distribution \(\mathcal{D}\) over \(\mathcal{X}\times\mathcal{Y}\) and learns a concept \(h^{\mathcal{L}}:\mathcal{X}\to\mathcal{Y}\). In the realizable setting, we assume we have a hypothesis (concept) class \(\mathcal{H}\) and target concept \(h^{*}\in\mathcal{H}\) such that the _true label_ of any \(x\in\mathcal{X}\) is given by \(h^{*}(x)\). In particular, \(S=\{(x_{i},h^{*}(x_{i}))\}_{i=1}^{m}\) in this setting. Given the 0-1 loss function \(\ell:\mathcal{H}\times\mathcal{X}\to\{0,1\}\), define \(\operatorname{err}_{S}(h,\ell)=\frac{1}{m}\sum_{(x,y)\in S}\ell(h,x)\). We use \(\mathcal{D}_{\mathcal{X}}\) to denote the marginal distribution over \(\mathcal{X}\). We use \(\mathbb{I}[\cdot]\) to denote the indicator function that takes values in \(\{0,1\}\). We also define \(B_{\mathcal{D}}^{\mathcal{H}}(h^{*},r)=\{h\in\mathcal{H}\mid\operatorname{Pr} _{\mathcal{D}}[h(x)\neq h^{*}(x)]\leq r\}\) as the set of hypotheses in \(\mathcal{H}\) that disagree with \(h^{*}\) with probability at most \(r\). During test-time, the learner makes a prediction on a test-point \(z\in\mathcal{X}\). We consider the following settings

1. **Adversarial test-time attack.** We consider adversarial attacks with perturbation function \(\mathcal{U}:\mathcal{X}\to 2^{\mathcal{X}}\) that can perturb a test point \(x\) to an arbitrary point \(z\) from the perturbation set \(\mathcal{U}(x)\subseteq\mathcal{X}\)[13]. We assume that the adversary has access to the learned concept \(h^{\mathcal{L}}\) as well as the test point \(x\), and can perturb this data point to any \(z\in\mathcal{U}(x)\) and then provide this perturbed data point to the learner at test-time. We want to provide pointwise robustness and reliability guarantees in this setting. We will assume that \(x\in\mathcal{U}(x)\) for all \(x\in\mathcal{X}\). For any point \(z\), we have \(\mathcal{U}^{-1}(z):=\{x\in\mathcal{X}|z\in\mathcal{U}(x)\}\), the set of points that can be perturbed to \(z\). We use _perturbation_ to refer to a point \(z\in\mathcal{U}(x)\) and the perturbation sets \(\mathcal{U}(x)\) interchangeably.
2. **Distribution shift.** We consider when a test point \(z\) is drawn from a different distribution from the training samples. In this case, we want to provide a pointwise reliability guarantee. We will discuss more on this in Section 5.

### Robust loss functions

In the applied and theoretical literature, various definitions of adversarial success have been explored, each dependent on the interpretation of robustness; depending on whether the perturbed point must have the same label as the original point, or in lieu of this, whether the algorithm should predict the true label of the perturbed point, or the same label as the original point. To capture these, we formally consider the following loss functions.

**Definition 1** (Robust loss functions).: _For a hypothesis \(h\), a test point \(x\), and a perturbation function \(\mathcal{U}\), we consider the following adversarially successful events._

1. _Constrained Adversary loss_ _[_1_,_ 1_,_ 1_]__. There exists a perturbation_ \(z\) _of_ \(x\) _that does not change the true label of an original point_ \(x\) _but_ \(h(z)\) _is incorrect._ \[\ell_{\text{CA}}^{h^{*}}(h,x)=\sup_{\begin{subarray}{c}z\in\mathcal{U}(x)\\ h^{*}(z)-h^{*}(x)\end{subarray}}\mathbb{I}[h(z)\neq h^{*}(z)].\] _For a fixed perturbation_ \(z\in\mathcal{U}(x)\)_, define_ \(\ell_{\text{CA}}^{h^{*}}(h,x,z)=\mathbb{I}[h(z)\neq h^{*}(z)\wedge h^{*}(z)=h^ {*}(x)].\)__
2. _True Label loss_ _[_1_,_ 1_]__. There exists a perturbation_ \(z\) _of_ \(x\) _such that_ \(h(z)\) _is incorrect._ \[\ell_{\text{TL}}^{h^{*}}(h,x)=\sup_{z\in\mathcal{U}(x)}\mathbb{I}[h(z)\neq h^ {*}(z)].\]_In this case, if the true label of the \(z\) changes, then the learner need to match its prediction with the new label. For a fixed perturbation \(z\in\mathcal{U}(x)\), define \(\ell_{\text{TL}}^{h^{*}}(h,x,z)=\mathbb{I}[h(z)\neq h^{*}(z)]\)._
3. _Stability loss_[1, 1, 10, 11]_. There exists a perturbation \(z\) of \(x\) such that \(h(z)\) is different from \(h^{*}(x)\)._ \[\ell_{\text{ST}}^{h^{*}}(h,x)=\sup_{z\in\mathcal{U}(x)}\mathbb{I}[h(z)\neq h^{ *}(x)].\] _In this case, we focus on the consistency aspect where we want the prediction of any perturbation_ \(z\) _to be the same as the prediction of_ \(x\) _and this has to be correct w.r.t._ \(x\) _i.e. equals to_ \(h^{*}(x)\)_. For a fixed perturbation_ \(z\in\mathcal{U}(x)\)_, define_ \(\ell_{\text{ST}}^{h^{*}}(h,x,z)=\mathbb{I}[h(z)\neq h^{*}(x)]\)_._
4. _Incentive-aware Adversary loss_[1]_. We take inspiration from economics application where we assume that the label_ \(1\) _is a more favorable outcome e.g. loan approval for which an adversary has no incentive to make any perturbation when the original label is_ \(1\)_. Define the perturbation set_ \[\mathcal{U}_{\text{IA}}(x,h^{*})=\begin{cases}\mathcal{U}(x)&;\,\,h^{*}(x)=0 \\ \{x\}&;\,\,h^{*}(x)=1\end{cases}\] _and define an incentive-aware adversary loss as_ \[\ell_{\text{IA}}^{h^{*}}(h,x)=\sup_{z\in\mathcal{U}_{\text{IA}}(x,h^{*})} \mathbb{I}[h(z)\neq h^{*}(x)].\]

_For a fixed perturbation \(z\in\mathcal{U}(x)\), define \(\ell_{\text{IA}}^{h^{*}}(h,x,z)=\mathbb{I}[h(z)\neq h^{*}(x)\wedge z\in \mathcal{U}_{\text{IA}}(x,h^{*})]\)._

_We say that \(h\) is robust to a perturbation function \(\mathcal{U}\) at \(x\) w.r.t. a robust loss \(\ell\) if \(\ell_{\text{IA}}^{h^{*}}(h,x)=0\)._

**Remark**.: \(\ell_{\text{ST}},\ell_{\text{IA}}\) are robust losses that we can always evaluate in practice on the training data since we are comparing \(h(z)\) with \(h^{*}(x)\) which is known to us on the training data. For \(\ell_{\text{CA}},\ell_{\text{TL}}\), we are comparing \(h(z)\) with \(h^{*}(z)\) for which \(z\) may lie outside of the support of the natural data distribution and we may not have access to \(h^{*}(z)\). We illustrate the relationship between these losses by making a few useful observations.

* In the robustly-realizable case [1] when the perturbation function \(\mathcal{U}\) does not change the _true_ label of any \(x\) in the training or test data, then all the losses \(\ell_{\text{CA}},\ell_{\text{TL}},\ell_{\text{ST}}\) are equivalent. This corresponds to a common assumption in the adversarial robustness literature, that the perturbations are "human-imperceptible", which is usually quantified as the set of perturbations within a small metric ball around the data point.
* We provide an illustration of the perturbation set considered in various robust losses in Figure 1. By considering these perturbation set, we have the following implication \(\ell_{\text{TL}}\to\ell_{\text{CA}}\), \(\ell_{\text{ST}}\to\ell_{\text{CA}}\) and \(\ell_{\text{ST}}\to\ell_{\text{IA}}\) where \(\ell_{1}\to\ell_{2}\) means robustness w.r.t. \(\ell_{1}\) implies robustness w.r.t. \(\ell_{2}\).

Figure 1: Different perturbation sets considered in \(\ell_{\text{TL}},\ell_{\text{ST}}\) (left), \(\ell_{\text{CA}}\) (mid) and \(\ell_{\text{IA}}\) (right). The dashed line represents the decision boundary of \(h^{*}\) and the background color of red and blue represents the label \(0\) and \(1\) respectively. The ball around each point describes the possible perturbation set \(\mathcal{U}(x)\) and the shaded area inside each ball is the allowed perturbation. In \(\ell_{\text{TL}},\ell_{\text{ST}}\), we consider all perturbation in \(\mathcal{U}(x)\) while in \(\ell_{\text{CA}}\), we consider perturbations that do not change the true label of the perturbed point. Lastly, in \(\ell_{\text{IA}}\), an adversary only perturb points where the original true label is \(0\).

Robustly-reliable learners w.r.t. metric ball attacks

Although our robust losses are defined for any general perturbation set, we first consider the case where the perturbation sets are balls in some metric space. Such attacks are widely studied in the literature, in particular, for balls with bounded \(L_{p}\)-norm. Moreover, the radius of the metric ball serves as a natural notion of adversarial strength that allows us to quantify the level of robustness. We will later (Theorem C.1) present results for general perturbation sets as well.

Let \(\mathcal{M}=(\mathcal{X},d)\) be a metric space equipped with distance metric \(d\). We use the notation \(\mathbf{B}_{\mathcal{M}}(x,r)=\{x^{\prime}\in\mathcal{X}\mid d(x,x^{\prime}) \leq r\}\) (resp. \(\mathbf{B}^{\mathcal{B}}_{\mathcal{M}}(x,r)=\{x^{\prime}\in\mathcal{X}\mid d (x,x^{\prime})<r\}\)) to denote a closed (resp. open) ball of radius \(r\) centered at \(x\). We will sometimes omit the underlying metric \(\mathcal{M}\) from the subscript to reduce notational clutter. We formally define a metric ball attack as follows.

**Definition 2**.: _Metric-ball attacks are defined as the class of perturbation functions \(\mathcal{U}_{\mathcal{M}}=\{u_{\eta}:\mathcal{X}\to 2^{\mathcal{X}}\mid u_{ \eta}(x)=\mathbf{B}_{\mathcal{M}}(x,\eta)\}\), induced by the metric \(\mathcal{M}=(\mathcal{X},d)\) defined over the instance space._

At test-time, given a test-point \(z\in\mathcal{X}\), we would like to make a prediction at \(z\) with a reliability guarantee. We consider this type of learner, a _robustly-reliable_ learner defined formally as follows.

**Definition 3** (Robustly-reliable learner w.r.t. \(\mathcal{M}\)-ball attacks).: _A learner \(\mathcal{L}\) is robustly-reliable w.r.t. \(\mathcal{M}\)-ball attacks for hypothesis space \(\mathcal{H}\) and robust loss function \(\ell\) if, **for any target** concept \(h^{*}\in\mathcal{H}\), given \(S\) labeled by \(h^{*}\), the learner outputs functions \(h^{\mathcal{E}}_{\mathcal{M}}:\mathcal{X}\rightarrow\mathcal{Y}\) and \(r^{\mathcal{E}}_{S}:\mathcal{X}\rightarrow[0,\infty)\cup\{-1\}\) such that for all \(x,z\in\mathcal{X}\) if \(r^{\mathcal{E}}_{S}(z)=\eta>0\) and \(z\in\mathbf{B}^{\mathcal{B}}_{\mathcal{M}}(x,\eta)\) then \(\ell^{h^{*}}(h^{\mathcal{E}}_{S},x,z)=0\). Further, if \(r^{\mathcal{E}}_{S}(z)=0\), then \(h^{*}(z)=h^{\mathcal{E}}_{S}(z)\)._

Note that \(\mathcal{L}\) outputs a prediction and a real value \(r\) (the "reliability radius") for any test input. \(r=-1\) corresponds to abstention (even in the absence of perturbation) i.e. when the learner is incapable of giving a reliability guarantee for that prediction), and \(r=\eta>0\) is a guarantee from the learner that if the adversary's attack is in \(\mathbf{B}^{\mathcal{B}}_{\mathcal{M}}(x,\eta)\) then we are correct i.e. if an adversary changes the original test point \(x\) to \(z\), the attack will not succeed if the adversarial budget is less than \(\eta\). Lastly, when \(r=0\), the learner provides a guarantee that the learner's prediction at \(z\) is correct.

**Definition 4** (Robustly-reliable region w.r.t. \(\mathcal{M}\)-ball attacks).: _For a robustly-reliable learner \(\mathcal{L}\) w.r.t. \(\mathcal{M}\)-ball attacks for sample \(S\), hypothesis space \(\mathcal{H}\) and robust loss function \(\ell\) defined above, the robustly-reliable region of \(\mathcal{L}\) at a reliability level \(\eta\) is defined as \(\text{RR}^{\mathcal{L}}(S,\eta)=\{x\in\mathcal{X}\mid r^{\mathcal{E}}_{S}(x) \geq\eta\}\) for sample \(S\) and \(\eta\geq 0\)._

The robustly-reliable region contains all points with a reliability guarantee of at least \(\eta\). We use \(\text{RR}^{\mathcal{E}}_{W}\) to denote robustly-reliable regions with respect to losses \(\ell_{W}\) for \(W\in\{\text{CA},\text{TL},\text{ST},\text{IA}\}\). A natural goal is to find a robustly-reliable learner \(\mathcal{L}\) that has the largest robustly-reliable region possible. First, we note that predictions that are known by the learner to be correct are still known to be correct even when the test points are attacked. Therefore, a test point \(z\) lies in the robustly-reliable region w.r.t. \(\ell_{\text{CA}},\ell_{\text{TL}}\), as long as we can be sure that \(h^{\mathcal{E}}_{S}(z)\) is correct. This is equivalent to \(z\) being classified perfectly, i.e. according to the true label. Therefore, the robustly-reliable region w.r.t. \(\ell_{\text{CA}},\ell_{\text{TL}}\) is given by the agreement region of the version space, which is the largest region where we can be sure of what the correct label is in the absence of any adversarial attack [1]. We recall the definition of version space [14] and agreement region [11, 1].

**Definition 5**.: _For a set \(H\subseteq\mathcal{H}\) of hypothesis, and any set of samples \(S\), let \(\operatorname{DIS}(H)=\{x\in\mathcal{X}:\exists h_{1},h_{2}\in\mathcal{H}\) s.t. \(h_{1}(x)\neq h_{2}(x)\}\) be the **disagreement region** and \(\operatorname{Agree}(H)=\mathcal{X}\setminus\operatorname{DIS}(H)\) be the **agreement region**. Let \(\mathcal{H}_{0}(S)=\{h\in\mathcal{H}|\operatorname{err}_{S}(h)=0\}\) be a **version space**: the set of all hypotheses that correctly classify \(S\). More generally, \(\mathcal{H}_{\nu}(S)=\{h\in\mathcal{H}|\operatorname{err}_{S}(h)\leq\nu\}\) for \(\nu\geq 0\)._

We can also characterize the robustly-reliable region with respect to other robust losses in terms of the agreement region in the following Theorem.

**Theorem 3.1**.: _Let \(\mathcal{H}\) be any hypothesis class. With respect to \(\mathcal{M}\)-ball attacks and \(\ell_{W}\), for \(\eta\geq 0\),_

1. _there exists a robustly-reliable learner_ \(\mathcal{L}\) _such that_ \(\text{RR}^{\mathcal{L}}_{W}(S,\eta)\supseteq A_{W}\)_, and_
2. _for any robustly-reliable learner_ \(\mathcal{L}\)_,_ \(\text{RR}^{\mathcal{L}}_{W}(S,\eta)\subseteq A_{W}\)_._

_Specifically, for the robust loss \(\ell_{W}\), the optimal robustly-reliable region \(A_{W}\) are_Proof.: (Sketch) We provide the construction of the optimal robustly-reliable learner \(\mathcal{L}_{\text{opt}}\) such that \(\text{RR}_{W}^{\mathcal{L}_{\text{opt}}}(S,\eta)\supseteq A_{W}\) and later show that for any robustly-reliable learner \(\mathcal{L}\), we must also have \(\text{RR}_{W}^{\mathcal{L}_{\text{opt}}}(S,\eta)\subseteq A_{W}\). We start with \(\ell_{\text{CA}},\ell_{\text{TL}}\), consider a learner \(\mathcal{L}_{\text{opt}}\) that predicts using an ERM classifier and outputs \(\eta=\infty\) for all points in the agreement region of \(\mathcal{H}_{0}(S)\). Any prediction in \(\operatorname{Agree}(\mathcal{H}_{0}(S))\) is reliable because it also agrees with \(h^{*}\) (\(h^{*}\in\mathcal{H}_{0}(S)\) by realizability). On the other hand, for \(z\in\operatorname{DIS}(\mathcal{H}_{0}(S))\), there exist \(h_{1},h_{2}\in\mathcal{H}_{0}(S)\) that disagree on \(z\). For any learner \(\mathcal{L}\), it is not possible to guarantee that \(h^{\mathcal{L}}(z)\) is correct as we may have \(h^{*}=h_{1}\) or \(h^{*}=h_{2}\).

Now, for \(\ell_{\text{ST}}\), consider a learner \(\mathcal{L}_{\text{opt}}\) that classifies using an ERM but the reliability radius is now the largest \(\eta>0\) for which \(\mathbf{B}^{o}(z,\eta)\subseteq\operatorname{Agree}(\mathcal{H}_{0}(S))\) and \(h(x)=h(z),\ \forall\ x\in\mathbf{B}^{o}(z,\eta),h\in\mathcal{H}_{0}(S)\), else \(\eta=0\) if \(z\in\operatorname{Agree}(\mathcal{H}_{0}(S))\), and \(-1\) otherwise. The first condition guarantees that \(h(x)=h^{*}(x),\forall x\in\mathbf{B}^{o}(z,\eta)\). Combined with the second condition we have \(h(z)=h(x)=h^{*}(x),\forall x\in\mathbf{B}^{o}(z,\eta)\). Thus, \(\mathcal{L}_{\text{opt}}\) is a robustly-reliable learner. On the other hand, for a robustly-reliable learner \(\mathcal{L}\), consider \(z\in\text{RR}_{\text{ST}}^{\mathcal{L}}(S,\eta)\) for \(\eta>0\). We must have \(h^{\mathcal{L}}(z)=h^{*}(x),\forall x\in\mathbf{B}^{o}(z,\eta)\). Using a similar argument to the case for \(\ell_{\text{CA}},\ell_{\text{TL}}\), we have \(z\in\operatorname{Agree}(\mathcal{H}_{0}(S))\). If there exists \(x\in\mathbf{B}^{o}(z,\eta)\) that \(x\not\in\operatorname{Agree}(\mathcal{H}_{0}(S))\), there exists \(h_{1},h_{2}\in\mathcal{H}_{0}(S)\) that \(h^{\mathcal{L}}(z)\neq h_{1}(x)\) or \(h^{\mathcal{L}}(z)\neq h_{2}(x)\). It is not possible to guarantee that \(h^{\mathcal{L}}(z)=h^{*}(x)\) as we may have \(h^{*}=h_{1}\) or \(h^{*}=h_{2}\). Therefore, we must have \(\mathbf{B}^{o}(z,\eta)\subseteq\operatorname{Agree}(\mathcal{H}_{0}(S))\). Finally, we cannot have \(x\in\mathbf{B}^{o}(z,\eta)\) that \(h(z)=h^{*}(z)\neq h^{*}(x)\) since this contradict with \(h(z)=h^{*}(x)\). Therefore, we must have \(h^{*}(x)=h^{*}(z)\). Since we have \(x\in\operatorname{Agree}(\mathcal{H}_{0}(S))\), this implies that \(h(x)=h(z)\) for \(h\in\mathcal{H}_{0}(S)\). For \(\ell_{\text{IA}}\), the construction is similar to \(\ell_{\text{ST}}\). For full proof, we refer to Appendix B. 

For \(\ell_{\text{ST}}\), the learner is able to certify a subset of the agreement region, which satisfies two additional conditions: \(h_{S}^{\mathcal{L}}\) must be correct on all possible points \(x\) that could be perturbed to an observed test point \(z\), and the true label of \(z\) should match the true label of \(x\). We denote the second condition of \(h(z)=h(x),\forall x\in\mathbf{B}^{o}(z,\eta),\forall h\in\mathcal{H}_{0}(S)\) as the **label consistency condition**. For \(\ell_{\text{IA}}\), since robustness w.r.t. \(\ell_{\text{ST}}\) implies robustness w.r.t. \(\ell_{\text{IA}}\), we have \(A_{\text{ST}}\subset A_{\text{IA}}\). In addition, with an incentive-aware adversary, whenever \(h(z)=0\), we must have \(h^{*}(x)=0\) since the adversary does not perturb a data point with the original label \(1\). Therefore, we can additionally provide a guarantee on \(z\) that lies in the agreement region and \(h(z)=0\). We remark that we can identify the term \(h^{*}(z)\) for any point \(z\in\operatorname{Agree}(\mathcal{H}_{0}(S))\) due to the realizability assumption.

We have provided a robustly-reliable learner with the largest possible robustly-reliable region for losses \(\ell_{\text{CA}},\ell_{\text{TL}},\ell_{\text{ST}},\ell_{\text{IA}}\). However, we note that the probability mass of the robustly-reliable region may not be a meaningful way to quantify the overall reliability of a learner because a perturbation \(z\) may lie outside of the support of the natural data distribution and have zero probability mass. It seems more useful to measure the mass of points \(x\) where any perturbation \(z\) of \(x\) still lies within the robustly-reliable region. We formally define this region as the safely-reliable region.

**Definition 6** (Safely-reliable region w.r.t. \(\mathcal{M}\)-ball attacks).: _Let \(\mathcal{L}\) be a robustly-reliable learner w.r.t. \(\mathcal{M}\)-ball attacks for sample \(S\), hypothesis class \(\mathcal{H}\) and robust loss function \(\ell\). The safely-reliable region of learner \(\mathcal{L}\) at reliability levels \(\eta_{1},\eta_{2}\) is defined as_

1. \(\text{SR}_{\mathcal{L}}^{\mathcal{L}}(S,\eta_{1},\eta_{2})=\{x\in\mathcal{X}\mid \mathbf{B}_{\mathcal{M}}(x,\eta_{1})\cap\{z\mid h^{*}(z)=h^{*}(x)\}\subseteq \text{RR}_{\mathcal{L}}^{\mathcal{L}}(S,\eta_{2})\},\)__
2. \(\text{SR}_{\text{IV}}^{\mathcal{L}}(S,\eta_{1},\eta_{2})=\{x\in\mathcal{X}\mid \mathbf{B}_{\mathcal{M}}(x,\eta_{1})\subseteq\text{RR}_{W}^{\mathcal{L}}(S,\eta _{2})\}\) _for_ \(W\in\{\text{TL},\text{ST}\}\)_,_
3. \(\text{SR}_{\text{IA}}^{\mathcal{L}}(S,\eta_{1},\eta_{2})=\{x\in\mathcal{X}\mid h ^{*}(x)=0\wedge\mathbf{B}_{\mathcal{M}}(x,\eta_{1})\subseteq\text{RR}_{\text{IA}}^ {\mathcal{L}}(S,\eta_{2})\}\cup\{x\in\mathcal{X}\mid h^{*}(x)=1\wedge x\in \text{RR}_{\text{IA}}^{\mathcal{L}}(S,\eta_{2}))\}\)_._

The safely-reliable region contains any point that retains a reliability radius of at least \(\eta_{2}\) even after being attacked by an adversary with strength \(\eta_{1}\). In the safely-reliable region, we consider a set of potential natural (before attack) points \(x\), while in the robustly-reliable region, we consider a set of potential test points \(z\). In the following subsections, we show that in interesting cases commonly studied in the literature, the probability mass of the safely-reliable region is actually quite large.

### Safely-reliable region for linear separators under log-concave distributions is large

We provide the probability mass of safely-reliable regions with respect to different losses for linear separators when the data distribution follows an isotropic (mean zero and identity covariance matrix) log-concave (logarithm of density function is concave) distribution under a bounded \(L_{2}\)-norm ball attack. For full proof, we refer to Appendix D. We will rely on the following key lemma which states that the agreement region of a linear separator cannot contain points that are arbitrarily close to the decision boundary of \(h^{*}\) for any sample \(S\).

**Lemma 3.2**.: _Let \(\mathcal{D}\) be a distribution over \(\mathbb{R}^{d}\) and \(\mathcal{H}=\{h:x\rightarrow\operatorname{sign}(\langle w_{h},x\rangle)\mid w _{h}\in\mathbb{R}^{d},\|w_{h}\|_{2}=1\}\) be a class of linear separators. For \(h^{*}\in\mathcal{H}\), for a set of samples \(S\sim\mathcal{D}^{\prime n}\) such that there is no data point in \(S\) that lies on the decision boundary, for any \(0<c<d\), there exists \(\delta(S,c,d)>0\) such that for any \(x\) with \(c\leq\|x\|\leq d\) and \(|\langle w_{h^{*}},x\rangle|<\delta\), we have \(x\notin\operatorname{Agree}(\mathcal{H}_{0}(S))\)._

A direct implication of the lemma is that any \(L_{2}\)-ball \(\mathbf{B}(x,\eta)\) that lies in the agreement region must not contain the decision boundary of \(h^{*}\) and must contain points with the same label. This allows us to remove the _label consistency condition_ and instead focus on whether the ball \(\mathbf{B}(x,\eta)\) lies in the agreement region. Intuitively, the reliable region is now given by the '\(\eta\)-buffered' agreement region where we only select points that have a distance at least \(\eta\) from the boundary of the agreement region (Figure 2). We provide bounds for the probability mass of the safely-reliable region below and we refer to the full proof in Appendix D.

**Theorem 3.3**.: _Let \(\mathcal{D}\) be isotropic log-concave over \(\mathbb{R}^{d}\) and \(\mathcal{H}=\{h:x\rightarrow\operatorname{sign}(\langle w_{h},x\rangle)\mid w _{h}\in\mathbb{R}^{d},\|w_{h}\|_{2}=1\}\) be the class of linear separators. Let \(\mathbf{B}(\cdot,\eta)\) be a \(L_{2}\) ball perturbation with radius \(\eta\). For \(S\sim\mathcal{D}^{\prime n}\), for \(m=\mathcal{O}(\frac{1}{\varepsilon^{2}}(\operatorname{VCdim}(\mathcal{H})+ \ln\frac{1}{\delta}))\), for an optimal robustly-reliable learner \(\mathcal{L}\),_

1. \(\Pr(\text{SR}^{\mathcal{L}}_{\mathcal{I}}(S,\eta_{1},\eta_{2}))\geq 1-2\eta_{1}- \tilde{\mathcal{O}}(\sqrt{d}\varepsilon)\) _with probability at least_ \(1-\delta\)_,_
2. \(\text{SR}^{\mathcal{L}}_{\mathcal{C}A}(S,\eta_{1},\eta_{2})=\text{SR}^{ \mathcal{L}}_{\mathcal{I}L}(S,\eta_{1},\eta_{2})\) _almost surely,_
3. \(\Pr(\text{SR}^{\mathcal{L}}_{\mathcal{I}S}(S,\eta_{1},\eta_{2}))\geq 1-2(\eta_{1}+ \eta_{2})-\tilde{\mathcal{O}}(\sqrt{d}\varepsilon)\) _with probability at least_ \(1-\delta\)_,_
4. \(\Pr(\text{SR}^{\mathcal{L}}_{\mathcal{I}A}(S,\eta_{1},\eta_{2}))\geq 1-(\eta_{1}+ \eta_{2})-\tilde{\mathcal{O}}(\sqrt{d}\varepsilon)\) _with probability at least_ \(1-\delta\)_._

_The \(\tilde{\mathcal{O}}\)-notation suppresses dependence on logarithmic factors and distribution-specific constants._

We remark that we can't always remove the label consistency condition for a general perturbation set. For example, consider \(\mathcal{U}(x)=\mathbf{B}(x-a,\eta)\cup\{x\}\cup\mathbf{B}(x+a,\eta)\), is made of two \(L_{2}\) balls with center \(x-a,x+a\), with appropriate value of \(a,\eta\), we may have each ball lie in the different side of the agreement region so that the whole perturbation set lie in the agreement region but contain points with different labels (Figure 3). We also provide bounds on the probability mass of the safely-reliable region for more general concept spaces beyond linear separators, specifically, classifiers with smooth boundaries in Appendix E.

## 4 On computational efficiency

Given the definition, it is possible to implement computationally efficient robustly-reliable learners. For example, for linear separator concept classes under bounded \(L_{2}\)-norm attack. The optimal

Figure 2: The robustly-reliable region for \(\ell_{\text{CA}},\ell_{\text{TL}}\) (left), \(\ell_{\text{ST}}\) (mid) and \(\ell_{\text{IA}}\)(right) for linear separators with an \(L_{2}\)-ball perturbation. The background color of blue and red represents the agreement region of class \(1\) and \(0\) respectively. In this case, we can remove the label consistency condition and reduce the robustly-reliable region into the ‘\(\eta\)-buffered’ agreement region.

robustly-reliable learner \(\mathcal{L}_{\text{opt}}\), described above may be implemented via a linearly constrained quadratic program that computes the (squared) distance of the test point \(z\) to the closest point \(z^{\prime}\) in the disagreement region. This gives us the reliability radius, since for linear separators one must cross the decision boundary to perturb a point to a differently labeled point

\[\min_{w,w^{\prime},z^{\prime}}||z-z^{\prime}||^{2}\] s.t. \[\langle w,x_{i}\rangle y_{i}\geq 0, \text{for each }(x_{i},y_{i})\in S,\] \[\langle w^{\prime},x_{i}\rangle y_{i}\geq 0, \text{for each }(x_{i},y_{i})\in S,\] \[\langle w,z^{\prime}\rangle\langle w^{\prime},z^{\prime}\rangle\leq 0.\]

Given training sample \(S\), for any given test point \(z\), the learner \(\mathcal{L}\) can efficiently compute the solution \(s^{*}\) to the above program and output \(\sqrt{s^{*}}\) as the reliability radius. We show that the variant of this objective also provides a reliability radius for a wide range of hypothesis classes under \(L_{2}\) ball attacks (see Lemma F.1). In addition, we can relax this objective into a regularized objective that gives a lower bound on the reliability radius of \(||z-z^{*}||^{2}\), when \(z^{*}\) is the solution of

\[h_{1},h_{2},z^{*}=\operatorname*{argmin}_{h,h^{\prime},z^{\prime}}||z-z^{ \prime}||^{2}+\lambda(\hat{R}(h,S\cup\{(z^{\prime},0)\})+\hat{R}(h^{\prime},S \cup\{(z^{\prime},1)\}))\]

when \(\hat{R}(h,S)\) is the empirical risk of \(h\) on \(S\). We provide a more detailed discussion in Appendix F.

## 5 Robustly-reliable learning under distribution shift

We now consider the reliability aspect for distribution shift, a different kind of test-time robustness challenge when the test data comes from a different distribution than the training data. Formally, let \(\mathcal{P}\) be the training distribution and let \(\mathcal{Q}\) be the test distribution. We assume the _realizable distribution shift_ setting, i.e. there is a target concept \(h^{*}\in\mathcal{H}\) such that the true label of any \(x\in\mathcal{X}\) is given by \(h^{*}(x)\) at training time and test time, or \(\operatorname{err}_{\mathcal{P}}(h^{*})=\operatorname{err}_{\mathcal{Q}}(h^{* })=0\). As observed earlier, points that

Figure 4: The disagreement region and the agreement region under a distribution shift where \(\mathcal{P}\) and \(\mathcal{Q}\) are isotropic (left) and where there is a mean shift (right).

Figure 3: The perturbation set is represented by two dashed balls. This lies inside the agreement region but contains points with different labels.

are known by the learner to be correct (reliable) are still known to be correct even when it is drawn from a different distribution. This reliability guarantee holds even when the distributions \(\mathcal{P}\) and \(\mathcal{Q}\) do not share a common support, a setting for which many prior theoretical works result in vacuous bounds. For example, suppose \(\mathcal{X}=\mathbb{R}^{n}\), \(\mathcal{P}\) and \(\mathcal{Q}\) are supported on disjoint \(n\)-balls, and \(\mathcal{H}\) is the class of linear separators. Then the total variation distance, the \(\mathcal{H}\)-divergence [1, 2] as well as the discrepancy distance [13] between \(\mathcal{P}\) and \(\mathcal{Q}\) are all 1. While recent work of [16] does apply in this setting, they do not focus on the reliability guarantee. In this work, we are interested in quantifying the transferability of reliability guarantee transfer between distributions \(\mathcal{P}\) and \(\mathcal{Q}\). We recall the notion of reliable prediction [1].

**Definition 7** (Reliability).: _A learner \(\mathcal{L}\) is reliable w.r.t. concept space \(\mathcal{H}\) if, for any target concept \(h^{*}\in\mathcal{H}\), given any sample \(S\) labeled by \(h^{*}\), the learner outputs functions \(h^{\mathcal{L}}_{S}:\mathcal{X}\to\mathcal{Y}\) and \(a^{\mathcal{L}}_{S}:\mathcal{X}\to\{0,1\}\) such that for all \(x\in\mathcal{X}\) if \(a^{\mathcal{L}}_{S}(x)=1\) then \(h^{\mathcal{L}}_{S}(x)=h^{*}(x)\). Else, if \(a^{\mathcal{L}}_{S}(x)=0\), the learner abstains from prediction. The reliable region of \(\mathcal{L}\) is \(R^{\mathcal{L}}(S)=\{x\in\mathcal{X}\mid a^{\mathcal{L}}_{S}(x)=1\}\)._

We define the following metric to measure the reliability of a learner under distribution shift.

**Definition 8** (\(\mathcal{P}{\to}\mathcal{Q}\) reliable correctness).: _The \(\mathcal{P}{\to}\mathcal{Q}\)-reliable correctness of \(\mathcal{L}\) (at sample rate \(m\), for distribution shift from \(\mathcal{P}\) to \(\mathcal{Q}\)) is defined as the expected probability mass of its reliable region under \(Q\) when trained on a random training \(S\sim\mathcal{P}^{m}\), i.e. \(\Pr_{x\sim Q,S\sim P^{m}}[x\in R^{\mathcal{L}}(S)]\)._

The disagreement coefficient was originally introduced to study the label complexity in agnostic active learning [14] and is also known to characterize reliable learning in the absence of any distribution shift [1]. We propose the following refinement to the notion of disagreement coefficient, which we will use to give bounds on a learner's \(\mathcal{P}{\to}\mathcal{Q}\) reliable correctness.

**Definition 9** (\(\mathcal{P}{\to}\mathcal{Q}\) disagreement coefficient).: _For a hypothesis class \(\mathcal{H}\), the \(\mathcal{P}{\to}\mathcal{Q}\) disagreement coefficient of \(h^{*}\in\mathcal{H}\) with respect to \(\mathcal{H}\) is given by_

\[\Theta_{\mathcal{P}{\to}\mathcal{Q}}(\varepsilon)=\sup_{r\geq\varepsilon} \frac{\Pr_{\mathcal{Q}}[\mathrm{DIS}(B_{\mathcal{P}}(h^{*},r))]}{r},\]

_where \(B_{\mathcal{P}}(h^{*},r)=\{h\in\mathcal{H}\mid\Pr_{\mathcal{P}}[h(x)\neq h^{* }(x)]\leq r\}\)._

This quantifies the rate of disagreement over \(\mathcal{Q}\) among classifiers which are within disagreement-balls w.r.t. \(h^{*}\) under \(\mathcal{P}\), relative to the version space radius. The proposed metric is asymmetric between \(\mathcal{P}\) and \(\mathcal{Q}\), and also depends on the target concept \(h^{*}\). More simple examples are in Appendix H. We show that the \(\mathcal{P}{\to}\mathcal{Q}\)-reliable correctness of our learner may be bounded in terms of the \(\mathcal{P}{\to}\mathcal{Q}\) disagreement coefficient using a uniform convergence based argument. The proof details are in Appendix I.

**Theorem 5.1**.: _Let \(\mathcal{Q}\) be a realizable distribution shift of \(\mathcal{P}\) with respect to \(\mathcal{H}\), and \(h^{*}\in\mathcal{H}\) be the target concept. Given sufficiently large sample size \(m\geq\frac{c}{\varepsilon^{2}}(d+\ln\frac{1}{\delta})\), the \(\mathcal{P}{\to}\mathcal{Q}\)-reliable correctness of \(\mathcal{L}\), the optimal robustly-reliable learner, is at least_

\[\Pr_{x\sim Q,S\sim P^{m}}[x\in R^{\mathcal{L}}(S)]\geq 1-\Theta_{\mathcal{P}{ \to}\mathcal{Q}}\cdot\varepsilon-\delta.\]

_Here \(c\) is an absolute constant, and \(d\) is the VC-dimension of \(\mathcal{H}\)._

In Appendix J, we show that this \(\mathcal{P}\to\mathcal{Q}\) disagreement coefficient can be small for several examples which implies that it is possible to transfer the reliability guarantee from one distribution to the other. In particular, when learning linear separators, we provide bounds for transferring from \(\beta_{1}\) log-concave to \(\beta_{2}\) log-concave and to \(s\)-concave distributions (Theorems G.1, G.2). In addition, when learning classifiers with general smooth classification boundaries, we provide bounds for transferring between smooth distributions (Theorem G.3).

## 6 Safely-reliable correctness under distribution shift

There is a growing practical [17, 18] as well as recent theoretical interest [1] in the setting of 'robustness transfer', where one simultaneously expects adversarial test-time attacks as well as distribution shift. We will study the reliability aspect for this more challenging setting. We note that the definition of a robustly-reliable learner does not depend on the data distribution (seeDefinition 3) as the guarantee is pointwise. Our optimality result in Section 3 applies even when a test point is drawn from a different distribution \(\mathcal{Q}\). In this case, the safely-reliable region instead would have a different probability mass.

**Definition 10** (\(\mathcal{P}{\rightarrow}\mathcal{Q}\) safely-reliable correctness).: _The \(\mathcal{P}{\rightarrow}\mathcal{Q}\) safely-reliable correctness of \(\mathcal{L}\) (at sample rate \(m\), for distribution shift from \(\mathcal{P}\) to \(\mathcal{Q}\), w.r.t. robust loss \(\ell\)) is defined as the probability mass of its safely-reliable region under \(Q\), on a sample \(S\sim\mathcal{P}^{m}\), i.e._

\[\text{PQR}_{\ell}^{\mathcal{L}}(S,\eta_{1},\eta_{2}):=\Pr_{x\sim Q,S\sim P^{m }}[x\in\text{SR}_{\ell}^{\mathcal{L}}(S,\eta_{1},\eta_{2})].\]

We consider an example when the training distribution \(\mathcal{P}\) is isotropic log-concave and the test distribution \(\mathcal{Q}_{\mu}\) is log-concave with its mean shifted by \(\mu\) but the covariance matrix is still an identity matrix (see Figure 4, right). We provide the bound on the \(\mathcal{P}{\rightarrow}\mathcal{Q}\) safely-reliable correctness of this example in Appendix J (see Theorem J.2).

## 7 Reliability in the agnostic setting

In the above, we have assumed that the training samples \(S\) are realizable under our concept class \(\mathcal{H}\), i.e. there is a target concept \(h^{*}\) consistent with our (uncorrupted) data. In the agnostic setting, we can have \(\min_{h\in\mathcal{H}}\operatorname{err}_{S}(h)>0\), meaning no single concept is always correct. We define a _\(\nu\)-tolerably robustly-reliable_ learner under test-time attacks in the agnostic setting as the learner whose reliable predictions agree with every low-error hypothesis (error at most \(\nu\)) on the training sample ([1]) have proposed the corresponding definition for data poisoning attacks).

**Definition 11** (\(\nu\)-tolerably robustly-reliable learner w.r.t. \(\mathcal{M}\)-ball attacks).: _A learner \(\mathcal{L}\) is robustly-reliable w.r.t. \(\mathcal{M}\)-ball attacks for sample \(S\), hypothesis space \(\mathcal{H}\) and robust loss function \(\ell\) if, for **every** concept \(h^{*}\in\mathcal{H}\) with \(\operatorname{err}_{S}(h^{*})\leq\nu\), the learner outputs functions \(h^{\mathcal{L}}_{S}:\mathcal{X}\rightarrow\mathcal{Y}\) and \(r^{\mathcal{L}}_{S}:\mathcal{X}\rightarrow[0,\infty)\cup\{-1\}\) such that for all \(x,z\in\mathcal{X}\) if \(r^{\mathcal{L}}_{S}(z)=\eta>0\) and \(z\in\mathbf{B}^{\mathcal{O}}_{\mathcal{M}}(x,\eta)\) then \(\ell^{h^{*}}(h^{\mathcal{L}}_{S},x,z)=0\). Further, if \(r^{\mathcal{L}}_{S}(z)=0\), then \(h^{*}(z)=h^{\mathcal{L}}_{S}(z)\). Given sample \(S\) such that some concept \(h^{*}\in\mathcal{H}\) satisfies \(\operatorname{err}_{S}(h^{*})\leq\nu\), the robustly-reliable region of \(\mathcal{L}\) is defined as \(RR^{\mathcal{L}}(S,\nu,\eta)=\{x\in\mathcal{X}\mid r^{\mathcal{L}}_{S}(x)\geq\eta\}\) for \(\nu,\eta\geq 0\)._

We generalize our results from Section 3 to the agnostic setting (proof details in Appendix K). Here \(\mathcal{H}_{\nu}(S)=\{h\in\mathcal{H}\mid\operatorname{err}_{S}(h)\leq\nu\}\).

**Theorem 7.1**.: _Let \(\mathcal{H}\) be any hypothesis class. With respect to \(\mathcal{M}\)-ball attacks and \(\ell_{\text{CA}}\), for \(\eta\geq 0\),_

1. _There exists a robustly-reliable learner_ \(\mathcal{L}\) _such that_ \(RR^{\mathcal{L}}_{\text{CA}}(S,\nu,\eta)\supseteq\operatorname{Agree}( \mathcal{H}_{\nu}(S))\)_,_
2. _For any robustly-reliable learner_ \(\mathcal{L}\)_,_ \(RR^{\mathcal{L}}_{\text{CA}}(S,\nu,\eta)\subseteq\operatorname{Agree}( \mathcal{H}_{\nu}(S))\)_._

## 8 Discussion

In this work, we generalize the classical line of works on reliable learning to address challenging test-time environments. We propose a novel robustly-reliability criterion that is applicable to several variations of test-time attacks. Our analysis leads to an easy-to-derive algorithm that can be implemented efficiently in many cases. Additionally, we introduce a \(\mathcal{P}\rightarrow\mathcal{Q}\) disagreement coefficient to capture the transferability of the reliability guarantee between distributions. The proposed robustly-reliability criterion and the \(\mathcal{P}\rightarrow\mathcal{Q}\) disagreement coefficient together provide a comprehensive framework for handling test-time attacks and evaluating the reliability of learning models. This contributes to the advancement of reliable learning methodologies in the face of challenging real-world scenarios, facilitating the development of more resilient and trustworthy machine learning systems. Notably, key questions remain open, including, how to efficiently implement the algorithm for a class of neural networks, and how to learn reliably with respect to any general robust loss function?

## 9 Acknowledgements

This work was supported in part by NSF grants CCF-1910321 and SES-1919453, the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003, a Bloomberg Data Science PhD fellowship, and a Simons Investigator Award.

## References

* [AB99] Martin Anthony and Peter L Bartlett. _Neural network learning: Theoretical foundations_. Cambridge University Press, 1999.
* [ABGLP19] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* [ABHU15] Pranjal Awasthi, Maria-Florina Balcan, Nika Haghtalab, and Ruth Urner. Efficient learning of linear separators under bounded noise. In _Conference on Learning Theory_, pages 167-190. PMLR, 2015.
* [ABL14] Pranjal Awasthi, Maria Florina Balcan, and Philip M Long. The power of localization for efficiently learning linear separators with noise. In _Proceedings of the forty-sixth annual ACM Symposium on Theory of Computing_, pages 449-458, 2014.
* [AK91] David Applegate and Ravi Kannan. Sampling and integration of near log-concave functions. In _Proceedings of the twenty-third annual ACM Symposium on Theory of Computing_, pages 156-163, 1991.
* [AKM19] Idan Attias, Aryeh Kontorovich, and Yishay Mansour. Improved generalization bounds for robust learning. In _Algorithmic Learning Theory_, pages 162-183. PMLR, 2019.
* [BBHS22] Maria-Florina Balcan, Avrim Blum, Steve Hanneke, and Dravyansh Sharma. Robustly-reliable learners under poisoning attacks. In _Conference on Learning Theory_. PMLR, 2022.
* [BBL06] Maria-Florina Balcan, Alina Beygelzimer, and John Langford. Agnostic active learning. In _Proceedings of the 23rd International Conference on Machine Learning_, pages 65-72, 2006.
* [BBS07] Steffen Bickel, Michael Bruckner, and Tobias Scheffer. Discriminative learning for differing training and test distributions. In _Proceedings of the 24th International Conference on Machine Learning_, pages 81-88, 2007.
* [BBSZ23] Maria-Florina Balcan, Avrim Blum, Dravyansh Sharma, and Hongyang Zhang. On the power of abstention and data-driven decision making for adversarial robustness. _Journal of Machine Learning Research_, 2023.
* [BCKW15] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In _International Conference on Machine Learning_, pages 1613-1622. PMLR, 2015.
* [BDBC\({}^{+}\)10] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. _Machine Learning_, 79:151-175, 2010.
* [BDBCP06] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. _Advances in Neural Information Processing Systems_, 19, 2006.
* [BH21] Maria-Florina Balcan and Nika Haghtalab. Noise in classification. _Beyond the Worst-Case Analysis of Algorithms_, page 361, 2021.
* [BL13] Maria-Florina Balcan and Phil Long. Active and passive learning of linear separators under log-concave distributions. In _Conference on Learning Theory_, pages 288-316. PMLR, 2013.
* [BPRZ23] Maria-Florina Balcan, Rattana Pukdee, Pradeep Ravikumar, and Hongyang Zhang. Nash equilibria and pitfalls of adversarial training in adversarial robustness games. In _The 26nd International Conference on Artificial Intelligence and Statistics_. PMLR, 2023.

* [BZ17] Maria-Florina Balcan and Hongyang Zhang. Sample and computationally efficient learning algorithms under s-concave distributions. _Advances in Neural Information Processing Systems_, 30, 2017.
* [CAL94] David Cohn, Les Atlas, and Richard Ladner. Improving generalization with active learning. _Machine Learning_, 15:201-221, 1994.
* [CDG\({}^{+}\)18] Corinna Cortes, Giulia DeSalvo, Claudio Gentile, Mehryar Mohri, and Scott Yang. Online learning with abstention. In _International Conference on Machine Learning_, pages 1059-1067. PMLR, 2018.
* [CDM16] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Boosting with abstention. _Advances in Neural Information Processing Systems_, 29, 2016.
* [CRK19] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized smoothing. In _International Conference on Machine Learning_, pages 1310-1320. PMLR, 2019.
* [CRS\({}^{+}\)19] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled data improves adversarial robustness. _Advances in Neural Information Processing Systems_, 32, 2019.
* [CW17] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In _2017 IEEE symposium on security and privacy_, pages 39-57. IEEE, 2017.
* [CWG\({}^{+}\)19] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. _Advances in Neural Information Processing Systems_, 32, 2019.
* [Dan16] Amit Daniely. Complexity theoretic limitations on learning halfspaces. In _Proceedings of the forty-eighth annual ACM Symposium on Theory of Computing_, pages 105-117, 2016.
* [DGH\({}^{+}\)23] Yuyang Deng, Nidham Gazagnadou, Junyuan Hong, Mehrdad Mahdavi, and Lingjuan Lyu. On the hardness of robustness transfer: A perspective from Rademacher complexity over symmetric difference hypothesis space. _arXiv preprint arXiv:2302.12351_, 2023.
* [DGT19] Ilias Diakonikolas, Themis Gouleakis, and Christos Tzamos. Distribution-independent pac learning of halfspaces with massart noise. _Advances in Neural Information Processing Systems_, 32, 2019.
* [DN21] John C Duchi and Hongseok Namkoong. Learning models with uniform performance via distributionally robust optimization. _The Annals of Statistics_, 49(3):1378-1406, 2021.
* [EYW10] Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. _Journal of Machine Learning Research_, 11(5), 2010.
* [EYW12] Ran El-Yaniv and Yair Wiener. Active learning via perfect selective classification. _Journal of Machine Learning Research_, 13(2), 2012.
* [GG16] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In _International Conference on Machine Learning_, pages 1050-1059. PMLR, 2016.
* [GKKM20] Shafi Goldwasser, Adam Tauman Kalai, Yael Kalai, and Omar Montasser. Beyond perturbations: Learning guarantees with arbitrary adversarial test examples. _Advances in Neural Information Processing Systems_, 33:15859-15870, 2020.
* [GKKW21] Pascale Gourdeau, Varun Kanade, Marta Kwiatkowska, and James Worrell. On the hardness of robust classification. _The Journal of Machine Learning Research_, 22(1):12521-12549, 2021.

* [GPSW17] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In _International Conference on Machine Learning_, pages 1321-1330. PMLR, 2017.
* [GSS15] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. _International Conference on Learning Representations_, 2015.
* [Han07] Steve Hanneke. A bound on the label complexity of agnostic active learning. In _Proceedings of the 24th International Conference on Machine Learning_, pages 353-360, 2007.
* [HGB\({}^{+}\)06] Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Scholkopf, and Alex Smola. Correcting sample selection bias by unlabeled data. _Advances in Neural Information Processing Systems_, 19, 2006.
* [HK19] Steve Hanneke and Samory Kpotufe. On the value of target data in transfer learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* [HKLM20] Max Hopkins, Daniel Kane, Shachar Lovett, and Gaurav Mahajan. Noise-tolerant, reliable active classification with comparison queries. In _Conference on Learning Theory_, pages 1957-2006. PMLR, 2020.
* [JSH20] Adel Javanmard, Mahdi Soltanolkotabi, and Hamed Hassani. Precise tradeoffs in adversarial training for linear regression. In _Conference on Learning Theory_, pages 2034-2078. PMLR, 2020.
* [KBDG04] Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In _VLDB_, volume 4, pages 180-191. Toronto, Canada, 2004.
* [KKMS08] Adam Tauman Kalai, Adam R Klivans, Yishay Mansour, and Rocco A Servedio. Agnostically learning halfspaces. _SIAM Journal on Computing_, 37(6):1777-1805, 2008.
* [KV94] Michael J Kearns and Umesh Vazirani. _An introduction to computational learning theory_. MIT press, 1994.
* [LAG\({}^{+}\)19] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to adversarial examples with differential privacy. In _2019 IEEE Symposium on Security and Privacy_, pages 656-672. IEEE, 2019.
* [LCWC19] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with additive noise. _Advances in Neural Information Processing Systems_, 32, 2019.
* [LPB17] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. _Advances in Neural Information Processing Systems_, 30, 2017.
* [LV07] Laszlo Lovasz and Santosh Vempala. The geometry of logconcave functions and sampling algorithms. _Random Structures & Algorithms_, 30(3):307-358, 2007.
* [LWS18] Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box predictors. In _International Conference on Machine Learning_, pages 3122-3130. PMLR, 2018.
* [MHS19] Omar Montasser, Steve Hanneke, and Nathan Srebro. VC classes are adversarially robustly learnable, but only improperly. In _Conference on Learning Theory_, pages 2512-2530. PMLR, 2019.
* [MHS20] Omar Montasser, Steve Hanneke, and Nati Srebro. Reducing adversarially robust learning to non-robust pac learning. _Advances in Neural Information Processing Systems_, 33:14626-14637, 2020.

* [MHS21] Omar Montasser, Steve Hanneke, and Nathan Srebro. Adversarially robust learning with unknown perturbation sets. In _Conference on Learning Theory_, pages 3452-3482. PMLR, 2021.
* [MHS22] Omar Montasser, Steve Hanneke, and Nathan Srebro. Adversarially robust learning: A generic minimax optimal learner and characterization. _Advances in Neural Information Processing Systems_, 2022.
* [MIG\({}^{+}\)19] Wesley J Maddox, Pavel Izmailov, Timur Garipov, Dmitry P Vetrov, and Andrew Gordon Wilson. A simple baseline for bayesian uncertainty in deep learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* [Mit82] Tom M Mitchell. Generalization as search. _Artificial Intelligence_, 18(2):203-226, 1982.
* [MMR08] Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation with multiple sources. _Advances in Neural Information Processing Systems_, 21, 2008.
* [MMS\({}^{+}\)18] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.
* [MTR\({}^{+}\)21] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization. In _International Conference on Machine Learning_, pages 7721-7735. PMLR, 2021.
* [ND16] Hongseok Namkoong and John C Duchi. Stochastic gradient methods for distributionally robust optimization with f-divergences. _Advances in Neural Information Processing Systems_, 29, 2016.
* [PY10] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. _IEEE Transactions on knowledge and data engineering_, 22(10):1345-1359, 2010.
* [PZ21] Nikita Puchkin and Nikita Zhivotovskiy. Exponential savings in agnostic active learning through abstention. In _Conference on Learning Theory_, pages 3806-3832. PMLR, 2021.
* [QCSSL08] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D Lawrence. _Dataset shift in machine learning_. MIT Press, 2008.
* [RRR21] Elan Rosenfeld, Pradeep Kumar Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. In _International Conference on Learning Representations_, 2021.
* [RRSS19] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In _International Conference on Machine Learning_, pages 5389-5400. PMLR, 2019.
* [RS88] Ronald L Rivest and Robert H Sloan. Learning complicated concepts reliably and usefully. In _Association for the Advancement of Artificial Intelligence_, pages 635-640, 1988.
* [RWK20] Leslie Rice, Eric Wong, and Zico Kolter. Overfitting in adversarially robust deep learning. In _International Conference on Machine Learning_, pages 8093-8104. PMLR, 2020.
* [RXY\({}^{+}\)20] Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John C Duchi, and Percy Liang. Understanding and mitigating the tradeoff between robustness and accuracy. In _Proceedings of the 37th International Conference on Machine Learning_, pages 7909-7919, 2020.
* [Shi00] Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-likelihood function. _Journal of Statistical Planning and Inference_, 90(2):227-244, 2000.

* [SIE\({}^{+}\)20] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry. Do adversarially robust imagenet models transfer better? _Advances in Neural Information Processing Systems_, 33:3533-3545, 2020.
* [SKHL20] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks. In _International Conference on Learning Representations_, 2020.
* [SKL17] Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. _Advances in Neural Information Processing Systems_, 30, 2017.
* [SNG\({}^{+}\)19] Ali Shafahi, Mahyar Najibi, Mohammad Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S Davis, Gavin Taylor, and Tom Goldstein. Adversarial training for free! _Advances in Neural Information Processing Systems_, 32, 2019.
* [SS16] Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In _Computer Vision-ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, 2016, Proceedings, Part III 14_, pages 443-450. Springer, 2016.
* [SST\({}^{+}\)18] Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust generalization requires more data. _Advances in Neural Information Processing Systems_, 31, 2018.
* [SSZ\({}^{+}\)20] Ali Shafahi, Parsa Saadatpanah, Chen Zhu, Amin Ghiasi, Christoph Studer, David Jacobs, and Tom Goldstein. Adversarially robust transfer learning. In _International Conference on Learning Representations_, 2020.
* [SZS\({}^{+}\)14] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In _International Conference on Learning Representations_, 2014.
* [TKP\({}^{+}\)18] Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. In _International Conference on Learning Representations_, 2018.
* [TS10] Lisa Torrey and Jude Shavlik. Transfer learning. In _Handbook of research on machine learning applications and trends: algorithms, methods, and techniques_, pages 242-264. IGI global, 2010.
* [TSE\({}^{+}\)19] Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Robustness may be at odds with accuracy. In _International Conference on Learning Representations_, 2019.
* [Vap98] Vladimir N Vapnik. _Statistical learning theory_. Wiley-Interscience, 1998.
* [vdVW96] Aad W van der Vaart and Jon A Wellner. _Weak convergence_. Springer, 1996.
* [Wan11] Liwei Wang. Smoothness, disagreement coefficient, and the label complexity of agnostic active learning. _Journal of Machine Learning Research_, 12(7), 2011.
* [WLF22] Wenxiao Wang, Alexander J Levine, and Soheil Feizi. Improved certified defenses against data poisoning with (deterministic) finite aggregation. In _International Conference on Machine Learning_, pages 22769-22783. PMLR, 2022.
* [WR06] Christopher KI Williams and Carl Edward Rasmussen. _Gaussian processes for machine learning_. MIT press Cambridge, MA, 2006.
* [YCJ16] Songbai Yan, Kamalika Chaudhuri, and Tara Javidi. Active learning from imperfect labelers. _Advances in Neural Information Processing Systems_, 29, 2016.
* [ZC21] Hanrui Zhang and Vincent Conitzer. Incentive-aware pac learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 5797-5804, 2021.

* [ZL19] Yuchen Zhang and Percy Liang. Defending against whitebox adversarial attacks via randomized discretization. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 684-693. PMLR, 2019.
* [ZLLJ19] Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm for domain adaptation. In _International Conference on Machine Learning_, pages 7404-7413. PMLR, 2019.
* [ZLWJ20] Yuchen Zhang, Mingsheng Long, Jianmin Wang, and Michael I Jordan. On localized discrepancy for domain adaptation. _arXiv preprint arXiv:2008.06242_, 2020.
* [ZN22] Yinglun Zhu and Robert D Nowak. Efficient active learning with abstention. In _Advances in Neural Information Processing Systems_, 2022.
* [ZYJ\({}^{+}\)19] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In _International Conference on Machine Learning_, pages 7472-7482. PMLR, 2019.