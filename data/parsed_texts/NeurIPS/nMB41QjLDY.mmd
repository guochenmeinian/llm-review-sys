# Provably Efficient Algorithm for Nonstationary Low-Rank MDPs

 Yuan Cheng

National University of Singapore

yuan.cheng@u.nus.edu

&Jing Yang

The Pennsylvania State University

yangjing@psu.edu

&Yingbin Liang

The Ohio State University

liang.889@osu.edu

###### Abstract

Reinforcement learning (RL) under changing environment models many real-world applications via nonstationary Markov Decision Processes (MDPs), and hence gains considerable interest. However, theoretical studies on nonstationary MDPs in the literature have mainly focused on tabular and linear (mixture) MDPs, which do not capture the nature of unknown representation in deep RL. In this paper, we make the first effort to investigate nonstationary RL under episodic low-rank MDPs, where both transition kernels and rewards may vary over time, and the low-rank model contains unknown representation in addition to the linear state embedding function. We first propose a parameter-dependent policy optimization algorithm called PORTAL, and further improve PORTAL to its parameter-free version of Ada-PORTAL, which is able to tune its hyper-parameters adaptively without any prior knowledge of nonstationarity. For both algorithms, we provide upper bounds on the average dynamic suboptimality gap, which show that as long as the nonstationarity is not significantly large, PORTAL and Ada-PORTAL are sample-efficient and can achieve arbitrarily small average dynamic suboptimality gap with polynomial sample complexity.

## 1 Introduction

Reinforcement learning (RL) has gained significant success in real-world applications such as board games of Go and chess (Silver et al., 2016, 2017, 2018), robotics (Levine et al., 2016; Gu et al., 2017), recommendation systems (Zhao et al., 2021) and autonomous driving (Bojarski et al., 2016; Ma et al., 2021). Most theoretical studies on RL have been focused on a stationary environment and evaluated the performance of an algorithm by comparing against only one best fixed policy (i.e., _static regret_). However, in practice, the environment is typically time-varying and _nonstationary_. As a result, the transition dynamics, rewards and consequently the optimal policy change over time.

There has been a line of research studies that investigated _nonstationary_ RL. Specifically, Gajane et al. (2018); Cheung et al. (2020); Mao et al. (2021) studied nonstationary tabular MDPs. To further overcome the curse of dimensionality, Fei et al. (2020); Zhou et al. (2020) proposed algorithms for nonstationary linear (mixture) MDPs and established upper bounds on the dynamic regret.

In this paper, we significantly advance this line of research by investigating nonstationary RL under _low-rank_ MDPs (Agarwal et al., 2020), where the transition kernel of each MDP admits a decomposition into a representation function and a state-embedding function that map to low dimensional spaces. Compared with linear MDPs where the representation is known, the low-rank MDP model contains unknown representation, and is hence much more powerful to capturerepresentation learning that occurs often in deep RL. Although there have been several recent studies on static low-rank MDPs (Agarwal et al., 2020; Uehara et al., 2022; Modi et al., 2021), nonstationary low-rank MDPs remain unexplored, and are the focus of this paper.

To investigate nonstationary low-rank MDPs, several challenges arise. (a) All previous studies of nonstationary MDPs took on-policy exploration, such a strategy will have difficulty in providing sufficiently accurate model (as well as representation) learning for nonstationary low-rank MDPs. (b) Under low-rank MDPs, since both representation and state-embedding function change over time, it is more challenging to use history data collected under previous transition kernels for current use.

The **main contribution** of this paper lies in addressing above challenges and designing a provably efficient algorithm for nonstationary low-rank MDPs. We summarize our contributions as follows.

* We propose a novel policy optimization algorithm with representation learning called PORTAL for nonstationary low-rank MDPs. PORTAL features new components, including off-policy exploration, data-transfer model learning, and target policy update with periodic restart.
* We theoretically characterize the average dynamic suboptimality gap (\(\mathrm{Gap_{Ave}}\)) of PORTAL, where \(\mathrm{Gap_{Ave}}\) serves as a new metric that captures the performance of target policies with respect to the best policies at each instance in the nonstationary MDPs under off-policy exploration. We further show that with prior knowledge on the degree of nonstationarity, PORTAL can select hyper-parameters that minimize \(\mathrm{Gap_{Ave}}\). If the nonstationarity is not significantly large, PORTAL enjoys a diminishing \(\mathrm{Gap_{Ave}}\) with respect to the number of iterations \(K\), indicating that PORTAL can achieve arbitrarily small \(\mathrm{Gap_{Ave}}\) with polynomial sample complexity. Our analysis features a few new developments. (a) We provide a new MLE guarantee under nonstationary transition kernels that captures errors of using history data collected under different transition kernels for benefiting current model estimation. (b) We establish trajectory-wise uncertainty bound for estimation errors via a square-root \(\ell_{\infty}\)-norm of variation budgets. (c) We develop an error tracking technique via auxiliary anchor representation for convergence analysis.
* Finally, we improve PORTAL to a _parameter-free_ algorithm called Ada-PORTAL, which does not require prior knowledge on nonstationarity and is able to tune the hyper-parameters adaptively. We further characterize \(\mathrm{Gap_{Ave}}\) of Ada-PORTAL as \(\tilde{O}(K^{-\frac{1}{6}}(\Delta+1)^{\frac{1}{6}})\), where \(\Delta\) captures the variation of the environment. Notably, based on PORTAL, we can also use the black-box method called MASTER in Wei and Luo (2021) to turn PORTAL into a parameter-free algorithm (called MASTER+PORTAL) with \(\mathrm{Gap_{Ave}}\) of \(\tilde{O}(K^{-\frac{1}{6}}\Delta^{\frac{1}{6}})\). Clearly, Ada-PORTAL performs better than MASTER+PORTAL when nonstationarity is not significantly small, i.e. \(\Delta\geq\tilde{O}(1)\).

To our best knowledge, this is the first study of nonstationary RL under low-rank MDPs.

## 2 Related Works

Various works have studied nonstationary RL under tabular and linear MDPs, most of which can be divided into two lines: policy optimization methods and value-based methods.

**Nonstationary RL: Policy Optimization Methods.** As a vast body of existing literature (Cai et al., 2020; Shani et al., 2020; Agarwal et al., 2020; Xu et al., 2021) has proposed policy optimization methods attaining computational efficiency and sample efficiency simultaneously in _stationary_ RL under various scenarios, only several papers investigated policy optimization algorithm in _nonstationary_ environment. Assuming time-varying rewards and time-invariant transition kernels, Fei et al. (2020) studied nonstationary RL under tabular MDPs. Zhong et al. (2021) assumed both transition kernels and rewards change over episodes and studied nonstationary linear mixture MDPs. These policy optimization methods all assumed prior knowledge on nonstationarity.

**Nonstationary RL: Value-based Methods.** Assuming that both transition kernels and rewards are time-varying, several works have studied nonstationary RL under tabular and linear MDPs, most of which adopted Upper Confidence Bound (UCB) based algrithms. Cheung et al. (2020) investigated tabular MDPs with infinite-horizon and proposed algorithms with both known variation budgets and unknown variation budgets. In addition, this work also proposed a Bandit-over-Reinforcement Learning (BORL) technique to deal with unknown variation budgets. Mao et al. (2021) proposed a model-free algorithm with sublinear dynamic regret bound. They then proved a lower bound for nonstationary tabular MDPs and showed that their regret bound is near min-max optimal. Touati& Vincent (2020); Zhou et al. (2020) considered nonstationary RL in linear MDPs and proposed algorithms achieving sublinear regret bounds with unknown variation budgets.

Besides these two lines of researches, Wei & Luo (2021) proposed a black-box method that turns a RL algorithm with optimal regret in a (near-)stationary environment into another algorithm that can work in a nonstationary environment with sublinear dynamic regret without prior knowledge on nonstationarity. In this paper, we show that our algorithm Ada-PORTAL outperforms such a type of black-box method (taking PORTAL as subroutine) if nonstationarity is not significantly small.

**Stationary RL under Low-rank MDPs.** Low-rank MDPs were first studied by Agarwal et al. (2020b) in a reward-free regime, and then Uehara et al. (2022) studied low-rank MDPs for both online and offline RL with known rewards. Cheng et al. (2023) studied reward-free RL under low-rank MDPs and improved the sample complexity of previous works. Modi et al. (2021) proposed a model-free algorithm MOFFLE under low-nonnegative-rank MDPs. Cheng et al. (2022); Agarwal et al. (2022) studied multitask representation learning under low-rank MDPs, and further showed the benefit of representation learning to downstream RL tasks.

## 3 Formulation

**Notations:** We use \([K]\) to denote set \(\{1,\ldots,K\}\) for any \(K\in\mathbb{N}\), use \(\left\|x\right\|_{2}\) to denote the \(\ell_{2}\) norm of vector \(x\), use \(\triangle(\mathcal{A})\) to denote the probability simplex over set \(\mathcal{A}\), use \(\mathcal{U}(\mathcal{A})\) to denote uniform sampling over \(\mathcal{A}\), given \(|\mathcal{A}|<\infty\), and use \(\triangle(\mathcal{S})\) to denote the set of all possible density distributions over set \(\mathcal{S}\). Furthermore, for any symmetric positive definite matrix \(\Sigma\), we let \(\left\|x\right\|_{\Sigma}:=\sqrt{x^{\top}\Sigma x}\). For distributions \(p_{1}\) and \(p_{2}\), we use \(D_{KL}(p_{1}(\cdot)\|p_{2}(\cdot))\) to denote the KL divergence between \(p_{1}\) and \(p_{2}\).

### Episodic MDPs and Low-rank Approximation

An episodic MDP is denoted by a tuple \(\mathcal{M}:=\big{(}\mathcal{S},\mathcal{A},H,P:=\{P_{h}\}_{h=1}^{H},r:=\{r_{h }\}_{h=1}^{H}\big{)}\), where \(\mathcal{S}\) is a possibly infinite state space, \(\mathcal{A}\) is a finite action space with cardinality \(A\), \(H\) is the time horizon of each episode, \(P_{h}(\cdot|\cdot,\cdot):\mathcal{S}\times\mathcal{A}\rightarrow\Delta( \mathcal{S})\) denotes the transition kernel at each step \(h\), and \(r_{h}(\cdot,\cdot):\mathcal{S}\times\mathcal{A}\rightarrow[0,1]\) denotes the deterministic reward function at each step \(h\). We further normalize the reward as \(\sum_{h=1}^{H}r_{h}\leq 1\). A policy \(\pi=\{\pi_{h}\}_{h\in[H]}\) is a set of mappings where \(\pi_{h}:\mathcal{S}\rightarrow\Delta(\mathcal{A})\). For any \((s,a)\in\mathcal{S}\times\mathcal{A}\), \(\pi_{h}(a|s)\) denotes the probability of selecting action \(a\) at state \(s\) at step \(h\). For any \((s,a)\in\mathcal{S}\times\mathcal{A}\), let \((s_{h},a_{h})\sim(P,\pi)\) denote that the state \(s_{h}\) is sampled by executing policy \(\pi\) to step \(h\) under transition kernel \(P\) and then action \(a_{h}\) is sampled by \(\pi_{h}(\cdot|s_{h})\).

Given any state \(s\in\mathcal{S}\), the value function for a policy \(\pi\) at step \(h\) under an MDP \(\mathcal{M}\) is defined as the expected value of the accumulative rewards as: \(V_{h,P,r}^{\pi}(s)=\sum_{h^{\prime}=h}^{H}\mathbb{E}_{(s_{h^{\prime}},a_{h^{ \prime}})\sim(P,\pi)}\left[r_{h^{\prime}}(s_{h^{\prime}},a_{h^{\prime}})|s_{h}=s\right]\). Similarly, given any state-action pair \((s,a)\in\mathcal{S}\times\mathcal{A}\), the action-value function (\(Q\)-function) for a policy \(\pi\) at step \(h\) under an MDP \(\mathcal{M}\) is defined as \(Q_{h,P,r}^{\pi}(s,a)=r_{h}(s,a)+\sum_{h^{\prime}=h+1}^{H}\mathbb{E}_{(s_{h^{ \prime}},a_{h^{\prime}})\sim(P,\pi)}\left[r_{h^{\prime}}(s_{h^{\prime}},a_{h^{ \prime}})|s_{h}=s,a_{h}=a\right]\). Denote \((P_{h}f)(s,a):=\mathbb{E}_{s^{\prime}\sim P_{h}(\cdot|s,a)}[f(s^{\prime})]\) for any function \(f:\mathcal{S}\rightarrow\mathbb{R}\). Then we can write the action-value function as \(Q_{h,P,r}^{\pi}(s,a)=r_{h}(s,a)+(P_{h}V_{h+1,P,r}^{\pi})(s,a)\). For any \(k\in[K]\), without loss of generality, we assume the initial state \(s_{1}\) to be fixed and identical, and we use \(V_{P,r}^{\pi}\) to denote \(V_{1,P,r}^{\pi}(s_{1})\) for simplicity.

This paper focuses on low-rank MDPs (Jiang et al., 2017; Agarwal et al., 2020b) defined as follows.

**Definition 3.1** (Low-rank MDPs).: A transition kernel \(P_{h}^{*}:\mathcal{S}\times\mathcal{A}\rightarrow\triangle(\mathcal{S})\) admits a low-rank decomposition with dimension \(d\in\mathbb{N}\) if there exist a representation function \(\phi_{h}^{*}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\) and a state-embedding function \(\mu_{h}^{*}:\mathcal{S}\rightarrow\mathbb{R}^{d}\) such that

\[P_{h}^{*}(s^{\prime}|s,a)=\left\langle\phi_{h}^{*}(s,a),\mu_{h}^{*}(s^{\prime} )\right\rangle,\quad\forall s,s^{\prime}\in\mathcal{S},a\in\mathcal{A}.\]

Without loss of generality, we assume \(\left\|\phi_{h}^{*}(s,a)\right\|_{2}\leq 1\) for all \((s,a)\in\mathcal{S}\times\mathcal{A}\) and for any function \(g:\mathcal{S}\mapsto[0,1]\), \(\left\|f\left\|\,\mu_{h}^{*}(s)g(s)ds\right\|_{2}\leq\sqrt{d}\). An MDP is a low-rank MDP with dimension \(d\) if for any \(h\in[H]\), its transition kernel \(P_{h}^{*}\) admits a low-rank decomposition with dimension \(d\). Let \(\phi^{*}=\{\phi_{h}^{*}\}_{h\in[H]}\) and \(\mu^{*}=\{\mu_{h}^{*}\}_{h\in[H]}\) be the true representation and state-embedding functions.

### Nonstationary Transition Kernels with Adversarial Rewards

In this paper, we consider an episodic RL setting under changing environment, where both transition kernels and rewards vary over time and possibly in an adversarial fashion.

Specifically, suppose the RL system goes by _rounds_, where each round have a fixed number of episodes, and the transition kernel and the reward remain the same in each round, and can change adversarially across rounds. For each round, say round \(k\), we denote the MDP as \(\mathcal{M}^{k}=(\mathcal{S},\mathcal{A},H,P^{k}:=\{P^{\star,k}_{h}\}_{h=1}^{H},r^{k}:=\{r^{k}_{h}\}_{h=1}^{H})\), where \(P^{\star,k}\) and \(r^{k}\) denote the true transition kernel and the reward of round \(k\). Further, \(P^{\star,k}\) takes the low-rank decomposition as \(P^{\star,k}=\langle\phi^{\star,k},\mu^{\star,k}\rangle\). Both the representation function \(\phi^{\star,k}\) and the state embedding function \(\mu^{\star,k}\) can change across rounds. Given the reward function \(r^{k}\), there always exists an optimal policy \(\pi^{\star,k}\) that yields the optimal value function \(V^{\pi^{\star,k}}_{P^{\star,k},r^{k}}=\sup_{\pi}V^{\pi}_{P^{\star,k},r^{k}}\), abbreviated as \(V^{\star}_{P^{\star,k},r^{k}}\). Clearly, the optimal policy also changes across rounds.

We assume the agent interacts with the nonstationary environment (i.e., the time-varying MDPs) over \(K\) rounds in total without the knowledge of transition kernels \(\{P^{k,\star}\}_{k=1}^{K}\). At the beginning of each round \(k\), the environment changes to a possibly adversarial transition kernel unknown to the agent, picks a reward function \(r^{k}\), which is revealed to the agent only at the end of round \(k\), and outputs a fixed initial state \(s_{1}\) for the agent to start the exploration of the environment for each episode. The agent is allowed to interact with MDPs via a few episodes with one or multiple _exploration_ policies at her choice to take samples from the environment and then should output an target policy to be executed during the next round. Note that in our setting, the agent needs to decide exploration and target policies only based on the information in previous rounds, and hence exploration samples and the reward information of the current round help only towards future rounds.

### Learning Goal and Evaluation Metric

In our setting, the agent seeks to find the optimal policy at each round \(k\) (with only the information of previous rounds), where both transition kernels and rewards can change over rounds. Hence we define the following notion of _average dynamic suboptimality gap_ to measure the convergence of the target policy series to the optimal policy series.

**Definition 3.2** (Average Dynamic Suboptimality Gap).: For \(K\) rounds, and any policy set \(\{\pi^{k}\}_{k\in[K]}\), the average dynamic suboptimality gap \((\mathrm{Gap_{Ave}})\) of the value functions over \(K\) rounds is given as \(\mathrm{Gap_{Ave}}(K)=\frac{1}{K}\sum_{k=1}^{K}[V^{\pi}_{P^{\star,k},r^{k}}-V^{ \pi^{k}}_{P^{\star,k},r^{k}}]\). For any \(\epsilon\), we say an algorithm is \(\epsilon\)-average suboptimal, if it outputs a policy set \(\{\pi^{k}\}_{k\in[K]}\) satisfying \(\mathrm{Gap_{Ave}}(K)\leq\epsilon\).

\(\mathrm{Gap_{Ave}}\) compares the agent's target policy to the optimal policy of each individual round in hindsight, which captures the dynamic nature of the environment. This is in stark contrast to the stationary setting where the comparison policy is a single fixed best policy over all rounds. This notion is similar to _dynamic regret_ used for nonstationary RL (Fei et al., 2020; Gajane et al., 2018), where the only difference is that \(\mathrm{Gap_{Ave}}\) evaluates the performance of target policies rather than the exploration policies. Hence, given any target accuracy \(\epsilon\geq 0\), the agent is further interested in the statistical efficiency of the algorithm, i.e., using as few trajectories as possible to achieve \(\epsilon\)-average suboptimal.

## 4 Policy Optimization Algorithm and Theoretical Guarantee

### Base Algorithm: PORTAL

We propose a novel algorithm called PORTAL (Algorithm 1), which features three main steps. Below we first summarize our main design ideas and then explain reasons behind these ideas as we further elaborate main steps of PORTAL.

**Summary of New Design Ideas:** PORTAL features the following main design ideas beyond previous studies on nonstationary RL under tabular and linear MDPs. (a) PORTAL features a specially designed _off-policy_ exploration which turns out to be beneficial for nonstationary low-rank /MDP models rather than the typical _on-policy_ exploration taken by previous studies of nonstationary tabular and linear MDP models. (b) PORTAL transfers history data collected under various different transition kernels for benefiting the estimation of the current model. (c) PORTAL updates targetpolicies with periodic restart. As a comparison, previous work using periodic restart (Fei et al., 2020) chooses the restart period \(\tau\) based on a certain smooth visitation assumption. Here, we remove such an assumption and hence our choice of \(\tau\) is applicable to more general model classes.

**Step 1. Off-Policy Exploration for Data Collection:** We take _off-policy_ exploration, which is beneficial for nonstationary low-rank MDPs than simply using the target policy for _on-policy_ exploration taken by the previous studies on nonstationary tabular or linear (mixture) MDPs (Zhong et al., 2021; Fei et al., 2020; Zhou et al., 2020). To further explain, we first note that under tabular or linear (mixture) MDPs studied in the previous work, a bonus term is introduced to the actual reward to serve as a _point-wise_ uncertainty level of the estimation error for each state-action pair at any step \(h\), so that for any step \(h\), \(\hat{Q}_{h}^{k}\) is a good optimistic estimation for \(Q_{h,P^{*,k},r^{k}}^{\pi}\). Hence it suffices to collect samples using the target policy. However, in low-rank MDPs, the bonus term \(\hat{b}_{h}^{k}\) cannot serve as a point-wise uncertainty measure. For step \(h\geq 2\), \(\hat{Q}_{h}^{k}\) is not a good optimistic estimation for the true value function if the agent only uses target policy to collect data (i.e., for on-policy exploration). Hence, more samples and a novel off-policy exploration are required for a good estimation under low-rank MDPs. Specifically, as line 5 in Algorithm 1, at the beginning of each round \(k\), for each step \(h\in[H]\), the agent explores the environment by executing the exploration policy \(\tilde{\pi}^{k-1}\) to state \(\tilde{\tilde{s}}_{h-1}^{k,h}\) and then taking two uniformly chosen actions1, where \(\tilde{\pi}^{k-1}\) is determined in Step 2 of the previous round.

Footnote 1: The subscript \(h-1\) in \(\tilde{s}_{h-1}^{k,h}\) indicates that the data is collected at time step \(h\) of each trajectory, and the superscript \((k,h)\) indicates in which loop the data is collected (as in line 3 and 4 of Algorithm 1)

```
1:Input: Rounds \(K\), hyper-parameters \(\tau,W\), regularizer \(\lambda_{k,W}\), coefficient \(\tilde{\alpha}_{k,W}\), stepsize \(\eta\) and models \(\{\Psi,\Phi\}\).
2:Initialization:\(\pi_{0}(\cdot|s)\) to be uniform; \(\tilde{\mathcal{D}}_{h}^{(0,0)}=\emptyset\).
3:for episode \(k=1,\ldots,K\)do
4:for step \(h=1,\ldots,H\)do
5: Roll into \(\tilde{s}_{h-1}^{(k,h)}\) using \(\tilde{\pi}^{k-1}\), uniformly choose \(\tilde{a}_{h-1}^{(k,h)},\tilde{a}_{h}^{(k,h)}\), and enter into \(\tilde{s}_{h}^{(k,h)}\), \(\tilde{s}_{h+1}^{(k,h)}\).
6: Update datasets \[\tilde{\mathcal{D}}_{h-1}^{(k,h,W)} =\left\{\tilde{s}_{h-1}^{(i,h)},\tilde{a}_{h-1}^{(i,h)},\tilde{s} _{h}^{(i,h)}\right\}_{i=1\lor k-W+1}^{k},\] \[\tilde{\mathcal{D}}_{h}^{(k,h,W)} =\left\{\tilde{s}_{h}^{(i,h)},\tilde{a}_{h}^{(i,h)},\tilde{s}_{h+ 1}^{(i,h)}\right\}_{i=1\lor k-W+1}^{k}.\]
7:endfor
8: Receive full information rewards \(r^{k}=\{r_{h}^{k}\}_{h\in[H]}\).
9: Estimate transition kernel and update the exploration policy \(\tilde{\pi}^{k}\) for the next round via: \[\mathrm{E}^{2}\mathrm{U}\left(k,\{\tilde{\mathcal{D}}_{h-1}^{(k,h,W)}\},\{ \tilde{\mathcal{D}}_{h}^{(k,h,W)}\}\right).\]
10:for step \(h=1,\ldots,H\)do
11: Update \(\hat{Q}_{h}^{k}=Q_{h,\tilde{P}^{k},r^{k}}^{\pi^{k}}\).
12:endfor
13:if\(k\mod\tau=1\)then
14: Set \(\{\hat{Q}_{h}^{k}\}_{h\in[H]}\) as zero functions and \(\{\pi_{h}^{k}\}_{h\in[H]}\) as uniform distributions on \(\mathcal{A}\).
15:endif
16:for step \(h=1,\ldots,H\)do
17: Update the target policy as in Equation (2).
18:endfor
19:endfor
20:Output:\(\{\pi^{k}\}_{k=1}^{K}\). ```

**Algorithm 1** **PORTA** (Policy **O**ptimization with **R**epresen**TA**tion **L**earning under nonstationary MDPs)

**Step 2. Data-Transfer Model Learning and E\({}^{2}\)U:** In this step, we transfer history data collected under previous different transition kernels for benefiting the estimation of the current model. This is theoretically grounded by our result that the model estimation error can be decomposed into variation budgets plus a diminishing term as the estimation sample size increases, which justifies that the usage of data generated by mismatched distributions within a certain window is beneficial for model learning as long as variation budgets is mild. Then, the estimated model will further facilitate the selection of future exploration policies accurately.

Specifically, the agent selects desirable samples only from the latest \(W\) rounds following a _forgetting rule_(Garivier and Moulines, 2011). Since nonstationary low-rank MDPs (compared to tabular and linear MDPs) also have additional variations on representations over time, the choice of \(W\) needs to incorporate such additional information. Then the agent passes these selected samples to a subroutine \(\text{E}^{2}\text{U}\) (see Algorithm 2), in which the agent estimates the transition kernels via the maximum likelihood estimation (MLE). Next, the agent updates the empirical covariance matrix \(\hat{U}^{k,W}\) and exploration-driven bonus \(\hat{b}^{k}\) as in lines 4 and 5 in Algorithm 2. We then define a _truncated value function_ iteratively using the estimated transition kernel and the exploration-driven reward as follows:

\[\hat{Q}^{\pi}_{h,\hat{P}^{k},\hat{b}^{k}}(s_{h},a_{h})=\min\left\{ 1,\hat{b}^{k}_{h}(s_{h},a_{h})+\hat{P}^{k}_{h}\hat{V}^{\pi}_{h+1,\hat{P}^{k}, \hat{b}^{k}}(s_{h},a_{h})\right\},\] \[\hat{V}^{\pi}_{h,\hat{P}^{k},\hat{b}^{k}}(s_{h})=\mathbb{E}_{\pi} \left[\hat{Q}^{\pi}_{h,\hat{P}^{k},\hat{b}^{k}}(s_{h},a_{h})\right].\] (1)

Although the bonus term \(\hat{b}^{k}_{h}\) cannot serve as a _point-wise_ uncertainty measure, the truncated value function \(\hat{V}^{\pi}_{\hat{P}^{k},\hat{b}^{k}}\) as the cumulative version of \(\hat{b}^{k}_{h}\) can serve as a _trajectory-wise_ uncertainty measure, which can be used to determine future exploration policies. Intuitively, for any policy \(\pi\), the model estimation error satisfies \(\mathbb{E}_{(s,a)\sim(P^{\star,k},\pi)}[\|\hat{P}^{k}(\cdot|s,a)-P^{\star,k}( \cdot|s,a)\|_{TV}]\leq\hat{V}^{\pi}_{\hat{P}^{k},\hat{b}^{k}}+\Delta\), where the error term \(\Delta\) captures the variation of both transition kernels and representations over time. As a result, by selecting the policy that maximizes \(\hat{V}^{\pi}_{\hat{P}^{k},\hat{b}^{k}}\) as the exploration policy as in line 7, the agent will explore the trajectories whose states and actions have not been estimated sufficiently well so far.

```
1:Input: round index \(k\), regularizer \(\lambda_{k,W}\) and coefficient \(\tilde{\alpha}_{k,W}\), datasets \(\{\tilde{\mathcal{D}}^{(k,h)}_{h-1}\}\),\(\{\tilde{\mathcal{D}}^{(k,h)}_{h}\}\) and models \(\{\Psi,\Phi\}\).
2:for step \(h=1,\ldots,H\)do
3: Learn the representation via MLE for step \(h\): \[\hat{P}^{k}_{h}=(\hat{\phi}^{k}_{h},\hat{\mu}^{k}_{h})=\arg\max_{\phi\in\Phi, \mu\in\Psi}\mathbb{E}_{\hat{\mathcal{D}}^{(k,h)}_{h}}\left[\log\langle\phi(s_{ h},a_{h}),\mu(s_{h+1})\rangle\right].\]
4: Compute the empirical covariance matrix as \[\hat{U}^{k,W}_{h}=\sum_{\hat{\mathcal{D}}^{(k,h+1)}_{h}}\hat{\phi}^{k}_{h}(s_{ h},a_{h})\hat{\phi}^{k}_{h}(s_{h},a_{h})^{\top}+\lambda_{k,W}I\]
5: Define exploration-driven bonus \(\hat{b}^{k}_{h}(\cdot,\cdot)=\min\{\alpha_{k,W}\|\hat{\phi}^{k}_{h}(\cdot, \cdot)\|_{(\hat{U}^{k,W}_{h})^{-1}},1\}\).
6:endfor
7: Find exploration policy \(\bar{\pi}^{k}=\arg\max_{\pi}\hat{V}^{\pi}_{\hat{P}^{k},\hat{b}^{k}}\), where \(\hat{V}^{\pi}_{\hat{P}^{k},\hat{b}^{k}}\) is defined as in Equation (1).
8:Output: Model \(\hat{P}^{k}\) and exploration policy \(\{\tilde{\pi}^{k}\}\). ```

**Algorithm 2****\(\text{E}^{2}\text{U}\)** (Model **E**stimation and **E**xploration Policy **U**pdate)

**Step 3: Target Policy Update with Periodic Restart:** The agent evaluates the target policy by computing the value function under the target policy and the estimated transition kernel. Then, due to the nonstationarity, the target policy is reset every \(\tau\) rounds. Compared with the previous work using periodic restart (Fei et al., 2020), whose choice of \(\tau\) is based on a certain smooth visitation assumption, we remove such an assumption and hence our choice of \(\tau\) is applicable to more general model classes. Finally, the agent uses the estimated value function for target policy update for the next round \(k+1\) via online mirror descend. The update step is inspired by the previous works (Cai et al., 2020; Schulman et al., 2017). Specifically, for any given policy \(\pi^{0}\) and MDP \(\mathcal{M}\), define the following function w.r.t. policy \(\pi\) :

\[L^{\mathcal{M},\pi_{0}}(\pi)=V^{\pi^{0}}_{P,r}+\sum_{h=1}^{H}\mathbb{E}_{s_{h} \sim(P,\pi^{0})}\left[\left\langle Q^{\pi^{0}}_{h,P,r},\pi_{h}(\cdot|s_{h})- \pi^{0}_{h}(\cdot|s_{h})\right\rangle\right].\]\(L^{\mathcal{M},\pi_{0}}(\pi)\) can be regarded as a local linear approximation of \(V^{\pi}_{P,r}\) at "point" \(\pi^{0}\)(Schulman et al., 2017). Consider the following optimization problem:

\[\pi^{k+1}=\arg\max_{\pi}L^{\mathcal{M}^{k},\pi^{k}}(\pi)-\frac{1}{\eta}\sum_{h \in[H]}\mathbb{E}_{s_{h}\sim(P^{*,h},\pi^{k})}\left[D_{KL}(\pi_{h}(\cdot|s_{h}) \|\pi^{k}_{h}(\cdot|s_{h}))\right].\]

This can be regarded as a mirror descent step with KL divergence, where the KL divergence regularizes \(\pi\) to be close to \(\pi^{k}\). It further admits a closed-form solution: \(\pi^{k+1}_{h}(\cdot|\cdot)\propto\pi^{k}_{h}(\cdot|\cdot)\cdot\exp\{\eta\cdot Q ^{\pi^{k}}_{h,P^{*,k},\pi^{k}}(\cdot,\cdot)\}\). We use the estimated version \(\hat{Q}^{k}_{h}\) to approximate \(Q^{\pi^{k}}_{h,P^{*,k},\pi^{k}}\) and get

\[\pi^{k+1}_{h}(\cdot|\cdot)\propto\pi^{k}_{h}(\cdot|\cdot)\cdot\exp\{\eta\cdot \hat{Q}^{k}_{h}(\cdot,\cdot)\}.\] (2)

### Technical Assumptions

Our analysis adopts the following standard assumptions on low-rank MDPs.

**Assumption 4.1**.: (Realizability). A learning agent can access to a model class \(\{(\Phi,\Psi)\}\) that contains the true model, namely, for any \(h\in[H],k\in[K]\), \(\phi^{*,k}_{h}\in\Phi,\mu^{*,k}_{h}\in\Psi\).

While we assume cardinality of the model class to be finite for simplicity, extensions to infinite classes with bounded statistical complexity are not difficult (Sun et al., 2019).

**Assumption 4.2** (Bounded Density).: Any model induced by \(\Phi\) and \(\Psi\) has bounded density, i.e. \(\forall P=\langle\phi,\mu\rangle,\phi\in\Phi,\mu\in\Psi\), there exists a constant \(B\geq 0\) such that \(\max_{(s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}}P(s^{ \prime}|s,a)\leq B\).

**Assumption 4.3** (Reachability).: For each round \(k\) and step \(h\), the true transition kernel \(P^{\star,k}_{h}\) satisfies that for any \((s,a,s^{\prime})\in\mathcal{S}\times\mathcal{A}\times\mathcal{S}\), \(P^{\star,k}_{h}(s^{\prime}|s,a)\geq p_{\min}\).

**Variation Budgets:** We next introduce several measures of nonstationarity of the environment: \(\Delta^{P}=\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{(s,a)\in\mathcal{S}\times \mathcal{A}}\|P^{\star,k+1}_{h}(\cdot|s,a)-P^{\star,k}_{h}(\cdot|s,a)\|_{TV}\),\(\Delta^{\sqrt{P}}=\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{(s,a)\in\mathcal{S}\times \mathcal{A}}\|P^{\star,k+1}_{h}(\cdot|s,a)-P^{\star,k}_{h}(\cdot|s,a)\|_{TV}^{1 /2}\Delta^{\phi}=\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{(s,a)\in\mathcal{S}\times \mathcal{A}}\|\phi^{*,k+1}_{h}(s,a)-\phi^{*,k}_{h}(s,a)\|_{2}\),\(\Delta^{\pi}=\sum_{k=1}^{K}\sum_{h=1}^{H}\max_{s\in\mathcal{S}}\|\pi^{ \star,k}_{h}(\cdot|s)-\pi^{*,k-1}_{h}(\cdot|s)\|_{TV}\). These notions are known as _variation budgets_ or _path lengths_ in the literature of online convex optimization (Besbes et al., 2015; Hazan, 2016; Hall and Willett, 2015) and nonstationary RL (Fei et al., 2020; Zhong et al., 2021; Zhou et al., 2020). The regret of nonstationary RL naturally depends on these notions that capture the variations of MDP models over time.

### Theoretical Guarantee

To present our theoretical result for PORTAL, we first discuss technical challenges in our analysis and the novel tools that we develop. Generally, large nonstationarity of environment can cause significant errors to MLE, empirical covariance and exploration-driven bonus design for low-rank models. Thus, different from static low-rank MDPs (Agarwal et al., 2020; Uehara et al., 2022), we devise several new techniques in our analysis to capture the errors caused by nonstationarity which we summarize below.

1. Characterizing nonstationary MLE guarantee. We provide a theoretical ground to support our design of leveraging history data collected under various different transition kernels in previous rounds for benefiting the estimation of the current model, which is somewhat surprising. Specifically, we establish an MLE guarantee of the model estimation error, which features a separation of variation budgets from a diminishing term as the estimation sample size \(W\) increases. Such a result justifies the usage of data generated by mismatched distributions within a certain window as long as the variation budgets is mild. Such a separation cannot be shown directly. Instead, we bridge the bound of model estimation error and the expected value of the ratio of transition kernels via Hellinger distance, and the latter can be decomposed into the variation budgets and a diminishing term as the estimation sample size increases.
2. Establishing trajectory-wise uncertainty for estimation error \(\hat{V}^{\pi}_{P^{k},\hat{b}^{k}}\). To this end, straightforward combination of our nonstationary MLE guarantee with previous techniques on low-rank MDPs would yield a coefficient \(\tilde{\alpha}_{k,W}\) that depends on local variation budgets. Instead, we convert the 

[MISSING_PAGE_FAIL:8]

Specifically, Ada-PORTAL divides the entire \(K\) rounds into \(\lceil K/M\rceil\) blocks with equal length of \(M\) rounds. Then two sets \(\mathcal{J}_{W}\) and \(\mathcal{J}_{\tau}\) are specified (see later part of this section), from which the window size \(W\) and the restart period \(\tau\) for each block are drawn.

For each block \(i\in\left[\lceil\frac{K}{M}\rceil\right]\), Ada-PORTAL treats each element of \(\mathcal{J}_{W}\times\mathcal{J}_{\tau}\) as an arm and take it as a bandit problem to select the best arm for each block. In lines 4 and 5 of Algorithm 3, a master algorithm is run to update parameters and select the desired arm, i.e., the window size \(W_{i}\) and the restart period \(\tau_{i}\). Here we choose EXP3-P (Bubeck & Cesa-Bianchi, 2012) as the master algorithm and discuss the details later. Then Algorithm 1 is called with input \(W_{i}\) and \(\tau_{i}\) as a subroutine for the current block \(i\). At the end of each block, the total reward of the current block is computed by summing up all the empirical value functions of the target policy of each episode within the block, which is then used to update the parameters for the next block.

We next set the feasible sets and block length of Algorithm 1. Since optimal \(W\) and \(\tau\) in PORTAL are chosen differently from previous works (Zhou et al., 2020; Cheung et al., 2019; Touati & Vincent, 2020) on nonstationary MDPs due to the low-rank structure, the feasible set here that covers the optimal choices of \(W\) and \(\tau\) should also be set differently from those previous works using BORL.

\(M_{W}=d^{\frac{1}{3}}H^{\frac{1}{3}}K^{\frac{1}{3}}\), \(M_{\tau}=K^{\frac{2}{3}}\), \(M=d^{\frac{1}{3}}H^{\frac{1}{3}}K^{\frac{2}{3}}\). \(J_{W}=\lfloor\log(M_{W})\rfloor\), \(\mathcal{J}_{W}=\{M_{W}^{0},\lfloor M_{W}^{\frac{1}{3W}}\rfloor,\ldots,M_{W}\}\), \(J_{\tau}=\lfloor\log(M_{\tau})\rfloor\), \(\mathcal{J}_{\tau}=\{M_{\tau}^{0},\lfloor M_{\tau}^{\frac{1}{3}}\rfloor, \ldots,M_{\tau}\}\), \(J=J_{W}\cdot J_{\tau}\).

Then the parameters of EXP3-P are initialized as follows:

\[\alpha=0.95\sqrt{\frac{\ln J}{J\lceil K/M\rceil}},\quad\beta=\sqrt{\frac{\ln J }{J\lceil K/M\rceil}},\gamma=1.05\sqrt{\frac{J\ln J}{\lceil K/M\rceil}}, \quad q_{(k,l),1}=0,(k,l)\in\mathcal{J},\] (4)

where \(\mathcal{J}=\{(k,l):k\in\{0,1,\ldots,J_{W}\}\,,l\in\{0,1,\ldots,J_{\tau}\}\}\). The parameter updating rule is as follows. For any \((k,l)\in\mathcal{J},i\in\lceil K/M\rceil\),

\[u_{(k,l),i}=(1-\gamma)\frac{\exp(\alpha q_{(k,l),i})}{\sum_{(k,l)\in\mathcal{ J}}\exp(\alpha q_{(k,l),i})}+\frac{\gamma}{J},\] (5)

where \(u_{(k,l),i}\) is a probability over \(\mathcal{J}\). From \(u_{(k,l),i}\), the agent samples a desired pair \((k_{i},l_{i})\) for each block \(i\), which corresponds to the index of feasible set \(\mathcal{J}_{W}\times\mathcal{J}_{\tau}\) and is used to set \(W_{i}\) and \(\tau_{i}\).

As a last step, \(R_{i}(W_{i},\tau_{i})\) is rescaled to update \(q_{(k,l),i+1}\).

\[q_{(k,l),i+1}=q_{(k,l),i}+\frac{\beta+1_{(k,l)=(k_{i},l_{i})}R_{i}(W_{i},\tau_ {i})/M}{u_{(k,l),i}}.\] (6)

We next establish a bound on \(\mathrm{Gap_{Ave}}\) for Ada-PORTAL.

**Theorem 5.1**.: _Under the same conditions of Theorem 4.4, with probability at least \(1-\delta\), the average dynamic suboptimality gap of Ada-PORTAL in Algorithm 3 is upper-bounded as \(\mathrm{Gap_{Ave}}(K)\leq\tilde{O}(H^{\frac{11}{6}}d^{\frac{3}{6}}A^{\frac{1}{2}} (A+d^{2})^{\frac{1}{2}}K^{-\frac{1}{6}}(\Delta^{\sqrt{P}}+\Delta^{\phi}+1)^{ \frac{1}{6}}+2HK^{-\frac{1}{3}}(\Delta^{P}+\Delta^{\pi}+1)^{\frac{1}{3}})\)._

**Comparison with Wei & Luo (2021):** It is interesting to compare Ada-PORTAL with an alternative black-box type of approach to see its advantage. A black-box technique called MASTER was proposed in Wei & Luo (2021), which can also work with any base algorithm such as PORTAL to handle unknown variation budgets. Such a combined approach of MASTER+PORTAL turns out to have a worse \(\mathrm{Gap_{Ave}}\) than our Ada-PORTAL in Algorithm 3. To see this, denote \(\Delta=\Delta^{\phi}+\Delta^{\sqrt{P}}+\Delta^{\pi}\). The \(\mathrm{Gap_{Ave}}\) of MASTER+PORTAL is \(\tilde{O}(K^{-\frac{1}{6}}\Delta^{\frac{1}{3}})\). Then if variation budgets are not too small, i.e. \(\Delta\geq\tilde{O}(1)\), this \(\mathrm{Gap_{Ave}}\) is worse than Ada-PORTAL. See detailed discussion in Appendix C.2.

## 6 Conclusion

In the paper, we investigate nonstationary RL under low-rank MDPs. We first propose a notion of average dynamic suboptimality gap \(\mathrm{Gap_{Ave}}\) to evaluate the performance of a series of policies in a nonstationary environment. Then we propose a sample-efficient policy optimization algorithm PORTAL and its parameter-free version Ada-PORTAL. We further provide upper bounds on \(\mathrm{Gap_{Ave}}\) for both algorithms. As future work, it is interesting to investigate the impact of various constraints such as safety requirements in nonstationary RL under function approximations.

## 7 Acknowledgement

The work of Y. Liang was supported in part by the U.S. National Science Foundation under the grants RINGS-2148253, DMS-2134145, and ECCS-2113860. The work of J. Yang was supported by the U.S. National Science Foundation under the grants CNS-1956276 and CNS-2003131.

## References

* Agarwal et al. (2020a) Agarwal, A., Henaff, M., Kakade, S. M., and Sun, W. PC-PG: policy cover directed exploration for provable policy gradient learning. In _Advances in Neural Information Processing Systems 33_, 2020a.
* Agarwal et al. (2020b) Agarwal, A., Kakade, S. M., Krishnamurthy, A., and Sun, W. FLAMBE: structural complexity and representation learning of low rank mdps. In _Advances in Neural Information Processing Systems 33_, 2020b.
* Agarwal et al. (2022) Agarwal, A., Song, Y., Sun, W., Wang, K., Wang, M., and Zhang, X. Provable benefits of representational transfer in reinforcement learning. _arXiv preprint arXiv:2205.14571_, 2022.
* Besbes et al. (2015) Besbes, O., Gur, Y., and Zeevi, A. Non-stationary stochastic optimization. _Oper. Res._, 63(5):1227-1244, 2015.
* Bojarski et al. (2016) Bojarski, M., Del Testa, D., Dworakowski, D., Firner, B., Flepp, B., Goyal, P., Jackel, L. D., Monfort, M., Muller, U., Zhang, J., et al. End to end learning for self-driving cars. _arXiv preprint arXiv:1604.07316_, 2016.
* Bubeck & Cesa-Bianchi (2012) Bubeck, S. and Cesa-Bianchi, N. Regret analysis of stochastic and nonstochastic multi-armed bandit problems. _Found. Trends Mach. Learn._, 2012.
* Cai et al. (2020) Cai, Q., Yang, Z., Jin, C., and Wang, Z. Provably efficient exploration in policy optimization. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* Cheng et al. (2022) Cheng, Y., Feng, S., Yang, J., Zhang, H., and Liang, Y. Provable benefit of multitask representation learning in reinforcement learning. In _Advances in Neural Information Processing Systems_, 2022.
* Cheng et al. (2023) Cheng, Y., Huang, R., Yang, J., and Liang, Y. Improved sample complexity for reward-free reinforcement learning under low-rank mdps. _arXiv preprint arXiv:2303.10859_, 2023.

Cheung, W. C., Simchi-Levi, D., and Zhu, R. Learning to optimize under non-stationarity. In _The 22nd International Conference on Artificial Intelligence and Statistics_, volume 89 of _Proceedings of Machine Learning Research_, pp. 1079-1087. PMLR, 2019.
* Cheung et al. (2020) Cheung, W. C., Simchi-Levi, D., and Zhu, R. Reinforcement learning for non-stationary markov decision processes: The blessing of (more) optimism. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* Dann et al. (2017) Dann, C., Lattimore, T., and Brunskill, E. Unifying PAC and regret: Uniform PAC bounds for episodic reinforcement learning. In _Advances in Neural Information Processing Systems 30_, 2017.
* Fei et al. (2020) Fei, Y., Yang, Z., Wang, Z., and Xie, Q. Dynamic regret of policy optimization in non-stationary environments. In _Advances in Neural Information Processing Systems 33_, 2020.
* Gajane et al. (2018) Gajane, P., Ortner, R., and Auer, P. A sliding-window algorithm for markov decision processes with arbitrarily changing rewards and transitions. _CoRR_, abs/1805.10066, 2018.
* Garivier & Moulines (2011) Garivier, A. and Moulines, E. On upper-confidence bound policies for switching bandit problems. In _International Conference on Algorithmic Learning Theory_, 2011.
* Gu et al. (2017) Gu, S., Holly, E., Lillicrap, T., and Levine, S. Deep reinforcement learning for robotic manipulation with asynchronous off-policy updates. In _IEEE international conference on robotics and automation_, 2017.
* Hall & Willett (2015) Hall, E. C. and Willett, R. M. Online convex optimization in dynamic environments. _IEEE J. Sel. Top. Signal Process._, 9(4):647-662, 2015.
* Hazan (2016) Hazan, E. Introduction to online convex optimization. _Found. Trends Optim._, 2(3-4):157-325, 2016.
* Jiang et al. (2017) Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R. E. Contextual decision processes with low bellman rank are pac-learnable. In _International Conference on Machine Learning_, 2017.
* Levine et al. (2016) Levine, S., Finn, C., Darrell, T., and Abbeel, P. End-to-end training of deep visuomotor policies. _The Journal of Machine Learning Research_, 17(1):1334-1373, 2016.
* Ma et al. (2021) Ma, X., Li, J., Kochenderfer, M. J., Isele, D., and Fujimura, K. Reinforcement learning for autonomous driving with latent state inference and spatial-temporal relationships. In _IEEE International Conference on Robotics and Automation_, 2021.
* Mao et al. (2021) Mao, W., Zhang, K., Zhu, R., Simchi-Levi, D., and Basar, T. Near-optimal model-free reinforcement learning in non-stationary episodic mdps. In _Proceedings of the 38th International Conference on Machine Learning_, 2021.
* Modi et al. (2021) Modi, A., Chen, J., Krishnamurthy, A., Jiang, N., and Agarwal, A. Model-free representation learning and exploration in low-rank mdps. _arXiv preprint arXiv:2102.07035_, 2021.
* Schulman et al. (2017) Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. _CoRR_, abs/1707.06347, 2017.
* Shani et al. (2020) Shani, L., Efroni, Y., Rosenberg, A., and Mannor, S. Optimistic policy optimization with bandit feedback. In _Proceedings of the 37th International Conference on Machine Learning_, 2020.
* Silver et al. (2016) Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484-489, 2016.
* Silver et al. (2017) Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al. Mastering chess and shogi by self-play with a general reinforcement learning algorithm. _arXiv preprint arXiv:1712.01815_, 2017.
* Silver et al. (2018) Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et al. A general reinforcement learning algorithm that masters chess, shogi, and go through self-play. _Science_, 362(6419):1140-1144, 2018.
* Silver et al. (2018)Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J. Model-based RL in contextual decision processes: PAC bounds and exponential improvements over model-free approaches. In _Conference on Learning Theory_, 2019.
* Touati and Vincent (2020) Touati, A. and Vincent, P. Efficient learning in non-stationary linear markov decision processes. _CoRR_, abs/2010.12870, 2020.
* Tsybakov (2009) Tsybakov, A. B. _Introduction to Nonparametric Estimation_. Springer series in statistics. Springer, 2009.
* Uehara et al. (2022) Uehara, M., Zhang, X., and Sun, W. Representation learning for online and offline RL in low-rank mdps. In _The Tenth International Conference on Learning Representations_, 2022.
* Wei and Luo (2021) Wei, C. and Luo, H. Non-stationary reinforcement learning without prior knowledge: an optimal black-box approach. In _Conference on Learning Theory_, 2021.
* Xu et al. (2021) Xu, T., Liang, Y., and Lan, G. CRPO: A new approach for safe reinforcement learning with convergence guarantee. In _Proceedings of the 38th International Conference on Machine Learning_, 2021.
* Zanette et al. (2021) Zanette, A., Cheng, C.-A., and Agarwal, A. Cautiously optimistic policy optimization and exploration with linear function approximation. In _Conference on Learning Theory_, 2021.
* Zhao et al. (2021) Zhao, X., Gu, C., Zhang, H., Yang, X., Liu, X., Tang, J., and Liu, H. DEAR: deep reinforcement learning for online advertising impression in recommender systems. In _Thirty-Fifth AAAI Conference on Artificial Intelligence_, 2021.
* Zhong et al. (2021) Zhong, H., Yang, Z., Wang, Z., and Szepesvari, C. Optimistic policy optimization is provably efficient in non-stationary mdps. _CoRR_, abs/2110.08984, 2021.
* Zhou et al. (2020) Zhou, H., Chen, J., Varshney, L. R., and Jagmohan, A. Nonstationary reinforcement learning with linear function approximation. _CoRR_, 2020.

## Supplementary Materials

### Appendix A Proof of Theorem 4.4

We summarize frequently used notations in the following list.

\[\begin{array}{ll}\zeta_{k,W}&\frac{2\log(2|\Phi||\Psi|kH/\delta)}{\sqrt{2WA} \left(\Phi\right|\min\{k,W\}TH/\delta)}\\ \lambda_{k,W}&O(d\log(|\Phi|\min\{k,W\}TH/\delta))\\ \alpha_{k,W}&\sqrt{2WA\zeta_{k,W}+\lambda_{k,W}d}=O(\sqrt{4A\log\left(2|\Phi ||\Psi|kH/\delta\right)+\lambda_{k,W}d})\\ \tilde{\alpha}_{k,W}&5\sqrt{2WA\zeta_{k,W}+\lambda_{k,W}d}\\ \beta_{k,W}&\sqrt{9dA(2WA\zeta_{k,W}+\lambda_{k,W}d)+\lambda_{k,W}d}\\ \eta&\sqrt{\frac{L\log A}{K}}\\ \Delta^{P}_{\mathcal{H},\mathcal{I}}&\sum_{h\in\mathcal{H}}\sum_{i\in \mathcal{I}}\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\left\|P_{h}^{\star,i +1}(\cdot|s,a)-P_{h}^{\star,i}(\cdot|s,a)\right\|_{TV}\\ \Delta^{\sqrt{P}}_{\mathcal{H},\mathcal{I}}&\sum_{h\in\mathcal{H}}\sum_{i\in \mathcal{I}}\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\sqrt{\left\|P_{h}^{ \star,i+1}(\cdot|s,a)-P_{h}^{\star,i}(\cdot|s,a)\right\|_{TV}}\\ \Delta^{\phi}_{\mathcal{H},\mathcal{I}}&\sum_{h\in\mathcal{H}}\sum_{i\in \mathcal{I}}\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\left\|\phi_{h}^{\star,i+1}(s,a)-\phi_{h}^{\star,i}(s,a)\right\|_{2}\\ \Delta^{\tau}_{\mathcal{H},\mathcal{I}}&\sum_{h\in\mathcal{H}}\sum_{i\in \mathcal{I}}\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\left\|r_{h}^{\star,i +1}(s,a)-r_{h}^{\star,i}(s,a)\right\|_{2}\\ \Delta^{\pi}_{\mathcal{H},\mathcal{I}}&\sum_{h\in\mathcal{H}}\sum_{i\in \mathcal{I}}\max_{s\in\mathcal{S}}\left\|\pi_{h}^{\star,i}(\cdot|s)-\pi_{h}^{ \star,i-1}(\cdot|s)\right\|_{TV}\\ f_{h}^{k}(s,a)&\|\hat{P}_{h}^{k,l}(\cdot|s,a)-P_{h}^{\star,h}(\cdot|s,a)\|_{TV} \\ U_{h,\phi}^{k,W}&\sum_{i=1\forall k-W}^{k-1}\mathbb{E}_{s_{h}\sim(P^{\star},\hat{\pi}^{\dagger}),a_{h}\sim\mathcal{U}(\mathcal{A})}\left[\phi(s_{h},a_{ h})(\phi(s_{h},a_{h}))^{\top}\right]+\lambda_{k,W}I_{d}\\ U_{h}^{k,W}&\sum_{\hat{P}_{h}^{(k,h+1)}}\hat{\phi}_{h}(s_{h},a_{h})\hat{\phi}_{h }(s_{h},a_{h})^{\top}+\lambda_{k,W}I_{d}\\ W_{h,\phi}^{k,W}&\sum_{i=1\forall k-W}^{k-1}\mathbb{E}_{(s_{h},a_{h})\sim(P^{ \star,i},\hat{\pi}^{\dagger})}\left[\phi(s_{h},a_{h})(\phi(s_{h},a_{h}))^{ \top}\right]+\lambda_{k,W}I_{d}\\ b_{h}^{k}&\min\left\{\alpha_{k,W}\left\|\hat{\phi}_{h}^{k}(s_{h},a_{h})\right\| _{(U_{h,\phi}^{k,W})^{-1}},1\right\}\\ \hat{b}_{h}^{k}&\min\left\{\tilde{\alpha}_{k,W}\left\|\hat{\phi}_{h}^{k}(s_{h},a_{h})\right\|_{(U_{h}^{k,W})^{-1}},1\right\}\end{array}\]

Proof Sketch of Theorem 4.4.: The proof contains the following three main steps.

**Step 1 (Appendix A.1):** We first decompose the average dynamic suboptimal gap into three terms as in Lemma A.1, which can be divided into two parts: one part corresponds to the model estimation error and the other part corresponds to the performance difference between the target policy chosen by the agent and optimal policy. We then bound the two parts separately.

**Step 2 (Appendix A.2):** For the first part corresponding to model estimation error, first by Lemmas A.13 and A.15, we show that the model estimation error can be bounded by the average of the truncated value functions of the bonus terms i.e. \(\frac{1}{K}\sum_{k=1}^{K}\hat{V}_{\hat{P}^{k},\hat{\nu}^{k}}^{\pi}\) plus a term w.r.t. variation budgets. We then upper bound the average of \(\hat{V}_{\hat{P}^{k},\hat{\nu}^{k}}^{\pi}\) as in Lemma A.18. To this end, we divide the total \(K\) rounds into blocks with equal length of \(W\) and adopt an auxiliary anchor representation for each block to deal with the challenge arising from time-varying representations when using standard elliptical potential based methods.

**Step 3 (Appendix A.3):** For the second part corresponding to the performance difference bound, similarly to dealing with changing representations, since the optimal policy changes over time, we adopt an auxiliary anchor policy and decompose the performance difference bound into two terms as in Equation (10) and bound the two terms separately. 

We further note that the above analysis techniques can also be applied to RL problems where model mis-specification exists, i.e. \(\phi^{\star,k}\notin\Phi,\mu^{\star,k}\notin\Psi\).

**Organization of the Proof for Theorem 4.4.** Our proof of Theorem 4.4 is organized as follows. In Appendix A.1, we provide the decomposition of the average dynamic suboptimality gap \(\operatorname{Gap}_{\mathrm{Ave}}\) in Equation (7); in Appendix A.2, we bound the first and third terms of \(\operatorname{Gap}_{\mathrm{Ave}}\); in Appendix A.3,we bound the second term of \(\mathrm{Gap}_{\mathrm{Ave}}\), and in Appendix A.4 we combine our results to complete the proof of Theorem 4.4. We provide all the supporting lemmas in Appendix A.5.

### Average Suboptimanility Gap Decomposition

**Lemma A.1**.: _We denote \(\pi_{h}^{\star,k}(\cdot|s)=\arg\max_{\pi}V_{P^{\star,k},r^{k}}^{\pi}\). Then the average dynamic suboptimality gap has the following decomposition:_

\[\mathrm{Gap}_{\mathrm{Ave}}(K) =\frac{1}{K}\sum_{k=1}^{K}V_{P^{\star,k},r^{k}}^{\star}-V_{P^{ \star,k},r^{k}}^{\pi^{k}}\] \[=\frac{1}{K}\sum_{k=1}^{K}\sum_{h\in[H]}\mathbb{E}_{(s_{h},a_{h}) \sim(P^{\star,k},\pi^{\star,k})}\left[\left\{P_{h}^{\star,k}-\hat{P}_{h}^{k} \right\}V_{h+1,\hat{P}^{k},r^{k}}^{\pi^{k}}\right]\] \[+\frac{1}{K}\sum_{k=1}^{K}\sum_{h\in H}\mathbb{E}_{s_{h}\sim(P^{ \star,k},\pi^{\star,k})}\left[\langle Q_{h,\hat{P}^{k},r^{k}}^{\pi^{k}}(s_{h},\cdot),\pi_{h}^{\star,k}(\cdot|s_{h})-\pi_{h}^{k}(\cdot|s_{h})\rangle\right]\] \[+\frac{1}{K}\sum_{k=1}^{K}V_{\hat{P}^{k},r^{k}}^{\pi^{k}}-V_{P^{ \star,k},r^{k}}^{\pi^{k}}\] (7)

Proof.: For any function \(f:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) and any \((k,h,s)\in[K]\times[H]\times\mathcal{S}\), define the following operators:

\[(\mathbb{J}_{k,h}^{\star}f)(s)=\langle f(s,\cdot),\pi_{h}^{\star,k}(\cdot|s) \rangle,\quad(\mathbb{J}_{k,h}f)(s)=\langle f(s,\cdot),\pi_{h}^{k}(\cdot|s)\rangle.\]

We next consider the following decomposition:

\[V_{P^{\star,k},r^{k}}^{\star}-V_{P^{\star,k},r^{k}}^{\pi^{k}}= \underbrace{V_{P^{\star,k},r^{k}}^{\star}-V_{\hat{P}^{k},r^{k}}^{\pi^{k}}}_{G _{1}}+V_{P^{\star},r^{k}}^{\pi^{k}}-V_{P^{\star,k},r^{k}}^{\pi^{k}},\] (8)

The term \(G_{1}\) can be bounded as follows:

\[G_{1} =V_{P^{\star,k},r^{k}}^{\star}-V_{\hat{P}^{k},r^{k}}^{\pi^{k}}\] \[=\left(\mathbb{J}_{k,1}^{\star}Q_{1,P^{\star,k},r^{k}}^{\pi^{ \star,k}}\right)-\left(\mathbb{J}_{k,1}Q_{1,\hat{P}^{k},r^{k}}^{\pi^{k}}\right)\] \[=(\mathbb{J}_{k,1}^{\star}(Q_{1,P^{\star,k},r^{k}}^{\pi^{\star,k} }-Q_{1,\hat{P}^{k},r^{k}}^{\pi^{k}}))+((\mathbb{J}_{k,1}^{\star}-\mathbb{J}_{k,1})Q_{1,\hat{P}^{k},r^{k}}^{\pi^{k}})\] \[=(\mathbb{J}_{k,1}^{\star}(r_{1}^{\star}(s,\cdot)+P_{1}^{\star,k} V_{2,P^{\star,k},r^{k}}^{\pi^{\star,k}}-(r_{1}^{k}(s,\cdot)+\hat{P}_{1}^{k}V_{2, \hat{P}^{k},r^{k}}^{\pi^{k}})))+((\mathbb{J}_{k,1}^{\star}-\mathbb{J}_{k,1})Q _{1,\hat{P}^{k},r^{k}}^{\pi^{k}})\] \[=(\mathbb{J}_{k,1}^{\star}(P_{1}^{\star,k}V_{2,P^{\star,k},r^{k} }^{\pi^{\star,k}}-V_{2,\hat{P}^{k},r^{k}}^{\pi^{k}})+\left((\mathbb{J}_{k,1}^ {\star}-\mathbb{J}_{k,1})Q_{1,\hat{P}^{k},r^{k}}^{\pi^{k}}\right)\] \[=\left(\mathbb{J}_{k,1}^{\star}\left\{P_{1}^{\star,k}\left\{V_{2,P^{\star,k},r^{k}}^{\pi^{\star,k}}-V_{2,\hat{P}^{k},r^{k}}^{\pi^{k}}\right\}+ \left\{P_{1}^{\star,k}-\hat{P}_{1}^{k}\right\}V_{2,\hat{P}^{k},r^{k}}^{\pi^{k} }\right\}\right)+((\mathbb{J}_{k,1}^{\star}-\mathbb{J}_{k,1})Q_{1,\hat{P}^{k },r^{k}}^{\pi^{k}})\] \[=\left(\mathbb{J}_{k,1}^{\star}\left\{P_{1}^{\star,k}-\hat{P}_{1} ^{k}\right\}V_{2,\hat{P}^{\prime},\hat{P}^{\prime},\hat{P}^{\prime}}^{\pi^{k}} \right)+\mathbb{E}_{g_{2\sim(P^{\star,k},r^{\star,k},\pi^{\star,k})}\left[V_{2, \hat{P}^{\prime},\hat{P}^{\prime},\hat{P}^{\prime},\hat{P}^{\prime}}^{\pi^{ \prime}}(s_{2})-V_{2,\hat{P}^{\prime},\hat{P}^{\prime},\hat{P}^{\prime},\hat{P }^{\prime}}^{\pi^{k}}(s_{2})\right]+((\mathbb{J}_{k,1}^{\star}-\mathbb{J}_{k,1})Q_{1,\hat{P}^{\prime},\hat{P}^{\prime},\hat{P}^{\prime}}^{\pi^{k}})\] \[=\sum_{h\in[H]}\mathbb{E}_{(s_{h},a_{h})\sim(P^{\star,k},\pi^{ \star,k})}\left[\left\{P_{h}^{\star,k}-\hat{P}_{h}^{k}\right\}V_{h+1,\hat{P}^{ \prime},\hat{P}^{\prime}}^{\pi^{k}}\right]\] \[+\sum_{h\in[H]}\mathbb{E}_{s_{h}\sim(P^{\star,k},\pi^{\star,k})} \left[\langle Q_{h,\hat{P}^{\prime},\hat{P}^{\prime}}^{\pi^{k}}(s_{h},\cdot),\pi _{h}^{\star,k}(\cdot|s_{h})-\pi_{h}^{k}(\cdot|s_{h})\rangle\right]\] (9)

Substituting the above result to Equation (8) completes the proof. 

First and Third Terms of \(\mathrm{Gap}_{\mathrm{Ave}}\) in Equation (7): Model Estimation Error Bound

#### a.2.1 First Term in Equation (7)

**Lemma A.2**.: _With probability at least \(1-\delta\), we have_

\[\frac{1}{K}\sum_{k=1}^{K}\sum_{h\in[H]}\mathbb{E}_{(s_{h},a_{h})\sim(P^{\star,k}, \pi^{\star,k})}\left[\left\{P_{h}^{\star,k}-\hat{P}_{h}^{k}\right\}V_{h+1,\hat{P},r}^{\pi^{k},k}\right]\]\[\leq O\big{(}\frac{H}{K}\big{[}\sqrt{KAA(A\log(|\Phi||\Psi|KH/\delta)+d^{2})} \big{[}H\sqrt{\frac{K\delta^{d}}{W^{d}}\log(W)}+\sqrt{HW^{2}\Delta_{[H],[K]}^{ \theta}}\big{]}+\sqrt{W^{3}AC_{B}}\Delta_{[H],[K]}^{\sqrt{p}}\big{]}\big{)}.\]

Proof.: We proceed the proof by deriving the bound:

\[\frac{1}{K} \sum_{k=1}^{K}\sum_{h\in[H]}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k}, \pi^{*,k})}\left[\left\{P_{h}^{*,k}-\hat{P}_{h}^{k}\right\}V_{h+1,\hat{P},r}^{ \pi^{*,k}}\right]\] \[\leq\frac{1}{K}\sum_{k=1}^{K}\sum_{h\in[H]}\mathbb{E}_{(s_{h},a_{ h})\sim(P^{*,k},\pi^{*,k})}\left[f_{h}^{k}(s_{h},a_{h})\right]\] \[=\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}_{a_{1}\sim\pi^{*,k}}\left[f _{1}^{k}(s_{1},a_{1})\right]+\frac{1}{K}\sum_{k=1}^{K}\sum_{h=2}^{H}\mathbb{E }_{(s_{h},a_{h})\sim(P^{*,k},\pi^{*,k})}\left[f_{h}^{k}(s_{h},a_{h})\right]\] \[=\frac{1}{K}\sum_{k=1}^{K}\mathbb{E}_{a_{1}\sim\pi^{*,k}}\left[f _{1}^{k}(s_{1},a_{1})\right]+\frac{1}{K}\sum_{k=1}^{K}\sum_{h=2}^{H}\mathbb{E }_{(s_{h},a_{h})\sim(\hat{P}^{k},\pi^{*,k})}\left[f_{h}^{k}(s_{h},a_{h})\right]\] \[+\frac{1}{K}\sum_{k=1}^{K}\sum_{h=2}^{H}\left\{\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\pi^{*,k})}\left[f_{h}^{k}(s_{h},a_{h})\right]-\mathbb{E}_ {(s_{h},a_{h})\sim(\hat{P}^{k},\pi^{*,k})}\left[f_{h}^{k}(s_{h},a_{h})\right]\right\}\] \[\overset{(i)}{\leq}\frac{2}{K}\sum_{h=2}^{H}\sum_{k=1}^{K}\hat{V }_{\hat{P}^{k},\hat{b}^{k}}^{\pi^{*,k}}+2\sum_{h=2}^{H}\left[\frac{1}{K}\sum_ {k=1}^{K}\sqrt{WA\left(\zeta_{k,W}+\frac{1}{2}C_{B}\Delta_{1,[k-W,k-1]}^{P} \right)}\right.\] \[\left.+\frac{1}{K}\sum_{k=1}^{K}\sum_{h^{\prime}=2}^{h}\sqrt{ \frac{1}{2d}WAC_{B}\Delta_{[h^{\prime}-1,h^{\prime}],[k-W,k-1]}^{P}}\right]\] \[\leq\frac{2H}{K}\sum_{k=1}^{K}\hat{V}_{\hat{P}^{k},\hat{b}^{k}}^{ \pi^{k}}+\frac{2H}{K}\left[\sum_{k=1}^{K}\sqrt{WA\left(\zeta_{k,W}+\frac{1}{2} C_{B}\Delta_{1,[k-W,k-1]}^{P}\right)}\right.\] \[\left.+\sum_{k=1}^{K}\sum_{h=2}^{H}\sqrt{\frac{1}{2d}WAC_{B} \Delta_{[h-1,h],[k-W,k-1]}^{P}}\right]\] \[\overset{(ii)}{\leq}\frac{2H}{K}\sum_{k=1}^{K}\hat{V}_{\hat{P}^ {k},\hat{b}^{k}}^{\pi^{k}}+\frac{2H}{K}\sum_{k=1}^{K}\sqrt{WA\zeta_{k,W}}+ \frac{2H}{K}\sqrt{W^{3}AC_{B}}\Delta_{[H],[K]}^{\sqrt{p}}\] \[\overset{(iii)}{\leq} O\big{(}\big{[}\sqrt{KA(A\log(|\Phi||\Psi|KH/\delta)+d^{2})} \big{[}H\sqrt{\frac{K\delta^{d}}{W^{d}}\log(W)}+\sqrt{HW^{2}\Delta_{[H],[K]}^{ \theta}}\big{]}+\sqrt{W^{3}AC_{B}}\Delta_{[H],[K]}^{\sqrt{p}}\big{]}\big{)}.\]

where \((i)\) follows from Lemmas A.13 and A.15, \((ii)\) follows because \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b},\forall a,b\geq 0\) and \(\sum_{k=1}^{K}\Delta_{\{h\},[k-W,k-1]}^{\sqrt{p}}\leq W\Delta_{\{h\},[K]}^{ \sqrt{p}}\), and \((iii)\) follows from Lemma A.18. 

#### a.2.2 Third Term in Equation (7)

**Lemma A.3**.: _With probability at least \(1-\delta\), we have_

\[\frac{1}{K} \sum_{k=1}^{K}\left[V_{\hat{P}^{k},r^{k}}^{\pi^{k}}-V_{\hat{P}^{*,k},r^{k}}^{\pi^{k}}\right]\] \[\leq O\big{(}\frac{k}{K}\sqrt{KA(A\log(|\Phi||\Psi|KH/\delta)+d^{2})} \big{[}H\sqrt{\frac{K\delta^{d}}{W^{d}}\log(W)}+\sqrt{HW^{2}\Delta_{[H],[K]}^{ \theta}}\big{]}+\sqrt{W^{3}AC_{B}}\Delta_{[H],[K]}^{\sqrt{p}}\big{)}\,.\]

Proof.: We define the model error as \(f_{h}^{k}(s_{h},a_{h})=\left\|P_{h}^{*}(\cdot|s_{h},a_{h})-\hat{P}_{h}(\cdot|s_ {h},a_{h})\right\|_{TV}\). We next derive the following bound:

\[\frac{1}{K} \sum_{k=1}^{K}\left[V_{\hat{P}^{k},r^{k}}^{\pi^{k}}-V_{P^{*,k},r^ {k}}^{\pi^{k}}\right]\] \[\overset{(i)}{\leq}\frac{1}{K}\sum_{k=1}^{K}\hat{V}_{\hat{P}^{k}, \hat{b}^{k}}^{\pi^{k}}+\frac{1}{K}\sum_{k=1}^{K}\sum_{h=2}^{H}\sqrt{\frac{3}{ \lambda_{W}}WAC_{B}\Delta_{\{h-1,h\},[k-W,k-1]}^{P}}\]\[\leq\frac{1}{2}\eta KH+\frac{1}{\eta}LH\log A+\tau\Delta_{[H],[K]}^{ \pi}.\]Proof.: We first decompose \((a)\) into two parts:

\[\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{h\in[H]}\mathbb{E}_{P^ {\star,(l-1)\tau+1,\pi^{\star},(l-1)\tau+1}}\left[\langle\hat{Q}_{h}^{k}(s_{h}, \cdot),\pi_{h}^{\star,k}(\cdot|s_{h})-\pi_{h}^{k}(\cdot|s_{h})\rangle\right]\] \[\quad=\underbrace{\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{ h\in[H]}\mathbb{E}_{P^{\star,(l-1)\tau+1,\pi^{\star},(l-1)\tau+1}}\left[ \langle\hat{Q}_{h}^{k}(s_{h},\cdot),\pi_{h}^{\star,(l-1)\tau+1}(\cdot|s_{h})- \pi_{h}^{k}(\cdot|s_{h})\rangle\right]}_{(I)}\] \[\quad+\underbrace{\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_ {h\in[H]}\mathbb{E}_{P^{\star,(l-1)\tau+1,\pi^{\star},(l-1)\tau+1}}\left[ \langle\hat{Q}_{h}^{k}(s_{h},\cdot),\pi_{h}^{\star,k}(\cdot|s_{h})-\pi_{h}^{ \star,(l-1)\tau+1}(\cdot|s_{h})\rangle\right]}_{(II)}\]

I) Bound the term (I)

By Lemma A.4, we have

\[(I) \leq\frac{1}{2}\eta KH+\sum_{h\in[H]}\frac{1}{\eta}\sum_{l\in[L]} \mathbb{E}_{P^{\star,(l-1)\tau+1,\pi^{\star},(l-1)\tau+1}}\] \[\times\left[\sum_{k=(l-1)\tau+1}^{l\tau}\left[D_{KL}\left(\pi^{ \star,(l-1)\tau+1}(\cdot|s_{h})\|\pi^{k}(\cdot|s_{h})\right)-D_{KL}\left(\pi^{ \star,(l-1)\tau+1}(\cdot|s_{h})\|\pi^{k+1}(\cdot|s_{h})\right)\right]\right]\] \[\leq\frac{1}{2}\eta KH+\sum_{h\in[H]}\frac{1}{\eta}\sum_{l\in[L]} \mathbb{E}_{P^{\star,(l-1)\tau+1,\pi^{\star},(l-1)\tau+1}}\] \[\times\left[D_{KL}\left(\pi^{\star,(l-1)\tau+1}(\cdot|s_{h})\|\pi^ {(l-1)\tau+1}(\cdot|s_{h})\right)-D_{KL}\left(\pi^{\star,(l-1)\tau+1}(\cdot|s_ {h})\|\pi^{l\tau+1}(\cdot|s_{h})\right)\right]\] \[\leq\frac{1}{2}\eta KH+\sum_{h\in[H]}\frac{1}{\eta}\sum_{l\in[L]} \mathbb{E}_{P^{\star,(l-1)\tau+1,\pi^{\star},(l-1)\tau+1}}\left[D_{KL}\left(\pi ^{\star,(l-1)\tau+1}(\cdot|s_{h})\|\pi^{(l-1)\tau+1}(\cdot|s_{h})\right)\right]\] \[\leq\frac{1}{2}\eta KH+\frac{1}{\eta}LH\log A,\]

where the last equation follows because

\[D_{KL}\left(\pi^{\star,(l-1)\tau+1}(\cdot|s_{h})\|\pi^{(l-1)\tau+ 1}(\cdot|s_{h})\right) =\sum_{a\in\mathcal{A}}\pi^{\star,(l-1)\tau+1}(a|s_{h})\log(A \cdot\pi^{(l-1)\tau+1}(\cdot|s_{h}))\] \[=\log A+\sum_{a}\pi^{\star,(l-1)\tau+1}(a_{h}|s_{h})\cdot\log\pi^ {\star,(l-1)\tau+1}(a_{h}|s_{h})\] \[\leq\log A.\]

II) Bound the term (II)

\[(II) \leq\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{h\in[H]} \mathbb{E}_{P^{\star,(l-1)\tau+1,\pi^{\star},(l-1)\tau+1}}\left[\left\|\pi_{h} ^{\star,k}(\cdot|s_{h})-\pi_{h}^{\star,(l-1)\tau+1}(\cdot|s_{h})\right\|_{TV}\right]\] \[\leq\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{h\in[H]} \sum_{t=(l-1)\tau+2}^{k}\mathbb{E}_{P^{\star,(l-1)\tau+1,\pi^{\star},(l-1)\tau+ 1}}\left[\left\|\pi_{h}^{\star,t}(\cdot|s_{h})-\pi_{h}^{\star,t-1}(\cdot|s_{h}) \right\|_{TV}\right]\] \[\leq\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{t=(l-1)\tau+ 2}^{k}\sum_{h\in[H]}\max_{s\in\mathcal{S}}\left[\left\|\pi_{h}^{\star,t}( \cdot|s)-\pi_{h}^{\star,t-1}(\cdot|s)\right\|_{TV}\right]\] \[\leq\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{t=(l-1)\tau+ 1}^{l\tau}\sum_{h\in[H]}\max_{s\in\mathcal{S}}\left[\left\|\pi_{h}^{\star,t}( \cdot|s)-\pi_{h}^{\star,t-1}(\cdot|s)\right\|_{TV}\right]\]\[\leq\tau\sum_{k\in[K]}\sum_{h\in[H]}\max_{s\in\mathcal{S}}\left[\left\| \pi_{h}^{\star,k}(\cdot|s)-\pi_{h}^{\star,k-1}(\cdot|s)\right\|_{TV}\right]\] \[\leq\tau\Delta_{[H],[K]}^{\pi}.\]

#### a.3.2 Bound \((b)\) in Equation (10)

**Lemma A.6**.: _The term \((b)\) in Equation (10) can be bounded as follows:_

\[\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{h\in[H]}\left( \mathbb{E}_{P^{\star,k},\pi^{\star,k}}-\mathbb{E}_{P^{\star,(l-1)\tau+1},\pi^{ \star,(l-1)\tau+1}}\right)\left[\langle\hat{Q}_{h}^{k}(s_{h},\cdot),\pi_{h}^{ \star,k}(\cdot|s_{h})-\pi_{h}^{k}(\cdot|s_{h})\rangle\right]\] \[\qquad\leq 2H\tau(\Delta_{[H],[K]}^{P}+\Delta_{[H],[K]}^{\pi}).\]

Proof.: Denote the indicator function of state \(s_{h}\) as \(\mathbb{I}(s_{h})\), and then we have

\[\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{h\in[H]}\left( \mathbb{E}_{P^{\star,k},\pi^{\star,k}}-\mathbb{E}_{P^{\star,(l-1)\tau+1},\pi^{ \star,(l-1)\tau+1}}\right)\left[\langle\hat{Q}_{h}^{k}(s_{h},\cdot),\pi_{h}^{ \star,k}(\cdot|s_{h})-\pi_{h}^{k}(\cdot|s_{h})\rangle\right]\] \[\qquad\leq\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{h\in[H] }\int_{s_{h}}\left|\mathbb{P}_{P^{\star,k}}^{\pi^{\star,k}}(s_{h})-\mathbb{P}_ {P^{\star,(l-1)\tau+1}}^{\pi^{\star,(l-1)\tau+1}}(s_{h})\right|ds_{h}\] \[\qquad\leq\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{h\in[H] }\sum_{t=(l-1)\tau+2}^{k}\int_{s_{h}}\left|\mathbb{P}_{P^{\star,t}}^{\pi^{ \star,t}}(s_{h})-\mathbb{P}_{P^{\star,t-1}}^{\pi^{\star,t-1}}(s_{h})\right|ds_ {h},\] (11)

where \(\mathbb{P}_{P}^{\pi}(s)\) denotes the visitation probability at state \(s\) under model \(P\) and policy \(\pi\).

Consider \(\int_{s_{h}}\left|\mathbb{P}_{P^{\star,t}}^{\pi^{\star,t}}(s_{h})-\mathbb{P}_ {P^{\star,t-1}}^{\pi^{\star,t-1}}(s_{h})\right|ds_{h}\) can be further decomposed as

\[\int_{s_{h}}\left|\mathbb{P}_{P^{\star,t}}^{\pi^{\star,t}}(s_{h}) -\mathbb{P}_{P^{\star,t-1}}^{\pi^{\star,t-1}}(s_{h})\right|ds_{h}\] \[\qquad\qquad\leq\int_{s_{h}}\left|\mathbb{P}_{P^{\star,t}}^{\pi^{ \star,t-1}}(s_{h})-\mathbb{P}_{P^{\star,t-1}}^{\pi^{\star,t-1}}(s_{h})\right| ds_{h}+\int_{s_{h}}\left|\mathbb{P}_{P^{\star,t}}^{\pi^{\star,t}}(s_{h})- \mathbb{P}_{P^{\star,t}}^{\pi^{\star,t-1}}(s_{h})\right|ds_{h}.\]

For the first term \(\int_{s_{h}}\left|\mathbb{P}_{P^{\star,t}}^{\pi^{\star,t-1}}(s_{h})-\mathbb{P} _{P^{\star,t-1}}^{\pi^{\star,t-1}}(s_{h})\right|ds_{h}\),

\[\int_{s_{h}}\left|\mathbb{P}_{P^{\star,t}}^{\pi^{\star,t-1}}(s_{h})- \mathbb{P}_{P^{\star,t-1}}^{\pi^{\star,t-1}}(s_{h})\right|ds_{h}\] \[\qquad\qquad\leq\int_{s_{h}}\sum_{i=1}^{h}\left|(P_{1}^{\star,t} )^{\pi^{\star,t-1}_{i}}\ldots(P_{i}^{\star,t})^{\pi^{\star,t-1}_{i}}\ldots(P_{h -1}^{\star,t-1})^{\pi^{\star,t-1}_{h-1}}(s_{h})\right|ds_{h}\] \[\qquad\qquad\qquad\qquad\qquad-(P_{1}^{\star,t})^{\pi^{\star,t-1} _{i}}\ldots(P_{i}^{\star,t-1})^{\pi^{\star,t-1}_{i}}\ldots(P_{h-1}^{\star,t-1}) ^{\pi^{\star,t-1}_{h-1}}(s_{h})\right|ds_{h}\] \[\qquad\qquad\leq\int_{s_{h}}\sum_{i=1}^{h}\left|\int_{s_{2}, \ldots,s_{h-1}}\left|(P_{i}^{\star,t})^{\pi^{\star,t-1}_{i}}(s_{i+1}|s_{i})-(P_ {i}^{\star,t-1})^{\pi^{\star,t-1}_{i}}(s_{i+1}|s_{i})\right|\] \[\qquad\qquad\qquad\qquad\qquad\prod_{j=1}^{i-1}(P_{j}^{\star,t} )^{\pi^{\star,t-1}_{j}}(s_{j+1}|s_{j})\prod_{j=i+1}^{h-1}(P_{j}^{\star,t-1})^{ \pi^{\star,t-1}_{j}}(s_{j+1}|s_{j})ds_{2}\ldots ds_{h-1}\right|ds_{h}\] \[\qquad\qquad\stackrel{{(i)}}{{\leq}}\int_{s_{h}} \sum_{i=1}^{h}\left|\int_{s_{2},\ldots,s_{i},s_{i+2},\ldots s_{h-1}}\int_{s_{i+1}} \left|(P_{i}^{\star,t})^{\pi^{\star,t-1}_{i}}(s_{i+1}|s_{i})-(P_{i}^{\star,t-1 })^{\pi^{\star,t-1}_{i}}(s_{i+1}|s_{i})\right|\]\[\max_{s_{i+1}\in\mathcal{S}}\prod_{j=1}^{i-1}\left(P_{j}^{\star,t} \right)^{\pi_{j}^{\star,t-1}}(s_{j+1}|s_{j})\prod_{j=i+1}^{h-1}\left(P_{j}^{ \star,t-1}\right)^{\pi_{j}^{\star,t-1}}(s_{j+1}|s_{j})ds_{2}\ldots ds_{h-1} \right|ds_{h}\] \[\stackrel{{(ii)}}{{\leq}}\int_{s_{h}}\sum_{i=1}^{h} \left|\int_{s_{2},\ldots,s_{i-1},s_{i+2},\ldots s_{h-1}}\max_{s_{i}\in \mathcal{S}}\int_{s_{i+1}}\left|\left(P_{i}^{\star,t}\right)^{\pi_{i}^{\star,t- 1}}(s_{i+1}|s_{i})-\left(P_{i}^{\star,t-1}\right)^{\pi_{i}^{\star,t-1}}(s_{i+1} |s_{i})\right|\right.\] \[\left.\int_{s_{i}}\max_{s_{i+1}\in\mathcal{S}}\prod_{j=1}^{i-1} \left(P_{j}^{\star,t}\right)^{\pi_{j}^{\star,t-1}}(s_{j+1}|s_{j})\prod_{j=i+1} ^{h-1}\left(P_{j}^{\star,t-1}\right)^{\pi_{j}^{\star,t-1}}(s_{j+1}|s_{j})ds_{ 2}\ldots ds_{h-1}\right|ds_{h}\] \[\stackrel{{(iii)}}{{\leq}}\int_{s_{h}}\sum_{i=1}^{h} \left|\max_{(s,a)\in\mathcal{S}\times\mathcal{A}}\left\|P_{i}^{\star,t}(\cdot |s,a)-P_{i}^{\star,t-1}(\cdot|s,a)\right\|_{TV}\underbrace{\int_{s_{1},\ldots, s_{i}}\prod_{j=1}^{i-1}\left(P_{j}^{\star,t}\right)^{\pi_{j}^{\star,t-1}}(s_{j+1}|s_{j}) ds_{1}\ldots ds_{i}}_{=1}\right.\] \[\underbrace{\int_{s_{i+2},\ldots,s_{h-1}}\max_{s_{i+1}\in \mathcal{S}}\prod_{j=i+1}^{h-1}\left(P_{j}^{\star,t-1}\right)^{\pi_{j}^{\star,t-1}}(s_{j+1}|s_{j})ds_{i+2}\ldots ds_{h-1}}_{\leq 1}\right|ds_{h}\] \[\leq\sum_{h\in[H]}\max_{(s,a)\in\mathcal{S}\times\mathcal{A}} \left\|P_{i}^{\star,t}(\cdot|s,a)-P_{i}^{\star,t-1}(\cdot|s,a)\right\|_{TV},\] (12)

where \((i)\) and \((ii)\) follow from Holder's inequality, and \((iii)\) follows from the definition of total variation distance.

Similarly for the second term and from Lemma A.19

\[\int_{s_{h}}\left|\mathbb{P}_{P^{\star,t}}^{\pi_{j}^{\star,t}}(s_{h})-\mathbb{ P}_{P^{\star,t}}^{\pi_{j}^{\star,t-1}}(s_{h})\right|ds_{h}\leq\sum_{h\in[H]}\max_{s \in\mathcal{S}}\left\|\pi_{h}^{\star,t}(\cdot|s)-\pi_{h}^{\star,t-1}(\cdot|s) \right\|_{TV}.\] (13)

Plug Equation (12) and Equation (13) into Equation (11), we have

\[\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{h\in[H]}\left( \mathbb{E}_{P^{\star,k},\pi^{\star,k}}-\mathbb{E}_{P^{\star,(l-1)\tau+1},\pi^{ \star,(l-1)\tau+1}}\right)\left[\langle\hat{Q}_{h}^{k}(s_{h},\cdot),\pi_{h}^{ \star,k}(\cdot|s_{h})-\pi_{h}^{k}(\cdot|s_{h})\rangle\right]\] \[\quad\leq 2\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{l\tau}\sum_{h\in[H]} \sum_{t=(l-1)\tau+2}^{k}\left(\sum_{i\in[H]}\max_{s\in\mathcal{S}}\left\|\pi_{i} ^{\star,t}(\cdot|s)-\pi_{i}^{\star,t-1}(\cdot|s)\right\|_{TV}\right.\] \[\quad\quad+\sum_{i\in[H]}\max_{(s,a)\in\mathcal{S}\times \mathcal{A}}\left\|P_{i}^{\star,t}(\cdot|s,a)-P_{i}^{\star,t-1}(\cdot|s,a) \right\|_{TV}\right)\] \[\quad\leq 2\sum_{h\in[H]}\left(\sum_{l\in[L]}\sum_{k=(l-1)\tau+1}^{ l\tau}\sum_{t=(l-1)\tau+1}^{l\tau}\sum_{i\in[H]}\left(\max_{s\in\mathcal{S}} \left\|\pi_{i}^{\star,t}(\cdot|s)-\pi_{i}^{\star,t-1}(\cdot|s)\right\|_{TV}\right.\] \[\quad\quad+\left.\max_{(s,a)\in\mathcal{S}\times\mathcal{A}} \left\|P_{i}^{\star,t}(\cdot|s,a)-P_{i}^{\star,t-1}(\cdot|s,a)\right\|_{TV}\right)\right)\] \[\quad=2H\tau(\Delta_{[H],[K]}^{P}+\Delta_{[H],[K]}^{\pi}).\]

[MISSING_PAGE_FAIL:20]

**Corollary A.9**.: _The following inequality holds for any \(k\in[K],h\in[H],s_{h}\in\mathcal{S},a_{h}\in\mathcal{A}\) with probability at least \(1-\delta\):_

\[\min\left\{\frac{\tilde{\alpha}_{k,W}}{5}\left\|\hat{\phi}_{h}^{k}(s_{h},a_{h}) \right\|_{(U_{h,\hat{\phi}^{k}}^{k,W})^{-1}},1\right\}\leq\hat{b}_{h}^{k}(s_{h},a_{h})\leq 3\tilde{\alpha}_{k,W}\left\|\hat{\phi}_{h}^{k}(s_{h},a_{h})\right\|_{( U_{h,\hat{\phi}^{k}}^{k,W})^{-1}},\]

_where \(\tilde{\alpha}_{k,W}=5\sqrt{2WA\zeta_{k,W}+\lambda_{k,W}d}\)._

We next provide the MLE guarantee for nonstationary RL, which shows that under any exploration policy, the estimation error can be bounded with high probability. Differently from Theorem 21 in Agarwal et al. (2020b), we capture the nonstationarity in the analysis.

**Lemma A.10** (Nonstationary MLE Guarantee).: _Given \(\delta\in(0,1)\), under Assumptions 4.1 to 4.3, let \(C_{B}=\sqrt{\frac{C_{B}}{p_{\min}}}\), and consider the transition kernels learned from line 3 in Algorithm 2. We have the following inequality holds for any \(n,h\geq 2\) with probability at least \(1-\delta/2\):_

\[\frac{1}{W}\sum_{i=1\vee(k-W)}^{k-1}\mathop{\mathbb{E}}_{\begin{subarray}{c }\varepsilon_{h-1}\sim(P^{*,i},v_{i})\\ a_{h-1},a_{h}\sim\mu(A)\\ s_{h}\sim P^{*,i}(\cdot|\cdot|h_{k-1}\cdot\alpha_{h-1})\end{subarray}}\left[f_{ h}^{k}(s_{h},a_{h})^{2}\right]\leq\zeta_{k,W}+2C_{B}\Delta_{h,[k-W,k-1]}^{P},\] (15)

_where, \(\zeta_{k,W}:=\frac{2\log(2|\Phi||\Psi|kH/\delta)}{W}\). In addition, for \(h=1\),_

\[\mathop{\mathbb{E}}_{a_{1}\sim\mathcal{U}(\mathcal{A})}\left[f_{1}^{k}(s_{1}, a_{1})^{2}\right]\leq\zeta_{k,W}+2C_{B}\Delta_{1,[k-W,k-1]}^{P}.\]

Proof of Lemma a.10.: For simplification, we denote \(x=(s,a)\in\mathcal{X},\mathcal{X}=\mathcal{S}\times\mathcal{A}\), \(y=p^{\star}\in\mathcal{Y},\mathcal{Y}=\mathcal{S}\). The model estimation process in Algorithm 1 can be viewed as a sequential conditional probability estimation setting with an instance space \(\mathcal{X}\) and a target space \(\mathcal{Y}\), where the conditional density is given by \(p^{i}(y|x)=P^{\star,i}(y|x)\) for any \(i\). We are given a dataset \(D:=\{(x_{i},y_{i})\}_{i=1\vee(k-W)}^{k}\), where \(x_{i}\sim\mathcal{D}_{i}=\mathcal{D}_{i}(x_{1:i-1},y_{1:i-1})\) and \(y_{i}\sim p^{i}(\cdot|x_{i})\). Let \(D^{\prime}\) denote a tangent sequence \(\{(x_{i}^{\prime},y_{i}^{\prime})\}_{i=1\vee(k-W)}^{k}\) where \(x_{i}^{\prime}\sim\mathcal{D}_{i}(x_{1:i-1},y_{1:i-1})\) and \(y_{i}^{\prime}\sim p^{i}(\cdot|x_{i}^{\prime})\). Further, we consider a function class \(\mathcal{F}=\Phi\times\Psi:(\mathcal{X}\times\mathcal{Y})\rightarrow\mathbb{R}\) and assume that the reachability condition \(P^{\star,i}\in\mathcal{F}\) holds for any \(i\).

We first introduce one useful lemma in Agarwal et al. (2020b) to decouple data.

**Lemma A.11** (Lemma 24 of Agarwal et al. (2020b)).: _Let \(D\) be a dataset with at most \(W\) samples and \(D^{\prime}\) be the corresponding tangent sequence. Let \(L(P,D)=\sum_{i=1\vee(k-W)}^{k}l(P,(x_{i},y_{i}))\) be any function that decomposes additively across examples where \(l\) is any function. Let \(\widehat{P}(D)\) be any estimator taking as input random variable \(D\) and with range \(\mathcal{F}\). Then_

\[\mathop{\mathbb{E}}_{D}\left[\exp\left(L(\widehat{P}(D),D)-\log\mathop{ \mathbb{E}}_{D^{\prime}}\left[\exp(L(\widehat{P}(D),D^{\prime}))\right]-\log |\mathcal{F}|\right)\right]\leq 1.\]

Suppose \(\widehat{f}(D)\) is learned from the following maximum likelihood problem:

\[\widehat{P}(D):=\arg\max_{P\in\mathcal{F}}\sum_{(x_{i},y_{i})\in D}\log f(x_{i },y_{i}).\] (16)

Combining the Chernoff bound and Lemma A.11, we obtain an exponential tail bound, i.e., with probability at least \(1-\delta\),

\[-\log\mathop{\mathbb{E}}_{D^{\prime}}\left[\exp(L(\widehat{P}(D),D^{\prime})) \right]\leq-L(\widehat{P}(D),D)+\log|\mathcal{F}|+\log(1/\delta).\] (17)

To proceed, we let \(L(P,D)=\sum_{i=1\vee(k-W)}^{k-1}-\frac{1}{2}\log(P^{\star,k}(x_{i},y_{i})/P(x_{ i},y_{i}))\) where \(D\) is a dataset \(\{(x_{i},y_{i})\}_{i=1\vee(k-W)}^{k}\)(and \(D^{\prime}=\{(x_{i}^{\prime},y_{i}^{\prime})\}_{i=1\vee(k-W)}^{k}\) is tangent sequence). Then \(L(P^{\star,k},D)=0\leq L(\hat{P},D)\).

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

\[\begin{split}&\stackrel{{(iii)}}{{\leq}}\sum_{i\in\mathcal{I}} \underset{\begin{subarray}{c}s_{h-1}\sim(P^{i,*},\Pi)\\ a_{h-1}\sim\Pi\end{subarray}}{\mathbb{E}}\left[\underset{\begin{subarray}{c }s_{h}\sim P^{i,*}_{h-1}\\ a_{h}\sim\Pi_{h}\end{subarray}}{\mathbb{E}}\bigg{[}g(s_{h},a_{h})^{2}\bigg{|} s_{h-1},a_{h-1}\bigg{]}\right]+\lambda_{k,W}dB^{2}\\ &+\sum_{i\in\mathcal{I}}B^{2}\underset{\begin{subarray}{c}s_{h-1} \sim(P^{i,*},\Pi)\\ a_{h-1}\sim\Pi\end{subarray}}{\mathbb{E}}\left[\left\|P^{i,*}_{h-1}(s_{h-1},a_{ h-1})-P^{k,*}_{h-1}(s_{h-1},a_{h-1})\right\|_{TV}\right]\\ &+\sum_{i\in\mathcal{I}}B^{2}\underset{\begin{subarray}{c}s_{h-1} \sim(P^{i,*},\Pi)\\ a_{h-1}\sim\Pi\end{subarray}}{\mathbb{E}}\left[f^{k}_{h-1}(s_{h-1},a_{h-1})^{2 }\right]\\ &\stackrel{{(iv)}}{{\leq}}\sum_{i\in\mathcal{I}} \underset{\begin{subarray}{c}s_{h-1}\sim(P^{i,*},\Pi)\\ a_{h}\sim\Pi\end{subarray}}{\mathbb{E}}\left[g(s_{h},a_{h})^{2}\right]+\lambda _{k,W}dB^{2}+WB^{2}\Delta^{P}_{h-1,\mathcal{I}}+\sum_{i\in\mathcal{I}}B^{2} \underset{\begin{subarray}{c}s_{h-1}\sim(P^{i,*},\Pi)\\ a_{h-1}\sim\Pi\end{subarray}}{\mathbb{E}}\left[f^{k}_{h-1}(s_{h-1},a_{h-1})^{2 }\right],\end{split}\]

where \((i)\) follows from the assumption that \(\|g\|_{\infty}\leq B\), \((ii)\) follows from Jensen's inequality, \((iii)\) follows because \(f^{k}_{h-1}(s_{h-1},a_{h-1})\) is the total variation between \(P^{k,*}_{h-1}\) and \(P^{k}_{h-1}\) at time step \(h-1\), and \((iv)\) follows from importance sampling and the definition of \(\Delta^{P}_{h-1,\mathcal{I}}\). This finishes the proof. 

Recall that \(f^{k}_{h}(s,a)=\|\hat{P}^{k}_{h}(\cdot|s,a)-P^{\star,k}_{h}(\cdot|s,a)\|_{TV}\). Using Lemma A.12, we have the following lemma to bound the expectation of \(f^{k}_{h}(s,a)\) under estimated transition kernels.

**Lemma A.13**.: _Denote \(\alpha_{k,W}=\sqrt{2WA\zeta_{k,W}+\lambda_{k,W}d}\). For any \(k\in[K]\), policy \(\pi\) and reward \(r\), for all \(h\geq 2\), we have_

\[\begin{split}\mathbb{E}_{(s_{h},a_{h})\sim(P^{k},\pi)}\left[f^{ k}_{h}(s_{h},a_{h})\bigg{|}s_{h-1},a_{h-1}\right]\\ \leq&\min\left\{\alpha_{k,W}\left\|\hat{\phi}^{k}_{h- 1}(s_{h-1},a_{h-1})\right\|_{(U^{k}_{h-1,\hat{\phi}^{k}})^{-1}},1\right\}+ \sqrt{\frac{1}{2d}WAC_{B}\Delta^{P}_{[h-1,h],[k-W,k-1]}},\end{split}\] (22)

_and for \(h=1\), we have_

\[\underset{a_{1}\sim\pi}{\mathbb{E}}\left[f^{k}_{1}(s_{1},a_{1})\right]\leq \sqrt{A\left(\zeta_{k,W}+\frac{1}{2}C_{B}\Delta^{P}_{1,[k-W,k-1]} \right)}.\] (23)

Proof.: For \(h=1\), we have

\[\underset{a_{1}\sim\pi}{\mathbb{E}}\left[f^{k}_{1}(s_{1},a_{1})\right]\stackrel{{ (i)}}{{\leq}}\sqrt{\underset{a_{1}\sim\pi}{\mathbb{E}}\left[f^{k}_{1}(s_{1},a_ {1})^{2}\right]}\stackrel{{(ii)}}{{\leq}}\sqrt{A\left(\zeta_{k,W }+2C_{B}\Delta^{P}_{1,[k-W,k-1]}\right)},\]

where \((i)\) follows from Jensen's inequality, and \((ii)\) follows from the importance sampling.

Then for \(h\geq 2\), we derive the following bound:

\[\begin{split}&\underset{(s_{h},a_{h})\sim(P^{k},\pi)}{\mathbb{E}} \left[f^{k}_{h}(s_{h},a_{h})\bigg{|}s_{h-1},a_{h-1}\right]\\ &\stackrel{{(i)}}{{\leq}}\underset{a_{h-1}\sim\pi}{ \mathbb{E}}\left[\left\|\hat{\phi}^{k}_{h-1}(s_{h-1},a_{h-1})\right\|_{(U^{k} _{h-1,\hat{\phi}^{k}})^{-1}}\right.\right.\left.\left.\left(A\sum_{i=1\lor(k-W )}^{k-1}\underset{\begin{subarray}{c}s_{h-1}\sim(P^{i,*},i^{*})\\ a_{h-1}\sim P^{i,*}(i)\end{subarray}}{\mathbb{E}}\left[f^{k}_{h}(s_{h},a_{h})^{2 }\right]\right.\\ &\left.\left.+W\Delta^{P}_{h-1,[k-W,k-1]}+\lambda_{k,W}d+A\sum_{i=1 \lor(k-W)}^{k-1}\underset{\begin{subarray}{c}s_{h-2}\sim(P^{*},i^{*})\\ a_{h-2}\sim(P^{*},i^{*})\\ a_{h-2}\sim(P^{*},i^{*})\end{subarray}}{\mathbb{E}}\left[f^{k}_{h-1}(s_{h-1},a_{ h-1})^{2}\right]\right)^{-\frac{1}{2}}\\ &\stackrel{{(ii)}}{{\leq}}\underset{\mathbb{E}_{a_{h-1} \sim\pi}\left[\sqrt{WA(\zeta_{k,W}+2C_{B}\Delta^{P}_{h,[k-W,k-1]})+WA(\zeta_{k,W}+2C_{B}\Delta^{P}_{h-1,[k-W,k-1]})+W\Delta^{P}_{h-1,[k-W,k-1]}+\lambda_{k,W }d}\right.\end{split}\right.\\ &\stackrel{{(iii)}}{{\leq}}\underset{\mathbb{E}_{a_{h-1} \sim\pi}\left[\sqrt{WA(\zeta_{k,W}+2C_{B}\Delta^{P}_{h,[k-W,k-1]})+WA(\zeta_{k,W}+2C_{B}\Delta^{P}_{h-1,[k-W,k-1]})+W\Delta^{P}_{h-1,[k-W,k-1]}+\lambda_{k,W }d}\right.\end{split}\right.\\ &\stackrel{{(iii)}}{{\leq}}\underset{\mathbb{E}_{a_{h-1} \sim\pi}\left[\sqrt{WA(\zeta_{k,W}+2C_{B}\Delta^{P}_{h,[k-W,k-1]})+WA(\zeta_{k,W}+2C_{B}\Delta^{P}_{h-1,[k-W,k-1]})+W\Delta^{P}_{h-1,[k-W,k-1]}+\lambda_{k,W }d}\right.\end{split}\right.\end{split}\]\[\times\left\|\hat{\phi}_{h-1}^{k}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hat{ \phi}^{k}}^{k})^{-1}}\] \[\stackrel{{(iii)}}{{=}}\operatorname*{\mathbb{E}}_{a _{h-1}\sim\pi}\left[\alpha_{k,W}\left\|\hat{\phi}_{h-1}^{k}(s_{h-1},a_{h-1}) \right\|_{(U_{h-1,\hat{\phi}^{k}}^{k})^{-1}}\right]+\sqrt{\frac{3}{\lambda_{W }}WAC_{B}\Delta_{\{h-1,h\},[k-W,k-1]}^{P}},\]

where \((i)\) follows from Lemma A.12 and because \(|f_{h}^{k}(s_{h},a_{h})|\leq 1\); specially the first term inside the square root follows from the definition of \(U_{h-1,\hat{\phi}^{k}}^{k}\), the third term inside the square root follows from the importance sampling; \((ii)\) follows from Lemma A.10, and \((iii)\) follows because \(\sqrt{a+b}\leq\sqrt{a}+\sqrt{b},\forall a,b\geq 0\) and \(\left\|\hat{\phi}_{h-1}^{k}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hat{\phi}^{k}} ^{k})^{-1}}\leq\sqrt{\frac{1}{\lambda_{W}}}\). 

The next lemma follows a similar argument to that of Lemma A.13 with the only difference being the expectation over which \(f_{h}^{k}(s_{h},a_{h})\) takes.

**Lemma A.14**.: _Denote \(\alpha_{k,W}=\sqrt{2WA\zeta_{k,W}+\lambda_{k,W}d}\). For any \(k\in[K]\), policy \(\pi\) and reward \(r\), for all \(h\geq 2\), we have_

\[\operatorname*{\mathbb{E}}_{(s_{h},a_{h})\sim(P^{*,k},\pi)} \left[f_{h}^{k}(s_{h},a_{h})\middle|s_{h-1},a_{h-1}\right]\] \[\leq \min\left\{\alpha_{k,W}\left\|\phi_{h-1}^{*,k}(s_{h-1},a_{h-1}) \right\|_{(U_{h-1,\phi^{*,k}}^{k})^{-1}},1\right\}+\sqrt{\frac{1}{2d}WAC_{B} \Delta_{\{h-1,h\},[k-W,k-1]}^{P}},\] (24)

_and for \(h=1\), we have_

\[\operatorname*{\mathbb{E}}_{a_{1}\sim\pi}\left[f_{1}^{k}(s_{1},a_{1})\right] \leq \sqrt{A\left(\zeta_{k,W}+\frac{1}{2}C_{B}\Delta_{1,[k-W,k-1]}^{P} \right)}.\] (25)

Proof.: For \(h=1\),

\[\operatorname*{\mathbb{E}}_{a_{1}\sim\pi}\left[f_{1}^{k}(s_{1},a_{1})\right] \stackrel{{(i)}}{{\leq}}\sqrt{\operatorname*{\mathbb{E}}_{a_{1} \sim\pi}\left[f_{1}^{k}(s_{1},a_{1})^{2}\right]}\stackrel{{(ii)}} {{\leq}}\sqrt{A\left(\zeta_{k,W}+\frac{1}{2}C_{B}\Delta_{1,[k-W,k-1]}^{P} \right)},\]

where \((i)\) follows from Jensen's inequality, and \((ii)\) follows from the importance sampling.

Then for \(h\geq 2\), we derive the following bound:

\[\operatorname*{\mathbb{E}}_{(s_{h},a_{h})\sim(P^{*,k},\pi)} \left[f_{h}^{k}(s_{h},a_{h})\middle|s_{h-1},a_{h-1}\right]\] \[\stackrel{{(i)}}{{\leq}} \operatorname*{\mathbb{E}}_{a_{h-1}\sim\pi}\left[\left|\phi_{h-1} ^{*,k}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\phi^{*,k}}^{k})^{-1}}\right|^{\times }\sqrt{\begin{subarray}{c}A\sum_{i=1\lor(k-W)}^{k-1}\operatorname*{\mathbb{E}} &\frac{s_{h-1}\sim(P^{*,i},\pi^{i})}{s_{h-1}\sim(P^{*,i},\pi^{i})}\left[f_{h}^ {k}(s_{h},a_{h})^{2}\right]+\lambda_{k,W}d\\ s_{h-1}\sim\mu^{*,k}(U^{*}|s_{h-1},a_{h-1})\end{subarray}}\right]\] \[\stackrel{{(ii)}}{{\leq}} \operatorname*{\mathbb{E}}_{a_{h-1}\sim\pi}\left[\sqrt{wA(\zeta_{ k,W}+\frac{1}{2}C_{B}\Delta_{h,[k-W,k-1]}^{P})+wA(\zeta_{k,W}+\frac{1}{2}C_{B} \Delta_{h-1,(k-W,k-1]}^{P})+AC_{B}\Delta_{h,[k-W,k-1]}^{P}+\lambda_{k,W}d}\right.\] \[\left.\times\left\|\phi_{h-1}^{*,k}(s_{h-1},a_{h-1})\right\|_{(U_ {h-1,\phi^{*,k}}^{k})^{-1}}\right]\] \[\stackrel{{(iii)}}{{=}} \operatorname*{\mathbb{E}}_{a_{h-1}\sim\pi}\left[\alpha_{k,W}\left\| \phi_{h-1}^{*,k}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\phi^{*,k}}^{k})^{-1}} \right]+\sqrt{\frac{1}{2d}WAC_{B}\Delta_{[h-1,h],[k-W,k-1]}^{P}},\]

where \((i)\) follows from Lemma A.12 and because \(|f_{h}^{k}(s_{h},a_{h})|\leq 1\), the first term inside the square root follows from the definition of \(U_{h-1,\widehat{\phi}^{k}}^{k}\), the third term inside the square root follows from the importance sampling, and \((ii)\) follows from Lemma A.10.

The proof is completed by noting that \(|f_{h}^{k}(s_{h},a_{h})|\leq 1\).

The following lemma is a direct application of Lemma A.13. By this lemma, we can show that the difference of value functions can be bounded by truncated value function plus a variation term.

**Lemma A.15** (Bounded difference of value functions).: _For \(k\in[K]\), \(\delta\geq 0\) any policy \(\pi\) and reward \(r\), with probability at least \(1-\delta\), we have_

\[\left|V^{\pi}_{P^{*,k},r}-V^{\pi}_{\hat{P}^{k},r}\right|\] \[\quad\leq\hat{V}^{\pi}_{\hat{P}^{k},\hat{b}^{k}}+\sum_{h=2}^{H} \sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta^{P}_{\{h-1,h\},\{k-W,k-1\}}}+\sqrt{A \left(\zeta_{k,W}+2C_{B}\Delta^{P}_{1,\{k-W,k-1\}}\right)}.\] (26)

Proof.: **(1):** We first show that \(\left|V^{\pi}_{P^{*,k},r}-V^{\pi}_{\hat{P}^{k},r}\right|\leq\hat{V}^{\pi}_{ \hat{P}^{k},f^{k}}\).

Recall the definition of the estimated value functions \(\hat{V}^{\pi}_{h,\hat{P}^{k},r}(s_{h})\) and \(\hat{Q}^{\pi}_{h,\hat{P}^{k},r}(s_{h},a_{h})\) for policy \(\pi\):

\[\hat{Q}^{\pi}_{h,\hat{P}^{k},r}(s_{h},a_{h})=\min\left\{1,r_{h}(s _{h},a_{h})+\hat{P}^{k}_{h}\hat{V}^{\pi}_{h+1,\hat{P}^{k},r}(s_{h},a_{h}) \right\},\] \[\hat{V}^{\pi}_{h,\hat{P}^{k},r}(s_{h})=\operatorname*{\mathbb{E}} _{\pi}\left[\hat{Q}^{\pi}_{h,\hat{P}^{k},r}(s_{h},a_{h})\right].\]

We develop the proof by backward induction.

When \(h=H+1\), we have \(\left|V^{\pi}_{H+1,P^{*,k},r}(s_{H+1})-V^{\pi}_{H+1,\hat{P}^{k},r}(s_{H+1}) \right|=0=\hat{V}^{\pi}_{H+1,\hat{P}^{k},f^{k}}(s_{H+1})\).

Suppose that for \(h+1\), \(\left|V^{\pi}_{h+1,P^{*,k},r}(s_{h+1})-V^{\pi}_{h+1,\hat{P}^{k},r}(s_{h+1}) \right|\leq\hat{V}^{\pi}_{h+1,\hat{P}^{k},f^{k}}(s_{h+1})\) holds for any \(s_{h+1}\).

Then, for \(h\), by Bellman equation, we have,

\[\left|Q^{\pi}_{P^{*,k},r}(s_{h},a_{h})-Q^{\pi}_{h,\hat{P}^{k},r} (s_{h},a_{h})\right|\] \[\quad=\left|P^{*,k}_{h}V^{\pi}_{h+1,P^{*,k},r}(s_{h},a_{h})-\hat{ P}^{k}_{h}V^{\pi}_{h,\hat{P}^{k},r}(s_{h},a_{h})\right|\] \[\quad=\left|\hat{P}^{k}_{h}\left(V^{\pi}_{h+1,P^{*,k},r}-V^{\pi}_ {h+1,\hat{P}^{k},r}\right)(s_{h},a_{h})+\left(P^{*,k}_{h}-\hat{P}^{k}_{h} \right)V^{\pi}_{h,P^{*,k},r}(s_{h},a_{h})\right|\] \[\quad\stackrel{{(i)}}{{\leq}}\min\left\{1,f^{k}_{h} (s_{h},a_{h})+\hat{P}^{k}_{h}\middle|V^{\pi}_{h+1,P^{*,k},r}-V^{\pi}_{h+1,\hat {P}^{k},r}\middle|(s_{h},a_{h})\right\}\] \[\quad\stackrel{{(ii)}}{{\leq}}\min\left\{1,f^{k}_{h} (s_{h},a_{h})+\hat{P}^{k}_{h}\hat{V}^{\pi}_{h+1,\hat{P}^{k},f^{k}}(s_{h},a_{h })\right\}\] \[\quad=\hat{Q}^{\pi}_{h,\hat{P}^{k},f^{k}}(s_{h},a_{h}),\] (27)

where \((i)\) follows because \(\left\|\hat{P}^{k}_{h}(\cdot|s_{h},a_{h})-P^{*,k}_{h}(\cdot|s_{h},a_{h}) \right\|_{TV}=f^{k}_{h}(s_{h},a_{h})\) and the value function is at most 1, and \((ii)\) follows from the induction hypothesis.

Then, by the definition of \(\hat{V}^{\pi}_{h,\hat{P}^{k},r}(s_{h})\), we have

\[\left|V^{\pi}_{h,\hat{P}^{k},r}(s_{h})-V^{\pi}_{h,P^{*,k},r}(s_{ h})\right|\] \[\quad=\left|\operatorname*{\mathbb{E}}_{\pi}\left[Q^{\pi}_{h,\hat {P}^{k},r}(s_{h},a_{h})\right]-\operatorname*{\mathbb{E}}_{\pi}\left[Q^{\pi}_{h,P^{*,k},r}(s_{h},a_{h})\right]\,\right|\] \[\quad\leq\operatorname*{\mathbb{E}}_{\pi}\left[\left|Q^{\pi}_{h, \hat{P}^{k},r}(s_{h},a_{h})-Q^{\pi}_{h,P^{*,k},r}(s_{h},a_{h})\right|\right]\] \[\quad\stackrel{{(i)}}{{\leq}}\operatorname*{\mathbb{E}} _{\pi}\left[\hat{Q}^{\pi}_{h,\hat{P}^{k},f^{k}}(s_{h},a_{h})\right]\]\[=\hat{V}_{h,\hat{P}^{k},f^{k}}^{\pi}(s_{h}),\]

where \((i)\) follows from Equation (27).

Therefore, by induction, we have

\[\left|V_{P^{*},r}^{\pi}-V_{\hat{P}^{k},r}^{\pi}\right|\leq\hat{V}_{\hat{P}^{k}, f^{k}}^{\pi}.\]

**(2):** Then we prove that

\[\hat{V}_{\hat{P}^{k},f^{k}}^{\pi}\leq\hat{V}_{\hat{P}^{k},\hat{P}^{k}}^{\pi}+ \sum_{h=2}^{H}\sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{h-1,h\},[k-W,k-1]}^{P} }+\sqrt{A\left(\zeta_{k,W}+2C_{B}\Delta_{1,[k-W,k-1]}^{P}\right)}.\]

By Lemma A.13 and the fact that the total variation distance is upper bounded by 1, \(\forall h\geq 2\), with probability at least \(1-\delta/2\), we have

\[\mathop{\mathbb{E}}_{\hat{P}^{k},\pi}\left[f_{h}^{k}(s_{h},a_{h}) \bigg{|}s_{h-1}\right] \leq\mathop{\mathbb{E}}_{\pi}\left[\min\left(\alpha_{k,W}\left\| \hat{\phi}_{h-1}^{k}(s_{h-1},a_{h-1})\right\|_{(U_{h-1,\hat{P}^{k}}^{k})^{-1}},1\right)\right]\] \[+\sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{h-1,h\},[k-W,k-1]}^{ P}}.\] (28)

Similarly, when \(h=1\),

\[\mathop{\mathbb{E}}_{a_{1}\sim\pi}\left[f_{1}^{k}(s_{1},a_{1})\right]\leq \sqrt{A\left(\zeta_{k,W}+2C_{B}\Delta_{1,[k-W,k-1]}^{P}\right)}.\] (29)

Based on Corollary A.9, Equation (28) and \(\tilde{\alpha}_{k,W}=5\alpha_{k,W}\), we have

\[\mathop{\mathbb{E}}_{\pi}\left[\hat{b}_{h}^{k}(s_{h},a_{h}) \bigg{|}s_{h}\right]+\sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{h,h+1\},[k-W, k-1]}^{P}}\] \[\geq\mathop{\mathbb{E}}_{\pi}\left[\min\left(\alpha_{k,W}\left\| \hat{\phi}_{h}^{k}(s_{h},a_{h})\right\|_{(U_{h,\hat{P}^{k}}^{k})^{-1}},1 \right)\right]+\sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{h-1,h\},[k-W,k-1]}^{ P}}\] \[\geq\mathop{\mathbb{E}}_{\hat{P}^{k},\pi}\left[f_{h+1}^{k}(s_{h+ 1},a_{h+1})\bigg{|}s_{h}\right].\] (30)

For the base case \(h=H\), we have

\[\mathop{\mathbb{E}}_{\hat{P}^{k},\pi}\left[\hat{V}_{H,\hat{P}^{k },f^{k}}^{\pi}(s_{H})\bigg{|}s_{h-1},a_{h-1}\right]\] \[=\mathop{\mathbb{E}}_{\hat{P}^{k},\pi}\left[f_{H}^{k}(s_{H},a_{H} )\bigg{|}s_{H-1}\right]\] \[\leq\mathop{\mathbb{E}}_{\pi}\left[b_{H-1}^{k}(s_{H-1},a_{H-1}) |s_{H-1}\right]+\sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{H-1,H\},[k-W,k-1]} ^{P}}\] \[\leq\min\left\{1,\mathop{\mathbb{E}}_{\pi}\left[\hat{Q}_{H-1, \hat{P}^{k},\hat{P}^{k}}^{\pi}(s_{H-1},a_{H-1})\bigg{|}s_{h-1},a_{h-1}\right] \right\}+\sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{H-1,H\},[k-W,k-1]}^{P}}\] \[=\hat{V}_{H-1,\hat{P}^{k},\hat{P}^{k}}^{\pi}(s_{H-1})+\sqrt{ \frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{H-1,H\},[k-W,k-1]}^{P}}.\]

For any step \(h+1,h\geq 2\), assume that \(\mathop{\mathbb{E}}_{\hat{P}^{k},\pi}\left[\hat{V}_{h+1,\hat{P}^{k},f^{k}}^{ \pi}(s_{h+1})\bigg{|}s_{h}\right]\ \leq\ \hat{V}_{h,\hat{P}^{k},[k}^{\pi}(s_{h})\ +\ \sum_{h^{\prime}=h+1}^{H}\sqrt{\frac{3}{ \lambda_{W}}WAC_{B}\Delta_{\{h^{\prime}-1,h^{\prime}\},[k-W,k-1]}^{P}}\) holds. Then, by Jensen's inequality, we obtain

\[\mathop{\mathbb{E}}_{\hat{P}^{k},\pi}\left[\hat{V}_{h,\hat{P}^{k},f^{k}}^{\pi} (s_{h})\bigg{|}s_{h-1},a_{h-1}\right]\]\[\leq\min\left\{1,\underset{\hat{P}^{k},\pi}{\mathbb{E}}\left[f_{h}^{k}(s_ {h},a_{h})+\hat{P}_{h}^{k}\hat{V}_{h+1,\hat{P}^{k},f^{k}}^{\pi}(s_{h},a_{h}) \bigg{|}s_{h-1},a_{h-1}\right]\right\}\] \[\overset{(i)}{\leq}\min\left\{1,\underset{\pi}{\mathbb{E}}\left[ \hat{b}_{h-1}^{k}(s_{h-1},a_{h-1})\right]+\sqrt{\frac{3}{\lambda_{W}}WAC_{B} \Delta_{\{h-1,h\},[k-W,k-1]}^{P}}\right.\] \[\qquad+\underset{\hat{P}^{k},\pi}{\mathbb{E}}\left[\underset{\hat {P}^{k},\pi}{\mathbb{E}}\left[\hat{V}_{h+1,\hat{P}^{k},f^{k}}^{\pi}(s_{h+1}) \bigg{|}s_{h}\bigg{]}\bigg{|}s_{h-1},a_{h-1}\right]\right\}\] \[\overset{(ii)}{\leq}\min\left\{1,\underset{\pi}{\mathbb{E}}\left[ b_{h-1}^{k}(s_{h-1},a_{h-1})\right]+\sqrt{\frac{3}{\lambda_{W}}WAC_{B} \Delta_{\{h-1,h\},[k-W,k-1]}^{P}}\right.\] \[\qquad+\underset{\hat{P}^{k},\pi}{\mathbb{E}}\left[\hat{V}_{h, \hat{P}^{k},\hat{b}^{k}}^{\pi}(s_{h})\bigg{|}s_{h-1},a_{h-1}\right]+\sum_{h^{ \prime}=h+1}^{H}\sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{h^{\prime}-1,h^{ \prime}\},[k-W,k-1]}^{P}}\right\}\] \[=\min\left\{1,\underset{\pi}{\mathbb{E}}\left[\hat{Q}_{h-1,\hat{ P}^{k},\hat{b}^{k}}^{\pi}(s_{h-1},a_{h-1})\right]\right\}+\sum_{h^{\prime}=h}^{H} \sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{h^{\prime}-1,h^{\prime}\},[k-W,k-1 ]}^{P}}\] \[=\hat{V}_{h-1,\hat{P}^{k},\hat{b}^{k}}^{\pi}(s_{h-1})+\sum_{h^{ \prime}=h}^{H}\sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{h^{\prime}-1,h^{ \prime}\},[k-W,k-1]}^{P}},\]

where \((i)\) follows from Equation (30), and \((ii)\) is due to the induction hypothesis.

By induction, we conclude that

\[\hat{V}_{\hat{P}^{k},f^{k}}^{\pi} =\underset{\pi}{\mathbb{E}}\left[f_{1}^{(s)}(s_{1},a_{1})\right]+ \underset{\hat{P}^{k},\pi}{\mathbb{E}}\left[\hat{V}_{2,\hat{P}^{k},f^{k}}^{\pi }(s_{2})\bigg{|}s_{1}\right]\] \[\leq\sqrt{A\left(\zeta_{k,W}+2C_{B}\Delta_{1,[k-W,k-1]}^{P}\right) }+\hat{V}_{\hat{P}^{k},\hat{b}^{k}}^{\pi}+\sum_{h^{\prime}=2}^{H}\sqrt{\frac{ 3}{\lambda_{W}}WAC_{B}\Delta_{\{h^{\prime}-1,h^{\prime}\},[k-W,k-1]}^{P}}.\]

Combining Step 1 and Step 2, we conclude that

\[\bigg{|}V_{P^{*},r}^{\pi}-V_{\hat{P}^{k},r}^{\pi}\bigg{|}\] \[\quad\leq\hat{V}_{\hat{P}^{k},\hat{b}^{k}}^{\pi}+\sqrt{A\left( \zeta_{k,W}+2C_{B}\Delta_{1,[k-W,k-1]}^{P}\right)}+\sum_{h^{\prime}=2}^{H} \sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{h^{\prime}-1,h^{\prime}\},[k-W,k-1 ]}^{P}}.\]

Similarly to Lemma A.15, we can prove that the total variance distance is bounded by \(\hat{V}_{\hat{P}^{k},\hat{b}^{k}}^{\pi_{k}}\) plus a variation budget term as follows. Lemmas A.15 and A.16 together justify the choice of exploration policy for the off-policy exploration.

**Lemma A.16**.: _Fix \(\delta\in(0,1)\), for any \(h\in[H],k\in[K]\), any policy \(\pi\), with probability at least \(1-\delta/2\),_

\[\underset{\overset{s_{h}\sim(P^{k},k,\pi)}{\sim_{h}\sim_{\pi}}} {\mathbb{E}}\left[f_{h}^{k}(s_{h},a_{h})\right]\] \[\quad\leq 2\left(\hat{V}_{\hat{P}^{k},\hat{b}^{k}}^{\pi}+\sum_{h=2}^{H} \sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta_{\{h-1,h\},[k-W,k-1]}^{P}}+\sqrt{A \left(\zeta_{k,W}+2C_{B}\Delta_{1,[k-W,k-1]}^{P}\right)}\right).\]

Proof.: Fix any policy \(\pi\), for any \(h\geq 2\), we have

\[\underset{\overset{s_{h}\sim(P^{k},k,\pi)}{\sim_{h}\sim_{\pi}}} {\mathbb{E}}\left[\hat{Q}_{h,\hat{P}^{k},\hat{b}^{k}}^{\pi}(s_{h},a_{h})\right]\] \[\quad=\underset{\overset{s_{h-1}\sim(P^{k},\pi)}{\sim_{h-1}\sim_{ \pi}}}{\mathbb{E}}\left[\hat{P}_{h}^{k}\hat{V}_{h,\hat{P}^{k},\hat{b}^{k}}^{\pi }(s_{h-1},a_{h-1})\right]\]\[\leq \mathop{\mathbb{E}}_{s_{h-1}\sim(\hat{P}^{k},\hat{p}^{k})}\left[\min \left\{1,\hat{b}^{k}_{h-1}(s_{h-1},a_{h-1})+\hat{P}^{k}_{h-1}\hat{V}^{\pi}_{h, \hat{p}^{k},\hat{b}^{k}}(s_{h-1},a_{h-1})\right\}\right]\] \[= \mathop{\mathbb{E}}_{s_{h-1}\sim(\hat{P}^{k},\hat{p}^{k})}\left[ \hat{Q}^{\pi}_{h-1,\hat{p}^{k},\hat{b}^{k}}(s_{h-1},a_{h-1})\right]\] \[\leq \ldots\] \[\leq \mathop{\mathbb{E}}_{a_{1}\sim\pi}\left[\hat{Q}^{\pi}_{1,\hat{p} ^{k},\hat{b}^{k}}(s_{1},a_{1})\right]\] \[= \hat{V}^{\pi}_{\hat{P}^{k},\hat{b}^{k}}.\] (31)

Hence, for \(h\geq 2\), we have

\[\mathop{\mathbb{E}}_{s_{h}\sim(\hat{P}^{k},n)}\left[f^{k}_{h}(s_{h },a_{h})\right] \stackrel{{(i)}}{{\leq}}\mathop{\mathbb{E}}_{s_{h-1} \sim\pi}\left[\hat{b}^{k}_{h-1}(s_{h-1},a_{h-1})\right]+\sqrt{\frac{3}{\lambda _{W}}WAC_{B}\Delta^{P}_{\{h-1,h\},[k-W,k-1]}}\] \[\stackrel{{(ii)}}{{\leq}}\mathop{\mathbb{E}}_{s_{h- 1}\sim\pi}\left[\hat{Q}^{\pi}_{h-1,\hat{P}^{k},\hat{b}^{k}}(s_{h-1},a_{h-1}) \right]+\sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta^{P}_{\{h-1,h\},[k-W,k-1]}}\] \[\stackrel{{(iii)}}{{\leq}}\hat{V}^{\pi}_{\hat{P}^{k},\hat{b}^{k}}+\sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta^{P}_{\{h-1,h\},[k-W,k-1] }},\] (32)

where \((i)\) follows from Equation (30), \((ii)\) follows from the definition of \(\hat{Q}^{\pi}_{h-1,\hat{P}^{k},\hat{b}^{k}}(s_{h-1},a_{h-1})\), and \((iii)\) follows from Equation (31).

\[\mathop{\mathbb{E}}_{s_{h}\sim(\hat{P}^{k},\hat{b}^{k})}\left[f^{ k}_{h}(s_{h},a_{h})\right]\] \[\leq 2\left(\hat{V}^{\pi}_{\hat{P}^{k},\hat{b}^{k}}+\sum_{h=2}^{H} \sqrt{\frac{3}{\lambda_{W}}WAC_{B}\Delta^{P}_{\{h-1,h\},[k-W,k-1]}}+\sqrt{A \left(\zeta_{k,W}+2C_{B}\Delta^{P}_{1,[k-W,k-1]}\right)}\right),\]

where the last equation follows from Equation (32) and Lemma A.15. 

**Lemma A.17**.: _Denote \(\tilde{\alpha}_{k,W}=5\alpha_{k,W}\), \(\alpha_{k,W}=\sqrt{2WA\zeta_{k,W}+\lambda_{k,W}d}\), and \(\beta_{k,W}=\sqrt{9dA\alpha_{k,W}^{2}+\lambda_{k,W}d}\). For any \(k\in[K]\), policy \(\pi\) and reward \(r\), for all \(h\geq 2\), we have_

\[\mathop{\mathbb{E}}_{(s_{h},a_{h})\sim(\hat{P}^{*,k},\pi)} \left[\hat{b}^{k}_{h}(s_{h},a_{h})\bigg{|}s_{h-1},a_{h-1}\right]\] \[\leq \beta_{k,W}\left\|\phi^{*,k}_{h-1}(s_{h-1},a_{h-1})\right\|_{(W^{ k}_{h-1,\phi^{*,k}})^{-1}}+\sqrt{\frac{A}{d}}\Delta^{\sqrt{P}}_{h-1,[k-W,k-1]},\] (33)

_and for \(h=1\), we have_

\[\mathop{\mathbb{E}}_{a_{1}\sim\pi}\left[\hat{b}^{k}_{1}(s_{1},a_{1 })\right]\leq \sqrt{\frac{9Ad\alpha_{k,W}^{2}}{W}}.\] (34)

Proof.: For \(h=1\),

\[\mathop{\mathbb{E}}_{a_{1}\sim\pi}\left[\hat{b}^{k}_{1}(s_{1},a_{1 })\right]\stackrel{{(i)}}{{\leq}}\sqrt{\mathop{\mathbb{E}}_{a_{1 }\sim\pi}\left[\hat{b}^{k}_{1}(s_{1},a_{1})^{2}\right]}\stackrel{{ (ii)}}{{\leq}}\sqrt{\frac{9Ad\alpha_{k,W}^{2}}{W}},\]

where \((i)\) follows from Jensen's inequality, and \((ii)\) follows from the importance sampling.

[MISSING_PAGE_EMPTY:30]

[MISSING_PAGE_EMPTY:31]

[MISSING_PAGE_EMPTY:32]

\[\leq\sqrt{K\left(2WA\zeta_{k,W}+\lambda_{k,W}d\right)\left[\frac{2KAd}{W} \log(1+\frac{W}{d\lambda_{0}})+\frac{2W}{\lambda_{W}}\Delta_{\{h\},[K]}^{\phi}+ \frac{2W^{2}}{\lambda_{W}^{2}}\Delta_{\{h\},[K]}^{\phi}\right]}\] (44)

where the second equation follows from Equation (43).

Then we derive the following bound:

\[\sum_{k=1}^{K}V_{P^{*,k},f^{k}}^{\pm}\]\[=\sum_{k\in[K]}\sum_{h\in[H]}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k}, \tilde{\pi}^{k})}\left[f_{h}^{k}(s_{h},a_{h})\right]\] \[\overset{(i)}{\leq}\sum_{k\in[K]}\left\{\sum_{h=2}^{H}\left[ \mathbb{E}_{(s_{h-1},a_{h-1})\sim(P^{*,k},\tilde{\pi}^{k})}\left[\alpha_{k,W} \left\|\phi_{h-1}^{*,k}(s_{h-1},a_{h-1})\right\|_{(U_{h-1}^{k},\phi^{*,k})^{-1 }}\right]+\sqrt{\frac{1}{2k}WAC_{B}\Delta_{[h-1,h],[k-W,k-1]}^{P}}\right]\] \[+\sqrt{A\left(\zeta_{k,W}+\frac{1}{2}C_{B}\Delta_{1,[k-W,k-1]}^{P} \right)}\right\}\] \[\leq\sum_{h=1}^{H-1}\sum_{k\in[K]}\mathbb{E}_{(s_{h},a_{h})\sim(P^ {*,k},\tilde{\pi}^{k})}\left[\alpha_{k,W}\left\|\phi_{h}^{*,k}(s_{h},a_{h}) \right\|_{(U_{h,\phi^{*,k}}^{k})^{-1}}\right]\] \[\qquad\quad+\sum_{k\in[K]}\sum_{h=1}^{H}\sqrt{WAC_{B}\Delta_{[h-1, h],[k-W,k-1]}^{P}}+\sum_{k\in[K]}\sqrt{A\zeta_{k,W}}\] \[\overset{(ii)}{\leq}\sum_{h=1}^{H-1}\sqrt{K\left(2WA\zeta_{k,W}+ \lambda_{k,W}d\right)\left[\frac{2AKd}{W}\log(1+\frac{W}{d\lambda_{0}})+\frac {2W}{\lambda_{W}}\Delta_{\{h\},[K]}^{\phi}+\frac{2W^{2}}{\lambda_{W}^{2}} \Delta_{\{h\},[K]}^{\phi}\right]}\] \[+\sum_{k\in[K]}\sum_{h=1}^{H}\sqrt{WAC_{B}\Delta_{[h-1,h],[k-W,k-1 ]}^{P}}+\sum_{k\in[K]}\sqrt{A\zeta_{k,W}}\] \[\leq\sqrt{K\left(2WA\zeta_{k,W}+\lambda_{k,W}d\right)}\left[H \sqrt{\frac{2AKd}{W}\log(1+\frac{W}{d\lambda_{0}})}+\sqrt{\frac{2HW}{\lambda_{ W}}\Delta_{[H],[K]}^{\phi}}+\sqrt{\frac{2HW^{2}}{\lambda_{W}^{2}}\Delta_{[H],[K]}^{ \phi}}\right]\] \[+\sqrt{W^{3}AC_{B}}\Delta_{[H],[K]}^{\sqrt{P}}+\sum_{k\in[K]} \sqrt{A\zeta_{k,W}}\] \[\leq O\left(\sqrt{K(A\log(|\Phi||\Psi|KH/\delta)+d^{2})}\left[H \sqrt{\frac{2AKd}{W}\log(W)}+\sqrt{HW^{2}\Delta_{[H],[K]}^{\phi}}\right]+ \sqrt{W^{3}AC_{B}}\Delta_{[H],[K]}^{\sqrt{P}}\right),\] (45)

where \((i)\) follows from Lemma A.14, and \((ii)\) follows from Equation (44).

**Step 2:** We next bound \(\sum_{k=1}^{K}V_{P^{*,k},\tilde{p}^{k}}^{\tilde{\pi}^{k}}\) via an **auxiliary anchor representation**.

Similarly to the proof **Step 1**, we further bound \(\sum_{k\in[K]}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\tilde{\pi}^{k})}\left[ \beta_{k,W}\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W_{h,\phi^{*,k}}^{k})^{ -1}}\right]\).

We define \(W_{h,\phi^{*,k}}^{k,W}=\sum_{i=1}^{k-1}(k-W)\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,i},\tilde{\pi}^{i})}\left[\phi_{h}^{*,k}(s_{h},a_{h})\phi_{h}^{*,k}(s_{h},a_{ h})^{\top}\right]+\lambda_{k,W}I_{d}\) and \(\tilde{W}_{h,\phi^{*,k}}^{k,W,t}=\sum_{i=tW+1}^{k-1}\mathbb{E}_{(s_{h},a_{h}) \sim(P^{*,i},\tilde{\pi}^{i})}\left[\phi_{h}^{*,k}(s_{h},a_{h})\phi_{h}^{*,k}( s_{h},a_{h})^{\top}\right]+\lambda_{k,W}I_{d}\). We first note that for any \(h\), we have

\[\sum_{k\in[K]}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\tilde{\pi}^{ k})}\left[\beta_{k,W}\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W_{h,\phi^{*,k}}^{ k})^{-1}}\right]\] \[=\sqrt{\sum_{k\in[K]}\beta_{k,W}^{2}\sum_{k\in[K]}\mathbb{E}_{(s_ {h},a_{h})\sim(P^{*,k},\tilde{\pi}^{k})}\left[\left\|\phi_{h}^{*,k}(s_{h},a_{h}) \right\|_{(W_{h,\phi^{*,k}}^{k,W})^{-1}}\right]^{2}}\] \[=\sqrt{\sum_{k\in[K]}\beta_{k,W}^{2}\sum_{t=0}^{\lfloor K/W\rfloor }\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\tilde{\pi}^{k})} \left[\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W_{h,\phi^{*,k}}^{k,W})^{-1}} \right]^{2}}\] (46)

The \(\phi^{\star,k}\) and \(W\) in Equation (46) both change with the round index \(k\). To deal with this issue, we decompose it as follows. We first derive the following equation:

\[\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\tilde{\pi}^{k})} \left[\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W_{h,\phi^{*,k}}^{k,W})^{-1}}^ {2}\right]\]\[-\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\hat{ \pi}^{k})}\left[\left\|\phi_{h}^{*,tW+1}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^ {*},tW+1})^{-1}}^{2}\right]\] \[=\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\hat{ \pi}^{k})}\left[\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^{ *},tW+1})^{-1}}^{2}-\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^ {*},tW+1})^{-1}}^{2}\right]\] \[=\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\hat{ \pi}^{k})}\left[\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^ {*},tW+1})^{-1}}^{2}-\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h, \phi^{*},tW+1})^{-1}}^{2}\right.\right.\] \[+\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^{ *},tW+1})^{-1}}^{2}-\left\|\phi_{h}^{*,tW+1}(s_{h},a_{h})\right\|_{(W^{k,W}_{h, \phi^{*},tW+1})^{-1}}^{2}\right].\] \[=\underbrace{\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim( P^{*,k},\hat{\pi}^{k})}\left[\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h, \phi^{*},k})^{-1}}^{2}-\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^{*},tW+1})^{-1}}^{2}\right]}_{(III)}\] \[+\underbrace{\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim( P^{*,k},\hat{\pi}^{k})}\left[\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h, \phi^{*},tW+1})^{-1}}^{2}-\left\|\phi_{h}^{*,tW+1}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^{*},tW+1})^{-1}}^{2}\right]}_{(IV)}.\] (47)

**For term \((IV)\)**, we have

\[\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\hat{ \pi}^{k})}\left[\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^{ *},tW+1})^{-1}}^{2}-\left\|\phi_{h}^{*,tW+1}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^{*},tW+1})^{-1}}^{2}\right]\] \[\leq\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k}, \hat{\pi}^{k})}\left[\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{2}\left\|(W^{ k,W}_{h,\phi^{*},tW+1})^{-1}\right\|_{2}\left\|\phi_{h}^{*,k}(s_{h},a_{h})-\phi_{h}^{*, tW+1}(s_{h},a_{h})\right\|_{2}\right]\] \[+\left\|\phi_{h}^{*,k}(s_{h},a_{h})-\phi_{h}^{*,tW+1}(s_{h},a_{h} )\right\|_{2}\left\|\left(W^{k,W}_{h,\phi^{*},tW+1})^{-1}\right\|_{2}\left\| \phi_{h}^{*,tW+1}(s_{h},a_{h})\right\|_{2}\right]\] \[\leq\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k}, \hat{\pi}^{k})}\left[\frac{2}{\lambda_{W}}\left\|\phi_{h}^{*,k}(s_{h},a_{h})- \phi_{h}^{*,tW+1}(s_{h},a_{h})\right\|_{2}\right]\] \[\leq\frac{2W}{\lambda_{W}}\Delta_{\{h\},[tW+1,t(W+1)-1]}^{\phi},\] (48)

where \((i)\) follows from the property of the matrix norms induced by vector \(\ell_{2}\)-norm.

**For term \((III)\)**, we derive the following bound:

\[\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\hat{ \pi}^{k})}\left[\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^{* },k})^{-1}}^{2}-\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^{*},tW+1})^{-1}}^{2}\right]\] \[=\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\hat {\pi}^{k})}\left[\phi_{h}^{*,k}(s_{h},a_{h})^{\top}\left((W^{k,W}_{h,\phi^{*},k})^ {-1}-(W^{k,W}_{h,\phi^{*},tW+1})^{-1}\right)\phi_{h}^{*,k}(s_{h},a_{h})\right]\] \[=\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\hat {\pi}^{k})}\left[\phi_{h}^{*,k}(s_{h},a_{h})^{\top}(W^{k,W}_{h,\phi^{*},k})^{-1}\right.\] \[\times\left(W^{k,W}_{h,\phi^{*,tW+1}}-W^{k,W}_{h,\phi^{*},k} \right)(W^{k,W}_{h,\phi^{*,tW+1}})^{-1}\phi_{h}^{*,k}(s_{h},a_{h})\right]\]\[\leq 2d\log(1+\frac{W}{d\lambda_{0}}),\] (50)

where the last equation follows from Lemma D.2.

Then combining Equations (39) to (41) and (50), we have

\[\sum_{k=tW+1}^{(t+1)W}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\bar{ \pi}^{k})}\left[\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(W^{k,W}_{h,\phi^{* },k})^{-1}}\right]\] \[\leq 2d\log(1+\frac{W}{d\lambda_{0}})+\frac{2W}{\lambda_{W}}\Delta_{(h ),[tW+1,t(W+1)-1]}^{\phi}+\sum_{k=tW+1}^{(t+1)W}\sum_{i=1\lor k-W}^{k-1}\frac{2 }{\lambda_{W}^{2}}\Delta_{h,[tW+1,k-1]}^{\phi}.\] (51)

Substituting Equation (51) into Equation (46), we have

\[\sum_{k\in[K]}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\bar{\pi}^{k})}\left[ \beta_{k,W}\left\|\phi_{h}^{*,k}(s_{h},a_{h})\right\|_{(U^{k}_{h,\phi^{*},k})^ {-1}}\right]\]\[\leq\sqrt{\sum_{k\in[K]}\beta_{k,W}^{2}\sum_{t=0}^{\lfloor K/W\rfloor} \sum_{k=tW+1}^{\lfloor(t+1)W\rrbracket}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k}, \hat{\pi}^{k})}\left[\left\|\hat{\phi}_{h}^{\star,k}(s_{h},a_{h})\right\|_{(U_{h,\phi^{\star},k}^{h,W})^{-1}}\right]^{2}}\] \[\leq \sqrt{\sum_{k\in[K]}\beta_{k,W}^{2}\sum_{t=0}^{\lfloor K/W\rfloor} \left[2d\log(1+\frac{W}{d\lambda_{0}})+\frac{2W}{\lambda_{W}}\Delta_{(h), \{tW+1,(W+1)-1\}+\sum_{k=tW+1}^{(t+1)W}\sum_{i=1Vk-W}^{2}\frac{\lambda}{\lambda _{W}^{0}}\Delta_{[iW+1,k-1]}^{\phi}\right]}}\] \[\leq \sqrt{K(9dA(2WA\mathcal{K}_{k,W}+\lambda_{k,W}d)+\lambda_{k,W}d) \left[\frac{2K\delta}{W^{4}}\log(1+\frac{W}{d\lambda_{0}})+\frac{2W}{d\lambda _{W}^{0}}\Delta_{(h),\lceil K\rceil}^{\phi}+\frac{2W^{2}}{\lambda_{W}^{0}} \Delta_{(h),\lceil K\rceil}^{\phi}\right]}.\] (52)

where the second equation follows from Equation (51).

Then, we derive the following bound:

\[\sum_{k\in[K]}V_{P^{*,k},\hat{b}^{k}}^{\hat{\pi}^{k}}=\sum_{k\in[ K]}\sum_{h\in[H]}\mathbb{E}_{(s_{h},a_{h})\sim(P^{*,k},\hat{\pi}^{k})}\left[ \hat{b}_{h}^{k}(s_{h},a_{h})\right]\] \[\leq\sum_{h=1}^{H-1}\sum_{k\in[K]}\mathbb{E}_{(s_{h},a_{h})\sim(P^ {*,k},\hat{\pi}^{k})}\left[\beta_{k,W}\left\|\phi_{h}^{\star,k}(s_{h},a_{h}) \right\|_{(W_{h,\phi^{\star},k}^{k})^{-1}}\right]\] \[+\sqrt{\frac{A}{d}\Delta_{h-1,[k-W,k-1]}^{P}}\Bigg{\}}+\sqrt{ \frac{9Ad\alpha_{k,W}^{2}}{w}}\Bigg{\}}\] \[\leq\sum_{h=1}^{H-1}\sum_{k\in[K]}\mathbb{E}_{(s_{h},a_{h})\sim(P^ {*,k},\hat{\pi}^{k})}\left[\beta_{k,W}\left\|\phi_{h}^{\star,k}(s_{h},a_{h}) \right\|_{(W_{h,\phi^{\star},k}^{k})^{-1}}\right]\] \[+W\sqrt{\frac{A}{d}\Delta_{[H],[K]}^{\sqrt{P}}}+\sum_{k\in[K]} \sqrt{\frac{9Ad\alpha_{k,W}^{2}}{W}}\] \[\overset{(ii)}{\leq}\sqrt{K\left(9dA(2WA\zeta_{k,W}+\lambda_{k,W}d )+\lambda_{k,W}d\right)}\left[H\sqrt{\frac{2Kd}{W}\log(1+\frac{W}{d\lambda_{0}} )}+\sqrt{\frac{2HW}{\lambda_{W}}\Delta_{[H],[K]}^{\phi}}\right.\] \[\left.+\sqrt{\frac{2HW^{2}}{\lambda_{W}^{2}}}\Delta_{[H],[K]}^{ \phi}\right]+W\sqrt{\frac{A}{d}}\Delta_{[H],[K]}^{\sqrt{P}}\] \[\leq O\left(\sqrt{KdA(A\log(|\Phi||\Psi|KH/\delta)+d^{2})}\left[H \sqrt{\frac{Kd}{W}\log(W)}+\sqrt{HW^{2}\Delta_{[H],[K]}^{\phi}}\right]\right.\] \[\left.+W\sqrt{A}\Delta_{[H],[K]}^{\sqrt{P}}\right),\]

where \((i)\) follows from Lemma A.17, and \((ii)\) follows from Equation (52).

Finally, combining Equations (37), (45) and (53), we have

\[\sum_{k=1}^{K}\tilde{V}_{P^{k},\hat{b}^{k}}^{\hat{\pi}^{k}}\] \[+O\big{(}\sqrt{KdA(A\log(|\Phi||\Psi|KH/\delta)+d^{2})}\big{[}H \sqrt{\frac{2Ad\log(W)}{W}+\sqrt{HW^{2}\Delta_{[H],[K]}^{\phi}}}\big{]}+W \sqrt{A}\Delta_{[H],[K]}^{\sqrt{P}}\big{)}\] \[\leq O\big{(}\sqrt{KdA(A\log(|\Phi||\Psi|KH/\delta)+d^{2})}\big{[}H \sqrt{\frac{Kd}{W}\log(W)}+\sqrt{HW^{2}\Delta_{[H],[K]}^{\phi}}\big{]}+\sqrt{ W^{3}A}\Delta_{[H],[K]}^{\sqrt{P}}\big{)}.\]

The following visitation probability difference lemma is similar to lemma 5 in Fei et al. (2020), but we remove their Assumption 1.

**Lemma A.19**.: _For any transition kernels \(\{P_{h}\}_{h=1}^{H}\),\(h\in[H],j\in[h-1],s_{h}\in\mathcal{S}\) and policies \(\{\pi_{i}\}_{i=1}^{H}\) and \(\pi_{j}^{\prime}\), we have_

\[\left|P_{1}^{\pi_{1}}\dots P_{j}^{\pi_{j}}\dots P_{h-1}^{\pi_{h-1}}(s_{h})-P_{1 }^{\pi_{1}}\dots P_{j}^{\pi_{j}^{\prime}}\dots P_{h-1}^{\pi_{h-1}}(s_{h}) \right|\leq\max_{s\in\mathcal{S}}\left\|\pi_{j}(\cdot|s)-\pi_{j}^{\prime}(\cdot| s)\right\|_{TV}\]Proof.: To prove this lemma, the only difference from lemma is that we need to show \(\max_{s_{j}}\sum_{s_{j+1}}|P_{j}^{\pi_{j}}(s_{j+1}|s_{j})-P_{j}^{\pi_{j}^{\prime} }(s_{j+1}|s_{j})|\leq 2\max_{s\in\mathcal{S}}\left\|\pi_{j}(\cdot|s)-\pi_{j}^{ \prime}(\cdot|s)\right\|_{TV}\) holds without assumption. We show this as follows:

\[\max_{s_{j}}\sum_{s_{j+1}}|P^{\pi_{j}}(s_{j+1}|s_{j})-P^{\pi_{j}^{ \prime}}(s_{j+1}|s_{j})|\] \[=\max_{s_{j}}\sum_{s_{j+1}}|\sum_{a}P(s_{j+1}|s_{j},a)\pi_{j}(a|s_ {j})-\sum_{a}P(s_{j+1}|s_{j},a)\pi_{j}^{\prime}(a|s_{j})|\] \[\leq\max_{s_{j}}\sum_{s_{j+1}}\sum_{a}P(s_{j+1}|s_{j},a)|\pi_{j}( a|s_{j})-\pi_{j}^{\prime}(a|s_{j})|\] \[=\max_{s_{j}}\sum_{a}\sum_{s_{j+1}}P(s_{j+1}|s_{j},a)|\pi_{j}(a|s_ {j})-\pi_{j}^{\prime}(a|s_{j})|\] \[=\max_{s_{j}}\sum_{a}|\pi_{j}(a|s_{j})-\pi_{j}^{\prime}(a|s_{j})| =2\max_{s\in\mathcal{S}}\|\pi_{j}(\cdot|s)-\pi_{j}^{\prime}(\cdot|s)\|_{TV}.\]

## Appendix B Further Discussion and Proof of Corollary 4.5

In this section, we first provide a detailed version and further discussion of Corollary 4.5 in Appendix B.1, then present the proof in Appendix B.2, and finally present an interesting special case in Appendix B.3.

### Further Discussion of Corollary 4.5

We present a detailed version of Corollary 4.5 as follows. Let \(\Pi_{[1,K]}(N)=\min\{K,\max\{1,N\}\}\) for any \(K,N\in\mathbb{N}\).

**Corollary B.1** (Detailed version of Corollary 4.5).: _Under the same conditions of Theorem 4.4, if the variation budgets are known, then for different variation budget regimes, we can select the hyper-parameters correspondingly to attain the optimality for both \((I)\) w.r.t. \(W\) and \((II)\) w.r.t. \(\tau\) in Equation (3). For \((I)\), with \(W=\Pi_{[1,K]}(\lfloor H^{\frac{1}{3}}d^{\frac{1}{3}}K^{\frac{1}{3}}(\Delta^{ \sqrt{P}}+\Delta^{\phi})^{-\frac{1}{3}}\rfloor)\), part \((I)\) is upper-bounded by_

\[\begin{cases}\sqrt{\frac{H^{4}d^{2}A}{K}(A+d^{2})},&\left(\Delta^{\sqrt{P}}+ \Delta^{\phi}\right)\leq\frac{Hd}{K^{2}},\\ H^{2}d^{\frac{5}{3}}d^{\frac{1}{2}}(A+d^{2})^{\frac{1}{2}}(HK)^{-\frac{1}{6}} \left(\Delta^{\sqrt{P}}+\Delta^{\phi}\right)^{\frac{1}{6}},&\left(\Delta^{ \sqrt{P}}+\Delta^{\phi}\right)>\frac{Hd}{K^{2}},\end{cases}\] (54)

_For \((II)\) in Equation (3), with \(\tau=\Pi_{[1,K]}(\lfloor K^{\frac{2}{3}}(\Delta^{P}+\Delta^{\pi})^{-\frac{2}{ 3}}\rfloor)\), part \((II)\) is upper bounded by_

\[\begin{cases}\frac{2H}{\sqrt{K}},&\left(\Delta^{P}+\Delta^{\pi}\right)\leq \frac{1}{\sqrt{K}},\\ 2H^{\frac{1}{3}}(HK)^{-\frac{1}{3}}(\Delta^{P}+\Delta^{\pi})^{\frac{1}{3}},& \frac{1}{\sqrt{K}}<(\Delta^{P}+\Delta^{\pi})\leq K,\\ H+\frac{H(\Delta^{P}+\Delta^{\pi})}{K},&K<(\Delta^{P}+\Delta^{\pi})\end{cases}\] (55)

_For any \(\epsilon\geq 0\), if nonstationarity is not significantly large, i.e., there exists a constant \(\gamma<1\) such that \((\Delta^{P}+\Delta^{\pi})\leq(2HK)^{\gamma}\) and \((\Delta^{\sqrt{P}}+\Delta^{\phi})\leq(2HK)^{\gamma}\), then PORTAL can achieve \(\epsilon\)-average suboptimal with polynomial trajectories._

As a direct consequence of Theorem 4.4, Corollary 4.5 indicates that if variation budgets are known, then the agent can choose the best hyper-parameters directly based on the variation budgets. The \(\operatorname{Gap_{\text{Ave}}}\) can be different depending on which regime the variation budgets fall into, as can be seen in Equations (54) and (55).

At the high level, we further explain how the window size \(W\) depends on the variations of environment as follows. If the nonstationarity is moderate and not significantly large, Corollary 4.5 indicates that for any \(\epsilon\), Algorithm 1 achieves \(\epsilon\)-average suboptimal with polynomial trajectories (see the specific form in Equation (59) in Appendix B.2). If the environment is near stationary and the variation is relatively small, i.e., \((\Delta^{\sqrt{P}}+\Delta^{\phi})\leq Hd/K^{2},(\Delta^{P}+\Delta^{\pi})\leq 1 /\sqrt{K}\), then the best window size and the policy restart period \(\tau\) are both \(K\). This indicates that the agent does not need to take any forgetting rules to handle the variation. Then the \(\mathrm{Gap}_{\mathrm{Ave}}\) reduces to \(\tilde{O}\left(\sqrt{H^{4}d^{2}A(A+d^{2})/K}\right)\), which matches the result under a stationary environment.2

Footnote 2: We convert the sample complexity bound under infinite horizon MDPs in Uehara et al. (2022) to the average dynamic suboptimality gap under episodic MDPs.

Furthermore, it is interesting to consider a special mildly changing environment, in which the representation \(\phi^{\star}\) stays identical and only the state-embedding function \(\mu^{\star,k}\) changes over time. The average dynamic suboptimality gap in Equation (3) reduces to

\[\tilde{O}\Big{(}\underbrace{\sqrt{\frac{H^{4}d^{2}A(A+d^{2})}{W}+\sqrt{\frac{ H^{2}W^{3}A}{K^{2}}\Delta^{\sqrt{P}}}{\kappa^{2}}}}_{(I)}+\underbrace{\frac{H }{\sqrt{\tau}}+\frac{H\tau(\Delta^{P}+\Delta^{\pi})}{K}}_{(II)}\Big{)}.\]

The part \((II)\) is the same as \((II)\) in Equation (3) and by choosing the best window size of \(\overline{W}=H^{\frac{1}{2}}d^{\frac{1}{2}}(A+d^{2})^{\frac{1}{4}}K^{\frac{1}{ 2}}(\Delta^{\sqrt{P}})^{-\frac{1}{2}}\), part \((I)\) becomes

\[\tilde{O}\left(H^{2}d^{\frac{3}{4}}A^{\frac{1}{2}}\left(A+d^{2}\right)^{\frac {3}{8}}(HK)^{-\frac{1}{4}}\left(\Delta^{\sqrt{P}}\right)^{\frac{1}{4}}\right).\] (56)

Compared with the second regime in Equation (54), Equation (56) is much smaller, benefited from identical representation function \(\phi^{\star}\). In this way, samples in previous rounds can help to estimate the representation space so that \(\overline{W}\) can be larger than \(W\) in terms of the order of \(K\), which yields efficiency gain compared with changing \(\phi^{\star}\).

On the other hand, if the nonstationarity is significantly large, for example, scales linearly with \(K\), then for each round, the previous samples cannot help to estimate current best policy. Thus, the best \(W\) and \(\tau\) are both 1, and the average dynamic suboptimality gap reduces to \(\tilde{O}\left(\sqrt{H^{4}d^{2}A\left(A+d^{2}\right)}\right)\). This indicates that for a fixed small accuracy \(\epsilon\geq 0\), no matter how large the round \(K\) is, Algorithm 1 can never achieve \(\epsilon\)-average suboptimality.

### Proof of Corollary b.1 (i.e., Detailed Version of Corollary 4.5)

If variation budgets are known, for different variation budgets regimes, we can tune the hyperparameters correspondingly to reach the optimality for both the term \((I)\) that contains \(W\) and the term \((II)\) that contains \(\tau\).

For the first term \((I)\) in Equation (14), there are two regimes:

* Small variation: \(\left(\Delta^{\sqrt{P}}+\Delta^{\phi}\right)\leq\frac{Hd}{K^{2}}\),
* The best window size \(W\) is \(K\), which means that the variation is pretty mild and the environment is near stationary. In this case, by choosing window size \(W=K\), the agent takes no forgetting rules to handle the variation. Then the first term \((I)\) reduces to \(\sqrt{\frac{H^{4}d^{2}A}{K}\left(A+d^{2}\right)}\), which matches the result under a stationary environment.3 Footnote 3: We convert the regret bound under infinite horizon MDPs in Uehara et al. (2022) to the average dynamic suboptimality gap under episodic MDP.
* Then for any \(\epsilon\geq 0\), with \(HK\) no more than \(\tilde{O}\left(\frac{H^{5}d^{2}A(A+d^{2})}{\epsilon^{2}}\right)\), term \((I)\leq\epsilon\).
* Large variation: \(\left(\Delta^{\sqrt{P}}+\Delta^{\phi}\right)>\frac{Hd}{K^{2}}\).
* By choosing the window size \(W=H^{\frac{1}{3}}d^{\frac{1}{3}}K^{\frac{1}{3}}\left(\Delta^{\sqrt{P}}+ \Delta^{\phi}\right)^{-\frac{1}{3}}\), the term \((I)\) reduces to \(H^{2}d^{\frac{1}{6}}A^{\frac{1}{2}}\left(A+d^{2}\right)^{\frac{1}{2}}\left(HK \right)^{-\frac{1}{6}}\left(\Delta^{\sqrt{P}}+\Delta^{\phi}\right)^{\frac{1}{6}}\).
* Since \(\left(\Delta^{\sqrt{P}}+\Delta^{\phi}\right)\leq 2HK\), there exists \(\gamma\leq 1\) s.t. \(\left(\Delta^{\sqrt{P}}+\Delta^{\phi}\right)\leq(2HK)^{\gamma}\). Then for any \(\epsilon\geq 0\), if \(\gamma\neq 1\), with \(HK\) no more than \(\tilde{O}\left(\frac{d^{\frac{\gamma\cdot\cdot\gamma}{H}+\frac{1\gamma\cdot \gamma}{H}+\frac{1\gamma\cdot\gamma}{H}+\frac{1\gamma\cdot\gamma}{H}+\frac{1 \gamma\cdot\gamma}{H}}}{\epsilon^{\frac{\gamma\cdot\cdot\gamma}{H}+\frac{1 \gamma\cdot\gamma}{H}}}\right)\), term \((I)\leq\epsilon\).
For the second term \((II)\) in Equation (14), there are three regimes elaborated as follows:

* Small variation: \((\Delta^{P}+\Delta^{\pi})\leq\frac{1}{\sqrt{K}}\),
* The best policy restart period \(\tau\) is \(K\), which means that the variation is pretty mild and the agent does not need to handle the variation. Then term \((II)\leq\frac{2H}{\sqrt{K}}\).
* Then for any \(\epsilon\geq 0\), with \(HK\) no more than \(\tilde{O}\left(\frac{H^{2}}{\epsilon^{2}}\right)\), term \((II)\leq\epsilon\).
* Moderate variation: \(\frac{1}{\sqrt{K}}<(\Delta^{P}+\Delta^{\pi})\leq K\),
* The best policy restart period \(\tau=K^{\frac{2}{3}}(\Delta^{P}+\Delta^{\pi})^{-\frac{2}{3}}\), and the term \((II)\) reduces to \(2HK^{-\frac{1}{3}}(\Delta^{P}+\Delta^{\pi})^{\frac{1}{3}}\).
* Since \(\left(\Delta^{P}+\Delta^{\pi}\right)\leq 2HK\), there exists \(\gamma\leq 1\) s.t. \(\left(\Delta^{P}+\Delta^{\pi}\right)\leq(2HK)^{\gamma}\). Then the term \((II)\leq 4H^{\frac{1}{3}}(HK)^{-\frac{1-\gamma}{3}}\). Then for any \(\epsilon\geq 0\), if \(\gamma\neq 1\), with \(HK\) no more than \(\tilde{O}\left(\frac{H^{\frac{1}{3}}\gamma}{\epsilon^{\frac{1}{2-\gamma}}}\right)\), term \((II)\leq\epsilon\).
* Large variation: \(K<(\Delta^{P}+\Delta^{\pi})\),
* The variation budgets scale linearly with \(K\), which indicates that the nonstationarity of the environment is significantly large and lasts for the entire rounds. Hence in each round, the previous sample can not help to estimate the current best policy. So the best policy restart period \(\tau=1\), and the second term \((II)\) reduces to \(H+\frac{H(\Delta^{P}+\Delta^{\pi})}{K}=O(H)\), which implies that Algorithm 1 can never achieve small average dynamic suboptimality gap for any large \(K\).

In conclusion, the first term is upper bounded by

\[(I)\leq\begin{cases}\sqrt{\frac{H^{4}d^{2}A}{K}\left(A+d^{2}\right)},&\left( \Delta^{\sqrt{P}}+\Delta^{\phi}\right)\leq\frac{Hd}{K^{2}},\\ H^{2}d^{\frac{5}{3}}A^{\frac{1}{2}}\left(A+d^{2}\right)^{\frac{1}{3}}(HK)^{- \frac{1}{6}}\left(\Delta^{\sqrt{P}}+\Delta^{\phi}\right)^{\frac{1}{6}},&\left( \Delta^{\sqrt{P}}+\Delta^{\phi}\right)>\frac{Hd}{K^{2}},\end{cases}\] (57)

and the second term is upper bounded by

\[(II)\leq\begin{cases}\frac{2H}{\sqrt{K}},&\left(\Delta^{P}+\Delta^{\pi} \right)\leq\frac{1}{\sqrt{K}},\\ 2H^{\frac{4}{3}}(HK)^{-\frac{1}{3}}(\Delta^{P}+\Delta^{\pi})^{\frac{1}{3}},& \frac{1}{\sqrt{K}}<(\Delta^{P}+\Delta^{\pi})\leq K,\\ H+\frac{H(\Delta^{P}+\Delta^{\pi})}{K},&K<(\Delta^{P}+\Delta^{\pi})\end{cases}\] (58)

In addition, if the variation budgets are not significantly large, i.e. scale linearly with \(K\), for any \(\epsilon\geq 0\), Algorithm 1 can achieve \(\epsilon\)-average dynamic suboptimality gap with at most polynomial samples. Specifically, if there exists a constant \(\gamma<1\) such that the variation budgets satisfying \(\left(\Delta^{P}+\Delta^{\pi}\right)\leq(2HK)^{\gamma}\) and \(\left(\Delta^{\sqrt{P}}+\Delta^{\phi}\right)\leq(2HK)^{\gamma}\), then to achieve \(\epsilon\)-average dynamic suboptimality gap, i.e.,

[MISSING_PAGE_EMPTY:41]

* (\(W^{\uparrow},\tau^{\dagger}\)): (\(W^{\dagger},\tau^{\dagger}\)) denotes the set of best choices of the window size \(W\) and the policy restart period \(\tau\) in feasible set that maximize \(\sum_{i=1}^{\lceil K/M\rceil}R_{i}(W,\tau)\).

Then we can decompose the average dynamic suboptimality gap as

\[\mathrm{Gap}_{\mathrm{Ave}}(K) =\frac{1}{K}\sum_{k\in[K]}\left[V_{P^{*},k}^{\pi^{*}}-V_{P^{*},k}^ {\pi^{k}}\right]\] \[=\frac{1}{K}\underbrace{\sum_{k=1}^{K}V_{P^{*},k}^{\pi^{*}}- \sum_{i=1}^{\lceil K/M\rceil}\mathbb{E}\left[R_{i}(\overline{W},\overline{\tau })\right]}_{(I)}+\frac{1}{K}\underbrace{\sum_{i=1}^{\lceil K/M\rceil}\mathbb{ E}[R_{i}(\overline{W},\overline{\tau})]-\sum_{i=1}^{\lceil K/M\rceil}\mathbb{ E}[R_{i}(W_{i},\tau_{i})]}_{(II)},\]

where the last inequality follows because if \(\{\pi^{k}\}_{k=1}^{K}\) is the output of Algorithm 3 with the chosen window size \(\{W_{i}\}_{i=1}^{\lceil T/M\rceil}\), \(\mathbb{E}[R_{i}(W_{i},\tau_{i})]=\mathbb{E}\left[\sum_{k=(i-1)M+1}^{\min\{iM, K\}}V_{1}^{k}\right]=\sum_{k=(i-1)M+1}^{\min\{iM,K\}}V_{P^{*},k}^{\pi^{k}}\) holds.

We next bound Terms \((I)\) and \((II)\) separately.

**Term (I):** We derive the following bound:

\[\frac{1}{K} \left\{\sum_{k=1}^{K}V_{1}^{\pi^{*,k},k}-\sum_{i=1}^{\lceil K/M \rceil}R_{i}(\overline{W},\overline{\tau})\right\}\] \[\stackrel{{(i)}}{{\leq}}\tilde{O}\left(\sqrt{\frac{ H^{4}d^{2}A}{\overline{W}}\left(A+d^{2}\right)}+\sqrt{\frac{H^{3}dA}{K}\left(A+d^{2} \right)\overline{W}^{2}\Delta_{[H],[K]}^{\phi}}+\sqrt{\frac{H^{2}\overline{W}^ {3}A}{K^{2}}}\Delta_{[H],[K]}^{\sqrt{P}}\right)\] \[+\tilde{O}\left(\frac{2H}{\sqrt{\overline{\tau}}}+\frac{3H \overline{\tau}}{K}(\Delta_{[H],[K]}^{P}+\Delta_{[H],[K]}^{\pi})\right)\] \[\stackrel{{(ii)}}{{\leq}}\tilde{O}\left(\sqrt{\frac{ H^{4}d^{2}A}{W^{\star}}\left(A+d^{2}\right)}+\sqrt{\frac{H^{3}dA}{K}\left(A+d^{2} \right){W^{\star}}^{2}\Delta_{[H],[K]}^{\phi}}+\sqrt{\frac{H^{2}{W^{\star}}^{ 3}A}{K^{2}}}\Delta_{[H],[K]}^{\sqrt{P}}\right)\] \[+\tilde{O}\left(\frac{2H}{\sqrt{\overline{\tau}^{\ast}}}+\frac{3H \tau^{\star}}{K}(\Delta_{[H],[K]}^{P}+\Delta_{[H],[K]}^{\pi})\right)\] \[\stackrel{{(iii)}}{{\leq}}\tilde{O}\left(H^{\frac{11} {d}}d^{\frac{5}{6}}A^{\frac{1}{2}}\left(A+d^{2}\right)^{\frac{1}{2}}K^{-\frac{ 1}{6}}\left(\Delta^{\sqrt{P}}+\Delta^{\phi}+1\right)^{\frac{1}{6}}\right)+ \tilde{O}\left(2HK^{-\frac{1}{3}}(\Delta^{P}+\Delta^{\pi}+1)^{\frac{1}{3}} \right),\]

where \((i)\) follows from Equation (14), \((ii)\) follows from the definition of \(\overline{W}\) and \((iii)\) follows from the definition of \(W^{\star}\) at the beginning of the proof.

**Term (II):** We derive the following bound:

\[\frac{1}{K} \sum_{i=1}^{\lceil K/M\rceil}\mathbb{E}\left[R_{i}(\overline{W}, \overline{\tau})\right]-\sum_{i=1}^{\lceil K/M\rceil}\mathbb{E}\left[R_{i}(W_ {i},\tau_{i})\right]\] \[\stackrel{{(i)}}{{\leq}}\frac{1}{K}\sum_{i=1}^{ \lceil K/M\rceil}\mathbb{E}\left[R_{i}(W^{\dagger},\tau^{\dagger})\right]-\sum_ {i=1}^{\lceil K/M\rceil}\mathbb{E}\left[R_{i}(W_{i},\tau_{i})\right]\] \[\stackrel{{(ii)}}{{\leq}}\tilde{O}(M\sqrt{J\lceil K /M\rceil}/K)\] \[=\tilde{O}(\sqrt{JKM})=\tilde{O}(H^{\frac{1}{6}}d^{\frac{1}{6}}K^{ -\frac{1}{6}}),\]

where \((i)\) follows from the definition of \(W^{\dagger}\) and \((ii)\) follows from Theorem 3.3 in Bubeck & Cesa-Bianchi (2012) with the adaptation that reward \(R_{i}\leq M\) and the number of iteration is \(\lceil K/M\rceil\). Then combining the bounds on terms \((I)\) and \((II)\), we have

\[\mathrm{Gap}_{\mathrm{Ave}}(K)\leq\tilde{O}\left(H^{\frac{11}{6}}d^{\frac{5}{6}}A ^{\frac{1}{2}}\left(A+d^{2}\right)^{\frac{1}{2}}K^{-\frac{1}{6}}\left(\Delta^{ \sqrt{P}}+\Delta^{\phi}+1\right)^{\frac{1}{6}}\right)+\tilde{O}\left(2HK^{-\frac{ 1}{3}}(\Delta^{P}+\Delta^{\pi}+1)^{\frac{1}{3}}\right).\]

### Detailed Comparison with Wei & Luo (2021)

Based on the theoretical results Theorem 4.4 and taking Algorithm 1 PORTAL as a base algorithm, we can also use the black-box techniques called MASTER in Wei & Luo (2021) to handle the unknown variation budgets. But such an approach of MASTER+PORTAL turns out to have a larger average dynamic suboptimality gap than Algorithm 3. Denote \(\Delta=\Delta^{\phi}+\Delta^{\sqrt{P}}+\Delta^{\pi}\).

To explain, first choose \(\tau=k\) and \(W=K\) in Algorithm 1. Then Algorithm 1 reduces to a base algorithm with the suboptimality gap of \(\tilde{O}(\sqrt{H^{4}d^{2}A(A+d^{2})/K}+\sqrt{H^{3}dAK(A+d^{2})}\Delta)\), which satisfies Assumption 1 in Wei & Luo (2021) with \(\rho(t)=\sqrt{H^{4}d^{2}A(A+d^{2})/t}\) and \(\Delta_{[1,t]}=\sqrt{H^{3}dAt\,(A+d^{2})}\Delta\). Then by Theorem 2 in Wei & Luo (2021), the dynamic regret using MASTER+PORTAL can be upper-bounded by \(\tilde{O}(H^{\frac{11}{d}}d^{\frac{5}{6}}A^{\frac{1}{2}}\left(A+d^{2}\right)^ {\frac{1}{2}}K^{\frac{7}{3}}\Delta^{\frac{1}{3}})=\tilde{O}(K^{\frac{5}{6}} \Delta^{\frac{1}{3}})\). Here, we find the order dependency on \(d,H,A\) is the same as Theorem 5.1 and hence mainly focus on the order dependency on \(K\) and \(\Delta\) in the following comparison. Such a bound on dynamic regret can be converted to the average dynamic suboptimality gap as \(\tilde{O}(K^{-\frac{1}{6}}\Delta^{\frac{1}{3}})\). Then it can obersed that for not too small variation budgets, i.e., \(\Delta\geq\tilde{O}(1)\), the order dependency on \(\Delta\) is higher than that of Algorithm 3.

Specifically, if we consider the special case when the representation stays identical and denote \(\tilde{\Delta}=\Delta^{\sqrt{P}}+\Delta^{\pi}\), then the average dynamic suboptimality gap of MASTER+PORTAL is still \(\tilde{O}(K^{-\frac{1}{6}}\tilde{\Delta}^{\frac{1}{3}})\). With small modifications on the parameters, by following the analysis similar to that of Theorem 5.1 and Appendix B.3, we can show that the average dynamic suboptimality gap of Algorithm 3 satisfies \(\tilde{O}(K^{-\frac{1}{6}}\tilde{\Delta}^{\frac{1}{4}})\), which is smaller than MASTER+PORTAL.

## Appendix D Auxiliary Lemmas

In this section, we provide two lemmas that are commonly used for the analysis of MDP problems.

The following lemma (Dann et al., 2017) is useful to measure the difference between two value functions under two MDPs and reward functions.

**Lemma D.1**.: (Simulation Lemma). _Suppose \(P_{1}\) and \(P_{2}\) are two MDPs and \(r_{1}\), \(r_{2}\) are the corresponding reward functions. Given a policy \(\pi\), we have,_

\[V_{h,P_{1},r_{1}}^{\pi}(s_{h})-V_{h,P_{2},r_{2}}^{\pi}(s_{h})\] \[\qquad=\sum_{h^{\prime}=h}^{H}\mathop{\mathbb{E}}_{s_{h^{\prime }}\sim\pi}\left[r_{1}(s_{h^{\prime}},a_{h^{\prime}})-r_{2}(s_{h^{\prime}},a_{h ^{\prime}})+(P_{1,h^{\prime}}-P_{2,h^{\prime}})V_{h^{\prime}+1,P_{1},r}^{\pi}( s_{h^{\prime}},a_{h^{\prime}})|s_{h}\right]\] \[\qquad=\sum_{h^{\prime}=h}^{H}\mathop{\mathbb{E}}_{s_{h^{\prime }}\sim\pi}\left[r_{1}(s_{h^{\prime}},a_{h^{\prime}})-r_{2}(s_{h^{\prime}},a_{ h^{\prime}})+(P_{1,h^{\prime}}-P_{2,h^{\prime}})V_{h^{\prime}+1,P_{2},r}^{\pi}( s_{h^{\prime}},a_{h^{\prime}})|s_{h}\right].\]

The following lemma is a standard inequality in the regret analysis for linear MDPs in reinforcement learning (see Lemma G.2 in Agarwal et al. (2020) and Lemma 10 in Uehara et al. (2022)).

**Lemma D.2** (Elliptical Potential Lemma).: _Consider a sequence of \(d\times d\) positive semidefinite matrices \(X_{1},\ldots,X_{N}\) with \(\operatorname{tr}(X_{n})\leq 1\) for all \(n\in[N]\). Define \(M_{0}=\lambda_{0}I\) and \(M_{n}=M_{n-1}+X_{n}\). Then the following bound holds:_

\[\sum_{n=1}^{N}\operatorname{tr}\left(X_{n}M_{n-1}^{-1}\right)\leq 2\log \det(M_{N})-2\log\det(M_{0})\leq 2d\log\left(1+\frac{N}{d\lambda_{0}}\right).\]