# MILP-StuDio: MILP Instance Generation via Block Structure Decomposition

 Haoyang Liu1, Jie Wang1, Wanbo Zhang1, Zijie Geng1, Yufei Kuang1, Xijun Li2,3, Yongdong Zhang1, Bin Li1, Feng Wu1

1MoE Key Laboratory of Brain-inspired Intelligent Perception and Cognition,

University of Science and Technology of China

2 Shanghai Jiao Tong University

3 Noah's Ark Lab, Huawei Technologies

{dgyoung,zhang_wb,ustcgzj,yfkuang}@mail.ustc.edu.cn,

lixijun@sjtu.edu.cn,

{jiewangx,zhyd73,binli,fengwu}@ustc.edu.cn

###### Abstract

Mixed-integer linear programming (MILP) is one of the most popular mathematical formulations with numerous applications. In practice, improving the performance of MILP solvers often requires a large amount of high-quality data, which can be challenging to collect. Researchers thus turn to generation techniques to generate additional MILP instances. However, existing approaches do not take into account specific block structures--which are closely related to the problem formulations--in the constraint coefficient matrices (CCMs) of MILPs. Consequently, they are prone to generate computationally trivial or infeasible instances due to the disruptions of block structures and thus problem formulations. To address this challenge, we propose a novel MILP generation framework, called Block Structure Decomposition (MILP-StuDio), to generate high-quality instances by preserving the block structures. Specifically, MILP-StuDio begins by identifying the blocks in CCMs and decomposing the instances into block units, which serve as the building blocks of MILP instances. We then design three operators to construct new instances by removing, substituting, and appending block units in the original instances, enabling us to generate instances with flexible sizes. An appealing feature of MILP-StuDio is its strong ability to preserve the feasibility and computational hardness of the generated instances. Experiments on commonly-used benchmarks demonstrate that with instances generated by MILP-StuDio, the learning-based solvers are able to significantly reduce over 10% of the solving time.

## 1 Introduction

Mixed-integer linear programming (MILP) is a fundamental mathematical optimization problem that finds extensive applications in the real world, such as scheduling [1], planning [2], and chip design [3, 4]. In industrial scenarios, the solving efficiency of MILPs is associated with substantial economic value. To speed up the solving process, a great number of high-quality MILP instances are required to develop or test the solvers. Here we give the following two examples. First, both traditional solvers [5, 6] and learning-based solvers [7, 8, 9, 10] rely heavily on a lot of MILP instances for hyperparameter tuning or model training. Second, evaluating the robustness of solvers needs a comprehensive MILP benchmark consisting of numerous instances. However, acquiring many instances is often difficult due to high acquisition costs or privacy concerns [11, 12]. As a result, the limited data availability poses great challenges and acts as a bottleneck for solver performance.

This challenge motivates a wide range of MILP generation techniques. In the past, researchers relied on problem-specific techniques for generation [13, 14, 15, 16, 17]. These methods assumed knowledge of problem types and generated instances based on the corresponding mathematical formulations, such as the satisfiability problem [18], set covering problem [15], and others. However, these techniques require much expert knowledge to design and are limited to specific MILP problems. They also face limitations in practical scenarios where problem formulations are unknown [19].

In recent years, there has been some progress in general MILP generation that does not require explicit knowledge of the problem formulations. These approaches can be broadly classified into statistics-based and learning-based methods. Statistics-based approaches utilize a few instance statistics to sample in the MILP space [20]. More advanced learning-based approaches, exemplified by G2MILP [19], leverage deep learning models to capture global instance features and iteratively modify constraints in the original instances. Though in the early stage, learning-based techniques offer convenience and strong adaptability for MILP generation, making them applicable in a wider range of practical scenarios [19]. However, they still suffer from significant challenges. (1) They fail to account for the inherent problem structures adequately and disrupt instances' mathematical properties. This leads to low-quality instances with degrading computational hardness or infeasible regions. (2) Existing methods fail to generate instances with different sizes from the original ones, limiting instance diversity. (3) The iterative style to modify constraints becomes time-consuming when dealing with large-scale instances.

Therefore, a natural question arises: can we analyze and exploit the problem structures during generation to address the above challenges? Consider a MILP instance with constraints \(\bm{A}\bm{x}\leq\bm{b}\), where \(\bm{A}\) is the constraint coefficient matrix (CCM), \(\bm{x}\) is the decision variable and \(\bm{b}\) is a vector. As shown in Figure 0(a), we observe that a great number of real-world MILP problems exhibit structures with repeated patterns of block units in their CCMs. In operational research, researchers have long noticed the similar block structures of CCMs across instances from the same problem type, and they have been aware of the critical role of CCMs in determining problem formulation and mathematical properties [21, 22, 23, 24]. Although a wide suite of CCM-based techniques have been developed to solve MILPs [25, 26, 27], existing works on MILP generation rarely pay attention to CCMs. Consequently, these works fail to preserve the block structures during the generation process.

In light of this, we propose a novel MILP generation framework called Block Structure Decomposition (MILP-StuDio), which takes into account the block structures throughout the generation process and addresses Challenge (1)-(3) simultaneously. Specifically, MILP-StuDio consists of three key steps. We begin by identifying the block structures in CCMs and decomposing the instances into block units, which serve as the building blocks in the MILP instances. We then construct a library of the block units, enabling efficient storage, retrieval, and utilization of the comprehensive block characteristics during the subsequent process. Leveraging this library, we design three block operators on the original instances to generate new ones, including block reduction (eliminating certain blocks from the original instances), block mix-up (substituting some blocks with others sampled from the library), and block expansion (appending selected blocks from the library). These operators enable us to generate instances with flexible sizes, effectively improving the diversity of instances.

Experiments demonstrate that MILP-StuDio has the following advanced features. (1) Hardness preservation. MILP-StuDio can effectively preserve the computational hardness and feasibility in the

Figure 1: Figure 0(a) visualizes the CCMs of four instances from the FA problem, where the white points represent the nonzero entries in CCMs. As we can see, the CCMs exhibit similar block structures across instances, with the patterns in red boxes being the block units. Figure 0(b) illustrates the block decomposition process, advanced features, and applications of our proposed MILP-StuDio.

generated instances. (2) Scalable generation. MILP-StuDio can generate instances with flexible sizes. (3) High efficiency. MILP-StuDio can reduce over two-thirds of the generation time in real-world large datasets. We observe an over 10% reduction in the solving time for learning-based solvers using instances generated by MILP-StuDio.

## 2 Background

### MILP and MILP with Block Structure

A MILP instance takes the form of:

\[\min_{\bm{x}\in\mathbb{R}^{n}}\quad\bm{c}^{\top}\bm{x},\quad\text{s.t.}\quad \bm{A}\bm{x}\leq\bm{b},\bm{l}\leq\bm{x}\leq\bm{u},\bm{x}\in\mathbb{Z}^{p}\times \mathbb{R}^{n-p}.\] (1)

In Formula (1), \(\bm{x}\) denotes the decision variables, \(\bm{c}\in\mathbb{R}^{n}\) denotes the coefficients in the objective function, \(\bm{A}\in\mathbb{R}^{m\times n}\) is the constraint coefficient matrix (CCM) and \(\bm{b}\in\mathbb{R}^{m}\) denotes the terms on the right side of the constraints, respectively. The vectors \(\bm{l}\in(\mathbb{R}\cup\{-\infty\})^{n}\) and \(\bm{u}\in(\mathbb{R}\cup\{+\infty\})^{n}\) denote lower and upper bounds for the variables, respectively.

In real-world applications, a significant portion of MILPs exhibit block structures--consisting of many block units--in their constraint coefficient matrices (CCMs) \(\bm{A}\). These problems, referred as MILPs with block structures, include many commonly-used and widely-studied datasets in recent papers on learning-based solvers [8; 9; 10; 28], such as combinatorial auctions (CA), capacitated facility location (FA), item placement (IP), multiple knapsacks (MIK), and workload balancing (WA). In Figure 2, we visualize the CCMs of MILP instances using a black-and-white digital _image representation_[29]. In this representation, the rows and columns of the digital images correspond to the constraints and variables in the MILPs, respectively. To construct the digital image, we assign a pixel value of 255 (white) to the entry \((i,j)\) if the corresponding entry in the CCM \(\bm{A}[i,j]\) is nonzero. Conversely, if \(\bm{A}[i,j]\) is zero, we set the pixel value to 0 (black). This mapping allows us to depict the sparsity patterns and structural characteristics of CCMs visually. For each problem, the CCMs of the instances present a similar block structure, characterizing specific mathematical formulations.

The importance of block-structured CCMs in the context of MILP solving has long been acknowledged by operational researchers, where instances with similar block structures share similar mathematical properties [29; 30; 31]. Furthermore, the block structures of CCMs are closely related to the problem formulations [30]. Thus, the block matrices have shown great potential in accelerating the solution process for a family of MILP problems [21; 22; 23; 24]. One notable technique developed to exploit this structure is Dantzig-Wolfe decomposition [25] for solving large-scale MILP instances.

### Bipartite Graph Representation of MILPs

A MILP instance can be represented as a weighted bipartite graph \(\mathcal{G}=(\mathcal{W}\cup\mathcal{V},\mathcal{E})\)[8]. The two sets of nodes \(\mathcal{W}=\{w_{1},\cdots,w_{m}\}\) and \(\mathcal{V}=\{v_{1},\cdots,v_{n}\}\) in the bipartite graph correspond to the MILP's constraints and variables, respectively. The edge set \(\mathcal{E}=\{e_{ij}\}\) comprises edges, each connecting a constraint node \(w_{i}\in\mathcal{W}\) with a variable node \(v_{j}\in\mathcal{V}\). The presence of an edge \(e_{ij}\) is determined by the coefficient matrix, with \(\bm{e}_{ij}=(e_{ij})\) as its edge feature, and an edge \(e_{ij}\) does not exist if \(\bm{A}[i,j]=0\). Please refer to Appendix I.1 for more details on the graph features we use in this paper.

## 3 Motivated Experiments

Preserving the mathematical properties of the original instances is a fundamental concern in MILP generation [19]. These properties encompass feasibility, computational hardness, and problem structures, with the latter being particularly crucial. The problem structure directly determines the problem formulation and, consequently, impacts other mathematical properties. However, it

Figure 2: Visualization of the CCMs of instances in four widely recognized benchmarks. The block structures can be commonly seen in MILP problems.

is important to note that the term "problem structure" can be ambiguous and confusing. There are different understandings and definitions of the problem structure--such as the bipartite graph structure [19] and the CCM's block structure [29]--and we are supposed to identify the most relevant and useful ones that contribute to the mathematical properties of MILPs. Analyzing and exploiting these specific structure types become key factors in improving the quality of the generated instances.

### Challenges of Low-Quality Generation

G2MILP is the first learning-based approach for MILP instance generation. While it has shown promising performance, we observe that G2MILP still encounters difficulties in generating high-quality instances for MILPs with block structures. To evaluate its performance, we compare the graph structural distributional similarity and solving properties of the original and generated instances from the workload appointment (WA) benchmark [32] using Gurobi [5], a state-of-the-art traditional MILP solver. We set the masking ratio of G2MILP--which determines the proportion of constraints to be modified--to 0.01. The results are summarized in Table 1. In this table, the _Similarity_ metric refers to the graph structural distributional similarity score [19] (defined in Appendix I.3), _Time_ represents the average solving time (with a time limit 1,000s), and _Feasible ratio_ indicates the proportion of feasible instances out of the total instances. Results show that although the generated instances achieve a high similarity score, most of them are infeasible. Furthermore, the feasible instances exhibit a severe degradation in computational hardness.

### Visualization of CCMs

The aforementioned experiments provide evidence that by iterative modifications of sampled constraints, G2MILP can generate MILP with high graph structural distributional similarities to the original instances, but may lead to disruptions in mathematical properties. Consequently, it becomes necessary to explore alternative definitions of problem structures that offer stronger correlations to the mathematical properties of the instances and can be effectively preserved during the generation process. One such promising structure is the block structures within CCMs.

The concept of block structures within CCMs originates from traditional operational research and has proven to be effective for problem analysis [21; 22; 23; 24]. It is widely recognized that MILPs with similar block structures in CCMs often share similar formulations, resulting in similar mathematical properties [29]. We visualize and compare the CCMs of the original and generated instances in Figure 3. In the middle figure, we observe that G2MILP breaks the block pattern in the left and introduces a noisy block in the bottom right. It becomes evident that the generation operation in G2MILP breaks the block structures presenting in the original instances. Thus, it motivates us that exploring and preserving the block structures in CCMs can hold the potential for generating high-quality instances.

## 4 Generation Framework Using Block Structure Decomposition

In this section, we introduce the proposed MILP-StuDio framework to generate high-quality instances. MILP-StuDio comprises three steps: block decomposition, construction of structure library, and block manipulations. We begin by presenting the concept of block decomposition for MILP in Section 4.1,

\begin{table}
\begin{tabular}{l c c c} \hline \hline  & Similarity & Time & Feasible Ratio \\ \hline Original & 1.000 & 1000.00 & 100.00\% \\ G2MILP & 0.854 & 12.01 & 10.00\% \\ \hline \hline \end{tabular}
\end{table}
Table 1: The comparison of graph similarity and solving properties between the original instances and the generated instances using G2MILP [19], a popular MILP generation framework. Here we use 100 original instances to generate 1,000 instances.

Figure 3: Visualization of CCMs from original instances (left), instances generated by G2MILP (middle), and instances generated by MILP-StuDio (right).

as it forms the core of our method. The subsequent sections, from Section 4.2 to Section 4.4, provide a detailed explanation of each step. The overview of MILP-StuDio is depicted in Figure 4.

### Block Decomposition of MILP

In this part, we specify the block structures that we are interested in. CCMs are often reordered to achieve the well-studied block structures [30], including the block-diagonal (BD), bordered block-diagonal (BBD), and doubly bordered block-diagonal (DBBD) structures. We highlight the _block unit_ of the decomposition in blue in Equation (2). We can see that the former two structures are special cases of the latter one. Despite the simplicity, they are the building blocks for more complex block structures and are widely used in operational research [30].

\[\begin{pmatrix}\bm{D}_{1}&&&\\ &\bm{D}_{2}&&\\ &&\ddots&\\ &&&\bm{D}_{k}\end{pmatrix}\quad\begin{pmatrix}\bm{D}_{1}&&&\\ &\bm{D}_{2}&&\\ &&\ddots&\\ &&\bm{D}_{k}\\ \bm{B}_{1}&\bm{B}_{2}&\cdots&\bm{B}_{k}\end{pmatrix}\quad\begin{pmatrix}\bm{D }_{1}&&&\bm{F}_{1}\\ &\bm{D}_{2}&&\bm{F}_{2}\\ &&\ddots&&\vdots\\ &&&\bm{D}_{k}&\bm{F}_{k}\\ \bm{B}_{1}&\bm{B}_{2}&\cdots&\bm{B}_{k}&\bm{C}\end{pmatrix}\] (2)

\((a)\) Block-diagonal \((b)\) Bordered block-diagonal \((c)\) Doubly bordered block-diagonal

Formally, a MILP with a DBBD structure can be written as

\[\begin{split}\min_{\bm{x}\in\mathbb{R}^{n}}&\bm{c}_{1}^{\top} \bm{x}_{1}+\bm{c}_{2}^{\top}\bm{x}_{2}+\cdots+\bm{c}_{k}^{\top}\bm{x}_{k}+\bm{ c}_{k+1}^{\top}\bm{x}_{k+1},\\ \text{s.t.}&\bm{D}_{i}\bm{x}_{i}+\bm{F}_{i}\bm{x}_{k+1}\leq\bm{b}_{i}, \quad 1\leq i\leq k,\quad\text{ (B-Cons if $\bm{F}_{i}=\bm{O}$, otherwise DB-Cons)}\\ &\sum_{i=1}^{k}\bm{B}_{i}\bm{x}_{i}+\bm{C}\bm{x}_{k+1}\leq\bm{b}_ {k+1},\quad\quad\quad\quad\text{(M-Cons)}\\ &\bm{l}\leq\bm{x}\leq\bm{u},\quad\bm{x}\in\mathbb{Z}^{p}\times \mathbb{R}^{n-p},\end{split}\] (3)

where the partition \(\bm{c}=(\bm{c}_{1}^{\top},\cdots,\bm{c}_{k+1}^{\top})^{\top}\), \(\bm{x}=(\bm{x}_{1}^{\top},\cdots,\bm{x}_{k+1}^{\top})^{\top}\) and \(\bm{b}=(\bm{b}_{1}^{\top},\cdots,\bm{b}_{k+1}^{\top})^{\top}\). To process more complex block structures beyond the three basic ones, we specify different types of constraints and variables in a CCM (and the corresponding MILP). First, we classify variables as _block_ and _bordered_ variables (Bl-Vars and Bd-Vars). The block variables DBBD are \(\bm{x}_{\text{block}}=(\bm{x}_{1}^{\top},\cdots,\bm{x}_{k}^{\top})^{\top}\), which are involved in the blocks \(\bm{D}_{i}\), \((1\leq i\leq k)\). The bordered variables are

Figure 4: An overview of MILP-StuDio. (1) We detect the block structures in the original instances and decompose the CCMs into sub-matrices of block units. (2) The sub-matrices are transferred into the corresponding sub-graphs of instances’ bipartite graph representations. These sub-graphs are used to construct the structure library. (3) We sample instances and sub-graphs of block units and perform block manipulations, including block reduction, mix-up and expansion.

defined to be those in \(\bm{F}_{i}\), \((1\leq i\leq k)\), i.e. the variables \(\bm{x}_{k+1}\). Notice that all the variables in BD and BBD are block variables. Then, we classify the constraints in an instance as _master_, _block_ and _doubly block constraints_ (M-Cons, B-Cons, and DB-Cons), which we have illustrated in Equation (2). As we can see, BD only contains B-Cons, BBD contains B-Cons and M-Cons, and DBBD contains DB-Cons and M-Cons. The classifications of constraints and variables make it possible for us to investigate more delicate structures in the instances--such as the combination of the three basic ones--in the subsequent process (please see Appendix H.2 and H.4).

### Block Decomposition

Reordering Rows and Columns in CCMsGiven a MILP instance, the raw orders of rows and columns for a CCM are determined by the orders of constraints and variables respectively, which is defined when we establish the instance. Generally, the block structures are not readily apparent in this raw form. To identify and exploit these block structures, we employ a structure detector implemented in the Generic Column Generation (GCG) solver [33] for CCM reordering. This detector identifies row and column permutations that effectively cluster the nonzero coefficients, thereby revealing distinct block structures within CCMs.

Block DecompositionWe employ an enhanced variable partition algorithm based on the image representations of CCMs for block decomposition, using the constraint-variable classification results mentioned in Section 4.1. Specifically, we extract the sub-matrices of the _block units_\(\mathcal{BU}\) in CCMs, i.e., which take the form of \(\bm{D}_{i}\) in BD, \(\begin{pmatrix}\bm{D}_{i}\\ \bm{B}_{i}\end{pmatrix}\) in BBD, and \(\begin{pmatrix}\bm{D}_{i}&\bm{F}_{i}\\ \bm{B}_{i}\end{pmatrix}\) in DBBD. Finally, we partition and decompose the CCMs into sub-matrices of block units. The algorithm enables us to handle more complex structures beyond the basic three found by GCG, such as instances in WA with M-Cons, B-Cons and BD-Cons. In the case of WA, the sub-matrices are in the form of \(\begin{pmatrix}\bm{D}_{i}^{(1)}&\bm{F}_{i}\\ \bm{B}_{i}^{(2)}\\ \end{pmatrix}\), where \(\bm{D}_{i}^{(1)}\) represents the diagonal block with DB-Cons, and \(\bm{D}_{i}^{(2)}\) represents the diagonal block with B-Cons. Please refer to Section H.3 for the detailed implementation of the decomposition process.

### Construction of Structure Library

As we can see in Figure 0(a), the block units across instances exhibit striking similarities in terms of the internal structures. These common characteristics indicate that the distribution of block units holds valuable information about the problem formulations, making it an ideal building block for reconstructing new instances. Given block-unit sub-matrices of CCMs obtained in Section 4.2, we proceed to extract the corresponding bipartite sub-graphs within the graph representations of the original instances. Compared to the image representation, graph representation offers more convenience for modifying MILP instances during block manipulation. Specifically, suppose that a sub-matrix contains constraints \(\tilde{\mathcal{W}}=\{w_{i_{1}},\cdots,w_{i_{k}}\}\) and variables \(\tilde{\mathcal{V}}=\{v_{i_{1}},\cdots,v_{i_{l}}\}\) in the instances, we then extract the sub-graph containing \(\tilde{\mathcal{W}}\), \(\tilde{\mathcal{V}}\) and the edges connecting \(\tilde{\mathcal{W}}\) and \(\tilde{\mathcal{V}}\) in the bipartite graph representation of the original instance. Subsequently, we collect these sub-graphs from all the training instances and utilize them to construct a comprehensive structure library denoted as \(\mathcal{L}\). This structure library serves as a repository for the collected sub-graphs, allowing efficient storage, retrieval, and utilization of the block information.

### Scalable Generation via Block Manipulations

With the structure library, we devise three types of generation operators that enable the generation of high-quality MILP instances with flexible sizes. These operators, namely block reduction, block mix-up, and block expansion, play a crucial role in the instance generation process.

* **Reduction.** This operator involves randomly sampling a block unit \(\mathcal{BU}_{\text{ins}}\) from the original instances and then removing it. The reduction operator generates MILP instances with smaller sizes compared to the original ones, reducing the complexity of the problem.
* **Mix-up.** This operator involves randomly sampling one block unit \(\mathcal{BU}_{\text{ins}}\) from the original instances and another block unit \(\mathcal{BU}\) from the structure library \(\mathcal{L}\). We then replace \(\mathcal{BU}_{\text{ins}}\) with \(\mathcal{BU}\) to generate a new instance. The mix-up operator introduces structural variations through the incorporation of external block units.

* **Expansion.** This operator involves randomly sampling a block unit \(\mathcal{BU}\) from the structure library \(\mathcal{L}\) and appending it to the original instances. This process generates new instances of larger sizes compared to the original ones, potentially introducing more complex structures.

To preserve the block structures, the operators should leverage the constraint-variable classification results. Taking the expansion operator as an example, the coefficients of M-Cons in the external block unit should be properly inserted into the M-Cons of the original instances. Meanwhile, we construct new constraints for the B- and DB-Cons of the block using the corresponding right-hand-side terms \(\bm{b}_{i}\). Finally, we design a coefficient refinement algorithm to align the coefficients of the external blocks during mix-up and expansion (please see Appendix H.4 for details).

## 5 Experiments

### Experiment Settings

BenchmarksWe consider four MILP problem benchmarks: combinatorial auctions (CA) [16], capacitated facility location (FA) [17], item placement (IP) [32] and workload appointment (WA) [32]. The first two benchmarks, CA and FA, are commonly-used benchmarks proposed in [8]. The last two benchmarks, IP and WA, come from two challenging real-world problem families used in NeurIPS ML4CO 2021 competition [32]. The numbers of training, validation, and testing instances are 100, 20, and 50. More details on the benchmarks are in Appendix I.2.

MetricsWe leverage three metrics to evaluate the similarity between the original and generated instances. (1) Graph statistics are composed of 11 classical statistics of the bipartite graph [34]. Following [19], we compute the Jensen-Shannon divergence for each statistic between the generated and original instances. We then standardize the metrics into similarity scores ranging from 0 to 1. (2) Computational hardness is measured by the average solving time of the instances using the Gurobi solver [5]. (3) Feasible ratio is the proportion of feasible instances out of the total ones.

BaselinesWe consider two baselines for MILP generation. The first baseline is the statistics-based MILP generation approach Bowly [20], which generates MILP instances by controlling specific statistical features, including the coefficient density and coefficient mean. We set these features to match the corresponding statistics of the original instances so as to generate instances with high statistical similarity with the original ones. The second baseline is the learning-based approach G2MILP [19]. By leveraging masked VAE, G2MILP iteratively masks one constraint node in the bipartite graph and replaces it with a generated one.

Downstream TasksWe consider three downstream tasks to demonstrate the effectiveness of the generated instances in practical applications. (1) Improving the performance of learning-based solvers, including predict-and-search (PS) [10] in Section 5.3 and the GNN approach for learning-to-branch [8] in Appendix F.1. (2) Hyperparameter tuning for the traditional solver (please see Appendix F.3). In the above two tasks, we use MILP-StuDio and the baselines to generate new instances to enrich the training data. We also consider the task of (3) hard instances generation in Section 5.4, which reflects the ability to construct hard benchmarks.

Variants of MILP-StuDioDuring the generation process, we use the three operators to generate one-third of the instances, respectively. We can choose different modification ratios, which implies that we will remove (substitute or append) block units when performing the reduction (mix-up or expansion) operator until the proportions of variables modified in the instance reaches \(\eta\).

### Similarity between the Generated and the Original Instances

For each generation technique, we use 100 original instances to generate 1,000 instances and evaluate the similarity between them. As shown in Table 2, we present the graph structural distributional similarity scores between the original and generated instances. We do not consider Bowly in WA since the instance sizes in WA are so large that the generation time is over 200 hours. The results suggest that MILP-StuDio shows a high graph structural similarity compared to the baselines. In FA, instances generated from G2MILP present a low similarity, while our proposed MILP-StuDio can still achieve a high similarity score. As the modification ratio increases, the similarity scores of G2MILP and MILP-StuDio decrease, whereas G2MILP suffers from a more severe degradation.

We also evaluate the computational hardness and feasibility of the generated instances in Table 3. We report the average solving time and feasibility ratio for each dataset. Results demonstrate that the instances generated by the baselines represent a severe degradation in computational hardness.

Moreover, most of the instances generated by G2MILP in FA and WA datasets are infeasible. Encouragingly, MILP-StuDio is able to preserve the computational hardness and feasibility of the original instances, making it a strong method for maintaining instances' mathematical properties.

We visualize the CCMs of the original and generated instances to demonstrate the effectiveness of MILP-StuDio in preserving block structures in Appendix G. MILP-StuDio is able to preserve the block structures while all the baselines fail to do so.

### Improving the Performance of Learning-based Solvers

GNN branching policy [8] and predict-and-search [10] are two representative approaches of learning-based solvers. Here we report the results of PS built on Gurobi and leave the other in Appendix F.1. In alignment with [10], we consider two additional baselines Gurobi and BKS. For each instance, we run Gurobi on a single-thread mode for 1,000 seconds. We also run Gurobi [5] for 3,600 seconds and denote the obtained objective values as the best-known solution (BKS). For more details on the implementation of PS, please refer to Appendix H.1. Table 4 demonstrates the performance of MILP-StuDio-enhanced PS and the baselines, where we set the modification ratio \(\eta=0.05\). We report three metrics to measure the solving performance. (1) Obj represents the objective values achieved by different methods. (2) \(\text{gap}_{\text{abs}}\) is the absolute primal gap defined as \(\text{gap}_{\text{abs}}:=|\text{Obj}-\text{BKS}|\), where smaller gaps indicate superior primal solutions and, consequently, a better performance. (3) Time denotes the average time used to find the solutions.

In CA and FA, all the approaches can solve the instances to the optimal with \(\text{gap}_{\text{abs}}=0\), thus we compare the Time metric. In IP and WA, all the approaches reach the time limit of 1,000s, thus we mainly focus on the \(\text{gap}_{\text{abs}}\) metric. Methods built on PS does not perform well in CA compared to Gurobi, since CA is easy for Gurobi to solve and PS needs to spend time for network inference. Even so, PS+MILP-StuDio achieves a comparable performance to Gurobi. In the other three datasets, MILP-StuDio-enhanced PS outperforms other baselines, achieving the best solving time or \(\text{gap}_{\text{abs}}\).

### Hard Benchmark Generation

The hard instance generation task is important as it provides valuable resources for evaluating solvers and thus potentially motivates more efficient algorithms. The objective of this experiment is to test the ability to generate harder instances within a given number of iterations. We use 30 original instances to construct the pool. In each iteration, for each instance in the pool, we employ mix-up or expansion operators to generate two new ones, and We then select the instance among and with the longest solving time to replace in the pool. This setting is to preserve the diversity of the pool. We observe that there exist slight differences in the hardness of the original instances, and the generated instances

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \multirow{2}{*}{Bowly} & \multicolumn{2}{c}{\(\eta=0.01\)} & \multicolumn{2}{c}{\(\eta=0.05\)} & \multicolumn{2}{c}{\(\eta=0.10\)} \\ \cline{3-7}  & & G2MILP & MILP-StuDio & G2MILP & MILP-StuDio & G2MILP & MILP-StuDio \\ \hline CA & 0.567 & 0.997 & 0.997 & 0.995 & 0.981 & 0.990 & 0.946 \\ FA & 0.077 & 0.358 & 0.663 & 0.092 & 0.646 & 0.091 & 0.618 \\ IP & 0.484 & 0.717 & 0.661 & 0.352 & 0.528 & 0.336 & 0.493 \\ WA & Timeout & 0.854 & 0.980 & 0.484 & 0.853 & 0.249 & 0.783 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Structural Distributional similarity scores between the generated instances and the original ones. The higher score implies higher similarity. _Timeout_ implies the generation time is over 200h.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline  & \multicolumn{1}{c}{CA} & FA & IP & WA \\ \hline Original Instances & 0.50 (100.0\%) & 4.78 (100.0\%) & 1000 (100.0\%) & 1000 (100.0\%) \\ \hline Bowl & 0.02 (100.0\%) & 0.07 (100.0\%) & 56.45 (100.0\%) & Timeout \\ \hline G2MILP \(\eta=0.01\) & 0.58 (100.0\%) & 0.02 (1.7\%) & 802.3 (100.0\%) & 12.01 (10.0\%) \\ MILP-StuDio \(\eta=0.01\) & **0.50 (100.0\%)** & **4.94 (100.0\%)** & **1000 (100.0\%)** & **1000 (100.0\%)** \\ \hline G2MILP \(\eta=0.05\) & 0.69 (100.0\%) & 0.01 (1.8\%) & 0.14 (100.0\%) & 0.01 (0.0\%) \\ MILP-StuDio \(\eta=0.05\) & **0.48 (100.0\%)** & **4.92 (100.0\%)** & **691.66 (100.0\%)** & **1000 (100.0\%)** \\ \hline G2MILP \(\eta=0.10\) & 0.72 (100.0\%) & 0.01 (2.0\%) & 0.03 (100.0\%) & 0.02 (0.0\%) \\ MILP-StuDio \(\eta=0.10\) & **0.40 (100.0\%)** & **4.64 (100.0\%)** & **550.33 (100.0\%)** & **1000 (94.5\%)** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average solving time (s) and feasible ratio (in parentheses) of the instances. We set the solving time limit to be 1,000s. We mark the **values closest to those in original instances** in bold.

derived from the harder original instances are also harder than those from easier ones. If we had simply generated 60 instances and selected the hardest 30, the proportion of instances generated from the hard original instances would have continuously increased, reducing the diversity of the pool. In Figure 5, we depict the growth curve of the average solving time of instances in the pool during 10 iterations. The solving time of the final set is two times larger than that of the initial set, suggesting MILP-StuDio's ability to generate increasingly harder instances during iterations.

### Extensive Studies

Generation EfficiencyWA is a challenging benchmark with large instance sizes. The instances in WA have over 60,000 constraints and variables, making generation in WA especially time-consuming. We compare the generation time of the generation techniques to generate 1,000 instances. We set the time limit to 200 hours. The results in Table 6 show that MILP-StuDio significantly archives \(3\times\) acceleration compared to G2MILP, demonstrating high generation efficiency.

More ResultsWe conduct ablation studies on three operators and modification ratios in Appendix F.2, and we also try to extend MILP-StuDio to MILPs without block structures in Appendix F.4 and MILPs in the real-world industrial dataset in Appendix F.5.

## 6 Related Work on MILP Generation

The field of MILP generation encompasses two main categories: problem-specific generation and general MILP generation. Problem-specific generation methods rely on expert knowledge of the mathematical formulation of problems to generate instances. Examples include set covering [15], combinatorial auctions [16], and satisfiability [18]. While these methods can generate instances tailored to specific problem types, they are limited in their applicability and require much modeling expert knowledge. On the other hand, general MILP generation techniques aim to generate MILPs using statistical information [20] or by leveraging neural networks to capture instance distributions [19]. G2MILP [19] is the first learning-based generation framework designed for generating general MILPs. This approach represents MILPs as bipartite graphs and utilizes a masked variational auto-encoder [35] to iteratively corrupt and replace parts of the original graphs to generate new ones.

## 7 Limitations and Future Avenue

Our method originates from the field of operational research and is designed for instances with block structures. The performance of the detector in GCG influences the overall generation quality. Although the detector can identify a wide range of useful structures in the real world, it is still limited when facing instances with extremely complex structures. The exploration of enhancing the performance of the detector, such as those involving prior knowledge of the instances, represents a promising avenue for future research.

## 8 Conclusion

In this paper, we propose a novel MILP generation framework (MILP-StuDio) to generate high-quality MILP instances. Inspired by the studies of CCMs in operational research, MILP-StuDio manipulates the block structures in CCMs to preserve the mathematical properties--including the computational hardness and feasibility--of the instances. Furthermore, MILP-StuDio has a strong ability of scalable generation and high generation efficiency. Experiments demonstrate the effectiveness of MILP-StuDio in improving the performance of learning-based solvers.

## 9 Acknowledgement

The authors would like to thank all the anonymous reviewers for their insightful comments and valuable suggestions. This work was supported by the National Key R&D Program of China under contract 2022ZD0119801 and the National Nature Science Foundations of China grants U23A20388 and 62021001.

## References

* [1] John A Muckstadt and Richard C Wilson. An application of mixed-integer programming duality to scheduling thermal generating systems. _IEEE Transactions on Power Apparatus and Systems_, (12), 1968.
* [2] Yves Pochet and Laurence A Wolsey. _Production planning by mixed integer programming_, volume 149. Springer, 2006.
* [3] Kefan Ma, Liquan Xiao, Jianmin Zhang, and Tiejun Li. Accelerating an fpga-based sat solver by software and hardware co-design. _Chinese Journal of Electronics_, 28(5):953-961, 2019.
* [4] Zhihai Wang, Jie Wang, Yinqi Bai Qingyue Yang, Xing Li, Lei Chen, Jianye Hao, Mingxuan Yuan, Bin Li, Yongdong Zhang, and Feng Wu. Towards next-generation logic synthesis: A scalable neural circuit generation framework. In _The Thirty-eighth Annual Conference on Neural Information Processing Systems_, 2024.
* [5] LLC Gurobi Optimization. Gurobi optimizer. _URL http://www. gurobi. com_, 2021.
* [6] Tobias Achterberg. Scip: solving constraint integer programs. _Mathematical Programming Computation_, 1:1-41, 2009.
* [7] He He, Hal Daume III, and Jason M Eisner. Learning to search in branch and bound algorithms. _Advances in neural information processing systems_, 27, 2014.
* [8] Maxime Gasse, Didier Chetelat, Nicola Ferroni, Laurent Charlin, and Andrea Lodi. Exact combinatorial optimization with graph convolutional neural networks. _Advances in neural information processing systems_, 32, 2019.
* [9] Zhihai Wang, Xijun Li, Jie Wang, Yufei Kuang, Mingxuan Yuan, Jia Zeng, Yongdong Zhang, and Feng Wu. Learning cut selection for mixed-integer linear programming via hierarchical sequence model. In _The Eleventh International Conference on Learning Representations_, 2023.
* [10] Qingyu Han, Linxin Yang, Qian Chen, Xiang Zhou, Dong Zhang, Akang Wang, Ruoyu Sun, and Xiaodong Luo. A gnn-guided predict-and-search framework for mixed-integer linear programming. In _The Eleventh International Conference on Learning Representations_, 2023.

* [11] Byoung-Woon Kim and Chong-Min Kyung. Exploiting intellectual properties with imprecise design costs for system-on-chip synthesis. _IEEE Transactions on Very Large Scale Integration (VLSI) Systems_, 10(3):240-252, 2002. doi: 10.1109/TVLSI.2002.1043327.
* [12] Jun Sakuma and Shigenobu Kobayashi. A genetic algorithm for privacy preserving combinatorial optimization. In _Proceedings of the 9th annual conference on Genetic and evolutionary computation_, pages 1372-1379, 2007.
* [13] Russ J Vander Wiel and Nikolaos V Sahinidis. Heuristic bounds and test problem generation for the time-dependent traveling salesman problem. _Transportation Science_, 29(2):167-183, 1995.
* 495, 2015.
* [15] Egon Balas and Andrew Ho. _Set covering algorithms using cutting planes, heuristics, and subgradient optimization: a computational study_. Springer, 1980.
* [16] Kevin Leyton-Brown, Mark Pearson, and Yoav Shoham. Towards a universal test suite for combinatorial auction algorithms. In _Proceedings of the 2nd ACM Conference on Electronic Commerce_, pages 66-76, 2000.
* [17] Gerard Cornuejols, Ramaswami Sridharan, and Jean-Michel Thizy. A comparison of heuristics and relaxations for the capacitated plant location problem. _European Journal of Operational Research_, 50:280-297, 1991.
* [18] Jiaxuan You, Haoze Wu, Clark Barrett, Raghuram Ramanujan, and Jure Leskovec. G2sat: learning to generate sat formulas. _Advances in neural information processing systems_, 32, 2019.
* [19] Zijie Geng, Xijun Li, Jie Wang, Xiao Li, Yongdong Zhang, and Feng Wu. A deep instance generative framework for milp solvers under limited data availability. In _Advances in Neural Information Processing Systems_, 2023.
* [20] Simon Andrew Bowly. _Stress testing mixed integer programming solvers through new test instance generation methods_. PhD thesis, School of Mathematical Sciences, Monash University, 2019.
* [21] Dimitris Bertsimas and John N Tsitsiklis. _Introduction to linear optimization_, volume 6. Athena Scientific Belmont, MA, 1997.
* [22] Guy Desaulniers, Jacques Desrosiers, and Marius M Solomon. _Column generation_, volume 5. Springer Science & Business Media, 2006.
* [23] Marco E Lubbecke and Jacques Desrosiers. Selected topics in column generation. _Operations research_, 53(6):1007-1023, 2005.
* [24] Daniel Bienstock George Nemhauser and L Wolsey. Integer programming and combinatorial optimization, 1993.
* [25] George B. Dantzig and Philip Wolfe. Decomposition principle for linear programs. _Operations Research_, 8:101-111, 1960. URL https://api.semanticscholar.org/CorpusID:61768820.
* [26] FG Commoner. A sufficient condition for a matrix to be totally unimodular. _Networks_, 3(4):351-365, 1973.
* [27] Ilias Mitrai, Wentao Tang, and Prodromos Daoutidis. Stochastic blockmodeling for learning the structure of optimization problems. _AIChE Journal_, 68(6):e17415, 2022.
* [28] Jiacheng Lin, Meng XU, Zhihua Xiong, and Huangang Wang. CAMBranch: Contrastive learning with augmented MILPs for branching. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=K6kt502AiG.

* Steever et al. [2022] Zachary Steever, Chase C. Murray, Junsong Yuan, Mark H. Karwan, and Marco E. Lubbecke. An image-based approach to detecting structural similarity among mixed integer programs. _INFORMS J. Comput._, 34(4):1849-1870, 2022. doi: 10.1287/JIOC.2021.1117. URL https://doi.org/10.1287/ijoc.2021.1117.
* MARE [2011] JAKUB MARE. _Exploiting structure in integer programs_. PhD thesis, Citeseer, 2011.
* Gleixner et al. [2021] Ambros Gleixner, Gregor Hendel, Gerald Gamrath, Tobias Achterberg, Michael Bastubbe, Timo Berthold, Philipp Christophel, Kati Jarck, Thorsten Koch, Jeff Linderoth, et al. Miplib 2017: data-driven compilation of the 6th mixed-integer programming library. _Mathematical Programming Computation_, 13(3):443-490, 2021.
* Gasse et al. [2022] Maxime Gasse, Simon Bowly, Quentin Cappart, Jonas Charfreitag, Laurent Charlin, Didier Chetelat, Antonia Chmiela, Justin Dumouchelle, Ambros Gleixner, Aleksandr M. Kazachkov, Elias Khalil, Pawel Lichocki, Andrea Lodi, Miles Lubin, Chris J. Maddison, Morris Christopher, Dimitri J. Papageorgiou, Augustin Parjadis, Sebastian Pokutta, Antoine Prouvost, Lara Scavuzzo, Giulia Zarpellon, Linxin Yang, Sha Lai, Akang Wang, Xiaodong Luo, Xiang Zhou, Haohan Huang, Shengcheng Shao, Yuanming Zhu, Dong Zhang, Tao Quan, Zixuan Cao, Yang Xu, Zhewei Huang, Shuchang Zhou, Chen Binbin, He Minggui, Hao Hao, Zhang Zhiyu, An Zhiwu, and Mao Kun. The machine learning for combinatorial optimization competition (ml4co): Results and insights. In Douwe Kiela, Marco Ciccone, and Barbara Caputo, editors, _Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track_, volume 176 of _Proceedings of Machine Learning Research_, pages 220-231. PMLR, 06-14 Dec 2022. URL https://proceedings.mlr.press/v176/gasse22a.html.
* Gamrath and Lubbecke [2010] Gerald Gamrath and Marco E. Lubbecke. Experiments with a generic dantzig-wolfe decomposition for integer programs. In _The Sea_, 2010. URL https://api.semanticscholar.org/CorpusID:14380165.
* Brown et al. [2019] Nathan Brown, Marco Fiscato, Marwin HS Segler, and Alain C Vaucher. Guacamol: benchmarking models for de novo molecular design. _Journal of chemical information and modeling_, 59(3):1096-1108, 2019.
* Kingma and Welling [2013] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* Li et al. [2024] Xijun Li, Fangzhou Zhu, Hui-Ling Zhen, Weilin Luo, Meng Lu, Yimin Huang, Zhenan Fan, Zirui Zhou, Yufei Kuang, Zhihai Wang, Zijie Geng, Yang Li, Haoyang Liu, Zhiwu An, Muming Yang, Jianshu Li, Jie Wang, Junchi Yan, Defeng Sun, Tao Zhong, Yong Zhang, Jia Zeng, Mingxuan Yuan, Jianye Hao, Jun Yao, and Kun Mao. Machine learning insides optverse ai solver: Design principles and applications, 2024.
* Gupta et al. [2020] Prateek Gupta, Maxime Gasse, Elias Khalil, Pawan Mudigonda, Andrea Lodi, and Yoshua Bengio. Hybrid models for learning to branch. _Advances in neural information processing systems_, 33:18087-18097, 2020.
* Gupta et al. [2022] Prateek Gupta, Elias B Khalil, Didier Chetelat, Maxime Gasse, Yoshua Bengio, Andrea Lodi, and M Pawan Kumar. Lookback for learning to branch. _arXiv preprint arXiv:2206.14987_, 2022.
* Nair et al. [2020] Vinod Nair, Sergey Bartunov, Felix Gimeno, Ingrid Von Glehn, Pawel Lichocki, Ivan Lobov, Brendan O'Donoghue, Nicolas Sonnerat, Christian Tjandraitmadja, Pengming Wang, et al. Solving mixed integer programs using neural networks. _arXiv preprint arXiv:2012.13349_, 2020.
* Wang et al. [2024] Jie Wang, Zhihai Wang, Xijun Li, Yufei Kuang, Zhihao Shi, Fangzhou Zhu, Mingxuan Yuan, Jia Zeng, Yongdong Zhang, and Feng Wu. Learning to cut via hierarchical sequence/set model for efficient mixed-integer programming. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, pages 1-17, 2024. doi: 10.1109/TPAMI.2024.3432716.
* Tang et al. [2020] Yunhao Tang, Shipra Agrawal, and Yuri Faenza. Reinforcement learning for integer programming: Learning to cut. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 9367-9376. PMLR, 13-18 Jul 2020. URL https://proceedings.mlr.press/v119/tang20a.html.

* Huang et al. [2021] Zeren Huang, Kerong Wang, Furui Liu, Hui-Ling Zhen, Weinan Zhang, Min jie Yuan, Jianye Hao, Yong Yu, and Jun Wang. Learning to select cuts for efficient mixed-integer programming. _Pattern Recognit._, 123:108353, 2021. URL https://api.semanticscholar.org/CorpusID:235247851.
* Ling et al. [2024] Haotian Ling, Zhihai Wang, and Jie Wang. Learning to stop cut generation for efficient mixed-integer linear programming. _Proceedings of the AAAI Conference on Artificial Intelligence_, 38(18):20759-20767, Mar. 2024. doi: 10.1609/aaai.v38i18.30064. URL https://ojs.aaai.org/index.php/AAAI/article/view/30064.
* Labassi et al. [2022] Abdel Ghani Labassi, Didier Chetelat, and Andrea Lodi. Learning to compare nodes in branch and bound with graph neural networks. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 32000-32010. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/cf5bb18807a3e9cfaaa51e667e18f807-Paper-Conference.pdf.
* Kuang et al. [2023] Yufei Kuang, Xijun Li, Jie Wang, Fangzhou Zhu, Meng Lu, Zhihai Wang, Jia Zeng, Houqiang Li, Yongdong Zhang, and Feng Wu. Accelerate presolve in large-scale linear programming via reinforcement learning. _arXiv preprint arXiv:2310.11845_, 2023.
* Liu et al. [2024] Chang Liu, Zhichen Dong, Haobo Ma, Weilin Luo, Xijun Li, Bowen Pang, Jia Zeng, and Junchi Yan. L2p-MIP: Learning to presolve for mixed integer programming. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=McfYbKnpT8.
* Ye et al. [2024] Huigen Ye, Hua Xu, and Hongyan Wang. Light-MILPopt: Solving large-scale mixed integer linear programs with lightweight optimizer and small-scale training dataset. In _The Twelfth International Conference on Learning Representations_, 2024. URL https://openreview.net/forum?id=2oWRunm67L.
* Ye et al. [2023] Huigen Ye, Hua Xu, Hongyan Wang, Chengming Wang, and Yu Jiang. GNN&GBDT-guided fast optimizing framework for large-scale integer programming. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 39864-39878. PMLR, 23-29 Jul 2023. URL https://proceedings.mlr.press/v202/ye23e.html.
* Sonnerat et al. [2021] Nicolas Sonnerat, Pengming Wang, Ira Ktena, Sergey Bartunov, and Vinod Nair. Learning a large neighborhood search algorithm for mixed integer programs. _ArXiv_, abs/2107.10201, 2021. URL https://api.semanticscholar.org/CorpusID:236154746.
* Huang et al. [2023] Taoan Huang, Aaron Ferber, Yuandong Tian, Bistra N. Dilkina, and Benoit Steiner. Searching large neighborhoods for integer linear programs with contrastive learning. In _International Conference on Machine Learning_, 2023. URL https://api.semanticscholar.org/CorpusID:256598329.
* Ralphs and Galati [2011] Ted K. Ralphs and Matthew V. Galati. Decomposition methods for integer programming. 2011. URL https://api.semanticscholar.org/CorpusID:122662809.
* Dantzig and Wolfe [1960] George B. Dantzig and Philip Wolfe. Decomposition principle for linear programs. _Operations Research_, 8:101-111, 1960. URL https://api.semanticscholar.org/CorpusID:61768820.
* Dantzig and Thapa [2003] George Bernard Dantzig and Mukund Narain Thapa. _Linear programming: Theory and extensions_, volume 2. Springer, 2003.
* Denton et al. [2010] Brian T. Denton, Andrew J. Miller, Hari Balasubramanian, and Todd R. Huschka. Optimal allocation of surgery blocks to operating rooms under uncertainty. _Oper. Res._, 58(4-Part-1):802-816, 2010. doi: 10.1287/OPRE.1090.0791. URL https://doi.org/10.1287/opre.1090.0791.

* Kellerer et al. [2004] Hans Kellerer, Ulrich Pferschy, and David Pisinger. _Multiple Knapsack Problems_, pages 285-316. Springer Berlin Heidelberg, Berlin, Heidelberg, 2004. ISBN 978-3-540-24777-7. doi: 10.1007/978-3-540-24777-7_10. URL https://doi.org/10.1007/978-3-540-24777-7_10.
* Constante-Flores and Conejo [2024] Gonzalo E. Constante-Flores and Antonio J. Conejo. Security-constrained unit commitment: A decomposition approach embodying kron reduction. _Eur. J. Oper. Res._, 319(2):427-441, 2024. doi: 10.1016/J.EJOR.2023.06.013. URL https://doi.org/10.1016/j.ejor.2023.06.013.
* Hu [1963] T. C. Hu. Multi-commodity network flows. _Operations Research_, 11:344-360, 1963. URL https://api.semanticscholar.org/CorpusID:121932073.
* Lonardi et al. [2022] Alessandro Lonardi, Mario Putti, and Caterina De Bacco. Multicommodity routing optimization for engineering networks. _Scientific Reports_, 12(1):7474, May 2022. ISSN 2045-2322. doi: 10.1038/s41598-022-11348-9. URL https://doi.org/10.1038/s41598-022-11348-9.
* Cowen-Rivers et al. [2022] Alexander Cowen-Rivers, Wenlong Lyu, Rasul Tutunov, Zhi Wang, Antoine Grosnit, Ryan-Rhys Griffiths, Alexandre Maravel, Jianye Hao, Jun Wang, Jan Peters, and Haitham Bou Ammar. Hebo: Pushing the limits of sample-efficient hyperparameter optimisation. _Journal of Artificial Intelligence Research_, 74, 07 2022.
* Prouvost et al. [2020] Antoine Prouvost, Justin Dumouchelle, Lara Scavuzzo, Maxime Gasse, Didier Chetelat, and Andrea Lodi. Ecole: A gym-like library for machine learning in combinatorial optimization solvers. In _Learning Meets Combinatorial Algorithms at NeurIPS2020_, 2020. URL https://openreview.net/forum?id=IVc9hqgibyB.

## Table of Contents for Appendix

* A More Related Work
* A.1 Machine Learning for Solving MILP
* A.2 MILP with Block Structure
* B Broader Impacts
* C The Importance of MILPs with Block Structures
* D Explanation: Why do the MILPs Exhibit Block Structures?
* E Introductions the Underlying Learning-Based Solvers
* E.1 Learning to Branch
* E.2 Predict-and-Search
* F Extensive Experiment Results
* F.1 Experiments on Learning to Branch
* F.2 More Results on Different Generation Operators and Modification Ratios
* F.3 Enhancing the Traditional Solvers via Hyperparameter Tuning
* F.4 More Results on Non-Structured MILPs
* F.5 More Results on Real-world Industrial Dataset
* G Visualizations of CCMs
* H Implementation Details
* H.1 Implementation of Predict-and-Search
* H.2 Implementation of Classification Algorithm for Constraints and Variables
* H.3 Implementation of Partition Algorithm for Block Units
* H.4 Details on Block Manipulation
* I More Details on the Data and Experiments
	* I.1 Details on Bipartite Graph Representations
	* I.2 Details on the Benchmarks
	* I.3 Details on Graph Distributional Similarity
More Related Work

### Machine Learning for Solving MILP

In recent years, there has been notable progress in utilizing machine learning approaches to accelerate MILP solvers [36]. The learning-based approaches for solving MILPs can be broadly categorized into two main groups. The first group of works replaces specific components of the solver that greatly impact solving efficiency, such as branching [37; 38; 8; 39], cut selection [40; 41; 42; 9; 43], node selection [7; 44] and presolve [45; 46]. These approaches are integrated into exact MILP solvers, ensuring that the resulting solutions are optimal. However, they also inherit the limitation of exact solvers, which can be time-consuming for solving large-scale instances.

The second category includes learning-aided search approaches, which encompass techniques such as predict-and-optimize [10; 47; 48], large neighbor search [49; 50] and so on. Among these methods, predict-and-search (PS) [10] has gained significant popularity. In PS, a machine learning model is first employed to predict a feasible solution, which is then used as an initial point for a traditional solver to explore the solution space and find the best possible solution within a defined neighborhood. By leveraging the predicted feasible solution, PS effectively reduces the search space of the solver, leading to accelerated search and the discovery of high-quality solutions.

By leveraging machine learning techniques, these approaches have demonstrated substantial improvements in the solving efficiency and solution quality of MILP problems. However, training such learning-based models requires many MILP instances as training data, which can be challenging to obtain in real-world applications [19]. There is still ongoing research in this area to improve the sample efficiency of these solvers [10] and study the MILP generation techniques [19].

### MILP with Block Structure

Operational researchers have observed that a great number of real-world MILP problems exhibit block structures within their constraint coefficient matrices (CCMs) [30; 27]. These problems are prevalent in various applications, including scheduling, planning, and knapsack scenarios [31; 16; 17]. Thus, many researchers have focused on understanding the impact of these block structures on the mathematical properties of the instances, and how to leverage them to accelerate the solving process[21; 22; 23; 24; 51]. Among this stream of research, the Dantzig-Wolfe decomposition (DWD) [52] stands out as one of the most successful applications. Many classical textbooks on linear programming dedicate sections to discussing this decomposition algorithm [53; 21], underscoring its importance. DWD is widely utilized when a CCM contains both block-diagonals and coupling constraints [52], and can significantly accelerate the solving process for large-scale linear programming and mixed-integer linear programming.

To identify the block structures in a raw CCM, the state-of-the-art detecting algorithm is implemented in GCG (Generic Column Generation) [33], distributed with the SCIP framework [6]. This sophisticated algorithm utilizes row and column permutations to effectively cluster the nonzero coefficients in CCMs, thereby revealing distinct block structures, including the useful block-diagonal, bordered block-diagonal, and doubly bordered block-diagonal structures.

## Appendix B Broader Impacts

This paper introduces a MILP instance generation framework (MILP-StuDio) that aims to generate additional MILP instances with highly similar computational properties to the original instances. Our approach holds significant potential in numerous practical applications and important engineering scenarios, including scheduling, planning, facility location, etc. With instances generated by MILP-StuDio, we can improve the performance of block-box learning-based or traditional solvers at a low data acquisition cost. One notable advantage of our proposed method is that it does not rely on the knowledge of MILP formulations and can be applied to general MILP instances, eliminating concerns regarding privacy disclosure. Furthermore, as a plug-and-play module, MILP-StuDio offers great convenience for users.

The Importance of MILPs with Block Structures

The MILPs with block structures are important in industrial and academic fields. We found that MILP instances with block structures are commonly encountered in practical scenarios and have been an important topic in operations research (OR) with much effort [54, 55, 56, 57, 25, 58].

MILP with block structures is an important topic in OR. Analyzing block structures is a critical tool for analyzing the mathematical properties of instances or accelerating the solving process (e.g., Dantzig-Wolfe decomposition [25]) in OR. The MIPLIB dataset also provides visualization results of the constraint coefficient matrices for each instance, highlighting the prevalence of block structures.

The MILP instances with block structures are common and have wide applications in daily production and life. There are many examples where the instances present block structures, including the allocation and scheduling problems [54], the multi-knapsack problem [55], the security-constrained unit commitment problem in electric power systems [56], multicommodity network flow [57], multicommodity transportation problem [58] and so on. In real-world optimization scenarios, there are different types of similar items--such as different workers or machines in planning and scheduling problems, a set of power generation units in the electric power systems, vehicles in the routing problems, and so on--with relevant variables naturally presents a block-structured form in the mathematical models.

The datasets we used in this paper (IP and WA) are from real-world applications. The NeruIPS 2021 Competition of Machine Learning for Combinatorial Optimization [32] released three well-recognized challenging datasets from real-world applications (IP, WA, and the anonymous dataset). Two of the three competition datasets (IP and WA) have block structures. Moreover, instances from the anonymous dataset are selected from MIPLIB with large parts having block structures. These further reflect the wide application of block structures in real-world applications. Thus, our method works in a wide range of problems in practice.

Researchers have investigated specific MILP problems with block structures. MILP with block structures has a large scope in the optimization field and there has been a wide range of works on specific problems with block structures, and they have developed a suite of optimization problems tailored to these problems. For example, the tailored algorithm for the security-constrained unit commitment problem in electric power systems [4], multicommodity transportation problem [6], vehicle routing problem [7], and so on.

Thus, MILP with block structures has a large scope in production and optimization. It has drawn much attention in the industry and academic fields.

## Appendix D Explanation: Why do the MILPs Exhibit Block Structures?

The key reasons why MILP instances exhibit block structures can be summarized as follows.

* Repeated items or entities with similar attributes. In many real-world applications involving scheduling, planning, and packing problems, we often encounter multiple items or entities that share the same type or attributes. For instance, in a scheduling problem, there may be multiple destinations or vehicles that exhibit similar characteristics. Similarly, in a knapsack problem, there can be multiple packages or items that are interchangeable from the perspective of operational research or mathematical modeling.
* Symmetric interactions between different types of items. These repeated items or entities, as well as their interactions, lead to symmetries in the mathematical formulation of the MILP instances. For example, in a scheduling problem, all the vehicles may be able to pick up items from the same set of places and satisfy the demand of the same set of locations.

These symmetries in the problem structure are reflected in the blocks of CCMs, where each block may represent the information associated with a certain vehicle, destination, or other item type.

It is important to note that although MILP-StuDio is designed primarily for MILP instances with block structures, it has a broader application to a wider category of problems, including those with more complex block structures (Appendix H.4), blocks of different sizes (Appendix H.4), and even non-structural MILPs (Appendix F.4).

## Appendix E Introductions the Underlying Learning-Based Solvers

### Learning to Branch

Branching is a critical component of branch-and-bound (B&B) solvers for mixed-integer linear programming (MILP) problems. It involves selecting a fractional variable to partition the feasible region in each iteration. The effectiveness of the chosen variable and the time required to make the branching decision are crucial factors that heavily impact the size of the branch-and-bound search tree and, consequently, the solver's efficiency. Thus, there have been many efforts to develop effective and efficient branching policies. Among the conventional branching policies, the strong branching policy has been demonstrated to yield the smallest branch-and-bound trees. This policy identifies the fractional variable that provides the largest improvement in the bound before performing the branching operation. However, the evaluation process associated with strong branching involves solving a considerable number of linear relaxations of the original MILP, resulting in unacceptable time and computational costs.

To address the limitations of strong branching, [8] proposes a GNN-based approach that employs behavior cloning to imitate the strong branching policy. In this approach, the B&B process is formulated as a Markov decision process, with the solver acting as the environment. At each step, the branching policy receives the current state \(\mathbf{s}\), which includes information on past branching decisions and current solving statistics. Subsequently, the policy selects an action \(\mathbf{a}\) from the set of integer variables with fractional values in the current state. The researchers parameterize the branching policy as a GNN model, which serves as a fast approximation of the strong branching policy.

During the training process, we first run the strong branching expert on the training and testing instances to collect the state-action pair \((\mathbf{s},\mathbf{a})\), forming the training dataset \(\mathcal{D}_{\text{train}}\) and testing dataset \(\mathcal{D}_{\text{test}}\). The GNN branching policy \(\pi_{\theta}\) is then trained by minimizing the cross-entropy loss

\[L(\theta)=-\frac{1}{|\mathcal{D}_{\text{train}}|}\sum_{(\mathbf{s},\mathbf{a}) \in\mathcal{D}_{\text{train}}}\log\pi_{\theta}(\mathbf{a}\mid\mathbf{s}).\]

The imitation accuracy refers to the consistency of the learned GNN policy compared to the strong branching expert on the testing dataset \(\mathcal{D}_{\text{test}}\):

\[\text{ACC}=\frac{1}{|\mathcal{D}_{\text{test}}|}\sum_{(\mathbf{s},\mathbf{a}) \in\mathcal{D}_{\text{test}}}\mathbb{I}\left(\operatorname*{arg\,max}_{ \hat{\mathbf{a}}}\pi_{\theta}(\hat{\mathbf{a}}\mid\mathbf{s})=\mathbf{a} \right),\]

where \(\mathbb{I}\) is the indicator function defined as \(\mathbb{I}(\mathbf{c})=1\) if the condition \(\mathbf{c}\) is true, and \(\mathbb{I}(\mathbf{c})=0\) otherwise. The imitation accuracy of the GNN model reflects the similarity between the learned branching policy and the expert policy, and it significantly impacts the solver's efficiency.

### Predict-and-Search

Different from learning-to-branch, which replaces certain heuristics in a traditional B&B solver, Predict-and-Search (PS) [10] belongs to another category of learning-based solvers that directly predict a feasible solution and subsequently perform the neighborhood search. PS aims to leverage a learning-based model to approximate the solution distribution \(p(\mathbf{x}\mid\mathcal{I})\) given an instance \(\mathcal{I}\) from the instance dataset \(\mathcal{D}\). Given an instance \(\mathcal{I}\), the solution distribution \(p(\mathbf{x}\mid\mathcal{I})\) is defined as follows:

\[p(\mathbf{x}\mid\mathcal{I})=\frac{\exp(-E(\mathbf{x},\mathcal{I}))}{\sum_{ \hat{\mathbf{x}}}\exp(-E(\tilde{\mathbf{x}},\mathcal{I}))},\text{ where the energy function }E(\mathbf{x},\mathcal{I})=\begin{cases}\mathbf{c}^{\top}\mathbf{x},&\text{if } \mathbf{x}\text{ is feasible,}\\ +\infty,&\text{otherwise.}\end{cases}\]

In the prediction step, PS employs a GNN model to approximate the solution distribution for the binary variables in a MILP instance. To train the GNN predictor \(p_{\theta}(\mathbf{x}\mid\mathcal{I})\), PS adopts the assumption, as described in [39], that the variables are independent of each other, i.e., \(p_{\theta}(\mathbf{x}\mid\mathcal{I})=\prod_{i=1}^{n}p_{\theta}(x_{i}\mid \mathcal{I})\). To calculate the prediction target, PS collects a set of \(m\) feasible solutions for each instance \(\mathcal{I}\), from which a vector \(\mathbf{p}=(p_{1},p_{2},\dots,p_{n})^{\top}\) is constructed. Here, \(p_{i}=p(x_{i}=1|\mathcal{I})\) represents the probability of variable \(x_{i}\) being assigned the value 1, given the instance \(\mathcal{I}\). The GNN predictor then outputs the predicted probability \(p_{\theta}(x_{i}=1|\mathcal{I})\) for each variable. Finally, the predictor \(p_{\theta}\) is trained by minimizing the cross-entropy loss

\[L(\theta)=-\frac{1}{N}\sum_{j=1}^{N}\sum_{i=1}^{n}\left(p_{i}\log p_{\theta}(x_ {i}=1\mid\mathcal{I}_{j})+(1-p_{i})\log(1-p_{\theta}(x_{i}=1\mid\mathcal{I}_{ j}))\right),\]where \(N\) represents the number of instances in the training set.

In the search step, PS performs the neighborhood search based on the predicted partial solution \(\hat{\bm{x}}\). This involves employing a traditional solver, such as SCIP [6] or Gurobi [5], to explore the neighborhood \(\mathcal{B}(\hat{\bm{x}},\triangle)\) of \(\hat{\bm{x}}\) in search of an optimal feasible solution. Here \(\triangle\) represents the trust region radius, and \(\mathcal{B}(\hat{\bm{x}},\triangle)=\{\bm{x}\in\mathbb{R}^{n}\mid\|\hat{\bm{x}} -\bm{x}\|_{1}\leq\triangle\}\) is the trust region. The neighborhood search process is formulated as the following MILP problem,

\[\min_{\bm{x}\in\mathbb{R}^{n}}\quad\bm{c}^{\top}\bm{x},\quad\text{s.t.}\quad \bm{A}\bm{x}\leq\bm{b},\bm{l}\leq\bm{x}\leq\bm{u},\bm{x}\in\mathcal{B}(\hat{ \bm{x}},\triangle),\bm{x}\in\mathbb{Z}^{p}\times\mathbb{R}^{n-p}.\]

It is worth noting that the accuracy of the GNN predictor strongly influences the overall solving performance. A reliable and accurate prediction of a feasible solution can effectively reduce the search time. Conversely, an inaccurate prediction may result in an inferior search neighborhood and sub-optimal solution.

## Appendix F Extensive Experiment Results

### Experiments on Learning to Branch

In this section, we conduct experiments on the learning-to-branch task to further demonstrate the effectiveness of MILP-StuDio in enhancing the learning-based solvers.

Experiment SetupWe conduct the learning-to-branch experiments following the setting described in the original paper [8]. In our experiments, we evaluate the performance of the solvers using two benchmark problems: the capacitated facility location (FA) and item placement (IP) problems. The capacitated facility location problem (FA) is chosen as a representative problem that solvers can successfully solve within the given time limit. This problem serves as a benchmark to test the solving efficiency of solvers. The item placement problem (IP) is selected as a benchmark where the solvers struggle to find the optimal solution within the given time limit. By including this challenging problem, we can gain insights into the solvers' ability to find a high-quality primal solution.

The training, validation, and testing instance sets used in this work are identical to those described in the main paper. Specifically, for each benchmark, we use 100 training instances, 20 validation instances, and 50 testing instances. To generate training samples, we execute the strong branching expert on instances in each benchmark, collecting 1,000 training samples, 200 validation samples, and 200 testing samples. For the final evaluation, we test the solving performance on 50 instances.

We implement the model with the code available at https://github.com/ds4dm/learn2branch. This code leverages the state-of-the-art open-source solver SCIP 8.0.3 [6] as the backend solver. During testing, the solving time limit for each instance is set to 1,000 seconds. Consistent with the setting in [8], we disabled solver restarts and only allowed the cutting plane generation module to be employed at the root node.

BaselinesThe first baseline we consider is the GNN model [8], which is trained on an initial set of 1,000 samples collected from 100 original instances (_GNN_). To explore the effectiveness of different instance generation techniques, we use the same set of 100 training instances to generate an additional 1,000 instances using each technique. Subsequently, we collect 10,000 training samples by running a strong branching expert again on these instances. In combination with the original 1,000 samples, we have a comprehensive enriched training dataset consisting of 11,000 samples. Thus, the three generation techniques lead to the following approaches: _GNN+Bowly_, _GNN+G2MILP_\(\eta=x\), _GNN+MILP-StuDio_\(\eta=x\), where \(\eta=x\) implies that we set the modification ratio to be \(x\). Additionally, we compare our method with a GNN model trained on 11,000 samples collected from 1,100 original instances (_GNN-11000_). This comparison allows us to demonstrate that the improvement achieved by our approach is not only due to an increase in the number of training samples but also the high quality of the generated instances. To provide a comprehensive evaluation, we also compare the solving performance of our method with that of the SCIP solver [6] (_SCIP_), which serves as our backend solver for comparison purposes.

Experiment ResultsIn this section, we conduct a comprehensive comparison of imitation accuracy and solving performance. As discussed in Appendix E.1, imitation accuracy serves as a crucial metric for evaluating the performance of the GNN model. Thus, we first compare our methods with other GNN-based baselines, including GNN, GNN-11000, GNN+Bowly, and GNN+G2MILP regarding imitation accuracy. The corresponding results are presented in Table 5, where we observe that MILP-StuDio yields the most significant improvement in imitation accuracy. Interestingly, we find that the instances generated by Bowly in FA are computationally trivial, such that the solver does not need to perform any branching operations to solve them. Consequently, we are unable to collect additional branching samples from these trivial instances to further train the GNN model.

Furthermore, we evaluate the solving performance of the different baselines, as summarized in Table 6. Notably, GNN+MILP-StuDio outperforms all the generation-technique-enhanced baselines and achieves comparable solving performance of GNN-11000. This suggests that incorporating the MILP-StuDio method can boost the performance of the GNN branching policy. We also find that the generated data from MILP-StuDio with different modification ratios can be beneficial for training the GNN model while using instances generated by the G2MILP method may potentially disrupt the training of the GNN model. These findings substantiate the effectiveness of MILP-StuDio in enhancing the GNN branching policy, both in terms of imitation accuracy and solving performance.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \multicolumn{2}{c}{FA} & \multicolumn{2}{c}{IP} \\ \cline{2-4}  & Obj \(\downarrow\) & Time \(\downarrow\) & Obj \(\downarrow\) & Time \(\downarrow\) \\ \hline SCIP & 17865.38 & 58.18 & 22.03 & 1000.00 \\ GNN-1000 & 17865.38 & 51.63 & 20.72 & 1000.00 \\ \hline GNN & 17865.38 & 55.06 & 21.04 & 1000.00 \\ \hline GNN+Bowly & Trivial samples & 22.58 & 1000.00 \\ GNN+G2MILP \(\eta=0.01\) & 17865.38 & 58.81 & 21.09 & 1000.00 \\ GNN+G2MILP \(\eta=0.05\) & 17865.38 & 61.33 & 22.11 & 1000.00 \\ GNN+G2MILP \(\eta=0.10\) & 17865.38 & 59.64 & 20.93 & 1000.00 \\ \hline GNN+MILP-StuDio \(\eta=0.01\) & 17865.38 & 48.28 & **20.03** & **1000.00** \\ GNN+MILP-StuDio \(\eta=0.05\) & **17865.38** & **47.58** & 21.00 & 1000.00 \\ GNN+MILP-StuDio \(\eta=0.10\) & 17865.38 & 53.70 & 20.39 & 1000.00 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Comparison of solving performance in learning-to-branch between our approach and baseline methods, under a \(1,000\)-second time limit. ‘Obj’ represents the objective values achieved by different methods and ‘Time’ denotes the average time used to find the solutions. ‘\(\downarrow\)’ indicates that lower is better. We mark **the best** values in bold. ‘Trivial samples’ implies that the computational hardness of the generated instances is so low that the solver does not need to perform branching to solve them. As a result, we are unable to collect any branching samples for these trivial instances.

\begin{table}
\begin{tabular}{c c c} \hline \hline  & FA & IP \\ \hline GNN-11000 & 0.621 & 0.780 \\ \hline GNN & 0.545 & 0.640 \\ \hline GNN+Bowly & Trivial samples & 0.358 \\ GNN+G2MILP \(\eta=0.01\) & 0.535 & 0.765 \\ GNN+G2MILP \(\eta=0.05\) & 0.545 & 0.705 \\ GNN+G2MILP \(\eta=0.10\) & 0.560 & 0.705 \\ \hline GNN+MILP-StuDio \(\eta=0.01\) & 0.585 & **0.800** \\ GNN+MILP-StuDio \(\eta=0.05\) & **0.605** & 0.790 \\ GNN+MILP-StuDio \(\eta=0.10\) & 0.560 & 0.740 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Comparison of branching accuracy on the testing datasets. ‘Trivial samples’ implies that the computational hardness of the generated instances is so low that the solver does not need to perform branching to solve them. As a result, we are unable to collect any branching samples for these trivial instances. We mark **the best** performance in bold among the methods using the generation technique. We can see that MILP-StuDio can consistently improve the branching accuracy of the learned models.

### More Results on Different Generation Operators and Modification Ratios

In this part, we investigate the influence of different generation operators and modification ratios of MILP-StuDio. 'MILP-StuDio (Optor) \(\eta=x\)' denotes the generation approach that uses the operator Optor ('red' means reduction,'mix' represents mix-up, and 'exp' denotes expansion) and modification ratio \(\eta=x\).

Influence on the Similarity between the Generated and the Original InstancesWe conduct experiments on the FA benchmark. We first compute the graph distributional similarity score for the 100 original instances and 1,000 instances generated using different operators and modification ratios. The results presented in Table 7 suggest that the choice of generation operator and modification ratio can impact the values of the similarity scores. (1) MILP-StuDio consistently outperforms G2MILP in similarity scores. (2) Among the three operators, mix-up achieves the highest similarity scores, with values exceeding 0.8 across the modification ratios. Although the reduction and expansion operators yield lower similarity scores, they are still able to achieve comparable performance in the downstream tasks, as shown in the following paragraph. This finding indicates that there is not necessarily a positive correlation between the structural similarity scores and the benefits for the downstream tasks.

We also evaluate the computational properties of the generated instances. The results in Table 8 show that the three operators are able to preserve the computational hardness of the original instances. Importantly, all the operators succeed in generating feasible instances, which is a crucial requirement for the downstream tasks. In contrast, the instances generated by the G2MILP approach are found to have an extremely low feasible ratio.

Moreover, as the modification ratio increases, we observe a decrease in the similarity scores. Considering the good computational properties we have discussed, this finding suggests that the increasing modification ratios lead to the generation of more novel instances. This observation indicates that MILP-StuDio is able to generate instances increasingly distinct from the original data, while still preserving the computational hardness and feasibility properties.

Influence of Generation Operators on the Performance of Predict-and-SearchWe investigate the influence of instances generated with different operators on the performance of Predict-and-Search (PS). Specifically, we conduct experiments on the FA and IP benchmarks, using 1,000 generated instances based on 100 original instances and different operators. For comparison, we also report the results of PS+MILP-StuDio \(\eta=0.05\). In PS+MILP-StuDio \(\eta=0.05\), we use the three

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \(\eta=0.01\) & \(\eta=0.05\) & \(\eta=0.10\) \\ \hline G2MILP & 0.358 & 0.092 & 0.091 \\ \hline MILP-StuDio (red) & 0.541 & 0.515 & 0.474 \\ \hline MILP-StuDio (mix) & 0.907 & 0.908 & 0.907 \\ \hline MILP-StuDio (exp) & 0.543 & 0.517 & 0.475 \\ \hline \hline \end{tabular}
\end{table}
Table 7: Structural similarity scores between the generated and original instances in FA. We compare the effect of different generation operators and modification ratios. We find that the mix-up operator can generate the most realistic instances.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \(\eta=0.01\) & \(\eta=0.05\) & \(\eta=0.10\) \\ \hline G2MILP & 0.02 (1.7\%) & 0.01 (1.8\%) & 0.01 (2.0\%) \\ \hline MILP-StuDio (red) & 4.29 (100.0\%) & 4.46 (100.0\%) & 4.26 (100.0\%) \\ \hline MILP-StuDio (mix) & 5.35 (100.0\%) & 5.00 (100.0\%) & 4.26 (100.0\%) \\ \hline MILP-StuDio (exp) & 5.18 (100.0\%) & 5.30 (100.0\%) & 5.42 (100.0\%) \\ \hline \hline \end{tabular}
\end{table}
Table 8: Average solving time (s) and feasibility of instances solved by Gurobi. \(\eta\) is the masking ratio. Numbers in the parentheses are feasible ratios in the instances. The instances generated by the G2MILP are found to have an extremely low feasible ratio.

[MISSING_PAGE_FAIL:22]

additional instances generated by the MILP-StuDio framework (tuned-MILP-StuDio) consistently outperforms the other baseline methods.

### More Results on Non-Structured MILPs

While MILP-StuDio is primarily designed to handle MILP instances with block structures in their CCMs, it is also important to evaluate its performance on wider classes of MILP problems, including those that may not exhibit such structured characteristics. Evaluating MILP-StuDio's capabilities in this broader context will highlight its potential for broader applications.

To extend our method to general MILP instances, the framework remains unchanged and we just need to adjust the block decomposition and partition Algorithms 1 and 2. Specifically, the constraint

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Hyperparameter & Type & Min & Max & Description \\ \hline Heuristics & double & 0 & 1 & Time spent in feasibility heuristics. \\ MIPFocus & integer & 0 & 3 & Set the focus of the MIP solver. \\ VarBranch & integer & -1 & 3 & Branch variable selection strategy. \\ BranchDir & integer & -1 & 1 & Preferred branch direction. \\ Presolve & integer & -1 & 2 & Controls the presolve level. \\ PrePasses & integer & -1 & 20 & Presolve pass limit. \\ Cuts & integer & -1 & 3 & Global cut generation control. \\ Method & integer & -1 & 5 & Algorithm used to solve continuous models. \\ \hline \hline \end{tabular}
\end{table}
Table 13: Selected hyperparameters of Gurobi.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & \multicolumn{3}{c}{FA (BKS 17865.38)} & IP (BKS 11.16) \\ \cline{2-4}  & Obj \(\downarrow\) & gap\({}_{\text{abs}}\downarrow\) & Time \(\downarrow\) & Obj \(\uparrow\) & gap\({}_{\text{abs}}\downarrow\) \\ \hline PS & 17865.38 & 0.00 & 7.19 & 11.40 & 0.24 \\ \hline PS+G2MILP \(\eta=0.01\) & \multicolumn{3}{c}{Infeasible} & 11.57 & 0.41 \\ PS+MILP-StuDio \(\eta=0.01\) & 17865.38 & 0.00 & 7.08 & **11.22** & **0.06** \\ \hline PS+G2MILP \(\eta=0.05\) & \multicolumn{3}{c}{Infeasible} & 11.55 & 0.39 \\ PS+MILP-StuDio \(\eta=0.05\) & **17865.38** & **0.00** & **7.07** & 11.29 & 0.13 \\ \hline PS+G2MILP \(\eta=0.10\) & \multicolumn{3}{c}{Infeasible} & 11.62 & 0.46 \\ PS+MILP-StuDio \(\eta=0.10\) & 17865.38 & 0.00 & 7.11 & 11.53 & 0.37 \\ \hline \hline \end{tabular}
\end{table}
Table 12: Comparison of solving performance in PS with different modification ratios of MILP-StuDio and G2MILP, under a \(1,000\)-second time limit. In the IP benchmark, all the approaches reach the time limit of 1,000s, thus we do not consider the Time metric. The notation ‘Infeasible’ implies that a majority of the generated instances are infeasible and thus cannot be used as the training data for PS. ‘\(\downarrow\)’ indicates that lower is better. We mark **the best** values in bold.

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & \multicolumn{3}{c}{FA (BKS 17865.38)} & IP (BKS 11.16) \\ \cline{2-4}  & Obj \(\downarrow\) & gap\({}_{\text{abs}}\downarrow\) & Time \(\downarrow\) & Obj \(\uparrow\) & gap\({}_{\text{abs}}\downarrow\) \\ \hline PS & 17865.38 & 0.00 & 7.19 & 11.40 & 0.24 \\ \hline PS+G2MILP \(\eta=0.01\) & \multicolumn{3}{c}{Infeasible} & 11.57 & 0.41 \\ PS+MILP-StuDio \(\eta=0.01\) & 17865.38 & 0.00 & 7.08 & **11.22** & **0.06** \\ \hline PS+G2MILP \(\eta=0.05\) & \multicolumn{3}{c}{Infeasible} & 11.55 & 0.39 \\ PS+MILP-StuDio \(\eta=0.05\) & **17865.38** & **0.00** & **7.07** & 11.29 & 0.13 \\ \hline PS+G2MILP \(\eta=0.10\) & \multicolumn{3}{c}{Infeasible} & 11.62 & 0.46 \\ PS+MILP-StuDio \(\eta=0.10\) & 17865.38 & 0.00 & 7.11 & 11.53 & 0.37 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Selected hyperparameters of Gurobi.

[MISSING_PAGE_FAIL:24]

### More Results on Real-world Industrial Dataset

To further demonstrate the effectiveness in real-world applications, we also conduct experiments on a real-world scheduling dataset from an anonymous enterprise, which is one of the largest global commercial technology enterprises. The instances do not present clear block structures. The results in Table 19 show that the extended framework generalizes well on the general MILP datasets and has promising potential for real-world applications. The dataset contains 36 training and 12 testing instances. The few training instances reflect the data inavailability problem in real-world applications. We use different data generation methods to enhance the performance of the MLPS solver and list the solving time, objective value, and node number in Table 19 as follows. MILP-StuDio outperforms other baselines in this dataset, highlighting its strong performance and applicability.

## Appendix G Visualizations of CCMs

To provide further insights into the characteristics of the instances generated by MILP-StuDio, we visualize the Constraint Coefficient Matrices (CCMs) of both the original and generated instances in Figure 7-10. The CCM visualization is a powerful tool to understand the structural properties of MILP instances, as it captures the patterns and relationships between the constraints and variables. By comparing the CCMs of the original and generated instances, we can understand how well MILP-StuDio is able to preserve the key structural characteristics of the input problems.

As shown in Figure 7-10, MILP-StuDio is able to successfully maintain the block structures observed in the original instances across the various benchmark problems. This indicates that the generated instances share similar underlying problem structures with the original data, which is a desirable property. This ability to preserve the structural properties of MILP instances is crucial for the effective training and evaluation of machine learning-based MILP solvers. In contrast, Bowly struggles to

\begin{table}
\begin{tabular}{c|c|c c c} \hline  & Original & Bowly & G2MILP & MILP-StuDio \\ \hline Similarity & 1.00 & 0.182 & **0.921** & 0.919 \\ Solving Time & 300.0 & 1.38 & **300.0** & 300.0 \\ Feasibility Ratio & 100\% & 100\% & **100\%** & 100\% \\ \hline \end{tabular}
\end{table}
Table 17: The similarity score, computational hardness, and feasibility ratio between the original and generated instances on the non-structural MIS benchmark. We set the solving time limit 300s and \(\eta=0.05\). We mark **the best** performance in bold.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline  & \multicolumn{3}{c}{Time (1000s time limit)} & \multicolumn{3}{c}{Obj} & \multicolumn{3}{c}{Node Number} \\ \hline  & PS+G2MILP & PS+MILP-StuDio & PS & PS+G2MILP & PS+MILP-StuDio & PS & PS+G2MILP & PS+MILP-StuDie \\ \hline instance1 & 1509652.0 & 1509652.0 & 1509652.0 & 1509652.0 & 144.21 & 146.67 & 131.80 & 38767.00 & 37647.00 & 39513.00 \\ instance2 & 205107.0 & 205197.0 & 205197.0 & 5.83 & 9.51 & 10.11 & 273.00 & 346.00 & 10470.00 \\ instance3 & 7483.00 & 7483.00 & 7483.00 & 29.50 & 32.68 & 38.49 & 555.00 & 751.00 & 901.00 \\ instance4 & 447781.00 & 348454.9 & 318675.99 & 10000.00 & 1000.00 & 1000.00 & 16545.00 & 6261.00 & 5186.00 \\ instance5 & 146600.10 & 1465601.00 & 1465601.00 & 1366.85 & 81.67 & 63.55 & 24317.00 & 11481.00 & 9732.0 \\ instance6 & 1293554.00 & 1293554.00 & 1293554.00 & 38.65 & 25.41 & 32.45 & 3471.00 & 2357.00 & 3002.00 \\ instance7 & 2193554.00 & 1293554.00 & 1293554.00 & 38.96 & 24.82 & 31.76 & 3471.00 & 2357.00 & 3002.00 \\ instance8 & 621510.0 & 621511.00 & 0.18 & 0.35 & 0.28 & 13.00 & 55.00 & 13.00 \\ instance9 & 1578141.00 & 1578141.00 & 1578141.00 & 28.53 & 26.74 & 23.61 & 3083.00 & 3207.00 & 3150.00 \\ instance10 & 1149250.00 & 149250.00 & 1149250.00 & 7.33 & 8.89 & 8.05 & 1.00 & 154.00 & 1.00 \\ instance11 & 100535.00 & 1003555.00 & 1003555.00 & 0.27 & 0.35 & 0.34 & 1.00 & 1.00 & 1.00 \\ instance12 & 1216445.00 & 1216445.00 & 1216445.00 & 23.64 & 23.10 & 13.83 & 1384.00 & 1384.00 & 1.00 \\ \hline Average & 984113.66 & 978386.50 & **973384.92** & 125.35 & 115.02 & **112.86** & 7604.25 & 5528.42 & **5462.42** \\ \hline \end{tabular}
\end{table}
Table 18: The performance of the PS solver trained by instances generated by different methods on the non-structural MIS benchmark. We set the solving time limit 300s and \(\eta=0.05\). We mark **the best** performance in bold.

capture the intricate CCM structures, while G2MILP introduces additional noise that disrupts the block structures in the generated CCMs.

## Appendix H Implementation Details

### Implementation of Predict-and-Search

For model structure, the PS models used in this paper align with those outlined in the original papers [10]. We use the code in https://github.com/sribdcn/Predict-and-Search MILP method to implement PS. For the PS predictor, we leverage a graph neural network comprising four half-convolution layers. We conducted all the experiments on a single machine with NVidia GeForce GTX 3090 GPUs and Intel(R) Xeon(R) E5-2667 V4CPUs 3.20GHz.

In the training process of MILP-StuDio, we set the initial learning rate to be 0.001 and the training epoch to be 1000 with early stopping. In addition, the partial solution size parameter \((k_{0},k_{1})\) and neighborhood parameter \(\Delta\) are two important parameters in PS. The partial solution size parameter (\(k_{0},k_{1},\Delta\)) represents the numbers of variables fixed with values 0 and 1 in a partial solution. The neighborhood parameter \(\Delta\) defines the radius of the searching neighborhood. We list these two parameters used in our experiments in Table 20.

\begin{table}
\begin{tabular}{c c c c c} \hline Benchmark & CA & FA & IP & WA \\ \hline (\(k_{0},k_{1},\Delta\)) & (300,0,10) & (20,0,10) & (50,5,10) & (0,600,5) \\ \hline \end{tabular}
\end{table}
Table 20: The partial solution size parameter (\(k_{0},k_{1},\Delta\)) and neighborhood parameter \(\Delta\).

Figure 8: The visualization of the CCMs of original and generated instances from FA.

Figure 7: The visualization of the CCMs of original and generated instances from CA.

Figure 10: The visualization of the CCMs of original and generated instances from WA.

Figure 9: The visualization of the CCMs of original and generated instances from IP.

[MISSING_PAGE_FAIL:29]

``` Input: The CCM \(\bm{A}\in\mathbb{R}^{m\times n}\) to be analyzed, hyperparameter \(\phi_{1}\), \(\phi_{2}\), \(\phi_{3}\), \(\phi_{4}\), and \(\phi_{5}\), the binary controller \(DB\) to determine whether identify DB-Vars Output: The classification results for each row and column: B-Conslist, M-Conslist, DB-Conslist, Bl-Varslist, and Bd-Varslist
1 Initialize: set B-Conslist, M-Conslist, DB-Conslist, Bl-Varslist, and Bd-Varslist to be empty
2 Compute the column features and row features of \(\bm{A}\) and normalize them
3if\(DB=\)Truethen
4for\(j\) in \(\{1,\cdots,n\}\)do
5ifthe features of the \(j\)-th variable \(\texttt{col\_feat[0]}\)\(>\)\(\phi_{1}\)and \(\texttt{col\_feat[1]}\)\(>\)\(\phi_{2}\)then
6 Append \(j\) to Bd-Varslist
7 end if
8
9 Append \(j\) to Bl-Varslist
10 end if
11
12 end for
13 Append the indices of constraints that contain variables in Bl-Varslist into DB-Conslist
14for\(i\) in \(\{1,\cdots,m\}\)do
15ifthe \(i\)-th constraint is not in DB-Cons and the features of the \(i\)-th constraint \(\texttt{row\_feat[0]}\)\(>\)\(\phi_{3}\), \(\texttt{row\_feat[1]}\)\(>\)\(\phi_{4}\)and \(\texttt{row\_feat[2]}\)\(>\)\(\phi_{5}\)then
16 Append \(i\) to M-Conslist
17 end if
18
19else
20 Append \(i\) to B-Conslist
21
22 end if
23
24 end for
25return B-Conslist, M-Conslist, DB-Conslist, Bl-Varslist, and Bd-Varslist ```

**Algorithm 1**Classification algorithm for constraints and variables in CCMs

``` Input: The image representation \(\tilde{\bm{A}}\in\mathbb{R}^{m\times n}\) of reordered CCM to be analyzed Output: The partition results for the columns
1 Initialize: set Partitionlist to be empty, cut-off point p and q, detection horizon \(\zeta\).
2Criterion 1:\(\tilde{\bm{A}}[i-1][j-1]=\tilde{\bm{A}}[i-2][j-2]=\cdots=\tilde{\bm{A}}[i-\zeta ][j-\zeta]=255\) and \(\tilde{\bm{A}}[i+1][j+1]=0\).
3Criterion 2:\(\tilde{\bm{A}}[i-1][j]=\tilde{\bm{A}}[i-2][j]=\cdots=\tilde{\bm{A}}[i-\zeta ][j]=255\) and \(\tilde{\bm{A}}[i+1][j]=0\).
4for\(j\) in \(\{1,\cdots,n\}\)do
5for\(i\) in \(\{1,\cdots,m\}\)do
6if\(\tilde{\bm{A}}[i][j]=255\)then
7ifCriterion 1 or Criterion 2 is satisfiedthen
8\(q=j\) Append \([p:q]\) to Partitionlist \(p=q\)break
9
10 end if
11
12 end for
13
14 end for
15
16 end for return Partitionlist ```

**Algorithm 2**Partition algorithm for variables in CCMs

``` Input: The image representation \(\tilde{\bm{A}}\in\mathbb{R}^{m\times n}\) of reordered CCM to be analyzed Output: The partition results for the columns
1 Initialize: set Partitionlist to be empty, cut-off point p and q, detection horizon \(\zeta\).
2Criterion 1:\(\tilde{\bm{A}}[i-1][j-1]=\tilde{\bm{A}}[i-2][j-2]=\cdots=\tilde{\bm{A}}[i- \zeta][j-\zeta]=255\) and \(\tilde{\bm{A}}[i+1][j+1]=0\).
3Criterion 2:\(\tilde{\bm{A}}[i-1][j]=\tilde{\bm{A}}[i-2][j]=\cdots=\tilde{\bm{A}}[i-\zeta ][j]=255\) and \(\tilde{\bm{A}}[i+1][j]=0\).
4for\(j\) in \(\{1,\cdots,n\}\)do
5for\(i\) in \(\{1,\cdots,m\}\)do
6if\(\tilde{\bm{A}}[i][j]=255\)then
7ifCriterion 1 or Criterion 2 is satisfiedthen
8\(q=j\) Append \([p:q]\) to Partitionlist \(p=q\)break
9
10 end if
11
12 end for
13
14 end for
15
16 end for
17
18 end for
19
20 end for return Partitionlist ```

**Algorithm 3**Partition algorithm for variables in CCMs

``` Input: The image representation \(\tilde{\bm{A}}\in\mathbb{R}^{m\times n}\) of reordered CCM to be analyzed Output: The partition results for the columns
1 Initialize: set Partitionlist to be empty, cut-off point p and q, detection horizon \(\zeta\).
2Criterion 1:\(\tilde{\bm{A}}[i-1][j-1]=\tilde{\bm{A}}[i-2][j-2]=\cdots=\tilde{\bm{A}}[i-\zeta ][j-\zeta]=255\) and \(\tilde{\bm{A}}[i+1][j+1]=0\).
3Criterion 2:\(\tilde{\bm{A}}[i-1][j]=\tilde{\bm{A}}[i-2][j]=\cdots=\tilde{\bm{A}}[i-\zeta ][j]=255\) and \(\tilde{\bm{A}}[i+1][j]=0\).
4for\(j\) in \(\{1,\cdots,n\}\)do
5for\(i\) in \(\{1,\cdots,m\}\)do
6if\(\tilde{\bm{A}}[i][j]=255\)then
7ifCriterion 1 or Criterion 2 is satisfiedthen
8\(q=j\) Append \([p:q]\) to Partitionlist

[MISSING_PAGE_FAIL:31]

[MISSING_PAGE_FAIL:32]

### Details on Graph Distributional Similarity

To evaluate the distributional similarity between the training and generated MILP instances, we compute 11 graph statistics [19], as detailed in Table 23. First, we calculate these statistics for both the original training instances and the generated instances. Then, we compute the Jensen-Shannon divergence (JSD) \(D_{\text{JS},i}\), for each of the 11 statistics, where \(i=1,\cdots,11\). The JSD ranges from 0 to \(\log 2\), so we standardized the values as follows: \(D_{\text{JS},i}^{\text{std}}=\frac{1}{\log 2}(\log 2-D_{\text{JS},i})\). Finally, we obtain an overall similarity score by taking the mean of the standardized JSD values, \(\text{score}=\frac{1}{11}\sum_{i=1}^{11}D_{\text{JS},i}^{\text{std}}\). The resulting score falls within the range of \([0,1]\), where a higher value indicates stronger distributional similarity between the training and generated instances.

\begin{table}
\begin{tabular}{l l} \hline \hline Feature & Description \\ \hline coef\_dens & Proportion of non-zero entries in CCM \(\bm{A}\). \\ cons\_degree\_mean & Average degree of the constraint vertices. \\ cons\_degree\_std & Standard deviation of the constraint vertex degrees. \\ var\_degree\_mean & Average degree of variable vertices. \\ var\_degree\_std & Standard deviation of the variable vertex degrees. \\ lhs\_mean & Mean of non-zero entries in CCM \(\bm{A}\). \\ lhs\_std & Standard deviation of non-zero entries in CCM \(\bm{A}\). \\ rhs\_mean & Mean of the right-hand-side term \(\mathbf{b}\). \\ rhs\_std & Standard deviation of the right-hand-side term \(\mathbf{b}\). \\ clustering\_coef & Clustering coefficient of the graph. \\ modularity & Modularity of the graph. \\ \hline \hline \end{tabular}
\end{table}
Table 23: Statistics for computing structural distributional similarity

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & CA & FA & IP & WA \\ \hline Constraint Number & 193 & 10201 & 195 & 64306 \\ Variable Number & 500 & 10100 & 1083 & 61000 \\ Block Number & 94 & 100 & 105 & 60 \\ Constraint Type (-Cons) & M, B & M, B & M, B & M, D and DB \\ Variable Type (-Vars) & Bl & Bl & Bl & Bl and Bd \\ \hline \hline \end{tabular}
\end{table}
Table 22: Statistical information of the benchmarks we used in this paper.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We do so in the abstract and introduction of this paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please refer to Section 7 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: This paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Please refer to Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: We will release the code if the paper is accepted. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Please see Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. ** It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please refer to Appendix H. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Please refer to Appendix B. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Please refer to Appendix H and I.2. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [NA] Justification: The paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details on the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.