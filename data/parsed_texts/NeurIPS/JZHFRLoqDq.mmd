# Energy-Guided Continuous Entropic Barycenter Estimation for General Costs

Alexander Kolesov

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

a.kolesov@skoltech.ru

&Petr Mokrov & Igor Udovichenko

Skolkovo Institute of Science and Technology

Moscow, Russia

{p.mokrov, i.udovichenko}@skoltech.ru

Milena Gazdieva

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

m.gazdieva@skoltech.ru

&Gudmund Pammer

Graz University of Technology

Graz, Austria

gudmund.pammer@tugraz.at

Anastasis Kratsios

Vector Institute, McMaster University

Ontario, Canada

kratsioa@mcmaster.ca

&Evgeny Burnaev & Alexander Korotin

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

{e.burnaev, a.korotin}@skoltech.ru

###### Abstract

Optimal transport (OT) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. In short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given OT discrepancies. We propose a novel algorithm for approximating the continuous Entropic OT (EOT) barycenter for arbitrary OT cost functions. Our approach is built upon the dual reformulation of the EOT problem based on weak OT, which has recently gained the attention of the ML community. Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seamlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. For validation, we consider several low-dimensional scenarios and image-space setups, including _non-Euclidean_ cost functions. Furthermore, we investigate the practical task of learning the barycenter on an image manifold generated by a pretrained generative model, opening up new directions for real-world applications. Our code is available at [https://github.com/justkolesov/EnergyGuidedBarycenters](https://github.com/justkolesov/EnergyGuidedBarycenters).

## 1 Introduction

Averaging is a fundamental concept in mathematics and plays a central role in numerous applications. While it is a straightforward operation when applied to scalars or vectors in a linear space, the situation complicates when working in the space of probability distributions. Here, simple convex combinations can be inadequate or even compromise essential geometric features, which necessitates a different way of taking averages. To address this issue, one may carefully select a measure of distance that properly captures similarity in the space of probabilities. Then, the task is to find a procedure which identifies a 'center' that, on average, is closest to the reference distributions.

One good choice for comparing and averaging probability distributions is provided by the family of Optimal Transport (OT) discrepancies [110]. They have clear geometrical meaning and practicalinterpretation [89; 97]. The corresponding problem of averaging probability distributions using OT discrepancies is known as the OT barycenter problem [1]. OT-based barycenters find application in various practical domains: domain adaptation [79], shape interpolation [98], Bayesian inference [101; 102], text scoring [18], style transfer [80], reinforcement learning [77].

Over the past decade, the substantial demand from practitioners sparked the development of various methods tackling the barycenter problem. The research community's initial efforts were focused on the discrete OT barycenter setting, see Appendix B.1 for more details. The **continuous setting** turns out to be even more challenging, with only a handful of recent works devoted to this setup [72; 17; 59; 55; 32; 82; 14]. Most of these works are devoted to specific OT cost functions, e.g., deal with \(\ell_{2}^{2}\) barycenters [59; 55; 32; 82]; while others require non-trivial _a priori_ selections [72] and have limiting expressivity and generative ability [17; 14], see SS3 for a detailed discussion.

**Contributions**. We propose a novel approach for solving Entropy-regularized OT (EOT) barycenter problems, which alleviates the aforementioned limitations of existing continuous OT solvers.

1. We reveal an elegant reformulation of the EOT barycenter problem by combining weak dual form of EOT with the congruence condition (SS4.1); we derive a simple optimization procedure which closely relates to the standard training algorithm of Energy-Based models (SS4.2).
2. We establish the generalization bounds as well as the universal approximation guarantees for our recovered EOT plans, which push the reference distributions to the barycenter (SS4.3).
3. We validate the applicability of our approach on various toy and large-scale setups, including the RGB image domain (SS5). In contrast to previous works, we also pay attention to non-Euclidean OT costs. Specifically, we conduct a series of experiments looking for a barycenter on an image manifold of a pretrained GAN. In principle, the image manifold support may contribute to the interpretability and plausibility of the resulting barycenter distribution in downstream tasks.

**Notations.** We write \(\overline{K}=\{1,2,\ldots,K\}\). Throughout the paper \(\mathcal{X}\subset\mathbb{R}^{D^{\prime}},\mathcal{Y}\subset\mathbb{R}^{D}\) and \(\mathcal{X}_{k}\subset\mathbb{R}^{D_{k}}\) (\(k\in\overline{K}\)) are compact subsets of Euclidean spaces. Continuous functions on \(\mathcal{X}\) are denoted as \(\mathcal{C}(\mathcal{X})\). Probability distributions on \(\mathcal{X}\) are \(\mathcal{P}(\mathcal{X})\). Absolutely continuous probability distributions on \(\mathcal{X}\) are denoted by \(\mathcal{P}_{\text{ac}}(\mathcal{X})\subset\mathcal{P}(\mathcal{X})\). Given \(\mathbb{P}\in\mathcal{P}(\mathcal{X}),\mathbb{Q}\in\mathcal{P}(\mathcal{Y})\), we use \(\Pi(\mathbb{P},\mathbb{Q})\) to designate the set of _transport plans_, i.e., probability distributions on \(\mathcal{X}\times\mathcal{Y}\) with the first and second marginals given by \(\mathbb{P}\) and \(\mathbb{Q}\), respectively. The density of \(\mathbb{P}\in\mathcal{P}_{\text{ac}}(\mathcal{X})\) w.r.t. the Lebesgue measure is denoted by \(\frac{\mathrm{d}\mathbb{P}(x)}{\mathrm{d}x}\).

## 2 Background

First, we recall the formulations of EOT (SS2.1) and the barycenter problem (SS2.2). Next, we clarify the computational setup of the considered EOT barycenter task (SS2.3).

### Entropic Optimal Transport

Consider distributions \(\mathbb{P}\in\mathcal{P}_{\text{ac}}(\mathcal{X})\), \(\mathbb{Q}\in\mathcal{P}_{\text{ac}}(\mathcal{Y})\), a continuous cost function \(c:\mathcal{X}\times\mathcal{Y}\rightarrow\mathbb{R}\) and a regularization parameter \(\epsilon>0\). The _entropic optimal transportation_ (EOT) problem between \(\mathbb{P}\) and \(\mathbb{Q}\)[15; 78] consists of finding a minimizer of

\[\text{EOT}_{c,c}(\mathbb{P},\mathbb{Q})\mathop{=}\limits^{\text{def}}_{\pi\in \Pi(\mathbb{P},\mathbb{Q})}\Bigl{\{}\mathop{\mathbb{E}}_{(x,y)\sim\pi}c(x,y)- \epsilon\mathop{\mathbb{E}}_{x\sim\mathbb{P}}H(\pi(\cdot|x))\Bigr{\}}. \tag{1}\]

Note that (1) is not the only way to formulate EOT. One more popular and equivalent formulation [19; 83; 36] substitutes the conditional entropy term \(\mathop{\mathbb{E}}_{x\sim\mathbb{P}}H(\pi(\cdot|x))\) in (1) with full entropy \(H(\pi)\).

Figure 1: Entropic barycenter \(\mathbb{Q}^{*}\) (5) of \(N=4\) von Mises distributions \(\mathbb{P}_{n}\) on the sphere (see ยง5.1) estimated with our barycenter solver (Algorithm 1). The used transport costs are \(c_{k}(x_{k},y)=\frac{1}{2}\bigl{(}\arccos\left\langle x_{k},y\right\rangle \bigr{)}^{2}\).

A minimizer \(\pi^{*}\in\Pi(\mathbb{P},\mathbb{Q})\) of (1) is called the EOT plan; its existence and uniqueness are guaranteed, see, e.g., [16, Th. 3.3]. In practice, we usually do not require the EOT plan \(\pi^{*}\) but its conditional distributions \(\pi^{*}(\cdot|x)\in\mathcal{P}(\mathcal{Y})\) as they prescribe how points \(x\in\mathcal{X}\) are stochastically mapped to \(\mathcal{Y}\)[42, SS2]. We refer to \(\pi^{*}(\cdot|x)\) as the _conditional plans_.

**Weak OT dual formulation of the EOT problem**. The EOT problem permits several dual formulations. In our paper, we use the one derived from the weak OT theory [39, Theorem 9.5]:

\[\text{EOT}_{c,\epsilon}(\mathbb{P},\mathbb{Q})=\sup_{f\in\mathcal{C}(\mathcal{ Y})}\Big{\{}\underset{x\sim\mathbb{P}}{\mathbb{E}}f^{C}(x)+\underset{y\sim \mathbb{Q}}{\mathbb{E}}f(y)\Big{\}}, \tag{2}\]

where \(f^{C}:\mathcal{X}\rightarrow\mathbb{R}\) is the so-called **weak** entropic \(c\)-transform [9, Eq. 1.2] of the function (_potential_) \(f\). The transform is defined by

\[f^{C}(x)\overset{\text{def}}{=}\underset{\mu\in\mathcal{P}(\mathcal{Y})}{ \min}\Big{\{}\underset{y\sim\mu}{\mathbb{E}}c(x,y)-\epsilon H(\mu)-\underset{ y\sim\mu}{\mathbb{E}}f(y)\Big{\}}. \tag{3}\]

We use the capital \(C\) in \(f^{C}\) to distinguish the weak transform from the classic \(c\)-transform [89, SS1.6] or \((c,\epsilon)\)-transform [76, SS2]. In particular, formulation (2) is not to be confused with the conventional EOT dual, see [78, Appendix A].

For each \(x\in\mathcal{X}\), the minimizer \(\mu_{x}^{f}\in\mathcal{P}(\mathcal{Y})\) of the weak \(c\)-transform (3) exists and is unique. Its density has particular form [78, Theorem 1]. Let \(Z_{c}(f,x)\frac{\text{def}}{=}\int_{\mathcal{Y}}\exp\big{(}\frac{f(y)-c(x,y)}{ \epsilon}\big{)}\text{d}y\), then

\[\frac{\text{d}\mu_{x}^{f}(y)}{\text{d}y}\overset{\text{def}}{=}\frac{1}{Z_{c} (f,x)}\exp\left(\frac{f(y)-c(x,y)}{\epsilon}\right). \tag{4}\]

By substituting (4) into (3) and carrying out straightforward manipulations, we arrive at an explicit formula \(f^{C}(x)=-\epsilon\log Z_{c}(f,x)\), see [78, Equation (14)].

### Entropic OT Barycenter

Consider distributions \(\mathbb{P}_{k}\in\mathcal{P}_{\text{ac}}(\mathcal{X}_{k})\) and continuous cost functions \(c_{k}(\cdot,\cdot):\mathcal{X}_{k}\times\mathcal{Y}\rightarrow\mathbb{R}\) for \(k\in\overline{K}\). Given weights \(\lambda_{k}>0\) with \(\sum_{k=1}^{K}\lambda_{k}=1\), the EOT Barycenter problem is:

\[\mathcal{L}^{*}\underset{\mathbb{Q}\in\mathcal{P}(\mathcal{Y})}{\inf}\sum_{k= 1}^{K}\lambda_{k}\text{EOT}_{c_{k},\epsilon}(\mathbb{P}_{k},\mathbb{Q}), \tag{5}\]

The case where \(\epsilon=0\) corresponds to the classical OT barycenter [1] and falls out of the scope of this paper. Note that the majority of previous research [20, 22, 30, 25, 68, 67] consider a bit different but equivalent EOT barycenter formulation, i.e., which has the **same minimizers**. The objective (5) is known as _Schrodinger_ barycenter problem [15, Table 1], see the extended discussion in Appendix B.3. It is worth noting that under mild assumptions the barycenter \(\mathbb{Q}^{*}\) which delivers optimal value of (5) exists and is unique, see Appendix A.7.

### Computational aspects of the EOT barycenter task

Barycenter problems, such as (5), are known to be challenging in practice [2]. To our knowledge, even when \(\mathbb{P}_{1},\ldots,\mathbb{P}_{K}\) are Gaussian distributions, there is no direct analytical solution neither for our entropic case (\(\epsilon>0\)), see the additional discussion in App. C.4, nor for the unregularized case [3, \(\epsilon=0\)]. Moreover, in real-world scenarios, the distributions \(\mathbb{P}_{k}\) (\(k\in\overline{K}\)) are typically not available explicitly but only through empirical samples (datasets). This aspect leads to the next **learning setup**.

We assume that each \(\mathbb{P}_{k}\) is accessible only by a limited number of i.i.d. empirical samples

\(X_{k}=\{x_{k}^{1},x_{k}^{2},\ldots x_{k}^{N_{k}}\}\sim\mathbb{P}_{k}\). Our aim is to approximate the optimal conditional plans \(\pi_{k}^{*}(\cdot|x_{k})\) between the entire source distributions \(\mathbb{P}_{k}\) and the entire (unknown) barycenter \(\mathbb{Q}^{*}\) solving (5). The recovered plans should provide the _out-of-sample_ estimation, i.e., allow generating samples from \(\pi_{k}^{*}(\cdot|x_{k}^{\text{new}})\), where \(x_{k}^{\text{new}}\) is a new sample from \(\mathbb{P}_{k}\) which is not necessarily present in the train sample.

This setup corresponds to **continuous OT**[72, 59]. It differs from the **discrete OT** setup [19, 20] which aims to solve the barycenter task for _discrete_ empirical distributions. Discrete OT are not well-suited for the out-of-sample estimation required in the continuous OT setup.

## 3 Related works

The taxonomy of OT solvers is large. Due to space constraints, we discuss here only methods within the _continuous OT learning setup_ that solve the (E-)OT barycenter problem. These methods approximate OT maps or plans between the distributions \(\mathbb{P}_{k}\) and the barycenter \(\mathbb{Q}^{*}\) rather than just their empirical counterparts that are available from the training samples. A broader discussion of general-purpose discrete/continuous (E-)OT solvers is in Appendix B.1.

**Continuous OT barycenter solvers are based on the unregularized or regularized OT barycenter problem within the continuous OT learning setup. The works [59, 32, 82, 55] are designed _exclusively_ for the quadratic Euclidean cost \(\ell^{2}(x,y)\stackrel{{\text{def}}}{{=}}\frac{1}{2}\|x-y\|_{2}^{2}\). The OT problem with this particular cost exhibits several advantageous theoretical properties [4, SS2] which are exploited by the aforementioned articles to build efficient procedures for barycenter estimation algorithms. In particular, [59, 32] utilize ICNNs [6] which parameterize convex functions, and [82] relies on a specific tree-based Schrodinger Bridge reformulation. In contrast, our proposed approach is designed to handle the EOT problem with _arbitrary_ cost functions \(c_{1},\ldots,c_{K}\). In [72], they also consider regularized OT with non-Euclidean costs. Similar to our method, they take advantage of the dual formulation and exploit the so-called congruence condition (SS4). However, their optimization procedure substantially differs. It necessitates selecting a _fixed prior_ for the barycenter, which can be non-trivial. The work [14] takes a step further by directly optimizing the barycenter distribution in a variational manner, eliminating the need for a _fixed prior_. This modification increases the complexity of optimization and requires specific parametrization of the variational barycenter. In [17], the authors also parameterize the barycenter as a generative model. Their approach does not recover the OT plans, which differs from our learning setup (SS2.3). A summary of the key properties is provided in Table 1, highlighting the fact that our approach overcomes many imperfections of competing methods. We are also aware of the novel continuous OT barycenter solver [52]. This approach is more recent than ours and is _significantly_ based on the ideas from our article. Because of this, we exclude it from our comparisons.

## 4 Proposed Barycenter Solver

In the first two subsections, we work out our optimization objective (SS4.1) and its practical implementation (SS4.2). In SS4.3, we alleviate the gap between the theory and practice by offering finite sample approximation guarantees and universality of NNs to approximate the solution.

### Deriving the optimization objective

In what follows, we analyze (5) from the dual perspectives. We introduce \(\mathcal{L}:\mathcal{C}(\mathcal{Y})^{K}\to\mathbb{R}\):

\[\mathcal{L}(f_{1},\ldots,f_{K})\stackrel{{\text{def}}}{{=}}\sum_ {k=1}^{K}\lambda_{k}\Big{\{}\underset{x_{k}\sim\mathbb{P}_{k}}{\mathbb{E}}f_{k} ^{C_{k}}(x_{k})\Big{\}}\qquad\Big{[}\qquad=-\epsilon\sum_{k=1}^{K}\lambda_{k} \Big{\{}\underset{x_{k}\sim\mathbb{P}_{k}}{\mathbb{E}}\log Z_{c_{k}}(f_{k},x_{k })\Big{\}}\Big{]}.\]

Here \(f_{k}^{C_{k}}\) denotes the weak entropic \(c_{k}\)-transform (3) of \(f_{k}\). Following SS2.1, we see that it coincides with \(-\epsilon\log Z_{c_{k}}(f_{k},x_{k})\). Below we formulate our main theoretical result, which will allow us to solve the EOT barycenter task without optimization over all distributions on \(\mathcal{Y}\).

**Theorem 4.1** (Dual formulation of the EOT barycenter problem [proof ref.]).: _Problem (5) permits the following dual formulation:_

\[\mathcal{L}^{*}=\underset{f_{1},\ldots,f_{K}\in\mathcal{C}(\mathcal{Y});}{ \sup}\mathcal{L}(f_{1},\ldots,f_{K}). \tag{6}\]

We refer to the constraint \(\sum_{k=1}^{K}\lambda_{k}f_{k}=0\) as the **congruence** condition w.r.t. weights \(\{\lambda_{k}\}_{k=1}^{K}\). The potentials \(f_{k}\) appearing in (6) play the same role as in (2). Notably, when \(\mathcal{L}(f_{1},\ldots,f_{K})\) is close to \(\mathcal{L}^{*}\), the conditional optimal transport plans \(\pi_{k}^{*}(\cdot|x_{k}),x_{k}\in\mathcal{X}_{k}\), between \(\mathbb{P}_{k}\) and the barycenter distribution \(\mathbb{Q}^{*}\) can be approximately recovered through the potentials \(f_{k}\). This intuition is formalized in Theorem 4.2 below. First, for \(f_{k}\in\mathcal{C}(\mathcal{Y})\), we define

\[\mathrm{d}\pi^{f_{k}}(x_{k},y)\stackrel{{\text{def}}}{{=}} \mathrm{d}\mu^{f_{k}}_{x_{k}}(y)\mathrm{d}\mathbb{P}_{k}(x_{k})\]

and set \(\mathbb{Q}^{f_{k}}\in\mathcal{P}(\mathcal{Y})\) to be the second marginal of \(\pi^{f_{k}}\).

**Theorem 4.2** (Quality bound of plans recovered from dual potentials [proof ref.]).: _Let \(\{f_{k}\}_{k=1}^{K},f_{k}\in\mathcal{C}(\mathcal{Y})\) be congruent potentials. Then we have_

\[\mathcal{L}^{*}-\mathcal{L}(f_{1},\ldots,f_{K})=\epsilon\sum_{k=1}^{K}\lambda _{k}\textbf{KL}\left(\pi_{k}^{*}\|\pi^{f_{k}}\right)\geq\epsilon\sum_{k=1}^{K }\lambda_{k}\textbf{KL}\left(\mathbb{Q}^{*}\|\mathbb{Q}^{f_{k}}\right), \tag{7}\]

_where \(\pi_{k}^{*}\in\Pi(\mathbb{P}_{k},\mathbb{Q}^{*}),k\in\overline{K}\) are the EOT plans between \(\mathbb{P}_{k}\) and the barycenter distribution \(\mathbb{Q}^{*}\)._

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Method** & \begin{tabular}{c} **Admissible** \\ **OT costs** \\ \end{tabular} & \begin{tabular}{c} **Learns** \\ **OT plans** \\ \end{tabular} & \begin{tabular}{c} **Max considered** \\ **data dim** \\ \end{tabular} & 
\begin{tabular}{c} **Regularization** \\ **Remarkation** \\ \end{tabular} \\ \hline
[72] & general & yes & 8D, no images & 
\begin{tabular}{c} Entropic/Quadratic \\ with fixed prior \\ \end{tabular} \\
[17] & general & no & 
\begin{tabular}{c} 133233 (MINST) \\ \end{tabular} & Entropic (Sinkhorn) \\
[59] & only \(l_{2}^{2}\) & yes & \begin{tabular}{c} 2650, no images \\ 13328,238 (MINST) \\ \end{tabular} & 
\begin{tabular}{c} requires fixed prior \\ \end{tabular} \\
[55] & only \(l_{2}^{2}\) & yes & \begin{tabular}{c} 133628,238 (MINST) \\ \end{tabular} & 
\begin{tabular}{c} requires fixed prior \\ \end{tabular} \\
[55] & only \(l_{2}^{2}\) & yes & \begin{tabular}{c} 13628,238 (MINST) \\ \end{tabular} & 
\begin{tabular}{c} requires fixed prior \\ \end{tabular} \\
[58] & only \(l_{2}^{2}\) & yes & 
\begin{tabular}{c} 13628,238 (MINST) \\ \end{tabular} & Entropic \\
[14] & general & yes & 
\begin{tabular}{c} 2560, Gaussions only \\ \end{tabular} & Entropic/Quadratic \\ \hline
**Ours** & general & yes & 
\begin{tabular}{c} 336484 (CelebA) \\ \end{tabular} & Entropic \\ \hline \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of continuous OT bary solvers

[MISSING_PAGE_FAIL:5]

where \(\xi_{l}\sim\mathcal{N}(0,I_{D})\), \(l\in\{0,1,2,\ldots,L\}\), \(L\) is a number of steps, and \(\eta>0\) is a step size. Note that the iteration procedure above could be straightforwardly adapted to a batch scenario, i.e., we can simultaneously simulate the whole batch of samples \(Y_{k}^{(l)}\) conditioned on \(X_{k}^{(l)}\). The particular values of number of steps \(L\) and step size \(\eta\) are reported in the details of the experiments, see Appendix C. An alternative importance sampling-based approach for optimizing (9) is presented in Appendix D.

Inference. We use the same ULA procedure for sampling from the recovered optimal conditional plans \(\pi^{f_{\theta^{*},k}}(\cdot|x_{k})\), see the details on the hyperparameters \(\bar{L},\eta\) in SS5.

**Relation to prior works.** Learning a distribution of interest via its energy function (EBMs) is a well-established direction in generative modelling research [69, 111, 28, 100]. Similar to ours, the key step in most energy-based approaches is the MCMC procedure which recovers samples from a distribution accessible only by an unnormalized log density. Typically, various techniques are employed to improve the stability and convergence speed of MCMC, see, e.g., [27, 34, 113]. The majority of these techniques can be readily adapted to complement our approach. At the same time, the primary goal of this study is to introduce and validate the methodology for computing EOT barycenters in an energy-guided manner. Therefore, we opt for the **simplest** MCMC algorithm, even **without the replay buffer**[46], as it serves our current objectives.

### Generalization Bounds and Universal Approximation with Neural Nets

In this subsection, we answer the question of how far the recovered plans are from the EOT plan \(\pi_{k}^{*}\) between \(\mathbb{P}_{k}\) and \(\mathcal{Q}\). In practice, for each distribution \(\mathbb{P}_{k}\) we know only the empirical samples \(X_{k}=\{x_{k}^{1},x_{k}^{2},\ldots x_{k}^{N_{k}}\}\sim\mathbb{P}_{k}\), i.e., finite datasets. Besides, the available potentials \(f_{k}\), \(k\in\overline{K}\) come from restricted classes of functions and satisfy the congruence condition. More precisely, we have \(f_{k}=g_{k}-\sum_{k=1}^{K}\lambda_{k}g_{k}\) (SS4.2), where each \(g_{k}\) is picked from some class \(\mathcal{G}_{k}\) of neural networks. Formally, we write \((f_{1},\ldots,f_{K})\in\overline{\mathcal{F}}\) to denote the congruent potentials constructed this way from the functional classes \(\mathcal{G}_{1},\ldots,\mathcal{G}_{K}\). Hence, in practice, we optimize the _empirical version_ of (8):

\[\max_{(f_{1},\ldots,f_{K})\in\overline{\mathcal{F}}} \widehat{\mathcal{L}}(f_{1},\ldots,f_{K}) \stackrel{{\text{def}}}{{=}} \max_{(f_{1},\ldots,f_{K})\in\overline{\mathcal{F}}}\sum_{k=1}^{K} \frac{\lambda_{k}}{N_{k}}\sum_{n=1}^{N_{k}}f_{k}^{C_{k}}(x_{k}^{n});\] \[(\widehat{f_{1}},\ldots\widehat{f_{Substituting (11) or (12) to (10) immediately provides the _statistical consistency_ when \(N_{1},\ldots,N_{K}\to\infty\), i.e., vanishing of the estimation error when the sample size grows.

The case **(a)** here is not very practically useful as the rate suffers from the curse of dimensionality. Still, this result points to one intriguing property of our solver. Namely, we may take **arbitrarily large** set \(\mathcal{F}_{k}\) (even \(\mathcal{F}_{k}=\mathcal{C}(\mathcal{Y})\)!) and still have the guarantees of learning the barycenter. This happens because of \(C_{k}\)-transforms: informally, they make functions \(f_{k}\in\mathcal{F}_{k}\) smoother and "simplify" the set \(\mathcal{F}_{k}\). In our experiments, we always work with the costs as in **(b)**. As a result, our estimation error is \(O(\sum_{k=1}^{K}N_{k}^{-1/2})\); this is a _standard fast and dimension-free convergence rate_. In practice, \(\mathcal{F}_{k}\) are usually neural nets. They are indeed bounded, as required in **(b)**, if their weights are constrained.

While the estimation error usually decreases when the sample sizes tend to infinity, it is natural to wonder whether the approximation error can be also made arbitrarily small. We positively answer this question when the standard fully-connected neural nets (multi-layer perceptrons) are used.

**Theorem 4.6** (Vanishing Approximation Error [proof ref.]).: _Let \(\sigma:\mathbb{R}\to\mathbb{R}\) be an activation function. Assume that it is non-affine and there is an \(\widetilde{x}\in\mathbb{R}\) at which \(\sigma\) is differentiable and \(\sigma^{\prime}(\widetilde{x})\neq 0\). Then for every \(\delta>0\) there exist \(K\) multi-layer perceptrons \(g_{k}:\mathbb{R}^{D}\to\widetilde{\mathbb{R}}\) with activations \(\sigma\) for which the congruent functions \(f_{k}=g_{k}-\sum_{k=1}^{K}\lambda_{k}g_{k}\) satisfy_

\[\sum_{k=1}^{K}\lambda_{k}\text{KL}\left(\pi_{k}^{*}\|\pi^{f_{k}}\right)=( \mathcal{L}^{*}-\mathcal{L}(f_{1},\ldots,f_{K}))/\epsilon<\delta/\epsilon.\]

_Furthermore, each \(g_{k}\) has width at-most \(D+4\)._

Importantly, our Theorem 4.6 is more than just result on universal approximation since it deals with (i) _congruent_ potentials and (ii) entropic \(C_{k}\)-transforms. In particular, only specific properties of the entropic \(C_{k}\)-transforms allow deriving the desired universal approximation statement, see the proof.

**Summary.** Our results of this section show that both the estimation and approximation errors can be made arbitrarily small given a sufficient amount of data and large neural nets, allowing to perfectly recover the EOT plans \(\pi_{k}^{*}\).

**Relation to prior works.** To our knowledge, the generalization and the universal approximation are novel results with no analogues established for any other continuous barycenter solver. Our analysis shows that the EOT barycenter objective (8) is well-suited for statistical learning and approximation theory tools. This aspect distinguishes our work from the predecessors, where complex optimization objectives may not be as amenable to rigorous study.

### Learning EOT barycenter on data manifold

Averaging complex data distributions by means of EOT barycenter directly in the data space may be undesirable. In particular, for image data domain:

* the entropic barycenter contains noisy images, see, e.g., our MNIST 0/1 experiment, SS5.2. This is due to the "blurring bias" bias [15, 49] of our entropic barycenter setup and reliance on MCMC.
* searching for (entropic) barycenter is not very practical for standard OT cost functions like \(\ell^{2}\). It is known that the true unregularized (\(\epsilon=0\)) \(\ell^{2}\)-barycenter of several image domains consists of just some pixel-wise averages of images from these source domains, which is not practically useful.

To alleviate the problem, we propose solving the (entropic) barycenter problem on some _a priori_ known data manifold \(\mathcal{M}\), where we want the barycenter to be concentrated on. In our experiments (SS5.2, SS5.3) these manifolds are given by pre-trained StyleGAN [50] generator models \(G:\mathcal{Z}\to\mathcal{Y}\); \(\mathcal{Z}\) is the _latent_ space, \(\mathcal{M}=G(\mathcal{Z})\). Technically speaking, to adapt our Alogithm 1 for manifold-constrained setup, we propose solving the barycenter problem in _latent_ space \(\mathcal{Z}\) with _modified_ cost functions \(c_{k,G}(x_{k},z):=c_{k}(x_{k},G(z))\). We emphasize that such costs are **general** (not \(\ell^{2}\) cost!) because \(G\) is a non-trivial StyleGAN generator. Hence, while our proposed manifold-constrained barycenter learning setup could be used on par with other OT barycenter solvers, these barycenter solvers **should** support general costs. In particular, the majority of competitive methods from Table 1_are not adjustable to the manifold setup_ as they work exclusively with \(\ell^{2}\).

**Relation to prior works.** While the utilization of data manifolds given by pre-trained (foundational) models is ubiquitous in generative modeling, the adaptation of this technique for Optimal Transport barycenter is a novel idea. Apart from our work, this idea is exploited in follow-up paper [52].

Experiments

We assess the performance of our barycenter solver on small-dimensional illustrative setups (SS5.1) and in image spaces (SS5.2, SS5.3). The source code for our solver is written in the PyTorch framework and available at [https://github.com/justkolesov/EnergyGuidedBarycenters](https://github.com/justkolesov/EnergyGuidedBarycenters). The experiments are issued in the form of convenient *.ipynb notebooks. Reproducing the most challenging experiments (SS5.2, SS5.3) requires less than \(12\) hours on a single TeslaV100 GPU. The details of the experiments, extended experimental results are in Appendix C, additional experiments with single-cell data are given in Appendix C.5.

**Disclaimer.** Evaluating how well our solver recovers the EOT barycenter is challenging because the ground truth barycenter is typically unknown. In some cases, the true _unregularized_ barycenter (\(\epsilon=0\)) can be derived (see below). The EOT barycenter for sufficiently small \(\epsilon>0\) is expected to be close to the unregularized one. Therefore, in most cases, our evaluation strategy is to compare the computed EOT barycenter (for small \(\epsilon\)) with the unregularized one. In particular, we use this strategy to quantitatively evaluate our solver in the Gaussian case, see Appendix C.4.

### Barycenters of Toy Distributions

**2D Twister.** Consider the map \(u:\mathbb{R}^{2}\to\mathbb{R}^{2}\) which, in the _polar coordinate system_, is represented by \(\mathbb{R}_{+}\times[0,2\pi)\ni(r,\theta)\mapsto\left(r,(\theta-r)\text{ mod }2\pi\right)\). The cartesian version of \(u\) is presented in Appendix C.1. Let \(\mathbb{P}_{1},\mathbb{P}_{2},\mathbb{P}_{3}\) be \(2\)-dimensional distributions as shown in Fig. 1(a). For these distributions and uniform weights \(\lambda_{k}=\frac{1}{3}\), the unregularized barycenter (\(\epsilon=0\)) for the **twisted** cost \(c_{k}(x_{k},y)=\frac{1}{2}\|u(x_{k})-u(y)\|^{2}\) can be derived analytically, see Appendix C.1. The barycenter is the centered Gaussian distribution which is also shown in Fig. 1(a). We run the experiment for this cost with \(\epsilon=10^{-2}\), and the results are recorded in Fig. 1(b). We see that it qualitatively coincides with the true barycenter. For completeness, we also show the EOT barycenter computed with our solver for \(\ell^{2}(x,y)=\frac{1}{2}\|x-y\|^{2}\) costs (Fig. 1(c)) and the same regularization \(\epsilon\). The true \(\ell^{2}\) barycenter is estimated by using the free_support_barycenter solver from POT package [33]. We stress that the twisted cost barycenter and \(\ell^{2}\) barycenter differ, and so do the learned conditional plans: the \(\ell^{2}\) EOT plan (Fig. 1(d)) expectedly looks more well-structured while for the twisted cost (Fig. 1(b)) it becomes more chaotic due to non-trivial structure of this cost.

**Sphere.** In this experiment, we look for the barycenter of four von Mises distributions \(\mathbb{P}_{n}\) supported on 3D sphere, see Figure 1. The cost functions are \(c_{k}(x_{k},y)=\frac{1}{2}\arccos^{2}\langle x_{k},y\rangle\), the regularization is \(\epsilon=10^{-2}\). The learned potentials \(f_{\theta,k}\) operate with ambient \(\mathbb{R}^{3}\) vectors. When performing MCMC, we project each Langevin step to the sphere. Our qualitative results are shown on Figure 1. While the ground truth solution to the considered problem is unknown, the learned barycenter looks reasonable. This showcases the applicability of our approach to non-standard non-quadratic experimental setups.

### Barycenters of MNIST Classes 0 and 1

A classic experiment considered in the continuous barycenter literature [32, 55, 82, 17] is averaging of distributions of MNIST 0/1 digits with weights \((\frac{1}{2},\frac{1}{2})\) in the grayscale image space \(\mathcal{X}_{1}\!=\!\mathcal{X}_{2}\!=\!\mathcal{Y}\!=\![-1,1]^{32\times 32}\). The true unregularized (\(\epsilon=0\)) \(\ell^{2}\)-barycenter images \(y\) are direct pixel-wise averages \(\frac{x_{1}+x_{2}}{2}\) of pairs of images \(x_{1}\) and

Figure 3: Samples from the StyleGAN \(G\) defining the polluted manifold \(\mathcal{M}\).

Figure 2: _2D wister example:_ The true barycenter of 3 comets vs. the one computed by our solver with \(\epsilon=10^{-2}\). Two costs \(c_{k}\) are considered: the twisted cost (1(a), 1(b)) and \(\ell^{2}\) (1(c), 1(d)).

coming from the \(\ell^{2}\) OT plan between 0's (\(\mathbb{P}_{1}\)) and 1's (\(\mathbb{P}_{2}\)). In Fig. 5, we show the unregularized \(\ell^{2}\) barycenter computed by [32, SCWB], [55, WIN].

**Data space EOT barycenter.** To begin with, we employ our solver to compute the \(\epsilon\)-regularized EOT \(\ell^{2}\)-barycenter directly in the image space \(\mathcal{Y}\) for \(\epsilon=10^{-2}\). We emphasize that the true entropic barycenter slightly differs from the unregularized one. To be precise, it is expected that regularized barycenter images are close to the unregularized barycenter images but with additional noise. In Fig. 5, we see that our solver (data space) recovers the noisy barycenter images exactly as expected.

**Manifold-constrained EOT barycenter.** Following the reasoning from SS4.4, we propose to restrict the search space for our algorithm to some pre-defined manifold \(\mathcal{M}\). As discussed earlier, the support of the image-space unregularized \(\ell^{2}\)-barycenter is a certain _subset_\(\mathcal{M}^{\text{\tiny\text{def}}}\equiv\left\{\frac{x_{1}+x_{2}}{2} \mid x_{1}\in\text{Supp}(\mathbb{P}_{1}),x_{2}\in\text{Supp}(\mathbb{P}_{2})\right\}\). To achieve this, we train a StyleGAN [50] model \(G:\mathcal{Z}\rightarrow\mathcal{Y}\) with \(\mathcal{Z}=\mathbb{R}^{512}\) to generate some **even larger** manifold \(\mathcal{M}=G(\mathcal{Z})\) which is expected to contain \(\mathcal{M}^{\prime}\). Namely, we use all possible pixel-wise half-sums \(\frac{x_{1}+x_{2}}{2}\) of digits \(0\) as \(x_{1}\) and \(\{1,4,7\}\) as \(x_{2}\), see Figure 3 with the trained StyleGAN samples. That is, our final constructed manifold \(\mathcal{M}\) is **polluted** with additional samples (e.g., averages of digits 0 and 7) which should not to lie in the support of the barycenter. Then, we use our solver with \(\epsilon=10^{-2}\) to search for the barycenter of 0/1 digit distributions on \(\mathcal{X}_{1},\mathcal{X}_{2}\) which lies in the latent space \(\mathcal{Z}\) w.r.t. costs \(c_{k,G}(x,z)\stackrel{{\text{def}}}{{=}}\frac{1}{2}\|x-G(z)\|^{2}\). This can be interpreted as learning the EOT \(\ell^{2}\)-barycenter in the ambient space but constrained to

Figure 4: _Experiment on the Aw, celeba! barycenter dataset._ The plots compare the transported inputs \(x_{k}\sim\mathbb{P}_{k}\) to the barycenter learned by various solvers. The true unregularized \(\ell^{2}\) barycenter of \(\mathbb{P}_{1},\mathbb{P}_{2},\mathbb{P}_{3}\) are the clean celebrity faces, see [55, ยง5].

Figure 5: Qualitative comparison of barycenters of MNIST 0/1 digit classes computed with barycenter solvers in the image space w.r.t. the pixel-wise \(\ell^{2}\). Solvers SCWB and WIN only learn the unregularized barycenter (\(\epsilon=0\)) directly in the data space. In turn, our solver learns the EOT barycenter in data space as well as it can learn EOT barycenter restricted to the StyleGAN manifold (\(\epsilon=10^{-2}\)).

the StyleGAN-parameterized manifold \(G(\mathcal{Z})\). The barycenter \(\mathbb{Q}^{*}\) is some distribution of the latent variables \(z\), which can be pushed to the manifold \(G(\mathcal{Z})\subset\mathcal{Y}\) via \(G(z)\).

The results are in Fig. 5. There is **(a)** no noise compared to the data-space EOT barycenter because of the manifold constraint, and **(b)** our solvers correctly ignores polluted samples from \(\mathcal{M}\).

### Evaluation on the Ave, celeba! Dataset

In [55], the authors developed a theoretically grounded methodology for finding probability distributions whose unregularized \(\ell^{2}\) barycenter is known by construction. Based on the CelebA faces dataset [73], they constructed an Ave, celeba! dataset containing 3 degraded subsets of faces. The true \(\ell^{2}\) barycenter w.r.t. the weights \((\frac{1}{4},\frac{1}{2},\frac{1}{4})\) is the distribution of Celeba faces itself. This dataset is used to test how well our approach recovers the barycenter.

We follow the EOT manifold-constrained setup (SS4.4) and train the StyleGAN on unperturbed celeba faces. This might sound a little bit unfair, but our goal is to demonstrate the learned transport plan to the constrained barycenter rather than unconditional barycenter samples (recall the setup in SS2.3). Hence, we learn the constrained EOT barycenter with \(\epsilon=10^{-4}\). In Fig. 4, we present the results, depicting samples from the learned plans from each \(\mathbb{P}_{k}\) to the barycenter. Overall, the map is qualitatively good, although sometimes failures in preserving the image content may occur. This is presumably due to MCMC inference getting stuck in local minima of the energy landscape. For comparison, we also show the results of the solvers by [32, SCWB], [55, WIN]. Additionally, we report the FID score [45] for images mapped to the barycenter in Table 2 (std. deviations for our method correspond to running the inference with different random seeds). Owing to the manifold-constrained setup, the FID score of our solver is significantly smaller.

## 6 Potential Impact, Limitations and Broader Impact

**Potential impact.** In our work, we propose a novel approach for solving EOT barycenter problems which is applicable to _general OT costs_. From the practical viewpoint, we demonstrate the ability to restrict the sought-for barycenter to the _image manifold_ by utilizing a pretrained generative model. Our findings may be applicable to a list of important real-world applications, see Appendix B.2. We believe that our large-scale barycenter solver will leverage industrial & socially-important problems.

**Methodological limitations**. The methodological limitations of our approach are mostly the same as those of EBMs. It is worth mentioning the usage of MCMC during the training/inference. The basic ULA algorithm which we use in SS4.2 may poorly converge to the desired distribution \(\mu_{x}^{f}\). In addition, MCMC sampling is time-consuming. We leave the search for more efficient sampling procedures for our solver, e.g., [71, 99, 43, 81, 47, 108, 66, 26], for future research. We also note that our theoretical analysis in SS4.3 does not take into the account the optimization errors appearing due to the gradient descent and MCMC. The analysis of these quantities is a completely different domain in machine learning and out of the scope of our work. As the most generative modelling research, we do not attempt to analyse these errors.

**Problem setup limitations.** Our paper aims at solving Entropic OT barycenter problem. In the image data space, due to utilization of the Entropy, the learned barycenter distribution may contain noisy images. However, the utilization of our proposed StyleGAN-inspired manifold technique **entirely** alleviates the problem with the noise. This is demonstrated by our latent-space experiments with MNIST 0/1 (manifold space) and Ave Celeba! dataset.

**Broader impact.** This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

## 7 Acknowledgements

Skoltech was supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021). AK acknowledges financial support from the NSERC Discovery Grant No. RGPIN-2023-04482. We would like to express special thanks to Vladimir Vanovskiy from Skoltech for the insightful discussions and details on geological modelling (Appendix B.2).

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline _Solver_ & \multicolumn{3}{c|}{_FID_ of plans to barycenter} \\ \hline _k=1_ & \(k\) = 2 & \(k\) = 3 \\ \hline SCWB [32] & 36.7 & 53.2 & 58.8 \\ \hline WIN [55] & 49.3 & 46.9 & 61.5 \\ \hline
**Ours** & **84.**(3) & **87.**(3) & **102.**(7) \\ \hline \end{tabular}
\end{table}
Table 2: FID scores of images mapped from inputs \(\mathbb{P}_{k}\) to the barycenter.

## References

* [1] Martial Agueh and Guillaume Carlier. Barycenters in the wasserstein space. _SIAM Journal on Mathematical Analysis_, 43(2):904-924, 2011.
* [2] Jason M Altschuler and Enric Boix-Adsera. Wasserstein barycenters are np-hard to compute. _SIAM Journal on Mathematics of Data Science_, 4(1):179-203, 2022.
* [3] Pedro C Alvarez-Esteban, E Del Barrio, JA Cuesta-Albertos, and C Matran. A fixed-point approach to barycenters in wasserstein space. _Journal of Mathematical Analysis and Applications_, 441(2):744-762, 2016.
* [4] Luigi Ambrosio and Nicola Gigli. _A User's Guide to Optimal Transport_, pages 1-155. Springer Berlin Heidelberg, Berlin, Heidelberg, 2013.
* [5] Brandon Amos. On amortizing convex conjugates for optimal transport. In _The Eleventh International Conference on Learning Representations_, 2022.
* [6] Brandon Amos, Lei Xu, and J Zico Kolter. Input convex neural networks. In _International Conference on Machine Learning_, pages 146-155. PMLR, 2017.
* [7] Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I Jordan. An introduction to mcmc for machine learning. _Machine learning_, 50:5-43, 2003.
* [8] Arip Asadulaev, Alexander Korotin, Vage Egiazarian, and Evgeny Burnaev. Neural optimal transport with general cost functionals. In _The Twelfth International Conference on Learning Representations_, 2024.
* [9] Julio Backhoff-Veraguas, Mathias Beiglbock, and Gudmund Pammer. Existence, duality, and cyclical monotonicity for weak transport costs. _Calculus of Variations and Partial Differential Equations_, 58(6):203, 2019.
* [10] Peter L. Bartlett and Shahar Mendelson. Rademacher and Gaussian complexities: risk bounds and structural results. _J. Mach. Learn. Res._, 3:463-482, 2002.
* [11] Jean-David Benamou, Guillaume Carlier, Marco Cuturi, Luca Nenna, and Gabriel Peyre. Iterative bregman projections for regularized transportation problems. _SIAM Journal on Scientific Computing_, 37(2):A1111-A1138, 2015.
* [12] Elsa Cazelles, Felipe Tobar, and Joaquin Fontbona. A novel notion of barycenter for probability distributions based on optimal weak mass transport. _Advances in Neural Information Processing Systems_, 34:13575-13586, 2021.
* [13] Tianrong Chen, Guan-Horng Liu, and Evangelos Theodorou. Likelihood training of schrodinger bridge using forward-backward SDEs theory. In _International Conference on Learning Representations_, 2022.
* [14] Jinjin Chi, Zhiyao Yang, Ximing Li, Jihong Ouyang, and Renchu Guan. Variational wasserstein barycenters with c-cyclical monotonicity regularization. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 7157-7165, 2023.
* [15] Lenaic Chizat. Doubly regularized entropic wasserstein barycenters. _arXiv preprint arXiv:2303.11844_, 2023.
* [16] Christian Clason, Dirk A Lorenz, Hinrich Mahler, and Benedikt Wirth. Entropic regularization of continuous optimal transport problems. _Journal of Mathematical Analysis and Applications_, 494(1):124432, 2021.
* [17] Samuel Cohen, Michael Arbel, and Marc Peter Deisenroth. Estimating barycenters of measures in high dimensions. _arXiv preprint arXiv:2007.07105_, 2020.
* [18] Pierre Colombo, Guillaume Staerman, Chloe Clavel, and Pablo Piantanida. Automatic text evaluation through the lens of Wasserstein barycenters. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 10450-10466, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
* [19] Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 26. Curran Associates, Inc., 2013.
* [20] Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In _International conference on machine learning_, pages 685-693. PMLR, 2014.

* Cuturi and Peyre [2016] Marco Cuturi and Gabriel Peyre. A smoothed dual approach for variational wasserstein problems. _SIAM Journal on Imaging Sciences_, 9(1):320-343, 2016.
* Cuturi and Peyre [2018] Marco Cuturi and Gabriel Peyre. Semidual regularized optimal transport. _SIAM Review_, 60(4):941-965, 2018.
* Daniels et al. [2021] Max Daniels, Tyler Maunu, and Paul Hand. Score-based generative neural networks for large-scale optimal transport. _Advances in neural information processing systems_, 34:12955-12965, 2021.
* De Bortoli et al. [2021] Valentin De Bortoli, James Thornton, Jeremy Heng, and Arnaud Doucet. Diffusion schrodinger bridge with applications to score-based generative modeling. _Advances in Neural Information Processing Systems_, 34:17695-17709, 2021.
* del Barrio and Loubes [2020] Eustasio del Barrio and Jean-Michel Loubes. The statistical effect of entropic regularization in optimal transportation, 2020.
* Du et al. [2023] Yilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, and Will Sussman Grathwohl. Reduce, reuse, recycle: Compositional generation with energy-based diffusion models and mcmc. In _International Conference on Machine Learning_, pages 8489-8510. PMLR, 2023.
* Du et al. [2021] Yilun Du, Shuang Li, B. Joshua Tenenbaum, and Igor Mordatch. Improved contrastive divergence training of energy based models. In _Proceedings of the 38th International Conference on Machine Learning (ICML-21)_, 2021.
* Du and Mordatch [2019] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. _Advances in Neural Information Processing Systems_, 32, 2019.
* Du et al. [1951] J. Dugundji. An extension of Tietze's theorem. _Pacific J. Math._, 1:353-367, 1951.
* Duvrechenskii et al. [2018] Pavel Duvrechenskii, Darina Dvinskikh, Alexander Gasnikov, Cesar Uribe, and Angelia Nedich. Decentralize and randomize: Faster algorithm for wasserstein barycenters. _Advances in Neural Information Processing Systems_, 31, 2018.
* Fan et al. [2023] Jiaojiao Fan, Shu Liu, Shaojun Ma, Hao-Min Zhou, and Yongxin Chen. Neural monge map estimation and its applications. _Transactions on Machine Learning Research_, 2023. Featured Certification.
* Fan et al. [2021] Jiaojiao Fan, Amirhossein Taghvaei, and Yongxin Chen. Scalable computations of wasserstein barycenter via input convex neural networks. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 1571-1581. PMLR, 18-24 Jul 2021.
* Flamary et al. [2021] Remi Flamary, Nicolas Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurelie Boisbunon, Stanislas Chambon, Laetitia Chapel, Adrien Corenflos, Kilian Fatras, Nemo Fournier, Leo Gautheron, Nathalie T.H. Gayraud, Hicham Janati, Alain Rakotomamonjy, Ievgen Redko, Antoine Rolet, Antony Schutz, Vivien Seguy, Danica J. Sutherland, Romain Tavenard, Alexander Tong, and Titouan Vayer. Pot: Python optimal transport. _Journal of Machine Learning Research_, 22(78):1-8, 2021.
* Gao et al. [2021] Ruiqi Gao, Yang Song, Ben Poole, Ying Nian Wu, and Diederik P Kingma. Learning energy-based models by diffusion recovery likelihood. In _International Conference on Learning Representations_, 2021.
* Gazdieva et al. [2023] Milena Gazdieva, Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Extremal domain translation with neural optimal transport. In _Advances in Neural Information Processing Systems_, 2023.
* Genevay [2019] Aude Genevay. _Entropy-regularized optimal transport for machine learning_. PhD thesis, Paris Sciences et Lettres (ComUE), 2019.
* Genevay et al. [2016] Aude Genevay, Marco Cuturi, Gabriel Peyre, and Francis Bach. Stochastic optimization for large-scale optimal transport. _Advances in neural information processing systems_, 29, 2016.
* Gottlieb et al. [2016] Lee-Ad Gottlieb, Aryeh Kontorovich, and Robert Krauthgamer. Adaptive metric dimensionality reduction. volume 620, pages 105-118. 2016.
* Gozlan et al. [2017] Nathael Gozlan, Cyril Roberto, Paul-Marie Samson, and Prasad Tetali. Kantorovich duality for general transport costs and applications. _Journal of Functional Analysis_, 273(11):3327-3405, 2017.

* [40] Hao Guan and Mingxia Liu. Domain adaptation for medical image analysis: a survey. _IEEE Transactions on Biomedical Engineering_, 69(3):1173-1185, 2021.
* [41] Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry Vetrov, and Evgeny Burnaev. Entropic neural optimal transport via diffusion processes. In _Advances in Neural Information Processing Systems_, 2023.
* [42] Nikita Gushchin, Alexander Kolesov, Petr Mokrov, Polina Karpikova, Andrey Spiridonov, Evgeny Burnaev, and Alexander Korotin. Building the bridge of schr\(\backslash\)" odinger: A continuous entropic optimal transport benchmark. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.
* [43] Raza Habib and David Barber. Auxiliary variational mcmc. In _International Conference on Learning Representations_, 2018.
* [44] Pierre Henry-Labordere. (martingale) optimal transport and anomaly detection with neural networks: A primal-dual algorithm. _arXiv preprint arXiv:1904.04546_, 2019.
* [45] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in neural information processing systems_, 30, 2017.
* [46] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. _Neural computation_, 14(8):1771-1800, 2002.
* [47] Matthew Hoffman, Pavel Sountsov, Joshua V Dillon, Ian Langmore, Dustin Tran, and Srinivas Vasudevan. Neutra-lizing bad geometry in hamiltonian monte carlo using neural transport. _arXiv preprint arXiv:1903.03704_, 2019.
* [48] Syunsuke Ikeda, Gary Parker, and Kenji Sawai. Bend theory of river meanders. part 1. linear development. _Journal of Fluid Mechanics_, 112:363-377, 1981.
* [49] Hicham Janati, Marco Cuturi, and Alexandre Gramfort. Debiased sinkhorn barycenters. In _International Conference on Machine Learning_, pages 4692-4701. PMLR, 2020.
* [50] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 4401-4410, 2019.
* [51] Dominik Klein, Theo Uscidda, Fabian Theis, and Marco Cuturi. Generative entropic neural optimal transport to map within and across spaces. _arXiv preprint arXiv:2310.09254_, 2023.
* [52] Alexander Kolesov, Petr Mokrov, Igor Udovichenko, Milena Gazdieva, Gudmund Pammer, Evgeny Burnaev, and Alexander Korotin. Estimating barycenters of distributions with neural optimal transport. In _Forty-first International Conference on Machine Learning_, 2024.
* [53] Ekaterina Kondrateva, Marina Pominova, Elena Popova, Maxim Sharaev, Alexander Bernstein, and Evgeny Burnaev. Domain shift in computer vision models for mri data analysis: an overview. In _Thirteenth International Conference on Machine Vision_, volume 11605, pages 126-133. SPIE, 2021.
* [54] Alexander Korotin, Vage Egiazarian, Arip Asadulaev, Alexander Safin, and Evgeny Burnaev. Wasserstein-2 generative networks. In _International Conference on Learning Representations_, 2021.
* [55] Alexander Korotin, Vage Egiazarian, Lingxiao Li, and Evgeny Burnaev. Wasserstein iterative networks for barycenter estimation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [56] Alexander Korotin, Nikita Gushchin, and Evgeny Burnaev. Light schrodinger bridge. In _The Twelfth International Conference on Learning Representations_.
* [57] Alexander Korotin, Alexander Kolesov, and Evgeny Burnaev. Kantorovich strikes back! wasserstein gans are not optimal transport? _Advances in Neural Information Processing Systems_, 35:13933-13946, 2022.
* [58] Alexander Korotin, Lingxiao Li, Aude Genevay, Justin M Solomon, Alexander Filippov, and Evgeny Burnaev. Do neural optimal transport solvers work? a continuous wasserstein-2 benchmark. _Advances in Neural Information Processing Systems_, 34:14593-14605, 2021.

* [59] Alexander Korotin, Lingxiao Li, Justin Solomon, and Evgeny Burnaev. Continuous wasserstein-2 barycenter estimation without minimax optimization. In _International Conference on Learning Representations_, 2021.
* [60] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Kernel neural optimal transport. In _The Eleventh International Conference on Learning Representations_, 2023.
* [61] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Neural optimal transport. In _The Eleventh International Conference on Learning Representations_, 2023.
* [62] Anastasis Kratsios and Leonie Papon. Universal approximation theorems for differentiable geometric deep learning. _J. Mach. Learn. Res._, 23:Paper No. [196], 73, 2022.
* [63] Roman Krawtschenko, Cesar A Uribe, Alexander Gasnikov, and Pavel Dvurechensky. Distributed optimization with quantization for computing wasserstein barycenters. _arXiv preprint arXiv:2010.14325_, 2020.
* [64] Alexey Kroshnin, Nazarii Tupitsa, Darina Dvinskikh, Pavel Dvurechensky, Alexander Gasnikov, and Cesar Uribe. On the complexity of approximating wasserstein barycenters. In _International conference on machine learning_, pages 3530-3540. PMLR, 2019.
* [65] Rafsanjany Kushol, Alan H Wilman, Sanjay Kalra, and Yee-Hong Yang. Dsmri: Domain shift analyzer for multi-center mri datasets. _Diagnostics_, 13(18):2947, 2023.
* [66] John Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning with sampler-induced distributions. _Advances in Neural Information Processing Systems_, 32, 2019.
* [67] Khang Le, Dung Q Le, Huy Nguyen, Dat Do, Tung Pham, and Nhat Ho. Entropic gromov-wasserstein between gaussian distributions. In _International Conference on Machine Learning_, pages 12164-12203. PMLR, 2022.
* [68] Khang Le, Huy Nguyen, Quang M Nguyen, Tung Pham, Hung Bui, and Nhat Ho. On robust optimal transport: Computational complexity and barycenter computation. _Advances in Neural Information Processing Systems_, 34:21947-21959, 2021.
* [69] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. _Predicting structured data_, 1(0), 2006.
* [70] Christian Leonard. A survey of the schrodinger problem and some of its connections with optimal transport. _Discrete and Continuous Dynamical Systems_, 34(4):1533-1574, 2013.
* [71] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing hamiltonian monte carlo with neural networks. In _International Conference on Learning Representations_, 2018.
* [72] Lingxiao Li, Aude Genevay, Mikhail Yurochkin, and Justin M Solomon. Continuous regularized wasserstein barycenters. _Advances in Neural Information Processing Systems_, 33:17755-17765, 2020.
* [73] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_, December 2015.
* [74] Ashok Makkuva, Amirhossein Taghvaei, Sewoong Oh, and Jason Lee. Optimal transport mapping via input convex neural networks. In _International Conference on Machine Learning_, pages 6672-6681. PMLR, 2020.
* [75] Anton Mallasto, Augusto Gerolin, and Ha Quang Minh. Entropy-regularized 2-wasserstein distance between gaussian measures. _Information Geometry_, 5(1):289-323, 2022.
* [76] Simone Di Marino and Augusto Gerolin. An optimal transport approach for the schrodinger bridge problem and convergence of sinkhorn algorithm. _Journal of Scientific Computing_, 85(2):27, 2020.
* [77] Alberto Maria Metelli, Amarildo Likmeta, and Marcello Restelli. Propagating uncertainty in reinforcement learning via wasserstein barycenters. _Advances in Neural Information Processing Systems_, 32, 2019.
* [78] Petr Mokrov, Alexander Korotin, Alexander Kolesov, Nikita Gushchin, and Evgeny Burnaev. Energy-guided entropic neural optimal transport. In _The Twelfth International Conference on Learning Representations_, 2024.

* [79] Eduardo Fernandes Montesuma and Fred Maurice Ngole Mboula. Wasserstein barycenter for multi-source domain adaptation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 16785-16793, 2021.
* [80] Youssef Mroueh. Wasserstein style transfer. In Silvia Chiappa and Roberto Calandra, editors, _Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics_, volume 108 of _Proceedings of Machine Learning Research_, pages 842-852. PMLR, 26-28 Aug 2020.
* [81] Kirill Neklyudov, Max Welling, Evgenii Egorov, and Dmitry Vetrov. Involutive mcmc: a unifying framework. In _International Conference on Machine Learning_, pages 7273-7282. PMLR, 2020.
* [82] Maxence Noble, Valentin De Bortoli, Arnaud Doucet, and Alain Durmus. Tree-based diffusion schrodinger bridge with applications to wasserstein barycenters. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [83] Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data science. _Foundations and Trends(r) in Machine Learning_, 11(5-6):355-607, 2019.
* [84] Aram-Alexandre Pooladian, Carles Domingo-Enrich, Ricky T. Q. Chen, and Brandon Amos. Neural optimal transport with lagrangian costs. In _ICML Workshop on New Frontiers in Learning, Control, and Dynamical Systems_, 2023.
* [85] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 1(2):3, 2022.
* [86] Gareth O Roberts and Richard L Tweedie. Exponential convergence of langevin distributions and their discrete approximations. _Bernoulli_, pages 341-363, 1996.
* [87] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 10684-10695, 2022.
* [88] Litu Rout, Alexander Korotin, and Evgeny Burnaev. Generative modeling with optimal transport maps. In _International Conference on Learning Representations_, 2021.
* [89] Filippo Santambrogio. Optimal transport for applied mathematicians. _Birkauser, NY_, 55(58-63):94, 2015.
* [90] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking the power of gans for fast large-scale text-to-image synthesis. In _International conference on machine learning_, pages 30105-30118. PMLR, 2023.
* [91] Vivien Seguy, Bharath Bhushan Damodaran, Remi Flamary, Nicolas Courty, Antoine Rolet, and Mathieu Blondel. Large scale optimal transport and mapping estimation. In _International Conference on Learning Representations_, 2018.
* [92] Dino Sejdinovic, Heiko Strathmann, Maria Lomeli Garcia, Christophe Andrieu, and Arthur Gretton. Kernel adaptive metropolis-hastings. In _International conference on machine learning_, pages 1665-1673. PMLR, 2014.
* [93] Shai Shalev-Shwartz and Shai Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* [94] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and Arnaud Doucet. Diffusion schrodinger bridge matching. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [95] Dror Simon and Aviad Aberdam. Barycenters of natural images constrained wasserstein barycenters for image morphing. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7910-7919, 2020.
* [96] Maurice Sion. On general minimax theorems. _Pacific Journal of Mathematics_, 8(1):171-176, 1958.
* [97] Justin Solomon. Optimal transport on discrete domains. _AMS Short Course on Discrete Differential Geometry_, 2018.

* [98] Justin Solomon, Fernando De Goes, Gabriel Peyre, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation on geometric domains. _ACM Transactions on Graphics (ToG)_, 34(4):1-11, 2015.
* [99] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-mc: Adversarial training for mcmc. _Advances in Neural Information Processing Systems_, 30, 2017.
* [100] Yang Song and Diederik P Kingma. How to train your energy-based models. _arXiv preprint arXiv:2101.03288_, 2021.
* [101] Sanvesh Srivastava, Volkan Cevher, Quoc Dinh, and David Dunson. Wasp: Scalable bayes via barycenters of subset posteriors. In _Artificial Intelligence and Statistics_, pages 912-920. PMLR, 2015.
* [102] Sanvesh Srivastava, Cheng Li, and David B Dunson. Scalable bayes via barycenter in wasserstein space. _The Journal of Machine Learning Research_, 19(1):312-346, 2018.
* [103] Karin Stacke, Gabriel Eilertsen, Jonas Unger, and Claes Lundstrom. Measuring domain shift for deep learning in histopathology. _IEEE journal of biomedical and health informatics_, 25(2):325-336, 2020.
* [104] Heiko Strathmann, Dino Sejdinovic, Samuel Livingstone, Zoltan Szabo, and Arthur Gretton. Gradient-free hamiltonian monte carlo with efficient kernel exponential families. _Advances in Neural Information Processing Systems_, 28, 2015.
* [105] Surya T Tokdar and Robert E Kass. Importance sampling: a review. _Wiley Interdisciplinary Reviews: Computational Statistics_, 2(1):54-60, 2010.
* [106] Alexander Tong, Kilian FATRAS, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. _Transactions on Machine Learning Research_, 2024. Expert Certification.
* [107] Alexander Y Tong, Nikolay Malkin, Kilian Fatras, Lazar Atanackovic, Yanlei Zhang, Guillaume Huguet, Guy Wolf, and Yoshua Bengio. Simulation-free schrodinger bridges via score and flow matching. In _International Conference on Artificial Intelligence and Statistics_, pages 1279-1287. PMLR, 2024.
* [108] Konstantin S Turitsyn, Michael Chertkov, and Marija Vucelja. Irreversible monte carlo algorithms for efficient sampling. _Physica D: Nonlinear Phenomena_, 240(4-5):410-414, 2011.
* [109] Francisco Vargas, Pierre Thodoroff, Austen Lamacraft, and Neil Lawrence. Solving schrodinger bridges via maximum likelihood. _Entropy_, 23(9):1134, 2021.
* [110] Cedric Villani et al. _Optimal transport: old and new_, volume 338. Springer, 2009.
* [111] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In _International Conference on Machine Learning_, pages 2635-2644. PMLR, 2016.
* [112] Wenjun Yan, Yuanyuan Wang, Shengjia Gu, Lu Huang, Fuhua Yan, Liming Xia, and Qian Tao. The domain shift problem of medical image segmentation and vendor-adaptation by unet-gan. In _Medical Image Computing and Computer Assisted Intervention-MICCAI 2019: 22nd International Conference, Shenzhen, China, October 13-17, 2019, Proceedings, Part II 22_, pages 623-631. Springer, 2019.
* [113] Yang Zhao, Jianwen Xie, and Ping Li. Learning energy-based generative models via coarse-to-fine expanding and sampling. In _International Conference on Learning Representations_, 2021.

Proofs

### Auxiliary Statements

We start by showing some basic properties of the \(C\)-transform which will be used in the main proofs.

**Proposition A.1** (Properties of the \(C\)-transform).: _Let \(f_{1},f_{2}\colon\mathcal{Y}\to\mathbb{R}\) be two measurable functions which are bounded from below. It holds that_

1. _[label=()]_
2. **Monotonicity**_:_ \(f_{1}\leq f_{2}\) _implies_ \(f_{1}^{C}\geq f_{2}^{C}\)_;_
3. **Constant additivity**_:_ \((f_{1}+a)^{C}=f_{1}^{C}-a\) _for all_ \(a\in\mathbb{R}\)_;_
4. **Concavity**_:_ \((\lambda f_{1}+(1-\lambda)f_{2})^{C}\geq\lambda f_{1}^{C}+(1-\lambda)f_{2}^{C}\) _for all_ \(\lambda\in[0,1]\)_;_
5. **Continuity**_:_ \(f_{1},f_{2}\) _bounded implies_ \(\sup_{x\in X}|f_{1}^{C}(x)-f_{2}^{C}(x)|\leq\sup_{y\in\mathcal{Y}}|f_{1}(y)-f_ {2}(y)|\)_._

Proof of Proposition a.1.: We recall the definition of the \(C\)-transform

\[f_{1}^{C}(x)=\inf_{\mu\in\mathcal{P}(\mathcal{Y})}\left\{C(x,\mu)-\int_{ \mathcal{Y}}f_{1}(y)\mathrm{d}\mu(y)\right\},\]

where \(C(x,\mu)\stackrel{{\text{def}}}{{=}}\int_{\mathcal{Y}}c(x,y) \mathrm{d}\mu(y)-\epsilon H(\mu)\). Monotonicity (i) and constant additivity (ii) are immediate from the definition.

To see (iii), observe that the dependence of \(\int_{\mathcal{Y}}f_{1}(y)\mathrm{d}\mu(y)\) on \(f_{1}\) is linear. Thus, \(f_{1}^{C}\) is the pointwise infimum of a family of linear functionals and thus concave.

Finally, to show (iv) we have by monotonicity of the integral that

\[\left|\int_{\mathcal{Y}}f_{1}(y)\mathrm{d}\mu(y)-\int_{\mathcal{Y}}f_{2}(y) \mathrm{d}\mu(y)\right|\leq\sup_{y\in\mathcal{Y}}|f_{1}(y)-f_{2}(y)| \tag{13}\]

for any \(\mu\in\mathcal{P}(\mathcal{Y})\). For fixed \(x\in\mathcal{X}\) we have

\[f_{1}^{C}(x)-f_{2}^{C}(x) =\inf_{\mu\in\mathcal{P}(\mathcal{Y})}\left[C(x,\tilde{\mu})-\int _{\mathcal{Y}}f_{1}(y)\mathrm{d}\tilde{\mu}(y)\right]-\inf_{\mu\in\mathcal{P}( \mathcal{Y})}\left[C(x,\mu)-\int_{\mathcal{Y}}f_{2}(y)\mathrm{d}\mu(y)\right]\] \[=\sup_{\mu\in\mathcal{P}(\mathcal{Y})}\inf_{\tilde{\mu}\in \mathcal{P}(\mathcal{Y})}\left[C(x,\tilde{\mu})-C(x,\mu)-\int_{\mathcal{Y}}f_ {1}(y)\mathrm{d}\tilde{\mu}(y)+\int_{\mathcal{Y}}f_{2}(y)\mathrm{d}\mu(y) \right].\]

By setting \(\tilde{\mu}=\mu\) we increase the value and obtain

\[f_{1}^{C}(x)-f_{2}^{C}(x)\leq\sup_{\mu\in\mathcal{P}(\mathcal{Y})}\int_{ \mathcal{Y}}\left[f_{2}(y)-f_{1}(y)\right]\mathrm{d}\mu(y)\leq\sup_{y\in \mathcal{Y}}|f_{1}(y)-f_{2}(y)|, \tag{14}\]

where the last inequality follows from (13). For symmetry reasons, we can swap the roles of \(f_{1}\) and \(f_{2}\) in (14), which yields the claim. 

### Proof of Theorem 4.1

Proof.: By substituting in (5) the primal EOT problems (1) with their dual counterparts (2), we obtain a dual formulation, which is the starting point of our analysis:

\[\mathcal{L}^{*}=\min_{\mathbb{Q}\in\mathcal{P}(\mathcal{Y})}\sup_{f_{1},\ldots, f_{K}\in\mathcal{C}(\mathcal{Y})}\underbrace{\sum_{k=1}^{K}\lambda_{k}\bigg{\{} \int_{\mathcal{X}_{k}}f_{k}^{C_{k}}(x_{k})\mathrm{d}\mathbb{P}_{k}(x_{k})+\int _{\mathcal{Y}}f_{k}(y)\mathrm{d}\mathbb{Q}(y)\bigg{\}}}_{\stackrel{{ \text{def}}}{{=}}\widetilde{\mathcal{L}}\left(\mathbb{Q}_{\cdot}\{f_{k}\}_{k=1 }^{K}\right)}. \tag{15}\]

Here, we replaced \(\inf\) with \(\min\) because of the existence of the barycenter (SS2.2). Moreover, we refer to the entire expression under the \(\min\sup\) as a functional \(\widetilde{\mathcal{L}}\colon\mathcal{P}(\mathcal{Y})\times\mathcal{C}( \mathcal{Y})^{K}\to\mathbb{R}\). For brevity, we introduce, for \((f_{1},\ldots,f_{K})\in\mathcal{C}(\mathcal{Y})^{K}\), the notation

\[\bar{f}\stackrel{{\text{def}}}{{=}}\sum_{k=1}^{K}\lambda_{k}f_{k} \quad\text{and}\quad M\stackrel{{\text{def}}}{{=}}\inf_{y\in \mathcal{Y}}\bar{f}(y)=\inf_{\mathbb{Q}\in\mathcal{P}(\mathcal{Y})}\int\bar{f} (y)\mathrm{d}\mathbb{Q}(y), \tag{16}\]where the equality follows from two elementary observations that **(a)**\(M\leq\int\bar{f}(y)\mathrm{d}\mathbb{Q}(y)\) for any \(\mathbb{Q}\in\mathcal{P}(\mathcal{Y})\) and **(b)**\(\bar{f}(y)=\int\bar{f}(y^{\prime})\mathrm{d}\delta_{y}(y^{\prime})\) where \(\delta_{y}\) denotes a Dirac mass at \(y\in\mathcal{Y}\).

On the one hand, \(\mathcal{P}(\mathcal{Y})\) is compact w.r.t. the weak topology because \(\mathcal{Y}\) is compact, and for fixed potentials \((f_{1},\dots,f_{K})\in\mathcal{P}(\mathcal{Y})^{K}\) we have that \(\widetilde{\mathcal{L}}(\cdot,(f_{k})_{k=1}^{K})\) is continuous and linear. In particular, \(\widetilde{\mathcal{L}}(\cdot,(f_{k})_{k=1}^{K})\) is convex and l.s.c. On the other hand, for a fixed \(\mathbb{Q}\), the functional \(\widetilde{\mathcal{L}}(\mathbb{Q},\cdot)\) is concave by (iii) in Proposition A.1. These observations allow us to apply Sion's minimax theorem [96, Theorem 3.4] to swap \(\min\) and \(\inf\) in (15) and obtain using (16)

\[\mathcal{L}^{*} =\sup_{f_{1},\dots,f_{K}\in\mathcal{C}(\mathcal{Y})}\min_{ \mathbb{Q}\in\mathcal{P}(\mathcal{Y})}\sum_{k=1}^{K}\lambda_{k}\bigg{\{}\int_{ \mathcal{X}_{k}}f_{k}^{C_{k}}(x_{k})\mathrm{d}\mathbb{P}_{k}(x_{k})+\int_{ \mathcal{X}}f_{k}(y)\mathrm{d}\mathbb{Q}(y)\bigg{\}}\] \[=\sup_{f_{1},\dots,f_{K}\in\mathcal{C}(\mathcal{Y})}\bigg{\{}\sum _{k=1}^{K}\lambda_{k}\int_{\mathcal{X}_{k}}f_{k}^{C_{k}}(x_{k})\mathrm{d} \mathbb{P}_{k}(x_{k})+\min_{\mathbb{Q}\in\mathcal{P}(\mathcal{Y})}\int_{ \mathcal{X}}\bar{f}(y)\mathrm{d}\mathbb{Q}(y)\bigg{\}}\] \[=\sup_{f_{1},\dots,f_{K}\in\mathcal{C}(\mathcal{Y})}\underbrace{ \bigg{\{}\sum_{k=1}^{K}\lambda_{k}\int_{\mathcal{X}_{k}}f_{k}^{C_{k}}(x_{k}) \mathrm{d}\mathbb{P}_{k}(x_{k})+\min_{y\in\mathcal{Y}}\bar{f}(y)\bigg{\}}}_{ \stackrel{{\text{\tiny{def}}}}{{\widetilde{\mathcal{L}}}(f_{1}, \dots,f_{K})}}. \tag{17}\]

Next, we show that the \(\sup\) in (17) can be restricted to tuplets satisfying the congruence condition \(\sum_{k=1}^{K}\lambda_{k}f_{k}=0\). It remains to show that for every tuple \((f_{1},\dots,f_{K})\in\mathcal{C}(\mathcal{Y})^{K}\) there exists a _congruent_ tuple \((\tilde{f}_{1},\dots,\tilde{f}_{K})\in\mathcal{C}(\mathcal{Y})^{K}\) such that \(\widetilde{\mathcal{L}}(\tilde{f}_{1},\dots,\tilde{f}_{K})\geq\widetilde{ \mathcal{L}}(f_{1},\dots,f_{K})\).

To this end, fix \((f_{1},\dots,f_{K})\) and define the congruent tuple

\[(\tilde{f}_{1},\dots,\tilde{f}_{K})\stackrel{{\text{\tiny{def}}} }{{=}}\left(f_{1},\dots,f_{K-1},f_{K}-\frac{\bar{f}}{\lambda_{K}}\right). \tag{18}\]

We find \(\tilde{M}\stackrel{{\text{\tiny{def}}}}{{=}}\inf_{y\in\mathcal{Y} }\sum_{k=1}^{K}\lambda_{k}\tilde{f}_{k}=0\) by the congruence and derive

\[\widetilde{\mathcal{L}}(\tilde{f}_{1},\dots,\tilde{f}_{K})- \widetilde{\mathcal{L}}(f_{1},\dots,f_{K}) =\lambda_{K}\int_{\mathcal{X}_{K}}\left[\tilde{f}_{K}^{C_{K}}( x_{K})-f_{K}^{C_{K}}(x_{K})\right]\mathrm{d}\mathbb{P}_{K}(x_{K})-M\] \[\geq\lambda_{K}\int_{\mathcal{X}_{K}}\left[\left(f_{K}-\frac{M}{ \lambda_{K}}\right)^{C_{K}}(x_{K})-f_{K}^{C_{K}}(x_{K})\right]\mathrm{d} \mathbb{P}(x_{K})-M\] \[=\lambda_{K}\int_{\mathcal{X}_{K}}\frac{M}{\lambda_{K}}\mathrm{d} \mathbb{P}(x_{K})-M=0,\]

where the first inequality follows from \(\tilde{f}_{K}=f_{K}-\frac{\bar{f}}{\lambda_{K}}\leq f_{K}-\frac{M}{\lambda_{K}}\) combined with monotonicity of the \(C\)-transform, see (i) in Proposition A.1. The second to last equality follow from constant additivity, see (ii) in Proposition A.1.

In summary, we obtain

\[\mathcal{L}^{*}=\sup_{\begin{subarray}{c}f_{1},\dots,f_{k}\in\mathcal{C}( \mathcal{Y})\\ \sum_{k=1}^{K}f_{k}=0\end{subarray}}\widetilde{\mathcal{L}}(f_{1},\dots,f_{K}). \tag{19}\]

Finally, observe that for congruent \((f_{1},\dots,f_{K})\) we have \(\widetilde{\mathcal{L}}(f_{1},\dots,f_{K})=\mathcal{L}(f_{1},\dots,f_{K})\). Hence, we can replace \(\widetilde{\mathcal{L}}\) by \(\mathcal{L}\) in (19), which yields (6). 

### Proof of Theorem 4.2

Proof.: Write \(\mathbb{Q}^{*}\) for the barycenter and \(\pi_{k}^{*}\) for the optimizer of \(\text{EOT}_{c_{k},\epsilon}(\mathbb{P}_{k},\mathbb{Q}^{*})\). Consider congruent potentials \(f_{1},\dots,f_{K}\in\mathcal{C}(\mathcal{Y})\) and define the probability distribution

\[\mathrm{d}\pi^{f_{k}}(x_{k},y)\stackrel{{\text{\tiny{def}}}}{{=}} \mathrm{d}\mu^{f_{k}}_{x_{k}}(y)\,\mathrm{d}\mathbb{P}_{k}(x_{k}),\]

where

\[\frac{\mathrm{d}\mu^{f_{k}}_{x_{k}}(y)}{\mathrm{d}y}\stackrel{{\text {\tiny{def}}}}{{=}}\frac{1}{Z_{c_{k}}(f_{k},x_{k})}\exp\left(\frac{f_{k}(y)-c_ {k}(x_{k},y)}{\epsilon}\right), \tag{20}\]\[Z_{c_{k}}(f_{k},x_{k})\stackrel{{\text{def}}}{{=}}\log\left(\int_{ \mathcal{Y}}e^{\frac{f_{k}(y)-e_{k}(x_{k},y)}{\epsilon}}\mathrm{d}y\right). \tag{21}\]

Then we have by [78, Thm. 2]:

\[\text{EOT}_{c_{k},\epsilon}(\mathbb{P}_{k},\mathbb{Q}^{*})-\left(\int_{ \mathcal{X}_{k}}f^{C_{k}}(x_{k})\mathrm{d}\mathbb{P}(x_{k})+\int_{\mathcal{Y}} f(y)\mathrm{d}\mathbb{Q}^{*}(y)\right)=\epsilon\text{KL}\left(\pi_{k}^{*}\|\pi^{f_{k}} \right). \tag{22}\]

Multiplying (22) by \(\lambda_{k}\) and summing over \(k\) yields

\[\epsilon\sum_{k=1}^{K}\lambda_{k}\text{KL}\left(\pi_{k}^{*}\|\pi^{ f_{k}}\right) =\sum_{k=1}\lambda_{k}\left\{\text{EOT}_{c_{k},\epsilon}(\mathbb{ P}_{k},\mathbb{Q}^{*})-\int_{\mathcal{X}_{k}}f_{k}^{C_{k}}(x_{k})\mathrm{d} \mathbb{P}_{k}(x_{k})\right\}-\int_{\mathcal{Y}}\underbrace{\sum_{k=1}^{K} \lambda_{k}f(y)}_{=0}\mathrm{d}\mathbb{Q}^{*}(y)\] \[=\mathcal{L}^{*}-\mathcal{L}(f_{1},\ldots,f_{k}), \tag{23}\]

where the last equality follows by congruence, i.e., \(\sum_{k=1}^{K}\lambda_{k}f_{k}\equiv 0\).

The remaining inequality in (7) is a consequence of the data processing inequality for \(f\)-divergences which we invoke here to get

\[\text{KL}\left(\pi_{k}^{*}\|\pi^{f_{k}}\right)\geq\text{KL}\left(\mathbb{Q}^{* }\|\mathbb{Q}^{f_{k}}\right),\]

where \(\mathbb{Q}^{*}\) and \(\mathbb{Q}^{f_{k}}\) are the second marginals of \(\pi_{k}^{*}\) and \(\pi^{f_{k}}\), respectively. 

### Proof of Theorem 4.3

Proof.: The desired equation (9) could be derived exactly the same way as in [78, Theorem 3]. 

### Proof of Proposition 4.4 and Theorem 4.5

The derivations of the quantitative bound for Proposition 4.4 and Theorem 4.5 relies on the following standard definitions from learning theory, which we now recall for convenience (see, e.g. [93, SS26]). Consider some class \(\mathcal{S}\) of functions \(s:\mathcal{X}\to\mathbb{R}\) and a distribution \(\mu\) on \(\mathcal{X}\). Let \(X=\{x^{1},\ldots,x^{N}\}\) be a sample of \(N\) points in \(\mathcal{X}\).

The **representativeness** of the sample \(X\) w.r.t. the class \(\mathcal{S}\) and the distribution \(\mu\) is defined by

\[\text{Rep}_{X}(\mathcal{S},\mu)\stackrel{{\text{def}}}{{=}}\sup_ {s\in\mathcal{S}}\big{[}\int_{\mathcal{X}}s(x)\mathrm{d}\mu(x)-\frac{1}{N} \sum_{n=1}^{N}s(x^{n})\big{]}. \tag{24}\]

The **Rademacher complexity** of the class \(\mathcal{S}\) w.r.t. the distribution \(\mu\) and sample size \(N\) is given by

\[\mathcal{R}_{N}(\mathcal{S},\mu)\stackrel{{\text{def}}}{{=}} \frac{1}{N}\mathbb{E}\bigg{\{}\sup_{s\in\mathcal{S}}\sum_{n=1}^{N}s(x^{n}) \sigma_{n}\bigg{\}}, \tag{25}\]

where \(\{x^{n}\}_{n=1}^{N}\sim\mu\) are mutually independent, \(\{\sigma^{n}\}_{n=1}^{N}\) are mutually independent Rademacher random variables, i.e., \(\text{Prob}\big{(}\sigma^{n}=1\big{)}=\text{Prob}\big{(}\sigma^{n}=-1\big{)}=0.5\), and the expectation is taken with respect to both \(\{x_{n}\}_{n=1}^{N}\), \(\{\sigma_{n}\}_{n=1}^{N}\). The well-celebrated relation between (25) and (24), as shown in [93, Lemma 26.2], is

\[\mathbb{E}\text{Rep}_{X}(\mathcal{S},\mu)\leq 2\cdot\mathcal{R}_{N}(\mathcal{S},\mu), \tag{26}\]

where the expectation is taken w.r.t. random i.i.d. sample \(X\sim\mu\) of size \(N\).

**Proposition 4.4**.: Observe that by (23) we may write

\[\epsilon\sum_{k=1}^{K}\lambda_{k}\text{KL}\left(\pi_{k}^{*}\|\pi^{\widehat{f}_ {k}}\right)=\mathcal{L}^{*}-\mathcal{L}(\widehat{\mathbf{f}})=\underbrace{ \left[\mathcal{L}^{*}-\max_{\mathbf{f}\in\overline{\mathcal{F}}}\mathcal{L}( \mathbf{f})\right]}_{\text{Approximation error}}+\underbrace{\left[\max_{\mathbf{ f}\in\overline{\mathcal{F}}}\mathcal{L}(\mathbf{f})-\mathcal{L}(\widehat{\mathbf{f}}) \right]}_{\text{Estimation error}}. \tag{27}\]

Let \(\widehat{\mathbf{f}}\) be a maximizer of \(\mathcal{L}(\mathbf{f})\) within \(\overline{\mathcal{F}}\). Analysing the estimation error in (27) yields

\[\max_{\mathbf{f}\in\overline{\mathcal{F}}}\mathcal{L}(\mathbf{f})-\mathcal{L}( \widehat{\mathbf{f}})=\mathcal{L}(\widehat{\mathbf{f}})-\mathcal{L}(\widehat{ \mathbf{f}})\]\[=\big{[}\mathcal{L}(\mathbf{\tilde{f}})-\widehat{\mathcal{L}}( \mathbf{\tilde{f}})\big{]}+\underbrace{\big{[}\widehat{\mathcal{L}}(\mathbf{ \tilde{f}})-\widehat{\mathcal{L}}(\mathbf{\tilde{f}})\big{]}}_{\leq 0}+ \big{[}\widehat{\mathcal{L}}(\mathbf{\tilde{f}})-\mathcal{L}(\mathbf{ \tilde{f}})\big{]} \tag{28}\] \[\leq\,2\sup_{\mathbf{f}\in\overline{\mathcal{F}}}\big{|}\mathcal{L }(\mathbf{f})-\widehat{\mathcal{L}}(\mathbf{f})\big{|}, \tag{29}\]

where central term in line (28) is bounded above by \(0\) due the maximality of \(\mathbf{\tilde{f}}\), that is, \(\widehat{\mathcal{L}}(\mathbf{\tilde{f}})=\max_{\mathbf{f}\in\overline{ \mathcal{F}}}\widehat{\mathcal{L}}(\mathbf{f})\geq\widehat{\mathcal{L}}( \mathbf{\tilde{f}})\). Due to (29), we can bound the estimation error using the Rademacher complexity

\[\sup_{\mathbf{f}\in\overline{\mathcal{F}}}\big{|}\mathcal{L}( \mathbf{f})-\widehat{\mathcal{L}}(\mathbf{f})\big{|}\leq\sum_{k=1}^{K}\lambda_ {k}\sup_{f_{k}\in\mathcal{F}_{k}}\left[\int_{\mathcal{X}_{k}}f_{k}^{C_{k}}(x_{ k})\mathrm{d}\mathbb{P}_{k}(x_{k})-\frac{1}{N_{k}}\sum_{n=1}^{N_{k}}f_{k}^{C_{k}}(x_{ k}^{n})\right]=\sum_{k=1}^{K}\lambda_{k}\text{Rep}_{X_{k}}(\mathcal{F}_{k}^{C_{k}}, \mathbb{P}_{k}).\]

Proof of Theorem 4.5.: **Case (a) - Lipschitz costs**: Assume that, for \(k\in\overline{K}\), \(x\mapsto c_{k}(x,y)\) is Lipschitz with constant \(L_{k}\geq 0\) for every \(y\in\mathcal{Y}\). Recall that \(f_{k}^{C_{k}}\) is defined as the pointwise supremum of \(L_{k}\)-Lipschitz functions and, therefore, Lipschitz continuous with the same constant. Since the value of the representativeness of a sample w.r.t. a function class is invariant under translating individual elements of said class, we have that \(\text{Rep}_{X}(\mathcal{F}_{k}^{C_{k}},\mathbb{P}_{k})\) coincides with \(\text{Rep}_{X}(\mathcal{G}_{k},\mathbb{P}_{k})\) where

\[\mathcal{G}_{k}\stackrel{{\text{def}}}{{=}}\{f^{C_{k}}-f^{C_{k}}( \tilde{x}_{k}):f\in C(\mathcal{Y})\},\]

for some fixed \(\tilde{x}_{k}\in\mathcal{X}_{k}\). All the functions in this class are \(L_{k}\)-Lipschitz and, therefore, bounded by \(L_{k}\cdot\text{diam}(\mathcal{X}_{k})\). We may therefore apply [38, Theorem 4.3] to the class \(\mathcal{G}_{k}\) and obtain

\[\mathbb{E}_{X\sim\mathbb{P}_{k}}\text{Rep}_{X}(\mathcal{F}_{k}^{C_{k}}, \mathbb{P}_{k})=\mathbb{E}_{X\sim\mathbb{P}_{k}}\text{Rep}_{X}(\mathcal{G}_{k },\mathbb{P}_{k})\leq 2\mathcal{R}_{N}(\mathcal{G}_{k},\mathbb{P}_{k})\leq O (N_{k}^{-\frac{1}{D_{k}+1}}).\]

**Case (b) - Feature-based quadratic costs \(\frac{1}{2}\|u_{k}(\cdot)-v(\cdot)\|_{2}^{2}\)**: Alternatively, consider the case where \(c_{k}(x_{k},y)=\frac{1}{2}\|u_{k}(x_{k})-v(y)\|_{2}^{2}\) and \(\mathcal{F}_{k}\subseteq\mathcal{C}(\mathcal{Y})\) is bounded (w.r.t. the supremum norm).

To this end, recall that for a measurable and bounded function \(f:\mathcal{Y}\to\mathbb{R}\), the weak entropic \(c_{k}\)-transform satisfies

\[f^{C_{k}}(x_{k})=-\epsilon\log(Z_{c_{k}}(f,x_{k})),\]

where \(Z_{c_{k}}(f,x_{k})=\int_{\mathcal{Y}}\exp\big{(}\frac{f(y)-c_{k}(x_{k},y)}{ \epsilon}\big{)}\,dy\). Recall that \(\mathbb{R}^{D^{\prime\prime}}\times\mathbb{R}^{D^{\prime\prime}}\ni(a,b)\mapsto \exp\Bigl{(}-\frac{\|a-b\|^{2}}{2\epsilon}\Bigr{)}\) is a positive definite kernel which is widely known as the **Gaussian kernel**. This means that there exists a Hilbert space \(\mathcal{H}\) and a feature map \(\phi:\mathbb{R}^{D^{\prime\prime}}\to\mathcal{H}\) such that \(\exp\Bigl{(}-\frac{\|a-b\|^{2}}{2\epsilon}\Bigr{)}=\langle\phi(a),\phi(b) \rangle_{\mathcal{H}}\). Due to the particular form of \(c_{k}\), we may write

\[\exp\Big{(}-\frac{c_{k}(x_{k},y)}{\epsilon}\Big{)}=\exp\Big{(}-\frac{\|u_{k}( x)-v(y)\|^{2}}{2\epsilon}\Big{)}=\langle\phi(u_{k}(x_{k})),\phi(v(y))\rangle_{ \mathcal{H}}. \tag{30}\]

Notice that \(\{\phi(u_{k}(x_{k})):x_{k}\in\mathcal{X}_{k}\}\subseteq\{v\in\mathcal{H}:\, \|v\|_{\mathcal{H}}=1\}\) since \(\|\phi(a)\|_{\mathcal{H}}^{2}=\langle\phi(a),\phi(a)\rangle_{\mathcal{H}}=1\) for every \(a\in\mathbb{R}^{D^{\prime\prime}}\).

Using the identity in (30), we can express \(Z_{c_{k}}(f,x_{k})\) by

\[Z_{c_{k}}(f,x_{k})=\int_{\mathcal{Y}}\langle\phi(u_{k}(x_{k})),\phi(v(y)) \rangle_{\mathcal{H}}\,e^{\frac{f(y)}{\epsilon}}\,dy=\Bigl{\langle}\phi\big{(} u_{k}(x_{k})\big{)},\int_{\mathcal{Y}}\phi(v(y))e^{\frac{f(y)}{\epsilon}}\,dy\Bigr{\rangle}_{ \mathcal{H}},\]

where the last equality is justified as \(\mathcal{Y}\) is compact; furthermore, we note the integrals are well-defined by the measurability of \(\phi\), \(u_{k}\) and \(v\). Moreover, using the boundedness of each \(\mathcal{F}_{k}\) and compactness of \(\mathcal{Y}\), we get

\[R\stackrel{{\text{def}}}{{=}}\max_{k=1,\ldots,K}\sup_{f\in \mathcal{F}_{k}}\Big{\|}\int_{\mathcal{Y}}\phi(v(y))e^{\frac{f(y)}{\epsilon}} \,dy\Big{\|}_{\mathcal{H}}<\infty. \tag{31}\]

Define \(\mathcal{G}_{k}\stackrel{{\text{def}}}{{=}}\{Z_{c_{k}}(f,\cdot):f \in\mathcal{F}_{k}\}\) and observe that \(\mathcal{G}_{k}\subseteq\mathcal{G}_{k}^{\prime}\stackrel{{\text{def }}}{{=}}\{\big{\langle}\phi\big{(}u_{k}(\cdot)\big{)},w\rangle_{\mathcal{H}}: \|w\|_{\mathcal{H}}\leq R\}\). This implies that \(\mathcal{R}_{N_{k}}(\mathcal{G}_{k},\mathbb{P}_{k})\leq\mathcal{R}_{N_{k}}( \mathcal{G}_{k}^{\prime},\mathbb{P}_{k})\) by the properties of the Rademacher complexity.

[MISSING_PAGE_FAIL:21]

Now we use (36) to derive

\[|\mathcal{L}(f_{1},\ldots,f_{K})-\mathcal{L}(f^{\prime}_{1},\ldots,f^ {\prime}_{K})| \leq\sum_{k=1}^{K}\lambda_{k}\left|\int_{\mathcal{X}_{k}}f_{k}^{C_{ k}}(x_{k})\mathrm{d}\mathbb{P}_{k}(x_{k})-\int_{\mathcal{X}_{k}}(f^{\prime}_{k})^{C_ {k}}(x_{k})\mathrm{d}\mathbb{P}_{k}(x_{k})\right|\] \[\leq\sum_{k=1}^{K}\lambda_{k}\int_{\mathcal{X}_{k}}|f_{k}^{C_{k}}( x_{k})-(f^{\prime}_{k})^{C_{k}}(x_{k})|\mathrm{d}\mathbb{P}_{k}(x_{k})\] \[\leq\sum_{k=1}^{K}\lambda_{k}\|f_{k}^{C_{k}}-(f^{\prime}_{k})^{C_ {k}}\|_{\infty}<\underbrace{(\sum_{k=1}^{K}\lambda_{k})}_{=1}\frac{\delta}{2}= \frac{\delta}{2}. \tag{37}\]

Next we combine (33) with (37) to get

\[\mathcal{L}^{*}-\mathcal{L}(f_{1},\ldots,f_{K})\leq\underbrace{[\mathcal{L}^{ *}-\mathcal{L}(f^{\prime}_{1},\ldots,f^{\prime}_{K})]}_{<\delta/2}+\underbrace {|\mathcal{L}(f_{1},\ldots,f_{K})-\mathcal{L}(f^{\prime}_{1},\ldots,f^{\prime} _{K})]}_{<\delta/2}<\delta. \tag{38}\]

By using (38) together with Theorem 4.2 we obtain

\[\epsilon\sum_{k=1}^{K}\lambda_{k}\text{KL}\left(\pi_{k}^{*}\|\pi^{f_{k}} \right)=\mathcal{L}^{*}-\mathcal{L}(f_{1},\ldots,f_{K})<\delta\]

which completes the proof. 

### Existence and uniqueness of the barycenter distribution which solves (5)

We introduce an auxiliary functional which is the argument of minimization problem (5):

\[\mathcal{B}(\mathbb{Q})\mathop{=}\limits^{\text{def}}\sum_{k=1}^{K}\lambda_{k }\text{EOT}_{c_{k},\epsilon}(\mathbb{P}_{k},\mathbb{Q}), \tag{39}\]

i.e. the optimal value of (5) could be defined as \(\mathcal{L}^{*}=\inf\limits_{\mathbb{Q}\in\mathcal{P}(\mathcal{Y})}\mathcal{B }(\mathbb{Q})\).

Note that the functional \(\mathbb{Q}\mapsto\mathcal{B}(\mathbb{Q})\) is strictly convex and lower semicontinuous (w.r.t. the weak topology) as each component \(\mathbb{Q}\mapsto\text{EOT}_{c_{k},\epsilon}(\mathbb{P}_{k},\mathbb{Q})\) is strictly convex and l.s.c. (lower semi-continuous) itself. The latter follows from [9, Th. 2.9] by noting that on \(\mathcal{P}(\mathcal{Y})\) the map \(\mu\mapsto\mathbb{E}_{y\sim\mu}c_{k}(x,y)-H(\mu)\) is l.s.c, bounded from below and strictly convex thanks to the entropy term. Since \(\mathcal{P}(\mathcal{Y})\) is weakly compact (as \(\mathcal{Y}\) is compact due to Prokhorov's theorem, see, e.g., [89, Box 1.4]), it holds that \(\mathcal{B}(\mathbb{Q})\) admits at least one minimizer due to the Weierstrass theorem [89, Box 1.1], i.e., a barycenter \(\mathbb{Q}^{*}\) exists. In the paper, _we work under the reasonable assumption that there exists at least one \(\mathbb{Q}\) for which \(\mathcal{B}(\mathbb{Q})\textless\infty\)_. In this case, the barycenter \(\mathbb{Q}^{*}\) is unique due to the strict convexity of \(\mathcal{B}\).

## Appendix B Extended discussions

### Extended discussion of related works

**Discrete OT-based solvers** provide solutions to OT-related problems between discrete distributions. A comprehensive overview can be found in [83]. The discrete OT methods for EOT barycenter estimation are [20, 98, 11, 21, 22, 30, 63, 49, 68]. An alternative formulation of the barycenter problem based on weak mass transport and corresponding discrete solver could be found in [12]. In spite of sound theoretical foundations and established convergence guarantees [64], these approaches can not be directly adapted to our learning setup, see SS2.3.

**Continuous OT solvers.** Beside the continuous EOT solvers discussed in SS3, there exist a variety of neural OT solver for the non-entropic (unregularized, \(\epsilon=0\)) case. For example, solvers such as [44, 74, 54, 58, 57, 32, 35, 88, 5, 31], are based on optimizing the dual form, similar to our (2), with neural networks. We mention these methods because they serve as the basis for certain continuous unregularized barycenter solvers. For example, ideas of [55] are employed in the barycenter method presented in [59]; the solver from [74] is applied in [32]; max-min solver introduced in [58] is used in [55]. It is also worth noting that there exist several neural solvers that cater to more general OT problem formulations [61, 60, 31, 8, 84]. These can even be adapted to the EOT case [42] but require substantial technical effort and the usage of restrictive neural architectures.

**Continuous EOT solvers** aim to recover the optimal EOT plan \(\pi^{*}\) in EOT problems like (1) between unknown distributions \(\mathbb{P}\) and \(\mathbb{Q}\) which are only accessible through a limited number of samples. One group of methods [37, 91, 23] is based on the dual formulation of OT problem regularized with KL divergences [37, Eq. (\(\mathcal{P}_{e}\))] which is equivalent to (1). Another group of methods [109, 24, 13, 41, 106, 94] takes advantage of the dynamic reformulation of EOT via Schrodinger bridges [70, 76]. In turn, [51] solve EOT with conditional flow matching [106].

In [78], the authors propose an approach to tackle (1) by means of Energy-Based models [69, 100, EBM]. They develop an optimization procedure resembling standard EBM training which retrieves the optimal dual potential \(f^{*}\) appearing in (2). As a byproduct, they recover the optimal conditional plans \(\mu_{x}^{f^{*}}=\pi^{*}(\cdot|x)\). Our approach for solving the EOT barycenter (5) is primarily inspired by this work. In fact, we manage to overcome the theoretical and practical difficulties that arise when moving from the EOT problem guided with EBMs to the EOT barycenter problem (multiple marginals, optimization with respect to an _unfixed_ marginal distribution \(\mathbb{Q}\)), see SS4 for details of our method.

**Other related works.** Another relevant work is [95], where the authors study the barycenter problem and restrict the search space to a manifold produced by a GAN. This idea is also utilized in SS5.2 and SS5.3, but their overall setting drastically differs from our setup and actually is not applicable. We search for a barycenter of \(K\)_high-dimensional image distributions_ represented by their random samples (datasets). In contrast, they consider \(K\) images, represent each _image as a \(2D\) distribution_ via its intensity histogram and search for a **single image** on the GAN manifold whose density is the barycenter of the input images. To compute the barycenter, they use discrete OT solver. In summary, neither our barycenter solver is intended to be used in their setup, nor their method is targeted to solve the problems considered in our paper.

### Extended discussion of potential applications

It is not a secret that despite considerable efforts in developing continuous barycenter solvers [72, 59, 55, 32, 82, 14], these solvers have not found yet a real working practical application. The reasons for this are two-fold:

1. Existing continuous barycenter solvers (Table 1) are yet not scalable enough and/or work exclusively with the quadratic cost (\(\ell^{2}\)), which might be not sufficient for the practical needs.
2. Potential applications of barycenter solvers are too technical and, unfortunately, require substantial efforts (challenging and costly data collection, non-trivial design of task-specific cost functions, unclear performance metrics, etc.) to be implemented in practice.

Despite these challenges, there exist rather inspiring practical problem formulations where the continuous barycenter solvers may potentially shine and we name a few below. These potential applications motivate the research in the area. More generally, we hope that our developed solver could be a step towards applying continuous barycenters to practical tasks that benefit humanity.

**1. Solving domain shift problems in medicine (Fig. 6a).** In medicine, it is common that the data is collected from multiple sources (laboratories, clinics) and using different equipment from various vendors, each with varying technical characteristics [40, 65, 53, 103, 112]. Moreover, the data coming from each source may be of limited size. These issues complicate building robust and reliable machine learning models by using such datasets, e.g., learning MRI segmentation models to assist doctors.

A potential way to overcome the above-mentioned limitations is to find a common representation of the data coming from multiple sources. This representation would require translation maps that can transform the new (previously unseen) data from each of the sources to this shared representation. This formulation closely aligns with the continuous barycenter learning setup (SS2.3) studied in our paper. In this context, the barycenter could play the role of the shared representation.

To apply barycenters effectively to such domain averaging problems, two crucial ingredients are likely required: appropriate cost functions \(c_{k}\) and a suitable data manifold \(\mathcal{M}\) in which to search for the barycenters. The design of the cost itself may be a challenge requiring certain domain-specific knowledge that necessitates involving experts in the field. Meanwhile, the manifold constraint is required to avoid practically meaningless barycenters such as those considered in SS5.2. Nowadays, with the rapid growth of the field of generative models, it is reasonable to expect that soon the new large models targeted for medical data may appear, analogously to DALL-E [85], StableDiffusion [87] or StyleGAN-T [90] for general image synthesis. These future models could potentially parameterize the medical data manifolds of interest, opening new possibilities for medical data analysis.

**2. Mixing geological simulators (Fig. 6b).** In geological modeling, variuos simulators exist to model different aspects of underground deposits. Sometimes one needs to build a generic tool which can take into account several desired geological factors which are successfully modeled by separate simulators.

**Flumy1** is a process-based simulator that uses hydraulic theory [48] to model specific channel depositional processes returning a detailed three-dimensional geomodel informed with deposit lithotype, age and grain size. However, its result is a 3D segmentation field of facies (rock types) and it does not produce the real valued porosity field needed for hydrodynamical modeling.

Footnote 1: [https://flumy.minesparis.psl.eu](https://flumy.minesparis.psl.eu)

**Petrel2** software is the other popular simulator in the oil and gas industry. It is able to model complex real-valued geological maps such as the distribution of porosity. The produced porosity fields may not be realistic enough due to paying limited attention to the geological formation physics.

Footnote 2: [https://www.software.slb.com/products/petrel](https://www.software.slb.com/products/petrel)

Both Flumy and Petrel simulators contain some level of stochasticity and are hard to use in conjunction. Even when conditioned on common prior information about the deposit, they may produce maps of facies and permeability which do not meaningfully correspond to each other. This limitation provides potential _prospects for barycenter solvers_ which could be used to _get the best from both simulators_ by mixing the distributions produced by each of them.

From our personal discussions with the experts in the field of geology, such task formulations are of considerable interest both for scientific community as well as industry. Applying our barycenter solver in this context is a challenge for future research. We acknowledge that this would also require overcoming considerable technical and domain-specific issues, including the data collection and the choice of costs \(c_{k}\).

### Extended discussion on doubly-regularized OT barycenters

The objective (5) is not the only way to formulate Entropic OT barycenter problem. Recent studies [15, 82] consider the so-called _doubly_-regularized Entropic OT barycenters. Following the notations from [15], for \(\lambda\geq 0\) and \(\tau\geq 0\) we define:

\[\text{EOT}^{\text{tr}}_{c,\lambda,\tau}(\mathbb{P},\mathbb{Q})\stackrel{{ \text{def}}}{{=}}\min_{\pi\in\Pi(\mathbb{P},\mathbb{Q})}\Big{\{} \underset{(x,y)\sim\pi}{\mathbb{E}}c(x,y)+\lambda KL(\pi\|\mathbb{P}\otimes \mathbb{Q})+\tau H(\mathbb{Q})\Big{\}}.\]

The corresponding _doubly_-regularized EOT barycenter problem is as follows:

\[\mathcal{L}^{*,\text{dr}}\stackrel{{\text{def}}}{{=}}\underset{ \mathbb{Q}\in\mathcal{P}(\mathcal{Y})}{\inf}\mathcal{B}^{\text{dr}}(\mathbb{Q })\stackrel{{\text{def}}}{{=}}\underset{\mathbb{Q}\in\mathcal{P}( \mathcal{Y})}{\inf}\sum_{k=1}^{K}\lambda_{k}\text{EOT}^{\text{dr}}_{c,\lambda,\tau}(\mathbb{P}_{k},\mathbb{Q}). \tag{40}\]

Figure 6: A schematical presentation of potential applications of barycenter solvers.

It turns out that our considered EOT barycenter (5) with regularization strength \(\epsilon\) is the particular case of (40) with \(\lambda=\tau=\epsilon\). According to the classification [15, Table 1], this case is known as _Schrodinger_ barycenter. The natural question arizes: Is it possible to adapt our energy-guided methodology for the case \(\lambda\neq\tau\)?

To answer this question, in what follows, we have a closer look at the differences between general _doubly_-regularized formulations and our particular _Schrodinger_ specification. The important property of our case is that the entropy term \(H(\mathbb{Q})\) completely disappears from the barycenter objective. In all the other regularized cases, when \(\lambda\neq\tau\), this entropy term immediately reappears (either with the plus or minus sign). The presence of \(H(\mathbb{Q})\) term notably differs from ours and seems like to be not suitable for our solver. In what follows, we explain the reasons.

In the _Schrodinger_ case, the barycenter problem can be solved via optimizing conditional distributions \(\pi_{k}(y|x_{k})\). Namely, we use potentials \(f_{k}\) combined with costs \(c_{k}\) to approximate the energy functions (unnormalized log-densities) of these conditionals. We employ EBM-based techniques to compute the gradient of the learning objective which avoids the direct computation of the entropy terms \(H(\pi_{k}(y|x_{k}))\) appearing in the \(C_{k}\)-transforms.

If we further add the non-zero term \(H(\mathbb{Q})\) to the barycenter objective, this will presumably require (in the dual objective) a separate computation of the entropy terms \(H(\pi_{k}(y))\) of second marginals \(\pi_{k}(y)\) of each \(\pi_{k}\). **This is highly non-trivial** and it seems like our _solver does not easily generalize to this case_. In our framework, we can get samples of \(\pi_{k}(y)\) by MCMC (via sampling \(x_{k}\sim\mathbb{P}_{k}\) and then running MCMC for \(\pi(y|x_{k})\)). However, estimation of entropy of \(\pi_{k}(y)\) from raw samples still remains infeasible. In particular, EBM-like techniques (which we employ) can not be used here to derive the gradient of the objective. This is because the required unnormalized density of \(\pi_{k}(y)\) is itself unknown (we only know it for conditional distributions \(\pi_{k}(y|x)\)).

## Appendix C Experimental Details

The hyperparameters of our solver are summarized in Table 4. Some hyperparameters, e.g., \(L,S,iter\), are chosen primarily from time complexity reasons. Typically, the increase in these numbers positively affects the quality of the recovered solutions, see, e.g., [42, Appendix E, Table 16]. However, to reduce the computational burden, we report the figures which we found to be reasonable. Working with the manifold-constraint setup, we parameterize each \(g_{\theta_{k}}(z)\) in our solver as \(h_{\theta_{k}}\circ G(z)\), where \(G\) is a pre-trained (frozen) StyleGAN and \(h_{\theta_{k}}\) is a neural network with the ResNet architecture. We empirically found that this strategy provides better results than a direct MLP parameterization for the function \(g_{\theta_{k}}(z)\).

To train the StyleGAN for MNSIT01 & Ave, celeba! experiments, we employ the official code from

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|l|l|} \hline Experiment & \(D\) & \(K\) & \(\epsilon\) & \(\lambda_{1}\) & \(\lambda_{2}\) & \(\lambda_{3}\) & \(\lambda_{4}\) & \(f_{\theta,k}\) & \(lr_{g_{\theta,k}}\) & \(iter\) & \(\sqrt{\eta}\) & \(L\) & \(S\) \\ \hline Toy 2D & 2 & 3 & \(10^{-2}\) & 1/3 & 1/3 & 1/3 & - & MLP & \(10^{-4}\) & 200 & 1.0 & 300 & 256 \\ \hline MNIST 0/1 & 1024 & 2 & \(10^{-2}\) & 0.5 & 0.5 & - & - & ResNet & \(10^{-4}\) & 1000 & 0.1 & 500 & 32 \\ \hline MNIST 0/1 & 512 & 2 & \(10^{-2}\) & 0.5 & 0.5 & - & - & ResNet & \(10^{-4}\) & 1000 & 0.1 & 250 & 32 \\ \hline Ave, celeba! (Data) & 3*64\({}^{2}\) & 3 & \(10^{-2}\) & 0.25 & 0.5 & 0.25 & - & ResNet & \(10^{-4}\) & 5000 & 0.1 & 500 & 64 \\ \hline Ave, celeba! (Manifold) & 512 & 3 & \(10^{-4}\) & 0.25 & 0.5 & 0.25 & - & ResNet & \(10^{-4}\) & 1000 & 0.1 & 250 & 128 \\ \hline Gaussians & 2-64 & 3 & \(10^{-2}\), 1 & 0.25 & 0.25 & 0.5 & - & MLP & \(10^{-3}\) & 50000 & 0.1 & 700 & 1024 \\ \hline Sphere & 3 & 4 & \(10^{-2}\) & 0.25 & 0.25 & 0.25 & 0.25 & MLP & \(3\times 10^{-3}\) & 1000 & 1.0 & 300 & 256 \\ \hline Single-cell & 1000 & 2 & \(10^{-2}\) & 0.25 & 0.75 & - & - & MLP & \(5\times 10^{-4}\) & 1000 & 0.05 & 1000 & 1024 \\ \hline \end{tabular}
\end{table}
Table 4: Hyperparameters that we use in the experiments with our Algorithm 1.

\begin{table}
\begin{tabular}{|c|c|c|} \hline Experiment & training time & inference time \\ \hline Toy \(2D\) **(Ours)** & 3b & 20s \\ \hline Gaussians **(Ours)** & 6h & 40s \\ \hline Sphere **(Ours)** & 3h & 20s \\ \hline MNIST 0/1 manifold **(Ours)** & 10h & 1m \\ \hline MNIST 0/1 data **(Ours)** & 20h & 1m \\ \hline Ave Celeba manifold **(Ours)** & 66h & 2m \\ \hline Ave Celeba data **(Ours)** & 60h & 2m \\ \hline Ave Celeba data **(WINE)** & 160h & 10s \\ \hline Ave Celeba data **(SCWB)** & 20h & 10s \\ \hline \end{tabular}
\end{table}
Table 3: Computational complexity for **Ours** (all experiments) and baselines (Ave Celeba).

[https://github.com/NVlabs/stylegan2-ada-pytorch](https://github.com/NVlabs/stylegan2-ada-pytorch)

**Computational complexity.** We report the (approximate) time for training and inference (batch size \(S=128\)) of our method on different experimental setups, see Table 3 (the hardware is a single V100 gpu). For Ave Celeba experiment, we additionally report the computational complexity of the competitors. As we can see, all the methods in this experiment (Ave Celeba) require a comparable amount of time for training. The inference with our approach is expectedly costlier than competitors due to the reliance on MCMC.

### Barycenters of 2D/3D Distributions

**Cartesian representation of twister map.** In SS5.1 we define twister map \(u\) using polar coordinate system. For clearness, we give its form on cartesian coordinates. Let \(x\in\mathbb{R}^{2}\), \(x=(x_{(1)},x_{(2)})\). Note that \(\|x\|=\sqrt{x_{(1)}^{2}+x_{(2)}^{2}}\) Then,

\[u(x)=u(x_{(1)},x_{(2)})=\begin{pmatrix}\cos\big{(}\|x\|\big{)}&-\sin\big{(}\|x \|\big{)}\\ \sin\big{(}\|x\|\big{)}&\cos\big{(}\|x\|\big{)}\end{pmatrix}\begin{pmatrix}x_{ (1)}\\ x_{(2)}\end{pmatrix},\]

i.e., the twister map rotates input points \(x\) by angles equal to \(\|x\|\).

**Analytical barycenter distribution for 2D Twister experiment.** Below, we provide a mathematical derivation that the true unregularized barycenter of the distributions \(\mathbb{P}_{1},\mathbb{P}_{2},\mathbb{P}_{3}\) in Fig. 1(a) coincides with a Gaussian.

We begin with a rather general observation. Consider \(\mathcal{X}_{k}=\mathcal{Y}=\mathbb{R}^{D}\) (\(k\in\overline{K}\)) and let \(\text{OT}_{c}\stackrel{{\text{def}}}{{=}}\text{EOT}_{c,0}\) denote the unregularized OT problem (\(\epsilon=0\)) for a given continuous cost function \(c\). Let \(u:\mathbb{R}^{D}\to\mathbb{R}^{D}\) be a measurable bijection and consider \(\mathbb{P}^{\prime}_{k}\in\mathcal{P}(\mathbb{R}^{D})\) for \(k\in\overline{K}\). By using the change of variables formula, we have for all \(\mathbb{Q}^{\prime}\in\mathcal{P}(\mathbb{R}^{D})\) that

\[\text{OT}_{c\circ(u\times u)}\big{(}u_{\#}^{-1}(\mathbb{P}^{\prime}),u_{\#}^{ -1}(\mathbb{Q}^{\prime})\big{)}=\text{OT}_{c}(\mathbb{P}^{\prime},\mathbb{Q}^ {\prime}), \tag{41}\]

where \(\#\) denotes the pushforward operator of distributions and \([c\circ(u\times u)](x,y)=c\big{(}u(x),u(y)\big{)}\). Note that by (41) the barycenter of \(\mathbb{P}^{\prime}_{1},\ldots,\mathbb{P}^{\prime}_{K}\) for the unregularized problem with cost \(c\) coincides with the result of applying the pushforward operator \(u_{\#}^{-1}\) to the barycenter of the very same problem but with cost \(c\circ(u\times u)\).

Next, we fix \(u\) to be the twister map (SS5.1). In Fig. 1(a) we plot the distributions \(\mathbb{P}_{1}\stackrel{{\text{def}}}{{=}}u_{\#}^{-1}\mathbb{P}^ {\prime}_{1},u_{\#}^{-1}\mathbb{P}^{\prime}_{1},u_{\#}^{-1}\mathbb{P}^{\prime}_ {3}\) which are obtained from Gaussian distributions \(\mathbb{P}^{\prime}_{1}=\mathcal{N}\big{(}(0,4),I_{2}\big{)},\mathbb{P}^{ \prime}_{2}=\mathcal{N}\big{(}(-2,2\sqrt{3}),I_{2}\big{)},\mathbb{P}^{\prime} _{3}=\mathcal{N}\big{(}(2,2\sqrt{3}),I_{2}\big{)}\) by the pushforward. Here \(I_{2}\) is the 2-dimensional identity matrix. For the unregularized \(\ell^{2}\) barycenter problem, the barycenter of such shifted Gaussians can be derived analytically [3]. The solution coincides with a zero-centered standard Gaussian \(\mathbb{Q}^{\prime}=\mathcal{N}\big{(}0,I_{2}\big{)}\). Hence, the barycenter of \(\mathbb{P}_{1},\ldots,\mathbb{P}_{K}\) w.r.t. the cost \(\ell^{2}\circ(u\times u)\) is given by \(\mathbb{Q}^{*}=u_{\#}^{-1}\mathbb{Q}^{\prime}\). From the particular choice of \(u\) it is not hard to see that \(\mathbb{Q}^{*}=\mathbb{Q}^{\prime}=\mathcal{N}\big{(}0,I_{2}\big{)}\) as well.

### Barycenters of MNIST Classes

**Additional qualitative examples** of our solver's results are given in Figure 7.

**Details of the baseline solvers.** For the solver by [32, SCWB], we run their publicly available code from the official repository

[https://github.com/sbyebss/Scalable-Wasserstein-Barycenter](https://github.com/sbyebss/Scalable-Wasserstein-Barycenter)

The authors do no provide checkpoints, and we train their barycenter model from scratch. In turn, for the solver by [55, WIN], we also employ the publicly available code

[https://github.com/iamalexkorotin/WassersteinIterativeNetworks](https://github.com/iamalexkorotin/WassersteinIterativeNetworks)

Here we do not train their models but just use the checkpoint available in their repo.

### Barycenters of the Ave, Celeba! Dataset

**Additional qualitative examples** of our solver's results are given in Figure 8.

**Extra experiment in Data Space.** Our method in the manifold constrained setup on MNIST dataset performs better than in the data space setup (see Figure 5). It generates less noised images and demonstrates better perceptual quality. For this reason, we provide only Manifold-constrained setting for experiment with Ave, Celeba! dataset in SS5.3.

For completeness, here we also test our method directly in the data space setup for Ave, Celeba! dataset. Hyperparameters of our solver are listed in Table 4. In Figure 10, we show images obtained in Data space setup. As expected, the FID scores in Data space are not that good as the images are more noised since we solve the entropy-regularized problem. But we stress one more time that our method permits StyleGAN manifold trick, which greatly improves the performance, see the images in Figure 4 for the manifold-constrained setup.

**Convergence at training.** We provide the behaviour of \(\mathcal{L}_{2}\)-UVP metric (between the unregularized ground truth barycenter mapping and the entropic barycenter mapping for our learned \(\pi^{f_{k}}(y|x_{k})\)) for Ave Celeba experiment SS5.3, see Figure 9. We emphasize that \(\mathcal{L}_{2}\)-UVP directly measures (by computing pointwise MSE) how the learned mapping differs from the true mapping.

Figure 7: Experiment with averaging MNIST 0/1 digit classes. The plot shows additional examples of samples transported with **our** solver to the barycenter.

**Convergence at inference.** Since Langevin Dynamics is at the heart of our method for sampling from a conditional plan, it is important to demonstrate the dependence of the inference times/quality on the number of \(L\) Langevin steps in Manifold constrained as well as in the Data space setup. We demonstrate Tables 5 and 6, where we show the trade-off between number of Langevin steps \(L\) and the obtained quality. To provide the comprehensive analysis, we report FID scores as well as sampling time for different number of steps \(L\). Expectedly, inference time linearly depends on \(L\) both setups.

Our results testify that (in Manifold constrained setting) our method achieves good quality even with rather small number of Langevin steps, i.e., the computational burden of our approach could be considerably reduced with rather little trade-off in quality.

**Details of the baseline solvers.** For the [55, WIN] solver, we use their pre-trained checkpoints provided by the authors in the above-mentioned repository. Note that the authors of [32, SCWB] do not consider such a high-dimensional setup with RGB images. Hence, to implement their approach in this setting, we follow [55, Appendix B.4].

Figure 8: _Experiment on the Ave, celeba! barycenter dataset._ The plots show additional examples of samples transported with **our** solver to the barycenter.

Figure 9: Training curves of \(\mathcal{L}2\)-UVP vs. time for **OUR** proposed method in Manifold-constrained setup with Style-GAN on Ave, Celeba! dataset. The duration of the training is 100 h (1 GPU V100).

[MISSING_PAGE_FAIL:29]

We consider 3 Gaussian distributions \(\mathbb{P}_{1},\mathbb{P}_{2},\mathbb{P}_{3}\) in dimensions \(D=2,4,8,16,64\) and compute the approximate EOT barycenters \(\pi_{k}^{\widehat{T}_{k}}\) for \(\epsilon=0.01,1\) w.r.t. weights \((\lambda_{1},\lambda_{2},\lambda_{3})=(\frac{1}{4},\frac{1}{4},\frac{1}{2})\) with our solver. To initialize these distributions, we follow the strategy of [55, Appendix C.2]. The ground truth unregularized barycenter \(\mathbb{Q}^{*}\) is estimated via the above-mentioned iterative procedure. We use the code from WIN repository available via the link mentioned in Appendix C.2. To assess the WIN solver, we use the unexplained variance percentage metrics defined as \(\mathcal{L}_{2}\)-UVP\((\widehat{T})=100\cdot[\|\hat{T}-T^{*}\|]_{\mathbb{P}}^{2}\) where \(T^{*}\) denotes the optimal transport map \(T^{*}\), see [54, SS5.1]. Since our solver computes EOT plans but not maps, we evaluate the barycentric projections of the learned plans, i.e., \(\widehat{\overline{T}}_{k}(x)=\int_{\mathcal{Y}}y\pi_{k}^{\widehat{T}_{k}}(y|x)\), and calculate \(\mathcal{L}_{2}\)-UVP\((\widehat{\overline{T}}_{k},T_{k}^{*})\). We evaluate this metric using \(10^{4}\) samples. To estimate the barycentric projection in our solver, we use \(10^{3}\) samples \(y\sim\pi_{k}^{\widehat{T}_{k}}(y|x_{k})\) for each \(x_{k}\). To keep the table with the results simple, in each case we report the average of this metric for \(k=1,2,3\) w.r.t. the weights \(\lambda_{k}\).

We see that for small \(\epsilon=0.01\) and dimension up to \(D=16\), our algorithm gives the results even better than WIN solver designed specifically for the unregularized case (\(\epsilon=0\)). As was expected, larger \(\epsilon=1\) leads to the increased bias in the solutions of our algorithm and \(\mathcal{L}_{2}\)-UVP metric increases.

**Effect of the batch size.** In order to test how our method is affected by batch size, we conduct an additional empirical study with varying batch size. We follow our Gaussian experimental setup from above and pick \((D,\epsilon)=(2,0.1)\), \((D,\epsilon)=(16,0.01)\). As the batch sizes, we consider \(2,4,8,16,32,64,128,256,512,1024\). As we can see in Table 9, the quality of the recovered plans \(\pi^{f_{k}}\) between reference and barycenter distributions naturally grows with the batch size. In all our other experiments, we typically choose the batch size to be a reasonable number which, on the one hand, allows achieving sufficient quality and, on the other hand, provides reasonable computational burden. In the image experiments, we use batch size \(\leq 128\) as it already provides reasonable performance. Going beyond that is challenging due to the computational restrictions.

**Effect of sampling steps number at training.** We conducted an ablation study testing how our method is affected by chosen number of discretized Langevin dynamic steps (\(L\) from SS4.2) at training, the results are presented in Figure 11. In this experiment, we try to learn the barycenter of Gaussian distributions; we pick the dimensionality \(D=64\) (which is the highest-dimensional case among the considered); \(\epsilon=10^{-1}\). As we can see, performance drops when the number of steps is insufficient. Overall, in all our experiments, the number of Langevin steps is chosen to achieve reasonable qualitative/quantitative results.

### Single-cell experiment

We consider the problem of predicting the interpolation between single cell populations at multiple timepoints from [107]. The objective is to interpolate the distribution of cell population at any intermediate point in time, call it \(t_{i}\), given the cell population distributions at past and future time-points \(t_{i+1}\) and \(t_{i-1}\). Since this is an interpolation problem, it is natural to expect that the intermediate population is a (entropy-regularized) barycentric average (with \(\ell^{2}\) cost) of both the population distributions available at the nearest preceding and future times. We leverage the data pre-processing and metrics from paper [56], wherein the authors provide a complete

\begin{table}
\begin{tabular}{c|c|c|c} \hline
**Dim / Method** & Ours (\(\epsilon=1\)) & Ours (\(\epsilon=0.01\)) & [55, WIN] \\ \hline
2 & 1.12 & **0.02** & 0.03 \\ \hline
4 & 1.6 & **0.05** & 0.08 \\ \hline
8 & 1.85 & **0.06** & 0.13 \\ \hline
16 & 1.32 & **0.09** & 0.25 \\ \hline
64 & 1.83 & 0.84 & **0.75** \\ \hline \end{tabular}
\end{table}
Table 7: \(\mathcal{L}_{2}\)-UVP for our method with \(\epsilon=0.01,1\) and WIN, \(D=2,4,8,16,64\).

Figure 11: The training curves of \(\mathcal{L}_{2}\)-UVP vs. iterations for **OUR** proposed method for the barycenter of Gaussian distributions depending on number of Langevin steps \(L\).

notebook with the code to launch the setup similar to [107]. There are 3 experimental settings with dimensions \(D=50,100,1000\), each setting contains two setups: predicting day 3 given days 2 and 4 and predicting day 4 given days 3 and 7. The metric is MMD; see [107] or Section 5.3 from [56] for additional experimental details. We report the result in Table 8 where we find that in most cases, our general-purpose entropic barycenter approach nearly matches the performance of leading baselines. This underscores the scope of problems in which our barycentric optimal transport technology can act as a viable foundation model, directly out-of-the-box.

## Appendix D Alternative EBM training procedure

In this section, we describe an alternative **simulation-free** training procedure for learning EOT barycenter distribution via our proposed methodology. The key challenge behind our approach is to estimate the gradient of the dual objective (4.3). To overcome the difficulty, in the main part of our manuscript, we utilize MCMC sampling from conditional distributions \(\mu_{x_{k}}^{f_{\theta,k}}\) and estimate the loss with Monte-Carlo. Here we discuss a potential alternative approach based on **importance sampling** (IS) [105]. That is, we evaluate the internal integral over \(\mathcal{Y}\) in (4.3):

\[\mathcal{I}(x_{k})\stackrel{{\text{def}}}{{=}}\int_{\mathcal{Y}} \left[\frac{\partial}{\partial\theta}f_{\theta,k}(y)\right]\mathrm{d}\mu_{x_{ k}}^{f_{\theta,k}}(y) \tag{42}\]

with help of an auxiliary proposal (continuous) distribution accessible by samples with the known density \(q(y)\). Let \(Y^{q}=\{y_{1}^{q},\ldots,y_{P}^{q}\}\) be a sample from \(q(y)\). Define the weights:

\[\omega_{k}(x_{k},y_{p}^{q})\stackrel{{\text{def}}}{{=}}\exp\bigg{(} \frac{f_{\theta,k}(y_{p}^{q})-c(x_{k},y_{p}^{q})}{\epsilon}\bigg{)}q(y_{p}^{q}).\]

Then (42) permits the following stochastic estimate:

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Solver/DIM & 50 & 100 & 1000 \\ \hline
**OUR** & \(\mathbf{2.32}\pm 0.12\) & \(2.26\pm 0.09\) & \(1.34\pm 0.12\) \\ \hline LightSB-M [2] & \(2.33\pm 0.09\) & \(\mathbf{2.172}\pm 0.08\) & \(\mathbf{1.33}\pm 0.05\) \\ \hline SFM-sink [3] & \(2.66\pm 0.18\) & \(2.52\pm 0.17\) & \(1.38\pm 0.05\) \\ \hline EGNOT [1] & \(2.39\pm 0.06\) & \(2.32\pm 0.15\) & \(1.46\pm 0.20\) \\ \hline \end{tabular}
\end{table}
Table 8: Energy distance (averaged for two setups and 5 random seeds) on the MSCI dataset along with 95%-confidence interval (\(\pm\)-intervals). The best solver according to the mean value is **bolded**.

Figure 12: _2D wrister example._ _Trained with importance sampling_: The true barycenter of 3 comets vs. the one computed by our solver with \(\epsilon=10^{-2}\). Two costs \(c_{k}\) are considered: the twisted cost (12a, 12b) and \(\ell^{2}\) (12c, 12d). We employ the _simulation-free_ importance sampling procedure for training.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|l|} \hline \((D,\epsilon)\) & Batch size & 2 & 4 & 8 & 16 & 32 & 64 & 128 & 256 & 512 & 1024 \\ \hline (2.0.1) & \(\mathcal{L}_{2}\)-UVP \(\downarrow\) & 0.20 & 0.14 & 0.07 & 0.05 & 0.05 & 0.04 & 0.02 & 0.03 & 0.02 & 0.02 \\ \hline (16,0.01) & 0.56 & 0.40 & 0.37 & 0.30 & 0.25 & 0.22 & 0.20 & 0.17 & 0.15\[\mathcal{I}(x_{k})\approx\frac{\sum_{p=1}^{P}\big{[}\frac{\partial}{\partial\theta}f_ {\theta,k}(y)\big{]}\,\omega_{k}(x_{k},y_{p}^{q})}{\sum_{p=1}^{P}\omega_{k}(x_{k},y_{p}^{q})}. \tag{43}\]

**Experimental illustration.** To demonstrate the applicability of IS-based training procedure to our barycenter setting, we conduct the experiment following our **2D Twister** setup, see SS5.1. We employ zero-mean \(16I\)-covariance Gaussian distribution as \(q\) and pick the batch size \(P=1024\). Our results are shown in Figure 12. As we can see, the alternative training procedure yields similar results as Figure2 but converges faster (\(\approx 1\) min. VS \(\approx 18\) min. of the original MCMC-based training).

**Concluding remarks.** We note that IS-based methods requires accurate selection of the proposal distribution \(q\) to reduce the variance of the estimator [105]. It may be challenging in real-world scenarios. We leave the detailed study of more advanced IS approaches in the context of energy-based models and their applicability to our EOT barycentric setup to follow-up research.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the Introduction section, we completely describe our contributions. For every contribution, we provide a link to the section about it.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss limitations in Section 6.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Proofs are provided in Appendix A.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Experimental details are discussed in Appendix C. Code for the experiments is available at publicly accessible github. All the datasets are available in public.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification:Code is publicly available at a github repository. Experimental details are provided in Appendix C. All the datasets are publicly available.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experimental details are discussed in Appendix C; the hyperparameters and peculiarities are hard-coded in our publicly available source code.
7. **Experiment Statistical Significance** Question: Does the paper report bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Not all of the experiments are conducted following the statistical significance protocol due to computational restrictions.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes]Justification: In Appendix C, we mention time and resources.
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Research conforms with NeurIPS Code of Ethics.
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss broader impact in Appendix 6.
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The research does not need special safeguards.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cite each used assets.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: New code is available at public github repository, the code is self-documented and follows our experimental protocol from the manuscript. The license is MIT.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The research does not engage with Crowdsourcing or Human subjects.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The research does not engage with Crowdsourcing or Human subjects.