# Simplicity Prevails: Rethinking Negative Preference Optimization for LLM Unlearning

Anonymous authors

Paper under double-blind review

###### Abstract

In this work, we address the problem of large language model (LLM) unlearning, aiming to remove unwanted data influences and associated model capabilities (_e.g._, copyrighted data or harmful content generation) while preserving essential model utilities, without the need for retraining from scratch. Despite the growing need for LLM unlearning, a principled optimization framework remains lacking. To this end, we revisit the state-of-the-art approach, negative preference optimization (NPO), and identify the issue of reference model bias, which could undermine NPO's effectiveness, particularly when unlearning forget data of varying difficulty. Given that, we propose a simple yet effective unlearning optimization framework, called SimNPO, showing that'simplicity' in removing the reliance on a reference model (through the lens of simple preference optimization) benefits unlearning. We also provide deeper insights into SimNPO's advantages, supported by analysis using mixtures of Markov chains. Furthermore, we present extensive experiments validating SimNPO's superiority over existing unlearning baselines in benchmarks like TOFU and MUSE, and robustness against relearning attacks.

## 1 Introduction

The rapid advancement of large language models (LLMs) has raised security and safety concerns, including issues related to copyright violations and sociotechnical harms (Huang et al., 2024; Wang et al., 2023; Li et al., 2024; Shi et al., 2024). However, retraining these models to remove undesirable data influences is often impractical due to the substantial costs and time required for such processes. This gives rise to the problem of **LLM unlearning**, which aims to effectively remove undesired data influences and/or model behaviors while preserving the utility for essential, unrelated knowledge generation, and maintaining efficiency without the need for retraining (Eldan and Russinovich, 2023; Yao et al., 2023; Liu et al., 2024; Blanco-Justicia et al., 2024).

To trace its origins, the concept of _machine unlearning_ was initially developed for data removal to comply with privacy regulations such as the "right to be forgotten" (Rosen, 2011; Hoofnagle et al., 2019), with early studies focusing on vision models (Cao and Yang, 2015; Warnecke et al., 2021; Bourtoule et al., 2021; Thudi et al., 2022; Kurmanji et al., 2024; Jia et al., 2023; Gandikota et al., 2023; Fan et al., 2024). However, it is soon adapted to LLMs to remove unwanted data, knowledge, or specific model capabilities (Eldan and Russinovich, 2023; Yao et al., 2023; Liu et al., 2024; Ji et al., 2024; Li et al., 2024; Shi et al., 2024; Maini et al., 2024; Zhang et al., 2024; Jia et al., 2024). Compared to vision model unlearning, designing effective and efficient unlearning methods for

Figure 1: (a) _Systematic overview_ of an LLM (\(\bm{\theta}\)) post-unlearning using the proposed SimNPO optimization principle, compared to the popular NPO (negative preference optimization) framework (Zhang et al., 2024) and the reference model (_i.e._, model prior to unlearning). (b) & (c) _Experiment highlights_ on the TOFU dataset with a 5% forget size (Maini et al., 2024) and on the MUSE News dataset (Shi et al., 2024). Unlearning effectiveness is measured by forget quality for TOFU and PriVLeaf for MUSE, while utility preservation is evaluated using model utility for TOFU and KnowMem on \(\mathcal{D}_{l}\) for MUSE (see Table 1 for details on task-specific metrics). In both tasks, Retrain serves as the gold standard for unlearning by fully removing the influence of the forget data.

[MISSING_PAGE_FAIL:2]

current efforts, NPO (negative preference optimization) (Zhang et al., 2024) stands out as a promising approach by framing the unlearning problem as a variant of direct preference optimization (Rafailov et al., 2024). It has demonstrated competitive performance in benchmarks like TOFU and MUSE. Thus, our work aims to conduct an in-depth exploration of NPO, identifying its current limitations, and proposing potential improvements.

**Preference optimization.** In this work, we advance LLM unlearning through the lens of preference optimization. This is motivated by aligning LLMs with human values, known as reinforcement learning from human feedback (RLHF) (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022). However, online preference optimization algorithms are often complex and challenging to optimize (Santaccoe et al., 2023; Zheng et al., 2023), driving interest in more efficient offline alternatives. Direct preference optimization (**DPO**) (Rafailov et al., 2024) introduced an offline approach that eliminates the need for a reward model, sparking the development of several reward-free offline preference objectives (Zhao et al., 2023; Azar et al., 2024; Hong et al., 2024; Ethayarajh et al., 2024; Meng et al., 2024; Yuan et al., 2024). Notable methods include RRHF (Yuan et al., 2024), SLic-HF (Zhao et al., 2023), IPO (Azar et al., 2024), KTO (Ethayarajh et al., 2024), ORPO (Hong et al., 2024), and SimPO (Meng et al., 2024). Among these methods, SimPO is a reference-free, length-normalized variant of DPO, and we will demonstrate that it is well-suited for integrating into LLM unlearning and improving NPO.

## 3 A Primer on LLM Unlearning

**Problem formulation of LLM unlearning.** Unlearning tasks can take various forms and are typically associated with a specific set of data points to be removed, known as the _forget set_ (\(\mathcal{D}_{\mathrm{f}}\)). In addition, these tasks often require a complementary set of non-forgotten data points, known as the _retain set_ (\(\mathcal{D}_{\mathrm{r}}\)), to preserve model utility by penalizing the divergence caused by unlearning. As a result, the problem of LLM unlearning can be cast as a regularized optimization problem that balances the forget and retain objectives (Liu et al., 2024; Yao et al., 2023; Zhang et al., 2024):

\[\underset{\bm{\theta}}{\text{minimize}} \frac{\mathbb{E}_{(x,y)\in\mathcal{D}_{\mathrm{r}}}[\ell_{\mathrm{ f}}(y|x;\bm{\theta})]}{\text{Fogero loss}}+\lambda\underbrace{\mathbb{E}_{(x,y)\in\mathcal{D}_{\mathrm{r}}}[ \ell_{\mathrm{r}}(y|x;\bm{\theta})]}_{\text{Rotan loss}},\] (1)

where \(\bm{\theta}\) represents the model parameters to be updated during unlearning, \(\lambda\geq 0\) is a regularization parameter to penalize the 'divergence' of unlearning, and \(\ell_{\mathrm{f}}\) and \(\ell_{\mathrm{r}}\) represent forget and retain losses incurred when using model parameters \(\bm{\theta}\) to generate the desired response (\(y\)) given the input \(x\).

Substantial research has focused on designing and analyzing appropriate forget and retain loss functions to solve problem (1) (Liu et al., 2024; Yao et al., 2023; Zhang et al., 2024; Maini et al., 2024; Shi et al., 2024; Eldan and Russinovich, 2023; Jia et al., 2024). For instance, let \(\pi_{\bm{\theta}}(y|x)\) represent the prediction probability of the model \(\bm{\theta}\) given the input-response pair \((x,y)\). The retain loss is typically chosen as the cross-entropy-based sequence prediction loss, \(\ell_{\mathrm{r}}(y|x,\bm{\theta})=-\log\pi_{\bm{\theta}}(y|x)\), whose minimization encourages the model to perform well on the retain data \((x,y)\in\mathcal{D}_{\mathrm{r}}\). If we specify the forget loss as the _negative_ token prediction loss \(\ell_{\mathrm{f}}(y|x,\bm{\theta})=\log\pi_{\bm{\theta}}(y|x)\), whose minimization then _discourages_ the model from learning the forget data \((x,y)\in\mathcal{D}_{\mathrm{f}}\). Minimizing such a forget loss is known as the _gradient ascent_ (**GA**) method (Maini et al., 2024; Thudi et al., 2022). Similarly, minimizing the regularized loss that integrates GA with the retain loss is known as the _gradient difference_ (**GradDiff**) method (Liu et al., 2022; Maini et al., 2024; Yao et al., 2023).

**Negative preference optimization (NPO).** A popular optimization framework for solving problem (1) is NPO (Zhang et al., 2024). It treats the forget data as negative examples in DPO (Rafailov et al., 2024), transforming the unbounded GA-based forget loss into a _bounded loss from below_, which helps prevent catastrophic collapse, and an _adaptive weight smoothing_ applied to the forget loss gradients, allowing for more controlled and stable unlearning. These benefits can be clearly seen from the NPO loss and its gradient as follows:

\[\ell_{\mathrm{NPO}}(\bm{\theta})=\mathbb{E}_{(x,y)\in\mathcal{D}_{\mathrm{f}}} \underbrace{-\frac{2}{\beta}\log\sigma\left(-\beta\log\left(\frac{\pi_{\bm{ \theta}}(y|x)}{\pi_{\mathrm{ref}}(y|x)}\right)\right)}_{\overline{\alpha}:= \ell_{\mathrm{f}}(y|x),\,\bm{\theta}),\,\text{the specified forget loss in (1)}},\] (2)

\[\nabla_{\bm{\theta}}\bm{\ell}_{\mathrm{NPO}}(\bm{\theta})=\mathbb{E}_{(x,y) \in\mathcal{D}_{\mathrm{f}}}\left[\underbrace{\left(\frac{2\pi_{\bm{\theta}}(y |x)^{\beta}}{\pi_{\bm{\theta}}(y|x)^{\beta}+\pi_{\mathrm{ref}}(y|x)^{\beta}} \right)}_{\overline{\alpha}:=w_{\bm{\theta}}(x,y),\,\text{subject weight}}\cdot \nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(y|x)\right],\] (3)

[MISSING_PAGE_FAIL:4]

may be biased toward generating longer but lower-quality sequences (Meng et al., 2024). In such cases, an effective unlearning method should allocate less optimization effort to long-sequence forget data, while focusing more on shorter-length data that are more challenging to unlearn. See Fig. 1-(a) for an illustration. To validate this, **Fig. 2** presents the distributions of truth ratios of forget samples with different response lengths, comparing NPO with Retrain, based on the TOFU setup outlined in Table 1, using a forget set size of 5% (known as the Forget05 unlearning scenario in TOFU). Recall that a truth ratio distribution closer to that of Retrain indicates higher forget quality (FQ), with FQ\(=1\) representing optimal unlearning (_i.e._, Retrain). As shown, NPO exhibits a greater distance from Retrain when unlearning the top 50% shortest-length forget data, resulting in a lower FQ of \(0.58\). In contrast, NPO performs better unlearning for the longer 50% of the forget set, yielding a higher FQ of \(0.81\). Therefore, NPO could be ineffective at unlearning short responses. Additional analyses on the limitation (L1) will be provided in Sec. 5.

**(L2) NPO may cause ineffective gradient weight smoothing and over-unlearning.** Another issue introduced by the reference model \(\pi_{\mathrm{ref}}\) concerns the effectiveness of NPO's gradient weight smoothing, _i.e._, \(w_{\bm{\theta}}(x,y)=(2\pi_{\bm{\theta}}(y|x^{\beta})^{\beta})/(\pi_{\bm{\theta }}(y|x)^{\beta}+\pi_{\mathrm{ref}}(y|x)^{\beta})\) in (3). During the early optimization stage of NPO, we find \(w_{\bm{\theta}}(x,y)\approx 1\) regardless of the varying data-specific unlearning difficulties since the initialization of the unlearned model \(\bm{\theta}\) is given by the reference model. **Fig. 3-(a,b)** support this finding by displaying the gradient smoothing weights of NPO at epoch one (Fig. 3a) and their trajectory over the course of unlearning epochs (Fig. 3b). As shown, the gradient smoothing weights of NPO show large variance, but most values are concentrated around \(w_{\bm{\theta}}(x,y)\approx 1\) at epoch one. This suggests that NPO behaves similarly to GA in the early stage of unlearning, potentially causing over-unlearning and a large utility drop even if the weight decreases in later optimization. **Fig. 3-(c,d)** justify the above by presenting the forget quality and model utility of NPO on TOFU against unlearning epochs. As shown, NPO tends to cause a larger utility drop at early epochs compared to _SimNPO_, the improved alternative to NPO that we will introduce in Sec. 5. Additionally, NPO remains less effective in forgetting than SimNPO throughout the process.

## 5 SimNPO: Advancing NPO by Simple Preference Optimization

In the following, we address the reference model bias in NPO by using a reference-free optimization method, **SimPO** (simple preference optimization) (Meng et al., 2024). We refer to the NPO alternative derived from SimPO as **SimNPO**, simple negative preference optimization.

**Motivation of SimNPO and its forget objective.** The simplest solution to mitigating NPO's reference model bias is to directly remove \(\pi_{\mathrm{ref}}\) from the gradient in (3), setting \(\pi_{\mathrm{ref}}=0\). However, this variant would be _ineffective_, as the reference-free gradient reduces to GA, with \(w_{\bm{\theta}}(x,y)=1\). This negates NPO's advantages.

To develop a better solution for improving NPO, we address the reference model issue by revisiting the context of preference optimization and investigating whether the reference model can be excluded while still retaining the unlearning benefits provided by NPO. Our idea parallels how NPO was originally inspired by DPO (Rafailov et al., 2024). We adopt SimPO, a reference-free alternative to DPO,

Figure 3: Experimental evidence of ineffective weight smoothing and over-unlearning for NPO on TOFU with 5% forget set size: (a) NPO’s gradient weights (\(w_{\bm{\theta}}\)) at epoch 1 vs. response length \(|y|\). (b) Trajectory of \(w_{\bm{\theta}}\) for NPO over unlearning epochs, visualized using box plots to represent the distribution of gradient weights across forget samples for each epoch. (c)-(d) Forget quality and model utility of NPO across epochs.

Figure 2: Truth ratio distribution of top 50% shortest-length forget data points and the other 50% longer-length data for Retrain and NPO on TOFU with forget size 5%.

as the optimization framework for unlearning, leading to the SimNPO method. The _key difference_ between SimPO and DPO lies in their reward formulation for preference optimization. In DPO, the reward formulation is given by the comparison with the reference model, _i.e._, \(\beta\log(\pi_{\bm{\theta}}(y|x)/\pi_{\mathrm{ref}}(y|x))\). This formulation was used by NPO. In contrast, SimPO takes a _reference-free but length-normalized_ reward formulation: \((\beta/|y|)\log\pi_{\bm{\theta}}(y|x)\), where \(|y|\) denotes the response length.

Taking the inspiration of SimPO, we can mitigate the reference model bias in NPO by replacing its reward formulation \(\beta\log(\pi_{\bm{\theta}}(y|x)/\pi_{\mathrm{ref}}(y|x))\) in (2) with the SimPO-based reward formulation \((\beta/|y|)\log(\pi_{\bm{\theta}}(y|x))\). This modification transforms (2) into the **SimNPO loss**:

\[\ell_{\mathrm{SimNPO}}(\bm{\theta})=\mathbb{E}_{(x,y)\in\mathcal{D}_{t}}\left[ -\frac{2}{\beta}\log\sigma\left(-\frac{\beta}{|y|}\log\pi_{\bm{\theta}}(y|x)- \gamma\right)\right],\] (4)

where \(\gamma\geq 0\) is the reward margin parameter, inherited from SimPO, which defines the margin of preference for a desired response over a dispreferred one. However, unless otherwise specified, we set \(\gamma=0\) to align with the NPO loss (2). This is also desired because \(\gamma\) introduces a constant shift to the prediction loss \(-(\beta/|y|)\log\pi_{\bm{\theta}}(y|x)\). Consequently, a larger \(\gamma\) requires greater compensation to further suppress token prediction, enforcing a stricter unlearning condition. This can accelerate the utility drop during unlearning. See Fig. 13 for an empirical justification. The SimNPO loss (4), when integrated with the regularized optimization in (1), forms the SimNPO method.

**Insights into SimNPO.** Similar to NPO, the SimNPO loss (4) is bounded from below, with a minimum value of \(0\). Approaching this minimum drives the unlearning. However, the _key distinction_ of SimNPO from NPO is its forget data-aware, length-normalized reward formulation, \((\beta/|y|)\log\pi_{\bm{\theta}}(y|x)\) in (4). This eliminates the reference model bias and results in an improved gradient smoothing scheme. Specifically, the gradient of the SimNPO loss (with \(\gamma=0\)) yields (as derived in Appendix A):

\[\nabla_{\bm{\theta}}\ell_{\mathrm{SimNPO}}(\bm{\theta})=\mathbb{E}_{(x,y)\in \mathcal{D}_{t}}\left[\underbrace{\frac{2(\pi_{\bm{\theta}}(y|x))^{\beta/|y|} }{1+(\pi_{\bm{\theta}}(y|x))^{\beta/|y|}}\cdot\frac{1}{|y|}}_{:=\bm{w}_{\bm{ \theta}}^{*}(x,y)}\cdot\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(y|x)\right].\] (5)

Similar to NPO in (3), the gradient in (5) can be divided into two components: weight smoothing (\(w_{\bm{\theta}}^{\prime}\)) and GA. However, in SimNPO, the weight smoothing is _no longer influenced by the reference model and is instead normalized by the length_\(|y|\). This introduces two key advantages (a)-(b) below, in response to NPO's limitations (L1)-(L2).

(a) SimNPO addresses the biased allocation of unlearning power by using the (data-specific) response length as a guide. For example, when \(|y|\) is large, less optimization power is allocated as long-sequence forget data could be closer to the unlearning boundary and require less intervention (Fig. 2). In the extreme case where \(\beta\to 0\), the SimNPO gradient reduces to a _weighted GA_: \(\nabla_{\bm{\theta}}\ell_{\mathrm{SimNPO}}(\bm{\theta})\to\mathbb{E}_{(x,y) \in\mathcal{D}_{t}}[1/|y|\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(y|x)]\). This is different from NPO, which becomes GA as \(\beta\to 0\). **Fig. 4** empirically demonstrates the advantage of length normalization in SimNPO on TOFU, comparing the forget quality and model utility of SimNPO with other baselines and Retrain. As shown, SimNPO outperforms NPO in both forget quality and model utility, coming closest to Retrain. Even in the special case where \(\beta=0\) (_i.e._, Weighted-GradDiff), the length normalization provides benefits over the vanilla GradDiff baseline.

(b) In addition, the reference-free, length-normalized weight smoothing prevents early-stage ineffectiveness during unlearning. It can be easily shown from (5) that \(w_{\bm{\theta}}^{\prime}(x,y)<2/|y|\), with the distribution of weights \(w_{\bm{\theta}}^{\prime}(x,y)\) depending on the specific forget data samples. This contrasts with NPO, where the weight distribution concentrated around \(w_{\bm{\theta}}(x,y)\approx 1\) during the early unlearning stage, as shown in Fig. 3-(a). Furthermore, **Fig. 5** provides a detailed comparison between the gradient weights of SimNPO and NPO. As shown, SimNPO exhibits a much stronger correlation with the response length \(|y|\) during the first two unlearning epochs, prioritizing short-length forget data that are initially harder to forget. At later epochs, the gradient weights become more uniform, reflecting that SimNPO can then treat different forget data with even optimization power. This trend is different

Figure 4: Forget quality vs. model utility on TOFU with forget set size of 5%. Weighted-GradDiff (W-GradDiff) is the variant of SimNPO at \(\beta=0\).

from NPO, which assigns more uniform gradient weights early on and only accounts for data-specific difficulty when \(w_{\bm{\theta}}(x,y)\) decreases in the later stages of unlearning.

**Further analyses via a mixture of Markov chains.** In addition to the above insights, we further validate SimNPO's advantages to overcome NPO's limitations (L1)-(L2) (Sec. 4) using a synthetic setup. For ease of controlling the unlearning difficulties of different forget data points, we consider the problem of unlearning on a mixture of Markov chains with a state space of size 10 (\(s=1,\ldots,10\)). The _retain distribution_ consists of Markov chains that transition uniformly among states \(\{1,2,3\}\). The _forget distribution_ is a mixture of two components: _Forget1_, where the chains transition uniformly among \(\{4,5,6\}\), and _Forget2_, where they move uniformly among \(\{7,8,9\}\). A small leakage probability allows the chains to transition outside their designated states occasionally, including state \(10\), which is not a designated state for any of the chains. We generate 10,000 samples for the retain distribution and 5,000 samples each for Forget1 and Forget2. A GPT-2 model is pretrained on these samples and serves as the initial model. We apply NPO and SimNPO to unlearn the forget distributions. Forget and retain performance is evaluated using the KL-divergence between predicted and true transition probabilities of the Markov chains. See Appendix B for details. We present our results in **Fig. 6** and summarize the insights below.

_In response to (L1), SimNPO is easier to unlearn short responses than NPO._ To validate this, we set the retain distribution and Forget1 with a sequence length of 20, while Forget2 is assigned a shorter sequence length of 5, representing a mix of long and short responses. **Fig. 6 (left)** shows that NPO exhibits a worse tradeoff between retain distance and forget quality on short responses (_i.e._, Forget2) compared with SimNPO. That is, to achieve the same forget quality on Forget2 as the retrained model (with forget quality \(0.44\)), NPO incurs a higher retain distance than SimNPO. As a result, NPO has an overall larger retain distance when unlearning the entire Forget distribution. In contrast, SimNPO shows more consistent performance across Forget1 and Forget2, with less variance in its tradeoff.

_In response to (L2), SimNPO unlearns already unlearned data less aggressively than NPO._ In the second case, we set the retain distribution, Forget1 and Forget2 all with a sequence length of 20. However, we exclude Forget2 during pretraining. This setup simulates a scenario where the initial model (_i.e._, the reference model in NPO) has already unlearned part of the forget dataset (_i.e._, Forget2).

**Fig. 6 (right)** shows that NPO unlearns Forget2 faster than SimNPO, even though Forget2 was already unlearned. However, NPO performs worse on Forget1 than SimNPO, likely due to overlearning Forget2, thereby reducing the overall model utility.

Figure 5: Gradient weight smoothing of NPO (\(w_{\bm{\theta}}\)) and SimNPO (\(w^{\prime}_{\bm{\theta}}\)) vs. forget data response length \(|y|\) across different epochs (1, 2, 3, and 10) on TOFU with forget set size of 5%. Each point represents a sample. The Pearson correlation in the upper right corner indicates the relationship between gradient weight smoothing and response length. The SimNPO’s weights \(w^{\prime}_{\bm{\theta}}\) have been rescaled (by \(\times 10\)) for ease of visualization.

Figure 6: Tradeoffs between forget quality (higher \(\uparrow\) is better) and retain distance (lower \(\downarrow\) is better) along the unlearning path of NPO and SimNPO in the synthetic experiments. Left: Forget1 and Forget2 have different sequence lengths. Right: unlearning from an initial model that has not seen Forget2. The symbols \((\star,\bullet)\) near the \(y\)-axis of both figures indicate the performance of the retrained model on Forget1 and Forget2, respectively.

## 6 Other Experiments

In what follows, we present more experiment results to demonstrate the effectiveness of SimNPO. See detailed experiment setups and hyperparameter selections in Appendix C.1-C.2.

**Performance on TOFU.** In **Table 2**, we present the unlearning performance of SimNPO and its various baselines on TOFU, covering both effectiveness metrics and utility metrics as shown in Table 1. Recall that 'Original' refers to the model performance prior to unlearning, serving as the _lower bound_ for unlearning effectiveness. In contrast, 'Retrain' refers to the model retrained excluding the forget set influence, serving as the _upper bound_ for unlearning effectiveness. 'FQ' stands for forget quality, and 'MU' represents model utility. These two metrics serve as the primary performance indicators for LLM unlearning on TOFU. SimNPO outperforms NPO in both FQ and MU, and is the closest approximate unlearning method to Retrain. Except for NPO, the other unlearning baselines (GA, GradDiff, and IDK) are not effective, as implied by their FQ values being smaller than \(0.01\), where FQ indicates the \(p\)-value for rejecting the indistinguishability between the unlearned model and Retrain on TOFU. In **Table 4** of **Appendix** D, we also provide examples of model responses after unlearning using SimNPO, Retrain, and NPO, along with label to degenerate. We observe that, in some cases (_e.g._, responses against Q1 and Q2 in Table A4), the NPO-unlearned model generates _repeated texts_ in response. While this repetition does not reveal the information intended for unlearning, it negatively impacts model utility and differs noticeably from Retrain's behavior. In contrast, SimNPO produces unlearning responses more closely aligned with those generated by Retrain. We conduct a follow-up study of Fig. 2 to delve deeper into the comparison between SimNPO and NPO across forget data with varying response lengths. **Fig. A4 in Appendix C.3** shows that SimNPO's improvement over NPO is most evident in forgetting short-length data, aligning with the NPO's limitation (L1) as illustrated in Sec. 4. We also find that SimNPO is more efficient than NPO in Appendix C.3..

**Additional experiments for SimNPO.** We further evaluated SimNPO on the MUSE and WMDP datasets and assessed its robustness using the relearning attacks, as described in Appendix C.3.

## 7 Conclusion

We revisited the current unlearning optimization framework, negative preference optimization (NPO), and identified its reference model bias issue, which compromises unlearning effectiveness, particularly for forget data of varying difficulty. To address this, we introduced SimNPO, a simple yet effective framework that eliminates reliance on a reference model by leveraging simple preference optimization. We provided deep insights into SimNPO's advantages through both synthetic data analysis and evaluations on existing unlearning benchmarks such as TOFU, MUSE, WMDP, and relearning attacks. In future work, we will further investigate the limitations of SimNPO and enhance it for tasks involving model capability removal. See further discussions in Appendix E-F.

## References

* Azar et al. [2024] Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. A general theoretical paradigm to understand learning from human preferences. In _International Conference on Artificial Intelligence and Statistics_, pp. 4447-4455. PMLR, 2024.
* Barrett et al. [2023] Clark Barrett, Brad Boyd, Elie Bursztein, Nicholas Carlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury, Mihai Christodorescu, Anupam Datta, Soheil Feizi, et al. Identifying and mitigating the security risks of generative ai. _Foundations and Trends(r) in Privacy and Security_, 6(1):1-52, 2023.
* Blanco-Justicia et al. [2024] Alberto Blanco-Justicia, Najeeb Jebreel, Benet Manzanares, David Sanchez, Josep Domingo-Ferrer, Guillem Collell, and Kuan Eeik Tan. Digital forgetting in large language models: A survey of unlearning methods. _arXiv preprint arXiv:2404.02062_, 2024.
* Bourtoule et al. [2021] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In _2021 IEEE Symposium on Security and Privacy (SP)_, pp. 141-159. IEEE, 2021.
* Cao and Yang [2015] Yinzhi Cao and Junfeng Yang. Towards making systems forget with machine unlearning. In _2015 IEEE symposium on security and privacy_, pp. 463-480. IEEE, 2015.
* Christiano et al. [2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.
* Eldan and Russinovich [2023] Ronen Eldan and Mark Russinovich. Who's harry potter? approximate unlearning in llms, 2023.
* Ethayarajh et al. [2024] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. _arXiv preprint arXiv:2402.01306_, 2024.
* Fan et al. [2024] Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu. Salun: Empowering machine unlearning via gradient-based weight saliency in both image classification and generation. In _International Conference on Learning Representations_, 2024.
* Gandikota et al. [2023] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 2426-2436, 2023.
* Hong et al. [2024] Jiwoo Hong, Noah Lee, and James Thorne. Reference-free monolithic preference optimization with odds ratio. _arXiv preprint arXiv:2403.07691_, 2024.
* Hoofnagle et al. [2019] Chris Jay Hoofnagle, Bart van der Sloot, and Frederik Zuiderveen Borgesius. The european union general data protection regulation: what it is and what it means. _Information & Communications Technology Law_, 28(1):65-98, 2019.
* Hu et al. [2024] Shengyuan Hu, Yiwei Fu, Zhiwei Steven Wu, and Virginia Smith. Jogging the memory of unlearned model through targeted relearning attack. _arXiv preprint arXiv:2406.13356_, 2024.
* Huang et al. [2024] Yue Huang, Lichao Sun, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, et al. Position: TrustLLM: Trustworthiness in large language models. In _Proceedings of the 41st International Conference on Machine Learning_, volume 235 of _Proceedings of Machine Learning Research_, pp. 20166-20270, 21-27 Jul 2024.
* Ilharco et al. [2022] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. _arXiv preprint arXiv:2212.04089_, 2022.
* Jang et al. [2022] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. _arXiv preprint arXiv:2210.01504_, 2022.
* Jang et al. [2021]* Ji et al. [2024] Jiabao Ji, Yujian Liu, Yang Zhang, Gaowen Liu, Ramana Rao Kompella, Sijia Liu, and Shiyu Chang. Reversing the forget-retain objectives: An efficient llm unlearning framework from logit difference. _arXiv preprint arXiv:2406.08607_, 2024.
* Jia et al. [2023] Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, and Sijia Liu. Model sparsity can simplify machine unlearning. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* Jia et al. [2024] Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, and Sijia Liu. Soul: Unlocking the power of second-order optimization for llm unlearning. _arXiv preprint arXiv:2404.18239_, 2024.
* Kadhe et al. [2024] Swanand Ravindra Kadhe, Farhan Ahmed, Dennis Wei, Nathalie Baracaldo, and Inkit Padhi. Split, unlearn, merge: Leveraging data attributes for more effective unlearning in lms. _arXiv preprint arXiv:2406.11780_, 2024.
* Kurmanji et al. [2024] Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou. Towards unbounded machine unlearning. _Advances in neural information processing systems_, 36, 2024.
* Li et al. [2024] Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et al. The wmdp benchmark: Measuring and reducing malicious use with unlearning. _arXiv preprint arXiv:2403.03218_, 2024.
* Liu et al. [2022] Bo Liu, Qiang Liu, and Peter Stone. Continual learning and private unlearning. In _Conference on Lifelong Learning Agents_, pp. 243-254. PMLR, 2022.
* Liu et al. [2024a] Chris Yuhao Liu, Yaxuan Wang, Jeffrey Flanigan, and Yang Liu. Large language model unlearning via embedding-corrupted prompts. _arXiv preprint arXiv:2406.07933_, 2024a.
* Liu et al. [2024b] Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R Varshney, et al. Rethinking machine unlearning for large language models. _arXiv preprint arXiv:2402.08787_, 2024b.
* Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* Lu et al. [2022] Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi. Quark: Controllable text generation with reinforced unlearning. _Advances in neural information processing systems_, 35:27591-27609, 2022.
* Lynch et al. [2024] Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. Eight methods to evaluate robust unlearning in lms. _arXiv preprint arXiv:2402.16835_, 2024.
* Maini et al. [2024] Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, and J. Zico Kolter. Tofu: A task of fictitious unlearning for lms, 2024.
* Meng et al. [2024] Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a reference-free reward. _arXiv preprint arXiv:2405.14734_, 2024.
* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.
* Patil et al. [2024] Vaidehi Patil, Peter Hase, and Mohit Bansal. Can sensitive information be deleted from llms? objectives for defending against extraction attacks. _ICLR_, 2024.
* Pawelczyk et al. [2023] Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. In-context unlearning: Language models as few shot unlearners. _arXiv preprint arXiv:2310.07579_, 2023.
* Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* Radford et al. [2020]* [540] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _Advances in Neural Information Processing Systems_, 36, 2024.
* [543] Jeffrey Rosen. The right to be forgotten. _Stan. L. Rev. Online_, 64:88, 2011.
* [545] Michael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, and Yelong Shen. Efficient rhlf: Reducing the memory usage of ppo. _arXiv preprint arXiv:2309.00754_, 2023.
* [547] Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C Lipton, and J Zico Kolter. Rethinking llm memorization through the lens of adversarial compression. _arXiv preprint arXiv:2404.15146_, 2024.
* [550] Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. _arXiv preprint arXiv:2310.16789_, 2023.
* [554] Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke Zettlemoyer, Noah A Smith, and Chiyuan Zhang. Muse: Machine unlearning six-way evaluation for language models. _arXiv preprint arXiv:2407.06460_, 2024.
* [557] Pratiksha Thaker, Yash Maurya, and Virginia Smith. Guardrail baselines for unlearning in llms. _arXiv preprint arXiv:2403.03329_, 2024.
* [560] Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. Unrolling sgd: Understanding factors influencing machine unlearning. In _2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P)_, pp. 303-319. IEEE, 2022.
* [561] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: A comprehensive assessment of trustworthiness in gpt models. In _NeurIPS_, 2023.
* [562] Yu Wang, Ruihan Wu, Zexue He, Xiusi Chen, and Julian McAuley. Large scale knowledge washing. _arXiv preprint arXiv:2405.16720_, 2024.
* [563] Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning of features and labels. _arXiv preprint arXiv:2108.11577_, 2021.
* [571] Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong. Depn: Detecting and editing privacy neurons in pretrained language models. _arXiv preprint arXiv:2310.20138_, 2023.
* [574] Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. _arXiv preprint arXiv:2310.10683_, 2023.
* [575] Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language models by partitioning gradients. In _Findings of the Association for Computational Linguistics: ACL 2023_, pp. 6032-6048, 2023.
* [580] Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. Rrhf: Rank responses to align language models with human feedback. _Advances in Neural Information Processing Systems_, 36, 2024.
* [583] Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catastrophic collapse to effective unlearning. _arXiv preprint arXiv:2404.05868_, 2024.
* [586] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood calibration with human feedback. _arXiv preprint arXiv:2305.10425_, 2023.
* [587] Rui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Yuhao Zhou, et al. Secrets of rhlf in large language models part i: Ppo. _arXiv preprint arXiv:2307.04964_, 2023.
* [591] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1909.08593_, 2019.
* [592]

## Appendix A Gradient Analysis of SimNPO

Following is the detailed derivation of (5). First, let \(\mathrm{R}=\frac{\log\pi_{\bm{\theta}}(y|x)+\gamma|y|/\beta}{|y|}\). We then have the following steps:

\[\nabla_{\bm{\theta}}\ell_{\mathrm{SimNPO}}(\bm{\theta}) =\mathbb{E}_{(x,y)\in\mathcal{D}_{\mathrm{f}}}\nabla_{\bm{\theta }}\left[-\frac{2}{\beta}\log\sigma(-\beta\mathrm{R})\right]\] (A1) \[=\mathbb{E}_{(x,y)\in\mathcal{D}_{\mathrm{f}}}\nabla_{\bm{\theta }}\left[\frac{2}{\beta}\log\sigma(1+\exp(\beta\mathrm{R}))\right]\] (A2) \[=\mathbb{E}_{(x,y)\in\mathcal{D}_{\mathrm{f}}}\left[\frac{2}{ \beta}\cdot\frac{\beta\exp(\beta\mathrm{R})}{1+\exp(\beta\mathrm{R})}\cdot \nabla_{\bm{\theta}}\mathrm{R}\right]\] (A3) \[=\mathbb{E}_{(x,y)\in\mathcal{D}_{\mathrm{f}}}\left[\frac{2\exp( \beta\log\frac{\pi_{\bm{\theta}}(y|x)+\gamma|y|/\beta}{|y|})}{1+\exp(\beta \log\frac{\pi_{\bm{\theta}}(y|x)+\gamma|y|/\beta}{|y|})}\cdot\frac{1}{|y|} \cdot\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(y|x)\right]\] (A4)

When \(\gamma=0\), the gradient simplifies to the following, which matches (5):

\[\nabla_{\bm{\theta}}\ell_{\mathrm{SimNPO}}(\bm{\theta}) =\mathbb{E}_{(x,y)\in\mathcal{D}_{\mathrm{f}}}\left[\frac{2\exp( \frac{\beta\log\pi_{\bm{\theta}}(y|x)}{|y|})}{1+\exp(\frac{\beta\log\pi_{\bm{ \theta}}(y|x)}{|y|})}\cdot\frac{1}{|y|}\cdot\nabla_{\bm{\theta}}\log\pi_{\bm{ \theta}}(y|x)\right]\] (A5) \[=\mathbb{E}_{(x,y)\in\mathcal{D}_{\mathrm{f}}}\left[\frac{2(\pi_{ \bm{\theta}}(y|x))^{\beta/|y|}}{1+(\pi_{\bm{\theta}}(y|x))^{\beta/|y|}}\cdot \frac{1}{|y|}\cdot\nabla_{\bm{\theta}}\log\pi_{\bm{\theta}}(y|x)\right]\] (A6)

## Appendix B Additional Details on the Synthetic Study

Synthetic experiment setup.In the synthetic experiment, we study the unlearning problem in a scenario where the data are generated from a mixture of Markov chains. Namely, we assume the Markov chains have a shared state space of size \(10\) (denoted by \(s=1,2,\ldots,10\)), and the retain distribution and the forget distribution have the formulas as follows:

* **Retain distribution**: Markov chain with initial distribution \(\pi_{r}\in\mathbb{R}^{10}\) and transition matrix \(T_{r}\in\mathbb{R}^{10\times 10}\), where \[\pi_{r,j} =\frac{1-\epsilon}{3}\quad\text{for }j\leq 3, \pi_{r,j} =\frac{\epsilon}{7}\quad\text{for }j\geq 4.\] \[T_{r,i\cdot} =\pi_{r}\quad\text{for }i\leq 3, T_{r,i} =0.1\cdot\mathbf{1}_{10}\quad\text{for }i\geq 4.\]
* **Forget distribution**: a mixture of two Markov chains (denoted by Forget1 and Forget2) with equal probability. Let \((\pi_{f_{1}},T_{f_{1}})\) and \((\pi_{f_{2}},T_{f_{2}})\) denote the initial distribution and transition matrix for Forget1 and Forget2. We assume \[\pi_{f_{1},j} =\frac{1-\epsilon}{3}\quad\text{for }j\in\{4,5,6\}, \pi_{f_{1},j} =\frac{\epsilon}{7}\quad\text{for }j\notin\{4,5,6\},\] \[T_{f_{1},i\cdot} =\pi_{f_{1}}\quad\text{for }i\in\{4,5,6\}, T_{f_{1},i\cdot} =0.1\cdot\mathbf{1}_{10}\quad\text{for }i\notin\{4,5,6\},\] and \[\pi_{f_{2},j} =\frac{1-\epsilon}{3}\quad\text{for }j\in\{7,8,9\}, \pi_{f_{2},j} =\frac{\epsilon}{7}\quad\text{for }j\notin\{7,8,9\},\] \Evaluation.We evaluate the model performance using Forget Quality (higher \(\uparrow\) is better) and Retain Loss (lower \(\downarrow\) is better), which are the average KL divergence between the predicted probabilities of the model and the true transition probabilities of the Markov chains, on the forget (Forget1 or Forget2) and the retain test data, respectively.

Unlearning.Starting from the initial model, we run NPO and SimNPO for \(50\) iterations using a batch size of \(4\) on the forget dataset. We choose AdamW for optimization with a learning rate of \(\eta=0.0005\). The hyperparameter \(\beta\) in both NPO and SimNPO is selected via grid search to optimize the tradeoff between forget quality and retain loss.

Choise of hyperparameters.In the first experiment (cf. **Fig. 6 left**), we set the hyperparameters \(\beta_{\mathrm{NPO}}=0.2,\beta_{\mathrm{SimNPO}}=4\), the retain sample length \(L_{r}=20\), and the Forget1 and Forget2 sample lengths \(L_{f_{1}}=20,L_{f_{2}}=5\). In the second experiment (cf. **Fig. 6 right**), we choose \(\beta_{\mathrm{NPO}}=1.0,\beta_{\mathrm{SimNPO}}=4\), the retain sample length \(L_{r}=20\), and the Forget1 and Forget2 sample lengths \(L_{f_{1}}=20,L_{f_{2}}=20\).

## Appendix C Experiment Details and Results

### Experiment Setups

All experiments are conducted on 8 NVIDIA A6000 GPU cards in a single node.

Datasets, tasks, and models.Our experiments cover unlearning tasks across three benchmark datasets: TOFU (Maini et al., 2024), MUSE (Shi et al., 2024), and WMDP (Li et al., 2024), as summarized in Table 1. For TOFU, we focus on two unlearning scenarios, termed 'Forget05' and 'Forget10', which refer to forget set sizes of 5% and 10%, respectively. In MUSE, we also explore two unlearning scenarios: forgetting the Harry Potter books (termed 'Books') and news articles (termed 'News'), respectively. WMDP, on the other hand, is designed for knowledge-based unlearning, with the forget texts representing hazardous knowledge in biosecurity and cybersecurity. The LLM models used for each unlearning benchmark are listed in Table 1.

LLM unlearning methods and evaluation.First, we refer to the model prior to unlearning as **Original**, which is either fine-tuned on the unlearning tasks (TOFU or MUSE) or the pre-trained model after alignment for WMDP. Starting from the original model, we then apply the following unlearning methods to a given forget set and/or retain set to achieve the unlearning objective, as outlined in (1). Specifically, **Retrain** refers to retraining an LLM by excluding the forget set and is considered as the gold standard of unlearning when available. Retrain is provided in both the TOFU and MUSE benchmarks. As introduced in Sec. 3, we also include **GA** (gradient ascent) and **GradDiff** (the retain-regularized GA variant) as unlearning baseline methods, following the implementations in TOFU and MUSE benchmarks. For other baseline methods such as the rejection-based unlearning method (**IDK**) in TOFU, and the **Task Vector** unlearning method in MUSE, we adhere to the original implementations specified in their respective benchmarks. **NPO** with the retain regularization in (1) serves as the primary baseline.

To implement the proposed method **SimNPO**, we adopt a setting similar to NPO but adjust the temperature parameter \(\beta\). Due to the presence of length normalization in (4), a larger value for \(\beta\) is preferred compared to that in NPO. See the specific choices in Appendix C.2.

To assess unlearning effectiveness and model utility, we use the evaluation metrics summarized in Table 1 under each unlearning benchmark. In addition, we evaluate the robustness of an unlearned model using relearning-based attacks (Hu et al., 2024), which aim to recover the forgotten information by fine-tuning the unlearned models on a small subset of the forget set after unlearning. We select \(20\%\) of the original TOFU forget05 set as the relearning set over three epochs.

**TOFU Experiment Setup** For all experiments, we use a linear warm-up learning rate during the first epoch, followed by a linearly decaying learning rate in the remaining epochs. We initialize the process with LLaMA-2 7B and fine-tune the model on TOFU for 5 epochs with a batch size of 32 and a learning rate of \(10^{-5}\) to obtain the original model. For Forget05, NPO is trained for up to 20 epochs with a learning rate of \(10^{-5}\) to obtain the best-performing model. We conducted a grid search for \(\beta\) in the range of [0.05, 0.2] and for \(\lambda\) in the range of [0.5, 1.5]. SimNPO is trained for 10 epochswith a learning rate of \(10^{-5}\). The parameter \(\beta\) is grid-searched over the range [1.5, 3.5], \(\gamma\) is searched between [0.0, 2.0] with the default choice \(\gamma=0\), and \(\lambda\) is explored within the range [0.05, 0.25]. For Forget10, NPO is trained for 10 epochs with a learning rate of \(10^{-5}\). We conducted a grid search for \(\beta\) in the range of [0.05, 0.2] and for \(\lambda\) in the range of [0.5, 1.5]. SimNPO is trained for 10 epochs with a learning rate of \(10^{-5}\). The parameter \(\beta\) is tuned using a grid search within the range [2.5, 5.5], \(\gamma\) is grid-searched between [0.0, 2.0], and \(\lambda\) is grid-searched within [0.05, 0.25]. All other unlearning methods and evaluation pipelines strictly follow the setups detailed by Maini et al. (2024) and Zhang et al. (2024).

**MUSE Experiment Setup** For News, we use LLaMA-2 7B fine-tuned on BBC news articles as the original model. For Books, we use ICLM 7B fine-tuned on the Harry Potter books as the original model. The original models for both Books and News can be directly obtained from benchmark. For SimNPO, we trained for 10 epochs with a learning rate of \(10^{-5}\). We performed a grid search for \(\beta\) in the range of [0.5, 1.0], for \(\lambda\) in the range of [0.05, 0.25], and for \(\gamma\) in the range of [0.0, 2.0] on both the Books and News. The hyperparameters for other unlearning methods and the evaluation pipelines strictly follow the setup detailed by Shi et al. (2024). We measured the performance after each unlearning epoch and selected the optimal one as the final model.

**WMDP Experiment Setup** For WMDP (Li et al., 2024), we use Zephyr-7B-beta, provided as the origin model in the benchmark. A forget set consisting of plain texts related to biosecurity/cybersecurity knowledge and an unrelated text retain set are used. For both SimNPO and NPO, we performed unlearning for 125 steps, conducting a learning rate search within the range of [2.5\(\times 10^{-6}\), 5\(\times 10^{-6}\)] and a grid search for \(\beta\) in the range of [0.05, 7.5], with \(\lambda\) fixed at 5.0.

### Ablation Studies on SimNPO's Hyperparameter Selection

As shown in (4), \(\beta\) and \(\gamma\) are the two hyperparameters that control the unlearning effectiveness and utility preservation of SimNPO. Similar to NPO, \(\beta\) is a temperature hyperparameter used to regulate the intensity of unlearning but normalized by the response length \(|y|\) in SimNPO. As \(\beta\to 0\), SimNPO approaches weighted GA in Fig. 4. \(\gamma\) is the reward margin parameter from SimPO, which introduces a constant shift to the (per-sample) prediction loss \(-(\beta/|y|)\log\pi_{\theta}(y|x)\) in SimNPO. Consequently, a larger \(\gamma\) imposes a stricter unlearning margin, which could further suppress the model utility.

**Fig. A1-(a)** and **Fig. A1-(b)** illustrate the forget quality and model utility of SimNPO under various values of \(\beta\) and \(\gamma\) on TOFU forget05. The results show that when \(\beta\) is too small or \(\gamma\) is too large, forget quality tends to decrease towards zero. Additionally, for a fixed \(\beta\), increasing \(\gamma\) leads to lower model utility. Notably, setting \(\gamma=0\) consistently yields the best balance between unlearning performance and utility preservation across different \(\beta\) values, which supports our choice of \(\gamma=0\) in SimNPO.

### Additional Experiment Results

**Performance on MUSE and WMDP.** Table A1 compares the performance of SimNPO with baseline methods, including Task Vector (Shi et al., 2024; Ilharco et al., 2022), on both the MUSE News and Books datasets. The evaluation metrics are summarized in Table 1, with PrivLeak serving as the primary metric to indicate the gap with Retrain. As we can see, SimNPO consistently achieves PrivLeak values closest to \(0\) for both News (\(11.90\)) and Books (\(-19.82\)) compared to other unlearning baselines, suggesting that it is most aligned with complete forget data removal, as defined in MUSE (Shi et al., 2024). Compared to Task Vector, SimNPO shows a slight utility drop, which is expected since both SimNPO and NPO are divergence-driven unlearning methods, with gradient weight 

[MISSING_PAGE_FAIL:15]

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_EMPTY:17]

## Appendix E Limitations

While SimNPO mitigates the reference model bias present in NPO and improves gradient weight smoothing to better adjust divergence speed based on the varying unlearning difficulties of forget data samples, both frameworks still rely on promoting divergence to achieve unlearning. This reliance inevitably results in some degree of utility loss. This limitation becomes especially evident in knowledge unlearning or model capability removal scenarios, such as in the WMDP unlearning benchmark. Consequently, SimNPO has yet to fully resolve the challenge of balancing unlearning effectiveness with model utility. Addressing this problem will require further investigation into the limitations of both NPO and SimNPO, as well as the development of new strategies to strike an optimal tradeoff between unlearning and utility retention.

## Appendix F Broader Impacts

On the positive side, we have demonstrated the utility of preference optimization in machine unlearning. This connection enables more efficient unlearning operations in LLMs, improving data privacy protections and supporting compliance with regulatory requirements. Additionally, given the relationship between preference optimization and model editing, our work encourages further exploration in these areas, contributing to the development of models that are easier to customize and become safer to deploy. On the negative side, the methods we developed could be misused to selectively erase "essential" (rather than "unwanted") concepts or knowledge, raising ethical and legal concerns. To mitigate this risk, it is essential to ensure that unlearning applications adhere to strict ethical guidelines to prevent misuse. We hope our research fosters the development of safe, reliable, and human-aligned LLMs.