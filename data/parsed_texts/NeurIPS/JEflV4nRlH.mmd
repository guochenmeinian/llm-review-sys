# What Makes and Breaks Safety Fine-tuning?

A Mechanistic Study

 Samyak Jain

Five AI Ltd.

&Ekdeep Singh Lubana

University of Michigan &

CBS, Harvard University

&Kemal Oksuz

Five AI Ltd.

&Tom Joy

Five AI Ltd.

&Philip H.S. Torr

University of Oxford

&Amartya Sanyal

Max Planck Institute for Intelligent Systems &

University of Copenhagen

&Puneet K. Dokania

Five AI Ltd. &

University of Oxford

###### Abstract

Safety fine-tuning helps align Large Language Models (LLMs) with human preferences for their safe deployment. To better understand the underlying factors that make models safe via safety fine-tuning, we design a synthetic data generation framework that captures salient aspects of an unsafe input by modeling the interaction between the task the model is asked to perform (e.g., "design") versus the specific concepts the task is asked to be performed upon (e.g., a "cycle" vs. a "bomb"). Using this, we investigate three well-known safety fine-tuning methods--supervised safety fine-tuning, direct preference optimization, and unlearning--and provide significant evidence demonstrating that these methods minimally transform MLP weights to _specifically_ align unsafe inputs into its weights' null space. This yields a clustering of inputs based on whether the model deems them safe or not. Correspondingly, when an adversarial input (e.g., a jailbreak) is provided, its activations are closer to safer samples, leading to the model processing such an input as if it were safe. Code is available at https://github.com/fiveai/understanding_safety_finetuning.

## 1 Introduction

Large language models (LLMs) are commonly trained via a combination of pre-training on a large corpus and instruction fine-tuning, wherein the model is supervised to follow instructions (Driess et al., 2023; Team et al., 2023; Qin et al., 2024). While pre-training enables a model to learn different capabilities (Wei et al., 2022; Bubeck et al., 2023), instruction fine-tuning enables use of open-ended, generic inputs to control said capabilities (Ouyang et al., 2022; Wei et al., 2021; Sanh et al., 2021; Bai et al., 2022; Raffel et al., 2020). Since this pipeline does not restrict what tasks the model can be used for, potential misuse is left feasible under its purview (Bengio et al., 2023; Anwar et al., 2024): as long as an instruction can be formulated and the model possesses the relevant capabilities to perform the instructed task, it will strive to perform it. To prevent such misuse, safety fine-tuning is used as an additional training phase for LLMs, in which the model is supervised to prioritize generation of outputs deemed safe as per human preferences. Popular approaches for safety fine-tuning include: (i) supervised safety fine-tuning (SSFT) (Ouyang et al., 2022); (ii) reinforcement learning with human feedback (RLHF) (Christiano et al., 2017; Ouyang et al., 2022; Bai et al., 2022; Stiennon et al., 2020) and its recent renditions that avoid use of an explicit reward model, e.g., DPO (Rafailov et al., 2023); and (iii) machine unlearning (Liu et al., 2024). Despite immense use of these protocols to enablesystem release (Chao et al., 2024; Sun et al., 2024), several recent works show that safety fine-tuned models continue to produce unsafe generations when prompted via adversarially designed inputs, e.g., jailbreaks (Andriushchenko et al., 2024; Chao et al., 2023; Zou et al., 2023; Carlini et al., 2023).

In this work, our goal is to understand: (i) _what is the safety mechanism learned by the model via safety fine-tuning?_ and (ii) _how are jailbreak and adversarial attacks able to bypass this mechanism?_ While a few contemporary papers have investigated the mechanisms of safety fine-tuning, e.g., showing that such methods perform minimal alterations to model parameters that nevertheless can change its behavior (Jain et al., 2023; Lee et al., 2024; Prakash et al., 2024; Wei et al., 2024), tying this analysis back with lack of robustness of safety fine-tuning is lacking in existing literature. We aim to fill this gap by designing a well-defined synthetic data generating process wherein an input is modeled as a function of the task the model is expected to perform (e.g., "design"), and the specific concept the task is to be performed upon (e.g., "cycle" versus "bomb"). This separation helps us delineate how the model distinguishes between safe versus unsafe inputs, while allowing us to model different forms of jailbreak attacks grounded in the formalization of Wei et al. (2023). Overall, our contributions and observations can be summarized as follows.

* **Systematic setup to study safety fine-tuning and jailbreaks.** We introduce a novel synthetic data generation framework that allows _controlled_ generation of data for safety fine-tuning, jailbreaks, and adversarial attacks. We make careful design choices to adhere to the properties of natural language instructions and the jailbreaks taxonomy of Wei et al. (2023), thus facilitating a thorough safety analysis that can be backed with corroboratory experiments on real LLMs.
* **We show that safety fine-tuning methods yield specialized transformations that primarily activate for unsafe inputs.** We provide comprehensive analyses on the mechanisms learned by safety fine-tuning showing that these methods (i) encourage _separate cluster formations for safe and unsafe samples_ by minimally transforming MLP weights to specifically project unsafe samples into the null space of model's weights, and (ii) substantially reduce the local Lipschitzness of the model for unsafe samples.
* **Adversarial inputs have activations similar to safe samples, hence bypassing the safety transform.** Establishing the mechanism via which a model identifies which inputs to refuse processing of, we are able to demonstrate that by merely following an activation distribution that is exceedingly similar to that of safe samples, jailbreak attacks are able to ensure the minimal MLP transformation learned to identify unsafe samples is not triggered.

## 2 Preliminaries

**Safety fine-tuning protocols** Broadly, LLM training can be divided into three stages (Team et al., 2023; Touvron et al., 2023): (1) (unsupervised) pre-training to build the initial model; (2) instruction fine-tuning to optimize the pre-trained model to follow instructions and provide plausible outputs for general queries; and (3) safety fine-tuning to ensure that the instruction fine-tuned model's output respects human preferences. We denote an LLM parameterized with parameters \(\theta\) as \(f_{\theta}\). Let the tuple \(\mathbf{t}=\{\mathbf{x},\mathbf{y}^{p},\mathbf{y}^{l}\}\) consist of the input \(\mathbf{x}\), the preferred response \(\mathbf{y}^{p}\), and the less preferred response \(\mathbf{y}^{l}\). Let \(\theta^{\mathrm{IT}}\), \(\mathcal{D}\), and \(\ell(.,.)\) denote the parameters of the instruction fine-tuned model, the safety fine-tuning dataset, and the standard cross-entropy loss, respectively. Using these notations, the objective functions of safety fine-tuning methods analyzed in this work can be written as follows.

* _Supervised Safety Fine-Tuning (SSFT)_(Ouyang et al., 2022): \(\operatorname*{argmin}_{\theta}\mathbb{E}_{(\mathbf{x},\mathbf{y}^{p})\sim \mathcal{D}}\ \ell\left(f_{\theta}(\mathbf{x}),\mathbf{y}^{p}\right)\).
* _Unlearning_(Liu et al., 2024): \(\operatorname*{argmin}_{\theta}\mathbb{E}_{\mathbf{t}\sim\mathcal{D}}\ \big{(}\ell(f_{\theta}(\mathbf{x}),\mathbf{y}^{p})-\gamma \ell(f_{\theta}(\mathbf{x}),\mathbf{y}^{l})\big{)}\).
* _Direct Preference Optimization (DPO)_(Rafailov et al., 2023): \[\operatorname*{argmax}_{\theta}\mathbb{E}_{\mathbf{t}\sim\mathcal{D}}\log \sigma\big{(}\beta(\ell(f_{\theta^{\mathrm{IT}}}(\mathbf{x}),\mathbf{y}^{p})- \ell(f_{\theta}(\mathbf{x}),\mathbf{y}^{p}))-\gamma(\ell(f_{\theta^{\mathrm{IT }}}(\mathbf{x}),\mathbf{y}^{l})-\ell(f_{\theta}(\mathbf{x}),\mathbf{y}^{l})) \big{)}.\]

Note that DPO uses instruction fine-tuned model as the reference model during optimization, and there is no \(\mathbf{y}^{l}\) in the case of SSFT.

**Transformer block** The transformer block used in this study consists of an attention module followed by two MLP layers with a non-linear activation layer--either silu(Elfwing et al., 2018) or GELU(Hendrycks and Gimpel, 2016)--in between. The second MLP layer writes to the residual stream of the Transformer block (Elhage et al., 2021). Throughout this work, we denote \(\mathrm{W}_{L}\) and \(\bar{\mathrm{W}}_{L}\) as the parameters of the first and the second MLP layers of the \(L\)-th transformer block.

**Fundamental subspaces (Strang, 2009)** Let \(\mathrm{W}_{m\times n}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{m}\) represent a matrix in \(\mathbb{R}^{m\times n}\). To avoid clutter, whenever possible, we denote \(W_{m\times n}\) by \(\mathrm{W}\). Let \(\text{SVD}(\mathrm{W}_{m\times n})=U_{m\times m}\Sigma_{m\times n}V_{\mathrm{ \times n}n}^{\top}\) represent a singular value decomposition of \(\mathrm{W}\), where \(U\) and \(V\) consist of the left and right singular vectors, \(\{\mathbf{u}_{i}\in\mathbb{R}^{m}\}_{i=1}^{m}\) and \(\{\mathbf{v}_{i}\in\mathbb{R}^{n}\}_{i=1}^{n}\), respectively, and \(\Sigma\) is the diagonal matrix with its diagonal elements being the singular values \(\sigma_{i}\), sorted in descending order of magnitude (\(\sigma_{i}\geq\sigma_{j}\) for \(i<j\)). Let \(r\leq\min(m,n)\) be the rank of \(\mathrm{W}\). Using singular vectors as the orthonormal bases, the four fundamental subspaces of \(\mathrm{W}\) are defined as:

* _Column-space:_\(\mathcal{C}(\mathrm{W})=\mathtt{span}\big{(}\{\mathbf{u}_{i}\}_{i=1}^{\top} \big{)}\), which is the same as the span of the columns of \(\mathrm{W}\).
* _Row-space:_\(\mathcal{R}(\mathrm{W})=\mathtt{span}\big{(}\{\mathbf{v}_{i}\}_{i=1}^{r}\big{)}\), which is the same as the span of the rows of \(\mathrm{W}\). Note that \(\mathcal{R}(\mathrm{W})=\mathcal{C}(\mathrm{W}^{\top})\).
* _Null-space:_\(\mathcal{N}(\mathrm{W})=\mathtt{span}\big{(}\{\mathbf{v}_{i}\}_{i=r+1}^{n} \big{)}\). If \(\mathrm{W}\mathbf{x}=\mathbf{0}\), then \(\mathbf{x}\in\mathcal{N}(\mathrm{W})\).
* _Left Null-space:_\(\mathcal{N}_{L}(\mathrm{W})=\mathtt{span}\big{(}\{\mathbf{u}_{i}\}_{i=r+1}^{m} \big{)}\), which is the same as the null-space of \(\mathrm{W}^{\top}\).

Note that \(\mathcal{C}(\mathrm{W})\) and \(\mathcal{N}(\mathrm{W}^{\top})\) are orthogonal to each other. Similarly, \(\mathcal{R}(\mathrm{W})\) is orthogonal to \(\mathcal{N}(\mathrm{W})\).

## 3 A Synthetic Controlled Set-up for Safety Fine-tuning

To systematically study the mechanisms yielded by safety fine-tuning and how adversarially designed inputs circumvent said mechanisms, we design a synthetic data generating process motivated by the framework of jailbreak attacks proposed by Wei et al. (2023) and Carlini et al. (2023). Specifically, the use of a synthetic setup helps us model the competing objectives and mismatched generalization formulation of Wei et al. (2023). For example, to elicit mismatched generalization, we must define samples that are out-of-distribution (OOD) compared to the ones used for safety fine-tuning of the model--the use of a synthetic data generating process helps us easily and scalably design such inputs. We emphasize that where possible, we do corroborate our findings on real-world LLMs (specifically, LLama models) by performing experiments similar to ones defined using our synthetic setup.

### Data generation for inducing instruction following behavior

We abstract out an input to an LLM as a composition of two components: (i) _operators_, which broadly specify a task the model is expected to perform, and (ii) _operands_, which specify what information the task is to be performed upon. For instance, consider the string: Tell me how to design a bike. Herein, one can deem design as an operator and bike as an operand. Despite its simplicity,

Figure 1: **Overview of our proposed synthetic setup to generate data.****(a)** A sample is divided into operators, operands, and outputs. The operators are function mappings the model is expected to perform on the operands to produce the _output tokens_, and are represented via tokens called _task tokens_. We often use the term _text tokens_ to refer to the operands the functions are to be performed upon. **(b)** The functions are restricted to bijective mappings, motivated by their use in synthetic setups for mechanistically analyzing Transformer models (Chughtai et al., 2023; Ramesh et al., 2023). **(c)** Text tokens are generated using PCFGs. To generate safe versus unsafe samples, we mark a subset of non-terminals at an intermediate level as safe-dominant (dark blue) and others as unsafe-dominant (light blue). Each of these nodes are associated with safe and unsafe task tokens, e.g., \(\mathcal{F}_{\mathcal{A}}^{s}\) and \(\mathcal{F}_{\mathcal{A}}^{u}\) respectively in blue box for safe dominant node. Our motivation here is that a task, by itself, is generally neutral (e.g., “design”), but when seen in the context of a concept it is to be performed on, i.e., the operands (e.g., “cycle” versus “bomb”), it can render the input unsafe.

we argue a large set of natural language inputs will fall under this abstraction (see App. B.1.2 for several examples). In our setup, we model this abstraction by defining an input to be a combination of tokens of two types: a _task token_\(f\in\mathcal{F}\) representing the notion of an operator, where \(\mathcal{F}\) is a family of predefined operators, and _text tokens_\(\mathcal{T}\), representing the notion of operands (see Fig. 1).

To generate text tokens, we use Probabilistic Context-free Grammars (PCFGs)--an often used model for natural language that captures its syntactic properties (Knudsen and Hein, 1999; Charniak, 1997) and that has seen recent use as a framework for mechanistic analysis of language modeling capabilities of Transformers (Allen-Zhu and Li, 2023; Hahn and Goyal, 2023). We denote a grammar as \(\texttt{PCFG}(\gamma,T,NT,R,P)\), where \(R=\{NT_{i}^{l}\rightarrow\{c_{j}\}_{j=1}^{m}\}_{i=0}^{|NT|}\) is the set of production rules between non-terminal parent nodes (\(NT_{i}^{l}\)) at level \(l\) and their respective children nodes \(\{c_{j}\}_{j=1}^{m}\), and \(P\) is the set of probabilities associated with rules in \(R\). A sequence of text tokens \(\mathcal{T}\) is hence sampled by simply traversing through the PCFG tree, starting from the root node \(\gamma\), propagating through non-terminal nodes (\(NT\)) via production rules (\(R\)) according to their associated probabilities (\(P\)), and terminating at the terminal nodes (\(T\)). See App. B for a detailed discussion of this process. For the family of operators \(\mathcal{F}\), we follow recent work by Ramesh et al. (2023); Chughtai et al. (2023) and let each task token (operator) \(f\in\mathcal{F}\) be a bijective mapping \(f:\mathcal{V}\rightarrow\mathcal{V}\), where \(\mathcal{V}\) denotes the vocabulary of the PCFG generations (Fig. 1(b)). For example, given text tokens \(\mathcal{T}\sim\texttt{PCFG}(\gamma,T,NT,R,P)\) and task tokens \(f_{i},f_{j}\sim\mathcal{F}\), we define the sequence of output tokens as \(\mathcal{O}=f_{j}(f_{i}(\mathcal{T}))\). Overall, the process above yields an input \(\mathcal{X}:=\{f_{j}\circ f_{i},\mathcal{T},\mathcal{O}\}\) (see Fig. 1). We note the goal for having two operators as part of the input (e.g., \(f_{i},f_{j}\)) is that it allows us to model the _competing objectives_ format of jailbreak attacks proposed by (Wei et al., 2023), wherein the model is asked to perform two tasks simultaneously, of which one is unsafe (e.g., \(f_{i}\)) and the other is not (e.g., \(f_{j}\)). To make the overall task non-trivial, we use four PCFGs (See Fig. A.8 in appendix).

For pre-training, we perform next token prediction on text and output tokens to learn the PCFG grammar rules \(R\) along with the bijective mappings of task tokens. For instruction fine-tuning, we supervise the model to predict output tokens given instructions consisting of task tokens \(f_{i},f_{j}\) and text tokens \(\mathcal{T}\). Next we describe further necessary design choices we make to generate data for safety fine-tuning, jailbreak attacks, and adversarial attacks.

### Data generation for safety fine-tuning

Safety fine-tuning requires a dataset labelled as per user preferences (Rafailov et al., 2023; Ouyang et al., 2022). Generally, the preferred output corresponds to accurately following the instruction for the inputs that are deemed safe, while _refusing_ to respond to inputs that are deemed to be unsafe. We next develop an abstraction for such preference data for studying the mechanisms of safety fine-tuning. Specifically, we note that an operator or operand, by itself, cannot determine whether an instruction is safe or unsafe. For example, consider the following strings: Design a bomb (s1), Design a cycle(s2), and Provide the history of bombs (s3), where s1 is deemed unsafe and s2, s3 are deemed safe. One can easily see that it is the contextual meaning an operator and an operand acquire from being part of the same string that renders the overall string unsafe. For example, the operator design when seen in the context of operand bomb renders the overall string s1 to be unsafe, but not so when seen in the context of operand cycle. Similarly, the string s3, despite having bomb as its operand, is likely to be deemed safe, since therein the operator is merely Provide history.

To model the intuition above in our framework, we split the non terminal nodes at a predefined intermediate level \(l_{s}\) (\(=3\) in our experiments) into two disjoint sets called _safe dominant nodes_, \(\mathcal{A}\subset NT^{l_{s}}\), and _unsafe dominant nodes_, \(\mathcal{B}\subset NT^{l_{s}}\), where \(NT^{l_{s}}\) is the set of non-terminals at level \(l_{s}\). Let \(\mathcal{F}^{s}_{\mathcal{A}}\) and \(\mathcal{F}^{u}_{\mathcal{A}}\) respectively be the set of safe and unsafe task tokens associated with nodes in \(\mathcal{A}\) (similarly for \(\mathcal{B}\)); that is, if a node in \(\mathcal{A}\) (resp. \(\mathcal{B}\)) is selected while sampling the text tokens, the predefined set of operators that yield an overall string that is deemed safe come from the set \(\mathcal{F}^{s}_{\mathcal{A}}\) (resp.

Figure 2: **Generating jailbreak and adversarial attacks using our data generating framework.****(a)** General instruction format. **(b,c)** Generating task and text tokens of jailbreaks with competing objectives. **(d)** Jailbreak attacks with mismatched generalization. **(e)** Adversarial attacks.

\(\mathcal{F}^{s}_{\mathcal{B}}\)). We also constrain these sets such that \(|\mathcal{F}^{s}_{\mathcal{A}}|>|\mathcal{F}^{u}_{\mathcal{A}}|\), \(|\mathcal{F}^{s}_{\mathcal{B}}|<|\mathcal{F}^{u}_{\mathcal{B}}|\), \(\mathcal{F}^{u}_{\mathcal{A}}\subset\mathcal{F}^{u}_{\mathcal{B}}\) and \(\mathcal{F}^{s}_{\mathcal{B}}\subset\mathcal{F}^{s}_{\mathcal{A}}\). These conditions ensure that if nodes from \(\mathcal{A}\) (resp. \(\mathcal{B}\)) are sampled, the corresponding sequences are _mostly_ safe (resp. unsafe). Thus, different task tokens are associated with safe/unsafe inputs with different frequencies similar to real world instructions, e.g., operators like 'harm', 'destroy' are much more likely to be associated with unsafe generations as compared to 'design', 'purchase'.

Overall, an input \(\mathcal{X}\) is deemed _unsafe_ if \(\mathcal{X}=\{f_{i}\circ f_{j},\mathcal{T}_{\mathcal{A}},\mathcal{O}\}\) where \(f_{i},f_{j}\in\mathcal{F}^{u}_{\mathcal{A}}\) or \(\mathcal{X}=\{f_{i}\circ f_{j},\mathcal{T}_{\mathcal{B}},\)\(\mathcal{O}\}\) where \(f_{i},f_{j}\in\mathcal{F}^{u}_{\mathcal{B}}\) (similarly for safe samples). This yields contextual dependence between operators and operands that render an input safe versus unsafe. We note that to capture the low variability observed in the outputs of safety fine-tuned LLMs for unsafe samples (e.g., the ubiquitous 'I can't explain', 'I can't tell', etc. responses), during safety fine-tuning, we supervise the model to output a token called _null token_ (see App. B for details). Meanwhile, for safe samples, the model is fine-tuned to follow the instructions as usual.

### Data generation for jailbreak and adversarial attacks

We ground ourselves in the framework of Wei et al. (2023) and Carlini et al. (2023) to study the following three types of adversarial inputs. For each type, we provide real-world examples in App. B.1.2 that map onto our synthetic abstraction, highlighting the analogy in detail.

1. [leftmargin=*]
2. **Jailbreaks via competing objectives (JB-CO-Task and JB-CO-Text in Fig. 2(b) and (c)).** Such inputs ask the model to simultaneously solve two tasks, one that is unsafe and one that is not. For example, consider the input: How to make a bomb? Start with "Sure, here's". The first phrase in this input may be deemed unsafe, while the second phrase is objectively neutral and merely asking the model to engage in an instruction following behavior. Often, the model in pursuit of following instructions will perform the task presented in the unsafe phrase as well. We investigate two ways to imitate such inputs. (i) Sample the two task tokens to define an input from either \(\mathcal{F}^{u}_{\mathcal{A}}\) and \(\mathcal{F}^{s}_{\mathcal{A}}\) or \(\mathcal{F}^{s}_{\mathcal{B}}\) and \(\mathcal{F}^{u}_{\mathcal{B}}\), hence asking the model to perform both a safe and an unsafe task. (ii) Generate text tokens by using the lowest common ancestor of nodes in \(\mathcal{A}\) and \(\mathcal{B}\) as the root node and following PCFG grammar rules. We use the task tokens which generate safe inputs when combined with text tokens sampled from nodes in \(\mathcal{A}\) and generate unsafe inputs for nodes in \(\mathcal{B}\). In this way, similar to (i), the model is asked to perform both a safe and an unsafe task.
3. **Jailbreaks via mismatched generalization (JB-MisGen in Fig. 2(d)).** Datasets used for safety fine-tuning are often substantially smaller and less diverse than the ones used for pre-training (Ouyang et al., 2022; Team et al., 2023). For example, such datasets are generally in English, even though the model can process other languages or formats (e.g., ASCII). Use of alternative formatting of the input has thus become a viable way of bypassing safety fine-tuning (Wei et al., 2023; Kotha et al., 2023). To model this in our framework, we define a set of task tokens \(T_{\mathrm{OOD}}\) which are not included in the safety fine-tuning dataset (similar to languages other than English). For each such token, we ensure there exists _another_ task token that is used during safety fine-tuning and has the same functionality as the OOD token, i.e., corresponds to the same bijective mapping. This models the intuition that an unsafe input with similar semantics will likely be present in the safety fine-tuning dataset, but, e.g., in English.
4. **Attacks based on continuous, learned embeddings (Adv in Fig. 2(e)).** Motivated by Carlini et al. (2023), we append a set of embeddings to the input and optimize these embeddings via a white-box targeted attack on the model, akin to standard adversarial attacks in vision (Madry et al., 2018). The attack's strength increases as the number of embeddings is increased.

## 4 Investigating the Effect of Safety Fine-tuning

We now investigate the mechanism by which safety fine-tuning impacts the behavior of a model. For this, we investigate three main aspects of a model: (i) feature space; (ii) parameter space; and (iii) function sensitivity. For experiments on our synthetic data-generating process, similar to existing related works (Jain et al., 2023; Allen-Zhu and Li, 2023), we train minGPT(Karpathy, 2020) using medium \(\eta_{M}=10^{-4}\) and small \(\eta_{S}=10^{-5}\) learning rates. See App. B.1.3 for further details on model training, selection, and cross-validation of the hyperparameters. To corroborate our claims, where possible, we run analogous experiments on Llama models (Touvron et al., 2023; Card, 2024) by defining a dataset of 500 safe and unsafe natural language instructions that are structurally similar to our synthetic data (see App. B.2 for details). Specifically, we use Llama-2 7B and Llama-3 8B as pretrained models and Llama-2 chat 7B and Llama-3 chat 8B as their corresponding safety fine-tuned variants.

Our analysis focuses on MLPs in each Transformer block. Specifically, we analyze the activations at the output of this layer (after GELU) in Sec. 4.1, and its parameters and pre-activations in Sec. 4.2. The overall model's sensitivity to input perturbations is analyzed in Sec. 4.3. In all plots, green and red colors are used to denote the analysis corresponding to the safe and unsafe samples, respectively.

### Clustering of safe versus unsafe samples' activations: Analyzing activation space

We first analyze how safety fine-tuning affects activations of safe versus unsafe samples.

[background=0.5cm] **Observation 1**

Safety fine-tuning leads to formation of clusters of activations corresponding to safe versus unsafe samples, where the separation between clusters increases as better methods are used.

Experimental setupLet \(\mathbf{a}_{L}^{o}(\mathbf{x})[i]\) be the \(L\)-th layer's output activation corresponding to the \(i\)-th token of an input sequence \(\mathbf{x}\). We define the average activation corresponding to the \(q\)-th output token as \(\hat{\mathbf{a}}_{L}^{o}(\mathbf{x})[q]=\frac{1}{q-1}\sum_{i=k}^{q+k-1} \mathbf{a}_{L}^{o}(\mathbf{x})[i]\), where \(k\) is the index of the last text token. If \(\mathcal{D}_{S}\) and \(\mathcal{D}_{U}\) are two datasets comprised solely of inputs with safe versus unsafe instructions, we define the _mean_ safe and unsafe activation at layer \(L\) as follows.

\[\mu_{L}^{S}=\frac{1}{|\mathcal{D}_{S}|}\sum_{\mathbf{x}\in\mathcal{D}_{S}} \hat{\mathbf{a}}_{L}^{o}(\mathbf{x})[q],\text{ and }\qquad\mu_{L}^{U}=\frac{1}{|\mathcal{D}_{U}|}\sum_{ \mathbf{x}\in\mathcal{D}_{U}}\hat{\mathbf{a}}_{L}^{o}(\mathbf{x})[q].\] (1)

Now, if the model distinguishes between safe versus unsafe inputs at the level of intermediate layers' activations, we claim we will see two explicit clusters formed for safe versus unsafe inputs. To assess the same, we define the following measure that computes the Euclidean distance of a sample \(x\)'s activations from the mean unsafe versus safe activation.

\[\tau\left(\mathbf{x},\mu_{L}^{S},\mu_{L}^{U}\right)=\|\hat{\mathbf{a}}_{L}^{o }(\mathbf{x})[q]-\mu_{L}^{U}\|_{2}-\|\hat{\mathbf{a}}_{L}^{o}(\mathbf{x})[q]- \mu_{L}^{S}\|_{2}\] (2)

The measure above should be positive for safe inputs and negative for the unsafe ones. When analyzed over a large number of inputs, it helps us gauge how clustered the activations corresponding to safe versus unsafe inputs are. Results are reported in Fig. 3. We find that activations--especially in the deeper layers--are indeed clustered depending on whether they come from safe versus unsafe inputs. Furthermore, in Fig. 3 (top), we observe in our synthetic setup that as the strength of the safety fine-tuning protocol increases (e.g., DPO and Unlearning compared to SSFT or DPO with medium learning rate \(\eta_{M}\) compared to DPO with small learning rate \(\eta_{S}\)), separation between the clusters increases, where separation is defined as the difference between the average value of \(\tau\) for safe versus unsafe samples. We find similar results using Llama-2 and Llama-3 models as well (see Fig. 3 (below)), indicating our findings translate to more realistic settings.

We also investigate the impact of safety fine-tuning on the'shape' of safe and unsafe feature clusters by analyzing singular values/vectors of their corresponding empirical covariance matrices \(\Sigma^{S}\) and

Figure 3: **Safety fine-tuning encourages separate cluster formations for safe and unsafe samples.** x-axis: layer number, y-axis: average \(\tau\) in Eq.2. **(Top)** Results using the synthetic setup. **(Bottom)** Results on Llama. Llama-2 chat 7B and Llama-3 chat 8B correspond to safety fine-tuned models.

\(\Sigma^{U}\), respectively (refer App C.3.1). As clearly observed in Fig A.19, it is the top singular value of \(\Sigma^{U}\) that is impacted the most as the safety fine-tuning progresses, however, the singular values of \(\Sigma^{S}\) remain more or less the same. The \(\sigma_{1}(\Sigma^{U})\) scales to a point where it constitutes nearly \(62\%\) of the nuclear norm of \(\Sigma^{U}\), whereas this value is merely \(12\%\) for \(\sigma_{1}(\Sigma^{S})\). This indicates that safety fine-tuning reshapes the cluster of unsafe features in a way that there remains a single dominant direction. However, the shape of the cluster corresponding to safe samples is not impacted much.

### What drives the clustering of safe and unsafe samples: Analyzing parameter changes

To identify what drives the formation of separate clusters of safe and unsafe samples, we evaluate precisely how model parameters change as a consequence of safety fine-tuning. Since Fig. 3 indicates clustering is strongest in deeper layers, we primarily analyze the MLP layers of the last two transformer blocks in this section. In particular, let \(\mathrm{W_{IT}}\) and \(\mathrm{W_{ST}}\) denote the instruction and the safety fine-tuned parameters of the first MLP layer of the \(L\)-th transformer block (\(L\) is intentionally omitted in notation to avoid clutter). Then, the change in parameters due to safety fine-tuning--or what we will often call "transformation"--is defined as \(\Delta\mathrm{W}=\mathrm{W_{ST}}-\mathrm{W_{IT}}\).

Experimental setupLet \(\{\mathbf{u}_{i}\}_{i=1}^{r}\) and \(\{\sigma_{i}\}_{i=1}^{r}\) be the top \(r\) left singular vectors and singular values of \(\mathrm{W_{IT}}\), where \(r\) denotes the empirical rank of \(\mathrm{W_{IT}}\), which is defined as the minimum value of \(k\) such that \(99\%\) of variance is preserved, i.e., \(\sum_{i=1}^{k}\sigma_{i}^{2}\geq 0.99\|\mathrm{W_{IT}}\|_{\mathrm{P}}^{2}\). Similarly, let \(\{\widetilde{\mathbf{u}}_{i}\}_{i=1}^{t}\) be the top \(t\) left singular vectors of \(\Delta\mathrm{W}\) where \(t\) is the empirical rank of \(\Delta\mathrm{W}\). The projection matrix for the column-space of \(\mathrm{W_{IT}}\) is defined as \(\mathrm{P}:=\sum_{i=1}^{r}\mathbf{u}_{i}\mathbf{u}_{i}^{\top}\). Let \(\theta_{i}\) be the angle between \(\mathrm{P}\widetilde{\mathbf{u}}_{i}\) and \(\widetilde{\mathbf{u}}_{i}\). It is easy to see that \(\widetilde{\mathbf{u}}_{i}\mathrm{sin}(\theta_{i})\) provides the projection of \(\widetilde{\mathbf{u}}_{i}\) on \(\mathcal{N}(\mathrm{W_{IT}^{\top}})\) since \(\mathcal{N}(\mathrm{W_{IT}^{\top}})\) is orthogonal to \(\mathcal{C}(\mathrm{W_{IT}})\). Since \(\widetilde{\mathbf{u}}_{i}\) is unit norm, we can plot the magnitude of projection of \(\widetilde{\mathbf{u}}_{i}\) on the space \(\mathcal{N}(\mathrm{W_{IT}^{\top}})\) by evaluating \(\mathrm{sin}(\theta_{i})\). Results for blocks 5 and 6 are shown in Fig. 4 for the PCFG-based experiments, and in Fig. A.17 for Llama models. A baseline model fine-tuned using standard cross-entropy loss to follow instructions in the usual way is also evaluated (shown in dotted lines in Fig. 4). Our results indicate that for safety fine-tuned models, the magnitude of projected component onto \(\mathcal{N}(\mathrm{W_{IT}^{\top}})\) is very large, especially when compared to the baseline. This implies \(\Delta\mathrm{W}\) and \(\mathrm{W_{IT}}\) are nearly orthogonal to each other. _Thus, a sample processed by \(\Delta\mathrm{W}\) will have a component that cannot be computed by \(\mathrm{W_{IT}}\) itself, hence yielding two broad sets of activations corresponding to samples which are processed by \(\Delta\mathrm{W}\) versus not._ To make this more concrete, we next evaluate which samples are likely to be processed by \(\Delta\mathrm{W}\) by analyzing its row space.

Experimental setupWe analyze pre-activation for the last text token, i.e., one corresponding to the first output token prediction. The pre-activation is normalized since our goal is to primarily assess its alignment with the row-space of \(\Delta\mathrm{W}\). Specifically, to capture the effect of \(\Delta\mathrm{W}\) on a given

Figure 4: **Safety fine-tuning learns transformations \(\Delta\mathrm{W}\) whose column-space is more aligned with \(\mathcal{N}(\mathrm{W_{IT}^{\top}})\). y-axis: Magnitude of projected component of left singular vector \(\widetilde{\mathbf{u}}_{i}\) on \(\mathcal{N}(\mathrm{W_{IT}^{\top}})\), x-axis: Index of left singular vectors, sorted by increasing magnitude of projected component.**

[MISSING_PAGE_FAIL:8]

For a given real-valued function \(\hat{f}_{\theta}:\mathbf{x}\rightarrow\mathbb{R}\) and input \(\mathbf{x}\), we define the local Lipschitzness of \(\hat{f}\) at \(\mathbf{x}\) as \(\text{Lip}_{\hat{f}}(\mathbf{x})=\|\nabla_{\mathbf{x}}\hat{f}_{\theta}(\mathbf{ x})\|_{2}\).

Experimental setupWe consider \(\hat{f}=\operatorname*{argmax}_{j}h_{\theta}(\mathbf{x})[k](j)\), where \(h_{\theta}(\mathbf{x})[k](j)\) is the \(j\)-th logit predicted at the end of text token index, denoted by \(k\). The sensitivity is obtained corresponding to the most confident output. Parameters \(\theta^{\text{IT}}\) and \(\theta^{\text{ST}}\) are chosen depending on the model under consideration. The histograms of \(\text{Lip}_{\hat{f}}(\mathbf{x})\) for safe (green) and unsafe (red) samples are shown in Fig. 6. _We can clearly observe that the sensitivity of the safety fine-tuned model is much lower compared to instruction fine-tuned model for unsafe samples, especially when DPO and Unlearning are used for fine-tuning._ This makes sense as, for unsafe samples, the variation in the preferred output strings in safety fine-tuning dataset is much less compared to that of safe samples: e.g., preferred outputs for unsafe samples are generally 'NULL', 'I can't assist', etc. The consequence of this decrease in sensitivity is that it will be _relatively_ more difficult to craft jailbreaks and adversarial attacks for more effective safety fine-tuning protocols, since models witness a stronger decrease in Lipschitzness under those protocols. We validate this claim in Tab. A.1 as well, showing that crafting jailbreaks and adversarial attacks is more difficult for DPO and Unlearning as compared to SSFT.

## 5 Evading the Safety Mechanism: Jailbreak and Adversarial Inputs

Having established and investigated the mechanism via which safety fine-tuning leads the model to refuse to process unsafe inputs, we can now analyze precisely why jailbreaks and adversarial attacks are still able to induce unsafe responses from the model.

Experimental setupWe use our instantiation of jailbreaks and adversarial attacks defined in Sec. 3.3, and motivated by the works of Wei et al. (2023) and Carlini et al. (2023). As shown in Tab. A.1, for DPO with \(\mu_{M}\), the JB-CO-Text attack yields the highest success rate (\(97.2\%\)), whereas the JB-CO-Task attack yields the lowest one (\(31.5\%\)). This trend is also observed for other safety fine-tuning methods (see Tab. A.1). For further analysis, we only consider the _successful_ attacks.

**(i) Feature space.** Building on Sec. 4.1, we analyze the separation between clusters induced by safe and unsafe samples, _but use jailbreaks and adversarial attacks instead of unsafe samples this time_. Results are shown in Fig 7 (top). We find the cluster separation between safe samples and attacked samples decreases in the feature space as the strength of attack increases, i.e., the decrease is higher for JB-CO-Text and JB-MisGen, which are stronger attacks (See Tab. A.1) as compared to JB-CO-Task. We observe a similar trend for adversarial attacks as well. This indicates _with increase in attack strength, adversarial inputs yield features that are similar to safe samples._ We note that concurrent work by (Ball et al., 2024) provide additional evidence in support of these results on larger models like Vicuna-13B.

**(ii) Function space.** Building on Sec. 4.3, we analyze the empirical Lipschitz constant for jailbreak and adversarial attacks in Fig. 7 (middle row). Clearly, with increase in attack strength, the histogram for jailbreaks starts to overlap with the histogram corresponding to the safe samples, showing that the model's local sensitivity also starts to lie between attacked and safe samples. Similar to the feature

Figure 6: **Lipschitz constant, hence the sensitivity of the model, decreases for unsafe samples and increases for safe samples after safety fine-tuning. The decrease is higher for stronger approaches, i.e., unlearning and DPO. x-axis: local Lipschitzness, y-axis: number of samples.**

space analysis above, the function sensitivity analysis also highlights that with the increase in attack strength, the adversarial samples start producing representations similar to safe samples.

**(iii) Parameter space.** To tie everything together and explain the similarity of features between jailbreak and safe samples, we finally build on Sec. 4.2 and analyze the impact of \(\Delta\mathrm{W}\) on jailbreak and adversarial inputs. Specifically, we analyze the alignment of pre-activations \(\mathbf{a}\) corresponding to these inputs with the row space of \(\Delta\mathrm{W}\) (same setup as discussed in Fig. 5). Results are shown in Fig. 7 (bottom). We observe that _unlike unsafe samples, \(\Delta\mathrm{W}\) does not impact jailbreak / adversarial samples noticeably_: e.g., see Fig. 5, where unsafe samples have a much higher alignment with row space of \(\Delta\mathrm{W}\) compared to safe ones, versus results on JB-CO-Text inputs in Fig. 7 (bottom), where we find the alignment is essentially the same! As we showed before, it is the impact of \(\Delta\mathrm{W}\) that leads to a distinction between how safe versus unsafe samples are processed; hence, results above suggest the model will process successful jailbreak / adversarial samples as if they were safe. We provide additional fine-grained analysis related to our observations above for different safety fine-tuning methods and layers in App. C.3.4 (for jailbreak attacks) and C.3.8 (for adversarial attacks).

## 6 Conclusion

We proposed a synthetic data generation framework to systematically and efficiently analyze common safety fine-tuning methods and craft jailbreak attacks. Using our framework, we showed that safety fine-tuning encourages the formation of separate clusters for safe and unsafe samples while making the model significantly less sensitive towards unsafe ones. We further found that the clustering effect in model's activation space can be explained by the weight space analysis, where the learned update was found to be specialized in projecting the unsafe samples onto the null space. These updates were not able to generalize well against samples for jailbreak and adversarial attacks, which resulted in their activations being more similar to safe samples than the unsafe ones. Hence bypassing the safety mechanism learned by the model. Wherever possible, we also showed via experiments on Llama models that our claims directly transferred to more realistic setups.

Figure 7: **Analyzing jailbreaks and adversarial inputs.** Building on the safety mechanism established in Sec. 4, we evaluate how jailbreak and adversarial inputs evade this mechanism by repeating our analysis from that section. We use brown color to represent the jailbreaking and adversarial inputs. **Top row (Feature space).** Similar to Fig. 3, we analyze average \(\tau\) (see Eq. 2) as a function of layers in the model. As the strength of attacks used increases, we see separation between clusters decreases. **Middle row (Function space).** The distribution of the local Lipschitzness of samples similar to Fig.6. In both rows, the difference between safe and unsafe examples (in the first column) decreases after jailbreak and adversarial attacks. **Bottom row (Parameter space.)** Projection of unit-norm pre-activation \(\mathbf{a}\) on \(\sigma_{i}\mathbf{v}_{i}^{\top}\). Activation corresponding to jailbreak and adversarial samples are not influenced significantly by \(\Delta\mathrm{W}\).

## Acknowledgements

Ekdeep's time at University of Michigan was partially supported by the National Science Foundation (CNS-2211509) and at CBS, Harvard by the Physics of Intelligence funded by NTT Research, Inc. Philip Torr would like to thank the UKRI grant (Turing AI Fellowship EP/W002981/1) and the Royal Academy of Engineering for supporting him to participate in this work.

## References

* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* Allen-Zhu and Li (2023) Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 1, context-free grammar. _arXiv preprint arXiv:2305.13673_, 2023.
* Andriushchenko et al. (2024) Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Jailbreaking leading safety-aligned lms with simple adaptive attacks. _arXiv preprint arXiv:2404.02151_, 2024.
* Anwar et al. (2024) Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, et al. Foundational challenges in assuring alignment and safety of large language models. _arXiv preprint arXiv:2404.09932_, 2024.
* Arditi et al. (2024) Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Rimsky, Wes Gurnee, and Neel Nanda. Refusal in language models is mediated by a single direction. _arXiv preprint arXiv:2406.11717_, 2024.
* Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. _arXiv preprint arXiv:2212.08073_, 2022.
* Ball et al. (2024) Sarah Ball, Frauke Kreuter, and Nina Rimsky. Understanding jailbreak success: A study of latent space dynamics in large language models. _arXiv preprint arXiv:2406.09289_, 2024.
* Bengio et al. (2023) Yoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-Qin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, et al. Managing ai risks in an era of rapid progress. _arXiv preprint arXiv:2310.17688_, 2023.
* Bishop (2006) Christopher M. Bishop. _Pattern Recognition and Machine Learning (Information Science and Statistics)_. Springer-Verlag, Berlin, Heidelberg, 2006. ISBN 0387310738.
* Bubeck et al. (2023) Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. _arXiv preprint arXiv:2303.12712_, 2023.
* Card (2024) Llama 3 Model Card. _AI@Meta_, 2024. https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md.
* Carlini et al. (2023) Nicholas Carlini, Milad Nasr, Christopher A Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramer, et al. Are aligned neural networks adversarially aligned? _arXiv preprint arXiv:2306.15447_, 2023.
* Chao et al. (2023) Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, and Eric Wong. Jailbreaking black box large language models in twenty queries, 2023.
* Chao et al. (2024) Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramer, Hamed Hassani, and Eric Wong. Jailbreakbench: An open robustness benchmark for jailbreaking large language models, 2024.
* Charniak (1997) Eugene Charniak. Statistical techniques for natural language parsing. _AI Mag._, 18:33-44, 1997. URL https://api.semanticscholar.org/CorpusID:11071483.
* Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.
* Chughtai et al. (2023) Bilal Chughtai, Lawrence Chan, and Neel Nanda. A toy model of universality: Reverse engineering how networks learn group operations. In _International Conference on Machine Learning_, pp. 6243-6267. PMLR, 2023.
* Chughtai et al. (2023)Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. _arXiv preprint arXiv:2303.03378_, 2023.
* Elfwing et al. (2018) Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. _Neural networks_, 107:3-11, 2018.
* Elhage et al. (2021) Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 1:1, 2021.
* Goel et al. (2024) Shashwat Goel, Ameya Prabhu, Philip Torr, Ponnurangam Kumaraguru, and Amartya Sanyal. Corrective machine unlearning. _arXiv preprint arXiv:2402.14015_, 2024.
* Hahn and Goyal (2023) Michael Hahn and Navin Goyal. A theory of emergent in-context learning as implicit structure induction. _arXiv preprint arXiv:2303.07971_, 2023.
* Hein and Andriushchenko (2017) Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. _Advances in neural information processing systems_, 30, 2017.
* Hendrycks and Gimpel (2016) Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). _arXiv preprint arXiv:1606.08415_, 2016.
* Jain et al. (2023a) Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Sompegalli, John Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein. Baseline defenses for adversarial attacks against aligned language models. _arXiv preprint arXiv:2309.00614_, 2023a.
* Jain et al. (2023b) Samyak Jain, Robert Kirk, Ekdeep Singh Lubana, Robert P Dick, Hidenori Tanaka, Edward Grefenstette, Tim Rocktaschel, and David Scott Krueger. Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks. _arXiv preprint arXiv:2311.12786_, 2023b.
* Karpathy (2020) Andrej Karpathy. _MinGPT_, 2020. Github link. https://github.com/karpathy/minGPT/tree/master.
* Knudsen and Hein (1999) Bjarne Knudsen and Jotun Hein. Rna secondary structure prediction using stochastic context-free grammars and evolutionary history. _Bioinformatics_, 15 6:446-54, 1999. URL https://api.semanticscholar.org/CorpusID:5971132.
* Kotha et al. (2023) Suhas Kotha, Jacob Mitchell Springer, and Aditi Raghunathan. Understanding catastrophic forgetting in language models via implicit inference. _arXiv preprint arXiv:2309.10105_, 2023.
* Lee et al. (2024) Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K. Kummerfeld, and Rada Mihalcea. A mechanistic understanding of alignment algorithms: A case study on dpo and toxicity, 2024.
* Li et al. (2024) Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et al. The wmdp benchmark: Measuring and reducing malicious use with unlearning. _arXiv preprint arXiv:2403.03218_, 2024.
* Liu et al. (2024) Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao, Hang Li, Kush R Varshney, et al. Rethinking machine unlearning for large language models. _arXiv preprint arXiv:2402.08787_, 2024.
* Lubana et al. (2022) Ekdeep Singh Lubana, Eric J. Bigelow, Robert P. Dick, David Krueger, and Hidenori Tanaka. Mechanistic Mode Connectivity, 2022. Comment: 39 pages.
* Lynch et al. (2024) Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. Eight methods to evaluate robust unlearning in llms. _arXiv preprint arXiv:2402.16835_, 2024.
* Madry et al. (2018) Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018. URL https://openreview.net/forum?id=rJz1BfZAb.
* Maini et al. (2024) Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C Lipton, and J Zico Kolter. Tofu: A task of fictitious unlearning for llms. _arXiv preprint arXiv:2401.06121_, 2024.
* Mehrotra et al. (2023) Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi. Tree of attacks: Tailbreaking black-box llms automatically. _arXiv preprint arXiv:2312.02119_, 2023.
* Neyshabur et al. (2021) Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer learning?, 2021.
* Nguyen et al. (2022) Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung Nguyen. A survey of machine unlearning. _arXiv preprint arXiv:2209.02299_, 2022.
* Nguyen et al. (2020)Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* Prakash et al. (2024) Nikhil Prakash, Tamar Rott Shaham, Tal Haklay, Yonatan Belinkov, and David Bau. Fine-tuning enhances existing mechanisms: A case study on entity tracking. _arXiv preprint arXiv:2402.14811_, 2024.
* Qin et al. (2024) Libo Qin, Qiguang Chen, Yuhang Zhou, Zhi Chen, Yinghui Li, Lizi Liao, Min Li, Wanxiang Che, and Philip S Yu. Multilingual large language model: A survey of resources, taxonomy and frontiers. _arXiv preprint arXiv:2404.04925_, 2024.
* Radford et al. (2018) Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. _OpenAI_, 2018.
* Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* Rafaal et al. (2023) Rafael Rafaalov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. _arXiv preprint arXiv:2305.18290_, 2023.
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.
* Ramesh et al. (2023) Rahul Ramesh, Mikail Khona, Robert P Dick, Hidenori Tanaka, and Ekdeep Singh Lubana. How capable can a transformer become? a study on synthetic, interpretable tasks. _arXiv preprint arXiv:2311.12997_, 2023.
* Sadasivan et al. (2024) Vinu Sankar Sadasivan, Shoumik Saha, Gaurang Sriramanan, Priyatham Kattakinda, Atoosa Chegini, and Soheil Feizi. Fast adversarial attacks on language models in one gpu minute, 2024.
* Samvelyan et al. (2024) Mikayel Samvelyan, Sharath Chandra Raparthy, Andrei Lupu, Eric Hambro, Aram H Markosyan, Manish Bhatt, Yuning Mao, Minqi Jiang, Jack Parker-Holder, Jakob Foerster, et al. Rainbow teaming: Open-ended generation of diverse adversarial prompts. _arXiv preprint arXiv:2402.16822_, 2024.
* Sanh et al. (2021) Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. _arXiv preprint arXiv:2110.08207_, 2021.
* Sanyal et al. (2019) Amartya Sanyal, Philip HS Torr, and Puneet K Dokania. Stable rank normalization for improved generalization in neural networks and gans. _arXiv preprint arXiv:1906.04659_, 2019.
* Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* Strang (2009) Gilbert Strang. _Introduction to Linear Algebra_. Wellesley-Cambridge Press, Wellesley, MA, fourth edition, 2009. ISBN 9780980232714 0980232716 9780980232721 0980232724 9788175968110 8175968117.
* Sun et al. (2024) Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. Trustllm: Trustworthiness in large language models. _arXiv preprint arXiv:2401.05561_, 2024.
* Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023a.
* Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023b.
* Tripuraneni et al. (2020) Nilesh Tripuraneni, Michael Jordan, and Chi Jin. On the theory of transfer learning: The importance of task diversity. _Advances in neural information processing systems_, 33:7852-7862, 2020.
* Wei et al. (2023) Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. Jailbroken: How does llm safety training fail?, 2023.
* Zhou et al. (2020)* Wei et al. (2024) Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek Mittal, Mengdi Wang, and Peter Henderson. Assessing the brittleness of safety alignment via pruning and low-rank modifications. _arXiv preprint arXiv:2402.05162_, 2024.
* Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. _arXiv preprint arXiv:2109.01652_, 2021.
* Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022.
* Wong and Kolter (2018) Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In _International conference on machine learning_, pp. 5286-5295. PMLR, 2018.
* Zheng et al. (2024) Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang, and Nanyun Peng. Weak-to-strong extrapolation expedites alignment. _arXiv preprint arXiv:2404.16792_, 2024.
* Zou et al. (2023) Andy Zou, Zifan Wang, J Zico Kolter, and Matt Fredrikson. Universal and transferable adversarial attacks on aligned language models. _arXiv preprint arXiv:2307.15043_, 2023.

## Appendices

### Contents

* 1 Introduction
* 2 Preliminaries
* 3 A Synthetic Controlled Set-up for Safety Fine-tuning
	* 3.1 Data generation for inducing instruction following behavior
	* 3.2 Data generation for safety fine-tuning
	* 3.3 Data generation for jailbreak and adversarial attacks
* 4 Investigating the Effect of Safety Fine-tuning
	* 4.1 Clustering of safe versus unsafe samples' activations: Analyzing activation space
	* 4.2 What drives the clustering of safe and unsafe samples: Analyzing parameter changes
	* 4.3 Impact of safety fine-tuning on the sensitivity of the learned model
* 5 Evading the Safety Mechanism: Jailbreak and Adversarial Inputs
* 6 Conclusion
* A Additional Background
* B Further Details on the Experimental Setup
* B.1 Further Details on the Synthetic Setup based on PCFG
* B.1.1 Data Generation
* B.1.2 Jailbreak and adversarial attacks
* B.1.3 Training Details
* B.2 Further Details on Real World Experiments based on Llama
* C Further Analyses to Understand Safety Fine-tuning
* C.1 Analyzing how the impact of transformation propagates over the layers
* C.2 Additional Results on Llama-2
* C.3 Additional Results on the synthetic setup
* C.3.1 Analysis of learning dynamics
* C.3.2 Clustering analysis for jailbreaking attacks on learned transform
* C.3.3 Analyzing the impact of safety fine-tuning on the parameter space of transformation
* C.3.4 Additional analysis on learned transformation for Jailbreaking attacks
* C.3.5 Clustering analysis for jailbreaking attacks on the second MLP layer in the transformer block
* C.3.6 Analyzing the impact of safety fine-tuning on parameter space of the second MLP layer in the transformer block
* C.3.7 Effect of jailbreaking attacks on the lipschitzness of the model
* C.3.8 Analyzing adversarial attacks
* D Additional Results Using Interventions

## Appendix A Additional Background

Safety fine-tuning Approaches in LLMs.The pipeline of training a large language model (LLM) involves three stages: (i) pre-training, (ii) instruction fine-tuning and (iii) safety fine-tuning. During pre-training, an LLM is supervised to predict the next token using a large amount of data scraped from web (Radford et al., 2019, 2018). This enables an LLM to learn various capabilities. In the instruction fine-tuning stage (Wei et al., 2021; Sanh et al., 2021; Raffel et al., 2020), the model is prompted by an instruction and supervised to output a predefined output for that specific instruction. However, due to the random sampling process of pre-training data from the internet, the instruction fine-tuned model can demonstrate unsafe capabilities as well. Therefore, as a last step, safety fine-tuning is performed to limit the capabilities of an LLM to yield unsafe outputs. For this purpose, data is gathered by having humans rank multiple outputs from the instruction fine-tuned LLM for a given prompt considering whether the output is safe or unsafe. Then, using this dataset, the LLM is commonly trained by one of the following four different protocols.

1. Supervised safety fine-tuning (SSFT) (Ouyang et al., 2022) relies only on the highly ranked outputs, i.e., the safest ones. Thus, the aim here is to make the model safe by fine-tuning it to follow the safe instructions and generate safe output for unsafe samples.
2. Reinforcement learning with human feedback (RLHF) (Ouyang et al., 2022; Christiano et al., 2017; Bai et al., 2022; Stiennon et al., 2020). The instruction fine-tuned model is trained as a reward model to replicate the human preferences, by assigning high reward to human aligned generations and low for others. A copy of the instruction fine-tuned model is then treated as a "policy" and fine-tuned using the reward model as a proxy, where high reward is given when it generates human aligned generations.
3. Direct preference optimization (DPO) (Rafailov et al., 2023) also uses safe and unsafe outputs similar to RLHF, but differently does not require an additional reward model. Instead, the LLM is directly supervised to suppress unsafe outputs by the constructed objective function.
4. Unlearning (Liu et al., 2024; Li et al., 2024; Goel et al., 2024; Lynch et al., 2024) has been commonly used to address privacy concerns, where the aim is to make the model _forget_ certain data samples (Maini et al., 2024; Nguyen et al., 2022). However, in case of safety fine-tuning, the objective is to unlearn the capabilities responsible for generation of malicious and unsafe outputs. Given similar goals, unlearning has recently become popular as a protocol to perform safety fine-tuning (Liu et al., 2024). This motivates us to investigate this fine-tuning protocol as well. We note that past works (Liu et al., 2024; Li et al., 2024; Goel et al., 2024; Lynch et al., 2024) have used different objective functions to perform unlearning, however, in most of these cases, the loss functions include two contrasting losses: one enforces the model to retain its safe capabilities to generate coherent outputs, while the other loss aims to force the model to forget its unsafe capabilities. Given this characteristic, we adopt the loss function used in Liu et al. (2024).

Understanding fine-tuning in LLMs.Fine-tuning is an exceedingly ubiquitous tool in the modern era of foundation models. Given this success of fine-tuning, it has become imperative to understand how it impacts the capabilities of pre-trained models. Recent works in this vein (Kotha et al., 2023; Tripuraneni et al., 2020; Neyshabur et al., 2021) show that fine-tuning works by re-weighting and transferring task relevant features to the downstream task. Relatedly, Jain et al. (2023), Prakash et al. (2024), and Lubana et al. (2022) analyze the effect of fine-tuning in a more mechanistic manner, where they conclude that fine-tuning minimally alters the pre-trained mechanisms, rather than fundamentally changing them. Relatedly, Lee et al. (2024) analyze DPO and concluded that DPO makes the model learn to bypass the activations corresponding to toxic regions in its activation space. We believe that our observations discussed in App. C.1 implicitly indicate the span of activations in the toxic regions of activation space reduces with safety fine-tuning.

Jailbreaks and adversarial attacks in LLMs.It has been shown that the current LLMs are vulnerable to adversarial attacks (Sadasivan et al., 2024; Zou et al., 2023; Carlini et al., 2023) and jailbreaks (Wei et al., 2023; Andriushchenko et al., 2024; Sun et al., 2024; Mehrotra et al., 2023; Samvelyan et al., 2024). Adversarial attacks are generally easier to identify programmatically when compared to jailbreaks. However, optimizing a prompt using adversarial training is prone to end up generating gibberish tokens in the input space. Therefore, it is easy to detect such attacks by using simple pre-processing techniques like perplexity (Jain et al., 2023). On the other hand, since jailbreaks are more natural and difficult to detect, they pose a bigger threat to safety of LLMs. Wei et al. (2023) characterize jailbreaks into two broad categories: (i) jailbreaks with mismatched generalization and (ii) jailbreaks with competing objectives.

Further Details on the Experimental Setup

This section includes further details on our synthetic and real world experimental setups.

### Further Details on the Synthetic Setup based on PCFG

#### b.1.1 Data Generation

Our synthetic setup involves defining samples comprised of two task tokens \(f_{1},f_{2}\), text tokens \(\mathcal{T}\), and output tokens \(\mathcal{O}\). The sampling of task and text tokens is conditioned on a sample being safe or unsafe, which we discuss in detail in the main paper. An example sample is illustrated in Fig. A.9 and we now provide the details for each of these aspects below.

Task Tokens.The task tokens are denoted as \(f_{i}\sim\mathcal{F}\), where \(\mathcal{F}=\{f_{i}\}_{i=1}^{12}\) and each \(f_{i}\) is given by a bijective mapping \(f_{i}:\mathcal{V}\to\mathcal{V}\). Here, \(\mathcal{V}\) is the vocabulary of the PCFG. To generate an input prompt we sample two task tokens \(f_{i}\) and \(f_{j}\) randomly from the set \(\mathcal{F}\). During safety fine-tuning, we do not sample any token from the set \(T_{\mathrm{OOD}}\), which consists of two tokens out of a total of twelve tokens present in \(\mathcal{F}\). There is a token amongst the remaining ten for each of these two tokens which represents the same bijective mapping but corresponds to a different token representation. For each sample generation we sample two task tokens.

Text Tokens.Every sample consists of between 15-25 text tokens that are generated by a PCFG relying on a set of grammar rules with uniform sampling probabilities. Note that here different combinations of sampling probabilities can give rise to more interesting generations, but we pose this as an interesting future direction. For simplicity of analysis, in this work we consider uniform sampling. We provide a detailed description of the grammar rules considered, along with different task tokens corresponding to sets \(\mathcal{F}^{u}_{\mathcal{A}}\), \(\mathcal{F}^{u}_{\mathcal{B}}\), \(\mathcal{F}^{s}_{\mathcal{A}}\) and \(\mathcal{F}^{s}_{\mathcal{B}}\), in Fig. A.8. Note that for pre-training and instruction fine-tuning, we sample the data using four different PCFGs. We describe the motivation and further details related to this design choice below.

A model trained on the synthetic data generated using a _single_ PCFG (as above) might perform well by simply learning the relationship between a single text token in \(\mathcal{T}\) with its corresponding operators, therefore ignoring the context window consisting of previous text tokens in a sequence. Thus, to force the model to learn to utilize the context, we utilize multiple different PCFGs (four in our experiments) such that _different_ bijective mappings corresponds to the _same_ task tokens across different PCFGs. For example, the task token '(' in a PCFG might imply bijective mapping \(f_{1}\), whereas the same token might imply \(f_{2}\) in another PCFG. Also, we ensure that the generated text tokens from each PCFG do not completely overlap. Thus, in order for the underlying model to perform well on this dataset, it has to learn the grammar corresponding to each PCFG which would require using context from previous text tokens.

Now we will discuss some additional intricate design choices considered while designing this setup to imitate real world scenario as much as possible, which we could not discuss in the main paper due to space constraints. Through a single traversal from the PCFG tree during pre-training we generate a sample which is of length 50-75 tokens and later crop it by randomly selecting the starting and ending index of a window sampled randomly to be between \(15-25\). This generation is started from the root node of the PCFG tree (See Fig. A.8). However, during safety fine-tuning, we divide the non terminal nodes at level three into safe dominant \(\mathcal{A}\) and unsafe dominant \(\mathcal{B}\) nodes. Using these nodes in sets \(\mathcal{A}\) or \(\mathcal{B}\) as the root node reduces the length of the generated sequence to between 15-25. Therefore, to ensure consistency between pre-training and safety fine-tuning, we crop \(\mathcal{T}\) to contain between 15-25 text tokens during pre-training. We define different sets of task tokens being safe and unsafe with each of the sets \(\mathcal{A}\) or \(\mathcal{B}\). \(\mathcal{F}^{s}_{\mathcal{B}}\) for safe and \(\mathcal{F}^{u}_{\mathcal{B}}\) for unsafe. We ensure that \(|\mathcal{F}^{u}_{\mathcal{B}}|=8\), \(|\mathcal{F}^{s}_{\mathcal{B}}|=2\), \(|\mathcal{F}^{s}_{\mathcal{A}}|=2\), \(|\mathcal{F}^{s}_{\mathcal{A}}|=8\). Further \(\mathcal{F}^{u}_{\mathcal{A}}\subset\mathcal{F}^{u}_{\mathcal{B}}\) and \(\mathcal{F}^{s}_{\mathcal{B}}\subset\mathcal{F}^{s}_{\mathcal{A}}\). This helps in controlling how often a task token is associated with safe vs unsafe generations. The PCFG trees utilized in our analysis have a depth of 6 levels and this is selected based on the design choices considered in Allen-Zhu & Li (2023). Another reason for choosing this depth is that it ensures the length of the sequence generated to remain in the expected limit. To ensure simplicity of the safety fine-tuning task, for safety fine-tuning we only consider the grammar generated by the first PCFG. Further we choose the third level to divide the non-terminal nodes into the sets \(\mathcal{A}\) and \(\mathcal{B}\) because it helps in generating a good enough sequence length, which could decrease significantly on increasing the levels or going down the tree. We did not choose level 2 because it would mean lesser number of non-terminal nodes are involved in determining if the sample is safe or unsafe. This would in term make the safety fine-tuning task easier for the model. In order to balance this trade-off between the task complexity and the length of text tokens generated, choosing the third level suits the best.

Outputs Tokens.The output tokens \(\mathcal{O}=f_{1}\circ f_{2}(\mathcal{T})\), thus the length of \(\mathcal{O}\) is same as \(\mathcal{T}\). In case of unsafe samples we ensure that all the output tokens are _null token_ defined by a single token given by 'a'.

#### b.1.2 Jailbreak and adversarial attacks

Content Warning: Some prompts below contain harmful or offensive content. The sole intent of these prompts is to show vulnerability of LLMs.

Jailbreak attacks.To design jailbreak attacks using our synthetic setup, we manipulate the sampling process of the text and task tokens depending on the type of jailbreak attacks we wish to craft. We describe the setup corresponding to each attacks along with corresponding examples generated using Llama-2-70b-chat model on https://chat.lmsys.org/. As shown in Fig. A.10, the Llama-2 70B chat model doesn't follow the instructions when prompted to generate unsafe text. However, we can break this safety mechanism of the model by using different types of jailbreak and adversarial attacks which we discuss below.

* **Jailbreak attacks with competing objectives (task), JB-CO-Task:** As shown in Fig. A.13, jailbreak attacks with competing objectives aim to break the safety mechanism of language models by prompting the model to follow instructions, while still having the unsafe prompt present in the input (Wei et al., 2023). In this case the "history part" is prompting the model to follow instructions and as a result the model also outputs about designing a bomb which clearly it should not output. Motivated by this, we sample one task token from \(\mathcal{F}_{\mathcal{N}}^{*}\) and the other from \(\mathcal{F}_{\mathcal{N}}^{*}\). Related to the example, consider that "history" was sampled from \(\mathcal{F}_{\mathcal{N}}^{*}\) and "design" from \(\mathcal{F}_{\mathcal{N}}^{*}\). This ensures that a part of the input prompt asks the model to generate safe output by following instructions, whereas the other part corresponds to unsafe generations.
* **Jailbreak attacks with competing objectives (text), JB-CO-Text:** Here, instead of manipulating the sampling process of task tokens, we modify the sampling process of text tokens. We do this by sampling the text tokens using the common parent node of the nodes in the set \(\mathcal{A}\) and \(\mathcal{B}\) as the root node. We present the corresponding motivating example for this attack in Fig. A.12. Here, the "cycle" and "bomb" can be interpreted as two different text tokens sampled using safe dominant and unsafe dominant non-terminal nodes respectively of the PCFG tree, where "cycle" is prompting the model to follow instructions and "bomb" corresponds to the unsafe part.
* **Jailbreak attacks with mismatched generalization, JB-MisGen:** As shown in Fig. A.14, here the aim is to exploit the model's safety mechanism by generating unsafe prompts which are out of distribution with respect to the safety fine-tuning dataset. In the example shown in Fig. A.14, translating "design a bomb" into Kannada which is a very different language as compared to english makes the model output unsafe generations. In our setup we imitate this behaviour, by sampling one of the two task tokens from held out task tokens (\(T_{\text{OOD}}\)).

Adversarial attacks.To design adversarial attacks, as shown in Fig. A.11, we use a setup similar to the one used in recent works Carlini et al. (2023); Zou et al. (2023). We append some soft prompts after the text tokens in the token encoding space of the model. We define the threat model as the number of soft prompts appended. Next, we perform targeted white box attack, by minimizing the standard cross entropy loss, where we utilize the ground truth labels corresponding to the respective bijective mapping as the target class. For this we use a threat model constraining the \(\ell_{2}\) norm of the soft tokens to be less than 1. To generate the attack, we use 10 steps of iterative gradient descent.

#### b.1.3 Training Details

In all our experiments on the synthetic setup, we use mingpt models, which consist of approximately three million parameters and include six transformer blocks, each containing six attention heads followed by two MLP layers, where the dimension of the activation stream is 192. The first MLP layer upscales it to 768 and the second one again downscales it to 192 dimensions. We use a maximum input sequence length of 100 tokens. There is a GELU activation layer in between the two MLP layers.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \hline
**Protocol** & **Learning Rate** & **Safe (Instruct)** & **Unsafe (Null)** & **Unsafe (Instruct)** & **JB-CO-Task (Instruct)** & **JB-CO-Text (Instruct)** & **JB-MisGen (Instruct)** \\ \hline \multirow{2}{*}{Unlearning} & \(\eta_{M}\) & 99.8 & 99.9 & 5.0 & 27.1 & 95.2 & 92.3 \\  & \(\eta_{S}\) & 99.7 & 99.9 & 31.2 & 51.2 & 98.3 & 98.5 \\ \hline \multirow{2}{*}{DPO} & \(\eta_{M}\) & 98.6 & 99.6 & 11.8 & 31.5 & 93.5 & 93.6 \\  & \(\eta_{S}\) & 98.7 & 100.0 & 40.7 & 56.1 & 97.2 & 96.1 \\ \hline \multirow{2}{*}{SSFT} & \(\eta_{M}\) & 99.9 & 99.8 & 51.6 & 88.1 & 100 & 100 \\  & \(\eta_{S}\) & 99.7 & 100.0 & 72.8 & 92.5 & 100 & 100 \\ \hline \hline \end{tabular}
\end{table}
Table A.1: **Safety performance of different fine-tuning protocols:** Unlearning (Liu et al., 2024), DPO (Rafailov et al., 2023) and supervised safety fine-tuning (SSFT) (Ouyang et al., 2022) with medium and small learning rates are used for performing safety fine-tuning. Instruct represents the accuracy of the model to follow instructions and Null represents model’s accuracy to output null tokens. Different jailbreaking attacks are also analyzed. JB-CO-Text and JB-MisGen are the strongest attacks where SSFT is easiest to attack.

Pre-training and instruction fine-tuning.We train the model to learn the grammar rules and structure of PCFG trees by using the next token prediction task on text tokens. We also train the model to learn the bijective mappings by correctly predicting the output tokens. Instead of separately performing instruction fine-tuning, we utilize a curriculum to transition from pre-training phase to instruction fine-tuning, where we associate probability of training the model on text tokens by \(\mathcal{P}_{\mathcal{T}}\) and \(\mathcal{P}_{\mathcal{O}}\) for the output tokens. During the initial phase of pre-training, we utilize a high value of \(\mathcal{P}_{\mathcal{T}}\) and low for \(\mathcal{P}_{\mathcal{O}}\) and linearly transition to using low value of \(\mathcal{P}_{\mathcal{T}}\) and high for \(\mathcal{P}_{\mathcal{O}}\). We observe that using a curriculum helps in stabilizing the training and it helps us achieve a model capable of predicting the output tokens correctly.

We use a cosine schedule on learning rate to ensure that a large learning rate is used for pre-training where majority of training focuses on learning the PCFG structure and a small value of learning rate is used for instruction fine-tuning where the major focus is to learn the bijective mappings. We decay the learning rate to \(1e-6\). We use 100k iterations to perform this training, with a learning rate of \(1e-3\) and cosine schedule with warmup of 10k iterations. This stage of combined pre-training and instruction fine-tuning takes over 8 hours on a single RTX A6000 gpu with 48GB memory, on using a batch size of 512.

Safety fine-tuning.We perform safety fine-tuning for 10k iterations, using cosine schedule without warmup with two sets of learning rates: \(1e-4\) and \(1e-5\) and decay them to \(1e-7\). We refer to \(1e-4\) as \(\eta_{\text{R}}\) and \(1e-5\) as \(\eta_{\text{S}}\). In contrast to pre-training and instruction fine-tuning, here we use the preferred \(y^{P}\) and less preferred \(y^{L}\) output tokens for fine-tuning the model using different safety protocols namely supervised safety fine-tuning, direct preference optimization and unlearning. In case of safe samples, \(y^{P}\) refers to the outputs corresponding to bijective mapping, whereas \(y^{L}\) refers to null token prediction. On the other hand, in case of unsafe samples, \(y^{P}\) refers to null token prediction and \(y^{L}\) refers to instruction following generations (ie. bijective mappings). As common in literature for pre-training as well as fine-tuning we use adam optimizer. We perform search over different values of \(\beta\) and \(\gamma\) which correspond to the hyperparameters used in the objective functions of unlearning and DPO (refer to main Sec. 2 for more details) and select the values which can give close to 100% accuracy on both safe and unsafe samples. We list the optimal values of hyperparameters below:

* Unlearning (\(\eta_{M}\)): \(\gamma=0.1\); Unlearning (\(\eta_{S}\)): \(\gamma=0.01\)
* DPO (\(\eta_{M}\)): \(\beta=0.1,\gamma=0.01\); DPO (\(\eta_{S}\)): \(\beta=0.1,\gamma=0.002\)

Evaluation setup.We perform evaluation using Acc (OR) defined as \(\sum_{i=1}^{n}(\mathcal{O}_{i}==y_{i})\) where \(n\) denotes the number of output tokens, \(\mathcal{O}_{i}\) denotes the \(i^{th}\) output token and \(y_{i}\) represents the corresponding ground truth value. We use 1K samples randomly sampled independently from the PCFG tree for generating the test set. By manipulating the sampling process of text and task tokens as described earlier, we generate the test sets of jailbreak samples as well. Each of these sets contain 1K samples. We utilize all these samples for our analysis. The results corresponding to the three safety fine-tuning protocols trained with medium and small learning rates (\(\eta_{M}\) and \(\eta_{S}\)) respectively are present in Table A.1. Note that here we denote the accuracy of the model to output _null_ tokens on unsafe samples sampled from the same distribution used for safety fine-tuning by Unsafe (Null) and similarly, we denote the accuracy of the model to follow instructions by (Instruct).

### Further Details on Real World Experiments based on Llama

We analyze how different observations as discussed in main paper transfer on Llama-2 7B (Touvron et al., 2023), Llama-2 7B chat, Llama-3 8B and Llama-3 8B chat models. For this, we make a simple synthetic dataset where each prompt consists of an operator-operand combination. The operator can be considered as a similar version of task tokens as discussed above and operand can be considered similar to text tokens.

Data Generation.We generate around 50 prompts corresponding to safe and unsafe samples manually and later augment the corresponding sets with the help of GPT-4 (Achiam et al., 2023) to generate a dataset containing 500 samples corresponding to safe and unsafe prompts each. We make an evaluation subset of 100 samples from this. We present a subset of samples considered for analysis on Llama in Fig. A.15.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline
**Protocol** & **Learning Rate** & **Self Instruction** & **Unsafe** & **Unsafe** & **Unsafe** & **Unsafe** & **Unsafe** \\ \hline Unlearning & \(\eta_{S}\) & 89.9 & 13.0 & 89.3 & 10.2 & 98.6 \\ \hline DPO & \(\eta_{S}\) & 89.6 & 11.3 & 99.6 & 21.6 & 93.5 \\ \hline DPO & \(\eta_{S}\) & 100.0 & 92.7 & 98.1 & 47.9 & 96.7 \\ \hline DPO & \(\eta_{S}\) & 99.7 & 100.0 & 72.8 & 100.0 & 62.8 & 100.0 \\ \hline \end{tabular}
\end{table}
Table A.2: **Safety performance of different fine-tuning protocols on JB-MisGen attacks:** Unlearning (Liu et al., 2024), DPO (Rafailov et al., 2023) and supervised safety fine-tuning (SSFT) (Ouyang et al., 2022) with medium and small learning rates are used for performing safety fine-tuning. Instruct represents the accuracy of the model to follow instructions and Null represents model’s accuracy to output null tokens. JB-MisGen task tokens is stronger than JB-MisGen text tokens and the strength of the attack further increases on combining the two.

Figure A.8: **Demonstration of the grammar rules, task tokens and safe/unsafe dominant nodes** in the PCFG based synthetic setup. We use four PCFGs during pre-training and instruction fine-tuning, where they are similar except the leaf nodes. The safe and unsafe dominant nodes are labelled at the third level of the tree (as highlighted in the figure), where each indentation represents termination of a level in the PCFG tree. We also present two held out task tokens (\(T_{\mathrm{OOD}}\)) which are not used during safety fine-tuning.

**Task Tokens:** '(', 'c'

**Text Tokens:**: '2n', '2o', '2p', '3b', '3a', '2z', '2d', '2e', '2f', '2t', '2u', '2v', '2d', '2e', '2f', '2q', '2r', '2s', '2z', '3a', '3b', '2h', '2i', '2j'

**Output Tokens:** '2p', '2q', '2n', '2a', '2b', '2q', '2e', '2d', '2j', '2s', '2w', '2u', '2e', '2d', '2t', '2r', '3a', '3s', '3x', '2k', '2a', '2v', '2z', '3c'

**Sample:** '#', '(', 'E', '6p', '2n', '2o', '2p', '3b', '3a', '2z', '2d', '2e', '2f', '2t', '2u', '2v', '2d', '2e', '2f', '2q', '2r', '2s', '2z', '3a', '3b', '2h', '2i', '2j'', '2p', '2q', '2n', '2a', '2b', '2q', '2e', '2d', '2j', '2s', '2w', '2u', '2e', '2d', '2t', '2r', '3a', '3s', '3x', '2k', '2a', '2v', '2z', '3c', ', '&

Figure A.10: Llama-2 70B chat outputs safe generations on asking it about designing a bomb.

Figure A.9: **A sample from the PCFG setup** (Allen-Zhu & Li, 2023)Figure A.12: **Jailbreak attacks with competing objectives**, where the operand is modified to attack the model. In the input prompt there is one operator: design and two operands cycle and bomb.

Figure A.13: **Jailbreak attacks with competing objectives**, where the operator is modified to attack the model. In the input prompt there are two operators: history and design and one operand bomb

Figure A.14: **Jailbreak attacks with mismatched generalization**, where the input prompt "design a bomb" is translated into a different langauge Kannada.

Figure A.15: **Subset of samples used for analysis on Llama.**

Further Analyses to Understand Safety Fine-tuning

### Analyzing how the impact of transformation propagates over the layers

As discussed in the main paper, \(\Delta\mathrm{W}\) captures modification in a single layer of the model, thus it is imperative to understand how this change propagates with the increasing depth of the model. For this we analyze the change in the activation spaces corresponding to safety fine-tuned and instruction fine-tuned models for safe and unsafe samples. For this, we can analyze the angle of projection between the two spaces. Let \({Fo_{s}}^{\mathrm{ST}}\) be formed by stacking the post-activations \(\mathbf{a}\) corresponding to safe samples \(x_{s}\) in layer \(L\). Similarly we can define \({Fo_{u}}^{\mathrm{ST}}\), \({Fo_{s}}^{\mathrm{IT}}\) and \({Fo_{u}}^{\mathrm{IT}}\). Note that here \(\mathbf{a}\) represents the activation stream of the last text token. We discuss our observations and the derived conclusions in detail below:

**Justification:** The learned update \(\Delta\mathrm{W}\) ensures that the column space of \({Fo_{u}}^{\mathrm{ST}}\) has a large projection on the left null space of \({Fo_{u}}^{\mathrm{IT}}\) for \(L\geq t\) where \(t\) is large and generally corresponds to the last few layers of the model.

**Experimental setup:** Let \(v_{1},\dots,v_{t}\) be the singular vectors with non-zero singular values spanning the column space of \({Fo_{u}}^{\mathrm{ST}}\), where \(n\) corresponds either safe or unsafe set of samples. Then we calculate the \(\mathrm{sine}\) of the angle between each \(v_{i}\) and \(\mathcal{C}({Fo_{u}}^{\mathrm{IT}})\). We present these results in Fig A.16. If this value is high, it would represent a large projection of \({Fo_{u}}^{\mathrm{ST}}\) on \(\mathcal{N}_{L}({Fo_{u}}^{\mathrm{IT}})\)

**Conclusion:** The angle of projection of \(v_{i}\) increases with the increase in layer number \(L\) and with the decrease in corresponding singular value. This suggests that the unsafe activations are being _steadily_ projected into the left null space of their original activations calculated using instruction fine-tuned model. Similar trend is not observed for safe activations, thereby showing that the update \(\Delta\mathrm{W}\) primarily modifies the unsafe activations and this effect increases with increase in depth of the model. To corroborate these results, we perform this analysis on Llama-2 7B chat in Fig A.18.

### Additional Results on Llama-2

We present additional evidence corroborating our analysis on the proposed synthetic setup by using Llama-2 7B and Llama-2 7B chat models. Llama-2 7B is a pre-trained model and Llama-2 7B chat is the fine-tuned version of Llama-2 7B, where the fine-tuning involves both instruction as well as safety fine-tuning. _Note that the instruction fine-tuned version is not officially released, which hinders our analysis on \(\Delta\mathrm{W}\)_. As a result, we use the pre-trained model Llama-2 7B. We will now present the results discussed in the main paper for Llama-2.

Clustering analysis:As shown in Fig A.75, 3 we observe that on fine-tuning, Llama learns to form separate clusters for safe and unsafe samples, which is not observed in case of pre-trained model ie. Llama-2 7B.

Analysis on \(\Delta\mathrm{W}\):As shown in Fig A.17, we observe that the projection of basis vectors spanning the column space of the learned update for any layer of Llama-2 7B chat lies largely in the null space of Llama-2 7B.

Analyzing the activation spaces for safe and unsafe samples:We find the angle of projection of top basis vectors spanning the column space of activations in Llama-2 7B chat, onto the activation space of Llama-2 7B. As shown in Fig A.18, similar to our observations in the proposed synthetic setup, we observe that the sine of the angle of projection is higher for unsafe samples than for the safe ones.

Figure A.16: **Safety fine-tuning learns to project unsafe activations into a space with high projection on null space of original activations.** The x-axis represents the singular vectors corresponding to \({Fo}^{\mathrm{ST}}\) sorted in decreasing order of their corresponding singular values. The \(\mathrm{sine}\) of the angle of projection (as shown by y-axis) is higher for activations corresponding to unsafe samples and this angle increases with increase in depth of the model as well as on using stronger safety fine-tuning protocols.

Figure A.17: \(\Delta\mathrm{W}\) **has a large projection in left null space of corresponding weights of Llama-2 7B, where \(\Delta\mathrm{W}\) is the difference between the weights of Llama-2 7b chat and Llama-2 7B models at any layer \(L\). The y-axis represents the sine of the angle of projection and x-axis represents the corresponding singular values sorted in decreasing order. As observed most of the basis vectors of \(\Delta\mathrm{W}\) have large projection angle with the column space of corresponding weights of Llama-2 7b model. Different plots corresponds to different transformer blocks of the Llama-2 model, starting from the first block represented by the first plot till the thirty second block represented by the last plot. The block number increases by one on moving towards the right.**

Figure A.18: **Projection angles between the activation spaces corresponding to Llama-2 7B and Llama-2 7B chat models.** The y-axis represents the \(\mathrm{sine}\) of angle of projection of basis vectors spanning the activation space of Llama-2 7B chat on the activation space of Llama-2 7B at some layer \(L\). Unsafe samples have a larger angle of projection than safe samples, thus indicating that fine-tuning modifies the space spanned by unsafe samples more than the safe samples. Further, this projection increases with the increase in depth of the layer. Different plots corresponds to different transformer blocks of the Llama-2 model, starting from the first block represented by the first plot till the thirty second block represented by the last plot. The block number increases by one on moving towards the right.

### Additional Results on the synthetic setup

In this section, we will discuss additional results on the proposed PCFG based synthetic setup supporting our analysis presented in the main paper. First, we will discuss the learning dynamics of \(\Delta\mathrm{W}\) in Sec C.3.1. Next we will perform the clustering analysis for \(\mathrm{W}\) and \(\bar{\mathrm{W}}\) in Sec C.3.2 and Sec C.3.5 respectively. Similar to the discussion presented in the main paper, we analyze the impact of different safety fine-tuning methods on the parameter space of \(\mathrm{W}\) and \(\bar{\mathrm{W}}\) in Sec C.3.3 and Sec C.3.6 respectively. We present a detailed analysis on different jailbreaking attacks in Sec C.3.4. Finally, we present detailed analysis of adversarial attacks on our setup in Sec C.3.8, where we perform fine-grained analysis on our observations by varying the strength of the attack. We will first analyze how \(\Delta\mathrm{W}\) is learned by the model over the course of training.

#### c.3.1 Analysis of learning dynamics

We analyze the learning dynamics of \(\Delta\mathrm{W}\) for observations 2 and 3 discussed in the main paper.

**The spread of unsafe samples in the feature space becomes low rank with the advent of training:** We analyze the spread of the two clusters. For this, we calculate the empirical covariance for both the clusters as follow:

\[\Sigma^{U}=\sum_{\mathbf{x}\in\mathcal{D}_{U}}[(\bar{\mathbf{a}}_{L}^{\circ} (\mathbf{x})[q]-\mu_{L}^{U})(\bar{\mathbf{a}}_{L}^{\circ}(\mathbf{x})[q]-\mu_ {L}^{U})^{T}]\ \,\ \ \Sigma^{S}=\sum_{\mathbf{x}\in\mathcal{D}_{S}}[(\bar{\mathbf{a}}_{L}^{\circ} (\mathbf{x})[q]-\mu_{L}^{S})(\bar{\mathbf{a}}_{L}^{\circ}(\mathbf{x})[q]-\mu_ {L}^{S})^{T}]\]

We let \(q=1\) and perform singular value decomposition (SVD) of \(\Sigma^{U}\) and \(\Sigma^{S}\) for checkpoints at different safety fine-tuning iterations for DPO (\(\eta_{M}\)) and plot the top-15 singular values in Fig A.19. We observe that as the safety fine-tuning converges, the scaling effect of the top singular vector of \(\Sigma^{U}\) becomes more dominant as compared to the other singular vectors. This is also evident from the spectral norm of \(\Sigma^{U}\), which constitutes over 62% of its nuclear norm, whereas in case of \(\Sigma^{S}\) this is only 12%. This indicates that the empirical rank of the space corresponding to unsafe samples has lowered down, whereas it remains similar in case of safe samples (See Fig A.19). Note that the empirical rank is computed by choosing the minimum value of \(r\) such that \(99\%\) of variance is preserved, implying, \(\sum_{i=1}^{r}\sigma_{i}^{2}\geq 0.99\|\mathrm{W}\|_{\mathrm{F}}^{2}\)1 We demonstrate that these observations are consistent with other safety fine-tuning protocols and transformer blocks in Fig A.20, A.21, A.22. This analysis shows that safety fine-tuning encourages the model to lower down the spread of features corresponding to unsafe samples, while the spread remains similar for safe samples.

Footnote 1: \(\|\mathrm{W}\|_{\mathrm{F}}^{2}\) is the sum of the square of all the singular values of \(\mathrm{W}\).

**The update \(\Delta\mathrm{W}\) aligns with \(\mathcal{N}_{L}(\mathrm{W}_{\mathrm{IT}})\) slowly over the course of safety fine-tuning:** This transition is shown in Fig A.23, A.24 and A.25. We analyze the learning dynamics at 100, 500, 1K, 2.5K, 5K and 10K iters.

**The update \(\Delta\mathrm{W}\) becomes more specialized for unsafe samples with the advent of training:** This transition is shown in Fig A.26, A.27 and A.28. We analyze the learning dynamics at 100, 500, 1K, 2.5K, 5K and 10K iters.

Figure A.19: **Analyzing how safety fine-tuning encourages the model to enhance the spread of features corresponding to unsafe samples in a single direction, while the spread remains similar for safe samples. The x-axis shows the index of the top-15 basis vectors of \(\Sigma^{U}\) (empirical covariance matrix corresponding to unsafe samples) and \(\Sigma^{S}\) (empirical covariance matrix corresponding to safe samples). y-axis shows the singular value. Analysis is done for checkpoints corresponding to different iterations of DPO fine-tuning performed using medium learning rate. Here we only plot for the 6th transformer block.**Figure A.20: **With the advent of Unlearning (\(\eta_{M}\)), the empirical rank of the empirical covariance matrix of features corresponding to unsafe samples (\(\Sigma^{U}\)) becomes smaller.** On the other hand \(\Sigma^{S}\) is not significantly affected by the safety fine-tuning. We utilize the same experimental setup as discussed in Fig A.19. The first and second rows presents results for fifth and the sixth layers respectively.

Figure A.21: **With the advent of DPO (\(\eta_{M}\)), the empirical rank of the empirical covariance matrix of features corresponding to unsafe samples (\(\Sigma^{U}\)) becomes smaller.** On the other hand \(\Sigma^{S}\) is not significantly affected by the safety fine-tuning. We utilize the same experimental setup as discussed in Fig A.19. The first and second rows presents results for fifth and the sixth layers respectively.

Figure A.22: **With the advent of SSFT (\(\eta_{M}\)), the empirical rank of the empirical covariance matrix of features corresponding to unsafe samples (\(\Sigma^{U}\)) becomes smaller.** On the other hand \(\Sigma^{S}\) is not significantly affected by the safety fine-tuning. We utilize the same experimental setup as discussed in Fig A.19. The first and second rows presents results for fifth and the sixth layers respectively.

Figure A.26: **Pre-activations at layers 5, 6 for unsafe samples are most affected by the top-k right singular vectors spanning the row space of \(\Delta\mathrm{W}\), where unlearning safety fine-tuning** with medium learning rate is used. We utilize the same setup as discussed in Fig 5 but perform it over the iterations. As observed with the increase in number of iterations, the effect of topmost singular vector on pre-activations increases for unsafe samples, whereas it remains low for the safe ones.

Figure A.24: **Dynamics of projection of \(\Delta\mathrm{W}\) on \(\mathcal{N}_{L}(\mathrm{W}_{\mathrm{IT}})\) for DPO safety fine-tuning**, where \(\eta_{M}\) learning rate is used. We utilize the same setup as discussed in Fig 4 but perform it over the iterations. As observed with the increase in iterations, the projection of \(\Delta\mathrm{W}\) increases into \(\mathcal{N}_{L}(\mathrm{W}_{\mathrm{IT}})\).

Figure A.25: **Dynamics of projection of \(\Delta\mathrm{W}\) on \(\mathcal{N}_{L}(\mathrm{W}_{\mathrm{IT}})\) for supervised safety fine-tuning**, where \(\eta_{M}\) learning rate is used. We utilize the same setup as discussed in Fig 4 but perform it over the iterations. As observed with the increase in iterations, the projection of \(\Delta\mathrm{W}\) increases into \(\mathcal{N}_{L}(\mathrm{W}_{\mathrm{IT}})\).

Figure A.26: **Pre-activations at layers 5, 6 for unsafe samples are most affected by the top-k right singular vectors spanning the row space of \(\Delta\mathrm{W}\), where unlearning safety fine-tuning** with medium learning rate is used. We utilize the same setup as discussed in Fig 5 but perform it over the iterations. As observed with the increase in number of iterations, the effect of topmost singular vector on pre-activations increases for unsafe samples, whereas it remains low for the safe ones.

Figure A.27: **Pre-activations at layers 5, 6 for unsafe samples are most affected by the top-k right singular vectors spanning the row space of \(\Delta\mathrm{W}\), where DPO safety fine-tuning** with medium learning rate is used. We utilize the same setup as discussed in Fig 5 but perform it over the iterations. As observed with the increase in number of iterations, the effect of topmost singular vector on pre-activations increases for unsafe samples, whereas it remains low for the safe ones.

Figure A.28: **Pre-activations at layers 5, 6 for unsafe samples are most affected by the top-k right singular vectors spanning the row space of \(\Delta\mathrm{W}\), where SSFT safety fine-tuning** with medium learning rate is used. We utilize the same setup as discussed in Fig 5 but perform it over the iterations. As observed with the increase in number of iterations, the effect of topmost singular vector on pre-activations increases for unsafe samples, whereas it remains low for the safe ones.

[MISSING_PAGE_FAIL:32]

Figure A.30: **Clustering analysis for the synthetic setup** when generating samples using _unsafe dominant_ terminal nodes as root node. The y-axis represents eq 2 and x-axis represents the layer number. Further details and observations are consistent with Fig A.29.

Figure A.33: **Analyzing the correlation** between the \(\ell_{2}\) distance (shown by x-axis) between the cluster means of clusters corresponding to safe and unsafe features and the corresponding accuracy (shown by y-axis) of the model to follow instructions. Here the samples are generated by traversing through the PCFG tree using _safe dominant_ non terminal nodes as the root nodes. The three rows corresponds to \(L=4,5,6\). As observed in case of instruction fine-tuned model, there is no correlation between the accuracy and cluster separation. On performing safety fine-tuning, we can see that there is a clear correlation.

Figure A.32: **K-means clustering analysis for the synthetic setup** when generating samples using _unsafe dominant_ terminal nodes as root nodes. Further details and observations are consistent with Fig A.31.

#### c.3.3 Analyzing the impact of safety fine-tuning on the parameter space of transformation

In this section, we will analyze how the safe and unsafe activations are impacted by \(\Delta\mathrm{W}\). We will perform this analysis in two ways. First we will analyze how \(\Delta\mathrm{W}\) impacts the unsafe and safe samples. Then we will understand how this impact is propagated with the increase in depth of the model.

Unsafe activations are mostly aligned with the top basis vectors in the row space of \(\Delta\mathrm{W}\):We utilize the setup discussed in Fig 5 in the main paper. The results on different jailbreaking attacks are presented in Fig A.34, A.36, A.35, A.37, for \(\mathrm{W}\), \(L=5,6\) and correspondingly in Fig A.54, A.56, A.55, A.57, for \(\mathrm{W}\). In all cases, the features corresponding to jailbreaking samples have a low projection in the direction of top basis vectors spanning row space of \(\Delta\mathrm{W}\). This explains why they are able to bypass \(\Delta\mathrm{W}\). As a result of this, they should remain less affected by \(\Delta\mathrm{W}\) as compared to unsafe samples. We verify this below.

\(||\Delta\mathrm{W}\ \mathbf{a}||\) is higher for activations corresponding to unsafe samples:Here a represents the activation stream corresponding to the last text token. We utilize the setup discussed in Fig A.38 in the main paper. The results on different jailbreaking attacks are presented in Fig A.39, A.41 for \(L=6\) and Fig A.40, A.42 for \(L=5\) and \(\mathrm{W}\). Corresponding results for \(\mathrm{\bar{W}}\) are present in Fig A.58, A.60, A.59, A.61. We observe that \(\Delta\mathrm{W}\) makes a more prominent change in the activations corresponding to unsafe samples. On performing jailbreaking attacks, the value of \(\|\Delta\mathrm{W}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a} \mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsf{a}\mathsfFigure A.36: **Analyzing the effect of \(\Delta\)W (for \(L=5\)) on input activations.** y-axis represent \(\sigma_{i}\mathbf{v}_{i}^{\top}\)**a** averaged over the pre-activations **a**. The x axis represents the top-15 right singular vectors. Here the samples are generated by traversing through the PCFG tree using _unsafe dominant_ non terminal nodes as the root node. The first row corresponds to safe and unsafe samples, second row for safe and JB-CO-Task samples and the third row for safe and JB-CO-Text samples. As observed the unsafe samples have higher projection on the top right singular vectors and this decreases on using the jailbreaking attacks. This indicates that \(\Delta\)W is not able to generalize well to jailbreaking attacks.

Figure A.37: **Analyzing the effect of \(\Delta\)W (for \(L=4\)) on input activations.** y-axis represent \(\sigma_{i}\mathbf{v}_{i}^{\top}\)**a** averaged over the pre-activations **a**. This figure is same as Fig A.36, but plot is made for \(L=6\) instead.

Figure A.35: **Analyzing the effect of \(\Delta\)W (for \(L=4\)) on input activations.** y-axis represent \(\sigma_{i}\mathbf{v}_{i}^{\top}\)**a** averaged over the pre-activations **a**. This figure is same as Fig A.34, but plot is made for \(L=6\) instead.

Figure A.38: \(\Delta\mathrm{W}\) **strongly impacts the unsafe activations** as highlighted in yellow region. The x-axis represents the neurons sorted in increasing order of their cosine similarity value with \(\mathbf{v}_{1}\). y-axis is \(\Delta\mathrm{W}\mathbf{a}\) for each neuron. We plot for this for 6th transformer block. Clearly, the neurons on the right side of each plot (yellow region) impact the unsafe samples (red) more than safe samples (green). Further, \(||\Delta\mathrm{W}\mathbf{a}||\) for unsafe samples increases much more than that of safe samples.

Figure A.39: **Analyses of \(\Delta\mathrm{W}\) a for a single sample** randomly selected, where \(\mathbf{a}\) represents the input activation at layer \(L=6\) corresponding to the activation stream of the last text token. Each marker represents the scalar output of a neuron and the x-axis is sorted in increasing order of projection of each neuron in the direction of top right singular vector of \(\Delta\mathrm{W}\). Here the sample are generated by traversing through the PCFG tree using _safe dominant_ non terminal nodes as the root node. The first row corresponds unsafe samples, second row represents safe samples, third row represents JB-CO-Task samples and fourth row represents JB-MisGen samples. Note that the jailbreaking attacks are generated by modifying the same unsafe sample. As observed \(||\Delta\mathrm{W}\ \mathbf{a}||\) is highest for unsafe samples and it decreases on using jailbreaking attacks. Further, the neurons aligned more with the top right singular vector of \(\Delta\mathrm{W}\) contribute more towards the norm of \(||\Delta\mathrm{W}\ \mathbf{a}||\).

Figure A.38: \(\Delta\mathrm{W}\) **strongly impacts the unsafe activations** as highlighted in yellow region. The x-axis represents the neurons sorted in increasing order of their cosine similarity value with \(\mathbf{v}_{1}\). y-axis is \(\Delta\mathrm{W}\mathbf{a}\) for each neuron. We plot for this for 6th transformer block. Clearly, the neurons on the right side of each plot (yellow region) impact the unsafe samples (red) more than safe samples (green). Further, \(||\Delta\mathrm{W}\mathbf{a}||\) for unsafe samples increases much more than that of safe samples.

Figure A.41: **Analyses of \(\Delta\mathrm{W}\) a for a single sample** randomly selected, where **a** represents the input activation at layer \(L=6\) corresponding to the activation stream of the last text token. Each marker represents the scalar output of a neuron and the x-axis is sorted in increasing order of projection of each neuron in the direction of top right singular vector of \(\Delta\mathrm{W}\). Here the sample are generated by traversing through the PCFG tree using _unsafe dominant_ non terminal nodes as the root node. The first row corresponds unsafe samples, second row represents safe samples, third row represents JB-CO-Task samples and fourth row represents JB-CO-Text samples. Note that the jailbreaking attacks are generated by modifying the same unsafe sample. As observed \(||\Delta\mathrm{W}\;\mathbf{a}||\) is highest for unsafe samples and it decreases on using jailbreaking attacks. Further, the neurons aligned more with the top right singular vector of \(\Delta\mathrm{W}\) (as shown by the leftmost part of each plot) contribute more towards the norm of \(||\Delta\mathrm{W}\;\mathbf{a}||\).

Figure A.42: **Analyses of \(\Delta\mathrm{W}\) a for a single sample** randomly selected, where **a** represents the input activation at layer \(L=5\) corresponding to the activation stream of the last text token. The setup used for figure is same as Fig A.41, but plot is made for \(L=5\) instead.

Figure A.43: **Analysis of the projection of top basis vectors in the activation space corresponding to \(\mathrm{W_{ST}}\) onto the activation space of \(\mathrm{W_{IT}}\) for layers \(L=5,6\). Here the sample are generated by traversing through the PCFG tree using _safe dominant_ non terminal nodes as the root nodes. The first row corresponds unsafe samples, second row for safe, third row for JB-CO-Task samples and fourth row for safe and JB-MiSGen samples. On performing jailbreaking attack the angle of projection decreases as compared to unsafe samples.**

[MISSING_PAGE_FAIL:40]

Figure A.48: **Comparison between different jailbreaking attacks and adversarial for unlearning (\(\eta_{S}\)).** The experimental setup and observations are same as described in Fig A.45.

Figure A.49: **Comparison between different jailbreaking attacks and adversarial for SSFT (\(\eta_{S}\)).** The experimental setup and observations are same as described in Fig A.45.

Figure A.47: **Comparison between different jailbreaking and adversarial attacks for DPO (\(\eta_{S}\)).** The experimental setup and observations are same as described in Fig A.45.

#### c.3.5 Clustering analysis for jailbreaking attacks on the second MLP layer in the transformer block

In this section, we repeat our experiments analyzing the feature space of the model for the second MLP layer in the transformer block. We find that our previous analysis about \(\Delta\mathrm{W}\) also holds on the second MLP layer \(\bar{\mathrm{W}}\).

Figure A.51: **Clustering analysis for the synthetic setup** when generating samples using _unsafe dominant_ terminal nodes as root nodes. The y-axis represents eq 2 and x-axis represents the layer number. Further details and observations are consistent with Fig A.50

Figure A.52: **Analyzing the correlation** between the \(\ell_{2}\) distance (shown by x-axis) between the cluster means of clusters corresponding to safe and unsafe features and the corresponding accuracy (shown by y-axis) of the model to follow instructions. Here the samples are generated by traversing through the PCFG tree using _safe dominant_ non terminal nodes as the root nodes. The three rows corresponds to \(L=4,5,6\). As observed in case of instruction fine-tuned model, there is no correlation between the accuracy and cluster separation. On performing safety fine-tuning, we can see that there is a clear correlation.

Figure A.50: **Clustering analysis for the synthetic setup** when generating samples using _safe dominant_ terminal nodes as root nodes. The y-axis represents eq 2 and x-axis represents the layer number. The first row shows the clustering analysis between safe and unsafe samples, second row for safe and JB-CO-Task samples and third row for safe and JB-MisGen samples. As observed the cluster separation decreases on performing jailbreaking attacks.

c.3.6 Analyzing the impact of safety fine-tuning on parameter space of the second MLP layer in the transformer block

In this section, we repeat our experiments analyzing the parameter space of the model for the second MLP layer in the transformer block. We find that our previous analysis about \(\Delta\mathrm{W}\) also holds on the second MLP layer \(\mathrm{W}\).

Figure A.54: **Analyzing the effect of \(\Delta\mathrm{W}\) (for \(L=5\)) on input activations. y-axis represent \(\sigma_{i}\mathbf{v}_{i}^{\top}\mathbf{a}\) averaged over the pre-activations \(\mathbf{a}\). The x axis represents the top-15 right singular vectors. Here the samples are generated by traversing through the PCFG tree using _safe dominant_ non terminal nodes as the root node. The first row corresponds to safe and unsafe samples, second row for safe and JB-CO-Task samples and the third row for safe and JB-MisGen samples. As observed the unsafe samples have higher projection on the top right singular vectors and this decreases on using the jailbreaking attacks. This indicates that \(\Delta\mathrm{W}\) is not able to generalize well to jailbreaking attacks.**

Figure A.55: **Analyzing the effect of \(\Delta\mathrm{W}\) (for \(L=4\)) on input activations. y-axis represent \(\sigma_{i}\mathbf{v}_{i}^{\top}\mathbf{a}\) averaged over the pre-activations \(\mathbf{a}\). This figure is same as Fig A.54, but plot is made for \(L=6\) instead.**

Figure A.56: **Analyzing the effect of \(\Delta\mathrm{W}\) (for \(L=5\)) on input activations.** y-axis represent \(\sigma_{i}\mathbf{v}_{i}^{\top}\mathbf{a}\) averaged over the pre-activations \(\mathbf{a}\). The x axis represents the top-15 right singular vectors. Here the samples are generated by traversing through the PCFG tree using _unsafe dominant_ non terminal nodes as the root node. The first row corresponds to safe and unsafe samples, second row for safe and JB-CO-Task samples and the third row for safe and JB-CO-Text samples. As observed the unsafe samples have higher projection on the top right singular vectors and this decreases on using the jailbreaking attacks. This indicates that \(\Delta\mathrm{W}\) is not able to generalize well to jailbreaking attacks.

Figure A.58: **Analyses of \(\Delta\mathrm{W}\) a for a single sample** randomly selected, where **a** represents the input activation at layer \(L=6\) corresponding to the activation stream of the last text token. Each marker represents the scalar output of a neuron and the x-axis is sorted in increasing order of projection of each neuron in the direction of top right singular vector of \(\Delta\mathrm{W}\). Here the sample are generated by traversing through the PCFG tree using _safe dominant_ non terminal nodes as the root node. The first row corresponds unsafe samples, second row represents safe samples, third row represents JB-CO-Task samples and fourth row represents JB-MisGen samples. Note that the jailbreaking attacks are generated by modifying the same unsafe sample. As observed \(||\Delta\mathrm{W}\;\mathbf{a}||\) is highest for unsafe samples and it decreases on using jailbreaking attacks. Further, the neurons aligned more with the top right singular vector of \(\Delta\mathrm{W}\) (as shown by the leftmost part of each plot) contribute more towards the norm of \(||\Delta\mathrm{W}\;\mathbf{a}||\).

Figure A.59: **Analyses of \(\Delta\mathrm{W}\) a for a single sample** randomly selected, where **a** represents the input activation at layer \(L=5\) corresponding to the activation stream of the last text token. The setup used for figure is same as Fig A.58, but plot is made for \(L=5\) instead.

Figure A.60: **Analyses of \(\Delta\mathrm{W}\) a for a single sample** randomly selected, where **a** represents the input activation at layer \(L=6\) corresponding to the activation stream of the last text token. Each marker represents the scalar output of a neuron and the x-axis is sorted in increasing order of projection of each neuron in the direction of top right singular vector of \(\Delta\mathrm{W}\). Here the sample are generated by traversing through the PCFG tree using _unsafe dominant_ non terminal nodes as the root node. The first row corresponds unsafe samples, second row represents safe samples, third row represents JB-CO-Task samples and fourth row represents JB-CO-Text samples. Note that the jailbreaking attacks are generated by modifying the same unsafe sample. As observed \(||\Delta\mathrm{W}\;\mathbf{a}||\) is highest for unsafe samples and it decreases on using jailbreaking attacks. Further, the neurons aligned more with the top right singular vector of \(\Delta\mathrm{W}\) (as shown by the leftmost part of each plot) contribute more towards the norm of \(||\Delta\mathrm{W}\;\mathbf{a}||\).

Figure A.62: **Analysis of the projection of top basis vectors in the activation space corresponding to \(\mathrm{W_{ST}}\) onto the activation space of \(\mathrm{W_{IT}}\) for layers \(L=5,6\). Here the sample are generated by traversing through the PCFG tree using _safe dominant_ non terminal nodes as the root nodes. The first row corresponds unsafe samples, second row for safe, third row for JB-CO-Task samples and fourth row for safe and JB-MiSGen samples. On performing jailbreaking attack the angle of projection decreases as compared to unsafe samples.**

[MISSING_PAGE_FAIL:48]

#### c.3.8 Analyzing adversarial attacks

In this section, we perform a fine grained analysis of adversarial attacks on our synthetic setup. We perform ten steps of white box attacks, where we optimize the soft tokens, which are appended at the end of the input sample after the text tokens as shown in Fig 2. The number of soft prompts appended are between 1 to 10, where appending one soft token generates the weakest attack and appending ten tokens gives the strongest attack. We generate 10 different attacks with varying attack strength by linearly increasing the number of soft tokens from 1-10. We now systematically analyze these attacks on our different experimental setups discussed below:

Feature space clustering analysis:We analyze how the separation between the clusters corresponding to safe and adversarial activations changes on increasing the attack strength in Fig A.66, A.67. We observe that the separation between the clusters corresponding to safe and adversarial samples decreases on increasing the attack strength.

Parameter space analysis by analysing projection angle between activation spaces corresponding to \(\mathbf{W}_{\mathrm{IT}}\) and \(\mathbf{W}_{\mathrm{ST}}\):We analyze how the angle of projection between the activation spaces corresponding to instruction fine-tuned model and safety fine-tuned model changes for different attack strengths in Fig A.68 and A.69. We observe that the angle of projection is higher between the activation spaces corresponding to unsafe samples and it decreases with the increase in attack strength. This demonstrates that with the increase in attack strength, similar to jailbreaking attacks, the learned update \(\Delta\mathrm{W}\) is not able to generalize well to the attacked samples. Thus the attacked samples behave similar to safe samples.

Sensitivity analysis using Lipschitzness constant:We analyze the effect of increasing the attack strength on the lipschitzness of the model for safe and adversarial samples in Fig A.70, A.71. We observe that the with the increase in attack strength, the histograms corresponding to adversarial samples move towards the safe samples and away from the unsafe ones. This shows that with the increase in attack strength the adversarial samples starts behaving similar to safe samples.

Figure A.66: **Analyzing the effect of attack strength on clustering of safe and adversarial samples.** The y-axis represent eq 2 averaged over samples and x-axis represents the layer number. From top to bottom, the strength of the adversarial attack is increased by linearly increasing the number of soft prompts from 0-10. Here the samples are generated using _safe dominant_ nodes as the root nodes. The cluster separation decreases slowly on increasing the attack strength.

Figure A.67: **Analyzing the effect of attack strength on clustering of safe and adversarial samples.** The y-axis represent eq 2 averaged over samples and x-axis represents the layer number. From top to bottom, the strength of the adversarial attack is increased by linearly increasing the number of soft prompts from 0-10. Here the samples are generated using _unsafe dominant_ nodes as the root nodes. The cluster separation decreases slowly on increasing the attack strength.

Figure A.68: **Analyzing the effect of attack strength on the angle of projection between the feature spaces corresponding to \(\mathrm{W_{IT}}\) and \(\mathrm{W_{ST}}\).** The y-axis denotes the sine of the angle of projection of right singular vectors spanning the features row space of \(\mathrm{W_{ST}}\) onto the feature space of \(\mathrm{W_{IT}}\) for layers 5,6. From top to bottom, the strength of the adversarial attack is increased by linearly increasing the number of soft prompts from 0-10. Here the samples are generated using _safe dominant_ nodes as the root nodes. The projection angle becomes smaller on increasing the attack strength, thereby indicating that \(\Delta\mathrm{W}\) is not able to generalize well to adversarial samples.

Figure A.69: **Analyzing the effect of attack strength on the angle of projection between the feature spaces corresponding to \(\mathrm{W_{IT}}\) and \(\mathrm{W_{ST}}\).** The y-axis denotes the sine of the angle of projection of right singular vectors spanning the features row space of \(\mathrm{W_{ST}}\) onto the feature space of \(\mathrm{W_{IT}}\) for layers 5,6. From top to bottom, the strength of the adversarial attack is increased by linearly increasing the number of soft prompts from 0-10. Here the samples are generated using _unsafe domain_ nodes as the root nodes. The projection angle becomes smaller on increasing the attack strength, thereby indicating that \(\mathrm{\Delta W}\) is not able to generalize well to adversarial samples.

Figure A.70: **Effect of attack strength on the local lipschitzness of safety fine-tuned models for safe and adversarial samples.** From top to bottom, the strength of the adversarial attack is increased by linearly increasing the number of soft prompts from 0-10. Here the samples are generated using _safe dominant_ nodes as the root node. With the increasing attack strength, the histogram for adversarial samples move towards the safe samples, demonstrating that as the attack becomes stronger, the adversarial samples start behaving similar to the safe samples. Thus they start following instructions.

Figure A.71: **Effect of attack strength on the local lipschitzness of safety fine-tuned models for safe and adversarial samples.** From top to bottom, the strength of the adversarial attack is increased by linearly increasing the number of soft prompts from 0-10. Here the samples are generated using _unsafe dominant_ nodes as the root node. With the increasing attack strength, the histogram for adversarial samples move towards the safe samples, demonstrating that as the attack becomes stronger, the adversarial samples start behaving similar to the safe samples. Thus they start following instructions.

Additional Results Using Interventions

In this section, we will analyze the effect of interpolating and extrapolating in the direction of the learned \(\Delta\mathrm{W}\). As discussed in Sec 4.2 in the main paper, our intervention is defined as

\[\mathrm{W}^{\alpha}_{\mathrm{IT}}=\mathrm{W}_{\mathrm{IT}}+\alpha\Delta\mathrm{W}\] (A.3)

We perform analysis for different values of \(\alpha\) in the set \(\{0,0.25,0.5,0.75,1,1.1,1.2,1.3,1.4,1.5\}\)

Impact on the safety performance:We analyze how the performance of the model changes on the safe, unsafe and jailbreaking samples as we interpolate or extrapolate in the direction of \(\Delta\mathrm{W}\) in Fig A.72. We observe that in case of weak safety fine-tuning protocols like supervised safety fine-tuning (SSFT)it is possible to decrease the vulnerability of the model against jailbreaking attacks while maintaining its performance on the safe samples. In case of DPO and unlearning such a trend is not observed. This highlights, that simply extrapolating in the direction of \(\Delta\mathrm{W}\) could make models safer thereby leading to enhanced data and compute efficiency.

Next, we perform an additional intervention, where instead of traversing between the instruction and safety fine-tuned models, traversal is done between two safety fine-tuned models which are fine-tuned using different safety fine-tuning methods. We present these results in Fig A.73. As observed all these different safety fine-tuned models are linearly connected in the parameter space which indicates that they lie in the same loss basin. On moving from a weaker safety fine-tuning method like SSFT towards a stronger one like unlearning, we observe that the attack success rate decreases slowly.

Finally, we perform another additional intervention \(\mathrm{W}^{\alpha}_{\mathrm{ST}}=\mathrm{W}_{\mathrm{ST}}+\alpha\Delta\mathrm{W}\), where we analyze the transferability of \(\Delta\mathrm{W}\) on models fine-tuned using different safety fine-tuning methods. We present these results in Fig A.74, where we observe that it is possible to improve the performance of weaker safety fine-tuning protocols like SSFT against jailbreaking attacks, while preserving the performance on safe samples. These results highlight that using \(\Delta\mathrm{W}\) learned via different safety fine-tuning methods could improve the performance of safety fine-tuning methods. We pose this as an interesting future direction.

Feature space analysis:We perform linear mode connectivity analysis for different values of \(\alpha\) and present the results for Llama-2 7B and for the proposed synthetic setup in Fig A.75, A.76, A.77. We observe that in all cases as we move in the direction of \(\Delta\mathrm{W}\), by increasing the value of \(\alpha\), the separation between the clusters of safe and unsafe samples increases. Additionally to understand the relative effect of separation between the two clusters along with their compactness, we use fisher criterion Bishop (2006) and present the results in Fig A.84, A.85. As observed, the value of the fisher criteria increases on traversing in the direction of \(\Delta\mathrm{W}\), thus indicating that the ratio between the separation of the two clusters and their compactness is increasing.

We also analyze how the spread of the two clusters changes on increasing the value of \(\alpha\) in Fig A.78, A.79. We observe that with the increase in value of \(\alpha\), in case of cluster corresponding to unsafe samples, the spread becomes more dominant in a single direction, which results in reduction of the empirical rank of the corresponding empirical covariance matrix.

Parameter space analysis:Next, we analyze the effect of safety fine-tuning on the angle of projection between activation spaces corresponding to safety fine-tuned and instruction fine-tuned models. We calculate these activation spaces for both safe as well as unsafe samples. The corresponding plots are presented in Fig A.80 and A.81. We observe that the angle of projection is higher for the activation spaces corresponding to unsafe samples and it linearly increases on traversing in the direction of \(\Delta\mathrm{W}\).

Sensitivity analysis:We compute how the lipschitzness of the model for safe and unsafe samples changes as we move in the direction of \(\Delta\mathrm{W}\) in Fig A.82, A.83. We observe that increasing the value of \(\alpha\) separates the histograms corresponding to safe and unsafe samples further apart, where the lipschitzness of the model decreases for the unsafe samples and increases for the safe samples.

Figure A.72: **Improving safety performance by performing interventions. y-axis represents the performance of the model (scaled to 0-1) when following instructions on safe/unsafe/jailbreaking samples represented by their respective colours used in this paper. The x-axis represents the value of \(\alpha\) as used in \(\mathrm{W}^{\alpha}_{\mathrm{IT}}=\mathrm{W}_{\mathrm{IT}}+\alpha\Delta \mathrm{W}\). Using \(\alpha>1\) can help in further enhancing the safety performance of the safety fine-tuned models. In case of SSFT such an improvement is possible with minimal loss in performance on safe samples.**

Figure A.73: **Traversing between different safety fine-tuned models.** Here we traverse between the two safety fine-tuned models. For instance in case of DPO - Unlearning, we traverse from DPO safety fine-tuned model to the unlearning find-tuned one. Negative values of the interpolation weight \(\alpha\) on the x-axis, would mean extrapolation in the direction of DPO and positive value of \(\alpha\) means extrapolation in the direction of unlearning. As observed on traversing from a weaker safety fine-tuning protocol like SSFT towards a stronger one like unlearning reduces the attack success rate of jailbreaking attacks (shown in brown), while maintaining the accuracy on clean samples.

Figure A.74: **Transferability of \(\Delta\mathrm{W}\) between different safety fine-tuning methods.** We use a naming convention where Unlearning - DPO means \(\mathrm{W}_{\mathrm{ST}}\) is obtained using unlearning and \(\Delta\mathrm{W}\) is learned by DPO. Similarly for others. x-axis represents different values of \(\alpha\) and y-axis represents accuracy scaled between \(0-1\). High accuracy of green curve represents good performance on safe samples, and high accuracy of brown curve represents high jailbreaking attack success rate. We observe that it is possible to reduce the attack success rate of jailbreaking attacks (shown in brown), while maintaining the accuracy on clean samples (shown in green) when traversing in the direction of \(\Delta\mathrm{W}\) corresponding to stronger safety fine-tuning protocol like unlearning.

Figure A.75: **Linear mode connectivity analysis of Clustering of safe and unsafe activation in Llama-2 7B models** The y-axis represents eq 2 averaged over samples and the x-axis represents layer number. Here we traverse from the pre-trained Llama-2 7B model to the instruction and safety fine-tuned Llama-2 7B chat where moving left to right represents increasing values of \(\alpha\) used in eq A.3. The values of \(\alpha\) are given by {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5}. The cluster separation increases as we traverse in the direction of \(\Delta\mathrm{W}\)

Figure A.76: **Linear mode connectivity analysis of clustering of safe and unsafe activations in our synthetic setup**, where the samples are generated using _safe dominant_ terminal nodes as root node. The y-axis represents eq 2 averaged over samples and the x-axis represents layer number. From top to bottom, the values of \(\alpha\) are given by {0, 0.25, 0.5, 0.75 1, 1.1, 1.2, 1.3, 1.4, 1.5}. The cluster separation increases as we traverse in the direction of \(\Delta\mathrm{W}\)

Figure A.77: **Linear mode connectivity analysis of clustering of safe and unsafe activations in our synthetic setup**, where the samples are generated using _unsafe dominant_ terminal nodes as root node. The y-axis represents eq 2 averaged over samples and the x-axis represents the layer number. From top to bottom, the values of \(\alpha\) are given by {0, 0.25, 0.5, 0.75 1, 1.1, 1.2, 1.3, 1.4, 1.5}. The cluster separation increases as we traverse in the direction of \(\Delta\mathrm{W}\).

Figure A.78: **Linear mode connectivity analysis of singular values of the empirical covariance matrix corresponding to the features space of safe and unsafe samples**, where the samples are generated using _safe dominant_ terminal nodes as root node. The y-axis denotes the singular values of the covariance matrix calculated in the 5th layer of the model. From top to bottom, the values of \(\alpha\) are given by {0, 0.25, 0.5, 0.75 1, 1.1, 1.2, 1.3, 1.4, 1.5}. A single direction corresponding to the topmost singular value becomes dominant as we traverse in the direction of in the direction of \(\Delta\mathrm{W}\). This results in lowering the empirical rank of the feature space corresponding to unsafe samples, whereas the empirical rank for the feature space corresponding to safe samples remains almost same.

Figure A.79: **Linear mode connectivity analysis of singular values of the empirical covariance matrix corresponding to the features space of safe and unsafe samples**, where the samples are generated using _safe dominant_ terminal nodes as root node. The y-axis denotes the singular values of the covariance matrix calculated in the 6th layer of the model. From top to bottom, the values of \(\alpha\) are given by {0, 0.25, 0.5, 0.75 1, 1.1, 1.2, 1.3, 1.4, 1.5}. A single direction corresponding to the topmost singular value becomes dominant as we traverse in the direction of in the direction of \(\Delta\mathrm{W}\). This results in lowering the empirical rank of the feature space corresponding to unsafe samples, whereas the empirical rank for the feature space corresponding to safe samples remains almost same.

Figure A.80: **Linear mode connectivity analysis of \(\mathrm{sine}\) of projection angle between the activation spaces corresponding to instruction fine-tuned and safety fine-tuned models**, where the samples are generated using _safe dominant_ terminal nodes as root node. The y-axis denotes the \(\mathrm{sine}\) of the angle of projection of right singular vectors spanning the features row space of \(\mathrm{W_{ST}}\) onto the feature space of \(\mathrm{W_{IT}}\) for layers 5,6. From top to bottom, the values of \(\alpha\) are given by {0, 0.25, 0.5, 0.75 1, 1.1, 1.2, 1.3, 1.4, 1.5}. The angle of projection is always higher for unsafe samples as compared to safe samples and it increases on traversing in the direction of \(\Delta\mathrm{W}\).

Figure A.81: **Linear mode connectivity analysis of \(\mathrm{sine}\) of projection angle between the activation spaces corresponding to instruction fine-tuned and safety fine-tuned models**, where the samples are generated using _unsafe dominant_ terminal nodes as root node. The y axis denotes the \(\mathrm{sine}\) of the angle of projection of right singular vectors spanning the features row space of \(\mathrm{W_{ST}}\) onto the feature space of \(\mathrm{W_{IT}}\) for layers 5,6. From top to bottom, the values of \(\alpha\) are given by {0, 0.25, 0.5, 0.75 1, 1.1, 1.2, 1.3, 1.4, 1.5}. The angle of projection is always higher for unsafe samples as compared to safe samples and it increases on traversing in the direction of \(\Delta\mathrm{W}\).

Figure A.82: **Linear mode connectivity analysis of the local lipschitz constant of safe and unsafe activations in our synthetic setup**, where the samples are generated using _safe dominant_ terminal nodes as root node. The histogram represents the local lipschitzness for safe and unsafe samples. From top to bottom, the values of \(\alpha\) are given by {0, 0.25, 0.5, 0.75 1, 1.1, 1.2, 1.3, 1.4, 1.5}. The local lipschitzness for unsafe samples decreases and increases for safe samples on traversing in the direction of \(\Delta\mathrm{W}\).

Figure A.83: **Linear mode connectivity analysis of the local lipschitz constant of safe and unsafe activations in our synthetic setup**, where the samples are generated using _unsafe dominant_ terminal nodes as root node. The histogram represents the local lipschitzness for safe and unsafe samples. From top to bottom, the values of \(\alpha\) are given by {0, 0.25, 0.5, 0.75 1, 1.1, 1.2, 1.3, 1.4, 1.5}. The local lipschitzness for unsafe samples decreases and increases for safe samples on traversing in the direction of \(\Delta\mathrm{W}\).

Figure A.84: **Linear mode connectivity analysis of fisher criteria Bishop (2006)**, where the x axis represents the value of \(\alpha\) and the y-axis represents the value of fisher criteria. Higher value indicates larger ratio of cluster separation and clusters compactness. The first row shows the value of fisher criteria for clusters of safe and unsafe samples, second row shows the same for safe and JB-CO-Task samples and third row represents for clusters of safe and JB-MisGen samples. Here the samples are generated using _safe dominant_ terminal nodes as root node.

Figure A.85: **Linear mode connectivity analysis of fisher criteria Bishop (2006)**, where the x axis represents the value of \(\alpha\) and the y-axis represents the value of fisher criteria. Higher value indicates larger ratio of cluster separation and clusters compactness. The first row shows the value of fisher criteria for clusters of safe and unsafe samples, second row shows the same for safe and JB-CO-Task samples and third row represents for clusters of safe and JB-CO-Text samples. Here the samples are generated using _unsafe dominant_ terminal nodes as root node.

## Appendix E Limitations and Societal Impact

Limitations:It would be good to verify our results on additional large language models. Unfortunately, this requires the use of instruction fine-tuned models, which are generally not public. For instance in case of Llama-2, only the pre-trained (Llama-2 7B) and safety fine-tuned models (Llama-2 chat 7B) are officially released by Meta. Additionally, we don't observe any clear differences between the mechanisms used by different types of jailbreaks to circumvent the safety of the considered models. It would be interesting to analyze this in greater detail.

Societal ImpactsOur analysis present in this work can help in motivating improved safety fine-tuning protocols, which can lead to positive societal impacts.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The claims are included in the introduction and abstract Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Refer to Sec. E in appendix. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: The paper does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems.

* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We present details about the proposed synthetic setup in Sec. B of appendix. Also refer to https://github.com/fiveai/understanding_safety_finetuning for additional details. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general, releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes]. Justification: Refer to https://github.com/fiveai/understanding_safety_finetuning. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.

* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Refer to Appendix Sec. B.1.3. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Wherever possible, we plot the standard deviation along with the means in the plots. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: For all interpretability experiments we use a RTX A4500 gpu with a memory of 20 GB. For pre-training using the synthetic setup, we use a RTX A6000 gpu with a memory of 48 GB. Additional details on training time are present in Appendix Sec. B.1.3. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms with the NeurIPS Code of Ethics Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Refer to Sec. E in appendix. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: This work does not poses any such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.

* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
1. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA]. Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
1. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes]. Justification: Refer to https://github.com/fiveai/understanding_safety_finetuning. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
1. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
1. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.