# Consistent Aggregation of Objectives with Diverse Time Preferences Requires Non-Markovian Rewards

 Silivu Pitis

University of Toronto and Vector Institute

spitis@cs.toronto.edu

###### Abstract

As the capabilities of artificial agents improve, they are being increasingly deployed to service multiple diverse objectives and stakeholders. However, the composition of these objectives is often performed ad hoc, with no clear justification. This paper takes a normative approach to multi-objective agency: from a set of intuitively appealing axioms, it is shown that Markovian aggregation of Markovian reward functions is not possible when the time preference (discount factor) for each objective may vary. It follows that optimal multi-objective agents must admit rewards that are non-Markovian with respect to the individual objectives. To this end, a practical non-Markovian aggregation scheme is proposed, which overcomes the impossibility with only one additional parameter for each objective. This work offers new insights into sequential, multi-objective agency and intertemporal choice, and has practical implications for the design of AI systems deployed to serve multiple generations of principals with varying time preference.

## 1 Introduction

The idea that we can associate human preferences with scalar utility values traces back hundreds of years and has found usage in numerous applications [9; 71; 28; 49]. One of the most recent, and perhaps most important, is the design of artificial agents. In the field of reinforcement learning (RL), this idea shows up as the _reward hypothesis_[74; 67; 10], which lets us define objectives in terms of a discounted sum of Markovian rewards. While foundational results from decision theory [81; 62] and inverse RL [52; 54] justify the reward hypothesis when a single objective or principal is considered, complexities arise in multi-objective scenarios [61; 77]. The literature on social choice is largely defined by impossibilities [5], and multi-objective composition in the RL and machine learning literature is typically restrictive [68; 51], applied without clear justification [29], or based on subjective evaluations of empirical efficacy [21]. Addressing these limitations is crucial for the development of artificial agents capable of effectively serving the needs of diverse stakeholders.

This paper extends previous normative work in RL by adopting an axiomatic approach to the aggregation of objectives. The approach is based on a set of intuitively appealing axioms: the von Neumann-Morgenstern (VNM) axioms, which provide a foundation for rational choice under uncertainty; Pareto indifference, which efficiently incorporates individual preferences; and dynamic consistency, which ensures time-consistent decision-making. From these axioms, an impossibility is derived, leading to the conclusion that optimal multi-objective agents with diverse time preferences must have rewards that are non-Markovian with respect to the individual objectives. To address this challenge, a practical state space expansion is proposed, which allows for the Markovian aggregation of objectives requiring only one parameter per objective. The results prompt an interesting discussion on dynamic preferences and intertemporal choice, leading to a novel "historical discounting" strategy that trades off dynamic consistency for intergenerational fairness. Finally, it is shown how both our results can be extended (albeit non-normatively) to stochastic policies.

The remainder of this paper is organized as follows: Section 2 motivates the problem by modeling human procrastination behavior as an aggregation of two objectives, work and play, and showing how a plan that appears optimal today may lead to the worst possible future outcome. Section 3 presents the axiomatic background and the key impossibility result. Section 4 presents the corresponding possibility result and a practical state expansion to implement it. Section 5 relates the results to intertemporal choice, proposes \(N\)-step commitment and historical discounting strategies for managing intergenerational tradeoffs, extends the results to stochastic policies, and discusses related topics in RL. Section 6 concludes with some final thoughts and potential future research directions.

## 2 Motivation: The Procrastinator's Peril

We begin with a numerical example of how the naive aggregation of otherwise rational preferences can lead to undesirable behavior. The example, which will be referred to throughout as the "Procrastinator's Peril", involves repeated procrastination, a phenomenon to which the reader might relate. An agent aggregates two competing objectives: work and play. At each time step the agent can choose to either work or play. The pleasure of play is mostly from today, and the agent doesn't value future play nearly as much as present play. On the other hand, the consequences of work are delayed, so that work tomorrow is valued approximately as much as work today.

Let us model the agent's preferences for work and play as two separate Markov Decision Processes (MDP), each with state space \(\mathcal{S}=\emptyset\) and action space \(\mathcal{A}=\{\mathtt{w},\mathtt{p}\}\). In the play MDP, we have rewards \(R(\mathtt{p})=0.5\), \(R(\mathtt{w})=0\) and a discount factor of \(\gamma_{\mathtt{play}}=0.5\). In the work MDP, we have rewards \(R(\mathtt{p})=0\), \(R(\mathtt{w})=0.3\) and a discount factor of \(\gamma_{\mathtt{work}}=0.9\). One way to combine the preferences for work and play is to value each trajectory under both MDPs and then add up the values. Not only does this method of aggregation seem reasonable, but it is actually _implied_ by some mild and appealing assumptions about preferences (Axioms 1 and 3 in the sequel). Using this approach, the agent assigns values to trajectories as follows:

\begin{tabular}{l l l l} \hline \hline \(\tau_{1}\) & \(\mathtt{p},\mathtt{p},\mathtt{p},\mathtt{p}...\) & \(V(\tau_{1})=\sum_{t}(0.5)^{t}\cdot 0.5\) & \(=1.00\) \\ \(\tau_{2}\) & \(\mathtt{w},\mathtt{w},\mathtt{w},\mathtt{w}...\) & \(V(\tau_{2})=\sum_{t}(0.9)^{t}\cdot 0.3\) & \(=3.00\) \\ \(\tau_{3}\) & \(\mathtt{p},\mathtt{w},\mathtt{w},\mathtt{w}...\) & \(V(\tau_{3})=0.5+0.9\cdot V(\tau_{2})\) & \(=3.20\) \\ \(\tau_{4}\) & \(\mathtt{p},\mathtt{p},\mathtt{w},\mathtt{w}...\) & \(V(\tau_{3})=0.75+0.9^{2}\cdot V(\tau_{2})\) & \(=3.18\) \\ \hline \hline \end{tabular}

We see that the agent most prefers \(\tau_{3}\): one period (and one period only!) of procrastination is optimal. Thus, the agent procrastinates and chooses to play today, planning to work from tomorrow onward. Come tomorrow, however, the agent is faced with the same choice, and once again puts off work in favor of play. The process repeats and the agent ends up with the least preferred alternative \(\tau_{1}\).

This plainly irrational behavior illustrates the impossibility theorem. Observe that the optimal policy \(\tau_{3}\) is non-Markovian--it must remember that the agent has previously chosen play in order to work forever. But any MDP has a stationary optimal policy [55], so it follows that we need rewards that are non-Markovian with respect to the original state-action space. Alternatively, we will see in Subsection 4.2 that we can expand the state space to make the optimal policy Markovian.

## 3 Impossibility of Dynamically Consistent, Pareto Indifferent Aggregation

NotationWe assume familiarity with Markov Decision Processes (MDPs) [55] and reinforcement learning (RL) [74]. We denote an MDP by \(\mathcal{M}=\langle\mathcal{S},\mathcal{A},T,R,\gamma\rangle\), where \(\gamma:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{+}\) is a state-action dependent discount function. This generalizes the usual "fixed" \(\gamma\in\mathbb{R}\) and covers both the episodic and continuing settings [85]. We use lowercase letters for generic instances, e.g. \(s\in\mathcal{S}\), and denote distributions using a tilde, e.g. \(\tilde{s}\). In contrast to standard notation we write both state- and state-action value functions using a unified notation that emphasizes the dependence of each on the future policy: we write \(V(s,\pi)\) and \(V(s,a\pi)\) instead of \(V^{\pi}(s)\) and \(Q^{\pi}(s,a)\). We extend \(V\) to operate on probability distributions of states, \(V(\tilde{s},\Pi)=\mathbb{E}_{s\sim\tilde{s}}V(s,\Pi)\), and we allow for non-stationary, history dependent policies (denoted by uppercase \(\Pi,\Omega\)). With this notation, we can understand \(V\) as an expected utility function defined over prospects of the form \((\tilde{s},\Pi)\). We use the letter \(h\) to denote histories (trajectories of states and actions)--these may terminate on either a state or action, as may be inferred from the context. For convenience, we sometimes directly concatenate histories, states, actions and/or policies (e.g., \(hs\), \(sa\), \(s\Pi\), \(a\Pi\)) to represent trajectory segments and/or the associated stochastic processes. For simplicity, we assume finite \(|\mathcal{S}|,|\mathcal{A}|\).

### Representing rational preferences

This paper is concerned with the representation of aggregated preferences, where both the aggregation and its individual components satisfy certain axioms of rationality. We define the objects of preference to be the stochastic processes ("prospects") generated by following (potentially non-stationary and stochastic) policy \(\Pi\) from state \(s\). Distributions or "lotteries" over these prospects may be represented by (not necessarily unique) tuples of state lottery and policy \((\tilde{s},\Pi)\in\mathcal{L}(\mathcal{S})\times\mathbf{\Pi}=:\mathcal{L}( \mathcal{P})\). We write \((\tilde{s}_{1},\Pi)\succ(\tilde{s}_{2},\Omega)\) if \((\tilde{s}_{1},\Pi)\) is strictly preferred to \((\tilde{s}_{2},\Omega)\) under preference relation \(\succ\).

To be "rational", we require \(\succ\) to satisfy the "VNM axioms" [81], which is capture in Axiom 1:

**Axiom 1** (VNM).: _For all \(\tilde{p},\tilde{q},\tilde{r}\in\mathcal{L}(\mathcal{P})\) we have:_

_Asymmetry_: If \(\tilde{p}\succ\tilde{q}\), then not \(\tilde{q}\succ\tilde{p}\);

_Negative Transitivity_: If not \(\tilde{p}\succ\tilde{q}\) and not \(\tilde{q}\succ\tilde{r}\), not \(\tilde{p}\succ\tilde{r}\);

_Independence_: If \(\tilde{p}\succ\tilde{q}\), then \(\alpha\tilde{p}+(1-\alpha)\tilde{r}\succ\alpha\tilde{q}+(1-\alpha)\tilde{r}\), \(\forall\alpha\in(0,1]\);

_Continuity_: If \(\tilde{p}\succ\tilde{q}\succ\tilde{r}\), then \(\exists\;\alpha,\beta\in(0,1)\) such that \(\alpha\tilde{p}+(1-\alpha)\tilde{r}\succ\tilde{q}\succ\beta\tilde{p}+(1-\beta )\tilde{r}\);

_where \(\alpha\tilde{p}+(1-\alpha)\tilde{q}\) denotes the mixture lottery with \(\alpha\%\) chance of \(\tilde{p}\) and \((1-\alpha)\%\) chance of \(\tilde{q}\)._

Asymmetry and negative transitivity together form the basic requirements of a strict preference relation--equivalent to completeness and transitivity of the corresponding weak preference relation, \(\succeq\) (defined as \(p\succeq q\Leftrightarrow q\not\succ p\)). Independence can be understood as an irrelevance of unrealized alternatives, or consequentialist, axiom: given that the \(\alpha\%\) branch of the mixture is realized, preference between \(\tilde{p}\) and \(\tilde{q}\) is independent of the rest of the mixture (i.e., what could have happened on the \((1-\alpha)\%\) branch). Finally, continuity is a natural assumption given that probabilities are continuous.

We further require \(\succ\) to be _dynamically consistent_[70, 40, 44]:

**Axiom 2** (Dynamic consistency).: \((s,a\Pi)\succ(s,a\Omega)\) _if and only if \((T(s,a),\Pi)\succ(T(s,a),\Omega)\) where \(T(s,a)\) is the distribution over next states after taking action \(a\) in state \(s\)._

This axiom rules out the irrational behavior in the Procrastinator's Peril, by requiring today's preferences for tomorrow's actions to be the same as tomorrow's preferences. While some of these axioms (particularly independence and dynamic consistency) have been the subject of debate (see, e.g., [44]), note that the standard RL model is _more_ restrictive than they require [54].

The axioms produce two key results that we rely on (see Kreps [39] and Pitis [54] for proofs):

**Theorem 1** (Expected utility representation).: _The relation \(\succ\) defined on the set \(\mathcal{L}(\mathcal{P})\) satisfies Axiom 1 if and only if there exists a function \(V:\mathcal{P}\rightarrow\mathbb{R}\) such that, \(\forall\;\tilde{p},\tilde{q}\in\mathcal{L}(\mathcal{P})\):_

\[\tilde{p}\succ\tilde{q}\iff\sum\nolimits_{z\in\text{supp}(\tilde{p})}\tilde{ p}(z)V(z)>\sum\nolimits_{z\in\text{supp}(\tilde{q})}\tilde{q}(z)V(z).\]

_Another function \(V^{\dagger}\) gives this representation iff \(V^{\dagger}\) is a positive affine transformation of \(V\)._

Using Theorem 1, we extend the domain of value function \(V\) to \(\mathcal{L}(\mathcal{P})\) as \(V(\tilde{p})=\sum\nolimits_{z}\tilde{p}(z)V(z)\).

**Theorem 2** (Generalized Bellman representation).: _If \(\succ\) satisfies Axioms 1-2 and \(V\) is an expected utility representation of \(\succ\), there exist \(R:S\times A\rightarrow\mathbb{R}\), \(\gamma:S\times A\rightarrow\mathbb{R}^{+}\) such that \(\forall\;s,a,\Pi\),_

\[V(s,a\Pi)=R(s,a)+\gamma(s,a)V(T(s,a),\Pi).\]

**Remark 3.1.1**.: Instead of preferences over stochastic processes of potential futures, one could begin with preferences over trajectories [86, 64, 10]. The author takes issue with this approach, however, as it's unclear that such preferences should satisfy _Asymmetry_ or _Independence_ without additional assumptions (humans often consider counterfactual outcomes when evaluating the desirability of a trajectory) [54]. By using Theorem 2 to unroll prospects, one can extend preferences over prospects to define preferences over trajectories according to their discounted reward.

**Remark 3.1.2**.: Theorem 2, as it appeared in Pitis [54], required an additional, explicit "Irrelevance of unrealizable actions" axiom, since prospects were defined as tuples \((\tilde{s},\Pi)\). This property is implicit in our redefinition of prospects as stochastic processes.

**Remark 3.1.3** In this line of reasoning only the preference relation \(\succ\) is primitive; \(V\) and its Bellman form \((R,\gamma)\) are simply representations of \(\succ\) whose existence is guaranteed by the axioms. Not all numerical representations of \(\succ\) have these forms [84]. In particular, (strictly) monotonically increasing transforms preserve ordering, so that any increasing transform \(V^{\dagger}\) of a Theorem 1 representation \(V\) is itself a valid numerical representation of \(\succ\) (although lotteries will no longer be valued by the expectation over their atoms unless the transform is affine, per Theorem 1).

### Representing rational aggregation

Let us now consider the aggregation of several preferences. These may be the preferences of an agent's several principals or preferences representing a single individual's competing interests. Note at the outset that it is quite natural for different objectives or principals to have differing time preference. We saw one example in the Procrastinator's Peril, but we can also consider a household robot that seeks to aggregate the preferences of Alice and her husband Bob, for whom there is no reason to assume equal time preference [26].

An intuitively appealing axiom for aggregation is Pareto indifference, which says that if each individual preference is indifferent between two alternatives, then so too is the aggregate preference.

**Axiom 3** (Pareto indifference).: _If \(\tilde{p}\approx_{i}\tilde{q}\)\((\forall i\in\mathcal{I})\), \(\tilde{p}\approx_{\mathbb{E}}\tilde{q}\)._

Here, \(\tilde{p}\approx\tilde{q}\) means indifference (not \(\tilde{p}\succ\tilde{q}\) and not \(\tilde{q}\succ\tilde{p}\)), so that \(\approx_{i}\) is the \(i\)th individual indifference relation, \(\mathcal{I}\) is a finite index set over the individuals, and \(\approx_{\mathbb{E}}\) indicates the aggregate relation. There exist stronger variants of Pareto property that require monotonic aggregation (e.g., if all individuals prefer \(\tilde{p}\), so too does the aggregate; see Axiom 3\({}^{\prime}\) in Subsection 5.1). We opt for Pareto indifference to accommodate potentially deviant individual preferences (e.g., if all individuals are indifferent but for a sociopath, the aggregate preference may be opposite of the sociopath's).

If we require individual and aggregate preferences to satisfy Axiom 1 and, jointly, Axiom 3, we obtain a third key result due to Harsanyi [32]. (See Hammond [31] for proof).

**Theorem 3** (Harsanyi's representation).: _Consider individual preference relations \(\{\succ_{\ast};i\in\mathcal{I}\}\) and aggregated preference relation \(\succ_{\Sigma}\), each defined on the set \(\mathcal{L}(\mathcal{P})\), that individually satisfy Axiom 1 and jointly satisfy Axiom 3. If \(\{V_{i};i\in\mathcal{I}\}\) and \(V_{\Sigma}\) are expected utility representations of \(\{\succ_{\ast};i\in\mathcal{I}\}\) and \(\succ_{\Sigma}\), respectively, then there exist real-valued constant \(c\) and weights \(\{w_{i};i\in\mathcal{I}\}\) such that:_

\[V_{\Sigma}(\tilde{p})=c+\sum\nolimits_{i\in\mathcal{I}}w_{i}V_{i}(\tilde{p}).\]

_That is, the aggregate value can be expressed as a weighted sum of individual values (plus a constant)._

According to Harsanyi's representation theorem, the aggregated value function is a function of the individual value functions, _and nothing else_. In other words, Pareto indifferent aggregation of VNM preferences that results in VNM preference is necessarily _context-free_--the same weights \(\{w_{i}\}\) apply regardless of state and policy.

We will also make use of two technical conditions to eliminate certain edge cases. Though sometimes left implicit, these are both common requirements for aggregation functions [5].

**Axiom 4** (Technical conditions on \(\succ_{\Sigma}\)).: _Unrestricted Domain_: \(\succ_{\Sigma}\) is defined for all valid individual preference sets \(\{\succ_{\ast};i\in\mathcal{I}\}\). _Sensitivity_: \(\forall i\in\mathcal{I}\), holding \(\succ_{j},j\neq i\) constant, there exist \(\succ_{i}^{1},\succ_{i}^{2}\) resulting in different \(\succ_{\Sigma}\).

The first condition allows us to consider conflicting objectives with different time preference. The second condition implies that the weights \(w_{i}\) in Theorem 3 are nonzero.

**Remark 3.2.1** It is worth clarifying here the relation between Harsanyi's theorem and a related class of aggregation theorems, occasionally cited within the machine learning literature (e.g., [6]), based on axioms originating with Debreu [19]. One instance of this class states that any aggregate preference satisfying six reasonable axioms can be represented in the form: \(V_{\Sigma}(\tilde{p})=\sum_{i\in\mathcal{I}}m(V_{i}(\tilde{p}))\), where \(m\) is a strictly increasing monotonic function from the family \(\{x^{p}\,|\,0<p\leq 1\}\cup\{\log(x)\,|\,p=0\}\cup\{-x^{p}\,|\,p<0\}\)[47]. If we (1) drop the symmetry axiom from this theorem to obtain variable weights \(w_{i}\), and (2) add a monotonicity axiom to Harsanyi's theorem to ensure positive weights \(w_{i}\)[32], then the only difference between the theorems is the presence of monotonic function \(m\). Butnote that applying \(m\) to each \(V_{i}\) or to \(V_{\Xi}\) individually does not change the preference ranking they represent; it does, however, decide whether \(V_{i}\) and \(V_{\Xi}\) are VNM representations of \(\succ\). Thus, we can understand Harsanyi's theorem as saying: if \(V_{i},V_{\Xi}\) are VNM representations of \(\succ\), then \(m\) must be linear. Or conversely, if \(m\) is non-linear (\(p\neq 1\)), \(V_{i}\) and \(V_{\Xi}\) are not VNM representations.

**Remark 3.2.2**.: A caveat of Harsanyi's theorem is that it implicitly assumes that all individual preferences, and the aggregate preference, use the same set of agreed upon, "objective" probabilities. This is normatively justifiable if we use the same probability distribution (e.g., that of the aggregating agent) to impose "ideal" preferences \(\succ_{i}\) on each individual, which may differ from their implicit subjective or revealed preferences [62]. As noted by Desai et al. [20], Harsanyi's theorem fails if the preferences being aggregated use subjective probabilities. Note, however, that the outcome of the "bargaining" construction in Desai et al. [20] is socially suboptimal when the aggregating agent has better information than the principals, which suggests that effort should be made to unify subjective probabilities. We leave exploration of this to future work.

### Impossibility result

None of Theorems 1-3 assume all Axioms 1-4. Doing so leads to our key result, as follows.

**Theorem 4** (Impossibility).: _Assume there exist distinct policies, \(\Pi,\Omega,\Lambda\), none of which is a mixture (i.e., convex combination) of the other two, and consider the aggregation of arbitrary individual preference relations \(\{\succ_{i};i\in\mathcal{I}\}\) defined on \(\mathcal{L}(\mathcal{P})\) that individually satisfy Axioms 1-2. There does not exist aggregated preference relation \(\succ_{\Xi}\) satisfying Axioms 1-4._

Sketch of Proof.: The full proof is in Appendix B. Briefly, we consider \(|\mathcal{I}|=2\) and use Axiom 4 (Unrestricted Domain) to construct mixtures of \(\Pi\) and \(\Omega\) so that each mixture is considered indifferent to \(\Lambda\) by one of the individual preference relations. Then, by applying Theorem 2 and Theorem 3 in alternating orders to the difference between the value of the mixture policy and the value of \(\Lambda\), and doing some algebra, we arrive at the equations

\[w_{1}\gamma_{1}(s,a)=w_{1}\gamma_{\Xi}(s,a)\quad\text{and}\quad w_{2}\gamma_{2 }(s,a)=w_{2}\gamma_{\Xi}(s,a),\] (1)

from which we conclude that \(\gamma_{\Xi}(s,a)=\gamma_{1}(s,a)=\gamma_{2}(s,a)\). But this contradicts our assumption that individual preferences \(\{\succ_{i};i\in\mathcal{I}\}\) may be chosen arbitrarily, completing the proof. 

**Intuition of Proof.** Under mild conditions, we can find two policies (a \(\Pi/\Omega\) mixture, and \(\Lambda\)) between which individual preference \(\succ_{1}\) is indifferent, but individual preference \(\succ_{2}\) is not. Then for mixtures of this \(\Pi/\Omega\) mixture and \(\Lambda\), \(\succ_{1}\) remains indifferent, but the strength of \(\succ_{2}\) changes, so that \(\succ_{\Xi}\) must have the same time preference as \(\succ_{2}\). By an analogous argument, \(\succ_{\Xi}\) must have the same time preference as \(\succ_{1}\), leading to a violation of Unrestricted Domain.

The closest results from the economics literature consider consumption streams (\(S=\mathbb{R}\)) [88; 15; 83]. Within reinforcement learning, equal time preference has been assumed, without justification, when merging MDPs [68; 41] and value functions for different tasks [29].

**Remark 3.3.1**.: A consequence of Unrestricted Domain, critical to the impossibility result, is that individual preferences may have diverse time preferences (i.e., different discount functions). If discounts are equal, Markovian aggregation is possible.

**Remark 3.3.2**.: Per Remark 3.2.2, by applying Harsanyi's theorem, we are implicitly assuming that all preferences are formed using the same "objective" probabilities over prospects; is there a notion of "objective" time preference that should be used? If so, this would resolve the impossibility (once again, requiring that we impose a notion of ideal preference on individuals that differs from their expressed preference). We leave this consideration for future work.

**Remark 3.3.3**.: Theorem 4 applies to the standard RL setup, where \(\gamma_{1}\), \(\gamma_{2}\), \(\gamma_{\Xi}\) are constants.

## 4 Escaping Impossibility with Non-Markovian Aggregation

An immediate consequence of Theorem 4 is that any scalarized approach to multi-objective RL [79] is generally insufficient to represent composed preferences. But the implications run deeper: insofar as general tasks consist of several objectives, Theorem 4 pushes Sutton's reward hypothesis to its limits. To escape impossibility, the Procrastinator's Peril is suggestive: to be productive, repeat play should not be rewarded. And for this to happen, we must keep track of past play, which suggests that **reward must be non-Markovian, even when all relevant objectives are Markovian**. That is, even if we have settled on some non-exhaustive state representation that is "sufficiently" Markovian, an extra aggregation step could render it no longer sufficient.

**Relaxing Markov Preference** The way in which non-Markovian rewards (or equivalently, non-Markovian utilities) can be used to escape Theorem 4 is quite subtle. Nowhere in the proofs of Theorems 1-4 is the Markov assumption explicitly used. Nor does it obviously appear in any of the Axioms. The Markov assumption _is_, however, invoked in two places. First, to establish history-independent comparability between the basic objects of preference--prospects \((s,\Pi)\)--and second, to extend that comparison set to include "prospects" of the form \((T(s,a),\Pi)\). To achieve initial comparability, Pitis [54] applied a "Markov preference" assumption (preferences over prospects are independent of history) together with an "original position" construction that is worth repeating here:

\[\begin{array}{l}\mbox{[I]It is admittedly difficult to express empirical preference over prospects \(\ldots\)an agent only}\\ \mbox{ever chooses between prospects originating in the same state \(\ldots\)[Nevertheless,] we imagine}\\ \mbox{a hypothetical state from which an agent chooses between [lotteries] of prospects, denoted}\\ \mbox{by }\mathcal{L}(\mathcal{P})\mbox{. We might think of this choice being made from behind a "veil of ignorance" [58]. }\end{array}\] (2)

In other words, to allow for comparisons between prospect \((s_{1},\Pi)\) and \((s_{2},\Pi)\), we prepend some pseudo-state, \(s_{0}\), and compare prospects \((s_{0}s_{1},\Pi)\) and \((s_{0}s_{2},\Pi)\). Markov preference then lets us cut off the history, so that our preferences between \((s_{1},\Pi)\) and \((s_{2},\Pi)\) are cardinal.

The impossibility result suggests, however, that aggregate preference is _not_ independent of history, so that construction 2 cannot be applied. Without this construction, there is no reason to require relative differences between \(V(s_{1},*)\) and \(V(s_{2},*)\) to be meaningful, or to even think about lotteries/mixtures of the two prospects (as done in Axiom 1). Letting go of this ability to compare prospects starting in different states means that Theorem 1 is applicable only to sets of prospects with matching initial states, unless we shift our definition of "prospect" to include the history; i.e., letting \(h\) represent the history, we now compare "historical prospects" with form \((h,\Pi)\).

Though this does not directly change the conclusion of Theorem 2, \(T(s,a)\) in \(V(T(s,a),\Pi)\) includes a piece of history, \((s,a)\), and can no longer be computed as \(\mathbb{E}_{s^{\prime}\sim T(s,a)}V(s^{\prime},\Pi)\). Instead, since the agent is not choosing between prospects of form \((s^{\prime},\Pi)\) but rather (abusing notation) prospects of form \((sas^{\prime},\Pi)\), the expectation should be computed as \(\mathbb{E}_{s^{\prime}\sim T(s,a)}V(sas^{\prime},\Pi)\).

The inability to compare prospects starting in different states also changes the conclusion of Theorem 3, which implicitly uses such comparisons to find constant coefficients \(w_{i}\) that apply everywhere in the original \(\mathcal{L}(\mathcal{P})\). Relaxing the application of Harsanyi's theorem to not make inter-state comparisons results in weights \(w_{i}(h)\) that are history dependent when aggregating the historical prospects.

### Possibility Result

Allowing the use of history dependent coefficients in the aggregation of \(V_{i}(T(s,a),\Pi)\) resolves the impossibility. The following result shows that given some initial state dependent coefficients \(w_{i}(s)\), we can always construct history dependent coefficients \(w_{i}(h)\) that allow for dynamically consistent aggregation satisfying all axioms. In the statement of the theorem, \(\mathcal{L}(\mathcal{P}_{h})\) is used to denote the set of lotteries over prospects starting with history \(h\) of arbitrary but finite length (not to be confused with the set of lotteries over all historical prospects, of which there is only one). Note that if a preference relation satisfies Axioms 1-2 with respect to \(\mathcal{L}(\mathcal{P})\), the natural extension to \(\mathcal{L}(\mathcal{P}_{hs})\), \((hs,\Pi_{1})\succ(hs,\Pi_{2})\iff(s,\Pi_{1})\succ(s,\Pi_{2})\), satisfies Axioms 1-2 with respect to \(\mathcal{L}(\mathcal{P}_{hs})\). Here, we are using \(hs\) to denote a history terminating in state \(s\).

**Theorem 5** (Possibility).: _Consider the aggregation of arbitrary individual preference relations \(\{\succ_{\prec};i\in\mathcal{I}\}\) defined on \(\mathcal{L}(\mathcal{P})\), and consequently \(\mathcal{L}(\mathcal{P}_{h}),\ \forall h\), that individually satisfy Axioms 1-2. There exists aggregated preference relations \(\{\succ^{\frac{h}{2}}\}\), defined on \(\mathcal{L}(\mathcal{P}_{h}),\ \forall h\), that satisfy Axioms 1-4._

_In particular, **given**\(s,a\), \(V_{\Sigma}\), \(\{V_{i}\}\), \(\{w_{i}(s)\}\), **where (A)** each \(V_{i}\) satisfies Axioms 1-2 on \(\mathcal{L}(\mathcal{P})\), and \(V_{\Sigma}\) satisfies Axioms 1-2 on \(\mathcal{L}(\mathcal{P}_{h}),\forall h\), **and (B)**\(V_{\Sigma}(s,a\Pi)=\sum_{i}w_{i}(s)V_{i}(s,a\Pi))\), **then**, choosing_

\[w_{i}(sas^{\prime}):=w_{i}(sa):=w_{i}(s)\gamma_{i}(s,a)\quad\mbox{ for all }i,s^{\prime}\] (3)implies that \(V_{\Sigma}(sas^{\prime},\Pi)\propto\sum_{i}w_{i}(sa)V_{i}(s^{\prime},\Pi)\) so that the aggregated preferences \(\{\succ_{\Sigma}^{sas^{\prime}}\}\) satisfy Axiom 3 on \(\mathcal{L}(\mathcal{P}_{sas^{\prime}})\). Unrolling this result--\(w_{i}(hasas^{\prime}):=w_{i}(hs)\gamma_{i}(s,a)\)--produces a set of constructive, history dependent weights \(\{w_{i}(h)\}\) such that Axiom 3 is satisfied for all histories \(\{h\}\)._

Sketch of Proof.: The full proof is in Appendix B. Following the proof of Theorem 4, we arrive at

\[w_{1}(s)\gamma_{1}(s,a)=w_{1}(sa)\gamma_{\Sigma}(s,a)\quad\text{and}\quad w_{2 }(s)\gamma_{2}(s,a)=w_{2}(sa)\gamma_{\Sigma}(s,a),\] (4)

from which we conclude that:

\[\frac{w_{2}(sa)}{w_{1}(sa)}=\frac{w_{2}(s)\gamma_{2}(s,a)}{w_{1}(s)\gamma_{1}( s,a)}.\] (5)

This shows the existence of weights \(w_{i}(sa)\), unique up to a constant scaling factor, for which \(V_{\Sigma}(T(s,a),\Pi)\propto\sum_{i}w_{i}(sa)V_{i}(T(s,a),\Pi)\), that apply regardless of how individual preferences are chosen or aggregated at \(s\). Unrolling the result completes the proof. 

From Theorem 5 we obtain a rather elegant result: rational aggregation over time discounts the aggregation weights assigned to each individual value function proportionally to its respective discount factor. In the Procrastinator's Peril, for instance, where we started with \(w_{\mathtt{p}}(s)=w_{\mathtt{v}}(s)=1\), at the initial (and only) state \(s\), we might define \(w_{\mathtt{p}}(\mathtt{sp}s)=0.5\) and \(w_{\mathtt{v}}(\mathtt{sp}s)=0.9\). With these non-Markovian aggregation weights and \(\gamma_{\mathtt{E}}(s\mathtt{sp})=1\), you can verify that (1) the irrational procrastination behavior is solved, and (2) the aggregated rewards for work and play are now non-Markovian.

**Remark 4.1** (Important!): The discount \(\gamma_{\Sigma}\) is left undetermined by Theorem 5. One might determine it several ways: by appealing to construction 2 with respect to historical prospects in order to establish inter-state comparability, by setting it to be the highest individual discount (0.9 in the Procrastinator's Peril), by normalizing the aggregation to weights to sum to 1 at each step, or perhaps by another method. In any case, determining \(\gamma_{\Sigma}\) would also determine the aggregation weights, per equation 4 (and vice versa). We leave the consideration of different methods for setting \(\gamma_{\Sigma}\) and establishing inter-state comparability of \(V_{\Sigma}\) to future work. (NB: _This is a normative question_, which we leave unanswered. While one can make assumptions, as we will for our numerical example in Subsection 5.2, future research should be wary of accepting a solution just because it seems to work.)

### A Practical State Space Expansion

The basic approach to dealing with non-Markovian rewards is to expand the state space in such a way that rewards becomes Markovian [27; 14; 2]. However, naively expanding \(\mathcal{S}\) to a history of length \(H\) could have \(O((|\mathcal{S}|+|\mathcal{A}|)^{H})\) complexity. Fortunately, the weight update in equation 4 allows us to expand the state using a single parameter per objective. In particular, for history \(hsa\) and objective \(i\), we append to the state the factors \(y_{i}(hsa):=y_{i}(h)\gamma_{i}(s,a)/\gamma_{\Sigma}(s,a)\), which are defined for every history, and can be accumulated online while executing a trajectory. Then, given a composition with any set of initial weights \(\{w_{i}\}\), we can compute the weights of augmented state \(s_{\text{aug}}=(s,y_{i}(hs))\) as \(w_{i}(s_{\text{aug}})=y_{i}(hs)\cdot w_{i}\). Letting \(\mathcal{L}(\mathcal{P}^{(y)})\) be the set of prospects on the augmented state set, we get the following corollary to Theorem 5:

**Corollary 1**.: _Consider the aggregation of arbitrary individual preference relations \(\{\succ_{i};i\in\mathcal{I}\}\) defined on \(\mathcal{L}(\mathcal{P})\), and consequently \(\mathcal{L}(\mathcal{P}^{(y)})\), that individually satisfy Axioms 1-2. There exists aggregated preference relation \(\{\succ_{\Sigma}\}\), defined on \(\mathcal{L}(\mathcal{P}^{(y)})\), that satisfies Axioms 1-4._

## 5 Discussion and Related Work

### A Fundamental Tension in Intertemporal Choice

The state space expansion of Subsection 4.2 allows us to represent the (now Markovian) values of originally non-Markovian policies in a dynamically consistent way. While this allows us to design agents that implement these policies, it doesn't quite solve the intertemporal choice problem.

In particular, it is known that dynamic consistency (in the form of Koopmans' Stationarity [38]), together with certain mild axioms, implies a first period dictatorship: the preferences at time \(t=1\) are decisive for all time [25] (in a sense, this is the very definition of dynamic consistency!). Generally speaking, however, preferences at time \(t\neq 1\) are not the same as preferences at time \(t=1\) (this is what got us into our Procrastinator's Peril to begin with!) and we would like to care about the value at all time steps, not just the first.

A typical approach is to treat each time step as a different generation (decision maker), and then consider different methods of aggregating preferences between the generations [33]. Note that (1) this aggregation assumes intergenerational comparability of utilities (see Remark 4.1), and (2) each generation is expressing their personal preferences about what happens in _all_ generations, not just their own. Since this is a single aggregation, it will be dynamically consistent (we can consider the first period dictator as a benevolent third party who represents the aggregate preference instead of their own). A sensible approach might be to assert a stronger version of Axiom 3 that uses preference:

**Axiom 3\({}^{\prime}\)** (Strong Pareto Preference) If \(\tilde{p}\succeq_{\Sigma}\tilde{q}\) (\(\forall i\in\mathcal{I}\)), then \(\tilde{p}\succeq_{\Sigma}\tilde{q}\); and if, furthermore, \(\exists j\in\mathcal{I}\) such that \(\tilde{p}\succ_{j}\tilde{q}\), then \(\tilde{p}\succ_{\Sigma}\tilde{q}\).

Using Axiom 3\({}^{\prime}\) in place of Axiom 3 for purposes of Theorem 3 gives a representation that assigns strictly positive weights to each generation's utility. Given infinite periods, it follows (e.g., by the Borel-Cantelli Lemma for general measure spaces) that if utilities are bounded, some finite prefix of the infinite stream decides the future and we have a "finite prefix dictatorship", which is not much better than a first period one.

The above discussion presents a strong case _against_ using dynamic consistency to determine a long horizon policy. Intuitively, this makes sense: preferences change over time, and our current generation should not be stuck implementing the preferences of our ancestors. One way to do this is by taking time out of the equation, and optimizing the expected individual utility of the stationary state-action distribution, \(d_{\pi}\) (cf. Sutton and Barto [74] (Section 10.4) and Naik et al. [50]):

\[J(\pi)=\sum\nolimits_{s}d_{\pi}(s)V_{\pi}(s)\] (6)

Unfortunately, as should be clear from the use of a stationary policy \(\pi\), this time-neutral approach falls short for our purposes, which suggests the use of a non-Markovian policy. While optimizing equation \(6\) would find the optimal stationary policy in the Procrastinator's Peril (work forever with \(V(\tau_{2})=3.0\)), it seems clear that we _should_ play at least once (\(V(\tau_{3})=3.2\)) as this hurts no one and makes the current decision maker better off--i.e., simply optimizing (6) violates the Pareto principle.

This discussion exemplifies a known tension in intertemporal choice between Pareto optimality and the requirement to treat every generation equally--it is impossible to have both [16; 42]. In a similar spirit to Chichilnisky [16], who has proposed an axiomatic approach requiring both finite prefixes of generations and infinite averages of generations to have a say in the social preference, we will examine a compromise (to the author's knowledge novel) between present and future decision makers in next Subsection.

### N-Step Commitment and Historical Discounting

We now consider two solutions to the intertemporal choice problem that deviate just enough from dynamic consistency to overcome finite period dictatorships, while capturing almost all value for each decision maker. In other words, they are "almost" dynamically consistent. We leverage the following observation: due to discounting, the first step decision maker cares very little about the far off future. If we play once, then work for 30 steps in the Procrastinator's Peril, we are already better off than the best stationary policy, regardless of what happens afterward.

This suggests that, rather than following the current decision maker forever, we allow them commit to a non-Markovian policy for some \(N<\infty\) steps that brings them within some \(\epsilon\) of their optimal policy, without letting them exert complete control over the far future. To implement this, we could reset the accumulated factors \(y_{i}\) to \(1\) every \(N\) steps. This approach is the same as grouping every consecutive \(N\) step window into a single, dynamically consistent generation with a first period dictator.

A more fluid and arguably better approach is to discount the past when making current decisions ("historical discounting"). We can implement historical discounting with factor \(\eta\in[0,1]\) by changing the update rule for factors \(y_{i}\) to

\[y_{i}(hsa)\mathrel{\mathop{:}}=\eta\left[y_{i}(h)\frac{\gamma_{i}(s,a)}{\gamma_ {\Sigma}(s,a)}\right]+(1-\eta)w_{i}^{n}(s),\]

where \(w_{i}^{n}(s)\) denotes the initial weight for objective \(i\) at the \(n\)th generation (if preferences do not change over time, \(w_{i}^{n}=w_{i}\)). This performs an exponential moving average of preferences over time high \(\eta<1\) initially gives the first planner full control, but eventually discounts their preferences until they are negligible. This obtains a qualitatively different effect from \(N\)-step commitment, since the preferences of all past time steps have positive weight (by contrast, on the \(N\)th step, \(N\)-step commitment gives no weight to the \(N-1\) past preferences). As a result, in the Procrastinator's Peril, any sufficiently high \(\eta\) returns policy \(\tau_{3}\), whereas \(N\)-step commitment would play every \(N\) steps.

Historical discounting is an attractive compromise because it is both Pareto efficient, and also anonymous with respect to tail preferences--it both optimizes equation \(6\) in the limit, _and_ plays on the first step. Another attractive quality is that historical discounting allows changes in preference to slowly deviate from dynamic consistency over time--the first period is _not_ a dictator.

As a numerical example, we can consider what happens in the Procrastinator's Peril if we start to realize that we value occasional play, and preferences shift away from the play MDP to a playn MDP. The playn MDP has fixed \(\gamma_{\texttt{playn}}=0.9\) and non-Markovian reward \(R(\texttt{p}\,|\,\texttt{no play in last N-1 steps})=0.5,\ R(\texttt{ anything else})=0\). We assume preferences shift linearly over the first 10 timesteps, from \(w_{\texttt{play}}=1,w_{\texttt{playn}}=0\) at \(t=0\) to \(w_{\texttt{play}}=0,w_{\texttt{playn}}=1\) at \(t=10\) (and \(w_{\texttt{work}}=1\) throughout). Then, the optimal trajectories \(\tau^{*}\) for different \(\eta\), and resulting discounted reward for the _first_ time step, \(V^{1}(\tau^{*})\), are as follows:

\begin{tabular}{l l l} \hline \hline \(\eta\) & \(\tau^{*}\) & \(V^{1}(\tau^{*})\) \\ \hline \(0.00\) & play for 5 steps, then play every 10 steps & \(2.635\) \\ \(0.30\) & play for 5 steps, then play every 10 steps & \(2.635\) \\ \(0.50\) & play for 3 steps, then play every 10 steps & \(2.932\) \\ \(0.90\) & play, then work for 14 steps, then play every 10 steps & \(3.105\) \\ \(0.95\) & play, then work for 23 steps, then play every 10 steps & \(3.163\) \\ \(0.98\) & play, then work for 50 steps, then play every 10 steps & \(3.198\) \\ \(1.00\) & play, then work forever (same as \(\tau_{3}\)) & \(3.200\) \\ \hline \hline \end{tabular} When \(\eta=0\), each time step acts independently, and since the play MDP has high weight at the start, we experience a brief period of consistent play in line with the original Procrastinator's Peril, before preferences fully shift to occasional play. With high \(\eta<1\), we capture almost all value for the first time step, while also eventually transitioning to the equilibrium "play every 10 steps" policy.

### Extension to Boltzmann Policies

The preference relation \(\succ\) is deterministic, but the associated policy does not have to be. In many cases--partially-observed, multi-agent, or even fully-observed settings [74, 30]--stochastic policies outperform deterministic ones. And for generative sequence models such as LLMs [13], stochastic policies are inherent. We can extend our analysis--impossibility, possibility, state space expansion, and intertemporal choice rules--to such cases by adopting a stochastic choice rule.

To formalize this, we will take the stochastic choice rule as primitive, and use it to define a (deterministic) relation \(\succ\) that, for practical purposes, satisfies Axioms 1-4 [18]. We assume our choice rule satisfies Luce's Choice Axiom [43], which says that the relative rate of choosing between two alternatives in a choice set of \(n\) alternatives is constant regardless of the choice set. This can be implemented numerically with a Bradley-Terry choice model [11] by associating each alternative \(a_{i}\) with a scalar \(\Omega(a_{i})\), so that given choice set \(\{a_{i},a_{j}\}\), \(p(a_{i})=\Omega(a_{i})/(\Omega(a_{i})+\Omega(a_{j}))\). (An interesting interpretation for \(\Omega(a_{i})\) that connects to both probability matching [82, 65] and statistical mechanics [45] is as the number of "outcomes" (microstates) for which \(a_{i}\) is the best choice.)

We then simply "define" preference as \(a_{i}\succ a_{j}\iff\Omega(a_{i})>\Omega(a_{j})\), and utility as \(V(a_{i}):=k\log\Omega(a_{i})\), so that the policy is a softmax of the utilities. The way these utilities are used in practice (e.g., [30, 17]) respects Axioms 1-2. And summing utilities is a common approach to composition [21, 29], which is consistent with Harsanyi's representation (Theorem 3). For practical purposes then, the impossibility result applies whenever composed objectives may have different time preference.

**Remark 5.3**: Unlike our main results, this extension to Boltzmann policies is motivated by practical, rather than normative, considerations. Simple counterexamples to Luce's Choice Axiom exist [87] and probability matching behavior is evidently irrational in certain circumstances [65]. We note, however, that certain theoretical works tease at the existence of a normative justification for Boltzmann policies [18, 8, 73, 23]; given the practice, a clear justification would be of great value.

### Related Work in RL

**Task Definition**: Tasks are usually defined as the maximization of expected cumulative reward in an MDP [74; 55; 67]. Preference-based RL [86] avoids rewards, operating directly with preferences (but note that preference aggregation invokes Arrow's impossibility theorem [5; 48]), while other works translate preferences into rewards [17; 72; 12; 35]. This paper joins a growing list of work [54; 1; 76; 77] that challenges the implicit assumption that "MDPs are enough" in many reward learning papers, particularly those that disaggregate trajectory returns into stepwise rewards [22; 56; 59].

**Discounting**: Time preference or discounting can be understood as part of the RL task definition. Traditionally, a constant \(\gamma\) has been used, although several works have considered other approaches, such as state-action dependent discounting [85; 66] and non-Markovian discounting [24; 63]. Several works have considered discounting as a tool for optimization [80] or regularization [36; 4; 57].

**Task Compositionality**: Multi-objective RL [61] represents or optimizes over multiple objectives or general value functions [75], which are often aggregated with a linear scalarization function [7; 3]. Rather than scalarizing to obtain a single solution to a multi-objective problem, one can also seek out sets of solutions, such as the set of Pareto optimal policies [78] or the set of acceptable policies [46]. Several works have also considered formal task decompositions [14; 51] where simple addition of MDPs is insufficient [68]. More broadly, in machine learning, composition can be done via mixtures of experts and/or energy-based modeling [34; 21], which have also been applied to RL [29; 41]. Our results provide normative justification for linear scalarization when time preference is the same for all objectives, but call for non-Markovian adjustments when time preferences differ.

**Non-Markovian Rewards**: The necessity of non-Markovian rewards was demonstrated in other settings by Abel et al. [1] and more recently, in a concurrent work by Skalse and Abate [69]. Though several papers explicitly consider RL with non-Markovian rewards [27; 60], this is usually motivated by task compression rather than necessity, and the majority of the RL literature restricts itself to Markovian models. Many popular exploration strategies implicitly use non-Markovian rewards [37; 53]. Our work is unique in that non-Markovian rewards arise from aggregating strictly Markovian quantities, rather non-Markovian quantities present in the task definition or algorithm.

## 6 Conclusion and Future Work

The main contribution of this work is an impossibility result from which one concludes that non-Markovian rewards (or an equivalent state expansion) are likely necessary for agents that pursue multiple objectives or serve multiple principals. It's possible that this will be the case for any advanced agent whose actions impact multiple human stakeholders. To accurately align such agents with diverse human preferences we need to endow them with the capacity to solve problems requiring non-Markovian reward, for which this paper has proposed an efficient state space expansion that uses one new parameter per aggregated objective. While the proposed state space expansion allows multi-objective agents to have dynamically consistent preferences for future prospects, it does not, in itself, solve the intertemporal choice problem. To that end, we have proposed "historical discounting", a novel compromise between dynamic consistency and fair consideration of future generations.

Interesting avenues for future work include quantifying the inefficiency of Markovian representations, investigating normative approaches to aggregating preferences based on subjective world models (Remark 3.2.2), considering the existence of an "objective" time preference (Remark 3.3.2), improving methods for determining subjective time preference (e.g., [63]), implementing and comparing approaches to determining \(\gamma_{\Sigma}\) (Remark 4.1), investigating historical discounting in more detail (Subsection 5.2), and considering the existence of a normative justification for Boltzmann policies and their composition (Subsection 5.3).

## Acknowledgments and Disclosure of Funding

I thank Elliot Creager, for a fruitful discussion that prompted Subsection 5.1 and motivated me to turn this into a full paper; Duncan Bailey, who assisted with an earlier workshop version; the anonymous reviewers, who provided detailed reviews and suggestions that helped improve the final manuscript; and Jimmy Ba and the Ba group for early discussions. This work was supported by an NSERC CGS D Award and a Vector Research Grant.

## References

* [1] David Abel, Will Dabney, Anna Harutyunyan, Mark K Ho, Michael Littman, Doina Precup, and Satinder Singh. On the expressivity of markov reward. _Advances in Neural Information Processing Systems_, 34:7799-7812, 2021.
* [2] David Abel, Andre Barreto, Michael Bowling, Will Dabney, Steven Hansen, Anna Harutyunyan, Mark Ho, Ramana Kumar, Michael Littman, Doina Precup, and Satinder Singh. Expressing non-markov reward to a markov agent. In _RLDM_, pages 1-5, 2022.
* [3] Axel Abels, Diederik M Roijers, Tom Lenaerts, Ann Nowe, and Denis Steckelmacher. Dynamic weights in multi-objective deep reinforcement learning. _arXiv preprint arXiv:1809.07803_, 2018.
* [4] Ron Amit, Ron Meir, and Kamil Ciosek. Discount factor as a regularizer in reinforcement learning. In _International conference on machine learning_, pages 269-278. PMLR, 2020.
* [5] Kenneth J Arrow, Amartya Sen, and Kotaro Suzumura. _Handbook of social choice and welfare_, volume 2. Elsevier, 2010.
* [6] Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John Aslanides, Matt Botvinick, et al. Fine-tuning language models to find agreement among humans with diverse preferences. _Advances in Neural Information Processing Systems_, 35:38176-38189, 2022.
* [7] Andre Barreto, Will Dabney, Remi Munos, Jonathan J Hunt, Tom Schaul, Hado P van Hasselt, and David Silver. Successor features for transfer in reinforcement learning. _Advances in neural information processing systems_, 30, 2017.
* [8] Sergio Batiz-Solorzano. _On decisions with multiple objectives: Review and classification of prescriptive methodologies, a group value function problem, and applications of a measure of information to a class of multiattribute problems_. Rice University, 1979.
* [9] D. Bernoulli. Specimen theoriae novae de mensura sortis, Commentarii Academiae Scientiarum Imperialis Petropolitanae (5, 175-192, 1738). _Econometrica_, 22:23-36, 1954. Translated by L. Sommer.
* [10] Michael Bowling, John D Martin, David Abel, and Will Dabney. Settling the reward hypothesis. _arXiv preprint arXiv:2212.10420_, 2022.
* [11] Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952.
* [12] Daniel S Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond suboptimal demonstrations via inverse reinforcement learning from observations. _arXiv preprint arXiv:1904.06387_, 2019.
* [13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [14] Alberto Camacho, Rodrigo Toro Icarte, Toryn Q Klassen, Richard Anthony Valenzano, and Sheila A McIlraith. Ltl and beyond: Formal languages for reward function specification in reinforcement learning. In _IJCAI_, volume 19, pages 6065-6073, 2019.
* [15] Christopher P Chambers and Federico Echenique. On multiple discount rates. _Econometrica_, 86(4):1325-1346, 2018.
* [16] Graciela Chichilnisky. An axiomatic approach to sustainable development. _Social choice and welfare_, 13:231-257, 1996.
* [17] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. In _Advances in Neural Information Processing Systems_, 2017.
* [18] Gerard Debreu. Stochastic choice and cardinal utility. _Econometrica: Journal of the Econometric Society_, pages 440-444, 1958.
* [19] Gerard Debreu. Topological methods in cardinal utility theory, mathematical methods in the social sciences (kj arrow, s. karlin, and p. suppes, eds.), 1960.

* Desai et al. [2018] Nishant Desai, Andrew Critch, and Stuart J Russell. Negotiable reinforcement learning for pareto optimal sequential decision-making. _Advances in Neural Information Processing Systems_, 31, 2018.
* Du et al. [2020] Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation with energy based models. _Advances in Neural Information Processing Systems_, 33:6637-6647, 2020.
* Efroni et al. [2021] Yonathan Efroni, Nadav Merlis, and Shie Mannor. Reinforcement learning with trajectory feedback. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 7288-7295, 2021.
* Eysenbach and Levine [2022] Benjamin Eysenbach and Sergey Levine. Maximum entropy rl (provably) solves some robust rl problems. In _International Conference on Learning Representations_, 2022.
* Fedus et al. [2019] William Fedus, Carles Gelada, Yoshua Bengio, Marc G Bellemare, and Hugo Larochelle. Hyperbolic discounting and learning over multiple horizons. _arXiv preprint arXiv:1902.06865_, 2019.
* Ferejohn and Page [1978] John Ferejohn and Talbot Page. On the foundations of intertemporal choice. _American Journal of Agricultural Economics_, 60(2):269-275, 1978.
* Frederick et al. [2002] Shane Frederick, George Loewenstein, and Ted O'Donoghue. Time discounting and time preference: A critical review. _Journal of economic literature_, 40(2):351-401, 2002.
* Gaon and Brafman [2020] Maor Gaon and Ronen Brafman. Reinforcement learning with non-markovian rewards. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 3980-3987, 2020.
* Glimcher [2011] Paul W Glimcher. Understanding dopamine and reinforcement learning: the dopamine reward prediction error hypothesis. _Proceedings of the National Academy of Sciences_, 108(supplement_3):15647-15654, 2011.
* Haarnoja et al. [2018] Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey Levine. Composable deep reinforcement learning for robotic manipulation. In _2018 IEEE international conference on robotics and automation (ICRA)_, pages 6244-6251. IEEE, 2018.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In _International conference on machine learning_, pages 1861-1870. PMLR, 2018.
* Hammond [1992] Peter J Hammond. Harsanyi's utilitarian theorem: A simpler proof and some ethical connotations. In _Rational Interaction_, pages 305-319. Springer, 1992.
* Harsanyi [1955] John C Harsanyi. Cardinal welfare, individualistic ethics, and interpersonal comparisons of utility. _Journal of political economy_, 63(4):309-321, 1955.
* Heal [2005] Geoffrey Heal. Intertemporal welfare economics and the environment. _Handbook of environmental economics_, 3:1105-1145, 2005.
* Jacobs et al. [1991] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. _Neural computation_, 3(1):79-87, 1991.
* Jeon et al. [2020] Hong Jun Jeon, Smitha Milli, and Anca Dragan. Reward-rational (implicit) choice: A unifying formalism for reward learning. _Advances in Neural Information Processing Systems_, 33:4415-4426, 2020.
* Jiang et al. [2015] Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning horizon on model accuracy. In _Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems_, pages 1181-1189, 2015.
* Kolter and Ng [2009] J Zico Kolter and Andrew Y Ng. Near-bayesian exploration in polynomial time. In _Proceedings of the 26th annual international conference on machine learning_, pages 513-520, 2009.
* Koopmans [1960] Tjalling C Koopmans. Stationary ordinal utility and impatience. _Econometrica: Journal of the Econometric Society_, pages 287-309, 1960.
* Kreps [1988] David Kreps. _Notes on the Theory of Choice_. Westview press, 1988.
* Kreps and Porteus [1978] David M Kreps and Evan L Porteus. Temporal resolution of uncertainty and dynamic choice theory. _Econometrica: journal of the Econometric Society_, pages 185-200, 1978.
* Laroche et al. [2017] Romain Laroche, Mehdi Fatemi, Joshua Romoff, and Harm van Seijen. Multi-advisor reinforcement learning. _arXiv preprint arXiv:1704.00756_, 2017.

* Lauwers [1998] Luc Lauwers. Intertemporal objective functions: Strong pareto versus anonymity. _Mathematical Social Sciences_, 35(1):37-55, 1998.
* Luce [1959] R Duncan Luce. Individual choice behavior. 1959.
* Machina [1989] Mark J Machina. Dynamic consistency and non-expected utility models of choice under uncertainty. _Journal of Economic Literature_, 27(4):1622-1668, 1989.
* Mandl [1991] Franz Mandl. _Statistical physics_, volume 14. John Wiley & Sons, 1991.
* Miura [2023] Shuwa Miura. On the expressivity of multidimensional markov reward. _arXiv preprint arXiv:2307.12184_, 2023.
* Moulin [2004] Herve Moulin. _Fair division and collective welfare_. MIT press, 2004.
* Muandet [2022] Krikamol Muandet. Impossibility of collective intelligence. _arXiv preprint arXiv:2206.02786_, 2022.
* Mullainathan and Thaler [2000] Sendhil Mullainathan and Richard H Thaler. Behavioral economics, 2000.
* Naik et al. [2019] Abhishek Naik, Roshan Shariff, Niko Yasui, Hengshuai Yao, and Richard S Sutton. Discounted reinforcement learning is not an optimization problem. _arXiv preprint arXiv:1910.02140_, 2019.
* Tasse et al. [2020] Geraud Nangue Tasse, Steven James, and Benjamin Rosman. A boolean task algebra for reinforcement learning. _Advances in Neural Information Processing Systems_, 33:9497-9507, 2020.
* Ng and Russell [2000] Andrew Y Ng and Stuart J Russell. Algorithms for inverse reinforcement learning. In _The Seventeenth International Conference on Machine Learning_, pages 663-670, 2000.
* Pathak et al. [2017] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration by self-supervised prediction. In _International conference on machine learning_. PMLR, 2017.
* Pitis [2019] Silviu Pitis. Rethinking the discount factor in reinforcement learning: A decision theoretic approach. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 7949-7956, 2019.
* Puterman [2014] Martin L Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* Raposo et al. [2021] David Raposo, Sam Ritter, Adam Santoro, Greg Wayne, Theophane Weber, Matt Botvinick, Hado van Hasselt, and Francis Song. Synthetic returns for long-term credit assignment. _arXiv preprint arXiv:2102.12425_, 2021.
* Rathnam et al. [2023] Sarah Rathnam, Sonali Parbhoo, Weiwei Pan, Susan Murphy, and Finale Doshi-Velez. The unintended consequences of discount regularization: Improving regularization in certainty equivalence reinforcement learning. In _International Conference on Machine Learning_, pages 28746-28767. PMLR, 2023.
* Rawls [2009] John Rawls. _A theory of justice: Revised edition_. Harvard university press, 2009.
* Ren et al. [2023] Zhizhou Ren, Ruihan Guo, Yuan Zhou, and Jian Peng. Learning long-term reward redistribution via randomized return decomposition. In _International Conference on Learning Representations_, 2023.
* Rens et al. [2020] Gavin Rens, Jean-Francois Raskin, Raphael Reynoaud, and Giuseppe Marra. Online learning of non-markovian reward models. In _Proceedings of the Thirteenth International Conference on Agents and Artificial Intelligence_. Scitepress, 2020.
* Roijers et al. [2013] Diederik M Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley. A survey of multi-objective sequential decision-making. _Journal of Artificial Intelligence Research_, 48:67-113, 2013.
* Savage [1954] Leonard J Savage. _The foundations of statistics_. Wiley, 1954.
* Schultheis et al. [2022] Matthias Schultheis, Constantin A Rothkopf, and Heinz Koeppl. Reinforcement learning with non-exponential discounting. In _Advances in neural information processing systems_, 2022.
* Shakerinava and Ravanbakhsh [2022] Mehran Shakerinava and Siamak Ravanbakhsh. Utility theory for sequential decision making. In _International Conference on Machine Learning_, pages 19616-19625. PMLR, 2022.
* Shanks et al. [2002] David R Shanks, Richard J Tunney, and John D McCarthy. A re-examination of probability matching and rational choice. _Journal of Behavioral Decision Making_, 15(3):233-250, 2002.
* Silver et al. [2017] David Silver, Hado van Hasselt, Matteo Hessel, Tom Schaul, Arthur Guez, Tim Harley, Gabriel Dulac-Arnold, David Reichert, Neil Rabinowitz, Andre Barreto, and Thomas Degris. The predictron: End-to-end learning and planning. In _The Thirty-fourth International Conference on Machine Learning_, 2017.

* Silver et al. [2021] David Silver, Satinder Singh, Doina Precup, and Richard S Sutton. Reward is enough. _Artificial Intelligence_, 299:103535, 2021.
* Singh and Cohn [1998] Satinder P Singh and David Cohn. How to dynamically merge markov decision processes. In _Advances in neural information processing systems_, pages 1057-1063, 1998.
* Skalse and Abate [2023] Joar Skalse and Alessandro Abate. On the limitations of markovian rewards to express multi-objective, risk-sensitive, and modal tasks. In _Uncertainty in Artificial Intelligence_, pages 1974-1984. PMLR, 2023.
* Sobel [1975] Matthew J Sobel. Ordinal dynamic programming. _Management science_, 21(9):967-975, 1975.
* Solow et al. [1970] Robert M Solow et al. Growth theory. an exposition. In _Growth theory. An exposition._ Oxford: Clarendon Press., 1970.
* Stiennon et al. [2020] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* Suppes [1961] Patrick Suppes. Behaviistic foundations of utility. _Econometrica: Journal of The Econometric Society_, pages 186-202, 1961.
* Sutton and Barto [2018] Richard S Sutton and Andrew G Barto. _Reinforcement Learning: An Introduction_. MIT Press, 2018.
* Sutton et al. [2011] Richard S Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick M Pilarski, Adam White, and Doina Precup. Horde: A scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction. In _The 10th International Conference on Autonomous Agents and Multiagent Systems-Volume 2_, pages 761-768. International Foundation for Autonomous Agents and Multiagent Systems, 2011.
* Szepesvari [2020] Csaba Szepesvari. Constrained mdps and the reward hypothesis. _Musings about machine learning and other things (blog)_, 2020.
* Vamplew et al. [2021] Peter Vamplew, Benjamin J Smith, Johan Kallstrom, Gabriel Ramos, Roxana Radulescu, Diederik M Roijers, Conor F Hayes, Fredrik Heintz, Patrick Mannion, Pieter JK Libin, et al. Scalar reward is not enough: A response to silver, singh, precup and sutton (2021). _Autonomous Agents and Multi-Agent Systems_, 36(2):41, 2022.
* Van Moffaert and Nowe [2014] Kristof Van Moffaert and Ann Nowe. Multi-objective reinforcement learning using sets of pareto dominating policies. _The Journal of Machine Learning Research_, 15(1):3483-3512, 2014.
* Van Moffaert et al. [2013] Kristof Van Moffaert, Madalina M Drugan, and Ann Nowe. Scalarized multi-objective reinforcement learning: Novel design techniques. In _2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)_, pages 191-199. IEEE, 2013.
* Seijen et al. [2019] Harm Van Seijen, Mehdi Fatemi, and Arash Tavakoli. Using a logarithmic mapping to enable lower discount factors in reinforcement learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* Neumann and Morgenstern [1953] John Von Neumann and Oskar Morgenstern. _Theory of games and economic behavior_. Princeton university press, 1953.
* Vulkan [2000] Nir Vulkan. An economist's perspective on probability matching. _Journal of economic surveys_, 14(1):101-118, 2000.
* Weitzman [2001] Martin L Weitzman. Gamma discounting. _American Economic Review_, 91(1):260-271, 2001.
* Weymark [2005] John A Weymark. Measurement theory and the foundations of utilitarianism. _Social Choice and Welfare_, 25(2-3):527-555, 2005.
* White [2017] Martha White. Unifying task specification in reinforcement learning. In _The Thirty-fourth International Conference on Machine Learning_, 2017.
* Wirth et al. [2017] Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes Furnkranz. A survey of preference-based reinforcement learning methods. _Journal of Machine Learning Research_, 18(136):1-46, 2017.
* Yellott Jr [1977] John I Yellott Jr. The relationship between luce's choice axiom, thurstone's theory of comparative judgment, and the double exponential distribution. _Journal of Mathematical Psychology_, 15(2):109-144, 1977.
* Zuber [2011] Stephane Zuber. The aggregation of preferences: can we ignore the past? _Theory and decision_, 70(3):367-384, 2011.

## Appendix A Notation Glossary

\begin{table}
\begin{tabular}{l l} \hline \hline
**Basic notation** & \\ \(\mathcal{L}(\cdot)\) & the set of probability distributions over its argument \\ \(\{\cdot\}\) & a set of its argument \\ \(\mathcal{M}\) & a generalized MDP \((\mathcal{S},\mathcal{A},\mathcal{T},\mathcal{T}_{0},R,\gamma)\) \\ \(\mathcal{S},s\) & the state space \(\mathcal{S}\) with generic state \(s\in\mathcal{S}\) \\ \(\mathcal{A},a\) & the action space \(\mathcal{A}\) with generic action \(a\in\mathcal{A}\) \\ \(\mathcal{T}\) & the transition function \(\mathcal{T}:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{L}(\mathcal{S})\) \\ \(R\) & the reward function \(R:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) \\ \(\gamma\) & the generalized discount factor \(\gamma:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{+}\) \\ \(\tau\) & a trajectory \(\tau=[(s_{0},a_{0}),(s_{1},a_{1}),\dots]\), which may or may not terminate \\ \(h\) & a history (trajectory to up to time \(t\)) \\ \(\pi,\omega\) & stationary policies \(\pi,\omega:\mathcal{S}\rightarrow\mathcal{L}(\mathcal{A})\) \\ \(\Pi,\Omega,\Lambda\) & non-stationary policies \((\pi_{t}\,|\,\tau_{t}^{0},\pi_{t+1}\,|\,\tau_{0}^{t+1},\dots)\) \\ \(\mathbf{\Pi}\) & policy space, so \(\pi,\omega,\Pi,\Omega\in\mathbf{\Pi}\); note \(\mathcal{L}(\mathbf{\Pi})=\mathbf{\Pi}\) \\ \(\mathcal{P}\) & the space \(\mathcal{P}=\mathcal{S}\times\mathbf{\Pi}\) of prospects over which preferences are expressed; note \(\mathcal{L}(P)=\mathcal{L}(S)\times\mathbf{\Pi}\) \\ \(\mathcal{P}_{h}\) & the set of historical prospects starting with \(h\); NB: \((h_{2},\Pi)\not\in\mathcal{P}_{h_{1}}\) for \(h_{1}\neq h_{2}\). \\ \(\mathcal{P}^{(y)}\) & the set of prospects on augmented state space \(\mathcal{S}\cup\{y_{i}\}\) \\ \((s,\Pi)\in\mathcal{P}\) & a prospect (stochastic process representing the controlled future) \\ \((h,\Pi)\) & a historical prospect (state space has been expanded to include \(h\)) \\ \(hs,sa,s\Pi,a\Pi,\dots\) & concatenations of subcomponents of trajectories/histories/policies; for example, the non-stationary policy \(s\Pi:=(a,\Pi_{0},\Pi_{1},\dots)\) \\ \(V\) & value function \(V:\mathcal{L}(\mathcal{S})\times\mathbf{\Pi}\rightarrow\mathbb{R}\), which is equal to the discounted sum of future rewards: \(V(\tilde{s},\Pi)=\mathbb{E}_{s_{0}\sim s,\Pi}\big{\{}\sum_{t=0}^{\infty} \big{[}\prod_{k=1}^{t}\gamma(s_{t-1},a_{t-1})\big{]}r(s_{t},a_{t})\big{\}}\) \\ \(\text{supp}(\tilde{p})\) & the support of distribution \(\tilde{p}\) \\ \(\mathcal{I}\) & index set \(\mathcal{I}\) of individuals (assumed finite) \\ \(w_{i},w_{i}(s),w_{i}(h)\) & the \(i\)th aggregation weight, either a constant, or a function of state or history \\ \(y_{i}(h)\) & the \(i\)th aggregation factor for history \(h\) \\ \(d_{\pi}\) & the stationary state-action distribution for policy \(\pi\) \\
**Generic modifiers** & \\ \(\cdot_{i}\) & index \(i\) of a sequence, vector, or collection of agents \\ \(\cdot_{i}^{j}\) & the slice from \(i\) to \(j\) of a sequence or vector; e.g., \(\tau_{t}^{t+k}\) is the trajectory slice \\ \(\cdot_{i}\) & \([(s_{t},a_{t}),\dots,(s_{t+k},a_{t+k})]\) \\ \(\cdot_{\tilde{\Sigma}}\) & indicates an aggregated item (e.g., \(\succ_{\tilde{\Sigma}}\) is social/aggregated preference) \\ \(\cdot^{\prime}\) & indicates next timestep when time implicit (e.g., \(s,s^{\prime}\)) \\ \(\cdot\) & indicates a probability distribution (e.g., \(\tilde{s}\in\mathcal{L}(\mathcal{S})\)) \\ \(\succ^{h}\) & indicates a preference relation on \(\mathcal{P}_{h}\) \\
**Operators** & \\ \(:=(=:)\) & defined as (is the definition of) \\ \(\succ\) & strict preference \\ \(\succeq\) & weak preference (\(p\succeq q\leftrightarrow q\not\succ p\)) \\ \(\approx\) & indifference (\(p\not\succ q\) \\ and \(q\not\succ p\)) & \\ \hline \hline \end{tabular}
\end{table}
Table 1: Notation summary.

[MISSING_PAGE_FAIL:16]

**Theorem 5** (Possibility).: _Consider the aggregation of arbitrary individual preference relations \(\{\succ_{\prec};i\in\mathcal{I}\}\) defined on \(\mathcal{L}(\mathcal{P})\), and consequently \(\mathcal{L}(\mathcal{P}_{h}),\ \forall h\), that individually satisfy Axioms 1-2. There exists aggregated preference relations \(\{\succ_{\frac{h}{\Sigma}}^{h}\}\), defined on \(\mathcal{L}(\mathcal{P}_{h}),\ \forall h\), that satisfy Axioms 1-4._

_In particular, **given**\(s,a\), \(V_{\Sigma}\), \(\{V_{i}\}\), \(\{w_{i}(s)\}\), **where (A)** each \(V_{i}\) satisfies Axioms 1-2 on \(\mathcal{L}(\mathcal{P})\), and \(V_{\Sigma}\) satisfies Axioms 1-2 on \(\mathcal{L}(\mathcal{P}_{h}),\forall h\), **and (B)**\(V_{\Sigma}(s,a\Pi)=\sum_{i}w_{i}(s)V_{i}(s,a\Pi))\), **then**, choosing_

\[w_{i}(sas^{\prime}):=w_{i}(sa):=w_{i}(s)\gamma_{i}(s,a)\quad\text{for all}\ i,s^{\prime}\] (13)

_implies that \(V_{\Sigma}(sas^{\prime},\Pi)\propto\sum_{i}w_{i}(sa)V_{i}(s^{\prime},\Pi)\) so that the aggregated preferences \(\{\succ_{\frac{sas^{\prime}}{\Sigma}}^{s}\}\) satisfy Axiom 3 on \(\mathcal{L}(\mathcal{P}_{sas^{\prime}})\). Unrolling this result--\(w_{i}(hsas^{\prime}):=w_{i}(hs)\gamma_{i}(s,a)\)--produces a set of constructive, history dependent weights \(\{w_{i}(h)\}\) such that Axiom 3 is satisfied for all histories \(\{h\}\)._

Proof.: We follow the proof of Theorem 4. Fix \(s,a\). Using Theorem 2, choose \(\{r_{i},\gamma_{i}\},r_{\Sigma},\gamma_{\Sigma}\) to represent individual and aggregate preferences over \(\mathcal{L}(\mathcal{P}_{s})\) and \(\mathcal{L}(\mathcal{P}_{sas^{\prime}})\). Define mixture policy \(\Pi_{\beta}:=\beta\Pi+(1-\beta)\Omega\). W.l.o.g. assume \(|\mathcal{I}|=2\).

We use Axiom 4 (Unrestricted Domain) to set

\[\begin{split}(T(s,a),\Pi)&\succ_{1}(T(s,a),\Lambda )\succ_{1}(T(s,a),\Omega),\\ (T(s,a),\Omega)&\succ_{2}(T(s,a),\Lambda)\succ_{2}(T (s,a),\Pi),\end{split}\] (14)

and, using Theorem 1 to shift \(V_{1},V_{2}\), we have

\[\begin{split} V_{1}(T(s,a),\Lambda)&=V_{2}(T(s,a), \Lambda)=0,\\ V_{1}(T(s,a),\Pi)&>0>V_{1}(T(s,a),\Omega)\quad\text{ s.t.}\quad V_{1}(T(s,a),\Pi_{\beta_{1}})=0,\\ V_{2}(T(s,a),\Omega)&>0>V_{2}(T(s,a),\Pi)\quad\text{ s.t.}\quad V_{2}(T(s,a),\Pi_{\beta_{2}})=0,\end{split}\] (15)

for some \(\beta_{1},\beta_{2}\in(0,1)\) with \(\beta_{1}\neq\beta_{2}\) (again appealing to Unrestricted Domain).

We now apply Theorem 3 followed by Theorem 2 to the expression \(V_{\Sigma}(s,a\Pi_{\beta})-V_{\Sigma}(s,a\Lambda)\). We again invoke Theorem 1 to shift \(V_{\Sigma}\) and eliminate the constant term, so that by Theorem 3\(\exists\left\{w_{i}(s)\right\}\) for which,

\[\begin{split} V_{\Sigma}(s,a\Pi_{\beta})-V_{\Sigma}(s,a\Lambda)& =\sum_{i\in\mathcal{I}}w_{i}(s)V_{i}(s,a\Pi_{\beta})-\sum_{i\in \mathcal{I}}w_{i(s)}V_{i}(s,a\Lambda)\\ &=w_{1}(s)\gamma_{1}(s,a)\left[V_{1}(T(s,a),\Pi_{\beta})-V_{1}(T (s,a),\Lambda)\right]+\\ &\quad\quad w_{2}(s)\gamma_{2}(s,a)\left[V_{2}(T(s,a),\Pi_{\beta} )-V_{2}(T(s,a),\Lambda)\right]\\ &=w_{1}(s)\gamma_{1}(s,a)V_{1}(T(s,a),\Pi_{\beta})+w_{2}(s) \gamma_{2}(s,a)V_{2}(T(s,a),\Pi_{\beta}).\end{split}\] (16)

where the second line applies Theorem 2, with rewards cancelling out.

It suffices to find one set of satisfactory \(w_{i}(h)\), so we can assume that, given \(s,a\), \(w_{i}(sas^{\prime}):=w(sa)\) is the same for all \(s^{\prime}\). This will allow us to factor it out below. Then, applying Theorem 2 followed by Theorem 3 to the same expression yields,

\[\begin{split} V_{\Sigma}(s,a\Pi_{\beta})-V_{\Sigma}(s,a\Lambda)& =\gamma_{\Sigma}(s,a)V_{\Sigma}(T(s,a),\Pi_{\beta})-\gamma_{\Sigma }(s,a)V_{\Sigma}(T(s,a),\Lambda)\\ &=w_{1}(sa)\gamma_{\Sigma}(s,a)\left[V_{1}(T(s,a),\Pi_{\beta})-V_{ 1}(T(s,a),\Lambda)\right]+\\ &\quad\quad w_{2}(sa)\gamma_{\Sigma}(s,a)\left[V_{2}(T(s,a),\Pi_{ \beta})-V_{2}(T(s,a),\Lambda)\right]\\ &=w_{1}(sa)\gamma_{\Sigma}(s,a)V_{1}(T(s,a),\Pi_{\beta})+w_{2}(sa )\gamma_{\Sigma}(s,a)V_{2}(T(s,a),\Pi_{\beta})\end{split}\] (17)

Combining equations 16 and 17, setting \(\beta\) to be \(\beta_{1}\) or \(\beta_{2}\) in equation, and rearranging, we obtain the equalities:

\[w_{1}(s)\gamma_{1}(s,a)=w_{1}(sa)\gamma_{\Sigma}(s,a)\quad\text{and}\quad w_{2}( s)\gamma_{2}(s,a)=w_{2}(sa)\gamma_{\Sigma}(s,a),\] (18)

from which we conclude that:

\[\frac{w_{2}(sa)}{w_{1}(sa)}=\frac{w_{2}(s)\gamma_{2}(s,a)}{w_{1}(s)\gamma_{1}(s,a)}\] (19)

This shows the existence of weights \(w_{i}(sa)\), unique up to a constant scaling factor, for which \(V_{\Sigma}(T(s,a),\Pi)\propto\sum_{i}w_{i}(sa)V_{i}(T(s,a),\Pi)\), that apply regardless of how individual preferences are chosen or aggregated at \(s\). Unrolling the result completes the proof.