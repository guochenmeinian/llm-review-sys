# SAME: Uncovering GNN Black Box with Structure-aware Shapley-based Multipiece Explanation

Ziyuan Ye\({}^{1,2,*}\), Rihan Huang\({}^{1,3,*}\), Qilin Wu\({}^{1,4}\), Quanying Liu\({}^{1,\dagger}\)

\({}^{1}\)Southern University of Science and Technology, \({}^{2}\)The Hong Kong Polytechnic University,

\({}^{3}\)King Abdullah University of Science and Technology, \({}^{4}\)Carnegie Mellon University

{ziyuanye9801, rihanhuang.work, kyrinwu}@gmail.com, liuqy@sustech.edu.cn

Equal contribution, co-first author.Corresponding author.

###### Abstract

Post-hoc explanation techniques on graph neural networks (GNNs) provide economical solutions for opening the black-box graph models without model retraining. Many GNN explanation variants have achieved state-of-the-art explaining results on a diverse set of benchmarks, while they rarely provide theoretical analysis for their inherent properties and explanatory capability. In this work, we propose Structure-Aware Shapley-based Multipiece Explanation (SAME) method to address the structure-aware feature interactions challenges for GNNs explanation. Specifically, SAME leverages an expansion-based Monte Carlo tree search to explore the multi-grained structure-aware connected substructure. Afterward, the explanation results are encouraged to be informative of the graph properties by optimizing the combination of distinct single substructures. With the consideration of fair feature interactions in the process of investigating multiple connected important substructures, the explanation provided by SAME has the potential to be as explainable as the theoretically optimal explanation obtained by the Shapley value within polynomial time. Extensive experiments on real-world and synthetic benchmarks show that SAME improves the previous state-of-the-art fidelity performance by 12.9% on BBBP, 7.01% on MUTAG, 42.3% on Graph-SST2, 38.9% on Graph-SST5, 11.3% on BA-2Motifs and 18.2% on BA-Shapes under the same testing condition. Code is available at [https://github.com/same2023heurips/samc](https://github.com/same2023heurips/samc)

## 1 Introduction

Graph neural networks (GNNs) have demonstrated a powerful representation learning ability to deal with data in non-Euclidean space. However, the explanation techniques for deep learning models on images and text cannot directly apply to understand GNNs, [27][18][1]. There is a gap in the understanding of how GNNs work, which largely limits GNNs' application in many fields.

Many GNN explanation techniques aim to examine the extent to which GNNs depend on individual nodes and edges of the graph [22][35][19][24][37]. However, graph features within nodes and edges contribute different amounts of information when considered individually than contextualized with topology [3][39]. Therefore, discovering the most important explanation with one or more connected components given an input graph and a well-trained GNN raises the additional challenge of handling structure-aware feature interactions. Recently, a number of studies, including GNN-LRP [25], SubgraphX [33] and GStarX [40] have endeavored to address this issue to some extent. Although many of the current GNN explanation techniques have empirically achieved state-of-the-artexplainability performance, the design of new GNN explanation techniques mainly relies upon empirical intuition, iterative experiments, and heuristics principles. To the best of our knowledge, the explanatory capability, potential limitations and inherent properties of GNN explanation techniques have not yet been thoroughly studied from a theoretical perspective.

In this work, we propose a novel Structure-Aware Shapley-based Multipiece Explanation (SAME) technique for fairly considering the multi-level structure-aware feature interactions over the graph by introducing expansion-based Monte Carlo tree search (MCTS). Our construction is inspired by the recently proposed perturbation-based GNN explanation methods [23][40], which have proven effective for providing explainability from a cooperative game perspective. We summarize the main differences between SAME and previous work in Table[1]**The main contributions and novelties** of this work include the following. (1) _Theoretical aspect:_ i) We review the characteristics of previous methods [22][35][19][25][38][40] and highlight several desired properties (see Table 1 that can be considered by explanation methods for GNNs. ii) We provide the loss bound of the MCTS-based explanation techniques and further verify the superiority of our expansion-based MCTS in SAME compared to previous work [38] in an intuitive manner (Sec. 3.2). (2) _Empirical aspect:_ Our experiments cover both real-world and synthetic datasets. The results show that i) SAME outperforms previous SOTA with fidelity and harmonic fidelity metrics under the same testing condition (Sec. 5.1). ii) SAME qualitatively achieves a more human-intuitive explanation compared to previous methods across multiple datasets (Sec. 5.2).

## 2 Related Work

Improving the explainability of GNN models in a post-hoc fashion has been a theme in deep graph learning. An intuitive way to explain a well-trained GNN is to trace the gradient in the models [31][22], where the larger gradient indicates the higher importance of node or edge of the graph. Previous work has also studied the decomposition-based methods [26][25][9] which decompose the final prediction into several terms and mark them as the important scores for input features. Another line of GNN explanation techniques lies in perturbation-based methods [19][31][11][34] which usually obtain a mask for the input graph in various ways to identify the important input feature. SubgraphX [38], as one of a perturbation-based method, samples the subgraphs from the input graph by the pruning-based MCTS and finds the most important one via the Shapley value. However, the pruning-based MCTS in SubgraphX leads to a much larger search space and thus causes higher computational costs. Moreover, SubgraphX can only provide a single connected explanation for each graph, which limits its explanatory power in many scenarios that require multipiece explanation. Most recently, GStarX [40] scores node importance based on the Hamiache-Navarro (HN) value. Although GStarX also fairly considers the structure-aware feature interactions, it fails to account for the multi-grained importance, which might result in a suboptimal explanation. For other categories in GNN explanation techniques, including surrogate methods [50][41][8][12], generation-based methods [36][15][32], and counterfactual-based methods [17][16], we refer readers to a recent survey [37].

\begin{table}
\begin{tabular}{c|c c c c c c|c} \hline \multirow{2}{*}{PropertiesMethods} & \multirow{2}{*}{
\begin{tabular}{c} Grad \\ CAM \\ \end{tabular} } & \multirow{2}{*}{GNNExpAligner} & \multirow{2}{*}{PGExplainer} & \multicolumn{3}{c|}{GNN} & \multirow{2}{*}{SubgraphX} & \multirow{2}{*}{GStarX} & \multirow{2}{*}{**SOME(Ours)**} \\  & & & & & & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\  & & & & & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \\ \hline Graph-level tasks & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ Node-level tasks & & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\ \hline Feature interactions & & & & & & ✓ & ✓ & ✓ \\ Structure awareness & & & & & ✓ & ✓ & ✓ & ✓ \\ Multipiece explanation & ✓ & ✓ & ✓ & ✓ & & ✓ & ✓ \\ \hline Node-wise importance & ✓ & & & & & ✓ & ✓ \\ Substructure-wise importance & & & & ✓ & ✓ & & ✓ \\ Composite-wise importance & & ✓ & ✓ & ✓ & ✓ & & ✓ \\ Priority-based integration & & ✓ & ✓ & & ✓ & & ✓ \\ Redundancy consideration & & ✓ & ✓ & & ✓ & ✓ \\ \hline \end{tabular} Note: _Feature interactions_ and _structure awareness_ are discussed in Sec. 3.1_Multipiece explanation_ is provided in Sec. 3.2_Multi-grained importance (node / substructure / composite), priority-based integration_ and _redundancy consideration_ are presented in Sec. 4 Detailed mathematical definitions are provided in Appendix B.1.

\end{table}
Table 1: Comparison of the properties of different GNN explanation techniques.

Theoretical Motivation

**Notation and Preliminaries**. The well-trained target GNN to be explained can be formulated as \(f:\mathcal{G}\rightarrow\mathcal{Y}\), where \(\mathcal{G}\) denotes the space of input graphs and \(\mathcal{Y}\) refers to the related label space. A graph can be denoted as \(G=(V,X,E)\), where \(V\in\mathbb{R}^{n\times n}\) represents node set, \(X\in\mathbb{R}^{n\times d}\) is the node feature set and \(E\in\mathbb{R}^{n\times n}\) denotes edge set. Given a well-trained GNN model \(f(\cdot)\) and an input graph \(G\), the goal of GNN explanation is to find the most important explanation \(G^{*}_{ex}\) from \(G\). Formally, this can be defined as an optimization problem that maximizes the importance of the explanation \(G_{ex}\) for a given graph \(G\) with \(n\) nodes using an importance scoring function \(I(f(\cdot),G_{ex},G)\):

\[G^{*}_{ex}=\operatorname*{arg\,max}_{G_{ex}\subseteq G}\;I(f(\cdot),G_{ex},G), \tag{1}\]

where each explanation \(G^{i}_{ex}\) has \(n_{i}\) nodes, and the other nodes not in the explanation can be expressed as \(\{G\backslash G^{i}_{ex}\}=\{v_{j}\}_{j=n_{i}+1}^{n}\). It is noteworthy that each explanation \(G^{i}_{ex}\) might contain one or more substructures (_i.e._, connected components).

### Structure-aware Shapley-based Explanations Satisfies Fairness Axioms

Unlike grid-like images or sequence-like texts, graphs have more complex and abstract structures. The importance scoring function of explanations determines the reliability and explanatory power of the GNN explanation method. Therefore, we present the idea that a comprehensive assessment of the importance of an explanation should consider the feature interactions under the constraints of the input graph's topology.

Shapley value [28], originating from cooperative game theory, is the unique credit allocation scheme that satisfies the fairness axioms. This concept is similar to the importance scoring function for explanation with the consideration of feature interactions. Some previous work have brought Shapley value into deep learning explanation methods [18][14]. In our study, the importance assessment of explanation is treated as a cooperative game, where the explanation \(G^{i}_{ex}\) and all nodes not in the explanation \(\{G\backslash G^{i}_{ex}\}\) are the players in the game. Therefore, when scoring the importance of any explanation \(G^{i}_{ex}\), a set of players participating in the game can be denoted as:

\[P_{i}=\{G^{i}_{ex},\underbrace{v_{n_{i}+1},v_{n_{i}+2},\ldots,v_{n}}_{\{G \backslash G^{i}_{ex}\}}\}.\]

Inspired by the close connection between _feature interactions_ and Shapley value, we define several desirable properties of importance scoring function for explanation according to fairness axioms:

**Property 1**.: _(Efficiency). The sum importance of all players \(p_{j}\) in \(P_{i}\) is the same as the improvement of GNN \(f(\cdot)\) on \(P_{i}\) over an empty set, \(\sum_{j=1}^{|P_{i}|}I(f(\cdot),p_{j},G)=f(P_{i})-f(\emptyset)\)._

**Property 2**.: _(Symmetry). For any explanation \(G^{i}_{ex}\), if there exist two other players \(p_{j},p_{k}\in\{P_{i}/G^{i}_{ex}\}\) that satisfy \(f(G^{i}_{ex}\cup p_{j})=f(G^{i}_{ex}\cup p_{j})\), then \(I(f(\cdot),p_{j},G)=I(f(\cdot),p_{k},G)\)._

**Property 3**.: _(Dummy). If a player \(p_{j}\) makes no contribution to GNN \(f(\cdot)\), i.e. \(f(G^{i}_{ex}\cup p_{j})=f(G^{i}_{ex})\) holds for any \(G^{i}_{ex}\), then \(I(f(\cdot),p_{j},G)=0\)._

**Property 4**.: _(Monotonicity). Consider two well-trained GNN models \(f_{1}(\cdot)\) and \(f_{2}(\cdot)\), given an explanation \(G^{i}_{ex}\), if for any player \(p_{j}\), \(f_{1}(G^{i}_{ex}\cup p_{j})-f_{1}(p_{j})\geq f_{2}(G^{i}_{ex}\cup p_{j})-f_{2} (p_{j})\) always holds, then \(I(f_{1}(\cdot),G^{i}_{ex},G)\geq I(f_{2}(\cdot),G^{i}_{ex},G)\)._

Shapley value can well satisfy the above four fairness Properties[14] However, the Shapley-based importance scoring function attempts to take all nodes in the graph except explanation \(G^{i}_{ex}\) into the cooperation game, which not only ignores the topological information of the input graph but also brings huge computational costs. Fortunately, [28] alleviate this issue by modifying the Shapley-based importance scoring function as 'k-hop Shapley', which also makes it _structure-aware_:

\[I(f(\cdot),G^{i}_{ex},G)=\sum_{p_{i}\subseteq\{P_{i,khop}\backslash G^{i}_{ex} \}}\frac{|p_{i}|(|P_{i,khop}|-|p_{i}|-1)!}{|P_{i,khop}|!}(f(p_{i}\cup G^{i}_{ ex})-f(p_{i})), \tag{2}\]where \(I(f(\cdot),G^{i}_{ex},\mathcal{G})\) denotes the weighted sum of the marginal contribution of explanation \(G^{i}_{ex}\), and \(P_{i,khop}=\{G^{i}_{ex},v_{n,i+1},v_{n,i+2},\ldots,v_{n+i+k_{i}}\}\) includes the nodes within the \(G^{i}_{ex}\) as well as the nodes in the k-hop neighbors of \(G^{i}_{ex}\).

### Structure-aware Shapley-based Multipiece Explanation Provide Strong Explainability

The characteristics and properties within a graph or node tend to be _jointly_ influenced by more than one high-order connected community of the graph. This implies that the appropriate explanation within this context requires the GNN explanation method to provide the multiple connected substructures simultaneously.

To solve the above challenge, we first define the mathematical formalization of the search processes on graphs by utilizing the MCTS-based GNN explanation methods. Then we propose a mathematically coherent framework to explore the explanatory power of MCTS-based GNN explanation methods using computational methods for the multi-armed bandit problem in the MCTS algorithm [13][1].

Our mathematical framework is based on the hierarchical partitioning of the MCTS search space \(\mathcal{X}\). More precisely, the smoothness of the MCTS search space \(\mathcal{X}\) can be defined by the inequality \(|f(x_{i})-f(x_{j})|\leq l(x_{i},x_{j})\), where \(l(x_{i},x_{j})\) refers to the Lipschitz continuity between any two substructures \(x_{i}\) and \(x_{j}\) in \(\mathcal{X}\). It serves as a critical metric to ascertain the boundedness of the change in the explanation method \(f(\cdot)\) concerning the change in input substructures. Assuming that the function \(f(\cdot)\) is Lipschitz continuous and the \(l\) is given, an evaluation of the function \(f(\cdot)\) at any point \(x_{t}\) enables us to define an upper bounding function \(B_{t}(x)\) for \(f(\cdot)\). This upper bounding function can be refined after each evaluation of \(f(\cdot)\):

\[\forall x\in\mathcal{X},f(x)\leq B_{t}(x)\mathop{=}\limits^{def}\mathop{min} \limits_{1\leq s\leq t}[f(x_{s})+l(x,x_{s})], \tag{3}\]

where metric \(l\) satisfies the following Assumptions [1][3]. In the context of computational uncertainties associated with MCTS, the evaluation strategy in [3] offers the potential to describe specific numerical estimates within an undefinable space.

**Assumption 1**.: _(Local smoothness). There exists at least one stage-optimal substructure \(x^{\star}\in\mathcal{X}\) of \(f(\cdot)\) (i.e. \(f(x^{\star})=sup_{x\in\mathcal{X}}f(x)\)) and \(\forall x\in\mathcal{X},f(x^{\star})-f(x)\leq l(x,x^{\star})\) holds._

**Assumption 2**.: _(Decreasing diameters). There exists a decreasing sequence \(\delta(h)>0\), such that for any depth \(h\geq 0\) and for any cell \(\mathcal{X}_{h,i}\) of depth \(h\), \(\sup_{\mathcal{X}_{h,i}}\)\(l(\mathcal{X},\mathcal{X}_{h,i})\leq\delta(h)\) holds._

**Assumption 3**.: _(Well-shaped cells). There exists \(\nu>0\) such that for any depth \(h\geq 0\), any cell \(\mathcal{X}_{h,i}\) contains a \(l\)-ball of radius \(\nu\delta(h)\) centered in \(x_{h,i}\)._

With the given assumptions, the search space \(\mathcal{X}\) can be partitioned into \(K^{H}\) subsets (_i.e._, cells) \(\mathcal{X}_{h,i}\) using a \(K\)-ary tree of depth \(H\), where \(0\leq h\leq H,0\leq i\leq K^{h-1}\). Based on this partitioning method, the MCTS process can be treated as the expansion of this \(K\)-ary tree. The root of \(K\)-ary tree (_i.e._, cell \(\mathcal{X}_{0,0}\)) corresponds to the whole search space \(\mathcal{X}\). Each cell \(\mathcal{X}_{h,i}\) corresponds to a node \((h,i)\) of the tree, where \(h\) denotes the depth of the tree and \(i\) refers to the index. Each node \((h,i)\) possesses \(K\) children nodes \(\{(h+1,i_{k})\}_{1\leq k\leq K}\) s.t. the associated cells \(\{\mathcal{X}_{h+1,i_{k}},1\leq k\leq K\}\) form a partition of the parent's cell \(\mathcal{X}_{h,i}\). Consequently, expanding one node requires adding one of its \(K\) children to the current tree, which corresponds to subdividing the cell \(\mathcal{X}_{h,j}\) into \(K\) children cells \(\{\mathcal{X}_{h+1,j_{1}},\ldots,\mathcal{X}_{h+1,j_{K}}\}\). Assume that there exist a decreasing sequence \(\delta(h)\geq 0\) that satisfies \(\sup_{\mathcal{X}_{h,i}}\)\(l(\mathcal{X},\mathcal{X}_{h,i})\leq\delta(h)\) for any \(\mathcal{X}_{h,i}\). The decreasing sequence \(\delta(h)\) ensures that each cell size reduces with increasing depth.

The search space \(\mathcal{X}\) can be divided through the above partitioning method according to \(\delta(h)\), with respect to \(l\)-open balls. Let \(\mathbf{T}_{t}\) denote nodes of the current tree, and \(\mathbf{L}_{t}\) denotes the incoming leaves of \(\mathbf{T}_{t}\) to be expanded at round \(t\). Recalling our earlier definition of \(B_{t}(h)\) which was derived from the Lipschitz continuity, we can now generalize it to a new representation that connects to \(\delta(h)\). Formally, it is expressed as:

\[b_{h,i}=f(x_{h,i})+\delta(h). \tag{4}\]Based on this, we now consider which nodes will be expanded during the search. Note that Assumption 2 implies that the b-value of any cell contains \(x^{\star}\) upper bounds \(f^{\star}\). In other words, for any cell \(\mathcal{X}_{h,i}\) such that \(x^{\star}\in\mathcal{X}_{h,i}\),

\[b_{h,i}=f(x_{h,i})+\delta(h)\geq f(x_{h,i})+l(x_{h,i},x^{\star})\geq f^{\star}. \tag{5}\]

This means that a leaf \((h,i)\) of a \(K\)-ary tree will never be expanded if \(f(x_{h,i})+\delta(h)<f^{\star}\). Therefore, under this partitioning strategy, the only set of nodes that will be expanded could be defined as \(I\stackrel{{ def}}{{=}}\cup_{h\geq 0}I_{h}\), which could be stated as

\[I_{h}\stackrel{{ def}}{{=}}\{nodes\{h,i\}\text{ such that }f(x_{h,i})+ \delta(h)\geq f^{\star}\}. \tag{6}\]

In order to derive a loss bound, we now define a measure of the quantity of near-optimal states, called _near-optimality dimension_. For any \(\epsilon>0\), the set of \(\epsilon\)-optimal states can be defined as

\[\mathcal{X}_{\epsilon}:=\{x\in\mathcal{X},f(x)\geq f^{\star}-\epsilon\}. \tag{7}\]

**Definition 1**.: _(\(\eta\)-near-optimality dimension). The \(\eta\)-near-optimality dimension is the smallest \(d\geq 0\) such that there exists \(C\geq 0\), for all \(\epsilon>0\), the maximum number of disjoint \(l\)-balls of radius \(\eta\epsilon\) with centers in \(\mathcal{X}_{\epsilon}\) is less than \(C\epsilon^{-d}\)._

Definition 1 represents the number of near-optimal states for a function \(f(\cdot)\) around its optimal solution. It is important to note that \(d\) is not an intrinsic property of \(f(\cdot)\) as we are packing near-optimal states using \(l\)-balls. Instead, it characterizes both \(f(\cdot)\) and \(l\) and depends on the constant. In order to relate this measure to the algorithmic details, we also need to correlate it with the characteristics of the partitioning, specifically the shape of the cells. That is, the near-optimality dimension \(d\) is dependent on a particular constant, which will be determined in accordance with the parameter \(\nu\) as defined in Assumption 2

**Lemma 1**.: _Let \(d\) be the \(\nu\)-near-optimality dimension, and \(C\) the corresponding constant. Then \(|I_{h}|\leq C\delta(h)^{-d}\)._

Building upon Lemma 1 we further analyze the loss of the MCTS-based GNN explanation methods across \(n\) iterations in Theorem 1

**Theorem 1**.: _Let us write \(h(n)\) the smallest integer \(h\) such that \(C\Sigma_{l=0}^{h}\delta(l)^{(-d)}\geq n\), then the loss of the MCTS-based GNN explanation methods is bounded as:_

\[r_{n}\leq\delta(h(n)) \tag{8}\]

The loss \(r_{n}\) reflects the gap between the obtained and optimal explanations over \(n\) iterations. We aim to bound this loss by \(\delta(h(n))\), illustrating that refining the partition of the search space \(\mathcal{X}\) reduces the loss, thus better approximating the optimal explanation. We provide a complete description of the mathematical properties and theorem proofs of the framework in Appendix B.2.

Leveraging the mathematical underpinnings provided above, as demonstrated in Figure 1 we employ an example to contextualize the theoretical insights within the GNN explanation. The ground truth

Figure 1: Illustration of ground truth explanation and the possible sub-optimal explanations provided by pruning-based MCTS explanation techniques.

explanation is highlighted in yellow which includes two connected components. When searching for explanations starting from any node in the two components through the pruning-based MCTS, _the nodes in other components_ are accessible only via the unimportant node which is highlighted in blue. In this situation, given sparsity constraints, the pruning-based MCTS can only generate the suboptimal explanation. As a consequence, regardless of the search trajectory adopted, the diameter of the \(l\)-ball remains unyielding, failing to converge to the \(\eta\)-near optimality numerical solution. Therefore, it is necessary to design an explanation method that can accurately retain important nodes while avoiding irrelevant nodes, thus increasing the likelihood of discovering the optimal explanation. This ambition resonates with previously proposed loss bound \(r_{n}\leq\delta(h(n))\), emphasizing the need for advanced exploration to reduce losses and approximate the optimal explanation with higher precision.

## 4 Structure-aware Shapley-based Multipiece Explanation Method

As we discussed in Section 3 the structure-aware Shapley-based multipiece explanation provides a potential way to effectively uncover the GNN black box. In order to approximate the optimal Shapley-based explanation, we propose a two-phase framework, Structure-aware Shapley-based Multipiece Explanation (SAME) method, which is composed of (1) an _important substructure initialization_ phase and (2) an _explanation exploration_ phase, as shown in Figure. 2

In the first phase (Section4.1), we extend an expansion-based Monte Carlo tree search as an important substructure initializer to generate connected components not only of high importance but also of multi-grained diversity. In the second phase (Section4.2), we apply the important substructure set as an action set in another expansion-based Monte Carlo tree search to explore potential explanations.

### Important Substructure Initialization

In this section, we propose an expansion-based MCTS approach for important substructure initialization. It is intuitive that a favorable initialization should not exclude important substructures at any scale and should not include redundant substructures. Formally, these can be defined as:

**Property 5**.: _(Node-wise importance). Given an input graph \(G\) to be explained, for any node \(v_{i}\in G\), its \(I(f(\cdot),v_{i},G)\) importance will be considered._

**Property 6**.: _(Substructure-wise importance). Given an input graph \(G\) to be explained, for any substructure \(G_{sub_{i}}\subseteq G\), its importance \(I(f(\cdot),G_{sub_{i}},G)\) will be considered._

Figure 2: **Overview of Structure-A**ware Shapley-based **M**ultipiece **E**xplanation (SAME) method. (a) _Important substructure initialization phase_ aims at searching the single connected important substructure. (b) _Explanation exploration phase_ provides a candidate set of explanations by optimizing the combination of different important substructures. (c) The comparison of the final explanation with the highest importance score from the candidate set with the optimal explanation.

**Property 7**.: _(Composite-wise importance). Given an explanation \(G^{i}_{com}\subseteq G\) consisting of one or more substructures, its importance \(I(f(\cdot),G^{i}_{com},G)\) will be considered._

**Property 8**.: _(Priority-based integration). Given an explanation \(G^{i}_{ex}\) with any size, the node \(v_{i}\in\{G\backslash G^{j}_{ex}\}\) will be added on \(G^{j}_{ex}\) to get a new explanation \(G^{i}_{ex}\), if and only if for any \(v_{l}\in\{G\backslash(G^{j}_{ex}\cup v_{i})\}\), \(I(f(\cdot),G^{j}_{ex}\cup v_{i},G)>I(f(\cdot),G^{j}_{ex}\cup v_{l},G)\) holds._

**Property 9**.: _(Redundancy consideration). Given an explanation \(G_{ex}\subseteq G\), if \(I(f(\cdot),G_{ex}\backslash\{i\},G)>I(f(\cdot),G_{ex},G)\) holds, the new explanation \(G^{\prime}_{ex}=G_{ex}\backslash\{i\}\) will be chosen._

Taking the above properties into account, we propose our expansion-based Monte Carlo tree search for important substructure initialization which aims at providing important connected components of the graph. The detailed Algorithm is presented in Appendix C. Given a graph \(G\) to be explained, the node in the MCTS is defined as \(N_{i}\) which contains the following variables:

\[N_{i}:\{G^{i}_{sub},T_{i},R_{i},A_{i},C_{i},W_{i}\}\]

where \(G^{i}_{sub}\) denotes the corresponding substructure of graph \(G\) for node \(N_{i}\) in the search tree. \(T_{i}\) is the visiting time of node \(N_{i}\) in the search tree, \(R_{i}\) refers to the the importance (reward) of substructure \(G^{i}_{sub}\). \(A_{i}\) represents the action set of node \(N_{i}\). \(C_{i}=\{N_{j}\}_{j=1}^{|A_{i}|}\) represents a set of children with respect to node \(N_{i}\), and we denote \(N_{j}\) as one of the child nodes obtained through the action \(a_{j}\in A_{i}\) at parent node \(N_{i}\). \(W_{i}\) means the sum of the children's rewards.

The expansion-based MCTS is initialized with an empty set, \(N_{0}=\emptyset\). At the beginning of each iteration, our method will randomly choose an unvisited node from the graph, or choose the node with the highest reward if all nodes have been visited. Then, the tree will be iteratively expanded according to the _node-wise tree expansion_ until it reaches a leaf node. Specifically, given an MCTS node \(N_{i}\), it will only choose the child node within 1-hop neighbors of the associate \(G^{i}_{sub}\) to expand, which means that the action set is topology dependent. Therefore, the substructure of any nodes in the search tree will be a connected component. During the child node selection, the reward of each child node \(R_{j}\) of \(N_{i}\) will be calculated following Equation 1. Finally, the chosen action is decided following the child selection strategy:

\[a^{\star}=\operatorname*{arg\,max}_{a_{j}}\ \frac{W_{j}}{T_{j}}+\beta R_{j} \frac{\sqrt{\sum_{k\in C_{i}}T_{k}}}{1+T_{j}} \tag{9}\]

where \(\beta\) is a hyperparameter for balancing exploration-exploitation trade-off 1. We define the maximum substructure size as \(\gamma\), and for any \(G^{i}_{sub}\subseteq G\), \(|G^{i}_{sub}|\leq\gamma\) always holds. After reaching the maximum substructure size, the reward of associated leaf nodes is backpropagated up the tree along the search path, updating all the information stored in each node of the path. The important substructure set includes all nodes in MCTS after performing \(M_{1}\) times iterations.

### Explanation Exploration

In this section, we propose another expansion-based MCTS for exploring high-explainable explanations. The detailed algorithm is provided in Appendix C. Different from the previous expansion-based MCTS provided in Section 4.1 we propose a slight modification on it such that we change the action set from node level to substructure level. As Section 3.2discussed, this can be useful not only to obtain more flexible explanation results on real-world cases but also to provide the higher potential to approximate the theoretically optimal Shapley-based explanation.

The action set of MCTS in this phase is built upon the substructure set derived from the important substructure initialization phase. Similar to the previous MCTS, at the beginning of each iteration, it will randomly select an unvisited substructure from the set, or choose the substructure with the highest reward to expand if all substructures have been visited. Afterward, the _substructure-wise tree expansion_ also follows the Equation 1 and 2 to develop the tree and provide the explanation candidate. The action set corresponding to each node of MCTS in the phase is the whole substructure set. To further accelerate the exploration, we filter the unimportant substructures by only keeping the top K important substructure in the action set.

Notice that calculating all possible combinations of either substructures or nodes can definitely obtain the optimal Shapley-based explanation. Nevertheless, such node-wise or substructure-wise brute force methods lead to \(O(2^{|V|})\) computational complexity, which is an NP-hard problem. We provide a feasible solution to approximate the optimal Shapley-based explanation in polynomial time \(\mathcal{O}(M_{1}\gamma|V|^{2}+M_{1}\gamma|V|\times|E|+M_{2}Kt_{s}\frac{|V| \times(1-sparsity)}{\gamma})\), where \(M_{1}\) and \(M_{2}\) denote maximum number of iterations of the first phase and second phase respectively, \(\gamma\) is the maximum substructure size, \(K\) refers to the size of the important substructure set and \(t_{s}\) is the time budget of MCTS in the second phase. We leave the derivation of the time complexity in Appendix B.4.

## 5 Experiments

Our objective of the experiments is to understand the following two questions. 1) Are the explanations provided by SAME more informative and faithful compared to other methods under the same test conditions? 2) Can SAME provide a more human-intuitive explanation than others? To this end, we perform extensive quantitative and qualitative analysis to evaluate the explanatory power of SAME, following previous literature [38][40]. The SAME is compared with various competitive baselines and shows state-of-the-art (SOTA) results in all the cases.

**Dataset**. The experiments are conducted on six datasets with diverse categories, including molecular graphs (e.g., BBBP [31] and MUTAG [7]), sentiment graphs (e.g., Graph-SST2 and Graph-SST5 [29]) and synthetic Barabasi-Albert graphs (e.g., BA-2Motifs [13] and BA-Shapes [35]). We conduct a node classification task for the BA-Shapes dataset, and graph classification tasks for the rest five datasets. More detailed descriptions of datasets are provided in Appendix D.

**Metrics**. In this work, we use several criteria [37] to evaluate our approach: (1) _Sparsity_ quantifies how compact are the explanations, and further facilitates fair comparison by restricting the different explanations to the same size. (2) _Fidelity_ determines how informative and faithful are the explanations by removing the selected nodes. (3) _Inv-Fidelity_ measures the explanations from the same aspect as fidelity while it keeps the selected nodes. (4) _Harmonic fidelity_[40] normalizes fidelity by sparsity and takes a harmonic mean to make different explanations comparable with a single metric. We leave detailed mathematical definitions of the above metrics in Appendix E.

**Experimental setup**. In the important substructure initialization phase, we set the MCTS iteration number \(M_{1}\) to 20. The exploration-exploitation trade-off \(\beta\) is set to 5 for BBBP and 10 for other datasets. The substructure size \(\gamma\) has different settings in different datasets. In the explanation exploration phase, we set the hyperparameter \(K=7\) for important substructure filtering, \(M_{2}=10\) for the MCTS iteration number, and the other hyperparameters of MCTS remain the same as the previous phase. We follow [37][40] to set other baselines hyperparameters. All methods are implemented in PyTorch [7] and PyG [10]. Our experiments are conducted on a single Nvidia V100 GPU with an Intel Xeon Gold 5218 CPU. We leave the detailed settings in Appendix F.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline \multirow{2}{*}{Dataset} & \multicolumn{6}{c|}{Graph classification} & Node classif. \\ \cline{2-7}  & \multicolumn{2}{c|}{Molecular graph} & \multicolumn{2}{c|}{Semantic graph} & \multicolumn{2}{c}{Synthetic graph} \\ \cline{2-7}  & BBBP & MUTAG & Graph-SST2 & Graph-SST5 & BA-2Motifs & BA-Shapes \\ \hline Grad-CAM [7] & 0.226\(\pm\)0.036 & 0.261\(\pm\)0.018 & 0.257\(\pm\)0.056 & 0.229\(\pm\)0.042 & 0.472\(\pm\)0.010 & - \\ GNNExplainer [35] & 0.148\(\pm\)0.041 & 0.188\(\pm\)0.031 & 0.143\(\pm\)0.041 & 0.170\(\pm\)0.046 & 0.442\(\pm\)0.026 & 0.154\(\pm\)0.000 \\ PGExplainer [19] & 0.197\(\pm\)0.043 & 0.156\(\pm\)0.004 & 0.219\(\pm\)0.040 & 0.207\(\pm\)0.036 & 0.431\(\pm\)0.011 & 0.135\(\pm\)0.020 \\ GNN-LRP [25] & 0.111\(\pm\)0.040 & 0.253\(\pm\)0.030 & 0.103\(\pm\)0.042 & 0.131\(\pm\)0.057 & 0.146\(\pm\)0.010 & 0.155\(\pm\)0.000 \\ SubgraphX [35] & 0.433\(\pm\)0.073 & 0.379\(\pm\)0.030 & 0.262\(\pm\)0.027 & 0.283\(\pm\)0.042 & 0.493\(\pm\)0.003 & 0.181\(\pm\)0.005 \\ GStarX [40] & 0.117\(\pm\)0.043 & 0.656\(\pm\)0.096 & 0.183\(\pm\)0.050 & 0.186\(\pm\)0.050 & 0.476\(\pm\)0.014 & - \\ \hline
**SAME** & **0.489\(\pm\)0.034** & **0.702\(\pm\)0.125** & **0.373\(\pm\)0.042** & **0.393\(\pm\)0.02** & **0.549\(\pm\)0.004** & **0.214\(\pm\)0.000** \\
**Relative Improve** & **12.9\%\(\uparrow\)** & **7.01\%\(\uparrow\)** & **42.3\%\(\uparrow\)** & **38.9\%\(\uparrow\)** & **11.3\%\(\uparrow\)** & **18.2\%\(\uparrow\)** \\ \hline \hline \end{tabular} Note: The fidelity results are averaged across different sparsity from 0.5 to 0.8. The quantitative results are presented in the form of mean \(\pm\) std. The previous SOTA results on different datasets are marked with an underline. _Relative Improve_ denotes the relative improvement of our SAME method over the SOTA methods.

\end{table}
Table 2: Comparison of our SAME and other baseline using fidelity.

### Quantitative Analysis

To validate the overall explainability performance, we compare the proposed SAME with a series of competitive baselines under different metrics. Table 2 shows the averaged fidelity under different sparsity (_i.e._, sparsity=[0.5,0.6,0.7,0.8]). The proposed SAME significantly outperforms the previous state-of-the-art on both real-world and synthetic datasets. Specifically, the performance improvement of SAME is 12.9% on BBBP, 7.01% on MUTAG, 42.3% on Graph-SST2, 38.9% on Graph-SST5, 11.3% on BA-2Motifs and 18.2% on BA-Shapes. Notably, we also demonstrate reliable improvements of SAME over previous SOTA methods in terms of harmonic fidelity at different sparsities, with an average improvement of 1.92% on the graph classification task. This result implies SAME has a higher ability to discover important components in the graph than all baseline methods. As for the inv-fidelity metric, the explanatory power of SAME is competitive compared with the previous SOTA. Detailed results for harmonic fidelity and inv-fidelity are in Appendix G.1. Since the proposed SAME is a model-agnostic method, it works well with GAT and GIN. More comparisons with GraphSVX [3] and OrphicX [10] are provided in Appendix G.3.

The computational cost of SAME and other baselines on different datasets are summarized in Table 3. We show that SAME consistently achieves much lower computational cost compared to GStarX and SubgraphX, reflecting its efficiency and robustness. This further verifies that expansion-based MCTS in SAME can work effectively in various scenarios.

### Qualitative Analysis

Figure 4 presents the visualization comparison of the explanations on sentiment graphs. The nodes of adjectives and adverbs are considered to be important for they reveal the attitude of a sentence. Thus, the optimal explanation here is therefore the word or phrase with a positive meaning. In this sense,

Figure 3: Comparison of the explanations on Graph-SST2 with GCN classifier.

\begin{table}
\begin{tabular}{l|c c c c c c} \hline \hline MethodsDataset & BBBP & MUTAG & Graph-SST2 & Graph-SST5 & BA-2Motifs & BA-Shapes \\ \hline Grad-CAM [22] & 0.16 & 0.23 & 0.39 & 0.44 & 0.14 & - \\ GNNExplainer [13] & 7.56 & 1.96 & 7.64 & 19.39 & 1.89 & 2.72 \\ PGExplainer [19] & 0.15 & 0.21 & 0.35 & 0.43 & 0.12 & 0.13 \\ GNN-LRP [25] & 2.37 & 1.97 & 5.84 & 5.47 & 3.30 & 51.77 \\ SubgraphX [28] & 26.72 & 151.75 & 36.48 & 71.32 & 85.50 & 162.80 \\ GStarX [40] & 84.54 & 25.24 & 30.64 & 54.49 & 77.99 & - \\ \hline
**SAME** & 7.86 & 5.67 & 6.06 & 8.83 & 8.19 & 14.08 \\ \hline \hline \end{tabular} Note: The PGExplainer needs training before inferring the explanation.

\end{table}
Table 3: Comparison of inference time (in seconds) on different datasets.

SAME can better capture the adjectives-or-adverbs-like graph structures than other baselines. For instance, SubgraphX focuses on adjectives and adverbs but fails to capture the "but" word which bears significant weight under the contrasting relationship. Intuitively, without the "but", the contribution of "tortured" and "unsettling" should be negative. GStarX achieves to identify words that are consistent with the opinion such as "carefully" and "alive," yet overlooks the crucial transitional relationship between "alive" and "unsettling". Figure 1 shows the visualization of explanations on molecular graphs. The ground truth explanations (_i.e._, functional group \(-\mathbf{NO_{2}}\)) of MUTAG are labelled by human experts. We see that SAME is able to provide the explanations the same as the ground truth. We also illustrate the explanation of the synthetic graph in Figure 1. The ground-truth label of all the graphs in BA-2Motifs is a 5-node-house-structure motif. Results show that SAME exactly finds the ground-truth explanation compared to other baselines. We leave more comparisons in Appendix G.2.

## 6 Conclusion

Structure-aware Shapley-based Multipiece Explanation provides strong explainability over GNN models, while this ability is limited by only using the single connected substructure. Moving forward from the theoretical deduction, we propose the SAME method for explaining GNN, a novel perturbation-based method that is aware of input graph structure, feature interactions, and multi-grained importance. Experimental results demonstrate that our SAME consistently outperforms SOTA methods on multiple benchmarks by a large margin with various metrics and provides a more understandable explanation.

**Limitations.** In the implementation of SAME, the Shapley value is obtained through approximation following [33]. Under this approximation, the fairness axioms discussed in Section [3.1] no longer hold. This is also identified as an unsolved issue for the Shapley value in machine learning by [23]. In addition, the scalability of SAME on large graphs can also become a potential challenge. As the time complexity shown in Section [4] the time overhead caused by the increase in the number of nodes will become very expensive when scaling up the size of the input graph.

Figure 4: Comparison of the explanations on MUTAG with GIN classifier.

Figure 5: Comparison of the explanations on BA-2Motifs with GCN classifier.

## Acknowledgements

This work was funded in part by the National Key R&D Program of China (2021YFF1200804), National Natural Science Foundation of China (62001205), Shenzhen Science and Technology Innovation Committee (2022410129, KCXFZ2020122117340001), Shenzhen-Hong Kong-Macao Science and Technology Innovation Project (SGDX2020110309280100), Guangdong Provincial Key Laboratory of Advanced Biomaterials (2022B1212010003).

We would like to extend our sincere appreciation to Dr. Jialin Liu for her insightful suggestions on the manuscript composition and the elucidation of definitions.

## References

* [1] S. Ali, T. Abuhmed, S. El-Sappagh, K. Muhammad, J. M. Alonso-Moral, R. Confalonieri, R. Guidotti, J. Del Ser, N. Diaz-Rodriguez, and F. Herrera. Explainable artificial intelligence (xai): What we know and what is left to attain trustworthy artificial intelligence. _Information Fusion_, page 101805, 2023.
* [2] M. Bajaj, L. Chu, Z. Y. Xue, J. Pei, L. Wang, P. C.-H. Lam, and Y. Zhang. Robust counterfactual explanations on graph neural networks. _Advances in Neural Information Processing Systems_, 34:5644-5655, 2021.
* [3] F. Baldassarre and H. Azizpour. Explainability techniques for graph convolutional networks. In _International Conference on Machine Learning (ICML) Workshops, 2019 Workshop on Learning and Reasoning with Graph-Structured Representations_, 2019.
* [4] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. _IEEE Transactions on Computational Intelligence and AI in Games_, 4(1):1-43, 2012.
* [5] I. Covert, S. M. Lundberg, and S.-I. Lee. Understanding global feature contributions with additive importance measures. _Advances in Neural Information Processing Systems_, 33:17212-17223, 2020.
* [6] I. C. Covert, S. Lundberg, and S.-I. Lee. Explaining by removing: A unified framework for model explanation. _The Journal of Machine Learning Research_, 22(1):9477-9566, 2021.
* [7] A. K. Debnath, R. L. Lopez de Compadre, G. Debnath, A. J. Shusterman, and C. Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. _Journal of Medicinal Chemistry_, 34(2):786-797, 1991.
* [8] A. Duval and F. D. Malliaros. Graphsvx: Shapley value explanations for graph neural networks. In _Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part II 21_, pages 302-318. Springer, 2021.
* [9] Q. Feng, N. Liu, F. Yang, R. Tang, M. Du, and X. Hu. Degree: Decomposition based explanation for graph neural networks. In _International Conference on Learning Representations_, 2022.
* [10] M. Fey and J. E. Lenssen. Fast graph representation learning with pytorch geometric. _arXiv preprint arXiv:1903.02428_, 2019.
* [11] T. Funke, M. Khosla, M. Rathee, and A. Anand. Zorro: Valid, sparse, and stable explanations in graph neural networks. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* [12] Q. Huang, M. Yamada, Y. Tian, D. Singh, and Y. Chang. Graphlime: Local interpretable model explanations for graph neural networks. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* [13] R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandits in metric spaces. In _Proceedings of the fortieth annual ACM symposium on Theory of computing_, pages 681-690, 2008.
* [14] I. E. Kumar, S. Venkatasubramanian, C. Scheidegger, and S. Friedler. Problems with shapley-value-based explanations as feature importance measures. In _International Conference on Machine Learning_, pages 5491-5500. PMLR, 2020.
* [15] W. Lin, H. Lan, and B. Li. Generative causal explanations for graph neural networks. In _International Conference on Machine Learning_, pages 6666-6679. PMLR, 2021.

* [16] W. Lin, H. Lan, H. Wang, and B. Li. Orphicx: A causality-inspired latent variable model for interpreting graph neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13729-13738, 2022.
* [17] A. Lucic, M. A. Ter Hoeve, G. Tolomei, M. De Rijke, and F. Silvestri. Cf-gnnexplainer: Counterfactual explanations for graph neural networks. In _International Conference on Artificial Intelligence and Statistics_, pages 4499-4511. PMLR, 2022.
* [18] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. _Advances in Neural Information Processing Systems_, 30, 2017.
* [19] D. Luo, W. Cheng, D. Xu, W. Yu, B. Zong, H. Chen, and X. Zhang. Parameterized explainer for graph neural network. _Advances in Neural Information Processing Systems_, 33:19620-19631, 2020.
* [20] R. Munos et al. From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning. _Foundations and Trends(r) in Machine Learning_, 7(1):1-129, 2014.
* [21] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Automatic differentiation in pytorch. 2017.
* [22] P. E. Pope, S. Kolouri, M. Rostami, C. E. Martin, and H. Hoffmann. Explainability methods for graph convolutional neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10772-10781, 2019.
* [23] B. Rozemberczki, L. Watson, P. Bayer, H.-T. Yang, O. Kiss, S. Nilsson, and R. Sarkar. The shapley value in machine learning. _arXiv preprint arXiv:2202.05594_, 2022.
* [24] M. S. Schlichtkrull, N. De Cao, and I. Titov. Interpreting graph neural networks for nlp with differentiable edge masking. In _International Conference on Learning Representations_, 2021.
* [25] T. Schnake, O. Eberle, J. Lederer, S. Nakajima, K. T. Schutt, K.-R. Muller, and G. Montavon. Higher-order explanations of graph neural networks via relevant walks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(11):7581-7596, 2021.
* [26] R. Schwarzenberg, M. Hubner, D. Harbecke, C. Alt, and L. Hennig. Layerwise relevance visualization in convolutional text graph classifiers. _EMNLP-IJCNLP 2019_, page 58, 2019.
* [27] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 618-626, 2017.
* [28] L. S. Shapley et al. A value for n-person games. 1953.
* [29] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Y. Ng, and C. Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing_, pages 1631-1642, 2013.
* [30] M. Vu and M. T. Thai. Pgm-explainer: Probabilistic graphical model explanations for graph neural networks. _Advances in Neural Information Processing Systems_, 33:12225-12235, 2020.
* [31] X. Wang, Y. Wu, A. Zhang, X. He, and T.-S. Chua. Towards multi-grained explainability for graph neural networks. _Advances in Neural Information Processing Systems_, 34:18446-18458, 2021.
* [32] Z. Wang, L. Cao, W. Lin, M. Jiang, and K. C. Tan. Robust graph meta-learning via manifold calibration with proxy subgraphs. In _AAAI Conference on Artificial Intelligence_, 2023.
* [33] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. Pappu, K. Leswing, and V. Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical Science_, 9(2):513-530, 2018.
* [34] Z. Ye, Y. Qu, Z. Liang, M. Wang, and Q. Liu. Explainable fmri-based brain decoding via spatial temporal-pyramid graph convolutional network. _Human Brain Mapping_, 44(7):2921-2935, 2023.
* [35] Z. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec. Gnnexplainer: Generating explanations for graph neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* [36] H. Yuan, J. Tang, X. Hu, and S. Ji. Xgnn: Towards model-level explanations of graph neural networks. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 430-438, 2020.

* [37] H. Yuan, H. Yu, S. Gui, and S. Ji. Explainability in graph neural networks: A taxonomic survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [38] H. Yuan, H. Yu, J. Wang, K. Li, and S. Ji. On explainability of graph neural networks via subgraph explorations. In _International Conference on Machine Learning_, pages 12241-12252. PMLR, 2021.
* [39] H. Zhang, B. Wu, X. Yuan, S. Pan, H. Tong, and J. Pei. Trustworthy graph neural networks: Aspects, methods and trends. _arXiv preprint arXiv:2205.07424_, 2022.
* [40] S. Zhang, Y. Liu, N. Shah, and Y. Sun. Gstarx: Explaining graph neural networks with structure-aware cooperative games. In _Advances in Neural Information Processing Systems_, 2022.
* [41] Y. Zhang, D. Defazio, and A. Ramesh. Relex: A model-agnostic relational model explainer. In _Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society_, pages 1042-1049, 2021.

Appendix to "SAME: Uncovering GNN Black Box with Structure-aware Shapley-based Multipiece Explanation"

###### Contents

* A Notations
* B Theoretical Analysis
* B.1 Definitions of Desire GNN Explanation Technique Properties
* B.2 Discussion about Four Fairness Properties
* B.3 Detailed Proof of Regret Bound
* B.4 Analysis of Time Complexity
* B.4.1 Important Substructure Initialization Phase
* B.4.2 Explanation exploration phase
* C Detailed Pseudo Code of SAME
* D Dataset Description
* E Metrics
* F Detailed Experimental Settings
* F.1 Computation Details
* F.2 Hyperparameter Settings
* G Additional Results
* G.1 Results under Other Metrics
* G.2 Additional Visualization Results
* G.3 Additional Quantitative Analysis

This appendix comprises both theoretical and experimental materials and is structured as follows. Section A presents a notation table that includes all notations used in this paper. Section B provides the full theoretical analysis. Section C presents detailed algorithms related to Section 3 in the manuscript. Section D-F show the detailed settings of the experiments in this work. Section G provides additional quantitative and qualitative results.

## Appendix A Notations

Table S1 contains the notations organized into three sections, each pertaining to different aspects of our paper. These include:

* **Graph**: These symbols are related to graph theory, representing elements such as graph nodes, edges, and other attributes of graphs.
* **MCTS**: These symbols represent elements related to the Monte Carlo Tree Search (MCTS) algorithm, such as MCTS nodes, actions, and rewards.
* **Importance score**: These symbols are used to denote values associated with measuring the importance of explanations in our explanation algorithm.

## Appendix B Theoretical Analysis

### Definitions of Desire GNN Explanation Technique Properties

The following content is a detailed definition of desired properties in Table 1 of the main manuscript.

* **Graph-level tasks**: the GNN explanation method can handle the graph classification/regression tasks.
* **Node-level tasks**: the GNN explanation method can handle the node classification/regression tasks.
* **Feature interactions**: given a graph \(G\) to be explained, when measuring the importance of the explanation result \(G_{ex}\), the GNN explanation method can consider the influence of \(\{G/G_{ex}\}\) on the importance of \(G_{ex}\).
* **Structure awareness**: given a graph \(G\) to be explained, when measuring the importance of the explanation result \(G_{ex}\), the GNN explanation method is sensitive to the topology of the given input graph \(G\).
* **Multipiece explanation**: the GNN explanation method can provide the explanation \(G_{ex}\) that can be composed of one or more connected components.
* **Node-wise importance**: given any node \(v_{i}\in G\), its importance \(I(f(\cdot),v_{i},G)\) can be considered by the GNN explanation method.
* **Substructure-wise importance**: given any substructure \(G^{i}_{sub}\subseteq G\), it can be directly calculated as a whole through the GNN explanation method to obtain its importance \(I(f(\cdot),G^{i}_{sub},G)\). Note that any substructure is a connected component which has more than one node.

* **Composite-wise importance**: given any composite \(G^{i}_{com}\subseteq G\) consisting of more than two nodes or substructures, it can be directly calculated as a whole through the GNN explanation method to obtain its importance \(I(f(\cdot),G^{i}_{com},G)\).
* **Priority-based integration**: given an explanation \(G^{j}_{ex}\subseteq G\) with any size, the node \(v_{i}\in\{G/G^{j}_{ex}\}\) will be added by the GNN explanation method on \(G^{j}_{ex}\) to get a new explanation, if and only if for any node \(v_{k}\in\{G/G^{j}_{ex}\}\) except \(v_{i}\), \(I(f(\cdot),\{v_{i}\cup G^{j}_{ex}\},G)\geq I(f(\cdot),\{v_{k}\cup G^{j}_{ex}\},G)\) always holds.
* **Redundancy consideration**: given an explanation \(G^{j}_{ex}\subseteq G\), if there exist a node \(v_{i}\in\{G/G^{j}_{ex}\}\) that satisfies \(I(f(\cdot),\{G^{j}_{ex}/v_{i}\},G)\geq I(f(\cdot),G^{j}_{ex},G)\), the GNN explanation technique would not choose \(G^{j}_{ex}\) as the final explanation.

As Figure S1 shows, the characteristics and properties within a graph or node tend to be _jointly_ influenced by more than one high-order connected community of the graph. The "multipiece explanation" property is aware of such an attribute by considering whether multiple disconnected pieces are allowed to appear together in the explanation output. For example, when "A" and "B" are two orthogonal connected components and the ground truth explanation for a prediction is "A and B", then the GNN explanation method has a probability of providing the correct "A and B" explanation.

We call a GNN explanation method that has the "structure-awareness" property if it can process the structural information of the input graph, whether through explicit mechanisms, as demonstrated by GstarX, or implicit modalities, as exemplified by SubgraphX and this study.

### Discussion about Four Fairness Properties

_Efficiency_ demonstrates that the aggregated importance of individual substructures is equivalent to the prediction of GNN on the entire substructure set. _Symmetry_ and _Dummy_ take into account the equal importance of substructures with the same interactions and the substructures that are completely unimportant, respectively. _Monotonicity_ ensures that the explanation method is consistent in the trend of results between different well-trained GNN models given the same dataset.

### Detailed Proof of Regret Bound

The proper definition of the regret phenomenon that the algorithm or the function \(f\) encounters must be considered to make the objective clear. Prior studies have recognized two principal definitions:

We recall the Lemma 1 in the main manuscript as follows.

**Lemma 1**.: _Let \(d\) be the \(\nu\)-near-optimality dimension (where \(\nu\) is defined in Assumption 3 of the main manuscript), and \(C\) is the corresponding constant. Then \(|I_{h}|\leq C\delta(h)^{-d}\)._

Proof.: From Assumption 3 in the main manuscript, each cell \((h,i)\) contains a ball of radius \(\nu\delta(h)\) centered at \(x_{h,i}\). Consequently, if \(|I_{h}|=|\{x_{h,i}\in X_{\delta(h)}\}|\) exceeds \(C\delta(h)^{-d}\), this would imply the existence of more than \(C\delta(h)^{-d}\) disjoint \(l\)-balls of radius \(\nu\delta(h)\), each centered within \(X_{\delta(h)}\). This assertion would contradict the definition of \(d\). 

We recall the Theorem 1 in the main manuscript as follows.

**Theorem 1**.: _Let us write \(h(n)\) the smallest integer \(h\) such that \(C\sum_{l=0}^{h}\delta(l)^{-d}\geq n\). Then the loss of algorithm is bounded as \(r_{n}\leq\delta(h(n))\)._

Proof.: Consider the tuple \((h_{\max},j_{\max})\) to be the node with the maximum depth that the algorithm expands up to the \(n\)th round. Given that the algorithm's expansion is limited to nodes within the set \(I\), we can provide that the total number of nodes expanded, denoted by \(n\), adheres to the following condition:

\[n =\sum_{l=0}^{h_{\max}}K^{l}\sum_{j=0}^{K^{l}-1}\nvdash\{(h,j)\text { has been expanded}\}\] \[\leq\sum_{l=0}^{h_{\max}}|I_{l}|\] \[\leq C\sum_{l=0}^{h_{\max}}\delta(l)^{-d},\]

from Lemma 1. Now from the definition of \(h(n)\) we have \(h_{\max}\geq h(n)\). Finally, since node \((h_{\max},j_{\max})\) has been expanded, we have that \((h_{\max},j_{\max})\in I\), thus \(f(x(n))\geq f(x_{h_{\max},j_{\max}})\geq f^{\star}-\delta(h_{\max})\geq f^{ \star}-\delta(h(n))\). 

**Corollary 1**.: _Assume that \(\delta(h)=c\gamma^{h}\) for some constants \(c>0\) and \(\gamma<1\), if \(d>0\), then the loss decreases polynomially fast:_

\[r_{n}\leq(\frac{C}{1-\gamma^{d}})^{1/d}n^{-1/d}\]

_If \(d=0\), then the loss decreases exponentially fast:_

\[r_{n}\leq c\gamma^{n/C-1}\]

Proof.: From Theorem 1,whenever \(d>0\) we have

\[n\leq C\sum_{l=0}^{h(n)}\delta(l)^{-d}=Cc^{-d}\frac{\gamma^{-d(h(n)+1)-1}}{ \gamma^{-d}-1} \tag{1}\]

thus \(\gamma^{-dh(n)}\geq\frac{n}{Cc^{-d}}(1-\gamma^{d})\), therefore we can also have:

\[r_{n}\leq\delta(h(n))\leq c\gamma^{h(n)}\leq(\frac{C}{1-\gamma^{d}})^{1/d}n^{-1 /d} \tag{2}\]

Now, if \(d=0\) then \(n\leq C\sum_{l=0}^{h(n)}\delta(l)^{-d}=C(h(n)+1)\), and we deduce that the loss is bounded as \(r_{n}\leq\delta(h(n))=c\gamma^{n/C-1}\). 

### Analysis of Time Complexity

Given a graph \(G=(V,E)\), we take a single iteration as an example to analyze the time complexity of the expansion-based MCTS in our proposed SAME. A single iteration includes four steps: selection, expansion, simulation and backpropagation [1].

#### b.4.1 Important Substructure Initialization Phase

In this phase, the goal of our SAME is to figure out a group of important substructures whose size is smaller than \(\gamma\). In the selection step, our method chooses an unvisited node from the graph or chooses the node with the highest reward if all nodes have been visited (\(\mathcal{O}(|V|)\)). Then, in the expansion step, our method selects the node within 1-hop neighbors whose value is largestaccording to \(G\), which requires \(\mathcal{O}(1)\), and adds it to the tree. In the simulation step, only the adjacency node will be appended each time. For the graph \(G\), in the worst case SAME will append all the nodes by visiting all the edges in the graph. Thus, the time complexity of simulation is \(\mathcal{O}(|V|+|E|)\). For the backpropagation step, since the maximum size of a substructure is \(\gamma\), the depth of the MCTS tree will not exceed \(\gamma\). The time complexity of reward backpropagation is bounded by \(\mathcal{O}(\gamma)\). Since we perform \(M_{1}\) iterations, according to the law of multiplication, the time complexity of the first phase is \(\mathcal{O}(M_{1})\times\mathcal{O}(selection)\times\mathcal{O}(expansion)\times \mathcal{O}(simulation)\times\mathcal{O}(backpropagation)\) = \(\mathcal{O}(M_{1}\gamma|V|^{2}+M_{1}\gamma|V|\times|E|)\).

#### b.4.2 Explanation exploration phase

In the selection step, the MCTS is initialized with an unvisited substructure from _Important Substructure Set_ (size = \(K\), where \(K\ll|V|\)) or chooses the substructure with the highest reward if all substructures have been visited (\(\mathcal{O}(K)\)). The expansion step is \(\mathcal{O}(1)\) as we only need to append the substructure with the largest value to the current state. In the following simulation step, our method will append other substructures to the current state until the size exceeds the sparsity limit, which is bounded by \(\mathcal{O}(2^{K})\) for finding all possible combination. Notice that \(\mathcal{O}(2^{K})\) can be very large, we set a maximum simulation time \(t_{s}\) as budget which requires \(\mathcal{O}(t_{s})\). Since the final explanation is highly related to the sparsity, the cost of backpropagation step is bounded by \(\mathcal{O}(\frac{|V|\times(1-sparsity)}{\gamma}))\), where \(\gamma\) is the maximum size of each substructure. With \(M_{2}\) iterations, the time complexity in this phase is \(\mathcal{O}(M_{2})\times\mathcal{O}(selection)\times\mathcal{O}(expansion)\times \mathcal{O}(simulation)\times\mathcal{O}(backpropagation)\) = \(\mathcal{O}(M_{2}Kt_{s}\frac{|V|\times(1-sparsity)}{\gamma})\).

Overall, time complexity of SAME is \(\mathcal{O}(M_{1}\gamma|V|^{2}+M_{1}\gamma|V|\times|E|+M_{2}Kt_{s}\frac{|V| \times(1-sparsity)}{\gamma})\). As the \(t_{s}\), \(\gamma\), \(K\), \(M_{1}\), \(M_{2}\) and \(sparsity\) is predefined, therefore SAME is a polynomial-time method under these constraints.

## Appendix C Detailed Pseudo Code of SAME

In this section, we are going to provide a detailed overview of the processes of SAME which includes _important substructure initialization_ and _explanation exploration_.

Algorithm 1 outlines the process of using Monte Carlo Tree Search (MCTS) to find a set of important substructures, as mentioned in Section 3.1 of the main manuscript. Algorithm 2 and 3 illustrate the method to discover an optimal explanation from these important substructures, as described in Section 3.2. Algorithm 4 provides a detailed description of how the importance of an explanation is assessed using the structure-aware Shapley value, which was proposed in Section 2.1 of your document. This algorithm outlines the steps necessary to approximate the Shapley value for a given explanation, effectively measuring the contribution of each individual component via sampling.

## Appendix D Dataset Description

We provide details of the datasets used in our experiments, including BBBP [11], MUTAG [2], BA-2Motifs [8], BA-Shapes [12], Graph-SST2 [7], and Graph-SST5 [7].

**Molecular graphs.** We use the BBBP [11] containing approximately 2000 molecular graphs, which are classified into two classes over the property of blood-brain barrier penetration (BBBP). Another dataset MUTAG [2] is a collection of molecules with \(-NO_{2}\) functional groups. The goal is to predict whether these molecules are mutation-induced.

**Barabasi-Albert graphs.** The BA-2Motifs [8] and the BA-Shapes [12] are used for graph classification and node classification respectively. For each instance in BA-2Motifs, it is a Barabasi-Albert graph attached by motifs with a structure either house-like or five-node-cycle-like. The instances are labelled according to the type of motifs they get. The graph in BA-Shapes is a Barabasi-Albert graph with house-structured network motifs. The nodes in the graph will be classified into four classes, with labels 0 for the nodes belonging to the original graph and labels 1-3 for the nodes on the middle, bottom, or top of the house-like structures respectively.

**Sentiment graphs.** The sentiment graphs are built from real-world text sequences, and labelled according to the semantic meanings. Specifically, the nodes in the graph represent the words with an ```
Input:\(f(\cdot)\), well-trained GNN model; \(G\), graph to be explained; \(M_{1}\), max iteration number of MCTS; \(\gamma_{1}\), threshold of max single substructure size; \(\pi\), child selection strategy Output:\(\mathcal{N}\), all nodes in MCTS
1 Initialize the root of the MCTS as \(N_{0}\)
2\(G^{0}_{sub}\leftarrow\emptyset\)
3\(\mathcal{N}\leftarrow\{N_{0}\}\)
4for\(i=1,2,\ldots,M_{1}\)do
5\(N_{cur}\gets N_{0}\)
6\(curPath\leftarrow[N_{0}]\)
7\(G^{cur}_{sub}\gets G^{0}_{sub}\)
8while\(|G^{cur}_{sub}|<N_{max}\)do
9forall\(v_{j}\) in the \(\{adj(G^{cur}_{sub})\}\)do
10 Expand \(N_{i}\) to get \(N_{j}\)
11\(G^{j}_{sub}\gets G^{i}_{sub}\cup\{v_{j}\}\) // Compute the Shapley-based reward according to Algorithm 4
12\(R_{j}\gets I(f(\cdot),G^{j}_{sub},G)\)
13\(\mathcal{N}\leftarrow\mathcal{N}\cup\{N_{j}\}\)
14
15 end for
16 Select the child \(N_{next}\) and its substructure \(G^{next}_{sub}\) according to the strategy \(\pi\)
17\(N_{cur}\gets N_{next}\)
18\(G^{cur}_{sub}\gets G^{next}_{sub}\)
19\(curPath\gets curPath+N_{next}\).
20\(\mathcal{N}\leftarrow\mathcal{N}\cup\{N_{cur}\}\)
21 end while
22\(N_{l}\gets N_{cur}\) // \(N_{l}\) is a leaf node
23forall\(node\)\(N_{path_{i}}\) in the \(curPath\)do
24\(T_{path_{i}}\gets T_{path_{i}}+1\)
25\(W_{path_{i}}\gets W_{path_{i}}+I(f(\cdot),G^{l}_{sub},G)\)
26 end while
27
28 end while
```

**Algorithm 1**Important Substructure Initialization

```
Input:\(f(\cdot)\), well-trained GNN model; \(G\), graph to be explained; \(\mathcal{N}\), set of all the nodes in MCTS; \(K\), number of the top most important structures; \(K_{t}\), threshold to use MCTS; \(M_{2}\), max iteration number of MCTS; \(\gamma_{2}\), max explanation size; \(\pi\), child selection strategy. Output:\(G_{ex}\), the best explanation.
1 Sort the \(\mathcal{N}\) in descending order of the corresponding reward \(R_{i}\).
2\(\mathcal{N}_{K}\leftarrow\) top \(K\) substructures \(\{N_{1},N_{2},\ldots,N_{K}\}\) in \(\mathcal{N}\)
3\(\mathcal{G}_{ex}\leftarrow\{(G_{sub_{i}},R_{i})\}_{i=1}^{M_{2}}\) // Find explanations through MCTS in Algorithm 3.
4\(\mathcal{G}^{\prime}_{ex}\leftarrow\) MCTS(\(f(\cdot)\), \(G\), \(M_{2}\), \(N_{max}\), \(\pi\), \(\mathcal{N}_{K}\))
5\(\mathcal{G}_{ex}\leftarrow\mathcal{G}_{ex}\cup\mathcal{G}^{\prime}_{ex}\)
6 Sort the \(\mathcal{G}_{ex}\) in descending order of \(R\). // \(G_{ex}\) is the explanation at the first element in the sorted \(\mathcal{G}_{ex}\)
7\(G_{ex}\leftarrow\mathcal{G}_{ex}[0]\)
```

**Algorithm 2**Explanation Exploration

``` Input:\(f(\cdot)\), well-trained GNN model; \(G\), graph to be explained; \(\mathcal{N}\), set of all the nodes in MCTS; \(K\), number of the top most important structures; \(K_{t}\), threshold to use MCTS; \(M_{2}\), max iteration number of MCTS; \(\gamma_{2}\), max explanation size; \(\pi\), child selection strategy. Output:\(G_{ex}\), the best explanation.
1 Sort the \(\mathcal{N}\) in descending order of the corresponding reward \(R_{i}\).
2\(\mathcal{N}_{K```
Input:\(f(\cdot)\), well-trained GNN model; \(G\), graph data; \(G^{i}_{ex}\), explanation of \(G\); \(T\), sampling times; \(k\), number of neighboring hop Output:\(\mathcal{I}_{i}\), importance of \(G^{i}_{ex}\) for GNN \(f(\cdot)\).
1 Obtain the \(k\)-hop neighbor nodes of substructure \(G^{i}_{ex}\), denoted as \(\{v_{n_{i}+1},v_{n_{i}+2},\ldots,v_{n_{i}+k_{i}}\}\).
2 Then, the players set \(P_{i,khop}=\{G^{i}_{ex},v_{n_{i}+1},v_{n_{i}+2},\ldots,v_{n_{i}+k_{i}}\}\)
3for\(i=1,2,\ldots,T\)do
4 Randomly sample a set \(S_{i}\subseteq\{P\backslash G^{i}_{ex}\}\)
5 Obtain \(f(S_{i}\cup\{G^{i}_{ex}\})\) and \(f(S_{i})\) by setting the features of the nodes not in the input structure with zero. // \(m(S_{i},G^{i}_{ex})\) denotes the marginalized contribution
6\(m(S_{i},G^{i}_{ex})\gets f(S_{i}\cup\{G^{i}_{ex}\})-f(S_{i})\).
7
8 end for
9\(I(f(\cdot),G^{i}_{ex},G)\leftarrow\frac{1}{T}\sum_{t=1}^{T}m(S_{i},G^{i}_{ex})\)
10\(\mathcal{I}_{i}\gets I(f(\cdot),G^{i}_{ex},G)\)
```

**Algorithm 3**MCTS Explanation Exploration

initial embedding from the pre-trained BERT [3] and the edges denote the relationships between the words which are extracted by the Biaffine parser [5]. We take experiments on the Graph-SST2 and Graph-SST5 datasets for the sentiment graphs. Note that both Graph-SST2 and Graph-SST5 are the datasets for graph classification of two classes and five classes respectively.

## Appendix E Metrics

The Fidelity, Inverse Fidelity, and Sparsity are formally defined as:

\[\text{Fidelity}(G,G_{ex}) =[f(G)]_{e^{*}}-[f(G/G_{ex})]_{e^{*}} \tag{3}\] \[\text{Fidelity}_{\text{Inv}}(G,G_{ex}) =[f(G)]_{e^{*}}-[f(G_{ex})]\] (4) \[\text{Sparsity}(G,G_{ex}) =1-\frac{|G_{ex}|}{|G|} \tag{5}\]

Harmonic Fidelity (H-Fidelity) takes a harmonic mean of normalized fidelity (N-Fidelity) and normalized inverse fidelity (N-FidelityInv). According to [15], the H-Fidelity can be formally defined as:

\[\text{N-Fidelity}(G,G_{ex}):m_{1} =\text{Fidelity}(G,G_{ex})\cdot\text{Sparsity}(G,G_{ex}) \tag{6}\] \[\text{N-Fidelity}_{\text{Inv}}(G,G_{ex}):m_{2} =\text{Fidelity}_{\text{Inv}}(G,G_{ex})\cdot(1-\text{Sparsity}( G,G_{ex}))\] (7) \[\text{H-Fidelity}(G,G_{ex}) =\frac{(1+m_{1})(1-m_{2})}{(2+m_{1}-m_{2})} \tag{8}\]

## Appendix F Detailed Experimental Settings

In this section, we provide the computational details and detailed hyperparameter settings on different datasets.

### Computation Details

All experiments were performed on a single Nvidia V100 GPU with an Intel Xeon Gold 5218 CPU. Since it takes more than 24 hours for some baselines to explain the whole dataset, our inference time analysis is implemented by randomly sampling from the datasets. For BBBP, BA-2Motifs and BA-Shapes, we use all the test data. And all the data (train, evaluation and test data) is used for MUTAG dataset. For semantic graphs like Graph-SST2 and Graph-SST5, we randomly choose 30 graphs.

### Hyperparameter Settings

We adhere to the hyperparameter settings for GNN training as described in [13, 15], detailed in Table S2. For the qualitative analysis, the GCN in Figures 2, 4, S2, and S3 aligns with the settings depicted in Table S2. Moreover, the GIN featured in Figure 3 is trained for 800 epochs with a batch size of 64 and without adding self-loops, while all other hyperparameters remain constant to Table S2.

The hyperparameters in SAME include \(\gamma\) and \(K\) as we didn't use a time budget in our MCTS. In order to see the effect of these two hyperparameters, we run our SAME by randomly selecting 30 graphs from Graph-SST5 datasets using GCN, the fidelity w.r.t. \(\gamma\) and \(K\) are shown in the Table S3, and we report the average inference time in Table S4. Notice that since we only selected 30 graphs from Graph-SST5 to obtain the results, the results provided in Table S3 might be slightly different from the results given in the main manuscript.

It is noteworthy that \(\gamma\)=1 could lead to optimal fidelity in most benchmarks. However, when \(\gamma\) is small, the final explanation may be composed of disconnected nodes at different positions in the graph, which usually causes the visualization of explanation results not human-understandable. Therefore, we fine-tuned \(\gamma\) and \(K\) according to the visualization of explanation results so that the provided explanations are more human-intuitive. For the inference time, a larger \(K\) leads to a larger search space for the _explanation exploration phase_. When \(\gamma\) is small, the size of a single candidate substructure is small, indicating that we need to select more substructures from the candidates to obtain an explanation with the desired sparsity. Thus, the inference time increases as the depth of searching grows. The detailed hyperparameter settings are provided in Table S5. For the sentiment network Twitter [13] and molecular network BACE [11], we use them to compare SAME with more SOTA explainers in Section G.3

## Appendix G Additional Results

### Results under Other Metrics

Here, we present the results over inverse fidelity and harmonic fidelity under the same experimental settings in Table S2. Table S6 demonstrates that SAME is competitive over the 5/6 datasets. Table S7 reports the harmonic fidelity quantities. SAME outperforms the baselines in all the graph tasks.

### Additional Visualization Results

We are presenting additional visualization results for BBBP and Graph-SST2 datasets. Figure S2 presents explanations for the BBBP dataset. SAME identifies critical functional groups (for instance, carbonyl group =C=O), which result in a high fidelity score. Figure S3 provides a visualization for the negative label of the Graph-SST2 dataset. In this case, SAME uniquely identifies the word "only," which contributes to the negative prediction of the GNN due to the contrasting relationship it establishes.

Table S6: Comparison of our SAME and other baseline explainers using 1 - Inverse Fidelity.

\begin{tabular}{c|c c c c c c} \hline \multirow{2}{*}{Dataset} & \multicolumn{5}{c|}{Graph tasks} & \multicolumn{2}{c}{Node tasks} \\ \cline{2-7}  & \multicolumn{2}{c|}{Molecular graph} & \multicolumn{2}{c|}{Semantic graph} & \multicolumn{2}{c}{Synthetic graph} \\ \cline{2-7}  & BBBP & MUTAG & Graph-SST2 & Graph-SST5 & BA-2Motifs & BA-Shapes \\ \hline Grad-CAM [9] & 0.897\(\pm\)0.029 & 0.901\(\pm\)0.048 & 1.006\(\pm\)0.029 & 0.937\(\pm\)0.047 & 0.587\(\pm\)0.092 & - \\ GNNExplainer [12] & 0.829\(\pm\)0.012 & 0.810\(\pm\)0.007 & 0.855\(\pm\)0.037 & 0.860\(\pm\)0.034 & 0.562\(\pm\)0.015 & 0.953\(\pm\)0.032 \\ PGExplainer [8] & 0.838\(\pm\)0.036 & 0.793\(\pm\)0.030 & 0.956\(\pm\)0.036 & 0.850\(\pm\)0.058 & 0.582\(\pm\)0.021 & 0.902\(\pm\)0.008 \\ GNN-LRP [10] & 0.670\(\pm\)0.044 & 0.768\(\pm\)0.013 & 0.784\(\pm\)0.047 & 0.771\(\pm\)0.041 & 0.560\(\pm\)0.015 & **0.988\(\pm\)0.010** \\ SubgraphX [14] & **1.010\(\pm\)0.011** & 1.053\(\pm\)0.017 & 1.008\(\pm\)0.008 & 1.022\(\pm\)0.040 & **0.906\(\pm\)0.162** & 0.978\(\pm\)0.034 \\ GStarX [15] & 0.939\(\pm\)0.011 & **1.124\(\pm\)0.025** & **1.072\(\pm\)0.003** & **1.140\(\pm\)0.031** & 0.587\(\pm\)0.117 & - \\ \hline
**SAME** & 0.993\(\pm\)0.014 & 1.095\(\pm\)0.008 & 1.058\(\pm\)0.010 & 1.039\(\pm\)0.001 & 0.857\(\pm\)0.046 & 0.881\(\pm\)0.000 \\ \hline \end{tabular}

Note: The previous SOTA results on different datasets are marked with an underline. _Relative Improve_ denotes the relative improvement of our SAME method over the SOTA methods.

\begin{tabular}{c|c c c c c c} \hline \multirow{2}{*}{Dataset} & \multicolumn{5}{c|}{Graph tasks} & \multicolumn{2}{c}{Node tasks} \\ \cline{2-7}  & \multicolumn{2}{c|}{Molecular graph} & \multicolumn{2}{c|}{Semantic graph} & \multicolumn{2}{c}{Synthetic graph} \\ \cline{2-7}  & BBBP & MUTAG & Graph-SST2 & Graph-SST5 & BA-2Motifs & BA-Shapes \\ \hline Grad-CAM [9] & 0.520\(\pm\)0.002 & 0.527\(\pm\)0.002 & 0.538\(\pm\)0.005 & 0.529\(\pm\)0.003 & 0.502\(\pm\)0.014 & - \\ GNNExplainer [12] & 0.504\(\pm\)0.002 & 0.505\(\pm\)0.007 & 0.507\(\pm\)0.001 & 0.512\(\pm\)0.001 & 0.487\(\pm\)0.027 & 0.514\(\pm\)0.001 \\ PGExplainer [8] & 0.511\(\pm\)0.002 & 0.500\(\pm\)0.006 & 0.528\(\pm\)0.003 & 0.516\(\pm\)0.000 & 0.493\(\pm\)0.023 & 0.504\(\pm\)0.003 \\ GNN-LRP [10] & 0.488\(\pm\)0.003 & 0.501\(\pm\)0.013 & 0.495\(\pm\)0.

[MISSING_PAGE_EMPTY:24]

## References

* [1] C. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez, S. Samothrakis, and S. Colton. A survey of monte carlo tree search methods. _IEEE Transactions on Computational Intelligence and AI in Games_, 4(1):1-43, 2012.
* [2] A. K. Debnath, R. L. Lopez de Compadre, G. Debnath, A. J. Shusterman, and C. Hansch. Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity. _Journal of Medicinal Chemistry_, 34(2):786-797, 1991.
* [3] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _ArXiv_, abs/1810.04805, 2019.
* [4] A. Duval and F. D. Malliaros. Graphsvx: Shapley value explanations for graph neural networks. In _Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13-17, 2021, Proceedings, Part II 21_, pages 302-318. Springer, 2021.
* [5] M. Gardner, J. Grus, M. Neumann, O. Tafjord, P. Dasigi, N. Liu, M. Peters, M. Schmitz, and L. Zettlemoyer. Allennlp: A deep semantic natural language processing platform. 2018.
* [6] W. Lin, H. Lan, H. Wang, and B. Li. Orphicx: A causality-inspired latent variable model for interpreting graph neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13729-13738, 2022.
* [7] M. Liu, Y. Luo, L. Wang, Y. Xie, H. Yuan, S. Gui, H. Yu, Z. Xu, J. Zhang, Y. Liu, et al. Dig: A turnkey library for diving into graph deep learning research. _The Journal of Machine Learning Research_, 22(1):10873-10881, 2021.
* [8] D. Luo, W. Cheng, D. Xu, W. Yu, B. Zong, H. Chen, and X. Zhang. Parameterized explainer for graph neural network. _Advances in Neural Information Processing Systems_, 33:19620-19631, 2020.
* [9] P. E. Pope, S. Kolouri, M. Rostami, C. E. Martin, and H. Hoffmann. Explainability methods for graph convolutional neural networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10772-10781, 2019.
* [10] T. Schnake, O. Eberle, J. Lederer, S. Nakajima, K. T. Schutt, K.-R. Muller, and G. Montavon. Higher-order explanations of graph neural networks via relevant walks. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(11):7581-7596, 2021.
* [11] Z. Wu, B. Ramsundar, E. N. Feinberg, J. Gomes, C. Geniesse, A. S. Pappu, K. Leswing, and V. Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical Science_, 9(2):513-530, 2018.
* [12] Z. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec. Gnnexplainer: Generating explanations for graph neural networks. _Advances in Neural Information Processing Systems_, 32, 2019.
* [13] H. Yuan, H. Yu, S. Gui, and S. Ji. Explainability in graph neural networks: A taxonomic survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [14] H. Yuan, H. Yu, J. Wang, K. Li, and S. Ji. On explainability of graph neural networks via subgraph explorations. In _International Conference on Machine Learning_, pages 12241-12252. PMLR, 2021.
* [15] S. Zhang, Y. Liu, N. Shah, and Y. Sun. Gstarx: Explaining graph neural networks with structure-aware cooperative games. In _Advances in Neural Information Processing Systems_, 2022.