# Identifiability Analysis of Linear ODE Systems with Hidden Confounders

 Yuanyuan Wang

The University of Melbourne

yuanyuanw2@student.unimelb.edu.au

&Biwei Huang

University of California, San Diego

bih007@ucsd.edu

&Wei Huang

The University of Melbourne

wei.huang@unimelb.edu.au

&Xi Geng

The University of Melbourne

xi.geng@unimelb.edu.au

&Mingming Gong

The University of Melbourne

mingming.gong@unimelb.edu.au

Corresponding author.

###### Abstract

The identifiability analysis of linear Ordinary Differential Equation (ODE) systems is a necessary prerequisite for making reliable causal inferences about these systems. While identifiability has been well studied in scenarios where the system is fully observable, the conditions for identifiability remain unexplored when latent variables interact with the system. This paper aims to address this gap by presenting a systematic analysis of identifiability in linear ODE systems incorporating hidden confounders. Specifically, we investigate two cases of such systems. In the first case, latent confounders exhibit no causal relationships, yet their evolution adheres to specific functional forms, such as polynomial functions of time \(t\). Subsequently, we extend this analysis to encompass scenarios where hidden confounders exhibit causal dependencies, with the causal structure of latent variables described by a Directed Acyclic Graph (DAG). The second case represents a more intricate variation of the first case, prompting a more comprehensive identifiability analysis. Accordingly, we conduct detailed identifiability analyses of the second system under various observation conditions, including both continuous and discrete observations from single or multiple trajectories. To validate our theoretical results, we perform a series of simulations, which support and substantiate our findings.

## 1 Introduction

Understanding the dynamics of systems governed by Ordinary Differential Equations (ODEs) is fundamental in various scientific disciplines, from physics [9; 23; 24; 47], biology [16; 27; 29; 33; 36] to economics [13; 38; 39; 43]. These ODE systems provide a natural framework for modeling causal relationships among system variables, enabling us to make reliable interpretations and interventions [25; 30; 31]. Central to unraveling the causal mechanisms of such systems is the concept of identifiability analysis, which aims to uncover conditions under which system parameters can be uniquely determined from error-free observations. Identifiability is crucial for ensuring reliable parameter estimates, thereby guaranteeing reliable causal inferences about the system [41]. The motivation for our research on the identifiability analysis of ODE systems arises from the necessity of making reliable causal inferences about these systems.

Our research focuses on the homogeneous linear ODE system, represented as:

\[\dot{\bm{x}}(t)=A\bm{x}(t)\,,\quad\bm{x}(0)=\bm{x}_{0}\,,\] (1)

where \(t\in[0,\infty)\) denotes time, \(\bm{x}(t)\in\mathbb{R}^{d}\) represents the system's state at time \(t\), \(\dot{\bm{x}}(t)\) denotes the first derivative of \(\bm{x}(t)\) w.r.t. time, and \(\bm{x}_{0}\) represents the initial condition of the system. The solution (trajectory) of the system, denoted as \(\bm{x}(t;\bm{x}_{0},A)\) for \(t\in[0,\infty)\), is a single \(d\)-dimensional trajectory initialized with \(\bm{x}_{0}\).

Existing literature has extensively examined the identifiability of linear ODE systems under the assumption of complete observability, where all state variables are directly observable [5; 14; 15; 17; 28; 34; 42]. Specifically, researchers have investigated identifiability of the ODE system (1) from a single whole trajectory [28; 34], and extended analysis to discrete observations sampled from the trajectory [42]. However, practical scenarios often entail systems with latent variables, rendering them not entirely observable. In this paper, we explore the identifiability analysis of this ODE system under latent confounders, particularly examining cases where no causal relationships exist from observable variables to latent variables, a commonly assumed condition in causality analysis with hidden variables [10; 11; 20; 22; 44; 45].

In this paper, we focus on two scenarios:

1. **Independent latent confounders:** Latent variables exhibit no causal relationships among themselves, leading to the following linear ODE system: \[\begin{bmatrix}\dot{\bm{x}}(t)\\ \dot{\bm{z}}(t)\end{bmatrix}=\begin{bmatrix}A&B\\ \bm{0}&\bm{0}\end{bmatrix}\begin{bmatrix}\bm{x}(t)\\ \bm{z}(t)\end{bmatrix}+\begin{bmatrix}\bm{0}\\ \bm{f}(t)\end{bmatrix}\,,\quad\begin{bmatrix}\bm{x}(0)\\ \bm{z}(0)\end{bmatrix}=\begin{bmatrix}\bm{x}_{0}\\ \bm{z}_{0}\end{bmatrix}\,.\] (2)
2. **Causally related latent confounders:** Latent variables exhibit causal relationships among themselves, specifically, they follow a DAG structure, represented as: \[\begin{bmatrix}\dot{\bm{x}}(t)\\ \dot{\bm{z}}(t)\end{bmatrix}=\begin{bmatrix}A&B\\ \bm{0}&G\end{bmatrix}\begin{bmatrix}\bm{x}(t)\\ \bm{z}(t)\end{bmatrix}\,,\quad\begin{bmatrix}\bm{x}(0)\\ \bm{z}(0)\end{bmatrix}=\begin{bmatrix}\bm{x}_{0}\\ \bm{z}_{0}\end{bmatrix}\,.\] (3)

In these two ODE systems, \(\bm{x}(t)\in\mathbb{R}^{d}\) denotes the state of observable variables \(\bm{x}=(x_{1},x_{2},\ldots,x_{d})\), while \(\bm{z}(t)\in\mathbb{R}^{p}\) denotes the state of latent variables \(\bm{z}=(z_{1},z_{2},\ldots,z_{p})\). Example causal structures of these two ODE systems are illustrated in Figure 1. It is noteworthy that the structure may include cycles and self-loops within the observable variables. Additionally, two real-world examples are provided in Appendix B.

This paper provides an identifiability analysis for the ODE system (2) under specific latent variable evolutions, such as polynomial functions of time \(t\). Additionally, we conduct a systematic identifiability analysis of the ODE system (3) when the causal structure of the latent variables can be described by a DAG.

## 2 Background

### Causal interpretation of the ODE system

When an ODE system describes the underlying causal mechanisms governing a dynamic system, it provides a natural framework for modeling causal relationships among system variables. The causal

Figure 1: Example causal structures of the ODE system (2) and (3).

structure inherent in such systems can be directly read off [25, 31]. For instance, in the ODE system (1), where the \(ij\)-th entry of the parameter matrix \(A\) is denoted as \(A_{ij}\), the presence of \(A_{ij}\neq 0\) signifies that the derivative of \(x_{i}(t)\) is influenced by \(x_{j}(t)\), thus indicating a causal link from \(x_{j}\) to \(x_{i}\). Here, \(x_{i}\) denotes the \(i\)-th variable of the ODE system (1), and \(x_{i}(t)\) represents its state at time \(t\). Since the right hand side of the ODE system (1) does not explicitly depend on time \(t\), the causal structure of this ODE system is time-invariant.

An essential prerequisite for reliably inferring the causal structure and effects of an ODE system, for purposes of interpretation or intervention, is the identifiability analysis of such systems. To underscore this necessity, we provide an illustrative example. Consider the ODE system (3). Set

\[\bm{x}_{0}=\begin{bmatrix}1\\ 1\end{bmatrix}\,,\ \ \bm{z}_{0}=\begin{bmatrix}1\\ 1\end{bmatrix}\,,\ \ B=\begin{bmatrix}1&1\\ 1&1\end{bmatrix}\,,\ \ G=\begin{bmatrix}0&1\\ 0&0\end{bmatrix}\,,\]

\[A=\begin{bmatrix}1&0\\ 0&1\end{bmatrix}\,,\ \ A^{\prime}=\begin{bmatrix}0&1\\ 1&0\end{bmatrix}\,,\ \ M=\begin{bmatrix}A&B\\ \bm{0}&G\end{bmatrix}\,,\ \ M^{\prime}=\begin{bmatrix}A^{\prime}&B\\ \bm{0}&G\end{bmatrix}\,.\]

Calculations reveal that the solutions (trajectory) of the ODE system (3) with parameter matrices \(M\) or \(M^{\prime}\) are identical, i.e.,

\[\begin{bmatrix}\bm{x}(t)\\ \bm{z}(t)\end{bmatrix}=e^{Mt}\begin{bmatrix}\bm{x}_{0}\\ \bm{z}_{0}\end{bmatrix}=e^{M^{\prime}t}\begin{bmatrix}\bm{x}_{0}\\ \bm{z}_{0}\end{bmatrix}\,.\]

This indicates that using observations sampled from this trajectory to estimate parameter matrix \(M\) may end up yielding \(M^{\prime}\), which exhibits a fundamentally distinct causal relationship between \(x_{1}\) and \(x_{2}\), see Figure 2.

This discrepancy in parameter estimation, wherein \(M^{\prime}\) is obtained instead of the true underlying parameter matrix \(M\), may lead to misleading interpretations and causal inferences, potentially influencing decision-making, particularly regarding interventions. For instance, intervention with \(x_{1}(t)=1\), under the true underlying parameter matrix \(M\), yields the trajectory \(x_{2}(t)=4e^{t}-t-3\) (post-intervention), whereas under matrix \(M^{\prime}\), the trajectory becomes \(x_{2}(t)=t^{2}/2+3t+1\) (post-intervention). Detailed calculations are provided in Appendix C.

### Identifiability analysis of the linear ODE system (1)

The identifiability analysis of the ODE system (1) has been well studied. Here, we present a fundamental definition and theorem essential for understanding identifiability in the ODE system (1). Denoting its solution as \(\bm{x}(t;\bm{x}_{0},A)\), it is noteworthy that the system is fully observable, without latent variables interacting with it. We present the identifiability definition and theorem as follows.

**Definition 2.1**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},A\in\mathbb{R}^{d\times d}\), the ODE system (1) is said to be \((\bm{x}_{0},A)\)-identifiable, if for all \(\bm{x}^{\prime}_{0}\in\mathbb{R}^{d}\) and all \(A^{\prime}\in\mathbb{R}^{d\times d}\), with \((\bm{x}_{0},A)\neq(\bm{x}^{\prime}_{0},A^{\prime})\), it holds that \(\bm{x}(\cdot;\bm{x}_{0},A)\neq\)\(\bm{x}(\cdot;\bm{x}^{\prime}_{0},A^{\prime})\).2_

Footnote 2: \(\bm{x}(\cdot;\bm{x}_{0},A)=\{\bm{x}(t;\bm{x}_{0},A):0\leqslant t<\infty\}\), this inequation means that there exists at least one \(t\geqslant 0\) such that \(\bm{x}(t;\bm{x}_{0},A)\neq\)\(\bm{x}(t;\bm{x}^{\prime}_{0},A^{\prime})\).

**Lemma 2.1**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},A\in\mathbb{R}^{d\times d}\), the ODE system (1) is \((\bm{x}_{0},A)\)-identifiable if and only if condition **A0** is satisfied._

**A0**: _the set of vectors \(\{\bm{x}_{0},A\bm{x}_{0},\ldots,A^{d-1}\bm{x}_{0}\}\) is linearly independent._

Figure 2: Causal structures of the ODE system (3) with parameter matrix \(M\) and \(M^{\prime}\).

Definition 2.1 and Theorem 2.1 are adapted from [42, Definition 1] and [42, Lemma2]. We use \(\bm{x}_{0}^{\prime}\) and \(A^{\prime}\) to distinguish other system parameters from the true system parameters \(\bm{x}_{0}\) and \(A\); \(\bm{x}_{0}^{\prime}\) and \(A^{\prime}\) can represent any \(d\)-dimensional initial conditions and any \(d\times d\) parameter matrices, respectively. Here instead of describing a collective property of a set of systems, we describe an intrinsic property of a single system with parameters (\(\bm{x}_{0},A\)). In practice, the aim is to ascertain whether the true underlying system parameter (\(\bm{x}_{0},A\)) is uniquely determined by observations. Hence, (\(\bm{x}_{0},A\))-identifiability offers a more intuitive description of the identifiability of the ODE system from a practical perspective.

From a geometric perspective, condition **A0** stated in Lemma 2.1 indicates that the initial condition \(\bm{x}_{0}\) is not contained in an \(A\)-invariant **proper** subspace of \(\mathbb{R}^{d}\). Intuitively, this means the trajectory of this system started from \(\bm{x}_{0}\) spans the entire \(d\)-dimensional state space. That is, our observations cover information on all dimensions of the state space, thus rendering the identifiability of the system. Additionally, condition **A0** is generic, as noted in [41], meaning that the set of system parameters violating this condition has Lebesgue measure zero. Thus, condition **A0** is satisfied for almost all combinations of \(\bm{x}_{0}\) and \(A\).

## 3 Identifiability analysis of the linear ODE system (2)

In this section, we present the identifiability condition for the linear ODE system (2). We consider the function \(\bm{f}(t)\) in (2) as a specific function of time \(t\). Here we first define \(\bm{f}(t)\) as a \(r\)-degree polynomial function of time \(t\), expressed as follows:

\[\bm{f}(t)=\sum_{k=0}^{r}\bm{v}_{k}t^{k}\,,\ \ \bm{v}_{k}\in\mathbb{R}^{p}\,.\] (4)

Simple calculations show that

\[\bm{z}(t)=\sum_{k=0}^{r}\frac{\bm{v}_{k}}{k+1}t^{k+1}+\bm{z}_{0}\,.\]

Thus,

\[\dot{\bm{x}}(t)=A\bm{x}(t)+B\bm{z}(t)=A\bm{x}(t)+\sum_{k=0}^{r}\frac{B\bm{v}_ {k}}{k+1}t^{k+1}+B\bm{z}_{0}\,.\] (5)

We denote the unknown parameters of the ODE system (2) as \(\bm{\theta}\), specifically, \(\bm{\theta}:=(\bm{x}_{0},\bm{z}_{0},A,B,\{\bm{v}_{k}\}_{0}^{r})\), where \(\{\bm{v}_{k}\}_{0}^{r}\) denotes all the \(\bm{v}_{k}\)'s for \(k=0,\ldots,r\). Let \([\bm{x}^{T}(t;\bm{\theta}),\bm{z}^{T}(t;\bm{\theta})]^{T}\) denote the solution of the ODE system (2). It is important to note that under our hidden variables setting, only \(\bm{x}(t;\bm{\theta})\) is observable. Based on Equation (5), we present the following identifiability definition.

**Definition 3.1**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d \times d},B\in\mathbb{R}^{d\times p}\) and \(\{\bm{v}_{k}\}_{0}^{r}\in\mathbb{R}^{p}\), for all \(\bm{x}_{0}^{\prime}\in\mathbb{R}^{d}\), all \(\bm{z}_{0}^{\prime}\in\mathbb{R}^{p}\), all \(A^{\prime}\in\mathbb{R}^{d\times d}\), all \(B^{\prime}\in\mathbb{R}^{d\times p}\), and all \(\{\bm{v}_{k}^{\prime}\}_{0}^{r}\in\mathbb{R}^{p}\), we denote \(\bm{\theta}^{\prime}:=(\bm{x}_{0}^{\prime},\bm{z}_{0}^{\prime},A^{\prime},B^{ \prime},\{\bm{v}_{k}^{\prime}\}_{1}^{r})\), we say the ODE system (2) is \(\bm{\theta}\)-identifiable: if \((\bm{x}_{0},A,B\bm{z}_{0},\{B\bm{v}_{k}\}_{0}^{r})\neq(\bm{x}_{0}^{\prime},A^{ \prime},B^{\prime}\bm{z}_{0}^{\prime},\{B^{\prime}\bm{v}_{k}^{\prime}\}_{0}^{r})\), it holds that \(\bm{x}(\cdot;\bm{\theta})\neq\bm{x}(\cdot;\bm{\theta}^{\prime})\)._

In the ODE system (2), where only variables \(\bm{x}\) are observable, we will, with some terminological leniency, refer to \(\bm{x}(\cdot;\bm{\theta})\) as the trajectory of the ODE system (2) with parameters \(\bm{\theta}\). According to Definition 3.1, if the ODE system (2) with a polynomial \(\bm{f}(t)\) is \(\bm{\theta}\)-identifiable, then the trajectory of the system can uniquely determine the values of \((\bm{x}_{0},A,B\bm{z}_{0},\{B\bm{v}_{k}\}_{0}^{r})\). This determination is sufficient to identify the causal relationships between observable variables \(\bm{x}\) as described by Equation (5). Consequently, one can safely intervene in the observable variables of the ODE system and make reliable causal inferences, despite the fact that matrix \(B\) cannot be identified under this definition.

**Theorem 3.1**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d\times d },B\in\mathbb{R}^{d\times p}\), \(\{\bm{v}_{k}\}_{0}^{r}\in\mathbb{R}^{p}\), the ODE system (2) is \(\bm{\theta}\)-identifiable if and only if assumption **A1** is satisfied._

* _the set of vectors_ \(\{\bm{\beta},\bm{A}\bm{\beta},\ldots,A^{d-1}\bm{\beta}\}\) _is linearly independent, where_ \(\bm{\beta}=A^{r+1}(A\bm{x}_{0}+B\bm{z}_{0})+\sum_{j=0}^{r}j!A^{r-j}B\bm{v}_{j}\)_, and_ \(j!\) _denotes the factorial of_ \(j\)_._

The proof of Theorem 3.1 can be found in Appendix D.1. Condition **A1** is both sufficient and necessary, indicating, from a geometric perspective, that the vector \(\bm{\beta}\) is not contained in an \(A\)-invariant proper subspace of \(\mathbb{R}^{d}\)[34, Lemma 3.1].

The key point of the proof is the introduction of an augmented state \(\bm{y}(t)=[\bm{x}^{T}(t),1,t,t^{2},\ldots,t^{r+1}]^{T}\) with a corresponding ODE system:

\[\dot{\bm{y}}(t)=\underbrace{\begin{bmatrix}A&B\bm{z}_{0}&B\bm{v}_{0}&\ldots&B \bm{v}_{r-1}/r&B\bm{v}_{r}/(r+1)\\ \bm{0}_{d}&0&0&\ldots&0&0\\ \bm{0}_{d}&1&0&\ldots&0&0\\ \vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\ \bm{0}_{d}&0&0&\ldots&r+1&0\end{bmatrix}}_{\text{denoted as $F$}}\bm{y}(t)\,,\] (6)

\[\bm{y}(0)=[\bm{x}_{0}^{T},1,0,\ldots,0]^{T}:=\bm{y}_{0}\,,\]

where \(\bm{0}_{d}\) is a \(d\)-dimensional zero row vector, and matrix \(F\in\mathbb{R}^{(d+r+2)\times(d+r+2)}\). The ODE system (6) is a homogeneous linear ODE system analogous to (1) but with fully observable variables \(\bm{y}\). In other words, we transform our system of interest, (2), which includes hidden confounders, into a fully observable ODE system (6). This allows us to leverage existing identifiability results for homogeneous linear ODE systems, specifically Lemma 2.1, to derive the identifiability condition for the ODE system (2).

Based on this approach, if the state of the hidden variables \(\bm{z}(t)\), as determined by the function \(\bm{f}(t)\) in the ODE system (2), can be described by some linear combinations of observable functions of time \(t\), then the identifiability condition of the ODE system (2) can be derived. For an illustration, in the Appendix E, we provide identifiability conditions for the ODE system (2) when \(\bm{f}(t)=\bm{v}e^{t}\) and \(\bm{f}(t)=\bm{v}_{1}sin(t)+\bm{v}_{2}cos(t)\). While we do not enumerate all functions \(\bm{f}(t)\) that meet this condition, our primary objective is to demonstrate a method for deriving the identifiability condition for the ODE (2) when the evolution of its hidden variables conforms to certain specific functions. Researchers can apply this approach to find appropriate functions \(\bm{f}(t)\) according to their specific requirements.

## 4 Identifiability analysis of the linear ODE system (3)

In this section, we extend the identifiability analysis to linear ODE systems with causally related latent confounders. Specifically, we assume that the causal structure of latent variables satisfies the following latent DAG assumption.

_Latent DAG: the causal structure of latent variables can be described by a DAG._

The DAG assumption is commonly employed in causality studies [10; 11; 22; 26; 40; 44]. Under the latent DAG assumption, the matrix \(G\) can be permuted to be a strictly upper triangular matrix, i.e., an upper triangular matrix with zeros along the main diagonal [11; 19]. Without loss of generality, we set \(G\) as a strictly upper triangular matrix.

Since \(G\) is a strictly upper triangular matrix, by the Cayley-Hamilton theorem [35], \(G\) is a nilpotent matrix with an index \(\leqslant p\). Consequently, \(G^{k}=0\) for all \(k\geqslant p\).

Based on [34; 37], the solution of \(\bm{z}(t)\) can be expressed as:

\[\bm{z}(t)=e^{Gt}\bm{z}_{0}=\sum_{k=0}^{\infty}\frac{G^{k}\bm{z}_{0}}{k!}t^{k} =\sum_{k=0}^{p-1}\frac{G^{k}\bm{z}_{0}}{k!}t^{k}\,.\]

Thus,

\[\dot{\bm{x}}(t)=A\bm{x}(t)+B\bm{z}(t)=A\bm{x}(t)+\sum_{k=0}^{p-1}\frac{BG^{k} \bm{z}_{0}}{k!}t^{k}\,.\] (7)

We observe that Equation (7) has the same function form as Equation (5), but with different coefficients (system parameters) for the polynomial of time \(t\). Therefore, the ODE system (3) under the latent DAG assumption can be considered a more complex version of the ODE system (2) when \(\bm{f}(t)\) follows a polynomial function of time \(t\). Since the ODE system (3) incorporates causally related latent confounders, which is a more interesting and practical case, we will provide a more comprehensive identifiability analysis of the ODE system (3). The derived identifiability results can be easily generated to the case of the ODE system (2).

### Identifiability condition from a single whole trajectory

We denote the unknown parameters of the ODE system (3) as \(\bm{\eta}\), that is, \(\bm{\eta}:=(\bm{x}_{0},\bm{z}_{0},A,B,G)\). We further denote the solution of the ODE system (3) as \([\bm{x}^{T}(t;\bm{\eta}),\bm{z}^{T}(t;\bm{\eta})]^{T}\); note that under our latent variables setting, only \(\bm{x}(t;\bm{\eta})\) is observable. Thus, based on Equation (7), we present the following identifiability definition.

**Definition 4.1**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d\times d },B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\), under the latent DAG assumption, for all \(\bm{x}^{\prime}_{0}\in\mathbb{R}^{d}\), all \(\bm{z}^{\prime}_{0}\in\mathbb{R}^{p}\), all \(A^{\prime}\in\mathbb{R}^{d\times d}\), all \(B^{\prime}\in\mathbb{R}^{d\times p}\), and all \(G^{\prime}\in\mathbb{R}^{p\times p}\), we denote \(\bm{\eta}^{\prime}:=(\bm{x}^{\prime}_{0},\bm{z}^{\prime}_{0},A^{\prime},B^{ \prime},G^{\prime})\), we say the ODE system (3) is \(\bm{\eta}\)-identifiable: if \((\bm{x}_{0},A,B\bm{z}_{0},BG\bm{z}_{0},\ldots,BG^{p-1}\bm{z}_{0})\neq(\bm{x}^ {\prime}_{0},A^{\prime},B^{\prime}\bm{z}^{\prime}_{0},B^{\prime}G^{\prime}\bm{ z}^{\prime}_{0},\ldots,B^{\prime}G^{\prime p-1}\bm{z}^{\prime}_{0})\), it holds that \(\bm{x}(\cdot;\bm{\eta})\neq\ \bm{x}(\cdot;\bm{\eta}^{\prime})\)._

Similar to the case of the ODE system (2), we refer to \(\bm{x}(\cdot;\bm{\eta})\) as the trajectory of the ODE system (3) with parameters \(\bm{\eta}\). Definition 4.1 defines the identifiability of the ODE system (3) from a single whole trajectory \(\bm{x}(\cdot;\bm{\eta})\). Once the ODE system (3) is \(\bm{\eta}\)-identifiable, the causal relationships among the observable variables \(\bm{x}\) can be determined through Equation (7). We then establish the condition for the identifiability of the ODE system (3) based on Definition 4.1.

**Theorem 4.1**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d \times d},B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\), under the latent DAG assumption, the ODE system (3) is \(\bm{\eta}\)-identifiable if and only if assumption **B1** is satisfied._

* _the set of vectors_ \(\{\bm{\gamma},A\bm{\gamma},\ldots,A^{d-1}\bm{\gamma}\}\) _is linearly independent, where_ \(\bm{\gamma}=A^{p}\bm{x}_{0}+\sum_{j=0}^{p-1}A^{p-1-j}BG^{j}\bm{z}_{0}\)_._

The proof of Theorem 4.1 can be found in Appendix D.2. Condition **B1** is both sufficient and necessary, and from a geometric perspective, it indicates that the vector \(\bm{\gamma}\) is not contained in an \(A\)-invariant proper subspace of \(\mathbb{R}^{d}\)[34, Lemma 3.1].

### Identifiability condition from discrete observations sampled from a single trajectory

In practice, we often have access only to a sequence of discrete observations sampled from a trajectory rather than knowing the whole trajectory. Therefore, we also derive the identifiability conditions under the scenario where only discrete observations from a trajectory are available. Firstly, we extend the identifiability definition of the ODE system (3) as follows.

**Definition 4.2**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d \times d},B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\). For any \(n\geqslant 1,\) let \(t_{j},j=1,\ldots,n\) be any \(n\) time points and \(\bm{x}_{j}:=\bm{x}(t_{j};\bm{\eta})\) be the error-free observation of the trajectory \(\bm{x}(\cdot;\bm{\eta})\) at time \(t_{j}\). Under the latent DAG assumption, we say the ODE system (3) is \(\bm{\eta}\)-identifiable from \(\bm{x}_{1},\ldots,\bm{x}_{n}\), if for all \(\bm{x}^{\prime}_{0}\in\mathbb{R}^{d}\), all \(\bm{z}^{\prime}_{0}\in\mathbb{R}^{p}\), all \(A^{\prime}\in\mathbb{R}^{d\times d}\), all \(B^{\prime}\in\mathbb{R}^{d\times p}\), and all \(G^{\prime}\in\mathbb{R}^{p\times p}\) with \((\bm{x}_{0},A,B\bm{z}_{0},BG\bm{z}_{0},\ldots,BG^{p-1}\bm{z}_{0})\neq(\bm{x}^ {\prime}_{0},A^{\prime},B^{\prime}\bm{z}^{\prime}_{0},B^{\prime}G^{\prime}\bm {z}^{\prime}_{0},\ldots,B^{\prime}G^{\prime p-1}\bm{z}^{\prime}_{0})\), it holds that \(\exists j\in\{1,\ldots,n\}\) such that \(\bm{x}(t_{j};\bm{\eta})\neq\bm{x}(t_{j};\bm{\eta}^{\prime})\)._

Definition 4.2 defines the identifiability of the ODE system (3) from \(n\) observations sampled from the trajectory \(\bm{x}(\cdot;\bm{\eta})\). Then we establish the condition for the identifiability of the ODE system (3) from discrete observations based on Definition 4.2.

**Theorem 4.2**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d \times d},B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\). We define new observation \(\bm{y}_{j}:=[\bm{x}^{T}_{j},1,t_{j},t_{j}^{2},\ldots,t_{j}^{p-1}]^{T}\in\mathbb{ R}^{d+p}\), for \(j=1,\ldots,n\). Under the latent DAG assumption, the ODE system (3) is \(\bm{\eta}\)-identifiable from discrete observations \(\bm{x}_{1},\ldots,\bm{x}_{n}\), if and only if assumption **C1** is satisfied._

* _there exists_ \((d+p)\)__\(\bm{y}_{j}\)_'s with indices denoting as_ \(\{j_{1},j_{2},\ldots,j_{d+p}\}\subseteq\{1,2,\ldots,n\}\)_, such that the set of vectors_ \(\{\bm{y}_{j_{1}},\bm{y}_{j_{2}},\ldots,\bm{y}_{j_{d+p}}\}\) _is linearly independent._

The proof of Theorem 4.2 can be found in Appendix D.3. Condition **C1** is both sufficient and necessary. This theorem states that as long as there are \(d+p\) observations \(\bm{x}_{j}\)'s such that the corresponding augmented new observations \(\bm{y}_{j}\)'s are linearly independent, the ODE system (3) is \(\bm{\eta}\)-identifiable from these discrete observations. Under the latent DAG assumption, we can transfer the ODE system (3), which includes hidden confounders, into a \((d+p)\)-dimensional fully observable ODE system (1) through the augmented state \(\bm{y}(t)\). Condition **C1** indicates that our observations span the entire \((d+p)\)-dimensional state space, thus rendering the system identifiable.

Both Definition 4.1 and Definition 4.2 define the identifiability of the ODE system (3) to some extent of the unknown parameters. In other words, given the available observations, under Definition 4.1 and Definition 4.2, one can only identify the values of \((\bm{x}_{0},A,B\bm{z}_{0},B\bm{G}\bm{z}_{0},\ldots,BG^{p-1}\bm{z}_{0})\), but not the values of \((\bm{z}_{0},B,G)\). Based on Equation (7), this level of identifiability is sufficient to identify the causal relationships between observable variables \(\bm{x}\), enabling safe intervention on the observable variables with reliable causal inferences. However, in scenarios where practitioners can intervene in the latent variables and require inferring the causal effects of the intervened system, identifying the matrices \(B\) and \(G\) becomes essential for reliable causal references. For instance, in chemical kinetics, where the evolution of chemical concentrations over time can often be modeled by an ODE system [8; 12], some chemicals may not be measurable during the reaction, rendering them latent variables. Nonetheless, practitioners can intervene in these latent variables by setting specific initial concentrations. Therefore, we provide an identifiability analysis of the linear ODE system (3) when practitioners can control the initial condition of the latent variables: \(\bm{z}_{0}\).

### Identifiability condition from \(p\) controllable whole trajectories

Assuming the initial condition of the latent variables \(\bm{z}_{0}\) is controllable, which means that the values of \(\bm{z}_{0}\) can be treated as given values, we denote it as \(\bm{z}_{0}^{*}\). In the following, we provide the identifiability condition of the ODE system (3) when we are given \(p\) initial conditions \(\bm{z}_{0}^{*}\), denoting as \(\bm{z}_{0}^{*i}\). We first present the definition.

**Definition 4.3**.: _Given \(\bm{z}_{0}^{*i}\in\mathbb{R}^{p}\) for \(i=1,\ldots,p\), for \(\bm{x}_{0}\in\mathbb{R}^{d},A\in\mathbb{R}^{d\times d},B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\), under the latent DAG assumption, for all \(\bm{x}_{0}^{\prime}\in\mathbb{R}^{d}\), all \(A^{\prime}\in\mathbb{R}^{d\times d}\), all \(B^{\prime}\in\mathbb{R}^{d\times p}\), and all \(G^{\prime}\in\mathbb{R}^{p\times p}\), we denote \(\bm{\eta}_{i}:=(\bm{x}_{0},\bm{z}_{0}^{*i},A,B,G)\) and \(\bm{\eta}_{i}^{\prime}:=(\bm{x}_{0}^{\prime},\bm{z}_{0}^{*i},A^{\prime},B^{ \prime},G^{\prime})\), we say the ODE system (3) is \(\{\bm{\eta}_{i}\}_{1}^{p}\)-identifiable: if \((\bm{x}_{0},A,B,G)\neq(\bm{x}^{\prime},A^{\prime},B^{\prime},G^{\prime})\), it holds that \(\exists i\) such that \(\bm{x}(\cdot;\bm{\eta}_{i})\neq\bm{x}(\cdot;\bm{\eta}_{i}^{\prime})\), \(\forall i=1,\ldots,p\), and under this definition, matrix \(B\) and \(G\) are also identifiable. Based on this definition, we provide the identifiability condition.

**Theorem 4.3**.: _Given \(\bm{z}_{0}^{*i}\in\mathbb{R}^{p}\) for \(i=1,\ldots,p\), for \(\bm{x}_{0}\in\mathbb{R}^{d},A\in\mathbb{R}^{d\times d},B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\), under the latent DAG assumption, the ODE system (3) is \(\{\bm{\eta}_{i}\}_{1}^{p}\)-identifiable if assumptions **B\({}_{2}\)**, **B\({}_{3}\)** and **B\({}_{4}\)** are all satisfied._

1. _each_ \(\bm{z}_{0}^{*i}\) _for_ \(i=1,\ldots,p\)_, satisfies assumption_ _B1_. That is, if we set_ \(\bm{\gamma}_{i}=A^{p}\bm{x}_{0}+\sum_{j=0}^{p-1}A^{p-1-j}BG^{j}\bm{z}_{0}^{*i}\)_, then the set of vectors_ \(\{\bm{\gamma}_{i},A\bm{\gamma}_{i},\ldots,A^{d-1}\bm{\gamma}_{i}\}\) _is linearly independent for all_ \(i=1,\ldots,p\)_._
2. _the set of vectors_ \(\{\bm{z}_{0}^{*1},\bm{z}_{0}^{*2},\ldots,\bm{z}_{0}^{*p}\}\) _is linearly independent._
3. _the matrix composed by vertically stack the matrices_ \(\{B,BG,\ldots,BG^{p-1}\}\) _has rank_ \(p\)_._

The proof of Theorem 4.3 can be found in Appendix D.4. Assumption **B2** ensures that the ODE system (3) is \(\bm{\eta}_{i}\)-identifiable for all \(i=1,\ldots,p\). Consequently, \((\bm{x}_{0},A,B\bm{z}_{0}^{*i},BG\bm{z}_{0}^{*i},\ldots,BG^{p-1}\bm{z}_{0}^{*i})\) for all \(i=1,\ldots,p\) is identifiable. Then, under assumption **B3**, the identifiability of matrix \(B\) is established. To identify matrix \(G\), assumption **B4** is required. While the ability to control the initial condition of the latent variables may appear strict, it is a reasonable assumption in our context. This is because identifying matrices \(B\) and \(G\) is necessary only when practitioners can intervene in the latent variables, thereby allowing control over their initial conditions. An alternative approach to identifying \(B\) and \(G\) involves intervening in the initial condition of each latent variable \(z_{i}\) independently, rather than controlling the initial condition of all latent variables \(\bm{z}\) simultaneously. This method draws inspiration from the "genetic single-node intervention" proposed by [32], where one can intervene at each latent node individually. Further details of this method can be found in Appendix F.

### Identifiability condition from discrete observations sampled from \(p\) controllable trajectories

We also extend the identifiability analysis of the ODE system (3) to cases where only discrete observations from \(p\) controllable trajectories are available.

**Definition 4.4**.: _Given \(\bm{z}_{0}^{*i}\in\mathbb{R}^{p}\) for \(i=1,\ldots,p\), for \(\bm{x}_{0}\in\mathbb{R}^{d},A\in\mathbb{R}^{d\times d},B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\). For any \(n\geqslant 1\), let \(t_{j},j=1,\ldots,n\) be any \(n\) time points and \(\bm{x}_{ij}:=\bm{x}(t_{j};\bm{\eta}_{i})\) be the error-free observation of the trajectory \(\bm{x}(\cdot;\bm{\eta}_{i})\) at time \(t_{j}\). Under the latent DAG assumption, we say the ODE system (3) is \(\{\bm{\eta}_{i}\}_{1}^{p}\)-identifiable from \(\bm{x}_{i1},\ldots,\bm{x}_{in}\), \(i=1,\ldots,p\), if for all \(\bm{x}_{0}^{\prime}\in\mathbb{R}^{d}\), all \(A^{\prime}\in\mathbb{R}^{d\times d}\), all \(B^{\prime}\in\mathbb{R}^{d\times p}\), and all \(G^{\prime}\in\mathbb{R}^{p\times p}\) with \((\bm{x}_{0},A,B,G)\neq(\bm{x}_{0}^{\prime},A^{\prime},B^{\prime},G^{\prime})\), it holds that \(\exists i\in\{1,\ldots,p\}\) and \(j\in\{1,\ldots,n\}\) such that \(\bm{x}(t_{j};\bm{\eta}_{i})\neqneq\bm{x}(t_{j};\bm{\eta}_{i}^{\prime})\)._

Based on Definition 4.4 we present the identifiability condition.

**Theorem 4.4**.: _Given \(\bm{z}_{0}^{*i}\in\mathbb{R}^{p}\) for \(i=1,\ldots,p\), for \(\bm{x}_{0}\in\mathbb{R}^{d},A\in\mathbb{R}^{d\times d},B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\). We define new observation \(\bm{y}_{ij}:=[\bm{x}_{ij}^{T},1,t_{j},t_{j}^{2},\ldots,t_{j}^{p-1}]^{T}\in \mathbb{R}^{d+p}\), for \(i=1,\ldots,p\) and \(j=1,\ldots,n\). Under the latent DAG assumption, the ODE system (3) is \(\{\bm{\eta}_{i}\}_{1}^{p}\)-identifiable from discrete observations \(\bm{x}_{i1},\ldots,\bm{x}_{in}\), \(i=1,\ldots,p\), if assumptions **C2**, **B3** and **B4** are all satisfied._

* _for each_ \(i\in\{1,\ldots,p\}\) _there exists_ \((d+p)\)__\(\bm{y}_{ij}\)_'s with indexes denoting as_ \(\{j_{i1},j_{i2},\ldots,j_{i,d+p}\}\subseteq\{1,2,\ldots,n\}\)_, such that the set of vectors_ \(\{\bm{y}_{ij_{i1}},\bm{y}_{ij_{i2}},\ldots,\bm{y}_{ij_{i,d+p}}\}\) _is linearly independent._

The proof of Theorem 4.4 can be found in Appendix D.5. Assumption **C2** ensures that the ODE system (3) is \(\bm{\eta}_{i}\)-identifiable from discrete observations \(\bm{x}_{i1},\ldots,\bm{x}_{in}\) for all \(i=1,\ldots,p\). As in Subsection 4.3, under assumptions **B3** and **B4**, the matrices \(B\) and \(G\) are also identifiable.

## 5 Simulations

To evaluate the validity of the identifiability conditions established in Section 3 and 4, we present the results of simulations. As previously indicated, the ODE system (3) can be treated as a more intricate version of the ODE system (2); hence, our simulation experiments are centered on the former.

**Simulation design.** We conduct four sets of simulations, which include one identifiable case and one unidentifiable case for both the \(\bm{\eta}\)-identifiable check and the \(\{\bm{\eta}_{i}\}_{1}^{p}\)-identifiable check. The dimensions of both observable variables, \(d\), and latent variables, \(p\), are set to 3. The true underlying parameters of the systems are provided below. Observations are simulated from the true ODE systems for each case, with \(n\) equally-spaced observations generated from the time interval \([0,1]\) for each trajectory, and we only keep the values of the observable variables \(\bm{x}\).

\[A=\begin{bmatrix}2&-2&1\\ 1&1&-1\\ 1&0&2\end{bmatrix},\;\;B=\begin{bmatrix}-2&-2&2\\ 0&-1&-2\\ -1&-1&-2\end{bmatrix}\;,\;\;G=\begin{bmatrix}0&2&1\\ 0&0&-2\\ 0&0&0\end{bmatrix}\,,\;\;A^{\prime}=\begin{bmatrix}1&0&0\\ 0&1&0\\ 0&0&1\end{bmatrix}\,,\]

\[\bm{x}_{0}=\begin{bmatrix}-1\\ 1\\ 1\end{bmatrix}\,,\;\;\;\bm{z}_{0}=\begin{bmatrix}1\\ -2\\ -1\end{bmatrix}\,,\;\;\;\bm{z}_{0}^{*1}=\begin{bmatrix}1\\ 0\\ 0\end{bmatrix}\,,\;\;\;\bm{z}_{0}^{*2}=\begin{bmatrix}0\\ 1\\ 0\end{bmatrix}\,,\;\;\;\bm{z}_{0}^{*3}=\begin{bmatrix}0\\ 0\\ 1\end{bmatrix}\,.\]

\(\bm{\eta}\)-identifiable: \(\bm{\eta}=(\bm{x}_{0},\bm{z}_{0},A,B,G)\), unidentifiable: \(\bm{\eta}=(\bm{x}_{0},\bm{z}_{0},A^{\prime},B,G)\).

\(\{\bm{\eta}_{i}\}_{1}^{p}\)-identifiable: \(\bm{\eta}_{i}=(\bm{x}_{0},\bm{z}_{0}^{*i},A,B,G),\) unidentifiable: \(\bm{\eta}_{i}=(\bm{x}_{0},\bm{z}_{0}^{*i},A^{\prime},B,G),i=1,2,3\).

**Parameter estimation.** The Nonlinear Least Squares (NLS) method is employed for parameter estimation, a widely used technique for estimating parameters in nonlinear regression models, including ODEs [7, 21, 46]. The _"least_squares"_ function from the _"scipy.optimize"_ Python module, with default hyperparameter settings, is utilized for implementation. Given that the NLS loss function for our simulation is non-convex, parameter initialization is performed near the true values to promote convergence to the global minimum. Specifically, for the \(\bm{\eta}\)-(un)identifiable cases, initial parameter values are set to the true parameters plus a random value drawn from a uniform distribution \(U(-0.1,0.1)\) for each replication. For \(\{\bm{\eta}_{i}\}_{1}^{p}\)-(un)identifiable cases, initial parameter values are set to the true values plus a random value from \(U(-0.3,0.3)\).

**Evaluation metric.** Mean Squared Error (MSE) is adopted as the metric to assess the accuracy of the parameter estimator. To ensure the reliability of the estimation results, 100 independent random replications are run for each configuration, and we report the mean and variance of the squared error.

**Results analysis.** Table 1 and Table 2 present the simulation results for the \(\bm{\eta}\)-(un)identifiable cases and the \(\{\bm{\eta}_{i}\}_{1}^{p}\)-(un)identifiable cases, respectively. According to Definition 4.1 and Definition 4.3, for the \(\bm{\eta}\)-(un)identifiable cases, the identifiability of \((\bm{x}_{0},A,B\bm{z}_{0},BG\bm{z}_{0},BG^{2}\bm{z}_{0})\) needs to be checked, while for the \(\{\bm{\eta}_{i}\}_{1}^{p}\)-(un)identifiable cases, we need to check the identifiability of \((\bm{x}_{0},A,B,G)\)Since \(\bm{x}_{0}\) is consistently identifiable (with MSE less than \(1.00\)E-\(10\)) across all (un)identifiable cases, its results are not presented.

In both Tables, for identifiable cases, as the number of samples \(n\) increases, the MSEs for all parameters of interest decrease and approach zero. However, in the unidenifiable cases, where the identifiability condition **B1/B2** stated in Theorem 4.1/4.3 is unmet, the MSEs for certain parameters remain high irrespective of sample size. These results offer strong empirical support for the validity of the identifiability conditions outlined in Theorem 4.1 and Theorem 4.3. It is noteworthy that in the \(\{\bm{\eta}_{i}\}_{1}^{p}\) case, where observations are sampled from \(p=3\) controllable trajectories, remarkably accurate parameter estimates can be obtained even with a limited number of samples.

For the \(\bm{\eta}\)-(un)identifiable cases, assumption **C1** stated in Theorem 4.2 holds true for all values of \(n\) in the identifiable cases, while it is violated across all \(n\) in the unidentifiable cases. In the \(\{\bm{\eta}_{i}\}_{1}^{p}\)-(un)identifiable cases, condition **C2** stated in Theorem 4.4 is satisfied for all values of \(n\) in the identifiable cases, but is found to be violated for all values of \(n\) in the unidentifiable cases. These findings provide strong empirical evidence supporting the validity of the identifiability conditions proposed in Theorem 4.2 and Theorem 4.4.

In Appendix G, we present additional simulation results for higher-dimensional cases, along with simulations that incorporate a variety of ground-truth parameter configurations. These results consistently affirm the validity of our proposed identifiability conditions. For further details, please refer to Appendix G.

## 6 Related work

**Identifiability analysis of linear ODE systems.** Within control theory, extensive research has been conducted on the identifiability analysis of linear dynamical systems governed by ODEs [5; 14; 15; 17]. In the applied mathematics area, Stanhope et al. [34] and Qiu et al. [28] have systematically investigated the identifiability of linear ODE systems based on a single trajectory. Furthermore, Wang et al. [42] have extended these findings to scenarios where only discrete observations sampled

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{\(\bm{n}\)} & \multicolumn{4}{c}{**Identifiable**} & \multicolumn{4}{c}{**Unidentifiable**} \\ \cline{2-9}  & \(A\) & \(B\bm{z}_{0}\) & \(BG\bm{z}_{0}\) & \(BG^{2}\bm{z}_{0}\) & \(A\) & \(B\bm{z}_{0}\) & \(BG\bm{z}_{0}\) & \(BG^{2}\bm{z}_{0}\) \\ \hline \multirow{2}{*}{10} & 6.00E-05 & 0.0004 & 0.0044 & 0.0007 & 0.0994 & 0.0494 & 0.9185 & 0.6482 \\  & (\(\pm\)5.40E-08) & (\(\pm\)3.45E-06) & (\(\pm\)0.0004) & (\(\pm\)3.91E-06) & (\(\pm\)0.0157) & (\(\pm\)0.1243) & (\(\pm\)8.3148) & (\(\pm\)1.4306) \\ \multirow{2}{*}{100} & 4.15E-05 & 0.0003 & 0.0029 & 0.0005 & 0.0372 & 0.0174 & 0.3517 & 0.5767 \\  & (\(\pm\)1.62E-08) & (\(\pm\)8.52E-07) & (\(\pm\)9.42E-05) & (\(\pm\)2.90E-06) & (\(\pm\)0.0032) & (\(\pm\)0.0087) & (\(\pm\)0.3460) & (\(\pm\)1.4055) \\ \multirow{2}{*}{500} & 2.65E-05 & 0.0002 & 0.0019 & 0.0002 & 0.0461 & 0.1071 & 0.5783 & 0.3648 \\  & (\(\pm\)8.71E-09) & (\(\pm\)4.38E-07) & (\(\pm\)4.84E-05) & (\(\pm\)8.38E-07) & (\(\pm\)0.0099) & (\(\pm\)0.1768) & (\(\pm\)2.5747) & (\(\pm\)0.4507) \\ \hline \hline \end{tabular}
\end{table}
Table 1: MSEs of the \(\bm{\eta}\)-(un)identifiable cases of the ODE (3)

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{\(\bm{n}\)} & \multicolumn{4}{c}{**Identifiable**} & \multicolumn{4}{c}{**Unidentifiable**} \\ \cline{2-9}  & \(A\) & \(B\) & \(G\) & \(A\) & \(B\) & \(G\) \\ \hline \multirow{2}{*}{10} & 5.83E-22 & 2.85E-21 & 2.27E-21 & 0.6349 & 0.1913 & 0.0044 \\  & (\(\pm\)7.41E-42) & (\(\pm\)2.75E-40) & (\(\pm\)5.69E-41) & (\(\pm\)0.7464) & (\(\pm\)0.0686) & (\(\pm\)0.0011) \\ \multirow{2}{*}{30} & 1.50E-22 & 7.80E-22 & 5.76E-22 & 0.6169 & 0.1850 & 0.0045 \\  & (\(\pm\)3.23E-43) & (\(\pm\)1.14E-41) & (\(\pm\)5.28E-42) & (\(\pm\)0.7194) & (\(\pm\)0.0657) & (\(\pm\)0.0007) \\ \multirow{2}{*}{50} & 5.16E-23 & 3.01E-22 & 2.39E-22 & 0.5876 & 0.1761 & 0.0045 \\  & (\(\pm\)6.20E-44) & (\(\pm\)3.27E-42) & (\(\pm\)8.46E-43) & (\(\pm\)0.6895) & (\(\pm\)0.0627) & (\(\pm\)0.0008) \\ \hline \hline \end{tabular}
\end{table}
Table 2: MSEs of the \(\{\bm{\eta}_{i}\}_{1}^{p}\)-(un)identifiable cases of the ODE (3)from a single trajectory are available. However, existing studies primarily concentrate on linear ODE systems with fully observable variables. To the best of our knowledge, our work represents the inaugural endeavor to systematically analyze the identifiability of linear ODE systems in the presence of hidden confounders.

**Connection between causality and differential equations.** Differential equations provide a natural framework for understanding causality within dynamic systems, particularly in the context of continuous-time processes [1; 31]. Consequently, significant efforts have been directed towards establishing a theoretical link between causality and differential equations. In the deterministic case, Mooij et al. [25] and Rubenstein et al. [30] have established a mathematical connection between ODEs and Structural Causal Models (SCMs). Wang et al. [42] have proposed a method for inferring the causal structure of linear ODEs. In the domain of neural ODEs, Aliee et al. [2; 3] have applied various regularization techniques to enhance the recovery of the causal relationships. Turning to the stochastic case, Hansen et al. [18] and Wang et al. [41] have proposed causal interpretations and identifiability analysis of Stochastic Differential Equations (SDEs). Additionally, Bellot et al. [6] have introduced a method for consistently discovering the causal structure of SDE systems using penalized neural ODEs. These works aim to establish a theoretical connection between causality and differential equations in various ways. Our contribution to this scholarly landscape lies in the systematic analysis of the identifiability of linear ODEs, particularly in the presence of hidden confounders.

## 7 Conclusion

This paper presents a systematic identifiability analysis of linear ODE systems incorporating hidden confounders. Specifically, we establish a sufficient and necessary identifiability condition for the linear ODE system with independent latent confounders. Additionally, we provide four identifiability conditions for the linear ODE system with causally related latent confounders, wherein the causal structure of the latent confounders adheres to a DAG.

A notable limitation of our work lies in the practical verification of these identifiability conditions, given that the true underlying system parameters are often unavailable in real-world scenarios. However, our study significantly contributes to the understanding of the intrinsic structure of linear ODE systems with hidden confounders. By providing insights into the identifiability aspects, our findings empower practitioners to utilize models that adhere to the proposed conditions (e.g., through constrained parameter estimation) for learning from real-world data while ensuring identifiability.

## Acknowledgements

YW was supported by the Australian Government Research Training Program (RTP) Scholarship from the University of Melbourne. BH was supported by NSF DMS-2428058. XG was supported by ARC DE210101352. MG was supported by ARC DE210101624 and ARC DP240102088.

## References

* [1] O. O. Aalen, K. Rosysland, J. M. Gran, and B. Ledergerber. Causality, mediation and time: a dynamic viewpoint. _Journal of the Royal Statistical Society: Series A (Statistics in Society)_, 175(4):831-861, 2012.
* [2] H. Aliee, T. Richter, M. Solonin, I. Ibarra, F. Theis, and N. Kilbertus. Sparsity in continuous-depth neural networks. _Advances in Neural Information Processing Systems_, 35:901-914, 2022.
* [3] H. Aliee, F. J. Theis, and N. Kilbertus. Beyond predictions in neural odes: Identification and interventions. _arXiv preprint arXiv:2106.12430_, 2021.
* [4] M.-C. Anisiu. Lotka, volterra and their model. _Didactica mathematica_, 32(01), 2014.
* [5] R. Bellman and K. J. Astrom. On structural identifiability. _Mathematical biosciences_, 7(3-4):329-339, 1970.
* [6] A. Bellot, K. Branson, and M. van der Schaar. Neural graphical modelling in continuous-time: consistency guarantees and algorithms. In _International Conference on Learning Representations_, 2021.
* [7] H. G. Bock. Recent advances in parameterization techniques for ode. In _Numerical Treatment of Inverse Problems in Differential and Integral Equations: Proceedings of an International Workshop, Heidelberg, Fed. Rep. of Germany, August 30--September 3, 1982_, pages 95-121. Springer, 1983.
* [8] S. C. Burnham, D. P. Searson, M. J. Willis, and A. R. Wright. Inference of chemical reaction networks. _Chemical Engineering Science_, 63(4):862-873, 2008.
* [9] I. Bzhikhatlov and S. Perepelkina. Research of robot model behaviour depending on model parameters using physic engines bullet physics and ode. In _2017 International Conference on Industrial Engineering, Applications and Manufacturing (ICIEAM)_, pages 1-4. IEEE, 2017.
* [10] R. Cai, F. Xie, C. Glymour, Z. Hao, and K. Zhang. Triad constraints for learning causal structure of latent variables. _Advances in neural information processing systems_, 32, 2019.
* [11] W. Chen, K. Zhang, R. Cai, B. Huang, J. Ramsey, Z. Hao, and C. Glymour. Fritl: A hybrid method for causal discovery in the presence of latent confounders. _arXiv preprint arXiv:2103.14238_, 2021.
* [12] G. Craciun and M. Feinberg. Multiple equilibria in complex chemical reaction networks: I. the injectivity property. _SIAM Journal on Applied Mathematics_, 65(5):1526-1546, 2005.
* [13] E. Dockner. _Differential games in economics and management science_. Cambridge University Press, 2000.
* [14] B. Gargash and D. Mital. A necessary and sufficient condition of global structural identifiability of compartmental models. _Computers in biology and medicine_, 10(4):237-242, 1980.
* [15] K. Glover and J. Willems. Parametrizations of linear dynamical systems: Canonical forms and identifiability. _IEEE Transactions on Automatic Control_, 19(6):640-646, 1974.
* [16] D.-E. Gratie, B. Iancu, and I. Petre. Ode analysis of biological systems. _Formal Methods for Dynamical Systems: 13th International School on Formal Methods for the Design of Computer, Communication, and Software Systems, SFM 2013, Bertinoro, Italy, June 17-22, 2013. Advanced Lectures_, pages 29-62, 2013.
* [17] M. Grewal and K. Glover. Identifiability of linear and nonlinear dynamical systems. _IEEE Transactions on automatic control_, 21(6):833-837, 1976.
* [18] N. Hansen and A. Sokol. Causal interpretation of stochastic differential equations. _Electronic Journal of Probability_, 19:1-24, 2014.

* [19] P. O. Hoyer, S. Shimizu, A. J. Kerminen, and M. Palviainen. Estimation of causal effects using linear non-gaussian causal models with hidden variables. _International Journal of Approximate Reasoning_, 49(2):362-378, 2008.
* [20] B. Huang, C. J. H. Low, F. Xie, C. Glymour, and K. Zhang. Latent hierarchical causal structure discovery with rank constraints. _Advances in Neural Information Processing Systems_, 35:5549-5561, 2022.
* [21] R. I. Jennrich. Asymptotic properties of non-linear least squares estimators. _The Annals of Mathematical Statistics_, 40(2):633-643, 1969.
* [22] D. Kaltenpoth and J. Vreeken. Causal discovery with hidden confounders using the algorithmic markov condition. In _Uncertainty in Artificial Intelligence_, pages 1016-1026. PMLR, 2023.
* [23] M. N. Koleva and L. G. Vulkov. Two-grid quasilinearization approach to odes with applications to model problems in physics and mechanics. _Computer Physics Communications_, 181(3):663-670, 2010.
* [24] V. Mandelzweig and F. Tabakin. Quasilinearization approach to nonlinear problems in physics with application to nonlinear odes. _Computer Physics Communications_, 141(2):268-281, 2001.
* [25] J. Mooij, D. Janzing, and B. Scholkopf. From ordinary differential equations to structural causal models: the deterministic case. In _29th Conference on Uncertainty in Artificial Intelligence (UAI 2013)_, pages 440-448. AUAI Press, 2013.
* [26] J. Pearl. _Causality_. Cambridge university press, 2009.
* [27] A. Polynikis, S. Hogan, and M. Di Bernardo. Comparing different ode modelling approaches for gene regulatory networks. _Journal of theoretical biology_, 261(4):511-530, 2009.
* [28] X. Qiu, T. Xu, B. Soltanalizadeh, and H. Wu. Identifiability analysis of linear ordinary differential equation systems with a single trajectory. _Applied Mathematics and Computation_, 430:127260, 2022.
* [29] M. Quach, N. Brunel, and F. d'Alche Buc. Estimating parameters and hidden variables in non-linear state-space models based on odes for biological networks inference. _Bioinformatics_, 23(23):3209-3216, 2007.
* [30] P. Rubenstein, S. Bongers, B. Scholkopf, and J. Mooij. From deterministic odes to dynamic structural causal models. In _34th Conference on Uncertainty in Artificial Intelligence (UAI 2018)_, pages 114-123. Curran Associates, Inc., 2018.
* [31] B. Scholkopf, F. Locatello, S. Bauer, N. R. Ke, N. Kalchbrenner, A. Goyal, and Y. Bengio. Toward causal representation learning. _Proceedings of the IEEE_, 109(5):612-634, 2021.
* [32] C. Squires, A. Seigal, S. S. Bhate, and C. Uhler. Linear causal disentanglement via interventions. In _International Conference on Machine Learning_, pages 32540-32560. PMLR, 2023.
* [33] P. Stadter, Y. Schalte, L. Schmiester, J. Hasenauer, and P. L. Stapor. Benchmarking of numerical integration methods for ode models of biological systems. _Scientific reports_, 11(1):2696, 2021.
* [34] S. Stanhope, J. E. Rubin, and D. Swigon. Identifiability of linear and linear-in-parameters dynamical systems from a single trajectory. _SIAM Journal on Applied Dynamical Systems_, 13(4):1792-1815, 2014.
* [35] H. Straubing. A combinatorial proof of the cayley-hamilton theorem. _Discrete Mathematics_, 43(2-3):273-279, 1983.
* [36] W.-H. Su, C.-S. Chou, and D. Xiu. Deep learning of biological models from data: applications to ode models. _Bulletin of Mathematical Biology_, 83:1-19, 2021.
* [37] G. Teschl. _Ordinary differential equations and dynamical systems_, volume 140. American Mathematical Soc., 2012.

* [38] A. Tsoularis. On some important ordinary differential equations of dynamic economics. _Recent developments in the solution of nonlinear differential equations_, pages 147-153, 2021.
* [39] P. N. Tu. _Dynamical systems: an introduction with applications in economics and biology_. Springer Science & Business Media, 2012.
* [40] T. Verma and J. Pearl. Causal networks: Semantics and expressiveness. In _Machine intelligence and pattern recognition_, volume 9, pages 69-76. Elsevier, 1990.
* [41] Y. Wang, X. Geng, W. Huang, B. Huang, and M. Gong. Generator identification for linear sdes with additive and multiplicative noise. _Advances in Neural Information Processing Systems_, 36, 2024.
* [42] Y. Wang, W. Huang, M. Gong, X. Geng, T. Liu, K. Zhang, and D. Tao. Identifiability and asymptotics in learning homogeneous linear ode systems from discrete observations. _Journal of Machine Learning Research_, 25(154):1-50, 2024.
* [43] T. A. Weber. _Optimal control theory with applications in economics_. MIT press, 2011.
* [44] F. Xie, R. Cai, B. Huang, C. Glymour, Z. Hao, and K. Zhang. Generalized independent noise condition for estimating latent variable causal graphs. _Advances in neural information processing systems_, 33:14891-14902, 2020.
* [45] F. Xie, B. Huang, Z. Chen, Y. He, Z. Geng, and K. Zhang. Identification of linear non-gaussian latent hierarchical structure. In _International Conference on Machine Learning_, pages 24370-24387. PMLR, 2022.
* [46] H. Xue, H. Miao, and H. Wu. Sieve estimation of constant and time-varying coefficients in nonlinear ordinary differential equation models by considering both numerical error and measurement error. _Annals of statistics_, 38(4):2351, 2010.
* [47] Y. D. Zhong, B. Dey, and A. Chakraborty. Symplectic ode-net: Learning hamiltonian dynamics with control. _arXiv preprint arXiv:1909.12077_, 2019.

[MISSING_PAGE_EMPTY:14]

Real world examples

In this section, we present two real-world examples that correspond to the ODE systems (2) and (3). These examples initially assume fully observable systems, with latent variables introduced by us based on prior experience or established physical laws.

### Damped harmonic oscillators model

Consider a one-dimensional system comprising \(D\) point masses \(m_{i}\) for \(i=1,\ldots,D\) with positions \(Q_{i}(t)\in\mathbb{R}\) and momenta \(P_{i}(t)\in\mathbb{R}\). These masses are interconnected by springs characterized by spring constants \(k_{i}\) and equilibrium lengths \(l_{i}\), and each mass is subject to friction with coefficient \(b_{i}\). The system's boundary conditions are fixed at \(Q_{0}(t)=0\) and \(Q_{D+1}(t)=L\).

The dynamics of this system are described by the following linear ODE system [25]:

\[\dot{P}_{i}(t) =k_{i}(Q_{i+1}(t)-Q_{i}(t)-l_{i})-k_{i-1}(Q_{i}(t)-Q_{i-1}(t)-l_{i- 1})-b_{i}P_{i}(t)/m_{i}\] (8) \[\dot{Q}_{i}(t) =P_{i}(t)/m_{i}\]

where \(Q_{0}(t)=0\) and \(Q_{D+1}(t)=L\) represent the fixed boundary conditions. External forces \(F_{j}(t)\) (e.g., wind force or a varying magnetic field) may influence the entire system of coupled oscillators. These external forces can be modeled here as latent variables with constant derivatives. Consequently, the system can be reformulated as follows:

\[\dot{P}_{i}(t) =k_{i}(Q_{i+1}(t)-Q_{i}(t)-l_{i})-k_{i-1}(Q_{i}(t)-Q_{i-1}(t)-l_{i -1})-b_{i}P_{i}(t)/m_{i}+\sum_{j}\alpha_{ij}F_{j}(t)\] (9) \[\dot{Q}_{i}(t) =P_{i}(t)/m_{i}\] \[\dot{F}_{j}(t) =c_{j}\]

where \(\alpha_{ij}\) is a constant determining the effect of the external force \(F_{j}(t)\) on the \(i\)-th mass, and \(c_{j}\) is the constant rate of change of the external force \(F_{j}(t)\). This model aligns with our ODE system (2), and an illustrative causal structure for this model is provided in Figure 3.

In regions with predictable wind patterns, such as during monsoon seasons or in controlled experimental settings, wind force can be approximated with a constant rate, making this an ideal context for modeling external forces with constant derivatives. Furthermore, constant forces or those represented as polynomial functions of time align well with our ODE system structure. For instance, a uniform magnetic field acting on the system would produce a constant force. These examples demonstrate that various latent factors can effectively fit within our ODE structure.

Figure 3: Example causal structure of the damped harmonic oscillators model with \(3\) oscillators and \(2\) latent variables.

### Population model

The growth of a population \(P(t)\) can be described by a linear ODE [4]:

\[\dot{P}(t)=aP(t),\]

where \(a\) is a constant representing the population growth rate. This system may also be influenced by latent variables \(L_{i}\), such as environmental factors or food supply. By incorporating these latent influences, the system can be expressed as:

\[\dot{P}(t) =aP(t)+bL_{1}(t)+cL_{2}(t)\] \[\dot{L_{1}}(t) =lL_{2}(t)\] \[\dot{L_{2}}(t) =m\]

where \(a,b,c,l\) and \(m\) are constants. Here, \(L_{1}(t)\) represents the food supply, which is influenced by the environmental factor \(L_{2}(t)\). \(L_{2}(t)\) corresponds to an environmental factor, such as temperature or pollution level, that changes steadily over time. This model aligns well with our ODE system (3), and an illustrative causal structure for this model is provided in Figure 4.

An example of an environmental factor changing at a constant rate is pollution from an industrial plant that continuously releases a fixed amount of pollutants, or from a wastewater treatment plant that discharges a specified amount of treated wastewater into a river on an hourly basis.

Figure 4: Causal structure of the population model.

An example of an unidentififiable case of the linear ODE system (3)

Recall that the parameters of the ODE system (3) are:

\[\bm{x}_{0}=\begin{bmatrix}1\\ 1\end{bmatrix}\,,\ \ \bm{z}_{0}=\begin{bmatrix}1\\ 1\end{bmatrix}\,,\ \ B=\begin{bmatrix}1&1\\ 1&1\end{bmatrix}\,,\ \ G=\begin{bmatrix}0&1\\ 0&0\end{bmatrix}\,,\]

\[A=\begin{bmatrix}1&0\\ 0&1\end{bmatrix}\,,\ \ \ A^{\prime}=\begin{bmatrix}0&1\\ 1&0\end{bmatrix}\,,\ \ \ M=\begin{bmatrix}A&B\\ \bm{0}&G\end{bmatrix}\,,\ \ \ M^{\prime}=\begin{bmatrix}A^{\prime}&B\\ \bm{0}&G\end{bmatrix}\,.\]

We first calculate the solution of \(\bm{z}(t)\),

\[\bm{z}(t) =e^{Gt}\bm{z}_{0}\] \[=\sum_{k=0}^{\infty}\frac{G^{k}\bm{z}_{0}}{k!}t^{k}=\sum_{k=0}^{ 1}\frac{G^{k}\bm{z}_{0}}{k!}t^{k}=\begin{bmatrix}1+t\\ 1\end{bmatrix}\]

We intervene \(x_{1}(t)=1\), then under matrix \(M\):

\[\dot{x}_{2}(t) =x_{2}(t)+z_{1}(t)+z_{2}(t)\] \[=x_{2}(t)+t+2\,.\]

To solve this differential equation, we rewrite it in the standard linear form and multiply both sides by the integrating factor \(e^{-t}\),

\[e^{-t}\dot{x}_{2}(t)-e^{-t}x_{2}(t)=(t+2)e^{-t}\,.\]

The left-hand side of this equation is the derivative of \(e^{-t}x_{2}(t)\):

\[\frac{d}{dt}(e^{-t}x_{2}(t))=(t+2)e^{-t}\,.\]

Next, integrate both sides w.r.t. \(t\):

\[\int\frac{d}{dt}(e^{-t}x_{2}(t))dt=\int(t+2)e^{-t}dt\,.\]

The left-hand side integrates to:

\[e^{-t}x_{2}(t)\,.\]

Next, we use integration by parts to find the integral on the right-hand side:

\[\int(t+2)e^{-t}dt =-(t+2)e^{-t}-\int-e^{-t}dt\] \[=-(t+2)e^{-t}-e^{-t}\] \[=-(t+3)e^{-t}\,.\]

Thus:

\[e^{-t}x_{2}(t)=-(t+3)e^{-t}+C\,,\]

where \(C\) is the constant of the integration.

Multiplying both sides by \(e^{t}\) to solve for \(x_{2}(t)\):

\[x_{2}(t)=-t-3+Ce^{t}\,.\]

Now, use the initial condition \(x_{2}(0)=1\), we get

\[C=4\,.\]

Therefore,

\[x_{2}(t)=4e^{t}-t-3\,.\]

Whereas under matrix \(M^{\prime}\):

\[\dot{x}_{2}(t) =x_{1}(t)+z_{1}(t)+z_{2}(t)\] \[=t+3\,.\]

Simple calculations show that

\[\bm{x}_{2}(t)=t^{2}/2+3t+1\,.\]Detailed proofs

### Proof of Theorem 3.1

Proof.: Recall that the first derivative of \(\bm{x}(t)\) can be expressed as:

\[\dot{\bm{x}}(t) =A\bm{x}(t)+B\bm{z}(t)\] \[=A\bm{x}(t)+\sum_{k=0}^{r}\frac{B\bm{v}_{k}}{k+1}t^{k+1}+B\bm{z}_{0}\,.\]

Set

\[\bm{y}(t)=\begin{bmatrix}\bm{x}(t)\\ 1\\ t\\ t^{2}\\ \vdots\\ t^{r+1}\end{bmatrix}\,,\]

we see that \(\bm{y}(t)\in\mathbb{R}^{d+r+2}\), and the first derivative of \(\bm{y}(t)\) w.r.t. time \(t\) can be expressed as

\[\dot{\bm{y}}(t) =\begin{bmatrix}\dot{\bm{x}}(t)\\ 0\\ 1\\ 2t\\ \vdots\\ (r+1)t^{r}\end{bmatrix}\] \[=\begin{bmatrix}A&B\bm{z}_{0}&B\bm{v}_{0}&\frac{B\bm{v}_{1}}{2}& \ldots&\frac{B\bm{v}_{r-1}}{r}&\frac{B\bm{v}_{r}}{r+1}\\ \bm{0}_{d}&0&0&0&\ldots&0&0\\ \bm{0}_{d}&1&0&0&\ldots&0&0\\ \bm{0}_{d}&0&2&0&\ldots&0&0\\ \vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\ \bm{0}_{d}&0&0&0&\ldots&r+1&0\end{bmatrix}\underbrace{\begin{bmatrix}\bm{x}(t )\\ 1\\ t\\ t^{2}\\ \vdots\\ t^{r+1}\end{bmatrix}}_{\bm{y}(t)}\,,\]

where \(\bm{0}_{d}\) denotes a \(d\) dimensional zero row vector. Obviously,

\[\bm{y}(0)=\left[\bm{x}_{0}^{T},1,0,0,\ldots,0\right]^{\top},\]

we denote it as \(\bm{y}_{0}\). Therefore, \(\bm{y}(t)\) follows a homogeneous linear ODE system that can be expressed as:

\[\dot{\bm{y}}(t) =F\bm{y}(t)\,,\] (10) \[\bm{y}(0) =\bm{y}_{0}\,,\]

where \(F\in\mathbb{R}^{(d+r+2)\times(d+r+2)}\). Worth noting that all state variables in the ODE system (10) are observable. Then according to Lemma 2.1, the identifiability of the dynamical system described by the ODE system (10) is contingent upon the linear independence of the vectors \(\{\bm{y}_{0},F\bm{y}_{0},F^{2}\bm{y}_{0},\ldots,F^{d+r+1}\bm{y}_{0}\}\). Specifically, the system is \((\bm{y}_{0},F)\)-identifiable if and only if this set of vectors is linearly independent, indicating that the matrix formed by these vectors, denoted by \(H\), has a rank of \(d+r+2\). In the following, we will elucidate that if and only assumption **A1** is satisfied, the rank of this matrix \(H\) equals \(d+r+2\).

Some calculations show that,

\[F^{k}\bm{y}_{0}=\begin{bmatrix}A^{k-1}(A\bm{x}_{0}+B\bm{z}_{0})+\sum_{j=0}^{k-2}j!A ^{k-2-j}B\bm{v}_{j}\\ 0\\ \vdots\\ 0\\ k!\\ 0\\ \vdots\\ 0\end{bmatrix}\quad\text{for }k=1,2,\ldots,r+1\,,\] (11)

where \(k!\) is the \((d+k+1)\)-th element.

And

\[F^{k}\bm{y}_{0}=\begin{bmatrix}A^{k-(r+2)}(A^{r+2}\bm{x}_{0}+A^{r+1}B\bm{z}_{0} +\sum_{j=0}^{r}j!A^{r-j}B\bm{v}_{j})\\ 0\\ \vdots\\ 0\end{bmatrix}\quad\text{for }k=r+2,\ldots,r+d+1\,.\] (12)

According to assumption **A1** in Theorem 3.1,

\[\bm{\beta}=A^{r+2}\bm{x}_{0}+A^{r+1}B\bm{z}_{0}+\sum_{j=0}^{r}j!A^{r-j}B\bm{v} _{j}\,,\]

therefore, \(F^{k}\bm{y}_{0}\) can also be expressed as

\[F^{k}\bm{y}_{0}=\begin{bmatrix}A^{k-(r+2)}\bm{\beta}\\ 0\\ \vdots\\ 0\end{bmatrix}\quad\text{for }k=r+2,\ldots,r+d+1\,.\] (13)

We denote the matrix

\[H :=\begin{bmatrix}\bm{y}_{0}&F\bm{y}_{0}&F^{2}\bm{y}_{0}&\ldots&F^ {r+1}\bm{y}_{0}&F^{r+2}\bm{y}_{0}&\ldots&F^{d+r+1}\bm{y}_{0}\end{bmatrix}\] \[:=\begin{bmatrix}H_{11}&H_{12}\\ H_{21}&H_{22}\end{bmatrix}\]

as a block matrix. Then, based on Equations (11) and (13), one obtains that

\[H_{11} =\begin{bmatrix}x_{0}&A\bm{x}_{0}+B\bm{z}_{0}&A^{2}\bm{x}_{0}+AB \bm{z}_{0}+B\bm{v}_{0}&\ldots&A^{r+1}\bm{x}_{0}+A^{r}B\bm{z}_{0}+\sum_{j=0}^{r -1}j!A^{r-1-j}B\bm{v}_{j}\end{bmatrix}\] \[\in\mathbb{R}^{d\times(r+2)}\,,\] \[H_{12} =\begin{bmatrix}\beta&A\beta&\ldots&A^{d-1}\beta\end{bmatrix}\] \[\in\mathbb{R}^{d\times d}\,,\] \[H_{21} =\begin{bmatrix}1&0&0&\ldots&0\\ 0&1&0&\ldots&0\\ 0&0&2!&\ldots&0\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ 0&0&0&\ldots&(r+1)!\end{bmatrix}\in\mathbb{R}^{(r+2)\times(r+2)}\,,\] \[H_{22} =\bm{0}_{(r+2)\times d}\in\mathbb{R}^{(r+2)\times d}\,.\]

Some calculations show that

\[\text{rank}(H)=\text{rank}(H_{12})+\text{rank}(H_{21})\,.\]

It is apparent that

\[\text{rank}(H_{21})=r+2\,.\]To achieve \(\text{rank}(H)=d+r+2\), the rank of \(H_{12}\) must be \(d\). The rank of \(H_{12}\) equals \(d\) if and only if the set of vectors \(\{\bm{\beta},A\bm{\beta},\dots,A^{d-1}\bm{\beta}\}\) is linearly independent, that is, assumption **A1** is satisfied.

Now that we have proved that the ODE system (10) is \((\bm{y}_{0},F)\)-identifiable if and only if assumption **A1** is satisfied. That is, under assumption **A1**, the trajectory \(\bm{y}(\cdot;\bm{y}_{0},F)\) uniquely determines both \(\bm{y}_{0}\) and matrix \(F\). Consequently, it also uniquely determines \((\bm{x}_{0},A,B\bm{z}_{0},B\bm{v}_{0},\dots,B\bm{v}_{r})\), thus establishing that the ODE system (2) is \(\bm{\theta}\)-identifiable if and only if assumption **A1** is satisfied. 

### Proof of Theorem 4.1

Proof.: Recall that the first derivative of \(\bm{x}(t)\) can be expressed as:

\[\dot{\bm{x}}(t) =A\bm{x}(t)+B\bm{z}(t)\] \[=A\bm{x}(t)+\sum_{k=0}^{p-1}\frac{BG^{k}\bm{z}_{0}}{k!}t^{k}\,.\]

Set

\[\bm{y}(t)=\begin{bmatrix}\bm{x}(t)\\ 1\\ t\\ t^{2}\\ \vdots\\ t^{p-1}\end{bmatrix}\,,\]

we see that \(\bm{y}(t)\in\mathbb{R}^{d+p}\), and the first derivative of \(\bm{y}(t)\) w.r.t. time \(t\) can be expressed as

\[\dot{\bm{y}}(t) =\begin{bmatrix}\dot{\bm{x}}(t)\\ 0\\ 1\\ 2t\\ \vdots\\ (p-1)t^{p-2}\end{bmatrix}\] \[=\underbrace{\begin{bmatrix}A&B\bm{z}_{0}&BG\bm{z}_{0}&\frac{BG^ {2}\bm{z}_{0}}{2!}&\dots&\frac{BG^{p-2}\bm{z}_{0}}{(p-2)!}&\frac{BG^{p-1}\bm{ z}_{0}}{(p-1)!}\\ \bm{0}_{d}&0&0&0&\dots&0&0\\ \bm{0}_{d}&1&0&0&\dots&0&0\\ \bm{0}_{d}&0&2&0&\dots&0&0\\ \vdots&\vdots&\vdots&\vdots&\ddots&\vdots&\vdots\\ \bm{0}_{d}&0&0&0&\dots&p-1&0\end{bmatrix}}_{\text{denoted as $F$}}\underbrace{\begin{bmatrix}\bm{x}(t)\\ 1\\ t\\ t^{2}\\ \vdots\\ t^{p-1}\end{bmatrix}}_{\bm{y}(t)}\,,\]

where \(\bm{0}_{d}\) denotes a \(d\) dimensional zero row vector. Obviously,

\[\bm{y}(0)=[\bm{x}_{0}^{T},1,0,0,\dots,0]^{\top}\,,\]

we denote it as \(\bm{y}_{0}\). Therefore, \(\bm{y}(t)\) follows a homogeneous linear ODE system that can be expressed as:

\[\begin{split}\dot{\bm{y}}(t)&=F\bm{y}(t)\,,\\ \bm{y}(0)&=\bm{y}_{0}\,,\end{split}\] (14)

where \(F\in\mathbb{R}^{(d+p)\times(d+p)}\). Worth noting that all state variables in the ODE system (14) are observable. Then according to Lemma 2.1, the identifiability of the dynamical system described by the ODE system (14) is contingent upon the linear independence of the vectors \(\{\bm{y}_{0},F\bm{y}_{0},F^{2}\bm{y}_{0},\dots,F^{d+p-1}\bm{y}_{0}\}\). Specifically, the system is \((\bm{y}_{0},F)\)-identifiable if and only if this set of vectors is linearly independent, indicating that the matrix formed by these vectors, denoted by \(H\), has a rank of \(d+p\). In the following, we will elucidate that if and only assumption **B1** is satisfied, the rank of this matrix \(H\) equals \(d+p\)Some calculations show that,

\[F^{k}\bm{y}_{0}=\begin{bmatrix}A^{k}\bm{x}_{0}+\sum_{j=0}^{k-1}A^{k-1-j}BG^{j} \bm{z}_{0}\\ 0\\ \vdots\\ 0\\ k!\\ 0\\ \vdots\\ 0\end{bmatrix}\quad\text{for}\;k=1,2,\ldots,p-1\,,\] (15)

where \(k!\) is the \((d+k+1)\)-th element.

And

\[F^{k}\bm{y}_{0}=\begin{bmatrix}A^{k-p}(A^{p}\bm{x}_{0}+\sum_{j=0}^{p-1}A^{p-1-j }BG^{j}\bm{z}_{0})\\ 0\\ \vdots\\ 0\end{bmatrix}\quad\text{for}\;k=p,p+1,\ldots,p+d-1\,.\] (16)

According to assumption **B1** in Theorem 3.1,

\[\bm{\gamma}=A^{p}\bm{x}_{0}+\sum_{j=0}^{p-1}A^{p-1-j}BG^{j}\bm{z}_{0}\,,\]

therefore, \(F^{k}\bm{y}_{0}\) can also be expressed as

\[F^{k}\bm{y}_{0}=\begin{bmatrix}A^{k-p}\bm{\gamma}\\ 0\\ \vdots\\ 0\end{bmatrix}\quad\text{for}\;k=p,p+1,\ldots,p+d-1\,.\] (17)

We denote the matrix

\[H: =\begin{bmatrix}\bm{y}_{0}&F\bm{y}_{0}&F^{2}\bm{y}_{0}&\ldots&F^ {p-1}\bm{y}_{0}&F^{p}\bm{y}_{0}&\ldots&F^{p+d-1}\bm{y}_{0}\end{bmatrix}\] \[: =\begin{bmatrix}H_{11}&H_{12}\\ H_{21}&H_{22}\end{bmatrix}\]

as a block matrix. Then, based on Equations (15) and (17), one obtains that

\[H_{11} =\begin{bmatrix}\bm{x}_{0}&A\bm{x}_{0}+B\bm{z}_{0}&A^{2}\bm{x}_{0 }+AB\bm{z}_{0}+BG\bm{z}_{0}&\ldots&A^{p-1}\bm{x}_{0}+\sum_{j=0}^{p-2}A^{p-2-j }BG^{j}\bm{z}_{0}\end{bmatrix}\] \[\in\mathbb{R}^{d\times p}\,,\] \[H_{12} =\begin{bmatrix}\gamma&A\gamma&\ldots&A^{d-1}\bm{\gamma}\end{bmatrix}\] \[\in\mathbb{R}^{d\times d}\,,\] \[H_{21} =\begin{bmatrix}1&0&0&\ldots&0\\ 0&1&0&\ldots&0\\ 0&0&2!&\ldots&0\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ 0&0&0&\ldots&(p-1)!\end{bmatrix}\in\mathbb{R}^{p\times p}\,,\] \[H_{22} =\bm{0}_{p\times d}\in\mathbb{R}^{p\times d}\,.\]

Some calculations show that

\[\text{rank}(H)=\text{rank}(H_{12})+\text{rank}(H_{21})\,.\]

It is apparent that

\[\text{rank}(H_{21})=p\,.\]To achieve \(\text{rank}(H)=d+p\), the rank of \(H_{12}\) must be \(d\). The rank of \(H_{12}\) equals \(d\) if and only if the set of vectors \(\{\bm{\gamma},A\bm{\gamma},\dots,A^{d-1}\bm{\gamma}\}\) is linearly independent, that is, assumption **B1** is satisfied.

Now that we have proved that the ODE system (14) is \((\bm{y}_{0},F)\)-identifiable if and only if assumption **B1** is satisfied. That is, under assumption **B1**, the trajectory \(\bm{y}(\cdot;\bm{y}_{0},F)\) uniquely determines both \(\bm{y}_{0}\) and the matrix \(F\). Consequently, it also uniquely determines \((\bm{x}_{0},A,B\bm{z}_{0},BG\bm{z}_{0},\dots,BG^{p-1}\bm{z}_{0})\), thus establishing that the ODE system (3) is \(\bm{\eta}\)-identifiable if and only if assumption **B1** is satisfied. 

### Proof of Theorem 4.2

Before providing the main proof, we first present two lemmas we will use for our proof.

**Lemma D.1**.: _[_34_, Theorem 3.4]_ _The ODE system (1) is \((\bm{x}_{0},A)\)-identifiable if and only if the trajectory \(\bm{x}(\cdot;\bm{x}_{0},A)\) is not confined to a proper subspace of \(\mathbb{R}^{d}\)._

**Lemma D.2**.: _[_34_, Lemma 6.1]_ _Trajectory \(\bm{x}(\cdot;\bm{x}_{0},A)\) is not confined to a proper subspace of \(\mathbb{R}^{d}\) if and only if there exists \(t_{1},t_{2},\dots,t_{d}\) such that \(\bm{x}_{1},\bm{x}_{2},\dots,\bm{x}_{d}\) are linearly independent._

Proof.: In the proof of Theorem 4.1, we demonstrated that the ODE system (3), under latent DAG assumption, can be transformed into a fully observable homogeneous linear ODE system (14). According to Lemma D.1, the ODE system (14) is \((\bm{y}_{0},F)\)-identifiable if and only if trajectory \(\bm{y}(\cdot;\bm{y}_{0},F)\) is not confined to a proper subspace of \(\mathbb{R}^{d+p}\). Furthermore, based on Lemma D.2, this condition holds if and only if there exists time points \(t_{1},t_{2},\dots,t_{d+p}\) such that the vectors \(\bm{y}_{1},\bm{y}_{2},\dots,\bm{y}_{d+p}\) are linearly independent (i.e., assumption **C1**). Therefore, if and only if assumption **C1** is satisfied, the trajectory \(\bm{y}(\cdot;\bm{y}_{0},F)\) is not confined to a proper subspace of \(\mathbb{R}^{d+p}\), ensuring that the ODE system (14) is \((\bm{y}_{0},F)\)-identifiable. Consequently, the ODE system (3) is \(\bm{\eta}\)-identifiable. 

### Proof of Theorem 4.3

Proof.: Under assumption **B2**, since each \(\bm{z}_{0}^{*i}\) satisfies assumption **B1**, Theorem 4.1 implies that the ODE system (3) is \(\bm{\eta}_{i}\)-identifiable for all \(i=1,\dots,p\). That is, one can identify

\[(\bm{x}_{0},A,B\bm{z}_{0}^{*i},BG\bm{z}_{0}^{*i},\dots,BG^{p-1}\bm{z}_{0}^{*i})\]

for all \(i=1,\dots,p\).

Next, we will prove that matrix \(B\) is identifiable under assumption **B3**.

Define the matrix

\[S:=\begin{bmatrix}B\bm{z}_{0}^{*1}&B\bm{z}_{0}^{*2}&\dots&B\bm{z}_{0}^{*p} \end{bmatrix}\,,\]

we know that \(S\in\mathbb{R}^{d\times p}\), and \(S\) is identifiable. The matrix \(S\) can also be expressed as:

\[S =B\begin{bmatrix}\bm{z}_{0}^{*1}&\bm{z}_{0}^{*2}&\dots&\bm{z}_{0}^ {*p}\end{bmatrix}\] \[: =BZ\,,\]

where under assumption **B3**, the matrix \(Z\) is invertible. Therefore,

\[B=SZ^{-1}\,.\]

Since \(Z\) is a known matrix, \(B\) is identifiable.

Similarly, we can prove that \(BG^{j}\) for \(j=1,\dots,p-1\) is also identifiable.

We now show that, under assumption **B4**, the matrix \(G\) is identifiable.

Define the matrix

\[W:=\begin{bmatrix}B\\ BG\\ \vdots\\ BG^{p-1}\end{bmatrix}\,,\]

we know that \(W\in\mathbb{R}^{dp\times p}\), and \(W\) is identifiable.

Since \(G\) is a \(p\times p\) nilpotent matrix, \(G^{p}=\mathbf{0}\), thus \(BG^{p}=\mathbf{0}\). If we define the matrix

\[V:=\begin{bmatrix}BG\\ BG^{2}\\ \vdots\\ BG^{p}\end{bmatrix}\,,\]

then \(V\in\mathbb{R}^{dp\times p}\), and \(V\) is identifiable. The matrix \(V\) can also be expressed as:

\[V=\begin{bmatrix}B\\ BG\\ \vdots\\ BG^{p-1}\end{bmatrix}G=WG\,.\] (18)

Under assumption **B4**, one can find \(p\) linearly independent rows in matrix \(W\). Denote the matrix composed of these \(p\) linearly independent rows as \(W_{p}\), which is invertible. Denote the matrix composed of the corresponding \(p\) rows of \(V\) as \(V_{p}\), we have

\[V_{p}=W_{p}G\,.\]

Since \(W_{p}\) is invertible, then

\[G=W_{p}^{-1}V_{p}\,.\]

Because both \(V_{p}\) and \(W_{p}\) are identifiable, \(G\) is also identifiable. 

### Proof of Theorem 4.4

Proof.: Under assumption **C2**, for each \(i\in\{1,\dots,p\}\), the corresponding observations satisfy assumption **C1**. Based on Theorem 4.2, the ODE system (3) is \(\boldsymbol{\eta}_{i}\)-identifiable for all \(i=1,\dots,p\). This implies that one can identify

\[(\boldsymbol{x}_{0},A,B\boldsymbol{z}_{0}^{*i},BG\boldsymbol{z}_{0}^{*i}, \dots,BG^{p-1}\boldsymbol{z}_{0}^{*i})\]

for all \(i=1,\dots,p\).

According to the proof of Theorem 4.3, under assumptions **B3** and **B4**, matrices \(B\) and \(G\) are also identifiable.

Identifiability conditions of the linear ODE system (2) with other _f(t)_

In this section, we provide identifiability conditions for the linear ODE system (2) with \(\bm{f}(t)=\bm{v}e^{t}\) and \(\bm{f}(t)=\bm{v}_{1}sin(t)+\bm{v}_{2}cos(t)\). For notational simplicity, we slightly abuse notation by using the same symbols as in Section 3.

### When _f(t)_ follows an exponential function of time \(\bm{t}\)

We define \(\bm{f}(t)\) in the ODE system (2) as:

\[\bm{f}(t)=\bm{v}e^{t}\,,\quad\bm{v}\in\mathbb{R}^{p}\,.\]

Simple calculations show that

\[\bm{z}(t)=\bm{v}e^{t}+\bm{z}_{0}-\bm{v}\,.\]

Thus,

\[\dot{\bm{x}}(t) =A\bm{x}(t)+B\bm{z}(t)\] \[=A\bm{x}(t)+B\bm{v}e^{t}+B\bm{z}_{0}-B\bm{v}\,.\] (19)

We denote the unknown parameters of the ODE system (2) with this \(\bm{f}(t)\) as \(\bm{\theta}\), specifically, \(\bm{\theta}:=(\bm{x}_{0},\bm{z}_{0},A,B,\bm{v})\). Let \([\bm{x}^{T}(t;\bm{\theta}),\bm{z}^{T}(t;\bm{\theta})]^{T}\) denote the solution of the ODE system (2). It is important to note that under our hidden variables setting, only \(\bm{x}(t;\bm{\theta})\) is observable. Based on Equation (19), we present the following identifiability definition.

**Definition E.1**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d \times d},B\in\mathbb{R}^{d\times p}\) and \(\bm{v}\in\mathbb{R}^{p}\), for all \(\bm{x}^{\prime}_{0}\in\mathbb{R}^{d}\), all \(\bm{z}^{\prime}_{0}\in\mathbb{R}^{p}\), all \(A^{\prime}\in\mathbb{R}^{d\times d}\), all \(B^{\prime}\in\mathbb{R}^{d\times p}\), and all \(\bm{v}^{\prime}\in\mathbb{R}^{p}\), we denote \(\bm{\theta}^{\prime}:=(\bm{x}^{\prime}_{0},\bm{z}^{\prime}_{0},A^{\prime},B^{ \prime},\bm{v}^{\prime})\), we say the ODE system (2) is \(\bm{\theta}\)-identifable: if \((\bm{x}_{0},A,B\bm{z}_{0},B\bm{v})\neq(\bm{x}^{\prime}_{0},A^{\prime},B^{ \prime}\bm{z}^{\prime}_{0},B^{\prime}\bm{v}^{\prime})\), it holds that \(\bm{x}(\cdot;\bm{\theta})\neq\bm{x}(\cdot;\bm{\theta}^{\prime})\)._

According to Definition E.1, if the ODE system (2) with an exponential \(\bm{f}(t)\) is \(\bm{\theta}\)-identifable, then the trajectory of the system can uniquely determine the values of \((\bm{x}_{0},A,B\bm{z}_{0},B\bm{v})\). This determination is sufficient to identify the causal relationships between observable variables \(\bm{x}\) as described by Equation (19). Consequently, one can safely intervene in the observable variables of the ODE system and make reliable causal inferences, despite the fact that matrix \(B\) cannot be identified under this definition.

**Theorem E.1**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d \times d},B\in\mathbb{R}^{d\times p}\), and \(\bm{v}\in\mathbb{R}^{p}\), the ODE system (2) is \(\bm{\theta}\)-identifiable if and only if assumption **D1** is satisfied._

* _the set of vectors_ \(\{\bm{y}_{0},F\bm{y}_{0},\ldots,F^{d+1}\bm{y}_{0}\}\) _is linearly independent, where_ \(\bm{y}_{0}=[\bm{x}^{T}_{0},1,1]^{T}\)_, and_ \[F=\begin{bmatrix}A&B\bm{v}&B\bm{z}_{0}-B\bm{v}\\ \bm{0}_{d}&1&0\\ \bm{0}_{d}&0&0\end{bmatrix}\,,\] \[\bm{0}_{d}\] _denotes a_ \[d\] _dimensional zero row vector._

The proof of Theorem E.1 is presented below. Condition **D1** is both sufficient and necessary, indicating, from a geometric perspective, that the vector \(\bm{y}_{0}\) is not contained in an \(F\)-invariant proper subspace of \(\mathbb{R}^{d+2}\).

Proof.: Set

\[\bm{y}(t)=\begin{bmatrix}\bm{x}(t)\\ e^{t}\\ 1\end{bmatrix}\,,\]

we see that \(\bm{y}(t)\in\mathbb{R}^{d+2}\), and the first derivative of \(\bm{y}(t)\) w.r.t. time \(t\) can be expressed as

\[\dot{\bm{y}}(t)=\begin{bmatrix}\dot{\bm{x}}(t)\\ e^{t}\\ 0\end{bmatrix}=\underbrace{\begin{bmatrix}A&B\bm{v}&B\bm{z}_{0}-B\bm{v}\\ \bm{0}_{d}&1&0\\ \bm{0}_{d}&0&0\end{bmatrix}}_{F}\underbrace{\begin{bmatrix}\bm{x}(t)\\ e^{t}\\ 1\end{bmatrix}}_{\bm{y}(t)}\,,\]where \(\mathbf{0}_{d}\) denotes a \(d\) dimensional zero row vector. Obviously,

\[\bm{y}(0)=[\bm{x}_{0}^{T},1,1]^{T}=\bm{y}_{0}\,.\]

Therefore, \(\bm{y}(t)\) follows a homogeneous linear ODE system that can be expressed as:

\[\dot{\bm{y}}(t) =F\bm{y}(t)\,,\] (20) \[\bm{y}(0) =\bm{y}_{0}\,,\]

where \(F\in\mathbb{R}^{(d+2)\times(d+2)}\). Worth noting that all state variables in the ODE system (20) are observable. Then according to Lemma 2.1, the system (20) is \((\bm{y}_{0},F)\)-identifiable if and only if condition **D1** stated in Theorem E.1 is satisfied. That is, under assumption **D1**, the trajectory \(\bm{y}(\cdot;\bm{y}_{0},F)\) uniquely determines both \(\bm{y}_{0}\) and matrix \(F\). Consequently, it also uniquely determines \((\bm{x}_{0},A,B\bm{z}_{0},B\bm{w})\), thus establishing that the ODE system (2) is \(\bm{\theta}\)-identifiable if and only if assumption **D1** is satisfied. 

### When _f(t)_ follows an trigonometric function of time \(t\)

We define \(\bm{f}(t)\) in the ODE system (2) as:

\[\bm{f}(t)=\bm{v}_{1}sin(t)+\bm{v}_{2}cos(t)\,,\quad\bm{v}_{1},\bm{v}_{2}\in \mathbb{R}^{p}\,.\]

Simple calculations show that

\[\bm{z}(t)=\bm{v}_{2}sin(t)-\bm{v}_{1}cos(t)+\bm{z}_{0}+\bm{v}_{1}\,.\]

Thus,

\[\begin{split}\dot{\bm{x}}(t)&=A\bm{x}(t)+B\bm{z}(t )\\ &=A\bm{x}(t)+B\bm{v}_{2}sin(t)-B\bm{v}_{1}cos(t)+B\bm{z}_{0}+B\bm{ v}_{1}\,.\end{split}\] (21)

We denote the unknown parameters of the ODE system (2) with this \(\bm{f}(t)\) as \(\bm{\theta}\), specifically, \(\bm{\theta}:=(\bm{x}_{0},\bm{z}_{0},A,B,\bm{v}_{1},\bm{v}_{2})\). Let \([\bm{x}^{T}(t;\bm{\theta}),\bm{z}^{T}(t;\bm{\theta})]^{T}\) denote the solution of the ODE system (2). It is important to note that under our hidden variables setting, only \(\bm{x}(t;\bm{\theta})\) is observable. Based on Equation (21), we present the following identifiability definition.

**Definition E.2**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d \times d},B\in\mathbb{R}^{d\times p}\) and \(\bm{v}_{1},\bm{v}_{2}\in\mathbb{R}^{p}\), for all \(\bm{x}^{\prime}_{0}\in\mathbb{R}^{p}\), all \(\bm{x}^{\prime}_{0}\in\mathbb{R}^{p}\), all \(A^{\prime}\in\mathbb{R}^{d\times d}\), all \(B^{\prime}\in\mathbb{R}^{d\times p}\), and all \(\bm{v}^{\prime}_{1},\bm{v}^{\prime}_{2}\in\mathbb{R}^{p}\), we denote \(\bm{\theta}^{\prime}:=(\bm{x}^{\prime}_{0},\bm{z}^{\prime}_{0},A^{\prime},B^{ \prime},\bm{v}^{\prime}_{1},\bm{v}^{\prime}_{2})\), we say the ODE system (2) is \(\bm{0}\)-identifiable: if \((\bm{x}_{0},A,B\bm{z}_{0},B\bm{v}_{1},B\bm{v}_{2})\neq(\bm{x}^{\prime}_{0},A^ {\prime},B^{\prime}z^{\prime}_{0},B^{\prime}\bm{v}^{\prime}_{1},B^{\prime}\bm{ v}^{\prime}_{2})\), it holds that \(\bm{x}(\cdot;\bm{\theta})\neq\bm{x}(\cdot;\bm{\theta}^{\prime})\)._

According to Definition E.2, if the ODE system (2) with a trigonometric \(\bm{f}(t)\) is \(\bm{\theta}\)-identifiable, then the trajectory of the system can uniquely determine the values of \((\bm{x}_{0},A,B\bm{z}_{0},B\bm{v}_{1},B\bm{v}_{2})\). This determination is sufficient to identify the causal relationships between observable variables \(\bm{x}\) as described by Equation (21). Consequently, one can safely intervene in the observable variables of the ODE system and make reliable causal inferences, despite the fact that matrix \(B\) cannot be identified under this definition.

**Theorem E.2**.: _For \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d \times d},B\in\mathbb{R}^{d\times p}\), and \(\bm{v}_{1},\bm{v}_{2}\in\mathbb{R}^{p}\), the ODE system (2) is \(\bm{\theta}\)-identifiable if and only if assumption **E1** is satisfied._

* _the set of vectors_ \(\{\bm{y}_{0},F\bm{y}_{0},\ldots,F^{d+2}\bm{y}_{0}\}\) _is linearly independent, where_ \(\bm{y}_{0}=[\bm{x}_{0}^{T},0,1,1]^{T}\)_, and_ \[F=\begin{bmatrix}A&B\bm{v}_{2}&-B\bm{v}_{1}&B\bm{z}_{0}+B\bm{v}_{1}\\ \bm{0}_{d}&0&1&0\\ \bm{0}_{d}&-1&0&0\\ \bm{0}_{d}&0&0&0\end{bmatrix}\,,\] \[\mathbf{0}_{d}\] _denotes a_ \[d\] _dimensional zero row vector._

The proof of Theorem E.2 is presented below. Condition **E1** is both sufficient and necessary, indicating, from a geometric perspective, that the vector \(\bm{y}_{0}\) is not contained in an \(F\)-invariant proper subspace of \(\mathbb{R}^{d+3}\).

Proof.: Set

\[\bm{y}(t)=\begin{bmatrix}\bm{x}(t)\\ sin(t)\\ cos(t)\\ 1\end{bmatrix}\,,\]

we see that \(\bm{y}(t)\in\mathbb{R}^{d+3}\), and the first derivative of \(\bm{y}(t)\) w.r.t. time \(t\) can be expressed as

\[\dot{\bm{y}}(t)=\begin{bmatrix}\dot{\bm{x}}(t)\\ cos(t)\\ -sin(t)\\ 0\end{bmatrix}=\underbrace{\begin{bmatrix}A&B\bm{v}_{2}&-B\bm{v}_{1}&B\bm{z} _{0}+B\bm{v}_{1}\\ \bm{0}_{d}&0&1&0\\ \bm{0}_{d}&-1&0&0\\ \bm{0}_{d}&0&0&0\end{bmatrix}}_{F}\underbrace{\begin{bmatrix}\bm{x}(t)\\ sin(t)\\ cos(t)\\ 1\end{bmatrix}}_{\bm{y}(t)}\,,\]

where \(\bm{0}_{d}\) denotes a \(d\) dimensional zero row vector. Obviously,

\[\bm{y}(0)=[\bm{x}_{0}^{T},0,1,1]^{T}=\bm{y}_{0}\,.\]

Therefore, \(\bm{y}(t)\) follows a homogeneous linear ODE system that can be expressed as:

\[\dot{\bm{y}}(t) =F\bm{y}(t)\,,\] (22) \[\bm{y}(0) =\bm{y}_{0}\,,\]

where \(F\in\mathbb{R}^{(d+3)\times(d+3)}\). Worth noting that all state variables in the ODE system (22) are observable. Then according to Lemma 2.1, the system (22) is \((\bm{y}_{0},F)\)-identifiable if and only if condition **E1** stated in Theorem E.2 is satisfied. That is, under assumption **E1**, the trajectory \(\bm{y}(\cdot;\bm{y}_{0},F)\) uniquely determines both \(\bm{y}_{0}\) and matrix \(F\). Consequently, it also uniquely determines \((\bm{x}_{0},A,B\bm{z}_{0},B\bm{v}_{1},B\bm{v}_{2})\), thus establishing that the ODE system (2) is \(\bm{\theta}\)-identifiable if and only if assumption **E1** is satisfied.

An alternative approach to identifying matrices \(B\) and \(G\) in the ODE system (3)

### Identifiability condition from _2p_ controllable whole trajectories

Recall that \(\bm{z}_{0}\) denotes the initial condition of the latent variables in the ODE system (3). We further specify the initial condition of the latent variable \(z_{j}\) as \(z_{0j}\) for \(j=1,\ldots,p\). Assume that it is possible to control the initial condition of each latent variable, \(z_{0j}\), independently. Specifically, for each experiment, researchers can intervene in the initial condition of a latent variable, denoted as \(z_{0j}^{*}\). The value of \(z_{0j}^{*}\) is treated as a given value. Under this intervention, the initial conditions of the latent variables are adjusted to \([z_{01},\ldots,z_{0j}^{*},\ldots,z_{0p}]^{T}\), which we denote as \(\tilde{\bm{z}}_{0j}\).

To identify matrices \(B\) and \(G\), it is necessary to have at least two intervened initial conditions for each latent variable, denoted as \(z_{0j}^{*1}\) and \(z_{0j}^{*2}\) for the latent variable \(z_{j}\). Consequently, the corresponding intervened initial conditions for all latent variables can be represented as \(\tilde{\bm{z}}_{0j}^{1}\) and \(\tilde{\bm{z}}_{0j}^{2}\). Under these conditions, we present the definition of the identifiability of the ODE system (3).

**Definition F.1**.: _Given \(z_{0j}^{*1},z_{0j}^{*2}\in\mathbb{R}\) for \(j=1,\ldots,p\), for \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d \times d},B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\), under the latent DAG assumption, for all \(\bm{x}_{0}^{\prime}\in\mathbb{R}^{d}\), all \(\bm{z}_{0}^{\prime}\in\mathbb{R}^{p}\), all \(A^{\prime}\in\mathbb{R}^{d\times d}\), all \(B^{\prime}\in\mathbb{R}^{d\times p}\), and all \(G^{\prime}\in\mathbb{R}^{p\times p}\), we denote \(\tilde{\bm{z}}_{0j}^{i}=[z_{01},\ldots,z_{0j}^{*i},\ldots,z_{0p}]^{T}\) and \((\tilde{\bm{z}}_{0j}^{\prime})^{i}=[z_{01}^{\prime},\ldots,z_{0j}^{*i},\ldots,z _{0p}^{\prime}]^{T}\), we further denote \(\bm{\eta}_{j}^{i}:=(\bm{x}_{0},\tilde{\bm{z}}_{0j}^{i},A,B,G)\) and \((\bm{\eta}_{j}^{\prime})^{i}:=(\bm{x}_{0}^{\prime},(\tilde{\bm{z}}_{0j}^{ \prime})^{i},A^{\prime},B^{\prime},G^{\prime})\) for \(i=1,2\), we say the ODE system (3) is \(\{\bm{\eta}_{j}^{1,2}\}_{1}^{p}\)-identifiable: if \((\bm{x}_{0},A,B,G)\neq(\bm{x}^{\prime},A^{\prime},B^{\prime},G^{\prime})\), it holds that \(\exists i\in\{1,2\}\) and \(j\in\{1,\ldots,p\}\) such that \(\bm{x}(\cdot;\bm{\eta}_{j}^{i})\neq\bm{x}(\cdot;(\bm{\eta}_{j}^{\prime})^{i})\)._

Definition F.1 establishes the identifiability of the ODE system (3) from \(2p\) whole trajectories \(\bm{x}(\cdot;\bm{\eta}_{j}^{i})\) with \(i=1,2\) and \(j=1,\ldots,p\). According to this definition, both matrices \(B\) and \(G\) are identifiable. Based on this definition, we present the identifiability condition.

**Theorem F.1**.: _Given \(z_{0j}^{*1},z_{0j}^{*2}\in\mathbb{R}\) with \(z_{0j}^{*1}\neq z_{0j}^{*2}\) for \(j=1,\ldots,p\), for \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d \times d},B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\), under the latent DAG assumption, the ODE system (3) is \(\{\bm{\eta}_{j}^{1,2}\}_{1}^{p}\)-identifiable if assumptions **B\({}_{5}\)** and **B\({}_{4}\)** are both satisfied._

* _each_ \(\tilde{\bm{z}}_{0j}^{i}\) _for_ \(i=1,2\) _and_ \(j=1,\ldots,p\)_, satisfies assumption_ _B1_. That is, if we set_ \(\bm{\gamma}_{j}^{i}=A^{p}\bm{x}_{0}+\sum_{k=0}^{p-1}A^{p-1-k}BG^{k}\tilde{\bm{z }}_{0j}^{i}\)_, then the set of vectors_ \(\{\bm{\gamma}_{j}^{i},A\bm{\gamma}_{j}^{i},\ldots,A^{d-1}\bm{\gamma}_{j}^{i}\}\) _is linearly independent for all_ \(i=1,2\) _and_ \(j=1,\ldots,p\)_._

The proof of Theorem F.1 is presented below. Assumption **B5** ensures that the ODE system (3) is \(\bm{\eta}_{j}^{i}\)-identifiable for all \(i=1,2\) and \(j=1,\ldots,p\). Consequently, \((\bm{x}_{0},A,B\tilde{\bm{z}}_{0j}^{i},BG\tilde{\bm{z}}_{0j}^{i},\ldots,BG^{p- 1}\tilde{\bm{z}}_{0j}^{i})\) for all \(i=1,2\) and \(j=1,\ldots,p\) is identifiable. Through straightforward calculations, the identifiability of matrix \(B\) is established. To identify matrix \(G\), assumption **B4** is required.

The assumption that the initial condition of each latent variable \(z_{i}\) can be controlled independently is inspired by the "genetic single-node intervention" proposed in [32], where interventions can be made at each latent node individually. This assumption is relatively more relaxed compared to controlling the initial condition of all latent variables \(\bm{z}\) simultaneously, as discussed in Subsection 4.3. However, this method requires \(p\) more trajectories, totalling \(2p\) trajectories, to identify matrices \(B\) and \(G\).

Proof.: Under assumption **B5**, since each \(\tilde{\bm{z}}_{0j}^{i}\) satisfies assumption **B1**. By Theorem 4.1, the ODE system (3) is \(\bm{\eta}_{j}^{i}\)-identifiable for all \(i=1,2\) and \(j=1,\ldots,p\). Consequently,

\[(\bm{x}_{0},A,B\tilde{\bm{z}}_{0j}^{i},BG\tilde{\bm{z}}_{0j}^{i},\ldots,BG^{p- 1}\tilde{\bm{z}}_{0j}^{i})\]

for all \(i=1,2\) and \(j=1,\ldots,p\) is identifiable.

We express \(B\tilde{\bm{z}}_{0j}^{i}\) as

\[B\tilde{\bm{z}}_{0j}^{i}=\begin{bmatrix}B_{11}&\ldots&B_{1j}&\ldots&B_{1p}\\ \vdots&\ddots&\vdots&\ddots&\vdots\\ B_{d1}&\ldots&B_{dj}&\ldots&B_{dp}\end{bmatrix}\begin{bmatrix}z_{01}\\ \vdots\\ z_{0j}^{*i}\\ \vdots\\ z_{0p}\end{bmatrix}\,.\]

We know that \(B\tilde{\bm{z}}_{0j}^{i}\in\mathbb{R}^{d}\) is identifiable for \(i=1,2\). Thus, the first entry of \(B\tilde{\bm{z}}_{0j}^{i}\), denoted as \((B\tilde{\bm{z}}_{0j}^{i})_{1}\), is identifiable and can be expressed as

\[(B\tilde{\bm{z}}_{0j}^{1})_{1}=B_{11}z_{01}+\ldots+B_{1j}z_{0j}^{*1}+\ldots+B_ {1p}z_{0p}\,\]

\[(B\tilde{\bm{z}}_{0j}^{2})_{1}=B_{11}z_{01}+\ldots+B_{1j}z_{0j}^{*2}+\ldots+B_ {1p}z_{0p}\,.\]

Since \(z_{0j}^{*1}\) and \(z_{0j}^{*2}\) are given values, we can easily calculate the value of \(B_{1j}\). Similarly, one can calculate the values of \(B_{mj}\) for all \(m=1,\ldots,d\) and \(j=1,\ldots,p\), thereby establishing the identifiability of matrix \(B\).

In a similar manner, matrices \(BG,BG^{2},\ldots,BG^{p-1}\) are also identifiable. Then, according to the proof D.4 of Theorem 4.3, the matrix \(G\) is identifiable under assumption **B4**. 

### Identifiability condition from discrete observations sampled from _2p_ controllable trajectories

We further extend the identifiability analysis of the ODE system (3) to cases where only discrete observations from \(2p\) controllable trajectories are available.

**Definition F.2**.: _Given \(z_{0j}^{*1},z_{0j}^{*2}\in\mathbb{R}\) for \(j=1,\ldots,p\), for \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d\times d },B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\). For any \(n\geqslant 1\), let \(t_{k},k=1,\ldots,n\) be any \(n\) time points and \(\bm{x}_{jk}^{i}:=\bm{x}(t_{k};\bm{\eta}_{j}^{i})\) be the error-free observation of the trajectory \(\bm{x}(\cdot;\bm{\eta}_{j}^{i})\) at time \(t_{k}\). Under the latent DAG assumption, we say the ODE system (3) is \(\{\bm{\eta}_{j}^{1,2}\}_{1}\)-identifiable from \(\bm{x}_{j1}^{i},\ldots,\bm{x}_{jn}^{i}\), \(i=1,2\) and \(j=1,\ldots,p\), if for all \(\bm{x}_{0}^{\prime}\in\mathbb{R}^{d}\), all \(\bm{z}_{0}^{\prime}\in\mathbb{R}^{p}\), all \(A^{\prime}\in\mathbb{R}^{d\times d}\), all \(B^{\prime}\in\mathbb{R}^{d\times p}\), and all \(G^{\prime}\in\mathbb{R}^{p\times p}\) with \((\bm{x}_{0},A,B,G)\neq(\bm{x}_{0}^{\prime},A^{\prime},B^{\prime},G^{\prime})\), it holds that \(\exists i\in\{1,2\},j\in\{1,\ldots,p\}\) and \(k\in\{1,\ldots,n\}\) such that \(\bm{x}(t_{k};\bm{\eta}_{j}^{i})\neq\bm{x}(t_{k};(\bm{\eta}_{j}^{\prime})^{i})\)._

Based on Definition F.2 we present the identifiability condition.

**Theorem F.2**.: _Given \(z_{0j}^{*1},z_{0j}^{*2}\in\mathbb{R}\) with \(z_{0j}^{*1}\neq z_{0j}^{*2}\) for \(j=1,\ldots,p\), for \(\bm{x}_{0}\in\mathbb{R}^{d},\bm{z}_{0}\in\mathbb{R}^{p},A\in\mathbb{R}^{d \times d},B\in\mathbb{R}^{d\times p}\) and \(G\in\mathbb{R}^{p\times p}\). We define new observation \(\bm{y}_{jk}^{i}:=[(\bm{x}_{jk}^{i})^{T},1,t_{k},t_{k}^{2},\ldots,t_{k}^{p-1}]^{T }\in\mathbb{R}^{d+p}\), for \(i=1,2,j=1,\ldots,p\) and \(k=1,\ldots,n\). Under the latent DAG assumption, the ODE system (3) is \(\{\bm{\eta}_{j}^{1,2}\}_{1}^{p}\)-identifiable from discrete observations \(\bm{x}_{j1}^{i},\ldots,\bm{x}_{jn}^{i}\), \(i=1,2\) and \(j=1,\ldots,p\), if assumptions **C3** and **B4** are both satisfied._

* _for each_ \(i\in\{1,2\},j\in\{1,\ldots,p\}\) _there exists_ \((d+p)\)__\(\bm{y}_{jk}^{i}\)_'s with indexes denoting as_ \(\{k_{j1}^{i},k_{j2}^{i},\ldots,k_{j,d+p}^{i}\}\subseteq\{1,2,\ldots,n\}\)_, such that the set of vectors_ \(\{\bm{y}_{jk_{j1}^{i}}^{i},\bm{y}_{jk_{j2}^{i}}^{i},\ldots,\bm{y}_{jk_{j,d+p}^{ i}}^{i}\}\) _is linearly independent._

The proof of Theorem F.2 is presented below. Assumption **C3** ensures that the ODE system (3) is \(\bm{\eta}_{j}^{i}\)-identifiable from discrete observations \(\bm{x}_{j1}^{i},\ldots,\bm{x}_{jn}^{i}\) for all \(i=1,2\) and \(j=1,\ldots,p\). As in Subsection F.1, matrix \(B\) is identifiable. Then, under assumption **B4**, matrix \(G\) is also identifiable.

Proof.: Under assumption **C3**, for each \(i\in\{1,2\}\) and \(j\in\{1,\ldots,p\}\), the corresponding observations satisfy assumption **C1**. Based on Theorem 4.2, the ODE system (3) is \(\bm{\eta}_{j}^{i}\)-identifiable for all \(i=1,2\) and \(j=1,\ldots,p\). Consequently,

\[(\bm{x}_{0},A,B\tilde{\bm{z}}_{0j}^{i},BG\tilde{\bm{z}}_{0j}^{i},\ldots,BG^{p-1 }\tilde{\bm{z}}_{0j}^{i})\]

for all \(i=1,2\) and \(j=1,\ldots,p\) is identifiable.

Following the proof of Theorem F.1, matrix \(B\) is identifiable. Under assumption **B4**, matrix \(G\) is also identifiable.

More simulation results

In this section, we present additional simulation results for higher-dimensional cases, along with simulations that incorporate a variety of ground-truth parameter configurations.

### Higher dimensional cases

In this subsection, for the \(\bm{\eta}\)-(un)identifiable cases of the ODE system (3), we provide a case with \(d=5\) and \(p=5\). The true underlying parameters of the systems are provided below. Initial parameter values are set to the true parameters plus a random value drawn from a uniform distribution \(U(-0.14,0.14)\) for each replication. To ensure reliability in the estimation results, we perform 50 independent random replications for each configuration, reporting the mean and variance of the squared error in Table 5.

\[A=\begin{bmatrix}2&-2&1&1&1\\ -1&1&0&2&-2\\ -2&2&0&-1&-2\\ -1&-1&-2&-1&2\\ 1&-2&1&-2&0\end{bmatrix}\,,\,\,\,B=\begin{bmatrix}1&-2&-1&1&1\\ 1&-2&-1&-1&-1\\ -2&0&2&1&1\\ 0&2&0&-2&-2\\ 2&-2&2&-1&2\end{bmatrix}\,,\] \[G=\begin{bmatrix}0&0&0&-2&-1\\ 0&0&-1&1&1\\ 0&0&0&1&2\\ 0&0&0&0&2\\ 0&0&0&0&0\end{bmatrix}\,,\,\,\,A^{\prime}=\bm{I}_{5}\,,\,\,\,\bm{x}_{0}= \begin{bmatrix}2\\ -2\\ 2\\ 1\\ 0\end{bmatrix}\,,\,\,\,\,\bm{z}_{0}=\begin{bmatrix}-2\\ -1\\ -1\\ -2\\ \end{bmatrix}\,,\] \[\bm{\eta}\text{-identifiable: }\bm{\eta}=(\bm{x}_{0},\bm{z}_{0},A,B,G), \,\text{unidentifiable: }\bm{\eta}=(\bm{x}_{0},\bm{z}_{0},A^{\prime},B,G)\,.\]

For \(\{\bm{\eta}_{i}\}_{1}^{p}\)-(un)identifiable cases of the ODE system (3), we consider a case with \(d=10\) and \(p=5\). To accelerate estimation, sparsity is introduced in the parameter matrices by randomly setting 70, 35, and 20 entries in matrices \(A\), \(B\) and \(G\), respectively, as zero. The true underlying parameters of the systems are provided below. Initial parameter values are set to the true parameters plus a random value drawn from a uniform distribution \(U(-0.1,0.1)\) for each replication. To ensure reliability in the estimation results, we perform 50 independent random replications for each configuration,

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & \(\bm{n}\) & \(A\) & \(B\bm{z}_{0}\) & \(BG\bm{z}_{0}\) & \(BG^{2}\bm{z}_{0}\) & \(BG^{3}\bm{z}_{0}\) & \(BG^{4}\bm{z}_{0}\) \\ \hline \multirow{4}{*}{**DDE**} & \multirow{4}{*}{100} & 0.0148 & 0.3911 & 0.9624 & 0.7316 & 0.1037 & 0.0096 \\  & & (\(\pm\)0.0006) & (\(\pm\)0.5989) & (\(\pm\)3.9249) & (\(\pm\)1.8971) & (\(\pm\)0.0374) & (\(\pm\)0.0003) \\ \cline{1-1}  & & & 0.0059 & 0.1529 & 0.1726 & 0.2447 & 0.0212 & 0.0012 \\ \cline{1-1}  & & & (\(\pm\)4.01E-05) & (\(\pm\)0.0277) & (\(\pm\)0.0541) & (\(\pm\)0.0748) & (\(\pm\)0.0007) & (\(\pm\)1.10E-05) \\ \cline{1-1}  & & & 0.0053 & 0.1394 & 0.1241 & 0.2119 & 0.0164 & 0.0004 \\ \cline{1-1}  & & & (\(\pm\)2.92E-05) & (\(\pm\)0.0200) & (\(\pm\)0.0251) & (\(\pm\)0.0479) & (\(\pm\)0.0004) & (\(\pm\)6.00E-07) \\ \hline \multirow{4}{*}{**DDE**} & \multirow{4}{*}{100} & 0.0853 & 1.0067 & 3.7422 & 2.7696 & 0.9229 & 0.0508 \\  & & (\(\pm\)0.0075) & (\(\pm\)1.3518) & (\(\pm\)55.8402) & (\(\pm\)24.5043) & (\(\pm\)2.7959) & (\(\pm\)0.0111) \\ \cline{1-1}  & & & 0.0357 & 0.4091 & 1.0428 & 0.9782 & 0.3871 & 0.0256 \\ \cline{1-1}  & & & (\(\pm\)0.0019) & (\(\pm\)0.3812) & (\(\pm\)2.1792) & (\(\pm\)5.3654) & (\(\pm\)0.6747) & (\(\pm\)0.0032) \\ \cline{1-1} \cline{2-7}  & & & 0.0332 & 0.3286 & 0.7123 & 0.9782 & 0.5487 & 0.0393 \\ \cline{1-1} \cline{2-7}  & & & (\(\pm\)0.0017) & (\(\pm\)0.1824) & (\(\pm\)1.8836) & (\(\pm\)2.3163) & (\(\pm\)0.9240) & (\(\pm\)0.0047) \\ \hline \hline \end{tabular}
\end{table}
Table 5: MSEs of the \(\bm{\eta}\)-(un)identifiable cases of the ODE (3) with \(d=5,p=5\)reporting the mean and variance of the squared error in Table 6.

\[A=\begin{bmatrix}0&0&-2&-1&1&2&0&-2&-1&0\\ 0&0&0&0&0&2&0&-2&2&0\\ 0&0&0&0&0&2&0&1&1&0\\ 0&0&0&-1&0&0&1&0&-2&0\\ 2&0&0&-1&0&-2&0&0&-1&1\\ 2&0&0&0&0&0&0&2&0&-2\\ 0&2&0&0&0&0&0&0&0\\ -2&-1&0&0&0&0&0&0&0&0\\ 0&0&0&-2&0&0&0&0&0&-2\\ 0&0&0&0&-1&0&0&0&0&-1\end{bmatrix}\,,\,\,\,B=\begin{bmatrix}-1&0&0&0&2\\ 0&-1&0&2&0\\ 0&-1&0&0&0\\ 0&0&0&1&1\\ 0&0&0&0&0\\ 0&0&0&0&0\\ 0&1&0&0&1\\ 0&0&-1&0&0\\ 1&0&0&0&-1\end{bmatrix}\,,\] \[G=\begin{bmatrix}0&1&-1&0&2\\ 0&0&2&0&0\\ 0&0&0&-1&0\\ 0&0&0&0&0\end{bmatrix}\,,\,\,\,A^{\prime}=\boldsymbol{I}_{10}\,,\] \[\boldsymbol{x}_{0}=[-2&0&0&-2&2&-1&1&0&1&1\end{bmatrix}^{\top}\,, \,\,\,\,\boldsymbol{z}_{0}^{*i}=\boldsymbol{e}_{i}\,,\text{for}\,\,i=1, \ldots,5\,.\] \[\{\boldsymbol{\eta}_{i}\}_{1}^{p}\text{-identifiable: }\boldsymbol{\eta}_{i}=(\boldsymbol{x}_{0},\boldsymbol{z}_{0}^{*i},A,B,G),\, \text{unidentifiable: }\boldsymbol{\eta}_{i}=(\boldsymbol{x}_{0},\boldsymbol{z}_{0}^{*i},A^{ \prime},B,G)\,.\]

Tables 5 and 6 present results similar to those in Tables 1 and 2, providing strong empirical support for the validity of our proposed identifiability conditions.

### Various true parameters

To further support our proposed identifiability conditions, we conduct additional simulations incorporating a variety of ground-truth parameter configurations, rather than a fixed underlying parameter set. Specifically, for each simulation run, a unique ground-truth parameter configuration was generated using different random seeds, and we subsequently reported the mean and variance of the squared error across all results. For the low-dimensional \(\boldsymbol{\eta}\) and \(\{\boldsymbol{\eta}_{i}\}_{1}^{p}\) (un)identifiable cases, we perform 100 replications, while for the higher-dimensional cases, we perform 50 replications. Additionally, in the \(\{\boldsymbol{\eta}_{i}\}_{1}^{p}\)-(un)identifiable cases, we initialize the parameter values as the true parameters plus a random value drawn from \(U(-0.1,0.1)\) for the \(d=3,p=3\) case and from \(U(-0.05,0.05)\) for the \(d=10,p=5\) cases. For the \(\boldsymbol{\eta}\)-(un)identifiable cases, the initialization settings are the same as those used in the fixed-parameter configurations.

The simulation results are presented in Tables 7, 8, 9, and 10. Across all these tables, parameter estimates in the identifiable cases are notably more accurate than in the unidentifiable cases, providing strong empirical support for the validity of our proposed identifiability conditions.

It is noteworthy, however, that even in theoretically identifiable cases, certain scenarios emerge where parameter identification is challenging in practice; we refer to these as hard estimate cases. In these instances, estimates may deviate significantly from satisfactory values, similar to challenges encountered in fully observable ODE systems (1) as discussed in [28]. Consequently, for identifiable cases

[MISSING_PAGE_EMPTY:31]

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{\(\boldsymbol{n}\)} & \multicolumn{3}{c}{**Identifiable**} & \multicolumn{3}{c}{**Unidentifiable**} \\ \cline{2-7}  & \(A\) & \(B\) & \(G\) & \(A\) & \(B\) & \(G\) \\ \hline \multirow{2}{*}{10} & 0.0044 & 0.0350 & 0.0287 & 0.6266 & 0.1310 & 0.0054 \\  & (\(\pm\)0.0001) & (\(\pm\)0.0098) & (\(\pm\)0.0053) & (\(\pm\)0.1524) & (\(\pm\)0.0269) & (\(\pm\)0.0004) \\ \multirow{2}{*}{30} & 0.0067 & 0.1258 & 0.0315 & 0.5833 & 0.1058 & 0.0021 \\  & (\(\pm\)0.0005) & (\(\pm\)0.5097) & (\(\pm\)0.0104) & (\(\pm\)0.2085) & (\(\pm\)0.0114) & (\(\pm\)8.79E-05) \\ \multirow{2}{*}{50} & 0.0033 & 0.0323 & 0.0354 & 0.5193 & 0.1108 & 0.0021 \\  & (\(\pm\)5.66E-05) & (\(\pm\)0.0103) & (\(\pm\)0.0084) & (\(\pm\)0.0982) & (\(\pm\)0.0146) & (\(\pm\)9.02E-05) \\ \hline \hline \end{tabular}
\end{table}
Table 10: MSEs of the \(\{\boldsymbol{\eta}_{i}\}_{1}^{p}\)-(un)identifiable cases of the ODE (3) with \(d=10,p=5\) - with various true parameters

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the contributions and scope of our paper? Answer: [Yes] Justification: The main claims presented in our abstract and introduction accurately reflect our paper's contributions and scope. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The limitations of our work are discussed in Section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide the full set of assumptions and a complete proof for each theoretical result presented in our paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We provide all experimental details in Section 5 and include the code in the supplemental material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide our code in the supplemental material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all experimental details in Section 5, and it is noteworthy that our experiments do not require any training phase. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We conduct 100 independent random replications for each configuration and report the mean and variance of the squared error. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: Since our experiments solely consist of simulations designed to validate our theoretical findings, the computational resources employed are not a consideration for our research objectives. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]. Justification: We claim that this work does not present any foreseeable positive or negative social impact. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: This paper does not require safeguards. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA]. Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA]. Justification: We did not use crowdsourcing or conduct research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA]. Justification: We did not use crowdsourcing or conduct research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.