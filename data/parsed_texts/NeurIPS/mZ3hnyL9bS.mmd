# Towards the Difficulty for a Deep Neural Network to Learn Concepts of Different Complexities

 Dongrui Liu\({}^{a}\)   Huiqi Deng\({}^{a}\)   Xu Cheng\({}^{a}\)   Qihan Ren\({}^{a}\)   Kangrui Wang\({}^{b}\)

**Quanshi Zhang\({}^{a}\)**

Shanghai Jiao Tong University  \({}^{b}\)University of Chicago

Equal contributionThis research is done under the supervision of Dr. Quanshi Zhang. He is with the Department of Computer Science and Engineering, the John Hopcroft Center at the Shanghai Jiao Tong University, China. Correspondence to: Quanshi Zhang <zqs1022@sjtu.edu.cn>.

###### Abstract

This paper theoretically explains the intuition that simple concepts are more likely to be learned by deep neural networks (DNNs) than complex concepts. In fact, recent studies have observed [45, 27] and proved [47] the emergence of interactive concepts in a DNN, _i.e._, it is proven that a DNN usually only encodes a small number of interactive concepts, and can be considered to use their interaction effects to compute the inference score. Each interactive concept is encoded by the DNN to represent the collaboration between a set of input variables. Therefore, in this study, we aim to theoretically explain that interactive concepts involving more input variables (_i.e._, more complex concepts) are more difficult to learn. Our finding clarifies the exact conceptual complexity that boosts the learning difficulty.

## 1 Introduction

Deep neural networks (DNNs) have exhibited superior performance in various tasks, but the reason for their superior performance remains an open problem. To this end, many attempts have been made to explain the representation capacity of DNNs from different perspectives. For example, Montufar et al. [35] used the number of linear response regions in a DNN to evaluate the expressive power of the DNN. Dinh et al. [13] and Petzka et al. [39] used the flatness of loss functions at minima to explain the generalization power.

In this paper, we explore a fundamental yet not well-formulated problem in terms of the representation capacity of DNNs, _i.e._, what types of concepts are easier to be learned by DNNs. However, the core challenge of this problem is that researchers have not reached a consensus on how to define a concept encoded by DNNs. Therefore, previous findings [5, 28, 34] that _DNNs easily learn simple concepts_ remain intuition or empirical observations, without a clear theoretical formulation and explanation.

Fortunately, Ren et al. [45], Li and Zhang [27] have observed and Ren et al. [47] have, for the first time, mathematically proven that the emergence of interactive concepts is a common phenomenon shared by different DNNs. _I.e._, **it is proved that under some common conditions, a well-trained DNN will encode just a small number of interactive concepts for inference.** Specifically, the interactive concept encoded by a DNN is defined as a Harsanyi interaction [18] in game theory, which represents an AND relationship between input variables in a specific set \(S\). For example, as Figure 1(a) shows, the DNN encodes the co-appearance relationship between input variables (image patches) \(x_{1}\), \(x_{2}\), and \(x_{3}\) to form the _mouth concept_\(S=\{x_{1},x_{2},x_{3}\}\). Only when all three patches are all present, the mouth concept is triggered, and adds a numerical effect \(I(S)\) to the inference score \(y\). The masking of any patches (_e.g._, \(x_{1}\)) will break this AND/co-appearance relationship of the mouthconcept, and removes the numerical effect \(I(S)\), _i.e._, making \(I(S)=0\). More crucially, it proved in [47] that **the network output \(y\) on each input sample can be represented as the sum of numerical effects of such a few interactive concepts**, \(y=\sum_{S\in\text{CoucepSet}}I(S)\).

The mathematical proof in [47] makes the interactive concept the **first concept whose emergence in DNNs is certificated**. Furthermore, Li and Zhang [27] have discovered that the interactive concepts **have considerable transferability over different samples and strong discrimination power for classification.** In this way, we can roughly consider such interactive concepts a rigorous definition of concepts encoded by DNNs.

**Explaining the difficulty of learning complex interactive concepts.** Therefore, in this study, given the above interactive concepts, we aim to further explore the learning difficulty of these concepts, and explain that a DNN is more likely to encode simple interactive concepts. Here, we use the number of variables in an interactive concept \(S\) to represent the complexity of the concept, which is also termed _the order_ of the concept. Thus, low-order interactive concepts usually represent simple AND interactions among a few input variables.

Specifically, we find that low-order interactive concepts encoded by DNNs are generally more stable to inevitable noises in data. In this way, low-order interactive concepts usually exhibit consistently positive (or negative) effects on the inference score of different samples in the same category. In comparison, a high-order interactive concept is more likely to have significantly diverse effects on inference scores of different samples. This explains the reason why DNNs easily learn simple interactive concepts.

In addition, we discover that our research may provide a new perspective to explain previous findings/understandings of the conceptual complexity [5; 34; 63; 28]. Besides, the conceptual complexity can directly explain the adversarial robustness of a DNN. Thus, our study is of considerable value in explaining the representation capacity of a DNN.

**Interactive concepts vs. cognitive concepts.** Although the Harsanyi interactive concepts visualized by [27] seem partially aligned with cognitive concepts to some extent, we do not think such interactive concepts fit humans' cognition [24; 53]. For example, the recognition of a _big ball concept_ and that of a _small ball concept_ have almost the same cognitive complexity for humans. In comparison, it is more difficult for the DNN to recognize a large ball, because it has to use a high-order interaction that contains a lot of pixels to examine whether all pixels within the ball have the same color. Thus, this study clarifies the exact formulation for the conceptual complexity on interactive concepts that boosts the learning difficulty, which is different from the cognitive difficulty.

## 2 Explainable AI (XAI) theories based on game-theoretic interactions

Explaining DNNs based on a system of game-theoretic interactions becomes an emerging direction in recent years. Specifically, such a system aims to tackle the following challenges in XAI.

Figure 1: (a) Interactive concepts encoded by a DNN. Each interactive concept \(S\) represents an AND relationship between input variables (image patches) in \(S\). Masking any input variables in \(S\) will deactivate the concept \(S\). (b) Extensive experiments illustrate the common concept-emergence phenomenon. Various DNNs all encode very sparse interactive concepts for inference. Only a small number of interactions have salient effects \(I(S)\) and are termed interactive concepts. Other interactions _w.r.t._ other \(S\subseteq N\) all have almost zero effect \(I(S)\approx 0\), thus being termed noisy patterns. For clarity, we sort the strength of interaction effects in a descending order.

\(\bullet\)_Defining, extracting, and quantifying interactive concepts encoded by DNNs._ Quantifying the interactions between different input variables [57; 58] is a new perspective to formulate the knowledge encoded by a DNN. Based on game theory, Zhang et al. [66; 68; 67] introduced multi-variate interaction and multi-order interaction to quantify the interactions encoded by the DNN. Ren et al. [45] empirically observed and Ren et al. [47] further proved that a DNN usually just encoded a small number of salient interactions. Ren et al. [44] defined the optimal baseline value for computing the Shapley value based on these interactions. Li and Zhang [27] found that salient interactions usually have high transferability over different samples and DNNs. These findings showed that these salient interactions could be viewed as interactive concepts encoded by a DNN.

\(\bullet\)_Using interactive concepts to explain/measure the representation power of DNNs._ The representation power of DNNs has been explained by game-theoretic multi-order interaction, including generalization power [67; 70], adversarial robustness [45; 70], and adversarial transferability [60]. Besides, Cheng et al. [8] used interactions to investigate how shapes and textures were encoded in DNNs. Cheng et al. [9] found that salient interactions often represented prototypical patterns encoded by DNNs. Deng et al. [11] derived that the DNN was less likely to encode mid-order interactions. In comparison, Ren et al. [46] proved that the Bayesian neural network was less likely to encode high-order interactions, thereby obtaining good adversarial robustness.

\(\bullet\)_Unifying the common underlying mechanism shared by previous empirical findings._ Interactions have superior representation power, and we find that many previous studies can be re-explained from the perspective of interactions. Deng et al. [12] proved that fourteen attribution methods could be reformulated and considered as a certain allocation of interaction effects. Besides, Zhang et al. [69] proved that reducing the interactions was the common underlying mechanism shared by twelve previous studies for improving adversarial transferability.

## 3 Explaining simple interactive concepts are easy to learn

### Preliminaries: sparse interactive concepts emerge in DNNs

It is generally believed that the learning of DNNs is a fitting problem between the ground-truth label and the model output, instead of explicitly learning specific concepts like graphical models. However, Ren et al. [45], Li and Zhang [27] have empirically observed and Ren et al. [47] have mathematically proven the counter-intuitive emergence of concepts in DNNs, _i.e._, **it is proven that under a set of common conditions, a DNN encodes just**_a small number of_ **interactive concepts for inference.**

Specifically, given a pre-trained DNN \(v\) and an input sample \(\bm{x}=[x_{1},\dots,x_{n}]\) with \(n\) variables indexed by \(N=\{1,\dots,n\}\), \(v(\bm{x})\in\mathbb{R}\) denotes a scalar output of the DNN3. Each interaction in [45; 27; 47] is defined as Harsanyi dividend (or Harsanyi interaction) [18] in game theory, which represents a collaboration (AND relationship) between input variables in a specific set \(S\) (\(S\subseteq N\)). For example, as Figure 1(a) shows, the co-appearance of image patches forms a mouth interaction \(S=\{x_{1},x_{2},x_{3}\}\). Only when these three patches are all present, the mouth interaction will be triggered, and make a certain interaction effect \(I(S)\) on the network output. The absence (masking) of any patches of \(x_{1}\), \(x_{2}\), and \(x_{3}\) will deactivate the mouth interaction and remove the interaction effect, _i.e.,_\(I(S)=0\). The interaction effect \(I(S|\bm{x})\) on the input sample \(\bm{x}\) is computed as follows.

Footnote 3: Here, \(v(\bm{x})\in\mathbb{R}\) can be implemented as either a scalar output of the DNN or a dimension of an output vector (_e.g._, the confidence score of classifying the sample \(\bm{x}\) to the ground-truth category \(v(\bm{x})=\log\frac{p(y=\text{\tiny{bmbm}}\|\bm{x})}{1-p(y=\text{\tiny{bm}} \|\bm{x})}\)).

\[I(S|\bm{x})=\sum\nolimits_{\bm{T}\subseteq S}(-1)^{|S|-|T|}\cdot v(\bm{x}_{T}),\] (1)

where \(\bm{x}_{T}\) denotes the masked input sample, when we mask input variables in \(N\setminus T\) and keep variables in \(T\) unchanged. Please see more properties of the Harsanyi dividend in the supplementary material.

Theoretically, there are \(2^{n}\) potential subsets \(S\) of input variables (\(S\in 2^{N}=\{S|S\subseteq N\}\)). The proven concept-emergence phenomenon refers to that **only a small number of subsets**\(S\in\Omega_{\text{salient}}\subseteq 2^{N}\) **make salient interaction effects**\(I(S|\bm{x})\) **on the network output, and can be considered as**_interactive concepts._ Interaction effects of all other subsets are close to zero (\(I(S|\bm{x})\approx 0\)), which can be considered as ignorable _noisy patterns_. Therefore, **the network output**\(v(\bm{x})\) **can be well approximated by interaction effects of a small number of interactive concepts,**_i.e._,

\[v(\bm{x})=\sum\nolimits_{S\subseteq N}I(S|\bm{x})\approx\sum\nolimits_{S\in \Omega_{\text{salient}}}I(S|\bm{x})\] (2)We also conduct experiments to illustrate the concept-emergence phenomenon on various DNNs, including multi-layer perceptrons (MLPs), long short-term memory (LSTM), AlexNet [23], ResNet [19], convolutional neural networks (CNNs), and PointNet [41] trained on different types of data, including tabular data (UCI dataset [6]), natural language data (CoLA [61] and SST-2 [56]), image data (MNIST [26] and CelebA [31]), and point cloud data (ShapeNet [65]). We follow experimental settings in [45; 27], and Figure 1(b) shows that interactive concepts encoded by a DNN are usually indeed sparse.

**Theorem 1**.: _Given an input sample \(\bm{x}\in\mathbb{R}^{n}\) with \(n\) variables, there are \(2^{n}\) different masked samples \(\bm{x}_{T}\) w.r.t. all potential subsets \(T\subseteq N\). Then, [45] has proven that_

\[\forall\,T\subseteq N,\;v(\bm{x}_{T})=\sum\nolimits_{S\subseteq T}I(S|\bm{x} )\approx\sum\nolimits_{S\in\Omega_{\text{shlent}}k\in S\subseteq T}I(S|\bm{x}).\] (3)

Theorem 1 means that we can use a small number of interactive concepts in \(\Omega_{\text{shlent}}\) to well approximate network outputs on anyone \(\bm{x}_{T}\) of the \(2^{n}\) masked samples.

**Trustworthiness of interactive concepts.** The mathematical proof in [47] makes \(I(S)\) become the first concepts whose emergence in DNNs is certificated. Equations (2) and (3) further guarantee that interactive concepts can faithfully explain the output of DNNs. In addition, Li and Zhang [27] have discovered that (i) interactive concepts are transferable across different samples; (ii) interactive concepts are discriminative, _i.e._, if a concept \(S\) has salient interaction effects \(I(S)\) on a set of samples, then the concept tends to push these samples to be classified towards the same category. In this way, we can roughly consider such interactive concepts a relatively rigorous definition of concepts.

**Complexity (order) of interactive concepts.** The complexity of an interactive concept \(S\) is defined as the number of input variables involved in \(S\), which is also termed _the order_ of the interactive concept, _i.e._, \(\text{complexity}(S)=\text{order}(S)=|S|\). Thus, low-order interactive concepts usually represent simple AND relationships among a few input variables. In comparison, high-order interactive concepts often refer to as relatively complex AND relationships among a large number of input variables.

### Simple interactive concepts in data are more stable and easier to learn

Unlike other interaction metrics [16; 32; 57], sparse interactive concepts are certificated to emerge in DNN, and can faithfully represent the inference score of DNNs. Therefore, in this study, we aim to further explore the learning difficulty of these certificated concepts. Specifically, we theoretically explain that a DNN is more likely to encode simple (_i.e._, low-order) interactive concepts.

In this subsection, we show that low-order interactive concepts encoded by DNNs are generally more stable to inevitable noises in data, compared to high-order interactive concepts. Specifically, we derive an approximate analytical solution to the variance (instability) of interactive concepts' effects _w.r.t._ data noise, and show that the variance (instability) increases along the order of concepts in an exponential manner.

**Theorem 2** (proven in the supplementary material).: _Given a neural network \(v\) and an arbitrary input sample \(\bm{x}^{\prime}\in\mathbb{R}^{n}\), the network output can be decomposed by using the Taylor expansion i.e., \(v(\bm{x}^{\prime})=\sum_{S\subseteq N}\sum_{\bm{x}\in\mathcal{O}_{S}}U_{S,\bm {\pi}}\cdot J(S,\bm{\pi}|\bm{x}^{\prime})\). In this way, according to Equation (1), the Harsanyi interaction effect \(I(S|\bm{x}^{\prime})\) on the sample \(\bm{x}^{\prime}\) can be reformulated as follows._

\[I(S|\bm{x}^{\prime})=\sum\nolimits_{\bm{\pi}\in Q_{S}}U_{S,\bm{\pi}}\cdot J(S,\bm{\pi}|\bm{x}^{\prime}).\] (4)

_Here, \(J(S,\bm{\pi}|\bm{x}^{\prime})\!\!=\!\prod_{i\in S}\left(\operatorname{sign}(x ^{\prime}_{i}-b_{i})\cdot\frac{x^{\prime}_{i}-b_{i}}{\tau}\right)^{\pi_{i}}\) denotes a Taylor expansion term of the degree \(\bm{\pi}\), where the degree \(\bm{\pi}\in Q_{S}=\{[\pi_{1},\ldots,\pi_{n}]|\forall i\in S,\pi_{i}\in\mathbb{ N}^{+};\forall i\not\in S,\pi_{i}=0\}\) and \(b_{i}\) is the baseline value to mask the input variable \(x_{i}\). In addition, \(U_{S,\bm{\pi}}\!\!=\!\!\frac{\tau^{m}}{\prod_{i=1}^{m}\pi_{i}^{i}}\frac{ \partial^{m}v(\bm{x}^{\prime}_{0})}{\partial x^{\pi_{1}}_{1}\cdots\partial x^{ \pi_{n}}_{n}}\cdot\prod_{i\in S}[\operatorname{sign}(x^{\prime}_{i}-b_{i})]^{ \pi_{i}}\), where \(\bm{x}^{\prime}_{0}\) denotes the sample whose input variables are all masked. \(m=\sum_{i=1}^{m}\pi_{i}\)._

Theorem 2 rewrites the Harsanyi interaction effect \(I(S|\bm{x})\) of each concept from a new perspective, to facilitate the analysis of the instability of interactive concepts. The derivation of Theorem 2 is inspired by [12; 70; 46], in which the Harsanyi interaction effect is re-written as the sum of the Taylor interaction effects. Therefore, in this study, we define a new baseline value \(b_{i}\) to further extend the finding in [12] and obtain Theorem 2, which significantly simplifies the further proof. Specifically, the new baseline value \(b_{i}\) is set as follows to represent the masked state of the \(i\)-th input variable. _i.e._,\(x_{i}\gets b_{i}\). Previous studies usually set \(b_{i}=\mu_{i}=\mathbb{E}_{\bm{x}}[x_{i}]\) as the average value over different samples to represent the masked state [4, 12]. However, we consider that pushing the input variable \(x_{i}\) to move a large distance \(\tau\in\mathbb{R}\) towards \(\mu_{i}\) has been significant enough to remove its information. To this end, we set \(b_{i}=x_{i}-\tau^{4}\), if \(x_{i}>\mu_{i}\); else, \(b_{i}=x_{i}+\tau^{4}\). The above new setting ensures comparable perturbation magnitudes over different dimensions.

Next, we analyze the variance (instability) of the interaction effect \(I(S|\bm{x}^{\prime}=\bm{x}+\bm{\epsilon})\) when we add a Gaussian perturbation \(\bm{\epsilon}\sim\mathcal{N}(\bm{0},\sigma^{2}\bm{I})\) to the input sample \(\bm{x}\). Here, the Gaussian perturbation \(\bm{\epsilon}\in\mathbb{R}^{n}\) is considered as a rough representation of inevitable variations in data, _e.g._, shape deformation and object rotation variations in image classification. However, such variations are quite difficult to formulate, so we use a Gaussian perturbation as a rough representation. Fortunately, we find that our reformulation of interactions in Theorem 2 enables us to directly apply the finding in [46] to prove the variance (instability) of the interaction effect \(I(S|\bm{x}^{\prime}=\bm{x}+\bm{\epsilon})\).

**Theorem 3**.: _Let us add a Gaussian perturbation \(\bm{\epsilon}\sim\mathcal{N}(\bm{0},\sigma^{2}\bm{I})\) to the input sample \(\bm{x}\). Let us first consider the case with the lowest degree \(\hat{\bm{\pi}}=[\hat{\pi}_{1},\dots,\hat{\pi}_{n}]\in Q_{S}\), satisfying that \(\forall i\in S,\hat{\pi}_{i}=1;\forall i\notin S,\hat{\pi}_{i}=0\). The mean and variance of \(J(S,\hat{\bm{\pi}}|\bm{x}+\bm{\epsilon})\) over the Gaussian perturbation \(\bm{\epsilon}\) are given as_

\[\mathbb{E}_{\bm{\epsilon}}[J(S,\hat{\bm{\pi}}|\bm{x}+\bm{\epsilon})]=1,\quad \mathrm{Var}_{\bm{\epsilon}}[J(S,\hat{\bm{\pi}}|\bm{x}+\bm{\epsilon})]=\left( 1+\left(\frac{\sigma}{\tau}\right)^{2}\right)^{|S|}-1.\] (5)

_Furthermore, for the more general case with an arbitrary degree \(\bm{\pi}\in Q_{S}=\{[\pi_{1},\cdots,\pi_{n}]|\forall i\in S,\pi_{i}\in\mathbb{ N}^{+};\forall i\notin S,\pi_{i}=0\}\), the mean and variance of \(J(S,\bm{\pi}|\bm{x}+\bm{\epsilon})\) are computed as_

\[\mathbb{E}_{\bm{\epsilon}}[J(S,\bm{\pi}|\bm{x}+\bm{\epsilon})]=\mathbb{E}_{ \bm{\epsilon}}[\prod\nolimits_{i\in S}(1+\frac{\epsilon_{i}}{\tau})^{\pi_{i}} ],\ \mathrm{Var}_{\bm{\epsilon}}[J(S,\bm{\pi}|\bm{x}+\bm{\epsilon})]=\mathrm{ Var}_{\bm{\epsilon}}[\prod\nolimits_{i\in S}(1+\frac{\epsilon_{i}}{\tau})^{\pi_{i}}].\] (6)

Theorem 3 shows the (variance) instability of interactive concepts of different complexities (orders) _w.r.t._ data variations. It indicates that the variance of \(J(S,\bm{\pi}|\bm{x}+\bm{\epsilon})\) roughly increases along with the order\((S)=|S|\) in an exponential manner. Similar conclusions are also introduced in [70]. Furthermore, according to Equation (4), the interaction effect \(I(S|\bm{x}+\bm{\epsilon})\) of the concept \(S\) can be represented as the weighted sum of \(J(S,\bm{\pi}|\bm{x}+\bm{\epsilon})\), and coefficients \(U_{S,\bm{\pi}}\)_w.r.t._ different orders \(s\) and degrees \(\bm{\pi}\) are usually chaotic. Hence, _we can roughly consider that the variance (instability) of the interaction effect \(I(S|\bm{x}+\bm{\epsilon})\) increases along with the order \(|S|\) exponentially, as well._ In other words, **compared to high-order concepts, low-order concepts are much more stable to slight input perturbations.**

**Experimental verification.** Here, we conduct experiments to verify whether the variance (instability) of interaction effects indeed increased along with the order in an approximately exponential manner. Specifically, to mimic variations in the data, we add Gaussian perturbations \(\bm{\epsilon}\sim\mathcal{N}(\bm{0},0.02^{2}\bm{I})\) to each training sample. Then, we compute the average mean \(E^{(s)}\)=\(\mathbb{E}_{\bm{x}\in\mathcal{X}}[\mathbb{E}_{S\subseteq\sum_{i}|S|=s}[ \mathbb{E}_{\bm{\epsilon}}[I(S|\bm{x}+\bm{\epsilon})]]]\) and the average variance \(V^{(s)}\)=\(\mathbb{E}_{\bm{x}\in\mathcal{X}}[\mathbb{E}_{S\subseteq\sum_{i}|S|=s}[ \mathrm{Var}_{\bm{\epsilon}}[I(S|\bm{x}+\bm{\epsilon})]]]\) of interaction effects of the \(s\)-th order concepts _w.r.t._ Gaussian perturbations. For verification, we calculate such metrics on DNNs trained for image classification and DNNs trained on tabular data. We train AlexNet [23], VGG-11 [55], ResNet-18/20 [19] on the CIFAR-10 dataset [22] and the Tiny-ImageNet dataset [25], respectively. We also train a five-layer MLP [45] on the UCI census dataset (namely _census dataset_) and the UCI TV news dataset (namely _TV news dataset_) [6], respectively. In addition, considering the computational cost of \(I(S|\bm{x}+\bm{\epsilon})\) in Equation (1) is intolerable if each pixel is considered as an input variable, we divide images into 8\(\times\)8 patches [45] to calculate \(I(S|\bm{x}+\bm{\epsilon})\). Please see the supplementary material for more implementation details.

Figure 2 shows that the variance (instability) \(V^{(s)}\) of interaction effects increases in an exponential manner along with the order \(s\) of the interactive concept. In addition, the relative stability \(E^{(s)}/\sqrt{V^{(s)}}\) of interaction effects decreases significantly along with the order \(s\). Such phenomenon successfully verifies findings in Theorem 3.

#### 3.2.1 Formulating the conceptual learning as a linear regression problem

Note that, Li and Zhang [27] have discovered that interactive concepts have high transferability over samples in the same category, _i.e._, most interactive concepts extracted from different samples may all belong to the same subset \(\Omega_{c}\subseteq\Omega_{\text{salient}}\). Therefore, we can consider the interactive concepts as common knowledge learned by the DNN, and we aim to analyze the difficulty of learning each interactive concept with different complexity.

To facilitate the analysis, we first simplify the conceptual learning as a linear regression problem. Specifically, we first rewrite the interaction effect of an interactive concept \(S\). Given an input sample \(\bm{x}\), according to Equation (4), the interaction effect of the concept \(S\) on the sample \(\bm{x}^{\prime}\) (obtained by applying some transformations on \(\bm{x}\)), \(I(S|\bm{x}^{\prime})\), can be rewritten as

\[I(S|\bm{x}^{\prime})=U_{S}\cdot C_{S}(\bm{x}^{\prime}),\] (7)

where the constant \(U_{S}=I(S|\bm{x})\) denotes the interaction effect on the sample \(\bm{x}\), and the function for the triggering state on the transformed sample \(\bm{x}^{\prime}\) is given as \(C_{S}(\bm{x}^{\prime})=\sum_{\bm{\pi}\in Q_{S}}U_{S,\bm{\pi}}J(S,\bm{\pi}|\bm{ x}^{\prime})/U_{S}\).

**Theorem 4**.: _Given an arbitrarily masked sample \(\bm{x}_{T}(T\subseteq N)\), the function \(C_{S}(\bm{x}_{T})\) defined above can be computed as the binary triggering state of the concept \(S\) in the sample \(\bm{x}_{T}\)._

\[\forall\;T\subseteq N,\;C_{S}(\bm{x}_{T})=\prod_{i\in S}A_{i}(\bm{x}_{T})= \mathbbm{1}(S\subseteq T),\] (8)

_where \(A_{i}(\bm{x}_{T})\in\{0,1\}\) denotes whether the variable \(x_{i}\) is present \(A_{i}(\bm{x}_{T})=1\) or being masked \(A_{i}(\bm{x}_{T})=0\) in the sample \(\bm{x}_{T}\)._

The function \(C_{S}(\bm{x}_{T})\) represents the triggering state of the concept \(S\) under an arbitrarily masking condition \(\forall\;T\subseteq N\). Only when all variables in \(S\) are present under the masking condition \(T\), the concept \(S\) is triggered \(C_{S}(\bm{x}_{T})=1\). If any of variables in \(S\) is masked, then the concept \(S\) will not be triggered \(C_{S}(\bm{x}_{T})=0\), yielding zero interaction effect \(I(S|\bm{x}_{T})=0\). Theorem 4 shows that given a masked sample \(\bm{x}_{T}\), the concept \(S\) is only triggered when the sample \(\bm{x}_{T}\) contains all variables in \(S\), _i.e._, \(T\supseteq S\).

Thus, inspired by [46], we can extend the conclusion to a continuous version that explains the output of the DNN as a **linear regression of very few salient interactive concepts** based on Equation (3).

\[v(\bm{x}^{\prime})=\sum\nolimits_{S\subseteq N}U_{S}\cdot C_{S}(\bm{x}^{ \prime})\approx\sum\nolimits_{S\in\Omega_{\text{alient}}}U_{S}\cdot C_{S}( \bm{x}^{\prime}),\] (9)

where the triggering state \(C_{S}(\bm{x}^{\prime})\) of each interactive concept \(S\) can be considered as an input dimension of the linear function, which reflects whether the input sample \(\bm{x}^{\prime}\) contains the concept \(S\).

Therefore, the absolute value of the weight coefficient \(U_{S}\) in Equation (9) can be viewed as _the strength of the DNN encoding the interactive concept \(S\)_. Considering the sparsity of interactive concepts discussed in Section 3.1, most interactive concepts have ignorable coefficients \(U_{S}\approx 0\), and not so many concepts \(S\) have large absolute value of \(|U_{S}|\). Thus, we can consider that the DNN only learns a small number of salient interactive concepts.

#### 3.2.2 Explaining the learning difficulty of concepts

Equation (9) in the previous subsection enables us to understand a DNN for the classification task as a pseudo-linear function. Then, if an input feature dimension has a stable value (_i.e._, the triggering state \(C_{S}\) of an interactive concept \(S\) stably being present/absent) across all samples in the same category, then we consider this feature dimension (_i.e.,_ the concept) is easy to learn. In comparison, when we extract the same concept from different samples in the same category, if this concept exhibits inconsistent interaction effects (_e.g._, such a concept does not consistently present or absent over different samples), then this feature dimension (_i.e.,_ the concept) is hard to learn.

Figure 2: The logarithm of the variance of interaction effects \(\log[V^{(s)}]\) and the stability of interaction effects (measured by \(E^{(s)}/\sqrt{V^{(s)}}\)). The variance \(V^{(s)}\) of interaction effects increases along with the order of interactive concepts exponentially. The stability of effects decreases along with the order.

To analyze the consistency/stability of the concept _w.r.t_ data variations, we use small perturbations \(\bm{\epsilon}\) to represent various inevitable variations in the data, such as shape deformation and object rotation variations. Theorem 3 shows that high-order interactive concepts are much more unstable to inevitable variations in the data than low-order interactive concepts. This makes high-order concepts more likely to be influenced by data variations and less likely to be consistently present or absent in samples of the same category, which boosts the learning difficulty.

**Experimental verification.** We conduct experiments to verify the claim that high-order interactive concepts are less stably extracted under data variations than low-order interaction. We use the following two metrics to evaluate the each interactive concept \(S\) (_i.e.,_ each single input dimension of the above linear function) of a certain order. Specifically, we use the first metric \(\beta(S)=\mathbb{E}_{c}[[\mathbb{E}_{\bm{x}\in\bm{X}_{\epsilon}}[I(S|\bm{x})] ]/[\mathrm{Std}_{\bm{x}\in\bm{X}_{\epsilon}}[I(S|\bm{x})]]\) to measure the relative consistency of the interactive concept appearing over different input samples in a certain category \(c\). Here, \(\bm{X}_{c}\) denotes a set of training samples belonging to the category \(c\), and \(\mathrm{Std}_{\bm{x}\in\bm{X}_{\epsilon}}[I(S|\bm{x})]\) indicates the standard deviation of \(I(S|\bm{x})\) over different input samples. A large \(\beta(S)\) value means that the interactive concept \(S\) in all samples of the category \(c\) has similar/consistent effects \(I(S|\bm{x})\). It is easier for a DNN to learn such a consistent concept \(S\).

In addition, we use another metric \(\kappa(S)\) to verify whether the interaction effect of the high-order interactive concept is usually less stably extracted than those of the low-order interactive concept. To this end, the metric \(\kappa(S)=\)\(\mathbb{E}_{\bm{x}\in\bm{X}}[\mathbb{E}_{\epsilon}[|I(S|\bm{x}+\bm{\epsilon}) -I(S|\bm{x})|]]/\mathbb{E}_{\bm{x}\in\bm{X}}\left[|I(S|\bm{x})|\right]=\mathbb{ E}_{\bm{x}\in\bm{X}}[\mathbb{E}_{\epsilon}[|C_{S}(\bm{x}+\bm{\epsilon})-C_{S}( \bm{x})|]]/\mathbb{E}_{\bm{x}\in\bm{X}}\left[|C_{S}(\bm{x})|\right]\) measures the relative instability of the interactive concept \(S\) to the inevitable slight variations \(\bm{\epsilon}\) in the data. Here, \(C_{S}(\bm{x}+\bm{\epsilon})=I(S|\bm{x}+\bm{\epsilon})/U_{S}\) computed in Equation (7) denotes the trigger state of the interactive concept \(S\) on the sample \(\bm{x}^{\prime}=\bm{x}+\bm{\epsilon}\).

To this end, we use DNNs and experimental settings in the _experimental verification_ paragraph of Section 3.2 for evaluation. Figure 3 and Figure 4 show the change of the average consistency \(\beta(S)\) and the average instability \(\kappa(S)\) of \(s\)-order interactive concepts through the learning process, respectively. At each training epoch, low-order concepts usually obtain higher consistency \(\beta(S)\) and

Figure 4: Instability \(\kappa(S)\) of \(s\)-order interactive concepts to data variations. The curve shows the average instability \(\mathbb{E}_{S\subseteq N,|S|=s}[\kappa(S)]\) over different interactive concepts of the \(s\)-th order, and the shade represents the standard deviation \(\mathrm{Std}_{S\subseteq N,|S|=s}[\kappa(S)]\). Effects of high-order concepts are more sensitive to data variations than those of low-order concepts.

Figure 3: Consistency \(\beta(S)\) of \(s\)-order interactive concepts. The curve shows the mean consistency \(\mathbb{E}_{S\subseteq N,|S|=s}[\beta(S)]\) over different interactive concepts of the \(s\)-th order, and the shade indicates the standard deviation \(\mathrm{Std}_{S\subseteq N,|S|=s}[\beta(S)]\). Effects of low-order concepts are more consistent than those of high-order concepts.

lower instability \(\kappa(S)\) than high-order concepts. It means that low-order concepts usually are more consistent and more stable. Thus, this experiment explains the reason why low-order interactive concepts are easier to learn.

#### 3.2.3 Fast learning of low-order concepts

In this subsection, we illustrate the phenomenon that low-order interactive concepts are usually learned faster than high-order concepts, as a support for the claim that low-order interactive concepts are easier to be learned. We have theoretically proven in Section 3.2 and experimentally verified that low-order interactive concepts are more consistently present or consistently absent in different samples of the same category, which makes low-order interactive concepts easier to be learned. Then, this experiment is conducted to check whether low-order concepts are really learned faster than high-order concepts.

Specifically, we examine whether interactive concepts encoded by the finally-learned DNN \(v_{\text{final}}(\bm{x})\) have already been encoded by the DNN that is \(v_{t}(\bm{x})\) trained after \(t\) epochs. If so, we consider such interactive concepts are learned fast. Specifically, let \(\bm{I}_{t}^{(s)}=[I_{t}(S_{1}|\bm{x}),\cdots,I_{t}(S_{d}|\bm{x})]^{\top}\in \mathbb{R}^{d}\) denote a vector of interaction effects for all \(d=\binom{n}{s}\) interactive concepts of \(s\)-order. Then, we compute the Jaccard similarity between \(s\)-order interactive concepts encoded by the DNN \(v_{\text{final}}(x)\) and those encoded by the DNN \(v_{t}(x)\), _i.e._, \(\text{sim}(\bm{I}_{t}^{(s)},\bm{I}_{\text{final}}^{(s)})\!=\!\min(\bm{\tilde{ I}}_{t}^{(s)},\bm{\tilde{I}}_{\text{final}}^{(s)})\|_{1}/\|\max(\bm{\tilde{ I}}_{t}^{(s)},\bm{\tilde{I}}_{\text{final}}^{(s)})\|_{1}\) and \(\|\cdot\|_{1}\) denotes L1-norm. We extend the \(d\)-dimensional vector \(\bm{I}_{t}^{(s)}\) into a \(2d\)-dimensional vector with non-negative elements \(\bm{\tilde{I}}_{t}^{(s)}=[(\bm{I}_{t}^{(s),+})^{\top},(\bm{I}_{t}^{(s),-})^{ \top}]^{\top}=[\max(\bm{I}_{t}^{(s)},0)^{\top},-\min(\bm{I}_{t}^{(s)},0)^{\top }]^{\top}\in\mathbb{R}^{2d}\). Similarly, we compute \(\bm{\tilde{I}}_{\text{final}}^{(s)}\) with non-negative elements based on \(\bm{I}_{\text{final}}^{(s)}\). In this way, a large similarity \(\text{sim}(\bm{I}_{t}^{(s)},\bm{I}_{\text{final}}^{(s)})\) at an earlier epoch \(t\) indicates that interactive concepts are easier to be learned.

To this end, let us use DNNs and experimental settings introduced in the _experimental verification_ paragraph of Section 3.2 for evaluation. Figure 5 shows that DNNs first learn low-order interactive concepts, and then learn high-order interactive concepts. Such a phenomenon verifies the conclusion that a DNN easily learns low-order (simple) interactive concepts.

## 4 Explaining findings in previous studies

### Explaining adversarial robustness

Previous study [43] has discovered that low-order interactions are more robust to adversarial attacks than high-order interactions. Notice that their high-order (low-order) interactions are different from, but highly related to our high-order (low-order) interactive concepts. Specifically, our interactive concepts can be explained as elementary components for such multi-order interactions in [43]. Please see the supplementary material for the proof. _Therefore, from this perspective, our conclusion that "low-order interactive concepts are easy to learn" can also explain how a DNN encodes concepts of different adversarial robustness_.

Figure 5: The weighted Jaccard similarity \(\text{sim}(\bm{I}_{t}^{(s)},\bm{I}_{\text{final}}^{(s)})\) between \(s\)-order interactive concepts learned after the intermediate epoch \(\bm{I}_{t}^{(s)}\) and those learned after the final epoch \(\bm{I}_{\text{final}}^{(s)}\). Low-order concepts usually have higher Jaccard similarity during the learning process, which indicates that DNNs first learn low-order concepts and then gradually learn more about high-order concepts.

**Can we use interactive concepts in this paper to verify the heuristic findings of adversarial robustness in [43]?** According to Equation (2), we can represent the inference logic of a DNN by a set of interactive concepts, _i.e.,_\(v(\bm{x})\approx\sum_{S\in\Omega_{\text{clean}}}I(S|\bm{x})\). In this way, we conduct experiments to evaluate the sensitivity of each interactive concept \(S\) to adversarial perturbations. The symbolic conceptual representation of interactive concepts allows us to measure the adversarial robustness in a more direct way. That is, if the effect of an interactive concept does not significantly change under adversarial attacks, we consider this interactive concept is robust to adversarial attacks. In contrast, if the effect of an interactive concept changes significantly under adversarial attacks, we consider this interactive concept is sensitive to adversarial attacks.

To this end, we use the metric \(\alpha(S)=\mathbb{E}_{\bm{x}\in\bm{\mathcal{X}}}[|I(S|\bm{x}+\bm{\delta})-I(S| \bm{x})|]\)\(/\mathbb{E}_{\bm{x}\in\bm{X}}\big{[}|I(S|\bm{x})|]\) to evaluate the sensitivity of the interactive concept \(S\) to adversarial perturbations, where \(\delta\) denotes the adversarial perturbation generated by the \(\ell_{\infty}\) attack [33]. In this way, a small \(\alpha(S)\) value indicates that the interactive concept \(S\) is robust to the adversarial attack.

We follow experimental settings in the _experimental verification_ paragraph of Section 3.2 to evaluate different DNNs. Figure 6 shows that compared to high-order interactive concepts, low-order concepts usually obtain smaller \(A^{(s)}=\mathbb{E}_{S\subseteq N,|S|=s}[\alpha(S)]\) values. Such phenomena demonstrate that low-order interactive concepts are more robust to adversarial attacks.

### Connections to existing findings on what are learned first by a DNN

In this subsection, we discuss some related studies on which kind of knowledge is usually first learned by a DNN. Most previous studies conducted experiments to explore the knowledge that was easier to be learned by a DNN, without providing much theoretical support. However, we find that our theorems can partially explain mechanisms behind some previous findings.

\(\bullet\) Arpit et al. [5] trained DNNs to classify both normal samples and white-noise samples to different object categories. In this way, they considered that the DNN encoded simple concepts to classify normal samples, but the DNN had to learn complex concepts to classify white noises to randomly-assigned labels. They observed that the DNN usually learned normal samples first, because the classification accuracy of normal samples increased before that of white noises. To this end, our research provides more insights into such an observation. Specifically, Cheng et al. [8] have proven that the classification of noisy data usually depends on high-order concepts, _i.e._, the classification of noisy data forces the DNN to memorize each specific white-noise sample as a specific high-order interactive concept. Let us combine this conclusion with our finding that high-order interactive concepts are hard to learn. Then, we can easily owe the slow learning of white-noise samples observed in [5] to the difficulty of learning high-order concepts.

\(\bullet\) Mangalam and Prabhu [34] considered/defined easy samples as training samples that could be correctly classified by shallow machine learning models, such as support vector machine (SVM) and random forests (RF). They discovered that DNNs first learned easy samples, and then gradually learned more about hard samples. To this end, our research verifies such observation. Specifically, we claim that easy samples mainly contain low-order interactive concepts. Thus, hard samples mainly contain high-order interactive concepts corresponding to complex interactions between numerous

Figure 6: Average adversarial sensitivity \(A^{(s)}\) of \(s\)-order interactive concepts to adversarial perturbations. Low-order interactive concepts are usually much less sensitive to adversarial attacks than high-order interactive concepts.

input variables, and thus are difficult to be classified by shallow models (_e.g._, the SVM and the RF). In this way, the fast learning of low-order concepts is another understanding of the finding in [34].

\(\bullet\) Xu et al. [63] discovered that during the training process, DNNs usually first learned samples of low frequencies (_e.g.,_ robust to noises), and then encoded samples of high frequencies (_e.g.,_ sensitive to noises). However, the original design of DNNs is not towards learning specific spectrums, and techniques of deep learning are not developed by assuming a periodic loss landscape of training samples. Therefore, we believe there should be a more direct explanation for the spectrum-learning phenomenon discovered by [63]. To this end, our research explains this phenomenon as the difficulty of learning high-order concepts. According to Section 4.1, low-order concepts usually are less sensitive to input perturbations, thereby corresponding to low-frequency components defined in the loss landscape in [63]. Accordingly, high-order concepts correspond to high-frequency components.

\(\bullet\) Liu et al. [28] discovered that during adversarial training, the training loss of the DNN trained on easy samples decreased faster than that of the DNN trained on hard samples. To this end, we consider easy samples in adversarial training mentioned by [28] may mainly contain low-order interactive concepts. It is because, as discussed in Section 4.1, [43] discovered that low-order interactions were robust to adversarial perturbations. Thus, the fast learning of easy samples in adversarial training can be roughly owing to the learning of low-order concepts.

## 5 Conclusion and discussion

In this paper, we theoretically explain the trend of DNNs learning simple concepts. Since the emergence of interactive concepts of DNNs has been observed [45; 27] and proved [47], we aim to theoretically explain the explicit theoretical connection between the conceptual complexity and the difficulty of learning concepts. In this way, we prove that low-order interactive concepts in the data are much more stable than high-order interactive concepts, which makes low-order interactive concepts more likely to be encoded. Besides, our research can also provide new insights into several previous empirical understandings [5; 34; 63; 28]_w.r.t._ the conceptual representation of DNNs.

There are very few ways to define and examine what is a "concept." In this paper, we only use the following three properties to support the faithfulness of using sparse salient interactions as concepts encoded by the DNN.

\(\bullet\) Ren et al. [47] have proved that a well-trained DNN can encode just a small number of salient interactions for inference under some common conditions.

\(\bullet\) Ren et al. [45] have proved that, such a small number of salient interactions extracted from a sample can well mimic DNN's outputs on numerous masked samples.

\(\bullet\) Li and Zhang [27] have discovered that salient interactions have considerable transferability and strong discrimination power.

However, the above properties cannot guarantee a clear correspondence between an interactive concept and a concept in human cognition. Up to now, we cannot mathematically formulate what is a concept in cognitive science. Thus, there is still a long way to unify the learning difficulty of a DNN and the cognitive difficulty of human beings.

## Acknowledgments

This work is partially supported by the National Nature Science Foundation of China (62276165), National Key R&D Program of China (2021ZD0111602), Shanghai Natural Science Foundation (21JC1403800,21ZR1434600), National Nature Science Foundation of China (U19B2043). This paper is also partially supported by Huawei Technologies Inc.

## References

* [1] Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations through noisy computation. _IEEE transactions on pattern analysis and machine intelligence_, 40(12):2897-2905, 2018.
* [2] Sravanti Addepalli, Anshul Nasery, Venkatesh Babu Radhakrishnan, Praneeth Netrapalli, and Prateek Jain. Feature reconstruction from outputs can mitigate simplicity bias in neural networks. 2022.

* [3] Rana Ali Amjad and Bernhard C Geiger. Learning representations for neural network-based classification using the information bottleneck principle. _IEEE transactions on pattern analysis and machine intelligence_, 42(9):2225-2239, 2019.
* [4] Marco Ancona, Cengiz Oztireli, and Markus Gross. Explaining deep neural networks with a polynomial time algorithm for shapley value approximation. In _International Conference on Machine Learning_, pages 272-281. PMLR, 2019.
* [5] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In _International conference on machine learning_, pages 233-242. PMLR, 2017.
* [6] Arthur Asuncion and David Newman. Uci machine learning repository, 2007.
* [7] Chuanchuan Chen, Dongrui Liu, Changqing Xu, and Trieu-Kien Truong. Saks: Sampling adaptive kernels from subspace for point cloud graph convolution. _IEEE Transactions on Circuits and Systems for Video Technology_, 2023.
* [8] Xu Cheng, Chuntung Chu, Yi Zheng, Jie Ren, and Quanshi Zhang. A game-theoretic taxonomy of visual concepts in dnns. _arXiv preprint arXiv:2106.10938_, 2021.
* [9] Xu Cheng, Xin Wang, Haotian Xue, Zhengyang Liang, and Quanshi Zhang. A hypothesis for the aesthetic appreciation in neural networks. _arXiv preprint arXiv:2108.02646_, 2021.
* [10] Huiqi Deng, Na Zou, Mengnan Du, Weifu Chen, Guocan Feng, and Xia Hu. A general taylor framework for unifying and revisiting attribution methods. _arXiv preprint arXiv:2105.13841_, 2021.
* [11] Huiqi Deng, Qihan Ren, Xu Chen, Hao Zhang, Jie Ren, and Quanshi Zhang. Discovering and explaining the representation bottleneck of dnns. _International Conference on Learning Representations_, 2022.
* [12] Huiqi Deng, Na Zou, Mengnan Du, Weifu Chen, Guocan Feng, Ziwei Yang, Zheyang Li, and Quanshi Zhang. Understanding and unifying fourteen attribution methods with taylor interactions. _arXiv preprint arXiv:2303.01506_, 2023.
* [13] Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize for deep nets. In _International Conference on Machine Learning_, pages 1019-1028. PMLR, 2017.
* [14] Stanislav Fort, Pawel Krzysztof Nowak, Stanislaw Jastrzebski, and Srini Narayanan. Stiffness: A new perspective on generalization in neural networks. _arXiv preprint arXiv:1901.09491_, 2019.
* [15] Robert Geirhos, Jorn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. _Nature Machine Intelligence_, 2(11):665-673, 2020.
* [16] Michel Grabisch and Marc Roubens. An axiomatic approach to the concept of interaction among players in cooperative games. _International Journal of game theory_, 28(4):547-565, 1999.
* [17] Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic gradient descent. In _International conference on machine learning_, pages 1225-1234. PMLR, 2016.
* [18] John C Harsanyi. A simplified bargaining model for the n-person cooperative game. _International Economic Review_, 4(2):194-220, 1963.
* [19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.

* [20] Minyoung Huh, Hossein Mobahi, Richard Zhang, Brian Cheung, Pulkit Agrawal, and Phillip Isola. The low-rank simplicity bias in deep networks. _Transactions on Machine Learning Research_, 2022.
* [21] S Kornblith, M Norouzi, H Lee, and G Hinton. Similarity of neural network representations revisited. In _International Conference on Machine Learning_, pages 3519-3529. PMLR, 2019.
* [22] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
* [23] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. _Advances in neural information processing systems_, 25, 2012.
* [24] Stephen Laurence and Eric Margolis. Concepts and cognitive science. 1999.
* [25] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. _CS 231N_, 7(7):3, 2015.
* [26] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* [27] Mingjie Li and Quanshi Zhang. Does a neural network really encode symbolic concept? In _International conference on machine learning_. PMLR, 2023.
* [28] Chen Liu, Zhichao Huang, Mathieu Salzmann, Tong Zhang, and Sabine Susstrunk. On the impact of hard adversarial instances on overfitting in adversarial training. _arXiv preprint arXiv:2112.07324_, 2021.
* [29] Dongrui Liu, Shaobo Wang, Jie Ren, Kangrui Wang, Sheng Yin, Huiqi Deng, and Quanshi Zhang. Trap of feature diversity in the learning of mlps. _arXiv preprint arXiv:2112.00980_, 2021.
* [30] Dongrui Liu, Chuanchaun Chen, Changqing Xu, Robert C Qiu, and Lei Chu. Self-supervised point cloud registration with deep versatile descriptors for intelligent driving. _IEEE Transactions on Intelligent Transportation Systems_, 2023.
* [31] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In _Proceedings of the IEEE international conference on computer vision_, pages 3730-3738, 2015.
* [32] Scott M Lundberg, Gabriel G Erion, and Su-In Lee. Consistent individualized feature attribution for tree ensembles. _arXiv preprint arXiv:1802.03888_, 2018.
* [33] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _International Conference on Learning Representations_, 2018.
* [34] Kartikeya Mangalam and Vinay Uday Prabhu. Do deep neural networks learn shallow learnable examples first? 2019.
* [35] Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear regions of deep neural networks. _Advances in neural information processing systems_, 27, 2014.
* [36] Ari Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural networks with canonical correlation. _Advances in Neural Information Processing Systems_, 31, 2018.
* [37] Roman Novak, Yasaman Bahri, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Sensitivity and generalization in neural networks: an empirical study. _arXiv preprint arXiv:1802.08760_, 2018.
* [38] Razvan Pascanu, Guido Montufar, and Yoshua Bengio. On the number of response regions of deep feed forward networks with piece-wise linear activations. _arXiv preprint arXiv:1312.6098_, 2013.

* [39] Henning Petzka, Michael Kamp, Linara Adilova, Cristian Sminchisescu, and Mario Boley. Relative flatness and generalization. _Advances in Neural Information Processing Systems_, 34:18420-18432, 2021.
* [40] Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron C Courville, Doina Precup, and Guillaume Lajoie. Gradient starvation: A learning proclivity in neural networks. _Advances in Neural Information Processing Systems_, 34:1256-1272, 2021.
* [41] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 652-660, 2017.
* [42] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability. _Advances in neural information processing systems_, 30, 2017.
* [43] Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Yiting Chen, Xu Cheng, Xin Wang, Meng Zhou, Jie Shi, et al. A unified game-theoretic interpretation of adversarial robustness. _35th Conference on Neural Information Processing Systems (NeurIPS 2021)_, 2021.
* [44] Jie Ren, Zhanpeng Zhou, Qirui Chen, and Quanshi Zhang. Can we faithfully represent absence states to compute shapley values on a dnn? In _The Eleventh International Conference on Learning Representations_, 2022.
* [45] Jie Ren, Mingjie Li, Qirui Chen, Huiqi Deng, and Quanshi Zhang. Defining and quantifying the emergence of sparse concepts in dnns. In _IEEE Conference on Computer Vision and Pattern Recognition_, 2023.
* [46] Qihan Ren, Huiqi Deng, Yunuo Chen, Siyu Lou, and Quanshi Zhang. Bayesian neural networks avoid encoding complex and perturbation-sensitive concepts. _Proceedings of the 40th International Conference on Machine Learning_, pages 28889-28913, 2023.
* [47] Qihan Ren, Jiayang Gao, Wen Shen, and Quanshi Zhang. Where we have arrived in proving the emergence of sparse symbolic concepts in ai models. _arXiv preprint arXiv:2305.01939_, 2023.
* [48] Joshua Robinson, Li Sun, Ke Yu, Kayhan Batmanghelich, Stefanie Jegelka, and Suvrit Sra. Can contrastive learning avoid shortcut solutions? _Advances in neural information processing systems_, 34:4974-4986, 2021.
* [49] Luca Scimeca, Seong Joon Oh, Sanghyuk Chun, Michael Poli, and Sangdoo Yun. Which shortcut cues will dnns choose? a study from the parameter-space perspective. _arXiv preprint arXiv:2110.03095_, 2021.
* [50] Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, and Praneeth Netrapalli. The pitfalls of simplicity bias in neural networks. _Advances in Neural Information Processing Systems_, 33:9573-9585, 2020.
* [51] Lloyd S Shapley. A value for n-person games. _Contributions to the Theory of Games_, 2(28):307-317, 1953.
* [52] Wen Shen, Qihan Ren, Dongrui Liu, and Quanshi Zhang. Interpreting representation quality of dnns for 3d point cloud processing. _Advances in Neural Information Processing Systems_, 34:8857-8870, 2021.
* [53] Thomas J Shuell. Cognitive conceptions of learning. _Review of educational research_, 56(4):411-436, 1986.
* [54] Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via information. _arXiv preprint arXiv:1703.00810_, 2017.
* [55] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. _arXiv preprint arXiv:1409.1556_, 2014.

* [56] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 conference on empirical methods in natural language processing_, pages 1631-1642, 2013.
* [57] Mukund Sundararajan, Kedar Dhamdhere, and Ashish Agarwal. The shapley taylor interaction index. In _International Conference on Machine Learning_, pages 9259-9268. PMLR, 2020.
* [58] Che-Ping Tsai, Chih-Kuan Yeh, and Pradeep Ravikumar. Faith-shap: The faithful shapley interaction index. _arXiv preprint arXiv:2203.00870_, 2022.
* [59] Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, and Quanshi Zhang. A unified approach to interpreting and boosting adversarial transferability. _arXiv preprint arXiv:2010.04055_, 2020.
* [60] Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, and Quanshi Zhang. A unified approach to interpreting and boosting adversarial transferability. In _International Conference on Learning Representations_, 2021.
* [61] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. _Transactions of the Association for Computational Linguistics_, 7:625-641, 2019.
* [62] Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach. _arXiv preprint arXiv:1801.10578_, 2018.
* [63] Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang Xiao, and Zheng Ma. Frequency principle: Fourier analysis sheds light on deep neural networks. _arXiv preprint arXiv:1901.06523_, 2019.
* [64] Zhiqin John Xu. Understanding training and generalization in deep learning by fourier analysis. _arXiv preprint arXiv:1808.04295_, 2018.
* [65] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections. _ACM Transactions on Graphics (ToG)_, 35(6):1-12, 2016.
* [66] Die Zhang, Hao Zhang, Huilin Zhou, Xiaoyi Bao, Da Huo, Ruizhao Chen, Xu Cheng, Mengyue Wu, and Quanshi Zhang. Building interpretable interaction trees for deep NLP models. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pages 14328-14337. AAAI Press, 2021.
* [67] Hao Zhang, Sen Li, YinChao Ma, Mingjie Li, Yichen Xie, and Quanshi Zhang. Interpreting and boosting dropout from a game-theoretic view. In _International Conference on Learning Representations_, 2020.
* [68] Hao Zhang, Yichen Xie, Longjie Zheng, Die Zhang, and Quanshi Zhang. Interpreting multivariate shapley interactions in dnns. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pages 10877-10886. AAAI Press, 2021.
* [69] Quanshi Zhang, Xin Wang, Jie Ren, Xu Cheng, Shuyun Lin, Yisen Wang, and Xiangming Zhu. Proving common mechanisms shared by twelve methods of boosting adversarial transferability. _arXiv preprint arXiv:2207.11694_, 2022.
* [70] Huilin Zhou, Hao Zhang, Huiqi Deng, Dongrui Liu, Wen Shen, Shih-Han Chan, and Quanshi Zhang. Concept-level explanation for the generalization of a dnn. _arXiv preprint arXiv:2302.13091_, 2023.

Literature on the representation capacity of DNNs

Formulating and evaluating the representation ability of DNNs is an emerging perspective to explain DNNs. Pascanu et al. [38] and Montufar et al. [35] evaluated the representation capacity of DNNs based on the number of linear response regions. Kornblith et al. [21], Raghu et al. [42], and Morcos et al. [36] analyzed representations similarly between DNNs by using canonical correlation analysis. Shen et al. [52], Chen et al. [7], and Liu et al. [30] measured the quality of knowledge representations encoded in DNNs for point cloud processing. The information-bottleneck theory [54] quantified information encoded in DNNs, and was extended to improve the representation capacity of DNNs [1; 3]. Xu [64] and Xu et al. [63] explained the generalization of DNNs from the perspective of Fourier analysis. Furthermore, several metrics were proposed to evaluate the robustness or generalization capacity of DNNs, including the flatness of loss functions at minima [13], the stability of optimization [17], the CLEVER score [62], the stiffness [14], and the sensitivity metrics [37].

In contrast to previous empirical studies, we mathematically formulate concepts encoded by DNNs and theoretically prove that DNNs mainly learn simple concepts.

## Appendix B Literature on interactions

Many studies investigated interactions between input variables of DNNs in recent years. Grabisch and Roubens [16] proposed the Shapley interaction index to measure the interaction between players in a cooperative game, based on the Shapley value [51]. Lundberg et al. [32] used the Shapley interaction index to analyze tree ensembles. Sundararajan et al. [57] proposed the Shapley-Taylor interaction index, and Tsai et al. [58] defined Faith-Shap, which was another interaction index.

In this paper, we use interactions between input variables of a DNN to represent concepts encoded by the DNN. In this way, We theoretically explain and empirically verify that DNN is easier to learn simple interactive concepts.

## Appendix C Literature on the shortcut learning and simplicity bias of DNNs

Many studies focused on the underlying principles and limitations of DNNs. Geirhos et al. [15], Robinson et al. [48], and Scimeca et al. [49] investigated the _shortcut learning_ in DNNs. _I.e._, DNNs may find decision rules that perform well on standard datasets, but fail to generalize to real-world scenarios. For example, DNNs successfully detected pneumonia by identifying hospital-specific metal tokens on the scan, but they failed to learn much about pneumonia and achieved low performance for scans from novel hospitals. Meanwhile, many studies proposed _simplicity bias_, _i.e._, standard training procedures for DNNs have a bias towards learning simple models [50; 20; 40]. Specifically, DNNs tend to learn low-rank embeddings [20] or simple features [50; 29; 40; 2].

From the perspective of the simplicity bias, our study considers the definition of interactive concepts in [47], and analyzes the bias towards learning simple concepts. This work clarifies an exact form of the complexity of concepts that a DNN is difficult to learn.

## Appendix D Axioms and theorems for the Harsanyi dividend interaction

In this section, we introduce that the Harsanyi dividend interaction \(I(S)\) satisfies several desirable axioms and theorems.

The Harsanyi dividend interactions \(I(S)\) satisfies the _efficiency, linearity, dummy, symmetry, anonymity, recursive_ and _interaction distribution_ axioms, as follows.

(1) _Efficiency axiom_ (proved by [18]). The reward of a neural network can be decomposed into interaction effects of different contexts, _i.e._\(v(N)=\sum_{S\subseteq N}I(S)\).

(2) _Linearity axiom_. If we merge rewards of two neural networks \(w\) and \(v\) as the reward of model \(u\), _i.e._\(\forall S\subseteq N\), \(u(S)=w(S)+v(S)\), then their interaction effects \(I_{v}(S)\) and \(I_{w}(S)\) can be represented as \(\forall S\subseteq N,I_{u}(S)=I_{v}(S)+I_{w}(S)\).

(3) _Dummy axiom_. If a variable \(i\in N\) is a dummy variable, _i.e._\(\forall S\subseteq N\backslash\{i\},v(S\cup\{i\})=v(S)+v(\{i\})\), then it does not interact with other variables, \(\forall\emptyset\neq S\subset N\setminus\{i\}\), \(I(S\cup\{i\})=0\).

(4) _Symmetry axiom_. Given two input variables \(i,j\in N\), if \(\forall S\subseteq N\backslash\{i,j\},v(S\cup\{i\})=v(S\cup\{j\})\), then \(\forall S\subseteq N\backslash\{i,j\},I(S\cup\{i\})=I(S\cup\{j\})\).

(5) _Anonymity axiom_. For any permutations \(\pi\) on \(N\), we have \(\forall S\subseteq N,I_{v}(S)\!=\!I_{\pi v}(\pi S)\), where \(\pi\!\{\pi(i)|i\!\in\!S\}\), and the new model \(\pi v\) is defined by \((\pi v)(\pi S)\!\!=\!\!v(S)\). This indicates that interaction effects are not changed by permutation.

(6) _Recursive axiom_. The interaction effects can be calculated recursively. Given \(i\in N\) and \(S\subseteq N\backslash\{i\}\), the interaction effect of the pattern \(S\cup\{i\}\) can be represented as the interaction effect of \(S\) with \(i\) minus the interaction effect of \(S\) without \(i\), _i.e._\(\forall S\!\subseteq\!N\backslash\{i\},I(S\cup\{i\})\!=\!I(S|i\) is always present)\(-I(S)\). \(I(S|i\) is always present) denotes the interaction effect when the variable \(i\) is always present as a constant context, _i.e._\(I(S|i\) is always present) \(=\sum_{L\subseteq S}(-1)^{|S|-|L|}\cdot v(L\cup\{i\})\).

(7) _Interaction distribution axiom_. This axiom shows how interactions are distributed for "interaction functions" [57]. The definition of the interaction function \(v_{T}\) parameterized by a subset of variables \(T\) is given as follows. \(\forall S\subseteq N\), if \(T\subseteq S\), \(v_{T}(S)=c\); otherwise, \(v_{T}(S)=0\). The function \(v_{T}\) models pure interaction among the variables in \(T\), because only if all variables in \(T\) are present, the output value will be increased by \(c\). The interactions encoded in the function \(v_{T}\) satisfies \(I(T)=c\), and \(\forall S\neq T\), \(I(S)=0\).

## Appendix E Sparsity of interactive concepts

In this section, we conducted experiments to verify the sparsity of interactive concepts. To this end, we computed effects \(U_{S}\) of all \(2^{n}\) interactive concepts encoded by a DNN following [45]. Specifically, we trained a five-layer MLP on the _census_ dataset and the _TV news_ dataset), respectively. For better visualization, we re-scaled effects of interactive concepts \(U_{S}\) by \(|U_{S}|/\max_{S^{\prime}\subseteq N}|U_{S^{\prime}}|\). Moreover, the strength of effects are average over different samples in each dataset. Fig. 7 shows histograms of absolute effects of interactive concepts.

## Appendix F Complexity of interactive concepts

Many previous studies [11, 67, 59, 45, 43] used multi-order interactions to analyze DNNs. Specifically, Given a pre-trained DNN \(v\) and a masked sample \(\bm{x}_{S}\), the multi-order interaction \(I^{(m)}(i,j)\) used in [59, 11, 67] is given as follows:

\[I^{(m)}(i,j)=\mathbb{E}_{S\subseteq N\backslash\{i,j\},|S|=m}[\Delta v(i,j,S)],\] (1)

where \(\Delta v(i,j,S)=v(\bm{x}_{S\cup\{i,j\}})-v(\bm{x}_{S\cup\{i\}})-v(\bm{x}_{S \cup\{j\}})+v(\bm{x}_{S})\). They consider that \(I^{(m)}(i,j)\) reflects the collaboration with \(m\) contextual variables (\(m\) pixels). In this way, the order \(m\) measures the number of variables in \(S\). Therefore, a low-order interaction denotes a relatively simple collaboration between input variables with a small context \(S\). In contrast, a high-order interaction represents a complex collaboration between input variables with a large context \(S\).

In this paper, we consider that the number of variables in an interactive concept \(S\) can measure the complexity of an interactive concept encoded by a DNN, namely _the order of the interactive concept_, \(\text{complexity}(S)\!\!=\!\!\text{order}(S)\!=\!|S|\). Thus, low-order concepts usually represent simple AND relationships among a few input variables. In comparison, high-order concepts often refer to as relatively complex AND relationships among a large number of input variables.

Figure 7: The average histograms of absolute effects of interactive concepts encoded by five-layer MLPs.

Proof of Theorems

### Proof of Theorem 2 in the main paper

**Theorem 2**.: _Given a neural network \(v\) and an arbitrary input sample \(\bm{x}^{\prime}\in\mathbb{R}^{n}\), the network output can be decomposed by using the Taylor expansion [10], i.e., \(v(\bm{x}^{\prime})=\sum_{S\subseteq N}\sum_{\bm{\pi}\in Q_{S}}U_{S,\bm{\pi}}\cdot J (S,\bm{\pi}|\bm{x}^{\prime})\). In this way, according to Eq. (1), the interaction effect \(I(S|\bm{x}^{\prime})\) on the sample \(\bm{x}^{\prime}\) can be reformulated as follows._

\[I(S|\bm{x}^{\prime})=\sum_{\bm{\pi}\in Q_{S}}U_{S,\bm{\pi}}\cdot J (S,\bm{\pi}|\bm{x}^{\prime}).\] (2)

_Here, \(J(S,\bm{\pi}|\bm{x}^{\prime})\)\(=\prod_{i\in S}\left(\mathrm{sign}(x^{\prime}_{i}-b_{i})\cdot\frac{x^{\prime}_{i}-b_{i}} {\tau}\right)^{\pi_{i}}\) denotes a Taylor expansion term of the degree \(\bm{\pi}\), where the degree \(\bm{\pi}\in Q_{S}=\{[\pi_{1},\ldots,\pi_{n}]|\forall i\in S,\pi_{i}\in\mathbb{ N}^{+};\forall i\not\in S,\pi_{i}=0\}\) and \(b_{i}\) is the baseline value to mask the input variable \(x_{i}\). In addition, \(U_{S,\bm{\pi}}\)\(=\)\(\frac{\tau^{m}}{\prod_{i=1}^{n}\pi_{i}!}\frac{\partial^{m}v(\bm{x}^{\prime}_{0})}{ \partial x^{\pi_{1}}_{1}\cdots\partial x^{\pi_{n}}_{n}}\cdot\prod_{i\in S}[ \mathrm{sign}(x^{\prime}_{i}-b_{i})]^{\pi_{i}}\), where \(\bm{x}^{\prime}_{0}\) denotes the sample whose input variables are all masked. \(m=\sum_{i=1}^{n}\pi_{i}\)._

Proof.: Let us denote the function on the right of Eq. (2) by \(\tilde{I}(S|\bm{x}^{\prime})\), _i.e._,

\[\tilde{I}(S|\bm{x}^{\prime})=\sum_{\pi\in Q_{S}}U_{S,\pi}J(S,\pi| \bm{x}^{\prime})\] (3)

We need to prove that for any arbitrary input sample \(\forall\bm{x}^{\prime}\in\mathbb{R}^{n}\), \(\tilde{I}(S|\bm{x}^{\prime})=I(S|\bm{x}^{\prime})\).

Actually, it has been proven in [16] and [45] that the Harsanyi dividend \(I(S|\bm{x}^{\prime})\) is the **unique** metric satisfying the faithfulness requirement mentioned in the main paper, _i.e._, satisfying

\[\forall\;T\subseteq N,\;v(\bm{x}^{\prime}_{T})=\sum_{S\in\Omega,S \subseteq T}I(S|\bm{x}^{\prime}).\] (4)

Thus, as long as we can prove that \(\tilde{I}(S|\bm{x}^{\prime})\) also satisfies the above faithfulness requirement, we can obtain \(\tilde{I}(S|\bm{x}^{\prime})=I(S|\bm{x}^{\prime})\).

To this end, we only need to prove \(\tilde{I}(S|\bm{x}^{\prime})\) also satisfies the faithfulness requirement in Eq. (4). Specifically, given an input sample \(\forall\bm{x}^{\prime}\in\mathbb{R}^{n}\), let us consider the Taylor expansion of the network output \(v(\bm{x}_{T})\) of an arbitrarily masked sample \(\bm{x}^{\prime}_{T}(T\subseteq N)\), which is expanded at \(\bm{x}^{\prime}_{\emptyset}=[b_{1},\ldots,b_{n}]\). Then, we have

\[\forall\;T\subseteq N,\;\;\;v(\bm{x}^{\prime}_{T})=\sum_{\pi_{1} \subseteq\pi_{2}}^{\infty}\sum_{\pi_{2}=0}^{\infty}\cdots\sum_{\pi_{n}=0}^{ \infty}\frac{1}{\prod_{i=1}^{n}\pi_{i}!}\frac{\partial^{m}v(\bm{x}^{\prime}_{ \emptyset})}{\partial x^{\pi_{1}}_{1}\cdots\partial x^{\pi_{n}}_{n}}\cdot \prod_{i=1}^{n}[(\bm{x}^{\prime}_{T})_{i}-b_{i}]^{\pi_{i}},\] (5)

where \(\bm{\pi}\in\{[\pi_{1},\ldots,\pi_{n}]|\forall i\in N,\pi_{i}\in\mathbb{N}\}\) denotes the degree vector of Taylor expansion terms, and \(m=\sum_{i=1}^{n}\pi_{i}\). In addition, \(b_{i}\) denotes the reference value to mask the input variable \(x_{i}\).

According to the definition of the masked sample \(\bm{x}^{\prime}_{T}\), we have that all variables in \(T\) keep unchanged and other variables are masked to the reference value. That is, \(\forall\;i\in T,(\bm{x}^{\prime}_{T})_{i}=x_{i};\forall\;i\not\in T,(\bm{x}^ {\prime}_{T})_{i}=b_{i}\). Hence, we obtain \(\forall i\not\in T,[(\bm{x}^{\prime}_{T})_{i}-b_{i}]^{\pi_{i}}=0\). Then, among all Taylor expansion terms, only terms corresponding to degrees \(\bm{\pi}\) in the set \(P=\{[\pi_{1},\ldots,\pi_{n}]|\forall i\in T,\pi_{i}\in\mathbb{N};\forall i\not \in T,\pi_{i}=0\}\) may not be zero. Therefore, Eq. (5) can be rewritten as

\[\forall\;T\subseteq N,\;\;\;v(\bm{x}^{\prime}_{T})=\sum_{\bm{\pi} \in P}\;\frac{1}{\prod_{i=1}^{n}\pi_{i}!}\frac{\partial^{m}v(\bm{x}^{\prime}_{ \emptyset})}{\partial x^{\pi_{1}}_{1}\cdots\partial x^{\pi_{n}}_{n}}\cdot \prod_{i\in T}(x^{\prime}_{i}-b_{i})^{\pi_{i}}.\] (6)

We find that the set \(P\) can be divided into multiple disjoint sets as follows, \(P=\cup_{S\subseteq T}Q_{S}\), where \(Q_{S}=\{[\pi_{1},\ldots,\pi_{n}]|\forall i\in S,\pi_{i}\in\mathbb{N}^{+}; \forall i\not\in S,\pi_{i}=0\}\). Then, we can derive that

\[\begin{split}\forall\;T\subseteq N,\;\;\;v(\bm{x}^{\prime}_{T})& =\sum_{S\subseteq T}\sum_{\bm{\pi}\in Q_{S}}\frac{\prod_{i=1}^{n} \pi_{i}!}\frac{\partial^{m}v(\bm{x}^{\prime}_{\emptyset})}{\partial x^{\pi_{1} }_{1}\cdots\partial x^{\pi_{n}}_{n}}\cdot\prod_{i\in S}(x^{\prime}_{i}-b_{i})^{ \pi_{i}}\\ &=\sum_{S\subseteq T}\sum_{\bm{\pi}\in Q_{S}}\underbrace{\frac{ \tau^{m}}{\prod_{i=1}^{n}\pi_{i}!}\frac{\partial^{m}v(\bm{x}^{\prime}_{ \emptyset})}{\partial x^{\pi_{1}}_{1}\cdots\partial x^{\pi_{n}}_{n}}\prod_{i\in S }(\delta_{i})^{\pi_{i}}}_{\text{\tiny{temmed}}\,U_{S,\bm{\pi}}}\cdot\underbrace{ \prod_{i\in S}(\delta_{i}\frac{x^{\prime}_{i}-b_{i}}{\tau})^{\pi_{i}}}_{ \text{\tiny{temmed}}\,(S,\pi|\bm{x}^{\prime})},\end{split}\] (7)

where \(\tau\in\mathbb{R}\) is a pre-defined constant and \(\delta_{i}=\mathrm{sign}(x_{i}-r_{i})\in\{-1,1\}\) is a sign function satisfying \(\delta_{i}^{2m}=1\) (\(m=\sum_{i=1}^{n}\pi_{i}\)). Then, Eq. (7) can be re-written as

\[\forall\;T\subseteq N,\;v(\bm{x}^{\prime}_{T})=\sum_{S\subseteq T}\sum_{\bm{ \pi}\in Q_{S}}U_{S,\bm{\pi}}\cdot J(S,\bm{\pi}|\bm{x}^{\prime})=\sum_{S \subseteq T}\tilde{I}(S|\bm{x}^{\prime}).\] (8)Thus, \(\tilde{I}(S|\bm{x}^{\prime})\) satisfies the faithfulness requirement in Eq. (4) when \(\Omega=2^{N}\).

Therefore, Theorem 2 holds. 

### Proof of Theorem 3 in the main paper

**Theorem 3**.: _Let us add a Gaussian perturbation \(\bm{\epsilon}\sim\mathcal{N}(\bm{0},\sigma^{2}\bm{I})\) to the input sample \(\bm{x}\). Let us first consider the case with the lowest degree \(\hat{\bm{\pi}}=[\hat{\pi}_{1},\ldots,\hat{\pi}_{n}]\in Q_{S}\), satisfying that \(\forall i\in S,\hat{\pi}_{i}=1;\forall i\notin S,\hat{\pi}_{i}=0\). The mean and variance of \(J(S,\hat{\bm{\pi}}|\bm{x}+\bm{\epsilon})\) over the Gaussian perturbation \(\bm{\epsilon}\) are given as_

\[\mathbb{E}_{\bm{\epsilon}}[I(S|\bm{x}+\bm{\epsilon})]=U_{S,\hat{\bm{\pi}}}, \quad\mathrm{Var}_{\bm{\epsilon}}[I(S|\bm{x}+\bm{\epsilon})]=U_{S,\hat{\bm{ \pi}}}^{2}\bigg{[}\left(1+(\sigma/\tau)^{2}\right)^{s}-1\bigg{]}.\]

_Furthermore, for the more general case with an arbitrary degree \(\bm{\pi}\in Q_{S}=\{[\pi_{1},\cdots,\pi_{n}]|\forall i\in S,\pi_{i}\in\mathbb{ N}^{+};\forall i\notin S,\pi_{i}=0\}\), the mean and variance of \(J(S,\bm{\pi}|\bm{x}+\bm{\epsilon})\) are computed as_

\[\mathbb{E}_{\bm{\epsilon}}[J(S,\bm{\pi}|\bm{x}+\bm{\epsilon})]=\mathbb{E}_{ \bm{\epsilon}}[\prod\nolimits_{i\in S}(1+\epsilon_{i}/\tau)^{\pi_{i}}],\quad \mathrm{Var}_{\bm{\epsilon}}[J(S,\bm{\pi}|\bm{x}+\bm{\epsilon})]=\mathrm{Var }_{\bm{\epsilon}}[\prod\nolimits_{i\in S}(1+\epsilon_{i}/\tau)^{\pi_{i}}].\]

Proof.: If we only consider Taylor expansion term of the lowest degree, then \(I(S|\bm{x}^{\prime})\approx U_{S,\hat{\bm{\pi}}}\cdot J(S,\hat{\bm{\pi}}|\bm{ x}^{\prime})\), where \(J(S,\hat{\bm{\pi}}|\bm{x}^{\prime})=\prod_{i\in S}\mathrm{sign}(x^{\prime}_{i}-b _{i})\cdot\frac{x^{\prime}_{i}-b_{i}}{\tau}\).

Let us add a Gaussian perturbation \(\bm{\epsilon}\sim\mathcal{N}(\bm{0},\sigma^{2}\bm{I})\) to the input sample \(\bm{x}\). Then, we have

\[\begin{split} I(S|\bm{x}+\bm{\epsilon})&\approx U_{S,\hat{\bm{\pi}}}\cdot J(S,\hat{\bm{\pi}}|\bm{x}+\bm{\epsilon})\\ J(S,\hat{\bm{\pi}}|\bm{x}+\bm{\epsilon})&=\prod_{i \in S}\mathrm{sign}(x_{i}+\epsilon_{i}-b_{i})\cdot\frac{x_{i}+\epsilon_{i}-b_{ i}}{\tau}\\ &=\prod_{i\in S}\bigg{(}\mathrm{sign}(x_{i}+\epsilon_{i}-b_{i}) \cdot\frac{x_{i}-b_{i}}{\tau}+\mathrm{sign}(x_{i}+\epsilon_{i}-b_{i})\cdot \frac{\epsilon_{i}}{\tau}\bigg{)}\end{split}\] (9)

According to the setting of the baseline value, we have \(\forall i\in S,x_{i}-b_{i}\in\{-\tau,\tau\}\). In Section 2.2, we have assumed that the perturbation is small, _i.e._, \(\forall i\in S,|\epsilon_{i}|\leq\tau\). In this way, we have \(\mathrm{sign}(x_{i}+\epsilon_{i}-b_{i})=\mathrm{sign}(x_{i}-b_{i})\), and we can obtain

\[\begin{split} J(S,\hat{\bm{\pi}}|\bm{x}+\bm{\epsilon})& =\prod_{i\in S}\bigg{(}\mathrm{sign}(x_{i}-b_{i})\cdot\frac{x_{i} -b_{i}}{\tau}+\mathrm{sign}(x_{i}-b_{i})\cdot\frac{\epsilon_{i}}{\tau}\bigg{)} \\ &=\prod_{i\in S}\Big{(}1+\mathrm{sign}(x_{i}-b_{i})\cdot\frac{ \epsilon_{i}}{\tau}\Big{)}\end{split}\] (10)

\[\begin{split}\Rightarrow\mathbb{E}_{\bm{\epsilon}}[J(S,\hat{\bm{ \pi}}|\bm{x}+\bm{\epsilon})]&=\mathbb{E}_{\bm{\epsilon}}\left[ \prod_{i\in S}\Big{(}1+\mathrm{sign}(x_{i}-b_{i})\cdot\frac{\epsilon_{i}}{\tau }\Big{)}\right]\\ \mathrm{Var}_{\bm{\epsilon}}[J(S,\hat{\bm{\pi}}|\bm{x}+\bm{ \epsilon})]&=\mathrm{Var}_{\bm{\epsilon}}\left[\prod_{i\in S} \Big{(}1+\mathrm{sign}(x_{i}-b_{i})\cdot\frac{\epsilon_{i}}{\tau}\Big{)} \right]\end{split}\] (11)

Since \(\mathrm{sign}(x_{i}-b_{i})\in\{-1,1\}\), we have \(1+\mathrm{sign}(x_{i}-b_{i})\cdot\frac{\epsilon_{i}}{\tau}\sim\mathcal{N}(1,( \sigma/\tau)^{2}),\forall i\in S\).

**Proposition 1**.: _If random variables \(X_{1},X_{2},\cdots,X_{k}\) are independent of each other, then \(\mathbb{E}[X_{1}X_{2}\cdots X_{k}]=\prod_{i=1}^{k}\mathbb{E}[X_{i}]\), and \(\mathrm{Var}[X_{1}X_{2}\cdots X_{k}]=\prod_{i=1}^{k}(\mathbb{E}[X_{i}]^{2}+ \mathrm{Var}[X_{i}]^{2})-\prod_{i=1}^{k}\mathbb{E}[X_{i}]^{2}\)._

According to the above proposition, we have

\[\begin{split}\mathbb{E}_{\bm{\epsilon}}[J(S,\hat{\bm{\pi}}|\bm{x}+ \bm{\epsilon})]&=\prod_{i\in S}1=1\\ \mathrm{Var}_{\bm{\epsilon}}[J(S,\hat{\bm{\pi}}|\bm{x}+\bm{ \epsilon})]&=\prod_{i\in S}\Big{(}1^{2}+\left(\sigma/\tau\right)^{2 }\Big{)}-\prod_{i\in S}1^{2}\\ &=\Big{(}1+\left(\sigma/\tau\right)^{2}\Big{)}^{|S|}-1\end{split}\] (12)

[MISSING_PAGE_EMPTY:19]

### Proof of Theorem 4 in the main paper

**Theorem 4**.: _Given an arbitrarily masked sample \(\bm{x}_{T}(\forall T\subseteq N)\), the function \(C_{S}(\bm{x}_{T})\) defined above can well fit the binary activation state of the concept \(S\) in the sample \(\bm{x}_{T}\)._

\[\forall\,T\subseteq N,\;C(S|\bm{x}_{T})=\prod_{i\in S}A_{i}(\bm{x}_{T})= \mathbbm{1}(S\subseteq T),\] (19)

_where \(A_{i}(\bm{x}_{T})\in\{0,1\}\) denotes whether the variable \(x_{i}\) is present \(A_{i}(\bm{x}_{T})=1\) or being masked \(A_{i}(\bm{x}_{T})=0\) in the sample \(\bm{x}_{T}\)._

Proof.: We know that the function of the activation state on an arbitrary sample \(\bm{x}^{\prime}\) is given by

\[\begin{split} C_{S}(\bm{x}^{\prime})&=\sum\nolimits _{\bm{\pi}\in Q_{S}}U_{S,\bm{\pi}}J(S,\bm{\pi}|\bm{x}^{\prime})/U_{S},\\ \text{where }J(S,\bm{\pi}|\bm{x}^{\prime})&=\prod _{i\in S}\left(\operatorname{sign}(x^{\prime}_{i}-b_{i})\cdot\frac{x^{\prime} _{i}-b_{i}}{\tau}\right)^{\pi_{i}}\end{split}\] (20)

Specifically, now we consider a masked sample \(\bm{x}_{T}\), and we will prove that \(C_{S}(\bm{x}_{T})=\mathbbm{1}(S\subseteq T)\). We consider the following two cases.

**Case 1:**\(S\subsetneq T\). Then there exists some \(j\in S\setminus T\). Since \(j\notin T\), according to the masking rule of the sample \(\bm{x}_{T}\), we have \((\bm{x}_{T})_{j}-b_{j}=0\). Since \(j\in S\), we have

\[\forall\pi\in Q_{S},\quad J(S,\bm{\pi}|\bm{x}_{T})=\prod_{i\in S}\left( \operatorname{sign}((\bm{x}_{T})_{i}-r_{i})\cdot\frac{(\bm{x}_{T})_{i}-b_{i}}{ \tau}\right)^{\pi_{i}}=0\] (21)

In this way, we have \(C_{S}(\bm{x}_{T})=\sum_{\bm{\pi}\in Q_{S}}U_{S,\bm{\pi}}J(S,\bm{\pi}|\bm{x}_{ T})/U_{S}=0\).

**Case 2:**\(S\subseteq T\). In this case, \(\forall i\in S\), we have \(i\in T\). According to the setting of the reference value in Section 2.3 of the main paper, we have \((\bm{x}_{T})_{i}-b_{i}\in\{\tau,-\tau\}\). Then we have \(\operatorname{sign}((\bm{x}_{T})_{i}-b_{i})\cdot\frac{(\bm{x}_{T})_{i}-b_{i}}{ \tau}=1,\;\forall i\in S\). This further implies that

\[\forall\pi\in Q_{S},\quad J(S,\bm{\pi}|\bm{x}_{T})=\prod_{i\in S}\left( \operatorname{sign}((\bm{x}_{T})_{i}-b_{i})\cdot\frac{(\bm{x}_{T})_{i}-b_{i}}{ \tau}\right)^{\pi_{i}}=1.\] (22)

Therefore, we can derive

\[C_{S}(\bm{x}_{T})=\sum\nolimits_{\bm{\pi}\in Q_{S}}U_{S,\bm{\pi}}/U_{S}.\] (23)

Recall that \(U_{S}=I(S|\bm{x})=\sum_{\bm{\pi}\in Q_{S}}U_{S,\bm{\pi}}J(S,\bm{\pi}|\bm{x})\), in which \(J(S,\bm{\pi}|\bm{x})=\prod_{i\in S}\left(\operatorname{sign}(x_{i}-r_{i})\cdot \frac{x_{i}-b_{i}}{\tau}\right)^{\pi_{i}}=1\), because \(x_{i}-b_{i}\in\{\tau,-\tau\}\). So actually, we have

\[U_{S}=\sum\nolimits_{\bm{\pi}\in Q_{S}}U_{S,\bm{\pi}}.\] (24)

Therefore, \(C_{S}(\bm{x}_{T})=\sum_{\bm{\pi}\in Q_{S}}U_{S,\bm{\pi}}/U_{S}=1\).

Combining the two cases, we can conclude that \(C_{S}(\bm{x}_{T})=\mathbbm{1}(S\subseteq T)=\prod_{i\in S}A_{i}(\bm{x}_{T})\).

### Relation between interactive concepts and multi-order interactions

In this section, we derive that high-order interactive concepts (computed via the Harsanyi dividend [18]) can be considered as elementary components for high-order interactions used in [43].

Given a pre-trained DNN \(v\) and a masked sample \(\bm{x}_{S}\), the multi-order interaction \(I^{(m)}(i,j)\) used in [43] is given as follows:

\[I^{(m)}(i,j)=\mathbb{E}_{S\subseteq N\setminus\{i,j\},|S|=m}[\Delta v(i,j,S)],\] (25)

where \(\Delta v(i,j,S)=v(\bm{x}_{S\cup\{i,j\}})-v(\bm{x}_{S\cup\{i\}})-v(\bm{x}_{S \cup\{j\}})+v(\bm{x}_{S})\).

Let \(\;\Delta v_{T}(S)=\sum_{L\subseteq T}(-1)^{|T|-|L|}v(\bm{x}_{L\cup S})\) denote the marginal benefit of variables in \(\;T\subseteq N\setminus S\), given the environment \(S\). In this way, \(\;\Delta v_{T}(S)\) can be represented as the sum of interaction effects inside \(\;T\) and sub-environments of \(S\), _i.e._\(\Delta v_{T}(S)=\sum_{S^{\prime}\subseteq S}I(T\cup S^{\prime})\)[45].

Thus, the \(I^{(m)}(i,j)\) can be represented as follows,

\[I^{(m)}(i,j)= \mathbb{E}_{S\subseteq N\setminus\{i,j\},|S|=m}[\Delta v(i,j,S)]\] \[= \mathbb{E}_{S\subseteq N\setminus\{i,j\},|S|=m}[\sum_{L\subseteq S }I(L\cup\{i,j\})]\] \[= \frac{1}{\binom{n-2}{m}}\sum_{\begin{subarray}{c}S\subseteq N \setminus\{i,j\}\\ |S|=m\end{subarray}}[\sum_{L\subseteq S}I(L\cup\{i,j\})]\] \[= \sum_{\begin{subarray}{c}L\subseteq N\setminus\{i,j\}\\ |L|\leq m\end{subarray}}I(L\cup\{i,j\})\sum_{\begin{subarray}{c}L\subseteq S \subseteq N\setminus\{i,j\}\\ |S|=m\end{subarray}}\frac{1}{\binom{n-2}{m}}\] \[= \sum_{\begin{subarray}{c}L\subseteq N\setminus\{i,j\}\\ |L|\leq m\end{subarray}}I(L\cup\{i,j\})\frac{\binom{n-2}{m-1}}{\binom{n-2}{m}}\] \[= \sum_{\begin{subarray}{c}L\subseteq N\setminus\{i,j\}\\ |L|=m\end{subarray}}\frac{\binom{n-2}{m-1}}{\binom{n-2}{m}}I(L\cup\{i,j\}).\]

Therefore, we prove that \(I^{(m)}(i,j)=\sum_{l=0}^{m}\sum_{\begin{subarray}{c}L\subseteq N\setminus\{i, j\}\\ |L|=m\end{subarray}}\frac{\binom{n-2}{m-1}}{\binom{n-2}{m}}I(L\cup\{i,j\})\).

### Theoretical connection between variance and learning difficulty

There is a close connection between the variance of interactive concepts and the learning difficulty. In fact, a similar connection has been discovered by [46]. The theoretical connection is proved as follows. Specifically, we simplify the learning of a DNN into a linear regression problem. In this very simple setting, we assume each \(i\)-th concept encoded by the DNN to be a specific feature, and use \(f_{i}\) to represent the triggering (presence) state of the \(i\)-th concept in a training sample. Because according to Eq. (9) in the main paper, the network output \(v(\cdot)\) is the sum of a small number of salient interactive concepts, we roughly simplify and rewrite the inference output of the DNN as the following linear function

\[y=\bm{w}^{\top}\bm{f}=w_{1}f_{1}+\cdots+w_{d}f_{d}\,\]

where \(\bm{f}=[f_{1},\cdots,f_{d}]^{\top}\). Then, \(w_{i}\) can be viewed as the strength of the DNN encoding the \(i\)-th interactive concept. In this way, if \(w_{i}\approx 0\), the DNN does not learn the \(i\)-th interactive concept.

We suppose that training samples are sampled from \(f\sim P(f)=N(\mu,\Sigma^{2})\) and \(y^{*}\) denotes the ground truth label. Specifically, \(\bm{\mu}=[\mu_{1},\mu_{2},...,\mu_{d}]^{\top}\) and \(\bm{\Sigma}=\mathrm{diag}(\sigma_{1}^{2},\sigma_{2}^{2},...,\sigma_{d}^{2})\).

The toy regression problem can be formulated as follows,

\[L=E_{P(f)}[\frac{1}{2}(\bm{w}^{\top}\bm{f}-y^{*})^{2}]\] (26)

We derive the optimal weights to the above regression task in three steps.

**Step 1.** The optimal weight \(w_{i}\) for the regression problem in Eq. (26) is computed by

\[\forall 1\leq i\leq d,\quad w_{i}=\frac{1}{\det\bm{K}}\det(\bm{K}_{1},\cdots, \bm{K}_{i-1},\bm{\zeta},\bm{K}_{i+1},\cdots,\bm{K}_{d})\] (27)

where \(\bm{K}=\bm{\mu}\bm{\mu}^{\top}+\bm{\Sigma}^{2}\), \(\bm{\zeta}=y^{*}\bm{\mu}\), and \(\bm{K}_{j}\) denotes the \(j\)-th column of the matrix \(\bm{K}\).

**Step 2**. We further prove that for the optimal \(\bm{w}\), we have

\[\forall 1\leq i,j\leq d,\quad\frac{|w_{i}|}{|w_{j}|}=\frac{|\mu_{i}/\sigma_{i}^{ 2}|}{|\mu_{j}/\sigma_{j}^{2}|}\] (28)

**Step 3**. Based on Step 2, we can derive \(|w_{i}|\propto|1/\sigma_{i}^{2}|\), where \(w_{i}\) denotes the strength of the DNN encoding the \(i\)-th interactive concept. \(|w_{i}|\propto|1/\sigma_{i}^{2}|\) indicates that the learning effect of the \(i\)-th interactive concept is inversely proportional to the variance of the \(i\)-th interactive concept. Therefore, interactive concepts with high variances (high-order interactive concepts) are more difficult to learn.

Experimental Settings

**Training details.** We trained AlexNet [23], VGG-11 [55], ResNet-18/20 [19] on the CIFAR-10 dataset [22] and the Tiny ImageNet dataset [25], respectively. We also trained a five-layer MLP on the UCI census dataset (namely _census dataset_) and the UCI TV news dataset (namely _TV news dataset_) [6], respectively. Each layer of the MLP contained 100 neurons. We trained each neural networks for 200 epochs with the SGD optimizer.

**Sampling details.** Since, the computational cost of \(I(S|\bm{x})\) was intolerable in real implementation, we applied the sampling-based approximation method in [67] to calculate \(I(S|\bm{x})\). Due to the high dimension of image data (_e.g._ 224 \(\times\) 224 for ImageNet), we uniformly split the input image into 8\(\times\)8 patches. Furthermore, we random sampled 12 patches and considered these patches as input variables for each image. The remaining 52 patches are set to the baseline value.

**Implementations details.** Here, we introduce how to measure \(\beta(S)\) and \(\kappa(S)\) in Section 2.3. On the Tiny ImageNet dataset, we randomly sampled 100 training images. These training images were randomly sampled from different 10 classes. On the CIFAR-10 dataset, we randomly sampled 10 training images from each class. For tabular datasets, we randomly sampled 50 training samples from each class. For image datasets, we set \(\tau=2\) when the input data is normalized by its standard deviation. For tabular datasets, we set \(\tau=1\) when the input data is normalized by its standard deviation. In this way, we set the baseline \(b_{i}=max(x_{i}-\tau,\mu)\), if \(x_{i}>\mu_{i}\), and we set the baseline \(b_{i}=min(x_{i}+\tau,\mu)\), if \(x_{i}<\mu_{i}\). For Gaussian perturbation \(\bm{\epsilon}\), we set \(\sigma=0.02\). Besides, for each training sample, we randomly sampled five Gaussian perturbation with five different seeds, respectively.

**Adversarial attack.** Here, we introduce how to measure \(A^{(s)}\) in Section 3.1. For tabular datasets, we randomly sampled 50 training samples from each class from the training set. On the Tiny-ImageNet dataset, we randomly sampled 100 training images. These training images were randomly sampled from different 10 classes. On the CIFAR-10 dataset, we randomly sampled 10 training images from each class. We used the \(l_{\infty}\) untargeted PGD attack by following [33], in which the constraint \(\epsilon=16/255\), and the attack was conducted with 5 steps with the step size \(\epsilon=3/255\).