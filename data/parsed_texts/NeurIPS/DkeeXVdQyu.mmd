# How to Scale Your EMA

Dan Busbridge\({}^{\ast}\)  Jason Ramapuram\({}^{\ast}\)  Pierre Ablin\({}^{\ast}\)  Tatiana Likhomanenko\({}^{\ast}\)

Eeshan Gunesh Dhekane  Xavier Suau  Russ Webb

###### Abstract

Preserving training dynamics across batch sizes is an important tool for practical machine learning as it enables the trade-off between batch size and wall-clock time. This trade-off is typically enabled by a scaling rule, for example, in stochastic gradient descent, one should scale the learning rate linearly with the batch size. Another important machine learning tool is the model EMA, a functional copy of a target model, whose parameters move towards those of its target model according to an Exponential Moving Average (EMA) at a rate parameterized by a momentum hyperparameter. This model EMA can improve the robustness and generalization of supervised learning, stabilize pseudo-labeling, and provide a learning signal for Self-Supervised Learning (SSL). Prior works have not considered the optimization of the model EMA when performing scaling, leading to different training dynamics across batch sizes and lower model performance. In this work, we provide a scaling rule for optimization in the presence of a model EMA and demonstrate the rule's validity across a range of architectures, optimizers, and data modalities. We also show the rule's validity where the model EMA contributes to the optimization of the target model, enabling us to train EMA-based pseudo-labeling and SSL methods at small and large batch sizes. For SSL, we enable training of BYOL up to batch size 24,576 without sacrificing performance, a 6\(\times\) wall-clock time reduction under idealized hardware settings.

## 1 Introduction

With data and models becoming progressively larger (Chen et al., 2020; Kaplan et al., 2020; Bommasani et al., 2021; Srivastava et al., 2022), the ability to reduce training wall-clock time is a requirement for practical Machine Learning (ML) at scale. Optimizer scaling rules allow us to find faster learning procedures that produce similar results. For example, the _linear scaling rule_ for Stochastic Gradient Descent (SGD) (Krizhevsky, 2014; Goyal et al., 2017), states that the learning rate should be scaled linearly with the batch size. This optimizer scaling works _both ways_. Access to larger computational resources means one can train equivalent models in reduced wall-clock time. Alternatively, with access to limited computational resources, larger distributed computations can be replicated at increased wall-clock time.

Many ML algorithms rely on a _model EMA_, a functional copy of a _target model_2, whose parameters move towards those of its target model according to an Exponential Moving Average (EMA) (Definition 1.1) at a rate parameterized by a momentum hyperparameter \(\rho\).

Footnote 2: The target model usually undergoes gradient-based optimization, but this does not have to be the case.

**Definition 1.1** (EMA Update).: _The EMA update for the model EMA parameters \(\xi_{t}\) following target model parameters \(\theta_{t}\) at iteration \(t\) with momentum \(\rho\equiv 1-\beta_{\rho}\) is_

\[\xi_{t+1}=\rho\,\xi_{t}+(1-\rho)\,\theta_{t}\equiv(1-\beta_{\rho})\,\xi_{t}+ \beta_{\rho}\,\theta_{t}.\] (1)

The _model EMA_ has a number of desirable properties: i) the model EMA inhabits wider minima than the target model, reducing overfitting and improving generalization (Ruppert, 1988; Polyak and Juditsky, 1992; Huang et al., 2017; Izmailov et al., 2018; He et al., 2022); ii) compared to the target model, the model EMA moves slowly, making it useful as a stabilizer for networks governing Bellman updates in reinforcement learning, (Lillicrap et al., 2016); and iii) the model EMA is relatively cheap to compute, whilst providing a valid model but _different_ to the target model. This third property has made the model EMA a common choice for the _teacher_ in many distillation setups, from semi-supervised learning (Tarvinen and Valpola, 2017; Sohn et al., 2020; Manohar et al., 2021; Higuchi et al., 2022), to Self-Supervised Learning (SSL) methods like Bootstrap Your Own Latent (BYOL) (Grill et al., 2020), DINO (Caron et al., 2021), and data2vec (Baevski et al., 2022, 2020).

Despite its significant role in optimization, a recipe for adapting the EMA Update (Definition 1.1) when changing batch size has, to the best of our knowledge, been absent. To address this, we derive an EMA Scaling Rule (Definition 1.2) which states how the EMA momentum \(\rho\) hyperparameter _should_ be modified3.

Footnote 3: We stress that the study of momentum in gradient-based optimizers is not the focus of this work. We refer to Smith and Le (2018); Li et al. (2019) for a discussion on scaling rules for these methods.

**Definition 1.2** (EMA Scaling Rule).: _When computing the EMA update (Definition 1.1) of a model undergoing stochastic optimization with batch size \(\hat{B}=\kappa B\), use a momentum \(\hat{\rho}=\rho^{\kappa}\) and scale other optimizers according to their own scaling rules._

In Definition 1.2, the momentum \(\rho\), which is defined at batch size \(B\), typically corresponds to a "good hyperparameter choice", although this does not need to be the case in general. In this paper, we make the following contributions.

1. With the assumptions of Goyal et al. (2017), we derive an EMA Scaling Rule: the EMA update _momentum_ should be scaled _exponentially_ with the batch size (Definition 1.2).
2. To validate this EMA Scaling Rule theoretically, we propose Stochastic Differential Equation (SDE) approximations of optimization in the presence of a model EMA (Section 2.2). This model EMA contributes to the loss, covering semi-supervised learning and SSL. We prove that these approximations are first order weak approximations, and that our EMA Scaling Rule is correct in the SDE limit under realistic gradient assumptions (Corollary 2.1.1).
3. We empirically validate the EMA Scaling Rule in synthetic settings (Section 3.1) and real-world settings where the model EMA plays an increasingly significant role in optimization: i) where the model EMA is used during inference instead of the target model (Section 3.2); ii) pseudo-labeling, where the model EMA (_teacher_) follows the target model (_student_), and the _student_ is optimized on a mixture of a) labeled data and b) data without labels, whose pseudo-labels are produced by the _teacher_ (Section 3.3); and iii) self-supervised learning, which is the same as the semi-supervised case, except there is no labeled data (Section 3.4).
4. We observe that pseudo-labeling and SSL training dynamics during optimizer warm-up are not always able to be replicated at large batch sizes using _only_ the EMA Scaling Rule. We propose and verify practical methods to overcome this limitation, enabling us to scale to a batch size of 24,576 with BYOL Vision Transformers (ViTs), reducing wall-clock training by 6\(\times\) under idealized hardware scenarios while maintaining performance of the batch size 4096 baseline.

Finally, to aid practitioners looking to scale, in Appendix C we provide a _Scaling Toolbox_, which gives practical advice on how to scale systematically, collecting known scaling rules, and explaining how to think about the SDE perspective of optimization.

The EMA Scaling Rule

We begin with an informal discussion of scaling rules and motivate the existence of an exponential scaling rule for the momentum parameter controlling the update of the model EMA.

### Background and an informal discussion of scaling rules

Consider a model with parameters \(\bm{\theta}_{t}\) at iteration \(t\) updated with SGD (Definition 2.1).

**Definition 2.1** (SGD Update).: _The SGD update for a model with parameters \(\bm{\theta}_{t}\) at iteration \(t\) given a minibatch \(\mathbb{B}=\{x^{(b)}\sim P_{\mathbf{x}}:b=1,2,\dots,B\}\) of \(B=|\mathbb{B}|\) samples with learning rate \(\eta\) is_

\[\bm{\theta}_{t+1}=\bm{\theta}_{t}-\eta\times\frac{1}{B}\sum_{x\in\mathbb{B}} \nabla_{\bm{\theta}}\mathcal{L}(x;\bm{\theta}_{t}),\] (2)

_where \(\mathcal{L}\) is the loss function, \(\nabla_{\bm{\theta}}\mathcal{L}(x;\theta_{t})\) is the parameter gradient for the sample \(x\) at iteration \(t\), and the \(x\in\mathbb{B}\) are Independent and Identically Distributed (i.i.d.) from \(P_{\mathbf{x}}\)._

Iterating over a sequence of independent minibatches \(\mathbb{B}_{0},\mathbb{B}_{1},\dots,\mathbb{B}_{\kappa-1}\) produces model parameters

\[\bm{\theta}_{t+\kappa}=\bm{\theta}_{t}-\eta\times\frac{1}{B}\sum_{j=0}^{\kappa -1}\sum_{x\in\mathbb{B}_{j}}\nabla_{\bm{\theta}}\mathcal{L}(x;\bm{\theta}_{t+ j}).\] (3)

If gradients vary slowly \(\nabla_{\bm{\theta}}\mathcal{L}(x;\theta_{t+j})\approx\nabla_{\bm{\theta}} \mathcal{L}(x;\theta_{t})\), \(j=0,\dots,\kappa-1\), _one_ SGD step with \(\hat{\eta}=\kappa\eta\) on a batch \(\widehat{\mathbb{B}}=\cup_{i}\mathbb{B}_{i}\) of size \(\hat{B}=\kappa B\) results in \(\hat{\bm{\theta}}_{t+1}\approx\bm{\theta}_{t+k}\), yielding the SGD Scaling Rule (Definition 2.2).

**Definition 2.2** (SGD Scaling Rule).: _When running SGD (Definition 2.1) with batch size \(\hat{B}=\kappa B\), use a learning rate \(\hat{\eta}=\kappa\eta\)(Krizhevsky, 2014; Goyal et al., 2017)._

For clarity in this work, we adopt the naming convention _[Algorithm Name] Scaling Rule_, which means all parameters of those algorithms are appropriately scaled from batch size \(B\) to \(\kappa B\).

As discussed in Goyal et al. (2017), although the assumption of slowly changing gradients is strong, if it is true, then \(\bm{\theta}_{t+k}\approx\hat{\bm{\theta}}_{t+1}\)_only_ if \(\hat{\eta}=\kappa\eta\). The validity of the SGD Scaling Rule has been formally studied. In particular, there was ambiguity regarding whether the scaling should be a square-root or linear (Krizhevsky, 2014). SDE approaches have resolved this ambiguity, and have been used to estimate the scaling \(\kappa\) when the SGD Scaling Rule is no longer guaranteed to hold (Li et al., 2021).

To address model parameter EMAs, we first restate the EMA Update (Definition 1.1).

**Definition 1.1** (EMA Update).: _The EMA update for the model EMA parameters \(\xi_{t}\) following target model parameters \(\bm{\theta}_{t}\) at iteration \(t\) with momentum \(\rho\equiv 1-\beta_{\rho}\) is_

\[\xi_{t+1}=\rho\,\xi_{t}+(1-\rho)\,\bm{\theta}_{t}\equiv(1-\beta_{\rho})\,\xi _{t}+\beta_{\rho}\,\bm{\theta}_{t}.\] (1)

The model EMA parameters \(\xi\) do not typically receive gradient information, we take the convention that \(\rho\) is close to one, and the \(\beta_{\rho}\) subscript will be omitted where it is clear from the context.

Assuming again that gradients change slowly \(\nabla_{\bm{\theta}}\mathcal{L}(x;\bm{\theta}_{t+j},\xi_{t+j})\approx\nabla_{ \bm{\theta}}\mathcal{L}(x;\bm{\theta}_{t},\xi_{t})\approx\bm{\mathrm{g}}\), for gradient \(\bm{\mathrm{g}}\), iterating over \(\kappa\) independent minibatches produces model states (see Appendix E.1 for derivation)

\[\begin{bmatrix}\bm{\theta}_{t+\kappa}\\ \xi_{t+\kappa}\\ \bm{\mathrm{g}}\end{bmatrix}=\begin{bmatrix}1&0&-\eta\\ (1-\rho)&\rho&0\\ 0&0&1\end{bmatrix}^{\kappa}\cdot\begin{bmatrix}\bm{\theta}_{t}\\ \xi_{t}\\ \bm{\mathrm{g}}\end{bmatrix}=\begin{bmatrix}\theta_{t}-\eta\,\kappa\,\bm{ \mathrm{g}}\\ \rho^{\kappa}\,\xi_{t}+(1-\rho^{\kappa})\,\bm{\mathrm{g}}+\mathcal{O}\left(\eta \times\beta_{\rho}\right)\\ \bm{\mathrm{g}}\end{bmatrix}.\] (4)

The first row is the SGD Scaling Rule (Definition 2.2). The third row implements the _slowly changing gradients_ assumption for the first row. The second row is equivalent to a single EMA update (Definition 1.1) with momentum \(\hat{\rho}=\rho^{\kappa}\); we can take a _single_ SGD update with batch size \(\hat{B}=\kappa B\) and learning rate \(\hat{\eta}=\kappa\eta\), and a _single_ EMA update with momentum \(\hat{\rho}=\rho^{\kappa}\), and we get \((\hat{\bm{\theta}}_{t+1},\hat{\bm{\xi}}_{t+1})\approx(\bm{\theta}_{t+\kappa}, \xi_{t+\kappa})\) up to terms \(\mathcal{O}(\eta\times\beta_{\rho})\). This yields the EMA Scaling Rule (Definition 1.2).

**Definition 1.2** (EMA Scaling Rule).: _When computing the EMA update (Definition 1.1) of a model undergoing stochastic optimization with batch size \(\hat{B}=\kappa B\), use a momentum \(\hat{\rho}=\rho^{\kappa}\) and scale other optimizers according to their own scaling rules._

The EMA Scaling Rule was derived for SGD, and is extended to other optimizers in the following way. An optimizer scaling rule ensures \(\hat{\bm{\theta}}_{t+1}=\bm{\theta}_{t+\kappa}\), satisfying identification for the first row. Next, the zeroth order term in \(\eta\times\beta_{\rho}\) in the second row in Equation 4 is optimizer-independent, and therefore unchanged. Finally, the first order terms in \(\eta\times\beta_{\rho}\) in the second row, corresponding to the scaling rule error, are an EMA accumulation of target model \(\bm{\theta}\) updates under optimization, which is still \(\mathcal{O}(\eta\times\beta_{\rho})\), although its functional form may be different for different optimizers.

The above discussion is intended to give an intuition for why the EMA momentum should be scaled exponentially. As we have used the same slow-moving gradient assumption as the original SGD Scaling Rule, this may cast doubt on whether our rule is correct. To remove this ambiguity, we will follow Smith and Le (2018); Li et al. (2021); Malladi et al. (2022), and show that the EMA Scaling Rule (Definition 1.2) is correct in the SDE limit under more realistic gradient assumptions.

### The EMA Scaling Rule through the lens of stochastic differential equations

SDEs are a tool typically used to obtain scaling rules from first principles (Li et al., 2021; Malladi et al., 2022). In the following, we use SDEs to obtain strong theoretical guarantees for the EMA Scaling Rule found in Section 2.1. We consider the following discrete dynamics for EMA:

\[\begin{split}&\theta_{k+1}=\theta_{k}-\eta\,\mathrm{g}_{k},\ \ \text{with}\ \mathrm{g}_{k}=\nabla f(\theta_{k},\xi_{k})+\sigma\,\bm{\epsilon}_{k},\ \text{and}\ \bm{\epsilon}_{k}\sim\mathcal{E}_{\sigma}(\theta_{k},\xi_{k}),\\ &\xi_{k+1}=\rho\,\xi_{k}+(1-\rho)\,\theta_{k},\end{split}\] (5)

where \(\sigma>0\) is the noise scale, \(\mathcal{E}_{\sigma}(\theta_{k},\xi_{k})\) is the gradient noise distribution, assumed to be zero-mean and variance \(\Sigma(\theta_{k},\xi_{k})\) independent of \(\sigma\), and \(\nabla f(\theta_{k},\xi_{k})\equiv\nabla_{\bm{\theta}}f(\theta_{k},\xi_{k})\). We posit a dependency of the loss \(f\) on the EMA \(\xi\) in order to cover semi-supervised (Section 3.3) and SSL (Section 3.4). The case of Polyak-Ruppert averaging (Section 3.2), is covered by letting \(f\) be independent of \(\xi\).

We aim to obtain an SDE approximation of Equation 5 as \(\eta\) goes to zero. The scaling rule for iterations of \(\bm{\theta}\) is well known (Li et al., 2021): we let \(\sigma_{0}=\sigma\sqrt{\eta}\). The analysis of Section 2.1 gives the scaling rule \(\hat{\eta}=\eta\kappa\) and \(\hat{\rho}=\rho^{\kappa}\). Linearizing this rule near \(\eta=0\) gives \(\hat{\rho}=1-\kappa\times(1-\rho)\), which is a linear relationship between \(1-\rho\) and \(\eta\). We therefore let \(\beta_{0}=(1-\rho)/\eta\) and consider the SDE

\[\begin{split}& d\Theta_{t}=-\nabla f(\Theta_{t},Z_{t})\,dt+ \sigma_{0}\,\Sigma(\Theta_{t},Z_{t})^{\frac{1}{2}}\,dW_{t},\ \ \text{with}\ W_{t}\ \text{a Wiener process},\\ & dZ_{t}=\beta_{0}(\Theta_{t}-Z_{t})dt,\end{split}\] (6)

where \(\Theta_{t}\) and \(Z_{t}\) are SDE variables relating to model and EMA parameters respectively. The SDE in Equation 6 approximates the discrete iterations of Equation 5 when the learning rate \(\eta\) goes to zero. One way to see this is that an Euler-Maruyama discretization of the SDE with learning rate \(\eta\) exactly recovers the discrete iterations. More formally, we have Theorem 2.1, which is in the same spirit as those found in Li et al. (2021); Malladi et al. (2022). In the theorem, \(G^{\alpha}\) is the set of functions with derivatives up to order \(\alpha\) that have at most polynomial growth (see Definition D.1).

**Theorem 2.1** (SDE for SGD + EMA; informal see Theorem D.1).: _Assume that \(f\) is continuously differentiable, with \(f\in G^{3}\). Let \(\Theta_{t},Z_{t}\) be solutions of Equation 6, and \(\bm{\theta}_{k},\xi_{k}\) iterations of Equation 5 with \(\Sigma^{\frac{1}{2}}\in G^{2}\). Then, for any time horizon \(T>0\) and function \(g\in G^{2}\), there exists a constant \(c>0\) independent of \(\eta\) such that_

\[\max_{k=0,\,\dots,\,\lfloor T/\eta\rfloor}\left|\mathbb{E}[g(\Theta_{\eta k},Z_{\eta k})]-\mathbb{E}[g(\theta_{k},\xi_{k})]\right|\leq c\times\eta.\] (7)

Theorem 2.1 formalizes the intuition that the SDE is an accurate approximation of the discrete iterations. In turn, it allows validating the scaling rule in the same spirit as in Malladi et al. (2022).

**Corollary 2.1.1** (Validity of the EMA Scaling Rule).: _Assume that \(f\) is continuously differentiable, with \(f\in G^{3}\) and \(\Sigma^{\frac{1}{2}}\in G^{2}\). Let \(\theta^{(B)}_{k},\xi^{(B)}_{k}\) be iterations of Equation 5 with batch size \(B\) and hyperparameters \(\eta,\rho\). Let \(\bm{\theta}^{(\kappa B)}_{k},\xi^{(\kappa B)}_{k}\) be iterates with batch size \(\kappa B\), and \(\hat{\eta}\) determined by the SGD Scaling Rule (Definition 2.2) and \(\hat{\rho}\) determined by the EMA Scaling Rule (Definition 1.2). Then, for any time horizon \(T>0\) and function \(g\in G^{2}\), there exists a constant \(c>0\) independent of \(\eta\) such that_

\[\max_{k=0,\,\dots,\,\lfloor T/\eta\rfloor}\left|\mathbb{E}[g(\bm{\theta}^{( \kappa B)}_{\lfloor k/\kappa\rfloor},\bm{\zeta}^{(\kappa B)}_{\lfloor k/\kappa \rfloor})]-\mathbb{E}[g(\bm{\theta}^{(B)}_{k},\bm{\zeta}^{(B)}_{k})]\right| \leq c\times\eta.\] (8)Corollary 2.1.1 shows that two trajectories with different batch sizes are close in the limit of small learning rate, demonstrating the validity of Definition 1.2. A natural follow-up question is _what happens when an adaptive optimizer is used instead of SGD?_ Malladi et al. (2022) study this without an EMA and characterize how hyperparameters change with the noise scale. In particular, they show that under a high gradient noise hypothesis, there exists a limiting SDE. In Appendix D, we derive the limiting SDEs for RMSProp and Adam with an EMA. Although a formal proof of closeness between the iterations and these SDEs is beyond the scope of this work, these SDEs indicate that the EMA Scaling Rule holds for adaptive algorithms. We demonstrate this empirically in Section 3.

## 3 Experiments

Now that we have derived and shown the validity of the EMA Scaling Rule, we verify it empirically. The experiments validate the EMA Scaling Rule for a variety of uses of EMA and are ordered by increasing influence of the role of EMA on the optimization procedure (see Table 1). The baseline in all of our experiments is _without the EMA Scaling Rule_, which applies all known relevant scaling rules _except_ the EMA Scaling Rule, and represents previous best practice.

### Polyak-Ruppert averaging in a simple setting

At inference, it is typical to use a model EMA, known as Polyak-Ruppert Averaging (Definition 3.1).

**Definition 3.1** (Polyak-Ruppert Average).: _When optimizing model parameters \(\bm{0}\), compute their EMA \(\xi\) (Definition 1.1). Use \(\xi\) instead of \(\bm{0}\) at inference (Polyak & Juditsky, 1992; Ruppert, 1988)._

We begin by showing the EMA Scaling Rule is _required_ to match parameter trajectories in a simple setting. Consider the optimization of \(\theta\) in a _noisy parabola_ whose loss \(\mathcal{L}(\theta)\) is parameterized by coefficients for curvature \(a>0\), scaled additive noise \(b\geq 0\), and additive noise \(c\geq 0\):

\[\mathcal{L}(\theta)=\frac{a}{2}\,\theta^{2},\qquad\quad\theta_{k+1}=\theta_{k }-\eta\,\mathsf{g}_{k},\qquad\quad\mathsf{g}_{k}=a\,\theta_{k}+\epsilon_{k}, \qquad\quad\epsilon_{k}\sim\mathcal{N}\left(0,\tfrac{b\,\mathsf{g}_{k}^{2}+ \epsilon}{\kappa}\right).\] (9)

The scaling factor \(\kappa\) in the covariance denominator implements gradient noise reduction as scaling (i.e. batch size) increases (Jastrzebski et al., 2017). Let \(\theta\in\mathbb{R}\) be optimized with SGD (Definition 2.1) and \(\zeta\in\mathbb{R}\) be a Polyak-Ruppert average (Definition 3.1) for \(\theta\) with momentum \(\rho=1-\beta\). At scaling \(\kappa=1\), we use \(\beta_{B}=\eta_{B}=10^{-4}\) and \(I_{B}=10^{4}\) iterations, to yield a total time \(T=I_{B}\times\eta_{B}=1\). To keep gradients \(\mathcal{O}(1)\) and gradient noise non-negligible, we take \(a=1\), \(b=0.5\), and \(c=0\).

First, we observe the effect of scaling on a single run (Figure 0(a)) by tracking the position of the model EMA. We see that at scaling \(\kappa=8\) or \(\kappa=256\), the runs using the EMA Scaling Rule match the baseline trajectory, whereas the runs using the baseline momentum do not, with a greater deviation induced by greater scaling \(\kappa\). Even at \(\kappa=8\), there is a significant difference between scaled and unscaled trajectories, despite the seemingly small numerical difference of their momenta4.

Footnote 4: Momentum enters optimization exponentially; small changes can lead to very different updates.

Second, we consider whether the EMA Scaling Rule is optimal. To do this, inspired by the SDE analysis (Section 2.2), we define the approximation error, \(\operatorname{Err}(\rho,\kappa,g)\), of a test function \(g\) for a given

\begin{table}
\begin{tabular}{p{113.8pt} p{113.8pt}} \hline \hline Technique & Role of Model EMA \\ \hline Polyak-Ruppert average, Sec. 3.2 & \(\bm{0}\) undergoes optimization and is tracked by \(\xi\), which does not affect \(\bm{0}\). \(\xi\) is an estimate of \(\bm{0}\) with a time horizon and variance determined by \(B\) and \(\rho\). \\ \hline Continuous pseudo-labeling, Sec. 3.3 & _Pre-Training_ is as above in Polyak-Ruppert Averaging. _After Pre-Training_, \(\xi\) (_teacher_) produces targets for \(\bm{0}\) (_student_) from unlabeled data, which is combined with labeled data. The optimization endpoint is dependent on \(B\) and \(\rho\). \\ \hline Self-supervised Learning, Sec. 3.4 & As above in _After Pre-Training_, except there is no labeled data. The optimization endpoint is dependent on \(B\) and \(\rho\). \\ \hline \hline \end{tabular}
\end{table}
Table 1: The role of the model EMA \(\xi\) in the optimization of \((\bm{0},\xi)\) given a target model \(\bm{0}\) for different techniques, ordered by increasing influence of the EMA model. All statements assume a momentum \(0\leq\rho<1\) and that the target model \(\bm{0}\) is subject to stochastic optimization at a batch size \(B\).

scaling \(\kappa\) using momentum \(\rho\), and the value of the momentum \(\rho^{*}(\kappa,g)\) that minimizes this error:

\[\rho^{*}(\kappa,g)= \operatorname*{arg\,min\,Err}(\rho,\kappa,g),\qquad\operatorname{ Err}(\rho,\kappa,g)\equiv\max_{k=0,\dots,T/\eta}\left|\mathbb{E}\,g(\xi_{k})- \mathbb{E}\,g(\xi_{k/\kappa}^{(\kappa,\rho)})\right|.\] (10)

For scalings \(\kappa\in\{1,2,4,\dots,1024\}\), we determine the optimal momentum \(\rho^{*}\) and compare it to the EMA Scaling Rule (Figure 0(b), left). The scaling rule tracks the \(\rho^{*}\) until \(\kappa=256\), when the \(\rho^{*}\) become systematically higher. We see target model error increase at \(\kappa=256\) (Figure 0(b), right). As the target model error is EMA-independent, this indicates that the SGD Scaling Rule is breaking. At the lower scaling \(\kappa=64\), there is an inflection point in the EMA Scaling Rule approximation error, before the model error grows. This difference indicates the \(O(\eta\times\beta_{\rho})\) terms of Equation 4 are beginning to influence the EMA update. Finally, these observations are true in \(D=100\) dimensions, (Appendix F.1), and we stress that _not_ changing the momentum at every scaling \(\kappa\) induces large approximation error, indicating there is merit to using the EMA Scaling Rule.

### Supervised learning on real data with Polyak-Ruppert averaging

We now turn to real-world classification where the target model \(\mathbf{\theta}\) optimizes a parametric log-likelihood \(\max_{\mathbf{\theta}}\log p(\mathbf{y}|\mathbf{x};\mathbf{\theta})\) with inputs and labels \((\bm{x},\bm{y})\) drawn from a joint distribution \(p(\mathbf{y},\mathbf{x})\).

Image ClassificationWe consider a variant of the original SGD Scaling Rule result (Goyal et al., 2017) and train a ResNetv2 (He et al., 2016) on ImageNet1k (Russakovsky et al., 2014) (Figure 2) using a three step learning rate schedule. The base momentum \(\rho_{B}=0.9999\) at batch size 1024 was found by hyperparameter optimizing for EMA test performance, and we seek to achieve this optimized performance at different batch sizes. We _do not_ apply the EMA Scaling Rule on the Batch Normalization (Ioffe and Szegedy, 2015) statistics5. We observe that _without_ the EMA Scaling Rule, there is a significant drop in model EMA test performance, whereas _with_ the EMA Scaling Rule, we

Figure 1: (a) We show the effect of scaling by comparing model EMA trajectories of the baseline (\(\kappa=1\), black dashed) to \(\kappa=8\) (left) and \(\kappa=256\) (right), with (\(\rho=\rho_{B}^{\kappa}\), blue) and without (\(\rho=\rho_{B}\), red) the EMA Scaling Rule. (b, left) The momentum according for different scaling rules and the empirically optimal \(\rho^{*}\) (Equation 10). (b, right) The approximation error (Equation 10) of trajectories in (b, left) and the target model (orange).

Figure 2: _ResNetv2-50 Polyak-Ruppert averaging on ImageNet1k_ for different scalings \(\kappa\). The baseline model (\(\kappa=1\), black dashed) uses batch size 1024 and momentum \(\rho_{B}=0.9999\), is scaled down to a batch size of 512 (left), and up to a batch size of 4096 (right) with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule (Definition 1.2). Bands indicate the mean and standard deviation across three runs.

can approximate the baseline model EMA test top-1 performance across all batch sizes. We match baseline EMA statistics across the full trajectory batch size 2048, where the test EMA performance diverges. This is due to non-EMA test performance dropping for high \(\kappa\) (see Appendix F.2). We observe that model EMA top-1 is approximately 0.2% to 0.3% higher than the target model.

**Automatic Speech Recognition (ASR)** We train a transformer (Vaswani et al., 2017) using the Connectionist Temporal Classification (CTC) loss (Graves et al., 2006) and Adam optimizer on the _train-clean-100_ subset (100h) of LibriSpeech (Panayotov et al., 2015) (for details see Appendix G). We apply the Adam Scaling Rule (Malladi et al. (2022), Definition C.3) and use dynamic batching (minibatch size \(\times\) sequence length = const = 290s, and \(s\) indicates audio duration in seconds).

_Without_ the EMA Scaling Rule, there is a significant difference in model EMA test Word Error Rate (WER) trajectories compared to the baseline, whereas _with_ the EMA Scaling Rule, trajectories match, as is shown in Figure 3. We note that compared to image classification, in ASR, the model EMA converges to similar final performance irrespective of use of the scaling rule. This convergence is due to the longer training time compared to the EMA horizon as discussed in Table 1 (see Appendix E.2 for a proof sketch). Although in this specific case one can achieve similar _final performance_ without the EMA Scaling Rule, it is _necessary_ to use the EMA Scaling Rule in order to replicate the full training trajectory, which gives _guarantees_ on properties like final performance (see Corollary 2.1.1). We also observe a growing gap between the baseline and EMA-scaled trajectories as we increase \(\kappa\). Inspecting the train loss and non-EMA test WER, which _do not_ depend on the EMA update (see Figure 14, Appendix G.1), indicates this is due to a breakdown of the Adam Scaling Rule. _In summary, evaluation on ASR shows that the EMA Scaling Rule holds in practice for sequential data with dynamic batch sizes, as well as when using adaptive optimization._

### Semi-supervised speech recognition via pseudo-labeling

We continue using the same ASR model and training pipeline of Section 3.2. However, we consider semi-supervised learning via continuous pseudo-labeling where labeled (_train-clean-100_, 100h) and unlabeled (the rest of LibriSpeech, 860h) data are given during training, and the model EMA is involved in the overall optimization (Likhomarenko et al., 2021, 2022; Manohar et al., 2021; Higuchi et al., 2022). We first pre-train a target model (_student_) on a limited labeled set for a short period (e.g. 20k steps of \(B=8\times 290s^{6}\)). Concurrently, the student updates a model EMA (_teacher_). After pre-training, we continue training the student with both labeled and unlabeled data, with the teacher first transcribing unlabeled data from the batch producing Pseudo-Labels (PLs). These PLs are treated by the student as ground-truth transcriptions, and standard supervised optimization is performed.

Compared to Polyak-Ruppert Averaging (Section 3.2), where the model EMA plays no role in the joint optimization, we observe that in PL it is _essential_ to employ the EMA Scaling Rule in order to match the model trajectories at scaled batch sizes. When the EMA Scaling Rule is not used, Figure 4 reveals a significant difference in PL quality trajectory, leading to a higher test WER.

For \(\kappa>2\), we found the Adam Scaling Rule does not perfectly match the reference trajectory in the pre-training phase. This results in a significantly different PL quality at the start of pseudo-labeling (20k steps of \(B=8\times 290s\)), which affects the training dynamics (Berrebbi et al., 2023). To

Figure 3: _Transformer Polyak-Ruppert averaging on LibriSpeech (trained on train-clean-100)_ with different scalings \(\kappa\). The baseline (\(\kappa=1\), black dashed) is trained with Adam and momentum \(\rho_{B}=0.99995\) at a _dynamic batch size_\(B=8\times 290s\), which corresponds to a single train step on the \(x\)-axis. We investigate dynamic batch sizes down to \(B=2\times 290s\) (left) and up to \(B=32\times 290s\) (right), with (blue, \(\rho=\rho_{B}^{\text{s}}\)), and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule. The Adam Scaling Rule (Malladi et al. (2022), Definition C.3) is used throughout.

alleviate the Adam Scaling Rule mismatch effect for \(\kappa>2\), we postpone the pseudo-labeling until pre-training on labeled data gives similar validation WER, see Appendix G. With this heuristic, we can match the baseline trajectory with the EMA Scaling Rule up to \(\kappa=8\) (Figure 4).

_In summary, (a) model EMA affects the optimization process of pseudo-labeling in ASR resulting in the necessity of EMA Scaling Rule to be applied while scaling the batch size; (b) an optimizer scaling rule breakdown results in the EMA Scaling Rule breakdown but this effect can be alleviated by longer pre-training on labeled data having similar PLs quality at the start across different scalings._

### Self-supervised image representation learning

Finally, we turn our attention to distillation based Self-Supervised Learning (SSL). where the model EMA is the _teacher_(Grill et al., 2020; Nizzumi et al., 2023; Caron et al., 2021; Oquab et al., 2023).

We will use BYOL (Grill et al. (2020), Definition 1.1)7 for our investigation into scaling as it is well-studied (Tian et al., 2021; Richemond et al., 2023), relatively simple to implement due to minimal hyper-parameters, and obtains competitive results (Grill et al., 2020; Koppula et al., 2022). Since BYOL learns through self-referential distillation, momentum plays a significant role in optimization. We analyze: i) a ResNet-18 (He et al., 2016) on CIFAR10 (Krizhevsky et al., 2014) (Figure 5) using SGD (Definition 2.1); and ii) a ViT-B/16 (Dosovitskiy et al., 2021) on ImageNet1k using AdamW (Loshchilov and Hutter, 2019). A recipe for BYOL using ViTs is provided in Appendix H.3.

Footnote 7: The BYOL EMA update (Equation 74) uses \(\theta_{t+1}\) instead of our analyzed \(\theta_{t}\) (Equation 4). The effect upon the overall EMA update is \(\mathcal{O}(\eta\times\beta_{\rho})\) and so is captured by the EMA Scaling Rule (Definition 1.2).

**ResNet-18 on CIFAR-10** We begin with a ResNet-18 model and short training duration to enable quick iteration, and an SGD optimizer as it has as _known_ scaling rule. This allows us to probe the EMA Scaling Rule without potential confounders like poor gradient-based optimizer scaling8.

Footnote 8: For competitive performance with the reference BYOL (Grill et al., 2020) using a ResNet-50, adaptive optimization, and longer training duration, see Appendix H.10 and Figure 26.

We observe that _without_ the EMA Scaling Rule, there is a drop in test top-1 linear probe (Definition H.3) performance compared to the baseline, whereas _with_ the EMA Scaling Rule, we closely match the baseline model until batch size 4096. We show that this result is consistent for a range of base learning rates \(\eta_{B}\) and momenta \(\rho_{B}\) in Appendix H.8. At batch size 8192, we see a performance gap between the scaled model using the EMA Scaling Rule and the baseline. We speculate that this is due to dynamics early in the BYOL training process that are challenging to replicate at larger batch sizes. To test, and potentially circumvent this, we introduce _Progressive Scaling_ (Definition 3.2).

**Definition 3.2** (Progressive Scaling, informal; see Appendix C.4).: _Given batch size \(B\) and hyperparameters at \(B\), slowly increase the batch size to the desired largest batch size during training. At any intermediate batch size \(\tilde{B}=\kappa B\), all hyperparameters are scaled according to their scaling rules._

Figure 4: _Transformer pseudo-labeling on LibriSpeech with different scalings \(\kappa\). The baseline (\(\kappa=1\), black dashed) is trained with Adam at a dynamic batch size of \(8\times 290\) seconds, which corresponds to a single train step on the \(x\)-axis. The model EMA (teacher) is updated with momentum \(\rho_{B}=0.9999\). We investigate dynamic batch sizes down to \(B=4\times 290s\) (left) and up to \(B=64\times 290s\) (right), with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule. The Adam Scaling Rule (Malladi et al. (2022), Definition C.3) is used throughout. For \(\kappa\leq 2\), we start pseudo-labeling after \(20\kappa/\kappa\) training steps; while for \(\kappa>2\), we start when pre-training WER matches the baseline WER._

We see that transitioning to the higher batch size _during_ the warmup period results in a model optimization trajectory that diverges from the baseline, whereas transitioning _after_ warmup results in matching final trajectories of the scaled and baseline models. In summary, _progressive scaling_ allows us to match BYOL dynamics at large batch sizes, provided we transition after the warmup period. This observation is consistent with our hypothesis regarding BYOL dynamics during warmup.

Vision Transformers on ImageNet1kProgressive Scaling coupled with the EMA Scaling Rule is required when scaling BYOL ViTs (Figure 6), enabling baseline loss tracking to a batch size of 24,576. Perfect scaling fails at batch size 32,768, consistent with observations in supervised learning (Goyal et al., 2017; Huo et al., 2021). Despite the breakdown, there is only a small drop in 1.6% probe performance when using the EMA Scaling Rule, compared to as 44.56% drop _without_ it. We also observe that it is sometimes possible to match test model performance using _only_ Progressive Scaling and _not_ the EMA Scaling Rule, although this still induces a training loss mismatch. We stress that such an approach is _not_ guaranteed to work and discuss when this approach succeeds and fails in Appendix H.6 and Figure 22.

Figure 5: _ResNet-18 BYOL on CIFAR10_ for different \(\kappa\). The baseline (\(\kappa=1\), black dashed) uses batch size 1024 and momentum \(\rho_{B}=0.992\), and is scaled from batch size 2048 (left) to 8192 (third) with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule. At \(\kappa=8\) we also run _progressive scaling_ (right), with transitions at 10 (green) and 30 (orange) epochs. Bands indicate mean and standard deviation across three runs.

Figure 6: _BYOL ViT-B/16 on ImageNet1k_ for different scalings \(\kappa\). The baseline model (\(\kappa=1\), black dashed) uses batch size 4096 and teacher momentum \(\rho_{B}=0.99\), and is scaled from batch size 8192 (left) to 32768 (right) with progressive scaling and the EMA Scaling Rule (Definition 3.2) (orange, \(\rho=\rho_{B}^{\kappa}\)), with the EMA Scaling Rule but without progressive scaling (blue, \(\rho=\rho_{B}^{\kappa}\)), without the EMA Scaling Rule but with progressive scaling (purple, \(\rho=\rho_{B}\)), and without either (red, \(\rho=\rho_{B}\)). Progressive scaling transitions from the reference model at epoch 60. See Appendix H.6 for a discussion on BYOL progressive scaling.

At the transition point between batch sizes, an impulse perturbation1 is measured at the student, visible from the training loss. This is recovered from by the learning process, and the new model matches the reference batch size. This perturbation happens in both the AdamW and SGD settings, leading us to suspect this is due to the BYOL learning process, rather than an artifact of optimizer or momentum scaling. However, since this is not directly related to the EMA Scaling Rule proposed in this work, we defer this analysis to future investigation.

Footnote 1: Instead of a single large batch transition as in Figure 6 we perform a sequential transition in Appendix H.5. We find that a slow increase in batch size minimizes the magnitude of the perturbation and leads to a final model with higher effective linear probe top-1 than the reference by approximately \(1.17\%\).

## 4 Related work

**Optimizer scaling rules from SDEs** The SDE perspective has uncovered optimizer scaling rules and allowed an understanding of their limitations. Smith and Le (2018) used SDEs to uncover the SGD Scaling Rule, while (Li et al., 2021) used SDEs to explain that rule's breakdown in terms of discretization error. The SDE analysis was extended to adaptive optimization by (Malladi et al., 2022), producing an Adam Scaling Rule (Definition C.3), indicating that along with the learning rate, the \(\beta_{1,2}\) and \(\epsilon\) parameters transform. The \(\beta_{1,2}\) transformation is consistent with the EMA Scaling Rule in the SDE limit. Our work differs as it considers a model EMA that alters the objective.

**Varying the batch size during training**Smith et al. (2018) investigated the benefits of scheduling the batch size at a fixed learning rate as an alternative to scheduling the learning rate at a fixed batch size. These two are equivalent through the SGD Scaling Rule. The authors _do not_ scale the optimizer hyperparameters during this procedure, as they are intentionally replicating the training dynamics of a learning rate schedule. This is in contrast with _Progressive Scaling_ (Definition 3.2) which scales the hyperparameters to _maintain_ the optimization process at different levels of discretization.

**Large batch training of SSL distillation methods** SSL methods learn representations without labels, meaning they can take advantage of web-scale data. Large batch optimization is required to make use of this data in a reasonable amount of time. Grill et al. (2020) demonstrated algorithmic robustness when _reducing_ the batch size through gradient accumulation and EMA update skipping, which implements an approximation of our EMA Scaling Rule for \(\kappa<1\). Our work provides a recipe to scale down _and up_ in \(\kappa\). MoCo-v3 (Chen et al., 2021) enables contrastively distilled ViTs up to a batch size of 6144, where the model drops in performance. More recently, methods like DINO (Caron et al., 2020) present a worse scenario, and are unable to scale beyond batch size 1024 (Koppula et al., 2022). In contrast, our work presents practical tools to scale to large batch sizes in the presence of an EMA, enabling practical training of these SSL methods on large scale data.

## 5 Conclusion

We provide an EMA Scaling Rule: when changing the batch size by a factor of \(\kappa\), exponentiate the momentum of the EMA update to the power of \(\kappa\). This scaling rule should be applied in addition to optimizer scaling rules (for example, linearly scaling the SGD learning rate), and enables the scaling of methods which rely on EMA and are sensitive to the choice of EMA momentum.

We prove the validity of the EMA Scaling Rule by deriving first-order SDE approximations of discrete model optimization when a model EMA is present and can contribute to the model objective. We demonstrate empirical support for a variety of uses of EMA, ordered by increasing influence of the role of EMA on the optimization procedure: supervised model tracking (i.e. Polyak-Ruppert averaging) in speech and vision domains, pseudo-labeling in speech, and self-supervised image representation learning. In almost all scenarios, using the EMA Scaling Rule enables matching of training dynamics under batch size modification, whereas not using it results in significant differences in optimization trajectories. For example, we can scale the BYOL self-supervised method to a batch size of 24,576 without any performance loss _only_ when using the EMA Scaling Rule.

While learning rate scaling rules are relatively commonplace in ML, the role of EMA has been overlooked. With this work, we highlight the importance of scaling the EMA momentum, and hope that future works will use the EMA Scaling Rule to scale the EMA momentum correctly, in the same way that learning rates and other optimizer hyperparameters are scaled.

## 6 Acknowledgements

We thank Miguel Sarabia del Castillo, Adam Golinski, Pau Rodriguez Lopez, Skyler Seto, Amitis Shidani, Barry Theobald, Vimal Thilak, Floris Weers, Luca Zappella, and Shaungfei Zhai for their helpful feedback and critical discussions throughout the process of writing this paper; Okan Akalin, Hassan Babaie, Denise Hui, Mubarak Seyed Ibrahim, Li Li, Cindy Liu, Rajat Dhull, Evan Samanas, Guillaume Seguin, and the wider Apple infrastructure team for assistance with developing and running scalable, fault tolerant code; and Kaifeng Lyu and Abhishek Panigrahi for discussion and details regarding scaling rules for adaptive optimizers. Names are in alphabetical order by last name within group.

## References

* Ba et al. (2016) Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. _CoRR_, abs/1607.06450, 2016. URL http://arxiv.org/abs/1607.06450.
* Baevski et al. (2022a) Alexei Baevski, Arun Babu, Wei-Ning Hsu, and Michael Auli. Efficient self-supervised learning with contextualized target representations for vision, speech and language. _CoRR_, abs/2212.07525, 2022a. doi: 10.48550/arXiv.2212.07525. URL https://doi.org/10.48550/arXiv.2212.07525.
* Baevski et al. (2022b) Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. In _International Conference on Machine Learning_, pp. 1298-1312. PMLR, 2022b.
* Berrebbi et al. (2023) Dan Berrebbi, Ronan Collobert, Samy Bengio, Navdeep Jaitly, and Tatiana Likhomanenko. Continuous pseudo-labeling from the start. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=m3twGT2bAug.
* Bommasani et al. (2021) Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi, and et al. On the opportunities and risks of foundation models. _CoRR_, abs/2108.07258, 2021. URL https://arxiv.org/abs/2108.07258.
* Brock et al. (2021) Andy Brock, Soham De, Samuel L. Smith, and Karen Simonyan. High-performance large-scale image recognition without normalization. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pp. 1059-1071. PMLR, 2021. URL http://proceedings.mlr.press/v139/brock21a.html.
* Caron et al. (2020) Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/70feb62b69f16e0238f741fab228fec2-Abstract.html.
* Caron et al. (2021) Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. _CoRR_, abs/2104.14294, 2021. URL https://arxiv.org/abs/2104.14294.
* Chen et al. (2020) Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E. Hinton. Big self-supervised models are strong semi-supervised learners. In Hugo Larochelle,Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/fcbc95ccdd551da181207c0c1400c655-Abstract.html.
* Chen et al. (2021) Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In _2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021_, pp. 9620-9629. IEEE, 2021. doi: 10.1109/ICCV48922.2021.00950. URL https://doi.org/10.1109/ICCV48922.2021.00950.
* Dosovitskiy et al. (2021) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_. OpenReview.net, 2021. URL https://openreview.net/forum?id=YicbFdNTIY.
* The 23rd Conference on Learning Theory, Haifa, Israel, June 27-29, 2010_, pp. 257-269. Omnipress, 2010. URL http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf#page=265.
* Fetterman and Albrecht (2020) Abe Fetterman and Josh Albrecht. Understanding self-supervised and contrastive learning with "bootstrap your own latent" (byol), Aug 2020. URL https://generallyintelligent.ai/understanding-self-supervised-contrastive-learning.html.
* Goyal et al. (2017) Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. _CoRR_, abs/1706.02677, 2017. URL http://arxiv.org/abs/1706.02677.
* Graves et al. (2006) Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In _Proceedings of the 23rd international conference on Machine learning_, pp. 369-376, 2006.
* A new approach to self-supervised learning. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html.
* He et al. (2020) Junxian He, Jiatao Gu, Jiajun Shen, and Marc'Aurelio Ranzato. Revisiting self-training for neural sequence generation. In _International Conference on Learning Representations_, 2020. URL https://openreview.net/forum?id=SJgdnAVKDH.
* He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015_, pp. 1026-1034. IEEE Computer Society, 2015. doi: 10.1109/ICCV.2015.123. URL https://doi.org/10.1109/ICCV.2015.123.
* He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016_, pp. 770-778. IEEE Computer Society, 2016a. doi: 10.1109/CVPR.2016.90. URL https://doi.org/10.1109/CVPR.2016.90.
* He et al. (2016)Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.), _Computer Vision - ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV_, volume 9908 of _Lecture Notes in Computer Science_, pp. 630-645. Springer, 2016b. doi: 10.1007/978-3-319-46493-0_38. URL https://doi.org/10.1007/978-3-319-46493-0_38.
* He et al. (2022) Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022_, pp. 15979-15988. IEEE, 2022. doi: 10.1109/CVPR52688.2022.01553. URL https://doi.org/10.1109/CVPR52688.2022.01553.
* Higuchi et al. (2022) Yosuke Higuchi, Niko Moritz, Jonathan Le Roux, and Takaaki Hori. Momentum pseudo-labeling: Semi-supervised asr with continuously improving pseudo-labels. _IEEE Journal of Selected Topics in Signal Processing_, 16(6):1424-1438, 2022.
* Huang et al. (2017) Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger. Snapshot ensembles: Train 1, get M for free. In _5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net, 2017. URL https://openreview.net/forum?id=BJYwwY911.
* Huo et al. (2021) Zhouyuan Huo, Bin Gu, and Heng Huang. Large batch optimization for deep learning using new complete layer-wise adaptive rate scaling. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021_, pp. 7883-7890. AAAI Press, 2021. URL https://ojs.aaai.org/index.php/AAAI/article/view/16962.
* Ieee standard for floating-point arithmetic (2019) IEEE. Ieee standard for floating-point arithmetic. _IEEE Std 754-2019 (Revision of IEEE 754-2008)_, pp. 1-84, 2019. doi: 10.1109/IEEESTD.2019.8766229.
* Ioffe and Szegedy (2015) Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Francis R. Bach and David M. Blei (eds.), _Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015_, volume 37 of _JMLR Workshop and Conference Proceedings_, pp. 448-456. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/ioffe15.html.
* Izmailov et al. (2018) Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In Amir Globerson and Ricardo Silva (eds.), _Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018_, pp. 876-885. AUAI Press, 2018. URL http://auai.org/uai2018/proceedings/papers/313.pdf.
* Jastrzebski et al. (2017) Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos J. Storkey. Three factors influencing minima in SGD. _CoRR_, abs/1711.04623, 2017. URL http://arxiv.org/abs/1711.04623.
* Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _CoRR_, abs/2001.08361, 2020. URL https://arxiv.org/abs/2001.08361.
* Kingma and Ba (2015) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio and Yann LeCun (eds.), _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_, 2015. URL http://arxiv.org/abs/1412.6980.
* Kish (1965) Leslie Kish. _Survey Sampling_, volume 59. Cambridge University Press, 1965. doi: 10.1017/S0003055400132113.
* Koppula et al. (2022) Skanda Koppula, Yazhe Li, Evan Shelhamer, Andrew Jaegle, Nikhil Parthasarathy, Relja Arandjelovic, Joao Carreira, and Olivier J. Henaff. Where should I spend my flops? efficiency evaluations of visual pre-training methods. _CoRR_, abs/2209.15589, 2022. doi: 10.48550/arXiv.2209.15589. URL https://doi.org/10.48550/arXiv.2209.15589.
* Krizhevsky et al. (2014)Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. _CoRR_, abs/1404.5997, 2014. URL http://arxiv.org/abs/1404.5997.
* Krizhevsky et al. (2014) Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research). 2014. URL http://www.cs.toronto.edu/~kriz/cifar.html.
* Li et al. (2018) Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of neural nets. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett (eds.), _Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montreal, Canada_, pp. 6391-6401, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html.
* Li et al. (2019) Qianxiao Li, Cheng Tai, and E Weinan. Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations. _The Journal of Machine Learning Research_, 20(1):1474-1520, 2019.
* Li et al. (2021) Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora. On the validity of modeling SGD with stochastic differential equations (sdes). In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan (eds.), _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pp. 12712-12725, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/69f62956429865909921fa916d61c1f8-Abstract.html.
* Likhomanenko et al. (2021a) Tatiana Likhomanenko, Qiantong Xu, Jacob Kahn, Gabriel Synnaeve, and Ronan Collobert. slimipl: Language-model-free iterative pseudo-labeling. _Proc. Interspeech_, 2021a.
* Likhomanenko et al. (2021b) Tatiana Likhomanenko, Qiantong Xu, Gabriel Synnaeve, Ronan Collobert, and Alex Rogozhnikov. Cape: Encoding relative positions with continuous augmented positional embeddings. _Advances in Neural Information Processing Systems_, 34, 2021b.
* Likhomanenko et al. (2022) Tatiana Likhomanenko, Ronan Collobert, Navdeep Jaitly, and Samy Bengio. Continuous soft pseudo-labeling in ASR. In _I Can't Believe It's Not Better Workshop: Understanding Deep Learning Through Empirical Falsification_, 2022. URL https://openreview.net/forum?id=aoiqVW4ui51.
* Lillicrap et al. (2016) Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua Bengio and Yann LeCun (eds.), _4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings_, 2016. URL http://arxiv.org/abs/1509.02971.
* Loshchilov and Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019_. OpenReview.net, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.
* Malladi et al. (2022) Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On the sdes and scaling rules for adaptive gradient algorithms. In _NeurIPS_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/32ac710102f0620d0f28d5d05a44fe08-Abstract-Conference.html.
* Manohar et al. (2021) Vimal Manohar, Tatiana Likhomanenko, Qiantong Xu, Wei-Ning Hsu, Ronan Collobert, Yatharth Saraf, Geoffrey Zweig, and Abdelrahman Mohamed. Kaizen: Continuously improving teacher using exponential moving average for semi-supervised speech recognition. In _2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)_, pp. 518-525. IEEE, 2021.
* Niizumi et al. (2023) Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, and Kunio Kashino. BYOL for audio: Exploring pre-trained general-purpose audio representations. _IEEE ACM Trans. Audio Speech Lang. Process._, 31:137-151, 2023. doi: 10.1109/TASLP.2022.3221007. URL https://doi.org/10.1109/TASLP.2022.3221007.
* Nississerman et al. (2019)Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. _CoRR_, abs/2304.07193, 2023. doi: 10.48550/arXiv.2304.07193. URL https://doi.org/10.48550/arXiv.2304.07193.
* Ott et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In _Proceedings of NAACL-HLT 2019: Demonstrations_, 2019.
* Panayotov et al. (2015) Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In _2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pp. 5206-5210. IEEE, 2015.
* Park et al. (2019) Daniel S Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D Cubuk, and Quoc V Le. Specaugment: A simple data augmentation method for automatic speech recognition. _Proc. Interspeech 2019_, pp. 2613-2617, 2019.
* Polyak and Juditsky (1992) B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. _SIAM Journal on Control and Optimization_, 30(4):838-855, 1992. doi: 10.1137/0330046. URL https://doi.org/10.1137/0330046.
* Qiao et al. (2019) Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan L. Yuille. Weight standardization. _CoRR_, abs/1903.10520, 2019. URL http://arxiv.org/abs/1903.10520.
* Richemond et al. (2020) Pierre H. Richemond, Jean-Bastien Grill, Florent Altche, Corentin Tallec, Florian Strub, Andrew Brock, Samuel L. Smith, Soham De, Razvan Pascanu, Bilal Piot, and Michal Valko. BYOL works even without batch statistics. _CoRR_, abs/2010.10241, 2020. URL https://arxiv.org/abs/2010.10241.
* Richemond et al. (2023) Pierre H. Richemond, Allison C. Tam, Yunhao Tang, Florian Strub, Bilal Piot, and Felix Hill. The edge of orthogonality: A simple view of what makes BYOL tick. _CoRR_, abs/2302.04817, 2023. doi: 10.48550/arXiv.2302.04817. URL https://doi.org/10.48550/arXiv.2302.04817.
* Ruppert (1988) David Ruppert. Efficient estimations from a slowly convergent robbins-monro process. 1988.
* Russakovsky et al. (2014) Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, and Li Fei-Fei. Imagenet large scale visual recognition challenge. _CoRR_, abs/1409.0575, 2014. URL http://arxiv.org/abs/1409.0575.
* Shaw et al. (2018) Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)_, pp. 464-468, 2018.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL https://openreview.net/forum?id=BJij4yg0Z.
* May 3, 2018, Conference Track Proceedings_. OpenReview.net, 2018. URL https://openreview.net/forum?id=B1Yy1BxCZ.
* Sohn et al. (2020) Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. _Advances in neural information processing systems_, 33:596-608, 2020.
* Sohn et al. (2018)Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Amana Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmuller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. _CoRR_, abs/2206.04615, 2022. doi: 10.48550/arXiv.2206.04615. URL https://doi.org/10.48550/arXiv.2206.04615.
* Tarvainen and Valpola (2017) Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pp. 1195-1204, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/68053af2923e00204c3ca7c6a3150cf7-Abstract.html.
* Tian et al. (2021) Yuandong Tian, Xinlei Chen, and Surya Ganguli. Understanding self-supervised learning dynamics without contrastive pairs. In Marina Meila and Tong Zhang (eds.), _Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event_, volume 139 of _Proceedings of Machine Learning Research_, pp. 10268-10278. PMLR, 2021. URL http://proceedings.mlr.press/v139/tian21a.html.
* Tieleman et al. (2012) Tijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. _COURSERA: Neural networks for machine learning_, 4(2):26-31, 2012.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett (eds.), _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pp. 5998-6008, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
* ECCV 2018
- 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XIII_, volume 11217 of _Lecture Notes in Computer Science_, pp. 3-19. Springer, 2018. doi: 10.1007/978-3-030-01261-8_1. URL https://doi.org/10.1007/978-3-030-01261-8_1.
* Xu et al. (2020) Qiantong Xu, Tatiana Likhomanenko, Jacob Kahn, Awni Hannun, Gabriel Synnaeve, and Ronan Collobert. Iterative pseudo-labeling for speech recognition. _Proc. Interspeech 2020_, pp. 1006-1010, 2020.
* You et al. (2017) Yang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for imagenet training. _CoRR_, abs/1708.03888, 2017. URL http://arxiv.org/abs/1708.03888.
* You et al. (2020) Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training BERT in 76 minutes. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020._ OpenReview.net, 2020. URL https://openreview.net/forum?id=Syx4wnEtvH.

## Appendix A Broader impact

### Limitations

* **The scaling toolbox: practical methods for enabling systematic scaling*
* C.1 The continuous time/SDE perspective
* C.2 Scaling rules for optimization
* C.3 Commonly used values of hyperparameters at different batch sizes
* C.4 Progressive scaling
* **ED EMA approximation theorems with SDEs*
* D.1 SGD with model EMA
* D.2 Adaptive gradient methods with model EMA
* **E Additional proofs*
* E.1 Iterations of SGD + EMA
* E.2 Limiting behavior of Polyak-Ruppert averaging
* **F Additional details and results for Polyak-Ruppert averaging*
* F.1 Noisy parabola
* F.2 Image Classification
* F.3 Applying the EMA Scaling Rule to Batch Normalization
* **G Additional details and results for Automatic Speech Recognition (ASR)*
* G.1 Additional experimental settings and detailed metrics
* G.2 Scaling to \(\kappa=16\) with Progressive Scaling
* **H Additional details and results for self-supervised image representation learning*
* H.1 Components of self-supervised learning
* H.2 A ResNet-18 recipe for BYOL
* H.3 A Vision Transformer recipe for BYOL
* H.4 The role of Batch Normalization and Layer Normalization in BYOL with ViTs
* H.5 Longer training duration with incremental Progressive Scaling
* H.6 Building intuition around Progressive Scaling and momentum sensitivity
* H.7 Compute usage for ViT BYOL investigation
* H.8 ResNet-18 hyperparameter sensitivity analysis
* H.9 ResNet-18 additional scaling analysis
* H.10 Scaling a ResNet-50 BYOL using LARS and Progressive Scaling
* H.11 Preventing collapse phenomena in DINO at scale
* **I Additional details on numerical stability*
* J Contributions
Broader impact

This work shows how to adapt Machine Learning (ML) optimization in the presence of a model Exponential Moving Average (EMA). There are a number of benefits to this:

1. Scaling rules democratize the training of ML models: they give ML researchers the ability to replicate the optimization of large scale systems, even if those researchers _do not_ have access to i) significant parallel computational resources _or_ ii) the technical tooling to do so.
2. Our EMA Scaling Rule lowers compute usage as it removes the necessity for a hyperparameter search over momenta; in the case where our scaling assumptions hold, if we know the value of the optimal momentum \(\rho_{B}\) at some batch size \(B\) (for example, the momentum that gives the best transfer performance), then the optimal value at another batch size \(\hat{B}\) is exactly the one given by the EMA Scaling Rule \(\hat{\rho}=\rho_{B}^{\kappa}\), for scaling \(\kappa=\hat{B}/B\).
3. Our EMA Scaling Rule enables researchers to more quickly iterate through experimental ideas, and opens up access to large-scale training (for example, larger models and larger datasets) for Pseudo-Labeling and Self-Supervised Learning (SSL) techniques.

These points have potential negative consequences:

1. As our EMA Scaling Rule enables researchers to iterate the same experiments more quickly, and perform large-scale training with EMA-based methods, this may encourage a greater number of experiments, or the training of larger models. Either of these possibilities leads to greater energy consumption.
2. As the need to determine momentum hyperparameters has now been removed, researchers who were previously discouraged from attempting to scale these methods due to an _extra_ hyperparameter to tune may begin to perform such experiments, leading, once more, to greater energy consumption.

The environmental impact of each of these two points may be significant.

## Appendix B Limitations

The EMA Scaling Rule provides a recipe for producing training dynamics independent of the batch size used in stochastic optimization. The technology underpinning it will not _always_ give the desired behavior, however.

The first issue occurs with the wording present in the EMA Scaling Rule: _[...] and scale other optimizers according to their own scaling rules_ (Definition 1.2):

1. This statement requires that the given Stochastic Differential Equation (SDE) approximation we are using for the model optimizer is itself providing well-behaved scaling, that is, that in the _absence_ of a model EMA, the model optimization trajectories at the batch sizes \(B\) and \(\kappa B\), with optimizer hyperparameters appropriately scaled, are close. In general we know this is not true. First, we know that the SDE approximation for Stochastic Gradient Descent (SGD) breaks at a given \(\kappa\) due to discretization error (Li et al., 2021). Second, we know that if the gradient noise is not sufficiently large, the SDE approximation for Adam does not exist (Malladi et al., 2022), i.e. an SDE motivated scaling rule has no meaning.
2. This statement requires knowledge of how to scale the corresponding model optimizer. We have principled ways to achieve this for SGD (Li et al., 2021), and for the adaptive optimization methods RMSProp and Adam (Malladi et al., 2022). Empirically, a square-root scaling law for LAMB (You et al., 2020) has been observed, however; it has not been derived formally. Problematically, there is no known hyperparameter scaling law or SDE approximation known for LARS (You et al., 2017), which has been used in Bootstrap Your Own Latent (BYOL) (Grill et al., 2020) and many other large-scale training procedures for convolution-based architectures. Despite this, we are able to demonstrate in Appendix H.10 that a combination of the EMA Scaling Rule and progressive scaling can match, or surpass baseline BYOL performance at a batch size of 32,768 using LARS, indicating that although the theoretical guarantees may not hold, there is still practical utility in the tools we provide in this work.

3. It may be the case that the optimal performance attainable by a given model setup exists at a level of discretization/gradient noise where no SDE exists. In this case, SDE-derived scaling rules can never be valid, and no scaling of this dynamics can be achieved with known tools.

The second issue is related to the case when the optimizer scaling rule is valid. In this case, the error for the EMA Scaling Rule at finite learning rate \(\eta\) at large \(\kappa\) can be considerable. In cases where the model EMA plays a role in the overall optimization, the error introduced by the EMA Scaling Rule can break the preservation of model dynamics.

Put another way, an optimizer scaling rule and the EMA Scaling Rule each introduce their own discretization errors. In the case where EMA plays a role in optimization, as soon as the discretization error of _either_ the optimizer scaling rule _or_ the EMA Scaling Rule is large, the error for the joint optimization procedure is large. This is _at least_ as bad as cases that _do not_ use a model EMA during the optimization process.

## Appendix C The scaling toolbox: practical methods for enabling systematic scaling

There are many different components involved in preserving optimization dynamics at different batch sizes. In this appendix we collect into a single place the different concepts and values that we found useful in practice, in an attempt to make the practice of scaling as accessible as possible.

### The continuous time/SDE perspective

Here we discuss the mindset difference required when trying to preserve training dynamics. In ML we typically use stochastic optimization, leading us to think of the optimization in terms of _performing updates_, or _stepping the optimizer_. This notion has become more common in the era of large datasets, where it may be the case that we only see a fraction of the dataset during optimization.

For dynamics preservation under scaling, we suggest that it is simpler to consider the _amount of data_ seen by the training process, or alternatively, the amount of _continuous time_ in the discretization of SDEs view. The reason is the following. The SDE scaling rule results (Definition 1.2, Li et al. (2019, 2021); Malladi et al. (2022)) follow from showing that different discretizations of the SDE are close to that SDE, providing we appropriately scale hyperparameters (see Section 2.2). Each of these discretizations shares the _total continuous time_\(T=\hat{\eta}\times\widehat{N}_{\text{iter}}\)10 of the underlying SDE, but each discretization has a _different_ number of iterations \(\widehat{N}_{\text{iter}}=N_{\text{iter}}/\kappa\).

Footnote 10: This is in the case of SGD, for RMSProp and Adam one should use \(T=\hat{\eta}^{2}\times\widehat{N}_{\text{iter}}\)(Malladi et al., 2022).

This perspective is already adopted, perhaps by accident in some domains. For example, in Computer Vision (CV), it is typical to compare model performance after optimization on ImageNet1k after a _number of epochs_, whilst also specifing a learning rate warmup after a _number of epochs_. This transforms the schedule into the form _wait until the process meets [condition]_, where here _[condition]_ is _when the process has seen sufficiently many samples_.

More generally, we can specify any _condition_ that is not a property of the discretization procedure itself. Instead, the discretization procedure should be viewed as a numerical approximation method for the SDE we are evolving, and the properties of that discretization process (like _number of steps_) are not _of specific interest_ in the world view where we do decouple optimization from the batch size. A specific example of this more general case is present in Section 3.3, where for scaling \(\kappa>2\) we wait until the pre-training Word Error Rate (WER) is sufficiently low.

There may be cases where one is working with a setup that is explicitly defined in terms of quantities related to the discretization process. Indeed, the optimizer hyperparameters are examples of these, and need to be scaled accordingly with \(\kappa\). The other typical example of this is conditions based on the _number of optimizer steps_, rather than the number of epochs. In this case, these quantities should be scaled to achieve the desired condition in the same amount of time, i.e. as above \(\widehat{N}_{\text{iter}}=N_{\text{iter}}/\kappa\), where \(N_{\text{iter}}\) is the number of iterations specified at the base batch size \(B\). Concretely, if training is specified in a number of steps, then doubling the batch size implies you should train for half the number of steps.

### Scaling rules for optimization

For ease of reference, we collect all the scaling rules related to batch size modification we are aware of. We begin with the most well-known, the SGD Scaling Rule (Definitions 2.2 and C.1).

**Definition C.1** (SGD Scaling Rule).: _When running SGD (Definition 2.1) with batch size \(\hat{B}=\kappa B\), use a learning rate \(\hat{\eta}=\kappa\eta\)(Krizhevsky, 2014; Goyal et al., 2017)._

The SGD Scaling Rule is also known as the Linear Scaling Rule (LSR), although for clarity, this work adopts the naming convention _[Algorithm Name] Scaling Rule_, which means all parameters of those algorithms are appropriately scaled from batch size \(B\) to \(\kappa B\).

Next, we give the two scaling rules known for the adapative optimizers RMSProp (Tieleman et al., 2012) and Adam (Kingma and Ba, 2015) in Definition C.2 and Definition C.3 respectively.

**Definition C.2** (RMSProp Scaling Rule).: _When running RMSProp (Tieleman et al., 2012) with batch size \(\hat{B}=\kappa B\), use a learning rate \(\hat{\eta}=\sqrt{\kappa}\eta\), beta coefficient \(\hat{\beta}=1-\kappa\times(1-\beta)\), and adaptivity parameter \(\hat{\epsilon}=\frac{\kappa}{\sqrt{\kappa}}\)(Malladi et al., 2022)._

**Definition C.3** (Adam Scaling Rule).: _When running Adam (Kingma and Ba, 2015) with batch size \(\hat{B}=\kappa B\), use a learning rate \(\hat{\eta}=\sqrt{\kappa}\eta\), beta coefficients \(\hat{\beta}_{1}=1-\kappa\times(1-\beta_{1})\), \(\hat{\beta}_{2}=1-\kappa\times(1-\beta_{2})\), and adaptivity parameter \(\hat{\epsilon}=\frac{\kappa}{\sqrt{\kappa}}\)(Malladi et al., 2022)._

Next, we present a contribution of this work, the EMA Scaling Rule (Definitions 1.2 and C.4), which extends the above scaling rules to allow the presence of a model EMA which is able to contribute to the overall optimization (see Appendices D and E.1 for derivations).

**Definition C.4** (EMA Scaling Rule).: _When computing the EMA update (Definition 1.1) of a model undergoing stochastic optimization with batch size \(\hat{B}=\kappa B\), use a momentum \(\hat{\rho}=\rho^{\kappa}\) and scale other optimizers according to their own scaling rules._

Concretely, if we are using SGD in the presence of a model EMA, Definitions C.1 and C.4 state that we should take \(\hat{\eta}=\kappa\eta\) and \(\hat{\rho}=\rho^{\kappa}\) when scaling by \(\kappa=\hat{B}/B\).

The final scaling rule is for weight decay, and follows from the scaling logic discussed in Appendix C.1 and Krizhevsky (2014). If we take the weight decay regularization penalty \(\lambda\) defined at batch size \(B\), what should the weight decay \(\hat{\lambda}\) be for batch size \(\hat{B}=\kappa B\)? For simplicity, consider \(\kappa\) updates of optimization of parameters \(\mathbf{\theta}_{t}\) in the presence of weight decay only

\[\mathbf{\theta}_{t+\kappa}=\mathbf{\theta}_{t+\kappa-1}-\eta\,\lambda\, \mathbf{\theta}_{t+\kappa-1}=(1-\eta\,\lambda)\,\mathbf{\theta}_{t+\kappa-1 }=(1-\eta\,\lambda)^{\kappa}\,\mathbf{\theta}_{t}.\] (11)

Therefore, to match the effect of weight decay with a single iteration step, we need to match

\[1-\hat{\eta}\,\hat{\lambda}=(1-\eta\,\lambda)^{\kappa}.\] (12)

Solving for \(\hat{\lambda}\) and expanding around \(\eta\approx 0\) gives

\[\hat{\lambda}=\frac{1-(1-\eta\,\lambda)^{\kappa}}{\hat{\eta}}\approx\frac{ \eta}{\hat{\eta}}\times\kappa\,\lambda+\mathcal{O}(\eta).\] (13)

This leads to the Weight Decay Scaling Rule (Definition C.5).

**Definition C.5** (Weight Decay Scaling Rule).: _When using weight decay with batch size \(\hat{B}=\kappa B\), use a penalty term \(\hat{\lambda}=(\kappa\hat{\eta}/\eta)\,\lambda\), where \(\hat{\eta}\) and \(\eta\) represent the scaled and unscaled learning rates of the corresponding optimizer (Krizhevsky, 2014; Li et al., 2018; Loshchilov and Hutter, 2019)._

The Weight Decay Scaling Rule implies that using _linear_ scaling for the learning rate \(\eta\) then the weight decay penalty is automatically scaled, and when using _square-root_ scaling for the learning rate \(\eta\) (e.g. in the case of the Adam Scaling Rule (Definition C.3)) then the weight decay penalty should also be scaled with a _square-root_ as is proposed in Loshchilov and Hutter (2019).

Finally, we see that if the implementation of weight decay does not have an update scaled by the learning rate, i.e. the update is \(\mathbf{\theta}_{t+1}=(1-\lambda)\,\mathbf{\theta}_{t}\), then the scaling rule is optimizer-independent, and becomes linear for small weight decay, i.e. \(\hat{\lambda}=\kappa\lambda\), and for arbitrary \(\lambda\) takes the form \(\hat{\lambda}=1-(1-\lambda)^{\kappa}\)

### Commonly used values of hyperparameters at different batch sizes

In the literature it is common to give a base learning rate \(\eta\) defined at batch size 256, implicitly using the SGD Scaling Rule, even when using the Adam optimizer. Because the scaling of other optimization hyperparameters was not understood until recently, it is also common to just present these _for the experiment_, e.g. the Adam betas and epsilon, and the EMA momentum, implicitly defined at the scale of the experiment, for example at batch size 4096. One way to deal with this in practice is to define a single reference batch size \(B\) at which _all_ hyperparameters are defined, and then scale from there. In this case, it is easiest to compute _using linear scaling_ the learning rate at the redefined base batch size \(\eta=\tilde{\kappa}\,\eta_{\text{orig}}\), where \(\tilde{\kappa}=B/B_{\text{orig}}\), and then scale this new reference \(\eta\) as \(\hat{\eta}=\kappa\eta\), \(\kappa=\hat{B}/B\), along with e.g. the momentum defined at \(B\).

As this process can be slightly frustrating, we have provided tables of typical learning rates in Table 2 and momenta in Table 3.

### Progressive scaling

In Section 3.4 we introduced Progressive Scaling (Definition 3.2) to test our hypothesis that early in the BYOL training procedure, there are dynamics that are challenging to replicate at larger batch

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline  & \multicolumn{3}{c}{\(\hat{\eta}=\kappa\eta\) [SGD]} & \multicolumn{3}{c}{\(\hat{\eta}=\sqrt{\kappa\eta}\) [RMSProp, Adam]} \\ \cline{2-7}  & \multicolumn{2}{c}{\(B=256\)} & \multicolumn{2}{c}{\(B=512\)} & \multicolumn{2}{c}{\(B=256\)} & \multicolumn{2}{c}{\(B=4096\)} \\ \cline{2-7} Batch size \(\hat{B}\) & \(\eta=0.1\) & \(\eta=0.3\) & \(\eta=0.1\) & \(\eta=10^{-3}\) & \(\eta=4.8\) & \(\eta=10^{-3}\) \\ \hline
32 & 0.0125 & 0.0375 & 0.00625 & 0.00035 & 0.42426 & 0.00009 \\
64 & 0.025 & 0.075 & 0.0125 & 0.0005 & 0.6 & 0.00013 \\
128 & 0.05 & 0.15 & 0.025 & 0.00071 & 0.84853 & 0.00018 \\
256 & **0.1** & **0.3** & 0.05 & **0.001** & 1.2 & 0.00025 \\
512 & 0.2 & 0.6 & **0.1** & 0.00141 & 1.69706 & 0.00035 \\
1024 & 0.4 & 1.2 & 0.2 & 0.002 & 2.4 & 0.0005 \\
2048 & 0.8 & 2.4 & 0.4 & 0.00283 & 3.39411 & 0.00071 \\
4096 & 1.6 & 4.8 & 0.8 & 0.004 & **4.8** & **0.001** \\
8192 & 3.2 & 9.6 & 1.6 & 0.00566 & 6.78823 & 0.00141 \\
16384 & 6.4 & 19.2 & 3.2 & 0.008 & 9.6 & 0.002 \\
32768 & 12.8 & 38.4 & 6.4 & 0.01131 & 13.57645 & 0.00283 \\
65536 & 25.6 & 76.8 & 12.8 & 0.016 & 19.2 & 0.004 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Scaled learning rates \(\hat{\eta}\) at different batch sizes \(\hat{B}=\kappa B\) given reference learning rates \(\eta\) defined at batch size \(B\). The reference values of each column are boldened. Note that this is only valid when there is a notion of _single sample_. In the sequence learning setup (for example, in Section 3.3), the notion of batch size should be appropriately replaced with the _dynamic batch size_, i.e. total sequence length.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{\(B=256\)} & \multicolumn{3}{c}{\(B=4096\)} \\ \cline{2-7} Batch size \(\hat{B}\) & \(\rho=0.9999\) & \(\rho=0.999\) & \(\rho=0.99\) & \(\rho=0.996\) & \(\rho=0.992\) & \(\rho=0.99\) & \(\rho=0.97\) \\ \hline
32 & 0.99999 & 0.99987 & 0.99874 & 0.99997 & 0.99994 & 0.99992 & 0.99976 \\
64 & 0.99997 & 0.99975 & 0.99749 & 0.99994 & 0.99987 & 0.99984 & 0.99952 \\
128 & 0.99995 & 0.9995 & 0.99499 & 0.99987 & 0.99975 & 0.99969 & 0.99905 \\
256 & **0.9999** & **0.999** & **0.99** & 0.99975 & 0.9995 & 0.99937 & 0.9981 \\
512 & 0.9998 & 0.9980 & 0.9801 & 0.9995 & 0.999 & 0.99874 & 0.9962 \\
1024 & 0.9996 & 0.99601 & 0.9606 & 0.999 & 0.99799 & 0.99749 & 0.99241 \\
2048 & 0.9992 & 0.99203 & 0.92274 & 0.998 & 0.99599 & 0.99499 & 0.98489 \\
4096 & 0.9984 & 0.98412 & 0.85146 & **0.996** & **0.992** & **0.99** & **0.97** \\
8192 & 0.9968 & 0.96849 & 0.72498 & 0.99202 & 0.98406 & 0.9801 & 0.9409 \\
16384 & 0.99362 & 0.93798 & 0.5256 & 0.9841 & 0.96838 & 0.9606 & 0.88529 \\
32768 & 0.98728 & 0.8798 & 0.27625 & 0.96844 & 0.93776 & 0.92274 & 0.78374 \\
65536 & 0.97472 & 0.77405 & 0.07632 & 0.93788 & 0.8794 & 0.85146 & 0.61425 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Scaled EMA momenta \(\hat{\rho}=\rho^{\kappa}\) at different batch sizes \(\hat{B}=\kappa B\) given reference momenta \(\rho\) defined at batch size \(B\). The reference values of each column are boldened. Again in the sequence learning setup, batch size should be appropriately replaced with a notion of sequence length.

sizes. To remove ambiguity, in Algorithm 1 we provide pseudo-code for how to use Progressive Scaling.

In Algorithm 1, the prefactor of the SGD update could also have been written \(\eta/B\), although an equivalent use of the base momentum is not possible.

Finally, we outline how to extend Algorithm 1 to more complex setups, like those presented in Section 3.4:

1. Optimizer scaling rules are used appropriately, for example the Adam scaling rule in case of using the Adam optimizer to update parameters \(\bm{\theta}\).
2. Schedules for hyperparameters are computed using the base hyperparameters, and are then modified by application of the scaling law in epoch (outer) loop.
3. Schedules for hyperparameters at the _step_ rather than epoch level can be achieved in practice through recomputing the schedule and updating the notion of minibatch index appropriately throughout training.

All of the above techniques are used in Section 3.4. In addition, scheduling batch sizes within epoch is possible, providing one maintains a notion of computation within some fixed continuous time \(T_{\text{fixed}}\). We did not investigate this scenario.

## Appendix D EMA approximation theorems with SDEs

### SGD with model EMA

We will now derive the EMA scaling rule when tracking model parameters and the model is trained using SGD. We employ a strategy similar to Malladi et al. (2022), where we associate to each iterative process a Stochastic Differential Equation (SDE). In order to control the distance between the SDE and the discrete process, we use the tools from Li et al. (2019).

**Definition D.1** (Polynomial growth, Definition 1 in (Li et al., 2019)).: _The set \(G\) is the set of continuous functions \(\mathbb{R}^{d}\xrightarrow{}\mathbb{R}\) with at most polynomial growth, i.e., for \(g\in G\) there exists two scalars \(\kappa_{1},\kappa_{2}>0\) such that for all \(\bm{x}\in\mathbb{R}^{d}\), we have \(|g(\bm{x})|\leq\kappa_{1}(1+\|\bm{x}\|^{\kappa_{2}})\)._

_For an integer \(\alpha>0\), \(G^{\alpha}\) is the set of functions \(\mathbb{R}^{d}\xrightarrow{}\mathbb{R}\) that are \(\alpha\)-times continuously differentiable and such that all their derivatives up to order \(\alpha\) are in \(G\)._

Similarly to Malladi et al. (2022), we use Noisy Gradient Oracle with Scale Parameter (NGOS) to define the update rules on the parameters.

**Definition D.2** (Noisy Gradient Oracle with Scale Parameter (NGOS), adaptation of (Malladi et al., 2022)).: _A NGOS is a tuple \(\mathcal{G}_{\sigma}=(f,\Sigma,\mathcal{Z}_{\sigma})\). Given a noise scale parameter \(\sigma>0\), the NGOS \(\mathcal{G}_{\sigma}\)takes as input the parameters \(\bm{\theta}\) and outputs a random vector \(\mathbf{g}=\nabla f(\bm{\theta},\bm{\zeta})+\sigma\bm{\epsilon}\) where \(\nabla f(\bm{\theta},\bm{\zeta})\) is the gradient of \(f\) with respect to \(\bm{\theta}\) at \((\bm{\theta},\bm{\zeta})\), and \(\bm{\epsilon}\) is a random vector drawn from the distribution \(\mathcal{Z}_{\sigma}(\bm{\theta},\bm{\zeta})\) with zero mean and covariance \(\Sigma(\bm{\theta},\bm{\zeta})\)._

Note that in the above definition, the probability distribution \(\mathcal{Z}_{\sigma}(\bm{\theta},\bm{\zeta})\) is allowed to change with the scale \(\sigma\), but its first two moments -- its mean and its covariance -- are fixed with \(\sigma\). We have the following theorem for model EMA under optimization with SGD:

**Theorem D.1** (SDE for SGD + EMA).: _Consider the couple \(\mathbf{x}_{k}=(\bm{\theta}_{k},\xi_{k})\) where \(\bm{\theta}_{k}\) are the iterates of SGD with a NGOS (Definition D.2) and \(\xi_{k}\) is an EMA of \(\bm{\theta}_{k}\), defined, starting from \(\mathbf{x}_{0}=\bm{x}_{0}\), by_

\[\bm{\theta}_{k+1}=\bm{\theta}_{k}-\eta\mathbf{g}_{k},\ \ \text{with}\ \bm{ \theta}_{k}=\nabla f(\bm{\theta}_{k},\xi_{k})+\sigma\bm{\epsilon}_{k},\ \text{and}\ \bm{\epsilon}_{k}\sim\mathcal{Z}_{\sigma}(\bm{\theta}_{k},\xi_{k}),\] (14) \[\bm{\zeta}_{k+1}=\rho\xi_{k}+(1-\rho)\bm{\theta}_{k}\ \.\] (15)

_Define \(\beta_{0}=(1-\rho)/\eta\), \(\sigma_{0}=\sigma\sqrt{\eta}\), and define the SDE for \(X_{t}=(\bm{\theta}_{t},Z_{t})\), starting from \(X_{0}=\bm{x}_{0}\), by_

\[d\bm{\Theta}_{t} =-\nabla f(\bm{\Theta}_{t},Z_{t})dt+\sigma_{0}\Sigma(\bm{\Theta}_ {t},Z_{t})^{\frac{1}{2}}dW_{t},\ \ \text{with}\ \ W_{t}\ \text{a Wiener process}\] (16) \[dZ_{t} =\beta_{0}(\bm{\Theta}_{t}-Z_{t})dt\ \.\] (17)

_Assume that \(f\) is continuously differentiable, with \(f\in G^{3}\) and \(\Sigma^{\frac{1}{2}}\in G^{2}\) (Definition D.1). Then, for any time horizon \(T>0\) and test function \(g\in G^{2}\), there exists a constant \(c>0\) such that_

\[\max_{k=0,\ldots,\lfloor T/\eta\rfloor}\left|\mathbb{E}[g(X_{pk})]-\mathbb{E }[g(\mathbf{x}_{k})]\right|\leq c\times\eta\ \.\] (18)

Proof.: The proof uses the same tools as in Li et al. (2019). Define \(\Delta(\bm{\theta},\bm{\zeta})=\eta(-\nabla f(\bm{\theta},\bm{\zeta})+\sigma \bm{\epsilon},\beta_{0}(\bm{\theta}-\bm{\zeta}))\) with \(\bm{\epsilon}\sim\mathcal{Z}_{\sigma}(\bm{\theta},\bm{\zeta})\) the one-step update for the SGD + EMA update, such that \(\mathbf{x}_{k+1}=\mathbf{x}_{k}+\Delta(\mathbf{x}_{k})\). We have the first two moments:

\[\mathbb{E}[\Delta(\bm{\theta},\bm{\zeta})] =\eta(-\nabla f(\bm{\theta},\bm{\zeta}),\beta_{0}(\bm{\theta}- \bm{\zeta}))\] (19) \[\mathbb{V}[\Delta(\bm{\theta},\bm{\zeta})] =\eta\sigma_{0}^{2}\begin{bmatrix}\Sigma(\bm{\theta},\bm{\zeta}) &0\\ 0&0\end{bmatrix}\] (20)

and the higher-order moments are \(O(\eta^{2})\). Similarly, let \(\tilde{\Delta}(\bm{\theta},\bm{\zeta})\) be the solution at time \(\eta\) of the SDE defined by Equation 6 starting from \(X_{0}=(\bm{\theta},\bm{\zeta})\). From Ito's formula, we also obtain

\[\mathbb{E}[\tilde{\Delta}(\bm{\theta},\bm{\zeta})] =\eta(-\nabla f(\bm{\theta}),\beta_{0}(\bm{\theta}-\bm{\zeta}))\] (21) \[\mathbb{V}[\tilde{\Delta}(\bm{\theta},\bm{\zeta})] =\eta\sigma_{0}^{2}\begin{bmatrix}\Sigma(\bm{\theta},\bm{\zeta}) &0\\ 0&0\end{bmatrix}\] (22)

and the higher-order moments are \(O(\eta^{2})\). Hence, the moments of the discrete iteration and of the SDE match up to second order. Following the same proof technique as in Li et al. (2019) then leads to the advertized theorem. 

This theorem is a simple adaptation of the results of Li et al. (2019). Intuitively, it is expected that \(X_{t}\) and \(\mathbf{x}_{k}\) are close since \(\mathbf{x}_{k}\) is the Euler-Maruyama discretization of \(X_{t}\) with learning rate \(\eta\). We then have the corollary.

**Corollary D.1.1** (Validity of the EMA Scaling Rule).: _Assume that \(f\) is continuously differentiable, with \(f\in G^{3}\) and \(\Sigma^{\frac{1}{2}}\in G^{2}\). Let \(\bm{\theta}_{k}^{B},\xi_{k}^{B}\) the iterates of the Equation 5 with batch size \(B\) and hyperparameters \(\eta,\rho\). Let \(\bm{\theta}_{k}^{KB},\bm{\zeta}_{k}^{KB}\) be iterates with batch size \(\kappa B\), learning rate \(\eta\) determined by the SGD Scaling Rule (Definition 2.2) and momentum determined by the EMA Scaling Rule, linear version (Definition 1.2). Then, for any time horizon \(T>0\) and function \(g\in G^{2}\), there exists a constant \(d>0\) such that_

\[\max_{k=0,\ldots,\lfloor T/\eta\rfloor}\left|\mathbb{E}[g(\bm{\theta}_{\lfloor k /\kappa\rfloor}^{KB},\bm{\zeta}_{\lfloor k/\kappa\rfloor}^{KB})]-\mathbb{E}[g( \bm{\theta}_{k},\xi_{k})]\right|\leq d\times\eta\ \.\] (23)

Proof.: The proof is similar to Malladi et al. (2022). Under the scaling rule, both \(\mathbf{x}_{k}=(\bm{\theta}_{k},\xi_{k})\) and \(\hat{\mathbf{x}}_{\lfloor k/\kappa\rfloor}=(\bm{\theta}_{\lfloor k/\kappa \rfloor}^{KB},\bm{\zeta}_{\lfloor k/\kappa\rfloor}^{KB})\) have the same limiting SDE. Hence we have from the previous theorem that for all test function \(g\), we can find \(c,c^{\prime}\) such that

\[\max_{k=0,\ldots,\lfloor T/\eta\rfloor}\left|\mathbb{E}[g(X_{pk})]-\mathbb{E}[g (\mathbf{x}_{k})]\right|\leq c\times\eta\ \text{and}\ \max_{k=0,\ldots,\lfloor T/\eta\rfloor}\left|\mathbb{E}[g(X_{pk})]-\mathbb{E}[g (\hat{\mathbf{x}}_{\lfloor k/\kappa\rfloor})]\right|\leq c^{\prime}\times\eta.\] (24)The triangle inequality then gives

\[\max_{k=0,\ldots,\lfloor T/\eta\rfloor}|\mathbb{E}[g(\hat{\mathbf{x}}_{\lfloor k/ \kappa\rfloor})]-\mathbb{E}[g(\mathbf{x}_{k})]|\leq(c+c^{\prime})\times\eta.\] (25)

Hence, taking \(d=c+c^{\prime}\) gives the expected result. 

### Adaptive gradient methods with model EMA

We now turn to the case where one uses an adaptive gradient method rather than SGD to train the model. We follow derivations similar to those of Malladi et al. (2022), with an added EMA. Like above, we consider that the loss function \(f\) also depends on the EMA tracking parameter \(\xi_{k}\). We begin with RMSProp with EMA, which iterates:

\[\mathbf{v}_{k+1} =\gamma\mathbf{v}_{k}+(1-\gamma)\mathbf{g}_{k}^{2},\ \ \text{with}\ \mathbf{g}_{k}=\nabla f(\mathbf{\theta}_{k},\xi_{k})+\sigma\mathbf{\epsilon} _{k},\ \text{and}\ \mathbf{\epsilon}_{k}\sim\mathcal{Z}_{\sigma}(\mathbf{\theta}_{k},\xi_{k}),\] (26) \[\mathbf{\theta}_{k+1} =\mathbf{\theta}_{k}-\eta(\sqrt{\mathbf{v}_{k}}+\epsilon)^{-1} \times\mathbf{g}_{k}\] (27) \[\zeta_{k+1} =\rho\xi_{k}+(1-\rho)\mathbf{\theta}_{k}.\] (28)

Like in Malladi et al. (2022), we place ourselves in the high noise regime, in which the term \(\mathbf{g}_{k}^{2}\) in Equation 26 is approximated by \(\mathbf{g}_{k}^{2}\simeq\sigma^{2}\text{diag}(\Sigma(\mathbf{\theta}_{k},\xi_ {k}))\). We use the same scaling rules, with an additional one for \(\rho\):

\[\gamma_{0}=(1-\gamma)/\eta^{2},\ \ \sigma_{0}=\sigma\eta,\ \ \epsilon_{0}=\epsilon\eta,\ \text{and}\ \beta_{0}=(1-\rho)/\eta^{2},\] (29)

and we let \(\mathbf{u}_{k}=\mathbf{v}_{k}/\sigma^{2}\). The equations for RMSProp with EMA then become, using only these new variables and \(\eta\):

\[\mathbf{u}_{k+1}-\mathbf{u}_{k} =\eta^{2}\gamma_{0}(\text{diag}(\Sigma(\mathbf{\theta}_{k},\xi_ {k}))-\mathbf{u}_{k}),\] (30) \[\mathbf{\theta}_{k+1}-\mathbf{\theta}_{k} =-(\sqrt{\mathbf{u}_{k}}+\epsilon_{0})^{-1}\left(\eta^{2}\nabla f (\mathbf{\theta}_{k},\xi_{k})+\eta\mathbf{\epsilon}_{k}\right)\] (31) \[\xi_{k+1}-\xi_{k} =\eta^{2}\beta_{0}(\mathbf{\theta}_{k}-\xi_{k}).\] (32)

This formulation makes it clear that these iterations can be seen as the discretization of the SDE

\[dU_{t} =\gamma_{0}(\text{diag}(\Sigma(\Theta_{t},Z_{t}))-U_{t})dt,\] (33) \[d\Theta_{t} =-(\sigma_{0}\sqrt{U_{t}}+\epsilon_{0})^{-1}(\nabla f(\Theta_{t},Z_{t})dt+\sigma_{0}\Sigma(\Theta_{t},Z_{t})^{1/2}dWt)\] (34) \[dZ_{t} =\beta_{0}(\Theta_{t}-Z_{t})dt,\] (35)

with step size \(\eta^{2}\). Of course, we recover the SDE of Malladi et al. (2022) in the case where \(\beta_{0}=0\). A formal proof of closeness between the iterates and the SDE trajectory is out of the scope of the present paper since it would imply redoing much of the theoretical work developed in Malladi et al. (2022). Still, the previous informal analysis hints that for RMSProp, the scaling rule in Equation 29 should be used. In other words, given a certain set of hyperparameters \(\gamma,\eta\) and \(\rho\), if the batch size goes from \(B\) to \(\hat{B}=\kappa\times B\), the noise level becomes \(\hat{\sigma}=\sigma/\sqrt{\kappa}\), and keeping the quantities in Equation 29 constant means that we should use as new hyperparameters

\[\hat{\gamma}=1-(1-\gamma)\times\kappa,\ \ \hat{\eta}=\eta\times\sqrt{\kappa},\ \text{and}\ \hat{\rho}=1-(1-\rho)\times\kappa\ \.\]

The linear rule \(\hat{\rho}=1-(1-\rho)\times\kappa\) is at the first order equivalent to the exponential scaling rule \(\hat{\rho}=\rho^{\kappa}\). Hence, even though the limiting SDE differs greatly from that of SGD, and even though the scaling rule regarding the learning rate differs, we recover for the momentum term \(\rho\) the exact same scaling rule as for SGD.

We finish the discussion with the case of Adam, which leads once again to the same rule as for SGD. Adam with EMA tracking of the network parameters iterates

\[\mathbf{m}_{k+1} =\beta_{1}\mathbf{m}_{k}+(1-\beta_{1})\mathbf{g}_{k},\ \ \text{with}\ \mathbf{g}_{k}=\nabla f(\mathbf{\theta}_{k},\xi_{k})+\sigma\mathbf{\epsilon} _{k},\ \text{and}\ \mathbf{\epsilon}_{k}\sim\mathcal{Z}_{\sigma}(\mathbf{\theta}_{k},\xi_{k}),\] (36) \[\mathbf{v}_{k+1} =\beta_{2}\mathbf{v}_{k}+(1-\beta_{2})\mathbf{g}_{k}^{2}\] (37) \[\tilde{\mathbf{m}}_{k+1} =\mathbf{m}_{k+1}/(1-\beta_{1}^{k+1})\] (38) \[\tilde{\mathbf{v}}_{k+1} =\mathbf{v}_{k+1}/(1-\beta_{2}^{k+1})\] (39) \[\mathbf{\theta}_{k+1} =\mathbf{\theta}_{k}-\eta(\sqrt{\tilde{\mathbf{v}}_{k}}+\epsilon)^ {-1}\times\tilde{\mathbf{m}}_{k+1}\] (40) \[\xi_{k+1} =\rho\xi_{k}+(1-\rho)\mathbf{\theta}_{k}\ \.\] (41)Here, we use the same minor modification of the iterations as in Malladi et al. (2022), where we use \(\mathbf{v}_{k}\) instead of \(\mathbf{v}_{k+1}\) in the denominator of the \(\theta_{k}\) update.

We consider the following scaling for the hyperparameters

\[c_{1}=(1-\beta_{1})/\eta^{2},\ \ c_{2}=(1-\beta_{2})/\eta^{2},\ \ \sigma_{0}=\sigma\eta,\ \ \varepsilon_{0}=\epsilon\eta,\ \ \text{and}\ \beta_{0}=(1-\rho)/\eta^{2},\] (42)

and \(\gamma_{1}(t)=1-\exp(-c_{1}t)\), \(\gamma_{2}(t)=1-\exp(-c_{2}t)\), and \(\mathbf{u}_{k}=\mathbf{v}_{k}/\sigma^{2}\). The SDE for Adam + EMA is given by

\[dM_{t} =c_{1}\left((\nabla f(\Theta_{t},Z_{t})-M_{t})dt+\sigma_{0}\Sigma (\Theta_{t},Z_{t})^{1/2}dW_{t}\right)\] (43) \[dU_{t} =c_{2}(\text{diag}(\Sigma(\Theta_{t},Z_{t}))-U_{t})dt\] (44) \[d\Theta_{t} =-\frac{\sqrt{\gamma_{2}(t)}}{\gamma_{1}(t)}(\sigma_{0}\sqrt{U_{ t}}+\varepsilon_{0}\sqrt{\gamma_{2}(t)})^{-1}\times M_{t}dt\] (45) \[dZ_{t} =\beta_{0}(\Theta_{t}-Z_{t})dt.\] (46)

This is once again the same SDE as in Malladi et al. (2022) with the added EMA term. Like previously, this SDE hints at the fact that the scaling rule in eq. 42 should be used. In other words, given a set of hyperparameters \(\beta_{1},\beta_{2},\eta,\) and \(\rho\), if the batch size goes from \(B\) to \(\kappa\times B\), then the noise level becomes \(\hat{\sigma}=\sigma/\sqrt{\kappa}\) and keeping quantities in eq. 42 constant means that we should use as new hyperparameters

\[\hat{\beta}_{1}=1-(1-\beta_{1})\times\kappa,\ \ \hat{\beta}_{2}=1-(1-\beta_{2}) \times\kappa,\ \ \hat{\eta}=\eta\times\sqrt{\kappa},\ \text{and}\ \hat{\rho}=1-(1-\rho) \times\kappa.\]

We once again recover a linear rule for \(1-\rho\) which is equivalent to the exponential scaling rule \(\hat{\rho}=\rho^{\kappa}\) in the limit \(\rho\to 0\).

## Appendix E Additional proofs

### Iterations of SGD + EMA

Here we derive a critical component of the EMA Scaling Rule, the matrix equation of Equation 4 from which the EMA Scaling Rule (Definition 1.2) follows.

**Theorem E.1** (Iterations of SGD + EMA).: _Assuming that gradients change slowly over iterations of SGD (Definition 2.1) and EMA (Definition 1.1): \(\nabla_{\theta}\mathcal{L}(x;\theta_{t+j},\xi_{t+j})\approx\nabla_{\theta} \mathcal{L}(x;\theta_{t},\xi_{t})\approx\mathbf{g}\), for \(j=1,2,\ldots,\kappa\) and representative gradient \(\mathbf{g}\), iterating over \(\kappa\) independent minibatches produces model states_

\[\begin{bmatrix}\theta_{t+\kappa}\\ \xi_{t+\kappa}\\ \mathbf{g}\end{bmatrix}=\begin{bmatrix}1&0&-\eta\\ 1-\rho&\rho&0\\ 0&0&1\end{bmatrix}^{\kappa}\cdot\begin{bmatrix}\theta_{t}\\ \xi_{t}\\ \mathbf{g}\end{bmatrix}=\begin{bmatrix}\theta_{t}-\eta\,\kappa\,\mathbf{g} \\ \rho^{\kappa}\,\xi_{t}+(1-\rho^{\kappa})\,\theta_{t}+\mathcal{O}\left(\eta \times\beta_{\rho}\right)\\ \mathbf{g}\end{bmatrix}.\] (47)

Proof.: First note that for matrices of the form

\[\bm{A}=\begin{bmatrix}1&0&a_{0,2}\\ 1-a_{1,1}&a_{1,1}&0\\ 0&0&1\end{bmatrix},\] (48)

their multiplication follows

\[\bm{A}\,\bm{B} =\begin{bmatrix}1&0&a_{0,2}\\ 1-a_{1,1}&a_{1,1}&0\\ 0&0&1\end{bmatrix}\begin{bmatrix}1&0&b_{0,2}\\ 1-b_{1,1}&b_{1,1}&0\\ 0&0&1\end{bmatrix}\] \[=\begin{bmatrix}1&0&a_{0,2}+b_{0,2}\\ 1-a_{1,1}&b_{1,1}&a_{1,1}\,b_{1,1}&(1-a_{1,1})\,b_{0,2}\\ 0&0&1\end{bmatrix},\] (49)

and

\[\bm{A}\,\bm{B}\,\bm{C} =\begin{bmatrix}1&0&a_{0,2}+b_{0,2}\\ 1-a_{1,1}&b_{1,1}&a_{1,1}\,b_{1,1}&(1-a_{1,1})\,b_{0,2}\\ 0&0&1\end{bmatrix}\begin{bmatrix}1&0&c_{0,2}\\ 1-c_{1,1}&c_{1,1}&0\\ 0&0&1\end{bmatrix}\] \[=\begin{bmatrix}1&0&a_{0,2}+b_{0,2}+c_{0,2}\\ 1-a_{1,1}&b_{1,1}\,c_{1,1}&a_{1,1}\,b_{1,1}\,c_{1,1}&(1-a_{1,1})\,b_{0,2}+(1-a_ {1,1}\,b_{1,1})\,c_{0,2}\\ 0&0&1\end{bmatrix}.\] (50)By induction

\[\bm{A}^{\kappa} =\begin{bmatrix}1&0&\kappa\times a_{0,2}\\ 1-a_{1,1}^{\kappa}&a_{1,1}^{\kappa}&\delta(a_{0,2},a_{1,1},\kappa)\\ 0&0&1\end{bmatrix},\] (51)

where

\[\delta(a_{0,2},a_{1,1},\kappa)=a_{0,2}\,\sum_{i=1}^{\kappa-1}\left(1-a_{1,1}^{ i}\right)=a_{0,2}\left(\kappa-\frac{1-a_{1,1}^{\kappa}}{1-a_{1,1}}\right),\quad \text{for }a_{1,1}\neq 1.\] (52)

It follows that

\[\begin{bmatrix}1&0&-\eta\\ 1-\rho&\rho&0\\ 0&0&1\end{bmatrix}^{\kappa} =\begin{bmatrix}1&0&-\kappa\,\eta\\ 1-\rho^{\kappa}&\rho^{\kappa}&\delta(\eta,\rho,\kappa)\\ 0&0&1\end{bmatrix}\] (53)

where the EMA Scaling Rule error

\[\delta(\eta,\rho,\kappa)=(-\eta)\,\left(\kappa-\frac{1-\rho^{\kappa}}{1-\rho} \right)\approx(-\eta)\,\left(\kappa-\kappa+\mathcal{O}(\beta_{\rho})\right)= 0+\mathcal{O}(\eta\times\beta_{\rho}),\] (54)

where \(\beta_{\rho}\equiv 1-\rho\) and the approximation is around \(\rho=1\). 

### Limiting behavior of Polyak-Ruppert averaging

Here we sketch the asymptotic behavior of a target model \(\theta\) and its EMA \(\zeta\). Let us assume that \(\theta\) converges to the stationary distribution \(\lim_{t\to\infty}\theta_{t}=0^{*}\), \(0^{*}\sim p_{\infty}(\theta)\). We are interested in statistical properties of \(\zeta^{*}=\lim_{t\to\infty}\zeta_{t}\), as this will formalize the notion of how the EMA depends on the a time-horizon defined by its momentum \(\rho\) as discussed in Table 1.

As a warm-up, for \(n\) independent random variables \(\mathrm{x}_{1},\ldots,\mathrm{x}_{2}\), we know that the sample mean \(\tilde{x}=\frac{1}{n}(x_{1},x_{2},\ldots,x_{n})\) has the statistical properties

\[\mathbb{E}[\tilde{x}] =\mu, \operatorname{Var}[\tilde{x}] =\frac{\sigma^{2}}{n},\] (55)

where \(\mu\) and \(\sigma\) are the population mean and variance. This gives us an idea of what to expect. As we will now show, the expectation of \(\zeta^{*}\) should have no time-horizon dependence, whereas the variance of \(\zeta^{*}\) will depend on its time horizon (i.e. the number of samples it integrates over) which is defined by \(\rho\).

In the case of a weighted sum

\[\tilde{x}^{(\,\text{w})} =\sum_{i=1}^{n}w_{i}\,x_{i},\] (56)

then if the \(x_{i}\) are Independent and Identically Distributed (i.i.d.), then

\[\mathbb{E}[\tilde{x}^{(\,\text{w})}] =\sum_{i=1}^{n}w_{i}\,\mathbb{E}[x_{i}] =n\,\tilde{w}\,\mu, \tilde{w} =\frac{1}{n}\sum_{i=1}^{n}w_{i},\] (57)

and for the variance (Kish, 1965)

\[\operatorname{Var}[\tilde{x}^{(\,\text{w})}] =n\cdot\overline{w^{2}}\cdot\sigma^{2} \overline{w^{2}} =\frac{1}{n}\sum_{i=1}^{n}w_{i}^{2}, \sigma^{2} =\operatorname{Var}[x_{i}].\] (58)

We can verify that we reproduce the well-known result in Equation 55 in the case where all weights are equal to \(\frac{1}{n}\) as follows

\[\forall i:w_{i}=\frac{1}{n}\implies\overline{w^{2}}=\frac{1}{n} \cdot\sum_{i=1}^{n}\left(\frac{1}{n}\right)^{2}=\frac{1}{n^{2}}\implies \operatorname{Var}[\tilde{x}^{(\,\text{w})}]=n\cdot\frac{1}{n^{2}}\cdot \sigma^{2}=\frac{\sigma^{2}}{n}.\] (59)In the case of an exponential moving average we have

\[\zeta_{t+1}=\rho\,\zeta_{t}+(1-\rho)\,\theta_{t}=\rho^{t}\,\zeta_{1}+(1-\rho)\sum_ {i=0}^{t-1}\rho^{i}\theta_{t-i}.\] (60)

Let's consider the specific case where we are at iteration \(k\) which is sufficiently large that \(\zeta\) and \(\theta\) have converged to their stationary distributions. From \(k\), the iterations unfold as

\[\zeta_{t+1}=\rho^{t+1-k}\,\zeta_{k}+(1-\rho)\sum_{i=0}^{t-k}\rho^{i}\theta_{t- i}.\] (61)

We rearrange for terms in \(\zeta\)

\[\zeta_{t+1}-\rho^{t+1-k}\,\zeta_{k}=(1-\rho)\sum_{i=0}^{t-k}\rho^{i}\,\theta_{t -i},\] (62)

and before proceeding to the final result, using \(n=t+1-k\), we compute the convenient quantities

\[\bar{\rho} =\frac{1}{n}\sum_{i=0}^{n-1}\rho^{i}=\frac{1}{n}\times\frac{1- \rho^{n}}{1-\rho}\] (63) \[\overline{\rho^{2}} =\frac{1}{n}\sum_{i=0}^{n-1}\rho^{2i}=\frac{1}{n}\times\frac{1- \rho^{2n}}{1-\rho^{2}}.\] (64)

Taking expectation of Equation 62 and setting statistics to their stationary values, we have

\[(1-\rho^{n})\,\mathbb{E}[\zeta^{*}]=(1-\rho)\,n\,\bar{\rho}\,\mathbb{E}[\theta ^{*}]=(1-\rho^{n})\,\mathbb{E}[\theta^{*}],\] (65)

where we have used the result in Equation 57. It follows that for \(\rho\neq 1\) we have

\[\mathbb{E}[\zeta^{*}]=\mathbb{E}[\theta^{*}],\] (66)

independent of \(\rho\). Finally, we can take the variance of Equation 62. First the left hand side

\[\mathrm{Var}\left[\zeta_{t+1}-\rho^{n}\,\zeta_{k}\right]=\mathrm{Var}\left[ \zeta_{t+1}\right]+\rho^{2n}\,\,\mathrm{Var}\left[\zeta_{k}\right]=(1+\rho^{2 n})\,\,\mathrm{Var}\left[\zeta^{*}\right].\] (67)

Next the right hand side

\[\mathrm{Var}\left[(1-\rho)\sum_{t=0}^{n-1}\rho^{i}\,\theta_{t-i}\right]=(1- \rho)^{2}\,\,\mathrm{Var}\left[\sum_{t=0}^{n-1}\rho^{i}\,\theta_{t-i}\right]=( 1-\rho)^{2}\cdot\left(\frac{1-\rho^{2n}}{1-\rho^{2}}\right)\cdot\mathrm{Var}[ \theta^{*}].\] (68)

Finally, equating left and right hand sizes and rearranging for \(\mathrm{Var}[\zeta^{*}]\) gives

\[\mathrm{Var}\left[\zeta^{*}\right]=\frac{1-\rho^{2n}}{1+\rho^{2n}}\cdot\frac{1 -\rho}{1+\rho}\cdot\mathrm{Var}\left[\theta^{*}\right]\] (69)

In the limit \(t\to\infty\), the momentum-dependent prefactor becomes

\[\lim_{t\to\infty}\left(\frac{1-\rho^{2n}}{1+\rho^{2n}}\cdot\frac{1-\rho}{1+ \rho}\right)=\frac{1-\rho}{1+\rho}\implies\lim_{t\to\infty}\mathrm{Var}\left[ \zeta^{*}\right]=\frac{1-\rho}{1+\rho}\cdot\mathrm{Var}\left[\theta^{*}\right].\] (70)

Equations 69 and 70 validate our intuition. When \(\rho\to 0\), then \(\zeta\) behaves like \(\theta\) independent of \(T\), with their variance and expectation matching. When \(\rho>0\), the momentum-dependent prefactor serves as an aggregator over the history when \(t\) is sufficiently large compared to \(k\), reducing the variance \(\mathrm{Var}[\zeta^{*}]\) but preserving its expectation. This formalizes the notion of time horizon discussed in Table 1.

## Appendix F Additional details and results for Polyak-Ruppert averaging

Additional backgroundPolyak-Ruppert averaging (Definition 3.1) is a simplification of Stochastic Weight Averaging (SWA) (Izmailov et al., 2018) which uses a more complex multi-cycle schedule based weighting of the model parameters. Both Definition 3.1 and SWA present similar favorable properties like wider minima and better generalization (Izmailov et al., 2018). For example, He et al. (2022) observed that a supervised ViT-H/14 overfits on ImageNet1k (Russakovsky et al., 2014) without a model EMA, achieving an accuracy of 80.9%. Equipping a Polyak-Ruppert average (\(\rho=0.9999\)) alleviated overfitting and gave a 83.1% accuracy.

OrganizationIn this appendix, we look at additional momenta for one-dimensional noisy parabola, as well as extensions to \(D\)-dimensions (Appendix F.1), provide a more detailed view of the results of Section 3.2 (Appendix F.2), and investigate the scenario where the EMA Scaling Rule (Definition 1.2) is applied to batch normalization (Ioffe and Szegedy, 2015) coefficients (Appendix F.3).

### Noisy parabola

Additional one-dimensional examplesFirst we consider additional one-dimensional examples, investigating the effect of modifying the base momentum \(\rho_{B}\). We present \(\rho_{B}=0.99\) in Figure 7, and \(\rho_{B}=0.999\) in Figure 8.

The results for \(\rho_{B}=0.999\) are presented in main text in Figure 1.

As described by the scaling error term in Equation 54, the approximation error at a given \(\kappa\) is higher for lower momenta \(\rho\). For a large range of scalings \(\kappa\), the EMA Scaling Rule and the optimal momenta \(\rho^{*}\) are consistent. In summary, we see the synthetic experiments validate the results of Section 3.1 for a range of momenta \(\rho\).

Examples in higher dimensionsOur final use of the synthetic _noisy_ parabola will consider an extension to \(D\) dimensions. Consider the optimization of \(\bm{\theta}\in\mathbb{R}^{D}\) in a _noisy parabola_ at the origin:

\[\mathcal{L}(\bm{\theta})=\frac{a}{2}\,\bm{\theta}^{*}\bm{\theta},\qquad\bm{ \theta}_{k+1}=\bm{\theta}_{k}-\eta\,\mathbf{g}_{k},\qquad\mathbf{g}_{k}=a\, \bm{\theta}_{k}+\bm{\epsilon}_{k},\qquad\bm{\epsilon}_{k}\sim\mathcal{N} \left(\bm{0},\tfrac{b\,\mathbf{g}_{k}^{*}+c}{\kappa}\right),\] (71)

for curvature \(a>0\), scaled additive \(b>0\), and additive \(c>0\) noise coefficients. The scaling factor \(\kappa\) in the covariance denominator implements the reduction in gradient noise as the scaling (i.e., the batch size) increases (Jastrzebski et al., 2017). Let \(\bm{\theta}\in\mathbb{R}^{D}\) be optimized with SGD (Definition 2.1)

Figure 8: (a) We show the effect of scaling by comparing model EMA trajectories of the baseline (\(\kappa=1\), black dashed) to \(\kappa=8\) (left) and \(\kappa=256\) (right), with (\(\rho=\rho_{B}^{*}\), blue) and without (\(\rho=\rho_{B}\), red) the EMA Scaling Rule. (b, left) The momentum according for different scaling rules and the empirically optimal \(\rho^{*}\) (Equation 10). (b, right) The approximation error (Equation 10) of trajectories in (b, left) and the target model (orange). Error for \(\rho^{*}\) is computed using a hold-out to mitigate overfitting.

Figure 7: (a) We show the effect of scaling by comparing model EMA trajectories of the baseline (\(\kappa=1\), black dashed) to \(\kappa=8\) (left) and \(\kappa=256\) (right), with (\(\rho=\rho_{B}^{*}\), blue) and without (\(\rho=\rho_{B}\), red) the EMA Scaling Rule. (b, left) The momentum according for different scaling rules and the empirically optimal \(\rho^{*}\) (Equation 10). (b, right) The approximation error (Equation 10) of trajectories in (b, left) and the target model (orange). Error for \(\rho^{*}\) is computed using a hold-out to mitigate overfitting.

[MISSING_PAGE_EMPTY:29]

### Image Classification

HyperparametersWe present the base hyperparameters for our image experiments in Table 4.

DataFor large scale vision evaluation, we use the ImageNet1k dataset (Russakovsky et al., 2014), a widely used dataset containing approximately 1.2 million labeled images, distributed almost uniformly across 1000 different object classes, like animals, plants, and vehicles.

The images in ImageNet1k are are not consistent in resolution. To handle this, they are resized and cropped to a standard size (in our case, \(224\times 224\)), before further processing. This is part of the standard ImageNet augmentation stack for convolutional networks mentioned in Table 4.

Compute usageThe compute usage image classification Polyak-Ruppert averaging is summarized in Table 5.

Additional resultsIn Figure 12 we present a more detailed view of the results in Section 3.2. First, we see that for all train metrics, model trajectories match, and that a learning rate step schedule after warmup is present. As discussed in Figure 12, a gap in EMA Test Top-1 trajectories begins at scaling \(\kappa=4\), with a more pronounced effect visible at \(\kappa=8\). From Figure 12 it is clear that the (non-EMA)

\begin{table}
\begin{tabular}{l c} \hline \hline  & Supervised ResNet2-50 \\ \hline ImageNet1k Test Top-1 & \(76.27\pm 0.10\%\) \\ ImageNet1k EMA Test Top-1 & \(76.55\pm 0.07\%\) \\ \hline Weight initialization & kaiming\_normal(relu) \\ Backbone normalization & BatchNorm \\ Synchronized BatchNorm over replicas & No \\ Learning rate schedule & Multi step: \(\times 0.1\) at \([30,60,80]\) epochs \\ Learning rate warmup (epochs) & 5 \\ Learning rate minimum value & \(1\times 10^{-6}\) \\ Training duration (epochs) & 90 \\ Optimizer & SGD + Momentum \\ SGD momentum & 0.9 \\ Optimizer scaling rule & Linear \\ Base learning rate & 0.4 \\ Base batch size & 1024 \\ Base Polyak momentum & 0.9999 \\ Weight decay & \(1\times 10^{-4}\) \\ Weight decay scaling rule & None \\ Weight decay skip bias & Yes \\ Numerical precision & bf16 \\ Augmentation stack & ImageNet \\ Label smoothing rate & 0.1 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Supervised ResNet2-50 hyperparameters used in Polyak-Ruppert Averaging experiments.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Batch Size & GPUs & Time (h) & Compute/Run (GPUh) & Runs & Compute (GPUh) \\ \hline
512 & 8 & 35.3 & 282.4 & 9 & 2,541.6 \\
1,024 & 8 & 17.1 & 137.0 & 3 & 410.9 \\
2,048 & 8 & 13.3 & 106.7 & 9 & 960.6 \\
4,096 & 8 & 4.2 & 33.5 & 9 & 301.9 \\
8,192 & 16 & 2.8 & 44.8 & 9 & 403.6 \\ \hline All other compute, e.g. code development, runs with errors, and debugging & & & 25,768.3 \\ \hline
**Total** & & & & **30386.8** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Compute usage for image classification Polyak-Ruppert averaging in Figures 2 and 13. The three runs for the batch size 1,024 baseline correspond to three seeds, and the nine runs for all other batch sizes correspond to using and not using the EMA Scaling Rule shown in Figure 2, and its application to Batch Normalization shown in Figure 13. All experiments conducted are using 80Gb A100s.

Test Top-1 performance trajectory is no longer matching at these scalings, demonstrating that the problem is not due to a breakdown of the EMA Scaling Rule, but rather, that the model is overfitting at larger batch sizes due to batch normalization (Ioffe and Szegedy, 2015).

### Applying the EMA Scaling Rule to Batch Normalization

In Section 3.2 and Appendix F.2, we investigated a range of scalings \(\kappa\), _with_ and _without_ applying the EMA Scaling Rule to the Polyak momentum. In those experiments, we maintained Batch Normalization (Ioffe and Szegedy, 2015) coefficients of \(\rho_{\text{BN}}=0.9\) throughout11, i.e. the EMA Scaling Rule is not applied. The running statistics of Batch Normalization _are_ an EMA with values determined by \(\rho_{\text{BN}}\) and so it is reasonable to suspect we should apply the EMA Scaling Rule to \(\rho_{\text{BN}}\) also.

Footnote 11: In many ML frameworks, this value is defined using \(\beta_{\rho}=1-\rho\), i.e. the default is \(0.1\) and corresponds to \(\beta_{\text{BN}}\) rather than \(0.9\) corresponding to \(\rho_{\text{BN}}\). We use \(\rho_{\text{BN}}\) to maintain consistency across this work.

In Figure 13 we investigate the effect of applying the EMA Scaling Rule to Batch Normalization coefficients, using \(\beta_{\text{BN}}=\rho_{\text{BN}}^{\kappa}\). We observe that the Test Top-1 trajectories _with_ the EMA Scaling Rule applied are slightly closer to the reference trajectories for scalings \(\kappa\geq 2\) than those trajectories _without_ the EMA Scaling Rule. As the effect is not particularly large, at least in this setup, we do pursue further ablating applications of the EMA Scaling Rule to batch normalization coefficients, and always use \(\rho_{\text{BN}}=0.1\) for Batch Normalization, independent of \(\kappa\).

Figure 12: _ResNetv2-50 Polyak-Ruppert averaging on ImageNet_k_ for different scalings \(\kappa\). The baseline model (\(\kappa=1\), black dashed) uses batch size 1024 and momentum \(\rho_{\text{B}}=0.9999\), is scaled down to a batch size of 512 (left), and up to a batch size of 4096 (right) with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule (Definition 1.2). Bands indicate the mean and standard deviation across three runs.

## Appendix G Additional details and results for Automatic Speech Recognition (ASR)

In this section we provide additional details for the speech recognition experiments in both the supervised and semi-supervised case.

DataWe use the LibriSpeech dataset (Panayotov et al., 2015) which is a dataset of audio-transcription pairs. For supervised Polyak-Ruppert averaging experiments, we use _train-clean-100_ as training data, and for semi-supervised pseudo-labeling experiments, we use _train-clean-100_ as the labeled and _train-clean-360_ and _train-other-500_ as the unlabeled data. The standard LibriSpeech validation sets (_dev-clean_ and _dev-other_) are used to tune all hyperparameters, as well as to select the best models. Test sets (_test-clean_ and _test-other_) are only used for reporting final model performance, measured in WER without an external language model. We maintain the original 16kHz sampling rate, and compute log-mel filterbanks with 80 coefficients for a 25ms sliding window, strided by 10ms, later normalized to zero mean and unit variance for each input sequence.

Acoustic modelWe employ a vanilla encoder-based only transformer model trained with the Connectionist Temporal Classification (CTC) loss (Graves et al., 2006). We use the training configuration from Likhomanenko et al. (2021), which has three stages: i) 1D convolutions to perform striding (kernel of 7 with stride of 3); ii) a Transformer encoder with 36 layers, post-LayerNorm, four attention heads, an embedding dimension of 768, an MLP dimension of 3072, a dropout frequency of 0.3, and a layer drop frequency of 0.3; and iii) a linear layer to map to the target vocabulary12. To reduce model training time by a factor of approximately \(2-3\times\), and to reduce memory footprint,

Figure 13: _ResNetv2-50 Polyak-Ruppert averaging on ImageNet1\(k\)_ for different scalings \(\kappa\). The baseline model (\(\kappa=1\), black dashed) uses batch size 1024 and momentum \(\rho_{\text{B}}=0.9999\), is scaled down to a batch size of 512 (left), and up to a batch size of 4096 (right) with the EMA Scaling Rule applied to _only_ model parameters (blue, \(\rho=\rho_{B}^{\kappa}\)), and model parameters _and_ buffers (orange, \(\rho=\rho_{B}^{\kappa}\) (\(\dagger\))). Bands indicate the mean and standard deviation across three runs.

we use CAPE positional embeddings (Likhomanenko et al., 2021) instead of relative positional embeddings (Shaw et al., 2018): both models perform similarly.

TrainingHere we discuss our training procedure for base batch size \(B=8\times 290s\), which is adapted from Likhomanenko et al. (2021), and is summarized in Table 6. We use SpecAugment (Park et al., 2019) activated after 5k steps of training: two frequency masks with frequency mask parameter \(F=30\), ten time masks with maximum time-mask ratio \(p=0.1\) and time mask parameter \(T=50\) are used; time warping is not used.

One difference in setup is we use the Adam optimizer, whereas Likhomanenko et al. (2021) used Adagrad (Duchi et al., 2010). Even though Adagrad can be viewed as a particular limit (\(\beta_{1}=0\) and \(\beta_{2}\to 1\)) of Adam (Kingma and Ba, 2015), we were unable to produce reasonable optimization in practice when applying the Adam Scaling Rule of Malladi et al. (2022) in this limit. As a consequence, we chose to work with the Adam optimizer, where its scaling rule has been shown to work (Malladi et al., 2022), and we take \(\beta_{1}=0.995\), \(\beta_{2}=0.999\), and \(\epsilon=10^{-8}\). We obtained similar results for \(\beta_{1}=0.99\). Finally, we use a linear learning rate warmup (64k steps) after which the learning rate is kept constant until convergence. This performance can be improved further by using a step decay schedule as shown in prior work. We also apply gradient clipping of 1, and do not use weight decay.

Pseudo-LabelingThe pseudo-labeling process comprises of two stages: i) The pre-training phase, where we train model on labeled data for 20k steps with model EMA accumulation starting after 19k steps; and ii) the pseudo-labeling phase, where we involve unlabeled data by generating pseudo-labels from the model EMA (teacher) and provide them to the model (student) as if they were ground-truth labels. Pseudo-labels are generated without any dropout applied to the teacher, and no data augmentation is applied for the corresponding inputs. To produce the pseudo-label, we use _hard transcription_ (Definition G.1)

**Definition G.1** (Hard Transcription).: _For a sequence of frames, select the most probable token per frame, removing repetitions and the CTC blank token. For example, "h#ecllll#ll#oo" is transformed into "hello", where "#" is the CTC blank token._

These hard transcriptions are then used as transcription for student optimization. We use a 1:3 proportion of labeled to unlabeled data as this was found to be optimal in Likhomanenko et al. (2021), and we decrease model dropout and layer drop rates to 0.1 after pre-training phase. As we have access to the ground-truth labels on the data being treated as unlabeled, we can track

\begin{table}
\begin{tabular}{l c c} \hline \hline  & Supervised & Pseudo-Labeling \\ \hline Librispeech test-clean / test-other WER & 7.8/19.1 & 4.8/11.5 \\ \hline Optimizer & Adam & Adam \\ Optimizer scaling rule & Adam & Adam \\ Base (\(\beta_{1},\beta_{2}\)) & (0.995, 0.999) & (0.995, 0.999) \\ Base learning rate & 0.0001 & 0.0001 \\ Base learning rate warmup (steps) & 64k & 64k \\ Learning rate schedule & Fixed (no decay) & Fixed (no decay) \\ Learning rate minimum value & 0 & 0 \\ Base training duration (steps) & 400k & 500k \\ Base batch size (dynamic) & \(8\times 290s\) & \(8\times 290s\) \\ Base teacher momentum & 0.99995 & 0.9999 \\ Weight decay & None & None \\ Numerical precision & bf16 & bf16 \\ Augmentation stack & SpecAug & SpecAug \\ Dropout & 0.3 & \(0.3\to 0.1\) \\ Layer drop & 0.3 & \(0.3\to 0.1\) \\ Gradient clipping & 1 & 1 \\ Labeled:unlabeled data ratio & N/A & 1:3 \\ Base pre-training steps & N/A & 20k \\ Base start of EMA accumulation (steps) & N/A & 19k \\ \hline \hline \end{tabular}
\end{table}
Table 6: Hyperparameters summary for speech recognition task for supervised (left) and semi-supervised pseudo-labeling (right) training with a vanilla transformer. The \(0.3\to 0.1\) in the dropout and layer drop rates indicates that a rate of 0.3 is used during pre-training, and a rate of 0.1 is used during pseudo-labeling.

[MISSING_PAGE_FAIL:34]

gives discrepancies with the reference trajectory, however they are negligible compared to models trained without EMA Scaling Rule. For the semi-supervised training, to alleviate the difficulties with a breakdown of the Adam Scaling Rule for large \(\kappa\) we postpone the pseudo-labeling process until the model reaches similar WER as the baseline. This allows us to align the initial model conditions for pseudo-labeling. In this scenario we are able to match the reference trajectory up to \(\kappa=8\).

Figure 14: _Transformer Polyak-Ruppert averaging on LibriSpeech (trained on train-clean-100)_ with different scalings \(\kappa\). The baseline (\(\kappa=1\), black dashed) is trained with Adam and momentum \(\rho_{B}=0.99995\) at a _dynamic batch size_\(B=8\times 290\varsigma\), which corresponds to a single train step on the \(x\)-axis. We investigate dynamic batch sizes down to \(B=2\times 290\varsigma\) (left) and up to \(B=32\times 290\varsigma\) (right), with (blue, \(\rho=\rho_{B}^{\kappa}\)), and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule (model non-EMA is marked by orange). The Adam Scaling Rule (Malladi et al. (2022), Definition C.3) is used throughout. For momentum \(\rho_{B}=0.9999\) we observe similar trajectories for all models.

Figure 15: _Transformer Polyak-Ruppert averaging on LibriSpeech (trained on train-clean-100)_ with different scalings \(\kappa\). The baseline (\(\kappa=1\), black dashed) is trained with Adam and momentum \(\rho_{B}=0.999\) at a _dynamic batch size_\(B=8\times 290\varsigma\), which corresponds to a single train step on the \(x\)-axis. We investigate dynamic batch sizes down to \(B=2\times 290\varsigma\) (left) and up to \(B=32\times 290\varsigma\) (right), with (blue, \(\rho=\rho_{B}^{\kappa}\)), and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule (model non-EMA is marked by orange). The Adam Scaling Rule (Malladi et al. (2022), Definition C.3) is used throughout. If momentum \(\rho_{B}\) is small and accumulation history is short we observe no any significant difference between models which all are matching the reference trajectory despite scaling \(\kappa\).

We note that this result reveals that errors for the Adam Scaling Rule _and_ the EMA Scaling Rule are contributing, although the way in which they contribute is different, and one can dominate the other. We observe in Figure 16 that if the initial conditions of the models are similar (attained by using the same WER as a condition to begin pseudo-labeling) then the error from the EMA Scaling Rule dominates over that of the Adam Scaling Rule, causing a divergence in training dynamics.

Figure 16: _Transformer pseudo-labeling on LibriSpeech (trained on train-clean-100 as labeled and the rest of LibriSpeech as unlabeled)_ with different scalings \(\kappa\). The baseline (\(\kappa=1\), black dashed) is trained with Adam at a _dynamic batch size_ of \(8\times 290\) seconds, which corresponds to a single train step on the \(x\)-axis. The model EMA (_teacher_) is updated with momentum \(\rho_{B}=0.9999\). We investigate dynamic batch sizes down to \(B=2\times 290s\) (left) and up to \(B=64\times 290s\) (right), with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule. The Adam Scaling Rule (Malladi et al. (2022), Definition C.3) is used throughout. For \(\kappa\leq 2\), we start pseudo-labeling after \(20k/\kappa\) training steps; while for \(\kappa>2\), we start when pre-training WER matches the baseline WER (\(24k/\kappa\) for \(\kappa=4\) and \(29k/\kappa\) for \(\kappa=8\)). For \(\kappa=4\) we experimented with both variants: we start pseudo-labeling after \(20k/\kappa\) (dashed) and when pre-training WER matches the baseline WER (solid, \(24k/\kappa\)).

Figure 17: _Transformer pseudo-labeling on LibriSpeech (using train-clean-100 as labeled)_ with different scalings \(\kappa\). The baseline (\(\kappa=1\), black dashed) is trained with Adam at a _dynamic batch size_ of \(8\times 290\) seconds, which corresponds to a single train step on the \(x\)-axis. The model EMA (_teacher_) is updated with momentum \(\rho_{B}=0.999\). We investigate dynamic batch sizes down to \(B=2\times 290s\) (left) and up to \(B=16\times 290s\) (right), with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule. The Adam Scaling Rule is used throughout.

Second, we observe in practice that the EMA Scaling Rule holds for both fixed batching (a sequence length in the batch can vary significantly) and for dynamic batching (when total number of frames in the batch is fixed, though padding still is accounted to the this amount). This shows that EMA Scaling Rule is applicable to sequential data too.

Third, we observe in Figures 15 and 17 that for smaller values of \(\rho_{\text{B}}\), scaling with or without EMA Scaling Rule behave similarly, and reference trajectories match in the supervised and semi-supervised cases. However, if the momentum is too large, the _teacher_ moves slowly and is uninformative, whereas if the momentum is too low, the _teacher_ and the _student_ are effectively be the same model, implying: i) the student will self-predict with high confidence, removing any benefits of distillation14; and ii) training instability or model divergence will happen in the low-resource settings (Likhomanenko et al., 2021; Higuchi et al., 2022).

Footnote 14: He et al. (2020) alleviated the problem with the proper amount of noise during _student_ model training, whilst Xu et al. (2020) used beam-search decoding with a language model.

### Scaling to \(\kappa=16\) with Progressive Scaling

Finally, we aim to scale semi-supervised pseudo-labeling further to \(\kappa=16\). In this case we observe that Adam Scaling Rule does not hold in the pre-training phase and there is no model convergence. To overcome this, we apply Progressive Scaling (Definition 3.2). We pre-train models on supervised data with \(\kappa=8\) for 29k of reference steps (model EMA accumulation starts at 28k steps). We then scale to \(\kappa=16\) and begin pseudo-labeling. We see in Figure 18 that Progressive Scaling enables us to scale pseudo-labeling to \(\kappa=16\) with (middle) and without (left) the EMA Scaling Rule. Second, models _with_ the EMA Scaling Rule track the baseline much closer than models without the EMA Scaling Rule, although a small gap is present. We further experimented with Progressive Scaling, postponed the transition condition to the \(\kappa=16\) until 75k reference steps. In Figure 18 (right), we see this scaled model tracks the reference trajectory, and so using a combination of the EMA Scaling Rule and Progressive Scaling, we are able to scale pseudo-labeling to \(\kappa=16\), corresponding to a dynamic batch size of \(128\times 290s\).

Figure 18: _Transformer pseudo-labeling on LibriSpeech (trained on train-clean-100 as labeled and the rest of LibriSpeech as unlabeled) with different Progressive Scaling from \(\kappa=8\) to \(\kappa=16\) (\(\kappa=8\to 16\)). The baseline (\(\kappa=1\), black dashed) is trained with Adam at a dynamic batch size of \(8\times 290\) seconds, which corresponds to a single train step on the \(x\)-axis. The model EMA (_teacher_) is updated with momentum \(\rho_{\text{B}}=0.9999\). The scaling with \(\kappa=8\) is shown with lighter color for reference from Figure 16. We investigate dynamic batch sizes progressively from \(B=64\times 290s\) to \(B=128\times 290s\), with (blue, \(\rho=\rho_{\text{B}}^{\kappa}\)) and without (red, \(\rho=\rho_{\text{B}}\)) the EMA Scaling Rule. For reference (top) we show the learning rate schedule with Progressive Scaling. The Adam Scaling Rule (Malladi et al. (2022), Definition C.3) is used throughout. Left and middle correspond to Progressive Scaling with scale from \(\kappa=8\) to \(\kappa=16\) at 29k steps, while right corresponds to 75k steps._

Additional details and results for self-supervised image representation learning

OrganizationThis appendix is structured into three sections. We first give an overview of our chosen SSL method BYOL (Appendix H.1), our recipe for training BYOL using ResNet 18s (Appendix H.2), our recipe for training BYOL using Vision Transformers (ViTs) (Appendix H.3), ablations of normalization approaches that lead to the development of this recipe (Appendix H.4), and additional results corresponding to longer training duration (Appendix H.5) and further understanding the impact of Progressive Scaling (Appendix H.6).

Second, we demonstrate that the EMA Scaling Rule combined with Progressive Scaling can scale a ResNet-50 BYOL model trained with LARS to batch size 32,768 without performance drop, demonstrating the empirical utility of the tools we provide outside of their theoretical validity (Appendix H.10).

Finally, we show that it is possible to systematically scale DINO (Caron et al., 2021) using a combination of Progressive Scaling and the EMA Scaling Rule, providing a solution for researchers and practitioners wanting to train DINO at scale.

### Components of self-supervised learning

First, a key component of many SSL methods is the _stop-gradient_ or StopGrad (Definition H.1).

**Definition H.1** (Stop Gradient/StopGrad( \(\cdot\) )).: _The stop-gradient operator StopGrad( \(\cdot\) ) prevents the flow of gradient information_

\[\frac{df(StopGrad(h(x;\bm{\omega}));\bm{\theta})}{d\bm{\omega}}\equiv 0\] (72)

_for all parametric functions \(h\) and \(f\) and for all parameters \(\bm{\theta}\) and \(\bm{\omega}\)._

Applying a _stop-gradient_ is sometimes called _detaching_ in the literature. Now, we introduce the update rule of our representative SSL method BYOL in Definition H.2.

**Definition H.2** (BYOL Update).: _BYOL learns unsupervised features by minimizing the cosine distance between the predictions of a student backbone \(f(\,\cdot\,;\bm{\theta})\) (typically a ResNet or Vision Transformer), projected through \(h(\,\cdot\,;\bm{\omega})\) (typically a Multi-Layer Perceptron (MLP)), and the predictions of an EMA teacher \(f(\,\cdot\,;\bm{\xi})\)(Grill et al., 2020). The update for the parameters of BYOL is then_

\[(\bm{\theta}_{t+1},\bm{\omega}_{t+1}) =(\bm{\theta}_{t},\bm{\omega}_{t})-\eta\times\frac{1}{B}\sum_{x\in \mathbb{B}}\nabla_{(\bm{\theta},\bm{\omega})}\mathcal{L}(x;\bm{\theta}_{t}, \bm{\omega}_{t},\bm{\xi}_{t})\] (73) \[\xi_{t+1} =\rho\,\xi_{t}+(1-\rho)\,\bm{\theta}_{t+1}\] (74) \[\text{with}\ \ \ \mathcal{L}(x;\bm{\theta}_{t},\bm{\omega}_{t},\bm{\zeta}_{t}) =\frac{1}{2}\cos\left[h(f(x_{1};\bm{\theta}_{t});\bm{\omega}_{t}),\text{StopGrad}(f(x_{2};\xi_{t}))\right]+(x_{1}\leftrightarrow x_{2}),\] (75)

_where \(\cos(\bm{a},\bm{b})\equiv 1-\bm{a}\cdot\bm{b}/(||\bm{a}||\,||\bm{b}||)\) is the cosine distance, and \(x_{1}\) and \(x_{2}\) are two views of a single variate \(x\), often produced by augmentations, and \(x_{1}\leftrightarrow x_{2}\) denotes symmetrization over \(x_{1}\) and \(x_{2}\)._

As noted in Section 3.4,the BYOL EMA update (Equation 74) uses \(\bm{\theta}_{t+1}\) instead of our analyzed \(\bm{\theta}_{t}\) (Equation 4). The effect upon the overall EMA update is \(\mathcal{O}(\eta\times\beta_{\rho})\) and so is captured by the EMA Scaling Rule (Definition 1.2).

One more piece of technology typically employed in SSL is a _tracking probe_ (Definition H.3) which we will use to evaluate the performance of BYOL on downstream tasks of interest, for example, image classification.

**Definition H.3** (Tracking Probe/Linear Probe).: _When optimizing model parameters \(\bm{\omega}_{t}\) of an SSL method, simultaneously optimize the parameters \(\xi\) of a probe model \(r(\,\cdot\,;\xi)\) under a downstream objective \(\mathcal{L}^{(d)}\). For example, in classification, with data \(x\) and samples \(y\)_

\[\mathcal{L}^{(d)}(x,y,\bm{\theta}_{t},\bm{\xi}_{t}) =-\log P(y|r(\text{StopGrad}(h(x;\bm{\omega}_{t}));\xi))\] (76) \[\mathcal{L}^{(total)}(x,y;\bm{\theta}_{t},\bm{\omega}_{t},\bm{ \zeta}_{t},\bm{\xi}_{t}) =\mathcal{L}(x;\bm{\theta}_{t},\bm{\omega}_{t},\bm{\zeta}_{t})+ \mathcal{L}^{(d)}(x,y,\bm{\omega}_{t},\bm{\xi}_{t}),\] (77)

_The is a probe for the teacher, which is typically the better choice due to Polyak-Ruppert averaging effects (see Section 3.2). When the \(r\) is a linear model, the tracking probe is called a linear probe._It is also typical to use a Batch Normalization layer _without_ trainable affine terms before this linear layer as in He et al. (2022) to stabilize probe training. In this case, the running statistics can be absorbed into a definition of the linear layer weights and biases, and so this is still a _linear probe_, although we will call this a _pre-bn linear probe_ to remove ambiguity.

### A ResNet-18 recipe for BYOL

HyperparametersWe present the base hyperparameters for training BYOL with a ResNet-18 backbone using SGD in Table 9. This recipe was developed by starting from a well-known BYOL ResNet-50 recipe (Grill et al., 2020), adapting the input augmentations for CIFAR10, and performing a search over learning rate choices for an SGD optimizer.

### A Vision Transformer recipe for BYOL

HyperparametersWe present the base hyperparameters for training BYOL with a ViT-B/16 backbone in Table 10. This recipe was developed by starting from a well-known supervised ViT-B/16 recipe (He et al., 2022) and performing a search over weight decay and learning rate hyperparameter choices. We find that BYOL performs well with heavy weight decay (\(\lambda=0.3\)) and a low learning rate (\(\eta=10^{-3}\)) at a base batch size \(B=4096\). The AdamW optimizer is used, and so for scaling to other batch sizes \(\tilde{B}=\kappa B\) we use the Adam Scaling Rule (Definition C.3)15 We use a pre-bn linear probe as discussed in Appendix H.1. Finally, the performance of BYOL can be further improved by employing multicorp (Caron et al., 2020) by \(\approx+2\%\) in absolute test top-1 performance on ImageNet1k compared to without multicorp, however, as this is not our focus, we omit this from the presented recipe.

Footnote 15: We note that Adam (Kingma and Ba, 2015) and AdamW (Loshchilov and Hutter, 2019) are equivalent in the limit of zero weight decay, and that the Adam Scaling Rule (Definition C.3) was derived with zero weight decay (Malladi et al., 2022).

Additional backgroundAchieving large scale SSL training with ViTs to large scale SSL training has been a long standing goal in the community. MoCo-v3 (Chen et al., 2021) enables the use of ViTs with contrastive learning, but achieves this through modifications of the ViT training procedures, including gradient freezing on the image patching layer, and re-introducing Batch Normalization to post-attention MLP layers. Despite these modifications, MoCo-v3 was only trained up to a batch size of 6144, where model performance begins to suffer (Chen et al., 2021). In Figure 6 we demonstrate that combining dynamic batch scaling (Appendix C.4) with the EMA Scaling Rule

\begin{table}
\begin{tabular}{l c} \hline \hline  & ResNet-18 \\ \hline Weight initialization & kaiming\_uniform(He et al., 2015) \\ Backbone normalization & BatchNorm \\ Head normalization & BatchNorm \\ Synchronized BatchNorm over replicas & Yes \\ Learning rate schedule & Single Cycle Cosine \\ Learning rate warmup (epochs) & 20 \\ Learning rate minimum value & 0 \\ Training duration (epochs) & 100 \\ Optimizer & SGD \\ Optimizer scaling rule & SGD \\ Optimizer momentum & 0.9 \\ Gradient clipping & 0.1 \\ Base learning rate & 0.02 \\ Base batch size & 1024 \\ Base teacher momentum & 0.992 \\ Weight decay & \(1\times 10^{-6}\) \\ Weight decay scaling rule & None \\ Weight decay skip bias & Yes \\ Numerical precision & tf32 \\ Augmentation stack & BYOL CIFAR10 \\ \hline \hline \end{tabular}
\end{table}
Table 9: BYOL ResNet-18 hyperparameters for CIFAR10(Definition 1.2) enables BYOL to be trained using ViTs to batch sizes of 24,576 without any drop in performance compared to the reference batch size of 4096. We emphasize that the piecewise transitions in the schedules are important for preserving training dynamics.

### The role of Batch Normalization and Layer Normalization in BYOL with ViTs

Here we compare the roles of Batch Normalization (BatchNorm, Ioffe and Szegedy (2015)) and Layer Normalization (LayerNorm, Ba et al. (2016)) in the projection and prediction heads of BYOL (Grill et al., 2020) using ViTs.

It has been observed that BatchNorm plays a critical role in BYOL predictor and projector dynamics (Fetterman and Albrecht, 2020), and using either LayerNorm or _no normalization_ significantly decreases model performance. Subsequently, it was demonstrated (Richemond et al., 2020) that competitive BYOL performance could be achieved through a combination of Group Normaliza

\begin{table}
\begin{tabular}{l c} \hline \hline  & BYOL ViT-B/16 \\ \hline ImageNet1k Linear Probe Test Top-1 & 74.47\% (Figure 19) \\ \hline Weight initialization & trunc\_normal(.02) \\ Backbone normalization & LayerNorm \\ Head normalization & BatchNorm \\ Synchronized BatchNorm over replicas & No \\ Learning rate schedule & Single Cycle Cosine \\ Learning rate warmup (epochs) & 40 \\ Learning rate minimum value & \(1\times 10^{-6}\) \\ Training duration (epochs) & 480 \\ Optimizer & AdamW \\ Optimizer scaling rule & Adam \\ Base \((\beta_{1},\beta_{2})\) & (0.9, 0.95) \\ Base learning rate & \(1\times 10^{-3}\) \\ Base batch size & 4096 \\ Base teacher momentum & 0.99 \\ Weight decay & 0.3 \\ Weight decay scaling rule & None \\ Weight decay skip bias & Yes \\ Numerical precision & bf16 \\ Augmentation stack & BYOL (Grill et al., 2020) \\ Stochastic depth & 0.1 \\ \hline \hline \end{tabular}
\end{table}
Table 10: BYOL ViT-B/16 hyperparameters.

Figure 19: _BYOL ViT-B/16 on ImageNet1k_ for different scalings \(\kappa\). We present runs comparing LayerNorm (blue) to BatchNorm (red) in the projection and prediction heads of BYOL ViT models for batch size 3072 (dashed) and 24,576 (solid) _without the EMA Scaling Rule_. \(\kappa=1\) corresponds to \(B=4096\). In all scenarios the transformer backbone _only_ uses LayerNorm. We truncate the training of the large batch size LayerNorm variant to preserve compute (indicated by \(\times\)).

tion (GroupNorm, Wu and He (2018)) and Weight Standardization (Qiao et al., 2019). Additionally, Richemond et al. (2020) showed that if BatchNorm is used in the backbone, one can use LayerNorm or _no normalization_ in the predictor and projector without any performance drop.

In this work, we we show it is possible to train BYOL ViT using _only LayerNorm_ across the backbone, projector and predictor (see Figure 19), decoupling BYOL's reliance on batch statistics, a desirable trait for a representation learning algorithm (Brock et al., 2021). At batch size 3072, using LayerNorm in the predictor and projector achieves competitive performance (74.10%), performing slightly worse than using BatchNorm (74.47%). At the larger batch size of 24,576, runs perform significantly worse as the EMA Scaling Rule was not applied.

### Longer training duration with incremental Progressive Scaling

Here we use the same base hyperparameters as Table 10, except that we train for 480 instead of 300 epochs. To mitigate the student impulse phenomena discussed in Section 3.4, in Figure 20 we investigate increasing the batch size every 60 epochs using Progressive Scaling (Definition 3.2). We observe that this more gradual procedure enables closer tracking of the baseline train loss trajectory. Additionally, this procedure results in a scaled linear probe performance that outperforms the baseline (75.64% compared to the baseline performance of 74.47%). The same procedure can be applied to the LayerNorm variant discussed in Appendix H.4, which produces a similar result (75.09% compared to the baseline performance of 74.10%).

### Building intuition around Progressive Scaling and momentum sensitivity

Our final BYOL ViT results are to help build intuition around Progressive Scaling (Definition 3.2), as well as when the EMA Scaling Rule is most important. In Figure 21 we explore transitioning from the baseline batch size 4096 model to batch size 24,576 in a _single transition_ after 60 epochs. After this transition, we continue training for 240 epochs for a range of momenta: \(\rho\in\{0.8,0.9,0.95,0.97,0.9867,0.994,0.999\}\)_without_ the EMA Scaling Rule.

We observe that after the transition, any \(0.9\leq\rho\leq 0.994\) produces a linear probe performance that matches or outperforms the baseline at the end of training. This indicates that after the initial training period, BYOL becomes less sensitive to the choice of teacher momentum. Note that without the initial 60 epochs of training with batch size 4096, _all models_, including those employing the EMA Scaling Rule diverge (see \(B=24,576\) in Figure 6).

We present an illustration for why this might happen in Figure 22. First, we see that using the EMA Scaling Rule _always_ keeps the model within the acceptable momentum region. We also wee that _not_ using the EMA Scaling Rule can keep the model within the acceptable momentum region for a range of batch sizes, depending on how large wide in momenta the acceptable region is at the base batch size. Finally, we see that the momentum value matters much more at low values of momenta

Figure 20: _BYOL ViT-B/16 on ImageNet1k_ for different scalings \(\kappa\). The baseline model (\(\kappa=0.75\), black dashed) uses batch size 3072 and teacher momentum \(\rho_{B}=0.99\). We increment the batch size by 3072 every 60 epochs to a final batch size of 24,576 using Progressive Scaling (Definition 3.2).

[MISSING_PAGE_EMPTY:42]

Next, the cost of a single momentum ablation presented in Figure 21 is 240 epochs at batch size 24,576, which is \(\approx 240/480\times 1900.4\,\text{GPUh}=950.2\,\text{GPUh}\), giving a total cost over seven runs of \(6651.4\,\text{GPUh}\).

Finally, providing a full view of the investigations carried out for the ViT BYOL is given in Table 14.

### ResNet-18 hyperparameter sensitivity analysis

To demonstrate that the EMA Scaling Rule works for a broad range of optimization hyperparameters (i.e. _beyond_ those presented in Figure 5 and Section 3.4), we provide a sensitivity analysis for base teacher momentum \(\rho_{B}\) and base learning rate \(\eta_{B}\) in the challenging setting of BYOL.

Base teacher momentumIn Figure 23 we show the effect of changing the base teacher momentum \(\rho_{B}\), defined at batch size 1024. The EMA Scaling Rule is robust to modifications of momentum down to \(\rho_{B}\approx 0.946\) in this particular setting. Below \(\rho_{B}\approx 0.946\), matching is poor, although the smallest momentum in this setting corresponds to \(0.841^{4}\approx 0.5\), which is a particularly small teacher momentum, and is unlike to provide utility over the using the target model (see Appendix E.2).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Stage & Batch Size & GPUs & Time (h) & Compute (GPUh) \\ \hline
1 & 6,144 & 32 & 3.5 & 113.0 \\
2 & 9,216 & 48 & 3.1 & 149.8 \\
3 & 12,288 & 64 & 2.8 & 176.0 \\
4 & 15,360 & 80 & 2.3 & 186.5 \\
5 & 18,432 & 96 & 2.1 & 202.9 \\
6 & 21,504 & 112 & 2.1 & 235.8 \\
7 & 24,576 & 128 & 1.9 & 241.3 \\ \hline
**Total** & & & & **1,305.2** \\ \hline \hline \end{tabular}
\end{table}
Table 13: Compute usage for ViT BYOL investigation into incremental scaling in Figure 20. Values _include_ node allocation times (typically a small % of corresponding total runtime), giving a practical estimate of reproduction cost. All experiments conducted are using 80Gb A100s for 60 epochs. Stage 0 corresponding to the baseline in Figure 20 is the run detailed in the first row of Table 12, using a batch size of 3,072, Batch Normalization, and 16 GPUs. Computing only the first 60 epochs of stage 0 corresponds to approximately 127.7 GPUh, which would bring the total cost of Figure 20 to 1,432.9 GPUh.

\begin{table}
\begin{tabular}{l c c} \hline \hline  & & Compute (GPUh) \\ \hline Baselines (Figure 6 and Table 11) & 8,863.1 \\ BatchNorm and LayerNorm (Figure 19 and Table 12) & 3,886.2 \\ Incremental scaling (Figure 20 and Table 13) & 1,305.2 \\ Momentum ablations (Figure 21) & 6,651.4 \\ All other compute, e.g. code development, runs with errors, and debugging & 84,984.1 \\ \hline
**Total** & **105,690.0** \\ \hline \hline \end{tabular}
\end{table}
Table 14: Total compute usage for ViT BYOL investigations.

Base learning rateIn Figure 24 we show the effect of changing the base learning rate \(\eta_{B}\), defined at batch size 1024. The EMA Scaling Rule is robust over a wide range of learning rates. At the largest learning rate \(\eta_{B}=0.5\) matching starts to become poor at scaling \(\kappa=4\).

Figure 23: _ResNet-18 BYOL on CIFAR10 teacher momentum sensitivity_ (\(\eta_{B}=0.08\)) for scalings \(\kappa\in\{2,4\}\) and base teacher momenta \(\rho_{B}\in\{0.841,0.946,0.974,0.987,0.992,0.997\}\) defined at \(\kappa=1\). The baseline (\(\kappa=1\), black dashed) uses batch size 1024, and is scaled from batch size 2048 (left) to 4096 (right) with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule. Bands indicate mean and standard deviation across three runs.

### ResNet-18 additional scaling analysis

To demonstrate that the EMA Scaling Rule works for a broad range of scalings \(\kappa\) (i.e. _beyond_ those presented in Figure 5 and Section 3.4), we investigate scaling down to \(\kappa=1/8\) in Figure 25. We see that the EMA Scaling Rule works well down to the small batch size of 128, although matching is not perfect. We suspect this is due to the presence of Batch Normalization layers in the ResNet-18 architecture, which underperform at small batch sizes (Ioffe and Szegedy, 2015). The synthetic analysis of Section 3.1 instead demonstrated the EMA Scaling Rule holding for scalings spanning factors of \(\kappa\) that differ by 1024, with scaling error insensitive to the value of \(\kappa\) for sufficiently low \(\kappa\) (see Figure 0(b)).

Figure 24: _ResNet-18 BYOL on CIFAR10 learning rate sensitivity_ (\(\rho_{B}=0.992\)) for scalings \(\kappa\in\{2,4\}\) and base learning rates \(\eta_{B}\in\{0.01,0.02,0.04,0.15,0.20,0.50\}\) defined at \(\kappa=1\). The baseline (\(\kappa=1\), black dashed) uses batch size 1024, and is scaled from batch size 2048 (left) to 4096 (right) with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule. Bands indicate mean and standard deviation across three runs.

### Scaling a ResNet-50 BYOL using LARS and Progressive Scaling

Here we investigate whether Progressive Scaling and the EMA Scaling Rule can be used in practice where there is no known optimizer SDE approximation. We use the default 300 epoch configuration for BYOL (Grill et al., 2020) in Figure 26. We see that although trajectories during training do not match, we are able to match or surpass the linear probe performance of the BYOL baseline at the larger batch size if 32,768. _This indicates that the contributions of our work have practical utility beyond the theoretical constraints._

The compute usage for the BYOL ResNet using LARS is detailed in Table 15.

Figure 25: _ResNet-18 BYOL on CIFAR10 lower scaling analysis (\(\eta_{B}=0.08\), \(\rho_{B}=0.992\)) for scalings \(\kappa\in\{1/8,1/4,1/2\}\). The baseline (\(\kappa=1\), black dashed) uses batch size 1024, and is scaled from batch size 128 (left) to 512 (right) with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule. Bands indicate mean and standard deviation across three runs._

\begin{table}
\begin{tabular}{c c c c} \hline Batch Size & GPUs & Time (h) & Compute (GPUh) \\ \hline \(4,096\to 32,768\) (120 Epochs) & 128 & 14.1 & 1809.8 \\ \(4,096\to 32,768\) (60 Epochs) & 128 & 12.9 & 1655.9 \\ \(4,096\) & 16 & 32.8 & 524.9 \\ \hline All other compute, e.g. code development, runs with errors, and debugging & 17,654.6 \\ \hline
**Total** & & **21645.2** \\ \hline \end{tabular}
\end{table}
Table 15: Compute usage for ResNet 50 LARS investigation in Figure 26. Values _include_ node allocation times (typically a small % of corresponding total runtime), giving a practical estimate of reproduction cost. All experiments conducted are using 80Gb A100s.

### Preventing collapse phenomena in DINO at scale

Until now, our representatives SSL method has been BYOL for reasons discussed in Section 3.4. Here, we will turn our attention to DIstillation with NO labels (DINO) (Caron et al., 2021), which has the update rule presented in Definition H.4.

**Definition H.4** (DINO Update).: _DINO learns unsupervised features by matching predictions over emergent pseudo-labels of a student backbone and head \(f(\cdot\,;\theta)\) to those of an EMA teacher \(f(\cdot\,;\xi)\) through a cross-entropy guided distillation procedure. DINO has a additional centering procedure, which is a form of batch normalization with momentum \(\rho_{\mathrm{c}}=0.9\) which we do not scale using the

Figure 26: _ResNet50 BYOL on ImageNet1k using LARS for different configurations of progressive scaling. The baseline (black dashed) uses batch size 4096 and momentum \(\rho_{B}=0.99\). We consider progressive scaling (blue) smoothly from epoch 60 for 60 epochs (left) and 120 epochs (right) up until batch size 32,768, scaling the learning rate linearly, and applying the EMA Scaling Rule.__EMA Scaling Rule. The update for the parameters of DINO is_

\[\theta_{t+1} =\theta_{t}-\eta\times\frac{1}{B}\sum_{x\in\mathbb{B}}\nabla_{ \theta}\mathcal{L}(x;\theta_{t},\zeta_{t},\mathbf{c}_{t})\] (78) \[\zeta_{t+1} =\rho\,\zeta_{t}+\left(1-\rho\right)\theta_{t+1}\] (79) \[\mathbf{c}_{t+1} =\rho_{c}\,\mathbf{c}_{t}+\left(1-\rho_{c}\right)\mathbb{E}_{x^{ \prime}}\zeta(x^{\prime})\] (80) \[\text{with}\;\;\mathcal{L}(x;\theta_{t},\zeta_{t},\mathbf{c}_{t}) =H\big{(}f(x_{1},\theta_{t}),f(x_{2},\zeta_{t})-\mathbf{c}_{t} \big{)}+(x_{1}\leftrightarrow x_{2}),\] (81)

_where \(H(\bm{a},\bm{b})\equiv-\sum_{m=1}^{M}p_{m}(\bm{a})\log p_{m}(\bm{b})\) is the cross-entropy between categorical distributions over \(M\) (emergent pseudo-)classes given logits \(\bm{a},\bm{b}\in\mathbb{R}^{M}\), \(x_{1}\) and \(x_{2}\) are two views of a single variate \(x\), often produced by augmentations, and \(x_{1}\leftrightarrow x_{2}\) denotes symmetrization over \(x_{1}\) and \(x_{2}\)._

In practice, DINO employs multi-crop (Caron et al., 2021). We omit this detail for clarity of presentation, although we _do_ use multi-crop in the experiments that follow.

Our interest DINO is due to the difficulty in its optimization16, and in particular, preventing collapse phenomena in DINO at batch sizes above 1024, which is an open research problem. In this section, we will show that a combination of the EMA Scaling Rule (Definition 1.2) and Progressive Scaling (Definition 3.2) enable training of DINO beyond batch size 1024 without sacrificing performance.

Footnote 16: For an example, see https://github.com/facebookresearch/dino/issues/43#issuecomment-881453515.

HyperparametersBase hyperparameters are presented in Table 16.

ResultsIn Figures 27 and 28 we show the results obtained training DINO on CIFAR-10 with \(\rho_{B}=0.996\) and \(\rho_{B}=0.992\) respectively at the reference batch size of 1024. We employ smooth Progressive Scaling (Definition 3.2) between epochs 120 and 180.

At batch size 2048, the training loss matches the reference _only_ when the EMA Scaling Rule is applied, whereas the run _without_ the scaling rule diverges from the reference. The impact of this divergence is emphasized as we consider the larger batch size of 4096. Here. there is also a gap _with_ the EMA Scaling Rule, however is approximately three times smaller than the gap _without_ the EMA Scaling Rule.

\begin{table}
\begin{tabular}{l c} \hline \hline  & DINO ViT-B/16 \\ \hline CIFAR10 Linear Probe Top-1 (\(\rho_{B}=0.996\)) & 85.38\% \\ CIFAR10 Linear Probe Top-1 (\(\rho_{B}=0.992\)) & 86.96\% \\ \hline Weight initialization & trunc\_normal(.02) \\ Normalization & Layer Norm \\ Learning rate schedule & Single Cycle Cosine \\ Learning rate warmup (epochs) & 50 \\ Learning rate minimum value & \(1\times 10^{-6}\) \\ Training duration (epochs) & 280 \\ Optimizer & AdamW \\ Optimizer scaling rule & Adam \\ Base (\(\beta_{1}\), \(\beta_{2}\)) & (0.9, 0.95) \\ Base learning rate & \(3\times 10^{-4}\) \\ Base batch size (\(B\)) & 1024 \\ Base teacher momentum (\(\rho_{B}\)) & 0.992 or 0.996 \\ Base weight decay & 0.04 \\ Weight decay scaling rule & Linear \\ Weight decay skip bias & Yes \\ Center Momentum & 0.9 \\ Center Momentum Scaling Rule & None \\ Precision & bf16 \\ Augmentation stack & DINO multi-crop (Caron et al., 2020) \\ \hline \hline \end{tabular}
\end{table}
Table 16: DINO ViT-B/16 Training hyperparameters.

Additionally, we observe that using \(\rho_{B}=0.992\) yields higher Top-1 accuracy over \(\rho_{B}=0.996\), and in our experiments, using the EMA Scaling Rule _always_ performs better in terms of linear probe performance than not using the scaling rule.

Figure 28: _DINO ViT-B/16 on CIFAR-10_ for different scalings \(\kappa\) and base teacher momentum \(\rho_{B}=0.992\). The baseline model (\(\kappa=1\), black dashed) uses batch size 1024 and center momentum \(\rho_{C}=0.9\), and is scaled up from batch size 2048 (left) to 4096 (right) with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule. Between epochs 100 and 180 we scale the batch size using progressive scaling (Definition 3.2).

Figure 27: _DINO ViT-B/16 on CIFAR-10_ for different scalings \(\kappa\) and base teacher momentum \(\rho_{B}=0.996\). The baseline model (\(\kappa=1\), black dashed) uses batch size 1024 and center momentum \(\rho_{C}=0.9\), and is scaled up from batch size 2048 (left) to 4096 (right) with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule. Between epochs 100 and 180 we scale the batch size using progressive scaling (Definition 3.2).

In Figure 29 we show how the hyperparameters \(\rho\), \(B\) and learning rate change with the progressive scaling in Definition 3.2.

We also attempted to use a sharp batch size transition (Figures 30 and 31), which leads to the collapse pheonomena observed in prior work. This collapse happens with and without the EMA Scaling Rule. We suspect this is due to dynamics specific to DINO's early phase that are even more challenging to replicate under discretization than those of BYOL.

Figure 30: _DINO ViT-B/16 on CIFAR-10_ for different scalings \(\kappa\) and base teacher momentum \(\rho_{B}=0.992\). The baseline model (\(\kappa=1\), black dashed) uses batch size 1024 and center momentum \(\rho_{c}=0.9\), and is scaled up from batch size 2048 (left) to 4096 (right) with (blue, \(\rho=\rho_{B}^{\kappa}\)) and without (red, \(\rho=\rho_{B}\)) the EMA Scaling Rule. Progressive Scaling is employed with a sharp transition at epoch 100, leading to a collapse phenomenon.

Figure 29: _DINO ViT-B/16 on CIFAR-10_ for different scalings \(\kappa\) and base teacher momentum \(\rho_{B}=0.992\). We show how the hyperparameters \(\rho\), \(B\) and learning rate change with the Progressive Scaling in Definition 3.2. These hyperparameters correspond to the training runs in Figure 28. Those for Figure 27 are identical, with the exception of \(\rho\) that starts at 0.996 instead of 0.992.

### Compute

The compute usage for the DINO investigations is detailed in Table 17.

Our results in this section show it is possible to scale DINO to large batch sizes _without_ sacrificing performance by using _both_ the EMA Scaling Rule and Progressive Scaling, providing the batch size schedule of Progressive Scaling is not sudden.

## Appendix I Additional details on numerical stability

A general analysis of overflow and underflow of the EMA Update (Definition 1.1) or EMA Scaling Rule (Definition 1.2) for different momenta \(\rho\), particularly for IEE-754 floating point values, is beyond the scope of this work due to non-linearity from mechanisms like gradual underflow (IEEE, 2019).

In our setting, do not suffer from practical overflow or underflow issues through exponentiation when applying the EMA Scaling Rule, as FP32 precision allows a maximum \(\rho=1-\epsilon\), or minimum \(\rho=\epsilon\) with \(\epsilon\approx 1.2\times 10^{-7}\). Take self-supervised image representation learning (Section 3.4) as a baseline, with \(\kappa=1\) corresponding to batch size \(B=4096\) with momentum \(\rho_{B}=0.996\). The maximum value of \(\rho\) corresponds to scaling \(\kappa=\log(\rho_{B})/\log(\epsilon)\approx 1/(32K)\), give a batch size less than one, while the minimum value of \(\rho\) corresponds to scaling \(\kappa=\log(\rho_{B})/\log(1-\epsilon)\approx 4K\), giving a batch size \(B\approx 8M\) which is beyond current hardware feasibility, and beyond the breakdown of known optimizer scaling rules (Li et al., 2021).

To examine how momentum may induce numerical errors in practice during training, we train a linear regression model with a Polyak-Ruppert average Definition 3.1, and and track the difference between FP32 model weights and weights in i) BF16; ii) FP16; and iii) a second FP32 run, which act as a proxy for overflow and underflow. In Figure 32 we plot these differences using

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Batch Size & GPUs & Time (h) & Compute/Run (GPUh) & Runs & Compute (GPUh) \\ \hline \(1,024\) & 24 & 6.8 & 163.5 & 2 & 327.0 \\ \(1,024\to 2,048\) & 40 & 4.6 & 182.4 & 1 & 182.4 \\ \(1,024\to 3,072\) & 48 & 4.0 & 189.9 & 1 & 189.9 \\ \(1,024\to 4,096\) & 64 & 3.3 & 212.3 & 1 & 212.3 \\ \(1,024\to 2,048\) (100 Epochs) & 40 & 4.8 & 190.6 & 4 & 762.3 \\ \(1,024\to 3,072\) (100 Epochs) & 48 & 4.0 & 192.5 & 4 & 769.9 \\ \(1,024\to 4,096\) (100 Epochs) & 64 & 3.6 & 232.1 & 4 & 928.2 \\ \hline \hline \end{tabular} All other compute, e.g. code development, runs with errors, and debugging

\begin{tabular}{c c c c c} \hline \hline
**Total** & & & & **41,611.1** \\ \hline \hline \end{tabular}
\end{table}
Table 17: Compute usage for DINO investigations. Values _include_ node allocation times (typically a small % of corresponding total runtime), giving a practical estimate of reproduction cost. All experiments conducted are using 80Gb A100s.

Figure 31: _DINO ViT-B/16 on CIFAR-10_ with \(\rho_{B}=0.992\) and a sharp transition in batch size at epoch 100. We show how the hyperparameters \(\rho\), \(B\) and learning rate change with sudden scaling. These hyperparameters correspond to the training runs in Figure 30.

the maximum absolute difference between model parameters, where the maximum is taken over individual weights

\[\text{MaxAbsDiff}(\text{dttype})=\max_{i=1}^{P}\left|\theta_{i}^{\text{FP3}2}- \theta_{i}^{\text{dttype}}\right|,\] (82)

where \(P\) is the number of parameters in the model. We observe that when model weights and EMA weights are FP16 (never done in practice), an increasing variance happens for FP16 as the value of the momentum \(\rho\) approaches 0.99999, whereas BF16 and FP32 are stable. We stress that all experiments presented in the paper store weights for target model _and_ EMA in FP32 and use automatic-mixed precision to cast them to BF16 during training, and so do not encounter momentum-induced overflow or underflow.

## Appendix J Contributions

All authors contributed to writing this paper, designing the experiments, discussing results at each stage of the project.

Preliminary workDerivation of the EMA Scaling Rule with learning rate \(\eta=0\), initial synthetic and self-supervised ImageNet1k experiments done by Dan Busbridge.

EMA scaling rules for constant gradientsOriginal proof of Equation 4 and the form of \(\delta(\eta,\rho,\kappa)\) in Equation 54 done by Eeshan Gunesh Dhekane. Final proof presented in Appendix E.1 done by Dan Busbridge, verified by Eeshan Gunesh Dhekane and Pierre Ablin.

EMA approximation theorems with SDEsProofs of validity of EMA Scaling Rule in the SDE limit presented in Section 2.2 and Appendix D done by Pierre Ablin.

Polyak-Ruppert averaging in a simple settingDesign of noisy parabola setting of Section 3.1 and initial experiments done by Russ Webb. Design of \(\rho^{*}\)-optimality search (Equation 10), final experiments and analysis of Section 3.1 and Appendix F.1 done by Dan Busbridge.

Polyak-Ruppert averaging on image classificationResNetv2-50 reproduction (Table 4) and baseline momentum identification done by Jason Ramapuram. Final ImageNet1k experiments and analysis of Section 3.2 and Appendices F.2 and F.3 done by Dan Busbridge.

Automatic speech recognitionExperiments and analysis of automatic speech recognition using Polyak-Ruppert averaging (Section 3.2) and continuous pseudo-labeling (Section 3.3 and Appendix G), as well as design choice of a seed model to start pseudo-labeling (aligning quality of the seed models for different batch size settings before pseudo-labeling process) done by Tatiana Likhomanenko.

Self-supervised image representation learningBYOL ResNet-18 recipe (Table 9) and experiments on CIFAR10 using SGD (Figure 5), and BYOL ResNet-50 experiments using LARS (Appendix H.10) done by Dan Busbridge. BYOL ResNet 50 baseline implementation and BYOL ViT

Figure 32: Numerical precision of target and EMA networks compared to an FP32 reference on a regression task for a range of momenta.

recipe (Table 10) done by Jason Ramapuram. BYOL ViT exploratory ablations done by Eeshan Gunesh Dhekane and Jason Ramapuram. All final BYOL ViT experiments and analysis (Figure 6 and Appendices H.4 to H.6) done by Jason Ramapuram. Baseline DINO reproduction done by Dan Busbridge. DINO experiments and analysis (Appendix H.11) done by Xavier Suau Cuadros.

Progressive ScalingProgressive Scaling (Definition 3.2 and Algorithm 1) is proposed by Dan Busbridge based on discussions with Xavier Suau Cuadros, Tatiana Likhomanenko, Jason Ramapuram, Russ Webb, and the authors of Malladi et al. (2022). Adaptation of progressive scaling to semi-supervised learning in automatic speech recognition (Appendix G.2) done by Tatiana Likhomanenko, and to self-supervised learning in vision done by Dan Busbridge and Jason Ramapuram for BYOL (Figures 5 and 6 and Appendices H.4, H.5 and H.10) and Xavier Suau Cuadros for DINO (Appendix H.11).

Limiting behavior of Polyak-Ruppert averagingOriginal proof of limiting behavior of Polyak-Ruppert averaging done by Eeshan Gunesh Dhekane. Final proof presented in Appendix E.2 done by Dan Busbridge, verified by Eeshan Gunesh Dhekane.

Numerical stability analysisPolyak-Ruppert experiment (Figure 32) using linear regression for various floating point precisions done by Jason Ramapuram.

Implementation detailsInvestigations carried out in two distributed, scalable frameworks: Jax for automatic speech recognition experiments, done by Tatiana Likhomanenko; and PyTorch for all remaining investigations, done by Dan Busbridge, Xavier Suau Cuadros, Eeshan Gunesh Dhekane, Jason Ramapuram and Russ Webb. Initial implementation of progressive scaling experiments for incremental-style strategies (e.g. Appendix H.5) showing feasibility done by Jason Ramapuram, and subsequent progressive scaling implementations for smooth strategies (e.g. Appendices H.10 and H.11) done by Dan Busbridge and Xavier Suau Cuadros.