[MISSING_PAGE_FAIL:1]

Introduction

Recent work (Brown et al., 2020) has observed language models (LMs) tend to be increasingly capable of in-context learning as their model size grows. The emergent capability (Wei et al., 2022) allows instructing a large LM at run time using a descriptive natural language (NL) prompt to solve a specified task with out-of-distribution (OOD) robustness (Liu et al., 2022).

Nonetheless, it is not always easy to come up with a descriptive prompt, especially for tasks involving fine-grain specifications that are beyond words. For example, it is hard to elaborate a person's language style using NL to prompt an LM to write in his/her language, unless it is well-known (e.g., _William Shakespeare_ style).

To provide access to delivering more descriptive prompts, we propose eXtensible Prompt (X-Prompt), inspired by Textual Inversion (Gal et al., 2022). Compared with NL prompts, X-Prompt additionally introduces an extensible vocabulary of imaginary words that are learned to represent what NL words hardly describe. For example, an imaginary word2\(\widehat{w}_{u}\) representing a specific person \(u\)'s style can be combined with various prompt contexts to instruct the LM to generate specified content in \(u\)'s language, as shown in Table 1.

Footnote 2: In this paper, we use \(\widehat{w}\) to denote an imaginary word, as opposed to \(w\) denoting a natural language word.

In contrast to soft prompt (Qin and Eisner, 2021) that is for fitting in-distribution (ID) data and thus is likely to fail in OOD prompting scenarios (Yu et al., 2022; Su et al., 2021; Lester et al., 2022), imaginary words in X-Prompt are designed to be OOD robust and well generalized so that they can be used like NL words for various (even unseen) prompt purposes, as illustrated in Table 2.

To ensure the general usability of imaginary words, we propose context-augmented learning (CAL). It guides imaginary words to learning towards their general use against overfitting (in-distribution) training data, playing a key role to derive an X-Prompt that can be both descriptive and OOD robust.

We conduct experiments that use X-Prompts for style customization as a case study. We show X-Prompt has both powerful descriptive capabilities and high OOD robustness, demonstrating a success of combining merits of NL and soft prompts (Li and Liang, 2021; Lester et al., 2021) and presenting a promising extensible interface for advanced interaction between humans and large LMs.

Our contributions can be summarized as follows:

* We propose X-Prompt as a pioneering technology to expand the scope of large language model prompting. It is among the earliest attempts that enhance descriptiveness by using a mixture of natural language and imaginary words, while also maintaining a focus on out-of-distribution (OOD) robustness in the field of Natural Language Processing.
* We show X-Prompt can achieve promising results in generating appropriate content in a specific person's style, demonstrating its effectiveness in the challenging zero-shot language style customization task.

\begin{table}
\begin{tabular}{c|c|c|l} \hline \hline  & \multicolumn{1}{c|}{**Soft prompt**} & \multicolumn{1}{c}{**X-Prompt**} \\ \hline
**Train** & **SOTH**\(\rightarrow\)**(Trump’s text)** & Write in **Final** & style: \(\rightarrow\)**(Trump’s text)** \\ \hline \multirow{3}{*}{**ID Prompt**} & **SOTH**_Make America Great Again!!!!_\(\boldsymbol{\hat{O}}\)_ & Write in **Final** & style: _Great job by @leonmnsk._\(\boldsymbol{\hat{O}}\)_ \\ \cline{2-4}  & **SOTH**_Republicans should have gotten rid_ & Write in **Final** & style: _Fantastic!? Thank you so much_ \\ \cline{2-4}  & _of Section 230 in the Defense Bill_\(\boldsymbol{\hat{O}}\) & _for your support_\(\boldsymbol{\hat{O}}\)\(\boldsymbol{\hat{O}}\) \\ \hline \multirow{3}{*}{**OOD Prompt**} & **SOTH**_Concern about cloning technology: & **Coupon about the cloning technology in **Final** & style: _Bio view fraud information coming out concerning Georgia, Stay tuned:_\(\boldsymbol{\hat{\bullet}}\) & _The United States is a nation of laws. We do not clone people. We do not clone animals._\(\boldsymbol{\hat{O}}\) \\ \cline{2-4}  & **SOTH**_Criticize the C++ language: All the & **Criticize the C++ language in **Final** & style: _C++ is a \\ \cline{2-4}  & _Ralied! Id Demers are trying to do with the Post Office hearings is like the Republicans_ & _very difficult language to learn and understand. It is very complicated and full of bad habits, which will make it much more difficult to fix problems._\(\boldsymbol{\hat{O}}\) \\ \cline{2-4}  & _licans for the FRAUD that will occur_\(\ldots\) & \(\boldsymbol{\hat{\bullet}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: A comparison between soft prompt and X-Prompt in both ID and OOD prompt scenarios during inference (gray text is the generated text by the model given the prefix). In contrast to soft prompt (i.e., the soft token [SOTH]) that works well in ID but performs poorly in OOD prompt scenarios, X-Prompt has significantly better OOD robustness, whose imaginary word (i.e., [Trump]) can be used like an NL word in various contexts for different prompt purposes.

eXtensible Prompt

### Imaginary words

Imaginary words are a supplement to the NL vocabulary to help represent complicated, abstractive or even indescribable concepts (characteristics of a specific person's language). For an X-Prompt \((w_{p_{1}}\dots w_{p_{m}})\), a prompt token \(w_{p_{i}}\) can come from either the NL vocabulary \(V\) or the extensible imaginary word vocabulary \(\widetilde{V}\) (i.e., \(w_{p_{i}}\in V\cup\widetilde{V}\)).

Different from previous work (Li and Liang, 2021; Lester et al., 2021) that learns a soft prompt focusing on fitting ID task data, X-Prompt aims to learn an imaginary word for general usability with high OOD robustness like an NL word, which can be compatible and combined with various contexts for different prompting purposes.

To obtain imaginary words with general usability for OOD robust X-Prompts, we propose context-augmented learning (CAL) to guide imaginary words to learning towards their intended representation against overfitting ID training data.

### Context-augmented learning

As in Figure 1(a), when the imaginary word \(\widetilde{w}_{u}\) is mixed with NL in an X-Prompt, the NL context is intuitively expected to guide the imaginary word \(\widetilde{w}_{u}\) to learning towards a distributed representation for its general use. Formally, given an X-Prompt \((w_{p_{1}},\dots,\widetilde{w}_{u},\dots,w_{p_{m}})\) where \(\widetilde{w}_{u}\) is the imaginary word mixed with other prompt tokens3, \(\widetilde{w}_{u}\) is learned to maximize the following objective:

Footnote 3: In this paper, we mainly discuss X-Prompts with only 1 imaginary word token.

\[\mathcal{F}(\widetilde{w_{u}})=\log P(\bm{x}|w_{p_{1}},\dots,\widetilde{w}_{u },\dots,w_{p_{m}})\] (1)

where \(\bm{x}=(w_{x_{1}},\dots,w_{x_{n}})\) is a text sequence training example. In practice, however, learning the imaginary word \(\widetilde{w_{u}}\) with only one prompt context is risky because \(\widetilde{w}_{u}\) is likely to overfit for this

Figure 1: Learning of imaginary words: **(a)** The imaginary word \(\widetilde{w_{u}}\) is mixed with NL tokens in an X-Prompt to guide its learning. Except \(\widetilde{w_{u}}\) that is allowed to be updated, all other weights are frozen; **(b)** As a method for context-augmented learning, **template augmentation** augments X-Prompt templates through prompt engineering to prevent \(\widetilde{w_{u}}\) overfitting for one prompt template; **(c)** To derive more diverse contexts, **content augmentation** augments \(\widetilde{w_{u}}\)’s prompt contexts with an indicative keyword to relieve its responsibility for memorizing specific content like _GPT-3_ and _NASA_ and improve its general usability (i.e., style representation), benefiting X-Prompt in terms of OOD robustness.

prompt context and thus cannot work well in other prompt contexts, resulting in losing its general usability and degrading into conventional prompt tuning (Lester et al., 2021).

To address the challenge, we propose context-augmented learning (CAL), including 2 specific approaches that are orthogonal and thus can work together to help learning of imaginary words.

#### 2.2.1 Template augmentation

As shown in Figure 1(b), we augment an X-Prompt's prompt context by designing multiple templates through prompt engineering. As a result, an imaginary word \(\widetilde{w}_{u}\) can learn to be compatible with various prompt contexts, which improves its general usability. Formally, given \(T\) X-prompt templates \(\{(w_{p_{1}}^{(t)},\dots,\widetilde{w}_{u},\dots,w_{p_{m_{t}}}^{(t)})|1\leq t \leq T\}\), the objective function is:

\[\mathcal{F}(\widetilde{w_{u}})=\frac{1}{T}\sum_{t=1}^{T}\log P(\bm{x}|w_{p_{1} }^{(t)},\dots,\widetilde{w}_{u},\dots,w_{p_{m_{t}}}^{(t)})\] (2)

#### 2.2.2 Content augmentation

Although template augmentation may alleviate the risk of overfitting, its effect is limited because we can only augment a limited and small number of templates (i.e., \(T\)) by prompt engineering. Also, as these prompts are not indicative enough, an imaginary word \(\widetilde{w}_{u}\) will inevitably learn to memorize specific content for maximizing the objective \(\mathcal{F}\), deviating from its general use. To prevent \(\widetilde{w}_{u}\) being over-responsible for optimizing \(\mathcal{F}\), we propose content augmentation - an advanced CAL method.

Content augmentation augments an X-Prompt by including content information such as an indicative keyword in the prompt to provide hints for the LM about what to generate, as shown in Figure 1(c). Content augmentation can not only relieve the responsibility of \(\widetilde{w}_{u}\) to fit training data but also make the prompt context of \(\widetilde{w}_{u}\) become much more diverse, which benefits \(\widetilde{w}_{u}\) to learn a better distributed representation for its general use.

In this work, we use an indicative keyword for content augmentation. To select an indicative keyword, we only use the frozen LM itself without leveraging any other models or tools, as illustrated in Figure 2: We prompt the frozen LM to extract multiple keyword candidates [\(w_{k}^{1}\), \(\dots\)\(w_{k}^{c}\), \(\dots\), \(w_{k}^{C}\)] for the training example \(\bm{x}\) where \(C\) is the number of extracted keyword candidates; then the keyword candidates are inserted to a prompt template to rank for the most indicative one:

\[w_{k}^{*}=\arg\max_{w_{k}^{c}}\log P(\bm{x}|\bm{r}(w_{k}^{c}))\] (3)

where \(\bm{r}(w_{k}^{c})=(w_{p_{1}}^{(r)},\dots,w_{k}^{c},\dots,w_{p_{m_{r}}}^{(r)})\) is called the ranking prompt template.

Figure 2: Keyword selection for content augmentation: **(a)** An NL prompt (i.e., “_Top keywords of the above text are:_” in this example) following an input sequence for keyword extraction; **(b)** The extracted keywords (i.e., “_GPT-3_” and “_AI_” in this example) are then inserted into the ranking prompt template (i.e., “_Write with keyword —._” in this example) to be conditioned on by the frozen LM for scoring the input sequence as in Eq (3), which ranks for the most indicative keyword (i.e., “_GPT-3_” in this example) for the input sequence.

Experiments

We conduct experiments to evaluate X-Prompts for language style customization. We mainly focus on open-ended text generation (Section 3.1) to evaluate how well X-Prompts can instruct an LM to generate user-specific language. We also test in the style transfer (rewriting) setting (Section 3.2) as supplementary evaluation.

### Open-ended text generation

#### 3.1.1 Data and evaluation setting

We use the publicly available Top 20 most followed users in Twitter social platform dataset4 which contains over 50K tweets from 20 users (20-user dataset), and the Sentiment dataset5 from which we extract top 800 users' (in total 68K) tweets (800-user dataset) to verify the capability of X-Prompt to instruct an LM to generate user-specific language. We show the statistics of the datasets in Table 3. We split the datasets in 90/5/5 by user for training, validation and test. Specially, we discard the test examples that share indicative keywords with training examples from the same user, resulting in 15% test examples discarded to ensure no test prompts are seen during training for OOD evaluation (see Table 5). We use perplexity and accuracy of next word prediction as our quantitative evaluation metrics.

Footnote 4: https://shorturl.at/htDHT

Footnote 5: https://shorturl.at/pvBLX

For qualitative studies, we use X-Prompts with imaginary words representing the following distinct language styles6 to generate text for human evaluation: _Donald Trump_'s and _Satya Nadella_'s latest tweets and transcripts7 of _Sheldon Cooper_ from _Big Bang Theory_. The statistics of data is shown in Table 4. By default, we use top-\(p\) sampling (Holtzman et al., 2019) for text generation.

Footnote 6: We use these three people’s language styles for qualitative studies because they are familiar to most audiences (to better understand our presented examples) and annotators (to better judge generation results).

Footnote 7: https://shorturl.at/aJLM8

#### 3.1.2 Model configuration

We use the OPT-6.7b (Zhang et al., 2022) as the base LM to test our approach. The model has 32 layers and 32 attention heads with an embedding dimension of 4096 and an FFN dimension of 16384.

We use one8 imaginary word (token) in an X-Prompt to represent a specific user's language style. As illustrated in Figure 1(a), we keep all the OPT's original weights frozen and we only update the embedding of imaginary words. The prompt contexts we use for learning and evaluating X-Prompt are presented in Table 5.

Footnote 8: The study of using more imaginary tokens is presented in Appendix B.2

We use Adam optimizer (Kingma and Ba, 2014) with the max learning rate of 2e-4 with a warmup for the first 10% training steps followed by a linear decay. We run up to 6000 updates with a global batch size of 8192 tokens on 8 Nvidia V100 GPUs using DeepSpeed ZeRO-2 (Rajbhandari et al., 2020).

\begin{table}
\begin{tabular}{c|c|c} \hline \hline
**Style** & **Genre** & **Size** \\ \hline Satya & tweets & 800 \\ Trump & tweets & 3000 \\ Sheldon & transcripts & 7000 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Statistics of data for Satya, Trump and Sheldon’s styles

\begin{table}
\begin{tabular}{c|c c|c c c c} \hline \hline
**Dataset** & \#**tweets** & \#**users** & \multicolumn{4}{c}{**\#tweets per user**} \\  & & & **max** & **min** & **avg** & **std** \\ \hline
**20-user** & 52541 & 20 & 3146 & 1841 & 2626 & 396 \\
**800-user** & 68123 & 800 & 548 & 52 & 84 & 41 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Statistics of the 20-user and 800-user dataset

#### 3.1.3 Quantitative evaluation

Table 6 shows quantitative evaluations results in both ID and OOD settings. For ID evaluation where test prompts are all seen (see Table 5) during training, X-Prompt outperforms "No prompt" and few-shot learning baselines significantly in both perplexity and accuracy, demonstrating its superior descriptive capabilities; while it slightly underperforms prompt tuning and its ablated counterpart (i.e., _w/o_ CAL) because they focus on fitting ID data.

When it comes to OOD evaluation where test prompts are unseen during training, X-Prompt shows its significant advantage over prompt tuning, indicating its excellent OOD robustness. In contrast, its ablated version (X-Prompt _w/o_ CAL) substantially loses OOD robustness and almost degrades into prompt tuning, demonstrating the importance of CAL to X-Prompt.

#### 3.1.4 Qualitative evaluation

For qualitative evaluation, we brainstorm (Ouyang et al., 2022) 100 prompts9 (like examples in Table 1 and Table 2) that are unseen during training and let the model generate in Satya, Trump and Sheldon's styles respectively. As it is difficult to ground open-ended generations, we have two annotators manually evaluate10 generation results in three dimensions: _Content_, _Style_ and _Overall_. According to Table 7, NL prompts achieve a high content score, while prompt tuning and X-Prompts

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c|}{**800 Users (ID)**} & \multicolumn{2}{c|}{**20 Users (ID)**} & \multicolumn{2}{c}{**20 Users (OOD)**} \\  & **PPL\(\downarrow\)** & **Accuracy\(\uparrow\)** & **PPL \(\downarrow\)** & **Accuracy \(\uparrow\)** & **PPL \(\downarrow\)** & **Accuracy \(\uparrow\)** \\ \hline No prompt & 73.2 & 27.1 & 38.9 & 34.8 & 37.7 & 35.2 \\ \hline
8-shot & 69.9 & 27.2 & 36.0 & 35.0 & - & - \\
16-shot & 68.9 & 27.5 & 35.5 & 35.3 & - & - \\
32-shot & 62.7 & 28.5 & 34.0 & 36.4 & - & - \\ \hline Prompt tuning & 56.0 & 29.5 & 29.9 & **37.8** & 29.5 & 38.0 \\ \hline X-Prompt & 56.2 & 29.3 & 30.8 & 37.2 & **28.5** & **38.6** \\ X-Prompt (_w/o_ CAL) & **55.7** & **29.9** & **29.7** & 37.7 & 29.4 & 37.9 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Quantitative evaluation results in 800-user and 20-user datasets. **No prompt** denotes the original OPT-6.7b baseline without any prompt and \(k\)**-shot** denotes a baseline which prepends \(k\) examples from a user’s training set as a prompt for customizing this user’s style.

\begin{table}
\begin{tabular}{c|c|c} \hline \hline
**Prompt ID** & **Used for** & **Prompts (w/ keyword) for quantitative evaluation** \\ \hline
1 & train & The style of \(\boxplus\) is clear in the following text (with keyword [KEYWORD]) \\ \hline
2 & train & The style of \(\boxplus\) can be identified in the following text (mentioning keyword [KEYWORD]) \\ \hline
3 & train & An example text (with keyword [KEYWORD]) in the style of \(\boxplus\) is presented below \\ \hline
4 & OOD dev & We can easily identify the style of \(\boxplus\) in the following text with keyword [KEYWORD] \\ \hline
5 & OOD test & The following text (about [KEYWORD]) is in the style of \(\boxplus\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Prompts for training and evaluation. We use the first 3 prompts for training and ID evaluation. In ID evaluation, prompt texts do not include [KEYWORD] so that test prompts are all seen during training. To automatically harvest unseen test prompts for OOD evaluation, we employ the idea of CAL: we obtain Prompt 4 and 5 as our dev and test prompt respectively with template augmentation; in addition, we prompt the LM to generate content about unseen keywords with content augmentation to test its OOD robustness, resulting in a variety of unseen test prompts for OOD evaluation. For the ablated X-Prompt (X-Prompt w/o CAL), it is trained only with Prompt 1 without keywords.

learned w/o CAL achieve good style scores but they all perform poorly in other dimensions. In contrast, X-Prompt achieves significantly better overall quality, while it does not perform best in content and style sub-dimensions.

In addition to famous people's styles (e.g., Satya and Trump), we also test X-Prompt on the styles of individuals unknown to the OPT. This is to verify that X-Prompt is not limited to styles already known by the LLM; instead, it can be applied to any style customization. We use a senior Chinese media professional - Hu Xijin11 whose writing style is distinctive and has always been popular among Chinese netizens for imitation - as an example. We include the detail of this evaluation in Appendix C.3.

Footnote 11: https://en.wikipedia.org/wiki/Hu_Xijin

By looking into the results, we present Table 8 to show concrete examples to compare NL prompts to X-Prompts learned with and without CAL. We observe NL prompts are good at generating appropriate content but have no way to control the style; while X-Prompts _w/o_ CAL do well in generating _Donald Trump_'s language but fail to follow the prompt to generate specified content, degrading into soft prompts (as in Table 2) that only focus on fitting ID data and losing OOD capabilities with unseen prompt contexts. Consistent with results in Table 6 and 7, X-Prompts are robust and can prompt the LM to generate desirable content in the appropriate style, accounting for its best overall quality in zero-shot style customization.

Table 9 shows the effect of the model size on the performance of X-Prompt. While we observe a clear difference between prompt tuning and X-Prompt in the OPT-6.7b model, as we reduce the model size, the difference become less significant, showing a trend that X-Prompt gradually loses its OOD robustness and degrades into prompt tuning. This indicates that X-Prompt may not work with a small LM but performs well in large LMs as in-context learning.

Finally, we present more examples in Table 4 in Appendix D to demonstrate both descriptive capabilities and OOD robustness of X-Prompts.

\begin{table}
\begin{tabular}{l|c|c|c} \hline \hline
**Prompt (shortened)** & **Prompt Type** & **Generation** \\ \hline \multirow{2}{*}{\begin{tabular}{l} Criticize the C++ \\ language: \\ \end{tabular} } & \multirow{2}{*}{NL} & _The C++ language is a mess. It is a mess because it is a language that is designed to be used by people who are not programmers..._ \\ \hline \multirow{4}{*}{\begin{tabular}{l} Criticize the C++ language \\ in \end{tabular} } & \multirow{2}{*}{X-Prompt} & _The Radical Left Democrats, working closely with their_ _beloved partner, the Fake News Media, is using every trick_ _in the book to SILENCE a majority of our Country_ \\ \cline{2-3}  & & _C++ is a very difficult language to learn and understand. It is very complicated and full of bad habits, which will make it much more difficult to fix problems..._ \\ \hline \hline \multirow{2}{*}{\begin{tabular}{l} Concer about cloning \\ technology: \\ \end{tabular} } & \multirow{2}{*}{NL} & _The cloning of human beings is a serious moral issue. It is a matter of grave concern that the cloning of human beings is being..._ \\ \hline \multirow{4}{*}{
\begin{tabular}{l} Concer about cloning \\ technology in \end{tabular} } & \multirow{2}{*}{X-Prompt} & _I'm not a fan of the term ’wooke’. I don’t know what it means._ \\ \cline{2-3}  & & _I don’t like it..._ \\ \cline{1-1} \cline{2-3}  & & _X-Prompt_ & _The United States is a nation of laws. We do not clone_ _people. We do not clone animals._ \\ \hline \hline \end{tabular}
\end{table}
Table 8: A comparison between X-Prompts learned with and without CAL in OOD evaluation 

### Style transfer

In addition to open-ended generation, we also evaluate in the style transfer (rewriting) where model outputs can be grounded in human references for straightforward quantitative evaluation (i.e., end-to-end generation evaluation instead of evaluating perplexity or next word prediction accuracy).

Among various style transfer datasets, we use the Entertainment (EM) subset of GYAFC (informal \(\rightarrow\) formal) (Rao and Tetreault, 2018) and PoliteRewrite (impolite \(\rightarrow\) polite) (Wang et al., 2022) as our evaluation datasets (see Table 10) because they have high-quality annotation with well-defined language style. Following previous work (Rao and Tetreault, 2018; Xu et al., 2019; Zhang et al., 2020; Li et al., 2022), we use BLEU to evaluate generation results' lexical similarity with references, accuracy12 to evaluate style appropriateness and use harmonic (H-) and geometric (G-) mean as overall performance. Table 11 shows how we perform zero-shot style transfer with NL and X-Prompts. Model configuration is the same as Section 3.1.2.

Footnote 12: Following previous work, we fine-tune a BERT-base (Devlin et al., 2019) model with the training sets, which annotate a text with its style label, as a style classifier to test accuracy.

Table 12 shows the comparison of results of using NL, prompt tuning and X-Prompt to prompt the LM for zero-shot style transfer. The NL baseline has the best BLEU score but low accuracy because we observe that it rarely really rewrites a sentence: in most cases, it just copies the text without any revision. Its undesirable performance also indicates that the OPT-6.7b model itself is not so good at zero-shot style transfer. For the prompt tuning baseline, its style accuracy looks good but has a low BLEU score, because it fails to follow rewriting instructions that are unseen during training.

\begin{table}
\begin{tabular}{c|c c c|c c c} \hline \hline
**Dataset** & \multicolumn{3}{c|}{**GYAFC (EM)**} & \multicolumn{3}{c}{**PoliteRewrite**} \\
**Split** & **Train** & **Dev** & **Test** & **Train** & **Dev** & **Test** \\ \hline
**\#sentence** & 53K & 3K & 1K & 10K & 3K & 2K \\ \hline \hline \end{tabular}
\end{table}
Table 10: Statistics of the datasets for style transfer

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline
**Prompt methods** & **Phase** & \multicolumn{2}{c}{**Prompt + Target (Training) /** Expected Generation (Inference)} \\ \hline \multirow{3}{*}{NL} & Train & \multicolumn{3}{c}{N/A} \\ \cline{3-5}  & Inference & \multicolumn{3}{c}{[IMPOLITETEXT]’ Above text can be rewritten to improve its politeness as follows:} \\ \cline{3-5}  & & [POLITETEXT] & \\ \hline \multirow{3}{*}{Prompt tuning} & Train & \multicolumn{3}{c}{[SOFT] [POLITETEXT]} \\ \cline{3-5}  & Inference & \multicolumn{3}{c}{[IMPOLITETEXT]’ Above text can be rewritten into the style of [SOFT] [POLITETEXT]} \\ \hline \multirow{3}{*}{X-Prompt} & Train & \multicolumn{3}{c}{The style of \(\overline{\boxplus}\) can be identified in the following text with keyword [KEYWORD]:} \\ \cline{3-5}  & Inference & \multicolumn{3}{c}{[IMPOLITETEXT]’ Above text can be rewritten into the style of \(\overline{\boxplus}\) [POLITETEXT]} \\ \hline \hline \end{tabular}
\end{table}
Table 11: NL, prompt tuning and X-Prompt for zero-shot _impolite_\(\rightarrow\)_polite_ style transfer. **[SOFT]** and \(\overline{\boxplus}\) are learnable and denote the soft token and the imaginary word in prompt tuning and X-Prompt respectively. For zero-shot style transfer, we use beam search (\(b=5\)) as the default decoding method.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline
**Model** & **Method** & **Content\(\uparrow\)** & **Style\(\uparrow\)** & **Overall\(\uparrow\)** \\ \hline
350m & Prompt tuning & 0.22 & 0.81 & 0.15 \\  & X-Prompt & 0.30 (+0.08) & 0.79 (-0.02) & 0.24 (+0.09) \\ \hline
1.3b & Prompt tuning & 0.27 & 0.87 & 0.24 \\  & X-Prompt & 0.45 (+0.18) & 0.82 (-0.05) & 0.37 (+0.13) \\ \hline
6.7b & Prompt tuning & 0.34 & 0.92 & 0.30 \\  & X-Prompt & **0.69 (+0.35)** & 0.83 (-0.09) & **0.54 (+0.24)** \\ \hline \hline \end{tabular}
\end{table}
Table 9: X-Prompt tends to perform better and show more significant OOD robustness advantage over prompt tuning in larger foundation LMs for zero-shot style customization.

In contrast, X-Prompt achieves a good balance of BLEU and accuracy and preferred by human evaluations (Table 13) with better overall quality in the zero-shot style transfer. Its good performance with style rewriting prompts that are never seen during training further strengthens the evidence of its OOD robustness.

## 4 Related work

Since GPT (Brown et al., 2020) reveals that large pre-trained language models are good at zero-shot learning, much innovative research work has been proposed in recent years, ranging from prompt template design (i.e, engineering) (Schick and Schutze, 2020) to prompt mining (Jiang et al., 2019), generating (Gao et al., 2021; Ben-David et al., 2021) and scoring (Davison et al., 2019), finding that prompting the LLM with natural language can solve many downstream tasks as long as the prompt is well clear and rewritten for the model (Gonen et al., 2022).

As natural language prompts' descriptive capability is limited, there is another branch of research studying continuous prompts (Li and Liang, 2021; Lester et al., 2021; Liu et al., 2021; Han et al., 2022; Hu et al., 2021) for fitting downstream tasks. However, these approaches are mainly for fitting ID task data with little consideration of OOD robustness, which means that their learned continuous prompts can hardly be used for OOD tasks or data.

Recently, Gal et al. (2022) proposed Textual Inversion in the multimodal context, which learns a virtual token to represent an object from an image and reveals that the learned virtual token can be used in unseen prompts for creative image generation (Kumari et al., 2022). X-Prompt is inspired by Gal et al. (2022), trying to learn OOD robust imaginary words to represent what natural language hardly describes to further expand zero-shot learning capabilities for the LLM, although we find it much more challenging to achieve this in NLP than text2image generation, which motivates us to propose context-augmented learning (CAL). To the best of our knowledge, our work is one of the earliest explorations in this direction in the NLP community.

## 5 Conclusion and Future Work

We propose X-Prompt, an extensible interface for prompting a large language model beyond natural language. X-Prompt can expand in-context learning capabilities to handle more complex instructions for language model customization and may open up many exciting opportunities, such as creative language generation, patching language models with new knowledge of entities (Zaporojets et al., 2022) and events (Ge et al., 2018), and detoxifying and debiasing in language generation (Welbl et al., 2021), far beyond style customization as demonstrated in this work, approaching advanced interaction between humans and large language models.

\begin{table}
\begin{tabular}{c|c|c|c} \hline \hline
**Method** & **Content** & **Style** & **Overall** \\ \hline Prompt tuning & 0.27 & 0.82 & 0.23 \\ X-Prompt & 0.64 & 0.80 & 0.60 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Human evaluation of zero-shot style transfer

\begin{table}
\begin{tabular}{c|c c c c|c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c|}{GYAFC} & \multicolumn{4}{c}{PoliteRewrite} \\  & BLEU & Style & H-mean & G-mean & BLEU & Style & H-mean & G-mean \\ \hline No Edit & 50.2 & 4.3 & 7.9 & 14.7 & **24.7** & 3.0 & 5.4 & 8.6 \\ \hline NL & **50.8** & 20.0 & 28.7 & 31.9 & 24.6 & 6.7 & 10.5 & 12.8 \\ \hline Prompt tuning & 16.2 & **76.7** & 26.8 & 35.2 & 15.4 & **80.8** & 25.9 & 35.3 \\ \hline X-Prompt & 38.7 & 71.9 & **50.3** & **52.7** & 18.9 & 79.5 & **30.5** & **38.8** \\ \hline \hline \end{tabular}
\end{table}
Table 12: Results of zero-shot style transfer (i.e., rewriting)For future work, we plan to investigate how X-Prompt can facilitate more complex decoding and prompting methods (Wei et al., 2022; Yao et al., 2022; Wang et al., 2023) to minimize the interaction effort between humans and large language models.

## Limitations

Due to computing resource limitations, we only conduct experiments on pretrained language models that contain up to 6.7 billion parameters. Although we speculate that our approach should become more effective as the language model size increases, as suggested by Table 9, we cannot be completely certain if this trend can be safely extrapolated.

Furthermore, X-Prompt still requires back-propagation through the entire language model, even though we only update the imaginary word's embedding. This somewhat limits its application scenarios, preventing X-Prompts from being used as easily as natural language prompts. However, our subsequent work (Ge et al., 2023) has addressed this issue by forwarding an encoder for context compression. We anticipate that this series of improvements will better enhance a deployed language model's capabilities in practice from an in-context perspective, with minimal additional effort.

## Acknowledgments

We would like to express our gratitude to the reviewers for their valuable comments and suggestions, which have significantly improved this work. The corresponding author for this paper is Tao Ge.

## References

* Ben-David et al. (2021) Eyal Ben-David, Nadav Oved, and Roi Reichart. 2021. Pada: Example-based prompt learning for on-the-fly adaptation to unseen domains. _Transactions of the Association for Computational Linguistics_, 10:414-433.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901.
* Davison et al. (2019) Joe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In _Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)_, pages 1173-1178, Hong Kong, China. Association for Computational Linguistics.
* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In _Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)_, pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.
* Gal et al. (2022) Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-or. 2022. An image is worth one word: Personalizing text-to-image generation using textual inversion. In _The Eleventh International Conference on Learning Representations_.
* Gao et al. (2021) Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In _Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_, pages 3816-3830, Online. Association for Computational Linguistics.
* Ge et al. (2018) Tao Ge, Lei Cui, Baobao Chang, Zhifang Sui, Furu Wei, and Ming Zhou. 2018. Eventwiki: a knowledge base of major events. In _Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)_.
* Ge et al. (2023) Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, and Furu Wei. 2023. In-context autoencoder for context compression in a large language model. _arXiv preprint arXiv:2307.06945_.
* Ge et al. (2021)Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and Luke Zettlemoyer. 2022. Demystifying prompts in language models via perplexity estimation. _arXiv preprint arXiv:2212.04037_.
* Han et al. (2022) Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2022. Ptr: Prompt tuning with rules for text classification. _AI Open_, 3:182-192.
* Holtzman et al. (2019) Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. _arXiv preprint arXiv:1904.09751_.
* Hu et al. (2021) Shengding Hu, Ning Ding, Huadong Wang, Zhiyuan Liu, Juanzi Li, and Maosong Sun. 2021. Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification. _arXiv preprint arXiv:2108.02035_.
* Jiang et al. (2019) Zhengbao Jiang, Frank F. Xu, J. Araki, and Graham Neubig. 2019. How can we know what language models know? _Transactions of the Association for Computational Linguistics_, 8:423-438.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_.
* Kumari et al. (2022) Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2022. Multi-concept customization of text-to-image diffusion. _arXiv preprint arXiv:2212.04488_.
* Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. _arXiv preprint arXiv:2104.08691_.
* Lester et al. (2022) Brian Lester, Joshua Yurtsever, Siamak Shakeri, and Noah Constant. 2022. Reducing retraining by recycling parameter-efficient prompts. _arXiv preprint arXiv:2208.05577_.
* Li et al. (2022) Jingjing Li, Zichao Li, Tao Ge, Irwin King, and Michael R Lyu. 2022. Text revision by on-the-fly representation optimization. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 10956-10964.
* Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. _arXiv preprint arXiv:2101.00190_.
* Liu et al. (2022) Nelson F Liu, Ananya Kumar, Percy Liang, and Robin Jia. 2022. Are sample-efficient nlp models more robust? _arXiv preprint arXiv:2210.06456_.
* Liu et al. (2021) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. _arXiv preprint arXiv:2103.10385_.
* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744.
* Qin and Eisner (2021) Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with mixtures of soft prompts. In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 5203-5212.
* Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In _SC20: International Conference for High Performance Computing, Networking, Storage and Analysis_, pages 1-16. IEEE.
* Rao and Tetreault (2018) Sudha Rao and Joel Tetreault. 2018. Dear sir or madam, may i introduce the gyafc dataset: Corpus, benchmarks and metrics for formality style transfer. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)_, pages 129-140.
* Schick and Schutze (2020) Timo Schick and Hinrich Schutze. 2020. Exploiting cloze questions for few shot text classification and natural language inference. _arXiv preprint arXiv:2001.07676_.
* Schutze et al. (2021)Yusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan, Yankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan Liu, Peng Li, Juanzi Li, et al. 2021. On transferability of prompt tuning for natural language processing. _arXiv preprint arXiv:2111.06719_.
* Vu et al. (2022) Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. 2022. Spot: Better frozen model adaptation through soft prompt transfer. In _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 5039-5059.
* Wang et al. (2022) Xun Wang, Tao Ge, Allen Mao, Yuki Li, Furu Wei, and Si-Qing Chen. 2022. Pay attention to your tone: Introducing a new dataset for polite language rewrite. _arXiv preprint arXiv:2212.10190_.
* Wang et al. (2023) Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2023. Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. _arXiv preprint arXiv:2307.05300_.
* Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_.
* Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837.
* Welbl et al. (2021) Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in detoxifying language models. _arXiv preprint arXiv:2109.07445_.
* Xu et al. (2019) Ruochen Xu, Tao Ge, and Furu Wei. 2019. Formality style transfer with hybrid textual annotations. _arXiv preprint arXiv:1903.06353_.
* Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. _arXiv preprint arXiv:2210.03629_.
* Zaporojets et al. (2022) Klim Zaporojets, Lucie-Aimee Kaffee, Johannes Deleu, Thomas Demeester, Chris Develder, and Isabelle Augenstein. 2022. Tempel: Linking dynamically evolving and newly emerging entities. _Advances in Neural Information Processing Systems_, 35:1850-1866.
* Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_.
* Zhang et al. (2020) Yi Zhang, Tao Ge, and Xu Sun. 2020. Parallel data augmentation for formality style transfer. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 3221-3228.

## Appendix A Shortened prompt text

We shorten prompt texts in Table 1, Table 2 and Table 8 of the main submission for saving space. Here, we present the corresponding actual prompt texts of shortened prompt texts in Table 14.

## Appendix B Supplementary details of experiments

### Baseline prompting methods

We supplement details of baseline prompting methods' prompt texts for evaluation in Table 15.

### Length of imaginary words

In this paper, we mainly test imaginary words with a length of 1 (i.e., 1 token) because we observe that for zero-shot style transfer, increasing the number of imaginary tokens does not substantially increase OOD performance, while it can help enhance the fitting ability and increase ID performance (which is not our focus), as shown in Figure 3.

\begin{table}
\begin{tabular}{c|c|c} \hline \hline
**Prompting methods** & **w/o keyword** & **w/ keyword** \\ \hline NL & [NO PROMPT] & The keyword of the following text is [KEYWORD] \\ \hline Prompt tuning & [SOFT] & [SOFT] The keyword of the following text is [KEYWORD] \\ \hline \hline \end{tabular}
\end{table}
Table 15: Prompt texts of baseline methods for open-ended generation (Section 3.1)

Figure 3: The effects of the length of imaginary tokens on the result.

### Output length control

It is notable that the OPT model tends to generate text without stopping at the end of one sentence and may generate multi-line texts.

In our experiments, we only evaluate the text in the first line of output (for human evaluation and style transfer). Specially, for style transfer, we will further truncate the output text if its length exceeds 150% of the input text length.

## Appendix C Qualitative evaluation

### Human evaluation

We recruit 4 graduate student volunteers who are both proficient in English to judge each open-ended text generation result in three dimensions:

* Content: To judge if the generated text is relevant according to the prompt. The annotators are asked to rate with a three-way score (0 denotes irrelevant; 1 denotes relevant; 0.5 denotes somewhat relevant).
* Style: To judge if the generated text is in the specific user's language style. The annotators are asked to rate a three-way score (0 denotes the clearly wrong style; 1 denotes the appropriate style; 0.5 denotes that the annotator is not sure about the style appropriateness).
* at least both content relevant and in the appropriate style. The annotators are asked to rate a three-way score (0 denotes low-quality, either irrelevant or in a wrong style; 1 denotes good-quality, much resembling words from the specific person according to the prompt; 0.5 denotes the quality between low and high quality, it is for the case where the text does not looks natural or the text style is not so distinct).

We first pool all models/methods' outputs and anonymize their model/method source. Then we split the data into 2 groups and assign 2 annotators for each group.

We measure the inter-annotator agreement in Cohen's \(\kappa\) and show the results in Table 16.

### GPT-4 evaluation

In our follow-up experiments (conducted after the full paper submission date), we use the GPT-4 to help evaluate the content faithfulness with the prompt:

Please evaluate whether the following text follows the instruction "{prompt}":

{text}

If it follows the instruction, please rate 1; otherwise, rate 0

The scores by the GPT-4 are presented in Table 17 and they are highly consistent with the human evaluation results (Pearson correlation score \(r=0.73\)), indicating that the content faithfulness evaluation is reliable.

\begin{table}
\begin{tabular}{c|c|c|c} \hline  & Content & Style & Overall \\ \hline Cohen’s \(\kappa\) & 0.83 & 0.65 & 0.71 \\ \hline \end{tabular}
\end{table}
Table 16: Inter-annotator agreement in our human evaluation.

[MISSING_PAGE_FAIL:15]

[MISSING_PAGE_FAIL:16]