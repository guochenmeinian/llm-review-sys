# Fairness under Noise Perturbation: from the Perspective of Distribution Shift

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Much work on fairness assumes access to clean data during training. In practice, however, due to privacy or legal concern, the collected data can be inaccurate or intentionally perturbed by agents. Under such scenarios, fairness measures on noisy data become a biased estimation of ground-truth discrimination, leading to unfairness for a seemingly fair model during deployment. Current work on noise-tolerant fairness assumes a group-wise universal flip, which can become trivial during training, and requires extra tools for noise rate estimation. In light of existing limitations, in this work, we consider such problem from a novel perspective of distribution shift, where we consider a normalizing flow framework for noise-tolerant fairness without requiring noise rate estimation, which is applicable to both _sensitive attribute noise_ and _label noise_. We formulate the noise perturbation as both group- and label-dependent, and we discuss theoretically the connections between fairness measures under noisy and clean data. We prove theoretically the transferability of fairness from noisy to clean data under both types of noise. Experimental results on three datasets show that our method outperforms state-of-the-art alternatives, with better or comparable improvements in group fairness and with relatively small decrease in accuracy under single exposure and the simultaneous presence of two types of noise.

## 1 Introduction

As machine learning systems are increasingly used in high-stake social areas, there have been arising concerns that automatic decision-making systems, if not properly regulated or intervened, would perpetuate or amplify existing biases and discrimination in society (Angwin et al., 2016; Dressel and Farid, 2018; De-Arteaga et al., 2022; Ricci Lara et al., 2022). It has been shown that merely removing sensitive information during training is not sufficient to ensure fairness, as there may be correlation or causality between sensitive attributes and other features used in the training process, which could result in discriminatory outcomes (Jackson, 2018; Mehrabi et al., 2021). In response, different metrics and methods on fairness (Hardt et al., 2016; Zafar et al., 2017; Choi et al., 2020; Diana et al., 2022) have been proposed to quantify discrimination and to achieve parity for machine learning models.

Current literature on fairness generally assumes access to full and clean sensitive information when imposing fairness intervention. In practice, however, due to privacy or legal concern, it is sometimes infeasible to collect or use such information, greatly hindering the application of conventional methods on fairness (Lahoti et al., 2020; Chai et al., 2022); moreover, the collected sensitive information can be subject to noisy perturbation, leading to inaccurate estimation of unfairness (Fioretto et al., 2022). Despite recent works on proxy sensitive attribute (Yan et al., 2020; Gari et al., 2021), it has been shown that noisy protected information alone, without extra regulation, is not a sufficientsubstitution for ground-truth sensitive information (Lamy et al., 2019). Therefore, it is crucial to study the problem of fairness under noisy sensitive information.

Much of current work on fairness under noisy sensitive information requires access to noise rate or external tools for noise rate estimation and uses group-dependent noise rate to rectify measures of unfairness during training (Wang et al., 2020; Celis et al., 2021; Mehrotra and Celis, 2021). However, the estimation process can be costly and inaccurate up to varied estimation methods, and such formulations may not work well under varying noise rates between training and testing data. Besides, much of current formulation regarding noisy sensitive information assumes uniform flip within different groups, which in return, could lead to trivial modifications of fairness constraints during training, especially in terms of complex neural networks. Instead, we seek to find alternative ways to quantify disparities and to improve fairness under noisy sensitive information, without using extra tools for noise evaluation.

We draw inspirations from fairness under distribution shift, where the goal is to ensure the transferability of fairness and accuracy between source (training) distribution and target (testing) distribution (Rezaei et al., 2021; Singh et al., 2021) for a given classifier. Specifically, in terms of noisy sensitive information, we can readily think of the noisy distribution as source, and clean distribution as target. However, most work on distribution shift requires access to the target distribution, which in return, requires external tools for noise rate evaluation.

In light of current limitations in both aspects, in this work, we propose a general framework for fairness under noisy sensitive attribute from the perspective of distribution shift. We consider group- and class-dependent noise rates within each subgroup, and we show that under such formulation, fairness metrics under noisy attributes are not necessarily proportional to those under clean attributes. We propose to solve the problem from the perspective of fair representation learning, where the idea is to train a fair encoder such that its latent representation achieves desired fairness and accuracy properties. We quantify disparities between noisy and clean distributions from the perspective of group- and class-dependent distribution shift under our formulation of noisy sensitive information, and we show theoretically that under bounded divergence between noisy distributions of different subgroups, we have the transferability of fairness guarantee between noisy and clean data, where disparities under clean data are upper-bounded by disparities under noisy data up to addictive and multiplicative constants. In this way, we are able to achieve fairness under noisy protected information, without applying extra techniques for noise rate estimation. What's more. we extend our method to fairness under label noise, where we show both theoretically and experimentally that our method improves fairness under group- and label-dependent label noise.

We summarize our contribution as follows:

1. We discuss two types of noise (i.e., sensitive attribute noise and label noise) under group- and label-dependent assumptions, and we derive the theoretical connections between fairness measures under noisy and clean data in the presence of each type of noise.
2. We formulate fairness under sensitive attribute noise through a novel perspective of distribution shift, from which we introduce a representation learning framework without requiring extra techniques for noise rate estimation. Moreover, we extend our framework to address fairness under label noise.
3. We prove theoretically the transferability of fairness between noisy and clean data both under sensitive attribute noise and label noise.
4. We validate the effectiveness of our method in improving fairness through experiments on three benchmark datasets, where we evaluate its performance under both single exposure and simultaneous presence of sensitive attribute and label noise.

## 2 Related work

**Fairness in Machine Learning**: Discrepancies in machine learning systems against certain groups or subgroups are generally considered to be originated from biased training data, rather than the training process (Kleinberg et al., 2016). To quantify such disparities, different fairness notions have been proposed, including disparate impact (Willborn, 1984), equal opportunity and equalized odds Hardt et al. (2016), Lipschitz continuity (Dwork et al., 2012; Yurochkin et al., 2019) and calibration(Dwork et al., 2012) for individual fairness. Accordingly, different methods have been proposed to mitigate bias during the training process. Preprocessing methods (Tan et al., 2020; Li and Liu, 2022; Kleindessner et al., 2023) aims at obtaining a rectified distribution of input features or labels such that the desired fairness measures are satisfied on the training set. Inprocessing methods (Madras et al., 2018; Roh et al., 2020; Chai et al., 2022) aim at reagulating the training process with relaxed fairness constraints. Postprocessing methods aim at adjusting decision thresholds (Hardt et al., 2016; Corbett-Davies et al., 2017; Hsu et al., 2022) for each group or learning a instance-wise mapping of soft labels based on expected fairness measures. However, most of existing work on fairness is formulated without considering the effect of label or attribute noise.

**Noise-Tolerant Fairness**: Existing work on fairness under attribute noise relies on the estimation of noise rates. Lamy et al. (2019) first proposes a general framework for fairness under group-dependent attribute noise, and propose to rectify unfairness tolerance during training based on noise rate estimation. Celis et al. (2021) considers the problem similarly by rectifying fairness constraints during training with noise transition matrix. Wang et al. (2020) considers the problem from the perspective of distributionally robust optimization and uses soft group assignment to rectify fairness constraint. Mehrotra and Celis (2021) proposes a preprocessing framework based on sample selection with relaxed weight constraints specified by noise rates.

Methods on fairness under label noise generally focuses on rectifying fairness measures based on estimated noise rates. Work including (Wang et al., 2021; Wu et al., 2022) proposes to replace fairness constraints on noisy data with their corresponding surrogate measures on clean data. Zhang et al. (2023) proposes a VAE-based framework to achieve disentanglement between input feature and sensitive information and uses mutual information between noisy and clean label as penalty term.

**Fairness under Distribution Shift**: Distribution shift has been shown to be non-trivial in fairness and could significantly deteriorate discrimination of a fair classifier (Mishler and Dalmasso, 2022; Schrouff et al., 2022; Chai and Wang, 2023). A general assumption in distribution shift is that labelled source distribution \((X,Y,A)\sim P_{src}\) and unlabelled target distribution \((X,A)\sim P_{trg}\) are accessible during training. Generally, methods on fairness under distribution shift falls into two categories: importance reweighting (Sugiyama et al., 2007; Cortes et al., 2010), where the idea is to reweight instance-wise training loss based on the corresponding ratio between source and target distribution, and robust log loss (Rezaei et al., 2020; Singh et al., 2021; Rezaei et al., 2021; An et al., 2022), where the idea is to formulate training problem as a mini-max optimization problem with robust training loss. Chen et al. (2022) proposes to quantify transferability of fairness under bounded distribution shift represented by group-wise shift vectors, where feature shift and label shift are considered separately.

## 3 Method

Throughout this section, we use \(mea\) to denote measures under clean data, \(\tilde{mea}\) and \(\widetilde{mea}\) to denote measures under sensitive attribute noise and under label noise, respectively. For example, we use \(\{A,Y\}\) to denote the random variables of sensitive attribute and label under clean data, \(\{\hat{A},Y\}\) the random variables under attribute noise, and \(\{A,\widetilde{Y}\}\) the random variables under label noise. We use \(\eta\) and \(\beta\) to denote sensitive attribute noise rate and label noise rate, respectively.

### Problem Formulation

Let \(\{(x_{i},y_{i},a_{i}),1\leqslant i\leqslant N\}\) be the training set where \(x_{i}\in\mathbbm{R}^{n}\) is the input feature, \(y_{i}\in\{0,1\}\) the training label and \(a_{i}\in\{0,1\}\) the sensitive attribute, let \(f\) be the function of classifier, a general fair classification problem can be formulated as

\[\operatorname*{arg\,min}_{f}\frac{1}{N}\sum_{i=1}^{N}l(f(x_{i}),y_{i}),\text{ s.t. }l_{f}(f(x_{i}),y_{i},a_{i})\leqslant\epsilon,\]

where \(l\) is the classification loss and \(l_{f}\) is the fairness constraint specified by designated fairness notions. For example, \(l_{f}=|\frac{\sum_{\{i|a_{i}=a_{i}\}}1\{f(x_{i})\geq 0.5\}}{|\{i|a_{i}=a\}|}- \frac{\sum_{\{i|a_{i}=a^{\prime}\}}1\{f(x_{i})\geq 0.5\}}{|\{i|a_{i}=a^{\prime} \}|}|\), where \(a^{\prime}=|1-a|\), corresponds to disparate impact (DI), and \(l_{f}=|\frac{\sum_{\{i|a_{i}=a_{i},y_{i}=0\}}1\{f(x_{i})\geq 0.5\}}{|\{i|a_{i}=a,y_{i}=0\}|}-\frac{\sum_{\{i|a_{i}=a^{\prime},y_{i}=0\}}1\{f(x_{i})\geq 0.5\}}{|\{i|a_{i}=a,y_{i}=0\}|}|+|\frac{\sum_{\{i|a_{i}=a,y_{i}=1\}}1\{f(x_{i})\geq 0.5\}}{|\{i|a_{i}=a,y_{i}=1\}|}-\frac{\sum_{\{i|a_{i}=a^{\prime},y_{i}=1\}}1\{f(x_{i})\geq 0.5\}}{|\{i|a_{i}=a ^{\prime},y_{i}=1\}|}|\) corresponds to equalized odds (EOd).

In the presence of sensitive attribute noise, such formulation can result in a biased estimation of discrimination on training data. Previous work (Lamy et al., 2019; Celis et al., 2021) has shown that under group-dependent sensitive attribute noise rate \(\eta_{a}:=p\left[A\neq\hat{A}|\hat{A}=a\right]\), fairness measure under noisy data is proportional to that under clean data:

\[\hat{l}_{f}=(1-\eta_{a}-\eta_{a^{\prime}})l_{f}.\]

However, such formulation can become trivial during training, especially for deep neural networks, where different noise rates can become ignorable under hyperparameter-tuning due to the proportionality. Instead, we consider a more general version of attribute flip, where noise rates are both group-dependent and label-dependent. Specifically, let \(P_{ya}\) and \(Q_{ya}\) be the distribution of data and predicted soft labels in the clean subgroup \(\mathbb{S}_{ya}:=\{i|y_{i}=y,a_{i}=a\}\) respectively, let \(\eta_{ya}:=p\left[A\neq\hat{A}|Y=y,A=a\right]\) be the sensitive attribute noise rate in the corresponding subgroup, we have the following relationship regarding noisy and clean distribution:

\[\hat{P}_{ya}=(1-\eta_{ya})P_{ya}+\eta_{ya^{\prime}}P_{ya^{\prime}}.\] (1)

Correspondingly, we have the following relationship regarding DI and EOd under clean and noisy data:

**Lemma 1**.: _Under group- and label-dependent attribute noise rate \(\eta_{ya}\), we have_

\[\hat{EOd}=(1-\eta_{10}-\eta_{11})\text{DTPR}+(1-\eta_{00}-\eta_{01})\text{ DTNR},\]

\[\hat{DI}=|\lambda_{0}\text{FPR}_{0}-\lambda_{1}\text{FPR}_{1}+( \hat{\alpha}_{0}-\hat{\alpha}_{0}\eta_{10}-\hat{\alpha}_{1}\eta_{11})\text{ TPR}_{0}-(\hat{\alpha}_{1}-\hat{\alpha}_{0}\eta_{10}-\hat{\alpha}_{1}\eta_{11}) \text{TPR}_{1}|\,,\]

\[\lambda_{a}=[1-(\hat{\alpha}_{a}+\eta_{0a})+\hat{\alpha}_{a}\eta_{0a}-\eta_{0 a^{\prime}}+\hat{\alpha}_{a^{\prime}}\eta_{0a^{\prime}}],\]

where DTPR (disparate true positive rate) \(=|\text{TPR}_{0}-\text{TPR}_{1}|\) is the difference in true positive rate (TPR) between the two sensitive groups \(\{i|a_{i}=0\}\) and \(\{i|a_{i}=1\}\), DTNR (disparate true negative rate) \(=|\text{TNR}_{0}-\text{TNR}_{1}|\), and \(\hat{\alpha}_{a}=\frac{[\{i|\hat{a}_{i}=a,y_{i}=1\}]}{[\{i|\hat{a}_{i}=a\}]}\) is the base rate of noisy data at group \(\{i|\hat{a}_{i}=a\}\). Here we assume \(\eta_{ya}+\eta_{ya^{\prime}}\leqslant 1\); for \(\eta_{ya}+\eta_{ya^{\prime}}\geqslant 1\), it is easy to come to equivalent expressions due to symmetry. From Lemma 1, we observe that under group- and label-dependent noise, EOd under noisy data can be expressed as a weighted sum of disparate TPR and TNR under clean data, while DI under noisy data takes a more complicated form involving both noisy base rates and noise rates and does not have a similar relationship with DI under clean data due to possible change in base rates. Correspondingly, optimizing over DI or EOd directly on noisy data may not lead to satisfying improvement in fairness, if without noise estimation.

### From the Perspective of Distribution Shift

Estimation of sensitive attribute noise can be inaccurate. Instead, we aim to find a general way for fairness under attribute noise without using extra tools for noise estimation. Note from Lemma 1 that the deviation in fairness measure under noisy data is, in fact, induced by the disparities between noisy and clean distribution, leading to skewed estimation of group-wise utilities. Thus, one direct implication is to consider the problem from the perspective of _covariate shift_ on training set. Specifically, we have the clean distribution of data as weighted subtraction of noisy distributions:

\[P_{ya}=\frac{1-\eta_{ya^{\prime}}}{1-\eta_{ya}-\eta_{ya^{\prime}}}\hat{P}_{ya }-\frac{\eta_{ya}}{1-\eta_{ya}-\eta_{ya^{\prime}}}\hat{P}_{ya^{\prime}}.\] (2)

Consider noisy data as the source distribution and clean data as target, we have the KL-divergence between noisy and clean distribution at each subgroup as follows:

\[D_{KL}(\hat{P}_{ya}||P_{ya})=\int\hat{P}_{ya}\log\frac{\hat{P}_{ya}}{P_{ya}}=- \int\hat{P}_{ya}\log\left[\frac{1-\eta_{ya^{\prime}}}{1-\eta_{ya}-\eta_{ya^{ \prime}}}-\frac{\eta_{ya}\frac{\hat{P}_{ya^{\prime}}}{P_{ya}}}{1-\eta_{ya}- \eta_{ya^{\prime}}}\right].\] (3)

This indicates that the discrepancy, or shift between noisy and clean distribution are in fact, controlled by the discrepancy between corresponding noisy subgroups. By minimizing the divergence between data distribution \(\hat{P}_{ya}\) and \(\hat{P}_{ya^{\prime}}\), which, in return, minimizes the divergence between predicted soft label distribution \(Q_{ya}\) and \(\hat{Q}_{ya}\) and thus provides fairness guarantee for noisy data, we are able to minimize the divergence between noisy and clean distribution. Therefore, by minimizing the divergence between \(\hat{P}_{ya}\) and \(\hat{P}_{ya^{\prime}}\) we are able to ensure the transferability of fairness improvement between noisy and clean data. Specifically, when \(\hat{P}_{ya}=\hat{P}_{ya^{\prime}}\), we have \(D_{KL}(\hat{P}_{ya}||P_{ya})=0\).

### Fair Representation Learning

Inspired by Eq. (3), transferability of fairness between clean and noisy data can be ensured under equalized distribution on noisy data: \(\hat{P}_{ya}=\hat{P}_{ya^{\prime}},\,\forall y\). Due to disparities on training data, however, such requirement is generally infeasible without applying extra regularization. Therefore, we consider a fair representation learning method for fairness under noisy attribute based on normalizing flow. Let \(g_{ya}\) be the function of bijective encoder for samples in the noisy subgroup \(\hat{\mathbb{S}}_{ya}\) and \(h\) be the function of classification head, let \(z_{ya}=g_{ya}(x)\) be the latent representation of the corresponding subgroup and \(P_{z_{ya}}\) be the corresponding density, we can use change of variables formula to calculate the densities of \(z_{ya}\) as:

\[\log P_{z_{ya}}(\bm{z})=\log P_{ya}\left(g_{ya}^{-1}(\bm{z})\right)+\log \left|\det\frac{\partial g_{ya}^{-1}(\bm{z})}{\partial\bm{z}}\right|.\] (4)

Following Balunovic et al. (2021), we use symmetrized KL-divergence to approximate the statistical distance between subgroups:

\[\mathcal{L}_{y}=\frac{1}{B}\sum_{j=1}^{B}\left(\log P_{z_{ya}}\left(\bm{z}_{ya }^{j}\right)-\log P_{z_{ya^{\prime}}}\left(\bm{z}_{ya}^{j}\right)+\log P_{z_{ya ^{\prime}}}\left(\bm{z}_{ya^{\prime}}^{j}\right)-\log P_{z_{ya}}\left(\bm{z}_{ y^{\prime}}^{j}\right)\right),\forall y\] (5)

where \(B\) is the batch size. And the overall training objective can be written as

\[\operatorname*{arg\,min}_{g_{00},g_{01},g_{10},g_{11},h}\lambda_{0}\mathcal{ L}_{0}(g_{00},g_{01})+\lambda_{1}\mathcal{L}_{1}(g_{10},g_{11})+(1-\lambda_{0}- \lambda_{1})\mathcal{L}_{cls}(g_{00},g_{01},g_{10},g_{11},h).\] (6)

### Theoretical Analysis

It is easy to see from Eq. (2) that when \(\hat{P}_{ya}=\hat{P}_{ya^{\prime}}\), we also have \(P_{ya}=P_{ya^{\prime}}\) regardless of noise rates, and the classifier achieves perfect EOd on both clean and noisy data. In reality, however, it is hard to achieve prefect fairness. The following theorem states a general relationship between fairness measure under clean and noisy data:

**Theorem 1**.: _Let \(Q_{ya}\) and \(\hat{Q}_{ya}\) be the distribution of predicted soft labels in the clean subgroup \(\mathbb{S}_{ya}\) and noisy group respectively, let \(\eta_{ya}:=p\left[A\neq\hat{A}|Y=y,\hat{A}=a\right]\) be the group- and class-dependent noise rate. For \(D_{KL}(\hat{Q}_{ya},\hat{Q}_{ya^{\prime}})\leq\epsilon_{y}\), we have the following upper- and lower-bound regarding EOd under clean distribution and EOd under noisy distribution:_

\[\text{EOd}\leq\text{EOd}\leq\text{EOd}+\frac{\eta_{00}+\eta_{01}}{1-\eta_{00}- \eta_{01}}\sqrt{\epsilon_{0}}+\frac{\eta_{10}+\eta_{11}}{1-\eta_{10}-\eta_{11 }}\sqrt{\epsilon_{1}}.\] (7)

We defer full proof to appendix. Theorem 1 shows that, despite EOd itself serves as a biased estimation of ground-truth EOd, by minimizing the KL-divergence between distributions of soft labels in label-wise subgroups, we are able to minimize the upper-bound of clean EOd, which validates the feasibility of our method.

### Fairness under Noisy Labels

We further extent our method to fairness under noisy labels. Following previous work on fairness under label noise (Wang et al., 2021), we consider group- and label-dependent noise rates. Let \(\beta_{ya}:=p\left[Y\neq\widetilde{Y}|\widetilde{Y}=y,A=a\right]\) be the label noise rate at the subgroup \(\widetilde{S}_{ya}\), we have the following relationship regarding the distribution of clean and noisy data:

\[\widetilde{P}_{ya}=(1-\beta_{ya})P_{ya}+\beta_{ya}P_{y^{\prime}a},\] (8)where \(y^{\prime}=|1-y|\). Correspondingly, we have the following relationship regarding fairness measures under clean and noisy data:

**Lemma 2**.: _Under group- and label-dependent label noise rate \(\beta_{ya}\), we have_

\[\widetilde{\text{EO}d}= |\text{TPPR}_{0}-\text{TPR}_{1}+\beta_{10}(\text{FPR}_{0}-\text{TPR }_{0})-\beta_{11}(\text{FPR}_{1}-\text{TPR}_{1})|\] \[+|\text{INR}_{0}-\text{INR}_{1}+\beta_{00}(\text{FNR}_{0}-\text{ INR}_{0})-\beta_{01}(\text{FNR}_{1}-\text{INR}_{1})|,\]

which shows that under label noise, \(\widetilde{\text{DI}}\) itself serves as an unbiased estimation, while \(\widetilde{\text{EO}d}\) is not an unbiased estimation of EOd. A natural question here is, _does our method also work under label noise?_ The following lemma shows the connection between \(\widetilde{\text{EO}d}\) and EOd:

**Lemma 3**.: _let \(\beta_{ya}\) be the group- and class-dependent label noise rate, we have the following upper-bound regarding EOd under clean distribution and \(\widetilde{\text{EO}d}\) under noisy distribution:_

\[\text{EOd}\leqslant\min\left\{\frac{1}{1-\beta_{00}-\beta_{10}}+\beta,\frac{ 1}{1-\beta_{01}-\beta_{11}}+\beta\right\}\widetilde{\text{EO}d}+2\beta,\]

\[\beta=\max\left\{\left|\frac{\beta_{00}}{1-\beta_{00}-\beta_{10}}-\frac{ \beta_{01}}{1-\beta_{01}-\beta_{11}}\right|,\left|\frac{1-\beta_{00}}{1-\beta _{10}-\beta_{00}}-\frac{1-\beta_{01}}{1-\beta_{11}-\beta_{01}}\right|\right\}.\]

We defer full proof to appendix. Therefore, under label noise, optimizing over \(\widetilde{\text{EO}d}\) can still benefit EOd, which is upper-bounded by \(\widetilde{\text{EO}d}\) up to an multiplicative constant and an addictive constant determined by the noise rates.

## 4 Experiments

### Experimental Setup

We validate our method on three benchmark datasets: **COMPAS**: The COMPAS dataset (Larson et al., 2016) contains 7,215 samples with 11 attributes. Following previous works on fairness (Zafar et al., 2017), we only select black and white defendants in COMPAS dataset, and the modified dataset contains 6,150 samples. The goal is to predict whether a defendant reoffends within two years, and we choose _race_ as sensitive attributes. **Adult**: The Adult dataset (Dua and Graff, 2017) contains 65,123 samples with \(14\) attributes. The goal is to predict whether an individual's income exceeds \(50K\), and we choose _gender_ as sensitive attributes. **CelebA**: The CelebA dataset (Liu et al., 2015) contains 202,599 face images, each of resolution \(178\times 218\), with 40 binary attributes. We choose gender as labels and _age_ as sensitive attributes.

We implement our method in PyTorch 2.0.0 on one RTX-3090 GPU. We use accuracy as utility measure, DI (Willborn, 1984) and EOd (Hardt et al., 2016) as fairness measure. We use RealNVP (Dinh et al., 2016) to build our models, and network structures for other methods are chosen as MLP for COMPAS and Adult datasets, and ResNet-18 for CelebA dataset. We repeat experiments on each dataset three times and report the average results. In each repetition, we use a \(80\%\)-\(20\%\) training-testing partition of data.

We compare our method with following related methods:

* **Baseline**: Neural network without fairness regularization.
* **Inprocessing**: Neural network with relaxed EOd constraint by (Wang et al., 2022). This is a fairness method without considering noisy data.
* **DLR**: Neural network with fairness constraints rectified by noise transition matrix (Celis et al., 2021). This method focuses on fairness with noisy sensitive attributes.
* **FairExpec**: Neural network with instance-wise reweighing as specified by (Mehrotra and Celis, 2021). This method focuses on fairness with noisy sensitive attributes.
* **CorScale**: Neural network with rectified fairness constraints (Lamy et al., 2019). This method focuses on fairness with noisy sensitive attributes.
* **SurrogateLoss**: Neural network with modified EOd constraint by (Wang et al., 2021). This method focuses on fairness with noisy labels.

### Fairness with Noisy Protected Attributes

#### 4.2.1 Fairness under given noise rates

Results on fairness under noisy sensitive attributes are shown in Table 1-3. Compared with baseline and inprocessing which also do not require estimation of noise rates our method achieves a better trade-off in terms of fairness and accuracy, with significant improvement in fairness with smaller or comparable sacrifice in accuracy. Compared with methods that require noise estimation (DLR, FairExpec and CorScale), our method achieves better or comparable performance in terms of both fairness and accuracy, which validates the effectiveness of our method.

#### 4.2.2 Fairness under Varying Noise Rates

We move on to discuss results on fairness under varying noise rates. Specifically, we use noise rates in previous sections as baseline rates and vary each component within the range of \([0,0.5]\). Results under varying noise rates are shown in Fig. 1-3. As shown in the figures, under varying noise rates, our method achieves relatively stable performance for both DI and EOd compared with other methods, which indicates that our method performs robustly under different noise rates.

### Fairness under Noisy Sensitive Attributes and Noisy Labels

As discussed in Lemma 3, apart from sensitive attribute noise, our method can also be generalized to fairness under the exposure of label noise. Therefore, we also validate our method in the presence of both attribute and label noise, and results are shown in Table 4-6. While existing methods typically address one type of noise, our method is capable of handling both types of noise simultaneously, with better or comparable performance in terms of both fairness improvement and accuracy, and without requiring extra tools for noise rate estimation. This also validates our analysis in the previous section.

\begin{table}
\begin{tabular}{l l l l} \hline Method & Accuracy & Disparate Impact & EOd \\ \hline Baseline & 66.80\(\pm\)0.34\% & 24.13\(\pm\)1.46\% & 42.96\(\pm\)2.02\% \\ Inprocessing (Wang et al., 2022) & 62.35\(\pm\)0.65\% & 13.34\(\pm\)1.15\% & 17.68\(\pm\)1.47\% \\ DLR (Celis et al., 2021) & 60.34\(\pm\)0.79\% & 11.26\(\pm\)1.35\% & 10.46\(\pm\)1.89\% \\ FairExpec (Mehrotra and Celis, 2021) & 62.27\(\pm\)1.18\% & 10.36\(\pm\)1.27\% & 12.26\(\pm\)1.52\% \\ CorScale (Lamy et al., 2019) & 61.37\(\pm\)0.68\% & 15.25\(\pm\)1.26\% & 21.37\(\pm\)2.21\% \\ Ours & 63.65\(\pm\)0.87\% & 9.94\(\pm\)1.45\% & 8.67\(\pm\)2.95\% \\ \hline \end{tabular}
\end{table}
Table 1: Experimental results on COMPAS dataset under sensitive attribute noise. The noise rates are set as \(\eta_{00}=0.2\), \(\eta_{01}=0.1\), \(\eta_{10}=0.3\), \(\eta_{11}=0.2\).

\begin{table}
\begin{tabular}{l l l l} \hline Method & Accuracy & Disparate Impact & EOd \\ \hline Baseline & 84.16\(\pm\)0.45\% & 16.67\(\pm\)1.35\% & 20.27\(\pm\)1.13\% \\ Inprocessing (Wang et al., 2022) & 82.27\(\pm\)0.69\% & 13.34\(\pm\)1.58\% & 16.29\(\pm\)1.53\% \\ DLR (Celis et al., 2021) & 78.67\(\pm\)0.66\% & 9.64\(\pm\)1.35\% & 11.17\(\pm\)1.28\% \\ FairExpec (Mehrotra and Celis, 2021) & 81.65\(\pm\)0.59\% & 9.94\(\pm\)1.45\% & 12.28\(\pm\)1.13\% \\ CorScale (Lamy et al., 2019) & 80.27\(\pm\)0.45\% & 11.96\(\pm\)1.12\% & 14.57\(\pm\)1.86\% \\ Ours & 82.11\(\pm\)0.64\% & 9.97\(\pm\)1.32\% & 6.84\(\pm\)1.59\% \\ \hline \end{tabular}
\end{table}
Table 2: Experimental results on Adult dataset under sensitive attribute noise. The noise rates are set as \(\eta_{00}=0.15\), \(\eta_{01}=0.1\), \(\eta_{10}=0.1\), \(\eta_{11}=0.3\).

\begin{table}
\begin{tabular}{l l l l} \hline Method & Accuracy & Disparate Impact & EOd \\ \hline Baseline & 89.43\(\pm\)0.57\% & 22.69\(\pm\)1.86\% & 18.32\(\pm\)1.67\% \\ Inprocessing (Wang et al., 2022) & 86.47\(\pm\)0.83\% & 16.49\(\pm\)1.52\% & 15.21\(\pm\)1.46\% \\ DLR (Celis et al., 2021) & 86.27\(\pm\)0.62\% & 12.54\(\pm\)1.76\% & 11.58\(\pm\)1.29\% \\ FairExpec (Mehrotra and Celis, 2021) & 85.54\(\pm\)0.69\% & 11.45\(\pm\)1.84\% & 11.27\(\pm\)1.65\% \\ CorScale (Lamy et al., 2019) & 85.34\(\pm\)1.17\% & 14.26\(\pm\)1.33\% & 13.16\(\pm\)1.58\% \\ Ours & 87.14\(\pm\)0.68\% & 8.84\(\pm\)1.42\% & 8.43\(\pm\)1.19\% \\ \hline \end{tabular}
\end{table}
Table 3: Experimental results on CelebA dataset under sensitive attribute noise. The noise rates are set as \(\eta_{00}=0.1\), \(\eta_{01}=0.2\), \(\eta_{10}=0.3\), \(\eta_{11}=0.1\).

Figure 1: Change of EOd and DI as noise rates \(\eta_{ya}\) vary on COMPAS dataset.

Figure 2: Change of EOd and DI as noise rates \(\eta_{ya}\) vary on Adult dataset.

## 5 Conclusion

Fairness under noisy perturbation is an important yet less studied problem. In this paper, we formulate noisy perturbation as both group- and label-dependent, and we propose a fair representation learning framework based on normalizing flow to solve the problem without using extra tools for noise estimation. We prove theoretically the transferability of fairness between noisy and clean data under noisy sensitive attributes, and we show theoretically the connection between fairness measures of clean and noisy data under label noise. We validate from experiments that our method performs better or comparably in the improvement of fairness under both label noise and sensitive attributes noise generated under both static and varying noise rates, compared with state-of-the-art alternatives, with relatively small sacrifice in accuracy. Future directions include alternative methods for fair representation learning, and alternative formulations of noise perturbation.

\begin{table}
\begin{tabular}{l l l l} \hline Method & Accuracy & Disparate Impact & EOd \\ \hline Baseline & 87.23\(\pm\)0.69\% & 21.27\(\pm\)1.83\% & 19.34\(\pm\)1.28\% \\ Inprocessing Wang et al. (2022) & 83.25\(\pm\)0.82\% & 15.54\(\pm\)1.37\% & 14.23\(\pm\)1.15\% \\ DLR Celis et al. (2021) & 84.36\(\pm\)0.67\% & 12.27\(\pm\)1.56\% & 12.21\(\pm\)1.34\% \\ FairExpec Mehrotra and Celis (2021) & 83.87\(\pm\)0.47\% & 10.59\(\pm\)1.26\% & 11.65\(\pm\)1.44\% \\ CorScale Lamy et al. (2019) & 84.21\(\pm\)1.36\% & 13.47\(\pm\)1.25\% & 12.29\(\pm\)1.17\% \\ SurrogateLoss Wang et al. (2021) & 85.23\(\pm\)0.69\% & 12.37\(\pm\)1.64\% & 11.16\(\pm\)1.43\% \\ Ours & 85.11\(\pm\)0.69\% & 9.74\(\pm\)1.28\% & 8.78\(\pm\)1.27\% \\ \hline \end{tabular}
\end{table}
Table 6: Results on CelebA dataset under label and sensitive attribute noise. The noise rates are set as \(\eta_{00}=0.1\), \(\eta_{01}=0.2\), \(\eta_{10}=0.3\), \(\eta_{11}=0.1\), \(\beta_{00}=0.25\), \(\beta_{01}=0.1\), \(\beta_{10}=0.15\), \(\beta_{11}=0.3\).

Figure 3: Change of EOd and DI as noise rates \(\eta_{ya}\) vary on CelebA dataset.

\begin{table}
\begin{tabular}{l l l l} \hline Method & Accuracy & Disparate Impact & EOd \\ \hline Baseline & 81.54\(\pm\)0.85\% & 16.85\(\pm\)1.65\% & 21.75\(\pm\)1.42\% \\ Inprocessing Wang et al. (2022) & 77.46\(\pm\)0.58\% & 14.27\(\pm\)1.48\% & 16.63\(\pm\)1.25\% \\ DLR Celis et al. (2021) & 78.59\(\pm\)0.86\% & 10.52\(\pm\)1.17\% & 12.46\(\pm\)1.37\% \\ FairExpec Mehrotra and Celis (2021) & 79.69\(\pm\)1.16\% & 11.37\(\pm\)1.53\% & 10.47\(\pm\)2.23\% \\ CorScale Lamy et al. (2019) & 78.76\(\pm\)1.24\% & 12.66\(\pm\)1.83\% & 15.43\(\pm\)1.76\% \\ SurrogateLoss Wang et al. (2021) & 79.14\(\pm\)1.56\% & 11.56\(\pm\)1.35\% & 12.67\(\pm\)1.52\% \\ Ours & 80.27\(\pm\)0.67\% & 8.56\(\pm\)1.67\% & 7.47\(\pm\)1.85\% \\ \hline \end{tabular}
\end{table}
Table 5: Results on Adult dataset under label and sensitive attribute noise. The noise rates are set as \(\eta_{00}=0.15\), \(\eta_{01}=0.1\), \(\eta_{10}=0.1\), \(\eta_{11}=0.3\), \(\beta_{00}=0.45\), \(\beta_{01}=0.3\), \(\beta_{10}=0.15\), \(\beta_{11}=0.35\).

## References

* An et al. (2022) An, B., Che, Z., Ding, M., and Huang, F. (2022). Transferring fairness under distribution shifts via fair consistency regularization. _arXiv preprint arXiv:2206.12796_.
* Angwin et al. (2016) Angwin, J., Larson, J., Mattu, S., and Kirchner, L. (2016). Machine bias. In _Ethics of Data and Analytics_, pages 254-264. Auerbach Publications.
* Balunovic et al. (2021) Balunovic, M., Ruoss, A., and Vechev, M. (2021). Fair normalizing flows. _arXiv preprint arXiv:2106.05937_.
* Celis et al. (2021) Celis, L. E., Huang, L., Keswani, V., and Vishnoi, N. K. (2021). Fair classification with noisy protected attributes: A framework with provable guarantees. In _International Conference on Machine Learning_, pages 1349-1361. PMLR.
* Chai et al. (2022) Chai, J., Jang, T., and Wang, X. (2022). Fairness without demographics through knowledge distillation. _Advances in Neural Information Processing Systems_, 35:19152-19164.
* Chai and Wang (2023) Chai, J. and Wang, X. (2023). To be robust and to be fair: Aligning fairness with robustness. _arXiv preprint arXiv:2304.00061_.
* Chen et al. (2022) Chen, Y., Raab, R., Wang, J., and Liu, Y. (2022). Fairness transferability subject to bounded distribution shift. _arXiv preprint arXiv:2206.00129_.
* Choi et al. (2020) Choi, Y., Dang, M., and Van den Broeck, G. (2020). Group fairness by probabilistic modeling with latent fair decisions. _arXiv preprint arXiv:2009.09031_.
* Corbett-Davies et al. (2017) Corbett-Davies, S., Pierson, E., Feller, A., Goel, S., and Huq, A. (2017). Algorithmic decision making and the cost of fairness. In _Proceedings of the 23rd acm sigkdd international conference on knowledge discovery and data mining_, pages 797-806.
* Cortes et al. (2010) Cortes, C., Mansour, Y., and Mohri, M. (2010). Learning bounds for importance weighting. _Advances in neural information processing systems_, 23.
* De-Arteaga et al. (2022) De-Arteaga, M., Feuerriegel, S., and Saar-Teschansky, M. (2022). Algorithmic fairness in business analytics: Directions for research and practice. _Production and Operations Management_, 31(10):3749-3770.
* Diana et al. (2022) Diana, E., Gill, W., Kearns, M., Kenthapadi, K., Roth, A., and Sharifi-Malvajerdi, S. (2022). Multiaccurate proxies for downstream fairness. In _2022 ACM Conference on Fairness, Accountability, and Transparency_, pages 1207-1239.
* Dinh et al. (2016) Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2016). Density estimation using real nvp. _arXiv preprint arXiv:1605.08803_.
* Dressel and Farid (2018) Dressel, J. and Farid, H. (2018). The accuracy, fairness, and limits of predicting recidivism. _Science advances_, 4(1):eaao5580.
* Dua and Graff (2017) Dua, D. and Graff, C. (2017). UCI machine learning repository.
* Dwork et al. (2012) Dwork, C., Hardt, M., Pitassi, T., Reingold, O., and Zemel, R. (2012). Fairness through awareness. In _Proceedings of the 3rd innovations in theoretical computer science conference_, pages 214-226.
* Fioretto et al. (2022) Fioretto, F., Tran, C., Van Hentenryck, P., and Zhu, K. (2022). Differential privacy and fairness in decisions and learning tasks: A survey. _arXiv preprint arXiv:2202.08187_.
* Gari et al. (2021) Gari, V., Lamprier, S., and Detyniecki, M. (2021). Fairness without the sensitive attribute via causal variational autoencoder. _arXiv preprint arXiv:2109.04999_.
* Hardt et al. (2016) Hardt, M., Price, E., and Srebro, N. (2016). Equality of opportunity in supervised learning. _Advances in neural information processing systems_, 29.
* Hsu et al. (2022) Hsu, B., Mazumder, R., Nandy, P., and Basu, K. (2022). Pushing the limits of fairness impossibility: Who's the fairest of them all? _arXiv preprint arXiv:2208.12606_.
* Hubert et al. (2016)Jackson, J. R. (2018). Algorithmic bias. _Journal of Leadership, Accountability and Ethics_, 15(4):55-65.
* Kleinberg et al. (2016) Kleinberg, J., Mullainathan, S., and Raghavan, M. (2016). Inherent trade-offs in the fair determination of risk scores. _arXiv preprint arXiv:1609.05807_.
* Kleindessner et al. (2023) Kleindessner, M., Donini, M., Russell, C., and Zafar, M. B. (2023). Efficient fair pca for fair representation learning. In _International Conference on Artificial Intelligence and Statistics_, pages 5250-5270. PMLR.
* Lahoti et al. (2020) Lahoti, P., Beutel, A., Chen, J., Lee, K., Prost, F., Thain, N., Wang, X., and Chi, E. (2020). Fairness without demographics through adversarially reweighted learning. _Advances in Neural Information Processing Systems_, 33:728-740.
* Lamy et al. (2019) Lamy, A., Zhong, Z., Menon, A. K., and Verma, N. (2019). Noise-tolerant fair classification. _Advances in neural information processing systems_, 32.
* Larson et al. (2016) Larson, J., Mattu, S., Kirchner, L., and Angwin, J. (2016). Compas analysis. _GitHub, available at: https://github. com/propublica/compas-analysis_.
* Li and Liu (2022) Li, P. and Liu, H. (2022). Achieving fairness at no utility cost via data reweighing with influence. In _International Conference on Machine Learning_, pages 12917-12930. PMLR.
* Liu et al. (2015) Liu, Z., Luo, P., Wang, X., and Tang, X. (2015). Deep learning face attributes in the wild. In _Proceedings of International Conference on Computer Vision (ICCV)_.
* Madras et al. (2018) Madras, D., Creager, E., Pitassi, T., and Zemel, R. (2018). Learning adversarially fair and transferable representations. In _International Conference on Machine Learning_, pages 3384-3393. PMLR.
* Mehrabi et al. (2021) Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., and Galstyan, A. (2021). A survey on bias and fairness in machine learning. _ACM Computing Surveys (CSUR)_, 54(6):1-35.
* Mehrotra and Celis (2021) Mehrotra, A. and Celis, L. E. (2021). Mitigating bias in set selection with noisy protected attributes. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, pages 237-248.
* Mishler and Dalmasso (2022) Mishler, A. and Dalmasso, N. (2022). Fair when trained, unfair when deployed: Observable fairness measures are unstable in performative prediction settings. _arXiv preprint arXiv:2202.05049_.
* Rezaei et al. (2020) Rezaei, A., Fathony, R., Memarrast, O., and Ziebart, B. (2020). Fairness for robust log loss classification. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 5511-5518.
* Rezaei et al. (2021) Rezaei, A., Liu, A., Memarrast, O., and Ziebart, B. D. (2021). Robust fairness under covariate shift. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 9419-9427.
* Ricci Lara et al. (2022) Ricci Lara, M. A., Echeveste, R., and Ferrante, E. (2022). Addressing fairness in artificial intelligence for medical imaging. _nature communications_, 13(1):4581.
* Roh et al. (2020) Roh, Y., Lee, K., Whang, S. E., and Suh, C. (2020). Fairbatch: Batch selection for model fairness. _arXiv preprint arXiv:2012.01696_.
* Schrouff et al. (2022) Schrouff, J., Harris, N., Koyejo, S., Alabdulmohsin, I. M., Schnider, E., Opsahl-Ong, K., Brown, A., Roy, S., Mincu, D., Chen, C., et al. (2022). Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. _Advances in Neural Information Processing Systems_, 35:19304-19318.
* Singh et al. (2021) Singh, H., Singh, R., Mhasawade, V., and Chunara, R. (2021). Fairness violations and mitigation under covariate shift. In _Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency_, pages 3-13.
* Sugiyama et al. (2007) Sugiyama, M., Krauledat, M., and Muller, K.-R. (2007). Covariate shift adaptation by importance weighted cross validation. _Journal of Machine Learning Research_, 8(5).
* Sukhukh et al. (2019)Tan, Z., Yeom, S., Fredrikson, M., and Talwalkar, A. (2020). Learning fair representations for kernel models. In _International Conference on Artificial Intelligence and Statistics_, pages 155-166. PMLR.
* Wang et al. (2021) Wang, J., Liu, Y., and Levy, C. (2021). Fair classification with group-dependent label noise. In _Proceedings of the 2021 ACM conference on fairness, accountability, and transparency_, pages 526-536.
* Wang et al. (2022) Wang, J., Wang, X. E., and Liu, Y. (2022). Understanding instance-level impact of fairness constraints. In _International Conference on Machine Learning_, pages 23114-23130. PMLR.
* Wang et al. (2020) Wang, S., Guo, W., Narasimhan, H., Cotter, A., Gupta, M., and Jordan, M. (2020). Robust optimization for fairness with noisy protected groups. _Advances in neural information processing systems_, 33:5190-5203.
* Willborn (1984) Willborn, S. L. (1984). The disparate impact model of discrimination: Theory and limits. _Am. UL Rev._, 34:799.
* Wu et al. (2022) Wu, S., Gong, M., Han, B., Liu, Y., and Liu, T. (2022). Fair classification with instance-dependent label noise. In _Conference on Causal Learning and Reasoning_, pages 927-943. PMLR.
* Yan et al. (2020) Yan, S., Kao, H.-t., and Ferrara, E. (2020). Fair class balancing: Enhancing model fairness without observing sensitive attributes. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 1715-1724.
* Yurochkin et al. (2019) Yurochkin, M., Bower, A., and Sun, Y. (2019). Training individually fair ml models with sensitive subspace robustness. _arXiv preprint arXiv:1907.00020_.
* Zafar et al. (2017) Zafar, M. B., Valera, I., Rogriguez, M. G., and Gummadi, K. P. (2017). Fairness constraints: Mechanisms for fair classification. In _Artificial Intelligence and Statistics_, pages 962-970. PMLR.
* Zhang et al. (2023) Zhang, Y., Zhou, F., Li, Z., Wang, Y., and Chen, F. (2023). Fair representation learning with unreliable labels. In _International Conference on Artificial Intelligence and Statistics_, pages 4655-4667. PMLR.