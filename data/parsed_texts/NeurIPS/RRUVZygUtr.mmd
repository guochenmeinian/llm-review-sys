# Spectral Invariant Learning for Dynamic Graphs

under Distribution Shifts

 Zeyang Zhang1, Xin Wang1, Ziwei Zhang1, Zhou Qin2,

Weigao Wen2, Hui Xue2, Haoyang Li1, Wenwu Zhu1

1Department of Computer Science and Technology, BNRist, Tsinghua University, 2Alibaba Group

zy-zhang20@mails.tsinghua.edu.cn, {xin_wang, zwzhang}@tsinghua.edu.cn,

{qinzhou.qinzhou, weigao.wen, hui.xueh}@alibaba-inc.com,

lihy18@mails.tsinghua.edu.cn, wwzhu@tsinghua.edu.cn

This work was done during the author's internship at Alibaba GroupCorresponding authors

###### Abstract

Dynamic graph neural networks (DyGNNs) currently struggle with handling distribution shifts that are inherent in dynamic graphs. Existing work on DyGNNs with out-of-distribution settings only focuses on the time domain, failing to handle cases involving distribution shifts in the spectral domain. In this paper, we discover that there exist cases with distribution shifts unobservable in the time domain while observable in the spectral domain, and propose to study distribution shifts on dynamic graphs in the spectral domain for the first time. However, this investigation poses two key challenges: i) it is non-trivial to capture different graph patterns that are driven by various frequency components entangled in the spectral domain; and ii) it remains unclear how to handle distribution shifts with the discovered spectral patterns. To address these challenges, we propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (**SILD**), which can handle distribution shifts on dynamic graphs by capturing and utilizing invariant and variant spectral patterns. Specifically, we first design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums, allowing the mixed dynamic graph patterns to be transformed into separate frequency components. We then develop a disentangled spectrum mask to filter graph dynamics from various frequency components and discover the invariant and variant spectral patterns. Finally, we propose invariant spectral filtering, which encourages the model to rely on invariant patterns for generalization under distribution shifts. Experimental results on synthetic and real-world dynamic graph datasets demonstrate the superiority of our method for both node classification and link prediction tasks under distribution shifts.

## 1 Introduction

Dynamic graph neural networks (DyGNNs) have achieved remarkable success in many predictive tasks over dynamic graphs [1; 2]. Existing DyGNNs exhibit limitations in handling distribution shifts, which naturally exist in dynamic graphs due to multiple uncontrollable factors [3; 4; 5; 6]. Existing work on out-of-distribution generalized DyGNNs focuses on handling distribution shifts in the time domain. For example, DIDA [7] utilizes dynamic graph attention to mask the graph trajectories to capture the invariant patterns on dynamic graphs, which assumes that in the time domain, the distribution shift is observable and the invariant and variant patterns can be easily disentangled.

However, there exist cases that the distribution shift is unobservable in the time domain while observable in the spectral domain, as shown in Figure 1. The shift in frequency components canbe clearly observed in the spectral domain, while these components are indistinguishable in the time domain. Moreover, in real-world applications, the observed dynamic graphs usually consist of multiple mixed graph structural and featural dynamics from various frequency components [8; 9; 10].

To address this problem, in this paper, we study the problem of handling distribution shifts on dynamic graphs in the spectral domain for the first time, which poses the following two key challenges: i) it is non-trivial to capture different graph patterns that are driven by various frequency components entangled in the spectral domain, and ii) it remains unclear how to handle distribution shifts with the discovered spectral patterns.

To tackle these challenges, we propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (**SILD3**). Our proposed **SILD** model can effectively handle distribution shifts on dynamic graphs by discovering and utilizing invariant and variant spectral patterns. Specifically, we first design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums so that the mixed graph dynamics can be transformed into separate frequency components. Then we develop a disentangled spectrum mask that leverages the amplitude and phase information of the ego-graph trajectory spectrums to obtain invariant and variant spectrum masks so that graph dynamics from various frequency components can be filtered. Finally, we propose an invariant spectral filtering that discovers the invariant and variant patterns via the disentangled spectrum masks, and minimize the variance of predictions with exposure to various variant patterns. As such, **SILD** is able to exploit invariant patterns to make predictions under distribution shifts. Experimental results on several synthetic and real-world datasets, including both node classification and link prediction tasks, demonstrate the superior performance of our **SILD** model compared to state-of-the-art baselines under distribution shifts. To summarize, we make the following contributions:

Footnote 3: The codes are available at Github.

* We propose to study distribution shifts on dynamic graphs in the spectral domain, to the best of our knowledge, for the first time.
* We propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (**SILD**), which can handle distribution shifts on dynamic graphs in the spectral domain.
* We employ DyGNN with Fourier transform to obtain the node spectrums, design a disentangled spectrum mask to obtain invariant and variant spectrum masks in the spectral domain, and propose the invariant spectral filtering mechanism so that **SILD** is able to handle distribution shifts.
* We conduct extensive experiments on several synthetic and real-world datasets, including both node classification and link prediction tasks, to demonstrate the superior performance of our method compared to state-of-the-art baselines under distribution shifts.

Figure 1: An illustration example: the graph dynamics from different frequency components are entangled in the temporal domain, while it is much easier to distinguish different frequency components by masking the spectrums in the spectral domain. In this case, the frequency components in the invariant spectrums determine the node labels, while the relationship between the variant spectrums and labels is not stable under distribution shifts.

## 2 Problem Formulation and Notations

Dynamic GraphsA dynamic graph can be represented as \(\mathcal{G}=(\{\mathcal{G}^{t}\}_{t=1}^{T})\), where \(T\) represents the total number of time stamps, and each \(\mathcal{G}^{t}=(\mathcal{V}^{t},\mathcal{E}^{t})\) corresponds to a graph snapshot at time stamp \(t\) with the node set \(\mathcal{V}^{t}\) and the edge set \(\mathcal{E}^{t}\). For simplicity, we also represent a graph snapshot as \(\mathcal{G}^{t}=(\mathbf{X}^{t},\mathbf{A}^{t})\), which includes the node feature matrix \(\mathbf{X}^{t}\) and the adjacency matrix \(\mathbf{A}^{t}\). We further denote a random variable of \(\mathcal{G}^{t}\) as \(\mathbf{G}^{t}\). The prediction task on dynamic graphs aims to utilize past graph snapshots to make predictions, _i.e._, \(p(\mathbf{Y}^{t}|\mathbf{G}^{1:t})\), where \(\mathbf{G}^{1:t}=\{\mathbf{G}^{1},\mathbf{G}^{2},\ldots,\mathbf{G}^{t}\}\) denotes the graph trajectory, and the label \(\mathbf{Y}^{t}\) represent the node properties or the links at time \(t+1\). For brevity, we take node-level prediction tasks as an example in this paper. Following [7], the distribution of graph trajectory can be factorized into ego-graph trajectories, such that \(p(\mathbf{Y}^{t}\mid\mathbf{G}^{1:t})=\prod_{v}p(\mathbf{y}_{v}^{t}\mid \mathbf{G}_{v}^{1:t})\).

Distribution Shifts on Dynamic GraphsThe common optimization objective for prediction tasks on dynamic graphs is to learn an optimal predictor with empirical risk minimization (ERM), _i.e._\(\min_{\theta}\mathbb{E}_{(y^{t},\mathcal{G}^{1:t}_{v})\sim p_{tr}(\mathbf{y}^{t}, \mathbf{G}^{1:t}_{v})}\mathcal{L}(f_{\theta}(\mathcal{G}^{1:t}_{v}),y^{t})\), where \(f_{\theta}\) is a learnable dynamic graph neural networks. Under distribution shifts, however, the optimal predictor trained with ERM and the training distribution may not generalize well to the test distribution, since the risk minimization objectives under two distributions are different due to \(p_{tr}(\mathbf{Y}^{t},\mathbf{G}^{1:t})\neq p_{te}(\mathbf{Y}^{t},\mathbf{G}^{ 1:t})\). The distribution shift on dynamic graphs is complex that may originate from temporal distribution shifts [11; 6; 12; 13; 14] as well as structural distribution shifts [15; 16; 17]. For example, trends or community structures can affect interaction patterns in co-author networks [18] and recommendation networks [19], _i.e._, the distribution of ego-graph trajectories may vary through time and structures.

Following out-of-distribution (OOD) generalization literature [7; 15; 11; 20; 21; 22], we make the following assumptions of distribution shifts on dynamic graphs:

**Assumption 1**: _For a given task, there exists a predictor \(f(\cdot)\), for samples (\(\mathcal{G}_{v}^{1:t},y^{t}\)) from any distribution, there exists an invariant pattern \(P_{I}^{t}(v)\) and a variant pattern \(P_{V}^{t}(v)\) such that the following conditions are satisfied: 1) the invariant patterns are sufficient to predict the labels, \(y_{v}^{t}=f(P_{I}^{t}(v))+\epsilon\), where \(\epsilon\) is a random noise, 2) the observed data is composed of invariant and variant patterns, \(P_{I}^{t}(v)=\mathcal{G}_{v}^{1:t}\backslash P_{V}^{t}(v)\), 3) the influence of the variant patterns on labels is shielded by the invariant patterns, \(\mathbf{y}_{v}^{t}\perp\mathbf{P}_{V}^{t}(v)\mid\mathbf{P}_{I}^{t}(v)\)._

In the next section, inspired by [16], we give a motivation example to provide some high-level intuition before going to our formal method.

## 3 Motivation Example

Here we introduce a toy dynamic graph example to motivate learning invariant patterns in the spectral domain. We assume that the invariant and variant patterns lie in the 1-hop neighbors, _i.e._, each node has an invariant subgraph and a variant subgraph. For simplicity, we focus on the number of neighbors, _i.e._, each node \(v\) has an invariant subgraph related degree \(\mathbf{d}_{v,1}\in\mathbb{R}^{T\times 1}\) and a variant subgraph related degree \(\mathbf{d}_{v,2}\in\mathbb{R}^{T\times 1}\). Only the former determines the node label, _i.e._, \(y_{v}=\mathbf{g}^{\top}\mathbf{d}_{v,1}\). Note that invariant and variant subgraphs are not observed in the data. We further assume a one-dimensional constant feature for each node, which is set as \(1\) without loss of generality.

For the model in the spatial-temporal domain, we adopt sum pooling as one-layer graph convolution, _i.e._, the message passing for each node and time is \(\mathbf{h}_{v}=\sum_{u\in\mathcal{N}_{v}}1=\mathbf{d}_{v,1}+\mathbf{d}_{v,2}\). We further adopt a mask \(\mathbf{m}\in\mathbb{R}^{T\times 1}\) to filter patterns in the temporal domain and make predictions by a linear classifier, _i.e._, \(\hat{y}_{v}=\mathbf{w}^{\top}(\mathbf{m}\odot\mathbf{h}_{v})\), where \(\mathbf{w}\in\mathbb{R}^{T\times 1}\) denotes the learnable parameters. Then, the empirical risk in the training dataset \(D_{tr}\) is \(R_{tr}(\mathbf{w})=\frac{1}{|D_{tr}|}\sum_{v\in D_{tr}}(\hat{y}_{v}-y_{v})^{2}\). We have the following proposition.

**Proposition 1**: _For any mask \(\mathbf{m}\in\mathbb{R}^{T\times 1}\), for the optimal classifier in the training data \(\mathbf{w}^{*}=\arg\min_{\mathbf{w}}R_{tr}(\mathbf{w})\), if \(||\mathbf{m}\odot\mathbf{w}^{*}||_{2}\neq 0\), there exist OOD nodes with unbounded error, i.e., \(\exists v\) s.t. \(\lim_{||\mathbf{d}_{v,2}||\rightarrow\infty}(\hat{y}_{v}-y_{v})^{2}=\infty\)._

The proposition 1 shows that a classifier trained with masks and empirical risk minimization has unbounded risks in testing data under distribution shifts as the classifier uses variant patterns to make predictions. Next, we show that under mild conditions, an invariant linear classifier in the spectraldomain can solve this problem. Denote \(\bm{\Phi}\in\mathbb{C}^{T\times T}\) as the Fourier bases, where \(\bm{\Phi}_{k,t}=\frac{1}{\sqrt{T}}e^{-j\frac{2\pi kt}{T}}\). Denote \(\mathbf{z}_{v}=\bm{\Phi}\sum_{u\in\mathcal{N}_{v}}1\) as the spectral representation after a linear message-passing. The prediction is \(\hat{y}_{v}=\mathbf{w}^{\text{H}}(\mathbf{m}\odot\mathbf{z}_{v})\), where \(\mathbf{m}\in\mathbb{C}^{T\times 1}\) is the mask to filter the spectral patterns, \(\mathbf{w}\in\mathbb{C}^{T\times 1}\) is a linear classifier, and \((\cdot)^{\text{H}}\) denotes Hermitian transpose. We have the following proposition.

**Proposition 2**: _If \(\left(\overline{\bm{\Phi}\mathbf{d}_{v,1}}\odot\bm{\Phi}\mathbf{d}_{v,1} \right)\odot\left(\overline{\bm{\Phi}\mathbf{d}_{v,2}}\odot\bm{\Phi}\mathbf{d}_ {v,2}\right)=\bm{0},\forall\mathbf{d}_{v,1},\mathbf{d}_{v,2}\), then \(\exists\mathbf{m}\in\mathbb{C}^{T\times 1}\) such that the optimal spectral classifier in the training data has bounded error, i.e., for \(\mathbf{w}^{*}=\arg\min_{\mathbf{w}}R_{tr}(\mathbf{w})\), \(\exists\epsilon>0\), \(\forall v,\lim_{\left\lvert\mathbf{d}_{v,2}\right\rvert\rightarrow\infty}( \hat{y}_{v}-y_{v})^{2}<\epsilon\)._

The proposition 2 shows that if the frequency bandwidths of invariant and variant patterns do not have any overlap, there exists a spectral mask such that a linear classifier trained with empirical risk minimization in the spectral domain will have bounded risk in any testing data distribution. This example motivates us to capture invariant and variant patterns in the spectral domain, which is not feasible in the spatial-temporal domain.

## 4 Method

In this section, we introduce our method named Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (**SILD**) to handle distribution shifts in dynamic graphs, including three modules, dynamic graph neural networks with Fourier transform, disentangled spectrum mask, and invariant spectral filtering. The framework of our method is shown in Figure 2.

### Dynamic Graph Neural Network with Spectral Transform

Dynamic Graph Trajectories ModelingEach node on the dynamic graph has its ego-graph trajectory evolving through time that may determine the node properties or the occurrence of future links. Following [23; 24; 25], we adopt a message-passing network for each graph snapshot to aggregate the neighborhood information at the current time, _i.e._,

\[\mathbf{m}_{u\to v}^{t}\leftarrow\mathrm{MSG}(\mathbf{h}_{u}^{t}, \mathbf{h}_{v}^{t}),\mathbf{h}_{v}^{t}\leftarrow\mathrm{AGG}(\{\mathbf{m}_{u \to v}^{t}\mid u\in\mathcal{N}^{t}(v)\},\mathbf{h}_{v}^{t})),\] (1)

where 'MSG' and 'AGG' denote message and aggregation functions, \(\mathbf{m}_{u\to v}^{t}\) is the message from node \(u\) to node \(v\), \(\mathbf{h}_{u}^{t}\) is the node embedding for node \(u\) at time \(t\), \(\mathcal{N}^{t}(v)=\{u\mid(u,v)\in\mathcal{E}^{t}\}\) is node \(v\)'s neighborhood at time \(t\). To model the high-order neighborhood information, we can stack multiple message-passing layers. In this way, the node embedding along time \(\{\mathbf{h}_{u}^{t}\}_{t=1}^{T}\) summarizes the evolution of node \(u\)'s ego-graph trajectories. We denote \(\mathbf{H}\in\mathbb{R}^{T\times N\times d}\) as the ego-graph trajectory signals for all nodes on the dynamic graph, where \(T\) denotes the total time length, \(N\) denotes the number of nodes and \(d\) denotes the hidden dimensionality.

Spectral TransformAs some patterns on dynamic graphs are unobservable in the time domain, while observable in the spectral domain, we transform the summarized ego-graph trajectory signals \(\mathbf{H}\) into the spectral domain via Fourier transform for each node and hidden dimension, _i.e._,

\[\bm{\Phi}_{k,t}=\frac{1}{\sqrt{T}}e^{-j\frac{2\pi kt}{T}},\mathbf{Z}=\bm{\Phi }\mathbf{H},\] (2)

where \(\bm{\Phi}\in\mathbb{C}^{K\times T}\) denotes the Fourier bases, \(K\) denotes the number of frequency components, and \(\mathbf{Z}\in\mathbb{C}^{K\times N\times d}\) denote the node embeddings along frequency components in the spectral domain, and \(\mathbf{Z}_{k,n,m}=\sum_{t=1}^{T}\bm{\Phi}_{k,t}\mathbf{H}_{t,n,m}\). By choosing the Fourier bases, our spectral transform has the following advantages: 1) we can use fast Fourier transform (FFT) [26] to accelerate the computation. The computation complexity of Eq. (2) can be reduced from \(O(NdT^{2})\) to \(O(NdTlogT)\). 2) Each basis has clear semantics, _e.g._, \(\mathbf{Z}_{k}\) denotes the node embeddings at the \(k\)-th frequency component in the spectral domain. In this way, we can observe how the nodes on the dynamic graph evolve in different frequency bandwidths. 3) Fourier transform is able to capture global and periodic patterns [27; 28], which are common in real-world dynamic graphs, _e.g._, the interactions on e-commerce networks may result from seasonal sales or product service cycles.

### Disentangled Spectrum Mask

To capture invariant patterns in the spectral domain, we propose to explicitly learn spectrum masks to disentangle the invariant and variant patterns. The embeddings in the spectral domain contain the amplitude information as well as the phase information for each node

\[\text{Amp}(\mathbf{Z})=\Big{(}\text{Imag}^{2}(\mathbf{Z})+\text{Real}^{2}( \mathbf{Z})\Big{)}^{\frac{1}{2}},\phi(\mathbf{Z})=\arctan\frac{\text{Imag}( \mathbf{Z})}{\text{Real}(\mathbf{Z})},\] (3)

where \(\text{Real}(\cdot)\) and \(\text{Imag}(\cdot)\) denote the real and imaginary part of the complex number, _i.e._, \(\mathbf{Z}=\text{Real}(\mathbf{Z})+j\text{Imag}(\mathbf{Z})\), \(j\) denotes the imaginary unit, \(\text{Amp}(\mathbf{Z})\in\mathbb{R}^{K\times N\times d}\) and \(\phi(\mathbf{Z})\in\mathbb{R}^{K\times N\times d}\) denote the amplitude and phase information respectively. For brevity, the tensor operators in Eq. (3) are all element-wise, _e.g._, \((\text{Imag}^{2}(\mathbf{Z}))_{i,j,k}=(\text{Imag}(\mathbf{Z}_{i,j,k}))^{2}\). Then, we obtain the spectrum masks by leveraging both the amplitude and phase information

\[\mathbf{M}=\text{MLP}(\text{Real}(\mathbf{Z})||\text{Imag}(\mathbf{Z})), \mathbf{M}_{I}=\text{sigmoid}(\mathbf{M}/\tau),\mathbf{M}_{V}=\text{sigmoid}(- \mathbf{M}/\tau),\] (4)

where MLP denotes multi-layer perceptrons, \(\tau\) is the temperature, \(\mathbf{M}_{I}\in[0,1]^{K}\) and \(\mathbf{M}_{V}\in[0,1]^{K}\) denote the spectrum mask for invariant and variant patterns, and \(||\) represents the concatenation of the embeddings. In this way, the invariant and variant masks have a negative relationship, and each node can have its own spectrum mask. As the phase information includes high-level semantics in the original signals [29; 30; 31; 32; 33], we keep the phase information unchanged to reduce harm in the fine-grained semantic information for the graph trajectories, and filter the spectrums by the learned disentangled masks in terms of amplitudes,

\[\mathbf{Z}_{I}=\Big{(}\mathbf{M}_{I}\odot\text{Amp}(\mathbf{Z})\Big{)}\odot( \cos\phi(\mathbf{Z})+j\sin\phi(\mathbf{Z})),\mathbf{Z}_{V}=\Big{(}\mathbf{M}_ {V}\odot\text{Amp}(\mathbf{Z})\Big{)}(\cos\phi(\mathbf{Z})+j\sin\phi(\mathbf{ Z})),\] (5)

where \(\mathbf{Z}_{I}\) and \(\mathbf{Z}_{V}\) denote the summarized invariant and variant patterns in the spectral domain. For node classification tasks, we can directly adopt the spectrums for the classifier to predict classes. For link prediction tasks, we can utilize inverse fast Fourier transform (IFFT) to transform the embeddings into the temporal domain for future link prediction

\[\mathbf{H}_{I}^{\prime}=\mathbf{\Phi}^{\text{H}}\mathbf{Z}_{I},\mathbf{H}_{V} ^{\prime}=\mathbf{\Phi}^{\text{H}}\mathbf{Z}_{V},\] (6)

Figure 2: The framework of our proposed method **SILD**. Given a dynamic graph evolving through time, the dynamic graph neural networks with spectral transform first obtain the ego-graph trajectory spectrums in the spectral domain. Then the disentangled spectrum mask leverages the amplitude and phase information of the ego-graph trajectory spectrums to obtain invariant and variant spectrum masks. Last, invariant spectral filtering discovers the invariant and variant patterns via the disentangled spectrum masks, and minimizes the variance of predictions with exposure to various variant patterns, to help the model exploit invariant patterns to make predictions under distribution shifts.

where \((\cdot)^{\mathbf{H}}\) is Hermitian transpose, \(\mathbf{H}_{I}^{\prime}\) and \(\mathbf{H}_{V}^{\prime}\) denote the filtered invariant and variant patterns that are transformed back into the temporal domain respectively.

### Invariant Spectral Filtering

Under distribution shifts, the variant patterns on dynamic graphs have varying relationships with labels, while the invariant patterns have sufficient predictive abilities with regard to labels. We propose invariant spectral filtering to capture the invariant and variant patterns in the spectral domain, and help the model focus on invariant patterns to make predictions, thus handling distribution shifts. We take node classification tasks for an example as follows.

Let \(\mathbf{Z}_{I}\in\mathbb{C}^{K\times N\times d}\) and \(\mathbf{Z}_{V}\in\mathbb{C}^{K\times N\times d}\) be the filtered invariant and variant spectrums in the spectral domain. Then we can utilize the invariant and variant node spectrums to calculate the task loss

\[\mathcal{L}_{I}=l(f_{I}(\mathbf{Z}_{I}),\mathbf{Y}),\mathcal{L}_{V}=l(f_{V}( \mathbf{Z}_{V}),\mathbf{Y}),\] (7)

where \(f_{I}(\cdot)\) and \(f_{V}(\cdot)\) are the classifiers for invariant and variant patterns respectively, \(\mathbf{Y}\) is the labels, and \(l\) is the loss function. The task loss is utilized to capture the patterns with the predictive abilities of labels. Recall in Assumption 1, the influence of variant patterns on labels is shielded given invariant patterns as the invariant patterns have sufficient predictive abilities w.r.t labels, and thus the model's predictions should not change when being exposed to different variant patterns and the original invariant patterns. Inspired by [15; 7], we calculate the invariance loss by

\[\mathcal{L}_{INV}=\text{Var}(\{\mathcal{L}_{m}\mid\tilde{\mathbf{z}}:\tilde{ \mathbf{z}}\in\mathcal{S}\}),\] (8)

where \(\mathcal{L}_{m}\mid\tilde{\mathbf{z}}\) denotes the mixed loss to measure the model's prediction ability with exposure to the specific variant pattern \(\tilde{\mathbf{z}}\in\mathbb{C}^{K\times d}\) that is sampled from a set of variant patterns \(\mathcal{S}\). We adopt all the node embeddings in \(\mathbf{Z}_{V}\) to construct the set of variant patterns \(\mathcal{S}\). Inspired by [34; 15], we calculate the mixed loss as

\[\mathcal{L}_{m}\mid\tilde{\mathbf{z}}=l(f_{I}(\mathbf{Z}_{I})\odot\sigma(f_{V }(\tilde{\mathbf{z}})),\mathbf{Y}),\] (9)

where \(\sigma\) denotes the sigmoid function. Then, the final training objective is

\[\min_{\theta,f_{I}}\mathcal{L}_{I}+\lambda\mathcal{L}_{INV}+\min_{f_{V}} \mathcal{L}_{V},\] (10)

where \(\theta\) is the parameters that encompass all the model parameters except the classifiers, \(\lambda\) is a hyper-parameter to balance the trade-off between the model's predictive ability and invariance properties. A larger \(\lambda\) encourages the model to capture patterns with better invariance under distribution shifts, with the potential risk of lower predictive ability during training, as the shortcuts brought by the variant patterns might be discarded in the training process. After training, we only adopt invariant patterns to make predictions in the inference stage. The overall algorithm for training on node classification datasets is summarized in Algo. 1.

```
0: Training epochs \(L\), sample number \(S\), hyperparameter \(\lambda\)
1:for\(l=1,\dots,L\)do
2: Obtain the node embeddings \(\mathbf{H}\) with snapshot-wise message passing as Eq. (1)
3: Transform the node embeddings into the spectral domain with FFT as Eq. (2)
4: Calculate the disentangled spectrum masks as Eq. (4)
5: Filter spectrums into invariant and variant patterns as Eq. (5)
6: Calculate the task loss as Eq. (7)
7: Sample \(S\) variant patterns from collections of \(\mathbf{Z}_{V}\) and calculate the invariance loss as Eq. (8)
8: Update the model according to Eq. (10)
9:endfor ```

**Algorithm 1** Training pipeline for **SILD** on node classification datasets

## 5 Experiments

In this section, we conduct extensive experiments to verify that our proposed method can handle distribution shifts on dynamic graphs by discovering and utilizing invariant patterns in the spectral domain. More details of the settings and other results can be found in Appendix.

**Baselines.** We adopt several representative dynamic GNNs and Out-of-Distribution(OOD) generalization methods as our baselines:* Dynamic GNNs: **GCRN**[35] is a representative dynamic GNN that first adopts a GCN[36] to obtain node embeddings and then a GRU [37] to model the network evolution. **EGCN**[25] adopts an LSTM [38] or GRU [37] to flexibly evolve the GCN [36] parameters through time. **DySAT**[24] aggregates neighborhood information at each graph snapshot using structural attention and models network dynamics with temporal self-attention.
* OOD generalization methods: **IRM**[20] aims at learning an invariant predictor which minimizes the empirical risks for all training domains. **GroupDRO**[39] puts more weight on training domains with larger errors to minimize the worst-group risks across training domains. **V-REx**[40] reduces the differences in the risks across training domains to reduce the model's sensitivity to distributional shifts. As these methods are not specifically designed for dynamic graphs, we adopt the best-performed dynamic GNNs as their backbones on each dataset.
* OOD generalization methods for dynamic graphs: **DIDA**[7] utilizes disentangled attention to capture invariant and variant patterns in the spatial-temporal domain, and conducts spatial-temporal intervention mechanism to let the model focus on invariant patterns to make predictions.

### Real-world Datasets

SettingsWe use 3 real-world dynamic graph datasets, including Collab [41; 7], Yelp [24; 7] and Aminer [42; 43]. Following [7], we adopt the challenging inductive future link prediction task on Collab and Yelp, where the model should exploit historical graphs to predict the occurrence of links in the next time step. To measure the model's performance under distribution shifts, the model is tested on another dynamic graph with different fields, which is unseen during training. For node classification, we adopt Aminer, a citation network, where nodes represent papers, and edges from \(u\) to \(v\) with timestamp \(t\) denote the paper \(u\) published at year \(t\) sites the paper \(v\). The task is to predict the venues of the papers. We train on papers published between 2001 - 2011, validate on

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Task** & **Link Prediction (AUC\%)** & \multicolumn{3}{c}{**Node Classification (ACC\%)**} \\
**Dataset** & **Collab** & **Yelp** & **Aminer15** & **Aminer16** & **Aminer17** \\ \hline GCRN & 69.72\(\pm\)0.45 & 54.68\(\pm\)7.59 & 47.96\(\pm\)1.12 & 51.33\(\pm\)0.62 & 42.93\(\pm\)0.71 \\ EGCN & 76.15\(\pm\)0.91 & 53.82\(\pm\)2.06 & 44.14\(\pm\)1.12 & 46.28\(\pm\)1.84 & 37.71\(\pm\)1.84 \\ DySAT & 76.59\(\pm\)0.20 & 66.09\(\pm\)1.42 & 48.41\(\pm\)0.81 & 49.76\(\pm\)0.96 & 42.39\(\pm\)0.62 \\ IRM & 75.42\(\pm\)0.87 & 56.02\(\pm\)16.08 & 48.44\(\pm\)0.13 & 50.18\(\pm\)0.73 & 42.40\(\pm\)0.27 \\ VREx & 76.24\(\pm\)0.77 & 66.41\(\pm\)1.87 & 48.70\(\pm\)0.73 & 49.24\(\pm\)0.27 & 42.59\(\pm\)0.37 \\ GroupDRO & 76.33\(\pm\)0.29 & 66.97\(\pm\)0.61 & 48.73\(\pm\)0.61 & 49.74\(\pm\)0.26 & 42.80\(\pm\)0.36 \\ DIDA & 81.87\(\pm\)0.40 & 75.92\(\pm\)0.90 & 50.34\(\pm\)0.81 & 51.43\(\pm\)0.27 & 44.69\(\pm\)0.06 \\ \hline
**SILD** & **84.09\(\pm\)0.16** & **78.65\(\pm\)2.22** & **52.35\(\pm\)1.04** & **54.11\(\pm\)0.62** & **45.54\(\pm\)1.19** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results of different methods on real-world link prediction and node classification datasets. The best results are in bold and the second-best results are underlined. The year in the Aminer dataset denotes the test split, _e.g._, ‘Aminer15’ denotes the average test accuracy in 2015.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Dataset** & \multicolumn{3}{c}{**Link-Synthetic (AUC\%)**} & \multicolumn{3}{c}{**Node-Synthetic (ACC\%)**} \\
**Shift** & **0.4** & **0.6** & **0.8** & **0.4** & **0.6** & **0.8** \\ \hline GCRN & 72.57\(\pm\)0.72 & 72.29\(\pm\)0.47 & 67.26\(\pm\)0.22 & 27.19\(\pm\)2.18 & 25.95\(\pm\)0.80 & 29.26\(\pm\)0.69 \\ EGCN & 69.00\(\pm\)0.53 & 62.70\(\pm\)1.14 & 60.13\(\pm\)0.89 & 24.01\(\pm\)2.29 & 22.75\(\pm\)0.96 & 24.98\(\pm\)1.32 \\ DySAT & 70.24\(\pm\)1.26 & 64.01\(\pm\)0.19 & 62.19\(\pm\)0.39 & 40.95\(\pm\)2.89 & 37.94\(\pm\)1.01 & 30.90\(\pm\)1.97 \\ IRM & 69.40\(\pm\)0.09 & 63.97\(\pm\)0.37 & 62.66\(\pm\)0.33 & 33.23\(\pm\)4.70 & 30.29\(\pm\)1.71 & 29.43\(\pm\)1.38 \\ VREx & 70.44\(\pm\)1.08 & 63.99\(\pm\)0.21 & 62.21\(\pm\)0.40 & 41.78\(\pm\)1.30 & 38.11\(\pm\)2.81 & 29.56\(\pm\)0.44 \\ GroupDRO & 70.30\(\pm\)1.23 & 64.05\(\pm\)0.21 & 62.13\(\pm\)0.35 & 41.35\(\pm\)2.19 & 35.74\(\pm\)3.93 & 31.03\(\pm\)1.24 \\ DIDA & 85.20\(\pm\)0.84 & 82.89\(\pm\)0.23 & 72.59\(\pm\)3.31 & 43.33\(\pm\)7.74 & 39.48\(\pm\)7.93 & 28.14\(\pm\)3.07 \\ \hline
**SILD** & **85.95\(\pm\)0.18** & **84.69\(\pm\)1.18** & **78.01\(\pm\)0.71** & **43.62\(\pm\)2.74** & **39.78\(\pm\)3.56** & **38.64\(\pm\)2.76** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Results of different methods on synthetic link prediction and node classification datasets. The best results are in bold and the second-best results are underlined. A larger ‘shift’ denotes a higher distribution shift level.

those published in 2012 - 2014, and test on those published since 2015. On this dataset, the model is tested to exploit the invariant patterns and make stable predictions under distribution shifts, where the patterns on the dynamic graph may vary in different years.

ResultsBased on the results in Table 1, we have the following observations: 1) _Under distribution shifts, the general OOD generalization baselines have limited improvements over the dynamic GNNs, e.g.,_ GroupDRO improves over DySAT with 0.9% in Yelp and 0.3% in Aminer15 respectively. A plausible reason is that they are not specially designed to handle distribution shifts on dynamic graphs, and may not consider the graph structural and temporal dynamics to capture invariant patterns. Another reason might be that they strongly rely on high-quality environment labels to capture invariant patterns, which are almost unavailable on real-world dynamic graphs. 2) _Our method can better handle distribution shifts than the baselines_. The datasets have strong distribution shifts, e.g., COVID-19 happens midway and has considerable influence on the consumer behavior on Yelp, and the citation patterns may shift with the outbreak of deep neural networks on Aminer. Nevertheless, our method **SILD** has significant improvements over the state-of-the-art OOD generalization baseline for dynamic graphs DIDA on all datasets, e.g., 2% on average for most datasets, which verifies that our method can better capture the invariant and variant patterns in the spectral domain, and thus handling distribution shifts on dynamic graphs.

### Synthetic Datasets

SettingsTo evaluate the model's generalization ability under distribution shifts, we conduct experiments on synthetic link prediction and node classification datasets, which are constructed by introducing manually-designed distribution shifts. For link prediction datasets, we follow [7] to generate additional varying features for each node and timestamps on the original dataset Collab, where these additional features are constructed with spurious correlations w.r.t the labels, the links in the next timestamps. The spurious correlation degree is determined by a shift level parameter. On this dataset, to have better generalization ability, the model should not rely on variant patterns that exploit the additional features with spurious correlations. For node classification, we briefly introduce the construction of the synthetic dataset as follows. We generate the dynamic graph with stochastic block model [44], where the link probability between nodes at each graph snapshot is determined by two frequency factors. The correlation of one of the factors with class labels is always 1, while the other factor has a variant relationship with labels, where the relationship is also controlled by a shift level parameter. The model should discover and focus on the invariant frequency factors whose relationship with labels is invariant under distribution shifts. For both datasets, we set the shift level parameters as 0.4, 0.6, 0.8 for training and validation splits, and 0 for test splits.

ResultsBased on the results in Table 2, we have the following observations: 1) _Our method can better handle distribution shifts than the baselines, especially under stronger distribution shifts_. **SILD** consistently outperforms DyGNN and general OOD generalization baselines by a significantly large margin, which can credit to our special design to handle distribution shifts on dynamic graphs in the spectral domain. Our method also has a significant improvement over the best-performed baseline under the strongest distribution shift, e.g., with absolute improvements of 5% in Link-Synthetic(0.8) and 7% in Node-Synthetic(0.8) respectively. 2) _Our method can exploit invariant patterns to consistently alleviate the harmful effects of variant patterns under different distribution shift levels_. As the distribution shift level increases, almost all methods decline in performance since the relationship between variant patterns and labels goes stronger, so that the variant patterns are much easier to be exploited by the model, misleading the training process. However, the performance drop of **SILD** is significantly lower than baselines, which demonstrates that our method can alleviate the harmful effects of variant patterns under distribution shifts by exploiting invariant patterns in the spectral domain.

### Ablation Studies

We conduct ablation studies to verify the effectiveness of the proposed disentangled spectrum mask and invariant spectral filtering in **SILD**. The ablated version 'SILD w/o I' removes invariant spectral filtering in **SILD** by setting \(\lambda=0\), and 'SILD w/o M' is trained without the disentangled spectrum masks. From Figure 3, we have the following observations. First, our proposed **SILD** outperforms all the variants as well as the best-performed baseline on all datasets, demonstrating the effectiveness of each component of our proposed method. Second, 'SILD w/o I' and 'SILD w/o M' drop drastically in performance on all datasets compared to the full version, which verifies that our proposed disentangled spectrum mask and spectral invariant learning can help the model to focus on invariant patterns to make predictions and significantly improve the performance under distribution shifts.

## 6 Related Works

Dynamic Graph Neural NetworksDynamic graphs ubiquitously exist in real-world applications [45; 46; 47; 48; 49; 50; 51; 52; 53] such as event forecasting, recommendation, etc. In comparison with static graphs [54; 55; 56; 57; 58; 59], dynamic graphs contain rich temporal information. Considerable research attention has been devoted to dynamic graph neural networks (DyGNNs) [1; 2; 60] to model the complex graph dynamics that include structures and features evolving through time. Some works adopt GNN to aggregate neighborhood information for each graph snapshot, and then utilize a sequence module to model the temporal information [61; 62; 63; 35; 24]. Some others utilize time-encoding techniques to encode the temporal links into time-aware embeddings and adopt a GNN or memory module [64; 65; 66; 67] to process structural information. Some other related works leverage spectral graph neural networks [68], global graph framelet convolution [69], and graph wavelets [70] to obtain better dynamic graph representations. However, distribution shifts remain largely unexplored in dynamic graph neural networks literature. The sole prior work DIDA [7] handles spatial-temporal distribution shifts on dynamic graphs in the temporal domain. To the best of our knowledge, this is the first study of handling distribution shifts on dynamic graphs in the spectral domain.

Out-of-Distribution GeneralizationA significant proportion of existing machine learning methodologies operate on the assumption that training and testing data are independent and identically distributed (i.i.d.). However, this assumption may not always hold true, especially in the context of complex real-world scenarios [71], and the uncontrollable distribution shifts between training and testing data distribution may lead to a significant decline in the model performance. Out-of-Distribution (OOD) generalization problem has recently drawn great attention in various areas [72; 71; 73]. Some works handle structural distribution shifts on static graphs [74; 15; 16; 75; 5; 76; 77; 78; 79; 80; 81] and temporal distribution shifts on time-series data [11; 12; 6; 13; 14; 82]. However, how to handle distribution shifts on dynamic graphs in the spectral domain remains unexplored.

Spectral Methods in Neural NetworksThe applications of spectral methods in neural networks have been broadly explored in many areas, including static graph data [83; 84; 85; 86; 87], time-series data [68; 88; 89; 90; 91], etc., for their advantages of modeling global patterns, powerful expressiveness and interpretability [92; 28]. Some work [93] proposes to reconstruct the image in the spectral domain to obtain robust image representations. Some work [29] proposes to augment the image data by perturbing the amplitude information in the spectral domain. Some work [94] proposes a multiwavelet-based method for compressing operator kernels. However, these methods are not applicable to dynamic graphs, not to mention the more complex scenarios under distribution shifts.

Figure 3: Results of ablation studies, where ‘w/o I’ removes invariant spectral filtering in **SILD**, ‘w/o M’ removes disentangled spectrum masks, and ‘Best baseline’ denotes the best-performed baseline on each dataset. The error bars report the standard deviations. (Best viewed in color)

Conclusion

In this paper, we propose a novel model named Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (**SILD**), which can handle distribution shifts on dynamic graphs in the spectral domain. We design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums. Then we propose a disentangled spectrum mask and invariant spectral filtering to discover the invariant and variant patterns in the spectral domain, and help the model rely on invariant spectral patterns to make predictions. Extensive experimental results on several synthetic and real-world datasets, including both node classification and link prediction tasks, demonstrate the superior performance of our method compared to state-of-the-art baselines under distribution shifts. One limitation is that in this paper we mainly focus on dynamic graphs in scenarios of discrete snapshots, and we leave extending our methods to continuous dynamic graphs for further explorations.

## Acknowledgements

This work was supported by the National Key Research and Development Program of China No. 2020AAA0106300, National Natural Science Foundation of China (No. 62222209, 62250008, 62102222, 62206149), Beijing National Research Center for Information Science and Technology under Grant No. BNR2023RC01003, BNR2023TD03006, China National Postdoctoral Program for Innovative Talents No. BX20220185, China Postdoctoral Science Foundation No. 2022M711813, and Beijing Key Lab of Networked Multimedia. All opinions, findings, conclusions, and recommendations in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.

## References

* [1] Joakim Skarding, Bogdan Gabrys, and Katarzyna Musial. Foundations and modeling of dynamic networks using dynamic graph neural networks: A survey. _IEEE Access_, 9:79143-79168, 2021.
* [2] Yuecai Zhu, Fuyuan Lyu, Chengming Hu, Xi Chen, and Xue Liu. Learnable encoder-decoder architecture for dynamic graph: A survey. _arXiv preprint arXiv:2203.10480_, 2022.
* [3] Stephen J Brown, William Goetzmann, Roger G Ibotson, and Stephen A Ross. Survivorship bias in performance studies. _The Review of Financial Studies_, 5(4):553-580, 1992.
* [4] Richard A Berk. An introduction to sample selection bias in sociological data. _American sociological review_, pages 386-398, 1983.
* [5] Qi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. Shift-robust gnns: Overcoming the limitations of localized graph training data. _Advances in Neural Information Processing Systems_, 34, 2021.
* [6] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series forecasting against distribution shift. In _International Conference on Learning Representations_, 2021.
* [7] Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Zhou Qin, and Wenwu Zhu. Dynamic graph neural networks under spatio-temporal distribution shift. In _Advances in Neural Information Processing Systems_, 2022.
* [8] Jayanta Mondal and Amol Deshpande. Managing large dynamic graphs efficiently. In _Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data_, pages 145-156, 2012.
* [9] Tuan Nhon Dang, Nick Pendar, and Angus Graeme Forbes. Timearcs: Visualizing fluctuations in dynamic networks. In _Computer Graphics Forum_, volume 35, pages 61-69. Wiley Online Library, 2016.
* [10] Jozef Barunik, Michael Ellington, et al. Dynamic networks in large financial and economic systems. _arXiv preprint arXiv:2007.07842_, 2020.
* [11] Jean-Christophe Gagnon-Audet, Kartik Ahuja, Mohammad-Javad Darvishi-Bayazi, Guillaume Dumas, and Irina Rish. Woods: Benchmarks for out-of-distribution generalization in time series tasks. _arXiv preprint arXiv:2203.09978_, 2022.
* [12] Yuntao Du, Jindong Wang, Wenjie Feng, Sinno Pan, Tao Qin, Renjun Xu, and Chongjun Wang. Adarnn: Adaptive learning and forecasting of time series. In _Proceedings of the 30th ACM International Conference on Information & Knowledge Management_, pages 402-411, 2021.

* [13] Praveen Venkateswaran, Vinod Muthusamy, Vatche Isahagian, and Nalini Venkatasubramanian. Environment agnostic invariant risk minimization for classification of sequential datasets. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 1615-1624, 2021.
* [14] Wang Lu, Jindong Wang, Yiqiang Chen, and Xinwei Sun. Diversify to generalize: Learning generalized representations for time series classification. _arXiv preprint arXiv:2209.07027_, 2021.
* [15] Ying-Xin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. _arXiv preprint arXiv:2201.12872_, 2022.
* [16] Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. Handling distribution shifts on graphs: An invariance perspective. _arXiv preprint arXiv:2202.02466_, 2022.
* [17] Mucong Ding, Kezhi Kong, Jiuhai Chen, John Kirchenbauer, Micah Goldblum, David Wipf, Furong Huang, and Tom Goldstein. A closer look at distribution shifts and out-of-distribution generalization on graphs. In _NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications_, 2021.
* [18] Tian Jin, Qiong Wu, Xuan Ou, and Jianjun Yu. Community detection and co-author recommendation in co-author networks. _International Journal of Machine Learning and Cybernetics_, 12(2):597-609, 2021.
* [19] Wenjie Wang, Xinyu Lin, Fuli Feng, Xiangnan He, Min Lin, and Tat-Seng Chua. Causal representation learning for out-of-distribution recommendation. In _Proceedings of the ACM Web Conference 2022_, pages 3562-3571, 2022.
* [20] Martin Arjovsky, Leon Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization. _arXiv preprint arXiv:1907.02893_, 2019.
* [21] Shiyu Chang, Yang Zhang, Mo Yu, and Tommi Jaakkola. Invariant rationalization. In _International Conference on Machine Learning_, pages 1448-1458. PMLR, 2020.
* [22] Kartik Ahuja, Karthikeyan Shanmugam, Kush Varshney, and Amit Dhurandhar. Invariant risk minimization games. In _International Conference on Machine Learning_, pages 145-155. PMLR, 2020.
* [23] Jiaxuan You, Tianyu Du, and Jure Leskovec. Roland: graph learning framework for dynamic graphs. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 2358-2366, 2022.
* [24] Aravind Sankar, Yanhong Wu, Liang Gou, Wei Zhang, and Hao Yang. Dysat: Deep neural representation learning on dynamic graphs via self-attention networks. In _Proceedings of the 13th International Conference on Web Search and Data Mining_, pages 519-527, 2020.
* [25] Aldo Pareja, Giacomo Domeniconi, Jie Chen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kanezashi, Tim Kaler, Tao Schard, and Charles Leiserson. Evolvegcn: Evolving graph convolutional networks for dynamic graphs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 5363-5370, 2020.
* [26] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series. _Mathematics of computation_, 19(90):297-301, 1965.
* [27] Ronald Newbold Bracewell and Ronald N Bracewell. _The Fourier transform and its applications_, volume 31999. McGraw-Hill New York, 1986.
* [28] Kun Yi, Qi Zhang, Shoujin Wang, Hui He, Guodong Long, and Zhendong Niu. Neural time series analysis with fourier transform: A survey. _arXiv preprint arXiv:2302.02173_, 2023.
* [29] Qinwei Xu, Ruipeng Zhang, Ya Zhang, Yanfeng Wang, and Qi Tian. A fourier-based framework for domain generalization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14383-14392, 2021.
* [30] Bruce C Hansen and Robert F Hess. Structural sparseness and spatial phase alignment in natural scenes. _JOSA A_, 24(7):1873-1885, 2007.
* [31] Alan V Oppenheim and Jae S Lim. The importance of phase in signals. _Proceedings of the IEEE_, 69(5):529-541, 1981.
* [32] Leon N Piotrowski and Fergus W Campbell. A demonstration of the visual importance and flexibility of spatial-frequency amplitude and phase. _Perception_, 11(3):337-346, 1982.

* [33] A Oppenheim, Jae Lim, Gary Kopec, and SC Pohlig. Phase in speech and pictures. In _ICASSP'79. IEEE International Conference on Acoustics, Speech, and Signal Processing_, volume 4, pages 632-637. IEEE, 1979.
* [34] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. Rubi: Reducing unimodal biases for visual question answering. _Advances in neural information processing systems_, 32, 2019.
* [35] Youngjoo Seo, Michael Defferrard, Pierre Vandergheynst, and Xavier Bresson. Structured sequence modeling with graph convolutional recurrent networks. In _International Conference on Neural Information Processing_, pages 362-373. Springer, 2018.
* [36] Thomas N Kipf and Max Welling. Variational graph auto-encoders. _arXiv preprint arXiv:1611.07308_, 2016.
* [37] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In _EMNLP_, 2014.
* [38] Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. _Neural computation_, 9(8):1735-1780, 1997.
* [39] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. _arXiv preprint arXiv:1911.08731_, 2019.
* [40] David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai Zhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). In _International Conference on Machine Learning_, pages 5815-5826. PMLR, 2021.
* [41] Jie Tang, Sen Wu, Jimeng Sun, and Hang Su. Cross-domain collaboration recommendation. In _KDD'2012_, 2012.
* [42] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and mining of academic social networks. In _Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 990-998, 2008.
* [43] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-june Paul Hsu, and Kuansan Wang. An overview of microsoft academic service (mas) and applications. In _Proceedings of the 24th international conference on world wide web_, pages 243-246. ACM, 2015.
* [44] Paul W. Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps. _Social Networks_, 5(2):109-137, 1983.
* [45] Lei Cai, Zhengzhang Chen, Chen Luo, Jiaping Gui, Jingchao Ni, Ding Li, and Haifeng Chen. Structural temporal graph neural networks for anomaly detection in dynamic graphs. In _Proceedings of the 30th ACM international conference on Information & Knowledge Management_, pages 3747-3756, 2021.
* [46] Songgaojun Deng, Huzefa Rangwala, and Yue Ning. Dynamic knowledge graph based multi-event forecasting. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 1585-1595, 2020.
* [47] Jiaxuan You, Yichen Wang, Aditya Pal, Pong Eksombatchai, Chuck Rosenburg, and Jure Leskovec. Hierarchical temporal convolutional networks for dynamic recommender systems. In _The world wide web conference_, pages 2236-2246, 2019.
* [48] Yanbang Wang, Pan Li, Chongyang Bai, and Jure Leskovec. Tedic: Neural modeling of behavioral patterns in dynamic social interaction networks. In _Proceedings of the Web Conference 2021_, pages 693-705, 2021.
* [49] Haoyang Li, Peng Cui, Chengxi Zang, Tianyang Zhang, Wenwu Zhu, and Yishi Lin. Fates of microscopic social ecosystems: Keep alive or dead? In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 668-676, 2019.
* [50] Jiapeng Wu, Meng Cao, Jackie Chi Kit Cheung, and William L Hamilton. Temp: Temporal message passing for temporal knowledge graph completion. _arXiv preprint arXiv:2010.03526_, 2020.
* [51] Zeyang Zhang, Ziwei Zhang, Xin Wang, Yijian Qin, Zhou Qin, and Wenwu Zhu. Dynamic heterogeneous graph attention neural architecture search. In _Thirty-Seventh AAAI Conference on Artificial Intelligence_, 2023.

* [52] Zeyang Zhang, Xingwang Li, Fei Teng, Ning Lin, Xueling Zhu, Xin Wang, and Wenwu Zhu. Out-of-distribution generalized dynamic graph neural network for human albumin prediction. In _IEEE International Conference on Medical Artificial Intelligence_, 2023.
* [53] Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, and Wenwu Zhu. Llm4dyg: Can large language models solve problems on dynamic graphs? _arXiv preprint_, 2023.
* [54] Ziwei Zhang, Peng Cui, and Wenwu Zhu. Deep learning on graphs: A survey. _IEEE Transactions on Knowledge and Data Engineering_, 34(1):249-270, 2020.
* [55] Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Guangyao Shen, Shiqi Shen, and Wenwu Zhu. Unsupervised graph neural architecture search with disentangled self-supervision. In _Advances in Neural Information Processing Systems_, 2023.
* [56] Chaoyu Guan, Ziwei Zhang, Haoyang Li, Heng Chang, Zeyang Zhang, Yijian Qin, Jiyan Jiang, Xin Wang, and Wenwu Zhu. Autogl: A library for automated graph learning. In _ICLR 2021 Workshop GTRL_, 2021.
* [57] Yijian Qin, Ziwei Zhang, Xin Wang, Zeyang Zhang, and Wenwu Zhu. Nas-bench-graph: Benchmarking graph neural architecture search. In _Thirty-Sixth Conference on Neural Information Processing Systems_, 2022.
* [58] Yijian Qin, Xin Wang, Zeyang Zhang, and Wenwu Zhu. Graph differentiable architecture search with structure learning. In _Thirty-Fifth Conference on Neural Information Processing Systems_, 2021.
* [59] Ziwei Zhang, Haoyang Li, Zeyang Zhang, Yijian Qin, Xin Wang, and Wenwu Zhu. Large graph models: A perspective. _arXiv preprint arXiv:2308.14522_, 2023.
* [60] Chao Chen, Haoyu Geng, Nianzu Yang, Xiaokang Yang, and Junchi Yan. Easydgl: Encode, train and interpret for continuous-time dynamic graph learning. _arXiv preprint arXiv:2303.12341_, 2023.
* [61] Menglin Yang, Min Zhou, Marcus Kalander, Zengfeng Huang, and Irwin King. Discrete-time temporal network embedding via implicit hierarchical learning in hyperbolic space. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 1975-1985, 2021.
* [62] Li Sun, Zhongbao Zhang, Jiawei Zhang, Feiyang Wang, Hao Peng, Sen Su, and Philip S Yu. Hyperbolic variational graph neural network for modeling dynamic graphs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 4375-4383, 2021.
* [63] Ehsan Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan, Nick Duffield, Mingyuan Zhou, and Xiaoning Qian. Variational graph recurrent neural networks. _Advances in neural information processing systems_, 32, 2019.
* [64] Yanbang Wang, Yen-Yu Chang, Yunyu Liu, Jure Leskovec, and Pan Li. Inductive representation learning in temporal networks via causal anonymous walks. _arXiv preprint arXiv:2101.05974_, 2021.
* [65] Weilin Cong, Yanhong Wu, Yuandong Tian, Mengting Gu, Yinglong Xia, Mehrdad Mahdavi, and Chuncheng Jason Chen. Dynamic graph representation learning via graph transformer networks. _arXiv preprint arXiv:2111.10447_, 2021.
* [66] Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. Inductive representation learning on temporal graphs. _arXiv preprint arXiv:2002.07962_, 2020.
* [67] Emanuele Rossi, Ben Chamberlain, Fabrizio Frasca, Davide Eynard, Federico Monti, and Michael Bronstein. Temporal graph networks for deep learning on dynamic graphs. _arXiv preprint arXiv:2006.10637_, 2020.
* [68] Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui Huang, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, et al. Spectral temporal graph neural network for multivariate time-series forecasting. _Advances in neural information processing systems_, 33:17766-17778, 2020.
* [69] Bingxin Zhou, Xinliang Liu, Yuehua Liu, Yunying Huang, Pietro Lio, and Yu Guang Wang. Well-conditioned spectral transforms for dynamic graph representation. In _Learning on Graphs Conference_, pages 12-1. PMLR, 2022.
* [70] Anson Bastos, Abhishek Nadgeri, Kuldeep Singh, Toyotaro Suzumura, and Manish Singh. Learnable spectral wavelets on dynamic graphs to capture global interactions. _arXiv preprint arXiv:2211.11979_, 2022.

* [71] Zheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards out-of-distribution generalization: A survey. _arXiv preprint arXiv:2108.13624_, 2021.
* [72] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A survey. _arXiv e-prints_, pages arXiv-2103, 2021.
* [73] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-of-distribution robustness via selective augmentation. In _Proceeding of the Thirty-ninth International Conference on Machine Learning_, 2022.
* [74] Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Out-of-distribution generalization on graphs: A survey. _arXiv preprint arXiv:2202.07987_, 2022.
* [75] Yongqiang Chen, Yonggang Zhang, Han Yang, Kaili Ma, Binghui Xie, Tongliang Liu, Bo Han, and James Cheng. Invariance principle meets out-of-distribution generalization on graphs. _arXiv preprint arXiv:2202.05441_, 2022.
* [76] Yijian Qin, Xin Wang, Ziwei Zhang, Pengtao Xie, and Wenwu Zhu. Graph neural architecture search under distribution shifts. In _International Conference on Machine Learning_, pages 18083-18095. PMLR, 2022.
* [77] Haoyang Li, Xin Wang, Ziwei Zhang, and Wenwu Zhu. Ood-gnn: Out-of-distribution generalized graph neural network. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* [78] Zeyang Zhang, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning to solve travelling salesman problem with hardness-adaptive curriculum. _arXiv preprint arXiv:2204.03236_, 2022.
* [79] Ziwei Zhang, Xin Wang, Zeyang Zhang, Peng Cui, and Wenwu Zhu. Revisiting transformation invariant geometric deep learning: Are initial representations all you need? _arXiv preprint arXiv:2112.12345_, 2021.
* [80] Shaohua Fan, Xiao Wang, Chuan Shi, Peng Cui, and Bai Wang. Generalizing graph neural networks on out-of-distribution graphs. _arXiv preprint arXiv:2111.10657_, 2021.
* [81] Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning invariant graph representations for out-of-distribution generalization. In _Thirty-Sixth Conference on Neural Information Processing Systems_, 2022.
* [82] Huaxiu Yao, Caroline Choi, Yoonho Lee, Pang Wei Koh, and Chelsea Finn. Wild-time: A benchmark of in-the-wild distribution shift over time. In _Proceedings of the Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2022.
* [83] Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. _Advances in neural information processing systems_, 29, 2016.
* [84] Bingbing Xu, Huawei Shen, Qi Cao, Yunqi Qiu, and Xueqi Cheng. Graph wavelet neural network. _arXiv preprint arXiv:1904.07785_, 2019.
* [85] Henry Kenlay, Dorina Thanou, and Xiaowen Dong. Interpretable stability bounds for spectral graph filters. In _International conference on machine learning_, pages 5388-5397. PMLR, 2021.
* [86] Xiyuan Wang and Muhan Zhang. How powerful are spectral graph neural networks. In _International Conference on Machine Learning_, pages 23341-23362. PMLR, 2022.
* [87] Deyu Bo, Chuan Shi, Lele Wang, and Renzijie Liao. Specformer: Spectral graph neural networks meet transformers. _arXiv preprint arXiv:2303.01028_, 2023.
* [88] Robert Salomone, Matias Quiroz, Robert Kohn, Mattias Villani, and Minh-Ngoc Tran. Spectral subsampling mcmc for stationary time series. In _International Conference on Machine Learning_, pages 8449-8458. PMLR, 2020.
* [89] Henning Lange, Steven L Brunton, and J Nathan Kutz. From fourier to koopman: Spectral methods for long-term time series prediction. _The Journal of Machine Learning Research_, 22(1):1881-1918, 2021.
* [90] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting. In _International Conference on Machine Learning_, pages 27268-27286. PMLR, 2022.
* [91] Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and Marinka Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. _Advances in Neural Information Processing Systems_, 35:3988-4003, 2022.

* [92] Deyu Bo, Xiao Wang, Yang Liu, Yuan Fang, Yawen Li, and Chuan Shi. A survey on spectral graph neural networks. _arXiv preprint arXiv:2302.05631_, 2023.
* [93] Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy. Focal frequency loss for image reconstruction and synthesis. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 13919-13929, 2021.
* [94] Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for differential equations. _Advances in neural information processing systems_, 34:24048-24062, 2021.