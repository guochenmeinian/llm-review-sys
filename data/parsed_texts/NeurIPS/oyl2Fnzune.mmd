# Uni-Med: A Unified Medical Generalist Foundation Model For Multi-Task Learning Via Connector-MoE

Xun Zhu1 Ying Hu1 Fanbin Mo2 Miao Li1,* Ji Wu1,3,4

1 Department of Electronic Engineering, Tsinghua University
2 Beijing University of Posts and Telecommunications 3 College of AI, Tsinghua University

4 Beijing National Research Center for Information Science and Technology

{zhu-x24, yinghu_y}@mails.tsinghua.edu.cn mofanbin@bupt.edu.cn

{miao-li, wuj_ee}@tsinghua.edu.cn *corresponding author

###### Abstract

Multi-modal large language models (MLLMs) have shown impressive capabilities as a general-purpose interface for various visual and linguistic tasks. However, building a unified MLLM for multi-task learning in the medical field remains a thorny challenge. To mitigate the tug-of-war problem of multi-modal multi-task optimization in MLLMs, recent advances primarily focus on improving the LLM components, while neglecting the connector that bridges the gap between modalities. In this paper, we introduce Uni-Med, a novel medical generalist foundation model which consists of a universal visual feature extraction module, a connector mixture-of-experts (CMoE) module, and an LLM. Benefiting from the proposed CMoE that leverages a well-designed router with a mixture of projection experts at the connector, Uni-Med achieves efficient solution to the tug-of-war problem and can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. To the best of our knowledge, Uni-Med is the first effort to tackle multi-task interference at the connector in MLLMs. Extensive ablation experiments validate the effectiveness of introducing CMoE under any configuration, with up to an average 8% performance gains. We further provide interpretation analysis of the tug-of-war problem from the perspective of gradient optimization and parameter statistics. Compared to previous state-of-the-art medical MLLMs, Uni-Med achieves competitive or superior evaluation metrics on diverse tasks. Code and resources are available at https://github.com/MSIIP/Uni-Med.

## 1 Introduction

Driven by the growth of datasets, the increase in model size, and advances in generative language foundation models [1, 23], multi-modal large language models (MLLMs) now offer unprecedented abilities as general-purpose interfaces. These advancements are spurring innovation across various visual and linguistic tasks [3, 15, 16]. While significant strides have been made in building a unified foundation model for natural scenery [3, 14, 17], the development of generalist medical artificial intelligence is still in its early stages [18].

The goal of a unified and generalist medical foundation model is to enable joint training on massive medical datasets. This model aims to handle multiple tasks and modalities within a single architecture with shared parameters [17, 19]. It seeks to eliminate the need for task-specific modules and further fine-tuning, thereby revolutionizing the traditional task-specificapproach to model development [Wu _et al._2023b; Tu _et al._2024]. However, existing open-source efforts have not yet fully achieved these ambitious goals.

A key challenge in creating a unified medical foundation model is the complexity of multi-modal, multi-task learning, often exacerbated by the tug-of-war problem [Hadsell _et al._2020]. Inherent task conflicts and data imbalances can cause interference during the simultaneous learning of different tasks. This problem is particularly acute in the medical field, where tasks and modalities are highly specialized and diverse. As a result, the performance of each task may degrade compared to task-specialized models [Yu _et al._2020; Zhu _et al._2022].

To mitigate the tug-of-war problem in multi-task learning, recent advances introduce the well-known Mixture-of-Experts (MoE) [Jacobs _et al._1991] into MLLMs. Figure 1 illustrates three distinct hypotheses and their corresponding architectural implementations for multi-task learning in MLLMs. The first "synergy hypothesis" suggests that all tasks benefit from a fully shared backbone comprising a visual encoder, connector, and language model, which is the standard architecture for MLLMs. The second "conflict hypothesis", proposes that each task requires its own specific adaptations, thereby preventing knowledge sharing among tasks. The third "conflict-synergy coexistence hypothesis", posits that all tasks share multi-task adaptations, which reduces interference and promotes more efficient knowledge sharing. However, current research [Zadouri _et al._2023; Gou _et al._2023; Liu _et al._2023b; Lin _et al._2024] mainly tailors the MoE approach to the language model components, overlooking the potential benefits of exploring and enhancing the connector in MLLMs. Furthermore, the optimization of the tug-of-war problem lacks a detailed, explainable analysis.

In this study, we first identify a tug-of-war problem in multi-task learning at the connector level within standard MLLM architectures. This issue indicates that different tasks may emphasize different types of features in multi-modal, multi-task scenarios. Consequently, a fully shared connector may fall short as it cannot accommodate the diverse modal features required by each task. Drawing inspiration from the successful application of MoE in LLMs, we introduce Connector-MoE (CMoE), a novel approach that employs a mixture of projection experts to align visual and language embedding spaces effectively, thus mitigating the tug-of-war problem. As a pioneering effort in constructing a unified generalist foundation model in the medical field, we present Uni-Med. This model comprises a universal visual feature extraction module, a CMoE module, and an LMM. Uni-Med demonstrates impressive performance across six distinct medical tasks, with minimal training computational overhead. It achieves joint training on 12 datasets on a single A800 in under 10 hours. The effectiveness and generalization of CMoE are underscored through ablation experiments. Additionally, an interpretable analysis reveals that Uni-Med provides a superior solution to the tug-of-war problem at the connector level. Overall, Uni-Med delivers competitive or even superior performance compared to open-source, state-of-the-art medical MLLMs on all test sets. Our contributions can be summarized as:

* We present Uni-Med, an open-source medical generalist foundation model with a unified interface and shared parameters, which can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification.
* We propose CMoE, a well-designed connector component for MLLMs, which significantly outperforms baselines under any configuration, with up to an average 8% performance gains. To our knowledge, Uni-Med is the first attempt to focus on the connector in MLLMs to mitigate the tug-of-war problem, which is critical but has always been overlooked.

Figure 1: Three hypotheses and corresponding architectural implementations for multi-task learning in MLLMs. (a) Synergy hypothesis. (b)-(c) Conflict hypothesis in LLM and connector, respectively. (d)-(e) Conflict-synergy coexist hypothesis in LLM and connector, respectively.

* Focusing on the question of how the tag-of-war problem is optimized, which has never been quantitatively discussed, we provide detailed interpretability analysis and instructive findings from the perspective of gradient optimization and parameter statistics.
* Uni-Med achieves competitive or superior performance compared to the open-source, state-of-the-art medical MLLMs on test set of diverse tasks and datasets, which demonstrates the huge potential of medical generalist foundation models.

## 2 Related work

Medical foundation modelsThe increasing availability of medical data, as well as advances in multi-modal LLM technologies, have paved the way for the emergence of medical foundational models. Med-Flanning (Moor _et al._, 2023) continues pre-training on paired and interleaved medical image-text data based on OpenFlamingo (Awadalla _et al._, 2023). LLaVA-Med (Li _et al._, 2024) curates a medical multi-modal instruction following dataset and fine-tunes LLaVA (Liu _et al._, 2024) with it. YrayGPT (Thawkar _et al._, 2023) can analyze and answer open-ended questions about chest X-rays. BiomedGPT (Zhang _et al._, 2023) is a multi-task foundation model pretrained on a diverse source of medical images, literature, and clinical notes. However, most of these efforts require further fine-tuning on task-specific data to support downstream applications. One step further, the generalist foundation model uses the same weight to excel at various tasks without fine-tuning. RadFM (Wu _et al._, 2023) is dedicated to build a generalist foundation model for radiology. Med-PaLM M (Tu _et al._, 2024) is directly trained in a unified framework to jointly handle many tasks, which is perhaps most similar to our effort, but it does not provide access for usage. In addition, recent studies (Wu _et al._, 2023; Yan _et al._, 2024; Xia _et al._, 2024) have suggested the the necessity for a more comprehensive and detailed evaluation of the capabilities of medical MLLMs.

MoE in multi-task learningMoE is originally considered to increase the model capacity (Riquelme _et al._, 2021; Fedus _et al._, 2022) and gains popularity in mitigating multi-task interference (Chen _et al._, 2023, 2024). It achieves this by utilizing a router to determine the token set handled by each expert, thus reducing interference between different types of samples. Recent studies have focused on combining MoE with LLM, such as MoE-LLaVA (Lin _et al._, 2024) and Mikrtal 8x7B (Jiang _et al._, 2024), or combining MoE with one of the representative parameter-efficient tuning techniques, i.e., LoRA (Hu _et al._, 2021), such as Octavius (Chen _et al._, 2023), MoCLE (Gou _et al._, 2023), MTLoRA (Agiza _et al._, 2024) and MOELoRA(Liu _et al._, 2024). However, neither of them introduces MoE into the connector component for MLLMs. Furthermore, there is a lack of clear and explicit interpretable analysis on how the multi-task interference is mitigated through the use of MoE.

Cross-modality connector in MLLThe connector between the multi-modal encoder and the LLM is critical in aligning multi-modal features (Song _et al._, 2023). One of the most popular paradigms is to map multi-modal features into a feature space that aligns with language, such as linear projection (Liu _et al._, 2024) and MLP projection (Liu _et al._, 2023; Chen _et al._, 2023). Another paradigm is to transform multi-modal features into multi-modal tokens that are consistent with the embedded representation space of LLM, such as cross-attention (Ye _et al._, 2023; Li _et al._, 2022; Ye _et al._, 2023), perceiver resampler (Alayrac _et al._, 2022; Peng _et al._, 2023) and Q-Former (Li _et al._, 2023; Zhu _et al._, 2023). However, existing paradigms use the same connector when processing the same modal data for different tasks, ignoring the imperative to acquire distinct alignment patterns tailored to the demands of each task.

## 3 Methodology

### Preliminaries

#### 3.1.1 Multi-task interference

To quantify the intricate tag-of-war problem in a unified foundation model, we provide interpretability from the perspective of gradient optimization and parameter statistics.

Perspective of gradient optimizationWhen optimizing the shared parameters \(\theta\) according to task \(j\), the change in the update direction of loss \(L_{i}\) for task \(i\) can be defined as (Zhu _et al._, 2022):\[\Delta_{j}L_{i}\left(x_{i}\right)\doteq\mathbb{E}_{x_{j}}\left(L_{i}\left(x_{i}; \theta\right)-L_{i}\left(x_{i};\theta-\lambda\frac{\nabla_{\theta}L_{j}\left(x_ {j}\right)}{\left\|\nabla_{\theta}L_{j}\left(x_{j}\right)\right\|_{2}}\right) \right)\approx\lambda\mathbb{E}_{x_{j}}\left(\frac{\nabla_{\theta}L_{j}\left(x_ {j}\right)^{T}}{\left\|\nabla_{\theta}L_{j}\left(x_{j}\right)\right\|_{2}} \nabla_{\theta}L_{i}\left(x_{i}\right)\right)\] (1)

where \(x_{i}\) and \(x_{j}\) are the sampled training batches of task \(i\) and \(j\), respectively. The interference of task \(j\) on task \(i\) in the update direction can be quantified as:

\[\mathcal{GD}_{i,j}=\mathbb{E}_{x_{i}}\left(\frac{\Delta_{j}L_{i}\left(x_{i} \right)}{\Delta_{i}L_{i}\left(x_{i}\right)}\right)\] (2)

The gradient magnitude similarity between task \(i\) and task \(j\) can be defined as:

\[\mathcal{GM}_{i,j}=\mathcal{GM}_{j,i}=\frac{2\mathbb{E}_{x_{i}}\left(\left\| \nabla_{\theta}L_{i}\left(x_{i}\right)\right\|_{2}\right)\mathbb{E}_{x_{j}} \left(\left\|\nabla_{\theta}L_{j}\left(x_{j}\right)\right\|_{2}\right)}{\left( \mathbb{E}_{x_{i}}\left(\left\|\nabla_{\theta}L_{i}\left(x_{i}\right)\right\|_ {2}\right)\right)^{2}+\left(\mathbb{E}_{x_{j}}\left(\left\|\nabla_{\theta}L_{ j}\left(x_{j}\right)\right\|_{2}\right)\right)^{2}}\] (3)

\(\mathcal{GM}_{i,j}\) goes to zero when the difference in gradient magnitudes is large, indicating that some task is dominant (Yu _et al._, 2020). For all \(T\) tasks, we can get \(\boldsymbol{\mathcal{GD}},\boldsymbol{\mathcal{GM}}\in\mathbf{R}^{T\times T}\). Then, we define the tug-of-war indexes for each task in multi-task learning through the function \(G\) as follows:

\[\text{tug-of-war indexes}=G(\boldsymbol{\mathcal{GD}},\boldsymbol{\mathcal{ GM}})=\left[\sum\nolimits_{j=1}^{T}\mathcal{GD}_{i,j}\cdot\mathcal{GM}_{i,j} \right]_{i=1}^{T}\] (4)

Perspective of parameter statisticsInspired by the Gradient Positive Sign Purity proposed by Chen _et al._ (2020), we define the statistics score of a single parameter in multi-task learning:

\[\text{statistics score}=\left|\frac{\sum_{i}^{T}\nabla_{\theta}L_{i}}{\left| \sum_{i}^{T}\left|\nabla_{\theta}L_{i}\right|\right|}\right|\] (5)

where \(\nabla_{\theta}L_{i}\) is the gradient for task \(i\). The range of the statistics score is [0, 1], and a value close to 1 indicates that this parameter suffers less gradient conflict during multi-task training. Upon collecting the statistics scores of all parameters, we can intuitively demonstrate and analyze the phenomenon of multi-task interference.

To be specific, we sample 100 batches for each datasets and record the gradients to calculate all of the above metrics. Figure 2 shows the dataset-level (more granular than task-level) multi-task interference of the synergy hypothesis model at the connector in the standard MLLM architecture.

#### 3.1.2 Mixture-of-Experts

A Mixture-of-Experts (MoE) contains a set of expert networks \(E_{1},E_{2},...,E_{N}\) along with a routing network \(R\). For each token \(x_{i}\) in the input sequence \(\boldsymbol{X}=\{x_{i}\}_{i=1}^{L}\), the output of MoE is the weighted sum of outputs from each expert, where the weight is calculated by the router:

\[y_{i}=\sum_{k=1}^{N}R(x_{i})_{k}\cdot E_{k}(x_{i})\] (6)

Figure 2: Dataset-level multi-task interference of the synergy hypothesis model at the connector in MLLMs. (a) Perspective of gradient direction \(\boldsymbol{\mathcal{GD}}\). (b) Perspective of gradient magnitude \(\boldsymbol{\mathcal{GM}}\).

The types of \(R\) can mainly be divided into: 1) Constant router, which assigns equal weight to each expert. 2) Hard router, which enforces one-to-one mapping between tasks and experts. 3) Sparse router, which selects Top-K experts with the maximum routing weight. 4) Soft router, which calculates the routing weights for each expert. For more details on the routing networks, see Appendix A.1.

### Model Architecture

With the primary goal of achieving a unified medical generalist foundation model and mitigating the tug-of-war problem of multi-task learning in mind, we design the overall architecture of Uni-Med as illustrated in Figure 3, which contains three components: a universal vision feature extraction module, a connector-MoE module and an LLM. Detailed descriptions are presented in the following sections.

#### 3.2.1 Visual feature extraction module

Taking one of the multi-modal medical images \(\bm{I}\in\mathbf{R}^{H\times W\times C}\) as input, the visual encoder \(\bm{V}_{en}\) extracts the image tokens \(\bm{f}_{v}\in\mathbf{R}^{N_{v}\times D_{v}}\) for image perception, where \(N_{v}=HW/P^{2}\) is the number of image patches and \(D_{v}\) is the hidden size of visual embeddings.

To alleviate the efficiency issues caused by prolonged visual input tokens during the training and inference, we scheme a resumpler with a compression rate \(\alpha\) for visual feature aggregation. Concretely, \(\alpha\) adjacent visual tokens are concatenated and projected into one single embedding. Thus we obtain aggregated image tokens \(\bm{f}_{v}^{ag}\in\mathbf{R}^{N_{v}/\alpha\times D_{v}\alpha}\) as follows:

\[\bm{f}_{v}^{ag}=\textit{resumpler}\left(\bm{V}_{en}\left(\bm{I}\right),\alpha\right)\] (7)

#### 3.2.2 Connector-MoE module

Aligning the visual space with the language embedding space of the large language model is a critical process, especially in the complex and diverse input of multi-task multi-modal medical image text pairs. Based on the conflict-synergy coexist hypothesis, we propose the Connector-MoE (CMoE) module, which aims to adaptively minimize task conflict and maximize task synergy at the connector. CMoE module has \(N\) projection experts \(E_{1},E_{2},...,E_{N}\), where each expert is a two-layer MLP, and a soft router \(R_{\textit{soft}}\) to control the contribution of each expert.

According to Figure 2, we find that: (1) Gradient optimization conflict is common and consistent at the task level. (2) Even for the same task, there are significant differences in conflict and synergy at dataset-level. To alleviate the above problems, we randomly initialize vision-level special task tokens \(\{\bm{f}_{t}^{sp}\}_{t\in T}\), where \(\bm{f}_{t}^{sp}\in\mathbf{R}^{D_{v}\alpha}\) and \(T\) is the set of tasks. \(R_{\textit{soft}}\) is a lightweight MLP designed to receive the concatenated inputs of \(\bm{f}_{v}^{ag}\) (token level) and \(\bm{f}_{t}^{sp}\) (task level), and calculate the routing weights \(\bm{w}_{\textit{soft}}\in\mathbf{R}^{D_{v}/\alpha\times N}\) of each expert for each image token, which can be formulated as:

Figure 3: Overall architecture of Uni-Med, which consists of a universal vision feature extraction module, a connector-MoE module and an LLM. Uni-Med can perform six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification.

\[\bm{w}_{\textit{soft}}\left(\bm{f}_{v}^{ag}\right)=\sigma\cdot R_{\textit{soft}} \left(\left[\bm{f}_{v}^{ag},Repeat\left(\bm{f}_{t}^{sp}\right)\right]\right)\] (8)

where \([,]\) denotes concatenation operation, \(\sigma\) is _SoftMax_ function. Then we can obtain aligned visual tokens \(\bm{f}_{v}^{align}\in\mathbf{R}^{N_{v}/\alpha\times D_{t}}\) through a weighted sum of all experts' output as follows:

\[\bm{f}_{v}^{align}=\sum_{k=1}^{N}\bm{w}_{\textit{soft},k}\cdot E_{k}(\bm{f}_{v }^{ag})\] (9)

where \(D_{t}\) is the hidden size of the language embedding space of the large language model and \(\bm{w}_{\textit{soft},k}\) denotes the routing weight of the \(k\)-th projection expert. We discuss and analyze the effects of router type, router strategy, and number of experts in Section 4.2.1.

#### 3.2.3 Large language model

Similar to the vision-level special task tokens, we assign the text-level special task identifiers for question answering (QA), visual question answering (VQA), report generation (RG), referring expression comprehension (REC), referring expression generation (REG) and image classification (CLS) as shown in Table 1, which can help reduce multi-task ambiguity [3]. The text prompt is designed as "<Img> < ImageFeature> </Img> [Task Identifier] Instruction", which merges the converted image features with the textual instructions. See details about our multi-task instruction template in Appendix C.

After word embedding, we can obtain textual tokens \(\bm{f}_{t}\in\mathbf{R}^{N_{t}\times D_{t}}\), where \(N_{t}\) denotes the number of textual tokens. LLM generates the response \(\bm{O}=\{O_{i}\}_{i=1}^{L}\) conditioned on the aligned visual tokens \(\bm{f}_{v}^{align}\) and textual tokens \(\bm{f}_{t}\) inputs in an autoregressive manner, which can be formulated as:

\[p\left(\mathbf{O}_{t}\mid\bm{f}_{v}^{align},\bm{f}_{t}\right)=\prod_{i=1}^{L }p\left(O_{i}\mid\bm{f}_{v}^{align},\bm{f}_{t},O_{<i}\right)\] (10)

where \(L\) is the length of output tokens. We use low-rank adaption (LoRA) [11] for efficient LLM fine-tuning, which is applied to all the linear layers.

## 4 Experiments

### Experiment settings

Tasks and datasetsText-only data is collected from MedQA [11] and PubMedQA [11] for the task of QA. Image-text pairs are collected from Path-VQA [12] and Slake-VQA [11] for the task of VQA, MIMIC-CXR [10] and MPx-Single [12] for the task of RG, MedMNIST v2 [23] for the task of CLS. For tasks such as REG and REC that require representation of spatial locations, we use the bounding boxes of the format "<\(X_{min}\)><\(Y_{min}\)><\(X_{max}\)><\(Y_{max}\)>", which denotes the coordinates of objects. Then, we respectively process datasets Slake-VQA [11] and SA-Med2D-20M [24] to get datasets Slake-REC, Slake-REG, SA-Med2D-REC, and SA-Med2D-REG. For a detailed description, processing and splitting of all datasets, see Appendix B.

Implementation detailsWe adapt the open-sourced ViT-G/14 from EVA-CLIP [13] and LLaMA2-Chat (7B) [25] as our visual backbone and LLM, respectively. During the training process, each task is assigned a sample rate that is calculated in proportion to the respective task's data volume. The visual backbone remains frozen with an input image resolution of 224*224 and the LLM is fine-tuned through LoRA [11] with the rank of 8. The compression rate \(\alpha\)=4 and the number of projection experts \(N\)=5. Uni-Med only requires one-stage training on a NVIDIA A800-SXM4-80GB GPU, with the first 10k iterations to warm-up and a total

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Task**} & Question & Visual & Report & Referring & Referring & Image \\  & Answering & Question & Generation & Expression & Expression & Generation & Classification \\ \hline
**Identifier** & [qz] & [vzq] & [caption] & [refer] & [identify] & [cls] \\ \hline \hline \end{tabular}
\end{table}
Table 1: Text-level special task identifiers for different tasks.

of 100k iterations with a batch size of 4, which lasts roughly 10 hours. The peak learning rate is set to 1e-6 and it decays to 1e-7 following the cosine strategy. We use AdamW [Loshchilov and Hutter, 2017] optimizer with \(\beta_{1}\)=0.9, \(\beta_{2}\)=0.95 and weight decay of 0.05.

Evaluation metricsFor ablation studies, we report BLEU-1 for the task of VQA, REG, and RG, IoU for the task of REC, Accuracy for the task of CLS. In addition, we use \(\Delta=\frac{1}{3}\sum_{i=1}^{S}\left(M_{m,i}-M_{b,i}\right)/M_{b,i}\times 100\%\) to evaluate the performance gains, where \(M_{m,i}\) and \(M_{b,i}\) are the metrics of our model and baseline model, \(S\) can be the number of datasets or tasks. For the overall comparison between models, we report more metrics such as F1, ROUGE, METEOR, RadGraph F1 and RadCliQ [Yu _et al._, 2023]. See details at Appendix D.1.

### Ablation study

#### 4.2.1 Ablation on module design

Connector designTaking the connector of a two-layer MLP as baseline setup, we first discuss the performance of different multi-task learning hypothesis. In Table 2 (a), connectors based on conflict-synergy coexist hypothesis (CMoE with sparse / soft router) show a more holistic improvement trend in multi-task learning compared to connectors based on the conflict hypothesis (CMoE with

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Connector**} & \multirow{2}{*}{\begin{tabular}{c} **Router** \\ **Strategy** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **VQA** \\ BLEU-1 \\ \end{tabular} } & \multirow{2}{*}{\(\Delta\) (\(\uparrow\))} & \multirow{2}{*}{\begin{tabular}{c} **REC** \\ IoU \\ \end{tabular} } & \multirow{2}{*}{\(\Delta\) (\(\uparrow\))} & \multirow{2}{*}{\begin{tabular}{c} **REG** \\ BLEU-1 \\ \end{tabular} } & \multirow{2}{*}{\(\Delta\) (\(\uparrow\))} & \multirow{2}{*}{\begin{tabular}{c} **BG** \\ BLEU-1 \\ \end{tabular} } & \multirow{2}{*}{\(\Delta\) (\(\uparrow\))} & \multirow{2}{*}{\begin{tabular}{c} **CLS** \\ \end{tabular} } & \multirow{2}{*}{\(\Delta\) (\(\uparrow\))} & \multirow{2}{*}{\begin{tabular}{c} **Class** \\ \end{tabular} } & \multirow{2}{*}{\(\Delta\) (\(\uparrow\))} & \multirow{2}{*}{\begin{tabular}{c} **Total** \\ \(\Delta\) (\(\uparrow\)) \\ \end{tabular} } \\ \hline \multicolumn{11}{c}{**(a) Connector design**} \\ \hline \multirow{11}{*}{
\begin{tabular}{c} Limer \\ MLP \\ \end{tabular} } & - & - & 77.90 / 56.27 & -1.44\% & 28.44 / 11.59 & -23.98 / 74.98 / 55.61 & -2.18 / 13.00 / 15.58 & -11.69\% & 72.47 / 69.39 & -5.44 / 8.9 \\  & - & - & 79.81 / 56.48 & 35.18 / 16.26 & 74.54 / 55.84 & 18.55 / 15.50 & 76.26 / 73.64 & - & - \\  & Constant & - & 82.74 / 53.23 & 2.66 / 3.94 / 15.49 & -15.49 / 73.58 / 85.61 & -4.65 / 23.16 / 15.88 & 75.91 / 76.50 & 1.78 / 2.76 \\  & Hard & 81.85 / **55.09** & 3.60 / 10.19 / 23.17 & 70.90 / 58.04 & -2.99 / 15.79 / 15.94 & **3.15** / **15.88** / 16.5 & **0.05** \\  & Sparse & Token & 80.68 / 55.02 & 1.09 / 37.01 / 18.41 & 9.36 / 76.86 / 60.08 & 3.09 / 24.02 / 15.73 & 15.56 / 3.74 / 74.93 & -1.09 / 5.66 \\  & & 81.79 / 57.29 & 3.55 / 15.79 & 7.24 / 74.43 / 61.82 & **2.46** / 15.61 & **2.12** / 76.56 / 27.11 & 2.69 / 6.78 \\  & Soft & Task & **82.51** / 57.43 & 2.35 / **33.38** / 16.98 / 15.08 & **77.48** / 60.67 & 45.4 / 32.53 / 15.89 & 14.27hard router) and synergy hypothesis (linear, MLP, CMoE with constant router). Though the hard router has a obvious lead on the CLS task, implying that the CLS task is better suited to a separate connector to avoid conflicts with other tasks. The soft router achieves the best multi-task performance, indicating that it not only alleviates conflicts between tasks, but also promotes collaboration between tasks. We then discuss three types of router strategy. The strategy of combining token-level with task-level information is superior to using each information separately, indicating the effectiveness for considering the tug-of-war problem from both token and task level.

Resampler designWe explore whether aggregating visual features through resampler has unfavorable effects in Table 2 (b). Despite an increase in compression rate \(\alpha\) from 1 to 4, the performance of models utilizing projection aggregation is improved. While the performance of average pooling and max pooling approaches is not satisfactory, especially the latter has severe performance degradation, which may be attributed to the excessive loss of feature information. This phenomenon shows that appropriate visual feature compression can bring efficiency to the training process without losing or even improving performance.

Number of projection expertsThe number of projection experts \(N\) is one of the most significant hyperparameters, which is closely related to the number of tasks and modalities that the CMoE module can accommodate. It is a challenging study as the complexity of the scenario can end up overfitting to simpler tasks and modalities or underfitting complex ones. As shown in Table 2 (c), increasing the number of experts \(N\), namely an augmentation in parameters, still brings performance gains on some datasets, but the average gain tends to stabilize across all tasks and datasets. Therefore, CMoE with 5 projection experts is sufficient to handle the tug-of-war problem in the existing medical multi-task learning scenarios and training configuration. A higher value of \(N\) does not bring the desired further improvement in total \(\Delta\).

#### 4.2.2 Ablation on module generalization

We demonstrate the generalization capability of the CMoE module in any configuration, especially when the key hyperparameters and strategies for LLM fine-tuning change. We first focus on the rank of LoRA, which directly determines the LLM capacity, i.e., trainable parameters. Our observations in Table 2 (d) reveal that CMoE with soft router can steadily improve multi-task performance when LoRA rank increases from 4 to 64. In Table 2 (e), we introduce MoE to LoRA, namely LoRA-MoE, which is considered a favorable parameter-efficient tuning solution for multi-task applications (Liu _et al._, 2023b; Chen _et al._, 2024). See details of LoRA-MoE at Appendix A.2. We find that separate LoRA-MoE results in significant performance improvement in 3 tasks while degradation in 2 tasks, indicating that it does not achieve the efficient solution to the tug-of-war problem. After combining CMoE with soft router, we achieve a balance of consistent performance gains, further demonstrating the necessity and effectiveness of mitigating the tug-of-war problem at the connector level in MLLMs.

### Interpretation

We conduct interpretation analysis of the tug-of-war problem based on methods mentioned in Section 3.1.1. Specifically, we focus on the changes in the connector using CMoE compared to MLP and show how the tug-of-war problem is optimized: (1) From the perspective of gradient optimization, we use maximum normalization to make the tug-of-war indexes comparable under different architectures. CMoE results in a more consistent tug-of-war indexes, i.e. higher mean and smaller standard deviation, among different tasks or datasets, implying each individual gets a more balanced optimization, as shown in Figure 4 (a). (2) From the perspective of parameter statistics, we discrete the statistics scores into ten intervals and count the ratio of all parameters at connector by interval. CMoE results in an increase in the proportion of high-value intervals in Figure 4 (b). We show the routing weights of projection experts after the warm-up stage and the final model in Figure 4 (c). CMoE adaptively learns different patterns of routing weights for different tasks.

To better reflect the coexistence of conflict and synergy among tasks, as well as the critical role played by the connector, we visualize the distribution of visual features before and after passing through the connector using the t-SNE method (Van der Maaten and Hinton, 2008). From the perspective of multi-task learning, we randomly select 200 samples from each task. It can be observed that CMoE promotes the optimization of the tug of war problem when aligning the visual space with the textual space of the LLM in Figure 5. Specifically, visual features of the same task are more 

[MISSING_PAGE_EMPTY:9]

### Overall comparison

To demonstrate the model capabilities of Uni-Med on multi-task learning, four open source and state-of-the-art medical MLLMs including Med-Flamingo [13], RadFM [23], LLAVA-Med [14], and XrayGPT [21] are used for performance comparison in Table 3. Any method of fine-tuning will inevitably lead to changes in the initial capability of the model. Therefore, we use readily available model checkpoints for testing, following the prompt template requirements of different models. Under this comparison strategy, if the training datasets of a model and Uni-Med intersect and strictly follow the official partition, it is fair and comparable to Uni-Med on these datasets. Specifically, LLAVA-MED provides dataset-specific fine-tuning checkpoints on Slake-VQA and Path-VQA separately. XrayGPT focuses on the task of report generation and utilizes MIMIC-CXR as training dataset. RadFM provides a model checkpoint for joint fine-tuning on Slake-VQA, MIMIC-CXR and MPx-Single. However, we do not list performance of RadFM on MPx-Single as we have identified the issue of data leakage, see Appendix D.2.

The results in Table 3 show that our Uni-Med achieves leading and competitive evaluation metrics across all tasks, which has the following prominent advantages: (1) Uni-Med is able to handle a greater variety of medical tasks, which is attributed to multi-task learning during training process. Due to the fact that the above MLLMs do not support input and output in coordinate form, we report the performance of Uni-Med on REC and REG tasks at Appendix D.5. Based on the different input and output forms supported by each model, we have also listed the zero-shot results in Table 3 for reference only. (2) Uni-Med achieves better results through joint training fine-tuning rather than dataset-specific fine-tuning like LLAVA-Med, which benefits from efficient optimization of the tug-of-war problem. In addition to directly compare the capability of existing models, we take LLAVA-Med as an example to compare the capability of model architectures in Appendix D.6.

## 5 Conclusion

In this paper, we present a novel open-source medical generalist foundation model Uni-Med, which can handle six different medical tasks including question answering, visual question answering, report generation, referring expression comprehension, referring expression generation and image classification. Benefiting from the proposed CMoE, which combines MoE with the connector, Uni-Med achieves efficient solution to the tug-of-war problem in multi-task learning. Uni-Med not only achieves competitive or superior performance compared to the open-source state-of-the-art medical MLLMs, but also provides interpretability analysis from the perspective of gradient optimization and parameter statistics on how the tug-of-war problem is optimized. We hope Uni-Med can greatly promote the development of medical generalist foundation models and inspire more research toward generalist medical artificial intelligence.

## 6 Limitations

While Uni-Med has demonstrated strong potential as a unified and generalist medical foundation model, it still exhibits several limitations: (1) Limitations in handling genuine 3D medical image inputs. Most commonly used medical image are in 3D. Same as most medical MLLMs, we process 3D images into 2D slices as input, resulting in significant information loss. (2) The potential of performance gains in more complex multi-modal and multi-task learning scenarios has not yet been explored. Uni-Med use 12 datasets of 6 medical tasks, with a total data volume of 140k. (3) The potential of performance gains in different LLM backbones has not yet been explored. Uni-Med utilizes LLAMA2-7B. (4) Deeper theoretical analysis of tug of war problem remains to be explored. We attempt to combine the existing methods to analyze it from the perspective gradient optimization and parameter statistics. (5) Potential negative societal impacts. We cannot prevent potential malicious or unintended uses, such as generating fake profiles or wrong medical diagnoses, and provide necessary safeguards.

## Acknowledgements

The work is supported by Noncommunicable Chronic Diseases-National Science and Technology Major Project (2023ZD0506501) and National Key R&D Program of China (2021ZD0113404).

## References

* Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* Agiza et al. (2024) Ahmed Agiza, Marina Nesseem, and Sherief Reda. Mtlora: A low-rank adaptation approach for efficient multi-task learning. _arXiv preprint arXiv:2403.20320_, 2024.
* Alayrac et al. (2022) Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. _Advances in neural information processing systems_, 35:23716-23736, 2022.
* Awadalla et al. (2023) Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. Openflamingo: An open-source framework for training large autoregressive vision-language models. _arXiv preprint arXiv:2308.01390_, 2023.
* Bilic et al. (2023) Patrick Bilic, Patrick Christ, Hongwei Bran Li, Eugene Vorontsov, Avi Ben-Cohen, Georgios Kaissis, Adi Szeskin, Colin Jacobs, Gabriel Erfain Humpire Mamani, Gabriel Chartrand, et al. The liver tumor segmentation benchmark (lits). _Medical Image Analysis_, 84:102680, 2023.
* Chen et al. (2020) Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign dropout. _Advances in Neural Information Processing Systems_, 33:2039-2050, 2020.
* Chen et al. (2022) Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey E Hinton. A unified sequence interface for vision tasks. _Advances in Neural Information Processing Systems_, 35:31333-31346, 2022.
* Chen et al. (2023) Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. _arXiv preprint arXiv:2305.04160_, 2023.
* Chen et al. (2023) Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: large language model as a unified interface for vision-language multi-task learning. _arXiv preprint arXiv:2310.09478_, 2023.
* Chen et al. (2023) Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. _arXiv preprint arXiv:2311.12793_, 2023.
* Chen et al. (2023) Zeren Chen, Ziqin Wang, Zhen Wang, Huayang Liu, Zhenfei Yin, Si Liu, Lu Sheng, Wanli Ouyang, Yu Qiao, and Jing Shao. Octavius: Mitigating task interference in mllms via moe. _arXiv preprint arXiv:2311.02684_, 2023.
* Chen et al. (2023) Zitian Chen, Yikang Shen, Mingyu Ding, Zhenfang Chen, Henghuang Zhao, Erik G Learned-Miller, and Chuang Gan. Mod-squad: Designing mixtures of experts as modular multi-task learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11828-11837, 2023.
* Chen et al. (2024) Shaoxiang Chen, Zequn Jie, and Lin Ma. Llava-mole: Sparse mixture of lora experts for mitigating data conflicts in instruction finetuning mllms. _arXiv preprint arXiv:2401.16160_, 2024.
* Codella et al. (2018) Noel Codella, Veronica Rotemberg, Philipp Tschandl, M Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic). _arXiv preprint arXiv:1902.03368_, 2019.
* Fang et al. (2023) Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19358-19369, 2023.
* Fedus et al. (2022) William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. _Journal of Machine Learning Research_, 23(120):1-39, 2022.
* Gou et al. (2023) Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James T Kwok, and Yu Zhang. Mixture of cluster-conditional lora experts for vision-language instruction tuning. _arXiv preprint arXiv:2312.12379_, 2023.
* Hadsell et al. (2020) Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu. Embracing change: Continual learning in deep neural networks. _Trends in cognitive sciences_, 24(12):1028-1040, 2020.
* Huang et al. (2020)Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. _arXiv preprint arXiv:2003.10286_, 2020.
* Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* Jacobs et al. [1991] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of local experts. _Neural computation_, 3(1):79-87, 1991.
* Jain et al. [2021] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew P Lungren, Andrew Y Ng, et al. Radgraph: Extracting clinical entities and relations from radiology reports. _arXiv preprint arXiv:2106.14463_, 2021.
* Jiang et al. [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. _arXiv preprint arXiv:2401.04088_, 2024.
* Jin et al. [2019] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W Cohen, and Xinghua Lu. Pubmedqa: A dataset for biomedical research question answering. _arXiv preprint arXiv:1909.06146_, 2019.
* Jin et al. [2021] Di Jin, Eileen Pan, Nassim Oufatolde, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? a large-scale open domain question answering dataset from medical exams. _Applied Sciences_, 11(14):6421, 2021.
* Johnson et al. [2019] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng, Roger G Mark, and Steven Horng. Mimic-cxr, a de-identified publicly available database of chest radiographs with free-text reports. _Scientific data_, 6(1):317, 2019.
* Li et al. [2022] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International conference on machine learning_, pages 12888-12900. PMLR, 2022.
* Li et al. [2023] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _International conference on machine learning_, pages 19730-19742. PMLR, 2023.
* Li et al. [2024] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. _Advances in Neural Information Processing Systems_, 36, 2024.
* Lin et al. [2024] Bin Lin, Zhenyu Tang, Yang Ye, Jiaxi Cui, Bin Zhu, Peng Jin, Junwu Zhang, Munan Ning, and Li Yuan. Moe-llava: Mixture of experts for large vision-language models. _arXiv preprint arXiv:2401.15947_, 2024.
* Liu et al. [2021] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering. In _2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)_, pages 1650-1654. IEEE, 2021.
* Liu et al. [2023] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. _arXiv preprint arXiv:2310.03744_, 2023.
* Liu et al. [2023] Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. Moelora: An moe-based parameter efficient fine-tuning method for multi-task medical applications. _arXiv preprint arXiv:2310.18339_, 2023.
* Liu et al. [2024] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. _Advances in neural information processing systems_, 36, 2024.
* Liu et al. [2024] Qidong Liu, Xian Wu, Xiangyu Zhao, Yuanshao Zhu, Derong Xu, Feng Tian, and Yefeng Zheng. When moe meets llms: Parameter efficient fine-tuning for multi-task medical applications. In _Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 1104-1114, 2024.
* Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* Lu et al. [2022] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A unified model for vision, language, and multi-modal tasks. In _The Eleventh International Conference on Learning Representations_, 2022.
* Lu et al. [2021]Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. _arXiv preprint arXiv:2312.17172_, 2023.
* Lyu et al. (2023) Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image, audio, video, and text integration. _arXiv preprint arXiv:2306.09093_, 2023.
* Moor et al. (2023) Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. _Nature_, 616(7956):259-265, 2023.
* Moor et al. (2023) Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: a multimodal medical few-shot learner. In _Machine Learning for Health (ML4H)_, pages 353-367. PMLR, 2023.
* Peng et al. (2023) Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the world. _arXiv preprint arXiv:2306.14824_, 2023.
* Riquelme et al. (2021) Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andre Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. _Advances in Neural Information Processing Systems_, 34:8583-8595, 2021.
* Song et al. (2023) Shezheng Song, Xiaopeng Li, and Shasha Li. How to bridge the gap between modalities: A comprehensive survey on multimodal large language model. _arXiv preprint arXiv:2311.07594_, 2023.
* Su et al. (2023) Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. _arXiv preprint arXiv:2305.16355_, 2023.
* Thawkar et al. (2023) Omkar Thawkar, Abdelrahman Shaker, Sahal Shaji Mullappilly, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, Jorma Laaksonen, and Fahad Shahbaz Khan. Xraygpt: Chest radiographs summarization using medical vision-language models. _arXiv preprint arXiv:2306.07971_, 2023.
* Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* Tschandl et al. (2018) Philipp Tschandl, Cliff Rosendahl, and Harald Kittler. The ham1000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. _Scientific data_, 5(1):1-9, 2018.
* Tu et al. (2024) Tao Tu, Shekoofeh Azizi, Danny Driess, Mike Schaekermann, Mohamed Amin, Pi-Chuan Chang, Andrew Carroll, Charles Lau, Ryutaro Tanno, Ira Ktena, et al. Towards generalist biomedical ai. _NEJM AI_, 1(3):Aloa2300138, 2024.
* Van der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* Wu et al. (2023) Chaoyi Wu, Jiayu Lei, Qiaoyu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis. _arXiv preprint arXiv:2310.09909_, 2023.
* Wu et al. (2023) Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology. _arXiv preprint arXiv:2308.02463_, 2023.
* Xia et al. (2024) Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: A comprehensive benchmark of trustworthiness in medical vision language models. _arXiv preprint arXiv:2406.06007_, 2024.
* Xu et al. (2019) Xuanang Xu, Fugen Zhou, Bo Liu, Dongshan Fu, and Xiangzhi Bai. Efficient multiple organ localization in ct image using 3d region proposal network. _IEEE transactions on medical imaging_, 38(8):1885-1898, 2019.
* Yan et al. (2024) Qianqi Yan, Xuehai He, Xiang Yue, and Xin Eric Wang. Worse than random? an embarrassingly simple probing evaluation of large multimodal models in medical vqa. _arXiv preprint arXiv:2405.20421_, 2024.
* Yang et al. (2023) Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pfister, and Bingbing Ni. Medmnist v2-a large-scale lightweight benchmark for 2d and 3d biomedical image classification. _Scientific Data_, 10(1):41, 2023.
* Zhang et al. (2019)Jin Ye, Junlong Cheng, Jianpin Chen, Zhongying Deng, Tianbin Li, Haoyu Wang, Yanzhou Su, Ziyan Huang, Jilong Chen, Lei Jiang, et al. Sa-med2d-20m dataset: Segment anything in 2d medical imaging with 20 million masks. _arXiv preprint arXiv:2311.11969_, 2023.
* Ye et al. (2023) Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. _arXiv preprint arXiv:2304.14178_, 2023.
* Ye et al. (2024) Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13040-13051, 2024.
* Yu et al. (2020) Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. _Advances in Neural Information Processing Systems_, 33:5824-5836, 2020.
* Yu et al. (2023) Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan, Andy Tsai, Eduardo Pontes Reis, Eduardo Kaiser Urruahy Nunes Fonseca, Henrique Min Ho Lee, Zahra Shakeri Hossein Abad, Andrew Y Ng, et al. Evaluating progress in automatic chest x-ray radiology report generation. _Patterns_, 4(9), 2023.
* Zadouri et al. (2023) Ted Zadouri, Ahmet Ustun, Arash Ahmadian, Beyza Ermis, Acyr Locatelli, and Sara Hooker. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. _arXiv preprint arXiv:2309.05444_, 2023.
* Zhang et al. (2023) Kai Zhang, Jun Yu, Zhiling Yan, Yixin Liu, Eashan Adhikarla, Sunyang Fu, Xun Chen, Chen Chen, Yuyin Zhou, Xiang Li, et al. Biomedgpt: A unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks. _arXiv preprint arXiv:2305.17100_, 2023.
* Zhu et al. (2022) Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang, Hongsheng Li, Xiaogang Wang, and Jifeng Dai. Uniperceiver-moe: Learning sparse generalist models with conditional moes. _Advances in Neural Information Processing Systems_, 35:2664-2678, 2022.
* Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. _arXiv preprint arXiv:2304.10592_, 2023.

Component design

### Type of the routing network

Constant routerThe simplest routing network is to assign equal weights to the output of each expert, which can be expressed as:

\[R_{\text{constant}}(x_{i})=\{1/N\}_{k=1}^{N}\] (11)

Hard routerEach token is assigned to a specific expert based on its type (task / modal), with the number of experts being equal to the number of token types. It can be formulated as:

\[R_{\text{hard}}\left(x_{i}\right)=\{\text{IsType}\ \left(x_{i},k\right)\}_{k=1}^{N}\] (12)

Sparse routerUsing a small network \(g\), the sparse router computes a score vector for each token, with a length equal to the number of experts \(N\). Subsequently, the Top-\(K\) function retains the top-\(K\) values in the vector, while setting all other values to zero. Finally, the _Softmax_ function is applied to obtain the final routing vector. The whole process is shown as follows:

\[R_{\text{sparse}}\left(x_{i}\right)=\textit{Softmax}\left(\text{Top-}K\left(g \left(x_{i}\right),K\right)\right)\]

\[\text{Top-}K(v,K)=\begin{cases}v,&\text{if $v$ is in the top $K$}\\ 0,&\text{otherwise}\end{cases}\] (13)

Soft RoutSimilar to the sparse router, the soft router computes a score vector for each token through a small network \(g\). Subsequently, it applies the _Sigmoid_ function to the score vector and normalizes it, yielding the final routing vector. It can be formulated as:

\[R_{\text{soft}}(x_{i})=\frac{\textit{Sigmoid}(g(x_{i}))}{\textit{Sum}( \textit{Sigmoid}(g(x_{i})))}\] (14)

### LoRA-MoE

LoRA-MoE freezes the original parameters of the model to preserve world knowledge and introduces LoRA experts to learn new knowledge, thereby improving performance across multiple downstream tasks with few parameters.

Specifically, given a frozen linear layer with a weight matrix \(W_{0}\in\mathbf{R}^{d_{\text{in}}\times d_{\text{out}}}\), LoRA-MoE creates \(N\) low-rank trainable matrix pairs \(A_{k}\) and \(B_{k}\), where \(A_{k}\in\mathbf{R}^{d_{\text{in}}\times r}\), \(B_{k}\in\mathbf{R}^{r\times d_{\text{out}}}\), and the rank \(r\ll min(d_{\text{in}},d_{out})\). As in the case of LoRA, \(A_{k}\) is initialized with a random Gaussian distribution, and \(B_{k}\) is initialized to zero. During training, the parameters of \(W_{0}\) are frozen, and the parameters of \(A_{k}\) and \(B_{k}\) are updated. The forward process of a LoRA-MoE layer can be represented as:

\[h=W_{0}x_{i}+\Delta Wx_{i}=W_{0}x_{i}+\frac{\alpha}{r}\sum_{k=1}^{N}R(x_{i})A_ {k}B_{k}x_{i}\] (15)

where \(x_{i}\) is the input token, \(R\) is the router in the LoRA-MoE layer, \(\alpha\) is the learning rate scaling factor, and \(h\) is the output token. In ablation experiments, we transform each linear layer in the LLM into a LoRA-MoE layer with a sparse router. The rank \(r=4\), the learning rate scaling factor \(\alpha=8\), the number of LoRA experts \(N=5\), and select the top 2 experts.

## Appendix B Dataset

### Data source

MedQAMedQA (Jin _et al._, 2021) is a open-domain multiple-choice question answering dataset for solving medical problems. These questions are sourced from professional medical board exams, which feature diverse content and typically demand a comprehensive understanding of related medical concepts learned from medical textbooks in order to provide accurate answers. This dataset covers three languages: English, simplified Chinese, among which there are 12,723 QA pairs for English.

PubMedQAPubMedQA[Jin _et al._, 2019] is a biomedical question answering dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe using the corresponding abstracts. It has 1K expert-annotated, 61.2K unlabeled and 211.3K artificially generated QA instances. Each instance consists of: (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion,(3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion.

Slake-VQASlake-VQA[Liu _et al._, 2021] is a semantically annotated, knowledge-enhanced bilingual (English and Chinese) VQA dataset for radiology images. It contains 642 annotated images accompanied by 14,028 question-answer pairs, spanning 12 diseases, 39 organ systems, and 3 imaging modalities (CT, MRI, and X-ray). Questions are either open-ended (free-form) or closed-ended (balanced yes/no) related to various aspects of the image content such as plane, quality, position, organ, abnormality, size, color, shape, and knowledge graph.

Path-VQAPath-VQA[He _et al._, 2020] is a pathology VQA dataset comprising 4,998 pathology images and 32,799 question-answer pairs. These pathology images are sourced from medical textbooks and online digital libraries. Each image is associated with multiple QA pairs pertaining to different aspects of the pathology including color, location, appearance, shape, etc. The dataset includes 16,465 open-ended questions, which make up 50.2% of the total and are categorized into six types: what, where, when, whose, how, and how much/how many. The remaining questions are close-ended "yes/no" questions, with a balanced distribution of 8,145 "yes" answers and 8,189 "no" answers. In the official dataset split, the training set, validation set and test set contain 19,755, 6,279 and 6,761 QA pairs, respectively.

SA-Med2D-20MSA-Med2D-20M[Ye _et al._, 2023a] is a large-scale segmentation dataset of 2D medical images built upon numerous public and private datasets. It consists of 4.6 million 2D medical images and 19.7 million corresponding masks, covering almost the whole body and showing significant diversity. It comprises 10 modalities, with CT and MR modalities being predominant in both the number of images and masks. Specifically, there are 2338,753 images and 12547,037 masks for CT and 2217,633 images and 7147,784 masks for MR. This is primarily attributed to their widespread presence in public medical image segmentation datasets and the 3D dimension of CT and MR scans, which yields a high volume of slices when segmented across three axes.

Mimic-CxMMIMIC-CXR[Johnson _et al._, 2019] is a large dataset of chest radiographs with free-text radiology reports. A total of 377,110 images are available in the dataset from 227,835 image studies collected for 65,379 patients. Each patient may have multiple studies and each study may contain one or more images associated with the same free-text report. Images in MIMIC-CXR are collected from multiple view positions: e.g., anterior-posterior (AP), posterior- anterior, and lateral (LA). Protected health information (PHI) in radiology reports and images is removed, which results in missing information in some sentences of the reports.

The MIMIC-CXR-JPG dataset is derived from MIMIC-CXR, providing JPG format files derived from the DICOM images and structured labels derived from the free-text reports. The aim of MIMIC-CXR-JPG is to provide a convenient processed version of MIMIC-CXR, as well as to provide a standard reference for data splits and image labels.

RadFM[Wu _et al._, 2023b] processes radiology reports in MIMIC-CXR by extracting the indication, findings, and impression sections, and removing redundant white spaces. Images without reports and reports where the findings section can not be extracted are discarded from both the training and test sets. Additionally, reports with findings sections exceeding 800 characters are filtered out. To enhance the model's capability to process images from different view positions, images of different orientations associated with the same report are treated as independent samples.

MpxMPx[Wu _et al._, 2023b] is a report generation dataset collected from the MedPix website (https://medpix.nlm.nih.gov/) and organized by cases. Each case includes multiple radiologic scans, general clinical findings, discussions, and diagnostic results. Additionally, MPx provides scan-level annotations, such as image modality, shooting plane, and captions for each scan. The dataset is divided into MPx-Single and MPx-Multi, with annotations provided at the case level and scan level, respectively.

MedMNIST v2MedMNIST v2 (Yang _et al._, 2023) is a large-scale MNIST-like collection of standardized biomedical images, including 2D datasets with resolutions up to 224x224 pixels and 3D datasets with resolutions up to 64x64x64 voxels. The 2D datasets include 12 subsets: PathMNIST, ChestMNIST, DermMNIST, OCTMNIST, PneumoniaMNIST, RetinaMNIST, BreastMNIST, BloodMNIST, TissueMNIST, OrganAMNIST, OrganCMNIST, and OrganSMNIST. The 3D datasets comprise 6 subsets: OrganMNIST3D, NoduleMNIST3D, FractureMNIST3D, AdrenalMNIST, VesselMNIST3D, and SynapseMNIST3D. Covering primary data modalities in biomedical images, it is designed to perform classification on lightweight 2D and 3D images with various data scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression and multi-label). The comprehensive dataset, comprising approximately 708K 2D images and 10K 3D images, supports a wide range of research and educational purposes in biomedical image analysis, computer vision, and machine learning.

DermaMNIST, a 2D subset of MedMNIST v2, is based on HAM10000 (Tschandl _et al._, 2018; Codella _et al._, 2019), a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Comprising 10,015 dermatoscopic images, the dataset is categorized into 7 distinct classes: actinic keratoses and intraepithelial carcinoma, basal cell carcinoma, benign keratosis-like lesions, dermatofibroma, melanoma, melanocytic nevi, and vascular lesions.

OrganSMNIST, another 2D subset of MedMNIST v2, is based on 3D computed tomography (CT) images from Liver Tumor Segmentation Benchmark (LiTS) (Bilic _et al._, 2023). Organ labels are obtained by using bounding-box annotations of 11 body organs from another study (Xu _et al._, 2019). Hounsfield-Unit (HU) of the 3D images are transformed into grey scale with a abdominal window. Subsequently, 2D images are cropped from the center slices of the 3D bounding boxes in sagittal views. Comprising 25,211 images, the dataset is categorized into 11 distinct classes: bladder, left femur, right femur, heart, left kidney, right kidney, liver, left lung, right lung, pancreas, and spleen.

Custom dataset splittingTo prevent the model from encountering training images during testing, the official dataset split from Slake-VQA is not utilized. Instead, we randomly divide all images into training and testing sets at a ratio of 6:1, along with their respective QA pairs and bounding boxes. Consequently, the training set comprises 550 images, 6018 English QA pairs, and 1421 bounding boxes, while the testing set includes 92 images, 1014 English QA pairs, and 201 bounding boxes.

For MIMIC-CXR, JPG images provided in MIMIC-CXR-JPG and the corresponding reports from RadFM are used for the report generation task. The training set is a subset of the original training set, containing 9,997 samples, while the test set remains the same as the original test set, containing 3,858 samples.

### Well-crafted datasets for REC and REG tasks

Slake-REC / Slake-REGAs a semantically-labeled knowledge-enhanced dataset for medical visual question answering, Slake-VQA provides bounding boxes for each object in the image. As shown in Figure 6 (a), the original format of each bounding box is \([X,Y,W,H]\). First, we convert it to the \([X_{min},Y_{min},X_{max},Y_{max}]\) format. Assuming the relative size of each image is 100x100, we then normalize each coordinate value in the bounding box to fall within the range of 0 to 100.

As shown in Figure 6 (c), in the REC task, an image and object name are given to find the object's bounding box. In the REG task, an image and object bounding box are provided to identify the object's name. The Slake-REC and Slake-REG datasets are thus created.

SA-Med2D-REC / SA-Med2D-REGEach image in the SA-Med2D-20M dataset has one or more masks, with each mask corresponding to an object. As shown in Figure 6 (b), we calculate the bounding box for each mask and normalize it to a range of 0 to 100, resulting in a bounding box for each object in the \([X_{min},Y_{min},X_{max},Y_{max}]\) format.

The SA-Med2D-REC and SA-Med2D-REG datasets are organized as depicted in Figure 6 (c). 10,000 samples each are selected from the CT and MR subsets as the training set, and 2,000 samples each are selected as the test set.

### Data availability

In the Table B.3, we list the links for each dataset, the number of samples in the training and test sets, and their licenses.

## Appendix C Multi-task instruction template

We have designed different instruction templates for different datasets. During the training process, when a sample from a dataset is selected, an instruction template is also sampled from the corresponding dataset's template pool and used to format the sample. Examples of instruction templates for each dataset are shown below.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Dataset** & **Link** & **Train / Test Split** & **License** \\ \hline MedQA & https://github.com/jind11/MedQA & 10178 / 1273 & MIT License \\ PubMedQA & https://github.com/pubmedqa/pubmedqa & 500 / 500 & MIT License \\ Slake-VQA & & 6018 / 1014 & Open Access \\ Slake-REC & https://www.med-vqa.com/slake & 1421 / 201 & - \\ Slake-REG & & 1421 / 201 & - \\ Path-VQA & https://github.com/UCSD-AI4H/PathVQA & 19755 / 6761 & MIT License \\ SA-Med2D-20M & & - & Apache-20 license \\ SA-Med2D-REC & https://opensxlab.org.cn/datasets/GMAI/SA-Med2D-20M & 20000 / 4000 & - \\ SA-Med2D-REG & & 20000 / 4000 & - \\  & https://physionet.org/content/mimic-cxr-jpg/2.1.0 & & PhysioNet Credentialed \\ MIMIC-CXR & https://huggingface.co/datasets/chaoyi-wu/RadFM_data_csv & 9997 / 3858 & Health Data License 1.5.0 \\ MPIx-Single & https://huggingface.co/datasets/chaoyi-wu/MedPix-Images & & Apache-20 license \\  & https://huggingface.co/datasets/chaoyi-wu/RadFM_data_csv & & Open Access \\ DermanMNIST & https://medmnist.com & 7007 / 2005 & Apache-2.0 license \\ OrganSMNIST & https://medmnist.com & 13932 / 8827 & Apache-2.0 License \\ \hline \hline \end{tabular}
\end{table}
Table 4: Data availability.

Figure 6: Data production process for REC and REG tasks. (a) the process of transforming bounding boxes in Slake-VQA, (b) the process of obtaining bounding boxes from masks in SA-Med2D, (c) the input-output organization of REC and REG tasks.

**Example 1**: [qa] A researcher evaluates healthy breast tissue from 100 women, 50 women that were pregnant at the time of the study and 50 age-matched non-pregnant women. The breast tissue in pregnant women contained an increased number of acinar glands with epithelial proliferation compared to the non-pregnant women. Which process caused this change?

**Example 2**: [qa] If you are a doctor, please answer the following question briefly: a researcher evaluates healthy breast tissue from 100 women, 50 women that were pregnant at the time of the study and 50 age-matched non-pregnant women. The breast tissue in pregnant women contained an increased number of acinar glands with epithelial proliferation compared to the non-pregnant women. Which process caused this change?

**Example 1**: [qa] Does the severity of obstructive sleep apnea predict patients requiring high continuous positive airway pressure?

**Example 2**: [qa] If you are a doctor, please answer the following question using "yes", "no" or "maybe": does the severity of obstructive sleep apnea predict patients requiring high continuous positive airway pressure?

**State-VQA** / Path-VQA

**Example 1**: <lmg> <lmageFeature> </Img> [vqa] What modality is used to take this image?

**Example 2**: <lmg> <lmageFeature> </Img> [vqa] Based on the image, respond to this question with a short answer: what modality is used to take this image?

**State-NEC/SAN-TED-NEC**

**Example 1**: <lmg> <lmageFeature> </Img> [refer] Liver.

**Example 2**: <lmg> <lmageFeature> </Img> [refer] Give me the location of liver.

**Example 3**: <lmg> <lmageFeature> </Img> [refer] Where is liver?

**Example 4**: <lmg> <lmageFeature> </Img> [refer] From this image, tell me the location of liver.

**Example 5**: <lmg> <lmageFeature> </Img> [refer] The location of liver is

**Example 6**: <lmg> <lmageFeature> </Img> [refer] Could you tell me the location for liver?

**Example 7**: <lmg> <lmageFeature> </Img> [refer] Where can 1 locate the liver?

**State-NEC/SAN-TED-NEC**

**Example 1**: <lmg> <lmageFeature> </Img> [identify] <16><36><42><61>

**Example 2**: <lmg> <lmageFeature> </Img> [identify] What object is in this location <16><36><42><61>?

**Example 3**: <lmg> <ImageFeature> </Img> [identify] Identify the object present at this location <16><36><42><61>.

**Example 4**: <lmg> <ImageFeature> </Img> [identify] What is it in <16><36><42><61>?

**Example 5**: <lmg> <ImageFeature> </Img> [identify] Describe this object in <16><36><42><61>.

**Example 6**: <lmg> <ImageFeature> </Img> [identify] This <16><36><42><61> is

**Example 7**: <lmg> <ImageFeature> </Img> [identify] The object in <16><36><42><61> is

**MMICEAR**

**Example 1**: <lmg> <ImageFeature> </Img> [caption] Describe the given chest x-ray image in detail.

**Example 2**: <lmg> <ImageFeature> </Img> [caption] Take a look at this chest x-ray and describe the findings and impression.

**Example 3**: <lmg> <ImageFeature> </Img> [caption] Could you provide a detailed description of the given x-ray image?

**Example 4**: <lmg> <ImageFeature> </Img> [caption] Describe the given chest x-ray image as detailed as possible.

**Example 5**: <lmg> <ImageFeature> </Img> [caption] What are the key findings in this chest x-ray image?

**MPV-Simple**

**Example 1**: <lmg> <ImageFeature> </Img> [caption] Describe this input image.

**Example 2**: <lmg> <ImageFeature> </Img> [caption] Help captioning the image.

**Example 3**: <lmg> <ImageFeature> </Img> [caption] What can be inflected from the scan?

**Example 4**: <lmg> <ImageFeature> </Img> [caption] Can you give a caption for this image?

**Example 5**: <lmg> <ImageFeature> </Img> [caption] Can you provide a brief summary of the radiology image?

**Example 6**: <lmg> <ImageFeature> </Img> [caption] Please write a report about the image?

**Example 7**: <lmg> <ImageFeature> </Img> [caption] Can you provide an analysis of this image?

**Example 8**: <lmg> <ImageFeature> </Img> [caption] Can you explain what is shown in this image?

**Example 9**: <lmg> <ImageFeature> </Img> [caption] What can be indicated from the radiologic scans?

**Example 10**: <lmg> <ImageFeature> </Img> [caption] What can you infer from this photograph?

## Appendix D Experiments

### Evaluation metrics

F1 ScoreAssuming \(m\) is the number of common words in the candidate \(C\) and the reference \(R\) with the number of words of \(c\) and \(r\), the precision and recall for a candidate sentence can be calculated as:

\[\text{{precision}}=\frac{m}{c}\] (16)

\[\text{{recall}}=\frac{m}{r}\] (17)

Considering class imbalance, F1 score is used to evaluate the performance of the model on both the VQA and REG tasks, which means the harmonic mean of precision and recall. A higher average F1 score for the dataset indicates a higher performance of the model.

\[\text{{F1}}=\frac{2\times\text{{precision}}\times\text{{recall}}}{\text{{ precision}}+\text{{recall}}}\] (18)

Bleu-NWe use BLEU-1 to assess the model's performance on both the VQA and REG tasks, while employing both BLEU-1 and BLEU-4 to evaluate its performance in the report generation task. Given the candidate \(C\) and reference \(R\), BLEU-N is defined as:

\[\text{{BLEU-N}}=\frac{\sum_{\text{gram}_{N}\in C}Count_{clip}(\text{gram}_{N} )}{\sum_{\text{gram}_{N}\in C}Count(\text{gram}_{N})}\] (19)

When N=1, the above formula calculates BLEU-1; when N=4, it calculates BLEU-4.

Rouge-NWe use ROUGE-1 and ROUGE-2 to evaluate the performance of the model on the RG task. Given the candidate \(C\) and reference \(R\), ROUGE-N is defined as:

\[\text{{ROUGE-N}}=\frac{\sum_{\text{gram}_{N}\in R}Count_{match}(\text{gram}_{ N})}{\sum_{\text{gram}_{N}\in R}Count(\text{gram}_{N})}\] (20)

When N=1, the above formula calculates ROUGE-1; when N=2, it calculates ROUGE-2.

Rouge-LROUGE-L is also used to evaluate the quality of the generated text on the task of report generation, which stands for recall-oriented understudy for gisting evaluation with the longest common subsequence. Given the candidate \(C\) and reference \(R\), let \(LCS(C,R)\) be the length of the longest common subsequence, which is determined by using dynamic programming, it can be an defined as:

\[\text{{ROUGE-L}}=\frac{(1+\beta^{2})R_{LCS}p_{LCS}}{R_{LCS}+\beta^{2}p_{LCS}}\] (21)

where \(R_{LCS}=\frac{LCS(C,R)}{L_{C}}\), \(p_{LCS}=\frac{LCS(C,R)}{L_{R}}\), \(\beta=\frac{P_{LCS}}{R_{LCS}}\). \(L_{C}\) and \(L_{R}\) represent the length of the candidate and reference. A higher ROUGE-L score means that the generated text shares more of the same sequences of words as the reference text, which typically indicates better quality in terms of capturing the salient points of the reference.

METEORMETEOR is also used to evaluate the quality of the generated text on the task of report generation, which stands for metric for evaluation of translation with explicit ordering. METEOR for a sentence is computed as:

\[\text{METEOR}=(1-p)\times\frac{\textit{precision}\times\textit{recall}}{\alpha \times\textit{precision}+(1-\alpha)\times\textit{recall}}\] (22)

where \(p=\gamma(\frac{ch}{m})^{\theta}\) is the penalty factor. \(ch\) is the number of chunks, which means a contiguous ordered block. \(\alpha\), \(\theta\) and \(\gamma\) are hyperparameters determined according to different datasets.

RadGraph F1To assess the semantic accuracy in the task of report generation, RadGraph F1 computes the overlap in clinical entities and relations between a machine-generated report and a radiologist-generated report. Specifically, following the criteria in RadGraph [17], two entities are matched if their tokens (words in the original report) and labels (entity type) match. Two relations are matched if their start and end entities match and the relation type matches. RadGraph F1 metric computes the overlap in entities and relations separately and reports their average.

RadCliQRadCliQ (radiology report clinical quality) is also used to assess the semantic accuracy in the task of report generation. Two versions of the RadCliQ metric: RadCliQ-v0 and RadCliQ-v1 both use a machine learning model to take in values from other metrics, such as BERTScore and CheXbert vector similarity, and then produce a composite score based on these input values, which predict the total number of errors in a report.

IoUWe use IoU (Intersection over Union) to evaluate the performance of the model on the REC task. It can be formulated as:

\[\text{IoU}=\frac{P\cap G}{P\cup G}\] (23)

where \(P\) is the prediction area of the model, \(G\) is the area of the ground truth.

R@0.5We use use R@0.5 to evaluate the performance of the model on the referring expression comprehension task. R stands for recall, and 0.5 denotes the IoU threshold. When the IoU between the prediction and the ground truth is greater than or equal to 0.5, it is considered a true positive (TP). When the IoU is less than 0.5, it is considered a false negative (FN). Therefore, for a sample with only one bounding box, R@0.5 can be formalized as:

\[\text{R@0.5}=\frac{TP}{TP+FN}=\begin{cases}1,&\text{IoU}\geq 0.5\\ 0,&\text{IoU}<0.5\end{cases}\] (24)

### Data leakage issue of RadFM on MPx-Single

When we directly use the model checkpoint provided by RadFM open-source repository for model inference, we find that the model outputs for many samples were completely consistent with ground truth. This issue only occurs on the MPx-Single dataset, while we strictly follow the official test set split. This appears to be unreasonable, raising suspicions of potential data leakage. Here are some examples:

**Instruction**: Describe this input image.

**Ground truth**: Neuroradiology Quiz

**RadFM**: Neuroradiology Quiz

**Uni-Med**: axial noncontrast ct image shows a large, well-circumscribed, non-aggressive appearing mass in the right frontal lobe.

### Ablation on special token and identifier

We have designed vision-level special task tokens and text-level special task identifiers for visual features and text prompt, respectively. Through ablation experiment, we verify whether they have a positive effect on model performance. As shown in Table 5, we observe that text-level special task identifiers bring limited improvement. In contrast, vision-level special task tokens significantly improve the model's overall performance on all datasets, further illustrating the effectiveness of mitigating the tug-of-war problem at the connector.

### Visualization analysis of visual features on image modalities

We use t-SNE to visualize the distribution of visual features by modalities in Figure 7. We first observe the visual feature distribution of different modalities under the same task in Figure 7 (a-c). The feature of CT and MRI modalities in the REG task already have good discriminability after

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Connector} & \multirow{2}{*}{Special Token / Identifier Text-level Vision-level} & \multicolumn{2}{c}{VQA} & \multirow{2}{*}{Avg.} & \multicolumn{2}{c}{REC} & \multirow{2}{*}{\(\Delta\) (\(\uparrow\))} & \multicolumn{2}{c}{REG} & \multirow{2}{*}{\(\Delta\) (\(\uparrow\))} & \multicolumn{2}{c}{RG} & \multirow{2}{*}{\(\Delta\) (\(\uparrow\))} & \multicolumn{2}{c}{CLS} & \multirow{2}{*}{\(\Delta\) (\(\uparrow\))} & \multirow{2}{*}{Total} \\  & & & & & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & & & & & \\ \hline MLP & - & - & - & 79.81 & 56.48 & 35.18 & 16.26 & 74.54 & 58.42 & 18.55 & 15.50 & 76.26 & 73.64 & \\ \hline \multirow{5}{*}{CMoE} & - & - & 81.95 & 57.35 & 1.95 & 36.76 & 18.74 & 9.94 & 76.07 & 58.81 & 1.49 & 24.71 & 15.42 & 16.45 & 74.46 & 76.07 & 0.55 & 6.08 \\  & - & **✓** & 81.33 & 57.29 & 1.76 & **37.85** & 20.14 & 15.79 & 77.23 & **62.72** & **5.55\%** & 23.29 & **15.74** & 13.6\% & **76.76** & 76.55 & 2.3\% & 7.8\% \\ \cline{1-1}  & **✓** & - & **81.79** & 57.69 & **2.3\%** & 35.51 & 17.79 & 5.2\% & 74.43 & 61.34 & 2.46 & **26.27** & 15.61 & **21.2\%** & 76.56 & **77.21** & **2.64\%** & 6.7\% \\ \cline{1-1}  & **✓** & **✓** & 81.52 & **57.75** & 2.2\% & 37.54 & **20.30** & **15.8\%** & **77.45** & 60.42 & 3.7\% & 24.70 & 15.55 & 16.7\% & 75.61 & 76.92 & 1.8\% & **8.09\%** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Ablation Experiments on special token and identifier.

passing through the frozen visual encoder. After passing through the connector, the improvement in Silhouette score (from 0.3049 to 0.3335) is relatively limited. In addition, we select 100 samples from each of the 8 modalities and observe their visual feature distributions after passing through different visual encoders in Figure 7 (d-f). It can still be observed that visual features of different modalities already have specific patterns in the feature space, whether using EVA-CLIP, CLIP or BiomedCLIP.

The above findings also provide an explanation for why we attempt to explicitly introduce task information instead of modality information in CMoE. When aligning visual and language embedding spaces through the connector in Uni-Med's multi-modal and multi-task scenario, task information is more difficult to distinguish than modality information.

### Performance of Uni-Med on REC and REG tasks

We report the metrics of Uni-Med on the tasks of referring expression comprehension and referring expression generation in Table 6. The mean and standard deviation of performance of Uni-Med are obtained after several 300k iterations.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Task** & **Dataset** & **Metric** & **Uni-Med** \\ \hline \multirow{3}{*}{Referring Expression Comprehension} & \multirow{3}{*}{Sake-REC} & IoU & 37.71\(\pm\)0.52 \\  & & R@0.5 & 39.30\(\pm\)0.76 \\ \cline{2-4}  & & IoU & 21.60\(\pm\)2.19 \\ \cline{2-4}  & & R@0.5 & 14.42\(\pm\)3.20 \\ \hline \multirow{3}{*}{Referring Expression Generation} & \multirow{3}{*}{Sake-REG} & BLEU-1 & 75.78\(\pm\)2.05 \\  & & F1 & 77.35\(\pm\)1.97 \\ \cline{1-1} \cline{2-4}  & & Accuracy & 68.16\(\pm\)1.32 \\ \cline{1-1} \cline{2-4}  & & BLEU-1 & 61.47\(\pm\)1.76 \\ \cline{1-1} \cline{2-4}  & & F1 & 62.17\(\pm\)1.90 \\ \cline{1-1} \cline{2-4}  & & Accuracy & 57.69\(\pm\)1.07 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Performance of Uni-Med on REC and REG tasks.

Figure 7: Visual features distribution on image modalities. (a)-(c) The feature distribution of CT and MRI modalities in the REG task. (a) passing through the frozen visual encoder.(b) passing through the MLP connector. (c) passing through the CMoE. (d)-(f) The feature distribution of 8 modalities after passing through the frozen visual encoder. (d) EVA-CLIP ViT-G/14. (e) CLIP ViT-L/14. (f) BiomedCLIP ViT-B/16.

### Comparison of architecture capability between Uni-Med and LLaVA-Med

In addition to directly compare the capability of existing models, we take LLaVA-Med as an example to compare the capability of model architectures.

Specifically, we use the checkpoints of the second stage (medical instruction tuning) to perform two strategies of LLM full parameter fine-tuning: (1) Dataset-specific fine-tuning; (2) Joint training fine-tuning. The data split and the prompt format are completely consistent with Uni-Med and LLaVA-Med, respectively. Both strategies last for 3 epochs (the same as Uni-Med). The results are shown in Table 7.

Following the model architecture of LLaVA-Med, there is a serious tug-of-war problem when we implement joint fine-tuning strategy on multiple tasks and datasets. While the strategy of dataset-specific fine-tuning has significantly improved the evaluation metrics of each dataset.

It is worth noting that Uni-Med has achieved competitive and leading results through joint training, without dataset-specific fine-tuning. It can be concluded that the model architecture of Uni-Med, especially the design of CMoE, has achieved a superior solution to the tug-of-war problem, which reduces interference and promotes more efficient knowledge sharing.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{**Task**} & \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Metric**} & \multicolumn{2}{c}{**LLaVA-Med**} & \multirow{2}{*}{**Uni-Med**} \\  & & & & **Joint Training** & **Dataset-specific** & **Joint Training** \\ \hline \multirow{4}{*}{Visual Question Answering} & \multirow{2}{*}{Slake-VQA} & BLEU-1 & 33.69 & 72.00 & **82.12** \\  & & F1 & 35.83 & 73.07 & **83.07** \\ \cline{2-6}  & \multirow{2}{*}{Path-VQA} & BLEU-1 & 37.79 & 56.86 & **58.07** \\  & & F1 & 38.55 & 57.51 & **58.74** \\ \hline \multirow{4}{*}{Report Generation} & \multirow{4}{*}{MMIC-CXR} & BLEU-1 & 20.43 & 21.03 & **27.79** \\  & & BLEU-4 & 4.86 & 4.96 & **6.46** \\  & & ROUGE-1 & 26.11 & 28.28 & **28.81** \\  & & ROUGE-2 & 7.66 & 9.01 & **9.62** \\  & & ROUGE-1 & 19 & 20.61 & **22.58** \\  & & METEOR & 8.73 & 8.89 & **10.59** \\ \cline{2-6}  & \multirow{4}{*}{MPx-Single} & BLEU-1 & 15.11 & 14.63 & **15.80** \\  & & BLEU-4 & 2.4 & 1.75 & **2.47** \\  & & ROUGE-1 & 13.22 & 13.03 & **14.32** \\  & & ROUGE-2 & 2.39 & 2.19 & **2.68** \\  & & ROUGE-L & 10.99 & 10.85 & **12.29** \\  & & METEOR & 5.83 & 5.79 & **5.92** \\ \hline \multirow{4}{*}{Image Classification} & \multirow{2}{*}{DermanMNIST} & Accuracy & 25.84 & **79.95** & 76.96 \\ \cline{2-6}  & & OrganSMNIST & Accuracy & 66.80 & 77.84 & **78.07** \\ \hline \multirow{4}{*}{Referring Expression Comprehension} & \multirow{2}{*}{Slake-REC} & IoU & 4.07 & 22.41 & **37.71** \\  & & R@0.5 & 1.99 & 18.41 & **39.30** \\ \cline{1-1} \cline{2-6}  & & \multirow{2}{*}{SA-Med2D-REC} & IoU & 8.64 & 17.67 & **21.60** \\  & & R@0.5 & 4.75 & 9.98 & **14.42** \\ \hline \multirow{4}{*}{Referring Expression Generation} & \multirow{4}{*}{Slake-REG} & BLEU-1 & 27.21 & 50.79 & **75.78** \\  & & F1 & 30.97 & 53.15 & **77.35** \\ \cline{1-1}  & & Accuracy & 20.40 & 44.78 & **68.16** \\ \cline{1-1} \cline{2-6}  & \multirow{2}{*}{SA-Med2D-REG} & BLEU-1 & 45.83 & 55.15 & **61.47** \\ \cline{1-1}  & & F1 & 47.11 & 55.98 & **62.17** \\ \cline{1-1}  & & Accuracy & 40.80 & 50.92 & **57.69** \\ \hline \hline \end{tabular}
\end{table}
Table 7: Comparison of architecture capability between Uni-Med and LLaVA-Med. We utilize dataset-specific fine-tuning and joint training fine-tuning on LLaVA-Med, respectively.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We clearly state our contributions and scope in abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of this work in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Although the paper does not include theoretical results, but uses and combines existing theoretical methods for interpretability analysis about the tug-of-war problem. All assumptions are clearly stated or referenced in the statement. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We fully disclose all the information needed to reproduce the main experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Code and resources are available at https://github.com/tsinghua-msip/Uni-Med Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We have provided all the training and test details including data splits, hyperparameters, how they were chosen, type of optimizer, etc. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: In overall comparison, we report the mean and standard deviation of performance of our model on all tasks, which is calculated through three different random seed configurations. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have provided the information about computer resources such as GPU and time of execution in the paper. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This research conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We have discussed the potential positive societal impacts in section of Introduction and Conclusion, and negative societal impacts in Section 6. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: For our open-source model, providing effective safeguards is a challenge. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All creators or original owners of assets used in the paper are properly credited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We have produced new datasets for several tasks and described the processing process in Appendix B.2. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.