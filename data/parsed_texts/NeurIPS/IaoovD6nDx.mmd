# Toward Causal-Aware RL:

State-Wise Action-Refined Temporal Difference

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Although it is well known that exploration plays a key role in Reinforcement Learning (RL), prevailing exploration strategies for continuous control tasks in RL are mainly based on naive isotropic Gaussian noise regardless of the causality relationship between action space and the task and consider all dimensions of actions equally important. In this work, we propose to conduct interventions on the primal action space to discover the causal relationship between the action space and the task reward. We propose the method of State-Wise Action Refined (SWAR), which addresses the issue of action space redundancy and promote causality discovery in RL. We formulate causality discovery in RL tasks as a state-dependent action space selection problem and propose two practical algorithms as solutions. The first approach, TD-SWAR, detects task-related actions during temporal difference learning, while the second approach, Dyn-SWAR, reveals important actions through dynamic model prediction. Empirically, both methods provide approaches to understand the decisions made by RL agents and improve learning efficiency in action-redundant tasks.

## 1 Introduction

Although model-free RL has achieved great success in various challenging tasks and outperforms experts in most cases [21; 26; 17; 34; 4], the design of action space always requires elaboration. For example, in the game StarCraftII, hundreds of units can be selected and controlled to perform various actions. To tackle the difficulty in exploration caused by the extremely large action and state space, hierarchical action space design and imitation learning are used [27; 34] to reduce the exploration space. Both of those approaches require expert knowledge of the task. On the other hand, even in the context of imitation learning where expert data is assumed to be accessible, causal confusion will still hinder the performance of an agent [8]. Those defects motivate us to explore the causality-awareness of an agent that permits an agent to discover the causal relationship for the environment and select useful dimensions of action space during policy learning in pursuance of improved learning efficiency.

Another motivating example is the in-hand manipulation tasks [2]: robotics equipped with touch sensors outperforms the policies learned without sensors by a clear margin in hand-in manipulation tasks [20], showing the importance of causality discovery between actions and feedbacks in RL. A similar example can be found in human learning: knowing nothing about how to control the finger joints flexibly may not hinder a baby learns to walk, and a baby has not learned how to walk can still learn to use forks and spoons skillfully, inspiring us to believe that the challenge for exploration can be greatly eased after the causality between action space and the given task is learned.

In this work, the recent advance of instance-wise feature selection technique [38] is improved to be more suitable in large-scale state-wise action selection tasks and adapted to the time-series causal discovery setting to select state-conditioned action space in RL with redundant action space. With theproposed method, the agent learns to perform intervention, discover the true structural causal model (SCM) and select task-related actions for a given task, remarkably reduces the burden of exploration and obtains on-par learning efficiency as well as asymptotic performance compared with agents trained in the oracle settings where the action spaces are pruned according to given tasks manually.

## 2 Preliminary

Markov Decision ProcessesRL tasks can be formally defined as Markov Decision Processes (MDPs), where an agent interacts with the environment and learns to make decision at every timestep. Formally, we consider the deterministic MDP with a fixed horizon \(H\in\mathbb{N}^{+}\) denoted by a tuple \((\mathcal{S},\mathcal{A},H,r,\gamma,\mathcal{T},\rho_{0})\), where \(\mathcal{S}\) and \(\mathcal{A}\) are the \(|\mathcal{S}|\)-dimensional state and \(|\mathcal{A}|\)-dimensional action space; \(r:\mathcal{S}\times\mathcal{A}\mapsto\mathbb{R}\) denotes the reward function; \(\gamma\in(0,1]\) is the discount factor indicating importance of present returns compared with long-term returns; \(\mathcal{T}:\mathcal{S}\times\mathcal{A}\mapsto\mathcal{S}\) denotes the transition dynamics; \(\rho_{0}\) is the initial state distribution.

We use \(\Pi\) to represent the stationary deterministic policy class, i.e., \(\Pi=\{\pi:\mathcal{S}\mapsto\mathcal{A}\}\). The learning objective of an RL algorithm is to find \(\pi^{*}\in\Pi\) as the solution of the following optimization problem: \(\max_{\pi\in\Pi}\mathbb{E}_{\tau\sim\rho_{0},\pi,\mathcal{T}}[\sum_{t=1}^{H} \gamma^{t}r_{t}]\) where the expectation is taken over the trajectory \(\tau=(s_{1},a_{1},r_{1},\ldots,s_{H},a_{H},r_{H})\) generated by policy \(\pi\) under the environment \(\mathcal{T}\), starting from \(s_{0}\sim\rho_{0}\).

InvaselInvasel is proposed by [38] to perform instance-wise feature selection to reduce overfitting in predictive models. The learning objective is to minimize the KL-Divergence of the full-conditional distribution and the minimal-selected-features-only conditional distribution of the outcome, i.e., \(\min_{F}\mathcal{L}\), with

\[\mathcal{L}=\mathcal{D}_{KL}(p(Y|X=x)||p(Y|X^{(F(x))}=x^{(F(x))}))+\lambda|F(x )|_{0}.\] (1)

where \(F:\mathcal{X}\rightarrow\{0,1\}^{d}\) is a feature selection function and \(|F(x)|_{0}\) denotes the cardinality (\(l_{0}\) norm) of selected features, i.e., the number of \(1\)'s in \(F(x)\). 1\(d\) is the dimension of input features.

Figure 1: **Block diagram of INVASE in temporal difference learning. States and actions sampled from replay buffer are fed into the selector network that predicts the selection probabilities of different dimensions of actions. A selection mask is then generated according to such a selection probability vector. The critic network and the baseline network are trained to minimize temporal difference error with states and the selected dimension of actions and primal action respectively. The difference of TD-Error is used to conduct a policy gradient to update the selector network.**

\(x^{(F(x))}=F(x)\odot x\) denotes the element-wise product of \(x\) and generated mask \(m=F(x)\). Ideally, the optimal selection function \(F\) should be able to minimize the two terms in Equation (1) simultaneously.

INVASE applies the Actor-Critic framework in the optimization of \(F\) through sampling, where \(f_{\theta}(\cdot|x)\), parameterized by a neural network \(\theta\)2, is used as a stochastic actor. Two predictive networks \(C_{\phi}(\cdot),B_{\psi}(\cdot)\) are considered as the critic and the baseline network used for variance reduction [36] and trained with the Cross-Entropy loss to produce return signal \(\mathcal{L}\), based on which \(f_{\theta}(\cdot|x)\) can be optimized through policy gradient:

Footnote 2: In this work, subscripts \(\phi,\psi,\theta,w\) are used to denote the parameter of neural networks.

\[\mathbb{E}_{(x,y)\sim p}[\mathbb{E}_{m\sim f_{\theta}(\cdot|x)}[\mathcal{L} \nabla_{\theta}\log f_{\theta}(\cdot|x)]].\] (2)

Finally, \(F(x)=(F_{1}(x),...,F_{d}(x))\) can be get by sampling from \(f(\cdot|x)=(f_{1}(x),...,f_{d}(x))\), with

\[F_{i}(x)=\begin{cases}1,&\mathbf{w}.\mathbf{p}.\quad f_{i}(\cdot|x).\\ 0,&\mathbf{w}.\mathbf{p}.\quad 1-f_{i}(\cdot|x).\end{cases}\] (3)

## 3 Proposed Method

The objective of this work is to carry out state-wise action selection in RL through intervention, and thereby enhance the learning efficiency with a pruned task-related action space after finding the correct causal model. Section 3.1 starts with the formalization of the action space refinery objective in RL tasks under the framework of causal discovery. Section 3.2 introduces SWAR, which improves the scalability of INVASE in high dimensional variable selection tasks. We integrate SWAR with deterministic policy gradient methods [25] in Section 3.3 to perform state-wise action space pruning, resulting in two practical causality-aware RL algorithms.

### Temporal Difference Objective with Structural Causal Models

In modern RL algorithms, the most general approach is based on the Actor-Critic framework [15], where the critic \(Q_{w}(s,a)\) approximates the return of given state-action pair \((s,a)\) and guides the Actor to maximize the approximated return at state \(s\). The Critic is optimized to reduce Temporal Difference (TD) error [29], defined as

\[\mathcal{L}_{TD}=\mathbb{E}_{s_{i},a_{i},r_{i},s^{\prime}_{i}\sim b}[(r_{i}+ \gamma Q_{w}(s^{\prime}_{i},a^{\prime}_{i})-Q_{w}(s_{i},a_{i}))^{2}].\] (4)

where \(\mathcal{B}=(s_{i},a_{i},r_{i},s^{\prime}_{i})_{i=1,2,...}\) is the replay buffer used for off-policy learning [17; 10; 12; 28], and \(a^{\prime}_{i}=\pi(s^{\prime}_{i})\) is the predicted action for state \(s^{\prime}_{i}\). In practice, the calculations of \(Q_{w}(s^{\prime}_{i},a^{\prime}_{i})\) are usually based on another set of slowly updated target networks for stability [10; 12]. Henceforth, TD-learning can be roughly simplified as regression with notion \(y_{i}=r_{i}+\gamma Q_{w}(s^{\prime}_{i},a^{\prime}_{i})\):

\[\mathcal{L}_{TD}=\mathbb{E}_{s_{i},a_{i},r_{i},s^{\prime}_{i}\sim b}[(y_{i}-Q _{w}(s_{i},a_{i}))^{2}].\] (5)

Assume there are only \(M<L\) actions are related to a specific task among the \(L\)-dimensional actions \(a_{i}=a^{(1)}_{i},...,a^{(L)}_{i}\), i.e., \(Q_{w}(\cdot,\cdot)\) is function of \(s_{i},a^{(1)}_{i},...,a^{(M)}_{i}\). Learning with the primal redundant action space will lead to around \(\frac{L+|\mathcal{S}|}{M+|\mathcal{S}|}\) times sample complexity [9; 39]. Therefore, we are motivated to improve the learning efficiency of \(Q\) by pruning those task-irrelevant action dimensions \(a^{(M+1)}_{i},...,a^{(L)}_{i}\) by finding an action selection function \(G\), satisfying

\[\min_{G,Q_{w}}\mathbb{E}_{s_{i},a_{i},r_{i},s^{\prime}_{i}\sim b}[(y^{\prime} _{i}-Q_{w}(s_{i},a^{(G(a_{i}|s_{i}))}_{i}))^{2}]+\lambda|G(a_{i}|s_{i})|_{0}.\] (6)

where \(y^{\prime}_{i}=r_{i}+\gamma Q_{w}(s^{\prime}_{i},a^{{}^{\prime}G(a^{\prime}_{ i}|s_{i})}_{i})\).

Such a problem can be addressed from the perspective of causal discovery. Formally, we can use the Structural Causal Models (SCMs) to represent the underlying causal structure of a sequential decision making process, as shown in Figure 2. Under this language, we use the notion of **causal** actions to denote \(a^{(1,...,M)}_{i}\), and **nuisance** actions for other dimension of actions. In our work, we use IC-INVASE for causal discovery. Ideally, the action selection function \(G\) should be able to distinguish between nuisance action dimensions and the causal ones that has causal relation with either dynamics or reward mechanism. We present in the next section our causal discovery algorithms.

### Iterative Curriculum INVASE (IC-INVASE)

Instead of directly applying INVASE to solve Equation (6). We first propose two improvements to make the vanilla INVASE more suitable for large-scale variable selection tasks as the action dimension in RL might be extremely large [34]. Specifically, the first improvement, based on curriculum learning, is introduced to tackle the exploration difficulty when \(\lambda\) in Equation (1) is large, where INVASE tends to converge to poor sub-optimal solutions and prune all variables including the useful ones [38]. The second improvement is based on the iterative structure of variable selection tasks: the feature selection operator \(G\) can be applied multiple times to conduct hierarchical feature selection without introducing extra computation expenses.

#### 3.2.1 Curriculum Learning For High Dimensional Variable Selection

The work of [3] first introduces Curriculum Learning to mimic human learning by gradually learn more complex concepts or handle more difficult tasks. Effectiveness of the method has been demonstrated in various set-ups [3, 19, 7, 35, 37]. In general, it should be easier to select \(M\) useful variables out of \(L\) input variables when \(M\) is larger. The most trivial case is to select all \(L\) variables, with an identical mapping \(x^{(G(x))}=G(x)\odot x=x\). Formally, we have

**Proposition 1** (Curriculum Property in Variable Selection).: _Assume \(M\) out of \(L\) variables are outcome-related, let \(M\leq N_{1}<N_{2}\leq L\), \(G_{N_{1}}(x)\) minimizes \(\mathcal{D}_{KL}(p(Y|X=x)||p(Y|X^{(G(x))}=x^{(G(x))}))+\lambda||G(x)|_{0}-N_{1}\). Then_

\(G_{N_{2}}(x)\) _minimizes \(\mathcal{D}_{KL}(p(Y|X=x)||p(Y|X^{G(x)}=x^{G(x)}))+\lambda||G(x)|_{0}-N_{2}|\) can be get through: \(G_{N_{2}}(x)\in\{G_{N_{1}}(x)\vee[G_{N_{1}}(x)\mathbf{XOR}\ \mathbf{1}]_{1_{N_{2}-N_{1}}}\}\), where \([\cdot]_{1_{N_{2}-N_{1}}}\) means keep \(N_{2}-N_{1}\) none-zero elements unchanged while replacing other elements by \(0\)._

Proof.: By the definition of the \([\cdot]_{1_{N_{2}-N_{1}}}\) operator, \(||G(x)|_{0}-N_{2}|=0\) is minimized. On the other hand, starting from \(N_{1}=M\), minimizing \(\mathcal{D}_{KL}(p(Y|X=x)||p(Y|X^{(G(x))}=x^{(G(x))}))\) requires all the \(M\) outcome-related variables being selected by \(G_{N_{1}}\). Therefore, \(G_{N_{2}}\) also minimizes the KL-divergence by the independent assumption of the other \(L-M\) variables with the outcomes. 

The proposition indicates the difficulty of selecting \(N\) useful out of \(L\) variables decreases monotonically as \(N\geq M\) increase from \(M,M+1,...,L\). In this work, two classes of practical curriculum are designed: 1. curriculum on the \(l_{0}\) penalty coefficient, and 2. curriculum on the proportion of variables to be selected.

Curriculum on \(l_{0}\) Penalty CoefficientIn this curriculum design, the penalty coefficient \(\lambda\) in Equation (1) is increased from \(0\) to a pre-determined number (e.g., \(1.0\)). Increasing the value of \(\lambda\) will lead to a larger penalty on the number of variables selected by the feature selector. Experiments in [38] has shown a large \(\lambda\) always lead to a trivial selector that does not select any variable.

Curriculum on the Proportion of Selected FeaturesIn this curriculum design, the proportion of variables to be selected, denoted by \(p_{r}\), is adjusted from the default setting \(0\) to a decreasing number

Figure 2: SCM of temporal difference learning. Among all executable actions, there can be only a subset have effect on the dynamical changes or the reward mechanism. In our work, we use IC-INVASE as a causal discovery tool to distinguish the causal irrelevant actions and hence improve learning efficiency.

from a pre-determined value (e.g., \(0.5\)) to \(0\). i.e., the \(l_{0}\) penalty term \(\lambda|G(x)|_{0}\) in Equation (1) is revised to be \(\lambda||G(x)|_{0}-d\cdot p_{r}|\), where \(d\) is the dimension of input \(x\). When the proportion is set to be \(p_{r}=0.5\), the selector will be penalized whenever less or more than half of all variables are selected. Such a curriculum design forces the feature selector to learn to select less but increasingly more important variables gradually.

Thus, we get the learning objective of curriculum-INVASE:

\[\mathcal{L}=\mathcal{D}_{KL}(p(Y|X=x)||p(Y|X^{(G(x))}=x^{(G(x))}))+\lambda||G( x)|_{0}-d\cdot p_{r}|.\] (7)

where \(\lambda\) increases from \(0\) to some value and \(p_{r}\) decreases from a value in \([0,1)\) to \(0\).

#### 3.2.2 Iterative Variable Selection

The second improvement proposed in this work is based on the iterative structure of variable selection tasks. Specifically, the \(G(x)\) mapping \(x\in\mathcal{X}\) to \(\{0,1\}^{d}\) is an iterative operator, which can be applied for multiple times to perform coarse-to-fine variable selection. Although in practice we follow [38] to apply an element-wise product in producing \(x^{(G(x))}\): \(x^{(G(x))}=G(x)\odot x\in\mathcal{X}\). In more general cases, the i-th element of \(x_{i}^{(G(x))}\) is

\[x_{i}^{(G(x))}=\begin{cases}1,&\textbf{if}\quad G_{i}(x)=1.\\ *,&\textbf{if}\quad G_{i}(x)=0.\end{cases}\] (8)

where \(*\) can be an arbitrary identifiable indicator that represents the variable is not selected.

On the other hand, once the outputs \(G(x)\) of the selector have been recorded, \(*\) can be replaced by any label-independent variable \(G(x)\odot z\), where \(z\sim p_{z}(\cdot)\) is outcome-independent. Then \(x^{(G(x))}\) can be regarded as a new sample and be fed into the variable selector, resulting in a hierarchical variable selection process:

\[\begin{array}{l}x^{(1)}=(G(x)\odot x)\oplus(G(x)\odot z),\\ x^{(2)}=(G(x^{(1)})\odot x^{(1)})\oplus(G(x^{(1)})\odot z),\\...\\ x^{(n)}=(G(x^{(n-1)})\odot x^{(n-1)})\oplus(G(x^{(n-1)})\odot z),\end{array}\] (9)

where \(z\sim p_{z}(\cdot)\), and \(\oplus\) is the element-wise sum operator. Moreover, if the distribution of irrelevant variable \(p_{x}(\cdot)\) is known, applying the variable selection operator obtained from Equation (7) for multiple times with \(p_{z}(\cdot)\stackrel{{ d}}{{=}}p_{x}(\cdot)\) has the meaning of hierarchical variable selection: after each operation, the most obvious \(1-p_{r}\) irrelevant variables are discarded. e.g., when \(p_{r}=0.5\), ideally top-\(50\%,25\%,12.5\%\) most important variables will be selected after the first three selection operations. In this work, a coarse approximation is utilized by selecting \(z\) to be \(z=0\) for simplicity. 3

Footnote 3: \(p_{z}(\cdot)\) may be learned through generative models to approximate \(p_{x}(\cdot)\), and Equation (9) can be regarded as a kind of data-augmentation or ensemble method. This idea is left for the future work.

Combining those two improvements lead to an Iterative Curriculum version of INVASE (IC-INVASE) that addresses the exploration difficulty in high-dimensional variable selection tasks. Curriculum learning helps IC-INVASE to achieve better asymptotic performance, i.e., achieve higher True Positive Rate (TPR) and lower False Discovery Rate (FDR), while iterative application of the selection operator contributes to higher learning efficiency: selectors models with different level of TPR/FDR can be generated on-the-fly.

### State-Wise Action Refinery with IC-INVASE

#### 3.3.1 Temporal Difference State-Wise Action Refinery

With the techniques introduced in the previous section, higher dimensional variable selection tasks can be better solved, therefore we are ready to use IC-INVASE to solve Equation (6). The resulting algorithm is called Temporal Difference State-Wise Action Refinery (TD-SWAR).

In this work, TD3 [10] is used as the basic algorithm we build TD-SWAR up on. In addition to the policy network \(\pi_{\nu}\), double critic networks \(C_{\phi_{1}}\), \(C_{\phi_{2}}\) and their corresponding target networks used in vanilla TD3, TD-SWAR includes an action selector model \(G_{\theta}\) and two baseline networks \(B_{\psi_{1}}\), \(B_{\psi_{2}}\) following [38] to reduce the variance in policy gradient learning. Pseudo-code for the proposed algorithm is shown in Algorithm 1. And the block diagram in Figure 1 illustrates how different modules in TD-SWAR updates their parameters.

#### 3.3.2 Static Approximation: Model-Based Action Selection

While IC-INVASE can be formally integrated with temporal difference learning, the learning stability is not guaranteed. Different from general regression tasks where the label for every instance is fixed across training, in temporal difference learning, the regression target is closely related to the present critic function \(C_{\phi}\), the policy \(\pi_{\nu}\) that generates the transition tuples used for training, and the selector model of IC-INVASE itself. In this section, a static approach is proposed to approximately solve the challenge of instability in TD-SWAR 4.

Footnote 4: Analysis on the approximation is provided in Appendix A

Other than applying the IC-INVASE algorithm to solve Equation (6), another way of leveraging IC-INVASE in action space pruning is to combine it with the model-based methods [11, 16, 13, 14], where a dynamic model \(\mathcal{P}:\mathcal{S}\times\mathcal{A}\mapsto\mathcal{S}\) is learned through regression:

\[\mathcal{P}=\arg\min_{\mathcal{P}}\mathbb{E}_{(s,a,s^{\prime})\sim\pi,\mathcal{ T}}(s^{\prime}-\mathcal{P}(s,a))^{2}\] (10)

Although the task of precise model-based prediction is in general challenging [24], in this work, we only adopt model-based prediction in action selection, and the target is action discovery other than precise prediction. As the dynamic models are always static across learning, such an approach can be much more stable than TD-SWAR. We name this method as Dyn-SWAR and present the pseudo-code in Algorithm 2, where we infuse IC-INVASE to Equation (10) and get the learning objective:

\[\min_{G,\mathcal{P}}\mathbb{E}_{(s,a,s^{\prime})\sim\pi,\mathcal{T}}(s^{ \prime}-\mathcal{P}(s,a^{(G(a|s))}))^{2}\] (11)

## 4 Experiment

In this section, we apply our proposed methodologies to five continuous control RL tasks characterized by redundant action spaces, wherein our methods facilitate causality-aware RL. We also present a quantitative comparison between IC-INVASE and the standard INVASE on synthetic datasets in Appendix B, which serves to underscore the enhanced scalability of our approach.

In the present set of experiments, we employed five RL environments (Figure 5), detailed in Table 15. The symbol \(|\mathcal{S}|\) designates the dimension of the state space for each task, while \(|\mathcal{A}|\) signifies the dimension of the action space relevant to the task, and \(|\mathcal{A}_{red.}|\) represents the dimension of the redundant action space incorporated into each task. These surplus dimensions of actions don't impact state transitions or reward calculations, but it is essential for an agent to identify these redundant dimensions for efficient learning.

Footnote 5: For comprehensive descriptions of the environments, please consult Appendix C

We assessed both TD-SWAR, which combines IC-INVASE with temporal difference learning, and its static counterpart, Dyn-SWAR, which employs IC-INVASE in dynamics prediction. The results are benchmarked against two base conditions: the **Oracle**, where redundant action dimensions are

Figure 3: Environments used in experimentsmanually removed; and **TD3**, which is the standard TD3 algorithm devoid of any explicit action redundancy reduction.

In our experimental findings, we observed that Dyn-SWAR's deployment demonstrates superior efficiency with respect to both sample complexity and computational cost. In contrast, TD-SWAR requires a persistent update of all parameters for the IC-INVASE selector to maintain congruence with the real-time policy and value networks, given the fluctuating regression label over time. However, the Dyn-SWAR selector necessitates a significantly reduced data set for training, specifically between 10,000 and 25,000 timesteps of environmental interaction. This attribute can seamlessly integrate with the warm-up technique utilized in TD3 [10]. Namely, the Dyn-SWAR selector could be trained with warm-up transition tuples gathered during the random exploration phase, and then remain static throughout the subsequent learning process. Compared to traditional RL configurations that generally require millions of environmental interactions, the training of Dyn-SWAR incurs only a minuscule computational cost.

These findings are illustrated in Figure 4. Across all environments, agent learning with IC-INVASE in both TD- and Dyn- methods exceeds the performance of the standard TD3 baseline. Dyn-SWAR achieves a learning efficiency that is on par with oracle benchmarks. However, the performance of TD-SWAR in tasks of higher dimensions (Walker2d-v2 and BipedalWalker-v3) indicates significant potential for enhancement. Accordingly, future work should prioritize enhancing the stability and scalability of instance-wise variable selection within temporal difference learning.

\begin{table}
\begin{tabular}{l l l l} \hline \hline Task/Dimension & \(|\mathcal{S}|\) & \(|\mathcal{A}|\) & \(|\mathcal{A}_{red.}|\) \\ \hline Pendulum-v0 & 3 & 1 & 100 \\ FourRewardMaze & 2 & 2 & 100 \\ LunarLanderContinuous-v2 & 8 & 2 & 100 \\ BipedalWalker-v3 & 24 & 4 & 100 \\ Walker2d-v2 & 17 & 6 & 100 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Tasks used in evaluating SWAR in temporal difference learning

Figure 4: Performance of agents in five different environments. The curves shows averaged learning progress and the shaded areas show standard deviation.

Related Work

Instance-Wise Feature SelectionWhile traditional feature selection method like LASSO [31] aims at finding globally important features across the whole dataset, instance-wise feature selection try to discover the feature-label dependency on a case-by-case basis. L2X [5] performs instance-wise feature selection through mutual information maximization with the technique of Gumbel softmax. L2X requires pre-determined hyper-parameter \(k\) to indicate how many features should be selected for each instance, which limits its performance while the number of label-relevant features varies across instances. In this work, we build our instance-wise action selection model on top of INVASE [38], where policy gradient is applied to replace the Gumbel softmax trick and the size of chosen features per instance is more flexible. [32] considers instance-wise feature selection problems in time-series setting, and build generative models to capture counterfactual effects in time series data. Their work enables evaluation of the importance of features over time, which is crucial in the context of healthcare. [18] formally defines different types of feature redundancy and leverages mutual information maximization in instance-wise feature group discovery and introduces theoretical guidance to find the optimal number of different groups.

Our work is distinguished from previous works for instance-wise feature selection in two aspects. First, while previous works focus on static scenarios like classification and regression, this work focus on temporal difference learning where there is no static label. Second, the scalability of previous methods in variable selection is challenged as there might exist hundreds of redundant actions in the context of RL.

Dimension Reduction in RLIn the context of RL, attention models [33] have been applied to interpret the behaviors of learned policies. [30] proposes to perceive the state information through a self-attention bottleneck in vision-based RL tasks, which concentrates on the state space redundancy reduction with image inputs. The work of [22] also applies the attention mechanism to learn task-relevant information. The proposed method achieves state-of-the-art performance on Atari games with image input while being more understandable with top-down attention models.

Different from those papers, this work considers relatively tight state representations (vector input), and focuses on the task-irrelevant action reduction. We aim at finding the task-related actions and improving the learning efficiency without wasting samples in learning the task-irrelevant dimensions of actions. Our work is most closely related to AE-DQN [39] in that we both consider the problem of redundant action elimination. AE-DQN tackles action space redundancy with an action-elimination network that eliminates sub-optimal actions. Yet its discussion is limited in the discrete settings. In contrast, our work focuses on action elimination in continuous control tasks.

## 6 Conclusion and Future Work

In this study, we address the issue of pruning the action space in action redundant RL tasks. We employ the recent advancements in instance-wise feature selection technology (INVASE), incorporating both curriculum learning and iterative processes, to aim for improved scalability and efficiency. This leads to the creation of the IC-INVASE method, which is then adapted to the RL environment where we introduce two novel algorithms, TD-SWAR and Dyn-SWAR, to implement causality-conscious RL. The former algorithm directly addresses the issue of action redundancy in temporal difference learning, whereas the latter algorithm leverages model-based prediction to capture dynamic causality. Experimental evidence from a range of tasks underscores the importance of causality-awareness for RL agents to achieve efficient learning in action-redundant settings.

As for future research, the iterative characteristic of this method could be further investigated to apply ensemble methods in variable selection. Additionally, the design of a more appropriate curriculum could enhance the fusion of multiple curricula. From the RL perspective, the stability of TD-SWAR could be further optimized to enhance sample efficiency. The design of the curriculum could potentially offer benefits. For instance, an agent might initially learn to identify actions of general importance before concentrating on discerning state-dependent crucial actions. Furthermore, the selection process can be extended to include both the state space and action space, allowing for efficient temporal difference learning that is mindful of the causal relationships among states, actions, and the task at hand. Additionally, model-based prediction could be broadened to anticipate future returns.

## References

* Abadi et al. [2016] Martin Abadi, Paul Barham, Jianmin Chen, et al. Tensorflow: A system for large-scale machine learning. In _OSDI_, 2016.
* Andrychowicz et al. [2020] OpenAI: Marcin Andrychowicz, Bowen Baker, Maciek Chociej, et al. Learning dexterous in-hand manipulation. _IJRR_, 2020.
* Bengio et al. [2009] Yoshua Bengio, Jerome Louradour, et al. Curriculum learning. In _ICML_, 2009.
* Berner et al. [2019] Christopher Berner, Greg Brockman, Brooke Chan, et al. Dota 2 with large scale deep reinforcement learning. _arXiv:1912.06680_, 2019.
* Chen et al. [2018] Jianbo Chen, Le Song, et al. Learning to explain: An information-theoretic perspective on model interpretation. _arXiv:1802.07814_, 2018.
* Chollet et al. [2015] Francois Chollet et al. Keras. https://github.com/fchollet/keras, 2015.
* Czarnecki et al. [2018] Wojciech Marian Czarnecki, Siddhant M Jayakumar, Max Jaderberg, et al. Mix&match-agent curricula for reinforcement learning. _arXiv:1806.01780_, 2018.
* de Haan et al. [2019] Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In _NeurIPS_, pages 11698-11709, 2019.
* Even-Dar et al. [2006] Eyal Even-Dar, Shie M., et al. Action elimination and stopping conditions for the multi-armed bandit and reinforcement learning problems. _JMLR_, 2006.
* Fujimoto et al. [2018] Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. _arXiv:1802.09477_, 2018.
* Ha and Schmidhuber [2018] David Ha and Jurgen Schmidhuber. World models. _arXiv:1803.10122_, 2018.
* Haarnoja et al. [2018] Tuomas Haarnoja, Aurick Zhou, et al. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. _arXiv:1801.01290_, 2018.
* Hafner et al. [2019] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, et al. Dream to control: Learning behaviors by latent imagination. _arXiv:1912.01603_, 2019.
* Janner et al. [2019] Michael Janner, Justin Fu, Marvin Zhang, et al. When to trust your model: Model-based policy optimization. In _NeurIPS_, 2019.
* Konda and Tsitsiklis [2000] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In _NeurIPS_, 2000.
* Langlois et al. [2019] Eric Langlois, Shunshi Zhang, Guodong Zhang, et al. Benchmarking model-based reinforcement learning. _arXiv:1907.02057_, 2019.
* Lillicrap et al. [2015] Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, et al. Continuous control with deep reinforcement learning. _arXiv:1509.02971_, 2015.
* Masoomi et al. [2020] Aria Masoomi, Chieh Wu, Tingting Zhao, et al. Instance-wise feature grouping. _NeurIPS_, 2020.
* Matiisen et al. [2019] Tambet Matiisen, Avital Oliver, et al. Teacher-student curriculum learning. _TNNLS_, 2019.
* Melnik et al. [2015] Andrew Melnik, Luca Lach, Matthias Plappert, et al. Tactile sensing and deep reinforcement learning for in-hand manipulation tasks.
* Mnih et al. [2015] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, et al. Human-level control through deep reinforcement learning. _Nature_, 2015.
* Mott et al. [2019] Alexander Mott, Daniel Zoran, et al. Towards interpretable reinforcement learning using attention augmented agents. In _NeurIPS_, 2019.
* Paszke et al. [2017] Adam Paszke, Sam Gross, Soumith Chintala, et al. Automatic differentiation in pytorch. 2017.
* Sharma et al. [2019] Archit Sharma, Shixiang Gu, Sergey Levine, et al. Dynamics-aware unsupervised discovery of skills. _arXiv:1907.01657_, 2019.

* Silver et al. [2014] David Silver, Guy Lever, et al. Deterministic policy gradient algorithms. In _ICML_, 2014.
* Silver et al. [2016] David Silver, Aja Huang, Chris J Maddison, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 2016.
* Sun et al. [2018] Peng Sun, Xinghai Sun, Lei Han, et al. Tstarbots: Defeating the cheating level builtin ai in starcraft ii in the full game. _arXiv:1809.07193_, 2018.
* Sun et al. [2020] Hao Sun, Ziping Xu, Yuhang Song, Meng Fang, Jiechao Xiong, Bo Dai, and Bolei Zhou. Zeroth-order supervised policy improvement. _arXiv preprint arXiv:2006.06600_, 2020.
* Sutton and Barto [1998] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. 1998.
* Tang et al. [2020] Yujin Tang, Duong Nguyen, and David Ha. Neuroevolution of self-interpretable agents. _arXiv:2003.08165_, 2020.
* Tibshirani [1996] Robert Tibshirani. Regression shrinkage and selection via the lasso. _JRSS_, 1996.
* Tonekaboni et al. [2020] Sana Tonekaboni, S. Joshi, et al. What went wrong and when? instance-wise feature importance for time-series black-box models. _NeurIPS_, 2020.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, et al. Attention is all you need. In _NeurIPS_, 2017.
* Vinyals et al. [2019] Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. _Nature_, 2019.
* Weinshall et al. [2018] Daphna Weinshall, Gad Cohen, et al. Curriculum learning by transfer learning: Theory and experiments with deep networks. _arXiv:1802.03796_, 2018.
* Williams [1992] Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine learning_, 8(3-4):229-256, 1992.
* Xu et al. [2020] Benfeng Xu, L. Zhang, et al. Curriculum learning for natural language understanding. In _ACL_, 2020.
* Yoon et al. [2018] Jinsung Yoon, James Jordon, and Mihaela van der Schaar. Invase: Instance-wise variable selection using neural networks. In _ICLR_, 2018.
* Zahavy et al. [2018] Tom Zahavy, Matan Haroush, et al. Learn what not to learn: Action elimination with deep reinforcement learning. In _NeurIPS_, 2018.