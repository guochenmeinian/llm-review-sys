# Laughing Hyena Distillery

Extracting Compact Recurrences From Convolutions

 Stefano Massaroli\({}^{\ast,1}\), Michael Poli\({}^{\ast,2}\), Daniel Y. Fu\({}^{\ast,2}\),

**Hermann Kumbong\({}^{2}\)**, **Rom N. Parnichkun\({}^{3}\)**, **Aman Timalsina\({}^{4}\)**,

**David W. Romero\({}^{5}\)**, **Quinn McIntyre\({}^{2}\)**, **Beidi Chen\({}^{6}\)**, **Atri Rudra\({}^{7}\)**, **Ce Zhang\({}^{8}\)**,

**Christopher Re\({}^{2,\dagger}\), **Stefano Ermon\({}^{2,\dagger}\)**, **Yoshua Bengio\({}^{1,\dagger}\)**

\({}^{\ast}\)Equal contribution. \(\dagger\) Equal senior authorship. \({}^{1}\)Mila and Universite de Montreal. \({}^{2}\)Stanford University.

\({}^{3}\)The University of Tokyo. \({}^{4}\)Purdue University. \({}^{5}\)Virje Universiteit Amsterdam. \({}^{6}\)Camegie Mellon University

and Meta AI (FAIR). \({}^{7}\)University of Buffalo, SUNY. \({}^{8}\)University of Chicago and Together Computer.

###### Abstract

Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, _long_ convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads - naively requiring a full pass (or _caching_ of activations) over the input sequence for each generated token - similarly to attention-based models. In this paper, we seek to enable \(\mathcal{O}(1)\) compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into _heads_, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves \(10\times\) higher throughput than Transformers and \(1.5\times\) higher than Hyena at \(1.3\)B parameters, without any loss in quality after distillation.

## 1 Introduction

Attention-free approaches such as _long convolution sequence models_ (LCSMs), e.g., H3 [1], Hyena [2], have shown promise in matching Transformer [3; 4] performance across a wide range of tasks, with sub-quadratic complexity with respect to sequence length. Despite the improved efficiency during training on long sequences, unless the convolution filters are either _short_ or admit a _low_-dimensional state-state-space realization, LCSMs still need to process the entire growing sequence at every step of auto-regressive generation, similarly to Transformers.

In this work, we seek to refine LCSMs in both **efficiency** and **quality**. First, we study the inference stage, and propose methods to enable a _recurrent_ mode for auto-regressive generation. Recurrent modes prescribe the existence of a _state_ encoding the past information of the process in a fixed-dimension memory, enabling **constant per-step time** and **constant-memory** in generation. Then, we draw upon an analysis of pre-trained models to develop architectural enhancements for the Hyena block, simultaneously improving model quality and efficiency of the distillation procedure.

Distilling fast recurrencesWe introduce LaughingHvena, the first distillation approach for LCSMs that enables recurrent inference without impacting downstream quality. LaughingHvena seeks compact recurrences in the form of _state-space models_ (SSMs) [5; 6] as the solution of a nonlinear interpolation problem involving the convolution filters of a pre-trained model. Since the total memory cost of SSMs grows linearly in the state dimension \(d\), our distillation procedure enables high throughput by enabling processing of large batches during generation. We identify and address three core challenges related to distillation, including the identification of:* **Target state dimension:** we identify candidate state dimensions of our distilled SSMs by analyzing the spectrum of the Hankel operator associated with each convolution [7].
* **Parametrization:** we address issues with naive parametrizations by introducing a factorized _modal_ form, inspired by barycentric [8] and Prony-like [9] methods.
* **Approximation metric:** to ensure compatibility with any downstream task, we choose discrepancy metrics on the convolution filter, rather than model outputs.

In auto-regressive workloads, LaughingHyena-distilled models with state dimension \(d\) can generate \(K\) tokens in \(\mathcal{O}(dK)\) time and with constant \(\mathcal{O}(d)\) memory - improving over the \(\mathcal{O}(K^{2})\) time and \(\mathcal{O}(K)\) memory usage of _kv-cached_ Transformers and naively executed long convolutions. At model sizes above one billion parameters, LaughingHyena achieves \(10\times\) higher peak throughput over comparable Transformers (Figure 1.1), and can process larger batch sizes. Constant memory generation enables larger \(K\) for a given a memory constraint e.g., generating \(512\) tokens with LaughingHyena requires \(3\times\) less memory than with a Transformer. At smaller batch sizes, latency of LaughingHyena is also competitive with Transformers, reaching \(\geq 2\times\) speedups at longer prompt lengths.

Improving pre-training qualityWe leverage our analysis of the distillation process to open up new avenues of improvement for LCSM architectures. Indeed, the high compression rates achievable through LaughingHyena hint at sub-utilization of the convolution. We revisit the multi-headed design of H3 [1]; tying weights across channels pushes long convolution filters towards larger effective dimension, and as an additional advantage reduces the runtime of post-training distillation and inference memory footprint. Further, multi-head Hyena models improve on pre-training perplexity over regular Hyena and GPT [10] architectures on the language dataset The Pile [11].

## 2 Preliminaries and Related Work

We discuss convolutions, state spaces and auto-regressive generation workloads for sequence models.

ConvolutionsLet \(*\) denote the convolution operator. It is defined as the dual operation to pointwise multiplication under Fourier transform. In signal processing and deep learning alike, one often encounters the causal linear convolution of a filter \(h\) (which may extend indefinitely) with an input \(u\) of length \(L\):

\[(h*u)_{t}=\sum_{j=0}^{t}h_{t-j}u_{j}.\] (2.1)

Generally, \(u_{t}\in\mathbb{R}^{D}\) where \(D\) is the width of the signal - or in deep learning parlance - the number of _channels_. Without loss of generality, we specialize our analysis to _single input single output_ layers, i.e. with \(D=1\). For the input-output relations of type (2.1), we use the terms _convolution layer_ and _linear system_ interchangeably. Similarly, the function \(t\mapsto h_{t}\) is referred to as both the _filter_ and the _impulse response_ of a linear system. Existing convolution sequence models can be classified in terms of the parametrization used for their filters. The class of _implicit_ convolutions represent the filter as a parametric function \(\gamma_{\theta}:t\mapsto h_{t}\).

State-space realizationOne option is to select \(\gamma_{\theta}\) as the _impulse response_ function of a discrete linear time-invariant system,

\[\begin{split} x_{t+1}&=\mathsf{A}x_{t}+\mathsf{B}u_ {t}\\ y_{t}&=\mathsf{C}x_{t}+h_{0}u_{t}\end{split}, \quad t\mapsto h_{t}=\begin{cases}h_{0}&t=0\\ \mathsf{C}\mathsf{A}^{t-1}\mathsf{B}&t>0\end{cases}\] (2.2)

with _state_\(x_{t}\in\mathbb{R}^{d}\), _input_\(u_{t}\in\mathbb{R}\), and _output_\(y_{t}\in\mathbb{R}\). The matrices \(\mathsf{A}\in\mathbb{R}^{d\times d}\), \(\mathsf{B}\in\mathbb{R}^{d\times 1}\), \(\mathsf{C}\in\mathbb{R}^{1\times d}\), and \(h_{0}\in\mathbb{R}\) are the learnable parameters of the model while the initial state \(x_{0}\) is usually set to zero such that \(u\mapsto y\) is a pure convolution. While linear systems (2.2) are the staple of signal processes and control theory, their use as implicit parametrization of convolution filters in deep neural networks have only recently emerged [12; 6]. Other parametrizations [13; 14; 2] select \(\gamma_{\theta}(t)\) as different flavors

Figure 1.1: Throughput (in generated tokens) of Transformers, H3 and Hyena models. LaughingHyena is a recurrent model distilled from a pre-trained Hyena. Workload involves generating \(256\) tokens given a prompt of length \(512\).

of implicit representation neural networks [15; 16]. The latter are generally more powerful in terms of the class of filters they can represent and flexibility during training, at the cost of losing a fixed state dimension.

### Long Convolution Sequence Models

The \(\mathsf{H}\)-family of convolution sequence models \(-\mathsf{H3}\)[1] and \(\mathsf{Hyena}\)[2] - relies on a combination of long convolutions and data-controlled gating to replace attention with sub-quadratic scaling in sequence length1. We use the deep learning convention of naming different projections as _query_\(q\), _key_\(k\) and _value_\(v\). Let \(\mathsf{M}_{q}\) and \(\mathsf{M}_{k}\) be the \(L\)-by-\(L\) diagonal matrices whose respective main diagonal entries are the respective entries of length-\(L\) sequences \(q\) and \(k\). A \(\mathsf{H}\)-block realizes a surrogate attention matrix with a data-controlled, parameterized decomposition in three terms:

Footnote 1: In this work, we consider second-order \(\mathsf{Hyena}\) blocks [2] to automatically extend our findings to \(\mathsf{H3}\)[1].

\[(q,k,v)\mapsto\mathsf{H}(q,k)v,\quad\mathsf{H}(q,k)=\mathsf{M}_{q}\mathsf{ T}_{h}\mathsf{M}_{k}\] (2.3)

where \(\mathsf{T}_{h}{\in}\mathbb{R}^{L\times L}\) is the Toeplitz matrix constructed from the learnable long convolution filter \(h\), i.e., \(\mathsf{T}_{h}{=}(h_{i-j})_{i,j=0}^{L-1}\). The \(qkv\)-projections are themselves the output of a convolution between the input sequence and three distinct _short_ filters. The degrees of freedom in \(\mathsf{H}\)-block design are the three short filters2 and the _long_ filter \(h\). The long filter can be parameterized using an implicit neural representation [2], state-space model [1], or explicit values [17]. The threefold decomposition of the attention operator, allows evaluation of (2.3) in just \(\tilde{\mathcal{O}}(L)\coloneqq\mathcal{O}(L\log_{2}L)\) time (two convolutions3 and two element-wise products), \(y_{t}{=}q_{t}(h\ast kv)_{t}\). The overall operator acts on an input \(u\) by constructing a third-order multi-variate polynomial of \(u\) whose coefficients are controlled (nonlinearly) by parameters of the block.

Footnote 2: The short filters are _explicitly_ parameterized, see [2].

Footnote 3: The \(qkv\) short convolutions can be evaluated in batch with a single pass. The second convolution is the one with the long filter \(h\) and performed via Fast Fourier Transform (FFT), hence the \(\tilde{\mathcal{O}}(L)\) complexity.

### Auto-Regressive Generation

A typical workload for sequence models is auto-regressive generation. Given a length-\(T\)_prompt_\(u\in\mathbb{R}^{T}\), the model is tasked with producing the following \(K\) additional outputs - one at a time - for a resulting output sequence \(y\) of length \(L{=}T{+}K\).

Convolution sequence modelsAfter processing the initial prompt in \(\tilde{\mathcal{O}}(T)\) time and obtaining a length-\(T\) output \(u\mapsto y_{0},\ldots,y_{T-1}\), a generic convolution layer can _cache_ the output sequence and generate any additional outputs using (2.1) auto-regressively, i.e. \(y_{t+1}{=}\sum_{j=0}^{t}h_{t-j}y_{j}\) for \(t{=}T{-}1,\ldots,T{+}K{-}1\). It is important to note that auto-regressive generation with generic long convolutions is expensive. It comes with a **quadratic** cost in the number \(K\) of tokens to be generated and require storing a cache of length up to \(L\).

**Lemma 2.1**.: _Generating \(K\) tokens with a long convolution layer (2.1) from a length-\(T\) prompt has time complexity \(\mathcal{O}(T\log_{2}T+TK+K^{2})\) and requires \(\mathcal{O}(L)\) memory._

State-space modelsWhen the linear system admits a state space realization (2.2), i.e. it is able to switch between convolution and recurrent mode, the cost of auto-regressive generation can be dramatically reduced. The memory footprint is \(\mathcal{O}(d)\): all we need to cache is the state \(x_{t}\), a \(d\)-dimensional vector. With some further machinery that we develop in next section, we can retain \(\tilde{\mathcal{O}}(T)\) time and \(\mathcal{O}(T)\) memory to process the prompt4 and initialize the state \(x_{T-1}\). Each additional generation step only requires \(\mathcal{O}(d)\) time.

Footnote 4: In §3.4 we show that multiple pre-filling strategies exist, with different trade-offs in time and memory.

**Lemma 2.2**.: _Generating \(K\) tokens with a state-space model (2.2) from a length-\(T\) prompt has time complexity \(\mathcal{O}(T\log_{2}T{+}dK)\) and requires \(\mathcal{O}(T+d)\) memory._

Note that long filters \(h\) truncated to length \(d\) (i.e. \(h_{t}{=}0\) for \(t>d-1\)) can also be interpreted as \(d\)-dimensional SSMs (see Appendix A.7) where the state (a cache) coincides with the last \(d\) inputs.

TransformersSelf-attention is certainly less efficient than long convolutions in processing the prompt, coming with a hefty \(\mathcal{O}(T^{2})\) time complexity. However, Transformers can achieve a similar

Figure 2.1: \(\mathsf{H}\)-block. \(\mathsf{T}^{(q)}\), \(\mathsf{T}^{(k)}\), \(\mathsf{T}^{(v)}\) are _short_-convolution operators.

efficiency in auto-regressive generation by **caching** the sequences of past keys \(\{k_{t}\}\) and values \(\{v_{t}\}\). Specifically, from \(t{=}T{-}1\) onward, the new projections \((q_{t{+}1},k_{t{+}1},v_{t{+}1})\) are evaluated from the current output \(y_{t}\), and the new output \(y_{t+1}\) can be computed in linear time with two reductions

\[y_{t+1}=\frac{\sum_{j=0}^{t+1}\varphi(q_{t{+}1}k_{j})v_{j}}{\sum_{i=0}^{t+1} \varphi(q_{t{+}1}k_{j})}\quad\text{where }\varphi:\mathbb{R}\to\mathbb{R}\text{ is usually chosen as }\varphi(x)=e^{x}.\]

**Lemma 2.3**.: _Generating \(K\) tokens with self-attention from a length-\(T\) prompt has time complexity \(\mathcal{O}(T^{2}{+}TK{+}K^{2})\) and requires \(\mathcal{O}(L)\) memory._

## 3 The Laughing Hyena Distillery

In this section, we introduce our distillation method. We discuss choosing an approximation objective, a parametrization for the approximant and setting a target state dimension.

Given any pre-trained LCSM, the objective of the distillation procedure is to convert each pre-trained convolution filter into a distinct state-space model (2.2). This should be achieved with the smallest state dimension \(d\) which preserves, up to a certain tolerance, the input-output characteristics of the convolution layer. Formally, given a filter \(h\) the **distillation problem** is defined as follows.

Given the sequence \(h_{1},\dots,h_{L}\), find a state-space model (2.2) of dimension \(d\ll L\), whose input-output behavior _approximates_ the one of the convolution with \(h\) over the largest class of input sequences.

The choice of approximation metrics and assumptions on the input sequences yield different _distillation objectives_. A _distillation algorithm_ constitutes a systematic procedure for optimally choosing the systems matrices with respect to a particular objective. In instances where the original filter \(h\) is itself the impulse response of a finite-dimensional state-space model, e.g., when attempting distillation of H3 or S4[6] filters, the term distillation becomes analogous to _model-order reduction_. Hence, in such cases, the distillation algorithm should yield a state-space representation of a lower order state-dimension.

There exist several algebraic solutions to the model reduction problem [18; 19; 20], typically seeking low-rank structures of the state space by inspecting some invariant of the system, e.g. the _Gramians_ in _balanced truncation_[19; Ch. 7]. The lower-order system is then obtained as a projection of the system dynamics onto the found subspace where the system retains desired characteristics, e.g., input-output behavior, stability, etc.

Truncated filtersIn theory, implicitly parameterized convolution filters can represent arbitrarily long signals. In practice, these filters are trained on a fixed _maximum length_\(L\). At inference time the model can then be evaluated for sequences longer than \(L\). During distillation it is nonetheless reasonable to treat the pre-trained filters as potentially very long (even beyond \(L\)) but _finite_ impulse response functions [21; 22; 23; 24]. We show how this choice is supported by empirical evidence displaying how pre-trained filters typically decay to zero in finite time (see Appendix D).

Transfer function representationAn alternative description of the system (2.2) is its _transfer function_\(H\), defined as the \(z\)-transform of the impulse response \(H(z){=}\sum_{t=0}^{\infty}h_{t}z^{-t}\) for all \(z{\in}\mathbb{C}\) where the sum converges. The transfer function is a _proper rational function_ of \(z\)

\[H(z)=h_{0}+\mathbb{C}(z{\rm l}-\mathsf{A})^{-1}\mathsf{B}=h_{0}+\frac{b_{1}z^{ -1}+\ \cdots\ +b_{d}z^{-d}}{1+a_{1}z^{-1}+\ \cdots\ +a_{d}z^{-d}}.\] (3.1)

In the \(z\)-domain, the transfer function defines the input-output map as \(Y(z)=H(z)U(z)\). Here, \(H(z)\) is defined outside the \(\mathbb{C}\)-plane circle of radius \(\rho(\mathsf{A})\), \(\mathbb{D}_{\rho(\mathsf{A})}{:=}\{z\in\mathbb{C}:|z|>\rho(\mathsf{A})\}\) where \(\rho(\mathsf{A})\) is the spectral radius of \(\mathsf{A}\), i.e. the amplitude of its largest eigenvalue. We can recover all characteristics of a given system equivalently from either its transfer function or state-space representations (see Appendix A.3 for further details and derivations). Notably, the transfer function is an _invariant_ of the system: if we apply a change of variables to the state, the transfer function remains unchanged (Lemma A.3). This alone should discourage attempts at modeling filters by learning _dense_ state-space matrices \(\mathsf{A},\mathsf{B},\mathsf{C}\) as such: there are infinitely many equivalent state-space realizations that map to the same system. Starting from coefficients \((a_{i})\) and \((b_{i})\) of the rational transfer function (3.1), we can compute the impulse response in \(\tilde{\mathcal{O}}(L)\) time (Lemma A.6). Moreover, we can map back the transfer function to a special state-space realization - the _companion_ canonical form - whose recurrence has time complexity \(\mathcal{O}(d)\) (Lemma A.7), compared to the \(\mathcal{O}(d^{2})\) of dense state-space matrices. From Lemmas A.3 and A.7 we can also prove that any stable state-space model can be converted by _canonicalization_ into its companion form, and thus can be equipped with an efficient recurrence (Thm. A.8).

The distillation problem presents several challenges:

1. **Defining the distillation objective.** A primary decision involves selecting a distillation objective. We are primarily interested in metrics of pure discrepancy between each filter of a pre-trained deep model and its approximator, rather than the expected input-output loss over a distribution of inputs.
2. **Choosing a state-space parametrization.** It is crucial to determine a suitable parametrization of the distilled state-space realization. Once this is decided, the task is to identify the parameters that minimize the distillation desiderata, which can involve challenging optimization problems in itself.
3. **Selecting the target state dimension.** Lastly, a challenge is to estimate the degree to which the model's order can be reduced. In other words, we must select the target state dimension of the distillation process to identify the right trade-off between efficiency and accuracy.

In the following, we address each of these challenges, and provide a comprehensive approach (summarized in Figure 3.1) to distill recurrences from convolution-based architectures.

### Data-Free Distillation Objectives

We focus on distillation objectives that are independent of the training data and the overall architecture of the neural network under consideration. The distillation loss should be chosen as a pure measure of discrepancy between each convolution filter \(h_{t}\) of the model and their finite-dimensional approximations \(\hat{h}_{t}=\mathsf{C}\mathsf{A}^{t-1}\mathsf{B}\). This approach ensures that we do not require a full sequential inference pass over the pre-trained model at each step of distillation procedure and the distilled model can be more broadly applied to downstream tasks. This choice is supported by Young's convolution inequality [25; 26], which indicates that the output approximation error has a bound \(\|y-\hat{y}\|_{r}\leq\|h-\hat{h}\|_{q}\|u\|_{p}\) for properly chosen norms5. For maximum numerical stability and freedom of parametrization for the approximants, we favor modern unconstrained gradient-based approaches to then solve the resulting distillation program6. We design distillation algorithms which either match filters in _time domain_ minimizing the \(\ell_{2}\) error (\(\|h\|_{2}\coloneqq[\sum_{t\in\mathbb{Z}}|h_{t}|^{2}]^{1/2}\)) or match their transfer functions optimally with respect to the \(\mathcal{H}_{2}\) norm (\(\|H\|_{2}\coloneqq[(1/2\pi)\int_{-\pi}^{\pi}|H(e^{i\omega})|^{2}\mathrm{d} \omega^{1/2})^{\gamma}\). As the distillation is carried out via gradient methods, \(\ell_{2}\) is a natural candidate. \(\mathcal{H}_{2}\) error minimization can instead be used to uniformy bound the worst-case discrepancy as \(\|h-\hat{h}\|_{\infty}\leq\|H-\hat{H}\|_{2}\) (see Appendix A.2 for further details). Footnote 5: \(p,q,r>0\) should satisfy \(1/q+1/p=1/r+1\). In the case of infinite sequences defined on the all \(\mathbb{Z}\), the norms are taken in a \(\ell_{p},\ell_{q},\ell_{r}\) sense, respectively. The bound is potentially sharp [27; 28]

Footnote 6: For completeness, we also test _balanced_ and _modal_ truncation techniques on a suite of pre-trained H3 and Hrena models in Appendix E.3.

### Making Hyena Laugh with Modal Interpolation

Our degrees of freedoms to solve the distillation problem are the matrices \(\mathsf{A}\), \(\mathsf{B}\), and \(\mathsf{C}\) of the state-space realization, which determine the filter for all \(t>0\). In distilled SSMs, the passthrough (residual) term cannot be freely assigned: it is simply \(h_{0}\), the value of the original filter at zero. Alternatively, given its

Figure 3.1: The LaughingHyena long convolution sequence model distillation blueprint.

appealing invariance properties, we can parametrize a proper rational function \(\hat{H}(z)\) (3.1) and fit it to the (truncated) transfer function8 of the original filter \(H_{L}(z){:=}\sum_{t=0}^{L}h_{t}z^{-t}\) (see Appendix B.2).

Footnote 8: As already partially discussed in [6], the truncation introduces a correction term in the approximant transfer function. See Appendix A.4.

Modal canonical formOptimizing the full transfer function can be numerically challenging for several reasons e.g., ensuring stability9, and ill-posedness for high-order polynomials. A natural solution, inspired by barycentric approaches to rational function approximation [29, 8], is to assume \(d\) distinct roots \(\lambda_{n}\) in the denominator's polynomial, \(\lambda_{n}\in\mathsf{roots}(\mathsf{poly}(a))\).

Footnote 9: i.e normalizing denominator polynomial coefficients to constrain roots within the unit circle.

**Proposition 3.1** ([5]).: _If \(\mathsf{poly}(a)\) has distinct roots \(\{\lambda_{n}\in\mathbb{C}\}\), then the transfer function of the system can be factorized as \(\hat{H}(z){=}\sum_{n=1}^{d}R_{n}/(z-\lambda_{n}),\quad\forall z\in\mathbb{D}_{ \rho(\mathsf{A})}\) where \(\{R_{n}\in\mathbb{C}\}\) is the residue associated with the pole \(\lambda_{n}\)._

Computing the inverse transform of the expanded transfer function via, e.g., the _Cauchy residue theorem_[30], shows that the resulting impulse response \(\hat{h}\) corresponds to a truncated basis of exponentially decaying complex sinusoids

\[\hat{h}_{t}=\sum_{n=1}^{d}R_{n}\lambda_{n}^{t-1},\quad R_{n},\lambda_{n}\in \mathbb{C},t>0.\] (3.2)

In practice, this corresponds to the impulse response of state-space model with diagonal matrix \(\mathsf{A}=\mathrm{diag}(\lambda_{1},\ldots,\lambda_{d})\) and such that \(\mathsf{B}_{i}\mathsf{C}_{i}=R_{i}\) for all \(i=1,\ldots,d\). The distillation problem can be then defined in terms of the \(L\)-point nonlinear least squares interpolation error (squared \(\ell_{2}\)) between \(h_{1},\ldots,h_{L}\) and (3.2) evaluated for \(t{=}1,\ldots,L\): \(\min_{\{\lambda_{n},R_{n}\}}\|\hat{h}-h\|_{2}^{2}\). Note that in case of the target filter \(h\) being real-valued, the objective can be replaced by \(\|\mathfrak{R}[\hat{h}]-h\|_{2}^{2}\).

Although we find solutions of the distillation (interpolation) problem via modern gradient-based optimization techniques, it is worth mentioning that Prony showed how the nonlinear least square solution can be computed solving two linear problems [9]. However, similar to Pade's method for rational approximation [31], these techniques can be numerically unstable. We opt for a parametrization similar to [32, 33] where each eigenvalue is parameterized in polar form \(\lambda_{n}{:=}A_{n}e^{i\theta_{n}}\) and the residues in cartesian form10. Note that, with this parametrization we have \(\mathfrak{R}[\hat{h}_{t}]=\sum_{n}A_{n}^{t-1}[\mathfrak{R}(R_{n})\cos(\theta _{n}(t-1))-\mathfrak{I}(R_{n})\sin(\theta_{n}(t-1))]\). We can also solve the distillation problem in the \(\mathcal{H}_{2}\) sense by evaluating \(\hat{h}_{t}\) and \(h_{t}\) at \(t=0,\ldots,L-1\) and taking their respective (discrete) Fourier transform before computing the objective. Efficient evaluation of (3.2) is crucial for distillation. In particular we show the following:

Footnote 10: We report additional details on the nuances of the parametrization in Appendix B.1.

**Lemma 3.1**.: _Evaluation of \((\hat{h}_{t})_{t=0}^{L-1}\) (3.2) can be done in \(\mathcal{O}(dL)\) time from its modal form and in \(\hat{\mathcal{O}}(L)\) time from its proper rational form._

### Minimal Distillation Orders

Distilling into lower-dimensional systems is always desirable as they require fewer parameters to be optimized and they yield recurrences that are (linearly) more efficient in terms of time and memory complexity in post-distillation auto-regressive inference workloads. The dimension of _the smallest possible state-space model with impulse response exactly \(\{h_{t}\}_{t\in\mathbb{N}}\)_ is the so-called _McMillan degree_[34]:

\[d^{*}=\arg\min_{d}d\ :\ \exists\mathsf{A}\in\mathbb{C}^{d\times d},\mathsf{B} \in\mathbb{C}^{d\times 1},\mathsf{C}\in\mathbb{C}^{1\times d}\text{ with }h_{t}=\mathsf{ CA}^{t-1}\mathsf{B},\ \forall t>0\] (3.3)

**Theorem 3.1** (Ho-Kalman [35, Theorem 2, Corollary]).: _Let \(\mathsf{S}\) be the (infinite) Hankel matrix constructed with \(h\), i.e. \(\mathsf{S}\coloneqq(h_{i+j})_{i,j=1}^{\infty}\). Then, \(d^{*}=\mathrm{rank}(\mathsf{S})\)._

Figure 3.2: Example of modal interpolation. The approximant is a linear combination of exponentially-decaying complex exponential basis functions with learned decay rate.

A lower bound for \(d^{*}\) can be estimated from a truncated filter of length \(L\) by constructing the \(L\times L\) principal sub-matrix \(\mathsf{S}_{L}\) and using the fact that \(\operatorname{rank}(\mathsf{S})\geq\operatorname{rank}(\mathsf{S}_{L})\). Inspecting how fast the Hankel singular values \((\sigma_{n})_{n=1}^{L}\) decay in pre-trained convolution models can be predictive of the approximation quality at a fixed dimension. As a rule of thumb, \(d\) needs to be sufficiently large for \(\sigma_{d+1}\) to be sufficiently small11. Specifically, we can prove that the _last_ singular value \(\sigma_{d}\) determines the upper bound of distillation quality with a SSM of dimension \(d\), in terms of the Hankel norm [19]. This is a direct consequence of Adamjan-Arov-Krein theorem [7] and can be informally stated as follows.

Footnote 11: Formally, this is related to low-rank approximation characteristics of the Hankel operator; rigorous bounds can be constructed by application of the Eckart–Young–Mirsky theorem [36].

**Theorem 3.2** (Informal).: _Let \(h\) be a length-\(L\) filter, \(\hat{h}\) a distilled filter of order \(d<L\) and let \(\mathsf{S}_{L},\hat{\mathsf{S}}_{L}\) be the respective Hankel matrices. Then \(\inf_{\hat{\mathsf{S}}_{L}}\|\mathsf{S}_{L}-\hat{\mathsf{S}}_{L}\|_{2}=\sigma_ {d}\)._

### Deploying the Recurrence

Once all the filters of a pre-trained model have been distilled with the proposed modal interpolation technique described above, the model unlocks a _recurrent mode_ which allocates a state \(x_{t}\in\mathbb{C}^{d}\) for each filter and enables fast auto-regressive inference. Deployment of distilled model involves two critical steps: the _pre-filling_ and the recurrent update rule itself.

Fast pre-fillingDuring auto-regressive generation, when a length-\(T\) prompt is fed to the model, we need to compute the state \(x_{T}\) to start generating new tokens. Using the recurrence, the time complexity of initializing \(x_{T}\) would be \(\mathcal{O}(dT)\) with a \(\mathcal{O}(d)\) memory footprint. One can alternatively distribute the computation on \(d\) processors with a _parallel scan_ operation [37; 38] to reach a parallel time complexity \(\mathcal{O}(d\log_{2}T)\) while incurring in an increased memory requirement of \(\mathcal{O}(dT)\)12. A third option is to use a single FFT convolution to obtain \(x_{T}\) in \(\tilde{\mathcal{O}}(T)\) time and \(\mathcal{O}(T)\) memory.

Footnote 12: This strategy can also be use to evaluate the filter \(\hat{h}\) alternatively to the standard \(\mathcal{O}(dL)\) method

**Proposition 3.2**.: \(x_{T}=(\nu_{T},\ldots,\nu_{T-d})\) _where \(\nu_{t}=(g*u)_{t}\) and \(g\) is the filter whose transfer function is \(1/\mathsf{den}(\hat{H})(z)\) and can be evaluated in \(\tilde{\mathcal{O}}(T)\)._

Note that, the fast pre-filling algorithm established by this result requires evaluating the denominator polynomial of \(\hat{H}\) from its roots before deployment. This is equivalent to converting the transfer function from its factorized representation to its rational form (3.1).

Recurrent stepThe update rule is diagonal, thus efficiently evaluated in \(\mathcal{O}(d)\) time and memory:

**Proposition 3.3**.: _The filter (3.2) has a state space matrices \(\mathsf{A}=\mathsf{diag}(\lambda_{1},\ldots,\lambda_{d})\in\mathbb{C}^{d \times d},\ \mathsf{B}=(1,\ldots,1)^{\top}\in\mathbb{C}^{d\times 1},\ \mathsf{C}=(R_{1},\ldots,R_{d})\in \mathbb{C}^{1\times d}\) whose step can be evaluated in \(\mathcal{O}(d)\) time and memory._

As we generally want the output \(y_{t}\) to be real-valued, we can simply update the complex state \(x_{t+1}=\mathsf{A}x_{t}+\mathsf{B}u_{t}\) and then take the real part of the output, \(y_{t}=\mathfrak{R}[\mathsf{C}x_{t}]+h_{0}u_{t}\).

## 4 Multi-head Long Convolutions

We can leverage the Hankel spectrum analysis discussed in Section 3.3 to study the dynamics of the effective dimensionality of each convolution filter during LCSMs pre-training. We find that, at initialization, filters correspond to high-dimensional SSMs, and gradually converge to lower-dimensional representations during training. See Appendix E.2 for examples on Hyena and H3 models.

This observation leads to the question: _is it advantageous to perform independent long convolutions on each channel, or can we reduce the total number of filters without loss in quality?_ To answer this, we adapt the multi-head layer design proposed by H3[1] to Hyena[2]:

1. Given the projections \(q,k,v\in\mathbb{R}^{L\times D}\), we split them into \(M\) chunks of size \(N{=}D/M\), \(q^{m},k^{m},v^{m}\in\mathbb{R}^{L\times N}\).
2. Each chunk is processed by a modified Hyena operator: first, we perform the outer product of \(k^{m}\) and \(v^{m}\) along the spatial dimension, \(z^{m}{:=}k^{m}\otimes v^{m}\in\mathbb{R}^{L\times N\times N}\), apply a long convolution with filter \(h^{m}\) to all \(N{\times}N\) elements independently, then compute \(y_{t}^{m}{=}(h^{m}*z^{m})_{t}q_{t}^{m}\), \(y^{m}\in\mathbb{R}^{L\times N}\) as shown in Figure 4.
3. Finally, we compose \(y^{1},\ldots,y^{m}\) into a single output \(y\in\mathbb{R}^{L\times D}\) via concatenation.

An instance of a MultiJena is equipped with \(M<D\) distinct long convolution filters, which leads to (\(a\)) faster distillation, with less filters to approximate, (\(b\)) lower memory footprint, via a total reduction of the states to cache during generation and (\(c\)) faster filter generation, by tying the weights of filter parameters. We note that tying weights of key-value projections has also been shown to be an effective technique to reduce memory cost in Transformers [39; 40].

Crucially, the multi-head structure of MultiJena enables us to prove favorable scaling in the _associative recall_ synthetic task, which was shown in [2] to be predictive of performance at scale. In associative recall, the model is given a sequence of key-value pairs and a query, and is tasked with matching the query to a key in the sequence by returning its associated value. The difficulty of the task grows with the vocabulary size \(s\): larger vocabularies necessitate wider models.

**Theorem 4.1**.: _The MultiJena layer, with \(\mathcal{O}(\log s)\) heads and model size \(\mathcal{O}(\sqrt{s}\log s)\) can solve the associative recall problem, where \(s\) denotes the vocabulary size._

In Appendix E.1, we empirically verify improved scaling in vocabulary size with multiple heads.

## 5 Experiments

* **Pretraining:** We pretrain a suite of MultiJena language models on The Pile [11], investigating scaling of perplexity with different amounts of total tokens (5, 10, 15 billion), as well as larger training runs for 300 billion tokens. MultiJena outperforms Transformers and Hylena.
* **Distillation analysis:** We investigate the relation between optimal distillation orders, Hankel spectrum, and errors on the logits of distilled models.
* **Post-distillation downstreams:** We evaluate the downstream impact of distilling long convolutional language models, reporting HELM [41] and LM-Eval-Harness [42] results.
* **Benchmarking:** We benchmark latency, throughput and memory along the different axes of batch size, sequence length, number of generated tokens. We include base models, distilled models and equivalent Transformers.

### Pre-training

To validate the multi-head formulation, we train \(150\) and \(350\) million parameter MultiJena models on The Pile [11] using \(8\) heads and otherwise the same architecture as equivalent Hylena models, following the setup of [2]. Via the multi-head structure introduced in \(4\), MultiJena outperforms both Hylena and Transformers, including on data scaling runs with increasing numbers of tokens and full \(300\)B tokens runs (Table 5.1).

### Distillation Analysis

Next, we verify whether Hankel singular values are predictive of downstream errors, and whether large models can be distilled without loss in quality. We apply LaughingHylena distillation to pre-trained MultiJena, Hylena and H3 of different sizes. Concretely, for each layer and channel of a model, we parametrize the poles \(\{\lambda_{n}\}\) of the modal canonical forms (Section 3.2) at different orders \(d\), and solve for each \(\ell_{2}\) approximation problem.

Approximation errors and spectrumWe investigate the magnitude of approximation errors introduced by LaughingHylena distillation. Given a pretrained MultiJena model, we compute the errors between original and distilled filters at each layer, averaged across channels. We repeat this process for different distillation orders (state dimension of the model form of Section 3.2). Figure 5.2 visualizes

Figure 4.1: A single head of a multi-head Hylena.

\begin{table}
\begin{tabular}{c|c|c c c c} \hline \hline Model & Perp. & Model & 5B & 10B & 15B \\ \hline GPT & 9.3 & GPT (125M) & 13.3 & 11.9 & 11.2 \\ Hylena & 9.3 & Hylena (153M) & 13.3 & 11.8 & 11.1 \\ MultiJena & **8.7** & MultiJena (153M) & **12.1** & **11.0** & **10.6** \\ \hline \hline \end{tabular} 
\begin{tabular}{c|c c c} \hline \hline Model & 5B & 10B & 15B \\ \hline GPT (355M) & 11.4 & 9.8 & 9.1 \\ Hylena (355M) & 11.3 & 9.8 & 9.2 \\ MultiJena (355M) & **10.6** & **9.4** & **8.9** \\ \hline \hline \end{tabular}
\end{table}
Table 5.1: **[Left]** Perplexity of small models on The Pile, after pre-training for \(300\) billion tokens. **[Center and Right]** Perplexity on The Pile for models trained until a total number of tokens e.g., 5 billion (different runs for each token total).

minimum, maximum and average errors, per-layer errors and the distribution of the singular values of the Hankel operator associated to each filter. We observe distillation orders (\(>16\)) that yield smalls errors to be predicted by the distribution of singular values. Thus, analysis of the Hankel operator's spectrum is verified to be an effective approach to direct estimation of the optimal distillation order. We also note that the optimal order changes across layers, offering options for further optimization.

Output errorsNext, we compute relative \(\ell_{1}\) error between output logits of pre-trained and distilled models to ensure LaughingHyena can be used in generation workloads. The optimal minimal distillation order estimated via Hankel operators (16) is sufficient to keep the output distribution over the vocabulary (\(>50\)k entries) close to the pre-trained model, as shown in Figure 5.2. Inspecting the error profile over logits sorted by magnitude reveals our approach to be robust to different sampling strategies for generation, including greedy decoding, top-\(k\), top-\(p\)[43]. Indeed, the relative errors are \(<\!10^{-2}\) up to and including the \(99.99\%\) percentile of the distribution, meaning e.g., a top-\(p\) sampling strategy with large \(p\) can be used on a distilled model without drift in outputs (mis-classified tokens). We note that the relative errors are maximum on small-norm logits, which are not required by most sampling strategies. In Appendix D.2, we provide a similar distillation error analysis for Hyena and H3 models. We find that Hyena and can be distilled with less than \(32\) orders and H3 with less than \(8\).

### Downstream Evaluation

We check how distillation affects downstream performance on language benchmarks. We apply distillation of order \(8\), \(16\) and \(32\) to our The Pile-pretrained MultiJyena language model and benchmark (Table 5.3) its performance on a suite of canonical (zero shot) tasks from LM-Eval-Harness [42] and HELM [41]. The results are consistent with our error analysis: distillation orders equal or greater to \(16\) introduce little-to-no quality degradation.

Figure 5.1: Errors between logits of pretrained and distilled MultiJyena. In blue, we plot (ordered) logits, in light blue the cumulative distribution function, and in black the relative errors. The green dotted line indicates the 99.99% percentile. As the errors grow slowly as function of the percentiles, model outputs do not diverge from the base model.

\begin{table}
\begin{tabular}{l|c c c c c} \hline \hline Model & LAMBADA & Winogrande & PIQA & HellaSwag & OpenbookQA \\  & acc & acc & acc & acc norm. & acc norm. \\ \hline Pythia (160M) & 32.8 & **53.1** & 61.6 & 31.6 & **29.2** \\ \hline MultiJyena (154M) & **43.2** & 52.7 & **64.6** & **34.1** & 29.0 \\ LaughingHyena-16 & 43.1 & 52.6 & 64.7 & 34.1 & 28.9 \\ LaughingHyena-8 & 0.0 & 51.8 & 51.5 & 32.7 & 28.2 \\ LaughingHyena-4 & 0.0 & 49.6 & 53.7 & 26.4 & 26.4 \\ \hline \hline \end{tabular}
\end{table}
Table 5.2: Evaluation of LaughingHyena-distilled models pre and post modal distillation. We test on LM-Eval-Harness [42] and HELM [41] tasks, reporting Pythia [44] performance as a Transformer baseline trained on the same data. LaughingHyena-\(d\) is a MultiJyena model with each filter distilled of order \(d\).

### Benchmarking

We measure throughput, latency and memory usage of LaughingHyena for auto-regressive generation workloads, with initial prompt length \(T\) and number of generated tokens \(K\). The throughput is computed as number of generated tokens over latency. For each setting (and additional benchmarks), we provide details in Appendix D.4.

Peak throughputDistilled models do not need \(kv\)-caches. This reduces memory requirement during generation, enabling higher peak throughput in large-batch workloads. We achieve \(10\times\) higher throughput than Transformers at size \(1.3\) billion parameters (Figure 1). Throughput is higher than Transformers even at fixed batch sizes, indicating lower latency.

SSM state dimension and throughputFor typical distillation orders (\(<100\)), peak throughput is not greatly affected. We measure a \(2\%\) reduction in throughput from \(32\) to \(64\).

Prompt lengthThe throughput of LaughingHyena-distilled models is \(4\times\) larger than Transformers at fixed batch size \(64\) and prompt length \(1536\) (Figure 3). As prompt length increases, the runtime gap between pre-filling via convolutions in LCSMs and pre-filling in Transformers widens (e.g., \(\tilde{\mathcal{O}}(T)\) as detailed in Section 3.4, compared to \(\mathcal{O}(T^{2})\)).

Memory footprintRecurrent models do not require \(kv\)-caches and use constant memory for generation of an arbitrary number of tokens (Figure 4).

## 6 Conclusion

We study the efficiency and quality of state-of-the-art long convolutional sequence models. First, we introduce LaughingHyena, a novel distillation method inspired by rational function approximation and model-order reduction techniques. LaughingHyena can be applied after training to extract compact state-space models from each convolutional filter, without loss of quality. Distilled models achieve higher throughput than equivalently-sized Transformers, and can perform auto-regressive generation in constant memory by sidestepping the need to cache previous outputs. We theoretically and empirically investigate the trade-offs of different strategies for fast inference of recurrent models, and introduce architectural improvements to Hyena that improve pretraining quality.

Figure 4: Peak GPU memory for generation.

Figure 3: Scaling in prompt \(T\).

## Acknowledgments

We would like to thank Together Computer for providing the compute used to train models in this paper. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); US DEVCOM ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, and members of the Stanford DAWN project: Facebook, Google, and VMWare. This work is supported by NSF (1651565), AFOSR (FA95501910024), ARO (W911NF-21-1-0125), ONR, DOE (DE-SC0022222), CZ Biohub, and Sloan Fellowship. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. AR's work is supported by NSF grant# CCF-2247014.

## Broader Impact

In this work, we focus on advances related to efficient models for long sequences.

EfficiencyOur distillation methods for constant-memory, high throughput inference in _long convolution sequence models_ (\(\mathsf{LCSMs}\)) can lead to energy savings during model deployement, enabling processing of longer-form content at a fraction of the cost and reducing environmental impact. Improved efficiency may also affect other aspects of AI safety, as it may make it easier produce malicious or harmful content.

AccessibilityBy improving the efficiency of training and generation,\(\mathsf{LCSMs}\) and \(\mathsf{LaughingHyena}\) may contribute to increased accessibility of large language models, lowering the hardware barrier to entry for individuals and organizations with limited resources.

SteerabilityNew method based on \(\mathsf{LCSMs}\) enable sequence models to process long-form prompts previously inaccessible by Transformers, which may lead to increased control over models via e.g., conditioning on additional instructions [45].

## References

* [1] Daniel Y Fu et al. "Hungry Hungary Hippos: Towards Language Modeling with State Space Models". In: 2023 (cit. on pp. 1-3, 7, 23).
* [2] Michael Poli et al. "Hyena Hierarchy: Towards Larger Convolutional Language Models". In: (2023). arXiv: 2302.10866 (cit. on pp. 1-3, 7, 8, 31, 35, 43).
* [3] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. "Neural machine translation by jointly learning to align and translate". In: (2014). arXiv: 1409.0473 (cit. on p. 1).
* [4] Ashish Vaswani et al. "Attention is all you need". In: _Advances in neural information processing systems_ 30 (2017) (cit. on p. 1).
* [5] Chi-Tsong Chen. _Linear system theory and design_. Saunders college publishing, 1984 (cit. on pp. 1, 6, 19, 21).
* [6] Albert Gu, Karan Goel, and Christopher Re. "Efficiently modeling long sequences with structured state spaces". In: (2021). arXiv: 2111.00396 (cit. on pp. 1, 2, 4, 6, 20, 24).
* [7] Vadim Movsesovich Adamyan, Damir Zyamovich Arov, and Mark Grigor'evich Krein. "Analytic properties of Schmidt pairs for a Hankel operator and the generalized Schur-Takagi problem". In: _Matematicheskii Sbornik_ 128.1 (1971), pp. 34-75 (cit. on pp. 2, 7, 29).
* [8] Yuji Nakatsukasa, Olivier Sete, and Lloyd N Trefethen. "The AAA algorithm for rational approximation". In: _SIAM Journal on Scientific Computing_ 40.3 (2018), A1494-A1522 (cit. on pp. 2, 6).
* [9] GRB Prony. "Essai experimental et analytique sur les lois de la dilatalrite de fluids elastiques et sur cells de la vapeur de l'alcool, a differents tempoeratures". In: _Journal de l'Ecole Polytechnique (Paris)_ 1 (1795), pp. 24-76 (cit. on pp. 2, 6).
* [10] Alec Radford et al. "Language models are unsupervised multitask learners". In: _OpenAI blog_ 1.8 (2019), p. 9 (cit. on p. 2).
* [11] Leo Gao et al. "The pile: An 800gb dataset of diverse text for language modeling". In: (2020). arXiv: 2101.00027 (cit. on pp. 2, 8, 35).
* [12] Albert Gu et al. "Hippo: Recurrent memory with optimal polynomial projections". In: _Advances in Neural Information Processing Systems_ 33 (2020), pp. 1474-1487 (cit. on pp. 2, 24).
* [13] David W Romero et al. "Ckconv: Continuous kernel convolution for sequential data". In: (2021). arXiv: 2102.02611 (cit. on p. 2).
* [14] David W Romero et al. "Flexconv: Continuous kernel convolutions with differentiable kernel sizes". In: (2021). arXiv: 2110.08059 (cit. on p. 2).
* [15] Vincent Sitzmann et al. "Implicit neural representations with periodic activation functions". In: _Advances in neural information processing systems_ 33 (2020), pp. 7462-7473 (cit. on p. 3).
* [16] Rizal Fathony et al. "Multiplicative filter networks". In: _International Conference on Learning Representations_. 2020 (cit. on p. 3).
* [17] Daniel Y. Fu et al. "Simple Hardware-Efficient Long Convolutions for Sequence Modeling". In: _International Conference on Machine Learning_ (2023) (cit. on pp. 3, 24).
* [18] Kemin Zhou and John Comstock Doyle. _Essentials of robust control_. Vol. 104. Prentice hall Upper Saddle River, NJ, 1998 (cit. on p. 4).
* [19] Athanasios C Antoulas. _Approximation of large-scale dynamical systems_. SIAM, 2005 (cit. on pp. 4, 7, 29).
* [20] Wilhelmus HA Schilders, Henk A Van der Vorst, and Joost Rommes. _Model order reduction: theory, research aspects and applications_. Vol. 13. Springer, 2008 (cit. on p. 4).
* [21] Sun-Yuan Kung. "A new identification and model reduction algorithm via singular value decomposition". In: _Proc. 12th Asilomar Conf. on Circuits, Systems and Computer_. 1978, pp. 705-714 (cit. on p. 4).
* [22] D Friedman. "On approximating an FIR filter using discrete orthonormal exponentials". In: _IEEE Transactions on Acoustics, Speech, and Signal Processing_ 29.4 (1981), pp. 923-926 (cit. on p. 4).
* [23] J Bednar. "On the approximation of FIR by IIR digital filters". In: _IEEE Transactions on Acoustics, Speech, and Signal Processing_ 31.1 (1983), pp. 28-34 (cit. on p. 4).
* [24] Bartlomiej Beliczynski, Izzet Kale, and Gerald D Cain. "Approximation of FIR by IIR digital filters: An algorithm based on balanced model reduction". In: _IEEE Transactions on Signal Processing_ 40.3 (1992), pp. 532-542 (cit. on pp. 4, 44).

* [25] William Henry Young. "On the multiplication of successions of Fourier constants". In: _Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character_ 87.596 (1912), pp. 331-339 (cit. on p. 5).
* [26] William Beckner. "Inequalities in Fourier analysis on Rn". In: _Proceedings of the National Academy of Sciences_ 72.2 (1975), pp. 638-641 (cit. on p. 5).
* [27] John Fournier. "Sharpness in Young's inequality for convolution". In: _Pacific Journal of Mathematics_ 72.2 (1977), pp. 383-397 (cit. on p. 5).
* [28] Tong S Quek and Leonard YH Yap. "Sharpness of Young's inequality for convolution". In: _Mathematica Scandinavica_ 53.2 (1983), pp. 221-237 (cit. on p. 5).
* [29] Jean-Paul Berrut and Lloyd N Trefethen. "Barycentric lagrange interpolation". In: _SIAM review_ 46.3 (2004), pp. 501-517 (cit. on p. 6).
* [30] Marcos Vicente Moreira and Joao Carlos Basilio. "Fair and Square Computation of Inverse Z-Transforms of Rational Functions". In: _IEEE Transactions on Education_ 55.2 (2011), pp. 285-290 (cit. on p. 6).
* [31] Henri Pade. "Sur la representation approchee d'une fonction par des fractions rationnelles". In: _Annales scientifiques de l'Ecole normale superieure_. Vol. 9. 1892, pp. 3-93 (cit. on pp. 6, 27).
* [32] Ankit Gupta, Albert Gu, and Jonathan Berant. "Diagonal state spaces are as effective as structured state spaces". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 22982-22994 (cit. on pp. 6, 24).
* [33] Antonio Orvieto et al. "Resurrecting Recurrent Neural Networks for Long Sequences". In: (2023). arXiv: 2303.06349 (cit. on pp. 6, 24, 26).
* [34] Jeffrey M Hokanson. "A data-driven McMillan degree lower bound". In: _SIAM Journal on Scientific Computing_ 42.5 (2020), A3447-A3461 (cit. on p. 6).
* [35] L Ho and Rudolf E Kalman. "Effective construction of linear state-variable models from input/output functions". In: _at-Automatisierungstechnik_ 14.1-12 (1966), pp. 545-548 (cit. on p. 6).
* [36] Carl Eckart and Gale Young. "The approximation of one matrix by another of lower rank". In: _Psychometrika_ 1.3 (1936), pp. 211-218 (cit. on p. 7).
* [37] Guy E Blelloch. "Prefix sums and their applications". In: (1990) (cit. on p. 7).
* [38] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. "Simplified state space layers for sequence modeling". In: (2022). arXiv: 2208.04933 (cit. on p. 7).
* [39] Noam Shazeer. "Fast transformer decoding: One write-head is all you need". In: (2019). arXiv: 1911.02150 (cit. on p. 8).
* [40] Joshua Ainslie et al. "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints". In: (2023). arXiv: 2305.13245 (cit. on p. 8).
* [41] Percy Liang et al. "Holistic evaluation of language models". In: (2022). arXiv: 2211.09110 (cit. on pp. 8, 9, 36).
* [42] Leo Gao et al. _A framework for few-shot language model evaluation_. Version v0.0.1. Sept. 2021. doi: 10.5281/zenodo.5371628. url: https://doi.org/10.5281/zenodo.5371628 (cit. on pp. 8, 9, 36).
* [43] Ari Holtzman et al. "The curious case of neural text degeneration". In: (2019). arXiv: 1904.09751 (cit. on p. 9).
* [44] Stella Biderman et al. "Pythia: A suite for analyzing large language models across training and scaling". In: (2023). arXiv: 2304.01373 (cit. on pp. 9, 36).
* [45] Yuntao Bai et al. "Constitutional AI: Harmlessness from AI Feedback". In: (2022). arXiv: 2212.08073 (cit. on p. 11).
* [46] Alan V Oppenheim. _Discrete-time signal processing_. Pearson Education India, 1999 (cit. on p. 21).
* [47] Lennart Ljung. _System identification_. Springer, 1998 (cit. on p. 21).
* [48] RP Guidorzi. "Certain models from uncertain data: the algebraic case". In: _Systems & control letters_ 17.6 (1991), pp. 415-424 (cit. on p. 21).
* [49] Irwin W Sandberg. "On the theory of linear multi-loop feedback systems". In: _Bell System Technical Journal_ 42.2 (1963), pp. 355-382 (cit. on p. 22).
* [50] Michael Zhang et al. "Effectively Modeling Time Series with Simple Discrete State Spaces". In: (2023). arXiv: 2303.09489 (cit. on p. 24).

* [51] Luca Perotti and Michal Wojtylak. "Matrix methods for Pade approximation: Numerical calculation of poles, zeros and residues". In: _Linear Algebra and its Applications_ 548 (2018), pp. 95-122 (cit. on p. 27).
* [52] Amer Abu-Omar and Fuad Kittaneh. "Estimates for the numerical radius and the spectral radius of the Frobenius companion matrix and bounds for the zeros of polynomials". In: _Annals of Functional Analysis_ 5.1 (2014), pp. 56-62 (cit. on p. 27).
* [53] Julia Eaton et al. "Polynomial root radius optimization with affine constraints". In: _Mathematical Programming_ 165 (2017), pp. 509-528 (cit. on p. 27).
* [54] Gerlind Plonka and Vlada Pototskaia. "Application of the AAK theory for sparse approximation of exponential sums". In: (2016). arXiv: 1609.09603 (cit. on p. 29).
* [55] Jimmy Ba et al. "Using fast weights to attend to the recent past". In: _Advances in neural information processing systems_ 29 (2016) (cit. on p. 31).
* [56] William B Johnson. "Extensions of Lipschitz mappings into a Hilbert space". In: _Contemp. Math._ 26 (1984), pp. 189-206 (cit. on p. 31).
* [57] Devdatt P Dubhashi and Alessandro Panconesi. _Concentration of measure for the analysis of randomized algorithms_. Cambridge University Press, 2009 (cit. on p. 34).
* [58] Ilya Loshchilov and Frank Hutter. "Decoupled weight decay regularization". In: (2017). arXiv: 1711.05101 (cit. on p. 35).
* [59] Dale F. Enns. "Model reduction with balanced realizations: An error bound and a frequency weighted generalization". In: _The 23rd IEEE Conference on Decision and Control_ (1984), pp. 127-132 (cit. on p. 44).

Laughing Hyena Distillery

_Supplementary Material_

###### Contents

* 1 Introduction
* 2 Preliminaries and Related Work
	* 2.1 Long Convolution Sequence Models
	* 2.2 Auto-Regressive Generation
* 3 The Laughing Hyena Distillery
	* 3.1 _Data-Free_ Distillation Objectives
	* 3.2 Making Hyena Laugh with Modal Interpolation
	* 3.3 Minimal Distillation Orders
	* 3.4 Deploying the Recurrence
* 4 Multi-head Long Convolutions
* 5 Experiments
	* 5.1 Pre-training
	* 5.2 Distillation Analysis
	* 5.3 Downstream Evaluation
	* 5.4 Benchmarking
* 6 Conclusion
* A Linear Systems
* A.1 Extended Notation and System Theory Preliminaries
* A.2 Systems Norms
* A.3 Transfer Function of State-Space Models
* A.4 Truncated Transfer Functions
* A.5 From Transfer Function to State-Space
* A.5.1 Isolating the \(h_{0}\)-term from Transfer Function by Long division
* A.5.2 Construction of the State-Space from the Transfer Function
* A.6 From State-Space to Transfer Function
* A.7 State-Space Representation of Truncated Filters
* A.8 Efficient Computation of State-Space Models
* A.8.1 Fast Evaluation of the Transfer Function
* A.8.2 Fast Companion Recurrence
* A.8.3 Canonization of State-Space Models
* B LaughingHyena: Further Details
* B.1 Parametrization of Modal Interpolators
* B.2 Distillation as Rational Interpolation
* C Proofs
* C.1 Proof of Lemma 2.1
* C.2 Proof of Lemma 2.2
* C.3 Proof of Lemma 2.3
* C.4 Proof of Proposition 3.1
* C.5 Proof of Lemma 3.1
* C.6 Proof of Theorem 3.2
* C.7 Proof of Proposition 3.3
* C.8 Proof of Proposition 3.2
* C.9 Proof of Theorem 4.1
* [4] Experimental Details D.1 Pre-training D.2 Distillation Analysis D.2.1 Pretrained Filters: Effective Dimension D.3 Downstream Evaluation D.4 Benchmarking
* [5] Additional Experiments E.1 Associative Recall with MultiHyena E.2 Analysis of Hankel Singular Values of Pretrained Large Convolution Sequence Models E.3 Model Order Reduction of H3 E.3.1 Modal Truncation E.3.2 Balanced Truncation

## Authors Contribution

* Conceptualized the research; coordinated collaborations; lead theory development; conducted distillation experiments.
* Conceptualized the research; coordinated collaborations; lead the experimental (model pre-training, distillation, benchmarks, downstream evaluation) efforts; coordinated writing and conference submission; optimized inference stack.
* Assisted in development of MultiHyena; assisted in pre-training and subsequent benchmarking of distilled models; assisted in writing.
* Developed benchmarking suite and interpreted results; assisted in writing.
* Assisted in theory and algorithmic development; performed model-order reduction experiments of H3 models; assisted in writing.
* Conceived and proved Theorem 4.1; assisted in writing.
* Assisted in Hankel operator spectral analysis; Assisted in writing.
* Assisted in theory development.
* Supervised development of benchmarking suite and model deployment.
* Supervised theory development (solving associative recall with MultiHyena, Th. 4.1).
* Supervised research; secured compute resources.
* Supervised research; reviewed manuscript; secured compute resources.
* Supervised research; reviewed manuscript.
* Supervised research; reviewed manuscript.
* Supervised research; reviewed manuscript.
* Supervised research; reviewed manuscript.
* _Stefano Massaroli, Michael Poli, and Dan Fu contributed equally to this work. Christopher Re, Stefano Ermon, and Yoshua Bengio share equal senior authorship_. All authors read and approved the final manuscript.

Linear Systems

### Extended Notation and System Theory Preliminaries

We first introduce the notation and some mathematical concepts that will be used throughout the paper. By \(\mathbb{Z}\) we denote the set of integers, by \(\mathbb{R}\) the set of reals, and by \(\mathbb{C}\) the set of complex numbers. The variable \(t\) stands for _time_. \(\ell_{p}(\mathbb{Z})\) denotes the Banach space of complex-valued sequences \((x_{t})_{t\in\mathbb{Z}}\) with finite energy, i.e. \(\|x\|_{p}\coloneqq[\sum_{t\in\mathbb{Z}}|x_{t}|^{p}]^{1/p}<\infty\) for some \(1\leq p<\infty\). \(\ell_{\infty}(\mathbb{Z})\) is instead is the space of sequences for which \(\|x\|_{\infty}\coloneqq\sup_{t\in\mathbb{Z}}|x_{t}|<\infty\). With \(\mathbb{S}\) denoting the unit circle in the complex plane, \(\mathbb{S}\coloneqq\{z\in\mathbb{C}:|z|=1\}\) we define \(\mathcal{H}_{p}(\mathbb{S})\) as the space of functions \(X\) from \(\mathbb{C}\) to itself such that \(\|X\|_{p}\coloneqq[(1/2\pi)\int_{-\pi}^{\pi}|X(e^{i\omega})|^{p}\mathrm{d} \omega]^{1/p}<\infty\) and \(\mathcal{H}_{\infty}(\mathbb{S})\) the space for which \(\|X\|_{\infty}\coloneqq\sup_{z\in\mathbb{S}}|X(z)|<\infty\). Particularly, \(\mathcal{K}_{2}(\mathbb{S})\) is a Hilbert space with inner product \(\langle X,Y\rangle\coloneqq(1/2\pi)\int_{-\pi}^{\pi}X(e^{i\omega})Y^{*}(e^{i \omega})\mathrm{d}\omega\) where "\(*\)" denotes complex conjugation. Although we acknowledge we are using the same notation for norms in both \(\ell_{p}(\mathbb{Z})\) and \(\mathcal{H}_{p}(\mathbb{S})\), the correct meaning will always be made clear by the context. The \(\mathcal{Z}\)-transform of a sequence \(x=(x_{t})_{t\in\mathbb{Z}}\) is \(X(z)=\mathcal{Z}[x](z)\coloneqq\sum_{t\in\mathbb{Z}}x_{t}z^{-t}\). We embrace the system theory convention of using capital letters to identify transformed sequences. The \(\mathcal{Z}\)-transform is a projection of the sequence onto a basis of powers \(e_{t}=r^{-t}e^{i\omega t}\). This basis is not orthogonal unless \(r=1\). That is the basis of the discrete-time Fourier transform \(\mathcal{F}\). Hence, \(\mathcal{F}\) is defined as \(\mathcal{F}[x](e^{i\omega})=X(e^{i\omega})\coloneqq\sum_{t\in\mathbb{Z}}x_{t }e^{-i\omega t}\). The discrete-time Fourier transform is an isometric isomorphism between \(\ell_{2}(\mathbb{Z})\) and \(L_{2}(\mathbb{S})\). We say that sequences live in the _time domain_ and their \(\mathcal{Z}\) (or \(\mathcal{F}\)) transforms in the _frequency domain_.

A _linear system_ is a linear operator transforming an input sequence \(u\) to an output sequence \(y\). If the sequences have continuous support, i.e. \(t\) ranges over a continuous set (e.g. \(\mathbb{R}\)), we have a _continuous-time_ system. Conversely, if the sequences have discrete support, i.e. \(t\) ranges over a discrete set (e.g. \(\mathbb{Z}\)), we have a _discrete-time_ or _digital_ system. **In this manuscript we restrict ourselves to _discrete-time_ systems**. Systems can be _single-input single-output_ (SISO) if \(u\) and \(y\) are scalar functions or _multi-input multi-output_ if either \(u\) or \(y\) are vector-valued. **We limit our discussion to SISO systems**. The _impulse response_ of a system is the output sequence \(y\) when the input sequence \(u\) is the Kronecker delta function \(\delta_{t}\) and is usually denoted by the letter \(h\). The values \(h_{t}\) of the impulse response sequence are also known as the _Markov parameters_ of the system. The most common mathematical representation of a linear system is its convolution form: \(y=h*u\), i.e. \(y_{t}=\sum_{j\in\mathbb{Z}}h_{t-j}u_{j}=\sum_{j\in\mathbb{Z}}h_{j}u_{t-j},\;t \in\mathbb{Z}\). In matrix form the input-output relation is given by the Toeplitz operator \(\mathsf{T}_{h}\) corresponding to the (possibly infinitely long) sequence \(h\), i.e. \(y=\mathsf{T}_{h}u\). Taking the \(\mathcal{Z}\)-transforms, we can write the input-output relation as \(Y(z)=H(z)U(z)\) (this is just the Fourier convolution theorem extended outside the unit circle). \(H(z)\) is called the _transfer function_ of the system. When \(z=e^{i\omega}\), \(H(e^{i\omega})\) is just the discrete-time Fourier transform of \(h\) which is called the _frequency response_ of the system. A linear system is _causal_ if \(h_{t}=0\) for \(t<0\). A system is called _stable_ if the \(\mathsf{T}_{h}\) is a bounded operator. If \(u,y\in\ell_{2}(\mathbb{Z})\), then stability implies \(h\in\ell_{\infty}\). **In the following, we mainly focus on causal stable systems**.

### Systems Norms

When quantitatively characterizing linear systems, several norms play a crucial role. These norms provide measures of various characteristics of the systems, which are essential in both analysis and filter design.

The \(\ell_{2}\) and \(\mathcal{H}_{2}\) normsAs defined above, the \(\ell_{2}\) norm represents the _energy_ of a signal \(h\),

\[\|h\|_{2}\coloneqq\big{[}\sum_{t\in\mathbb{Z}}|h|_{t}^{2}\big{]}^{1/2}\]

while \(\mathcal{H}_{2}\) is the energy of the (continuous) spectrum of \(h\),

\[\|H\|_{2}\coloneqq\big{[}\frac{1}{2\pi}\int_{-\pi}^{\pi}|X(e^{i\omega})|^{2} \mathrm{d}\omega]^{1/2}\]

By Parseval's theorem, the \(\ell_{2}\) and \(\mathcal{H}_{2}\) norms are equal, \(\|h\|_{2}=\|H\|_{2}\). Further these norms are useful to study the approximation of convolutional filter. The following holds:

**Lemma A.1** (\(\ell_{\infty}\) output error).: _Consider the class of \(\ell_{2}\) measurable inputs such that \(\|u\|_{2}\leq\zeta\), then for all \(H,\hat{H}\in\mathcal{H}_{2}\),_

\[\|y-\hat{y}\|_{\infty}\leq\zeta\|H-\hat{H}\|_{2}\]Proof.: \[\sup_{t>0}\;|y_{t}-\hat{y}_{t}| =\sup_{t>0}\left|\frac{1}{2\pi}\int_{-\pi}^{\pi}\left[Y(e^{i\omega}) -\hat{Y}(e^{i\omega})\right]e^{i\omega t}\mathrm{d}\omega\right|\] \[\leq\frac{1}{2\pi}\int_{-\pi}^{\pi}|Y(e^{i\omega})-\hat{Y}(e^{i \omega})|\mathrm{d}\omega\] \[=\frac{1}{2\pi}\int_{\pi}^{\pi}|H(e^{i\omega})-\hat{H}(e^{i \omega})|U(e^{i\omega})|\mathrm{d}\omega\] \[\leq\left[\frac{1}{2\pi}\int_{-\pi}^{\pi}|H(e^{i\omega})-\hat{H} (e^{i\omega})|^{2}\mathrm{d}\omega\right]^{1/2}\left[\frac{1}{2\pi}\int_{-\pi} ^{\pi}|U(e^{i\omega})|^{2}\mathrm{d}\omega\right]^{1/2}\quad\text{H\"{o}lder Inequality}\] \[\leq\left[\frac{1}{2\pi}\int_{-\pi}^{\pi}|H(e^{i\omega})-\hat{H} (e^{i\omega})|^{2}\mathrm{d}\omega\right]^{1/2}\|u\|_{2}\] Parseval Theorem \[\leq\zeta\|H-\hat{H}\|_{\mathcal{H}_{2}}\]

If \(u\) is the unit impulse function \(u_{t}=\delta_{t}\) then \(\zeta=1\). The results also holds for finite sequences of length \(L\) using the discrete Fourier transform.

**Lemma A.2** (Impulse response error on finite sequences).: _Consider filters \(h,\hat{h}\) with finite length_

_L. Then, the following holds._

\[\|h-\hat{h}\|_{\infty}\leq\|H-\hat{H}\|_{2}\]

_where \(H\) and \(\hat{H}\) denote the discrete Fourier transforms of \(h\) and \(\hat{h}\), respectively._

Proof.: \[\|y-\hat{y}\|_{\infty}\coloneqq\sup_{t>0}\;|y_{t}-\hat{y}_{t}| =\sup_{t>0}\left|\frac{1}{2\pi}\sum_{n=0}^{L-1}\left[Y_{n}-\hat{ Y}_{n}\right]e^{i2\pi nt/L}\right|\] \[\leq\frac{1}{2\pi}\sum_{n=0}^{L-1}|Y_{n}-\hat{Y}_{n}|\] \[=\frac{1}{2\pi}\sum_{n=0}^{L-1}|H_{n}-\hat{H}_{n}||U_{n}|\] \[\leq\left[\frac{1}{2\pi}\sum_{n=0}^{L-1}(H_{n}-\hat{H}_{n})^{2} \right]^{1/2}\left[\frac{1}{2\pi}\sum_{n=0}^{L-1}U_{n}^{2}\right]^{1/2}\quad \text{H\"{o}lder Inequality}\] \[\leq\left[\frac{1}{2\pi}\sum_{n=0}^{L-1}(H_{n}-\hat{H}_{n})^{2} \right]^{1/2}\|u\|_{2}\] Parseval Theorem \[=\|H-\hat{H}\|_{2}\] using

### Transfer Function of State-Space Models

The transfer function (3.1) is derived by taking the \(z\)-transform of input and state, \(U(z)=\mathcal{Z}[u](z),X(z)=\mathcal{Z}[x](z)\). Plugging \(U(z),\;X(z)\) in the state equation (2.2), it holds

\[zX(z)=\mathsf{A}X(z)+\mathsf{B}U(z)\;\Leftrightarrow\;X(z)=(z \mathsf{I}-\mathsf{A})^{-1}\mathsf{B}U(z)\]

Substituting in the output equation yields

\[Y(z)=\mathsf{C}(z\mathsf{I}-\mathsf{A})^{-1}\mathsf{B}U(z)+h_{0}U(z)\]

The transfer function is then defined as

\[H(z)=\frac{Y(z)}{U(z)}=\mathsf{C}(z\mathsf{I}-\mathsf{A})^{-1} \mathsf{B}+h_{0}.\] (A.1)

Alternative derivationThe transfer function can also be derived by direct \(z\)-transform of the impulse response \(h_{t}\) of the system. This derivation is useful to highlight the region of convergence of the transfer function.

\[\begin{split} H(z)&=h_{0}+\sum_{t=1}^{\infty}z^{-t} \mathsf{CA}^{t-1}\mathsf{B}\qquad\qquad\quad h_{0}\text{ is pulled out via }h_{0}z^{0}=h_{0}\\ &=h_{0}+\mathsf{C}\left[\sum_{t=1}^{\infty}z^{-t}\mathsf{A}^{t-1} \right]\mathsf{B}\qquad\qquad\quad\text{multiplication distributes over sum.}\\ &=h_{0}+z^{-1}\mathsf{C}\left[\sum_{t=1}^{\infty}z^{-(t-1)} \mathsf{A}^{t-1}\right]\mathsf{B}\quad\text{multiply by }z/z\\ &=h_{0}+z^{-1}\mathsf{C}\left[\sum_{t=0}^{\infty}(z^{-1}\mathsf{A })^{t}\right]\mathsf{B}\qquad\qquad\text{change of index and collect like terms}\end{split}\] (A.2)

We look at the convergence of the series \(\sum_{t=0}^{\infty}\|z^{-1}\mathsf{A}\|_{2}^{t}\). We have

\[\begin{split}\|z^{-1}\mathsf{A}\|_{2}&\leq\|z^{-1 }\|_{2}\|\mathsf{A}\|_{2}\\ &=\|r^{-1}e^{-i\omega}\|_{2}\|\mathsf{A}\|_{2}\qquad\text{ using }z\coloneqq re^{i\omega}\in\mathbb{C},\ r,\omega\in\mathbb{R}\\ &\leq r^{-1}\|\mathsf{A}\|_{2}=r^{-1}\rho(\mathsf{A})\end{split}\]

The series converges to \(1/(1-r^{-1}\rho(\mathsf{A}))\) if and only if \(r^{-1}\rho(\mathsf{A})<1\) i.e. for \(r>\rho(\mathsf{A})\). Thus, in the exterior of the disk with radius \(\rho(\mathsf{A})\), \(\mathbb{D}_{\rho(\mathsf{A})}\coloneqq\{z\in\mathbb{C}:|z|>\rho(\mathsf{A})\}\), \(\sum_{t=0}^{\infty}(z^{-1}\mathsf{A})^{t}\) converges to \((\mathsf{l}-z^{-1}\mathsf{A})^{-1}\) and

\[z\in\mathbb{D}_{\rho(\mathsf{A})}\ \Rightarrow\ H(z)=h_{0}+z^{-1}\mathsf{C}( 1-z^{-1}\mathsf{A})^{-1}\mathsf{B}=h_{0}+\mathsf{C}(z\mathsf{l}-\mathsf{A})^ {-1}\mathsf{B}\]

The transfer function \(H(z)=h_{0}+\mathsf{C}(z\mathsf{l}-A)^{-1}\mathsf{B}\) of a stable lumped discrete-time system is defined outside the disc in the complex plane that encloses all the eigenvalues of \(\mathsf{A}\).

Invariance of the transfer function\(H(z)\) as defined in (A.1) is a _proper_13 rational function of \(z\). In case \(h_{0}=0\), \(H(z)\) is strictly proper and the denominator is monic:

Footnote 13: i.e. such that the denominator’s order is not less than the numerator’s one.

\[H(z)=\frac{b_{1}z^{-1}+\cdots+b_{d}z^{-d}}{1+a_{1}z^{-1}+\cdots+a_{d}z^{-d}}\] (A.3)

Specifically, the denominator could be derived from \(\mathsf{A}\) with \(\mathsf{det}(z\mathsf{l}-\mathsf{A})\), and the numerator is \(\mathsf{det}(z\mathsf{l}-\mathsf{A}+\mathsf{BC})+\mathsf{det}(z\mathsf{l}- \mathsf{A})\). We provide a detailed derivation below in Section A.6. While state-space representation involves the analysis and synthesis of model matrices \(\mathsf{A},\mathsf{B},\mathsf{C}\), the transfer function is entirely characterized by the coefficients \(a=(a_{n})_{n=1}^{d},\ b=(b_{n})_{n=1}^{d}\) of numerator and denominator polynomials. Notably, the transfer function is an _invariant_ of the system: if we apply a change of variables to the state, the transfer function remains unchanged.

 **Lemma A.3**.: _Coefficients \(a,b\) are **invariant** under any invertible change of variables._

Proof.: The proof can be found in [5, pp.95] and follows from the definition of _equivalence transformation_. Consider the state-space matrices of under change of variables \(\hat{x}=\mathsf{K}x\),

\[\hat{\mathsf{A}}=\mathsf{K}\mathsf{A}\mathsf{K}^{-1},\quad\hat{\mathsf{B}}= \mathsf{K}\mathsf{B},\quad\hat{\mathsf{C}}=\mathsf{C}\mathsf{K}^{-1},\quad \hat{h}_{0}=h_{0}.\]

The resulting transfer function \(H(z)\) can then be computed as

\[\hat{H}(z)=\hat{\mathsf{C}}(z\mathsf{l}-\hat{\mathsf{A}})^{-1}\hat{\mathsf{B} }+\hat{h}_{0}=\mathsf{C}\mathsf{K}^{-1}[\mathsf{K}(z\mathsf{l}-\mathsf{A}) \mathsf{K}^{-1}]^{-1}\mathsf{K}\mathsf{B}+h_{0}=H(z)\]

### Truncated Transfer Functions

In the case of generic _truncated_ (finite) impulse response filters, such that \(h_{t}=0\) for all \(t\) greater than a certain value \(L\) (which we refer to as the _length_ of the filter), the transfer function is simply a polynomial in the complex variable \(z\) of order \(L\), i.e.

\[H(z)=\sum_{t=0}^{\infty}h_{t}z^{-t}=\sum_{t=0}^{L}h_{t}z^{-t}=h_{0}+h_{1}z^{-1}+ \cdots+h_{L}z^{-L}\] (A.4)

In case the filter is generated by a finite dimensional (lumped parameters) system, i.e. \(h_{t}=\mathsf{CA}^{t-1}\mathsf{B}\)\(t=1,\ldots,L\), then (A.4) can still be represented exactly by a rational function of order \(d\).

**Lemma A.4** (Truncated rational transfer functions).: _Consider the \(L\)-truncated impulse response \(h_{t}\in\ell_{2}(\mathbb{N})\) of a lumped-parameter filter \((\mathsf{A},\mathsf{B},\mathsf{C},h_{0})\),_

\[h_{t}=\begin{cases}h_{0}&t=0\\ \mathsf{CA}^{t-1}\mathsf{B}&1\leq t\leq L\\ 0&t>L\end{cases}.\]

_Then its truncated transfer function is_

\[H_{L}(z)=\mathcal{Z}\{h\}(z)=h_{0}+\mathsf{C}(\mathsf{I}-z^{-L}\mathsf{A}^{L })(z\mathsf{I}-\mathsf{A})^{-1}\mathsf{B}\]

Proof.: By definition of \(z\)-transform we have

\[\begin{split} H_{T}(z)&=\sum_{t=0}^{\infty}h_{t}z^{-t }=h_{0}+\sum_{t=1}^{L}z^{-t}\mathsf{CA}^{t-1}\mathsf{B}\\ &=h_{0}+\mathsf{C}\left[\sum_{t=1}^{L}z^{-t}\mathsf{A}^{t-1} \right]\mathsf{B}=h_{0}+z^{-1}\mathsf{C}\left[\sum_{t=0}^{L-1}(z^{-1}\mathsf{ A})^{t}\right]\mathsf{B}\end{split}\] (A.5)

The sum \(\sum_{t=0}^{L-1}(z^{-1}\mathsf{A})^{t}\) is a partial Neumann series and can be manipulated as follows.

\[\begin{split}\sum_{t=0}^{L-1}(z^{-1}\mathsf{A})^{t}(\mathsf{I}- z^{-1}\mathsf{A})&=\sum_{t=0}^{L-1}(z^{-1}\mathsf{A})^{t}-\sum_{t=0}^{L-1 }(z^{-1}\mathsf{A})^{t+1}\\ &=\mathsf{I}-(z^{-1}A)^{L}.\end{split}\]

Thus,

\[\sum_{t=0}^{L-1}(z^{-1}\mathsf{A})^{t}=(\mathsf{I}-z^{-L}\mathsf{A}^{L})( \mathsf{I}-z^{-1}\mathsf{A})^{-1},\]

which plugged in (A.5) gives \(H_{L}(z)=h_{0}+\mathsf{C}(\mathsf{I}-z^{-L}\mathsf{A}^{L})(z\mathsf{I}- \mathsf{A})^{-1}\mathsf{B}\), proving the result. 

Because of truncation, evaluating the transfer function \(H_{L}(z)\) on the \(L\)_roots of unity_\(z=e^{i\omega_{k}}\), \(w_{k}=2\pi k/T\) for \(k=0,\ldots L\) gives the length-\(L\) discrete Fourier transform (DFT) of the filter:

\[\bar{H}_{k}\coloneqq H_{L}(e^{i\omega_{k}})=\sum_{t=0}^{L-1}h_{t}e^{-i2\pi k/ L},\quad k=0,\ldots,L-1.\]

In practice, this means that \(\bar{H}\in\mathbb{C}^{L}\) is the \(\mathsf{FFT}\) of \(h\), \(\bar{H}=\mathsf{FFT}_{L}[h]\). If we can find an efficient and stable algorithm to evaluate \(\bar{H}\) from the system matrices \((\mathsf{A},\mathsf{B},\mathsf{C},\bar{h}_{0})\), then the \(\mathsf{FFT}\)-based convolution of truncated filter with an input sequence \(u\in\mathbb{R}^{L}\) can be evaluated in \(\bar{O}(L)\) time.

ReparametrizationAssume training a \(\mathsf{LCSM}\) equipped with SSM filters with input/target sequences to be all of length \(L\) (smaller sequences can be padded with zeros to the maximum length). Thus, for training purposes, we are only interested in evaluating \(\bar{H}\) for the \(\mathsf{FFT}\)-based convolution.

The truncated transfer function \(H_{L}\) is equal to the original one with a correction term \(\mathsf{I}-z^{-L}\mathsf{A}^{L}\) on the numerator polynomial. As already noted in S4[6], \(z^{-L}\) is conveniently equal to one on the roots of unity, \(z^{i\omega_{k}L}=e^{-i2\pi k}=1\) for all \(k=0,\ldots,L-1\). Hence, the correction term due to truncation becomes constant: \(H_{k}=\mathsf{C}(\mathsf{I}-\mathsf{A}^{L})(\exp(-i2\pi k/L)\mathsf{I}- \mathsf{A})^{-1}\mathsf{B}\); in DFT domain the truncated filter behaves as the infinitely long one with a perturbed \(\mathsf{C}\) matrix

\(\bar{\mathsf{C}}=\mathsf{C}-\mathsf{CA}^{L}\)

If -as assumed- the SSM is stable \(\rho(\mathsf{A})<1\), \((i)\) the transfer function is defined on the unit circle, term \(\mathsf{CA}^{L}\) will go to zero exponentially fast as \(L\to\infty\) and \(\bar{\mathsf{C}}=\mathsf{C}\) (as expected). As advised in [6], it is desirable to parametrize directly \(\bar{\mathsf{C}}\); the expensive computation of the correction term \(\mathsf{C}(\mathsf{I}-\mathsf{A}^{L})\) is never carried out during training. Instead, the real \(\mathsf{C}\) matrix can be retrieved for recurrent inference by inverting the correction term \(\mathsf{C}=\bar{\mathsf{C}}(\mathsf{I}-\mathsf{A}^{L})^{-1}\), always invertible for stable systems although possibly ill conditioned by eigenvalues too close to the stability margin (the unit circle).

### From Transfer Function to State-Space

Suppose the coefficients of the numerator and denominator polynomials of a proper transfer function \(H\) is given:

\[H(z)=\frac{b_{0}+b_{1}z^{-1}+\ \cdots\ +b_{d}z^{-d}}{1+a_{1}z^{-1}+\ \cdots\ +a_{d}z^{-d}}.\] (A.6)

A state-space representation of the form (2.2) can be rapidly realized in two steps:1. **Get delay-free path** From (A.6) we first notice that the _bias_ term \(h_{0}\) is \(h_{0}=b_{0}\). We thus want to isolate \(b_{0}\) from the rest of the numerator. This can be obtained via long division (see SSA.5.1) and results in \[H(z)=\frac{\beta_{1}z^{-1}+\ \cdots\ +\beta_{N}z^{-d}}{1+a_{1}z^{-1}+\ \cdots\ +a_{d}z^{-d}}+b_{0},\quad\beta_{n}=b_{n}-b_{0}a_{n}\] (A.7)
2. **Get state-space matrices** Given the transfer function \(H(z)\) with the isolated pass-through coeffient \(b_{0}\) as in (A.7), we can construct the state-space matrices by _companion_ canonical realization: \[\left[\begin{array}{c|c}\mathsf{A}&\mathsf{B}\\ \hline\mathsf{C}&h_{0}\end{array}\right]=\left[\begin{array}{ccccc|c}-a_{1}& -a_{2}&\cdots&-a_{d-1}&-a_{d}&1\\ 1&0&\cdots&0&0&0\\ 0&1&\cdots&0&0&0\\ \vdots&\vdots&\ddots&\vdots&\vdots&\vdots\\ 0&0&\cdots&1&0&0\\ \hline\beta_{1}&\beta_{2}&\cdots&\beta_{d-1}&\beta_{d}&b_{0}\end{array}\right]\] (A.8)

Details on the complete _a la_[5] derivation can be found in SSA.5.2. A linear system with finite-dimensional state can be equivalently characterized: by its state-space matrices \((\mathsf{A},\mathsf{B},\mathsf{C},h_{0})\), by its impulse response function \(h\), or by the coefficients \(a,b\) (or \(\beta\)) of the transfer function. A fourth representation is its _linear-constant-coefficients difference equation_ form

\[y_{t}=\sum_{j=0}^{d}b_{j}u_{t-j}-\sum_{n=1}^{d}a_{j}y_{t-j},\]

typically used in signal processing literature in the theory of _infinite impulse response_ filters (see [46]) and known, in the context of system identification of error-in-variables models, as _auto-regressive moving-average_ filters [47, 48].

#### a.5.1 Isolating the \(h_{0}\)-term from Transfer Function by Long division

If the rational transfer function \(H(z)\) accounts for the \(h_{0}\) term, then it is simply proper (order of numerator equals the order of denominator), \(h_{0}\) is necessarily \(h_{0}=b_{0}\) (the _delay-free_ path). Given the transfer function in this form, we can isolate the \(b_{0}\) term and the strictly rational term of (A.3) by long division. We start by expanding the fraction as

\[H(z)=\frac{q(z)}{p(z)}=\frac{b_{0}}{p(z)}+\frac{b_{1}z^{-1}+\ \cdots\ +b_{d}z^{-d}}{p(z)}.\]

and

\[\frac{b_{0}}{p(z)}=\frac{b_{0}z^{d}}{z^{d}+a_{1}z^{d-1}+\ \cdots\ +a_{d}}\]

We then use the long division method to compute \(b_{0}/p(z)\):

\[z^{d}+a_{1}z^{d-1}+\ \cdots\ +a_{d} \frac{b_{0}}{b_{0}z^{d}}\] \[\frac{b_{0}z^{d}+b_{0}a_{1}z^{d-1}+\cdots+b_{0}a_{d}}{-b_{0}a_{1}z^ {d-1}-\cdots-b_{0}a_{d}}\quad\text{(reminder)}\]

to finally get

\[H(z) =b_{0}-\frac{b_{0}a_{1}z^{d-1}+\ \cdots\ +b_{0}a_{d}}{z^{d}+a_{1}z^{d-1 }+\ \cdots\ +a_{d}}+\frac{b_{1}z^{-1}+\ \cdots\ +b_{d}z^{-d}}{p(z)}\] \[=b_{0}+\frac{(b_{1}-b_{0}a_{1})z^{-1}+\ \cdots\ +(b_{d}-b_{0}a_{d})z^{-d}}{1+a_{1}z^{-1}+ \ \cdots\ +a_{d}z^{-d}}\]

Note that the coefficients \(b_{n}\) in (A.3) correspond to \(b_{n}-b_{0}a_{n}\) in (A.6), \(b_{n}\gets b_{n}-b_{0}a_{n}\). It is indifferent to parameterize the coefficients of the transfer function in either forms. However, if we choose the simply proper representation (A.6), we need to apply the derived correction factor to the numerator coefficients when we separate the \(h_{0}\) term and strictly proper part of \(H(z)\).

#### a.5.2 Construction of the State-Space from the Transfer Function

Chen's derivationThe derivation is based on the steps reported for the continuous-time _multi-input multi-output_ case in [5]. First, we define a pseudo-state \(v\) such that

\[p(z)V(z)=U(z)\quad\Leftrightarrow\quad V(z)=\frac{1}{p(z)}U(z).\] (A.9)Then, we define the state \(x_{t}:=(x_{t}^{1},\ldots,x_{t}^{d})\in\mathbb{R}^{d}\) as

\[x_{t}=(v_{t-1},v_{t-2},\cdots,v_{t-d})\quad\Leftrightarrow\quad\mathcal{Z}\{x \}(z)=X(z)=\begin{bmatrix}z^{-1}\\ \vdots\\ z^{-d}\end{bmatrix}V(z).\] (A.10)

From (A.9) we have

\[V(z)+a_{1}z^{-1}V(z)+\cdots+a_{d}z^{-d}V(z)=U(z) \Leftrightarrow\] \[V(z)=-a_{1}z^{-1}V(z)-\cdots-a_{d}z^{-d}V(z)+U(z) \Leftrightarrow\] \[v_{t}=-a_{1}v_{t-1}-\cdots-a_{d}v_{t-d}+u_{t} \Leftrightarrow\] time-delay prop. of \[\mathcal{Z}\] -transform \[x_{t+1}^{1}=-a_{1}x_{t}^{1}-\cdots-a_{d}x_{t}^{d}+u_{t} \Leftrightarrow\] by def. of state (A.10).

Thus, we have the overall recurrence

\[x_{t+1}^{1} =-a_{1}x_{t}^{1}-\cdots-a_{d}x_{t}^{d}+u_{t}\] \[x_{t+1}^{2} =x_{t}^{1}\] \[\vdots\] \[x_{t+1}^{d} =x_{t}^{d-1}\]

which can be written in matrix form as

\[x_{t+1}=\begin{bmatrix}-a_{1}&-a_{2}&\cdots&-a_{N}\\ 1&0&\cdots&0\\ 0&1&\cdots&0\\ \vdots&\vdots&\ddots&\vdots\\ 0&0&\cdots&1\end{bmatrix}x_{t}+\begin{bmatrix}1\\ 0\\ \vdots\\ 0\\ 0\end{bmatrix}u_{t}\]

The output spectrum is then given by

\[Y(z) =H(z)U(z)=\frac{q(z)}{p(z)}U(z)+b_{0}U(z)\] \[=q(z)V(z)+b_{0}U(z) \text{by def. of }V(z).\]

Therefore,

\[Y(z) =q(z)V(z)+b_{0}U(z)=\begin{bmatrix}\beta_{1}&\beta_{2}&\cdots&\beta_{N} \end{bmatrix}\begin{bmatrix}z^{-1}\\ z^{-2}\\ \vdots\\ z^{-d}\end{bmatrix}V(z)+b_{0}U(z)\] \[=\begin{bmatrix}\beta_{1}&\beta_{2}&\cdots&\beta_{d}\end{bmatrix} X(z)+b_{0}U(z)\]

and the output equation in time-domain is given by

\[y_{t}=\begin{bmatrix}\beta_{1}&\beta_{2}&\cdots&\beta_{d}\end{bmatrix}x_{t}+b_{0}u_{t}.\]

yielding state-space matrices (A.8).

### From State-Space to Transfer Function

We detail an implementation oriented method to compute the coefficients \((a_{n})_{n=1}^{d},(b_{n})_{n=0}^{d}\) of a SSM's transfer function. Recall that

\[H(z)=\mathsf{C}[z\mathsf{I}-\mathsf{A}]^{-1}\mathsf{B}+h_{0}=\frac{\mathsf{C} \operatorname{Adj}(z\mathsf{I}-\mathsf{A})\mathsf{B}+\mathsf{det}(z\mathsf{I}- \mathsf{A})h_{0}}{\mathsf{det}(z\mathsf{I}-\mathsf{A})}\] (A.11)

Hence, the denominator coefficients \((a_{n})_{n=1}^{d}\) are simply the coefficients of the characteristic polynomial of matrix \(\mathsf{A}\). They can be easily obtained by 1. computing the eigenvalues of \(\mathsf{A}\) and 2. calculating the coefficients of the polynomial whose roots are such eigenvalues. On the other hand, the numerator apparently involves more complex symbolic manipulation. This can be simplified recalling a classic matrix-determinant identity:

**Lemma A.5** ([49]).: _Let \(\mathsf{M}\), \(\mathsf{B}\), and \(\mathsf{C}\) respectively denote matrices of orders \(d\times d\), \(d\times 1\), and \(1\times d\). Then,_

\[\mathsf{det}(\mathsf{M}+\mathsf{BC})=\mathsf{det}(\mathsf{M})+\mathsf{C} \operatorname{Adj}(\mathsf{M})\mathsf{B}.\]

Applying Lemma A.5 to (A.11) we obtain

\[H(z)=\frac{\mathsf{det}(z\mathsf{I}-\mathsf{A}+\mathsf{BC})+\mathsf{det}(z \mathsf{I}-\mathsf{A})(h_{0}-1)}{\mathsf{det}(z\mathsf{I}-\mathsf{A})}.\]Let \(\mathsf{poly}(r)\) denote the coefficients of the polynomials with roots \(r=(r_{1},\ldots,r_{d})\). Then \(a=\mathsf{poly}(\mathsf{eig}(\mathsf{A}))\). Since \(\mathsf{A}\) and \(\mathsf{A}-\mathsf{BC}\) are of equal dimension, their characteristic polynomials have equal order and therefore

\[b=\mathsf{poly}(\mathsf{eig}(\mathsf{A}-\mathsf{BC}))+\mathsf{poly}(\mathsf{ eig}(\mathsf{A}))(h_{0}-1)\]

``` defget_tf_from_ss(A,B,C,h0): a=poly(eig(A) b=poly(eig(A -- outer(B,C))) + (h0--1)*a returna,b ```

Listing 1: State-space \(\rightarrow\) transfer function conversion code

### State-Space Representation of Truncated Filters.

A truncated filter \(h_{0},\ldots,h_{L}\) - as the ones found in any standard convolutional neural network - can be represented by a \(L\)-dimensional companion canonical SSM. The filter's transfer function \(H(z)=h_{0}+h_{1}z^{-1}+\cdots+h_{L}z^{-L}\) is polynomial, i.e. a rational function with the denominator's coefficients set to zero. Following the canonical realization process detailed in Section A.5, the truncated filter has state-space form:

\[x_{t+1}=\begin{bmatrix}0&0&\cdots&0&0\\ 1&0&\cdots&0&0\\ 0&1&\cdots&0&0\\ \vdots&\vdots&\ddots&\vdots&\vdots\\ 0&0&\cdots&1&0\end{bmatrix}x_{t}+\begin{bmatrix}1\\ 0\\ \vdots\\ 0\\ 0\end{bmatrix}u_{t}\]

\[y_{t}=[h_{1}\quad h_{2}\quad\cdots\quad h_{L}]\,x_{t}+h_{0}u_{t}.\]

If \(x_{0}=\emptyset_{L}\) and \(u_{t}=0\) for negative \(t\), then at each \(t>0\) the state is a shifted copy of the input sequence \(x_{t}=(u_{t-1},\ldots,u_{t-L})\in\mathbb{R}^{L}\). Nonetheless, the asymptotic complexity of computing one recurrent step is \(\mathcal{O}(L)\) as it requires only a shift operation and a length-\(L\) dot product

\[x_{t+1}^{1} =u_{t}\] (A.12) \[x_{t+1}^{2:L} =\mathsf{shift}(x_{t})\] \[y_{t} =\langle h_{1:L},x_{t}\rangle+h_{0}u_{t}.\]

The memory footprint is also \(\mathcal{O}(L)\). In [1] it is proposed the use of shift-type SSMs to parametrize one of the filters of the H3 block.

### Efficient Computation of State-Space Models

#### a.8.1 Fast Evaluation of the Transfer Function

Computing \(H(z)\) at any point \(z\in\mathbb{C}\) concerns the evaluation of the \(d\)-order polynomial of numerator and denominator,

\[H(z)=\frac{q(z)}{p(z)}=\frac{\sum_{n=1}^{d}b_{n}z^{-n}}{1+\sum_{n=1}^{d}a_{n}z ^{-n}}\]

In practice, we are mainly interested in a fast algorithm that allows computing \(H\) on the \(L\) roots of unity to obtain the DFT of the filter. The DFT of the filter can be then readily used to perform a FFT-based convolution with a length-\(L\) input sequence \(u\) or to recover the impulse response function via inverse DFT. We prove the following:

**Lemma A.6**.: _Given the coefficients \(a\), \(b\) of the transfer function, the frequency and impulse response of the filter can be evaluated in \(\tilde{\mathcal{O}}(L)\) time._

Proof.: The result is proven showing that the transfer function can be evaluated in \(\tilde{\mathcal{O}}(L)\) time on the \(L\) roots of unity. The fastest method to evaluate polynomials on \(L\) arbitrary points \(z\) of the complex plane is generally the Horner's scheme. This method is based on a sequence of nested multiplications and computes the polynomial from its vector of coefficients, delivering a time complexity of \(\mathcal{O}(dL)\). More explicitly, Horner's scheme determines \(p(z)\) as \(p(z)=((\cdots((a_{d}z^{-1}+a_{d-1})z^{-1}+a_{d-2})\cdots)z^{-1}+a_{2})z^{-1}+a_ {1})z^{-1}+1\). Each step involves a multiplication and an addition, making a total of \(2d\) operations per evaluation point. Thus, for \(L\) points, the total number of operations amounts to \(\mathcal{O}(dL)\).

Effectively, Horner's approach implements the matrix-vector product of an \(L\)-by-\((d+1)\) Vandermonde matrix \(\mathsf{V}\in\mathbb{C}^{L\times(d+1)}\) constructed by \(L\) evaluation points \((z_{0},\ldots,z_{L-1})\) with the vector of coefficients\(a=(1,a_{1},\dots,a_{d})^{\top}\):

\[\begin{bmatrix}p(z_{0})\\ p(z_{1})\\ \vdots\\ p(z_{L-1})\end{bmatrix}=\begin{bmatrix}1&z_{0}^{-1}&z_{0}^{-2}&\cdots&z_{0}^{-d}\\ 1&z_{1}^{-1}&z_{1}^{-2}&\cdots&z_{1}^{-d}\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ 1&z_{L-1}^{-1}&z_{L-1}^{-2}&\cdots&z_{L-1}^{-d}\end{bmatrix}\begin{bmatrix}1\\ a_{1}\\ \vdots\\ a_{d}\end{bmatrix}=\mathsf{V}a\]

Significantly, if the polynomial is required to be evaluated at the roots of unity, the Vandermonde matrix simplifies corresponds to the \(L\times(d+1)\) DFT matrix. Further, zero-padding the coefficient vector to length \(L\), enables the use a single length-\(L\) FFT to compute the matrix-vector product in \(\tilde{\mathcal{O}}(L)\) time. Thus, the numerator and denominator polynomials of the transfer function can be evaluated, on the roots of unity, in \(\tilde{\mathcal{O}}(L)\) time by taking the FFT of the padded numerator / denominator coefficients \(a,b\) and subsequently dividing element-wise the two sequences as \(\mathsf{FFT}_{L}[b]/\mathsf{FFT}_{L}[a]\). The overall time complexity to obtain the impulse response is also \(\tilde{\mathcal{O}}(L)\) since \(h\) can be recovered taking an inverse FFT of the frequency response. 

#### a.8.2 Fast Companion Recurrence

The recurrent step of a generic SSM (2.2) with dense system matrices usually requires \(\mathcal{O}(d^{2})\) operations due to the matrix-vector product \(\mathsf{A}x_{t}\). We show how the recurrence of SSMs in _companion canonical form_, i.e. with system's matrices (A.8), requires only \(\mathcal{O}(d)\) operations.

**Lemma A.7**.: _The recurrent step of a state-space model in companion canonical form (A.8) can be evaluated in \(\mathcal{O}(d)\) time and memory._

Proof.: The companion state matrix \(\mathsf{A}\) can be broken down into a lower shift matrix \(\mathsf{L}_{N}\) and a low-rank term. Particularly, with \(e_{1}\) the first element of the canonical basis of \(\mathbb{R}^{N}\) and \(\alpha=(a_{1},\dots,a_{N})\), we have

\[\mathsf{A}=\mathsf{L}_{N}-e_{1}\otimes\alpha.\]

It follows that the recurrent update can be simplified to

\[x_{t+1} =\left(\mathsf{L}_{N}-e_{1}\otimes\alpha\right)x_{t}+\mathsf{B}u_{t}\] \[y_{t} =\mathsf{C}x_{t}+b_{0}u_{t}\]

The peculiarity of this formulation is that we never need to construct the matrices to perform the recurrence. In particular we have:

\[x_{t+1}^{1} =u_{t}-\alpha^{\top}x_{t}\] \[x_{t+1}^{2:N} =\mathsf{shift}(x_{t})\] \[y_{t} =\beta^{\top}x_{t}+b_{0}u_{t}\]

Thus, each step only requires two inner products (\(d\) multiplications and \(d\) sums each) and one shift operation, totaling \(\mathcal{O}(d)\) operations. 

The proof of Lemma A.7 yields the practical implementation of the recurrence:

``` defstep(x,u,alpha,beta,b0): y=dot(beta,x)+b0*u lr=u-dot(alpha,x) x=roll(x) x[0]=lr returnx,y ```

Listing 2: Python implementation of the companion canonical recurrence

#### a.8.3 Canonization of State-Space Models

The companion canonical form discussed in Section A.5 is the ideal representation to deploy SSM-based convolutional layers: \(i)\) it comes with a \(\mathcal{O}(d)\) fast recurrence and \(ii)\) allows to swiftly switch between time and frequency domains with a direct mapping between state-space matrices and coefficient of the transfer function (which in turn allow \(\tilde{\mathcal{O}}(L)\) fast convolutions).

Aside from [50], which directly parametrizes S4 layers in companion canonical form, all the other parameterizations [12, 6, 32, 17, 33] can be _converted_ (_canonized_), under mild assumptions.

**Lemma A.8** (Canonization of SSMs).: _Any state-space model (2.2) with proper transfer function can be converted in companion canonical form._

Proof.: The result can be proved following the two-step conversion process.

1. **Get the coefficients of the transfer function**: Given the original state-space matrices \((\mathsf{A},\mathsf{B},\mathsf{C},h_{0})\), the transfer function is given by \(H(z)=\mathsf{C}(z!-\mathsf{A})^{-1}\mathsf{B}+h_{0}\). A proper rational function has the form \(H(z)=q(z)/p(z)\) where the numerator \(q(z)\) has coefficients \(b=(b_{n})_{n=0}^{d}\) and the denominator has coefficients \(a=(a_{n})_{n=0}^{d}\) (\(a_{0}=1\) since \(p\) is monic). As shown in Section A.6, the coefficients of the transfer function can be extracted in closed-form as \(b=\mathsf{poly}(\mathsf{eig}(\mathsf{A}-\mathsf{BC}))+\mathsf{poly}(\mathsf{ eig}(\mathsf{A}))(1-h_{0})\) and \(a=\mathsf{poly}(\mathsf{eig}(\mathsf{A}))^{\mbox{\tiny$\perp$}}\)4; Footnote 4: \(\mathsf{eig}(\mathsf{A})\) contains the eigenvalues of \(\mathsf{A}\). \(\mathsf{poly}(r)\) yields the coefficients of the polynomial whose roots are the elements of \(r\in\mathbb{C}^{d}\).
2. **Construct companion matrices** Given the coefficients \(a\) and \(b\) a new set of canonical state-space matrices which realize the transfer function can be obtained following the recipe of Section A.5.

The resulting companion SSM is equivalent the the original one since they share the same transfer function.

LaughingHyena: Further Details

### Parametrization of Modal Interpolators

**Complex-conjugate states** Assuming even distilling dimension \(d\), we pick poles \(\lambda_{n}\) and residues \(\mathbb{R}_{n}\) in complex-conjugate pairs:

\[\mathsf{A}=\operatorname{diag}(\lambda_{1},\cdots,\lambda_{d/2},\lambda_{1}^{ *},\cdots,\lambda_{d/2}^{*})\] (B.1)

\[\mathsf{C}=\frac{1}{2}[R_{1},\cdots,R_{d/2},R_{1}^{*},\cdots,R_{d/2}^{*}]\]

which allow partitioning the state-space matrices as

\[\mathsf{A}=\begin{bmatrix}\lambda&\lambda^{*}\end{bmatrix},\quad\mathsf{C}= \frac{1}{2}\begin{bmatrix}R&R^{*}\end{bmatrix},\] (B.2)

where

\[\lambda=\operatorname{diag}(\lambda_{1},\cdots,\lambda_{d/2})\;\text{ and }\;R=[R_{1},\cdots,R_{d/2}].\] (B.3)

If we also partition the state as \(x=(\bar{x},\bar{x}),\;\bar{x},\bar{x}\in\mathbb{C}^{d/2}\), the resulting recurrence has the form

\[\bar{x}_{t+1} =\lambda\bar{x}_{t}+\mathbb{1}_{d/2}\;u_{t}\] (B.4) \[\tilde{x}_{t+1} =\lambda^{*}\tilde{x}_{t}+\mathbb{1}_{d/2}\;u_{t}\]

We have

\[\bar{x}_{t}=\lambda^{t}\bar{x}_{0}+\sum_{j=0}^{t-1}\lambda^{t-j-1}\mathbb{1}_ {d/2}u_{t},\quad\tilde{x}_{t}=[\lambda^{*}]^{t}\tilde{x}_{0}+\sum_{j=0}^{t-1}[ \lambda^{*}]^{t-j-1}\mathbb{1}_{d/2}u_{t}\] (B.5)

Thus, if \(\tilde{x}_{0}=\bar{x}_{0}^{*}\), then \(\tilde{x}_{t}=\bar{x}_{t}^{*}\) for all \(t>0\). Hence, at inference time we only need to propagate forward half of the state - say \(\bar{x}\) - and then compute the output as

\[y_{t} =\mathsf{D}u_{t}+\frac{1}{2}(R\bar{x}_{t}+R^{*}\bar{x}_{t}^{*})\] (B.6) \[=\mathsf{D}u_{t}+\mathfrak{R}\{R\bar{x}_{t}\}\] \[=\mathsf{D}u_{t}+\mathfrak{R}\{R\}\mathfrak{R}\{\bar{x}_{t}\}- \mathfrak{I}\{R\}\mathfrak{I}\{\bar{x}_{t}\}\]

This parametrization allows to update only half of the state, reducing the time and memory cost compared to an _unconstrained_ linear system with complex coefficients. However, the _implicitly_ achieved realness of the output (assuming \(\mathsf{D}=h_{0}\in\mathbb{R}\) and \(u_{t}\in\mathbb{R}\)) comes at a cost of expressivity: such a system is equivalent to an unconstrained complex linear system of dimension \(d/2\) of which we only keep the real part of the output.

**Poles and residues** For the modal interpolation, the parametrization is analogous to the one of a diagonal state space model [33]. Poles \(\lambda_{n}\) and residues \(R_{n}\) need both to be complex numbers. In [33] the authors suggest parametrizing real and imaginary components of \(\mathsf{B}\) and \(\mathsf{C}\) matrices while representing the eigenvalues \(\lambda_{n}\) in polar form, \(\lambda_{n}=r_{n}e^{\mathsf{i}\alpha_{n}}\) with \(r_{n}\) and \(\alpha_{n}\) being themselves exponential functions of the actual trainable parameters, \(r_{n}=e^{-e^{\nu_{n}}},\;\alpha_{n}=e^{\zeta_{n}}\) leading to

\[\lambda_{n}=\exp\{-\exp\{\nu_{n}\}+i\exp\{\zeta_{n}\}\},\quad\nu_{n},\zeta_{n }\in\mathbb{R}\] (B.7)

This ensures stability of the poles \(|\lambda_{n}|<1\) and positive-only phases \(\alpha_{n}\). For the purpose of distillation we propose a simplified parametrization as follows:

1. We only parametrize the \(\mathsf{C}\) vector. Parametrizing both \(\mathsf{B}\) and \(\mathsf{C}\) is redundant and increases the computational cost of performing each step of the recurrence. The residues \(R_{n}\) correspond in fact to \(R_{n}=\mathsf{C}_{n}\mathsf{B}_{n}\) of a diagonal state space model. Setting \(\mathsf{B}=\mathbb{1}_{d}\) saves parameters without harming expressivity. Further if \(\mathsf{B}\) is different from \(\mathbb{1}_{k}\) it needs to be multiplied to \(u_{t}\) at each recurrence step. \(\mathsf{C}_{n}=R_{n}=\mathfrak{R}[R_{n}]+i\mathfrak{I}[R_{n}]\) and \(\mathfrak{R}[R_{n}],\mathfrak{I}[R_{n}]\) are the trainable parameters of the residue.
2. For the purpose of distillation we have no benefit in forcing the eigenvalues of the model to be stable, i.e. constrained to lie strictly inside the unit circle. Instead, such constrain may actually harm the expressivity of the approximant. We choose the the simpler parametrization \(\lambda_{n}=r_{n}e^{\mathsf{i}\alpha_{n}},\;r_{n},\alpha_{n}\in\mathbb{R}\).

### Distillation as Rational Interpolation

**Distillation as _rational interpolation_** Approximating a filter with an SSM can be thus achieved by fitting a proper rational function to the (truncated) transfer function of the original filter \(H_{L}(z){:=}\sum_{t=0}^{L}h_{t}z^{-t}\). That is,\[\text{Find }a,\ b\text{ such that }\ h_{0}+h_{1}z^{-1}+\dots+h_{L}z^{-L}\approx h_{0}+Q_{b}(z)/P _{a}(z).\] (B.8)

A modern15 way to solve this problem by \(\mathcal{H}_{2}\) error minimization via gradient descent16. We can use the Fast Fourier Transform (FFT) to evaluate both the target and distilled transfer functions and solve:

Footnote 15: In the late 19th century, Henri Padé had already proposed a closed-form solution of the above problem that achieves \(o(z^{-L})\) error for \(z\to\infty\) using \(L{=}2d\) samples of the impulse response. His method [31] solves a \(L\)-dimensional linear problem that, however, is known to often become numerically ill-conditioned even with small \(d\)[51]

\[\min_{a,b\in\mathbb{R}^{d}}\ \sum_{k=0}^{L}|\mathsf{FFT}_{L}[h]_{k}-h_{0}- \mathsf{FFT}_{L}[b]_{k}/\mathsf{FFT}_{L}[a]_{k}|^{2}.\] (B.9)

To ensure stability of the distilled filters and well-conditioned gradient descent dynamics, the roots of the denominator polynomial must strictly lie inside the unit circle (\(\rho(\mathsf{A})<1\)). This, in turn, requires constraining the coefficients \(a\) into the region \(\{a:\mathsf{poly}(a)\text{ is stable}\}\) which is by itself an open research problem [52, 53]. Experimentally, we observe that standard coefficient normalization techniques overly restrict the parameters space and lead to poor distillation performances at reasonable order.

Proofs

### Proof of Lemma 2.1

Generating \(K\) tokens with a long convolution layer (2.1) from a length-\(T\) prompt has time complexity \(\mathcal{O}(T\log_{2}T+TK+K^{2})\) and requires \(\mathcal{O}(L)\) memory.

Proof.: We compute the time complexity memory of a length-\(T\) prompt processing (_pre-filling_) and subsequent auto-regressive decoding of \(K\) tokens. The auto-regressive generation of long convolution computes the next token as by

\[t=T,\dots,T+K-1\ \Rightarrow\ y_{t}=\sum_{j=0}^{t-1}h_{t-j}y_{j}\] (C.1)

The pre-filling step is needed to prime this recurrence by computing the first \(T\) outputs till \(y_{T-1}\) from the length-\(T\) prompt \(u\). This is just a convolution between two length-\(T\) signal and requires \(\mathcal{O}(T\log_{2}T)\) time and linear memory. The auto-regressive decoding of \(K\) tokens requires \(K\) steps (C.1) with the length of the sequences increasing by \(1\) at each step. Thus we have a total asymptotic complexity of

\[\sum_{k=0}^{K-1}(T+k)=TK+\frac{1}{2}K(K+1).\] (C.2)

and requires at worst (\(k=K-1\)) to store the length \(T+K=L\) generated output sequence, i.e. \(\mathcal{O}(L)\) memory. In the limit we thus have a total time complexity of \(\mathcal{O}(T\log_{2}T+TK+K^{2})\) and \(\mathcal{O}(L)\) memory. 

### Proof of Lemma 2.2

Generating \(K\) tokens with a SSM (2.2) from a length-\(T\) prompt has time complexity \(\mathcal{O}(T\log_{2}T+dK)\) and requires \(\mathcal{O}(d)\) memory.

Proof.: In autoregressive mode, the cost of generating one token is the cost of evaluating the state recurrence (2.2). Each step then requires \(\mathcal{O}(d)\) time and memory for the class of SSMs considered in this work (see Lemma A.8). Hence, generating \(K\) tokens costs \(\mathcal{O}(dK)\) time and constant \(\mathcal{O}(d)\) memory (we only need to store the current state).

The recurrence is initialized for autoregressive generation with the post-prompt state \(x_{T-1}\) and output \(y_{T-1}\). The latter can be recovered in linear time and memory \(\mathcal{O}(T)\) by definition \(y_{T-1}=\sum_{j=0}^{T-1}h_{t-j}u_{j}\) (assuming to have the impulse response \(h\) available) and state \(x_{T-1}\) in \(\mathcal{O}(dT)\) time and \(d\) memory through the recurrence. The overall asymptotic cost is therefore \(\mathcal{O}(dL)\) time and \(\mathcal{O}(d)\) memory. 

Note that, for prompts and SSMs of practical sizes we usually have \(d>\log_{2}T\). In such a case the state \(x_{T-1}\) can be computed in \(T\log_{2}T\) time rather than \(dT\) by Proposition 3.2.

### Proof of Lemma 2.3

Generating \(K\) tokens with self-attention from a length-\(T\) prompt has time complexity \(\mathcal{O}(T^{2}+TK+K^{2})\) and requires \(\mathcal{O}(L)\) memory.

Proof.: The proof is identical to the one of Lemma 2.1, with the only difference of a quadratic asymptotic cost \(\mathcal{O}(T^{2})\) to process the prompt obtain the \(kv\) cache. 

Self-attention suffers with long contexts: it is significantly more expensive in prefilling than long convolutions and SSMs due to its quadratic cost. Nonetheless, in autoregressive mode, self-attention reaches the same overall asymptotic complexity \(\mathcal{O}(TK+K^{2})\) as long convolutions (with the memory overhead of having to cache \(k\) and \(v\)).

### Proof of Proposition 3.1

If A has semi-simple eigenvalues \(\lambda_{n}\in\mathbb{C}\), then the transfer function of the system can be decomposed as \(\hat{H}(z){=}\sum_{n=1}^{d}R_{n}/(z-\lambda_{n})\) where \(R_{n}\in\mathbb{C}\) is the residue associated with the pole \(\lambda_{n}\).

[MISSING_PAGE_EMPTY:29]

D is set to \(h_{0}\) by default. The result is proven showing that (3.2) can be written in the form \(\hat{h}_{t}=\mathsf{CA}^{t-1}\mathsf{B}\) for \(t>0\). If we choose \(\mathsf{A}=\mathsf{diag}(\lambda)\) then the impulse response becomes

\[\hat{h}_{t}=\sum_{n=1}^{d}R_{n}\lambda_{n}^{t-1}=\mathsf{C}[\mathsf{diag}( \lambda)]^{t-1}\mathsf{B}=\sum_{n=1}^{d}\mathsf{C}_{n}\mathsf{B}_{n}\lambda_{ n}^{t-1}\]

The choice \(\mathsf{B}_{n}=1\) for all \(n=1,\ldots,d\), \(\mathsf{B}=\mathbb{1}_{d}\) and \(\mathsf{C}_{n}=R_{n}\) finalizes a modal canonical state-space realization of the distilled filter. The \(\mathcal{O}(d)\) time complexity of the corresponding recurrent step is guaranteed by the decoupling of each state equation from another,

\[x_{t+1}^{n} =\lambda_{n}x_{t}^{n}+u_{t}\quad n=1,\ldots,d\] \[y_{t} =\sum_{n=1}^{d}R_{n}x_{t}^{n}+h_{0}u_{t}.\]

Each of the \(d\) state equations can be computed (in parallel) in \(\mathcal{O}(1)\) time. The output equation is a dot product requiring \(d\) multiplications and \(d\) additions, hence the \(\mathcal{O}(d)\) time compexity of the recurrence.

### Proof of Proposition 3.2

\(x_{T}=(v_{T},\ldots,v_{T-d})\) where \(v=g*u\) and \(g\) is the filter whose transfer function is \(1/\mathsf{den}(\hat{H})(z)\)

and can be evaluated in \(\tilde{\mathcal{O}}(T)\).

Without loss of generality, let us assume to have converted the distilled filter in canonical form (i.e. we have unrestricted access to the coefficients of the rational transfer function) and let \(\mathsf{D}=0\). We use the notation of Section A.5. In \(z\)-domain, the state-to-input relation is given by

\[Y(z)=\mathsf{C}X(z)=\left[\beta_{1}\quad\cdots\quad\beta_{d}\right]X(z)\]

On the other hand \(Y(z)=\hat{H}(z)U(z)=q(z)/p(z)U(z)\). Therefore,

\[\left[\beta_{1}\quad\cdots\quad\beta_{d}\right]X(z)=\frac{q(z)}{p (z)}U(z)\] \[\Leftrightarrow\] \[\Leftrightarrow\] \[\Leftrightarrow X(z)=\begin{bmatrix}z^{-1}\\ \vdots\\ z^{-d}\end{bmatrix}\frac{1}{p(z)}U(z)\]

Let \(V(z)=U(z)/p(z)\). From the shift property of the \(z\)-transform it holds,

\[\mathcal{Z}\{x\}(z)=X(z)=\begin{bmatrix}z^{-1}\\ \vdots\\ z^{-d}\end{bmatrix}V(z)\quad\Leftrightarrow\quad x_{t}=\left(v_{t-1},v_{t-2}, \cdots,v_{t-d}\right)\ \forall t>0.\]

\(v\) can be obtained in \(\tilde{\mathcal{O}}(L)\) time via an FFT-convolution of the input \(u\) and \(g\), the filter resulting from inverse transforming \(1/p(z)\). The proof is convoluded setting \(t=L\)

### Proof of Theorem 4.1

Notation.We will be denoting the all \(1\) row vector of size \(k\), given by \([1\quad 1\quad\dots\quad 1\quad 1]\), and the all \(0\) row vector of size \(k\), given by \([0\quad 0\quad\dots\quad 0\quad 0]\), as \(\mathbf{1}^{k}\) and \(\mathbf{0}^{k}\), respectively. We will also construe the standard basis vector \(e_{i}\) as a column vector in these notes. Next, we will adhere to the following matrix indexing convention: \(\mathsf{A}_{ij}\) is the entry in the \(i\)th row and the \(j\)th column, \(\mathsf{A}[i,:]\in\mathbb{F}^{1\times n}\) denotes the \(i\)th row, and \(\mathsf{A}[:,j]\in\mathbb{F}^{m\times 1}\) denotes the \(j\)th column of \(\mathsf{A}\in\mathbb{F}^{m\times n}\). Here, we also use \(\mathsf{0}^{m\times n}\in\mathbb{R}^{m\times n}\) and \(\mathsf{I}_{n}\) to denote the matrix of all zeros and the identity matrix of dimension \(n\), respectively. Moreover, we extend the outer product between two vectors to a tensor product using the symbol \(\otimes,\) the computation of which is carried out batch-wise with some dimension of one or both of the input tensors. Finally, we express the binary encoding of \(i\in[n]\) in a row vector form, given by \(\mathsf{B}_{i}\in\mathbb{Z}_{2}^{P_{n}},\) where \(P_{n}\) is the closest power of 2 to \(n\).

Language and Model Description.The language \(\Lambda\) has \(s\) keys and \(s\) values: \(L_{K}:=\{k_{1},\dots,k_{s}\},\ L_{V}:=\{v_{1},\dots,v_{s}\}.\) Formally, the language \(\Lambda\) consists of sequences \(x\in(L_{K}\times L_{V})^{s}\times L_{K}\), where there is an associated mapping \(f_{x}:L_{K}\to L_{V}.\) For each sequence, the odd indices in \([L]\) belong to \(L_{K}\), for \(x_{1},x_{3},\dots,x_{L}\), and we define

\[x_{2\cdot i}=f_{x}(x_{2\cdot i-1})\] (C.3)

The last item \(x_{L}\in\{x_{1},x_{3},\dots,x_{L-1}\}\), called the _query_, must be one of the keys that has appeared in \(x\) already. Our goal is to produce \(f_{x}(x_{L})\) at the end of the sequence, which we refer as the _associated value_. This problem is termed as the _associative recall problem_[55].

We will now outline the Hyena layer [2] with multiple heads as follows.

```
0: Input sequence \(u\in\mathbb{R}^{L\times D}\) from the previous layer, long convolution filter \(\mathsf{T}_{h}\), number of heads \(M\).
1:\(q^{m},k^{m},v^{m}\leftarrow\mathsf{Projection}(u)\) for \(m\in[M]\).
2:for\(m=1,\dots,M\)do
3: Perform the outer product \(z^{m}\gets k^{m}\otimes v^{m}\in\mathbb{R}^{L\times N\times N},\) where \(N:=D/M\).
4: Apply the convolution independently and compute \(y_{t}^{m}\leftarrow\mathsf{T}_{h}(z_{t}^{m})q_{t}^{m}\in\mathbb{R}^{L\times N}\)
5: Average the output \(\overline{y}\leftarrow(\sum_{m}y^{m}/M\)
6: Retrieve the value \(f(k_{L})\) of the key \(k_{L}\) from \(\overline{y}[L,:]\). ```

**Algorithm 1** Hyena

In order to prove Theorem 4.1, we need the following technical statement concerning sparse recovery of a heavy-hitter.

**Proposition C.1** (Heavy-Hitter Recovery).: _Let \(x\in\mathbb{R}^{s}\) be a vector with one entry bounded by \(1\pm\frac{1}{3}\overline{\sqrt{s}}\)--referred as the heavy-hitter--and the rest of the entries bounded by \(\pm\frac{1}{3}\overline{\sqrt{s}}\). Then, there exists a matrix \(\mathsf{S}^{(m)}\in\mathbb{R}^{s\times O(\sqrt{s}\log s)}\) such that the position of the heavy-hitter in \(x\) can be inferred from the average of \(M\) measurements with \(\mathsf{S}^{(m)}\) given by \(\left(\sum_{m}x\mathsf{S}^{(m)}\right)/M\) with probability of error \(\leq\frac{1}{s}\)._

Before presenting the proof of Proposition C.1, we use it to prove Theorem 4.1 as follows.

Proof of Theorem 4.1.: We take \(D=O(\sqrt{s}\log^{2}s)\) and \(M=243\cdot\log s\) so that \(N=O(\sqrt{s}\log s)\) and use the same projections and filters for each head. We will start by describing the projections of the input. To this end, let \(E:[L]\to 2s\) define a map from the row indices of \(u\) to the keys \(k_{i}\) and values \(f_{x}(k_{i})\) given by

\[E(t)=\begin{cases}i,&\text{$t$ odd, $x_{t}=k_{i}$},\\ i+s,&\text{$t$ even, $x_{t-1}=k_{i}$},\end{cases}\] (C.4)

Here, we note that we also have

\[E(t)=E(t-1)+s,\quad\text{$t$ even}\] (C.5)

as the even indices are defined as \(x_{t}=f_{x}(x_{t-1})\) for \(t\) even (C.3), whence \(x_{t-1}\in L_{K}\) as \(t-1\) is odd.

Next, we can separate the keys \(q\), queries \(q\) and values \(v\) from the input sequence \(u\). For keys and queries, we will be using the Johnson-Lindenstrauss embedding [56]. We state its guarantee here.

For a set of points \(P\subseteq\mathbb{R}^{s}\), let \(\epsilon,\delta>0\) with \(k\geq 2\ln\left(\frac{2s}{\delta}\right)/\epsilon^{2}\), and \(f:\mathbb{R}^{s}\rightarrow\mathbb{R}^{k}\) be the randomly constructed linear map from [56], then with probability of error \(\leq\delta,\) we have

\[|\langle f(x),f(y)\rangle-\langle x,y\rangle|\leq\epsilon\]for all \(x,y\in P\).

More precisely, we take \(\mathsf{R}\in\mathbb{R}^{O(\sqrt{s}\log s)\times s}\) to be the matrix representation of \(f\) with \(\epsilon:=\frac{1}{3\sqrt{s}}\) and \(\delta:=\frac{1}{s^{c}}\) for some \(c>1\) so that \(\mathsf{R}[:,i]=f(e_{i})\). Thus, we define

\[q^{m}[t,:]=\begin{cases}\mathsf{R}[:,E(t-1)],&E(t-1)\leq s\\ 0,&\text{otherwise}.\end{cases}\] (C.6)

For values, we use the heavy-hitter recovery matrix as described in Proposition C.1 so that we have

\[v^{m}[t,:]=\begin{cases}\mathsf{S}^{(m)}[E(t),:],&E(t)>s\\ 0,&\text{otherwise},\end{cases}\] (C.7)

Further, using \(\mathsf{1DConv}\) (equivalently, in terms of polynomials, \(h(X):=X\)), we can shift the queries to get the projection for keys \(k\) so that we have \(k^{m}[t,:]=q^{m}[t-1,:]\)

The Hyena filters, along with the specific convolution being performed by \(\mathsf{T}_{h}\), are specifically described in terms of polynomial multiplications, for all \(m\in 1,\ldots,M\), as follows.

\[\mathsf{T}_{h}(X):=\sum_{i=0}^{L}X^{i}.\]

Here, we note that \(\mathsf{T}_{h}(u)\) takes the cumulative sum over the input. That is, for all \(i\), we have

\[\mathsf{T}_{h}(u)[i,:]=\sum_{j=0}^{i}u[j,:]\]

We will now compute \(z^{m}\) as follows

\[z^{m}=k^{m}\otimes v^{m}\]

Further, applying the convolution, we get

\[\mathsf{T}_{h}(z^{m}_{t})=\sum_{i=0}^{t}k^{m}[i,:]\otimes v^{m}[i,:]\]

For inference, it suffices to show that the last row of the output \(y\) recovers the output with high probability. Indeed, let \(t^{\prime}\in[L]\) denote the row index of the value associated to the query such that the corresponding key has the following relation

\[u_{t^{\prime}-1}=u_{L}.\] (C.8)

Finally, we multiply by the query \(q\) across \(L\). Specifically, we now look at the computation of the \(L\)th row of \(y\):

\[y^{m}[L,:] =\left(\sum_{t=0}^{L}k^{m}[t,:]\otimes v^{m}[t,:]\right)q^{m}[t,:]\] \[=\sum_{t=0}^{L}\left(q^{m}[L,:]^{\top}k^{m}[t,:]\right)v^{m}[t,:]\] \[=\sum_{t=0}^{L}\left(q^{m}[t^{\prime}-1,:]^{\top}q^{m}[t-1,:] \right)v^{m}[t,:]\] (C.9) \[=\sum_{\begin{subarray}{c}t\in[L]\\ t\text{ even}\end{subarray}}\left(\left(\mathsf{R}[:,E(t^{\prime}-1)]\right)^{ \top}\mathsf{R}[:,E(t-1)]\right)\mathsf{S}^{(m)}[E(t),:]\] (C.10) \[=\sum_{\begin{subarray}{c}t\in[L]\\ t\text{ even}\end{subarray}}^{L}\left(\mathsf{R}e^{\top}_{E(t^{\prime}-1)} \mathsf{R}e_{E(t-1)}\right)\mathsf{S}^{(m)}[E(t),:]\] (C.11)

Here, we are using the fact that \(k^{m}[t^{\prime}-1,:]=q^{m}[L,:]\) due to (C.8) in (C.9). We then change the indexing from (C.9) and (C.11) by observing that all the odd entries corresponding to values are zeroed out in \(\mathbf{K}\) (cf. C.6). Finally, we simply substitute (C.6) and (C.7) in (C.10) and (C.11), respectively.

Next, we define \(x\in\mathbb{R}^{s+1}\) with

\[x_{j}:=\mathsf{R}e^{\top}_{E(t^{\prime}-1)}\mathsf{R}e_{j}\] (C.12)

where \(j=t-1\) with \(t\in[L]\) and \(t\) even. Here, \(x\in\mathbb{R}^{n+1}\) is a vector of size \(s+1\) as there are \(s+1\) such even numbers in \([L]\). Note that \(x\) is the vector with a heavy-hitter from Proposition C.1. To see this, observe that we have \(\big{|}x_{E(t^{\prime}-1)}-1\big{|}\leq\frac{1}{3\sqrt[s]{s}}\) and \(|x_{j}|\leq\frac{1}{3\sqrt[s]{s}}\) for all \(j\neq E(t^{\prime}-1)\). Using (C.11), with probability \(\geq 1-\frac{1}{s^{\epsilon}}\), we then have

\[y^{m}[L,:]=x\mathsf{S}^{(m)}.\] (C.13)

By Proposition C.1, we can then infer the position of the key at \(t^{\prime}\) with probability of error \(\frac{1}{s}\). By the union bound, we can then retrieve the corresponding value with probability at least \(1-(\frac{1}{s}+\frac{1}{s^{\epsilon}})\). 

We will now prove C.1 as follows.

Proof of C.1.: We will assume that \(s\) is a power of \(2\) for the sake of simplicity. We first specify how we will construct such an \(\mathsf{S}^{(m)}\in\mathbb{R}^{s\times O(\sqrt{s}\log s)}\). Let \(h:[s]\to[\sqrt{s}]\) be a hash function. We define \(\tilde{\mathsf{S}}\in\mathbb{R}^{s\times\sqrt{s}}\) to be

\[\tilde{\mathsf{S}}[:,i]=\sum_{j:h(j)=i}e_{j}.\]

That is, each column \(i\) of \(\tilde{\mathsf{S}}\) is the sum of the standard basis vectors \(\mathbf{e}_{j}\) such that \(j\) is mapped by \(h\) to \(i\). In other words, the locations of the non-zero entries in column \(i\) correspond to the preimage of \(i\) under \(h\). We then multiply each non-zero entry of \(\tilde{\mathsf{S}}\) independently at random by \(\pm 1\). Next, we replace the \(k\)th row in \(\tilde{\mathsf{S}}\) by multiplying all non-zero \(\pm 1\) entries at index \(i\) with the binary representation of \(i\) to get a matrix \(\mathsf{S}^{(m)}\in\mathbb{R}^{s\times(\sqrt{s}\times\log s)}\). That is, for a non-zero entry at index \(i\) in row \(k\), we replace the \(i\)th entry with \(\pm 1\cdot\mathbf{B}_{i}\). Note here that each column still has at most \(\sqrt{s}\) non-zero entries. Finally, we stack \(243\cdot\log s\)-many copies of \(\mathsf{S}^{(m)}\) as heads so that each copy produces independent measurements \(x\mathsf{S}^{(m)}\). Here, we want to emphasize that each such copy uses fresh randomness for multiplying the non-zero entry of \(\tilde{\mathsf{S}}\) independently at random by \(\pm 1\).

Now, we will show that the average of the measurements with matrices \(\mathsf{S}^{(m)}\in\mathbb{R}^{s\times\sqrt{s}\log s}\) can locate the heavy-hitter in \(x\), where \(x\) is the vector of inner products from (C.12). For this purpose, we first specify the algorithm for decoding the heavy-hitter.

```
0: The vector \(y\) such that \(y=x\mathsf{S}\).
1: Split \(y\) into \(243\cdot\log s\) blocks \(y^{(m)}\in\mathbb{R}^{\sqrt{s}\log s}\), each of which is a result of multiplying \(x\) by \(\mathsf{S}^{(m)},m\in[243\cdot\log n]\).
2: Take the average \(\overline{y}\leftarrow\frac{1}{243\cdot\log s}\sum_{k}y^{(m)}\).
3:\(\bm{b}\gets I_{r}(|\overline{y}|)\in\mathbb{R}^{\sqrt{s}\log s}\), cf. (C.14).
4: Retrieve \(\bm{b}\) by isolating the binary representation of the position of the heavy-hitter in \(x\). ```

**Algorithm 2**Decoder

Here, we define the function \(I_{r}:\mathbb{R}^{\sqrt{s}\log s}\to\mathbb{Z}_{2}^{\sqrt{s}\log s}\) to \([x\mathsf{S}]_{m}\) that rounds each entry of its input to the nearest integer:

\[I_{r}\left([x\mathsf{S}]_{m}\right)=\mathsf{S}^{(m)}[i,:].\] (C.14)

That is, in both cases, we retrieve the row in \(\mathsf{S}^{(m)}\) that corresponds to the heavy-hitter in \(x\). Since the rows in \(\mathsf{S}^{(m)}\) are distinct, we can also infer the position of the heavy-hitter in \(x\) with probability 1.

We now show that \(y=x\mathsf{S}\), which consists of \(243\cdot\log n\) many independent copies of \(y^{(m)}=x\tilde{\mathsf{S}}^{(m)}\). Instead, notice that we can analyze \(\tilde{y}=x\tilde{\mathsf{S}}\) since each \(\tilde{y}\) is the replacement of non-zero entries of \(\tilde{\mathsf{S}}\) with the binary representation of their indices times \(y\). We will drop the superscript for now to avoid cluttering the notation. We can then make the following claim:

\[|\tilde{y}_{i}|=1\pm O(\epsilon\cdot\sqrt[s]{s})\text{ and, for }j\neq i,|\tilde{y}_{j}|=O( \epsilon\cdot\sqrt[s]{s}).\] (C.15)

For the above claim, we note that the first part follows from the latter as it suffices to show that all the non-zero heavy-hitters contribute \(O(\epsilon\sqrt[s]{s})\) to the sum \(\tilde{y}_{i}=\langle x,\tilde{\mathsf{S}}[:,i]\rangle\). Since each column in \(\tilde{\mathsf{S}}\) only interacts with \(\sqrt{s}\) sized sub-vector of \(x\), each \(\tilde{y}_{j}\) for non-heavy hitters can be expressed as

\[\tilde{y}_{j}=\langle\overline{x},\overline{\mathsf{S}}_{j}\rangle\]

 with

\[\overline{x},\overline{\mathsf{S}}_{j}\in\mathbb{R}^{\sqrt{s}}\]

,

where \(\overline{\mathsf{S}}_{j}\in\mathbb{R}^{\sqrt{s}}\) contains the non-zero entries of \(\tilde{\mathsf{S}}_{j}\) and \(\overline{x}\) is obtained by extracting the entries with corresponding indices from \(x\). Here, we have \(\|\overline{x}\|_{2}\leq\epsilon\sqrt[s]{s}\) since each entry associated with the non-heavy hitter is bounded as \(x_{i}\leq\epsilon\), and thus, \(\|\overline{x}\|_{2}=\sqrt{\sum_{i}x_{i}^{2}}\leq\sqrt{\sum_{i}\epsilon^{2}}= \sqrt{\sqrt{s}\cdot\epsilon^{2}}=\epsilon\sqrt[s]{s}\).

Consequently, as \(\tilde{\mathsf{S}}_{j}\) is independently random \(\pm 1\), we then must have

\[\big{|}\langle\overline{x},\overline{\mathsf{S}}_{j}\rangle\big{|}\leq\frac{1}{3}\] (C.16)with constant probability for \(j\neq i\). To see this, note that

\[\mathbb{E}[\langle\overline{x},\overline{\mathsf{S}}_{j}\rangle^{2}] =\sum_{k,\ell}\mathbb{E}[\overline{\mathsf{S}}_{jk}\cdot\overline{ \mathsf{S}}_{j\ell}]\cdot\overline{x}_{i}\cdot\overline{x}_{j}\] \[=\lVert\overline{x}\rVert_{2}^{2},\]

where the last equality follows since \(\mathbb{E}[\mathsf{S}_{jk}\cdot\mathsf{S}_{j\ell}]=\delta_{k,\ell}\) by the distribution on entries of \(\mathsf{S}_{j}\). Now, we use Jensen's inequality [57] to get the following bound on the expectation of \(\big{|}\langle\overline{x},\overline{\mathsf{S}}_{j}\rangle\big{|}\).

\[\mathbb{E}\left[\big{|}\langle\overline{x},\overline{\mathsf{S}}_{j}\rangle \big{|}\right]\leq\sqrt{\mathbb{E}[\langle\overline{x},\overline{\mathsf{S}}_ {j}\rangle^{2}]}\leq\epsilon\sqrt[4]{s}.\] (C.17)

We then use the expectation above to bound the relevant probability as follows:

\[\Pr\left[\big{|}\langle\overline{x},\overline{\mathsf{S}}_{j} \rangle\big{|}\leq\frac{1}{3}\right] \geq 1-\Pr\left[\big{|}\langle\overline{x},\overline{\mathsf{S}}_{j} \rangle\big{|}\geq\frac{1}{3}\right]\] \[\geq 1-3\epsilon\cdot\sqrt[4]{s},\]

where we apply Markov's inequality [57] in (C.18). That is, we have shown that \(\tilde{y}_{j}\) is bounded by \(1/3\) with constant probability for \(j\neq i\), and \(\tilde{y}_{i}\) is thus bounded by \(1\pm\frac{1}{3}\). Note here that each of the \(m\)-copies \(\tilde{y}_{i}^{(m)}\) will have identical guarantees.

Now, define the average \(\overline{\tilde{y}}_{j}:=\frac{1}{243\cdot\log s}\sum_{m}\tilde{y}^{(m)}\) so that \(\overline{y}_{j}\) (line 3 in Decoder) is the corresponding replacement of the non-zero entries with the binary representation of their indices. We now claim that this average \(\overline{\tilde{y}}_{j}\leq 4/9<1/2\) with high probability for \(j\neq i\). To this end, we employ the multiplicative Chernoff bound [57] on the independent random variables \(\{\tilde{y}_{j}^{(h)}\}_{h}[0,1]\) with \(\mathbb{E}\left[\sum_{m}\tilde{y}_{j}^{(m)}\right]\leq 81\cdot\log s\) to get

\[\Pr\left[\overline{\tilde{y}}_{j}>\frac{4}{9}\right] =\Pr\left[\sum_{m}\tilde{y}_{j}^{(m)}>\left(1+\frac{1}{3}\right) \frac{1}{3}\cdot 243\cdot\log s\right]\] \[\leq\Pr\left[\sum_{m}\tilde{y}_{j}^{(m)}\geq\left(1+\frac{1}{3} \right)81\cdot\log s\right],\] \[\leq\exp\left(-\left(\frac{1}{3^{2}}\cdot 81\cdot\log s\right)/3 \right),\] \[=\frac{1}{s^{3}}.\]

Therefore, we have shown that the average \(\overline{\tilde{y}}_{j}\) is less than \(1/2\) with probability at least \(1-\frac{1}{s^{3}}\) for \(j\neq i\). Consequently, we will have \(\overline{\tilde{y}}_{i}\) bounded by \(1\pm\frac{1}{2}\). Using the union bound over each \(j\neq i\) and the \(\log s\) bits in the binary representation of \(j\), we can then show that \(\overline{y}_{j+m}<1/2\) for each \(j\neq i,m\in[0,\log s]\) with probability \(1-\frac{\log s}{s^{2}}\gg 1-\frac{1}{s}\).

[MISSING_PAGE_FAIL:35]

### Downstream Evaluation

We benchmark the downstream performance of MultiHyena and distilled MultiHyena on standard language modeling tasks from the LM-Eval-Harness [42] and HELM [41] suites. As a reference baseline, we evaluate Pythia [44]\(160\)M.

Our objective is to quantify the absolute performance of MultiHyena and the downstream impact of distillation. We use the same procedure outlined in Section D.2 to distill MultiHyena.

### Benchmarking

To demonstrate the superior performance of Laughing Hyena for autoregressive generation, we conduct a series of experiments to benchmark its latency, throughput, and memory usage for autoregressive generation with initial prompt length \(T\) and number of generated tokens \(K\). For each experiment, we compare the performance of Laughing Hyena against a Transformer, a hybrid H3-attention model with 2 attention layers and a Hyena model. The latter two have been shown to match or achieve lower perplexity than Transformers on standard datasets (Wikitext103 and The Pile). All experiments are carried out on a NVIDIA A100 with 80GB in float16 precision. Missing measurements for any model indicate Out of Memory (OOM) errors while doing autoregressive inference for that particular model.

Figure D.2: Mean, lower and upper bounds across channels and layers of the distillation errors on 355M H3 model for both its IIR and FIR filters.

Figure D.3: Mean, lower and upper bounds across channels and layers of the distillation errors on 1.3B H3 model for both its IIR and FIR filters.

Peak throughputWe first evaluate the throughput (number of tokens generated per second) across different batch sizes, using a typical generation workload consisting of a prompt of length 512 and generating 256 tokens. Figure 1.1 measures peak throughput of different models. Since Laughing Hyena does not require caching intermediate kv-projections during generation, reduced memory requirements at a fixed model size allow it to process larger batch sizes.

Prompt lengthAutoregressive generation in Laughing Hyena is achieved through a two-step process: an initial prefill step that uses the length\(-T\) prompt to initialize the state \(x_{T}\) and that generates all \(K\) tokens. In Figure 5.3 we demonstrate how the prefill step scales for different prompt lengths, keeping batch sizes fixed at \(64\). Since prefilling in Laughing Hyena is carried out efficiently via convolutions (as described in Section 3.4), throughput scales more favorably than Transformers. Other models capable of prefilling via convolutions also achieve higher throughputs than Transformers but are ultimately slower than Laughing Hyena during the generation phase.

State throughputWe measure the impact of SSM state dimension on the throughput of Laughing Hyena. Keeping batch sizes fixed reveals minimal impact for all dimensions smaller than

Figure D.4: Mean, lower and upper bounds across channels and layers of the distillation errors on 2.7B H3 model for both its IIR and FIR filters.

Figure D.5: Mean, lower and upper bounds across channels and layers of the distillation errors on Hyena and MultiJena models.

\(100\), which are sufficient to distill all models discussed in this work. All other measurements provided in this Section are carried out with a standard order \(16\). We note that it may be possible to further increase peak throughput by leveraging reduced memory footprints achieved by extremely small SSMs.

Latency over sequence lengthWe benchmark the time taken to generate a variable number of tokens, starting from a prompt of length 512 tokens at batch size 1 (Figure D.11). Laughing Hyena tracks highly optimized Transformers. We note that Laughing Hyena is asymptotically more efficient than Transformers; however, this regime is bottlenecked by hardware-specific implementation details and optimizations. We expect optimized, platform-specific implementations of Laughing Hyena to outperform Transformers even at batch size 1. When the prompt is long, the prefilling step becomes the bottleneck, and all convolutional models outperform Transformers.

Parameter scalingTo better understand how the performance of Laughing Hyena scales, we benchmark its latency, throughput, and peak memory utilization for autoregressive generation and 125M, 355M, 1.3B, 2.7B and 6.7B parameters. We compare the performance to that of Transformers, HybridH3, and Hyena at the same number of parameters and report the results in Figure D.11. For the latency measurement, we use a batch size of 1 and benchmark the time taken to generate 128 tokens, starting from a prompt of length 512 tokens. For throughput and peak memory scaling against the number of parameters, we use a batch size of 64 and measure the throughput for generating 256 tokens starting with a prompt of length 512.

Figure D.6: Initialized and pre-trained convolution filters of H3.

Figure D.7: Initialized and pre-trained long convolution filters of MultiHyena.

Figure D.8: Initialized and pre-trained long convolution filters of Hyena (355 \(M\)).

Figure D.10: Distribution of Hankel singular values for H3 long convolution filters. The values decay rapidly.

Figure D.9: Distribution of Hankel singular values for Hyena and MultiHyena long convolution filters. MultiHyena filters have larger effective dimension, as evidenced by slower decay.

Figure D.11: Generation latency, throughput and peak memory of Transformers, H3, Hyena and Laughing Hyena.

Additional Experiments

### Associative Recall with MultiHyena

We follow the setup of [2] and train 2-layer Hyena and MultiHyena (with \(8\) heads) to solve associative recall via a standard next-token prediction objective. We focus on the sequence length \(64\)k, high vocabulary size setting, and push vocabulary sizes past the maximum values considered in [2]. At vocabulary size \(60\), a difference between MultiHyena and Hyena can be observed (Table E.1), as experimental support for Theorem 4.1.

### Analysis of Hankel Singular Values of Pretrained Large Convolution Sequence Models

#### Model Order Reduction of H3

The H3 model is constructed with a combination of diagonal SSMs and shift SSMs. There exists various classical model order reduction techniques for these different types of SSMs. The following sections aim to present the formulation and effectiveness of two classical approaches on obtaining the compressed representation of a H3 model. More specifically, we study modal truncation and balanced truncation for compressing diagonal SSMs and shift SSMs respectively.

#### e.3.1 Modal Truncation

A discrete diagonal SSM (\(\text{A}=\text{diag}(\lambda_{1},\dots,\lambda_{d})\), \(\text{B}\in\mathbb{C}^{d\times 1}\), and \(\text{C}\in\mathbb{C}^{1\times d}\)) can be directly converted into a residue-pole transfer function as follows:

\[(\text{A},\text{B},\text{C})\to H(z)=\sum_{i=1}^{d}\frac{r_{i}}{z- \lambda_{i}},\] (E.1)

where residue \(r_{i}=\text{B}_{i}\text{C}_{i}\). Modal truncation aims to compress such a transfer function by essentially reducing the summation over \(d\) to \(n<d\), of the \(n\) most influential modes. The influence from each node can be isolated by expressing it using the \(h_{\infty}\) norm of the system as follows:

\[\|H(z)\|_{\infty}=\sum_{i=1}^{d}\left\|\frac{r_{i}}{z-\lambda_{i}}\right\|_{ \infty}\leq\sum_{i=1}^{d}\frac{|r_{i}|}{|1-|\lambda_{i}||}.\] (E.2)

Each mode \(i\) can be ranked using the bound formulated above. Subsequently, the \(d-n\) lowest modes could be discarded to form a reduced order model. Figure E.1 illustrates the monotonically decreasing \(l_{\infty}\) error with the increase in system order. However, this model reduction approach is only suitable for diagonalizable SSMs.

Figure E.1: Modal truncation model reduction error (\(||h(t)-h_{n}(t)||_{\infty}\)) across all diagonal SSM layers of the trained H3 125M model.

\begin{table}
\begin{tabular}{|c|c|} \hline Model & Accuracy \\ \hline Hyena & \(65\) \\ MultiHyena & \(98\) \\ \hline \end{tabular}
\end{table}
Table E.1: Associative recall accuracy, sequence length \(64\)k, vocabulary size \(60\).

[MISSING_PAGE_FAIL:44]

Figure E.3: Balanced truncation model reduction error (\(||h(t)-h_{n}(t)||_{\infty}\)) across all convolutional layers of the trained MultiJena 155M model.

Figure E.4: Balanced truncation model reduction error (\(||h(t)-h_{n}(t)||_{\infty}\)) across all convolutional layers of the trained Hyena 155M model.