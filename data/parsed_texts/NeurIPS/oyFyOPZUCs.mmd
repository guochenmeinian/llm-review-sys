# Language-based Action Concept Spaces Improve Video Self-Supervised Learning

Kanchana Ranasinghe

Stony Brook University

kranasinghe@cs.stonybrook.edu &Michael Ryoo

Stony Brook University

mryoo@cs.stonybrook.edu

###### Abstract

Recent contrastive language image pre-training has led to learning highly transferable and robust image representations. However, adapting these models to video domain with minimal supervision remains an open problem. We explore a simple step in that direction, using language tied self-supervised learning to adapt an image CLIP model to the video domain. A backbone modified for temporal modeling is trained under self-distillation settings with train objectives operating in an _action concept space_. Feature vectors of various action concepts extracted from a language encoder using relevant textual prompts construct this space. A large language model aware of actions and their attributes generates the relevant textual prompts. We introduce two train objectives, _concept distillation_ and _concept alignment_, that retain generality of original representations while enforcing relations between actions and their attributes. Our approach improves zero-shot and linear probing performance on three action recognition benchmarks.

## 1 Introduction

Actions in videos are defined by individual objects, their relationships, and interaction [1; 2]. Video self-supervised learning focuses on discovering representations aware of such action attributes directly from video content with no human supervision [3]. Particularly in the case of videos, where manual human annotation can be both expensive and noisy, such self-supervised approaches are invaluable.

A recent variant of self-supervision explores learning with loosely paired image-caption pairs, leading to highly transferable and robust representations such as CLIP [4]. These approaches obtain zero-shot performance often comparable to fully-supervised methods. However, their counterparts in the video domain [5; 6; 7; 8; 9; 10; 11] do not exhibit the same generality. In fact, some approaches training CLIP on videos [11; 12] perform subpar to image-CLIP under zero-shot settings (see Table 2). Such behaviour can be attributed to lesser availability and more noisy nature of labelled (or paired caption) video datasets [3]. This motivates exploration into self-supervised learning (SSL) techniques that can learn from videos under less supervision while utilizing existing image CLIP [4] like representations. Existing state-of-the-art video SSL approaches [13; 14] learn highly transferable representations from videos, but combining these with image CLIP representations is not straightforward. In fact, despite methods like SVT [13] being able to utilize image SSL representations [15] for weight initialization to achieve better performance, using image CLIP representations instead for weight initialization leads to performance subpar to image CLIP (see Table 4). This raises necessity for alternate video SSL approaches compatible with CLIP like image representations and is our key motivation.

In this work, we explore self-supervised learning techniques that adapt image CLIP models [4] to video domain under entirely self-supervised settings, dependent on no form of video level labels or captions. Under this setting, natural language can still provide strong cues regarding attributes that compose an action category [16; 17]. We leverage this idea to propose a novel _language-based_ self-supervised learning objective. Following a standard self-distillation and multi-view based SSLformulation [15; 13], we introduce language aligned feature spaces, _action concept spaces_, where our SSL objectives operate. Large-language models (LLMs) [18], given their extensive world knowledge [19; 20], serve as an ideal tool to generate necessary textual concepts for these spaces. We also introduce regularization suitable for our language aligned SSL objective to prevent collapse during training. Our resulting framework is termed _Language-based Self-Supervision_, or LSS.

In contrast to existing video self-supervised learning approaches [13; 14], our proposed LSS retains and improves transferability of image CLIP representations much better (see Tables 1 and 4). Additionally, our language aligned learning framework allows direct zero-shot operation on downstream tasks. Moreover, unlike video CLIP methods with similar zero-shot capabilities [5; 6; 7; 8; 9; 10; 11] that utilize per-video labels / captions for learning, our proposed LSS requires only videos for training.

We summarize our key contributions as follows:

* Self-supervised learning paradigm capable of retaining and improving strengths of CLIP like image representations for video domain operation
* Video specific self-supervised learning objectives, namely _concept distillation_ and _concept alignment_, that enforce relations between action categories and their visual attributes
* Novel language-based video self-supervised learning framework operating zero-shot on downstream action classification tasks without requiring per-video labels / captions for training

Experiments on action recognition datasets showcase state-of-the-art performance for our learned representations under linear-probing, standard zero-shot, and transductive zero-shot settings.

## 2 Related Work

**Self-Supervised Learning in Videos** was initially dominated by pretext tasks specific to the video domain [21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32]. Recently a shift to contrastive losses led to [33; 34; 35; 36; 37; 38] with some variants focused on video specific view generation [39; 40; 41; 42; 13]. An alternate direction has been masked auto-encoders [14]. To the best of our knowledge, existing video self-supervised learning (SSL) approaches operate purely within the visual domain. By video SSL, we refer to methods that utilize only videos with no paired captions (or labels) for each video during training. In contrast, our proposed LSS learns purely from videos in a self-supervised manner, integrating pre-trained language-image models to learn language aligned representations.

**Zero-shot Action Recognition** began with manual attribute and feature selection [43; 44; 45; 46; 47] with later works utilizing action word embeddings [16; 17]. The idea of connecting action categories with elaborate descriptions of those actions, within language embedding spaces [48; 49] has been a next step and is closely related to our work. This idea is also explored in image domain to boost zero-shot performance [50]. While our work is inspired by such approaches, in contrast, we use relations between such actions and descriptions as self-supervised signals for learning. Recent image CLIP models [4; 51] are another line of works achieving strong performance on some video classification tasks, with only single frame processing. Multiple approaches build on image CLIP [4] to learn video level representations [11; 52; 53; 54; 12; 55] under fully-supervised settings. While achieving strong performance on the training datasets, their zero-shot improvements over CLIP are minimal or even subbar (see Table 2). Therein, LSS focuses on zero-shot performance under self-supervised settings while retaining (and improving) the generality of the representation space.

**Self-training** methods leverage pseudo-labels on unlabeled data [56; 57; 58] for supervised-fashion training. Recently they have been combined with CLIP models for zero-shot operation [59; 60]. While inspired by such self-training approaches, our proposed LSS differs in its continuous feature space self-distillation, language-based relations enforcing, video domain operation, and cross-dataset transfer for zero-shot operation.

**Adapting image-CLIP models to video** under fully-supervised settings has gathered much interest [5; 6; 7; 8; 9; 10]. Expanding backbones for temporal modeling, multi-modal fusion, secondary training objectives, partial parameter updates, and scaling-up data are key ideas explored [55; 10]. In contrast, LSS is a first to operate under self-supervised settings using no video annotations.

**Contemporary work** in [61] adapts image CLIP features to video tasks label free similar to our work. ViFi-CLIP [62] introduces zero-shot action recognition benchmarks and similarly adapts CLIP to videos retaining generality. Using LLMs for action recognition is also explored in [63].

## 3 Language-based Self-Supervision (LSS)

In this section, we present our proposal, Language-based Self-Supervision (LSS). The generality and robustness of shared image-language representation spaces such as that of CLIP [4] allow interesting manipulations of visual representations using language. We explore such manipulations under the setting of visual self-supervised learning focusing on video understanding. Self-supervised objectives can operate within a latent space constructed with language, retaining language alignment of learned visual representations. This allows better interpretability of representations as well as zero-shot inference. We discuss the four key components of our approach: backbone architecture, concept distillation objective, modifications to avoid collapse, and concept alignment objective.

### Backbone Architecture

Our approach introduces a _text classifier_ to self-distillation based SSL works [15; 13], in place of the projector network. Given a data sample \(x\), let \(x_{1},x_{2}\in\mathbb{R}^{(C,T,H,W)}\) be two augmented views generated using video specific transformations following [13], where \(C=3,T=8,H=W=224\) are channel, time, and spatial dimensions respectively.

**Visual Encoder:** A visual encoder, \(\theta_{v}\), processes \(x_{i}\) to produce feature \(f_{i}\in\mathbb{R}^{768}\). We utilize the pre-trained image encoder of CLIP [4] expanded for temporal modelling using factorized space-time attention. The vision transformer variant of CLIP is selected to allow our factorized space-time attention. In particular, we use ViT-B/16 architecture for the the image encoder, in which for a given augmented view with \(H=W=224\) and \(T=8\), each transformer block processes 8 temporal and 196 spatial tokens separately in sequential order, and the embedding dimension of each token is \(\mathbb{R}^{768}\). In addition to the input tokens from the data sample, one classification token [64; 65] serves as the final feature vector output by the network, namely \(f_{i}\), which is common to the CLIP image encoder. This classification token is inflated and processed suitably following [66] to accommodate our modifications for factorized space-time attention. We follow [66] to zero-initialize additional time-attention parameters, achieving outputs identical to the pre-trained CLIP image encoder at start of training.

**Text Classifier:** Inspired by [67], a set of \(n\) language embeddings extracted from the CLIP text encoder, \(\theta_{t}\), are used to construct the weight parameter of a linear layer (with no bias term), which

Figure 1: Our overall setup contains three components: visual teacher model (green), visual student model (red), and language model (blue). We utilize the text encoder of CLIP as our language model and extract _concept vectors_ relevant to action labels and descriptions of those actions. A visual encoder (containing a space-time backbone) is partially initialized with the visual encoder of CLIP and used to obtain sample specific features. The generated concept vectors are used to project these features to a _concept space_ where our proposed _concept distillation_ and _concept alignment_ losses are applied.

we call our text classifier, \(\theta_{c}\). The role of this text classifier is to project visual features \(f_{i}\) to a vector space defined by those \(n\) embeddings, producing \(\tilde{f_{i}}\in\mathbb{R}^{n}\). Next we discuss these vector spaces (referred to as action concept spaces) and the text classifier module in detail.

### Action Concept Spaces

Self-supervised learning approaches following exponential moving average (EMA) based self-distillation [68; 15; 13] utilize a projector network (MLP) to operate in a higher dimensional feature space. This is expected to minimize train-test domain gaps, handle noisy positive sample pairs, and better discriminate nuanced feature differences [69]. Focused on these notions, we propose an alternate _concept space_ composed of a set of basis vectors defined by language-based action concepts. Our language-based self-supervision objectives operate within such concept spaces.

**Concept Spaces:** Building off the assumption that text encoder features capture subtle differences between distinct actions categories, we hypothesize that necessary nuanced distinctions between these actions will be better captured in our proposed concept spaces. The defining parameters of concept spaces are their basis vectors, \(b_{i}\). Normalized embeddings (extracted from text encoder, \(\theta_{t}\)) of various natural language captions (\(c_{i}\)) relevant to action categories are used as these basic vectors.

\[b_{i} =\theta_{t}(c_{i})\ /\left||\theta_{t}(c_{i})\right|_{2}^{2}\] (1) \[\mathbf{b} =\left[b_{1},b_{2},...\,b_{n}\right]^{T};\mathbf{b}\in\mathbb{ R}^{(n,d)}\] (2)

Note that these basis vectors are not necessarily orthogonal. As illustrated in Fig. 2, a single set of basis vectors, \(\mathbf{b}\), defines one action concept space. We define two sets of basic vectors: action category vectors and action description vectors. Action category vectors relate to a single action label which is converted to a caption using textual prompting following [4]. Action description vectors are averaged embeddings of multiple descriptions and visual characteristics relevant to individual action categories. These two distinct sets of basic vectors lead to two distinct concept spaces which we name _category concept space_ and _description concept space_ respectively.

**Category Concept Space:** We explore 3 different strategies to construct the category concept space. The base setup uses action labels from Kinetics-400 [70], UCF-101 [71], and HMDB-51 [72] datasets, leading to a set of 530 (400 + 101 + 51, ignoring overlaps) basis vectors. Our next goal of connecting LLMs and their action awareness occurs in the second two strategies. We utilize LLMs [18] and visual-LLMs [73] to extract large sets of action category labels. While we explore this idea of expanding the basis vector set with LMM based additional action labels in Section 4, the base setup containing a modest 530 categories was sufficient to improve downstream task performance.

**Description Concept Space:** This space is constructed conditioned on the previous category concept space. For each action label used in the latter, we extract 4 distinct descriptions and a set of visual characteristics relevant to that action label using a large language model (LLM). The role of the LLM is to inject its world knowledge (i.e. awareness on videos, actions, and their attributes) into our learned representations during self-supervised learning. In detail, we prompt GPT-3 [18] to generate such descriptions and characteristics using procedure outlined in Appendix A. We highlight that GPT-3 is used here as an intelligent LLM containing world knowledge on videos and actions, in order to create natural language descriptions for given action category labels. The textual outputs generated for each action label are processed by our text encoder to produce multiple embeddings for a single action label. These embeddings are averaged to produce the corresponding basis vector for the description concept space. Note how this leads to a common dimensionality between the two concept spaces as well as one to one correspondences between the basic vectors of the spaces, which we leverage in our self-supervision objectives.

Figure 2: We illustrate a toy concept space constructed with the three action concepts: run, swim, and walk. In this example, the text classifier projects visual feature \(f_{i}\) into the 3-dimensional toy concept space to produce \(\tilde{f_{i}}\).

### Concept Distillation

We now describe our primary self-supervised learning objective, concept distillation. Standard multi-view based self-supervision enforces a network to encode the common information between two augmented (distorted) views of a data sample [69]. This common information can be considered as the augmentation invariant signal present in the original data sample [69; 74]. In the case of self-distillation based approaches [15; 13], a higher dimensional feature space is utilized to enforce the self-supervision objectives. Instead, we propose to use action concept spaces as an alternative.

Proposed concept distillation depends on an action concept space and visual video features aligned to the basis vectors of that space. Given our visual features \(f_{i}\in\mathbb{R}^{d}\), we obtain projected \(\tilde{f}_{i}\in\mathbb{R}^{n}\) as,

\[\tilde{f}_{i}=\mathbf{b}\;(\;f_{i}/\|f_{i}\|_{2}^{2})=[b_{1}\cdot f_{i}^{\prime },b_{2}\cdot f_{i}^{\prime},...\;b_{n}\cdot f_{i}^{\prime}]^{T}\] (3)

**Similarity Calculation:** Projecting normalized visual video features to a concept space corresponds to calculating the dot-product similarity with each basic vector of the concept space. The projected vector \(\tilde{f}_{i}\) can be viewed as a similarity _score distribution_ across all basis vectors of the concept space. Inspired by [67], we implement this similarity calculation as a linear layer with weight matrix \(\mathbf{b}\) and bias terms zero. We refer to this layer as the _text classifier_. Similar to [67], our text classifier remains frozen (no parameter updates), but in our case, this is to retain the original language distribution.

**Concept Distillation Objective:** Viewing projected features for two augmented views of a single video as score distributions, we argue that the underlying signal of the original video would relate to a unique score distribution to which score distributions of each view should be similar. Therein, following our EMA teacher based self-distillation setup (see Section 3.1 for details), we enforce the score distribution to be consistent across views. Given two views \(x_{1},x_{2}\) of a single video, our teacher and student visual encoders process them respectively to produce \(f_{1},f_{2}\). The text classifier projects these to concept space, producing score distributions \(\tilde{f}_{1},\tilde{f}_{2}\). We obtain our objective, \(\mathcal{L}_{\text{CD}}\) as:

\[\hat{f}_{i}[k] =\frac{\exp(\tilde{f}_{i}[k]/\lambda_{i})}{\sum_{j=1}^{n}\exp( \tilde{f}_{i}[j]/\lambda_{i})}\] (4) \[w_{s} =\max(\hat{f}_{1})\] (5) \[\mathcal{L}_{\text{CD}}(\tilde{f}_{1},\tilde{f}_{2}) =-w_{s}\cdot\sum_{j=1}^{n}\hat{f}_{1}[j]\log\hat{f}_{2}[j]\] (6)

The teacher and student score distributions, \(\tilde{f}_{1},\tilde{f}_{2}\), are softmax normalized in Eq. (4), with temperature terms \(\lambda_{1}=0.1,\lambda_{2}=1\) for sharpening only the teacher score distribution. A significance score \(w_{s}\) is calculated for each sample in Eq. (5). In the softmax normalized teacher score distribution (\(\hat{f}_{1}\)), the maximum value is high when peaked at a single action concept and low when peaked at multiple action concepts. Considering the noisy nature of multi-peak teacher score distributions, we utilize \(w_{s}\) to minimize their overall effect during training. Our overall \(\mathcal{L}_{\text{CD}}\) is thus implemented as in Eq. (6).

**Distinct Concept Spaces:** Given the two distinct action concept spaces defined in Section 3.2, we utilize two parallel text classifiers to implement each, and obtain two score distributions, one for each concept space. Defining score distributions \(\tilde{f}_{i}^{C},\tilde{f}_{i}^{D}\) for category and description concept spaces respectively, we apply our \(\mathcal{L}_{\text{CD}}\) on each pair separately to obtain two losses \(\mathcal{L}_{\text{CD}}^{\text{X}}\) for X\(\in\){C,D} as:

\[\mathcal{L}_{\text{CD}}^{\text{X}}=\mathcal{L}_{\text{CD}}(\tilde{f}_{1}^{ \text{X}},\tilde{f}_{2}^{\text{X}})\] (7)

We highlight how our concept spaces implemented as text classifiers are maintained intact by freezing the text classifier during training. This allows our approach to perform direct zero-shot inference, making concept distillation additionally advantageous over standard video SSL techniques.

### Uniform Distribution Prior

Avoiding collapse is a key concern in SSL methods [15; 13; 69] and recent self-distillation based approaches utilize feature sharpening and centering operations to avoid collapse [15; 13]. While we similarly perform sharpening operations on the teacher outputs, given the nature of our action concept space, performing a learned vector mean subtraction based centering operations can break the meaningful structure of score distributions. Instead, we enforce a uniform distribution prior onthe expected score distribution over the entire training dataset. The centering operation proposed in [15] acts similarly pushing representations towards a uniform distribution while the sharpening operation counters its effect. We approximate expectation over the dataset as a moving average of mean score distributions at each train iteration and the uniform prior is enforced as:

\[\tilde{f}^{\text{X}}_{\text{MA}} =\tau\cdot\tilde{f}^{\text{X}}_{2}+(1-\tau)\cdot\tilde{f}^{\text{X }}_{\text{MA}}\] (8) \[\mathcal{L}^{\text{X}}_{\text{UP}} =-\frac{1}{n}\sum_{j}\log\tilde{f}^{\text{X}}_{\text{MA}}[j]\] (9)

where the hyper-parameter \(\tau=0.5\) is fixed during training. We highlight that \(\mathcal{L}_{\text{UP}}\) is necessary for convergence with concept distillation and is added to the concept distillation objective, \(\mathcal{L}^{\text{X}}_{\text{CD}}\).

### Concept Alignment

Aligning action category labels and their descriptions or attributes within some embedding space has been explored in video SSL under multiple settings [48; 49]. Motivated by these promising results, we explore how such alignment can be integrated to improve our framework with _concept spaces_. In Section 3.2, we define two distinct action concept spaces constructed from category labels and detailed category descriptions respectively. We hypothesize that explicit alignment of video features between these two spaces based on their one to one relationship can learn additional information. Therein, we introduce our concept alignment objective, \(\mathcal{L}_{\text{CA}}\), as follows:

\[\mathcal{L}_{\text{CA}}=\mathcal{L}_{\text{CD}}(\tilde{f}^{\text{C}}_{1}, \tilde{f}^{\text{D}}_{2})+\mathcal{L}_{\text{CD}}(\tilde{f}^{\text{D}}_{1}, \tilde{f}^{\text{C}}_{2})\] (10)

**Overall SSL Objective:** Reusing \(\mathcal{L}_{\text{CD}}\) from Eq. (6), we match score distributions across our two concept spaces instead of within a single concept space. \(\mathcal{L}_{\text{CD}}(\tilde{f}^{\text{C}}_{1},\tilde{f}^{\text{D}}_{2})\) aligns student description score distribution \(\tilde{f}^{\text{D}}_{2}\) to teacher category score distribution \(\tilde{f}^{\text{C}}_{1}\) while \(\mathcal{L}_{\text{CD}}(\tilde{f}^{\text{C}}_{1},\tilde{f}^{\text{D}}_{2})\) aligns student category score distribution \(\tilde{f}^{\text{C}}_{2}\) to teacher description score distribution \(\tilde{f}^{\text{D}}_{1}\). Combining all terms, we obtain:

\[\mathcal{L}=(\mathcal{L}^{\text{C}}_{\text{CD}}+\mathcal{L}^{\text{C}}_{\text{ UP}})+(\mathcal{L}^{\text{D}}_{\text{CD}}+\mathcal{L}^{\text{D}}_{\text{UP}})+ \mathcal{L}_{\text{CA}}\] (11)

### Concept Space Variants

Our baseline concept space (described in Section 3.2) utilizes labels from three standard video datasets (Kinetics-400, UCF-101, HMDB-51). However, we want to ensure scalability with more data and no label leakage to downstream evaluation tasks. With this goal, we propose 2 additional variants of action concept spaces tagged LSS-B and LSS-C. These variants do not use any form of ground truth textual labels from datasets. Moreover, they leverage the world awareness (i.e. knowledge on videos and actions) of LLMs to generate extensive action categories. Our baseline setup is hereafter referred as LSS-A.

For LSS-B, we use GPT-3 [18] to generate a large set of action labels. We first prompt GPT to categorize all common human actions / activities into 20 groups. For each group, we again ask GPT to generate at least 100 visually diverse action categories. These are all collected to create a set of 2000 action labels. We then use projections of these labels in CLIP text-encoder representation space to eliminate labels of high semantic similarity (spectral clustering in feature space from [75] to identify similar features), achieving 1000 diverse action categories. So our 1000 action categories for LSS-B are generic, not tied to any of our training datasets, and scalable with more data.

For LSS-C, we generate a label set using only videos from the training dataset. We use PCA based clustering to identify 2000 representative videos from a randomly sampled subset (50,000) of our training dataset and then use image-captioning models (LLaVa [73]) on video center frames to generate a diverse set of 2000 action labels. This is further reduced to 500 eliminating labels that are similar in feature space of the CLIP text encoder. In this case, our generated labels are tied to the training dataset, but uses no textually annotated category labels. We use only the videos (and an image-to-text captioning model) to generate our label set, still resulting in a scalable framework.

Note that each of these alternate strategies relates to construction of our category concept space. Given the selected set of textual category labels of this space, the description concept space is constructed in the same common way (as described in Section 3.2). We also reiterate that LSS-B and LSS-C variants use no category information from train / test datasets.

Experiments

In this section, we first describe our experimental setup followed by discussion of results for linear probing self-supervised representations and zero-shot analysis.

**Datasets:** We use three standard action recognition benchmark datasets in our experiments: Kinetics-400 [70], UCF-101 [71], and HMBD-51 [72]. Kinetics-400 is a large-scale dataset containing 240,000 training videos and 20,000 validation videos belonging to 400 different action classes. On average, these videos are of duration around 10 seconds, with 25 frames per second (i.e., around 250 frames per video). UCF-101 and HMBD-51 are small-scale datasets each containing 13k videos (9.5k/3.7k train/test) belonging to 101 classes and 5k (3.5k/1.5k train/test) videos belonging to 51 classes respectively. They also contain similar duration videos.

**Self-supervised Training:** Our SSL training phase uses the train split of Kinetics-400 dataset [70]_without_ using any per-video labels. We train for 15 epochs using a batch size of 32 across 4 NVIDIA-A5000 GPUs using ADAM-W [76; 77] optimizer on the student model with an initial learning rate of \(1e-5\) following a cosine decay schedule. The EMA teacher is updated from student weights after each training iteration with a decay ratio of \(2e-4\). Unless otherwise specified, this model is used for all downstream task evaluations.

**Transductive Training:** For selected experiments, we additionally perform self-supervised training directly on the train split of each downstream dataset. For Kinetics-400, we follow the same setup described above. In the case of HMDB-51 and UCF-101, we perform self-supervised training for a longer duration of 100 epochs (smaller train sets) leaving all other hyper-parameters unchanged.

**View Generation:** Our self-supervised setup requires two views of a single video. We sample two clips from a video following global view generation in [13]. In detail, we select two random intervals from a video, and uniformly sample (equal time gaps between frames) 8 frames of 224x224 spatial dimensions from within that interval. Standard video augmentations from [78] are also applied randomly for each view.

**Linear Probing:** We follow standard linear probing settings on our two downstream datasets to evaluate quality of representations learned by our self-supervised learning phase. We follow the same settings in [13] for fair comparison. Our visual encoder is frozen and a randomly initialized linear layer is trained on the train split of the downstream dataset in a fully-supervised manner. We train for 15 epochs using a batch size of 128 across 4 NVIDIA-A5000 GPUs using ADAM-W [76; 77] optimizer with an initial learning rate of \(1e-3\) following a cosine decay schedule. During inference, we sample three 224x224 dimensional spatial crops with 8 uniformly spaced frames from each video following prior work [13; 36].

**Zero-Shot Inference:** For zero-shot inference, we project class labels of downstream datasets to our text encoder feature space, and construct an alternate text classifier. Using this text classifier, we make zero-shot predictions. This setup is identical to dot-product similarity based inference in CLIP [4] (explanation in Section 3.3). In line with prior work [13; 36], we feed three 224x224 dimensional spatial crops with 8 uniformly spaced frames sampled from each video to the visual encoder and average its output feature embedding prior to normalized dot-product calculation in the text encoder.

### Linear-Probing Analysis

We first evaluate LSS under linear probing settings on HMDB-51 & UCF-101 datasets. Our results (top-1 accuracy) are reported in Table 1. Our proposed LSS achieves state-of-the-art results on both datasets, outperforming prior approaches. Note that MoDist [81] and BraVe [89], both of which additionally utilize video-level optical flow (OF) for self-supervision, are not directly comparable. Still, our LSS showcases competitive performance to those, even without such motion information.

### Zero-Shot Analysis

Our LSS provides the additional advantage of zero-shot operation unlike standard video SSL approaches. To this end, we conduct two forms of zero-shot experiments. First, we evaluate LSS on standard zero-shot classification, where our model trained on Kinetics-400 (under SSL settings) is evaluated on the two downstream datasets, HMDB-51 and UCF-101. We report these results (top-1 accuracy) in Table 2. Compared to prior work utilizing per-video labels / captions for training, we achieve competitive performance. We note that MOV [7] trained under supervised settings with per-video labels and additional audio information is not a direct comparison.

In contrast to most prior approaches, LSS uses no video level labels for its Kinetics-400 training. In particular, LSS has not seen any labelled videos during its training process. Compared to prior

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline \hline Method & Backbone & ITP & TFLOPS & Frames & Epochs & HMDB & UCF \\ \hline MemDPC [34] (ECCV’20) & R2D3D-34 & ✗ & - & 64 & - & 30.5 & 54.1 \\ CoCLR [35] (NeNeiIPS’20) & S3D & ✗ & 0.07 & 32 & 100 & 52.4 & 77.8 \\ VideoMoCo [80]\({}_{\text{CVPR~{}21}}\) & R(2+1)D & ✗ & 17.5 & 32 & 200 & 49.2 & 78.7 \\ CVRL [36]\({}_{\text{CVPR~{}21}}\) & R3D-50 & ✗ & 3.19 & 32 & 800 & 57.3 & 89.2 \\ MoDis[81] (Aviv’21) & R3D-50 & ✗ & 3.19 & 8 & 100 & 63.0 & 91.5 \\ BraVe [38]\({}_{\text{CVPR~{}21}}\) & R3D-50 & ✗ & 3.19 & 16 & - & 68.3 & 92.5 \\ Vi2CLR [82] (occup’21) & S3D & ✗ & 0.07 & 32 & 300 & 47.3 & 75.4 \\ MCN [83] (occup’21) & R3D & ✗ & 3.19 & 32 & 50 & 42.9 & 73.1 \\ CORP [84] (occup’21) & R3D-50 & ✗ & 3.19 & 16 & 800 & 58.7 & 90.2 \\ SVT [13]\({}_{\text{CVPR~{}22}}\) & ViT-B & ✗ & 0.59 & 16 & 20 & 57.8 & 90.8 \\ VideoMAE [14]\({}_{\text{NeNeiIPS~{}22}}\) & ViT-B & ✗ & 0.59 & 16 & 800 & 60.3 & 84.7 \\ \hline MERLOT [85]\({}_{\text{NeNeiIPS~{}21}}\) & ViT-B & ✓ & - & 16 & - & 55.4 & 80.1 \\ VATT [86]\({}_{\text{NeNeiIPS~{}21}}\) & ViT-B & ✓ & - & 32 & - & 66.4 & 87.6 \\ TVTS [87]\({}_{\text{CVPR~{}23}}\) & ViT-B & ✓ & 0.59 & 16 & 20 & 58.4 & 83.4 \\ LaViLa [88]\({}_{\text{CVPR~{}23}}\) & ViT-L & ✓ & - & 4 & 5 & 61.5 & 88.1 \\ LSS-A (ours) & ViT-B & ✓ & 0.59 & 8 & 20 & 69.2 & 91.0 \\ LSS-B (ours) & ViT-B & ✓ & 0.59 & 8 & 20 & **69.4** & **91.1** \\ LSS-C (ours) & ViT-B & ✓ & 0.59 & 8 & 20 & 69.1 & 90.8 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Linear Probing on HMDB-51 [72] and UCF-101 [79]: We compare our method against prior work, reporting top-1 (%) accuracy (following evaluation procedure in [13]). ‘ITP’ stands for image-text pre-training. Gray shaded methods use additional optical flow (OF) inputs for training. Nevertheless, our performance is comparable to such methods using per-video OF modality information. In contrast, we use generic language modality information and require no one-to-one language relations with individual videos during training.**

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c} \hline \hline Method & Backbone & ITP & Video Labels & Frames & HMDB & UCF \\ \hline TS-GCN [90]\({}_{\text{AAAI~{}}\text{-}\text{19}}\) & GCN & ✗ & ✓ & 16 & 23.2 & 34.2 \\ E2E [91]\({}_{\text{CVPR~{}20}}\) & CNN & ✗ & ✓ & 16 & 32.7 & 48.0 \\ ER-ZSAR [48]\({}_{\text{ICCV~{}21}}\) & CNN & ✗ & ✓ & 8 & 35.3 & 51.8 \\ ActionCLIP [11] & ViT-B & ✓ & ✓ & 32 & 40.8 & 58.3 \\ X-CLIP [12]\({}_{\text{ICCV~{}22}}\) & ViT-B & ✓ & ✓ & 32 & 44.6 & 72.0 \\ VicTR [55] & ViT-B & ✓ & ✓ & 32 & 51.0 & 72.4 \\ ViFi [62]\({}_{\text{CVPR~{}23}}\) & ViT-B & ✓ & ✓ & 32 & 51.3 & 76.8 \\ \hline \hline \(\text{MTE~{}\text{[92]}}_{\text{ICCV~{}16}}\) & - & ✗� & ✗ & - & 19.7 & 15.8 \\ ASR [93]\({}_{\text{ICML~{}17}}\) & CNN & ✗ & ✗ & 16 & 21.8 & 24.4 \\ ZSECOC [94]\({}_{\text{CVPR~{}17}}\) & - & ✗ & ✗ & - & 22.6 & 15.1 \\ UR [49]\({}_{\text{CVPR~{}18}}\) & CNN & ✗ & ✗ & 1 & 24.4 & 17.5 \\ CLIP [4]\({}_{\text{ICML~{}21}}\) & ViT-B & ✓ & ✗ & 1 & 46.5 & 69.8 \\ CLIP [4]\({}_{\text{ICML~{}21}}\) & ViT-B & ✓ & ✗ & 8 & 47.2 & 70.3 \\ LaViLa [88]\({}_{\text{CVPR~{}23}}\) & ViT-L & ✓ & ✗ & 4 & 16.6 & 18.2 \\ \hline LSS-A (ours) & ViT-B & ✓ & ✗ & 8 & 49.5 & 72.0 \\ LSS-B (ours) & ViT-B & ✓ & ✗ & 8 & 50.2 & 73.8 \\ LSS-C (ours) & ViT-B & ✓ & ✗ & 8 & **51.4** & **74.2** \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Zero-shot Transfer on HMDB-51 [72] and UCF-101 [79]: We compare LSS against prior work, reporting top-1 accuracy (%). Mean across three test splits is reported following [55]. ‘ITP’ stands for image-text pre-training and ‘Video Labels’ refers to using per-video annotations (or paired captions) for supervision during video-based training. We highlight how among directly comparable unsupervised (at video level) approaches as well as over the CLIP [4] baseline, LSS boosts zero-shot performance.**work operating under these settings, LSS achieves state-of-the-art performance on both downstream datasets as seen in the bottom half of Table 2.

An alternate setting in prior zero-shot work is transductive training, where self-supervised learning is performed directly on train splits of downstream datasets. Under this setting, we evaluate on all three datasets, Kinetics-400, HMDB-51, and UCF-101, reporting results (top-1 accuracy) in Table 3. In the case of HMBD-51 and Kinetics-400, our method achieves state-of-the-art performance. For UCF-101, we achieve competitive results, and clear improvements over a CLIP [4] baseline.

### Ablations

We next study the contribution of each component within our approach. All ablative experiments follow the same SSL phase on the Kinetics-400 train set (as described in Section 4) followed by zero-shot analysis on validation sets of HMDB-51 and UCF-101. In the case of linear probing results, training is conducted following same settings (see Section 4) on the train set of HMDB-51 followed by evaluation on its validation set.

**SSL Objectives:** First we ablate each proposed component in Eq. (11) and report results in Table 4. In addition to a direct CLIP [4] baseline, we construct two additional baselines building off CLIP [4] and SVT [13] for better comparison. CLIP\({}^{\dagger}\) baseline applies our backbone modifications (for temporal modeling) with no training, which is identical to averaging per-frame visual encoder features. SVT\({}^{\lx@sectionsign}\) baseline performs SVT [13] training with CLIP visual encoder initialization (note that language alignment breaks and zero-shot operation is not possible for this baseline). In comparison to the CLIP baselines, each proposed component, concept distillation in category and description concept spaces as well as concept alignment, leads to improvements. The comparison against SVT\({}^{\lx@sectionsign}\) highlights how our SSL approach better preserves language aligned information (contained in CLIP) that is useful even in linear probing. In contrast, the lower performance of SVT\({}^{\lx@sectionsign}\) compared to CLIP baselines indicates that generic SSL techniques may be losing useful information contained in CLIP.

\begin{table}
\begin{tabular}{l|c|c|c|c|c} \hline \hline  & CD\({}^{\circ}\) & CD\({}^{\circ}\) & CA & LP & ZS \\ \hline CLIP & ✗ & ✗ & ✗ & 63.9 & 46.5 \\ CLIP\({}^{\dagger}\) & ✗ & ✗ & ✗ & 67.3 & 47.2 \\ SVT\({}^{\lx@sectionsign}\) & ✗ & ✗ & ✗ & 62.2 & - \\ Ours & ✓ & ✗ & ✗ & 68.5 & 48.8 \\ Ours & ✓ & ✓ & ✗ & 69.0 & 49.2 \\ Ours & ✓ & ✓ & ✓ & 69.2 & 49.5 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Ablation on SSL objectives:** We ablate our proposed concept distillation (CD) applied on category (CD\({}^{\circ}\)) and description (CD\({}^{\circ}\)) concept spaces and concept alignment (CA) using linear probing (LP) & zero-shot transfer (ZS) on HMDB-51 [72] dataset. Since our approach cannot be trained without any objective, we construct two new baselines from CLIP [4] and SVT [13] (details in Section 4.3). Each proposed component obtains clear improvements over the baselines.

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline \hline Method & Backbone & ITP & Video Labels & Frames & HMDB & UCF & K400 \\ \hline UR [49]\({}_{\text{CVPR}^{\text{\tiny{(}}}\text{\tiny{18}}\text{\tiny{)}}}\) & CNN & ✗ & ✗ & 1 & 28.9 & 20.1 & - \\ TS-GCN [90]\({}_{\text{(AAAI}^{\text{\tiny{-19}}}\text{\tiny{)}}}\) & GCN & ✗ & ✓ & 16 & 23.2 & 34.2 & - \\ CLIP [4]\({}_{\text{(ICML}^{\text{\tiny{}}}\text{\tiny{21}}\text{\tiny{)}}}\) & ViT-B & ✓ & ✗ & 8 & 47.2 & 70.3 & 49.7 \\ MUST [59]\({}_{\text{(ACLR}^{\text{\tiny{-23}}}\text{\tiny{)}}}\) & ViT-B & ✓ & ✗ & 1 & 48.9 & **81.1** & 51.2 \\ \hline LSS (ours) & ViT-B & ✓ & ✗ & 8 & **55.0** & 75.6 & **54.3** \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Transductive Zero-shot Transfer on HMDB-51 [72], UCF-101 [79], and Kinetics-400 [70]:** We report top-1 accuracy (%) following the evaluation procedure in [55]. Similar to prior work, we perform dataset specific unsupervised fine-tuning (using our self-supervised objective) on the train-splits of each downstream dataset (no labels used). ‘ITP’ refers to image-text pretaining, and ‘Video Labels’ refers to video level supervised training. Note that CLIP [4] is not transductive and is included only for comparison purposes.

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline Method & Action Labels & HMDB & UCF \\ \hline CLIP & - & 47.2 & 70.3 \\ \hline Ours & K400 only (w/o U,H) & 48.4 & 71.1 \\ \hline Ours & K400+U+H (A) & 49.5 & 72.0 \\ \hline Ours & (A)+2000 words & 48.6 & 71.8 \\ Ours & (A)+10K sentences & 49.6 & 71.4 \\ \hline \hline \end{tabular}
\end{table}
Table 5: **Concept Space Ablation:** We report zero-shot accuracy (%) on HMDB-51 and UCF-101 datasets. ‘K400 only’ shows transfer to unseen downstream classes. The labels of K400 overlapping with UCF/HMDB are not used here. A CLIP [4] baseline (modified for video domain without re-training) is reported in row 1 for comparison purposes. For 2000 words & 10,000 sentences, we utilize most common nouns / verbs and example sentences for action verbs from WordNet [95; 96].

**Concept Spaces:** Our next focus is on construction of concept spaces. We explore how separately augmenting each concept space affects downstream task performance measured with zero-shot transfer. These results are reported in Table 5. First, focused on the category concept space, we construct additional category labels using 1000 most common nouns and verbs each (total of 2000) from the WordNet dataset [95; 96]. Next, we augment the description concept space using 10,000 sentences. We select these from example sentences provided for action verbs in the WordNet dataset [95; 96]. In these experiments, only the concept distillation objective is applied on these augmented spaces and concept alignment operates only on the base category set. This is because independently augmenting one of the action spaces eliminates their shared and aligned dimensionality. Results for these two settings are reported in row 2 & 3 respectively in Table 5.

**Uniform Distribution Prior:** We ablate on proposed uniform distribution prior (UDP) which acts as a regularization to prevent collapse (Section 3.4). Our results in Table 6 (left) indicate clear necessity of such regularization to prevent collapse during SSL training.

**Significance Weight in Concept Distillation:** In Eq. (6), we utilize a significance weight term, \(w_{s}\), which represents the confidence of the target concept space projection for a given sample. We note how each sample during training is a clip sampled from a video (which covers a temporal crop of video). Our intuition for this weight is to act as a way of prioritizing more important clips over the less important ones. Our ablations in Table 6 (right) indicate usefulness of this weight term.

## 5 Conclusion

We introduce a novel language-based self-supervised learning (SSL) approach for videos, termed LSS, capable of adapting strong language-aligned image representations (CLIP [4]) to the video domain. In particular, we propose two self-distillation based SSL objectives, _concept distillation_ and _concept alignment_. Our approach trains with no video level labels or paired captions similar to prior video SSL works, but retains language alignment from image CLIP enabling direct zero-shot inference. We demonstrate state-of-the art performance in terms of linear probing with the learned representations on downstream tasks. For zero-shot operation, LSS demonstrates strong performance under both standard and transductive settings, indicating a promising direction for video SSL.

**Limitations, Future Work, & Broader Impact**: The language alignment of LSS may be limited mostly to per-frame static information since the alignment is derived from image CLIP [4]. LSS cannot distinguish motion based categories like "moving object left to right". Moreover, while containing highly discriminative and generic information at image level, CLIP features lack spatial awareness at an object level [75]. Our proposed model building off these representations in inherently limited in understanding object level motion and interaction within videos. However, recent progress in localization aware CLIP models [75; 97; 98] opens avenues for leveraging their object-centric or pixel-level representations to better model such video motion patterns, opening up interesting future directions. In terms of broader impact, the datasets and pre-trained models we use possibly contain biases, which may be reflected in LSS. However, our reduced reliance on human annotations may lower additional biases.

**Reproducibility Statement**: We build a codebase derived from source code of SVT [13] & CLIP [4] and use pre-trained CLIP weights from https://github.com/openai. All experiments use publicly available datasets. Our action descriptions will be released publicly along with our codebase.

**Acknowledgements**: We thank Xiang Li for helpful discussions and server setup. We also thank Kumara Kahatapitiya and Cristina Mata for helpful discussions.

\begin{table}
\begin{tabular}{l|c|c} \hline \hline Method & HMDB & UCF \\ \hline LSS & 48.4 & 71.1 \\ LSS w/o UDP & 33.4 & 54.3 \\ \hline \hline \end{tabular}
\end{table}
Table 6: **Ablation on Regularization and Significance Weight: The effect of proposed regularization (left) and significance weight (right) is demonstrated. UDP regularization (see Section 3.4) is particularly essential to prevent collapse during the SSL training phase. Significance weight (\(w_{s}\)) also improves performance.**

## References

* [1] Michael S. Ryoo and Jake K. Aggarwal. Recognition of composite human activities through context-free grammar based representation. _2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)_, 2:1709-1718, 2006.
* 43, 2011.
* [3] Madeline Chantry Schiappa, Yogesh Singh Rawat, and Mubarak Shah. Self-supervised learning for videos: A survey. _ACM Computing Surveys_, 2022.
* [4] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In _ICML_, pages 8748-8763. PMLR, 2021.
* [5] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. CLIP-ViP: Adapting Pre-trained Image-Text Model to Video-Language Representation Alignment. _arXiv preprint arXiv:2209.06430_, 2022.
* [6] Shen Yan, Tao Zhu, Zirui Wang, Yuan Cao, Mi Zhang, Soham Ghosh, Yonghui Wu, and Jiahui Yu. Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners. _arXiv preprint arXiv:2212.04979_, 2022.
* [7] Rui Qian, Yeqing Li, Zheng Xu, Ming-Hsuan Yang, Serge Belongie, and Yin Cui. Multimodal Open-Vocabulary Video Classification via Pre-Trained Vision and Language Models. _arXiv preprint arXiv:2207.07646_, 2022.
* [8] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting Visual-Language Models for Efficient Video Understanding. In _ECCV_, pages 105-124. Springer, 2022.
* [9] Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Fine-tuned CLIP Models are Efficient Video Learners. _arXiv preprint arXiv:2212.03640_, 2022.
* [10] Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit Bansal, and Gedas Bertasius. VindLU: A Recipe for Effective Video-and-Language Pretraining. _arXiv preprint arXiv:2212.05051_, 2022.
* [11] Mengmeng Wang, Jiazheng Xing, and Yong Liu. ActionCLIP: A New Paradigm for Video Action Recognition. _arXiv preprint arXiv:2109.08472_, 2021.
* [12] Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, and Haibin Ling. Expanding language-image pretrained models for general video recognition. In _European Conference on Computer Vision_, 2022.
* [13] Kanchana Ranasinghe, Muzammal Naseer, Salman Hameed Khan, Fahad Shahbaz Khan, and Michael S. Ryoo. Self-supervised video transformer. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2864-2874, 2021.
* [14] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. _ArXiv_, abs/2203.12602, 2022.
* [15] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _ICCV_, 2021.
* [16] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking zero-shot video classification: End-to-end training for realistic applications. In _CVPR_, 2020.
* [17] Xun Xu, Timothy Hospedales, and Shaogang Gong. Transductive zero-shot action recognition by word-vector embedding. _IJCV_, 2017.
* [18] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.
* [19] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Z. Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong Wen. A survey of large language models. _ArXiv_, abs/2303.18223, 2023.
* [20] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Nick Barnes, and Ajmal S. Mian. A comprehensive overview of large language models. _ArXiv_, abs/2307.06435, 2023.
* [21] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. In _ICLR_, 2016.

* [22] Viorica Patraucean, Ankur Handa, and Roberto Cipolla. Spatio-temporal video autoencoder with differentiable memory. In _ICLR (Workshop)_, 2016.
* [23] Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial Hebert. An uncertain future: Forecasting from static images using variational autoencoders. In _ECCV_, 2016.
* [24] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In _ICML_, 2015.
* [25] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In _NeurIPS_, 2016.
* [26] Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin Murphy. Tracking emerges by colorizing videos. In _ECCV_, 2018.
* [27] Pulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In _ICCV_, 2015.
* [28] Ross Goroshin, Joan Bruna, Jonathan Tompson, David Eigen, and Yann LeCun. Unsupervised learning of spatiotemporally coherent metrics. In _ICCV_, 2015.
* [29] Phillip Isola, Daniel Zoran, Dilip Krishnan, and Edward H. Adelson. Learning visual groups from co-occurrences in space and time. In _ICLR_, 2016.
* [30] Ishan Misra, C. Lawrence Zitnick, and Martial Hebert. Shuffle and learn: Unsupervised learning using temporal order verification. In _ECCV_, 2016.
* [31] Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In _ICCV_, 2015.
* [32] AJ Piergiovanni, Anelia Angelova, and Michael S. Ryoo. Evolving losses for unsupervised video representation learning. In _CVPR_, 2020.
* [33] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsupervised spatiotemporal representation learning. _Arxiv_, 2021.
* [34] Tengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive coding. In _ICCV_, 2019.
* [35] Tengda Han, Weidi Xie, and Andrew Zisserman. Self-supervised Co-training for Video Representation Learning. _NeurIPS_, 33:5679-5690, 2020.
* [36] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. _CVPR_, 2021.
* [37] R Devon et al. Representation learning with video deep infomax. _Arxiv_, 2020.
* [38] Adria Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu Wang, Florian Strub, Corentin Tallec, Mateusz Malinowski, Viorica Patraucean, Florent Altche, Michal Valko, et al. Broaden your views for self-supervised video learning. _ICCV_, 2021.
* [39] Deng Huang, Wenhao Wu, Weiwen Hu, Xu Liu, Dongliang He, Zhihua Wu, Xiangmiao Wu, Mingkui Tan, and Errui Ding. Ascent: Self-supervised video representation learning with appearance-speed consistency. In _ICCV_, 2021.
* [40] Peihao Chen, Deng Huang, Dongliang He, Xiang Long, Runhao Zeng, Shilei Wen, Mingkui Tan, and Chuang Gan. Rspnet: Relative speed perception for unsupervised video representation learning. In _AAAI_, volume 1, 2021.
* [41] Nadine Behrmann, Mohsen Fayyaz, Juergen Gall, and Mehdi Noroozi. Long short view feature decomposition via contrastive video representation learning. In _ICCV_, 2021.
* [42] Ishan Rajendra Dave, Rohit Gupta, Mamshad Nayeem Rizve, and Mubarak Shah. TCLR: Temporal contrastive learning for video representation. _Arxiv_, 2021.
* [43] Jingen Liu, Benjamin Kuipers, and Silvio Savarese. Recognizing human actions by attributes. In _CVPR_, 2011.
* [44] Rowan Zellers and Yejin Choi. Zero-shot activity recognition with verb attribute induction. _arXiv preprint arXiv:1707.09468_, 2017.
* [45] Mihir Jain, Jan C Van Gemert, Thomas Mensink, and Cees GM Snoek. Objects2action: Classifying and localizing actions without any video example. In _ICCV_, 2015.
* [46] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. I know the relationships: Zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs. In _AAAI_, 2019.
* [47] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. Learning to model relationships for zero-shot video classification. _TPAMI_, 2020.
* [48] Shizhe Chen and Dong Huang. Elaborative Rehearsal for Zero-shot Action Recognition. In _ICCV_, pages 13638-13647, 2021.

* [49] Yi Zhu, Yang Long, Yu Guan, Shawn Newsam, and Ling Shao. Towards Universal Representation for Unseen Action Recognition. In _CVPR_, pages 9436-9445, 2018.
* [50] Sachit Menon and Carl Vondrick. Visual Classification via Description from Large Language Models. _arXiv preprint arXiv:2210.07183_, 2022.
* [51] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In _ICML_, pages 4904-4916. PMLR, 2021.
* [52] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. CLIP4Clip: An Empirical Study of CLIP for End to End Video Clip Retrieval. _Neurocomputing_, 508:293-304, 2022.
* [53] Max Bain, Arsha Nagrani, Gul Varol, and Andrew Zisserman. A CLIP-Hitchhiker's Guide to Long Video Retrieval. _arXiv preprint arXiv:2205.08508_, 2022.
* [54] Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng Li. Frozen CLIP Models are Efficient Video Learners. _arXiv preprint arXiv:2208.03550_, 2022.
* [55] Kumara Kahatapitiya, Anurag Arnab, Arsha Nagrani, and Michael S. Ryoo. Victor: Video-conditioned text representations for activity recognition. _ArXiv_, abs/2304.02560, 2023.
* [56] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In _NIPS_, pages 1195-1204, 2017.
* [57] Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In _NeurIPS_, 2020.
* [58] David Berthelot, Nicholas Carlini, Ekin D. Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. In _ICLR_, 2020.
* [59] Junnan Li, Silvio Savarese, and Steven C. H. Hoi. Masked unsupervised self-training for zero-shot image classification. _ArXiv_, abs/2206.02967, 2022.
* [60] Jonathan Kahana, Niv Cohen, and Yedid Hoshen. Improving zero-shot models with label distribution priors. _ArXiv_, abs/2212.00784, 2022.
* [61] Wei Lin, Leonid Karlinsky, Nina Shvetsova, Horst Possegger, Mateusz Kozinski, Rameswar Panda, Rogerio Schmidt Feris, Hilde Kuehne, and Horst Bischof. Match, expand and improve: Unsupervised finetuning for zero-shot action recognition with language knowledge. _ArXiv_, abs/2303.08914, 2023.
* [62] Hanoona Rasheed, Muhammad Uzair Khattak, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. Fine-tuned clip models are efficient video learners. _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 6545-6554, 2022.
* [63] Laura Hanu, Anita Lilla Vero, and James Thewlis. Language as the medium: Multimodal video classification through text only. _ArXiv_, abs/2309.10783, 2023.
* [64] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _Arxiv_, 2018.
* [65] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _ICLR_, 2021.
* [66] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is Space-Time Attention All You Need for Video Understanding? In _ICML_, page 4, 2021.
* [67] Wenhao Wu, Zhun Sun, and Wanli Ouyang. Revisiting Classifier: Transferring Vision-Language Models for Video Recognition. _AAAI_, 2023.
* [68] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. In _NeurIPS_, 2020.
* [69] Randall Balestiriero, Mark Ibrahim, Vlad Sobal, Ari S. Morcos, Shashank Shekhar, Tom Goldstein, Florian Bordes, Adrien Bardes, Gregoire Mialon, Yuandong Tian, Avi Schwarzschild, Andrew Gordon Wilson, Jonas Geiping, Quentin Garrido, Pierre Fernandez, Amir Bar, Hamed Pirsiavash, Yann LeCun, and Micah Goldblum. A cookbook of self-supervised learning. _ArXiv_, abs/2304.12210, 2023.
* [70] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the Kinetics dataset. In _CVPR_, 2017.
* [71] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A dataset of 101 human actions classes from videos in the wild. _Arxiv_, 2012.

* [72] Hildegard Kuehne, Hueihan Jhuang, Estibaliz Garrote, Tomaso Poggio, and Thomas Serre. HMDB: A large video database for human motion recognition. In _ICCV_, pages 2556-2563. IEEE, 2011.
* [73] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
* [74] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for self-supervised learning. _ArXiv_, abs/2105.04906, 2021.
* [75] Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi, Yinfei Yang, Alexander Toshev, and Jonathon Shlens. Perceptual grouping in contrastive vision-language models. _ICCV_, 2023.
* [76] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* [77] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. _ICLR_, 2019.
* [78] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal Contrastive Video Representation Learning. In _CVPR_, pages 6964-6974, 2021.
* [79] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild. _arXiv preprint arXiv:1212.0402_, 2012.
* [80] Tian Pan, Yibing Song, Tianyu Yang, Wenhao Jiang, and Wei Liu. Videomoco: Contrastive video representation learning with temporally adversarial examples. In _CVPR_, 2021.
* [81] Fanyi Xiao, Joseph Tighe, and Davide Modolo. Modist: Motion distillation for self-supervised video representation learning. _Arxiv_, 2021.
* [82] Ali Diba, Vivek Sharma, Reza Safdari, Dariush Lotfi, Saquib Sarfraz, Rainer Stiefelhagen, and Luc Van Gool. Vi2Clr: Video and image for visual contrastive learning of representation. In _ICCV_, 2021.
* [83] Yuanze Lin, Xun Guo, and Yan Lu. Self-supervised video representation learning with meta-contrastive network. In _ICCV_, 2021.
* [84] Kai Hu, Jie Shao, Yuan Liu, Bhiksha Raj, Marios Savvides, and Zhiqiang Shen. Contrast and order representations for video self-supervised learning. In _ICCV_, 2021.
* [85] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. _ArXiv_, abs/2106.02636, 2021.
* [86] Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. _ArXiv_, abs/2104.11178, 2021.
* [87] Ziyun Zeng, Yuying Ge, Xihui Liu, Bin Chen, Ping Luo, Shutao Xia, and Yixiao Ge. Learning transferable spatiotemporal representations from natural script knowledge. _ArXiv_, abs/2209.15280, 2022.
* [88] Yue Zhao, Ishan Misra, Philipp Krahenbuhl, and Rohit Girdhar. Learning video representations from large language models. _ArXiv_, abs/2212.04501, 2022.
* [89] Adria Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu Wang, Florian Strub, Corentin Tallec, Mateusz Malinowski, Viorica Patrucean, Florent Altche, Michal Valko, et al. Broaden Your Views for Self-Supervised Video Learning. In _ICCV_, pages 1255-1265, 2021.
* [90] Junyu Gao, Tianzhu Zhang, and Changsheng Xu. I Know the Relationships: Zero-Shot Action Recognition via Two-Stream Graph Convolutional Networks and Knowledge Graphs. In _AAAI_, pages 8303-8311, 2019.
* [91] Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, and Krzysztof Chalupka. Rethinking Zero-shot Video Classification: End-to-end Training for Realistic Applications. In _CVPR_, pages 4613-4623, 2020.
* [92] Xun Xu, Timothy M Hospedales, and Shaogang Gong. Multi-Task Zero-Shot Action Recognition with Prioritised Data Augmentation. In _ECCV_, pages 343-359. Springer, 2016.
* [93] Qian Wang and Ke Chen. Alternative Semantic Representations for Zero-Shot Human Action Recognition. In _ECML-PKDD_, pages 87-102. Springer, 2017.
* [94] Jie Qin, Li Liu, Ling Shao, Fumin Shen, Bingbing Ni, Jiaxin Chen, and Yunhong Wang. Zero-Shot Action Recognition with Error-Correcting Output Codes. In _CVPR_, pages 2833-2842, 2017.
* [95] George A Miller. Wordnet: a lexical database for english. In _Communications of the ACM_, pages 39-41. ACM, 1995.
* [96] Christiane Fellbaum. Wordnet and wordnets. In Keith Brown et al., editors, _Encyclopedia of Language & Linguistics_, pages 665-670. Elsevier, 2nd edition, 2005.
* [97] Jilan Xu, Junlin Hou, Yuejie Zhang, Rui Feng, Yi Wang, Yu Qiao, and Weidi Xie. Learning open-vocabulary semantic segmentation models from natural language supervision. _ArXiv_, 2023.
* [98] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. GroupViT: Semantic Segmentation Emerges from Text Supervision. In _CVPR_, pages 18134-18144, 2022.

[MISSING_PAGE_FAIL:15]