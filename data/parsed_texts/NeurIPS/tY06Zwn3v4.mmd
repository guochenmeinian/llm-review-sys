# EG-SIF: Improving Appearance Based Gaze Estimation using Self Improving Features

Vasudev Singh\({}^{\ast}\)\({}^{\ast}\)\({}^{\ast}\)

\({}^{\ast}\)Equal contribution
they provide better gaze estimation performance (Murthy and Biswas, 2021; Cheng et al., 2020). From images, gaze can be estimated through full face or head of a subject or only through eye images. In applications, where considering privacy risks are important, working on face images for training the model is difficult. Moreover, in applications like driver monitoring, eye images are preferred over face images as it needs real-time estimation on the edge devices with low computational capability.

Nevertheless, the presence of inherent adverse factors, such as inadequate lighting conditions, subjects' head movements away from the camera during eye image acquisition, etc. results in the introduction of noise and compromised resolution in certain images within the dataset. We hypothesize that datasets like MPIIGaze (Zhang et al., 2017) and RT-GENE (Fischer et al., 2018) contain a subset of images affected by these adversities, significantly impacting the accuracy of gaze estimation. Therefore, it becomes imperative for gaze estimation methods to robustly identify and address these challenges.

In order to model a robust CNN architecture, we propose an eye gaze estimation with self improving features (EG-SIF) method for learning noise independent features. Our methods consists of three main parts: 1. Segregating the dataset into two disjoint sets i.e., good images that are relatively free from adverse effects and the adverse quality images. 2. A transformation that generates adverse quality image given an image from the good set by transferring the noise distribution of the image from the adverse set. 3. A multitask end-to-end training framework with image enhancement as an auxiliary task along with gaze estimation to make network robust towards noise. This type of segregation of varying quality images and training them in different way has been accomplished for the first time in this paper. With the proposed method,1 it is observed that model is able to perform better on adverse quality images and also able to surpass the current state-of-the-art methods.

Footnote 1: https://github.com/vasu-dev/EG_SIF/

The main contributions of this paper are summarized in the following:

* We propose a novel framework for gaze estimation that can operate reliably even in case of image quality adversities.
* Data segregation into two disjoint sets based on quality of images in the dataset for the first time.

Figure 1: Segregated good and adverse images in MPIIGaze and RT-Gene dataset.

* A novel end to end training framework with image enhancement at the core as an auxiliary task to make the network robust to noise.
* Adaptive binning based mixed regression with intermediate supervision for the first time in gaze estimation.
* With the above proposed methods we are able to obtain the state-of-the-art performance on benchmark datasets that lay the foundation for future eye appearance based gaze estimation models.

## 2 Related work

**Gaze estimation**: Traditional methodologies predominantly focus on the identification and analysis of ocular movement patterns, encompassing phenomena such as fixations, saccades, and smooth pursuits. In contrast, model-based approaches concentrate on the extraction of geometric attributes like pupil centre, contours, and eye corners (Park et al., 2018). With the advancements in deep learning techniques and the abundance of data, appearance-based methods have emerged as a dominant paradigm. They focus on learning a non-linear mapping between the eye image and the associated gaze (Tan et al., 2002). Appearance-based methods can be further divided on the basis of the input provided: eye images or full-face images.

_Features from eye images_: Early deep-learning methods (Zhang et al., 2015) provided a single-eye greyscale low-resolution image in conjunction with head pose information to estimate the gaze from the features. As computational resources burgeoned, deeper networks (Zhang et al., 2017) based on a vanilla VGG-16 or ResNet, were designed to push the boundaries of gaze estimation accuracy. Subsequently, it was found that concatenating features from two eyes yielded better accuracy (Fischer et al., 2018), after which even a four-stream network (Cheng et al., 2018) was built to extract features. (Chen and Shi, 2019) uses dilated convolutions as a means to extract high-level eye features, which efficiently increases the receptive field size of the convolutional filters without reducing spatial resolution.

_Features from face images_: In the realm of gaze estimation, researchers have recognized the potential richness of information contained within facial images, which encompass details such as head pose and offer higher-resolution representations compared to individual eye images and thus recently more attention has moved towards face image-based gaze estimation. (Krafka et al., 2016) was one of the first attempts where the face image along with the left and right eye images were used to estimate gaze. Various facial and eye detectors are used to crop out the region of interest. In (Abdelrahman et al., 2022) they divide the gaze range into discrete bins converting the regression problem to a classification problem alongside running independent networks for different components of 2D gaze (yaw and pitch). Gaze-360 (Kellnhofer et al., 2019) used a pinball loss function to predict error quantiles, indicating confidence in the prediction. However, studies found that just concatenating the features is insufficient and attention-based mechanisms came into the picture. (Cheng et al., 2020) argues that the weights of two eye features are determined by face images due to their specific task, so they assign weights with the guidance of facial features. Recently (Murthy and Biswas, 2021) used an attention branch in parallel to feature extraction to eventually attain weighted eye features and to be used with the face features. More recently transformer-based models (Cheng and Lu, 2021; Yu et al., 2021) have also been applied to test their efficacy in attending to features and generating diverse features. To the best of our knowledge, we are the first ones to utilise deep supervision to aggregate multi-layer features for supervision.

Attempts have been made to utilise additional information like eye landmark features (Wu et al., 2019), and pupil centre (Lee et al., 2020) to improve gaze estimation accuracy. But eye appearance varies much across different people thus making the task of cross-person testing extremely difficult. To solve this either calibrated methods are used or invariant features are obtained via the network. (Park et al., 2018) convert the original eye images into a unified gaze representation, which is a pictorial representation of the eyeball, the iris and the pupil and further regress the gaze from the representation. FAZE (Park et al., 2019) uses an autoencoder to learn the compact latent representation of gaze, head pose and appearance. They introduce a geometric constraint on gaze representations, i.e., the rotation matrix between the two given images transforms the gaze representation of one image to another. Further, they train a highly adaptable gaze estimation network through meta-learning. The network can be converted into a person-specific network once training with target person samples. Nowadays more attention has moved towards gaze redirection (via rotation equivariance (Jin et al., 2023) or NeRFs (Ruzzi et al., 2023)), synthesis and scene-based understanding of gaze.

**Image enhancement:** Data-driven methods are largely categorized into two branches, namely CNN-based and GAN-based methods. The nature of architecture is not of much interest in the context of this work, but the variation in loss functions plays a crucial role. (Zhao et al., 2017) discusses the effectiveness of using a mixed loss which accommodates both structural similarity-based losses and simple error functions (\(l_{1}andl_{2}\)

Figure 2: Our Proposed Network Architecture. The network consists of four main components, the encoder block, image enhancement head, the gaze head and a adabins module. The input to the network is a normalized eye crop of spatial dimension H x W and the output is the 3D Gaze (Yaw and Pitch).

## 3 Gaze estimation using self improving features

In this section, we discuss the details of gaze estimation using self improving features. It consists of 3 parts as discussed in detail in the following subsections:

### Image segregation

The first step is to identify and segregate the entire training set \(I\) into two disjoint subsets \(S\) and \(F\), where \(s_{i}\in S\) are images with lower adverse effects and of high quality and \(f_{i}\in F\) are with poor quality. For this, we did some preliminary analysis on the image properties namely RMSE contrast and blur co-efficient on both the datasets, namely MPIIGaze and RT-Gene. The intuition behind this is that eye images with high constrast indicate noise and high blur leads to loss of essential information needed for gaze estimation. The results of this study are depicted in the Figure 3. As it can be seen in the Figure, there is significant number of images in the long tail of datasets in terms of both RMSE and blurriness.

Figure 3: (a) RMSE contrast distribution for MPIIGaze, highlighting the long tail and corresponding images (b) RMSE contrast distribution for RTGene (c) Blur coefficient distribution for MPIIGaze, highlighting low blur coefficient area along with corresponding images(d) Blur coefficient distribution for RTGene.

Based on the above analysis, the segregation function \(y:I\rightarrow\{s,f\}\) that maps each image to its respective category is defined as:

\[y_{i}=\begin{cases}f,&\text{if RMSE-contrast}(i)>\lambda_{r}\text{ and Blur-coefficient}(i)<\lambda_{c}\\ s,&\text{otherwise}\end{cases}\] (1)

where,

\[Blur-coefficient(i)=Variance(Laplacian(i))\] (2)

and

\[RMSE-contrast(i)=\sqrt{(p_{j}-\mu)^{2}/n}\] (3)

whereas, \(p_{j}\) is the intensity of \(j^{th}\) pixel of image \(i\), \(\mu\) is the mean intensity value of \(i\) and \(n\) is the total number of pixels in \(i\).

It is observed that high RMSE-contrast and low blur-coefficient values resulted in a noisy images. In case of MPIIGaze, we observed that images with RMSE Constrast \(>\) 75 (Fig. 3(_a_)) and Blur coefficient \(<\) 200 (Fig. 3(_c_)) fall into the subset \(F\), hence, \(\lambda_{r}\) is fixed at 75 and \(\lambda_{c}\) at 200. whereas, for RT-Gene \(\lambda_{r}\) at 10 (Fig.3(_b_)) and \(\lambda_{c}\) at 10 (Fig. 3(_d_)). With careful analysis of the baseline models on these segregation, we found that most of them perform poorly on adverse images. As it can be seen in Table 1, in both MPIIGaze and RT-Gene dataset the angular error trained on baseline (ResNet-18) for adverse images is significantly higher than the overall average.

To test that the segregation of good and adverse is accurate, a comparative study was done assessing the entropies of the subsets as shown in Table 2. Inferences were run on 3 pre-trained models with similar architectures and the uncertainty in gaze prediction was used as the basis to calculate the entropy of an image. The analysis revealed a lateral positive shift in the distribution of the entropies of adverse images as compared to the good images.

### Good and adverse pair generation

Once the training set \(I\) is segregated into the two disjoint subsets, \(I=S\cup F\) and \(S\cap F=\Phi\), we use the images from \(S\) and the noise distribution from set \(F\) through histogram matching \(H(f,s)\) to generate adverse samples (similar to image in set \(F\)) from \(S\). We take a random pair of images (\(s_{j}\in S,f_{i}\in F\)) from both the sets and then generate a adverse quality image as a linear combination of the matched histogram \(H(f_{i},s_{j})\), that transfers the pixel

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline
**Dataset** & **Total** & **Good** & **Adverse** \\ \hline MPIIGaze & 5.59 & 5.53 & 5.79 \\ RT-Gene & 8.9 & 8.83 & 9.61 \\ \hline \end{tabular}
\end{table}
Table 1: Angular errors on average, good, and adverse subsets of the datasets using baseline.

distribution, salt-n-pepper noise (\(N_{s}\)) for high contrast generation and Gaussian noise (\(N_{g}\)) for blur effect as given in the equation below:

\[f^{{}^{\prime}}_{ij}=H(f_{i},s_{j})+\alpha N_{s}+\beta N_{g}\] (4) \[\text{where, }s_{j}\in S;\;f_{i}\in F;\;\alpha,\beta\in[0,1]\]

Now we have a pair(\(f^{{}^{\prime}}_{ij},s_{j}\)) of good image(\(s_{j}\)) and a corresponding generated adverse image(\(f^{{}^{\prime}}_{ij}\)) with the same gaze direction but added noise. This acts as a training pair for image denoising/enhancement. Fig. 4 shows the transition of a good image to a adverse image. As it can be noted that the transformed good images are poor in quality.

### Multitask network architecture

It has been observed in numerous studies (Park et al., 2019) that gaze estimation accuracy suffers significantly during cross-person testing, thus the architecture is primarily designed to be robust to such settings. The proposed EG-SIF network draws inspiration from the proven attributes of the U-Net (Ronneberger et al., 2015) architecture, including the encoder's proficiency in capturing multi-scale contextual information, the utility of skip connections for preserving fine-grained details of learned features and the robustness towards noise. We modify it into a multi-task network with 2 task-specific decoder heads focusing on gaze estimation and image enhancement/denoising (refer to Fig. 2). The rationale underlying this multi-task approach lies in the intuition that regressing gaze coordinates from a denoised image is more tractable, thus the reconstruction head forces the encoder to learn self-improving features which effectively serve as proxy features of a denoised image. These information-dense features from the encoder are subsequently fed into the gaze head to predict 3D gaze. Following the ideology of (Abdelrahman et al., 2022) we model the horizontal(yaw) and vertical(pitch) components separately by using independent regressors or fully connected layers. This allows the network to capture the unique characteristics of each component independently. They also suggests that this improves the learning of the

\begin{table}
\begin{tabular}{|c|c|} \hline
**Data split** & **Joint entropy** \\ \hline Good & 6.2 \(\pm\) 2.1 \\ \hline Adverse & 7.3 \(\pm\) 2.4 \\ \hline \end{tabular}
\end{table}
Table 2: Joint entropy calculated on the segregated good and adverse set for MPIIGaze using mean \(\pm\) standard deviation of 3 different runs

Figure 4: Conversion of a source image into a noisy image using the properties of a reference image as well as salt-n-pepper noise.

network as it now has two signals that backpropagate. Additionally, we use the following elements to increase the robustness of the network:

1. **Cross-parameter sharing:** To provide both high-level spatial features and deep features for gaze estimation, parameter sharing across the heads inspired from Wang et al. (2020) was utilised (refer to Algorithm 1 for details). This ensures multi-layer information sharing and also encourages the generalisation of the network as it promotes the network to learn common features and representations that are useful for both tasks. Additionally, parameter sharing augments the network's capacity to learn noise independent features that possess meaningful attributes across distinct scales or layers of the architecture. ```

``` \(y\) : (Features from Gaze Head) \(x\) : (Features from Reconstruction Head) \(z=Concat(x,y)\) \(i_{g}=F_{3}(y,z)\) (Intermediate Gaze Features) \(i_{r}=F_{4}(x,z)\) (Intermediate Reconstruction Features) \(z_{1}=Concat(x,i_{g})\) \(z_{2}=Concat(y,i_{r})\) Where: \(F_{i}\) represents a simple CNN block and in case of intermediate features, help in cross-fusion, and \(z_{i}\) are the latent features which further propagate into the decoders ```

**Algorithm 1** Cross-feature sharing
2. **Adabins:** Initially proposed by (Abdelrahman et al., 2022), gaze bin classification converts the regression task into a simpler classification task which can serve as a coarse estimation of the final gaze. However, using fixed predefined bins might not prove effective for the datasets in consideration since most of the gaze directions are distributed near the origin and have a sparse population near the ends. Thus we propose to use Adaptive bins (Adabins) for gaze bin classification inspired by their recent success in depth estimation (Bhat et al., 2021). Adaptive binning allows for the dynamic adjustment of bin boundaries based on the characteristics of the gaze data. This adaptability ensures that bins are optimized to capture gaze patterns effectively for a given dataset or user, enhancing the accuracy of gaze estimation. Additionally, Adaptive binning can create bins that are finer in regions with high gaze density and coarser in regions with sparse gaze data (refer to Fig. 5). Once the bins are obtained, the bin centres (b(c)) are calculated and scaled accordingly. \[b(c_{i})=g_{\min}+(g_{\max}-g_{\min})\left(b_{i}+\frac{b_{i+1}-b_{i}}{2}\right)\] (5) Where: \(g_{max}\) and \(g_{min}\) are the minimum and maximum gaze values corresponding to the dataset, and \(b_{i}\) is the predicted bin width A parallel, fully connected layer (x) is then utilized for generating predictions associated with each bin. These predictions are subjected to a softmax activation function, resulting in the assignment of probability scores to each individual bin. A coarse gaze is calculated as the expectation of the bin centres. The final gaze is calculated by adding a multi-layered perceptron (MLP) on top of the predictions regressing a single value.

\[G_{coarse}=\sum_{i=1}^{N}b(c_{i})\cdot\text{Softmax}(x_{i})\] (6)

#### 3.3.1 Loss functions

Many Convolutional Neural Network (CNN)-based models for gaze estimation commonly output predictions in the form of 3D gaze direction angles, typically represented in spherical coordinates as yaw and pitch. They frequently employ the mean-squared error (\(l_{2}\) loss) for penalizing their networks. We propose the following loss function for training the EG-SIF network:

1. **Reconstruction loss:** Based on (Zhao et al., 2017) we use the suggested mixed loss. The mixed loss allows one to strike a balance between pixel-level accuracy (sharpness) and perceptual quality. \(l_{2}\) loss (MSE) alone tends to produce overly smooth images because it primarily penalizes pixel-wise differences. The inclusion of multi-scale SSIM(Structural Similarity) helps mitigate this issue by encouraging the preservation of structural and textural details. The overall reconstruction loss is: \[L_{\text{R}}=\lambda L_{\text{MSE}}+(1-\lambda)L_{\text{SSIM}}\] (7) Where, \(\lambda\) is a weight factor

Figure 5: Example showing comparison b/w uniform and adaptive bins, as can be seen from the figure using adaptive bins can lead to better results as the bins are adapted to each input image individually.

2. **Gaze-estimation loss:** We utilize both regression and classification losses along with a regularization loss on the gaze heads. * _Coarse and direct gaze-estimation loss:_ The standard MSE loss is used to penalise both the directly regressed gaze and the calculated coarse gaze. \[MSE(y,p)=\frac{1}{N}\sum_{i=1}^{N}(y_{i}-p_{i})^{2}\] (8) Whereas \(p_{i}\) is the regressed or calculated gaze and \(y_{i}\) is the ground truth value * _Gaze classification loss:_ To penalise the predicted bin class, the standard cross-entropy loss is used. \[H(y,p)=-\sum_{i}y_{i}\log p_{i}\] (9) Whereas, \(p_{i}\) is the predicted class vector and \(y_{i}\) is the ground truth one hot class label * _Bin-width regularisation:_ This novel loss acts as a form of regularization, discouraging extreme values of bin widths. This can help prevent over-fitting and improve the generalization performance of a model. \[\text{LimitLoss}(LL)=\max\left(0,\max(b(c_{i}))-\text{weight}\cdot\text{ threshold}\right)\] (10) Whereas, the threshold is set as the validation score of the previous epoch The overall gaze-estimation loss can be represented as: \[L_{G}=\alpha\cdot(LL+H(y,p))+\beta\cdot MSE_{coarse}(y,p)+MSE_{direct}(y,p)\] (11) Where: \(alpha\) and \(beta\) are hyper parameters for loss-weighting Thus the final loss for the network becomes: \[L_{EG-SIF}=\rho L_{G}+(1-\rho)L_{\text{R}}\] (12) Where: \(\rho\) is again a hyperparameter

## 4 Experiments and results

### Datasets

With the development of appearance-based gaze estimation methods, many large-scale datasets with variations in gaze direction, head pose and appearance have been proposed. In order to assess the network, we train and evaluate our model on two popular datasets: MPIIGaze (Zhang et al., 2017) and RT-Gene dataset(Fischer et al., 2018).

**MPIIGaze** is the most popular dataset for gaze-estimation methods. It contains a total of 213,659 images (for each eye) obtained from 15 different subjects, collected over several months thus providing variations in illumination. The dataset contains normalisedeye images for corresponding face images, which is of our main interest. It also consists of a standard evaluation set which comprises 3,000 images (1,500 left-eye and 1,500 right-eye images) for each subject. The standard testing practice for the MPIIGaze dataset is the leave-one-out setting to check the model accuracy in a cross-person fashion.

**RT-Gene** dataset was collected in a controlled laboratory environment, ensuring high-quality data. The dataset includes eye-tracking data obtained from participants using head-mounted eye-tracking glasses resulting in a much higher variation in the gaze angle distribution. It contains 122,531 inpainted and original images (229,116 distinct eye crops) acquired from 15 distinct subjects. Since the inpainted images are heavily noised images as reported by (Murthy and Biswas, 2021), we use original images for training and testing. The standard testing practice for RT-Gene is using a 3-fold cross-validation.

### Data pre-processing

We use normalised eye images of 36x60 resolution for both datasets and do not make use of any other additional information (e.g. full-face image, head pose vector) to obtain the results. The ground truth class labels for the images were calculated during training. As a result, the dataset had both class labels and continuous gaze labels making them suitable for our proposed overall loss function. A total of 30,966 adverse images were obtained in the MPIIGaze dataset leaving 396,352 training and validation samples. In the case of the RT-Gene dataset, 18,971 adverse samples and 103,650 training and validation samples were used respectively. The generated good-adverse pairs are such that the adverse samples of a certain subject inherit noise properties from all the available subjects making the

Figure 6: Results of using different hyperparamters(\(\alpha\), \(\beta\)) for MPIIGaze.

dataset more exhaustive. Another modification was to horizontally invert right eye images to align geometrically similar features with left eye, thus making it easier to train a model. Horizontally inverting left eye images without inverting right eye images is strongly expected to yield similar results.

### Training and results

We performed leave-one-out cross-validation on the MPIIGaze dataset using the proposed models. The entire dataset apart from the held-out set was used for training and the held-out set was used for validation. All the experiments were done using a yytorch framework utilising adam optimiser. With a simple grid search initial learning rate was fixed at 0.001 and a multi-step decay with a decay constant at 0.5. Dropout was used for fully connected layers with a dropout probability of 0.25. We train the network for 25 epochs each using a batch size of 512. Loss weights(\(\alpha\), \(\beta\)) were changed and the performance was monitored and compared with the state-of-the-art. Downsampling and cross-parameter sharing were done twice in the network. For all the conducted experiments the hyperparameters were fixed at \(\rho\) = 0.5, \(\lambda\) = 0.5.

We utilize gaze angular error (\(\theta\)) as the evaluation metric following most gaze estimation methods. Assuming the ground-truth gaze direction is \(\mathbf{g}\in\mathbb{R}^{3}\) and the predicted gaze vector is \(\mathbf{\hat{g}}\in\mathbb{R}^{3}\), the gaze angular error (\(\theta\)) can be computed as:

\[\theta=\arccos\left(\frac{\mathbf{g}\cdot\mathbf{\hat{g}}}{\|\mathbf{g}\|\cdot \|\mathbf{\hat{g}}\|}\right)\] (13)

The initial experiments were undertaken to evaluate the comparative performance of an encoder-gaze regressor network in contrast to an encoder equipped with two distinct decoders--one for gaze estimation and the other for image enhancement/denoising (Table 3). The enhancement based experiment was additionally constrained structural similarity loss along with the MSE. Notably, the introduction of cross-parameter sharing mechanisms yielded a significant enhancement in the network's performance, indicating the strong correlation between the tasks.

In the context of cross-person evaluation, using proper augmentations while training plays a pivotal role in enhancing the overall robustness of neural networks. The base augmentations used were brightness (0.5,1.5), contrast (0.5, 1.5), saturation (0.5, 1.5), hue (-0.1,

\begin{table}
\begin{tabular}{|c|c|} \hline
**Network architecture** & **Angular error** \\ \hline Baseline (Resnet-18) & 5.6 \(\pm\) 0.9 \\ \hline Reconstruction & 5.31 \(\pm\) 0.52 \\ \hline Reconstruction + cross-param sharing & 5.06 \(\pm\) 0.77 \\ \hline \end{tabular}
\end{table}
Table 3: Angular errors on model trained using the generated adverse dataset demonstrating the effectiveness of the mixed-loss and cross-parameter sharing on MPIIGaze. The results reported are mean \(\pm\) standard deviation in evaluating cross persons.

0.1).These adjustments were applied in a randomized manner with a combined probability of 50%, imparting stochastic variability to the training data. Additionally guassian blur with a mean of 5 and std (1,3) with 50% probability was applied. Later we incorporated a customized patch augmentation technique. It helped the model focus on the pupil rather than the surroundings.The proposed patch augmentation applies white patches on the edge of an image with variable size at a variable position. This resulted in a slight increase in the accuracy of the model and a decrease in the variance. Another finding while visual inspection was that adding salt and pepper noise to the training images increases robustness towards noise in the image. Table 4 shows the respective improvements by using a combination of these augmentations.

Table 5 shows the effectiveness of adaptive bins(Adabins) over fixed bins. As proposed by Abdelrahman et al. (2022), we use 28 bins for the MPIIGaze and 42 bins for RT-Gene.

Table 6 shows the comparison of our method with other methods, as it can be seen the proposed method EG-SIF produces significantly lower error on MPII dataset and slightly better performance on RT-Gene dataset. This is in comparison to the current state-of-the-art methods. Fig. 6 shows a comparative analysis of the effect of changing hyperparameters and subject-wise performance. From this we can observe that the hyperparameters are very sensitive to change in appearance and thus affect the average performance.

## 5 Conclusion

In this paper, we introduce, for the first time a method for eye gaze estimation with self-improving features (EG-SIF). As we observe that eye crop based dataset for gaze estimation

\begin{table}
\begin{tabular}{|c|c|} \hline
**Augmentation type** & **Angular error** \\ \hline Normal & 4.9 \(\pm\) 0.83 \\ \hline Patch & 4.82 \(\pm\) 0.64 \\ \hline Patch + SP noise & 4.79 \(\pm\) 0.48 \\ \hline \end{tabular}
\end{table}
Table 4: Angular errors on model trained using the generated adverse dataset demonstrating the effectiveness of various augmentations on MPIIGaze. The results reported are mean \(\pm\) standard deviation in evaluating cross persons.

\begin{table}
\begin{tabular}{|c|c|} \hline
**Type of bins** & **Angular error** \\ \hline Fixed & 4.72 \(\pm\) 0.52 \\ \hline Adaptive (adabins) & 4.53 \(\pm\) 0.46 \\ \hline \end{tabular}
\end{table}
Table 5: Angular errors on model trained using the generated adverse dataset demonstrating the effectiveness of Adabins on MPIIGaze. The results reported are mean \(\pm\) standard deviation in evaluating cross persons.

contains both good and adverse images due to various inherent reasons and the current state-of-the-art methods do not explicitly focus on the image quality, hence compromising the performance. EG-SIF segregates images based on their quality. Once images are segregated, a transformation is defined to obtain adverse set of images from good images. This pair is used in the multi-task model where the task is to enhance/denoise the generated adverse image along with gaze estimation. Apart from this, methods including cross-parameter sharing and adabins are proposed to increase the efficiency. On evaluation of our methods, it is observed that it outperforms current state-of-the-art methods by a good margin. This concludes that adverse quality images in the dataset require separate treatment which can not only enhance the performance in the dataset but also during inference in real-time condition during those adversities.

## References

* A. A. Abdelrahman, T. Hempel, A. Khalifa, and A. Al-Hamadi (2022)L2cs-net: Fine-grained gaze estimation in unconstrained environments. External Links: 2202.0220 Cited by: SS1.
* D. Beymer and D. M. Russell (2005)Webgazeanalyzer: a system for capturing and analyzing web reading behavior using eye gaze. In CHI '05 Extended Abstracts on Human Factors in Computing Systems, CHI EA '05, New York, NY, USA, pp. 1913-1916. External Links: ISBN 1595930027, Link, Document Cited by: SS1.
* S. Farooq Bhat, I. Alhashim, and P. Wonka (2021)AdaBins: depth estimation using adaptive bins. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vol., pp.. External Links: Document Cited by: SS1.
* Z. Chen and B. E. Shi (2019)Appearance-based gaze estimation using dilated-convolutions. External Links: 1905.0002 Cited by: SS1.
* Y. Cheng and F. Lu (2021)Gaze estimation using transformer. External Links: 2102.02101 Cited by: SS1.

\begin{table}
\begin{tabular}{|c|c|c|} \hline \multirow{2}{*}{**Model**} & \multicolumn{2}{c|}{**Angular error**} \\ \cline{2-3}  & **MPIIGaze** & **RTGene** \\ \hline ARENet & 5.02\({}^{*}\) & - \\ \hline RTGene & 4.8\({}^{*}\) & 8.6\({}^{*}\) \\ \hline AGENet & 4.64\({}^{*}\) & 7.44\({}^{*}\) \\ \hline
**Ours(EG-SIF)** & **4.53\({}^{*}\)** & **7.41\({}^{*}\)** \\ \hline \end{tabular}
\end{table}
Table 6: Performance comparison with state-of-the-art gaze estimation methods.

* Cheng et al. (2018) Yihua Cheng, Feng Lu, and Xucong Zhang. Appearance-based gaze estimation via evaluation-guided asymmetric regression. In _Proceedings of the European Conference on Computer Vision (ECCV)_, September 2018.
* Cheng et al. (2020) Yihua Cheng, Shiyao Huang, Fei Wang, Chen Qian, and Feng Lu. A coarse-to-fine adaptive network for appearance-based gaze estimation, 2020.
* ECCV 2018_, pages 339-357, Cham, 2018. Springer International Publishing. ISBN 978-3-030-01249-6.
* Jin et al. (2023) Shiwei Jin, Zhen Wang, Lei Wang, Ning Bi, and Truong Nguyen. Redirtrans: Latent-to-latent translation for gaze and head redirection. _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5547-5556, 2023. URL https://api.semanticscholar.org/CorpusID:258822908.
* Kellnhofer et al. (2019) Petr Kellnhofer, Adria Recasens, Simon Stent, Wojciech Matusik, and Antonio Torralba. Gaze360: Physically unconstrained gaze estimation in the wild, 2019.
* Krafka et al. (2016) Kyle Krafka, Aditya Khosla, Petr Kellnhofer, Harini Kannan, Suchendra Bhandarkar, Wojciech Matusik, and Antonio Torralba. Eye tracking for everyone, 2016.
* ECCV 2020_, pages 36-52, Cham, 2020. Springer International Publishing. ISBN 978-3-030-58529-7.
* Li et al. (2019) Peng Li, Xuebin Hou, Xingguang Duan, Hiuman Yip, Guoli Song, and Yunhui Liu. Appearance-based gaze estimator for natural interaction control of surgical robots. _IEEE Access_, 7:25095-25110, 2019. doi: 10.1109/ACCESS.2019.2900424.
* Murthy and Biswas (2021) LRD Murthy and Pradipta Biswas. Appearance-based gaze estimation using attention and difference mechanism. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops_, pages 3143-3152, June 2021.
* Park et al. (2018) Seonwook Park, Xucong Zhang, Andreas Bulling, and Otmar Hilliges. Learning to find eye region landmarks for remote gaze estimation in unconstrained settings. In _Proceedings of the 2018 ACM Symposium on Eye Tracking Research &amp Applications_. ACM, jun 2018. doi: 10.1145/3204493.3204545. URL https://doi.org/10.1145%2F3204493.3204545.
* Park et al. (2019) Seonwook Park, Shalini De Mello, Pavlo Molchanov, Umar Iqbal, Otmar Hilliges, and Jan Kautz. Few-shot adaptive gaze estimation, 2019.
* Patney et al. (2016) Anjul Patney, Marco Salvi, Joohwan Kim, Anton Kaplanyan, Chris Wyman, Nir Benty, David Luebke, and Aaron Lefohn. Towards foveated rendering for gaze-tracked virtual reality. _ACM Trans. Graph._, 35(6), dec 2016. ISSN 0730-0301. doi: 10.1145/2980179.2980246. URL https://doi.org/10.1145/2980179.2980246.

. Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation, 2015.
* Ruzzi et al. (2023) Alessandro Ruzzi, Xiangwei Shi, Xi Wang, Gengyan Li, Shalini De Mello, Hyung Jin Chang, Xucong Zhang, and Otmar Hilliges. Gazenerf: 3d-aware gaze redirection with neural radiance fields, 2023.
* Tan et al. (2002) Kar-Han Tan, D.J. Kriegman, and N. Ahuja. Appearance-based eye gaze estimation. In _Sixth IEEE Workshop on Applications of Computer Vision, 2002. (WACV 2002). Proceedings._, pages 191-195, 2002. doi: 10.1109/ACV.2002.1182180.
* Wang et al. (2015) Haofei Wang, Xujiong Dong, Zhaokang Chen, and Bertram E. Shi. Hybrid gaze/eeg brain computer interface for robot arm control on a pick and place task. In _2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)_, pages 1476-1479, 2015. doi: 10.1109/EMBC.2015.7318649.
* Wang et al. (2020) Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. _IEEE transactions on pattern analysis and machine intelligence_, 43(10):3349-3364, 2020.
* Wu et al. (2019) Zhengyang Wu, Srivignesh Rajendran, Tarrence van As, Joelle Zimmermann, Vijay Badrinarayanan, and Andrew Rabinovich. Eyenet: A multi-task network for off-axis eye gaze estimation and user understanding, 2019.
* Yu et al. (2021) Qihang Yu, Yingda Xia, Yutong Bai, Yongyi Lu, Alan Yuille, and Wei Shen. Glance-and-gaze vision transformer, 2021.
* Zhang et al. (2015) Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. Appearance-based gaze estimation in the wild. In _2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 4511-4520, 2015. doi: 10.1109/CVPR.2015.7299081.
* Zhang et al. (2017) Xucong Zhang, Yusuke Sugano, Mario Fritz, and Andreas Bulling. Mpiigaze: Real-world dataset and deep appearance-based gaze estimation, 2017.
* Zhang et al. (2019) Xucong Zhang, Yusuke Sugano, and Andreas Bulling. Evaluation of appearance-based methods and implications for gaze-based applications. In _Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems_. ACM, may 2019. doi: 10.1145/3290605.3300646. URL https://doi.org/10.1145%2F3290605.3300646.
* Zhao et al. (2017) Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for image restoration with neural networks. _IEEE Transactions on Computational Imaging_, 3(1):47-57, 2017. doi: 10.1109/TCI.2016.2644865.