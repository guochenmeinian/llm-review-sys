# State-Action Similarity-Based Representations for Off-Policy Evaluation

 Brahma S. Pavse and Josiah P. Hanna

University of Wisconsin - Madison

pavse@wisc.edu, jphanna@cs.wisc.edu

###### Abstract

In reinforcement learning, off-policy evaluation (ope) is the problem of estimating the expected return of an evaluation policy given a fixed dataset that was collected by running one or more different policies. One of the more empirically successful algorithms for ope has been the fitted q-evaluation (fge) algorithm that uses temporal difference updates to learn an action-value function, which is then used to estimate the expected return of the evaluation policy. Typically, the original fixed dataset is fed directly into fge to learn the action-value function of the evaluation policy. Instead, in this paper, we seek to enhance the data-efficiency of fge by first transforming the fixed dataset using a learned encoder, and then feeding the transformed dataset into fge. To learn such an encoder, we introduce an ope-tailored state-action behavioral similarity metric, and use this metric and the fixed dataset to learn an encoder that models this metric. Theoretically, we show that this metric allows us to bound the error in the resulting ope estimate. Empirically, we show that other state-action similarity metrics lead to representations that cannot represent the action-value function of the evaluation policy, and that our state-action representation method boosts the data-efficiency of fge and lowers ope error relative to other ope-based representation learning methods on challenging ope tasks. We also empirically show that the learned representations significantly mitigate divergence of fge under varying distribution shifts. Our code is available here: https://github.com/Badger-RL/ROPE.

## 1 Introduction

In real life applications of reinforcement learning, practitioners often wish to assess the performance of a learned policy before allowing it to make decisions with real life consequences (Theocharous et al., 2015). That is, they want to be able to evaluate the performance of a policy without actually deploying it. One approach of accomplishing this goal is to apply methods for off-policy evaluation (ope). ope methods evaluate the performance of a given evaluation policy using a fixed offline dataset previously collected by one or more policies that may be different from the evaluation policy.

One of the core challenges in ope is that the offline datasets may have limited size. In this situation, it is often critical that ope algorithms are data-efficient. That is, they are able produce accurate estimates of the evaluation policy value even when only small amounts of data are available. In this paper, we seek to enhance the data-efficiency of ope methods through representation learning. While prior works have studied representation learning for ope, they have mostly considered representations that induce guaranteed convergent learning without considering whether data-efficiency increases (Chang et al., 2022; Wang et al., 2021). For example, Chang et al. (2022) introduce a method for learning Bellman complete representations for fge but empirically find that having such a learned representation provides little benefit compared to fge without the learned representation. Thus, in this work we ask the question, "can explicit representation learning lead to more data-efficient ope?"To answer this question, we take inspiration from recent advances in learning state similarity metrics for control (Castro et al., 2022; Zhang et al., 2021). These works define behavioral similarity metrics that measure the distance between two states. They then show that state representations can be learned such that states that are close under the metric will also have similar representations. In our work, we introduce a new ope-tailored behavioral similarity metric called **R**epresentations for **O**ff-**P**olicy **E**valuation (rope) and show that learning rope representations can lead to more accurate ope.

Specifically, rope first uses the fixed offline dataset to learn a state-action encoder based on this ope-specific state-action similarity metric, and then applies this encoder to the same dataset to produce a new representation for all state-action pairs. The transformed data is then fed into the fitted q-evaluation (fge) algorithm (Le et al., 2019) to produce an ope estimate. We theoretically show that the error between the policy value estimate with rope + rope and the true evaluation policy value is upper-bounded in terms of how rope aggregates state-action pairs. We empirically show that rope improves the data-efficiency of fge and leads to lower ope error compared to other ope-based representation learning baselines. Additionally, we empirically show that rope representations mitigate divergence of fge under extreme distribution. To the best of our knowledge, our work is the first to propose an ope-specific state-action similarity metric that increases the data-efficiency of ope.

## 2 Background

In this section, we formalize our problem setting and discuss prior work.

### Notation and Problem Setup

We consider an infinite-horizon Markov decision process (mdp) (Puterman, 2014), \(\mathcal{M}=\langle\mathcal{S},\mathcal{A},\mathcal{R},P,\gamma,d_{0}\rangle\), where \(\mathcal{S}\) is the state-space, \(\mathcal{A}\) is the action-space, \(\mathcal{R}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta([0,\infty))\) is the reward function, \(P:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\) is the transition dynamics function, \(\gamma\in[0,1)\) is the discount factor, and \(d_{0}\in\Delta(\mathcal{S})\) is the initial state distribution, where \(\Delta(X)\) is the set of all probability distributions over a set \(X\). We refer to the joint state-action space as \(\mathcal{X}:=\mathcal{S}\times\mathcal{A}\). The agent acting, according to policy \(\pi\), in the mdp generates a trajectory: \(S_{0},A_{0},R_{0},S_{1},A_{1},R_{1},...\), where \(S_{0}\sim d_{0}\), \(A_{t}\sim\pi(\cdot|S_{t})\), \(R_{t}\sim\mathcal{R}(S_{t},A_{t})\), and \(S_{t+1}\sim P(\cdot|S_{t},A_{t})\) for \(t\geq 0\). We define \(r(s,a):=\mathds{E}[\mathcal{R}(s,a)]\).

We define the performance of policy \(\pi\) to be its expected discounted return, \(\rho(\pi)\coloneqq\mathds{E}[\sum_{t=0}^{\infty}\gamma^{t}R_{t}]\). We then have the action-value function of a policy for a given state-action pair, \(q^{\pi}(s,a)=r(s,a)+\gamma\,\mathds{E}_{S^{\prime}\sim P(s,a),A^{\prime}\sim \pi}[q^{\pi}(S^{\prime},A^{\prime})]\), which gives the expected discounted return when starting in state \(s\) and then taking action \(a\). Then \(\rho(\pi)\) can also be expressed as \(\rho(\pi)=\mathds{E}_{S_{0}\sim d_{0},A_{0}\sim\pi}[q^{\pi}(S_{0},A_{0})]\).

It is often more convenient to work with vectors instead of atomic states and actions. We use \(\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\) to denote a representation function that maps state-action pairs to vectors with some dimensionality \(d\).

### Off-Policy Evaluation (OPE)

In off-policy evaluation, we are given a fixed dataset of \(m\) transition tuples \(\mathcal{D}:=\{(s_{i},a_{i},s^{\prime}_{i},r_{i})\}_{i=1}^{m}\) and an evaluation policy, \(\pi_{\mathrm{e}}\). Our goal is to use \(\mathcal{D}\) to estimate \(\rho(\pi_{\mathrm{e}})\). Crucially, \(\mathcal{D}\) may have been generated by a set of _behavior_ policies that are different from \(\pi_{\mathrm{e}}\), which means that simply averaging the discounted returns in \(\mathcal{D}\) will produce an inconsistent estimate of \(\rho(\pi_{\mathrm{e}})\). We do _not_ assume that these behavior policies are known to us, however, we do make the standard assumption that \(\forall s\in\mathcal{S},\forall a\in\mathcal{A}\) if \(\pi_{\mathrm{e}}(a|s)>0\) then the state-action pair \((s,a)\) has non-zero probability of appearing in \(\mathcal{D}\).

As done by Fu et al. (2021), we measure the accuracy of an ope estimator with the _mean absolute error_ (mae) to be robust to outliers. Let \(\hat{\rho}(\pi_{\mathrm{e}},\mathcal{D})\) be the estimate returned by an ope method using \(\mathcal{D}\). The mae of this estimate is given as:

\[\textsc{mae}[\hat{\rho}]\coloneqq\mathds{E}_{\mathcal{D}}[|\hat{\rho}(\pi_{ \mathrm{e}},\mathcal{D})-\rho(\pi_{\mathrm{e}})|].\]

While in practice \(\rho(\pi_{\mathrm{e}})\) is unknown, it is standard for the sake of empirical analysis (Voloshin et al., 2021; Fu et al., 2021) to estimate it by executing rollouts of \(\pi_{\mathrm{e}}\).

### Fitted Q-Evaluation

One of the more successful ope methods has been fitted q-evaluation (fqe) which uses batch temporal difference learning (Sutton, 1988) to estimate \(\rho(\pi_{\mathrm{e}})\)(Le et al., 2019). fqe involves two conceptual steps: 1) repeat temporal difference policy evaluation updates to estimate \(q^{\pi_{\mathrm{e}}}(s,a)\) and then 2) estimate \(\rho(\pi_{\mathrm{e}})\) as the mean action-value at the initial state distribution. Formally, let the action-value function be parameterized by \(\xi\) i.e. \(q_{\xi}\), then the following loss function is minimized to estimate \(q^{\pi_{\mathrm{e}}}\):

\[\mathcal{L}_{\text{FQE}}(\xi):=\operatorname{E}_{(s,a,s^{\prime},r)\sim \mathcal{D}}\left[\left(r(s,a)+\gamma\operatorname{E}_{a^{\prime}\sim\pi_{ \mathrm{e}}(\cdot|s^{\prime})}[q_{\bar{\xi}}(s^{\prime},a^{\prime})]-q_{\xi}( s,a)\right)^{2}\right]\]

where \(\bar{\xi}\) is a separate copy of the parameters \(\xi\) and acts as the target function approximator (Mnih et al., 2015) that is updated to \(\xi\) at a certain frequency. The learned \(q_{\xi^{*}}\) is then used to estimate the policy value: \(\hat{\rho}(\pi_{\mathrm{e}})\coloneqq\operatorname{E}_{s_{0}\sim d_{0},a_{0} \sim\pi_{\mathrm{e}}}[q_{\xi^{*}}(s_{0},a_{0})]\). While conceptually fqe can be implemented with many classes of function approximator to represent the \(q_{\xi}\), in practice, deep neural networks are often the function approximator of choice. When using deep neural networks, fqe can be considered a policy evaluation variant of neural fitted q-iteration (Riedmiller, 2005).

### Related Work

In this section, we discuss the most relevant prior literature on off-policy evaluation and representation learning. Methods for ope are generally categorized as importance-sampling based (Precup et al., Thomas et al., 2021; Liu et al., 2018; Yang et al., 2020), model-based (Yang and Nachum, 2021; Zhang et al., 2021; Hanna et al., 2017), value-function-based (Le et al., 2019; Uehara et al., 2020), or hybrid (Jiang and Li, 2016; Thomas and Brunskill, 2016; Farajtabar et al., 2018). Our work focuses on fqe, which is a representative value-function-based method, since it has been shown to have strong empirical performance (Fu et al., 2021; Chang et al., 2022). We refer the reader to Levine et al. (2020) for an in-depth survey of ope methods.

Representation Learning for Off-policy Evaluation and Offline RLA handful of works have considered the interplay of representation learning with ope methods and offline RL. Yang and Nachum (2021) benchmark a number of existing representation learning methods for offline RL and show that pre-training representation can be beneficial for offline RL. They also consider representation learning based on behavioral similarity and find that such representations do not enable successful offline RL. However, their study is focused on evaluating existing algorithms and on control. Pavse and Hanna (2023) introduced state abstraction (Li et al., 2006) as an approach to lower the variance of ope estimates in importance-sampling based methods. However, their work made the strict assumption of granting access to a bisimulation abstraction in theory and relied on a hand-specified abstraction in practice. Only recently have works started to consider learning representations specifically for ope. Chang et al. (2022) introduced a method for learning Bellman complete representations that enabled convergent approximation of \(q^{\pi_{\mathrm{e}}}\) with linear function approximation. Wang et al. (2021) show that using the output of the penultimate layer of \(\pi_{\mathrm{e}}\)'s action-value function provides realizability of \(q_{\pi_{\mathrm{e}}}\), but is insufficient for accurate policy evaluation under extreme distribution shift. Our work explicitly focuses on boosting the data-efficiency of ope methods and lowers the error of ope estimates compared to Chang et al. (2022) and Wang et al. (2021).

Representation Learning via Behavioral SimilarityThe representation learning method we introduce builds upon prior work in learning representations in which similar states share similar representations. Much of this prior work is based on the notion of a bisimulation abstraction in which two states with identical reward functions and that lead to identical groups of next states should be classified as similar (Ferns et al., 2004, 2011; Ferns and Precup, 2014; Castro, 2019). The bisimulation metric itself is difficult to learn both computationally and statistically and so recent work has introduced various approximations (Castro et al., 2022; Castro, 2019; Zhang et al., 2021; Gelada et al., 2019). To the best of our knowledge, all of this work has considered the _online, control_ setting and has only focused on state representation learning. In contrast, we introduce a method for learning _state-action_ representations for ope with a fixed dataset. One exception is the work of Dadashi et al. (2021), which proposes to learn state-action representations for offline policy _improvement_. However, as we will show in Section 4, the distance metric that they base their representations on is inappropriate in the ope context.

## 3 ROPE: State-Action Behavioral Similarity Metric for Off-Policy Evaluation

In this section, we introduce our primary algorithm: **R**epresentations for **OPE** (rope), a representation learning method based on state-action behavioral similarity that is tailored to the off-policy evaluation problem. That is, using a fixed off-policy dataset \(\mathcal{D}\), rope learns similar representations for state-action pairs that are similar in terms of the action-value function of \(\pi_{\mathrm{e}}\).

Prior works on representation learning based on state behavioral similarity define a metric that relates the similarity of two states and then map similar states to similar representations (Castro et al., 2022; Zhang et al., 2021a). We follow the same high-level approach except we focus instead on learning state-action representations for ope. One advantage of learning state-action representations over state representations is that we can learn a metric specifically for \(\pi_{\mathrm{e}}\) by directly sampling actions from \(\pi_{\mathrm{e}}\) instead of using importance sampling, which can be difficult when the multiple behavior policies are unknown. Moreover, estimating the importance sampling ratio from data is known to be challenging (Hanna et al., 2021; Yang et al., 2020).

Our new notion of similarity between state-action pairs is given by the recursively-defined rope distance, \(d_{\pi_{\mathrm{e}}}(s_{1},a_{1};s_{2},a_{2}):=|r(s_{1},a_{1})-r(s_{2},a_{2})|+ \gamma\,\mathrm{E}_{s^{\prime}_{1},s^{\prime}_{2}\sim P,a^{\prime}_{1},a^{ \prime}_{2}\sim\pi_{\mathrm{e}}}[d_{\pi_{\mathrm{e}}}(s^{\prime}_{1},a^{ \prime}_{1};s^{\prime}_{2},a^{\prime}_{2})]\). Intuitively, \(d_{\pi_{\mathrm{e}}}\) measures how much two state-action pairs, \((s_{1},a_{1})\) and \((s_{2},a_{2})\), differ in terms of short-term reward and discounted expected distance between next state-action pairs encountered by \(\pi_{\mathrm{e}}\). In order to compute \(d_{\pi_{\mathrm{e}}}\), we define the rope operator:

**Definition 1** (rope operator).: _Given an evaluation policy \(\pi_{\mathrm{e}}\), the rope operator \(\mathcal{F}^{\pi_{\mathrm{e}}}:\mathbb{R}^{X\times\mathcal{X}}\to\mathbb{R}^{ X\times\mathcal{X}}\) is given by:_

\[\mathcal{F}^{\pi_{\mathrm{e}}}(d)(s_{1},a_{1};s_{2},a_{2}):=\underbrace{|r(s_{ 1},a_{1})-r(s_{2},a_{2})|}_{\text{short-term distance}}+\gamma\underbrace{ \mathrm{E}_{s^{\prime}_{1},s^{\prime}_{2}\sim P,a^{\prime}_{1},a^{\prime}_{2} \sim\pi_{\mathrm{e}}}[d(s^{\prime}_{1},a^{\prime}_{1};s^{\prime}_{2},a^{ \prime}_{2})]}_{\text{long-term distance}}\] (1)

_where \(d:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\), \(s^{\prime}_{1}\sim P(s^{\prime}_{1}|s_{1},a_{1}),s^{\prime}_{2}\sim P(s^{ \prime}_{2}|s_{2},a_{2}),a^{\prime}_{1}\sim\pi_{\mathrm{e}}(\cdot|s^{\prime}_ {1}),a^{\prime}_{2}\sim\pi_{\mathrm{e}}(\cdot|s^{\prime}_{2})\)_

Given the operator, \(\mathcal{F}^{\pi_{\mathrm{e}}}\), we show that the operator is a contraction mapping, computes the rope distance, \(d_{\pi_{\mathrm{e}}}\), and that \(d_{\pi_{\mathrm{e}}}\) is a _diffuse metric_. For the background on metrics and full proofs, refer to the Appendix A and B.

**Proposition 1**.: _The operator \(\mathcal{F}^{\pi_{\mathrm{e}}}\) is a contraction mapping on \(\mathbb{R}^{\mathcal{X}\times\mathcal{X}}\) with respect to the \(L^{\infty}\) norm._

**Proposition 2**.: _The operator \(\mathcal{F}^{\pi_{\mathrm{e}}}\) has a unique fixed point \(d_{\pi_{\mathrm{e}}}\in\mathbb{R}^{X\times\mathcal{X}}\). Let \(d_{0}\in\mathbb{R}^{X\times\mathcal{X}}\), then \(\lim_{t\to\infty}\mathcal{F}^{\pi_{\mathrm{e}}}_{t}(d_{0})=d_{\pi_{\mathrm{e}}}\)._

Propositions 1 and 2 ensure that repeatedly applying the operator on some function \(d:\mathcal{X}\times\mathcal{X}\to\mathbb{R}\) will make \(d\) converge to our desired distance metric, \(d_{\pi_{\mathrm{e}}}\). An important aspect of \(d_{\pi_{\mathrm{e}}}\) is that it is a diffuse metric:

**Proposition 3**.: \(d_{\pi_{\mathrm{e}}}\) _is a diffuse metric._

where a diffuse metric is the same as a psuedo metric (see Definition 3 in Appendix A) except that self-distances can be non-zero i.e. it may be true that \(d_{\pi_{\mathrm{e}}}(s,a;s,a)>0\). This fact arises due to the stochasticity in the transition dynamics and action sampling from \(\pi_{\mathrm{e}}\). If we assume a deterministic transition function and a deterministic \(\pi_{\mathrm{e}}\), \(d_{\pi_{\mathrm{e}}}\) will reduce to a pseudo metric, which gives zero self-distance. In practice, we use a sample approximation of the rope operator to estimate \(d_{\pi_{\mathrm{e}}}\).

Given that \(d_{\pi_{\mathrm{e}}}\) is well-defined, we have the following theorem that shows why it is useful in the ope context:

**Theorem 1**.: _For any evaluation policy \(\pi_{\mathrm{e}}\) and \((s_{1},a_{1}),(s_{2},a_{2})\in\mathcal{X}\), we have that \(|q^{\pi_{\mathrm{e}}}(s_{1},a_{1})-q^{\pi_{\mathrm{e}}}(s_{2},a_{2})|\leq d_{ \pi_{\mathrm{e}}}(s_{1},a_{1},;s_{2},a_{2})\)._

Given that our goal is learn representations based on \(d_{\pi_{\mathrm{e}}}\), Theorem 1 implies that whenever \(d_{\pi_{\mathrm{e}}}\) considers two state-action pairs to be close or have similar representations, they will also have close action-values. In the context of ope, if the distance metric considers two state-action pairs that have _different_ action-values to be zero distance apart/have the same representation, then fge will have to output two different action-values for the same input representation, which inevitably means fge must be inaccurate for at least one state-action pair.

### Learning State-Action Representations with ROPE

In practice, our goal is to use \(d_{\pi_{\mathrm{e}}}\) to learn a state-action representation \(\phi(s,a)\in\mathbb{R}^{d}\) such that the distances between these representations matches the distance defined by \(d_{\pi_{\mathrm{e}}}\). To do so, we follow the approach by Castro et al. (2022) and directly parameterize the value \(d_{\pi_{\mathrm{e}}}(s_{1},a_{1};s_{2},a_{2})\) as follows:

\[d_{\pi_{\mathrm{e}}}(s_{1},a_{1};s_{2},a_{2})\approx\tilde{d}_{ \omega}(s_{1},a_{1};s_{2},a_{2})\coloneqq\frac{||\phi_{\omega}(s_{1},a_{1})|| _{2}^{2}+||\phi_{\omega}(s_{2},a_{2})||_{2}^{2}}{2}\\ +\beta\theta(\phi_{\omega}(s_{1},a_{1}),\phi_{\omega}(s_{2},a_{2}))\] (2)

in which \(\phi\) is parameterized by some function approximator whose parameter weights are denoted by \(\omega\), \(\theta(\cdot,\cdot)\) gives the angular distance between the vector arguments, and \(\beta\) is a parameter controlling the weight of the angular distance. We can then learn the desired \(\phi_{\omega}\) through a sampling-based bootstrapping procedure (Castro et al., 2022). More specifically, the following loss function is minimized to learn the optimal \(\omega^{*}\):

\[\mathcal{L}_{\text{ROPE}}(\omega):=\mathds{E}_{\mathcal{D}}\left[\left(|r(s_{ 1},a_{1})-r(s_{2},a_{2})|+\gamma\,\mathds{E}_{\pi_{\mathrm{e}}}[\tilde{d}_{ \omega}(s^{\prime}_{1},a^{\prime}_{1};s^{\prime}_{2},a^{\prime}_{2})]-\tilde{ d}_{\omega}(s_{1},a_{1};s_{2},a_{2})\right)^{2}\right]\] (3)

where \(\bar{\omega}\) is separate copy of \(\omega\) and acts as a target function approximator (Mnih et al., 2015), which is updated to \(\omega\) at a certain frequency. Once \(\phi_{\omega^{*}}\) is obtained using \(\mathcal{D}\), we use \(\phi_{\omega^{*}}\) with fde to perform ope with the same data. Conceptually, the fqe procedure is unchanged except the learned action-value function now takes \(\phi_{\omega^{*}}(s,a)\) as its argument instead of the state and action directly.

With rope, state-action pairs are grouped together when they have small pairwise rope distance. Thus, a given group of state-action pairs have similar state-action representations and are behaviorally similar (i.e, have similar rewards and lead to similar future states when following \(\pi_{\mathrm{e}}\)). Consequently, these state-action pairs will have a similar action-value, which allows data samples from any member of the group to learn the group's shared action-value as opposed to learning the action-value for each state-action pair individually. This generalized usage of data leads to more data-efficient learning. We refer the reader to Appendix C for rope's pseudo-code.

### Action-Value and Policy Value Bounds

We now theoretically analyze how rope state-action representations help fqe estimate \(\rho(\pi_{\mathrm{e}})\). For this analysis, we focus on hard groupings where groups of similar state-action pairs are aggregated into one cluster and no generalization is performed across clusters; in practice, we learn state-action representations in which the difference between representations approximates the rope distance between state-action pairs. Furthermore, for theoretical analysis, we consider exact computation of the rope diffuse metric and of action-values using dynamic programming. First, we present the following lemma. For proofs, refer to Appendix B.

**Lemma 1**.: _Assume the rewards \(\mathcal{R}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta([0,1])\) then given an aggregated mdp\(\widetilde{\mathcal{M}}=\langle\widetilde{\mathcal{S}},\widetilde{\mathcal{A}}, \widetilde{\mathcal{R}},\widetilde{P},\gamma,\tilde{d}_{0}\rangle\) constructed by aggregating state-actions in an \(\epsilon\)-neighborhood based on \(d_{\pi_{\mathrm{e}}}\), and an encoder \(\phi:\mathcal{X}\rightarrow\widetilde{\mathcal{X}}\) that maps state-actions in \(\mathcal{X}\) to these clusters, the action-value for the evaluation policy \(\pi_{\mathrm{e}}\) in the two mdps are bounded as:_

\[|q^{\pi_{\mathrm{e}}}(x)-\tilde{q}^{\pi_{\mathrm{e}}}(\phi(x))|\leq\frac{2 \epsilon}{(1-\gamma)}\]

Lemma 1 states that the error in our estimate of the true action-value function of \(\pi_{\mathrm{e}}\) is upper-bounded by the clustering radius \(d_{\pi_{\mathrm{e}}}\), \(\epsilon\). Lemma 1 then leads us to our main result:

**Theorem 2**.: _Under the same conditions as Lemma 1, the difference between the expected fitted \(q\)-evaluation (fde) estimate and the expected estimate of fqe+rope is bounded:_

\[\big{|}\,\mathds{E}_{s_{0},a_{0}\sim\pi_{\mathrm{e}}}[q^{\pi_{\mathrm{e}}}(s_{ 0},a_{0})]-\mathds{E}_{s_{0},a_{0}\sim\pi_{\mathrm{e}}}[q^{\pi_{\mathrm{e}}}( \phi(s_{0},a_{0}))]\big{|}\leq\frac{2\epsilon}{(1-\gamma)}\]

Theorem 2 tells us that the error in our estimate of \(\rho(\pi_{\mathrm{e}})\) is upper-bounded by the size of the clustering radius \(\epsilon\). The implication is that grouping state-action pairs according to the rope diffuse metric enables us to upper bound error in the ope estimate. At an extreme, if we only group state-action pairs with _zero_ rope distance together then we obtain zero absolute error meaning that the action-value function for the aggregated mdp is able to realize the action-value function of the original mdp.

## 4 Empirical Study

In this section, we present an empirical study of rope designed to answer the following questions:

1. Does rope group state-actions that are behaviorally similar according to \(q^{\pi_{e}}\)?
2. Does rope improve the data-efficiency of fqe and achieve lower ope error than other ope-based representation methods?
3. How sensitive is rope to hyperparameter tuning and extreme distribution shifts?

### Empirical Set-up

We now describe the environments and datasets used in our experiments.

**Didactic Domain.** We provide intuition about rope on our gridworld domain. In this tabular and deterministic environment, an agent starts from the bottom left of a \(3\times 3\) grid and moves to the terminal state at the top right. The reward function is the negative of the Manhattan distance from the top right. \(\pi_{\mathrm{e}}\) stochastically moves up or right from the start state and then deterministically moves towards the top right, and moves deterministically right when it is in the center. The behavior policy \(\pi_{b}\) acts uniformly at random in each state. We set \(\gamma=0.99\).

**High-Dimensional Domains.** We conduct our experiments on five domains: HumanoidStandup, Swimmer, HalfCheetah, Hopper, and Walker2D, each of which has \(393\), \(59\), \(23\), \(14\), and \(23\) as the native state-action dimension respectively. We set \(\gamma=0.99\).

**Datasets.** We consider \(12\) different datasets: \(3\) custom datasets for HumanoidStandup, Swimmer, and HalfCheetah; and \(9\) D4rl datasets [Fu et al., 2020] for HalfCheetah, Hopper, and Walker2D. Each of the three custom datasets is of size \(100\)K transition tuples with an equal split between samples generated by \(\pi_{\mathrm{e}}\) and a lower performing behavior policy. For the d4rl datasets, we consider three types for each domain: random, medium, medium-expert, which consists of samples from a random policy, a lower performing policy, and an equal split between a lower performing and expert evaluation policy (\(\pi_{\mathrm{e}}\)). Each dataset has 1M transition tuples. Note that due to known discrepancies between environment versions and state-action normalization procedures 1, we generate our own datasets using the publicly available policies2 instead of using the publicly available datasets. See Appendix D for the details on the data generation procedure.

Footnote 1: https://github.com/Farama-Foundation/D4RL/tree/master

Footnote 2: https://github.com/google-research/deep_ope

**Evaluation Protocol.** Following Fu et al. [2021], Voloshin et al. [2021] and to make error magnitudes more comparable across domains, we use relative mean absolute error (RMAE). RMAE is computed using a single dataset \(\mathcal{D}\) and by generating \(n\) seeds: \(\textsc{rMAE}_{i}(\hat{\rho}(\pi_{e})):=\frac{[\hat{\rho}(\pi_{e})-\hat{\rho} _{i}(\pi_{e})]}{[\hat{\rho}(\pi_{e})-\rho(\pi_{\text{rand}})]}\), where \(\hat{\rho}_{i}(\pi_{e})\) is computed using the \(i^{\text{th}}\) seed and \(\rho(\pi_{\text{rand}})\) is the value of a random policy. We then report the Interquartile Mean (iqm) [Agarwal et al., 2021b] of these \(n\) rmaes.

**Representation learning + OPE.** Each algorithm is given access to the same fixed dataset to learn \(q^{\pi_{e}}\). The representation learning algorithms (rope and baselines) use this dataset to first pre-train a representation encoder, which is then used to transform the fixed dataset. This transformed dataset is then used to estimate \(q^{\pi_{e}}\). Vanilla fqe directly operates on the original state-action pairs.

### Empirical Results

We now present our main empirical results.

#### 4.2.1 Designing ROPE: A State-Action Behavioral Similarity Metric for OPE

The primary consideration when designing a behavioral similarity distance function for ope, and specifically, for fqe is that the distance function should not consider two state-action pairs with different \(q^{\pi_{e}}\) values to be the same. Suppose we have a distance function \(d\), two state-actions pairs, \((s_{1},a_{1})\) and \((s_{2},a_{2})\), and their corresponding \(q^{\pi_{e}}\). Then if \(d(s_{1},a_{1};s_{2},a_{2})=0\), it should be the case that \(q^{\pi_{e}}(s_{1},a_{1})=q^{\pi_{e}}(s_{2},a_{2})\). On the other hand, if \(d(s_{1},a_{1};s_{2},a_{2})=0\) but \(q^{\pi_{e}}(s_{1},a_{1})\) and \(q^{\pi_{e}}(s_{2},a_{2})\) are very different, then fqe will have to output _different_ action-values for the _same_ input, thus inevitably making fqe inaccurate on these state-action pairs.

While there have been a variety of proposed behavioral similarity metrics for control, they do not always satisfy the above criterion for ope. We consider various state-action behavioral similarity metrics. Due to space constraints, we show results only for: on-policy mico[Castro et al., 2022]\(d_{\pi_{\mathrm{s}}}(s_{1},a_{1};s_{2},a_{2}):=|r(s_{1},a_{1})-r(s_{2},a_{2})|+ \gamma\operatorname{\mathbb{E}}_{a^{\prime}_{i},a^{\prime}_{2}\sim\pi_{ \mathrm{s}}}[d_{\pi_{\mathrm{s}}}((s^{\prime}_{1},a^{\prime}_{1}),(s^{\prime }_{2},a^{\prime}_{2}))]\), which groups state-actions that have equal \(q^{\pi_{\mathrm{e}}}\), and defer results for the random-policy metric [Dadashi et al., 2021] and policy similarity metric [Agarwal et al., 2021a] to the Appendix D.

We visualize how these different metrics group state-action pairs in our gridworld example where a state-action is represented by a triangle in the grid (Figure 1). The gridworld is \(3\times 3\) grid represented by \(9\) squares (states), each having \(4\) triangles (actions). A numeric entry in a given triangle represents either: 1) the action-value of that state-action pair for \(\pi_{\mathrm{e}}\) (Figure 1(a)) or 2) the group ID of the given state-action pair (Figures 1(b) and 1(c)). Along with the group ID, each state-action pair is color-coded indicating its group. In this tabular domain, we compute the distances using dynamic programming with expected updates.

The main question we answer is: does a metric group two state-action pairs together when they have the same action-values under \(\pi_{\mathrm{e}}\)? In Figure 1(a) we see the \(q^{\pi_{\mathrm{e}}}\) values for each state-action where all state-action pairs that have the same action-value are grouped together under the same color (e.g. all state-action pairs with \(q^{\pi_{\mathrm{e}}}(\cdot,\cdot)=-6\) belong to the same group (red)). In Figure 1(b), we see that rope's grouping is exactly aligned with the grouping in Figure 1(a) i.e. state-action pairs that have the same action-values have the same group ID and color. On the other hand, from Figure 1(c), we see that on-policy mico misaligns with Figure 1(a). In Appendix D, we also see similar misaligned groupings using the random-policy metric Dadashi et al. [2021] and policy similarity metric Agarwal et al. [2021a]. The misalignment of these metrics is due to the fact that they do not group state-action pairs togethers that share \(q^{\pi_{\mathrm{e}}}\) values.

#### 4.2.2 Deep OPE Experiments

We now consider ope in challenging, high dimensional continuous state and action space domains. We compare the rmae achieved by an ope algorithm using _different_ state-action representations as input. If algorithm A achieves lower error than algorithm B, then A is more data-efficient than B.

Custom Dataset ResultsFor the custom datasets, we consider mild distribution shift scenarios, which are typically easy for ope algorithms. In Figure 2, we report the rmae vs. training iterations of fqe with different state-action features fed into fqe. We consider three different state-action features: 1) rope (ours), 2) \(\pi_{\mathrm{e}}\)-critic, which is a representation outputted by the penultimate layer of the action-value function of \(\pi_{\mathrm{e}}\)[Wang et al., 2021], and 3) the original state-action features. Note that there is no representation _learning_ involved for 2) and 3). We set the learning rate for all neural network training (encoder and fqe) to be the same, hyperparameter sweep rope across \(\beta\) and the dimension of rope's encoder output, and report the lowest rmae achieved at the end of fqe training. For hyperparameter sensitivity results, see Section 4.2.3. For training details, see Appendix D.

Figure 1: Figure (a): \(q^{\pi_{\mathrm{e}}}\): center number in each triangle is the \(q^{\pi_{\mathrm{e}}}\) for that state-action pair. Center and right: group clustering according to rope (ours; Figure (b)) and on-policy mico (Figure (c)) (center number in each triangle is group ID). Two state-action pairs are grouped together if their distance according to the specific metric is \(0\). The top right cell is blank since it is the terminal state and is not grouped.

We find that fqe converges to an estimate of \(\rho(\pi_{\mathrm{e}})\) when it is fed these different state-action features. We also see that when fqe is fed features from rope it produces more data-efficient ope estimates than vanilla fqe. Under these mild distribution shift settings, \(\pi_{\mathrm{e}}\)-critic also performs well since the output of the penultimate layer of \(\pi_{\mathrm{e}}\)'s action-value function should have sufficient information to accurately estimate the action-value function of \(\pi_{\mathrm{e}}\).

D4RL Dataset ResultsOn the d4rl datasets, we analyze the final performance achieved by representation learning + ope algorithms on datasets with varying distribution shift. In addition to the earlier baselines, we evaluate Bellman Complete Learning Representations (bcrl) (Chang et al., 2022), which learns linearly Bellman complete representations and produces an ope estimate with Least-Squares Policy Evaluation (lspe) instead of fqe. We could not evaluate \(\pi_{\mathrm{e}}\)-critic since the d4rl\(\pi_{\mathrm{e}}\) critics were unavailable3. For bcrl, we use the publicly available code 4. For a fair comparison, we hyperparameter tune the representation output dimension and encoder architecture size of bcrl. We hyperparameter tune rope the same way as done for the custom datasets. We set the learning rate for all neural network training (encoder and fqe) to be the same. In Table 1, we report the lowest rmae achieved at the end of the ope algorithm's training. For the corresponding training graphs, see Appendix D.

Footnote 3: https://github.com/google-research/deep_ope

Footnote 4: https://github.com/CausalML/bcrl

We find that rope improves the data-efficiency of fqe substantially across varying distribution shifts. bcrl performs competitively, but its poorer ope estimates compared to rope is unsurprising since it is not designed for data-efficiency. It is also known that bcrl may produce less accurate ope estimates compared to fqe (Chang et al., 2022). fqe performs substantially worse on some datasets;

\begin{table}
\begin{tabular}{l l l l} \hline \hline  & \multicolumn{3}{c}{Algorithm} \\ \cline{2-4} Dataset & bcrl & fqe & rope (ours) \\ \hline HalfCheetah-random & \(0.979\pm 0.000\) & \(\mathbf{0.807\pm 0.010}\) & \(0.990\pm 0.001\) \\ HalfCheetah-medium & \(0.830\pm 0.007\) & \(0.770\pm 0.007\) & \(\mathbf{0.247\pm 0.001}\) \\ HalfCheetah-medium-expert & \(0.685\pm 0.013\) & \(0.374\pm 0.001\) & \(\mathbf{0.078\pm 0.043}\) \\ \hline Walker2D-random & \(1.022\pm 0.001\) & Diverged & \(\mathbf{0.879\pm 0.009}\) \\ Walker2D-medium & \(0.953\pm 0.019\) & Diverged & \(\mathbf{0.462\pm 0.093}\) \\ Walker2D-medium-expert & \(0.962\pm 0.037\) & Diverged & \(\mathbf{0.252\pm 0.126}\) \\ \hline Hopper-random & Diverged & Diverged & \(\mathbf{0.680\pm 0.05}\) \\ Hopper-medium & \(61.223\pm 92.282\) & Diverged & \(\mathbf{0.208\pm 0.048}\) \\ Hopper-medium-expert & \(9.08\pm 4.795\) & Diverged & \(\mathbf{0.192\pm 0.055}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Lowest rmae achieved by algorithm on d4rl datasets. iqm of errors for each domain were computed over \(20\) trials with \(95\%\) confidence intervals. Algorithms that diverged had a significantly high final error and/or upward error trend (see Appendix D for training curves). Lower is better.

Figure 2: rmae vs. training iterations of fqe on the custom datasets. iqm of errors for each domain were computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better.

however, it is known that fqe can diverge under extreme distribution shift (Wang et al., 2020, 2021). It is interesting, however, that rope is robust in these settings. We observe this robustness across a wide range of hyperparameters as well (see Section 4.2.3). We also find that when there is low diversity of rewards in the batch (for example, in the random datasets), it is more likely that the short-term distance component of rope is close to \(0\), which can result in a representation collapse.

#### 4.2.3 Ablations

Towards a deeper understanding of rope, we now present an ablation study of rope.

Hyperparameter SensitivityIn ope, hyperparameter tuning with respect to rmae is difficult since \(\rho(\pi_{e})\) is unknown in practice (Paine et al., 2020). Therefore, we need ope algorithms to not only produce accurate ope estimates, but also to be robust to hyperparameter tuning. Specifically, we investigate whether rope's representations produce more data-efficient ope estimates over fqe across rope's hyperparameters. In this experiment, we set the action-value function's learning rate to be the same for both algorithms. The hyperparameters for rope are: 1) the output dimension of the encoder and 2) \(\beta\), the weight on the angular distance between encodings. We plot the results in Figure 3 and observe that rope is able to produce substantially more data-efficient estimates compared to fqe for a wide range of its hyperparameters on the Walker2D-medium dataset, where fqe diverged (see Table 1). While it is unclear what the optimal hyperparameters should be, we find similar levels of robustness on other datasets as well (see Appendix D).

ROPE Representations Mitigate FQE DivergenceIt has been shown theoretically (Wang et al., 2020) and empirically (Wang et al., 2021) that under extreme distribution shift, fqe diverges i.e. it produces ope estimates that have arbitrarily large error. In Table 1, we also see similar results where fqe produces very high error on some datasets. fqe tends to diverge due to the deadly triad (Sutton and Barto, 2018): 1) off-policy data, 2) bootstrapping, and 3) function approximation.

A rather surprising but encouraging result that we find is that even though rope faces the deadly triad, it produces representations that _significantly_ mitigate fqe's divergence across a large number of trials and hyperparameter variations. To investigate how much rope aids convergence, we provide the performance profile5(Agarwal et al., 2021) based on the rmae distribution plot in Figure 4. Across all trials and hyperparameters, we plot the fraction of times an algorithm achieved an error less than some threshold. In addition to the earlier baselines, we also plot the performance of 1) fqe-clip which is fqe but whose bootstrapping targets are clipped between \(\left[\frac{r_{\text{min}}}{1-r_{\text{max}}},\frac{r_{\text{max}}}{1-r_{ \text{max}}}\right]\), where \(r_{\text{min}}\) and \(r_{\text{max}}\) are the minimum and maximum rewards in the fixed dataset; and 2) fqe-deep, which is regular fqe but whose action-value function network is double the capacity of fqe (see Appendix D for specifics).

Footnote 5: https://github.com/google-research/rliable/tree/master

From Figure 4, we see that nearly \(\approx 100\%\) of the runs of rope achieve an rmae of \(\leq 2\), while none of the fqe and fqe-deep runs produce even \(\leq 10\) rmae. The failure of fqe-deep suggests that the

Figure 3: Hyperparameter sensitivity. fqe vs. rope when varying rope’s encoder output dimension (top) and \(\beta\) (bottom) on the Walker2D-medium d4rl dataset. 1om of errors are computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better.

extra capacity rope has over fge (since rope has its own neural network encoder) is insufficient to explain why rope produces accurate ope estimates. We also find that in order to use fge with the native state-action representations, it is necessary to use domain knowledge and clip the bootstrapped target. While fge-clip avoids divergence, it is very unstable during training (see Appendix D). rope's ability to produce stable learning in fge without any clipping is promising since it suggests that it is possible to improve the robustness of fge if an appropriate representation is learned.

## 5 Limitations and Future Work

In this work, we showed that rope was able to improve the data-efficiency of fge and produce lower-error ope estimates than other ope-based representations. Here, we highlight limitations and opportunities for future work. A limitation of rope and other bisimulation-based metrics is that if the diversity of rewards in the dataset is low, they are susceptible to representation collapse since the short-term distance is close to \(0\). Further investigation is needed to determine how to overcome this limitation. Another very interesting future direction is to understand why rope's representations significantly mitigated fge's divergence. A starting point would be to explore potential connections between rope and Bellman complete representations (Szepesvari and Munos, 2005) and other forms of representation regularizers for fge6.

Footnote 6: https://offline-rl-neurips.github.io/2021/pdf/17.pdf

## 6 Conclusion

In this paper we studied the challenge of pre-training representations to increase the data efficiency of the fge ope estimator. Inspired by work that learns state similarity metrics for control, we introduced rope, a new diffuse metric for measuring behavioral similarity between state-action pairs for ope and used rope to learn state-action representations using available offline data. We theoretically showed that rope: 1) bounds the difference between the action-values between different state-action pairs and 2) results in bounded error between the value of \(\pi_{\mathrm{e}}\) according to the ground action-value and the action-value function that is fed with rope representations as input. We empirically showed that rope boosts the data-efficiency of fge and achieves lower ope error than other ope-based representation learning algorithms. Finally, we conducted a thorough ablation study and showed that rope is robust to hyperparameter tuning and _significantly_ mitigates fge's divergence, which is a well-known challenge in ope. To the best of our knowledge, our work is the first that successfully uses representation learning to improve the data-efficiency of ope.

Figure 4: rmae distributions across all runs and hyperparameters for each algorithm, resulting in \(\geq 20\) runs for each algorithm. The shaded region is a \(95\%\) confidence interval. Larger area under the curve is better. For visualization, we cut off the horizontal axis at \(10\) rmae. fge and fge-deep are flat at \(0\) i.e. neither had runs that produced an error less than \(10\).

## Remarks on Negative Societal Impact

Our work is largely focused on studying fundamental rl research questions, and thus we do not see any immediate negative societal impacts. The aim of our work is to enable effective ope in many real world domains. Effective ope means that a user can estimate policy performance prior to deployment which can help avoid deployment of poor policies and thus positively impact society.

## Acknowledgments

Thanks to Adam Labiosa and the anonymous reviewers for feedback that greatly improved our work. Support for this research was provided by American Family Insurance through a research partnership with the University of Wisconsin--Madison's Data Science Institute.

## References

* Agarwal et al. (2021) Rishabh Agarwal, Marlos C. Machado, Pablo Samuel Castro, and Marc G Bellemare. Contrastive behavioral similarity embeddings for generalization in reinforcement learning. In _International Conference on Learning Representations_, 2021a. URL https://openreview.net/forum?id=qda7-sVg84.
* Agarwal et al. (2021b) Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare. Deep reinforcement learning at the edge of the statistical precipice. _Advances in Neural Information Processing Systems_, 34, 2021b.
* Ba et al. (2016) Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. _ArXiv_, abs/1607.06450, 2016.
* Castro (2019) Pablo Samuel Castro. Scalable methods for computing state similarity in deterministic Markov Decision Processes, November 2019. URL http://arxiv.org/abs/1911.09291. arXiv:1911.09291 [cs, stat].
* Castro et al. (2022) Pablo Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland. MICo: Improved representations via sampling-based state similarity for Markov decision processes. _arXiv:2106.08229 [cs]_, January 2022. URL http://arxiv.org/abs/2106.08229. arXiv: 2106.08229.
* Chang et al. (2022) Jonathan Chang, Kaiwen Wang, Nathan Kallus, and Wen Sun. Learning Bellman Complete Representations for Offline Policy Evaluation. In _Proceedings of the 39th International Conference on Machine Learning_, pages 2938-2971. PMLR, June 2022. URL https://proceedings.mlr.press/v162/chang22b.html. ISSN: 2640-3498.
* Dadashi et al. (2021) Robert Dadashi, Shideh Rezaeifar, Nino Vieillard, Leonard Hussenot, Olivier Pietquin, and Matthieu Geist. Offline reinforcement learning with pseudometric learning. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 2307-2318. PMLR, 18-24 Jul 2021. URL https://proceedings.mlr.press/v139/dadashi21a.html.
* Farajtabar et al. (2018) Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More Robust Doubly Robust Off-policy Evaluation. In _Proceedings of the 35th International Conference on Machine Learning_, pages 1447-1456. PMLR, July 2018. URL https://proceedings.mlr.press/v80/farajtabar18a.html. ISSN: 2640-3498.
* Ferns and Precup (2014) Norm Ferns and Doina Precup. Bisimulation metrics are optimal value functions. In _Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence_, UAI'14, page 210-219, Arlington, Virginia, USA, 2014. AUAI Press. ISBN 9780974903910.
* Ferns et al. (2004) Norm Ferns, Prakash Panangaden, and Doina Precup. Metrics for finite markov decision processes. In _Proceedings of the 20th Conference on Uncertainty in Artificial Intelligence_, UAI '04, page 162-169, Arlington, Virginia, USA, 2004. AUAI Press. ISBN 0974903906.
* Ferns et al. (2011) Norm Ferns, Prakash Panangaden, and Doina Precup. Bisimulation metrics for continuous markov decision processes. _SIAM Journal on Computing_, 40(6):1662-1714, 2011. doi: 10.1137/10080484X. URL https://doi.org/10.1137/10080484X.
* Furns et al. (2017)Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2020.
* Fu et al. (2021) Justin Fu, Mohammad Norouzi, Ofir Nachum, George Tucker, Ziyu Wang, Alexander Novikov, Mengjiao Yang, Michael R. Zhang, Yutian Chen, Aviral Kumar, Cosmin Paduraru, Sergey Levine, and Thomas Paine. Benchmarks for deep off-policy evaluation. In _ICLR_, 2021. URL https://openreview.net/forum?id=kWSeGEeHvF8.
* Gelada et al. (2019) Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G. Bellemare. Deep-MDP: Learning Continuous Latent Space Models for Representation Learning. Technical Report arXiv:1906.02736, arXiv, June 2019. URL http://arxiv.org/abs/1906.02736. arXiv:1906.02736 [cs, stat] type: article.
* Haarnoja et al. (2018) Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 1861-1870. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/haarnoja18b.html.
* Hanna et al. (2017) Josiah Hanna, Peter Stone, and Scott Niekum. Bootstrapping with Models: Confidence Intervals for Off-Policy Evaluation. In _Proceedings of the 16th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)_, May 2017. event-place: Sao Paolo, Brazil.
* Hanna et al. (2021) Josiah P. Hanna, Scott Niekum, and Peter Stone. Importance Sampling in Reinforcement Learning with an Estimated Behavior Policy. _Machine Learning (MLJ)_, 110(6):1267-1317, May 2021.
* Jiang and Li (2016) Nan Jiang and Lihong Li. Doubly Robust Off-policy Value Evaluation for Reinforcement Learning. May 2016. URL http://arxiv.org/abs/1511.03722. arXiv: 1511.03722.
* Kemertas and Aumentado-Armstrong (2021) Mete Kemertas and Tristan Aumentado-Armstrong. Towards robust bisimulation metric learning. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual_, pages 4764-4777, 2021. URL https://proceedings.neurips.cc/paper/2021/hash/256bf8e6923a52fda8ddf7dc050a1148-Abstract.html.
* Kozen (2006) Dexter Kozen. Coinductive proof principles for stochastic processes. In _Proceedings of the 21st Annual IEEE Symposium on Logic in Computer Science_, LICS '06, page 359-366, USA, 2006. IEEE Computer Society. ISBN 0769526314. doi: 10.1109/LICS.2006.18. URL https://doi.org/10.1109/LICS.2006.18.
* Le et al. (2019) Hoang M. Le, Cameron Voloshin, and Yisong Yue. Batch Policy Learning under Constraints. In _International Conference on Machine Learning (ICML)_. arXiv, March 2019. URL http://arxiv.org/abs/1903.08738. arXiv: 1903.08738 [cs, math, stat].
* Levine et al. (2020) Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems, November 2020. URL http://arxiv.org/abs/2005.01643. arXiv:2005.01643 [cs, stat].
* Li et al. (2006) Lihong Li, Thomas J Walsh, and Michael L Littman. Towards a Unified Theory of State Abstraction for MDPs. page 10, 2006.
* Liu et al. (2018) Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation. _arXiv:1810.12429 [cs, stat]_, October 2018. URL http://arxiv.org/abs/1810.12429. arXiv: 1810.12429.
* Matthews (1992) Steve Matthews. The topology of partial metric spaces. 1992.
* Mnih et al. (2015) Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529-533, February 2015. ISSN 00280836. URL http://dx.doi.org/10.1038/nature14236.
* Mnih et al. (2015)Tom Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna, Alexander Novikov, Ziyu Wang, and Nando de Freitas. Hyperparameter selection for offline reinforcement learning. _CoRR_, abs/2007.09055, 2020. URL https://arxiv.org/abs/2007.09055.
* Pavse and Hanna (2023) Brahma S Pavse and Josiah P Hanna. Scaling Marginalized Importance Sampling to High-Dimensional State-Spaces via State Abstraction. 2023.
* Precup et al. (2014) Doina Precup, Richard S Sutton, and Sanjoy Dasgupta. Off-Policy Temporal-Difference Learning with Function Approximation.
* Puterman (2014) Martin L Puterman. _Markov decision processes: discrete stochastic dynamic programming_. John Wiley & Sons, 2014.
* First Experiences with a Data Efficient Neural Reinforcement Learning Method. In Joao Gama, Rui Camacho, Pavel B. Brazdil, Alipio Mario Jorge, and Luis Torgo, editors, _Machine Learning: ECML 2005_, Lecture Notes in Computer Science, pages 317-328, Berlin, Heidelberg, 2005. Springer. ISBN 978-3-540-31692-3. doi: 10.1007/11564096_32.
* Sutton (1988) Richard S. Sutton. Learning to predict by the methods of temporal differences. _Machine Learning_, 3(1):9-44, August 1988. ISSN 1573-0565. doi: 10.1007/BF00115009. URL https://doi.org/10.1007/BF00115009.
* Sutton and Barto (2018) Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. The MIT Press, second edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.html.
* Szepesvari and Munos (2005) Csaba Szepesvari and Remi Munos. Finite time bounds for sampling based fitted value iteration. In _Proceedings of the 22nd International Conference on Machine Learning_, ICML '05, page 880-887, New York, NY, USA, 2005. Association for Computing Machinery. ISBN 1595931805. doi: 10.1145/1102351.1102462. URL https://doi.org/10.1145/1102351.1102462.
* Theocharous et al. (2015) Georgios Theocharous, Philip S Thomas, and Mohammad Ghavamzadeh. Personalized Ad Recommendation Systems for Life-Time Value Optimization with Guarantees. page 7, 2015.
* Thomas and Brunskill (2016) Philip S. Thomas and Emma Brunskill. Data-Efficient Off-Policy Policy Evaluation for Reinforcement Learning, April 2016. URL http://arxiv.org/abs/1604.00923. arXiv:1604.00923 [cs].
* Thomas et al. (2017) Philip S Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. High-Confidence Off-Policy Evaluation. page 7.
* Uehara et al. (2020) Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax Weight and Q-Function Learning for Off-Policy Evaluation, October 2020. URL http://arxiv.org/abs/1910.12809. Number: arXiv:1910.12809 arXiv:1910.12809 [cs, stat].
* Villani (2008) Cedric Villani. Optimal transport: Old and new. 2008.
* Voloshin et al. (2021) Cameron Voloshin, Hoang Minh Le, Nan Jiang, and Yisong Yue. Empirical study of off-policy policy evaluation for reinforcement learning. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)_, 2021. URL https://openreview.net/forum?id=IsK8iKDL-I.
* Wang et al. (2020) Ruosong Wang, Dean P. Foster, and Sham M. Kakade. What are the statistical limits of offline RL with linear function approximation? _CoRR_, abs/2010.11895, 2020. URL https://arxiv.org/abs/2010.11895.
* Wang et al. (2021) Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, and Sham Kakade. Instabilities of Offline RL with Pre-Trained Neural Representation. In _Proceedings of the 38th International Conference on Machine Learning_, pages 10948-10960. PMLR, July 2021. URL https://proceedings.mlr.press/v139/wang21z.html. ISSN: 2640-3498.
* Yang and Nachum (2021) Mengjiao Yang and Ofir Nachum. Representation Matters: Offline Pretraining for Sequential Decision Making. In _Proceedings of the 38th International Conference on Machine Learning_, pages 11784-11794. PMLR, July 2021. URL https://proceedings.mlr.press/v139/yang21h.html. ISSN: 2640-3498.
* Yang et al. (2020)Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via the regularized lagrangian. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 6551-6561. Curran Associates, Inc., 2020a. URL https://proceedings.neurips.cc/paper/2020/file/488e4104520c6aab692863cc1dba45af-Paper.pdf.
* Yang et al. (2020b) Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-Policy Evaluation via the Regularized Lagrangian. _arXiv:2007.03438 [cs, math, stat]_, July 2020b. URL http://arxiv.org/abs/2007.03438. arXiv: 2007.03438.
* Zhang et al. (2021a) Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. Learning Invariant Representations for Reinforcement Learning without Reconstruction, April 2021a. URL http://arxiv.org/abs/2006.10742. arXiv:2006.10742 [cs, stat].
* Zhang et al. (2021b) Michael R Zhang, Tom Le Paine, Ofir Nachum, Cosmin Paduraru, George Tucker, Ziyu Wang, and Mohammad Norouzi. AUTORERESSIVE DYNAMICS MODELS FOR OFFLINE POLICY EVALUATION AND OPTIMIIZATION. 2021b.
* Lukaszyk (2004) Szymon Lukaszyk. A new concept of probability metric and its applications in approximation of scattered data sets. _Computational Mechanics_, 33:299-304, 2004.

Theoretical Background

In this section, we include relevant background material.

**Definition 2** (Metric).: _A metric, \(d:X\times X\to\mathbb{R}_{\geq 0}\) has the following properties for some \(x,y,z\in X\):_

1. \(d(x,x)=0\)__
2. \(d(x,y)=0\Longleftrightarrow x=y\)__
3. _Symmetry:_ \(d(x,y)=d(y,x)\)__
4. _Triangle inequality:_ \(d(x,z)\leq d(x,y)+d(y,z)\)__

**Definition 3** (Pseudo Metric).: _A pseudo metric, \(d:X\times X\to\mathbb{R}_{\geq 0}\) has the following properties for some \(x,y,z\in X\):_

1. \(d(x,x)=0\)__
2. _Symmetry:_ \(d(x,y)=d(y,x)\)__
3. _Triangle inequality:_ \(d(x,z)\leq d(x,y)+d(y,z)\)__

_Crucially, a pseudo metric differs from a metric in that if \(d(x,y)=0\) it may be the case that \(x\neq y\)._

**Definition 4** (Diffuse Metric).: _A diffuse metric, \(d:X\times X\to\mathbb{R}_{\geq 0}\) has the following properties for some \(x,y,z\in X\):_

1. \(d(x,x)\geq 0\)__
2. _Symmetry:_ \(d(x,y)=d(y,x)\)__
3. _Triangle inequality:_ \(d(x,z)\leq d(x,y)+d(y,z)\)__

_Crucially, a diffuse metric differs from a pseudo metric in that self-distances may be non-zero._

For readers interested in distances that admit non-zero self-distances, we refer them to material on _partial metrics_[12]. We make the following note as Castro et al. [2022]: the original definition of partial metrics (see Matthews [1992]) uses a different triangle inequality criterion than the one in Definition 4 and is too strict (i.e. diffuse metrics violate this triangle inequality criterion), so we consider the diffuse metric definition presented in this paper.

We now present background material on the Wasserstein and related distances.

**Definition 5** (Wasserstein Distance [2008]).: _Let \(d:X\times X\to\mathbb{R}_{\geq 0}\) be a distance function and \(\Omega\) the set of all joint distributions with marginals \(\mu\) and \(\lambda\) over the space \(X\), then we have:_

\[W(d)(\mu,\lambda)=\left(\inf_{\omega\in\Omega}\mathbb{E}_{x_{1},x_{2}\sim \omega}[d(x_{1},x_{2})]\right)\] (4)

**Definition 6** (Dual formulation of the Wasserstein Distance [2008]).: _Let \(d:X\times X\to\mathbb{R}_{\geq 0}\) be a distance function and marginals \(\mu\) and \(\lambda\) over the space \(X\), then we have:_

\[W(d)(\mu,\lambda)=\sup_{f\in\text{Lip}_{1,d}(X)}\mathbb{E}_{x_{1}\sim\mu}[f(x _{1})]-\mathbb{E}_{x_{2}\sim\lambda}[f(x_{2})]\] (5)

_where \(\text{Lip}_{1,d}(X)\) denotes the \(1-\)Lipschitz functions \(f:X\to\mathbb{R}\) such that \(|f(x_{1})-f(x_{2})|\leq d(x_{1},x_{2})\)._

**Definition 7** (Lukaszyk-Karmowski distance [12]).: _Let \(d:X\times X\to\mathbb{R}_{\geq 0}\) be a distance function and marginals \(\mu\) and \(\lambda\) over the space \(X\), then we have:_

\[D_{\text{LK}}(d)(\mu,\lambda)=(\mathbb{E}_{x_{1}\sim\mu,x_{2}\sim\lambda}[d(x_ {1},x_{2})])\] (6)

We then have the following fact: \(W(d)(\mu,\lambda)\leq D_{\text{LK}}(d)(\mu,\lambda)\) i.e. the Wasserstein distance is upper-bounded by the Lukaszyk-Karmowski distance [12].

## Appendix B Theoretical Results

**Proposition 1**.: _The operator \(\mathcal{F}^{\pi_{e}}\) is a contraction mapping on \(\mathbb{R}^{\mathcal{X}\times\mathcal{X}}\) with respect to the \(L^{\infty}\) norm._

Proof.: Consider \(d,d^{\prime}\in\mathbb{R}^{\mathcal{X}\times\mathcal{X}}\), then we have:

\[||(\mathcal{F}^{\pi_{e}}d)(s_{1},a_{1};s_{2},a_{2}) -(\mathcal{F}^{\pi_{e}}d^{\prime})(s_{1},a_{1};s_{2},a_{2})||_{\infty}\] \[=||\gamma\operatorname{E}_{s^{\prime}_{1},s^{\prime}_{2}\sim P,a^ {\prime}_{1},a^{\prime}_{2}\sim\pi_{e}}[d(s^{\prime}_{1},a^{\prime}_{1};s^{ \prime}_{2},a^{\prime}_{2})-d^{\prime}(s^{\prime}_{1},a^{\prime}_{1};s^{\prime }_{2},a^{\prime}_{2})]||_{\infty}\] \[=|\gamma|\cdot||\operatorname{E}_{s^{\prime}_{1},s^{\prime}_{2} \sim P,a^{\prime}_{1},a^{\prime}_{2}\sim\pi_{e}}[d(s^{\prime}_{1},a^{\prime} _{1};s^{\prime}_{2},a^{\prime}_{2})-d^{\prime}(s^{\prime}_{1},a^{\prime}_{1};s^ {\prime}_{2},a^{\prime}_{2})]||_{\infty}\] \[\leq\gamma\max_{s^{\prime}_{1},a^{\prime}_{1},s^{\prime}_{2},a^{ \prime}_{2}}|d(s^{\prime}_{1},a^{\prime}_{1};s^{\prime}_{2},a^{\prime}_{2})-d ^{\prime}(s^{\prime}_{1},a^{\prime}_{1};s^{\prime}_{2},a^{\prime}_{2})]|= \gamma||d-d^{\prime}||_{\infty}\]

**Proposition 2**.: _The operator \(\mathcal{F}^{\pi_{e}}\) has a unique fixed point \(d_{\pi_{e}}\in\mathbb{R}^{\mathcal{X}\times\mathcal{X}}\). Let \(d_{0}\in\mathbb{R}^{\mathcal{X}\times\mathcal{X}}\), then \(\lim_{t\to\infty}\mathcal{F}^{\pi_{e}}_{t}(d_{0})=d_{\pi_{e}}\)._

Proof.: Since \(\mathcal{F}^{\pi_{e}}\) is a contraction mapping and that \(\mathbb{R}^{\mathcal{X}\times\mathcal{X}}\) is complete under the \(L^{\infty}\) norm, by Banach's fixed-point theorem, \(\lim_{t\to\infty}\mathcal{F}^{\pi_{e}}_{t}(d)=d_{\pi_{e}}\). 

**Proposition 3**.: \(d_{\pi_{e}}\) _is a diffuse metric._

Proof.: To prove that \(d_{\pi_{e}}\) is a diffuse metric, we need to show it has the following properties for \((s_{1},a_{1}),(s_{2},a_{2}),(s_{3},a_{3})\in\mathcal{X}\). We follow Castro et al. [2022]'s strategy (see Proposition 4.10) to prove that a distance function is a diffuse metric. Recall that \(d_{\pi_{e}}(s_{1},a_{1};s_{2},a_{2}):=|r(s_{1},a_{1})-r(s_{2},a_{2})|+\gamma \operatorname{E}_{s^{\prime}_{1},s^{\prime}_{2}\sim P,a^{\prime}_{1},a^{\prime }_{2}\sim\pi_{e}}[d_{\pi_{e}}(s^{\prime}_{1},a^{\prime}_{1};s^{\prime}_{2},a^ {\prime}_{2})]\).

1. Non-negativity i.e. \(d_{\pi_{e}}(s_{1},a_{1};s_{2},a_{2})\geq 0\). Since \(|r(s_{1},a_{1})-r(s_{2},a_{2})|\geq 0\), recursively rolling out the definition of \(d_{\pi_{e}}\) means that \(d_{\pi_{e}}(s_{1},a_{1};s_{2},a_{2})\) is a sum of discounted non-negative terms.
2. Symmetry i.e. \(d_{\pi_{e}}(s_{1},a_{1};s_{2},a_{2})=d_{\pi_{e}}(s_{2},a_{2};s_{1},a_{1})\). Since \(|r(s_{1},a_{1})-r(s_{2},a_{2})|=|r(s_{2},a_{2})-r(s_{1},a_{1})|\), unrolling \(d_{\pi_{e}}(s_{1},a_{1};s_{2},a_{2})\) and \(d_{\pi_{e}}(s_{2},a_{2};s_{1},a_{1})\) recursively results in the discounted sum of the same terms.
3. Triangle inequality i.e. \(d_{\pi_{e}}(s_{1},a_{1};s_{2},a_{2})\leq d_{\pi_{e}}(s_{1},a_{1};s_{3},a_{3}) +d_{\pi_{e}}(s_{2},a_{2};s_{3},a_{3})\). To show this fact, we will first consider an initialization to the distance function \(d_{0}(s_{1},a_{1};s_{2},a_{2})=0,\forall(s_{1},a_{1}),(s_{2},a_{2})\in \mathcal{X}\) and consider repeated applications of the operator \(\mathcal{F}^{\pi_{e}}\) to \(d_{0}\), which we know will make \(d_{0}\) converge to \(d_{\pi_{e}}\) (Proposition 2). We will show by induction that each successive update \(d_{t+1}=\mathcal{F}^{\pi_{e}}(d_{t})\) satisfies the triangle inequality, which implies that \(d_{\pi_{e}}\) satisfies the triangle inequality. We have the base the case at \(t=0\) trivially holding true due to the initialization of \(d_{0}\). Now let the inductive hypothesis be true for all \(t>1\) i.e. \(d_{t}(s_{1},a_{1};s_{2},a_{2})\leq d_{t}(s_{1},a_{1};s_{3},a_{3})+d_{t}(s_{3},a_ {3};s_{2},a_{2})\) for any \((s_{1},a_{1}),(s_{2},a_{2}),(s_{3},a_{3})\in\mathcal{X}\). However, we know that: \[d_{t+1}(s_{1},a_{1};s_{2},a_{2}) =|r(s_{1},a_{1})-r(s_{2},a_{2})|+\gamma \operatorname{E}_{s^{\prime}_{1},s^{\prime}_{2}\sim P,a^{\prime}_{1},a^{\prime}_ {2}\sim\pi_{e}}[d_{t}(s^{\prime}_{1},a^{\prime}_{1};s^{\prime}_{2},a^{\prime}_ {2})]\] \[\stackrel{{(a)}}{{=}}|r(s_{1},a_{1})-r(s_{2},a_{2})|+r (s_{3},a_{3})-r(s_{3},a_{3})+\gamma\operatorname{E}_{s^{\prime}_{1},s^{\prime}_{2} \sim P,a^{\prime}_{1},a^{\prime}_{2}\sim\pi_{e}}[d_{t}(s^{\prime}_{1},a^{\prime} _{1};s^{\prime}_{2},a^{\prime}_{2})]\] \[\stackrel{{(b)}}{{\leq}}|r(s_{1},a_{1})-r(s_{3},a_{3} )|+|r(s_{2},a_{2})-r(s_{3},a_{3})|\] \[+\gamma\operatorname{E}_{s^{\prime}_{1},s^{\prime}_{2}\sim P,a^{ \prime}_{1},a^{\prime}_{2}\sim\pi_{e}}[d_{t}(s^{\prime}_{1},a^{\prime}_{1};s^{ \prime}_{2},a^{\prime}_{2})]\] \[\stackrel{{(c)}}{{\leq}}|r(s_{1},a_{1})-r(s_{3},a_{3} )|+|r(s_{2},a_{2})-r(s_{3},a_{3})|\] \[+\gamma\operatorname{E}_{s^{\prime}_{1},s^{\prime}_{2},s^{\prime}_{ 3}\sim P,a^{\prime}_{1},a^{\prime}_{2},a^{\prime}_{3}\sim\pi_{e}}[d_{t}(s^{ \prime}_{1},a^{\prime}_{1};s^{\prime}_{3},a^{\prime}_{3})+d_{t}(s^{\prime}_{3},a^{ \prime}_{3};s^{\prime}_{2},a^{\prime}_{2})]\] \[=|r(s_{1},a_{1})-r(s_{3},a_{3})|+\gamma\operatorname{E}_{s^{ \prime}_{1},s^{\prime}_{2}\sim P,a^{\prime}_{1},a^{\prime}_{1},a^{\prime}_{3} \sim\pi_{e}}[d_{t}(s^{\prime}_{1},a^{\prime}_{1};s^{\prime}_{3},a^{\prime}_{3} )]\] \[+|r(s_{2},a_{2})-r(s_{3},a_{3})|+\gamma\operatorname{E}_{s^{ \prime}_{2},s^{\prime}_{3}\sim P,a^{\prime}_{2},a^{\prime}_{3}\sim\pi_{ewhere (a) is due to adding and subtracting \(r(s_{3},a_{3})\), (b) is due to Jensen's inequality, (c) is due to application of the inductive hypothesis. Thus, the triangle inequality is satisfied for all \(t\geq 0\), and given that \(d_{t+1}\to d_{\pi_{e}}\), we have that \(d_{\pi_{e}}\) also satisfies the triangle inequality.

**Theorem 1**.: _For any evaluation policy \(\pi_{e}\) and \((s_{1},a_{1}),(s_{2},a_{2})\in\mathcal{X}\), we have that \(|q^{\pi_{e}}(s_{1},a_{1})-q^{\pi_{e}}(s_{2},a_{2})|\leq d_{\pi_{e}}(s_{1},a_{1 },;s_{2},a_{2})\)._

Proof.: To prove this fact, we follow Castro et al. (2022) (see Proposition 4.8) and use a co-inductive argument (Kozen, 2006). We will show that if \(|q^{\pi_{e}}(s_{1},a_{1})-q^{\pi_{e}}(s_{2},a_{2})|\leq d(s_{1},a_{1},;s_{2},a_ {2})\) holds true for some specific symmetric \(d\in\mathbb{R}^{\mathcal{X}\times\mathcal{X}}\), then the statement also holds true for \(\mathcal{F}^{\pi_{e}}(d)\), which means it will hold for \(d_{\pi_{e}}\).

We have that for any \((s,a)\in\mathcal{X}\), \(\max_{s,a}\frac{-|r(s,a)|}{1-\gamma}\leq q^{\pi_{e}}(s,a)\leq\max_{s,a}\frac{ |r(s,a)|}{1-\gamma}\). Thus, for any \((s_{1},a_{1}),(s_{2},a_{2})\in\mathcal{X}\), we have that \(|q^{\pi_{e}}(s_{1},a_{1})-q^{\pi_{e}}(s_{2},a_{2})|\leq 2\max_{s,a}\frac{ |r(s,a)|}{1-\gamma}\). We can then assume that our specific symmetric \(d\) is the constant function \(d(s_{1},a_{1};s_{2},a_{2})=2\max_{s,a}\frac{|r(s,a)|}{1-\gamma}\), which satisfies our requirement that \(|q^{\pi_{e}}(s_{1},a_{1})-q^{\pi_{e}}(s_{2},a_{2})|\leq d(s_{1},a_{1},;s_{2},a _{2})\).

Therefore, we have \(q^{\pi_{e}}(s_{1},a_{1})-q^{\pi_{e}}(s_{2},a_{2})\)

\[=r(s_{1},a_{1})-r(s_{2},a_{2})+\gamma\sum_{s^{\prime}_{1}\in \mathcal{S}}\sum_{a^{\prime}_{1}\in\mathcal{A}}P(s^{\prime}_{1}|s_{1},a_{1}) \pi_{e}(a^{\prime}_{1}|s^{\prime}_{1})q^{\pi_{e}}(s^{\prime}_{1},a^{\prime}_{1 })-\gamma\sum_{s^{\prime}_{2}\in\mathcal{S}}\sum_{a^{\prime}_{2}\in\mathcal{A} }P(s^{\prime}_{2}|s_{2},a_{2})\pi_{e}(a^{\prime}_{2}|s^{\prime}_{2})q^{\pi_{e} }(s^{\prime}_{2},a^{\prime}_{2})\] \[\leq|r(s_{1},a_{1})-r(s_{2},a_{2})|+\gamma\sum_{s^{\prime}_{1},s^ {\prime}_{2}\in\mathcal{S}}\sum_{a^{\prime}_{1},a^{\prime}_{2}\in\mathcal{A} }P(s^{\prime}_{1}|s_{1},a_{1})\pi_{e}(a^{\prime}_{1}|s^{\prime}_{1})P(s^{ \prime}_{2}|s_{2},a_{2})\pi_{e}(a^{\prime}_{2}|s^{\prime}_{2})(q^{\pi_{e}}(s^ {\prime}_{1},a^{\prime}_{1})-q^{\pi_{e}}(s^{\prime}_{2},a^{\prime}_{2}))\] \[\stackrel{{(a)}}{{\leq}}|r(s_{1},a_{1})-r(s_{2},a_{2 })|+\gamma\sum_{s^{\prime}_{1},s^{\prime}_{2}\in\mathcal{S}}\sum_{a^{\prime}_{1 },a^{\prime}_{2}\in\mathcal{A}}P(s^{\prime}_{1}|s_{1},a_{1})\pi_{e}(a^{\prime} _{1}|s^{\prime}_{1})P(s^{\prime}_{2}|s_{2},a_{2})\pi_{e}(a^{\prime}_{2}|s^{ \prime}_{2})d(s^{\prime}_{1},a^{\prime}_{1};s^{\prime}_{2},a^{\prime}_{2})\] \[=\mathcal{F}^{\pi_{e}}(d)(s_{1},a_{1};s_{2},a_{2})\]

where (a) follows from the induction hypothesis. Similarly, by symmetry, we can show that \(q^{\pi_{e}}(s_{2},a_{2})-q^{\pi_{e}}(s_{1},a_{1})\leq\mathcal{F}^{\pi_{e}}(d)(s_ {1},a_{1};s_{2},a_{2})\). Thus, we have it that \(|q^{\pi_{e}}(s_{1},a_{1})-q^{\pi_{e}}(s_{2},a_{2})|\leq d_{\pi_{e}}(s_{1},a_{1 },;s_{2},a_{2})\). 

**Lemma 1**.: _Assume the rewards \(\mathcal{R}:\mathcal{S}\times\mathcal{A}\to\Delta([0,1])\) then given an aggregated mdp\(\widetilde{\mathcal{M}}=\langle\widetilde{\mathcal{S}},\widetilde{\mathcal{A}}, \widetilde{\mathcal{R}},\widetilde{P},\gamma,\tilde{d}_{0}\rangle\) constructed by aggregating state-actions in an \(\epsilon\)-neighborhood based on \(d_{\pi_{e}}\), and an encoder \(\phi:\mathcal{X}\to\widetilde{\mathcal{X}}\) that maps state-actions in \(\mathcal{X}\) to these clusters, the action-value for the evaluation policy \(\pi_{e}\) in the two mdps are bounded as:_

\[|q^{\pi_{e}}(x)-\tilde{q}^{\pi_{e}}(\phi(x))|\leq\frac{2\epsilon}{(1-\gamma)}\]

Proof.: The proof closely follows that of Lemma 8 of Kemertas and Aumentado-Armstrong (2021), which is in turn based on Theorem 5.1 of Ferns et al. (2004). The main difference between their theorems and ours is that the former is based on state representations and the latter is based on optimal state-value functions, while ours is focused on state-action representations for \(\pi_{e}\).

We first remark that this new aggregated MDP, \(\widetilde{\mathcal{M}}\), can be viewed as a Markov reward process (MRP) where the "states" are aggregated state-action pairs of the original MDP, \(\mathcal{M}\). We now define the reward function and transition dynamics of the clustered MRP \(\widetilde{\mathcal{M}}\), where \(|\phi(x)|\) is the size of the cluster \(\phi(x)\). Note that P denotes the probability of the event.

\[\tilde{r}(\phi(x)) =\frac{1}{|\phi(x)|}\sum_{y\in\phi(x)}r(y)\] \[\widetilde{P}(\phi(x^{\prime})|\phi(x)) =\frac{1}{|\phi(x)|}\sum_{y\in\phi(x)}\mathds{P}(\phi(x^{\prime})|y)\]Then we have: \(|q^{\pi_{e}}(x)-\tilde{q}^{\pi_{e}}(\phi(x))|\)

\[=\left|r(x)-\tilde{r}(\phi(x))+\gamma\sum_{x^{\prime}\in\mathcal{X} }P(x^{\prime}|x)q^{\pi_{e}}(x^{\prime})-\gamma\sum_{\phi(x^{\prime})\in \widetilde{\mathcal{X}}}\widetilde{P}(\phi(x^{\prime})|\phi(x))\tilde{q}^{\pi _{e}}(\phi(x^{\prime}))\right|\] \[\stackrel{{(a)}}{{=}}\left|r(x)-\frac{1}{|\phi(x)|} \sum_{y\in\phi(x)}r(y)+\gamma\sum_{x^{\prime}\in\mathcal{X}}P(x^{\prime}|x)q^{ \pi_{e}}(x^{\prime})-\gamma\frac{1}{|\phi(x)|}\sum_{\phi(x^{\prime})\in \widetilde{\mathcal{X}}}\sum_{y\in\phi(x)}\mathds{P}(\phi(x^{\prime})|y)\tilde {q}^{\pi_{e}}(\phi(x^{\prime}))\right|\] \[\stackrel{{(b)}}{{=}}\frac{1}{|\phi(x)|}\left| \left|\phi(x)|r(x)-\sum_{y\in\phi(x)}r(y)+\gamma|\phi(x)|\sum_{x^{\prime}\in \mathcal{X}}P(x^{\prime}|x)q^{\pi_{e}}(x^{\prime})-\gamma\sum_{\phi(x^{\prime })\in\widetilde{\mathcal{X}}}\sum_{y\in\phi(x)}\mathds{P}(\phi(x^{\prime})|y) \tilde{q}^{\pi_{e}}(\phi(x^{\prime}))\right.\] \[\stackrel{{(c)}}{{=}}\frac{1}{|\phi(x)|}\left|\sum_ {y\in\phi(x)}(r(x)-r(y))+\sum_{y\in\phi(x)}\left(\gamma\sum_{x^{\prime}\in \mathcal{X}}P(x^{\prime}|x)q^{\pi_{e}}(x^{\prime})-\gamma\sum_{\phi(x^{\prime })\in\widetilde{\mathcal{X}}}\mathds{P}(\phi(x^{\prime})|y)\tilde{q}^{\pi_{e}} (\phi(x^{\prime}))\right)\right|\] \[\stackrel{{(d.1)}}{{\leq}}\frac{1}{|\phi(x)|}\sum_{y \in\phi(x)}\left(\left|r(x)-r(y)\right|+\gamma\left|\sum_{x^{\prime}\in \mathcal{X}}P(x^{\prime}|x)q^{\pi_{e}}(x^{\prime})-\sum_{\phi(x^{\prime})\in \widetilde{\mathcal{X}}}\mathds{P}(\phi(x^{\prime})|y)\tilde{q}^{\pi_{e}}( \phi(x^{\prime}))\right|\right)\] \[\stackrel{{(d.2)}}{{=}}\frac{1}{|\phi(x)|}\sum_{y \in\phi(x)}\left(\left|r(x)-r(y)\right|+\gamma\left|\sum_{x^{\prime}\in \mathcal{X}}P(x^{\prime}|x)q^{\pi_{e}}(x^{\prime})-\sum_{\phi(x^{\prime})\in \widetilde{\mathcal{X}}}\sum_{z\in\phi(x^{\prime})}P(z|y)\tilde{q}^{\pi_{e}} (\phi(x^{\prime}))\right|\right)\] \[\stackrel{{(d.3)}}{{=}}\frac{1}{|\phi(x)|}\sum_{y \in\phi(x)}\left(\left|r(x)-r(y)\right|+\gamma\left|\sum_{x^{\prime}\in \mathcal{X}}P(x^{\prime}|x)q^{\pi_{e}}(x^{\prime})-\sum_{x^{\prime}\in \mathcal{X}}P(x^{\prime}|y)\tilde{q}^{\pi_{e}}(\phi(x^{\prime}))\right|\right)\] \[\stackrel{{(e)}}{{\leq}}\frac{1}{|\phi(x)|}\sum_{y \in\phi(x)}\left(\left|r(x)-r(y)\right|+\gamma\left|\sum_{x^{\prime}\in \mathcal{X}}\left(P(x^{\prime}|x)q^{\pi_{e}}(x^{\prime})-P(x^{\prime}|y)\tilde {q}^{\pi_{e}}(\phi(x^{\prime}))\right)\right|\right)\] \[\stackrel{{(f)}}{{\leq}}\frac{1}{|\phi(x)|}\sum_{y \in\phi(x)}\left(\left|r(x)-r(y)\right|+\gamma\left|\sum_{x^{\prime}\in \mathcal{X}}\left(P(x^{\prime}|x)q^{\pi_{e}}(x^{\prime})-P(x^{\prime}|y)q^{\pi _{e}}(x^{\prime})\right)\right|\right)\] \[+\frac{\gamma}{|\phi(x)|}\sum_{y\in\phi(x)}\left(\left|\sum_{x^{ \prime}\in\mathcal{X}}P(x^{\prime}|y)(q^{\pi_{e}}(x^{\prime})-\tilde{q}^{\pi_{ e}}(\phi(x^{\prime})))\right|\right)\] \[\stackrel{{(g)}}{{\leq}}\frac{1}{|\phi(x)|}\sum_{y \in\phi(x)}\left(\left|r(x)-r(y)\right|+\gamma\left|\sum_{x^{\prime}\in \mathcal{X}}\left(P(x^{\prime}|x)-P(x^{\prime}|y)\right)q^{\pi_{e}}(x^{\prime}) \right|+\gamma\left\|q-\tilde{q}\right\|_{\infty}\right)\] \[\stackrel{{(h)}}{{=}}\frac{1}{|\phi(x)|}\sum_{y\in \phi(x)}\left(\left|r(x)-r(y)\right|+\gamma\left|\mathbb{E}_{x^{\prime}\sim P( \cdot|x)}[q^{\pi_{e}}(x^{\prime})]-\mathbb{E}_{x^{\prime}\sim P(\cdot|y)}[q^{ \pi_{e}}(x^{\prime})]\right|+\gamma\left\|q-\tilde{q}\right\|_{\infty}\right)\]

where (a) is due to the definition of \(\bar{\mathrm{r}}\) and \(\widetilde{P}\), (b) is due to multiplying and dividing by \(|\phi(x)|\), (c) is due to re-arranging terms, (d.1) is due to Jensen's inequality, (d.2 and d.3) are disaggregating the sums over clustered state-actions into sums over original state-actions by expanding \(\mathds{P}(\phi(x^{\prime})|y)=\sum_{x\in\phi(x^{\prime})}P(x|y)\) for each clustered state-action, \(\phi(x^{\prime})\), (e) is grouping the terms, (f) is by adding and subtracting \(\frac{1}{|\phi(x)|}\sum_{y\in\phi(x)}P(x^{\prime}|y)q^{\pi_{e}}(x^{\prime})\), (g) is since the infinity norm of the difference of the action-values is greater than the expected difference, (h) is re-writing the expression in terms of expectations.

From Theorem 1 we know \(q^{\pi_{e}}\) is \(1\)-Lipschitz with respect to the distance function \(d_{\pi_{e}}\). Notice that (h) contains the dual formulation of the Wasserstein distance where \(f=q^{\pi_{e}}\) (see Definition 6). Wecan then re-write (h) in terms of original definition of the Wasserstein distance:

\[|q^{\pi_{e}}(x)-\tilde{q}^{\pi_{e}}(\phi(x))| \leq\frac{1}{|\phi(x)|}\sum_{y\in\phi(x)}\left(|r(x)-r(y)|+\gamma W( d_{\pi_{e}})(P(\cdot|x),P(\cdot|y))+\gamma\left\|q-\widetilde{q}\right\|_{\infty}\right)\] \[\overset{(i)}{\leq}\frac{1}{|\phi(x)|}\sum_{y\in\phi(x)}\left(|r( x)-r(y)|+\gamma D_{\text{LK}}(d_{\pi_{e}})(x^{\prime},y^{\prime})+\gamma\left\|q- \widetilde{q}\right\|_{\infty}\right)\] \[\overset{(j)}{=}\frac{1}{|\phi(x)|}\sum_{y\in\phi(x)}\left(|r(x)- r(y)|+\gamma\operatorname{E}_{x^{\prime}\sim\operatorname{P}^{\pi_{e}},y^{ \prime}\sim\operatorname{P}^{\pi_{e}}}[d_{\pi_{e}}(x^{\prime},y^{\prime})]+ \gamma\left\|q-\widetilde{q}\right\|_{\infty}\right)\] \[\overset{(k)}{=}\frac{1}{|\phi(x)|}\sum_{y\in\phi(x)}\left(d_{ \pi_{e}}(x,y)+\gamma\left\|q-\widetilde{q}\right\|_{\infty}\right)\] \[\overset{(l)}{\leq}2\epsilon+\gamma\left\|q-\widetilde{q}\right\| _{\infty}\] \[|q^{\pi_{e}}(x)-\tilde{q}^{\pi_{e}}(\phi(x))| \overset{(m)}{\leq}\frac{2\epsilon}{1-\gamma},\forall x\in\mathcal{X}\]

where (i) is due the fact that the Lukaszyk-Karmowski, \(D_{\text{LK}}\), upper bounds the Wasserstein distance, (j) is using Definition 7, (k) is due to the definition of \(d_{\pi_{e}}\), and (l) is due the fact that the maximum distance between any two \(x,y\in\phi(x)\) is at most \(2\epsilon\), which is greater than the average distance between any one point to every other point in the cluster, and (m) is due to \(\left\|q-\widetilde{q}\right\|_{\infty}\leq\frac{2\epsilon}{1-\gamma}\). 

**Theorem 2**.: _Under the same conditions as Lemma 1, the difference between the expected fitted \(q\)-evaluation (fqe) estimate and the expected estimate of fqe+rope is bounded:_

\[\big{|}\operatorname{E}_{s_{0},a_{0}\sim\pi_{e}}[q^{\pi_{e}}(s_{0},a_{0})]- \operatorname{E}_{s_{0},a_{0}\sim\pi_{e}}[q^{\pi_{e}}(\phi(s_{0},a_{0}))] \big{|}\leq\frac{2\epsilon}{(1-\gamma)}\]

Proof.: From Lemma 1 we have that \(|q^{\pi_{e}}(s_{0},a_{0})-q^{\pi_{e}}(\phi(s_{0},a_{0}))|\leq\frac{2\epsilon}{ (1-\gamma)}\).

\[\big{|}\operatorname{E}_{s_{0},a_{0}\sim\pi_{e}}[q^{\pi_{e}}(s_{0},a_{0})]-\operatorname{E}_{s_{0},a_{0}\sim\pi_{e}}[q^{\pi_{e}}(\phi(s_{0},a_{0 }))]\big{|} =|\operatorname{E}_{s_{0},a_{0}\sim\pi_{e}}[q^{\pi_{e}}(s_{0},a_{ 0})-q^{\pi_{e}}(\phi(s_{0},a_{0}))]|\] \[\overset{(a)}{\leq}\operatorname{E}_{s_{0},a_{0}\sim\pi_{e}}[|q ^{\pi_{e}}(s_{0},a_{0})-q^{\pi_{e}}(\phi(s_{0},a_{0}))|]\] \[\overset{(b)}{\leq}\operatorname{E}_{s_{0},a_{0}\sim\pi_{e}} \left(\frac{2\epsilon}{1-\gamma}\right)\] \[=\frac{2\epsilon}{(1-\gamma)},\]

where (a) follows from Jensen's inequality and (b) follows from Lemma 1. 

## Appendix C ROPE Pseudo-code

```
1:Input: policy to evaluate \(\pi_{e}\), batch \(\mathcal{D}\), encoder parameters class \(\Omega\), action-value parameter class \(\Xi\), encoder function \(\phi:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\), action-value function \(q:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\).
2:\(\hat{\omega}:=\operatorname*{arg\,min}_{\omega\in\Omega}\) \(\mathbb{E}_{(s_{1},a_{1},s^{\prime}_{1}),(s_{2},a_{2},s^{\prime}_{2})\sim \mathcal{D}}\left[\rho\left(|r(s_{1},a_{1})-r(s_{2},a_{2})|+\gamma \operatorname{E}_{a^{\prime}_{1},a^{\prime}_{2}\sim\pi_{e}}[\tilde{d}_{\tilde {\omega}}(s^{\prime}_{1},a^{\prime}_{1};s^{\prime}_{2},a^{\prime}_{2})|- \tilde{d}_{\omega}(s_{1},a_{1};s_{2},a_{2})\right)\right]\) {rope training phase; where \(\tilde{d}_{\omega}(s_{1},a_{1};s_{2},a_{2}):=\frac{||\phi_{\omega}(s_{1},a_{1})|| _{2}^{2}+||\phi_{\omega}(s_{2},a_{2})||_{2}^{2}}{2}+\beta\theta(\phi_{\omega}(s_ {1},a_{1}),\phi_{\omega}(s_{2},a_{2}))\), \(\bar{\omega}\) are fixed parameters of target network, and \(\rho\) is the Huber loss. See Section 3.1 for more details.}
3:\(\hat{\xi}:=\operatorname*{arg\,min}_{\xi\in\Xi}\operatorname{E}_{(s,a,s^{ \prime})\sim\mathcal{D}}\left[\rho\left(r(s,a)+\gamma\operatorname{E}_{a^{ \prime}\sim\pi_{e}}[q_{\xi}(\phi_{\tilde{\omega}}(s^{\prime},a^{\prime}))]-q_{ \xi}(\phi_{\tilde{\omega}}(s,a))\right)\right]\) {fqe using fixed encoder \(\phi_{\tilde{\omega}}\) from Step 2, where \(\rho\) is the Huber loss.}
4:Return \(q_{\xi}\) {Estimated action-value function of \(\pi_{e}\), \(q^{\pi_{e}}\).} ```

**Algorithm 1** rope+fqeEmpirical Results

We now include additional experiments that were deferred from the main text.

### Gridworld Visualizations

In Section 4.2.1, we visualize how rope and on-policy mico group state-actions pairs. We now consider two additional metrics that group state-action pairs:

1. Policy similarity metric [Agarwal et al., 2021a]: \(d_{\text{PSM}}(s_{1},a_{1};s_{2},a_{2}):=|\,\pi_{\text{e}}(a_{1}|s_{1})-\pi_{ \text{e}}(a_{2}|s_{2})|+\gamma\,\mathds{E}_{a_{1}^{\prime},a_{2}^{\prime} \sim\pi_{\text{e}}}[d_{\text{PSM}}((s_{1}^{\prime},a_{1}^{\prime}),(s_{2}^{ \prime},a_{2}^{\prime}))]\). This metric measures short- and long-term similarity based on how \(\pi_{\text{e}}\) acts in different states, not in terms of the rewards and returns it receives.
2. Random policy similarity metric [Dadashi et al., 2021]: \(d_{\text{RAND}}(s_{1},a_{1};s_{2},a_{2}):=|r(s_{1},a_{1})-r(s_{2},a_{2})|+ \gamma\,\mathds{E}_{a^{\prime}\sim l\mathcal{U}(\mathcal{A})}[d_{\text{RAND} }((s_{1}^{\prime},a^{\prime}),(s_{2}^{\prime},a^{\prime}))]\). Similar to \(d_{\pi_{\text{e}}}\), but considers behavior of a random policy that samples actions uniformly.

From Figure 5, we reach the same conclusion as we did in Section 4.2.1: that existing state-action similarity metrics are unsuitable for learning \(q^{\pi_{\text{e}}}\) due to how they group state-action pairs.

### Deep OPE Experiments

We now present additional details on our empirical setup and additional experiments.

#### d.2.1 Additional Empirical Setup Details

Before applying any of the algorithms, we normalize the states of the dataset to make the each feature dimension have \(0\) mean and \(1\) standard deviation.

FQE Training DetailsIn all experiments and all datasets, we use a neural network as fqe's action-value function with \(2\) layers and \(256\) neurons using relu activation function. We use mini-batch gradient descent to train the fqe network with mini-batch sizes of \(512\) and for \(300\)K gradient steps. We use the Adam optimizer with learning rate \(1e^{-5}\) and weight decay \(1e^{-2}\). fqe minimizes the Huber loss. The only changes for fqe-deep are that it uses a neural network size of \(4\) layers with \(256\) neurons and trains for \(500\)K gradient steps. Preliminary results with lower learning rates such as \(5e^{-6}\) and \(1e^{-6}\) did not make a difference. fqe uses an exponentially-moving average target network with \(\tau=0.005\) updated every epoch.

ROPE and BCRL DetailsIn all experiments and datasets, we use a neural network as the state-action encoder for rope with \(2\) layers and \(256\) neurons with the relu activation. We use mini-batch gradient descent to train the the encoder network with mini-batch sizes of \(512\) and for \(300\)

Figure 5: Figure (a): \(q^{\pi_{\text{e}}}\) for \(\pi_{\text{e}}\). Center and right: group clustering according to psm (Figure (b)) and random-policy metric (Figure (c)) (center number in each triangle is group ID). Two state-action pairs are grouped together if their distance according to the specific metric is \(0\). The top right cell is blank since it is the terminal state, which is not grouped.

gradient steps. For rope and bcrl, we hyperparameter sweep the output dimension of the encoder. Additionally, for rope, we sweep over the angular distance scalar, \(\beta\). For the output dimension, we sweep over dimensions: \(\{|X|/3,|X|/2,|X|\}\), where \(|X|\) is the dimension of the original state-action space of the environment. For \(\beta\), we sweep over \(\{0.1,1,10\}\). The best performing hyperparameter set is the one that results in lowest rmae (from \(\rho(\pi_{\mathrm{e}})\)) at the end of fqe training. rope uses an exponentially-moving average target network with \(\tau=0.005\) updated every epoch. Finally, the output of rope's encoder is fed through a LayerNorm (Ba et al., 2016) layer, followed by a tanh layer. rope minimizes the Huber loss.

When computing \(d^{\pi_{\mathrm{e}}}\approx\tilde{d}_{\omega}\) rope uses the same procedure as mico (appendix C.2. of Castro et al. (2022)):

\[\tilde{d}_{\omega}(s_{1},a_{1};s_{2},a_{2})\coloneqq\frac{||\phi_{\omega}(s_{1 },a_{1})||_{2}^{2}+||\phi_{\tilde{\omega}}(s_{2},a_{2})||_{2}^{2}}{2}+\beta \theta(\phi_{\omega}(s_{1},a_{1}),\phi_{\tilde{\omega}}(s_{2},a_{2}))\]

where it applies the target network parameters, \(\bar{\omega}\), on the \((s_{2},a_{2})\) pair for stability. For the angular distance \(\theta(\phi_{\omega}(s_{1},a_{1}),\phi_{\omega}(s_{2},a_{2}))\), we have the cosine-similarity and the angle as below. Note in practice, for numerical stability, a small constant (e.g. \(1e^{-6}\) or \(5e^{-5}\)) may have to be added when computing the square-root.

\[\text{CS}(\phi_{\omega}(s_{1},a_{1}),\phi_{\omega}(s_{2},a_{2})) =\frac{\langle\phi_{\omega}(s_{1},a_{1}),\phi_{\omega}(s_{2},a_{2} )\rangle}{||\phi_{\omega}(s_{1},a_{1})||||\phi_{\omega}(s_{2},a_{2})||}\] \[\theta(\phi_{\omega}(s_{1},a_{1}),\phi_{\omega}(s_{2},a_{2})) =\text{arctan2}\left(\sqrt{1-\text{CS}(\phi_{\omega}(s_{1},a_{1}),\phi_{\omega}(s_{2},a_{2}))^{2}},\text{CS}(\phi_{\omega}(s_{1},a_{1}),\phi_{ \omega}(s_{2},a_{2}))\right)\]

Custom DatasetsWe generate the datasets by training policies in the environment using sac (Haarnoja et al., 2018) and take the final policy at the end of training as \(\pi_{\mathrm{e}}\) and we use an earlier policy with lower performance as the behavior policy. The expected discounted return of the policies and datasets for each domain is given in Table 2 (\(\gamma=0.99\)). The values for the evaluation and behavior policies were computed by running each for \(300\) rollout trajectories, which was more than a sufficient amount for the estimate to converge, and averaging the discounted return (note that Chang et al. (2022) use \(200\) rollout trajectories).

D4RL DatasetsDue to known discrepancy issues between newer environments of gym7, we generate our datasets instead of using the publicly available ones. To generate the datasets, we use the publicly available policies 8. For each domain, the expert and evaluation policy was the \(10\)th (last policy) from training. The medium and behavior policy was the \(5\)th policy. We added a noise of \(0.1\) to the policies.

Footnote 7: https://github.com/Farama-Foundation/D4RL/tree/master

Footnote 8: https://github.com/google-research/deep_ope

#### d.2.2 FQE Training Iteration Curves for D4RL Datasets

In this section, we include the remaining fqe training iteration curves (ope error vs. gradient steps) for the d4rl dataset (Figure 6). We can see that fqe diverges in multiple settings while rope is very stable. While fqe-clip does not diverge, it is still highly unstable.

#### d.2.3 Ablation: ROPE Hyperparameter Sensitivity

Similar to the results in Section 4.2.3, we show rope's hyperparameter sensitivity on all the custom and d4rl datasets. In general, we find that rope is robust to hyperparameter tuning, and it produces more data-efficient ope estimates than fqe for a wide variety of its hyperparameters. See Figures 7 to 10.

\begin{table}
\begin{tabular}{l|l|l|} HumanoidStandup & \(\rho(\pi_{\mathrm{e}})\) & \(\rho(\pi_{b})\) \\ Swimmer & \(43\) & \(31\) \\ HalfCheetah & \(544\) & \(308\) \\ \end{tabular}
\end{table}
Table 2: Policy values of the evaluation policy and behavior policy.

Note that in the bar graphs, we limit the vertical axis to \(1\). In the Hopper and Walker d4rl experiments, fqe diverged and had an error significantly larger than \(1\).

#### d.2.4 Ablation: RMAE Distributions

In this section, show the remaining rmae distribution curves [1] of each algorithm on all datasets. We reach the similar conclusion that on very difficult datasets, rope significantly mitigates the divergence of fqe and that to avoid fqe divergence it is necessary to clip the bootstrapping target. See Figures 11 to 14.

#### d.2.5 Training Loss Curves for ROPE and FQE

In this section, we include the training loss curves for rope's training, fqe's training using rope representations as input, and normal fqe and fqe-clip. The training curves are a function of the algorithms hyperparameters (learning rate for fqe, \(\beta\) and representation output dimension for rope). We can see that on difficult datasets, the loss of fqe diverges. On the other hand, with rope, fqe's divergence is significantly mitigated. Note that rope does not eliminate the divergence. See Figures 15 to 18.

Figure 6: rmae vs. training iterations of fqe on the d4rl datasets. iqm of errors for each domain were computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better.

Figure 7: foe vs. rope when varying rope’s encoder output dimension (top) and \(\beta\) (bottom) on the custom datasets. iqm of errors are computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better.

## 6 Conclusion

Figure 8: foe vs. rope when varying rope’s encoder output dimension (top) and \(\beta\) (bottom) on the d4rl datasets. iqm of errors are computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better.

## 6 Conclusion

Figure 9: foe vs. rope when varying rope’s encoder output dimension (top) and \(\beta\) (bottom) on the d4rl datasets. iqm of errors are computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better.

Figure 10: fqe vs. rope when varying rope’s encoder output dimension (top) and \(\beta\) (bottom) on the d4rl datasets. lqm of errors are computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better.

Figure 11: rmae distributions across all runs and hyperparameters for each algorithm, resulting in \(\geq 20\) runs for each algorithm. Shaded region is \(95\%\) confidence interval. Larger area under the curve is better.

Figure 14: rmae distributions across all runs and hyperparameters for each algorithm, resulting in \(\geq 20\) runs for each algorithm. Shaded region is \(95\%\) confidence interval. Larger area under the curve is better.

Figure 12: rmae distributions across all runs and hyperparameters for each algorithm, resulting in \(\geq 20\) runs for each algorithm. Shaded region is \(95\%\) confidence interval. Larger area under the curve is better.

Figure 13: rmae distributions across all runs and hyperparameters for each algorithm, resulting in \(\geq 20\) runs for each algorithm. Shaded region is \(95\%\) confidence interval. Larger area under the curve is better.

#### d.2.6 Understanding the ROPE Representations

In this section, we try to understand the nature of the rope representations. We do so by plotting the mean of the: 1) mean feature dimension and 2) standard deviation feature dimension. For example, if there \(N\) state-action pairs, each with dimension \(D\), we compute the mean and standard deviation feature dimension for each of the \(D\) dimensions across the \(N\) examples, and then compute the mean along the \(D\) dimensions. If the standard deviation value is close \(0\), it indicates that there may be a representation collapse. See Figure 19.

### Hardware For Experiments

For all experiments, we used the following compute infrastructure:

* Distributed cluster on HTCondor framework
* Intel(R) Xeon(R) CPU E5-2470 0 @ 2.30GHz
* RAM: 7GB

Figure 15: fQE training loss vs. training iterations on the d4rl datasets. IQM of errors for each domain were computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better. Vertical axis is log-scaled.

* Disk space: 4GB

Figure 16: fpe training loss vs. training iterations on the d4rl datasets. IgM of errors for each domain were computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better. Vertical axis is log-scaled.

Figure 17: fge training loss vs. training iterations on the d4rl datasets. iqm of errors for each domain were computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better. Vertical axis is log-scaled.

Figure 18: rope training loss vs. training iterations on the d4rl datasets. iqm of errors for each domain were computed over \(20\) trials with \(95\%\) confidence intervals. Lower is better.

Figure 19: Mean of feature dimension stats vs. training iterations on the d4rl Hopper dataset. iQM of errors for each domain were computed over \(20\) trials with \(95\%\) confidence intervals.