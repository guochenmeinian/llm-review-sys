# Exploring and Addressing Reward Confusion in Offline Preference Learning

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Spurious correlations in a reward model's training data can prevent Reinforcement Learning from Human Feedback (RLHF) from identifying the desired goal and induce unwanted behaviors. In this work, we study the reward confusion problem in offline RLHF where spurious correlations exist in data. We create a lightweight benchmark to study this problem and propose a method that can reduce reward confusion by leveraging model uncertainty and the transitivity of preferences with active learning.

## 1 Introduction

For many real-world tasks, designing adequate reward functions is challenging, which has led to the rise of Reinforcement Learning from Human Feedback (RLHF) [6]. In this work, we study a failure mode of offline RLHF that we refer to as _reward confusion_. This occurs when the reward \(R\) in a Markov Decision Process (MDP) is a function of features \(z_{1},\ldots,z_{n}\) inferred from the observation-action pair \((o,a)\). In a simplified scenario, \(R\) relies on \(z_{1}\) but not \(z_{2}\), yet \(z_{1}\) and \(z_{2}\) are highly correlated in the training data. An empirical risk minimizer might mistakenly conclude that \(z_{2}\) affects \(R\). As we'll see, this incorrect dependence can lead to failures when training a policy against the learned reward function. We graphically illustrate this problem in Figure 1.

To better understand this phenomenon, we created a benchmark environment called _Confusing Minigrid (CMG)_ that tests reward confusion in models. We carefully designed six tasks with three types of spurious information for the minigrid environment, which we introduce in detail in Appendix A. We will open source the benchmark's code soon.

Besides the CMG benchmark, one other our major contributions is an algorithm named Information-Guided Preference Chain (IMPEC) designed to address the reward confusion problem. It involves two stages of training: First, we use information gain as the acquisition function to select comparison rollouts that reduce uncertainty about the reward function. Second, we form a complete preference ordering over the set of selected rollouts, rather than just a partial ordering as in traditional RLHF.

Our experiments show that these techniques together improve sample efficiency while reducing reward confusion. We show in Section 4 that using the same comparison budget, IMPEC can

Figure 1: Illustration of a simplified MDP (left) and reward confusion (right). The reward \(R\) is a function of the feature \(z_{1}\), but not \(z_{2}\). Spurious correlation between \(z_{1}\) and \(z_{2}\) can cause a network to wrongly model \(R\) as a function of \(z_{2}\).

outperform many other active preference learning baselines. To the best of our knowledge, it is the first algorithm that attempts to solve the reward confusion problem in preference learning.

## 2 Related Work

Causal ConfusionThe problem of _causal confusion_, which refers to models learning to depend on spurious correlations in the training data, has been studied in behavioral cloning [7], reinforcement learning [12], and reward learning [20]. Past work shows empirically and theoretically that spurious correlations and confounders in the training set can worsen an agent's deployment performance [22; 9]. Reward confusion is essentially causal confusion that occurs during reward learning.

Goal MisgeneralizationWhile past work on causal confusion studies it as a cause of complete failure to learn goal-directed behavior, it can also make agents optimize for incorrect goals, i.e. goal misgeneralization. For example, in Procgen's CoinRun, the coin to be picked up is always on the right. RL agents can confuse "running to the right" with the real goal of "getting the coin" [10]. Generally, a model's behavior can be consistent with a goal, but it may not be the test-time goal [16].

Preference LearningLearning reward models from preference labels [6] have gained traction due to their low cost compared to expert demonstrations [23] or language inputs [21]. We have also seen progress in other tasks of "reward engineering", e.g. reward hacking [18].

## 3 Method

ModelsWe consider an agent in an environment following the Markov Decision Process (MDP) defined by \((\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R})\). \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, \(p:\mathcal{S}\times\mathcal{S}\times\mathcal{A}\rightarrow[0,\infty)\) is the transition probability density. A rollout \(\xi=(s_{t},a_{t})\) is a sequence of states and actions. Given unranked rollouts \(\Xi\), our algorithm actively collects ranking information to sort them into an ordered list \(T=\langle\xi_{1},\xi_{2},...,\xi_{n}\rangle\). The rank of rollout \(\xi\in T\) is denoted by \(\psi_{\xi}^{T}\). On each transition, the environment emits a reward \(\mathcal{R}:\mathcal{S}\times\mathcal{A}\rightarrow\mathcal{R}\). Our goal is to obtain \(\mathcal{R}^{*}\) that induces correct policies.

PreferencesWe model the human's probability of preferring \(\xi_{1}\) in a pair \((\xi_{1},\xi_{2})\) through the Shepard-Luce choice rule [17; 13]: \(P[\xi_{1}\succ\xi_{2}]=\frac{\exp\sum_{t}r(o_{t}^{*},a_{t}^{*})}{\exp\sum_{t}r (o_{t}^{*},a_{t}^{*})+\exp\sum_{t}r(o_{t}^{*},a_{t}^{*})}\). We extend this model to a ternary one by allowing the human to flag when two rollouts are equally good, \(\xi_{1}\equiv\xi_{2}\). We use cross-entropy loss to improve reward model's predictions for human's true preference.

### Information-Guided Preference Chain (IMPEC)

Key intuition: Increase Contrast Among Valuable Rollouts.In most preference comparison algorithms, a rollout \(\xi_{1}\)'s relation is considered explicitly only with another one \(\xi_{2}\). Suppose that in the ground truth, \(\xi_{1}\succ\xi_{2}\succ\xi_{3}\succ\xi_{4}\), and we already know \(\xi_{1}\succ\xi_{2}\), \(\xi_{3}\succ\xi_{4}\). To figure out \(\xi_{1}\) and \(\xi_{2}\)'s relationship with \(\xi_{3}\) and \(\xi_{4}\), the most efficient query is whether \(\xi_{2}\succ\xi_{3}\). Once we establish that, we can immediately obtain the preference relations on all four rollouts.

Creating and Maintaining a Preference ChainWe maintain an ordered chain for rollouts. Starting from an empty chain, for each new rollout we queried from the dataset, we imitate insertion sort by recursively finding the ranking of it using human's preference labels. Hence, by the time we observe

Figure 2: The IMPEC algorithm creates a sorted preference chain of \(n\) buckets, each containing one or more rollouts with equal returns.

all the rollouts, we have a sorted list of rollouts, ordered according to human preferences. Rollouts can have identical returns, so we treat each element of the chain as a _bucket_\(b\in\mathcal{B}\) of rollouts with the same return. If the human decides that a new rollout \(\xi_{\text{new}}\) is equally preferred to \(\xi_{m}\) in bucket \(b_{m}\), then \(\xi_{\text{new}}\) will be added to \(b_{m}\). On the other hand, if \(\xi_{\text{new}}\succ\xi_{m}\) and \(\xi_{\text{new}}\prec\xi_{m-1}\) (\(\xi_{m-1}\) resides in a previous bucket \(b_{m-1}\)), then the algorithm will insert a new bucket containing only \(\xi_{\text{new}}\) in between \(b_{m}\) and \(b_{m-1}\). This ensures that \(b_{0}\) contains the best rollouts seen so far and \(b_{n}\) contains the least preferred rollouts (where \(n\) is the chain length). We illustrate this process in Figure 2.

Our reward model is a Bayesian neural network (BNN) [3] which maintains a Gaussian distribution over a network's weights and biases. As we will see, this allows us to incorporate epistemic uncertainty over reward functions into the active selection procedure. In the noiseless case, insertion sort needs \(O(\log n)\) queries to find the position for \(\xi_{\text{new}}\). However, we have access to a partially trained reward network, which we use to guess the rank for \(\xi_{\text{new}}\), reducing the number of buckets we must search over. We include more design details in Appendix B for the design of the fast query.

Information GainGiven an existing chain of rollouts, we use information gain as the acquisition function to decide which rollouts to compare next, so we reduce the most uncertainty over network weights. The information gain over network weights \(\theta\) by selecting a rollout \(\xi\in\Xi\) for ranking is

\[I\left(\theta;\psi_{\xi}^{T}\mid T,\xi\right)=H(\theta\mid T,\xi)-H\left(\theta \mid\psi_{\xi}^{T},T,\xi\right) \tag{1}\]

where \(\psi_{\xi}^{T}\) is the rollout's ranking on chain \(T\). Intuitively, it measures how much we expect to reduce uncertainty about the weights after observing the ranking \(\psi_{\xi}^{T}\) of rollout \(\xi\). As shown in Appendix C, Equation 1 (information gain) can be approximated by drawing \(M\) weight samples, \(\theta_{1},\theta_{2},\ldots,\theta_{M}\sim\theta\), from the posterior through

\[\frac{1}{M}\sum_{i=1}^{M}\sum_{\psi}P\left(\psi_{\xi}^{T}\mid T,\theta_{i},\xi \right)\cdot\log\left(\frac{M\cdot P\left(\psi_{\xi}^{T}\mid T,\theta_{i},\xi \right)}{\sum_{\theta_{j}}P\left(\psi_{\xi}^{T}\mid T,\theta_{j},\xi\right)}\right) \tag{2}\]

\(P\left(\psi_{\xi}^{T}\mid T,\theta,\xi\right)\) is a complicated distribution, and so we (loosely) approximate it with Equation 3. Intuitively, it is proportional to the probability that \(\xi_{i}\succ\xi\succ\xi_{i+1}\).

\[P\left(\psi_{\xi}^{T}=i\mid T,\theta,\xi\right)\propto P\left(\xi_{i}\succ\xi \mid\theta\right)\cdot P\left(\xi\succ\xi_{i+1}\mid\theta\right) \tag{3}\]

We summarize the complete process in Algorithm 1. The network is first supervised trained on the preference dataset \(D\) using cross entropy loss with \(P\left[\xi_{1}\succ\xi_{2}\right]\) modeled through the Shepard-Luce choice rule. With the limited query budget for human preferences, we first find out the rollout \(\xi\) whose ranking \(\psi_{\xi}^{T}\) on chain \(T\) will provide the most information gain over the model weights \(\theta\). Then we use insertion sort to find out \(\xi\)'s real ranking \(\psi_{\xi}^{T*}\) in the chain. We add the rollout \(\xi\) onto the appropriate position of the chain \(T\), then based on its position, derive preference labels with all other rollouts on the chain. We repeat this process until the network weight has converged.

```
0: Preference dataset D, network \(\theta\), query budget Q \(T\leftarrow[]\) while not converged do \(\theta\leftarrow\) SupervisedTrain\((\theta,D)\) ifbudget not reached then \(\xi\leftarrow\arg\max_{\xi}I(\theta;\psi_{\xi}^{T}\mid T,\xi)\) \(\psi_{\xi}^{T*}\leftarrow\) InsertionSort\((\xi,T,\theta)\) \(T\gets T\cup\xi\) \(D\gets D\cup\) DerivePreferences\((\xi,T,\psi_{\xi}^{T})\) endif \(i\gets i+1\) endwhile
```

**Algorithm 1** The IMPEC Algorithm

## 4 Experiments

Experiment SettingsWe compare our method with the standard RLHF algorithm, and two other RLHF with active learning methods: pairwise information gain [2] and pairwise volume removal [14]. The information gain (IG) method is similar to ours but reasons only about individual preference pairs and not about the result of the ranking process. The volume removal method was designed in the linear reward setting to reduce the volume of weight vectors supported under the posterior after each preference update. We conduct experiments on 6 CMG tasks. Detailed information on each task and their added spurious correlations can be found in Appendix A.

We first perform offline reward learning, and then apply online reinforcement learning using the learned reward function to obtain a policy. The RL agent receives rewards from the learned function instead of the environment, and is trained with Proximal Policy Optimization [15]. More detailed experiment and hyperparameter settings can be found in Appendices D and E.

**Main Results** Performance on all six tasks can be found in Table 1, with each run repeated over five seeds. We further compute the p-values of the results being better than baseline performance, with complete results in Appendix H. Except for the task Go To Door where all algorithms perform poorly, IMPEC has a higher mean return than other algorithms, and often a lower standard deviation. Although IMPEC achieves a higher mean than the other methods on most tasks, its p-values are \(\leq\) 0.1 (per appendix H). This provides some (albeit not particularly strong) statistical evidence that IMPEC has a better mean performance distribution from the baseline. In contrast, the volume removal and information gain methods are often statistically indistinguishable from the baseline.

The decisive factor for each algorithm's performance is how often they fail: Out of the 5 seeds, how often does the reward function learn to optimize for the spurious goal? All methods but IMPEC have fairly high probability of taking the spurious feature as the "correct" feature, and hence rewarding incorrect behaviors and obtaining low ground truth returns. We plot the learning curves for each algorithm in the Lava task, where the baseline curve has the largest standard deviations. We observe similar phenomena in other tasks. We include an ablation study in Appendix F. The complete learning curves are included in Appendix J.

**Limitations and Future Work** A limitation of IMPEC is its potential sensitivity to noise in preferences. In our experiments, we keep a relatively low noise level, and we believe more sophisticated algorithms could improve robustness to noise, perhaps inspired by past work on noisy binary search [8, 5]. Our results suggest a deeper connection between the quality of preference datasets and the efficiency of preference learning algorithms. In appendix G, we show some first steps of a graph theoretic analysis for reasoning about preference dataset quality. We are interested in further exploring the influence of graph-theoretic qualities and their effects on preference learning, and using the insights in future algorithm design.

## 5 Conclusion

This work studies the reward confusion problem. Our experiments on the Confusing Minigrid benchmark show that reward confusion in offline preference learning can lead to undesired policy behaviors. The benchmark is easy to configure, and we expect it to be particularly useful for iterative research. In addition, we proposed IMPEC to reduce the impact of reward confusion. It exploits preference transitivity and obtains decent empirical performance on tasks with different sources of reward confusion. We believe that the findings of our work will be helpful for making AI more aligned with human values.

## References

* Bellemare et al. [2013] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. _Journal of Artificial Intelligence Research_, 47:253-279, 2013.
* Biyik et al. [2019] Erdem Biyik, Malayandi Palan, Nicholas C Landolfi, Dylan P Losey, and Dorsa Sadigh. Asking easy questions: A user-friendly approach to active reward learning. _arXiv preprint arXiv:1910.04365_, 2019.
* Blundell et al. [2015] Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. In _International conference on machine learning_, pages 1613-1622. PMLR, 2015.
* Chevalier-Boisvert et al. [2018] Maxime Chevalier-Boisvert, Lucas Willems, and Suman Pal. Minimalistic gridworld environment for gymnasium, 2018.
* Chiu [2019] Sung-En Chiu. _Noisy binary search: Practical algorithms and applications_. University of California, San Diego, 2019.
* Christiano et al. [2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.
* De Haan et al. [2019] Pim De Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. _Advances in Neural Information Processing Systems_, 32, 2019.
* Karp and Kleinberg [2007] Richard M Karp and Robert Kleinberg. Noisy binary search and its applications. In _Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms_, pages 881-890. Citeseer, 2007.
* Kumor et al. [2021] Daniel Kumor, Junzhe Zhang, and Elias Bareinboim. Sequential causal imitation learning with unobserved confounders. _Advances in Neural Information Processing Systems_, 34:14669-14680, 2021.
* Langosco et al. [2022] Lauro Langosco, Jack Koch, Lee D Sharkey, Jacob Pfau, and David Krueger. Goal misgeneralization in deep reinforcement learning. In _International Conference on Machine Learning_, pages 12004-12019. PMLR, 2022.
* Latora and Marchiori [2001] Vito Latora and Massimo Marchiori. Efficient behavior of small-world networks. _Physical review letters_, 87(19):198701, 2001.
* Li et al. [2020] Minne Li, Mengyue Yang, Furui Liu, Xu Chen, Zhitang Chen, and Jun Wang. Causal world models by unsupervised deconfounding of physical dynamics. _arXiv preprint arXiv:2012.14228_, 2020.
* Luce [1959] R Duncan Luce. _Individual choice behavior: A theoretical analysis_. John Wiley & Sons, Inc., 1959.
* Sadigh et al. [2017] Dorsa Sadigh, Anca D Dragan, Shankar Sastry, and Sanjit A Seshia. _Active preference-based learning of reward functions_. 2017.
* Schulman et al. [2017] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* Shah et al. [2022] Rohin Shah, Vikrant Varma, Ramana Kumar, Mary Phuong, Victoria Krakovna, Jonathan Uesato, and Zac Kenton. Goal misgeneralization: Why correct specifications aren't enough for correct goals. _arXiv preprint arXiv:2210.01790_, 2022.
* Shepard [1957] Roger N Shepard. Stimulus and response generalization: A stochastic model relating generalization to distance in psychological space. _Psychometrika_, 22(4):325-345, 1957.
* Skalse et al. [2022] Jorar Skalse, Nikolaus HR Howe, Dmitrii Krasheninnikov, and David Krueger. Defining and characterizing reward hacking. _arXiv preprint arXiv:2209.13085_, 2022.

* [19] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. _arXiv preprint arXiv:1801.00690_, 2018.
* [20] Jeremy Tien, Jerry Zhi-Yang He, Zackory Erickson, Anca D Dragan, and Daniel Brown. A study of causal confusion in preference-based reward learning. _arXiv preprint arXiv:2204.06601_, 2022.
* [21] Hsiao-Yu Tung, Adam W Harley, Liang-Kang Huang, and Katerina Fragkiadaki. Reward learning from narrated demonstrations. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 7004-7013, 2018.
* [22] Junzhe Zhang, Daniel Kumor, and Elias Bareinboim. Causal imitation learning with unobserved confounders. _Advances in neural information processing systems_, 33:12263-12274, 2020.
* [23] Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. In _Aaai_, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.

## Appendix A Confusing Minigrid Definition

Reward learning methods are often evaluated on benchmarks originally developed for reinforcement learning algorithms, like Atari [1] or MuJoCo continuous control [19]. These reinforcement learning benchmarks cannot easily be used to test reward confusion failures. Thus we created six new tasks based on Minigrid [4], which together form the Confusing Minigrid benchmark.

### Spurious Correlations from Extra Observations

This set of tasks tests whether spurious correlations with redundant observation dimensions can interfere with learning. In these tasks, an agent needs to navigate to the goal cell. It can observe the state of a glass of water it is holding, which exhibits "ripples" as it moves, and calms down if the agent's position remains unchanged. On rollouts where the agent moves straight to the goal and stops, the level of ripples will be predictive of whether the agent has reached the goal, even though in general it is possible to cause the ripples to disappear by stopping in any location and not just at the goal. The training and testing variants of these tasks are the same.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Task name & Extra & Position & Color & DistShift \\ \hline Empty & Y & N* & N* & N \\ Dynamic Obs. & Y & N* & N* & N \\ Lava & Y & N* & N* & N \\ Lava-Position & N* & Y & N* & Y \\ Go to door & N* & Y & N* & Y \\ Fetch & N* & N* & Y & Y \\ \hline \hline \end{tabular}
\end{table}
Table 2: A summary of Confusing Minigrid tasks. Y/N is short for yes/no. An asterisk means that a feature is off by default but can be optionally turned on. The “extra” column refers for having extra observation dimensions for the task. The position and color columns refer to spurious correlations that come from position and color information, respectively. DistShift means there is a difference between the training and testing environments.

Figure 4: The six Confusing Minigrid tasks. The tasks require agents to move to goal positions, go to doors, or fetch objects.

EmptyThis is the simplest task in the benchmark. The agent needs to navigate to the goal cell, with all other cells being empty. The environment gives a positive reward when the agent reaches the goal, and zero otherwise.

Dynamic ObstaclesThis task augments the Empty task with obstacles that show up randomly in non-goal cells at each time step, which block the agent's path. We include this task because the obstacles may stop the agent at a random grid while it is collecting rollouts, which increases the chance that the reward learning algorithm later realizes that "stabilizing the water" is not the correct goal.

LavaThe environment contains a row of lava cells with a gap that allows the agent to cross and reach the goal. Standing on the lava cells gives a reward of -1. We use this task to observe if negative rewards will have any impacts on learning spurious correlations, and if the correlation is exploited, whether the agent will at least avoid the lava (low reward) area.

### Spurious Correlations from Distributional Shifts

We design these tasks with different training and testing variants. The training environments have a 90% probability where the goal configuration is spurious.

Lava-PositionIt is a variant of Lava with changing goal positions. In the training variant, the goal cell is usually located at one particular location. In the test variant, the goal grid can appear at other locations too.

Go to DoorIn this task, an agent is asked to move to a position adjacent to the goal door embedded in one of the four walls surrounding the grid. There are always four doors in the environment, and the goal door is most likely to be placed in the upper wall during training. During testing, the goal door can be placed in any of the four walls.

FetchThe agent's goal in this task is to pick up a key. There is usually a distractor object in the environment that an agent can also pick up. In the training variant of this task, most keys are yellow, and most distractor objects are non-yellow. At the test time, the keys and distractor objects can appear in any color with equal chance.

For all these tasks, the agent receives a reward of \(+1\) when the goal condition is satisfied. We summarize the task settings in Table 2. Changing the confounding type in Confusing Minigrid is as easy as modifying a keyword argument when initializing the environment. Training a standard preference learning algorithm on the tasks with the simplest observation type takes around 2.5 hours on a single Nvidia A6000 GPU, which is faster than many other image-based environments or complex control tasks.

## Appendix B Use a Partially Trained Model to Reduce Queries

The network will first update its reward predictions on rollouts in each bucket \(\hat{R}_{b_{i}}\), then predict the incoming rollout's reward \(\hat{R}_{\xi_{\text{new}}}\). Instead of using a point estimate \(\hat{R}_{\xi_{\text{new}}}\), we use an interval \([\hat{R}_{\xi_{\text{new}}}-\epsilon,\hat{R}_{\xi_{\text{new}}}+\epsilon]\) to fast-guess where \(\xi_{\text{new}}\) may belong. IMPEC queries human preferences of two pairs \((\xi_{\text{new}},\xi_{l})\) and \((\xi_{\text{new}},\xi_{u})\). \(\xi_{l}\) and \(\xi_{u}\) are the rollouts that are closest to the prediction lower bound \(\hat{R}_{\xi_{\text{new}}}-\epsilon\) and the upper bound \(\hat{R}_{\xi_{\text{new}}}+\epsilon\), respectively. The \(\epsilon\) we use is the standard deviation of \(\hat{R}_{\xi_{\text{new}}}\) by \(M\) samples of the BNN. After \(\xi_{\text{new}}\) is added into the chain, we can derive preference relations between it and other rollouts by transitivity.

The Information Gain Objective Derivation

This derivation is adapted from [2].

\[I(\theta;\psi_{\xi}^{T}\mid T,\xi) =H(\theta|T,\xi)-H(\theta|\psi_{\xi}^{T},T,\xi)\] \[=-\mathbb{E}_{\theta,T,\xi}\left[\log P(\theta|T,\xi)\right]+ \mathbb{E}_{\theta,\psi_{\xi}^{T},\xi,T}\left[\log P(\theta|\psi_{\xi}^{T},T, \xi)\right]\] \[=-\mathbb{E}_{\theta,\psi_{\xi}^{T},\xi,T}\left[\log P(\theta|T, \xi)\right]+\mathbb{E}_{\theta,\psi_{\xi}^{T},\xi,T}\left[\log P(\theta|\psi_{ \xi}^{T},T,\xi)\right]\] \[=\mathbb{E}_{\theta,\psi_{\xi}^{T},\xi,T}\left[\log P(\theta|\psi _{\xi}^{T},T,\xi)-\log P(\theta|T,\xi)\right]\] \[=\mathbb{E}_{\theta,\psi_{\xi}^{T},\xi,T}\left[\log\frac{P(\psi_ {\tau}^{T}|T,\theta,\xi)P(T,\theta,\xi)}{P(\psi_{\xi}^{T},T,\xi)}-\log\frac{P( \theta,T,\xi)}{P(T,\xi)}\right]\] \[=\mathbb{E}_{\theta,\psi_{\xi}^{T},\xi,T}\left[\log\frac{P(\psi_ {\xi}^{T}|T,\theta,\xi)}{P(\psi_{\xi}^{T}|T,\xi)P(T,\xi)}-\log\frac{P(\theta,T, \xi)}{P(T,\xi)}\right]\] \[=\mathbb{E}_{\theta,\psi_{\xi}^{T},\xi,T}\left[\log\frac{P(\psi_ {\xi}^{T}|T,\theta,\xi)}{P(\psi_{\xi}^{T}|T,\xi)}\right]\]

Evaluating this expression requires us to compute a conditional probability with respect to \(\theta\), which is a random variable capturing our current uncertainty over the reward network weights. We can approximate the distribution \(p(\theta)\) by sampling \(M\) weights \(\theta_{1},\theta_{2},\ldots,\theta_{M}\sim p(\theta)\) and then treating \(\theta\) as if it were a uniform mixture over the samples; i.e.

\[p(\theta)\approx\frac{1}{M}\sum_{i=1}^{M}\delta_{\theta=\theta_{i}},\]

where \(\delta_{\theta=\theta_{i}}\) denotes a Dirac distribution at \(\theta_{i}\). Using this approximation, our mutual information becomes:

\[I(\theta;\psi_{\xi}^{T}\mid T,\xi) \approx\mathbb{E}_{\theta,\psi_{\xi}^{T},\xi,T}\left[\log\frac{P (\psi_{\xi}^{T}|T,\theta,\xi)}{\frac{1}{M}\sum_{j=1}^{M}P(\psi_{\xi}^{T}|T, \theta_{j},\xi)}\right]\] \[=\mathbb{E}_{\theta,\psi_{\xi}^{T},\xi,T}\left[\log\frac{M\cdot P (\psi_{\xi}^{T}|T,\theta,\xi)}{\sum_{j=1}^{M}P(\psi_{\xi}^{T}|T,\theta_{j},\xi )}\right]\] \[\approx\frac{1}{M}\sum_{i=1}^{M}\sum_{\psi_{\xi}^{T}}P(\psi_{\xi} ^{T}|T,\theta_{i},\xi)\cdot\log\frac{M\cdot P(\psi_{\xi}^{T}|T,\theta_{i},\xi )}{\sum_{j=1}^{M}P(\psi_{\xi}^{T}|T,\theta_{j},\xi)}\]

## Appendix D Detailed Experiment Settings

Offline Dataset and TasksIn real-world settings where failures are much more costly than minor failures, rollout datasets will be skewed towards higher-return rollouts. We emulate this by constraining the number of rollouts in the low reward region (return \(\leq\) 5) to be at most 10% of the dataset.

Query and Data BudgetsPreference comparison algorithms typically obtain \(n\) pairs of (query, label), \([(\xi_{i1},\xi_{i2}),\text{label}_{i}]_{i=1}^{n}\) for binary classification. For simpler tasks (Empty, DynObs, Lava, and Fetch), we set the query budget to be 300. That is, baselines have access to 300 (query, label) pairs, \([(\xi_{i1},\xi_{i2}),\text{label}_{i}]_{i=1}^{300}\). IMPEC requires additional queries to precisely rank each sampled rollout within the candidate list, so we constrain it to use 150 pairs \([(\xi_{i1},\xi_{i2}),\text{label}_{i}]_{i=1}^{150}\), and use the remaining 150 query budget to perform insertion sort for a selected subset of rollouts (decided by IG). For the harder tasks (Go to Door and Lava-Position), all algorithms are given a budget of 600 (query, label) pairs. IMPEC can access 400 data pairs, and use the remaining 200 query budget for sorting.

Dataset CreationFor each task, we first train an RL agent to the expert level, saving its policies at various timesteps. We then take 3 of its policy snapshots - an almost random policy, an expert policy, and one in-between to generate rollouts. The number of rollouts falling within the low-reward (\(\leq 5\)) region are controlled to take up within 10% of the dataset.

IMPEC TrainingWe typically train an algorithm with 20 epochs. For IMPEC, we evenly divide its query budget from epoch 1 to epoch 15, using up all queries in this period and ranking as much uncertain rollouts as possible. We then use the remaining 5 epochs for learning the full dataset. After sampling the initial preference pairs, IMPEC will not obtain new rollouts from the dataset, and perform learning only by querying humans for ranking the given rollouts.

Environment ObservationsThe environment observations are in the vector form, which contains [agent position, agent direction, special grid info]. The special grid can be the goal/lava/door, etc., and its information is an encoding of its grid type, current position, and color.

Ablation StudiesIn the "no active learning" experiment, we turn off the active selection function, and randomly pick rollouts from the candidate list. For "no derived prefs", we remove the preference derivation part of the learning. Note that the preference pairs generated during the sorting process are still added to the dataset. Finally, "no ranking" means that the algorithm still selects preference pairs with an information gain acquisition function, but does not maintain a preference chain (so there are also no transitively derived preferences). This is simply the information gain algorithm of [2].

## Appendix E Training Hyperparameters

The preference learning hyperparameters:

\begin{tabular}{l r} \hline \hline
**Hyperparameter** & **Value** \\ \hline \multicolumn{2}{c}{All algorithms} \\ \hline Optimizer & Adam \\ Learning Rate & 1e-4 \\ Weight Decay & 3e-5 \\ Batch Size & 32 \\ Temperature & 0.1 \\ Fragment Length & 30 \\ Training Epochs & 20 \\ \hline \multicolumn{2}{c}{IMPEC} \\ \hline Max. Preference Chain Size & 30 \\ Stop querying at & Epoch 15 \\ M & 10 \\ \hline \hline \end{tabular}

The PPO training hyperparameters:

\begin{tabular}{l r} \hline \hline
**Hyperparameter** & **Value** \\ \hline Training steps & 500,000 \\ Learning rate & 0.0017 \\ Gamma & 0.98 \\ Lambda & 0.975 \\ Entropy Coefficient & 0.15 \\ Batch size & 64 \\ Clip range & 0.2 \\ \hline \hline \end{tabular}

Ablation Studies

To understand what leads to IMPEC's performance, we experiment with removing three different components: (1) the active learning process; (2) preference derivations; and (3) the ranking process. The results can be found in Table 3.

We conduct each ablation experiment with 25 seeds and report the number of failed runs for each algorithm. A run fails when its final policy's average return \(\leq 10\).

Our results suggest that there is no one component that is responsible for the entire gap between IMPEC and the baseline. However, the combination of ranking and active learning can be quite powerful: comparing the "no derived prefs" and the baseline, the failure rate immediately dropped by 50%.

## Appendix G Graph-theoretical approach

The Preference DatasetsWe visualize the preference datasets gathered by the baseline and IMPEC on Lava-Position in Figure 5. The baseline dataset is randomly sampled at the start of the training, while we take a snapshot of the IMPEC dataset (which is constructed iteratively) at its last training epoch. The datasets are visualized as graphs, with each node being a unique rollout, and each edge representing a preference label. Both IMPEC and the baseline have a query budget of 600 pairwise queries, and IMPEC uses the first 400 of its queries to do pairwise comparisons between randomly selected rollouts, as opposed to actively selected rollouts. The IMPEC and baseline datasets have 405 and 538 unique rollouts as well as 791 and 594 unique edges, respectively. We include several other graph statistics in Table 4.

IMPEC and the baseline exhibit three notable differences in the graph properties. The first is their clustering coefficient, which measures the degree to which nodes tend to cluster together. The IMPEC graph has a higher clustering coefficient because of the many preferences it derives from the ranked chain. This is relevant to the number of chains in the graph: since most nodes are connected to IMPEC's central cluster through some edges, it creates many more chains between two nodes across the graph. In both graphs, we also observe that there are many chains of length 1 that are not connected to any larger cluster. We suspect that the algorithms' sample efficiency can be further improved if these pairs can be meaningfully linked to the clusters.

Finally, we measure the graph's efficiency, which is a metric from network science that is intended to measure the flow of information between "communicating" nodes [11]. The assumption which underlies the graph efficiency metric is that more distant nodes are less efficient at information exchange. To avoid inflating the metric with the many length 1 that are not connected to the rest

\begin{table}
\begin{tabular}{l r r} \hline \hline  & **IMPEC** & **Baseline** \\ \hline Num. Nodes & 405 & 538 \\ Num. Edges & 791 & 594 \\ Cluster Coef. & 0.0642 & 0.0013 \\ Num. Chains & 416 & 81 \\ Efficiency & 0.19 & 0.11 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Properties of the dataset graph.

Figure 5: The preference datasets visualized as graphs. The IMPEC dataset connects more scattered rollouts through its central cluster.

of the graph, we compute efficiency only on the two graphs' biggest connected components. We empirically observe that the IMPEC dataset graph has a higher average global efficiency than the baseline graph, which means that the average shortest path between vertex pairs in IMPEC is shorter than for the baseline. This raises an interesting question: Is the assumption in network theory, where the distance of nodes influences information efficiency, also applicable to preference learning? We do not have matured results establishing connections between data connectivity in RLHF and the training performance yet, but it would be an interesting next step for research.

## Appendix H The Complete P-Value Table

## Appendix I Baseline Comparison and Data Scaling

We expect that the baseline should suffer less from reward confusion if given more comparison data. To test this hypothesis, fig. 6 shows return for the baseline with different query budgets, along with IMPEC results from our main experiments (which used 150 preference pairs and 300 queries). We tested the baseline with 6\(\times\) the data used by IMPEC (i.e., 900 preference pairs and queries), then gradually pushed up the amount to 1500 (10\(\times\) of IMPEC data). With 6\(\times\) the data, we still see some learning failures, and the seeds' standard deviation decreases to IMPEC's level only when we use 10\(\times\) data.

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & **IMPEC** & **Information Gain** & **Volume Removal** \\ \hline Empty & 0.10 & 0.82 & 0.37 \\ Dynamic Obstacles & 0.09 & 0.66 & 0.77 \\ Lava & 0.08 & 0.26 & 0.44 \\ Lava Position & 0.01 & 0.46 & 0.20 \\ Fetch & 0.26 & 0.65 & 0.45 \\ Go To Door & 0.37 & 0.55 & 0.39 \\ \hline \hline \end{tabular}
\end{table}
Table 5: A complete list of all p-values of the algorithms performing better than the baseline for all tasks

Figure 6: The learning curves for IMPEC and baseline with different amounts of data.

## Appendix J Learning Curves for All Tasks