# On the Complexity of Learning Sparse Functions with Statistical and Gradient Queries

Nirmit Joshi

Toyota Technological Institute at Chicago

nirmit@ttic.edu

&Theodor Misiakiewicz

Toyota Technological Institute at Chicago

theodor.misiakiewicz@ttic.edu

&Nathan Srebro

Toyota Technological Institute at Chicago

nati@ttic.edu

###### Abstract

The goal of this paper is to investigate the complexity of gradient algorithms when learning sparse functions (juntas). We introduce a type of Statistical Queries (SQ), which we call Differentiable Learning Queries (DLQ), to model gradient queries on a specified loss with respect to an arbitrary model. We provide a tight characterization of the query complexity of DLQ for learning the support of a sparse function over generic product distributions. This complexity crucially depends on the loss function. For the squared loss, DLQ matches the complexity of Correlation Statistical Queries (CSQ)--potentially much worse than SQ. But for other simple loss functions, including the \(\ell_{1}\) loss, DLQ always achieves the same complexity as SQ. We also provide evidence that DLQ can indeed capture learning with (stochastic) gradient descent by showing it correctly describes the complexity of learning with a two-layer neural network in the mean field regime and linear scaling.

## 1 Introduction

In recent years, major efforts have been devoted to understanding which distributions can be learned efficiently using gradient-type algorithms on generic models (Abbe and Sandon, 2020; Allen-Zhu and Li, 2020; Malach et al., 2021; Damian et al., 2022; Abbe et al., 2021b, a, 2022, 2023; Bietti et al., 2023; Glasgow, 2023; Dandi et al., 2023, 2024; Edelman et al., 2024; Kou et al., 2024). In this paper, we focus on learning sparse functions (i.e. "juntas" (Blum and Langley, 1997)), that is functions that depend only on a small number \(P\) out of a much larger set \(d\gg P\) of input coordinates. The challenge in this setting is to identify the few relevant coordinates. For some sparse functions, such as noisy parities, learning is believed to require \(O(d^{P})\) runtime (Kearns, 1998), while others, such as linear functions, are easy to learn in \(\tilde{O}(d)\) time. Which functions are easy to learn and which are hard? What is the complexity of learning a specific sparse function? Recent works (Abbe et al., 2022, 2023) unveiled a rich "leap" hierarchical structure and saddle-to-saddle dynamics that drives learning in this setting. The goal of the present paper is to provide a general characterization for the complexity of learning sparse functions that go beyond _(i)_ hypercube data and Fourier analysis, and _(ii)_ CSQ (see below) and focusing only on the squared loss.

The notion of complexity we consider is the _Statistical Query_ (SQ) complexity, which studies learning by measuring expectations up to some worst-case tolerance (see (Kearns, 1998; Bshouty and Feldman, 2002; Reyzin, 2020) and Section 3). Although based on worst-case error (or almost equivalently, additive independent noise) rather than the sampling error encountered in practice, statistical query complexity has been proven to be a useful guideline for studying the complexity of learning. Inparticular, gradient computations are a special case of statistical queries. In specific cases which include binary functions or gradients on the squared or cross-entropy loss, gradient queries are equivalent1 to the restricted class of _Correlation Statistical Queries_ (CSQ) which are strictly less powerful than general statistical queries. Lower bounds on the CSQ complexity have thus been seen as corresponding to the complexity of gradient-based learning2 in these restricted cases. Part of the motivation for this paper is to emphasize that this relationship is limited to very specific loss functions and does not hold more generally. In order to study the complexity of gradient algorithms for general output and loss functions, we introduce a type of statistical query which we call _Differentiable Learning Query_ (DLQ). These queries are defined with respect to a specific loss \(\ell\)--denoted by DLQ\({}_{\ell}\)--and are given by gradients on a loss \(\ell\) with respect to an arbitrary model. Specifically, DLQ\({}_{\ell}\) algorithms correspond to SQ algorithms with queries of the type

Footnote 1: This equivalence holds if the input distribution is known, which is the case we consider here.

Footnote 2: In Section 8, we discuss recent work Dandi et al. (2024) that showed that even with the squared loss, multiple gradient evaluations on the same batch can be strictly more powerful than CSQ.

\[\phi(y,\bm{x})=\frac{\partial}{\partial\omega}\ell(f(\bm{x},\omega),y)\Big{|} _{\omega=0},\quad\text{where}\quad f:\mathcal{X}^{d}\times\mathbb{R}\to\mathbb{R}.\] (1)

Depending on the loss \(\ell\) and target distributions, learning with DLQ\({}_{\ell}\) can be less, equal, or more powerful than CSQ.

For inputs on the hypercube \(\bm{x}\sim\mathrm{Unif}(\{\pm 1\}^{d})\) and CSQ, Abbe et al. (2022, 2023) showed that the complexity of learning sparse functions is sharply captured by a _leap exponent_ defined in terms of the non-zero Fourier coefficients of the sparse function. Informally, it states that \(\Theta(d^{k_{*}})\) queries are necessary and sufficient to learn with CSQ, where \(k_{*}\) is the minimum number of coordinates one need to add at once to "leap" between non-zero Fourier coefficients. E.g., consider the following two sparse functions:

\[y_{1} =x_{s_{*}(1)}+x_{s_{*}(1)}x_{s_{*}(2)}+x_{s_{*}(1)}x_{s_{*}(2)}x_ {s_{*}(3)}+x_{s_{*}(1)}x_{s_{*}(2)}x_{s_{*}(3)}x_{s_{*}(4)},\] (2) \[y_{2} =x_{s_{*}(1)}x_{s_{*}(2)}x_{s_{*}(3)}+x_{s_{*}(1)}x_{s_{*}(2)}x_ {s_{*}(4)}+x_{s_{*}(1)}x_{s_{*}(3)}x_{s_{*}(4)}+x_{s_{*}(2)}x_{s_{*}(3)}x_{s_{ *}(4)}.\]

Both functions depend on an (unknown) subset of \(P=4\) coordinates \(\{s_{*}(1),s_{*}(2),s_{*}(3),s_{*}(4)\}\). For \(y_{1}\), the monomials are ordered such that we add only one new coordinate to the support at a time: the leap exponent is \(1\) and \(\Theta(d)\) queries are sufficient to learn \(y_{1}\). For \(y_{2}\), we need to add three new coordinates at once: the leap exponent is \(3\) and \(\Theta(d^{3})\) queries are required to learn \(y_{2}\).

We extend and generalize this charactarization in several significant ways:

* We go beyond binary input space and allow for arbitrary measurable space with product distribution. In particular, the leap complexity arises from the structure of the permutation group rather than a specific functional basis.
* We go beyond CSQ and squared loss, and tightly characterize the query complexity for learning sparse functions with SQ and DLQ\({}_{\ell}\) algorithms with any loss \(\ell\). This complexity is in terms of a leap exponent defined analogously as above, but now over a set system \(\mathcal{C}_{\mathsf{A}}\subseteq 2^{|P|}\) of _"detectable"_ subsets \(S\subseteq[P]\), where "detectability" depends on the type of queries \(\mathsf{A}\in\{\textsf{SQ},\textsf{CSQ},\textsf{DLQ}_{\ell}\}\). Learning with different loss functions are not necessarily comparable to CSQ or SQ and depend on the sparse function. However, we show that some losses, such as \(\ell_{1}\) or exponential loss, are "generic", i.e. \(\mathcal{C}_{\textsf{DLQ}_{\ell}}=\mathcal{C}_{\textsf{SQ}}\) for any sparse function, and always match the SQ-leap exponent. This shows that differentiable learning with these losses is as powerful as learning with SQ for juntas.
* Finally, we introduce a _cover exponent_--defined on the same system of detectable sets \(\mathcal{C}_{\mathsf{A}}\)--which captures learning with \(\mathsf{A}\in\{\textsf{SQ},\textsf{CSQ},\textsf{DLQ}_{\ell}\}\) when the queries are chosen _non-adaptively_, i.e., without adapting to the responses of previous queries. This can be roughly thought of as learning with a single gradient step, versus many consecutive steps in the adaptive case. The contrast between the two helps understand to what extent learning relies on adaptivity and can help frame other related results--see discussion in Section 8.

We summarize the complexity of learning a sparse function with SQ, CSQ, DLQ\({}_{\ell}\) and adaptive/non-adaptive queries in Table 1. To see some of these notions play out, consider again the two examples in Eq. (2):

\[y_{1}: \qquad\mathsf{Leap}(\mathcal{C}_{\textsf{CSQ}})=1,\quad\textsf{ Cover}(\mathcal{C}_{\textsf{CSQ}})=4, \qquad\qquad\mathsf{Leap}(\mathcal{C}_{\textsf{SQ}})=\textsf{ Cover}(\mathcal{C}_{\textsf{SQ}})=1,\] \[y_{2}: \qquad\mathsf{Leap}(\mathcal{C}_{\textsf{CSQ}})=\textsf{ Cover}(\mathcal{C}_{\textsf{CSQ}})=3,\qquad\qquad\mathsf{Leap}(\mathcal{C}_{\textsf{SQ}})=\textsf{ Cover}(\mathcal{C}_{\textsf{SQ}})=1.\]For \(y_{1}\), \(\mathsf{DLQ}_{\ell}\) will require \(\Theta(d)\) adaptive or \(\Theta(d^{4})\) non-adaptive queries to learn with squared loss--equivalent to \(\mathsf{CSQ}\), while \(\Theta(d)\) adaptive/non-adaptive queries suffice with \(\ell_{1}\)-loss--equivalent to \(\mathsf{SQ}\). For \(y_{2}\), \(\mathsf{DLQ}_{\ell}\) will only require \(\Theta(d)\) queries, either adaptive or non-adaptive, with \(\ell_{1}\)-loss, compared to \(\Theta(d^{3})\) queries with the squared loss. Proposition 6.2.(c) provides an example where \(\mathsf{Leap}(\mathcal{C}_{\mathsf{SQ}})<\mathsf{Leap}(\mathcal{C}_{\mathsf{ DLQ}_{\ell}})<\mathsf{Leap}(\mathcal{C}_{\mathsf{CSQ}})\).

\(\mathsf{DLQ}_{\ell}\) algorithms are quite different than (stochastic) gradient descent algorithms. On one hand, \(\mathsf{DLQ}_{\ell}\) allows evaluating arbitrary gradients rather than following a trajectory, and is thus potentially more powerful than (S)GD on generic models3. On the other hand, the lower bound on \(\mathsf{DLQ}_{\ell}\) algorithms in Table 1 is against _worst-case_ noise--much more pessimistic than sampling noise encountered in SGD or GD-on-the-training-set (Abbe et al., 2021). Nevertheless, \(\mathsf{DLQ}_{\ell}\) does provide useful guidance and we expect that it _does_ capture important aspects of the computational complexity of GD in a number of settings. To demonstrate this, Section 7 considers learning sparse functions on the hypercube using online SGD on a two-layer neural network. We show that in this setting, the \(\mathsf{DLQ}_{\ell}\)-leap exponent correctly captures learnability in the mean-field regime and linear scaling. Namely, for \(\mathsf{DLQ}_{\ell}\)-leap \(1\) functions, SGD on loss \(\ell\) learns in \(O(d)\)-steps, while for leap greater than \(1\), the dynamics remains stuck in a suboptimal saddle subspace and require \(\omega(d)\)-steps to escape.

Footnote 3: Following Abbe et al. (2021), we can construct an “emulation” differentiable model, i.e., a model \(f(\bm{x};\bm{w})\) on which GD emulates \(\mathsf{DLQ}_{\ell}\). However, this offers limited insights as the model \(f(\bm{x};\bm{w})\) is highly non-standard.

## 2 Setting

Junta Recovery Problem.A sparse recovery (junta learning) problem with \(P\) relevant coordinates is defined in terms of a coordinate input space \(\mathcal{X}\), an output space \(\mathcal{Y}^{4}\), a marginal distribution \(\mu_{x}\) over \(\mathcal{X}\) and a link distribution \(\mu_{y|\bm{z}}\) over \(\mathcal{Y}\) given elements of \(\mathcal{X}^{P}\). This specifies a joint distribution \(\mu_{y,\bm{z}}\) over a measurable space \(\mathcal{Y}\times\mathcal{X}^{P}\), where the marginal distribution \(\bm{z}\sim\mu_{x}^{P}\) is the product distribution, and \(y\mid\bm{z}\sim\mu_{y|\bm{z}}\). We further denote \(\mu_{y}\) to be the marginal distribution over \(\mathcal{Y}\). We further denote the junta problem by a tuple \(\mu:=(\mu_{x},\mu_{y|\bm{z}})\), where the support size \(P\) and the spaces \(\mathcal{X}\) and \(\mathcal{Y}\) are implicit. Consider some examples,

\[\bm{z}\sim\mathrm{Unif}(\{\pm 1\}^{P}),\quad y=h_{*}(\bm{z})+\varepsilon\ \ \text{for some}\ \ h_{*}:\{\pm 1\}^{P}\to\mathbb{R}\ \ \text{and noise}\ \varepsilon;\] (3)

\[\bm{z}\sim\mathsf{N}(\bm{0},\bm{\mathds{1}}_{P}),\quad y\sim\text{Bernoulli }(\text{sigmoid}(h_{*}(\bm{z})))\ \ \text{for some target}\ \ h_{*}:\mathbb{R}^{P}\to\mathbb{R}.\] (4)

For \(d\geq P\), we define the junta recovery problem as learning the family of distributions

\[\mathcal{H}^{d}_{\mu}:=\big{\{}\mathcal{D}^{d}_{\mu,s}:s\in\mathsf{P}([d],P) \big{\}}\] (5)

where \(\mathsf{P}([d],P)\) is the set of non-repeating sequences from \([d]\) of length \(P\), and the distribution \(\mathcal{D}^{d}_{\mu,s}\) is a distribution over \(\mathcal{Y}\times\mathcal{X}^{d}\) such that

\[\bm{x}\sim\mu^{d}_{x}\ \ \text{and}\ \ \ y\mid(x_{s(1)},\ldots,x_{s(P)})\sim\mu_{y |\bm{z}}.\]

In words, \(\mathcal{H}^{d}_{\mu}\) is the set of distributions on \((y,x_{1},\ldots,x_{d})\) where \(\bm{x}\) follows the product distribution \(\mu^{d}_{x}\) and the output \(y\) only depends on an (unknown) sequence of \(P\) coordinates. For ease of notation, we will denote \(\mathcal{D}^{d}_{s}=\mathcal{D}^{d}_{\mu,s}\) when the junta problem \(\mu=(\mu_{x},\mu_{y|\bm{z}})\) is clear from context.

\begin{table}
\begin{tabular}{|c|c|c|} \hline \(q/\tau^{2}=\Theta(d^{k_{*}})\) & Adaptive & Non-adaptive \\ \hline \(\mathsf{SQ}\) & \(k_{*}=\mathsf{Leap}(\mathcal{C}_{\mathsf{SQ}})\) & \(k_{*}=\mathsf{Cover}(\mathcal{C}_{\mathsf{SQ}})\) \\ \hline \(\mathsf{CSQ}\) & \(k_{*}=\mathsf{Leap}(\mathcal{C}_{\mathsf{CSQ}})\) & \(k_{*}=\mathsf{Cover}(\mathcal{C}_{\mathsf{CSQ}})\) \\ \hline \(\mathsf{DLQ}_{\ell}\) & \(k_{*}=\mathsf{Leap}(\mathcal{C}_{\mathsf{DLQ}_{\ell}})\) & \(k_{*}=\mathsf{Cover}(\mathcal{C}_{\mathsf{DLQ}_{\ell}})\) \\ \hline \end{tabular}
\end{table}
Table 1: Complexity of learning a sparse function over \(d\) input coordinates with different query types based on Theorem 5.1. \(\mathsf{DLQ}_{\ell}\) is defined in Section 2, \(\mathsf{Leap}\) and \(\mathsf{Cover}\) of a set system in Definition 1, and systems \(\mathcal{C}_{\mathsf{A}}\) of detectable sets in Definition 2 based on test functions depending on query type \(\mathsf{A}\), as specified in Eq. (14).

Success as Support Recovery.For any sequence \(s\in\operatorname{\mathbb{P}}([d],P)\), we additionally denote the associated unordered set of its elements by \(S:=\{s(i):i\in[P]\}\). We consider learning as "succeeding" if it outputs the correct index set. That is, we say a junta learning algorithm "succeeds" in learning \(\mathcal{H}_{\mu}^{d}\) if for every \(s_{*}\in\operatorname{\mathbb{P}}([d],P)\), it outputs \(\hat{S}\subseteq[d]\) such that \(\hat{S}=S_{*}=\{s_{*}(i):i\in[P]\}\).

For our purpose, just the recovery of relevant coordinates is the objective as we only care about the complexity as a scaling of \(d\). Once we recover the relevant coordinates \(S_{*}\), learning the ordered sequence \(s_{*}\) corresponds to a problem of size \(P\), and thus its complexity is independent of \(d\). Note that the precise ordering \(s_{*}\) may not be identifiable if for some other \(s^{\prime}_{*}\) with \(S_{*}=S^{\prime}_{*}\) we have \(\mathcal{D}^{d}_{s_{*}}=\mathcal{D}^{d}_{s^{\prime}}\); this is possible when the measure \(\mu_{y|\bm{z}}\) has symmetries with respect to coordinates of \(\bm{z}\). We further emphasize that even the support \(S_{*}\) may not be identifiable, when \(y\) is independent of some coordinates of \(\bm{z}\) or when only using a restricted query access to the distribution--we will return to this issue in Section 5.

Does the learner know the link \(\mu_{y|\bm{z}}\)?We always assume that the learner knows \(\mu_{x}\). In our formulation, we also assume that the learner additionally knows the link \(\mu_{y|\bm{z}}\), and the only unknown is \(s_{*}\). Indeed, our lower bounds hold even for this easier setting. But under mild assumptions, our upper bounds can be obtained with an algorithm that does not require knowing \(\mu_{y|\bm{z}}\) (the complexity still depends on the problem \(\mu\))--see discussion in Section 5.

Well Behaved Distributions.For our lower bounds, we will consider junta problems \(\mu=(\mu_{x},\mu_{y|\bm{z}})\), which are "well-behaved" in a way that is standard in the hypothesis testing literature. Let \(\mu_{y,\bm{z}}\) be the induced joint distribution on \((y,\bm{z})\) where \(\bm{z}\sim\mu_{x}^{P}\) and \(y\mid\bm{z}\sim\mu_{y|\bm{z}}\). For any subset \(U\subseteq[P]\), let \(\mu_{y,\bm{z},U}\) be the marginal distribution of \((y,(z_{i})_{i\in U})\) and

\[\mu^{0}_{y,\bm{z},U}=\mu_{y,\bm{z},U}\otimes\mu_{\mathcal{X}}^{P-|U|},\] (6)

meaning that \((y,z_{1},\ldots,z_{P})\sim\mu^{0}_{y,\bm{z},U}\) has \((y,(z_{i})_{i\in U})\sim\mu_{y,\bm{z},U}\) and \((z_{i})_{i\in[P]\setminus U}\stackrel{{ iid}}{{\sim}}\mu_{x}\) independently of \((y,(z_{i})_{i\in U})\) (we replace \(z_{i}\) for \(i\notin U\) with independent draws from the marginal). The marginal distribution of \(\bm{z}\) under \(\mu^{0}_{y,\bm{z},U}\) is still \(\mu_{x}^{P}\).

**Assumption 2.1**.: _For any \(U\subseteq[P]\), we have \(\mu_{y,\bm{z}}\ll\mu^{0}_{y,\bm{z},U}\), and the Radon-Nikodym derivative \(\mathrm{d}\mu_{y,\bm{z}}/\mathrm{d}\mu^{0}_{y,\bm{z},U}\) is square integrable w.r.t. \(\mu^{0}_{y,\bm{z},U}\), i.e._

\[\frac{\mathrm{d}\mu_{y,\bm{z}}}{\mathrm{d}\mu^{0}_{y,\bm{z},U}}\in L^{2}(\mu^{ 0}_{y,\bm{z},U})\ \ \text{for all}\ \ \ U\subseteq[P].\] (7)

This is a standard and implicit assumption in the hypothesis testing literature whenever a corresponding null distribution is considered (here all \(\mu^{0}_{y,\bm{z},U}\), i.e., with label \(y\) depending only on a strict subset \(U\subseteq[P]\)); more specifically for statistical query lower bounds (Feldman et al., 2017; Damian et al., 2024), low-degree likelihood ratio (Hopkins, 2018; Kunisky et al., 2019), or contiguity lower bounds (Perry et al., 2018). It always holds when \(\mathcal{X}\) is finitely supported. We further comment on the necessity of this assumption in Appendix A.1.

## 3 Statistical and Differentiable Learning Queries

We will consider three classes of learning algorithms, all based on the statistical query paradigm, but differing in the type of queries allowed, as captured by a set \(\mathcal{Q}\) of allowed queries.

For a number of queries \(q\), tolerance \(\tau>0\) and a set \(\mathcal{Q}\subseteq\mathbb{R}^{\mathcal{Y}\times\mathcal{X}^{d}}\) of measurable functions \(\mathcal{Y}\times\mathcal{X}^{d}\to\mathbb{R}\), a \(\mathcal{Q}\)**-restricted statistical query algorithm**\(\mathcal{A}\in\mathcal{Q}\)-\(\mathsf{SQ}(q,\tau)\) for junta learning takes an input distribution \(\mathcal{D}\) over \(\mathcal{Y}\times\mathcal{X}^{d}\) and operates in \(q\) rounds where at each round \(t\in\{1,\ldots,q\}\), it issues a query \(\phi_{t}\in\mathcal{Q}\), and receives a response \(v_{t}\) such that

\[|v_{t}-\mathbb{E}_{\mathcal{D}}\left[\phi_{t}(y,\bm{x})\right]|\leq\tau\sqrt{ \mathbb{E}_{\mathcal{D}_{0}}[\phi_{t}(y,\bm{x})^{2}]},\] (8)

where \(\mathcal{D}_{0}=\mathcal{D}_{y}\otimes\mathcal{D}_{\bm{x}}\) is the associated decoupled "null" distribution where \(\bm{x}\) and \(y\) are independent, but follow their marginals5. The query \(\phi_{t}\) can depend on the past responses \(v_{1},\ldots,v_{t-1}\). Afterissuing \(q\) queries, the learner \(\mathcal{A}\) outputs \(\hat{S}\subseteq[d]\). We say that \(\mathcal{A}\)**succeeds in learning**\(\mathcal{H}^{d}_{\mu}\) if for any \(\mathcal{D}_{s_{*}}\in\mathcal{H}^{d}_{\mu}\) and any responses \(v_{t}\) satisfying (8) for \(\mathcal{D}=\mathcal{D}_{s_{*}}\), \(\mathcal{A}\) outputs \(\hat{S}=S_{*}\).

Above we allow the queries to be chosen adaptively, i.e., depending on past responses. We also consider \(\mathcal{Q}\)**-restricted non-adaptive statistical query algorithms**, which we denote by \(\mathcal{Q}\)-\(\mathsf{naSG}(q,\tau)\), where the query functions \(\{\phi_{t}\}_{t\in[q]}\) are fixed in advance and do not depend on the past responses. I.e. a non-adaptive algorithm is specified by a list of queries and a mapping from the responses to an output \(\hat{S}\subseteq[d]\).

Statistical Queries (Sq):In regular, unrestricted _Statistical Query_ learning, the allowed query set, denoted by \(\mathcal{Q}_{\mathsf{SQ}}\), is the set of all measurable functions. With slight overloading of notation, we refer to the class of these algorithms simply as \(\mathsf{SQ}(q,\tau)\) and \(\mathsf{naSG}(q,\tau)\).

Correlation Statistical Queries (Csq):It is a special subclass of statistical queries, which require \(\mathcal{Y}\subseteq\mathbb{R}\) (usually we allow the input and output spaces to be abstract), and are restricted to:

\[\mathcal{Q}_{\mathsf{CSQ}}=\left\{\phi(y,\bm{x})=y\cdot\tilde{\phi}(\bm{x})\mid \tilde{\phi}:\bm{x}\to\mathbb{R}\;\text{measurable}\right\}.\] (9)

We denote the class of adaptive and non-adaptive algorithms making such queries as \(\mathsf{CSQ}(q,\tau)\) and \(\mathsf{naCSQ}(q,\tau)\) respectively.

Differentiable Learning Queries (DLQ) with Loss \(\ell\):Let \(\mathcal{F}\subseteq V\), which is an open subset of some normed vector space \(V\), be the output space of our models, e.g. usually \(V=\mathcal{F}=\mathbb{R}\) for models with a single output unit, but we may have \(V=\mathcal{F}=\mathbb{R}^{r}\) with multiple output units. We consider a loss function \(\ell:\mathcal{F}\times\mathcal{Y}\to\mathbb{R}\) that is locally Lipschitz continuous in its first argument for every \(y\in\mathcal{Y}\). The loss is additionally equipped with a derivative operator \(\nabla\ell:\mathcal{F}\times\mathcal{Y}\to V\) as a part of the loss definition such that for any \(\bm{u}\in\mathcal{F}\) and \(y\in\mathcal{Y}\), we have \(\nabla\ell(\bm{u},y)\in\partial_{1}\ell(\bm{u},y)\), the set of generalized Clarke subderivatives of \(\ell(\cdot,y)\) at \(\bm{u}\in\mathcal{F}\). This is a standard generalization of derivatives to non-differentiable and non-convex losses; in particular, note that \((i)\) for differentiable losses, \(\partial_{1}\ell(\bm{u},y)\) is a singleton with the true gradient of \(\ell(\cdot,y)\) at \(\bm{u}\in\mathcal{F}\), and \((ii)\) for convex losses (in the first argument), \(\partial_{1}\ell(\bm{u},y)\) is the set of subderivatives of \(\ell(\cdot,y)\) at \(\bm{u}\in\mathcal{F}\). Finally, let \(\mathcal{M}:=\{f:\mathcal{X}^{d}\times\mathbb{R}\to\mathcal{F}\mid f(\bm{x}, \omega)\) is differentiable at \(\omega=0\) for all \(\bm{x}\in\mathcal{X}^{d}\}\) be the set of allowed models8. Then the allowed differentiable learning query set is:

Footnote 8: We restrict the model class \(\mathcal{M}\) to only contain models that are always differentiable at \(\omega=0\) without loss of generality, as for any non-differentiable model \(f_{1}:\mathcal{X}^{d}\times\mathbb{R}\to\mathcal{F}\) under any “reasonable” generalized notion of derivative with \(\frac{\mathrm{d}}{\mathrm{d}\omega}f_{1}(\bm{x},\omega)\mid_{\omega=0}=g(\bm{ x})\in V\), one can define a differentiable model \(f_{2}(\bm{x},\omega)=f_{1}(\bm{x},0)+\omega\cdot g(\bm{x})\) such that \(f_{2}(\bm{x},0)=f_{1}(\bm{x},0)\) and \(\frac{\mathrm{d}}{\mathrm{d}\omega}f_{2}(\bm{x},\omega)\mid_{\omega=0}=g(\bm{ x})\).

\[\mathcal{Q}_{\mathsf{DLQ}_{\ell}}=\left\{\phi(y,\bm{x})=\left[\frac{\mathrm{d} }{\mathrm{d}\omega}f(\bm{x},\omega)\right]_{\omega=0}^{\mathsf{T}}\nabla\ell( f(\bm{x},0),y)\mid f\in\mathcal{M}\right\}.\] (10)

That is, at each round the algorithm chooses a parametric model \(f(\bm{x},\omega)\), parameterized by a single scalar \(\omega\), and the query corresponds to the derivative (with respect to the single parameter \(\omega\)) of the loss applied to the model, at \(\omega=0\). This captures the first gradient calculation for a single-parameter model initialized at zero. But the derivative at any other point can also be obtained by querying at a shifted model \(f_{(\nu)}(\bm{x},\omega)=f(\bm{x},\nu+\omega)\), and the gradient with respect to a model \(f(\bm{x},\bm{w})\) with \(r\) parameters \(\bm{w}\in\mathbb{R}^{r}\) can be obtained by issuing \(r\) queries, one for each coordinate, of the form \(f_{(\bm{w},i)}(\bm{x},\omega)=f(\bm{x},\bm{w}+\omega e_{i})\), where \(e_{i}\) is the standard basis vector. Queries of the form \(\mathcal{Q}_{\mathsf{DLQ}_{\ell}}\) can thus be used to implement gradient calculations for any differentiable model, noting that the number of queries \(q\) is the number of gradient calculations _times the number of parameters_. Finally, observe that, for differentiable losses, the queries of the form (1) are equivalent to the form mentioned in (10) due to the chain rule.

We denote the class of adaptive and non-adaptive algorithms making such queries as

\(\mathsf{DLQ}_{\ell}(q,\tau)\):=\(\mathcal{Q}_{\mathsf{DLQ}_{\ell}}\)-\(\mathsf{SAQ}(q,\tau)\) and \(\mathsf{naDLQ}_{\ell}(q,\tau)\):=\(\mathcal{Q}_{\mathsf{DLQ}_{\ell}}\)-\(\mathsf{naSG}(q,\tau)\).

**Remark 3.1**.: _More common in the SQ literature is to restrict the query in \(L^{\infty}\) (or equivalently, require precision relative to \(L^{\infty}\)). Precision relative to \(L^{2}(\mathcal{D}_{0})\) is more similar to VSTAT [15], and is more powerful than relative to \(L^{\infty}\), and our lower bounds hold against this stronger notion. In our algorithms and upper bounds we only need this additional power when \(y\mapsto\nabla\ell(\bm{u},y)\) is unbounded. If we further assume that these functions are bounded, e.g., the labels \(y\) are bounded and \(\nabla\ell\) continuous, our queries have bounded \(L^{\infty}\) and thus operate in the more familiar SQ setting._

## 4 Leap and Cover Complexities

**Definition 1**.: _We define the leap and cover complexities for any system of subsets \(\mathcal{C}\subseteq 2^{[P]}\)._

1. _For any system of subsets_ \(\mathcal{C}\subseteq 2^{[P]}\)_, its_ leap complexity _is defined as_ \[\mathsf{Leap}(\mathcal{C}):=\min_{\begin{subarray}{c}U_{1},\ldots,U_{r}\in \mathcal{C}\\ \bigcup\limits_{i=1}^{r}U_{i}=[P]\end{subarray}}\max_{i\in[r]}\ |U_{i} \setminus\cup_{j=0}^{i-1}U_{j}|.\] (11)
2. _We define the_ cover complexity _of_ \(\mathcal{C}\) _to be_ \[\mathsf{Cover}(\mathcal{C}):=\max_{i\in[P]}\ \min_{U\in\mathcal{C},i\in U}\ |U|.\] (12)

**Remark 4.1**.: _We always have \(\mathsf{Leap}(\mathcal{C})\leq\mathsf{Cover}(\mathcal{C})\). Both \(\mathsf{Leap}\) and \(\mathsf{Cover}\) complexities are closed under taking the union of subsets in \(\mathcal{C}\). Also, when \(\operatorname{supp}(\mathcal{C})=\cup_{U\in\mathcal{C}}U\neq[P]\), then we use the convention \(\mathsf{Leap}(\mathcal{C})=\mathsf{Cover}(\mathcal{C})=\infty\). See a discussion about this convention in Appendix A.3 and in particular, the definition of the relative leap and cover complexities in Definition 3._

Here \(\mathcal{C}\) is the system of subsets which are "detectable", and will depend on the the query access model. Intuitively, the leap and cover complexities of \(\mathcal{C}\) capture the exponent of \(d\) in the query complexity when recovering the support of an unknown \(s_{*}\in\mathsf{P}([d],P)\), for adaptive and non-adaptive algorithms respectively. To discover the relevant coordinates of \(s_{*}\), that correspond to \(U\in\mathcal{C}\), one needs to enumerate over \(\Theta(d^{|U|})\). Hence, a non-adaptive algorithm, which fixes the queries in advance, requires \(\Theta(d^{k_{i}})\) queries to discover \(i^{\mathrm{th}}\) relevant coordinate i.e. \(s_{*}(i)\), where \(k_{i}=\min_{i\in U\in\mathcal{C}}|U|\). Therefore, non-adaptive algorithms need a total number of queries that scales as \(\Theta(d^{\mathsf{Cover}(\mathcal{C})})\) to learn \(\operatorname{supp}(s_{*})\). On the other hand, adapting queries using previous answers can greatly reduce this complexity as seen in the example in (2) in Section 1. This is captured by the leap complexity, which measures the maximum number of coordinates we need to discover at once. Finally, the set system of detectable subsets will depend on the type of allowed queries.

**Definition 2** (Detectable Subsets).: _Let \(\mu\) be a junta problem. Denote_

\[L_{0}^{2}(\mu_{x})=\big{\{}T\in L^{2}(\mu_{x}):\mathbb{E}_{z\sim\mu_{x}}[T(z) ]=0\big{\}}\]

_the set of zero-mean functions. For a set of test function \(\Psi\subseteq L_{2}(\mu_{y})\) we say that \(U\subseteq[P]\) is \(\Psi\)**-detectable** iff_

\[\exists T\in\Psi,\ \exists T_{i}\in L_{0}^{2}(\mu_{x})\ \text{for each}\ i\in U \quad\text{such that}\quad\mathbb{E}_{\mu_{y,x}}\Big{[}T(y)\prod_{i\in U}T_{i}(z_ {i})\Big{]}\neq 0.\] (13)

_We denote \(\mathcal{C}_{\Psi}(\mu)\) the set of \(\Psi\)-detectable sets, i.e. those sets satisfying (13)._

The set of relevant test functions depend on the query types allowed and we define:

\[\begin{array}{ll}\text{For}\ \mathsf{SQ},&\Psi_{\mathsf{SQ}}=L^{2}(\mu_{y}) \ \text{(i.e. all}\ L^{2}\ \text{functions)}.\\ \text{For}\ \mathsf{CSQ}\ \text{(recall}\ \mathcal{Y}\subseteq\mathbb{R}),&\Psi_{ \mathsf{CSQ}}=\{y\mapsto y\}\ \text{(just the identity)}.\\ \text{For}\ \mathsf{DLQ}_{\ell},&\Psi_{\mathsf{DLQ}_{\ell}}=\{y\mapsto\bm{v}^{ \mathsf{T}}\nabla\ell(\bm{u},y):\bm{u}\in\mathcal{F},\bm{v}\in V\}.\end{array}\] (14)

While the queries of the form (13), where \(\Psi_{\mathsf{A}}\) for \(\mathsf{A}\in\{\mathsf{SQ},\mathsf{CSQ},\mathsf{DLQ}_{\ell}\}\) is given by (14), are less general than \(\phi(y,\bm{x})\) with \(\phi\in\mathcal{Q}_{\mathsf{A}}\), they can be implemented by the corresponding query types \(\mathcal{Q}_{\mathsf{A}}\) and are sufficient for deciding between "\(S\subseteq[d]\) maps to the corresponding \(U\subseteq[P]\)" or "\(S\not\subseteq S_{*}\)". The sets \(\Psi_{\mathsf{SQ}},\Psi_{\mathsf{CSQ}},\Psi_{\mathsf{DLQ}_{\ell}}\) from (14) used for detectibility arise naturally in the proof of the lower bounds of Theorem 5.1.

To ease notation, for query type \(\mathsf{A}\), we use the shorthand \(\mathcal{C}_{\mathsf{A}}:=\mathcal{C}_{\Psi_{\mathsf{A}}}\), and

\[\mathsf{Leap}_{\mathsf{A}}(\mu):=\mathsf{Leap}(\mathcal{C}_{\mathsf{A}}(\mu))= \mathsf{Leap}(\mathcal{C}_{\Psi_{\mathsf{A}}}(\mu)),\qquad\mathsf{Cover}_{ \mathsf{A}}(\mu):=\mathsf{Cover}(\mathcal{C}_{\mathsf{A}}(\mu))=\mathsf{Cover }(\mathcal{C}_{\Psi_{\mathsf{A}}}(\mu)).\]

We refer to these as the \(\mathsf{A}\)_-leap exponent_ and \(\mathsf{A}\)_-cover exponent_ of the problem \(\mu\).

Main Result: Characterizing the Complexity of Learning Juntas

**Theorem 5.1**.: _For any junta problem \(\mu\) and any loss \(\ell\), there exists \(C>c>0\) (that depend on \(P\),\(\mu\) and the loss, but not on \(d\)), such that for query types \(\mathsf{A}\in\{\mathsf{SQ},\mathsf{CSQ},\mathsf{DLQ}_{\ell}\}\) with corresponding test function sets \(\Psi_{\mathsf{A}}\) as defined in (14):_

**Adaptive**.: _Let \(k_{*}=\mathsf{Leap}_{\mathsf{A}}(\mu)\). There exists an algorithm \(\mathcal{A}\in\mathsf{A}(q,\tau)\) that succeeds in learning \(\mathcal{H}^{d}_{\mu}\) with \(\tau=c\) and \(q=Cd^{k_{*}}\). And if \(\mu\) satisfies Assumption 2.1, then for any \((q,\tau)\) such that \(q/\tau^{2}\leq cd^{k_{*}}\), no algorithm \(\mathcal{A}\in\mathsf{A}(q,\tau)\) succeeds at learning \(\mathcal{H}^{d}_{\mu}\)._

**Non-adaptive**.: _Let \(k_{*}=\mathsf{Cover}_{\mathsf{A}}(\mu)\). There exists an algorithm \(\mathcal{A}\in\mathsf{naA}(q,\tau)\) that succeeds in learning \(\mathcal{H}^{d}_{\mu}\) with \(\tau=c\) and \(q=Cd^{k_{*}}\). And if \(\mu\) satisfies Assumption 2.1, then for any \((q,\tau)\) such that \(q/\tau^{2}\leq cd^{k_{*}}\), no algorithm \(\mathcal{A}\in\mathsf{naA}(q,\tau)\) succeeds at learning \(\mathcal{H}^{d}_{\mu}\)._

**Remark 5.2**.: _In the positive results in Theorem 5.1, we used all the allowed complexity to have many (\(q=\Theta(d^{k_{*}})\)) queries, and kept the tolerance constant. More generally, it is possibly to trade off between the number of queries \(q\) and tolerance \(\tau\), at a cost of a log-factor: For \(k_{*}=\mathsf{Leap}_{\mathsf{A}}(\mu)\) and \(k_{*}=\mathsf{Cover}_{\mathsf{A}}(\mu)\), respectively, there exists algorithms \(\mathcal{A}\in\mathsf{A}(q,\tau)\) and \(\mathcal{A}\in\mathsf{naA}(q,\tau)\) that learn \(\mathcal{H}^{d}_{\mu}\), for any \(q\geq C\log(d)\) and \(\tau\leq c\) with \(q/\tau^{2}\geq Cd^{k_{*}}\log(d)\)._

The proof of this theorem is deferred to Appendix B. Theorem 5.1 shows that the leap and cover complexities sharply capture the scaling in \(d\) of statistical query algorithms when learning \(\mathcal{H}^{d}_{\mu}\).

**Remark 5.3**.: _The above upper bound uses that \(\mu\) and therefore the \(T,T_{i}\) in Definition 2 are known. In the case when \(\mu\) is unknown, one can follow a similar strategy as in Damian et al. (2024b) and randomize the transformations \(T\) and \(T_{i}\) over a sufficiently large (but finite independent of \(d\)) linear combination of functions in \(\Psi_{\mathsf{SQ}}\) and \(L^{2}_{0}(\mu_{x})\). Under some regularity assumption on \(\mu\) and \(\ell\), one can show by anti-concentration that with constant probability, the expectation in condition (13) is bounded away from \(0\) by a constant independent of the dimension._

## 6 Relationship Between \(\mathsf{SQ},\mathsf{CSQ}\) and \(\mathsf{DLQ}_{\ell}\)

Obviously, \(\mathsf{CSQ}\subseteq\mathsf{SQ}\), and indeed we see that \(\mathcal{C}_{\mathsf{CSQ}}\subseteq\mathcal{C}_{\mathsf{SQ}}\) in Definition 2 because of which \(\mathsf{Leap}_{\mathsf{CSQ}}\geq\mathsf{Leap}_{\mathsf{SQ}}\) and \(\mathsf{Cover}_{\mathsf{CSQ}}\geq\mathsf{Cover}_{\mathsf{SQ}}\). For binary \(\mathcal{Y}\), these query models collapse, but otherwise there can be an arbitrary gap.

**Proposition 6.1** (\(\mathsf{SQ}\) versus \(\mathsf{CSQ}\)).: _For any \(\mu\), let \(\mathcal{C}_{\mathsf{SQ}}:=\mathcal{C}_{\mathsf{SQ}}(\mu)\), and \(\mathcal{C}_{\mathsf{CSQ}}:=\mathcal{C}_{\mathsf{CSQ}}(\mu)\). If \(|\mathcal{Y}|=2\) (binary output), then we always have \(\mathcal{C}_{\mathsf{CSQ}}=\mathcal{C}_{\mathsf{SQ}}\) and \(\mathsf{Leap}_{\mathsf{SQ}}=\mathsf{Leap}_{\mathsf{CSQ}}\) and \(\mathsf{Cover}_{\mathsf{SQ}}=\mathsf{Cover}_{\mathsf{CSQ}}\). On the other hand if \(|\mathcal{Y}|>2\), the \(\mathsf{SQ}\)-exponents can be much smaller than the \(\mathsf{CSQ}\)-exponents: e.g., there exist a setting with \(\mathsf{Leap}_{\mathsf{SQ}}=\mathsf{Cover}_{\mathsf{SQ}}=1\) and \(\mathsf{Leap}_{\mathsf{CSQ}}=\mathsf{Cover}_{\mathsf{CSQ}}=P\)._

Similarly, \(\mathsf{DLQ}_{\ell}\subseteq\mathsf{SQ}\) by definition, and thus, \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}\geq\mathsf{Leap}_{\mathsf{SQ}}\) and \(\mathsf{Cover}_{\mathsf{DLQ}_{\ell}}\geq\mathsf{Cover}_{\mathsf{SQ}}\).

**Proposition 6.2** (\(\mathsf{DLQ}_{\ell}\) versus \(\mathsf{SQ}\) and \(\mathsf{CSQ}\)).: _Consider any \(\mu=(\mu_{y},\mu_{y|\bm{z}})\)._

* _Let_ \(\mathcal{Y},\mathcal{F}\subseteq\mathbb{R}\)_. For the squared loss_ \(\ell:(u,y)\mapsto(u-y)^{2}\)_, we always have_ \(\mathcal{C}_{\mathsf{DLQ}_{\ell}}=\mathcal{C}_{\mathsf{CSQ}}\)_, and thus,_ \(\mathsf{Leap}_{\mathsf{CSQ}}=\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}\) _and_ \(\mathsf{Cover}_{\mathsf{CSQ}}=\mathsf{Cover}_{\mathsf{DLQ}_{\ell}}\)_._
* _A sufficient condition for_ \(\mathcal{C}_{\mathsf{SQ}}=\mathcal{C}_{\mathsf{DLQ}_{\ell}}\) _is to have_ \(\mathrm{span}(\Psi_{\mathsf{DLQ}_{\ell}})\) _dense in_ \(L^{2}_{0}(\mu_{y})\)_. Conversely, if there exists nonzero_ \(T\in L^{2}_{0}(\mu_{y})\) _bounded with_ \(T(y)\) _orthogonal in_ \(L^{2}(\mu_{y})\) _to any functions in_ \(\Psi_{\mathsf{DLQ}_{\ell}}\)_, then there exists a problem_ \(\mu\) _such that_ \(\mathsf{Cover}_{\mathsf{DLQ}_{\ell}}=\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}>\mathsf{ Cover}_{\mathsf{SQ}}=\mathsf{Leap}_{\mathsf{SQ}}\)_._
* _There exists a loss_ \(\ell\) _and a junta problem_ \(\mu\) _such that_ \(\mathsf{Leap}_{\mathsf{SQ}}(\mu)<\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}(\mu)<\mathsf{ Leap}_{\mathsf{CSQ}}(\mu)\)_. Similarly, we can have_ \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}>\mathsf{Leap}_{\mathsf{CSQ}}\)_._

The condition in Proposition 6.2.(b) can be seen as a universal approximation property of neural networks with activation \(y\mapsto\nabla\ell(\bm{u},y)\)(Cybenko, 1989; Hornik, 1991; Sonoda and Murata, 2017). The next lemma gives a few examples of losses with \(\mathsf{DLQ}_{\ell}=\mathsf{SQ}\). For concreteness, we consider \(\mathcal{F}=\mathbb{R}\) and \(\mathcal{Y}\subseteq\mathbb{R}\).

**Theorem 6.3**.: _For \(\ell\in\{\ell_{1}:(u,y)\mapsto|u-y|,\ell_{\text{hinge}}:(u,y)\mapsto\max(1-uy,0)\}\), then \(\mathcal{C}_{\mathsf{QL}_{\ell}}(\mu)=\mathcal{C}_{\mathsf{SQ}}(\mu)\) for all7 problems \(\mu\). If we further assume \(\mathcal{Y}\subseteq[-M,M]\) for \(M\geq 0\), then \(\ell(u,y)=e^{-uy}\) (exponential loss) has \(\mathcal{C}_{\mathsf{DLQ}_{\ell}}(\mu)=\mathcal{C}_{\mathsf{SQ}}(\mu)\) for all problems \(\mu\)._

Footnote 7: For Hinge loss, we further restrict \(\mu_{y}\) to measures with finite second moment so that \(\Psi_{\mathsf{A}}\subseteq L^{2}(\mu_{y})\).

The cases of \(\ell_{1}\) and Hinge loss follow directly from universal approximation of neural networks with linear threshold activation. The proofs of the above propositions and lemma can be found in Appendix C. Propositions 6.1 and 6.2 combined with Theorems 6.3 and 5.1 directly imply a number of separation results between adaptive and non-adaptive algorithms and between different loss functions. See examples (2) in the introduction, and further examples in Appendix C.

## 7 Gradient Descent on Neural Networks

The goal of this section is to connect the complexity of \(\mathsf{DLQ}\) to gradient descent on standard neural networks. We focus on the simple case of \(\bm{x}\sim\mathrm{Unif}(\{+1,-1\}^{d})\) uniformly distributed on the hypercube, and \(\mathcal{Y}\subseteq\mathbb{R}\) and \(\mathcal{F}=\mathbb{R}\). In this setting, condition (13) in Definition 2 simplifies to: there exists \(T\in\Psi_{\mathsf{A}}\) such that \(\mathbb{E}_{\mu_{y,\bm{z}}}\left[T(y)\prod_{i\in U}z_{i}\right]=\mathbb{E}_{ \mu_{y,\bm{z}}}\left[T(y)\chi_{U}(\bm{z})\right]\neq 0\), where \(\chi_{U}(\bm{z}):=\prod_{i\in U}z_{i}\) denote the standard Fourier-Walsh basis. In particular, the set \(\mathcal{C}_{\mathsf{CSQ}}\) contains exactly all non-zero Fourier coefficients of \(h_{\bm{z}}(\bm{z}):=\mathbb{E}[y|\bm{z}]\), and we recover the leap exponent of Abbe et al. (2023) as discussed in the introduction.

We train a standard two layer neural network (see (NN1) in Appendix D) of width \(M\), using online SGD with a loss function \(\ell:\mathbb{R}\times\mathcal{Y}\rightarrow\mathbb{R}_{\geq 0}\), i.e. SGD on \(\mathbb{E}_{(\bm{x},y)\sim\mathcal{D}_{\bm{z}}^{d}}\left[\ell(f_{\mathsf{NN}} (\bm{x};\bm{\Theta}),y)\right]\) for \(\mathcal{D}_{s_{\bm{z}}}^{d}\in\mathcal{H}_{\mu}^{d}\). More specifically, we train the parameters \(\bm{\Theta}=\{\bm{\theta}_{j}:j\in[M]\}\) using batch-SGD with loss \(\ell\) and batch size \(b\) from initialization \((\bm{\theta}_{j})_{j\in[M]}\stackrel{{\text{i.i.d.}}}{{\sim}}\rho_ {0}\) specified in (69). At each step, given samples \((\{(\bm{x}_{ki},y_{ki}):i\in[b]\})_{k\geq 0}\), the weights are updated using

\[\bm{\theta}_{j}^{k+1}=\bm{\theta}_{j}^{k}-\frac{\eta}{b}\left(\sum_{i\in[b]} \ell^{\prime}(f_{\mathsf{NN}}(\bm{x}_{ki};\bm{\Theta}^{t}),y_{ki})\nabla_{\bm {\theta}}\sigma_{*}(\bm{x}_{ki};\bm{\theta}_{j}^{k})+\lambda\,\bm{\theta}_{j}^ {k}\right),\] ( \[\ell\] -bSGD)

where \(\eta\) is the step-size and we allow for a \(\ell_{2}\) regularization with parameter \(\lambda\in\mathbb{R}_{+}\). Recall that \(\ell^{\prime}(u,y)\in\partial_{1}\ell(u,y)\) is the defined derivative of \(\ell(\cdot,y)\) at \(u\in\mathbb{R}\). We define the test error

\[\mathcal{R}(f)=\mathbb{E}_{\mathcal{D}_{s_{\bm{z}}}^{d}}\,\left[\ell(f(\bm{x} ),y)\right],\] (15)

and further introduce the excess test error \(\overline{\mathcal{R}}(f)=\mathcal{R}(f)-\inf_{f:\{\pm 1\}^{d}\rightarrow\mathbb{R}} \mathcal{R}(\bar{f})\).

Dimension-free dynamics.In the junta learning setting, when \(y\) only depends on \(P\lll d\) coordinates, following Abbe et al. (2022, Secion 3), the SGD dynamics (\(\ell\)-bSGD) concentrates on an effective _dimension-free_ (DF) dynamics as \(M,d\rightarrow\infty\) and \(\eta\to 0\). This equivalence holds under a certain assumption on the loss function, and other assumptions on the initialization and activation that are similar to the setup of Abbe et al. (2022) (see Appendix D for details, especially DF-PDE).

Dimension-free dynamics' alignment with the support. In the above limiting regime, \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}\) crisply characterizes (DF-PDE) dynamics' alignment with the support.

**Theorem 7.1** (Informal version of Theorem D.6).: _If \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}=1\) then for some time \(t\), the output of the model at time \(t\) of (DF-PDE) dynamics depends on all coordinates \(z_{i}\). On the other hand, if \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}>1\), then there exists a coordinate \(i\) such that for any time \(t\geq 0\), the output of the model at time \(t\) of (DF-PDE) dynamics does no depend on \(z_{i}\)._

This establishes that the ability of (DF-PDE) dynamics (comparable to (\(\ell\)-bSGD) in the linear scaling) to learn all relevant coordinates depends on \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}(\mu)=1\). That is if \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}>1\), then (DF-PDE) remains stuck in a suboptimal saddle subspace. On the other hand, if \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}=1\), then (DF-PDE) dynamics escapes this subspace and the weights align with the entire support.

Learning of \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}=1\) with finite SGD.Showing directly that (DF-PDE) dynamics indeed reach a near global minimizers of the test error remains challenging. Alternatively, we show that a specific layer-wise training dynamics similar to Abbe et al. (2022) achieves a vanishing excess error for \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}(\mu)=1\) settings in the linear scaling of samples.

Roughly speaking, we train the first layer for \(k_{1}=P\) steps and then the second layer weights for \(k_{2}=O_{d}(1)\) steps using batch-SGD with batch size \(b=O_{d}(d)\), both for a loss \(\ell\). We consider a polynomial activation \(\sigma(x)=(1+x)^{L}\) of degree \(L\geq 2^{8P}\). The most notable difference from Abbe et al. (2022) is that we further slightly perturb step-sizes for each coordinate \(\eta^{w_{i}}=\eta\kappa_{i}\) with \(\kappa_{i}\in[1/2,3/2]\), and denote \(\bm{\kappa}=(\kappa_{1},\dots,\kappa_{d})\in\mathbb{R}^{d}\) for the first layer training. This perturbation is necessary to break possible coordinate symmetries; see Remark 7.3.

**Theorem 7.2** (Informal version of Theorem E.1).: _For a convex and analytic loss \(\ell\), almost surely over the perturbation \(\bm{\kappa}\) and the initial bias \(\bar{c}\in\mathbb{R}\) the following holds. If \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}(\mu)=1\), then the above layer-wise SGD training dynamics with total sample size \(n=\Theta_{d}(d)\) and \(M=\Theta_{d}(1)\) achieves excess test error \(\overline{\mathcal{R}}(f_{\mathsf{NN}}(\cdot;\bm{\Theta}^{k_{1}+k_{2}}))=o(1)\) with high probability._

The formal statement and the precise training specifications can be found in Appendix D. This result generalizes Abbe et al. (2022, Theorem 9) beyond squared loss.

**Remark 7.3**.: _A slight coordinate-wise perturbation in the step-sizes for the first layer training is necessary to break the potential coordinate symmetries in the output \(y\)--see discussion in Abbe et al. (2022, Appendix A). This can be removed by either stating the theorem for all but some measure zero set of Leap-1 functions as in Abbe et al. (2022), or by studying the dynamics for \(O(\log d)\) steps._

The query complexity of \(b\)-batch SGD on \(M\) neurons for \(\Upsilon\) SGD-steps is \(T_{\mathsf{C}}=\Theta(bM\Upsilon d)\). The above theorems show that for \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}(\mu)=1\), \(\Upsilon=\Theta(d/b)\) steps with \(M=\Theta(1)\) neurons--and therefore \(T_{\mathsf{C}}=\Theta(d^{2})\)--suffices to learn the support and minimize the excess test error. Furthermore, for \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}(\mu)>1\) and neural networks trained in the mean-field regime, \(\Upsilon=\Theta(d)\) (and therefore \(T_{\mathsf{C}}=\Theta(d^{2})\)) is not enough. We further comment on the general conjectural picture in Appendix A.2.

Numerical Simulation.We consider a function similar to \(y_{2}\) in (2) with \(\mathsf{Leap}_{\mathsf{CSQ}}=3\) but \(\mathsf{Leap}_{\mathsf{SQ}}=1\). Specifically, we set \(P=4\) and \(\mathcal{C}=\{\{1,2,3\},\{1,2,4\},\{1,3,4\},\{2,3,4\}\}\), and define \(y=h_{*}(\bm{z})\) where

\[h_{*}(\bm{z})=\sum_{U\in\mathcal{C}}\hat{h}_{*}(U)\chi_{U}(\bm{z}),\ \text{where}\ \hat{h}_{*}(U)\sim\mathrm{Unif}([-2,2])\ \text{for all}\ U\in\mathcal{C}.\] (16)

We train with online 1-batch SGD (\(\ell\)-bSGD) on a two-layer net with different loss functions (without any regularization) and stepsize \(\eta\propto 1/d\), where we consider ambiant dimensions \(d\in\{100,300,500\}\). In Figure 1, we plot the test mean squared error versus \(\eta\times\text{SGD-iterations}\) (thus also scaled with \(1/d\)), over 10 trials. Additionally, we also plot the continuous time (DF-PDE) in (dashed black line) that corresponds to the limit \(d\to\infty\).

Figure 1: The function \(h_{*}(\bm{z})\) in (16) has \(\mathsf{Leap}_{\mathsf{CSQ}}=3\) but \(\mathsf{Leap}_{\mathsf{SQ}}=1\). For the squared loss (left plot), (DF-PDE) remains stuck at initialization (no learning), and to escape the saddle, SGD requires a number of iterations that increases faster than \(O(d)\). For the absolute loss (center plot) or the other loss (right plot), we have \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}=\mathsf{Leap}_{\mathsf{SQ}}=1\), and the SGD dynamics learns in \(\Theta(d)\) steps and (DF-PDE) learns in \(O(1)\) continuous time.

Conclusion and Outlook

In this paper, we considered learning juntas over general product distributions with statistical query algorithms. To capture learning with gradient evaluations over a general loss and arbitrary model, we introduced _Differentiable Learning Queries_ (\(\mathsf{DLQ}_{\ell}\)), which can be seen as a generalization of correlation statistical queries beyond squared loss. We then showed that the complexity of learning juntas with either \(\mathsf{SQ}\), \(\mathsf{CSQ}\), or \(\mathsf{DLQ}_{\ell}\) algorithms is sharply characterized in terms of a _leap exponent_ (adaptive queries) or a _cover exponent_ (non-adaptive queries). These exponents are defined in terms of a minimal combination of detectable sets to cover the support, where the system of detectable sets depends on the allowed queries. In general, the leap and cover exponents for different losses are not comparable. However, we identify "generic" losses, including \(\ell_{1}\), where \(\mathsf{DLQ}_{\ell}\) algorithms are as powerful as \(\mathsf{SQ}\) algorithms for learning juntas. We further showed that \(\mathsf{DLQ}_{\ell}\) can indeed capture the complexity of learning with SGD in the case of data on the hypercube.

Worst-case v.s. One-pass v.s. Multi-pass SGD.\(\mathsf{DLQ}_{\ell}\) (like \(\mathsf{SQ}\)) is defined in terms of worst-case noise. It is well understood that worst-case noise is theoretically very different from estimating population gradients based on samples (either independent samples as in one-pass (\(\mathsf{S}\))GD, or with repeated use of samples as in full-batch or multi-pass) when highly specialized models are allowed [1]. However, we expect \(\mathsf{DLQ}_{\ell}\) to still capture the complexity of one-pass SGD in many settings of interest--with "regular" models--such as in Section 7. With sample reuse (e.g. multi-pass) the situation is more complex: Dandi et al. (2024) showed that two steps of full-batch gradient descent with square loss goes beyond \(\mathsf{CSQ}\). Heuristically, and in light of our work, two steps on the same batch can be seen as a single gradient evaluation on a modified loss, thus going beyond \(\mathsf{CSQ}=\mathsf{DLQ}_{\ell_{ego}}\), but remaining inside \(\mathsf{DLQ}_{\ell}\subseteq\mathsf{SQ}\) for some perturbed non-quadratic loss \(\tilde{\ell}\). Indeed, we expect generally that multi-pass SGD on a "regular" model will remain in \(\mathsf{SQ}\).

Multi-index and beyond.In this paper, we focused on learning sparse functions. We hope the modular nature of our analysis framework (defining detectable sets in terms of test functions, and leap and cover complexities of set systems), our definition of \(\mathsf{DLQ}\), and the distinctions we emphasize between \(\mathsf{CSQ}\), \(\mathsf{SQ}\) and \(\mathsf{DLQ}\) and between adaptive and non-adaptive complexities, will be helpful in guiding and contextualizing analysis in other settings such as learning single-index or multi-index functions [e.g. 18, 19, 20, 21, 22, 23]. For example, the information exponent for single-index [1] can be seen as analogous to our \(\mathsf{CSQ}\)-cover exponent, the generative exponent for single-index [1] as analogous to our \(\mathsf{SQ}\)-cover exponent, and the isoLeap exponent for multi-index [1, 23, 19] as analogous to our \(\mathsf{CSQ}\)-leap exponent. It would be interesting to obtain a unified understanding of these specialized treatments and extend our general framework to multi-index models, and also to learning under other invariances beyond permutations and rotations.

In our setup, we emphasize generic input and output spaces, without a field structure. This emphasizes that when learning juntas, polynomials or degree of input or output coordinates is irrelevant. Defining multi-index models and introducing rotational invariance necessitates a field structure and gives rise to the relevance of polynomial degrees and decomposition.

An important point is that when considering permutation (as in juntas) vs. rotational (as in multi-index models) invariance, one must consider not only the invariance structure of the target function class, but also the input distribution (i.i.d. coordinates as in our case, or more generally exchangeable vs. spherical) and learning rule. E.g., learning a parity over input coordinates requires only \(\Theta(\log d)\) samples, but a rotationally equivariant algorithm effectively learns parities also over rotated axis, which requires \(\Omega(d)\) samples [19] (and thus \(\Omega(d^{2})\) runtime). This also explains the need for \(\Theta(d)\) steps and thus \(\Theta(d^{2})\) runtime to learn leap-1 functions using SGD on a rotationally invariant neural net in Section 7. In order to break the \(\Omega(d)\)-sample lower bound, we need to break the rotation-equivariance, e.g. using sparse initialization and \(\ell_{1}\) regularization, which indeed can achieve \(\Theta(\log(d))\) sample complexity.

Acknowledgement.This research was done as part of the NSF-Simons Sponsored Collaboration on the Theoretical Foundations of Deep Learning and the NSF Tripod Institute on Data, Econometrics, Algorithms, and Learning (IDEAL).

## References

* Abbe and Boix-Adsera (2022) Emmanuel Abbe and Enric Boix-Adsera. On the non-universality of deep learning: quantifying the cost of symmetry. _Advances in Neural Information Processing Systems_, 35:17188-17201, 2022.
* Abbe and Sandon (2020) Emmanuel Abbe and Colin Sandon. On the universality of deep learning. _Advances in Neural Information Processing Systems_, 33:20061-20072, 2020.
* Abbe et al. (2021a) Emmanuel Abbe, Enric Boix-Adsera, Matthew S Brennan, Guy Bresler, and Dheeraj Nagaraj. The staircase property: How hierarchical structure can guide deep learning. _Advances in Neural Information Processing Systems_, 34:26989-27002, 2021a.
* Abbe et al. (2021b) Emmanuel Abbe, Pritish Kamath, Eran Malach, Colin Sandon, and Nathan Srebro. On the power of differentiable learning versus pac and sq learning. _Advances in Neural Information Processing Systems_, 34:24340-24351, 2021b.
* Abbe et al. (2022) Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks. In _Conference on Learning Theory_, pages 4782-4887. PMLR, 2022.
* Abbe et al. (2023) Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2552-2623. PMLR, 2023.
* Allen-Zhu and Li (2020) Zeyuan Allen-Zhu and Yuanzhi Li. Backward feature correction: How deep learning performs deep learning. _arXiv e-prints_, pages arXiv-2001, 2020.
* Brous et al. (2021) Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent on non-convex losses from high-dimensional inference. _Journal of Machine Learning Research_, 22(106):1-51, 2021.
* Bietti et al. (2022) Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song. Learning single-index models with shallow neural networks. _Advances in Neural Information Processing Systems_, 35:9768-9783, 2022.
* Bietti et al. (2023) Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning gaussian multi-index models with gradient flow. _arXiv preprint arXiv:2310.19793_, 2023.
* Blum and Langley (1997) Avrim L Blum and Pat Langley. Selection of relevant features and examples in machine learning. _Artificial intelligence_, 97(1-2):245-271, 1997.
* Bshouty and Feldman (2002) Nader H Bshouty and Vitaly Feldman. On using extended statistical queries to avoid membership queries. _Journal of Machine Learning Research_, 2(Feb):359-395, 2002.
* Bubeck et al. (2015) Sebastien Bubeck et al. Convex optimization: Algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357, 2015.
* Celentano et al. (2021) Michael Celentano, Theodor Misiakiewicz, and Andrea Montanari. Minimum complexity interpolation in random features models. _arXiv preprint arXiv:2103.15996_, 2021.
* Chizat and Bach (2018) Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. _Advances in neural information processing systems_, 31, 2018.
* Cybenko (1989) George Cybenko. Approximation by superpositions of a sigmoidal function. _Mathematics of control, signals and systems_, 2(4):303-314, 1989.
* Damian et al. (2024a) Alex Damian, Eshaan Nichani, Rong Ge, and Jason D Lee. Smoothing the landscape boosts the signal for sgd: Optimal sample complexity for learning single index models. _Advances in Neural Information Processing Systems_, 36, 2024a.
* Damian et al. (2024b) Alex Damian, Loucas Pillaud-Vivien, Jason D Lee, and Joan Bruna. The computational complexity of learning gaussian single-index models. _arXiv preprint arXiv:2403.05529_, 2024b.
* Damian et al. (2022) Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn representations with gradient descent. In _Conference on Learning Theory_, pages 5413-5452. PMLR, 2022.
* Damian et al. (2020)Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. Learning two-layer neural networks, one (giant) step at a time. _arXiv preprint arXiv:2305.18270_, 2023.
* Dandi et al. (2024) Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborova, and Florent Krzakala. The benefits of reusing batches for gradient descent in two-layer networks: Breaking the curse of information and leap exponents. _arXiv preprint arXiv:2402.03220_, 2024.
* Dudley (2018) Richard M Dudley. _Real analysis and probability_. Chapman and Hall/CRC, 2018.
* Edelman et al. (2024) Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Pareto frontiers in deep feature learning: Data, compute, width, and luck. _Advances in Neural Information Processing Systems_, 36, 2024.
* Feldman et al. (2017) Vitaly Feldman, Elena Grigorescu, Lev Reyzin, Santosh S Vempala, and Ying Xiao. Statistical algorithms and a lower bound for detecting planted cliques. _Journal of the ACM (JACM)_, 64(2):1-37, 2017.
* Glasgow (2023) Margalit Glasgow. Sgd finds then tunes features in two-layer neural networks with near-optimal sample complexity: A case study in the xor problem. In _The Twelfth International Conference on Learning Representations_, 2023.
* Hopkins (2018) Samuel Hopkins. _Statistical inference and the sum of squares method_. PhD thesis, Cornell University, 2018.
* Hornik (1991) Kurt Hornik. Approximation capabilities of multilayer feedforward networks. _Neural networks_, 4(2):251-257, 1991.
* Kearns (1998) Michael Kearns. Efficient noise-tolerant learning from statistical queries. _Journal of the ACM (JACM)_, 45(6):983-1006, 1998.
* Kou et al. (2024) Yiwen Kou, Zixiang Chen, Quanquan Gu, and Sham M Kakade. Matching the statistical query lower bound for k-sparse parity problems with stochastic gradient descent. _arXiv preprint arXiv:2404.12376_, 2024.
* Kunisky et al. (2019) Dmitriy Kunisky, Alexander S Wein, and Afonso S Bandeira. Notes on computational hardness of hypothesis testing: Predictions using the low-degree likelihood ratio. In _ISAAC Congress (International Society for Analysis, its Applications and Computation)_, pages 1-50. Springer, 2019.
* Malach et al. (2021) Eran Malach, Pritish Kamath, Emmanuel Abbe, and Nathan Srebro. Quantifying the benefit of using differentiable learning over tangent kernels. In _International Conference on Machine Learning_, pages 7379-7389. PMLR, 2021.
* Mei et al. (2018) Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. _Proceedings of the National Academy of Sciences_, 115(33):E7665-E7671, 2018.
* Mousavi-Hosseini et al. (2022) Alireza Mousavi-Hosseini, Sejun Park, Manuela Giroti, Ioannis Mitliagkas, and Murat A Erdogdu. Neural networks efficiently learn low-dimensional representations with sgd. _arXiv preprint arXiv:2209.14863_, 2022.
* Newman and Slater (1979) DJ Newman and Morton Slater. Waring's problem for the ring of polynomials. _Journal of Number Theory_, 11(4):477-487, 1979.
* Perry et al. (2018) Amelia Perry, Alexander S Wein, Afonso S Bandeira, and Ankur Moitra. Optimality and sub-optimality of pca i: Spiked random matrix models. _The Annals of Statistics_, 46(5):2416-2451, 2018.
* Refinetti et al. (2021) Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborova. Classifying high-dimensional gaussian mixtures: Where kernel methods fail and neural networks succeed. In _International Conference on Machine Learning_, pages 8936-8947. PMLR, 2021.
* Reyzin (2020) Lev Reyzin. Statistical queries and statistical algorithms: Foundations and applications. _arXiv preprint arXiv:2004.00557_, 2020.
* Reyzin et al. (2019)Grant M Rotskoff and Eric Vanden-Eijnden. Neural networks as interacting particle systems: Asymptotic convexity of the loss landscape and universal scaling of the approximation error. _stat_, 1050:22, 2018.
* Sirignano and Spiliopoulos [2020] Justin Sirignano and Konstantinos Spiliopoulos. Mean field analysis of neural networks: A central limit theorem. _Stochastic Processes and their Applications_, 130(3):1820-1852, 2020.
* Song et al. [2017] Le Song, Santosh Vempala, John Wilmes, and Bo Xie. On the complexity of learning neural networks. _Advances in neural information processing systems_, 30, 2017.
* Sonoda and Murata [2017] Sho Sonoda and Noboru Murata. Neural network with unbounded activation functions is universal approximator. _Applied and Computational Harmonic Analysis_, 43(2):233-268, 2017.
* Valiant [2012] Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and juntas. In _2012 IEEE 53rd Annual Symposium on Foundations of Computer Science_, pages 11-20. IEEE, 2012.
* Vempala and Wilmes [2019] Santosh Vempala and John Wilmes. Gradient descent for one-hidden-layer neural networks: Polynomial convergence and sq lower bounds. In _Conference on Learning Theory_, pages 3115-3117. PMLR, 2019.

Additional discussions from the main text

### Necessity of Assumption 2.1

We further elaborate on the necessity of Assumption 2.1 in our setting. Informally, we require the label to have "enough noise". This is necessary to obtain meaningful lower-bounds for general SQ algorithms, as noted in Valiant (2012); Song et al. (2017); Vempala and Wilmes (2019). When learning real-valued target function classes, allowing for general measurable queries is too weak (note that arbitrary measurable functions are not practical anyway). Indeed, Vempala and Wilmes (2019) showed that any finite set \(\mathcal{H}\) of (noiseless) functions can be learned with \(\log|\mathcal{H}|\) (bounded) queries and constant tolerance8. They identify three possible approaches to address this challenge: (i) Require \(y\mapsto\phi(y,\bm{x})\) to be Lipschitz for every fixed \(\bm{x}\); (ii) Insist on noisy concepts, e.g., \(y=f(\bm{x})+\zeta\), with \(\zeta\sim\mathsf{N}(0,\sigma^{2})\), which is equivalent to Lipschitz queries; (iii) Restrict the form of the queries, such as in \(\mathsf{CSQ}\). Our Assumption 2.1 can be viewed as a quantitative version of approach (ii). For approach (iii), if we restrict ourselves to \(\mathsf{CSQ}\) or \(\mathsf{DLQ}_{\ell}\) with \(\ell\) piecewise analytic, we can indeed remove Assumption 2.1 by modifying the proof of Theorem 5.1.

Footnote 8: The result holds under a “non-degeneracy” condition, namely, for any \(f,g\in\mathcal{H}\), \(f\neq g\) almost surely. Consider for example \(y=x_{s_{*}(1)}\) with \(\bm{x}\sim\mathsf{N}(\mathbf{0},\mathbf{I}_{d})\): using the construction in Vempala and Wilmes (2019), we can learn \(\mathcal{H}_{\mu}^{d}\) with \(q/\tau^{2}=\Theta(\log(d))\) queries instead of \(q/\tau^{2}=\Theta(d)\) as in Theorem 5.1.

### The conjectural picture of complexity of learning (\(\ell\)-\(\mathsf{bSGD}\))

For \(k_{*}:=\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}(\mu)\), learning the junta problem with online SGD on loss \(\ell\) requires

\[k_{*} =1: n=\Theta(d), T_{\mathsf{C}} =\Theta(d^{2}),\] \[k_{*} =2: n=\Theta(d\log d), T_{\mathsf{C}} =\widetilde{\Theta}(d^{2}),\] \[k_{*} >2: n=\widetilde{\Theta}(d^{k_{*}-1}), T_{\mathsf{C}} =\widetilde{\Theta}(d^{k_{*}}).\]

Partial evidence was provided towards this conjecture for the square loss in Abbe et al. (2023); Bietti et al. (2023): they showed that learning in this setting happens through a saddle-to-saddle dynamics, where each time the neural network needs to align to \(k>1\) new coordinates, a saddle appears and SGD requires \(\widetilde{\Theta}(d^{k-1})\) steps to escape.

### Identifiability and Relative Leap and Cover Complexities

The \(\mathsf{Leap}\) and \(\mathsf{Cover}\) complexity may be infinite when \(\operatorname{supp}(\mathcal{C}_{\mathsf{A}})=\cup_{U\in\mathcal{C}_{\mathsf{ A}}}U\subsetneq[P]\), in which case Theorem 5.1 (correctly) implies that we cannot recover the relevant coordinates that correspond to \([P]\setminus\operatorname{supp}(\mathcal{C}_{\mathsf{A}})\) using queries of type \(\mathsf{A}\) (even with perfect precision). In this case, we can characterize the complexity of recovering \(\operatorname{supp}(\mathcal{C}_{\mathsf{A}})\) instead, and this is captured by the following "relative" complexities:

Figure 2: We repeat the same experiment, with the step size \(\eta\propto 1/d\) as in Figure 1 but now considering the order 3 parity function \(h_{*}(\bm{z})=z_{1}z_{2}z_{3}\). Since \(\mathcal{Y}=\{-1,+1\}\), by Proposition 6.1, we have \(\mathsf{Leap}_{\mathsf{SQ}}=\mathsf{Leap}_{\mathsf{CLQ}_{\ell}}=3\) for all the three loss functions considered. We observe that online SGD learns in \(O(d^{2})\) iterations for all the losses.

**Definition 3**.: _For any system of subsets \(\mathcal{C}\subseteq 2^{[P]}\) the relative leap and cover complexities are:_

\[\mathsf{relLeap}(\mathcal{C}):=\min_{U_{1},\ldots,U_{r}\in\mathcal{C }}\ \ \max_{i\in[r]}\ |U_{i}\setminus\cup_{j=0}^{i-1}U_{j}|,\] (17) \[\mathsf{relCover}(\mathcal{C}):=\max_{i\in\mathrm{supp}(\mathcal{C })}\ \min_{U\in\mathcal{C},i\in U}\ |U|.\] (18)

A slight variant of Theorem 5.1 can then be shown, where \(\mathsf{relLeap}_{\mathsf{A}}(\mu)\) and \(\mathsf{relCover}_{\mathsf{A}}(\mu)\) for \(\mathsf{A}\in\{\mathsf{SQ},\mathsf{CSQ},\mathsf{DLQ}_{\ell}\}\) characterizes the complexity of recovering \(\mathrm{supp}(\mathcal{C}_{\mathsf{A}})\).

For \(\mathsf{SQ}\), we may have \(\mathrm{supp}(\mathcal{C}_{\mathsf{SQ}})\subsetneq[P]\) only when \(y\) doesn't actually depend on some of the coordinates in \([P]\) (i.e. \(y\mid\bm{z}=y|(\bm{z}_{i})_{i\in\mathrm{supp}(\mathcal{C})}\)). In this case, once we recover \(\mathrm{supp}(\mathcal{C})\) (and possibly enumerating over permutations of its coordinates) we can also recover the conditional \(y|\bm{x}\). That is, \(\mathsf{relLeap}_{\mathsf{SQ}}\) and \(\mathsf{relCover}_{\mathsf{SQ}}\) characterize the complexity of learning the distribution \(\mathcal{D}_{s_{*}}\), even if not the (unidentifiable) set \(S_{*}\).

For \(\mathsf{DLQ}_{\ell}\), including \(\mathsf{CSQ}\), we may not be able to identify some coordinates even if \(y\) does depend on them. Consider for example the junta problem of size \(P=2\) where \(y|\bm{z}=z_{1}+\mathcal{N}(0,z_{2}^{2})\). Although \(y\) does depend on the second coordinate \(z_{2}\), it is not identifiable using \(\mathsf{CSQ}\) queries, \(\mathrm{supp}(\mathcal{C}_{\mathsf{CSQ}})=\{1\}\), and using \(\mathsf{CSQ}\) queries we cannot recover the conditional distribution \(\mathcal{D}_{s_{*}}\) (even if we know the link \(\mu_{y|\bm{z}}\)). What we _can_ say for \(\mathsf{DLQ}_{\ell}\) after recovering \(\hat{S}=\mathrm{supp}(\mathcal{C}_{\mathsf{DLQ}_{\ell}})\), is that although we might not know \(\mathcal{D}_{s_{*}}\), we can find the \(\ell\)-risk minimizer \(f_{*}=\arg\min_{f:X\to\mathcal{F}}\mathbb{E}_{(y,\bm{x})\sim\mathcal{D}_{s_{ *}}}[\ell(f(x),y)]\). That is, \(\mathsf{relLeap}_{\mathsf{DLQ}_{\ell}}\) and \(\mathsf{relCover}_{\mathsf{DLQ}_{\ell}}\) characterize the complexity of \(\ell\)-risk minimization.

### Property of SQ Leap and Cover Complexities

We show a useful property of the \(\mathsf{SQ}\)-exponent. This characterization is similar to the definition of the generative exponent for single-index models on Gaussian data in (Damian et al., 2024b, Definition 2.4), while the detectable set Definition 2 is similar to their variational representation (Damian et al., 2024b, Proposition 2.6).

**Proposition A.1** (Property of \(\mathsf{SQ}\)-detectable sets).: \(U\in\mathcal{C}_{\mathsf{SQ}}\) _if and only if there exists \(T_{i}\in L_{0}^{2}(\mu_{x})\) for all \(i\in U\) such that \(\|\xi_{U}\|_{L^{2}(\mu_{y})}>0\) where_

\[\xi_{U}(y)=\mathbb{E}_{\mu_{y,*}}\Big{[}\prod_{i\in U}T_{i}(z_{i})\Big{|}y \Big{]}.\]

Proof of Proposition a.1.: In the forward direction, consider any \(U\in\mathcal{C}_{\mathsf{SQ}}\), then by Definition 2, there exists \(T_{i}\in L_{0}^{2}(\mu_{x})\) and \(T\in L^{2}(\mu_{y})\) such that

\[0\neq\mathbb{E}_{\mu_{y,*}}\left[T(y)\prod_{i\in U}T_{i}(z_{i})\right]= \mathbb{E}_{\mu_{y}}\left[T(y)\xi_{U}(y)\right],\]

where we defined \(\xi_{U}(y):=\mathbb{E}_{\mu_{y,*}}\left[\prod_{i\in U}T_{i}(z_{i})\mid y\right]\). We then have

\[0<|\mathbb{E}_{\mu_{y}}\left[T(y)\xi_{U}(y)\right]|\leq\|T\|_{L^{2}(\mu_{y})} \|\xi_{U}\|_{L^{2}(\mu_{y})},\ \text{by Cauchy-Schwarz inequality}.\]

Therefore, we conclude that \(\|\xi_{U}\|_{L^{2}(\mu_{y})}>0\) as desired. In the opposite direction, let us consider \(U\subseteq[P]\) such that there exists \(T_{i}\in L_{0}^{2}(\mu_{x})\) for every \(i\in U\) such that \(\|\xi_{U}\|_{L^{2}(\mu_{y})}>0\), where

\[\xi_{U}(y)=\mathbb{E}_{\mu_{y,*}}\left[\prod_{i\in U}T_{i}(z_{i})\mid y\right].\]

Then let \(T(y):=\xi_{U}(y)\). It is straightforward to verify that

\[\|T\|_{\mu_{y}}^{2}=\mathbb{E}_{\mu_{y}}\left[T(y)^{2}\right]=\mathbb{E}_{\mu_ {y}}\left[\mathbb{E}_{\mu_{y,*}}\left[\prod_{i\in U}T_{i}(z_{i})\mid y\right]^ {2}\right]\leq\mathbb{E}_{\mu_{y,*}}\left[\prod_{i\in U}T_{i}(z_{i})^{2} \right]<\infty.\]We verified \(T(y)\in L^{2}(\mu_{y})\). Moreover,

\[\mathbb{E}_{\mu_{y}}\left[T(y)\prod_{i\in U}T_{i}(z_{i})\right] =\mathbb{E}_{\mu_{y}}\left[T(y)\mathbb{E}_{\mu_{y,\bm{z}}}\left[ \prod_{i\in U}T_{i}(z_{i})\mid y\right]\right]\] \[=\mathbb{E}_{\mu_{y}}\left[\xi_{U}(y)\xi_{U}(y)\right]=\|\xi_{U} \|_{L^{2}(\mu_{y})}^{2}\neq 0.\]

Therefore, by Definition 2, we obtain \(U\in\mathcal{C}_{\mathsf{SQ}}\). 

## Appendix B Proof of Theorem 5.1

### Preliminaries

Tensor basis of \(L^{2}(\mathcal{X}^{k},\mu_{x}^{k})\):Since we assumed that \((\mathcal{X},\mu_{x})\) is a Polish probability space, \(L^{2}(\mathcal{X},\mu_{x})\) is separable [10]. Consider \(\{\psi_{i}\}_{i\in\mathcal{I}}\), \(\mathcal{I}\subseteq\mathbb{N}\), an orthonormal basis of \(L^{2}(\mathcal{X},\mu_{x})\) such that \(\psi_{0}=1\) without loss of generality. In particular, the space \(L^{2}(\mathcal{X}^{k},\mu_{x}^{k})\) admits the following tensor basis

\[\{\psi_{\bm{i}}:=\psi_{i_{1}}\otimes\psi_{i_{2}}\otimes\ldots \otimes\psi_{i_{k}}\;:\;\bm{i}=(i_{1},\ldots,i_{k})\in\mathcal{I}^{k}\}.\] (19)

We will denote \(\operatorname{supp}(\bm{i})=\{j\in[d]:i_{j}>0\}\).

Distributions \(\nu_{\mathcal{S}}^{\sigma}\):For any non-repeating sequence \(\sigma\in\mathsf{P}([d],P)\) of length \(P\), a subset \(U\subseteq[P]\), and a system of subsets \(\mathcal{C}=\{U_{1},\ldots,U_{m}\}\subseteq 2^{[P]}\), we denote \(\sigma(U):=\{\sigma(i):i\in U\}\) and \(\sigma(\mathcal{C}):=\{\sigma(U_{1}),\ldots,\sigma(U_{m})\}\). The following analysis holds for fix \(\mu\) and hence we will often omit the subscript \(\mu\) by denoting \(\mathcal{D}_{\mu,\sigma}^{d}=\mathcal{D}_{\sigma}^{d}\) for any \(\sigma\in\mathsf{P}([d],P)\). For \(\sigma=\operatorname{Id}\), i.e. \(\sigma(1)=1,\ldots,\sigma(P)=P\), we will further omit writing \(\operatorname{Id}\) and denote \(\mathcal{D}^{d}=\mathcal{D}_{\operatorname{Id}}^{d}\). For \(U\subseteq[P]\), \(\mu_{y,\bm{z},U}^{0}\in\mathcal{P}(\mathcal{Y}\times\mathcal{X}^{P})\) is the distribution of \((y_{U},\bm{z})\) corresponding to decoupling \(y\) from \((z_{i})_{i\notin U}\) in \(\mu_{y,\bm{z}}\) following Eq. (6), i.e., \((y_{U},(z_{i})_{i\in U})\sim\mu_{y,\bm{z},U}\) and \((z_{i})_{i\notin U}\sim\mu_{x}^{P-|U|}\) independently.

For clarity, for every \(U\subseteq[P]\) and \(\sigma\in\mathsf{P}([d],P)\),

\[\nu_{U}^{\sigma}:=[\operatorname{Id}\otimes\sigma]_{\#}[\mu_{y,\bm{z},U}^{0} \otimes\mu_{x}^{d-P}],\] (20)

where we abuse the notation and view \(\sigma\) as a permutation in \(\Pi_{d}\), by extending the sequence \(\sigma\in\mathsf{P}([d],P)\) to an ordering of irrelevant coordinates, as notice that it yields the same \(\nu_{U}^{\sigma}\). In words, \(\nu_{U}^{\sigma}\) is the distribution of \((y,\bm{x})\) where we decoupled \(y\) from \((x_{\sigma(i)})_{i\notin U}\) in \(\mathcal{D}_{\sigma}^{d}\). For \(\sigma=\operatorname{Id}\), we denote \(\nu_{U}=\nu_{U}^{\operatorname{Id}}\). For \(U=[P]\), we simply denote \(\nu^{\sigma}:=\mathcal{D}_{\sigma}^{d}\). Note that \(\nu_{0}:=\nu_{\emptyset}=\mu_{y}\otimes\mu_{x}^{d}\) and \(\nu_{U}^{\sigma}=\nu_{U}\) if \(\sigma(i)=i\) for all \(i\in U\).

Basic properties under Assumption 2.1.Recall that our lower bounds in Theorem 5.1 are under Assumption 2.1. It states that the junta problem \(\mu=(\mu_{x},\mu_{y|\bm{z}})\) is such that Eq. (7) holds. An immediate consequence of this assumption, using Jensen's inequality is that

\[\frac{\mathrm{d}\mu_{y,\bm{z},U}^{0}}{\mathrm{d}\mu_{y}\otimes\mu_{x}^{P}}\in L ^{2}(\mu_{y}\otimes\mu_{x}^{P}).\] (21)

Indeed, this follows from

\[\left\|\frac{\mathrm{d}\mu_{y,\bm{z},U}^{0}}{\mathrm{d}\mu_{y}\otimes\mu_{x}^{P }}\right\|_{L^{2}(\mu_{y}\otimes\mu_{x}^{P})}^{2}\leq\left\|\frac{\mathrm{d} \mu_{y,\bm{z}}}{\mathrm{d}\mu_{y}\otimes\mu_{x}^{P}}\right\|_{L^{2}(\mu_{y} \otimes\mu_{x}^{P})}^{2}<\infty,\]

where the first inequality follows from Jensen's inequality and the second by (7).

Another consequence of this assumption is that, for any \(U\subseteq[P]\) and \(\sigma\in\mathsf{P}([d],P)\), we have \(L^{2}(\nu_{0})\subseteq L^{1}(\nu_{U}^{\sigma})\). This is because for any \(\phi\in L^{2}(\nu_{0})\)

\[\|\phi\|_{L^{1}(\nu_{U}^{\sigma})}=\mathbb{E}_{(y,\bm{x})\sim\nu_{U}^{\sigma}} \left[\|\phi(y,\bm{x})\|\right]=\mathbb{E}_{(y,\bm{x})\sim\nu_{0}}\left[ \frac{\mathrm{d}\nu_{U}^{\sigma}}{\mathrm{d}\nu_{0}}(y,\bm{x})\cdot|\phi(y, \bm{x})|\right]\leq\left\|\frac{\mathrm{d}\nu_{U}^{\sigma}}{\mathrm{d}\nu_{0}} \right\|_{L^{2}(\nu_{0})}\|\phi\|_{L^{2}(\nu_{0})}<\infty,\]

where we used Cauchy-Schwarz inequality and Eq. (21).

Algorithms in A:Throughout the proof, we consider \(\textsf{A}\in\{\textsf{SQ},\textsf{CSQ},\textsf{DLQ}_{\ell}\}\) and \(\mathcal{C}_{\textsf{A}}:=\mathcal{C}_{\textsf{A}}(\mu)\) defined in Definition 2. First, observe that one can alternatively define \(\mathcal{Q}_{\textsf{SQ}}=L^{2}(\nu_{0})\) because for any measurable \(\phi\notin L^{2}(\nu_{0})\), the received query response can infinite by (8) and the learner does not gain any information. Therefore, it suffices to prove lower bound against the query set \(\mathcal{Q}_{\textsf{SQ}}=L^{2}(\nu_{0})\). For the same reason, consider the allowed query sets, \(\mathcal{Q}_{\textsf{CSQ}}\subseteq L^{2}(\nu_{0})\) containing queries of the form \(\phi(y,\bm{x})=y\tilde{\phi}(\bm{x})\), and \(\mathcal{Q}_{\textsf{DLQ}_{\ell}}\subseteq L^{2}(\nu_{0})\) with queries of the form

\[\phi(y,\bm{x})=\left[\frac{\mathrm{d}}{\mathrm{d}\omega}f(\bm{x},\omega) \right]^{\mathsf{T}}_{\omega=0}\nabla\ell(f(\bm{x},0),y).\] (22)

We claim that, for \(\bm{i}\in\mathcal{I}^{d}\) and \(\psi_{i}\) defined in Eq. (19) and any \(\phi\in\mathcal{Q}_{\textsf{A}}\), Definition 2 implies that if \(\mathrm{supp}(\bm{i})\not\in\sigma(\mathcal{C}_{\textsf{A}})\), then almost surely over \(\tilde{\bm{x}}\sim\mu_{x}^{d}\)

\[\mathbb{E}_{(y,\bm{x})\sim\nu^{\sigma}}\left[\phi(y,\tilde{\bm{x}})\psi_{i}( \bm{x})\right]=0.\] (23)

This follows from Definition 2 of \(\Psi\)-detectibility with Eq. (14), and using the fact that \(\psi_{i}\) is the product of \(\psi_{i_{j}}(x_{j})\) with \(\psi_{i_{j}}\in L_{0}^{2}(\mu_{x})\) for \(j\in\mathrm{supp}(\bm{i})\). More specifically, since \(\phi\in L^{2}(\nu_{0})\), the function \(\phi(\cdot,\tilde{\bm{x}})\in L^{2}(\mu_{y})\) almost surely over \(\tilde{\bm{x}}\sim\mu_{x}^{d}\), and In particular, almost surely over \(\tilde{\bm{x}}\sim\mu_{x}^{d}\):

\[\text{For any }\phi\in\mathcal{Q}_{\textsf{SQ}}, \text{we have }\phi(\cdot,\tilde{\bm{x}})\in L^{2}(\mu_{y})=\Psi_{ \textsf{SQ}},\] \[\text{For any }\phi\in\mathcal{Q}_{\textsf{CSQ}}, \text{we have }\phi(\cdot,\tilde{\bm{x}})=y\tilde{\phi}(\tilde{\bm{x}})\text{ (i.e. a scaled identity)},\] \[\text{For any }\phi\in\mathcal{Q}_{\textsf{DLQ}_{\ell}}, \text{we have }\phi(\cdot,\tilde{\bm{x}})\in\Psi_{\textsf{DLQ}_{\ell}}\text{ (i.e. }\phi(\cdot,\tilde{\bm{x}})\text{ of the form }\bm{\bm{v}}^{\mathsf{T}}\nabla\ell(\bm{u},y)).\]

### Theorem 5.1.(a): lower bound for adaptive queries

Definition of set \(\mathcal{S}_{*}\):Let \(k_{*}=\mathsf{Leap}(\mathcal{C}_{\textsf{A}})\). Define \(\mathcal{C}_{*}\subset\mathcal{C}_{\textsf{A}}\) to be the maximal subset of \(\mathcal{C}_{\textsf{A}}\) such that \(\mathsf{Leap}(\mathcal{C}_{*})\leq k_{*}-1\), and \(\mathcal{S}_{*}=\cup_{U\in\mathcal{C}_{*}}U\). In words, \(\mathcal{S}_{*}\) is the collection of all coordinates in the support that can be reached by adding at most \(k_{*}-1\) coordinates at a time. In particular, by definition of \(\mathcal{C}_{*}\), we have \(|U\setminus\mathcal{S}_{*}|\geq k_{*}\) for every \(U\in\mathcal{C}_{\textsf{A}}\setminus\mathcal{C}_{*}\). Denote \(r=|\mathcal{S}_{*}|\) and without loss of generality, assume that \(\mathcal{S}_{*}=\{1,\ldots,r\}\). We decompose the covariate into \(\bm{x}=(\bm{x}_{\mathcal{S}_{*}},\bm{x}_{\mathcal{S}_{*}^{c}})\) where \(\mathcal{S}_{*}^{c}=[d]\setminus\mathcal{S}_{*}\). Using these notations, \((y^{\not\!\mathcal{S}_{*}}_{\mathcal{S}_{*}},\bm{x})\sim\nu^{\not\!\mathcal{S} _{*}}_{\mathcal{S}_{*}}\) has \((y^{\not\!\mathcal{S}_{*}}_{\mathcal{S}_{*}},\bm{x}_{\sigma(\mathcal{S}_{*})} )\sim\mathcal{D}_{y,\bm{z},\mathcal{S}_{*}}\) and \(\bm{x}_{\sigma(\mathcal{S}_{*}^{c})}\sim\mu_{x}^{d-r}\) independently.

Definition of \(\mathcal{H}^{d}_{\mu,\mathcal{S}_{*}}\):We introduce \(\texttt{P}([d],P,\mathcal{S}_{*})\subseteq\texttt{P}([d],P)\) the subset of non-repeating sequence of length \([P]\) from elements \([d]\) such that \(\sigma(i)=i\) for all \(i\in\mathcal{S}_{*}\), and \(\mathcal{H}^{d}_{\mu,\mathcal{S}_{*}}\subseteq\mathcal{H}^{d}_{\mu}\) the subset of hypotheses \(\mathcal{D}^{d}_{\sigma}\) with \(\sigma\in\texttt{P}([d],P,\mathcal{S}_{*})\). Recall that for all \(\sigma\in\texttt{P}([d],P,\mathcal{S}_{*})\), we have \(\nu^{\sigma}_{\mathcal{S}_{*}}=\nu_{\mathcal{S}_{*}}\). To prove our lower bound, we will lower bound the complexity of learning \(\mathcal{H}^{d}_{\mu,\mathcal{S}_{*}}\) which implies a lower bound on \(\mathcal{H}^{d}_{\mu}\). Specifically, we will show that with \(q/\tau^{2}\leq cd^{k_{*}}\) for some constant \(c>0\) that only depends on \(P,\mu\) (and the loss \(\ell\) for \(\textsf{DLQ}_{\ell}\)), statistical query algorithms in A cannot distinguish for all \(\sigma\in\texttt{P}([d],P,\mathcal{S}_{*})\) between \(\nu^{\sigma}=\mathcal{D}^{d}_{\sigma}\) and \(\nu_{\mathcal{S}_{*}}\).

Proof outline:For any \(\sigma\in\texttt{P}([d],P,\mathcal{S}_{*})\) and query \(\phi\in\mathcal{Q}_{\textsf{A}}\subseteq L^{2}(\nu_{0})\), define

\[\Delta_{\sigma}(\phi):=\mathbb{E}_{\nu^{\sigma}}[\phi]-\mathbb{E}_{\nu_{\mathcal{ S}_{*}}}[\phi].\] (24)

We will show below that there exists a constant \(C>0\) that only depends on \(\mu\) such that

\[\sup_{\phi\in\mathcal{Q}_{\textsf{A}}}\frac{\mathbb{E}_{\sigma}[\Delta_{\sigma}( \phi)^{2}]}{\|\phi\|_{L^{2}(\nu_{0})}^{2}}\leq Cd^{-k_{*}},\] (25)

where \(\mathbb{E}_{\sigma}\) denote the expectation with respect to \(\sigma\sim\mathrm{Unif}(\texttt{P}([d],P,\mathcal{S}_{*}))\). The intuition behind this crucial bound in (25) is that we defined the null distribution as the distribution with matching marginals \((y,\bm{x})\) where the label only depends on the support of \(\bm{x}\) discoverable with "leap" \(k_{*}\). Hence, to make any progress, any new detectable set added to the support needs to contain at least \(k_{*}\) new coordinates. In the second moment, to have a non-zero contribution, we need the permutation to map the support to a detectable subset, i.e. the permutation to map to a subset of size at least \(k_{*}\), which has probability \(O(d^{-k_{*}})\) over permutation.

Let \(\phi_{1},\ldots,\phi_{q}\) be the sequence of adaptive queries with responses \(\mathbb{E}_{\nu_{\mathcal{S}_{*}}}[\phi_{t}]\). Then, by Markov's inequality,

\[\mathbb{P}_{\sigma}\left(\exists t\in[q],\ |\Delta_{\sigma}(\phi_{t})|>\tau\|\phi \|_{L^{2}(\nu_{0})}\right)\leq\frac{q}{\tau^{2}}\sup_{\phi\in\mathcal{Q}_{ \Lambda}}\frac{\mathbb{E}_{\sigma}[\Delta_{\sigma}(\phi)^{2}]}{\|\phi\|_{L^{2 }(\nu_{0})}^{2}}\leq C\frac{qd^{-k_{*}}}{\tau^{2}}.\] (26)

Hence, for \(q/\tau^{2}<d^{k_{*}}/C\), there exists \(\sigma\in\mathtt{P}([d],P,\mathcal{S}_{*})\) such that for every \(t\in[q]\), we have \(|\mathbb{E}_{\nu^{\sigma}}[\phi_{t}]-\mathbb{E}_{\nu_{\mathcal{S}_{*}}}[\phi_{ t}]|\leq\tau\|\phi_{t}\|_{L^{2}(\nu_{0})}\). Therefore Using Eq (26), we obtain that that there are two members from \(\mathcal{H}_{\bm{\mu}}^{d}\) whose query responses are compatible with the null \(\nu_{\mathcal{S}_{*}}\), and hence also mutually compatible and indistinguishable from each other. This then establishes the failure of the learning algorithm in recovering the correct support.

Decomposing \(\Delta_{\sigma}(\phi)\):We rewrite \(\Delta_{\sigma}(\phi)\) in terms of \(\mathbb{E}_{\nu_{\mathcal{S}_{*}}}\) using the Radon-Nikodym derivative:

\[\Delta_{\sigma}(\phi)=\mathbb{E}_{\nu_{\mathcal{S}_{*}}}\left[\frac{\mathrm{d }\nu^{\sigma}}{\mathrm{d}\nu_{\mathcal{S}_{*}}}\phi\right]-\mathbb{E}_{\nu_{ \mathcal{S}_{*}}}[\phi]=\mathbb{E}_{\nu_{\mathcal{S}_{*}}}\left[\left(\frac{ \mathrm{d}\nu^{\sigma}}{\mathrm{d}\nu_{\mathcal{S}_{*}}}(y_{\mathcal{S}_{*}}, \bm{x})-1\right)\phi(y_{\mathcal{S}_{*}},\bm{x})\right].\] (27)

Recall that \((y_{\mathcal{S}_{*}},\bm{x}_{\mathcal{S}_{*}})\) is independent of \(\bm{x}_{\sigma(\mathcal{S}_{*}^{c})}\sim\mu_{x}^{d-r}\) under \(\nu_{\mathcal{S}_{*}}\), and that \(\mathrm{d}\nu^{\sigma}/\mathrm{d}\nu_{\mathcal{S}_{*}}\in L^{2}(\nu_{\mathcal{ S}_{*}})\) by Assumption 2.1. Therefore, we have the following orthogonal decomposition in \(L^{2}(\nu_{\mathcal{S}_{*}})\) using the tensor basis (19):

\[\frac{\mathrm{d}\nu^{\sigma}}{\mathrm{d}\nu_{\mathcal{S}_{*}}}(y_{\mathcal{S}_ {*}},\bm{x})-1=\sum_{\bm{i}\in\mathcal{I}^{d-r}\setminus\{\bm{0}\}}\xi_{\bm{ i}}^{\sigma}(y_{\mathcal{S}_{*}},\bm{x}_{\mathcal{S}_{*}})\psi_{\bm{i}}(\bm{x}_{ \mathcal{S}_{*}^{c}}),\] (28)

where

\[\xi_{\bm{i}}^{\sigma}(y_{\mathcal{S}_{*}},\bm{x}_{\mathcal{S}_{*}}):=\mathbb{ E}_{\nu_{\mathcal{S}_{*}}}\left[\frac{\mathrm{d}\nu^{\sigma}}{\mathrm{d}\nu_{ \mathcal{S}_{*}}}(y_{\mathcal{S}_{*}},\bm{x})\psi_{\bm{i}}(\bm{x}_{\mathcal{S} _{*}^{c}})\Big{|}y_{\mathcal{S}_{*}},\bm{x}_{\mathcal{S}_{*}}\right]=\mathbb{ E}_{\nu^{\sigma}}\left[\psi_{\bm{i}}(\bm{x}_{\mathcal{S}_{*}^{c}})\big{|}y_{ \mathcal{S}_{*}},\bm{x}_{\mathcal{S}_{*}}\right].\] (29)

Similarly, we decompose \(\phi\) orthogonally in \(L^{2}(\nu_{\mathcal{S}_{*}})\):

\[\phi(y_{\mathcal{S}_{*}},\bm{x})=\sum_{\bm{i}\in\mathcal{I}^{d-r}}\alpha_{\bm{ i}}(y_{\mathcal{S}_{*}},\bm{x}_{\mathcal{S}_{*}})\psi_{\bm{i}}(\bm{x}_{\mathcal{S}_{ *}^{c}}),\] (30)

where

\[\alpha_{\bm{i}}(y_{\mathcal{S}_{*}},\bm{x}_{\mathcal{S}_{*}}):=\mathbb{E}_{\nu _{\mathcal{S}_{*}}}\left[\phi(y_{\mathcal{S}_{*}},\bm{x})\psi_{\bm{i}}(\bm{x}_ {\mathcal{S}_{*}^{c}})\big{|}y_{\mathcal{S}_{*}},\bm{x}_{\mathcal{S}_{*}} \right].\] (31)

We deduce that we can decompose \(\Delta_{\sigma}(\phi)\) as

\[\Delta_{\sigma}(\phi)=\sum_{\bm{i}\in\mathcal{I}^{d-r}\setminus\{\bm{0}\}} \mathbb{E}_{\nu_{\mathcal{S}_{*}}}\left[\xi_{\bm{i}}^{\sigma}(y_{\mathcal{S}_{ *}},\bm{x}_{\mathcal{S}_{*}})\alpha_{\bm{i}}(y_{\mathcal{S}_{*}},\bm{x}_{ \mathcal{S}_{*}})\right].\] (32)

For convenience, denote \(m_{\bm{i}}^{\sigma}\) the summand. From the expressions (29) and (31) and by Fubini's theorem, we can rewrite \(m_{\bm{i}}^{\sigma}\) as

\[\begin{split} m_{\bm{i}}^{\sigma}&=\mathbb{E}_{y_{ \mathcal{S}_{*}},\bm{x}_{\mathcal{S}_{*}}}\left[\mathbb{E}_{\nu^{\sigma}} \left[\psi_{\bm{i}}(\bm{x}_{\mathcal{S}_{*}^{c}})\big{|}y_{\mathcal{S}_{*}}, \bm{x}_{\mathcal{S}_{*}}\right]\cdot\mathbb{E}_{\tilde{\bm{x}}_{\mathcal{S}_{ *}^{c}}\sim\mu_{x}^{P-r}}\left[\phi(y_{\mathcal{S}_{*}},\bm{x}_{\mathcal{S}_{*}},\tilde{\bm{x}}_{\mathcal{S}_{*}^{c}})\psi_{\bm{i}}(\tilde{\bm{x}}_{\mathcal{S} _{*}^{c}})\right]\right]\\ &=\mathbb{E}_{(y,\bm{x},\tilde{\bm{x}}_{\mathcal{S}_{*}^{c}})\sim \nu^{\sigma}\otimes\mu_{x}^{P-r}}\left[\psi_{\bm{i}}(\bm{x}_{\mathcal{S}_{*}}) \phi(y,\bm{x}_{\mathcal{S}_{*}},\tilde{\bm{x}}_{\mathcal{S}_{*}})\psi_{\bm{i}}( \tilde{\bm{x}}_{\mathcal{S}_{*}^{c}})\right]\\ &=\mathbb{E}_{\tilde{\bm{x}}_{\mathcal{S}_{*}^{c}}\sim\mu_{x}^{P- r}}\left[\psi_{\bm{i}}(\tilde{\bm{x}}_{\mathcal{S}_{*}^{c}})\mathbb{E}_{(y,\bm{x}) \sim\nu^{\sigma}}\left[\phi(y,\bm{x}_{\mathcal{S}_{*}},\tilde{\bm{x}}_{\mathcal{S} _{*}^{c}})\psi_{\bm{i}}(\bm{x}_{\mathcal{S}_{*}^{c}})\right]\right].\end{split}\] (33)

Let us decouple \(y\) from \(\bm{x}_{\mathcal{S}_{*}}\) inside the query \(\phi\) using the Radon-Nikodym derivative: conditional on \(\tilde{\bm{x}}_{\mathcal{S}_{*}^{c}}\), we have

\[\begin{split}\mathbb{E}_{(y,\bm{x})\sim\nu^{\sigma}}& \left[\phi(y,\bm{x}_{\mathcal{S}_{*}},\tilde{\bm{x}}_{\mathcal{S}_{*}^{c}}) \psi_{\bm{i}}(\bm{x}_{\mathcal{S}_{*}^{c}})\right]\\ &=\mathbb{E}_{(y_{\mathcal{S}_{*}^{c}},\bm{x})\sim\nu^{\sigma}_{ \mathcal{S}_{*}^{c}}}\left[\frac{\mathrm{d}\nu^{\sigma}}{\mathrm{d}\nu^{\sigma}_{ \mathcal{S}_{*}^{c}}}(y_{\mathcal{S}_{*}^{c}},\bm{x})\cdot\phi(y_{\mathcal{S}_{*} ^{c}},\bm{x}_{\mathcal{S}_{*}},\tilde{\bm{x}}_{\mathcal{S}_{*}^{c}})\psi_{\bm{i}}( \bm{x}_{\mathcal{S}_{*}^{c}})\right].\end{split}\] (34)

Similarly to Eq. (28), we have the following orthogonal decomposition in \(L^{2}(\nu^{\sigma}_{\mathcal{S}_{*}^{c}})\):

\[\frac{\mathrm{d}\nu^{\sigma}}{\mathrm{d}\nu^{\sigma}_{\mathcal{S}_{*}^{c}}}(y_{ \mathcal{S}_{*}^{c}},\bm{x})=\sum_{\bm{j}\in\mathcal{I}^where

\[\xi_{j}^{\sigma}(y_{\mathcal{S}_{\varepsilon}^{c}},\bm{x}_{\mathcal{S}_{ \varepsilon}^{c}}):=\mathbb{E}_{\nu^{\sigma}}\left[\psi_{j}(\bm{x}_{\mathcal{S}_ {\varepsilon}})\big{|}y_{\mathcal{S}_{\varepsilon}^{c}},\bm{x}_{\mathcal{S}_{ \varepsilon}^{c}}\right].\] (36)

By Fubini's theorem,

\[\mathbb{E}_{(y_{\mathcal{S}_{\varepsilon}^{c}},\tilde{\bm{x}}_{ \mathcal{S}_{\varepsilon}},\bm{x}_{\mathcal{S}_{\varepsilon}^{c}})\sim\nu_{ \mathcal{S}_{\varepsilon}^{c}}^{c}}\left[\xi_{j}^{\sigma}(y_{\mathcal{S}_{ \varepsilon}^{c}},\bm{x}_{\mathcal{S}_{\varepsilon}^{c}})\psi_{j}(\tilde{\bm{ x}}_{\mathcal{S}_{\varepsilon}})\cdot\phi(y_{\mathcal{S}_{\varepsilon}^{c}}, \tilde{\bm{x}}_{\mathcal{S}_{\varepsilon}},\tilde{\bm{x}}_{\mathcal{S}_{ \varepsilon}^{c}})\psi_{i}(\bm{x}_{\mathcal{S}_{\varepsilon}^{c}})\right]\] (37) \[=\mathbb{E}_{\tilde{\bm{x}}_{\mathcal{S}_{\varepsilon}}\sim\mu_{x }^{c}}\left[\psi_{j}(\tilde{\bm{x}}_{\mathcal{S}_{\varepsilon}})\mathbb{E}_{(y,\bm{x})\sim\nu^{\sigma}}\left[\phi(y,\tilde{\bm{x}}_{\mathcal{S}_{\varepsilon }},\tilde{\bm{x}}_{\mathcal{S}_{\varepsilon}^{c}})\psi_{j}(\bm{x}_{\mathcal{S }_{\varepsilon}})\right]\right].\]

Combining Eqs. (34), (35), and (37) into Eq. (33) yields the expression

\[m_{\bm{\mathrm{z}}}^{\sigma}=\sum_{\bm{j}\in\mathcal{I}^{c}}\mathbb{E}_{\tilde {\bm{x}}\sim\mu_{x}^{d}}\left[\psi_{\bm{j}}(\tilde{\bm{x}}_{\mathcal{S}_{ \varepsilon}})\psi_{i}(\tilde{\bm{x}}_{\mathcal{S}_{\varepsilon}^{c}}) \mathbb{E}_{(y,\bm{x})\sim\nu^{\sigma}}\left[\phi(y,\tilde{\bm{x}})\psi_{\bm{ j}}(\bm{x}_{\mathcal{S}_{\varepsilon}})\psi_{i}(\bm{x}_{\mathcal{S}_{ \varepsilon}^{c}})\right]\right].\] (38)

From Eq. (32), we deduce the following simple decomposition

\[\Delta_{\sigma}(\phi)=\sum_{\bm{i}\in\mathcal{I}^{d},\;\mathrm{supp}(\bm{i}) \not\subseteq\mathcal{S}_{\bm{\mathrm{z}}}}\mathbb{E}_{\tilde{\bm{x}}\sim\mu _{x}^{d}}\left[\psi_{\bm{i}}(\tilde{\bm{x}})\mathbb{E}_{(y,\bm{x})\sim\nu^{ \sigma}}\left[\phi(y,\tilde{\bm{x}})\psi_{\bm{i}}(\bm{x})\right]\right].\] (39)

Bounding \(\mathbb{E}_{\sigma}[\Delta_{\sigma}(\phi)^{2}]\):If \(\mathrm{supp}(\bm{i})\not\in\sigma(\mathcal{C}_{\mathsf{A}}\setminus\mathcal{C }_{*})\), we have by Eq. (23) that

\[\mathbb{E}_{\tilde{\bm{x}}\sim\mu_{x}^{d}}\left[\psi_{i}(\tilde{\bm{x}}) \mathbb{E}_{(y,\bm{x})\sim\nu^{\sigma}}\left[\phi(y,\tilde{\bm{x}})\psi_{i}( \bm{x})\right]\right]=0.\] (40)

Therefore we can rewrite the sum in Eq. (39) with

\[\Delta_{\sigma}(\phi) =\sum_{\bm{i}\in\mathcal{I}^{d}}\;\sum_{S\in\mathcal{C}_{ \mathsf{A}}\setminus\mathcal{C}_{*}}\mathbb{E}_{(y,\bm{x},\tilde{\bm{x}})\sim \nu^{\sigma}\otimes\mu_{x}^{d}}\left[\psi_{i}(\tilde{\bm{x}})\phi(y,\tilde{ \bm{x}})\psi_{i}(\bm{x})\right]\cdot\mathbbm{1}[\mathrm{supp}(\bm{i})=\sigma(S)]\] (41) \[=\sum_{\bm{i}\in\mathcal{I}^{d}}\;\sum_{S\in\mathcal{C}_{\mathsf{A }}\setminus\mathcal{C}_{*}}\mathbb{E}_{(y,\bm{x},\tilde{\bm{x}})\sim\nu\otimes \mu_{x}^{d}}\left[\psi_{i}(\tilde{\bm{x}})\phi(y,\tilde{\bm{x}})\psi_{i}(\bm{ x})\right]\cdot\mathbbm{1}[\sigma(\mathrm{supp}(\bm{i}))=S].\]

Recall that for every \(S\in\mathcal{C}_{\mathsf{A}}\setminus\mathcal{C}_{*}\) by construction. Hence,

\[\mathbb{E}_{\sigma}\left[\sum_{S_{1},S_{2}\in\mathcal{C}_{\mathsf{A}}\setminus \mathcal{C}_{*}}\mathbbm{1}[\sigma(\mathrm{supp}(\bm{i}))=S_{1},\sigma( \mathrm{supp}(\bm{j}))=S_{2}]\right]\leq Cd^{-k_{*}}.\] (42)

By squaring Eq. (41) and the previous display, we deduce that

\[\mathbb{E}_{\sigma}[\Delta_{\sigma}(\phi)^{2}]\] (43) \[\leq Cd^{-k_{*}}\left(\sum_{\bm{i}\in\mathcal{I}^{d}}\left| \mathbb{E}_{(y,\bm{x},\tilde{\bm{x}})\sim\nu\otimes\mu_{x}^{d}}\left[\psi_{i}( \tilde{\bm{x}})\phi(y,\tilde{\bm{x}})\psi_{i}(\bm{x})\right]\right|\right)^{2}\] \[\leq Cd^{-k_{*}}\left(\sum_{\bm{i}\in\mathcal{I}^{d}}\left| \mathbb{E}_{y\sim\mu_{y}}\left[\mathbb{E}_{\bm{x}\sim\mu_{x}^{d}}\left[\frac{ \mathrm{d}\nu}{\mathrm{d}\nu_{0}}(y,\bm{x})\psi_{i}(\bm{x})\right]\mathbb{E}_ {\tilde{\bm{x}}\sim\mu_{x}^{d}}\left[\phi(y,\tilde{\bm{x}})\psi_{i}(\tilde{\bm {x}})\right]\right]\right|\right)^{2}\] \[\leq Cd^{-k_{*}}\left(\sum_{\bm{i}\in\mathcal{I}^{d}}\mathbb{E}_{ \mu_{y}}\left[\mathbb{E}_{\mu_{x}^{d}}\left[\frac{\mathrm{d}\nu}{\mathrm{d}\nu_{0 }}(y,\bm{x})\psi_{i}(\bm{x})\right]^{2}\right]^{1/2}\mathbb{E}_{\mu_{y}}\left[ \mathbb{E}_{\tilde{\bm{x}}\sim\mu_{x}^{d}}\left[\phi(y,\tilde{\bm{x}})\psi_{i}( \tilde{\bm{x}})\right]^{2}\right]^{1/2}\right)^{2}\] \[\leq Cd^{-k_{*}}\left(\sum_{\bm{i}\in\mathcal{I}^{d}}\mathbb{E}_{ \mu_{y}}\left[\mathbb{E}_{\mu_{x}^{d}}\left[\frac{\mathrm{d}\nu}{\mathrm{d}\nu_{0 }}(y,\bm{x})\psi_{i}(\bm{x})\right]^{2}\right]\right)\left(\sum_{\bm{i}\in \mathcal{I}^{d}}\mathbb{E}_{\mu_{y}}\left[\mathbb{E}_{\tilde{\bm{x}}\sim\mu_{x}^{d }}\left[\phi(y,\tilde{\bm{x}})\psi_{i}(\tilde{\bm{x}})\right]^{2}\right]\right)\] \[\leq Cd^{-k_{*}}\mathbb{E}_{y\sim\mu_{y}}\left[\left\|\frac{ \mathrm{d}\nu}{\mathrm{d}\nu_{0}}(y,\cdot)\right\|_{L^{2}(\mu_{x}^{d})}^{2} \right]\cdot\mathbb{E}_{y\sim\mu_{y}}\left[\|\phi(y,\cdot)\|_{L^{2}(\mu_{x}^{d})}^{2}\right]\] \[=Cd^{-k_{*}}\left\|\frac{\mathrm{d}\nu}{\mathrm{d}\nu_{0}}\right\|_ {L^{2}(\nu_{0})}^{2}\|\phi\|_{L^{2}(\nu_{0})}^{2}\leq C^{\prime}d^{-k_{*}}\| \phi\|_{L^{2}(\nu_{0})}^{2},\]

where we used in the third and fourth line Cauchy-Schwarz inequality, and in the last line

\[\frac{\mathrm{d}\nu}{\mathrm{d}\nu_{0}}=\frac{\mathrm{d}\mu_{y,\bm{z}}}{\mathrm{d} \mu_{y}\otimes\mu_{x}^{P}}\in L^{2}(\nu_{0}),\] (44)

by Assumption 2.1. Note that the above bound is uniform over all \(\phi\in L^{2}(\nu_{0})\). This concludes the proof.

**Remark B.1**.: _We remark that indeed these proofs, after alternatively defining \(k_{*}=\mathsf{relLeap}(\mathcal{C}_{\mathsf{A}})\) and \(\mathcal{C}_{*}\) to be the maximal subset of \(\mathcal{C}_{\mathsf{A}}\) such that \(\mathsf{relLeap}(\mathcal{C}_{*})=k_{*}-1\), shows that if \(q/\tau^{2}\leq cd^{k_{*}}\), then mthods in \(\mathsf{A}\) fail to recover coordinates from \(\mathcal{C}_{\mathsf{A}}\setminus\mathcal{C}_{*}\)._

### Theorem 5.1.(b): lower bound for non-adaptive queries

The proof follows from a similar argument to the adaptive case.

Proof outline:Let \(k_{*}=\mathsf{Cover}(\mathcal{C}_{\mathsf{A}})\) and \(l_{*}\) that maximizes \(\min_{S\in\mathcal{C}_{\mathsf{A}}l\in S}|S|\). By definition of the cover complexity, \(|S|\geq k_{*}\) for all \(S\in\mathcal{C}_{\mathsf{A}}\) with \(l_{*}\in S\). Let \(\mathcal{S}_{*}:=[P]\setminus\{l_{*}\}\). We define in the non-adaptive case for all \(\phi\in\mathcal{Q}_{\mathsf{A}}\) and \(\sigma\in\mathsf{P}([d],P)\),

\[\Delta_{\sigma}(\phi)=\mathbb{E}_{\nu^{\sigma}}[\phi]-\mathbb{E}_{\nu^{ \sigma}_{\mathcal{S}_{*}}}[\phi],\] (45)

meaning that we compare the query expectation between \(\nu^{\sigma}\) and \(\nu^{\sigma}_{\mathcal{S}_{*}}\) where we decoupled the label \(y\) from coordinate \(\sigma(l_{*})\). We show again that the bound (25) holds on the second moment of \(\Delta_{\sigma}(\phi)/\|\phi\|_{L^{2}(\nu_{\mathrm{sp}})}\). Therefore, for any set of \(q\) queries \(\{\phi_{t}\}_{t\in[q]}\subset\mathcal{Q}_{\mathsf{A}}\), we get a similar bound (26) on the probability of all queries being less or equal to \(\tau\) in absolute value. We deduce that for \(q/\tau^{2}\leq cd^{k_{*}}\), the algorithm will not be able to distinguish for all \(\sigma\in\mathsf{P}([d],P)\) between \(\nu^{\sigma}\) and \(\nu^{\sigma}_{\mathcal{S}_{*}}\), and therefore recover the coordinate \(\sigma(l_{*})\) in the support.

Bounding \(\mathbb{E}_{\sigma}[\Delta_{\sigma}(\phi)^{2}]\):Following the same decomposition of \(\Delta_{\sigma}(\phi)\) as above, we first obtain

\[\Delta_{\sigma}(\phi)=\sum_{i\in\mathcal{I}\setminus\{0\}}\mathbb{E}_{\nu^{ \sigma}_{\mathcal{S}_{*}}}\left[\xi^{\sigma}_{i}(y^{\sigma}_{\mathcal{S}_{*}}, \bm{x}_{\sigma(\mathcal{S}_{*})})\alpha_{i}(y^{\sigma}_{\mathcal{S}_{*}},\bm{ x}_{\sigma(\mathcal{S}_{*})})\right],\] (46)

and then

\[\Delta_{\sigma}(\phi)=\sum_{\bm{i}\in\mathcal{I}^{d},\ \sigma(l_{*})\in \operatorname{supp}(\bm{i})\not\subseteq\sigma(\mathcal{S}_{*})}\mathbb{E}_{ \tilde{\bm{x}}\sim\mu^{d}_{\varepsilon}}\left[\psi_{\bm{i}}(\tilde{\bm{x}}) \mathbb{E}_{(y,\bm{x})\sim\nu^{\sigma}}\left[\phi(y,\tilde{\bm{x}})\psi_{\bm{ i}}(\bm{x})\right]\right].\] (47)

Therefore, by Eq. (23), the summand is non zero only if \(\operatorname{supp}(\bm{i})\subseteq\sigma(\mathcal{C}_{\mathsf{A}})\) with \(\sigma(l_{*})\in\operatorname{supp}(\bm{i})\). Recall that in this case \(|\operatorname{supp}(\bm{i})|\geq k_{*}\) by the choice of \(l_{*}\). The rest of the proof follows using the same computation as in the previous section.

### Upper Bounds for Adaptive and Non-Adaptive Queries

Define

\[\beta=\min_{U\in\mathcal{C}_{\mathsf{A}}}\left|\mathbb{E}_{\mu_{y,*}}\Big{[}T ^{U}(y)\prod_{i\in U}T^{U}_{i}(z_{i})\Big{]}\right|,\] (48)

with \(T^{U}\in\Psi_{\mathsf{A}}\) and \(T^{U}_{i}\in L^{2}_{0}(\mu_{x})\) chosen to make the expectation non-zero, where without loss of generality we normalize them so that \(\|T^{U}\|_{L^{2}(\mu_{y})}^{2}\prod_{i\in S}\|T^{U}_{i}\|_{L^{2}(\mu_{x})}=1\). We can then emulate the support recovery algorithm described below Definition 1 in Section 4 using for every \(U_{*}\in\mathcal{C}_{\mathsf{A}}\) and \(|S|=|U_{*}|\), \(\phi_{S}(y,\bm{x})=T^{U_{*}}(y)\prod_{i\in S}T^{U_{*}}_{i}(x_{i})\) (note that here \(S\) is an ordered subset) and precision \(\tau<\beta/2\), so that if the response \(|v_{l}|>\beta/2\), then \(S\in\sigma_{*}(\mathcal{C}_{\mathsf{A}})\). Note that \(\phi_{S}(y,\bm{x})\) are indeed in \(\mathcal{Q}_{\mathsf{A}}\): for \(\mathsf{SQ}\), \(\mathsf{CSQ}\) this is direct, for \(\mathsf{DLQ}_{\ell}\), we can take \(f_{t}(\bm{x};\omega)=\bm{u}+\omega\cdot\bm{v}\prod_{i\in S}T^{U_{i}}_{i}(x_{i})\).

If we take \(\tau<\beta/2\), the number of queries only needs to scale as \(O(d^{\mathsf{Cover}_{\mathsf{A}}})\) or \(O(d^{\mathsf{Leap}_{\mathsf{A}}})\). We can further trade-off the precision and accuracy and get the result of several of the above queries with one query with small enough \(\tau\). For example, consider testing \(d\) subset \(\{i\}\) to find \(s_{*}(1)\). Then, one can group each of this query in \(\Theta(\log(d))\) groups \(G_{k}\) following the binary representation of \(i\). For each of these groups, consider the query

\[\frac{1}{\sqrt{d}}\sum_{s\in G_{k}}T^{\{1\}}(y)T^{\{1\}}_{1}(z_{i}).\]

Note that this is indeed a query in \(\Psi_{\mathsf{A}}\): it has \(L^{2}(\nu_{0})\) norm \(O(1)\) (indeed \(\mathbb{E}[T^{(i)}(z_{i})T^{(j)}(z_{j})]=0\) for any \(i\neq j\) with at least one not in the support). With precision \(O(1/\sqrt{d})\), one can test if one of those expectation is non-zero. Repeating for each of the \(\Theta(\log(d))\) groups allows to recover the binary representation of \(\{s_{*}(1)\}\). This idea allows to trade-off the query and precision for any \(q/\tau^{2}\geq Cd^{k_{*}}\log(d)\).

Proofs of Technical Results

**Proposition 6.1**: _Denote \(\mathcal{Y}=\{a,b\}\). For any \(U\in\mathcal{C}_{\mathsf{SQ}}(\mu)\), by Definition 2, there exists \(T\in L^{2}(\mu_{y})\) and \(\{T_{i}\in L^{2}_{0}(\mu_{x})\}_{i\in S}\) such that_

\[0 \neq\mathbb{E}\Big{[}T(y)\prod_{i\in U}T_{i}(z_{i})\Big{]}\] (49) \[=T(a)\mu_{y,\,\bm{z}}(y=a)\mathbb{E}\Big{[}\prod_{i\in U}T_{i}(z _{i})\Big{|}y=a\Big{]}+T(b)\mu_{y,\,\bm{z}}(y=b)\mathbb{E}\Big{[}\prod_{i\in U }T_{i}(z_{i})\Big{|}y=b\Big{]}\] \[=(T(b)-T(a))\mu_{y,\,\bm{z}}(y=b)\mathbb{E}\Big{[}\prod_{i\in U}T _{i}(z_{i})\Big{|}y=b\Big{]},\]

_where we used that \(\mathbb{E}[T_{i}(z_{i})]=0\). Hence, this expectation is non-zero for any mapping \(T(b)\neq T(a)\), in particular \(T=\mathrm{Id}\). We deduce that \(U\in\mathcal{C}_{\mathsf{CSQ}}(\mu)\) too._

_Consider \(\mathcal{X}=\mathbb{R}\) and \(\mu_{x}=\gamma\) the standard Gaussian measure. We assume that the label \(y=z_{1}z_{2}\cdots z_{P}\). Then \(\mathcal{C}_{\mathsf{CSQ}}\) only contains the entire support \([P]\) and \(\mathsf{Cover}_{\mathsf{CSQ}}(\mu)=\mathsf{Leap}_{\mathsf{CSQ}}(\mu)=P\). On the other hand, consider the mappings \(T(y)=\mathbbm{1}[|y|\geq\tau]\) for some \(\tau>0\), and \(T_{i}(z_{i})=z_{i}^{2}-1\). Then_

\[\mathbb{E}[T(y)T(z_{i})]=\mathbb{E}_{\bm{z}_{-i}}\Big{[}\mathbb{E}_{z_{i}} \Big{[}(z_{i}^{2}-1)\mathbbm{1}\big{[}|z_{i}|\cdot\prod_{j\neq i}|z_{j}|\geq \tau\big{]}\Big{]}\Big{]}>0,\] (50)

_and \(\mathcal{C}_{\mathsf{SQ}}(\mu)\) contains all the singletons \(\{i\}\) for \(i\in[P]\). We deduce \(\mathsf{Cover}_{\mathsf{SQ}}(\mu)=\mathsf{Leap}_{\mathsf{SQ}}(\mu)=1\)._

**Proposition 6.2.(a)**: _Note that \(\ell^{\prime}(u,y)=(u-y)\) for the squared loss. For any \(U\subseteq[P]\), and any \(\{T_{i}\in L^{2}_{0}(\mu_{x}):i\in[P]\}\), we have_

\[\mathbb{E}_{\mu_{y,\bm{z}}}\left[(u-y)\prod_{i\in U}T_{i}(z_{i})\right]= \mathbb{E}_{\mu_{y,\bm{z}}}\left[(u\prod_{i\in U}T_{i}(z_{i})\right]-\mathbb{E }_{\mu_{y,\bm{z}}}\left[y\prod_{i\in U}T_{i}(z_{i})\right]=-\mathbb{E}_{\mu_{y,\bm{z}}}\left[y\prod_{i\in U}T_{i}(z_{i})\right].\]

_Thus,_

\[\mathbb{E}_{\mu_{y,\bm{z}}}\left[(u-y)\prod_{i\in U}T_{i}(z_{i})\right]\neq 0 \text{ if and only if }\mathbb{E}_{\mu_{y,\bm{z}}}\left[(y\prod_{i\in U}T_{i}(z_{i})\right].\]

_This immediately implies \(\mathcal{C}_{\mathsf{DLQ}_{\ell}}=\mathcal{C}_{\mathsf{SQ}}\) by Definition 2, and as a consequence \(\mathsf{Leap}_{\mathsf{CSQ}}=\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}\) and \(\mathsf{Cover}_{\mathsf{CSQ}}=\mathsf{Cover}_{\mathsf{DLQ}_{\ell}}\)._

**Proposition 6.2.(b)**: _Consider \(U\in\mathcal{C}_{\mathsf{SQ}}\) with_

\[\mathbb{E}_{\mu_{y,\bm{z}}}\Big{[}T(y)\prod_{i\in U}T_{i}(z_{i})\Big{]}\neq 0.\] (51)

_Note that without loss of generality we can assume \(T\in L^{2}_{0}(\mu_{y})\). If \(\mathrm{span}(\Psi_{\mathsf{DLQ}_{\ell}})\) is dense in \(L^{2}_{0}(\mu_{y})\), then for every \(\varepsilon>0\), there exists \(M_{\varepsilon}\in\mathbb{N}\) and \((\bm{v}_{j},\bm{u}_{j})_{j\in[M_{\varepsilon}]}\subseteq\mathcal{V}\times \mathcal{F}\) such that_

\[\tilde{T}_{M_{\varepsilon}}(y)=\sum_{j\in[M_{\varepsilon}]}\bm{v}_{j}^{\mathsf{ T}}\nabla\ell(\bm{u}_{j},y)\] (52)

_has \(\|T-\tilde{T}_{M_{\varepsilon}}\|_{L^{2}(\mu_{y})}\leq\varepsilon\). In particular,_

\[\left|\mathbb{E}_{\mu_{y,\bm{z}}}\Big{[}\tilde{T}_{M_{\varepsilon}}(y)\prod_{i \in U}T_{i}(z_{i})\Big{]}-\mathbb{E}_{\mu_{y,\bm{z}}}\Big{[}T(y)\prod_{i\in U }T_{i}(z_{i})\Big{]}\right|\leq\|T-\tilde{T}_{M_{\varepsilon}}\|_{L^{2}(\mu_{y })}\prod_{i\in U}\|T_{i}\|_{L^{2}(\mu_{x})}.\] (53)

_Therefore taking \(\varepsilon\) sufficiently small, \(\tilde{T}_{M_{\varepsilon}}\) has non zero correlation with \(\prod_{i\in U}T_{i}\), and in particular, one of the \(\bm{v}_{j}^{\mathsf{T}}\nabla\ell(\bm{u}_{j},y)\) must have non-zero correlation too. We deduce that \(\mathcal{C}_{\mathsf{SQ}}\subseteq\mathcal{C}_{\mathsf{DLQ}_{\ell}}\) and we conclude using \(\mathcal{C}_{\mathsf{DLQ}_{\ell}}\subseteq\mathcal{C}_{\mathsf{SQ}}\)._

_Let us assume that there exists nonzero bounded \(T\in L^{2}_{0}(\mu_{y})\), which we take without loss of generality \(T(y)\subseteq[-1,1]\), such that \(\mathbb{E}[\bm{v}^{\mathsf{T}}\nabla\ell(\bm{u},y)T(y)]=0\) for all \(\bm{u}\in\mathcal{F},\bm{v}\in V\). The goal is to define \(\mu_{y,z}\) such that \(\mathcal{C}_{\mathsf{DLO}_{\ell}}\subsetneq\mathcal{C}_{\mathsf{SQ}}\). Specifically, we will construct a joint distribution on \((y,z_{i})\) with \(\{i\}\in\mathcal{C}_{\mathsf{SQ}}\) but \(\{i\}\not\in\mathcal{C}_{\mathsf{DLO}_{\ell}}\). Let \(A\subset\mathcal{X}\) with \(\mu_{x}(A)\in(0,1)\) and consider \(y\) that only depends on \(z_{i}\) through \(\mathbbm{1}[z_{i}\in\mathcal{A}]\). Denote \(P(A|y)=\mathbb{P}(z_{i}\in A|y)\). For any \(T_{i}\in L^{2}_{0}(\mu_{x})\),

\[\mathbb{E}[T_{i}(z_{i})|y]=P(A|y)\mathbb{E}[T_{i}(z_{i})|z_{i}\in A]+P(A^{c}|y )\mathbb{E}[T_{i}(z_{i})|z_{i}\not\in A].\] (54)

Denote \(\kappa(A)=\mathbb{E}[T_{i}(z_{i})|z_{i}\in A]\). By definition \(\mathbb{E}[T_{i}(z_{i})]=0=\mu_{x}(A)\kappa(A)+\mu_{x}(A^{c})\kappa(A^{c})\), and therefore \(\kappa(A^{c})=-\mu_{x}(A)\kappa(A)/(1-\mu_{x}(A))\). Hence,

\[\mathbb{E}[T_{i}(z_{i})|y]=\frac{\kappa(A)}{1-\mu_{x}(A)}\left[P(A|y)-\mu_{x} (A)\right].\] (55)

Taking \(\lambda>(1-\mu_{x}(A))/\mu_{x}(A)\), we can set

\[P(A|y):=\lambda^{-1}(1-\mu_{x}(A))T(y)+\mu_{x}(A)\in(0,1).\] (56)

We then define the joint distribution of \((y,z_{i})\) as

\[\mu_{y,z_{i}}:=[\mu_{x}(z_{i}|z_{i}\in A)P(A|y)+\mu_{x}(z_{i}|z_{i}\not\in A)P (A^{c}|y)]\mu_{y}(y).\] (57)

Note that \(\mathbb{E}[P(A|y)]=\mu_{x}(A)\) using that \(\mathbb{E}[T(y)]=0\), and therefore the marginals of \(\mu_{y,z_{i}}\) are indeed \(\mu_{y}\) and \(\mu_{x}\). For any \(T_{i}\in L^{2}_{0}(\mu_{x})\) and \((\bm{u},\bm{v})\in\mathcal{F}\times V\), we have

\[\mathbb{E}_{\mu_{y,z_{i}}}[\bm{v}^{\mathsf{T}}\nabla\ell(\bm{u},y)T_{i}(z_{i}) ]=\frac{\kappa(A)}{\lambda}\mathbb{E}_{\mu_{y}}[\bm{v}^{\mathsf{T}}\nabla\ell( \bm{u},y)T(y)]=0,\] (58)

and therefore \(\{i\}\not\in\mathcal{C}_{\mathsf{DLO}_{\ell}}\). On the other hand \(T\in L^{2}(\mu_{y})\) and therefore \(\{i\}\in\mathcal{C}_{\mathsf{SQ}}\). We conclude that the joint distribution \((y,z_{i})\) has \(\mathsf{Leap}_{\mathsf{DLO}_{\ell}}=\mathsf{Cover}_{\mathsf{DLO}_{\ell}}=\infty\) while \(\mathsf{Leap}_{\mathsf{SQ}}=\mathsf{Cover}_{\mathsf{SQ}}=1\).

Proposition 6.2.(c): We construct a simple example with \(\mathcal{X}=\{+1,-1\}\) and \(\mu_{x}=\mathrm{Unif}(\mathcal{X})\), i.e., uniform on the discrete hypercube. Consider \(P=2k\) and label

\[y=z_{1}z_{2}\ldots z_{2k}\left(\sum_{i\in[2k]}z_{i}\right).\] (59)

The set \(\mathcal{C}_{\mathsf{CSQ}}(\mu)\) contains all subsets \([2k]\setminus\{i\}\) for all \(i\in[2k]\), and therefore

\[\mathsf{Cover}_{\mathsf{CSQ}}(\mu)=\mathsf{Leap}_{\mathsf{CSQ}}(\mu)=2k-1.\] (60)

Consider the loss function \(\ell(u,y)=\frac{1}{2}(y-u)^{2}+\frac{1}{4}(y-u)^{4}\). The derivative is given by

\[\ell^{\prime}(u,y)=u-y+u^{3}-3u^{2}y+3uy^{2}-y^{3}.\] (61)

We have

\[y^{2}=2k+2\sum_{i<j}z_{i}z_{j},\qquad y^{3}=z_{1}z_{2}\ldots z_{2k}\left(\sum _{i\in[2k]}z_{i}\right)^{3}.\] (62)

For \(k\geq 3\), \(y^{3}\) only contains monomials over \(\geq 2k-3\geq 3\) coordinates. Therefore \(\mathcal{C}_{\mathsf{DLO}_{\ell}}(\mu)\) contains all pairs \(\{i,j\}\) but no singleton. Hence,

\[\mathsf{Cover}_{\mathsf{DLO}_{\ell}}(\mu)=\mathsf{Leap}_{\mathsf{DLO}_{\ell}}( \mu)=2.\] (63)

Finally, for \(\mathcal{C}_{\mathsf{SQ}}\), taking \(T(y)=y^{2k-1}\), it contains all singleton \(\{i\}\) and therefore

\[\mathsf{Cover}_{\mathsf{SQ}}(\mu)=\mathsf{Leap}_{\mathsf{SQ}}(\mu)=1.\] (64)

### Proof of Theorem 6.3

Note that \(\mathcal{C}_{\mathsf{A}}\) does not change if we add or remove constant functions to \(\Psi_{\mathsf{A}}\).

\(\ell_{1}\)-loss:First consider \(\ell(u,y)=|y-u|\). We have \(\nabla\ell(u,y):=\ell^{\prime}(u,y)=\text{sign}(u-y)\), where we can set without loss of generality \(\text{sign}(0)=0\). From Hornik (1991, Theorem 5) and Hornik (1991, Theorem 1), we directly conclude that for any probability measure \(\mu_{y}\) on \(\mathbb{R}\), we have \(\mathrm{span}(\Psi_{\mathsf{A}})\) dense in \(L^{2}(\mu_{y})\) and we conclude using Proposition 6.2.(b).

Hinge loss:Consider the Hinge loss \(\ell(u,y)=\max(1-yu,0)\). We get \(\ell^{\prime}(u,y)=-y\mathbbm{1}_{uy\leq 1}\) where without loss of generality, we set \(\ell^{\prime}(1/y,y)=-y\). For \(a>0\) and \(b<0\), we have \(\ell^{\prime}(a,y)-\ell^{\prime}(b,y)=y\mathbbm{1}[y>a]-y\mathbbm{1}[y<b]\). Taking \(b\to-\infty\), we have \(\ell^{\prime}(a,y)-\ell^{\prime}(b,y)\) that converges to \(y\mathbbm{1}[y>a]\) in \(L^{2}(\mu_{y})\) (recall that we assume that the second moment of \(\mu_{y}\) is bounded). Similarly, we can construct \(y\mathbbm{1}[y>a]\) for \(a\leq 0\). Following the proof of Hornik (1991, Theorem 1), if \(\mathrm{span}(\hat{\Psi}_{\text{A}})\) is not dense in \(L^{2}(\mu_{y})\), there exists \(g\in L^{2}(\mu_{y})\) such that

\[\int y\mathbbm{1}[y>u]g(y)\mathrm{d}\mu_{y}=0,\qquad\forall u\in\mathbb{R}.\] (65)

Define the signed measure \(\sigma(B)=\int_{B}yg(y)\mathrm{d}\mu_{y}\). It is finite since \(\int|yg(y)|\mathrm{d}\mu_{y}\leq\|y\|_{L^{2}(\mu_{y})}\|g\|_{L^{2}(\mu_{y})}<\infty\) by assumption. We deduce that

\[\int\mathbbm{1}[ay-u>0]\mathrm{d}\sigma(y)=0\qquad\forall(a,u)\in\mathbb{R}^ {2}.\] (66)

(The case \(a=0\) is obtained by taking \(u\to-\infty\) in Eq. (65).) We then use that \(\mathbbm{1}[ay-u>0]\) is discriminatory and therefore no such finite signed measure exists.

Exponential loss:Consider \(\mathcal{Y}\subseteq[-M,M]\) and \(\ell(u,y)=e^{-uy}\), so that \(\ell^{\prime}(u,y)=-ye^{-yu}\). Any Borel probability measure \(\mu_{y}\) on \([-M,M]\) is regular, and in particular continuous functions and therefore polynomials are dense in \(L^{2}(\mu_{y})\). To show that \(\mathrm{span}\{ye^{-yu}:u\in\mathbb{R}\}\) is dense in \(L^{2}(\mu_{y})\), it is sufficient to show that we can approximate any monomial \(y^{k}\) in \(L^{2}(\mu_{y})\). This is readily proven recursively, using that

\[ye^{-yu}=\sum_{l=0}^{k-1}\frac{(-1)^{l}}{l!}u^{l}y^{l+1}+O(u^{l})\cdot e^{M}.\] (67)

This concludes the proof of this theorem.

## Appendix D SGD, mean-field and limiting dimension-free dynamics

Fix a link distribution \(y|\bm{z}\sim\mu_{y|\bm{z}}\) and consider data distribution \((y,\bm{x})\sim\mathcal{D}^{d}_{s_{*}}\in\mathcal{H}^{d}_{\mu}\) with \(\mu=(\mu_{x},\mu_{y|\bm{z}})\) and (unknown) \(s_{*}\in\mathbb{P}([d],P)\). We consider learning using a two-layer neural network with parameters \(\bm{\Theta}\in\mathbb{R}^{M(d+2)+1}\) and an activation \(\sigma:\mathbb{R}\to\mathbb{R}\):

\[f_{\text{NN}}(\bm{x};\bm{\Theta})=c+\frac{1}{M}\sum_{j\in[M]}a_{j}\sigma( \langle\bm{w}_{j},\bm{x}\rangle+b_{j}),\qquad c,a_{j},b_{j},\in\mathbb{R}, \quad\bm{w}_{j}\in\mathbb{R}^{d},\text{ for }j\in[M]\] (NN1)

For convenience, we will reparametrize the neural network for \(\bm{\Theta}=(\bm{\theta}_{j})_{j\in[M]}\in\mathbb{R}^{M(d+3)}\) with \(\bm{\theta}_{j}=(a_{j},\bm{w}_{j},b_{j},c_{j})\in\mathbb{R}^{d+3}\) and

\[f_{\text{NN}}(\bm{x};\bm{\Theta})=\frac{1}{M}\sum_{j\in[M]}\sigma_{*}(\bm{x}; \bm{\theta}_{j}),\qquad\sigma_{*}(\bm{x};\bm{\theta}_{j})=a_{j}\sigma(\langle \bm{w}_{j},\bm{x}\rangle+b_{j})+c_{j}.\] (NN2)

Indeed, the two neural networks (NN1) and (NN2) remain equal throughout training under the initialization \(c_{1}=\ldots=c_{M}=c\).

Also, recall the definitions of the risk and the excess risk, respectively, for a data distribution \((y,\bm{x})\sim\mathcal{D}\), with respect to a loss function \(\ell:\mathbb{R}\times\mathcal{Y}\to\mathbb{R}_{\geq 0}\).

\[\mathcal{R}(f)=\mathbb{E}_{\mathcal{D}}\left[\ell(f(\bm{x}),y)\right],\quad \text{and}\quad\overline{\mathcal{R}}(f)=\mathcal{R}(f)\ -\inf_{f:\{\pm 1\}^{d}\to\mathbb{R}}\mathcal{R}(\bar{f}).\] (68)

We start with the initialization specified by \(\rho_{0}\)

\[(a^{0},b^{0},\sqrt{d}\cdot\bm{w}^{0},c^{0})\sim\mu_{a}\otimes\mu_{b}\otimes\mu _{w}^{\otimes d}\otimes\delta_{c=\overline{c}}:=\rho_{0}.\] (69)Batch Stochastic Gradient Descent.We train the parameters \(\bm{\Theta}\) using batch-SGD with loss \(\ell\) and the batch size \(b\). Even in more generality than stated in (\(\ell\)-bSGD), we allow for time-varying step size \((\bm{\eta}_{k})_{k\geq 0}\), where \(\bm{\eta}_{k}\in\mathbb{R}^{d+3}\) can be different for different parameters (e.g., different layers), and \(\ell_{2}\)-regularization \(\bm{\lambda}\in\mathbb{R}^{d+3}\). We initialize the weights \((\bm{\theta}_{j})_{j\in[M]}\stackrel{{\text{i.i.d.}}}{{\sim}} \rho_{0}\) from (69), and at each step, given samples \((\{(\bm{x}_{ki},y_{ki}):i\in[b]\})_{k\geq 0}\), the weights are updated using

\[\bm{\theta}_{j}^{k+1}=\bm{\theta}_{j}^{k}-\frac{1}{b}\sum_{i\in[b]}\ell^{ \prime}(f_{\text{NN}}(\bm{x}_{ki};\bm{\Theta}^{t}),y_{ki})\cdot\bm{H}_{k}\nabla _{\bm{\theta}}\sigma_{*}(\bm{x}_{ki};\bm{\theta}_{j}^{k})-\bm{H}_{k}\bm{ \Lambda}\bm{\theta}_{j}^{k},\] ( \[\ell\] -bSGD-g)

where we introduced \(\bm{H}_{k}:=\operatorname{diag}(\bm{\eta}_{k})\) and \(\bm{\Lambda}:=\operatorname{diag}(\bm{\lambda})\).

Mean-Field Dynamics.A rich line work (Chizat and Bach, 2018; Mei et al., 2018; Rotskoff and Vanden-Eijnden, 2018; Sirignano and Spiliopoulos, 2020) has established a crisp approximation between the dynamics of one-pass-batch-SGD (\(\ell\)-bSGD-g) and a continuous dynamics in the space of probability distributions in \(\mathbb{R}^{d+3}\). To any distribution \(\rho\in\mathcal{P}(\mathbb{R}^{d+3})\), we associate the infinite-width neural network

\[f_{\text{NN}}(\bm{x};\rho)=\int\sigma_{*}(\bm{x};\bm{\theta})\rho(\mathrm{d} \bm{\theta}).\] (70)

In particular, we recover the finite-width neural network (NN2) by taking \(\hat{\rho}^{(M)}:=\frac{1}{M}\sum_{j\in[M]}\delta_{\bm{\theta}_{j}}\).

Denote \(\hat{\rho}_{k}^{(M)}\) the empirical distribution of the weights \(\{\bm{\theta}_{j}^{k}\}_{j\in[M]}\) after \(k\)-steps of (\(\ell\)-bSGD-g). In our (\(\ell\)-bSGD-g) dynamics, we allow for a generic step-size schedule that is captured by

\[\bm{\eta}_{k}=\eta\bm{\xi}(k\eta)\quad\text{ for some function }\quad\bm{\xi}:\mathbb{R}\to \mathbb{R}_{\geq 0}^{d+3}\,,\] (71)

and that the data at each step are sampled i.i.d. from \(\mathcal{D}\); the case of \(\mathcal{D}\) empirical distribution on training data corresponds to multi-pass batch-SGD, while \(\mathcal{D}\) population distribution corresponds to online batch-SGD. Under some reasonable assumptions (that we will list below on the activation, step-size schedule \(\bm{\xi}\), initialization \(\rho_{0}\), and the loss \(\ell\)) on taking \(M\) sufficiently large and \(\eta\) sufficiently small, setting \(k=t/\eta\), \(\hat{\rho}^{(M)}\) is well approximated by a distribution \(\rho_{t}\in\mathcal{P}(\mathbb{R}^{d+3})\) that evolves according to the PDE:

\[\begin{split}\partial_{t}\rho_{t}&=\nabla_{\bm{ \theta}}\cdot(\rho_{t}\bm{H}(t)\nabla_{\bm{\theta}}\psi(\bm{\theta};\rho_{t})), \\ \psi(\bm{\theta};\rho_{t})&=\mathbb{E}_{\mathcal{D}}[ \ell^{\prime}(f_{\text{NN}}(\bm{x};\rho_{t}),y)\sigma_{*}(\bm{x};\bm{\theta})] +\frac{1}{2}\bm{\theta}^{\mathsf{T}}\bm{\Lambda}\bm{\theta},\end{split}\] (MF-PDE)

where the initial distribution is \(\rho_{0}\) and we defined \(\bm{H}(t)=\operatorname{diag}(\bm{\xi}(t))\). We will refer to the distribution dynamics (MF-PDE) as the mean-field (MF) dynamics. We now specify the assumptions on hyperparameters under which a non-asymptotic equivalence can be derived.

**A D.1** (Activation).: _Our \(\sigma:\mathbb{R}\to\mathbb{R}\) is three times differentiable with \(\|\sigma^{(k)}\|_{\infty}\leq K\), \(k=0,\dots,3\)._

**A D.2** (Bounded, Lipschitz step-size schedule).: \(t\mapsto\bm{\xi}(t)=(\xi_{i}(t))_{i\in[d+3]}\) _has bounded Lipschitz entries: \(\|\xi_{i}\|_{\infty}\leq K\) and \(\|\xi_{i}\|_{\operatorname{Lip}}\leq K\)._

**A D.3** (Initialization).: _The initial distribution \(\rho_{0}\in\mathcal{P}(\mathbb{R}^{d+3})\) is of the form:_

\[(a^{0},b^{0},\sqrt{d}\cdot\bm{w}^{0},c^{0})\sim\mu_{a}\otimes\mu_{b}\otimes\mu _{w}^{\otimes d}\otimes\delta_{c=\overline{c}},\]

_where \(\mu_{a},\mu_{w},\mu_{b}\) are independent of \(d\). We further assume that \(\mu_{a}\) supported on \(|a|\leq K\), \(|\overline{c}|\leq K\), and \(\mu_{w}\) is symmetric and \(K^{2}\)-sub-Gaussian._

**A D.4** (Loss).: _The loss \(\ell:\mathbb{R}\times\mathcal{Y}\to\mathbb{R}_{\geq 0}\) is twice-differentiable in its first argument for all \(y\in\mathcal{Y}\) and satisfies_

\[|\ell^{\prime}(u,y)|\leq K(1+|u|)\text{ and }|\ell^{\prime\prime}(u,y)|\leq K \text{ for all }y\in\mathcal{Y}.\]

Assumptions D.1-D.3 are similar to Abbe et al. (2022). Informally, Assumption D.4 states that loss has a quadratic upper bound (which is analogues to the smoothness assumption in the convex optimization literature). Later, to show the learnability of leap one junta settings, we will further require \(\ell(\cdot,y)\) to be convex and analytic. We note that together these assumptions holds for \((i)\) the standard logistic loss for the classification setting when \(\mathcal{Y}\in\{\pm 1\}\), and \((ii)\) for the squared loss and any of its "analytic perturbations" for the regression setting when \(\mathcal{Y}\subseteq[-B,B]\). In particular, we would like to mention \(\ell(u,y)=(u-y)\text{archsinh}(u-y)\), for which we have \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}=\mathsf{Leap}_{\mathsf{SQ}}\) and these assumptions also hold, and hence our results apply. For classification, by Proposition 6.1, anyway \(\mathsf{LeapSQ}=\mathsf{LeapCSQ}=\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}\) for any non-degenerate loss (including the standard logistic loss for which our results hold).

Limiting Dimension-Free Dynamics.In the junta learning setting, when \(y\) only depends on \(P\ll\!\!\ll d\) coordinates, following Abbe et al. (2022, Secion 3), the SGD dynamics (\(\ell\)-bSGD) concentrates on an effective _dimension-free_ (DF) dynamics as \(M,d\to\infty\) and \(\eta\to 0\). This equivalence holds under a certain assumption on the loss function, and other assumptions on the initialization and activation that are similar to the setup of Abbe et al. (2022).

Consider the isotropic parameters \(\bm{H}_{k}=\operatorname{diag}(\eta_{k}^{a},\eta_{k}^{v}\mathbf{I}_{d},\eta_{ k}^{b},\eta_{k}^{c})\) and \(\bm{\Lambda}=\operatorname{diag}(\lambda^{a},\lambda^{w}\mathbf{I}_{d},\lambda ^{b},\lambda^{c})\). Using technical ideas from Abbe et al. (2022, Secion 3), the (\(\ell\)-bSGD-g) concentrates on an effective _dimension-free_ dynamics in the limit \(M,d\to\infty\) and \(\eta\to 0\). This limiting dynamics corresponds to the gradient flow on \(\mathcal{R}(f)+\bm{\theta}^{\top}\bm{\Lambda}\bm{\theta}\) of the following effective _infinite-width_ neural network (recall \(\bm{z}\in\mathbb{R}^{P}\) is the support)

\[f_{\text{NN}}(\bm{z};\bar{\rho}_{t})=\int\overline{\sigma}_{*}(\bm{z};\bar{ \bm{\theta}}^{t})\bar{\rho}_{t}(\operatorname{d}\!\bar{\bm{\theta}}^{t}), \qquad\overline{\sigma}_{*}(\bm{z};\bar{\bm{\theta}}^{t})=c^{t}+a^{t}\mathbb{ E}_{G}[\sigma(\langle\bm{z},\bm{u}^{t}\rangle+s^{t}G+b^{t})],\] (72)

where \(G\sim\mathsf{N}(0,1)\) and \(\bar{\rho}_{t}\in\mathcal{P}(\mathbb{R}^{P+4})\) is the distribution over \(\bar{\bm{\theta}}^{t}=(a^{t},b^{t},\bm{u}^{t},c^{t},s^{t})\) with \(\bm{u}^{t}\in\mathbb{R}^{P}\).

\[\partial_{t}\bar{\rho}_{t} =\nabla_{\bar{\bm{\theta}}}\cdot\left(\bar{\rho}_{t}\overline{\bm {H}}(t)\cdot\nabla_{\bar{\bm{\theta}}}\psi(\bar{\bm{\theta}},\bar{\rho}_{t}) \right),\] (73) \[\psi(\bar{\bm{\theta}},\bar{\rho}_{t}) =\mathbb{E}_{\mu_{y},*}\left[\ell^{\prime}(f_{\text{NN}}(\bm{z}; \bar{\rho}_{t}),y)\overline{\sigma}_{*}(\bm{z};\bar{\bm{\theta}})\right]+ \frac{1}{2}\bar{\bm{\theta}}^{\top}\overline{\bm{\Lambda}}\bar{\bm{\theta}},\] (DF-PDE)

where \(\overline{\bm{H}}(t)=\operatorname{diag}(\xi^{a},\xi^{b},\xi^{w}\mathbf{I}_{P}, \xi^{c},\xi^{w})(t)\) and \(\overline{\bm{\Lambda}}=\operatorname{diag}(\lambda^{a},\lambda^{b},\lambda^{w} \mathbf{I}_{P},\lambda^{w})\) from initialization

\[\bar{\rho}_{0} :=\mu_{a}\otimes\mu_{b}\otimes\delta_{\bm{u}^{0}=\bm{0}}\otimes \delta_{c^{0}=\bar{c}}\otimes\delta_{s^{0}=\sqrt{m_{2}^{\varphi}}},\] (74)

where \(m_{2}^{w}=\mathbb{E}_{W\sim\mu_{w}}[W^{2}]^{1/2}\) is the second moment of \(\mu_{w}\). The non-asymptotic equivalence is characterized by the following theorem that can be seen as a generalization of Abbe et al. (2022, Theorem 5) to non-squared losses.

**Theorem D.5**.: _Fix activation, step-size schedule, initialization, and the loss such that Assumptions D.1 to D.4 hold and consider any \(T\geq 1\) independent of \(d\). Let \((\bar{\rho}_{t})_{t\geq 0}\) be the solution of (DF-PDE), and \(\{\bm{\Theta}_{k}\}_{k\geq 0}\) the trajectory of SGD (\(\ell\)-bSGD-g) with initialization \(\{\bm{\theta}^{0}_{j}\}_{j\in[M]}\overset{\text{\tiny{\it def}}}{\sim}\rho_{0}\). Then there exist a constant \(C_{K,T}>0\) that only depends on \(K,T\), such that for any \(d,M,\eta>\!\!0\) with \(M\leq e^{d}\) and \(0<\eta\leq 1/[C_{K,T}(d+\log(M))]\), and_

\[\left|\mathcal{R}(f_{\text{NN}}(\cdot;\bm{\Theta}^{k}))-\mathcal{R}(f_{\text{ NN}}(\cdot;\bar{\rho}_{\eta k}))\right|\leq C_{K,T}\left\{\sqrt{\frac{P}{d}}+ \sqrt{\frac{\log(M)}{M}}+\sqrt{\eta}\sqrt{\frac{d+\log(M)}{b}\lor 1}\right\}\] (75)

_for all \(k\leq T/\eta\), with probability at least \(1-1/M\)._

This theorem follows by a straightforward modification of the proof of Abbe et al. (2022, Theorem 16, Proposition 15) using propagation of chaos argument. We note that the only property about the loss that is needed is Assumption D.4.

### DF-PDE alignment with the support.

In this subsection, we provide a formal statement of Theorem 7.1 and its proof. The following is an extension of Abbe et al. (2022, Theorem 7) from square loss to general loss.

**Theorem D.6** (Formal statement of Theorem 7.1).: _Assume that \(\mathsf{Leap}_{\mathsf{DQL}_{\epsilon}}(\mu)>1\). Then there exists \(U\subset[P]\), \(|U|\geq 2\), such that for all \(t\geq 0\), the DF dynamics solution remains independent of coordinates \((z_{i})_{i\in U}\), i.e., \(f_{\text{NN}}(\bm{z};\rho_{t})=f_{\text{NN}}((z_{i})_{i\not\in U};\rho_{t})\), and therefore fails at recovering the support._

_Conversely, if \(\mathsf{Leap}_{\mathsf{DQL}_{\epsilon}}(\mu)=1\) and \(\ell\) is analytic with respect to its first argument, then almost surely over \(\overline{c}\), there exists \(t>0\) such that \(f_{\text{NN}}(\bm{z};\rho_{t})\) depends on all \([P]\)._

Proof of Theorem D.6.: The proof follows similarly to the proof of Abbe et al. (2022, Theorem 7). Assume \(\mathsf{Leap}_{\mathsf{DQL}_{\epsilon}}(\mu)>1\). Consider \(\mathcal{C}_{*}\subset\mathcal{C}_{\mathsf{DQL}_{\epsilon}}\) the maximal subset such that \(\mathsf{Leap}(\mathcal{C}_{*})=1\) and denote \(U_{*}=\bigcup_{U\in\mathcal{C}_{*}}U\). In words, \(U_{*}\) contains the subset of coordinates that are reachable by doing leaps of at most 1. By the assumption that \(\mathsf{Leap}_{\mathsf{DQL}_{\epsilon}}(\mu)>1\), we must have \(|[P]\setminus U_{*}|\geq 2\). Let us show that the weights \(u_{i}\) for \(i\in[P]\setminus U_{*}\) remain equal to \(0\) throughout the dynamics.

For simplicity, we forget about the other parameters which we set to \(b=c=s=0\). For convenience, denote \(\Omega=[P]\setminus U_{*}\). First, we can bound by Gronwall's inequality, for \(t\leq C\), \(|a^{t}|\leq KC^{\prime}\) for all \(a^{t}\) on the support of \(\rho_{t}\). From the proof of Theorem 5.1, conditioning on already explored coordinates does not change the leap-complexity. In particular,

\[\mathbb{E}[\ell^{\prime}(g(\bm{z}_{U_{*}}),y)z_{i}|\bm{z}_{U_{*}}]=0,\qquad \forall i\in\Omega.\]

Consider the time derivative of \(u_{i}^{t}\) for \(i\in\Omega\). We have

\[|\ell^{\prime}(f_{\mathsf{NN}}(\bm{z}_{U_{*}},\bm{z}_{\Omega}; \rho_{t}),y)\sigma^{\prime}(\langle\bm{u}_{U_{*}}^{t},\bm{z}_{U_{*}}\rangle+ \langle\bm{u}_{\Omega}^{t},\bm{z}_{\Omega}\rangle)-\ell^{\prime}(f_{\mathsf{ NN}}(\bm{z}_{U_{*}},\bm{0};\rho_{t}),y)\sigma^{\prime}(\langle\bm{u}_{U_{*}}^{t}, \bm{z}_{U_{*}}\rangle)|\] \[\leq K\|\sigma\|_{\infty}\int|\tilde{a}^{t}\|\sigma(\langle \tilde{\bm{u}}_{U_{*}}^{t},\bm{z}_{U_{*}}\rangle+\langle\tilde{\bm{u}}_{\Omega }^{t},\bm{z}_{\Omega}\rangle)-\sigma(\langle\tilde{\bm{u}}_{U_{*}}^{t},\bm{z}_ {U_{*}}\rangle)|\rho_{t}(\mathrm{d}\tilde{\bm{\theta}})\] \[\quad+K\|f_{\mathsf{NN}}(\cdot;\rho_{t})\|_{\infty}|\sigma( \langle\bm{u}_{U_{*}}^{t},\bm{z}_{U_{*}}\rangle+\langle\bm{u}_{\Omega}^{t}, \bm{z}_{\Omega}\rangle)-\sigma(\langle\bm{u}_{U_{*}}^{t},\bm{z}_{U_{*}}\rangle)|\] \[\leq K\sup_{j\in\Omega,u_{j}^{t}\in\mathrm{supp}(\rho_{t})}|u_{j}^ {t}|,\]

where we used Assumptions D.1 and D.4. Hence,

\[\left|\frac{\mathrm{d}}{\mathrm{d}t}u_{i}^{t}\right| =\left|a^{t}\mathbb{E}[\ell^{\prime}(f_{\mathsf{NN}}(\bm{z},\rho_{ t}),y)\sigma^{\prime}(\langle\bm{u}^{t},\bm{z}\rangle)z_{i}]\right|\] \[\leq\left|a^{t}\mathbb{E}[\ell^{\prime}(f_{\mathsf{NN}}(\bm{z}_{U _{*}},\bm{0};\rho_{t}),y)\sigma^{\prime}(\langle\bm{u}_{U_{*}}^{t},\bm{z}_{U_{ *}}\rangle)z_{i}]\right|+K\sup_{j\in\Omega,u_{j}^{t}\in\mathrm{supp}(\rho_{t})}| u_{j}^{t}|\]

The first expectation is equal to \(0\). Note that \(m_{\Omega}^{t}:=\sup_{j\in\Omega,u_{j}^{t}\in\mathrm{supp}(\rho_{t})}|u_{j}^{t}|\) has \(m_{\Omega}^{0}=0\) and therefore \(m_{\Omega}^{t}=0\) for all \(t\geq 0\). 

## Appendix E Learning Leap One Juntas with SGD: Formalizing Theorem 7.2

In this section, we provide our precise layer-wise training algorithm and the proof that it succeeds in learning \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}=1\) junta problems. This section follows the similar training procedure from Abbe et al. (2022) for the layer-wise training but adapted to training with general loss function \(\ell:\mathbb{R}\times\mathcal{Y}\rightarrow\mathbb{R}_{\geq 0}\) to show Theorem 7.2. The key difference in the training algorithm from Abbe et al. (2022) is the use of coordinate-wise perturbed step-sizes for the first layer weights to break the symmetry between the coordinates. This allows us to learn even some degenerate cases having coordinate symmetries with leap one, in contrast to Abbe et al. (2022).

Discrete Dimension-free Dynamics.We first consider the dimension-free dynamics similar to (DF-PDE) but for discrete step-size regime (see (Abbe et al., 2022, Appendix C)). We initialize to

\[(a^{0},b^{0},\bm{u}^{0},c^{0},s^{0})\sim\bar{\rho}_{0}\in\mathcal{P}(\mathbb{R}^ {P+4}),\]

and \((\bar{\rho}_{k})_{k\in\mathbb{N}}\) are induced distribution on \((a^{0},b^{0},\bm{u}^{0},c^{0},s^{0})\) recursively defined as

\[a^{k+1} =a^{k}-\eta_{k}^{a}\left(\mathbb{E}_{\bm{z},G}[\ell^{\prime}( \hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k}),y)\sigma(\langle\bm{u}^{k},\bm{ z}\rangle+s^{k}G+b^{k})]+\lambda^{a}a^{k}\right)\] \[\bm{u}^{k+1} =\bm{u}^{k}-\bm{\eta}_{k}^{\bm{u}}\circ\left(\mathbb{E}_{\bm{z}, G}[\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k}),y)a^{k}\sigma^{ \prime}(\langle\bm{u}^{k},\bm{z}\rangle+s^{k}G+b^{k})\bm{z}]+\lambda^{w}\bm{u }^{k}\right)\] \[s^{k+1} =s^{k}-\eta_{k}^{w}\left(\mathbb{E}_{\bm{z},G}[\ell^{\prime}( \hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k}),y)a^{k}\sigma^{\prime}(\langle\bm {u}^{k},\bm{z}\rangle+s^{k}G+b^{k})G]+\lambda^{w}s^{k}\right)\quad\text{(d-DF-PDE)}\] \[b^{k+1} =b^{k}-\eta_{k}^{b}\left(\mathbb{E}_{\bm{z},G}[\ell^{\prime}(\hat{f }_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k}),y)a^{k}\sigma^{\prime}(\langle\bm{u}^{k},\bm{z}\rangle+s^{k}G+b^{k})]+\lambda^{b}b^{k}\right)\] \[c^{k+1} =c^{k}-\eta_{k}^{c}\left(\mathbb{E}_{\bm{z},G}[\ell^{\prime}(\hat{f }_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k}),y)]+\lambda^{c}c^{k}\right).\]

### Training Algorithm

Choose a loss \(\ell:\mathbb{R}\times\mathcal{Y}\rightarrow\mathbb{R}_{\geq 0}\) and the activation \(\sigma(x)=(1+x)^{L}\) for \(L\geq 2^{8P}\). We train for \(\Theta(1)\) total steps in two phases and the batch size of \(\Theta(d)\) with the following choice of hyperparameters.

* **No regularization:** set the regularization parameters \(\lambda^{w}=\lambda^{a}=\lambda^{b}=\lambda^{c}=0\).

* **Initialization:** Initialize the first layer weights and biases to deterministically 0. \[(b_{j}^{0},\sqrt{d}\bm{w}_{j}^{0})\stackrel{{\text{i.i.d}}}{{\sim}}\mu _{b}\otimes\mu_{w}^{\otimes d}\equiv\delta_{b=0}\otimes\delta_{w=0}^{\otimes d}.\] The second layer weights are sampled uniformly from \([-1,1]\), e.g. \(a_{j}\stackrel{{\text{i.i.d}}}{{\sim}}\mu_{a}\equiv\mathrm{Unif}([- 1,1])\). Finally, choose \(c^{0}\sim\delta_{c=\bar{c}}\) for the given global bias choice \(\bar{c}\in\mathbb{R}\).
* For the dimension-free dynamics, this corresponds to taking \[(a^{0},b^{0},\bm{u}^{0},c^{0},s^{0})\sim\bar{\rho}^{0}\equiv\mathrm{Unif}([-1, 1])\otimes\delta_{b=0}\otimes\delta_{w=0}^{\otimes p}\otimes\delta_{c=\bar{c}} \otimes\delta_{s=0}.\]
* We deploy a two-phase training procedure. Given parameters \(\eta\) and \(\bm{\kappa}\in[1/2,3/2]^{d}\): 1. **Phase 1:** For all \(k\in\{0,\dots,k_{1}-1\}\), set \(\eta_{k}^{a}=\eta_{k}^{b}=\eta_{k}^{c}=0\) and \(\bm{\eta}_{k}^{\bm{w}}\in\mathbb{R}^{d}\) such that \(\eta_{k}^{w_{i}}=\eta\kappa_{i}\) for all \(i\in[d]\). For the dimension-free dynamics, train the first layer weights \(\bm{u}^{k}\) for \(k_{1}\) steps, while keeping other parameters fixed, i.e \(a^{k}=a^{0},b^{k}=0,c^{k}=\bar{c}\). 2. **Phase 2:** Set \(\eta_{k}^{a}=\eta\) and \(\bm{\eta}_{k}^{\bm{w}}=\bm{0},\eta_{k}^{b}=\eta_{k}^{c}=0\) for \(k\in\{k_{1},\dots,k_{1}+k_{2}-1\}\). In words, train the second layer weights \(a^{k}\) for \(k_{2}\) steps, while keeping the first layer weights fixed at \(\bm{u}^{k}=\bm{u}^{k_{1}}\).

We will take \(\eta>0\) to be a small constant to be specified layer and \(\kappa_{i}\in\mathrm{Unif}([1/2,3/2])\) be the random perturbation. We also let \(b=\Omega(d)\), hiding constants in \(\eta,\varepsilon,P,K,\mu\). For the first phase, it suffices to train for \(k_{1}=P\) time steps. For the second phase, we train for \(k_{2}=k_{2}(\eta,\varepsilon,P,K,\mu)=\Theta(1)\) time steps to be specified later as we analyze. Note that only unspecified hyperparameter is the global bias initialization \(\bar{c}\) and the results holds almost surely over this choice.

**Theorem E.1** (Formal statement of Theorem 7.2).: _Assume \(\ell\) is analytic, convex and that Assumption D.4 holds. Then for any \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}(\mu)=1\) setting, for any \(\mathcal{D}_{\mu,s_{*}}^{d}\in\mathcal{H}_{\mu}^{d}\), almost surely over the initialization \(\bar{c}\in\mathbb{R}\), and \(\bm{\kappa}\in[1/2,3/2]^{d}\), the following holds. For any \(\varepsilon>0\), with_

\[b\geq\Omega(d),M=\Omega(1),\text{ and }k_{2}=\Omega(1),\]

_using \(\Omega(\cdot)\) to hide constants in \(\varepsilon,K,P,\mu,\eta,\bm{\kappa}\) and \(\bar{c}\), the above specified layer-wise training dynamics reaches \(\overline{\mathcal{R}}(f_{\mathsf{NN}}(\cdot;\bm{\Theta}^{k_{1}+k_{2}}))\leq\varepsilon\) with probability at least \(9/10\)._

### Proof of Theorem E.1

The proof of the theorem proceeds by first showing a non-asymptotic approximation guarantee between (d-DF-PDE) and (\(\ell\)-bSGD-g). To this end, we first start by noting a generalization of [1, Theorem 23] to any loss under satisfying Assumption D.4.

**Proposition E.2**.: _Assume that Assumption D.4 on the loss holds. Then for any \(0<\eta\leq K\) and for any \(\Upsilon\in\mathbb{N}\), there exists a constant \(C(K,\Upsilon)\) such that for (\(\ell\)-bSGD-g) and (d-DF-PDE) of the specified layer-wise training dynamics, with probability \(1-1/M\), we have_

\[\inf_{k=0,\dots,\Upsilon}\left|\mathcal{R}(\hat{f}_{\mathsf{NN}}(\cdot;\bm{ \Theta}^{k}))-\mathcal{R}(\hat{f}_{\mathsf{NN}}(\cdot;\bar{\rho}_{k}))\right| \leq C(K,\Upsilon)\left(\sqrt{\frac{P+\log d}{d}}+\sqrt{\frac{\log M}{M}}+ \sqrt{\frac{d+\log M}{b}}\right)\]

The proof of the proposition follows the similar arguments like propagation of chaos used in Theorem D.5. A direct corollary of this proposition is that it suffices to focus only on the (d-DF-PDE) to show Theorem E.1.

**Corollary E.3**.: _Assume that Assumption D.4 holds. Then for total \(\Upsilon\) number of (\(\ell\)-bSGD-g) iterations on a neural network with \(M\geq C_{0}(K,\Upsilon,\varepsilon)\) and the batch-size \(b\geq C_{1}(K,\Upsilon,\varepsilon)\cdot d\), the aforementioned layer-wise training dynamics, with probability at least \(9/10\)_

\[\inf_{k=0,\dots,\Upsilon}\left|\mathcal{R}(\hat{f}_{\mathsf{NN}}(\cdot;\bm{ \Theta}^{k}))-\mathcal{R}(\hat{f}_{\mathsf{NN}}(\cdot;\bar{\rho}_{k}))\right| \leq\frac{\varepsilon}{2}.\]

Proof.: It is easy to see that it is possible to choose \(M\geq C_{0}(K,\Upsilon,\varepsilon)\) and \(b\geq C_{1}(K,\Upsilon,\varepsilon)d\) for sufficiently large constants such that the right hand side of equality in Proposition E.2 is bounded by \(\varepsilon/2\) and \(1/M\leq 1/10\). The corollary then follows from Proposition E.2.

In light of Corollary E.3, it suffices to simply focus on (d-DF-PDE) and show that it achieves the excess error of at most \(\varepsilon/2\) within total steps \(\Upsilon=O(1)\) hiding constants in \(K,\mu,P,\varepsilon\). This will be done by analyzing each phase separately--\((i)\) showing that the training of the top layer weights for steps \(k\in\{k_{1},\ldots,k_{1}+k_{2}-1\}\) in Phase 2 is a linear model trained with a convex loss so it succeeds in reaching a vanishing risk as long as the corresponding kernel matrix of the feature map is non-degenerate; \((ii)\) indeed showing that the corresponding kernel matrix of the feature map after \(k_{1}=P\) steps of Phase 1 training on the first layer weights is non-degenerate.

Simplified dynamics.We first make a simple observation that \(b^{k}=0,s^{k}=0\) and \(c^{k}=\bar{c}\) throughout; this allows us to analyze a simpler dynamics. To see this, for \((b^{k},c^{k})\), we use \(\eta^{c}=\eta^{b}=0\) throughout, and thus, they remain the same from the initialization. We start with \(s^{0}=0\) and using (d-DF-PDE) (assuming \(s^{k}=0\)), gives us

\[s^{k+1}=0-\eta_{k}^{w}\left(\mathbb{E}_{\bm{z},G}[\ell^{\prime}(\hat{f}_{ \text{NN}}(\bm{z};\bar{\rho}_{k}),y)a^{k}\sigma^{\prime}(\langle\bm{u}^{k},\bm {z}\rangle+b^{k})G]+\lambda^{w}0\right)=0.\]

### Phase 2 (linear training)

We first analyze a slightly simpler Phase 2 of training. For a convex loss \(\ell\), the second layer training just corresponds to linear dynamics with kernel \(\bm{K}^{k_{1}}:\{+1,-1\}^{P}\times\{+1,-1\}^{P}\to\mathbb{R}\) given by

\[\bm{K}^{k_{1}}(\bm{z},\bm{z}^{\prime})=\mathbb{E}_{a\sim\mu_{a}}[\sigma( \langle\bm{u}^{k_{1}}(a),\bm{z}\rangle)\sigma(\langle\bm{u}^{k_{1}}(a),\bm{z} ^{\prime}\rangle)].\] (kernel)

We first show some helper lemmas, and using them, show the main lemma. We will then show the proof of the helper lemmas. We start by proving the existence of a sparse function with a small excess error.

**Lemma E.4**.: _For any \(\varepsilon>0\), there exists a function \(f_{*}:\{\pm 1\}^{d}\to\mathbb{R}\) such that_

\[\overline{\mathcal{R}}(f_{*})=\left[\mathcal{R}(f_{*})-\inf_{\bar{f}\cdot\{\pm 1 \}^{d}\to\mathbb{R}}\mathcal{R}(\bar{f})\right]\leq\frac{\varepsilon}{4}.\] (74)

_Moreover, \(f_{*}(\bm{x})=h_{*}(\bm{z})\) for some \(h_{*}:\{\pm 1\}^{P}\to\mathbb{R}\) independent of \(d\), where \(\bm{z}\) is the support of \(\mathcal{D}^{d}_{s_{*}}\)._

Approximation.The neural network output after training \(\bm{u}\) for \(k_{1}\) steps is given by:

\[\hat{f}_{\text{NN}}(\bm{z};\bar{\rho}_{k_{1}})=\bar{c}+\int a\sigma(\langle \bm{u}^{k_{1}}(a),\bm{z}\rangle)\mathrm{d}\mu_{a}=\bar{c}+\mathbb{E}_{a\sim\mu _{a}}\left[a\sigma(\langle\bm{u}^{k_{1}}(a),\bm{z}\rangle)\right].\]

Observing that for the next \(k_{2}\) steps, only \(a\) is trained while keeping \(\bm{u}^{k_{1}}\) fixed, the neural network output is of the following form

\[\hat{f}_{\text{NN}}(\bm{z};\bar{\rho})=\bar{c}+\int a\sigma(\langle\bm{u},\bm {z}\rangle)\mathrm{d}\bar{\rho}(a,\bm{u})=\bar{c}+\int a\sigma(\langle\bm{u}, \bm{z}\rangle)\mathrm{d}\bar{\rho}(a(\bm{u}))\mathrm{d}\bar{\rho}_{k_{1}}(\bm {u}).\] (75)

now start by showing that our neural network, there exists \(\bar{\rho}_{*}\) such that we can exactly represent \(h_{*}(\bm{z})\) as long as the resultant (kernel) after \(1^{\text{st}}\) layer training is non-degenerate.

**Lemma E.5**.: _If \(\lambda_{\min}(\bm{K}^{k_{1}})\geq c\) for some constant \(c\), then there exists \(\bar{\rho}_{*}\in\mathcal{P}(\mathbb{R}^{P+1})\) such that \(\mathrm{d}\bar{\rho}_{*}(a,\bm{u})=\mathrm{d}\bar{\rho}(a_{*}(\bm{u}))\mathrm{ d}\bar{\rho}_{k_{1}}(\bm{u})\) and for all \(\bm{z}\in\{\pm 1\}^{P}\)_

\[h_{*}(\bm{z})=\hat{f}_{\text{NN}}(\bm{z};\bar{\rho}_{*})=\bar{c}+\int a_{*} \sigma(\langle\bm{u},\bm{z}\rangle)\mathrm{d}\bar{\rho}_{*}(a_{*}(\bm{u})) \mathrm{d}\bar{\rho}_{k_{1}}(\bm{u}).\]

_Moreover, \(\|a^{*}\|_{\bar{\rho}_{*}}^{2}=\int a_{*}^{2}(\bm{u})\mathrm{d}\bar{\rho}_{*}( a_{*}(\bm{u}))\leq c_{1}\) for some constant \(c_{1}:=c_{1}(\lambda_{\min}(\bm{K}^{k_{1}}),h_{*},\bar{c})\)._

Convexity and Smoothness.Finally, we will show that the risk objective in terms of \(\bar{\rho}\) (of the form (75)) is convex and smooth in terms of \(\bar{\rho}\). The convexity follows from the convexity of \(\ell(\cdot,y)\) and observing that the neural network output in (75) is linear in \(\bar{\rho}(a(\bm{u}))\). The smoothness follows from Assumption D.4.

**Lemma E.6**.: _Consider \(\ell:\mathbb{R}\times\mathcal{Y}\to\mathbb{R}_{\geq 0}\) such that \(\ell(\cdot,y)\) is convex for every \(y\in\mathcal{Y}\) and that Assumption D.4 holds. Then_

\[\mathcal{R}(\bar{\rho})=\mathbb{E}[\ell(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho} ),y)],\quad\text{where }\mathrm{d}\bar{\rho}(a,\bm{u})=\mathrm{d}\bar{\rho}(a(\bm{u})) \mathrm{d}\bar{\rho}_{k_{1}}(\bm{u}),\]

_is convex and \(H\)-smooth with \(H=K^{3}\)._

While we denote \(\mathcal{R}(\bar{\rho})\) as an objective in terms of the joint measure \(\bar{\rho}(a,\bm{u})\), it is actually an objective in \(\bar{\rho}(a(\bm{u})))\) since it is of the form where \(\mathrm{d}(a,\bm{u})=\mathrm{d}\bar{\rho}(a(\bm{u}))\mathrm{d}\bar{\rho}_{k_{1 }}(\bm{u})\). Also, the convexity and smoothness is in terms of the argument \(\bar{\rho}(a(\bm{u}))\). Having established the convexity and smoothness of the \(2^{\mathrm{nd}}\) layer weights training, we will directly apply the convergence rate guarantees for convex and smooth objectives.

**Lemma E.7**.: _Consider any convex, differentiable objective \(f(x)\) that is \(H\)-smooth. Then for any step-size \(\eta\leq\frac{1}{H}\), the gradient descent from initialization \(x^{0}\) reaches an iterate \(x^{k}\) after \(k\) steps such that for any \(\tilde{x}\) (not necessarily a minimizer), we have_

\[f(x^{T})-f(\tilde{x})\leq\frac{\|x^{0}-\tilde{x}\|^{2}}{2\eta k}.\]

This is a classical convergence rate guarantee for gradient descent on convex smooth objective [1, Theorem 3.3], but noting that a proof generalizes even when \(\tilde{x}\) is not necessarily a minimizer. We will apply this guarantees with \(\tilde{x}\) as a near minimizer of \(f\), especially when the actual minimizer may not exist (for example with the logistic loss, the infimum is never achieved). We provide the proof with other lemmas for completeness.

**Lemma E.8**.: _Assume that \(\lambda_{\min}(\bm{K}^{k_{1}})>c>0\) for some constant \(c\), where \(\bm{K}^{k_{1}}\) is the (kernel) matrix. Then discrete dimension-free dynamics (d-DF-PDE) in Phase 2 of layer-wise training with step-size \(\eta\leq\frac{1}{K^{3}}\) as specified before after \(k_{2}:=k_{2}(\eta,\varepsilon,P,K,\mu,\bar{c})\) steps achieves_

\[\overline{\mathcal{R}}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{k_{1}+k_{2}})) =\left[\mathcal{R}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{k_{1}+k_{2}}))- \inf_{\bar{f}:\{\pm 1\}^{d}\to\mathbb{R}}\mathcal{R}(\bar{f})\right]\leq\frac{ \varepsilon}{2}.\]

Proof.: First of all, using Lemma E.4, there exists \(\bar{\rho}_{*}\) such that

\[\overline{\mathcal{R}}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{*}))=\mathcal{ R}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{*}))-\inf_{\bar{f}:\{\pm 1\}^{d}\to\mathbb{R}}\mathcal{R}(\bar{f})\leq\frac{ \varepsilon}{4}.\] (76)

Moreover, by Lemma E.6, the objective \(\mathcal{R}(\bar{\rho})\) is convex and \(K^{3}\)-smooth, and observe that (d-DF-PDE) are performing gradient descent with step-size \(\eta_{k}^{a}=\eta\leq\frac{1}{K^{3}}\) on this objective. Thus, using Lemma E.7

\[\left[\mathcal{R}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{k_{1}+k}))- \mathcal{R}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{*}))\right]:=\left[ \mathcal{R}(\bar{\rho}^{k_{1}+k})-\mathcal{R}(\bar{\rho}_{*})\right]\leq\frac {\|a^{k_{1}}-a^{*}\|_{\bar{\rho}_{k_{1}},\bar{\rho}_{*}}^{2}}{2\eta k}.\] (77)

Recall that \(a^{k_{1}}\sim\mu_{a}\equiv\mathrm{Unif}([-1,1])\).

\[\|a^{k_{1}}-a^{*}\|_{\bar{\rho}_{k_{1}},\bar{\rho}_{*}}^{2}=\int(a-a^{*}(\bm{u} ))^{2}\mathrm{d}\mu_{a}\mathrm{d}\bar{\rho}_{*}(a^{*}(\bm{u}))\mathrm{d}\bar{ \rho}_{k_{1}}(\bm{u})\leq 1+2\|a^{*}\|_{\bar{\rho}_{*}}+\|a^{*}\|_{\bar{\rho}_{*}}^{2} \leq c_{2},\]

for some constant \(c_{2}:=c_{2}(\lambda_{\min}(\bm{K}^{k_{1}}),P,\mu,\bar{c})\). Therefore, choosing \(k_{2}\geq\frac{2c_{2}}{\eta\varepsilon}\)

\[\left[\mathcal{R}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{k_{1}+k_{2}}))- \mathcal{R}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{*}))\right]\leq\frac{ \varepsilon}{4}.\] (78)

Combining (78) and (76)

\[\overline{\mathcal{R}}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{ k_{1}+k_{2}})) =\left[\mathcal{R}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{k_{1}+k_{2}}))- \mathcal{R}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{*}))\right]\] \[\quad+\left[\mathcal{R}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{* }))-\inf_{\bar{f}:\{\pm 1\}^{d}\to\mathbb{R}}\mathcal{R}(\bar{f})\right]\] \[\leq\frac{\varepsilon}{2}.\]

Thus, if we can show that \(\lambda_{\min}(\bm{K}^{k_{1}})\geq c>0\) for some constant \(c:=c(\eta,P,K,\mu)\) for a sufficiently small constant \(\eta>0\) that only depends on \(K,P,\mu\), then it immediately implies the desired result by Corollary E.3 with \(\Upsilon=k_{1}+k_{2}\). The goal of Appendix E.4 is to show that this holds after \(k_{1}=P\) steps of the first layer weight training. We now return to the deferred proofs of helper lemmas.

#### e.3.1 Proof of helper lemmas

Proof of Lemma e.4.: Note that Eq. (74) simply follows from the definition of infimum and the excess risk functional \(\overline{\mathcal{R}}(\cdot)\). To show that \(f_{*}(\bm{x})=h_{*}(\bm{z})\), we make an observation that for any function \(f:\{\pm 1\}^{d}\to\mathbb{R}\), one can define \(h_{*}:\{\pm 1\}^{P}\to\mathbb{R}\) that only depend on the support and achieve risk no worse than \(f_{*}\). Define \(h_{*}(\bm{z})=\mathbb{E}_{\bm{z}^{c}}[f_{*}(\bm{x})]\), where \(\bm{z}^{c}=\bm{x}\setminus\bm{z}\) (the coordinates outside the support). In other words, \(h_{*}\) is the boolean function after ignoring the monomials that do not depend on \(\bm{z}\). Then

\[\mathcal{R}(f_{*}) =\mathbb{E}_{(y,\bm{x})\sim\mathcal{D}_{*}^{d}}\left[\ell(f_{*}( \bm{x}),y)\right]=\mathbb{E}_{\mu_{y,\bm{x}}}[\mathbb{E}_{\mu_{\bm{x}^{c}|y, \bm{z}}}[\ell(f_{*}(\bm{x}),y)]]\] \[\geq\mathbb{E}_{\mu_{y,\bm{x}}}[\ell(\mathbb{E}_{\bm{z}^{c}|y,\bm {z}}[f_{*}(\bm{x})],y)]=\mathbb{E}[\ell(h_{*}(\bm{z}),y)]=\mathcal{R}(h),\]

where the only inequality follows from Jensen's inequality and convexity of \(\ell(\cdot,y)\). 

Proof of Lemma e.5.: Define \(g_{*}(\bm{z})=h_{*}(\bm{z})-\bar{c}\). Our goal is to show the existence of \(\bar{\rho}_{*}\) such that

\[g_{*}(\bm{z})=\mathbb{E}_{(a_{*},\bm{u})\sim\bar{\rho}_{*}}[a_{*}\sigma(\langle \bm{u},\bm{z}\rangle)].\]

Consider the following objective least square objective for the domain

\[\mathcal{L}(a(\bm{u});\lambda)=\sum_{\bm{z}\in\{\pm 1\}^{P}}\left(\mathbb{E}_{ \bm{u}\sim\bar{\rho}_{k_{1}}(\bm{u})}[a(\bm{u})\sigma(\langle\bm{u},\bm{z} \rangle)]-g_{*}(\bm{z})\right)^{2}+\lambda\int a^{2}(\bm{u})\mathrm{d}\bar{ \rho}_{k_{1}}(\bm{u})\]

While this is an infinite dimensional problem, the Representer's theorem holds and the interpolating solution exists. We refer the reader to (Celentano et al., 2021) for a detailed analysis of interpolation with the random feature model. Moreover,

\[\|a^{*}\|_{\bar{\rho}_{*}}\leq\lambda_{\min}(\bm{K}^{k_{1}})^{-1}\sqrt{\sum_{ \bm{z}\in\{\pm 1\}^{P}}g_{*}(\bm{z})^{2}}\leq\ c_{1},\]

for some constant \(c_{1}\) that only depends on \(\lambda_{\min}(\bm{K}^{k_{1}}),h_{*},P\) and \(\bar{c}\). 

Proof of Lemma e.6.: For \(t\in[0,1]\) and any \(\bar{\rho}^{(1)}\) and \(\bar{\rho}^{(2)}\), consider the density \(\bar{\rho}=t\bar{\rho}^{(1)}+(1-t)\bar{\rho}^{(2)}\). Then

\[\mathcal{R}(\bar{\rho}) =\mathcal{R}(t\bar{\rho}^{(1)}+(1-t)\bar{\rho}^{(2)})=\mathbb{E}[ \ell(\hat{f}_{\mathsf{NN}}(\bm{z};t\bar{\rho}^{(1)}+(1-t)\bar{\rho}^{(2)}),y)]\] \[=\mathbb{E}[\ell(t\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(1)}) +(1-t)\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(2)}),y)]\] \[\leq\mathbb{E}[t\ell(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(1 )}),y)+(1-t)\ell(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(2)}),y)]\] (using convexity) \[=t\mathbb{E}\left[\ell(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{( 1)}),y)\right]+(1-t)\mathbb{E}\left[\ell(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{ \rho}^{(2)}),y)\right]=t\,\mathcal{R}(\bar{\rho}^{(1)})+(1-t)\mathcal{R}(\bar{ \rho}^{(2)}).\]

To show smoothness, first observe that a direct consequence of Assumption D.4 (the condition \(|\ell^{\prime\prime}(u,y)|\leq K\)) is that for any \(u_{1},u_{2}\in\mathbb{R}\), we have

\[\ell(u_{1},y)\leq\ell(u_{2},y)+\ell^{\prime}(u_{2},y)(u_{1}-u_{2})+\frac{K}{2} (u_{2}-u_{1})^{2}.\] (79)

Using this, for any \(\bar{\rho}^{(1)}\) and \(\bar{\rho}^{(2)}\)

\[\mathcal{R}(\bar{\rho}^{(1)}) =\mathbb{E}[\ell(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(1)}),y)]\] \[\leq\mathbb{E}[\ell(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(2) }),y)]+\mathbb{E}\left[\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(2 )}),y)\left(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(1)})-\hat{f}_{\mathsf{NN }}(\bm{z};\bar{\rho}^{(2)})\right)\right]\] (using (79)) \[\qquad+\frac{K}{2}\mathbb{E}\left[\left(\hat{f}_{\mathsf{NN}}(\bm{ z};\bar{\rho}^{(1)})-\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(2)})\right)^{2}\right]\] (80)

Since \(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(1)})-\hat{f}_{\mathsf{NN}}(\bm{z}; \bar{\rho}^{(2)})=\mathbb{E}_{a^{(1)},a^{(2)},\bm{u}}\left[(a^{(1)}-a^{(2)}) \sigma(\langle\bm{u},\bm{z}\rangle)\right]\), further simplifying the above

\[\mathbb{E}\left[\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{ \rho}^{(2)}),y)\left(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(1)})-\hat{f}_{ \mathsf{NN}}(\bm{z};\bar{\rho}^{(2)})\right)\right]\] \[=\mathbb{E}_{(y,\bm{z})\sim\mu_{y,\bm{z}}}\left[\ell^{\prime}(\hat {f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{(2)}),y)\mathbb{E}_{a^{(1)},a^{(2)},\bm{u}} \left[(a^{(1)}-a^{(2)})\sigma(\langle\bm{u},\bm{z}\rangle)\right]\right]\] \[=\mathbb{E}_{a^{(1)},a^{(2)},\bm{u}}\left[\mathbb{E}_{(y,\bm{z}) \sim\mu_{y,\bm{z}}}\left[\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}^{( 2)}),y)\sigma(\langle\bm{u},\bm{z}\rangle)\right](a^{(1)}-a^{(2)})\right]\] \[=\left\langle\nabla_{a}\mathcal{R}(\bar{\rho}^{(2)}),a^{(1)}-a^{(2)} \right\rangle_{\bar{\rho}^{(1)},\bar{\rho}^{(2)}}.\] (81)Also, simplifying the third term of (80)

\[\frac{K}{2}\mathbb{E} \left[\left(\hat{f}_{\textsf{NN}}(\bm{z};\bar{p}^{(1)})-\hat{f}_{ \textsf{NN}}(\bm{z};\bar{p}^{(2)})\right)^{2}\right]=\frac{K}{2}\mathbb{E}_{(y,\bm{z})\sim\mu_{y,\star}}\left[\mathbb{E}_{a^{(1)},a^{(2)},\bm{u}}\left[(a^{(1 )}-a^{(2)})\sigma(\langle\bm{u},\bm{z}\rangle)\right]^{2}\right]\] \[\leq\frac{K}{2}\mathbb{E}_{(y,\bm{z})\sim\mu_{y,\star}}\mathbb{E}_ {a^{(1)},a^{(2)},\bm{u}}\left[(a^{(1)}-a^{(2)})^{2}\sigma^{2}(\langle\bm{u}, \bm{z}\rangle)\right]\] (by Jensen's ineqaulity) \[\leq\frac{K^{3}}{2}\|a^{(1)}-a^{(2)}\|^{2}_{\bar{p}^{(1)},\bar{p} ^{(2)}}.\] (82)

Finally, combining (81) and (82) with (80), we obtain

\[\mathcal{R}(\bar{\rho}^{(1)})=\mathcal{R}(\bar{\rho}^{(2)})+\left\langle \nabla_{a}\mathcal{R}(\bar{\rho}^{(2)}),a^{(1)}-a^{(2)}\right\rangle_{\bar{ \rho}^{(1)},\bar{\rho}^{(2)}}+\frac{K^{3}}{2}\|a^{(1)}-a^{(2)}\|^{2}_{\bar{ \rho}^{(1)},\bar{\rho}^{(2)}},\]

which, by definition, gives us \(H\)-smoothness with \(H=K^{3}\). 

Proof of Lemma e.7.: First, the classical descent lemma holds due to \(H\)-smoothness for step-size \(\eta\leq\frac{1}{H}\):

\[f(x^{j+1}) \leq f(x^{j})+\langle\nabla f(x^{j}),x^{j+1}-x^{j}\rangle+\frac{ H}{2}\|x^{j+1}-x^{j}\|^{2}_{2}\] \[=f(x^{j})-\eta\left(1-\frac{\eta H}{2}\right)\|\nabla f(x^{j})\| ^{2}_{2}\leq f(x^{j})-\frac{\eta}{2}\|\nabla f(x^{j})\|^{2}_{2}\leq f(x^{j}).\] (83)

By convexity, for any \(\tilde{x}\) (not necessarily a minimizer), we also have

\[f(x^{j})\leq f(\tilde{x})+\langle\nabla f(x^{j}),x^{j}-\tilde{x}\rangle.\]

Substituting this in (83)

\[f(x^{j+1}) \leq f(\tilde{x})+\langle\nabla f(x^{j}),x^{j}-\tilde{x}\rangle- \frac{\eta}{2}\|\nabla f(x^{j})\|^{2}_{2}\] \[=f(\tilde{x})+\frac{1}{2\eta}\left(2\eta\langle\nabla f(x^{j}),x ^{j}-\tilde{x}\rangle-\eta^{2}\|\nabla f(x^{j})\|^{2}_{2}\right)\] \[=f(\tilde{x})+\frac{1}{2\eta}\left(2\eta\langle\nabla f(x^{j}),x ^{j}-\tilde{x}\rangle-\eta^{2}\|\nabla f(x^{j})\|^{2}_{2}-\|x^{j}-\tilde{x} \|^{2}_{2}+\|x^{j}-\tilde{x}\|^{2}_{2}\right)\] \[=f(\tilde{x})+\frac{1}{2\eta}\left(\|x^{j}-\tilde{x}\|^{2}_{2}-\| x^{j+1}-\tilde{x}^{2}\|^{2}_{2}\right).\]

Therefore, summing over \(j\in\{0,\ldots,k-1\}\), since the right hand side is a telescopic sum

\[\sum_{j=0}^{k-1}\left(f(x^{j+1})-f(\tilde{x})\right)\leq\frac{1}{2\eta}\left( \|x^{0}-\tilde{x}\|^{2}_{2}-\|x^{k}-\tilde{x}\|^{2}_{2}\right)\leq\frac{\|x^ {0}-\tilde{x}\|^{2}_{2}}{2\eta}.\]

Note that the lemma statements trivially holds if \(f(x^{k})-f(\tilde{x})\leq 0\) already. Therefore, in the case when \(f(x^{k})>f(\tilde{x})\) and using the descent lemma, we finally conclude the proof.

\[f(x^{k})-f(\tilde{x})\leq\frac{1}{k}\sum_{j=0}^{k-1}\left(f(x^{j})-f(\tilde{x })\right)\leq\frac{\|x^{0}-\tilde{x}\|^{2}}{2\eta k}.\]

### Phase 1 (non-linear training)

We now analyze Phase 1 of the algorithm and show that \(\lambda_{\min}(\bm{K}^{k_{1}})\geq c\), for a constant \(c:=c(\eta,\mu,P,K)\) where \(\eta\) is also a small constant (in terms of \(\mu,P,K\)).

Writing the weight evolution with a polynomial.We will by considering a general polynomial activation \(\sigma(x)=\sum_{l=0}^{L}m_{l}x^{l}\) of degree \(L\), whose coefficients are given by \(\bm{m}=(m_{0},\ldots,m_{L})\).

**Lemma E.9**.: _(Training dynamics given by a polynomial) Let \(\bm{\xi}=(\xi_{S,k})_{S\subseteq[P],0\leq k\leq k_{1}-1}\in\mathbb{R}^{2^{P}k_{ 1}}\), \(\zeta\in\mathbb{R}\), \(\bm{\rho}\in\mathbb{R}^{L+1},\bm{\gamma}\in\mathbb{R}^{P}\) be variables. For each \(i\in[P]\), define \(p_{0,i}(\zeta,\bm{\xi},\bm{\rho},\bm{\gamma})\equiv 0\). For each, \(0\leq k\leq k_{1}-1\),_

\[p_{k+1,i}(\zeta,\bm{\xi},\bm{\rho},\bm{\gamma}) =p_{k,i}(\zeta,\bm{\xi},\bm{\rho},\gamma)+\zeta\gamma_{i}\rho_{1} \bm{\xi}_{\{i\},k}\] \[\quad+\zeta\gamma_{i}\sum_{r=1}^{L-1}\frac{\rho_{r+1}}{r!}\sum_{( i_{1},\ldots,i_{r})\in[P]^{r}}\xi_{i\oplus i_{1}\oplus\cdots\oplus i_{r},k} \prod_{l=1}^{r}p_{k,i_{l}}(\zeta,\bm{\xi},\bm{\rho},\bm{\gamma}).\]

_Then there is a constant \(c:=c(k_{1},P,K)\) such that for any \(0<\eta<c\)_

\[u_{i}^{k}(a)=p_{k,i}(\eta a,\bm{\beta},\bm{m},\bm{\kappa}),\]

_and \(\bm{\beta}=(\beta_{S,k})_{S\subseteq[P],0\leq k\leq k_{1}-1}\) is given by \(\beta_{S,k}=-\mathbb{E}[\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_ {k}),y)\chi_{S}(\bm{z})]\)._

The term \(\hat{f}_{\mathsf{NN}}(\cdot;\bar{\rho}_{k})\) evolves non-linearly, and difficult to analyze directly. However, for sufficiently small step size \(\eta\), the interaction term \(\hat{f}_{\mathsf{NN}}(\cdot;\bar{\rho}_{k})\) is small, and we show that it can be ignored. Formally, we define the simplified dynamics \(\hat{\bm{u}}^{k}(a)\) for each \(0\leq k\leq k_{1}\) by letting \(\hat{\bm{u}}^{0}(a)=\bm{0}\) and inductively setting for each \(k\in\{0,\ldots,k_{1}-1\}\),

\[\hat{\bm{u}}^{k+1}(a)=\hat{\bm{u}}^{k}(a)-\eta\bm{\kappa}\circ\mathbb{E}[\ell ^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{0}),y)\cdot a\cdot\sigma^{ \prime}(\langle\hat{\bm{u}}^{k}(a),\bm{z}\rangle)\bm{z}].\]

Using a similar argument, we may show:

**Lemma E.10** (Simplified training dynamics are given by a polynomial).: _There is a constant \(c>0\) depending only on \(k_{1},P,K\), such that for any \(0<\eta<c\), any \(i\in[P]\) and any \(0\leq k\leq k_{1}\), we have_

\[\hat{u}_{i}^{k}(a)=p_{k,i}(\eta a,\bm{\alpha},\bm{m},\bm{\kappa}),\]

_where we abuse notation (since \(\bm{\alpha}=(\alpha_{S})_{S\subseteq[P]}\) otherwise) and let \(\bm{\alpha}=(\alpha_{S,k})_{S\subseteq[P],0\leq k\leq k_{1}-1}\) be given by_

\[\alpha_{S,k}=\alpha_{S}=\mathbb{E}[-\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z };\bar{\rho}_{0}),y)\chi_{S}(\bm{z})]\]

Reducing to analyzing simplified dynamicsWe lower-bound \(\lambda_{\min}(\bm{K}^{k_{1}})\) in terms of the determinant of a certain random matrix. Let \(\bm{\zeta}=[\zeta_{1},\ldots,\zeta_{2^{P}}]\) be a vector of \(2^{P}\) variables. Define \(\bm{M}=\bm{M}(\bm{\zeta},\bm{\xi},\bm{\rho},\bm{\gamma})\in\mathbb{R}^{2^{P} \times 2^{P}}\) to be the matrix indexed by \(\bm{z}\in\{+1,-1\}^{P}\) and \(j\in[2^{P}]\) with entries

\[M_{\bm{z},j}(\bm{\zeta},\bm{\xi},\bm{\rho},\bm{\gamma})=\sum_{r=0}^{L}\frac{ \rho_{r}}{r!}\left(\sum_{i=1}^{P}z_{i}p_{k,i}(\zeta_{j},\bm{\xi},\bm{\rho},\bm{ \gamma})\right)^{r}.\]

This matrix is motivated by the following fact:

**Lemma E.11**.: _There is a constant \(c>0\) depending only on \(k_{1},P,K\), such that for any \(0<\eta<c\), and any \(\bm{a}=[a_{1},\ldots,a_{2^{P}}]\in[-1,1]^{2^{P}}\), we have_

\[M_{\bm{z},j}(\eta\bm{a},\bm{\beta},\bm{m},\bm{\kappa}) =\sigma(\langle\bm{u}^{k_{1}}(a_{j}),\bm{z}\rangle)\] \[M_{\bm{z},j}(\eta\bm{a},\bm{\alpha},\bm{m},\bm{\kappa}) =\sigma(\langle\hat{\bm{u}}^{k_{1}}(a_{j}),\bm{z}\rangle)\]

Under this notation, using the exactly same analysis as in [1, Lemma E.9], we can also show

**Lemma E.12**.: _There is a constant \(c>0\) depending on \(K,P\) such that for any \(0<\eta<c\),_

\[\lambda_{\min}(\bm{K}^{k_{1}})\geq c\mathbb{E}_{\bm{a}\sim\mu_{a}^{\otimes 2^{P}}}[ \det(\bm{M}(\eta\bm{a},\bm{\beta},\bm{m},\bm{\kappa}))^{2}].\]

On the other hand, we can prove a lower-bound on \(\mathbb{E}[\det(\bm{M}(\eta\bm{a},\bm{\beta},\bm{m},\bm{\kappa}))^{2}]\) simply by lower-bounding the sum of magnitudes of coefficients of \(\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\kappa}))\) when viewed as a polynomial in \(\bm{\zeta}\) almost surely over the choice of \(\bar{c}\). This is because of (a) the fact that \(\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\kappa}))\) and \(\det(\bm{M}(\bm{\zeta},\bm{\beta},\bm{m},\bm{\kappa}))\) have coefficients in \(\bm{\zeta}\) that are \(O(\eta)\)-close for \(\eta\) small, and (b) the fact that polynomials anti-concentrate over random inputs.

Analysis of simplified dynamics.The proof is now reduced to showing that \(\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\kappa}))\not\equiv 0\), as a polynomial in \(\bm{\zeta}\). In other words, by Lemma E.11, we are now focusing on only the simplified dynamics \(\hat{\bm{u}}^{k}\). If we can show that \(\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\kappa}))\not\equiv 0\) almost surely over the choice of \(\bm{\kappa}\in[1/2,3/2]^{P}\), then Theorem E.1 follows. Therefore, introducing the variables \(\bm{\gamma}\in\mathbb{R}^{P}\), it suffices to show that

\[\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\gamma}))\not\equiv 0,\text{ as a polynomial in }\bm{\zeta},\bm{\gamma}.\] (84)

We start by following simple observation.

**Claim 1**.: _Almost surely over the choice of initialization \(\bar{c}\), the set of subsets \(\mathcal{K}:=\{S:\alpha_{S}\neq 0\}\) has \(\mathsf{Leap}(\mathcal{K})=1\)._

Proof.: This basically follows from the fact that \(\ell\) is piecewise analytic. The argument is as follows: we have that \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}=\mathsf{Leap}(\mathcal{C}_{\mathsf{DLQ}_{ \ell}})=1\), where \(\mathcal{C}_{\mathsf{DLQ}_{\ell}}=\{S:\exists u\in\mathbb{R}\text{ such that }\mathbb{E}[\ell^{\prime}(u,y)\chi_{S}(\bm{z})]\neq 0\}\). Since \(\ell\) is piece-wise analytic, so is \(\ell^{\prime}(u,y)\) in the first argument. Thus, for any \(S\in\mathcal{C}_{\mathsf{DLQ}_{\ell}}\), the set \(\{u:\mathbb{E}[\ell^{\prime}(u,y)\chi_{S}(\bm{z})]=0\}\) has measure 0. Thus, almost surely over the choice of \(\bar{c}=\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{0})\), we have that for any \(S\in\mathcal{C}_{\mathsf{DLQ}_{\ell}}\)

\[\mathbb{E}[\ell^{\prime}(\bar{c},y)\chi_{S}(\bm{z})]=\mathbb{E}[\ell^{\prime} (\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{0}),y)\chi_{S}(\bm{z})]=-\alpha_{S} \neq 0.\]

This implies that \(S\in\mathcal{K}\) as well. We finally conclude that \(\mathsf{Leap}_{\mathsf{DLQ}_{\ell}}=\mathsf{Leap}(\mathcal{K})=1\). 

Using ideas similar to Abbe et al. (2022), we now prove (84) by analyzing the recurrence relations for \(p_{k,i}\) to show that to first-order the polynomials \(p_{k_{1},i}\) are distinct for all \(i\in[P]\), but now doing smooth analysis over \(\bm{\gamma}\) instead. Formally we show the following lemma.

**Lemma E.13**.: _Suppose that \(L\geq 2^{8P}\) and let \(m_{i}=i!\binom{L}{i}\) for all \(0\leq i\leq L\), which correspond to the activation function \(\sigma(x)=(1+x)^{L}\). Let \(k_{1}=P\). Then \(\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\gamma}))\not\equiv 0\), i.e. (84) holds._

**Claim 2**.: _For any time step \(0\leq k\leq k_{1}\) any \(j\in[N]\), a learning rate \(\eta<1/(4K^{2}(1+K)Pk)\), and any \(a\in[-1,1]\) we have_

\[\|\bm{u}^{k}(a)\|_{1},\|\hat{\bm{u}}^{k}(a)\|_{1}\leq 2\eta K^{2}(1+K)Pk\leq 1/2.\]

Proof.: We prove this by induction on \(k\). For \(k=0\), we have \(\bm{u}^{0}=\hat{\bm{u}}^{0}=\bm{0}\) and the claim holds trivially. For the inductive step, \(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k})\leq\mathbb{E}_{a\sim\mu_{a}}[|a| |\sigma(\langle\bm{u}^{k},\bm{z}\rangle)]\leq\|\sigma\|_{\infty}\leq K\), since \(a\sim\mathrm{Unif}([-1,1])\). Therefore

\[\|\bm{u}^{k+1}(a)\|_{1}\leq\|\bm{u}^{k}(a)\|_{1}+\eta\|\bm{\kappa} \circ\mathbb{E}_{y,\bm{z}}[-\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{ \rho}_{k}),y)a\sigma^{\prime}(\langle\bm{u}^{k},\bm{z}\rangle)\bm{z}]\|_{1}\] \[\leq\|\bm{u}^{k}(a)\|_{1}+\eta\|\bm{\kappa}\|_{\infty}\mathbb{E}_ {y,\bm{z}}[\|-\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k}),y)a \sigma^{\prime}(\langle\bm{u}^{k},\bm{z}\rangle)\bm{z}\|_{1}]\] \[\leq\|\bm{u}^{k}(a)\|_{1}+2\eta K(1+K)\cdot K\cdot\mathbb{E}_{ \bm{z}}[\|\bm{z}\|_{1}]\leq\|\bm{u}^{k}(a)\|_{1}+2\eta K^{2}(1+K)P\leq 2\eta K ^{2}(1+K)Pk.\]

Note that here we used \(\|\bm{\kappa}\|_{\infty}\leq 2\), \(|\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k}),y)|\leq K(1+|\hat{f} _{\mathsf{NN}}(\bm{z};\bar{\rho}_{k})|)\leq K(1+K)\) using \(\mathsf{A}2\), and \(|a\sigma^{\prime}(\langle\bm{u}^{k},\bm{z}\rangle)|\leq|a|\cdot|\sigma^{\prime}( \langle\bm{u}^{k},\bm{z}\rangle)|\leq K\). The bound for \(\|\hat{\bm{u}}^{k}(a)\|_{1}\) follows exactly the same argument but now we use \(|\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{0}),y)|\leq K\). 

Proof of Lemmas E.9 and E.10.: We have \(\zeta\in\mathbb{R}\), \(\bm{\xi}=(\xi_{S,k})_{S\subseteq[P],k\in\{0,\ldots,k_{1}-1\}}\), \(\bm{\rho}\in\mathbb{R}^{L+1}\), and \(\bm{\gamma}\in\mathbb{R}^{P}\) as variables. Let \(s_{0},\ldots,s_{k_{1}-1}:\{+1,-1\}^{P}\to\mathbb{R}\) be \(s_{k}(\bm{z})=\sum_{S\subseteq[P]}\xi_{S,k}\chi_{S}(\bm{z})\). Consider the recurrence relation \(\bm{\nu}^{k}\in\mathbb{R}^{P}\), where we initialize \(\bm{\nu}^{0}=\bm{0}\) and, for \(0\leq k\leq k_{1}-1\),

\[\bm{\nu}^{k+1}=\bm{\nu}^{k}+\zeta\bm{\gamma}\circ\mathbb{E}_{\bm{z}}\Big{[}s_{k}( \bm{z})\sum_{r=0}^{L-1}\frac{\rho_{r+1}}{r!}\langle\bm{\nu}^{k},\bm{z} \rangle^{r}\bm{z}\Big{]}.\] (85)This recurrence is satisfied by \(\zeta=\eta a\), \(\bm{\rho}=\bm{m}\), \(\bm{\gamma}=\bm{\kappa}\), and \(s_{k}(\bm{z})=\sum_{S}\beta_{S,k}\chi_{S}(\bm{z})\). This is because

\[\bm{u}^{k+1} =\bm{u}^{k}-\eta\cdot\bm{\kappa}\circ\mathbb{E}[\ell^{\prime}( \hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k}),y)a\sigma^{\prime}(\langle\bm{u}^{ k},\bm{z}\rangle)\bm{z}]\] \[=\bm{u}^{k}+\eta a\cdot\bm{\kappa}\circ\mathbb{E}[-\ell^{\prime}( \hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k}),y)\sum_{r=0}^{L-1}\frac{\bm{m}_{r+ 1}}{r!}\langle\bm{u}^{k},\bm{z}\rangle^{r}\bm{z}]\] \[=\bm{u}^{k}-\eta\cdot\bm{\kappa}\circ\mathbb{E}[\ell^{\prime}( \hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k}),y)a\sigma^{\prime}(\langle\bm{u}^ {k},\bm{z}\rangle)\bm{z}]\] \[=\bm{u}^{k}+\eta a\cdot\bm{\kappa}\circ\mathbb{E}[-\ell^{\prime}( \hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{k}),y)\sum_{r=0}^{L-1}\frac{\bm{m}_{r+ 1}}{r!}\langle\bm{u}^{k},\bm{z}\rangle^{r}\bm{z}]\] \[=\bm{u}^{k}+\eta a\cdot\bm{\kappa}\circ\mathbb{E}_{\bm{z}}[ \mathbb{E}_{y|\bm{z}}[-\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{ k}),y)]\sum_{r=0}^{L-1}\frac{\bm{m}_{r+1}}{r!}\langle\bm{u}^{k},\bm{z} \rangle^{r}\bm{z}]\]

Finally, noting that \(\mathbb{E}_{y|\bm{z}}[-\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_{ k}),y)]=\sum_{S}\beta_{S,k}\chi_{S}(\bm{z})\), we showed that recurrence (85) holds with specified value of variables. Similarly, \(\hat{\bm{u}}^{k}(a)\) with \(s_{k}(\bm{z})=\sum_{S}\alpha_{S}\chi_{S}(\bm{z})\) for \(\alpha_{S}=-\mathbb{E}[\ell^{\prime}(\hat{f}_{\mathsf{NN}}(\bm{z};\bar{\rho}_ {0},y)\chi_{S}(\bm{z})]\). This is because \(|\langle\bm{u}^{k},\bm{z}\rangle|,|\langle\hat{\bm{u}}^{k},\bm{z}\rangle|\leq 1 /2<1\) by Claim 2 and in the interval \((-1,1)\), we have \(\sigma(x)=\sum_{r=0}^{L}\frac{m_{r}}{r!}x^{r}\). It finally remains to show that

\[\nu_{i}^{k}=p_{k,i}(\zeta,\bm{\xi},\bm{\rho},\bm{\gamma}).\]

The proof is by induction on \(k\). For \(k=0\), it is true that \(p_{0,i}(\zeta,\bm{\xi},\bm{\rho},\bm{\gamma})=0=\nu_{i}^{0}\). For the inductive step, for any \(r\geq 1\) and \(i\in[d]\), we can write

\[\gamma_{i}\cdot\mathbb{E}_{\bm{z}}[s_{k}(\bm{z})\langle\bm{\nu}^{ k},\bm{z}\rangle^{r}z_{i}] =\gamma_{i}\mathbb{E}_{\bm{z}}\Big{[}s_{k}(\bm{z})z_{i}\sum_{(i_{ 1},\ldots,i_{r})\in[P]^{r}}\prod_{l=1}^{r}\nu_{i_{l}}^{k}z_{i_{l}}\Big{]}\] \[=\gamma_{i}\sum_{(i_{1},\ldots,i_{r})\in[P]^{r}}\mathbb{E}_{\bm{z }}\Big{[}s_{k}(\bm{z})\chi_{i}(\bm{z})\prod_{l=1}^{r}\chi_{i_{l}}(\bm{z})\Big{]} \prod_{l=1}^{r}p_{k,i_{l}}(\zeta,\bm{\xi},\bm{\rho},\bm{\gamma})\] \[=\gamma_{i}\sum_{(i_{1},\ldots,i_{r})\in[P]^{r}}\xi_{\{i\}\oplus\{ i_{1}\}\oplus\cdots\oplus\{i_{r}\},k}\prod_{l=1}^{r}p_{k,i_{l}}(\zeta,\bm{\xi}, \bm{\rho},\bm{\gamma}),\]

and \(\gamma_{i}\mathbb{E}_{\bm{z}}[s_{k}(\bm{z})\langle\bm{\nu}^{k},\bm{z}\rangle^{0 }z_{i}]=\gamma_{i}\mathbb{E}_{\bm{z}}[s_{k}(\bm{z})z_{i}]=\gamma_{i}\xi_{\{i\},k}\). The inductive step follows by linearity of expectation. 

### Proof of Lemma e.13

In Lemma E.13, we have already fixed \(\bm{m}\in\mathbb{R}^{L+1}\) to be \(m_{i}=i!\binom{L}{i}\) for all \(i\in\{0,\ldots,L\}\). This corresponds to the activation function \(\sigma(x)=(1+x)^{L}\).

#### e.5.1 Reducing to minimal Leap 1 structure

To show that \(\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\gamma}))\not\equiv 0\) as a polynomial in \(\bm{\zeta},\bm{\gamma}\), we first show that it suffices to consider "minimal" Leap 1 set structure, without loss of generality.

**Claim 3**.: _Let \(\mathcal{K}^{\prime}\subseteq\mathcal{K}\) such that \(\mathsf{Leap}(\mathcal{K}^{\prime})=1\). Then if_

\[\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\gamma}))\mid_{\alpha_{S}=0\text { for all }\alpha\in\mathcal{K}\setminus\mathcal{K}^{\prime}}\not\equiv 0,\text{ as a polynomial of }\bm{\zeta},\bm{\gamma}\]

_we have_

\[\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\gamma}))\not\equiv 0\text{ as a polynomial of }\bm{\zeta},\bm{\gamma}.\]

Proof.: Directly substituting \(\alpha_{S}=0\) for all \(S\in\mathcal{K}\setminus\mathcal{K}^{\prime}\). 

Therefore, without loss of generality (up to relabelling the indices of the variables), we assume that

\[\mathcal{K}=\{S_{1},\ldots,S_{P}\},\text{ where, for all }i\in[P]\,i\in S_{i}\text{ and }S_{i}\subseteq[i].\]

Otherwise, we could remove a set from \(\mathcal{K}\) while still having \(\mathsf{Leap}(\mathcal{K})=1\).

#### e.5.2 Weights to leading order

Let us define the polynomials \(q_{k,i}\) in variables \(\boldsymbol{\zeta},\boldsymbol{\phi},\boldsymbol{\rho},\boldsymbol{\gamma}\) where \(\boldsymbol{\phi}=(\phi_{S})_{S\in\mathcal{K}}\). For all \(k\in\{0,\ldots,k_{1}-1\}\) and \(i\in[P]\),

\[q_{k,i}(\zeta,\boldsymbol{\phi},\boldsymbol{\rho},\boldsymbol{\gamma})=p_{k,i} (\zeta,\boldsymbol{\xi},\boldsymbol{\rho},\boldsymbol{\gamma})\mid_{\xi_{S,k}=0}\text { for all }s_{S\notin\mathcal{K}\text{ and }\xi_{S,k}=\phi_{S}}\text{ for all }s_{\mathcal{K}}\.\]

Therefore \(\boldsymbol{M}(\boldsymbol{\zeta},\boldsymbol{\phi},\boldsymbol{\rho}, \boldsymbol{\gamma})\) has entries \(M_{\boldsymbol{z},j}(\boldsymbol{\zeta},\boldsymbol{\phi},\boldsymbol{\rho}, \boldsymbol{\gamma})=\sum_{r=0}^{L}\frac{\rho_{r}}{r!}\left(\sum_{i=1}^{P}q_{ k,i}(\zeta_{j},\boldsymbol{\phi},\boldsymbol{\rho},\boldsymbol{\gamma})\right)^{r}\). We will explicitly compute the lowest degree term in \(\zeta\). First, we show that many terms are zero. To this end, consider the following notation: Recursively define

\[o_{i}=1+\sum_{i^{\prime}\in S_{i}\setminus\{i\}}o_{i^{\prime}}\text{ for all }i\in[P],\]

where the sum over an empty set is \(0\) by convention. Additionally, let \(\tilde{q}_{k,i}(\phi,\boldsymbol{\rho},\boldsymbol{\gamma})\) to be the coefficient of the term in \(q_{k,i}(\zeta,\boldsymbol{\phi},\boldsymbol{\rho},\boldsymbol{\gamma})\) whose degree in \(\zeta\) is \(o_{i}\). Furthermore, recursively define

\[g_{i}(\boldsymbol{\gamma})=\gamma_{i}\text{ if }\{i\}\in\mathcal{K},\text{ and otherwise }g_{i}(\boldsymbol{\gamma})=\gamma_{i}\prod_{i^{\prime}\in S_{i} \setminus\{i^{\prime}\}}g_{i^{\prime}}(\boldsymbol{\gamma}).\]

**Lemma E.14**.: _Under the above notation, we have that \(q_{k,i}(\zeta,\boldsymbol{\phi},\boldsymbol{m},\boldsymbol{\gamma})\) has no nonzero terms of degree less than \(o_{i}\) in \(\zeta\). Furthermore, \(\tilde{q}_{k,i}(\boldsymbol{\phi},\boldsymbol{\rho},\boldsymbol{\gamma})\) can be decomposed as \(\tilde{q}_{k,i}(\boldsymbol{\phi},\boldsymbol{\rho},\boldsymbol{\gamma})=g_{ i}(\boldsymbol{\gamma})\cdot\hat{q}_{k,i}(\boldsymbol{\phi},\boldsymbol{\rho})\) for some \(\hat{q}_{k,i}(\boldsymbol{\phi},\boldsymbol{\rho})\)._

Proof.: We will prove this by induction on \(k\). The base case for \(k=0\) trivially holds because \(q_{0,i}\equiv 0\). For the inductive step, we assume the statement holds for all \(k^{\prime}\in\{0,\ldots,k\}\) and we prove the statement for \(k+1\). By the recurrence relation of the polynomials \(q_{k,i}\), we have

\[q_{k+1,i}(\zeta,\boldsymbol{\phi},\boldsymbol{\rho},\boldsymbol{ \gamma})=q_{k,i}(\zeta,\boldsymbol{\phi},\boldsymbol{\rho},\boldsymbol{\gamma} )+\gamma_{i}\zeta\rho_{1}\phi_{\{i\}}\mathbbm{1}\{\{i\}\in\mathcal{K}\}\] \[+\gamma_{i}\zeta\sum_{r=1}^{L-1}\frac{\rho_{r+1}}{r!}\sum_{(i_{1},\ldots,i_{r})\in[P]^{r}}\phi_{\{i\}\oplus\{i_{1}\}\oplus\cdots\oplus\{i_{r}\} }\mathbbm{1}(\{i\}\oplus\{i_{1}\}\oplus\cdots\oplus\{i_{r}\}\in\mathcal{K}) \prod_{l=1}^{r}q_{k,i_{l}}(\zeta,\boldsymbol{\phi},\boldsymbol{\rho}, \boldsymbol{\gamma}).\]

It suffices to show that the claim holds for each one of the three terms above.

1. The first term, \(q_{k,i}(\zeta,\boldsymbol{\phi},\boldsymbol{\rho},\boldsymbol{\gamma})\), is handled by the inductive hypothesis. We can conclude that all the terms of degree less than \(o_{i}\) in \(\zeta\) are zero. Moreover, \(\tilde{q}_{k,i}=g_{i}(\boldsymbol{\gamma})\hat{q}_{k,i}(\boldsymbol{\rho}, \boldsymbol{\gamma})\).
2. The second term is nonzero only in the case that \(\{i\}\in\mathcal{K}\), in which case \(S_{i}=\{i\}\in\mathbb{K}\) and \(o_{i}=1\), so we do not have a contradiction since the \(\zeta^{o_{i}}=\zeta\) only and we do not have terms with degree less than \(o_{i}\). Moreover, the coefficient of \(\zeta\) is \(\gamma_{i}\rho_{1}\phi_{\{i\}}=g_{i}(\boldsymbol{\gamma})\rho_{1}\phi_{\{i\}}\).
3. We break into cases. _Case \(a\)_. If \(\{i\}\oplus\{i_{1}\}\oplus\cdots\oplus\{i_{r}\}=S_{i}\), then \(S_{i}\setminus\{i\}\subset\{i_{1},\ldots,i_{r}\}\), so \(1+\sum_{l=1}^{r}o_{i_{l}}\geq o_{i}\), and so no new terms of degree less than \(o_{i}\) are added. Moreover, the only way the third term has degree \(o_{i}\) in \(\zeta\) is when \(i\neq i_{1}\neq\ldots\neq i_{r}\) while still having \(\{i,i_{1},\ldots,i_{r}\}=S_{i}\); otherwise the power of \(\zeta\) is \(1+\sum_{l=1}^{r}o_{i_{l}}>o_{i}\). In that case, letting \(r^{\prime}=|S_{i}|-1\), the coefficient of \(\zeta^{o_{i}}\) in the third term is \[\gamma_{i}\left(\frac{\rho_{r^{\prime}+1}}{r^{\prime}!}\phi_{S_{i}} \prod_{i^{\prime}\in S_{i}\setminus\{i\}}^{r}\tilde{q}_{k,i^{\prime}}( \boldsymbol{\phi},\boldsymbol{\rho},\boldsymbol{\gamma})\sum_{(i_{1},\ldots,i_{r^ {\prime}})\in[P]^{r^{\prime}}}\mathbbm{1}(\{i,i_{1},\ldots,i_{r^{\prime}}\}=S_{ i})\right)\] \[=\gamma_{i}\left(\frac{\rho_{r^{\prime}+1}}{r^{\prime}!}\phi_{S_{i} }\prod_{i^{\prime}\in S_{i}\setminus\{i\}}g_{i^{\prime}}(\boldsymbol{\gamma}) \hat{q}_{k,i^{\prime}}(\boldsymbol{\rho},\boldsymbol{\gamma})\sum_{(i_{1}, \ldots,i_{r^{\prime}})\in[P]^{r^{\prime}}}\mathbbm{1}(\{i,i_{1},\ldots,i_{r^{ \prime}}\}=S_{i})\right)\] \[=\gamma_{i}\prod_{i^{\prime}\in S_{i}\setminus\{i^{\prime}\}}g_{ i^{\prime}}(\boldsymbol{\gamma})\left(\frac{\rho_{r^{\prime}+1}}{r^{\prime}!}\phi_{S_{i}} \prod_{i^{\prime}\in S_{i}\setminus\{i\}}\hat{q}_{k,i^{\prime}}(\boldsymbol{ \rho},\boldsymbol{\gamma})\sum_{(i_{1},\ldots,i_{r^{\prime}})\in[P]^{r^{\prime}}} \mathbbm{1}(\{i,i_{1},\ldots,i_{r^{\prime}}\}=S_{i})\right)\] \[=g_{i}(\boldsymbol{\gamma})\cdot\hat{q}(\boldsymbol{\rho}, \boldsymbol{\gamma})\text{ for a certain }\hat{q}.\]_Case \(b\)._ If \(\{i\}\oplus\{i_{1}\}\oplus\cdots\oplus\{i_{r}\}=S_{i^{\prime}}\) for some \(i^{\prime}\neq i,\) then either \(i\in\{i_{1},\ldots,i_{r}\}\), in which case \(1+\sum_{l=1}^{r}o_{i_{l}}>o_{i}\). Otherwise, we must have \(i^{\prime}>i\). But in this case \(o_{i^{\prime}}>o_{i}\) since \(i\in S_{i^{\prime}}\), so we also have \(\sum_{l=1}^{r}o_{i_{l}}>o_{i}\) and again no new terms of degree less than \(o_{i}\) are added. In fact, only terms of degree strictly more than \(o_{i}\) are added. Thus, in either case the coefficient of the term with degree \(o_{i}\) in \(\zeta\) is simply \(0\).

Finally, we showed that each of the three terms have at least the degree \(o_{i}\) in \(\zeta\). Moreover, each term can be decomposed as \(g_{i}(\bm{\gamma})\hat{q}(\bm{\rho},\bm{\gamma})\) for a certain \(\hat{q}\). Thus, overall we can decompose

\[\tilde{q}_{k,i}(\bm{\phi},\bm{\rho},\bm{\gamma})=g_{i}(\bm{\gamma})\cdot\hat{ q}_{k+1,i}(\bm{\phi},\bm{\rho}),\text{concluding the proof of the lemma.}\]

**Claim 4**.: _Let_

\[r_{\bm{z}}(\zeta,\bm{\phi},\bm{\rho},\bm{\gamma})=\sum_{i}z_{i}q_{k_{1},i}( \zeta,\bm{\phi},\bm{\rho},\bm{\gamma}).\]

_Then, for each distinct pair \(\bm{z},\bm{z}^{\prime}\in\{+1,-1\}^{P}\), we have \(r_{\bm{z}}(\zeta,\bm{\phi},\bm{m})-r_{\bm{z}^{\prime}}(\zeta,\bm{\phi},\bm{m}) \not\equiv 0\) as a polynomial in \(\zeta\) and \(\bm{\gamma}\)._

Proof.: Recall the definition of \(o_{i}\) from Lemma E.14. Fix \(i\in[P]\) be such that \(z_{i}\neq z_{i}^{\prime}\) and \(o_{i}\) is minimized. It is always possible to choose such an \(i\) since \(\bm{z}\neq\bm{z}^{\prime}\). Again \(\tilde{r}_{\bm{z}}(\bm{\phi},\bm{\rho},\bm{\gamma})\) is the term with degree \(o_{i}\) in \(\zeta\). Using Lemma E.14 then

\[\tilde{r}_{\bm{z}}(\bm{\phi},\bm{\rho},\bm{\gamma})-\tilde{r}_{\bm{z}^{\prime }}(\bm{\phi},\bm{\rho},\bm{\gamma})=\sum_{i^{\prime}\text{ s.t. }o_{i^{\prime}}=o_{i},z_{i^{\prime}}\neq z_{i^{\prime}}^{ \prime}}(z_{i^{\prime}}-z_{i^{\prime}}^{\prime})\tilde{q}_{k_{1},i^{\prime}}( \bm{\phi},\bm{\rho},\bm{\gamma}),\]

but \(\tilde{q}_{k_{1},i^{\prime}}\) are non-zero polynomials in \(\bm{\gamma}\) with different dependence that is captured by \(g_{i^{\prime}}(\bm{\gamma})\) according to Lemma E.14. Thus, \(r_{\bm{z}}(\zeta,\bm{\phi},\bm{m})-r_{\bm{z}^{\prime}}(\zeta,\bm{\phi},\bm{m}) \not\equiv 0\). 

#### e.5.3 Linear Independence of Powers of Polynomials

Similar to Abbe et al. (2022), we finish our proof using the classical result of Newman and Slater (1979) about the linear independence of large powers of polynomials.

**Proposition E.15** (Remark 5.2 in Newman and Slater (1979)).: _Let \(R_{1},\ldots,R_{m}\in\mathbb{C}[\zeta]\) be non-constant polynomials such that for all \(i\neq i^{\prime}\in[m]\) we have \(R_{i}(\zeta)\) is not a constant multiple of \(R_{i^{\prime}}(\zeta)\). Then for \(L\geq 8m^{2}\) we have that \((R_{1})^{L},\ldots,(R_{m})^{L}\in\mathbb{C}[\zeta]\) are \(\mathbb{C}\)-linearly independent._

We will finally show that \(\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\gamma}))\not\equiv 0\).

Proof of Lemma e.13.: Let us fix \(\bm{\kappa}\in\mathbb{R}^{P}\) such that for all \(\bm{z}\neq\bm{z}^{\prime}\), we have \(r_{\bm{z}}(\zeta,\bm{\alpha},\bm{m},\bm{\kappa})-r_{\bm{z}^{\prime}}(\zeta,\bm {\alpha},\bm{m},\bm{\kappa})\not\equiv 0\) as polynomials in \(\zeta\). This can be ensured by drawing \(\bm{\kappa}\sim\operatorname{Unif}([0.5,1.5]^{P})\), since for all \(\bm{z}\neq\bm{z}^{\prime}\) we have \(r_{\bm{z}}(\zeta,\bm{\alpha},\bm{m},\bm{\gamma})-r_{\bm{z}^{\prime}}(\zeta, \bm{\alpha},\bm{m},\bm{\gamma})\not\equiv 0\) as polynomials in \(\zeta,\bm{\gamma}\) by Claim 4. We now rewrite \(\tilde{r}_{\bm{z}}(\zeta)=r_{\bm{z}}(\zeta,\bm{\alpha},\bm{m},\bm{\kappa})\) to further indicate that the variables \(\bm{\phi}=\bm{\alpha},\bm{\rho}=\bm{m}\) and \(\bm{\gamma}=\bm{\kappa}\) are instantiated, and that we are looking at a polynomial over \(\zeta\). We have also chosen \(m_{i}=i!\binom{L}{i}\) for all \(i\in\{0,\ldots,L\}\). We then have

\[M_{\bm{z},j}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\kappa})=(1+\tilde{r}_{\bm{z}}( \zeta_{j}))^{L}.\]

Also, observe from the recurrence relations \(\zeta\) divides \(q_{k_{1},i}(\zeta,\bm{\alpha},\bm{m},\bm{\kappa})\) for each \(i\in[P]\), so \(\zeta_{j}\) divides \(\tilde{r}_{\bm{z}}(\zeta_{j})=\sum_{i=1}^{P}z_{i}q_{k_{1},i}(\zeta_{j},\bm{ \alpha},\bm{m},\bm{\kappa})\). Therefore, polynomials \((1+\tilde{r}_{\bm{z}}(\zeta_{j}))\) and \(,(1+\tilde{r}_{\bm{z}^{\prime}}(\zeta_{j}))\) are not constant multiples of each other for any \(\bm{z}\neq\bm{z}^{\prime}\).

Now see \(\zeta\) as variables, and construct the Wronskian matrix over the \(L\)-th power polynomials \(\{(1+\tilde{r}_{\bm{z}}(\zeta))^{L}\}_{\bm{z}\in\{+1,-1\}^{P}}\). This is a \(2^{P}\times 2^{P}\) matrix \(\bm{H}(\zeta)\) whose entries are indexed by \(\bm{z}\) and \(l\in[2^{P}]\) and defined by:

\[H_{\bm{z},l}(\zeta)=\frac{\partial^{l-1}}{\partial\zeta^{l-1}}(1+\tilde{r}_{\bm{z }}(\zeta))^{L}.\]Applying Proposition E.15 implies the polynomials \(\{(1+\tilde{r}_{\bm{z}}(\zeta))^{L}\}_{\bm{z}\in\{+1,-1\}^{P}}\) are linearly-independent, and so the Wronskian determinant is nonzero as a polynomial in \(\zeta\):

\[\det(\bm{H}(\zeta))\not\equiv 0.\]

Also, observe that we can write \(\det(\bm{H}(\zeta))=\frac{\partial}{\partial\zeta_{2}}\frac{\partial^{2}}{ \partial\zeta_{3}^{2}}\cdots\frac{\partial^{2^{P-1}}}{\partial\zeta_{2^{P}}^{2 \prime P-1}}\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\kappa}))\mid_{\zeta= \zeta_{1}=\cdots=\zeta_{2^{P}}}\). This finally gives us \(\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\kappa}))\not\equiv 0\) as a polynomial in \(\bm{\zeta}\), and thus, \(\det(\bm{M}(\bm{\zeta},\bm{\alpha},\bm{m},\bm{\gamma}))\not\equiv 0\) as a polynomial in \(\bm{\zeta}\) and \(\bm{\gamma}\).

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and precede the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist"**,
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Yes, it does. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We do specify the limitations of our work.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Yes, we do provide all the assumptions and correct proofs. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We do provide the code and the experiments are fully reproducible. Guidelines: * The answer NA means that the paper does not include experiments.

* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We attach the code in the supplementary material. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). ** Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We specify our training algorithm and related details. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We do report the error bars whenever necessary. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: All experiments are relatively small scale and can be reproduced under 1 hr of compute on a standard computer with 16 GB RAM. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.

* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: It does. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: This is a theoretical work with no societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper does not have models or data with safety concerns. Guidelines: * The answer NA means that the paper poses no such risks.

* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use any existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new asset. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This does not require crowd sourcing nor research with human subject.

Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our study does not require IRB approval. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.