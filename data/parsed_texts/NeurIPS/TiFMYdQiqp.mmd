# Bayesian target optimisation for high-precision holographic optogenetics

Marcus A. Triplett\({}^{1,2,\dagger}\) Marta Gajowa\({}^{3}\) Hillel Adesnik\({}^{3}\) Liam Paninski\({}^{1,2}\)

\({}^{1}\)Department of Statistics, Columbia University

\({}^{2}\)Zuckerman Mind Brain Behavior Institute, Columbia University

\({}^{3}\)Department of Molecular and Cell Biology, UC Berkeley

\({}^{\dagger}\)marcus.triplett@columbia.edu

###### Abstract

Two-photon optogenetics has transformed our ability to probe the structure and function of neural circuits. However, achieving precise optogenetic control of neural ensemble activity has remained fundamentally constrained by the problem of off-target stimulation (OTS): the inadvertent activation of nearby non-target neurons due to imperfect confinement of light onto target neurons. Here we propose a novel computational approach to this problem called Bayesian target optimisation. Our approach uses nonparametric Bayesian inference to model neural responses to optogenetic stimulation, and then optimises the laser powers and optical target locations needed to achieve a desired activity pattern with minimal OTS. We validate our approach in simulations and using data from _in vitro_ experiments, showing that Bayesian target optimisation considerably reduces OTS across all conditions we test. Together, these results establish our ability to overcome OTS, enabling optogenetic stimulation with substantially improved precision.

## 1 Introduction

A key technological goal in neuroscience is to gain precise control over the activity of neurons. Currently, one of the most promising approaches for achieving such control is two-photon optogenetics [1, 2, 3, 4]. This technique relies on the use of two-photon excitation to activate opsin molecules expressed in the somatic membrane, allowing individual neurons to be targeted for stimulation. Holographic optogenetics further extends this technique by focusing two-photon excitation into \(\sim\)10 \(\mu\)m disks of light that illuminate all of a neuron's opsin molecules in parallel [5, 6, 7, 8, 9]. Replicating this illumination pattern across many neurons at once then grants simultaneous optogenetic control of entire neural ensembles [10, 11, 12]. However, the spatial precision of holographic optogenetics has remained fundamentally limited by the problem of off-target stimulation (OTS): if multiple opsin-expressing neurons lie in close proximity (in the most extreme case, if their membranes are in physical contact), two-photon excitation will frequently activate opsin molecules in each neuron when attempting to stimulate only one. This can inadvertently elicit spikes in non-target neurons, compromising the precision and specificity of the optogenetic manipulation [13].

Previous attempts at overcoming OTS have been either optical or molecular in nature (e.g. by using temporal focusing [2, 6, 7, 14] or improving the soma-targeting of the opsin [15, 16, 10]), but have not yet achieved "true" single-cell precision under many experimental conditions. Frequently, experimenters will simply express opsin sparsely in a population to avoid OTS, as this reduces the number of nearby opsin-expressing neurons that could be mistakenly activated [17]. However, sparse opsin expression reduces the number of neurons that can potentially be probed during anexperiment, and therefore limits the kinds of scientific questions that can be addressed using two-photon optogenetics. Conversely, in an ideal experiment, one could precisely stimulate any neuron despite a moderate or high density of opsin-expressing cells.

Here, we develop a novel computational strategy for all-optical experiments [18; 19; 20; 21] that reduces (and in some cases entirely eliminates) OTS (Figure 1a, b). The key insight is that heterogeneity in opsin expression and intrinsic excitability causes neurons to have different sensitivities to optical stimulation. This implies that in some cases laser power could be safely turned down to prevent the spiking of a nearby non-target neuron that is weakly driven by stimulation while still spiking a target neuron that is strongly driven by stimulation. Further, the spatial arrangement of neurons can be exploited by moving a holographic target off of a neuron's nucleus (the typical stimulation point) towards a less dense area (e.g. towards neuropil or less photosensitive neurons) while still exciting a sufficient number of the target neuron's opsin molecules to elicit a spike. While some prior experimental research has considered cell-specific tuning of laser powers [22], our advance is to identify both the optimal laser powers and target locations automatically using efficient Bayesian techniques.

To achieve highly precise optogenetic ensemble control, we propose a two-phase process called Bayesian target optimisation (Bataro). In the first phase, we map the "optogenetic receptive field" (ORF) of each neuron by stimulating at various locations around cell nuclei and at a range of laser powers to test whether they elicit spikes. To infer these ORFs without exhaustively testing every combination of location and power, we use Gaussian processes (GPs) to encode prior knowledge about how neurons respond to optogenetic stimulation. Further, we propose to use holographic stimulation to test multiple locations around one or many neurons simultaneously, ensuring that the mapping phase completes quickly. This also allows us to model how neurons integrate two-photon excitation from multiple nearby holograms. In the second phase, we then exploit knowledge of the ORFs to optimise holographic targets. We use properties of the GP to infer gradients of an objective function at a spatial resolution that exceeds the original sampling resolution, enabling us to optimise for the exact holographic targets and laser powers needed to evoke a specific neural activity pattern.

## 2 Related work

Our work is related to prior research in three areas: GP-based receptive field inference, optimal stimulus design, and statistical methods for estimating connectivity using optogenetic stimulation. Since ref. [23], many studies have used GPs to infer the relationship between neural spiking and multi-dimensional covariates. This includes using GPs to infer place fields [24; 25] and orientation preference maps [26], as well as with Bayesian active learning techniques for inferring sensory receptive fields [27; 28]. However, none to our knowledge have used GP inference to model responses to optogenetic stimulation. Closed-loop methods for optimal control of neural activity using one-photon or electrical stimulation have been recently explored [29; 30; 31; 32; 33; 34], though these approaches lack the necessary spatial specificity for highly precise ensemble control. Ref. [35] considers optimising spike-timing using two-photon stimulation (in addition to electrical stimulation), but focused on single neurons and did not use GP estimation methods. Finally, a number of papers have developed statistical models of optogenetic data to infer functional or synaptic connectivity [36; 37; 38; 39; 40; 41; 42; 43], but do not consider identifying the exact stimulation parameters needed to evoke specific activity patterns. Here, we unify these three approaches to develop a novel computational framework for optimal holographic stimulation of neural ensembles.

## 3 Methods

### Optogenetic receptive field model

Given a population of \(N\) neurons, we first wish to learn ORF functions \(g_{n}\) that model how each neuron responds to optogenetic stimulation at different laser powers and locations near the cell soma. Ideally, ORF mapping should be completed quickly, stimulating as few times as possible, so that stimulus optimisation and the desired experiment can begin. Therefore, an ORF model should leverage prior knowledge of how neurons respond to stimulation. Importantly, spike probability should increase monotonically with laser power until saturation [42], and the ORF should approximately match the shape of the somatic membrane (though enlarged to account for the \(\sim\)10 \(\mu\)m diameter of the holographic disk [22]). However, any residual expression of opsin molecules in the proximal dendrites of a neuron creates unique differences in its ORF shape compared to other neurons (Figure 1c). Further, neurons can express opsin in variable concentrations and vary in their intrinsic excitability, leading to an unpredictable dependence of spiking on power. Thus, we need an approach to modelling an ORF that roughly describes the typical shape and photosensitivity of a cell, but that can flexibly adapt to variation across neurons.

To simultaneously account for these factors, we use a novel variant of the GP-Bernoulli model. Let \(y_{nt}\in\{0,1\}\) denote the response of neuron \(n\) on trial \(t\) to a holographic stimulus \(\mathbf{x}_{t}\in\mathbb{R}^{J\times 3}\). As our approach is designed for all-optical experiments, the response to stimulation is assumed to be observed for all neurons at once using calcium or voltage imaging. Each stimulus delivers two-photon excitation to \(J\geq 1\) different target locations, with \(\mathbf{x}_{t}^{j}=(c_{1t}^{j},c_{2t}^{j},I_{t}^{j})\in\mathbb{R}^{3}\) representing the two-dimensional coordinates and laser power of the \(j\)th target (though note that we can also handle three-dimensional hologram coordinates without any substantial changes to the model). While in some cases the targets will be far enough apart in space to not interact with each other, frequently a neuron will integrate two-photon excitation from multiple nearby targets, increasing the risk of OTS. To account for this, we model the probability of evoking a spike in neuron \(n\) by summing across all \(J\) points on that neuron's ORF,

\[y_{nt}\sim\text{Bernoulli}(\sigma(\gamma_{n}(\mathbf{x}_{t})-\theta_{n})), \quad\gamma_{n}(\mathbf{x}_{t})=\sum_{j=1}^{J}g_{n}(\mathbf{x}_{t}^{j}),\] (1)

where \(\sigma\) is the logistic sigmoid function and \(\theta_{n}\in\mathbb{R}\) is a scalar threshold. We model each ORF using a three-dimensional GP prior that describes the effect of stimulating at a given location and power: \(g_{n}\sim\mathcal{GP}(m_{n}(\cdot),k(\cdot,\cdot))\), where \(m_{n}\) and \(k\) are respectively the mean and covariance functions of the GP. During inference (as discussed below) we constrain \(g_{n}\) to be non-negative, which ensures that stimulation cannot have an inhibitory effect. Further, using a non-negativity constraint rather than (e.g.) applying a rectifying nonlinearity to \(g_{n}\) guarantees that the posterior distribution remains log-concave, facilitating tractable identification of the global optimum.

To encode prior knowledge about optogenetic stimulation, the mean function should have the property that the probability of evoking a spike increases with laser power but decays quickly as the hologram

Figure 1: Off-target stimulation with two-photon optogenetics. (a) Two-photon holographic optogenetics can be used to elicit spikes in specific ensembles of experimenter-selected neurons. (b) OTS arises due to an inability to confine two-photon excitation to the soma of a target neuron. By repositioning the hologram away from the soma, OTS could be avoided while still activating the target neuron. (c) Data from real two-photon optogenetics experiment showing that at high power (e.g. 40 mW) a neuron can be activated from 15-20 \(\mu\)m away, though this depends on the specific pattern of opsin expression at the soma and proximal dendrites. Red and gray circles indicate locations where stimulation resulted in successful or unsuccessful spikes. Colour map shows the inferred probability of spiking (i.e., the ORF) using the log-barrier Newton method from Subsection 3.2. As the number of sampled locations and laser powers increases, the GP model adapts to the particular ORF shape (see supplementary material for the prior mean). This shape can then be exploited to precisely optimise holographic stimuli. Data from PV-neuron in L2/3 of V1 expressing the soma-targeted, excitatory opsin ChroME2\(\ell\)[44].

is positioned further away from the nucleus. To this end, we set

\[m_{n}(\mathbf{x})=\rho I\exp(-\|\mathbf{c}-L_{n}\|^{2}/\sigma_{m}^{2})\] (2)

for \(n=1,\ldots,N\) (note that we suppress reference to the target dimension \(j\) for notational clarity here and in the definition of the covariance function below). Here \(\mathbf{c}=(c_{1},c_{2})\) denotes the coordinates of the holographic target, \(L_{n}\) is the location of neuron \(n\), and \(\rho\) can be loosely interpreted as the average opsin expression level, determining how laser power affects the probability of spiking. While we treat \(\rho\) and \(\sigma_{m}^{2}\) as static hyperparameters, in future work one could consider a hierarchical model where the mean function is also learned, with its accuracy improving as additional ORFs are probed.

We model the covariance between points on the ORF using the radial basis function (RBF) kernel \(k\), which regularises the ORF by encouraging it to vary smoothly through space and power. However, we note that other covariance kernels could be used here provided that they are differentiable with respect to \(\mathbf{x}\). We define

\[k(\mathbf{x},\mathbf{x}^{\prime})=\alpha^{2}\exp\left(-\frac{1}{2}(\mathbf{x} -\mathbf{x}^{\prime})^{\top}\boldsymbol{\Lambda}(\mathbf{x}-\mathbf{x}^{ \prime})\right)+\sigma_{d}^{2}\delta_{\mathbf{x},\mathbf{x}^{\prime}},\] (3)

where \(\boldsymbol{\Lambda}=\text{diag}(\lambda_{s},\lambda_{s},\lambda_{I})\) is a diagonal matrix with each diagonal term giving the characteristic lengthscale for the corresponding GP dimension. We assume that the lengthscales \(\lambda_{s}\) in the two spatial dimensions are equal (a typical assumption in holography, see e.g. ref. [6]), but take a different lengthscale \(\lambda_{I}\) for the laser power dimension. Finally, we denote the hyperparameters of the ORF prior by \(\phi=(\alpha^{2},\sigma_{d}^{2},\lambda_{s},\lambda_{I},\rho,\sigma_{m}^{2})\).

### Inference

During an ORF mapping experiment we first collect calibration data by performing stimulation in a series of trials \(\mathbf{x}_{1},\ldots,\mathbf{x}_{T}\), where each \(\mathbf{x}_{t}\) targets \(J\) different locations and laser powers near a random set of neurons. Then we combine these stimuli with the corresponding observations of any evoked activity \(\mathbf{y}_{:,1},\ldots,\mathbf{y}_{:,T}\) to estimate the ORFs. Here each \(\mathbf{y}_{:,t}\in\{0,1\}^{N}\) describes the population response to the stimulus \(\mathbf{x}_{t}\). Due to the non-negativity constraints on \(g_{n}\), the posterior \(p(g_{n}\mid\mathbf{y}_{n},\{\mathbf{x}_{t}\}_{t=1}^{T},\theta_{n},\phi)\) is not itself a GP, and hence we perform maximum _a posteriori_ (MAP) inference rather than attempting to obtain a complete description of posterior uncertainty (though see the supplementary material for a full treatment of posterior uncertainty in the \(J=1\) case). To this end, the MAP estimates are obtained by maximising the log-posterior, which, via Bayes rule, is equivalent to computing

\[\hat{g}_{n},\hat{\theta}_{n}= \operatorname*{argmax}_{g_{n},\theta_{n}}\left\{\sum_{t=1}^{T} \ln p(y_{nt}\mid\mathbf{x}_{t},g_{n},\theta_{n})+\ln p(g_{n}(\mathbf{x}_{1}), \ldots,g_{n}(\mathbf{x}_{T})\mid\phi)\right\}\] (4) \[\text{such that }g_{n}(\mathbf{x}_{t})\geq 0\text{ for }t=1, \ldots,T.\]

Note that we have abbreviated \(\hat{g}_{n}(\mathbf{X})\) (the MAP version of the ORF) as \(\hat{g}_{n}\). To solve Equation 4 we alternate between using Newton's method with a log-barrier to update \(g_{n}\) (such that the optimised ORFs respect the non-negativity constraints), and performing gradient descent steps for updating \(\theta_{n}\). We also adaptively set the Newton step-size using a standard backtracking line-search method.

### Optimisation approach

Given the estimated ORFs, we then aim to identify holographic stimuli that evoke specific neural activity patterns as accurately as possible (i.e., minimising off-target activation). To do this, we introduce a novel optimisation approach that exploits properties of the GP to compute gradients of an error function between a target activity pattern and the (predicted) evoked activity. To this end, let \(\mathcal{G}=\{\hat{g}_{n},\hat{\theta}_{n}\}_{n=1}^{N}\) be the MAP-estimated ORFs, and define the predicted evoked activity as

\[\hat{y}(\mathbf{x},\mathcal{G})=(\sigma(\hat{\gamma}_{1}(\mathbf{x})-\hat{ \theta}_{1}),\ldots,\sigma(\hat{\gamma}_{N}(\mathbf{x})-\hat{\theta}_{N}))\in \mathbb{R}^{N},\] (5)

where \(\hat{\gamma}_{n}(\mathbf{x})=\sum_{j=1}^{J}\hat{g}_{n}(\mathbf{x}^{j})\). Let \(\boldsymbol{\Omega}\in\{0,1\}^{N}\) denote a target activity pattern. The optimal holographic stimulus is then

\[\mathbf{x}_{\text{optimal}}=\operatorname*{argmin}_{\mathbf{x}}\|\boldsymbol{ \Omega}-\hat{y}(\mathbf{x},\mathcal{G})\|^{2}\;\;\text{such that}\;\;0\leq I\leq I_{ \text{max}}.\] (6)Note that we include \(I_{\text{max}}\) as an upper bound on the laser power, which can be set to match the power practically deliverable by the microscopy system or to prevent tissue damage due to excess heating.

We propose to solve Equation 6 using a projected gradient descent algorithm. While for the ORF mapping phase we only need to compute the ORFs \(\hat{g}_{n}\) at a small set of stimulation points \(\mathbf{x}_{t}\), for the optimisation phase we must evaluate the gradients of Equation 6 (and therefore of the ORFs) at a series of new, unobserved points constituting an optimisation path. However, it is not immediately obvious how to evaluate these gradients for a general nonparametric function \(g_{n}\). The key idea for our approach is that at each step, gradient descent will provide an updated estimate \(\mathbf{x}^{*}\) of the optimal stimulus, which will act as a "test point" commonly used in GP regression. We then leverage properties of the GP to perform _inference_ of the ORF gradients at \(\mathbf{x}^{*}\), and subsequently perform a gradient descent update that reduces the value of the objective function in Equation 6.

Inferring the ORF gradients relies on the fact that a GP and its derivative are jointly GP-distributed, and hence the derivative can be inferred just from a small number of observations of the GP itself. To this end, note that the covariance between a GP and its derivative is given by [45, Sec 9.4]

\[\text{cov}\left(g_{n}(\mathbf{x}_{t}),\frac{\partial}{\partial x_{d}^{*}}g_{n }(\mathbf{x}^{*})\right)=\frac{\partial k(\mathbf{x}_{t},\mathbf{x}^{*})}{ \partial x_{d}^{*}},\] (7)

where \(d\) is any dimension of \(\mathbf{x}^{*}\) and where we have suppressed reference to the hologram target \(j\) for notational clarity here and in the following equation. The expression in Equation 7 can be used to obtain a predictive distribution over derivative functions consistent with the observed data points, which we use to evaluate the derivatives of \(\hat{g}_{n}\) at the test point \(\mathbf{x}^{*}\) via

\[\frac{\partial\hat{g}_{n}(\mathbf{x}^{*})}{\partial x_{d}^{*}}=\frac{\partial m _{n}(\mathbf{x}^{*})}{\partial x_{d}^{*}}+\text{cov}\left(g_{n}(\boldsymbol{ \mathcal{X}}),\frac{\partial g_{n}(\mathbf{x}^{*})}{\partial x_{d}^{*}}\right) ^{\top}\mathbf{K}^{-1}(\hat{g}_{n}(\boldsymbol{\mathcal{X}})-m_{n}( \boldsymbol{\mathcal{X}})),\] (8)

where \(\boldsymbol{\mathcal{X}}\in\mathbb{R}^{JT\times 3}\) is the collection of \(JT\) unique points on the ORF that were probed during ORF mapping, \(g_{n}(\boldsymbol{\mathcal{X}}),m_{n}(\boldsymbol{\mathcal{X}})\in\mathbb{R}^ {JT}\) give the value of the GP and mean function evaluated at each such point, and where \(\mathbf{K}\) is the corresponding GP covariance matrix obtained by evaluating the covariance kernel \(k\) at every pair of elements of \(\boldsymbol{\mathcal{X}}\). Further details are provided in the supplementary material.

Equation 8 allows us to define a closed-form gradient of the objective function, which we use in a projected gradient descent algorithm (Algorithm 1). Note, however, that the objective function is not convex in \(\mathbf{x}\), and therefore such updates are only guaranteed to converge to a locally optimal solution. We therefore typically run Algorithm 1 with several random initialisations and select the optimised target with the lowest predicted error.

```
1Compute MAP estimates of ORFs \(\{\hat{g}_{n},\hat{\theta}_{n}\}_{n=1}^{N}\) from calibration data \(\{\mathbf{y}_{n}\}_{n=1}^{N},\{\mathbf{x}_{t}\}_{t=1}^{T}\) using Newton's method with log-barrier.
2Initialise targets \(\mathbf{x}\in\mathbb{R}^{J\times 3}\) to random locations near the somas of the \(J\) target neurons and with random laser powers.
3whiletarget not convergeddo
4Construct gradient vectors \(\nabla_{\mathbf{x}}\hat{\gamma}_{n}(\mathbf{x})\) for \(n=1,\ldots,N\) using inference of ORF derivatives (Equation 8).
5Set \(\boldsymbol{\delta}_{\mathbf{x}}=-2\sum_{n=1}^{N}(\Omega_{n}-\sigma(\hat{ \gamma}_{n}(\mathbf{x})-\hat{\theta}_{n}))\sigma^{\prime}(\hat{\gamma}_{n}( \mathbf{x})-\hat{\theta}_{n})\nabla_{\mathbf{x}}\hat{\gamma}_{n}(\mathbf{x})\).
6Perform gradient descent update \(\mathbf{x}\leftarrow\mathbf{x}+\beta\boldsymbol{\delta}_{\mathbf{x}}\) with step-size \(\beta\).
7Project laser power onto feasible domain, \(I_{j}\leftarrow\text{min}(I_{j},I_{\text{max}})\) for \(j=1,\ldots,J\).
8
9 end while ```

**Algorithm 1**Bayesian target optimisation (Bataro).

## 4 Results

### Simulations

We first tested Bataro by simulating an optical "write-in" experiment, where we attempted to write specific activity patterns into a hypothetical neural population. To do this, we positioned 50 neurons randomly in a 250 \(\mu\)m \(\times\) 250 \(\mu\)m field of view (FOV), sampled each neuron's ORF from its GPprior, and generated responses to ensemble stimulation. To quantify the optical write-in error, we used the sum of squared errors \(\sum_{n}(\Omega_{n}-y_{n}(\mathbf{x}))^{2}\), where \(y_{n}(\mathbf{x})\) is the ground truth probability of neuron \(n\) spiking in response to stimulus \(\mathbf{x}\). This error takes the interpretable value of \(\sim\)\(m\) if \(m\) non-target neurons are inadvertently recruited due to OTS. In this simulation, holographic stimulation of an example ensemble consisting of 6 selected neurons resulted in a substantial recruitment of activity from non-target neurons due to OTS, especially when multiple holograms converged on nearby non-target neurons (Figure 2a; optical write-in error = 7.84).

To optimally reposition the holographic targets, we probed the population's ORFs using 10-target ensemble stimulation, and then used Bataro (Algorithm 1). This successfully repositioned holographic targets an appropriate distance from the nuclei of the target neurons, such that while minimal excitation was applied to the non-target neurons, the target neurons were still driven to spike with high probability (Figure 2b,c; write-in error = 1.61). To confirm that the improvement in write-in precision was robust, we repeated the optimisation for 20 additional random 6-target ensembles and found that off-target effects were substantially reduced in every case, with the write-in error reducing by 75% on average compared to naively stimulating the nucleus of each neuron (Figure 2d).

Next, we investigated two variables known to critically constrain the ability to evoke specific neural activity patterns: (1) the density of opsin-expressing neurons in the stimulation FOV, and (2) the size of the ensemble that one is attempting to activate. Figure 3a illustrates how increasing the number of opsin-expressing neurons introduces many more potential non-target neurons that could be inadvertently activated. This causes errors induced by direct nuclear stimulation of (e.g.) 5-neuron ensembles to grow rapidly (Figure 3b, grey curve), though note that the magnitude of the error depends on many experimental factors including the size of the ORF, numerical aperture of the microscope, and the soma-targeting efficacy. By optimising holographic target locations and laser powers, the average write-in error was reduced by 76% relative to nuclear stimulation (Figure 3b, red curve). Similarly, in these simulations, nuclear stimulation resulted in an error that grew almost

Figure 2: Minimising off-target stimulation using Bayesian target optimisation. (a) Direct nuclear stimulation at 70 mW successfully activates the target neurons with high probability, but also activates nearby non-target neurons due to OTS. Triangles indicate target neurons. Shading indicates probability of spiking. Optical write-in error (bottom) given as the sum of squared errors between the evoked and desired activity patterns. (b) Optimised stimulation using Algorithm 1 repoils the hologram locations away from the nuclei of non-target neurons, resulting in a substantial reduction in off-target activation. (c) Optimisation trajectory of the 6 different target laser powers. Initial laser powers selected randomly between 50 and 70 mW. (d) Optimising holographic targets for 20 different random ensembles shows a robust reduction of optical write-in error (average reduction, 75%).

directly proportionally to the size of the stimulated ensemble, indicating that almost as many non-target neurons were activated as target neurons. However, Bataro reduced the average write-in error by 69% across ensembles of size 1-15 (Figure 3c).

### Application to holographic optogenetics experiments

To test whether Bataro could eliminate OTS in realistic settings, we created synthetic optogenetics experiments involving \(>\)100 neurons from a small number (\(n\)=4) of detailed cell-attached recordings in slice [42]. Briefly, each slice experiment was performed by first establishing a cell-attached electrophysiological recording of a single L2/3 interneuron from mouse V1 expressing the somatargeted opsin ChroME2f. Then, the recorded neuron was optogenetically stimulated at a dense grid

Figure 4: Performance of Bayesian target optimisation using simulations based on two-photon holographic optogenetics data. (a) Left: field of view from a real _in vitro_ optogenetics experiment showing every optimised single-target hologram. Opsin fused to red fluorescent protein mRuby3 so that opsin-expressing neurons can be visualised. Unfilled white circles represent putative neurons detected by automated cell segmentation method. Smaller filled white circles represent optimised holographic targets, shown in relation to the cell nucleus by a straight white line. Right: zoomed view of optimised targets, corresponding to dashed region in the left panel. (b) Optimised laser powers relative to their initialised values show how Bataro exploits differences in photosensitivity to avoid OTS. Each circle represents the power delivered to a single holographic target. (c) Reduction of optical write-in error using optimised holographic targets. Each circle represents the error when attempting to stimulate a single neuron.

Figure 3: Performance of Bayesian target optimisation in increasingly difficult contexts. (a) Two example scenarios with low (left) and high (right) density of opsin-expressing neurons. Triangles indicate example 5-target ensemble to be stimulated. At low density, the risk of OTS is low because neurons are often spaced far apart. However, at high density, OTS arising from direct nuclear stimulation at high power is unavoidable. (b) Optical write-in error for nuclear and optimised stimulation of 5-target ensembles. Error bars show the mean error \(\pm\) 1 s.d. over 10 different simulations. For each simulation we averaged the write-in error over 20 random ensembles. (c) Same as (b), but for a fixed population size of 50 and varying ensemble size.

of locations surrounding the cell and at a range of laser powers to comprehensively map its ORF (see Figure 1c, "100% sampled" column, also see supplementary material for additional examples). We used this data to create a set of four "ground truth" ORFs by fitting the GP-Bernoulli model from Equation 1, where the hyperparameters of the GP covariance kernel were selected using the cross-validated predictive log-likelihood. Next, we used a fluorescence image from a separate experiment to extract the locations of 116 putative opsin-expressing neurons, and randomly assigned each putative neuron one of four ground-truth ORFs. Together, this enabled us to simulate responses to optogenetic stimulation at arbitrary locations and laser powers and for a large number of neurons that were realistically distributed in space.

We sampled responses to optogenetic stimulation at a sparse grid of locations surrounding each neuron to map their ORFs in a hypothetical experiment (see supplementary methods for details). Then, we used Bataro to compute single-target stimuli that activated each neuron individually and minimised OTS (Figure 4a). We found that, on the one hand, a subset of neurons required no change from nuclear stimulation at high power due to being spatially isolated. However, regions with large numbers of closely-packed, opsin-expressing neurons required repositioning of the holographic targets (Figure 4a, right) and adjustment to the laser powers (Figure 4b) to eliminate OTS (average distance between nuclear and optimised stimuli, 4.9 \(\mu\)m). Across all single-target holograms, target optimisation reduced the average write-in error by 85% (Figure 4c). We also performed a control

Figure 5: Optimisation of holographic targets in three-dimensional space. (a) Example target neuron and optimised holographic stimulus (plane 3, 25 \(\mu\)m). By repositioning the hologram (primarily in the x/y dimensions), off-target activation is entirely eliminated (bottom spike probability plots, inset numbers show write-in error). Deepest plane labelled as 0 \(\mu\)m by convention. Solid white circle indicates target neuron. Dashed white circles indicate nearby non-target neurons that must be avoided. (b) Similar to (a), but for a neuron in plane 2 (50 \(\mu\)m) and with repositioning of the hologram primarily in the z dimension. (c) All optimised single-target holograms over four stimulation planes. Targets shown at their original depth for visualisation. Average displacement of optimised holographic targets relative to nuclei, 8.3 \(\mu\)m. Scale bar, 20 \(\mu\)m. (d) Max-projection over z-planes simultaneously showing locations of all segmented opsin-expressing neurons. (e) Optimised depths of holographic targets corresponding to neurons in (c). Depths are shown in comparison to one of four depths that the target neuron was segmented at during the experiment. (f) Optimised laser powers relative to their initialised values. (g) Reduction of optical write-in error for all neurons, across multiple depths.

analysis by matching the nuclear stimulation laser power to the average optimised laser power and obtained similar results (Figure S3).

Some holographic microscopy systems cannot position holograms at arbitrary continuous depths, instead either requiring holograms to be positioned in two dimensions or on a preselected number of discrete stimulation planes. Hence, thus far, we focused our efforts on the much more common case of two-dimensional optogenetics experiments. However, OTS is generally observed to be even more prevalent in three dimensions, as the optical point spread function is elongated in the "z"-axis [15; 6; 13; 17]. We therefore tested Bataro in the more general setting of three spatial dimensions and laser power (i.e., four dimensions in total), under the assumption that some microscopy systems may be able to flexibly reposition holograms continuously across depths.

We used the fact that the cell-attached recordings were repeated at multiple depths to generate synthetic optogenetics experiments with four-dimensional ORFs (Figure 5a,b). Optimisation of holographic targets successfully reduced OTS by exploiting both the x/y dimensions (Figure 5a), as well the z dimension (Figure 5b), in addition to automatically adjusting the laser power where needed. Repeating the optimisation for every single-target hologram reduced the average optical write-in error by 89% (from 0.7 to 0.08, Figure 5c-g; average distance between nuclear and optimised targets, 8.3 \(\mu\)m), demonstrating the feasibility of largely eliminating OTS in optogenetics experiments involving all three spatial dimensions.

## 5 Conclusions

We developed Bataro, a novel computational framework for two-photon optogenetics that optimises holographic stimuli to evoke neural activity patterns with minimal off-target activation. Our preliminary _in vitro_ experiments predict that specific adjustments to holographic target placements of 5-10 \(\mu\)m (on average) could substantially reduce OTS, even in the more challenging regime of three-dimensional optogenetic stimulation. Our key idea was to simultaneously exploit variability in photoexcitability, residual opsin expression in proximal dendrites (due to imperfections in opsin soma-targeting), and the specific spatial arrangement of neurons in the stimulation FOV. Optimisation using these three factors ultimately led to reductions in the optical write-in error of 85-89% for single-target holograms using data from _in vitro_ two-photon optogenetics experiments.

Whether achieving such stimulus precision is worth spending valuable experiment time mapping ORFs is, of course, highly dependent on the application. While for some experiments minimising off-target activation may not be critical, there are many cases where stimulating with as close to "true" single-cell precision as possible is essential to testing a scientific hypothesis. For example, "hub" neurons in the hippocampus are believed to orchestrate network-level function through their unilateral activity [46; 47]. Thus, using two-photon optogenetics to test whether an individual neuron represents a functional hub by stimulating that neuron alone requires true single-cell precision. In such cases, we believe devoting experiment time to ORF mapping and stimulus optimisation is a necessary trade-off.

Throughout our analysis we have assumed that ORFs are stable over time (i.e., are stationary). However, desensitisation of opsins following prolonged illumination has previously been observed [11], indicating that some nonstationarity could be encountered in real experiments. To account for this, we note that after the ORFs have been mapped and holographic stimuli optimised, it is straightforward to periodically recompute the MAP estimate of the ORF as the experiment proceeds. This would provide an updated estimate of the laser power required to maintain optimal precision. We hypothesise that this would not require repeating the entire ORF mapping phase as we do not expect the shape of the ORF to change drastically beyond desensitisation.

Standard GP regression methods scale cubically with the number of locations and laser powers probed, and therefore estimation of each neuron's ORF could encroach on experiment time if ORFs are mapped at high detail just due to required computation time. However, computational efficiency could be improved by appealing to modern GP techniques such as inducing point methods [48], or by adaptively mapping ORFs using Bayesian active learning to minimise the number of probed locations and powers [49; 50]. While in this work we have focused primarily on demonstrating the feasibility of overcoming OTS, in future work we plan to minimise both required experiment and computation time in order to deploy Bataro online.

## Acknowledgements

We thank Darcy Peterka, Benjamin Antin, Cole Hurwitz, Shizhe Chen, and Ben Shababo for helpful discussions and suggestions. This work was funded by NIH awards 1RF1MH120680 and 1U19NS107613-01 to HA and LP. MAT and LP were supported by the Gatsby Charitable Foundation and NSF NeuroNex award 1707398.

## References

* [1] John Peter Rickgauer and David W Tank. Two-photon excitation of channelrhodopsin-2 at saturation. _Proceedings of the National Academy of Sciences_, 106(35):15025-15030, 2009.
* [2] Eirini Papagiakoumou, Francesca Anselmi, Aurelien Begue, Vincent De Sars, Jesper Gluckstad, Ehud Y Isacoff, and Valentina Emiliani. Scanless two-photon excitation of channelrhodopsin-2. _Nature Methods_, 7(10):848-854, 2010.
* [3] Adam M Packer, Darcy S Peterka, Jan J Hirtz, Rohit Prakash, Karl Deisseroth, and Rafael Yuste. Two-photon optogenetics of dendritic spines and neural circuits. _Nature Methods_, 9(12):1202-1205, 2012.
* [4] Rohit Prakash, Ofer Yizhar, Benjamin Grewe, Charu Ramakrishnan, Nancy Wang, Inbal Goshen, Adam M Packer, Darcy S Peterka, Rafael Yuste, Mark J Schnitzer, et al. Two-photon optogenetic toolbox for fast inhibition, excitation and bistable modulation. _Nature Methods_, 9(12):1171-1179, 2012.
* [5] Oscar Hernandez, Eirini Papagiakoumou, Dimitri Tanese, Kevin Fidelin, Claire Wyart, and Valentina Emiliani. Three-dimensional spatiotemporal focusing of holographic patterns. _Nature Communications_, 7(1):1-11, 2016.
* [6] Nicolas C Pegard, Alan R Mardinly, Ian Anton Oldenburg, Savitha Sridharan, Laura Waller, and Hillel Adesnik. Three-dimensional scanless holographic optogenetics with temporal focusing (3d-shot). _Nature Communications_, 8(1):1-14, 2017.
* [7] Emiliano Ronzitti, Cathie Ventalon, Marco Canepari, Benoit C Forget, Eirini Papagiakoumou, and Valentina Emiliani. Recent advances in patterned photostimulation for optogenetics. _Journal of Optics_, 19(11):113001, 2017.
* [8] Nicolo Accanto, Clement Molinier, Dimitri Tanese, Emiliano Ronzitti, Zachary L Newman, Claire Wyart, Ehud Isacoff, Eirini Papagiakoumou, and Valentina Emiliani. Multiplexed temporally focused light shaping for high-resolution multi-cell targeting. _Optica_, 5(11):1478-1491, 2018.
* [9] Weijian Yang, Luis Carrillo-Reid, Yuki Bando, Darcy S Peterka, and Rafael Yuste. Simultaneous two-photon imaging and two-photon optogenetics of cortical circuits in three dimensions. _elife_, 7:e32671, 2018.
* [10] Alan R Mardinly, Ian Anton Oldenburg, Nicolas C Pegard, Savitha Sridharan, Evan H Lyall, Kirill Chesnov, Stephen G Brohawn, Laura Waller, and Hillel Adesnik. Precise multimodal optical control of neural ensemble activity. _Nature Neuroscience_, 21(6):881-893, 2018.
* [11] James H Marshel, Yoon Seok Kim, Timothy A Machado, Sean Quirin, Brandon Benson, Jonathan Kadmon, Cephra Raja, Adelaida Chibukhchyan, Charu Ramakrishnan, Masatoshi Inoue, et al. Cortical layer-specific critical dynamics triggering perception. _Science_, 365(6453):eaaw5202, 2019.
* [12] Luis Carrillo-Reid, Shuting Han, Weijian Yang, Alejandro Akrouh, and Rafael Yuste. Controlling visually guided behavior by holographic recalling of cortical ensembles. _Cell_, 178(2):447-457, 2019.
* [13] Hillel Adesnik and Lamiae Abdeladim. Probing neural codes with two-photon holographic optogenetics. _Nature Neuroscience_, 24(10):1356-1366, 2021.

* [14] Eirini Papagiakoumou, Emiliano Ronzitti, and Valentina Emiliani. Scanless two-photon excitation with temporal focusing. _Nature Methods_, 17(6):571-581, 2020.
* [15] Christopher A Baker, Yishai M Elyada, Andres Parra, and M McLean Bolton. Cellular resolution circuit mapping with temporal-focused excitation of soma-targeted channelrhodopsin. _Elife_, 5:e14193, 2016.
* [16] Or A Shemesh, Dimitri Tanese, Valeria Zampini, Changyang Linghu, Kiryl Piatkevich, Emiliano Ronzitti, Eirini Papagiakoumou, Edward S Boyden, and Valentina Emiliani. Temporally precise single-cell-resolution optogenetics. _Nature Neuroscience_, 20(12):1796-1806, 2017.
* [17] Travis A Hage, Alice Bosma-Moody, Christopher A Baker, Megan B Kratz, Luke Campagnola, Tim Jarsky, Hongkui Zeng, and Gabe J Murphy. Synaptic connectivity to l2/3 of primary visual cortex measured by two-photon optogenetic stimulation. _Elife_, 11:e71103, 2022.
* [18] Valentina Emiliani, Adam E Cohen, Karl Deisseroth, and Michael Hausser. All-optical interrogation of neural circuits. _Journal of Neuroscience_, 35(41):13917-13926, 2015.
* [19] Adam M Packer, Lloyd E Russell, Henry WP Dalgleish, and Michael Hausser. Simultaneous all-optical manipulation and recording of neural circuit activity with cellular resolution in vivo. _Nature Methods_, 12(2):140-146, 2015.
* [20] Zihui Zhang, Lloyd E Russell, Adam M Packer, Oliver M Gauld, and Michael Hausser. Closed-loop all-optical interrogation of neural circuits in vivo. _Nature Methods_, 15(12):1037-1040, 2018.
* [21] Lloyd E Russell, Henry WP Dalgleish, Rebecca Nutbrown, Oliver M Gauld, Dustin Herrmann, Mehmet Fisek, Adam M Packer, and Michael Hausser. All-optical interrogation of neural circuits in behaving mice. _Nature Protocols_, 17(7):1579-1620, 2022.
* [22] Hayley A Bounds, Masato Sadahiro, William D Hendricks, Marta Gajowa, Ian Anton Oldenburg, Karthika Gopakumar, Daniel Quintana, Tanya Daigle, Hongkui Zeng, and Hillel Adesnik. Multifunctional cre-dependent transgenic mice for high-precision all-optical interrogation of neural circuits. _bioRxiv_, page 463223, 2021.
* [23] Kamiar Rahnama Rad and Liam Paninski. Efficient, adaptive estimation of two-dimensional firing rate surfaces via gaussian process methods. _Network: Computation in Neural Systems_, 21(3-4):142-168, 2010.
* [24] Cristina Savin and Gasper Tkacik. Estimating nonlinear neural response functions using gp priors and kronecker methods. _Advances in Neural Information Processing Systems_, 29, 2016.
* [25] M Rule, P Chaudhuri-Vayalambrone, Marino Krstulovic, Marius Bauza, Julija Krupic, and Timothy O'Leary. Variational log-gaussian point-process methods for grid cells. _bioRxiv_, pages 2023-03, 2023.
* [26] Jakob H Macke, Sebastian Gerwinn, Leonard E White, Matthias Kaschube, and Matthias Bethge. Gaussian process methods for estimating cortical maps. _NeuroImage_, 56(2):570-581, 2011.
* [27] Mijung Park, Greg Horwitz, and Jonathan Pillow. Active learning of neural response functions with gaussian processes. _Advances in Neural Information Processing Systems_, 24, 2011.
* [28] Mijung Park, J Patrick Weller, Gregory D Horwitz, and Jonathan W Pillow. Bayesian active learning of neural firing rate maps with transformed gaussian process priors. _Neural Computation_, 26(8):1519-1541, 2014.
* [29] Jonathan P Newman, Ming-fai Fong, Daniel C Millard, Clarissa J Whitmire, Garrett B Stanley, and Steve M Potter. Optogenetic feedback control of neural activity. _Elife_, 4:e07192, 2015.
* [30] Yuxiao Yang, Allison T Connolly, and Maryam M Shanechi. A control-theoretic system identification framework and a real-time closed-loop clinical simulation testbed for electrical brain stimulation. _Journal of Neural Engineering_, 15(6):066007, 2018.

* [31] Michael F Bolus, Adam A Willats, Clarissa J Whitmire, Christopher J Rozell, and Garrett B Stanley. Design strategies for dynamic closed-loop optogenetic neurocontrol in vivo. _Journal of Neural Engineering_, 15(2):026011, 2018.
* [32] Nishal Shah, Sasidhar Madugula, Pawel Hottowy, Alexander Sher, Alan Litke, Liam Paninski, and EJ Chichilnisky. Efficient characterization of electrically evoked responses for neural interfaces. _Advances in Neural Information Processing Systems_, 32, 2019.
* [33] Michael F Bolus, Adam A Willats, Christopher J Rozell, and Garrett B Stanley. State-space optimal feedback control of optogenetically driven neural activity. _Journal of Neural Engineering_, 18(3):036006, 2021.
* [34] Yuxiao Yang, Shaoyu Qiao, Omid G Sani, J Isaac Sedillo, Breonna Ferrentino, Bijan Pesaran, and Maryam M Shanechi. Modelling and prediction of the dynamic responses of large-scale brain networks during direct electrical stimulation. _Nature Biomedical Engineering_, 5(4):324-345, 2021.
* [35] Yashar Ahmadian, Adam M Packer, Rafael Yuste, and Liam Paninski. Designing optimal stimuli to control neuronal spike timing. _Journal of Neurophysiology_, 106(2):1038-1053, 2011.
* [36] Tao Hu and Dmitri Chklovskii. Reconstruction of sparse circuits using multi-neuronal excitation (resume). _Advances in Neural Information Processing Systems_, 22:790-798, 2009.
* [37] Alyson K Fletcher, Sundeep Rangan, Lav R Varshney, and Aniruddha Bhargava. Neural reconstruction with approximate message passing (neuramp). _Advances in Neural Information Processing Systems_, 2011.
* [38] Ben Shababo, Brooks Paige, Ari Pakman, and Liam Paninski. Bayesian inference and online experimental design for mapping neural microcircuits. _Advances in Neural Information Processing Systems_, 26:1304-1312, 2013.
* [39] Alyson K Fletcher and Sundeep Rangan. Scalable inference for neuronal connectivity from calcium imaging. _Advances in Neural Information Processing Systems_, 2014.
* [40] Laurence Aitchison, Lloyd Russell, Adam Packer, Jinyao Yan, Philippe Castonguay, Michael Hausser, and Srinivas C Turaga. Model-based bayesian inference of neural activity and connectivity from all-optical interrogation of a neural circuit. _Advances in Neural Information Processing Systems_, 2017.
* [41] Anne Draelos and John Pearson. Online neural connectivity estimation with noisy group testing. _Advances in Neural Information Processing Systems_, 33, 2020.
* [42] Marcus A Triplett, Marta Gajowa, Benjamin Antin, Masato Sadahiro, Hilleel Adesnik, and Liam Paninski. Rapid learning of neural circuitry from holographic ensemble stimulation enabled by model-based compressed sensing. _bioRxiv_, page 507926, 2022.
* [43] Yoav Printz, Pritish Patil, Mathias Mahn, Asaf Benjamin, Anna Litvin, Rivka Levy, Max Bringmann, and Ofer Yizhar. Determinants of functional synaptic connectivity among amygdala-projecting prefrontal cortical neurons in male mice. _Nature Communications_, 14(1):1667, 2023.
* [44] Savitha Sridharan, Marta A Gajowa, Mora B Ogando, Uday K Jagadisan, Lamiae Abdeladim, Masato Sadahiro, Hayley A Bounds, William D Hendricks, Toby S Turney, Ian Tayler, et al. High-performance microbial opsins for spatially and temporally precise perturbations of large neuronal networks. _Neuron_, 2022.
* [45] Christopher KI Williams and Carl Edward Rasmussen. _Gaussian Processes for Machine Learning_, volume 2. MIT Press Cambridge, MA, 2006.
* [46] Paolo Bonifazi, Miri Goldin, Michel A Picardo, Isabel Jorquera, A Cattani, Gregory Bianconi, Alfonso Represa, Yehezkel Ben-Ari, and Rosa Cossart. Gabaergic hub neurons orchestrate synchrony in developing hippocampal networks. _Science_, 326(5958):1419-1424, 2009.
* [47] Marco Bocchio, Claire Gouny, David Angulo-Garcia, Tom Toulat, Thomas Tressard, Eleonora Quiroli, Agnes Baude, and Rosa Cossart. Hippocampal hub neurons maintain distinct connectivity throughout their lifetime. _Nature Communications_, 11(1):4559, 2020.

* [48] Michalis Titsias. Variational learning of inducing variables in sparse gaussian processes. In _Artificial Intelligence and Statistics (AISTATS)_, pages 567-574. PMLR, 2009.
* [49] Jeremy Lewi, Robert Butera, and Liam Paninski. Sequential optimal design of neurophysiology experiments. _Neural Computation_, 21(3):619-687, 2009.
* [50] Jonathan W Pillow and Mijung Park. Adaptive bayesian methods for closed-loop neurophysiology. _Closed Loop Neuroscience_, pages 3-18, 2016.
* [51] Andrew James McHutchon et al. _Nonlinear modelling and control using Gaussian processes_. PhD thesis, Citeseer, 2015.

Supplementary material

### Animal ethics statement

All experiments on animals were conducted with approval of the Animal Care and Use Committee of the University of California, Berkeley.

### Compute

All computational procedures were performed either on a desktop workstation running Ubuntu 18.04 with an Intel Xeon E5-2620 v4 CPU, four GTX 1080 Ti GPUs, and 112GB RAM, or on the Axon computer cluster based at the Zuckerman Institute (Columbia University) using nodes comprised of two Xeon E5-2660 v4 CPUs, eight GTX 1080 Ti GPUs, and 125GB RAM.

### Broader societal impact

Our work is significant for interventional approaches to studying the brain and its connection to disease. By minimising off-target activation, Bayesian target optimisation could enable (e.g.) more precise synaptic connectivity mapping, improving our understanding of neural circuitry. This advancement has potential implications for understanding brain disorders like epilepsy, where abnormal synaptic connections are central to seizure generation and propagation. Deepening our understanding of these diseases can lead to enhanced targeted interventions and more effective therapeutic strategies, benefiting individuals with neurological disorders.

### Code availability

An open-source implementation of Bayesian target optimisation is available in Python at https://github.com/marcustriplett/bataro.

### Single-target holographic stimulus optimisation with posterior uncertainty

Here we provide further mathematical details for optimising holographic stimuli. First, we develop the approach for single optogenetic targets, as this is most closely related to existing GP-based receptive field inference techniques. The single-target case also allows us to have a full treatment of posterior uncertainty (unlike for optimising ensemble stimuli) which may be desired in certain applications.

**Optogenetic receptive field model.** We use a GP-Bernoulli approach to model the response \(y_{nt}\) of neuron \(n\) on trial \(t\) to a single-target stimulus \(\mathbf{x}_{t}\),

\[y_{nt}\sim\text{Bernoulli}(\sigma(g_{n}(\mathbf{x}_{t}))),\] (9)

where the stimulus \(\mathbf{x}_{t}=(c_{1t},c_{2t},I_{t})\in\mathbb{R}^{3}\) represents the two-dimensional coordinates and laser power of the \(t\)-th hologram. Each ORF follows a three-dimensional GP prior \(g_{n}\sim\mathcal{GP}(m_{n}(\cdot),k(\cdot,\cdot))\), where \(m_{n}\) and \(k\) again are the mean and covariance functions of the GP.

**Posterior inference.** Unlike for ensemble stimulation, for single-target stimulation we do not require that the ORF \(g_{n}\) is non-negative. This is because now if a point on the ORF becomes inhibitory (by taking a negative value), it will not conflict with excitation from any other hologram. Consequently, the posterior of \(g_{n}\) is a GP, which allows us to work with a full description of posterior uncertainty. To compute the posterior, we use the conventional Laplace approximation. Briefly, this consists of approximating the posterior using a multivariate normal \(q(g_{n}\mid\boldsymbol{\mu}_{n},\boldsymbol{\Sigma}_{n})=\text{Normal}(g_{n} \mid\boldsymbol{\mu}_{n},\boldsymbol{\Sigma}_{n})\approx p(g_{n}\mid\mathbf{ y}_{n},\mathbf{X},\phi)\). The mean \(\boldsymbol{\mu}_{n}\) is obtained by maximising the log-posterior, given by the expression

\[\ln p(g_{n}\mid\mathbf{y}_{n},\mathbf{X},\phi)=\sum_{t=1}^{T}\ln p(\mathbf{y }_{nt}\mid\mathbf{x}_{t},g_{n})+\ln p(g_{n}(\mathbf{x}_{1}),\ldots,g_{n}( \mathbf{x}_{T})\mid\phi)+\text{const},\] (10)

where \(\mathbf{X}=(\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) and where const does not depend on \(g_{n}\). Since the posterior is log-concave in \(g_{n}\), we use Newton's method to identify the global optimum of Equation 10, and adaptively set the Newton step-size using a standard backtracking line-search method. Letting\(\mathbf{y}_{n},\mathbf{X},\phi)\) be the Hessian of the log-posterior, the posterior covariance matrix is obtained by setting \(\bm{\Sigma}_{n}=-\mathbf{H}^{-1}\mid_{g_{n}=\mu_{n}}\).

**Target optimisation**. Let \(G=(g_{1},\ldots,g_{N})\), and define the predicted evoked activity for single holographic targets as \(\hat{y}(\mathbf{x},G)=(\sigma(g_{1}(\mathbf{x})),\ldots,\sigma(g_{N}(\mathbf{x })))\). To minimise the error between a target binary activity pattern \(\bm{\Omega}\in\{0,1\}^{N}\) and the predicted evoked activity, we solve an optimisation problem that accounts for the uncertainty in the ORF estimates:

\[\mathbf{x}_{\text{optimal}}=\operatorname*{argmin}_{\mathbf{x}}\;\mathbb{E}_{ q(G|\bm{\mu},\bm{\Sigma})}\left[\left\|\bm{\Omega}-\hat{y}(\mathbf{x},G) \right\|^{2}\right]\quad\text{such that}\quad 0\leq I\leq I_{\text{max}},\] (11)

where \(q(G\mid\bm{\mu},\bm{\Sigma})=\prod_{n=1}^{N}q(g_{n}\mid\bm{\mu}_{n},\bm{ \Sigma}_{n})\) gives the joint posterior across all ORFs. To solve Equation 11, we first sample ORFs \(g_{n}^{(s)}\) (for \(s=1,\ldots,S\)) from their posterior distributions to approximate the expected error at the current estimate \(\mathbf{x}^{*}\),

\[\mathbb{E}_{q(G|\bm{\mu},\bm{\Sigma})}\left[\left\|\bm{\Omega}- \hat{y}(\mathbf{x}^{*},G)\right\|^{2}\right]\approx\frac{1}{S}\sum_{s=1}^{S} \sum_{n=1}^{N}\left(\Omega_{n}-\sigma(g_{n}^{(s)}(\mathbf{x}^{*}))\right)^{2}.\] (12)

Then, we compute the partial derivative (in dimension \(d\)) of the expected error by differentiating through the Monte Carlo approximation,

\[\frac{\partial}{\partial x_{d}^{*}}\mathbb{E}_{q(G|\bm{\mu}, \bm{\Sigma})}\left[\left\|\bm{\Omega}-\hat{y}(\mathbf{x}^{*},G)\right\|^{2} \right]\approx-\frac{2}{S}\sum_{s=1}^{S}\sum_{n=1}^{N}(\Omega_{n}-\sigma(g_{n }^{(s)}(\mathbf{x}^{*}))\sigma^{\prime}(g_{n}^{(s)}(\mathbf{x}^{*}))\frac{ \partial}{\partial x_{d}^{*}}g_{n}^{(s)}(\mathbf{x}^{*}).\] (13)

Next we must evaluate the partial derivative on the right-hand side of Equation 13. We use the fact that a GP and its derivative are jointly GP-distributed, and hence infer the derivative from observations of the ORF. The covariance between a GP and its derivative is given by [45, Sec 9.4]

\[\text{cov}\left(g_{n}(\mathbf{x}_{t}),\frac{\partial}{\partial x _{d}^{*}}g_{n}(\mathbf{x}^{*})\right)=\frac{\partial k(\mathbf{x}_{t},\mathbf{ x}^{*})}{\partial x_{d}^{*}}=\frac{\alpha^{2}}{\lambda_{d}^{2}}(x_{dt}-x_{d}^{*}) \exp\left(-\frac{\left\|\mathbf{x}_{t}-\mathbf{x}^{*}\right\|^{2}}{2\lambda_{d }^{2}}\right),\] (14)

where the second equality is specific to the RBF covariance. Thus, we can use Equation 14 to obtain the posterior predictive mean for the derivative GPs in closed form as [51, Sec 2.7]

\[\mathbb{E}_{q(g_{n}|\bm{\mu}_{n},\bm{\Sigma}_{n})}\left[\frac{ \partial g_{n}(\mathbf{x}^{*})}{\partial x_{d}^{*}}\right]=\frac{\partial m_{ n}(\mathbf{x}^{*})}{\partial x_{d}^{*}}+\text{cov}\left(g_{n}(\mathbf{X}), \frac{\partial g_{n}(\mathbf{x}^{*})}{\partial x_{d}^{*}}\right)^{\top}\mathbf{ K}^{-1}(\bm{\mu}_{n}-m_{n}(\mathbf{X})).\] (15)

Here \(\mathbf{X}=(\mathbf{x}_{1},\ldots,\mathbf{x}_{T})\) is the collection of unique points on the ORF probed during calibration. If Equation 15 is combined with an expression for the posterior predictive variance, one obtains a full predictive distribution over derivative functions consistent with the observed neural responses. However, rather than working with this full distribution, we instead use Equation 15 to approximate the derivatives of the Monte Carlo samples by replacing the posterior mean \(\bm{\mu}_{n}\) with a Monte Carlo sample,

\[\frac{\partial g_{n}^{(s)}(\mathbf{x}^{*})}{\partial x_{d}^{*}} \approx\frac{\partial m_{n}(\mathbf{x}^{*})}{\partial x_{d}^{*}}+\text{cov} \left(g_{n}(\mathbf{X}),\frac{\partial g_{n}(\mathbf{x}^{*})}{\partial x_{d}^{* }}\right)^{\top}\mathbf{K}^{-1}(g_{n}^{(s)}(\mathbf{X})-m_{n}(\mathbf{X})).\] (16)

Equation 16 then allows us to define a closed-form approximate gradient \(\tilde{\nabla}_{\mathbf{x}^{*}}g_{n}^{(s)}\) at test point \(\mathbf{x}^{*}\), defined as

\[\tilde{\nabla}_{\mathbf{x}^{*}}g_{n}^{(s)}=\left[\frac{\partial g _{n}^{(s)}(\mathbf{x}^{*})}{\partial x_{1}^{*}},\ldots,\frac{\partial g_{n}^{(s) }(\mathbf{x}^{*})}{\partial x_{D}^{*}}\right]^{\top},\] (17)

which we use in the single-target projected gradient descent algorithm (Algorithm 2). Note that one could also consider a quadrature approach to solving Equation 12, which may be more efficient than Monte Carlo sampling. However, the presentation of the Monte Carlo approach is instructive for deriving the optimisation of ensemble stimuli below.

### Additional details on ensemble stimulus optimisation approach

The approach for optimising holographic ensemble stimuli is based on the approach for single-target optimisation, but modified to account for differences in the ORF model and inference. In particular, we again seek to minimise the error between a target activity pattern \(\bm{\Omega}\) and the predicted evoked activity, but now using the MAP estimates \(\mathcal{G}=\{\hat{g}_{n_{j}}\hat{\theta}_{n}\}_{n=1}^{N}\) in place of the full posterior distributions. Let \(\hat{y}(\mathbf{x},\mathcal{G})=(\sigma(\hat{\gamma}_{1}(\mathbf{x})-\hat{ \theta}_{1}),\ldots,\sigma(\hat{\gamma}_{N}(\mathbf{x})-\hat{\theta}_{N}))\) be the predicted population response to an ensemble stimulus, where \(\hat{\gamma}_{n}(\mathbf{x})=\sum_{j=1}^{J}\hat{g}_{n}\left(\mathbf{x}^{j}\right)\). The optimal ensemble stimulus is now

\[\mathbf{x}_{\text{optimal}}=\operatorname*{argmin}_{\mathbf{x}}\|\bm{\Omega} -\hat{y}(\mathbf{x},\mathcal{G})\|^{2}=\operatorname*{argmin}_{\mathbf{x}}\sum _{n=1}^{N}\left(\Omega_{n}-\sigma(\hat{\gamma}_{n}(\mathbf{x})-\hat{\theta}_{n })\right)^{2}\] (18)

such that \(0\leq I\leq I_{\text{max}}\). Evaluating the partial derivative of Equation 18 with respect to dimension \(d\) of a test point \(\mathbf{x}^{*}\) yields,

\[\frac{\partial}{\partial x_{d}^{*}}\|\bm{\Omega}-\hat{y}(\mathbf{x}^{*}, \mathcal{G})\|^{2}=-2\sum_{n=1}^{N}(\Omega_{n}-\sigma(\hat{\gamma}_{n}( \mathbf{x}^{*})-\hat{\theta}_{n}))\sigma^{\prime}(\hat{\gamma}_{n}(\mathbf{x} ^{*})-\hat{\theta}_{n})\frac{\partial}{\partial x_{d}^{*}}\hat{\gamma}_{n}( \mathbf{x}^{*}).\] (19)

The derivative on the right-hand side of Equation 19 is given by \(\frac{\partial}{\partial x_{d}^{*}}\hat{\gamma}_{n}(\mathbf{x})=\sum_{j=1}^{J }\frac{\partial}{\partial x_{d}^{*}}\hat{g}_{n}(\mathbf{x}^{j})\), which requires computing the derivative of \(\hat{g}_{n}(\mathbf{x}^{j})\). To evaluate this derivative, we use a similar trick to Equation 16, but substituting the MAP estimate in place of the posterior mean or Monte Carlo sample,

\[\frac{\partial}{\partial x_{d}^{*}}\hat{g}_{n}(\mathbf{x}^{*})=\frac{\partial }{\partial x_{d}^{*}}m_{n}(\mathbf{x}^{*})+\text{cov}\left(g_{n}(\mathbf{X}), \frac{\partial}{\partial x_{d}^{*}}g_{n}(\mathbf{x}^{*})\right)^{\top}\mathbf{ K}^{-1}(\hat{g}_{n}(\mathbf{X})-m_{n}(\mathbf{X})).\] (20)

This expression can also be arrived at by first evaluating the posterior predictive mean of \(g_{n}(\mathbf{x}^{*})\), and then differentiating with respect to \(x_{d}^{*}\).

We use Equation 20 to define a closed-form gradient \(\nabla_{\mathbf{x}^{*}}\hat{\gamma}_{n}\) at test point \(\mathbf{x}^{*}\) via

\[\nabla_{\mathbf{x}^{*}}\hat{\gamma}_{n}=\left[\frac{\partial\hat{\gamma}_{n} (\mathbf{x}^{*})}{\partial x_{1}^{*}},\ldots,\frac{\partial\hat{\gamma}_{n}( \mathbf{x}^{*})}{\partial x_{D}^{*}}\right]^{\top}.\] (21)

Finally, Equation 21 is used in the projected gradient descent algorithm for optimising ensemble stimuli (Algorithm 1).

### Further details on simulations and "synthetic" optogenetics experiments

Simulations consisted of both ORF mapping and stimulus optimisation phases. ORF mapping required probing responses to stimulation at a range of laser powers and stimulus locations. We defined a grid of stimulation points surrounding each neuron. In the spatial dimensions, the grid ranged from \(-20~{}\mu\)m to 20 \(\mu\)m relative to the centroid of the neuron in steps of 10 \(\mu\)m, and powers ranged from 30 mW to 70 mW in steps of 20 mW. The complete grid was thus given by the Cartesian product \(\{-20,-10,0,10,20\}\times\{-20,-10,0,10,20\}\times\{30,50,70\}\). For opsin-expressing neurons that were spaced far apart, this coarse-resolution grid was sufficient because risk of OTS was low, and therefore ORF mapping was not needed at high detail. On the other hand, as the density of opsin-expressing neurons increased, the grids surrounding each neuron increasingly overlapped with each other, resulting in much denser sampling of the ORFs.

For the synthetic optogenetics experiments (based on the cell-attached recordings), we used the same spatial grid spacing but used laser powers of 10, 25, and 40 mW to match the range of powers used in the underlying slice experiment, though note that the slice experiment had a denser spacing than our chosen 15 mW (see example loose-patch recordings below), which we chose to reduce the ORF mapping time. For the optogenetics experiments involving three spatial dimensions, we extended the grid sampling to include depths of \(-60~{}\mu\)m to 60 \(\mu\)m in steps of 30 \(\mu\)m. We also explored the effect of reducing the number of probed grid points to further reduce the time spent mapping ORFs, and found that Bayesian target optimisation maintained high performance when probing with a \(3\times 3\) spatial grid of \(\{-12,0,12\}\times\{-12,0,12\}\) (Figure S1).

We selected the parameters of the GP covariance kernel using 5-fold cross-validation on a separate set of recordings that were made on the same set of four cells, ensuring the hyperparameter selection was using out-of-sample data. Cross-validation was performed using a grid search over a set of possible hyperparameters: the possible radial lengthscales were 2, 4, 8, 16, the power lengthscales were 2, 4, 8, 16, and the amplitudes were 1, 2, 4, 8, 16. For each hyperparameter combination \(\theta\) and for each cell, we used Newton's method to fit the GP-Bernoulli model to 80% of the trials in the loose-patch data, yielding an ORF estimate \(\hat{g}_{\theta}\). On the remaining 20% of the trials (denoted as \(\mathcal{T}_{\text{held-out}}\)), we evaluated the log-likelihood, \(\sum_{t\in\mathcal{T}_{\text{held-out}}}\{y_{t}\ln(\sigma(\hat{g}_{\theta}( \mathbf{x}_{t})))+(1-y_{t})\ln(1-\sigma(\hat{g}_{\theta}(\mathbf{x}_{t})))\}\). We averaged the log-likelihood across all five folds and across all four cells, and chose the hyperparameter combination \(\theta\) that yielded the largest average log-likelihood, resulting in a radial lengthscale of 8, a power lengthscale of 16, and a kernel amplitude of 8.

The GP parameters for generating the simulations in Figure 3, inferring the resulting ORFs, and generating synthetic optogenetics experiments with two and three spatial dimensions are given in Table S1. For reference, a typical ORF mean function is given in Figure S2.

Figure S2: Example mean function (shown at three powers) used for simulations (left column). Also shown are four samples from the ORF prior corresponding to this mean function (right four columns). Parameters given in Table S1.

\begin{table}
\begin{tabular}{|l|c|l|} \hline
**Parameter** & **Symbol** & **Value** \\ \hline \multicolumn{3}{|c|}{Simulations (data generation)} \\ \hline Mean function excitability & \(\rho\) & 0.125 \\ Mean function width & \(\sigma_{m}^{2}\) & \(3\times 10^{2}\)\(\mu\)m \\ Spike threshold & \(\theta\) & 3.5 \\ Kernel radial lengthscale & \(\lambda_{s}\) & 8 \(\mu\)m \\ Kernel power lengthscale & \(\lambda_{I}\) & 20 mW \\ Kernel amplitude & \(\alpha^{2}\) & 0.2 \\ Kernel marginal variance & \(\sigma_{d}^{2}\) & \(10^{-5}\) \\ Simultaneously stimulated neurons during ORF mapping & \(J\) & 10 \\ \hline \multicolumn{3}{|c|}{Simulations (ORF inference)} \\ \hline Mean function excitability & \(\rho\) & 0.125 \\ Mean function width & \(\sigma_{m}^{2}\) & \(3\times 10^{2}\)\(\mu\)m \\ Kernel radial lengthscale & \(\lambda_{s}\) & 5 \(\mu\)m \\ Kernel power lengthscale & \(\lambda_{I}\) & 16 mW \\ Kernel amplitude & \(\alpha^{2}\) & 1 \\ Kernel amplitude & \(\alpha^{2}\) & \(10^{-5}\) \\ Kernel marginal variance & \(\sigma_{d}^{2}\) & \(10^{-5}\) \\ Learning rate for spike thresholds (\(\{\theta_{n}\}_{n=1}^{N}\)) & \(-\) & \(5\) \\ Number of random initialisations & \(-\) & \(5\) \\ \hline \multicolumn{3}{|c|}{Synthetic optogenetics experiments (two spatial dimensions)} \\ \hline Mean function excitability & \(\rho\) & 0.175 \\ Mean function width & \(\sigma_{m}^{2}\) & \(3\times 10^{2}\)\(\mu\)m \\ Kernel radial lengthscale & \(\lambda_{s}\) & 8 \(\mu\)m \\ Kernel power lengthscale & \(\lambda_{I}\) & 16 mW \\ Kernel amplitude & \(\alpha^{2}\) & 8 \\ Kernel marginal variance & \(\sigma_{d}^{2}\) & \(10^{-5}\) \\ Learning rate for spike thresholds (\(\{\theta_{n}\}_{n=1}^{N}\)) & \(-\) & \(5\) \\ Number of random initialisations & \(-\) & \(5\) \\ \hline \multicolumn{3}{|c|}{Synthetic optogenetics experiments (three spatial dimensions)} \\ \hline Mean function excitability & \(\rho\) & 0.175 \\ Mean function width (x/y dimensions) & \(\sigma_{m}^{2}\) & \(3\times 10^{2}\)\(\mu\)m \\ Mean function width (z dimension) & \(-\) & \(3\times 10^{3}\)\(\mu\)m \\ Kernel radial lengthscale (x/y dimensions) & \(\lambda_{s}\) & 8 \(\mu\)m \\ Kernel axial lengthscale (z dimension) & \(\lambda_{z}\) & 32 \(\mu\)m \\ Kernel power lengthscale & \(\lambda_{I}\) & 16 mW \\ Kernel amplitude & \(\alpha^{2}\) & 8 \\ Kernel marginal variance & \(\sigma_{d}^{2}\) & \(10^{-5}\) \\ Learning rate for spike thresholds (\(\{\theta_{n}\}_{n=1}^{N}\)) & \(-\) & \(5\) \\ Number of random initialisations & \(-\) & \(5\) \\ \hline \end{tabular}
\end{table}
Table S1: Parameters used for simulations and generating synthetic optogenetics experiments.

### Additional examples of optogenetic receptive fields from cell-attached recordings

Figures S4 to S7 show examples of four ORFs that have been comprehensively mapped using two-photon optogenetic stimulation and cell-attached recordings of evoked spikes. Note the unpredictable differences in ORF shape across laser powers and depths, motivating a nonparametric approach.

Figure S5: Loose-patch recording and inferred ORF (experiment 2/4)

Figure S6: Loose-patch recording and inferred ORF (experiment 3/4)

Figure S7: Loose-patch recording and inferred ORF (experiment 4/4)