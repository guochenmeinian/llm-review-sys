# Library Learning Doesn't: The Curious Case of the Single-Use "Library"

 Ian Berlot-Attwell

University of Toronto

Vector Institute

ianberlot@cs.toronto.edu Frank Rudzicz

Dalhousie University

Vector Institute

frank@dal.ca Xujie Si

University of Toronto

Vector Institute

six@cs.toronto.edu

###### Abstract

Advances in Large Language Models (LLMs) have spurred a wave of LLM library learning systems for mathematical reasoning. These systems aim to learn a reusable library of _tools_, such as formal Isabelle lemmas [20] or Python programs that are tailored to a family of tasks. Many of these systems are inspired by the human structuring of knowledge into reusable and extendable concepts [9], but do current methods actually learn reusable libraries of tools?

We study two library learning systems for mathematics which both reported increased accuracy: LEGO-Prover [21] and TroVE [22]. We find that function reuse is extremely infrequent on miniF2F [11] and MATH [1]. Our followup ablation experiments suggest that, rather than reuse, self-correction and self-consistency are the primary drivers of the observed performance gains. Our code and data are available at https://github.com/ikb-a/curious-case.

## 1 Introduction

Mathematical progress is made by building with, and building upon, the tools of those who came before. Consequently, it is no surprise that there is research interest in developing systems that can automatically learn such reusable mathematical tools. Recently, LLMs have enabled new tool-learning methods with improved performance [21, 22, 23] - but are these systems truly learning generalized, reusable knowledge or is performance improved through other mechanisms? In this work, we study two prior systems: LEGO-Prover which aims to learn reusable formal Isabelle lemmas, and TroVE which aims to learn reusable Python functions. For both, our analysis of the model's behaviour reveals that direct reuse is negligible. Furthermore, we perform two ablation studies supporting our position that function reuse plays a limited role in these systems' improved mathematical reasoning.

## 2 Related Work

LLM library learning, i.e., creating and reusing tools, depends on LLMs' ability to use tools. Prior evaluations of tool-use (typically assuming tools as REST APIs) [24] included real-world queries [25], dedicated test environments [11], and metrics ranging from LLM-as-a-judge [24] to tracking task-checkpoint completion [10].

In contrast, the evaluation of library learning systems has been limited. Accuracy is the metric of choice (Wang et al., 2024; Wang et al., 2024; Yuan et al., 2024; Wang et al., 2024), but cannot capture the extent or quality of reuse: an excellent library is useless to a weak reasoner, and a powerful reasoner can ignore a useless library and derive results from first principles. Prior attempts to evaluate library learning have been limited to static measures of individual functions such as cyclomatic complexity (McCabe, 1976; Zhang et al., 2024) and abstract syntax tree depth (Wang et al., 2024), or have answered specific questions such as the ease of human verification (Wang et al., 2024), accuracy under domain transfer (Zhang et al., 2024; Qian et al., 2023), or performance in the sub-problem of refactoring ground truth solutions(Lin et al., 2024).

In this study, we evaluate two library learning systems for mathematical reasoning: LEGO-Prover, and TroVE (see Sections 2.1 and 2.2). For a review of library learning systems, see Appendix A.

### LEGO-Prover: Purpose & Architecture

LEGO-Prover consumes a set of proposed theorems to produce corresponding formal Isabelle (Paulson, 1994) proofs. It was evaluated on the miniF2F (Zheng et al., 2022) dataset: each problem was attempted 100 times, and the system obtained feedback from the Isabelle verifier after each attempt. LEGO-Prover was designed to perform library learning. Using the term _skills_ in place of _tools_, Wang et al. (2024) claimed that "LEGO-Prover enables LLMs to utilize existing skills retrieved from the library" and "[m]odular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems." LEGO-Prover performs library learning via two LLM systems: 1) The Prover which uses the library to create proofs, and 2) the Evolver which iteratively refines the library. They communicate through shared databases, such as the _request db_ which stores proposed lemmas to be proven and added to library.

### TroVE: Purpose & Architecture

TroVE is a "method for inducing a toolbox of reusable functions to use in solving programmatic tasks," designed to receive a stream of word problems without a ground truth or verifier (Wang et al., 2024). For each problem, it attempts to produce a Python program that prints the correct solution. TroVE's mathematical reasoning was evaluated with the MATH dataset Hendrycks et al. (2021). Each problem is considered once: an LLM generates 15 solutions, and the best is selected based on self-consistency (i.e., majority vote) (Wang et al., 2023). In generation, 5 solutions ignore the library and directly generate a program (Skip mode), 5 create a reusable helper function for inclusion in the library (Create mode), and 5 use a function from the library (Import mode).

## 3 Analysis of LEGO-Prover

We begin by analyzing the publicly released LEGO-Prover evaluation log files 1(Wang et al., 2024). These logs are a subset of the unreleased Prover logs corresponding to the final attempts on the

\begin{table}
\begin{tabular}{l r r r r r r} \hline \hline  & & & \multicolumn{3}{c}{Verbatim reused} & \multicolumn{2}{c}{Name reused} \\ \cline{4-7} Split & Problems Solved & Lemmas in Prompts & 1 & 2+ & 1 & 2+ \\ \hline valid+GPT & 127 & 374 & 0 & 0 & 1 & 0 \\ valid+Human & 135 & 265 & 0 & 0 & 1 & 0 \\ test+GPT & 111 & 255 & 0 & 0 & 2 & 0 \\ test+Human & 122 & 339 & 1 & 0 & 2 & 0 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Lemma reuse in LEGO-Prover released logs. Note that **lemma reuse is very uncommon**, and **no lemma reused twice**. For each split, we report the number of problems solved, the number of unique lemmas occurring in the Proverâ€™s input prompts, the number of lemmas reused verbatim once, or more than once, and the number of lemmas whose _name_ is reused once, or more than once. A lemma is reused \(N\) times if it appears in \(N+1\) solutions (i.e., the initial use, and then \(N\) reuses).

successfully solved problems. Note that LEGO-Prover was evaluated on 4 data splits, and learned over 20,000 lemmas overall (Wang et al., 2024).

We find that only 1,233 lemmas (\(\sim\)6%) are used in the final solving step (i.e., are inputs to the Prover). Of these, exactly one lemma is reused by the Prover, and it is reused once (i.e., appears verbatim in two solutions). As the Prover may be adjusting a lemma (e.g., paraphrasing, commenting, etc...) we repeat the analysis, checking only for the lemma's name. Again, lemma reuse is rare, and no lemma is reused more than once (i.e., no lemma has its _name_ appear in 3 or more solutions). See Table 1 for details. For an example of verbatim vs. name use, see Appendix B.

Given these findings, there are only two possibilities by which LEGO-Prover may be performing reuse: 1) indirect reuse (e.g., the learned tools are useful, reusable exemplars, rather than directly used in the final solution), or 2) direct reuse occurs in the Evolver.

Instead, we hypothesize that reuse is not significantly boosting performance. We propose that self-correction (Pan et al., 2023) via the _request db_ is the main mechanism of action. Note that the Prover populates the _request db_ by: 1) adding lemmas that the LLM suggests may be helpful sub-steps, and 2) adding lemmas from solution attempts that Isabelle could not verify. The Evolver uses the _request db_ to modify existing tools to "aid in solving requests", and to "resolv[e] decomposed sub-goals" using the library (Wang et al., 2024). Thus, the performance gains may be due to a combination of chain-of-thought (Wei et al., 2022) (through the Prover's proposal of helpful lemmas for the Evolver to solve) and self-correction (through the Evolver's retrying of failed lemmas).

To test whether any form of reuse is increasing performance, we ablate LEGO-Prover to remove cross-problem sharing: each theorem is solved with its own independent state and databases. E.g., in place of a global _request db_, each problem now has its own independent _request db_. We evaluate on a random size 12 subset of the validation split and use 50 attempts per problem. We perform our ablation using OpenAI's GPT-4o-mini as the original results were published using now deprecated versions of GPT-3.5-Turbo; see Appendix E for full details of the ablation. Running 2 trials, we find that the ablation's performance is strong, solving only 1 question less than the baseline (see Figure 1). Studying the problems solved by only the baseline, we find that only the simplest of the input lemmas are possibly used (namely \(a^{2}\geq 0\) and \(ax^{2}+bx+c=0\Rightarrow c=-(ax^{2}+bx)\); see Appendix C). It is unclear as these facts are not treated as lemmas, and are given different justifications. This suggests that: 1) the LLM may be too weak if it needs examples of basic facts 2) the LLM struggles at reuse as it does not copy the given, verified, proofs.

## 4 Analysis of TroVE

As TroVE logs were not released, we re-ran TroVE on MATH, achieving accuracy within \(\pm\)2% (absolute) of reported (see Appendix, Table 3). Note that the TroVE library also learns import statements; we ignore these in our analysis for two reasons. Firstly, our interest is in whether the system learns and reuses non-trivial tools, unlike statements such as "import math" and "from sympy import symbols". Secondly, as TroVE includes the entire library as part of the Import prompt, and import statements are innately simple, it is impossible to determine whether an import statement is included in the LLM output due to reuse, or the LLM's innate knowledge.

Figure 1: LEGO-Prover performance on a subset of the miniF2F validation split. The ablated model cannot reuse lemmas and performs similarly. The shaded region is one standard deviation, capturing variations in LLM output and race conditions.

Analyzing the logs, we find that TroVE's final libraries only contain 15 learned functions, having learned functions for only 3 of the 7 MATH subject test splits: counting, number, and pre-algebra. No functions are learned in the algebra, geometry, intermediate algebra, or pre-calculus splits. Of the 15 learned functions, only 2 are reused in a correct solution: is_perfect_square(n) is reused in one correct solution and is_prime(num) is reused in two correct solutions.

Given 3 successful reuses in 3,201 test questions, we believe that TroVE's improvements over the baselines are not due to function reuse. Instead, we believe that ensembling and self-consistency are responsible. To test this, we ablate the model by disabling Import mode, but maintaining the 15 solution attempts: we generate 8 solutions ignoring the library (i.e., Skip mode) and 7 attempting to create a helper function (i.e., Create mode). As in the original work we use CodeLlama-7b-Instruct-hf [Roziere et al., 2023]; see Appendix F for the full ablation details. Ablating Import mode prevents reuse as the library never appears in the model's input, thus also preventing library learning of import statements. As to why this ablation could still be performant, prior work established the benefits of self-consistency and increased sampling [Brown et al., 2024], and it's known that library-less tool-creation can boost performance by forcing abstract reasoning [Yuan et al., 2024].

We evaluate our ablated model on the intermediate_algebra test split (reportedly the largest performance gain over non-reuse baselines), and the geometry, number, and count test splits. On the intermediate_algebra, number, and count splits, our ablation exceeds the baseline's performance, with the improvement being statistically significant on two splits (See Table 2). On only the geometry split does the base model perform slightly better, though the learned libraries only contains import statements. From this we can conclude that library learning _import statements_ can be slightly beneficial, but only for certain domains. Typically, TroVE's library learning degrades its performance.

## 5 Conclusions

In this study, we find that both TroVE and LEGO-Prover do not directly reuse the tools they learn. Furthermore, the results of our ablations suggest that their performance gains cannot be solely attributed to indirect reuse either.

We intend that this paper be a call for the better understanding of the limitations of current library learning systems, and for improved evaluation. We show that accuracy is misleading in isolation: the system's reuse behaviour is paramount, and careful ablation is critical. Both papers studied made sensible claims as the created systems were deliberately designed for library learning and were tested against ablations that were not unreasonable - however they also relied heavily on accuracy as a metric instead of directly observing the systems' use of the library, and both chose ablations that in hindsight were too aggressive. It is clear that, particularly for ablations of library learning systems, minimal changes are preferable, and considerable thought should be put into other possible causes of improvements. There is a clear need for a broadly applicable framework for the evaluation of library learning specifically; this framework must rely on more than task accuracy and ablations to evaluate library learning and reuse.

Finally, considering library learning for mathematics in general: are LLMs capable learning tools and performing direct, verbatim reuse? Given that the observed improvements do not come from direct reuse, would direct reuse actually improve systems for mathematical reasoning, or is it overly brittle making soft reuse desirable? These important questions follow from our findings, and should inform the design of future research into library learning systems.

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & \multicolumn{4}{c}{Accuracy on MATH test split} \\ \cline{2-5} Model & count & geo & inte & num \\ \hline TroVE Reproduced & 0.236 \(\pm\) 0.008 & **0.058**\(\pm\) 0.004 & 0.120 \(\pm\) 0.006 & 0.258 \(\pm\) 0.007 \\ No Reuse Ablation & **0.250**\(\pm\) 0.000\(\dagger\) & 0.050 \(\pm\) 0.000 & **0.134**\(\pm\) 0.014 & **0.290**\(\pm\) 0.014\(\dagger\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: TroVE performance on MATH for the ablation and the baseline. Mean and standard deviation over 5 trials are reported. The variations arise from LLM output. \(\dagger\) indicates that mean ablation performance is significantly strictly higher than the baselineâ€™s, at the Bonferroni-corrected 0.05 level, using a 2-sample 1-sided Welchâ€™s t-test (note, this test assumes approximate normality).

## 6 Limitations & Broader Impact

Due to resource constraints, our ablation studies could be more thorough. Most obviously, we only study two models, and on two datasets. The LEGO-Prover ablation is not ideal, as library learning is disadvantaged by operating on a subset of the questions; this was necessary due to resource constraints. Another limitation is that LEGO-Prover's databases are pre-loaded with the full dataset of problems; consequently, the Evolvers are exposed to other problem statements - note, however, that the impact on testing reuse is minimal. Firstly, the Prover cannot attempt to solve any of these other problems, thus the _request db_ cannot gain pending lemmas related to other problems. Secondly, under the ablated model, tasks cannot share lemmas - any performance gains would come from having access to other sample problems instead of reuse.

While we demonstrate that the performance gains in mathematical reasoning seen by TroVE and LEGO-Prover cannot be attributed to the direct learning and reuse of tools, there is a very important but _subtly different_ question which remains unanswered: whether these systems are at all capable of library learning. It is possible that these systems have the capacity to learn reusable functions and lemmas, but the datasets do not provide the opportunity. Manually inspecting the MATH dataset, our tentative conclusion is that the dataset is intrinsically not amenable to function learning with Python - we suspect the questions are too diverse, with the shared components already being captured by standard libraries. How this could be more formally demonstrated remains an important open question that is beyond the scope of this work.

This work has no immediate societal impact, rather, it highlights current limitations and challenges assumptions in this field. However, deploying tool-learning systems may carry a security risk from executing LLM-generated code (we sandboxed TroVE). More generally, library learning systems are self-improving through code generation, an approach that has raised concerns (Zelikman et al., 2023). Unexpected behaviours may develop, thus requiring sandboxing and monitoring, at the very least.

## Acknowledgments and Disclosure of Funding

Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute www.vectorinstitute.ai/partnerships/. Generous support was also provided by the Microsoft Accelerating Foundation Models Research (AFMR) program.

We would also like to thank Zhiruo Wang, Zhaoyu Li, William Cunningham, and our anonymous reviewers for their time and conversations that helped in various ways to shape and improve this work. Finally, the lead author would like to thank Frank Rudzicz for years of guidance and support, and Xujie Si for both encouraging this work as being of interest to the mathematical reasoning community, and for providing critical resources without which it could not have been possible. Thank you everyone for helping make this work possible.

## References

* A Generic Theorem Prover_, volume 828 of _Lecture Notes in Computer Science_. Springer, 1994. ISBN 3-540-58244-4. doi: 10.1007/BFB0030541. URL https://doi.org/10.1007/BFb0030541.
* Ellis et al. (2021) Kevin Ellis, Catherine Wong, Maxwell I. Nye, Mathias Sable-Meyer, Lucas Morales, Luke B. Hewitt, Luc Cary, Armando Solar-Lezama, and Joshua B. Tenenbaum. DreamCoder: bootstrapping inductive program synthesis with wake-sleep library learning. In Stephen N. Freund and Eran Yahav, editors, _PLDI '21: 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, Virtual Event, Canada, June 20-25, 2021_, pages 835-850. ACM, 2021. doi: 10.1145/3453483.3454080. URL https://doi.org/10.1145/3453483.3454080.
* Wang et al. (2024) Haiming Wang, Huajian Xin, Chuanyang Zheng, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, Jian Yin, Zhenguo Li, and Xiaodan Liang. LEGO-Prover: Neural theorem proving with growing libraries. In _The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_. OpenReview.net, 2024a. URL https://openreview.net/forum?id=3f5PALef5B.

Zhiruo Wang, Graham Neubig, and Daniel Fried. TroVE: Inducing verifiable and efficient toolboxes for solving programmatic tasks. In _Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_. OpenReview.net, 2024b. URL https://openreview.net/forum?id=DCNCwAMJjI.
* Zheng et al. (2022) Kunhao Zheng, Jesse Michael Han, and Stanislas Polu. miniF2F: a cross-system benchmark for formal Olympiad-level mathematics. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_. OpenReview.net, 2022. URL https://openreview.net/forum?id=92PegFuFTFv.
* Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. _NeurIPS_, 2021.
* Zhang et al. (2024) Shaokun Zhang, Jieyu Zhang, Jiale Liu, Linxin Song, Chi Wang, Ranjay Krishna, and Qingyun Wu. Offline training of language model agents with functions as learnable weights. In _Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_. OpenReview.net, 2024a. URL https://openreview.net/forum?id=2xbkWiEuR1.
* Yuan et al. (2024) Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi Fung, Hao Peng, and Heng Ji. CRAFT: Customizing LLMs by creating and retrieving from specialized toolsets. In _The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_. OpenReview.net, 2024. URL https://openreview.net/forum?id=G0vdDSt9XM.
* Qu et al. (2024) Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and Ji-Rong Wen. Tool learning with large language models: A survey. _CoRR_, abs/2405.17935, 2024. doi: 10.48550/ARXIV.2405.17935. URL https://doi.org/10.48550/arXiv.2405.17935.
* Yan et al. (2024) Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard. https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html, 2024.
* Li et al. (2023) Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. API-Bank: A comprehensive benchmark for tool-augmented LLMs. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023_, pages 3102-3116. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023. EMNLP-MAIN.187. URL https://doi.org/10.18653/v1/2023.emnlp-main.187.
* Guo et al. (2024) Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. StableToolBench: Towards stable large-scale benchmarking on tool learning of large language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, _Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024_, pages 11143-11156. Association for Computational Linguistics, 2024. URL https://aclanthology.org/2024.findings-acl.664.
* Lu et al. (2024) Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, and Ruoming Pang. ToolSandbox: A stateful, conversational, interactive evaluation benchmark for LLM tool use capabilities. _CoRR_, abs/2408.04682, 2024. doi: 10.48550/ARXIV.2408.04682. URL https://doi.org/10.48550/arXiv.2408.04682.
* McCabe (1976) T.J. McCabe. A complexity measure. _IEEE Transactions on Software Engineering_, SE-2(4):308-320, 1976. doi: 10.1109/TSE.1976.233837.
* Qian et al. (2023) Cheng Qian, Chi Han, Yi Ren Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. CREATOR: Tool creation for disentangling abstract and concrete reasoning of large language models. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, _Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023_, pages 6922-6939. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FindINGS-EMNLP.462. URL https://doi.org/10.18653/v1/2023.findings-emnlp.462.
* Zhang et al. (2020)Xiaohan Lin, Qingxing Cao, Yinya Huang, Zhicheng Yang, Zhengying Liu, Zhenguo Li, and Xiaodan Liang. ATG: Benchmarking automated theorem generation for generative language models. In Kevin Duh, Helena Gomez-Adorno, and Steven Bethard, editors, _Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024_, pages 4465-4480. Association for Computational Linguistics, 2024. doi: 10.18653/V1/2024. FINDINGS-NAACL.279. URL https://doi.org/10.18653/v1/2024.findings-naacl.279.
* Wang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In _The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023_. OpenReview.net, 2023. URL https://openreview.net/forum?id=1PL1NIMrw.
* Pan et al. (2023) Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi Wang, and William Yang Wang. Automatically correcting large language models: Surveying the landscape of diverse self-correction strategies. _CoRR_, abs/2308.03188, 2023. doi: 10.48550/ARXIV.2308.03188. URL https://doi.org/10.48550/arXiv.2308.03188.
* December 9, 2022_, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.
* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton-Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code Llama: Open foundation models for code. _CoRR_, abs/2308.12950, 2023. doi: 10.48550/ARXIV.2308.12950. URL https://doi.org/10.48550/arXiv.2308.12950.
* Brown et al. (2024) Bradley C. A. Brown, Jordan Juravsky, Ryan Saul Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large Language Monkeys: Scaling inference compute with repeated sampling. _CoRR_, abs/2407.21787, 2024. doi: 10.48550/ARXIV.2407.21787. URL https://doi.org/10.48550/arXiv.2407.21787.
* Zelikman et al. (2023) Eric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (STOP): Recursively self-improving code generation. _CoRR_, abs/2310.02304, 2023. doi: 10.48550/ARXIV.2310.02304. URL https://doi.org/10.48550/arXiv.2310.02304.
* Cai et al. (2024) Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. In _The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_. OpenReview.net, 2024. URL https://openreview.net/forum?id=qV83K9d5WB.
* Wang et al. (2024) Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. _Trans. Mach. Learn. Res._, 2024, 2024c. URL https://openreview.net/forum?id=ehfRiFOR3a.
* Tan et al. (2024) Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, Ruyi An, Molei Qin, Chuqiao Zong, Longtao Zheng, Yujie Wu, Xiaoqiang Chai, Yifei Bi, Tianbao Xie, Pengjie Gu, Xiyun Li, Ceyao Zhang, Long Tian, Chaojie Wang, Xinrun Wang, Borje F. Karlsson, Bo An, Shuicheng Yan, and Zongqing Lu. Cradle: Empowering foundation agents towards general computer control, 2024. URL https://arxiv.org/abs/2403.03186.
* Wu et al. (2024) Zhiyong Wu, Chengcheng Han, Zichen Ding, Zhenmin Weng, Zhoumianze Liu, Shunyu Yao, Tao Yu, and Lingpeng Kong. OS-Copilot: Towards generalist computer agents with self-improvement. _CoRR_, abs/2402.07456, 2024. doi: 10.48550/ARXIV.2402.07456. URL https://doi.org/10.48550/arXiv.2402.07456.
* Wu et al. (2020)Haiteng Zhao, Chang Ma, Guoyin Wang, Jing Su, Lingpeng Kong, Jingjing Xu, Zhi-Hong Deng, and Hongxia Yang. Empowering large language model agents through action learning. _CoRR_, abs/2402.15809, 2024. doi: 10.48550/ARXIV.2402.15809. URL https://doi.org/10.48550/arXiv.2402.15809.
* Chen et al. (2024) Zhenfang Chen, Rui Sun, Wenjun Liu, Yining Hong, and Chuang Gan. GENOME: Generative neuro-symbolic visual reasoning by growing and reusing modules. In _The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_. OpenReview.net, 2024. URL https://openreview.net/forum?id=MNShbDSxKH.
* Kuan et al. (2024) Chun-Yi Kuan, Chih-Kai Yang, Wei-Ping Huang, Ke-Han Lu, and Hung-yi Lee. Speech-Copilot: Leveraging large language models for speech processing via task decomposition, modularization, and program generation. _CoRR_, abs/2407.09886, 2024. doi: 10.48550/ARXIV.2407.09886. URL https://doi.org/10.48550/arXiv.2407.09886.
* Zhang et al. (2024) Min Zhang, Jianfeng He, Shuo Lei, Murong Yue, Linhan Wang, and Chang-Tien Lu. Can LLM find the green circle? investigation and human-guided tool manipulation for compositional generalization. In _IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2024, Seoul, Republic of Korea, April 14-19, 2024_, pages 11996-12000. IEEE, 2024b. doi: 10.1109/ICASSP48485.2024.10446355. URL https://doi.org/10.1109/ICASSP48485.2024.10446355.
* Grand et al. (2024) Gabriel Grand, Lionel Wong, Matthew Bowers, Theo X. Olausson, Muxin Liu, Joshua B. Tenenbaum, and Jacob Andreas. LILO: Learning interpretable libraries by compressing and documenting code. In _The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_. OpenReview.net, 2024. URL https://openreview.net/forum?id=TqYbAWKMTe.
* Rendell (1983) Larry A. Rendell. Toward a unified approach for conceptual knowledge acquisition. _AI Mag._, 4(4):19-27, 1983. URL https://ojs.aaai.org/index.php/aimagazine/article/view/413.
* Solomonoff (1964) Ray J. Solomonoff. A formal theory of inductive inference. Part I. _Inf. Control._, 7(1):1-22, 1964. doi: 10.1016/S0019-9958(64)90223-2. URL https://doi.org/10.1016/S0019-9958(64)90223-2.
* Bengio and Malkin (2024) Yoshua Bengio and Nikolay Malkin. Machine learning and information theory concepts towards an AI mathematician. _CoRR_, abs/2403.04571, 2024. doi: 10.48550/ARXIV.2403.04571. URL https://doi.org/10.48550/arXiv.2403.04571.
* Li et al. (2024) Zhaoyu Li, Jialiang Sun, Logan Murphy, Qidong Su, Zenan Li, Xian Zhang, Kaiyu Yang, and Xujie Si. A survey on deep learning for theorem proving. _CoRR_, abs/2404.09939, 2024. doi: 10.48550/ARXIV.2404.09939. URL https://doi.org/10.48550/arXiv.2404.09939.
* Zhou et al. (2024) Jin Peng Zhou, Yuhuai Wu, Qiyang Li, and Roger Baker Grosse. REFACTOR: Learning to extract theorems from proofs. In _The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024_. OpenReview.net, 2024. URL https://openreview.net/forum?id=fgKjiVrm6u.
* Stengel-Eskin et al. (2024) Elias Stengel-Eskin, Archiki Prasad, and Mohit Bansal. ReGAL: Refactoring programs to discover generalizable abstractions. In _Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024_. OpenReview.net, 2024. URL https://openreview.net/forum?id=FovMazXUpj.

## Appendix A Extended Related Work

Current LLM-based library learning systems tend to fall into two main camps: systems designed for general word problem solving, typically including mathematical reasoning and typically generating Python functions (e.g., Cai et al. (2024), Yuan et al. (2024), Wang et al. (2024b)), and agentic systemsdesigned to interact with a specific, complex environment (e.g., Wang et al. (2024); Tan et al. (2024); Wu et al. (2024); Zhang et al. (2024); Zhao et al. (2024)).

Generally, such systems access the library via in-context learning (ICL); some place the entire library in the context (Wang et al., 2024; Zhang et al., 2024), whereas others first use a semantic-similarity retrieval step to allow for larger libraries. Yuan et al. (2024) in particular uses a retrieval system that incorporates a LLM-generated description of the tool to be retrieved; LEGO-Prover behaves similarly by having several phases where the system alternates between proposing useful tools to be added to the library, attempting to create these tools, and possibly retrieving these tools.

These systems are typically bottom-up (iteratively developing a library over time), though a handful of top-down approaches exist. These top-down approaches instead decompose a high-level description of the tasks into reusable modules (Chen et al., 2024; Kuan et al., 2024; Zhao et al., 2024; Zhang et al., 2024); to the best of the authors' knowledge this approach is yet to be applied to mathematical reasoning.

These LLM-based systems typically attempt to produce reusable tools via ICL: prompting the LLM to generate "reusable functions". In comparison, an older family of library learning work (e.g., Dreamcoder (Ellis et al., 2021) and LILO (Grand et al., 2024)) instead frame library learning as a matter of compression. In principle a function that compresses a set of solutions must be broadly applicable, and in practice a high-level function reduces the symbolic search space for program induction. More generally, compression has been of long standing interest in the field of artificial intelligence. Rendell (1983) defined conceptual knowledge as the ability to compress a raw space of possibilities into useful classes, and there are long-standing connections between compression and inductive reasoning. Framing inductive reasoning as the task of capturing the underlying pattern in a provided substring for the purposes of prediction, Solomonoff (1964) formalized induction as Bayesian reasoning under a prior favouring low Kolmogorov complexity. In other words, formalizing the concept of Occam's razor - that the simplest solution, that which can be highly compressed into a short description, is more likely. For a recent treatise on the value of compression, specifically within the area of mathematical reasoning, see Bengio and Malkin (2024).

Turing our attention to mathematics, deep learning in general and LLMs in particular have found broad application in theorem proving (Li et al., 2024). Considering library learning specifically, a very closely related branch of work considers the problem of refactoring a collection of ground-truth solutions into reusable components. ATG (Lin et al., 2024) and REFACTOR (Zhou et al., 2024) train models to extract reusable formal lemmas from a provided set of ground-truth formal proofs. Similarly, ReGAL (Stengel-Eskin et al., 2024) refactors ground-truth Python solutions for the MATH dataset into a reusable library. These systems are valuable and may represent a better first step towards reusable knowledge, but their dependence on ground-truth solutions prevents them from being conventional library learning systems. In comparison, LEGO-Prover attempts to learn reusable lemmas and produce formal proofs from only formal problem statements, and informal natural language proofs - furthermore, Wang et al. (2024) demonstrated that the latter could be automatically generated by ChatGPT with only a small degradation in system performance.

## Appendix B Example of Verbatim Use versus Name Use by LEGO-Prover

Figure 2 is an example of verbatim use where an input lemma to the Prover is used verbatim in the outputted solution.

In contrast, Figure 3 is an example of name use, where the name of the input lemma appears in the solution. In this case, the contents of the lemma are similar, but have significant differences. Note that an instance of verbatim use would, necessarily, also be an instance of name use.

A lemma is reused \(N\) times if it is used \(N+1\) times - i.e., if the lemma is used in \(N+1\) solutions.

## Appendix C LEGO-Prover Solutions not Found by Reuse-Free Ablation

We performed two runs of the original model, in both cases it outperformed the ablation by solving one additional problem. We present the found proofs and input lemmas in Figures 4 and 6. For improved legibility, we also provide a typeset approximation in Figures 5 and 7. In addition to the 

## Proof

Figure 2: Example of verbatim reuse by the LEGO-Prover. The input lemma is reproduced exactly in the Proverâ€™s output.

Figure 3: Example of name reuse by the LEGO-Prover. Only the name of the input lemma needs to be reproduced exactly in the output. In this case, the body of the input lemma has been significantly adjusted. Note Figure 2 is also an example of name reuse, as the input lemmaâ€™s name appears in the solution (in that particular case, along with the rest of the lemma).

observations in the main paper, it should be noted that there is redundancy among the retrieved lemmas - deduplication and retrieval of lemmas remain areas for improvement.

## Appendix D TroVE MATH reproduction

See table 3 for the best-of-five accuracies reported by TroVE, and achieved by our reproduction of their results.

## Appendix E LEGO-Prover Hyperparameters and Experiment Details

At the time of publication, the LEGO-Prover logs released by Wang et al. (2024) and used in our analysis are available at https://github.com/wiio12/LEGO-Prover/blob/357672c7751cd0c84aff6bf72a3d1bf97614e81d/result/lego_result.zip.

LEGO-Prover is built on OpenAI's GPT-3.5-Turbo and the 2022 release of the Isabelle proof assistant, specifically using its abilities as a proof verifier. Note that due to the deprecation of the LLMs originally used by LEGO-Prover (gpt-3.5-turbo-0301, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-16k, gpt-3.5-turbo-16k-0613), we upgrade the underlying LLM from GPT-3.5-Turbo to GPT-4o-mini.

We use the default LEGO-Prover hyperparameters, except for the number of retry attempts which, following Wang et al. (2024)'s ablations, we reduce to 50. See Table 4 for details.

Note that the LEGO-Prover is initialized with a seed library of tools, and our ablation retains this initialization. The core claim we aim to disprove is that the model's performance gains predominantly come from reusable lemmas, and our ablation prevents any cross-task reuse.

The specific 12 problems chosen uniformly at random for our ablation study are: aime_1991_p6.json, algebra_2varlineareq_xpeq7_2xpeq3_eeq11_xeqn4.json, amc12a_2008_p15.json, amc12a_2013_p8.json, amc12a_2021_p7.json, amc12b_2002_p3.json, amc12b_2003_p9.json, mathd_algebra_31.json, mathd_algebra_109.json, mathd_algebra_116.json, mathd_numbertheory_149.json, and numbertheory_sqmod4in01d.json

Note that LEGO-Prover requires both the problem statement, and an informal natural language proof for conversion. We use the same human-generated informal proofs as Wang et al. (2024). The authors bundled said informal proofs inside of the miniF2F jjson files listed above, available for download from https://github.com/wiio12/LEGO-Prover/tree/

\begin{table}
\begin{tabular}{l l} \hline \hline Hyperparameter & value \\ \hline Solution attempts per problem (num\_attempts) & 50 \\ Number of Prover processes (num\_prover) & 3 \\ Number of Evolver processes (num\_evolver) & 8 \\ Temperature (temperature) & 0.7 \\ \hline \hline \end{tabular}
\end{table}
Table 4: LEGO-Prover hyperparameters

\begin{table}
\begin{tabular}{l l l l l} \hline \hline  & \multicolumn{5}{c}{Best-of-5 accuracy on MATH test split} \\ \cline{2-5} Model & count & geo & inte & num \\ \hline TroVE, Reported & **0.26** & **0.08** & 0.11 & 0.25 \\ TroVE Reproduced (ours) & 0.24 & 0.06 & 0.13 & 0.27 \\ TroVE, Reported Create-only ablation & 0.14 & 0.06 & 0.05 & 0.16 \\ No Reuse Ablation (ours) & 0.25 & 0.05 & **0.15** & **0.31** \\ \hline \hline \end{tabular}
\end{table}
Table 3: TroVE performance on MATH. For comparison with Wang et al. (2024), all reported numbers are best over 5 trials. Variation between trials arises from the stochastic sampling of the underlying LLM.

[MISSING_PAGE_EMPTY:13]

Figure 5: A typical approximation of LEGO-Prover input lemmas (left) and found proof (right). The proof proves that \(\forall k\in\mathbb{R}:\) if \(x=(13-\sqrt{131})/4\) and \(2x^{2}-13x+k=0\) then \(k=19/4\). See Figure 4 for the original Isabelle lemmas and proof. Note that skill 1 may have been indirectly used in rewriting \(2x^{2}-13x+k=0\) as \(k=13x-2x^{2}\). Skills 2, 3 and 4 do not seem to be used directly or indirectly; furthermore skills 2 and 4 are the same lemma. Their use of smt and metis may have encouraged the prover to use these same tools.

Input Lemmas

Heresome useful skill for reference:

``` ####useful skill1: ########lemmaonneg_square: fixesa::real shows"a2><ge>o"proof- have"a2=a*a"by(simod: power2_eq_square) thus?thesisby(simod:mult_nonneg_nonneg)qed ```

####useful skill2: ########lemmasquare_non_negative: fixesa::real shows"a2>=o" proof have"a2=a*a"by(simod: power2_eq_square) thus?thesisby(simodod:mult_nonneg_nonneg)qed ```

####useful skill3: ########lemmasquare_sum_nonneg: fixesa::real shows"a2>=o" by(simp)

``` ####useful skill4: #########lemmamin_sum_of_squares: fixesxy::real shows"x2+yx2>=o"proof- have"x2>=o"byauto ultimatelyhave"x2+yx2>=o"by(simp add:add_nonneg_nonneg) thus"x2+yx2>=o"bysimp qed ```

``` (*Lema:Theexpressioncanbeexpressedintermsofsquarestoshowitsnonnegativity.*) lemmaexpression_nonneg: fixesxy::real shows"((x*y)-1)^2+(x+y)^2>=1" proof- (*Step1:Define theexpressionwewanttominimizeas\E=(xy-1)^2+(x+y)^2).*) let%E="((x*y)-1)^2+(x+y)^2" (*Step2:Expandtheexpression\(E\):\((xy-1)^2+(x+y)^2)*) have"((x*y)-1)^2=(xx2*y^2-2*x*y+1)"bysoso have"x2+y^2*(x2-2*x^y+1)+(x+2+2*x^y+2) byso by(*Thus,wehave:\(E=x^2*y^2+x^2+y^2+y^2+1)*) have"x2>=o"byauto have"y2>=o"byauto (*Therefore,theminimumvalueof\(E\)) occurswheneachofthesetermsisminimized.*) (*Theminimumoccurswhen\(x=o\)and\(y=o\).*) have"x2>=1"bysoso (*Step4:Set\(\x=o\))and\(y=o\)choosetheminimum:\(\(E(0,0)=1\)).*) have"x2>=1"when"x=o"and"y=o" usingthatbyauto (*Step5:Concludethattheleastpossiblevalueof\(E\)is\(1\).*) thus?thesisbysoqed (*formalstatementcopyfromtheinput') theoremamc12a_2021_p7: fixesxy::real shows"a1<te>((x*y)-1)^2+(x+y)^2" apply(autosimp:algebra_simpspower2_eq_square) usingexpression_nonneg byso end ```

Figure 6: LEGO-Prover input lemmas (left) and found proof (right). The proof proves that \(\forall x,y\in\mathbb{R}:1\leq(xy-1)^2+(x+y)^2\). See Figure 7 for a typeset approximation, and commentary of LEGO-Proverâ€™s use (and failure to use) the input lemmas.

[MISSING_PAGE_FAIL:16]

same table is performed using the same version of Python, scipy 1.8.1, scipy.stats.ttest_ind(), with the settings equal_var=False and alternative='less'.

Our experiments were run on an internal cluster, running up to 4 trials at once. Each trial used 1 Nvidia A40 GPU, 64 GB of RAM, 16 CPU cores, and ran within 12 hours. Smaller datasets completed more quickly. We upper bound the total compute time required to run our TroVE experiments at 480 hours. The full project required more compute than the experiments reported as we also tried running TroVE with quantized CodeLlama, CodeLlama 13B and 70B, and GPT-4o-mini.

Our code is modified from the released TroVE code base, available at https://github.com/zorazrw/trove[Wang et al., 2024b], which was released under the CC-BY-SA-4.0 license. Evaluation is done using the MATH Hendrycks et al. [2021] dataset, available at https://github.com/hendrycks/math, which was released under an MIT License.

Our code is documented and released, alongside the generated TroVE logs. It is a minor modification to the existing code base, and there is no training stage or new limitations. The code is released under the same license as the parent repository.

### Additional TroVE experiments

We also ran baseline TroVE using the larger CodeLlama 13B model, and found similar results with very little direct function use. The key difference with the 7B model was that a single function was learned for the geometry split, but it was never reused in a correct solution.

We also attempted to run baseline TroVE using the 70B model, however we discarded the results as the LLM's ethical safeguards were frequently tripped (e.g., giving reasons such as "it is not appropriate or ethical to provide assistance with academic assignments or graded exercises").

\begin{table}
\begin{tabular}{l l} \hline \hline Hyperparameter & value \\ \hline Library trim frequency (trim\_steps) & 500 \\ Solution execution timeout in seconds (exec\_timeout) & 100 \\ top-p (top\_p) & 0.95 \\ Samples per prompt (num\_return\_sequences) & 5 \\ Temperature (temperature) & 0.6 \\ Max decode length (max\_new\_tokens) & 512 \\ \hline \hline \end{tabular}
\end{table}
Table 5: TroVE hyperparameters

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We analyze LEGO-Prover logs and ablate the model in Section 3, and we analyze the TroVE logs and ablate the model in Section 4. In both cases we find little direct reuse, and our ablation performs similarly. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 6. Primary limitations are scope (2 models and 2 datasets), and resource constraints on the ablations. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: This work is empirical. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Hyperparameters are reported in Appendices E and F, the TroVE and LEGO-Prover codebases are publicly available as are the MATH and miniF2F datasets, our ablations are described in Sections 3 and 4, and we release our code, logs, and log analysis code. As to the underlying LLMs, TroVE uses open source CodeLama, and our LEGO-Prover ablation runs on a much smaller dataset to reduce the OpenAI API costs. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: As explained in the previous question on reproducibility, we release our code along with the logs analyzed. Furthermore, the core TroVE and LEGO-Prover code bases are already publicly available, and can be easily modified to implement the ablations described. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Hyperparameters are in Sections E and F, there is no training data, and the TroVE test set is the same as Wang et al. (2024), and the LEGO-Prover test set a subset of that used in Wang et al. (2024). The exact problems used in the subset are listed in the same section as the hyperparameters. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For the LEGO-Prover ablation, error regions of 1 standard deviation are displayed in Figure 1, the caption states that the source of variation is the LLM output and race conditions within the system; the method used to compute mean and standard deviation (numpy) is stated in Appendix E. For the TroVE ablation, we report the mean and standard deviation in Table 2. The best-of-five accuracy is reported in the Appendix, Table 3) so that our values are comparable to those reported in Wang et al. (2024). Both tables state that variation arises from sampling from the LLM. The method used to compute mean and standard deviation (numpy) is stated in Appendix F. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Outlined in Appendix E for the LEGO-Prover experiments, and Appendix F for the TroVE experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: There are no human subjects, to the best of our knowledge there are no data concerns, or immediate societal impact or harms (the possible future risks from deploying tool-learning systems, and the precautions that should be taken in future research in self-improving systems are outlined in Section 6), and to the best of our knowledge our work is reproducible and legal. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We do not anticipate any immediate societal impact or harms, but we do discuss the possible future risks from deploying tool-learning systems, and the precautions that should be taken in future research in self-improving systems in Section 6. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We present ablations of already publicly available models (LEGO-Prover and TroVE), neither of which we believe has a higher risk for misuse than the constituent publicly available LLM. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets**Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The creators of TroVE [Wang et al., 2024b], LEGO-Prover [Wang et al., 2024a], the MATH dataset [Hendrycks et al., 2021], and miniF2F [Zheng et al., 2022] are all cited in the abstract. The URLs and licenses are stated in Appendices E and F. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Our code is documented and released, alongside the log files used in our analysis. As new assets are minor modifications to existing code bases with no training or new limitations, we simply state as much in Appendices E and F; the code will be released under the same license as the parent repositories. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing or human subjects was done. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.

* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: There were no human study participants. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.