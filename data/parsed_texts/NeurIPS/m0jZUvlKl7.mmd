# AR-Pro: Counterfactual Explanations for Anomaly Repair with Formal Properties

 Xiayan Ji  Anton Xue  Eric Wong Oleg Sokolsky Insup Lee

Department of Computer and Information Science

University of Pennsylvania

Philadelphia, PA 19104

{xjiae,antonxue,exwong,sokolsky,lee}@seas.upenn.edu

Equal contribution.

###### Abstract

Anomaly detection is widely used for identifying critical errors and suspicious behaviors, but current methods lack interpretability. We leverage common properties of existing methods and recent advances in generative models to introduce counterfactual explanations for anomaly detection. Given an input, we generate its counterfactual as a diffusion-based repair that shows what a non-anomalous version _should have looked like_. A key advantage of this approach is that it enables a domain-independent formal specification of explainability desiderata, offering a unified framework for generating and evaluating explanations. We demonstrate the effectiveness of our anomaly explainability framework, AR-Pro, on vision (MVTec, VisA) and time-series (SWaT, WADI, HAI) anomaly datasets. The code used for the experiments is accessible at: https://github.com/xjiae/arpro.

## 1 Introduction

Anomaly detectors measure how much their inputs deviate from an established norm, where too much deviation implies an instance that warrants closer inspection [15, 52]. For example, unusual network traffic may indicate potential malicious attacks that necessitate a review of security logs [16, 30], irregular sensor readings in civil engineering may suggest structural weaknesses [40, 49, 53], and atypical financial transactions point to potential fraud [3, 47]. As a benign occurrence, unexpected anomalies in scientific data can also lead to new insights and discoveries [25, 45].

Although state-of-the-art anomaly detectors are good at catching anomalies, they often rely on black-box models. This opacity undermines reliability: inexperienced users might over-rely on the model without understanding its rationale, while experts may not trust model predictions that are not backed by well-founded explanations. This limitation of common machine learning techniques has led to growing interest in model explainability [12], particularly in domains such as medicine [50] and law [14]. We refer to [36, 44] and the references therein for recent surveys on explainability.

While many anomaly detection methods can localize which parts of the input are anomalous, this may not be a satisfactory explanation, especially when the data is complex. In medicine [20], heart sound recordings [17] and EEG brain data [5] can differ greatly between individuals; in industrial manufacturing, it can be hard to understand subtle defects of PCB for inexperienced workers [73]. Thus, even when the location of the anomaly is known, it may be hard to articulate _why_ it is anomalous. In such cases, it can be helpful to ask the **counterfactual** question: "What _should_ a non-anomalous version look like?" For example, a doctor might ask what changes in a patient's chest X-ray might improve diagnostic outcomes [58], while a quality assurance engineer may ask what changes would fix the defect [6].

To answer such counterfactual questions, we begin with two observations. First, anomalies are commonly localized to small regions of the input [13; 38; 73]. Second, recent generative models, e.g., diffusion [24] models, can be trained to produce high-quality, non-anomalous examples. These observations motivate us to _repair anomalies_ as a _counterfactual explanation_. However, simply generating a repair is not sufficient; we must also ensure its quality. For example, it may be undesirable for a repair to significantly improve the anomaly score but barely resemble the original input. Therefore, it is important to measure the quality of repairs using _formal properties_.

However, formalizing broadly applicable properties is challenging, as different domains (e.g., vision, time series) and tasks (e.g., manufacturing, security) have unique considerations. To overcome this, we further observe that many anomaly detectors in practice [4] satisfy a condition that we call _linear decomposability_: the overall anomaly score is an aggregation of feature-wise anomaly scores. Importantly, this is a strong but common condition with which we may formalize the desiderata of counterfactual explanations. Conveniently, many of these desiderata are, in fact, domain-independent. We leverage these conditions to formalize a general, domain-independent framework for counterfactual anomaly explanation: we use a generative model to produce a repair and then evaluate this repair with respect to the detector model.

We present an overview of our anomaly explainability framework, AR-Pro, in Figure 1. While anomaly repair [21; 31; 70] and counterfactual explanations [56] have been explored in the literature, we are the first to study this in a unified context. We summarize our contributions as follows:

* We observe that many anomaly detectors satisfy _linear decomposability_ and use this condition to define general, domain-independent properties for counterfactual explanations. This approach lets us measure explanation quality with respect to the anomaly detector, as shown in Figure 1.
* We use these properties to guide diffusion models towards a high-quality repair of the anomalous input. Such a repair serves as the counterfactual explanation of the anomaly.
* Our framework, AR-Pro, can produce semantically meaningful repairs that outperform off-the-shelf diffusion models with respect to our explainability criteria. Our vision anomaly benchmarks include MVTec [13] and VisA [73]. Our time-series anomaly benchmarks include SWaT [38], HAI [54], and WADI [2].

## 2 Overview and Formal Properties

In Section 2.1, we first observe that many anomaly detector paradigms are _linearly decomposable_. Intuitively, this condition means that the overall anomaly score is an aggregation of feature-wise anomaly scores. Next, we use linear decomposability in Section 2.2 to formalize general, domain-independent properties of counterfactual explanations. We will use the terms _counterfactual_, _explanation_, and _repair_ interchangeably throughout this paper.

Figure 1: Overview of the AR-Pro framework. We first identify an inputâ€™s anomalous region and then use property-guided diffusion to repair it. This repair is the counterfactual anomaly explanation, where the following properties are defined with respect to a _linearly decomposable_ anomaly detector (AD). (_Overall Improvement_) The repair has a lower anomaly score. (_Similarity_) The repair should resemble the original. (_Localized Improvement_) The score over the repaired region should improve. (_Non-degradation_) The score over the non-anomalous region should not significantly worsen.

### Common Anomaly Detectors are Linearly Decomposable

Many anomaly detection techniques use a scoring function \(s:\mathbb{R}^{n}\rightarrow\mathbb{R}\) to measure the anomalousness an input \(x\in\mathbb{R}^{n}\), where \(s(x)\) is called the _anomaly score_ of \(x\). This is commonly done in a two-stage process: the _feature-wise anomaly scores_\(\alpha_{1}(x),\ldots,\alpha_{n}(x)\in\mathbb{R}\) are first computed and then aggregated [15]. We observe that this aggregation often satisfies the following form:

**Definition 2.1** (Linear Decomposition).: The feature-wise anomaly scores \(\alpha:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\) and regularizer \(\beta:\mathbb{R}^{n}\rightarrow\mathbb{R}\)_linearly decomposes_ the anomaly score \(s:\mathbb{R}^{n}\rightarrow\mathbb{R}\) if for all \(x\in\mathbb{R}^{n}\):

\[s(x)=\alpha_{1}(x)+\cdots+\alpha_{n}(x)+\beta(x).\]

While linear decomposability appears to be a strong assumption, it is common in practice. We show a number of examples below, where we also flexibly refer to \(\alpha:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\) as the _feature scores_.

**Example 2.2**.: In reconstruction-based anomaly detection [8; 29], the input \(x\) is passed through an encoder-decoder architecture to generate a _reconstruction_\(\hat{x}\). The motivation is that it should be harder to reconstruct out-of-distribution (anomalous) inputs. Empirically, it is observed that if \(x\) is anomalous, then \(\hat{x}\) will have a large reconstruction error when feature \(i\) is in the anomalous region, e.g., a pixel in the defect area of a manufacturing artifact image. A typical example of such an anomaly score is:

\[s(x)=|\hat{x}_{1}-x_{1}|^{2}+\cdots+|\hat{x}_{n}-x_{n}|^{2},\]

which is also known as the \(\ell^{2}\) reconstruction error. This lets us define the feature-wise anomaly scores by \(\alpha_{i}(x)=|\hat{x}_{i}-x_{i}|^{2}\), and we illustrate this example in Figure 2.

**Example 2.3**.: In maximum likelihood-based anomaly detection [15], one measures the likelihood of a test input \(x\) with respect to a set of non-anomalous training examples. Intuitively, an out-of-distribution (anomalous) \(x\) should be unlikely with respect to the non-anomalous training examples and will thus have a lower likelihood. In variants such as normalizing flow-based anomaly detection for images [66], it is common to define a joint probability distribution over all the features:

\[s(x)=-\big{[}\log p_{1}(x)+\cdots+\log p_{n}(x)+\log\lvert\det J(x)\rvert \big{]},\]

where \(p_{1}(x),\ldots,p_{n}(x)\) are the probabilities of each feature that lets us define \(\alpha_{i}(x)=-\log p_{i}(x)\), while the change-of-variable Jacobian \(\log\lvert\det J(x)\rvert\) may be viewed as a regularization term.

**Example 2.4**.: In language modeling [61], it is common to measure the anomaly of a token sequence based on the likelihood of each token [7]. Although similar to the vision case, the standard formulation for language models is different: given a token sequence \(x_{1},\ldots,x_{n}\in\{1,\ldots,\texttt{vocab\_size}\}\), its measure of unlikeliness (anomalousness) may be defined as:

\[s(x)=-\frac{1}{n}\big{[}\log p(x_{1})+\log p(x_{2}|x_{1})+\log p(x_{3}|x_{1},x _{2})+\cdots+\log p(x_{n}|x_{1},\ldots,x_{n-1})\big{]},\]

where \(p\) is a probabilistic generative model, and so \(\alpha_{i}(x)=-(1/n)\log p(x_{i}|x_{1},\ldots,x_{i-1})\). This is also known as a _preplexity measure_, which has been used to detect jailbreak against LLMs [64].

Beyond the above examples, anomaly detectors for time-series data [29; 60; 63] and text [35] also commonly use this convention. We further remark that the feature-wise anomaly scores \(\alpha\) are related to _feature attribution scores_ in explainability literature [33; 51; 55].

Figure 2: Reconstruction-based anomaly detection exemplifies linear decomposition. The anomalous input \(x\in\mathbb{R}^{n}\) is first reconstructed into \(\hat{x}\in\mathbb{R}^{n}\), and the feature-wise anomaly scores are given by \(\alpha_{i}(x)=|\hat{x}_{i}-x_{i}|^{2}\in\mathbb{R}^{n}\) for \(i=1,\ldots,n\). Then, the standard \(\ell^{2}\) reconstruction-based anomaly score is a linear combination of the feature scores: \(s(x)=\alpha_{1}(x)+\cdots+\alpha_{n}(x)\).

### Formal Properties for Counterfactual Explanations

We now present the formal properties of counterfactual explanations. We will assume a given anomaly score function \(s\) that is linearly decomposed by the feature-wise score function \(\alpha\) and regularizer \(\beta\). Given some input \(x\), it is common to convert \(\alpha(x)\in\mathbb{R}^{n}\) into a binary-valued vector \(\omega(x)\in\{0,1\}^{n}\) to classify which input feature is anomalous, and this is commonly done by a threshold:

\[\omega(x)=(\alpha_{1}(x)\geq\tau_{1},\ldots,\alpha_{n}(x)\geq\tau_{n}),\quad \text{for some feature-wise threshold }\tau\in\mathbb{R}^{n}.\]

A binarization of the feature-wise scores suggests the need for region-specific anomaly scores, which we implement with the following indexing scheme on \(s(x)\):

\[s_{z}(x)=\beta(x)+\sum_{i:z_{i}=1}\alpha_{i}(x),\quad\text{for all }z\in\{0,1\}^{n}.\]

Then, the anomalous and non-anomalous regions have scores \(s_{\omega(x)}(x)\) and \(s_{\odot(x)}(x)\), respectively, where \(\overline{\omega}(x)=1-\omega(x)\) denotes non-anomalous region. We next enumerate some common desiderata of counterfactual explanations, where we will refer to the anomalous input as \(x_{\text{bad}}\) and the repaired version as \(x_{\text{fix}}\).

**Property 1** (Overall Improvement): The anomaly score should improve.: Because a "repaired" version should fix the anomaly by definition, one would reasonably expect that:

\[s(x_{\text{fix}})<s(x_{\text{bad}}).\] (P1)

**Property 2** (Similarity): The repair should resemble the original.: When \(s\) is generated by a complex machine-learning model, it may be the case that it has a value \(x_{\text{fix}}\) where \(s(x_{\text{fix}})\ll s(x_{\text{bad}})\), but \(x_{\text{fix}}\) and \(x_{\text{bad}}\) bear little resemblance. In the case of vision models, \(x_{\text{fix}}\) may even resemble static noise. Such extreme dissimilarities between \(x_{\text{fix}}\) and \(x_{\text{bad}}\) are not desirable because a user cannot be expected to feasibly interpret this information. Thus, we desire a similarity condition as:

\[\overline{\omega}(x_{\text{bad}})\odot x_{\text{bad}}\approx\overline{\omega} (x_{\text{bad}})\odot x_{\text{fix}},\] (P2)

where \(\odot\) denotes element-wise vector multiplication. This similarity condition states that the non-anomalous regions of the original and the repair, as given by \(\omega(x_{\text{bad}})\), should remain similar.

**Property 3** (Localized Improvement): The anomalous region should improve.: However, P1 and P2 are not sufficient. For example, one might have \(s(x_{\text{fix}})<s(x_{\text{bad}})\), but have a higher score on the anomalous region \(\omega(x_{\text{bad}})\). This is not desirable because it means that \(x_{\text{fix}}\) has not actually fixed the anomalous region of \(x_{\text{bad}}\). To ensure progress, we would like:

\[s_{\omega(x_{\text{bad}})}(x_{\text{fix}})<s_{\omega(x_{\text{bad}})}(x_{ \text{bad}}).\] (P3)

**Property 4** (Non-degradation): The non-anomalous region should not significantly worsen.: Even when the above properties are satisfied, it is possible that the proposed repair could inadvertently increase the anomaly score on the non-anomalous region \(\overline{\omega}(x_{\text{bad}})\). This would mean repairing the anomalous region at the cost of corrupting the non-anomalous parts. We thus state the property against this as follows, where \(\delta_{4}>0\) is a given tolerance threshold:

\[s_{\overline{\omega}(x_{\text{bad}})}(x_{\text{fix}})\leq s_{\overline{\omega} (x_{\text{bad}})}(x_{\text{bad}})+\delta_{4}.\] (P4)

The benefit of our above formulation is that it encapsulates general, domain-independent desiderata of anomaly repairs. Importantly, this is achieved under the mild assumption of a linearly decomposable anomaly detector. We comment that some overlap among our proposed formal properties may arise in certain scenarios, and alternative sets could be more tailored for specific applications. Our goal, however, is to provide a foundational set of properties that ensures broad applicability, allowing further customization to suit individual application needs.

## 3 Property-guided Generation of Counterfactual Explanations

We now outline the process of conducting a property-guided repair for an anomalous input. In Section 3.1, we first introduce the generalized setup, where we define the four previously outlined properties as objective functions and frame the problem using risk-constrained optimization. Although this formulation clarifies the objectives, it is generally intractable. Therefore, in Section 3.2, we propose a diffusion-based algorithm to approximate the solution and achieve high-quality repairs.

### A Generalized Formulation with Properties as Loss Functions

We first present a generalized setup for generating repairs. We conceptualize this in terms of a repair model \(\mathcal{R}_{s,\omega}\) parametrized by an anomaly score function \(s:\mathbb{R}^{n}\rightarrow\mathbb{R}\) with known linear decompositions \(\alpha,\beta\) and a feature-wise binarization \(\omega\). To generate a repair, we sample the model:

\[x_{\text{fix}}\sim\mathcal{R}_{s,\omega}(x_{\text{bad}}),\]

where a probabilistic formulation is relevant in the context of variational auto-encoders [27] or diffusion models [24]. However, it is important that \(x_{\text{fix}}\) obeys the formal properties as outlined in Section 2.2. To do this, we cast these properties as loss functions listed below:

\[L_{1} =s(x_{\text{fix}})\] (P1 loss) \[L_{2} =\left\lVert\overline{\omega}(x_{\text{bad}})\odot(x_{\text{ fix}}-x_{\text{bad}})\right\rVert_{2}\] (P2 loss) \[L_{3} =\max\bigl{\{}0,\,s_{\omega(x_{\text{bad}})}(x_{\text{fix}})-s_{ \omega(x_{\text{bad}})}(x_{\text{bad}})\bigr{\}}\] (P3 loss) \[L_{4} =\max\bigl{\{}0,\,s_{\overline{\omega}(x_{\text{bad}})}(x_{\text {fix}})-s_{\overline{\omega}(x_{\text{bad}})}(x_{\text{bad}})-\delta_{4}\bigr{\}}\] (P4 loss)

Our rationale is as follows. First, because the primary objective of anomaly repair is to reduce the anomaly score, we set \(L_{1}\) as simply the score of \(x_{\text{fix}}\). Second, we would like to ensure that \(x_{\text{fix}}\) and \(x_{\text{bad}}\) are similar in the non-anomalous region, and so formulate \(L_{2}\) as the \(\ell^{2}\) distance between \(x_{\text{fix}}\) and \(x_{\text{bad}}\) over \(\overline{\omega}(x_{\text{bad}})\). Third, we formulate \(L_{3}\) to apply a penalty when the anomalous region degrades in performance. Fourth, we allow for a degradation of score in the non-anomalous region \(\overline{\omega}(x_{\text{bad}})\), up to some tolerance threshold \(\delta_{4}\). We cast these as a risk-constrained optimization problem as follows:

\[\underset{\theta}{\text{minimize}} \operatorname*{\mathbb{E}}_{x_{\text{fix}}\sim\mathcal{R}_{s,\omega }(x_{\text{bad}};\theta)}s(x_{\text{fix}})\] (1) subject to \[\operatorname*{\mathbb{P}}_{x_{\text{fix}}\sim\mathcal{R}_{s, \omega}(x_{\text{bad}};\theta)}\bigl{[}L_{2}\leq\delta_{2},\,\,\,L_{3}\leq 0,\,\,\,L_{4}\leq 0\bigr{]}\geq 1-\delta\]

where \(\theta\) is a parameter of the repair model, \(\delta_{2}>0\) is a given threshold for \(L_{2}\) and \(\delta>0\) is a given failure probability for the violation of at least one of (P2), (P3), or (P4). We acknowledge that multiple formulations for property-based losses are valid; however, the chosen approach is optimally suited to our context.

### Formal Property-guided Diffusion

We adopt techniques from guided diffusion [19] to generate repairs \(x_{\text{fix}}\). We first give a brief overview of a standard diffusion process and then adapt it to perform property-guided generation. We refer to [62] for a comprehensive guide on diffusion but attempt to make the exposition self-contained.

**Background.** A basic variant of diffusion models takes the form:

\[p(x_{t-1}|x_{t})=\mathcal{N}(\mu_{\theta}(x_{t},t),b_{t}^{2}I),\quad\text{for }t=T,\ldots,1,\] (2)

where \(\mu_{\theta}\) is the _denoising model_ with parameters \(\theta\), and \(b_{1}<\cdots<b_{T}\) is the variance schedule. When \(\mu_{\theta}\) is trained on non-anomalous data (see [62] for training details), one can generate non-anomalous samples of the training distribution by running the following iterative process:

\[x_{T}\sim\mathcal{N}(0,I),\quad x_{t-1}=\mu_{\theta}(x_{t},t)+b_{t}z_{t},\quad z _{t}\sim\mathcal{N}(0,I),\quad\text{for }t=T,\ldots,1,\] (3)

where \(x_{T}\) is the initial noise and \(x_{0}\) is the output sample. The idea is to repeatedly remove noise from a Gaussian \(x_{T}\) using \(\mu_{\theta}\) until the final \(x_{0}\) resembles a high-resolution image without defects, for instance. The iterations (3) is also known as _backward process_.

**Property-guided Diffusion.** We next show how to adapt the denoising iterations (3) to produce repairs. We use two main ideas: first, we use guidance [19] to slightly nudge the iterates \(x_{t-1}\) of (3)

Figure 3: We run property-guided diffusion with masked in-filling.

at every step using a property-based loss, to encourage that the final iterate \(x_{0}\), which we take to be \(x_{\text{fix}}\), is more amenable to our properties. Second, we used masked-infilling [39] to ensure that the non-anomalous region \(\overline{\omega}(x_{\text{bad}})\) is generally preserved by the iterates. We implement this modified iteration as follows: beginning from the initial noise \(x_{\text{fix},T}\sim\mathcal{N}(0,I)\), let:

\[\hat{x}_{\text{fix},t-1} =\underbrace{\mu_{\theta}(x_{\text{fix},t},t)+b_{t}z_{t}}_{\text {Denoising step}}-\underbrace{\eta_{t}\nabla L(x_{\text{fix},t})}_{\text{ Guidance term}},\quad z_{t}\sim\mathcal{N}(0,I),\] (4) \[x_{\text{bad},t} =\sqrt{a_{t}}x_{\text{bad}}+\sqrt{1-a_{t}}\epsilon_{t},\quad \epsilon_{t}\sim\mathcal{N}(0,I),\] (5) \[x_{\text{fix},t-1} =\overline{\omega}(x_{\text{bad}})\odot x_{\text{bad},t}+\omega (x_{\text{bad}})\odot\hat{x}_{\text{fix},t-1},\] (6)

for \(t=T,\dots,1\), where \(a_{t}=\prod_{i=1}^{t}(1-b_{i})\) and \(\eta_{1}<\dots<\eta_{T}\) is the guidance schedule. The property-based loss is given by:

\[L(x_{\text{fix},t})=\lambda_{1}L_{1}+\lambda_{2}L_{2}+\lambda_{3}L_{3}+\lambda _{4}L_{4},\] (7)

with \(L_{1},L_{2},L_{3},L_{4}\) as in Section 3.1 and weights \(\lambda_{1},\lambda_{2},\lambda_{3},\lambda_{4}>0\). In the above, (4) first generates \(\hat{x}_{\text{fix},t-1}\) from \(x_{\text{fix},t}\) by combining a standard denoising step with a guidance term. Then, (6) combines \(\hat{x}_{\text{fix},t-1}\) with \(\hat{x}_{\text{bad},t}\), from (5), in a masked-infilling operation [39] to yield \(x_{\text{fix},t-1}\). This masked in-filling ensures similarity between \(x_{\text{bad}}\) and \(x_{\text{fix}}\) (i.e., \(x_{\text{fix},0}\)) over the non-anomalous region \(\overline{\omega}(x_{\text{bad}})\).

We emphasize that the diffusion iterations given by (4), (5), (6), and do not guarantee the satisfaction of our formal properties. Rather, these iterations tend toward an output that better respects these properties -- as we later shown in our experiments. In particular, our property-based losses define a way to evaluate the quality to which each property is satisfied or violated.

## 4 Experiments

Our experiments evaluate the performance of AR-Pro across vision and time-series datasets. In particular, we aim to address the following research questions:

* **(RQ1) Empirical Validation**: How well does AR-Pro repair anomalies for different domains? In particular, we investigate how well the four properties are satisfied when the diffusion process is guided or unguided, as in the baseline.
* **(RQ2) Ablation Study**: How do the different hyper-parameters affect the repair quality? We focus on the weights \(\lambda_{1},\lambda_{2},\lambda_{3},\lambda_{4}\) for the four property-based losses.

**Vision Anomaly Models and Datasets.** For anomaly detectors, we used the anomalib [4] implementation of Fastflow [66] (with ResNet-50-2 backbone [68]) and Efficient-AD [11]. For datasets, we used the VisA [73] and MVTec-AD [13] datasets. VisA and MVTec-AD involve anomaly detection in the context of industrial manufacturing, where VisA consists of 12 image classes, and MVTec-AD consists of 15 image classes. Both FastFlow and Efficient-AD were trained with AdamW and a learning rate of \(10^{-4}\) until convergence.

**Time-series Anomaly Datasets and Models.** For anomaly detectors, we used the GPT-2 [48] and Llama2 [59] architectures for time-series anomaly detection. In particular, we use only the first 6 layers of GPT-2 and the first 4 layers of Llama-2 (with an embedding dimension of 1024) to accelerate training. For datasets, we used the SWat (51 features) [38], HAI (86 features) [54], and WADI (127 features) [2] datasets, split into sliding windows of size 100. Both our versions of GPT-2 and Llama-2 were trained with AdamW and a learning rate of \(10^{-5}\) until convergence.

**Diffusion-based Repair Models.** We used the HuggingFace implementation of DDPM [24] for vision data and Diffusion-TS [67] for time-series data. Both models were trained on the non-anomalous instances of their respective datasets using AdamW and a learning rate of \(10^{-4}\) until convergence.

**Evaluation Metrics.** We use the four property-based loss functions defined in Section 3.1 as our evaluation metrics. In particular, we measure the improvement of property-guided diffusion over un-guided diffusion. We adapt these metrics below, where we write \(\omega\) to mean \(\omega(x_{\text{bad}})\) for brevity:

* **Property 1 (Overall Improvement):**\(M_{s}\equiv s(x_{\text{fix}})\).
* **Property 2 (Similarity):**\(M_{d}\equiv\|\overline{\omega}\odot(x_{\text{fix}}-x_{\text{bad}})\|_{2}\)
* **Property 3 (Localized Improvement):**\(M_{\omega}\equiv s_{\omega}(x_{\text{fix}})-s_{\omega}(x_{\text{bad}})\)
* **Property 4 (Non-degradation):**\(M_{\omega}\equiv s_{\overline{\omega}}(x_{\text{fix}})-s_{\overline{\omega}}(x_{\text{bad}})\)

[MISSING_PAGE_FAIL:7]

example in Figure 4, we observe that for categories PCB 1, PCB 2, PCB 3, and PCB 4, the baseline fails to rigorously repair the anomaly: the orientation of the board is reversed for PCB 1; unintended white marks appear on the lower left of PCB 2; the dark lines in the potentiometer change for PCB 3; and the "FC-75" label is missing for PCB 4. However, by integrating formal property guidance, our approach accurately reconstructs all these details while effectively removing the anomalies. For the MVTec-AD examples, AR-Pro produces repairs that more closely resemble the original inputs compared to those generated by the baseline. This demonstrates that our method's generated repairs adhere more rigorously to the formal properties.

In Figure 6, we present examples from our time-series repair, where the anomalous time segment is shaded in red. Our results demonstrate that AR-Pro generates a signal that resembles the original signal better, as shown in the first two plots of Figure 6. In addition, we recover the sensor time series to non-anomalous values when the baseline fails to repair the anomaly, as shown in the last two plots of Figure 6.

More repair examples are available in Appendix D. However, although the quality of the generated repairs has improved, we notice that this enhancement comes with a trade-off of increased inference time. Further details can be found in Appendix E.

### (RQ2) Ablation Study

We randomly sampled 100 instances to compute the mean of each metric in order to evaluate the effect of hyper-parameters \(\lambda_{1},\lambda_{2},\lambda_{3},\lambda_{4}\) associated with each property-based loss. Each line in the plots represents results obtained while keeping the other hyper-parameters at 1.0. The ablation results for the time series are presented in Figure 7, with additional plots in Appendix F.

\begin{table}
\begin{tabular}{c|c|c|c c|c c|c c} \hline \multirow{2}{*}{Model} & \multirow{2}{*}{Dataset} & \multicolumn{2}{c|}{\(M_{d}(\downarrow)\)} & \multicolumn{2}{c}{\(M_{d}(\downarrow)\)} & \multicolumn{2}{c}{\(M_{d}(\downarrow)\)} & \multicolumn{2}{c}{\(M_{d}(\downarrow)\)} \\ \cline{3-10}  & & Baseline & Guided & Baseline & Guided & Baseline & Guided & Baseline & Guided \\ \hline \multirow{4}{*}{**Llam2**} & SWAT & 0.83 \(\pm\) 0.05 & 0.99 \(\pm\) 0.04 & 3.05 \(\pm\) 0.61 & 7.25 \(\pm\) 2.50 & 0.084 \(\pm\) 0.026 & 0.026 \(\pm\) 0.008 & 0.19 \(\pm\) 0.02 & 0.05 \(\pm\) 0.012 \\  & WADI & 0.97 \(\pm\) 0.00 & 0.26 \(\pm\) 0.01 & 0.98 \(\pm\) 0.01 & 0.00 \(\pm\) 0.00 & 0.087 \(\pm\) 0.005 & 0.000 \(\pm\) 0.000 & 0.61 \(\pm\) 0.01 & 0.00 \(\pm\) 0.000 \\  & HAI & 0.92 \(\pm\) 0.00 & 0.58 \(\pm\) 0.01 & 0.75 \(\pm\) 0.00 & 0.00 \(\pm\) 0.00 & 0.166 \(\pm\) 0.008 & 0.000 \(\pm\) 0.000 & 0.18 \(\pm\) 0.01 & 0.00 \(\pm\) 0.000 \\ \hline \multirow{4}{*}{**GPT-2**} & \(\Delta(\uparrow)\) & **-46.36\%** & **-42.77\%** & **-410.32\%** & **-491.23\%** & **+91.23\%** \\ \cline{2-10}  & SWAT & 0.68 \(\pm\) 0.06 & 0.57 \(\pm\) 0.05 & 12.81 \(\pm\) 15.82 & 16.28 \(\pm\) 16.30 & -0.029 \(\pm\) 0.041 & -0.094 \(\pm\) 0.038 & 0.13 \(\pm\) 0.04 & 0.09 \(\pm\) 0.031 \\  & WAID & 0.71 \(\pm\) 0.04 & 0.32 \(\pm\) 0.00 & 34.45 \(\pm\) 0.06 & 0.42 \(\pm\) 0.94 & 0.018 \(\pm\) 0.003 & 0.019 \(\pm\) 0.002 & 0.43 \(\pm\) 0.04 & 0.07 \(\pm\) 0.002 \\  & HAI & 0.92 \(\pm\) 0.02 & 0.61 \(\pm\) 0.11 & 3.69 \(\pm\) 9.72 & 5.67 \(\pm\) 18.80 & 0.136 \(\pm\) 0.050 & -0.005 \(\pm\) 0.018 & 0.21 \(\pm\) 0.03 & 0.01 \(\pm\) 0.127 \\ \hline \multirow{4}{*}{**GPT-2**} & \(\Delta(\uparrow)\) & **+34.93\%** & **-34.37\%** & **+177.79\%** & **+37.24\%** \\ \cline{2-10}  & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \\ \end{tabular}
\end{table}
Table 2: Comparison of baseline and guided performance across four metrics for SWAT, WADI, and HAI dataset categories with Llama2 and GPT2 model. \(\Delta\) is the median improvement percentage of the guided result from baseline.

\begin{table}
\begin{tabular}{c c c c c c} \hline Dataset & Category & Baseline TNR (\(\uparrow\)) & Guided TNR (\(\uparrow\)) & Category & Baseline TNR (\(\uparrow\)) & Guided TNR (\(\uparrow\)) \\ \hline \multirow{4}{*}{VisA} & Candle & **1.00** & **1.00** & Fryum & 0.66 & **1.00** \\  & Capsules & 0.00 & **1.00** & Pipe Fryum & **1.00** & **1.00** \\  & Cashew & **1.00** & **1.00** & PCB 1 & **1.00** & 0.96 \\  & Chewinggum & **1.00** & **1.00** & PCB 2 & 1.00 & **1.00** \\  & Macaroni 1 & 0.11 & **1.00** & PCB 3 & 0.90 & **1.00** \\  & Macaroni 2 & 0.52 & **1.00** & PCB 4 & 0.18 & **1.00** \\ \hline \multirow{4}{*}{MVTec-AD} & Bottle & **1.00** & **1.00** & Grid & 0.00 & **1.00** \\  & Cable & **1.00** & **1.00** & Hazelnut & **1.00** & **1.00** \\  & Capsule & **1.00** & **1.00** & Leather & **1.00** & **1.00** \\  & Carpet & **1.00** & **1.00** & Metal Nut & **1.00** & **1.00** \\  & Pill & **1.00** & **1.00** & Screw & **1.00** & **1.00** \\  & Tile & **1.00** & **1.00** & Toothbrush & **1.00** & 0.96 \\  & Transistor & 0.56 & **1.00** & Wood & **1.00** & **1.00** \\  & Zipper & **1.00** & **1.00** & & \\ \hline \multirow{4}{*}{Overall} & VisA & 0.95 & **1.00** & MVTec-AD & **1.00** & **1.00** \\  & SWAT & 0.08 & **0.86** & HAI & 0.00 & **1.00** \\ \cline{1-1}  & WADI & 0.00 & **1.00** & & \\ \hline \end{tabular}
\end{table}
Table 3: True Negative Rate (TNR) for categories in the vision (VisA, MVTec) and time-series (SWaT, HAI, and WADI) anomaly detection datasets.

Our observations indicate that variations in the scale of property hyperparameters do not significantly impact the formal metrics, as the range of change remains relatively small. In addition, no consistent trends were observed when varying the \(\lambda_{1},\lambda_{2},\lambda_{3},\lambda_{4}\) hyper-parameters. This suggests that our framework demonstrates robust performance, and extensive tuning may be unnecessary.

## 5 Related Work

Numerous techniques have been developed for anomaly detection across various domains [3; 49]. Traditional approaches to anomaly detection include clustering [43; 57] and statistical methods [34] such as ARIMA [41] and Gaussian models [52]. However, these methods often struggle with high-dimensional data. More recently, deep learning-based anomaly detection [20; 28], including autoencoders [27] and GANs [18], can detect high-dimensional anomalies via reconstruction error. For time-series data, LSTMs [32] and transformer-based models [60; 63; 69; 72] have shown exceptional performance. Additionally, diffusion models are emerging as promising tools for visual anomaly detection [42; 71]. While these methods vary in strengths and are continually improving, explaining

Figure 4: The original input and ground truth anomaly mask are displayed in the first two columns. The baseline method fails to preserve close similarity to the input PCB boards, as highlighted in the third column. Guided vision repair examples in the fourth column address these deficiencies.

Figure 5: MVTec repairs with AR-Pro; better resemble the original compared to the baseline.

anomalies remains a challenge [10]. Most current methods rely on feature importance scores or visualizations [44], such as gradients or reconstructions [46], which often fail to provide actionable insights. The lack of formal frameworks [65] and consistent evaluation metrics [1] complicates this issue. For example, the absence of formal metrics leads to inconsistencies in evaluation [26], underscoring the need for more rigorous approaches and reliable criteria [37]. Most similar to our work is [56], which also performs time series-specific generation of counterfactual explanations in the form of anomaly repairs but considers a different set of properties and does not use diffusion. Our work also focuses on generative modeling to produce counterfactual explanations in the form of anomaly repairs. For vision, the current leading paradigms are diffusion models [24] and generative adversarial networks [23]. Diffusion models are also applicable to time-series data [67], and we refer to [22] for a survey on other techniques.

## 6 Discussion

The main theoretical contribution of our work is the identification of common counterfactual explanation desiderata for linearly decomposable anomaly detectors. While we have identified four formal properties, we acknowledge that other valid ones may also exist. Moreover, we recommend that practitioners evaluate and choose the properties necessary for the particular problem, and this is made possible by the form of our diffusion guidance function in (7). The quality of anomaly repairs depends on the performance of the anomaly detector and the generative model. While there may have been limitations in our efforts, we found it challenging to use variational auto-encoders [27] for generating high-quality repairs. Furthermore, our implementation is focused on diffusion models, but the ideas presented can also be extended to other generative techniques.

## 7 Conclusion

We present AR-Pro, a framework for generating and evaluating counterfactual explanations in anomaly detection. We use the fact that common anomaly detectors are linearly decomposable, which lets us define formal, general, domain-independent properties for explainability. Using these properties, we show how to generate high-quality counterfactuals using a property-guided diffusion setup. We demonstrate the effectiveness of AR-Pro on vision and time-series datasets and showcase our improvement over off-the-shelf diffusion models.

AcknowledgementThis work was supported in part by ARO MURI W911NF2010080, NSF-2125561, and NSF-2143274.

Figure 6: Original input is the blue line, property guided fix is the green line, and the baseline is the red line. The first image shows that the baseline generates a spurious signal when there is no anomaly. The second image shows that the baseline repairs the anomaly, but not as effectively as with guidance. The last two images show instances where the baseline fails to repair the anomaly.

Figure 7: Varying the hyper-parameters does significantly change \(M_{s}\), \(M_{d},M_{\omega}\), and \(M_{\overline{\omega}}\).

## References

* [1] C. Agarwal, S. Krishna, E. Saxena, M. Pawelczyk, N. Johnson, I. Puri, M. Zitnik, and H. Lakkaraju. Openxai: Towards a transparent evaluation of model explanations. _Advances in Neural Information Processing Systems_, 35:15784-15799, 2022.
* [2] C. M. Ahmed, V. R. Palleti, and A. P. Mathur. Wadi: a water distribution testbed for research in the design of secure cyber physical systems. In _Proceedings of the 3rd international workshop on cyber-physical systems for smart water networks_, pages 25-28, 2017.
* [3] M. Ahmed, A. N. Mahmood, and M. R. Islam. A survey of anomaly detection techniques in financial domain. _Future Generation Computer Systems_, 55:278-288, 2016.
* [4] S. Akcay, D. Ameln, A. Vaidya, B. Lakshmanan, N. Ahuja, and U. Genc. Anomalib: A deep learning library for anomaly detection. In _2022 IEEE International Conference on Image Processing (ICIP)_, pages 1706-1710. IEEE, 2022.
* [5] N. Alahmadi, S. A. Evdokimov, Y. Kropotov, A. M. Muller, and L. Jancke. Different resting state eeg features in children from switzerland and saudi arabia. _Frontiers in human neuroscience_, 10:559, 2016.
* [6] A. L. Alfeo and M. G. Cimino. Counterfactual-based feature importance for explainable regression of manufacturing production quality measure. In _ICPRAM_, pages 48-56, 2024.
* [7] G. Alon and M. Kamfonas. Detecting language model attacks with perplexity. _arXiv preprint arXiv:2308.14132_, 2023.
* [8] J. An and S. Cho. Variational autoencoder based anomaly detection using reconstruction probability. _Special lecture on IE_, 2(1):1-18, 2015.
* [9] A. N. Angelopoulos and S. Bates. A gentle introduction to conformal prediction and distribution-free uncertainty quantification. _arXiv preprint arXiv:2107.07511_, 2021.
* [10] L. Antwarg, R. M. Miller, B. Shapira, and L. Rokach. Explaining anomalies detected by autoencoders using shapley additive explanations. _Expert systems with applications_, 186:115736, 2021.
* [11] K. Batzner, L. Heckler, and R. Konig. Efficientad: Accurate visual anomaly detection at millisecond-level latencies. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pages 128-138, 2024.
* [12] V. Belle and I. Papantonis. Principles and practice of explainable machine learning. _Frontiers in big Data_, 4:688969, 2021.
* [13] P. Bergmann, M. Fauser, D. Sattlegger, and C. Steger. Mvtec ad-a comprehensive real-world dataset for unsupervised anomaly detection. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 9592-9600, 2019.
* [14] A. Bibal, M. Lognoul, A. De Streel, and B. Frenay. Legal requirements on explainability in machine learning. _Artificial Intelligence and Law_, 29:149-169, 2021.
* [15] V. Chandola, A. Banerjee, and V. Kumar. Anomaly detection: A survey. _ACM computing surveys (CSUR)_, 41(3):1-58, 2009.
* [16] Z. Chen, G. Xu, V. Mahalingam, L. Ge, J. Nguyen, W. Yu, and C. Lu. A cloud computing based network monitoring and threat detection system for critical infrastructures. _Big Data Research_, 3:10-23, 2016.
* [17] G. D. Clifford, C. Liu, B. Moody, D. Springer, I. Silva, Q. Li, and R. G. Mark. Classification of normal/abnormal heart sound recordings: The physionet/computing in cardiology challenge 2016. In _2016 Computing in cardiology conference (CinC)_, pages 609-612. IEEE, 2016.
* [18] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta, and A. A. Bharath. Generative adversarial networks: An overview. _IEEE signal processing magazine_, 35(1):53-65, 2018.

* [19] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. _Advances in neural information processing systems_, 34:8780-8794, 2021.
* [20] T. Fernando, H. Gammulle, S. Denman, S. Sridharan, and C. Fookes. Deep learning for medical anomaly detection-a survey. _ACM Computing Surveys (CSUR)_, 54(7):1-37, 2021.
* [21] A. Filos, P. Tigkas, R. McAllister, N. Rhinehart, S. Levine, and Y. Gal. Can autonomous vehicles identify, recover from, and adapt to distribution shifts? In _International Conference on Machine Learning_, pages 3145-3153. PMLR, 2020.
* [22] F. Gatta, F. Giampaolo, E. Prezioso, G. Mei, S. Cuomo, and F. Picciallli. Neural networks generative models for time series. _Journal of King Saud University-Computer and Information Sciences_, 34(10):7920-7939, 2022.
* [23] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks. _Communications of the ACM_, 63(11):139-144, 2020.
* [24] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851, 2020.
* [25] A. Host-Madsen, E. Sabeti, and C. Walton. Data discovery and anomaly detection using atypicality: Theory. _IEEE Transactions on Information Theory_, 65(9):5302-5322, 2019.
* [26] J. Jiang, F. Leofante, A. Rago, and F. Toni. Formalising the robustness of counterfactual explanations for neural networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2023.
* [27] D. P. Kingma and M. Welling. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [28] D. Kwon, H. Kim, J. Kim, S. C. Suh, I. Kim, and K. J. Kim. A survey of deep learning-based network anomaly detection. _Cluster Computing_, 22:949-961, 2019.
* [29] C.-Y. Lai, F.-K. Sun, Z. Gao, J. H. Lang, and D. S. Boning. Nominality score conditioned time series anomaly detection by point/sequential reconstruction. _arXiv preprint arXiv:2310.15416_, 2023.
* [30] X. A. Larriva-Novo, M. Vega-Barbas, V. A. Villagra, and M. S. Rodrigo. Evaluation of cybersecurity data set characteristics for their applicability to neural networks algorithms detecting cybersecurity anomalies. _IEEE Access_, 8:9005-9014, 2020.
* [31] V. Lin, K. J. Jang, S. Dutta, M. Caprio, O. Sokolsky, and I. Lee. Dc4l: Distribution shift recovery via data-driven control for deep learning models. In _6th Annual Learning for Dynamics & Control Conference_, pages 1526-1538. PMLR, 2024.
* [32] B. Lindemann, B. Maschler, N. Sahlab, and M. Weyrich. A survey on anomaly detection for technical systems using lstm networks. _Computers in Industry_, 131:103498, 2021.
* [33] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. _Advances in neural information processing systems_, 30, 2017.
* [34] C. Manikopoulos and S. Papavassiliou. Network intrusion and fault detection: a statistical anomaly approach. _IEEE Communications Magazine_, 40(10):76-82, 2002.
* [35] A. Manolache, F. Brad, and E. Burceanu. Date: Detecting anomalies in text via self-supervision of transformers. _arXiv preprint arXiv:2104.05591_, 2021.
* [36] R. Marcinkevics and J. E. Vogt. Interpretable and explainable machine learning: a methods-centric overview with concrete examples. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 13(3):e1493, 2023.
* [37] J. Marques-Silva and A. Ignatiev. Delivering trustworthy ai through formal xai. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2022.

* [38] A. P. Mathur and N. O. Tippenhauer. Swat: A water treatment testbed for research and training on ics security. In _2016 international workshop on cyber-physical systems for smart water networks (CySWater)_, pages 31-36. IEEE, 2016.
* [39] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. _arXiv preprint arXiv:2108.01073_, 2021.
* [40] A. Moallemi, A. Burrello, D. Brunelli, and L. Benini. Exploring scalable, distributed real-time anomaly detection for bridge health monitoring. _IEEE Internet of Things Journal_, 9(18):17660-17674, 2022.
* [41] H. Z. Moayedi and M. Masnadi-Shirazi. Arima model for network traffic prediction and anomaly detection. In _2008 international symposium on information technology_, volume 4, pages 1-6. IEEE, 2008.
* [42] A. Mousakhan, T. Brox, and J. Tayyub. Anomaly detection with conditioned denoising diffusion models. _arXiv preprint arXiv:2305.15956_, 2023.
* [43] G. Munz, S. Li, and G. Carle. Traffic anomaly detection using k-means clustering. In _Gi/itg workshop mmbnet_, volume 7, 2007.
* [44] M. Nauta, J. Trienes, S. Pathak, E. Nguyen, M. Peters, Y. Schmitt, J. Schlotterer, M. van Keulen, and C. Seifert. From anecdotal evidence to quantitative evaluation methods: A systematic review on evaluating explainable ai. _ACM Computing Surveys_, 55(13s):1-42, 2023.
* [45] G. Pallotta, M. Vespe, and K. Bryan. Vessel pattern knowledge discovery from ais data: A framework for anomaly detection and route prediction. _Entropy_, 15(6):2218-2245, 2013.
* [46] G. Pang and C. Aggarwal. Toward explainable deep anomaly detection. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 4056-4057, 2021.
* [47] T. Pourhabibi, K.-L. Ong, B. H. Kam, and Y. L. Boo. Fraud detection: A systematic literature review of graph-based anomaly detection approaches. _Decision Support Systems_, 133:113303, 2020.
* [48] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [49] D. Ramotsoela, A. Abu-Mahfouz, and G. Hancke. A survey of anomaly detection in industrial wireless sensor networks with critical water system infrastructure as a case study. _Sensors_, 18(8):2491, 2018.
* [50] M. Reyes, R. Meier, S. Pereira, C. A. Silva, F.-M. Dahlweid, H. v. Tengg-Kobligk, R. M. Summers, and R. Wiest. On the interpretability of artificial intelligence in radiology: challenges and opportunities. _Radiology: artificial intelligence_, 2(3):e190043, 2020.
* [51] M. T. Ribeiro, S. Singh, and C. Guestrin. " why should i trust you?" explaining the predictions of any classifier. In _Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining_, pages 1135-1144, 2016.
* [52] P. J. Rousseeuw and M. Hubert. Anomaly detection by robust statistics. _Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery_, 8(2):e1236, 2018.
* [53] K. K. Santhosh, D. P. Dogra, and P. P. Roy. Anomaly detection in road traffic using visual surveillance: A survey. _ACM Computing Surveys (CSUR)_, 53(6):1-26, 2020.
* [54] H.-K. Shin, W. Lee, J.-H. Yun, and H. Kim. {HAI} 1.0:{HIL-based} augmented {ICS} security dataset. In _13Th USENIX workshop on cyber security experimentation and test (CSET 20)_, 2020.
* [55] K. Simonyan. Deep inside convolutional networks: Visualising image classification models and saliency maps. _arXiv preprint arXiv:1312.6034_, 2013.

* [56] D. Sulem, M. Donini, M. B. Zafar, F.-X. Aubet, J. Gasthaus, T. Januschowski, S. Das, K. Kenthapadi, and C. Archambeau. Diverse counterfactual explanations for anomaly detection in time series. _arXiv preprint arXiv:2203.11103_, 2022.
* [57] I. Syarif, A. Prugel-Bennett, and G. Wills. Unsupervised clustering approach for network anomaly detection. In _Networked Digital Technologies: 4th International Conference, NDT 2012, Dubai, UAE, April 24-26, 2012. Proceedings, Part I 4_, pages 135-145. Springer, 2012.
* [58] J. J. Thiagarajan, K. Thopalli, D. Rajan, and P. Turaga. Training calibration-based counterfactual explainers for deep learning models in medical image analysis. _Scientific reports_, 12(1):597, 2022.
* [59] H. Touvron, L. Martin, K. Stone, P.-E. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Batra, A. Bhargava, S. Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [60] S. Tuli, G. Casale, and N. R. Jennings. Tranad: Deep transformer networks for anomaly detection in multivariate time series data. _arXiv preprint arXiv:2201.07284_, 2022.
* [61] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [62] L. Weng. What are diffusion models? _lilianweng.github.io_, Jul 2021.
* [63] J. Xu, H. Wu, J. Wang, and M. Long. Anomaly transformer: Time series anomaly detection with association discrepancy. _arXiv preprint arXiv:2110.02642_, 2021.
* [64] Z. Xu, Y. Liu, G. Deng, Y. Li, and S. Picek. A comprehensive study of jailbreak attack versus defense for large language models. In _Findings of the Association for Computational Linguistics ACL 2024_, pages 7432-7449, 2024.
* [65] J. Yu, A. Ignatiev, P. J. Stuckey, N. Narodytska, and J. Marques-Silva. Eliminating the impossible, whatever remains must be true: On extracting and applying background knowledge in the context of formal explanations. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2023.
* [66] J. Yu, Y. Zheng, X. Wang, W. Li, Y. Wu, R. Zhao, and L. Wu. Fastflow: Unsupervised anomaly detection and localization via 2d normalizing flows. _arXiv preprint arXiv:2111.07677_, 2021.
* [67] X. Yuan and Y. Qiao. Diffusion-ts: Interpretable diffusion for general time series generation. _arXiv preprint arXiv:2403.01742_, 2024.
* [68] S. Zagoruyko and N. Komodakis. Wide residual networks. _arXiv preprint arXiv:1605.07146_, 2016.
* [69] G. Zerveas, S. Jayaraman, D. Patel, A. Bhamidipaty, and C. Eickhoff. A transformer-based framework for multivariate time series representation learning. In _Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining_, pages 2114-2124, 2021.
* [70] A. Zhang, S. Song, J. Wang, and P. S. Yu. Time series data cleaning: From anomaly detection to anomaly repairing. _Proceedings of the VLDB Endowment_, 10(10):1046-1057, 2017.
* [71] H. Zhang, Z. Wang, Z. Wu, and Y.-G. Jiang. Diffusionad: Denoising diffusion for anomaly detection. _arXiv preprint arXiv:2303.08730_, 2023.
* [72] T. Zhou, P. Niu, L. Sun, R. Jin, et al. One fits all: Power general time series analysis by pretrained lm. _Advances in neural information processing systems_, 36, 2024.
* [73] Y. Zou, J. Jeong, L. Pemula, D. Zhang, and O. Dabeer. Spot-the-difference self-supervised pre-training for anomaly detection and segmentation. In _European Conference on Computer Vision_, pages 392-408. Springer, 2022.

[MISSING_PAGE_FAIL:15]

[MISSING_PAGE_FAIL:16]

Figure 8: More VisA examples. With AR-Pro, we have anomaly repairs resemble inputs better, compared with baseline.

Figure 9: More MVTec examples (Part 1). With AR-Pro, anomaly repairs resemble inputs better compared with the baseline.

Figure 11: The first image shows an instance where the baseline repairs the anomaly, but not as effectively as with property guidance. The last three images show instances where the baseline fails to repair the anomaly.

Figure 10: More MVTec examples (Part 2). With AR-Pro, anomaly repairs resemble inputs better compared with the baseline.

Figure 14: Effects of hyperparameter \(\lambda_{\phi}\) on \(M_{s}\), \(M_{d},M_{\omega}\) and \(M_{\overline{\omega}}\) on VisA cashew class; the effect of \(\lambda_{\phi}\) varies across metrics, but the range remain relatively small.

Figure 12: Effects of hyperparameter \(\lambda_{\phi}\) on \(M_{s}\), \(M_{d},M_{\omega}\) and \(M_{\overline{\omega}}\) on SWaT; the effect of \(\lambda_{\phi}\) varies across metrics, but the range remain relatively small.

Figure 13: Effects of hyperparameter on \(M_{s}\), \(M_{d},M_{\omega}\) and \(M_{\overline{\omega}}\) on VisA cashew class; the effect of \(\lambda_{\phi}\) varies across metrics, but the range remain relatively small.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope, as they clearly outline the development of a domain-independent framework for anomaly repair based on formal properties and the demonstration of its effectiveness through empirical results on VisA and SWaT datasets. This alignment ensures that the reader's expectations are met and that the paper delivers on its proposed objectives. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitation in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Although we did not have proofs, we listed the assumptions in Section 2 and Section 3. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We attach the code for reproducing the result to the zip file. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Please find the appendix for the implementation details. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We include the experiment details in Section 4. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We reported the mean and standard deviation for our main results in Table 1. We also report TNR with 95% statistical significance in Table 3. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We indicate GPU details in Appendix A and the amount of compute required for each of the individual experimental runs as well as estimate the total compute in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: There is no noticeable negative societal impact of the work performed. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [Yes] Justification: We call for users adhere to usage guidelines of GPT2 in Appendix B or implementing safety filters. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cited the original paper that produced the code packages and datasets. We also state which version of the model is used. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: We communicate the details of the code as part of our submissions in the zip file. This includes details about training, license, limitations, etc. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.