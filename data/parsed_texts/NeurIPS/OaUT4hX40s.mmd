# The Gain of Ordering in Online Learning

Vasilis Kontonis

UT Austin

vasilis@cs.utexas.edu &Mingchen Ma

UW-Madison

mingchen@cs.wisc.edu &Christos Tzamos

UW-Madison

tzamos@wisc.edu

###### Abstract

We study fixed-design online learning where the learner is allowed to choose the order of the datapoints in order to minimize their regret (aka self-directed online learning). We focus on the fundamental task of online linear regression: the learner is given a dataset \(X\) with \(n\) examples in \(d\) dimensions and at step \(t\) they select a point \(x_{t}\in X\), predict a value \(\widetilde{y}_{t}\), and suffer loss \((\widetilde{y}_{t}-w^{*}\cdot x_{t})^{2}\). The goal is to design algorithms that order the examples and achieve better regret than random- or worst-order online algorithms.

For an arbitrary dataset \(X\), we show that, under the Exponential Time Hypothesis, no efficient algorithm can approximate the optimal (best-order) regret within a factor of \(d^{1/\mathrm{poly}(\log\log d)}\).

We then show that, for structured datasets, we can bypass the above hardness result and achieve nearly optimal regret. When the examples of \(X\) are drawn i.i.d. from the uniform distribution on the sphere, we present an algorithm based on the greedy heuristic of selecting "easiest" examples first that achieves a \(\log d\)-approximation of the optimal regret.

## 1 Introduction

In online learning [14, 15, 16, 17] the learner receives an example and outputs a prediction about its label. The true label of the example is then revealed and the learner suffers loss depending on the "distance" of their prediction from the true label. The goal is to minimize the total loss over all learning rounds given the knowledge of the correct labels of previous rounds. Under worst-case assumptions, where an adversary controls the sequence of examples and labels presented to the learner, a wide range of algorithms based on exponential reweighting [23, 24, 25, 26] and online convex optimization [14, 15] have been developed.

Beyond Worst-Case Online LearningA less adversarial setting is _fixed-design_ (aka transductive) online learning first considered in [1] and subsequently studied in [21, 22]. In fixed-design online learning the pool of potential examples that the learner is going to face during the learning phase is fixed in advance and known to the learner. There are three main variants of fixed-design online learning considered in the literature (see, [1]): the _worst or adversarial order_ setting, i.e., when an adversary controls the order of examples presented to the learner, the _random order_, i.e., when the examples are presented to the learner in a random order, and the _best-order or self-directed_ setting [11], where the learner can choose the next example to predict its label at every round. Ordering examples during learning is common in practice: in the context of deep-learning, designing the order of examples when training a model is known as curriculum learning [1, 23, 24], where the focus is on finding ways to rank the training examples from "easy" to "hard", as well as using the right pacing function for introducing more difficult data. Another application is direct marketing [18, 25], one common business intelligence task, which is a process of identifying likely buyers to market products accordingly. In particular, an agent must study customers' characteristics and needs, and select customers to market their products. For example, astreaming service or social media platform may want to learn the content preferences of customers without making too many bad recommendations. In this application, the platform chooses the order in which to present the content (from a pool of available videos) in order to minimize the "regret" (and keep the user engaged). Finally, other non-adverserial online learning variants assume that the order of examples is chosen by a teacher who knows the ground-truth and presents the examples to help the learner [13, 14, 15] or make regularity assumptions about the dataset of examples [16, 17].

Self-Directed Learning and Gain from OrderingIn this work, we focus on the best-order or self-directed version of fixed-design online learning. We first formally define the self-directed online prediction model [11] and its random- and worst-order variants that we consider in this work.

**Definition 1.1** (Self-Directed Online Learning [11]).: _Let \(f\in\mathcal{C}\) be an unknown target concept from some concept class \(\mathcal{C}\) of functions from \(\mathbb{R}^{d}\) to \(\mathbb{R}\) and let \(X=\{x^{(1)},\ldots,x^{(n)}\}\) be a subset of \(n\in\mathbb{N}\) points in \(\mathbb{R}^{d}\). The learner has access to the full set of (unlabeled) points \(X\). Until the labels of all examples of \(X\) have been predicted:_

* _The learning algorithm picks a point_ \(x\in X\) _and makes a prediction_ \(z\in\mathbb{R}\) _about its label._
* _The true label_ \(f(x)\) _of_ \(x\) _is revealed and the learner suffers loss_ \((z-f(x))^{2}\)_._

_We say that the learner suffers \(L\) loss (or regret) to label \(X\) if, with probability at least \(99\%\), it holds that the total loss suffered by learner over all rounds is at most \(L\). In what follows, given a self-directed learner \(\mathcal{A}^{\mathrm{self}}\) we shall denote by \(\mathcal{L}(\mathcal{A}^{\mathrm{self}},X,f)\) its total loss \(L\)._

In this work, we investigate whether we can design efficient self-directed algorithms that exploit the power of ordering to improve the regret compared to the worst- and random-order settings.

**Remark 1.2** (Random-order and Worst-Order Online Learning).: _We shall refer to the setting where the example during the training phase is picked uniformly at random (without replacement) from the unlabeled data \(X\) as **random-order** learning. Moreover, we shall refer to the setting where the next example is chosen by an adversary as **worst-order** learning._

Before we continue, we remark that we will also consider "average-case" (Bayesian) settings where the target concept \(f\) is sampled from some prior distribution \(F\) over concepts.

**Remark 1.3** (Average and Worst-Case Targets).: _In Definition 1.1, we will also consider the case where the ground-truth \(f\) is drawn from some distribution over targets \(F\), in which case the total cost is defined to be the average cost over \(F\), i.e., \(\mathcal{L}(\mathcal{A}^{\mathrm{self}},X,F)\coloneqq\mathbf{E}_{f\sim F} \,\mathcal{L}(\mathcal{A}^{\mathrm{self}},X,f)\). In the worst case setting we will simply write \(\mathcal{L}(\mathcal{A}^{\mathrm{self}},X)\coloneqq\max_{f\in\mathcal{C}} \mathcal{L}(\mathcal{A}^{\mathrm{self}},X,f)\)._

To formalize the notion of how much self-directed learners "improve over worst or random settings" in a compact way, we introduce the "Gain from Ordering" (or simply gain) that is defined to be the ratio between the minimum possible total loss of an algorithm that works in the random-order setting and that of an algorithm that can order the examples.

**Definition 1.4** (Gain from Ordering).: _Let \(f\in\mathcal{C}\) be a target concept. Let \(X=\{x^{(1)},\ldots,x^{(n)}\}\) be a dataset of points in \(\mathbb{R}^{d}\). We define the Gain from Ordering of a self-directed learner \(\mathcal{A}^{\mathrm{self}}\) as_

\[\mathcal{G}(\mathcal{A}^{\mathrm{self}},X,F)\coloneqq\frac{\min_{\mathcal{A}^ {\mathrm{random}}}\mathcal{L}(\mathcal{A}^{\mathrm{random}},X,F)}{\mathcal{L }(\mathcal{A}^{\mathrm{self}},X,F)}\,.\]

_Moreover, we denote by \(\mathcal{G}^{*}(X,F)\coloneqq\max_{\mathcal{A}^{\mathrm{self}}}\mathcal{G}( \mathcal{A}^{\mathrm{self}},X,F)\) the maximum possible Gain from Ordering. Similarly, we define the Gain from Ordering in the worst-case (where the target \(f\) is chosen adversarially, see Remark 1.3) by setting \(\mathcal{G}(\mathcal{A}^{\mathrm{self}},X)\coloneqq\frac{\min_{\mathcal{A}^ {\mathrm{random}}}\mathcal{L}(\mathcal{A}^{\mathrm{random}},X)}{\mathcal{L }(\mathcal{A}^{\mathrm{self}},X)}\), and \(\mathcal{G}^{*}(X)=\max_{\mathcal{A}^{\mathrm{self}}}\mathcal{G}(\mathcal{A}^ {\mathrm{self}},X)\)._

We remark that in the above definition, we do not compare against worst-order algorithms as they always perform worse than the best random-order algorithm, and therefore, our results readily generalize when considering the Gain from Ordering with respect to worst-order algorithms.

Whether ordering the examples can improve the total loss suffered by the learner was first studied in the context of online classification, i.e., when the predictions of the algorithm and the ground truth labels are binary \(z,f(x)\in\{\pm 1\}\). For the class of one-dimensional thresholds over the real line, in [1] it was shown that, in the best-order setting, one mistake suffices. On the other hand in the random- and worst-order settings it is known [14, 15] that the total loss (or equivalently the number of mistakes) is \(\Theta(\log n)\), where \(n\) is the size of the dataset. Therefore, for thresholds on the real-line the Gain from Ordering is known to be \(\Theta(\log n)\). Positive results on the Gain from Ordering exist for other concept classes as well (e.g., for monotone monomials and axis-aligned rectangles) but there are also known concepts where ordering the examples does not help, see [1, 1].

Online Linear RegressionIn this paper, we focus on perhaps the most fundamental problem in online learning, namely online linear regression. An unknown target vector \(w^{*}\in\mathbb{R}^{d}\) is picked by an adversary (or sampled from some prior distribution, see Remark 1.3). The true label of an example \(x\) corresponds to \(f(x)=w^{*}\cdot x\). In the adversarial/worst-order setting there are numerous results studying online linear regression going back to the seminal work of Widrow and Hoff [13, 14, 15, 16, 17, 18, 19, 20]. In this setting, the optimal regret is well-understood and tight (even with respect to constant factors) upper and lower bounds exist [2, 20, 21]. The self-directed setting is much less understood both information-theoretically, i.e., what is the best possible Gain from Ordering, and computationally, i.e., whether there exist efficient self-directed algorithms that can achieve the optimal Gain from Ordering. In this work, we aim to make progress in this direction and ask the following fundamental question.

_Is there an efficient, self-directed learning algorithm for linear regression that achieves (approximately) optimal Gain from Ordering?_

### Our Results and Techniques

Our first result is an impossibility result showing that for _unstructured datasets_ it is computationally intractable to compute an ordering that approximates the optimal Gain from Ordering under the Exponential Time Hypothesis (ETH). More precisely, we show that, even for the "easier" version of the problem where the target vector is drawn uniformly at random from the unit sphere, no efficient self-directed learner can achieve better than \(d^{1/\operatorname{poly}(\log\log d)}\)-approximation of the optimal Gain from Ordering.

**Theorem 1.5** (Hardness of Approximation for the Optimal Gain from Ordering).: _Let \(X\) be an arbitrary set of \(n\) unit-norm examples and denote by \(\mathbb{S}^{d}\) the uniform distribution over the unit sphere. Under the Exponential Time Hypothesis (ETH), there is no polynomial time self-directed learner \(\mathcal{A}^{\operatorname{self}}\) such that \(\mathcal{G}(\mathcal{A}^{\operatorname{self}},X,\mathbb{S}^{d})\geq d^{-1/ \log\log^{c}d}\,\mathcal{G}^{*}(X,\mathbb{S}^{d})\), where \(c>0\) is some universal constant._

Our hardness result follows by a reduction to the Densest \(k\)-Subgraph (DkS) problem (see Definition 3.1) that was shown in [14] to be ETH-hard to approximate. To go from our learning problem to the combinatorial DkS problem we perform a sequence of approximation preserving reductions. At a high-level, we first show that selecting the best order for online linear regression is equivalent to an offline geometric problem where we need to sort the examples so that the sum of the distances of every example from the subspace spanned by the previous examples in the ordering is minimized, see Definition 3.3). We then show that this problem can be further reduced to a specific edge packing problem on graphs (see Definition 3.7) by only worsening the achieved approximation guarantee by a factor of \(2\). Finally, we show that any \(\alpha\)-approximate algorithm for the \(k\)-packing problem yields an \(O(\alpha^{2})\)-approximate algorithm for the Densest \(k\)-Subgraph problem. For more details, we refer to Section 3.

As our hardness result suggests, without any assumption over the structure of the data, there is no hope to design an efficient self-directed learner with "good" approximation guarantee - especially in high-dimensional settings, i.e., when \(d\) is large. This motivates us to study self-directed learning over structured datasets. We focus on the fundamental setting where the dataset \(X\) is i.i.d. drawn uniformly from the unit sphere, \(S^{d}\). In this case, we design an algorithm based on a greedy heuristic that picks the example that is "most similar" to the examples already seen achieves a \(\log d\)-approximation of the optimal Gain from Ordering.

**Theorem 1.6** (Efficient Self-Directed Learner under Spherical Data).: _Let \(X\) be a set of \(n\) examples drawn i.i.d. from \(\mathbb{S}^{d}\). There exists an efficient self-directed learner \(\mathcal{A}\) such that, with probability at least \(99\%\), it holds \(\mathcal{G}(\mathcal{A},X,\mathbb{S}^{d})\geq\ \Omega(1/\log d)\ \mathcal{G}^{*}(X, \mathbb{S}^{d})\). Moreover, it holds that \(\mathcal{G}(\mathcal{A},X,\mathbb{S}^{d})\geq\min(\Omega(d),\Omega(n^{2/d}))\)._We remark that, apart from presenting an efficient algorithm for approximating the Gain of Ordering, 1.6, gives the first information-theoretic bound for the optimal gain under spherical data, showing that \(\mathcal{G}^{*}(X,\mathbb{S}^{d})\) is roughly \(n^{2/d}\). At a high level, the fact that the optimal gain increases slower as the dimension increases is explained by the fact that, \(n\) samples from the uniform distribution on the sphere will be almost orthogonal (unless \(n\) is very large). Therefore, observing the labels of a subset of them reveals little information about the remaining points. See Section4 for more details.

Apart from showing that, under structured datasets, significantly improved approximation guarantees can be achieved our upper bound of Theorem1.6 serves as a formalization of the popular greedy heuristic of picking the "easiest examples first" used in curriculum learning ([1]) for linear regression. In particular, assuming that we have already observed the labels of the points \(x^{(1)},\ldots,x^{(i)}\) we pick the example that is closest to the subspace spanned by \(x^{(1)},\ldots,x^{(i)}\), see Algorithm1.

As an extension, we also study the self-directed ReLU regression problem, where the concept class is \(\mathcal{C}=\{\operatorname{Relu}(w^{*}\cdot x)\mid w^{*}\in\mathbb{R}^{d}\}\). ReLU regression is a very popular non-convex optimization task that has recently received significant attention both in online and offline settings, [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22], due to the fact that ReLU is a very common activation in deep learning models. In Section5, we present an efficient algorithm that achieves Gain of Ordering roughly \(\min(d,n^{2/d}/\log d+\log n/\log d)\), see Theorem5.1, assuming a "warm-start" labeled example. We observe that, even though ReLU regression is typically a harder task than linear regression, the Gain of Ordering that we achieve is larger (by a term of \(\log n/\log d\)) than that of linear regression. At a high-level this interesting phenomenon has to do with the the ReLU being constant on a large region of the space: a fact that the self-directed learner can exploit to improve its gain. It is an interesting direction for future research to further investigate the properties of the activation and the dataset that allow for improved gains by self directed learners.

### Related Work

Related to the setting of self-directed learning is active learning [1], where the learner has access to a large pool of unlabeled examples and chooses the "most informative" to ask for their labels. The goal is to find a classifier with good generalization while minimizing the number of label queries. There is a long line of research on active linear classification in the distribution-specific setting (e.g., under the uniform distribution on the unit sphere) [1, 10, 11]. We remark that our goal of minimizing the number of mistakes is orthogonal to that of active learning: at a high-level, our algorithms pick the examples for which the current hypothesis is most confident ("easiest examples") while in active learning one typically asks for the labels of the "hardest examples", e.g., those with the smallest margin with respect to the current guess (see, e.g., [1, 1, 19]).

In deep learning, stochastic gradient descent typically trains models by considering the examples in a random order. In the influential work of [1] the authors proposed curriculum learning: training machine learning models in a "meaningful order" - from easy examples to harder ones. There is a long line of research (see the surveys [12, 13, 14] and references therein) giving empirical evidence that curriculum learning provides significant benefits in convergence speed and generalization over training with random order. Our results provide theoretical evidence that ordering the examples from easier to harder significantly reduces the mistakes made by the learner.

There has been a long line of work studying online linear regression. We first review the results in the realizable setting, which means \(y(x)=w^{*}\cdot x\). In this case, the problem is also called the adaptive filtering problem by [13]. Online gradient descent was shown to be a minimax optimal method by [13, 14]. These works show if \(\|x\|\leq B\) and \(\|w^{*}\|\leq W\), then online gradient descent achieves a regret bound \(O(B^{2}W^{2})\). A matching lower bound \(\Omega(B^{2}W^{2})\) was given by [1] using a dataset that contains only one example. In the adversarial setting, online gradient descent and exponentiated gradient were studied by [1, 13]. Given the knowledge of \(B\), by suitable tuning the learning rate, the two algorithms achieve a regret bound \(O(L(w^{*}))\), which grows linearly with \(T\) in the worst case. Furthermore, if the algorithms know an error bound \(E\) for \(L(w^{*})\) and a bound \(W\) for \(\|w^{*}\|\), then these algorithms can achieve a regret bound \(O(\sqrt{E})\), which is \(O(\sqrt{T})\) in the worst case. These regret bounds cannot be further improved for such types of algorithms. On the other hand, assuming \(y(x)\leq Y\) and \(\|x\|\leq B\) for any \(x\in X\), [15, 16] obtained a regret bound of \(O\left(\left\|w^{*}\right\|^{2}+dY^{2}\log(TX^{2}/d)\right)\) using a so called online nonlinear ridge regression method.

A matching lower bound of \(\Omega(\left\lVert w^{*}\right\rVert^{2}+Y^{2}d\log T)\) were given by [1, 20]. Recently better lower bound of \(\Omega(dY^{2}\log T)\) were given by [1, 1]. Another interesting setting is the stochastic setting studied by [1], where \(y(x)=w^{*}\cdot x+\xi\), \(\xi\) is a zero-mean sub-gaussian noise with variance \(\sigma^{2}\). In particular, in such a setting, the label \(y\) can be unbounded. Ouhamma et al. showed that the online (nonlinear) ridge regression with high probability has a regret bound of \(O(d\sigma^{2}\log T\log\log T)\).

## 2 Notation and Preliminaries

In this section, we introduce the notations we will use in the paper. Let \(X\subseteq\mathbb{R}^{d}\) be the set of \(n\) examples Denote by \(w^{*}\in\mathbb{R}^{d}\) the target vector that labels each \(x\in X\) by \(y(x)=w^{*}\cdot x\). Let \(\mathcal{A}\) be a self-directed learner. For \(i\in[n]\), we use random vector \(x^{(i)}\in\mathbb{R}^{d}\) to denote the example in \(X\) that is selected by \(\mathcal{A}\) in the \(i\)-th round. We denote by \(L_{i}\) be subspace spanned by examples \(x^{(1)},\ldots,x^{(i)}\) and \(L_{i}^{\perp}\) the subspace that is orthogonal to \(L_{i}\). For every \(v\in\mathbb{R}^{d}\) and for every subspace \(L\subseteq\mathbb{R}^{d}\), we denote by \(v_{L}=\operatorname{proj}_{L}(v)\), the projection of \(v\) onto \(L\). Furthermore, we will use \(S^{d}\) to denote the unit sphere in \(\mathbb{R}^{d}\) and use \(\mathbb{S}^{d}\) to denote the uniform distribution over \(S^{d}\)

## 3 Optimal Gain from Ordering under Arbitrary Data is Hard

In this section, we show that self-directed linear regression, in general, is hard to approximate, even when \(X\subseteq S^{d}\) and \(w^{*}\) is drawn uniformly from \(S^{d}\).

In the rest of this section, we will give a high-level overview of the proof of Theorem1.5. The full proof of results in this section can be found in AppendixA. The key idea is that if we have a good efficient self-directed learner, then we can obtain an efficient algorithm that approximately solves the \(k\)-densest subgraph problem (D\(k\)S), which has been shown hard to approximate by [14].

**Definition 3.1** (Densest \(k\)-Subgraph Problem (D\(k\)S)).: _Let \(G=(V,E)\) be an undirected graph with \(n\) vertices and \(m\) edges, and let \(k\in[n]\). The goal is to find a subset of \(k\) vertices \(S\) such that the edge density \(\rho(S):=|E(S)|/\binom{[S]}{2}\) is maximized, where \(E(S)\) denotes the set of all edges among the vertices in \(S\). We define \(\operatorname{opt}\) to be the maximum density over all possible subsets of \(k\) vertices. Given \(G\) and \(k\), an \(\alpha\)-approximate algorithm for D\(k\)S problem outputs a subset of \(k\) vertices \(S\) such that \(\alpha\rho(S)\geq\operatorname{opt}\) in polynomial time._

The Sequential Spanning Problem (SSP)To begin with, we observe that every self-directed learner has two parts, selecting examples and making predictions. The first observation, which is stated as Lemma3.2, is that with a prior distribution of \(w^{*}\), it is easy to obtain a Bayesian optimal prediction in each round.

**Lemma 3.2**.: _Let \(X=\{x^{(1)},\ldots,x^{(n)}\}\subseteq\mathbb{R}^{d}\) be a set of \(n\) examples and \(w^{*}\) be a target vector drawn uniformly from \(S^{d}\) that labels \(y^{(i)}=w^{*}\cdot x^{(i)}\) for each \(x^{(i)}\in X\). Given given any set of labeled examples \((x^{(1)},y^{(1)}),\ldots,(x^{(i-1)},y^{(i-1)})\), denote by \(L_{i-1}\) the subspace spanned by \(x^{(1)},\ldots,x^{(i-1)}\) and \(w^{*}_{L_{i-1}}\) the projection of \(w^{*}\) onto \(L_{i-1}\). Let \(\mathcal{A}\) be a self-directed learner, denote by \(\hat{y}^{(i)}\) be the prediction of \(\mathcal{A}\) for the next example \(x^{(i)}\), then we have \(\mathbf{E}_{w^{*}}\left((\hat{y}^{(i)}-y^{(i)})^{2}\mid w^{*}_{L_{i-1}}\right)\geq \mathbf{E}_{w^{*}}\left((w^{*}_{L_{i-1}^{\perp}}\cdot x^{(i)})^{2}\mid w^{*} _{L_{i-1}}\right).\) Furthermore, the inequality holds for equality if \(\hat{y}^{(i)}=w^{*}_{L_{i-1}}\cdot x^{(i)}\)._

With the Bayesian optimal prediction, the problem becomes how to select a good ordering. We show in Proposition3.4, the problem can be equivalent to formulate as the following Sequential Spanning Problem.

**Definition 3.3** (\(k\)-Sequential Spanning Problem (\(k\)-Ssp)).: _Let \(X=\{x^{(1)},\ldots,x^{(m)}\}\) on \(\mathbb{R}^{d}\) be a set of \(m\) points with unit norm and let \(k\in\mathbb{N}\). Consider an ordered sequence \(\sigma\) of \(k\) points of \(X\), i.e., \(\sigma=x^{(i_{1})},x^{(i_{2})},\ldots,x^{(i_{k})}\) and define \(L_{j}\) to be the sub-space spanned by the first \(j\)-elements of \(\sigma\) i.e., \(L_{j}=\operatorname{span}(x^{(i_{1})},\ldots x^{(i_{j})})=\operatorname{span}(x ^{(\sigma(1))},\ldots x^{(\sigma(j))})\) and \(V_{0}=\emptyset\). Define the following cost,_

\[C(\sigma,k)=\sum_{i=1}^{k}\|\operatorname{proj}_{L_{i-1}^{\perp}}x^{(\sigma(i) )}\|_{2}^{2}\,.\]_We define \(\mathrm{opt}\) to be the minimum cost over all sequences of \(k\)-elements of \(X\). Given \(X\) and \(k\), an \(\alpha\)-approximate algorithm for \(k\)-SSP Problem outputs an ordered sequence of \(k\) points from \(X\) so that \(C(\sigma,k)\leq\alpha\ \mathrm{opt}\) in polynomial time._

_We shall refer to the special case of the problem where \(k=m\) simply as the Sequential Spanning Problem._

**Proposition 3.4** (From SSP to Self-Directed Learning).: _Let \(X=\{x^{(1)},\ldots,x^{(m)}\}\subseteq\mathbb{R}^{d}\) be a set of examples. For every self-directed linear regression algorithm \(\mathcal{A}\) over \(X\), with expected learning loss \(\mathcal{L}(\mathcal{A},\mathbb{S}^{d})\), we can use it to get a randomized algorithm \(\mathcal{A}^{\prime}\) for SSP over \(X\) with expected cost \(\mathbf{E}(C(\sigma,m))\leq d\ \mathcal{L}(\mathcal{A},\mathbb{S}^{d})\). Moreover, given a randomized algorithm \(\mathcal{A}^{\prime}\) for SSP over \(X\), we can get a self-directed linear regression algorithm \(\mathcal{A}\) over \(X\), with expected learning loss \(\mathcal{L}(\mathcal{A},\mathbb{S}^{d})=\mathbf{E}(C(\sigma,m))/d\). In particular, the construction can be done efficiently._

With the intuition above, we only need to show it is ETH-hard to approximate the SSP. The key technical result we obtain is the following proposition.

**Proposition 3.5** (From D\(k\)S to SSP).: _For every function \(\alpha(d,m):R^{+}\times R^{+}\to R^{+}\), if there is an \(\alpha(d,m)\)-approximate algorithm for the SSP, then there is a \(64\alpha^{2}(n^{3},n^{3})\)-approximate algorithm for the D\(k\)S problem._

As a direct corollary of Proposition3.5, we obtain the computational hardness of SSP.

**Corollary 3.6** (ETH-hardness of SSP).: _Assuming the Exponential Time Hypothesis (ETH) is true, then there is no algorithm that outputs an \(d^{1/\log\log^{\mathrm{c}}d}\)-approximate solution to the SSP in \(\mathrm{poly}(d,m)\) time, where \(c>0\) is a universal constant._

Proof of Corollary3.6.: By Corollary 1.3 in [11] if the Exponential Time Hypothesis (ETH) is true, then there is no polynomial time \(n^{1/\log\log^{\mathrm{c}}n}\)-approximate algorithm for \(k\)-densest subgraph problem. By Proposition3.5, if there is a polynomial time \(d^{1/\mathrm{poly}\log\log d}\)-approximate algorithm for SSP, we can obtain an efficient \(n^{1/\log\log^{\mathrm{c}}n}\) approximate algorithm for \(k\)-densest subgraph problem. Thus, if ETH is true it is hard to approximate SSP within a \(d^{1/\mathrm{poly}\log\log d}\) factor in polynomial time. 

In the rest of the section, we introduce the high-level of the proof of Proposition3.5 by breaking it down into two steps.

From \(k\)-Edge Packing to SSPIn the first step, we introduce an intermediate problem called \(k\)-Edge Packing. We want to show if we can approximate SSP efficiently then we can approximate \(k\)-Edge Packing efficiently.

**Definition 3.7** (\(k\)-Edge Packing).: _Let \(G=(V,E)\) be an undirected graph with \(n\) vertices and \(m\) edges, and let \(k\in[n]\). The goal is to find a subset of \(k\) edges \(S\) such that the number of vertices covered by \(S\), \(|V(S)|\) is minimized, where \(V(S)\) denotes the set of endpoints of edges in \(S\). We define \(\mathrm{opt}\) to be the minimum number of vertices covered by any possible subsets of \(k\) edges. Given \(G\) and \(k\), an \(\alpha\)-approximate algorithm for the \(k\)-edge packing problem outputs a subset of \(k\) edges \(S\) such that \(V(S)\leq\alpha\mathrm{opt}\) in polynomial time._

The key idea in this step is to show Lemma3.8, if we can approximate \(k\)-SSP efficiently, then we can approximate \(k\)-edge packing efficiently.

**Lemma 3.8**.: _For every function \(\alpha:R^{+}\times R^{+}\to R^{+}\), if there is an \(\alpha(d,m)\)-approximate algorithm for \(k\)-SSP problem, then there is a \(2\alpha(n,m)\)-approximate algorithm for \(k\)-edge packing problem_

To build a connection between a geometric problem and a combinatorial problem, we have the following construction. Given a graph \(G=(V,E)\) with \(n\) vertices and \(m\) edges, we will map every edge \((u,v)\) to a sparse \(n\)-dimensional vector \(x_{uv}\) such that the only non-zero entries are \(x_{u}=1\), \(x_{v}=-1\). In this way, we obtain a dataset \(X\) as the input of \(\mathcal{A}\), the approximate algorithm for \(k\)-SSP. Intuitively, if we select \(k\) edges in \(G\) that are disjoint, then no matter how to order the corresponding vectors of these \(k\) edges, the spanning cost is \(\Omega(k)\). But if we select \(k\) edges that form a clique, the corresponding vectors span a subspace of dimension \(O(\sqrt{k})\), so it is easy to order these vectors to get a very small spanning cost. With such an intuition, the key structure result we use here is that if a solution to the \(k\)-SSP problem has a sufficiently small cost, then the corresponding edges to these vectors must be sufficiently connected to each other and thus cover a sufficiently small number of vertices.

However, now we are only able to approximate the \(k\)-edge packing problem with an algorithm that approximately solves the \(k\)-SSP problem. To finish this step, we should also be able to use an algorithm for SSP as a subroutine to solve the \(k\)-SSP problem. We show this is possible in Lemma3.9.

**Lemma 3.9**.: _Assume that an algorithm for SSP in \(d\) dimensions exists that finds an \(\alpha(d,m)\)-approximate solution in \(\operatorname{poly}(d,m)\) time for some function \(\alpha:R^{+}\times R^{+}\to R^{+}\). Then an algorithm that finds a \(4\alpha(m(d+m),m(d+m))\)-approximate solution for \(k\)-SSP for every value of \(k\) in \(\operatorname{poly}(d,m)\) time exists._

The intuition here is that if the dataset \(X\) is in general position (every set of \(d\) examples are linearly independent) then SSP is actually a \(d\)-SSP. Given any dataset \(X\) with \(m\) points, we are able to map \(X\) to a dataset \(X^{\prime}\) in \(f(k,d,m)>d\) dimension that is in the general position. Such a map is done by making multiple copies for each example, lifting them to high dimension and adding tiny structured noise to each of them. We will show that such a transformation well preserves the information in the original dataset so that if we run \(\mathcal{A}\) over \(X^{\prime}\), we can extract a good approximate solution to the original \(k\)-SSP from the first \(f(k,d,m)\) terms of the output solution.

From D\(k\)S to \(k\)-Edge PackingSo far we have shown how to use an algorithm for SSP to solve the \(k\)-edge packing problem. Our final step is to show Lemma3.10, which implies that if we can approximately solve the \(k\)-edge packing problem then we can also solve the \(k\) densest subgraph problem.

**Lemma 3.10** (\(k\)-edge packing and D\(k\)S).: _For \(\alpha>0\), if there is an \(\alpha\)-approximate algorithm for \(k\)-edge packing problem, then there is an \(\alpha^{2}\)-approximate algorithm for D\(k\)S problem._

Notice that \(k\)-edge packing is seeking \(k\) edges that cover as few vertices as possible, which can be thought as a dual problem of D\(k\)S. If we are able to find the largest number \(f(k)\) such that \(f(k)\) edges can cover at most \(k\) vertices, then these \(f(k)\) edges induce the densest subgraph with \(k\)-vertices. The idea behind Lemma3.10 is that we can approximately find such \(f(k)\) using an approximate algorithm for \(k\)-edge packing and thus can approximately find the \(k\)-densest subgraph.

With the above three lemmas, we can prove Proposition3.5.

Proof of Proposition3.5Assume we have an \(\alpha(d,m)\)-approximate algorithm for SSP problem, then by Lemma3.9, we get an efficient \(4\alpha((d+m)m,(d+m)m)\)-algorithm for \(k\)-SSP problem. By Lemma3.8, we get an efficient \(8\alpha((m+n)m,(m+n)m)\)-approximate algorithm for \(k\)-edge packing problem. Since \((m+n)m\leq n^{3}\) always holds, we get an efficient \(8\alpha(n^{3},n^{3})\)-approximate algorithm for \(k\)-edge packing problem. By Lemma3.10, this gives us an efficient \(64\alpha^{2}(n^{3},n^{3})\)-approximate algorithm for \(k\)-densest subgraph problem.

## 4 A \(O(\log d)\)-Approximation for Spherical Data

Theorem1.5, suggests that without any assumption on the structure of the data, it is hard to obtain an efficient self-directed learners that approximates the best possible improvement over learning in random order. This motivates us to study the learning problem over datasets with natural structures. In this section, we consider perhaps the most natural setting, where the dataset \(X\) is drawn i.i.d. from \(\mathbb{S}^{d}\). Our main result in this section shows a simple greedy heuristic, Algorithm1, which selects the "easiest" example in each round, and has a nearly (i.e., off by a \(\log d\)-factor) optimal gain of ordering.

We first bound the Gain of Ordering achieved by the greedy heuristic described in Algorithm1.

**Proposition 4.1**.: _Let \(X\) be a set of \(n\geq\operatorname{poly}(d)\) examples drawn i.i.d. from \(\mathbb{S}^{d}\) and let \(\mathcal{A}\) denote Algorithm1, then over the randomness of the dataset \(X\), in expectation, \(\mathcal{G}(\mathcal{A},X,\mathbb{S}^{d})^{-1}\leq O(1/d+n^{-2/d})\)._We start with some intuition for Algorithm 1. Since the observed labels \(y^{(i)}\) are consistent with \(w^{*}\), after selecting examples \(x^{(1)},\ldots,x^{(i)}\), our guess \(w^{(i)}\) at step \(i\) coincides with the projection \(w^{*}_{L_{i}}\) of \(w^{*}\) onto the subspace \(L_{i}\) spanned by these examples. If we choose any \(x^{(i+1)}\) and predict \(w^{*}_{L_{i}}\cdot x^{(i+1)}\), then we will pay \(\ell_{i}^{2}=(w^{*}_{L_{i}^{+}}\cdot x^{(i+1)})^{2}=(w^{*}_{L_{i}^{+}}\cdot x ^{(i+1)}_{L_{i}^{+}})^{2}\). Since we make a random partition of the data in advance, we are able to show that in expectation \(\ell_{i}^{2}/\big{(}\left\|w^{*}_{L_{i}^{+}}\right\|^{2}\left\|x^{(i+1)}_{L_{i} ^{+}}\right\|^{2}\big{)}=1/(d-i)\). This suggests the cost we pay in each round is proportional to \(\left\|x^{(i+1)}_{L_{i}^{+}}\right\|^{2}\) and we should choose the example that is closest to \(L_{i}\) greedily. Our key technical lemma shows that in the \(i+1\)-th round, for a greedily chosen example and a randomly chosen example, the expected ratio of the loss is \(O(n^{-2/(d-i)})\).

We next show that the optimal Gain from Ordering can only be \(O(\log d)\) times larger than that of Algorithm 1.

**Proposition 4.2** (Bounding the Optimal Gain).: _Let \(X\subseteq S^{d}\) be a set of \(n>\operatorname{poly}(d)\) examples drawn i.i.d. uniformly from \(S^{d}\). For every \(\delta\in(3/n,1)\), with probability at least \(99\%\), \(\mathcal{G}^{*}(X,\mathbb{S}^{d})^{-1}\geq\Omega(1/d+\delta n^{-(2+2\delta)/d})\)._

Recall that in Section 3, we showed that if \(w^{*}\sim\mathbb{S}^{d}\), then the expected learning cost is proportional to the optimal spanning cost of a sequence of examples. This implies analyzing the best learner is equivalent to analyzing the sequence of examples in \(X\) with the smallest spanning cost. The key of our proof is to show that the distribution of \(\mathbb{S}^{d}\) is very concentrated and unless the size of \(X\) is very large there is no sequence of examples whose spanning cost is much smaller than the average spanning cost.

As a direct corollary, we are able to show Algorithm 1 has a gaining that is at most \(O(1/\log d)\mathcal{G}^{*}(X,\mathbb{S}^{d})\), which is an exponential improvement of the hardness of approximation obtained by Theorem 1.5.

``` Randomly partition \(X\) into \(d\) subsets \(X_{1},\ldots,X_{d}\) such that \(X_{i}\) contains \(O(n/(d-i)^{2})\) examples. for\(i=1,\ldots,d\)do  Set \(L_{i-1}=\operatorname{span}\{x^{(1)},\ldots,x^{(i-1)}\}\).  Find \(w^{(i)}\in L_{i-1}\) consistent with \((x^{(1)},y^{(1)}),\ldots,(x^{(i-1)},y^{(i-1)})\).  Select \(x^{(i)}=\operatorname{argmin}_{x\in X_{i}}\left\|\operatorname{proj}_{L_{i-1 }}(x)\right\|\).  Predict \(\hat{y}^{(i)}=w^{(i)}\cdot x^{(i)}\) and receive \(y^{(i)}\).  Label all unlabeled examples in \(X\) using \(w^{(d)}\). ```

**Algorithm 1**SelfDirectedLinearRegression\((X)\)

Proof of Theorem 1.6By Proposition 4.1 and Proposition 4.2, we know that with probability at least \(99\%\), \(\mathcal{G}(\mathcal{A},X,\mathbb{S}^{d})^{-1}\leq 1/d+n^{-2/d}\) and \(\mathcal{G}^{*}(X,\mathbb{S}^{d})^{-1}\geq 1/d+\delta n^{-(2+2\delta)/d}\). Here we ignore the hidden constant within the bound obtained from Proposition 4.1 and Proposition 4.2 because it will only add some multiplicative constant factor in our final result. It remains to tune the parameter \(\delta\) to show that \(\frac{1/d+n^{-2/d}}{1/d+\delta n^{-(2+2\delta)/d}}\) is at most \(O(\log d)\). To do this, we write \(s^{d/2}\) and we consider different ranges for \(s\). We consider the following two cases.

Case 1: If \(s\geq d\), then we have we have \(1/d+n^{-2/d}\leq 2/d\leq 2(1/d+\delta n^{-(2+2\delta)/d})\), which implies \(\mathcal{G}(\mathcal{A},X,\mathbb{S}^{d})\geq\Omega(1)\mathcal{G}^{*}(X, \mathbb{S}^{d})\).

Case 2: If \(s<d\), then \(n^{-2/d}\geq 1/d\) and \(1/d+\delta n^{-(2+2\delta)/d}\geq\delta n^{-(2+2\delta)/d}\). This implies \(\frac{1/d+n^{-2/d}}{1/d+\delta n^{-(2+2\delta)/d}}\leq\frac{2}{\delta}n^{2 \delta/d}=\frac{2}{\delta}s^{\delta}\). We set \(\delta=1/\log s\) and we obtain that \(\frac{1}{\delta}s^{\delta}=s^{1/\log s}\log s=O(\log s)\leq O(\log d)\). This implies that \(\mathcal{G}(\mathcal{A},X,\mathbb{S}^{d})\geq\Omega(1/\log d)\mathcal{G}^{*} (X,\mathbb{S}^{d})\).

**Remark 4.3**.: _We remark that although the statement of Proposition 4.1 and Proposition 4.2 are about the average performance of the algorithm, the same results also hold under the worst-case setting because, with a dataset from a spherical distribution, the learning cost of \(\mathcal{A}^{\rm random}\) under these two settings are asymptotically the same in expectation. A detailed discussion about this and the proof of Proposition4.1 and Proposition4.2 are deferred to AppendixB.

**Remark 4.4**.: _Although according to Theorem1.6, Algorithm1 approximates the optimal gaining within a \(O(\log d)\) factor, when \(n\leq s^{d}\) for some constant \(s\) that doesn't depend on \(d\) or \(n\geq d^{d/2}\) Algorithm1 approximates the optimal gaining within a constant factor and thus is nearly optimal._

## 5 Self-Directed ReLU Regression

Finally, we study the problem of self-directed ReLU regression, which shares a similar spirit to the one of self-directed linear regression. According to Proposition4.1, given a dataset \(X\) that is drawn from \(\mathbb{S}^{d}\), we are able to design an efficient learner with gaining \(\mathcal{G}(\mathcal{A},X)\geq\min(\Omega(d),\Omega(n^{2/d}))\) if each \(x\) is labeled by \(y(x)=w^{*}\cdot x\). In fact, the bound of \(\min(\Omega(d),\Omega(n^{2/d}))\) can also be obtained by the following Algorithm2, when each \(x\) is labeled by a ReLU function \(\operatorname{Relu}(w^{*}\cdot x)\). However, an interesting phenomenon we found is that if we give Algorithm2 some reference example \((x^{(0)},y^{(0)})\) as a warm start such that \(\theta(x^{(0)},w^{*})=\theta_{0}<\pi/2\), then we are able to improve the gaining to \(\min(\Omega(d),\Omega(n^{2/d}/\log d+\log n/\log d))\), which is a huge improvement when the dimension of the problem is large.

```  Let \((x^{(0)},y^{(0)})\) be a pair of reference example such that \(\theta(x^{(0)},w^{*})=\theta_{0}\leq\pi/2\). (Assume \((x^{(0)},y^{(0)})=(0,0)\) if there is no such a warm start)  Randomly partition \(X\) into \(d\) subsets \(X_{1},\dots,X_{d}\) such that \(X_{i}\) contains \(n/d\) examples. for\(i=1,\dots,d\)do  Set \(L_{i-1}=\operatorname{span}\{x^{(0)},\dots,x^{(i-1)}\}\), where \(x^{(j)}\) is example that has been selected with positive label for \(j\in\{0,\dots,i-1\}\).  Find \(w^{(i)}\in L_{i-1}\) consistent with \((x^{(0)},y^{(0)}),\dots,(x^{(i-1)},y^{(i-1)})\).  Keep selecting \(x\in X_{i}\) such that \(w^{(i)}\cdot x\in\operatorname{argmin}_{x^{\prime}\in X_{i}}w^{(i)}\cdot x^{\prime}\) and predict \(\operatorname{Relu}(w^{(i)}\cdot x)\) until we see some \((x^{(i)},y^{(i)})\) such that \(y^{(i)}>0\). Label all unlabeled examples in \(X\) using \(w^{(d+1)}\). ```

**Algorithm 2** SelfDirectedReLURegression(\(X\))

**Theorem 5.1**.: _Let \(X\subseteq\mathbb{R}^{d}\) be a set of \(n\geq\operatorname{poly}(d)\) examples drawn i.i.d. from \(\mathbb{S}^{d}\). Let \(\mathcal{G}(\mathcal{A},X)\) be the gain from ordering of Algorithm2 for the self-directed ReLU regression problem over \(X\). Then_

\[\mathcal{G}(\mathcal{A},X)^{-1}\leq O(\frac{1}{d})+\begin{cases}\min\{O(\frac{ \tan^{2}\theta_{0}\log d}{d}),O(\frac{\log d}{n^{2/d}})\}&\text{if }\frac{n\theta_{0}}{4\pi d\log }>\exp(\frac{d}{8}),\\ \min\{O(\frac{\tan^{2}\theta_{0}\log d}{\log(n\theta_{0})}),O(\frac{\log d}{n ^{2/d}})\}&\text{if }1\leq\frac{n\theta_{0}}{4\pi d\log d}\leq\exp(\frac{d}{8})\\ O(\frac{1}{n})&\text{if }\frac{n\theta_{0}}{4\pi d\log d}<1.\end{cases}\]

The main difference between ReLU regression and linear regression is that in ReLU regression, with a good warm start, we are able to start from the region that is labeled 0 by the target to seek the decision boundary. Before we see a positive example, we pay nothing. The first time we see a positive example \(x\), we pay \((w^{*}\cdot x)^{2}\). The key technical lemma, Lemma5.2 shows that as we keep selecting the most "negative" example with respect to our current guess, the first positive example we see must have a very small margin with respect to \(w^{*}\). In this way, Algorithm2 learns \(w^{*}\) with a very small cost. We refer to AppendixC for more details and proofs.

**Lemma 5.2** (Geometry Technical Lemma).: _Let \(w^{*}\in S^{d}\) be a target vector and let \(w\in S^{d}\) be an arbitrary vector such that \(\theta=\theta(w^{*},w)<\pi/2\). Denote by \(C=\{x\in S^{d}\mid w^{*}x\leq 0,w\cdot x\geq 0\}\). For every \(a,b\in(0,1)\), denote by \(K_{a}:=\{x\in C\mid w\cdot x\geq a\sin\theta\}\) and \(K_{a,b}:=\{x\in C\mid w\cdot x\geq a\sin\theta,w^{*}x\leq-b\sin\theta\}\). Let \(x\) be a point uniformly drawn from \(S^{d}\). There is some absolute constant \(c>1\) such that if \(a/b\geq c\) then \(\operatorname{\mathbf{Pr}}\left(x\in K_{a,b}\mid x\in K_{a}\right)\leq 2\exp \Big{(}-\frac{d}{3(1-a^{2})}(b^{2}+2ab\cos\theta)\Big{)}\)._

## 6 Conclusion, Limitations, and Broader Impact

In this work, we study the self-directed learning problem for linear regression. Our work presents novel results both computational and information-theoretic. Our first result shows that approximating the optimal self-directed regret within a \(d^{1/\mathrm{poly}\log\log d}\) factor is ETH-hard under arbitrary datasets. Our second result yields a novel characterization of the Gain of Ordering for data that are uniformly distributed on the sphere and gives an efficient approximation algorithm that bypasses the aforementioned hardness result and achieves close to optimal regret by exploiting the structure of the data. A limitation of our work is that our algorithms can currently handle linear (or ReLU regression). Moreover, our presented results assume that the labels are realizable - even in this fundamental setting, nothing was known prior to our work. Generalizing our results to other concept classes and investigating the effect of adding noise to the labels are natural questions for future investigation. Designing robust self-directed learning algorithms for broader concept classes under broader distributional assumptions are interesting direction for future work.

## 7 Acknowledgements

This work was supported by the NSF Award CCF-2144298 (CAREER).

## References

* [ABHU15] P. Awasthi, M. F. Balcan, N. Haghtalab, and R. Urner. Efficient learning of linear separators under bounded noise. In _Proceedings of The 28\({}^{\text{th}}\) Conference on Learning Theory, COLT 2015_, pages 167-190, 2015.
* [ABHZ16] P. Awasthi, M. F. Balcan, N. Haghtalab, and H. Zhang. Learning and 1-bit compressed sensing under asymmetric noise. In _Proceedings of the 29\({}^{\text{th}}\) Conference on Learning Theory, COLT 2016_, pages 152-192, 2016.
* [AW01] Katy S Azoury and Manfred K Warmuth. Relative loss bounds for on-line density estimation with the exponential family of distributions. _Machine learning_, 43:211-246, 2001.
* [B\({}^{+}\)54] D. Blackwell et al. Controlled random walks. In _Proceedings of the international congress of mathematicians_, volume 3, pages 336-338, 1954.
* [BDEK95] S. Ben-David, N. Eiron, and E. Kushilevitz. On self-directed learning. In _Proceedings of the eighth annual conference on Computational learning theory_, pages 136-143, 1995.
* [BDKM97] S. Ben-David, E. Kushilevitz, and Y. Mansour. Online learning versus offline learning. _Machine Learning_, 29:45-63, 1997.
* [BKM\({}^{+}\)15] Peter L Bartlett, Wouter M Koolen, Alan Malek, Eiji Takimoto, and Manfred K Warmuth. Minimax fixed-design linear regression. In _Conference on Learning Theory_, pages 226-239. PMLR, 2015.
* [BLCW09] Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In _Proceedings of the 26th annual international conference on machine learning_, pages 41-48, 2009.
* [BU16] M. Balcan and R. Urner. Active learning-modern learning theory., 2016.
* [Byl97] Tom Bylander. Worst-case absolute loss bounds for linear learning algorithms. In _AAAI/IAAI_, pages 485-490, 1997.
* [CAL94] D. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. _Machine learning_, 15:201-221, 1994.
* [CBL06] N. Cesa-Bianchi and G. Lugosi. _Prediction, learning, and games_. Cambridge university press, 2006.
* [CBLW96] Nicolo Cesa-Bianchi, Philip M Long, and Manfred K Warmuth. Worst-case quadratic loss bounds for prediction using linear functions and gradient descent. _IEEE Transactions on Neural Networks_, 7(3):604-619, 1996.

* [CBS13] Nicolo Cesa-Bianchi and Ohad Shamir. Efficient transductive online learning via randomized rounding. _Empirical Inference_, page 177, 2013.
* [CL06] Nicolo Cesa-Bianchi and Gabor Lugosi. _Prediction, learning, and games_. Cambridge University Press, 2006.
* [DGK\({}^{+}\)20] I. Diakonikolas, S. Goel, S. Karmalkar, A. R. Klivans, and M. Soltanolkotabi. Approximation schemes for ReLU regression. In _Conference on Learning Theory, COLT_, volume 125 of _Proceedings of Machine Learning Research_, pages 1452-1485. PMLR, 2020.
* [DKKZ20] I. Diakonikolas, D. M. Kane, V. Kontonis, and N. Zarifis. Algorithms and SQ lower bounds for pac learning one-hidden-layer ReLU networks. In _Conference on Learning Theory, COLT_, pages 1514-1539. PMLR, 2020.
* [DKM05] Sanjoy Dasgupta, Adam Tauman Kalai, and Claire Monteleoni. Analysis of perceptron-based active learning. In _Learning Theory: 18th Annual Conference on Learning Theory, COLT 2005, Bertinoro, Italy, June 27-30, 2005. Proceedings 18_, pages 249-263. Springer, 2005.
* [DKR23] I. Diakonikolas, D. M. Kane, and L. Ren. Near-optimal cryptographic hardness of agnostically learning halfspaces and relu regression under gaussian marginals. _CoRR_, abs/2302.06512, 2023.
* [DKTZ22] I. Diakonikolas, V. Kontonis, C. Tzamos, and N. Zarifis. Learning a Single Neuron with Adversarial Label Noise via Gradient Descent. In _COLT_. arXiv, June 2022.
* [DKTZ23] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Self-directed linear classification. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 2919-2947. PMLR, 2023.
* [DKZ20] I. Diakonikolas, D. M. Kane, and N. Zarifis. Near-optimal SQ lower bounds for agnostically learning halfspaces and ReLUs under Gaussian marginals. In _Advances in Neural Information Processing Systems, NeurIPS_, 2020.
* [DSZ10] T. Doliwa, H. Simon, and S. Zilles. Recursive teaching dimension, learning complexity, and maximum classes. In _Algorithmic Learning Theory: 21st International Conference, ALT 2010, Canberra, Australia, October 6-8, 2010. Proceedings 21_, pages 209-223. Springer, 2010.
* [FCG20] S. Frei, Y. Cao, and Q. Gu. Agnostic learning of a single neuron with gradient descent. In _Advances in Neural Information Processing Systems, NeurIPS_, 2020.
* [FS97] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. _Journal of Computer and System Sciences_, 55(1):119-139, 1997.
* [GGHS19] Pierre Gaillard, Sebastien Gerchinovitz, Malo Huard, and Gilles Stoltz. Uniform regret bounds over \(\{R\}^{d}\) for the sequential linear regression problem with the square loss. In _Algorithmic Learning Theory_, pages 404-432. PMLR, 2019.
* [GKK] S. Goel, S. Karmalkar, and A. R. Klivans. Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals. In _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019_.
* [GKKT17] S. Goel, V. Kanade, A. R. Klivans, and J. Thaler. Reliably learning the relu in polynomial time. In _Proceedings of the 30\({}^{\text{th}}\) Conference on Learning Theory, COLT 2017_, pages 1004-1042, 2017.
* [GM93] S. Goldman and D. Mathias. Teaching a smart learner. In _Proceedings of the sixth annual conference on computational learning theory_, pages 67-76, 1993.

* [GS94] S. A Goldman and R. H Sloan. The power of self-directed learning. _Machine Learning_, 14:271-294, 1994.
* [Han57] J. Hannan. Approximation to bayes risk in repeated play. _Contributions to the Theory of Games_, 3:97-139, 1957.
* [Han11] S. Hanneke. Rates of convergence in active learning. _Ann. Statist._, 39(1):333-361, February 2011.
* [Haz16] E. Hazan. Introduction to online convex optimization. _Foundations and Trends(r) in Optimization_, 2(3-4):157-325, 2016.
* [HW19] G. Hacohen and D. Weinshall. On the power of curriculum learning in training deep networks. In _International Conference on Machine Learning_, pages 2535-2544. PMLR, 2019.
* [JRSS15] A. Jadbabaie, A. Rakhlin, S. Shahrampour, and K. Sridharan. Online optimization: Competing with dynamic comparators. In _Artificial Intelligence and Statistics_, pages 398-406. PMLR, 2015.
* [KK05] Sham Kakade and Adam T Kalai. From batch to transductive online learning. _Advances in Neural Information Processing Systems_, 18, 2005.
* [KS09] A. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In _COLT_. Citeseer, 2009.
* [KW97] Jyrki Kivinen and Manfred K Warmuth. Exponentiated gradient versus gradient descent for linear predictors. _information and computation_, 132(1):1-63, 1997.
* [KWH06] Jyrki Kivinen, Manfred K Warmuth, and Babak Hassibi. The p-norm generalization of the lms algorithm for adaptive filtering. _IEEE Transactions on Signal Processing_, 54(5):1782-1793, 2006.
* [Lit88] N. Littlestone. Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm. _Machine Learning_, 2(4):285-318, 1988.
* [Lit89] N. Littlestone. From online to batch learning. In _Proceedings of the Second Annual Workshop on Computational Learning Theory_, pages 269-284, 1989.
* [LL98] Charles X Ling and Chenghui Li. Data mining for direct marketing: Problems and solutions. In _Kdd_, volume 98, pages 73-79, 1998.
* [LLW91] Nicholas Littlestone, Philip M Long, and Manfred K Warmuth. On-line learning of linear functions. In _Proceedings of the twenty-third annual ACM symposium on Theory of computing_, pages 465-475, 1991.
* [LW94] N. Littlestone and M. Warmuth. The weighted majority algorithm. _Information and Computation_, 108(2):212-261, February 1994.
* [Man17] Pasin Manurangsi. Almost-polynomial ratio eth-hardness of approximating densest k-subgraph. In _Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing_, pages 954-961, 2017.
* [Mat97] D. Mathias. A model of interactive teaching. _journal of computer and system sciences_, 54(3):487-501, 1997.
* [MSSZ22] F. Mansouri, H. Simon, A. Singla, and S. Zilles. On batch teaching with sample complexity bounded by vcd. In _Advances in Neural Information Processing Systems_, 2022.
* [NL11] E. Ni and C. Ling. Direct marketing with fewer mistakes. In _Advanced Data Mining and Applications: 7th International Conference, ADMA 2011, Beijing, China, December 17-19, 2011, Proceedings, Part I 7_, pages 256-269. Springer, 2011.

* [OCCB15] Francesco Orabona, Koby Crammer, and Nicolo Cesa-Bianchi. A generalized online mirror descent with applications to classification and regression. _Machine Learning_, 99:411-435, 2015.
* [OMP21] Reda Ouhamma, Odalric-Ambrym Maillard, and Vianney Perchet. Stochastic online linear regression: the forward algorithm to replace ridge. _Advances in Neural Information Processing Systems_, 34:24430-24441, 2021.
* [Ora19] F. Orabona. A modern introduction to online learning, 2019.
* [Rob51] H. Robbins. Asymptotically subminimax solutions of compound statistical decision problems. In _Proceedings of the second Berkeley symposium on mathematical statistics and probability_, volume 2, pages 131-149. University of California Press, 1951.
* [RS13] A. Rakhlin and K. Sridharan. Online learning with predictable sequences. In _Conference on Learning Theory_, pages 993-1019. PMLR, 2013.
* [SIRS22] P. Soviany, R. Ionescu, P. Rota, and N. Sebe. Curriculum learning: A survey. _International Journal of Computer Vision_, 130(6):1526-1565, 2022.
* [Sol17] M. Soltanolkotabi. Learning relus via gradient descent. _Advances in neural information processing systems_, 30, 2017.
* [Vov90] V. Vovk. Aggregating strategies. In _Annual Workshop on Computational Learning Theory: Proceedings of the third annual workshop on Computational learning theory, 1990_. Association for Computing Machinery, Inc, 1990.
* [Vov95] V. Vovk. A game of prediction with expert advice. In _Proceedings of the eighth annual conference on Computational learning theory_, pages 51-60, 1995.
* [Vov01] Volodya Vovk. Competitive on-line statistics. _International Statistical Review_, 69(2):213-248, 2001.
* [WCZ21] X. Wang, Y. Chen, and W. Zhu. A survey on curriculum learning. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(9):4555-4576, 2021.
* [WH60] Bernard Widrow and Marcian E Hoff. Adaptive switching circuits. Technical report, Stanford Univ Ca Stanford Electronics Labs, 1960.
* [WHGS22] Changlong Wu, Mohsen Heidari, Ananth Grama, and Wojciech Szpankowski. Sequential vs. fixed design regrets in online learning. In _2022 IEEE International Symposium on Information Theory (ISIT)_, pages 438-443. IEEE, 2022.
* [ZSA20] C. Zhang, J. Shen, and P. Awasthi. Efficient active learning of sparse halfspaces with arbitrary bounded noise. In _Advances in Neural Information Processing Systems, NeurIPS_, 2020.