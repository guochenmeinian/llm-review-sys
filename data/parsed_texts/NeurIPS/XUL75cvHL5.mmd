# The Collusion of Memory and Nonlinearity in

Stochastic Approximation With Constant Stepsize

 Dongyan Lucy Huo\({}^{1}\) Yixuan Zhang\({}^{2}\) Yudong Chen\({}^{2}\) Qiaomin Xie\({}^{2}\)

\({}^{1}\)Cornell University \({}^{2}\)University of Wisconsin-Madison

dh622@cornell.edu

{yzhang2554,yudong.chen,qiaomin.xie}@wisc.edu

###### Abstract

In this work, we investigate stochastic approximation (SA) with Markovian data and nonlinear updates under constant stepsize \(\alpha>0\). Existing work has primarily focused on either i.i.d. data or linear update rules. We take a new perspective and carefully examine the simultaneous presence of Markovian dependency of data and nonlinear update rules, delineating how the interplay between these two structures leads to complications that are not captured by prior techniques. By leveraging the smoothness and recurrence properties of the SA updates, we develop a fine-grained analysis of the correlation between the SA iterates \(\theta_{k}\) and Markovian data \(x_{k}\). This enables us to overcome the obstacles in existing analysis and establish for the first time the weak convergence of the joint process \((x_{k},\theta_{k})_{k\geq 0}\). Furthermore, we present a precise characterization of the asymptotic bias of the SA iterates, given by \(\mathbb{E}[\theta_{\infty}]-\theta^{*}=\alpha(b_{\text{m}}+b_{\text{a}}+b_{ \text{c}})+\mathcal{O}(\alpha^{3/2})\). Here, \(b_{\text{m}}\) is associated with the Markovian noise, \(b_{\text{n}}\) is tied to the nonlinearity of the SA operator, and notably, \(b_{\text{c}}\) represents a multiplicative interaction between the Markovian noise and the nonlinearity of the operator, which is absent in previous works. As a by-product of our analysis, we derive finite-time bounds on higher moment \(\mathbb{E}[||\theta_{k}-\theta^{*}||^{2p}]\) and present non-asymptotic geometric convergence rates for the iterates, along with a Central Limit Theorem.

## 1 Introduction

Stochastic Approximation (SA) is an iterative scheme for solving fixed-point equations using noisy observations. Its application spans various domains including stochastic control [9; 40], reinforcement learning (RL) [4; 62] and stochastic optimization [43]. A typical SA algorithm takes the form \(\theta_{k+1}=\theta_{k}+\alpha g(\theta_{k},x_{k})\), where \((x_{k})_{k\geq 0}\) represents the underlying noisy data sequence and \(\alpha>0\) is a constant stepsize. The goal of SA is to approximate the target solution \(\theta^{*}\) that solves \(\mathbb{E}_{x\sim\pi}[g(\theta^{*},x)]=0\), with \(\pi\) being the stationary distribution of the stochastic process \((x_{k})_{k\geq 0}\).

SA subsumes many important algorithms. A prime example is stochastic gradient descent (SGD) for minimizing a function \(J(\theta)\) given noisy estimates \(g(\theta,x)\) of its gradient. Linear SA schemes include SGD for quadratic objective functions, as well as various RL algorithms such as linear TD-Learning (in which \(g\) is not the gradient of any function and standard SGD results do not apply).

Of particular interest to us are SA updates given by a _nonlinear_ function \(g(\theta,x)\) of \(\theta\). One motivating example is learning a Generalized Linear Model (GLM) \(y\approx\sigma(z^{\top}\theta)\) with a nonlinear mean function \(\sigma:\mathbb{R}\rightarrow\mathbb{R}\). A power approach, developed in [19; 36; 37; 64], uses a surrogate loss function, where the corresponding SGD update takes the form \(\theta_{k+1}=\theta_{k}+\alpha(\sigma(w_{k}^{\top}\theta_{k})-y_{k})w_{k}\), where \(x_{k}=(w_{k},y_{k})\) is the observed covariate-response pair. Common choices of \(\sigma\) include the identity map for linear regression, the sigmoid function for logistic regression, as well as Rectified Linear Unit (ReLU) and its various smoothed versions (e.g., ELU and SoftPlus) for ReLU regression [7; 18; 19; 30; 36].

Furthermore, we are interested in the setting where the data sequence \((x_{k})_{k\geq 0}\) forms a _Markov chain_, going beyond the common i.i.d. data setting. The Markovian model captures a wide range of SA problems in machine learning where stochastic data exhibit serial dependence [5, 33, 38, 53].

Classical work on SA focuses on diminishing stepsizes [10, 58]. Constant stepsize schemes have recently gained popularity due to easy parameter tuning, fast initial convergence, and robust empirical performance. Non-asymptotic error bounds have been obtained for constant stepsize SA [17, 60]. Recent work further provides fine-grained characterization of the distributional and steady-state behaviors of the iterates [20, 33, 45, 67, 69]. Two recurring themes in these results are weak convergence of the distribution of \(\theta_{t}\) and the presence of an asymptotic bias \(\mathbb{E}[\theta_{\infty}]-\theta^{*}\propto\alpha\), both having important implications for iterate averaging, bias reduction and statistical inference [34].

Note that most previous work studied the nonlinear update setting and Markovian data setting _separately_--e.g., in [20, 67] for nonlinear SGD with i.i.d. data, and in [33, 34] for Markovian linear SA. The linearity or i.i.d. assumptions imposed in these prior works are restrictive, especially in the face of modern machine/reinforcement learning paradigms where nonlinear models are the norm and dependent data is common. Moreover, the absence of prior work dealing with Markovian nonlinear SA is not merely an overlook--as argued below, this setting is significantly more challenging.

Our ContributionsIn this work, we study constant-stepsize SA with _both_ Markovian data and nonlinear update. In Section 3, we elucidate the new challenges that arise from the simultaneous presence of these two structures, which break key steps in previous analyses of the i.i.d. or linear setting. Due to the interaction between these two structures, establishing weak convergence is far from obvious, and the asymptotic bias exhibits new behaviors. Consequently, analyzing the nonlinear Markovian setting requires more than simply combining previous techniques.

To address the above confounding complication, we exploit the smoothness and recurrence structures of the SA update, thereby developing a fine-grained analysis of the correlation of the parameter \(\theta_{k}\) and data \(x_{k}\). This allows us to establish for the first time the weak convergence of the joint process \((x_{k},\theta_{k})_{k\geq 0}\) to a unique invariant distribution, represented by the limiting random variable \((x_{\infty},\theta_{\infty})\). As a by-product of our analysis, we derive finite-time bounds on \(\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2p}]\), the \(2p\)-th moments of the errors, generalizing the results in [17, 20, 60] to higher moments and to the nonlinear Markovian setting. In addition, we prove a Central Limit Theorem (CLT) for averaged iterates.

Moreover, we show that nonlinearity and Markovian structure contribute in a _multiplicative_ way to the asymptotic bias of the SA iterates. We obtain the following bias characterization: \(\mathbb{E}[\theta_{\infty}^{(\alpha)}]-\theta^{*}=\alpha(b_{\text{m}}+b_{ \text{n}}+b_{\text{c}})+\mathcal{O}((\alpha\tau_{\alpha})^{3/2}).\) We provide explicit expressions for the vectors \(b_{\text{m}},b_{\text{n}},b_{\text{c}}\), which are independent of \(\alpha\). \(b_{\text{m}}\) represents the bias component due to Markovian data (quantified by the mixing property of \(x_{k}\)), and \(b_{\text{n}}\) the bias due to the nonlinearity of \(g\) (quantified by the second derivative \(g^{\prime\prime}\)). Importantly, we identify the compound term \(b_{\text{c}}\), which is absent in both nonlinear SA with i.i.d. data and linear SA with Markovian data. We explore the algorithmic implications of the above results on Polyak-Ruppert (PR) averaging [35, 56, 59] and Richardson-Romberg (RR) extrapolation [31]. We show that PR averaging reduces the variance but not the bias, whereas RR extrapolation eliminates the leading bias term \(\alpha(b_{\text{m}}+b_{\text{n}}+b_{\text{c}})\), reducing the asymptotic bias to a higher order of \(\alpha\).

Related WorkPostponing a detailed literature review to Section 6, here we remark on the recent works most relevant to ours [1, 44, 45, 46], all studying Markovian nonlinear SA. The authors of [1] present an _upper bound_ for the PR-averaged iterates and, similar to our results, demonstrate the effectiveness of RR-extrapolation in reducing bias, but lack weak convergence result for last iterates. In [44], the authors suggest adopting the ordinary differential equation framework to prove weak convergence of iterates and derive an _upper bound_ for the asymptotic bias, which contrasts with our equality characterization with a closed-form solution for the leading-order bias. In [45], the authors prove weak convergence of \((x_{t},\theta_{t})\) using coupling, but only in the linear setting, _not_ for nonlinear SA. In the latter setting, their weak convergence analysis is thwarted by challenges similar to what we elucidate in Section 3, the interplay between nonlinearity and Markovian data leading to "double recursions." The coupling technique differs as well: we couple two processes by sharing data \(x_{t}=x_{t}^{\prime}\), while in [45], they initialize two processes with different \(x_{0}\) and \(x_{0}^{\prime}\), and analyze the stopping time \(\tau\) when \(\theta_{\tau}=\theta_{\tau}^{\prime}\). Moreover, they only present an _upper bound_ for asymptotic bias, while ours presents a fine-grained characterization in Theorem 4.6_necessary_ for justifying RR-extrapolation. Lastly, the paper [46] discusses stepsize selection and its impact on the asymptotic statistics of PR-averaged SA with both constant and diminishing stepsizes.

NotationsThe Euclidean norm is denoted by \(\|\cdot\|\). The notation "\(u\otimes v\)" represents the tensor product between vectors \(u\) and \(v\), and "\(u^{\otimes k*}\) denotes the \(k\)-th tensor power of vector \(u\). The ball with radius \(\beta\) is \(B(\beta):=\{\theta\in\mathbb{R}^{d}:\|\theta\|\leq\beta\}\). \(\mathcal{L}(z)\) denotes the distribution of a random vector \(z\) and \(\mathrm{Var}(z)\) its covariance matrix. Let \(\mathcal{P}_{2}(\mathbb{R}^{d})\) be the space of square-integrable distributions on \(\mathbb{R}^{d}\) and \(\mathcal{P}_{2}(\mathcal{X}\times\mathbb{R}^{d})\) be the space of distributions \(\bar{\nu}\) on \(\mathcal{X}\times\mathbb{R}^{d}\) with square-integrable second marginal on \(\mathbb{R}^{d}\). The Wasserstein-2 between two probability measures \(\mu\) and \(\nu\) in \(\mathcal{P}_{2}(\mathbb{R}^{d})\) is defined as \(W_{2}(\mu,\nu)=\inf_{\psi\in\Pi(\mu,\nu)}\big{\{}\left(\mathbb{E}[\|\theta- \theta^{\prime}\|^{2}]\right)^{\frac{1}{2}}:\mathcal{L}(\theta)=\mu,\mathcal{ L}(\theta^{\prime})=\nu\big{\}},\) where \(\Pi(\mu,\nu)\) is the set of all couplings between \(\mu\) and \(\nu\). Extending to \(\mathcal{X}\times\mathbb{R}^{d}\), we define the metric \(\bar{d}\big{(}(x,\theta),(x^{\prime},\theta^{\prime})\big{)}:=\sqrt{\mathbbm{1 }\{x\neq x^{\prime}\}+\|\theta-\theta^{\prime}\|^{2}},\) and denote by \(\bar{W}_{2}\) the extended Wasserstein-2 distance w.r.t. \(\bar{d}\).

The lowercase letter \(c\) and its derivatives \(c^{\prime},c_{0}\), etc. denote universal numerical constants, whose value may change from line to line. We use \(s\equiv s(\theta_{0},\theta^{*},\mu,L,R)\) and its derivatives to denote quantities (scalars, vectors, or matrices) that are independent of the stepsize \(\alpha\) and the iteration index \(k\), but may depend on the initialization \(\theta_{0}\), SA primitives \(\theta^{*}\), \(\mu\) and \(L\), and the coefficient \(R\) for the geometric mixing rate of \((x_{k})\) in Assumption 1. As we are primarily interested in dependence on \(\alpha\) and \(k\), we adopt the following big-O notation: \(\|f\|=\mathcal{O}(h(\alpha,k))\) if it holds that \(\|f\|\leq s\cdot\|h(\alpha,k)\|\).

## 2 Problem Setup and Preliminaries

Let \((x_{k})_{k\geq 0}\) be a Markov chain on a general state space \(\mathcal{X}\). Consider the following projected stochastic approximation (SA) iteration:

\[\theta_{k+1}^{(\alpha)}=\Pi_{B(\beta)}\Big{[}\theta_{k}^{(\alpha)}+\alpha \big{(}g(\theta_{k}^{(\alpha)},x_{k})+\xi_{k+1}(\theta_{k}^{(\alpha)})\big{)} \Big{]},\] (2.1)

where \(g:\mathbb{R}^{d}\times\mathcal{X}\to\mathbb{R}^{d}\) is a deterministic function, \(\{\xi_{k}\}_{k\geq 1}\) are i.i.d. zero-mean random fields, \(\alpha>0\) is a constant stepsize, and \(\Pi_{B(\beta)}(\theta):=\arg\min_{z:\|z\|\leq\beta}\|z-\theta\|\) is the projection operator. We shall omit the superscript \({}^{(\alpha)}\) in \(\theta_{k}\) when the dependence on \(\alpha\) is clear from the context. In this work, we also consider the projection-free variant of the iteration (2.1) with \(\beta=\infty\).

We denote by \(\pi\) the stationary distribution of the Markov chain \((x_{k})_{k\geq 1}\) and define the shorthand \(\bar{g}(\theta):=\mathbb{E}_{\pi}[g(\theta,x)],\) where \(\mathbb{E}_{\pi}[\cdot]\) denotes the expectation with respect to \(x\sim\pi\). The algorithm (2.1) computes an estimation of the target vector \(\theta^{*}\) that solves the steady-state equation \(\mathbb{E}_{\pi}[g(\theta,x)]=0\). Our general goal is to characterize the relationship between the iterate \(\theta_{k}\) and the target solution \(\theta^{*}\).

In the following, we state the assumptions needed for our main results. For a more detailed discussion of the assumptions, we refer readers to Appendix B.

**Assumption 1** (Uniform Ergodicity).: \((x_{k})_{k\geq 0}\) _is a uniformly ergodic Markov chain on a countable state space \((\mathcal{X},\mathcal{B}(\mathcal{X}))\) with transition kernel \(P\) and a unique stationary distribution \(\pi\). That is, there exist constants \(r\in[0,1)\) and \(R>0\) such that \(\|P^{k}(x,\cdot)-\pi\|_{\mathrm{TV}}\leq Rr^{k},\forall x\in\mathcal{X}\)._

The countable state space ensures separability under the \(\mathbbm{1}\{x\neq x^{\prime}\}\) metric, necessary for constructing a valid coupling in the invariance proof and establishing a well-defined \(P^{*}\) for bias characterization. We keep the notation general to allow future extensions to general state space and broader applicability of our results. All irreducible, aperiodic, and finite state space Markov chains are uniformly ergodic. The uniform ergodicity assumption is common in prior work on SA with Markovian noise [6, 21, 24, 33, 48]. Relaxing this uniform ergodicity assumption, in the style of [45, 52, 60] is possible but orthogonal to our focus, and thus we do not pursue this direction in this work.

We allow the chain \((x_{k})_{k\geq 0}\) to be arbitrarily initialized rather than from the stationary distribution \(\pi\). An important quantity is the mixing time of the Markov chain, defined as follows.

**Definition 2.1**.: _For \(\epsilon\in(0,1)\), the \(\epsilon\)-mixing time of \((x_{k})_{k\geq 0}\), denoted by \(\tau_{\epsilon}\geq 1\), is defined as \(\tau_{\epsilon}:=\min\big{\{}k\geq 1:\sup_{x\in X}\|P^{k}(x,\cdot)-\pi\|_{ \mathrm{TV}}\leq\epsilon\big{\}}.\)_

Under Assumption 1, the \(\epsilon\)-mixing time satisfies \(\tau_{\epsilon}\leq K\log\frac{1}{\epsilon}\) for all \(\epsilon\in(0,1)\), where \(K\geq 1\) is independent of \(\epsilon\). In the sequel, unless otherwise specified, we always choose \(\epsilon=\alpha\) and let \(\tau\equiv\tau_{\alpha}\).

The following assumptions on the nonlinear function \(g\) in (2.1) is standard in the literature [17, 20, 32, 45, 48]. A wide family of \(g\) functions satisfies these assumptions, with the \(L_{2}\)-regularized logistic regression of GLM being a standard example.

**Assumption 2** (Differentiability and Linear Growth).: _For each \(x\in\mathcal{X}\), the function \(g(\theta,x)\) is three times continuously differentiable in \(\theta\) with uniformly bounded first to third derivatives, i.e., \(\sup_{\theta\in\mathbb{R}^{d}}\|g^{(i)}(\theta,x)\|<+\infty\) for \(i=1,2,3\), \(x\in\mathcal{X}\). Moreover, there exists a constant \(L_{1}>0\) such that (1)\(\|g^{(i)}(\theta,x)-g^{(i)}(\theta^{\prime},x)\|\leq L_{1},\) for all \(\theta,\theta^{\prime}\in\mathbb{R}^{d}\), \(i=0,1,2\) and \(x\in\mathcal{X}\), and (2) \(\|g(0,x)\|\leq L_{1}\) for all \(x\in\mathcal{X}\)._

The linear growth condition in Assumption 2 implies that \(g(\theta,x)\) is \(L_{1}\)-Lipschitz w.r.t. \(\theta\) uniformly in \(x\). When \(g\) is a linear function, i.e., \(g(\theta,x)=A(x)\theta+b(x)\), this assumption is satisfied with \(\sup_{x\in\mathcal{X}}\|A(x)\|<\infty\) and \(\sup_{x\in\mathcal{X}}\|b(x)\|<\infty\), which are commonly assumed for linear SA. The above assumption immediately implies that the growth rate of \(\|g\|\) and \(\|\bar{g}\|\) will be at most linear in \(\theta\), i.e., \(\|g(\theta,x)\|\leq L_{1}(\|\theta-\theta^{*}\|+1)\) and \(\|\bar{g}(\theta)\|\leq L_{1}(\|\theta-\theta^{*}\|+1)\).

**Assumption 3** (Strong Monotonicity).: _There exists \(\mu>0\) such that \(\langle\theta-\theta^{\prime},\bar{g}(\theta)-\bar{g}(\theta^{\prime})\rangle \leq-\mu\|\theta-\theta^{\prime}\|^{2},\,\forall\theta,\theta^{\prime}\in \mathbb{R}^{d}\). Consequently, the target equation \(\bar{g}(\theta)=0\) has a unique solution \(\theta^{*}\)._

When \(g\) is a gradient field, Assumption 3 is equivalent to strong convexity. For notational simplicity, we assume the strong monotonicity parameter satisfies \(\mu\leq 1-r\), where \(r\) is the convergence factor in Assumption 1. For general \(\mu\), our results remain valid with \(\mu\) replaced by \(\min\{\mu,1-r\}\).

We next consider the noise. Denote by \(\mathcal{F}_{k}\) the filtration generated by \(\{x_{t},\theta_{t},\xi_{t+1}\}_{t=0}^{k-1}\cup\{x_{k},\theta_{k}\}\).

**Assumption 4** (Noise Sequence).: _Let \(p\in\mathbb{Z}_{+}\) be given. The noise sequence \((\xi_{k})_{k\geq 1}\) is a collection of i.i.d. random fields satisfying the following conditions with \(L_{2,p}>0\):_

\[\mathbb{E}[\xi_{k+1}(\theta)|\mathcal{F}_{k}]=0\quad\text{and}\quad\mathbb{E} ^{1/(2p)}[\|\xi_{1}(\theta)\|^{2p}]\leq L_{2,p}(\|\theta-\theta^{*}\|+1),\quad \forall\theta\in\mathbb{R}^{d}.\] (2.2)

_Define \(C(\theta)=\mathbb{E}[\xi_{1}(\theta)^{\otimes 2}]\) and assume that \(C(\theta)\) is at least twice differentiable. There also exist \(M_{\epsilon},k_{\epsilon}\geq 0\) such that for \(\theta\in\mathbb{R}^{d}\), we have \(\max_{i=1,2}\left\|C^{(i)}(\theta)\right\|\leq M_{\epsilon}\left\{1+\|\theta- \theta^{*}\|^{k_{\epsilon}}\right\}\)._

In the sequel, we set \(L:=L_{1}+L_{2}\), and without loss of generality, we assume \(L\geq 1\).

When \(p=1\), the second inequality in (2.2) only requires linear growth _in expectation_, which relaxes the almost sure linear growth condition in [17]. The constraint on the covariance matrix \(C(\theta)\) is lenient and satisfied in most regular enough settings, as shown in [20].

## 3 Analytical Challenges and Techniques

In this section, we elaborate on the challenges and techniques in proving the above results.

Previous work has established weak convergence of \((x_{k},\theta_{k})\) separately for nonlinear SA with i.i.d. data, and for Markovian linear SA. The high-level approaches used in two representative prior works can be summarized as follows. The work [20] on nonlinear SGD leverages _local linearization_ of \(g\) through Taylor expansion. The work [33] on Markovian linear SA exploits the mixing property of the Markovian noise to regain approximate independence, particularly between \(x_{k}\) and \(\theta_{k-\tau}\) for sufficiently large \(\tau\). It is tempting to expect that nonlinear SA can be analyzed by combining these two approaches. Perhaps surprisingly, such a simple combination would not work due to the interplay between nonlinearity and Markovian structures.

To demonstrate this challenge, let us seek to establish weak convergence in the Wasserstein distance \(W_{2}\) via forward coupling [29], an approach employed by both [20, 33] as well as others [25]. Specifically, we consider two SA iterate sequences \((\theta_{k}^{[1]})_{k\geq 0}\) and \((\theta_{k}^{[2]})_{k\geq 0}\) from different initializations \(\theta_{0}^{[1]}\) and \(\theta_{0}^{[2]}\) coupled by sharing the data sequence \((x_{k})_{k\geq 0}\): \(\theta_{k+1}^{[1]}=\theta_{k}^{[1]}+\alpha g(\theta_{k}^{[1]},x_{k})\) and \(\theta_{k+1}^{[2]}=\theta_{k}^{[2]}+\alpha g(\theta_{k}^{[2]},x_{k})\). To establish convergence in \(W_{2}\), we consider the difference sequence

\[w_{k+1}:=\theta_{k+1}^{[1]}-\theta_{k+1}^{[2]}=w_{k}+\alpha\big{(}g(\theta_{k}^ {[1]},x_{k})-g(\theta_{k}^{[2]},x_{k})\big{)},\] (3.1)

and it suffices to prove \(w_{k}\) converges to \(0\) in mean square: \(\mathbb{E}[\|w_{k+1}\|^{2}]\lesssim\rho^{k}\mathbb{E}[\|w_{0}\|^{2}]\) for \(\rho<1\).

With this goal in mind and following the idea from [20], one may first linearize the right-hand side of the difference dynamic (3.1) and obtain the approximation

\[w_{k+1}\approx w_{k}+\alpha g^{\prime}(\theta_{k}^{[2]},x_{k})w_{k}.\] (3.2)Next, to analyze the drift of the Lyapunov function \(\mathbb{E}[\|w_{k}\|^{2}]\) and handle the Markovian noise \((x_{k}),\) we use the conditioning technique from [33]. We condition on the information of \(\tau\) steps before, denoted by \(\mathcal{F}_{k-\tau}:=\sigma((\theta_{t}^{[1]},\theta_{t}^{[2]},x_{t}):t\leq k-\tau).\) Ignoring higher-order terms and assuming a one-dimensional problem for simplicity, we obtain that

\[\mathbb{E}[\|w_{k+1}\|^{2}] \approx\mathbb{E}\Big{[}\mathbb{E}\big{[}\|w_{k}\|^{2}\big{(}1+2 \alpha g^{\prime}(\theta_{k}^{[2]},x_{k})\big{)}\mid\mathcal{F}_{k-\tau}\big{]} \Big{]}\] \[\approx\mathbb{E}\Big{[}\|w_{k-\tau}\|^{2}\big{(}1+2\alpha \mathbb{E}\big{[}g^{\prime}(\theta_{k}^{[2]},x_{k})\mid\mathcal{F}_{k-\tau} \big{]}\big{)}\Big{]},\] (3.3)

where we use \(w_{k}\approx w_{k-\tau}\) for small \(\alpha\) (this argument, which is made precise in [33, 60], essentially exploits the fact that \(x_{k}\) evolves faster than \(\theta_{k}\)).

To prove dynamic (3.3) converges, it boils down to showing the "gain matrix" \(\mathbb{E}\big{[}g^{\prime}(\theta_{k}^{[2]},x_{k})\mid\mathcal{F}_{k-\tau} \big{]}\) is negative/Hurwitz. To further simplify, we assume \(k\) is large so that the chain \((x_{k})\) is distributed per its stationary distribution \(\pi,\) in which case the gain matrix simplifies to \(\mathbb{E}_{x_{\infty}\sim\pi}[g^{\prime}(\theta_{\infty}^{[2]},x_{\infty})].\)

Analyzing this gain matrix is where our analysis diverges from previous work. If the SA update were _linear_, i.e., \(g(\theta,x)=A(x)\theta,\) then the gain \(\mathbb{E}[g^{\prime}(\theta_{\infty}^{[2]},x_{\infty})]=\mathbb{E}_{\pi}[A(x _{\infty})]\) would be independent of \(\theta_{\infty}^{[2]},\) and its Hurwitz property is a standard and necessary condition for proving convergence of linear SA. If the data sequence \((x_{k})\) were _i.i.d._, then \(\theta_{k}\) would be _independent_ of \(x_{k}\) and hence the gain becomes \(\mathbb{E}[g^{\prime}(\theta_{\infty}^{[2]},x_{\infty})]=\mathbb{E}[\mathbb{ E}[g^{\prime}(\theta_{\infty}^{[2]},x_{\infty})|\theta_{\infty}^{[2]}]]= \mathbb{E}[\bar{g}^{\prime}(\theta_{\infty}^{[2]})]\) with \(\bar{g}(\cdot):=\mathbb{E}_{x\sim\pi}[g(\cdot,x)],\) where the Hurwitz property again follows from standard assumptions on \(\bar{g}.\)

However, both arguments fail for the _Markovian nonlinear_ setting. Common assumptions for nonlinear SA only ensure Hurwitz \(\mathbb{E}_{x\sim\pi}[g^{\prime}(\theta,x)|\theta]\) given \(\theta.\) This does not imply the desired Hurwitz \(\mathbb{E}[g^{\prime}(\theta_{\infty}^{[2]},x_{\infty})],\) precisely owing to the simultaneous presence of (i) the _dependence_ of \(g^{\prime}\) on both \(\theta_{\infty}\) and \(x_{\infty}\) (due to nonlinearity) and (ii) the _correlation_ between \(\theta_{\infty}\) and \(x_{\infty}\) (due to Markovian).

Our ApproachWe overcome this challenge by carefully analyzing the properties of the above dependence and correlation. Therefore, for sufficiently large \(\tau,\) we further decompose (3.3) as

\[\mathbb{E}[\|w_{k+1}\|^{2}]\approx\mathbb{E}\Big{[}\|w_{k-\tau}\|^ {2}\big{(}1+2\alpha\mathbb{E}\big{[}g^{\prime}(\theta_{k}^{[2]},x_{k})\mid \mathcal{F}_{k-\tau}\big{]}\big{)}\Big{]}\] \[=\mathbb{E}\Big{[}\|w_{k-\tau}\|^{2}\Big{(}1+2\alpha\underbrace{ \mathbb{E}\big{[}g^{\prime}(\theta_{k-\tau}^{[2]},x_{k})\mid\mathcal{F}_{k- \tau}\big{]}}_{\approx\mathbb{E}[g^{\prime}(\theta_{k-\tau}^{[2]},x_{\infty}) \mid\mathcal{F}_{k-\tau}]}\ \text{Hurwitz}\] \[\lesssim\rho\mathbb{E}[\|w_{k-\tau}\|^{2}]+\alpha\mathbb{E}\Big{[} \mathbb{E}\big{[}\underbrace{\langle w_{k-\tau},g(\theta_{k}^{[1]},x_{k})-g( \theta_{k}^{[2]},x_{k})-g(\theta_{k-\tau}^{[1]},x_{k})+g(\theta_{k-\tau}^{[2] },x_{k})\rangle}_{\big{\spadesuit}}\mid\mathcal{F}_{k-\tau}\big{]}\Big{]},\]

where we approximate \(w_{k}\approx w_{k-\tau},\)\(w_{k}g^{\prime}(\theta_{t}^{[2]},x_{k})\approx(g(\theta_{t}^{[1]},x_{k})-g( \theta_{k}^{[2]},x_{k}))\) for \(t=k,k-\tau\) and obtain the second term in the last inequality. Next, we propose employing two different Taylor expansions to prove that \(\spadesuit\) is of higher orders of \(\alpha\). We first apply the Taylor expansion to \(g(\theta_{k}^{[1]},x_{k})-g(\theta_{k}^{[2]},x_{k})\) and \(g(\theta_{k-\tau}^{[1]},x_{k})-g(\theta_{k-\tau}^{[2]},x_{k})\). However, this only achieves \(\spadesuit\lesssim\|w_{k}\|^{2}\big{(}\|w_{k}\|+\alpha\tau T_{1}\big{)},\) where \(T_{1}=\min(\|\theta_{k}^{[1]}\|,\|\theta_{k}^{[2]}\|,\|\theta_{k-\tau}^{[1]}\|, \|\theta_{k-\tau}^{[2]}\|)+1.\) When \(\theta_{k}^{[1]}\) and \(\theta_{k}^{[2]}\) are not close to each order, i.e., when \(\|w_{k}\|\) is large, \(\spadesuit\) is not necessarily of higher order. Therefore, we consider a second type of Taylor expansion on \(g(\theta_{k}^{[1]},x_{k})-g(\theta_{k-\tau}^{[1]},x_{k})\) and \(g(\theta_{k}^{[2]},x_{k})-g(\theta_{k-\tau}^{[2]},x_{k})\). The intuition for the second type of Taylor expansion is to analyze and bound \(\spadesuit\) by the small distance between \(\theta_{k}^{[j]}\) and \(\theta_{k-\tau}^{[j]}\) for \(j\in\{1,2\},\) even when \(\|w_{k}\|\) is large. This achieves \(\spadesuit\lesssim\|w_{k}\|\alpha\tau T_{1}\big{(}\|w_{k}\|+\alpha\tau T_{1}\big{)}.\) Simultaneously applying the two Taylor expansions will yield \(\spadesuit\lesssim\alpha\tau\|w_{k}\|^{2}T_{1}\). Finally, we overcome this challenge by carefully analyzing the boundness of \(T_{1};\) see Theorem 4.1 and its proof.

In parallel to the above coupling approach, we also explore an alternative approach by verifying the joint Markov chain \((x_{k},\theta_{k})\) satisfies certain irreducibility and Lyapunov drift conditions, which in turn imply the chain is ergodic. To apply this approach, we exploit additional properties of the SA noise, namely minorization, which is satisfied in many applications where additional randomness is injected to the SA update. While the high level strategy of this approach is well developed [23, 50],carrying out the analysis of each step is technically involved. In particular, we need to translate the minorization property of the noise to the irreducibility of the joint chain \((x_{k},\theta_{k})\), which is nontrivial in the presence of Markovian noise and nonlinearity.

## 4 Main Results

### Weak Convergence of Projected SA

Our first main result proves the ergodicity of the joint process \((x_{k},\theta_{k})_{k\geq 0}\) of the projected SA (2.1).

**Theorem 4.1** (Ergodicity of Projected SA).: _Suppose that Assumption 1-4\((p=1)\) hold. The projected SA (2.1) is applied with radius parameter \(2\|\theta^{*}\|\leq\beta<\infty\). For stepsize \(\alpha>0\) that satisfies the constraint \(\alpha\tau_{\alpha}\leq\frac{\mu}{(940\cdot 96\beta)L^{2}}\), the Markov chain \((x_{k},\theta_{k})_{k\geq 0}\) converges to a unique stationary distribution \(\bar{\nu}_{\alpha}\in\mathcal{P}_{2}(\mathcal{X}\times\mathbb{R}^{d})\). Let \(\nu_{\alpha}:=\mathcal{L}(\theta_{\infty})\) be the second marginal of \(\bar{\nu}_{\alpha}\). For \(k\geq 2\tau_{\alpha}\), it holds that_

\[W_{2}(\mathcal{L}(\theta_{k}),\nu_{\alpha})\leq\bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}),\bar{\nu}_{\alpha})\leq(1-\alpha\mu)^{k/2}\cdot s(\theta_{0}, \theta^{*},\mu,L,R).\]

Theorem 4.1 generalizes prior weak convergence results for constant stepsize SA/SGD either under i.i.d. noise [20; 67] or linear update [33; 45]. Our stepsize condition \(\alpha\tau_{\alpha}\lesssim\mu/L^{2}\) coincides with [33; 60] on linear SA, a special case of our setting.

The proof of Theorem 4.1 highlights the stabilizing effect of the projection operation in (2.1). This effect, together with the smoothness of update function \(g\), controls how the Markovian correlation propagates through the nonlinear update, allowing us to overcome the challenges discussed in Section 3. It is unclear whether our proof, which is based on Markov chain coupling, can be fully generalized to SA without projection. Nevertheless, we show that such a generalization is possible for a sub-family of nonlinear SA where \(g\) possesses the additional structure termed "asymptotic linearity", which is satisfied by, e.g., SGD applied to certain settings of logistic regression. For a formal statement of this result and proof, we refer the readers to Appendix E.

As a by-product of our analysis, we establish the following non-asymptotic \(2p\)-th moment bound on the error \(\theta_{k}-\theta^{*}\). Let \(\theta_{t+1/2}:=\theta_{t}+\alpha(g(\theta_{t},x_{t})+\xi_{t+1}(\theta_{t}))\) denote the pre-projection iterate.

**Proposition 4.2**.: _Consider \((\theta_{k})_{k\geq 0}\) of iteration (2.1) with \(\beta\in[2\|\theta^{*}\|,\infty]\). Let Assumption 1-4\((2p)\) hold. If stepsize \(\alpha\) satisfies \(\alpha\tau_{\alpha}L^{2}\leq c_{p}\mu\), with \(c_{p}\leq 1\), the following holds for all \(k\geq\tau_{\alpha}\),_

\[\mathbb{E}[\|\theta_{k+1}-\theta^{*}\|^{2p}]\leq\mathbb{E}[\|\theta_{k+1/2}- \theta^{*}\|^{2p}]\leq c_{p,1}(1-\alpha\mu)^{k+1}\mathbb{E}[\|\theta_{0}- \theta^{*}\|^{2p}]+c_{p,2}(\alpha\tau_{\alpha})^{p}\cdot s(\theta_{0},\theta^ {*},L,\mu).\]

Proposition 4.2 implies that \(\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2p}]\lesssim(\alpha\tau)^{p}\) for sufficiently large \(k\), generalizing the results of [17; 20; 60] to higher moments and the nonlinear Markovian setting. Notably, this result holds even without the projection operation in the SA update (2.1), i.e., \(\beta=\infty\). Furthermore, Proposition 4.2 can be used to derive high-probability tail bounds using the Markov inequality.

### Weak Convergence without Projection

Parallel to the coupling approach, we consider an alternative approach for establishing weak convergence via verifying irreducibility, positive Harris recurrence, and \(V\)-uniform ergodicity [50] of the Markov chain \((x_{k},\theta_{k})\). This approach applies to nonlinear SA even without projection. To verify irreducibility, we exploit the following additional noise structure.

**Assumption 5** (Noise Minorization).: _For each \(\theta\in\mathbb{R}^{d}\), the distribution of the random variable \(\xi_{1}(\theta)\), denoted by \(\zeta_{\theta}\), can be decomposed as \(\zeta_{\theta}=\zeta_{1,\theta}+\zeta_{2,\theta}\), where the measure \(\zeta_{1,\theta}\) has a density, denoted by \(p_{\theta}\), which satisfies \(\inf_{\theta\in C}p_{\theta}(t)>0\) for any bounded set \(C\) and any \(t\in\mathbb{R}^{d}\)._

A similar assumption is considered in [5; 67]. This assumption is mild and satisfied by any continuous random field supported on \(\mathbb{R}^{d}\). Introducing such (small) continuous noise is often part of the algorithm design for inducing privacy [2; 22] or exploration [28; 55]. Without Assumption 5, the chain may fail to be irreducible even when the other assumptions are satisfied; see [33] for a counterexample.

Under Assumption 5, we obtain the following ergodicity result paralleling Theorem 4.1.

**Theorem 4.3** (Ergodicity of SA - Minorization).: _Suppose that Assumption 1-3, Assumption 4\((p=1)\), and Assumption 5 hold. For stepsize \(\alpha>0\) that satisfies the constraint \(\alpha\tau_{\alpha}L^{2}<c_{2}\mu\), the Markov chain \((x_{k},\theta_{k})_{k\geq 0}\) of (2.1) with \(\beta=\infty\) is \(V\)-uniformly ergodic with Lyapunov function \(V(x,\theta)=\|\theta-\theta^{*}\|^{2}+1\) and a unique stationary distribution \(\bar{\nu}_{\alpha}\in\mathcal{P}_{2}(\mathcal{X}\times\mathbb{R}^{d})\). Moreover, defining the \(V\)-norm \(\|\nu\|_{V}:=\int|\nu(\mathrm{d}x)|V(x)\), we have_

\[\big{\|}\mathcal{L}(x_{k},\theta_{k})-\bar{\nu}_{\alpha}\big{\|}_{V}\leq\kappa \rho^{k},\qquad\forall(x_{0},\theta_{0})\in\mathcal{X}\times\mathbb{R}^{d}, \forall k\geq 0,\] (4.1)

_where the constants \(\rho\in(0,1)\) and \(\kappa\in(0,\infty)\) may depend on \(\alpha\)._

### Non-Asymptotic Convergence Rate and Central Limit Theorem

In the sequel, let \((x_{\infty},\theta_{\infty}^{(\alpha)})\) denote the random vector whose law is the stationary distribution \(\bar{\nu}_{\alpha}\) given in Theorem 4.1. As a corollary, we have geometric convergence for the first 2 moments of \(\theta_{k}\).

**Corollary 4.4** (Non-Asymptotic Convergence Rate).: _Under the setting of Theorem 4.1, for any initialization of \(\theta_{0}\in\mathbb{R}^{d}\), we have_

\[\big{\|}\mathbb{E}[\theta_{k}]-\mathbb{E}[\theta_{\infty}^{(\alpha )}]\big{\|}\leq(1-\alpha\mu)^{k/2}\cdot s^{\prime}(\theta_{0},\theta^{*},\mu,L, R),\quad\text{and}\] \[\big{\|}\mathbb{E}[\theta_{k}\theta_{k}^{\top}]-\mathbb{E}[\theta _{\infty}^{(\alpha)}(\theta_{\infty}^{(\alpha)})^{\top}]\big{\|}\leq(1-\alpha \mu)^{k/2}\cdot s^{\prime\prime}(\theta_{0},\theta^{*},\mu,L,R).\]

Moreover, the convergence rate established in Theorem 4.1 is fast enough that we can use it to prove a Central Limit Theorem for the average iterates.

**Corollary 4.5** (Central Limit Theorem).: _Under the setting of Theorem 4.1, as \(k\to\infty\) we have \(\frac{1}{\sqrt{k}}\sum_{t=0}^{k-1}\big{(}\theta_{t}-\mathbb{E}[\theta_{\infty }]\big{)}\Rightarrow\mathcal{N}(0,\Sigma^{(a)})\), where \(\Sigma^{(\alpha)}:=\lim_{k\to\infty}\frac{1}{k}\mathbb{E}\big{[}\big{(}\sum_{ t=0}^{k-1}\big{(}\theta_{t}-\mathbb{E}[\theta_{\infty}^{(\alpha)}]\big{)}\big{)} ^{\otimes 2}\big{]}\)._

Establishing the CLT sets the stage for using the SA iterates for statistical inference tasks such as confidence interval estimation. We discuss this in greater detail in Section 4.4 after characterizing the asymptotic bias, another important ingredient for using SA for inference.

### Bias Characterization

In this subsection, we characterize the asymptotic bias \(\mathbb{E}[\theta_{\infty}^{(\alpha)}]-\theta^{*}\). Understanding the bias structure has important algorithmic implications for bias reduction, which we explore in Section 4.5, as well as for more efficient statistical inference and confidence interval estimation [34].

**Theorem 4.6** (Bias Characterization).: _Suppose Assumptions 1-4\((p=3)\) hold. For each stepsize \(\alpha>0\) satisfying \(\alpha\tau_{\alpha}L^{2}<c_{3}\mu\), the following holds for some vector \(b\) independent of \(\alpha:\)_

\[\mathbb{E}[\theta_{\infty}^{(\alpha)}]-\theta^{*}=\alpha b+\mathcal{O}\big{(}( \alpha\tau_{\alpha})^{3/2}\big{)}.\] (4.2)

_More specifically, the leading bias can be decomposed as \(b=b_{\text{m}}+b_{\text{n}}+b_{\text{c}}\), where_

\[b_{\text{m}} =-(\bar{g}^{\prime}(\theta^{*}))^{-1}\mathbb{E}[g^{\prime}(\theta ^{*},x_{\infty})h(\theta^{*},x_{\infty})],\] (4.3) \[b_{\text{n}} =\frac{1}{2}(\bar{g}^{\prime}(\theta^{*})^{-1}\bar{g}^{\prime \prime}(\theta^{*})A\Big{(}\mathbb{E}[g(\theta^{*},x_{\infty})^{\otimes 2}]+ \mathbb{E}[(\xi_{1}(\theta^{*}))^{\otimes 2}]\Big{)},\] (4.4) \[b_{\text{c}} =\frac{1}{2}(\bar{g}^{\prime}(\theta^{*})^{-1}\bar{g}^{\prime \prime}(\theta^{*})A\Big{(}\mathbb{E}[g(\theta^{*},x_{\infty})\otimes h(\theta^ {*},x_{\infty})]+\mathbb{E}[h(\theta^{*},x_{\infty})\otimes g(\theta^{*},x_{ \infty})]\Big{)},\] (4.5)

_with \(A=(\bar{g}^{\prime}(\theta^{*})\otimes I+I\otimes\bar{g}^{\prime}(\theta^{*}) )^{-1}\) and \(h(\theta^{*},x)=\int_{\mathcal{X}}(I-P^{*}+\Pi)^{-1}(P^{*}-\Pi)(x,\mathrm{d}x^{ \prime})g(\theta^{*},x^{\prime})\), with the kernel \(P^{*}\) being a regular conditional probability on \(\mathcal{X}\) that satisfies \(\int_{B}\pi(\mathrm{d}x)P(x,C)=\int_{C}\pi(\mathrm{d}y)P^{*}(y,B)\), for all \(B,C\in\mathcal{B}(\mathcal{X})\)._

We defer the detailed proof to Appendix I. A few remarks are in order. First, we emphasize that (4.2) is essentially an equality, indicating a non-zero bias of order \(\alpha\) whenever \(b\neq 0\) (up to higher order terms). Notably, the Polyak-Ruppert averaging of the iterates cannot eliminate this bias. Note that the bias expansion in (4.2) applies to both weakly converged projected and non-projected SA. Our analysis shows that compared with the non-projected SA, the projection operator induces an extra bias term of the order \(\mathcal{O}(\alpha^{2}\tau_{\alpha}^{3})\), which is negligible relative to the main terms in in (4.2).

More importantly, Theorem 4.6 provides an explicit expression of the leading bias, which decomposes into three components: the Markovian part, the nonlinearity contribution, and a _compound_ term,which is unique in nonlinear Markovian SA. Specifically, \(b_{\text{m}}\) in (4.3) is associated with the Markovian multiplicative noise, where the matrix \(P^{*}-\Pi\) in the \(h\) function determines the mixing time of the data sequence \((x_{k})_{k>}\). The term \(b_{\text{n}}\) in (4.4) is linked to nonlinearity, as reflected by the Hessian term \(\bar{g}^{\prime\prime}(\theta^{*})=\mathbb{E}[g^{\prime\prime}(\bar{\theta}^{* },x_{\infty})]\), which quantifies the nonlinearity of \(g\) and is equal to zero in the case of a linear \(g\). Lastly, \(b_{\text{c}}\) in (4.5) is the _compound_ term, due to its dependence on both the Markov noise (\(h\) function) and the nonlinearity measure \(\bar{g}^{\prime\prime}\). In particular, we note the following two special cases: (1) When \(g\) is a linear function, \(\bar{g}^{\prime\prime}(\theta^{*})=0\). Hence, \(b_{\text{n}}=b_{\text{c}}=0\), and \(b_{\text{m}}\) recovers the result in [33]; (2) When \((x_{k})_{k\geq 0}\) is i.i.d. sampled from the stationary distribution \(\pi\), we have \(h(\theta^{*},x)\equiv 0\)\(\forall x\in\mathcal{X}\), for \(P=P^{*}=\Pi\). As such, \(b_{\text{m}}=b_{\text{c}}=0\), recovering the result in [20]. The presence of the compound term \(b_{\text{c}}\) suggests that as the SA structure becomes more nonlinear and the underlying Markov chain mixes more slowly, the impact on the bias is _multiplicative_ rather than simply additive, a surprising phenomenon not unveiled in previous studies. It is possible to improve the residual order from \(\mathcal{O}(\alpha^{3/2})\) to \(\mathcal{O}(\alpha^{2})\) with a more refined characterization of the asymptotic second moment \(\mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}]\) by following a similar strategy as our current approach. We leave this refinement out of the scope of the current paper.

### Algorithmic Implications

We examine the practical implications of our weak convergence and bias characterization results, particularly for Polyak-Ruppert (PR) tail averaging and Richardson-Romberg (RR) extrapolation. In this subsection, we focus on the dependence on the stepsize \(\alpha\) and iteration index \(k\), and make use of the big-O notation from Section 1. Recall that \(b\) is the bias vector defined in Theorem 4.6.

PR averaging [56; 59] is a classical approach for reducing the variance and accelerating the convergence of SA. Here we consider the tail-averaging variant of PR averaging, defined as \(\bar{\theta}_{k_{0},k}:=\frac{1}{k-k_{0}}\sum_{t=k_{0}}^{k-1}\theta_{t},\) for \(k\geq k_{0}\), with a user-specified burn-in period \(k_{0}\geq 0\) (a common choice is \(k_{0}=k/2\)). The following corollary, proved in Appendix J, provides a non-asymptotic bound on the mean squared error (MSE) for the averaged iterates \(\bar{\theta}_{k_{0},k}\).

**Corollary 4.7** (Tail Averaging).: _Under the setting of Theorem 4.6, the tail-averaged iterates satisfy the following bounds for all \(k>k_{0}+2\tau_{\alpha}\) and \(k_{0}\geq\tau_{\alpha}+\frac{1}{\alpha\mu}\log\big{(}\frac{1}{\alpha\tau_{ \alpha}}\big{)}\),_

\[\mathbb{E}\Big{[}\|\bar{\theta}_{k_{0},k}-\theta^{*}\|^{2}\Big{]}=\underbrace {\alpha^{2}\|b\|^{2}+\mathcal{O}\Big{(}\alpha\cdot(\alpha\tau)^{\frac{3}{2}} \Big{)}}_{T_{1}:\text{asymptotic squared bias}}+\underbrace{\mathcal{O}\Big{(} \frac{\tau_{\alpha}}{k-k_{0}}\Big{)}}_{T_{2}:\text{variance}}+\underbrace{ \mathcal{O}\Big{(}\frac{(1-\alpha\mu)^{k_{0}/2}}{\alpha\left(k-k_{0}\right)^ {2}}\Big{)}}_{T_{3}:\text{optimization error}}.\]

Corollary 4.7 shows that the MSE can be decomposed into three terms and elucidates how these terms depend on \(\alpha,k\), and other problem parameters. In particular, the term \(T_{1}\) corresponds to the asymptotic squared bias \(\|\mathbb{E}[\theta_{\infty}^{(\alpha)}-\theta^{*}]\|^{2}\), which is not affected by averaging. The term \(T_{2}\) is associated with the variance \(\operatorname{Var}(\bar{\theta}_{k_{0},k})\), which decays at rate \(1/k\) due to averaging. Lastly, the term \(T_{3}\) represents the optimization error \(\|\mathbb{E}\bar{\theta}_{k_{0},k}-\theta_{\infty}\|^{2}\), which decays geometrically in \(k_{0}\) thanks to the use of a constant stepsize \(\alpha\) and the tail-averaging procedure.

Note that averaging does not affect the bias of order \(\alpha\). With the precise bias characterization in Theorem 4.6, we can order-wise reduce the bias to \(\mathcal{O}(\alpha^{3/2})\) by employing the RR extrapolation technique [61]. Let \(\bar{\theta}_{k_{0},k}^{(\alpha)}\) and \(\bar{\theta}_{k_{0},k}^{(2\alpha)}\) denote the tail-averaged iterates using two stepsizes \(\alpha\) and \(2\alpha\) with the same data \((x_{k})_{k\geq 0}\). The RR extrapolated iterates are defined as \(\widetilde{\theta}_{k_{0},k}^{(\alpha)}=2\bar{\theta}_{k_{0},k}^{(\alpha)}- \bar{\theta}_{k_{0},k}^{(2\alpha)}\).

**Corollary 4.8** (RR-Extrapolation).: _Under the setting of Theorem 4.6, the RR-extrapolated iterates satisfy the following bounds for all \(k>k_{0}+2\tau_{\alpha}\) and \(k_{0}\geq\tau_{\alpha}+\frac{1}{\alpha\mu}\log\big{(}\frac{1}{\alpha\tau_{ \alpha}}\big{)}\),_

\[\mathbb{E}\Big{[}\|\widetilde{\theta}_{k_{0},k}-\theta^{*}\|^{2}\Big{]}= \mathcal{O}\Big{(}(\alpha\tau_{\alpha})^{3}\Big{)}+\mathcal{O}\Big{(}\frac{ \tau_{\alpha}}{k-k_{0}}\Big{)}+\mathcal{O}\Big{(}\frac{(1-\alpha\mu)^{k_{0}/ 2}}{\alpha\left(k-k_{0}\right)^{2}}\Big{)}.\]

Backed by the CLT in Corollary 4.5, the iterates of constant-stepsize SA can be used to construct confidence intervals of \(\theta^{*}\). For i.i.d. data or linear SA, this approach has been explored in [34; 67; 66; 47] along with an appropriate variance estimator [26; 66]. In our Markovian nonlinear setting, where the iterates are biased, it is crucial to use RR extrapolation for bias reduction. Once the bias is accounted for, the power of using constant stepsizes reveals itself as it leads to rapid mixing and low correlation of the iterates. Together, they lead to efficient confidence interval estimation schemes using nonlinear Markovian SA; see the empirical results in [34] showing its efficacy. In contrast, the classical diminishing stepsize paradigm often suffers from high correlation [13] and in turn inaccurate variance estimation, resulting in unsatisfactory coverage probability with finite data [34].

### Implications for Learning GLM

Generalized linear models (GLM) extend linear regression to the model \(\mathbb{E}[Y|W]=\sigma(W^{\top}\theta^{*})\), where \(W\) is the covariate, \(Y\) the response variable, and \(\sigma\) is called the _mean function_. For any monotone (and potentially nonlinear) \(\sigma\), the powerful framework developed in [19; 36; 37; 64] allows one to formulate the estimation of \(\theta^{*}\) as minimizing an appropriate _convex_ (surrogate) loss function. Applying SGD to this loss leads to a nonlinear SA update, to which our results are applicable. Below we discuss their applications in two concrete examples of GLMs.

Logistic RegressionLogistic regression uses a sigmoid mean function \(\sigma(x)=\frac{1}{1+\exp(-x)}\). Suppose the covariate \(w_{k}\) is sequentially sampled from a uniformly ergodic Markov chain with a bounded state space \(\mathcal{W}\subset\mathbb{R}^{d}\), and conditioned on \(w_{k}\) the response \(y_{k}\) is Bernoulli distributed with parameter \((1+\exp(-w_{k}^{\top}\theta^{*}))^{-1}\). SGD applied to the \(L_{2}\)-regularized negative log-likelihood function takes the form of the SA update \(\theta_{k+1}=\theta_{k}+\alpha g(\theta_{k},x_{k})\), where \(x_{k}=(w_{k},y_{k})\in\mathcal{W}\times\{0,1\}\) and \(g(\theta_{k},x_{k})=-w_{k}\big{(}\sigma(-w_{k}^{\top}\theta_{k})-y_{k}\big{)} -\lambda\theta_{k}\). For simplicity, we do not consider \(\xi\)-perturbation, i.e., \(\xi_{k+1}(\theta_{k})\equiv 0\). It is easy to verify that this \(g\) is strongly monotone and sufficiently smooth with at most linear growth in \(|\theta|\), hence satisfying Assumption 1-3. Therefore, all the results in Sections 4.1-4.5 apply to logistic regression with constant stepsizes and Markovian data.

Smooth ReLU RegressionThe mean function \(\sigma\) can be interpreted as playing a similar role as the activation function in neural networks. Widely adopted is ReLU activation \(\sigma(x)=\max(0,x)\) as well as its various smooth approximations [7; 30]. The problem of learning \(\theta^{*}\) in this setting, sometimes called ReLU Regression, has been studied in the last decade and recently regained attention [19; 36; 37; 64]. Unlike linear or logistic regression, the least squares and maximum likelihood formulation associated with such nonlinear mean functions \(\sigma\) is non-convex. Nevertheless, the convex surrogate loss framework in [19; 64] still applies. As an example, we focus on the SoftPlus activation \(\sigma(x)=\log(1+\exp(\iota x))/\iota\) with a temperature parameter \(\iota>0\)[30]. With \(L_{2}\)-regularization the resulting SGD iteration is \(\theta_{k+1}=\theta_{k}-\alpha\big{(}w_{k}\big{(}\frac{1}{\iota}\log(1+\exp( \iota w_{k}^{\top}\theta_{k}))-y_{k}\big{)}+\lambda\theta_{k}\big{)},\) where the covariate-response pair \((\theta_{k},x_{k})\) is as before. This problem can again be cast as nonlinear SA with a strongly monotone and smooth \(g\), satisfying Assumptions 1-3. All results in Sections 4.1-4.5 apply.

## 5 Numerical Experiments

In this section, we provide numerical experiment results to verify our theoretical results. We run SGD on \(L_{2}\)-regularized logistic regression with Markovian data and constant stepsizes, where the covariate \(x_{t}\in\mathbb{R}\) is sequentially sampled from an autoregressive (AR) model of order 1; specifically, \(x_{t+1}=0.9x_{t}+\zeta_{t+1}\) with \(\zeta_{t}\) i.i.d. following \(N(0,1)\), and the stationary distribution is \(x_{\infty}\sim N(0,1/(1-0.9^{2}))\). The binary dependent variable \(y_{t}\) is sampled from Bernoulli\((1/(1+\exp(-w^{*}x_{t}))\) with \(w^{*}=1\). The regularized parameter is set to \(\lambda=0.0001\).1

Footnote 1: The experiments were conducted on two sockets of Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz with 566Gb of RAM. Implementation details and code are available at https://github.com/lucyhuodongyan/nonlinear-sa-bias.

To examine the asymptotic bias, we run the experiment for an episode length of \(10^{7}\), with Markovian data as well as i.i.d. data sampled from \(N(0,1/(1-0.9^{2}))\). We plot the errors (distance to \(\theta^{*}\)) of the PR averaged iterates and the RR extrapolated iterates, for different stepsizes \(\alpha\). Figure 1(a) verifies the presence of an asymptotic bias approximately proportional to the stepsize \(\alpha\), and illustrates the effectiveness of RR extrapolation in reducing this bias. In Figure 1(b), we compare the bias under Markovian data (\(x_{t+1}\sim P(\cdot|x_{t})\)) and i.i.d. data (\(x_{t}\sim x_{\infty}\)). Interestingly, Figure 1(b) reveals that Markovian data does not necessarily lead to a larger bias than i.i.d. data. This is consistent with our theory, as the three bias terms \(b_{m},b_{n},b_{c}\) may have opposite signs leading to cancellation. This result suggests that in the presence of nonlinearity, one should not avoid Markovian data simply for the sake of reducing bias. Rather, RR extrapolation may be more effective for bias reduction.

To verify the CLT in Corollary 4.5, we repeat the experiment 1000 times with an episode length of \(10^{6}\) and stepsize \(\alpha=0.8\). We compute the PR averaged iterates and plot the histogram and the quantile-quantile (QQ) plot in Figure 2 in Appendix C. The close alignment between the histogram and the normal curve in Figure 2(a) and the linearity of the points along the 45-degree reference line in the QQ plot in Figure 2(b) confirm that the empirical distribution follows a normal distribution.

## 6 Related Work

**General SA and SGD.** SA and SGD can be traced back to the seminal work of [58]. Classical work assumes a diminishing stepsize sequence, and has shown almost sure asymptotic convergence to \(\theta^{*}\)[58; 8]. Subsequent works propose the iterate averaging technique, now known as Polyak-Ruppert (PR) averaging, to reduce variance and accelerate convergence [56; 59], and also establish a Central Limit Theorem for the asymptotic normality of the averaged iterates [57]. The asymptotic convergence theory of SA and SGD is well developed and extensively addressed in many exemplary textbooks, see [3; 40; 65]. There are also recent works studying the non-asymptotic convergence with diminishing stepsizes [14; 12]. The recent work [15] establishes the high probability bound on the estimation error of contractive SA with diminishing stepsize.

**SA and SGD with Constant Stepsizes.** There has been an increasing interest in studying SA with constant stepsize. Many works in this line provide non-asymptotic upper bounds on mean squared error (MSE) \(\mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2}]\). Works in [25; 42; 51] study linear SA (LSA) under i.i.d. data. Recent works extend the analysis of the MSE to LSA with Markovian data, such as [24; 52; 60]. There are also works providing upper bounds of MSE for general contractive SA with Markovian noise [14; 17].

In addition to non-asymptotic guarantees, some works focus on the asymptotic behavior of SA iterates. Recent works have shown that when using constant stepsize, one loses the almost sure convergence guarantee in the diminishing stepsize sequence regime, and at best can achieve distributional convergence, as demonstrated in [16; 20; 25; 33; 66; 67; 69]. The presence of asymptotic bias is also a recurring theme in recent literature, with precise characterization given in [20] for strongly-convex SGD with i.i.d. data and in [33] for LSA with Markovian data. Works in [34; 51; 66; 67; 69] also establish Central Limit Theorems for averaged SA iterates with constant stepsizes.

## 7 Conclusion

We provide the first weak convergence and steady-state analysis for constant-stepsize SA with both nonlinear update and Markovian data. Our analysis elucidates the compound effect of nonlinearity and memory, which leads to new analytical challenges and behaviors. A limitation of our results is the use of a projection step or the noise minorization assumption. Whether they can be removed is worth investigating. Other future directions include refining the dimension dependence in our results, as well as a theoretical investigation of statistical inference.

Figure 1: Experiment results to illustrate the properties of asymptotic bias.

AcknowledgementY. Chen is partially supported by National Science Foundation (NSF) grants CCF-1704828 and NSF CCF-2233152. Y. Zhang and Q. Xie are supported in part by NSF grants CNS-1955997, EPCN-2339794 and EPCN-2432546. Y. Zhang is also supported in part by NSF Award DMS-2023239.

## References

* [1] Sebastian Allmeier and Nicolas Gast. Computing the bias of constant-step stochastic approximation with Markovian noise, 2024.
* [2] Borja Balle and Yu-Xiang Wang. Improving the Gaussian mechanism for differential privacy: Analytical calibration and optimal denoising. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 394-403. PMLR, 10-15 Jul 2018.
* [3] Albert Benveniste, Michel Metivier, and Pierre Priouret. _Adaptive Algorithms and Stochastic Approximations_. Springer Berlin Heidelberg, 1st edition, 1990.
* [4] Dimitri P. Bertsekas. _Reinforcement learning and Optimal Control_. Athena Scientific, Belmont, Massachusetts, USA, 2019.
* [5] Aleksandr Beznosikov, Sergey Samsonov, Marina Sheshukova, Alexander Gasnikov, Alexey Naumov, and Eric Moulines. First order methods with Markovian noise: from acceleration to variational inequalities, 2023.
* [6] Jalaj Bhandari, Daniel Russo, and Raghav Singal. A finite time analysis of temporal difference learning with linear function approximation. _Operations Research_, 69(3):950-973, May 2021.
* [7] Koushik Biswas, Sandeep Kumar, Shilpak Banerjee, and Ashish Kumar Pandey. Smooth maximum unit: Smooth activation function for deep networks using smoothing maximum technique. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 784-793, 2022.
* 386, 1954.
* [9] Vivek S. Borkar. _Stochastic Approximation: A Dynamical Systems Viewpoint_. Hindustan Book Agency Gurgaon, 2008.
* [10] Vivek S. Borkar and Sean P. Meyn. The O.D.E. method for convergence of stochastic approximation and reinforcement learning. _SIAM Journal on Control and Optimization_, 38(2):447-469, Jan 2000.
* [11] Sebastien Bubeck. Convex optimization: Algorithms and complexity. _Foundations and Trends in Machine Learning_, 8(3-4):231-357, November 2015.
* [12] Siddharth Chandak, Vivek S. Borkar, and Parth Dodhia. Concentration of contractive stochastic approximation and reinforcement learning. _Stochastic Systems_, Jul 2022.
* [13] Xi Chen, Jason D Lee, Xin T Tong, and Yichen Zhang. Statistical inference for model parameters in stochastic gradient descent. _Annals of Statistics_, 48(1):251-273, 2020.
* [14] Zaiwei Chen, Siva Theja Maguluri, Sanjay Shakkottai, and Karthikeyan Shanmugam. A Lyapunov theory for finite-sample guarantees of Markovian stochastic approximation. _Operations Research_, 2023.
* [15] Zaiwei Chen, Siva Theja Maguluri, and Martin Zubeldia. Concentration of contractive stochastic approximation: Additive and multiplicative noise. _arXiv preprint arXiv:2303.15740_, 2023.
* [16] Zaiwei Chen, Shancong Mou, and Siva Theja Maguluri. Stationary behavior of constant stepsize sgd type algorithms: An asymptotic characterization. _Proceedings of the ACM on Measurement and Analysis of Computing Systems_, 6(1), 02 2022.

* [17] Zaiwei Chen, Sheng Zhang, Thinh T. Doan, John-Paul Clarke, and Siva Theja Maguluri. Finite-sample analysis of nonlinear stochastic approximation with applications in reinforcement learning. _Automatica_, 146:110623, 2022.
* [18] Djork-Arne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (ELUs), 2016.
* [19] Ilias Diakonikolas, Surbhi Goel, Sushrut Karmalkar, Adam R. Klivans, and Mahdi Soltanolkotabi. Approximation schemes for ReLU regression. In Jacob Abernethy and Shivani Agarwal, editors, _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pages 1452-1485. PMLR, 09-12 Jul 2020.
* 1382, 2020.
* [21] Jing Dong and Xin T. Tong. Stochastic gradient descent with dependent data for offline reinforcement learning, 2022.
* [22] Jinshuo Dong, Aaron Roth, and Weijie J. Su. Gaussian Differential Privacy. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 84(1):3-37, 02 2022.
* [23] Randal Douc, Eric Moulines, Pierre Priouret, and Philippe Soulier. _Markov Chains_. Springer Cham, 1st edition, 2018.
* [24] Alain Durmus, Eric Moulines, Alexey Naumov, and Sergey Samsonov. Finite-time high-probability bounds for Polyak-Ruppert averaged iterates of linear stochastic approximation, 2022.
* [25] Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov, Kevin Scaman, and Hoi-To Wai. Tight high probability bounds for linear stochastic approximation with fixed stepsize. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, volume 34, pages 30063-30074. Curran Associates, Inc., 2021.
* 1070, 2010.
* [27] Gerald B. Folland. _Real analysis: modern techniques and their applications_. Wiley, New York, 2nd ed. edition, 1999.
* [28] Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg. Noisy networks for exploration, 2019.
* [29] Sergey G. Foss and Richard L. Tweedie. Perfect simulation and backward coupling. _Communications in Statistics. Stochastic Models_, 14(1-2):187-203, 1998.
* [30] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In Geoffrey Gordon, David Dunson, and Miroslav Dudik, editors, _Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics_, volume 15 of _Proceedings of Machine Learning Research_, pages 315-323, Fort Lauderdale, FL, USA, 11-13 Apr 2011. PMLR.
* [31] F B Hildebrand. _Introduction to numerical analysis_. Dover Books on Mathematics. Dover Publications, Mineola, NY, 2 edition, June 1987.
* [32] Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. On the convergence of single-call stochastic extra-gradient methods. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc., 2019.

* [33] Dongyan (Lucy) Huo, Yudong Chen, and Qiaomin Xie. Bias and extrapolation in Markovian linear stochastic approximation with constant stepsizes. In _Abstract Proceedings of the 2023 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems_, pages 81-82, 2023.
* [34] Dongyan (Lucy) Huo, Yudong Chen, and Qiaomin Xie. Effectiveness of constant stepsize in Markovian LSA and statistical inference, 2023.
* [35] Prateek Jain, Sham M. Kakade, Rahul Kidambi, Praneeth Netrapalli, and Aaron Sidford. Parallelizing stochastic gradient descent for least squares regression: Mini-batching, averaging, and model misspecification. _Journal of Machine Learning Research_, 18(223):1-42, 2018.
* [36] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized linear and single index models with isotonic regression. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 24. Curran Associates, Inc., 2011.
* [37] Adam Tauman Kalai and Ravi Sastry. The isotron algorithm: High-dimensional isotonic regression. In _COLT_, 2009.
* [38] Kyurae Kim, Jisu Oh, Jacob Gardner, Adji Bousso Dieng, and Hongseok Kim. Markov chain score ascent: A unifying framework of variational inference with markovian gradients. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 34802-34816. Curran Associates, Inc., 2022.
* [39] Harold Kushner. Stochastic approximation: a survey. _WIREs Computational Statistics_, 2(1):87-96, 2010.
* [40] Harold J. Kushner and G. George Yin. _Stochastic Approximation and Recursive Algorithms and Applications_. Stochastic Modelling and Applied Probability. Springer, New York, NY, USA, 2nd edition, 2003.
* [41] Simon Lacoste-Julien, Mark Schmidt, and Francis Bach. A simpler approach to obtaining an o(1/t) convergence rate for the projected stochastic subgradient method, 2012.
* [42] Chandrashekar Lakshminarayanan and Csaba Szepesvari. Linear stochastic approximation: How far does constant step-size and iterate averaging go? In Amos Storkey and Fernando Perez-Cruz, editors, _Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics_, volume 84 of _Proceedings of Machine Learning Research_, pages 1347-1355. PMLR, 09-11 Apr 2018.
* [43] Guanghui Lan. _First-order and stochastic optimization methods for machine learning_, volume 1. Springer, 2020.
* [44] Caio Kalil Lauand and Sean Meyn. Bias in stochastic approximation cannot be eliminated with averaging. In _2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)_, pages 1-4, 2022.
* [45] Caio Kalil Lauand and Sean Meyn. The curse of memory in stochastic approximation. In _2023 62nd IEEE Conference on Decision and Control (CDC)_, pages 7803-7809, 2023.
* [46] Caio Kalil Lauand and Sean Meyn. Revisiting step-size assumptions in stochastic approximation, 2024.
* [47] Tianyang Li, Liu Liu, Anastasios Kyrillidis, and Constantine Caramanis. Statistical inference using SGD. _Proceedings of the AAAI Conference on Artificial Intelligence_, 32(1), Apr. 2018.
* [48] Xiang Li, Jiadong Liang, and Zhihua Zhang. Online statistical inference for nonlinear stochastic approximation with Markovian data, 2023.
* 724, 2000.
* [50] Sean P. Meyn and Richard L. Tweedie. _Markov Chains and Stochastic Stability_. Cambridge Mathematical Library. Cambridge University Press, Cambridge, 2nd edition, 2009.
* [51] Wenlong Mou, Chris Junchi Li, Martin J. Wainwright, Peter L. Bartlett, and Michael I. Jordan. On linear stochastic approximation: Fine-grained Polyak-Ruppert and non-asymptotic concentration. In Jacob Abernethy and Shivani Agarwal, editors, _Proceedings of Thirty Third Conference on Learning Theory_, volume 125 of _Proceedings of Machine Learning Research_, pages 2947-2997. PMLR, 09-12 Jul 2020.
* [52] Wenlong Mou, Ashwin Pananjady, Martin J. Wainwright, and Peter L. Bartlett. Optimal and instance-dependent guarantees for Markovian linear stochastic approximation. _Mathematical Statistics and Learning_, 7(1):41-153, March 2024.
* [53] Dheeraj Nagaraj, Xian Wu, Guy Bresler, Prateek Jain, and Praneeth Netrapalli. Least squares regression with Markovian data: Fundamental limits and algorithms. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 16666-16676. Curran Associates, Inc., 2020.
* [54] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. _SIAM Journal on Optimization_, 19(4):1574-1609, 2009.
* [55] Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration. In _International Conference on Learning Representations_, 2018.
* [56] Boris T. Polyak. New stochastic approximation type procedures. _Automation and Remote Control_, 51(7):98-107, Jul 1990.
* [57] Boris T. Polyak and Anatoli B. Juditsky. Acceleration of stochastic approximation by averaging. _SIAM Journal on Control and Optimization_, 30(4):838-855, Jul 1992.
* 407, 1951.
* [59] David Ruppert. Efficient estimations from a slowly convergent Robbins-Monro process. Technical report, Cornell University, February 1988.
* [60] Rayadurgam Srikant and Lei Ying. Finite-time error bounds for linear stochastic approximation and TD learning. In _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 2803-2830. PMLR, 25-28 Jun 2019.
* [61] Josef Stoer and Roland Bulirsch. _Introduction to Numerical Analysis_. Springer, New York, NY, USA, 3rd edition, 2002.
* [62] Richard S. Sutton and Andrew G. Barto. _Reinforcement Learning: An Introduction_. A Bradford Book, Cambridge, MA, USA, 2018.
* [63] Cedric Villani. _Optimal Transport: Old and New_. Grundlehren der mathematischen Wissenschaften. Springer Berlin Heidelberg, 2009.
* [64] Puqian Wang, Nikos Zarifis, Ilias Diakonikolas, and Jelena Diakonikolas. Robustly learning a single neuron via sharpness. _arXiv preprint arXiv:2306.07892_, 2023.
* [65] Stephen J. Wright and Benjamin Recht. _Optimization for Data Analysis_. Cambridge University Press, 2022.
* [66] Chuhan Xie and Zhihua Zhang. A statistical online inference approach in averaged stochastic approximation. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, _Advances in Neural Information Processing Systems_, volume 35, pages 8998-9009. Curran Associates, Inc., 2022.

* [67] Lu Yu, Krishnakumar Balasubramanian, Stanislav Volgushev, and Murat A. Erdogdu. An analysis of constant step size SGD in the non-convex regime: Asymptotic normality and bias. In _Advances in Neural Information Processing Systems_, volume 34, pages 4234-4248. Curran Associates, Inc., 2021.
* [68] Yixuan Zhang, Dongyan (Lucy) Huo, Yudong Chen, and Qiaomin Xie. Prelimit coupling and steady-state convergence of constant-stepsize nonsmooth contractive SA. In _Abstracts of the 2024 ACM SIGMETRICS/IFIP PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems_, SIGMETRICS/PERFORMANCE '24, page 35-36, New York, NY, USA, 2024. Association for Computing Machinery.
* [69] Yixuan Zhang and Qiaomin Xie. Constant stepsize Q-learning: Distributional convergence, bias and extrapolation. _Reinforcement Learning Journal_, 3:1168-1210, 2024.

## Appendix A Additional Notations

General ProbabilityWe write \(z_{1}\perp\!\!\perp z_{2}\mid z_{3}\) if random variables \(z_{1}\) and \(z_{2}\) are conditionally independent given \(z_{3}\).

Recall that we define the metric \(\bar{d}\big{(}(x,\theta),(x^{\prime},\theta^{\prime})\big{)}:=\sqrt{\mathbbm{1} \{x\neq x^{\prime}\}+\|\theta-\theta^{\prime}\|^{2}}\) for the space \(\mathcal{X}\times\mathbb{R}^{d}\). Thus, for \(\bar{\mu}\) and \(\bar{\nu}\) in \(\mathcal{P}_{2}(\mathcal{X}\times\mathbb{R}^{d})\), the Wasserstein-2 distance w.r.t. \(\bar{d}\) is computed as

\[\bar{W}_{2}(\bar{\mu},\bar{\nu})=\inf\Big{\{}\big{(}\mathbb{E}[ \mathbbm{1}\{x\neq x^{\prime}\}+\|\theta-\theta^{\prime}\|^{2}]\big{)}^{ \frac{1}{2}}:\mathcal{L}\big{(}(x,\theta)\big{)}=\bar{\mu},\mathcal{L}\big{(}(x ^{\prime},\theta^{\prime})\big{)}=\bar{\nu}\Big{\}}.\]

General State Space Markov ChainsThroughout the paper, we assume that \(\mathcal{X}\) is a Borel space. Let \(P\) denote the transition kernel. We call \(\pi\) the stationary distribution of \(P\) if it satisfies \(\int_{\mathcal{X}}\pi(\mathrm{d}x)P(x,B)=\pi(B)\), for \(B\in\mathcal{B}(\mathcal{X})\). Define the \(\pi\)-weighted inner product \(\langle f,g\rangle_{L^{2}(\pi)}=\int_{\mathcal{X}}\pi(\mathrm{d}x)f^{\top}(x)g (x)\) and the induced norm \(\|f\|_{L^{2}(\pi)}=(\langle f,f\rangle_{L^{2}(\pi)})^{1/2}\). Let \(L^{2}(\pi)=\{f:\|f\|_{L^{2}(\pi)}<\infty\}\) denote the corresponding Hilbert space of \(\mathbb{R}^{d}\)-valued, square-integrable and measurable functions on \(\mathcal{X}\). For an operator \(T:L^{2}(\pi)\to L^{2}(\pi)\), its operator norm is defined as \(\|T\|_{L^{2}(\pi)}=\sup_{\|f\|_{L^{2}(\pi)}=1}\|Tf\|_{L^{2}(\pi)}\). The transition kernel is a bounded linear operator on \(L^{2}(\pi)\), in particular with norm \(\|P\|_{L^{2}(\pi)=1}\). Also, we define the kernel/operator \(\Pi=1\otimes\pi\) by \(\Pi(x,\cdot)=\pi\).

Throughout the paper, we assume that \(\mathcal{X}\) is a Borel space. Let \(P\) denote the transition kernel. We call \(\pi\) the stationary distribution of \(P\) if it satisfies \(\int_{\mathcal{X}}\pi(\mathrm{d}x)P(x,B)=\pi(B)\), for \(B\in\mathcal{B}(\mathcal{X})\). There exists a kernel \(P^{*}\) as a regular conditional probability that satisfies \(\int_{\mathcal{A}}\pi(\mathrm{d}s)P(x,B)=\int_{B}\pi(\mathrm{d}y)P^{*}(y,A)\), for \(A,B\in\mathcal{B}(\mathcal{X})\)[27, Chapter 21.4, Theorem 19], and \(P^{*}\) defines the probability law for the time-reversed chain of \((x_{k})_{k\geq 0}\).

## Appendix B Additional Discussion on Assumptions

In this section, we provide a more detailed discussion of the assumptions taken in this work.

Projection and MinorizationProjection steps have a longstanding presence in SA literature for tractability in convergence theory, as seen in many analyses of SGD [6, 11, 39, 41, 54]. Although not an algorithmic proposal, this additional projection step does not incur much computational cost in practice, as it only involves rescaling the iterates, and the projection radius can be estimated a priori. Before our work, no studies had proven weak convergence for non-linear SA with Markovian data and constant stepsize, with or without the projection. Thus, our result is the first to prove detailed weak convergence in this setting. Nonetheless, we provide an alternative proof of weak convergence in Theorem 4.3 using the Drift and Minorization technique, which does not require a projection.

DifferentiabilityThe differentiability condition of the SA update operator \(g\) in Assumption 2 ensures controlled evolution of the iterates \(\theta\). This differentiability assumption supports a third-order Taylor expansion of \(g\) with a bounded remainder, which is crucial for both analyzing the \(\spadesuit\) term in the convergence proof as discussed in Section 3 and for bias characterization. Moreover, some form of differentiability assumption is standard in SA literature, such as [1, 20, 45], particularly when one seeks a fine-grained characterization of the iterates' distributional property. Such an assumption is satisfied by many GLMs, such as logistic regression and Poisson regression. When \(g\) is not differentiable, the bias of SA iterates behaves drastically different [68], which is beyond the scope of this paper.

Strong MonotonicityThe strong monotonicity assumption is common in SA literature. Together with smoothness, it allows us to establish geometric distributional convergence. While some GLMs by themselves do not satisfy this condition, it is a common practice to apply \(L_{2}\)-regularization (equivalently, weight decay) to ensure strong convexity and improve statistical performance. It is a standard calculation that one can appropriately choose the regularization parameter to derive tight results for non-strongly-convex functions.

Additional Plots

In this section, we present the plots from numerical experiments in Section 5 that verify the Central Limit Theorem (CLT) in Corollary 4.1. In these plots, we plot the centered and scaled PR-averaged iterates, i.e, \(\sqrt{T}\Big{(}\bar{\theta}_{T}^{(k)}-\sum_{l=1}^{1000}\bar{\theta}_{T}^{(l)} \Big{)}\), where \(\bar{\theta}^{(k)}=\Big{(}\sum_{t=1}^{T}\theta_{t}\Big{)}/T\) with \(T=10^{6}\) is the PR-averaged iterates for the \(k\)-th repeat.

## Appendix D Proof of Pilot Results (Proposition 4.2)

In this section, we prove the pilot result, namely Proposition 4.2. We prove the desired moments for \(\beta=\infty\), i.e., without any projection. It is easy to see that when the projection radius \(\beta\in[2\|\theta^{*}\|,\infty]\),

\[\mathbb{E}[\|\theta_{t+1}-\theta^{*}\|^{2p}]\leq\mathbb{E}[\|\theta_{t+1/2}- \theta^{*}\|^{2p}],\]

where \(\theta_{t+1/2}\) denotes the iterate before projection. The term on the right hand side can be further bounded by the moment bounds for iteration without projection. Therefore, it suffices for us to prove the respective moment bounds without any projection.

Given Assumption 4.2 hold for \(2p\)-th moment, with \(p\geq 1\), we prove the moment bound in Proposition 4.2 for \(n\) with \(1\leq n\leq p\) by induction.

### Base Case

In this section, we prove the base case of Proposition 4.2, i.e., with \(n=1\). The base case gives the desired mean squared error (MSE) convergence bound, which will subsequently be used in the proof of weak convergence.

We start by noting the following decomposition,

\[\mathbb{E}[\|\theta_{k+1}-\theta^{*}\|^{2}]-\mathbb{E}[\|\theta_ {k}-\theta^{*}\|^{2}]\] \[=2\alpha\mathbb{E}[(\theta_{k}-\theta^{*},g(\theta_{k},x_{k}))]+ \alpha^{2}\mathbb{E}[\|g(\theta_{k},x_{k})\|^{2}]+\alpha^{2}\mathbb{E}[\| \xi_{k+1}(\theta_{k})\|^{2}]\] \[=2\alpha\mathbb{E}[(\theta_{k}-\theta^{*},g(\theta_{k},x_{k})- \bar{g}(\theta_{k}))]+2\alpha\mathbb{E}[\langle\theta_{k}-\theta^{*},\bar{g} (\theta_{k})\rangle]\] \[+\alpha^{2}\mathbb{E}[\|g(\theta_{k},x_{k})\|^{2}]+\alpha^{2} \mathbb{E}[\|\xi_{k+1}(\theta_{k})\|^{2}].\]

It is easy to see that under Assumption 3, we have

\[\langle\theta_{k}-\theta^{*},\bar{g}(\theta_{k})\rangle=\langle\theta_{k}- \theta^{*},\bar{g}(\theta_{k})-\bar{g}(\theta^{*})\rangle\leq-\mu\|\theta_{k} -\theta^{*}\|^{2}.\] (D.1)

Figure 2: Experiment results to verify the Central Limit Theorem.

Additionally, under Assumption 2 and 4, we have the following upper bound

\[\alpha^{2}\Big{(}\mathbb{E}[\|g(\theta_{k},x_{k})\|^{2}]+\mathbb{E}[ \|\xi_{k+1}(\theta_{k})\|^{2}]\Big{)}\] \[\leq\alpha^{2}\Big{(}L_{1}^{2}\mathbb{E}[(\|\theta_{k}-\theta^{*} \|+1)^{2}]+L_{2}^{2}\mathbb{E}[(\|\theta_{k}-\theta^{*}\|+1)^{2}]\Big{)}\] \[\leq 2\alpha^{2}L^{2}\Big{(}\mathbb{E}[\|\theta_{k}-\theta^{*}\|^ {2}]+1\Big{)}.\] (D.2)

Therefore, the key to analyze the remaining inner product \(\langle\theta_{k}-\theta^{*},g(\theta_{k},x_{k})-\bar{g}(\theta_{k})\rangle\).

Consider the following decomposition

\[\langle\theta_{k}-\theta^{*},g(\theta_{k},x_{k})-\bar{g}(\theta_ {k})\rangle =\langle\theta_{k}-\theta_{k-\tau},g(\theta_{k},x_{k})-\bar{g}( \theta_{k})\rangle\] (D.3) \[+\langle\theta_{k-\tau}-\theta^{*},g(\theta_{k-\tau},x_{k})-\bar {g}(\theta_{k-\tau})\rangle\] (D.4) \[+\langle\theta_{k-\tau}-\theta^{*},g(\theta_{k},x_{k})-g(\theta_ {k-\tau},x_{k})\rangle\] (D.5) \[+\langle\theta_{k-\tau}-\theta^{*},\bar{g}(\theta_{k})-\bar{g}( \theta_{k-\tau})\rangle.\] (D.6)

Hence, we need some upper bound on \(\|\theta_{k}-\theta_{k-\tau}\|\).

We next note the following technical Lemma, which is adapted from [17, 60] for the updated unbounded i.i.d. noise assumption in Assumption 4. The proof of the technical Lemma is delayed to Appendix D.1.

**Lemma D.1**.: _For \(16\alpha\tau\leq\mu/(4L^{2})\), we have_

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k-\tau}] \leq 2\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\|+2\alpha\tau L\] (D.7) \[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k-\tau}] \leq 4\alpha\tau L\mathbb{E}[\|\theta_{k}-\theta^{*}\|\mathcal{F}_ {k-\tau}]+4\alpha\tau L\] (D.8) \[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2}|\mathcal{F}_{k-\tau}] \leq 8\alpha^{2}\tau^{2}L^{2}\|\theta_{k-\tau}-\theta^{*}\|^{2}+8 \alpha^{2}\tau^{2}L^{2}\] (D.9) \[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2}|\mathcal{F}_{k-\tau}] \leq 32\alpha^{2}\tau^{2}L^{2}\mathbb{E}[\|\theta_{k}-\theta^{*}\| ^{2}|\mathcal{F}_{k-\tau}]+32\alpha^{2}\tau^{2}L^{2}.\] (D.10)

Given (D.10), we additionally note that

\[\|\theta_{k-\tau}-\theta^{*}\|^{2}+1 =\mathbb{E}[\|\theta_{k-\tau}-\theta^{*}\|^{2}|\mathcal{F}_{k- \tau}]+1\] \[\leq 2\Big{(}\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2}| \mathcal{F}_{k-\tau}]+\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}|\mathcal{F}_{k -\tau}]\Big{)}+1\] \[\leq 2\Big{(}32\alpha^{2}\tau^{2}L^{2}(\mathbb{E}[\|\theta_{k}- \theta^{*}\|^{2}|\mathcal{F}_{k-\tau}]+1)+\mathbb{E}[\|\theta_{k}-\theta^{*}\| ^{2}|\mathcal{F}_{k-\tau}]\Big{)}+1\] \[\leq 4(\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}|\mathcal{F}_{k- \tau}]+1).\] (D.11)

We next use the above four technical inequalities to analyze the four terms in (D.3)-(D.4).

To bound (D.3), we first note that

\[\|\mathbb{E}[\langle\theta_{k}-\theta_{k-\tau},g(\theta_{k},x_{k} )-\bar{g}(\theta_{k})\rangle|\mathcal{F}_{k-\tau}]\|\] \[\leq\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\cdot 2L(\|\theta_{k}- \theta^{*}\|+1)|\mathcal{F}_{k-\tau}]\] \[\overset{\text{(i)}}{\leq}2L\sqrt{\mathbb{E}[\|\theta_{k}- \theta_{k-\tau}\|^{2}|\mathcal{F}_{k-\tau}]}\sqrt{\mathbb{E}[(\|\theta_{k}- \theta^{*}\|+1)^{2}|\mathcal{F}_{k-\tau}]}\] \[\overset{\text{(ii)}}{\leq}2L\sqrt{32\alpha^{2}\tau^{2}L^{2}( \mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}|\mathcal{F}_{k-\tau}]+1)}\sqrt{2( \mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}|\mathcal{F}_{k-\tau}]+1)}\] \[\leq 16\alpha\tau L^{2}(\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}| \mathcal{F}_{k-\tau}]+1),\]

where (i) holds for the Cauchy-Schwarz inequality and (ii) holds for (D.10).

To bound (D.4), we next note that

\[\|\mathbb{E}[\langle\theta_{k-\tau}-\theta^{*},g(\theta_{k-\tau},x _{k})-\bar{g}(\theta_{k-\tau})\rangle|\mathcal{F}_{k-\tau}]\|\] \[=\|\langle\theta_{k-\tau}-\theta^{*},\mathbb{E}[g(\theta_{k-\tau},x _{k})-\bar{g}(\theta_{k-\tau})|\mathcal{F}_{k-\tau}]\rangle\|\] \[\leq\|\theta_{k-\tau}-\theta^{*}\|\|\mathbb{E}[g(\theta_{k-\tau},x _{k})-\bar{g}(\theta_{k-\tau})|\mathcal{F}_{k-\tau}]\|\] \[\overset{\text{(iii)}}{\leq}\|\theta_{k-\tau}-\theta^{*}\|\cdot \Big{(}\alpha L(\|\theta_{k-\tau}-\theta^{*}\|+1)\Big{)}\] \[\leq 2\alpha L(\|\theta_{k-\tau}-\theta^{*}\|^{2}+1)\]\[\leq 2(1-\alpha\mu)^{k-\tau}\Big{(}\mathbb{E}[\|\theta_{k}-\theta^{*} \|^{2}]+\mathbb{E}[\|\theta_{\tau}-\theta_{0}\|^{2}]\Big{)}+\frac{114\alpha\tau L ^{2}}{\mu}\] \[\leq 2(1-\alpha\mu)^{k-\tau}\Big{(}\mathbb{E}[\|\theta_{0}-\theta^{ *}\|^{2}]+8\alpha^{2}\tau^{2}L^{2}\Big{(}\mathbb{E}[\|\theta_{0}-\theta^{*}\|^{ 2}]+1\Big{)}+\frac{114\alpha\tau L^{2}}{\mu}\] \[\leq 4(1-\alpha\mu)^{k-\tau}\mathbb{E}[\|\theta_{0}-\theta^{*}\|^{ 2}]+\frac{122\alpha\tau L^{2}}{\mu}.\]

Lastly, we note that

\[\frac{1}{(1-\alpha\mu)^{\tau}}\stackrel{{\text{(i)}}}{{\leq}} \frac{1}{1-\alpha\tau\mu}\stackrel{{\text{(ii)}}}{{\leq}}\frac{1} {1-\alpha\tau L}\stackrel{{\text{(iii)}}}{{\leq}}2,\] (D.13)

where (i) holds by the Bernoulli inequality, that \((1+x)^{r}\geq 1+rx\) for \(x\geq-1\) and \(r\geq 1\); (ii) holds for \(\mu\leq L\); (iii) holds for \(\alpha\tau L<\mu/(114L)<\frac{1}{2}\).

Hence, for \(k\geq\tau\), we have

\[\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}]\leq c_{2,1}(1-\alpha\mu)^{k}\|\theta_{0 }-\theta^{*}\|^{2}+c_{2,2}\alpha\tau_{\alpha}\frac{L^{2}}{\mu},\]

for \(c_{2,1}\) and \(c_{2,2}\) some universal constants. As such, we have completed the proof of base case for Proposition 4.2.

#### d.1.1 Proof of Lemma d.1

In this section, we provide the proofs of the four technical inequalities in Lemma D.1.

**Proof of (D.7).**

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k-\tau}]\leq 2\alpha \tau L\|\theta_{k-\tau}-\theta^{*}\|+2\alpha\tau L.\]

Proof.: Note that

\[\|\theta_{k}-\theta_{k-\tau}\|\leq\sum_{t=k-\tau}^{k-1}\|\theta_{t+1}-\theta_{ t}\|,\]

so we start with analyzing \(\|\theta_{t+1}-\theta_{t}\|\).

\[\|\theta_{t+1}-\theta^{*}\|-\|\theta_{t}-\theta^{*}\| \leq\|\theta_{t+1}-\theta_{t}\|=\alpha\|g(\theta_{t},x_{t})+\xi_{ t+1}(\theta_{t})\|\] \[\leq\alpha\|g(\theta_{t},x_{t})\|+\alpha\|\xi_{t+1}(\theta_{t}) \|\leq\alpha L_{1}(\|\theta_{t}-\theta^{*}\|+1)+\alpha\|\xi_{t+1}(\theta_{t})\|\] \[\|\theta_{t+1}-\theta^{*}\| \leq(1+\alpha L_{1})\|\theta_{t}-\theta^{*}\|+\alpha L_{1}+\alpha \|\xi_{t+1}(\theta_{t})\|.\]

Recall that we assume

\[\mathbb{E}^{1/2}[\|\xi_{t+1}(\theta_{t})\|^{2}|\mathcal{F}_{t}]\leq L_{2}(\| \theta_{t}\|+1),\]

then we have for \(k-\tau\leq t\leq k\),

\[\mathbb{E}[\|\xi_{t+1}(\theta_{t})\|\mathcal{F}_{k-\tau}] =\mathbb{E}[\mathbb{E}[\|\xi_{t+1}(\theta_{k})|\mathcal{F}_{t}]| \mathcal{F}_{k-\tau}]\leq\mathbb{E}[L_{2}(\|\theta_{k}\|+1)|\mathcal{F}_{k- \tau}]\] \[\mathbb{E}[\|\theta_{t+1}-\theta_{t}\|\mathcal{F}_{k-\tau}] \leq\alpha L(\mathbb{E}[\|\theta_{t}-\theta^{*}\|\|\mathcal{F}_{k- \tau}]+1)\] \[\mathbb{E}[\|\theta_{k+1}-\theta^{*}\|\mathcal{F}_{k-\tau}] \leq(1+\alpha L)\mathbb{E}[\|\theta_{k}-\theta^{*}\|\mathcal{F}_{ k-\tau}]+\alpha L.\]

Hence, for \(0\leq n\leq\tau\),

\[\mathbb{E}[\|\theta_{k-\tau+n}-\theta^{*}\|\mathcal{F}_{k-\tau}] \leq(1+\alpha L)^{n}\mathbb{E}[\|\theta_{k-\tau}-\theta^{*}\| \mathcal{F}_{k-\tau}]+\alpha L\sum_{l=0}^{n-1}(1+\alpha L)^{l}\] \[=(1+\alpha L)^{n}\|\theta_{k-\tau}-\theta^{*}\|+((1+\alpha L)^{n} -1).\]

We next note that

\[(1+x)^{y}=e^{y\log(1+x)}\leq e^{xy}\leq 1+2xy,xy\in[0,1/2].\]

Hence, at this stage, if we require \(\alpha\tau L<\mu/(4L)<1/4\), we have the following upper bound

\[(1+\alpha L)^{n}\leq(1+\alpha L)^{\tau}\leq 1+2\alpha\tau L\leq 2.\]

Therefore, for \(0\leq n\leq\tau\),

\[\mathbb{E}[\|\theta_{k-\tau+n}-\theta^{*}\|\mathcal{F}_{k-\tau}] \leq(1+2\alpha\tau L)\|\theta_{k-\tau}-\theta^{*}\|+2\alpha\tau L\] \[\leq 2\|\theta_{k-\tau}-\theta^{*}\|+2\alpha\tau L.\]

As such, we have

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k-\tau}] \leq\sum_{t=k-\tau}^{k-1}\mathbb{E}[\|\theta_{t+1}-\theta_{t}\| \mathcal{F}_{k-\tau}]\] \[\leq\alpha L\sum_{t=k-\tau}^{k-1}\mathbb{E}[\|\theta_{t}-\theta^{ *}\|\mathcal{F}_{k-\tau}]+\alpha\tau L\] \[\leq\alpha\tau L(2\|\theta_{k-\tau}-\theta^{*}\|+2\alpha\tau L)+ \alpha\tau L\] \[\leq 2\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\|+2\alpha\tau L,\]

and prove the desired inequality.

**Proof of (D.8).**

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k-\tau}]\leq 4\alpha\tau L \mathbb{E}[\|\theta_{k}-\theta^{*}\|\mathcal{F}_{k-\tau}]+4\alpha\tau L.\]

Proof.: We prove this inequality based on the claim that we have just shown,

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k-\tau}]\leq 2\alpha\tau L \|\theta_{k-\tau}-\theta^{*}\|+2\alpha\tau L.\]

We simply note that

\[\|\theta_{k-\tau}-\theta^{*}\| =\mathbb{E}[\|\theta_{k-\tau}-\theta^{*}\|\mathcal{F}_{k-\tau}]\] \[\leq\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k-\tau }]+\mathbb{E}[\|\theta_{k}-\theta^{*}\|\mathcal{F}_{k-\tau}].\]

Hence,

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k-\tau}] \leq 2\alpha\tau L(\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\| \mathcal{F}_{k-\tau}]+\mathbb{E}[\|\theta_{k}-\theta^{*}\|\mathcal{F}_{k- \tau}]+1)\] \[(1-2\alpha\tau L)\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\| \mathcal{F}_{k-\tau}] \leq 2\alpha\tau L\mathbb{E}[\|\theta_{k}-\theta^{*}\|\mathcal{F}_ {k-\tau}]+2\alpha\tau L.\]

Therefore, we obtain

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k-\tau}]\leq 4\alpha \tau L\mathbb{E}[\|\theta_{k}-\theta^{*}\|\mathcal{F}_{k-\tau}]+4\alpha\tau L.\]

**Proof of (D.9).**

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2}|\mathcal{F}_{k-\tau}]\leq 32 \alpha^{2}\tau^{2}L^{2}\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}|\mathcal{F}_{k -\tau}]+32\alpha^{2}\tau^{2}L^{2}.\]

Proof.: To analyze \(\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2}|\mathcal{F}_{k-\tau}]\), we consider the following attempt.

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2}|\mathcal{F}_{k- \tau}] \leq\tau\sum_{t=k-\tau}^{k-1}\mathbb{E}[\|\theta_{t+1}-\theta_{t} \|^{2}|\mathcal{F}_{k-\tau}]\] \[=\alpha^{2}\tau\sum_{t=k-\tau}^{k-1}\mathbb{E}[(\|g(\theta_{t},x _{t})\|+\|\xi_{t+1}(\theta_{t})\|)^{2}|\mathcal{F}_{k-\tau}]\] \[\leq 2\alpha^{2}\tau\sum_{t=k-\tau}^{k-1}\left(\mathbb{E}[\|g( \theta_{t},x_{t})\|^{2}|\mathcal{F}_{k-\tau}]+\mathbb{E}[\|\xi_{t+1}(\theta_{ t})\|^{2}|\mathcal{F}_{k-\tau}]\right)\] \[\leq 2\alpha^{2}\tau L^{2}\sum_{t=k-\tau}^{k-1}\mathbb{E}[(\| \theta_{t}-\theta^{*}\|+1)^{2}|\mathcal{F}_{k-\tau}]\] \[\leq 4\alpha^{2}\tau L^{2}\sum_{t=k-\tau}^{k-1}\mathbb{E}[\| \theta_{t}-\theta^{*}\|^{2}|\mathcal{F}_{k-\tau}]+4\alpha^{2}\tau^{2}L^{2}.\]

Next, we study \(\mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2}|\mathcal{F}_{k-\tau}]\). We start with the following, for \(k-\tau\leq t<k\),

\[\mathbb{E}[\|\theta_{t+1}-\theta^{*}\|^{2}|\mathcal{F}_{k-\tau}]\] \[=\mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2}|\mathcal{F}_{k-\tau}]+2 \alpha\mathbb{E}[\langle\theta_{t}-\theta^{*},g(\theta_{t},x_{t})\rangle| \mathcal{F}_{k-\tau}]+\alpha^{2}\mathbb{E}[\|g(\theta_{t},x_{t})+\xi_{t+1}( \theta_{t})\|^{2}|\mathcal{F}_{k-\tau}]\] \[\leq\mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2}|\mathcal{F}_{k-\tau }]+2\alpha^{2}\Big{(}\mathbb{E}[\|g(\theta_{t},x_{t})\|^{2}|\mathcal{F}_{k- \tau}]+\mathbb{E}[\|\xi_{t+1}(\theta_{t})\|^{2}|\mathcal{F}_{k-\tau}]\Big{)}\] \[+2\alpha\mathbb{E}[\|\theta_{t}-\theta^{*}\|\|g(\theta_{t},x_{t}) \|\|\mathcal{F}_{k-\tau}].\]

We note that

\[2\mathbb{E}[\|\theta_{t}-\theta^{*}\|\|g(\theta_{t},x_{t})\| \mathcal{F}_{k-\tau}] \leq 2\sqrt{\mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2}|\mathcal{F}_{ k-\tau}]}\mathbb{E}[\|g(\theta_{t},x_{t})\|^{2}|\mathcal{F}_{k-\tau}]\] \[\leq 2\sqrt{\mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2}|\mathcal{F}_{ k-\tau}]}\mathbb{E}[(L(\|\theta_{t}-\theta^{*}\|+1))^{2}|\mathcal{F}_{k-\tau}]\] \[\leq 2\sqrt{\mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2}|\mathcal{F}_{ k-\tau}]}2L^{2}\mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2}+1|\mathcal{F}_{k-\tau}]\]\[\leq 2\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2}|\mathcal{F}_{k-\tau}]+2 \mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}|\mathcal{F}_{k-\tau}].\]

Hence,

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2}|\mathcal{F}_{k-\tau}] \leq 8\alpha^{2}\tau^{2}L^{2}(2\mathbb{E}[\|\theta_{k}-\theta_{k- \tau}\|^{2}|\mathcal{F}_{k-\tau}]+2\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}| \mathcal{F}_{k-\tau}]+1)\] \[(1-16\alpha^{2}\tau^{2}L^{2})\mathbb{E}[\|\theta_{k}-\theta_{k- \tau}\|^{2}|\mathcal{F}_{k-\tau}]\leq 16\alpha^{2}\tau^{2}L^{2}\mathbb{E}[\| \theta_{k}-\theta^{*}\|^{2}|\mathcal{F}_{k-\tau}]+8\alpha^{2}\tau^{2}L^{2}.\]

Again, under the assumption that \(16\alpha\tau L^{2}<\mu/4\), we can conclude that

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2}|\mathcal{F}_{k-\tau}]\leq 32 \alpha^{2}\tau^{2}L^{2}\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}|\mathcal{F}_{ k-\tau}]+32\alpha^{2}\tau^{2}L^{2}.\]

### Induction Step

In this step, assume that the moment bound in Proposition 4.2 has been proven for \(k\leq n-1\), we now proceed to show that the desired moment convergence holds for \(n\) with \(2\leq n\leq p\).

We start with the following decomposition of \(\|\theta_{k+1}-\theta^{*}\|^{2n}\)

\[\|\theta_{k+1}-\theta^{*}\|^{2n}\] \[=\Big{(}\|\theta_{k}-\theta^{*}\|^{2}+2\alpha\langle\theta_{k}- \theta^{*},g(\theta_{k},x_{k})+\xi_{k+1}(\theta_{k})\rangle+\alpha^{2}\|g( \theta_{k},x_{k})+\xi_{k+1}(\theta_{t})\|^{2}\Big{)}^{n}\] \[=\sum_{\begin{subarray}{c}i,j,l\\ i+j+l=n\end{subarray}}\binom{n}{i,j,l}\|\theta_{k}-\theta^{*}\|^{2i}\Big{(}2 \alpha(\theta_{k}-\theta^{*},g(\theta_{k},x_{k})+\xi_{k+1}(\theta_{k}))\Big{)} ^{j}\Big{(}\alpha\|g(\theta_{k},x_{k})+\xi_{k+1}(\theta_{k})\|\Big{)}^{2i}\]

We note the following cases.

1. \(i=n\), \(j=l=0\). In this case, the summand is simply \(\|\theta_{k}-\theta^{*}\|^{2i}\).
2. When \(i=n-1\), \(j=1\) and \(l=0\). In this case, the summand is of order \(\alpha\), i.e., \(\alpha 2n\langle\theta_{k}-\theta^{*},g(\theta_{k},x_{k})+\xi_{k+1}(\theta_{k}) \rangle^{j}\|\theta_{k}-\theta^{*}\|^{2(n-1)}\). We can further compose it as \[2n\alpha\langle\theta_{k}-\theta^{*},g(\theta_{k},x_{k})+\xi_{k+ 1}(\theta_{k})\rangle\|\theta_{k}-\theta^{*}\|^{2(n-1)}\] \[=\underbrace{2n\alpha\langle\theta_{k}-\theta^{*},g(\theta_{k},x_ {k})-\bar{g}(\theta_{k})+\xi_{k+1}(\theta_{k})\rangle\|\theta_{k}-\theta^{*} \|^{2(n-1)}}_{T_{1}}\] \[+\underbrace{2n\alpha\langle\theta_{k}-\theta^{*},\bar{g}(\theta _{k})\rangle\|\theta_{k}-\theta^{*}\|^{2(n-1)}}_{T_{2}}.\] Note that, when \((x_{k})\) is i.i.d. or from a martingale noise sequence, we have \[\mathbb{E}[T_{1}|\theta_{k}]=0.\] However, when \((x_{k})\) is Markovian, the above equality then does not hold and \(T_{1}\) requires a careful analysis. Nonetheless, under the strong monotonicity assumption, we have \[T_{2}\leq-2n\alpha\mu\|\theta_{k}-\theta^{*}\|^{2n}.\]
3. For the remaining terms, we see that they are of higher orders of \(\alpha\). Therefore, when \(\alpha\) is selected sufficiently small, these terms do not raise concern.

Therefore, to prove the desired moment bound, we spend the remaining section analyzing \(T_{1}\). Immediately, we note that

\[\mathbb{E}[T_{1}|\mathcal{F}_{k-\tau}] =\mathbb{E}\Big{[}2n\alpha\langle\theta_{k}-\theta^{*},g(\theta_{ k},x_{k})-\bar{g}(\theta_{k})+\mathbb{E}[\xi_{k+1}(\theta_{k})|\theta_{k}] \rangle\|\theta_{k}-\theta^{*}\|^{2(n-1)}|\mathcal{F}_{k-\tau}\Big{]}\] \[=\mathbb{E}\Big{[}\underbrace{2n\alpha\langle\theta_{k}-\theta^{ *},g(\theta_{k},x_{k})-\bar{g}(\theta_{k})\rangle\|\theta_{k}-\theta^{*}\|^{2(n -1)}}_{T^{\prime}_{1}}|\mathcal{F}_{k-\tau}\Big{]}.\]

Subsequently, we focus on analyzing \(T^{\prime}_{1}\).

We start with the following decomposition of \(T^{\prime}_{1}\).

\[2n\alpha\langle g(\theta_{k},x_{k})-\bar{g}(\theta_{k}),\theta_{ k}-\theta^{*}\rangle\|\theta_{k}-\theta^{*}\|^{2(n-1)}\] \[\leq 2n\alpha\|g(\theta_{k-\tau},x_{k})-\bar{g}(\theta_{k-\tau})\| \|\theta_{k-\tau}-\theta^{*}\|^{2n-1}\] (D.14) \[+2n\alpha\|g(\theta_{k},x_{k})-g(\theta_{k-\tau},x_{k})\|\| \theta_{k-\tau}-\theta^{*}\|^{2n-1}\] (D.15) \[+2n\alpha\|\bar{g}(\theta_{k-\tau})-\bar{g}(\theta_{k})\|\|\theta _{k-\tau}-\theta^{*}\|^{2n-1}\] (D.16) \[+2n\alpha\|g(\theta_{k},x_{k})-\bar{g}(\theta_{k})\|\|\theta_{k}- \theta^{*}\|^{2(n-1)}\|\theta_{k}-\theta_{k-\tau}\|\] (D.17) \[+2n\alpha\|g(\theta_{k},x_{k})-\bar{g}(\theta_{k})\|\|\theta_{k- \tau}-\theta^{*}\|\cdot\big{(}\|\theta_{k}-\theta^{*}\|^{2(n-1)}-\|\theta_{k- \tau}-\theta^{*}\|^{2(n-1)}\big{)}.\] (D.18)

We note the following technical lemma, which will offer significant help in the analysis of \(T^{\prime}_{1}\). We postpone the proof of the lemma to the end of this subsection.

**Lemma D.2**.: _For \(\tilde{c}_{n}\alpha\tau\leq\mu/(4L^{2})\), where \(\tilde{c}_{n}\) denotes some constant dependent of the higher-moment \(2n\), we have_

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2n}|\mathcal{F}_{k-\tau}]\leq c_{n} \alpha^{2n}\tau^{2n}L^{2n}(\|\theta_{k-\tau}-\theta^{*}\|^{2n}+1).\]

Following the lemma, we observe that a natural consequence is for any \(m\leq 2n\), we have

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{m}|\mathcal{F}_{k-\tau}] \leq\Big{(}\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2n}|\mathcal{ F}_{k-\tau}]\Big{)}^{\frac{m}{2n}}\] \[\leq\Big{(}c_{n}\alpha^{2n}\tau^{2n}L^{2n}(\|\theta_{k-\tau}- \theta^{*}\|^{2n}+1)\Big{)}^{\frac{m}{2n}}\] \[\leq c_{m}\alpha^{m}\tau^{m}L^{m}\Big{(}\|\theta_{k-\tau}-\theta ^{*}\|^{m}+1\Big{)},\]

where we use the inequality \(a^{p}+b^{p}>(a+b)^{p}\) for \(a,b>0\), \(p\in(0,1)\) to obtain the final inequality.

Now, we are ready to analyze (D.14)-(D.18). Firstly, for (D.14), we make use of the mixing assumption of \(\tau\), and have that

\[\mathbb{E}[|\eqref{eq:c1}||\mathcal{F}_{k-\tau}] \leq 2n\alpha\|\theta_{k-\tau}-\theta^{*}\|^{2n-1}\mathbb{E}[\| \theta(\theta_{k-\tau},x_{k})-\bar{g}(\theta_{k-\tau})\|\mathcal{F}_{k-\tau}]\] \[\leq 2n\alpha^{2}L\|\theta_{k-\tau}-\theta^{*}\|^{2n-1}(\|\theta _{k-\tau}-\theta^{*}\|+1)\] \[\leq 2n\alpha^{2}L\|\theta_{k-\tau}-\theta^{*}\|^{2n}+2n\alpha^{2 }L\|\theta_{k-\tau}-\theta^{*}\|^{2n-1}\] \[\leq 3n\alpha^{2}L\|\theta_{k-\tau}-\theta^{*}\|^{2n}+n\alpha^{2 }L\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)},\]

where we make use of the inequality \(2|x|^{3}\leq x^{2}+x^{4}\) to obtain the final step.

Next, we proceed to analyze (D.15). It is easy to see that

\[\mathbb{E}[|\eqref{eq:c1}||\mathcal{F}_{k-\tau}] =2n\alpha\|\theta_{k-\tau}-\theta^{*}\|^{2n-1}\mathbb{E}[\|g( \theta_{k},x_{k})-g(\theta_{k-\tau},x_{k})|\mathcal{F}_{k-\tau}]\] \[\leq 2n\alpha\|\theta_{k-\tau}-\theta^{*}\|^{2n-1}\mathbb{E}[\| \theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k-\tau}]\] \[\leq 2n\alpha\|\theta_{k-\tau}-\theta^{*}\|^{2n-1}\Big{(}2 \alpha\tau L(\|\theta_{k-\tau}-\theta^{*}\|+1)\Big{)}\] \[\leq 4n\alpha^{2}\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2n}+4n \alpha^{2}\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2n-1}\] \[\leq 6n\alpha^{2}\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2n}+2n \alpha^{2}\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)}.\]

The term in (D.16) can be analyzed in a similar fashion as the (D.15).

For (D.17), we first derive the following

\[\mathbb{E}[|\eqref{eq:c1}||\mathcal{F}_{k-\tau}]\] \[\leq 2n\alpha\mathbb{E}\Big{[}2L\Big{(}\|\theta_{k}-\theta^{*}\|+ 1\Big{)}\|\theta_{k}-\theta_{k-\tau}\|\|\theta_{k}-\theta^{*}\|^{2(n-1)}| \mathcal{F}_{k-\tau}\Big{]}\] \[=\underbrace{4n\alpha L\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\| \|\theta_{k}-\theta^{*}\|^{2n-1}|\mathcal{F}_{k-\tau}]}_{T_{n}}+\underbrace{4n \alpha L\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\|\theta_{k}-\theta^{*}\|^{2( n-1)}|\mathcal{F}_{k-\tau}]}_{T_{b}}.\]

We next analyze the two terms \(T_{a}\) and \(T_{b}\) respectively. Starting with \(T_{a}\), we have

\[4n\alpha L\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\|\theta_{k}- \theta^{*}\|^{2n-1}|\mathcal{F}_{k-\tau}]\] (D.19) \[\leq 4n\alpha L\mathbb{E}\Big{[}\|\theta_{k}-\theta_{k-\tau}\| \Big{(}\|\theta_{k}-\theta_{k-\tau}\|+\|\theta_{k-\tau}-\theta^{*}\|\Big{)}^{2n -1}|\mathcal{F}_{k-\tau}\Big{]}\] (D.20) \[\leq 2^{2(n-1)}4n\alpha L\mathbb{E}[\|\theta_{k}-\theta_{k-\tau} \|(\|\theta_{k}-\theta_{k-\tau}\|^{2n-1}+\|\theta_{k-\tau}-\theta^{*}\|^{2n-1}| \mathcal{F}_{k-\tau}]\] (D.21) \[=4^{n}n\alpha L\Big{(}\underbrace{\mathbb{E}[\|\theta_{k}-\theta_ {k-\tau}\|^{2n}|\mathcal{F}_{k-\tau}]}_{\text{by Lemma D.2}}+\|\theta_{k-\tau}- \theta^{*}\|^{2n-1}\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k-\tau} ]\Big{)}\] (D.22) \[\leq 4^{n}n\alpha L\Big{(}\underbrace{c_{n}\alpha^{2n}\tau^{2n}L^{2 n}}_{\leq 2\alpha\tau L}(\|\theta_{k-\tau}-\theta^{*}\|^{2n}+1)+\|\theta_{k-\tau}-\theta^{*}\|^{2n -1}(2\alpha\tau L(\|\theta_{k-\tau}-\theta^{*}\|+1))\Big{)}\] (D.23) \[\leq 4^{n}n\alpha L\Big{(}4\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\| ^{2n}+2\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2n-1}+c_{n}\alpha^{2n}\tau^{2 n}L^{2n}\Big{)}\] (D.24) \[\leq 4^{n}n\alpha L\Big{(}5\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\| ^{2n}+\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)}+c_{n}^{\prime}\alpha^{2n -1}\tau^{2n-1}L^{2n-1}\Big{)}.\] (D.25)For \(T_{b}\), we have

\[4n\alpha L\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\|\theta_{k}- \theta^{*}\|^{2(n-1)}|\mathcal{F}_{k-\tau}]\] \[\leq 4n\alpha L\mathbb{E}\Big{[}\|\theta_{k}-\theta_{k-\tau}\|\Big{(} \|\theta_{k}-\theta_{k-\tau}\|+\|\theta_{k-\tau}-\theta^{*}\|\Big{)}^{2(n-1)}| \mathcal{F}_{k-\tau}\Big{]}\] \[\leq 2^{2n-1}n\alpha L\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|( \|\theta_{k}-\theta_{k-\tau}\|^{2(n-1)}+\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)} |\mathcal{F}_{k-\tau}]\] \[=2^{2n-1}n\alpha L\Big{(}\underbrace{\mathbb{E}[\|\theta_{k}- \theta_{k-\tau}\|^{2n-1}|\mathcal{F}_{k-\tau}]}_{\text{by Lemma D\ref{lem:2}}}+ \|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)}\mathbb{E}[\|\theta_{k}-\theta_{k-\tau }\||\mathcal{F}_{k-\tau}]\Big{)}\] \[\leq 2^{2n-1}n\alpha L\Big{(}\underbrace{c_{n-1}\alpha^{2n-1} \tau^{2n-1}L^{2n-1}}_{\leq 2\alpha\tau L}(\|\theta_{k-\tau}-\theta^{*}\|^{2n-1}+1)\] \[\qquad\qquad\qquad+\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)}(2 \alpha\tau L(\|\theta_{k-\tau}-\theta^{*}\|+1))\Big{)}\] \[\leq 2^{2n-1}n\alpha L\Big{(}4\alpha\tau L\|\theta_{k-\tau}- \theta^{*}\|^{2n-1}+2\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)}+c_{n- 1}\alpha^{2n-1}\tau^{2n-1}L^{2n-1}\Big{)}\] \[\leq 2^{2n-1}n\alpha L\Big{(}2\alpha\tau L\|\theta_{k-\tau}- \theta^{*}\|^{2n}+4\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)}+c_{n-1} \alpha^{2n-1}\tau^{2n-1}L^{2n-1}\Big{)}.\]

Combining the analyses of the two terms, we get the following upper bound to (D.17)

\[\mathbb{E}[|\eqref{eq:22}||\mathcal{F}_{k-\tau}]\] \[\leq 4^{n}n\alpha L\Big{(}5\alpha\tau L\|\theta_{k-\tau}-\theta^{* }\|^{2n}+\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)}+c^{\prime}_{n} \alpha^{2n-1}\tau^{2n-1}L^{2n-1}\Big{)}\] \[+2^{2n-1}n\alpha L\Big{(}2\alpha\tau L\|\theta_{k-\tau}-\theta^{ *}\|^{2n}+4\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)}+c_{n-1}\alpha^{ 2n-1}\tau^{2n-1}L^{2n-1}\Big{)}\] \[=2^{2n-1}n\alpha L\Big{(}12\alpha\tau L\|\theta_{k-\tau}-\theta^{ *}\|^{2n}+6\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)}+c^{\prime\prime }_{n-1}\alpha^{2n-1}\tau^{2n-1}L^{2n-1}\Big{)}.\]

Lastly, we analyze (D.18). We first make use of the mean-value theorem, with \(a\in[0,1]\), we have

\[\|\theta_{k}-\theta^{*}\|^{2(n-1)}-\|\theta_{k-\tau}-\theta^{*}\|^ {2(n-1)}\] \[=\|\theta_{k}-\theta_{k-\tau}\|\cdot 2(n-1)\|a(\theta_{k}-\theta^{*}) +(1-a)(\theta_{k-\tau}-\theta^{*})\|^{2n-3}\] \[=\|\theta_{k}-\theta_{k-\tau}\|\cdot 2(n-1)\|a(\theta_{k}-\theta_{k- \tau})+\theta_{k-\tau}-\theta^{*}\|^{2n-3}\] \[\leq 2^{2n-3}(n-1)\|\theta_{k}-\theta_{k-\tau}\|\Big{(}\|\theta_{k} -\theta_{k-\tau}\|^{2n-3}+\|\theta_{k-\tau}-\theta^{*}\|^{2n-3}\Big{)}\]

Substituting the above upper bound back into (D.18), we obtain

\[\mathbb{E}[|\eqref{eq:22}||\mathcal{F}_{k-\tau}]\] \[\leq 2^{2n-1}n(n-1)\alpha L\|\theta_{k-\tau}-\theta^{*}\|\] \[\qquad\mathbb{E}\Big{[}(\|\theta_{k}-\theta^{*}\|+1)\|\theta_{k }-\theta_{k-\tau}\|\Big{(}\|\theta_{k}-\theta_{k-\tau}\|^{2n-3}+\|\theta_{k- \tau}-\theta^{*}\|^{2n-3}\Big{)}|\mathcal{F}_{k-\tau}\Big{]}\] \[\leq 2^{2n-1}n(n-1)\alpha L\] \[\qquad\qquad\qquad+\|\theta_{k-\tau}-\theta^{*}\|\mathbb{E}[\| \theta_{k}-\theta_{k-\tau}\|^{2n-2}|\mathcal{F}_{k-\tau}]+\|\theta_{k-\tau}- \theta^{*}\|^{2n-2}\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2}|\mathcal{F}_{k -\tau}]\] \[\qquad\qquad\qquad+\|\theta_{k-\tau}-\theta^{*}\|^{2n-1}\mathbb{E }[\|\theta_{k}-\theta_{k-\tau}\||\mathcal{F}_{k-\tau}]+\|\theta_{k-\tau}- \theta^{*}\|^{2n-2}\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|\mathcal{F}_{k- \tau}]\Big{)}\] \[\leq 2^{2n-1}n(n-1)\alpha L\Big{(}c_{n}\alpha\tau L\|\theta_{k- \tau}-\theta^{*}\|^{2n}+c_{n-1}\alpha\tau L\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1 )}+c_{n-1}\alpha^{2n-1}\tau^{2n-1}L^{2n-1}\Big{)}.\]

Combining the analyses above, we have the following bound for \(T_{1}\),

\[\mathbb{E}[|T_{1}||\mathcal{F}_{k-\tau}]\] \[\leq\mathbb{E}[\|\eqref{eq:22}||\mathcal{F}_{k-\tau}]+\mathbb{E}[ \|\eqref{eq:22}||\mathcal{F}_{k-\tau}]+\mathbb{E}[\|\eqref{eq:22}||\mathcal{F}_{k -\tau}]\] \[+\mathbb{E}[\|\eqref{eq:22}||\mathcal{F}_{k-\tau}]+\mathbb{E}[ \|\eqref{eq:22}||\mathcal{F}_{k-\tau}]\] \[\leq c_{n,1}\alpha^{2}\tau L^{2}\|\theta_{k-\tau}-\theta^{*}\|^ {2n}+c_{n,2}\alpha^{2}\tau L^{2}\|\theta_{k-\tau}-\theta^{*}\|^{2(n-1)}+c_{n,3} \alpha^{2n}\tau^{2n-1}L^{2n},\]where \(c_{n,1}\), \(c_{n,2}\) and \(c_{n,3}\) are some constants that depend on \(n\).

Additionally, we note that

\[\|\theta_{k-\tau}-\theta^{*}\|^{2n} =\mathbb{E}[\|\theta_{k-\tau}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}]\] \[\leq\mathbb{E}\Big{[}\Big{(}\|\theta_{k}-\theta_{k-\tau}\|+\| \theta_{k}-\theta^{*}\|\Big{)}^{2n}|\mathcal{F}_{k-\tau}\Big{]}\] \[\leq 2^{2n-1}\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2n}| \mathcal{F}_{k-\tau}]+2^{2n-1}\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2n}| \mathcal{F}_{k-\tau}]\] \[\leq c_{n}\alpha^{2n}\tau^{2n}L^{2n}(\|\theta_{k-\tau}-\theta^{* }\|^{2n}+1)+2^{2n-1}\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2n}|\mathcal{F}_{k- \tau}].\]

Therefore, for sufficiently small \(\alpha\tau L<\mu/(c_{n}^{\prime}L)\), we have

\[(1-c_{n}^{\prime}\alpha^{2n}\tau^{2n}L^{2n})\|\theta_{k-\tau}- \theta^{*}\|^{2n} \leq c_{n}^{\prime\prime}\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2n}| \mathcal{F}_{k-\tau}]+c_{n}\alpha^{2n}\tau^{2n}L^{2n}\] \[\Rightarrow\quad\|\theta_{k-\tau}-\theta^{*}\|^{2n} \leq 2c_{n}^{\prime\prime}\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2n}| \mathcal{F}_{k-\tau}]+2c_{n}\alpha^{2n}\tau^{2n}L^{2n}.\]

As such, for sufficiently small \(\alpha\), we have

\[\mathbb{E}[|T_{1}||\mathcal{F}_{k-\tau}]\] \[\leq c_{n,1}\alpha^{2}\tau L^{2}\Big{(}c_{n}\mathbb{E}[\|\theta_{ k}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}]+c_{n}^{\prime}\alpha^{2n}\tau^{2n}L^{2n} \Big{)}\] \[+c_{n,2}\alpha^{2}\tau L^{2}\Big{(}c_{n-1}\mathbb{E}[\|\theta_{k} -\theta^{*}\|^{2(n-1)}|\mathcal{F}_{k-\tau}]+c_{n,1}^{\prime}\alpha^{2(n-1)} \tau^{2(n-1)}L^{2(n-1)}\Big{)}+c_{n,3}\alpha^{2n}\tau^{2n-1}L^{2n}\] \[=c_{n,1}\alpha^{2}\tau L^{2}\mathbb{E}[\|\theta_{k}-\theta^{*}\| ^{2n}|\mathcal{F}_{k-\tau}]+c_{n,2}\alpha^{2}\tau L^{2}\mathbb{E}[\|\theta_{k} -\theta^{*}\|^{2(n-1)}|\mathcal{F}_{k-\tau}]+c_{n,3}\alpha^{2n}\tau^{2n-1}L^{2n}.\]

Hence, up til this point, we have obtained

\[\mathbb{E}[\|\theta_{k+1}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}]\] \[\leq(1-2n\alpha\mu)\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2n}| \mathcal{F}_{k-\tau}]\] \[+c_{n,1}\alpha^{2}\tau L^{2}\mathbb{E}[\|\theta_{k}-\theta^{*}\| ^{2n}|\mathcal{F}_{k-\tau}]+c_{n,2}\alpha^{2}\tau L^{2}\mathbb{E}[\|\theta_{k }-\theta^{*}\|^{2(n-1)}|\mathcal{F}_{k-\tau}]+c_{n,3}\alpha^{2n}\tau^{2n-1}L^ {2n}\] \[\leq(1-2n\alpha(\mu-c_{n,1}^{\prime}\alpha\tau L^{2}))\mathbb{E}[ \|\theta_{k}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}]\] \[+c_{n,2}\alpha^{2}\tau L^{2}\mathbb{E}[\|\theta_{k}-\theta^{*}\| ^{2(n-1)}]|\mathcal{F}_{k-\tau}]+c_{n,3}\alpha^{2n}\tau^{2n-1}L^{2n}.\]

Following the induction hypothesis, when \(k\) is sufficiently large, we have

\[\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2(n-1)}|\mathcal{F}_{k-\tau}]\leq c_{n- 1}\alpha^{n-1}\tau^{n-1}s(\theta_{0},L,\mu).\]

Substituting the above upper bound back into our analysis of the \(2n\)-th moment bound, we obtain

\[\mathbb{E}[\|\theta_{k+1}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}] \leq(1-2n\alpha(\mu-c_{n,1}^{\prime}\alpha\tau L^{2}))\mathbb{E}[ \|\theta_{k}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}]\] \[+\alpha^{n+1}\tau^{n}L^{2}c_{n,2}\cdot c_{n-1}s(\theta_{0},L,\mu) +c_{n,3}\alpha^{2n}\tau^{2n-1}L^{2n}.\]

Subsequently, if we set \(\alpha\) sufficiently small, such that

\[\alpha\tau L^{2}<c_{n}\cdot\mu,\]

we obtain

\[\mathbb{E}[\|\theta_{k+1}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}]\leq(1-\alpha \mu)\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}]+\alpha^{n+1 }\tau^{n}c_{n,2}^{\prime}\cdot s(\theta_{0},L,\mu),\]

where \(s(\theta_{0},L,\mu)\) is some constant that may depend on the initialization \(\theta_{0}\) and the problem primitives \(\mu\) and \(L\) but is independent of \(\alpha\).

Recursively, we get

\[\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2n}]\leq(1-\alpha\mu)^{k-\tau}\mathbb{E}[ \|\theta_{\tau}-\theta^{*}\|^{2n}]+\alpha^{n}\tau^{n}\cdot s(\theta_{0},L,\mu).\]

Lastly, we recall that

\[\mathbb{E}[\|\theta_{\tau}-\theta^{*}\|^{2n}] \leq 2^{2n-1}\mathbb{E}[\|\theta_{\tau}-\theta_{0}\|^{2n}]+2^{2n-1} \mathbb{E}[\|\theta_{0}-\theta^{*}\|^{2n}]\] \[\leq c_{n,1}\alpha^{2n}\tau^{2n}L^{2n}(\mathbb{E}[\|\theta_{0}- \theta^{*}\|^{2n}]+1)+c_{n,2}\|\theta_{0}-\theta^{*}\|^{2n}\] \[\leq c_{n,1}\mathbb{E}[\|\theta_{0}-\theta^{*}\|^{2n}]+c_{n,2}\alpha^ {2n}\tau^{2n}L^{2n}.\]

Substituting back, we obtain for sufficiently large \(k\),

\[\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2n}]\leq c_{n,1}(1-\alpha\mu)^{k-\tau} \mathbb{E}[\|\theta_{0}-\theta^{*}\|^{2n}]+\alpha^{2n}\tau^{2n}s(\theta_{0},L,\mu).\]

As such, we have proven the desired \(n\)-th moment bound.

#### d.2.1 Proof of Lemma d.2

We now come back to Lemma D.2 and provide the complete proof.

Proof.: The proof follows a similar strategy as (D.9) and (D.10) in Section D.1.1.

We start with the following relaxation and obtain that

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2n}|\mathcal{F}_{k- \tau}]\leq\mathbb{E}\Big{[}\Big{(}\sum_{t=k-\tau}^{k-1}\|\theta_{t+1}-\theta_{ t}\|\Big{)}^{2n}|\mathcal{F}_{k-\tau}\Big{]}\] \[\leq\tau^{2n-1}\sum_{t=k-\tau}^{k-1}\mathbb{E}[\|\theta_{t+1}- \theta_{t}\|^{2n}|\mathcal{F}_{k-\tau}]\] \[=\alpha^{2n}\tau^{2n-1}\sum_{t=k-\tau}^{k-1}\mathbb{E}[\|g( \theta_{t},x_{t})+\xi_{t+1}(\theta_{t})\|^{2n}|\mathcal{F}_{k-\tau}]\] \[\leq 2^{2n-1}\alpha^{2n}\tau^{2n-1}\sum_{t=k-\tau}^{k-1}\Big{(} \mathbb{E}[\|g(\theta_{t},x_{t})\|^{2n}|\mathcal{F}_{k-\tau}]+\mathbb{E}[\| \xi_{t+1}(\theta_{t})\|^{2n}|\mathcal{F}_{k-\tau}]\Big{)}\] \[\leq 2^{2n-1}\alpha^{2n}\tau^{2n-1}\sum_{t=k-\tau}^{k-1}\Big{(}L_ {1}^{2n}\mathbb{E}[(\|\theta_{t}-\theta^{*}\|+1)^{2n}|\mathcal{F}_{k-\tau}]+L_ {2}^{2n}(\mathbb{E}[\|\theta_{t}-\theta^{*}\|\|\mathcal{F}_{k-\tau}]+1)^{2n} \Big{)}\] \[\leq 4^{2n-1}\alpha^{2n}\tau^{2n-1}L^{2n}\sum_{t=k-\tau}^{k-1} \mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}]+4^{2n-1} \alpha^{2n}\tau^{2n}L^{2n}.\]

Next, in order to obtain a bound on \(\|\theta_{t}-\theta^{*}\|^{2n}\), we study the following term.

\[\mathbb{E}[\|\theta_{t+1}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}] \leq\mathbb{E}\Big{[}\Big{(}\|\theta_{t+1}-\theta_{t}\|+\|\theta_{t}-\theta^{ *}\|\Big{)}^{2n}|\mathcal{F}_{k-\tau}\Big{]}\] \[=\sum_{i=0}^{2n}\binom{2n}{i}\mathbb{E}[\|\theta_{t+1}-\theta_{t} \|^{i}\|\theta_{t}-\theta^{*}\|^{2n-i}|\mathcal{F}_{k-\tau}]\] \[=\mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}]+ \sum_{i=1}^{2n}\alpha^{i}\mathbb{E}[\|g(\theta_{t},x_{t})+\xi_{t+1}(\theta_{t} )\|^{i}\|\theta_{t}-\theta^{*}\|^{2n-i}|\mathcal{F}_{k-\tau}]\]

Note that

\[\mathbb{E}[\|g(\theta_{t},x_{t})+\xi_{t+1}(\theta_{t})\|^{i}\| \theta_{t}-\theta^{*}\|^{2n-i}|\mathcal{F}_{k-\tau}]\] \[\leq 2^{i-1}\mathbb{E}\Big{[}\Big{(}\|g(\theta_{t},x_{t})\|^{i}+ \|\xi_{t+1}(\theta_{t})\|^{i}\Big{)}\|\theta_{t}-\theta^{*}\|^{2n-i}|\mathcal{ F}_{k-\tau}\Big{]}\] \[=2^{i-1}\mathbb{E}\Big{[}\mathbb{E}[(\|g(\theta_{t},x_{t})\|^{i} +\|\xi_{t+1}(\theta_{t})\|^{i})|\theta_{t}]\|\theta_{t}-\theta^{*}\|^{2n-i}| \mathcal{F}_{k-\tau}\Big{]}\] \[\leq 2^{i-1}L^{i}\mathbb{E}\Big{[}(\|\theta_{t}-\theta^{*}\|+1)^{ 2n}|\mathcal{F}_{k-\tau}\Big{]}\] \[\leq 2^{2(n-1)}2^{i}L^{i}\Big{(}\mathbb{E}[\|\theta_{t}-\theta^{*} \|\mathcal{F}_{k-\tau}]+1\Big{)}\]

Substituting back, we obtain

\[\sum_{i=1}^{2n}\alpha^{i}\mathbb{E}[\|g(\theta_{t},x_{t})+\xi_{t+ 1}(\theta_{t})\|^{i}\|\theta_{t}-\theta^{*}\|^{2n-i}|\mathcal{F}_{k-\tau}]\] \[\leq 2^{2(n-1)}\sum_{i=1}^{2n}2^{i}\alpha^{i}L^{i}\Big{(}\mathbb{E }[\|\theta_{t}-\theta^{*}\|\|\mathcal{F}_{k-\tau}]+1\Big{)}\] \[=2^{2(n-1)}\Big{(}\mathbb{E}[\|\theta_{t}-\theta^{*}\|\| \mathcal{F}_{k-\tau}]+1\Big{)}\cdot 2\alpha L(1+2\alpha L)^{2n-1}\]\[\leq 4^{2n-1}\alpha L\Big{(}\mathbb{E}[\|\theta_{t}-\theta^{*}\| |\mathcal{F}_{k-\tau}]+1\Big{)}\]

Consolidating the terms, we have

\[\mathbb{E}[\|\theta_{t+1}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}]\leq(1+4^{2n-1} \alpha L)\Big{(}\mathbb{E}[\|\theta_{t}-\theta^{*}\|\|\mathcal{F}_{k-\tau}]+1 \Big{)}\]

Recursively, for \(0\leq l\leq\tau\), we have

\[\mathbb{E}[\|\theta_{k-\tau+l}-\theta^{*}\|^{2n}|\mathcal{F}_{k- \tau}] \leq(1+4^{2n-1}\alpha L)^{l}\|\theta_{k-\tau}-\theta^{*}\|^{2n}+4^{2n -1}\alpha L\sum_{i=0}^{l-1}(1+4^{2n-1}\alpha L)^{i}\] \[=(1+4^{2n-1}\alpha L)^{l}\|\theta_{k-\tau}-\theta^{*}\|^{2n}+(1+ 4^{2n-1}\alpha L)^{l}\]

Then, for

\[4^{2n-1}\alpha\tau L\leq\mu/4L<1/4,\]

we have for \(k-\tau\leq t\leq k\),

\[\mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}] \leq(1+2^{4n-1}\alpha\tau L)\|\theta_{k-\tau}-\theta^{*}\|^{2n}+ 2^{4n-1}\alpha\tau L\] \[\leq 2\|\theta_{k-\tau}-\theta^{*}\|^{2n}+2^{4n-1}\alpha\tau L\]

Finally, we have

\[\mathbb{E}[\|\theta_{k}-\theta_{k-\tau}\|^{2n}|\mathcal{F}_{k- \tau}] \leq 4^{2n-1}\alpha^{2n}\tau^{2n-1}L^{2n}\sum_{t=k-\tau}^{k-1} \mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2n}|\mathcal{F}_{k-\tau}]+4^{2n-1} \alpha^{2n}\tau^{2n}L^{2n}\] \[\leq 4^{2n-1}\alpha^{2n}\tau^{2n}L^{2n}\Big{(}2\|\theta_{k- \tau}-\theta^{*}\|^{2n}+2^{4n-1}\alpha\tau L+1\Big{)}\] \[\leq 2^{4n-1}\alpha^{2n}\tau^{2n}L^{2n}\Big{(}\|\theta_{k-\tau}- \theta^{*}\|^{2n}+1\Big{)}.\]

As such, we have completed the proof. 

## Appendix E Proof of Theorem 4.1

In this section, we prove the weak convergence result in Theorem 4.1. In fact, the proof of the projected SA weak convergence result can be seen as a special case of unprojected SA with the asymptotic linearity condition, which we have briefly discussed in Section 4. Therefore, the proof proceeds in the following two subsections. First, we formally define the asymptotic linearity condition and present our weak convergence result for unprojected SA under this additional assumption. Next, we relate this result for unprojected SA to projected SA and specialize the proof to obtain Theorem 4.1.

### Asymptotic Linearity

In this subsection, we formally introduce the asymptotic linearity condition, which is crucial for establishing weak convergence in the context of unprojected SA (\(\beta=\infty\)). Additionally, we explore the implications of this condition.

**Assumption 6** (Asymptotic Linearity).: _The noise sequence \((\xi_{k})_{k\geq 1}\) is a collection of i.i.d. random fields satisfying the following conditions: (1) \(\mathbb{E}[\xi_{k+1}(\theta)|\mathcal{F}_{k}]=0\), (2) there exists a constant \(L_{3}>0\) such that \(\xi_{1}\) is \(L_{3}\)-Lipschitz, i.e., \(\|\xi_{1}(\theta)-\xi_{1}(\theta^{\prime})\|\leq L_{3}\|\theta-\theta^{\prime }\|\), for all \(\theta,\theta^{\prime}\in\mathbb{R}^{d}\), and (3) \(\|\xi_{1}(0)\|\leq L_{3}\)._

_Moreover, there exists a function \(G(\cdot):\mathcal{X}\to\mathbb{R}^{d\times d}\) such that given \(\epsilon>0\), define_

\[\delta(\epsilon):=\min\big{\{}\delta:\|g^{\prime}(\theta,x)-G(x)\|\leq \epsilon,\,\forall x\in\mathcal{X}\quad\text{and}\quad\forall\theta\in\{ \theta:\|\theta\|\geq\delta\}\big{\}},\]

_and we have \(\lim_{\epsilon\to 0}\epsilon\delta(\epsilon)=0\)._

The first part of Assumption 6 states that the random field grows at most linearly in \(\theta\). The second part of Assumption 6 implies that \(g^{\prime}(\theta,x)\) converges to a limit \(G(x)\) when \(\|\theta\|\to\infty\) for all \(x\in\mathcal{X}\), which shows the asymptotic linearity of \(g(\theta,x)\). Furthermore, Assumption 6 also requires how fast \(g^{\prime}(\theta,x)\) converges to \(G(x)\). A sufficient condition under which the second part of Assumption 6 holds is that there exists \(\omega>0\) such that \(\|\theta\|^{1+\omega}\|g^{\prime}(\theta,x)-G(x)\|<\infty\) for \(\forall\theta\in\mathbb{R}^{d}\) and \(x\in\mathcal{X}\). We can verify that to ensure \(\|g^{\prime}(\theta,x)-G(x)\|<\epsilon\), we can set \(\|\theta\|\in\Theta(\epsilon^{-\frac{1}{1+\omega}})\), which can ensure \(\epsilon\delta(\epsilon)\in\mathcal{O}(\epsilon^{\frac{\omega}{1+\omega}})\to 0\) as \(\epsilon\to 0\). This sufficient condition implies that \(g^{\prime}(\theta,x)\) uniformly converge to \(G(x)\) with convergence rate of \(\mathcal{O}(\|\theta\|^{-(1+w)})\). By definition, we conclude that the structure of linear SA is also asymptotic linear. Besides that, the 1-dimensional logistic regression also satisfies Assumption 6. For 1-dimensional logistic regression, we have \(g(\theta,x,y)=x\Big{(}\frac{1}{1+e^{-\theta x}}-y\Big{)}+\lambda\theta\), where \((x,y)\) presents the data. Therefore, we have \(g^{\prime}(\theta,x,y)=\frac{x^{2}e^{-\theta x}}{(1+e^{-\theta x})^{2}}+\lambda\) and \(g^{\prime}(\theta,x,y)\) uniformly converges to \(\lambda\) with geometric convergence rate, thereby satisfies the Assumption 6.

### Proof Under Assumption 6

With the asymptotic linearity condition now formally defined, we proceed to prove the weak convergence for unprojected SA. For convenient reference, we state the theorem below.

**Theorem E.1** (Ergodicity of SA-Asymptotic Linearity).: _Suppose that Assumption 1-Assumption 4 hold. Additionally, assume 6. For stepsize \(\alpha>0\) that satisfies the constraint \(\alpha\tau_{\alpha}L^{2}<\min(c_{2}\mu,\kappa_{\mu})\), with \(c_{2}\) formalized in Proposition 4.2 and \(\kappa_{\mu}\) defined in (E.1), the Markov chain \((x_{k},\theta_{k})_{k\geq 0}\) converges to a unique stationary distribution \(\bar{\nu}_{\alpha}\in\mathcal{P}_{2}(\mathcal{X}\times\mathbb{R}^{d})\)._

_Moreover, there exist \(\kappa_{\mu}>0\) and some universal constant \(c^{\prime}\) such that_

\[\epsilon\delta(\epsilon)\leq c^{\prime}\mu,\quad\forall\epsilon\leq\kappa_{\mu}.\] (E.1)

_We let \(\nu_{\alpha}:=\mathcal{L}(\theta_{\infty})\) be the second marginal of \(\bar{\nu}_{\alpha}\). For \(k\geq 2\tau_{\alpha}\), it holds that_

\[W_{2}(\mathcal{L}(\theta_{k}),\nu_{\alpha})\leq\bar{W}_{2}(\mathcal{L}(x_{k}, \theta_{k}),\bar{\nu}_{\alpha})\leq(1-\alpha\mu)^{k/2}\cdot s(\theta_{0},L,\mu).\] (E.2)

The proof of Theorem E.1 consists of two major steps. Firstly, we assume that \(x_{0}\sim\pi\), and show that \((x_{k},\theta_{k})_{k\geq 0}\) converges to a unique limiting invariant distribution. Next, we relax the assumption of \(x_{0}\sim\pi\), and prove that for arbitrary initialization \((x_{0},\theta_{0})\in\mathcal{X}\times\mathbb{R}^{d}\), the Markov chain will converge to the same limit.

Step 1: Initialization with \(x_{0}\sim\pi\).To prove the convergence of the Markov chain, we consider the following coupling construction. We have a pair of Markov chains \((x_{k},\theta_{k}^{[1]})_{k\geq 0}\) and \((x_{k},\theta_{k}^{[2]})_{k\geq 0}\) sharing the same underlying process and noise, i.e., \((x_{k},\xi_{k+1})_{k\geq 0}\), i.e.,

\[\theta_{k+1}^{[1]} =\theta_{k}^{[1]}+\alpha(g(\theta_{k}^{[1]},x_{k})+\xi_{k+1}( \theta_{k}^{[1]})),\] (E.3) \[\theta_{k+1}^{[2]} =\theta_{k}^{[2]}+\alpha(g(\theta_{k}^{[2]},x_{k})+\xi_{k+1}( \theta_{k}^{[2]})).\]

We assume that the initial iterates \(\theta_{0}^{[1]}\) and \(\theta_{0}^{[2]}\) may depend on each other and on \(x_{0}\), but are independent of subsequent \((x_{k})_{k\geq 1}\) given \(x_{0}\). For the iterates difference \(\theta_{k}^{[1]}-\theta_{k}^{[2]}\), we have the following Proposition E.2, whose proof is given at the end of this subsection.

**Proposition E.2**.: \(\forall k\geq\tau\) _and \(\alpha\tau\leq\min(\frac{\mu}{908L^{2}},\frac{\kappa_{\mu}}{L^{2}}),\) we have_

\[\mathbb{E}[\|\theta_{k}^{[1]}-\theta_{k}^{[2]}\|^{2}]\leq 4(1-\mu\alpha)^{k- \tau}\mathbb{E}[\|\theta_{0}^{[1]}-\theta_{0}^{[2]}\|^{2}],\]

_where \(\kappa_{\mu}>0\) and \(\epsilon\delta(\epsilon)\leq\frac{\mu}{768},\forall\epsilon\leq\kappa_{\mu}\)._

By Proposition E.2 and the definition of \(W_{2}\) and \(\bar{W}_{2}\), we have

\[W_{2}^{2}\left(\mathcal{L}(\theta_{k}^{[1]}),\mathcal{L}(\theta_ {k}^{[2]})\right) \overset{\rm(i)}{\leq}\bar{W}_{2}^{2}\left(\mathcal{L}(x_{k}, \theta_{k}^{[1]}),\mathcal{L}(x_{k},\theta_{k}^{[2]})\right)\] (E.4) \[\overset{\rm(ii)}{\leq}\mathbb{E}[\|\theta_{k}^{[1]}-\theta_{k}^ {[2]}\|^{2}]\] \[\overset{\rm(iii)}{\leq}4(1-\mu\alpha)^{k-\tau}\mathbb{E}[\| \theta_{0}^{[1]}-\theta_{0}^{[2]}\|^{2}],\]

where (i) and (ii) hold by the definition of \(W_{2}\) and \(\bar{W}_{2}\) and (iii) holds by applying Proposition E.2.

Note that equation (E.4) always holds for any joint distribution of initial iterates \((x_{0},\theta_{0}^{[1]},\theta_{0}^{[2]})\). Recall that \(P^{*}\) represents the transition kernel for the time-reversed Markov chain of \(\{x_{k}\}_{k\geq 0}\), and the initial distribution of \(x_{0}\) is assumed to be mixed already. Given a specific \(x_{0}\), we sample \(x_{-1}\) from \(P^{*}(\cdot\mid x_{0})\). Additionally, we use \(\theta_{-1}^{[2]}\) to denote the random varible that satisfies \(\theta_{-1}^{[2]}\stackrel{{ d}}{{=}}\theta_{0}^{[1]}\) and is independent of \(\{x_{k}\}_{k\geq 0}\). Finally, we set \(\theta_{0}^{[2]}\) as

\[\theta_{0}^{[2]}=\theta_{-1}^{[2]}+\alpha(g(x_{-1},\theta_{-1}^{[2]})+\xi_{0} (\theta_{-1}^{[2]})).\]

By the property of time-reversed Markov chain, we have \(\{x_{k}\}_{k\geq-1}\stackrel{{ d}}{{=}}\{x_{k}\}_{k\geq 0}\). Given that \(\theta_{-1}^{[2]}\stackrel{{ d}}{{=}}\theta_{0}^{[1]}\) and \(\theta_{-1}^{[2]}\) is independent with \(\{x_{k}\}_{k\geq-1}\), we can prove \((x_{k},\theta_{k}^{[2]})\stackrel{{ d}}{{=}}(x_{k+1},\theta_{k+1 }^{[1]})\) for \(k\geq 0\). We thus have for all \(k\geq\tau\):

\[\bar{W}_{2}^{2}\left(\mathcal{L}\left(x_{k},\theta_{k}^{[1]} \right),\mathcal{L}\left(x_{k+1},\theta_{k+1}^{[1]}\right)\right) =\bar{W}_{2}^{2}\left(\mathcal{L}\left(x_{k},\theta_{k}^{[1]} \right),\mathcal{L}\left(x_{k},\theta_{k}^{[2]}\right)\right)\] \[\stackrel{{\text{(i)}}}{{\leq}}4(1-\mu\alpha)^{k- \tau}\mathbb{E}[||\theta_{0}^{[1]}-\theta_{0}^{[2]}||^{2}],\]

where (i) holds by inequality (E.4). Then, we have

\[\sum_{k=0}^{\infty}\bar{W}_{2}^{2}\left(\mathcal{L}\left(x_{k}, \theta_{k}^{[1]}\right),\mathcal{L}\left(x_{k+1},\theta_{k+1}^{[1]}\right)\right)\] \[\leq \sum_{k=0}^{t_{\alpha}-1}\bar{W}_{2}^{2}\left(\mathcal{L}\left(x_ {k},\theta_{k}^{[1]}\right),\mathcal{L}\left(x_{k+1},\theta_{k+1}^{[1]}\right) \right)+4\mathbb{E}[||\theta_{0}^{[1]}-\theta_{0}^{[2]}||^{2}]\sum_{k=0}^{ \infty}(1-\mu\alpha)^{k}\] \[< \infty.\]

Consequently, \(\{\mathcal{L}(x_{k},\theta_{k}^{[1]})\}_{k\geq 0}\) forms a Cauchy sequence w.r.t. the metric \(\bar{W}_{2}\). Since the space \(\mathcal{P}_{2}(\mathcal{X}\times\mathbb{R}^{d})\) endowed with \(\bar{W}_{2}\) is a Polish space, every Cauchy sequence converges [63, Theorem 6.18]. Furthermore, convergence in Wasserstein 2-distance also implies weak convergence [63, Theorem 6.9]. Therefore, we conclude that the sequence \((\mathcal{L}(x_{k},\theta_{k}^{[1]}))_{k\geq 0}\) converges weakly to a limit distribution \(\bar{\mu}\in\mathcal{P}_{2}(\mathcal{X}\times\mathbb{R}^{d})\).

Now that we have established the existence of a limiting distribution, we next proceed to show the uniqueness. We prove this by contradiction. Note that we currently assume that \(x_{0}\sim\pi\), hence to show that the limit \((x_{\infty},\theta_{\infty})\) is unique, we only need to show that the limit is independent of the initial distribution of \(\theta_{0}\), which can be correlated to \(x_{0}\).

Consider two Markov chains \((x_{k},\theta_{k})_{k\geq 0}\) and \((x_{k},\theta_{k}^{\prime})_{k\geq 0}\), sharing \((x_{k},\xi_{k+1})_{k\geq 0}\) but with arbitrary initialization of \(\theta_{0}\) and \(\theta_{0}^{\prime}\). For the sake of contradiction, we assume that \((x_{0},\theta_{0})\Rightarrow(x_{\infty},\theta_{\infty})\) and \((x_{0},\theta_{0}^{\prime})\Rightarrow(x_{\infty},\theta_{\infty}^{\prime})\) respectively. Then, by the triangle inequality, we have that

\[\bar{W}_{2}\Big{(}(x_{\infty},\theta_{\infty}),(x_{\infty},\theta_ {\infty}^{\prime})\Big{)}\] \[\leq\bar{W}_{2}\Big{(}(x_{\infty},\theta_{\infty}),(x_{k},\theta _{k})\Big{)}+\bar{W}_{2}\Big{(}(x_{k},\theta_{k}),(x_{k},\theta_{k}^{\prime}) \Big{)}+\bar{W}_{2}\Big{(}(x_{k},\theta_{k}^{\prime}),(x_{\infty},\theta_{ \infty}^{\prime})\Big{)}\] \[\to 0.\]

As such, we have shown that the limit \(\bar{\nu}\) is unique.

Lastly, we prove that \(\bar{\nu}\) is invariant. Suppose that we initialize the joint process at its limit, i.e., \((x_{0},\theta_{0})\sim\bar{\nu}\). We first apply the triangle inequality, and we obtain

\[\bar{W}_{2}\Big{(}(x_{1},\theta_{1}),(x_{0},\theta_{0})\Big{)}\leq\bar{W}_{2} \Big{(}(x_{1},\theta_{1}),(x_{k+1},\theta_{k+1})\Big{)}+\bar{W}_{2}\Big{(}(x_{k+ 1},\theta_{k+1}),(x_{0},\theta_{0})\Big{)}.\]

Clearly, as \(k\to\infty\), \(\bar{W}_{2}\Big{(}(x_{k+1},\theta_{k+1}),(x_{0},\theta_{0})\Big{)}\to 0\). To bound \(\bar{W}_{2}\Big{(}(x_{1},\theta_{1}),(x_{k+1},\theta_{k+1})\Big{)}\), we need the following lemma.

**Lemma E.3**.: _Consider two copies of the SA trajectory, where \(\mathcal{L}(x_{0},\theta_{0})=\bar{\nu}\) and \(\mathcal{L}(x_{0}^{\prime},\theta_{0}^{\prime})\) is allowed to be arbitrary._

\[\bar{W}_{2}^{2}\Big{(}\mathcal{L}(x_{1},\theta_{1}),\mathcal{L}(x_{1}^{\prime}, \theta_{1}^{\prime})\Big{)}\leq\rho_{1}\cdot\bar{W}_{2}^{2}\Big{(}\mathcal{L}(x_ {0},\theta_{0}),\mathcal{L}(x_{0}^{\prime},\theta_{0}^{\prime})\Big{)}+\rho_{2} \cdot\sqrt{\bar{W}_{2}^{2}\Big{(}\mathcal{L}(x_{0},\theta_{0}),\mathcal{L}(x_{0}^{ \prime},\theta_{0}^{\prime})\Big{)}},\]_where_

\[\rho_{1}:=1+2(1+\alpha L)^{2}+16\alpha^{2}L^{2}<\infty\quad\text{and}\quad\rho_{2} :=16\alpha^{2}L^{2}\sqrt{\mathbb{E}[\|\theta_{0}\|^{4}]}<\infty\]

_are independent of \(\mathcal{L}(x_{0}^{\prime},\theta_{0}^{\prime})\)._

Proof.: Consider the following coupling between the two processes \((x_{k},\theta_{0})_{k\geq 0}\) and \((x_{k}^{\prime},\theta_{k}^{\prime})_{k\geq 0}\)

\[\bar{W}_{2}^{2}\Big{(}\mathcal{L}(x_{0},\theta_{0}),\mathcal{L}(x _{0}^{\prime},\theta_{0}^{\prime})\Big{)} =\mathbb{E}\Big{[}d_{0}(x_{0},x_{0}^{\prime})+\|\theta_{0}-\theta _{0}^{\prime}\|^{2}\Big{]}\quad\text{and}\] \[x_{k+1} =x_{k+1}^{\prime}\quad\text{if }x_{k}=x_{k}^{\prime},\quad\forall k \geq 0.\]

Then, it is clear that

\[\bar{W}_{2}^{2}\Big{(}\mathcal{L}(x_{1},\theta_{1}),\mathcal{L}(x_{1}^{\prime },\theta_{1}^{\prime})\Big{)}\leq\mathbb{E}\Big{[}d_{0}(x_{1},x_{1}^{\prime}) +\|\theta_{1}-\theta_{1}^{\prime}\|^{2}\Big{]}.\]

Recall the metric \(d_{0}(x,x^{\prime})=\mathbbm{1}\{x\neq x^{\prime}\}\) and hence, we have

\[g(\theta_{0},x_{0})=g(\theta_{0},x_{0}^{\prime})+d_{0}(x_{0}^{\prime},x_{0})(g (\theta_{0},x_{0})-g(\theta_{0},x_{0}^{\prime})).\]

Therefore, it is easy to see that

\[\theta_{1}-\theta_{1}^{\prime} =\theta_{0}-\theta_{0}^{\prime}+\alpha(g(\theta_{0},x_{0})-g( \theta_{0}^{\prime},x_{0}^{\prime}))+\alpha(\xi_{1}(\theta_{0})-\xi_{1}( \theta_{0}^{\prime}))\] \[=\theta_{0}-\theta_{0}^{\prime}+\alpha(g(\theta_{0},x_{0})\mp g( \theta_{0},x_{0}^{\prime})-g(\theta_{0}^{\prime},x_{0}^{\prime}))+\alpha(\xi_ {1}(\theta_{0})-\xi_{1}(\theta_{0}^{\prime}))\] \[=\theta_{0}-\theta_{0}^{\prime}+\alpha(g(\theta_{0},x_{0}^{\prime })-g(\theta_{0}^{\prime},x_{0}^{\prime}))+\alpha(\xi_{1}(\theta_{0})-\xi_{1}( \theta_{0}^{\prime}))\] \[+\alpha d_{0}(x_{0}^{\prime},x_{0})(g(\theta_{0},x_{0})-g(\theta _{0},x_{0}^{\prime})),\]

whence

\[\|\theta_{1}-\theta_{1}^{\prime}\| \leq(1+\alpha L)\|\theta_{0}-\theta_{0}^{\prime}\|+\alpha d_{0}( x_{0}^{\prime},x_{0})\|g(\theta_{0},x_{0})-g(\theta_{0},x_{0}^{\prime})\|\] \[\leq(1+\alpha L)\|\theta_{0}-\theta_{0}^{\prime}\|+\alpha d_{0}( x_{0}^{\prime},x_{0})\cdot 2L(\|\theta_{0}\|+1).\]

As such, we see that

\[\mathbb{E}[d_{0}(x_{1},x_{1}^{\prime})+\|\theta_{1}-\theta_{1}^{ \prime}\|^{2}] \leq\mathbb{E}[d_{0}(x_{0},x_{0}^{\prime})]+2(1+\alpha L)^{2} \cdot\mathbb{E}[\|\theta_{0}-\theta_{0}^{\prime}\|^{2}]\] \[+16\alpha^{2}L^{2}\cdot\mathbb{E}[d_{0}(x_{0}^{\prime},x_{0})(\| \theta_{0}\|^{2}+1)].\]

Next, we make use of Cauchy-Schwarz inequality and obtain

\[\mathbb{E}[d_{0}(x_{0}^{\prime},x_{0})\cdot\|\theta_{0}\|^{2}]\leq\sqrt{ \mathbb{E}[d_{0}(x_{0}^{\prime},x_{0})]}\sqrt{\mathbb{E}_{\theta_{0}\sim\mu}[ \|\theta_{0}\|^{4}]}.\]

Because Assumption 6 implies Assumption 4\((p=2)\), by Proposition 4.2 and Fatou's lemma, we have

\[\mathbb{E}[\|\theta_{\infty}-\theta^{*}\|^{4}]\leq\liminf_{k\to\infty} \mathbb{E}[\|\theta_{k}-\theta^{*}\|_{\infty}^{4}]<\infty,\]

which implies \(\mathbb{E}[\|\theta_{\infty}\|^{4}]<\infty\). Hence, the desired inequality follows through. 

By Lemma E.3, we can set \(\mathcal{L}(x_{0}^{\prime},\theta_{0}^{\prime})=\mathcal{L}(x_{k},\theta_{k})\), then

\[\bar{W}_{2}^{2}\left(\mathcal{L}\left(x_{1},\theta_{1}\right),\mathcal{L}(x_{k +1},\theta_{k+1})\right)\leq\rho_{1}\bar{W}_{2}^{2}\left(\bar{\mu},\mathcal{L}( x_{k},\theta_{k})\right)+\rho_{2}\sqrt{\bar{W}_{2}^{2}\left(\bar{\nu},\mathcal{L}(x_{k}, \theta_{k})\right)}.\]

Therefore, \(\bar{W}_{2}^{2}\left(\mathcal{L}\left(x_{1},\theta_{1}\right),\mathcal{L}(x_{k +1},\theta_{k+1})\right)\to 0\) as \(k\to 0\), which implies \(\bar{W}_{2}\Big{(}(x_{1},\theta_{1}),(x_{0},\theta_{0})\Big{)}=0\). As such, we have proved the joint sequence \((x_{k},\theta_{k})_{k\geq 0}\) converges weakly to the unique invariant distribution \(\bar{\nu}\in\mathcal{P}_{2}(\mathcal{X}\times\mathbb{R}^{d})\). As a result, \(\{\theta_{k}\}_{k\geq 0}\) converges weakly to \(\mu\in\mathcal{P}_{2}(\mathbb{R}^{d})\), where \(\mu\) is the second marginal of \(\bar{\mu}\) over \(\mathbb{R}^{d}\).

Lastly, before proceeding to the next step, in which we remove the assumption \(x_{0}\sim\pi\), we first derive the convergence rate of \(\{\theta_{k}\}_{k\geq 0}\) under \(x_{0}\sim\pi\) as presented in the following lemma. This lemma will help us to establish the convergence rate without \(x_{0}\sim\pi\).

**Lemma E.4**.: _Under \(x_{0}\sim\pi\), Assumption 1-4 and 6 and the same setting as Proposition E.2,_

\[W_{2}^{2}\left(\mathcal{L}(\theta_{k}),\nu_{\alpha}\right)\leq\bar{W}_{2}^{2} \left(\mathcal{L}(x_{k},\theta_{k}),\bar{\nu}_{\alpha}\right)\leq 16(1-\mu \alpha)^{k}\cdot\left(\mathbb{E}\left[\|\theta_{0}^{[1]}-\theta^{*}\|^{2} \right]+c^{\prime}_{2,2}\right).\]

Proof.: Let us consider the coupled processes defined as equation (E.3). Suppose that the initial iterate \((x_{0},\theta_{0}^{[2]})\) follows the stationary distribution \(\bar{\nu}\), thus \(\mathcal{L}(x_{k},\theta_{k}^{[2]})=\bar{\nu}\) and \(\mathcal{L}(\theta_{k}^{[2]})=\nu\) for all \(k\geq 0\). By equation (E.4), we have for all \(k\geq\tau:\)

\[W_{2}^{2}\left(\mathcal{L}(\theta_{k}^{[1]}),\mu\right) =W_{2}^{2}\left(\mathcal{L}(\theta_{k}^{[1]}),\mathcal{L}(\theta _{k}^{[2]})\right)\] \[\leq\bar{W}_{2}^{2}\left(\mathcal{L}(x_{k},\theta_{k}^{[1]}), \mathcal{L}(x_{k},\theta_{k}^{[2]})\right)\] \[\leq 4(1-\mu\alpha)^{k-\tau}\mathbb{E}[\|\theta_{0}^{[1]}- \theta_{0}^{[2]}\|^{2}]\] (E.5) \[\leq 8(1-\mu\alpha)^{k-\tau}\cdot\left(\mathbb{E}\left[\|\theta_{0 }^{[1]}-\theta^{*}\|^{2}\right]+\mathbb{E}\left[\|\theta_{\infty}-\theta^{*} \|^{2}\right]\right)\] \[\leq 16(1-\mu\alpha)^{k}\cdot\left(\mathbb{E}\left[\|\theta_{0}^{[ 1]}-\theta^{*}\|^{2}\right]+\mathbb{E}\left[\|\theta_{\infty}-\theta^{*}\|^{2} \right]\right),\]

where we make use of the derivation in (D.13) to obtain the last inequality.

We note that

\[\mathbb{E}[\|\theta_{\infty}-\theta^{*}\|^{2}]\leq\lim\inf_{k\to\infty} \mathbb{E}[\|\theta_{k}-\theta^{*}\|_{\infty}^{2}]\leq c_{2,2}\cdot\alpha\tau L ^{2}/\mu\leq c^{\prime}_{2,2},\]

the last inequality holds for \(\alpha\tau\leq\min(\frac{\mu}{908L^{2}},\frac{\bar{\nu}_{\alpha}}{L^{2}}).\) Therefore, we prove the desired inequality

\[W_{2}^{2}\left(\mathcal{L}(\theta_{k}^{[1]}),\mu\right)\leq 16(1-\mu\alpha)^{k} \cdot\left(\mathbb{E}\left[\|\theta_{0}^{[1]}-\theta^{*}\|^{2}\right]+c^{ \prime}_{2,2}\right).\]

Step 2: Arbitrary Initialization for \((x_{0},\theta_{0})\).In this step, we remove the assumption of \(x_{0}\sim\pi\) needed in the previous step. We need the following lemma to prove our result.

**Lemma E.5**.: _Consider two trajectories \((x_{k},\theta_{k})_{k\geq 0}\) and \((x^{\prime}_{k},\theta^{\prime}_{k})_{k\geq 0}\). Suppose that \(\theta_{0}=\theta^{\prime}_{0},\)\(x^{\prime}_{0}\sim\pi\) and \(x_{0}\) is initialized from some arbitrary distribution that satisfies \(\|\mathcal{L}(x_{0})-\pi\|_{\mathrm{TV}}=\epsilon.\) Then for \(k\geq\tau,\) we have_

\[\bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}),\mathcal{L}(x^{\prime}_{k},\theta^{ \prime}_{k}))\leq\epsilon\Big{(}4c_{2,1}(1-\alpha\mu)^{k}\mathbb{E}[\|\theta_{ 0}-\theta^{*}\|^{2}]+4c_{s,2}\alpha\tau\cdot\frac{L^{2}}{\mu}+1\Big{)}^{1/2}.\]

Proof.: We consider the following coupling between two joint processes \((x_{k},\theta_{k})_{k\geq 0}\) and \((x^{\prime}_{k},\theta^{\prime}_{k})_{k\geq 0}\). We first apply the maximal coupling on \(x_{0}\) and \(x^{\prime}_{0}\) such that

\[\mathbb{P}(x_{0}\neq x^{\prime}_{0})=\|\mathcal{L}(x_{0})-\mathcal{L}(x^{\prime }_{0})\|_{\mathrm{TV}}=\epsilon.\]

For the case \(x_{0}=x^{\prime}_{0},\) we can couple the two Markov chains \(\{x_{k}\}_{k\geq 0}\) and \(\{x^{\prime}_{k}\}_{k\geq 0}\) such that

\[x_{k}\equiv x^{\prime}_{k},\forall k\geq 0.\]

Under this coupling, we have \(\theta_{k}\equiv\theta^{\prime}_{k},\forall k\geq 0.\)

For the case \(x_{0}\neq x^{\prime}_{0},\) we let the two processes \((x_{k},\theta_{k})_{k\geq 1}\) and \((x^{\prime}_{k},\theta^{\prime}_{k})_{k\geq 1}\) evolve independently.

Given the above coupling, we first observe that

\[\bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}),\mathcal{L}(x^{\prime}_{ k},\theta^{\prime}_{k})) =\mathbb{E}[\bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}),\mathcal{L}(x^{ \prime}_{k},\theta^{\prime}_{k}))|x_{0}=x^{\prime}_{0}]\mathbb{P}(x_{0}=x^{ \prime}_{0})\] \[+\mathbb{E}[\bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}),\mathcal{L}( x^{\prime}_{k},\theta^{\prime}_{k}))|x_{0}\neq x^{\prime}_{0}]\mathbb{P}(x_{0}\neq x^{ \prime}_{0})\] \[=\epsilon\mathbb{E}[\bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}), \mathcal{L}(x^{\prime}_{k},\theta^{\prime}_{k}))|x_{0}\neq x^{\prime}_{0}].\]

The second equality holds since \(\mathbb{E}[\bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}),\mathcal{L}(x_{k},\theta^{ \prime}_{k}))|x_{0}=x^{\prime}_{0}]=0\) and \(\mathbb{P}(x_{0}\neq x^{\prime}_{0})=\epsilon\).

Next, we note the following upper bound of the Wasserstein distance,

\[\bar{W}_{2}^{2}(\mathcal{L}(x_{k},\theta_{k}),\mathcal{L}(x^{\prime}_{k},\theta^{ \prime}_{k}))=\inf\mathbb{E}\Big{[}\,\mathbbm{1}\{x_{k}\neq x^{\prime}_{k}\}+\| \theta_{k}-\theta^{\prime}_{k}\|^{2}\Big{]}\]\[\leq 1+2\Big{(}\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}]+\mathbb{E}[\| \theta_{k}^{\prime}-\theta^{*}\|^{2}]\Big{)}.\]

Making use of Proposition 4.2, we have

\[\bar{W}_{2}^{2}(\mathcal{L}(x_{k},\theta_{k}),\mathcal{L}(x_{k}^{ \prime},\theta_{k}^{\prime}))\] \[\leq 2c_{2,1}(1-\alpha\mu)^{k}\Big{(}\mathbb{E}[\|\theta_{0}-\theta^{ *}\|^{2}]+\mathbb{E}[\|\theta_{0}^{\prime}-\theta^{*}\|^{2}]\Big{)}+4c_{2,2} \alpha\tau\cdot\frac{L^{2}}{\mu}+1\] \[\leq 4c_{2,1}(1-\alpha\mu)^{k}\mathbb{E}[\|\theta_{0}-\theta^{*}\|^{ 2}]+4c_{2,2}\alpha\tau\cdot\frac{L^{2}}{\mu}+1,\]

where the second inequality holds for \(\theta_{0}=\theta_{0}^{\prime}\) by assumption.

Note that the above upper bound to the Wasserstein distance is independent of the choice of \((x_{0},x_{0}^{\prime})\). Hence, we can conclude that

\[\bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}),\mathcal{L}(x_{k}^{ \prime},\theta_{k}^{\prime})) =\epsilon\mathbb{E}[\bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}), \mathcal{L}(x_{k}^{\prime},\theta_{k}^{\prime}))|x_{0}\neq x_{0}^{\prime}]\] \[\leq\epsilon\Big{(}4c_{2,1}(1-\alpha\mu)^{k}\mathbb{E}[\|\theta_ {0}-\theta^{*}\|^{2}]+4c_{2,2}\alpha\tau\cdot\frac{L^{2}}{\mu}+1\Big{)}^{1/2}.\]

We complete the proof of the lemma. 

By Lemma E.5, we see that when \(x_{0}\) is close to its stationary distribution \(\pi\), \(\theta_{k}\) would not deviate too much from \(\theta_{k}^{\prime}\), as if it were initialized from the stationary distribution.

Now we consider a joint process \((x_{k},\theta_{k})_{k\geq 0}\) with arbitrary initialization. By the property of uniform ergodicity of \((x_{k})_{k\geq 0}\), we know that \(\|\mathcal{L}(x_{k})-\pi\|_{\mathrm{TV}}\leq Rr^{k}\). Choose time \(t_{0}\geq 0\). We construct a second Markov chain \((x_{k}^{\prime},\theta_{k}^{\prime})_{k\geq t_{0}}\) with the following properties: (1) \(x_{t_{0}}^{\prime}\sim\pi\) and is maximally coupled to \(x_{k_{0}}\), i.e., \(\|\mathcal{L}(x_{t_{0}})-\mathcal{L}(x_{t_{0}}^{\prime})\|_{\mathrm{TV}}= \mathbb{P}(x_{t_{0}}\neq x_{t_{0}}^{\prime})\) and (2) \(\theta_{t_{0}}^{\prime}=\theta_{k_{0}}\). Under this construction, for \(k\geq t_{0}+\tau\), we have

\[\bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}),\bar{\nu})\leq \bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}),\mathcal{L}(x_{k}^{ \prime},\theta_{k}^{\prime}))+\bar{W}_{2}(\mathcal{L}(x_{k}^{\prime},\theta_{ k}^{\prime}),\bar{\nu})\] \[\leq Rr^{t_{0}}\Big{(}4c_{2,1}(1-\alpha\mu)^{k-t_{0}}\mathbb{E}[\| \theta_{t_{0}}-\theta^{*}\|^{2}]+4c_{2,2}\alpha\tau\cdot\frac{L^{2}}{\mu}+1 \Big{)}^{1/2}\] \[+16(1-\alpha\mu)^{k-t_{0}}\Big{(}\mathbb{E}\left[\|\theta_{t_{0}} -\theta^{*}\|^{2}\right]+c_{2,2}^{\prime}\Big{)},\]

where the last inequality follows from Lemma E.4 and Lemma E.5.

For each \(t\) with \(t_{0}\geq\tau\), set \(t_{0}=t/2\). From the above inequality, we obtain that

\[\bar{W}_{2}(\mathcal{L}(x_{t},\theta_{t}),\bar{\nu})\] \[\leq Rr^{t/2}\Big{(}4c_{2,1}(1-\alpha\mu)^{t/2}\mathbb{E}[\|\theta_{t _{0}}-\theta^{*}\|^{2}]+4c_{2,2}\alpha\tau\cdot\frac{L^{2}}{\mu}+1\Big{)}^{1/2}\] \[\leq Rr^{t/2}\Big{(}4c_{2,1}(1-\alpha\mu)^{t/2}\cdot\Big{(}8(1-\alpha \mu)^{t_{0}}\mathbb{E}[\|\theta_{0}-\theta^{*}\|^{2}]+c_{2,2}\alpha\tau\cdot \frac{L^{2}}{\mu}\Big{)}+4c_{2,2}\alpha\tau\cdot\frac{L^{2}}{\mu}+1\Big{)}^{1/2}\] \[+16(1-\alpha\mu)^{t/2}\Big{(}\Big{(}8(1-\alpha\mu)^{t_{0}} \mathbb{E}[\|\theta_{0}-\theta^{*}\|^{2}]+c_{2,2}\alpha\tau\cdot\frac{L^{2}}{ \mu}\Big{)}+c_{2,2}^{\prime}\Big{)}\] \[\leq \max(r,1-\alpha\mu)^{t/2}\cdot s(\theta_{0},\theta^{*},\mu,L,R)\] \[\leq (1-\alpha\mu)^{t/2}\cdot s(\theta_{0},\theta^{*},\mu,L,R)\]

where \(s(\theta_{0},L,\mu)\) denote some constant that depends on the initialization of \(\theta_{0}\), and problem primitives \(L,\mu\), but independent of stepsize \(\alpha\) and iteration index \(t\). Last inequality holds because \(\mu\leq 1-r\) and \(\alpha\leq\frac{\mu}{908L^{2}}\leq 1\).

Therefore, as \(t\to\infty\), we obtain that \(\bar{W}_{2}(\mathcal{L}(x_{k},\theta_{k}),\bar{\nu})\to 0\), which implies that the Markov chain \((x_{k},\theta_{k})_{k\geq 0}\) with arbitrary initailization converges to the same \(\bar{\nu}\). As such, we have proved the desired weak convergence result without the assumption on \(x_{0}\sim\pi\) initialization.

Additionally, we obtain the following convergence rate. For any initialization \((x_{0},\theta_{0})\in\mathcal{X}\times\mathbb{R}^{d}\), we have

\[W_{2}(\mathcal{L}(\theta_{t}),\mu)\leq\bar{W}_{2}(\mathcal{L}(x_{t},\theta_{t}), \bar{\nu})\leq(1-\alpha\mu)^{t/2}\cdot s(\theta_{0},L,\mu).\]

#### e.2.1 Proof of Proposition e.2

First, we present the following lemma that is similar to [17, Lemma 2.3].

**Lemma E.6**.: _For any \(k_{1}<k_{2}\) satisfying \(\alpha(k_{2}-k_{1})\leq\frac{1}{8L}\), the following six inequalities hold:_

\[\|\theta_{k_{2}}^{[1]}-\theta_{k_{2}}^{[2]}-\theta_{k_{1}}^{[1]}+ \theta_{k_{1}}^{[2]}\| \leq 8\alpha L(k_{2}-k_{1})\|\theta_{k_{2}}^{[1]}-\theta_{k_{2}}^{ [2]}\|\] \[\|\theta_{k_{2}}^{[1]}-\theta_{k_{1}}^{[2]}-\theta_{k_{1}}^{[1]}+ \theta_{k_{1}}^{[2]}\| \leq 8\alpha L(k_{2}-k_{1})\|\theta_{k_{1}}^{[1]}-\theta_{k_{1}}^{ [2]}\|\] \[\|\theta_{k_{2}}^{[1]}-\theta_{k_{1}}^{[1]}\| \leq 8\alpha L(k_{2}-k_{1})\left(\|\theta_{k_{2}}^{[1]}\|+1\right)\] \[\|\theta_{k_{2}}^{[1]}-\theta_{k_{1}}^{[1]}\| \leq 8\alpha L(k_{2}-k_{1})\left(\|\theta_{k_{1}}^{[1]}\|+1\right)\] \[\|\theta_{k_{2}}^{[2]}-\theta_{k_{1}}^{[2]}\| \leq 8\alpha L(k_{2}-k_{1})\left(\|\theta_{k_{2}}^{[2]}\|+1\right)\] \[\|\theta_{k_{2}}^{[2]}-\theta_{k_{1}}^{[2]}\| \leq 8\alpha L(k_{2}-k_{1})\left(\|\theta_{k_{1}}^{[2]}\|+1 \right).\]

Proof.: Consider the coupling given by equation (E.3), by Assumption 2 and 6, we have

\[\|\theta_{k+1}^{[1]}-\theta_{k+1}^{[2]}\|-\|\theta_{k}^{[1]}- \theta_{k}^{[2]}\| \leq\|\theta_{k+1}^{[1]}-\theta_{k+1}^{[2]}-\theta_{k}^{[1]}+ \theta_{k}^{[2]}\|\] \[=\alpha\|g(\theta_{k}^{[1]},x_{k})+\xi_{k+1}(\theta_{k}^{[1]})-g (\theta_{k}^{[1]},x_{k})-\xi_{k+1}(\theta_{k}^{[1]})\|\] \[\leq 2\alpha L\|\theta_{k}^{[1]}-\theta_{k}^{[2]}\|.\]

Given \(k_{1}<k_{2}\), for \(\forall t\in[k_{1},k_{2}]\), since \(1+x\leq e^{x}\) for \(\forall x\in R\), we have

\[\|\theta_{t}^{[1]}-\theta_{t}^{[2]}\| \leq\prod_{j=k_{1}}^{t-1}(1+2\alpha L)\|\theta_{k_{1}}^{[1]}- \theta_{k_{1}}^{[2]}\|\] \[\leq\exp(2\alpha(k_{2}-k_{1})L)\|\theta_{k_{1}}^{[1]}-\theta_{k_ {1}}^{[2]}\|\] \[\overset{(i)}{\leq}(1+4\alpha(k_{2}-k_{1})L)\|\theta_{k_{1}}^{[1 ]}-\theta_{k_{1}}^{[2]}\|\] \[\leq 2\|\theta_{k_{1}}^{[1]}-\theta_{k_{1}}^{[2]}\|,\]

where (i) holds for \(e^{x}\leq 1+2x\)\(\forall x\in[0,\frac{1}{2}]\) and \(\alpha(k_{2}-k_{1})\leq\frac{1}{8L}\).

Then, we have

\[\|\theta_{k_{2}}^{[1]}-\theta_{k_{2}}^{[2]}-\theta_{k_{1}}^{[1]}+ \theta_{k_{1}}^{[2]}\| \leq\sum_{t=k_{1}}^{k_{2}-1}\|\theta_{t+1}^{[1]}-\theta_{t+1}^{[2 ]}-\theta_{t}^{[1]}+\theta_{t}^{[2]}\|\leq 4\alpha(k_{2}-k_{1})L\|\theta_{k_{1}}^{[1]}- \theta_{k_{1}}^{[2]}\|.\]

Therefore, following \(\alpha(k_{2}-k_{1})\leq\frac{1}{8L}\), we have

\[\|\theta_{k_{2}}^{[1]}-\theta_{k_{2}}^{[2]}-\theta_{k_{1}}^{[1]}+ \theta_{k_{1}}^{[2]}\| \leq 4\alpha(k_{2}-k_{1})L\|\theta_{k_{1}}^{[1]}-\theta_{k_{1}}^{[2 ]}\|\] \[\leq 4\alpha(k_{2}-k_{1})L(\|\theta_{k_{2}}^{[1]}-\theta_{k_{2}}^{[2 ]}-\theta_{k_{1}}^{[1]}+\theta_{k_{1}}^{[2]}\|+\|\theta_{k_{2}}^{[1]}-\theta_ {k_{2}}^{[2]}\|)\] \[\leq\frac{1}{2}\|\theta_{k_{2}}^{[1]}-\theta_{k_{2}}^{[2]}- \theta_{k_{1}}^{[1]}+\theta_{k_{1}}^{[2]}\|+4\alpha(k_{2}-k_{1})L\|\theta_{k_ {2}}^{[1]}-\theta_{k_{2}}^{[2]}\|.\]

Then, by rearranging the terms, we have

\[\|\theta_{k_{2}}^{[1]}-\theta_{k_{2}}^{[2]}-\theta_{k_{1}}^{[1]}+ \theta_{k_{1}}^{[2]}\|\leq 8\alpha(k_{2}-k_{1})L\|\theta_{k_{2}}^{[1]}- \theta_{k_{2}}^{[2]}\|,\]

thereby we have proved the first two inequalities of Lemma E.6.

By Assumption 2 and 6 and [17, Lemma 2.3], we can prove the last four inequalities and we omit the details here. 

Now, we are ready to prove Proposition E.2. We start with the following decomposition. By equation (E.3), we first have

\[\mathbb{E}[\|\theta_{k+1}^{[1]}-\theta_{k+1}^{[2]}\|^{2}]= \mathbb{E}\left[\|\theta_{k}^{[1]}-\theta_{k}^{[2]}+\alpha\left(g (\theta_{k}^{[1]},x_{k})-g(\theta_{k}^{[2]},x_{k})+\xi_{k+1}(\theta_{k}^{[1] })-\xi_{k+1}(\theta_{k}^{[2]})\right)\|^{2}\right]\]\[= \mathbb{E}\left[\|\theta_{k}^{[1]}-\theta_{k}^{[2]}\|^{2}\right]+2 \alpha\underbrace{\mathbb{E}\left[\langle\theta_{k}^{[1]}-\theta_{k}^{[2]},g( \theta_{k}^{[1]},x_{k})-g(\theta_{k}^{[2]},x_{k})\rangle\right]}_{\mathcal{T}_{1}}\] \[+\alpha^{2}\underbrace{\mathbb{E}\left[\|g(\theta_{k}^{[1]},x_{k })-g(\theta_{k}^{[2]},x_{k})+\xi_{k+1}(\theta_{k}^{[1]})-\xi_{k+1}(\theta_{k}^ {[2]})\|^{2}\right]}_{\mathcal{T}_{2}}.\]

For \(T_{2}\), by Assumption 2 and 6, we have

\[T_{2}\leq 4L^{2}\mathbb{E}\left[\|\theta_{k}^{[1]}-\theta_{k}^{[2]}\|^{2} \right].\]

We denote \(\varepsilon(\cdot,x_{k}):=g(\cdot,x_{k})-\bar{g}(\cdot)\) to be the noise function. Then, by Assumption 2, we conclude that \(\varepsilon(\cdot,x_{k})\) is \(2L\)-Lipschitz continuous. Therefore, \(T_{1}\) can be rewritten as:

\[T_{1}=\mathbb{E}\left[\langle\theta_{k}^{[1]}-\theta_{k}^{[2]},\varepsilon( \theta_{k}^{[1]},x_{k})-\varepsilon(\theta_{k}^{[2]},x_{k})\rangle\right]+ \mathbb{E}\left[\langle\theta_{k}^{[1]}-\theta_{k}^{[2]},\bar{g}(\theta_{k}^{[ 1]})-\bar{g}(\theta_{k}^{[2]})\rangle\right].\]

For \(\mathbb{E}\left[\langle\theta_{k}^{[1]}-\theta_{k}^{[2]},\bar{g}(\theta_{k}^{[ 1]})-\bar{g}(\theta_{k}^{[2]})\rangle\right]\), by Assumption 3, we have

\[\mathbb{E}\left[\langle\theta_{k}^{[1]}-\theta_{k}^{[2]},\bar{g}(\theta_{k}^{ [1]})-\bar{g}(\theta_{k}^{[2]})\rangle\right]\leq-\mu\mathbb{E}\left[\|\theta _{k}^{[1]}-\theta_{k}^{[2]}\|^{2}\right].\]

Let \(\mathcal{F}_{k}:=\sigma\left((\theta_{k}^{[1]},\theta_{k}^{[2]},x_{t})\mid t \leq k\right)\). For \(\mathbb{E}\left[\langle\theta_{k}^{[1]}-\theta_{k}^{[2]},\varepsilon(\theta_ {k}^{[1]},x_{k})-\varepsilon(\theta_{k}^{[2]},x_{k})\rangle\right]\), we have

\[\mathbb{E}\left[\langle\theta_{k}^{[1]}-\theta_{k}^{[2]}, \varepsilon(\theta_{k}^{[1]},x_{k})-\varepsilon(\theta_{k}^{[2]},x_{k}) \rangle\right]\] \[= \mathbb{E}\left[\mathbb{E}\left[\langle\theta_{k}^{[1]}-\theta_{ k}^{[2]},\varepsilon(\theta_{k}^{[1]},x_{k})-\varepsilon(\theta_{k}^{[2]},x_{k}) \rangle\mid\mathcal{F}_{k-\tau}\right]\right]\] \[= \mathbb{E}\left[\mathbb{E}\left[\langle\theta_{k-\tau}^{[1]}- \theta_{k-\tau}^{[2]},\varepsilon(\theta_{k-\tau}^{[1]},x_{k})-\varepsilon( \theta_{k-\tau}^{[2]},x_{k})\rangle\mid\mathcal{F}_{k-\tau}\right]\right]\] ( \[T_{3}\] ) \[+\mathbb{E}\left[\langle\theta_{k}^{[1]}-\theta_{k}^{[2]}-\theta_ {k-\tau}^{[1]}-\theta_{k-\tau}^{[2]},\varepsilon(\theta_{k-\tau}^{[1]},x_{k}) -\varepsilon(\theta_{k-\tau}^{[2]},x_{k})\rangle\right]\] ( \[T_{4}\] ) \[+\mathbb{E}\Big{[}\langle\theta_{k}^{[1]}-\theta_{k}^{[2]}, \varepsilon(\theta_{k}^{[1]},x_{k})-\varepsilon(\theta_{k}^{[2]},x_{k})- \varepsilon(\theta_{k-\tau}^{[1]},x_{k})+\varepsilon(\theta_{k-\tau}^{[2]},x_{ k})\rangle\Big{]}.\]

We assume \(\alpha\tau\leq\frac{1}{8L}\). For \(T_{3}\), by definition of mixing time \(\tau\), we obtain

\[T_{3} =\mathbb{E}\left[\mathbb{E}\left[\langle\theta_{k-\tau}^{[1]}- \theta_{k-\tau}^{[2]},\varepsilon(\theta_{k-\tau}^{[1]},x_{k})-\varepsilon( \theta_{k-\tau}^{[2]},x_{k})\rangle\mid\theta_{k-\tau}^{[1]},\theta_{k-\tau}^ {[2]},x_{k-\tau}\right]\right]\] \[\leq 2\alpha L\mathbb{E}\left[\|\theta_{k-\tau}^{[1]}-\theta_{k- \tau}^{[2]}\|^{2}\right]\] \[\leq 4\alpha L\mathbb{E}\left[\|\theta_{k}^{[1]}-\theta_{k}^{[2]} \|^{2}\right]+4\alpha L\mathbb{E}\left[\|\theta_{k-\tau}^{[1]}-\theta_{k-\tau}^ {[2]}-\theta_{k}^{[1]}+\theta_{k}^{[2]}\|^{2}\right]\] \[\leq(4\alpha L+256\alpha^{3}\tau^{2}L^{3})\mathbb{E}\left[\| \theta_{k}^{[1]}-\theta_{k}^{[2]}\|^{2}\right],\]

where the last inequality holds by Lemma E.6.

For \(T_{4}\), we obtain

\[T_{4} \leq\mathbb{E}\left[\|\theta_{k}^{[1]}-\theta_{k}^{[2]}-\theta_{k -\tau}^{[1]}+\theta_{k-\tau}^{[2]}\|\|\varepsilon(\theta_{k-\tau}^{[1]},x_{k})- \varepsilon(\theta_{k-\tau}^{[2]},x_{k})\|\right]\] \[\leq 16\alpha\tau L^{2}\mathbb{E}\left[\|\theta_{k}^{[1]}-\theta_{k}^ {[2]}\|\|\theta_{k-\tau}^{[1]}-\theta_{k-\tau}^{[2]}\|\right]\] \[\leq(16\alpha\tau L^{2}+128\alpha^{2}\tau^{2}L^{3})\mathbb{E} \left[\|\theta_{k}^{[1]}-\theta_{k}^{[2]}\|^{2}\right].\]

Below, we bound term \(\spadesuit\) by two different Taylor expansions. One the one hand, there exist \(\lambda_{1},\lambda_{2}\in[0,1]\) such that \(h_{k}=\lambda_{1}\theta_{k}^{[1]}+(1-\lambda_{1})\theta_{k}^{[2]}\), \(h_{k-\tau}=\lambda_{2}\theta_{k-\tau}^{[1]}+(1-\lambda_{2})\theta_{k-\tau}^{[2]}\) and

\[|\spadesuit|= |\langle\theta_{k}^{[1]}-\theta_{k}^{[2]},\varepsilon^{\prime}(h_ {k},x_{k})(\theta_{k}^{[1]}-\theta_{k}^{[2]})-\varepsilon^{\prime}(h_{k-\tau},x_{ k})(\theta_{k-\tau}^{[1]}-\theta_{k-\tau}^{[2]})\rangle|\] \[= |\langle\theta_{k}^{[1]}-\theta_{k}^{[2]}|^{2},\varepsilon^{\prime}(h _{k-\tau},x_{k})(\theta_{k}^{[1]}-\theta_{k}^{[2]}-\theta_{k-\tau}^{[1]}+ \theta_{k-\tau}^{[2]})\rangle\]\[\|\spadesuit\|\mathbb{1}(\|\theta_{k}^{[2]}\|\leq 4\delta_{\alpha\tau L^{2}})\]

[MISSING_PAGE_EMPTY:37]

\[\leq (1-\mu\alpha)\mathbb{E}[\|\theta_{k}^{[1]}-\theta_{k}^{[2]}\|^{2}],\] \[\leq (1+\alpha\mu(-2+(160+96\beta)c+256c+524c))\,\mathbb{E}[\|\theta_{k} ^{[1]}-\theta_{k}^{[2]}\|^{2}]\] \[= (1-\mu\alpha)\mathbb{E}[\|\theta_{k}^{[1]}-\theta_{k}^{[2]}\|^{2} ],\]

where we set \(c=\frac{1}{940+96\beta}.\)

Therefore, \(\forall k\geq\tau\) and \(\alpha\tau\leq\frac{\mu}{(940+96\beta)L^{2}},\) we have

\[\mathbb{E}[\|\theta_{k}^{[1]}-\theta_{k}^{[2]}\|^{2}] \leq 4(1-\mu\alpha)^{k-\tau}\mathbb{E}[\|\theta_{0}^{[1]}- \theta_{0}^{[2]}\|^{2}].\]

Then, still by the non-expansion of \(\Pi_{B(\beta)}\) with respect to \(\|\cdot\|,\) the rest of the proof simply follows the same proof for non-projected SA. As such, we have proven Theorem 4.1.

## Appendix F Proof of Corollary 4.4

In this section, we present the proof of Corollary 4.4.

Recall that by Theorem 4.3, we obtain for \(k\geq 2\tau\),

\[W_{2}^{2}\Big{(}\mathcal{L}(\theta_{k}),\nu_{\alpha}\Big{)}\leq(1-\alpha\mu)^ {k}\cdot s(\theta_{0},\theta^{*},\mu,L,R).\]By [63, Theorem 4.1], there exists a coupling between \(\theta_{k}\) and \(\theta_{\infty}\) such that

\[W_{2}^{2}(\mathcal{L}(\theta_{k}),\nu_{\alpha})=\mathbb{E}[\|\theta_{k}-\theta_{ \infty}\|^{2}].\]

Applying Jensen's inequality twice, we obtain that

\[\|\mathbb{E}[\theta_{k}-\theta_{\infty}]\|^{2}\leq(\mathbb{E}[\|\theta_{k}- \theta_{\infty}\|])^{2}\leq\mathbb{E}[\|\theta_{k}-\theta_{\infty}\|^{2}]\leq( 1-\alpha\mu)^{k}\cdot s(\theta_{0},\theta^{*},\mu,L,R).\]

We thus have for all \(k\geq 2\tau\),

\[\|\mathbb{E}[\theta_{k}]-\mathbb{E}[\theta_{\infty}]\|\leq\mathbb{E}[\| \theta_{k}-\theta_{\infty}\|]\leq(1-\alpha\mu)^{k/2}\cdot s^{\prime}(\theta_{ 0},\theta^{*},\mu,L,R).\]

For the second moment, we first note that

\[\|\mathbb{E}[\theta_{k}\theta_{k}^{\top}]-\mathbb{E}[\theta_{ \infty}\theta_{\infty}^{\top}]\|\] \[=\|\mathbb{E}[(\theta_{k}-\theta_{\infty})(\theta_{k}-\theta_{ \infty})^{\top}]+\mathbb{E}[\theta_{\infty}(\theta_{k}-\theta_{\infty})^{\top }]+\mathbb{E}[(\theta_{k}-\theta_{\infty})\theta_{\infty}^{\top}]\|\] \[\leq\|\mathbb{E}[(\theta_{k}-\theta_{\infty})(\theta_{k}-\theta_ {\infty})^{\top}]\|+\|\mathbb{E}[\theta_{\infty}(\theta_{k}-\theta_{\infty})^ {\top}]\|+\|\mathbb{E}[(\theta_{k}-\theta_{\infty})\theta_{\infty}^{\top}]\|\] \[\leq\mathbb{E}[\|\theta_{k}-\theta_{\infty}\|^{2}]\|+2\mathbb{E}[ \|\theta_{\infty}\|\|\theta_{k}-\theta_{\infty}\|]\|\] \[\leq\mathbb{E}[\|\theta_{k}-\theta_{\infty}\|^{2}]\|+2\sqrt{ \mathbb{E}[\|\theta_{\infty}\|^{2}\|\theta_{k}-\theta_{\infty}\|^{2}]\|},\] (F.1)

where we apply Cauchy-Schwarz to obtain the last inequality.

Meanwhile, we have

\[\mathbb{E}\left[\|\theta_{k}-\theta_{\infty}\|^{2}\right]\leq(1-\alpha\mu)^{ k}\cdot s(\theta_{0},\theta^{*},\mu,L,R)\quad\text{and}\quad\mathbb{E}\left[\| \theta_{\infty}\|^{2}\right]=\mathcal{O}(1).\]

Substituting the above bounds into the right-hand side of inequality (F.1) yields

\[\left\|\mathbb{E}\left[\theta_{k}\theta_{k}^{\top}\right]-\mathbb{E}\left[ \theta_{\infty}\theta_{\infty}^{\top}\right]\right\|\leq(1-\alpha\mu)^{k/2} \cdot s^{\prime\prime}(\theta_{0},\theta^{*},\mu,L,R).\]

## Appendix G Proof of Corollary 4.5

In this section, we prove the CLT result.

Proof.: Consider the following centered test function \(\bar{h}:\mathcal{X}\times\mathbb{R}^{d}\to\mathbb{R}^{d}\) defined as

\[\bar{h}(x,\theta)=\theta-\mathbb{E}[\theta_{\infty}].\]

To prove that the CLT for function \(\bar{h}\), we need to verify the Maxwell-Woodroofe condition [49], i.e.,

\[\sum_{n=1}^{\infty}n^{-3/2}\Big{\|}\sum_{t=0}^{n-1}Q^{t}h\Big{\|}_{L^{2}(\bar {\nu})}<\infty,\]

where \(Q\) denotes the transition kernel of the joint Markov chain. If we can show the following

\[\Big{\|}\sum_{t=0}^{n-1}Q^{t}h\Big{\|}_{L^{2}(\bar{\nu})}=\mathcal{O}(n^{r})\] (G.1)

with \(r\in[0,1/2)\), then the Maxwell-Woodroofe condition is verified, as

\[\sum_{n=1}^{\infty}n^{-3/2}\Big{\|}\sum_{t=0}^{n-1}Q^{t}h\Big{\|}_{L^{2}(\bar{ \nu})}=\sum_{n=1}^{\infty}n^{-3/2}\mathcal{O}(n^{r})<\infty.\]

We now proceed to prove the desired order in (G.1). For sufficiently large \(n\geq 2\tau_{\alpha}\), we observe

\[\Big{\|}\sum_{t=0}^{n-1}Q^{t}h\Big{\|}_{L^{2}(\bar{\nu})}=\mathbb{ E}_{\bar{\nu}}\Big{\|}\sum_{t=0}^{n-1}Q^{t}h\Big{\|}_{2} \leq\sum_{t=0}^{n-1}\mathbb{E}_{\bar{\nu}}\|Q^{t}h\|_{2}\] \[=\underbrace{\sum_{t=0}^{2\tau_{\alpha}-1}\mathbb{E}_{\bar{\nu}}\| Q^{t}h\|_{2}}_{\mathcal{T}_{1}}+\underbrace{\sum_{t=2\tau_{\alpha}}^{n-1} \mathbb{E}_{\bar{\nu}}\|Q^{t}h\|_{2}}_{\mathcal{T}_{2}}.\]We now show that both terms \(T_{1}\) and \(T_{2}\) are of order \(\mathcal{O}(1)\) with respect to the parameter \(n\).

For \(T_{1}\), since \(Q\) is a transition kernel, so its \(\|Q\|_{L^{2}(\mu)}\) operator norm equals to \(1\). Hence, \(T_{1}\) can be upper bounded as

\[T_{1}\leq\tau_{\alpha}\mathbb{E}_{\bar{\mu}}[\|h(\theta,x)\|_{2}^{2}]=\tau_{ \alpha}\operatorname{Tr}(\operatorname{Var}(\theta_{\infty}))<C_{1},\]

where the last inequality follows from \(\operatorname{Tr}(\operatorname{Var}(\theta_{\infty}))\leq\alpha\tau\) established in (H.4) and \(\alpha\tau_{\alpha}^{2}\to 0\) by Definition 2.1.

Before proceeding to analyze the summation in \(T_{2}\), we first recall (E.2), that for \(t\geq 2\tau_{\alpha}\),

\[\tilde{W}_{2}(\mathcal{L}(x_{t},\theta_{t}),\bar{\nu})=\mathcal{O}((1-\alpha \mu)^{t/2}),\]

which holds for any \((x,\theta)\in\mathcal{X}\times\mathbb{R}^{d}\). Hence, by the property of Wasserstein distance [63], there always exists a coupling that attains the optimality, i.e.,

\[\mathbb{E}_{\Gamma((x_{t},\theta_{t}),\bar{\nu})}\Big{[}\|\theta_{t}-\theta^{ \prime}\|_{2}^{2}+\delta_{0}(x_{t}\neq x^{\prime})\Big{]}=\mathcal{O}((1- \alpha\mu)^{t}).\]

Making use of this relationship, we can therefore bound \(T_{2}\),

\[T_{2}=\sum_{t=\tau_{\alpha}}^{n-1}\mathbb{E}_{\bar{\nu}}\|Q^{t}h\|_{2}\leq\sum _{t=\tau_{\alpha}}^{\infty}\mathbb{E}_{\bar{\nu}}\|Q^{t}h\|_{2}=\mathcal{O} \Big{(}\frac{1}{1-(1-\alpha\mu)^{1/2}}\Big{)}=\mathcal{O}(1),\]

where the last \(\mathcal{O}(\cdot)\) is asymptotic in \(n\).

Combining the analysis of \(T_{1}\) and \(T_{2}\), we have shown the desired order in (G.1). Therefore, the Maxwell-Woodroofe condition has been verified and we establish the CLT for averaged nonlinear iterates with constant stepsize and Markovian data. 

## Appendix H Proofs under Minorization Condition

When assuming the perturbed continuous noise condition in Assumption 5, one takes the alternative route to prove weak convergence. This is achieved by establishing the satisfaction of both a minorization condition and a drift condition. In this section, we prove the weak convergence result in Theorem 4.3 by following this alternative approach. The subsequent corollaries of weak convergence, namely the non-asymptotic convergence rate in Corollary 4.4 and the Central Limit Theorem (CLT) in Corollary 4.5, also hold, and we will provide the proofs for these results as well.

### Proof of Theorem 4.3

In this section, we prove the weak convergence under Assumption 5(a). The proof consists of two major steps. Firstly, built upon the MSE convergence established in Proposition 4.2, we derive a multi-state drift condition. Subsequently, we show that under the minorization condition, the Markov chain is \((x_{k},\theta_{k})_{k\geq 0}\) is \(\varphi\)-irreducible. Then, follow [50, Theorem 19.1.3], we can conclude that the Markov chain \((x_{k},\theta_{k})_{k\geq 0}\) is geometrically ergodic.

For completeness, we include the Theorem 19.1.3 from [50] below.

**Theorem H.1**.: _Suppose that \(\Phi\) is a \(\varphi\)-irreducible chain on \(\mathcal{X}\), and let \(n(x)\) be a measurable function from \(\mathcal{X}\to\mathbb{Z}_{+}\). The chain is geometrically ergodic if it is aperiodic and there exists some petite set \(C\), a nonnegative function \(V\geq 1\) and bounded on \(C\), and positive constants \(\lambda<1\) and \(b\) satisfying_

\[\int P^{n(x)}(x,\mathsf{d}y)V(y)\leq\lambda^{n(x)}[V(x)+b1_{C}(x)].\] (H.1)

We note that the function \(n(x)\) can be interpreted as the number of steps we must wait, starting from any \(x\), for the drift to become negative.

Step 1: Deriving the Drift ConditionGiven the iteration step

\[\theta_{k+1}=\theta_{k}+\alpha(g(\theta_{k},x_{k})+\xi_{\ell+1}(\theta_{k})),\]

we have already shown the following convergence rate on the MSE in Proposition 4.2, that

\[\mathbb{E}[\|\theta_{k}-\theta^{*}\|^{2}]\leq c_{2,1}(1-\alpha\mu)^{k}\|\theta _{0}-\theta^{*}\|^{2}+c_{2,2}\alpha\tau_{\alpha}\frac{L^{2}}{\mu}.\]Inspired by the MSE convergence bound, we define the Lyapunov function \(V:\mathcal{X}\times\mathbb{R}^{d}\to[1,\infty]\)

\[V(x,\theta)=\|\theta-\theta^{*}\|^{2}+1.\] (H.2)

Therefore, the major goal in this step is to obtain the desired drift condition as shown in (H.1).

Therefore, from the above MSE convergence rate, we first obtain that

\[Q^{k}V(x_{0},\theta_{0})\leq c_{2,1}(1-\alpha\mu)^{k}V(x_{0},\theta_{0})+c_{2,2 }\frac{L^{2}}{\mu}\alpha\tau+1.\] (H.3)

Consider \(k=\min\{t\geq 0:c_{2,1}(1-\alpha\mu)^{t}<1\}\). Then, set \(\eta=8(1-\alpha\mu)^{k}\), \(\beta=\frac{1-\eta}{2}\), and \(m=c_{2,2}\frac{L^{2}}{\mu}\alpha\tau+1\), and we consider the following bounded sublevel set,

\[C_{\Theta}=\{\theta:(\|\theta-\theta^{*}\|^{2}+1)\leq m/\beta\}.\]

From (H.3), we derive that

\[Q^{k}V(x_{0},\theta_{0})-V(x_{0},\theta_{0})\leq-\beta V(x_{0},\theta_{0})+m \mathbbm{1}_{\bar{C}}(x_{0},\theta_{0}),\]

where \(\bar{C}=\mathcal{X}\times C_{\Theta}\). Rewriting the above Lyapunov drift condition, with \(b=m/(1-\beta)\), we obtain

\[Q^{k}V(x_{0},\theta_{0})\leq(1-\beta)\Big{(}V(x_{0},\theta_{0})+b\mathbbm{1}_ {\bar{C}}(x_{0},\theta_{0})\Big{)}.\]

Setting \(\lambda\) such that \(\lambda^{k}=1-\beta\), we have

\[Q^{k}V(x_{0},\theta_{0})\leq\lambda^{k}\Big{(}V(x_{0},\theta_{0})+b\mathbbm{1 }_{\bar{C}}(x_{0},\theta_{0})\Big{)},\]

which gives the desired multi-step drift condition.

Step 2: Proving the Minorization ConditionNow that we have established the desired multi-step drift condition, it remains for us to show that \(\bar{C}\) is accessible, small, and aperiodic.

Under this setup of \(\xi_{t}(\theta)\) in Assumption 5(a), it is straightforward to verify the accessibility of \(\bar{C}\). For any \((x,\theta)\in\mathcal{X}\times\mathbb{R}^{d}\), we have

\[Q((x,\theta),\bar{C}) =\mathbb{P}((x^{\prime},\theta^{\prime})\in\mathcal{X}\times C_{ \Theta}|(x,\theta))\] \[=\mathbb{P}(\theta^{\prime}\in C_{\Theta}|(x,\theta))\geq\int_{ \theta^{\prime}\in C_{\Theta}}\frac{1}{\alpha^{d}}p_{\theta}\Big{(}\frac{ \theta^{\prime}-\theta}{\alpha}-g(x,\theta)\Big{)}\text{d}\theta^{\prime}>0.\]

As such, we have shown that \(\bar{C}\) is accessible.

Assuming \(\bar{C}\) is small, we can directly conclude aperiodicity following the definition of period of an accessible small set, \(d(C)=\text{g.c.d.}\Big{\{}n\in\mathbb{N}^{*}:\inf_{x\in C}P^{n}(x,C)>0\Big{\}}\).

Therefore, what remains to show is that \(\bar{C}\) is small. For \((x,\theta)\in\bar{C}\) and \(\bar{A}\in\mathcal{B}(\mathcal{X})\times\mathcal{B}(\mathbb{R}^{d})\), we define the following projection sets,

\[A_{x}=\{\theta\in\mathbb{R}^{d}|(x,\theta)\in\bar{A}\}\quad\text{and}\quad A ^{\theta}=\{x\in\mathcal{X}|(x,\theta)\in\bar{A}\}.\]

Therefore,

\[Q^{m}((x,\theta),\bar{A})=\int_{\begin{subarray}{c}\{(x^{(k)}, \theta^{(k)})\}_{k=1}^{m-1}\\ \in(\mathcal{X}\times\mathbb{R}^{d})^{m-1}\end{subarray}}\mathbb{P}((x_{m}, \theta_{m})\in\bar{A}|(x_{m-1},\theta_{m-1})=(x^{(m-1)},\theta^{(m-1)}))\] \[\qquad\qquad\qquad\qquad\qquad\cdots\mathbb{P}((x_{1},\theta_{1}) =\text{d}(x^{(1)},\theta^{(1)})|(x_{0},\theta_{0})=(x,\theta))\] \[\geq\int_{\begin{subarray}{c}\{(x^{(k)},\theta^{(k)})\}_{k=1}^ {m-1}\\ \in(\mathcal{X}\times C_{\Theta})^{m-1}\end{subarray}}\mathbb{P}((x_{m},\theta_ {m})\in\bar{A}|(x_{m-1},\theta_{m-1})=(x^{(m-1)},\theta^{(m-1)}))\] \[\qquad\qquad\qquad\qquad\cdots\mathbb{P}((x_{1},\theta_{1})= \text{d}(x^{(1)},\theta^{(1)})|(x_{0},\theta_{0})=(x,\theta))\] \[=\int_{\begin{subarray}{c}\{(x^{(k)},\theta^{(k)})\}_{k=1}^{m-1} \\ \in(\mathcal{X}\times C_{\Theta})^{m-1}\end{subarray}}\Big{(}\int_{x^{\prime} \in\mathcal{X}}\mathbb{P}(x_{m}=\text{d}x^{\prime}|x_{m-1}=x^{(m-1)})\mathbb{ P}(\theta_{m}\in A_{x^{\prime}}|(x_{m-1},\theta_{m-1})=(x^{(m-1)},\theta^{(m-1)}))\Big{)}\]\[\mathbb{P}(x_{m-1}=\mathsf{d}x^{(m-1)}|x_{m-2}=x^{(m-2)})\mathbb{P}( \theta_{m-1}=\mathsf{d}\theta^{(m-1)}|(x_{m-2},\theta_{m-2})=(x^{(m-2)},\theta^{ (m-2)}))\Big{)}\] \[\cdots\] \[\mathbb{P}(x_{1}=\mathsf{d}x^{(1)}|x_{0}=x)\mathbb{P}(\theta_{1} =\mathsf{d}\theta^{(1)}|(x_{0},\theta_{0})=(x,\theta))\] \[=\int_{x^{\prime}\in\mathcal{X}}\int_{\begin{subarray}{c}\{x^{(k )}\}_{k=1}^{m-1}\\ \in\mathcal{X}^{m-1}\end{subarray}}\mathbb{P}(x_{m}=\mathsf{d}x^{\prime}|x_{m -1}=x^{(m-1)})\mathbb{P}(x_{m-1}=\mathsf{d}x^{(m-1)}|x_{m-2}=x^{(m-2)})\cdots \mathbb{P}(x_{1}=\mathsf{d}x^{(1)}|x_{0}=x)\] \[\Bigg{(}\int_{\begin{subarray}{c}\{\theta^{(k)}\}_{k=1}^{m-1}\\ \in C_{\Theta}^{m-1}\end{subarray}}\mathbb{P}(\theta_{m}\in A_{x^{\prime}}|(x _{m-1},\theta_{m-1})=(x^{(m-1)},\theta^{(m-1)}))\] \[\qquad\qquad\qquad\qquad\mathbb{P}(\theta_{m-1}=\mathsf{d}\theta ^{(m-1)}|(x_{m-2},\theta_{m-2})=(x^{(m-2)},\theta^{(m-2)}))\Big{)}\] \[\cdots\mathbb{P}(\theta_{1}=\mathsf{d}\theta^{(1)}|(x_{0},\theta_ {0})=(x,\theta))\Bigg{)}.\]

Next, for \((x,\theta)\in\bar{C}\), we observe that

\[\mathbb{P}(\theta_{k}=\mathsf{d}\theta^{\prime}|(x_{k-1},\theta _{k-1})=(x,\theta)) =\mathbb{P}\Big{(}\xi_{k}(\theta)=\mathsf{d}\big{(}\frac{\theta^ {\prime}-\theta}{\alpha}-g(x,\theta)\big{)}|(x_{k},\theta_{k})=(x,\theta) \Big{)}\] \[\geq\frac{1}{\alpha^{d}}p_{\theta}\Big{(}\frac{\theta^{\prime}- \theta}{\alpha}-g(x,\theta)\Big{)}\mathsf{d}\theta^{\prime}\] \[\geq\frac{1}{\alpha^{d}}\inf_{\hat{\theta}\in C_{\Theta}}p_{\hat{ \theta}}\Big{(}\frac{\theta^{\prime}-\theta}{\alpha}-g(x,\theta)\Big{)} \mathsf{d}\theta^{\prime}.\]

We next recall the linear growth assumption in Assumption 2 that \(\|g(x,\theta)\|\leq L(\|\theta\|+1).\) Hence, given \(\theta\in C_{\Theta}\), \(\forall x\in\mathcal{X}\), we have

\[\|\theta+\alpha g(x,\theta)\|\leq\|\theta\|+\alpha\|g(x,\theta)\| \leq(1+\alpha L)(\|\theta\|+1)\] \[\leq(1+\alpha L)(\sqrt{M/\beta}+\|\theta^{*}\|+1)\leq B,\]

for some bounded value \(B\).

We now define the measure \(\varsigma^{\dagger}\) on \(\mathbb{R}^{d}\) as \(\varsigma^{\dagger}(A)=\frac{1}{\alpha^{d}}\inf_{\|z\|\leq B}\int_{\theta^{ \prime}\in A\cap C_{\Theta}}\inf_{\hat{\theta}\in C_{\Theta}}p_{\hat{\theta}} \Big{(}\frac{\theta^{\prime}-z}{\alpha}\Big{)}\mathsf{d}\theta^{\prime}\). Hence, it is easy to see that \(\varsigma^{\dagger}(C_{\Theta}^{c})=0\). Moreover, we note the following property of measure \(\varsigma^{\dagger}\).

**Claim 1**.: _For \(A\subseteq C_{\Theta}\) and \(\lambda(A)>0\), \(\varsigma^{\dagger}(A)>0\), where \(\lambda\) denotes the Lebesgue measure._

We delay the proof to the end. Taking the claim as true, we derive that

\[\int_{\begin{subarray}{c}\{\theta^{(k)}\}_{k=1}^{m-1}\\ \in C_{\Theta}^{m-1}\end{subarray}}\mathbb{P}(\theta_{m}\in A_{x^{\prime}}|(x _{m-1},\theta_{m-1})=(x^{(m-1)},\theta^{(m-1)}))\] \[\qquad\mathbb{P}(\theta_{m-1}=\mathsf{d}\theta^{(m-1)}|(x_{m-2}, \theta_{m-2})=(x^{(m-2)},\theta^{(m-2)}))\cdots\mathbb{P}(\theta_{1}=\mathsf{d }\theta^{(1)}|(x_{0},\theta_{0})=(x,\theta))\] \[=\int_{\theta^{(m)}\in A_{x^{\prime}}}\int_{\theta^{(m-1)}\in C _{\Theta}}\mathbb{P}(\theta_{m}\mathsf{d}\theta^{(m)}|(x_{m-1},\theta_{m-1})= (x^{(m-1)},\theta^{(m-1)}))\] \[\qquad\qquad\qquad\int_{\theta^{(m-2)}\in C_{\Theta}}\mathbb{P}( \theta_{m-1}=\mathsf{d}\theta^{(m-1)}|(x_{m-2},\theta_{m-2})=(x^{(m-2)},\theta^{ (m-2)}))\Big{)}\] \[\cdots\] \[\geq\int_{\theta^{(m)}\in A_{x^{\prime}}}\int_{\theta^{(m-1)} \in C_{\Theta}}\frac{1}{\alpha^{d}}\inf_{\hat{\theta}\in C_{\Theta}}p_{\hat{ \theta}}\Big{(}\frac{\theta^{(m)}-\theta^{(m-1)}}{\alpha}-g(x^{(m-1)},\theta^{ (m-1)})\Big{)}\mathsf{d}\theta^{(m)}\] \[\qquad\qquad\qquad\int_{\theta^{(m-2)}\in C_{\Theta}}\mathbb{P}( \theta_{m-1}=\mathsf{d}\theta^{(m-1)}|(x_{m-2},\theta_{m-2})=(x^{(m-2)},\theta^{ (m-2)}))\Big{)}\]\[\begin{split}&\int_{\theta^{(1)}\in C_{\Theta}}\mathbb{P}(\theta_{2}= \mathsf{d}\theta^{(2)}|(x_{1},\theta_{1})=(x^{(1)},\theta^{(1)}))\mathbb{P}( \theta_{1}=\mathsf{d}\theta^{(1)}|(x_{0},\theta_{0})=(x,\theta))\\ \geq\inf_{(\tilde{x},\tilde{\theta})\in C}\int_{\theta^{(m)} \in A_{x^{\prime}}}\int_{\theta^{(m-1)}\in C_{\Theta}}\frac{1}{\alpha^{d}}\inf _{\theta\in C_{\Theta}}p_{\tilde{\theta}}\Big{(}\frac{\theta^{(m)}-\tilde{ \theta}}{\alpha}-g(\tilde{x},\tilde{\theta})\Big{)}\mathsf{d}\theta^{(m)}\\ \int_{\theta^{(m-2)}\in C_{\Theta}}\mathbb{P}(\theta_{m-1}= \mathsf{d}\theta^{(m-1)}|(x_{m-2},\theta_{m-2})=(x^{(m-2)},\theta^{(m-2)})) \Big{)}\\ \cdots\\ \int_{\theta^{(1)}\in C_{\Theta}}\mathbb{P}(\theta_{2}=\mathsf{d }\theta^{(2)}|(x_{1},\theta_{1})=(x^{(1)},\theta^{(1)}))\mathbb{P}(\theta_{1 }=\mathsf{d}\theta^{(1)}|(x_{0},\theta_{0})=(x,\theta))\\ \geq\inf_{\|z\|\leq B}\int_{\theta^{(m)}\in A_{x^{\prime}}}\int_ {\theta^{(m-1)}\in C_{\Theta}}\frac{1}{\alpha^{d}}\inf_{\tilde{\theta}\in C_{ \Theta}}p_{\tilde{\theta}}\Big{(}\frac{\theta^{(m)}-z}{\alpha}\Big{)} \mathsf{d}\theta^{(m)}\\ \int_{\theta^{(m-2)}\in C_{\Theta}}\mathbb{P}(\theta_{m-1}= \mathsf{d}\theta^{(m-1)}|(x_{m-2},\theta_{m-2})=(x^{(m-2)},\theta^{(m-2)})) \Big{)}\cdots\\ \int_{\theta^{(1)}\in C_{\Theta}}\mathbb{P}(\theta_{2}=\mathsf{d }\theta^{(2)}|(x_{1},\theta_{1})=(x^{(1)},\theta^{(1)}))\mathbb{P}(\theta_{1 }=\mathsf{d}\theta^{(1)}|(x_{0},\theta_{0})=(x,\theta))\\ \geq\varsigma^{\dagger}(A_{x^{\prime}})\int_{\theta^{(m-1)}\in C _{\Theta}}\int_{\theta^{(m-2)}\in C_{\Theta}}\mathbb{P}(\theta_{m-1}=\mathsf{ d}\theta^{(m-1)}|(x_{m-2},\theta_{m-2})=(x^{(m-2)},\theta^{(m-2)}))\Big{)}\cdots\\ \int_{\theta^{(1)}\in C_{\Theta}}\mathbb{P}(\theta_{2}=\mathsf{d }\theta^{(2)}|(x_{1},\theta_{1})=(x^{(1)},\theta^{(1)}))\mathbb{P}(\theta_{1 }=\mathsf{d}\theta^{(1)}|(x_{0},\theta_{0})=(x,\theta))\\ \geq\varsigma^{\dagger}(A_{x^{\prime\prime}})\varsigma^{\dagger} (C_{\Theta})^{m-1}.\end{split}\]

Therefore, by combining all the analyses, we obtain

\[\begin{split}& Q^{m}((x,\theta),\bar{A})\\ \geq&\int_{x^{\prime}\in\mathcal{X}}\int_{\begin{subarray} {c}\{x^{(k)}\}_{k=1}^{m-1}\\ \in\mathcal{X}^{m-1}\end{subarray}}\mathbb{P}(x_{m}=\mathsf{d}x^{\prime}|x_{m- 1}=x^{(m-1)})\mathbb{P}(x_{m-1}=\mathsf{d}x^{(m-1)}|x_{m-2}=x^{(m-2)})\\ &\cdots\mathbb{P}(x_{1}=\mathsf{d}x^{(1)}|x_{0}=x)\\ \end{split}\]

\[\begin{split}&\left(\int_{\begin{subarray}{c}\{\theta^{(k)}\}_{k=1}^{m-1} \\ \in C_{\Theta}^{m-1}\end{subarray}}\mathbb{P}(\theta_{m}\in A_{x^{\prime}}|(x_{m- 1},\theta_{m-1})=(x^{(m-1)},\theta^{(m-1)}))\right.\\ &\left.\mathbb{P}(\theta_{m-1}=\mathsf{d}\theta^{(m-1)}|(x_{m-2}, \theta_{m-2})=(x^{(m-2)},\theta^{(m-2)}))\right)\\ &\cdots\mathbb{P}(\theta_{1}=\mathsf{d}\theta^{(1)}|(x_{0}, \theta_{0})=(x,\theta))\right)\\ \geq\varsigma^{\dagger}(C_{\Theta})^{m-1}\int_{x^{\prime}\in \mathcal{X}}\varsigma^{\dagger}(A_{x^{\prime}})\\ =&\zeta\cdot(\phi\times\varsigma^{\dagger})(\bar{A}), \end{split}\]

where \(\zeta=\varsigma^{\dagger}(C_{\Theta})^{m-1}\) and \(\phi\times\varsigma^{\dagger}\) being the unique induced product measure on \(\mathcal{X}\times\mathbb{R}^{d}\).

As such, we have proven that \(\bar{C}\) is \((m,\phi\times\varsigma^{\dagger})\)-small, and hence \((\delta_{m},\phi\times\varsigma^{\dagger})\)-petite, and subsequently shown that \((x_{k},\theta_{k})_{k\geq 0}\) is geometrically ergodic.

By [50, Theorem 16.0.1 (iv)], we can further conclude that the geometrically ergodic \((\theta_{t},x_{t})_{t\geq 0}\) is also \(V\)-uniformly ergodic with the same \(V\) as defined in (H.2). Therefore, we have the following convergence rate in the \(V\)-norm \(\big{\|}\mathcal{L}(x_{k},\theta_{k}),-\bar{\nu}_{\alpha}\big{\|}_{V}\leq\kappa \rho^{k}\), where \(\kappa\) and \(\rho\) implicitly depend on the stepsize \(\alpha\).

Lastly, we provide the proof of Claim 1.

Proof.: We first recall that for any set \(B\subset\mathbb{R}^{d}\) such that \(\lambda(B)>0\), where \(\lambda\) refers to the Lebesgue measure, then for \(\theta\in C_{\Theta}\), we know that \(\int_{t\in B}p_{\theta}(t)\text{d}t\geq\int_{t\in B}\inf_{\theta\in C_{\Theta }}p_{\theta}(t)\text{d}t>0\). Moreover, for a given (translation) \(z\in\mathbb{R}^{d}\) and \(\theta\in C_{\Theta}\), we have

\[\int_{t\in B}p_{\theta}(t-z)\text{d}t=\int_{t^{\prime}\in B-z}p_{\theta}(t^{ \prime})\text{d}t^{\prime}\geq\int_{t^{\prime}\in B-z}\inf_{\theta\in C}p_{ \theta}(t^{\prime})\text{d}t^{\prime}>0.\]

Following the properties stated above, we define \(h(z)=\int_{t\in B}\inf_{\theta\in C}p_{\theta}(t-z)\text{d}t\), and it is easy to verify that \(h(z)\) is a continuous function. Hence, for a bounded set \(D\), we have \(\inf_{z\in D}h(z)>0\). Subsequently, we define the measure \(\varsigma^{\dagger}\) induced by \(h\) and we verify that

\[\varsigma^{\dagger}(A)=\inf_{\|z\|\leq B}\int_{t\in A\cap C_{\Theta}}\inf_{ \theta\in C}p_{\theta}(t-z)\text{d}t>0.\]

### Proof of Corollary 4.4

As shown in the previous section, the joint process \((x_{k},\theta_{k})_{k\geq 0}\) is \(V\)-uniformly ergodic and hence also exhibits a geometric non-asymptotic convergence rate under the \(V\)-weighted norm. Subsequently, this corresponds to a version of Corollary 4.4 resulting in a different set of convergence rate coefficients. Thus, we restate Corollary 4.4 in the context of the minorization setting and provide the proof below.

**Corollary H.2** (Non-Asymptotic Convergence Rate).: _For any initialization of \(\theta_{0}\in\mathbb{R}^{d}\), under the setting of Theorem 4.3, we have_

\[\big{\|}\mathbb{E}[\theta_{k}]-\mathbb{E}[\theta_{\infty}^{(\alpha)}]\big{\|} \leq\kappa\cdot\rho^{k}\cdot s^{\prime}(\theta_{0},L,\mu),\quad\text{and} \quad\big{\|}\mathbb{E}[\theta_{k}\theta_{k}^{\top}]-\mathbb{E}[\theta_{ \infty}^{(\alpha)}(\theta_{\infty}^{(\alpha)})^{\top}]\big{\|}\leq\kappa\cdot \rho^{k}\cdot s^{\prime\prime}(\theta_{0},L,\mu),\]

_where \(\kappa\) and \(\rho\) are defined in (4.1) and implicitly depend on \(\alpha\)._

Proof.: For \(V\)-uniformly ergodic Markov chain \((x_{k},\theta_{k})_{k\geq 0}\), when functions \(f:\mathcal{X}\times\mathbb{R}^{d}\to\mathbb{R}^{d}\) is dominated by the Lyapunov function, i.e., \(\|f\|\leq V\), it enjoys the following convergence property,

\[\|Q^{n}f(x_{0},\theta_{0})-\pi f\|\leq\kappa\rho^{n}V(\theta_{0}).\]

Consider test function \(f(\theta,x)=\theta-\theta^{*}\). It is easy to see that \(\|f\|\leq V\). Hence, we obtain

\[\|\mathbb{E}[\theta_{n}]-\mathbb{E}[\theta_{\infty}]\|=\|Q^{n}f(x,\theta)-\pi f \|\leq\kappa\rho^{n}V(\theta_{0}).\]

Next, consider \(f^{\prime}(x)=(\theta-\theta^{*})(\theta-\theta^{*})^{\top}\). Clearly, \(\|f^{\prime}\|\leq V\). Therefore,

\[\|\mathbb{E}[(\theta_{t}-\theta^{*})(\theta_{t}-\theta^{*})^{\top}]-\mathbb{E }[(\theta_{\infty}-\theta^{*})(\theta_{\infty}-\theta^{*})^{\top}]\|\leq\kappa \rho^{n}V(\theta_{0}).\]

For the LHS, we have

\[\|\mathbb{E}[(\theta_{t}-\theta^{*})(\theta_{t}-\theta^{*})^{\top }]-\mathbb{E}[(\theta_{\infty}-\theta^{*})(\theta_{\infty}-\theta^{*})^{\top}]\|\] \[=\|\mathbb{E}[\theta_{t}\theta_{t}^{\top}]-\mathbb{E}[\theta_{ \infty}\theta_{\infty}^{\top}]-\mathbb{E}[\theta_{t}-\theta_{\infty}](\theta ^{*})^{\top}-\theta^{*}\mathbb{E}[(\theta_{t}-\theta_{\infty})^{\top}]\|\] \[\geq\|\mathbb{E}[\theta_{t}\theta_{t}^{\top}]-\mathbb{E}[\theta_{ \infty}\theta_{\infty}^{\top}]\|-2\|\mathbb{E}[\theta_{t}-\theta_{\infty}]\| \|\theta^{*}\|.\]

Subsequently,

\[\|\mathbb{E}[\theta_{t}\theta_{t}^{\top}]-\mathbb{E}[\theta_{\infty}\theta_{ \infty}^{\top}]\|\leq(2\|\theta^{*}\|+1)\kappa\rho^{n}V(\theta_{0}).\]

The above results imply the convergence of the first two moments. Moreover, we conclude that

\[\operatorname{Var}(\theta_{\infty})=\operatorname{Var}(\theta_{\infty}-\theta ^{*})\leq\mathbb{E}[\|\theta_{\infty}-\theta^{*}\|^{2}]=\lim_{t\to\infty} \mathbb{E}[\|\theta_{t}-\theta^{*}\|^{2}]\lesssim\alpha\tau.\]

Additionally,

\[\mathbb{E}[\|\theta_{\infty}-\theta^{*}\|]^{2}\leq\mathbb{E}[\|\theta_{\infty}- \theta^{*}\|^{2}]\lesssim\alpha\tau.\] (H.4)

### Proof of Corollary 4.5

After establishing the \(V\)-uniform ergodicity of the joint process \((x_{k},\theta_{k})_{k\geq 0}\), the central limit theorem for averaged iterates follows as a straightforward consequence.

Proof.: For any test function \(h:\mathcal{X}\times\mathbb{R}^{d}\to\mathbb{R}^{d}\) that satisfies \(\|h\|^{2}\leq V\), by Theorem 17.0.1 in [50], it has the following CLT results,

\[\frac{1}{\sqrt{k}}\Big{[}\sum_{t=0}^{k-1}\big{(}h_{t}-\mathbb{E}[h_{\infty}] \big{)}\Big{]}\Rightarrow\mathcal{N}(0,\Sigma^{(a)}).\]

Therefore, consider \(h(x,\theta)=\theta-\theta^{*}\), it is easy to see that \(\|h\|^{2}\leq V\), and hence we naturally obtain the desired CLT result, \(\frac{1}{\sqrt{k}}\Big{[}\sum_{t=0}^{k-1}\big{(}\theta_{t}-\mathbb{E}[\theta_{ \infty}]\big{)}\Big{]}\Rightarrow\mathcal{N}(0,\Sigma^{(a)})\) as \(k\to\infty\). 

## Appendix I Proof of Theorem 4.6

We now provide the proof of Theorem 4.6 on characterizing the asymptotic bias of nonlinear SA.

### BAR and Preliminaries

The proof utilizes the basic adjoint relationship (BAR) approach to study the stationary distribution

\[\mathbb{E}_{\tilde{\rho}}[(P-I)h(\theta,x)]=0,\]

via carefully designed test functions \(h\). We refer readers to [33] for the derivation of the following properties of Markovian SA at stationarity,

\[\mathbb{E}[\mathbbm{1}\{\theta_{\infty}\in S\}\mid x_{\infty}](x) =\mathbb{E}[\mathbbm{1}\{\theta_{\infty+1}\in S\}\mid x_{\infty+1}](x),\] (I.1) \[\mathbb{E}[\theta_{\infty}\mid x_{\infty}](x) =\mathbb{E}[\theta_{\infty+1}\mid x_{\infty+1}](x),\] (I.2) \[\mathbb{E}[(\theta_{\infty})^{\otimes 2}\mid x_{\infty}](x) =\mathbb{E}[(\theta_{\infty+1})^{\otimes 2}\mid x_{\infty+1}](x).\] (I.3)

Following the Borel state space assumption in 1, \(\mathbb{E}[\mathbbm{1}\{\theta_{\infty}\in\cdot\}|x_{\infty}=x]\) induce a regular conditional probability measure, which we denote as \(\tilde{\nu}(\cdot,x_{\infty}=x)\), and hence (I.1) can be reformulated as

\[\tilde{\nu}(\theta_{\infty}\in S,x_{\infty}=x)=\tilde{\nu}(\theta_{\infty+1} \in S,x_{\infty+1}=x).\]

Before proceeding to the proof, we introduce the following shorthands and notations. For \(x\in\mathcal{X}\),

\[z_{i}(x) :=\mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes i}|x_{\infty} =x],\] \[\delta_{i}(x) :=z_{i}(x)-\pi z_{i},\]

Following the differentiability assumption of \(g\) in Assumption 3, and we can apply Taylor expansion to \(g\) and we note the following notation on residuals.

\[g(\theta,x) =g(\theta^{*},x)+g^{\prime}(\theta^{*},x)(\theta-\theta^{*})+ \frac{1}{2}g^{\prime\prime}(\theta^{*},x)(\theta-\theta^{*})^{\otimes 2}+R_{3}( \theta,x)\] (I.4) \[=g(\theta^{*},x)+g^{\prime}(\theta^{*},x)(\theta-\theta^{*})+R_{ 2}(\theta,x).\] (I.5)

By Assumption 3 and results from Proposition 4.2 and 4.2, we note that the residual \(R_{n}(\theta,x)\) satisfies

\[\sup_{x\in\mathcal{X},\theta\in\mathbb{R}^{d}}\Big{\{}\|R_{n}(\theta,x)\|/\| \theta-\theta^{*}\|^{n}\Big{\}}<+\infty.\]

Hence, we have

\[\|R_{n}(\theta,x_{\infty})\|_{L^{2}(\pi)}\lesssim\mathbb{E}[\|\theta_{\infty} -\theta^{*}\|^{n}]=\mathcal{O}((\alpha\tau)^{n/2}),\quad n=2,3,4.\]

Lastly, we denote

\[\bar{g}(\theta) :=\mathbb{E}_{x\sim\pi}[g(\theta,x)],\quad\bar{g}_{2}(\theta):= \mathbb{E}_{x\sim\pi}[(g(\theta,x))^{\otimes 2}]\] \[\bar{g}^{(1)}(\theta) :=\mathbb{E}_{x\sim\pi}[g^{\prime}(\theta,x)],\quad\bar{g}_{2}^{ (1)}(\theta):=\mathbb{E}_{x\sim\pi}[(g^{\prime}(\theta,x))^{\otimes 2}],\quad\bar{g}^{ (2)}(\theta):=\mathbb{E}_{x\sim\pi}[g^{\prime\prime}(\theta,x)].\]

We are now ready to present our proof. The proof consists of two major steps. For the complexity of this problem, we first set aside the projection constraint and focus on the BAR analysis, and we shall present the asymptotic bias characterization without the projection step. The analysis in this step thus shall work for the minorization proof technique as well. Then, in the second step, we elaborate on the impact brought along by the projection analysis and conclude our proof.

### Step 1: Bias Characterization without Projection

#### i.2.1 Step 1: First Moment Analysis

Consider test function \(h_{1}(x,\theta)=\theta-\theta^{*}\). Therefore, we first have

\[\mathbb{E}[\theta_{\infty+1}-\theta^{*}]=\mathbb{E}[\theta_{\infty}-\theta^{*}]+ \alpha\Big{(}\mathbb{E}[g(\theta_{\infty},x_{\infty})]+\mathbb{E}[\xi_{\infty+ 1}(\theta_{\infty})]\Big{)},\]

which immediately implies that

\[0=\mathbb{E}[g(\theta_{\infty},x_{\infty})].\] (I.6)

Substituting the Taylor expansion (I.4) back into (I.6), we have

\[0 =\mathbb{E}[g(\theta^{*},x)]+\mathbb{E}[g^{\prime}(\theta^{*},x_{ \infty})(\theta_{\infty}-\theta^{*})]+\frac{1}{2}\mathbb{E}[g^{\prime\prime}( \theta^{*},x_{\infty})(\theta_{\infty}-\theta^{*})^{\otimes 2}]+\mathbb{E}[R_{3}( \theta_{\infty},x_{\infty})]\] \[=\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}- \theta^{*})]+\frac{1}{2}\mathbb{E}[g^{\prime\prime}(\theta^{*},x_{\infty})( \theta_{\infty}-\theta^{*})^{\otimes 2}]+\mathcal{O}((\alpha\tau)^{3/2}),\] (I.7)

where we make use of \(\mathbb{E}[g(\theta^{*},x)]=\bar{g}(\theta^{*})=0\) by definition and the order or \(\mathbb{E}[R_{3}(\theta_{\infty},x_{\infty})]=\mathcal{O}((\alpha\tau)^{3/2})\) to obtain the second equality.

Next, we proceed to analyze the two terms in (I.7). For the first term, we have

\[\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}- \theta^{*})] =\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})z_{1}(x_{\infty})]\] \[=\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})\delta_{1}(x_{ \infty})]+\bar{g}^{(1)}(\theta^{*})\mathbb{E}[\theta_{\infty}-\theta^{*}].\] (I.8)

We now move on to analyze the second term, and obtain

\[\mathbb{E}[g^{\prime\prime}(\theta^{*},x_{\infty})(\theta_{\infty }-\theta^{*})^{\otimes 2}] =\mathbb{E}[g^{\prime\prime}(\theta^{*},x_{\infty})z_{2}(x_{\infty })]\] \[=\mathbb{E}[g^{\prime\prime}(\theta^{*},x_{\infty})\delta_{2}(x_{ \infty})]+\bar{g}^{(2)}(\theta^{*})\mathbb{E}[(\theta_{\infty}-\theta^{*})^{ \otimes 2}].\] (I.9)

Hence, substituting (I.8) and (I.9) back into (I.7), we reorganize the terms and arrive at

\[\mathbb{E}[\theta_{\infty}-\theta^{*}]\] \[=-(\bar{g}^{(1)}(\theta^{*}))^{-1}\Big{(}\mathbb{E}[g^{\prime}( \theta^{*},x_{\infty})\delta_{1}(x_{\infty})]\] (I.10) \[\qquad\qquad\qquad\qquad+\frac{1}{2}\Big{(}\mathbb{E}[g^{\prime \prime}(\theta^{*},x_{\infty})\delta_{2}(x_{\infty})]+\bar{g}^{(2)}(\theta^{* })\mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}]\Big{)}\Big{)}\] (I.11) \[+\mathcal{O}((\alpha\tau)^{3/2}).\]

To obtain a refined characterization of the asymptotic bias, we carefully analyze the remaining three terms in (I.10)-(I.11). We focus on each term in the next three sections respectively.

#### i.2.2 Step 2: Second Moment Analysis

We start with analyzing \(\mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}]\) in this section.

Following the BAR approach, we consider the test function \(h_{2}(x,\theta)=(\theta-\theta^{*})^{\otimes 2}\) and obtain

\[\mathbb{E}[(\theta_{\infty+1}-\theta^{*})^{\otimes 2}] =\mathbb{E}[(\theta_{\infty}-\theta^{*}+\alpha(g(\theta_{\infty},x_ {\infty})+\xi_{\infty+1}(\theta_{\infty}))^{\otimes 2}]\] \[=\mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}]+\alpha^{2}( \mathbb{E}[(g(\theta_{\infty},x_{\infty}))^{\otimes 2}]+\mathbb{E}[(\xi_{\infty+1}( \theta_{\infty}))^{\otimes 2}])\] \[+\alpha(\mathbb{E}[g(\theta_{\infty},x_{\infty})\otimes(\theta_{ \infty}-\theta^{*})]+\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes g(\theta_{ \infty},x_{\infty})]).\]

Simplifying the above expression, we have

\[0 =\alpha(\mathbb{E}[(g(\theta_{\infty},x_{\infty}))^{\otimes 2}]+ \mathbb{E}[(\xi_{\infty+1}(\theta_{\infty}))^{\otimes 2}])\] (I.12) \[+(\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes g(\theta_{ \infty},x_{\infty})]+\mathbb{E}[g(\theta_{\infty},x_{\infty})\otimes(\theta_{ \infty}-\theta^{*})]).\]

We adopt a similar approach in analyzing the above relationship that contains \(g(\theta_{\infty},x_{\infty})\) as in the previous step. We make use of the Taylor expansion of \(g\) at \(\theta^{*}\) but at a lower order. We substitute the Taylor expansion (I.5) into (I.12) and obtain

\[0=\alpha\mathbb{E}[(g(\theta^{*},x_{\infty})+g^{\prime}(\theta^{*},x_{\infty})( \theta_{\infty}-\theta^{*})+R_{2}(\theta_{\infty},x_{\infty}))^{\otimes 2}]+ \alpha\mathbb{E}[(\xi_{\infty+1}(\theta_{\infty}))^{\otimes 2}]\]\[+\mathbb{E}[(g(\theta^{*},x_{\infty})+g^{\prime}(\theta^{*},x_{ \infty})(\theta_{\infty}-\theta^{*})+R_{2}(\theta_{\infty},x_{\infty}))\otimes( \theta_{\infty}-\theta^{*})]\] \[+\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes(g(\theta^{*},x_{ \infty})+g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}-\theta^{*})+R_{2}( \theta_{\infty},x_{\infty}))]\] \[=\mathbb{E}[g(\theta^{*},x_{\infty})\otimes(\theta_{\infty}- \theta^{*})]+\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes g(\theta^{*},x_{ \infty})]\] (I.13) \[+\mathbb{E}[(g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}- \theta^{*}))\otimes(\theta_{\infty}-\theta^{*})]+\mathbb{E}[(\theta_{\infty}- \theta^{*})\otimes(g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}-\theta^{ *}))]\] (I.14) \[+\alpha\mathbb{E}[(g^{\prime}(\theta^{*},x_{\infty})(\theta_{ \infty}-\theta^{*}))^{\otimes 2}]\] (I.15) \[+\alpha\mathbb{E}[g(\theta^{*},x_{\infty})\otimes(g^{\prime}( \theta^{*},x_{\infty})(\theta_{\infty}-\theta^{*}))]+\alpha\mathbb{E}[(g^{ \prime}(\theta^{*},x_{\infty})(\theta_{\infty}-\theta^{*}))\otimes g(\theta^{ *},x_{\infty})]\] (I.16) \[+\mathbb{E}[R_{2}(\theta_{\infty},x_{\infty})\otimes(\theta_{ \infty}-\theta^{*})]+\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes R_{2}( \theta_{\infty},x_{\infty})]\] \[+\alpha\mathbb{E}[(g(\theta^{*},x_{\infty}))^{\otimes 2}]+ \alpha\mathbb{E}[(\delta_{\infty+1}(\theta_{\infty}))^{\otimes 2}]+\mathcal{O}( \alpha^{2}\tau).\]

Therefore, we proceed to analyze the terms in (I.13)-(I.16).

Starting with the terms in (I.13), we have

\[\mathbb{E}[g(\theta^{*},x_{\infty})\otimes(\theta_{\infty}-\theta ^{*})] =\mathbb{E}[g(\theta^{*},x_{\infty})\otimes(\delta_{1}(x_{\infty})+ \pi z_{1})]\] \[=\mathbb{E}[g(\theta^{*},x_{\infty})\otimes\delta_{1}(x_{\infty}) ]+\underbrace{\mathbb{E}[g(\theta^{*},x_{\infty})]}_{=0}\otimes\mathbb{E}[ \theta_{\infty}-\theta^{*}]\] \[=\mathbb{E}[g(\theta^{*},x_{\infty})\otimes\delta_{1}(x_{\infty})].\]

Similarly,

\[\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes g(\theta^{*},x_{ \infty})]=\mathbb{E}[\delta_{1}(x_{\infty})\otimes g(\theta^{*},x_{\infty})].\]

Next, for the terms in (I.14), we have

\[\mathbb{E}[(g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}- \theta^{*}))\otimes(\theta_{\infty}-\theta^{*})] =\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}- \theta^{*})^{\otimes 2}]\] \[=\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})(\delta_{2}(x_{ \infty})+\pi z_{2})]\] \[=\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})\delta_{2}(x_{ \infty})]+\bar{g}^{(1)}(\theta^{*})\mathbb{E}[(\theta_{\infty}-\theta^{*})^{ \otimes 2}].\]

Similarly,

\[\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes(g^{\prime}(\theta^{*},x_{ \infty})(\theta_{\infty}-\theta^{*}))]=\mathbb{E}[\delta_{2}(x_{\infty})g^{ \prime}(\theta^{*},x_{\infty})]+\mathbb{E}[(\theta_{\infty}-\theta^{*})^{ \otimes 2}]\bar{g}^{(1)}(\theta^{*}).\]

Moving on to the second term in (I.15)

\[\mathbb{E}[(g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}- \theta^{*}))^{\otimes 2}] =\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}- \theta^{*})^{\otimes 2}g^{\prime}(\theta^{*},x_{\infty})]\] \[=\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})(\delta_{2}(x_{ \infty})+\pi z_{2})g^{\prime}(\theta^{*},x_{\infty})]\] \[=\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})\delta_{2}(x_{ \infty})g^{\prime}(\theta^{*},x_{\infty})]+\bar{g}_{2}^{(1)}(\theta^{*}) \mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}].\]

Last, for the terms in (I.16)

\[\mathbb{E}[g(\theta^{*},x_{\infty})\otimes(g^{\prime}(\theta^{*}, x_{\infty})(\theta_{\infty}-\theta^{*}))] =\mathbb{E}[g(\theta^{*},x_{\infty})\otimes(g^{\prime}(\theta^{*}, x_{\infty})(\delta_{1}(x_{\infty})+\pi z_{1}))]\] \[=\mathbb{E}[g(\theta^{*},x_{\infty})\otimes(g^{\prime}(\theta^{*}, x_{\infty})\delta_{1}(x_{\infty}))]\] \[+\mathbb{E}[g(\theta^{*},x_{\infty})\otimes(g^{\prime}(\theta^{*}, x_{\infty})\mathbb{E}[\theta_{\infty}-x_{\infty}])].\]

Similarly,

\[\mathbb{E}[(g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}- \theta^{*}))\otimes g(\theta^{*},x_{\infty})] =\mathbb{E}[(g^{\prime}(\theta^{*},x_{\infty})\delta_{1}(x_{\infty})) \otimes g(\theta^{*},x_{\infty})]\] \[+\mathbb{E}[(g^{\prime}(\theta^{*},x_{\infty})\mathbb{E}[\theta_{ \infty}-x_{\infty}])\otimes g(\theta^{*},x_{\infty})].\]

Lastly, by a second order Taylor expansion around \(\theta^{*}\) of \(C(\theta_{\infty})=\mathbb{E}[(\xi_{\infty+1}(\theta_{\infty}))^{\otimes 2}]\), we have

\[C(\theta_{\infty})=C(\theta^{*})+\mathbb{C}^{\prime}(\theta^{*})\mathbb{E}[ \theta_{\infty}-\theta^{*}]+\mathbb{E}[R^{\prime}_{2}(\theta_{\infty})],\]

where \(R^{\prime}_{2}(\theta)\) satisfies \(\sup_{x\in\mathbb{R}^{d}}\left\{\|R^{\prime}_{2}(\theta)\|/(\|\theta-\theta^{*} \|^{2}+\|\theta-\theta^{*}\|^{k_{c}+2})\right\}<\infty\).

Substituting the above analyses of the terms and consolidating the terms, we obtain

\[-(\bar{g}^{(1)}(\theta^{*})\otimes I+I\otimes\bar{g}^{(1)}(\theta^{*})) \mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}]\] (I.17) \[=\alpha\bar{g}_{2}(\theta^{*})+\alpha\mathbb{E}[(\zeta_{\infty+1}( \theta^{*}))^{\otimes 2}]+\alpha\bar{g}_{2}^{(1)}(\theta^{*})\mathbb{E}[(\theta_{ \infty}-\theta^{*})^{\otimes 2}]\] \[+\mathbb{E}[g(\theta^{*},x_{\infty})\otimes\delta_{1}(x_{\infty})] +\mathbb{E}[\delta_{1}(x_{\infty})\otimes g(\theta^{*},x_{\infty})]\] \[+\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})\delta_{2}(x_{\infty })]+\mathbb{E}[\delta_{2}(x_{\infty})g^{\prime}(\theta^{*},x_{\infty})]\] \[+\alpha\mathbb{E}[g(\theta^{*},x_{\infty})\otimes(g^{\prime}( \theta^{*},x_{\infty})\delta_{1}(x_{\infty}))]+\alpha\mathbb{E}[g(\theta^{*}, x_{\infty})\otimes(g^{\prime}(\theta^{*},x_{\infty})\mathbb{E}[\theta_{ \infty}-x_{\infty}])]\] \[+\alpha\mathbb{E}[(g^{\prime}(\theta^{*},x_{\infty})\delta_{1}(x _{\infty}))\otimes g(\theta^{*},x_{\infty})]+\alpha\mathbb{E}[(g^{\prime}( \theta^{*},x_{\infty})\mathbb{E}[\theta_{\infty}-x_{\infty}])\otimes g(\theta ^{*},x_{\infty})]\] \[+\alpha\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})\delta_{2}(x _{\infty})g^{\prime}(\theta^{*},x_{\infty})]\] \[+\mathbb{E}[R_{2}(\theta_{\infty},x_{\infty})\otimes(\theta_{ \infty}-\theta^{*})]+\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes R_{2}( \theta_{\infty},x_{\infty})]+\alpha C^{\prime}(\theta^{*})\mathbb{E}[\theta_{ \infty}-\theta^{*}]\] \[+\mathcal{O}(\alpha^{2}\tau).\]

By far, we observe that the remaining terms all contain \(\delta_{1}\) and \(\delta_{2}\). Therefore, we conclude our analysis of \(\mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}]\) at this step and leave the analysis of \(\delta_{1}\) and \(\delta_{2}\) to the next section.

#### i.2.3 Step 3: Analysis of the \(\delta\)-System

In this section, we analyze \(\delta_{1}\) and \(\delta_{2}\).

Analysis of \(\delta_{1}\).Starting with \(\delta_{1}\), we first consider the following recursive relationship induced by (I.2).

\[\mathbb{E}[\theta_{\infty+1}-\theta^{*}|x_{\infty+1}=s]=\int_{ \mathbb{R}}P^{*}(s,\mathrm{d}s^{\prime})\mathbb{E}[\theta_{\infty+1}-\theta^{ *}|x_{\infty+1}=s,x_{\infty}=s^{\prime}]\] \[\overset{\text{(i)}}{=}\int_{\mathbb{R}}P^{*}(s,\mathrm{d}s^{ \prime})\mathbb{E}[\theta_{\infty}-\theta^{*}+\alpha g(\theta_{\infty},x_{ \infty})|x_{\infty}=s^{\prime}]\] \[\overset{\text{(ii)}}{=}\int_{\mathbb{R}}P^{*}(s,\mathrm{d}s^{ \prime})\mathbb{E}\Big{[}\theta_{\infty}-\theta^{*}+\alpha\Big{(}g(\theta^{*}, x_{\infty})+g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}-\theta^{*})+R_{2}( \theta_{\infty},x_{\infty})\Big{)}|x_{\infty}=s^{\prime}\Big{]}\] \[=\int_{\mathbb{R}}P^{*}(s,\mathrm{d}s^{\prime})\mathbb{E}\Big{[} \theta_{\infty}-\theta^{*}|x_{\infty}=s^{\prime}\Big{]}\] \[+\alpha\int_{\mathbb{R}}P^{*}(s,\mathrm{d}s^{\prime})\Big{(}g( \theta^{*},s^{\prime})+g^{\prime}(\theta^{*},s^{\prime})\mathbb{E}[\theta_{ \infty}-\theta^{*}|x_{\infty}=s^{\prime}]+\mathbb{E}[R_{2}(\theta_{\infty},x_ {\infty})|x_{\infty}=s^{\prime}]\Big{)},\]

where in (i) we make use of the update rule in (I.2), \(\mathbb{E}[\xi_{\infty+1}(\theta_{\infty})]=0\) and conditional independence \(x_{\infty+1}\perp\!\!\!\perp\theta_{\infty}|x_{\infty}\). Next, we substitute the Taylor expansion (I.5) to obtain (ii).

Writing with notation shorthands \(z\) and \(\delta\), we have

\[z_{1}(s) =\int_{\mathbb{R}}P^{*}(s,\mathrm{d}s^{\prime})z_{1}(s^{\prime})\] (I.18) \[+\alpha\int_{R}P^{*}(s,\mathrm{d}s^{\prime})\Big{(}g(\theta^{*},s ^{\prime})+g^{\prime}(\theta^{*},s^{\prime})z_{1}(s^{\prime})+\mathbb{E}[R_{2} (\theta_{\infty},s^{\prime})|x_{\infty}=s^{\prime}]\Big{)}.\]

If we apply \(\pi\) to both sides of (I.18), we obtain

\[\int\pi(\mathrm{d}s)P^{*}(s,\mathrm{d}s^{\prime})\Big{(}g(\theta^{*},s^{ \prime})+g^{\prime}(\theta^{*},s^{\prime})z_{1}(s^{\prime})+\mathbb{E}[R_{2}( \theta_{\infty},s^{\prime})|x_{\infty}=s^{\prime}]\Big{)}=0.\]

Analyzing the three terms closely, we observe that

\[\int\pi(\mathrm{d}s)\int P^{*}(s,\mathrm{d}s^{\prime})g(\theta^{*},s^{\prime})= \int g(\theta^{*},s^{\prime})\underbrace{\int\pi(\mathrm{d}s)P^{*}(s,\mathrm{d }s^{\prime})}_{=\pi(\mathrm{d}s^{\prime})}=\bar{g}(\theta^{*})=0\]

\[\int\pi(\mathrm{d}s)\int P^{*}(s,\mathrm{d}s^{\prime})g^{\prime}(\theta^{*},s^{ \prime})z_{1}(s^{\prime})=\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})z_{1}(x_{ \infty})]\]\[\int\pi(\mathrm{d}s)\int P^{*}(s,\mathrm{d}s^{\prime})\mathbb{E}[R_{2}(\theta_{ \infty},s^{\prime})|x_{\infty}=s^{\prime}]=\mathbb{E}[R_{2}(\theta_{\infty},x_{ \infty})],\]

and hence we first obtain

\[\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})z_{1}(x_{\infty})]+\mathbb{E}[R_{2} (\theta_{\infty},x_{\infty})]=0.\] (I.19)

We now subtract \(\pi z_{1}\) on both sides of (I.18), and obtain

\[\delta_{1}(s) =\int_{\mathbb{R}}P^{*}(s,\mathrm{d}s^{\prime})\delta_{1}(s^{ \prime})\] \[+\alpha\int_{R}P^{*}(s,\mathrm{d}s^{\prime})\Big{(}g(\theta^{*}, s^{\prime})+g^{\prime}(\theta^{*},s^{\prime})z_{1}(s^{\prime})+\mathbb{E}[R_{2} (\theta_{\infty},s^{\prime})|x_{\infty}=s^{\prime}]\Big{)}\] \[=\int_{\mathbb{R}}\Big{(}P^{*}(s,\mathrm{d}s^{\prime})-\pi( \mathrm{d}s^{\prime})\Big{)}\delta_{1}(s^{\prime})\] \[+\alpha\int_{R}\Big{(}P^{*}(s,\mathrm{d}s^{\prime})-\pi(\mathrm{ d}s^{\prime})\Big{)}\Big{(}g(\theta^{*},s^{\prime})+g^{\prime}(\theta^{*},s^{ \prime})z_{1}(s^{\prime})+\mathbb{E}[R_{2}(\theta_{\infty},s^{\prime})|x_{ \infty}=s^{\prime}]\Big{)}.\]

Consolidating the terms, we have

\[(I-P^{*}+\Pi)\delta_{1}(s)\] (I.20) \[=\alpha\int_{R}\Big{(}P^{*}(s,\mathrm{d}s^{\prime})-\pi(\mathrm{ d}s^{\prime})\Big{)}\Big{(}g(\theta^{*},s^{\prime})+g^{\prime}(\theta^{*},s^{ \prime})z_{1}(s^{\prime})+\mathbb{E}[R_{2}(\theta_{\infty},s^{\prime})|x_{ \infty}=s^{\prime}]\Big{)}.\]

We next note the following properties,

\[\|\mathbb{E}[R_{2}(\theta_{\infty},x_{\infty})|x_{\infty}=s]\|_{ L^{2}(\pi)}^{2}\leq\mathbb{E}[\|R_{2}(\theta_{\infty},x_{\infty})^{2}\|]\| ]=\mathcal{O}((\alpha\tau)^{2}),\] \[\|z_{1}(s)\|_{L^{2}(\pi)}^{2}\leq\mathbb{E}[\|\theta_{\infty}- \theta^{*}\|^{2}]=\mathcal{O}(\alpha\tau)=\mathcal{O}(1).\]

Therefore, we can first conclude that \(\|\delta\|_{L^{2}(\pi)}=\mathcal{O}(\alpha)\).

Subsequently, from (I.19), we can derive that

\[0 =\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})z_{1}(x_{\infty})]+ \mathbb{E}[R_{2}(\theta_{\infty},x_{\infty})]\] \[=\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})\delta_{1}(x_{ \infty})]+\bar{g}^{(1)}(\theta^{*})\mathbb{E}[\theta_{\infty}-\theta^{*}]+ \mathbb{E}[R_{2}(\theta_{\infty},x_{\infty})]\] \[\mathbb{E}[\theta_{\infty}-\theta^{*}] =-(\bar{g}^{(1)}(\theta^{*}))^{-1}\Big{(}\mathbb{E}[g^{\prime}( \theta^{*},x_{\infty})\delta_{1}(x_{\infty})]+\mathbb{E}[R_{2}(\theta_{\infty },x_{\infty})]\Big{)}.\] (I.21)

Hence, together with the relationship between \(\delta_{1}\) and \(z_{1}\), we have

\[z_{1}=\delta_{1}-(\bar{g}^{(1)}(\theta^{*}))^{-1}\Big{(}\mathbb{E}[g^{\prime}( \theta^{*},x_{\infty})\delta_{1}(x_{\infty})]+\mathbb{E}[R_{2}(\theta_{\infty },x_{\infty})]\Big{)}.\] (I.22)

Lastly, we substitute (I.22) back into (I.20) and obtain

\[(I-P^{*}+\Pi)\delta_{1}(s)\] \[=\alpha\int_{R}\Big{(}P^{*}(s,\mathrm{d}s^{\prime})-\pi(\mathrm{d }s^{\prime})\Big{)}g(\theta^{*},s^{\prime})\] \[+\alpha\int_{R}\Big{(}P^{*}(s,\mathrm{d}s^{\prime})-\pi(\mathrm{ d}s^{\prime})\Big{)}g^{\prime}(\theta^{*},s^{\prime})\Big{(}\delta_{1}(s^{\prime})-( \bar{g}^{(1)}(\theta^{*}))^{-1}\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty}) \delta_{1}(x_{\infty})]\Big{)}\] \[-\alpha\int_{R}\Big{(}P^{*}(s,\mathrm{d}s^{\prime})-\pi( \mathrm{d}s^{\prime})\Big{)}g^{\prime}(\theta^{*},s^{\prime})\Big{(}(\bar{g}^{(1 )}(\theta^{*}))^{-1}\mathbb{E}[R_{2}(\theta_{\infty},x_{\infty})]\Big{)}\] \[+\alpha\int_{R}\Big{(}P^{*}(s,\mathrm{d}s^{\prime})-\pi( \mathrm{d}s^{\prime})\Big{)}\mathbb{E}[R_{2}(\theta_{\infty},x_{\infty})|x_{ \infty}=s^{\prime}]\Big{)}\] \[=\alpha v(\theta^{*},s)+\mathcal{O}(\alpha^{2}\tau),\]

where

\[v(\theta^{*},s)=(I-P^{*}+\Pi)^{-1}(P^{*}-\Pi)g_{\theta^{*}}(s)=\int_{R}(I-P^{*} +\Pi)^{-1}(P^{*}-\Pi)(s,\mathrm{d}s^{\prime})g(\theta^{*},s^{\prime}).\]Therefore, for the terms that involve \(\delta_{1}\), we can conclude that

\[\mathbb{E}[g(\theta^{*},x_{\infty})\otimes\delta_{1}(x_{\infty})]= \alpha M+\mathcal{O}(\alpha^{2}\tau)\quad\text{and}\quad\mathbb{E}[\delta_{1}(x_ {\infty})\otimes g(\theta^{*},x_{\infty})]=\alpha M+\mathcal{O}(\alpha^{2}\tau),\] \[\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})\delta_{1}(x_{\infty })]=\alpha v^{\prime}+\mathcal{O}(\alpha^{2}\tau),\] (I.23)

where \(M\) and \(v\) are independent of \(\alpha\).

Note that (I.23) together with (I.21) implies that

\[\mathbb{E}[\theta_{\infty}-\theta^{*}]=\alpha v^{(1)}+\mathcal{O}(\alpha\tau).\]

Analysis of \(\delta_{2}\).For \(z_{2}\), we first note that

\[|\mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}|x_{\infty}]|^{2}_{L^{2}( \pi)}\leq\mathbb{E}[\|\theta_{\infty}-\theta^{*}\|^{4}]=\mathcal{O}((\alpha \tau)^{2}),\]

and hence this implies that

\[\|z_{2}\|_{L^{2}(\pi)}=\mathcal{O}(\alpha\tau).\]

Next, following (I.3), we obtain the following recursive relationship.

\[\mathbb{E}[(\theta_{\infty+1}-\theta^{*})^{\otimes 2}|x_{ \infty+1}=s^{\prime}]=\int P^{*}(s^{\prime},\mathrm{d}s)\mathbb{E}[(\theta_{ \infty+1}-\theta^{*})^{\otimes 2}|x_{\infty+1}=s^{\prime},x_{\infty}=s]\] \[\overset{\text{(i)}}{=}\int P^{*}(s^{\prime},\mathrm{d}s) \mathbb{E}[(\theta_{\infty}-\theta^{*}+\alpha(g(\theta_{\infty},x_{\infty})+ \xi_{\infty+1}(\theta_{\infty})))^{\otimes 2}|x_{\infty}=s]\] \[\overset{\text{(ii)}}{=}\int P^{*}(s^{\prime},\mathrm{d}s) \mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}|x_{\infty}=s]\] \[+\alpha\int P^{*}(s^{\prime},\mathrm{d}s)\Big{(}\mathbb{E}[( \theta_{\infty}-\theta^{*})\otimes g(\theta_{\infty},s)|x_{\infty}=s]+\mathbb{ E}[g(\theta_{\infty},s)\otimes(\theta_{\infty}-\theta^{*})|x_{\infty}=s]\Big{)},\]

where in (i) we make use of the update rule in (2.1) and conditional independence \(x_{\infty+1}\perp\!\!\!\perp\theta_{\infty}|x_{\infty}\). Next, we substitute the Taylor expansion (I.5) to obtain (ii).

Writing with \(z_{2}\) shorthand, we have

\[z_{2}(s^{\prime}) =\int P^{*}(s^{\prime},\mathrm{d}s)z_{2}(s)\] \[+\alpha^{2}\int P^{*}(s^{\prime},\mathrm{d}s)\mathbb{E}[(g(\theta _{\infty},s))^{\otimes 2}|x_{\infty}=s]+\alpha^{2}\int P^{*}(s^{\prime}, \mathrm{d}s)\mathbb{E}[(\xi_{\infty+1}(\theta_{\infty}))^{\otimes 2}|x_{ \infty}=s]\] \[+\alpha\int P^{*}(s^{\prime},\mathrm{d}s)\Big{(}\mathbb{E}[( \theta_{\infty}-\theta^{*})\otimes g(\theta_{\infty},s)|x_{\infty}=s]+ \mathbb{E}[g(\theta_{\infty},s)\otimes(\theta_{\infty}-\theta^{*})|x_{\infty }=s]\Big{)}.\]

Making use of the relationship \((P^{*}-\Pi)z_{2}=(P^{*}-\Pi)\delta_{2}\), we have

\[\delta_{2}(s^{\prime}) =\int\Big{(}P^{*}(s^{\prime},\mathrm{d}s)-\pi(\mathrm{d}s)\Big{)} \delta_{2}(s)\] \[+\alpha^{2}\int P^{*}(s^{\prime},\mathrm{d}s)\mathbb{E}[(g(\theta _{\infty},s))^{\otimes 2}|x_{\infty}=s]+\alpha^{2}\int P^{*}(s^{\prime}, \mathrm{d}s)\mathbb{E}[(\xi_{\infty+1}(\theta_{\infty}))^{\otimes 2}|x_{ \infty}=s]\] \[+\alpha\int P^{*}(s^{\prime},\mathrm{d}s)\Big{(}\mathbb{E}[( \theta_{\infty}-\theta^{*})\otimes g(\theta_{\infty},s)|x_{\infty}=s]+ \mathbb{E}[g(\theta_{\infty},s)\otimes(\theta_{\infty}-\theta^{*})|x_{\infty }=s]\Big{)}.\]

Hence,

\[(I-P^{*}+\Pi)\delta_{2}(s^{\prime})\] \[=\alpha\int P^{*}(s^{\prime},\mathrm{d}s)\Big{(}\mathbb{E}[( \theta_{\infty}-\theta^{*})\otimes g(\theta_{\infty},s)|x_{\infty}=s]+ \mathbb{E}[g(\theta_{\infty},s)\otimes(\theta_{\infty}-\theta^{*})|x_{\infty }=s]\] \[+\alpha^{2}\int P^{*}(s^{\prime},\mathrm{d}s)\Big{(}\mathbb{E}[g(( \theta_{\infty},s))^{\otimes 2}|x_{\infty}=s]+\mathbb{E}[(\xi_{\infty+1}(\theta_{ \infty}))^{\otimes 2}|x_{\infty}=s]\Big{)}.\]To analyze the above system of \(\delta_{2}\), we make use of the Taylor expansion of \(g(\theta_{\infty},s)\) and \(\xi_{\infty+1}(\theta_{\infty})\).

Starting with the first term, we have

\[\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes g(\theta_{\infty}, s)|x_{\infty}=s]\] \[=\mathbb{E}\Big{[}\Big{(}\theta_{\infty}-\theta^{*}\Big{)} \otimes\Big{(}g(\theta^{*},s)+g^{\prime}(\theta^{*},s)(\theta_{\infty}-\theta ^{*})+R_{2}(\theta,s)\Big{)}|x_{\infty}=s\Big{]}\] \[=z_{1}(s)\otimes g(\theta^{*},s)+z_{2}(s)g^{\prime}(\theta^{*},s) +\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes R_{2}(\theta,s)|x_{\infty}=s]\] \[=\Big{(}\delta_{1}(s)-(\bar{g}^{(1)}(\theta^{*}))^{-1}\Big{(} \mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})\delta_{1}(x_{\infty})]+\mathbb{E} [R_{2}(\theta_{\infty},x_{\infty})]\Big{)}\Big{)}\otimes g(\theta^{*},s)\] \[+z_{2}(s)g^{\prime}(\theta^{*},s)+\mathbb{E}[(\theta_{\infty}- \theta^{*})\otimes R_{2}(\theta,s)|x_{\infty}=s].\]

Similarly, we have the following relationship for the second term,

\[\mathbb{E}[g(\theta_{\infty},s)\otimes(\theta_{\infty}-\theta^{ *})|x_{\infty}=s]\] \[=g(\theta^{*},s)\otimes\Big{(}\delta_{1}(s)-(\bar{g}^{(1)}(\theta ^{*}))^{-1}\Big{(}\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})\delta_{1}(x_{ \infty})]+\mathbb{E}[R_{2}(\theta_{\infty},x_{\infty})]\Big{)}\Big{)}\] \[+g^{\prime}(\theta^{*},s)z_{2}(s)+\mathbb{E}[R_{2}(\theta,s) \otimes(\theta_{\infty}-\theta^{*})|x_{\infty}=s].\]

Next, we proceed to analyze the third term.

\[\mathbb{E}[(g(\theta_{\infty},s))^{\otimes 2}|x_{\infty}=s]\] \[=\mathbb{E}\Big{[}\Big{(}g(\theta^{*},s)+g^{\prime}(\theta^{*},s) (\theta_{\infty}-\theta^{*})+R_{2}(\theta_{\infty},s)\Big{)}^{\otimes 2}|x_{ \infty}=s\Big{]}\] \[=\mathbb{E}[(g(\theta^{*},s))^{\otimes 2}|x_{\infty}=s]+\mathbb{E}[(g^{ \prime}(\theta^{*},s)(\theta_{\infty}-\theta^{*}))^{\otimes 2}|x_{\infty}=s]\] \[+\mathbb{E}[g(\theta^{*},s)\otimes(g^{\prime}(\theta^{*},s)( \theta_{\infty}-\theta^{*}))|x_{\infty}=s]+\mathbb{E}[(g^{\prime}(\theta^{*},s )(\theta_{\infty}-\theta^{*}))\otimes g(\theta^{*},s)|x_{\infty}=s]\] \[+\mathbb{E}[g(\theta^{*},s)\otimes R_{2}(\theta_{\infty},s)|x_{ \infty}=s]+\mathbb{E}[R_{2}(\theta_{\infty},s)\otimes g(\theta^{*},s)|x_{ \infty}=s]\] \[+\mathbb{E}[(g^{\prime}(\theta^{*},s)(\theta_{\infty}-\theta^{*}) )\otimes R_{2}(\theta_{\infty},s)|x_{\infty}=s]x_{\infty}=s]\] \[+\mathbb{E}[R_{2}(\theta_{\infty},s)\otimes(g^{\prime}(\theta^{* },s)(\theta_{\infty}-\theta^{*}))|x_{\infty}=s]+\mathbb{E}[(R_{2}(\theta_{ \infty},s))^{\otimes 2}|x_{\infty}=s].\]

Lastly, for the noise term, we derive that

\[\mathbb{E}[(\xi_{\infty+1}(\theta_{\infty}))^{\otimes 2}|x_{\infty}=s]= \mathbb{E}[(\xi_{\infty+1}(\theta^{*}))^{\otimes 2}]+C^{\prime}(\theta^{*}) \mathbb{E}[\theta_{\infty}-\theta^{*}|x_{\infty}=s]+\mathbb{E}[R_{2}^{\prime} (\theta_{\infty})|x_{\infty}=s].\]

Leveraging on the respective orders, we can conclude that

\[\|\delta_{2}\|_{L^{2}(\pi)}=\mathcal{O}(\alpha^{2}\tau).\]

Therefore, for second-moment cross-terms, we have the following orders

\[\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})\delta_{2}(x_{\infty})] =\mathcal{O}(\alpha^{2}\tau)\quad\text{and}\quad\mathbb{E}[\delta_{2}(x_{ \infty})g^{\prime}(\theta^{*},x_{\infty})]=\mathcal{O}(\alpha^{2}\tau),\] \[\mathbb{E}[g^{\prime\prime}(\theta^{*},x_{\infty})\delta_{2}(x_{ \infty})] =\mathcal{O}(\alpha^{2}\tau).\] (I.24)

#### i.2.4 Step 4: Bias Characterization

Finally, we are ready to consolidate the above analyses and conclude the characterization of the asymptotic bias.

We recall that we have already shown the following expansion of the asymptotic bias

\[\mathbb{E}[\theta_{\infty}-\theta^{*}]\] \[=-(\bar{g}^{(1)}(\theta^{*}))^{-1}\Big{(}\mathbb{E}[g^{\prime}( \theta^{*},x_{\infty})\delta_{1}(x_{\infty})]+\frac{1}{2}\Big{(}\mathbb{E}[g^{ \prime\prime}(\theta^{*},x_{\infty})\delta_{2}(x_{\infty})]+\bar{g}^{(2)}(\theta ^{*})\mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}]\Big{)}\Big{)}\] \[+\mathcal{O}((\alpha\tau)^{3/2}).\]

By our analyses above, we have shown that

\[\delta_{1}=\alpha(I-P^{*}+\Pi)^{-1}(P^{*}-\Pi)g_{\theta}^{*}+ \mathcal{O}(\alpha^{2}\tau).\]

Hence, we derive that

\[\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})\delta_{1}(x_{\infty})]=\alpha \mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})(I-P^{*}+\Pi)^{-1}(P^{*}-\Pi)g_{ \theta}^{*}(x_{\infty})]+\mathcal{O}(\alpha^{2}\tau).\]For the second term, we simply use the order shown in (I.24).

Lastly, we substitute our analyses of \(\delta_{1}\) and \(\delta_{2}\) into the expansion of MSE (I.17) and derive that

\[-(\bar{g}^{(1)}(\theta^{*})\otimes I+I\otimes\bar{g}^{(1)}(\theta^{ *}))\mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}]\] \[=\alpha\bar{g}_{2}(\theta^{*})+\alpha\mathbb{E}[(\xi_{\infty+1}( \theta^{*}))^{\otimes 2}]\] \[+\alpha\mathbb{E}[g(\theta^{*},x_{\infty})\otimes(I-P^{*}+\Pi)^ {-1}(P^{*}-\Pi)g_{\theta^{*}}(x_{\infty})]\] \[+\alpha\mathbb{E}[(I-P^{*}+\Pi)^{-1}(P^{*}-\Pi)g_{\theta^{*}}(x_{ \infty})\otimes g(\theta^{*},x_{\infty})]+\mathcal{O}(\alpha^{2}\tau).\]

Therefore, combining all the analyses, we have shown the bias characterization in Theorem 4.6,

\[\mathbb{E}[\theta_{\infty}-\theta^{*}]\] \[=-\alpha\cdot(\bar{g}^{(1)}(\theta^{*}))^{-1}\mathbb{E}[g^{ \prime}(\theta^{*},x_{\infty})h(\theta^{*},x_{\infty})]\] \[+\alpha\cdot\frac{1}{2}(\bar{g}^{(1)}(\theta^{*}))^{-1}(\bar{g}^ {(1)}A\Big{(}\bar{g}_{2}(\theta^{*})+\alpha\mathbb{E}[(\xi_{\infty+1}(\theta^ {*}))^{\otimes 2}]\Big{)}\] \[+\alpha\cdot\frac{1}{2}(\bar{g}^{(1)}(\theta^{*}))^{-1}A\Big{(} \mathbb{E}[g(\theta^{*},x_{\infty})\otimes h(\theta^{*},x_{\infty})]+\mathbb{ E}[h(\theta^{*},x_{\infty})\otimes g(\theta^{*},x_{\infty})]\Big{)}+\mathcal{O}(( \alpha\tau)^{3/2}).\]

where

\[A =(\bar{g}^{(1)}(\theta^{*})\otimes I+I\otimes\bar{g}^{(1)}( \theta^{*}))^{-1},\] \[h(\theta^{*},s) =(I-P^{*}+\Pi)^{-1}(P^{*}-\Pi)g_{\theta^{*}}(s)\] \[=\int_{\mathcal{X}}(I-P^{*}+\Pi)^{-1}(P^{*}-\Pi)(s,\text{d}s^{ \prime})g(\theta^{*},s^{\prime}).\]

Therefore, we see that assuming weak convergence without projection, the bias admits a leading term of order \(\alpha\). We emphasize that the expansion holds as equality, rather than an upper bound.

### Step 2: Impact of Projection on Bias

Now, we proceed to analyze the impact of having the additional projection step on the asymptotic bias characterization. In the following, we use the shorthand \(\theta_{t+1/2}\) to denote the iterate we obtain before the projection step, i.e.,

\[\theta_{t+1/2}=\theta_{t}+\alpha\big{(}g(\theta_{t},x_{t})+\xi_{t+1}(\theta_ {t})\big{)}\quad\text{and}\quad\theta_{t+1}=\Pi_{B(\beta)}\theta_{t+1/2}.\]

Therefore, we see that our analysis from Step 1 can be understood as the analysis for \(\theta_{\infty+1/2}\).

Starting with the first moment analysis with test function \(h_{1}(x,\theta)=\theta-\theta^{*}\), we have

\[\mathbb{E}[\theta_{\infty+1}-\theta^{*}] =\mathbb{E}[\theta_{\infty+1/2}-\theta^{*}]+\mathbb{E}[\theta_{ \infty+1}-\theta_{\infty+1/2}]\] \[=\mathbb{E}[\theta_{\infty}-\theta^{*}]+\alpha\big{(}\mathbb{E} [g(\theta_{\infty},x_{\infty})]+\mathbb{E}[\xi_{\infty+1}(\theta_{\infty})] \big{)}+\mathbb{E}[\theta_{\infty+1}-\theta_{\infty+1/2}],\]

which implies that

\[-\frac{1}{\alpha}\mathbb{E}[\theta_{\infty+1}-\theta_{\infty+1/2 }]=\mathbb{E}[g^{\prime}(\theta^{*},x_{\infty})(\theta_{\infty}-\theta^{*})]+ \frac{1}{2}\mathbb{E}[g^{\prime\prime}(\theta^{*},x_{\infty})(\theta_{\infty} -\theta^{*})^{\otimes 2}]+\mathcal{O}((\alpha\tau)^{3/2}).\] (I.25)

Therefore, we turn our focus to analyzing \(\mathbb{E}[\theta_{\infty+1}-\theta_{\infty+1/2}]\).

\[\mathbb{E}[\theta_{\infty+1}-\theta_{\infty+1/2}]\] \[=\mathbb{E}[(\theta_{\infty+1}-\theta_{\infty+1/2})\mathbbm{1}\{ \|\theta_{t+1/2}-\theta^{*}\|<\beta\}]+\mathbb{E}[(\theta_{\infty+1}-\theta_{ \infty+1/2})\mathbbm{1}\{\|\theta_{t+1/2}-\theta^{*}\|\geq\beta\}]\] \[=\mathbb{E}[(\theta_{\infty+1}-\theta_{\infty+1/2})\mathbbm{1}\{ \|\theta_{t+1/2}-\theta^{*}\|\geq\beta\}],\]

where we note that when \(\|\theta_{t+1/2}-\theta^{*}\|\leq\beta\) implies that \(\|\theta_{t+1/2}\|\leq\beta+\|\theta^{*}\|\leq 2\beta\) and hence \(\theta_{\infty+1}=\theta_{t+1/2}\) in this case. To analyze the remaining term, we use Holder's inequality and obtain

\[\|\mathbb{E}[(\theta_{\infty+1}-\theta_{\infty+1/2})\mathbbm{1}\{ \|\theta_{t+1/2}-\theta^{*}\|\geq\beta\}]\|\] \[\leq\mathbb{E}^{1/p}[\|(\theta_{\infty+1}-\theta^{*})-(\theta_{ \infty+1/2}-\theta^{*})\|^{p}]\mathbb{E}^{1/q}[\mathbbm{1}\{\|\theta_{t+1/2}- \theta^{*}\|\geq\beta\}]\]\[\leq 2\mathbb{E}^{1/p}[\|\theta_{\infty+1/2}-\theta^{*}\|^{p}]\big{(} \mathbb{P}(\|\theta_{t+1/2}-\theta^{*}\|\geq\beta)\big{)}^{1/q}.\]

Setting \(p=6\) and \(q=6/5\), and making use of the property that \(\mathbb{E}[\|\theta_{\infty+1/2}-\theta^{*}\|^{6}]=\mathcal{O}((\alpha\tau)^{3})\) from Proposition 4.2, we have

\[\mathbb{E}^{1/6}[\|\theta_{\infty+1/2}-\theta^{*}\|^{6}]\big{(} \mathbb{P}(\|\theta_{t+1/2}-\theta^{*}\|\geq R)\big{)}^{5/6} \lesssim(\alpha\tau)^{3/6}\Big{(}\mathbb{E}[\|\theta_{t+1/2}- \theta^{*}\|^{6}]/R^{6})^{5/6}\] \[\lesssim(\alpha\tau)^{3}.\]

Hence, we can conclude that

\[\mathbb{E}[\theta_{\infty+1}-\theta_{\infty+1/2}]=\mathcal{O}((\alpha\tau)^{3}).\]

Substituting this order information back into (I.25), we can see that \(\mathbb{E}[\theta_{\infty+1}-\theta_{\infty+1/2}]/\alpha=\mathcal{O}(\alpha^ {2}\tau^{3})\). Recall that \(\tau=\mathcal{O}(\log(1/\alpha))\), and hence we can assimilate this residual order from projection into the existing \(\mathcal{O}((\alpha\tau)^{3/2})\) residual term.

For the remaining terms, we follow the existing analysis in Section I.2.1 and again obtain

\[\mathbb{E}[\theta_{\infty}-\theta^{*}] =-(\bar{g}^{(1)}(\theta^{*}))^{-1}\Big{(}\mathbb{E}[g^{\prime}( \theta^{*},x_{\infty})\delta_{1}(x_{\infty})]\] \[\qquad\qquad\qquad+\frac{1}{2}\Big{(}\mathbb{E}[g^{\prime\prime}( \theta^{*},x_{\infty})\delta_{2}(x_{\infty})]+\bar{g}^{(2)}(\theta^{*}) \mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}]\Big{)}\Big{)}\] \[+\mathcal{O}((\alpha\tau)^{3/2}).\]

Now, we proceed to analyze \(\mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}]\) and examine the impact of projection. Consider test function \(h_{2}(x,\theta)=(\theta-\theta^{*})^{\otimes 2}\) and follow a similar strategy as the first moment analysis, we have

\[\mathbb{E}[(\theta_{\infty+1}-\theta^{*})^{\otimes 2}] =\mathbb{E}[(\theta_{\infty+1/2}-\theta^{*})^{\otimes 2}]+ \mathbb{E}[(\theta_{\infty+1}-\theta^{*})^{\otimes 2}-(\theta_{\infty+1/2}- \theta^{*})^{\otimes 2}]\] \[=\mathbb{E}[(\theta_{\infty}-\theta^{*})^{\otimes 2}]+ \alpha^{2}\big{(}\mathbb{E}[(g(\theta_{\infty},x_{\infty}))^{\otimes 2}]+ \mathbb{E}[(\xi_{\infty+1}(\theta_{\infty}))^{\otimes 2}]\big{)}\] \[+\alpha(\mathbb{E}[g(\theta_{\infty},x_{\infty})\otimes(\theta_{ \infty}-\theta^{*})]+\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes g(\theta_{ \infty},x_{\infty})])\] \[+\mathbb{E}[(\theta_{\infty+1}-\theta^{*})^{\otimes 2}-( \theta_{\infty+1/2}-\theta^{*})^{\otimes 2}].\]

Hence, by reorganizing the terms, we have

\[-\frac{1}{\alpha}\mathbb{E}[(\theta_{\infty+1}-\theta^{*})^{ \otimes 2}-(\theta_{\infty+1/2}-\theta^{*})^{\otimes 2}]\] \[=\alpha(\mathbb{E}[(g(\theta_{\infty},x_{\infty}))^{\otimes 2}]+ \mathbb{E}[(\xi_{\infty+1}(\theta_{\infty}))^{\otimes 2}])\] \[+(\mathbb{E}[g(\theta_{\infty},x_{\infty})\otimes(\theta_{ \infty}-\theta^{*})]+\mathbb{E}[(\theta_{\infty}-\theta^{*})\otimes g(\theta_{ \infty},x_{\infty})]).\]

Therefore, we turn our focus to analyzing \(\mathbb{E}[(\theta_{\infty+1}-\theta^{*})^{\otimes 2}-(\theta_{\infty+1/2}-\theta^{*})^{\otimes 2}]\). Similar as the first moment analysis, we have

\[\mathbb{E}[(\theta_{\infty+1}-\theta^{*})^{\otimes 2}-(\theta_{ \infty+1/2}-\theta^{*})^{\otimes 2}]\] \[=\mathbb{E}[((\theta_{\infty+1}-\theta^{*})^{\otimes 2}-(\theta_{ \infty+1/2}-\theta^{*})^{\otimes 2})\mathds{1}\{\|\theta_{t+1/2}-\theta^{*}\|\geq\beta \}].\]

To analyze the term on the right-hand side, we again make use of Holder's inequality and obtain

\[\|\mathbb{E}[((\theta_{\infty+1}-\theta^{*})^{\otimes 2}-( \theta_{\infty+1/2}-\theta^{*})^{\otimes 2})\mathds{1}\{\|\theta_{t+1/2}-\theta^{*}\| \geq\beta\}]\|\] \[\leq\mathbb{E}^{1/p}[\|(\theta_{\infty+1}-\theta^{*})^{\otimes 2}-( \theta_{\infty+1/2}-\theta^{*})^{\otimes 2}\|^{p}]\mathbb{E}^{1/q}[\mathds{1}\{\| \theta_{t+1/2}-\theta^{*}\|\geq\beta\}]\] \[\leq 2\mathbb{E}^{1/p}[\|\theta_{\infty+1/2}-\theta^{*}\|^{2p}] \big{(}\mathbb{P}(\|\theta_{t+1/2}-\theta^{*}\|\geq\beta)\big{)}^{1/q}.\]

Setting \(p=3\) and \(q=3/2\), and making use of the property that \(\mathbb{E}[\|\theta_{\infty+1/2}-\theta^{*}\|^{6}]=\mathcal{O}((\alpha\tau)^{3})\) from Proposition 4.2, we have

\[\mathbb{E}^{1/3}[\|\theta_{\infty+1/2}-\theta^{*}\|^{6}]\big{(} \mathbb{P}(\|\theta_{t+1/2}-\theta^{*}\|\geq R)\big{)}^{2/3} \lesssim(\alpha\tau)\Big{(}\mathbb{E}[\|\theta_{t+1/2}-\theta^{*}\|^{6}]/ R^{6}\Big{)}^{2/3}\] \[\lesssim(\alpha\tau)^{3}.\]

Hence, we can conclude that

\[\mathbb{E}[(\theta_{\infty+1}-\theta^{*})^{\otimes 2}-(\theta_{\infty+1/2}- \theta^{*})^{\otimes 2}]=\mathcal{O}((\alpha\tau)^{3}).\]From the analyses above, we can also conclude that

\[\|\mathbb{E}[\theta_{\infty+1}-\theta_{\infty+1/2}|x_{\infty}]\|_{L^ {2}(\pi)}=\mathcal{O}((\alpha\tau)^{3/2})\] \[\|\mathbb{E}[(\theta_{\infty+1}-\theta^{*})^{\otimes 2}-(\theta_{ \infty+1/2}-\theta^{*})^{\otimes 2}|x_{\infty}]\|_{L^{2}(\pi)}=\mathcal{O}(( \alpha\tau)^{3/2}).\]

Therefore, combining the analyses above, we see that the projection only introduces error terms of order \(\mathcal{O}(\alpha^{2}\tau^{3})\). Hence, combining the analysis from Section I.2.1, we can conclude the same desired order that

\[\mathbb{E}[\theta_{\infty}^{(\alpha)}-\theta^{*}]=\alpha b+\mathcal{O}(( \alpha\tau)^{3/2}).\]

## Appendix J Additional Insights on TA and RR

In this section, we present more detailed results that characterize the first and second moment of Polayk-Ruppert (PR) tail-averaged iterates and Richardson-Romberg (RR) extrapolated iterates.

The following corollary provides non-asymptotic characterization for the first two moments of PR tail-averaged iterates \(\bar{\theta}_{k_{0},k}\).

**Corollary J.1** (Tail Averaging).: _Under the setting of Theorem 4.6, the tail-averaged iterates satisfy the following bounds for all \(k>k_{0}+2\tau\) and \(k_{0}\geq\tau+\frac{1}{\alpha\mu}\log\big{(}\frac{1}{\alpha\tau_{\alpha}}\big{)}\):_

\[\mathbb{E}[\bar{\theta}_{k_{0},k}^{(\alpha)}-\theta^{*}] =\alpha b+\mathcal{O}\big{(}(\alpha\tau_{\alpha})^{3/2}\big{)}+ \mathcal{O}\bigg{(}\frac{(1-\alpha\mu)^{k_{0}/2}}{\alpha(k-k_{0})}\bigg{)} \quad\text{and}\] (J.1) \[\mathbb{E}\Big{[}(\bar{\theta}_{k_{0},k}^{(\alpha)}-\theta^{*})( \bar{\theta}_{k_{0},k}^{(\alpha)}-\theta^{*})^{\top}\Big{]} =\alpha^{2}bb^{T}+\mathcal{O}(\alpha\cdot(\alpha\tau_{\alpha})^{3/ 2})+\mathcal{O}\bigg{(}\frac{\tau_{\alpha}}{k-k_{0}}+\frac{(1-\alpha\mu)^{k_{0 }/2}}{\alpha\left(k-k_{0}\right)^{2}}\bigg{)}.\] (J.2)

With this result, taking the trace on both sides of (J.2) recovers Corollary 4.7.

Proof.: First, we have

\[\mathbb{E}[\bar{\theta}_{k_{0},k}^{(\alpha)}-\theta^{*}]=(\mathbb{E}\left[ \theta_{\infty}\right]-\theta^{*})+\frac{1}{k-k_{0}}\sum_{t=k_{0}}^{k-1} \mathbb{E}\left[\theta_{t}-\theta_{\infty}\right].\]

By Corollary 4.4, we obtain

\[\|\mathbb{E}[\theta_{t}]-\mathbb{E}[\theta_{\infty}]\|\leq(1-\alpha\mu)^{ \frac{t}{2}}\cdot s^{\prime}(\theta_{0},L,\mu).\]

Hence, it follows that

\[\left\|\sum_{t=k_{0}}^{k-1}\mathbb{E}\left[\theta_{t}-\theta_{ \infty}\right]\right\| \leq\sum_{t=k_{0}}^{k-1}\|\mathbb{E}\left[\theta_{t}\right]- \mathbb{E}[\theta_{\infty}]\|\] \[\leq s^{\prime}(\theta_{0},L,\mu)\cdot(1-\alpha\mu)^{\frac{k_{0}} {2}}\frac{1}{1-\sqrt{(1-\alpha\mu)}}\] \[\leq s^{\prime}(\theta_{0},L,\mu)\cdot(1-\alpha\mu)^{\frac{k_{0} }{2}}\frac{2}{\alpha\mu}.\]

Together with Theorem 4.6, we have

\[\mathbb{E}\left[\bar{\theta}_{k_{0},k}^{(\alpha)}\right]-\theta^{*}=\alpha b+ \mathcal{O}\big{(}(\alpha\tau_{\alpha})^{3/2}\big{)}+\mathcal{O}\left(\frac{ (1-\alpha\mu)^{k_{0}/2}}{\alpha(k-k_{0})}\right),\]

thereby finishing the proof of the first moment.

To bound the second moment of the tail-averaged iterate, we follow the proof technique in [33, Section A.6.2]. We notice that

\[\mathbb{E}\left[\left(\bar{\theta}_{k_{0},k}^{(\alpha)}-\theta^{*} \right)\left(\bar{\theta}_{k_{0},k}^{(\alpha)}-\theta^{*}\right)^{\top}\right]\] \[= \mathbb{E}\left[\left(\bar{\theta}_{k_{0},k}^{(\alpha)}-\mathbb{E }\left[\theta_{\infty}\right]+\mathbb{E}\left[\theta_{\infty}\right]-\theta^{* }\right)\left(\bar{\theta}_{k_{0},k}^{(\alpha)}-\mathbb{E}\left[\theta_{ \infty}\right]+\mathbb{E}\left[\theta_{\infty}\right]-\theta^{*}\right)^{\top}\right]\] \[= \underbrace{\mathbb{E}\left[\left(\bar{\theta}_{k_{0},k}^{( \alpha)}-\mathbb{E}\left[\theta_{\infty}\right]\right)\left(\bar{\theta}_{k_{0 },k}^{(\alpha)}-\mathbb{E}\left[\theta_{\infty}\right]\right)^{\top}\right]}_ {T_{1}}+\underbrace{\mathbb{E}\left[\left(\bar{\theta}_{k_{0},k}^{(\alpha)}- \mathbb{E}\left[\theta_{\infty}\right]\right)\left(\mathbb{E}\left[\theta_{ \infty}\right]-\theta^{*}\right)^{\top}\right]}_{T_{2}}\] \[+\underbrace{\mathbb{E}\left[\left(\mathbb{E}\left[\theta_{ \infty}\right]-\theta^{*}\right)\left(\bar{\theta}_{k_{0},k}^{(\alpha)}- \mathbb{E}\left[\theta_{\infty}\right]\right)^{\top}\right]}_{T_{3}}+ \underbrace{\mathbb{E}\left[\left(\mathbb{E}\left[\theta_{\infty}\right]- \theta^{*}\right)\left(\mathbb{E}\left[\theta_{\infty}\right]-\theta^{*} \right)^{\top}\right]}_{T_{4}}.\]

For \(T_{2}\), we have

\[T_{2} =\frac{1}{k-k_{0}}\left(\sum_{t=k_{0}}^{k-1}\mathbb{E}\left[ \theta_{t}-\theta_{\infty}\right]\right)\left(\mathbb{E}[\theta_{\infty}]- \theta^{*}\right)^{\top}\] \[=\mathcal{O}\left(\frac{(1-\alpha\mu)^{k_{0}/2}}{\alpha(k-k_{0}) }\right)\cdot(\alpha b+\mathcal{O}\big{(}(\alpha\tau_{\alpha})^{3/2}\big{)})= \mathcal{O}\left(\frac{(1-\alpha\mu)^{k_{0}/2}}{(k-k_{0})}\right).\]

The term \(T_{3}\) is similar to \(T_{2}\) and obeys the same bound.

For \(T_{4}\), we have

\[T_{1}= \frac{1}{(k-k_{0})^{2}}\mathbb{E}\bigg{[}\Big{(}\sum_{t=k_{0}}^{ k-1}\big{(}\theta_{t}-\mathbb{E}[\theta_{\infty}]\big{)}\Big{)}\Big{(}\sum_{t=k_{0} }^{k-1}\big{(}\theta_{t}-\mathbb{E}[\theta_{\infty}]\big{)}\Big{)}^{\top} \bigg{]}\] \[= \frac{1}{(k-k_{0})^{2}}\sum_{t=k_{0}}^{k-1}\mathbb{E}\left[\big{(} \theta_{t}-\mathbb{E}[\theta_{\infty}]\big{)}\big{(}\theta_{t}-\mathbb{E}[ \theta_{\infty}]\big{)}^{\top}\right]\] (J.3) \[+\frac{1}{(k-k_{0})^{2}}\sum_{t=k_{0}}^{k-1}\sum_{l=t+1}^{k-1} \mathbb{E}\left[\big{(}\theta_{t}-\mathbb{E}[\theta_{\infty}]\big{)}\big{(} \theta_{t}-\mathbb{E}[\theta_{\infty}]\big{)}^{\top}\right]\] (J.4) \[+\frac{1}{(k-k_{0})^{2}}\sum_{t=k_{0}}^{k-1}\sum_{l=t+1}^{k-1} \mathbb{E}\left[\big{(}\theta_{t}-\mathbb{E}[\theta_{\infty}]\big{)}\big{(} \theta_{t}-\mathbb{E}[\theta_{\infty}]\big{)}^{\top}\right].\] (J.5)

By Corollary 4.4 and Proposition 4.2 we have

\[\mathbb{E}\Big{[}\big{(}\theta_{t}-\mathbb{E}[\theta_{\infty}] \big{)}\big{(}\theta_{t}-\mathbb{E}[\theta_{\infty}]\big{)}^{\top}\Big{]}\] \[= \Big{(}\mathbb{E}\big{[}\theta_{t}\theta_{t}^{\top}\big{]}- \mathbb{E}\big{[}\theta_{\infty}\theta_{\infty}{}^{\top}\big{]}\Big{)}+ \Big{(}\mathbb{E}\big{[}\theta_{\infty}\theta_{\infty}{}^{\top}\big{]}- \mathbb{E}[\theta_{\infty}]\mathbb{E}\big{[}\theta_{\infty}{}^{\top}\big{]} \Big{)}\] \[= \Big{(}\mathbb{E}\left[\theta_{t}\theta_{t}^{\top}\big{]}- \mathbb{E}\big{[}\theta_{\infty}\theta_{\infty}{}^{\top}\big{]}\Big{)}+ \mathrm{Var}\left(\theta_{\infty}\right)-\mathbb{E}\big{[}\theta_{t}-\theta_{ \infty}\big{]}\mathbb{E}\big{[}\theta_{\infty}{}^{\top}\big{]}-\mathbb{E}[ \theta_{\infty}]\mathbb{E}\Big{[}(\theta_{t}-\theta_{\infty})^{\top}\Big{]}\] \[= \mathcal{O}\left((1-\alpha\mu)^{\frac{t}{2}}+\alpha\tau_{\alpha} \right),\]

where we bound \(\mathrm{Var}\left(\theta_{\infty}\right)\) with \(\mathcal{O}(\alpha\tau_{\alpha})\) by Proposition 4.2 and Fatou's lemma.

Then, for (J.3), we have

(J.3) \[=\mathcal{O}\bigg{(}\frac{1}{{{{(k-{k_{0}})}^{2}}}}\sum_{t=k_{0}}^{ \infty}{{{(1-\alpha\mu)}^{\frac{t}{2}}}}\bigg{)}+\mathcal{O}\bigg{(}\frac{ \alpha\tau_{\alpha}}{k-k_{0}}\bigg{)}\] \[=\mathcal{O}\bigg{(}\frac{{{{(1-\alpha\mu)}^{{{k_{0}}}/{2}}}}}{ \alpha\left({k-{k_{0}}}\right)^{2}}+\frac{\alpha\tau_{\alpha}}{k-k_{0}}\bigg{)}.\]

We restate the following claim, whose proof closely resembles Claim 4 in [33].

**Claim 2**.: _For \(t\geq\tau+\frac{1}{{{\alpha\mu}}}\log\left(\frac{1}{{{\alpha\tau_{\alpha}}}}\right)\) and \(l\geq t+2\tau_{\alpha}\), we have_

\[\left\|\mathbb{E}\Big{[}\big{(}\theta_{t}-\mathbb{E}[\theta^{(\alpha)}]\big{)} \big{(}\theta_{l}-\mathbb{E}[\theta^{(\alpha)}]\big{)}^{\top}\Big{]}\right\| =\mathcal{O}\left({{{(\alpha\tau_{\alpha})}\cdot{{(1-\alpha\mu)}^{\frac{{{(l-t)} }}{2}}}}}\right).\]

Then, by [33, Claim 4], we have term (J.4) \(=\mathcal{O}\big{(}\frac{\tau_{\alpha}}{k-k_{0}}\big{)}\). Similarly, we have term (J.5)\(=\mathcal{O}\big{(}\frac{\tau_{\alpha}}{k-k_{0}}\big{)}\). Therefore, we have

\[T_{1}=\mathcal{O}\bigg{(}\frac{{{(1-\alpha\mu)}^{{{k_{0}}}/{2}}}}{ \alpha\left({k-{k_{0}}}\right)^{2}}\bigg{)}+\frac{\tau_{\alpha}}{k-k_{0}} \bigg{)}.\] (J.6)

By adding \(T_{1}\)-\(T_{4}\) together, we obtain

\[\mathbb{E}\left[\left(\tilde{\theta}_{k_{0},k}^{(\alpha)}-\theta ^{*}\right)\left(\bar{\theta}_{k_{0},k}^{(\alpha)}-\theta^{*}\right)^{\top}\right]= \alpha^{2}bb^{T}+\mathcal{O}(\alpha\cdot(\alpha\tau_{\alpha})^{3/ 2})+\mathcal{O}\bigg{(}\frac{{{(1-\alpha\mu)}^{{{k_{0}}}/{2}}}}{{{(k-{k_{0}} )}}}\bigg{)}\] \[+\mathcal{O}\bigg{(}\frac{{{(1-\alpha\mu)}^{{{k_{0}}}/{2}}}}{ \alpha\left({k-{k_{0}}}\right)^{2}}+\frac{\tau_{\alpha}}{k-k_{0}}\bigg{)}\] \[= \alpha^{2}bb^{T}+\mathcal{O}(\alpha\cdot(\alpha\tau_{\alpha})^{3/ 2})+\mathcal{O}\bigg{(}\frac{\tau_{\alpha}}{k-k_{0}}+\frac{{{(1-\alpha\mu)}^{ {{k_{0}}}/{2}}}}{\alpha\left({k-{k_{0}}}\right)^{2}}\bigg{)}.\]

Next, we present the following corollary formalizes the non-asymptotic characterization for the first two moments of the RR-extrapolated iterate \(\widetilde{\theta}_{k_{0},k}^{(\alpha)}\).

**Corollary J.2** (Richardson-Romberg Extrapolation).: _Under the setting of Theorem 4.6, the RR extrapolated iterates with stepsizes \(\alpha\) and \(2\alpha\) satisfy the following bounds for all \(k>{{k_{0}}}+2\tau_{\alpha}\) and \(k_{0}\geq\tau_{\alpha}+\frac{1}{{{\alpha\mu}}}\log\big{(}\frac{1}{{{\alpha \tau_{\alpha}}}}\big{)}\):_

\[\mathbb{E}[\tilde{\theta}_{k_{0},k}^{(\alpha)}-\theta^{*}] =\mathcal{O}\big{(}(\alpha\tau_{\alpha})^{3/2}\big{)}+\mathcal{O }\Big{(}\frac{{{(1-\alpha\mu)}^{{{k_{0}}}/{2}}}}{\alpha(k-{k_{0}})}\big{)}, \quad\text{and}\] (J.7) \[\mathbb{E}\Big{[}(\widetilde{\theta}_{k_{0},k}^{(\alpha)}-\theta ^{*})(\tilde{\theta}_{k_{0},k}-\theta^{*})^{\top}\Big{]} =\mathcal{O}\big{(}(\alpha\tau_{\alpha})^{3}\big{)}+\mathcal{O }\Big{(}\frac{{{\tau_{\alpha}}}}{k-{k_{0}}}+\frac{{{(1-\alpha\mu)}^{{{k_{0}}}/ {2}}}}{\alpha\left({k-{k_{0}}}\right)^{2}}\Big{)}.\] (J.8)

Proof.: By equation (J.1), we obtain

\[\mathbb{E}\left[\tilde{\theta}_{k_{0},k}^{(\alpha)}\right]-\theta ^{*}= \mathbb{E}\left[2\bar{\theta}_{k_{0},k}^{(\alpha)}-\bar{\theta}_{ k_{0},k}^{(2\alpha)}\right]-\theta^{*}=2\mathbb{E}\left[\bar{\theta}_{k_{0},k}^{( \alpha)}-\theta^{*}\right]-\mathbb{E}\left[\bar{\theta}_{k_{0},k}^{(2\alpha)}- \theta^{*}\right]\] \[= 2\left(\alpha b+\mathcal{O}\big{(}(\alpha\tau_{\alpha})^{3/2} \big{)}+\mathcal{O}\left(\frac{{{(1-\alpha\mu)}^{{{k_{0}}}/{2}}}}{\alpha(k-{k_ {0}})}\right)\right)\] \[-\left(2\alpha b+\mathcal{O}\big{(}(2\alpha\tau_{2\alpha})^{3/2} \big{)}+\mathcal{O}\left(\frac{{{(1-2\alpha\mu)}^{{{k_{0}}}/{2}}}}{\alpha(k-{k_ {0}})}\right)\right)\] \[= \mathcal{O}\big{(}(\alpha\tau_{\alpha})^{3/2}\big{)}+\mathcal{O} \left(\frac{{{(1-\alpha\mu)}^{{{k_{0}}}/{2}}}}{\alpha(k-{k_{0}})}\right).\]Let \(u_{1}:=\bar{\theta}_{k_{0},k}^{(\alpha)}-\mathbb{E}\left[\theta_{\infty}^{(\alpha)}\right]\), \(u_{2}:=\bar{\theta}_{k_{0},k}^{(2\alpha)}-\mathbb{E}\left[\theta_{\infty}^{(2 \alpha)}\right]\) and \(v:=2\mathbb{E}\left[\theta_{\infty}^{(\alpha)}\right]-\mathbb{E}\left[\theta_ {\infty}^{(2\alpha)}\right]-\theta^{*}\).

With these notations, \(\tilde{\theta}_{k_{0},k}-\theta^{*}=2u_{1}-u_{2}+v\). We then have the following bound

\[\left\|\mathbb{E}\left[\left(\tilde{\theta}_{k_{0},k}^{(\alpha)}- \theta^{*}\right)\left(\tilde{\theta}_{k_{0},k}^{(\alpha)}-\theta^{*}\right)^ {\top}\right]\right\|\leq \mathbb{E}\left[\left\|2u_{1}-u_{2}+v\right\|^{2}\right]\] \[\leq 3\mathbb{E}\left\|2u_{1}\right\|^{2}+3\mathbb{E}\left\|u_{2} \right\|^{2}+3\|v\|^{2}.\]

By equation (I.6), we have

\[\mathbb{E}\left\|u_{1}\right\|^{2}=\operatorname{Tr}\left(\mathbb{E}\left[u_ {1}u_{1}^{\top}\right]\right)=\mathcal{O}\left(\frac{(1-\alpha\mu)^{k_{0}/2}} {\alpha\left(k-k_{0}\right)^{2}}+\frac{\tau_{\alpha}}{k-k_{0}}\right).\]

Similarly, we have

\[\mathbb{E}\left\|u_{2}\right\|_{2}^{2}=\mathcal{O}\left(\frac{(1-2\alpha\mu)^ {k_{0}/2}}{\alpha\left(k-k_{0}\right)^{2}}+\frac{\tau_{2\alpha}}{k-k_{0}} \right).\]

By Theorem 4.6, we have \(\|v\|_{2}^{2}=\mathcal{O}\big{(}(\alpha\tau_{\alpha})^{3}\big{)}\).

Combining these bounds, we have

\[\mathbb{E}\Big{[}\big{(}\tilde{\theta}_{k_{0},k}-\theta^{*}\big{)}\big{(} \tilde{\theta}_{k_{0},k}-\theta^{*}\big{)}^{\top}\Big{]}=\mathcal{O}\big{(}( \alpha\tau_{\alpha})^{3}\big{)}+\mathcal{O}\Big{(}\frac{\tau_{\alpha}}{k-k_{0} }+\frac{(1-\alpha\mu)^{k_{0}/2}}{\alpha\left(k-k_{0}\right)^{2}}\Big{)}.\]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The main claims made in the abstract and introduction accurately reflect the paper's contributions and scope. The main results are elaborately discussed in Section 4. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 7. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: The assumptions for this paper are stated in Section 2 and Section 4, with a more detailed discussion provided in Appendix B. Complete proofs detailed in Appendices D-J.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The experiment details are fully described in Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The code is publicly available at https://github.com/lucyhuodongyan/nonlinear-sa-bias. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The experiment details are fully described in Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Given the experiment set-up, error bars are omitted for the convergence plots in Figure 1, because the focus is on the asymptotic bias, to which Polyak-Ruppert averaged iterates converge almost surely. As the bias approaches its limit, variability decreases, making error bars unnecessary. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The computation resources are disclosed in Section 5. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: The research conducted in the paper conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: This paper is theoretical and has no potential negative social impacts. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: This paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: This paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: This paper does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.

* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.