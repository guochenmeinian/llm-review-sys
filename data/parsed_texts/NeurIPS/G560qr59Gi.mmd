Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU Networks on Nearly-orthogonal Data

 Yiwen Kou

Zixiang Chen1

Quanquan Gu

Department of Computer Science

University of California, Los Angeles

Los Angeles, CA 90095

{evankou,chenzx19,qgu}@cs.ucla.edu

Equal contribution37th Conference on Neural Information Processing Systems (NeurIPS 2023).

###### Abstract

The implicit bias towards solutions with favorable properties is believed to be a key reason why neural networks trained by gradient-based optimization can generalize well. While the implicit bias of gradient flow has been widely studied for homogeneous neural networks (including ReLU and leaky ReLU networks), the implicit bias of gradient descent is currently only understood for smooth neural networks. Therefore, implicit bias in non-smooth neural networks trained by gradient descent remains an open question. In this paper, we aim to answer this question by studying the implicit bias of gradient descent for training two-layer fully connected (leaky) ReLU neural networks. We showed that when the training data are nearly-orthogonal, for leaky ReLU activation function, gradient descent will find a network with a stable rank that converges to \(1\), whereas for ReLU activation function, gradient descent will find a neural network with a stable rank that is upper bounded by a constant. Additionally, we show that gradient descent will find a neural network such that all the training data points have the same normalized margin asymptotically. Experiments on both synthetic and real data backup our theoretical findings.

## 1 Introduction

Neural networks have achieved remarkable success in a variety of applications, such as image and speech recognition, natural language processing, and many others. Recent studies have revealed that the effectiveness of neural networks is attributed to their implicit bias towards particular solutions which enjoy favorable properties. Understanding how this bias is affected by factors such as network architecture, optimization algorithms and data used for training, has become an active research area in the field of deep learning theory.

The literature on the implicit bias in neural networks has expanded rapidly in recent years (Vardi, 2022), with numerous studies shedding light on the implicit bias of gradient flow (GF) with a wide range of neural network architecture, including deep linear networks (Ji and Telgarsky, 2018, 2020; Gunasekar et al., 2018), homogeneous networks (Lyu and Li, 2019; Vardi et al., 2022a) and more specific cases (Chizat and Bach, 2020; Lyu et al., 2021; Frei et al., 2022b; Safran et al., 2022). The implicit bias of gradient descent (GD), on the other hand, is better understood for linear predictors (Soudry et al., 2018) and smoothed neural networks (Lyu and Li, 2019; Frei et al., 2022b). Therefore, an open question still remains:

_What is the implicit bias of leaky ReLU and ReLU networks trained by gradient descent?_

In this paper, we will answer this question by investigating gradient descent for both two-layer leaky ReLU and ReLU neural networks on specific training data, where \(\{\mathbf{x}_{i}\}_{i=1}^{n}\) are nearly-orthogonal(Frei et al., 2022b), i.e., \(\|\mathbf{x}_{i}\|_{2}^{2}\geq Cn\max_{k\neq i}|\langle\mathbf{x}_{i},\mathbf{x} _{k}\rangle|\) with a constant \(C\). Our main results are summarized as follows:

* For two-layer leaky ReLU networks trained by GD, we demonstrate that the neuron activation pattern reaches a stable state beyond a specific time threshold and provide rigorous proof of the convergence of the stable rank of the weight matrix to 1, matching the results of Frei et al. (2022b) regarding gradient flow.
* For two-layer ReLU networks trained by GD, we proved that the stable rank of weight matrix can be upper bounded by a constant. Moreover, we present an illustrative example using completely orthogonal training data, showing that the stable rank of the weight matrix converges to a value approximately equal to \(2\). To the best of our knowledge, this is the first implicit bias result for two-layer ReLU networks trained by gradient descent beyond the Karush-Kuhn-Tucker (KKT) point.
* For both ReLU and leaky ReLU networks, we show that weight norm increases at the rate of \(\Theta(\log(t))\) and the training loss converges to zero at the rate of \(\Theta(t^{-1})\), where \(t\) is the number of gradient descent iterations. This improves upon the \(O(t^{-1/2})\) rate proved in Frei et al. (2022b) for the case of a two-layer _smoothed_ leaky ReLU network trained by gradient descent and aligns with the results by Lyu and Li (2019) for smooth homogeneous networks. Additionally, we prove that gradient descent will find a neural network such that all the training data points have the same normalized margin asymptotically.

## 2 Related Work

Implicit bias in neural networks.Recent years have witnessed significant progress on implicit bias in neural networks trained by gradient flow (GF). Lyu and Li (2019) and Ji and Telgarsky (2020) demonstrated that homogeneous neural networks trained with exponentially-tailed classification losses converge in direction to the KKT point of a maximum-margin problem. Lyu et al. (2021) studied the implicit bias in two-layer leaky ReLU networks trained on linearly separable and symmetric data, showing that GF converges to a linear classifier maximizing the \(\ell_{2}\) margin. Frei et al. (2022b) showed that two-layer leaky ReLU networks trained by GF on nearly-orthogonal data produce a \(\ell_{2}\)-max-margin solution with a linear decision boundary and rank at most two. Other works studying the implicit bias of classification using GF in nonlinear two-layer networks include Chizat and Bach (2020); Phuong and Lampert (2021); Sarussi et al. (2021); Safran et al. (2022); Vardi et al. (2022a,b); Timor et al. (2023). Although implicit bias in neural networks trained by GF has been extensively studied, research on implicit bias in networks trained by gradient descent (GD) remains limited. Lyu and Li (2019) examined smoothed homogeneous neural network trained by GD with exponentially-tailed losses and proved a convergence to KKT points of a max-margin problem. Frei et al. (2022b) studied two-layer smoothed leaky ReLU trained by GD and revealed the implicit bias towards low-rank networks. Other works studying implicit bias towards rank minimization include Ji and Telgarsky (2018, 2020); Timor et al. (2023); Arora et al. (2019); Razin and Cohen (2020); Li et al. (2021). Lastly, Vardi (2022) provided a comprehensive literature survey on implicit bias.

Benign overfitting and double descent in neural networks.A parallel line of research aims to understand the benign overfitting phenomenon (Bartlett et al., 2020) of neural networks by considering a variety of models. For example, Allen-Zhu and Li (2020); Jelassi and Li (2022); Shen et al. (2022); Cao et al. (2022); Kou et al. (2023) studied the generalization performance of two-layer convolutional networks on patch-based data models. Several other papers studied high-dimensional mixture models (Chatterji and Long, 2021; Wang and Thrampoulidis, 2022; Cao et al., 2021; Frei et al., 2022a). Another thread of work Belkin et al. (2020); Hastie et al. (2022); Wu and Xu (2020); Mei and Montanari (2019); Liao et al. (2020) focuses on understanding the double descent phenomenon first empirically observed by Belkin et al. (2019).

## 3 Preliminaries

In this section, we introduce the notation, fully connected neural networks, the gradient descent-based training algorithm, and a data-coorrelated decomposition technique.

Notation.We use lower case letters, lower case bold face letters, and upper case bold face letters to denote scalars, vectors, and matrices respectively. For a vector \(\mathbf{v}=(v_{1},\cdots,v_{d})^{\top}\), we denote by \(\|\mathbf{v}\|_{2}:=\left(\sum_{j=1}^{d}v_{j}^{2}\right)^{1/2}\) its \(\ell_{2}\) norm. For a matrix \(\mathbf{A}\in\mathbb{R}^{m\times n}\), we use \(\|\mathbf{A}\|_{F}\) to denote its Frobenius norm and \(\|\mathbf{A}\|_{2}\) its spectral norm. We use \(\mathrm{sign}(z)\) as the function that is \(1\) when \(z>0\) and \(-1\) otherwise. For a vector \(\mathbf{v}\in\mathbb{R}^{d}\), we use \([\mathbf{v}]_{i}\in\mathbb{R}\) to denote the \(i\)-th component of the vector. For two sequence \(\{a_{k}\}\) and \(\{b_{k}\}\), we denote \(a_{k}=O(b_{k})\) if \(|a_{k}|\leq C|b_{k}|\) for some absolute constant \(C\), denote \(a_{k}=\Omega(b_{k})\) if \(b_{k}=O(a_{k})\), and denote \(a_{k}=\Theta(b_{k})\) if \(a_{k}=O(b_{k})\) and \(a_{k}=\Omega(b_{k})\). We also denote \(a_{k}=o(b_{k})\) if \(\lim|a_{k}/b_{k}|=0\).

Two-layer fully connected neural network.We consider a two-layer neural network described as follows: its first layer consists of \(m\) positive neurons and \(m\) negative neurons; its second layer parameters are fixed as \(+1/m\) and \(-1/m\) respectively for positive and negative neurons. Then the network can be written as \(f(\mathbf{W},\mathbf{x})=F_{+1}(\mathbf{W}_{+1},\mathbf{x})-F_{-1}(\mathbf{W} _{-1},\mathbf{x})\), where the partial network function of positive and negative neurons, i.e., \(F_{+1}(\mathbf{W}_{+1},\mathbf{x})\), \(F_{-1}(\mathbf{W}_{-1},\mathbf{x})\), are defined as:

\[F_{j}(\mathbf{W}_{j},\mathbf{x})=\frac{1}{m}{\sum_{r=1}^{m}} \sigma(\langle\mathbf{w}_{j,r},\mathbf{x}\rangle)\] (3.1)

for \(j\in\{\pm 1\}\). Here, \(\sigma(z)\) represents the activation function. For ReLU, \(\sigma(z)=\max\{0,z\}\), and for leaky ReLU, \(\sigma(z)=\max\{\gamma z,z\}\), where \(\gamma\in(0,1)\). \(\mathbf{W}_{j}\in\mathbb{R}^{m\times d}\) is the collection of model weights associated with \(F_{j}\), and \(\mathbf{w}_{j,r}\in\mathbb{R}^{d}\) denotes the weight vector for the \(r\)-th neuron in \(\mathbf{W}_{j}\). We use \(\mathbf{W}\) to denote the collection of all model weights.

Gradient Descent.Given a training data set \(\mathcal{S}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}\subseteq\mathbb{R}^{d} \times\{\pm 1\}\), instead of considering the gradient flow (GF) that is commonly studied in prior work on the implicit bias, we use gradient descent (GD) to optimize the empirical loss on the training data

\[L_{S}(\mathbf{W})=\frac{1}{n}\sum_{i=1}^{n}\ell(y_{i}\cdot f( \mathbf{W},\mathbf{x}_{i})),\]

where \(\ell(z)=\log(1+\exp(-z))\) is the logistic loss, and \(S=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}\) is the training data set. The gradient descent update rule of each neuron in the two-layer neural network can be written as

\[\mathbf{w}_{j,r}^{(t+1)}=\mathbf{w}_{j,r}^{(t)}-\eta\cdot\nabla_{ \mathbf{w}_{j,r}}L_{S}(\mathbf{W}^{(t)})=\mathbf{w}_{j,r}^{(t)}-\frac{\eta}{ nm}\sum_{i=1}^{n}\ell_{i}^{\prime(t)}\cdot\sigma^{\prime}(\langle\mathbf{w}_{j,r}^{(t )},\mathbf{x}_{i}\rangle)\cdot jy_{i}\mathbf{x}_{i}\] (3.2)

for all \(j\in\{\pm 1\}\) and \(r\in[m]\), where we introduce a shorthand notation \(\ell_{i}^{\prime(t)}=\ell^{\prime}[y_{i}\cdot f(\mathbf{W}^{(t)},\mathbf{x}_{i })]\) and assume the derivative of the ReLU activation function at 0 is \(\sigma^{\prime}(0)=1\) without loss of generality. Here \(\eta>0\) is the learning rate. We initialize the gradient descent by Gaussian initialization, where all the entries of \(\mathbf{W}^{(0)}\) are sampled from i.i.d. Gaussian distributions \(\mathcal{N}(0,\sigma_{0}^{2})\) with \(\sigma_{0}^{2}\) being the variance.

## 4 Main Results

In this section, we present our main theoretical results. For the training data set \(\mathcal{S}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{n}\subseteq\mathbb{R}^{d}\times \{\pm 1\}\), let \(R_{\min}=\min_{i}\|\mathbf{x}_{i}\|_{2}\), \(R_{\max}=\max_{i}\|\mathbf{x}_{i}\|_{2}\), \(p=\max_{i\neq k}|\langle\mathbf{x}_{i},\mathbf{x}_{k}\rangle|\), and suppose \(R=R_{\max}/R_{\min}\) is at most an absolute constant. For simplicity, we only consider the dependency on \(t\) when characterizing the convergence rates of the weight matrix related quantities and the training loss, omitting the dependency on other parameters such as \(m,n,\sigma_{0},R_{\min},R_{\max}\).

**Theorem 4.1** (Leaky ReLU Networks).: For two-layer neural network defined in (3.1) with leaky ReLU activation \(\sigma(z)=\max\{\gamma z,z\},\gamma\in(0,1)\). Assume the training data satisfy \(R_{\min}^{2}\geq CR^{2}\gamma^{-4}np\) for some sufficiently large constant \(C\). For any \(\delta\in(0,1)\), if the learning rate \(\eta\leq(CR_{\max}^{2}/nm)^{-1}\) and the initialization scale \(\sigma_{0}\leq\gamma\big{(}CR_{\max}\sqrt{\log(mn/\delta)}\big{)}^{-1}\), then with probability at least \(1-\delta\) over the random initialization of gradient descent, the trained network satisfies:

* The \(\ell_{2}\) norm of each neuron increases to infinity at a logarithmic rate: \(\|\mathbf{w}_{j,r}^{(t)}\|_{2}=\Theta(\log(t))\) for all \(j\in\{\pm 1\}\) and \(r\in[m]\).

* Throughout the gradient descent trajectory, the stable rank of the weights \(\mathbf{W}_{j}^{(t)}\) for all \(j\in\{\pm 1\}\) satisfies \[\lim_{t\to\infty}\|\mathbf{W}_{j}^{(t)}\|_{F}^{2}/\|\mathbf{W}_{j}^{(t)}\|_{2}^ {2}=1,\] with a convergence rate of \(O(1/\log(t))\).
* Gradient descent will find \(\mathbf{W}^{(t)}\) such that all the training data points possess the same normalized margin asymptotically: \[\lim_{t\to\infty}\big{|}y_{i}f(\mathbf{W}^{(t)}/\|\mathbf{W}^{(t)}\|_{F}, \mathbf{x}_{i})-y_{k}f(\mathbf{W}^{(t)}/\|\mathbf{W}^{(t)}\|_{F},\mathbf{x}_{ k})\big{|}=0,\,\forall i,k\in[n].\] If we assume that \(\mathbf{W}^{(t)}\) converges in direction, i.e., the limit of \(\mathbf{W}^{(t)}/\|\mathbf{W}^{(t)}\|_{F}\) exists, denoted by \(\bar{\mathbf{W}}\), then there exists a scaling factor \(\alpha>0\) such that \(\alpha\bar{\mathbf{W}}\) satisfies the Karush-Kuhn-Tucker (KKT) conditions for the following max-margin problem: \[\min_{\mathbf{W}}\frac{1}{2}\|\mathbf{W}\|_{F}^{2},\qquad\text{s.t.}\qquad y _{i}f(\mathbf{W},\mathbf{x}_{i})\geq 1,\,\forall i\in[n].\] (4.1)
* The empirical loss converges to zero at the following rate: \(L_{S}(\mathbf{W}^{(t)})=\Theta(t^{-1})\).

**Remark 4.2**.: In Theorem 4.1, we show that when using the leaky ReLU activation function on nearly orthogonal training data, gradient descent asymptotically finds a network with a stable rank of \(\mathbf{W}_{j}\) equal to \(1\). Additionally, we demonstrate that gradient descent will find a network by which all the training data points share the same normalized margin asymptotically. Moreover, if we assume the weight matrix converges in direction, then its limit will satisfy the KKT conditions of the max-margin problem (4.1). Furthermore, we analyze the rate of weight norm increase and the convergence rate of the stable rank for gradient descent, both of which exhibit a logarithmic dependency in \(t\).

**Theorem 4.3** (ReLU Networks).: For two-layer neural network defined in (3.1) with ReLU activation \(\sigma(z)=\max\{0,z\}\). Assume the training data satisfy \(R_{\min}^{2}\geq CR^{2}np\) for some sufficiently large constant \(C\). For any \(\delta\in(0,1)\), if the neural network width \(m\geq C\log(n/\delta)\), learning rate \(\eta\leq(CR_{\max}^{2}/nm)^{-1}\) and initialization scale \(\sigma_{0}\leq\big{(}CR_{\max}\sqrt{\log(mn/\delta)}\big{)}^{-1}\), then with probability at least \(1-\delta\) over the random initialization of gradient descent, the trained network satisfies:

* The Frobenious norm and the spectral norm of weight matrix increase to infinity at a logarithmic rate: \(\|\mathbf{W}_{j}^{(t)}\|_{F}=\Theta(\log(t))\) and \(\|\mathbf{W}_{j}^{(t)}\|_{2}=\Theta(\log(t))\) for all \(j\in\{\pm 1\}\).
* Throughout the gradient descent trajectory, the stable rank of the weights \(\mathbf{W}_{j}^{(t)}\) for all \(j\in\{\pm 1\}\) satisfies, \[\limsup_{t\to\infty}\|\mathbf{W}_{j}^{(t)}\|_{F}^{2}/\|\mathbf{W}_{j}^{(t)}\|_ {2}^{2}\leq c,\] where \(c\) is an absolute constant.
* Gradient descent will find a \(\mathbf{W}^{(t)}\) such that all the training data points possess the same normalized margin asymptotically: \[\lim_{t\to\infty}\big{|}y_{i}f(\mathbf{W}^{(t)}/\|\mathbf{W}^{(t)}\|_{F}, \mathbf{x}_{i})-y_{k}f(\mathbf{W}^{(t)}/\|\mathbf{W}^{(t)}\|_{F},\mathbf{x}_{ k})\big{|}=0,\,\forall i,k\in[n].\]
* The empirical loss converges to zero at the following rate: \(L_{S}(\mathbf{W}^{(t)})=\Theta(t^{-1})\).

**Remark 4.4**.: For ReLU networks, we provide an example in the appendix concerning fully orthogonal training data and prove that the activation pattern during training depends solely on the initial activation state. Specifically, when training a two-layer ReLU network with gradient descent using such data, the stable rank of the network's weight matrix \(\mathbf{W}_{j}\) converges to approximately 2. It is worth noting that this stable rank value is higher than the stable rank achieved by leaky ReLU networks, which is \(1\).

Comparison with previous work.One notable related work is Lyu et al. (2021), which also investigates the implicit bias of two-layer leaky ReLU networks. The main distinction between our work and Lyu et al. (2021) is the optimization method employed. We utilize gradient descent, whereas they utilize gradient flow. Additionally, our assumption is that the training data is nearly-orthogonal,while they assume the training data is symmetric. Our findings are more closely related to the work by Frei et al. (2022b), which investigates both gradient flow and gradient decent. In both our study and Frei et al. (2022b), we examine two-layer neural networks with leaky ReLU activations. However, they focus on networks trained via gradient flow, while we investigate networks trained using gradient descent. For the gradient descent approach, Frei et al. (2022b) provide a constant stable rank upper bound for smoothed leaky ReLU. In contrast, we prove that the stable rank of leaky ReLU networks converges to \(1\), aligning with the implicit bias of gradient flow proved in Frei et al. (2022b). Furthermore, they presented an \(O(t^{-1/2})\) convergence rate for the empirical loss, whereas our convergence rate is \(\Theta(t^{-1})\). Another related work is Lyu and Li (2019), which studied smooth homogeneous networks trained by gradient descent. Our results on the rate of weight norm increase and the convergence rate of training loss match those in Lyu and Li (2019), despite the fact that we study non-smooth homogeneous networks. It is worth noting that Lyu and Li (2019); Lyu et al. (2021); Frei et al. (2022b) demonstrated that neural networks trained by gradient flow converge to a Karush-Kuhn-Tucker (KKT) point of the max-margin problem. We do not have such a result unless we assume the directional convergence of the weight matrix.

## 5 Overview of Proof Techniques

In this section, we discuss the key techniques we invent in our proofs to analyze the implicit bias of ReLU and leaky ReLU networks.

### Refined Analysis of Decomposition Coefficient

_Signal-noise decomposition_, a technique initially introduced by Cao et al. (2022), is used to analyze the learning dynamics of two-layer convolutional networks. This method decomposes the convolutional filters into a linear combination of initial filters, signal vectors, and noise vectors, converting the neural network learning into a dynamical system of coefficients derived from the decomposition. In this work, we extend the signal-noise decomposition to _data-correlated decomposition_ to facilitate the analysis of the training dynamic for two-layer fully connected neural networks.

**Definition 5.1** (Data-correlated Decomposition).: Let \(\mathbf{w}_{j,r}^{(t)}\), \(j\in\{\pm 1\}\), \(r\in[m]\) be the weights of first-layer neurons at the \(t\)-th iteration of gradient descent. There exist unique coefficients \(\rho_{j,r,i}^{(t)}\) such that

\[\mathbf{w}_{j,r}^{(t)}=\mathbf{w}_{j,r}^{(0)}+\sum_{i=1}^{n}\rho_{j,r,i}^{(t) }\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}.\] (5.1)

By defining \(\overline{\rho}_{j,r,i}^{(t)}:=\rho_{j,r,i}^{(t)}\,\mathds{1}(\rho_{j,r,i}^{(t )}\geq 0)\), \(\underline{\rho}_{j,r,i}^{(t)}:=\rho_{j,r,i}^{(t)}\,\mathds{1}(\rho_{j,r,i}^{ (t)}\leq 0)\), (5.1) can be further written as

\[\mathbf{w}_{j,r}^{(t)}=\mathbf{w}_{j,r}^{(0)}+\sum_{i=1}^{n}\overline{\rho}_ {j,r,i}^{(t)}\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}+\sum_{i=1}^{ n}\rho_{j,r,i}^{(t)}\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}.\] (5.2)

As an extension of the signal-noise decomposition first proposed in Cao et al. (2022) for analyzing two-layer convolutional networks, _data-correlated decomposition_ defined in Definition 5.1 can be used to analyze two-layer fully-connected network, where the normalization factors \(\|\mathbf{x}_{i}\|_{2}^{-2}\) are introduced to ensure that \(\rho_{j,r,i}^{(t)}\approx\langle\mathbf{w}_{j,r}^{(t)},\mathbf{x}_{i}\rangle\). This is also inspired by previous works by Lyu and Li (2019); Frei et al. (2022b), which demonstrate that \(\mathbf{W}\) converges to a KKT point of the max-margin problem. This implies that \(\mathbf{w}_{j,r}^{(\infty)}/\|\mathbf{w}_{j,r}^{(\infty)}\|_{2}\) can be expressed as a linear combination of the training data \(\{\mathbf{x}_{i}\}_{i=1}^{n}\), with the coefficient \(\lambda_{i}\) corresponding to \(\rho_{j,r,i}^{(t)}\) in our analysis. This technique does not rely on the strictly increasing and smoothness properties of the activation function and will serve as the foundation for our analysis. Let us first investigate the update rule of the coefficient \(\overline{\rho}_{j,r,i}^{(t)},\underline{\rho}_{j,r,i}^{(t)}\).

**Lemma 5.2**.: The coefficients \(\overline{\rho}_{j,r,i}^{(t)},\underline{\rho}_{j,r,i}^{(t)}\) defined in Definition 5.1 satisfy the following iterative equations:

\[\overline{\rho}_{j,r,i}^{(0)},\underline{\rho}_{j,r,i}^{(0)}=0,\] (5.3)\[\overline{\rho}_{j,r,i}^{(t+1)} =\overline{\rho}_{j,r,i}^{(t)}-\frac{\eta}{nm}\cdot\ell_{i}^{\prime( t)}\cdot\sigma^{\prime}(\langle\mathbf{w}_{j,r}^{(t)},\mathbf{x}_{i}\rangle) \cdot\|\mathbf{x}_{i}\|_{2}^{2}\cdot\mathbf{1}(y_{i}=j),\] (5.4) \[\rho_{j,r,i}^{(t+1)} =\rho_{j,r,i}^{(t)}+\frac{\eta}{nm}\cdot\ell_{i}^{\prime(t)}\cdot \sigma^{\prime}(\langle\mathbf{w}_{j,r}^{(t)},\mathbf{x}_{i}\rangle)\cdot\| \mathbf{x}_{i}\|_{2}^{2}\cdot\mathbf{1}(y_{i}=-j),\] (5.5)

for all \(r\in[m]\), \(j\in\{\pm 1\}\) and \(i\in[n]\).

To study implicit bias, the first main challenge is to generalize the decomposition coefficient analysis to infinite time. The signal-noise decomposition used in Cao et al. (2022); Kou et al. (2023) requires early stopping with threshold \(T^{*}\) to facilitate their analysis. They only provided upper bounds of \(4\log(T^{*})\) for \(\overline{\rho}_{j,r,i}^{(t)},|\underline{\rho}_{j,r,i}^{(t)}|\) (See Proposition 5.3 in Cao et al. (2022), Proposition 5.2 in Kou et al. (2023)), and then carried out a two-stage analysis. To obtain upper bounds for \(\overline{\rho}_{j,r,i}^{(t)},|\underline{\rho}_{j,r,i}^{(t)}|\), they used an upper bound for \(|\ell_{i}^{\prime(t)}|\) and directly plugged it into (5.4) and (5.5) to demonstrate that \(\overline{\rho}_{j,r,i}^{(t)}\) and \(|\underline{\rho}_{j,r,i}^{(t)}|\) would not exceed \(4\log(T^{*})\), which is a fixed value related to the early stopping threshold. Therefore, dealing with infinite time requires new techniques. To overcome this difficulty, we propose a _refined analysis of decomposition coefficients_ which generalizes Cao et al. (2022)'s technique. We first give the following key lemma.

**Lemma 5.3**.: For non-negative real number sequence \(\{x_{t}\}_{t=0}^{\infty}\) satisfying

\[C_{1}\exp(-x_{t})\leq x_{t+1}-x_{t}\leq C_{2}\exp(-x_{t}),\] (5.6)

it holds that

\[\log(\exp(-x_{0})+C_{1}\cdot t)\leq x_{t}\leq\log(\exp(-x_{0})+C_{2}\exp(C_{2} )\cdot t).\] (5.7)

We can establish the relationship between (5.4), (5.5) and inequality (5.6) if we are able to express \(|\ell_{i}^{\prime(t)}|\) using coefficients \(\overline{\rho}_{j,r,i}^{(t)}\) and \(|\underline{\rho}_{j,r,i}^{(t)}|\). To achieve this, we can first approximate \(\ell_{i}^{(t)}\) using the margin \(y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})\) and then approximate \(F_{j}(\mathbf{W}_{j}^{(t)},\mathbf{x}_{i})\) using the coefficients \(\rho_{j,r,i}^{(t)}\). The approximation is given as follows:

\[\ell_{i}^{(t)}=\Theta(\exp(-y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i })))=\Theta\big{(}\exp\big{(}F_{-y_{i}}(\mathbf{W}_{-y_{i}}^{(t)},\mathbf{x}_{ i})-F_{y_{i}}(\mathbf{W}_{y_{i}}^{(t)},\mathbf{x}_{i})\big{)}\big{)},\] (5.8) \[\bigg{|}F_{j}(\mathbf{W}_{j}^{(t)},\mathbf{x}_{i})-\frac{1}{m} \sum_{r=1}^{m}\rho_{j,r,i}^{(t)}\bigg{|}\leq\sum_{i^{\prime}\neq i}\bigg{(} \frac{1}{m}\sum_{r=1}^{m}|\rho_{j,r,i^{\prime}}^{(t)}|R_{\min}^{-2}p\bigg{)},\] (5.9)

From (5.9), one can see that we need to decouple \(\ell_{i}^{(t)}\) from \(|\rho_{j,r,i^{\prime}}^{(t)}|(i^{\prime}\neq i)\). In order to accomplish this, we also prove the following lemma, which demonstrates that the ratio between \(\sum_{r=1}^{m}|\rho_{j,r,i}^{(t)}|\) and \(\sum_{r=1}^{m}|\rho_{j,r,i^{\prime}}^{(t)}|(i^{\prime}\neq i)\) will maintain a constant order throughout the training process. Here, we present the lemma for leaky ReLU networks.

**Lemma 5.4** (leaky ReLU automatic balance).: For two-layer leaky ReLU network defined in (3.1), for any \(t\geq 0\), we have \(\sum_{r=1}^{m}|\rho_{j,r,i}^{(t)}|\geq c\gamma^{2}\sum_{r=1}^{m}|\rho_{j,r,i^{ \prime}}^{(t)}|\) for any \(j\in\{\pm 1\}\) and \(i,i^{\prime}\in[n]\), where \(c\) is a constant.

By Lemma 5.4, we can approximate the neural network output using (5.9). This approximation expresses the output \(F_{j}(\mathbf{W}_{j}^{(t)},\mathbf{x}_{i})\) as a sum of the coefficients \(\rho_{j,r,i}^{(t)}\):

\[F_{j}(\mathbf{W}_{j}^{(t)},\mathbf{x}_{i})\approx\frac{1\pm c\gamma^{2}R_{\min }^{-2}pn}{m}\sum_{r=1}^{m}\rho_{j,r,i}^{(t)}.\] (5.10)

By combining (5.4), (5.5), (5.8), and (5.10), we obtain the following relationship:

\[\frac{1}{m}\sum_{r=1}^{m}|\rho_{j,r,i}^{(t+1)}|-\frac{1}{m}\sum_{r=1}^{m}|\rho_ {j,r,i}^{(t)}|=\Theta\Big{(}\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}}{nm}\Big{)} \cdot\exp\bigg{(}-\frac{1\pm c\gamma^{2}R_{\min}^{-2}pn}{m}\sum_{r=1}^{m}|\rho_ {j,r,i}^{(t)}|\bigg{)}.\]

This relationship aligns with the form of (5.6), if we set \(x_{t}=\frac{1\pm c\gamma^{2}R_{\min}^{-2}pn}{m}\sum_{r=1}^{m}|\rho_{j,r,i}^{(t)}|\). Thus, we can directly apply Lemma 5.3 to gain insights into the logarithmic rate of increase for the average magnitudes of the coefficients \(\frac{1}{m}\sum_{r=1}^{m}|\rho^{(t)}_{j,r,i}|\), which in turn implies that \(\|\mathbf{w}^{(t)}_{j,r}\|_{2}=\Theta(\log t)\) and \(\|\mathbf{W}^{(t)}\|_{F}=\Theta(\log t)\). In the case of ReLU networks, we have the following lemma that provides automatic balance:

**Lemma 5.5** (ReLU automatic balance).: For two-layer ReLU network defined in (3.1), there exists a constant \(c\) such that for any \(t\geq 0\), we have \(|\rho^{(t)}_{y_{i},r,i}|\geq c|\rho^{(t)}_{j,r^{\prime},i^{\prime}}|\) for any \(j\in\{\pm 1\}\), \(r\in S^{(0)}_{i}:=\{r\in[m]:\langle\mathbf{w}^{(0)}_{y_{i},r},\mathbf{x}_{i} \rangle\geq 0\}\), \(r^{\prime}\in[m]\) and \(i,i^{\prime}\in[n]\).

The automatic balance lemma guarantees that the magnitudes of coefficients related to the neurons of class \(y_{i}\), which are activated by \(\mathbf{x}_{i}\) during initialization, dominate those of other classes. With the help of Lemma 5.5, we can get the following approximation for the margin \(y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})\):

\[F_{y_{i}}(\mathbf{W}^{(t)}_{y_{i}},\mathbf{x}_{i})-F_{-y_{i}}(\mathbf{W}^{(t) }_{-y_{i}},\mathbf{x}_{i})\approx\frac{1\pm cR_{\text{min}}^{-2}pn}{m}\sum_{ r\in S^{(0)}_{i}}\rho^{(t)}_{y_{i},r,i}.\] (5.11)

By combining (5.4), (5.5), (5.8) and (5.11), we obtain the following relationship:

\[\sum_{r\in S^{(0)}_{i}}|\rho^{(t+1)}_{y_{i},r,i}|-\sum_{r\in S^{(0)}_{i}}|\rho ^{(t)}_{y_{i},r,i}|=\Theta\Big{(}\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}|S^{(0)}_ {i}|}{nm}\Big{)}\cdot\exp\bigg{(}-\frac{1\pm cR_{\text{min}}^{-2}pn}{m}\sum_{ r\in S^{(0)}_{i}}\rho^{(t)}_{y_{i},r,i}\bigg{)},\]

which precisely matches the form of (5.6) by setting \(x_{t}=\frac{1\pm cR_{\text{min}}^{-2}pn}{m}\sum_{r\in S^{(0)}_{i}}\rho^{(t)}_{ y_{i},r,i}\). Therefore, we can directly apply Lemma 5.3 and obtain the logarithmic increasing rate of \(|\rho^{(t)}_{y_{i},r,i}|\) for \(r\in S^{(0)}_{i}\). Consequently, this implies that \(\|\mathbf{W}^{(t)}\|_{F}=\Theta(\log t)\).

### Analysis of Activation Pattern

One notable previous work (Frei et al., 2022b) provided a constant upper bound for the stable rank of two-layer smoothed leaky ReLU networks trained by gradient descent in their Theorem 4.2. To achieve a better stable rank bound, we characterize the activation pattern of leaky ReLU network neurons after a certain threshold time \(T\) in the following lemma.

**Lemma 5.6** (leaky ReLU activation pattern).: Let \(T=C\eta^{-1}nmR_{\max}^{-2}\). For two-layer leaky ReLU network defined in (3.1), for any \(t\geq T\), it holds that \(\operatorname{sign}(\langle\mathbf{w}^{(t)}_{j,r},\mathbf{x}_{i}\rangle)=jy_{i}\) for any \(j\in\{\pm 1\}\) and \(r\in[m]\).

Lemma 5.6 indicates that the activation pattern will not change after time \(T\). Given Lemma 5.6, we can get \(\sigma^{\prime}(\langle\mathbf{w}^{(t)}_{j,r},\mathbf{x}_{i}\rangle)=\gamma\) for \(j\neq y_{i}\) and \(\sigma^{\prime}(\langle\mathbf{w}^{(t)}_{j,r},\mathbf{x}_{i}\rangle)=1\) for \(j=y_{i}\). Plugging this into (5.4) and (5.5) can give the following useful lemma.

**Lemma 5.7**.: Let \(T\) be defined in Lemma 5.6. For \(t\geq T\), it holds that

\[\overline{\rho}^{(t)}_{y_{i},r,i}-\overline{\rho}^{(T)}_{y_{i},r, i}=\overline{\rho}^{(t)}_{y_{i},r^{\prime},i}-\overline{\rho}^{(T)}_{y_{i},r,i}, \underline{\rho}^{(t)}_{-y_{i},r,i}-\underline{\rho}^{(T)}_{-y_{i},r,i}= \underline{\rho}^{(t)}_{-y_{i},r^{\prime},i}-\underline{\rho}^{(T)}_{-y_{i},r^ {\prime},i},\] \[\overline{\rho}^{(t)}_{y_{i},r,i}-\overline{\rho}^{(T)}_{y_{i},r, i}=(\underline{\rho}^{(t)}_{-y_{i},r^{\prime},i}-\underline{\rho}^{(T)}_{-y_{i},r^ {\prime},i})/\gamma,\]

for any \(i\in[n]\) and \(r,r^{\prime}\in[m]\).

This lemma reveals that beyond a certain time threshold \(T\), the increase in \(\rho^{(t)}_{j,r,i}\) is consistent across neurons within the same positive or negative class. However, for neurons belonging to the oppose class, this increment in \(\rho^{(t)}_{j,r,i}\) is scaled by a factor equivalent to the slope of the leaky ReLU function \(\gamma\). From this and (5.1), we can demonstrate that \(\|\mathbf{w}^{(t)}_{j,r}-\mathbf{w}^{(t)}_{j,r^{\prime}}\|_{2}(r\neq r^{\prime})\) can be upper bounded by a constant, leading to the following inequalities:

\[\|\mathbf{W}^{(t)}_{j}\|_{F}^{2}\leq m\|\mathbf{w}^{(t)}_{j,1}\|_{2}^{2}+mC_{1} \|\mathbf{w}^{(t)}_{j,1}\|_{2}+mC_{2},\,\|\mathbf{W}^{(t)}_{j}\|_{2}^{2}\geq m \|\mathbf{w}^{(t)}_{j,1}\|_{2}^{2}-mC_{3}\|\mathbf{w}^{(t)}_{j,1}\|_{2}-mC_{4}.\]

Considering that \(\|\mathbf{w}^{(t)}_{j,r}\|_{2}=\Theta(\log t)\), the stable rank of \(\mathbf{W}^{(t)}_{j}\) naturally converges to a value of \(1\). For ReLU networks, we can partially characterize the activation pattern as illustrated in the following lemma.

**Lemma 5.8**.: (ReLU activation pattern) For two-layer ReLU networks defined in (3.1), for any \(i\in[n]\), we have \(S_{i}^{(t)}\subseteq S_{i}^{(t+1)}\) for any \(t\geq 0\), where \(S_{i}^{(t)}:=\{r\in[m]:\langle\mathbf{w}_{y_{i},r}^{(t)},\mathbf{x}_{i}\rangle \geq 0\}\).

Lemma 5.8 suggests that once the neuron of class \(y_{i}\) is activated by \(\mathbf{x}_{i}\), it will remain activated throughout the training process. Leveraging such an activation pattern, we can establish a lower bound for \(\|\mathbf{W}_{j}^{(t)}\|_{2}\) as \(\Omega(\log(t))\). Together with the trivial upper bound for \(\|\mathbf{W}_{j}^{(t)}\|_{F}\) of order \(O(\log(t))\), it provides a constant upper bound for the stable rank of ReLU network weight.

### Analysis of Margin and Training Loss

Notably, Lyu and Li (2019) established in their Theorem 4.4 that any limit point of smooth homogeneous neural networks \(f(\mathbf{W},\mathbf{x})\) trained by gradient descent is along the direction of a KKT point for the max-margin problem (4.1). Additionally, Lyu and Li (2019) provided precise bounds on the training loss and weight norm for smooth homogeneous neural networks in their Theorem 4.3 as follows:

\[L_{S}(\mathbf{W}^{(t)})=\Theta\Big{(}\frac{1}{t(\log t)^{2-2/L}}\Big{)},\qquad \|\mathbf{W}^{(t)}\|_{F}=\Theta\big{(}(\log t)^{1/L}\big{)},\]

where \(L\) is the order of the homogeneous network satisfying the property \(f(c\mathbf{W},\mathbf{x})=c^{L}f(\mathbf{W},\mathbf{x})\) for all \(c>0\), \(\mathbf{W}\), and \(\mathbf{x}\). It is worth noting that the two-layer (leaky) ReLU neural network analyzed in this paper is 1-homogeneous but not smooth. In Section 5.1, we have already demonstrated that \(\|\mathbf{W}^{(t)}\|_{F}=\Theta(\log t)\), and in this subsection, we will discuss the proof technique employed to show a convergence rate of \(\Theta(t^{-1})\) for the loss and establish the same normalized margin for all the training data points asymptotically. These results align with those presented by Lyu and Li (2019) regarding smooth homogeneous networks.

By the nearly orthogonal property, we can bound the increment of margin as follows:

\[\frac{\eta}{5nm}\cdot|\ell_{i}^{\prime(t)}|\cdot\|\mathbf{x}_{i}\|_{2}^{2} \leq y_{i}f(\mathbf{W}^{(t+1)},\mathbf{x}_{i})-y_{i}f(\mathbf{W}^{(t)}, \mathbf{x}_{i})\leq\frac{3\eta}{nm}\cdot|\ell_{i}^{\prime(t)}|\|\mathbf{x}_{i }\|_{2}^{2}.\] (5.12)

Given (5.8) and (5.12), we can apply Lemma 5.3 and obtain

\[\Big{|}y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})-\log t-\log(\eta\|\mathbf{x}_{ i}\|_{2}^{2}/nm)\Big{|}\leq C_{3},\] (5.13)

where \(C_{3}\) is a constant. Utilizing (5.13) and the inequality \(z-z^{2}/2\leq\log(1+z)\leq z\) for \(z\geq 0\), we can derive:

\[L_{S}(\mathbf{W}^{(t)}) \leq\frac{1}{n}\sum_{i=1}^{n}\exp\big{(}-y_{i}f(\mathbf{W}^{(t)}, \mathbf{x}_{i})\big{)}\] \[\leq\frac{1}{n}\sum_{i=1}^{n}\exp\big{(}-\log t-\log(\eta\| \mathbf{x}_{i}\|_{2}^{2}/nm)+C_{3}\big{)}=O(t^{-1}),\] \[L_{S}(\mathbf{W}^{(t)}) \geq\frac{1}{n}\sum_{i=1}^{n}\exp\big{(}-y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})\big{)}-\exp\big{(}-2y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i}) \big{)}=\Omega(t^{-1}).\]

To demonstrate that all the training data points attain the same normalized margin as \(t\) goes to infinity, we first observe that (5.12) provides the following bounds for the increment of margin difference:

\[y_{k}f(\mathbf{W}^{(t+1)},\mathbf{x}_{k})-y_{i}f(\mathbf{W}^{(t+ 1)},\mathbf{x}_{i})\] (5.14) \[\leq y_{k}f(\mathbf{W}^{(t)},\mathbf{x}_{k})-y_{i}f(\mathbf{W}^{( t)},\mathbf{x}_{i})+\frac{3\eta}{nm}\cdot|\ell_{k}^{\prime(t)}|\cdot\| \mathbf{x}_{k}\|_{2}^{2}-\frac{\eta}{5nm}\cdot|\ell_{i}^{\prime(t)}|\cdot\| \mathbf{x}_{i}\|_{2}^{2}.\]

Now, we consider two cases:

* If the ratio \(|\ell_{i}^{\prime(t)}|/|\ell_{k}^{\prime(t)}|\) is relatively large, then \(y_{k}f(\mathbf{W}^{(t)},\mathbf{x}_{k})-y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})\) will not increase.
* If the ratio \(|\ell_{i}^{\prime(t)}|/|\ell_{k}^{\prime(t)}|\) is relatively small, then \(y_{k}f(\mathbf{W}^{(t)},\mathbf{x}_{k})-y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})\) will also be relatively small. In fact, it can be bounded by a constant due to the fact that \(|\ell_{i}^{\prime(t)}|/|\ell_{k}^{\prime(t)}|\) can be approximated by \(\exp(y_{k}f(\mathbf{W}^{(t)},\mathbf{x}_{k})-y_{i}f(\mathbf{W}^{(t)},\mathbf{ x}_{i}))\). By (5.14), we can show that\(y_{k}f(\mathbf{W}^{(t+1)},\mathbf{x}_{k})-y_{i}f(\mathbf{W}^{(t+1)},\mathbf{x}_{i})\) can also be bounded by a constant, provided that the learning rate \(\eta\) is sufficiently small.

By combining both cases, we can conclude that both \(|\ell_{i}^{\prime(t)}|/|\ell_{k}^{\prime(t)}|\) and \(y_{k}f(\mathbf{W}^{(t)},\mathbf{x}_{k})-y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})\) can be bounded by constants. This result is formally stated in the following lemma.

**Lemma 5.9**.: For two-layer neural networks defined in (3.1) with (leaky) ReLU activation, the following bounds hold for any \(t\geq 0\):

\[y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})-y_{k}f(\mathbf{W}^{(t)},\mathbf{x}_{k })\leq C_{1},\quad\ell_{i}^{\prime(t)}/\ell_{k}^{\prime(t)}\leq C_{2},\] (5.15)

for any \(i,k\in[n]\), where \(C_{1},C_{2}\) are positive constants.

By Lemma 5.9, which shows that the difference between the margins of any two data points can be bounded by a constant, and taking into account that \(\|\mathbf{W}^{(t)}\|_{F}=\Theta(\log t)\), we can deduce the following result:

\[\lim_{t\to\infty}\big{|}y_{i}f(\mathbf{W}^{(t)}/\|\mathbf{W}^{(t)}\|_{F}, \mathbf{x}_{i})-y_{k}f(\mathbf{W}^{(t)}/\|\mathbf{W}^{(t)}\|_{F},\mathbf{x}_{ k})\big{|}=0,\forall i,k\in[n].\]

This demonstrates that gradient descent will asymptotically find a neural network in which all the training data points achieve the same normalized margin.

## 6 Experiments

In this section, we present simulations of both synthetic and real data to back up our theoretical analysis in the previous section.

Synthetic-data experiments.Here we generate a synthetic mixture of Gaussian data as follows:

Let \(\boldsymbol{\mu}\in\mathbb{R}^{d}\) be a fixed vector representing the signal contained in each data point. Each data point \((\mathbf{x},y)\) with predictor \(\mathbf{x}\in\mathbb{R}^{d}\) and label \(y\in\{-1,1\}\) is generated from a distribution \(\mathcal{D}\), which we specify as follows:

1. The label \(y\) is generated as a Rademacher random variable, i.e. \(\mathbb{P}[y=1]=\mathbb{P}[y=-1]=1/2\).
2. A noise vector \(\boldsymbol{\xi}\) is generated from the Gaussian distribution \(\mathcal{N}(\mathbf{0},\sigma_{p}^{2}\mathbf{I}_{d})\). And \(\mathbf{x}\) is assigned as \(y\cdot\boldsymbol{\mu}+\boldsymbol{\xi}\) where \(\boldsymbol{\mu}\) is a fixed feature vector.

Specifically, we set training data size \(n=10,d=784\) and train the NN with gradient descent using learning rate \(0.1\) for \(50\) epochs. We set \(\boldsymbol{\mu}\) to be a feature randomly drawn from \(\mathcal{N}(0,10^{-4}\mathbf{I}_{d})\). We then generate the noise vector \(\boldsymbol{\xi}\) from the Gaussian distribution \(\mathcal{N}(\mathbf{0},\sigma_{p}^{2}\mathbf{I})\) with fixed standard deviation \(\sigma_{p}=1\). We train the FNN model defined in Section 3 with ReLU (or leaky-RelU) activation function and width \(m=100\). As we can infer from Figure 1, the stable rank will decrease faster for larger leaky ReLU slopes and have a smaller value when epoch \(t\to\infty\).

Real-data experiments on MNIST dataset.Here we train a two-layer feed-forward neural network defined in Section 3 with ReLU (or leaky-ReLU) functions. The number of widths is

Figure 1: Stable ranks and training loss for different leaky ReLU slopes \(\gamma\) across multiple runs. A slope of \(1\) corresponds to linear activation, while a slope of \(0\) corresponds to ReLU activation. Each line represents the mean stable rank or training loss for a given leaky ReLU slope, while the shaded regions indicate the variability of the values (\(\pm 3\) times the standard deviation) across the \(5\) runs.

set as \(m=1000\). We use the Gaussian initialization and consider different weight variance \(\sigma_{0}\in\{0.00001,0.00005,0.0001,0.0005,0.001\}\). We train the NN with stochastic gradient descent with batch size \(64\) and learning rate \(0.1\) for \(10\) epochs. As we can infer from Figures 2 and 3, the stable rank of ReLU or leaky ReLU networks will largely depend on the initialization and the training time. When initialization is sufficiently small, the stable rank will quickly decrease to a small value compared to its initialization values.

## 7 Conclusion and Future Work

This paper employs a data-correlated decomposition technique to examine the implicit bias of two-layer ReLU and Leaky ReLU networks trained using gradient descent. By analyzing the training dynamics, we provide precise characterizations of the weight matrix stable rank limits for both ReLU and Leaky ReLU cases, demonstrating that both scenarios will yield a network with a low stable rank. Additionally, we present an analysis for the convergence rate of the loss function. An important future work is to investigate the directional convergence of the weight matrix in neural networks trained via gradient descent, which is essential to prove the convergence to a KKT point of the max-margin problem. Furthermore, it is important to extend our analysis to fully understand the neuron activation patterns in ReLU networks. Specifically, we will explore whether certain neurons will switch their activation patterns by an infinite number of times throughout the training or if the activation patterns stabilize after a certain number of gradient descent iterations.

## Acknowledgements

We thank the anonymous reviewers and area chair for their helpful comments. YK, ZC, and QG are supported in part by the National Science Foundation CAREER Award 1906169 and IIS-2008981, and the Sloan Research Fellowship. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.

Figure 3: Stable ranks and test errors for different weight variances across multiple runs (leaky-ReLU Activation Function with slope \(0.1\)). Each line represents the mean stable rank or test accuracy for a given weight variance, while the shaded regions indicate the variability of the values (\(\pm 3\) times the standard deviation) across the \(5\) runs.

Figure 2: Stable ranks and test errors for different weight variances across multiple runs (ReLU Activation Function). Each line represents the mean stable rank or test accuracy for a given weight variance, while the shaded regions indicate the variability of the values (\(\pm 3\) times the standard deviation) across the \(5\) runs.

## References

* Allen-Zhu and Li (2020)Allen-Zhu, Z. and Li, Y. (2020). Towards understanding ensemble, knowledge distillation and self-distillation in deep learning. _arXiv preprint arXiv:2012.09816_.
* Arora et al. (2019)Arora, S., Cohen, N., Hu, W. and Luo, Y. (2019). Implicit regularization in deep matrix factorization. _Advances in Neural Information Processing Systems_**32**.
* Bartlett et al. (2020)Bartlett, P. L., Long, P. M., Lugosi, G. and Tsigler, A. (2020). Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_.
* Belkin et al. (2019)Belkin, M., Hsu, D., Ma, S. and Mandal, S. (2019). Reconciling modern machine-learning practice and the classical bias-variance trade-off. _Proceedings of the National Academy of Sciences_**116** 15849-15854.
* Belkin et al. (2020)Belkin, M., Hsu, D. and Xu, J. (2020). Two models of double descent for weak features. _SIAM Journal on Mathematics of Data Science_**2** 1167-1180.
* Cao et al. (2022)Cao, Y., Chen, Z., Belkin, M. and Gu, Q. (2022). Benign overfitting in two-layer convolutional neural networks. _arXiv preprint arXiv:2202.06526_.
* Cao et al. (2021)Cao, Y., Gu, Q. and Belkin, M. (2021). Risk bounds for over-parameterized maximum margin classification on sub-gaussian mixtures. _Advances in Neural Information Processing Systems_**34**.
* Chatterji and Long (2021)Chatterji, N. S. and Long, P. M. (2021). Finite-sample analysis of interpolating linear classifiers in the overparameterized regime. _Journal of Machine Learning Research_**22** 129-1.
* Chizat and Bach (2020)Chizat, L. and Bach, F. (2020). Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss. In _Conference on Learning Theory_. PMLR.
* Frei et al. (2022a)Frei, S., Chatterji, N. S. and Bartlett, P. (2022a). Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In _Conference on Learning Theory_. PMLR.
* Frei et al. (2022b)Frei, S., Vardi, G., Bartlett, P. L., Srebro, N. and Hu, W. (2022b). Implicit bias in leaky relu networks trained on high-dimensional data. _arXiv preprint arXiv:2210.07082_.
* Gunasekar et al. (2018)Gunasekar, S., Lee, J. D., Soudry, D. and Srebro, N. (2018). Implicit bias of gradient descent on linear convolutional networks. In _Advances in Neural Information Processing Systems_.
* Hastie et al. (2022)Hastie, T., Montanari, A., Rosset, S. and Tibshirani, R. J. (2022). Surprises in high-dimensional ridgeless least squares interpolation. _The Annals of Statistics_**50** 949-986.
* Jelassi and Li (2022)Jelassi, S. and Li, Y. (2022). Towards understanding how momentum improves generalization in deep learning. In _International Conference on Machine Learning_. PMLR.
* Ji and Telgarsky (2018)Ji, Z. and Telgarsky, M. (2018). Gradient descent aligns the layers of deep linear networks. _arXiv preprint arXiv:1810.02032_.
* Ji and Telgarsky (2020)Ji, Z. and Telgarsky, M. (2020). Directional convergence and alignment in deep learning. _Advances in Neural Information Processing Systems_**33** 17176-17186.
* Kou et al. (2023)Kou, Y., Chen, Z., Chen, Y. and Gu, Q. (2023). Benign overfitting for two-layer relu networks. _arXiv preprint arXiv:2303.04145_.
* Li et al. (2021)Li, Z., Zhou, Z.-H. and Gretton, A. (2021). Towards an understanding of benign overfitting in neural networks. _arXiv preprint arXiv:2106.03212_.
* Liao et al. (2020)Liao, Z., Couillet, R. and Mahoney, M. (2020). A random matrix analysis of random fourier features: beyond the gaussian kernel, a precise phase transition, and the corresponding double descent. In _34th Conference on Neural Information Processing Systems (NeurIPS 2020)_.
* Li et al. (2020)Lyu, K. and Li, J. (2019). Gradient descent maximizes the margin of homogeneous neural networks. _arXiv preprint arXiv:1906.05890_.
* Lyu et al. (2021)Lyu, K., Li, Z., Wang, R. and Arora, S. (2021). Gradient descent on two-layer nets: Margin maximization and simplicity bias. _Advances in Neural Information Processing Systems_**34** 12978-12991.
* Mei and Montanari (2019)Mei, S. and Montanari, A. (2019). The generalization error of random features regression: Precise asymptotics and double descent curve. _arXiv preprint arXiv:1908.05355_.
* Phuong and Lampert (2021)Phuong, M. and Lampert, C. H. (2021). The inductive bias of relu networks on orthogonally separable data. In _International Conference on Learning Representations_.
* Razin and Cohen (2020)Razin, N. and Cohen, N. (2020). Implicit regularization in deep learning may not be explainable by norms. _Advances in neural information processing systems_**33** 21174-21187.
* Safran et al. (2022)Safran, I., Vardi, G. and Lee, J. D. (2022). On the effective number of linear regions in shallow univariate relu networks: Convergence guarantees and implicit bias. _arXiv preprint arXiv:2205.09072_.
* Sarussi et al. (2021)Sarussi, R., Brutzkus, A. and Globerson, A. (2021). Towards understanding learning in neural networks with linear teachers. In _International Conference on Machine Learning_. PMLR.
* Shen et al. (2022)Shen, R., Bubeck, S. and Gunasekar, S. (2022). Data augmentation as feature manipulation: a story of desert cows and grass cows. _arXiv preprint arXiv:2203.01572_.
* Soudry et al. (2018)Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S. and Srebro, N. (2018). The implicit bias of gradient descent on separable data. _The Journal of Machine Learning Research_**19** 2822-2878.
* Timor et al. (2023)Timor, N., Vardi, G. and Shamir, O. (2023). Implicit regularization towards rank minimization in relu networks. In _International Conference on Algorithmic Learning Theory_. PMLR.
* Vardi (2022)Vardi, G. (2022). On the implicit bias in deep-learning algorithms. _arXiv preprint arXiv:2208.12591_.
* Vardi et al. (2022a)Vardi, G., Shamir, O. and Srebro, N. (2022a). On margin maximization in linear and relu networks. _Advances in Neural Information Processing Systems_**35** 37024-37036.
* Vardi et al. (2022b)Vardi, G., Yehudai, G. and Shamir, O. (2022b). Gradient methods provably converge to non-robust networks. _arXiv preprint arXiv:2202.04347_.
* Vershynin (2018)Vershynin, R. (2018). _High-Dimensional Probability: An Introduction with Applications in Data Science_. Cambridge Series in Statistical and Probabilistic Mathematics, Cambridge University Press.
* Wang and Thrampoulidis (2022)Wang, K. and Thrampoulidis, C. (2022). Binary classification of gaussian mixtures: Abundance of support vectors, benign overfitting, and regularization. _SIAM Journal on Mathematics of Data Science_**4** 260-284.
* Wu and Xu (2020)Wu, D. and Xu, J. (2020). On the optimal weighted \(\ell_{2}\) regularization in overparameterized linear regression. _Advances in Neural Information Processing Systems_**33**.

[MISSING_PAGE_EMPTY:13]

the stable rank for the leaky ReLU network with large slopes \(\gamma\) will converge to \(1\) when epoch \(t\rightarrow\infty\). In comparison, the stable rank for the large-width ReLU network will not converge to \(1\) but to \(2\).

### Additional Experiment on MINIST

Our focus is the training of a two-layer feed-forward neural network, as discussed in Section 3, utilizing either ReLU or leaky-ReLU activation functions. We examine different widths, specifically choosing from \(\{10,50,100,500,1000\}\).

The network initialization process follows a Gaussian distribution, with a variance of \(\sigma_{0}=0.00001\). Training is executed using stochastic gradient descent, a batch size of \(64\), and a learning rate of \(0.1\), for a total of 10 epochs. As discerned from Figures 6 and 7, the stable rank of networks utilizing either ReLU or leaky ReLU is weakly influenced by the width. For an exceedingly small width such as 10, the weight matrix is low rank with a correspondingly small stable rank. However, this also results in low test accuracy as the network cannot effectively learn all necessary features. As the width increases, the test accuracy and final stable rank will increase. However, for sufficiently large widths, an increase in width no longer corresponds to stable rank or test accuracy increases.

## Appendix B Preliminary Lemmas

In this section, we present some pivotal lemmas that illustrate some important properties of the data and neural network parameters at their random initialization and provide the update rule of coefficients from data-correlated decomposition.

Now turning to network initialization, the following lemma studies the inner product between a randomly initialized neural network neuron \(\mathbf{w}_{j,r}^{(0)}\) (\(j\in\{\pm 1\}\) and \(r\in[m]\)) and the training data. The calculations characterize how the neural network at initialization randomly captures the information in training data.

Figure 6: Stable ranks and test errors for different width across multiple runs (ReLU Activation Function). Each line represents the mean stable rank or test error for a given weight variance, while the shaded regions indicate the variability of the values (\(\pm 3\) times the standard deviation) across the \(5\) runs.

Figure 7: Stable ranks and test errors for different width across multiple runs (leaky-ReLU Activation Function). Each line represents the mean stable rank or test error for a given weight variance, while the shaded regions indicate the variability of the values (\(\pm 3\) times the standard deviation) across the \(5\) runs.

**Lemma B.1**.: Suppose that \(d=\Omega(\log(mn/\delta))\), \(m=\Omega(\log(1/\delta))\). Then with probability at least \(1-\delta\),

\[\sigma_{0}^{2}d/2\leq\|{\bf w}_{j,r}^{(0)}\|_{2}^{2}\leq 3 \sigma_{0}^{2}d/2,\] \[|\langle{\bf w}_{j,r}^{(0)},{\bf x}_{i}\rangle|\leq\sqrt{2\log(8 mn/\delta)}\cdot\sigma_{0}R_{\max}\]

for all \(r\in[m]\), \(j\in\{\pm 1\}\) and \(i\in[n]\).

Proof of Lemma b.1.: First of all, the initial weights \({\bf w}_{j,r}^{(0)}\sim\mathcal{N}({\bf 0},\sigma_{0}{\bf I})\). By Bernstein's inequality, with probability at least \(1-\delta/(4m)\) we have

\[\big{|}\|{\bf w}_{j,r}^{(0)}\|_{2}^{2}-\sigma_{0}^{2}d\big{|}=O( \sigma_{0}^{2}\cdot\sqrt{d\log(8m/\delta)}).\]

Therefore, if we set appropriately \(d=\Omega(\log(m/\delta))\), we have with probability at least \(1-\delta/2\), for all \(j\in\{\pm 1\}\) and \(r\in[m]\),

\[\sigma_{0}^{2}d/2\leq\|{\bf w}_{j,r}^{(0)}\|_{2}^{2}\leq 3 \sigma_{0}^{2}d/2.\]

Under definition, we have \(\|{\bf x}_{i}\|_{2}\leq R_{\max}\) for all \(i\in[n]\). It is clear that for each \(j,r\), \(\langle{\bf w}_{j,r}^{(0)},\boldsymbol{\mu}\rangle\) is a Gaussian random variable with mean zero and variance \(\sigma_{0}^{2}\|{\bf x}_{i}\|_{2}^{2}\). Therefore, by Gaussian tail bound and union bound, with probability at least \(1-\delta/2\),

\[|\langle{\bf w}_{j,r}^{(0)},{\bf x}_{i}\rangle|\leq\sqrt{2\log(8mn/\delta)} \cdot\sigma_{0}R_{\max}.\]

Next, we denote \(S_{i}^{(0)}\) as \(\{r\in[m]:\langle{\bf w}_{y_{i},r}^{(0)},{\bf x}_{i}\rangle>0\}\). We give a lower bound of \(|S_{i}^{(0)}|\) in the following two lemmas.

**Lemma B.2**.: Suppose that \(\delta>0\) and \(m\geq 50\log(2n/\delta)\). Then with probability at least \(1-\delta\),

\[0.4m\leq|S_{i}^{(0)}|\leq 0.6m,\,\forall i\in[n].\]

Proof of Lemma b.2.: Note that \(|S_{i}^{(0)}|=\sum_{r=1}^{m}\mathds{1}[\langle{\bf w}_{y_{i},r}^{(0)},{\bf x} _{i}\rangle>0]\) and \(P(\langle{\bf w}_{y_{i},r}^{(0)},{\bf x}_{i}\rangle>0)=1/2\), then by Hoeffding's inequality, with probability at least \(1-\delta/n\), we have

\[\left|\frac{|S_{i}^{(0)}|}{m}-\frac{1}{2}\right|\leq\sqrt{\frac{ \log(2n/\delta)}{2m}}.\]

Therefore, as long as \(m\geq 50\log(2n/\delta)\), by applying union bound, with probability at least \(1-\delta\), we have

\[0.4m\leq|S_{i}^{(0)}|\leq 0.6m,\,\forall i\in[n].\]

Now we give the update rule of coefficients from data-correlated decomposition. We will begin by analyzing the coefficients in the data-correlated decomposition in Definition 5.1. The following lemma presents an iterative expression for the coefficients.

**Lemma B.3**.: (Restatement of Lemma 5.2) The coefficients \(\bar{\rho}_{j,r,i}^{(t)},\rho_{j,r,i}^{(t)}\) defined in Definition 5.1 satisfy the following iterative equations:

\[\bar{\rho}_{j,r,i}^{(0)},\rho_{j,r,i}^{(0)}=0,\] \[\bar{\rho}_{j,r,i}^{(t+1)}=\bar{\rho}_{j,r,i}^{(t)}-\frac{\eta}{ nm}\cdot\ell_{i}^{\prime(t)}\cdot\sigma^{\prime}(\langle{\bf w}_{j,r}^{(t)},{\bf x }_{i}\rangle)\cdot\|{\bf x}_{i}\|_{2}^{2}\cdot\mathds{1}(y_{i}=j),\] \[\rho_{j,r,i}^{(t+1)}=\rho_{j,r,i}^{(t)}+\frac{\eta}{nm}\cdot\ell_ {i}^{\prime(t)}\cdot\sigma^{\prime}(\langle{\bf w}_{j,r}^{(t)},{\bf x}_{i} \rangle)\cdot\|{\bf x}_{i}\|_{2}^{2}\cdot\mathds{1}(y_{i}=-j),\]

for all \(r\in[m]\), \(j\in\{\pm 1\}\) and \(i\in[n]\).

Proof of Lemma b.3.: First, we iterate the gradient descent update rule (3.2) \(t\) times and get

\[\mathbf{w}_{j,r}^{(t+1)}=\mathbf{w}_{j,r}^{(0)}-\frac{\eta}{nm}\sum_{s=0}^{t} \sum_{i=1}^{n}\ell_{i}^{\prime(s)}\cdot\sigma^{\prime}(\langle\mathbf{w}_{j,r}^ {(s)},\mathbf{x}_{i}\rangle)\cdot jy_{i}\mathbf{x}_{i}.\]

According to the definition of \(\rho_{j,r,i}^{(t)}\), we have

\[\mathbf{w}_{j,r}^{(t)}=\mathbf{w}_{j,r}^{(0)}+\sum_{i=1}^{n}\rho_{j,r,i}^{(t)} \cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}.\]

Therefore, we have the unique representation

\[\rho_{j,r,i}^{(t)}=-\frac{\eta}{nm}\sum_{s=0}^{t}\ell_{i}^{\prime(s)}\cdot \sigma^{\prime}(\langle\mathbf{w}_{j,r}^{(s)},\mathbf{x}_{i}\rangle)\cdot\| \mathbf{x}_{i}\|_{2}^{2}\cdot jy_{i}.\]

Now with the notation \(\overline{\rho}_{j,r,i}^{(t)}:=\rho_{j,r,i}^{(t)}\,\mathds{1}(\rho_{j,r,i}^{ (t)}\geq 0)\), \(\rho_{j,r,i}^{(t)}:=\rho_{j,r,i}^{(t)}\,\mathds{1}(\rho_{j,r,i}^{(t)}\leq 0)\) and the fact \(\ell_{i}^{\prime(s)}<0\), we get

\[\overline{\rho}_{j,r,i}^{(t)}=-\frac{\eta}{nm}\sum_{s=0}^{t}\ell _{i}^{\prime(s)}\cdot\sigma^{\prime}(\langle\mathbf{w}_{j,r}^{(s)},\mathbf{x} _{i}\rangle)\cdot\|\mathbf{x}_{i}\|_{2}^{2}\cdot\mathds{1}(y_{i}=j),\] (B.1) \[\rho_{j,r,i}^{(t)}=\frac{\eta}{nm}\sum_{s=0}^{t}\ell_{i}^{\prime(s )}\cdot\sigma^{\prime}(\langle\mathbf{w}_{j,r}^{(s)},\mathbf{x}_{i}\rangle) \cdot\|\mathbf{x}_{i}\|_{2}^{2}\cdot\mathds{1}(y_{i}=-j).\] (B.2)

Writing out the iterative versions of (B.1) and (B.2) completes the proof. 

## Appendix C Coefficient Analysis of Leaky ReLU

In this section, we establish a series of results on the data-correlated decomposition for two-layer leaky ReLU network defined as

\[f(\mathbf{W}^{(t)},\mathbf{x}) =F_{+1}(\mathbf{W}_{+1}^{(t)},\mathbf{x})-F_{-1}(\mathbf{W}_{-1}^ {(t)},\mathbf{x})\] \[=\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{+1,r}^{(t)}, \mathbf{x}\rangle)-\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{-1,r}^{ (t)},\mathbf{x}\rangle),\] (C.1) \[\sigma(z)=\max\{\gamma z,z\},\gamma\in(0,1).\]

The results in Section C, D and G are based on Lemma B.1, which hold with high probability. Denote by \(\mathcal{E}_{\mathrm{prelim}}\) the event that Lemma B.1 in Section B holds (for a given \(\delta\), we see \(\mathbb{P}(\mathcal{E}_{\mathrm{prelim}})\geq 1-\delta\)). For simplicity and clarity, we state all the results in Section C, D and G conditional on \(\mathcal{E}_{\mathrm{prelim}}\).

Denote \(\beta=\max_{i,j,r}\{|\langle\mathbf{w}_{j,r}^{(0)},\mathbf{x}_{i}\rangle|\}\), \(R_{\max}=\max_{i\in[n]}\|\mathbf{x}_{i}\|_{2}\), \(R_{\min}=\min_{i\in[n]}\|\mathbf{x}_{i}\|_{2}\), \(p=\max_{i\neq k}|\langle\mathbf{x}_{i},\mathbf{x}_{k}\rangle|\) and suppose \(R=R_{\max}/R_{\min}\) is at most an absolute constant. Here we list the exact conditions for \(\eta,\sigma_{0},R_{\min},R_{\max},p\) required by the proofs in this section.

\[\sigma_{0}\leq\gamma\big{(}CR_{\max}\sqrt{\log(mn/\delta)}\big{)} ^{-1},\] (C.2) \[\eta\leq(CR_{\max}^{2}/nm)^{-1},\] (C.3) \[R_{\min}^{2}\geq Cr^{-4}R^{2}np,\] (C.4)

where \(C\) is a large enough constant. By Lemma B.1, we can upper bound \(\beta\) by \(2\sqrt{\log(12mn/\delta)}\cdot\sigma_{0}R_{\max}\). Then, by (C.2) and (C.4), it is straightforward to verify the following inequality:

\[\beta\leq c\gamma,\] (C.5) \[\gamma^{-4}R_{\min}^{-2}np\leq c,\] (C.6) \[\gamma^{-4}R_{\min}^{-2}R^{2}np\leq c,\] (C.7)where \(c\) is a sufficiently small constant.

Suppose the conditions listed in (C.2) and (C.4) hold, we claim that for any \(t\geq 0\) the following property holds.

**Lemma C.1**.: Under the same conditions as Theorem 4.1, for any \(t\geq 0\), we have that

\[\sum_{r=1}^{m}|\rho^{(t)}_{j,r,i}|\geq c_{1}\gamma^{2}\sum_{r=1}^{m}|\rho^{(t)} _{j^{\prime},r,i^{\prime}}|,\forall j,j^{\prime}\in\{\pm 1\},\forall i,i^{ \prime}\in[n],\] (C.8)

where \(c_{1}\) is a constant.

To prove Lemma C.1, we divide it into two lemmas, each addressing a specific case: \(0\leq t\leq T_{1}\) (Lemma C.2) when the logit \(|\ell^{(t)}_{i}|=\Theta(1)\), and \(t\geq T_{1}\) (Lemma C.3) when the logit \(|\ell^{(t)}_{i}|\) is smaller than constant order. Here, \(T_{1}=C^{\prime}\eta^{-1}nmR_{\max}^{-2}\), and \(C^{\prime}\) is a constant. For each case, we apply different techniques to establish the proof.

**Lemma C.2** (\(0\leq t\leq T_{1}\)).: Under the same conditions as Theorem 4.1, for any \(0\leq t\leq T_{1}=C^{\prime}\eta^{-1}nmR_{\max}^{-2}\), where \(C^{\prime}\) is a constant, we have that

\[|\rho^{(t)}_{j,r,i}|\geq c_{2}\gamma|\rho^{(t)}_{j^{\prime},r^{\prime},i^{ \prime}}|,\forall j,j^{\prime}\in\{\pm 1\},\forall r,r^{\prime}\in[m], \forall i,i^{\prime}\in[n],\] (C.9)

where \(c_{2}\) is a constant.

Proof of Lemma c.2.: In this lemma, we first show that (C.8) hold for \(t\leq T_{1}=C^{\prime}\eta^{-1}nmR_{\max}^{-2}\) where \(C^{\prime}=\Theta(1)\) is a constant. Recall from Lemma B.3 that

\[\overline{\rho}^{(t+1)}_{j,r,i} =\overline{\rho}^{(t)}_{j,r,i}-\frac{\eta}{nm}\cdot\ell^{(t)}_{i }\cdot\sigma^{\prime}(\langle\mathbf{w}^{(t)}_{j,r},\mathbf{x}_{i}\rangle) \cdot\|\mathbf{x}_{i}\|_{2}^{2}\cdot\mathbbm{1}(y_{i}=j),\] \[\rho^{(t+1)}_{j,r,i} =\rho^{(t)}_{j,r,i}+\frac{\eta}{nm}\cdot\ell^{(t)}_{i}\cdot\sigma ^{\prime}(\langle\mathbf{w}^{(t)}_{j,r},\mathbf{x}_{i}\rangle)\cdot\| \mathbf{x}_{i}\|_{2}^{2}\cdot\mathbbm{1}(y_{i}=-j),\]

we can get

\[\overline{\rho}^{(t)}_{-y_{i},r,i},\underline{\rho}^{(t)}_{y_{i},r,i}=0,\] (C.10)

and

\[\overline{\rho}^{(t+1)}_{y_{i},r,i} \leq\overline{\rho}^{(t)}_{y_{i},r,i}+\frac{\eta}{nm}\cdot\| \mathbf{x}_{i}\|_{2}^{2}\leq\overline{\rho}^{(t)}_{y_{i},r,i}+\frac{\eta R_{ \max}^{2}}{nm},\] (C.11) \[|\underline{\rho}^{(t+1)}_{-y_{i},r,i}| \leq|\underline{\rho}^{(t)}_{-y_{i},r,i}|+\frac{\eta}{nm}\cdot\| \mathbf{x}_{i}\|_{2}^{2}\leq|\underline{\rho}^{(t)}_{-y_{i},r,i}|+\frac{\eta R _{\max}^{2}}{nm}.\] (C.12)

Therefore, we have \(\max_{j,r,i}\{\overline{\rho}^{(t)}_{j,r,i},|\underline{\rho}^{(t)}_{j,r,i}| \}=O(1)\) for any \(t\leq T_{1}\) and hence \(\max_{i}\{F_{+1}(\mathbf{W}^{(t)}_{+1},\mathbf{x}_{i}),F_{-1}(\mathbf{W}^{(t) }_{-1},\mathbf{x}_{i})\}=O(1)\) for any \(t\leq T_{1}\). Thus there exists a positive constant \(\widetilde{c}\) such that \(|\ell^{(t)}_{i}|\geq\widetilde{c}\) for any \(t\leq T_{1}\). And it follows for any \(j\in\{\pm 1\},r\in[m],i\in[n]\) that

\[|\rho^{(t+1)}_{j,r,i}| \geq|\rho^{(t)}_{j,r,i}|+\frac{\gamma\eta}{nm}\cdot|\ell^{(t)}_{ i}|\cdot\|\mathbf{x}_{i}\|_{2}^{2}\geq|\rho^{(t)}_{j,r,i}|+\frac{\widetilde{c} \gamma\eta}{nm}\cdot\|\mathbf{x}_{i}\|_{2}^{2},\forall\,0\leq t\leq T_{1},\] \[|\rho^{(t)}_{j,r,i}| \geq\frac{\widetilde{c}\gamma\eta t}{nm}\cdot\|\mathbf{x}_{i}\|_{2 }^{2}\geq\frac{\widetilde{c}\gamma\eta R_{\min}^{2}t}{nm},\forall\,0\leq t \leq T_{1}.\] (C.13)

On the other hand, by (C.10), (C.11) and (C.12), we have for any \(j^{\prime}\in\{\pm 1\},r^{\prime}\in[m],i^{\prime}\in[n]\) that

\[|\rho^{(t)}_{j^{\prime},r^{\prime},i^{\prime}}|\leq\frac{\eta R_{\max}^{2}t}{ nm},\forall\,0\leq t\leq T_{1}.\] (C.14)

Dividing (C.14) by (C.13), we can get for any \(j,j^{\prime}\in\{\pm 1\},r,r^{\prime}\in[m],i,i^{\prime}\in[n]\) that

\[|\rho^{(t)}_{j,r,i}|\geq\frac{\widetilde{c}\gamma R_{\min}^{2}}{R_{\max}^{2}}| \rho^{(t)}_{j^{\prime},r^{\prime},i^{\prime}}|,\]

which indicates that the first bullet holds for time \(t\leq T_{1}\) as long as \(c_{2}\leq\widetilde{c}R_{\min}^{2}R_{\max}^{-2}\).

**Lemma C.3** (\(t\geq T_{1}\)).: Let \(T_{1}\) be defined in Lemma C.2. Under the same conditions as Theorem 4.1, for any \(t\geq T_{1}\), we have that

\[\sum_{r=1}^{m}|\rho_{j,r,i}^{(t)}|\geq c_{3}\gamma^{2}\sum_{r=1}^{m}|\rho_{j^{ \prime},r,i^{\prime}}^{(t)}|,\forall j,j^{\prime}\in\{\pm 1\},\forall i,i^{ \prime}\in[n],\] (C.15)

where \(c_{3}=\Theta(1)\) is a constant. Moreover, we also have the following increasing rate estimation of \(|\rho_{y_{i},r,i}^{(t)},|\rho_{-y_{i},r,i}^{(t)}|\):

* \(\frac{1}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)}\leq c_{4}^{-1}\log\bigg{(}1+ \frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}c_{5}e^{2\beta}}{nm}\cdot t\bigg{)}\),
* \(\frac{1}{m}\sum_{r=1}^{m}|\rho_{-y_{i},r,i}^{(t)}|\leq c_{5}^{-1}\gamma^{-1} \log\bigg{(}1+\frac{\gamma\eta\|\mathbf{x}_{i}\|_{2}^{2}c_{5}e^{2\beta}}{nm} \cdot t\bigg{)}\),
* \(\frac{1}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)}\geq c_{6}^{-1}\log\bigg{(}1+ \frac{\gamma\eta\|\mathbf{x}_{i}\|_{2}^{2}c_{6}e^{-(\gamma+1)\beta}}{nm}\cdot t \bigg{)}\),
* \(\frac{1}{m}\sum_{r=1}^{m}|\rho_{-y_{i},r,i}^{(t)}|\geq c_{6}^{-1}\gamma\log \bigg{(}1+\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}c_{6}e^{-(\gamma+1)\beta}}{nm} \cdot t\bigg{)}\),

where \(c_{4},c_{5},c_{6}\) are constants.

Proof of Lemma c.3.: We prove this lemma by induction. By Lemma C.2, we know that (C.15) holds for time \(t=T_{1}\) as long as \(c_{3}\leq c_{2}\). Suppose that there exists \(\widetilde{t}>T_{1}\) such that (C.15) holds for all time \(0\leq t\leq\widetilde{t}-1\). We aim to prove that they also hold for \(t=\widetilde{t}\). For any \(0\leq t\leq\widetilde{t}-1\), we have

\[\begin{split} F_{y_{i}}(\mathbf{W}_{y_{i}}^{(t)},\mathbf{x}_{i})& =\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{y_{i},r}^{(t) },\mathbf{x}_{i}\rangle)\\ &\geq\frac{1}{m}\sum_{r=1}^{m}\langle\mathbf{w}_{y_{i},r}^{(t)}, \mathbf{x}_{i}\rangle\\ &=\frac{1}{m}\sum_{r=1}^{m}\bigg{(}\langle\mathbf{w}_{y_{i},r}^{(0 )},\mathbf{x}_{i}\rangle+\sum_{i^{\prime}=1}^{n}\rho_{y_{i},r,i^{\prime}}^{(t) }\|\mathbf{x}_{i^{\prime}}\|_{2}^{-2}\cdot\langle\mathbf{x}_{i^{\prime}}, \mathbf{x}_{i}\rangle\bigg{)}\\ &\geq\frac{1}{m}\sum_{r=1}^{m}\bigg{(}\rho_{y_{i},r,i}^{(t)}- \sum_{i^{\prime}\neq i}|\rho_{y_{i},r,i^{\prime}}^{(t)}|R_{\min}^{-2}p\bigg{)} -\beta\\ &=\frac{1}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)}-\sum_{i^{\prime }\neq i}\bigg{(}\frac{1}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i^{\prime}}^{(t)}\bigg{)} R_{\min}^{-2}p-\beta\\ &\geq\frac{1-\gamma^{-2}c_{3}^{-1}R_{\min}^{-2}pn}{m}\sum_{r=1}^{ m}\rho_{y_{i},r,i}^{(t)}-\beta,\end{split}\] (C.16)

where the first inequality is by \(\sigma(z)\geq z\); the second equality is by (5.1); the third inequality is by triangle inequality and the definition of \(\beta,p,R_{min}\); the fourth inequality is by the induction hypothesis (C.15). Besides, for any \(0\leq t\leq\widetilde{t}-1\), we also have the following upper bound of\(F_{y_{i}}(\mathbf{W}^{(t)}_{y_{i}},\mathbf{x}_{i})\):

\[F_{y_{i}}(\mathbf{W}^{(t)}_{y_{i}},\mathbf{x}_{i}) =\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}^{(t)}_{y_{i},r },\mathbf{x}_{i}\rangle)\] \[=\frac{1}{m}\sum_{r=1}^{m}\sigma\bigg{(}\langle\mathbf{w}^{(0)}_{ y_{i},r},\mathbf{x}_{i}\rangle+\sum_{i^{\prime}=1}^{n}\rho^{(t)}_{y_{i},r,i^{ \prime}}\|\mathbf{x}_{i^{\prime}}\|_{2}^{-2}\cdot\langle\mathbf{x}_{i^{\prime }},\mathbf{x}_{i}\rangle\bigg{)}\] \[\leq\frac{1}{m}\sum_{r=1}^{m}\sigma\bigg{(}\rho^{(t)}_{y_{i},r,i} +\sum_{i^{\prime}\neq i}|\rho^{(t)}_{y_{i},r,i^{\prime}}|R^{-2}_{\min}p+\beta \bigg{)}\] \[=\frac{1}{m}\sum_{r=1}^{m}\bigg{(}\rho^{(t)}_{y_{i},r,i}+\sum_{i^ {\prime}\neq i}|\rho^{(t)}_{y_{i},r,i^{\prime}}|R^{-2}_{\min}p+\beta\bigg{)}\] \[=\frac{1}{m}\sum_{r=1}^{m}\rho^{(t)}_{y_{i},r,i}+\sum_{i^{\prime }\neq i}\bigg{(}\frac{1}{m}\sum_{r=1}^{m}\rho^{(t)}_{y_{i},r,i^{\prime}}\bigg{)} R^{-2}_{\min}p+\beta\] \[\leq\frac{1+\gamma^{-2}c_{3}^{-1}R^{-2}_{\min}pn}{m}\sum_{r=1}^{ m}\rho^{(t)}_{y_{i},r,i}+\beta,\]

where the first inequality is by triangle inequality and the definition of \(\beta,p,R_{min}\); the second inequality is by the induction hypothesis (C.15). On the other hand, for any \(0\leq t\leq t\), we can give following upper and lower bounds for \(F_{-y_{i}}(\mathbf{W}^{(t)}_{-y_{i}},\mathbf{x}_{i})\) by applying similar arguments like (C.16) and (C.17):

\[F_{-y_{i}}(\mathbf{W}^{(t)}_{-y_{i}},\mathbf{x}_{i}) \geq\frac{\gamma}{m}\sum_{r=1}^{m}\langle\mathbf{w}^{(t)}_{-y_{i},r},\mathbf{x}_{i}\rangle\] \[\geq\frac{\gamma}{m}\sum_{r=1}^{m}\bigg{(}\rho^{(t)}_{-y_{i},r,i} -\sum_{i^{\prime}\neq i}|\rho^{(t)}_{-y_{i},r,i^{\prime}}|R^{-2}_{\min}p-\beta \bigg{)},\] \[\geq\frac{\gamma(1+\gamma^{-2}c_{3}^{-1}R^{-2}_{\min}pn)}{m}\sum_ {r=1}^{m}\rho^{(t)}_{-y_{i},r,i}-\gamma\beta,\] (C.18)

and

\[F_{-y_{i}}(\mathbf{W}^{(t)}_{-y_{i}},\mathbf{x}_{i}) \leq\frac{1}{m}\sum_{r=1}^{m}\sigma\bigg{(}\rho^{(t)}_{-y_{i},r,i} +\sum_{i^{\prime}\neq i}|\rho^{(t)}_{-y_{i},r,i^{\prime}}|R^{-2}_{\min}p+\beta \bigg{)}\] \[\leq\frac{1}{m}\sum_{r=1}^{m}\bigg{[}\sigma(\rho^{(t)}_{-y_{i},r, i})+\sigma\Big{(}\sum_{i^{\prime}\neq i}|\rho^{(t)}_{-y_{i},r,i^{\prime}}|R^{-2}_{ \min}p\Big{)}+\sigma(\beta)\bigg{]}\] \[=\frac{\gamma}{m}\sum_{r=1}^{m}\rho^{(t)}_{-y_{i},r,i}+\sum_{i^{ \prime}\neq i}\bigg{(}\frac{1}{m}\sum_{r=1}^{m}|\rho^{(t)}_{-y_{i},r,i}|\bigg{)}+\beta\] \[=\frac{\gamma(1-\gamma^{-3}c_{3}^{-1}R^{-2}_{\min}pn)}{m}\sum_{r=1 }^{m}\rho^{(t)}_{-y_{i},r,i}+\beta,\] (C.19)

where the second inequality is by a property of leaky ReLU function that \(\sigma(a+b)\leq\sigma(a)+\sigma(b),\forall a,b\in\mathbb{R}\).

Next, we can bound \(|\ell_{i}^{\prime(t)}|\) for \(0\leq t\leq\widetilde{t}-1\):

\[|\ell_{i}^{\prime(t)}|\] \[=\frac{1}{1+\exp\{F_{y_{i}}(\mathbf{W}_{y_{i}}^{(t)},\mathbf{x}_{i })-F_{-y_{i}}(\mathbf{W}_{-y_{i}}^{(t)},\mathbf{x}_{i})\}}\] \[\leq\exp\{-F_{y_{i}}(\mathbf{W}_{y_{i}}^{(t)},\mathbf{x}_{i})+F_{ -y_{i}}(\mathbf{W}_{-y_{i}}^{(t)},\mathbf{x}_{i})\}\] \[\leq\exp\bigg{\{}-\frac{1-\gamma^{-2}c_{3}^{-1}R_{\min}^{-2}pn}{m }\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)}+\frac{\gamma(1-\gamma^{-3}c_{3}^{-1}R_{ \min}^{-2}pn)}{m}\sum_{r=1}^{m}\rho_{-y_{i},r,i}^{(t)}+2\beta\bigg{\}},\] (C.20)

where the second inequality is by (C.16) and (C.19). And

\[|\ell_{i}^{\prime(t)}|\] \[=\frac{1}{1+\exp\{F_{y_{i}}(\mathbf{W}_{y_{i}}^{(t)},\mathbf{x}_ {i})-F_{-y_{i}}(\mathbf{W}_{-y_{i}}^{(t)},\mathbf{x}_{i})\}}\] \[\geq\frac{1}{1+\exp\Big{\{}\frac{1+\gamma^{-2}c_{3}^{-1}R_{\min} ^{-2}pn}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)}-\frac{\gamma(1+\gamma^{-2}c_{ 3}^{-1}R_{\min}^{-2}pn)}{m}\sum_{r=1}^{m}\rho_{-y_{i},r,i}^{(t)}+(\gamma+1) \beta\Big{\}}}\] \[\geq\frac{1}{2}\exp\bigg{\{}-\frac{1+\gamma^{-2}c_{3}^{-1}R_{\min }^{-2}pn}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)}+\frac{\gamma(1+\gamma^{-2}c_{ 3}^{-1}R_{\min}^{-2}pn)}{m}\sum_{r=1}^{m}\rho_{-y_{i},r,i}^{(t)}-(\gamma+1) \beta\bigg{\}},\] (C.21)

where the first inequality is by (C.17) and (C.18); the last inequality is by \(1/(1+\exp(z))\geq\exp(-z)/2\) if \(z\geq 0\). By (C.20), we can get for \(0\leq t\leq\widetilde{t}-1\) that

\[|\ell_{i}^{\prime(t)}| \leq\exp\bigg{\{}-\frac{1-\gamma^{-2}c_{3}^{-1}R_{\min}^{-2}pn}{m }\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)}+2\beta\bigg{\}},\] (C.22) \[|\ell_{i}^{\prime(t)}| \leq\exp\bigg{\{}\frac{\gamma(1-\gamma^{-3}c_{3}^{-1}R_{\min}^{- 2}pn)}{m}\sum_{r=1}^{m}\rho_{-y_{i},r,i}^{(t)}+2\beta\bigg{\}}.\] (C.23)

By (C.21) and \(\gamma\rho_{y_{i},r,i}^{(t)}\leq|\rho_{-y_{i},r,i}^{(t)}|\leq\gamma^{-4}\rho_{ y_{i},r,i}^{(t)}\), we can get for \(0\leq t\leq\widetilde{t}-1\) that

\[|\ell_{i}^{\prime(t)}| \geq\frac{1}{2}\exp\bigg{\{}-\frac{2(1+\gamma^{-2}c_{3}^{-1}R_{ \min}^{-2}pn)}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)}-(\gamma+1)\beta\bigg{\}},\] (C.24) \[|\ell_{i}^{\prime(t)}| \geq\frac{1}{2}\exp\bigg{\{}\frac{(\gamma^{-1}+\gamma)(1+\gamma^{ -2}c_{3}^{-1}R_{\min}^{-2}pn)}{m}\sum_{r=1}^{m}\rho_{-y_{i},r,i}^{(t)}-(\gamma+ 1)\beta\bigg{\}}\] \[\geq\frac{1}{2}\exp\bigg{\{}\frac{2(1+\gamma^{-2}c_{3}^{-1}R_{ \min}^{-2}pn)}{\gamma m}\sum_{r=1}^{m}\rho_{-y_{i},r,i}^{(t)}-(\gamma+1)\beta \bigg{\}}.\] (C.25)

By (5.4), (5.5) and \(\sigma^{\prime}\in[\gamma,1]\), we have for \(0\leq t\leq\widetilde{t}-1\) that

\[\rho_{y_{i},r,i}^{(t+1)} \leq\rho_{y_{i},r,i}^{(t)}+\frac{\eta}{nm}\cdot|\ell_{i}^{\prime (t)}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\] (C.26) \[\rho_{y_{i},r,i}^{(t+1)} \geq\rho_{y_{i},r,i}^{(t)}+\frac{\gamma\eta}{nm}\cdot|\ell_{i}^{ \prime(t)}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\] \[|\rho_{-y_{i},r,i}^{(t+1)}| \leq|\rho_{-y_{i},r,i}^{(t)}|+\frac{\eta}{nm}\cdot|\ell_{i}^{\prime (t)}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\] \[|\rho_{-y_{i},r,i}^{(t+1)}| \geq|\rho_{-y_{i},r,i}^{(t)}|+\frac{\gamma\eta}{nm}\cdot|\ell_{i}^{ \prime(t)}|\cdot\|\mathbf{x}_{i}\|_{2}^{2}.\]By plugging (C.22), (C.24), (C.23) and (C.25) into (C.26), we have for \(0\leq t\leq\widetilde{t}-1\) that

\[\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t+1)}\leq\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)}+\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}e^{2\beta}}{n}\cdot\exp\bigg{\{} -\frac{1-\gamma^{-2}c_{3}^{-1}R_{\min}^{-2}pn}{m}\sum_{r=1}^{m}\rho_{y_{i},r, i}^{(t)}\bigg{\}},\] (C.27) \[\sum_{r=1}^{m}|\rho_{-y_{i},r,i}^{(t+1)}|\leq\sum_{r=1}^{m}|\rho_ {-y_{i},r,i}^{(t)}|+\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}e^{2\beta}}{n}\cdot\exp \bigg{\{}-\frac{\gamma(1-\gamma^{-3}c_{3}^{-1}R_{\min}^{-2}pn)}{m}\sum_{r=1}^{ m}|\rho_{-y_{i},r,i}^{(t)}|\bigg{\}},\] (C.28) \[\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t+1)}\geq\sum_{r=1}^{m}\rho_{y_ {i},r,i}^{(t)}+\frac{\gamma\eta\|\mathbf{x}_{i}\|_{2}^{2}e^{-(\gamma+1)\beta} }{2n}\cdot\exp\bigg{\{}-\frac{2(1+\gamma^{-2}c_{3}^{-1}R_{\min}^{-2}pn)}{m} \sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)}\bigg{\}},\] (C.29) \[\sum_{r=1}^{m}|\rho_{-y_{i},r,i}^{(t+1)}|\geq\sum_{r=1}^{m}|\rho_ {-y_{i},r,i}^{(t)}|+\frac{\gamma\eta\|\mathbf{x}_{i}\|_{2}^{2}e^{-(\gamma+1) \beta}}{2n}\cdot\exp\bigg{\{}-\frac{2(1+\gamma^{-2}c_{3}^{-1}R_{\min}^{-2}pn)} {\gamma m}\sum_{r=1}^{m}|\rho_{-y_{i},r,i}^{(t)}|\bigg{\}}.\] (C.30)

By applying Lemma H.1 to (C.27) and taking

\[x_{t}=\frac{1-\gamma^{-2}c_{3}^{-1}R_{\min}^{-2}pn}{m}\sum_{r=1}^{m}\rho_{y_{i },r,i}^{(t)},\]

we can get for \(0\leq t\leq\widetilde{t}\) that

\[\frac{1}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)} \leq c_{4}^{-1}\log\bigg{(}1+\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2} c_{4}e^{2\beta}}{nm}\exp\bigg{\{}\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}c_{4}e^{2 \beta}}{nm}\bigg{\}}\cdot t\bigg{)}\] (C.31) \[\leq c_{4}^{-1}\log\bigg{(}1+\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2} c_{4}e^{2\beta}}{nm}\cdot t\bigg{)},\]

where \(c_{4}:=1-\gamma^{-2}c_{3}^{-1}R_{\min}^{-2}pn\) and the last inequality is by \(\eta\leq(CR_{\max}^{2}/nm)^{-1}\) and \(C\) is a sufficiently large constant.

By applying Lemma H.1 to (C.28) and taking

\[x_{t}=\frac{\gamma(1-\gamma^{-3}c_{3}^{-1}R_{\min}^{-2}pn)}{m}\sum_{r=1}^{m}| \rho_{-y_{i},r,i}^{(t)}|,\]

we can get for \(0\leq t\leq\widetilde{t}\) that

\[\frac{1}{m}\sum_{r=1}^{m}|\rho_{-y_{i},r,i}^{(t)}| \leq c_{5}^{-1}\gamma^{-1}\log\bigg{(}1+\frac{\gamma\eta\|\mathbf{ x}_{i}\|_{2}^{2}c_{5}e^{2\beta}}{nm}\exp\bigg{\{}\frac{\gamma\eta\|\mathbf{x}_{i}\|_{2}^{2 }c_{5}e^{2\beta}}{nm}\bigg{\}}\cdot t\bigg{)}\] (C.32) \[\leq c_{5}^{-1}\gamma^{-1}\log\bigg{(}1+\frac{\gamma\eta\| \mathbf{x}_{i}\|_{2}^{2}c_{5}e^{2\beta}}{nm}\cdot t\bigg{)},\]

where \(c_{5}:=1-\gamma^{-3}c_{3}^{-1}R_{\min}^{-2}pn\) and the last inequality is by \(\eta\leq(CR_{\max}^{2}/nm)^{-1}\) and \(C\) is a sufficiently large constant.

By applying Lemma H.2 to (C.29) and taking

\[x_{t}=\frac{2(1+\gamma^{-2}c_{3}^{-1}R_{\min}^{-2}pn)}{m}\sum_{r=1}^{m}\rho_{y_ {i},r,i}^{(t)},\]

we can get

\[\frac{1}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)}\geq(2c_{6})^{-1}\log\bigg{(}1+ \frac{\gamma\eta\|\mathbf{x}_{i}\|_{2}^{2}c_{6}e^{-(\gamma+1)\beta}}{nm}\cdot t \bigg{)},\] (C.33)

where \(c_{6}:=1+\gamma^{-2}c_{3}^{-1}R_{\min}^{-2}pn\).

By applying Lemma H.2 to (C.30) and taking

\[x_{t}=\frac{2(1+\gamma^{-4}c_{3}^{-1}R_{\min}^{-2}pn)}{\gamma m}\sum_{r=1}^{m}| \rho_{-y_{i},r,i}^{(t)}|,\]

we can get

\[\frac{1}{m}\sum_{r=1}^{m}|\rho_{-y_{i},r,i}^{(t)}|\geq(2c_{6})^{-1}\gamma\log \bigg{(}1+\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}c_{6}e^{-(\gamma+1)\beta}}{nm} \cdot t\bigg{)},\] (C.34)

where \(c_{6}:=1+\gamma^{-2}c_{3}^{-1}R_{\min}^{-2}pn\).

In order to apply Lemma H.4 (requiring \(b>a\)), we loosen the bounds in (C.31), (C.32), (C.33) and (C.34) as follows:

\[\frac{1}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)} \leq c_{4}^{-1}\gamma^{-1}\log\bigg{(}1+\frac{\gamma\eta R_{ \max}^{2}c_{5}e^{2\beta}}{nm}\cdot t\bigg{)},\forall\,0\leq t\leq\widetilde{t},\] (C.35) \[\frac{1}{m}\sum_{r=1}^{m}|\rho_{-y_{i},r,i}^{(t)}| \leq c_{4}^{-1}\gamma^{-1}\log\bigg{(}1+\frac{\gamma\eta R_{ \max}^{2}c_{5}e^{2\beta}}{nm}\cdot t\bigg{)},\forall\,0\leq t\leq\widetilde{t},\] (C.36) \[\frac{1}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)} \geq(2c_{6})^{-1}\gamma\log\bigg{(}1+\frac{\eta R_{\min}^{2}c_{6} e^{-(\gamma+1)\beta}}{nm}\cdot t\bigg{)},\forall\,0\leq t\leq\widetilde{t},\] (C.37) \[\frac{1}{m}\sum_{r=1}^{m}|\rho_{-y_{i},r,i}^{(t)}| \geq(2c_{6})^{-1}\gamma\log\bigg{(}1+\frac{\eta R_{\min}^{2}c_{6} e^{-(\gamma+1)\beta}}{nm}\cdot t\bigg{)},\forall\,0\leq t\leq\widetilde{t},\] (C.38)

where (C.35) is by Bernoulli's inequality that \(1+\gamma^{-1}x\leq(1+x)^{\gamma^{-1}}\) for every real number \(0\leq r\leq 1\) and \(x\geq-1\); (C.37) is by Bernoulli's inequality that \(1+\gamma x\geq(1+x)^{\gamma}\) for every real number \(0\leq r\leq 1\) and \(x\geq-1\). If \(R_{\min}^{2}c_{6}e^{-(\gamma+1)\beta}\geq\gamma R_{\max}^{2}c_{5}e^{2\beta}\), we have

\[\frac{1}{m}\sum_{r=1}^{m}|\rho_{j,r,i}^{(t)}|\geq\frac{\gamma^{2}(2c_{6})^{-1} c_{4}}{m}\sum_{r=1}^{m}|\rho_{j^{\prime},r,i^{\prime}}^{(t)}|.\] (C.39)

If \(R_{\min}^{2}c_{6}e^{-(\gamma+1)\beta}<\gamma R_{\max}^{2}c_{5}e^{2\beta}\), by Lemma H.4, we have

\[\frac{\min\{\frac{1}{m}\sum_{r=1}^{m}\rho_{y_{i},r,i}^{(t)},\frac {1}{m}\sum_{r=1}^{m}|\rho_{-y_{i},r,i}^{(t)}|\}}{\max\{\frac{1}{m}\sum_{r=1}^{m }\rho_{y_{i},r,i^{\prime}}^{(t)},\frac{1}{m}\sum_{r=1}^{m}|\rho_{-y_{i},r,i^{ \prime}}^{(t)}|\}}\] \[\geq\gamma^{2}(2c_{6})^{-1}c_{4}\cdot\frac{\log\big{(}1+\frac{ \eta R_{\max}^{2}c_{6}e^{-(\gamma+1)\beta}}{nm}\cdot t\big{)}}{\log\big{(}1+ \frac{\gamma\eta R_{\max}^{2}c_{5}e^{2\beta}}{nm}\cdot t\big{)}}\] \[\geq\gamma^{2}(2c_{6})^{-1}c_{4}\cdot\frac{\log\big{(}1+\frac{ \eta R_{\max}^{2}c_{6}e^{-(\gamma+1)\beta}}{nm}\cdot T_{1}\big{)}}{\log\big{(}1+ \frac{\gamma\eta R_{\max}^{2}c_{5}e^{2\beta}}{nm}\cdot T_{1}\big{)}}\] \[\geq\gamma^{2}(2c_{6})^{-1}c_{4}\cdot\frac{\log(1+R^{-2}c_{6}e^{- (\gamma+1)\beta}C^{\prime})}{\log(1+\gamma c_{5}e^{2\beta}C^{\prime})}.\]

Therefore, we can get for \(0\leq t\leq\widetilde{t}\) that

\[\sum_{r=1}^{m}|\rho_{j,r,i}^{(t)}|\geq\gamma^{2}c_{3}\sum_{r=1}^{m}|\rho_{j^{ \prime},r,i^{\prime}}^{(t)}|,\forall j,j^{\prime}\in\{\pm 1\},\forall i,i^{ \prime}\in[n],\] (C.40)

as long as

\[c_{3}\leq(2c_{6})^{-1}c_{4}\cdot\min\bigg{\{}1,\frac{\log(1+R^{-2}c_{6}e^{-( \gamma+1)\beta}C^{\prime})}{\log(1+\gamma c_{5}e^{2\beta}C^{\prime})}\bigg{\}}.\]This condition holds under the following conditions:

\[\gamma^{-3}c_{3}^{-1}R_{\min}^{-2}pn\leq\frac{1}{2}\quad\Longrightarrow \quad c_{4},c_{5}\geq\frac{1}{2},c_{6}\leq\frac{3}{2},\] \[c_{3}=\frac{1}{6}\min\bigg{\{}1,\frac{\log(1+R^{-2}e^{-(\gamma+1) \beta}C^{\prime})}{\log(1+\gamma e^{2\beta}C^{\prime})}\bigg{\}}.\]

This implies that induction hypothesis (C.15) holds for \(t=\widetilde{t}\). 

**Lemma C.4** (Implication of Lemma C.1).: Under the same condition as Theorem 4.1, if (C.8) hold for time \(t\), then we have that

\[|\rho_{j,r,i}^{(t)}|\geq c_{1}\gamma^{4}|\rho_{j^{\prime},r^{\prime},i^{ \prime}}^{(t)}|,\]

where \(c_{1}\) is the same constant as defined in Lemma C.1.

Proof of Lemma C.4.: By \(\sigma^{\prime}\in[\gamma,1]\), (5.4) and (5.5), we have

\[|\rho_{j,r,i}^{(t+1)}|\geq|\rho_{j,r,i}^{(t)}|+\frac{\gamma\eta}{ nm}\cdot|\ell_{i}^{\prime(t)}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\forall j\in\{ \pm 1\},\forall r\in[m],\forall i\in[n],\] \[|\rho_{j,r,i}^{(t+1)}|\leq|\rho_{j,r,i}^{(t)}|+\frac{\eta}{nm} \cdot|\ell_{i}^{\prime(t)}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\forall j\in\{\pm 1 \},\forall r\in[m],\forall i\in[n].\]

Thus, we have

\[|\rho_{j,r,i}^{(t)}|\geq\frac{\gamma\eta\|\mathbf{x}_{i}\|_{2}^{2 }}{nm}\cdot\sum_{s=1}^{t-1}|\ell_{i}^{\prime(t)}|,\forall j\in\{\pm 1\}, \forall r\in[m],\forall i\in[n],\] \[|\rho_{j,r,i}^{(t)}|\leq\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}}{nm} \cdot\sum_{s=1}^{t-1}|\ell_{i}^{\prime(t)}|,\forall j\in\{\pm 1\}, \forall r\in[m],\forall i\in[n].\]

Therefore, \(|\rho_{j,r,i}^{(t)}|\geq\gamma|\rho_{j^{\prime},r^{\prime},i}^{(t)}|\) for any \(j,j^{\prime}\in\{\pm 1\}\), \(r^{\prime},r\in[m]\) and \(i\in[n]\), and hence

\[m|\rho_{j,r,i}^{(t)}|\geq\gamma\sum_{r=1}^{m}|\rho_{j,r,i}^{(t)}|,\] (C.41) \[\sum_{r=1}^{m}|\rho_{j^{\prime},r,i^{\prime}}^{(t)}|\geq m\gamma |\rho_{j^{\prime},r^{\prime},i^{\prime}}^{(t)}|.\]

Plugging (C.41) back into (C.8) completes the proof. 

**Lemma C.5**.: Let \(T_{1}\) be defined in Lemma C.2. Every neuron will never change its activation pattern after time \(T_{1}\), i.e.,

\[\mathrm{sign}(\langle\mathbf{w}_{j,r}^{(t)},\mathbf{x}_{i}\rangle)=\mathrm{ sign}(\langle\mathbf{w}_{j,r}^{(T_{1})},\mathbf{x}_{i}\rangle),\]

for any \(t\geq T_{1}\), \(j\in\{\pm 1\}\) and \(r\in[m]\). Moreover, it holds that

\[\mathrm{sign}(\langle\mathbf{w}_{j,r}^{(t)},\mathbf{x}_{i}\rangle)=jy_{i},\] (C.42)

for any \(t\geq T_{1}\), \(j\in\{\pm 1\}\) and \(r\in[m]\).

Proof of Lemma C.5.: For \(j=y_{i}\) and \(t\geq 0\), we have \(\underline{\rho}_{j,r,i}^{(t)}=0\), and so

\[\langle\mathbf{w}_{j,r}^{(t)},\mathbf{x}_{i}\rangle =\langle\mathbf{w}_{j,r}^{(0)},\mathbf{x}_{i}\rangle+\sum_{i^{ \prime}=1}^{n}\rho_{j,r,i^{\prime}}^{(t)}\|\mathbf{x}_{i^{\prime}}\|_{2}^{-2} \cdot\langle\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle\] \[=\langle\mathbf{w}_{j,r}^{(0)},\mathbf{x}_{i}\rangle+\overline{ \rho}_{j,r,i}^{(t)}+\sum_{i^{\prime}\neq i}\rho_{j,r,i^{\prime}}^{(t)}\| \mathbf{x}_{i^{\prime}}\|_{2}^{-2}\cdot\langle\mathbf{x}_{i^{\prime}},\mathbf{ x}_{i}\rangle\] \[\geq\overline{\rho}_{j,r,i}^{(t)}-\sum_{i^{\prime}\neq i}|\rho_{j,r,i^{\prime}}^{(t)}|R_{\min}^{-2}p-\beta\]\[\geq\overline{\rho}^{(t)}_{j,r,i}-\gamma^{-4}c_{1}^{-1}\overline{\rho}^{(t)}_{j,r, i}R_{\min}^{-2}pn-\beta\] \[=(1-\gamma^{-4}c_{1}^{-1}R_{\min}^{-2}pn)\cdot\overline{\rho}^{(t) }_{j,r,i}-\beta,\]

where the first inequality is by triangle inequality; the second inequality is by \(|\rho^{(t)}_{j,r,i^{\prime}}|\leq\gamma^{-4}c_{1}^{-1}\overline{\rho}^{(t)}_{y _{i},r,i}\) from Lemma C.1 and Lemma C.4.

By (C.13), we have for \(t\geq T_{1}\) that

\[\overline{\rho}^{(t)}_{y_{i},r,i}\geq\frac{\widetilde{c}\gamma\eta R_{\min}^ {2}T_{1}}{nm}=C^{\prime}\widetilde{c}\gamma R_{\min}^{2}R_{\max}^{-2}.\] (C.43)

Therefore, by (C.5), (C.6) and (C.43), we know that

\[(1-\gamma^{-1}c_{1}^{-4}R_{\min}^{-2}pn)\cdot\overline{\rho}^{(t)}_{y_{i},r,i }>\beta,\forall\,r\in[m],i\in[n].\]

and thus \(\mathrm{sign}(\langle\mathbf{w}^{(t)}_{j,r},\mathbf{x}_{i}\rangle)=1\) for any \(r\in[m],i\in[n],j=y_{i}\).

For \(j\neq y_{i}\) and any \(t\geq 0\), we have \(\overline{\rho}^{(t)}_{j,r,i}=0\), and so

\[\langle\mathbf{w}^{(t)}_{j,r},\mathbf{x}_{i}\rangle =\langle\mathbf{w}^{(0)}_{j,r},\mathbf{x}_{i}\rangle+\sum_{i^{ \prime}=1}^{n}\rho^{(t)}_{j,r,i^{\prime}}\|\mathbf{x}_{i^{\prime}}\|_{2}^{-2} \cdot\langle\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle\] \[=\underline{\rho}^{(t)}_{j,r,i}+\sum_{i^{\prime}\neq i}\rho^{(t)} _{j,r,i^{\prime}}\|\mathbf{x}_{i^{\prime}}\|_{2}^{-2}\cdot\langle\mathbf{x}_{ i^{\prime}},\mathbf{x}_{i}\rangle\] \[\leq\underline{\rho}^{(t)}_{j,r,i}+\sum_{i^{\prime}\neq i}|\rho^ {(t)}_{j,r,i^{\prime}}|R_{\min}^{-2}p+\beta\] \[\leq\underline{\rho}^{(t)}_{j,r,i}-\gamma^{-1}c_{2}^{-1} \underline{\rho}^{(t)}_{j,r,i}R_{\min}^{-2}pn-\beta\] \[=(1-\gamma^{-1}c_{2}^{-1}R_{\min}^{-2}pn)\underline{\rho}^{(t)}_ {j,r,i}-\beta,\]

where the first inequality is by triangle inequality; the second inequality is by \(|\rho^{(t)}_{j,r,i^{\prime}}|\leq\gamma^{-4}c_{1}^{-1}|\underline{\rho}^{(t)} _{-y_{i},r,i}|\) from Lemma C.1 and Lemma C.4.

By (C.13), we have

\[|\underline{\rho}^{(t)}_{-y_{i},r,i}|\geq\frac{\widetilde{c}\gamma\eta R_{\min }^{2}T_{1}}{nm}=C^{\prime}\widetilde{c}\gamma R_{\min}^{2}R_{\max}^{-2}.\] (C.44)

Therefore, by (C.5), (C.6) and (C.44), we know that

\[(1-\gamma^{-4}c_{1}^{-1}R_{\min}^{-2}pn)\cdot|\underline{\rho}^{(t)}_{-y_{i}, r,i}|>\beta,\forall\,r\in[m],i\in[n],\]

and thus \(\mathrm{sign}(\langle\mathbf{w}^{(t)}_{j,r},\mathbf{x}_{i}\rangle)=-1\) for \(j\neq y_{i}\), which completes the proof. 

## Appendix D Stable Rank of Leaky ReLU Network

In this section, we consider the properties of stable rank of the weight matrix \(\mathbf{W}^{(t)}\) found by gradient descent at time \(t\), defined as \(\|\mathbf{W}^{(t)}\|_{F}^{2}/\|\mathbf{W}^{(t)}\|_{2}^{2}\), the square of the ratio of the Frobenius norm to the spectral norm of \(\mathbf{W}^{(t)}\). Given Lemma C.5, we have following coefficient update rule for \(t\geq T_{1}\) where \(T_{1}\) is defined in Lemma C.2:

\[\overline{\rho}^{(t+1)}_{y_{i},r,i}=\overline{\rho}^{(t)}_{y_{i},r,i}+\frac{\eta}{nm}\cdot|\ell^{(t)}_{i}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\] (D.1) \[\underline{\rho}^{(t+1)}_{-y_{i},r,i}=\underline{\rho}^{(t)}_{-y_{ i},r,i}-\frac{\gamma\eta}{nm}\cdot|\ell^{(t)}_{i}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\] (D.2)

where

\[|\ell^{(t)}_{i}|=\frac{1}{1+\exp\{F_{y_{i}}(\mathbf{W}^{(t)}_{y_{i}},\mathbf{ x}_{i})-F_{-y_{i}}(\mathbf{W}^{(t)}_{-y_{i}},\mathbf{x}_{i})\}}.\]

Based on (D.1) and (D.2), we first introduce the following helpful lemmas.

**Lemma D.1**.: Let \(T_{1}\) be defined in Lemma C.3. For any \(r,r^{\prime}\in[m]\), \(i\in[n]\) and \(t\leq T_{1}\),

\[|\overline{\rho}^{(t)}_{y_{i},r,i}-\overline{\rho}^{(t)}_{y_{i},r^{\prime},i}| \leq C^{\prime},|\underline{\rho}^{(t)}_{-y_{i},r,i}-\underline{\rho}^{(t)}_{- y_{i},r^{\prime},i}|\leq C^{\prime}.\] (D.3)

Proof of Lemma D.1.: By (C.14), we can get

\[|\underline{\rho}^{(t)}_{j,r,i}|\leq\frac{\eta R_{\max}^{2}T_{1}}{nm}=C^{\prime},\]

for \(t\leq T_{1}\). Notice that

\[|\overline{\rho}^{(t)}_{y_{i},r,i}-\overline{\rho}^{(t)}_{y_{i}, r^{\prime},i}|\leq\max\{|\overline{\rho}^{(t)}_{y_{i},r,i}|,|\overline{\rho}^{(t) }_{y_{i},r^{\prime},i}|\},\] \[|\underline{\rho}^{(t)}_{-y_{i},r,i}-\underline{\rho}^{(t)}_{-y_ {i},r^{\prime},i}|\leq\max\{|\underline{\rho}^{(t)}_{-y_{i},r,i}|,|\underline{ \rho}^{(t)}_{-y_{i},r^{\prime},i}|\},\]

which completes the proof. 

**Lemma D.2**.: Let \(T_{1}\) be defined in Lemma C.3. For any \(r,r^{\prime}\in[m]\), \(i\in[n]\) and \(t\geq T_{1}\),

\[|\overline{\rho}^{(t)}_{y_{i},r,i}-\overline{\rho}^{(t)}_{y_{i},r^{\prime},i} |\leq C^{\prime},|\underline{\rho}^{(t)}_{-y_{i},r,i}-\underline{\rho}^{(t)}_ {-y_{i},r^{\prime},i}|\leq C^{\prime}.\]

Proof of Lemma D.2.: By (D.1) and (D.2), we can get for any \(r\in[m]\), \(i\in[n]\) and \(t\geq T_{1}\) that

\[\overline{\rho}^{(t)}_{y_{i},r,i}=\overline{\rho}^{(T_{1})}_{y_{i},r,i}+\frac {\eta}{nm}\sum_{s=T_{1}}^{t-1}|\ell^{\prime(t)}_{i}|\cdot\|\mathbf{x}_{i}\|_{ 2}^{2},\]

\[\underline{\rho}^{(t)}_{-y_{i},r,i}=\underline{\rho}^{(T_{1})}_{-y_{i},r,i}+ \frac{\eta}{nm}\sum_{s=T_{1}}^{t-1}|\ell^{\prime(t)}_{i}|\cdot\|\mathbf{x}_{i} \|_{2}^{2}.\]

Since \(\overline{\rho}^{(t)}_{y_{i},r,i},\overline{\rho}^{(t)}_{y_{i},r^{\prime},i}\) possess the same increment and \(\underline{\rho}^{(t)}_{-y_{i},r,i}\underline{\rho}^{(t)}_{-y_{i},r^{\prime},i}\) possess the same increment, we have

\[\overline{\rho}^{(t)}_{y_{i},r,i}-\overline{\rho}^{(t)}_{y_{i},r^ {\prime},i}=\overline{\rho}^{(T_{1})}_{y_{i},r,i}-\overline{\rho}^{(T_{1})}_{y_ {i},r^{\prime},i},\] \[\underline{\rho}^{(t)}_{-y_{i},r,i}-\underline{\rho}^{(t)}_{-y_ {i},r^{\prime},i}=\underline{\rho}^{(T_{1})}_{-y_{i},r,i}-\underline{\rho}^{(T _{1})}_{-y_{i},r^{\prime},i}.\]

Notice that

\[\max_{i,r,r^{\prime}}\{|\overline{\rho}^{(T_{1})}_{y_{i},r,i}- \overline{\rho}^{(T_{1})}_{y_{i},r^{\prime},i}|,|\underline{\rho}^{(T_{1})}_{- y_{i},r,i}-\underline{\rho}^{(T_{1})}_{-y_{i},r^{\prime},i}|\}\leq\max_{i,r,r^{ \prime}}\{|\overline{\rho}^{(T_{1})}_{y_{i},r,i}|,|\underline{\rho}^{(T_{1})}_{- y_{i},r,i}|\}\leq C^{\prime},\]

which completes the proof. 

**Lemma D.3**.: Let \(T_{1}\) be defined in Lemma C.3. For \(t\geq T_{1}\), it holds that

\[\overline{\rho}^{(t)}_{y_{i},r,i}-\overline{\rho}^{(T_{1})}_{y_{i },r,i}=\overline{\rho}^{(t)}_{y_{i},r^{\prime},i}-\overline{\rho}^{(T_{1})}_{y_ {i},r^{\prime},i},\] \[\underline{\rho}^{(t)}_{-y_{i},r,i}-\underline{\rho}^{(T_{1})}_{-y_ {i},r,i}=\underline{\rho}^{(t)}_{-y_{i},r^{\prime},i}-\underline{\rho}^{(T_{1})} _{-y_{i},r^{\prime},i},\] \[\overline{\rho}^{(t)}_{y_{i},r,i}-\overline{\rho}^{(T_{1})}_{y_{i },r,i}=\big{(}|\underline{\rho}^{(t)}_{-y_{i},r^{\prime},i}|-|\underline{\rho}^{( T_{1})}_{-y_{i},r^{\prime},i}|\big{)}/\gamma,\]

for any \(i\in[n]\) and \(r,r^{\prime}\in[m]\).

Proof of Lemma D.3.: By Lemma C.3 about the activation pattern after time \(T_{1}\), we can get

\[\overline{\rho}^{(t+1)}_{y_{i},r,i}=\overline{\rho}^{(t)}_{y_{i}, r,i}+\frac{\eta}{nm}\cdot|\ell^{\prime(t)}_{i}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\] (D.4) \[\underline{\rho}^{(t+1)}_{-y_{i},r,i}=\underline{\rho}^{(t)}_{-y_ {i},r,i}-\frac{\gamma\eta}{nm}\cdot|\ell^{\prime(t)}_{i}|\cdot\|\mathbf{x}_{i}\|_{ 2}^{2},\] (D.5)

for \(t\geq T_{1}\). Recursively using (D.4) and (D.5) \(t-T_{1}\) times, we can get

\[\overline{\rho}^{(t)}_{y_{i},r,i}-\overline{\rho}^{(T_{1})}_{y_{i},r,i}=\frac{ \eta\|\mathbf{x}_{i}\|_{2}^{2}}{nm}\sum_{s=T_{1}}^{t-1}|\ell^{\prime(s)}_{i}|,\]\[|\underline{\rho}_{-y_{i},r,i}^{(t)}|-|\underline{\rho}_{-y_{i},r,i}^{(T_{1})}|= \frac{\gamma\eta\|\mathbf{x}_{i}\|_{2}}{nm}\sum_{s=T_{1}}^{t-1}|\ell_{i}^{r(s)}|.\]

This indicates that for different \(r,r^{\prime}\in[m]\), \(\overline{\rho}_{y_{i},r,i}^{(t)}-\overline{\rho}_{y_{i},r,i}^{(T_{1})}\) and \(\overline{\rho}_{y_{i},r^{\prime},i}^{(t)}-\overline{\rho}_{y_{i},r^{\prime},i}^{(T_{1})}\) are the same, whereas \(\gamma(|\underline{\rho}_{-y_{i},r,i}^{(t)}|-|\underline{\rho}_{-y_{i},r,i}^ {(T_{1})}|)\) and \(\overline{\rho}_{y_{i},r^{\prime},i}^{(t)}-\overline{\rho}_{y_{i},r^{\prime},i}^{(T_{1})}\) are the same, which completes the proof. 

Now we are ready to prove the second bullet of Theorem 4.1.

**Lemma D.4**.: Throughout the gradient descent trajectory, the stable rank of the weights \(\mathbf{W}_{j}\) satisfies,

\[\lim_{t\to\infty}\frac{\|\mathbf{W}_{j}\|_{F}^{2}}{\|\mathbf{W}_{j}\|_{2}^{2} }=1,\forall j\in\{\pm 1\},\]

with a decreasing rate of \(O\big{(}1/\log(t)\big{)}\).

Proof of Lemma D.4.: By Definition 5.1, we have

\[\mathbf{w}_{j,r}^{(t)}=\mathbf{w}_{j,r}^{(0)}+\underbrace{\sum_{i=1}^{n}\rho_ {j,r,i}^{(t)}\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}}_{:=\mathbf{ v}_{j,r}^{(t)}}.\]

We first show that \(\|\mathbf{v}_{j,r}^{(t)}\|_{2}=\Theta(\log t)\).

\[\|\mathbf{v}_{j,r}^{(t)}\|_{2}^{2} =\bigg{(}\sum_{i=1}^{n}\rho_{j,r,i}^{(t)}\cdot\|\mathbf{x}_{i}\| _{2}^{-2}\cdot\mathbf{x}_{i}\bigg{)}^{2}\] \[=\sum_{i=1}^{n}(\rho_{j,r,i}^{(t)})^{2}\cdot\|\mathbf{x}_{i}\|_{2 }^{-2}+\sum_{i\neq i^{\prime}}\rho_{j,r,i}^{(t)}\rho_{j,r,i^{\prime}}^{(t)}\| \mathbf{x}_{i}\|_{2}^{-2}\|\mathbf{x}_{i^{\prime}}\|_{2}^{-2}\langle\mathbf{x }_{i},\mathbf{x}_{i^{\prime}}\rangle\] \[\geq\sum_{i=1}^{n}(\rho_{j,r,i}^{(t)})^{2}\cdot R_{\max}^{-2}- \sum_{i\neq i^{\prime}}|\rho_{j,r,i}^{(t)}||\rho_{j,r,i^{\prime}}^{(t)}|\cdot R _{\min}^{-4}p\] \[\geq R_{\max}^{-2}(1-R^{2}R_{\min}^{-2}c_{1}^{-1}\gamma^{-4}np) \sum_{i=1}^{n}(\rho_{j,r,i}^{(t)})^{2}\] \[=\Theta(nR_{\max}^{-2}\log^{2}(t)),\]

where the second last inequality is by triangle inequality; the last inequality is by \(|\rho_{j,r,i}^{(t)}|\leq\gamma^{-4}c_{1}^{-1}|\rho_{j,r,i^{\prime}}^{(t)}|\) from Lemma C.1 and Lemma C.4.

By the definition of \(\mathbf{v}_{j,r}^{(t)}\), we have

\[\|\mathbf{v}_{j,r}^{(t)}-\mathbf{v}_{j,r^{\prime}}^{(t)}\|_{2}^{2} =\bigg{\|}\sum_{i=1}^{n}\rho_{j,r,i}^{(t)}\cdot\|\mathbf{x}_{i}\| _{2}^{-2}\cdot\mathbf{x}_{i}-\sum_{i=1}^{n}\rho_{j,r^{\prime},i}^{(t)}\cdot\| \mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}\bigg{\|}_{2}^{2}\] \[=\bigg{\|}\sum_{i=1}^{n}(\rho_{j,r,i}^{(t)}-\rho_{j,r^{\prime},i}^{( t)})\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}\bigg{\|}_{2}^{2}\] \[=\sum_{i=1}^{n}(\rho_{j,r,i}^{(t)}-\rho_{j,r^{\prime},i}^{(t)})^{2} \cdot\|\mathbf{x}_{i}\|_{2}^{-2}+\sum_{i\neq i^{\prime}}(\rho_{j,r,i}^{(t)}- \rho_{j,r^{\prime},i}^{(t)})(\rho_{j,r,i^{\prime}}^{(t)}-\rho_{j,r^{\prime},i^{ \prime}}^{(t)})\frac{\langle\mathbf{x}_{i},\mathbf{x}_{i^{\prime}}\rangle}{\| \mathbf{x}_{i}\|_{2}^{2}\|\mathbf{x}_{i^{\prime}}\|_{2}^{2}}\] \[\leq(C^{\prime})^{2}nR_{\min}^{-2}+(C^{\prime})^{2}n^{2}R_{\min}^ {-4}p\] \[\leq 2(C^{\prime})^{2}nR_{\min}^{-2},\]

where the first inequality is by Lemma D.1 and Lemma D.2.

Now, we are ready to estimate the stable rank of \(\mathbf{W}^{(t)}\). On the one hand, for \(\|\mathbf{W}^{(t)}_{j}\|_{F}^{2}\), we have

\[\|\mathbf{W}^{(t)}_{j}\|_{F}^{2} =\sum_{r}\|\mathbf{w}^{(t)}_{j,r}\|_{2}^{2}\] \[=\sum_{r}\|\mathbf{w}^{(0)}_{j,r}+\mathbf{v}^{(t)}_{j,r}\|_{2}^{2}\] \[=\sum_{r}\|\mathbf{w}^{(0)}_{j,r}\|_{2}^{2}+\|\mathbf{v}^{(t)}_{j,r}\|_{2}^{2}+2\langle\mathbf{w}^{(0)}_{j,r},\mathbf{v}^{(t)}_{j,r}\rangle\] \[\leq\sum_{r}\|\mathbf{w}^{(0)}_{j,r}\|_{2}^{2}+(\|\mathbf{v}^{(t )}_{j,1}\|_{2}+\|\mathbf{v}^{(t)}_{j,r}-\mathbf{v}^{(t)}_{j,1}\|_{2})^{2}+2\| \mathbf{w}^{(0)}_{j,r}\|_{2}(\|\mathbf{v}^{(t)}_{j,1}\|_{2}+\|\mathbf{v}^{(t) }_{j,r}-\mathbf{v}^{(t)}_{j,1}\|_{2})\] \[=m\|\mathbf{v}^{(t)}_{j,1}\|_{2}^{2}+2\Big{(}\sum_{r}\|\mathbf{v }^{(t)}_{j,r}-\mathbf{v}^{(t)}_{j,1}\|_{2}+\|\mathbf{w}^{(0)}_{j,r}\|_{2} \Big{)}\|\mathbf{v}^{(t)}_{j,1}\|_{2}\] \[\qquad+\Big{(}\sum_{r}\|\mathbf{w}^{(0)}_{j,r}\|_{2}^{2}+\| \mathbf{v}^{(t)}_{j,r}-\mathbf{v}^{(t)}_{j,1}\|_{2}^{2}+2\|\mathbf{w}^{(0)}_{ j,r}\|_{2}\|\mathbf{v}^{(t)}_{j,r}-\mathbf{v}^{(t)}_{j,1}\|_{2}\Big{)}\] \[\leq m\|\mathbf{v}^{(t)}_{j,1}\|_{2}^{2}+mC_{1}\|\mathbf{v}^{(t)}_ {j,1}\|_{2}+mC_{2}.\]

where the first inequality is by triangle inequality and Cauchy inequality; the last inequality is by Lemma D.1, Lemma D.2 and taking

\[C_{1} =3(\sigma_{0}\sqrt{d}+C^{\prime}\sqrt{n}R_{\min}^{-1}),\] \[C_{2} =2(\sigma_{0}\sqrt{d}+C^{\prime}\sqrt{n}R_{\min}^{-1})^{2}.\]

On the other hand, for \(\|\mathbf{W}^{(t)}_{j}\|_{2}^{2}\), we have

\[\|\mathbf{W}^{(t)}_{j}\|_{2}^{2} =\max_{\mathbf{x}\in S^{d-1}}\|\mathbf{W}^{(0)}_{j}\mathbf{x}+ \mathbf{V}^{(t)}_{j}\mathbf{x}\|_{2}^{2}\] \[=\max_{\mathbf{x}\in S^{d-1}}\|\mathbf{W}^{(0)}_{j}\mathbf{x}\|_ {2}^{2}+\|\mathbf{V}^{(t)}_{j}\mathbf{x}\|_{2}^{2}+2\langle\mathbf{W}^{(0)}_{ j}\mathbf{x},\mathbf{V}^{(t)}_{j}\mathbf{x}\rangle\] \[=\max_{\mathbf{x}\in S^{d-1}}\sum_{r}\langle\mathbf{w}^{(0)}_{j,r },\mathbf{x}\rangle^{2}+\sum_{r}\langle\mathbf{v}^{(t)}_{j,r},\mathbf{x} \rangle^{2}+\sum_{r}\langle\mathbf{w}^{(0)}_{j,r},\mathbf{x}\rangle\cdot \langle\mathbf{v}^{(t)}_{j,r},\mathbf{x}\rangle\] \[\geq\sum_{r}\left\langle\mathbf{w}^{(0)}_{j,r},\frac{\mathbf{v}^{( t)}_{j,1}}{\|\mathbf{v}^{(t)}_{j,1}\|_{2}}\right\rangle^{2}+\sum_{r}\left\langle \mathbf{v}^{(t)}_{j,r},\frac{\mathbf{v}^{(t)}_{j,1}}{\|\mathbf{v}^{(t)}_{j,1} \|_{2}}\right\rangle^{2}\] \[\qquad+\sum_{r}\left\langle\mathbf{w}^{(0)}_{j,r},\frac{\mathbf{v}^ {(t)}_{j,1}}{\|\mathbf{v}^{(t)}_{j,1}\|_{2}}\right\rangle\cdot\left\langle \mathbf{v}^{(t)}_{j,r},\frac{\mathbf{v}^{(t)}_{j,1}}{\|\mathbf{v}^{(t)}_{j,1} \|_{2}}\right\rangle\] \[\geq\sum_{r}\left\langle\mathbf{v}^{(t)}_{j,r},\frac{\mathbf{v}^ {(t)}_{j,1}}{\|\mathbf{v}^{(t)}_{j,1}\|_{2}}\right\rangle^{2}-\sum_{r}\| \mathbf{w}^{(0)}_{j,r}\|_{2}^{2}-\sum_{r}\|\mathbf{w}^{(0)}_{j,r}\|_{2}\| \mathbf{v}^{(t)}_{j,r}\|_{2}\] \[\geq m\|\mathbf{v}^{(t)}_{j,1}\|_{2}^{2}+2\sum_{r}\|\mathbf{v}^{(t )}_{j,1}\|_{2}\cdot\left\langle\mathbf{v}^{(t)}_{j,r}-\mathbf{v}^{(t)}_{j,1}, \frac{\mathbf{v}^{(t)}_{j,1}}{\|\mathbf{v}^{(t)}_{j,1}\|_{2}}\right\rangle+ \sum_{r}\left\langle\mathbf{v}^{(t)}_{j,r}-\mathbf{v}^{(t)}_{j,1},\frac{ \mathbf{v}^{(t)}_{j,1}}{\|\mathbf{v}^{(t)}_{j,1}\|_{2}}\right\rangle^{2}\] \[\qquad-\sum_{r}\|\mathbf{w}^{(0)}_{j,r}\|_{2}^{2}-\sum_{r}\| \mathbf{w}^{(0)}_{j,r}\|_{2}\big{(}\|\mathbf{v}^{(t)}_{j,1}\|_{2}+\|\mathbf{v}^ {(t)}_{j,r}-\mathbf{v}^{(t)}_{j,1}\|_{2}\big{)}\] \[\geq m\|\mathbf{v}^{(t)}_{j,1}\|_{2}^{2}-\Big{(}\sum_{r}2\|\mathbf{ v}^{(t)}_{j,r}-\mathbf{v}^{(t)}_{j,1}\|_{2}+\|\mathbf{w}^{(0)}_{j,r}\|_{2}\Big{)} \cdot\|\mathbf{v}^{(t)}_{j,1}\|_{2}\] \[\qquad-\Big{(}\sum_{r}\|\mathbf{w}^{(0)}_{j,r}\|_{2}^{2}+\| \mathbf{w}^{(0)}_{j,r}\|_{2}\|\mathbf{v}^{(t)}_{j,r}-\mathbf{v}^{(t)}_{j,1}\|_{2} \Big{)}\] \[\geq m\|\mathbf{v}^{(t)}_{j,1}\|_{2}^{2}-mC_{3}\|\mathbf{v}^{(t)}_{j,1}\|_{2}-mC_{4}\]

where the first inequality is by taking \(\mathbf{x}=\mathbf{v}^{(t)}_{j,1}/\|\mathbf{v}^{(t)}_{j,1}\|_{2}\); the second inequality is by Cauchy inequality; the third inequality by breaking \(\mathbf{v}^{(t)}_{j,r}\) down into \(\mathbf{v}^{(t)}_{j,1}+\mathbf{v}^{(t)}_{j,r}-\mathbf{v}^{(t)}_{j,1}\) and then expanding the first term as well as applying triangle inequality to the last term; the fourth inequality is by Cauchy inequality; the last inequality is by Lemma D.1, Lemma D.2 and taking

\[C_{3} =1.5\sigma_{0}\sqrt{d}+3C^{\prime}\sqrt{n}R_{\min}^{-1},\] \[C_{4} =1.5\sigma_{0}^{2}d+3C^{\prime}\sigma_{0}\sqrt{d}\sqrt{n}R_{\min}^ {-1}.\]

By leverage the upper bound of \(\|\mathbf{W}_{j}^{(t)}\|_{F}^{2}\) as well as the lower bound of \(\|\mathbf{W}_{j}^{(t)}\|_{2}^{2}\), we can get

\[\frac{\|\mathbf{W}_{j}^{(t)}\|_{F}^{2}}{\|\mathbf{W}_{j}^{(t)}\|_{2}^{2}} \leq\frac{\|\mathbf{v}_{j,1}^{(t)}\|_{2}^{2}+C_{1}\|\mathbf{v}_{j,1}^{(t)}\|_ {2}+C_{2}}{\|\mathbf{v}_{j,1}^{(t)}\|_{2}^{2}-C_{3}\|\mathbf{v}_{j,1}^{(t)}\|_ {2}-C_{4}}.\]

Since \(\|\mathbf{W}_{j}^{(t)}\|_{F}^{2}/\|\mathbf{W}_{j}^{(t)}\|_{2}^{2}\geq 1\), \(\|\mathbf{v}_{j,1}^{(t)}\|_{2}=\Theta(\log t)\) and \(C_{1},C_{2},C_{3},C_{4}\) are constants, it follow that

\[\lim_{t\to\infty}\frac{\|\mathbf{W}_{j}^{(t)}\|_{F}^{2}}{\|\mathbf{W}_{j}^{(t) }\|_{2}^{2}}=1,\]

and

\[\frac{\|\mathbf{W}_{j}^{(t)}\|_{F}^{2}}{\|\mathbf{W}_{j}^{(t)}\|_{ 2}^{2}}-1 \leq\frac{(C_{1}+C_{3})\|\mathbf{v}_{j,1}^{(t)}\|_{2}+(C_{2}+C_{4 })}{\|\mathbf{v}_{j,1}^{(t)}\|_{2}^{2}-C_{3}\|\mathbf{v}_{j,1}^{(t)}\|_{2}-C_{ 4}}\] \[\leq\frac{C_{1}+C_{3}}{\|\mathbf{v}_{j,1}^{(t)}\|_{2}}\] \[\leq\frac{6(C^{\prime}\sqrt{n}R_{\min}^{-1}+\sigma_{0}\sqrt{d})}{ \|\mathbf{v}_{j,1}^{(t)}\|_{2}}\] \[=\Theta\Big{(}\frac{\sqrt{n}R_{\min}^{-1}+\sigma_{0}\sqrt{d}}{ \sqrt{n}R_{\max}^{-1}\log(t)}\Big{)}=\Theta\Big{(}\frac{1+\sigma_{0}\sqrt{d/n} R_{\max}}{\log(t)}\Big{)},\]

which completes the proof. 

## Appendix E Coefficient Analysis of ReLU

In this section, we discuss the stable rank of two-layer ReLU neural network, which is defined as

\[f(\mathbf{W},\mathbf{x})=F_{+1}(\mathbf{W}_{+1},\mathbf{x})-F_{ +1}(\mathbf{W}_{+1},\mathbf{x}),\] (E.1) \[F_{j}(\mathbf{W}_{j},\mathbf{x})=\frac{1}{m}\sum_{r=1}^{m}\sigma (\langle\mathbf{w}_{j,r},\mathbf{x}\rangle),\]

where \(\sigma(z)=\max\{0,z\}\) is ReLU activation function.

These results are based on the conclusions in Section B, which hold with high probability. Denote by \(\mathcal{E}_{\mathrm{prelim}}^{\prime}\) the event that all the results in Section B hold (for a given \(\delta\), we see \(\mathbb{P}(\mathcal{E}_{\mathrm{prelim}}^{\prime})\geq 1-2\delta\) by a union bound). For simplicity and clarity, we state all the results in this and the following sections conditional on \(\mathcal{E}_{\mathrm{prelim}}^{\prime}\).

Denote \(\beta=\max_{i,j,r}\{|\langle\mathbf{w}_{j,r}^{(0)},\mathbf{x}_{i}\rangle|\}\), \(R_{\max}=\max_{i\in[n]}\|\mathbf{x}_{i}\|_{2}\), \(R_{\min}=\min_{i\in[n]}\|\mathbf{x}_{i}\|_{2}\), \(p=\max_{i\neq k}\left|\langle\mathbf{x}_{i},\mathbf{x}_{k}\rangle\right|\) and suppose \(R=R_{\max}/R_{\min}\) is at most an absolute constant. Here we list the exact conditions for \(\eta,\sigma_{0},R_{\min},R_{\max},p\) required by the proofs in this section:

\[\sigma_{0}\leq\big{(}CR_{\max}\sqrt{\log(mn/\delta)}\big{)}^{-1},\] (E.2) \[\eta\leq(CR_{\max}^{2}/nm)^{-1},\] (E.3) \[R_{\min}^{2}\geq CR^{2}np,\] (E.4)where \(C\) is a large enough constant. By Lemma B.1, we can upper bound \(\beta\) by \(2\sqrt{\log(12mn/\delta)}\cdot\sigma_{0}R_{\max}\). Then, by (C.2) and (C.4), it is straightforward to verify the following inequality:

\[\beta\leq c,\] (E.5) \[R_{\min}^{-2}np\leq c,\] (E.6) \[R_{\min}^{-2}R^{2}np\leq c,\] (E.7)

where \(c\) is a sufficiently small constant.

We first introduce the following lemma which characterizes the increasing rate of coefficients \(\rho_{j,r,i}^{(t)}\).

**Lemma E.1**.: For two-layer ReLU neural network defined in (E.1), under the same condition as Theorem 4.3, the decomposition coefficients \(\rho_{j,r,i}^{(t)}\) satisfy following properties:

* \(\overline{\rho}_{y_{i},r,i}^{(t)}\geq c_{1}|\rho_{j,r^{\prime},i^{\prime}}^{(t )}|\) for any \(r\in S_{i}^{(0)}\), \(r^{\prime}\in[m]\), \(j\in\{\pm 1\}\) and \(i,i^{\prime}\in[n]\),
* \(\overline{\rho}_{y_{i},r,i}^{(t)}\geq c_{2}\log\left(1+\frac{\eta|S_{i}^{(0) }||\mathbf{x}_{i}||_{2}^{2}e^{-\beta}}{2nm^{2}}\cdot t\right)\) for any \(r\in S_{i}^{(0)}\) and \(i\in[n]\),
* \(\overline{\rho}_{y_{i},r,i}^{(t)}\leq c_{3}\log\left(1+\frac{2\eta|S_{i}^{(0) }||\mathbf{x}_{i}||_{2}^{2}e^{2\beta}}{nm^{2}}\cdot t\right)\) for any \(r\in S_{i}^{(0)}\) and \(i\in[n]\),

where \(c_{1},c_{2},c_{3}\) are constants. And the following activation pattern is also observed: \(S_{i}^{(0)}\subseteq S_{i}^{(t)}\) where \(S_{i}^{(t)}:=\{r\in[m]:\langle\mathbf{w}_{y_{i},r}^{(t)},\mathbf{x}_{i} \rangle\geq 0\}\), i.e., the on-diagonal neuron activated at initialization will remain activated throughout the training.

Proof of Lemma e.1.: We first show that the first bullet and \(S_{i}^{(0)}\subseteq S_{i}^{(t)}\) hold for \(t\leq T_{1}=C\eta^{-1}nmR_{\max}^{-2}\) where \(C=\Theta(1)\) is a constant. Now we prove this by induction. When \(t=0\), the two hypotheses hold naturally. Suppose that there exists time \(\widetilde{t}\leq T_{1}\) such that the two hypotheses hold for all time \(t\leq\widetilde{t}-1\). We aim to prove they also hold for \(t=\widetilde{t}\). Recall from Lemma B.3 that

\[\overline{\rho}_{j,r,i}^{(t+1)}= \overline{\rho}_{j,r,i}^{(t)}-\frac{\eta}{nm}\cdot\ell_{i}^{(t)} \cdot\sigma^{\prime}(\langle\mathbf{w}_{j,r}^{(t)},\mathbf{x}_{i}\rangle) \cdot\|\mathbf{x}_{i}\|_{2}^{2}\cdot\mathds{1}(y_{i}=j),\] \[\underline{\rho}_{j,r,i}^{(t+1)}= \underline{\rho}_{j,r,i}^{(t)}+\frac{\eta}{nm}\cdot\ell_{i}^{(t)} \cdot\sigma^{\prime}(\langle\mathbf{w}_{j,r}^{(t)},\mathbf{x}_{i}\rangle) \cdot\|\mathbf{x}_{i}\|_{2}^{2}\cdot\mathds{1}(y_{i}=-j),\]

we can get

\[\overline{\rho}_{j,r,i}^{(t+1)}\leq \overline{\rho}_{j,r,i}^{(t)}+\frac{\eta}{nm}\cdot\|\mathbf{x}_{i} \|_{2}^{2}\leq\overline{\rho}_{j,r,i}^{(t)}+\frac{\eta R_{\max}^{2}}{nm},\] (E.8) \[|\underline{\rho}_{j,r,i}^{(t+1)}|\leq |\underline{\rho}_{j,r,i}^{(t)}|+\frac{\eta}{nm}\cdot\|\mathbf{x}_{i} \|_{2}^{2}\leq|\underline{\rho}_{j,r,i}^{(t)}|+\frac{\eta R_{\max}^{2}}{nm}.\] (E.9)

Therefore, \(\max_{j,r,i}\{\overline{\rho}_{j,r,i}^{(t)},|\underline{\rho}_{j,r,i}^{(t)}| \}=O(1)\) for any \(t\leq T_{1}\) and hence \(\max_{i}\{F_{+1}(\mathbf{W}_{+1}^{(t)},\mathbf{x}_{i}),F_{-1}(\mathbf{W}_{+1}^ {(t)},\mathbf{x}_{i}\rangle\}=O(1)\) for any \(t\leq T_{1}\). Thus there exists a positive constant \(c\) such that \(|\ell_{i}^{(t)}|\geq c\) for any \(t\leq T_{1}\). By induction hypothesis, we have \(S_{i}^{(0)}\subseteq S_{i}^{(t)}\) for all \(0\leq t\leq\widetilde{t}-1\) and hence \(\sigma^{\prime}(\langle\mathbf{w}_{y_{i},r}^{(t)},\mathbf{x}_{i}\rangle)=1\) for all \(0\leq t\leq\widetilde{t}-1\). And it follows that for \(r\in S_{i}^{(0)}\)

\[\overline{\rho}_{y_{i},r,i}^{(t+1)}= \overline{\rho}_{y_{i},r,i}^{(t)}+\frac{\eta}{nm}\cdot|\ell_{i}^{ \prime(t)}|\cdot\|\mathbf{x}_{i}\|_{2}^{2}\geq\overline{\rho}_{y_{i},r,i}^{(t )}+\frac{c\eta}{nm}\cdot\|\mathbf{x}_{i}\|_{2}^{2},\forall\,0\leq t\leq \widetilde{t}-1,\] \[\overline{\rho}_{y_{i},r,i}^{(\widetilde{t})}\geq\frac{c\eta \widetilde{t}}{nm}\cdot\|\mathbf{x}_{i}\|_{2}^{2}\geq\frac{c\eta R_{\min}^{2} \widetilde{t}}{nm}.\] (E.10)

On the other hand, by (E.8) and (E.9), we have

\[\overline{\rho}_{j,r^{\prime},i^{\prime}}^{(\widetilde{t})}\leq \frac{\eta R_{\max}^{2}\widetilde{t}}{nm},\,|\underline{\rho}_{j,r^{\prime},i^ {\prime}}^{(\widetilde{t})}|\leq\frac{\eta R_{\max}^{2}\widetilde{t}}{nm} \Longrightarrow|\rho_{j,r^{\prime},i^{\prime}}^{(\widetilde{t})}|\leq\frac{\eta R _{\max}^{2}\widetilde{t}}{nm}.\] (E.11)Dividing (E.10) by (E.11), we can get

\[\frac{\overline{\rho}_{y_{i},r,i}^{(\overline{t})}}{|\rho_{j,r^{ \prime},i^{\prime}}^{(\overline{t})}|}\geq\frac{cR_{\min}^{2}}{R_{\max}^{2}}, \forall\,r\in S_{i}^{(0)},j\in\{\pm 1\},i,i^{\prime}\in[n],\] (E.12)

which indicates that the first bullet holds for time \(t=\widetilde{t}\) as long as \(c_{1}\leq(cR_{\min}^{2})/R_{\max}^{2}\). For \(r\in S_{i}^{(0)}\), we have

\[\langle\mathbf{w}_{y_{i},r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle =\langle\mathbf{w}_{y_{i},r}^{(0)},\mathbf{x}_{i}\rangle+\sum_{i^ {\prime}=1}^{n}\rho_{y_{i},r,i^{\prime}}^{(\widetilde{t})}\|\mathbf{x}_{i^{ \prime}}\|_{2}^{-2}\cdot\langle\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle\] \[=\langle\mathbf{w}_{y_{i},r}^{(0)},\mathbf{x}_{i}\rangle+ \overline{\rho}_{y_{i},r,i}^{(\widetilde{t})}+\sum_{i^{\prime}\neq i}\rho_{y _{i},r,i^{\prime}}^{(\widetilde{t})}\|\mathbf{x}_{i^{\prime}}\|_{2}^{-2}\cdot \langle\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle\] \[\geq\overline{\rho}_{y_{i},r,i}^{(\widetilde{t})}-\sum_{i^{ \prime}\neq i}|\rho_{y_{i},r,i^{\prime}}^{(\widetilde{t})}|R_{\min}^{-2}p\] \[\geq\overline{\rho}_{y_{i},r,i}^{(\widetilde{t})}-\sum_{i^{ \prime}\neq i}\frac{R_{\max}^{2}}{cR_{\min}^{2}}\overline{\rho}_{y_{i},r,i}^{ (\widetilde{t})}\cdot R_{\min}^{-2}p\] \[\geq\Big{(}1-\frac{R_{\max}^{2}}{cR_{\min}^{4}}pn\Big{)}\cdot \overline{\rho}_{y_{i},r,i}^{(\widetilde{t})}\geq 0,\]

where the second inequality is by (E.12). This implies that \(S_{i}^{(0)}\subseteq S_{i}^{(t)}\) holds for time \(t=\widetilde{t}\), which completes the induction. By then, we have already proved that the first bullet and \(S_{i}^{(0)}\subseteq S_{i}^{(t)}\) hold for \(t\leq T_{1}=C\eta^{-1}nmR_{\max}^{-2}\).

Next, we will prove by induction that the three bullets as well as \(S_{i}^{(0)}\subseteq S_{i}^{(t)}\) hold for any time \(t\geq 0\). The second and third bullets are obvious at \(t=0\) as all the coefficients are zero. Suppose there exists \(\widetilde{t}\) such that the three bullets as well as \(S_{i}^{(0)}\subseteq S_{i}^{(t)}\) hold for all time \(0\leq t\leq\widetilde{t}-1\). We aim to prove that they also hold for \(t=\widetilde{t}\). We first prove that the second and third bullets hold for \(t=\widetilde{t}\). To prove this, we first provide more precise upper and lower bounds for \(|\ell_{i}^{(t)}|\). For lower bound, we have

\[|\ell_{i}^{(t)}| =\frac{1}{1+\exp\big{\{}F_{y_{i}}(\mathbf{W}_{y_{i}}^{(t)}, \mathbf{x}_{i})-F_{-y_{i}}(\mathbf{W}_{-y_{i}}^{(t)},\mathbf{x}_{i})\big{\}}}\] \[\geq\frac{1}{1+\exp\big{\{}F_{y_{i}}(\mathbf{W}_{y_{i}}^{(t)}, \mathbf{x}_{i})\big{\}}}\] \[=\frac{1}{1+\exp\{\frac{1}{m}\sum_{r\in S_{i}^{(t)}}\langle \mathbf{w}_{y_{i},r}^{(t)},\mathbf{x}_{i}\rangle\}}\] (E.13)

and

\[\sum_{r\in S_{i}^{(t)}}\langle\mathbf{w}_{y_{i},r}^{(t)},\mathbf{ x}_{i}\rangle =\sum_{r\in S_{i}^{(t)}}\Big{(}\langle\mathbf{w}_{y_{i},r}^{(0)}, \mathbf{x}_{i}\rangle+\overline{\rho}_{y_{i},r,i}^{(t)}+\sum_{i^{\prime}\neq i }\rho_{y_{i},r,i^{\prime}}^{(t)}\|\mathbf{x}_{i^{\prime}}\|_{2}^{-2}\cdot \langle\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle\Big{)}\] \[\leq\sum_{r\in S_{i}^{(t)}}\overline{\rho}_{y_{i},r,i}^{(t)}+ \sum_{r\in S_{i}^{(t)}}\sum_{i^{\prime}\neq i}|\rho_{y_{i},r,i^{\prime}}^{(t)}|R _{\min}^{-2}p+|S_{i}^{(t)}|\cdot\beta\] \[\leq\sum_{r\in S_{i}^{(t)}}\overline{\rho}_{y_{i},r,i}^{(t)}+ \frac{|S_{i}^{(t)}|}{c_{1}|S_{i}^{(0)}|}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_ {y_{i},r,i^{\prime}}^{(t)}R_{\min}^{-2}pn+|S_{i}^{(t)}|\cdot\beta\] \[\leq\frac{|S_{i}^{(t)}|}{|S_{i}^{(0)}|}\sum_{r\in S_{i}^{(0)}} \overline{\rho}_{y_{i},r,i}^{(t)}+\frac{|S_{i}^{(t)}|}{c_{1}|S_{i}^{(0)}|}\sum _{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i^{\prime}}^{(t)}R_{\min}^{-2}pn+|S _{i}^{(t)}|\cdot\beta\] \[\leq c^{\prime}(1+R_{\min}^{-2}pn/c_{1})\sum_{r\in S_{i}^{(0)}} \overline{\rho}_{y_{i},r,i}^{(t)}+|S_{i}^{(t)}|\cdot\beta,\] (E.14)where the first inequality is by triangle inequality; the second inequality is by the first induction hypothesis that \(\overline{\rho}^{(t)}_{y_{i},r,i}\geq c_{1}|\rho^{(t)}_{y_{i},r^{\prime},i^{ \prime}}|\) for \(r\in S^{(0)}_{i}\) and \(0\leq t\leq\widetilde{t}-1\) and hence \(|\rho^{(t)}_{y_{i},r^{\prime},i^{\prime}}|\leq\frac{1}{c_{1}|S^{(0)}_{i}|} \sum_{r\in S^{(0)}_{i}}\overline{\rho}^{(t)}_{y_{i},r,i}\); the third inequality is by

\[\overline{\rho}^{(t)}_{y_{i},r^{\prime},i} =\frac{\eta}{nm}\sum_{s=0}^{t-1}|\ell^{\prime(s)}_{i}|\cdot\sigma ^{\prime}(\langle\mathbf{w}^{(s)}_{y_{i},r^{\prime}},\mathbf{x}_{i}\rangle) \cdot\|\mathbf{x}_{i}\|_{2}^{2}\leq\frac{\eta}{nm}\sum_{s=0}^{t-1}|\ell^{ \prime(s)}_{i}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\] \[\overline{\rho}^{(t)}_{y_{i},r,i} =\frac{\eta}{nm}\sum_{s=0}^{t-1}|\ell^{\prime(s)}_{i}|\cdot\| \mathbf{x}_{i}\|_{2}^{2},\]

and hence \(\overline{\rho}^{(t)}_{y_{i},r^{\prime},i}\leq\overline{\rho}^{(t)}_{y_{i},r, i},\forall r^{\prime}\in S^{(t)}_{i}\setminus S^{(0)}_{i},r\in S^{(0)}_{i}\) for \(0\leq t\leq\widetilde{t}-1\); the last inequality is by \(|S^{(t)}_{i}|\leq m\leq c^{\prime}|S^{(0)}_{i}|\) and \(c^{\prime}\) can be taken as \(2.5\) by Lemma B.2. By plugging (E.14) back into (E.13), we can get

\[|\ell^{(t)}_{i}| \geq\frac{1}{1+\exp\left\{\frac{c^{\prime}(1+R_{\min}^{-2}pn/c_{ 1})}{m}\sum_{r\in S^{(0)}_{i}}\overline{\rho}^{(t)}_{y_{i},r,i}+\frac{|S^{(t)} _{i}|}{m}\cdot\beta\right\}}\] (E.15) \[\geq\frac{1}{1+\exp\left\{\frac{c^{\prime}(1+R_{\min}^{-2}pn/c_{ 1})}{m}\sum_{r\in S^{(0)}_{i}}\overline{\rho}^{(t)}_{y_{i},r,i}+\beta\right\}}\] \[\geq\frac{1}{2}\exp\bigg{\{}-\frac{c^{\prime}(1+R_{\min}^{-2}pn/c_ {1})}{m}\sum_{r\in S^{(0)}_{i}}\overline{\rho}^{(t)}_{y_{i},r,i}-\beta\bigg{\}},\forall\,0\leq t\leq\widetilde{t}-1.\]

For upper bound of \(|\ell^{\prime(t)}_{i}|\), we first bound \(F_{y_{i}}(\mathbf{W}^{(t)}_{y_{i}},\mathbf{x}_{i})-F_{-y_{i}}(\mathbf{W}^{(t)} _{-y_{i}},\mathbf{x}_{i})\) as follows:

\[F_{y_{i}}(\mathbf{W}^{(t)}_{y_{i}},\mathbf{x}_{i})-F_{-y_{i}}( \mathbf{W}^{(t)}_{-y_{i}},\mathbf{x}_{i})\] \[\geq\frac{1}{m}\sum_{r\in S^{(t)}_{i}}\overline{\rho}^{(t)}_{y_{i},r,i}-\frac{2}{c_{1}|S^{(0)}_{i}|}\sum_{r\in S^{(0)}_{i}}\overline{\rho}^{(t)}_ {y_{i},r,i}R_{\min}^{-2}pn-2\beta\] \[\geq\frac{1}{m}\sum_{r\in S^{(0)}_{i}}\overline{\rho}^{(t)}_{y_{i},r,i}-\frac{2^{c}R_{\min}^{-2}pn}{c_{1}m}\sum_{r\in S^{(0)}_{i}}\overline{\rho }^{(t)}_{y_{i},r,i}R_{\min}^{-2}pn-2\beta\] \[\geq\frac{1}{m}\sum_{r\in S^{(0)}_{i}}\overline{\rho}^{(t)}_{y_{i},r,i}-\frac{2^{c}R_{\min}^{-2}pn}{c_{1}m}\sum_{r\in S^{(0)}_{i}}\overline{\rho }^{(t)}_{y_{i},r,i}-2\beta\] \[=\frac{1-2c^{\prime}R_{\min}^{-2}pn/c_{1}}{m}\sum_{r\in S^{(0)}_{ i}}\overline{\rho}^{(t)}_{y_{i},r,i}-2\beta,\]

where the first inequality is by

\[F_{y_{i}}(\mathbf{W}^{(t)}_{y_{i}},\mathbf{x}_{i}) =\frac{1}{m}\sum_{r\in S^{(t)}_{i}}\langle\mathbf{w}^{(t)}_{y_{i},r},\mathbf{x}_{i}\rangle\] \[=\frac{1}{m}\sum_{r\in S^{(t)}_{i}}\Big{(}\langle\mathbf{w}^{(0) }_{y_{i},r},\mathbf{x}_{i}\rangle+\overline{\rho}^{(t)}_{y_{i},r,i}+\sum_{i^{ \prime}\neq i}\rho^{(t)}_{y_{i},r,i^{\prime}}\|\mathbf{x}_{i^{\prime}}\|_{2}^ {-2}\cdot\langle\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle\Big{)}\] \[\geq\frac{1}{m}\bigg{(}\sum_{r\in S^{(t)}_{i}}\overline{\rho}^{(t )}_{y_{i},r,i}-\sum_{r\in S^{(t)}_{i}}\sum_{i^{\prime}\neq i}|\rho^{(t)}_{y_{i},r,i^{\prime}}|R_{\min}^{-2}p-|S^{(t)}_{i}|\cdot\beta\bigg{)},\]and

\[\langle\mathbf{w}_{-y_{i},r}^{(t)},\mathbf{x}_{i}\rangle =\langle\mathbf{w}_{-y_{i},r}^{(0)},\mathbf{x}_{i}\rangle+\sum_{i^ {\prime}=1}^{n}\rho_{-y_{i},r,i^{\prime}}^{(t)}\|\mathbf{x}_{i^{\prime}}\|_{2 }^{2}\cdot\langle\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle\] \[=\langle\mathbf{w}_{-y_{i},r}^{(0)},\mathbf{x}_{i}\rangle+\rho_ {-y_{i},r,i}^{(t)}+\sum_{i^{\prime}\neq i}\rho_{-y_{i},r,i^{\prime}}^{(t)}\| \mathbf{x}_{i^{\prime}}\|_{2}^{2}\cdot\langle\mathbf{x}_{i^{\prime}},\mathbf{ x}_{i}\rangle\] \[\leq\beta+\sum_{i^{\prime}\neq i}|\rho_{-y_{i},r,i^{\prime}}^{(t) }|R_{\min}^{-2}p,\]

and hence

\[F_{-y_{i}}(\mathbf{W}_{-y_{i}}^{(t)},\mathbf{x}_{i})=\frac{1}{m}\sum_{r=1}^{m }\sigma(\langle\mathbf{w}_{-y_{i},r}^{(t)},\mathbf{x}_{i}\rangle)\leq\frac{1} {m}\sum_{r=1}^{m}\Big{(}\beta+\sum_{i^{\prime}\neq i}|\rho_{-y_{i},r,i^{\prime }}^{(t)}|R_{\min}^{-2}p\Big{)};\] (E.16)

the second inequality is by the first induction hypothesis that \(\overline{\rho}_{y_{i},r,i}^{(t)}\geq c_{1}|\rho_{y_{i},r^{\prime},i^{\prime} }^{(t)}|,\overline{\rho}_{y_{i},r,i}^{(t)}\geq c_{1}|\rho_{-y_{i},r^{\prime}, i^{\prime}}^{(t)}|\) and hence \(|\rho_{y_{i},r^{\prime},i^{\prime}}^{(t)}|\leq\frac{1}{c_{1}|S_{i}^{(0)}|} \sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)},|\rho_{-y_{i},r^{ \prime},i^{\prime}}^{(t)}|\leq\frac{1}{c_{1}|S_{i}^{(0)}|}\sum_{r\in S_{i}^{( 0)}}\overline{\rho}_{y_{i},r,i}^{(t)}\) for \(r\in S_{i}^{(0)}\) and \(0\leq t\leq\widetilde{t}-1\); the third inequality is by \(|S_{i}^{(t)}|\leq m\); the fourth inequality is by \(m\leq c^{\prime}|S_{i}^{(0)}|\). Therefore,

\[|\ell_{i}^{(t)}| =\frac{1}{1+\exp\big{\{}F_{y_{i}}(\mathbf{W}_{y_{i}}^{(t)}, \mathbf{x}_{i})-F_{-y_{i}}(\mathbf{W}_{-y_{i}}^{(t)},\mathbf{x}_{i})\big{\}}}\] \[\leq\exp\Big{\{}-\frac{1-2c^{\prime}R_{\min}^{-2}pn/c_{1}}{m} \sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)}+2\beta\Big{\}},\forall \,0\leq t\leq\widetilde{t}-1.\]

By the induction hypothesis, we know that \(S_{i}^{(0)}\subseteq S_{i}^{(t)}\) for all \(0\leq t\leq\widetilde{t}-1\) and hence \(\sigma^{\prime}(\langle\mathbf{w}_{y_{i},r}^{(t)},\mathbf{x}_{i}\rangle)=1\) for all \(r\in S_{i}^{(0)}\) and \(0\leq t\leq\widetilde{t}-1\). By (E.15) and (E.16), it follows that for all \(0\leq t\leq\widetilde{t}-1\)

\[\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t+1)}\geq\sum_{r\in S_{i} ^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)}+\frac{\eta|S_{i}^{(0)}||\mathbf{x}_{i }||_{2}^{2}e^{-\beta}}{2nm}\cdot\exp\bigg{\{}-\frac{c^{\prime}(1+R_{\min}^{-2 }pn/c_{1})}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)}\bigg{\}},\] (E.17)

\[\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t+1)}\leq\sum_{r\in S_{i} ^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)}+\frac{\eta|S_{i}^{(0)}||\mathbf{x}_{i }||_{2}^{2}e^{2\beta}}{nm}\cdot\exp\bigg{\{}-\frac{1-2c^{\prime}R_{\min}^{-2 }pn/c_{1}}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)}\bigg{\}}.\] (E.18)

By applying Lemma H.2 to (E.17) and taking \(x_{t}=\frac{c^{\prime}(1+R_{\min}^{-2}pn/c_{1})}{m}\sum_{r\in S_{i}^{(0)}} \overline{\rho}_{y_{i},r,i}^{(t)}\), we can get

\[\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)}\geq\frac{m}{c^{\prime} (1+R_{\min}^{-2}pn/c_{1})}\log\bigg{(}1+\frac{c^{\prime}(1+R_{\min}^{-2}pn/c_{1} )\eta|S_{i}^{(0)}||\mathbf{x}_{i}||_{2}^{2}e^{-\beta}}{2nm^{2}}\cdot t\bigg{)}, \forall\,0\leq t\leq\widetilde{t}.\] (E.19)By applying Lemma H.1 to (E.18) and taking \(x_{t}=\frac{1-2c^{\prime}R_{\min}^{-2}pn/c_{1}}{m}\sum_{r\in S_{i}^{(0)}}\overline{ \rho}_{y_{i},r,i}^{(t)}\), we can get for any \(0\leq t\leq\widetilde{t}\) that

\[\begin{split}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{ (t)}&\leq\frac{m}{1-2c^{\prime}R_{\min}^{-2}pn/c_{1}}\log\left(1+ \frac{(1-2c^{\prime}R_{\min}^{-2}pn/c_{1})\eta|S_{i}^{(0)}||\mathbf{x}_{i}|| _{2}^{2}e^{2\beta}}{nm^{2}}\cdot\right.\\ &\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad\left.\exp\Big{(}\frac{(1-2c^{\prime}R_{ \min}^{-2}pn/c_{1})\eta|S_{i}^{(0)}||\mathbf{x}_{i}||_{2}^{2}e^{2\beta}}{nm^{2 }}\Big{)}\cdot t\right)\\ &\leq\frac{m}{1-2c^{\prime}R_{\min}^{-2}pn/c_{1}}\log\left(1+ \frac{2(1-2c^{\prime}R_{\min}^{-2}pn/c_{1})\eta|S_{i}^{(0)}||\mathbf{x}_{i}|| _{2}^{2}e^{2\beta}}{nm^{2}}\cdot t\right),\end{split}\] (E.20)

where the last inequality is by \(\eta\leq(CR_{\max}^{2}/nm)^{-1}\), \(C\) is a large enough constant and hence \((1-2c^{\prime}R_{\min}^{-2}pn/c_{1})\eta|S_{i}^{(0)}||\mathbf{x}_{i}||_{2}^{2} e^{2\beta}/nm^{2}\leq\log 2\). Since \(S_{i}^{(0)}\subseteq S_{i}^{(t)}\) for all \(0\leq t\leq\widetilde{t}-1\) and hence \(\sigma^{\prime}(\langle\mathbf{w}_{y_{i},r}^{(t)},\mathbf{x}_{i}\rangle)=1\) for all \(r\in S_{i}^{(0)}\) and \(0\leq t\leq\widetilde{t}-1\), we have

\[\overline{\rho}_{y_{i},r,i}^{(t)}=\frac{\eta}{nm}\sum_{s=0}^{t-1}|\ell_{i}^{ \prime(s)}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\forall\,0\leq t\leq\widetilde{t}.\]

Accordingly, it holds that

\[\overline{\rho}_{y_{i},r,i}^{(t)}=\overline{\rho}_{y_{i},r^{\prime},i}^{(t)}, \forall\,r,r^{\prime}\in S_{i}^{(0)},\forall\,0\leq t\leq\widetilde{t}.\]

Applying this to (E.19) and (E.20), we can get

\[\begin{split}\overline{\rho}_{y_{i},r,i}^{(t)}&\geq \frac{m}{c^{\prime}(1+R_{\min}^{-2}pn/c_{1})|S_{i}^{(0)}|}\log\left(1+\frac{c^{ \prime}(1+R_{\min}^{-2}pn/c_{1})\eta|S_{i}^{(0)}||\mathbf{x}_{i}||_{2}^{2}e^{- \beta}}{2nm^{2}}\cdot t\right)\\ &\geq\frac{1}{c^{\prime}(1+R_{\min}^{-2}pn/c_{1})}\log\left(1+ \frac{c^{\prime}(1+R_{\min}^{-2}pn/c_{1})\eta|S_{i}^{(0)}||\mathbf{x}_{i}||_{2 }^{2}e^{-\beta}}{2nm^{2}}\cdot t\right),\forall\,0\leq t\leq\widetilde{t},\\ \overline{\rho}_{y_{i},r,i}^{(t)}&\leq\frac{m}{(1-2c^ {\prime}R_{\min}^{-2}pn/c_{1})|S_{i}^{(0)}|}\log\left(1+\frac{2(1-2c^{\prime}R _{\min}^{-2}pn/c_{1})\eta|S_{i}^{(0)}||\mathbf{x}_{i}||_{2}^{2}e^{2\beta}}{nm^ {2}}\cdot t\right),\end{split}\] (E.21)

By taking

\[c_{2}=\frac{1}{c^{\prime}(1+R_{\min}^{-2}pn/c_{1})},\,c_{3}=\frac{c^{\prime}}{( 1-2c^{\prime}R_{\min}^{-2}pn/c_{1})},\]

the above inequalities indicates that the second and third bullets hold at time \(t=\widetilde{t}\). For the first bullet, it is only necessary to consider the situation where \(\widetilde{t}\geq T_{1}=C\eta^{-1}nmR_{\max}^{-2}\). In order to apply Lemma H.4 (requiring \(b>a\)), we loosen the bounds in (E.21) as follows:

\[\overline{\rho}_{y_{i},r,i}^{(t)}\geq c_{2}\log\left(1+\frac{\eta R _{\min}^{2}e^{-\beta}}{5nm}\cdot t\right),\forall\,0\leq t\leq\widetilde{t},\] (E.22) \[\overline{\rho}_{y_{i},r,i}^{(t)}\leq c_{3}\log\left(1+\frac{6\eta R _{\max}^{2}e^{2\beta}}{5nm}\cdot t\right),\forall\,0\leq t\leq\widetilde{t},\] (E.23)

where we use \(0.4m\leq|S_{i}^{(0)}|\leq 0.6m\).

By applying Lemma H.4 to (E.22) and (E.23), we can get for any \(i,i^{\prime}\in[n]\), \(r\in S_{i}^{(0)}\), \(r^{\prime}\in S_{i^{\prime}}^{(0)}\) and \(T_{1}\leq t\leq\widetilde{t}\) that

\[\frac{\overline{\rho}_{y_{i},r,i}^{(t)}}{\overline{\rho}_{y_{i^{ \prime}},r^{\prime},i^{\prime}}^{(t)}} \geq\frac{c_{2}}{c_{3}}\cdot\frac{\log\left(1+\frac{\eta R_{\min} ^{2}e^{\beta}}{5nm}\cdot t\right)}{\log\left(1+\frac{6\eta R_{\max}^{2}e^{2 \beta}}{5nm}\cdot t\right)}\] \[\geq\frac{c_{2}}{c_{3}}\cdot\frac{\log\left(1+\frac{\eta R_{\max }^{2}e^{2\beta}}{5nm}\cdot T_{1}\right)}{\log\left(1+\frac{6\eta R_{\max}^{2}e^ {2\beta}}{5nm}\cdot T_{1}\right)}\] \[=\frac{c_{2}}{c_{3}}\cdot\frac{\log\left(1+0.2Ce^{-\beta}R_{\min }^{2}\right)}{\log\left(1+1.2Ce^{2\beta}R_{\max}^{2}\right)}.\]

Notice that \(S_{i^{\prime}}^{(0)}\subseteq S_{i^{\prime}}^{(t)}\) for all \(0\leq t\leq\widetilde{t}-1\) and hence \(\sigma^{\prime}(\langle\mathbf{w}_{y_{i^{\prime}},r}^{(t)},\mathbf{x}_{i^{ \prime}}\rangle)=1\) for all \(r\in S_{i^{\prime}}^{(0)}\) and \(0\leq t\leq\widetilde{t}-1\), we have

\[|\rho_{j,r^{\prime\prime},i^{\prime}}^{(t)}| =\frac{\eta}{nm}\sum_{s=0}^{t-1}|\ell_{i^{\prime}}^{(s)}|\cdot \sigma^{\prime}(\langle\mathbf{w}_{j,r^{\prime\prime}}^{(s)},\mathbf{x}_{i^{ \prime}}\rangle)\cdot\|\mathbf{x}_{i}\|_{2}^{2}\] \[\leq\frac{\eta}{nm}\sum_{s=0}^{t-1}|\ell_{i^{\prime}}^{(s)}| \cdot\|\mathbf{x}_{i^{\prime}}\|_{2}^{2}, \forall j\in\{\pm 1\},r^{\prime\prime}\in[m],i^{\prime}\in[n],\] \[\overline{\rho}_{y_{i^{\prime}},r^{\prime},i^{\prime}}^{(t)} =\frac{\eta}{nm}\sum_{s=0}^{t-1}|\ell_{i^{\prime}}^{(s)}|\cdot\| \mathbf{x}_{i^{\prime}}\|_{2}^{2}, \forall r^{\prime}\in S_{i^{\prime}}^{(0)},i^{\prime}\in[n],\]

and hence \(|\rho_{j,r^{\prime},i^{\prime}}^{(t)}|\leq\overline{\rho}_{y_{i^{\prime}},r^ {\prime},i^{\prime}}^{(t)}\) for \(0\leq t\leq\widetilde{t}\). Therefore, as long as

\[c_{1}\leq\frac{c_{2}}{c_{3}}\cdot\frac{\log\left(1+0.2Ce^{-\beta}R_{\min}^{2} \right)}{\log\left(1+1.2Ce^{2\beta}R_{\max}^{2}\right)},\]

the first bullet hold for time \(t=\widetilde{t}\). This condition holds as long as

\[c^{\prime} =2.5,\] \[2c^{\prime}c_{1}^{-1}R_{\min}^{-2}pn\leq 0.5\quad\Longrightarrow\quad c _{2}\geq 0.37,c_{3}\leq 5,\] \[c_{1} =\frac{\log\left(1+0.2Ce^{-\beta}R_{\min}^{2}\right)}{14\log \left(1+1.2Ce^{2\beta}R_{\max}^{2}\right)}.\]

Finally, we verify that \(S_{i}^{(0)}\subseteq S_{i}^{(\widetilde{t})}\). For \(r\in S_{i}^{(0)}\), we have

\[\langle\mathbf{w}_{y_{i},r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle =\langle\mathbf{w}_{y_{i},r}^{(0)},\mathbf{x}_{i}\rangle+\sum_{i ^{\prime}=1}^{n}\rho_{y_{i},r,i^{\prime}}^{(\widetilde{t})}\|\mathbf{x}_{i^{ \prime}}\|_{2}^{-2}\cdot\langle\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle\] \[=\langle\mathbf{w}_{y_{i},r}^{(0)},\mathbf{x}_{i}\rangle+ \overline{\rho}_{y_{i},r,i}^{(\widetilde{t})}+\sum_{i^{\prime}\neq i}\rho_{y_{i},r,i^{\prime}}^{(\widetilde{t})}\|\mathbf{x}_{i^{\prime}}\|_{2}^{-2}\cdot \langle\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle\] \[\geq\overline{\rho}_{y_{i},r,i}^{(\widetilde{t})}-\sum_{i^{ \prime}\neq i}|\rho_{y_{i},r,i^{\prime}}^{(\widetilde{t})}|R_{\min}^{-2}p\] \[\geq\overline{\rho}_{y_{i},r,i}^{(\widetilde{t})}-(R_{\min}^{-2} pn/c_{1})\overline{\rho}_{y_{i},r,i^{\prime}}^{(\widetilde{t})}\] \[=(1-R_{\min}^{-2}pn/c_{1})\cdot\overline{\rho}_{y_{i},r,i}^{( \widetilde{t})}\geq 0,\]where the second inequality is by \(|\rho^{(\widetilde{t})}_{y_{i},r,i^{\prime}}|\leq\overline{\rho}^{(\widetilde{t})}_ {y_{i},r,i}/c_{1}\). This implies that \(S^{(0)}_{i}\subseteq S^{(t)}_{i}\) holds for time \(t=\widetilde{t}\), which completes the induction. 

Next, we show that \(|\rho^{(t)}_{-y_{i},r,i^{\prime}}|\) will be much smaller than \(|\overline{\rho}^{(t)}_{y_{i},r^{\prime},i}|\) as the training goes on.

**Lemma E.2**.: There exists time \(T_{2}\) and constant \(c\) such that for any time \(t\geq T_{2}\)

\[|\rho^{(t)}_{y_{i},r,i^{\prime}}|\leq cR_{\min}^{-2}pn|\overline{\rho}^{(t)}_ {y_{i},r^{\prime},i}|,\]

where \(r\in[m],r^{\prime}\in S^{(0)}_{i}\) and \(i,i^{\prime}\in[n]\) satisfying \(-y_{i}=y_{i^{\prime}}\).

Proof of Lemma e.2.: First, we will prove by induction that for \(r\in[m],r^{\prime}\in S^{(0)}_{i}\) and \(i,i^{\prime}\in[n]\) satisfying \(-y_{i}=y_{i^{\prime}}\) it holds that

\[|\underline{\rho}^{(t)}_{y_{i},r,i^{\prime}}|\leq\beta+1+R_{\min}^{-2}pn| \overline{\rho}^{(t)}_{y_{i},r^{\prime},i}|/c_{1}.\] (E.24)

This result holds naturally when \(t=0\) since all the coefficients are zero. Suppose that there exists time \(\widetilde{t}\) such that the induction hypothesis (E.24) holds for all time \(t\leq\widetilde{t}-1\). We aim to prove that (E.24) also holds for \(t=\widetilde{t}\). In the following analysis, two cases will be considered: \(|\rho^{(\widetilde{t}-1)}_{y_{i},r,i^{\prime}}|>\beta+\sum_{k\neq i^{\prime}} |\rho^{(\widetilde{t}-1)}_{y_{i},r,k}||\|\mathbf{x}_{k}\|_{2}^{-2}p\) and \(|\overline{\rho}^{(\widetilde{t}-1)}_{y_{i},r,i^{\prime}}|\leq\beta+\sum_{k \neq i^{\prime}}|\rho^{(\widetilde{t}-1)}_{y_{i},r,k}||\|\mathbf{x}_{k}\|_{2} ^{-2}p\).

For if \(|\underline{\rho}^{(\widetilde{t}-1)}_{y_{i},r,i^{\prime}}|>\beta+\sum_{k\neq i ^{\prime}}|\rho^{(\widetilde{t}-1)}_{y_{i},r,k}||\|\mathbf{x}_{k}\|_{2}^{-2}p\), then by the decomposition (5.1) we have

\[\langle\mathbf{w}^{(\widetilde{t}-1)}_{y_{i},r},\mathbf{x}_{i^{ \prime}}\rangle =\langle\mathbf{w}^{(0)}_{y_{i},r},\mathbf{x}_{i^{\prime}}\rangle +\underline{\rho}^{(\widetilde{t}-1)}_{y_{i},r,i^{\prime}}+\sum_{k\neq i^{ \prime}}\rho^{(\widetilde{t}-1)}_{y_{i},r,k}\|\mathbf{x}_{k}\|_{2}^{-2} \langle\mathbf{x}_{k},\mathbf{x}_{i^{\prime}}\rangle\] \[\leq\underline{\rho}^{(\widetilde{t}-1)}_{y_{i},r,i^{\prime}}+ \beta+\sum_{k\neq i^{\prime}}|\rho^{(\widetilde{t}-1)}_{y_{i},r,k}||\|\mathbf{x }_{k}\|_{2}^{-2}p<0.\]

and hence

\[\underline{\rho}^{(\widetilde{t})}_{y_{i},r,i^{\prime}}=\underline{\rho}^{( \widetilde{t}-1)}_{y_{i},r,i^{\prime}}+\frac{\eta}{nm}\cdot\underline{\rho}^{( \widetilde{t}-1)}_{i}\cdot\sigma^{\prime}(\langle\mathbf{w}^{(\widetilde{t}-1 )}_{y_{i},r},\mathbf{x}_{i^{\prime}}\rangle)\cdot\|\mathbf{x}_{i^{\prime}}\|_{2 }^{2}=\underline{\rho}^{(\widetilde{t}-1)}_{y_{i},r,i^{\prime}}.\]

Therefore, by induction hypothesis (E.24) at time \(t=\widetilde{t}-1\), we have

\[\underline{\rho}^{(\widetilde{t})}_{y_{i},r,i^{\prime}}=\underline{\rho}^{( \widetilde{t}-1)}_{y_{i},r,i^{\prime}}\leq\beta+1+R_{\min}^{-2}pn|\overline{ \rho}^{(\widetilde{t}-1)}_{y_{i},r^{\prime},i}|/c_{1}\leq\beta+1+R_{\min}^{-2 }pn|\overline{\rho}^{(\widetilde{t})}_{y_{i},r^{\prime},i}|/c_{1}.\]

For if \(|\underline{\rho}^{(\widetilde{t}-1)}_{y_{i},r,i^{\prime}}|\leq\beta+\sum_{k \neq i^{\prime}}|\rho^{(\widetilde{t}-1)}_{y_{i},r,k}||\|\mathbf{x}_{k}\|_{2}^{- 2}p\), by the first bullet in Lemma E.1, we have

\[|\underline{\rho}^{(\widetilde{t}-1)}_{y_{i},r,i^{\prime}}|\leq\beta+\sum_{k \neq i^{\prime}}|\overline{\rho}^{(\widetilde{t}-1)}_{y_{i},r^{\prime},i}|| \mathbf{x}_{k}\|_{2}^{-2}p/c_{1}\leq\beta+|\overline{\rho}^{(\widetilde{t}-1) }_{y_{i},r^{\prime},i}|R_{\min}^{-2}pn/c_{1},\] (E.25)

and thus

\[-\underline{\rho}^{(\widetilde{t})}_{y_{i},r,i^{\prime}} =-\underline{\rho}^{(\widetilde{t}-1)}_{y_{i},r,i^{\prime}}+\frac{ \eta}{nm}\cdot|\ell^{(\widetilde{t}-1)}_{i}|\cdot\sigma^{\prime}(\langle \mathbf{w}^{(\widetilde{t}-1)}_{y_{i},r},\mathbf{x}_{i^{\prime}}\rangle)\cdot \|\mathbf{x}_{i^{\prime}}\|_{2}^{2}\] \[\leq-\underline{\rho}^{(\widetilde{t}-1)}_{y_{i},r,i^{\prime}}+ \frac{\eta R_{\max}^{2}}{nm}\] \[\leq\beta+1+|\overline{\rho}^{(\widetilde{t})}_{y_{i},r^{\prime},i }|R_{\min}^{-2}pn/c_{1},\]

where the last inequality is by (E.25) and \(\eta\leq(CR_{\max}^{2}/nm)^{-1}\) with a sufficiently large constant \(C\). This demonstrates that inequality (E.24) holds for \(t=\widetilde{t}\), thereby completing the induction process. By Lemma E.1, we know that there exists time \(T^{\prime}\) such that

\[|\overline{\rho}^{(t)}_{y_{i},r^{\prime},i}|\geq c_{1}(\beta+1)R_{\min}^{2}/pn,\]

for any time \(t\geq T^{\prime}\). Taking \(T_{2}=T^{\prime}\) and \(c=2/c_{1}\), we have

\[|\underline{\rho}^{(t)}_{y_{i},r,i^{\prime}}|\leq\beta+1+R_{\min}^{-2}pn| \overline{\rho}^{(t)}_{y_{i},r^{\prime},i}|/c_{1}\leq 2R_{\min}^{-2}pn|\overline{\rho}^{(t)}_{y_{i},r^{ \prime},i}|/c_{1}=cR_{\min}^{-2}pn|\overline{\rho}^{(t)}_{y_{i},r,i}|,\]which completes the proof. 

Given Lemma E.2, the following corollary can be directly obtained.

**Corollary E.3**.: There exists time \(T_{3}\) and constant \(c\) such that for any time \(t\geq T_{3}\)

\[|\rho^{(t)}_{y_{i},r,i^{\prime}}|\leq cR_{\min}^{-2}pn|\overline{\rho}^{(t)}_{y _{i},r^{\prime},i}|,\forall r\in[m],r^{\prime}\in S_{i}^{(0)},i,i^{\prime}\in[n ]\text{ with }-y_{i}=y_{i^{\prime}}.\]

## Appendix F Stable Rank of ReLU Network

In this section, we consider the properties of stable rank of the weight matrix \(\mathbf{W}^{(t)}\) found by gradient descent at time \(t\), defined as \(\|\mathbf{W}^{(t)}\|_{F}^{2}/\|\mathbf{W}^{(t)}\|_{2}^{2}\). Given Lemma E.1, we have following coefficient update rule for any \(t\geq 0\), \(i\in[n]\) and \(r\in S_{i}^{(0)}\):

\[\overline{\rho}^{(t+1)}_{y_{i},r,i}=\overline{\rho}^{(t)}_{y_{i},r,i}+\frac{ \eta}{nm}\cdot|\ell^{\prime(t)}_{i}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\] (F.1)

where

\[|\ell^{\prime(t)}_{i}|=\frac{1}{1+\exp\{F_{y_{i}}(\mathbf{W}^{(t)}_{y_{i}}, \mathbf{x}_{i})-F_{-y_{i}}(\mathbf{W}^{(t)}_{-y_{i}},\mathbf{x}_{i})\}}.\]

Now we are ready to prove the first bullet of Theorem 4.3.

**Lemma F.1**.: For two-layer ReLU neural network defined in (E.1), under the same condition as Theorem 4.3, the stable rank of \(\mathbf{W}^{(t)}_{j}\) satisfies the following property:

\[\limsup_{t\to\infty}\frac{\|\mathbf{W}^{(t)}_{j}\|_{F}^{2}}{\|\mathbf{W}^{(t) }_{j}\|_{2}^{2}}\leq C,\]

where \(C=\Theta(1)\) is a constant.

Proof of Lemma F.1.: By decomposition (5.1), we have

\[\mathbf{w}^{(t)}_{j,r}-\mathbf{w}^{(0)}_{j,r}=\sum_{i=1}^{n}\rho^{(t)}_{j,r,i }\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}=\Big{[}\rho^{(t)}_{j,r,1 }\|\mathbf{x}_{1}\|_{2}^{-2},\cdots,\rho^{(t)}_{j,r,n}\|\mathbf{x}_{n}\|_{2}^{ -2}\Big{]}\cdot\begin{bmatrix}\mathbf{x}_{1}\\ \vdots\\ \mathbf{x}_{n}\end{bmatrix},\]

and

\[\mathbf{W}^{(t)}_{j}-\mathbf{W}^{(0)}_{j}=\underbrace{\begin{bmatrix}\rho^{(t )}_{j,1},\|\mathbf{x}_{1}\|_{2}^{-2}&\rho^{(t)}_{j,1,2}\|\mathbf{x}_{1}\|_{2}^ {-2}&\cdots&\rho^{(t)}_{j,1,n}\|\mathbf{x}_{n}\|_{2}^{-2}\\ \rho^{(t)}_{j,2},\|\mathbf{x}_{1}\|_{2}^{-2}&\rho^{(t)}_{j,2,2}\|\mathbf{x}_{1 }\|_{2}^{-2}&\cdots&\rho^{(t)}_{j,2,n}\|\mathbf{x}_{n}\|_{2}^{-2}\\ \vdots&\vdots&\vdots&\ddots&\vdots\\ \rho^{(t)}_{j,m,1}\|\mathbf{x}_{1}\|_{2}^{-2}&\rho^{(t)}_{j,m,2}\|\mathbf{x}_{1 }\|_{2}^{-2}&\cdots&\rho^{(t)}_{j,m,n}\|\mathbf{x}_{n}\|_{2}^{-2}\end{bmatrix} }_{\mathbf{A}_{i}}\underbrace{\begin{bmatrix}\mathbf{x}_{1}\\ \vdots\\ \mathbf{x}_{n}\end{bmatrix}}_{:=\mathbf{X}}.\]

Let \(\mathbf{a}_{i}(t)^{\top}\) be the \(i\)-th column of \(\mathbf{A}_{t}\). It follows that

\[\|\mathbf{W}^{(t)}_{j}-\mathbf{W}^{(0)}_{j}\|_{F}^{2} =\text{Tr}(\mathbf{A}_{t}\mathbf{X}\mathbf{X}^{\top}\mathbf{A}_{ t}^{\top})\] \[=\text{Tr}\left(\Big{(}\sum_{i=1}^{n}\mathbf{a}_{i}(t)^{\top} \mathbf{x}_{i}\Big{)}\Big{(}\sum_{i=1}^{n}\mathbf{x}_{i}^{\top}\mathbf{a}_{i}( t)\Big{)}\right)\] \[=\text{Tr}\left(\ \sum_{i=1}^{n}\sum_{i^{\prime}=1}^{n}\mathbf{a}_{i}(t)^{\top} \mathbf{x}_{i}\mathbf{x}_{i^{\prime}}^{\top}\mathbf{a}_{i^{\prime}}(t)\right)\] \[=\sum_{i=1}^{n}\sum_{i^{\prime}=1}^{n}\langle\mathbf{x}_{i}, \mathbf{x}_{i^{\prime}}\rangle\cdot\text{Tr}(\mathbf{a}_{i}(t)^{\top}\mathbf{ a}_{i^{\prime}}(t))\]\[=\sum_{i=1}^{n}\|\mathbf{x}_{i}\|_{2}^{2}\cdot\text{Tr}(\mathbf{a}_{i }(t)^{\top}\mathbf{a}_{i}(t))+\sum_{i\neq i^{\prime}}\langle\mathbf{x}_{i}, \mathbf{x}_{i^{\prime}}\rangle\cdot\text{Tr}(\mathbf{a}_{i}(t)^{\top}\mathbf{a }_{i^{\prime}}(t))\] \[\leq R_{\max}^{2}\sum_{i=1}^{n}\text{Tr}(\mathbf{a}_{i}(t)^{\top} \mathbf{a}_{i}(t))+p\sum_{i\neq i^{\prime}}|\text{Tr}(\mathbf{a}_{i}(t)^{\top }\mathbf{a}_{i^{\prime}}(t))|,\]

and

\[\text{Tr}(\mathbf{a}_{i}(t)^{\top}\mathbf{a}_{i}(t))=\sum_{r=1}^ {m}([\mathbf{a}_{i}(t)]_{r})^{2}=\sum_{r=1}^{m}(\rho_{j,r,i}^{(t)}\|\mathbf{x} _{i}\|_{2}^{-2})^{2}\leq\sum_{r=1}^{m}(\rho_{j,r,i}^{(t)})^{2}R_{\min}^{-4} \leq CmR_{\min}^{-4}(\log(t))^{2},\] \[|\text{Tr}(\mathbf{a}_{i}(t)^{\top}\mathbf{a}_{i^{\prime}}(t))| \leq\sum_{r=1}^{m}\big{|}[\mathbf{a}_{i}(t)]_{r}[\mathbf{a}_{i^{ \prime}}(t)]_{r}\big{|}=\sum_{r=1}^{m}|\rho_{j,r,i}^{(t)}\rho_{j,r,i^{\prime}}^ {(t)}\|\mathbf{x}_{i}\|_{2}^{-2}\|\mathbf{x}_{i^{\prime}}\|_{2}^{-2}\leq CmR_{ \min}^{-4}(\log(t))^{2}.\]

Accordingly, we have

\[\|\mathbf{W}_{j}^{(t)}-\mathbf{W}_{j}^{(0)}\|_{F}^{2} \leq CmnR_{\max}^{2}R_{\min}^{-4}(\log(t))^{2}+Cmn^{2}pR_{\min}^ {-4}(\log(t))^{2}\] \[=CmnR_{\max}^{2}R_{\min}^{-4}(1+R_{\max}^{-2}np)(\log(t))^{2}\] \[\leq C^{\prime}mnR_{\max}^{2}R_{\min}^{-4}(\log(t))^{2}.\]

On the other hand, we will give an lower bound for \(\|\mathbf{W}_{j}^{(t)}-\mathbf{W}_{j}^{(0)}\|_{2}\).

\[\|\mathbf{W}_{j}^{(t)}-\mathbf{W}_{j}^{(0)}\|_{2} =\max_{\mathbf{y}\in S^{d-1}}\|(\mathbf{W}_{j}^{(t)}-\mathbf{W}_{ j}^{(0)})\mathbf{y}\|_{2}\] \[\geq\frac{\|(\mathbf{W}_{j}^{(t)}-\mathbf{W}_{j}^{(0)})\mathbf{X }^{\top}(\mathbf{X}\mathbf{X}^{\top})^{-1}\mathbf{1}_{n}\|_{2}}{\|\mathbf{X} ^{\top}(\mathbf{X}\mathbf{X}^{\top})^{-1}\mathbf{1}_{n}\|_{2}}\] \[=\frac{\|\mathbf{A}_{t}\mathbf{1}_{n}\|_{2}}{\|\mathbf{X}^{\top}( \mathbf{X}\mathbf{X}^{\top})^{-1}\mathbf{1}_{n}\|_{2}}.\]

We first provide a lower bound for \(\|\mathbf{A}_{t}\mathbf{1}_{n}\|_{2}\). Note that

\[\mathbf{A}_{t}\mathbf{1}_{n}=\begin{bmatrix}\sum_{i=1}^{n}\rho_{j,i,i}^{(t)} \|\mathbf{x}_{i}\|_{2}^{-2}\\ \vdots\\ \sum_{i=1}^{n}\rho_{j,m,i}^{(t)}\|\mathbf{x}_{i}\|_{2}^{-2}\end{bmatrix},\]

we need to bound \(\sum_{i=1}^{n}\rho_{j,r,i}^{(t)}\|\mathbf{x}_{i}\|_{2}^{-2},r\in[m]\). By Corollary E.3, there exists time \(T\) such that for any \(t\geq T\), \(|\underline{\rho}_{y_{i},r,i^{\prime}}^{(t)}|\leq cR_{\min}^{-2}pn\bar{\rho}_{ y_{i},r,i}^{(t)},\forall i\in S_{r}^{(0)}\) and \(\forall i^{\prime}\in[n]\) with \(y_{i^{\prime}}=-y_{i}\). Therefore, we have for \(t\geq T\) that

\[\sum_{r=1}^{m}\sum_{i=1}^{n}\rho_{j,r,i}^{(t)}\|\mathbf{x}_{i}\|_ {2}^{-2} =\sum_{r=1}^{m}\bigg{(}\sum_{i\in S_{j}}\bar{\rho}_{j,r,i}^{(t)}\| \mathbf{x}_{i}\|_{2}^{-2}+\sum_{i\in S_{-j}}\underline{\rho}_{j,r,i}^{(t)}\| \mathbf{x}_{i}\|_{2}^{-2}\bigg{)}\] \[=\sum_{i\in S_{j}}\sum_{r=1}^{m}\bar{\rho}_{y_{i},r,i}^{(t)}\| \mathbf{x}_{i}\|_{2}^{-2}+\sum_{i\in S_{-j}}\sum_{r=1}^{m}\underline{\rho}_{-y_ {i},r,i}^{(t)}\|\mathbf{x}_{i}\|_{2}^{-2}\] \[\geq\sum_{i\in S_{j}}\sum_{r=1}^{m}\bar{\rho}_{y_{i},r,i}^{(t)}R_ {\max}^{-2}+\sum_{i\in S_{-j}}\sum_{r=1}^{m}\underline{\rho}_{-y_{i},r,i}^{(t)}R _{\min}^{-2}\] \[\geq\sum_{i\in S_{j}}\sum_{r\in S_{i}^{(0)}}\bar{\rho}_{y_{i},r,i}^ {(t)}R_{\max}^{-2}-\frac{m|S_{-j}|\cdot cR_{\min}^{-4}R_{\max}^{2}pn}{|S_{j}| \cdot\min_{i\in S_{j}}|S_{i}^{(0)}|}\sum_{i\in S_{j}}\sum_{r\in S_{i}^{(0)}} \bar{\rho}_{y_{i},r,i}^{(t)}R_{\max}^{-2}\]\[\geq\frac{R_{\max}^{-2}}{2}\sum_{i\in S_{j}}\sum_{r\in S_{i}^{(0)}} \overline{\rho}_{j,r,i}^{(t)},\] (F.2)

where the second inequality is by Corollary E.3 and hence

\[|\underline{\rho}_{-y_{i},r,i}^{(t)}| \leq cR_{\min}^{-2}pn\overline{\rho}_{y_{i},r^{\prime},i^{\prime}, i^{\prime}}^{(t)},\forall r,r^{\prime}\in[m],\forall i,i^{\prime}\in[n],\] \[|\underline{\rho}_{-y_{i},r,i}^{(t)}| \leq\frac{cR_{\min}^{-2}pn}{|S_{i^{\prime}}^{(0)}|}\sum_{r\in S_{ i^{\prime}}^{(t)}}\overline{\rho}_{y_{i},r,i^{\prime}}^{(t)},\forall r\in[m], \forall i,i^{\prime}\in[n],\] \[|\underline{\rho}_{-y_{i},r,i}^{(t)}| \leq\frac{1}{|S_{j}|}\sum_{i\in S_{j}}\frac{cR_{\min}^{-2}pn}{|S _{i}^{(0)}|}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)},\forall r \in[m],\forall i\in[n],\]

the last inequality is by

\[\frac{m|S_{-j}|\cdot cR_{\min}^{-4}R_{\max}^{2}pn}{|S_{j}|\cdot\min_{i\in S_{j} }|S_{i}^{(0)}|}\leq c^{\prime}cR_{\min}^{-4}R_{\max}^{2}pn\cdot\frac{|S_{-j}|} {|S_{j}|}\leq\frac{1}{2}.\]

Then, we have for \(t\geq T\) that

\[\|\mathbf{A}_{t}\mathbf{1}_{n}\|_{2} =\sqrt{\sum_{r=1}^{m}\bigg{(}\sum_{i=1}^{n}\rho_{j,r,i}^{(t)}\| \mathbf{x}_{i}\|_{2}^{-2}\bigg{)}^{2}}\] \[\geq\bigg{|}\sum_{r=1}^{m}\sum_{i=1}^{n}\rho_{j,r,i}^{(t)}\| \mathbf{x}_{i}\|_{2}^{-2}/\sqrt{m}\bigg{|}\] \[\geq\frac{R_{\max}^{-2}}{2\sqrt{m}}\sum_{i\in S_{j}}\sum_{r\in S_ {i}^{(0)}}\overline{\rho}_{j,r,i}^{(t)}\] \[\geq\frac{R_{\max}^{-2}|S_{j}||S_{i}^{(0)}|}{2\sqrt{m}}\cdot\log \Big{(}1+\frac{\eta R_{\min}^{2}e^{-\beta}}{2c^{\prime}nm}\cdot t\Big{)}\] \[\geq CR_{\max}^{-2}\sqrt{m}n\log(t)\]

where the second inequality is is by (F.2); the third inequality is by the second bullet of Lemma E.1. For \(\|\mathbf{X}^{\top}(\mathbf{X}\mathbf{X}^{\top})^{-1}\mathbf{1}_{n}\|_{2}\), we have

\[\|\mathbf{X}^{\top}(\mathbf{X}\mathbf{X}^{\top})^{-1}\mathbf{1}_{ n}\|_{2} =\sqrt{\mathbf{1}_{n}^{\top}(\mathbf{X}\mathbf{X}^{\top})^{-1} \mathbf{X}\mathbf{X}^{\top}(\mathbf{X}\mathbf{X}^{\top})^{-1}\mathbf{1}_{n}}\] \[=\sqrt{\mathbf{1}_{n}^{\top}(\mathbf{X}\mathbf{X}^{\top})^{-1} \mathbf{1}_{n}}\] \[\leq\frac{\|\mathbf{1}_{n}\|_{2}}{\sqrt{\lambda_{\min}(\mathbf{X} \mathbf{X}^{\top})}}\]

By the Gershgorin circle theorem, we know that \(\lambda_{\min}(\mathbf{X}\mathbf{X}^{\top})\) lies within at least one of the Gershgorin discs \(D((\mathbf{X}\mathbf{X}^{\top})_{ii},R_{i}),i\in[n]\) where \(D((\mathbf{X}\mathbf{X}^{\top})_{ii},R_{i})\) is a closed disc centered at \((\mathbf{X}\mathbf{X}^{\top})_{ii}=\|\mathbf{x}_{i}\|_{2}^{2}\) with radius \(R_{i}=\sum_{i^{\prime}\neq i}|(\mathbf{X}\mathbf{X}^{\top})_{ii^{\prime}}|=\sum _{i^{\prime}\neq i}|\langle\mathbf{x}_{i},\mathbf{x}_{i^{\prime}}\rangle|\). Assume \(\lambda_{\min}(\mathbf{X}\mathbf{X}^{\top})\) lies within \(D((\mathbf{X}\mathbf{X}^{\top})_{ii},R_{i})\), then we can get following lower bound for \(\lambda_{\min}(\mathbf{X}\mathbf{X}^{\top})\):

\[\lambda_{\min}(\mathbf{X}\mathbf{X}^{\top})\geq\|\mathbf{x}_{i}\|_{2}^{2}- \sum_{i^{\prime}\neq i}|\langle\mathbf{x}_{i},\mathbf{x}_{i^{\prime}}\rangle| \geq R_{\min}^{2}-np=(1-R_{\min}^{-2}np)R_{\min}^{2}\geq R_{\min}^{2}/2.\]

Therefore, we have

\[\|\mathbf{X}^{\top}(\mathbf{X}\mathbf{X}^{\top})^{-1}\mathbf{1}_{n}\|_{2}\leq \frac{\|\mathbf{1}_{n}\|_{2}}{\sqrt{\lambda_{\min}(\mathbf{X}\mathbf{X}^{\top} )}}\leq\frac{\sqrt{2n}}{R_{\min}}.\]

[MISSING_PAGE_EMPTY:39]

The result is natural at \(t=0\). Suppose that there exists \(\widetilde{t}\) such that (F.4) hold for all time \(0\leq t\leq\widetilde{t}-1\). By (5.4) and (F.4) at time \(\widetilde{t}-1\), we have

\[\overline{\rho}_{j,r,i}^{(\widetilde{t})}=\overline{\rho}_{j,r,i}^{(\widetilde {t}-1)}+\frac{\eta}{nm}\cdot|\ell_{i}^{\prime(\widetilde{t}-1)}|\cdot\|{\bf x }_{i}\|_{2}^{2}\geq\overline{\rho}_{j,r,i}^{(\widetilde{t}-1)}\geq 0\]

and hence the orthogonality of training data, we can get

\[\langle{\bf w}_{j,r}^{(\widetilde{t})},{\bf x}_{i}\rangle=\langle{\bf w}_{j,r }^{(0)},{\bf x}_{i}\rangle+\rho_{j,r,i}^{(\widetilde{t})}=\langle{\bf w}_{j,r }^{(0)},{\bf x}_{i}\rangle\geq 0.\]

Therefore, (F.4) hold at time \(\widetilde{t}\), which completes the induction.

For if \(\langle{\bf w}_{j,r}^{(0)},{\bf x}_{i}\rangle\geq 0\) and \(j=-y_{i}\), we first show that under the same condition as Theorem 4.3 it holds that

\[\underline{\rho}_{j,r,i}^{(T)}<-\langle{\bf w}_{j,r}^{(0)},{\bf x}_{i}\rangle.\]

Since \(T=C\eta^{-1}R_{\min}^{-2}/nm\), we have \(\max_{j,r,i}\{|\rho_{j,r,i}^{(t)}|\}=O(1)\) for \(t\leq T\). Therefore, we know that \(F_{+1}({\bf W}_{+1}^{(t)},{\bf x}_{i}),F_{-1}({\bf W}_{-1}^{(t)},{\bf x}_{i})= O(1)\). Thus there exists a positive constant \(c\) such that \(-\ell_{i}^{\prime(t)}\geq c\) for all \(i\in[n]\). Here we use the method of proof by contradiction. Assume \(\underline{\rho}_{j,r,i}^{(T)}\geq-\langle{\bf w}_{j,r}^{(0)},{\bf x}_{i}\rangle\). Since \(\underline{\rho}_{j,r,i}^{(T)}\leq\underline{\rho}_{j,r,i}^{(t)}\) for \(0\leq t\leq T\) which can be seen from (5.5), we have \(\underline{\rho}_{j,r,i}^{(t)}\geq-\langle{\bf w}_{j,r}^{(0)},{\bf x}_{i}\rangle\) for all \(t\leq T\). Then, we can get

\[\langle{\bf w}_{j,r}^{(t)},{\bf x}_{i}\rangle=\langle{\bf w}_{j,r}^{(0)},{\bf x }_{i}\rangle+\rho_{j,r,i}^{(t)}\geq 0,\forall t\leq T.\]

Therefore, by the non-negativeness of \(\langle{\bf w}_{j,r}^{(t)},{\bf x}_{i}\rangle\) and (5.5), we can get

\[|\underline{\rho}_{j,r,i}^{(t+1)}|=|\underline{\rho}_{j,r,i}^{(t)}|+\frac{ \eta}{nm}\cdot|\ell_{i}^{\prime(t)}|\cdot\|{\bf x}_{i}\|_{2}^{2}\geq|\underline {\rho}_{j,r,i}^{(t)}|+\frac{c\eta\|{\bf x}_{i}\|_{2}^{2}}{nm}\]

and hence

\[|\underline{\rho}_{j,r,i}^{(T)}|\geq\frac{c\eta\|{\bf x}_{i}\|_{2}^{2}T}{nm}= cC\|{\bf x}_{i}\|_{2}^{2}R_{\min}^{-2}\geq cC\geq\beta,\]

which is a contradiction. Therefore, \(\underline{\rho}_{j,r,i}^{(T)}<-\langle{\bf w}_{j,r}^{(0)},{\bf x}_{i}\rangle\). By (5.5), we have \(\underline{\rho}_{j,r,i}^{(t)}\leq\underline{\rho}_{j,r,i}^{(T)}<-\langle{ \bf w}_{j,r}^{(0)},{\bf x}_{i}\rangle\) for \(t\geq T\). Therefore,

\[\langle{\bf w}_{j,r}^{(t)},{\bf x}_{i}\rangle=\langle{\bf w}_{j,r}^{(0)},{ \bf x}_{i}\rangle+\rho_{j,r,i}^{(t)}<0,\forall t\geq T.\]

Plugging this into (5.5) gives us

\[\underline{\rho}_{j,r,i}^{(t+1)}=\underline{\rho}_{j,r,i}^{(t)}+\frac{\eta}{ nm}\cdot\ell_{i}^{(t)}\cdot\sigma^{\prime}(\langle{\bf w}_{j,r}^{(t)},{\bf x}_{i} \rangle)\cdot\|{\bf x}_{i}\|_{2}^{2}=\underline{\rho}_{j,r,i}^{(t)},\forall t \geq T.\]

This completes the proof of the first half of the lemma about the activation pattern as well as the first two properties of \(\rho_{j,r,i}^{(t)}\).

**Part 2.** Now we will show that

\[\lim_{t\to\infty}\overline{\rho}_{y_{i},r,i}^{(t)}/\log t=m/|S_{i}^{(0)}|,\] (F.5)

if \(\langle{\bf w}_{y_{i},r}^{(t)},{\bf x}_{i}\rangle\geq 0\). By the activation pattern, we can get

\[\overline{\rho}_{y_{i},r,i}^{(t+1)}=\overline{\rho}_{y_{i},r,i}^{(t)}+\frac{ \eta}{nm}\cdot|\ell_{i}^{\prime(t)}|\cdot\|{\bf x}_{i}\|_{2}^{2}, \forall t\geq 0,\] for \[\langle{\bf w}_{y_{i},i}^{(t)},{\bf x}_{i}\rangle\geq 0,\] (F.6) \[\underline{\rho}_{y_{i},r,i}^{(t)}=0, \forall t\geq 0,\] for \[\langle{\bf w}_{y_{i},i}^{(t)},{\bf x}_{i}\rangle<0,\]Given this activation pattern, we can get for \(t\geq 0\) that

\[F_{y_{i}}(\mathbf{W}_{y_{i}}^{(t)},\mathbf{x}_{i})-F_{-y_{i}}( \mathbf{W}_{-y_{i}}^{(t)},\mathbf{x}_{i}) =\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{y_{i},r}^{(t) },\mathbf{x}_{i}\rangle)-\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{-y_ {i},r}^{(t)},\mathbf{x}_{i}\rangle)\] \[\leq\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\langle\mathbf{w}_{y_{i},r }^{(t)},\mathbf{x}_{i}\rangle\] \[=\frac{1}{m}\sum_{r\in S_{i}^{(0)}}[\langle\mathbf{w}_{y_{i},r}^ {(0)},\mathbf{x}_{i}\rangle+\overline{\rho}_{y_{i},r,i}^{(t)}]\] \[\leq\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i }^{(t)}+\beta,\]

and

\[F_{y_{i}}(\mathbf{W}_{y_{i}}^{(t)},\mathbf{x}_{i})-F_{-y_{i}}( \mathbf{W}_{-y_{i}}^{(t)},\mathbf{x}_{i}) =\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{y_{i},r}^{(t) },\mathbf{x}_{i}\rangle)-\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}_{-y _{i},r}^{(t)},\mathbf{x}_{i}\rangle)\] \[\geq\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\langle\mathbf{w}_{y_{i},r }^{(t)},\mathbf{x}_{i}\rangle-\beta\] \[=\frac{1}{m}\sum_{r\in S_{i}^{(0)}}[\langle\mathbf{w}_{y_{i},r }^{(0)},\mathbf{x}_{i}\rangle+\overline{\rho}_{y_{i},r,i}^{(t)}]-\beta\] \[\geq\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i }^{(t)}-2\beta.\]

Therefore, we can get following upper and lower bounds for \(|\ell_{i}^{\prime(t)}|\):

\[|\ell_{i}^{\prime(t)}| \leq\exp\bigg{(}-\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho }_{y_{i},r,i}^{(t)}+2\beta\bigg{)},\forall t\geq 0,\] \[|\ell_{i}^{\prime(t)}| \geq\frac{1}{2}\exp\bigg{(}-\frac{1}{m}\sum_{r\in S_{i}^{(0)}} \overline{\rho}_{y_{i},r,i}^{(t)}-\beta\bigg{)},\forall t\geq 0.\]

And it follows that

\[\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t +1)} \leq\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{ (t)}+\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}e^{2\beta}}{nm}\cdot\exp\bigg{(}- \frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)}\bigg{)}, \forall t\geq 0,\] \[\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t +1)} \geq\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t )}+\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}e^{-\beta}}{2nm}\cdot\exp\bigg{(}- \frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)}\bigg{)}, \forall t\geq 0.\]

By leveraging Lemma H.1 as well as Lemma H.2 and taking

\[x_{t}=\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)},\]

we can get

\[\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)}\leq\log \bigg{(}1+\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}e^{2\beta}}{nm}\exp\Big{(}\frac{ \eta\|\mathbf{x}_{i}\|_{2}^{2}e^{2\beta}}{nm}\Big{)}\cdot t\bigg{)}\leq\log \bigg{(}1+\frac{2\eta\|\mathbf{x}_{i}\|_{2}^{2}e^{-\beta}}{nm}\cdot t\bigg{)},\]\[\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^{(t)}\geq\log\bigg{(} 1+\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}e^{-\beta}}{2nm}\cdot t\bigg{)}.\]

Therefore, we have

\[\lim_{t\to\infty}\frac{1}{m}\sum_{r\in S_{i}^{(0)}}\overline{\rho}_{y_{i},r,i}^ {(t)}/\log t=1.\]

Since \(\overline{\rho}_{y_{i},r,i}^{(t)}=\overline{\rho}_{y_{i},r^{\prime},i}^{(t)}\) for any \(r\neq r\in S_{i}^{(0)}\), we have

\[\lim_{t\to\infty}\overline{\rho}_{y_{i},r,i}^{(t)}/\log t=m/|S_{i}^{(0)}|,\]

which completes the proof. 

**Lemma F.3**.: For two-layer ReLU neural network defined in (E.1), there exists mutually orthogonal data \(\mathbf{x}_{1},\cdots,\mathbf{x}_{n}\) such that stable rank of \(\mathbf{W}_{j}^{(t)}\) will converge to \(2\pm o(1)\).

Proof of Lemma f.3.: By (5.1), we have

\[\mathbf{w}_{j,r}^{(t)}=\mathbf{w}_{j,r}^{(0)}+\underbrace{\sum_{i=1}^{n}\rho _{j,r,i}^{(t)}\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}}_{:=\mathbf{ v}_{j,r}^{(t)}}.\]

Given the definition of \(\mathbf{v}_{j,r}^{(t)}\), we have the following representation of \(\mathbf{v}_{j,r}^{(t)}\) and \(\mathbf{V}_{j}^{(t)}\).

\[\mathbf{V}_{j,r}^{(t)}=\begin{bmatrix}\rho_{j,r,1}^{(t)}\cdot\|\mathbf{x}_{1} \|_{2}^{-2}&\cdots&\rho_{j,r,n}^{(t)}\cdot\|\mathbf{x}_{n}\|_{2}^{-2}\\ \vdots&\ddots&\vdots\\ \rho_{j,m,1}^{(t)}\cdot\|\mathbf{x}_{1}\|_{2}^{-2}&\cdots&\rho_{j,m,n}^{(t)} \cdot\|\mathbf{x}_{n}\|_{2}^{-2}\end{bmatrix}\cdot\begin{bmatrix}\mathbf{x}_{ 1}\\ \vdots\\ \mathbf{x}_{n}\end{bmatrix}.\]

Assume \(n\) is an even number and \(\mathbf{x}_{1},\cdots,\mathbf{x}_{n/2}\) are with label \(+1\) while \(\mathbf{x}_{(n/2)+1},\cdots,\mathbf{x}_{n}\) are with label \(-1\). And we take \(\mathbf{x}_{1},\cdots,\mathbf{x}_{n}\) as \(\mathbf{e}_{1},\cdots,\mathbf{e}_{n}\). Given Lemma F.2, \(\mathbf{W}_{j}^{(t)}=\mathbf{W}_{j}^{(0)}+\mathbf{V}_{j}^{(t)}\) and such selection of training data, we have

\[\lim_{t\to\infty}\frac{\mathbf{W}_{+1}^{(t)}}{\log t}=[\mathbf{A}_{m\times(n /2)},\mathbf{0}_{m\times(n/2)}]\cdot[\mathbf{I}_{n},\mathbf{0}_{n\times(d-n) }]=[\mathbf{A}_{m\times(n/2)},\mathbf{0}_{m\times(d-(n/2))}],\]

\[\lim_{t\to\infty}\frac{\mathbf{W}_{+1}^{(t)}}{\log t}=[\mathbf{0}_{m\times(n /2)},\mathbf{B}_{m\times(n/2)}]\cdot[\mathbf{I}_{n},\mathbf{0}_{n\times(d-n) }]=[\mathbf{0}_{m\times(n/2)},\mathbf{B}_{m\times(n/2)},\mathbf{0}_{n\times(d- n)}],\]

\[\lim_{t\to\infty}\frac{\mathbf{W}^{(t)}}{\log t}=\begin{bmatrix}\mathbf{A}_{m \times(n/2)}&\mathbf{0}_{m\times(n/2)}&\mathbf{0}_{n\times(d-n)}\\ \mathbf{0}_{m\times(n/2)}&\mathbf{B}_{m\times(n/2)}&\mathbf{0}_{n\times(d-n)} \end{bmatrix}.\]

where

\[\mathbf{A}_{m\times(n/2)}=\underbrace{\begin{bmatrix}\mathds{1}[\langle\mathbf{ w}_{+1,1}^{(0)},\mathbf{x}_{1}\rangle\geq 0]&\cdots&\mathds{1}[\langle \mathbf{w}_{+1,1}^{(0)},\mathbf{x}_{n/2}\rangle\geq 0]\\ \vdots&\ddots&\vdots\\ \mathds{1}[\langle\mathbf{w}_{+1,m}^{(0)},\mathbf{x}_{1}\rangle\geq 0]&\cdots& \mathds{1}[\langle\mathbf{w}_{+1,m}^{(0)},\mathbf{x}_{n/2}\rangle\geq 0]\\ \end{bmatrix}}_{:=\mathbf{C}_{m\times(n/2)}}\cdot\mathrm{diag}\begin{bmatrix}m/|S_ {1}^{(0)}|\\ \vdots\\ m/|S_{n/2}^{(0)}|\end{bmatrix},\]\[\mathbf{B}_{m\times(n/2)}=\underbrace{\begin{bmatrix}\mathds{1}[\langle\mathbf{w}_{-1,1}^{(0)},\mathbf{x}_{(n/2)+1}\rangle\geq 0]&\cdots&\mathds{1}[\langle\mathbf{w}_{-1,1}^{(0)},\mathbf{x}_{n}\rangle\geq 0]\\ \vdots&\ddots&\vdots\\ \mathds{1}[\langle\mathbf{w}_{-1,m}^{(0)},\mathbf{x}_{(n/2)+1}\rangle\geq 0]& \cdots&\mathds{1}[\langle\mathbf{w}_{-1,m}^{(0)},\mathbf{x}_{n}\rangle\geq 0 ]\end{bmatrix}}_{:=\mathbf{D}_{m\times(n/2)}}\cdot\mathrm{diag}\begin{bmatrix}m/ |S_{(n/2)+1}^{(0)}|\\ \vdots\\ m/|S_{n}^{(0)}|\end{bmatrix}.\]

Then, we can get the stable rank limits as follows:

\[\lim_{t\to\infty}\frac{\|\mathbf{W}_{+1}^{(t)}\|_{F}^{2}}{\| \mathbf{W}_{+1}^{(t)}\|_{1}^{2}}=\frac{\|\mathbf{A}\|_{F}^{2}}{\|\mathbf{A} \|_{2}^{2}},\] \[\lim_{t\to\infty}\frac{\|\mathbf{W}_{-1}^{(t)}\|_{F}^{2}}{\| \mathbf{W}_{-1}^{(t)}\|_{2}^{2}}=\frac{\|\mathbf{B}\|_{F}^{2}}{\|\mathbf{B} \|_{2}^{2}},\] \[\lim_{t\to\infty}\frac{\|\mathbf{W}_{-1}^{(t)}\|_{F}^{2}}{\| \mathbf{W}^{(t)}\|_{2}^{2}}=\frac{\|\mathbf{A}\|_{F}^{2}+\|\mathbf{B}\|_{F}^ {2}}{(\max\{\|\mathbf{A}\|_{2},\|\mathbf{B}\|_{2}\})^{2}}.\]

Since \(\mathbf{x}_{1}=\mathbf{e}_{1},\cdots,\mathbf{x}_{n}=\mathbf{e}_{n}\), we can get

\[\mathds{1}[\langle\mathbf{w}_{j,r}^{(0)},\mathbf{x}_{i}\rangle\geq 0]= \mathds{1}[[\mathbf{w}_{j,r}^{(0)}]_{i}\geq 0].\]

Therefore, the entries of matrix \(\mathbf{C}\) and matrix \(\mathbf{D}\) can be regarded as i.i.d. random variables taking \(0\) or \(1\) with equal probability. For \(\|\mathbf{A}\|_{F}\) and \(\|\mathbf{B}\|_{F}\), we have

\[\|\mathbf{A}\|_{F}^{2} =\sum_{r=1}^{m}\sum_{i=1}^{n/2}\mathds{1}[[\mathbf{w}_{+1,r}^{(0)} ]_{i}\geq 0]\cdot(m/|S_{i}^{(0)}|)^{2},\] \[\|\mathbf{B}\|_{F}^{2} =\sum_{r=1}^{m}\sum_{i=(n/2)+1}^{n}\mathds{1}[[\mathbf{w}_{-1,r}^ {(0)}]_{i}\geq 0]\cdot(m/|S_{i}^{(0)}|)^{2}.\]

By Lemma B.2, we have with probability at least \(1-\delta\) that \(0.4m\leq|S_{i}^{(0)}|\leq 0.6m\). By Hoeffding's inequality, we have with probability at least \(1-2\delta\) that

\[\left|\|\mathbf{A}\|_{F}^{2}-\frac{m}{2}\sum_{i=1}^{n/2}(m/|S_{i}^{(0)}|)^{2 }\right|\leq\sqrt{\frac{m\log(2/\delta)}{2}\sum_{i=1}^{n/2}(m/|S_{i}^{(0)}|)^{ 4}}\leq\sqrt{\frac{625mn\log(2/\delta)}{32}},\]

\[\left|\|\mathbf{B}\|_{F}^{2}-\frac{m}{2}\sum_{i=(n/2)+1}^{n}(m/|S_{i}^{(0)}|) ^{2}\right|\leq\sqrt{\frac{m\log(2/\delta)}{2}\sum_{i=(n/2)+1}^{n}(m/|S_{i}^{( 0)}|)^{4}}\leq\sqrt{\frac{625mn\log(2/\delta)}{32}}.\]

Next, we estimate \(\|\mathbf{A}\|_{2}\) and \(\|\mathbf{B}\|_{2}\). Let \(\mathbf{A}=\widetilde{\mathbf{A}}+\mathbb{E}[\mathbf{A}]\) and \(\mathbf{B}=\widetilde{\mathbf{B}}+\mathbb{E}[\mathbf{B}]\). Assume \(\mathbf{G}\) be the \(m\times(n/2)\) matrix with all entries equal to \(1/2\). Then,

\[\mathbb{E}[\mathbf{A}]=\mathbf{G}\cdot\mathrm{diag}\underbrace{\begin{bmatrix} m/|S_{1}^{(0)}|\\ \vdots\\ m/|S_{n/2}^{(0)}|\end{bmatrix}}_{:=\mathbf{a}},\mathbb{E}[\mathbf{B}]=\mathbf{G }\cdot\mathrm{diag}\underbrace{\begin{bmatrix}m/|S_{(n/2)+1}^{(0)}|\\ \vdots\\ m/|S_{n}^{(0)}|\end{bmatrix}}_{:=\mathbf{b}}.\]

And the entries of matrix \(\widetilde{\mathbf{A}}\) and matrix \(\widetilde{\mathbf{B}}\) are independent, mean zero, sub-gaussian random variables. By Lemma H.3, we have with probability at least \(1-\delta\) that

\[\|\widetilde{\mathbf{A}}\|_{2} \leq\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+\sqrt{\log(2/\delta)}\big{)},\] \[\|\widetilde{\mathbf{B}}\|_{2} \leq\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+\sqrt{\log(2/\delta)} \big{)},\]where \(C\) is a constant. Let \(\mathbf{1}_{k}\) denote the row vector with \(k\) entries equal to 1. Then for \(\mathbb{E}[\mathbf{A}]\) and \(\mathbb{E}[\mathbf{B}]\), we have

\[\|\mathbb{E}[\mathbf{A}]\|_{2} =\max_{\mathbf{x}\in S^{\frac{3}{2-1}}}\|\mathbf{G}\mathrm{diag} (\mathbf{a})\mathbf{x}\|_{2}\] \[=\max_{\mathbf{x}\in S^{\frac{3}{2-1}}}\frac{1}{2}\|\mathbf{1}_{ \frac{7}{2}}^{\top}\mathbf{1}_{\frac{n}{2}}\mathrm{diag}(\mathbf{a})\mathbf{x }\|_{2}\] \[=\max_{\mathbf{x}\in S^{\frac{3}{2-1}}}\frac{\sqrt{m}}{2}|\mathbf{ 1}_{\frac{n}{2}}\mathrm{diag}(\mathbf{a})\mathbf{x}|\] \[=\max_{\mathbf{x}\in S^{\frac{3}{2-1}}}\frac{\sqrt{m}}{2}|\mathbf{ a}^{\top}\mathbf{x}|\] \[=\frac{\sqrt{m}\|\mathbf{a}\|_{2}}{2},\]

and

\[\|\mathbb{E}[\mathbf{B}]\|_{2}=\frac{\sqrt{m}\|\boldsymbol{b}\|_{2}}{2}.\]

By triangle inequality, we have

\[\|\mathbf{A}\|_{2} \geq\|\mathbf{C}\|_{2}-\|\widetilde{\mathbf{A}}\|_{2}\geq(\sqrt{ m}\|\mathbf{a}\|_{2})/2-\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+\sqrt{\log(2/\delta)} \big{)},\] \[\|\mathbf{A}\|_{2} \leq\|\mathbf{C}\|_{2}+\|\widetilde{\mathbf{A}}\|_{2}\leq(\sqrt{ m}\|\mathbf{a}\|_{2})/2+\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+\sqrt{\log(2/\delta)} \big{)},\] \[\|\mathbf{B}\|_{2} \geq\|\mathbf{C}\|_{2}-\|\widetilde{\mathbf{B}}\|_{2}\geq(\sqrt{ m}\|\boldsymbol{b}\|_{2})/2-\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+\sqrt{\log(2/ \delta)}\big{)},\] \[\|\mathbf{B}\|_{2} \leq\|\mathbf{C}\|_{2}+\|\widetilde{\mathbf{B}}\|_{2}\leq(\sqrt{ m}\|\boldsymbol{b}\|_{2})/2+\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+\sqrt{\log(2/ \delta)}\big{)}.\]

Notice that \(\|\mathbf{a}\|_{2}^{2}=\Theta(n)\) and \(\|\boldsymbol{b}\|_{2}^{2}=\Theta(n)\), then with probability at least \(1-2\delta\), we have

\[\frac{\|\mathbf{A}\|_{F}^{2}}{\|\mathbf{A}\|_{2}^{2}} \leq\frac{m\|\mathbf{a}\|_{2}^{2}/2+\sqrt{625mn\log(2/\delta)/32} }{\big{(}\sqrt{m}\|\mathbf{a}\|_{2}/2-\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+ \sqrt{\log(2/\delta)}\big{)}\big{)}^{2}}=2+o(1),\] \[\frac{\|\mathbf{A}\|_{F}^{2}}{\|\mathbf{A}\|_{2}^{2}} \geq\frac{m\|\mathbf{a}\|_{2}^{2}/2-\sqrt{625mn\log(2/\delta)/32} }{\big{(}\sqrt{m}\|\mathbf{a}\|_{2}/2+\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+ \sqrt{\log(2/\delta)}\big{)}\big{)}^{2}}=2-o(1),\] \[\frac{\|\mathbf{B}\|_{F}^{2}}{\|\mathbf{B}\|_{2}^{2}} \geq\frac{m\|\mathbf{b}\|_{2}^{2}/2+\sqrt{625mn\log(2/\delta)/32} }{\big{(}\sqrt{m}\|\boldsymbol{b}\|_{2}/2-\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+ \sqrt{\log(2/\delta)}\big{)}\big{)}^{2}}=2+o(1),\] \[\frac{\|\mathbf{B}\|_{F}^{2}}{\|\mathbf{B}\|_{2}^{2}} \geq\frac{m\|\mathbf{b}\|_{2}^{2}/2-\sqrt{625mn\log(2/\delta)/32} }{\big{(}\sqrt{m}\|\boldsymbol{b}\|_{2}/2+\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+ \sqrt{\log(2/\delta)}\big{)}\big{)}^{2}}=2-o(1).\]

This leads to

\[\lim_{t\to\infty}\frac{\|\mathbf{W}_{+1}^{(t)}\|_{F}^{2}}{\| \mathbf{W}_{+1}^{(t)}\|_{2}^{2}} =\frac{\|\mathbf{A}\|_{F}^{2}}{\|\mathbf{A}\|_{2}^{2}}=2\pm o(1),\] \[\lim_{t\to\infty}\frac{\|\mathbf{W}_{+1}^{(t)}\|_{F}^{2}}{\| \mathbf{W}_{-1}^{(t)}\|_{2}^{2}} =\frac{\|\mathbf{B}\|_{F}^{2}}{\|\mathbf{B}\|_{2}^{2}}=2\pm o(1).\]

For \(\mathbf{W}^{(t)}\), we have the following lower bound

\[\frac{\|\mathbf{A}\|_{F}^{2}+\|\mathbf{B}\|_{F}^{2}}{(\max\{\| \mathbf{A}\|_{2},\|\mathbf{B}\|_{2}\})^{2}} \geq\frac{m(\|\mathbf{a}\|_{2}^{2}+\|\boldsymbol{b}\|_{2}^{2})/2- \sqrt{625mn\log(2/\delta)/8}}{\big{(}\sqrt{m}\max\{\|\mathbf{a}\|_{2},\| \boldsymbol{b}\|_{2}\}/2+\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+\sqrt{\log(2/ \delta)}\big{)}\big{)}^{2}}\] \[\geq(2-o(1))\cdot\frac{\|\mathbf{a}\|_{2}^{2}+\|\boldsymbol{b}\|_ {2}^{2}}{(\max\{\|\mathbf{a}\|_{2},\|\boldsymbol{b}\|_{2}\})^{2}}\]\[\geq\frac{16}{9}-o(1),\]

where the third inequality is by \((5/3)\sqrt{n}\leq\|\mathbf{a}\|_{2}\leq(5/2)\sqrt{n}\) and \((5/3)\sqrt{n}\leq\|\bm{b}\|_{2}\leq(5/2)\sqrt{n}\) due to \(0.4m\leq|S_{i}^{(0)}|\leq 0.6m\). And

\[\frac{\|\mathbf{A}\|_{F}^{2}+\|\mathbf{B}\|_{F}^{2}}{(\max\{\| \mathbf{A}\|_{2},\|\mathbf{B}\|_{2}\})^{2}} \leq\frac{m(\|\mathbf{a}\|_{2}^{2}+\|\bm{b}\|_{2}^{2})/2+\sqrt{62 5mn\log(2/\delta)/8}}{\big{(}\sqrt{m}\max\{\|\mathbf{a}\|_{2},\|\bm{b}\|_{2}\}/ 2-\frac{C}{2}\big{(}\sqrt{m}+\sqrt{n}+\sqrt{\log(2/\delta)}\big{)}\big{)}^{2}}\] \[\leq(2+o(1))\cdot\frac{\|\mathbf{a}\|_{2}^{2}+\|\bm{b}\|_{2}^{2}} {(\max\{\|\mathbf{a}\|_{2},\|\bm{b}\|_{2}\})^{2}}\] \[\leq 9+o(1),\]

where the third inequality is by \((5/3)\sqrt{n}\leq\|\mathbf{a}\|_{2}\leq(5/2)\sqrt{n}\) and \((5/3)\sqrt{n}\leq\|\bm{b}\|_{2}\leq(5/2)\sqrt{n}\) due to \(0.4m\leq|S_{i}^{(0)}|\leq 0.6m\). Therefore,

\[\lim_{t\to\infty}\frac{\|\mathbf{W}\|_{F}^{2}}{\|\mathbf{W}\|_{2}^{2}}=\frac{ \|\mathbf{A}\|_{F}^{2}+\|\mathbf{B}\|_{F}^{2}}{(\max\{\|\mathbf{A}\|_{2},\| \mathbf{B}\|_{2}\})^{2}}\in[16/9-o(1),9+o(1)].\]

## Appendix G Margin Results and Loss Convergence

In this section, we prove the convergence rate of training loss as well as the increasing rate of margin for both two-layer ReLU and leaky ReLU networks defined as

\[f(\mathbf{W}^{(t)},\mathbf{x}) =F_{+1}(\mathbf{W}^{(t)}_{+1},\mathbf{x})-F_{-1}(\mathbf{W}^{(t) }_{-1},\mathbf{x})\] \[=\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}^{(t)}_{+1,r},\mathbf{x}\rangle)-\frac{1}{m}\sum_{r=1}^{m}\sigma(\langle\mathbf{w}^{(t)}_{ -1,r},\mathbf{x}\rangle),\] (G.1) \[\sigma \in\{\text{ReLU},\text{leaky ReLU}\}.\]

We first prove the following auxiliary lemma.

**Lemma G.1**.: For both two-layer leaky ReLU and ReLU neural networks defined in (G.1), the following properties hold for any \(t\geq 0\):

* \(y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})\geq-c\) for any \(i\in[n]\) where \(c\) is a positive constant.
* \(y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})-y_{k}f(\mathbf{W}^{(t)},\mathbf{x}_{ k})\leq C_{1}\) for any \(i,k\in[n]\) where \(C_{1}\) is a constant.
* \(\ell_{i}^{(t)}/\ell_{k}^{(t)}\leq C_{2}\) for any \(i,k\in[n]\) where \(C_{2}\) is a constant.
* \(S_{i}^{(t)}\subseteq S_{i}^{(t+1)}\) for any \(i\in[n]\), where \(S_{i}^{(t)}:=\{r\in[m]:\langle\mathbf{w}^{(t)}_{y_{i},r},\mathbf{x}_{i} \rangle\geq 0\}\).

Proof of Lemma g.1.: We prove this lemma by induction. When \(t=0\), since

\[|y_{i}f(\mathbf{W}^{(0)},\mathbf{x}_{i})| =\bigg{|}\sum_{j}jy_{i}F_{j}(\mathbf{W}^{(0)}_{j},\mathbf{x}_{i}) \bigg{|}\] \[=\bigg{|}\sum_{j}jy_{i}\cdot\frac{1}{m}\sum_{r=1}^{m}\sigma( \langle\mathbf{w}^{(0)}_{j,r},\mathbf{x}_{i}\rangle)\bigg{|}\] \[\leq\sum_{j}\frac{1}{m}\sum_{r=1}^{m}|\sigma(\langle\mathbf{w}^{( 0)}_{j,r},\mathbf{x}_{i}\rangle)|\] \[\leq\sum_{j}\frac{1}{m}\sum_{r=1}^{m}|\langle\mathbf{w}^{(0)}_{j,r},\mathbf{x}_{i}\rangle|\] \[\leq 2\beta,\]the first bullet holds as long as \(c\geq 2\beta\). We also have

\[y_{i}f(\mathbf{W}^{(0)},\mathbf{x}_{i})-y_{k}f(\mathbf{W}^{(0)},\mathbf{x}_{k}) \leq|y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})|+|y_{k}f(\mathbf{W}^{(0)},\mathbf{x }_{k})|\leq 4\beta,\]

which verifies the second bullet at time \(t=0\) as long as \(C_{1}\geq 4\beta\). This leads to

\[\frac{\ell_{i}^{\prime(0)}}{\ell_{k}^{\prime(0)}} =\frac{1+\exp(y_{k}f(\mathbf{W}^{(0)},\mathbf{x}_{k}))}{1+\exp(y_ {i}f(\mathbf{W}^{(0)},\mathbf{x}_{i}))}\] \[\leq 2\big{(}1+\exp(y_{k}f(\mathbf{W}^{(0)},\mathbf{x}_{k})-y_{i} f(\mathbf{W}^{(0)},\mathbf{x}_{i}))\big{)}\] \[\leq 2(1+\exp(C_{1}))\] \[\leq C_{2},\]

as long as \(C_{2}\geq 2(1+\exp(C_{1}))\). For any \(r\in S_{i}^{(0)}\), we have

\[\langle\mathbf{w}_{y_{i},r}^{(1)},\mathbf{x}_{i}\rangle =\langle\mathbf{w}_{y_{i},r}^{(0)},\mathbf{x}_{i}\rangle+\frac{ \eta}{nm}\sum_{i^{\prime}=1}^{n}|\ell_{i^{\prime}}^{\prime(0)}|\cdot\sigma^{ \prime}(\langle\mathbf{w}_{y_{i},r}^{(0)},\mathbf{x}_{i}\rangle)\cdot\langle y _{i^{\prime}}\mathbf{x}_{i^{\prime}},y_{i}\mathbf{x}_{i}\rangle\] \[=\langle\mathbf{w}_{y_{i},r}^{(0)},\mathbf{x}_{i}\rangle+\frac{ \eta}{nm}\cdot|\ell_{i}^{\prime(0)}|\cdot\|\mathbf{x}_{i}\|_{2}^{2}+\frac{ \eta}{nm}\sum_{i^{\prime}\neq i}|\ell_{i^{\prime}}^{\prime(0)}|\cdot\sigma^{ \prime}(\langle\mathbf{w}_{y_{i},r}^{(0)},\mathbf{x}_{i}\rangle)\cdot\langle y _{i^{\prime}}\mathbf{x}_{i^{\prime}},y_{i}\mathbf{x}_{i}\rangle\] \[\geq\langle\mathbf{w}_{y_{i},r}^{(0)},\mathbf{x}_{i}\rangle+\frac {\eta}{nm}\cdot|\ell_{i}^{\prime(0)}|\cdot\|\mathbf{x}_{i}\|_{2}^{2}-\frac{ \eta}{nm}\sum_{i^{\prime}\neq i}|\ell_{i^{\prime}}^{\prime(0)}|\cdot|\langle \mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle|\] \[\geq\langle\mathbf{w}_{y_{i},r}^{(0)},\mathbf{x}_{i}\rangle\geq 0,\]

where the second equality is by \(\langle\mathbf{w}_{y_{i},r}^{(0)},\mathbf{x}_{i}\rangle\geq 0\); the first inequality is by triangle inequality; the second inequality is by \(|\ell_{i^{\prime}}^{\prime(0)}|/|\ell_{i}^{\prime(0)}|\leq C_{2}\) and the condition that \(R_{\min}^{2}\geq Cnp\), \(C\) is a sufficiently large constant. This verifies the fourth bullet at time \(t=0\).

Now suppose there exists time \(\widetilde{t}\geq 0\) such that these four hypotheses hold for any \(0\leq t\leq\widetilde{t}\). We aim to prove that these conditions also hold for \(t=\widetilde{t}+1\). We first prove that \(y_{i}f(\mathbf{W}^{(\widetilde{t}+1)},\mathbf{x}_{i})\geq y_{i}f(\mathbf{W}^ {(\widetilde{t})},\mathbf{x}_{i})\). We have

\[y_{i}f(\mathbf{W}^{(\widetilde{t}+1)},\mathbf{x}_{i})-y_{i}f( \mathbf{W}^{(\widetilde{t})},\mathbf{x}_{i})\] \[=\sum_{j}y_{i}j\Big{(}F_{j}(\mathbf{W}_{j}^{(\widetilde{t}+1)}, \mathbf{x}_{i})-F_{j}(\mathbf{W}_{j}^{(\widetilde{t})},\mathbf{x}_{i})\Big{)}\] \[=\sum_{j}y_{i}j\cdot\frac{1}{m}\sum_{r=1}^{m}\Big{(}\sigma( \langle\mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle)-\sigma( \langle\mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle)\Big{)}\] \[=\sum_{j}y_{i}j\cdot\frac{1}{m}\sum_{r=1}^{m}\frac{\sigma( \langle\mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle)-\sigma( \langle\mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle)}{\langle \mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle-\langle\mathbf{w} _{j,r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle}\cdot\langle-\eta\cdot\nabla_{ \mathbf{w}_{j,r}}L_{S}(\mathbf{W}^{(\widetilde{t})}),\mathbf{x}_{i}\rangle\] \[=\sum_{j}y_{i}j\cdot\frac{1}{m}\sum_{r=1}^{m}\frac{\sigma( \langle\mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle)-\sigma( \langle\mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle)}{\langle \mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle-\langle\mathbf{w} _{j,r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle}\cdot\Big{\langle}\frac{\eta}{ nm}\sum_{i^{\prime}=1}^{n}|\ell_{i^{\prime}}^{\prime(\widetilde{t})}|\cdot\sigma^{ \prime}(\langle\mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{i^{\prime}} \rangle)\cdot jy_{i^{\prime}}\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\Big{\rangle}\] \[=\sum_{j}\frac{1}{m}\sum_{r=1}^{m}\frac{\sigma(\langle\mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle)-\sigma(\langle\mathbf{w}_{j,r}^{( \widetilde{t})},\mathbf{x}_{i}\rangle)}{\langle\mathbf{w}_{j,r}^{(\widetilde{t} +1)},\mathbf{x}_{i}\rangle-\langle\mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{ i}\rangle}\cdot\frac{\eta}{nm}|\ell_{i}^{\prime(\widetilde{t})}|\cdot\sigma^{ \prime}(\langle\mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle)\cdot \|\mathbf{x}_{i}\|_{2}^{2}\] \[\qquad+\sum_{j}\frac{1}{m}\sum_{r=1}^{m}\frac{\sigma(\langle \mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle)-\sigma(\langle \mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle)}{\langle\mathbf{w} _{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle-\langle\mathbf{w}_{j,r}^{( \widetilde{t})},\mathbf{x}_{i}\rangle}\cdot\frac{\eta}{nm}\sum_{i^{\prime}\neq i }|\ell_{i^{\prime}}^{\prime(\widetilde{t})}|\cdot\sigma^{\prime}(\langle \mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{i^{\prime}}\rangle)\cdot\langle y _{i^{\prime}}\mathbf{x}_{i^{\prime}},y_{i}\mathbf{x}_{i}\rangle.\]By the fourth induction hypothesis at time \(t=\widetilde{t}\), we have \(S_{i}^{(\widetilde{t}+1)}\subseteq S_{i}^{(\widetilde{t})}\) and hence

\[\frac{\sigma(\langle\mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle)- \sigma(\langle\mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle)}{ \langle\mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle-\langle \mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle}=\frac{\langle \mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle-\langle\mathbf{w}_ {j,r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle}{\langle\mathbf{w}_{j,r}^{( \widetilde{t}+1)},\mathbf{x}_{i}\rangle-\langle\mathbf{w}_{j,r}^{(\widetilde {t})},\mathbf{x}_{i}\rangle}=1,\] (G.2)

for \(j=y_{i}\) and \(r\in S_{i}^{(t)}\). For \(\sigma\in\{\text{ReLU},\text{leaky ReLU}\}\), \(\sigma\) is non-decreasing and \(1\)-Lipschitz continuous, which gives

\[0\leq\frac{\sigma(\langle\mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i} \rangle)-\sigma(\langle\mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{i} \rangle)}{\langle\mathbf{w}_{j,r}^{(\widetilde{t}+1)},\mathbf{x}_{i}\rangle- \langle\mathbf{w}_{j,r}^{(\widetilde{t})},\mathbf{x}_{i}\rangle}\leq 1.\] (G.3)

Then, we have

\[y_{i}f(\mathbf{W}^{(\widetilde{t}+1)},\mathbf{x}_{i})-y_{i}f( \mathbf{W}^{(\widetilde{t})},\mathbf{x}_{i}) \geq\frac{\eta}{nm^{2}}\sum_{r\in S_{i}^{(\widetilde{t})}}|\ell _{i}^{\prime(\widetilde{t})}|\cdot\|\mathbf{x}_{i}\|_{2}^{2}-\frac{\eta}{nm^ {2}}\sum_{j}\sum_{r=1}^{m}\sum_{i^{\prime}\neq i}|\ell_{i^{\prime}}^{\prime( \widetilde{t})}|\cdot|\langle\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle|\] \[\geq\frac{\eta}{2nm^{2}}\sum_{r\in S_{i}^{(\widetilde{t})}}|\ell _{i}^{\prime(\widetilde{t})}|\cdot\|\mathbf{x}_{i}\|_{2}^{2}\] \[=\frac{\eta|S_{i}^{(\widetilde{t})}|}{2nm^{2}}\cdot|\ell_{i}^{ \prime(\widetilde{t})}|\cdot\|\mathbf{x}_{i}\|_{2}^{2}\] \[\geq\frac{\eta}{5nm}\cdot|\ell_{i}^{\prime(\widetilde{t})}|\cdot \|\mathbf{x}_{i}\|_{2}^{2},\]

where the first inequality is by (G.2), (G.3), \(\sigma^{\prime}\in[0,1]\) and triangle inequality; the second inequality is by \(|\ell_{i^{\prime}}^{\prime(\widetilde{t})}|/|\ell_{i}^{\prime(\widetilde{t}) }|\leq C_{2}\), \(|S_{i}^{(\widetilde{t})}|\geq|S_{i}^{(0)}|\geq 0.4m\) and the condition that \(R_{\min}^{2}\geq Cnp\), \(C\) is a sufficiently large constant. And

\[y_{i}f(\mathbf{W}^{(\widetilde{t}+1)},\mathbf{x}_{i})-y_{i}f( \mathbf{W}^{(\widetilde{t})},\mathbf{x}_{i})\] \[\leq\sum_{j}\sum_{r=1}^{m}\frac{\eta}{nm^{2}}\sum_{r\in S_{i}^{( \widetilde{t})}}|\ell_{i}^{\prime(\widetilde{t})}|\cdot\|\mathbf{x}_{i}\|_{2 }^{2}+\frac{\eta}{nm^{2}}\sum_{j}\sum_{r=1}^{m}\sum_{i^{\prime}\neq i}|\ell_{ i^{\prime}}^{\prime(\widetilde{t})}|\cdot|\langle\mathbf{x}_{i^{\prime}}, \mathbf{x}_{i}\rangle|\] \[\leq\frac{3\eta}{nm^{2}}\sum_{j}\sum_{r=1}^{m}|\ell_{i}^{( \widetilde{t})}|\cdot\|\mathbf{x}_{i}\|_{2}^{2}\] \[=\frac{3\eta}{nm}\cdot|\ell_{i}^{\prime(\widetilde{t})}|\cdot\| \mathbf{x}_{i}\|_{2}^{2},\]

where the first inequality is by (G.3), \(\sigma^{\prime}\in[0,1]\) and triangle inequality; the second inequality is by \(|\ell_{i^{\prime}}^{\prime(\widetilde{t})}|/|\ell_{i}^{\prime(\widetilde{t}) }|\leq C_{2}\), \(|S_{i}^{(\widetilde{t})}|\geq|S_{i}^{(0)}|\geq 0.4m\) and the condition that \(R_{\min}^{2}\geq Cnp\), \(C\) is a sufficiently large constant. Now, we obtain

\[y_{i}f(\mathbf{W}^{(\widetilde{t}+1)},\mathbf{x}_{i}) \geq y_{i}f(\mathbf{W}^{(\widetilde{t})},\mathbf{x}_{i})+\frac{\eta}{5 nm}\cdot|\ell_{i}^{\prime(\widetilde{t})}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\] (G.4) \[y_{i}f(\mathbf{W}^{(\widetilde{t}+1)},\mathbf{x}_{i}) \leq y_{i}f(\mathbf{W}^{(\widetilde{t})},\mathbf{x}_{i})+\frac{3\eta}{ nm}\cdot|\ell_{i}^{\prime(\widetilde{t})}|\cdot\|\mathbf{x}_{i}\|_{2}^{2},\] (G.5)

which implies that \(y_{i}f(\mathbf{W}^{(\widetilde{t}+1)},\mathbf{x}_{i})\geq y_{i}f(\mathbf{W}^{( \widetilde{t})},\mathbf{x}_{i})\geq-c\). This verifies the first bullet at time \(t=\widetilde{t}+1\). By subtracting (G.5) from (G.4), we have

\[y_{k}f(\mathbf{W}^{(\widetilde{t}+1)},\mathbf{x}_{k})-y_{i}f( \mathbf{W}^{(\widetilde{t}+1)},\mathbf{x}_{i})\] \[\leq y_{k}f(\mathbf{W}^{(\widetilde{t})},\mathbf{x}_{k})-y_{i}f( \mathbf{W}^{(\widetilde{t})},\mathbf{x}_{i})+\frac{3\eta}{nm}\cdot|\ell_{k}^{ \prime(\widetilde{t})}|\cdot\|\mathbf{x}_{k}\|_{2}^{2}-\frac{\eta}{5nm}\cdot| \ell_{i}^{\prime(\widetilde{t})}|\cdot\|\mathbf{x}_{i}\|_{2}^{2}.\]If \(|\ell_{i}^{\prime(\overline{t})}|/|\ell_{k}^{\prime(\overline{t})}|\geq 15R^{2}\), then \(\frac{3\eta}{nm}\cdot|\ell_{k}^{\prime(\overline{t})}|\cdot\|\mathbf{x}_{k}\|_ {2}^{2}\leq\frac{\eta}{5nm}\cdot|\ell_{i}^{\prime(\overline{t})}|\cdot\| \mathbf{x}_{i}\|_{2}^{2}\) and hence

\[y_{k}f(\mathbf{W}^{(\overline{t}+1)},\mathbf{x}_{k})-y_{i}f(\mathbf{W}^{( \overline{t}+1)},\mathbf{x}_{i})\leq y_{k}f(\mathbf{W}^{(\overline{t})}, \mathbf{x}_{k})-y_{i}f(\mathbf{W}^{(\overline{t})},\mathbf{x}_{i})\leq C_{1}.\]

If \(|\ell_{i}^{\prime(\overline{t})}|/|\ell_{k}^{\prime(\overline{t})}|<15R^{2}\), then by Lemma H.5

\[y_{k}f(\mathbf{W}^{(\widetilde{t})},\mathbf{x}_{k})-y_{i}f(\mathbf{W}^{( \widetilde{t})},\mathbf{x}_{i})\leq\log(4|\ell_{i}^{(\widetilde{t})}|/|\ell_ {k}^{\prime(\overline{t})}|)<\log(60R^{2}),\]

and hence

\[y_{k}f(\mathbf{W}^{(\overline{t}+1)},\mathbf{x}_{k})-y_{i}f( \mathbf{W}^{(\overline{t}+1)},\mathbf{x}_{i}) \leq y_{k}f(\mathbf{W}^{(\overline{t})},\mathbf{x}_{k})-y_{i}f (\mathbf{W}^{(\overline{t})},\mathbf{x}_{i})+\frac{3\eta}{nm}\cdot|\ell_{k}^{ \prime(\overline{t})}|\cdot\|\mathbf{x}_{k}\|_{2}^{2}\] \[\leq\log(60R^{2})+1.\]

Combining the two cases, we have

\[y_{k}f(\mathbf{W}^{(\overline{t}+1)},\mathbf{x}_{k})-y_{i}f(\mathbf{W}^{( \overline{t}+1)},\mathbf{x}_{i})\leq C_{1},\]

as long as \(C_{1}\geq\max\{4\beta,\log(60R^{2})+1\}\), which verifies the fourth bullet at time \(t=\widetilde{t}+1\). By Lemma H.5, this leads to

\[\frac{\ell_{i}^{\prime(\overline{t}+1)}}{\ell_{k}^{\prime(\overline {t}+1)}} =\frac{1+\exp(y_{k}f(\mathbf{W}^{(\overline{t}+1)},\mathbf{x}_{k}))}{1+ \exp(y_{i}f(\mathbf{W}^{(\overline{t}+1)},\mathbf{x}_{i}))}\] \[\leq 2\big{(}1+\exp(y_{k}f(\mathbf{W}^{(\overline{t}+1)},\mathbf{ x}_{k})-y_{i}f(\mathbf{W}^{(\overline{t}+1)},\mathbf{x}_{i}))\big{)}\] \[\leq 2(1+\exp(C_{1}))\] \[\leq C_{2},\]

as long as \(C_{2}\geq 2(1+\exp(C_{1}))\). For any \(r\in S_{i}^{(\overline{t}+1)}\), we have

\[\langle\mathbf{w}_{y_{i},r}^{(\overline{t}+2)},\mathbf{x}_{i}\rangle =\langle\mathbf{w}_{y_{i},r}^{(\overline{t}+1)},\mathbf{x}_{i} \rangle+\frac{\eta}{nm}\sum_{i^{\prime}=1}^{n}|\ell_{i^{\prime}}^{\prime( \overline{t}+1)}|\cdot\sigma^{\prime}\big{(}\langle\mathbf{w}_{y_{i},r}^{( \overline{t}+1)},\mathbf{x}_{i}\rangle\big{)}\cdot\langle y_{i^{\prime}} \mathbf{x}_{i^{\prime}},y_{i}\mathbf{x}_{i}\rangle\] \[=\langle\mathbf{w}_{y_{i},r}^{(\overline{t}+1)},\mathbf{x}_{i} \rangle+\frac{\eta}{nm}\cdot|\ell_{i}^{(\overline{t}+1)}|\cdot\|\mathbf{x}_{i} \|_{2}^{2}\] \[\qquad+\frac{\eta}{nm}\sum_{i^{\prime}\neq i}|\ell_{i^{\prime}}^{ \prime(\overline{t}+1)}|\cdot\sigma^{\prime}\big{(}\langle\mathbf{w}_{y_{i},r}^ {(\overline{t}+1)},\mathbf{x}_{i}\rangle\big{)}\cdot\langle y_{i^{\prime}} \mathbf{x}_{i^{\prime}},y_{i}\mathbf{x}_{i}\rangle\] \[\geq\langle\mathbf{w}_{y_{i},r}^{(\overline{t}+1)},\mathbf{x}_{i} \rangle+\frac{\eta}{nm}\cdot|\ell_{i}^{(\overline{t}+1)}|\cdot\|\mathbf{x}_{i} \|_{2}^{2}-\frac{\eta}{nm}\sum_{i^{\prime}\neq i}|\ell_{i^{\prime}}^{\prime( \overline{t}+1)}|\cdot|\langle\mathbf{x}_{i^{\prime}},\mathbf{x}_{i}\rangle|\] \[\geq\langle\mathbf{w}_{y_{i},r}^{(\overline{t}+1)},\mathbf{x}_{i} \rangle\geq 0,\]

where the second equality is by \(\langle\mathbf{w}_{y_{i},r}^{(\overline{t}+1)},\mathbf{x}_{i}\rangle\geq 0\); the first inequality is by triangle inequality; the second inequality is by \(|\ell_{i^{\prime}}^{\prime(\overline{t}+1)}|/|\ell_{i}^{\prime(\overline{t}+1) }|\leq C_{2}\) and the condition that \(R_{\min}^{2}\geq Cnp\), \(C\) is a sufficiently large constant. This verifies the fourth bullet at time \(t=\widetilde{t}+1\). 

Notice that \(\|\mathbf{W}^{(t)}\|_{F}=\Theta(\log t)\) and considering the fact that the difference between any two margins can be bounded by a constant, the difference between any two margins can be bounded by a constant, we can derive the following lemma, which demonstrates that the normalized margin of all the training data points will converge to the same value.

**Lemma G.2**.: For both two-layer ReLU and leaky ReLU neural networks, gradient descent will asymptotically find a neural network such that all the training data points possess the same normalized margin, i.e.,

\[\lim_{t\to\infty}\left|y_{i}f\Big{(}\frac{\mathbf{W}^{(t)}}{\|\mathbf{W}^{(t )}\|_{F}},\mathbf{x}_{i}\Big{)}-y_{k}f\Big{(}\frac{\mathbf{W}^{(t)}}{\| \mathbf{W}^{(t)}\|_{F}},\mathbf{x}_{k}\Big{)}\right|=0,\]

for any \(i,k\in[n]\).

By Lemma G.1, we can establish the subsequent lemma regarding the logarithmic rate of increase in margin. This lemma will be beneficial in demonstrating the convergence rate of the training loss in subsequent proofs.

**Lemma G.3**.: There exists time \(T=\Theta(\eta^{-1}R_{\min}^{-2}nm)\) such that the following increasing rate of margin \(y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})\) holds:

\[\Big{|}y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})-\log t-\log(\eta\|\mathbf{x}_{i }\|_{2}^{2}/nm)\Big{|}\leq C_{3},\]

\[C_{4}\cdot\Big{(}\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}}{nm}\cdot t\Big{)}^{-1} \leq|\ell_{i}^{\prime(t)}|\leq C_{5}\cdot\Big{(}\frac{\eta\|\mathbf{x}_{i}\|_ {2}^{2}}{nm}\cdot t\Big{)}^{-1},\]

for any \(i\in[n]\) and \(t\geq T\), where \(C_{3},C_{4},C_{5}\) are constants.

Proof of Lemma G.3.: To prove this, we want to leverage Lemma H.1 and Lemma H.2. To achieve this, we need approximate \(|\ell_{i}^{\prime(t)}|\) by \(y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})\). We have

\[|\ell_{i}^{\prime(t)}|=\frac{1}{1+\exp\big{(}y_{i}f(\mathbf{W}^{(t)}_{y_{i}}, \mathbf{x}_{i})\big{)}}\leq\exp\big{(}-y_{i}f(\mathbf{W}^{(t)}_{y_{i}}, \mathbf{x}_{i})\big{)},\] (G.6)

and

\[|\ell_{i}^{\prime(t)}|=\frac{1}{1+\exp\big{(}y_{i}f(\mathbf{W}^{(t)}_{y_{i}}, \mathbf{x}_{i})\big{)}}\geq\frac{1}{1+e^{c}}\cdot\exp\big{(}-y_{i}f(\mathbf{W} ^{(t)}_{y_{i}},\mathbf{x}_{i})\big{)}\] (G.7)

by \(y_{i}f(\mathbf{W}^{(t)}_{y_{i}},\mathbf{x}_{i})\geq-c\). Plugging the upper and lower bounds of \(|\ell_{i}^{\prime(t)}|\) into (G.4) and (G.5), we obtain

\[y_{i}f(\mathbf{W}^{(t+1)},\mathbf{x}_{i})-y_{i}f(\mathbf{W}^{(t)}, \mathbf{x}_{i}) \leq\frac{3\eta\|\mathbf{x}_{i}\|_{2}^{2}}{nm}\cdot\exp\big{(}-y_ {i}f(\mathbf{W}^{(t)}_{y_{i}},\mathbf{x}_{i})\big{)},\] (G.8) \[y_{i}f(\mathbf{W}^{(t+1)},\mathbf{x}_{i})-y_{i}f(\mathbf{W}^{(t) },\mathbf{x}_{i}) \geq\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}}{5(1+e^{c})nm}\cdot\exp \big{(}-y_{i}f(\mathbf{W}^{(t)}_{y_{i}},\mathbf{x}_{i})\big{)}.\] (G.9)

By taking \(x_{t}=y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})-y_{i}f(\mathbf{W}^{(0)}, \mathbf{x}_{i})\) and applying Lemma H.1 to (G.8), we can get

\[y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})\leq y_{i}f(\mathbf{W}^{(0)},\mathbf{ x}_{i})+\log\bigg{(}1+\frac{3\eta\|\mathbf{x}_{i}\|_{2}^{2}}{e^{y_{i}f(\mathbf{W}^{(0)}, \mathbf{x}_{i})}nm}\exp\Big{(}\frac{3\eta\|\mathbf{x}_{i}\|_{2}^{2}}{e^{y_{i}f( \mathbf{W}^{(0)},\mathbf{x}_{i})}nm}\Big{)}\cdot t\bigg{)}.\]

As long as \(t\geq\frac{e^{y_{i}f(\mathbf{W}^{(0)},\mathbf{x}_{i})}nm}{3\eta\|\mathbf{x}_{i }\|_{2}^{2}}\exp\Big{(}-\frac{3\eta\|\mathbf{x}_{i}\|_{2}^{2}}{e^{y_{i}f( \mathbf{W}^{(0)},\mathbf{x}_{i})}nm}\Big{)}=\Theta(\eta^{-1}R_{\min}^{-2}nm)\), we have

\[y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i}) \leq y_{i}f(\mathbf{W}^{(0)},\mathbf{x}_{i})+\log\bigg{(}\frac{6 \eta\|\mathbf{x}_{i}\|_{2}^{2}}{e^{y_{i}f(\mathbf{W}^{(0)},\mathbf{x}_{i})}nm }\exp\Big{(}\frac{3\eta\|\mathbf{x}_{i}\|_{2}^{2}}{e^{y_{i}f(\mathbf{W}^{(0)}, \mathbf{x}_{i})}nm}\Big{)}\cdot t\bigg{)}\] \[=\log t+\log\Big{(}\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}}{nm}\Big{)} +\log 6+\frac{3\eta\|\mathbf{x}_{i}\|_{2}^{2}}{e^{y_{i}f(\mathbf{W}^{(0)}, \mathbf{x}_{i})}nm}\] \[\leq\log t+\log\Big{(}\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}}{nm} \Big{)}+\log 6+1,\]

where the last inequality is by \(y_{i}f(\mathbf{W}^{(0)},\mathbf{x}_{i})\leq 2\beta\) and \(\eta\leq(CR_{\max}^{2}/nm)^{-1}\), \(C\) is a sufficiently large constant. By taking \(x_{t}=y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i})-y_{i}f(\mathbf{W}^{(0)},\mathbf{x} _{i})\) and applying Lemma H.2 to (G.9), we can get

\[y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i}) \geq y_{i}f(\mathbf{W}^{(0)},\mathbf{x}_{i})+\log\Big{(}1+\frac{ \eta\|\mathbf{x}_{i}\|_{2}^{2}}{5(1+e^{c})e^{y_{i}f(\mathbf{W}^{(0)},\mathbf{x} _{i})}nm}\cdot t\Big{)}\] \[\geq y_{i}f(\mathbf{W}^{(0)},\mathbf{x}_{i})+\log\Big{(}\frac{ \eta\|\mathbf{x}_{i}\|_{2}^{2}}{5(1+e^{c})e^{y_{i}f(\mathbf{W}^{(0)},\mathbf{x} _{i})}nm}\cdot t\Big{)}\] \[=\log t+\log\Big{(}\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}}{nm}\Big{)} -\log\big{(}5(1+e^{c})\big{)}.\]Since

\[\frac{e^{y_{i}f(\mathbf{W}^{(0)},\mathbf{x}_{i})}nm}{3\eta\|\mathbf{x}_{i}\|_{2}^{ 2}}\exp\Big{(}-\frac{3\eta\|\mathbf{x}_{i}\|_{2}^{2}}{e^{y_{i}f(\mathbf{W}^{(0) },\mathbf{x}_{i})}nm}\Big{)}\leq\frac{e^{y_{i}f(\mathbf{W}^{(0)},\mathbf{x}_{i} )}nm}{3\eta\|\mathbf{x}_{i}\|_{2}^{2}}\leq\frac{e^{2\beta}}{3}\eta^{-1}R_{\min }^{-2}nm,\]

Taking

\[T=\left\lceil\frac{e^{2\beta}}{3}\eta^{-1}R_{\min}^{-2}nm\right\rceil\text{, }C_{3}=\max\big{\{}\log 6+1,\log\big{(}5(1+e^{c})\big{)} \big{\}}\]

completes the proof of the first equation. By plugging the margin upper bound into (G.6), we can get

\[|\ell_{i}^{\prime(t)}| \leq\exp\big{(}-y_{i}f(\mathbf{W}_{y_{i}}^{(t)},\mathbf{x}_{i}) \big{)}\] \[\leq\exp\big{(}-\log t-\log(\eta\|\mathbf{x}_{i}\|_{2}^{2}/nm)+C _{3}\big{)}\] \[\leq\exp(C_{3})\cdot\Big{(}\frac{\eta\|\mathbf{x}_{i}\|_{2}^{2}}{ nm}\cdot t\Big{)}^{-1}.\]

By plugging the margin lower bound into (G.7), we can get

\[|\ell_{i}^{\prime(t)}| \geq\frac{1}{1+e^{c}}\cdot\exp\big{(}-y_{i}f(\mathbf{W}_{y_{i}}^{ (t)},\mathbf{x}_{i})\big{)}\] \[\geq\frac{1}{1+e^{c}}\cdot\exp\big{(}-\log t-\log(\eta\|\mathbf{ x}_{i}\|_{2}^{2}/nm)-C_{3}\big{)}\] \[\geq\frac{1}{e^{C_{3}}(1+e^{c})}\cdot\Big{(}\frac{\eta\|\mathbf{ x}_{i}\|_{2}^{2}}{nm}\cdot t\Big{)}^{-1}.\]

Therefore, taking \(C_{4}=1/e^{C_{3}}(1+e^{c})\) and \(C_{5}=\exp(C_{3})\) completes the proof. 

Now we give the following lemma about the convergence rate of training loss.

**Lemma G.4**.: For both two-layer ReLU and leaky ReLU networks defined in (G.1), we have the following convergence rate of training loss

\[L_{S}(\mathbf{W}^{(t)})=\Theta(t^{-1}).\]

Proof.: Having obtained a lower bound for the margin in Lemma G.3, we can now use it to derive an upper bound for the loss function as follows:

\[L_{S}(\mathbf{W}^{(t)}) =\frac{1}{n}\sum_{i=1}^{n}\ell(y_{i}f(\mathbf{W}^{(t)},\mathbf{x }_{i}))\] \[=\frac{1}{n}\sum_{i=1}^{n}\log\big{(}1+\exp\big{(}-y_{i}f(\mathbf{ W}^{(t)},\mathbf{x}_{i})\big{)}\big{)}\] \[\leq\frac{1}{n}\sum_{i=1}^{n}\exp\big{(}-y_{i}f(\mathbf{W}^{(t)}, \mathbf{x}_{i})\big{)}\] \[\leq\frac{1}{n}\sum_{i=1}^{n}\exp\big{(}-\log t-\log(\eta\| \mathbf{x}_{i}\|_{2}^{2}/nm)+C_{3}\big{)}\] \[=\frac{1}{n}\sum_{i=1}^{n}\exp(C_{3})\cdot\Big{(}\frac{\eta\| \mathbf{x}_{i}\|_{2}^{2}}{nm}\cdot t\Big{)}^{-1}\] \[=O(t^{-1}).\]

where the first inequality is by \(\log(1+z)\leq z\); the second inequality is by Lemma G.3.

Having obtained an upper bound for the margin in Lemma G.3, we can now use it to derive a lower bound for the loss function as follows:

\[L_{S}(\mathbf{W}^{(t)})=\frac{1}{n}\sum_{i=1}^{n}\ell(y_{i}f(\mathbf{W}^{(t)}, \mathbf{x}_{i}))\]\[=\frac{1}{n}\sum_{i=1}^{n}\log\big{(}1+\exp\big{(}-y_{i}f(\mathbf{W}^{( t)},\mathbf{x}_{i})\big{)}\big{)}\] \[\geq\frac{1}{n}\sum_{i=1}^{n}\exp\big{(}-y_{i}f(\mathbf{W}^{(t)}, \mathbf{x}_{i})\big{)}-\exp\big{(}-2y_{i}f(\mathbf{W}^{(t)},\mathbf{x}_{i}) \big{)}\] \[\geq\frac{1}{n}\sum_{i=1}^{n}\exp\big{(}-\log t-\log(\eta\| \mathbf{x}_{i}\|_{2}^{2}/nm)-C_{3}\big{)}\] \[\qquad-\exp\big{(}-2(\log t+\log(\eta\|\mathbf{x}_{i}\|_{2}^{2}/ nm)-C_{3})\big{)}\] \[=\frac{1}{n}\sum_{i=1}^{n}\exp(-C_{3})\cdot\Big{(}\frac{\eta\| \mathbf{x}_{i}\|_{2}^{2}}{nm}\cdot t\Big{)}^{-1}-\exp(2C_{3})\cdot\Big{(}\frac {\eta\|\mathbf{x}_{i}\|_{2}^{2}}{nm}\cdot t\Big{)}^{-2}\] \[=\Omega(t^{-1}).\]

where the first inequality is by \(\log(1+z)\geq z-z^{2}/2\) for \(z\geq 0\); the second inequality is by Lemma G.3. This completes the proof. 

In addition to the aforementioned lemmas, in the case of leaky ReLU, assuming convergence in direction, we can demonstrate that the directional limit corresponds to a Karush-Kuhn-Tucker (KKT) point of the max-margin problem. This result is presented in the following lemma.

**Lemma G.5**.: For two-layer leaky ReLU network defined in (G.1), assume that \(\mathbf{W}^{(t)}\) converges in direction, i.e. the limit of \(\mathbf{W}^{(t)}/\|\mathbf{W}^{(t)}\|_{F}\) exists. Denote \(\lim_{t\to\infty}\mathbf{W}^{(t)}/\|\mathbf{W}^{(t)}\|_{F}\) as \(\bar{\mathbf{W}}\). There exists a scaling factor \(\alpha>0\) such that \(\alpha\bar{\mathbf{W}}\) satisfies Karush-Kuhn-Tucker (KKT) conditions of the following max-margin problem:

\[\min_{\mathbf{W}}\frac{1}{2}\|\mathbf{W}\|_{F}^{2},\qquad s.t.\qquad y_{i}f( \mathbf{W},\mathbf{x}_{i})\geq 1,\,\forall i\in[n].\] (G.10)

Proof.: We need to prove that there exists \(\lambda_{1},\cdots,\lambda_{n}\geq 0\) such that for every \(j\in\{\pm 1\}\) and \(r\in[m]\) we have

\[\bar{\mathbf{w}}_{j,r}=\sum_{i=1}^{n}\lambda_{i}\nabla_{\mathbf{w}_{j,r}} \big{(}y_{i}f(\bar{\mathbf{W}},\mathbf{x}_{i})\big{)}=\sum_{i=1}^{n}\lambda_{ i}y_{i}j\cdot\sigma^{\prime}(\langle\bar{\mathbf{w}}_{j,r},\mathbf{x}_{i} \rangle)\cdot\mathbf{x}_{i}.\] (G.11)

By (5.1), we know that

\[\bar{\mathbf{w}}_{j,r} =\lim_{t\to\infty}\frac{\mathbf{w}_{j,r}^{(t)}}{\|\mathbf{W}^{(t) }\|_{F}}\] \[=\lim_{t\to\infty}\frac{\mathbf{w}_{j,r}^{(0)}}{\|\mathbf{W}^{(t) }\|_{F}}+\sum_{i=1}^{n}\frac{\rho_{j,r,i}^{(t)}}{\|\mathbf{W}^{(t)}\|_{F}} \cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}\] \[=\lim_{t\to\infty}\sum_{i=1}^{n}\frac{\rho_{j,r,i}^{(t)}}{\| \mathbf{W}^{(t)}\|_{F}}\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}\] \[=\sum_{i=1}^{n}\lim_{t\to\infty}\frac{\rho_{j,r,i}^{(t)}}{\| \mathbf{W}^{(t)}\|_{F}}\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i},\]

where the second equality is by \(\|\mathbf{W}^{(t)}\|_{F}=\Theta(\log t)\) and the last equality is by the existence of \(\lim_{t\to\infty}\mathbf{W}^{(t)}/\|\mathbf{W}^{(t)}\|_{F}\) as well as the uniqueness of data-correlated decomposition. By Lemma D.3 and \(\|\mathbf{W}^{(t)}\|_{F}=\Theta(\log t)\), we can obtain that

\[\lim_{t\to\infty}\frac{\rho_{j,r,i}^{(t)}}{\|\mathbf{W}^{(t)}\|_{F}}=\lim_{t \to\infty}\frac{\rho_{j,r^{\prime},i}^{(t)}}{\|\mathbf{W}^{(t)}\|_{F}},\] (G.12)for any \(j\in\{\pm 1\}\), \(r,r^{\prime}\in[m]\), \(i\in[n]\), and

\[\lim_{t\to\infty}\frac{\rho^{(t)}_{j,r,i}}{\|\mathbf{W}^{(t)}\|_{F}} =-\gamma^{-1}\cdot\lim_{t\to\infty}\frac{\rho^{(t)}_{j,r^{\prime},i^{\prime}}} {\|\mathbf{W}^{(t)}\|_{F}},\] (G.13)

for any \(j\in\{\pm 1\}\), \(i,i^{\prime}\in[n]\) with \(j=y_{i}\), \(j=-y_{i^{\prime}}\) and \(r,r^{\prime}\in[m]\). Define

\[S_{j}=\{i\in[n]:y_{i}=j\},\,\lambda^{\prime}_{i}:=\lim_{t\to \infty}\frac{\rho^{(t)}_{y_{i},r,i}}{\|\mathbf{W}^{(t)}\|_{F}}.\]

By (G.12), we know \(\lambda^{\prime}_{i}\) is well defined and \(\lambda^{\prime}_{i}\geq 0\). And by (G.13), we know that for any \(r\in[m]\),

\[\lim_{t\to\infty}\frac{\rho^{(t)}_{-y_{i},r,i}}{\|\mathbf{W}^{(t)} \|_{F}}=-\gamma\lambda^{\prime}_{i}.\]

Then, we have

\[\widetilde{\mathbf{w}}_{j,r} =\sum_{i=1}^{n}\lim_{t\to\infty}\frac{\rho^{(t)}_{j,r,i}}{\| \mathbf{W}^{(t)}\|_{F}}\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}\] \[=\sum_{i\in S_{j}}\lim_{t\to\infty}\frac{\rho^{(t)}_{j,r,i}}{\| \mathbf{W}^{(t)}\|_{F}}\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}+ \sum_{i\in S_{-j}}\lim_{t\to\infty}\frac{\rho^{(t)}_{j,r,i}}{\|\mathbf{W}^{(t )}\|_{F}}\cdot\|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}\] \[=\sum_{i\in S_{j}}\lambda^{\prime}_{i}\cdot\|\mathbf{x}_{i}\|_{2 }^{-2}\cdot\mathbf{x}_{i}-\sum_{i\in S_{-j}}\gamma\lambda^{\prime}_{i}\cdot \|\mathbf{x}_{i}\|_{2}^{-2}\cdot\mathbf{x}_{i}.\]

By Lemma C.5, it holds for any \(t\geq T_{1}\) that

\[\sigma^{\prime}(\langle\mathbf{w}^{(t)}_{j,r},\mathbf{x}_{i} \rangle) =1, \forall j\in\{\pm 1\},\,i\in S_{j},\] \[\sigma^{\prime}(\langle\mathbf{w}^{(t)}_{j,r},\mathbf{x}_{i} \rangle) =\gamma, \forall j\in\{\pm 1\},\,i\in S_{-j}.\]

This leads to

\[\sigma^{\prime}(\langle\bar{\mathbf{w}}_{j,r},\mathbf{x}_{i} \rangle) =1, \forall j\in\{\pm 1\},\,i\in S_{j},\] \[\sigma^{\prime}(\langle\bar{\mathbf{w}}_{j,r},\mathbf{x}_{i} \rangle) =\gamma, \forall j\in\{\pm 1\},\,i\in S_{-j}.\]

Thus, we can get

\[\widetilde{\mathbf{w}}_{j,r} =\sum_{i\in S_{j}}\lambda^{\prime}_{i}\cdot\sigma^{\prime}( \langle\bar{\mathbf{w}}_{j,r},\mathbf{x}_{i}\rangle)\cdot\|\mathbf{x}_{i}\|_{2 }^{-2}\cdot\mathbf{x}_{i}-\sum_{i\in S_{-j}}\lambda^{\prime}_{i}\cdot\sigma^{ \prime}(\langle\bar{\mathbf{w}}_{j,r},\mathbf{x}_{i}\rangle)\cdot\|\mathbf{x}_ {i}\|_{2}^{-2}\cdot\mathbf{x}_{i}\] \[=\sum_{i=1}^{n}\lambda^{\prime}_{i}y_{i}j\cdot\sigma^{\prime}( \langle\bar{\mathbf{w}}_{j,r},\mathbf{x}_{i}\rangle)\cdot\|\mathbf{x}_{i}\|_{2 }^{-2}\cdot\mathbf{x}_{i}.\]

Taking \(\lambda_{i}=\lambda^{\prime}_{i}\|\mathbf{x}_{i}\|_{2}^{-2}\) completes the proof of (G.11). On the other hand, by Lemma G.2 and the assumption of the existence of \(\mathbf{W}^{(t)}/\|\mathbf{W}^{(t)}\|_{F}\), we can get

\[y_{i}f(\tilde{\mathbf{W}},\mathbf{x}_{i})=y_{k}f(\tilde{\mathbf{W}},\mathbf{x }_{k}),\]

for any \(i,k\in[n]\). Taking \(\alpha=1/y_{i}f(\tilde{\mathbf{W}},\mathbf{x}_{i})\), we have

\[y_{i}f(\alpha\tilde{\mathbf{W}},\mathbf{x}_{i})=1,\]

for any \(i\in[n]\), which completes the proof. 

## Appendix H Auxiliary Lemmas

**Lemma H.1**.: Let \(\{x_{t}\}_{t=0}^{\infty}\) be an non-negative sequence satisfying the following inequality:

\[x_{t+1}-x_{t}\leq C\cdot e^{-x_{t}},\forall\,t\geq 0\]then we have

\[x_{t}\leq\log(e^{x_{0}}+Ce^{C}\cdot t).\]

Proof of Lemma H.1.: Given the inequality \(x_{t+1}-x_{t}\leq C\cdot e^{-x_{t}}\) for all \(t\geq 0\), we want to prove that \(x_{T}\leq\log(e^{x_{0}}+Ce^{C}\cdot T)\) for \(T\geq 0\). We start by manipulating the inequality as follows:

\[x_{t+1}-x_{t}\leq C\cdot e^{-x_{t}}\] \[\implies x_{t+1}-x_{t}\leq C\cdot e^{-x_{t+1}+C}\] (using

\[x_{t+1}\]

 instead of

\[x_{t}\]

) \[\implies e^{x_{t+1}}(x_{t+1}-x_{t})\leq Ce^{C}\] (multiplying both sides by

\[e^{x_{t+1}}\]

).

Summing the inequality from \(t=0\) to \(t=T-1\), we get:

\[\sum_{t=0}^{T-1}e^{x_{t+1}}(x_{t+1}-x_{t})\leq Ce^{C}\cdot T.\]

Since \(e^{x}\) is a monotone increasing function, we can approximate the above sum with an integral:

\[\int_{x_{0}}^{x_{T}}e^{x}dx\leq Ce^{C}\cdot T.\]

Evaluating the integral, we get:

\[e^{x_{T}}-e^{x_{0}}\leq Ce^{C}\cdot T.\]

Rearranging the inequality, we get:

\[e^{x_{T}}\leq e^{x_{0}}+Ce^{C}\cdot T.\]

Taking the natural logarithm of both sides, we get:

\[x_{T}\leq\log(e^{x_{0}}+Ce^{C}\cdot T).\]

Therefore, we have shown that \(x_{T}\leq\log(e^{x_{0}}+Ce^{C}\cdot T)\), as required. 

**Lemma H.2**.: Let \(\{x_{t}\}_{t=0}^{\infty}\) be an sequence satisfying the following inequality:

\[x_{t+1}-x_{t}\geq C\cdot e^{-x_{t}},\forall\,t\geq 0\]

then we have

\[x_{t}\geq\log(e^{x_{0}}+C\cdot t).\]

Proof of Lemma H.2.: Given the inequality \(x_{t+1}-x_{t}\geq C\cdot e^{-x_{t}}\) for all \(t\geq 0\), we want to prove that \(x_{T}\geq\log(e^{x_{0}}+C\cdot T)\) for \(T\geq 0\). We start by manipulating the inequality as follows:

\[x_{t+1}-x_{t}\geq C\cdot e^{-x_{t}}\] \[\implies e^{x_{t}}(x_{t+1}-x_{t})\geq C\] (multiplying both sides by

\[e^{x_{t}}\]

).

Summing the inequality from \(t=0\) to \(t=T-1\), we get:

\[\sum_{t=0}^{T-1}e^{x_{t}}(x_{t+1}-x_{t})\geq C\cdot T.\]

Since \(e^{x}\) is a monotone increasing function, we can approximate the above sum with an integral:

\[\int_{x_{0}}^{x_{T}}e^{x}dx\geq C\cdot T.\]

Evaluating the integral, we get:

\[e^{x_{T}}-e^{x_{0}}\geq C\cdot T.\]Rearranging the inequality, we get:

\[e^{x_{T}}\geq e^{x_{0}}+C\cdot T.\]

Taking the natural logarithm of both sides, we get:

\[x_{T}\geq\log(e^{x_{0}}+C\cdot T).\]

Therefore, we have shown that \(x_{T}\geq\log(e^{x_{0}}+C\cdot T)\), as required. 

**Lemma H.3** (Theorem 4.4.5 in Vershynin (2018)).: Let \(\mathbf{A}\) be an \(m\times n\) random matrix whose entries \(a_{ij}\) are independent, mean zero, sub-gaussian random variables. Then for any \(t>0\) we have

\[\|\mathbf{A}\|_{2}\leq CK(\sqrt{m}+\sqrt{n}+t)\]

with probability at least \(1-2\exp(-t^{2})\). Here \(K=\max_{i,j}\|a_{ij}\|_{\phi_{2}}\) where \(\|\cdot\|_{\phi_{2}}\) is the sub-gaussian norm.

**Lemma H.4**.: For \(t\geq s>0\), we have

\[\frac{\log(1+at)}{\log(1+bt)}\geq\frac{\log(1+as)}{\log(1+bs)},\]

if \(b>a>0\).

Proof of Lemma h.4.: Let \(f(t)=\log(1+at)/\log(1+bt)\), and we want to prove that \(f^{\prime}(t)>0\) for all \(t>0\). To find the derivative of \(f(t)\), we use the quotient rule:

\[f^{\prime}(t) =\frac{(\log(1+bt))\frac{d}{dt}(\log(1+at))-(\log(1+at))\frac{d}{ dt}(\log(1+bt))}{(\log(1+bt))^{2}}\] \[=\frac{(\log(1+bt))\frac{a}{1+at}-(\log(1+at))\frac{b}{1+bt}}{( \log(1+bt))^{2}}\] \[=\frac{a(1+bt)\log(1+bt)-b(1+at)\log(1+at)}{(1+at)(1+bt)(\log(1+ bt))^{2}}.\]

Next, we define the function \(g(t)=\big{(}\frac{1}{b}+t\big{)}\log(1+bt)-\big{(}\frac{1}{a}+t\big{)}\log(1+at)\), and we aim to show that \(g^{\prime}(t)>0\) for all \(t>0\). We start by computing the derivative of \(g(t)\):

\[g^{\prime}(t)=\log(1+bt)-\log(1+at).\]

Since \(b>a\) and \(t>0\), we have \(1+bt>1+at\), which implies that \(\log(1+bt)>\log(1+at)\). Therefore, we have \(g^{\prime}(t)>0\) for all \(t>0\). Note that \(g(0)=0\), we then have \(g(t)>0\) for all \(t>0\). Therefore, we have \(a(1+bt)\log(1+bt)-b(1+at)\log(1+at)>0\) for all \(t>0\), which in turn implies that \(f^{\prime}(t)>0\) for all \(t>0\). Thus, we have shown that \(f(t)\) is increasing for \(t>0\) and hence \(f(t)>f(s)\), which completes the proof. 

**Lemma H.5**.: Let \(g(z)=\ell^{\prime}(z)=-1/(1+\exp(z))\), then we have that

\[\frac{g(z_{2})}{g(z_{1})}\leq 2\big{(}1+\exp(z_{1}-z_{2})\big{)},\,\forall z_{1}, z_{2}\in\mathbb{R},\]

and

\[\frac{g(z_{2})}{g(z_{1})}\geq\frac{1}{4}\exp(z_{1}-z_{2}),\,\forall z_{1}\in \mathbb{R},z_{2}\geq-1.\]

Proof of Lemma h.5.: We first prove the first inequality. For if \(z_{1}\leq 0\), we have

\[\frac{g(z_{2})}{g(z_{1})}=\frac{1+\exp(z_{1})}{1+\exp(z_{2})}\leq\frac{2}{1+ \exp(z_{2})}\leq 2.\]

[MISSING_PAGE_EMPTY:55]