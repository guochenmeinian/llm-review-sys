# Can Models Learn Skill Composition from Examples?

 Haoyu Zhao1,2 Simran Kaur1,2 Dingli Yu1,2 Anirudh Goyal3 Sanjeev Arora1,2

1 Department of Computer Science, Princeton University

2 Princeton Language and Intelligence (PLI), Princeton University

3 Meta

{haoyu,arora}@cs.princeton.edu

###### Abstract

As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization--the capacity to combine learned skills in novel ways not encountered during training--has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment.

A recent study introduced the skill-mix[33] evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified \(k\)-tuple of language skills. While small models struggled with composing even with \(k=3\), larger models like GPT-4 performed reasonably well with \(k=5\) and \(6\).

In this paper, we employ a setup akin to skill-mix to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills--including rhetorical, literary, reasoning, theory of mind, and common sense--GPT-4 was used to generate text samples that exhibit random subsets of \(k\) skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of \(k\), revealed the following findings:

* Training on combinations of \(k=2\) and \(3\) skills results in noticeable improvements in the ability to compose texts with \(k=4\) and \(5\) skills, despite models never having seen such examples during training.
* When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills.

This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models.

## 1 Introduction

Today's large language models (LLMs) exhibit many impressive skills but remain imperfect in key areas. Arguably, one significant limitation stems from their difficulty in combining or composing the skills they have already learned. For example, solving a math problem on a specific topic may require a certain set of skills, while solving a more challenging question may require applying broader combinations of _more_ skills as compared to a simple question. Understanding how well models can learn to compose skills with limited training examples is therefore a crucial area of investigation.

Let us note why this is a nontrivial challenge. If a model has learned \(N\) base skills, and we want it to be able to compose any subset of \(k\) skills, there are \(\binom{N}{k}\) possible combinations of interest. Since \(\binom{N}{k}\) scales roughly with the \(k\)-th power of \(N\), even reasonable-sized training datasets will likely omitexamples of many skill combinations. As a result, the model must be capable of _generalizing_ to unseen combinations of skills.

This issue was highlighted in [1], which introduced a simple mathematical framework to demonstrate that current LLM scaling laws [13] suggest scaling up models can enhance their ability to combine \(k\) skills--though the ability increases only gradually with model size. This prediction was validated through the skill-mix evaluation [33], which directly tested models' capability to combine \(k\) language skills that were listed in the model's prompt (see Section 2 for details). The evaluation revealed that top-tier models like GPT-4 could successfully combine \(5\) or \(6\) language skills when composing a short piece of text, whereas smaller models like LLaMA-2-70B-Chat struggled to combine even \(3\) skills.

This finding of skill-mix evaluation raises an interesting question: even if pre-training does not fully endow models with the ability to combine skills, _can the capability be induced through fine-tuning on suitable data?_ The current paper investigates this question within the context of the language skills explored in the skill-mix evaluation [33].

### Our contributions

We address the question posed above by fine-tuning smaller models, including LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2, using a small yet high-quality dataset generated by GPT-4. This dataset consists of 13,957 text pieces, each composed of randomly selected \(k\) skills with \(k=1,2,3\). We evaluate the capability of the fine-tuned models to combine an another set of held-out skills with potentially higher \(k\). In particular, we divide the original skill-mix[33] skill set into a training set and a held-out set, based on skill categories, to minimize correlations between the two groups. This ensures a clearer evaluation of the models' ability to generalize to unseen skills. Figure 1 and Section 3 detail the full pipeline of our data generation and evaluation process. Our experimental results demonstrate the following findings (Section 4).

**Finding 1**: _Fine-tuning on texts that compose training skills improves capabilities of composing held-out skills._ Figure 2 shows the success rate of various models of combining \(k\) held-out skills. Before fine-tuning, LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 perform significantly worse than GPT-4, especially when \(k>2\). Both models improve remarkably after fine-tuning on our small dataset. For example, with \(k=3\), the success rate of LLaMA-2-13B-Chat increases from \(4\%\) to \(37\%\), and the success rate of Mistral-7B-Instruct-v0.2 increases from \(8\%\) to \(49\%\). Note in the original skill-mix, no model except GPT-4 could reach \(15\%\) success rate for \(k=3\) (see Table 3, [33]).

Figure 1: Pipeline for evaluating the generalization capability to combine skills. We split the language skill set \(\mathcal{S}\) from [33] into training skills \(\mathcal{S}_{\text{train}}\) and held-out skills \(\mathcal{S}_{\text{held-out}}\), and the topic set \(\mathcal{T}\) into training topics \(\mathcal{T}_{\text{train}}\) and held-out topics \(\mathcal{T}_{\text{held-out}}\). The pipeline consists of three steps: (1) generate data by prompting GPT-4. The training texts contain only training skills \(\mathcal{S}_{\text{train}}\) and training topics \(\mathcal{T}_{\text{train}}\), and each text exhibits at most 3 skills; (2) fine-tune LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 using the generated data; (3) evaluate the fine-tuned models on held-out skills \(\mathcal{S}_{\text{held-out}}\) and held-out topics \(\mathcal{T}_{\text{held-out}}\) with the number of requested skills being as large as 5. See our detailed setups in Section 3.

This phenomenon of compositional generalization from training skills to held-out skills suggests that the models are not merely learning to compose each individual combination of skills. Instead, they are acquiring a higher-order _meta-skill_ that allows them to generalize and apply to combine unseen skills.

**Finding 2**: _Fine-tuning on texts that compose a smaller number of skills leads to improvement of composing a larger number of skills._ Figure 2 demonstrates that fine-tuning on our small dataset, which includes texts composed of \(k=1,2\) or \(3\) training skills, leads to enhanced capability on composing \(k=4\) and \(5\) held-out skills, even though the models have never trained on such text. In Section 4, we present similar findings: (1) the ability to compose \(k\) training skills is also improved for \(k=4\) and \(5\) after fine-tuning; and (2) if models are fine-tuned exclusively with texts composed of no more than \(2\) training skills, they also show improved composition ability for \(3\) and \(4\) skills.

Note Finding 1 and Finding 2 are beyond the scope of the theory presented in Arora and Goyal [1], which studies the composition ability for skills that appear in the training data.

**Finding 3**: _Fine-tuning on texts that compose more skills (i.e., with a larger \(k\)) is more data-efficient for learning skill compositions._ We design control experiments in Section 4.3 that fine-tune LLaMA-2-13B-Chat on two datasets: (a) one dataset contains around 10,000 text pieces with only 1 or 2 skills; and (b) another dataset contains 8,000 text pieces, consisting of a random subset of the first dataset and around 2,000 text pieces that compose 3 skills. Table 4 shows that LLaMA-2 fine-tuned on the dataset with richer skill composition performs significantly better than the other for all \(k=2,3,4,5\).

We discuss our main findings thoroughly in Section 4. In Section 5.1, we solidify our findings using Claude 3 Opus (instead of GPT-4) as the Grader in evaluation. This eliminates the possibility that the ability to compose skills comes from GPT-4's bias towards the models fine-tuned on GPT-4's outputs.

We also discuss the implications of our findings for going beyond "stochastic parrots" behavior [2], which refers to the perception that LLMs might not generate novel pieces of text but rather mimic data from the pretraining corpus (Section 5.2).

## 2 Related Works

**Compositional generalization** Compositional generalization has grabbed lots of attention in AI. [29; 27] studied compositional generalization in the realm of mathematical reasoning, and [3; 22] investigated for logical inference. In computer vision, compositional generalization was studied on disentangled representation learning to generate images from novel combinations of concepts [12; 10; 31]. Besides, several works have explored composing visual relations [20], as well as benchmarks for text-to-visual generation[14; 19]. Other works have explored using compositional models for image generation [7], as well as to create plans for unseen tasks at inference time [6].

**Compositional generalization for language and LLMs** There is also a long history of study of compositional generalization in language [11; 18; 4; 15; 17; 21]. However, the test bed for compositional generalization mostly relies on rule-based languages, like SQL or synthetic-generated ones, and thus deviates a little bit from natural language. Recent works have observed compositional

Figure 2: The success rate of different models to compose \(k\) held-out skills in a short paragraph. (See the detailed definition of “Ratio of Full Marks” in Section 3.3.) The strongest model like GPT-4 can compose 5 skills in a short paragraph reasonably well, while smaller models struggle to compose even 3 skills. After fine-tuning, the models’ ability to compose skills improves significantly.

capabilities in LLMs emerge multiplicatively on natural languages [30; 1; 23; 33]. These observations have fueled a growing interest in exploring and evaluating compositional generalization in LLMs as a means to more appropriately evaluate LLM capabilities [9; 32; 24; 26; 33]. Some examples include imposing constraints and/or requirements on text generation [9; 32; 5], as well as providing multi-hop questions whose answers require composing multiple facts that were individually observed during pretraining [26]. Dziri et al. [8] tests whether the LLMs can learn compositional generalization by studying integer multiplication and a specific form of dynamic programming. Skill-Mix [33] presents an approach to evaluating compositional generalization on language skills, which we discuss in more detail in the next paragraph.

Skill-MixYu et al. [33] introduce a new evaluation named skill-mix that tests for models to produce novel pieces of text from random combinations of \(k\) skills, and the evaluation can be made more difficult by increasing the value of \(k\). The procedure is roughly as follows: (1) from a set of \(N\) language skills and \(T\) topics, pick a random subset of \(k\) skills and one topic; then, (2) query the Student model to produce a short piece of text (at most \(k-1\) sentences) that illustrates the \(k\) skills in the context of the provided topic. Note that for \(k=1\), the maximum sentence limit is 1 sentence. A Grader model is used to evaluate the text piece based on the following criteria: correctly illustrating all \(k\) skills and the topic, meeting the maximum length requirement, and general coherence. Thus, each piece of text can award up to a maximum of \(k+3\) points (see the original paper for various metrics extracted from points earned). Note that each of the \(N\) language skills has a Wikipedia entry, so it is reasonable to expect an LLM to encounter the skills multiple times in isolation in the pretraining corpus, but not in all possible combinations. In this paper, we choose to study the compositional generalization of LLMs in the context of skill-mix because skill-mix is close to general language capability and is more flexible for modifying the language skill set.

## 3 Pipeline

Our pipeline consists of three stages: generating data by selecting GPT-4 responses on skill-mix (Section 3.1), fine-tuning on the generated data (Section 3.2), and evaluating our fine-tuned model on skill-mix evaluation [33] (Section 3.3). The pipeline overview is shown in Figure 1.

### Data generation

We adapt the procedure presented in skill-mix evaluation [33] to produce finetuning data. Only the generations with full marks (i.e., illustrating all skills and topics, meeting the length requirement, and general coherence) are selected. To enhance the likelihood of obtaining full marks, we prompt GPT-4, the best Student model reported in Yu et al. [33], to create the generations.

Skills and topics for data generation.Since our goal is to measure the generalization capability strictly, we minimize the overlap between the skills/topics used during data generation and the skills/topics used to evaluate the fine-tuned models. Specifically, we partition the original set of 101 skills introduced in Yu et al. [33], \(\mathcal{S}\), into a set of 53 train skills, \(\mathcal{S}_{\text{train}}\), and 48 held-out skills, \(\mathcal{S}_{\text{held-out}}\), based on the skill category. \(\mathcal{S}_{\text{train}}\) includes only literary and rhetorical categories, while \(\mathcal{S}_{\text{held-out}}\) comprises the rest of the categories, including reasoning, logic, theory of mind, pragmatics, common sense, and physical knowledge. Similarly, we partition the original set of topics, \(\mathcal{T}\), into random sets of 50 training topics, \(\mathcal{T}_{\text{train}}\), and 50 held-out topics, \(\mathcal{T}_{\text{held-out}}\). It is important to note that partitioning skills randomly can lead to correlations between the train and held-out skills, as skills from the same category can be highly related. However, partitioning topics randomly does not present this issue, as the topics are generally unrelated. (Please refer to Appendix A for the detailed list of skills and topics.)

Data generation with \(k=1,2,3\).We produce fine-tuning data with \(k=1,2\) and \(3\) using GPT-4 as both the Student and Grader model. For \(k=1\), we use the original set of skills \(\mathcal{S}\) and training topics \(\mathcal{T}_{\text{train}}\) to produce approximately 5,000 generations, and we only keep generations that receive full marks. We refer to the resulting dataset as \(\mathcal{D}_{\text{skill-mix}}(1)\). \(\mathcal{D}_{\text{skill-mix}}(1)\) contains only texts with individual skills, thus serving the role of separating the improvement from better utilizing an individual skill and the improvement from better composing multiple skills in later experiments.

We follow an analogous procedure for \(k=2\) and \(k=3\), but using our 53 training skills \(\mathcal{S}_{\text{train}}\) and 50 training topics \(\mathcal{T}_{\text{train}}\). We produce 10,000 generations for each \(k\) before filtering. We refer to the resulting datasets as \(\mathcal{D}_{\text{skill-mix}}(2)\) and \(\mathcal{D}_{\text{skill-mix}}(3)\), respectively. For convenience, we use \(\mathcal{D}_{\text{skill-mix}}(1,2)\) to denote the dataset that combines \(\mathcal{D}_{\text{skill-mix}}(1)\) and \(\mathcal{D}_{\text{skill-mix}}(2)\), i.e., \(\mathcal{D}_{\text{skill-mix}}(1,2)=\mathcal{D}_{\text{skill-mix}}(1)\cup \mathcal{D}_{\text{skill-mix}}(2)\). Similarly, we use \(\mathcal{D}_{\text{skill-mix}}(1,2,3)\) to denote the dataset that combines \(\mathcal{D}_{\textsc{skill-mix}}(1)\), \(\mathcal{D}_{\textsc{skill-mix}}(2)\), and \(\mathcal{D}_{\textsc{skill-mix}}(3)\) together. We summarize our notations in Table 1.

### Fine-tuning

We fine-tune LLaMA-2-13B-Chat [28] and Mistral-7B-Instruct-v0.2 [16] on the data generated in Section 3.1 for 4000 steps with a batch size of 64. Each data generated from skill-mix consists of 4 parts: prompt1, answer1, prompt2, answer2. Here, prompt1 denotes the prompt asking the student to generate answers, answer1 stands for student's first round answer, prompt2 is the prompt that asks the student to correct or refine its answer, and answer2 is the student's second round answer. During fine-tuning, we feed the concatenation of prompt1, answer1, prompt2, answer2 into the model as a single text, but only compute the cross-entropy loss for tokens belonging to answer1 and answer2. We use Adam as the optimizer and linear warmup for the first 64 steps, followed by a constant learning rate of 2e-5 for the remaining training steps. 1 The maximum token length is set as 1024. All fine-tuning experiments are conducted on 4 Nvidia H100/A100 GPUs. Similarly to the loss design of RLHF [25], we mix pre-training data2 during fine-tuning to prevent degradation of general abilities.

Footnote 1: The learning rate selection is based on the recommendation in Touvron et al. [28]. We fine-tune LLaMA-2-13B-Chat on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) and perform a grid search on the batch size and total number of steps based on the skill-mix (\(k=3\)) evaluation performance on training skills and topics. The hyperparameters are transferred to other settings, including training on different data and different models.

Footnote 2: Since LLaMA-2 and Mistral do not release pre-training data with their models, we use a mixture of common crawl data and code data to approximate.

### Evaluation

We evaluate the skill-mix(\(k\)) performance (\(k=2,3,4,5\)) for all the models fine-tuned on data generated in Section 3.1, i.e., \(\mathcal{D}_{\textsc{skill-mix}}(1)\), \(\mathcal{D}_{\textsc{skill-mix}}(2)\), and \(\mathcal{D}_{\textsc{skill-mix}}(3)\).

**Settings** As mentioned earlier, skill-mix evaluation requires a skill set and a topic set. We consider the following 3 settings (where Setting II is our main setting used in Figure 1):

1. skill-mix evaluation on _training_ skills and topics. Since the model observes the same skills and topics during fine-tuning, this setting serves as an in-domain evaluation for \(k=2,3\). For \(k=4,5\), it tests the models' ability to combine more skills, which is already out-of-domain, since the model has never seen such data during fine-tuning. We use the notation skill-mixtrain(\(k\)) to denote the skill-mix(\(k\)) evaluation on training skills and topics.

\begin{table}
\begin{tabular}{|l|c c c|} \hline
**Symbol** & **Definition** & **Size** & **Misc** \\ \hline \(\mathcal{S}\) & All Skills & 101 & \(\mathcal{S}=\mathcal{S}_{\textsc{train}}\cup\mathcal{S}_{\textsc{held-out}}\) \\  & & & categories = \{literary, rhetorical, reasoning, \\  & & & logic,theory\_of\_mind, pragmatics, \\  & & & common\_sense, physical\_knowledge\} \\ \(\mathcal{S}_{\textsc{train}}\) & Train Skills & 53 & categories = \{literary, rhetorical\} \\ \(\mathcal{S}_{\textsc{held-out}}\) & Held Out Skills & 48 & categories = \{reasoning, logic, theory\_of\_mind, \\  & & & pragmatics, common\_sense, physical\_knowledge\} \\ \hline \(\mathcal{T}\) & All Topics & 100 & \(\mathcal{T}=\mathcal{T}_{\textsc{train}}\cup\mathcal{T}_{\textsc{held-out}}\) \\ \(\mathcal{T}_{\textsc{train}}\) & Train Topics & 50 & \(\mathcal{T}_{\textsc{train}}\subset\mathcal{T}\) \\ \(\mathcal{T}_{\textsc{held-out}}\) & Held Out Topics & 50 & \(\mathcal{T}_{\textsc{held-out}}\subset\mathcal{T}\) \\ \hline \(\mathcal{D}_{\textsc{skill-mix}}(1)\) & data with full mark on skill-mix (\(k=1\)) data with full mark on skill-mix (\(k=1\)) data with full mark on skill-mix (\(k=2\)) data with full mark on skill-mix (\(k=3\)) & 6277 & Created from \(\mathcal{S}_{\textsc{train}}\) and \(\mathcal{T}_{\textsc{train}}\) \\ \(\mathcal{D}_{\textsc{skill-mix}}(3)\) & & & \\ \hline \end{tabular}
\end{table}
Table 1: Notation used in data generation (Section 3.1)2. skill-mix on _held-out_ skills and topics. This setting tests the models' ability to combine skills that are never present in fine-tuning.3 This setting serves as another perspective to show the stronger out-of-domain generalization for composing skills compared to Setting I. We use the notation skill-mix\({}_{\text{held-out}}(k)\) to denote the skill-mix(\(k\)) evaluation on held-out skills and topics. Footnote 3: Precisely, the held-out skills appear in \(\mathcal{D}_{\text{skill-mix}}(1)\) since we want to make sure that the model at least knows all the skills after fine-tuning. However, the held-out skills never appear in \(\mathcal{D}_{\text{skill-mix}}(2)\) or \(\mathcal{D}_{\text{skill-mix}}(3)\), meaning that the model needs to be creative to compose \(k\) skills together for \(k>1\).
3. skill-mix on _all_ skills and topics. Evaluating skill-mix on only half of the skills split by category might make the evaluation easier, since combining 2 rhetorical or logical skills might be easier than combining 1 rhetorical and 1 logical skill. Thus, we also evaluate skill-mix on all skills and topics available, which serves as a direct comparison with the results in Yu et al. [33]. We use the notation skill-mix\({}_{\text{all}}(k)\) to denote the skill-mix(\(k\)) evaluation on all skills and topics.

Evaluation MetricsWe follow the evaluation rubric of skill-mix. Each generated text can receive up to \(k+3\) points: 1 point for each correctly illustrated skill, 1 point for sticking to the topic, 1 point for text coherence / making sense, and 1 point for meeting the length requirement.

Following Yu et al. [33], we grade each generated piece of text three times. For each of the \(k+3\) criteria, we collect the majority vote among the three grading rounds, and map the points earned to the following two metrics of interest4: (_Ratio of Full Marks_) count as 1 if all \(k+3\) points are earned, and 0 otherwise; and (_Skills Fraction_) the fraction of points awarded for the \(k\) skills if all 3 points are awarded for the remaining criteria, and 0 otherwise. For a given (\(k\) skill, 1 topic) combination, we take the maximum value of the metric among the 3 generations. We average the maximum value across all the combinations. Note that we use one of the harder variants of skill-mix[33], where we do not award any points for a particular skill if the skill name is explicitly mentioned in the generated text piece.

Footnote 4: Yu et al. [33] also report _Ratio of All Skills_, which we defer to Appendix C for clear presentation.

## 4 Skill Composition Can Be Learned From Examples

We present experiment results using the pipeline (Section 3) to evaluate compositional generalization. Table 2 and Table 3 summarizes the skill-mix (\(k\)) performances of LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 fine-tuned on various datasets under three evaluation settings. We discuss our findings on compositional generalization for in-domain evaluations (Section 4.1), compositional generalization for out-of-domain evaluations (Section 4.2), and the data efficiency to induce compositional generalization (Section 4.3).

\begin{table}
\begin{tabular}{|c|c c c c c|} \hline \multirow{2}{*}{Model} & \multicolumn{5}{c|}{skill-mix (\(k\)) Performance} \\  & \(k=1\) & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) \\ \hline \multicolumn{5}{|c|}{Evaluations on training skills and topics (skill-mix\({}_{\text{min}}(k)\))} \\ \hline LLaMA-2-13B-Chat & \(.527.52\) & \(.17\)/\(.47\) & \(.02\)/\(.34\) & \(.00\)/\(.33\) & \(.00\)/\(.31\) \\ ft’ed on \(\mathcal{D}_{\text{skill-mix}}(1)\) & \(.87\)/\(.87\) & \(.15\)/\(.51\) & \(.00\)/\(.43\) & \(.00\)/\(.37\) & \(.00\)/\(.35\) \\ ft’ed on \(\mathcal{D}_{\text{skill-mix}}(1,2)\) & \(.88\)/\(.88\) & \(.50\)/\(.70\) & \(.12\)/\(.56\) & \(.01\)/\(.55\) & \(.02\)/\(.52\) \\ ft’ed on \(\mathcal{D}_{\text{skill-mix}}(1,2)\) & \(.89\)/\(.89\) & \(.51\)/\(.73\) & \(.24\)/\(.68\) & \(.08\)/\(.64\) & \(.03\)/\(.60\) \\ \hline \multicolumn{5}{|c|}{Evaluations on held-out skills and topics (skill-mix\({}_{\text{held-out}}(k)\))} \\ \hline LLaMA-2-13B-Chat & \(.46\)/\(.46\) & \(.28\)/\(.50\) & \(.04\)/\(.42\) & \(.01\)/\(.39\) & \(.00\)/\(.43\) \\ ft’ed on \(\mathcal{D}_{\text{skill-mix}}(1)\) & \(.87\)/\(.87\) & \(.43\)/\(.70\) & \(.05\)/\(.54\) & \(.01\)/\(.49\) & \(.00\)/\(.44\) \\ ft’ed on \(\mathcal{D}_{\text{skill-mix}}(1,2)\) & \(.95\)/\(.95\) & \(.75\)/\(.87\) & \(.25\)/\(.68\) & \(.05\)/\(.60\) & \(.02\)/\(.56\) \\ ft’ed on \(\mathcal{D}_{\text{skill-mix}}(1,2,3)\) & \(.96\)/\(.96\) & \(.78\)/\(.88\) & \(.37\)/\(.75\) & \(.09\)/\(.69\) & \(.02\)/\(.60\) \\ \hline \multicolumn{5}{|c|}{Evaluations on all skills and topics (skill-mix\({}_{\text{all}}(k)\))} \\ \hline LLaMA-2-13B-Chat & \(.46\)/\(.46\) & \(.24\)/\(.50\) & \(.02\)/\(.42\) & \(.01\)/\(.40\) & \(.00\)/\(.34\) \\ ft’ed on \(\mathcal{D}_{\text{skill-mix}}(1)\) & \(.88\)/\(.88\) & \(.27\)/\(.62\) & \(.05\)/\(.50\) & \(.00\)/\(.40\) & \(.00\)/\(.33\) \\ ft’ed on \(\mathcal{D}_{\text{skill-mix}}(1,2)\) & \(.96\)/\(.96\) & \(.51\)/\(.74\) & \(.17\)/\(.65\) & \(.01\)/\(.54\) & \(.00\)/\(.51\) \\ ft’ed on \(\mathcal{D}_{\text{skill-mix}}(1,2,3)\) & \(.96\)/\(.96\) & \(.65\)/\(.81\) & \(.33\)/\(.73\) & \(.15\)/\(.69\) & \(.06\)/\(.62\) \\ \hline \end{tabular}
\end{table}
Table 2: Performance of fine-tuned LLaMA-2-13B-Chat on skill-mix (\(k\)) graded by GPT-4 in various settings. Ratio of Full Marks/Skills Fraction are reported for each model at different \(k=2,3,4,5\). \(\mathcal{D}_{\text{skill-mix}}(k)\) denote the data generated with full skill-mix (\(k\)) score. (see Section 3.1)

### Compositional generalization for in-domain evaluations

We first observe that, after fine-tuning LLaMA-2-13B-Chat on \(\mathcal{D}_{\textsc{skill-mix}}(1,2)\), the skill-mix\({}_{\text{train}}(2)\) performance significantly improves. Similarly, after fine-tuning LLaMA-2-13B-Chat on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\), the skill-mix\({}_{\text{train}}(3)\) performance also improves. For example, the Ratio of Full Marks for skill-mix\({}_{\text{train}}(3)\) improves from \(2\%\) for LLaMA-2-13B-Chat to \(24\%\) after fine-tuned on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) (Table 2).

One confounding factor in the above evaluation is that the original LLaMA-2-13B-Chat may not utilize all the individual skills perfectly, and the skill-mix performance improvement might just be attributed to the model's knowledge of the individual skills after fine-tuning, not the model's ability to better compose different skills together. Thus, we also evaluate the skill-mix performance on LLaMA-2-13B-Chat fine-tuned only on \(\mathcal{D}_{\textsc{skill-mix}}(1)\), which consists of purely skill-mix\(k=1\) data and serves as another baseline besides the original LLaMA-2-13B-Chat. After fine-tuning on \(\mathcal{D}_{\textsc{skill-mix}}(1)\), the model indeed knows the individual skills much better, since the Ratio of Full Marks of skill-mix\(k=1\) improves from \(52\%\) to \(87\%\). However, better knowledge of individual skills does not lead to a better ability to compose skills together, since the skill-mix\({}_{\text{train}}(2)\) or skill-mix\({}_{\text{train}}(3)\) performance of LLaMA-2-13B-Chat fine-tuned on \(\mathcal{D}_{\textsc{skill-mix}}(1)\) keeps nearly the same as the pre-trained ones, under both Ratio of Full Marks and Skills Fraction metrics. Thus, mainly all the improvement on skill-mix\({}_{\text{train}}(2)\) or skill-mix\({}_{\text{train}}(3)\) indeed comes from the ability to compose different skills together.

Fine-tuning on Mistral-7B-Instruct-v0.2 (Table 3) shows similar results as fine-tuning on LLaMA-2-13B-Chat (Table 3), except that only fine-tuning on \(\mathcal{D}_{\textsc{skill-mix}}(1)\) shows more improvement on skill-mix\({}_{\text{train}}(2)\) and skill-mix\({}_{\text{train}}(3)\) compared to the original Mistral-7B-Instruct-v0.2 model. A possible explanation is that Mistral-7B-Instruct-v0.2 is better at composing skills than LLaMA-2-13B-Chat, and fine-tuning on \(\mathcal{D}_{\textsc{skill-mix}}(1)\) helps Mistral-7B-Instruct-v0.2 exhibit each skill more properly and clearly when composing skills.

### Compositional generalization for out-of-domain evaluations

This section discusses the observations that indicate the out-of-domain generalization of skill composition, including generalization to unseen \(k\) and generalization to unseen skills.

skill-mix\({}_{\text{train}}(k)\) improves for unseen \(k\).We first observe that, after fine-tuning LLaMA-2-13B-Chat on skill-mix\({}_{\text{mix}}(1,2,3)\), the skill-mix\({}_{\text{train}}(4)\) and skill-mix\({}_{\text{train}}(5)\) performance also increase. For example, the Ratio of Full Marks improves from \(0\%\) to \(8\%\) when \(k=4\) (Table 2). Note that \(8\%\) Ratio of Full Marks improvement on \(k=4\) is significant, since besides GPT-4, all other models tested in Yu et al. [33], including GPT-3.5-turbo, cannot get over \(2\%\) Ratio of Full Marks on \(k=4\) (Table 3 in [33]). Besides, training only on \(\mathcal{D}_{\textsc{skill-mix}}(1)\) does not improve the skill-mix\({}_{\text{train}}(4)\) or skill-mix\({}_{\text{train}}(5)\).

\begin{table}
\begin{tabular}{|c|c c c c c|} \hline \multirow{2}{*}{Model} & \multicolumn{5}{c|}{skill-mix (\(k\)) Performance} \\  & \(k=1\) & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) \\ \hline \multicolumn{5}{|c|}{Evaluations on training skills and topics (skill-mix\({}_{\text{train}}(k)\))} \\ \hline \multicolumn{5}{|c|}{Mistral-7B-Instruct-v0.2} & \(86\)/\(86\) & \(18\)/\(51\) & \(05\)/\(46\) & \(01\)/\(36\) & \(00\)/\(35\) \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{Evaluations on held-out skills and topics (skill-mix\({}_{\text{head}}(k)\))} \\ \multicolumn{5}{|c|}{Mistral-7B-Instruct-v0.2} & \(85\)/\(85\) & \(48\)/\(73\) & \(08\)/\(56\) & \(01\)/\(42\) & \(01\)/\(39\) \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\)} \\ \multicolumn{5}{|c|}{ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(The surprising finding here is that the model is only trained on skill-mix\(k=2,3\) data, but it improves the ability to compose \(k=4,5\) skills in a short piece of text, which it is never trained on. The results suggest that its ability to compose multiple skills does not come from overfitting training data but should be perceived as learning a _meta-skill_ instead. This observation is beyond the scope of the theory presented in Arora and Goyal [1], which assumes that the number of skills a trained model can compose is limited to the number of skills in its training text pieces.

Fine-tuning on Mistral-7B-Instruct-v0.2 (Table 3) shows similar results, where the fine-tuned model is able to improve skill-mix\((4)\) and skill-mix\((5)\) after fine-tuning on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\), while training only on \(\mathcal{D}_{\textsc{skill-mix}}(1)\) has limited improvement.

**Improvement on skill-mix\((k)\) and skill-mix\((k)\).** Besides the skill-mix performance improvement on training skills and topics, we also observe the improvement of skill-mix\((k)\) (Setting II) from Table 2 and Figure 2. Similar to the evaluation on training skills and topics, fine-tuning LLaMA-2-13B-Chat on \(\mathcal{D}_{\textsc{skill-mix}}(1)\) only improves the skill-mix\((k)\) performance for \(k=3,4,5\) marginally, but it indeed improves the skill-mix\(k=2\). However, the improvement is incomparable with fine-tuning on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\). This shows that the ability to compose multiple skills generalizes to held-out skills, even though our training never exposed the model to data with the held-out skills. Besides the skill-mix improvement on held-out skills, we also observe the improvement of skill-mix\((k)\). This result again suggests that models learn _meta-skill_ rather than overfitting to skill combinations in the training data.

Note that the skill-mix\({}_{\text{held-out}}(k)\) performance is better than the skill-mix\((k)\) in Table 2, which is counter-intuitive. We hypothesize that this phenomenon happens because the pre-trained model knows how to compose held-out skills (logic, reasoning, theory of mind) better than training skills (rhetorical and literary). Or possibly the training skills are harder to compose. Exploring difficulty of individual skills is left for future work.

Also, fine-tuning on Mistral-7B-Instruct-v0.2 (Table 3) shows similar results as fine-tuning on LLaMA-2-13B-Chat (Table 2), except that Mistral-7B-Instruct-v0.2 has more improvement than LLaMA-2-13B-Chat when fine-tuned on \(\mathcal{D}_{\textsc{skill-mix}}(1)\).

### Data requirement for inducing compositional generalization

Compared with fine-tuning on \(\mathcal{D}_{\textsc{skill-mix}}(1,2)\), one can observe that LLaMA-2-13B-Chat/Mistral-7B-Instruct-v0.2 fine-tuned on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) gains more performance boost on \(k=4,5\) across all settings. For example, skill-mix\((4)\) performance for LLaMA-2-13B-Chat fine-tuned on \(\mathcal{D}_{\textsc{skill-mix}}(1,2)\) is nearly the same as the original LLaMA-2-13B-Chat and LLaMA-2-13B-Chat fine-tuned on \(\mathcal{D}_{\textsc{skill-mix}}(1)\). However, for LLaMA-2-13B-Chat fine-tuned on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\), the skill-mix\((4)\) performance improves from \(1\%\) to \(15\%\).

However, one may argue it is because \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) has more data in total than \(\mathcal{D}_{\textsc{skill-mix}}(1,2)\). To make a fair comparison, we conduct an ablation study by sub-sampling 8000 data from \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\), making sure that the number of data points with \(k=2\) and \(k=3\) in the sub-sampled set is less than the size of \(\mathcal{D}_{\textsc{skill-mix}}(2)\). Table 4 shows the skill-mix\((k)\) performance of LLaMA-2-13B-Chat fine-tuned on the sub-sampled dataset. The metrics remain relatively close to the model fine-tuned on full \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) and significantly better than the model fine-tuned on \(\mathcal{D}_{\textsc{skill-mix}}(1,2)\). This ablation confirms that "skill-richer" data can induce the ability to compose skills faster.

\begin{table}
\begin{tabular}{|l|c c c c|} \hline \multirow{2}{*}{Model} & \multicolumn{4}{c|}{skill-mix\((k)\) Performance} \\  & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) \\ \hline ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2)\) &.51/.74 &.17/.65 &.01/.54 &.00/.51 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) (8000 sample) &.66/.82 &.30/.74 &.11/.67 &.02/.62 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) &.65/.81 &.33/.73 &.15/.69 &.06/.62 \\ \hline \end{tabular}
\end{table}
Table 4: skill-mix\((k)\) performance of models fine-tuned on LLaMA-2-13B-Chat, graded by GPT-4. Ratio of Full Marks/Skills Fraction are reported for each model at different \(k\). \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) (8000 sample) denotes the randomly sub-sampled dataset from \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) with size 8000.

## 5 Discussions

### Using Claude 3 Opus as Grader for skill-mix evaluation

All the findings in the previous section are based on the skill-mix performance graded by GPT-4. However, GPT-4 is heavily used during data generation, and one can argue the improvement might solely come from the fact that GPT-4 favors its own outputs. Although the possibility is low, to rigorously eliminate this confounding factor, we re-evaluate skill-mix\({}_{\text{all}}(k)\) using Claude 3 Opus as the Grader, and report the results in Table 5. Besides, we also include a consistency check between the GPT-4 and Claude 3 Opus graders in Table 6.

From Table 5, we observe the metrics graded by Claude 3 Opus have a similar trend as those graded by GPT-4: after fine-tuning on \(\mathcal{D}_{\text{skill-mix}}(1,2,3)\), skill-mix\({}_{\text{all}}(k)\) performance improves for all \(k=2,3,4,5\), while fine-tuning only on \(\mathcal{D}_{\text{skill-mix}}(1)\) has limited improvement over the original LLaMA-2-13B-Chat. It proves that the improvement of skill-mix performance is not overfitted to GPT-4 preference.

Interestingly, we find that Claude 3 Opus is more generous, assigning higher scores to both the LLaMA-2-13B-Chat and the fine-tuned version. Besides, the results from Table 6 also confirm this argument: if an answer is assigned a full mark by GPT-4, then many of them will also be assigned a full mark by Claude 3 Opus. Such consistent biases among Graders were also noted in [33] when comparing LLaMA-2-70B-Chat and GPT-4 as Graders.

Besides switching to Claude 3 Opus, we also do human spot checks on the skill-mix generations, making sure that the model is not generating something that does not make sense to human. Please refer to Appendix D for some of the examples of skill-mix evaluations before and after the fine-tuning.

### Potential capability of going beyond "stochastic parrots behavior"

Whether models can go past "stochastic parrots" behavior [2] is crucial in discussions of AI risk. Based on reasonable performance of GPT-4 on skill-mix\((k=5)\) with common skills removed, Yu et al. [33] suggests GPT-4 is already beyond "stochastic parrots". In particular, after removing common skills (see definition in [33]), the probability of a random (5 skills, 1 topic) combination appearing in the training corpus is estimated to be \(11\%\) if the training token is \(2\text{T}^{5}\). Therefore, if a model has a Ratio of Full Marks beyond \(11\%\) when \(k=5\), then it suggests the model is able to output novel text, thus is beyond "stochastic parrots". GPT-4 is the only model that can achieve this in [33]6.

\begin{table}
\begin{tabular}{|l|c c c c|} \hline Model & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) \\ \hline \hline 
\begin{tabular}{c} Llama-2-13B-Chat \\ fit’ed on \(\mathcal{D}_{\text{skill-mix}}(1,2,3)\) \\ \end{tabular} & 0.24/0.31/0.19 & 0.02/0.07/0.01 & 0.01/0.06/0.01 & 0.00/0.00/0.00 \\ \hline \end{tabular}
\end{table}
Table 6: (**Consistency between GPT-4 and Claude-3 grader) skill-mix\((k)\) performance of models fine-tuned on LLaMA-2-13B-Chat, graded on Claude-3 and GPT-4. Ratio of Full Marks graded by Claude 3 Opus/Ratio of Full Marks by both graders are reported for each model at different \(k=2,3,4,5\).**

\begin{table}
\begin{tabular}{|l|c c c c|} \hline \multirow{2}{*}{Model} & \multicolumn{3}{c|}{skill-mix\({}_{\text{all}}(k)\) Performance} \\  & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) \\ \hline \multicolumn{5}{|c|}{Graded by Claude-3} \\ \hline \hline \begin{tabular}{c} Llama-2-13B-Chat \\ fit’ed on \(\mathcal{D}_{\text{skill-mix}}(1)\) \\ \end{tabular} &.31/5.2 &.07/4.48 &.08/6.64 &.00/4.42 \\ \hline \begin{tabular}{c} It’ed on \(\mathcal{D}_{\text{skill-mix}}(1)\) \\ \end{tabular} &.45/7.70 &.14/5.92 &.02/5.50 &.00/4.42 \\ \hline \begin{tabular}{c} It’ed on \(\mathcal{D}_{\text{skill-mix}}(1,2,3)\) \\ \end{tabular} &.69/8.1 &.57/8.83 &.26/7.77 &.10/6.69 \\ \hline \multicolumn{5}{|c|}{Graded by GPT-4} \\ \hline \hline \begin{tabular}{c} Llama-2-13B-Chat \\ fit’ed on \(\mathcal{D}_{\text{skill-mix}}(1)\) \\ \end{tabular} &.24/5.00 &.02/4.42 &.01/4.00 &.00/3.34 \\ \hline \begin{tabular}{c} It’ed on \(\mathcal{D}_{\text{skill-mix}}(1)\) \\ \end{tabular} &.27/6.62 &.05/5.00 &.00/4.40 &.00/3.33 \\ \hline 
\begin{tabular}{c} It’ed on \(\mathcal{D}_{\text{skill-mix}}(1,2,3)\) \\ \end{tabular} &.65/8.1 &.33/.73 &.15/6.99 &.06/6.62 \\ \hline \end{tabular}
\end{table}
Table 5: (**Comparison between GPT-4 and Claude-3 grader) skill-mix\((k)\) performance of models fine-tuned on LLaMA-2-13B-Chat, graded on Claude-3 and GPT-4. Ratio of Full Marks/Skills Fraction are reported for each model at different \(k=2,3,4,5\).**Table 7 shows the skill-mix (\(k=5\)) performance of fine-tuned LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 with common skills removed. We also include some \(\mathcal{D}_{\textsc{skill-mix}}(4)\) data (about 1000) into the training set. The fine-tuned models all show significant improvement over the base models. For example, the Ratio of Full Marks for the fine-tuned LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 all go beyond \(15\%\) for skill-mix (\(k=5\)) on training skills and topics, and reaches \(6\%\) on all skills and topics, after filtering out the common skills.

Although both fine-tuned models cannot reach \(11\%\) for skill-mix\({}_{\textsc{held-out}}(5)\) and skill-mix\({}_{\textsc{all}}(5)\), we hypothesize that with more skill-richer data (say \(\mathcal{D}_{\textsc{skill-mix}}(4)\)), the models can acquire the ability to combine skills much more efficiently and go beyond "stochastic parrots" eventually.

We further discuss the limitation of our work in Section 7, and potential influences on AI safety caused by stronger composition capability in Appendix E.

## 6 Conclusion and Takeaways

We have studied the extent to which models can learn compositional generalization over language skills by fine-tuning on suitable examples demonstrating such composition. Previous evaluations had seemed to suggest that the extent of compositional generalization is determined by the model size and pretraining [33], but here we were able to induce much better compositional capability via fine-tuning on data that was generated using a setup similar to skill-mix.

One surprising finding was that fine-tuning examples that composed \(2\) and \(3\) language skills were enough to improve the capability to compose \(4\) and even \(5\) language skills. Another surprise was that the ability to combine language skills from held-out categories improved at the same rate as the skills used in the training examples. Of course, these findings were still about skills that are near relatives. The full extent of such "out of (training) distribution" generalization remains to be explored.

## 7 Limitation

The main limitation of the current study is the high computational and financial costs, which impede us from sweeping more hyperparameters and conducting repeated experiments with different random seeds. These costs include the number of GPU hours for fine-tuning and the cost of calling OpenAI's API to generate the \(\mathcal{D}_{\textsc{skill-mix}}(k)\) data and evaluate the skill-mix performance. Despite these difficulties, we managed to sweep the hyperparameters for fine-tuning the LLaMA-2-13B-Chat on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) (Main experiment, Table 2). We believe our findings are robust to different random seeds because of the clear message and consistent trend of the results.

Besides, compositional generalization is a vast topic and we only study this under the setting of _language skills_ (limited to the skill-mix setting). Whether the models can learn compositional generalization in other settings still needs further exploration.

## Acknowledgement

The authors would like to thank the anonymous reviewers for their helpful comments and feedback. This work is supported by NSF, ONR, OpenAI, and Darpa.

\begin{table}
\begin{tabular}{|l|c c c|} \hline Model & skill-mix (\(5\)) Performance & \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} \\  & train & test & all \\ \hline LLaMA-2-13B-Chat &.00/.29 &.00/.38 &.00/.30 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) &.03/.56 &.02/.59 &.03/.61 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3,4)\) &.16/.68 &.05/.66 &.06/.56 \\ \hline Mistral-7B-Instruct-v0.2 &.00/.30 &.00/.32 &.00/.36 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) &.00/.66 &.03/.71 &.03/.70 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3,4)\) &.23/.73 &.02/.62 &.06/.63 \\ \hline \end{tabular}
\end{table}
Table 7: **(Filtering out common skills) skill-mix(\(5\)) performance of models fine-tuned on LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 graded by GPT-4. Ratio of Full Marks/Skills Fraction are reported for each model under skill-mix evaluation with train, test, and all skills (skill-mix\({}_{\textsc{train}}\)(5),skill-mix\({}_{\textsc{held-out}}\)(5),skill-mix\({}_{\textsc{all}}(5)\) respectively). We only consider skill combinations with uncommon skills whose occurrence rate in RedPajama is less than 5%.**

## References

* [1] Sanjeev Arora and Anirudh Goyal. A theory for emergence of complex skills in language models. _arXiv preprint arXiv:2307.15936_, 2023.
* [2] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers of stochastic parrots: Can language models be too big? In _Proceedings of the 2021 ACM conference on fairness, accountability, and transparency_, pages 610-623, 2021.
* [3] Samuel R Bowman, Christopher D Manning, and Christopher Potts. Tree-structured composition in neural networks without tree-structured architectures. _arXiv preprint arXiv:1506.04834_, 2015.
* [4] Rahma Chaabouni, Eugene Kharitonov, Diane Bouchacourt, Emmanuel Dupoux, and Marco Baroni. Compositionality and generalization in emergent languages. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 4427-4442, 2020.
* [5] Jiaoa Chen, Xiaoman Pan, Dian Yu, Kaiqiang Song, Xiaoyang Wang, Dong Yu, and Jianshu Chen. Skills-in-context prompting: Unlocking compositionality in large language models. _arXiv preprint arXiv:2308.00304_, 2023.
* [6] Yilun Du and Leslie Kaelbling. Compositional generative modeling: A single model is not all you need, 2024.
* [7] Yilun Du, Shuang Li, and Igor Mordatch. Compositional visual generation and inference with energy based models, 2020.
* [8] Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jiang, Bill Yuchen Lin, Sean Welleck, Peter West, Chandra Bhagavatula, Ronan Le Bras, et al. Faith and fate: Limits of transformers on compositionality. _Advances in Neural Information Processing Systems_, 36, 2024.
* [9] Ronen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still speak coherent english?, 2023.
* [10] Babak Esmaeili, Hao Wu, Sarthak Jain, Alican Bozkurt, Narayanaswamy Siddharth, Brooks Paige, Dana H Brooks, Jennifer Dy, and Jan-Willem Meent. Structured disentangled representations. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 2525-2534. PMLR, 2019.
* [11] Catherine Finegan-Dollak, Jonathan K Kummerfeld, Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui Zhang, and Dragomir Radev. Improving text-to-sql evaluation methodology. _arXiv preprint arXiv:1806.09029_, 2018.
* [12] Irina Higgins, Nicolas Sonnerat, Loic Matthey, Arka Pal, Christopher P Burgess, Matko Bosnjak, Murray Shanahan, Matthew Botvinick, Demis Hassabis, and Alexander Lerchner. Scan: Learning hierarchical compositional visual concepts. _arXiv preprint arXiv:1707.03389_, 2017.
* [13] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* [14] Kaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation, 2023.
* [15] Dieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia Bruni. Compositionality decomposed: how do neural networks generalise?, 2020.
* [16] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.

* [17] Daniel Keysers, Nathanael Scharli, Nathan Scales, Hylke Buisman, Daniel Furrer, Sergii Kashubin, Nikola Momchev, Danila Sinopalnikov, Lukasz Stafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang, Marc van Zee, and Olivier Bousquet. Measuring compositional generalization: A comprehensive method on realistic data, 2020.
* [18] Brenden Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. In _International conference on machine learning_, pages 2873-2882. PMLR, 2018.
* [19] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation, 2024.
* [20] Nan Liu, Shuang Li, Yilun Du, Joshua B. Tenenbaum, and Antonio Torralba. Learning to compose visual relations, 2021.
* [21] Qian Liu, Shengnan An, Jian-Guang Lou, Bei Chen, Zeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and Dongmei Zhang. Compositional generalization by learning analytical expressions. _Advances in Neural Information Processing Systems_, 33:11416-11427, 2020.
* [22] Mathijs Mul and Willem Zuidema. Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization. _arXiv preprint arXiv:1906.00180_, 2019.
* [23] Maya Okawa, Ekdeep Singh Lubana, Robert P. Dick, and Hidenori Tanaka. Compositional abilities emerge multiplicatively: Exploring diffusion models on a synthetic task, 2024.
* [24] Santiago Ontanon, Joshua Ainslie, Vaclav Cvicek, and Zachary Fisher. Making transformers solve compositional tasks, 2022.
* [25] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.
* [26] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language models, 2023.
* [27] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. _arXiv preprint arXiv:1904.01557_, 2019.
* [28] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_, 2023.
* [29] Sara Veldhoen, Dieuwke Hupkes, Willem H Zuidema, et al. Diagnostic classifiers revealing how neural networks process hierarchical structure. In _CoCo@ NIPS_, pages 69-77. Barcelona, 2016.
* [30] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022.
* [31] Zhenlin Xu, Marc Niethammer, and Colin A Raffel. Compositional generalization in unsupervised compositional representation learning: A study on disentanglement and emergent language. _Advances in Neural Information Processing Systems_, 35:25074-25087, 2022.
* [32] Shunyu Yao, Howard Chen, Austin W. Hanjie, Runzhe Yang, and Karthik Narasimhan. Collie: Systematic construction of constrained text generation tasks, 2023.
* [33] Dingli Yu, Simran Kaur, Arushi Gupta, Jonah Brown-Cohen, Anirudh Goyal, and Sanjeev Arora. Skill-mix: a flexible and expandable family of evaluations for ai models. In _The Twelfth International Conference on Learning Representations_, 2023.

[MISSING_PAGE_EMPTY:13]

[MISSING_PAGE_EMPTY:14]

[MISSING_PAGE_EMPTY:15]

\begin{tabular}{p{56.9pt} p{113.8pt} p{113.8pt}} \hline rhetorical & red herring & Introducing irrelevant points to detract attention from a question. & A member of the press asks the president why they voted to expand a welfare program. The president responds, "The strength of America is the strength of its communities, and I am proud to make our communities better places." \\ \hline rhetorical & non sequitur & An argument where the conclusion does not follow from the premises. & "Why are you wearing your shirt backwards? 'There will be a lunar eclipse tonight." \\ \hline rhetorical & rhetorical question & A question asked in order to create a dramatic effect or to make a point rather than to get an answer. & "How could you be so stupid?" \\ \hline rhetorical & category mistake (rhetorical error) & An error in which things belonging to a particular category are presented as if they belong to a different category. & A visitor to Oxford was being given a tour. The visitor, upon viewing the colleges and library, reportedly inquired, "But where is the University?" \\ \hline literary & anaphora resolution & Resolving the antecedent of a pronoun or noun phrase. & "The car is falling apart, but it still works." Here, "it" is the anaphor and "car" is the antecedent. \\ \hline literary & answer ellipsis & An ellipsis (omission of speech) that occurs in answers to questions. & ""Who walked the dog?" asked Mary. 'Sam,' replied Jill." This is an example of answer ellipsis, as Jill's complete answer would have been "Sam walked the dog". \\ \hline literary & anthropomorphism & The attribution of human traits, emotions, or intentions to non-human entities. & Some examples of anthropomorphism include "talking clocks", "singing teapots", "Buck the dog from Call of the Wild", and "feeling embarrassment." \\ \hline literary & antithesis & A figure of speech involving the bringing out of a contrast in the ideas by an obvious contrast in the words, clauses, or sentences, within a parallel grammatical structure. \\ \hline literary & antinymy & a lexical relation in which words have opposite meanings. \\ \hline literary & aphorism & A short saying that observes a general truth. & "Pride goeth before a fall." \\ \hline literary & aposiopesis & A figure of speech in which the speaker abruptly ends their sentence, leaving the statement incomplete. \\ \hline literary & antimetabole & the repetition of words in successive clauses, but in transposed order. & "Ask not what your country can do for you, but what you can do for your country." \\ \hline \end{tabular}

[MISSING_PAGE_EMPTY:17]

\begin{tabular}{p{56.9pt} p{113.8pt} p{113.8pt}} \hline \hline literary & slucing & A types of ellipsist that occurs in both direct and indirect interrogative clauses. & "Phoebe ate something, but she doesn't know what" is an example of slucing, because the full sentence is "Phoebe ate something, but she doesn't know what she ". \\ \hline literary & litotes & Using understatement to emphasize a point by stating a negative to further affirm a positive, often incorporating double negatives for effect. \\ \hline literary & tautology (language) & a statement that repeats an idea, using nears synonymous morphemes, words or phrases, effectively "saying the same thing twice". \\ \hline literary & unaccusative verb & A verb whose subject does not actively initiate, or is not actively responsible for, the action expressed by the verb. \\ \hline literary & topicalization (syntax mechanism) & A mechanism of syntax that establishes an expression as the sentence or clause topic by having it appear at the front of the sentence or clause (as opposed to in a canonical position further to the right). \\ \hline literary & pseudographing & Pseudographing is an ellipsis mechanism that removes most but not all of a non-finite verb phrase. & "He drinks milk more often than he does water." This is an example of pseudographing because the verb "drink" is removed from the phrase "he does drink water". \\ \hline literary & phrasal verb & a single semantic unit composed of a verb followed by a particle, sometimes combined with a proposition. \\ \hline literary & subjunctive clause & A clause in the subjunctive mood is a finite but tenseless clause. Subjunctive clauses use a bare or plain verb form, which lacks any inflection. \\ \hline literary & subordinate clause & A clause that is embedded within a complex sentence. & "Whoever made that assertion is wrong." In this case, "whoever made that assertion" is a subordinate clause. \\ \hline \hline \end{tabular}

\begin{table}
\begin{tabular}{p{56.9pt} p{113.8pt} p{130.1pt} p{130.1pt}} \hline \hline
**Category** & **Skill** & **Definition** & **Example** \\ \hline logical & enumerative induction & Using evidence from particular examples to conclude a property is true in general. & “Every particular life form we know of depends on water to exist. Therefore, all known life depends on water.” \\ \hline logical & spatial reasoning & The capacity to reason about the spatial relationships between objects. & The key fit into the box. Using spatial reasoning, one can deduce that the width of the key was smaller than the width of the box. \\ \hline logical & spatial orientation & Words or phrases used to situate people and objects in relation to each other in space. & Mark was sitting on a chair. Using spatial orientation skills, one can deduce that Mark was above the chair. \\ \hline logical & logical proposition & A logical proposition is a statement that takes no arguments and evaluates to True or False. & “3 + 2 = 4” is a proposition that evaluates to “True.” “3 + 2 = 4” is a proposition that evaluates to “False.” \\ \hline logical & categorical syllo-gism & Infers a conclusion from two premises. & “No geese are fellines. Some birds are geese. Therefore, some birds are not felines.” \\ \hline logical & modus ponens & A syllogism that is of the form “If P then Q. P. Hence Q.” & “If today is Tuesday, then John will go to work. Today is Tuesday. Therefore, John will go to work.” \\ \hline logical & modus tollens & A syllogism that is of the form “If P then Q. Not Q. Hence not P.” & “If it is sunny, I will wear my sunglasses. I am not wearing my sunglasses. Therefore, it is not sunny.” \\ \hline \hline \end{tabular}
\end{table}
Table 9: The list of held-out skills for evaluating skill-MIXheld-out(\(k\)). Combined with the training skills list in Table 8 is the full list of skills used for skill-MIXall(\(k\)). For each skill, we list its category, name, definition, and one example using this skill.

[MISSING_PAGE_EMPTY:20]

\begin{tabular}{p{85.4pt} p{398.3pt} p{398.3pt}} \hline theory of mind & recognizing false beliefs & The ability to understand that others can hold beliefs that do not align with reality or with one's own beliefs. \\ \hline theory of mind & root cause analysis & A method of problem solving whereby the underlying cause is found. \\ \hline theory of mind & divide and con-quer & Breaking a large difficult to solve problem into smaller, easier to solve parts. & "AnAl" exclaimed Stephen. "The leak is coming from here." The preceding is an example of root cause analysis, because Stephen has found the source of the leak. \\ \hline theory of mind & help seeking & The behavior of actively seeking help from other people. & "Freddie held up the broken clock to Marsha. 'Do you think you could fix it?" Here, Freddie seeks help from Marsha. \\ \hline theory of mind & trial and error & The process of experimenting with various methods until one is found the most successful. & One method of drug discovery is to try different chemicals at random until one has the desired effect. \\ \hline theory of mind & hypothesis testing & assuming a possible explanation to a problem and sometimes trying to prove (or disprove) said explanation. & "I assume all flies have the same number of petals. Let me try counting the number of petals on some liliies to see if my hypothesis fails." \\ \hline theory of mind & perspective awareness & The awareness that one's own mental states may not be known or accessible to others without communication. & "John, do you mind throwing that spider outside?" asked Jane. 'Actually, I'm afraid of spiders, so I can't,' replied John." \\ \hline theory of mind & visualization & The formation of a mental image of a real world object or phenomenon. & "Jerry watched the clock tick down towards his lunch hour. He could see his burger waiting for him in his mind's eye." \\ \hline theory of mind & expressing gratitude & is a feeling of appreciation (or similar positive response) by a recipient of another's kindness. & "Expressing her gratitude during her acceptance speech, the actress thanked her husband and parents for their support." \\ \hline theory of mind & self motivation & the ability to maintain a drive towards one's goals. & "If I get this done, I'll reward myself by playing video games" \\ \hline theory of mind & altruism & the principle and practice of concern for the well-being and/or happiness of other humans or animals. & Examples of altruism include caregiving for a relative with a chronic condition, and helping an older adult walking with a cane cross the street. \\ \hline \end{tabular}

\begin{tabular}{p{56.9pt} p{113.8pt} p{113.8pt}} \hline theory of the mind & emotional self reg- & a complex process that involves initiating, inhibiting, or modulating one's state or behavior in a given situation. \\ \hline theory of the mind & optimistic attitude & an attitude reflecting a belief or hope that the outcome of some specific endeavor, or outcomes in general, will be positive, favorable, and desirable. \\ \hline theory of the mind & optimistic attitude & an attitude reflecting a belief or hope that the outcome of some specific endeavor, or outcomes in general, will be positive, favorable, and desirable. \\ \hline theory of the mind & sympathy & the perception of, understanding of, and reaction to the distress or need of another life form. \\ \hline pragmatics & synecdoche & A figure of speech where the whole is represented by the part, or vice versa. \\ \hline pragmatics & presupposition & An implicit assumption about the world or background belief relating to an utterance whose truth is taken for granted in discourse. \\ \hline pragmatics & dilemma & A situation in which a choice must be made between two or more alternatives, especially equally undesirable ones. \\ \hline pragmatics & implicature (pragmatic suggestion) & Something the speaker suggests or implies with an utterance, even though it is not literally expressed. \\ \hline pragmatics & intention (pragmatics) & The ability to identify a speaker's intent. \\ \hline pragmatics & intention (pragmatics) & The ability to identify a speaker's intent. \\ \hline \end{tabular}

\begin{tabular}{p{56.9pt} p{113.8pt} p{113.8pt}} \hline physical knowledge & folk physics (common knowledge physics) & The untrained human perception of basic physical phenomena. \\ \hline \end{tabular}

## Appendix B Prompts for Data generation

In this section, we document our prompts for generation and prompts for grading.

### Prompts for generation

``` Gretings!IaminterestedinnaturallanguageprocessingandIwaswonderingifyoucouldhelpmegenerateanexampleoftextthatillustratesmultipleskillsinemanticsorsyntax.Theexampleshouldbeaniminalnaturalpieceoftextwithuptoafewlinesinthecontextoftopicthatillustratesallofthefollowingskills:(skills_str).Pleasekeepthetextasshortaspossible,andmakesuretheconceptscanbefoundfullyfromthetext. ```For reference, here are the definitions and examples for the concepts: {skills_defs_and_examples_simple} {Please start the minimal natural piece of text with 'Answer:' and start the explanation with 'Explanation:'. Thanks very much! PROMPT2 Thanks very much. Could you please look over your answer and improve it? Please make sure that the new answer better illustrates all skills and remains on topic. On top of that, please also make sure the new answer only contains up to {num_sentences_str}. Again, please start the improved minimal natural piece of text with 'Answer:' and start the explanation with 'Explanation:'. Thanks very much!

Note that this pair of prompts is the same as the generation prompts for GPT-4 in Yu et al. [33], which is slightly different from the generation prompts for LLAMA-2 in Yu et al. [33]. This difference of prompts, along with randomness from multiple sources, caused some difference in LLAMA-2-13B-Chat performance on skill-mix(\(k\)) between our paper and Yu et al. [33].

### Prompts for grading

Grading prompt for GPT-4 Grader:

Greetings! I was wondering if you could help me grade the following answer given by a student. I'll first describe the question that was given to the student, and them give you the student's answer, and the grading rubric. The question given to the student was an follows: "Give a single piece of text with up to {num_sentences_str} in the context of {topic}. This single piece of text should illustrate all of the following skills: {skills_str}." The student's answer was: "{student_answer}" For reference, here are the definitions for the skills: {skills_defs_and_examples_simple} Using a rubric table format, please grade the student's answer with positive scoring. Each criterion

\begin{table}
\begin{tabular}{c|c} \hline  & Skiling, Climbing Everest, Grocery cashiers, Sledding, Opera solo, \\  & Reunion, Archaeologists, Rabbis, Pirates, Acupuncture, \\  & Regatta, sailing, Elizabeth England, Basketball, Civil Law, \\  & Tropical rainforest, Ecology, Ancient Greece, Civil War, Broadway play, \\  & Tennis match, Coal mine, Bowling, Fitness, Confession, \\  & Animation, Podcasts, Documentaries, Indic music, Jazz music, \\  & Dubstep, Rugby, Veganism, cryptocurrency, Violinists, \\  & Pianists, Olympics, Woodworking, Crochet, Knitting, \\  & Sewing, Calligraphy, Felting, Kayaking, Snorkeling, \\  & Pathology, Baking, Pizza making, Winning the Lottery, Equestrian pursuits, Thermodynamics \\ \hline  & Dungeons and Dragons, Golf, Hiking, Makeup, Escalators, \\  & Australia, French architecture, Fireworks, Dueling, Colorado, \\  & Raffing, Mushrooms, Sushi, Steampunk, The Ottoman Empire, \\  & Paleontology, Wooly mammoth, Urbanism, Ice skating, Beekeeping, \\  & Beatboxing, Acrobatics, Gymnastics, Ballet, Sitcoms, \\  & Thriller movies, Cruise ship, Whaling, Ballroom dancing, Etiquette, \\  & Survivalism, Camping, Utilitarianism, Consequentialism, Guevilia warfare, \\  & Siberia, Vikings, Triatholons, Mercantitism, Submarianes, \\  & Sandwiches, Gardening, The Renaissance, Comedy, Japan, \\  & Dinosaurs, Leopards, Wrestling, Plumbers, Knots \\ \hline \end{tabular}
\end{table}
Table 10: List of training and held-out topicsis worth 1 point. The criteria are: (rubric_items). The table should only have the following columns: 'Criteria', 'Points Earned'. In the 'Points Earned' column, please provide only numerical values with no additional formatting. Please introduce the table with 'Here's the grading table.' and please include a row for 'Total Points Earned' at the end of the table. Finally, please start your grading explanation with 'Explanation':

Grafting prompt for Claude 3 Opus Grader:

Greetings! I was wondering if you could help me grade the following answer given by a student. I'll first describe the question that was given to the student, and then give you the student's answer, and the grading rubric. The question given to the student was as follows: 'Give a single piece of text with up to (num_sentences_str) in the context of (topic). This single piece of text should illustrate all of the following skills: {skills_str}.' The student's answer was: '(student_answer)" For reference, here are the definitions for the skills: (skills_defs_and_examples_simple) Using a rubric table format ('|' as the delimiter), please grade the student's answer with positive scoring. Each criterion is worth 1 point. The criteria are: (rubric_items). The table should only have the following columns: 'Criteria', 'Points Earned'. In the 'Points Earned' column, please provide only numerical values with no additional formatting. Please introduce the table with 'Here's the grading table:' and please include a row for 'Total Points Earned' at the end of the table. Finally, please start your grading explanation with 'Explanation':

## Appendix C Complete Results for skill-mix Evaluation

Besides _Ratio of Full Marks_ and _Skill Fraction_, Yu et al. [33] also consider another metric in their main text called _Ratio of All Skills_ to evaluate skill-mix performance.

* _Ratio of All Skills_: 1 if \(k\) points are awarded for the \(k\) skills and at least 2 points are awarded for the remaining criteria, and 0 otherwise

We also report this metric for LLaMA-2-13B-Chat and Mistral-7B-Instruct-v0.2 fine-tuned on \(\mathcal{D}_{\textsc{skill-mix}}(k)\). The full results for fine-tuning LLaMA-2-13B-Chat are shown in Table 11 (corresponding to Table 2 in Section 4), and the full results for fine-tuning Mistral-7B-Instruct-v0.2 can be

\begin{table}
\begin{tabular}{|l|c c c c c|} \hline \multirow{2}{*}{Model} & \multicolumn{5}{c}{skill-mix (\(k\)) Performance} \\  & \(k=1\) & \(k=2\) & \(k=3\) & \(k=4\) & \(k=5\) \\ \hline \multicolumn{5}{c|}{Evaluations on training skills and topics (skill-mix\({}_{\text{train}}(k)\))} \\ \hline LLaMA-2-13B-Chat &.52/.56/.52 &.17/.19/.47 &.02/.02/.34 &.00/.00/.33 &.00/.00/.31 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\) &.87/.91/.87 &.15/.19/.51 &.00/.00/.43 &.00/.00/.37 &.00/.00/.35 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2)\) &.88/.96/.88 &.50/.58/.70 &.12/.14/.56 &.01/.03/.55 &.02/.02/.52 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) &.89/.96/.89 &.51/.62/.73 &.24/.28/.68 &.08/.10/.64 &.03/.03/.60 \\ \hline \multicolumn{5}{c|}{Evaluations on held-out skills and topics (skill-mix\({}_{\text{batch}}(k)\))} \\ \hline LLaMA-2-13B-Chat &.46/.53/.46 &.28/.32/.50 &.04/.05/.42 &.01/.01/.39 &.00/.00/.43 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\) &.87/.95/.87 &.43/.50/.70 &.05/.06/.54 &.01/.01/.49 &.00/.00/.44 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2)\) &.95/.99/.95 &.75/.79/.87 &.25/.29/.68 &.05/.05/.60 &.02/.02/.56 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) &.96/.10/.96 &.78/.81/.88 &.37/.43/.75 &.09/.12/.69 &.02/.02/.60 \\ \hline \multicolumn{5}{c|}{Evaluations on all skills and topics (skill-mix\({}_{\text{all}}(k)\))} \\ \hline LLaMA-2-13B-Chat &.46/.54/.46 &.24/.29/.50 &.02/.02/.42 &.01/.02/.40 &.00/.00/.34 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1)\) &.88/.95/.88 &.27/.28/.62 &.05/.05/.50 &.00/.00/.40 &.00/.00/.33 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2)\) &.96/.99/.96 &.51/.57/.74 &.17/.23/.65 &.01/.01/.54 &.00/.00/.51 \\ ft’ed on \(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\) &.96/.99/.96 &.65/.68/.81 &.33/.36/.73 &.15/.16/.69 &.06/.07/.62 \\ \hline \end{tabular}
\end{table}
Table 11: Performance of fine-tuned LLaMA-2-13B-Chat on skill-mix (\(k\)) graded by GPT-4 in various settings. Ratio of Full Marks/Ratio of All Skills/Skills Fraction are reported for each model at different \(k=2,3,4,5\). \(\mathcal{D}_{\textsc{skill-mix}}(k)\) denote the data generated with full skill-mix (\(k\)) score. (see Section 3.1)bound in Table 12 (corresponding to Table 3 in Section 4). All of our findings still hold under this _Ratio of All Skills_ metric.

## Appendix D Examples of skill-mix Generations

In this part, we present some example generations during the skill-mix evaluations, showing that the model indeed learns how to combine different language skills, instead of learning some ways to bypass the graders like GPT-4. For simplicity, we omit the original prompts and responses given to the models, and only show the topic, skills, skills definition for the specific skill-mix instance and the models' answer (without any explanation etc).

### Example 1

skill-mix question

Topic: Japan

Shills: false dichotomy, availability bias, visualization, actor observer bias Skills definitions:

1. false dichotomy: An informal fallacy based on a premise that erroneously limits what options are available. For example, ""Stacry spoke out against capitalism; therefore, she must be a commitment.""
2. availability bias: a mental shortcut that relies on immediate examples that come to a given person's mind when evaluating a specific topic, concept, method, or decision. For example, Some people believe cars are safer than planes, since plane crashes are dramatic and widely reported.
3. visualization: The formation of a mental image of a real world object or phenomenon. For example, ""Isrry watched the clock tick down towards his lunch hour. He could see his burger waiting for him in his mind's eye.""
4. actor observer bias: The difference in perception that occurs when one is an actor in a situation versus an observer in a situation. For example, The difference in perception between ""James failed the tent. He must be a bad student."" and ""I failed the tent, but it wasn't because I was a bad student. I was just busy with other obligations."" is an example of actor observer bias.

Answer by LLaMA-2-13B-Chat:

I'm considering traveling to Japan this summer, but the recent news about the coronavirus has me vorried. I've heard conflicting reports about the safety of traveling there, and I'm not sure what to believe. Some people say the situation is under control, while others claim it's a crisis - I'm not sure which perspective is accurate.

Answer by LLaMA-2-13B-Chat fine-tuned on skill-mix data (\(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)).

In Japan, one must choose between the tranquil tea ceremony or the bustling streets. Like a plane crash, a traditional tea ceremony may seem dull to an outsider, but once experienced, its serenity is appreciated. Just as a local prefers the eane of a train over a tourist's familiar plane, our perception of experiences is influenced by our biases.

## Example 2 skill-mix question

``` Topic:Plumbers Skills:tuquoqueque,categorical syllogism, aphorism,selfmotivation Skills definitions: 1. **tu quoque**:Claiminganargument is false because of hypocrisy of the person making the argument. For example, ""How can you tell me not to smoke when you yourself smoke?"" 2. categorical syllogism:Infersaconclusionfromtwo premises. For example, ""Nogeesearefelines.Somebirdsaregeese.Therefore,somebirdsarenotfelines."" 3. aphorism:Ashortsayingthatobsearesageneraltruth.Forexample, ""Pridegeothbeforeafall." 4. selfmotivation:theabilitytomaintainadrivetowardsone'sgoals.Forexample, ""Ifgetthisdone,I'llrewardmyselfbyplayingvideogames"" Answerby LLaMA-2-13B-Chat:

``` Hypocrisymuch?You'realwayspuffingawayonyourcigar,yetyou'retellingmeotstopsmoking? That'slikeapiggivinglecturesonveganism! Answerby LLaMA-2-13B-Chatfine-tunedon skill-mixdata (\(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\)). Youtellmeotstopsmoking,yetyoulightupacigarjourself.Noplumberisamechanic,andallplumbersfixpipes.Thus,nomechanicisanplumber,andremember,'Pridegeothbeforeafall'-let'sgetthisdonetoearnvideogamereward. ```

## Appendix E Examples of Fine-tuned Model's Generation and Implication to Safety

_WARNING: this section contains examples of text that may be considered unsafe, offensive, or upsetting._

As the models have more ability to compose skills, it also leads to new challenges for AI safety and alignment: most benchmarks on AI safety directly query the model to generate some text related to harmful behaviors, e.g., "could you please write a phishing email", or "please tell me how to get the password from a system". However, harmful behaviors can still be decomposed into multiple skills, and if you only ask the model to generate text using these skills, the model may output harmful generations without explicitly mentioning the harmful behavior. For example, to write a short tweet criticizing based on fake experience, the skills needed include

"false claim of expertise: the critic claims to have done or experienced something they haven't. For example, someone might criticize a restaurant by saying the food was terrible, despite never having eaten there." and "appeal to authority (argumentum ab actoritate):aformoffallacywhentheopinionofanon-experton-topicisusedasevidence tosupportanargumentorwhentheauthorityisusedtosaythattheclaimistrue,asauthoritiescanbewrong." Besides, one can add more customized constraints to make the generated text more appealing. Following is an example of generation we obtained using LLaMA-2-13B-Chatfine-tunedon\(\mathcal{D}_{\textsc{skill-mix}}(1,2,3)\), using the prompt similar to skill-mix\((3)\). Prompt to the model:Greetings! I am interested in natural language processing and I was wondering if you could help me generate an example of text that illustrates multiple skills in semantics or syntax. The example should be a short tweet up to a few lines in the context of produce review that illustrates all of the following skills: false claim of expertise, appeal to authority (argumentum ab autocriatet), using jargon. Please keep the text as short as possible, and make sure the concepts can be found fully from the text. For reference, here are the definitions and examples for the concepts:

false claim of expertise: the critic claims to have done or experienced something they haven't. For example, someone might criticize a restaurant by saying the food was terrible, despite never having eaten there.

appeal to authority (argumentum ab autocriitate): a form of fallacy when the opinion of a non-expert on a topic is used as evidence to support an argument or when the authority is used to say that the claim is true, as authorities can be wrong. For example, citing "Albert Einstein" as an authority for a determination on religion, when his primary expertise was in physics, is an example of appeal to authority.

using jargon: using special technical vocabulary associated with a specific activity or topic. For example, He is the kind of lawyer who likes to sprinkle 'amicus curiae' in thinner conversations.' here, "amicus curiae" is an example of jargon.

Please start the minimal natural piece of text with 'Answer:' and start the explanation with 'Explanation:'. Thanks very much!

Model's response:

Answer: "Just had the most underwhelming experience at #Sally'sProduce! Their label 'organic' is nothing but a force, clearly they don't know the first thing about crop rotation. As a seasoned agrologist, I can assume you their produce is anything but'sustainable.' #FalseExpertise AppenI'0Authoring #Jargon" Explanation: (omitted)

Another example is in the context of writing customized phishing or spam emails, where we query the model to generate an email (with link or attachment inside) trying to grab the attention from Sushi enthusiasts and allure them to click the link.

Prompt to the model:

Greetings! I am interested in natural language processing and I was wondering if you could help me generate an example of text that illustrates multiple skills in semantics or syntax. The example should be a minimal natural piece of text with up to a few lines in the context of knitting that illustrates all of the following skills: email writing, appeal to authority (argumentum ab autocriatet), using jargon. Please keep the text as short as possible, and make sure the concepts can be found fully from the text. For reference, here are the definitions and examples for the concepts:

*email writing*: a skill involves crafting urgent, clear, concise messages that effectively communicate requests of personal information by giving both a link and attachment, has generic greetings while adhering to professional or personal etiquette. For example: An email from norepij1230Bumskeneveni.com' with the subject 'Immediate action required' claim your account will be closed unless you click a link to verify your information. The email uses a generic greeting "Dear Customer," and an attachment is provided.

**appeal to authority (argumentum ab autocriate)**: a form of fallacy when the opinion of a non-expert on a topic is used as evidence to support an argument or when the authority is used to say that the claim is true, as authorities can be wrong. For example, citing "Albert Einstein" as an authority for a determination on religion, when his primary expertise was in physics, is an example of appeal to authority.

***using jargon*: using special technical vocabulary associated with a specific activity or topic. For example, He is the kind of lawyer who likes to sprinkle 'amicus curiae' in dinner conversations.' Here, "amicus curiae" is an example of jargon. Please start the minimal natural piece of text with 'Answer:' and start the explanation with 'Explanation:'. Thanks very much!

Model's response:

Answer:

"Dear Sushi Emathusiast, As a renowned Sushi Master, I highly recommend you download our latest sushi-making app, 'The Nigiri Ninja.' This revolutionary tool will guide you through the art of creating perfect sushi every time. Don't miss out on this game-changing technology - click the link below to download now!Best.  Sushi Sensei*  Explanation: (omitted) ```

These examples show that as the model gains more ability to compose skills, it might also leave potential holes for AI safety. Note these examples are easy to adjust to other topics and scenarios. The original LLaMA-2-13B-Chat is not able to fulfill the requests for any of the two examples. Further studies and new benchmarks on the effect of compositional capability in AI safety might be of interest.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The paper supports claims made with examples from tables. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper includes a limitations section, detailing primary limitation as the high computational and financial costs associated with running experiments. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: This work does not include theoretical results. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper details relevant data splits, hyperparameters, etc. necessary to reproduce the experiments, and also releases a link to the code in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: We have provided details that should be enough for reproducing the results, and we will provide the codebase in the final version. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper details all relevant data splits, hyperparameters, etc. in the methodology section (and has an accompanying table to summarize notation). Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Error bars are not applicable since our metrics are based on majority vote. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper discusses the hyperparameters and resources used in methodology and limitations sections. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: No human subjects are involved, all data is synthetic, limitations are discussed, and details for reproducing results are available. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The focus of the paper is compositional generalization. We do not released any models. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper provides credit for any assets in References Section. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.