# Topology Preserving Regularization for Independent Training of Inter-operable Models

 Nicolas Zilberstein

Rice University

nzilberstein@rice.edu

&Akshay Malhotra

InterDigital Communications Inc.

akshay.malhotra@interdigital.com

Shahab Hamidi-Rad

InterDigital Communications Inc.

Shahab.Hamidi-Rad@interDigital.com

&Yugeswar Denoo

InterDigital Communications Inc.

yugeswar.deenoo@interDigital.com

Work done during an internship at InterDigital

###### Abstract

Developing schemes to enable zero-shot stitching between different neural networks with minimal or no information exchange has become increasingly important in the era of large and powerful pre-trained models. Considering the example of an autoencoder based data compression framework, having the ability to select the architecture and train an encoder model completely independently of the decoder model while ensuring interoperability between them can revolutionize how these models are developed, deployed, and maintained. In this work, we propose a novel approach that utilizes topological regularizations to align the latent spaces of two different autoencoder models that can be trained independently, without coordination. Our solution introduces two distinct training schemes: _Data2Latent_ and _Latent2Latent_. The _Data2Latent_ scheme focuses on preserving the topological structure of the input data in the latent space, while the _Latent2Latent_ scheme preserves the latent space of a pre-trained, unconstrained model. Through numerical experiments in reconstruction tasks, we demonstrate that our approach yields a near-optimal solution, closely approximating the performance of an end-to-end model.

## 1 Introduction

Compressing data into meaningful low-dimensional representations is a long-standing challenge with applications in image compression [2; 20], audio compression [9], wireless communication [11; 24], and more. Neural networks address this by training an encoder-decoder pair, where the intermediate representation serves as compressed data. The decoder then reconstructs the original data, and the pair is trained end-to-end.

Recent works have studied the _zero-shot stitching_ problem [8] in the context of autoencoders, which involves interconnecting encoders and decoders trained independently using limited datasets. In [22], the authors proposed a method that maps the latent representation of each autoencoder to a relative space. However, this approach requires a specialized decoder trained in the relative representation, which is a significant drawback as it necessitates a large amount of data. More recent methods [14; 19] suggest that a linear transformation might suffice to align latent spaces. However, a significant performance gap remains between end-to-end autoencoders and interconnected models when trained on finite, small datasets, suggesting that linear transformations alone are insufficient.

We hypothesize that this performance gap arises due to a lack of geometric similarity between the latent spaces. The encoder's nonlinear mapping from input data to the latent space is often biased by the network architecture, training parameters and weight initializations, making the latent spaces of independently trained autoencoders incompatible when using simple linear transformations.

To address this, we propose preserving the topological features of the input data within the latent space. This regularization encourages architecture-agnostic latent spaces, making them easier to align. Our approach quantitatively measures the dataset's underlying topology and employs it as a regularizer in the training of interoperable autoencoder models. In particular, we build upon the topological regularizer proposed in [21]. Specifically, this regularization aligns the minimum spanning trees of the source (input data) and the target (latent space), leveraging persistent homology to preserve topological features2. We propose two training frameworks termed _Data2Latent_ (Fig. 1) and _Latent2Latent_ (Fig. 2).

Footnote 2: The referenced paper aligns persistence diagrams obtained via persistent homology, focusing on 0-dimensional topological features, effectively aligning the minimum spanning trees

**Contributions.** The contributions of this paper are twofold:

1) We propose two training frameworks termed _Data2Latent_ and _Latent2Latent_. Each one exploits the topological regularization differently: while the former aims to preserve the structure of the input data, the second one aims to preserve the topological structure of the latent space of unconstrainedly trained autoencoder.

2) Through numerical experiments we show that incorporating topological information facilitates the zero-shot stitching operation with a simple linear transformation.

## 2 Alignment with topological constraints

Direct alignment between latent spaces.Given two autoencoders \(\mathrm{AE}_{1}\) and \(\mathrm{AE}_{2}\) with latent spaces \(\mathbf{Z}_{1}\) and \(\mathbf{Z}_{2}\) respectively, we seek a method to align their latent space _with minimal interaction_ between the models; the ultimate goal is to interconnect the encoder \(\mathcal{E}(.)\) of \(\mathrm{AE}_{1}\) (\(\mathrm{AE}_{2}\)) with the decoder \(\mathcal{D}(.)\) of \(\mathrm{AE}_{2}\) (\(\mathrm{AE}_{1}\)). Based on the assumption that latent spaces of models trained independently tend to be similar, the authors in [14; 19] propose to estimate a transformation \(\mathbf{T}(.)\) by minimizing the mean square error as follow 3

Footnote 3: Although here we focus on the euclidean distance, a different function \(\mathcal{L}(.)\) can be used.

\[\mathbf{T}(.)=\underset{\mathbf{T}(.)\in\mathcal{T}}{\mathrm{argmin}}\| \mathbf{Z}_{1}-\mathbf{T}(\mathbf{Z}_{2})\|^{2}\] (1)

Although these works claimed that a linear transformation is enough to align the latent spaces of independently trained autoencoders \(\mathbf{Z}_{1}\) and \(\mathbf{Z}_{2}\), the gap in terms of reconstruction error between

Figure 1: **Data2latent scheme**: a,b) _Topological autoencoders._ In this case, we regularize the training of each autoencoder \(\mathrm{AE}_{1},\mathrm{AE}_{2}\) with the input data, preserving its topological structure in the latent space independently of the architecture of the encoder. As an example, we consider a 2D synthetic dataset with 5 classes, its corresponding latent space when considering a bottleneck of dimension two and the topological loss, and the output data which has the same shape as the input. Notice that each autoencoder maps the input data to a different latent space, but both latent spaces preserves the topology up to a rotation and a stretching. c) _Stitching between two regularized autoencoders._ The transformation is simplified, leading to a linear transformation (rotation).

the end-to-end autoencoder and the interconnection is still significant. This difference highlights that the linear assumption for interconnection is insufficient. While a non-linear transformation could potentially improve the alignment [17], it requires a large number of samples from \(\mathbf{Z}_{1}\) and \(\mathbf{Z}_{2}\), violating our goal of minimizing interaction between the models. To cope with this challenge, we propose to incorporate a geometric regularization to _linearize_ the relationship.

Topological regularizations aid similar latent spaces.We hypothesize that the relationship between the latent spaces can be linear, but achieving this requires an additional constraint. Specifically, we facilitate alignment by incorporating a geometric regularizer that enforces similar topology between both latent spaces. In particular, we aim to make both latent spaces share the same topological features. A natural way to achieve this is by preserving the _shape_ of the data in each latent space, which can be done using the topological loss introduced in [21]. In essence, this loss function aligns the minimum spanning tree between a source space and the learnable space by leveraging algebraic topology tools, particularly persistent homology; see Appendices B and C for more details on persistent homology and the topological loss. We incorporate this topological regularization and propose two training schemes that constrain the latent space:

1) _Data2latent_: In this first strategy, we regularize the latent space of both autoencoders so that each latent space retains the same connectivity and topological information as the input space. Formally, we define the loss function as

\[\mathcal{L}_{D2L}=\|\mathbf{x}-\mathcal{D}(\mathcal{E}(\mathbf{x}))\|^{2}+ \lambda\mathcal{L}_{topo}(\mathbf{x},\mathcal{E}(\mathbf{x}))\] (2)

where \(\mathcal{L}_{topo}(.)\) is defined in (4) and \(\lambda\) is a regularization constant. By doing this, and assuming both autoencoders are trained on the same dataset, we constrain the latent spaces to share the same topological structure as the input space, thereby inducing similarity between them. As a consequence, the transformation between \(\mathbf{Z}_{1}\) and \(\mathbf{Z}_{2}\) becomes simpler.

2) _Latent2latent_: In this second strategy, we assume an autoencoder \(\mathrm{AE}_{1}\) has been trained without any constraints. Then, given a subset of the training data, \(\mathcal{D}_{\mathrm{topo}}\) and the corresponding encoded vectors \(\{\mathcal{E}_{1}(\mathbf{x})|\mathbf{x}\in\mathcal{D}_{\mathrm{topo}}\}\), we train \(\mathrm{AE}_{2}\) to minimize the following loss function

\[\mathcal{L}_{L2L}=\|\mathbf{x}-\mathcal{D}_{2}(\mathcal{E}_{2}(\mathbf{x}))\| ^{2}+\lambda\mathcal{L}_{topo}(\mathcal{E}_{1}(\mathbf{x}),\mathcal{E}_{2}( \mathbf{x}))\] (3)

where the topological loss is evaluated only on \(\mathcal{D}_{\mathrm{topo}}\). Intuitively, during training, we are constraining \(\mathbf{Z}_{2}\) to share the topological structure with \(\mathbf{Z}_{1}\), harmonizing their topological features and simplifying the transformation between them.

Once both models are trained, we estimate the transformation \(\mathbf{T}(.)\) by minimizing (1) using a small subset of data points from the dataset denoted as \(\mathcal{D}_{T}\). We consider a linear transformation \(\mathbf{T}(\mathbf{Z}_{i})=\mathbf{T}\mathbf{Z}_{i}\).

## 3 Results

We present the results of the proposed scheme on two datasets, MNIST [5] and Fashion MNIST [25]. We utilize two completely different model architectures to show the robustness of the proposed

Figure 2: **Latent2latent scheme. In this case, we regularize the training of second autoencoder \(\mathrm{AE}_{2}\) with the latent space of _pre-trained_, _unconstrained_ autoencoder \(\mathrm{AE}_{1}\). We compute the topological loss using a pre-defined set \(\mathcal{D}_{topo}\).**

scheme for the model architectures of the two autoencoders. For the first autoencoder (AE1), a feed-forward architecture is utilized, and for the second autoencoder (AE2), a convolutional neural network architecture is utilized. For MNIST, we consider a bottleneck of 128, while for FashionMNIST 250. Details on the model architecture, hyperparameters, and training can be found in Appendix A.1.

### Comparison with other methods

As baselines, we consider the direct alignment with a linear transformation between two unconstrained autoencoders [14] and the relative representation, where a specialized decoder is trained to handle the relative representation [22]. For the direct alignment and our methods, we use 500 samples to estimate the linear transformation, while for the relative representation, we use 1000 anchor points. The normalized mean-square error (NMSE)4 is shown in Table 1, while qualitatively results are shown in Appendix A.2 in Figs. 4a and 4b. For the quantitative results, we average over five trials with different seeds. The topological constraint harmonizes the latent space, making them easy to stitch: the topological autoencoder achieves an NMSE almost as good as its upper bound (\(L_{11}\)) for FusionMNIST.

Footnote 4: We normalize w.r.t. the clean image.

### Ablation

Finally, we do an ablation study of our proposed methods compared to the linear transformation between unconstrained autoencoders [14]. We compare the performance when increasing the size of samples for the linear transformation. For the latent2latent training, we consider \(|\mathcal{D}_{topo}|=20\). The results for \(|\mathcal{D}_{T}|=\{10,100,200,500\}\) and MNIST is shown in Fig. 3a, while for the FashionMNIST is in 3b. We observe that for \(|D_{T}|\geq 200\), the performance of the interconnection between autoencoders gets closer to the upper bound, while the gap between the non-regularized case remains large. Notice that increasing the size of the bottleneck (recall for for FashionMNIST we consider a bottleneck of 250) improves the performance of our methods, achieving a near-optimal performance.

## 4 Conclusion

We study the problem of zero-shot stitching in autoencoders, showing that topological regularization aids a simplified relationship (linear) between two latent spaces. Our proposed solution encompasses two different training schemes: _Data2Latent_ and _Latent2Latent_, which aim to preserve the topological structure of the input data and the latent space of an unconstrained autoencoder, respectively. Through numerical experiments on MNIST and FashionMNIST, we showed that our method simplifies dramatically the relationship between two latent spaces. Future work includes studying alternatives geometric losses to align topological features, formalizing the trade-off between performance and samples for the topological loss [12], which is particularly relevant for for the _Latent2Latent_ scheme, and expanding the experiments on more complex datasets, such as ImageNet, and of different modalities, such as language and wireless data.

\begin{table}
\begin{tabular}{c c c} \hline \hline Method & MNIST & FashionMNIST \\ \hline \(L_{11}\) (MLP-MLP) & \(0.020\) & \(0.018\) \\ \(L_{22}\) (CNN-CNN) & \(0.014\) & \(0.010\) \\ \(L_{21}\) (CNN-MLP) & \(0.923\pm 0.002\) & \(0.644\pm 0.061\) \\ Direct alignment (CNN-MLP) [14] & \(0.611\pm 0.081\) & \(0.193\pm 0.03\) \\ Rel. representation [22] & \(0.225\pm 0.003\) & \(0.100\pm 0.003\) \\ \hline Data2Latent (CNN-MLP) (ours) & \(0.055\) & \(0.023\pm 0.003\) \\ Latent2Latent (CNN-MLP) (ours) & \(0.065\pm 0.001\) & \(0.028\pm 0.002\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: NMSE for related and our proposed methods on MNIST and FashionMNIST. \(L_{11}\) and \(L_{22}\) denote the NMSE of decoding using \(\mathrm{AE}_{1}\) (MLP) and \(\mathrm{AE}_{2}\) (CNN) respectively, while \(L_{21}\) is the NMSE of decoding using directly (\(\mathbf{T}=\mathbf{I}\)) the encoder from \(\mathrm{AE}_{2}\) with the decoder from \(\mathrm{AE}_{1}\). We do not include the standard deviation when it is in the fourth decimal place. We consider the average accross 4 trials with different seeds.

## References

* [1] Georgios Arvanitidis, Lars Kai Hansen, and Soren Hauberg. Latent space oddity: On the curvature of deep generative models. In _Intl. Conf. Learn. Repr. (ICLR)_, 2018.
* [2] Johannes Balle, Valero Laparra, and Eero P Simoncelli. End-to-end optimized image compression. In _Intl. Conf. Learn. Repr. (ICLR)_, 2017.
* [3] Irene Cannistraci, Marco Fumero, Luca Moschella, Valentino Maiorca, and Emanuele Rodola. Infusing invariances in neural representations. _Workshop Topology, Algebra, and Geometry in Machine Learning TAG-ML at ICML_, 2023.
* [4] Irene Cannistraci, Luca Moschella, Marco Fumero, Valentino Maiorca, and Emanuele Rodola. From bricks to bridges: Product of invariances to enhance latent space communication. In _Intl. Conf. Learn. Repr. (ICLR)_, 2024.
* [5] Li Deng. The MNIST database of handwritten digit images for machine learning research. _IEEE Signal Process. Mag._, 29(6):141-142, 2012.
* [6] Andres F Duque, Sacha Morin, Guy Wolf, and Kevin R Moon. Geometry regularized autoencoders. _IEEE Trans. Pattern Anal. Mach. Intell._, 45(6):7381-7394, 2022.
* [7] Marco Fumero, Marco Pegoraro, Valentino Maiorca, Francesco Locatello, and Emanuele Rodola. Latent functional map. _arXiv preprint arXiv:2406.14183_, 2024.
* [8] Michael Gygli, Jasper Uijlings, and Vittorio Ferrari. Towards reusable network components by learning compatible representations. In _AAAI Conf. Art. Intell._, volume 35, pages 7620-7629, 2021.
* [9] Cristina Garbacea, Aaron van den Oord, Yazhe Li, Felicia S C Lim, Alejandro Luebs, Oriol Vinyals, and Thomas C Walters. Low bit-rate speech coding with VQ-VAE and a wavenet decoder. In _IEEE Intl. Conf. Acoust., Speech and Signal Process. (ICASSP)_, pages 735-739, 2019. doi: 10.1109/ICASSP.2019.8683277.
* [10] Felix Hensel, Michael Moor, and Bastian Rieck. A survey of topological machine learning methods. _Front. Artif. Intell._, 4:681-108, 2021.
* [11] Teng-Hui Huang, Akshay Malhotra, and Shahab Hamidi-Rad. A deep learning method for joint compression and unsupervised denoising of CSI feedback. In _IEEE Intl. Conf. on Commun. (ICC))_, pages 4150-4156, 2023.
* [12] Saachi Jain, Adityanarayanan Radhakrishnan, and Caroline Uhler. A mechanism for producing aligned latent spaces with autoencoders. _arXiv preprint arXiv:2106.15456_, 2021.
* [13] Diederik P Kingma. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.

Figure 3: Performance analysis of our proposed methods as a function of the number of samples (\(|\mathcal{D}_{T}|\)) for estimating \(\mathbf{T}(.)\) in (1). a) NMSE vs \(|\mathcal{D}_{T}|\) for MNIST. b) NMSE vs \(|\mathcal{D}_{T}|\) for FashionMNIST.

* [14] Zorah Lahner and Michael Moeller. On the direct alignment of latent spaces. In _Workshop UniReps: the First Workshop on Unifying Representations in Neural Models at NeurIPS_, pages 158-169. PMLR, 2024.
* [15] Yonghyeon Lee. A geometric perspective on autoencoders. _arXiv preprint arXiv:2309.08247_, 2023.
* [16] Yonghyeon Lee, Hyeokjun Kwon, and Frank Park. Neighborhood reconstructing autoencoders. _Advances in Neural Inf. Process. Syst. (NIPS)_, 34:536-546, 2021.
* [17] Karel Lenc and Andrea Vedaldi. Understanding image representations by measuring their equivariance and equivalence. In _Proceedings of the IEEE/CVF Int. Conf. Comput. Vis. Pattern Recogn. (CVPR)_, pages 991-999, 2015.
* [18] Jungbin Lim, Jihwan Kim, Yonghyeon Lee, Cheongjae Jang, and Frank C Park. Graph geometry-preserving autoencoders. In _Intl. Conf. on Machine Learning (ICML)_, 2024.
* [19] Valentino Maiorca, Luca Moschella, Antonio Norelli, Marco Fumero, Francesco Locatello, and Emanuele Rodola. Latent space translation via semantic alignment. _Advances in Neural Inf. Process. Syst. (NIPS)_, 36, 2024.
* [20] Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Conditional probability models for deep image compression. In _Proceedings of the IEEE/CVF Int. Conf. Comput. Vis. Pattern Recogn. (CVPR)_, pages 4394-4402, 2018.
* [21] Michael Moor, Max Horn, Bastian Rieck, and Karsten Borgwardt. Topological autoencoders. In _Intl. Conf. on Machine Learning (ICML)_, pages 7045-7054. PMLR, 2020.
* [22] Luca Moschella, Valentino Maiorca, Marco Fumero, Antonio Norelli, Francesco Locatello, and Emanuele Rodola. Relative representations enable zero-shot latent space communication. In _Intl. Conf. Learn. Repr. (ICLR)_, 2023.
* [23] Leopold Vietoris. Uber den hoheren zusammenhang kompakter raume und eine klasse von zusammenhangstreuen abbildungen. _Mathematische Annalen_, 97(1):454-472, 1927.
* [24] Chao-Kai Wen, Wan-Ting Shih, and Shi Jin. Deep learning for massive MIMO CSI feedback. _IEEE Wireless Commun. Lett._, 7(5):748-751, 2018.
* [25] H Xiao. Fashion-MNIST: a novel image dataset for benchmarking machine learning algorithms. _arXiv preprint arXiv:1708.07747_, 2017.
* [26] Lee Yonghyeon, Sangwoong Yoon, Minjun Son, and Frank C Park. Regularized autoencoders for isometric representation learning. In _Intl. Conf. Learn. Repr. (ICLR)_, 2022.

## Appendix A Additional details

### Details of the architectures and training

We now expand on the details of the architectures and training process. The encoder and decoder of \(\mathrm{AE}_{1}\) are feed-forward architectures with three layers with ReLU activation function followed by batch normalization and a bottleneck of 128 for MNIST and 250 for FashionMNIST. The hidden dimension at the output of each layer is 1000 for the first layer, 500 for the second one, and 250 for the third. The non-linear activation function at the last layer of the decoder is \(\mathrm{Tanh}(.)\). The encoder and decoder of \(\mathrm{AE}_{2}\) are convolutional architectures with five convolutional layers. The first layer is a kernel with size of 3 and a stride of 2, with padding to preserve spatial dimensions. The number of channels at the output is 32. The second layer keeps the number of channels constant and has the same kernel size and stride of 1. The third layer has a kernel with size of 3 and a stride of 2, with padding, doubling the number of channels to 64. The next layer preserves the spatial dimensions and retains the same channel depth. The final convolutional layer applies a kernel with size three and a stride of 2 again, maintaining the number of channels. After the final convolutional operation, the output is flattened into a one-dimensional vector, which is then passed through a linear transformation which reduces the feature representation to the same MLP bottleneck (128 for MNIST and 250 for FashionMNIST). Each convolutional layer is followed by a LeakyReLU activation function.

We train for 200 epochs with a batch size of 256. We use ADAM optimizer [13] with a learning rate \(l_{r}=1\times 10^{-4}\). For the training scheme _Latent2Latent_ we consider \(|\mathcal{D}_{top}|=20\), i.e., \(20\) samplesfor the topological loss. Lastly, for both schemes we fixed \(\lambda=0.5\) for MNIST and \(\lambda=0.02\) for FashionMNIST. We consider 4 trials per method with different seeds (\(0,100,1000,10000\)) and report the average.

### Additional results

In Figs. 3(a) and 3(b) we show qualitative results for MNIST and FashionMNIST for the experiments described in Section 3.1

Running time.We compared the training times of the AEs with and without the topological loss. On an NVIDIA DGX A100, the AE without topological loss trains in approximately 10 minutes, while incorporating the topological loss increases the time to around 15 minutes. It's important to note that, in this case, we are only using 0-dimensional persistence features. Future work will include an ablation study on larger datasets to further analyze the impact.

## Appendix B Persistence homology

Our method leverages algebraic topology tools [10]. In particular, we build a simplicial complex given a dataset representing a point cloud in a N-dimensional space. A simplicial complex is a set composed of points, lines, triangles, and n-dimensional counterpart objects; for example, a 0-simplicial corresponds to the points of the simplicial. In particular, one-dimensional simplicial complexes are equivalent to an undirected graph. We use persistence homology to find topological features of the dataset, namely connectivity patterns such as connected components; the homology groups describe these topological features.

Formally, we assume that we have a dataset represented as a point cloud, where samples lie in an unknown manifold. Thus, we seek to approximate the unknown manifold. We represent the point cloud as a nested sequence of simplicial complexes; we consider the Vietoris-Rips complex [23]. The sequence is built following a filtration process: for \(0<\epsilon<\infty\), the Vietoris-Rips complex (at each scale \(\epsilon\)) contains all the simplices of the point cloud whose elements satisfy that \(dist(x_{i},x_{j})<\epsilon\quad\forall i,j\)[10]. Notice that this construction satisfies the nested relationship, i.e., for \(\epsilon_{i}<\epsilon_{j}\), the simplicial at scale \(\epsilon_{i}\) is contained in the simplicial at scale \(\epsilon_{j}\); for

The output of the persistence homology calculation of a point cloud are tuples of persistence diagrams \(D_{d}\) and persistence pairings \(P_{d}\), where \(d\) is the dimensionality. For example, the persistence pairing \(P_{0}\) contains the edge indices that are in the minimum spanning tree, i.e., the edges that are "relevant" from a topological perspective. The \(d\)-dimensional persistence diagram contains coordinates \((a,b)\), where \(a\) indicates the scale \(\epsilon\) at which a \(d\)-dimensional topological feature is created, while \(b\) indicates

Figure 4: Qualitative results for all the methods. From left to right: input, \(L_{11}\) (MLP), \(L_{22}\) (CNN), \(L_{21}\) (CNN-MLP), Direct Alignment (CNN-MLP) [14], Rel. Representation [22], Data2Latent (D2L), Latent2Latent (L2). For linear transformation and our two methods, we used \(|\mathcal{D}_{T}|=500\) samples to estimate the transformation; for Rel. Representation, we used 1000 anchor points.

the scale \(\epsilon^{\prime}\) at which it is destroyed. On the other hand, the persistence pairing contains indices \((i,j)\) that correspond to simplicies \(s_{i},s_{j}\) that create and destroy the topological feature identified by \((a,b)\).

## Appendix C Topological autoencoders

Building a tractable loss function that compares the topological features between the target (input) and learnable spaces is challenging. We consider the loss function defined in [21], which compares the persistence diagrams. We now briefly describe its construction and main ingredients. First, we compute the pair-wise distances between samples in the point cloud (points in the dataset). Let \(\mathbf{P}\) be the set of all pairs of data points in the point cloud, the set of all corresponding pairwise distances is denoted as \(\mathbf{A}\). We fix the maximum scale \(\epsilon=\max\mathbf{A}\) and the maximum dimension \(d=0\). Given this scale, we construct the Vietoris-Rips complex as described above and given the dimension \(d\), we keep all the persistence diagrams and pairings up to the dimension \(d\). Since, \(d=0\), the persistence diagram corresponds to the minimum spanning tree: recall that the persistence diagram for \(0\)-dimensional features indicates when two connected components are merged. We do this for both, the target space (i.e. input data space in the Data2Latent setting and latent space of the unconstrained trained autoencoder in the Latent2Latent scheme) and learnable spaces.

Denoting \(\mathbf{P_{X}}\) as the pairing indices of the connected points in the target space (edges of the persistence diagram/ minimum spanning tree in the target space), \(\mathbf{P_{Z}}\) as the pairing indices in the learnable space (edges of the persistence diagram/ minimum spanning tree in the learnable space), \(\mathbf{A_{X}}\) and \(\mathbf{A_{Z}}\) as the set of all possible pairwise distances in the target space and learnable space, respectively. We obtain the distance corresponding to the edges of the minimum spanning trees in the target and learnable spaces as, \(\mathbf{D_{XX}}=\mathbf{A_{X}}[\mathbf{P_{X}}]\) and \(\mathbf{D_{ZZ}}=\mathbf{A_{Z}}[\mathbf{P_{Z}}]\), respectively. Finally, we consider the union of all selected edges across the target and learnable spaces, i.e., \(\mathbf{D_{ZX}}=\mathbf{A_{Z}}[\mathbf{P_{X}}]\) and \(\mathbf{D_{XZ}}=\mathbf{A_{X}}[\mathbf{P_{Z}}]\). Finally, the topological loss is given by

\[\mathcal{L}_{topo}=\frac{1}{2}\|\mathbf{D_{XX}}-\mathbf{D_{ZX}}\|^{2}+\frac{ 1}{2}\|\mathbf{D_{XZ}}-\mathbf{D_{ZZ}}\|^{2}.\] (4)

Although we consider the Euclidean norm here, we can also consider other types of distance to compare the persistence diagram. In a nutshell, this loss function combines two terms: the first one seeks to minimize the dissimilarity between the adjacency matrix in the input and latent space when considering the pairing obtained from data graph. Similarly, the second term minimize the difference but when considering the pairing obtained from the latent graph. Considering the union instead of the intersection is essential to have informative gradients; otherwise, if we consider the difference between \(\mathbf{D_{XX}}\) and \(\mathbf{D_{ZZ}}\), then in the first step, the number of distances selected by the persistence pairing in the learnable spaces would be small due to its random initialization.

## Appendix D Related works

This section expands on the baseline methods and other related works.

### Relative representation

Relative representation [22] is one of the most popular papers in zero-shot stitching. The work builds on the observation that the representations learned by different neural networks trained on the same data are related via conformal maps; in other words, the angles between latent embeddings are preserved. Based on this observation, they propose to map the latent space of each autoencoder to a pre-defined relative representation, which is invariant by construction to the transformations induced by stochastic factors in the training process. More precisely, they select at random a set of anchor elements \(\mathcal{A}\in\mathcal{D}_{train}\); every sample in the anchor set is represented concerning the embedded anchors \(\bm{e_{a^{(j)}}}=\mathcal{E}\left(\bm{a}^{(j)}\right)\). Then, the relative representation is defined as

\[\bm{r}_{\bm{x}^{(i)}}=\left(\operatorname{sim}\left(\bm{e_{\bm{x}^{(i)}}}, \bm{e_{a^{(1)}}}\right),\operatorname{sim}\left(\bm{e_{\bm{x}^{(i)}}},\bm{e_{ a^{(2)}}}\right),\ldots,\operatorname{sim}\left(\bm{e_{\bm{x}^{(i)}}},\bm{e_{a^{( |\mathcal{A}|)}}}\right)\right)\] (5)

where \(\operatorname{sim}:\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\mathbb{R}\) is a similarity function, yielding a scalar score \(r\) between two absolute representations \(r=\operatorname{sim}\left(\bm{e_{\bm{x}^{(i)}}},\bm{e_{\bm{x}^{(j)}}}\right)\). In particular, they consider the cosine similarity. Notice that the number of anchor points gives the dimensionality of this new latent space; therefore, it is expectedto improve the performance when increasing the number of anchor points. This proposed method requires a specialized decoder trained on the relative representation.

While in the original paper, the authors considered only the cosine similarity, other similarity functions can be incorporated [3, 4]. Lastly, a recent work [7] proposes a method to do zero-shot stitching that leverages functional maps, i.e., by learning a linear transformation in the spectral domain.

### Autoencoders via geometric regularization

This work focused on topological autoencoders; however, several previous works incorporate a geometric regularization [15]. Many of them build on the notion of _curvature_ of a generator [1]. In [16], the authors propose first to build a neighborhood graph on the input data and use it to regularize the geometry and connectivity of the learned manifold. Remarkably, they propose a local approximation of the decoder instead of the encoder to extract local geometric information on the decoded manifold. In [26], a regularization based on conformal mapping, i.e., a mapping that preserves angles, is used to enforce the preservation of angles and relative distances between data and latent space. In [6], inspired by manifold learning, a regularization based on spectral embeddings is incorporated, encouraging the learned latent representation to align with the data geometry. Recently, a graph-preserving autoencoder was proposed in [18], which aims to preserve a pre-build Laplacian between the input data and the latent space.