# Memorize What Matters:

Emergent Scene Decomposition from Multitraverse

Yiming Li\({}^{1,2}\) Zehong Wang\({}^{1}\) Yue Wang\({}^{2,3}\) Zhiding Yu\({}^{2}\)

Zan Gojcic\({}^{2}\) Marco Pavone\({}^{2,4}\) Chen Feng\({}^{1}\) Jose M. Alvarez\({}^{2}\)

\({}^{1}\)NYU \({}^{2}\)NVIDIA \({}^{3}\)USC \({}^{4}\)Stanford University

Project Page: https://nvlabs.github.io/3DGM/

###### Abstract

Humans naturally retain memories of permanent elements, while ephemeral moments often slip through the cracks of memory. This selective retention is crucial for robotic perception, localization, and mapping. To endow robots with this capability, we introduce 3D Gaussian Mapping (3DGM), a _self-supervised_, _camera-only_ offline mapping framework grounded in 3D Gaussian Splatting. 3DGM converts multitraverse RGB videos from the same region into a Gaussian-based environmental map while concurrently performing 2D ephemeral object segmentation. Our key observation is that the environment remains consistent across traversals, while objects frequently change. This allows us to exploit self-supervision from repeated traversals to achieve environment-object decomposition. More specifically, 3DGM formulates multitraverse environmental mapping as a robust 3D representation learning problem, treating pixels of the environment and objects as inliers and outliers, respectively. Using robust feature distillation, feature residual mining, and robust optimization, 3DGM simultaneously performs 2D segmentation and 3D mapping without human intervention. We build the Mapverse benchmark, sourced from the Ithaca365 and nuPlan datasets, to evaluate our method in unsupervised 2D segmentation, 3D reconstruction, and neural rendering. Extensive results verify the effectiveness and potential of our method for self-driving and robotics.

## 1 Introduction

Vision-based 3D mapping is essential for autonomous driving but faces two key challenges: _(1)_ dynamic objects disrupting multi-view consistency and _(2)_ accurately reconstructing 3D structures from 2D images. Existing methods rely on pretrained segmentation models to filter out dynamic objects and LiDARs to enhance geometry. However, these approaches are limited by the need for human annotations during pretraining, along with the high costs and limited portability of LiDARs.

Motivated by the aforementioned challenges, we aim to develop a _self-supervised_ and _camera-only_ 3D mapping approach, reducing the reliance on human annotations and LiDARs. We consider a practical _multitraverse_ driving scenario, where autonomous vehicles repeatedly traverse the same routes or regions at different times. During each traversal, the ego-vehicle encounters new pedestrians and vehicles, much like how humans navigate the same 3D environment but encounter different groups of passersby each day. Inspired by humans' ability to memorize the **permanent** and ignore the **ephemeral1** during repeated spatial navigation, we pose the following question:

Footnote 1: We will use _ephemeral_ and _transient_ interchangeably to refer to objects that temporarily appear or disappear across various traversals of the same location, such as pedestrians, vehicles, or other temporary elements.

_Can we develop an autonomous mapping system that identifies and memorizes only the consistent environmental structures in a 3D world across multiple traversals, without relying on human supervision?_

We provide an affirmative answer to this question. Our key insight lies in using the consensus across repeated traversals as a self-supervision signal, ensuring that the learned map retains only consensus structures (_permanent environment_) while discarding dissensus elements (_transient objects_). We ground this insight in 3D Gaussian Splatting (3DGS) [1], which models a 3D scene using a group of 3D Gaussians with learnable attributes such as position, color, and opacity. This scene representation offers both geometric and photometric information, benefiting various downstream applications in autonomous driving. We leverage abundant images from multiple traversals to facilitate Gaussian initialization using Structure from Motion (SfM) [2], without relying on LiDARs. Subsequently, we learn the environmental Gaussians from multitraverse RGB videos by minimizing the rendering loss.

To optimize a time-invariant 3D representation from input images containing time-varying structures, we frame _multitraverse environmental mapping_ as a _robust representation learning_ problem, where pixels from transient objects are treated as outliers. Specifically, we distill self-supervised robust features--denoised DINOV2 [3, 4]--into Gaussians to facilitate outlier identification. We then employ a novel feature residual mining strategy to fully exploit the spatial information within the rendering loss map. This strategy aids in precise outlier grouping, improving transient object segmentation. Finally, we apply a robust loss function to optimize the 3D environmental Gaussians. As a result, we accurately learn the Gaussian-based environment map from inlier pixels and even generate 2D masks of transient objects for free, as illustrated in Fig. 1.

We build the _Mapping and segmentation through multitraverse_ (**Mapverse**) benchmark, sourced from the Ithaca365 [5] and nuPlan [6] datasets to evaluate our method in three tasks: unsupervised 2D segmentation, 3D reconstruction, and neural rendering. Quantitative and qualitative results demonstrate the effectiveness of our method in autonomous driving scenarios.

To summarize, our key innovations are listed as follows.

* **Problem formulation** We address the multitraverse RGB mapping problem through robust representation learning, treating pixels of the environment as inliers and objects as outliers.
* **Technical design** We introduce feature residual mining to leverage spatial information from rendering loss maps, enabling more accurate outlier segmentation in self-driving scenes.
* **System integration** We build 3D Gaussian Mapping (3DGM) that jointly generates 3D environmental Gaussians and 2D ephemerality masks without LiDARs and human annotations.
* **Dataset curation** We build a large-scale multitraverse driving benchmark from real-world driving data, featuring **40** locations, each with no less than **10** traversals, totaling **467** driving video clips and **35,304** images. Code and data are released at https://github.com/NVlabs/3DGM.

## 2 Related Works

Multitraverse drivingA vehicle generally operates within the same geographical area, resulting in multiple traversals of the same location. This repetition enriches the vehicle's memory of specific

Figure 1: **A high-level diagram of 3D Gaussian Mapping (3DGM). Given multitraverse RGB videos, 3DGM outputs a Gaussian-based environment map (EnvGS) and 2D ephemerality segmentation (EmerSeg). Note that the proposed framework is LiDAR-free and self-supervised.**

places, enhancing its capabilities in perception and localization [7, 8, 9, 10]. Regarding perception, the Hindsight framework [11] utilizes past LiDAR point clouds to learn memory features that are easy to query, thereby addressing the challenges of point sparsity and boosting 3D detection performance. Other studies have employed the persistence prior score [12, 13], which quantifies the consistency of a single LiDAR point across multiple traversals, for self-training of detectors and domain adaptation. In localization, a significant number of works focus on either metric [14, 15] or topological [16, 17] localization, aiming to match a query image with a set of reference images collected from different traversals under varying seasonal or lighting conditions. Closely related to our work is [18], which employs multiple traversals to map out ephemeral regions, enhancing monocular visual odometry in dense traffic conditions. However, this approach also depends on the consistency of LiDAR point clouds across traversals, remarking an unexplored gap in leveraging consensus in the 2D image space.

NeRF and 3DGSNeRF has recently revolutionized novel-view synthesis and scene reconstruction with image or video input, boasting a wide range of applications in graphics, vision, and robotics. NeRF employs a volumetric representation and trains neural networks to model density and color. The success of NeRF has sparked a surge in follow-up methods aiming to enhance quality [19, 20, 21] and increase speed [22, 23, 24]. The recent 3D Gaussian Splitting (3DGS) [1] uses an explicit Gaussian-based representation and splatting-based rasterization [25] to project anisotropic 3D Gaussians onto a 2D screen. It determines the pixel's color by performing depth sorting and \(\alpha\)-blending on the projected 2D Gaussians, thus avoiding the complex sampling strategy of ray marching and achieving real-time rendering. Subsequent works have applied 3DGS to scene editing [26], dynamic scene modeling [27, 28], sparse view reconstruction [29], mesh reconstruction [30], semantic understanding [31, 32], and indoor SLAM [33].

NeRF and 3DGS for self-drivingBeyond their use in object-centric scenarios and bounded indoor environments, NeRF and 3DGS have also been explored in unbounded driving scenes [34, 35]. Several works address the implicit surface reconstruction of static scenes [36, 37, 38]. A large body of research focuses on dynamic scene reconstruction from a single driving log. Most works use a compositional method and rely on bounding annotations/trained detectors to model dynamic objects [39, 40, 41, 42, 43, 44, 45]. EmerNeRF [46] is the first self-supervised method to learn 4D neural representations of driving scenes from LiDAR-camera recordings. It couples static, dynamic, and flow fields [24] and leverages the flow field to aggregate multi-frame information to enhance the feature representation of dynamic objects. Another line of research investigates the scalability of the neural representation to model large-scale scenes [47, 48, 49, 50, 51, 52]. Block-NeRF [47] segments the scene into separately trained NeRF models, processing camera images from multiple drives, and applies a semantic segmentation model [53] to exclude common movable objects. SUDS takes the input of multitraverse driving logs, leveraging RGB images, LiDAR point clouds, DINO [54], and 2D optical flow [55] for dynamic scene decomposition. In this work, we create an environment map represented by 3DGS without requiring LiDARs, leveraging the multitraverse consensus for self-supervised object removal.

Scene decompositionTraditional background subtraction approaches [56, 57] distinguish moving objects from static scenes by comparing successive video frames and identifying significant differences as foreground elements. Representative works include low-rank decomposition, which treats moving objects in the scene as pixel-wise sparse outliers [58, 59]. These methods are typically used in surveillance applications and are limited to static cameras. Follow-up works [60, 61] investigate background subtraction for mobile robotics, yet suffering from low performance. NeRF has recently emerged as a popular scene representation and has been applied to the self-supervised dynamic-static decomposition of indoor scenes by modeling _time-varying_ and _time-independent_ components separately [62, 63]. EmerNeRF [46] extends similar intuition to autonomous driving and obtains scene flow for free while achieving dynamic-static decomposition of a single traversal. Yet it still depends on the LiDAR inputs. In this study, we leverage signals of consensus and dissensus across multiple traversals to accomplish _permanence-ephemerality_ decomposition using only image inputs.

Vision foundation modelsInspired by the success of scaling in NLP [64], the field of computer vision intensively studies large-scale self-supervised pre-training with Transformers [65]. Vision Transformers (ViTs) [66], pre-trained on extensive datasets, achieve excellent image recognition results. DINO [54] further amplifies feature representation capabilities by harnessing self-supervised learning alongside knowledge distillation. Meanwhile, scene layouts emerge within the attention maps, enabling unsupervised semantic understanding. DINOv2 [4] scales up both the data and model size, achieving more robust visual features. Subsequent research focuses on examining noise artifacts to further enhance the performance of self-supervised descriptors, including training-free denoising of ViTs [3] and retraining ViTs with registered tokens [67]. In this work, we leverage denoised DINOv2 features [3, 4] to facilitate consensus verification across multiple traversals in pixel space, _as the high-dimensional features prove more resilient to changes in environmental appearance._

## 3 3DGS: 3D Gaussian Splatting

3D Gaussian Splatting [1] represents the 3D environment with a set of anisotropic 3D Gaussians, denoted by \(\mathbf{G}=\{\mathbf{G}_{i}\ |\ i=1,\ldots,N\}\), where \(N\) is the total number of Gaussians. Each Gaussian, \(\mathbf{G}_{i}\), is parameterized by its mean vector \(\boldsymbol{\mu}_{i}\in\mathbb{R}^{3}\), indicating the position, and a covariance matrix \(\boldsymbol{\Sigma}_{i}\in\mathbb{R}^{3\times 3}\), defining its shape. To guarantee positive semi-definiteness, the covariance matrix \(\boldsymbol{\Sigma}_{i}\) is further decomposed as \(\boldsymbol{\Sigma}_{i}=\mathbf{R}_{i}\mathbf{S}_{i}\mathbf{R}_{i}^{\top}\), with \(\mathbf{R}_{i}\) being an orthogonal rotation matrix and \(\mathbf{S}_{i}\) a diagonal scaling matrix. These are stored compactly as a rotation quaternion \(\mathbf{q}_{i}\in\mathbb{R}^{4}\) and a scaling factor \(\mathbf{s}_{i}\in\mathbb{R}^{3}\). Each Gaussian also incorporates an opacity value \(\alpha_{i}\in\mathbb{R}\) and a spherical harmonics coefficients \(\boldsymbol{\beta}_{i}\). Therefore, the learnable parameters for the \(i\)-th Gaussian are \(\mathbf{G}_{i}=[\boldsymbol{\mu}_{i},\mathbf{q}_{i},\mathbf{s}_{i},\alpha_{i},\boldsymbol{\beta}_{i}]\). Rendering from a viewpoint computes the color at pixel \(\mathbf{p}\) (denoted by \(\mathbf{c}_{\mathbf{p}}\)) via volumetric rendering, integrating \(K\) ordered Gaussians \(\{\mathbf{G}_{k}\ |\ k=1,\ldots,K\}\) overlapping pixel \(\mathbf{p}\), _i.e._, \(\mathbf{c}_{\mathbf{p}}=\sum_{k=1}^{K}\mathbf{c}_{k}\alpha_{k}\prod_{j=1}^{k =1}(1-\alpha_{j})\). Here, \(\alpha_{k}\) is derived by evaluating a 2D Gaussian projection [25] from \(\mathbf{G}_{k}\) onto pixel \(\mathbf{p}\), multiplied by the Gaussian's learned opacity, and \(\mathbf{c}_{k}\) is the color obtained by evaluating the spherical harmonics of \(\mathbf{G}_{k}\). The Gaussians are sorted by their depth from the viewpoint. The overall objective is to minimize the rendering loss:

\[\mathcal{L}=\sum_{t}\mathcal{L}_{rgb}(\mathbf{I}_{t}(\boldsymbol{\xi}_{t}; \mathbf{G}),\mathbf{I}_{t})\] (1)

where \(\mathbf{I}_{t}(\boldsymbol{\xi}_{t};\mathbf{G})\in\mathbb{R}^{w\times h\times 3}\) is the RGB image indexed by \(t\), with spatial dimensions \(w\times h\) and rendered from the pose \(\boldsymbol{\xi}_{t}\in\mathfrak{s}\mathfrak{e}(3)\), given Gaussians \(\mathbf{G}\). \(\mathbf{I}_{t}\in\mathbb{R}^{w\times h\times 3}\) is the paired ground truth image. \(\mathcal{L}_{rgb}\) is a loss function such as L1 loss. Initialized by COLMAP [2], all attributes of \(\mathbf{G}\) are learned by executing this view reconstruction task. Meanwhile, adaptive densification and pruning strategies are proposed to improve the fitting of the 3D scene.

## 4 3DGM: 3D Gaussian Mapping

### Problem Formulation

AssumptionWe make reasonable assumptions about the stability of the environment and the transience of objects within it. Specifically, we assume that there are no major environmental changes, a realistic expectation when data is collected over a certain period under consistent weather and lighting conditions. Meanwhile, we consider all movable objects to be transient; despite their potential static nature during a particular traversal, they are expected to eventually move somewhere else, allowing the camera to capture dissensus over time.

SetupWe conduct offline mapping of a specified spatial area by repeatedly traversing it with vehicles equipped with a monocular camera. The 3D environment map is represented by a set of 3D Gaussians, denoted as \(\mathbf{G}=\{\mathbf{G}_{i}\ |\ i=1,\ldots,N\}\). Each \(\mathbf{G}_{i}\) has a set of learnable parameters \([\boldsymbol{\mu}_{i},\mathbf{q}_{i},\mathbf{s}_{i},\alpha_{i},\boldsymbol{ \beta}_{i},\mathbf{f}_{i}]\), where \(\mathbf{f}_{i}\in\mathbb{R}^{d}\) is a self-supervised \(d\)-dimensional semantic feature such as DINO [4] for a more robust representation, and other parameters follow 3DGS as detailed in Sec. 3. This mapping approach not only captures the geometry but also the photometry of the environment, yielding a comprehensive scene representation for downstream tasks such as geometry reconstruction and view synthesis. The input to our approach comes from a set of unposed images, sourced from multitraverse RGB videos, denoted by \(\mathbf{I}=\{\mathbf{I}_{t}\in\mathbb{R}^{w\times h\times 3}\ |\ t=1,\ldots,T\}\), where \(T\) is the total number of images, \(w\) and \(h\) are the width and height of each image, respectively.

TargetThe target is to refine \(\mathbf{G}\) to a level where it can accurately render images \(\mathbf{I}_{t}(\boldsymbol{\xi}_{t};\mathbf{G})\) that closely match the real images \(\mathbf{I}_{t}\), captured from specific poses \(\boldsymbol{\xi}_{t}\). Although \(\mathbf{G}\) represents a 3D spatial map, the input images encompass 4D information with both spatial and temporal dimensions. Hence, our method needs to differentiate between the environment and ephemeral objects, like pedestriansand vehicles, to maintain robustness against pixels that represent transient entities. This necessitates addressing a robust optimization problem, where the outliers are transient objects--those that are either in motion or capable of moving--while the inliers are backgrounds.

### Overall Architecture

Figure 2 shows our overall workflow. Given RGB images \(\mathbf{I}\) collected across multiple traversals, we first leverage the classic Structure from Motion (SfM) [2] to jointly reconstruct sparse points for the initialization of Gaussians and obtain the camera poses \(\boldsymbol{\xi}=\{\boldsymbol{\xi}_{t}\ |\ t=1,\dots,T\}\). We then utilize the differential rendering pipeline of 3DGS to learn the positions, rotations, scales, opacities, colors, and semantic features of the 3D environmental Gaussians \(\mathbf{G}\), supervised by ground truth RGB \(\mathbf{I}\) and self-supervised feature maps [4] denoted by \(\mathbf{F}=\{\mathbf{F}_{t}\in\mathbb{R}^{w\times h\times d}\ |\ t=1,\dots,T\}\). Then we exploit the feature residual maps to extract ephemeral object masks denoted by \(\mathbf{M}=\{\mathbf{M}_{t}\in\mathbb{R}^{w\times h}\ |\ t=1,\dots,T\}\). Finally, we finetune 3D Gaussians \(\mathbf{G}\) through robust optimization by leveraging the ephemerality masks. In summary, 3DGM includes the three stages denoted by Initialization, EmerSeg, and EnvGS, as shown in Appendix A.1. We detail each stage from Sec. 4.3 to 4.5.

### Initialization: Structure from Motion

The SfM pipeline frequently faces challenges in single-traversal scenarios, largely due to the limited scene coverage achieved with RGB observations collected along a narrow and long camera trajectory. Conversely, RGB images from multiple traversals offer a broader array of viewpoints, significantly improving the triangulation and bundle adjustment processes. Additionally, this approach can leverage the 2D consensus of hand-crafted features in the correspondence search, improving robustness against transient objects, which manifest as dissensus pixels across traversals. Moreover, our empirical experiments underscore the importance of the number of traversals for smooth initialization. A reduction in traversals can lead to a lack of sufficient image data, thereby failing the SfM initialization.

### EmerSeg: Emerged Ephemerality Segmentation by Feature Residuals Mining

Feature distillationWe utilize robust feature representations to enhance consensus verification, as the feature space exhibits better robustness against lighting variations and embodies semantic meanings, facilitating the decomposition of the transient objects by removing groups of semantically dissensus pixels. We minimize the following RGB and feature rendering loss:

\[\mathcal{L}=\sum_{t}(\mathcal{L}_{rgb}(\mathbf{I}_{t}(\boldsymbol{\xi}_{t}; \mathbf{G}),\mathbf{I}_{t})+\mathcal{L}_{feat}(\mathbf{F}_{t}(\boldsymbol{\xi} _{t};\mathbf{G}),\mathbf{F}_{t}))\] (2)

where \(\mathbf{I}_{t}(\boldsymbol{\xi}_{t};\mathbf{G})\in\mathbb{R}^{w\times h\times 3}\) and \(\mathbf{F}_{t}(\boldsymbol{\xi}_{t};\mathbf{G})\in\mathbb{R}^{w\times h\times d}\) are the rendered RGB image and feature map given pose \(\boldsymbol{\xi}_{t}\in\mathfrak{se}(3)\) and Gaussians \(\mathbf{G}\). \(\mathbf{I}_{t}\) and \(\mathbf{F}_{t}\) are the corresponding ground truth RGB and feature map. \(\mathcal{L}_{rgb}\) and \(\mathcal{L}_{feat}\) are loss functions for RGB images and semantic features. _As inlier pixels substantially outweigh outlier pixels, the model is primarily steered by gradients from

Figure 2: **An overall illustration of 3DGM. Given RGB camera observations collected at different times, we use COLMAP to obtain the camera poses and initial Gaussian points. Then we utilize splatting-based rasterization to render both RGB images and robust features from the environmental Gaussians. We further leverage feature residuals to extract the object masks by mining spatial information of the residuals. Finally, we utilize the ephemerality masks to finetune the 3D Gaussians.**

consensus inlier pixels towards learning permanent features. As a result, pixels manifesting high loss in feature space are very likely to be outliers._

Feature residuals miningWe derive transient object masks by leveraging the spatial information in the feature residual maps, as shown in the right column of Fig. XVI-XXI. After training, we normalize the feature residuals and suppress pixels with residual values below a predefined threshold. Contours are then extracted from the normalized residual maps using spatial gradient information [68]. We refine these contours by applying spatial priors to eliminate those that are too small or located in the sky. Finally, we merge nearby contours and extract a convex hull for each merged contour. Ultimately, ephemerality masks \(\mathbf{M}\) are produced from simple postprocessing of feature residuals without additional training. More details are shown in Appendix A.2.

### EnvGS: Environmental Gaussian Splatting via Robust Optimization

After obtaining ephemerality masks \(\mathbf{M}\), we focus on minimizing the following robust loss function (taking L1 loss as an example):

\[\mathcal{L}=\sum_{t}\mathcal{L}_{rgb}(\mathbf{M}_{t}\odot\mathbf{I}_{t}( \boldsymbol{\xi}_{t};\mathbf{G}),\mathbf{M}_{t}\odot\mathbf{I}_{t})\] (3)

where \(\mathbf{M}_{t}\) is an ephemerality mask for the \(t\)-th image to downgrade the influence of outlier pixels. Optionally, we employ a depth smoothness loss and sky masks to further improve the geometry reconstruction, as illustrated in Appendix A.3.

### Comparison to Arts

The pioneering work addressing similar problems is NeRF-W [69], which learns volumetric representations from unconstrained photo collections. It employs uncertainty estimation to mask transient objects situated in image areas of high uncertainty. The following research efforts propose to learn a transient mask, aiming to eliminate occluders [70, 71]. Another related work is RobustNeRF [72] which models distractors in training data as outliers of an optimization problem and proposes a form of robust estimation for NeRF training.

We have three main differences from prior works.

* **Target problem** We formulate robotic multitraverse RGB mapping as a robust representation learning problem, unlike previous works that focus on object-centric neural rendering of outdoor landmarks or multiple objects in indoor scenarios.
* **Scene decomposition** Our method enables a clearer decomposition of foreground and background, producing both 2D segmentation and 3D environmental Gaussians without any supervision. This represents a significant improvement over previous methods, which produce only blurry results in outdoor scenarios.
* **Technical novelty** We use Gaussian Splatting instead of the conventional NeRF approach. Our robust feature distillation and feature residuals mining fully exploit the spatial information of the rendering loss map, resulting in much better ephemerality segmentation.

## 5 Experiments

DatasetMost NeRF benchmarks [38, 44, 46] for driving focus on a single-traversal video of the Waymo [73] or nuScenes [74]. To address the gap, we introduce the first _unsupervised Mapping and segmentation via multitraverse_ (**Mapverse**) benchmark, which comprises **Mapverse-Ithaca365** (see Appendix B.1) and **Mapverse-nuPlan** (see Appendix B.2) derived from the Ithaca365 [5] and nuPlan [6] datasets, respectively. **Mapverse** features 40 locations, each with 10\(\sim\)16 traversals, yielding a total of 467 videos and 35,304 images. Due to space constraints, we present results for **Mapverse-Ithaca365** (20 locations, 200 videos, 20,000 images) in the main text, with additional results in **Mapverse-nuPlan** provided in Appendix F-H. Sample data are visualized in Figs. I-IV.

Task and implementationWe benchmark three tasks: _(1) unsupervised 2D ephemerality segmentation_, _(2) 3D reconstruction_, and _(3) neural rendering_ in multitraversal driving. Our benchmark can inspire wide applications in unsupervised perception, autolabeling, camera-only 3D reconstruction and neural simulation in self-driving and robotics. For efficiency, we compress feature dimensions from 768 to 64 using PCA. Our model uses KL divergence for feature alignment and L1 loss for RGB reconstruction. All experiments are conducted on a single NVIDIA RTX 3090 GPU.

### Unsupervised 2D Ephemeral Object Segmentation

Task setupOur EmerSeg can segment ephemeral traffic participants in a multitraverse image collection, _without any supervision_. This will help identify moving objects like vehicles and pedestrians, as well as static objects with the potential for movement, such as parked cars or traffic cones. We use a _training-as-optimization_ pipeline and adopt the _Intersection over Union (IoU) metric_ for evaluation. Regarding comparison methods, we employ several state-of-the-art _semantic segmentation_ models trained with human annotations to create pseudo ground-truth masks for transient objects (pedestrians, vehicles, bicyclists, and motorcyclists). We also compare EmerSeg with _unsupervised segmentation_ methods. We report the main comparison results in Sec. 5.1.1 and ablation studies in Sec. 5.1.2.

#### 5.1.1 Quantitative and Qualitative Evaluations

Comparison against supervised methodsWe compare our method with state-of-the-art (SOTA) semantic segmentation methods: PSPNet [75], SegViT [76], Mask2Former [77], SegFormer [78], and InternImage [79]. _Note that these methods require dense pixel-level annotations to learn semantics_. We directly use these models trained on either ADE20K [80] or Cityscapes [81] to produce masks on **Mapverse-Ithaca365**. The overall IoU scores of EmerSeg average around 0.45 compared to the five supervised models; see Tab. 1. IoU scores across 20 locations are detailed in Fig. 3, with seven locations surpassing 50% IoU, and the highest score reaching 56% compared to SegFormer. These results highlight the promising potential of our unsupervised segmentation paradigm.

Comparison against unsupervised methodsWe compare EmerSeg with two SOTA unsupervised segmentation methods, _i.e._, STEGO [82] and CAUSE [83]. We train both methods on our dataset using their unsupervised objectives. _Note that these unsupervised baseline methods cannot grasp the semantics or the concept of ephemerality and can only perform clustering within a single image_. Following prior work, we use a Hungarian matching algorithm to align the unlabeled clusters with pseudo ground-truth masks for evaluation. As shown in Tab. 1, EmerSeg significantly outperforms STEGO and CAUSE, with a 21.36-point (89.8%) IoU improvement over STEGO using SegFormer masks. More importantly, EmerSeg can understand ephemerality, a capability lacking in prior works.

Qualitative comparisonEmerSeg performs well in various lighting and weather conditions, effectively segmenting cars, buses, and pedestrians; see Fig. 4. However, it struggles with small or distant objects due to low feature map resolution. _We empirically find that small objects have minimal impact on neural rendering as they occupy few pixels_. Additional qualitative results are in Fig. 5, with visualizations of baseline methods in Fig. 6. Detailed limitations are discussed in Appendix I.

Computation timeFigure 6 illustrates the convergence of our segmentation method, showing a rapid increase in IoU during the initial iterations, which stabilizes around iteration 4,000. Notably, a resolution of 110\(\times\)180 requires only 2,000 iterations to achieve an IoU score exceeding 40%, taking \(\sim\)8 minutes on a single NVIDIA RTX 3090 GPU for 1,000 images from 10 traversals of a location.

#### 5.1.2 Ablation Studies

Segmentation performance benefits from more traversalsWe evaluate 2D segmentation on 100 images from a single traversal, using inputs from varying numbers of traversals; see Tab. 2. Starting at 15.15% with one traversal, the IoU jumps to 42.31% with two, and continues to rise: 53.16% at 8 and 56.01% at 10 traversals. This shows a clear trend of improving IoU with more traversals, with significant gains between 1 and 2. Detailed visualizations are in Fig. 8.

Effective segmentation requires 32 feature dimensionsWe use PCA to compress the dimensions of DINOv2 features to save computation and storage. Our tests on segmentation performance at various dimensions revealed that 32 is an approximate threshold; IoU scores decrease significantly to around 10%-25% when the number of dimensions falls below 32, as shown in Tab. 2. Qualitative comparisons of different feature dimensions are demonstrated in Fig. 9.

A resolution of 70x110 can achieve an IoU >40%Table 2 shows IoU at various feature resolutions and sizes. IoU improves significantly as resolution increases from 25x40 (28.61%, 0.3 MB) to 110x180 (44.13%, 5.0 MB). However, higher resolutions like 140x210 and 160x260 result in slightly lower IoU scores of 42.48% and 41.19%, despite larger sizes. This indicates an optimal resolution at 110x180, balancing accuracy and efficiency. Visualizations at different resolutions are in Fig. 10.

Vision foundation model matters in unsupervised segmentationWe use robust features from self-supervised vision foundation models like DINO [54], DINOv2 [4], and DINOv2 with registers [67]. Additionally, we employ DVT [3] to reduce grid-like artifacts in ViT feature maps. As shown in Tab. 2, Denoised DINOv2 outperforms other models, highlighting the importance of robust, discriminative features for identifying transient clusters. Detailed visualizations are in Fig. 11.

### 3D Environment Reconstruction

Task setupOur EnvGS can extract 3D points from Gaussian Splatting, enabling the reconstruction of 3D environments from camera-only input while effectively ignoring transient objects across repeated traversals. We utilize a _training-as-optimization_ pipeline and employ the _Chamfer Distance (CD) metric_ for quantitative evaluation. For our comparison baseline, we use the state-of-the-art

\begin{table}
\begin{tabular}{l|c|c c|c c c|c c} \hline \hline \multicolumn{1}{c|}{**Number of Traversals**} & \multicolumn{3}{c|}{**Feature Dimension**} & \multicolumn{3}{c|}{**Feature Resolution**} & \multicolumn{2}{c}{**Feature Backbone**} \\ \hline \(\boldsymbol{\#}\) & **IoU (\%)** & **Dim.** & **Runtime** & **IoU (\%)** & **Res.** & **Size (MB)** & **IoU (\%)** & **Backbone** & **IoU (\%)** \\ \hline
1 & 15.15 & 4 & 00:13:50 & 9.45 & 25\(\times\)40 & 0.3 & 28.61 & DINO [54] & 16.51 \\
2 & 42.31 & 8 & 00:16:50 & 10.91 & 50\(\times\)80 & 1.0 & 35.91 & Denoised DINO [3] & 14.95 \\
3 & 46.62 & 16 & 00:18:37 & 26.32 & 70\(\times\)110 & 1.9 & 40.09 & DINOv2 [4] & 35.14 \\
5 & 53.68 & 32 & 00:24:48 & 37.51 & 110\(\times\)180 & 5.0 & **44.13** & Denoised DINOv2 [3] & **44.413** \\
9 & 54.50 & 64 & 00:40:25 & **44.13** & 140\(\times\)120 & 7.4 & 42.48 & DINOv2-reg [67] & 23.51 \\
10 & **56.01** & 128 & 01:13:53 & 42.55 & 160\(\times\)260 & 10.5 & 41.19 & Denoised DINOv2-reg [5] & 36.30 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Ablation Study Results of EmerSeg in Mapverse-Ithaca365.**

Figure 4: **Qualitative evaluations of EmerSeg in Mapverse-Ithaca365.**DepthAnything [84] model, which is trained with a combination of LiDAR ground truth (GT) depth data and unlabeled image data. This approach ensures that DepthAnything leverages diverse data sources to achieve satisfactory performance in zero-shot depth estimation.

Quantitative resultsFigure 5 demonstrates the large reduction in Chamfer Distance (CD) achieved by EnvGS across nearly all locations. Our method achieves an average CD of approximately 0.9 meters, showcasing its precision in 3D reconstruction. Notably, there are five locations where the CD is even lower than 0.5 meters, highlighting the good accuracy of our approach in these areas. In contrast, DepthAnything has an average CD of around 1.9 meters, indicating a notable performance gap between the two methods. More importantly, our method avoids the need for costly LiDAR sensors during training, making it a cost-effective autonomous mapping solution for self-driving and robotics. Leveraging techniques such as mesh reconstruction [30] and 2D Gaussian Splatting [85] could further enhance the geometric reconstruction capabilities of our method.

Qualitative resultsFigure 5 showcases depth visualizations of EnvGS across various driving scenarios. The depth maps generated by EnvGS exhibit superior accuracy, with smooth transitions from near to far objects and well-defined edges of scene structures. Additionally, EnvGS effectively removes transient objects without human supervision. Visualizations in 3D are shown in Fig. 11.

### Neural Environment Rendering

Task setupOur EnvGS can also achieve novel view synthesis through splatting-based rasterization. The challenge lies in ensuring the environment rendering automatically bypasses the non-environment pixels, _i.e._, transient objects. We evaluate the quality of rendered images using three metrics: Learned Perceptual Image Patch Similarity (LPIPS), Structural Similarity Index (SSIM), and Peak Signal-to-Noise Ratio (PSNR). Given the absence of ground truth RGB images for clean backgrounds, we utilize the pretrained SegFormer [78] model to isolate foreground regions, allowing us to focus our evaluation exclusively on the quality of the background rendering.

Baseline methodsOur baseline methods include two NeRF-based methods, leveraging the implementation framework of iNGP [24]. The first, VanillaNeRF, constructs the scene within a single, static hash table and directly learns grid features from multitraverse images. In contrast, RobustNeRF [72] introduces an adaptive weighting mechanism to filter out outliers. In addition to the original 3DGS framework, we introduce two 3DGS-based baseline methods. 3DGS+RobustNeRF integrates the loss function from RobustNeRF, and 3DGS+SegFormer utilizes masks generated by a supervised segmentation model. For a fair comparison, all methods exclusively rely on camera images as input.

Results and discussionsTable 3 presents a quantitative comparison of various methods, showing that 3DGS-based approaches outperform NeRF-based methods. Adding the RobustNeRF loss function does not improve rendering quality in driving scenes. However, incorporating SegFormer or EmerSeg masks achieves the best LPIPS and SSIM. This is notable within a purely self-supervised framework, showcasing the potential of our self-supervised paradigm in pushing the boundaries of neural mapping. We present qualitative examples in Fig. 6, where it is evident that the original

Figure 5: **Qualitative and quantitative evaluation of 3D geometry in Mapverse-Ithaca365.**

3DGS model struggles with accurately reconstructing background regions affected by transient objects. More interestingly, our method can identify and mask out not only the objects themselves but also their associated non-environmental elements, such as shadows, as shown in the third and sixth columns of Fig. 6. More qualitative examples can be found in Fig. XIV.

## 6 Conclusion

Broader impactsThe concept of vision-only neural representation learning through repeated traversals extends beyond object segmentation and environment mapping, benefiting the vision and robotics communities. With a neural map prior, our approach becomes a powerful self-supervised framework for change detection and object discovery. This capability to render and analyze multitraverse environments over time is crucial for identifying environmental changes, aiding in early intervention for deforestation, urban expansion, or post-disaster assessments. Additionally, our method can serve as a baseline for autolabeling 2D masks and has potential for 3D autolabeling with LiDAR integration.

LimitationsOur method faces limitations in modeling large environmental variations, including nighttime conditions, major seasonal shifts, and adversarial weathers. We also note the presence of noise in the segmentation outputs caused by motion blur or appearance shifts. Leveraging temporal information or more powerful vision foundation models could help address this issue. More discussions can be found in Appendix I.

SummaryWe introduce 3D Gaussian Mapping (3DGM), a novel self-supervised, camera-only framework that utilizes repeated traversals for simultaneous 3D environment mapping (EnvGS) and 2D unsupervised object segmentation (EmerSeg). Additionally, we develop the **Mapverse** benchmark, comprising nearly 500 driving video clips from the Ithaca365 and nuPlan datasets. Our method's effectiveness in unsupervised 2D segmentation, 3D reconstruction, and neural rendering is validated through both qualitative and quantitative assessments in repeated driving scenarios. Furthermore, 3DGM opens new research opportunities, such as online unsupervised object discovery and offline autolabeling. We believe our work will advance vision-centric and learning-based self-driving and robotics, setting new standards in multitraverse setups and self-supervised scene understanding.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline
**Metrics \(\backslash\) Methods** & **VanillaNeRF**[2.4] & **RobustNeRF**[72] & **3DGS+RobustNeRF** & **3DGS**[1] & **3DGS+SegFormer** & **EnvGS (Ours)** \\ \hline
**LPIPS** (\(\downarrow\)) & 0.423 & 0.443 & 0.416 & 0.227 & 0.212 & 0.213 \\
**SSIM** (\(\uparrow\)) & 0.603 & 0.609 & 0.654 & 0.798 & 0.806 & 0.806 \\
**PSNR** (\(\uparrow\)) & 19.18 & 19.22 & 19.97 & 22.92 & 22.81 & 22.78 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Quantitative evaluation of novel view synthesis.** We set test/training views as 1/8. Pixels corresponding to transient objects are removed in the evaluations since we do not have ground truth background pixels in these regions occluded by transient objects.

Figure 6: **Qualitative evaluations of the environment rendering.** Our method demonstrates robust performance against transient objects, and can even outperform the method equipped with a pretrained model in some cases. Notably, this includes the effective removal of object shadows.

## Acknowledgement

We express our deep gratitude to Jiawei Yang and Sanja Fidler for their valuable feedback throughout the project. We also thank Yurong You and Carlos A. Diaz-Ruiz for their support with the Ithaca365 dataset, and Shijie Zhou for his help with high-dimensional feature rendering in 3DGS. Yiming Li gratefully acknowledges support from the NVIDIA Graduate Fellowship Program.

## References

* [1] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. _ACM Trans. Graph._, 42(4), 2023.
* [2] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 4104-4113, 2016.
* [3] Jiawei Yang, Katie Z Luo, Jiefeng Li, Kilian Q Weinberger, Yonglong Tian, and Yue Wang. Denoising vision transformers. _arXiv preprint arXiv:2401.02957_, 2024.
* [4] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision. _Trans. Mach. Learn Res._, 2024.
* [5] Carlos A Diaz-Ruiz, Youya Xia, Yurong You, Jose Nino, Junan Chen, Josephine Monica, Xiangyu Chen, Katie Luo, Yan Wang, Marc Emond, et al. Ithaca365: Dataset and driving perception under repeated and challenging weather conditions. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 21383-21392, 2022.
* [6] Napat Karchanachari, Dimitris Geromichalos, Kok Seang Tan, Nanxiang Li, Christopher Eriksen, Shakiba Yaghoubi, Noushin Mehdipour, Gianmarco Bernasconi, Whye Kit Fong, Yiluan Guo, et al. Towards learning-based planning: The uplan benchmark for real-world autonomous driving. In _IEEE Int. Conf. Robot. Autom._, 2024.
* [7] Yurong You, Cheng Perng Phoo, Carlos Andres Diaz-Ruiz, Katie Z Luo, Wei-Lun Chao, Mark Campbell, Bharath Hariharan, and Kilian Q Weinberger. Better monocular 3d detectors with lidar from the past. In _IEEE Int. Conf. Robot. Autom._, 2024.
* [8] Yurong You. _Enhancing 3D Perception with Unlabeled Repeated Historical Data for Autonomous Vehicles_. PhD thesis, Cornell University, 2023.
* [9] Xuan Xiong, Yicheng Liu, Tianyuan Yuan, Yue Wang, Yilun Wang, and Hang Zhao. Neural map prior for autonomous driving. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 17535-17544, 2023.
* [10] Tianyuan Yuan, Yucheng Mao, Jiawei Yang, Yicheng Liu, Yue Wang, and Hang Zhao. Presight: Enhancing autonomous vehicle perception with city-scale nerf priors. In _Eur. Conf. Comput. Vis._, 2024.
* [11] Yurong You, Katie Z Luo, Xiangyu Chen, Junan Chen, Wei-Lun Chao, Wen Sun, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Hindsight is 20/20: Leveraging past traversals to aid 3d perception. In _Int. Conf. Learn. Represent._, 2021.
* [12] Yurong You, Katie Luo, Cheng Perng Phoo, Wei-Lun Chao, Wen Sun, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Learning to detect mobile objects from lidar scans without labels. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 1130-1140, 2022.
* [13] Yurong You, Cheng Perng Phoo, Katie Luo, Travis Zhang, Wei-Lun Chao, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Unsupervised adaptation from repeated traversals for autonomous driving. In _Adv. Neural Inform. Process. Syst._, volume 35, pages 27716-27729, 2022.
* [14] Will Maddern, Geoffrey Pascoe, Chris Linegar, and Paul Newman. 1 year, 1000 km: The oxford robotcar dataset. _The International Journal of Robotics Research_, 36(1):3-15, 2017.
** [15] Carl Toft, Will Maddern, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, et al. Long-term visual localization revisited. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44(4):2074-2088, 2020.
* [16] Stephanie Lowry, Niko Sunderhauf, Paul Newman, John J Leonard, David Cox, Peter Corke, and Michael J Milford. Visual place recognition: A survey. _ieee transactions on robotics_, 32(1):1-19, 2015.
* [17] Yiming Li, Zonglin Lyu, Mingxuan Lu, Chao Chen, Michael Milford, and Chen Feng. Collaborative visual place recognition. _arXiv preprint arXiv:2310.05541_, 2023.
* [18] Dan Barnes, Will Maddern, Geoffrey Pascoe, and Ingmar Posner. Driven to distraction: Self-supervised distractor learning for robust monocular visual odometry in urban environments. In _IEEE Int. Conf. Robot. Autom._, pages 1894-1900. IEEE, 2018.
* [19] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5855-5864, 2021.
* [20] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5480-5490, 2022.
* [21] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. In _Int. Conf. Comput. Vis._, 2023.
* [22] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5438-5448, 2022.
* [23] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plonoxels: Radiance fields without neural networks. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 5501-5510, 2022.
* [24] Thomas Muller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. _ACM Trans. Graph._, 41(4):1-15, 2022.
* [25] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa splatting. _IEEE Trans. Vis. Comput. Graph._, 8(3):223-238, 2002.
* [26] Yiwen Chen, Zilong Chen, Chi Zhang, Feng Wang, Xiaofeng Yang, Yikai Wang, Zhongang Cai, Lei Yang, Huaping Liu, and Guosheng Lin. Gaussianeditor: Swift and controllable 3d editing with gaussian splatting. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2024.
* [27] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. In _Int. Conf. 3D Vis._, 2023.
* [28] Zeyu Yang, Hongye Yang, Zijie Pan, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. In _Int. Conf. Learn. Represent._, 2024.
* [29] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds. _arXiv preprint arXiv:2403.20309_, 2024.
* [30] Antoine Guedon and Vincent Lepetit. Sugar: Surface-aligned gaussian splatting for efficient 3d mesh reconstruction and high-quality mesh rendering. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2024.
* [31] Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2024.
* [32] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2024.
* [33] Hidenobu Matsuki, Riku Murai, Paul H. J. Kelly, and Andrew J. Davison. Gaussian Splatting SLAM. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2024.

* [34] Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, and Li Zhang. Periodic vibration gaussian: Dynamic urban scene reconstruction and real-time rendering. _arXiv preprint arXiv:2311.18561_, 2023.
* [35] Cheng Zhao, Su Sun, Ruoyu Wang, Yuliang Guo, Jun-Jun Wan, Zhou Huang, Xinyu Huang, Yingjie Victor Chen, and Liu Ren. Tclc-gs: Tightly coupled lidar-camera gaussian splatting for surrounding autonomous driving scenes. _arXiv preprint arXiv:2404.02410_, 2024.
* [36] Konstantinos Rematas, Andrew Liu, Pratul P Srinivasan, Jonathan T Barron, Andrea Tagliascchi, Thomas Funkhouser, and Vittorio Ferrari. Urban radiance fields. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 12932-12942, 2022.
* [37] Zian Wang, Tianchang Shen, Jun Gao, Shengyu Huang, Jacob Munkberg, Jon Hasselgren, Zan Gojcic, Wenzheng Chen, and Sanja Fidler. Neural fields meet explicit geometric representations for inverse rendering of urban scenes. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 8370-8380, 2023.
* [38] Jianfei Guo, Nianchen Deng, Xinyang Li, Yeqi Bai, Botian Shi, Chiyu Wang, Chenjing Ding, Dongliang Wang, and Yikang Li. Streetsurf: Extending multi-view implicit surface reconstruction to street views. _arXiv preprint arXiv:2306.04988_, 2023.
* [39] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knott, and Felix Heide. Neural scene graphs for dynamic scenes. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 2856-2865, 2021.
* [40] Ziyang Xie, Junge Zhang, Wenye Li, Feihu Zhang, and Li Zhang. S-nerf: Neural radiance fields for street views. In _Int. Conf. Learn. Represent._, 2022.
* [41] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, and Raquel Urtasun. Unisim: A neural closed-loop sensor simulator. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 1389-1399, 2023.
* [42] Adam Tonderski, Carl Lindstrom, Georg Hess, William Ljungbergh, Lennart Svensson, and Christoffer Petersson. Neural: Neural rendering for autonomous driving. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2024.
* [43] Xiaoyu Zhou, Zhiwei Lin, Xiaojun Shan, Yongtao Wang, Deqing Sun, and Ming-Hsuan Yang. Drivinggaussian: Composite gaussian splatting for surrounding dynamic autonomous driving scenes. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2023.
* [44] Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, and Sida Peng. Street gaussians for modeling dynamic urban scenes. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2024.
* [45] Hongyu Zhou, Jiahao Shao, Lu Xu, Dongfeng Bai, Weichao Qiu, Bingbing Liu, Yue Wang, Andreas Geiger, and Yiyi Liao. Hugs: Holistic urban 3d scene understanding via gaussian splatting. _arXiv preprint arXiv:2403.12722_, 2024.
* [46] Jiawei Yang, Boris Ivanovic, Or Litany, Xinshuo Weng, Seung Wook Kim, Boyi Li, Tong Che, Danfei Xu, Sanja Fidler, Marco Pavone, and Yue Wang. Emergent: Emergent spatial-temporal scene decomposition via self-supervision. In _Int. Conf. Learn. Represent._, 2024.
* [47] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 8248-8258, 2022.
* [48] Yuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin. Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering. In _Eur. Conf. Comput. Vis._, pages 106-122. Springer, 2022.
* [49] Haithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 12922-12931, 2022.
* [50] Haithem Turki, Jason Y Zhang, Francesco Ferroni, and Deva Ramanan. Suds: Scalable urban dynamic scenes. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 12375-12385, 2023.
* [51] Ruilong Li, Sanja Fidler, Angjoo Kanazawa, and Francis Williams. Nerf-xl: Scaling nerfs with multiple gpus. _arXiv preprint arXiv:2404.16221_, 2024.
* [52] Jiaqi Lin, Zhihao Li, Xiao Tang, Jianzhuang Liu, Shiyong Liu, Jiayue Liu, Yangdi Lu, Xiaofei Wu, Songcen Xu, Youliang Yan, et al. Vastgaussian: Vast 3d gaussians for large scene reconstruction. _arXiv preprint arXiv:2402.17427_, 2024.

* [53] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 12475-12485, 2020.
* [54] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Int. Conf. Comput. Vis._, pages 9650-9660, 2021.
* [55] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In _Eur. Conf. Comput. Vis._, pages 402-419. Springer, 2020.
* [56] Massimo Piccardi. Background subtraction techniques: a review. In _IEEE international conference on systems, man and cybernetics_, volume 4, pages 3099-3104. IEEE, 2004.
* [57] Sebastian Brutzer, Benjamin Hoferlin, and Gunther Heidemann. Evaluation of background subtraction techniques for video surveillance. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 1937-1944. IEEE, 2011.
* [58] Xiaowei Zhou, Can Yang, and Weichuan Yu. Moving object detection by detecting contiguous outliers in the low-rank representation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 35(3):597-610, 2012.
* [59] Xin Liu, Guoying Zhao, Jiawen Yao, and Chun Qi. Background subtraction based on low-rank and structured sparse decomposition. _IEEE Trans. Image Process._, 24(8):2502-2514, 2015.
* [60] Hayman and Eklundh. Statistical background subtraction for a mobile observer. In _Int. Conf. Comput. Vis._, pages 67-74. IEEE, 2003.
* [61] Yaser Sheikh, Omar Javed, and Takeo Kanade. Background subtraction for freely moving cameras. In _Int. Conf. Comput. Vis._, pages 1219-1225. IEEE, 2009.
* [62] Wentao Yuan, Zhaoyang Lv, Tanner Schmidt, and Steven Lovegrove. Star: Self-supervised tracking and reconstruction of rigid objects in motion with neural rendering. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13144-13152, 2021.
* [63] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, Forrester Cole, and Cengiz Oztireli. D^2nerf: Self-supervised decoupling of dynamic and static objects from a monocular video. _Adv. Neural Inform. Process. Syst._, 35:32653-32666, 2022.
* [64] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Adv. Neural Inform. Process. Syst._, 33:1877-1901, 2020.
* [65] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Adv. Neural Inform. Process. Syst._, 30, 2017.
* [66] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In _Int. Conf. Learn. Represent._, 2020.
* [67] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In _Int. Conf. Learn. Represent._, 2023.
* [68] Satoshi Suzuki et al. Topological structural analysis of digitized binary images by border following. _Computer vision, graphics, and image processing_, 30(1):32-46, 1985.
* [69] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 7210-7219, 2021.
* [70] Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng, Xuan Wang, and Jue Wang. Hallucinated neural radiance fields in the wild. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 12943-12952, 2022.
* [71] Yifan Yang, Shuhai Zhang, Zixiong Huang, Yubing Zhang, and Mingkui Tan. Cross-ray neural radiance fields for novel-view synthesis from unconstrained image collections. In _Int. Conf. Comput. Vis._, pages 15901-15911, 2023.

* [72] Sara Sabour, Suhani Vora, Daniel Duckworth, Ivan Krasin, David J Fleet, and Andrea Tagliasacchi. Robustnerf: Ignoring distractors with robust losses. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 20626-20636, 2023.
* [73] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 2446-2454, 2020.
* [74] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 11621-11631, 2020.
* [75] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 2881-2890, 2017.
* [76] Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xiaolin Wei, Chunhua Shen, et al. Segvit: Semantic segmentation with plain vision transformers. In _Adv. Neural Inform. Process. Syst._, volume 35, pages 4971-4982, 2022.
* [77] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In _IEEE Conf. Comput. Vis. Pattern Recog._, pages 1290-1299, 2022.
* [78] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. In _Adv. Neural Inform. Process. Syst._, volume 34, pages 12077-12090, 2021.
* [79] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Interimimage: Exploring large-scale vision foundation models with deformable convolutions. In _IEEE Conf. Comput. Vis. Pattern Recog._, page IEEE Conf. Comput. Vis. Pattern Recog., 2023.
* [80] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 633-641, 2017.
* [81] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3213-3223, 2016.
* [82] Mark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T Freeman. Unsupervised semantic segmentation by distilling feature correspondences. In _Int. Conf. Learn. Represent._, 2022.
* [83] Junho Kim, Byung-Kwan Lee, and Yong Man Ro. Causal unsupervised semantic segmentation. _arXiv preprint arXiv:2310.07379_, 2023.
* [84] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2024.
* [85] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. _SIGGRAPH_, 2024.
* [86] Clement Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Unsupervised monocular depth estimation with left-right consistency. In _CVPR_, 2017.
* [87] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In _IEEE Conf. Comput. Vis. Pattern Recog._, 2017.
* [88] Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, and George Drettakis. A hierarchical 3d gaussian representation for real-time rendering of very large datasets. _ACM Transactions on Graphics_, 43(4), July 2024.

###### Contents (Appendix)

* A 3DGM: Additional Details
* A.1 Workflow of 3DGM
* A.2 Workflow of Feature Residuals Mining
* A.3 Additional Loss Function
* B The Mapverse Dataset
* B.1 Mapverse-Ithaca365
* B.2 Mapverse-nuPlan
* B.3 Visualization of Sample Data
* C Mapverse-Ithaca365: Additional Results of 2D Segmentation
* C.1 Additional Qualitative Results
* C.2 Visualizations of Supervised and Unsupervised Segmentation
* C.3 Performance over Training Iterations
* C.4 Ablation Study on Number of Traversal: Visualization and Discussion
* C.5 Ablation Study on Feature Dimension: Visualization and Discussion
* C.6 Ablation Study on Feature Resolution: Visualization and Discussion
* C.7 Ablation Study on Vision Foundation Model: Visualization and Discussion
* D Mapverse-Ithaca365: Additional Visualizations of 3D Reconstruction
* E Mapverse-Ithaca365: Additional Visualizations of Neural Rendering
* F Mapverse-nuPlan: Unsupervised 2D Segmentation
* F.1 Quantitative Results
* F.2 Qualitative Results
* G Mapverse-nuPlan: Depth Visualization
* H Mapverse-nuPlan: Neural Rendering
* I Limitations and Future Work
	* I.1 Unsupervised 2D Segmentation
	* I.2 Geometry Reconstruction
	* I.3 Neural Rendering

## Appendix A 3DGM: Additional Details

### Workflow of 3DGM

* Initialization with COLMAP.
* Input: RGB images **I*
* Output: A sparse set of 3D points and camera poses \(\bm{\xi}\).

* EmerSeg: Ephemerality Segmentation via Feature Residuals Mining.
* Input: RGB images **I**, semantic feature maps **F**, camera poses \(\bm{\xi}\).
* Output: 2D ephemerality masks **M**.

* EnvGS: Environmental Gaussian Splatting via Robust Optimization.
* Input: RGB images **I**, ephemerality masks **M**, camera poses \(\bm{\xi}\).
* Output: 3D environmental Gaussians **G**.

### Additional Loss Function

We offer an optional geometry-related loss function to enhance depth reconstruction when the focus is more on geometry than photometry.

Inverse Depth Smoothness LossThis loss function [86] encourages the smoothness of the depth map in non-edge areas with the penalty on the disparity gradients \(\nabla D_{i,j}\). Using image gradients \(\nabla I_{i,j}\) as weights reduce the impact of the loss in regions where edges are present, maintaining depth discontinuities at edges. The loss is formulated as:

\[\mathcal{L}_{depth}=\frac{1}{N}\sum_{i,j}\left(|\nabla D_{i,j}^{x}|\exp \left(-\|\nabla I_{i,j}^{x}\|\right)+|\nabla D_{i,j}^{y}|\exp\left(-\|\nabla I _{i,j}^{y}\|\right)\right)\] (4)

where D represents the inverse of the rendered depth map, and I is the ground truth image.

Sky LossWe aim to manipulate the opacity of the sky to 0 and other areas in the image to 1.

\[\mathcal{L}_{sky}=\frac{1}{N}\sum_{i,j}\left(|\mathcal{M}_{sky}-(1-\mathcal{O} )|\right)\] (5)

where \(\mathcal{M}_{sky}\) is the sky mask, with values of 1 for sky pixels and 0 for others, and \(\mathcal{O}\) denotes the rendered opacity ranging from 0 to 1.

[MISSING_PAGE_FAIL:18]

### Visualization of Sample Data

Figure 1: **Visualizations of Mapverse-Ithaca365 dataset (locations 1-10).** Each row represents image observations of the same location captured during different traversals, with five traversals shown for brevity. The figure encompasses diverse environments in the Ithaca area, from residential neighborhoods with houses, trees, and varying traffic, to suburban streets with signage and seasonal foliage changes, and finally to rural roads and highways with expansive landscapes. The columns provide comparative views of these locations under different conditions, highlighting the dynamic nature of the Mapverse-Ithaca365 dataset.

Figure 2: **Visualizations of Mapverse-Ithaca365 dataset (locations 11-20).** Each row captures image observations of the same location from different traversals, showing five traversals for brevity. The figure spans various environments within Ithaca, from expansive rural highways transitioning to suburban roads with clear signage, to wooded areas with parked vehicles, and urban intersections with notable buildings. The images depict the progression from rural outskirts to more densely populated urban centers, reflecting changes in traffic, lighting, and seasonal foliage. Columns provide comparative views of these locations under different conditions, emphasizing the dynamic and diverse nature of the Mapverse-Ithaca365 dataset.

Figure 3: **Visualizations of Mapverse-nuPlan dataset (locations 1-10).** Each row represents different image observations of the same location captured during multiple traversals, with five shown for brevity. The images encompass diverse environments in Las Vegas, including wide city streets with iconic buildings, billboards, palm trees, pedestrian bridges, and varying traffic conditions. Columns provide comparative views of the same locations under different conditions, illustrating the variability and complexity of the cityscape as captured in the Mapverse-nuPlan dataset.

Figure 4: **Visualizations of Mapverse-nuPlan dataset (locations 11-20).** Each row represents different image observations of the same location captured during multiple traversals, with five shown for brevity. The images cover various environments in Las Vegas, including city streets with overpasses, iconic buildings, palm trees, billboards, and varied traffic conditions. The sequence progresses from urban settings with heavy infrastructure and prominent landmarks to broader streets and intersections, capturing different times of day and lighting conditions. Columns provide comparative views of the same locations under different circumstances, showcasing the dynamic and ever-changing urban landscape of Las Vegas as recorded in the Mapverse-nuPlan dataset.

[MISSING_PAGE_EMPTY:23]

### Visualizations of Supervised and Unsupervised Segmentation

Figure 6: **Qualitative comparisons of our method and other supervised and unsupervised segmentation baselines.** This image demonstrates a comparison between our mask extraction and those derived from other semantic segmentation methods. The results indicate that our masks maintain superior integrity and detail in complex environments. Meanwhile, our method significantly outperforms unsupervised semantic segmentation models [82; 83] and is roughly equivalent to the masks generated by InternImage [79] and SegVit [76]. Although Mask2Former [77], PSPNet [87], and SegFormer [78] have advantages in recognizing people and other fine-grained objects, they can also lead to incorrect segmentation and noise in certain scenarios.

### Performance over Training Iterations

Figure 7 presents the IoU performance across iterations for two different feature resolutions (110x180 and 140x210), alongside visualizations of ephemerality masks and feature residuals at various iterations. The IoU graph on the left shows that both resolutions exhibit rapid improvement in the initial iterations, with the 110x180 resolution consistently outperforming the 140x210 resolution. The 110x180 resolution reaches an IoU of approximately 0.44, while the 140x210 resolution plateaus around 0.41. This indicates that the lower resolution (110x180) is more efficient in capturing ephemeral objects. On the right, the visualizations of ephemerality masks and feature residuals at different iterations (500 to 10000) demonstrate that higher iterations result in more detailed and accurate segmentation. Early iterations (500 and 1000) show sparse and less accurate masks. The progression also highlights the fast convergence of our method for effective segmentation.

SummaryIn summary, the figure demonstrates that the 110x180 feature resolution is more effective and efficient for segmentation, achieving higher IoU scores compared to the 140x210 resolution. The IoU increases rapidly in the initial iterations and stabilizes around iteration 4000. These results emphasize the importance of selecting an appropriate feature resolution and ensuring sufficient iterations to achieve optimal segmentation performance.

Figure 7: **IoU performance over iterations for different feature resolutions (110x180 and 140x210) and corresponding visualizations of ephemerality masks and feature residuals.** Visualizations at various iterations (500 to 10000) illustrate that higher iterations lead to more detailed and accurate segmentation. The results highlight the efficiency of the 110x180 resolution and the fast convergence of our method for effective segmentation.

[MISSING_PAGE_FAIL:26]

[MISSING_PAGE_FAIL:27]

### Ablation Study on Feature Resolution: Visualization and Discussion

Figure X presents the segmentation performance at different feature resolutions: 25x40, 50x80, 70x110, and 110x180. The RGB images on the left are segmented into ephemerality masks and feature residuals for each resolution. At the lowest resolution (25x40), the ephemerality masks capture very few objects with minimal detail. As the resolution increases, there is a noticeable improvement in object segmentation, with more objects being identified.

SummaryIn a word, the segmentation performance improves significantly with increased feature resolution. Lower resolutions (25x40 and 50x80) result in sparse object segmentation, indicating inadequate feature representation. The highest resolution (110x180) delivers the best results, with detailed and precise segmentation.

### Ablation Study on Vision Foundation Model: Visualization and Discussion

Figure XI compares the performance of different versions and configurations of the DINO model on ephemerality masks and feature residuals. For both DINOv1 and DINOv2, the raw versions show noisy feature residuals, indicating areas where the model fails to capture ephemeral objects accurately. The denoised versions show a notable improvement, with informative residuals and more accurate ephemerality masks. The DINOv2 models generally perform better than DINOv1, as evidenced by more precise object masks and more discriminative residuals. The inclusion of the register in DINOv2 does not introduce additional gains.

SummaryThe result highlights the progressive improvements in segmentation accuracy achieved through model evolution from DINOv1 to DINOv2, and the benefits of applying denoising techniques.

Figure XI: **Comparison of ephemerality masks and feature residuals using different versions of the DINO model.** The figure includes raw and denoised versions of DINOv1 and DINOv2, as well as raw and denoised versions of DINOv2 with a registration module (DINOv2-Register). Denoising enhances the quality of feature residuals, while registration does not yield notable gains.

[MISSING_PAGE_FAIL:30]

Figure XIII: **Visualizations of depth images in Mapverse-Ithaca365**

## Appendix E Mapverse-Ithaca365: Additional Visualizations of Neural Rendering

Figure XIV: **Qualitative evaluations of the environment rendering.** Our method demonstrates robust performance against transient objects.

[MISSING_PAGE_FAIL:33]

Figure XVI: **Qualitative results of EmerSeg for multiple traversals of location 1 of Mapverse-nuPlan.** From left to right: raw RGB image, extracted 2D ephemeral object masks, and normalized feature residuals visualized using a jet color map.

Figure XVII: **Qualitative results of EmerSeg for multiple traversals of location 2 of Mapverse-nuPlan.** From left to right: raw RGB image, extracted 2D ephemeral object masks, and normalized feature residuals visualized using a jet color map.

Figure XVIII: **Qualitative results of EmerSeg for multiple traversals of location 3 of Mapverse-nuPlan.** From left to right: raw RGB image, extracted 2D ephemeral object masks, and normalized feature residuals visualized using a jet color map.

Figure XIX: **Qualitative results of EmerSeg for multiple traversals of location 4 of Mapverse-nuPlan.** From left to right: raw RGB image, extracted 2D ephemeral object masks, and normalized feature residuals visualized using a jet color map.

Figure 16: **Qualitative results of EmerSeg for multiple traversals of location 5 of Mapverse-nuPlan.** From left to right: raw RGB image, extracted 2D ephemeral object masks, and normalized feature residuals visualized using a jet color map.

Figure XXI: **Qualitative results of EmerSeg for multiple traversals of location 6 of Mapverse-nuPlan.** From left to right: raw RGB image, extracted 2D ephemeral object masks, and normalized feature residuals visualized using a jet color map.

## Appendix G Mapverse-nuPlan: Depth Visualization

Figure XXII: **Visualizations of depth images in Mapverse-Ithaca365**

[MISSING_PAGE_EMPTY:41]

Limitations and Future Work

### Unsupervised 2D Segmentation

Shadow segmentationFigure XXIV illustrates the challenges encountered in accurately segmenting shadows. Each row represents different instances. The left column displays the original images, the middle column presents the segmentation output, and the right column highlights the areas where shadow removal failed, indicated by red circles. While there are some successful cases marked by green circles, our method lacks consistency across different scenes.

Large occludersFigure XXV illustrates the challenges faced when segmenting scenes with large and enduring occluders. When occluders occupy a significant portion of pixels and persist over time, our model tends to overfit to these occluders. This leads to a reduction in the feature residuals of the corresponding pixels, thereby failing the segmentation.

Long-range objectsFigure XXVI highlights the challenges encountered when segmenting scenes with small and long-range objects. The red circles indicate regions where the segmentation algorithm struggles to differentiate these objects from their surroundings, often missing or inaccurately segmenting them.

Reflective SurfacesFigure XXVII highlights the model's current limitations in handling reflective surfaces, which can vary significantly across traversals due to changes in lighting.

Future WorkFuture work will focus on developing better methods to robustly segment object shadows and leveraging temporal information to more effectively handle large and enduring occluders. Additionally, designing adaptive thresholds based on object distance will help better exploit the spatial information of the feature residuals. Furthermore, training a vision foundation model using large-scale, in-the-wild data will be crucial for enhancing the model's robustness.

### Geometry Reconstruction

There are still challenges in road reconstruction, particularly due to the textureless nature of road surfaces. To address this, integrating advanced techniques such as mesh reconstruction [30] and 2D

Figure XXIV: **Failure cases of shadow segmentation.**

Gaussian Splatting [85] could significantly enhance the geometric reconstruction capabilities of our method. By enhancing the geometric fidelity of road surfaces, these techniques can help overcome the limitations posed by textureless areas, ensuring a more comprehensive and reliable mapping of driving environments.

### Neural Rendering

Our method currently struggles with handling significant lighting variations and seasonal changes in the environment. Incorporating a 4D representation [28, 44], which accounts for changes over time, could further enhance the quality of neural rendering. Additionally, we have not yet investigated very large-scale scene reconstruction. Incorporating recent Level-of-Detail (LOD) techniques can help address this large-scale problem [88]. We leave these as future works.

Figure XXVI: **Failure cases when faced with reflective surfaces.**

Figure XXVII: **Failure cases when faced with small and long-range objects.**

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our contributions are elaborated as a list in the introduction and briefly introduced in the abstract. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of the work in Section 6 in the main text and Section I in the Appendix. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [NA] Justification: This work does not include theoretical results. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe our method clearly and fully in Section 4 of the main text and give more technical details in Section A of the Appendix. We also release our trained model, code, and data to assist in the reproduction of results for other researchers. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We release our code and provide detailed instructions on the GitHub repository to guide others to reproduce the main experimental results on our used public dataset. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide implementation details, data curation processes, and hyperparameter ablation studies in our main paper, with additional information available in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Error bars are not reported because it would be too computationally expensive. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide details on our GPUs and training duration for our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and make sure we obey all rules for this research. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discuss the broader impacts of our work in Section 6. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The release of our models and code has no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We have properly cited the original papers for the datasets used and mentioned their licenses in the Appendix. These datasets are used for academic purposes, and we fully respect their licenses and terms of use. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: This research does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.