Efficient RL with Impaired Observability: Learning to Act with Delayed and Missing State Observations

Minshuo Chen\({}^{1}\)   Yu Bai\({}^{2}\)   H. Vincent Poor\({}^{1}\)   Mengdi Wang\({}^{1}\)

\({}^{1}\)Princeton University  \({}^{2}\)Salesforce Research

###### Abstract

In real-world reinforcement learning (RL) systems, various forms of _impaired observability_ can complicate matters. These situations arise when an agent is unable to observe the most recent state of the system due to latency or lossy channels, yet the agent must still make real-time decisions. This paper introduces a theoretical investigation into efficient RL in control systems where agents must act with delayed and missing state observations. We establish near-optimal regret bounds, of the form \(\widetilde{\mathcal{O}}(\sqrt{\operatorname{poly}(H)SAK})\), for RL in both the delayed and missing observation settings. Despite impaired observability posing significant challenges to the policy class and planning, our results demonstrate that learning remains efficient, with the regret bound optimally depending on the state-action size of the original system. Additionally, we provide a characterization of the performance of the optimal policy under impaired observability, comparing it to the optimal value obtained with full observability.

## 1 Introduction

In Reinforcement Learning (RL), an agent engages with an environment in a sequential manner. In an ideal setting, at each time step, the agent would observe the current state of the environment, select an action to perform, and receive a reward Smallwood and Sondik (1973); Bertsekas (2012); Sutton and Barto (2018); Lattimore and Szepesvari (2020). However, real-world engineering systems often introduce impaired observability and latency, where the agent may not have immediate access to the instant state and reward information. In systems with lossy communication channels, certain state observations may even be permanently missing, never reaching the agent. Nevertheless, the agent is still required to make real-time decisions based on the available information.

The presence of impaired observability transforms the system into a complex interactive decision process (Figure 1), presenting challenges for both learning and planning in RL. With limited knowledge about recent states and rewards, the agent's policy must extract information from the observed history and utilize it to make immediate decisions. This introduces significant complexity to the policy class and poses difficulties for RL. Moreover, the loss of information due to permanently missing observations further hampers the efficiency of RL methods. Although a naive approach would involve augmenting the state and action space to create a fully observable Markov Decision Process (MDP), such a method would lead to exponential regret growth in the state-action size.

Why existing methods do not work.One may be tempted to cast the problem of impaired observability into a Partially Observed MDPs (POMDPs). However, this would not solve the problem. In POMDP, the system does not reveal its instant state to the agent but provides an emission state observation conditioned on the latent state. POMDPs are known to suffer from the curse of history Papadimitriou and Tsitsiklis (1987); Bertsekas (2012); Krishnamurthy (2016), unless additional assumptions are imposed. Existing efficient algorithms focus on subclasses of POMDPs with decodable or distinguishable partial observations Jin et al. (2020); Uehara et al. (2022); Zhan et al.

[2022], Chen et al. [2022], Liu et al. [2022], Zhong et al. [2022], Chen et al. [2023], where the unseen instant state can be inferred from recent observations. Unfortunately, MDPs with impaired observability do not fall into these benign subclasses. The reason behind this is that at each time step, a new observation, if any, is in fact a past state. Viewing it as an emission state of the current one leads to a time reversal posterior distribution depending on the underlying transitions, which suffers from the curse of history and makes the POMDP intractable. The problem becomes even harder if some observations get missing.

Empirical evidences suggested that efficient RL is possible even with impaired state observability [2008], Liu et al. [2014], Agarwal and Aggarwal [2021]. However, theoretical understanding of this problem is very limited. One notable work Walsh et al. [2007] studied learning with constant-time delayed observations. They identified subclasses of MDPs with nearly deterministic transitions that can be efficiently learned. Beyond this special case, efficient RL with impaired observability in MDPs with fully generality remains largely open.

Some recent works studied delayed feedback in MDPs [2023], Howson et al. [2023]. It is a fundamentally different problem where the agent's policy can still access real-time states but learning uses delayed data. Our problem is fundamentally harder because the agent's policy can only access the lossy and delayed history. See Section 1 for more discussions.

Our results.In this paper, we provide algorithms and regret analysis for learning the optimal policy in tabular MDPs with impaired observability. Note that this optimal policy is a different one from the optimal policy with full observability. To approach this problem, we construct an augmented MDP reformulation where the original state space is expanded to include available observations of past state and an action sequence. However, the expanded state space is much larger than the original one and naive application of known methods would lead to exponentially large regret bounds. In our analysis, we exploit structure of the augmented transition model to achieve efficient learning and sharp regret bounds. The main results are summarized as follows.

\(\bullet\) For MDPs with stochastic delays, we prove a sharp \(\widetilde{O}(H^{4}\sqrt{SAK})\) regret bound (Theorem 4.1) comparing to the best feasible policy, Here \(S\) and \(A\) are the sizes of the original state and action spaces, respectively, \(H\) is the horizon, and \(K\) is the number of episodes. Here we allows the delay to be stochastic and conditionally independent given on current state and action. Moreover, we quantify the performance degradation of optimal value due to impaired observability, compared to optimal value of fully observable MDPs (Proposition B.2). We also showcase in Proposition 4.2 that a short delay does not reduce the optimal value, but slightly longer delay leads to substantial degradation.

\(\bullet\) For MDPs with randomly missing observations, we provide an optimistic RL method that provably achieves \(\widetilde{\mathcal{O}}(\sqrt{H^{3}S^{2}AK})\) regret (Proposition 5.1). We also provide a sharper \(\widetilde{\mathcal{O}}(H^{4}\sqrt{SAK})\) regret in the case when the missing rate is sufficiently small (Theorem 5.2).

To our best knowledge, these results present a first set of theories for RL with delayed and missing observations. Remarkably, our regret bounds nearly match the minimax-optimal regret of standard MDP in their dependence on \(S,A\) (noting that the target optimal policies are different in the two cases). It implies that RL with impaired observability are provably as efficient as RL with full observability (up to poly factors of \(H\)).

Related work.Efficient algorithms for learning in the standard setting of tabular MDPs without impaired observability has been extensively studied Kearns and Singh [2002], Brafman and Tennenholtz [2002], Jaksch et al. [2010], Dann and Brunskill [2015], Azar et al. [2017], Agrawal and Jia [2017],

Figure 1: Reinforcement learning with impaired observability. At time \(h\), the agent only observes the past state \(s_{h-d}\) and actions \(a_{h-d},\ldots,a_{h-1}\). The policy depends on the observed information.

Jin et al. (2018); Dann et al. (2019); Zanette and Brunskill (2019); Zhang et al. (2020); Domingues et al. (2021), where the minimax optimal regret is \(\widetilde{\mathcal{O}}(\sqrt{H^{3}SAK})\)Azar et al. (2017); Domingues et al. (2021).

The delayed observation studied in this paper is related to delayed feedback in Howson et al. (2023); Yang et al. (2023), yet the setup is fundamentally different. In delayed feedback, an agent sends a policy to the environment for execution. The environment executes the policy on behalf of the agent for an episode, but the whole trajectory will be returned to the agent after some episodes. The policy executed by the environment is able to "see" instant state and reward. It is Markov and not played by the agent. Our setting concerns learning executable policies when delayed or missing states appear within an episode. The policy is no longer Markov and can only prescribe action based on history. Therefore, the algorithms and analyses for delayed feedback MDPs are not applicable to our settings.

Despite the distinct settings, there are existing fruitful results in efficiently learning MDPs or bandits with delayed feedback. Stochastic delayed feedback in bandits is studied in Agarwal and Duchi (2011); Dudik et al. (2011); Joulani et al. (2013); Vernade et al. (2017, 2020); Gael et al. (2020); Lancewicki et al. (2021). In the more challenging setting of reinforcement learning, Howson et al. (2023) considers tabular MDPs and Yang et al. (2023) generalizes to MDPs with function approximation and multi-agent settings.

On the other hand, results analyzing MDPs with missing observations are limited in literature, although missing data is a commonly recognized issue in applications Garcia-Laencina et al. (2010); Jerez et al. (2010); Little et al. (2012); Emmanuel et al. (2021). One notable result is Bouneffouf et al. (2020) for bandits with missing rewards.

**Notation**: For real numbers \(a,b\), we denote \(a\wedge b=\min\{a,b\}\). In episodic MDPs, we use the superscript \(k\) to denote the index of episodes, and the subscript \(h\) to denote the index of time. We denote \(\mathbf{a}_{i:j}=\{a_{i},\dots,a_{j}\}\) as the collection of actions from time \(i\) to \(j\). For two probability distributions \(\mu\) and \(\nu\), we denote their total variation distance as \(\|\mu-\nu\|_{\mathrm{TV}}\).

**MDP preliminary**: An episodic MDP is described by a tuple \((\mathcal{S},\mathcal{A},H,R,P)\), where \(\mathcal{S},\mathcal{A}\) are state and action spaces, respectively, \(H\) is the horizon, \(R=\{r_{h}\}_{h=1}^{H}\) is the reward function and \(P=\{p_{h}\}_{h=1}^{H}\) is the transition probability. We primarily focus on tabular MDPs, where \(S=|\mathcal{S}|\) and \(A=|\mathcal{A}|\) are both finite. We also assume that the reward is uniformly bounded with \(\|r_{h}\|_{\infty}\leq 1\) for any \(h\). An agent will interact with the environment for \(K\) episodes, hoping to find a good policy to maximize the cumulative reward. Within an episode, at the \(h\)-th step, the agent chooses an action based on the available information of the environment. After taking the action, the underlying environment produces a reward and transits to the next state. With full state observation, a policy \(\pi\) maps instant state \(s\) to an action \(a\) or an action distribution. Given such a policy \(\pi\), the value function is \(V_{h}^{\pi}(s_{1})=\mathbb{E}^{\pi}\left[\sum_{h^{\prime}=h}^{H}r_{h}(s_{h^{ \prime}},a_{h^{\prime}})\middle|s_{h}\right],\) where \(\mathbb{E}^{\pi}\) is the policy induced expectation.

## 2 Problem formulation

In this work, we study MDPs with impaired observability. We focus on two practical settings: 1) delayed observations and 2) missing observations.

### MDP with delayed observations

In any episode, we denote \(d_{h}\in\{0,1,\dots\}\) as the observational delay of the state and reward at step \(h\). That is, we receive \(s_{h}\) and \(r_{h}\) at time \(h+d_{h}\). The delay time \(d_{h}\) can be dependent on the state \(s_{h}\) and action \(a_{h}\) at time \(h\). To facilitate analysis, we denote the inter-arrival time between the arrival of observations for step \(h\) and \(h+1\) as \(\Delta_{h}=d_{h+1}-d_{h}\). With delays, at time \(h\), the nearest observable state is denoted as \(s_{t_{h}}\), where \(t_{h}=\operatorname*{argmax}\ \left\{I:\sum_{i=0}^{I}\Delta_{i}\leq h\right\}\). Then the executable policy class

\[\Pi_{\mathrm{exec}}=\{\pi_{h}(\cdot|s_{t_{h}},\mathbf{a}_{t_{h}:h-1})\text{ for }h=1,\dots,H\}\]

chooses actions depending on the nearest visible state and history actions. We impose the following assumption on the interarrival time.

**Assumption 2.1**. The interarrival time \(\Delta_{h}\) takes value in \(\{0,1,\dots\}\). The distribution \(\mathcal{D}_{h}(s_{h},a_{h})\) of \(\Delta_{h}\) can depend on \((s_{h},a_{h})\), but is conditionally independent of the MDP transitions given \((s_{h},a_{h})\).

Assumption 2.1 does not impose any specific distributional assumption on \(\Delta_{h}\), but only requires that the delayed observations arrive in order and at each time step, there is at most one new visible state and reward pair (\(\Delta_{h}\geq 0\)). A widely studied example of delays in literature is that the inter-arrival time is geometrically distributed Winsten (1959). Then the observation sequence \(\{h+d_{h}\}\) is known as a Bernoulli process, which is understood as the discretized version of a Poisson process.

Our delayed observation setting is newly proposed and substantially generalizes the Constant Delayed MDPs (CMDDPs) studied in Brooks and Leondes (1972); Bander and White III (1999); Katsikopoulos and Engelbrecht (2003); Walsh et al. (2007). When \(\Delta_{h}=0\) being deterministic for all \(h\geq 1\) and \(k\), our observation delay coincides with CDMDPs. In CDMDPs, a new past observation is guaranteed to arrive at each time step. However, our delayed model can result in no new observation at some time steps.

Observation delay leads to difficulty in planning, as the agent can only infer the current state and then choose an action. Therefore, the policy is naturally history dependent. We summarize the interaction protocol of the agent with the environment in Protocol 1.

```
1:for episode \(k=1,\dots,K\)do
2:for time \(h=1,\dots,H\)do
3: The agent observes a pair of new, if any, state and reward \((s^{k}_{t_{h}},a^{k}_{t_{h}})\). By memory, the agent also has access to past actions \(\mathbf{a}^{k}_{t_{h}:h-1}\).
4: The agent plays action \(a^{k}_{h}\) according to some executable policy \(\pi^{k}_{h}\in\Pi_{\mathrm{exec}}\).
5: The environment transits to next state \(s^{k}_{h+1}\sim p_{h}(\cdot|s^{k}_{h},a^{k}_{h})\), which is unobservable to the agent. The environment also decides the delay at step \(h+1\) as \(d^{k}_{h+1}=d^{k}_{h}+\Delta^{k}_{h}\) and \(t^{k}_{h+1}\).
6:endfor
7: The environment sends all unobserved pairs of state and reward as well as their corresponding delay time to the agent.
8:endfor ```

**Protocol 1** Interaction between the agent and the environment with delayed observations

All delayed observations, however, these observations are not used in planning. In reality, the agent can collect these observations by waiting after time \(H\). Protocol 1 is similar to hindsight observability in POMDPs studied in Lee et al. (2023). Yet their analysis for POMDPs is not directly transferable to our settings as mentioned in the introduction.

### MDP with missing observations

In addition to the stochastic delay in observations, we also consider randomly missing observations. In applications, an agent interacts with the environment through some communication channel. The communication channel is often imperfect and thus, observation can be lost during transmission. This type of missing is permanent and we describe in the following assumption.

**Assumption 2.2**.: Any pair of observation (state and reward) is independently observable in the communication channel. The observation rate is \(\lambda_{h}\) depending on \(h\), but independent of the MDP transitions. Moreover, there exists a constant \(\lambda_{0}\) such that \(\lambda_{h}\geq\lambda_{0}\) for any \(h\). The agent will be informed when an observation is missing.

Equivalently, the missing observation rate in Assumption 2.2 is \(1-\lambda_{h}\) and assumes the upper bound of \(1-\lambda_{0}\). We will show later that this missing rate directly influences the learning efficiency in Section 5.

## 3 Construction of augmented MDPs

To tackle the limited observability, we expand the original state space and define an augmented MDP. It will serve as the basis for our subsequent theoretical analysis.

### Augmented MDPs with expected reward

In the remainder of this section, we focus on the delayed observation case and defer the missing case to Section 5. Define \(\tau_{h}=\{s_{t_{h}},\mathbf{a}_{t_{h},h-1},\delta_{t_{h}}\}\) as the augmented state, where \(\delta_{t_{h}}\in[0,\Delta_{t_{h}}]\) is the delayed steps after observing \((s_{t_{h}},r_{t_{h}})\). Let \(\mathcal{S}_{\mathrm{aug}}\) denote the augmented state space of all possible \(\tau\)'s. Then the original MDP with delayed observations can be reformulated into a state-augmented one \(\texttt{MDP}_{\mathrm{aug}}=(\mathcal{S}_{\mathrm{aug}},\mathcal{A},H,R_{ \mathrm{aug}},P_{\mathrm{aug}})\). The reward is defined as

\[r_{h,\mathrm{aug}}(\tau_{h},a_{h})=\mathbb{E}\left[r_{h}(s_{h},a_{h})|\tau_{h}, a_{h}\right],\]

which is the expected reward given the nearest past state \(s_{t_{h}}\) and history actions \(\mathbf{a}_{t_{h}:h}\). We can define belief distribution \(\mathfrak{b}_{h}(s|\tau_{h})=\mathbb{P}(s_{h}=s|\tau_{h})\). Then \(r_{h,\text{aug}}(\tau_{h},a_{h})=\mathbb{E}_{s\sim\mathfrak{b}_{h}(\cdot| \tau_{h})}[r(s,a_{h})]\). Belief distributions are widely adopted in partially observed MDPs Ross et al. (2007), Poupart and Vlassis (2008). We will frequently use the belief distribution to study the expressivity of \(\Pi_{\mathrm{exec}}\) in Section 4.2.

The transition probabilities \(P_{\mathrm{aug}}\) are sparse. For any \(\tau_{h}=\{s_{t_{h}},\mathbf{a}_{t_{h}:h-1},\delta_{t_{h}}\}\) and \(\tau_{h+1}=\{s_{t_{h+1}},\mathbf{a}_{t_{h+1}:h},\delta_{t_{h+1}}\}\), we have

\[\begin{array}{c|c}\hline\text{ }\frac{p_{h,\text{aug}}(\tau_{h+1}|\tau_{h},a_{h}) }{\texttt{M}_{a}(\tau_{h},\tau_{h+1})\theta_{\mathrm{delay}}(s_{t_{h}},a_{t_{ h}},\delta_{t_{h}})p_{t_{h}}(s_{t_{h+1}}|s_{t_{h}},a_{t_{h}})}&\text{if }\delta_{t_{h+1}}=0\text{ \ and \ }t_{h+1}=t_{h}+1\\ \texttt{M}_{a}(\tau_{h},\tau_{h+1})(1-\theta_{\mathrm{delay}}(s_{t_{h}},a_{t_{ h}},\delta_{t_{h}}))&\text{if }\delta_{t_{h+1}}=\delta_{t_{h}}+1\text{ and }t_{h+1}=t_{h}\\ 0&\text{otherwise}\end{array}\]

where \(\texttt{M}_{a}(\tau_{h},\tau_{h+1})\) indicates whether the rolling actions are matched, i.e.,

\[\texttt{M}_{a}(\tau_{h},\tau_{h+1})=\mathds{1}\{\mathbf{a}_{t_{h}:h-1}= \mathbf{a}_{t_{h+1}:h-1}\},\]

and \(\theta_{\mathrm{delay}}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}})\) is defined as

\[\theta_{\mathrm{delay}}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}})=\mathbb{P}(\Delta_ {t_{h}}=\delta_{t_{h}}|s_{t_{h}},a_{t_{h}},\delta_{t_{h}})=\frac{\mathbb{P}( \Delta_{t_{h}}=\delta_{t_{h}}|s_{t_{h}},a_{t_{h}})}{1-\sum_{\delta<\delta_{t_{h }}}\mathbb{P}(\Delta_{t_{h}}=\delta|s_{t_{h}},a_{t_{h}})}.\]

The factored form of \(\theta_{\mathrm{delay}}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}})p_{t_{h}}(s_{t_{h+1 }}|s_{t_{h}},a_{t_{h}})\) follows from the conditional independence in Assumption 2.1. We define \(Q\)-functions and value functions as follows. For any \(\tau_{h},a_{h}\) and policy \(\pi\in\Pi_{\mathrm{exec}}\), we have

\[Q^{\pi}_{h,\text{aug}}(\tau_{h},a_{h}) =\mathbb{E}^{\pi}\left[\sum_{l^{\prime}=h}^{H}r_{h,\text{aug}}( \tau_{h^{\prime}},a_{h^{\prime}})\Big{|}\tau_{h},a_{h}\right]\quad\text{and}\] \[V^{\pi}_{h,\text{aug}}(\tau_{h}) =\left\langle Q^{\pi}_{h,\text{aug}}(\tau_{h},\cdot),\pi_{h}(\cdot |\tau_{h})\right\rangle.\]

We note that \(V^{\pi}_{h}\) is equivalent to \(V^{\pi}_{h,\text{aug}}\) for the same executable policy \(\pi\in\Pi_{\mathrm{exec}}\). We also denote \(\mathcal{P}_{h,\text{aug}}\) as the transition operator corresponding to \(P_{\mathrm{aug}}\). It can be checked that

\[Q^{\pi}_{h,\text{aug}}(\tau_{h},a_{h})=r_{h,\text{aug}}(\tau_{h},a_{h})+[ \mathcal{P}_{h,\text{aug}}V^{\pi}_{h,\text{aug}}](\tau_{h},a_{h}).\]

\(\texttt{MDP}_{\mathrm{aug}}\) also appears in makes all the policies in \(\Pi_{\mathrm{exec}}\) executable and Markov. Meanwhile, the reward function keeps track of all the expected reward for \(H\) steps. Although the expanded state space \(\mathcal{S}_{\mathrm{aug}}\) is much more complicated than the original state space \(\mathcal{S}\), the sparse structures in the transition probabilities still allow an efficient exploration. We note that \(p_{h,\text{aug}}\) only depends on the delay distribution and one-step Markov transitions. However, there is still one caveat for learning in \(\texttt{MDP}_{\mathrm{aug}}\) - the reward function depends belief distributions, which involve multi-step transitions.

### Augmented MDPs with past reward

To tackle the aforementioned challenge, we further define \(\overline{\texttt{MDP}}_{\mathrm{aug}}=(\widetilde{\mathcal{S}}_{\mathrm{aug} },\mathcal{A},\widetilde{H},\widetilde{R}_{\mathrm{aug}},\widetilde{P}_{ \mathrm{aug}})\) that shares the optimal policy in \(\texttt{MDP}_{\mathrm{aug}}\) with an enlonged horizon \(\widetilde{H}=2H\). The state space \(\widetilde{\mathcal{S}}_{\mathrm{aug}}\) consists of any \(\tau_{h}=\{s_{t_{h}},\mathbf{a}_{t_{h}:h\wedge H},\delta_{t_{h}}\}\). Comparing to \(\mathcal{S}_{\mathrm{aug}}\), we cut off the action at horizon \(H\), since \(a_{h}\) for \(h>H\) has no influence on the state and reward in time \([1,H]\). The reward function is defined as

\[\widetilde{r}_{h,\text{aug}}(\tau_{h},a_{h})=r_{t_{h}}(s_{t_{h}},a_{t_{h}}) \mathds{1}\{\delta_{t_{h}}=0\}\mathds{1}\{t_{h}\in\{1,\dots,H\}\}.\]By definition, \(\widetilde{r}_{\mathrm{aug}}(\tau_{h},a_{h})\) is a past reward. More importantly, \(\widetilde{r}_{h,\text{aug}}(\tau_{h},a_{h})\) zeros out rewards outside the original horizon \(H\). Meanwhile, between the arrival of two consecutive state observations, the reward only counts once. Lastly, the transition probabilities are

\begin{tabular}{c|c} \hline \(\widetilde{p}_{h,\text{aug}}(\tau_{h+1}|\tau_{h},a_{h})\) & Condition \\ \hline \(\texttt{M}_{a}(\tau_{h},\tau_{h+1})\theta_{\mathrm{delay}}(s_{t_{h}},a_{t_{h}}, \delta_{t_{h}})p_{t_{h}}(s_{t_{h+1}}|s_{t_{h}},a_{t_{h}})\) & if \(\delta_{t_{h+1}}=0,t_{h+1}=t_{h}+1\) and \(h<H\) \\ \(\texttt{M}_{a}(\tau_{h},\tau_{h+1})(1-\theta_{\mathrm{delay}}(s_{t_{h}},a_{t_{ h}},\delta_{t_{h}}))\) & if \(\delta_{t_{h+1}}=\delta_{t_{h}}+1,t_{h+1}=t_{h}\) and \(h<H\) \\ \(\texttt{M}_{a}(\tau_{h},\tau_{h+1})p_{t_{h}}(s_{t_{h}+1}|s_{t_{h}},a_{t_{h}})\) & if \(\delta_{t_{h+1}}=0,t_{h+1}=t_{h}+1\) and \(h>H\) \\  & otherwise \\ \hline \end{tabular}

We interpret the transitions as follows. When \(h\leq H\), the transition is the same as \(\texttt{MDP}_{\mathrm{aug}}\). When \(h>H\), we simply wait for unobserved states and rewards to come. As mentioned, actions taken beyond time \(H\) are irrelevant. We build an equivalence in the expected values of \(\texttt{MDP}_{\mathrm{aug}}\) and \(\widetilde{\texttt{MDP}}_{\mathrm{aug}}\).

**Proposition 3.1**.: Let \(\texttt{MDP}_{\mathrm{aug}}\) and \(\widetilde{\texttt{MDP}}_{\mathrm{aug}}\) be defined as in the previous paragraphs. Then for any initial state \(\tau_{1}\) and any policy \(\pi=\{\pi_{h}\}_{h=1}^{H}\in\Pi_{\mathrm{exec}}\), it holds that

\[\mathbb{E}^{\pi}\left[\sum_{h=1}^{H}r_{h,\text{aug}}(\tau_{h},a_{h})\Big{|} \tau_{1}\right]=\mathbb{E}^{\pi}\left[\sum_{h=1}^{\bar{H}}\widetilde{r}_{h, \text{aug}}(\tau_{h},a_{h})\Big{|}\tau_{1}\right],\]

where in the right-hand side, the policy for steps \(H+1\) to \(\widetilde{H}\) is arbitrary.

The proof is provided in Appendix A.1. Proposition 3.1 implies that learning in \(\texttt{MDP}_{\mathrm{aug}}\) until time \(H\) is equivalent to that in \(\widetilde{\texttt{MDP}}_{\mathrm{aug}}\) for \(\widetilde{H}\) steps.

## 4 RL with delayed observations and regret bound

In this section, we provide regret analysis of learning in MDPs with stochastic delays. For the sake of simplicity, we assume the reward is known, however, extension to unknown reward causes no real difficulty. Motivated by the augmented MDP reformulation, we introduce our learning algorithm in Algorithm 2. In Line 5, unobserved states and rewards are returned to the agent as described in Protocol 1. Using the data set, we construct bonus functions compensating the uncertainty in _one-step_ transitions of the original MDP. This largely sharpens the confidence region, yet still ensures a valid optimism. We emphasize that in Line 9, we are planning on \(\widetilde{\texttt{MDP}}_{\mathrm{aug}}\) involving the augmented transitions and expanded states of \(\tau\in\widetilde{\mathcal{S}}_{\mathrm{aug}}\). Only in this way, we can obtain an executable policy in delayed MDPs. The planning complexity is \(SA^{H}\) though.

### Regret bound

We define regret in delayed MDP as

\[\texttt{Regret}(K)=\sum_{k=1}^{K}\max_{\pi\in\Pi_{\mathrm{exec}}}V_{1}^{\pi}( s_{1}^{k})-\sum_{k=1}^{K}V_{1}^{\pi_{k}}(s_{1}^{k}),\]

where \(V_{1}^{\pi}\) is the value function of the original MDP. Although the regret here is defined on the original MDP, it is equivalent to the regret of the same policy on \(\texttt{MDP}_{\mathrm{aug}}\) and further \(\widetilde{\texttt{MDP}}_{\mathrm{aug}}\) by Proposition 3.1. Note that we are comparing with the best executable policy. The performance degradation caused by observation delay is discussed in Section 4.2. The following theorem bounds the regret.

**Theorem 4.1** (Regret bound for Delayed MDP).: Suppose Assumption 2.1 holds. Let \(\gamma\in(0,1)\) be any failure probability. With probaiblity \(1-\gamma\), the regret of Algorithm 2 satisfies

\[\texttt{Regret}(K)\leq c\left(H^{4}\sqrt{SAK\iota}+H^{4}S^{2}At^{2}\right),\]

where \(\iota=\log\frac{SAHK}{\gamma}\) and \(c\) is a constant.

The proof is provided in Appendix B.1. We discuss several implications.

Sharp dependence on \(S\) and \(A\)Theorem 4.1 has a sharp dependence on \(S\) and \(A\), although the expanded state space \(\widetilde{\mathcal{S}}_{\mathrm{aug}}\) has a cardinality bounded by \(SA^{H}\). Naively learning and planning in \(\widehat{\mathsf{MDP}}_{\mathrm{aug}}\) would suffer from the exponential enlargement of \(A^{H}\). However, we identify the sparse structures in the transition probabilities. As can be seen, \(\widetilde{p}_{h,\mathrm{aug}}\) only involves one-step transitions in the original MDP and some conditionally independent delay distributions. Such structures lead to a rather easy estimation of \(\widetilde{p}_{h,\mathrm{aug}}\), which can be constructed from the estimators of one-step transitions in the original MDP. Meanwhile, the sparse structures make exploration in \(\widehat{\mathsf{MDP}}_{\mathrm{aug}}\) efficient, due to many unreachable states.

Effect of the delay distribution and delay lengthTheorem 4.1 holds for arbitrary conditionally independent delay distributions, even include heavy-tailed distributions. In the worst case of unbounded delays, Theorem 4.1 gives rise to a \(\mathcal{O}(H^{4}\sqrt{SAK\iota})\) regret. The reason to this is that if the delay is larger than \(H\), then the corresponding state will only be observed after an episode ends and won't be used in planning. Therefore, we can truncate the delay at \(H\), regardless of its tail distributions.

When the maximal length of delay is bounded by \(D<H\), e.g., CDMDPs with \(d_{h}=D\) for any \(h\), Theorem 4.1 implies that the regret is bounded by

\[\mathtt{Regret}(K)\leq c\left((D+1)^{5/2}\sqrt{H^{3}SAK\iota}+H^{4}S^{2}A\iota ^{2}\right)\]

for a constant \(c\). A proof is provided in Appendix B.2. As can be seen, as the length of delay increases, the regret bound enlarges, reflecting the increased difficulty of long delays. Moreover, when \(D=0\), that is, no observation delays, the regret bound recovers that in standard MDPs.

### Performance degradation of policy class \(\Pi_{\mathrm{exec}}\)

This section devotes to quantify the performance degradation caused by delayed observations. In particular, we bound the value difference between the best executable policy and the best Markov policy in a no delay environment. Recall that \(V_{1}\) is the value function of the original MDP. We denote

\[\pi^{*}_{\mathrm{nodelay}}=\operatorname*{argmax}_{\pi}V_{1}^{\pi}(s_{1}) \quad\text{and}\quad\pi^{*}_{\mathrm{delay}}=\operatorname*{argmax}_{\pi\in \Pi_{\mathrm{exec}}}V_{1}^{\pi}(s_{1})\]as the best vanilla optimal policy and executable policy, respectively. The values achieved by \(\pi^{*}_{\mathrm{nodelay}}\) and \(\pi^{*}_{\mathrm{delay}}\) are denoted as \(V^{*}_{1,\mathrm{nodelay}}(s_{1})\) and \(V^{*}_{1,\mathrm{delay}}(s_{1})\), respectively. The gap between \(V^{*}_{1,\mathrm{nodelay}}\) and \(V^{*}_{1,\mathrm{delay}}\) quantifies the performance degradation, which is denoted as \(\mathsf{gap}(s_{1})=V^{*}_{1,\mathrm{nodelay}}(s_{1})-V^{*}_{1,\mathrm{delay}}( s_{1})\). We bound \(\mathsf{gap}\) in Proposition B.2 in Appendix due to space limit.

In a nutshell, we show that the performance degradation \(\mathsf{gap}\) is highly relevant to the belief distribution \(\mathfrak{b}_{h}(\cdot|\tau)\). When \(\mathfrak{b}_{h}(\cdot|\tau)\) is evenly spread, meaning that the entropy of \(\mathfrak{b}_{h}\) is high and inferring the current unseen state is difficult, we potentially suffer from a large \(\mathsf{gap}\). On the contrary, when \(\mathfrak{b}_{h}(\cdot|\tau)\) is nearly deterministic, the performance degradation is small. In the special case of deterministic transitions, we have \(\mathsf{gap}=0\).

### The (mysterious) effect of delay on the optimal value

To further understand the effect of the delay on the optimal value, we provide the following dichotomy. On the one hand, we show that there exists an MDP instance, such that a constant delay of \(d\) steps does not hurt the performance. On the other hand, in the same MDP instance, a constant delay of \(d+1\) steps suffers from a constant performance drop.

**Proposition 4.2**.: Consider constant delayed MDPs. Fix a positive integer \(d<H\). Then there exists an MDP instance such that the following two items hold simultaneously.

\(\bullet\) When delay is \(d\), it holds that \(\frac{1}{K}\sum_{k=1}^{K}\mathsf{gap}(s_{1}^{k})=0\).

\(\bullet\) When delay is \(d+1\), it holds that \(\frac{1}{K}\sum_{k=1}^{K}\mathsf{gap}(s_{1}^{k})\geq\frac{1}{2}-\sqrt{\frac{1}{ 2K}\log\frac{1}{\gamma}}\), with probability \(1-\gamma\).

The proof is provided in Appendix B.4. We remark that Proposition 4.2 says that observation delay can be dangerous, even with the slightest possible number of steps. The idea behind Proposition 4.2 is consistent with the analysis on \(\mathsf{gap}\). In particular, we construct an MDP instance demonstrated in Figure 2. The reward vanishes at all times but \(d+1\). When delay is \(d\), the initial state \(s_{1}\) is revealed and the policy can choose the best action to receive a reward. When delay is \(d+1\), however, there is always a \(1/2\) probability of missing the best action for any policy, which leads to a constant performance degradation.

## 5 RL with missing observations and regret analysis

We now switch our study to MDPs with missing observations. In such an environment, executable policies share the same structures as delayed MDPs, where an action is taken based on available history information. Compared to delayed observations, learning with missing observations is more challenging. Since unobserved states and rewards are never revealed, we are suffering from information loss. Besides, we will frequently deal with multi-step transitions, due to missing observations between two consecutive visible states.

### Optimistic planning with missing observations

Despite the difficulty, we present here algorithms that are efficient in learning and planning for MDPs with missing observations. We begin with an optimistic planning algorithm in Algorithm 3. To unify the notation, we denote \(s_{h}^{k}=\emptyset\) and \(r_{h}^{k}=\emptyset\) as missing the observation.

Figure 2: MDP instance on two states with two actions. The transition is lazy until time \(d\). Then the transition is uniform regardless of actions for time \(d+1\). Reward is nonzero only at time \(d+1\). This is an example with a delay of length \(d\) causes no degradation and a delay of \(d+1\) causes a constant performance degradation.

The majority of the algorithm resembles the typical optimistic planning Jaksch et al. (2010) but with some notable differences. In Line 4, the value function \(V_{1,\theta}\) is for the original MDP with transition probabilities parameterized by \(\theta\). Different from the typical optimistic planning, the underlying MDP here obeys the stochastic observable model in Assumption 2.2. Therefore, the value \(V_{1,\theta}\) is the sum of all possible values under missing observations. When counting \(N_{h}^{k}(s,a)\) in Line 6, we exclude data tuples missing the next state, which inevitably slows down the learning curve. Nonetheless, the effect of missing only contributes as a scaling factor in the regret.

**Proposition 5.1**.: Suppose Assumption 2.2 holds with \(\lambda_{h}\) known. Given a failure probability \(\gamma\), with probability \(1-\gamma\), the regret of Algorithm 4 satisfies

\[\texttt{Regret}(K)\leq c\left(\left\lceil\frac{1}{-\log(1-\lambda_{0}^{2})} \right\rceil\sqrt{H^{3}S^{2}AK\iota^{3}}+\sqrt{H^{4}K\iota}\right),\]

where \(\iota=\log\frac{SAHK}{\gamma}\) and \(c\) is a constant.

The proof is provided in Appendix C.1. Proposition 5.1 is optimal in the \(K\) dependence and achieves an \(S^{2}A\) dependence on the complexity of the underlying MDP. In the extreme case of \(\lambda_{0}\approx 0\), which implies that every state and reward are hardly observable, we have \(\texttt{Regret}(K)=\widetilde{\mathcal{O}}\left(\frac{1}{\lambda_{0}^{2}} \sqrt{H^{3}S^{2}AK}\right)\). Here \(\lambda_{0}^{2}\) is the probability of observing two consecutive states for estimating the transition probabilities. Proposition 5.1 requires knowledge of observable rate \(\lambda_{h}\). This is not a restrictive condition, as estimating \(\lambda_{h}\) from Bernoulli random variables is much easier than estimating transition probabilities.

### Model-based planning using augmented MDPs

Proposition 5.1 has a lenient dependence on the missing rate \(1-\lambda_{0}^{2}\), nonetheless, is not sharp on the dependence of \(S\). We next show that the augmented MDP approach is effective to tackle missing observations, when the observable rate satisfies additional conditions. Specifically, we assume that the observable rate \(\lambda_{h}\) is independent of \((s,a)\). We utilize the \(\texttt{MDP}_{\text{aug}}\) reformulation, except that we redefine the transition probabilities as

\[p_{h,\text{aug}}(\tau_{h+1}|\tau_{h},a_{h})=\begin{cases}\lambda_{h}p_{h}(s_{ h+1}|s_{t_{h}},\mathbf{a}_{t_{h}:h})&\text{if }t_{h+1}=h+1\\ \mathsf{M}_{\text{a}}(\tau_{h+1},\tau_{h})(1-\lambda_{h})&\text{if }t_{h+1}=t_{h}\\ 0&\text{otherwise}\end{cases}.\]

The first case in \(p_{h,\text{aug}}\) corresponds to receiving the state observation at time \(h+1\). In contrast to the delayed MDPs, the transition probabilities here potentially rely on multi-step transitions in the original MDP. The second case of the transition corresponds to missing the observation. We summarize the policy learning procedure in Algorithm 4 in Appendix C.2, which is similar to Algorithm 2, but with a new bonus function. The following theorem shows that Algorithm 4 is asymptotically efficient when the observable rate is relatively high.

**Theorem 5.2**.: Suppose Assumption 2.2 holds with \(\lambda_{0}\geq 1-A^{-(1+v)}\) for some positive constant \(v\). Given a failure probability \(\gamma\), with probability \(1-\gamma\), the regret of Algorithm 4 satisfies

\[\mathtt{Regret}(K)\leq c\left(H^{4}\sqrt{SAK\iota^{3}}+S^{2}\sqrt{H^{9}K^{\frac {1}{(1+v)}}}\iota^{6}\right),\]

where \(\iota=\log\frac{SAHK}{\gamma}\) and \(c\) is a constant.

The proof is provided in Appendix C.2. Some remarks are in order.

\(Sa\) **rate when \(K\) is large**When the number of episodes \(K\geq S^{3(1+v)/v}\), the first term \(H^{4}\sqrt{SAK\iota^{3}}\) in the regret bound dominates and attains a sharp dependence on \(S\) and \(A\). However, when the number of episodes are limited, the regret bound has a worse dependence on the state space size \(S\). We also observe that as the missing rate \(\lambda\) becomes small (equivalently, \(v\) becomes large), the regret is close to \(\widetilde{O}(H^{4}\sqrt{SAK\iota^{3}})\).

Observable rate smaller than \(1-1/A\)Theorem 5.2 holds for an observable rate \(\lambda_{0}>1-1/A\). The intuition behind is that to fully explore all the actions when a state observation is missing takes \(A\) trials. Therefore, in expectation, we will encounter a missing observation at least every \(A\) episodes as long as \(\lambda_{0}>1-1/A\). Nonetheless, when \(\lambda_{0}\leq 1-1/A\), the regret bound remains curiously underexplored. We conjecture that \(\lambda_{0}=1-1/A\) is a critical point distinguishes unique strategies for learning and planning in MDPs with missing observations. A detailed analysis goes beyond the scope of the current paper.

Proof sketchThe proof of Theorem 5.2 adapts the analysis of model-based UCBVI algorithms Azar et al. (2017). Let \(m\) denote the maximal length of consecutive missing observations. We denote \(\mathcal{E}_{m}\) as the event when the maximal length of consecutive missing is less than \(m\). On event \(\mathcal{E}_{m}\), a naive analysis leads to a \(\widetilde{\mathcal{O}}\left(\sqrt{\mathrm{poly}(H)SA^{m+1}K}\right)\) regret, in observation to the size of the expanded state space \(\mathcal{S}_{\mathrm{aug}}\). However, our analysis circumvents the \(A^{m}\) dependence by exploiting the occurrence of consecutive missing observations is rare (Lemma C.3). On the complement of event, the regret is bounded by \(KH(1-\mathbb{P}(\mathcal{E}_{m}))\). Summing up the two parts and choosing a proper \(m\) yield our result.

## 6 Conclusion

In this paper, we have studied learning and planning in impaired observability MDPs. We focus on MDPs with delayed and missing observations. Specifically, for delayed observations, we have shown an efficient \(\widetilde{O}(H^{4}\sqrt{SAK})\) regret. For missing observations, we have provided an optimistic planning algorithm achieving an \(\widetilde{O}(\sqrt{H^{3}S^{2}AK})\) regret. If the missing rate is relatively small, we have shown an efficient \(\widetilde{O}(H^{4}\sqrt{SAK})\) regret bound. Further, we have characterized the performance drop caused by impaired observability compared to full observability.

## Acknowledgement

The work of H. V. Poor was supported in part by a grant from the C3.ai Digital Transformation Institute. Mengdi Wang acknowledges the support by NSF grants DMS-1953686, IIS-2107304, CMMI-1653435, CPS-2312093, ONR grant 1006977, and C3.AI.

## References

* Agarwal and Duchi [2011] Alekh Agarwal and John C Duchi. Distributed delayed stochastic optimization. _Advances in Neural Information Processing Systems_, 24, 2011.
* Agarwal and Aggarwal [2021] Mridul Agarwal and Vaneet Aggarwal. Blind decision making: Reinforcement learning with delayed observations. _Pattern Recognition Letters_, 150:176-182, 2021.
* Agrawal and Jia [2017] Shipra Agrawal and Randy Jia. Optimistic posterior sampling for reinforcement learning: worst-case regret bounds. _Advances in Neural Information Processing Systems_, 30, 2017.
* Barthelemy et al. [2017]Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _Proceedings of the International Conference on Machine Learning_, pages 263-272. PMLR, 2017.
* Bander and White III (1999) James L Bander and Chelsea C White III. Markov decision processes with noise-corrupted and delayed state observations. _Journal of the Operational Research Society_, 50(6):660-668, 1999.
* Bertsekas (2012) Dimitri Bertsekas. _Dynamic Programming and Optimal Control: Volume I_, volume 1. Athena Scientific, 2012.
* Bouneffouf et al. (2020) Djallel Bouneffouf, Sohini Upadhyay, and Yasaman Khazaeni. Contextual bandit with missing rewards. arXiv preprint arXiv:2007.06368, 2020.
* Brafman and Tennenholtz (2002) Ronen I Brafman and Moshe Tennenholtz. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. _Journal of Machine Learning Research_, 3(Oct):213-231, 2002.
* Brooks and Leondes (1972) DM Brooks and Cornelius T Leondes. Markov decision processes with state-information lag. _Operations Research_, 20(4):904-907, 1972.
* Chen et al. (2022) Fan Chen, Yu Bai, and Song Mei. Partially observable RL with B-stability: Unified structural condition and sharp sample-efficient algorithms. arXiv preprint arXiv:2209.14990, 2022.
* Chen et al. (2023) Fan Chen, Huan Wang, Caiming Xiong, Song Mei, and Yu Bai. Lower bounds for learning in revealing POMDPs. arXiv preprint arXiv:2302.01333, 2023.
* Dann and Brunskill (2015) Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement learning. _Advances in Neural Information Processing Systems_, 28, 2015.
* Dann et al. (2019) Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill. Policy certificates: Towards accountable reinforcement learning. In _Proceedings of the International Conference on Machine Learning_, pages 1507-1516. PMLR, 2019.
* Domingues et al. (2021) Omar Darwiche Domingues, Pierre Menard, Emilie Kaufmann, and Michal Valko. Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited. In _Algorithmic Learning Theory_, pages 578-598. PMLR, 2021.
* Dudik et al. (2011) Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang. Efficient optimal learning for contextual bandits. arXiv preprint arXiv:1106.2369, 2011.
* Emmanuel et al. (2021) Tlamelo Emmanuel, Thabiso Maupong, Dimane Mpoeleng, Thabo Semong, Banyatsang Mphago, and Oteng Tabona. A survey on missing data in machine learning. _Journal of Big Data_, 8(1):1-37, 2021.
* Gael et al. (2020) Manegueu Anne Gael, Claire Vernade, Alexandra Carpentier, and Michal Valko. Stochastic bandits with arm-dependent delays. In _Proceedings of the International Conference on Machine Learning_, pages 3348-3356. PMLR, 2020.
* Garcia-Laencina et al. (2010) Pedro J Garcia-Laencina, Jose-Luis Sancho-Gomez, and Anibal R Figueiras-Vidal. Pattern classification with missing data: a review. _Neural Computing and Applications_, 19:263-282, 2010.
* Howson et al. (2023) Benjamin Howson, Ciara Pike-Burke, and Sarah Filippi. Delayed feedback in generalised linear bandits revisited. In _International Conference on Artificial Intelligence and Statistics_, pages 6095-6119. PMLR, 2023.
* Jaksch et al. (2010) Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. _Journal of Machine Learning Research_, 11(51):1563-1600, 2010. URL http://jmlr.org/papers/v11/jaksch10a.html.
* Jerez et al. (2010) Jose M Jerez, Ignacio Molina, Pedro J Garcia-Laencina, Emilio Alba, Nuria Ribelles, Miguel Martin, and Leonardo Franco. Missing data imputation using statistical and machine learning methods in a real breast cancer problem. _Artificial Intelligence in Medicine_, 50(2):105-115, 2010.
* Jin et al. (2018) Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably efficient? _Advances in Neural Information Processing Systems_, 31, 2018.
* Jin et al. (2019)Chi Jin, Sham Kakade, Akshay Krishnamurthy, and Qinghua Liu. Sample-efficient reinforcement learning of undercomplete POMDPs. _Advances in Neural Information Processing Systems_, 33:18530-18539, 2020.
* Joulani et al. (2013) Pooria Joulani, Andras Gyorgy, and Csaba Szepesvari. Online learning under delayed feedback. In _Proceedings of the International Conference on Machine Learning_, pages 1453-1461. PMLR, 2013.
* Katsikopoulos & Engelbrecht (2003) Konstantinos V Katsikopoulos and Sascha E Engelbrecht. Markov decision processes with delays and asynchronous cost collection. _IEEE Transactions on Automatic Control_, 48(4):568-574, 2003.
* Kearns & Singh (2002) Michael Kearns and Satinder Singh. Near-optimal reinforcement learning in polynomial time. _Machine Learning_, 49(2):209-232, 2002.
* Krishnamurthy (2016) Vikram Krishnamurthy. _Partially Observed Markov Decision Processes_. Cambridge University Press, 2016.
* Lancewicki et al. (2021) Tal Lancewicki, Shahar Segal, Tomer Koren, and Yishay Mansour. Stochastic multi-armed bandits with unrestricted delay distributions. In _Proceedings of the International Conference on Machine Learning_, pages 5969-5978. PMLR, 2021.
* Lattimore & Szepesvari (2020) Tor Lattimore and Csaba Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020.
* Lee et al. (2023) Jonathan N Lee, Alekh Agarwal, Christoph Dann, and Tong Zhang. Learning in pomdps is sample-efficient with hindsight observability. arXiv preprint arXiv:2301.13857, 2023.
* Little et al. (2012) Roderick J Little, Ralph D'Agostino, Michael L Cohen, Kay Dickersin, Scott S Emerson, John T Farrar, Constantine Frangakis, Joseph W Hogan, Geert Molenberghs, Susan A Murphy, et al. The prevention and treatment of missing data in clinical trials. _New England Journal of Medicine_, 367(14):1355-1360, 2012.
* Liu et al. (2022) Qinghua Liu, Praneeth Netrapalli, Csaba Szepesvari, and Chi Jin. Optimistic MLE-A generic model-based algorithm for partially observable sequential decision making. arXiv preprint arXiv:2209.14997, 2022.
* Liu et al. (2014) Shichao Liu, Xiaoyu Wang, and Peter Xiaoping Liu. Impact of communication delays on secondary frequency control in an islanded microgrid. _IEEE Transactions on Industrial Electronics_, 62(4):2021-2031, 2014.
* Lizotte et al. (2008) Daniel J Lizotte, Lacey Gunter, Eric Laber, and Susan A Murphy. Missing data and uncertainty in batch reinforcement learning. In _Advances in Neural Information Processing Systems_, 2008.
* Papadimitriou & Tsitsiklis (1987) Christos H Papadimitriou and John N Tsitsiklis. The complexity of markov decision processes. _Mathematics of Operations Research_, 12(3):441-450, 1987.
* Poupart & Vlassis (2008) Pascal Poupart and Nikos Vlassis. Model-based bayesian reinforcement learning in partially observable domains. In _Proceedings of the International Symposium on Artificial Intelligence and Mathematics_, pages 1-2, 2008.
* Ross et al. (2007) Stephane Ross, Brahim Chaib-draa, and Joelle Pineau. Bayes-adaptive pomdps. _Advances in Neural Information Processing Systems_, 20, 2007.
* Smallwood & Sondik (1973) Richard D Smallwood and Edward J Sondik. The optimal control of partially observable markov processes over a finite horizon. _Operations Research_, 21(5):1071-1088, 1973.
* Sutton & Barto (2018) Richard S Sutton and Andrew G Barto. _Reinforcement Learning: An Introduction_. MIT Press, 2018.
* Uehara et al. (2022) Masatoshi Uehara, Ayush Sekhari, Jason D Lee, Nathan Kallus, and Wen Sun. Provably efficient reinforcement learning in partially observable dynamical systems. arXiv preprint arXiv:2206.12020, 2022.
* Vernade et al. (2017) Claire Vernade, Olivier Cappe, and Vianney Perchet. Stochastic bandit models for delayed conversions. arXiv preprint arXiv:1706.09186, 2017.
* Vernade et al. (2018)Claire Vernade, Alexandra Carpentier, Tor Lattimore, Giovanni Zappella, Beyza Ermis, and Michael Brueckner. Linear bandits with stochastic delayed feedback. In _Proceedings of the International Conference on Machine Learning_, pages 9712-9721. PMLR, 2020.
* Wainwright (2019) Martin J Wainwright. _High-dimensional Statistics: A Non-asymptotic Viewpoint_, volume 48. Cambridge University Press, 2019.
* Walsh et al. (2007) Thomas J Walsh, Ali Nouri, Lihong Li, and Michael L Littman. Planning and learning in environments with delayed feedback. In _Proceedings of Machine Learning: ECML 2007: 18th European Conference on Machine Learning_, Warsaw, Poland, September 17-21, 2007, pages 442-453. Springer, 2007.
* Weissman et al. (2003) Tsachy Weissman, Erik Ordentlich, Gadiel Seroussi, Sergio Verdu, and Marcelo J Weinberger. Inequalities for the l1 deviation of the empirical distribution. _Hewlett-Packard Labs, Technical Report_, 2003.
* Winsten (1959) CB Winsten. Geometric distributions in the theory of queues. _Journal of the Royal Statistical Society: Series B (Methodological)_, 21(1):1-22, 1959.
* Yang et al. (2023) Yunchang Yang, Han Zhong, Tianhao Wu, Bin Liu, Liwei Wang, and Simon S Du. A reduction-based framework for sequential decision making with delayed feedback. arXiv preprint arXiv:2302.01477, 2023.
* Zanette and Brunskill (2019) Andrea Zanette and Emma Brunskill. Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds. In _Proceedings of the International Conference on Machine Learning_, pages 7304-7312. PMLR, 2019.
* Zhan et al. (2022) Wenhao Zhan, Masatoshi Uehara, Wen Sun, and Jason D Lee. PAC reinforcement learning for predictive state representations. arXiv preprint arXiv:2207.05738, 2022.
* Zhang et al. (2020) Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via reference-advantage decomposition. _Advances in Neural Information Processing Systems_, 33:15198-15207, 2020.
* Zhong et al. (2022) Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and Tong Zhang. A posterior sampling framework for interactive decision making. arXiv preprint arXiv:2211.01962, 2022.

Omitted proof in Section 3

### Proof of Proposition 3.1

Proof.: Consider an arbitrary fixed inter-arrival pattern \(\Delta_{0},\Delta_{1},\ldots,\Delta_{H-1}\). We show that the expected accumulated rewards under this inter-arrival pattern are identical for \(\mathtt{MDP}_{\mathrm{aug}}\) and \(\widehat{\mathtt{MDP}}_{\mathrm{aug}}\). In \(\widehat{\mathtt{MDP}}_{\mathrm{aug}}\), we have

\[\mathbb{E}^{\pi}\left[\sum_{h=1}^{\bar{H}}\widetilde{r}_{h, \text{aug}}(\tau_{h},a_{h})\,\Big{|}\,\tau_{1},\Delta_{0},\ldots,\Delta_{H-1}\right]\] \[\overset{(i)}{=}\mathbb{E}^{\pi}\left[\sum_{h=1}^{\bar{H}} \widetilde{r}_{t_{h},\text{aug}}(s_{t_{h}},a_{t_{h}})\mathds{1}\{\delta_{t_{ h}}=0\}\mathds{1}\{t_{h}\in\{1,\ldots,H\}\}\,\Big{|}\,\tau_{1},\Delta_{0}, \ldots,\Delta_{H-1}\right]\] \[\overset{(ii)}{=}\mathbb{E}^{\pi}\left[\sum_{h=1}^{H}r(s_{h},a_{ h})\,\Big{|}\,\tau_{1},\Delta_{0},\ldots,\Delta_{H-1}\right]\] \[=\mathbb{E}^{\pi}\left[\sum_{h=1}^{H}r_{h,\text{aug}}(\tau_{h}, a_{h})\,\Big{|}\,\tau_{1},\Delta_{0},\ldots,\Delta_{H-1}\right],\]

where equality \((i)\) invokes the definition of \(\widetilde{r}_{h,\text{aug}}\) and equality \((ii)\) eliminates zero reward terms. Now taking expectation over all possible inter-arrival patterns, we deduce

\[\mathbb{E}^{\pi}\left[\sum_{h=1}^{\bar{H}}\widetilde{r}_{\text{aug}}(\tau_{h},a_{h})\,\Big{|}\,\tau_{1}\right]=\mathbb{E}^{\pi}\left[\sum_{h=1}^{H}r_{h, \text{aug}}(s_{h},a_{h})\,\Big{|}\,\tau_{1}\right].\]

The proof is complete. 

## Appendix B Omitted proofs in Section 4

### Proof of Theorem 4.1

Proof.: We adapt the main steps from Azar et al. (2017) for proving the theorem. The proof consists of verifying a valid optimism and developing a regret analysis. We denote \(\widetilde{Q}^{*}_{h,\text{aug}}\) as the optimal \(Q\)-function for \(\widehat{\mathtt{MDP}}_{\mathrm{aug}}\). When analyzing the regret, we also denote \(\widetilde{Q}^{k}_{h,\text{aug}}\) as the optimal \(Q\)-function in the \(k\)-th episode.

Valid optimismTo begin with, we verify that the choice of the bonus functions leads to a valid optimism in the following lemma.

**Lemma B.1**.: Given any failure probability \(\gamma<1\), we set a bonus as

\[b^{k}_{h}(\tau_{h},a_{h})=c_{A}H\left(\sqrt{\frac{H\iota}{N_{t_{h}}(s_{t_{h}}, a_{t_{h}},\delta_{t_{h}})}}+\sqrt{\frac{H\iota}{N_{t_{h}}(s_{t_{h}},a_{t_{h}})}} \right),\]

where \(\iota=\log\left(\frac{SAHK}{\gamma}\right)\) and \(c_{A}\) is a constant. Then with probability \(1-\gamma\), it holds

\[\widetilde{Q}^{k}_{h,\text{aug}}(\tau_{h},a_{h})\geq\widetilde{Q}^{*}_{h, \text{aug}}(\tau_{h},a_{h}),\quad\widetilde{V}^{k}_{h,\text{aug}}(\tau_{h}) \geq\widetilde{V}^{*}_{h,\text{aug}}(\tau_{h})\quad\text{for any}\quad(k,h, \tau_{h},a_{h}).\]

Proof of Lemma b.1.: We compute the cardinality of the expanded state space \(\widetilde{\mathcal{S}}_{\mathrm{aug}}\) as

\[|\widetilde{\mathcal{S}}_{\mathrm{aug}}|\overset{(i)}{=}\sum_{i=0}^{H}HSA^{i}= HS\frac{A^{H+1}-1}{A-1}\leq 2HSA^{H}.\]

For a fixed episode \(k\), we show by backward induction that the assertion in Lemma B.1 holds. To ease the presentation, we omit all superscripts \(k\), all subscripts "aug", as well as the tilde \(\widetilde{\cdot}\) notation.

When \(h=\widetilde{H}+1\), the base assertion holds immediately. Suppose the assertion is true for time \(h+1\). At time \(h\), for any fixed \((\tau_{h},a_{h})\), if \(Q_{h}(\tau_{h},a_{h})=H\), the assertion holds true. Otherwise, we have

\[Q_{h}(\tau_{h},a_{h})-Q_{h}^{*}(\tau_{h},a_{h}) =[\widehat{\mathcal{P}}_{h}V_{h+1}](\tau_{h},a_{h})-[\mathcal{P}_ {h}V_{h+1}^{*}](\tau_{h},a_{h})+b_{h}^{k}(\tau_{h},a_{h})\] \[\geq\underbrace{\left([\widehat{\mathcal{P}}_{h}-\mathcal{P}_{h} ]V_{h+1}^{*}\right)(\tau_{h},a_{h})}_{(A)}+b_{h}^{k}(\tau_{h},a_{h}).\]

We show a lower bound on \((A)\). If \(h\geq H\), expanding the transition kernel \(\mathcal{P}_{h}\) leads to

\[(A) =\sum_{\tau_{h+1}}V_{h+1}^{*}(\tau_{h+1})(\widehat{p}_{h}(\tau_{h +1}|\tau_{h},a_{h})-p_{h}(\tau_{h+1}|\tau_{h},a_{h}))\] \[\overset{(i)}{=} \sum_{s_{t_{h}+1}}V_{h+1}^{*}(\tau_{h+1})(\widehat{p}_{t_{h}}(s_ {t_{h}+1}|s_{t_{h}},a_{t_{h}})-p_{t_{h}}(s_{t_{h}+1}|s_{t_{h}},a_{t_{h}}))\] \[\overset{(ii)}{\geq}-c_{A,1}H\sqrt{\frac{H\iota}{N_{t_{h}}(s_{t_ {h}},a_{t_{h}})}},\]

where equality \((i)\) requires \(\tau_{h+1}\) to take \(s_{t_{h}+1}\) as the new state observation, and inequality \((ii)\) follows from the Hoeffding's inequality (Lemma D.2) with a constant \(c_{A,1}\). Note that the \(H\iota\) term in the numerator comes from a union bound over \(\widetilde{\mathcal{S}}_{\mathrm{aug}}\times\mathcal{A}\).

On the other hand, if \(h<H\), expanding the transition kernel \(\mathcal{P}_{h}\) yields

\[(A) =\sum_{\tau_{h+1}}V_{h+1}^{*}(\tau_{h+1})\left(\widehat{p}_{h}( \tau_{h+1}|\tau_{h},a_{h})-p_{h}(\tau_{h+1}|\tau_{h},a_{h})\right)\] \[=\underbrace{\sum_{\tau_{h+1}}V_{h+1}^{*}(\tau_{h+1})\left( \widehat{p}_{h}(\tau_{h+1}|\tau_{h},a_{h})-p_{h}(\tau_{h+1}|\tau_{h},a_{h}) \right)\mathds{1}\{\delta_{t_{h+1}}=0\}\mathds{1}\{t_{h+1}=t_{h}+1\}}_{(A_{1}) }\] \[\quad+\underbrace{\sum_{\tau_{h+1}}V_{h+1}^{*}(\tau_{h+1})\left( \widehat{p}_{h}(\tau_{h+1}|\tau_{h},a_{h})-p_{h}(\tau_{h+1}|\tau_{h},a_{h}) \right)\mathds{1}\{\delta_{t_{h+1}}=\delta_{t_{h}}+1\}\mathds{1}\{t_{h+1}=t_{ h}\}}_{(A_{2})}.\]

Note that \((A_{1})\) accounts for receiving a new state observation in \(\tau_{h+1}\), and \((A_{2})\) accounts for no new state observation. We tackle these two terms separately. For \((A_{1})\), we have

\[(A_{1})\] \[=\sum_{s_{t_{h+1}}}V_{h+1}^{*}(\tau_{h+1})\left((1-\widehat{ \theta}_{t_{h}}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}}))-(1-\theta_{t_{h}}(s_{t_{h }},a_{t_{h}},\delta_{t_{h}}))\right)\widehat{p}_{t_{h}}(s_{t_{h+1}}|s_{t_{h}},a _{t_{h}})\] \[\quad+\sum_{s_{t_{h+1}}}V_{h+1}^{*}(\tau_{h+1})(1-\theta_{t_{h}}( s_{t_{h}},a_{t_{h}},\delta_{t_{h}}))\left(\widehat{p}_{t_{h}}(s_{t_{h+1}}|s_{t_{h}},a _{t_{h}})-p_{t_{h}}(s_{t_{h+1}}|s_{t_{h}},a_{t_{h}})\right)\] \[\overset{(i)}{\geq}-H\left|\widehat{\theta}_{t_{h}}(s_{t_{h}},a _{t_{h}},\delta_{t_{h}})-\theta_{t_{h}}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}}) \right|-c_{A,2}H\sqrt{\frac{H\iota}{N_{t_{h}}(s_{t_{h}},a_{t_{h}})}},\]

where in \((i)\), the first term is the estimation error of \(\widehat{\theta}\) using the collected data, the second term follows from Hoeffding's inequality, and \(c_{A,2}\) is an absolute constant. For \((A_{2})\), we have

\[(A_{2})\geq-H\left|\widehat{\theta}_{t_{h}}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}}) -\theta_{t_{h}}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}})\right|,\]

since \(\tau_{h+1}\) is now uniquely determined. Summing up \((A_{1})\) and \((A_{2})\), we obtain

\[(A)=(A_{1})+(A_{2})\geq-2H\left|\widehat{\theta}_{t_{h}}(s_{t_{h}},a_{t_{h}}, \delta_{t_{h}})-\theta_{t_{h}}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}})\right|-c_{A,2} H\sqrt{\frac{H\iota}{N_{t_{h}}(s_{t_{h}},a_{t_{h}})}}.\]It remains to bound the estimation error of \(\widehat{\theta}_{t_{h}}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}})\). Using the Hoeffding's inequality again, we obtain

\[\left|\widehat{\theta}_{t_{h}}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}})-\theta_{t_{h }}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}})\right|\leq c_{\theta}\sqrt{\frac{H \iota}{N_{t_{h}}(s_{t_{h}},a_{t_{h}},\delta_{t_{h}})}}.\]

Taking \(c_{A}=\max\{c_{A,1},c_{A,2},c_{\theta},2\}\), we have

\[(A)\geq-c_{A}H\left(\sqrt{\frac{H\iota}{N_{t_{h}}(s_{t_{h}},a_{t_{h}},\delta_{ t_{h}})}}+\sqrt{\frac{H\iota}{N_{t_{h}}(s_{t_{h}},a_{t_{h}})}}\right).\]

With the choice of the bonus function, it can be checked that

\[\widetilde{Q}_{h,\text{aug}}^{k}(\tau_{h},a_{h})-\widetilde{Q}_{h,\text{aug }}^{*}(\tau_{h},a_{h})\geq(A)+b_{h}^{k}(\tau_{h},a_{h})\geq 0\]

with probability \(1-\gamma\) for any \((\tau_{h},a_{h})\). 

Regret analysisIn the sequel, we omit subscripts "aug" and tilde \(\widetilde{\cdot}\) for simplicity. Thanks to Lemma B.1, we consider \(\left(Q_{h}^{k}-Q_{h}^{\pi_{k}}\right)(\tau_{h}^{k},a_{h}^{k})\) as an upper bound of \(\left(Q_{h}^{*}-Q_{h}^{\pi_{k}}\right)(\tau_{h}^{k},a_{h}^{k})\). We bound \(\left(Q_{h}^{k}-Q_{h}^{\pi_{k}}\right)(\tau_{h}^{k},a_{h}^{k})\) by

\[\left(Q_{h}^{k}-Q_{h}^{\pi_{k}}\right)(\tau_{h}^{k},a_{h}^{k})\] \[\leq\left([\widehat{\mathcal{P}}_{h}^{k}V_{h+1}^{k}-\mathcal{P}_{ h}V_{h+1}^{\pi_{k}}]\right)(\tau_{h}^{k},a_{h}^{k})+b_{h}^{k}(\tau_{h}^{k},a_{h}^{k})\] \[\leq\left([\widehat{\mathcal{P}}_{h}^{k}-\mathcal{P}_{h}]V_{h+1}^ {*}\right)(\tau_{h}^{k},a_{h}^{k})+\left([\widehat{\mathcal{P}}_{h}^{k}- \mathcal{P}_{h}][V_{h+1}^{k}-V_{h+1}^{*}]\right)(\tau_{h}^{k},a_{h}^{k})\] \[\quad+\left(\mathcal{P}_{h}[V_{h+1}^{k}-V_{h+1}^{\pi_{k}}]\right) (\tau_{h}^{k},a_{h}^{k})+b_{h}^{k}(\tau_{h},a_{h}^{k})\] \[\leq\underbrace{\left([\widehat{\mathcal{P}}_{h}^{k}-\mathcal{P}_{ h}][V_{h+1}^{k}-V_{h+1}^{*}]\right)(\tau_{h}^{k},a_{h}^{k})}_{(A)}+\left( \mathcal{P}_{h}[V_{h+1}^{k}-V_{h+1}^{\pi_{k}}]\right)(\tau_{h}^{k},a_{h}^{k})+ 2b_{h}^{k}(\tau_{h}^{k},a_{h}^{k}).\] (B.1)

Similar to Lemma B.1, for \(h\geq H\), we expand term \((A)\) into

\[(A) =\sum_{\tau_{h+1}}\left(\widehat{p}_{h}^{k}(\tau_{h+1}|\tau_{h}^{ k},a_{h}^{k})-p_{h}(\tau_{h+1}|\tau_{h}^{k},a_{h}^{k})\right)[V_{h+1}^{k}-V_{h+1}^ {*}](\tau_{h+1})\] \[=\sum_{s_{t_{h}+1}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1})\left( \widehat{p}_{t_{h}}^{k}(s_{t_{h}+1}|s_{t_{h}}^{k},a_{t_{h}}^{k})-p_{t_{h}}(s_{t _{h}+1}|s_{t_{h}}^{k},a_{t_{h}}^{k})\right).\] (B.2)

On the other hand, for \(h\leq H\), the decomposition of term \((A)\) is more complicated. We have

\[(A) =\sum_{\tau_{h+1}}\left(\widehat{p}_{h}^{k}(\tau_{h+1}|\tau_{h}^{ k},a_{h}^{k})-p_{h}(\tau_{h+1}|\tau_{h}^{k},a_{h}^{k})\right)[V_{h+1}^{k}-V_{h+1}^{ *}](\tau_{h+1})\] \[=\underbrace{\sum_{\tau_{h+1}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1 })\left(\widehat{p}_{h}^{k}(\tau_{h+1}|\tau_{h}^{k},a_{h}^{k})-p_{h}(\tau_{h+1} |\tau_{h}^{k},a_{h}^{k})\right)\mathds{1}\{\delta_{t_{h+1}}=0\}\mathds{1}\{t_ {h+1}=t_{h}^{k}+1\}}_{(A_{1})}\] \[\quad+\underbrace{\sum_{\tau_{h+1}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_ {h+1})\left(\widehat{p}_{h}^{k}(\tau_{h+1}|\tau_{h}^{k},a_{h}^{k})-p_{h}(\tau_{h +1}|\tau_{h}^{k},a_{h}^{k})\right)\mathds{1}\{\delta_{t_{h+1}}=\delta_{t_{h}^{k} }+1\}\mathds{1}\{t_{h+1}=t_{h}^{k}\}}_{(A_{2})}.\]

Term \((A_{2})\) can be directly bounded by

\[(A_{2}) \leq H\left|\widehat{\theta}_{t_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{ k},\delta_{t_{h}}^{k})-\theta_{t_{h}}(s_{t_{h}}^{k},a_{t_{h}}^{k},\delta_{t_{h}}^{k})\right|\] \[\leq c_{\theta}H\sqrt{\frac{H\iota}{N_{t_{h}}^{k}(s_{t_{h}}^{k},a_{t _{h}}^{k},\delta_{t_{h}}^{k})}}\]with probability \(1-\gamma\). To bound \((A_{1})\), we have

\[(A_{1}) =\sum_{s_{t_{h+1}}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1})\bigg{(} \left(1-\widehat{\theta}_{t_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k},\delta_{t_{h}}^ {k})\right)\widehat{p}_{t_{h}}^{k}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})\] \[\quad-\left(1-\theta_{t_{h}}(s_{t_{h}}^{k},a_{t_{h}}^{k},\delta_{t _{h}}^{k})\right)p_{t_{h}}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})\bigg{)}\] \[=\sum_{s_{t_{h+1}}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1})\left( \left(1-\widehat{\theta}_{t_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k},\delta_{t_{h }}^{k})\right)-\left(1-\theta_{t_{h}}(s_{t_{h}}^{k},a_{t_{h}}^{k},\delta_{t_{h }}^{k})\right)\right)\widehat{p}_{t_{h}}^{k}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h }}^{k})\] \[\quad+\sum_{s_{t_{h+1}}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1}) \left(1-\theta_{t_{h}}(s_{t_{h}}^{k},a_{t_{h}}^{k},\delta_{t_{h}}^{k})\right) \left(\widehat{p}_{t_{h}}^{k}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})-p_{t_{h }}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})\right)\] \[\leq\left(1-\theta_{t_{h}}(s_{t_{h}}^{k},a_{t_{h}}^{k},\delta_{t_{ h}}^{k})\right)\sum_{s_{t_{h+1}}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1})\left( \widehat{p}_{t_{h}}^{k}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})-p_{t_{h}}(s_{ t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})\right)\] \[\quad+c_{\theta}H\sqrt{\frac{H\iota}{N_{t_{h}}^{k}(s_{t_{h}}^{k}, a_{t_{h}}^{k},\delta_{t_{h}}^{k})}}.\]

Putting \((A_{1})\) and \((A_{2})\) together, we obtain

\[(A) \leq\left(1-\theta_{t_{h}}(s_{t_{h}}^{k},a_{t_{h}}^{k},\delta_{t_ {h}}^{k})\right)\sum_{s_{t_{h+1}}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1})\left( \widehat{p}_{t_{h}}^{k}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})-p_{t_{h}}(s_{ t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})\right)\] \[\quad+2c_{\theta}H\sqrt{\frac{H\iota}{N_{t_{h}}^{k}(s_{t_{h}}^{k}, a_{t_{h}}^{k},\delta_{t_{h}}^{k})}}.\] (B.3)

In both (B) and (B) for different ranges of \(h\), we apply the Bernstein inequality (Lemma D.1) to derive

\[\sum_{s_{t_{h+1}}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1})\left( \widehat{p}_{t_{h}}^{k}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})-p_{t_{h}}(s_{ t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})\right)\] \[\leq c\cdot\sum_{s_{t_{h+1}}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1} )\left[\sqrt{\frac{p_{t_{h}}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})\iota}{N_ {t_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k})}}+\frac{\iota}{N_{t_{h}}^{k}(s_{t_{h} }^{k},a_{t_{h}}^{k})}\right]\] \[\stackrel{{(i)}}{{\leq}}c\cdot\sum_{s_{t_{h+1}}}[V_{ h+1}^{k}-V_{h+1}^{*}](\tau_{h+1})\left[\frac{p_{t_{h}}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})} {2cH}+\frac{(2cH+1)\iota}{N_{t_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k})}\right]\] \[\leq c\cdot\left(\frac{SH(2cH+1)\iota}{N_{t_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k})}+\frac{1}{2cH}\sum_{s_{t_{h+1}}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_ {h+1})p_{t_{h}}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})\right),\] (B.4)

where inequality \((i)\) follows from \(\sqrt{ab}\leq a+b\). Substituting (B) into (B), for \(h\geq H\), we deduce

\[(A) \leq\frac{1}{2H}\sum_{s_{t_{h+1}}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_ {h+1})p_{t_{h}}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})+\frac{cSH(2cH+1)\iota}{N _{t_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k})}\] \[\stackrel{{(i)}}{{\leq}}\frac{1}{2H}\left(\mathcal{P} _{h}[V_{h+1}^{k}-V_{h+1}^{\tau_{h}}]\right)(\tau_{h}^{k},a_{h}^{k})+c^{\prime} \frac{mSH^{2}\iota}{N_{t_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k})},\]

where \(c^{\prime}\) is a sufficiently large constant. By the same reasoning, substituting (B) into (B), for \(h<H\), we have

\[(A) \leq\frac{1}{2H}\left(1-\theta_{t_{h}}(s_{t_{h}}^{k},a_{t_{h}}^{k}, \delta_{t_{h}}^{k})\right)\sum_{s_{t_{h+1}}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1} )p_{t_{h}}(s_{t_{h+1}}|s_{t_{h}}^{k},a_{t_{h}}^{k})+\frac{cSH(2cH+1)\iota}{N_{t_{h} }^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k})}\]\[\leq e\sum_{h=1}^{2H}\left(\xi_{h}^{k}+\zeta_{h}^{k}+2b_{h}^{k}+2c_{ \theta}H\sqrt{\frac{H\iota}{N_{t_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k},\delta_{t_ {h}}^{k})}}\right).\]

As a consequence, the total regret is bounded by

\[\texttt{Regret}(K)\leq e\sum_{k=1}^{K}\sum_{h=1}^{2H}\left(\xi_{h}^{k}+\zeta_{h }^{k}+2b_{h}^{k}+2c_{\theta}H\sqrt{\frac{H\iota}{N_{t_{h}}^{k}(s_{t_{h}}^{k},a_ {t_{h}}^{k},\delta_{t_{h}}^{k})}}\right).\] (B.6)

We need to sum over \(\zeta_{h}^{k},\xi_{h}^{k},b_{h}^{k}\). Consider \(\zeta_{h}^{k}\) first. We have

\[\sum_{k=1}^{K}\sum_{h=1}^{2H}\zeta_{h}^{k} =c^{\prime}\sum_{k=1}^{K}\sum_{h=1}^{2H}\frac{SH^{2}\iota}{N_{t_{ h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k})}\] \[\overset{(i)}{\leq}c^{\prime}H\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{ SH^{2}\iota}{N_{t_{h}}^{k}(s_{h}^{k},a_{t_{h}}^{k})}\] \[\overset{(ii)}{\leq}c_{\zeta}H^{4}S^{2}A\iota^{2},\] (B.7)

where inequality \((i)\) invokes the fact that \(t_{h}\) only takes value in \(\{1,\dots,H\}\) and each \(N_{t_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k})\) is repeated at most \(H\) times, and inequality \((ii)\) follows from the pigeon-hole argument in Azar et al. (2017).

Next we bound the summation over \(\xi_{h}^{k}\). This is a martingale difference sequence. We apply Azuma-Hoeffding's inequality (Lemma D.3) with \(n=2H\) and \(c_{i}=4H\) to obtain

\[\sum_{k=1}^{K}\sum_{h=1}^{2H}\xi_{h}^{k}\leq c_{\xi}\sqrt{KH^{4}\iota}.\] (B.8)The additional \(H\) dependence above comes from a union bound over \(\widetilde{\mathcal{S}}_{\mathrm{aug}}\times\mathcal{A}\). Lastly, we tackle the summation over bonus functions \(b_{h}^{k}\). We have

\[\sum_{k=1}^{K}\sum_{h=1}^{2H}b_{h}^{k} =\sum_{k=1}^{K}\sum_{h=1}^{2H}c_{A}H\sqrt{\frac{H\iota}{N_{h_{h}}^ {k}(s_{t_{h}},a_{t_{h}})}}\] \[\leq c_{A}H\sum_{k=1}^{K}\sum_{h=1}^{H}H\sqrt{\frac{H\iota}{N_{h_ {h}}^{k}(s_{t_{h}},a_{t_{h}})}}\] \[\leq c_{b}H^{7/2}\sqrt{SAK\iota}.\] (B.9)

Putting (B.7), (B.8) and (B.9) together, we deduce

\[\mathtt{Regret}(K)\leq c\left(H^{7/2}\sqrt{SAK\iota}+H^{4}S^{2}A\iota^{2}+ \sqrt{H^{4}K\iota}\right)+2ec_{\delta}H\sum_{k=1}^{K}\sum_{h=1}^{2H}\sqrt{ \frac{H\iota}{N_{h_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h}}^{k},\delta_{h_{h}}^{k})}}\]

for some constant \(c\). To this end, the only remaining task is to find \(\sum_{k=1}^{K}\sum_{h=1}^{2H}\sqrt{\frac{1}{N_{h_{h}}^{k}(s_{t_{h}}^{k},a_{t_{ h}}^{k},\delta_{t_{h}}^{k})}}\), which undergoes a similar argument as the bonus summation. We have

\[\sum_{k=1}^{K}\sum_{h=1}^{2H}\sqrt{\frac{1}{N_{h_{h}}^{k}(s_{t_{h} }^{k},a_{t_{h}}^{k},\delta_{t_{h}}^{k})}} \leq H\sum_{k=1}^{K}\sum_{h=1}^{H}\sqrt{\frac{1}{N_{h}^{k}(s_{h} ^{k},a_{h}^{k},\delta_{h}^{k})}}\] \[=H\sum_{(h,s,a,\delta)}\sum_{i=1}^{N_{h}^{K}(s,a,\delta)}\sqrt{ \frac{1}{i}}\] \[\overset{(i)}{\leq}2H\sum_{\delta}\sum_{(h,s,a)}\sqrt{N_{h}^{K}( s,a,\delta)}\] \[\overset{(ii)}{\leq}2H\sum_{\delta}\sqrt{SAKH}\] \[\overset{(iii)}{\leq}2H^{2}\sqrt{SAKH},\] (B.10)

where inequality \((i)\) invokes \(\sum_{i=1}^{n}1/\sqrt{i}\leq 2\sqrt{n}\), inequality \((ii)\) follows from Cauchy-Schwarz, and inequality \((iii)\) uses the fact that \(\delta\) is bounded by \(H\). Plugging (B.10) into the regret bound, we obtain the desired result

\[\mathtt{Regret}(K)\leq c\left(H^{4}\sqrt{SAK\iota}+H^{4}S^{2}A\iota^{2}+ \sqrt{H^{4}K\iota}\right)\]

with probability \(1-\gamma\). Absorbing \(\sqrt{H^{4}K\iota}\) into \(H^{4}\sqrt{SAK\iota}\) yields the bound in Theorem 4.1. 

### Regret under bounded delay

To obtain the regret bound under bounded delay, we only need to modify several steps in the proof of Theorem 4.1. Specifically, in Lemma B.1, we replace \(H\) by \(D+1\) in the square root, so that the bonus function becomes

\[b_{h}^{k}(\tau_{h},a_{h})=c_{A}H\left(\sqrt{\frac{(D+1)\iota}{N_{t_{h}}(s_{t_ {h}},a_{t_{h}},\delta_{t_{h}})}}+\sqrt{\frac{(D+1)\iota}{N_{t_{h}}(s_{t_{h}}, a_{t_{h}})}}\right).\]

The optimism still holds, since the expanded state space contains at most \(D\) historical actions.

The second modification is to observe that since the delay is bounded by \(D\), each counting number \(N_{h}^{k}\) only repeats at most \(D+1\) times. In this way, (B.7) becomes

\[\sum_{k=1}^{K}\sum_{h=1}^{2H}\zeta_{h}^{k}\leq c_{\zeta}(D+1)H^{3}S^{2}A\iota^{ 2}.\](B.8) and (B.9) are replaced by

\[\sum_{k=1}^{K}\sum_{h=1}^{2H}\xi_{h}^{k}\leq c_{\xi}(D+1)\sqrt{K(D+1)H^{3}\iota} \quad\text{and}\quad\sum_{k=1}^{K}\sum_{h=1}^{2H}b_{h}^{k}\leq c_{b}(D+1)^{3/2} \sqrt{H^{3}SAK\iota},\]

respectively. Lastly, we also have

\[\sum_{k=1}^{K}\sum_{h=1}^{2H}\sqrt{\frac{1}{N_{t_{h}}^{k}(s_{t_{h}}^{k},a_{t_{h} }^{k},\delta_{t_{h}}^{k})}}\leq 2(D+1)^{2}\sqrt{SAKH}.\]

Putting these updated upper bounds together and substituting into \(\mathtt{Regret}(K)\), we deduce

\[\mathtt{Regret}(K)\leq c\left((D+1)^{5/2}\sqrt{H^{3}SAK\iota}+H^{4}S^{2}A\iota ^{2}\right).\]

### Statement and proof of Proposition b.2

**Proposition b.2**.: In the setup of Section 4.2, we have

\[\mathtt{gap}(s_{1}) \leq\sum_{h=1}^{H}\Bigg{[}\underbrace{\int_{\tau}\left(\mathbb{ E}_{s\sim\mathfrak{b}_{h}(\cdot|\tau)}[\max_{a}r_{h}(s,a)]-\max_{a}\mathbb{E}_{s \sim\mathfrak{b}_{h}(\cdot|\tau)}[r_{h}(s,a)]\right)\left(\rho_{h}^{\pi_{ \mathrm{delay}}^{*}}\wedge\rho_{h}^{\pi_{\mathrm{nodelay}}^{*}}\right)(\tau) \mathrm{d}\tau}_{\mathcal{E}_{1}}\] \[\quad+2\underbrace{\|\rho_{h}^{\pi_{\mathrm{nodelay}}^{*}}-\rho_ {h}^{\pi_{\mathrm{delay}}^{*}}\|_{\mathrm{TV}}}_{\mathcal{E}_{2}}\Bigg{]}.\]

where \(\rho_{h}^{\pi_{\mathrm{nodelay}}^{*}}\) and \(\rho_{h}^{\pi_{\mathrm{delay}}^{*}}\) are visitation measures induced by \(\pi_{\mathrm{nodelay}}^{*}\) and \(\pi_{\mathrm{delay}}^{*}\), respectively.

Term \(\mathcal{E}_{1}\) is strictly larger than zero due to the convexity of the max operation. Term \(\mathcal{E}_{2}\) accounts for the difference in the visitation measure. When the original MDP has deterministic transitions, we can check that \(\mathcal{E}_{1}\) is zero, since the expectation over \(s\) is concentrated on a singleton that can be inferred from history. Hence, the visitation measures are also identical, which implies \(V_{1,\mathrm{nodelay}}^{*}(s_{1})-V_{1,\mathrm{delay}}^{*}(s_{1})=0\). On the contrary, when \(\mathfrak{b}_{h}(\cdot|\tau)\) is evenly spread, meaning that the entropy of \(\mathfrak{b}_{h}\) is high, we potentially suffer from a large performance drop, in that, inferring the current state is difficult.

Proof of Proposition b.2.: Let \(\tau_{1},\dots,\tau_{H}\) denote the states observed in the delayed environment. Since \(\pi_{\mathrm{nodelay}}^{*}\) is greedy and Markov, we obtain

\[V_{1,\mathrm{nodelay}}^{*}(s_{1}) =\mathbb{E}^{\pi_{\mathrm{nodelay}}^{*}}\left[\sum_{h=1}^{H-1}r_{ h}(s_{h},a_{h})|s_{1}\right]+\mathbb{E}^{\pi_{\mathrm{nodelay}}^{*}}\left[ \mathbb{E}[r_{H}(s_{H},a_{H})|\tau_{H}]|s_{1}\right]\] \[=\mathbb{E}^{\pi_{\mathrm{nodelay}}^{*}}\left[\sum_{h=1}^{H-1}r_{ h}(s_{h},a_{h})|s_{1}\right]+\mathbb{E}^{\pi_{\mathrm{nodelay}}^{*}}\left[ \sum_{s}\mathfrak{b}_{H}(s|\tau_{H})\max_{a}r_{H}(s,a)|s_{1}\right].\]

Recursively applying the above argument, we deduce

\[V_{1,\mathrm{nodelay}}^{*}(s_{1})=\mathbb{E}^{\pi_{\mathrm{nodelay}}^{*}} \left[\sum_{h=1}^{H}\sum_{s}\mathfrak{b}_{h}(s|\tau_{h})\max_{a}r_{h}(s,a)|s_{1 }\right].\]

We also rewrite \(V_{1,\mathrm{delay}}^{*}(s_{1})\) as

\[V_{1,\mathrm{delay}}^{*}(s_{1}) =\mathbb{E}^{\pi_{\mathrm{delay}}^{*}}\left[\sum_{h=1}^{H-1}r_{ h}(s_{h},a_{h})|s_{1}\right]+\mathbb{E}^{\pi_{\mathrm{delay}}^{*}}\left[ \mathbb{E}[r_{H}(s_{H},a_{H})|\tau_{H}]|s_{1}\right]\] \[=\mathbb{E}^{\pi_{\mathrm{delay}}^{*}}\left[\sum_{h=1}^{H-1}r_{ h}(s_{h},a_{h})|s_{1}\right]+\mathbb{E}^{\pi_{\mathrm{delay}}^{*}}\left[\max_{a} \sum_{s}\mathfrak{b}_{H}(s|\tau_{H})r_{H}(s,a)|s_{1}\right]\]\[=...\] \[=\mathbb{E}^{\pi^{*}_{\rm delay}}\left[\sum_{h=1}^{H}\max_{a}\sum_{s} \mathfrak{b}_{h}(s|\tau_{h})r_{h}(s,a)|s_{1}\right].\]

Then we write the difference between \(V^{*}_{1,{\rm nodelay}}(s_{1})\) and \(V^{*}_{1,{\rm delay}}(s_{1})\) as

\[V^{*}_{1,{\rm nodelay}}(s_{1})-V^{*}_{1,{\rm delay}}(s_{1})\] \[=\sum_{h=1}^{H}\Bigg{(}\int_{\tau}\sum_{s}\max_{a}\mathfrak{b}_{h }(s|\tau)r_{h}(s,a)\rho_{h}^{\pi^{*}_{\rm nodelay}}(\tau){\rm d}\tau-\int_{\tau }\max_{a}\sum_{s}\mathfrak{b}_{h}(s|\tau)r_{h}(s,a)\rho_{h}^{\pi^{*}_{\rm delay }}(\tau){\rm d}\tau\Bigg{)}\] \[=\sum_{h=1}^{H}\Bigg{(}\int_{\tau}\sum_{s}\max_{a}\mathfrak{b}_{h }(s|\tau)r_{h}(s,a)\rho_{h}^{\pi^{*}_{\rm nodelay}}(\tau){\rm d}\tau-\int_{ \tau}\max_{a}\sum_{s}\mathfrak{b}_{h}(s|\tau)r_{h}(s,a)\rho_{h}^{\pi^{*}_{\rm nodelay }}(\tau){\rm d}\tau\] \[\quad+\int_{\tau}\max_{a}\sum_{s}\mathfrak{b}_{h}(s|\tau)r_{h}(s, a)\rho_{h}^{\pi^{*}_{\rm nodelay}}(\tau){\rm d}\tau-\int_{\tau}\max_{a}\sum_{s} \mathfrak{b}_{h}(s|\tau)r_{h}(s,a)\rho_{h}^{\pi^{*}_{\rm delay}}(\tau){\rm d}\tau\Bigg{)}\] \[\leq\sum_{h=1}^{H}\left[\int_{\tau}\left(\mathbb{E}_{s\sim \mathfrak{b}_{h}(\cdot|\tau)}[\max_{a}r_{h}(s,a)]-\max_{a}\mathbb{E}_{s\sim \mathfrak{b}_{h}(\cdot|\tau)}[r_{h}(s,a)]\right)\rho_{h}^{\pi^{*}_{\rm nodelay} }(\tau){\rm d}\tau+2\|\rho_{h}^{\pi^{*}_{\rm nodelay}}-\rho_{h}^{\pi^{*}_{\rm delay }}\|_{\rm TV}\right].\]

We also have

\[V^{*}_{1,{\rm nodelay}}(s_{1})-V^{*}_{1,{\rm delay}}(s_{1})\] \[=\sum_{h=1}^{H}\Bigg{(}\int_{\tau}\sum_{s}\max_{a}\mathfrak{b}_{ h}(s|\tau)r_{h}(s,a)\rho_{h}^{\pi^{*}_{\rm nodelay}}(\tau){\rm d}\tau-\int_{ \tau}\sum_{s}\max_{a}\mathfrak{b}_{h}(s|\tau)r_{h}(s,a)\rho_{h}^{\pi^{*}_{\rm delay }}(\tau){\rm d}\tau\] \[\quad+\int_{\tau}\sum_{s}\max_{a}\mathfrak{b}_{h}(s|\tau)r_{h}(s, a)\rho_{h}^{\pi^{*}_{\rm delay}}(\tau){\rm d}\tau-\int_{\tau}\max_{a}\sum_{s} \mathfrak{b}_{h}(s|\tau)r_{h}(s,a)\rho_{h}^{\pi^{*}_{\rm delay}}(\tau){\rm d} \tau\Bigg{)}\] \[\leq\sum_{h=1}^{H}\left[\int_{\tau}\left(\mathbb{E}_{s\sim \mathfrak{b}_{h}(\cdot|\tau)}[\max_{a}r_{h}(s,a)]-\max_{a}\mathbb{E}_{s\sim \mathfrak{b}_{h}(\cdot|\tau)}[r_{h}(s,a)]\right)\rho_{h}^{\pi^{*}_{\rm delay}}( \tau){\rm d}\tau+2\|\rho_{h}^{\pi^{*}_{\rm nodelay}}-\rho_{h}^{\pi^{*}_{\rm delay }}\|_{\rm TV}\right].\]

Combining the above two inequalities, we obtain

\[V^{*}_{1,{\rm nodelay}}(s_{1})-V^{*}_{1,{\rm delay}}(s_{1})\] \[\leq\sum_{h=1}^{H}\Bigg{[}\int_{\tau}\left(\mathbb{E}_{s\sim \mathfrak{b}_{h}(\cdot|\tau)}[\max_{a}r_{h}(s,a)]-\max_{a}\mathbb{E}_{s\sim \mathfrak{b}_{h}(\cdot|\tau)}[r_{h}(s,a)]\right)\left(\rho_{h}^{\pi^{*}_{\rm delay }}\wedge\rho_{h}^{\pi^{*}_{\rm nodelay}}\right)(\tau){\rm d}\tau\] \[\quad+2\|\rho_{h}^{\pi^{*}_{\rm nodelay}}-\rho_{h}^{\pi^{*}_{\rm delay }}\|_{\rm TV}\Bigg{]}.\]

The proof is complete. 

### Proof of Proposition 4.2

Proof.: We construct an MDP instance \((\mathcal{S},\mathcal{A},H,R,P)\) for \(H>d\) as follows. Let \(\mathcal{S}=\{1,2\}\) and \(\mathcal{A}=\{a_{1},a_{2}\}\). For the reward function, we have

\[r_{h}(s,a)=\begin{cases}1&\text{if $a=a_{s}$ and $h=d+1$}\\ 0&\text{otherwise}\end{cases}.\]

The reward is nonzero only at time \(d+1\). The transition probabilities are defined as

\[p_{h}(s^{\prime}|s,a)=\begin{cases}\frac{1}{2}&\text{if $h=d+1$}\\ 1&\text{if $h\neq d+1$ and $s^{\prime}=s$}\\ 0&\text{otherwise}\end{cases}.\]The transition probability at step \(d+1\) says that \(s^{\prime}\) is uniform regardless of the previous state and action. Suppose a uniform initial distribution on \(s_{1}\). We first show that if the constant delay equals \(d\), then there exists a policy \(\pi^{*,d}\) achieving maximal value. Indeed, the policy is chosen as

\[\pi_{h}^{*,d}(\cdot|\{s_{h-d},\mathbf{a}_{h-d:h-1}\})=\begin{cases}a_{s_{h-d}}& \text{if }h=d+1\\ \operatorname{Uniform}(\mathcal{A})&\text{if }h\neq d+1.\end{cases}\]

It is straightforward to check that \(\pi^{*,d}\) is optimal, since at step \(d+1\), \(s_{1}\) is revealed and the policy takes the optimal action \(a_{s_{1}}\) to obtain reward \(1\).

On the other hand, if the constant delay equals \(d+1\), then any policy suffers from a constant performance degradation. To see this, in a single trajectory, since the starting state is only revealed at time \(d+2\), the policy at time \(d+1\) cannot exploit the information of the initial state. Therefore, any policy coincides with the best action with probability \(\frac{1}{2}\). For \(K\) episodes, with probability \(1-\gamma\), the total reward of any policy \(\pi\in\Pi_{\mathrm{exec}}\) is bounded by

\[\sum_{k=1}^{K}V_{1}^{\pi}(s_{1}^{k})\leq\frac{1}{2}K+\sqrt{\frac{K}{2}\log \frac{1}{\gamma}},\]

due to Hoeffding's inequality. As a result, the performance drop is at least

\[\mathsf{gap}(K)\geq\frac{1}{2}-\sqrt{\frac{1}{2K}\log\frac{1}{\gamma}}.\]

## Appendix C Omitted proofs in Section 5

### Proof of Proposition 5.1

Proof.: We first show that the ground-truth transition probabilities \(p_{h}^{\theta^{*}}\) belongs to \(\mathcal{B}_{k}\) with high probability. By Theorem 2.2 in Weissman et al. (2003) (see also Equation (44) in Jaksch et al. (2010)), at the \(k\)-th episode, for any fixed \((s,a,h)\), we have

\[\mathbb{P}\left(\|\widehat{p}_{h}^{k}(\cdot|s,a)-p_{h}^{\theta^{*}}(\cdot|s,a )\|_{\mathrm{TV}}\geq t\right)\leq(2^{S}-2)\exp\left(-\frac{N_{h}^{k}(s,a)t^{ 2}}{2}\right).\]

Setting \(t=c\sqrt{\frac{S_{t}}{N_{h}^{k}(s,a)}}\) for some constant \(c\) ensures that

\[\|\widehat{p}_{h}^{k}(\cdot|s,a)-p_{h}^{\theta^{*}}(\cdot|s,a)\|_{\mathrm{TV} }\leq c\sqrt{\frac{S_{t}}{N_{h}^{k}(s,a)}}\]

holds over any \((s,a,h,k)\) with probability \(1-\gamma\). As a consequence, the event \(p_{h}^{\theta^{*}}(\cdot|s,a)\in\mathcal{B}^{k}\) holds with probability \(1-\gamma\) over all \((s,a,h,k)\).

Conditioned on the high probability event \(p_{h}^{\theta^{*}}\in\mathcal{B}^{k}\) for all \((h,s,a)\), we have by standard performance difference arguments that

\[\sum_{k=1}^{K}\max_{\pi\in\Pi_{\mathrm{exec}}}V_{\theta^{*}}^{\pi }(s_{1}^{k})-V_{\theta^{*}}^{\pi^{k}}(s_{1}^{k}) \stackrel{{(i)}}{{\leq}}\sum_{k=1}^{K}V_{\theta^{k}} ^{\pi^{k}}(s_{1}^{k})-V_{\theta^{*}}^{\pi^{k}}(s_{1}^{k})\] \[\stackrel{{(ii)}}{{\leq}}\sum_{k=1}^{K}\sum_{h=1}^{ H}\mathbb{E}_{\theta^{*}}^{\pi^{k}}\Big{[}\Big{\langle}(\mathbb{P}_{h}^{ \theta^{k}}-\mathbb{P}_{h}^{\theta^{*}})(\cdot|s_{h},a_{h}),V_{\theta^{k},h+1 }^{\pi^{k}}(\cdot)\Big{\rangle}\Big{]}\] \[\leq\sum_{h=1}^{H}\sum_{k=1}^{K}\mathbb{E}_{\theta^{*}}^{\pi^{k}} \Bigg{[}c\sqrt{\frac{H^{2}S_{t}}{N_{h}^{k}(s_{h},a_{h})}}\wedge H\Bigg{]}\] \[\stackrel{{(iii)}}{{\leq}}\sum_{h=1}^{H}\sum_{k=1}^{ K}c^{\prime}\sqrt{\frac{H^{2}S_{t}}{N_{h}^{k}(s_{h}^{k},a_{h}^{k})}}+H\sqrt{H^{2}K_{t}}\]\[\stackrel{{(iv)}}{{\leq}}c^{\prime}\left(\left\lceil \frac{\log\frac{HK}{\gamma}}{-\log(1-\lambda_{0}^{2})}\right\rceil\sqrt{H^{2}S_{ t}\cdot SAHK}+\sqrt{H^{4}K_{t}}\right)\] \[\leq c^{\prime}\left(\left\lceil\frac{1}{-\log(1-\lambda_{0}^{2})} \right\rceil\sqrt{H^{3}S^{2}AK_{t}{}^{3}}+\sqrt{H^{4}K_{t}}\right),\]

where inequality \((i)\) follows from the valid optimism since Line 4 in Algorithm 3 is taken over double maximization, equality \((ii)\) recursively expands the value function and \(\langle\cdot,\cdot\rangle\) denotes the inner product, inequality \((iii)\) invokes Azuma-Hoeffding's inequality, and inequality \((iv)\) invokes Lemma C.2. 

### Algorithm and proof of Theorem 5.2

```
1:Input: Horizon \(H\).
2:Init:\(V_{H+1}(\tau)=0\) and \(Q_{H}(\tau,a)=H\) for any \(\tau,a\), data set \(\mathcal{D}^{0}=\emptyset\), initial policy \(\pi^{0}\).
3:for episode \(k=1,\ldots,K\)do
4: Execute policy \(\pi^{k-1}\).
5: After the episode ends, collect data \(\mathcal{D}^{k}=\mathcal{D}^{k-1}\cup\{(s_{h}^{k},a_{h}^{k},r_{h}^{k})\}_{h=1}^ {H}\).
6: On data set \(\mathcal{D}^{k}\), compute counting numbers \[N_{h}^{k}(\tau_{h},a_{h})=\sum_{j=1}^{k}\mathds{1}\{\tau_{h}^{k}=\tau_{h},a_{ h}^{k}=a_{h},s_{h+1}^{k}\neq\emptyset\}\quad\text{and}\quad N_{h,\lambda}^{k}= \sum_{j=1}^{k}\mathds{1}\{s_{h}^{k}=\emptyset\}.\]
7: Estimate transition probabilities and delay distributions via \[\widehat{p}_{h}^{k}(s_{h+1}|\tau_{h},a_{h})=\frac{N_{h}^{k}(\tau_{h},a_{h},s_{ h+1})}{N_{h}^{k}(\tau_{h},a_{h})}\quad\text{and}\quad\widehat{\lambda}_{h}^{k}=N_{h, \lambda}^{k}/k.\]
8: Set bonus function as \[b_{h}^{k}(\tau_{h},a_{h})=cH\left(\sqrt{\frac{H\iota}{N_{h}^{k}(\tau_{h},a_{h} )}}+\sqrt{\frac{\iota}{k}}\right)\] for \(\iota=\log\frac{SAKH}{\gamma}\) and \(c\) sufficiently large.
9: Run optimistic value iteration in \(\texttt{MDP}_{\text{aug}}\) for \(H\) steps and obtain \(\pi^{k}\in\Pi_{\text{exec}}\).
10:endfor
11:Return: Learned policy \(\pi^{k}\) for \(k=1,\ldots,K\). ```

**Algorithm 4** Policy learning for MDPs with missing observations

We remark that similar to delayed MDPs, in Line 9 the planning is on \(\texttt{MDP}_{\text{aug}}\) and the obtained policy is executable given any \(\tau\in\mathcal{S}_{\text{aug}}\) when state observation is missed. Therefore, the planning complexity is \(SA^{H}\). Different from Algorithm 2, the bonus function here depends on multi-step transitions, in that missing observations are permanently lost.

Proof of Theorem 5.2.: The proof utilizes similar steps as Theorem 4.1, with an extra care on the summation of bonus functions.

Valid optimismWe verify the choice of bonus functions leads to a valid optimism.

**Lemma C.1**.: Given any failure probability \(\gamma<1\), we set bonus functions as

\[b_{h}^{k}(\tau_{h},a_{h})=cH\left(\sqrt{\frac{H\iota}{N_{h}^{k}(\tau_{h},a_{h}) }}+\sqrt{\frac{\iota}{k}}\right)\quad\text{with}\quad\iota=\log\left(\frac{ SAHK}{\gamma}\right).\]

Then with probability \(1-\gamma\), it holds

\[Q_{h,\text{aug}}^{k}(\tau_{h},a_{h})\geq Q_{h,\text{aug}}^{*}(\tau_{h},a_{h}),\quad V_{h,\text{aug}}^{k}(\tau_{h})\geq V_{h,\text{aug}}^{*}(\tau_{h}) \quad\text{for any}\quad(k,h,\tau_{h},a_{h}).\]Proof of Lemma c.1.: In the proof, we omit subscript "aug" for simplicity. We use backward induction on time \(h\) again. The base case of \(H+1\) holds immediately due to the initial value of \(V_{H+1,\text{aug}}\). Suppose at time \(h+1\), the assertion holds. Then for time \(h\), if \(Q_{h,\text{aug}}=H\), the assertion holds trivially. Otherwise, we have

\[Q_{h}(\tau_{h},a_{h})-Q_{h}^{*}(\tau_{h},a_{h})\] \[=\widehat{r}_{h}(\tau_{h},a_{h})+[\widehat{\mathcal{P}}_{h}V_{h+1 }](\tau_{h},a_{h})-r_{h}(\tau_{h},a_{h})-[\mathcal{P}_{h}V_{h+1}^{*}](\tau_{h},a_{h})+b_{h}^{k}(\tau_{h},a_{h})\] \[\geq\underbrace{\left([\widehat{\mathcal{P}}_{h}-\mathcal{P}_{h}] V_{h+1}^{*}\right)(\tau_{h},a_{h})}_{(A)}+\underbrace{\widehat{r}_{h}(\tau_{h},a_{h}) -r_{h}(\tau_{h},a_{h})}_{(B)}+b_{h}^{k}(\tau_{h},a_{h}).\]

We lower bound \((A)\) and \((B)\) separately. For term \((A)\), we have

\[(A) =\sum_{\tau_{h+1}}V_{h+1}^{*}(\tau_{h+1})\left(\widehat{p}_{h}( \tau_{h+1}|\tau_{h},a_{h})-p_{h}(\tau_{h+1}|\tau_{h},a_{h})\right)\] \[=\sum_{\tau_{h+1}}V_{h+1}^{*}(\tau_{h+1})\left(\widehat{p}_{h}( \tau_{h+1}|\tau_{h},a_{h})-p_{h}(\tau_{h+1}|\tau_{h},a_{h})\right)\mathds{1} \{t_{h+1}=h+1\}\] \[\quad+\sum_{\tau_{h+1}}V_{h+1}^{*}(\tau_{h+1})\left(\widehat{p}_{ h}(\tau_{h+1}|\tau_{h},a_{h})-p_{h}(\tau_{h+1}|\tau_{h},a_{h})\right)\mathds{1} \{t_{h+1}=t_{h}\}\] \[=\underbrace{\sum_{s_{h+1}}V_{h+1}^{*}(\tau_{h+1})\left((1- \widehat{\lambda}_{h})\widehat{p}_{h}(s_{h+1}|s_{t_{h}},\mathbf{a}_{t_{h}:h})- (1-\lambda_{h})p_{h}(s_{h+1}|s_{t_{h}},\mathbf{a}_{t_{h}:h})\right)}_{(A_{1})}\] \[\quad+\underbrace{V_{h+1}^{*}(\{s_{t_{h}},\mathbf{a}_{t_{h}:h}\}) (\widehat{\lambda}_{h}-\lambda_{h})}_{(A_{2})}.\]

In \((A_{1})\), \(\tau_{h+1}\) is \(\{s_{h+1}\}\). We bound \((A_{1})\) as

\[(A_{1}) =\sum_{s_{h+1}}V_{h+1}^{*}(\tau_{h+1})\Big{(}(1-\widehat{\lambda} _{h})\widehat{p}_{h}(s_{h+1}|s_{t_{h}},\mathbf{a}_{t_{h}:h})-(1-\lambda_{h}) \widehat{p}_{h}(s_{h+1}|s_{t_{h}},\mathbf{a}_{t_{h}:h})\] \[\quad+(1-\lambda_{h})\widehat{p}_{h}(s_{h+1}|s_{t_{h}},\mathbf{a }_{t_{h}:h})-(1-\lambda_{h})p_{h}(s_{h+1}|s_{t_{h}},\mathbf{a}_{t_{h}:h})\Big{)}\] \[=\sum_{s_{h+1}}V_{h+1}^{*}(\tau_{h+1})(1-\lambda_{h})\left( \widehat{p}_{h}(s_{h+1}|s_{t_{h}},\mathbf{a}_{t_{h}:h})-p_{h}(s_{h+1}|s_{t_{h} },\mathbf{a}_{t_{h}:h})\right)\] \[\quad+\sum_{s_{h+1}}V_{h+1}^{*}(\tau_{h+1})(\lambda_{h}-\widehat {\lambda}_{h})\widehat{p}_{h}(s_{h+1}|s_{t_{h}},\mathbf{a}_{t_{h}:h})\] \[\overset{(i)}{\geq}-c_{A}H\sqrt{\frac{H\iota}{N_{h}(\tau_{h},a_{h })}}-H\left|\widehat{\lambda}_{h}-\lambda_{h}\right|,\]

where inequality \((i)\) invokes Hoeffding's inequality and holds with probability \(1-\gamma\) for any \(\tau_{h},a_{h}\) and some constant \(c_{A}\). Term \((A_{2})\) is immediately bounded by

\[(A_{2})\geq-H\left|\widehat{\lambda}_{h}-\lambda_{h}\right|.\]

Putting \((A_{1})\) and \((A_{2})\) together, we derive

\[(A)\geq-c_{A}H\sqrt{\frac{H\iota}{N_{h}(\tau_{h},a_{h})}}-2H\left|\widehat{ \lambda}_{h}-\lambda_{h}\right|\]

with high probabilty. For term \((B)\), we have

\[(B)=\sum_{s_{h}}r(s_{h},a_{h})\left(\widehat{\mathfrak{b}}_{h}(s_{h}|\tau_{h})- \mathfrak{b}_{h}(s_{h}|\tau_{h})\right)\geq-c_{B}\sqrt{\frac{H\iota}{N_{h}( \tau_{h},a_{h})}}.\]Taking \(c=c_{A}+c_{B}\) and summing up \((A)\) and \((B)\), we have

\[Q_{h}(\tau_{h},a_{h})-Q_{h}^{*}(\tau_{h},a_{h})\geq-cH\sqrt{\frac{H \iota}{N_{h}(\tau_{h},a_{h})}}-2H\left|\widehat{\lambda}_{h}-\lambda_{h}\right|+ b_{h}^{k}(\tau_{h},a_{h}).\]

We estimate \(\lambda_{h}\) by its empirical average. In episode \(k\geq 1\), we have access to \(k\) i.i.d. realizations of Bernoulli random variable with rate \(\lambda_{h}\) (observable or not). Therefore, by Hoeffding's inequality, we have

\[\left|\widehat{\lambda}_{h}^{k}-\lambda_{h}\right|\leq 2\sqrt{\frac{ \log\frac{HK}{\gamma}}{k}}\leq 2\sqrt{\frac{\iota}{k}}.\]

Substituting into \(Q_{h}^{k}(\tau_{h},a_{h})-Q_{h}^{*}(\tau_{h},a_{h})\) and reloading constant \(c\) give rise to

\[Q_{h}^{k}(\tau_{h},a_{h})-Q_{h}^{*}(\tau_{h},a_{h})\geq-cH\left( \sqrt{\frac{H\iota}{N_{h}^{k}(\tau_{h},a_{h})}}+\sqrt{\frac{\iota}{k}}\right) +b_{h}^{k}(\tau_{h},a_{h})\geq 0.\]

The proof is complete. 

Regret analysisWe omit subscript "aug" to ease the presentation. The same derivation in the proof of Theorem 4.1 gives rise to

\[(Q_{h}^{*}-Q_{h}^{\pi_{k}})\left(\tau_{h}^{k},a_{h}^{k}\right) \leq\left(Q_{h}^{k}-Q_{h}^{\pi_{k}}\right)(\tau_{h}^{k},a_{h}^{k})\] \[\leq\underbrace{\left([\widehat{\mathcal{P}}_{h}^{k}-\mathcal{P}_ {h}][V_{h+1}^{k}-V_{h+1}^{*}]\right)(\tau_{h}^{k},a_{h}^{k})}_{(A)}+\left( \mathcal{P}_{h}[V_{h+1}^{k}-V_{h+1}^{\pi_{k}}]\right)(\tau_{h}^{k},a_{h}^{k}) +2b_{h}^{k}(\tau_{h}^{k},a_{h}^{k}).\] (C.1)

Lemma C.1 shows that \((A)\) can be written as

\[(A) =\sum_{s_{h+1}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1})(1-\lambda_{h })\left(\widehat{p}_{h}^{k}(s_{h+1}|s_{t_{h}}^{k},\mathbf{a}_{t_{h}:h}^{k})-p _{h}(s_{h+1}|s_{t_{h}}^{k},\mathbf{a}_{t_{h}:h}^{k})\right)\] \[\quad+\sum_{s_{h+1}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1})(\lambda _{h}-\widehat{\lambda}_{h}^{k})\widehat{p}_{h}^{k}(s_{h+1}|s_{t_{h}}^{k}, \mathbf{a}_{t_{h}:h}^{k})\] \[\leq\sum_{s_{h+1}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{h+1})(1-\lambda _{h})\left(\widehat{p}_{h}^{k}(s_{h+1}|s_{t_{h}}^{k},\mathbf{a}_{t_{h}:h}^{k} )-p_{h}(s_{h+1}|s_{t_{h}}^{k},\mathbf{a}_{t_{h}:h}^{k})\right)+H\left|\widehat{ \lambda}_{h}^{k}-\lambda_{h}\right|\] \[\leq(1-\lambda_{h})\sum_{s_{h+1}}[V_{h+1}^{k}-V_{h+1}^{*}](\tau_{ h+1})\left(\widehat{p}_{h}^{k}(s_{h+1}|s_{t_{h}}^{k},\mathbf{a}_{t_{h}:h}^{k})-p _{h}(s_{h+1}|s_{t_{h}}^{k},\mathbf{a}_{t_{h}:h}^{k})\right)+2H\sqrt{\frac{ \iota}{k}}.\]

Following the derivation in (B.4), (B.5) and (B.6), we have

\[\mathtt{Regret}(K) \leq e\sum_{k=1}^{K}\sum_{h=1}^{H}\left(\xi_{h}^{k}+\zeta_{h}^{k} +2b_{h}^{k}+2H\sqrt{\frac{\iota}{k}}\right)\] \[\leq e\sum_{k=1}^{K}\sum_{h=1}^{H}\left(\xi_{h}^{k}+\zeta_{h}^{k} +2b_{h}^{k}\right)+2\sqrt{H^{4}K\iota}.\]

where \(\xi_{h}^{k}=\left(\mathcal{P}_{h}\left[V_{h}^{k}-V_{h}^{\pi_{k}}\right]\right)( \tau_{h}^{k},a_{h}^{k})-\left[V_{h+1}^{k}-V_{h+1}^{\pi_{k}}\right](\tau_{h+1}^ {k})\) is the martingale difference and \(\zeta_{h}^{k}=c^{\prime}\frac{SH^{2}\iota}{N_{h}^{k}(\tau_{h}^{k},a_{h}^{k})}\).

Counting number summationThe summation over \(\xi_{h}^{k}\) is standard. Using the Azuma-Hoeffding's inequality, we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\xi_{h}^{k}\leq c_{\xi}\sqrt{KH^{4} \iota}.\]It remains to find the summations involving \(N_{h}^{k}(\tau_{h}^{k},a_{h}^{k})\). First, we show that the event \(\mathcal{E}_{m}=\{h-t_{h}-1\leq m\}\), i.e., the maximal consecutive delay is upper bounded by \(m>0\), holds with high probability. We have

\[\mathbb{P}(\mathcal{E}_{m})\leq\left(1-H(1-\lambda_{0})^{m+1}\right)^{K},\]

since \(\lambda_{0}\) is a uniform lower bound of \(\lambda_{h}\). Next, we provide an upper bound on \(N_{h}^{K}(\tau_{h},a_{h})\). For a given tuple \((h,\tau_{h},a_{h},t_{h})\), the consecutive missing length is \(h-t_{h}-1\). Such a missing pattern appears with probability at most \((1-\lambda_{0})^{h-t_{h}-1}\). As a consequence, denote \(C_{h-t_{h}-1}^{K}\) as the number of \(h-t_{h}-1\) consecutive missings in \(K\) episodes. With probability \(1-\gamma\), we have

\[C_{h-t_{h}-1}^{K}\leq K(1-\lambda_{0})^{h-t_{h}-1}+\sqrt{K(1-\lambda_{0})^{h- t_{h}-1}H}\iota.\]

by Bernstein's inequality in Lemma D.1. Furthermore, at a fixed time \(h\), we use Lemma C.3 to bound the gap between two consecutive appearances of the same missing pattern. We instantiate Lemma C.3 with \(\theta=(1-\lambda_{0})^{h-t_{h}-1}\) and obtain that the gap is bounded by \(\left\lceil\frac{\iota}{-\log(1-(1-\lambda_{0})^{h-t_{h}-1})}\right\rceil\) with probability \(1-\gamma\). Within the gap, the number of consecutive delays of length larger than \(h-t_{h}-1\) is bounded by

\[C_{\geq h-t_{h}-1} \overset{(i)}{\leq}\left\lceil\frac{\iota}{-\log(1-(1-\lambda_{0 })^{h-t_{h}-1})}\right\rceil(1-\lambda_{0})^{h-t_{h}}\] \[\quad+\sqrt{\left\lceil\frac{\iota}{-\log(1-(1-\lambda_{0})^{h-t _{h}-1})}\right\rceil(1-\lambda_{0})^{h-t_{h}}H}\iota+\iota\] \[\overset{(ii)}{\leq}\sqrt{2(1-\lambda_{0})H}\iota+2(1-\lambda_{0 })+\iota,\]

where inequality \((i)\) follows from Bernstein's inequality again and inequality \((ii)\) invokes the fact \(x+\log(1-x)\leq 0\) for \(x\in[0,1)\) and bounds \(\lceil x\rceil\) by \(x+1\). Now we can bound the summation of the counting numbers. Conditioned on the event \(\mathcal{E}_{m}\), we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\sqrt{\frac{1}{N_{h}^{k}(\tau_{h}^{k},a_{h}^{k})}} \overset{(i)}{\leq}\sum_{(h,\tau,a,t_{h})}C_{\geq h-t_{h}-1}\sum_ {i=1}^{N_{h}^{K}(\tau,a)}\sqrt{\frac{1}{i}}\] \[\leq 2\left(\sqrt{2(1-\lambda_{0})H}\iota+2(1-\lambda_{0})+\iota \right)\sum_{(h,\tau,a,t_{h})}\sqrt{N_{h}^{K}(\tau,a)}\] \[\overset{(ii)}{\leq}2\left(\sqrt{2(1-\lambda_{0})H}\iota+2(1- \lambda_{0})+\iota\right)\sum_{h,t_{h}}\sqrt{SA^{h-t_{h}}C_{h-t_{h}-1}^{K}}\] \[\leq 2\left(\sqrt{2(1-\lambda_{0})H}\iota+2(1-\lambda_{0})+\iota \right)\] \[\quad\cdot\sum_{h,t_{h}}\sqrt{SA\left(K((1-\lambda_{0})A)^{h-t_{h }-1}+\sqrt{K(A^{2}(1-\lambda_{0}))^{h-t_{h}-1}H}\iota+A^{h-t_{h}-1}\iota\right)}\] \[\overset{(iii)}{\leq}2\left(\sqrt{2(1-\lambda_{0})H}\iota+2(1- \lambda_{0})+\iota\right)\sum_{h,t_{h}}\sqrt{SA\left(K+\sqrt{KA^{m}H}\iota+A^ {m}\iota\right)}\] \[\leq 2\left(\sqrt{2(1-\lambda_{0})H}\iota+2(1-\lambda_{0})+\iota \right)H^{2}\sqrt{SA\left(K+\sqrt{KA^{m}H}\iota+A^{m}\iota\right)}\] \[\leq 2\sqrt{H^{5}SA\iota^{2}\left(K+\sqrt{KA^{m}H}\iota+A^{m} \iota\right)},\]

where inequality \((i)\) follows since \(N_{h}^{k}\) is repeated at most \(C_{\geq h-t_{h}-1}\) times before getting an update and inequality \((ii)\) follows from Cauchy-Schwarz inequality, and inequality \((iii)\) invokes the assumption of \(\lambda A\leq 1\). Moreover, conditioned on the event \(\mathcal{E}_{m}\), we also have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{N_{h}^{k}(\tau_{h}^{k},a_{h}^{k})}\leq\sum _{(h,\tau,a,t_{h})}C_{\geq h-t_{h}-1}\sum_{i=1}^{N_{h}^{K}(\tau,a)}\frac{1}{i}\]\[\leq\left(\sqrt{2(1-\lambda_{0})H\iota}+2(1-\lambda_{0})+\iota\right) \sum_{(h,\tau,a,t_{h})}\log N_{h}^{K}(\tau,a)\] \[\leq\iota H^{5/2}SA^{m+1}\log K.\]

Combining the aboveOn event \(\mathcal{E}_{m}\), the regret is bounded by

\[\mathtt{Regret}(K) \overset{(i)}{\leq}c\left(\sqrt{H^{4}K\iota}+\sum_{k=1}^{K}\sum_ {h=1}^{H}\left[\frac{SH^{2}\iota}{N_{h}^{k}(\tau_{h}^{k},a_{h}^{k})}+H\sqrt{ \frac{H\iota}{N_{h}^{k}(\tau_{h}^{k},a_{h}^{k})}}\right]\right)\] \[\leq c\left(H^{4}\sqrt{SA\iota^{3}K\left(1+\sqrt{\frac{A^{m}H\iota }{K}}+\frac{A^{m}\iota}{K}\right)}+S^{2}A^{m}\sqrt{H^{9}\iota^{6}}+\sqrt{H^{4} K\iota}\right),\]

where \(c\) is a sufficiently large constant and we substitute the bonus functions into inequality \((i)\).

On the complement of \(\mathcal{E}_{m}\), the regret is bounded by \(H(1-\mathbb{P}(\mathcal{E}_{m}))\leq H^{2}K(1-\lambda_{0})^{m+1}\). We choose \(m=\frac{1}{2}\left\lfloor\frac{\log K}{-\log(1-\lambda_{0})}\right\rfloor\) such that \(H(1-\mathbb{P}(\mathcal{E}_{m}))\leq H^{2}K(1-\lambda_{0})^{m+1}\leq H^{2} \sqrt{K}\). We can now check that \(A^{m+1}=\exp\left(\frac{\log A}{-\log(1-\lambda_{0})}\log\sqrt{K}\right)\leq K ^{\frac{1}{2(1+\iota)}}\). Therefore, combining the regret on event \(\mathcal{E}_{m}\) and the complement event \(\mathcal{E}_{m}^{\mathcal{E}}\) leads to

\[\mathtt{Regret}(K)\leq c\left(H^{4}\sqrt{SAK\iota^{3}}+S^{2}\sqrt{H^{9}K^{ \frac{1}{(1+\upsilon)}}\iota^{6}}\right).\]

The proof is complete. 

### Supporting lemmas

Lemma C.2.: Suppose Assumption 2.2 holds. With probability \(1-\gamma\) for some failure probability \(\gamma>0\), we have

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{\sqrt{N_{h}^{k}(s_{h}^{k},a_{h}^{k})}} \leq\left\lceil\frac{\log\frac{HK}{\gamma}}{-\log(1-\lambda_{0}^{2})}\right\rceil \sqrt{SAKH}.\]

Proof of Lemma c.2.: For any time \(h\), we denote \(\mathcal{K}^{\mathrm{eff}}(h)\) as the collection of episodes that the \(h\)-th and \((h+1)\)-th step observations are available. It is clear that the cardinality of \(\mathcal{K}^{\mathrm{eff}}(h)\) is bounded by \(K\) for any \(h\). Within each \(\mathcal{K}^{\mathrm{eff}}(h)\), we would like to bound the gap between two observations. Thanks to Lemma C.3, the gap is bounded by \(q\) with probability \(1-K(1-\lambda_{0}^{2})^{q+1}\). We set \(K(1-\lambda_{0}^{2})^{q+1}=\gamma/H\), which implies \(q=\left\lceil\frac{\log\frac{HK}{\gamma}}{-\log(1-\lambda_{0}^{2})}\right\rceil\). Therefore, for any time step \(h\), available observations are at most separated by \(q\) episodes.

With these notations, we bound

\[\sum_{k=1}^{K}\sum_{h=1}^{H}\frac{1}{\sqrt{N_{h}^{k}(s_{h}^{k},a_ {h}^{k})}} \overset{(i)}{\leq}\left\lceil\frac{\log\frac{HK}{\gamma}}{-\log (1-\lambda_{0}^{2})}\right\rceil\sum_{h=1}^{H}\sum_{k\in\mathcal{K}^{\mathrm{ eff}}(h)}\frac{1}{\sqrt{N_{h}^{k}(s_{h}^{k},a_{h}^{k})}}\] \[\overset{(ii)}{\leq}\left\lceil\frac{\log\frac{HK}{\gamma}}{-\log (1-\lambda_{0}^{2})}\right\rceil\sum_{h=1}^{H}\sum_{k=1}^{K}\frac{1}{\sqrt{N_{h }^{k}(s_{h}^{k},a_{h}^{k})}}\] \[\overset{(iii)}{\leq}2\left\lceil\frac{\log\frac{HK}{\gamma}}{- \log(1-\lambda_{0}^{2})}\right\rceil\sqrt{SAHK},\]

where inequality \((i)\) follows since \(N_{h}^{k}\) will only be updated when \(h\in\mathcal{K}^{\mathrm{eff}}(h)\) and then repeat at most \(\left\lceil\frac{\log\frac{HK}{\gamma}}{-\log(1-\lambda_{0}^{2})}\right\rceil\) times, inequality \((ii)\) invokes the cardinality bound of \(\mathcal{K}^{\mathrm{eff}}(h)\), and inequality \((iii)\) follows from the standard pigeon-hole principle.

**Lemma C.3**.: Let \(\{u_{i}\}_{i=1}^{k}\) be i.i.d. Bernoulli random variables. Suppose \(\mathbb{P}(u_{i}=1)=\theta\). Define the largest gap between \(u_{i}\)'s as

\[g(k)=\sup\{j-i:u_{i}=0\text{ and }u_{j}=0\text{ with }u_{\ell}=1\text{ for }\ell=i+1,\ldots,j-1\}.\]

Then for any integer \(q>0\), the following tail probability bound holds

\[\mathbb{P}(g(k)>q)\leq k\theta^{q+1}.\]

Proof of Lemma c.3.: We denote \(I_{\mathrm{neg}}=\{\ell_{1},\ldots,\ell_{m}\}\) as the index set for \(u_{\ell_{i}}=0\) when \(i=1,\ldots,|I_{\mathrm{neg}}|\). Let \(v_{j}=\ell_{j+1}-\ell_{j}\), which is a geometric random variable with a success rate \(\theta\). Note that the cardinality of \(I_{\mathrm{neg}}\) is at most \(k\). Therefore, we have

\[\mathbb{P}(g(k)>q) \leq\mathbb{P}(\max_{j=1,\ldots,k}v_{j}>q)\] \[=1-\mathbb{P}\left(v_{j}\leq q\text{ for }j=1,\ldots,k\right)\] \[=1-\left(1-\theta^{q+1}\right)^{k}\] \[\leq k\theta^{q+1},\]

where the last inequality follows from \(1-k\theta^{q+1}\leq(1-\theta^{q+1})^{k}\). 

## Appendix D Helper concentration inequalities

**Lemma D.1** (Bernstein's inequality).: Let \(x_{1},\ldots,x_{n}\) be i.i.d. zero mean random variables. Suppose \(|x_{i}|\leq M\) for any \(i=1,\ldots,n\). Then for all positive \(t\), it holds

\[\mathbb{P}\left(\sum_{i=1}^{n}x_{i}>t\right)\leq\exp\left(-\frac{\frac{1}{2}t^ {2}}{\sum_{i=1}^{n}\mathrm{Var}[x_{i}]+\frac{1}{3}Mt}\right).\]

In particular, given a failure probability \(\gamma<1\), it holds

\[\mathbb{P}\left(\sum_{i=1}^{n}x_{i}>\sqrt{\sum_{i=1}^{n}\mathrm{Var}[x_{i}] \log\frac{1}{\gamma}}+M\log\frac{1}{\gamma}\right)\leq\gamma.\]

Proof of Lemma d.1.: The proof of Bernstein's inequality is standard, see for example [Wainwright, 2019, Section 2.1]. Here we verify the second claim. Let \(\exp\left(-\frac{\frac{1}{2}t^{2}}{\sum_{i=1}^{n}\mathrm{Var}[x_{i}]+\frac{1} {3}Mt}\right)\leq\gamma\) hold true. We find a suitable \(t\) by

\[\exp\left(-\frac{\frac{1}{2}t^{2}}{\sum_{i=1}^{n}\mathrm{Var}[x_{i }]+\frac{1}{3}Mt}\right)\leq\gamma\] \[\iff \frac{\frac{1}{2}t^{2}}{\sum_{i=1}^{n}\mathrm{Var}[x_{i}]+\frac{1} {3}Mt}\geq\log\frac{1}{\gamma}\] \[\iff t^{2}-\frac{2}{3}tM\log\frac{1}{\gamma}\geq\sum_{i=1}^{n} \mathrm{Var}[x_{i}]\log\frac{1}{\gamma}\] \[\iff t\geq\sqrt{\sum_{i=1}^{n}\mathrm{Var}[x_{i}]\log\frac{1}{ \gamma}+\frac{1}{9}M^{2}\log^{2}\frac{1}{\gamma}}+\frac{1}{3}M\log\frac{1}{ \gamma}.\]

It is enough to choose \(t=\sqrt{\sum_{i=1}^{n}\mathrm{Var}[x_{i}]\log\frac{1}{\gamma}}+M\log\frac{1}{\gamma}\). 

**Lemma D.2** (Hoeffding's inequality).: Let \(x_{1},\ldots,x_{n}\) be i.i.d. random variables. Suppose \(a_{i}\leq x_{i}\leq b_{i}\) for any \(i=1,\ldots,n\). Then for all positive \(t\), it holds

\[\mathbb{P}\left(\left|\sum_{i=1}^{n}x_{i}-\mathbb{E}\left[\sum_{i=1}^{n}x_{i} \right]\right|>t\right)\leq 2\exp\left(-\frac{2t^{2}}{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}} \right).\]In particular, given a failure probability \(\gamma<1\), it holds

\[\mathbb{P}\left(\frac{1}{n}\left|\sum_{i=1}^{n}x_{i}-\mathbb{E}\left[\sum_{i=1}^{ n}x_{i}\right]\right|>\sqrt{\frac{\sum_{i=1}^{n}(b_{i}-a_{i})^{2}\log\frac{2}{ \gamma}}{2n^{2}}}\right)\leq\gamma.\]

Proof of Lemma D.2.: The proof is standard; see [20, Section 2.1]. 

**Lemma D.3** (Azuma-Hoeffding's inequality).: Let \(x_{1},\ldots,x_{n}\) be a martingale adapted to filtration \(\mathcal{F}_{1}\subset\cdots\subset\mathcal{F}_{n}\). Suppose \(\mathbb{E}[x_{i}-\mathbb{E}[x_{i}||\mathcal{F}_{i-1}]=0\) and \(|x_{i}-\mathbb{E}[x_{i}||\leq c_{i}\). Then for all positive \(t\), it holds

\[\mathbb{P}\left(\sum_{i=1}^{n}x_{i}-\mathbb{E}[x_{i}]>t\right)\leq\exp\left(- \frac{t^{2}}{2\sum_{i=1}^{n}c_{i}^{2}}\right).\]

In particular, given a failure probability \(\gamma<1\), it holds

\[\mathbb{P}\left(\sum_{i=1}^{n}x_{i}-\mathbb{E}[x_{i}]>\sqrt{2\sum_{i=1}^{n}c_ {i}^{2}\log\frac{1}{\gamma}}\right)\leq\gamma.\]

Proof of Lemma D.3.: The proof is standard and applies Lemma D.2.