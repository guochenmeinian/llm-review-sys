Repetition In Repetition Out: Towards Understanding Neural Text Degeneration from the Data Perspective

 Huayang Li\({}^{\heartsuit}\)   Tian Lan\({}^{\clubsuit}\)   Zihao Fu\({}^{\clubsuit}\)   Deng Cai\({}^{\clubsuit}\)

Lemao Liu\({}^{\clubsuit}\)   Nigel Collier\({}^{\clubsuit}\)   Taro Watanabe\({}^{\heartsuit}\)   Yixuan Su\({}^{\clubsuit,\diamondsuit}\)

\({}^{\heartsuit}\)Nara Institute of Science and Technology  \({}^{\clubsuit}\)Tencent AI Lab

\({}^{\clubsuit}\)University of Cambridge  \({}^{\diamondsuit}\)Cohere

{li.huayang.lh6, taro}@is.naist.jp lantiangmftby@gmail.com {jcykcai, redmondliu}@tencent.com{zf268, nhc30, ys484}@cam.ac.uk

###### Abstract

There are a number of diverging hypotheses about the neural text degeneration problem, i.e., generating repetitive and dull loops, which makes this problem both interesting and confusing. In this work, we aim to advance our understanding by presenting a straightforward and fundamental explanation from the data perspective. Our preliminary investigation reveals a strong correlation between the degeneration issue and the presence of repetitions in training data. Subsequent experiments also demonstrate that by selectively dropping out the attention to repetitive words in training data, degeneration can be significantly minimized. Furthermore, our empirical analysis illustrates that prior works addressing the degeneration issue from various standpoints, such as the high-inflow words, the likelihood objective, and the self-reinforcement phenomenon, can be interpreted by one simple explanation. That is, penalizing the repetitions in training data is a common and fundamental factor for their effectiveness. Moreover, our experiments reveal that penalizing the repetitions in training data remains critical even when considering larger model sizes and instruction tuning. Our code is available at https://github.com/gmftbyQMFTBV/Rep-Dropout.

## 1 Introduction

The emergence of neural language models (LM) has led to significant achievements in various text generation tasks, such as machine translation [3, 24, 31], summarization [22], and open-ended text generation [17, 20]. However, in open-ended text generation, neural LMs exhibit a strikingly severe degeneration issue, producing unreasonably repetitive texts, particularly when employing maximum _a posteriori_ (MAP) decoding algorithms. As illustrated in Fig. 1, even a well-trained LM [20] may suffer from a severe degeneration issue.

There have been numerous attempts to explain the phenomenon of neural text degeneration, with many attributing this problem to flaws in the learning process. Fu et al. [8] claim that high-inflow words increase the probability of generating repetitions. A collection of studies [11, 15, 26, 34] argue that the likelihood objective is the primary factor, because it has the problem of exposure bias and focuses more on next-token prediction rather than sequence generation. Meanwhile, both Chiang and Chen [4] and Xu et al. [36] assert that the self-reinforcement mechanism can make neural LMs fall into the repetitive loops. Intriguingly, despite the divergence in these explanations, all corresponding methods proposed have been shown to effectively alleviate the text degeneration issue. This observation leads us to ponder: _could there exist more fundamental factors that can explain the degeneration issue?_

In this study, we strive to provide a straightforward and fundamental interpretation for the degeneration problem from the data perspective. Our preliminary investigation reveals that repetitive words in the training data play a crucial role in the issue. It is also worth noting that repetitions are not necessarily of low quality, because it is a natural and common phenomenon in human writing [1; 12; 28]. Across five datasets with different domains, we observe a strong correlation between repetition rates in training and generated text. This finding encourages us to further assess the impact of repetitive words in training data by randomly dropping out attention to them during training. Employing repetition dropout, we discover that the repetition rate in generated text can be substantially reduced. Lastly, we reconcile many previous hypotheses with our single explanation, asserting that penalizing repetitions in training data is a key factor to the success of alleviating the degeneration issue.

As large language models (LLMs) gain popularity, it appears that the degeneration issue has been somewhat solved. We investigate the impact of various factors associated with LLMs on degeneration, including increasing model size and training models using instruction-tuning data. Our experiments reveal that penalizing repetitions in training data continues to be crucial in the context of LLMs.

Our contributions are threefold:

* We demonstrate that the proportion of repetitive words in training data has a significant influence on the degeneration issue. Inspired by this finding, we also propose a method, namely, _repetition dropout_, to mitigate the degeneration.
* We find that penalizing repetitions in data is a more fundamental factor for reducing repetitions, which also provides a unified explanation for various existing hypotheses, such as the attributions to high-inflow words, likelihood objectives, and self-reinforcement phenomenon.
* We investigate the influence of factors associated with large language models on reducing repetitions, including model scale and instruction tuning.

## 2 Related Work

The degeneration (or repetition) issue in neural language models (LM), particularly in open-ended text generation, has garnered significant attention in recent years. Previous works have proposed disparate interpretations and solutions for this issue.

Many researchers hypothesize that factors within the learning process contributes to the degeneration issue. Fu et al. [8] assert that the high-inflow words in training data may increase the probability of generating repetitions, where the inflow of a word is defined as the probability sum of all its preceding words. Another factor identified is the likelihood objective. Welleck et al. [34] argue that likelihood objective has a discrepancy with the MAP decoding and focuses more on next-token prediction rather than sequence generation. Thus, they proposed an unlikelihood objective to address the two flaws. Based on the same principle, Lin et al. [15] propose to scale the gradient of specified non-novel tokens, i.e., words in the prefix context at each time step, to alleviate the degeneration issue. Su et al. [26] and Jiang et al. [11] pointed out that the degeneration issue may caused by the high similarity between token representations. Thus, they leveraged contrastive learning to learn a more distinct

Figure 1: Illustration of repetitions in human and generated text. The human text is from Wikitext-103, and generated text is by greedy search using the GPT-2 model trained on Wikitext-103. The underlined text is the prompt for generation. The blue words and red words indicate the repetitions in human text and machine-generated text, respectively.

representation for each token. In addition, both Xu et al. [36] and Chiang and Chen [4] argue that degeneration is caused by the self-reinforcement phenomenon and Xu et al. [36] address this issue by penalizing the repetitions in pseudo repetitive data.

Apart from issues within the learning process, other explanations for degeneration have been proposed. Many researchers contend that decoding methods [7; 10; 16; 20; 37] is the primary factor. Holtzman et al. [10] argue that word probabilities in human-generated text exhibit high variance and randomness, while high-probability text produced by MAP decoding methods tends to be repetitive and dull. This observation explains why sampling-based decoding methods [7; 10; 16; 20] can substantially mitigate the degeneration issue. Riley and Chiang [21] find that tasks with lower constraints, or larger solution spaces, suffer from more severe degeneration issue. The model architecture [8; 32; 34] and size [14] may also contribute, but the two factors have not been quantitatively evaluated.

However, with so many explanations, understanding the primary cause of the degeneration issue becomes increasingly challenging. In our work, we strive to provide a fundamental explanation for the previous hypotheses in the learning process. While our work may not encompass all the existing explanations, we believe it lays a solid foundation for further understanding of the degeneration issue.

## 3 Background

Language model (LM) aims to estimate the probability of a sentence in natural language according to the chain rule of probability:

\[P(\bm{x}) =P(x_{1})P(x_{2}|x_{1})\dots P(x_{L}|\bm{x}_{1:L-1})\] \[=P(x_{1})\prod_{i=2}^{L}P(x_{i}|\bm{x}_{1:i-1}),\] (1)

where \(\bm{x}=\text{$<\!\!x_{1},\dots,x_{L}$}\textgreater\) is a sequence of words with length \(L\), and \(\bm{x}_{1:i-1}\) is the previous \(i-1\) words, i.e., the context, for predicting word \(x_{i}\).

ModelSince attention-based LM [20; 31] has became the backbone of many tasks, we will use GPT-2 model [20] as the main architecture for our empirical studies. The core of GPT-2 model1 is to use the attention mechanism to update the representation of words in \(\bm{x}\):

Footnote 1: GPT-2 also incorporates essential sub-layers such as residual connections and layer normalization. While the model employs a multi-head attention mechanism, we have omitted details for brevity. For a comprehensive explanation of these sub-layers, refer to Vaswani et al. [31] and Radford et al. [20].

\[\mathrm{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V})=\mathrm{softmax}\Big{(} \frac{\mathbf{Q}\mathbf{K}^{T}+\mathbf{M}}{\beta}\Big{)}\mathbf{V},\] (2)

where \(\beta\) is a scalar to control the scale of the attention score, \(\mathbf{Q}\), \(\mathbf{K}\), and \(\mathbf{V}\) are the linear transformation of \(\bm{h}_{1:L}\in\mathbb{R}^{L\times d}\) using matrices \(\mathbf{W}_{q}\), \(\mathbf{W}_{k}\), and \(\mathbf{W}_{v}\in\mathbb{R}^{d\times d}\), respectively. \(\bm{h}_{1:L}\) is the current hidden representation of words \(\bm{x}_{1:L}\), and \(d\) is the hidden size. \(\mathbf{M}\) is masking matrix that makes sure only the information of \(\bm{h}_{1:i}\) is accessible for \(\bm{h}_{i}\) at each time step \(i\). If the word \(x_{i}\) can perceive the information of \(x_{j}\), then \(\mathbf{M}[i][j]\) equals to \(0\), otherwise \(-\infty\).

TrainingGenerally, neural LMs are trained by optimizing the likelihood objective:

\[\mathcal{L} =-\sum_{\bm{x}\in\mathcal{D}}\sum_{i=2}^{L}\log P(x_{i}|\bm{x}_{1 :i-1};\theta)\] \[=-\sum_{\bm{x}\in\mathcal{D}}\sum_{i=2}^{L}\log\mathrm{softmax}( \phi(\mathbf{H}_{i-1}))[x_{i}],\] (3)

where \(\bm{h}_{i-1}\in\mathbb{R}^{d}\) is the hidden vector of \(x_{i-1}\) output by the last layer of an neural LM, e.g., the GPT-2 model [20]. The \([x_{i}]\) is defined as taking the probability regarding to \(x_{i}\) in the distribution got from \(\mathrm{softmax}\). The \(\phi(\cdot)\) is a linear layer that transforms the \(\bm{h}_{i-1}\) to logits.

InferenceSince neural LMs are trained to maximize the likelihood objective, one intuitive practice for inference is to use the MAP decoding method, e.g., greedy search or beam search. However, in open-ended text generation, this tactic will cause an extremely severe degeneration issue on vanilla neural LMs trained by likelihood objective [10].

EvaluationThe main focus of this empirical study is to investigate the reason for the repetition issue. Therefore, the rep-\(n\) is an important evaluation metric in our work, following previous works [8; 25; 27; 34]:

\[\text{rep-}n=1.0-\frac{|\mathrm{UniqueNgrams}(\bm{x},n)|}{L-n+1}\] (4)

where \(n\) is the length of \(n\)-gram, and \(\mathrm{UniqueNgrams}\) is a function to find all unique \(n\)-grams in a sentence \(\bm{x}\). For the corpus-level evaluation, we report the averaged rep-\(n\) scores of instances in the dataset. To ensure that our results in main experiments are not biased to rep-\(n\), we also report the results of rep-\(w\) and rep-\(r\) in previous works [8; 34]. The rep-\(w=\frac{1}{L}\sum_{t=1}^{L}\mathds{1}\{x_{t}\in\bm{x}_{t-w-1:t-1}\}\), which measures the word-level repetition in a prefix window with length \(w\). In our experiments, we set \(w\) to 16, following Fu et al. [8]. The rep-\(r=\frac{1}{L}|\{i|(x_{i}=x_{j}\wedge x_{i+1}=x_{j+1},\exists j\neq i)\lor(x_{i} =x_{k}\wedge x_{i-1}=x_{k-1},\exists k\neq i)\}|\). It is for the portion of repetitive snippets measured by sentence length.

In addition to the measurement of repetition, we also consider the perplexity (PPL) on the real data, which demonstrates the performance in language modeling [2]. Although LMs with lower PPL may not consistently lead to better generation results, it is able to reflect trivial solutions for the degeneration issue, e.g., random or over-fitting models.

## 4 Preliminary Study: Rethinking Neural Text Degeneration from Data Perspective

The propensity of neural LMs with impressive performance to fall into naive repetitive loops is puzzling. Although numerous hypotheses have been proposed, many of them approach this issue from divergent aspects, and some are not intuitive for understanding. According to Ockham's Razor, a simpler explanation is often preferable. Therefore, in our preliminary study, we start by evaluating one elementary factor for most AI systems, the training data. Concretely, repetition is a natural and common phenomenon in human languages for various reasons [1; 12; 28]. It is intriguing to investigate whether there are connections between valid repetitions in natural language and incorrect repetitions in generated language. Moreover, it is important to note that data containing repetitions is not necessarily of low quality, as shown in Fig. 1.

Figure 2: (a) Relationship between rep-\(2\) scores of human and generated text. (b) Relationship between the scale of an LM and the rep-\(2\) score of generated text. Note that results with the same symbol in Fig. 2 are from models trained on different shards of the corresponding dataset. The rep-\(2\) score is defined in Eq. (4). We use the GPT-\(2\) and OPT LMs for Fig. 2 and 2, respectively, and use greedy search as the decoding method.

SetupTo assess the correlation between repetitions in generated text and those in human text, we propose to train GPT-2 models [20] on data with varying rep-\(2\) scores and then evaluate the rep-\(2\) scores of the text generated by the corresponding models. Specifically, we sorted the training instances in each dataset \(\mathcal{D}\) based on their rep-\(2\) scores. We then divided the sorted training data into six shards, each containing an equal number of words but a varying percentage of repetitions, and trained a GPT-2 model on each shard. Notably, we will use the full test set of a dataset \(\mathcal{D}\) to evaluate the models trained on different shards of \(\mathcal{D}\). More implementation details are in Appx. A.

Our preliminary study investigates five datasets across various domains. Wikitext-103 is a widely used dataset for language modeling [2, 5, 13] and open-ended generation [8, 34]. We adopt the standard split for training, validation, and test sets. The remaining four datasets, OpenWebText2, FreeLaw, PubMed, and ArXiv, are part of the Pile dataset [9]. To ensure consistent analysis, we sample an equivalent number of words as the Wikitext-103 dataset from each of the four Pile datasets. For the validation and test sets, we randomly sample 2,000 sentences from each of the four Pile datasets. The training, validation, and test sets are non-overlapping across all five datasets.

FindingsFig. 2(a) demonstrates a strong correlation between the rep-2 scores of human and generated text on each dataset, indicating that the degeneration issue becomes more severe as the percentage of repetitions in training data increases. However, due to varying data distributions across datasets from different domains, the rep-2 score of generated text may vary even when trained on data with the same rep-2 score. Another intriguing observation is that neural LM will amplify the repetition in data by more than 10 times, similar to the bias amplification found in Zhao et al. [39].

Investigations Related to Large Language ModelsAs LLMs, e.g., ChatGPT [17] and Llama[30], gain popularity, it appears that the degeneration issue has been somewhat solved. In this section, we will investigate the impact of various factors associated with LLMs on degeneration, including increasing model size and training models with instruction-tuning data.

Many amazing model abilities emerge when scaling up the model and data size [33]. An interesting question is, _whether the degeneration issue will be solved by simply scaling up_? We use a set of OPT models [38] with different model sizes to investigate this question. As shown in Fig. 2(b), the rep-2 score of generated text sharply drops before increasing the model size to 6.7 billion parameters, indicating that increasing the model size does alleviate the repetition issue to some extent. However, the gains achieved by increasing the model size diminish over time. The OPT-66B model still generates text with high rep-2 score. This observation shows that increasing the model size is not an efficient way to alleviate the degeneration.

Degeneration in certain instruction-tuned LLMs, such as ChatGPT [17], is relatively rare, leading to the hypothesis that the instruction-tuning phase, which trains LLMs on instruction-response pairs, could alleviate this issue. This conjecture implies that the utilization of instruction-tuning data is vital for mitigating degeneration. To explore this, we fine-tune the Llama2 model [30] using three instruction-tuning datasets: Alpaca[29], Alpaca + WT-103 50K, and WT-103 50K, as demonstrated in Table 1. The rep-2 scores for these datasets are 5.54, 9.67, and 10.31, respectively. Our experiments reveal that Llama2 fine-tuned on Alpaca exhibits less repetitions, while training on WT-103 50K and Alpaca + WT-103 50k still displays significant degeneration. This finding is consistent with our prior observations, where repetition issues strongly correlate with the presence of repetitions in the training data. It suggests that the low repetition rate in instruction-tuning data may contribute to the decreased degeneration.

\begin{table}
\begin{tabular}{c|l|c|c c c} \hline \hline \# & **Model** & **Rep-2** & **Rep-3** (\%) & **Rep-4** (\%) \\ \hline
1 & Llama2 w/o FT & – & \(47.79\) & \(41.97\) & \(38.52\) \\
2 & FT Llama2 on Alpaca & \(5.54\) & \(15.08\) & \(10.91\) & \(8.93\) \\
3 & FT Llama2 on Alpaca + WT-103 50K & \(9.67\) & \(41.63\) & \(35.64\) & 32.29 \\
4 & FT Llama2 on WT-103 50K & \(10.31\) & \(54.10\) & \(49.77\) & \(36.80\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Results of Llama2-7B on instruction-tuning data. The “FT” means fine-tuning. The column “Rep-2 of FT Data” indicates the rep-2 score of the training data. The rest Rep-\(n\) scores are evaluated on the generated text. The Alpaca is the instruction-tuning dataset used in [29], “WT-103 50K” is the instruction-tuning dataset we constructed based on Wikitext-103 (Appx. A.2), and “Alpaca + WT-103 50K” is the mixture of both.

[MISSING_PAGE_FAIL:6]

The main baseline method is MLE, which indicates the vanilla GPT-2 model trained by likelihood objective, as defined in Eq. (3). The + Rep-Dropout is the proposed method, i.e., repetition dropout. We also have several baseline methods. The first one is the + Rand-Dropout, which applies dropout on random tokens instead of repetitive tokens. Note that the number of random tokens selected for dropout for this baseline is constrained to the same as the repetitive tokens. We also compare with three previous works that also conducted experiments on Wikitext-103: re-encoding of high-inflow tokens (HI-Re) [8], training with scaled gradient (ScaleGrad) [15], and unlikelihood objective at token level (UL) [34]. During inference we use greedy search as the decoding method to generate 128 tokens, using the first 32 tokens of each line in the test set as the prompt. More details about those baseline methods are also shown in the Appx. A.

### Main Results

As shown in Tab. 2, we evaluate the baseline methods and our method on three datasets. On the Wikitext-103 dataset, both the MLE and MLE + Rand-Dropout methods suffer from the severe degeneration issue. More than 33% percent \(4\)-grams in the generated sentences of the two baseline methods are repetitive, which is different from the patterns in natural language. Nevertheless, MLE + Rep-Dropout can significantly reduce the repetition issue in generated sentences. This observation indicates that repetition in the training data is crucial for the degeneration issue. Compared with methods in previous works in Tab. 2, i.e., HI-Re, ScaleGrad, and UL, the Rep-Dropout also show better performance on alleviating the degeneration issue. The results on FreeLaw and OpenWebText2 datasets are consistent with those on Wikitext-103. We also conduct quantitative and qualitative experiments to evaluate + Rep-Dropout on larger LMs, e.g., GPT-XL[20], which are shown in Appx. B and C, respectively.

\begin{table}
\begin{tabular}{l|c c c c c|c} \hline \hline
**Model** & **Rep-\(2\)(\%)** & **Rep-\(3\)(\%)** & **Rep-\(4\)(\%)** & **Rep-\(w\) (\%)** & **Rep-\(r\) (\%)** & **PPL** \\ \hline \hline \multicolumn{6}{c}{_Wikitext-103_} \\ \hline \hline Hi-Re & 41.91 & 33.82 & 28.35 & 38.60 & 66.22 & — \\ ScaleGrad & 12.49 & 6.85 & 4.44 & 18.31 & 28.98 & 24.72 \\ UL & 36.77 & 28.22 & 22.88 & 39.13 & 61.89 & 21.93 \\ MLE & 47.05 & 38.46 & 32.64 & 46.42 & 72.59 & **21.98** \\ + Rand-Dropout & **38.33** & 28.84 & 22.66 & 37.76 & 66.19 & 23.50 \\ +Rep-Dropout & **9.78** & **4.34** & **2.14** & **22.56** & **25.45** & 28.26 \\ Human & 3.56 & 0.84 & 0.28 & 10.64 & 5.82 & — \\ \hline \hline \multicolumn{6}{c}{_FreeLaw_} \\ \hline \hline MLE & 51.74 & 46.19 & 42.22 & 39.22 & 73.06 & **16.13** \\ + Rand-Dropout & 38.82 & 32.19 & 27.78 & 31.31 & 61.30 & 18.85 \\ +Rep-Dropout & **10.15** & **5.60** & **3.49** & **17.55** & **23.21** & 20.68 \\ Human & 2.77 & 0.89 & 0.50 & 10.61 & 8.10 & – \\ \hline \hline \multicolumn{6}{c}{_OpenWebText2_} \\ \hline \hline MLE & 73.96 & 70.61 & 67.91 & 67.28 & 88.27 & **80.37** \\ + Rand-Dropout & 63.43 & 57.16 & 52.30 & 58.92 & 82.76 & 91.75 \\ +Rep-Dropout & **25.24** & **16.14** & **11.10** & **34.73** & **49.80** & 107.02 \\ Human & 4.53 & 1.39 & 0.61 & 12.41 & 13.09 & – \\ \hline \hline \end{tabular}
\end{table}
Table 2: Performances of language models on three datasets. Both Rand-Dropout and Rep-Dropout use a dropout rate \(0.6\). Rep-\(n\) is defined in Eq. (4). The decoding method for all models is greedy search. PPL is the perplexity score on the real test data. Note that we do not report the PPL for HI-Re, because its vocabulary is different from other baselines.

Figure 3: Rep-\(2\) score and perplexity of MLE + Rep-Dropout on the test set of Wikitext-103. The brown dash line is the human-level rep-\(2\) score.

We also evaluate the effect of MLE + Rep-Dropout with different dropout rates for the repetitions in Wikitext-103, which can be regarded as controlling the percentage of repetitions in a dataset. As shown in Fig. 3, increasing the dropout rate of our method will reduce the number of repetitions in generated sentences continuously. Moreover, we observe a clear trade-off between the rep-\(2\) score of generated sentences and the perplexity on the real data in test set. This suggests that learning on the repetitions in training data can be beneficial for language modeling. Consistent results are also shown on FreeLaw and OpenWebText2 datasets.

### Relation to Previous Hypotheses

Previous research has proposed various hypotheses and approaches to understanding and solving the problem of degeneration in neural text generation. However, we argue that many of these proposals can be explained by a simple explanation. That is, penalizing repetitions in data is a common and fundamental factor for their success.

The core idea of many previous works is to penalize a specific set of data, e.g., all the prefix words \(\bm{x}_{1:t-1}\) at timestep \(t\), to alleviate the degeneration. However, as shown in Fig. 4, many of them have implicit connections with the repetitions in data. For example, the set of high-inflow words (\(\bm{\cdot}\!\!>\)) penalized in Fu et al. [8] has a noticeable interaction with that of repetitive words (\(\bm{\cdot}\!\!>\)), and the set of prefix words (\(\bm{\cdot}\!\!\!-\)) penalized in [34] and [15] is the super-set of the repetitive words (\(\bm{\cdot}\!\!\!>\)). In Xu et al. [36], they directly penalize the repetitions in pseudo repetitive data (\(\bm{\cdot}\!\!\!>\)). In this section, we will show that repetitive words play an important role in previous works.

High-Inflow WordsSome researchers [8] attribute the degeneration issue to high-inflow words, whose probability sum of all the potential preceding words is higher than a threshold. Thus, they propose that merging high-inflow word pairs can alleviate the problem (HI-Re). However, we find that 26% of high-inflow word pairs are repetitive in each sentence of Wikitext-103, and merging these pairs can significantly reduce the rep-\(2\) score of real data. Therefore, we argue that merging high-inflow word pairs is actually an alternative way of reducing repetitions in training data.

We evaluate the argument by controlling a single variable, the type of high-inflow words to be merged, in line 4-6 of Tab. 3. The vanilla HI-Re method (Line 4) merges all the high-inflow words (\(\bm{\cdot}\!\!>\)), which takes \(31.1\%\) of the total training words. We find that the method (Line 5), which only merges repetitive high-inflow pairs, i.e., the intersection between high-inflow words and repetitive words (\(\bm{\cdot}\!\!>\) \(\bm{\cdot}\!\!\!<\)), achieves performance comparable to the original HI-Re method (Line 4). Note that the method in line 5 only merges \(8.1\%\) of the total training words, which is much less than the vanilla method. In contrast, the HI-Re method that merges random high-inflow pairs (Line 6), which has the same number as repetitive high-inflow words (\(\bm{\cdot}\!\!>\) \(\bm{\cdot}\!\!\!<\)), cannot alleviate the degeneration. This suggests that penalizing repetitions in data is critical in the success of Fu et al. [8].

Likelihood ObjectiveMany researchers [11; 15; 26; 34] think that the likelihood objective is the main factor for the degeneration issue. All of those works share the same principle that words in the prefix context (\(\bm{\cdot}\!\!\!>\)) cause the degeneration issue. Therefore, Welleck et al. [34] and Lin et al. [15] propose to reduce the probabilities of repetitions appearing in \(\bm{x}_{1:t-1}\) at time step \(t\). Su et al. [26] and Jiang et al. [11] leverage the contrastive learning to ensure that the hidden representation at time step \(t\) is distinctive to those of \(\bm{x}_{1:t-1}\).

However, the attribution to the likelihood objective merely serves as a superficial explanation, and the core of this issue lies in the fact that the model inevitably learns repetitive behavior when conducting maximum likelihood estimation on repetitive data. As shown in section 6.1, the GPT-2 model with simple repetition dropout, which is also optimized by likelihood objective, achieves an extremely low rep-\(4\) score on generated text. This indicates that likelihood objective might not be the most

Figure 4: Relationship between the penalized data in previous works. We use \(\bm{\cdot}\!\!\!>\), \(\bm{\cdot}\!\!\!\sqcap\), and \(\bm{\cdot}\!\!\!\!\!\sqcap\) to represent the sets of high-inflow words [8], prefix words [11; 15; 26; 34], and pseudo repetitive sentences [36], respectively. We also demonstrate the set of repetitive words in real data by \(\bm{\cdot}\!\!\!>\).

important factor in the degeneration issue. Nevertheless, all methods that penalize tokens in the prefix context () break the reliance on repetitions (). Thus, we hypothesize that preventing the model from learning on repetitions is a key factor in their success.

To evaluate the impact of repetitions on these methods, we conduct experiments following the same principle used to analyze high-inflow words. We choose the ScaleGrad[15] method as our baseline, which penalizes non-novel tokens by scaling the gradient [15]. The non-novel tokens are the entire prefix context at time step \(t\), i.e., \(100\%\) of the training words, for the vanilla ScaleGrad method. We also propose two variants of ScaleGrad: the first uses the repetitive words within prefix as the non-novel tokens, while the second randomly samples a subset, which has the same number of words as the first one (), from the prefix data (Subset of ). The two methods only penalize \(19.0\%\) of the total training words. As shown in Tab. 3, the ScaleGrad method on repetitive words (Line 8) achieves performance close to the standard ScaleGrad method (Line 7) in terms of the rep-\(n\) metric. In contrast, the ScaleGrad method on random subset (Line 9) is not effective in alleviating the degeneration issue as the the other two methods. Other works [11; 26; 34] that attribute the degeneration issue to likelihood objective also penalize tokens in prefix as the ScaleGrad[15], but with different techniques. Thus, we think that penalizing repetitions in data is also a crucial factor for the success of these methods.

Self-reinforcement PhenomenonBoth Xu et al. [36] and Chiang and Chen [4] find that degeneration is always accompanied by the self-reinforcement phenomenon, i.e., the probability of a predicted word becomes higher when it is repeated more times. Thus, they hypothesize that degeneration is (partially) caused by self-reinforcement. To mitigate this issue, [36] proposed a data-augmentation method, namely DITTO (Line 3 of Tab. 3), which constructs pseudo data by repeating a training sentence multiple times and penalizes the probabilities of repetitive tokens in the pseudo data ().

We argue that, as the degeneration issue, the self-reinforcement is also a by-product when neural LMs learning on repetitive patterns in real data. First, we observe a similar self-reinforcement phenomenon on the repetitive words in real data. For instance, the probabilities of the second appearances of the theme-related words are generally higher than their first appearances, as shown in Fig. 5(b). Second, we find that the model trained by repetition dropout can break the self-reinforcement loop, as shown by the examples in Appx.. Although the text generated by GPT-2 + Rep-Dropout on Wikitext-103 may contain a few inappropriate \(n\)-gram repetitions, it will not fall into the infinite repetition loop that frequently appear in the text generated by the vanilla GPT-2.

### Why LMs Learn the Repetition Patterns?

In the last empirical study, we make an attempt to investigate the reasons behind LMs learning repetition patterns, specifically the role these repetitions play in neural LMs. To address this inquiry, we examine 300 randomly selected instances, each featuring repetitive bi-grams within a 256-word sentence. These cases are broadly categorized into three groups, as per Altmann and Kohler [1] and Tannen [28]:

* **Grammar**: Repetitions for grammatical purposes, such as determiners, conjunctions, etc.
* **Theme**: Repetitions closely associated with the subject matter of the text.
* **Limited inventory**: Repetitions resulting from a language's restricted means of expressing a particular concept, leading to high-frequency occurrences, e.g., the phrase "pair of" in English.

\begin{table}
\begin{tabular}{l|l|c c|c c c|c} \hline \hline \# & **Model** & Penal. Scope & Word Percent (\%) & **Rep**-2(\%) & **Rep**-3(\%) & **Rep**-4(\%) & **PPL** \\ \hline
1 & MLE & N/A & 0.0 & 47.05 & 38.46 & 32.64 & 21.98 \\
2 & +Rep-Dropout & Subset of & 11.4 & **9.78** & **4.34** & **2.14** & 28.26 \\
3 & DITTO & & 25.0 & 44.24 & 34.74 & 28.34 & \\
4 & Hi-Re & & 31.f & **41.91** & 33.82 & 28.35 & \\
5 & Hi-Re & & 8.1 & 43.62 & **33.67** & **27.12** & – \\
6 & Hi-Re & Subset of & 8.1 & 52.41 & 43.72 & 37.56 & – \\
7 & ScaleGrad & & 100.0 & **12.49** & **6.85** & **4.44** & 24.72 \\
8 & ScaleGrad & & 19.0 & 17.53 & 10.38 & 6.94 & 23.33 \\
9 & ScaleGrad & Subset of & 19.0 & 22.97 & 15.22 & 10.94 & 23.18 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Impact of penalizing repetitions in different kinds of data on Wikitext-103. The meanings of the symbols,, are illustrated in Fig. 4. The “Subset of Shape” means a subset randomly sampled from Shape. The word percent is \(\frac{\#\text{Penalized Words}}{\#\text{Words in Data}}\).

We assign each case to the earliest category it satisfies if it meets the criteria for multiple categories. Detailed guidelines for human evaluators to classify repetitive words can be found in Appx. D. As demonstrated in Fig. 5(a), almost 50% of the repetitions in Wikitext-103 fulfill grammatical functions, while both theme-related and inventory-related repetitions constitute around 25% each.

To understand the impact of different types of repetitions on a vanilla neural LM, we measure the probability change of a prediction when masking the information of its repetitions in the context through attention mechanism. For example, given the human repetitions in Fig. 1, we will determine how the probability of the last appearance of "generals wore..." changes when masking its first appearance of "generals wore..." in the context.

As shown in Fig. 5(b), masking the theme-related repetitions will cause a noticeable drop of the prediction probabilities. However, the prediction of grammatical and inventory-related repetitions are not affected by masking their previous appearances in context. This observation indicates that neural LMs spend its effort on optimizing the prediction accuracy of those theme-related words by implicitly repeating words in context.

## 7 Conclusion & Limitations

In this work, we find that the repetition in training data is a fundamental factor for the degeneration (or repetition) problem. First, training data is an integral part of most AI systems, and our preliminary study demonstrates a strong correlation between the repetitions in training data and the degeneration issue. Second, we find that simply dropping out the attention to repetitions in training data can significantly reduce the degeneration issue, which is more effective than other baselines. Finally, we conduct extensive empirical analyses to demonstrate that penalizing repetitions in data is the key success factor for many previous works, such as those attribute degeneration issue to high-inflow words, the likelihood objective, and the self-reinforcement mechanism. Experiments also show that our findings are critical even in the context of large language models. We hope that the viewpoint we provide to understanding the degeneration can inspire more principled research in the future.

Our work also has some limitations. First, despite the excellent performance on reducing repetitions, the repetition dropout method may hurt the perplexity of the language model. Second, we examine our work mostly on standard benchmark of the language generation task, following previous works [8; 10; 26; 34]. It also deserves to extend our work to large-scale data and model.

## 8 Acknowledgement

The authors would like to express their sincere gratitude to the three anonymous reviewers and the meta reviewer for their insightful comments and constructive feedback. Furthermore, this work was partial supported by the JSPS KAKENHI Grant, under the number 23KJ1594.

Figure 5: (a) Percentages of three types of repetitions in Wikitext-103. (b) Probabilities of three types of words with and without masking their repetitions in context using the vanilla GPT-2 model.

## References

* [1]G. Altmann and R. Kohler (2015) Forms and degrees of repetition in texts: detection and analysis. Vol. 68, Walter de Gruyter GmbH & Co KG. Cited by: SS1.
* [2]A. Baevski and M. Auli (2019) Adaptive input representations for neural language modeling. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, External Links: Link Cited by: SS1.
* [3]D. Bahdanau, K. Cho, and Y. Bengio (2015) Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, External Links: Link Cited by: SS1.
* [4]T. Chiang and Y. Chen (2021) Relating neural text degeneration to exposure bias. In Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, Punta Cana, Dominican Republic, pp. 228-239. External Links: Link Cited by: SS1.
* [5]Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. Le, and R. Salakhutdinov (2019) Transformer-XL: attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, Florence, Italy, pp. 2978-2988. External Links: Link Cited by: SS1.
* [6]T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer (2023) Qlora: efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314. Cited by: SS1.
* [7]A. Fan, M. Lewis, and Y. Dauphin (2018) Hierarchical neural story generation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Melbourne, Australia, pp. 889-898. External Links: Link Cited by: SS1.
* [8]Z. Fu, W. Lam, A. Man-Cho So, and B. Shi (2021) A theoretical analysis of the repetition problem in text generation. In Thirty-Fifth AAAI Conference on Artificial Intelligence, Cited by: SS1.
* [9]L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang, H. He, A. Thite, N. Nabeshima, et al. (2020) The pile: an 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Cited by: SS1.
* [10]A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi (2020) The curious case of neural text degeneration. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020, External Links: Link Cited by: SS1.
* [11]S. Jiang, R. Zhang, S. Vakulenko, and M. de Rijke (2022) A simple contrastive learning objective for alleviating neural text degeneration. arXiv preprint arXiv:2205.02517. Cited by: SS1.
* [12]T. Kawamoto, H. Kamigaito, K. Funakoshi, and M. Okumura (2022) Generating repetitions with appropriate repeated words. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Seattle, United States, pp. 852-859. External Links: Link Cited by: SS1.
* [13]H. Li, D. Cai, J. Xu, and T. Watanabe (2022) Residual learning of neural text generation with n-gram language model. In Findings of the Association for Computational Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, pp. 1523-1533. External Links: Link Cited by: SS1.
* [14]X. L. Li, A. Holtzman, D. Fried, P. Liang, J. Eisner, T. Hashimoto, L. Zettlemoyer, and M. Lewis (2022) Contrastive decoding: open-ended text generation as optimization. CoRRabs/2210.15097. Cited by: SS1.
* [15]X. Lin, S. Han, and S. Joty (2021) Straight to the gradient: learning to use novel tokens for neural text generation. In Proceedings of the 38th International Conference on Machine Learning, Vol. 139 of Proceedings of Machine Learning Research, pp. 6642-6653. External Links: Link Cited by: SS1.

* Meister et al. [2023] Clara Meister, Tiago Pimentel, Gian Wier, and Ryan Cotterell. 2023. Locally Typical Sampling. _Transactions of the Association for Computational Linguistics_, 11:102-121.
* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744.
* Paszke et al. [2019] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In _Advances in Neural Information Processing Systems 32_, pages 8024-8035. Curran Associates, Inc.
* Pillutla et al. [2021] Krishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi, and Zaid Harchaoui. 2021. Mauve: Measuring the gap between neural text and human text using divergence frontiers. In _Advances in Neural Information Processing Systems_, volume 34, pages 4816-4828. Curran Associates, Inc.
* Radford et al. [2019] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners.
* Riley and Chiang [2022] Darcy Riley and David Chiang. 2022. A continuum of generation tasks for investigating length bias and degenerate repetition. In _Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP_, pages 426-440, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.
* See et al. [2017] Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization with pointer-generator networks. In _Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 1073-1083.
* Srivastava et al. [2014] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: A simple way to prevent neural networks from overfitting. _Journal of Machine Learning Research_, 15(56):1929-1958.
* Su et al. [2021] Yixuan Su, Deng Cai, Yan Wang, David Vandyke, Simon Baker, Piji Li, and Nigel Collier. 2021. Non-autoregressive text generation with pre-trained language models. In _Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume_, pages 234-243, Online. Association for Computational Linguistics.
* Su and Collier [2023] Yixuan Su and Nigel Collier. 2023. Contrastive search is what you need for neural text generation. _Transactions on Machine Learning Research_.
* Su et al. [2022] Yixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. 2022. A contrastive framework for neural text generation. In _Advances in Neural Information Processing Systems_.
* Su and Xu [2022] Yixuan Su and Jialu Xu. 2022. An empirical study on contrastive search and contrastive decoding for open-ended text generation. _arXiv preprint arXiv:2211.10797_.
* Tannen [1987] Deborah Tannen. 1987. Repetition in conversation: Toward a poetics of talk. _Language_, pages 574-605.
* Taori et al. [2023] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca: A strong, replicable instruction-following model. _Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html_, 3(6):7.
* Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. _arXiv preprint arXiv:2307.09288_.

* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 5998-6008.
* Vig [2018] Jesse Vig. 2018. Deconstructing bert: Distilling 6 patterns from 100 million parameters. _Medium, December_.
* Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837.
* Welleck et al. [2020] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. 2020. Neural text generation with unlikelihood training. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net.
* Wolf et al. [2020] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-Art Natural Language Processing. pages 38-45. Association for Computational Linguistics.
* Xu et al. [2022] Jin Xu, Xiaojiang Liu, Jianhao Yan, Deng Cai, Huayang Li, and Jian Li. 2022. Learning to break the loop: Analyzing and mitigating repetitions for neural text generation. In _Advances in Neural Information Processing Systems_.
* Yang et al. [2023] Haoran Yang, Deng Cai, Huayang Li, Wei Bi, Wai Lam, and Shuming Shi. 2023. A frustratingly simple decoding method for neural text generation. _arXiv preprint arXiv:2305.12675_.
* Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_.
* Zhao et al. [2017] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. 2017. Men also like shopping: Reducing gender bias amplification using corpus-level constraints. In _Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing_, pages 2979-2989, Copenhagen, Denmark. Association for Computational Linguistics.

Implementation Details

### Preliminary Study

The basic GPT-2 model4 is trained from scratch on each corpus, which has 12 transformer blocks and 12 attention heads with 768 hidden dimensions. The Huggingface transformers [35] and Pytorch toolkit [18] are used to train the GPT-2 model in the distributed manner on A100 GPU server. The hyper-parameters during training are shown in Tab. 4.

Footnote 4: Model details can be found at https://huggingface.co/gpt2

#### a.1.1 Our Method

Most of the hyper-parameters for our proposed method are the same as that in Tab. 4 for better variable controlling. The specific hyper-parameters for our proposed method are the length of repetitive \(n\)-gram and its repetition dropout rate \(p\), which are set as 2 and 0.6, respectively.

#### a.1.2 Baselines

In this subsection, the specific hyper-parameters for three baselines are described, and most of the hyper-parameters are the same as that in Tab. 5.

### Instruction Tuning

To evaluate the effect of instruction tuning, we conduct experiments on three datasets:

1. Alpaca: The instruction-tuning dataset used by Alpaca [29].
2. WT-103 50K: We randomly sample 50k non-title sentences from Wikitext-103 and convert them to the instruction-tuning format, following [29].
3. Alpaca + WT-103 50K: The mixture of both.

\begin{table}
\begin{tabular}{l|c} \hline \hline
**Hyper-parameter** & **Value** \\ \hline Optimization steps & 100K \\ Test interval & 10K \\ Dropout rate & 0.1 \\ Grad clipping & 1.0 \\ Learning rate & \(5e^{-5}\) \\ Batch size & 128 \\ Maximum sequence length & 256 \\ Warmup steps & 10K \\ Learning scheduler & Linear decay \\ Random seed & 0 \\ Number of GPUs & 4 \\ Learning objective & Cross-Entropy Loss \\ \hline \hline \end{tabular}
\end{table}
Table 4: The hyper-parameters during GPT-2 training procedure.

\begin{table}
\begin{tabular}{l|c} \hline \hline
**Hyper-parameter** & **Value** \\ \hline \hline _Re-encoding of High-inflow Tokens (H1-Re)_ \\ \hline \hline Re-encoding \(\gamma\) & 0.03 \\ \hline \hline _Scaled Gradient (ScaleGrad)_ \\ \hline \hline Scale grade \(\gamma\) & 0.2 \\ \hline _Token-level Unlikelihood Training (UL)_ \\ \hline \hline Rank alpha \(\alpha\) & 1.0 \\ \hline \end{tabular}
\end{table}
Table 5: The hyper-parameters of three baselines in this paper.

[MISSING_PAGE_FAIL:15]

## Appendix D Classification of Repetition Words

We categorize repetitions into three groups, as outlined by Altmann and Kohler [1] and Tannen [28]: _grammar_, _theme_, and _limited inventory_. For each sampled instance, we initially determine whether the repetitive \(n\)-gram falls under the grammar category, meaning any word in the \(n\)-gram is a determiner, preposition, conjunction, etc. Next, if the repetitive \(n\)-gram does not belong to the grammar category, we assess whether any words are closely related to the text's subject matter, thereby placing it in the theme category. For instance, "H. gammarus" is considered part of the theme category when repetitively used in an article about Homarus gammarus. The third category encompasses repetitions stemming from a language's limited means of expressing a specific concept, known as limited inventory. Popular phrases such as "pair of" are examples of repetitive \(n\)-grams in this category.

In cases where multiple repetitive \(n\)-grams appear within a 256-word sentence, we only take one into account. If a repetitive \(n\)-gram satisfies the criteria for more than one category, particularly theme and limited inventory, we allocate it to the earliest applicable category.

Figure 6: Example of generated text. The input text is in bleu, while the repetitions are in red. Both of the two models are fine-tuned on the Wikitext-103 for 3 epochs, as described in section B. We use greedy search for decoding.