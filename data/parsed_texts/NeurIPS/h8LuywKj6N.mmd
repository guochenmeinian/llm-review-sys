# GUI-World: A Dataset for GUI-Oriented Multimodal Large Language Models

###### Abstract

Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding code. However, current agents primarily exhibit excellent understanding capabilities in static environments and are predominantly applied in relatively simple domains, such as Web or mobile interfaces. We argue that a robust GUI agent should be capable of perceiving temporal information on the GUI, including dynamic Web content and multi-step tasks. Additionally, it should possess a comprehensive understanding of various GUI scenarios, including desktop software and multi-window interactions. To this end, this paper introduces a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM annotations, extensively covering six GUI scenarios and eight types of GUI-orientated questions in three formats. We evaluate the capabilities of current state-of-the-art MLLMs, including ImageLLMs and VideoLLMs, in understanding various types of GUI content, especially dynamic and sequential content. Our findings reveal that ImageLLMs struggle with dynamic GUI content without manually annotated keyframes or operation history. On the other hand, VideoLLMs fall short in all GUI-orientated tasks given the sparse GUI video dataset. Based on GUI-World, we take the initial step of leveraging a fine-tuned VideoLLM as a GUI agent, demonstrating an improved understanding of various GUI tasks. However, due to the limitations in the performance of base LLMs, we conclude that using VideoLLMs as GUI agents remains a significant challenge. We believe our work provides valuable insights for future research in dynamic GUI content understanding.

## 1 Introduction

Multimodal Large Language Models (MLLMs), such as GPT-4V(ision) [1] and LLaVA [2], have significantly contributed to the development of the visual-text domain [3]. These models bring forth innovative solutions and paradigms for traditional visual tasks, including visual reasoning [4], medical image interpretation [5; 6], and applications in embodied agents [7]. One particularly promising area is Graphical User Interface (GUI) understanding, which holds significant potential for real-world applications, such as webpage comprehension [8; 9] and navigation by GUI agents [10; 11; 12]. The key challenges of GUI understanding are twofold: effective GUI agents are expected to (1) possess a deep understanding of GUI elements, including webpage icons, text identified through Optical Character Recognition (OCR), and page layouts, and (2) exhibit an exceptional ability to follow instructions within GUI contexts, such as conducting searches through search engines.

Despite significant progress, as illustrated in Table 1, existing works suffer from the following limitations: (1) Most studies predominantly focus on the static features of GUI scenarios, neglecting the need for MLLMs to effectively process sequential information and dynamic operations. For instance, an agent's task performance can be disrupted by unexpected elements such as pop-up advertisements, underscoring a gap in handling dynamic sequential tasks. (2) Current research is typically restricted to Web-based environments, which limits the models' generalization and robustness. For instance, GUI agents may need to operate across diverse platforms such as Windows, macOS, Linux, iOS, Android, and XR environments. Additionally, operations may sometimes involve multiple windows. Therefore, expanding the scope of research to encompass these varied environments will enhance the adaptability and effectiveness of GUI agents.

To mitigate these gaps, this paper introduces GUI-World, a comprehensive dataset containing over 12,000 GUI videos, specifically designed to evaluate and enhance the capabilities of GUI agents. This dataset encompasses a wide range of GUI scenarios, including popular websites, desktop and mobile applications across various operating systems, multi-window interactions, as well as XR environments. The data collection process involves sourcing GUI videos from screen recordings and instructional videos on YouTube. Subsequently, we utilize an Human-MLLM collaborative approach to generate a diverse set of questions and instructions and finally construct GUI-World.

Likewise, we also establish a comprehensive benchmark for GUI understanding, which encompasses seven mainstream MLLMs, three keyframe selection strategies, six GUI scenarios, and a diverse array of queries in multiple-choice, free-form, and conversational formats, aiming to provide a thorough evaluation of the MLLMs' GUI-orientated capabilities. The assessment results indicate that most MLLMs struggle with GUI-World, highlighting their limited dynamic understanding of graphical interfaces and underscoring the need for further enhancement.

\begin{table}
\begin{tabular}{c|c c c c|c c c c|c c c|c} \hline \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Size} & \multirow{2}{*}{Sem.} & \multirow{2}{*}{VL} & Video & \multicolumn{3}{c|}{Env Type} & \multicolumn{3}{c|}{Task Coverage} & \multicolumn{3}{c}{Task} \\  & & & & & Web. & \multicolumn{1}{c}{Obs.} & \multicolumn{1}{c}{Obs.} & \multicolumn{1}{c|}{Obs.} & \multicolumn{1}{c|}{Obs.} & \multicolumn{1}{c|}{Obs.} & \multicolumn{1}{c|}{Obs.} & \multicolumn{1}{c|}{Obs.} & \multicolumn{1}{c|}{Obs.} & \multicolumn{1}{c|}{Obs.} & \multicolumn{1}{c|}{Obs.} \\ \hline Rico [13] & 72,219 & Low & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{UI Code/Layout Generation} \\ MetaGUI [14] & 1,125 & Low & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{Mobile Navigation} \\ UGIF [15] & 523 & High & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{Ul Grounded Instruction Following} \\ ATIV [16] & 715,142 & High & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{GUI Understanding} \\ Ferret-UI [17] & 123,702 & Low & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{Ul Grounding \& Understanding} \\ MiniWoB++ [18] & 100 & Low & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{Web Navigation} \\ WebArena [19] & 812 & Low & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{Web Navigation} \\ Mind2Web [20] & 2,350 & High & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{Web Navigation} \\ OmniAct [21] & 9,802 & Low & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{Code Generation} \\ \hline MMINA [22] & 1.050 & Low & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{Web Navigation} \\ AgentStudio [23] & 304 & High & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{General Control} \\ OSWorld [24] & 369 & High & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{\begin{tabular}{c} GUI Understanding \\ Instruction Following \\ \end{tabular} } \\ \hline \begin{tabular}{c} **GUI-World** \\ **(Ours)** \\ \end{tabular} & 12,379 & Both & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ & \multicolumn{1}{c}{
\begin{tabular}{c} GUI Understanding \\ Instruction Following \\ \end{tabular} } \\ \end{tabular}
\end{table}
Table 1: Comparison of GUI datasets. ‘Sem.’: semantic instruction level, ‘Seq.’: Tasks for sequential images, ‘Cro.’: Cross-app or multi-window tasks, ‘Dyn.’: Tasks for dynamic GUI content.

Figure 1: GUI-World: a comprehensive dataset for GUI understanding, holding significant potential for real-world applications.

Leveraging this dataset, we take the first step of fine-tuning a Video GUI Agent proficient in dynamic and sequential GUI tasks, which results in significant improvements in the general capabilities of GUI agents, thereby demonstrating the utility and effectiveness of GUI-World. Additionally, we delve into discussing various factors critical to GUI understanding, including the integration of textual information, the number of keyframes, and image resolutions.

Overall, the key contributions of this paper are three-fold:

\(\triangleright\)**A New Dataset.** We propose GUI-World, a comprehensive GUI dataset comprising over 12,000 videos specifically designed to assess and improve the GUI understanding capabilities of MLLMs, spanning a range of categories and scenarios, including desktop, mobile, and extended reality (XR), and representing the first GUI-oriented instruction-tuning dataset in the video domain.

\(\triangleright\)**A Novel Model.** Based on GUI-World, we propose GUI-Vid, a GUI-orientated VideoLLM with enhanced capabilities to handle various and complex GUI tasks. GUI-Vid shows a significant improvement on the benchmark and achieves results comparable to the top-performing models.

\(\triangleright\)**Comprehensive Experiments and Valuable Insights.** Our experiments indicate that most existing MLLMs continue to face challenges with GUI-oriented tasks, particularly in sequential and dynamic GUI content. Empirical findings suggest that improvements in vision perception, along with an increase in the number of keyframes and higher resolution, can boost performance in GUI-oriented tasks, thereby paving the way for the future of GUI agents.

## 2 GUI-World: A Comprehensive Dataset for GUI Understanding

### Overview

We introduce GUI-World, a comprehensive dataset covering six GUI scenarios including video, human-annotated keyframes, as well as detailed captions and diverse types of QA produced by our data curation framework, aiming at benchmarking and enhancing the general GUI-orientated capabilities. These GUI scenarios encompass desktop operating systems (_e.g._, macOS, Windows) and mobile platforms (_e.g._, Android and iOS), websites, software, and even extended-range technologies (XR) (_e.g._, GUI in Apple Vision Pro [25]). Discussion for each scenario are in subsection B.1.

As illustrated in Figure 2, the development of GUI-World is structured around a two-stage process. Details regarding video and query statistics are provided in Table 2, which includes distributions of the number of keyframes, video lengths, and the lengths of queries and their corresponding golden answers, as displayed in Figure 3. Refer to Appendix G for details in the case study.

### GUI Video Collection and Keyframe Annotation Process

We describe the pipeline for collecting screen recordings from student workers and GUI-related instructional videos from YouTube for GUI-World and the procedures followed to convert these videos into keyframe sequences.

A significant portion of our video data is derived from screen recordings executed by student workers, which can directly reflect real-life GUI usage scenarios. A typical video collection scenario involves assigning a student worker a specific software task. The student begins by familiarizing themselves

Figure 2: An overview construction pipeline of GUI-World.

with the software, followed by recording a series of operations in a short video clip, such as "Sign up", "Sign in", "Create a New Page", and "Invite Other Collaborators" in the software "Notion1".

Footnote 1: https://www.notion.so/

Despite the high fidelity of these manually recorded videos, we encounter several challenges: (1) Student workers often require substantial time to acquaint themselves with professional software (_e.g._, MATLAB, Adobe After Effects (Ae)), which can hinder the progress of data collection. (2) The videos may lack comprehensiveness, typically capturing only commonly used operations and overlooking rarer functions crucial for dataset completeness. To address these issues, we also source videos from social media platforms that host a diverse array of GUI-related content. Specifically, we download tutorial videos from YouTube--given its prevalence as a video-sharing platform--because they richly detail various GUI operations. These videos are then segmented into shorter clips, each representing a distinct sequence of operations.

The subsequent step involves annotating these video clips with keyframes and textual descriptions of each keyframe using custom-designed annotation software. Although several algorithms exist for keyframe extraction [26, 27, 28, 29], they typically underperform with GUI videos where changes between frames might be minimal (_e.g._, a slight movement in the mouse cursor). To ensure high-quality datasets, we therefore perform manual extraction of these keyframes. Each keyframe is meticulously annotated to include details such as the operation performed, the purpose between two keyframes, the software or website used, mouse actions (_e.g._, scroll, click), and keyboard inputs (_e.g._, copy (Ctrl + C), paste (Ctrl + V), specific input). We detail our annotation process in subsection B.3.

### GUI Tasks Generation from Human-MLLM Collaboration

Drawing insights from prior research [30, 31, 32, 33, 34], we develop a Human-MLLM collaboration pipeline to annotate captions and diverse types of QA specifically tailored for GUI comprehension. The process involves inputting an instructional prompt, a comprehensive description, key information (_e.g._, system or application), and a sequence of human-annotated keyframes into GPT-4V. As depicted in Table 3, GUI-World features an array of question types, as detailed in follows:

\(\triangleright\)**Detailed and Summarized Captioning:** This task challenges basic GUI knowledge and multimodal perception, also addressing the deficiency of detailed GUI content in video-caption pairs. Initially, GPT-4V generates two distinct descriptions for each video: one concentrating on fine-grained details and the other on the overall image sequences. Furthermore, GPT-4V provides a succinct summary, highlighting core operations and overarching objectives in the video.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c} \hline \hline
**Category** & **Total Videos** & **Free-form** & **MCQA** & **Conversation** & **Total Frame. (Avg.)** & **Avg. Anno.** \\ \hline Software & 4,720 & 27,840 & 9,440 & 9,440 & 23,520 (4.983) & 7.558 \\ Website & 2,499 & 14,994 & 4,998 & 4,998 & 15,371 (6.151) & 6.862 \\ IOS & 492 & 2,952 & 984 & 984 & 2,194 (4.459) & 7.067 \\ Multi & 475 & 2,850 & 950 & 950 & 2,507 (5.277) & 7.197 \\ XR & 393 & 2,358 & 786 & 786 & 1,584 (4.030) & 10.970 \\ Android & 3,800 & 15,199 & 7,600 & 7,600 & 38,000 (10.000) & - \\ \hline Summary & 12,379 & 76,673 & 24,758 & 24,758 & 83,176 (6.719) & 7.463 \\ \hline \hline \end{tabular}
\end{table}
Table 2: The statistics of GUI-World. For Android, we automatically sample 10 frames. _Avg. Frame_ refers to the average number of frames in each keyframe, and _Avg. Anno._ refers to the average number of manually annotated user actions in each keyframe.

Figure 3: Left: Distribution of the number of keyframes and video lengths. Right: Length distribution for each type of question and its golden answer.

[MISSING_PAGE_EMPTY:5]

image sequences, engaging in conversations, retrieving both static and dynamic GUI elements, and performing reasoning tasks.

As illustrated in Figure 4, We employ the two-stage training architecture utilizing VideoChat2 [35] as our foundational model. Initially, videos and images are encoded using the UMT-L visual encoder [36]. Subsequently, a QFormer compresses visual tokens into a smaller set of query tokens. Drawing inspiration from [37], we enhance the QFormer [38] by integrating instructions to enable it to extract visual representations pertinent to the given instructions. Additionally, we apply low-rank adaptation (LoRA [39]) to base LLM. This model is concurrently fine-tuned with the visual encoder and QFormer using a Vision-grounded Text Generation (VTG) loss: \(\mathcal{L}_{\text{VTG}}(\theta)=-\mathbb{E}\left[\log p(y|v;\theta)\right]\), where \(v\) represents the visual tokens derived from the QFormer, and \(y\) represents the text output grounded in the visual context.

## 4 Experiments and Analysis

### Experimental Setups

Models.We conduct evaluations on four of the most robust image-based MLLMs: GPT-4V(ision) [1], GPT-4o [40], Qwen-VL-Max [41], and Gemini-Pro-1.5 [42]. We benchmark on three keyframe selection settings: (1) _Random_, where frames are sampled at fixed time intervals within a video; (2) _Extracted_, with keyframes extracted using Kata2; and (3) _Human_, where keyframes are selected by humans during the annotation process. For the _Random_ and _Extracted_ settings, we input 10 frames into each MLLM, while the _Human_ setting uses an average of 6.719 frames, as detailed in Table 2. Each model's responses employ a three-step Chain-of-Thought (CoT) [43] process, i.e., "Describe-Analyze-Answer", to evaluate their peak performance. Additionally, we assessed three advanced VideoLLMs--ChatUnivi [44], Minigpt4-video [45], and Videochat2 [46]--for their performance on GUI content. For detailed experimental setups are referred to Appendix D.

Footnote 2: https://github.com/keplerlab/katna

Evaluation Metrics.To assess free-form questions and multiple-round conversations, we utilize the LLM-as-a-Judge methodology, which assigns a similarity score ranging from 1 to 5 between MLLM's response and a predefined golden answer, already validated by previous studies[47, 48, 49]. For a comprehensive evaluation, we also provide BLEU [50] and BERTScore [51] in Appendix E. For multiple-choice questions, we measure performance using accuracy as the primary evaluation metric.

Textual Information Integration.To investigate the effectiveness of integrating image-caption models to enlarge the context window for LLMs--typically employed in natural videos--and the helpfulness of GUI history content in accomplishing GUI-oriented tasks, we implement three experimental settings: Detailed Caption, Concise Caption, and Vision + Detailed Caption. GPT-4V is utilized to provide captions of these keyframes, integrating human annotators' operational intents to more accurately describe each frame, being validated in subsection B.3.

Keyframes and Resolution.To explore the upper bound of GUI-orientated capabilities, particularly in dynamic and sequential tasks, we conduct ablation studies focusing on the impact of the number

Figure 4: An overview of our fine-tuning architecture, focusing on GUI content alignment and instruction tuning.

[MISSING_PAGE_EMPTY:7]

Performance Variate in Different GUI Scenarios.GPT-4V and Gemini excel in common scenarios such as mobile and website interfaces but show marked deficiencies in more complex GUI environments like XR and multi-window interactions, across both captioning and intricate tasks. This performance gap highlights a significant shortfall in understanding environments where GUI elements are scattered and demand sophisticated interpretation. It emphasizes the critical need for specialized benchmarks and datasets tailored to these complex GUI scenarios, which is essential for enhancing the GUI-oriented capabilities of MLLMs, paving the way for them to become truly reliable and high-performing general control agents.

Keyframe Selection is Important for GUI-orientated Tasks.Across both basic tasks such as captioning and more complex tasks like prediction and reasoning, significant variations are evident among keyframe selection methods. GPT-4V and Gemini significantly benefit from using random-selected and human-selected keyframes, scoring approximately 0.2-0.3 points higher in both captioning and free-form tasks than those using programmatic extraction. This suggests that traditional keyframe technologies, designed for natural videos, are less effective for detecting essential GUI operations, particularly when subtle movements like mouse clicks and dynamic changes are involved. Conversely, the difference in performance is relatively smaller in Qwen-VL-Max, indicating that while keyframe selection methods are crucial for models proficient in GUI content, they exert less influence on less capable models.

Dynamic GUI Tasks Continue to Challenge MLLMs.In the fine-grained tasks depicted in Table 5, GPT-4V and GPT-4oE excel with static GUI content and prediction tasks over image sequences but struggle with providing detailed descriptions for entire videos and dynamic GUI content. This discrepancy is attributed to minor variations in GUI that significantly impact descriptions. Enhancing the number of keyframes and the granularity of perception might mitigate these issues. Among VideoLLMs, ChatUnivi excels in conversational tasks by effectively leveraging contextual nuances, particularly in subsequent rounds, yet it underperforms in GUI-oriented captioning tasks. In contrast, GUI-Vid demonstrates proficiency in sequential tasks but falls short in both captioning and static content handling. This gap is linked to deficiencies in GUI-Vid's pretraining, which lacked comprehensive GUI content crucial for effective vision-text alignment, as evidenced by its poor performance in Table 13 and an instruction tuning process also failed to fully address these shortcomings.

Vision Perception is Important for Sequential GUI Tasks.As demonstrated in Table 5, integrating detailed textual information slightly outperforms purely vision-based inputs or detailed captions, akin to a Chain of Thought (CoT) [43] setting. Surprisingly, GPT-4V excels in caption and prediction

\begin{table}
\begin{tabular}{c c|c|c|c|c|c|c|c} \hline \hline \multirow{2}{*}{Models} & \multirow{2}{*}{Setting} & \multicolumn{2}{c|}{Caption} & \multicolumn{2}{c|}{Complex Tasks} & \multicolumn{2}{c|}{Conversation} & \multicolumn{1}{c}{\multirow{2}{*}{Average}} \\  & & Concies & Detailed & Static & Dyn. & Pred. & Round 1 & Round 2 \\ \hline \multirow{2}{*}{Gemini-Pro-1.5} & R. & 3.659 & 2.837 & 2.969 & 2.822 & 3.450 & 3.608 & 3.845 & 3.339 \\  & E. & 3.350 & 2.468 & 2.741 & 2.431 & 3.292 & 3.458 & 3.837 & 3.152 \\ \hline \multirow{3}{*}{Qwen-VL-Max} & R. & 2.381 & 1.758 & 2.277 & 2.144 & 2.724 & 3.125 & 3.317 & 2.676 \\  & E. & 2.459 & 1.693 & 2.143 & 1.954 & 2.742 & 3.174 & 3.298 & 2.624 \\  & H. & 2.474 & 1.711 & 2.137 & 2.032 & 2.834 & 3.223 & 3.257 & 2.651 \\ \hline \multirow{6}{*}{GPT-4V} & R. & 3.579 & 2.676 & **3.243** & 3.011 & 3.630 & 3.925 & 4.131 & 3.589 \\  & E. & 3.141 & 2.301 & 2.927 & 2.627 & 3.541 & 3.844 & 4.103 & 3.407 \\  & H. & 3.352 & 2.509 & 3.053 & 2.849 & 3.609 & 3.928 & 4.163 & 3.520 \\  & C.C. & 3.454 & 2.547 & 1.818 & 2.335 & 3.577 & 3.521 & 3.884 & 3.028 \\  & D.C. & 3.412 & 2.627 & 2.603 & 2.591 & 3.723 & 3.759 & 4.072 & 3.350 \\  & H+D.C. & 3.436 & 2.677 & 2.927 & 2.750 & **3.791** & 3.857 & 4.148 & 3.494 \\ \hline GPT-to & H. & **4.048** & **3.028** & 3.125 & **3.117** & 3.562 & **4.129** & **4.318** & **3.644** \\ \hline ChatUnivi & - & 1.587 & 1.240 & 1.705 & 1.656 & 2.524 & 2.698 & 3.366 & 2.389 \\ Minigpt4Video & - & 1.246 & 1.073 & 1.249 & 1.235 & 1.675 & 1.494 & 1.719 & 1.475 \\ VideoChat2 & - & 1.992 & 1.312 & 1.812 & 1.682 & 2.158 & 2.342 & 2.720 & 2.144 \\ \hline
**GUI-Vid** & - & 3.562 & 2.058 & 2.376 & 2.090 & 3.435 & 3.080 & 3.260 & 2.847 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Detailed scores for each tasks in **Software** scenarios. ‘Dyn.’ refers to queries on dynamic GUI content, and ‘Pred.’ indicates prediction tasks.

tasks with just detailed captions, providing insights on enhancing specific GUI-oriented tasks through additional textual information. However, it still falls short in more challenging tasks, such as retrieving static or dynamic content. This underscores the critical role of visual perception in GUI environments, where even minor changes can significantly impact outcomes.

**Supreme Enhancement of GUI-Vid on Graphic-based Interface After Finetuned on GUI-World.**

As a pioneering study in training VideoLLMs as screen agents, GUI-Vid significantly outperforms the baseline model, showing an average improvement of 30% across various tasks and GUI scenarios, even surpassing the commercial ImageLLM, Qwen-VL-Max. This enhancement is particularly notable in captioning and prediction over image sequences, where GUI-Vid matches the performance of GPT-4V and Gemini-Pro. As shown in Figure 6, our two-stage progressive finutuning significantly enhances the performance in all GUI scenarios. Remarkably, GUI-Vid scored 3.747 in caption tasks within the XR scenario, highlighting its potential in XR applications and the high-quality annotations provided by our dataset. However, in Multiple-Choice QA and Chatbot tasks, GUI-Vid still lags behind industry leaders like GPT-4V and Gemini-Pro, a discrepancy likely due to the baseline LLM's weaker performance and the challenges of instruction-based fine-tuning.

**Upper Bound of GUI-orientated Capability with More Keyframes and High Resolution.**

As depicted in Table 6, our two ablation studies during the fine-tuning phase demonstrate that utilizing GUI image-text captioning data significantly enhances the model's preliminary understanding of GUI elements, outperforming training that relies solely on videos. Additionally, an increased number of keyframes correlates with improved performance across various scenarios, notably in environments featuring multiple windows and software applications. Further evidence from Table 7 reveals that higher image resolutions substantially boost task performance, both basic and complex, for GPT-4o. These findings underscore the potential for further developing a more robust GUI Agent.

## 5 Conclusion

In this paper, we have introduced GUI-World, a comprehensive GUI-oriented dataset designed to benchmark and enhance understanding of virtual interface, especially seqeuntial and dynamic tasks. This dataset extensively covers six scenarios and various tasks, addressing the previous research gap in comprehensively evaluating models' capabilities in graphic-based understanding. We conduct extensive benchmarks on leading MLLMs and the first Video Agent 'GUI-Vid' finetuned on our GUI-World specifically for tasks requiring temporal information, achieving results comparable to top-performing models, providing detailed insights into enhancing GUI-related capabilities.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Setting} & \multirow{2}{*}{F.K.} & \multirow{2}{*}{E.K.} & \multirow{2}{*}{Data} & \multicolumn{2}{c}{Software} & \multicolumn{2}{c}{Website} & \multicolumn{2}{c}{XR} & \multicolumn{2}{c}{Multi} & \multicolumn{2}{c}{IOS} & \multicolumn{2}{c}{Android} & \multicolumn{2}{c}{Avg.} \\ \cline{4-14}  & & I. & V. & MC & Free & MC & Free & MC & Free & MC & Free & MC & Free & MC & Free \\ \hline \multirow{2}{*}{Baseline} & - & 8 & - & 45.5\% & 2.144 & 42.6\% & 2.221 & 44.0\% & 2.005 & 44.0\% & 2.222 & 40.2\% & 2.169 & 44.7\% & 2.119 & 42.9\% & 2.147 \\  & - & 16 & - & 45.1\% & 2.144 & 41.8\% & 2.240 & 41.0\% & 2.007 & 40.7\% & 2.238 & 39.9\% & 2.138 & 44.7\% & 2.042 & 2.25 & 1.54 \\ \hline \multirow{4}{*}{**GUI-Vid**} & \multirow{4}{*}{8} & \multirow{4}{*}{16} & \multirow{4}{*}{-} & \multirow{4}{*}{58.3\%} & \multirow{4}{*}{2.709} & 53.6\% & 2.817 & 62.2\% & 2.626 & **54.2\%** & 2.673 & 51.1\% & 2.708 & 54.9\% & 2.501 & 56.0\% & 2.665 \\  & & & & & **59.9\%** & **2.856** & 54.1\% & 2.925 & 59.0\% & 2.751 & 52.1\% & 2.837 & 50.0\% & 2.756 & 54.0\% & 2.571 & 54.8\% & 2.782 \\  & & & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & & & & \\  & & & & & & & & & & & & & & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 6: The overall results for ablation study on GUI-Vid finetuning. F.K. and E.K. mean keyframes during the finetuning and evaluation process respectively. I. means Image, and V. means Video.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Res. & Desc. & Conv. & Dyn. & Static & Caption & Free \\ \hline Low & 2.794 & 3.912 & 3.150 & 2.869 & 3.672 & 3.394 \\ High & **3.031** & **4.056** & **3.318** & **3.131** & **3.911** & **3.573** \\ \hline \hline \end{tabular}
\end{table}
Table 7: GPT-4o average performance in six GUI scenarios under low and high resolution.

Figure 6: Two stages of progressive training enhance GUI ability.

## 6 Limitations

While our work presents significant advancements in the field of GUI agents, there are several limitations that need to be addressed. Firstly, despite expanding the dataset to include various GUI scenarios, our models still show limited generalization capabilities when applied to environments not represented in the training data. This highlights the need for further research to improve the adaptability and robustness of GUI agents in diverse and unseen environments. Additionally, the accuracy of our models heavily relies on the selection of keyframes. Automatically extracted keyframes often fail to capture the essential elements needed for accurate GUI understanding, indicating the need for more sophisticated keyframe extraction techniques. Furthermore, although VideoLLMs have shown improvements in handling dynamic content, their ability to understand and predict sequential information in GUI tasks remains suboptimal. This suggests a necessity for future work to focus on enhancing the temporal understanding capabilities of these models. Finally, the training and fine-tuning processes for VideoLLMs require significant computational resources, which may not be accessible to all researchers.

## 7 Potential Negative Societal Impacts

While our work aims to advance the capabilities of GUI agents for beneficial applications, it is important to consider potential negative societal impacts. The use of GUI agents, especially those capable of operating across multiple environments and platforms, raises significant privacy concerns. Ensuring that these agents operate within strict ethical guidelines and that user data is handled securely and responsibly is paramount. There is also the risk of misuse of advanced GUI agents for malicious purposes, such as unauthorized access to sensitive information or automated exploitation of software vulnerabilities. Establishing robust security measures and ethical usage policies is essential to mitigate these risks.

## References

* gpt-4-vision. https://openai.com/research/gpt-4v-system-card, 2023.
* [2] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In _NeurIPS_, 2023.
* [3] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. A survey on multimodal large language models, 2024.
* [4] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action, 2023.
* [5] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day, 2023.
* [6] Kai Zhang, Jun Yu, Eashan Adhikarla, Rong Zhou, Zhiling Yan, Yixin Liu, Zhengliang Liu, Lifang He, Brian Davison, Xiang Li, Hui Ren, Sunyang Fu, James Zou, Wei Liu, Jing Huang, Chen Chen, Yuyin Zhou, Tianming Liu, Xun Chen, Yong Chen, Quanzheng Li, Hongfang Liu, and Lichao Sun. Biomedgpt: A unified and generalist biomedical generative pre-trained transformer for vision, language, and multimodal tasks, 2024.
* [7] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world, 2024.

* Hong et al. [2023] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxuan Zhang, Juanzi Li, Bin Xu, Yuxiao Dong, Ming Ding, and Jie Tang. Cogagent: A visual language model for gui agents, 2023.
* Lai et al. [2024] Hanyu Lai, Xiao Liu, Iat Long long, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, and Jie Tang. Autowebglm: Bootstrap and reinforce a large language model-based web navigating agent, 2024.
* Zhang et al. [2023] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Apparent: Multimodal agents as smartphone users, 2023.
* Niu et al. [2024] Runliang Niu, Jindong Li, Shiqi Wang, Yali Fu, Xiyu Hu, Xueyuan Leng, He Kong, Yi Chang, and Qi Wang. Screenagent: A vision language model-driven computer control agent, 2024.
* Wang et al. [2024] Junyang Wang, Haiyang Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang, Fei Huang, and Jitao Sang. Mobile-agent: Autonomous multi-modal mobile device agent with visual perception, 2024.
* Deka et al. [2017] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. Rico: A mobile app dataset for building data-driven design applications. In _Proceedings of the 30th annual ACM symposium on user interface software and technology_, pages 845-854, 2017.
* Sun et al. [2022] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen Zhu, and Kai Yu. Meta-gui: towards multi-modal conversational agents on mobile gui. _arXiv preprint arXiv:2205.11029_, 2022.
* Venkatesh et al. [2022] Sagar Gubbi Venkatesh, Partha Talukdar, and Srini Narayanan. Ugif: Ui grounded instruction following. _arXiv preprint arXiv:2211.07615_, 2022.
* Rawles et al. [2023] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. Android in the wild: A large-scale dataset for android device control. _arXiv preprint arXiv:2307.10088_, 2023.
* You et al. [2024] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. Ferret-ui: Grounded mobile ui understanding with multimodal llms. _arXiv preprint arXiv:2404.05719_, 2024.
* Liu et al. [2018] Evan Zheran Liu, Kelvin Guu, Panupong Pasupat, Tianlin Shi, and Percy Liang. Reinforcement learning on web interfaces using workflow-guided exploration. In _International Conference on Learning Representations (ICLR)_, 2018. URL https://arxiv.org/abs/1802.08802.
* Zhou et al. [2023] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building autonomous agents. _arXiv preprint arXiv:2307.13854_, 2023.
* Deng et al. [2024] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards a generalist agent for the web. _Advances in Neural Information Processing Systems_, 36, 2024.
* Kapoor et al. [2024] Raghav Kapoor, Yash Parag Butala, Melisa Russak, Jing Yu Koh, Kiran Kamble, Waseem Alshikh, and Ruslan Salakhutdinov. Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web. _arXiv preprint arXiv:2402.17553_, 2024.
* Zhang et al. [2024] Ziniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu. Mmina: Benchmarking multihop multimodal internet agents. _arXiv preprint arXiv:2404.09992_, 2024.
* Zheng et al. [2024] Longtao Zheng, Zhiyuan Huang, Zhenghai Xue, Xinrun Wang, Bo An, and Shuicheng Yan. Agentstudio: A toolkit for building general virtual agents. _arXiv preprint arXiv:2403.17918_, 2024.

* Xie et al. [2024] Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Oswordl: Benchmarking multimodal agents for open-ended tasks in real computer environments. _arXiv preprint arXiv:2404.07972_, 2024.
* Apple [2024] Apple. Apple vision pro. https://www.apple.com/apple-vision-pro/, 2024.
* Zhu et al. [2016] Wangjiang Zhu, Jie Hu, Gang Sun, Xudong Cao, and Yu Qiao. A key volume mining deep framework for action recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1991-1999, 2016.
* Yan et al. [2018] Xiang Yan, Syed Zulqarmain Gilani, Hanlin Qin, Mingtao Feng, Liang Zhang, and Ajmal Mian. Deep keyframe detection in human action videos. _arXiv preprint arXiv:1804.10021_, 2018.
* Mahasseni et al. [2017] Behrooz Mahasseni, Michael Lam, and Sinisa Todorovic. Unsupervised video summarization with adversarial lstm networks. In _Proceedings of the IEEE conference on Computer Vision and Pattern Recognition_, pages 202-211, 2017.
* OpenCV [2024] OpenCV. Opencv. https://opencv.org/.
* Li et al. [2021] Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan, and Lichao Sun. I think, therefore i am: Benchmarking awareness of large language models using awarebench, 2024.
* Sun et al. [2021] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Yuan Li, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zhengliang Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, Bertie Vidgen, Bhavya Kailkhura, Caiming Xiong, Chaowei Xiao, Chunyuan Li, Eric Xing, Furong Huang, Hao Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, Manolis Kellis, Marinka Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, Joaquin Vanschoren, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, Michael Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, Suman Jana, Tianlong Chen, Tianming Liu, Tianyi Zhou, William Wang, Xiang Li, Xiangliang Zhang, Xiao Wang, Xing Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yong Chen, and Yue Zhao. Trustllm: Trustworthiness in large language models, 2024.
* Lei et al. [2024] Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao, and Kang Liu. S3eval: A synthetic, scalable, systematic evaluation suite for large language models, 2024.
* Dekoninck et al. [2024] Jasper Dekoninck, Marc Fischer, Luca Beurer-Kellner, and Martin Vechev. Understanding large language models through the lens of dataset generation, 2024. URL https://openreview.net/forum?id=miGp1hquyB.
* Yu et al. [2023] Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander Ratner, Ranjay Krishna, Jiaming Shen, and Chao Zhang. Large language model as attributed training data generator: A tale of diversity and bias, 2023.
* Li et al. [2023] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. _arXiv preprint arXiv:2305.06355_, 2023.
* Li et al. [2024] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He, Limin Wang, and Yu Qiao. Unmasked teacher: Towards training-efficient video foundation models, 2024.
* Dai et al. [2023] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.
* Zhang et al. [2023] Qiming Zhang, Jing Zhang, Yufei Xu, and Dacheng Tao. Vision transformer with quadrangle attention, 2023.

* Hu et al. [2021] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.
* Hello gpt-4o [2024] OpenAI. Hello gpt-4o, May 2024. URL https://openai.com/index/hello-gpt-4o/. Accessed: 2024-06-06.
* Bai et al. [2023] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.
* Gemini Team [2023] GeminiTeam. Gemini: A family of highly capable multimodal models, 2023.
* Wei et al. [2023] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023.
* Jin et al. [2023] Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, and Li Yuan. Chat-univi: Unified visual representation empowers large language models with image and video understanding. _arXiv preprint arXiv:2311.08046_, 2023.
* Ataallah et al. [2024] Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, and Mohamed Elhoseiny. Minigpt4-video: Advancing multimodal lms for video understanding with interleaved visual-textual tokens, 2024.
* Li et al. [2023] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: A comprehensive multi-modal video understanding benchmark. _arXiv preprint arXiv:2311.17005_, 2023.
* Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.
* Liu et al. [2023] Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, and Jie Tang. Alignbench: Benchmarking chinese alignment of large language models, 2023.
* Chen et al. [2024] Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, and Lichao Sun. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark, 2024.
* Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Proceedings of the 40th annual meeting of the Association for Computational Linguistics_, pages 311-318, 2002.
* Zhang et al. [2019] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. _arXiv preprint arXiv:1904.09675_, 2019.
* [52] OpenAI Team. Gpt-4 technical report, 2024.
* [53] Meta. Llama 2, 2023. https://llama.meta.com/llama2.
* [54] Meta. Llama 3, 2023. https://llama.meta.com/llama3.
* [55] OpenAI. Mistral ai, 2024. https://mistral.ai/company/.
* Li et al. [2023] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.

* [57] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning, 2022.
* [58] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning, 2023.
* [59] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogylm: Visual expert for pretrained language models, 2024.
* [60] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities, 2023.
* [61] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? _arXiv preprint arXiv:2307.06281_, 2023.
* [62] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi Wang, Yu Qiao, Dahua Lin, and Feng Zhao. Are we on the right way for evaluating large vision-language models?, 2024.
* [63] Chaoyi Wu, Jiayu Lei, Qiayu Zheng, Weike Zhao, Weixiong Lin, Xiaoman Zhang, Xiao Zhou, Ziheng Zhao, Ya Zhang, Yanfeng Wang, et al. Can gpt-4v (ision) serve medical applications? case studies on gpt-4v for multimodal medical diagnosis. _arXiv preprint arXiv:2310.09909_, 2023.
* [64] Naoki Wake, Atsushi Kanehira, Kazuhiro Sasabuchi, Jun Takamatsu, and Katsushi Ikeuchi. Gpt-4v (ision) for robotics: Multimodal task planning from human demonstration. _arXiv preprint arXiv:2311.12015_, 2023.
* [65] Sili Huang, Jifeng Hu, Zhejian Yang, Liwei Yang, Tao Luo, Hechang Chen, Lichao Sun, and Bo Yang. Decision namba: Reinforcement learning via hybrid selective sequence modeling, 2024.
* [66] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. _arXiv preprint arXiv:2306.05424_, 2023.
* [67] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual representation by alignment before projection. _arXiv preprint arXiv:2311.10122_, 2023.
* [68] Chaoyun Zhang, Liqun Li, Shilin He, Xu Zhang, Bo Qiao, Si Qin, Minghua Ma, Yu Kang, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. Ufo: A ui-focused agent for windows os interaction, 2024.
* [69] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, and Chunhua Shen. Mobilevlm : A fast, strong and open vision language assistant for mobile devices, 2023.
* [70] Weihao Tan, Ziluo Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng Yue, Haochong Xia, Jiechuan Jiang, Longtao Zheng, Xinrun Xu, et al. Towards general computer control: A multimodal agent for red dead redemption ii as a case study. _arXiv preprint arXiv:2403.03186_, 2024.

* Lin et al. [2023] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, et al. Mm-vid: Advancing video understanding with gpt-4v (ision). _arXiv preprint arXiv:2310.19773_, 2023.
* Song et al. [2024] Zirui Song, Yaohang Li, Meng Fang, Zhenhao Chen, Zecheng Shi, and Yuan Huang. Mmac-copilot: Multi-modal agent collaboration operating system copilot. _arXiv preprint arXiv:2404.18074_, 2024.
* Li et al. [2020] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge. Mapping natural language instructions to mobile ui action sequences, 2020.
* Zhang et al. [2023] Danyang Zhang, Hongshen Xu, Zihan Zhao, Lu Chen, Ruisheng Cao, and Kai Yu. Mobile-Env: An evaluation platform and benchmark for llm-gui interaction. _CoRR_, abs/2305.08144, 2023. URL https://arxiv.org/abs/2305.08144.
* Lu et al. [2024] Xing Han Lu, Zdenek Kasner, and Siva Reddy. Weblinx: Real-world website navigation with multi-turn dialogue, 2024.
* Yao et al. [2017] Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In _ArXiv_, preprint.
* Koh et al. [2024] Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. _arXiv preprint arXiv:2401.13649_, 2024.
* Liu et al. [2024] Junpeng Liu, Yifan Song, Bill Yuchen Lin, Wai Lam, Graham Neubig, Yuanzhi Li, and Xiang Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding?, 2024.
* Mialon et al. [2023] Gregoire Mialon, Clementine Fourrier, Craig Swift, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for general ai assistants, 2023.
* Zheng et al. [2024] Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v(ision) is a generalist web agent, if grounded, 2024.
* Liu et al. [2023] Xiao Liu, Hanyu Lai, Hao Yu, Yifan Xu, Aohan Zeng, Zhengxiao Du, Peng Zhang, Yuxiao Dong, and Jie Tang. Webglm: Towards an efficient web-enhanced question answering system with human preferences, 2023.
* Kousar et al. [2023] Ambreen Kousar, Saif Ur Rehman Khan, Shahid Hussain, M. Abdul Basit Ur Rahim, Wen-Li Wang, and Naseem Ibrahim. A systematic review on pattern-based gui testing of android and web apps: State-of-the-art, taxonomy, challenges and future directions. In _2023 25th International Multitopic Conference (INMIC)_, pages 1-7, 2023. doi: 10.1109/INMIC60434.2023.10465949.
* Jorge et al. [2014] Rodrigo Funabashi Jorge, Marcio Eduardo Delamaro, Celso Goncalves Camilo-Junior, and Auri Marcelo Rizzo Vincenzi. Test data generation based on gui: A systematic mapping. In _International Conference on Software Engineering Advances_, 2014. URL https://api.semanticscholar.org/CorpusID:64041598.
* Kulesovs [2015] Ivans Kulesovs. ios applications testing. 2015. URL https://api.semanticscholar.org/CorpusID:59015994.
* Cheng et al. [2024] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents, 2024.
* Hu et al. [2023] Han Hu, Haolan Zhan, Yujin Huang, and Di Liu. Pairwise gui dataset construction between android phones and tablets, 2023.
* Beltramelli [2017] Tony Beltramelli. pix2code: Generating code from a graphical user interface screenshot, 2017.

* Yan et al. [2023] An Yan, Zhengyuan Yang, Wanrong Zhu, Kevin Qinghong Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian J. McAuley, Jianfeng Gao, Zicheng Liu, and Lijuan Wang. Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation. _ArXiv_, abs/2311.07562, 2023. URL https://api.semanticscholar.org/CorpusID:265149992.
* Nakajima et al. [2013] Hajime Nakajima, Takeshi Masuda, and Ikuya Takahashi. Gui ferret: Gui test tool to analyze complex behavior of multi-window applications. _2013 18th International Conference on Engineering of Complex Computer Systems_, pages 163-166, 2013. URL https://api.semanticscholar.org/CorpusID:837553.
* Rauschnabel et al. [2022] Philipp A. Rauschnabel, Reto Felix, Christian Hinsch, Hamza Shahab, and Florain Alt. What is xr? towards a framework for augmented and virtual reality. _Comput. Hum. Behav._, 133:107289, 2022. URL https://api.semanticscholar.org/CorpusID:247861674.
* [91] Meta quest 3: New mixed reality vr headset. https://www.meta.com/quest/quest-3.
* Sanders et al. [2019] Brian K. Sanders, Yuzhong Shen, and Dennis A. Vincenzi. Understanding user interface preferences for xr environments when exploring physics and engineering principles. In _International Conference on Applied Human Factors and Ergonomics_, 2019. URL https://api.semanticscholar.org/CorpusID:197940610.
* [93] OpenAI. Chatgpt, 2023. https://openai.com/product/chatgpt.

Checklist

1. For all authors... 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] The abstract summarizes the key contributions, such as the introduction of the GUI-World dataset, evaluation of current MLLMs on dynamic GUI content, and the development of a fine-tuned VideoLLM as a GUI agent. 2. Did you describe the limitations of your work? [Yes] See section 6 3. Did you discuss any potential negative societal impacts of your work? [Yes] See section 7 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] All authors have read the ethics review guidelines and ensured that the paper conforms to them.
2. If you are including theoretical results... 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. If you ran experiments (e.g. for benchmarks)... 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] See Appendix D. 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? See Appendix D. 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] We have reported the detailed experimental setting (_e.g._, hyperparameters) of our study in Appendix D. 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See subsection D.2.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... 1. If your work uses existing assets, did you cite the creators? [Yes] All existing assets used in our work have been properly cited. See section 7. 2. Did you mention the license of the assets? [Yes] Specific details about the licenses are provided in the documentation of the code and datasets. 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] New assets, including our GUI-World dataset and the code for the experiments, are included in the supplemental material and are also available via a provided URL. 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] See subsection 2.2. 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes] We ensured that the data used and curated does not contain any personally identifiable information or offensive content. See subsection 2.2.
5. If you used crowdsourcing or conducted research with human subjects... 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [Yes] See subsection B.3. 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? We have discussed the potential negative impact of our study in section 7. 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]

## Part I Appendix

### Table of Contents

* 11 A Related Work
* 12 B Details of Dataset Construction
* 12 B.1 Six Main GUI Categories
* 12 B.2 Selected Website/Software
* 12 B.3 Human Keyframes Annotation Process
* 13 C Dataset Analysis
* 14 D Details of Experiments Setups
* 14 D.1 Finetune dataset construction
* 14 D.2 Hyperparameter Settings
* 14 D.3 Evaluation.
* 15 E Additional Experiments Results
* 16 F Prompts
* 17 G Case Study
* 18Related Work

MLLMs for GUI.Building upon the significant advancements in LLMs [52, 53, 54, 55] and advanced modality-mixing technologies [56, 57], groundbreaking MLLMs such as GPT-4V [1] and Gemini-Pro [42], along with open-source MLLMs like the LLaVA-1.6 series [2, 58], CogVLM [59], and QwenVL series [41], have shown outstanding performance across various tasks [60, 61, 62, 63, 64, 65]. Venturing beyond text and single image, several studies are now exploring the integration of video modalities for tasks requiring dynamic or sequential visual content [44, 35, 66, 67]. In the GUI domain, leveraging the robust vision perception capabilities of MLLMs, applications such as WebAgents [8, 68, 23] and Mobile Agents [17, 12, 69] have gained popularity for handling everyday tasks like navigation and VQA. Frontier research is also investigating the use of MLLMs as general control agents, such as in playing computer games [70, 71] and serving as OS co-pilots [72, 24], paving the way for more complex GUI operations.

GUI Benchmark & Dataset.Building upon the foundational work of Rico [13], the first mobile GUI video dataset, and AitW [16], which features 715k episodes of sequential images, research has extensively covered mobile [73, 74, 14] and web GUI environments [75, 19, 76, 77, 78]. Mind2Web [20] stands out in web-based datasets with over 2,000 tasks from 137 websites across 31 domains. Advances continue into desktop GUIs with new toolkits [23], benchmarks [79, 21], and frameworks [80, 81, 11]. Research on GUI also transfers from comprehending single images in a static workspace [8] to sequential operations or multi-hop scenarios [24, 22], challenging the understanding and operation capability of these powerful models.

## Appendix B Details of Dataset Construction

### Six Main GUI Categories

In earlier endeavors pertaining to GUI, such as those involving GUI testing [82, 83, 84], the focus was segmented into GUIs for Website, Software, IOS and Android platforms. However, as a comprehensive GUI dataset,we included all potential GUI scenarios into our dataset to ensure that our data is the most comprehensive knowledge that the GUI Agent needs to learn; we divided these scenarios into six categories:

* **Android.** This category focuses on the GUI scenarios that occur within the Android operating system, which is predominantly used on smartphones. Android's ubiquity in the mobile market has led to a wide variety of GUI designs and interaction patterns, making it a rich field for study. This category has been the subject of extensive scrutiny in scholarly works such as [13, 73, 16, 85].
* **Software.** This category encapsulates the GUI scenarios arising within software applications, whether they are standalone programs or components of a larger suite. The diversity of software applications, from productivity tools to creative suites, offers a wide range of GUI scenarios for exploration. The literature is rich with research in this area, such as [86].
* **Website.** This category is concerned with the GUI scenarios that manifest within a web browser. Given the ubiquity of web browsing in modern digital life, this category holds significant relevance. It holds a substantial representation in academic literature, with pioneering papers such as [20, 21] proposing excellent GUI datasets for websites.
* **IOS.** This category zeroes in on the GUI scenarios that transpire within the iOS operating system, the proprietary system for Apple devices like the iPhone and iPad. The iOS platform is known for its distinct design aesthetics and interaction patterns, providing a unique context for GUI research. A number of studies, such as [87, 88] make use of GUI information in IOS.
* **Multi Windows.** This category is dedicated to GUI scenarios that necessitate simultaneous interaction with multiple windows, a common occurrence in desktop environments where users often juggle between several applications or documents. Despite the common use of multi-window interaction in everyday GUI usage, there has been relatively little research into this area [89]. The need for efficient multitasking in such scenarios presents unique challenges and opportunities for GUI design and interaction research. As of our knowledge, there are no specific datasets catering to these multi-window GUI scenarios.
* **XR.** XR encompasses Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality (MR) [90]. Given the advancements in XR technology and the growing accessibility of commercial-grade head-mounted displays [25, 91], XR has emerged as a novel medium for human-computer interaction. This necessitates the exploration of GUI within XR environments. In these scenarios, the GUI takes on a 3D, immersive form [92], demanding the agent to comprehend and navigate a 3D space. The emerging field of XR presents a new frontier for GUI research, with unique challenges and opportunities due to its immersive and interactive nature. To date, as far as we are aware, there are no datasets that specifically address GUI in the realm of XR.

### Selected Website/Software

In our study, we selected a diverse range of websites and software to comprehensively evaluate GUI understanding capabilities across various user scenarios. These selections cover essential categories such as social media, productivity tools, online shopping, and educational platforms, providing a broad spectrum of GUI environments.

The chosen websites, as shown in Figure 7, include popular social media platforms like Instagram, Twitter, and LinkedIn, which are integral to understanding dynamic and interactive GUI elements. We also included widely used productivity tools such as Microsoft Teams, Notion, and Slack to evaluate GUI tasks in professional and collaborative settings.

For software shown in Figure 8, we incorporated key applications like Adobe Photoshop and MATLAB to assess GUI operations in specialized and technical environments. Additionally, video conferencing tools like Zoom and cloud storage services like Google Drive were included to represent common remote work and file management scenarios.

These selections ensure that our study encompasses a wide array of user interactions and GUI complexities, thereby providing a robust evaluation of the current state-of-the-art in GUI understanding by MLLMs and comprehensively constructing a high-quality dataset.

### Human Keyframes Annotation Process

Annotator's InformationThe annotation is conducted by 16 authors of this paper and 8 volunteers independently. As acknowledged, the diversity of annotators plays a crucial role in reducing bias and enhancing the reliability of the benchmark. These annotators have knowledge in the GUI domain, with different genders, ages, and educational backgrounds. The education backgrounds of annotators are above undergraduate. To ensure the annotators can proficiently mark the data, we provide them with detailed tutorials, teaching them how to use software to record videos or edit video clips. We also provide them with detailed criteria and task requirements in each annotation process.

Recording Video.For self-recording videos, we employ OBS3 on the Windows system for screen capturing and the official screen recording toolkit on the Mac/IOS system. This process necessitates human labelers to execute a series of targeted actions within specific websites or applications, which are subsequently captured as raw video footage. These actions, commonplace in everyday usage, enhance the reliability of our dataset. Subsequently, the raw videos are segmented into sub-videos, each encapsulating multiple actions (e.g., clicking a button) to achieve a specific objective (e.g., image search). The videos are then processed to extract keyframes annotated with detailed descriptions.

Footnote 3: https://obsproject.com/

Edition Based on YouTube Videos.For sourcing videos from YouTube, we utilize a search protocol formatted as "[website name/application name] + tutorial" to compile relevant video lists. Human labelers first review these videos to understand the primary operations they depict. These videos are then divided into sub-videos, each containing several actions directed towards a 

## Appendix A

Figure 7: List of desktop softwares in GUI-World.

## Appendix A

Figure 8: List of some websites in GUI-World.

single goal (e.g., image search). Like the self-recorded footage, these segments are processed to isolate keyframes and furnish them with descriptive annotations.

Keyframes Annotation.After obtaining the GUI video clips, human annotators will filter out the keyframes of the operations based on the video content and the mouse and keyboard actions at that time. They will also label the sub-operations or targets between the two keyframes. Once the annotation is complete, the annotators will provide an overall description of the entire video, summarizing the main goal of the human operations in the video. After all the information is annotated, we will use a Large Language Model (LLM) to refine the text content, reducing any errors made by human annotators and adjusting the sentence structure. The prompt we use for the LLM to polish the human annotations is shown in Figure 9 and Figure 10.

Human-LLM Cooperated Instruction Generation.To curate and refine the golden answer of each video-instruction pair generated by GPT-4V, given that the raw response from GPT-4V may contain harmful content or hallucinations. The role of humans in the golden answer generation process is to enhance the difficulty of the questions and remove harmful and incorrect content, as shown in Table 8.

Human verifying GPT-4V annotated captions.We evaluated the quality of annotations from GPT-4V by selecting 1,000 detailed descriptions and captions generated by GPT-4V, which were then assessed by human annotators. The high satisfaction rate of 98% underscores the quality and relevance of the GPT-4V annotations.

\begin{table}
\begin{tabular}{l|c} \hline \hline
**Raw response from GPT-4V** & **Golden answer from humans** \\ \hline “Q”: “What subcategories are visible under the ‘Lifestyle’ section in the static GUI content of the video?” & “Q”: “What subcategories are visible under the ‘Lifestyle’ section in the static GUI content of the video?” & “Lifestyle’ section in the static GUI content of the video?” \\ “A”: “In the ‘Lifestyle’ section of the telegram.com website, the visible subcategories are ‘Worcester Living’, ’Faith’, and ‘Food’. These are clearly displayed in a horizontal list just below the main ‘Lifestyle’ category header, indicating different areas of content that users can explore within the lifestyle section.” & “A”: “The ’Lifestyle’ section of the telegram.com website features visible subcategories such as ’Worcester Living’, ’Faith’, and ‘Food’. These are prominently shown in a horizontal list beneath the ‘Lifestyle’ header, offering users various content areas to explore within the section.” \\ \hline \hline \end{tabular}
\end{table}
Table 8: Comparison of raw answer from GPT-4V and golden answer from humans.

Figure 10: The interface for annotating a keyframe, consists of mouse action, keyboard action, and a short sub-action purpose.

Figure 9: The overall preview of our annotating software.

[MISSING_PAGE_EMPTY:25]

Details of Experiments Setups

### Finetune dataset construction

We use two settings to finetune GUI-Vid, one with video-text pairs only, and the other with video-text and image-text pairs, which are all GUI content:

* **Video Only.** In this setting, we only trained GUI-Vid with video-text pairs in GUI-World, as shown in Table 9.
* **Video-Image.** Inspired by the pre-trained process of Videochat2, we include image-text pairs to help the visual encoder align GUI knowledge. These images are selected from our GUI-World, MetaGUI [14], and OmniAct [21] for high-quality GUI content. Subsequently, we use GPT-4V to generate a detailed description and a concise caption for each image. Finally, we construct a dataset consisting of video-text and image-text pairs for gaining comprehensive GUI-orientated capabilities.

### Hyperparameter Settings

In this section, we will introduce the hyperparameters of MLLMs to facilitate experiment reproducibility and transparency. We divide them into three parts: the inference phase during benchmark and dataset construction, the LLM-as-a-Judge phase, and the fine-tuning phase. All our experiments were conducted on a server equipped with dual A800 and dual 4090 GPUs.

Inference.We empirically study 6 MLLMs, involving 3 Image-LLMs and 3 Video-LLMs, with their hyperparameters detailed as follows:

* **GPT-4V [1]:** We set the temperature and top-p as 0.9, max-token as 2048, and both all images input are set as high quality in _Instruction Dataset Construction_ and benchmarking.
* **Gemini-Pro-1.5 [42]:** We use the default settings, which set temperature as 0.4, top-p as 1, and max-token as 2048. It should be noted that during our project, Gemini-Pro-1.5 is still under the user request limit, which only provides 100 requests per day, making our benchmark difficult.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Stage & Data types & Source & Type & Amount \\ \hline \multirow{4}{*}{1} & \multirow{4}{*}{GUI-World} & Video & Detailed Description & 14,276 \\  & & & Concise Caption & 7,138 \\  & & Image & Detailed Description & 5,555 \\  & & & Concise Caption & 5,555 \\ \cline{2-5}  & \multirow{4}{*}{MetaGUI} & \multirow{4}{*}{Image} & Detailed Description & 19,626 \\  & & & Concise Caption & 19,626 \\  & & & Detailed Description & 260 \\  & & & Concise Caption & 260 \\ \hline \multirow{2}{*}{2} & \multirow{4}{*}{GUI-World} & \multirow{2}{*}{Video} & GUI VQA & 21,414 \\  & & & Multiple-Choice QA & 14,276 \\ \cline{1-1}  & & & Conversation & 7,138 \\ \hline \hline \end{tabular}
\end{table}
Table 10: Video-image finetune dataset.

\begin{table}
\begin{tabular}{c|c c} \hline \hline Stage & Data types & Amount \\ \hline \multirow{2}{*}{1} & Detailed Description & 14,276 \\  & Concise Caption & 7,138 \\ \hline \multirow{2}{*}{2} & GUI VQA & 21,414 \\  & Multiple-Choice QA & 14,276 \\  & Conversation & 7,138 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Video-only finetune dataset.

Given that Gemini hasn't launched Pay-as-you-go4, we will include benchmark results on 'Human' setting as soon as possible.

Footnote 4: https://ai.google.dev/pricing

* **Qwen-VL-Max [41]:** We use the default settings for Qwen-VL-Max, with top-p as 0.8 and max-token as 2048. Given that the input context window is merely 6,000 for Qwen, we scale the resolution for all images to 0.3.
* **ChatUnivi [44]:** We use ChatUnivi-7B built upon Vicuna-v0-7B and set the max frame as 100, temperature as 0.2, and max-token as 1024.
* **Minigpt4video [45]:** We use the suggested settings5 for this model and the max-frame are set as 45, with only the max-token being modified to 1024.
* **VideoChat2 & GUI-Vid [46]:** For a fair comparison, we set the same hyperparameters for VideoChat2 & GUI-Vid. We set the max-token as 1024, top-p as 0.9, temperature as 1.0, max-frame as 8/16, repetition penalty as 1.2, and length penalty as 1.2.

Footnote 5: https://github.com/Vision-CAIR/MiniGPT4-video

LLM-as-a-Judge.We studied four LLM-as-a-Judge in giving a similarity score for the MLLM's response and ground truth, namely GPT-4 [52], ChatGPT [93], LLaMA-3-70b-instruct [54], and Mixtral-8x22b-instruct-v0.1 [55]. Hyperparameter settings are detailed as follows:

* **GPT-4 & ChatGPT.** We set the temperature as 0.6 and others as default.
* **LLaMA-3-70b-instruct.** We set the temperature as 0.6, top-p as 0.9, top-k as 50.
* **Mixtral-8x22b-instruct-v0.1.** We set top-p as 0.7, top-k as 50, and temperature as 0.7.

Finetune.We include several hyperparameter settings in experiment settings and ablation studies, as shown in Table 11.

### Evaluation.

Given the complexity of free-form answers in GUI scenarios, the evaluation includes specific positions of GUI elements, textual content, and comparing the response to the golden answer. LLM-as-a-judge has been widely used in previous studies for complex evaluation tasks [47, 48]. Therefore, we leverage LLM-as-a-Judge [47] in a similar setting to MM-vet [60], which compares the MLLM's response to the golden answer. We carefully evaluate the accessibility of leveraging LLM-as-a-Judge, selecting 1,000 samples covering 6 free-form questions mentioned in our dataset. As shown in Table 12, GPT-4 outperforms other LLMs, exhibiting a better human alignment on providing a

\begin{table}
\begin{tabular}{l c} \hline \hline
**Config** & **Setting** \\ \hline input frame & 8 \\ input resolution & 224 \\ max text length & 512 \\ input modal & I. + V. \\ optimizer & AdamW \\ optimizer momentum & \(\beta_{1},\beta_{2}=0.9,0.999\) \\ weight decay & 0.02 \\ learning rate schedule & cosine decay \\ learning rate & 2e-5 \\ batch size & 4 \\ warmup epochs & 0.6 \\ total epochs & 3 \\ backbone drop path & 0 \\ QFormer drop path & 0.1 \\ QFormer dropout & 0.1 \\ QFormer token & 96 \\ flip augmentation & yes \\ augmentation & MultiScaleCrop [0.5, 1] \\ \hline \hline \end{tabular}
\end{table}
Table 11: Configuration settings for fine-tuning.

similarity score for the response compared to the golden answer, although it is approximately 10 times more expensive than other models.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Models & Pearson(\(\uparrow\)) & Spearman(\(\uparrow\)) & Kendall(\(\uparrow\)) & \$ per Benchmark(\(\downarrow\)) \\ \hline GPT-4 & **0.856** & **0.853** & **0.793** & \(120\$\) \\ ChatGPT & 0.706 & 0.714 & 0.627 & **125** \\ Llama-3-70b-instruct & 0.774 & 0.772 & 0.684 & **125** \\ Mixtral-8x22b-instruct-v0.1 & 0.759 & 0.760 & 0.670 & \(15\$\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: Evaluating LLM-as-a-Judge as a replacement for human judging in the scoring setting.

[MISSING_PAGE_FAIL:29]

[MISSING_PAGE_EMPTY:30]

\begin{table}
\begin{tabular}{c c|c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multirow{2}{*}{Setting} & \multicolumn{4}{c}{Sequential} & \multicolumn{4}{c}{Prediction} & \multicolumn{4}{c}{Description} & \multicolumn{4}{c}{Caption} & \multicolumn{4}{c}{Conversation} & \multicolumn{4}{c}{Avg.} \\ \hline  & BLEU & B.S. & BLEU & B.S. & BLEU & B.S. & BLEU & B.S. & BLEU & B.S. & BLEU & B.S. & BLEU & B.S. \\ \hline Gemini-Pro-1.5 & R. & 0.113 & 0.793 & 0.145 & 0.727 & 0.083 & 0.676 & **0.054** & **0.720** & 0.016 & **0.664** & 0.098 & 0.736 & 0.085 & 0.719 \\  & E. & 0.095 & 0.754 & 0.121 & 0.681 & 0.079 & 0.661 & 0.041 & 0.676 & 0.011 & 0.602 & 0.092 & 0.725 & 0.073 & 0.683 \\ \hline \multirow{3}{*}{Queen-VL-Max} & R. & 0.099 & 0.728 & 0.099 & 0.634 & 0.080 & 0.610 & 0.008 & 0.519 & 0.005 & 0.471 & 0.085 & 0.094 & 0.063 & 0.609 \\  & E. & 0.083 & 0.710 & 0.101 & 0.631 & 0.093 & 0.611 & 0.011 & 0.503 & 0.004 & 0.469 & 0.099 & 0.709 & 0.095 & 0.605 \\  & H. & 0.079 & 0.693

\begin{table}
\begin{tabular}{c c|c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Models} & \multirow{2}{*}{Setting} & \multicolumn{4}{c}{Static} & \multicolumn{4}{c}{Sequential} & \multicolumn{4}{c}{Prediction} & \multicolumn{4}{c}{Description} & \multicolumn{4}{c}{Caption} & \multicolumn{4}{c}{Conversation} & \multicolumn{4}{c}{Avg.} \\ \cline{3-14}  & & BLEU & B.S. & BLEU & B.S. & BLEU & B.S. & BLEU & B.S. & BLEU & B.S. & BLEU & B.S. & BLEU & B.S. \\ \hline Gemini-Pro-1.5 & R. & 0.113 & 0.739 & 0.126 & 0.693 & 0.086 & 0.658 & **0.061** & **0.685** & 0.012 & 0.586 & 0.090 & 0.674 & 0.081 & 0.673 \\  & E. & 0.106 & 0.728 & 0.131 & 0.680 & 0.072 & 0.622 & 0.055 & 0.655 & 0.015 & 0.550 & 0.084 & 0.679 & 0.077 & 0.652 \\ \hline \multirow{3}{*}{Qwen-VL-Max} & R. & 0.079 & 0.599 & 0.076 & 0.591 & 0.080 & 0.595 & 0.002 & 0.444 & 0.006 & 0.370 & 0.072 & 0.666 & 0.053 & 0.544 \\  & E. & 0.064 & 0.609 & 0.087 & 0.567 & 0.089 & 0.608 & 0.003 & 0.445 & 0.004 & 0.398 & 0.073 & 0.647 & 0.053 & 0.5Figure 15: Fine-grained performance of Qwen-VL-Max in each software and website.

[MISSING_PAGE_FAIL:34]

[MISSING_PAGE_EMPTY:35]

Figure 18: (Part 1) GPT-4V Generating GUI-oriented Tasks.

[MISSING_PAGE_EMPTY:37]

* [18] GPT-4V Generating GU-oriented Tasks.

Figure 20: (Part 3) GPT-4V Generating GUI-oriented Tasks.

"XR": "You are an AI visual assistant. Here are sequential images of Mixed-Reality combining GUI interface and real world, which are selected from a GUI video.",

"software": "You are an AI visual assistant. Here are sequential GUI interface images of a specific software, which are selected from a GUI video.",

"website": "You are an AI visual assistant. Here are sequential GUI interface images of a desktop website, which are selected from a GUI video.",

"mobile": "You are an AI visual assistant. Here are sequential GUI mobile interface images, which are selected from a GUI video.",

"multi": "You are an AI visual assistant. Here are sequential GUI interface images of interaction among multiple softwares and websites, which are selected from a GUI video.",

"IOS": "You are an AI visual assistant. Here are sequential GUI IOS interface images, which are selected from a GUI video.",

"Sequential-QA": "This is a question about sequential information in sequential images.",

"Prediction": "This is a question about predicting the next action base on the previous actions in the sequential images.",

"Reasoning": "This is a multiple choice question with only one correct answer. This question may need multiple steps of reasoning according to the vision information in sequential images.",

"Description1": "Please give me a detail description of these sequential images.",

"Description2": "Offer a thorough analysis of these sequential images",

"Caption": "Please give me a concise caption of these sequential images.",

"static QA": "This is a question about static information such as text, icon, layout in these sequential images.",

"MCQA": "This is a multiple choice question with only one correct answer. This question may require sequential analysis ability to the vision information in these sequential images.",

"Conversation1": "Act as an assistant to answer the user's question in these sequential images.",

"Conversation2": "This is a multi-turn conversation task. You will be provide the first round conversation and act as an assistant to answer the user's question in the second round according to these sequential images."

Notice = "You can first provide an overall description of these sequential images, and then analyze the user's question according to the sequential images and description. Finally, give an answer based on this description and the image information. Please format your output in a Json format, with key 'Description' for the description of these sequential images, key 'Analysis' for your analysis on the user's question and key 'Answer' for your answer to the User's question."

Figure 21: Prompts for Benchmarking MLLMs.

Figure 22: Prompt for LLM-as-a-Judge: Judging Free-form and Conversational Tasks.

Figure 23: Prompt for LLM-as-a-Judge: Judging Multiple-Choice QA Tasks.

Case Study

In this section, we provide detailed case studies for six GUI scenarios, each divided into two parts. Figure 24 and Figure 25 show example frames and various tasks associated with them. Figure 26 and Figure 27 for IOS, Figure 28 and Figure 29 for multiple-windows interaction, Figure 32 and Figure 33 for website, and Figure 34 and Figure 35 for XR respectively.

* [11] "The video starts with a view of a mobile app interface, specifically an Al-Qur'an Indonesia app with a list of Surahs. A user scrolls through the Surah list, selects a specific Surah, and the view transitions to a page displaying the verses in Arabic and their Indonesian translation. The user then returns to the Surah list. Following this, a pop-up appears indicating a configuration process that takes approximately 10-20 seconds. After the configuration, another pop-up notifies of an available update from the Google Play Store, which the user can accept or postpone. The user dis-misses this pop-up. The main menu is accessed, showing options such as 'Baca Quran', 'Terakhir Baca', 'Pencarian', 'Jadwal Sholat', and 'Pengatvan'. The user selects the 'Pencarian' option, which leads to a search page. Here, the user is instructed to enter a keyword to search in the translation database. Finally, the user navigates back to the main menu and then to the Surah list once more."
* [12] 'Sequential-QA': 'Question': 'After selecting a specific Surah from the list, what does the user do next before a pop-up notification appears?'
* [13] 'Answer': 'After selecting a Surah, the user views the verses in Arabic and their Indonesian translation. Then, the user returns to the Surah list before a pop-up notification appears.'
* [14] 'Question': 'What would likely be the next step in the app interface after the user is directed to the search page?'
* [15] 'Answer': 'After being directed to the search page, the likely next step would be for the user to enter a keyword into the search bar to find a translation or meaning within the app's database."
* [16] 'Conversation': 'User 1': 'I need to find a specific verse in the Quran. How do I search for it?'
* [17] 'Assistant 1': "You can search for a specific verse by tapping on the 'Pencarian' option in the main menu, which will take you to a search page where you can enter a keyword."
* [18] 'User 2': 'I see. And how do I get back to the main menu from the search page?'
* [19] 'Assistant 2': 'To return to the main menu from the search page, you can tap on the back arrow located at the top left corner of the screen.'
* [18] 'Reasoning': 'Question': 'If the user wants to read the verses of Surah 'Al-Infitar', what should they do after launching the app?"
* [19] 'Options': ["A. Go to the settings and select 'Al-Infitar'.", "B. Scroll through the Surah list and select 'Al-Infitar'.", "C. Choose the 'Pencarian' option and type 'Al-Infitar'.", "D. Wait for a pop-up and select 'Al-Infitar' from there."]
* [20] 'Correct Answer': "B. Scroll through the Surah list and select 'Al-Infitar'."

Figure 24: Case study for Android (part 1).

[MISSING_PAGE_EMPTY:44]

*Description1": "The video demonstrates a user navigating through the Khan Academy mobile application under the 'Computing' category. Initially, the user scrolls through the 'Computers and the Internet' section, viewing topics such as 'Digital information,' 'Bits and bytes,' 'The Internet,' and 'Online data security.' The user then scrolls to the bottom, revealing the 'Computing innovations' section and the 'Take Course Challenge' button. Subsequently, the user returns to the previous screen, displaying other computing sections like 'ARQ/College Computer Science Principles' and 'Computer science theory.' The user clicks to enter the 'Computer science theory' interface; the content is loading. After the content has loaded, revealing topics like 'Cryptography' and 'Information theory,' the user returns to the previous page and clicks on 'Code.org.'"
*Caption': "Navigating through computing courses on Khan Academy's mobile application, viewing sections, and attempting to enter 'Computer science theory.'
*static QA]: 'Question': "Which topic appears directly below 'Online data security' in the 'Computers and the Internet' section before scrolling down?"
*Answer': "Before scrolling down, the topic that appears directly below 'Online data security' is 'Computing innovations.' This can be confirmed from the initial frames of the video where the 'Computing innovations' section is partially visible, indicating that it is the next topic in the sequence after 'Online data security.' As the video progresses and the user scrolls down, the full 'Computing innovations' section comes into view, affirming its position in the GUI layout."
*MCQA#: 'Question': "What action does the user take after viewing the 'Computing innovations' section?"
*Options': ["A) Scrolls up to view 'Digital information' again.", "B) Returns to the previous screen showing different computing sections.', "C) Clicks on the 'Take Course Challenge' button.", "D) Taps on the 'Explore' tab at the bottom of the screen."]
*Correct Answer': '[[B]] Returns to the previous screen showing different computing sections.'

Figure 26: Case study for IOS (part 1).

* 'Description2': 'The video begins with the user viewing the 'Computers and the Internet' course section within the Khan Academy application. The user scrolls through various subsections such as 'Digital information,' 'Computers,' 'The Internet,' and 'Online data security,' each with a list of topics and a status of possible mastery points. The user continues to scroll down to the 'Computing innovations' section and then further down to a 'Course challenge' prompt. The user then scrolls back up, revealing previously seen sections in reverse order. The user eventually navigates back to the main 'Computing' category screen, showing an overview of all computing-related courses. From there, the user selects 'Computer science theory,' which briefly loads before displaying topics within that course such as 'Cryptography' and 'Information theory.' Following this, the user returns to the main 'Computing' category screen." 'Sequential-UA': 'Question': 'What action does the user take after scrolling through the 'Online data security' section, and what is displayed as a result of this action?", 'Answer': "After scrolling through the 'Online data security' section, the user scrolls down to the 'Computing innovations' section. As a result of this action, topics such as 'Communication innovations', 'Collaboration innovations', 'Crowdsourcing innovations', and 'Monitoring innovations' are displayed, followed by a 'Course challenge' prompt.' 'Prediction': 'Question': 'If the user were to select the 'Cryptography' section after it was displayed, what kind of content could you predict will be shown next?" 'Answer': 'If the user selects the 'Cryptography' section, one could predict that the next content shown would be detailed topics under Cryptography, such as 'Ancient cryptography,' 'Ciphers,' 'Cryptography challenge 101,' 'Modern cryptography,' 'Modular arithmetic,' 'Primality test,' and 'Randomized algorithms,' along with the status of possible mastery points for that section.' 'Conversation': 'User 1': 'Can you take me to the section about user authentication methods?' Assistant 1': 'Sure, you will need to scroll down the 'Computers and the Internet' course section until you reach the 'Online data security' subsection, where you will find the topic 'User authentication methods.' 'User 2': 'I scrolled too far down and reached the course challenge. How do I find 'User authentication methods' from here?' Assistant 2': 'No problem, you'll need to scroll back up past the 'Computing innovations' section until you see 'Online data security' subsection again. 'User authentication methods' is listed there among other topics.' 'Reasoning': 'Question': 'After browsing through the course topics in 'Computers and the Internet,' the user returns to a broader category view. Based on this behavior, what could be the reason for the user returning to the broader category view?" Options': ['A. The user wants to take a course challenge.', 'B. The user is looking for a different computing-related course.', 'C. The application automatically redirected the user.', 'D. The user intends to log out of the Khan Academy application.'] 'Correct Answer': 'B'

Figure 27: Case study for IOS (part 2).

[MISSING_PAGE_EMPTY:47]

* [11] "The video shows a Windows desktop with several application windows open, including Steam, OBS, and Edge. Initially, the OBS window is partially covering the Edge browser. The user clicks on the Edge browser, bringing it to the foreground, and then minimizes the OBS window, clearing the view. Next, the user moves the Steam window to the center of the screen and minimizes it as well. Afterward, the user opens a new tab in Edge and enters 'office' into the search bar, which leads to a Bing search results page for 'office'. The video ends with the user on the Bing search page with 'office' results displayed."
* [Sequential-QA]: 'Question': 'After moving the Steam window to the center, what did the user do next in the Edge browser?' 'Answer': "After moving the Steam window to the center, the user minimized the Steam application. Then, they opened a new tab in the Edge browser and entered 'office' into the search bar, which led them to the Bing search results page for 'office'." 'Prediction': 'Question': "What would be the likely next action the user performs after searching for 'office' on Bing?" 'Answer': "The likely next action the user would perform after searching for 'office' on Bing would be to click on one of the search results to find more information or to visit the Microsoft Office website to purchase or learn more about the office suite."
* [Conversation]: 'User 1': 'Can you minimize the OBS application for a better view of the browser?' 'Assistant 1': 'Certainly, the OBS application has been minimized, providing a clear view of the Edge browser.' 'User 2': 'Great, now can you search for Microsoft Office in the Edge browser?' 'Assistant 2': "Of course, a new tab has been opened in the Edge browser and 'office' has been entered into the search bar. The Bing search results for 'office' are now displayed." 'Reasoning': 'Question': 'If the user needs to record gameplay footage next, which application should they interact with and what would be their first step?' 'Options': ["A. They should open the Steam application and click on the 'STORE' tab.", "B. They should open the Edge browser and search for 'game recording software'.", "C. They should reopen the OBS application and click on the 'Start Recording' button.", "D. They should access the Windows Start menu and search for the 'Camera' app."] 'Correct Answer': 'C'

Figure 29: Case study for multiple-windows interaction (part 2).

* [1] "Description1": "The video shows a Python 3.7.4 Shell window on a Windows system. The user begins by typing the 'print' function followed by a pair of parentheses. Inside the parentheses, the user types a string, 'Hello World', which is enclosed in double quotes. Upon pressing Enter, the Python Shell executes the command and outputs the text 'Hello World' below the command line, indicating that the code ran successfully without any errors."
* [1] "Executing the print command in Python Shell to display 'Hello World'
* [1] "static QA": "Question": "What version of Python is shown running in the video?"
* [2] "Answer": "The version of Python running in the video is Python 3.7.4, as indicated by the text at the top of the Python Shell window."
* [3] "MCQA": "Question": "What operation does the user perform after typing the print command?"
* [3] "Options": ["A. The user saves the file.", "B. The user compiles the code.", "C. The user executes the print command.", "D. The user closes the Python Shell."]
* [4] "Correct Answer": "[[C]] The user executes the print command."

Figure 30: Case study for software (part 1).

*Description2: "The video shows a sequence of a user interacting with the Python Shell on a Windows system. Initially, the shell is open and waiting for input. The user begins by typing 'print*' and then corrects the typo by removing the asterisk, typing 'print'. They then type two quotation marks to prepare for a string input. Subsequently, the user moves the cursor between the quotation marks and types 'Hello World' to complete the 'print(Hello World)' command. After pressing enter, the shell executes the command, and the phrase 'Hello World' is outputted below the command prompt." *Sequential-QA?: "Question": "Can you describe the sequence of actions the user took to correct the command and output 'Hello World'?" "Answer": "Certainly. Initially, the user typed 'print*' but realized the asterisk was a mistake. They deleted the asterisk to correct the command to 'print'. Next, they entered two quotation marks to indicate the start of a string. The user then placed the cursor between the quotation marks and typed the string 'Hello World'. Finally, upon pressing enter, the Python Shell executed the command and displayed the output 'Hello World' beneath the command prompt." *Prediction?: "Question": "What would be the expected GUI change if the user were to type another command after the 'Hello World' output?" "Answer": "If the user were to type another command after the 'Hello World' output, the expected GUI change would be that the command prompt indicator '>>' would appear on a new line below the 'Hello World' output, signaling readiness for the next command input. Any subsequent command typed by the user would appear after this new prompt, awaiting execution." *Conversation?: "User 1": "I want to print a different message in Python. How can I do that?", "Assistant 1": "To print a different message, you can simply type 'print(Your Message Here)' where 'Your Message Here' is the text you wish to print. Make sure you enclose your message in quotation marks and hit enter to execute the command." "User 2": "What if I want to print 'Goodbye, World' instead?" "Assistant 2": "To print 'Goodbye, World', you would type 'print(Goodbye, World)' and press enter. The Python Shell will execute the command and display 'Goodbye, World' as the output below the command prompt." *Reasoning#: "Question": "What command did the user execute to get the output in the Python Shell?" "Options": ["A. print(Hello World)", "B. print(Hello World)", "C. print(Hello World)", "D. echo(Hello World)"] "Correct Answer": "C",

Figure 31: Case study for software (part 2).

* **Description1':** "The video begins with the Google search results page visible on a Windows system browser, displaying the query 'is oatmeal a healthy breakfast'. The mouse cursor scrolls down the page, revealing additional search results, and the 'People also ask' section with related questions. The user then scrolls back up to the top of the page. Next, the cursor moves to the search bar, and the 'X' button is clicked to clear the previous search content, leaving an empty search bar. The browser's suggested searches drop-down menu appears with various related search queries. Finally, the video fades to black, indicating the end of the sequence."
* **Caption**': 'Navigating Google Search Results and Clearing the Search Query on a Windows System Browser'
* **Static QAN**: 'Question': "What feature snippet is displayed at the top of the Google search results for the query 'is oatmeal a healthy breakfast'?" 'Answer': "The featured snippet at the top of the Google search results for the query 'is oatmeal a healthy breakfast' is from the Harvard T.H. Chan School of Public Health website. It includes an excerpt stating 'Whether it's steel-cut or rolled, quick-cooking or instant, oatmeal is good for you, experts say-with a few caveats. Oatmeal is rich in fiber, which promotes fullness, eases the insulin response, and benefits gut health. It's also a source of vitamins B and E, and minerals such as magnesium.' This snippet provides a concise summary of the health benefits of oatmeal, according to experts, highlighting its nutritional value and potential impact on fullness and insulin response. The presence of this snippet offers a quick and authoritative answer to the user's query, showcasing Google's ability to extract relevant information from web pages and present it prominently for ease of access."
* **MCQAN**: 'Question': 'What action did the user take after reviewing the search results?'
* **Options': ['A. The user clicked on one of the search results.', "B. The user scrolled through the 'People also ask' section.", 'C. The user cleared the search content in the search bar.', 'D. The user navigated to a different website.']
* **Correct Answer': '[[C]] The user cleared the search content in the search bar.',

Figure 32: Case study for website (part 1).

"Description2": "The video shows a sequence of actions on a Google search results page within a web browser on a Windows system. Initially, the mouse cursor moves over a search result discussing the health benefits of oatmeal. Next, the user scrolls down, revealing a 'People also ask' section with questions related to oatmeal and a 'Videos' section showcasing related content. Subsequently, the user scrolls back up to the original position, highlighting the same search result about oatmeal's health benefits. Finally, the user moves the cursor to the search bar and clicks the 'X' to clear the previous search content, resulting in a blank search bar with suggestions and related searches listed below it. The screen then goes black, indicating the end of the video." "Sequential-QAN: 'Question': "After reading about oatmeal's health benefits, what section of the page did the user explore next and did they return to read more about the health benefits afterward?" 'Answer': "After reading about the health benefits of oatmeal, the user scrolled down to explore the 'People also ask' section and the 'Videos' section related to oatmeal. Following this, the user scrolled back up to revisit the original search result that discussed the health benefits of oatmeal." 'Prediction': 'Question': 'Based on the sequence of events, what action is the user likely to perform after clearing the search bar?' 'Answer': 'After clearing the search bar, the user is likely to type in a new search query or select one of the suggested searches that appear below the search bar.' 'Conversation': 'User 1': "I want to find more articles on oatmeal's health benefits. Can you scroll back up?" 'Assistant 1': "I've scrolled back up and the article on oatmeal's health benefits from the Harvard T.H. Chan School of Public Health is highlighted again." 'User 2': 'Great, now can you clear the search and look for something else?' 'Assistant 2': 'The search content has been cleared, and the search bar is now empty, showing a list of related searches and previous search history suggestions for a new query.' 'Reasoning': 'Question': 'If the user wants to perform a new search after clearing the search bar, which of the following actions would they need to take next?', 'Options': ['A. Scroll down to view more search results' 'B. Type a new query into the search bar', "C. Click on one of the 'People also ask' questions", 'D. Close the browser window'] 'Correct Answer': 'B',

Figure 33: Case study for website (part 2).

* [1] "The video showcases a user navigating through various pages within the Apple Vision Pro browser on a Windows system. Initially, the browser displays the start page with Favorites and Reading List. The user then turns their head to the right, which triggers the transition to view a webpage on the right side. Following this, the user pinches with both hands to exit the page and then pinches with both hands and fingers moving towards the middle to expand the browser's various pages. This reveals multiple open browser tabs side by side. The user continues to turn their head left and right to view different pages on each side. Lastly, the user selects and expands a specific tab to fill the screen, displaying its content."
*Caption': 'Navigating through multiple browser pages using head movement and hand gestures in Apple Vision Pro on Windows'
*Static QA: 'Question': "What is the main category listed under the Favorites section on the browser's start page?"
*Answer': "The main category listed under the Favorites section on the browser's start page is 'Perplexity', denoted by a unique icon, followed by other favorites like Instagram and various websites."
*MCQA: 'Question': 'How does the user switch between different open tabs in the Apple Vision Pro browser?'
*Options': ['A. Using keyboard shortcuts', 'B. Turning their head left and right', 'C. Scrolling with a mouse', 'D. Typing the tab number']
*Correct Answer': '[[B]] Turning their head left and right'

Figure 34: Case study for XR (part 1).

* [11] "The Video starts with a full-screen view of a browser interface titled 'Apple Vision Pro' displaying various website thumbnails and bookmarks. The user then turns their head to the right, causing the right side of the browser to come into view. Next, the user looks at a dot at the bottom of the page and pinches with both hands, which causes the browser to exit the full-screen view and shrink to a smaller, windowed mode. The user then performs a pinching motion with both hands, bringing the fingers towards the middle, which causes the browser's various pages to expand, giving an overview of multiple open tabs. The user again turns their head to the right to view the right side page and then to the left to view the left side page. Throughout the video, the GUI elements such as tabs, the address bar, and website thumbnails respond dynamically to the user's head movements and hand gestures."
* [11] "Sequential-QA": 'Question': 'What action did the user take immediately after turning their head to the right to view the right side page?'Answer': 'After turning their head to the right to view the right side page, the user looked at a dot at the bottom of the page and pinched with both hands, which caused the browser to exit full-screen mode and shrink to a smaller windowed interface.'
* [11] "Question': 'If the user pinches with both hands and moves their fingers towards the middle after the browser shrinks to a smaller window, what will happen next?'Answer': 'If the user pinches with both hands and moves their fingers towards the middle after the browser shrinks, the action will cause the browser's various pages to expand, providing an overview of multiple open tabs."
* [11] "Conversation": 'User 1': 'How do I view all the pages I have open in the browser?' Assistant 1': 'You can view all the open pages by pinching with both hands and moving your fingers towards the middle. This will expand the browser and show an overview of all the open tabs.'
* [11] "User 2': 'Great, and how can I exit this overview?' Assistant 2': 'To exit the overview of open pages, you can look at the dot at the bottom of the page and pinch with both hands. This will exit the overview and return you to the individual page view.'
* [11] "Reasoning": 'Question': 'How can the user access the options to open a new tab or window from the current state?' Options': ['A. Turn their head to the left and select the plus sign.', 'B. Swipe left on the touchpad.', 'C. Turn their head to the right and select the 'Done' button.', 'D. Pinch with both hands to exit the current view and access the toolbar.'] "Correct Answer': 'D'

Figure 35: Case study for XR (part 2).