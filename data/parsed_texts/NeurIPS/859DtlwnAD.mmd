# Pin-Tuning: Parameter-Efficient In-Context Tuning

for Few-Shot Molecular Property Prediction

 Liang Wang\({}^{1}\)\({}^{2}\)Qiang Liu\({}^{1}\)\({}^{2}\)\({}^{\dagger}\)Shaozhen Liu\({}^{1}\)Xin Sun\({}^{3}\)Shu Wu\({}^{1}\)\({}^{2}\)Liang Wang\({}^{1}\)\({}^{2}\)\({}^{3}\)

\({}^{1}\)New Laboratory of Pattern Recognition (NLPR)

State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS)

Institute of Automation, Chinese Academy of Sciences (CASIA)

\({}^{2}\) School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{3}\) University of Science and Technology of China

liang.wang@cripac.ia.ac.cn, qiang.liu@nlpr.ia.ac.cn, liushaozhen2025@ia.ac.cn

sunxin000@mail.ustc.edu.cn, {shu.wu, wangliang}@nlpr.ia.ac.cn

###### Abstract

Molecular property prediction (MPP) is integral to drug discovery and material science, but often faces the challenge of data scarcity in real-world scenarios. Addressing this, few-shot molecular property prediction (FSMPP) has been developed. Unlike other few-shot tasks, FSMPP typically employs a pre-trained molecular encoder and a context-aware classifier, benefiting from molecular pre-training and molecular context information. Despite these advancements, existing methods struggle with the ineffective fine-tuning of pre-trained encoders. We attribute this issue to the imbalance between the abundance of tunable parameters and the scarcity of labeled molecules, and the lack of contextual perceptiveness in the encoders. To overcome this hurdle, we propose a parameter-efficient in-context tuning method, named Pin-Tuning. Specifically, we propose a lightweight adapter for pre-trained message passing layers (MP-Adapter) and Bayesian weight consolidation for pre-trained atom/bond embedding layers (Emb-BWC), to achieve parameter-efficient tuning while preventing over-fitting and catastrophic forgetting. Additionally, we enhance the MP-Adapters with contextual perceptiveness. This innovation allows for in-context tuning of the pre-trained encoder, thereby improving its adaptability for specific FSMPP tasks. When evaluated on public datasets, our method demonstrates superior tuning with fewer trainable parameters, improving few-shot predictive performance.1

Footnote 1: Corresponding author

Footnote 2: Code is available at: https://github.com/CRIPAC-DIG/Pin-Tuning

## 1 Introduction

In the field of drug discovery and material science, molecular property prediction (MPP) stands as a pivotal task [5; 9; 63]. MPP involves the prediction of molecular properties like solubility and toxicity, based on their structural and physicochemical characteristics, which is integral to the development of new pharmaceuticals and materials. However, a major challenge encountered in real-world MPP scenarios is data scarcity. Obtaining extensive molecular data with well-characterized properties can be time-consuming and expensive. To address this, few-shot molecular property prediction (FSMPP) has emerged as a crucial approach, enabling predictions with limited labeled molecules [1; 41; 4].

The methodology for general MPP typically adheres to an encoder-classifier framework [71; 23; 27; 56], as illustrated in Figure 2(a). In this streamlined framework, the encoder converts molecularstructures into vectorized representations [12; 28; 50; 67; 2], and then the classifier uses these representations to predict molecular properties. In the context of few-shot scenarios, two significant discoveries have been instrumental in advancing this task. Firstly, _pre-trained molecular encoders_ have demonstrated consistent effectiveness in FSMPP tasks [20; 14; 58]. This indicates the utility of leveraging pre-acquired knowledge in dealing with data-limited scenarios. Secondly, unlike typical few-shot tasks such as image classification [57; 48], FSMPP tasks greatly benefits from _molecular context information_. This involves comprehending the seen many-to-many relationships between molecules and properties [58; 45; 73], as molecules are multi-labeled by various properties. These two discoveries have collectively led to the development of the widely used FSMPP framework that utilizes a pre-trained encoder followed by a context-aware classifier, as shown in Figure 2(b).

Despite the progress, there are observed limitations in the current approaches to FSMPP. Notably, while using a pre-trained molecular encoder generally outperforms training from scratch, fine-tuning the pre-trained encoder often leads to inferior results compared to keeping it frozen, which can be observed in Figure 1.

The observed ineffective fine-tuning can be attributed to two primary factors: (i) Imbalance between the abundance of tunable parameters and the scarcity of labeled molecules: fine-tuning all parameters of a pre-trained encoder with few labeled molecules leads to a disproportionate ratio of tunable parameters to available data. This imbalance often results in over-fitting and catastrophic forgetting [7; 6]. (ii) Limited contextual perceptiveness in the encoder: while molecular context is leveraged to enhance the classifier [58; 73], the encoder typically lacks the explicit capability to perceive this context, relying instead on implicit gradient-based optimization. This leads to the encoder not directly engaging with the nuanced molecular context information that is critical in FSMPP tasks. In summary, while significant strides have been made, the challenges of imbalance between the number of parameters and labeled data, along with the need for contextual perceptiveness in the encoder, necessitate more sophisticated methodologies in this domain.

Based on the aforementioned analysis, we propose the parameter-efficient in-context tuning method, named Pin-Tuning, to address the two primary challenges in FSMPP. To overcome the parameter-data imbalance, we propose a parameter-efficient chemical knowledge adaptation approach for pre-trained molecular encoders. A lightweight adapters (MP-Adapter) are designed to tune the pre-trained message passing layers efficiently. Additionally, we impose a Bayesian weight consolidation (Emb-BWC) on the pre-trained embedding layers to prevent aggressive parameter updates, thereby mitigating the risk of over-fitting and catastrophic forgetting. To address the second challenge, we further endow the MP-Adapter with the capability to perceive context. This innovation allows for in-context tuning of the pre-trained molecular encoders, enabling them to adapt more effectively to specific downstream tasks. Our approach is rigorously evaluated on public datasets. The experimental results demonstrate that our method achieves superior tuning performance with fewer trainable parameters, leading to enhanced performance in few-shot molecular property prediction.

The main contributions of our work are summarized as follows:

* We analyze the deficiencies of existing FSMPP approaches regarding the adaptation of pre-trained molecular encoders. The key issues include an imbalance between the number of tunable parameters and labeled molecules, as well as a lack of contextual perceptiveness in the encoders.
* We propose Pin-Tuning to adapt the pre-trained molecular encoders for FSMPP tasks. This includes the MP-Adapter for message passing layers and the Emb-BWC for embedding layers, facilitating parameter-efficient tuning of pre-trained molecular encoders.
* We further endow the MP-Adapter with the capability to perceive context to allows for in-context tuning, which provides more meaningful adaptation guidance during the tuning process.
* We conduct extensive experiments on benchmark datasets, which show that Pin-Tuning outperforms state-of-the-art methods on FSMPP by effectively tuning pre-trained molecular encoders.

Figure 1: Comparison of molecular encoders trained via different paradigms: train-from-scratch, pretrain-then-freeze, and pretrain-then-finetune. The evaluation is conducted across two datasets and three encoder architectures [20; 47; 66]. The results consistently demonstrate that while pretraining outperforms training from scratch, the current methods do not yet effectively facilitate finetuning.

Related work

**Few-shot molecular property prediction.** Few-shot molecular property prediction aims to accurately predict the properties of new molecules with limited training data [49]. Early research applied general few-shot techniques to FSMPP. IterRefLSTM [1] is the pioneer work to leverage metric learning to solve FSMPP problem. Following this, Meta-GGNN [41] and Meta-MGNN [14] introduce meta-learning with graph neural networks, setting a foundational framework that subsequent studies have continued to build upon [39; 40; 4]. It is noteworthy that Meta-MGNN employs a _pre-trained molecular encoder_[20] and achieves superior results through fine-tuning in the meta-learning process compared to training from scratch. In fact, pre-trained graph neural networks [64; 36; 17; 54; 37] have shown promise in enhancing various graph-based downstream tasks [52; 13], including molecular property prediction [60; 62; 38; 72]. Recent efforts have shifted towards leveraging unique nature in FSMPP, such as the many-to-many relationships between molecules and properties arising from the multi-labeled nature of molecules, often referred to as the _molecular context_. PAR [58] initially employs graph structure learning [32; 55] to connect similar molecules through a homogeneous context graph. MHNs [45] introduces a large-scale external molecular library as context to augment the limited known information. GS-Meta [73] further incorporates auxiliary task to depict the many-to-many relationships.

**Parameter-efficient tuning.** As pre-training techniques have advanced, tuning of pre-trained models has become increasingly crucial. Traditional full fine-tuning approaches updates all parameters, often leading to high computational costs and the risk of over-fitting, especially when available data for downstream tasks are limited [33; 15]. This challenge has led to the emergence of parameter-efficient tuning [26; 29; 34]. The philosophy of parameter-efficient tuning is to optimize a small subset of parameters, reducing the computational costs while retaining or even improving performance on downstream tasks [19; 69]. Among the various strategies, the adapters [18; 42; 59] have gained prominence. Adapters are small modules inserted between the pre-trained layers. During the tuning process, only the parameters of these adapters are updated while the rest remains frozen, which not only improves tuning efficiency but also offers an elegant solution to the generalization [70; 30; 8]. By keeping the majority of the pre-trained parameters intact, adapters preserve the rich pre-trained knowledge. This attribute is particularly valuable in many real-world applications including FSMPP.

## 3 Preliminaries

### Problem formulation

Let \(\{\mathcal{T}\}\) be a collection of tasks, where each task \(\mathcal{T}\) involves the prediction of a property \(p\). The training set comprising multiple tasks \(\{\mathcal{T}_{\text{train}}\}\), is represented as \(\mathcal{D}_{\text{train}}=\{(m_{i},y_{i,t})|t\in\{\mathcal{T}_{\text{train}}\}\}\), with \(m_{i}\) indicating a molecule and \(y_{i,t}\) its associated label for task \(t\). Correspondingly, the test set \(\mathcal{D}_{\text{test}}\), formed by tasks \(\{\mathcal{T}_{\text{test}}\}\), ensures a separation of properties between training and testing phases, as the property sets \(\{p_{\text{train}}\}\) and \(\{p_{\text{test}}\}\) are disjoint (\(\{p_{\text{train}}\}\cap\{p_{\text{test}}\}=\emptyset\)).

The goal of FSMPP is to train a model using \(\mathcal{D}_{\text{train}}\) that can accurately infer new properties from a limited number of labeled molecules in \(\mathcal{D}_{\text{test}}\). Episodic training has emerged as a promising strategy in meta-learning [10; 16] to deal with few-shot problem. Instead of retaining all \(\{\mathcal{T}_{\text{train}}\}\) tasks in memory, episodes \(\{E_{t}\}_{t=1}^{B}\) are iteratively sampled throughout the training process. For each episode \(E_{t}\), a particular task \(\mathcal{T}_{t}\) is selected from the training set, along with corresponding support set \(\mathcal{S}_{t}\) and query set \(\mathcal{Q}_{t}\). Typically, the prediction task involves classifying molecules into two classes: _positive_ (\(y=1\)) or _negative_ (\(y=0\)). Then a 2-way \(K\)-shot episode \(E_{t}=(\mathcal{S}_{t},\mathcal{Q}_{t})\) is constructed. The support set \(\mathcal{S}_{t}=\{(m_{i}^{s},y_{i,t}^{s})\}_{i=1}^{2K}\) includes \(2K\) examples, each class contributing \(K\) molecules. The query set containing \(M\) molecules is denoted as \(\mathcal{Q}_{t}=\{(m_{i}^{q},y_{i,t}^{q})\}_{i=1}^{M}\).

### Encoder-classifier framework for FSMPP

Encoder-classifier framework is widely adopted in FSMPP methods. As illustrated in Figure 2(a), given a molecule \(m\) whose property need to be predicted, a molecular encoder \(f(\cdot)\) first learns the molecule's representation based on its structure, i.e., \(\bm{h}_{m}=f(m)\in\mathbb{R}^{d}\). The molecule \(m\) is generally represented as a graph \(m=(\mathcal{V},\mathbf{A},\mathbf{X},\mathbf{E})\), where \(\mathcal{V}\) denotes the nodes (atoms), \(\mathbf{A}\) represents the adjacent matrix defined by edges (chemical bonds), and \(\mathbf{X},\mathbf{E}\) denote the original feature of atomsand bonds, then graph neural networks (GNNs) are employed as the molecular encoders [44; 51; 21]. Subsequently, the learned molecular representation is fed into a classifier \(g(\cdot)\) to obtain the prediction \(\hat{y}=g(\bm{h}_{m})\). The model is trained by minimizing the discrepancy between \(\hat{y}\) and the ground truth \(y\).

Further, two key discoveries have been pivotal for FSMPP. The first is the proven effectiveness of pre-trained molecular encoders, while the second is the significant advantage gained from molecular context. Together, these discoveries have further reshaped the widely adopted FSMPP framework, which combines a _pre-trained encoder_ followed by a _context-aware classifier_, as shown in Figure 2(b).

### Pre-trained molecular encoders (PMEs)

Due to the scarcity of labeled data in molecular tasks, molecular pre-training has emerged as a crucial area, which involves training encoders on extensive molecular datasets to extract informative representations. Pre-GNN [20] is a classic pre-trained molecular encoder that has been widely used in addressing FSMPP tasks [14; 58; 73]. The backbone of Pre-GNN is a modified version of Graph Isomorphism Network (GIN) [65] tailored to molecules, which we call GIN-Mol, consisting of multiple atom/bond embedding layers and message passing layers.

**Atom/Bond embedding layers.** The raw atom features and bond features are both categorical vectors, denoted as \((i_{v,1},i_{v,2},\ldots,i_{v,|E_{n}|})\) and \((j_{e,1},j_{e,2},\ldots,j_{e,|E_{e}|})\) for atom \(v\) and bond \(e\), respectively. These categorical features are embedded as:

\[\bm{h}_{v}^{(0)}=\sum\nolimits_{a=1}^{|E_{n}|}\texttt{EmbAtom}_{a}(i_{v,a}), \quad\bm{h}_{e}^{(l)}=\sum\nolimits_{b=1}^{|E_{e}|}\texttt{EmbBond}_{b}^{(l) }(j_{e,b}),\] (1)

where \(\texttt{EmbAtom}_{a}(\cdot)_{a\in\{1,\ldots,|E_{n}|\}}\) and \(\texttt{EmbBond}_{b}(\cdot)_{b\in\{1,\ldots,|E_{e}|\}}\) represent embedding operations that map integer indices to \(d\)-dimensional real vectors, i.e., \(\bm{h}_{v}^{(0)},\bm{h}_{e}^{(l)}\in\mathbb{R}^{d}\), \(l\in\{0,1,\ldots,L-1\}\) represents the index of encoder layers, and \(L\) is the number of encoder layers. The atom embedding layer is present only in the first encoder layer, while an bond embedding layer exists in each layer.

**Message passing layers.** At the \(l\)-th encoder layer, atom representations are updated by aggregating the features of neighboring atoms and chemical bonds:

\[\bm{h}_{v}^{(l)}=\texttt{ReLU}\left(\texttt{MLP}^{(l)}\left(\sum\nolimits_{ u}\bm{h}_{u}^{(l-1)}+\sum\nolimits_{e=(v,u)}\bm{h}_{e}^{(l-1)}\right)\right),\] (2)

where \(u\in\mathcal{N}(v)\cup\{v\}\) is the set of atoms connected to \(v\), and \(\bm{h}_{v}^{(l)}\in\mathbb{R}^{d}\) is the learned representation of atom \(v\) at the \(l\)-th layer. \(\texttt{MLP}(\cdot)\) is implemented by 2-layer neural networks, in which the hidden dimension is \(d_{1}\). After \(\texttt{MLP}\), batch normalization is applied right before the \(\texttt{ReLU}\). The molecule-level representation \(\bm{h}_{m}\in\mathbb{R}^{d}\) is obtained by averaging the atom representations at the final layer.

## 4 The proposed Pin-Tuning method

This section delves into our motivation and proposed method. Our framework for FSMPP is depicted in Figure 2(c). The details of our principal design, Pin-Tuning for PMEs, is present in Figure 2(d).

As shown in Figure 1, pretraining then finetuning molecular encoders is a common approach. However, fully fine-tuning yields results inferior to simply freezing them. Thus, the following question arises:

_How to effectively adapt pre-trained molecular encoders to downstream tasks, especially in few-shot scenarios?_

We analyze the reasons of observed ineffective fine-tuning issue, and attribute it to two primary factors: (i) imbalance between the abundance of tunable parameters and the scarcity of labeled molecules, and (ii) limited contextual perceptiveness in the encoder.

### Parameter-efficient tuning for PMEs

To address the first cause of observed ineffective tuning, we reform the tuning method for PMEs. Instead of conducting full fine-tuning for all parameters, we propose tuning strategies specifically tailored to the message passing layers and embedding layers in PMEs, respectively.

#### 4.1.1 MP-Adapter: message passing layer-oriented adapter

For message passing layers in PMEs, the number of parameters is disproportionately large compared to the training samples. To mitigate this imbalance, we design a lightweight adapter targeted at the message passing layers, called MP-Adapter. The pre-trained parameters in each message passing layer include parameters in the MLP and the following batch normalization. We freeze all pre-trained parameters in message passing layers and add a lightweight trainable adapter after MLP in each message passing layer. Formally, the adapter module for \(l\)-th layer can be represented as:

\[\bm{z}_{v}^{(l)}=\texttt{FeedForward}_{\texttt{down}}(\bm{h}_{v}^{(l)})\in \mathbb{R}^{d_{2}},\] (3)

\[\Delta\bm{h}_{v}^{(l)}=\texttt{FeedForward}_{\texttt{up}}(\phi(\bm{z}_{v}^{(l) }))\in\mathbb{R}^{d},\] (4)

\[\tilde{\bm{h}}_{v}^{(l)}=\texttt{LayerNorm}(\bm{h}_{v}^{(l)}+\Delta\bm{h}_{v}^ {(l)})\in\mathbb{R}^{d},\] (5)

where \(\texttt{FeedForward}(\cdot)\) denotes feed forward layer and \(\texttt{LayerNorm}(\cdot)\) denotes layer normalization. To limit the number of parameters, we introduce a bottleneck architecture. The adapters downscale the original features from \(d\) dimensions to a smaller dimension \(d_{2}\), apply nonlinearity \(\phi\), then upscale back to \(d\) dimensions. By setting \(d_{2}\) smaller than \(d\), we can limit the number of parameters added. The adapter module has a skip-connection internally. With the skip-connection, we adopt the near-zero initialization for parameters in the adapter modules, so that the modules are initialized to approximate identity functions. Therefore, the encoder with initialized adapters is equivalent to the pre-trained encoder. Furthermore, we add a layer normalization after skip-connection for training stability.

#### 4.1.2 Emb-BWC: embedding layer-oriented Bayesian weight consolidation

Unlike message passing layers, embedding layers contain fewer parameters. Therefore, we directly fine-tune the parameters of the embedding layers, but impose a constraint to limit the magnitude of parameter updates, preventing aggressive optimization and catastrophic forgetting.

The parameters in an embedding layer consist of an embedding matrix used for lookups based on the indices of the original features. We stack the embedding matrices of all embedding layers to form \(\Phi\in\mathbb{R}^{E\times d}\), where \(E\) represents the total number of lookup entries. Further, \(\Phi_{i}\in\mathbb{R}^{d}\) denotes the \(i\)-th row's embedding vector, and \(\Phi_{i,j}\in\mathbb{R}\) represents the \(j\)-th dimensional value of \(\Phi_{i}\).

To avoid aggressive optimization of \(\Phi\), we derive a Bayesian weight consolidation framework tailored for embedding layers, called Emb-BWC, by applying Bayesian learning theory [3] to fine-tuning.

**Proposition 1**:: _(Emb-BWC ensures an appropriate stability-plasticity trade-off for pre-trained embedding layers.) Let \(\Phi\in\mathbb{R}^{E\times d}\) be the pre-trained embeddings before fine-tuning, and \(\Phi^{\prime}\in\mathbb{R}^{E\times d}\) be the fine-tuned embeddings. Then, the embeddings can both retain the atom and bond properties

Figure 2: (a) The vanilla encoder-classifier framework for MPP. (b) The framework widely adopted by existing FSMPP methods, which contains a pre-trained molecular encoder and a context-aware property classifier. (c) Our proposed framework for FSMPP, in which we introduce a Pin-Tuning method to update the pre-trained molecular encoder followed by the property classifier. (d) The details of our proposed Pin-Tuning method for pre-trained molecular encoders. In (b) and (c), we use the property names like SR-HSE to denote the molecular context in episodes.

obtained from pre-training and be appropriately updated to adapt to downstream FSMPP tasks, by introducing the following Emb-BWC loss into objective during the fine-tuning process:_

\[\mathcal{L}_{\text{Emb-BWC}}=-\frac{1}{2}\sum_{i=1}^{E}(\Phi_{i}^{\prime}-\Phi_{ i})^{\top}\mathbf{H}(\mathcal{D}_{\mathcal{P}},\Phi_{i})(\Phi_{i}^{\prime}-\Phi_{ i}),\] (6)

_where \(\mathbf{H}(\mathcal{D}_{\mathcal{P}},\Phi_{i})\in\mathbb{R}^{d\times d}\) is the Hessian of the log likelihood \(\mathcal{L}_{\mathcal{P}}\) of pre-training dataset \(\mathcal{D}_{\mathcal{P}}\) at \(\Phi_{i}\)._

Details on the theoretical derivation of Eq. (6) are given in Appendix A. Since \(\mathbf{H}(\mathcal{D}_{\mathcal{P}},\Phi_{i})\) is intractable to compute due to the great dimensionality of \(\Phi\), we adopt the diagonal approximation of Hessian. By approximating \(\mathbf{H}\) as a diagonal matrix, the \(j\)-th value on the diagonal of \(\mathbf{H}\) can be considered as the importance of the parameter \(\Phi_{i,j}\). The following three choices are considered.

_Identity matrix._ When using the identity matrix to approximate the negation of \(\mathbf{H}\), Eq. (6) is simplified to \(\mathcal{L}_{\text{Emb-BWC}}^{\text{IM}}=\frac{1}{2}\sum_{i=1}^{E}\sum_{j=1}^{d }(\Phi_{i,j}^{\prime}-\Phi_{i,j})^{2}\), assigning equal importance to each parameter. This loss function is also known as L2 penalty with pre-trained model as the starting point (L2-SP) [31].

_Diagonal of Fisher information matrix._ The Fisher information matrix (FIM) \(\mathbf{F}\) is the negation of the expectation of the Hessian over the data distribution, i.e., \(\mathbf{F}=-\mathbb{E}_{\mathcal{D}_{\mathcal{P}}}[\mathbf{H}]\), and the FIM can be further simplified with a diagonal approximation. Then, the Eq. (6) is simplified to \(\mathcal{L}_{\text{Emb-BWC}}^{\text{FIM}}=\frac{1}{2}\sum_{i=1}^{E}\hat{ \mathbf{F}}_{i}(\Phi_{i}^{\prime}-\Phi_{i})^{2}\), where \(\hat{\mathbf{F}}_{i}\in\mathbb{R}^{d}\) is the diagonal of \(\mathbf{F}(\mathcal{D}_{\mathcal{P}},\Phi_{i})\in\mathbb{R}^{d\times d}\) and the \(j\)-th value in \(\hat{\mathbf{F}}_{i}\) is computed as \(\mathbb{E}_{\mathcal{D}_{\mathcal{P}}}(\partial\mathcal{L}_{\mathcal{P}}/ \partial\Phi_{i,j})^{2}\). This is equivalent to elastic weight consolidation (EWC) [24].

_Diagonal of embedding-wise Fisher information matrix._ In different property prediction tasks, the impact of the same atoms and inter-atomic interactions may be significant or negligible. Therefore, we propose this choice to assign importance to parameters based on different embeddings, rather than treating each parameter individually. By defining \(\tilde{\Phi}_{i}=\sum_{j}\Phi_{i,j}\), the total update of the embedding \(\Phi_{i}\) can be represented as \(\Delta\Phi_{i}=\tilde{\Phi}_{i}^{\prime}-\tilde{\Phi}_{i}=\sum_{j}(\Phi_{i,j} ^{\prime}-\Phi_{i,j})\). Then, the Eq. (6) is reformulated to \(\mathcal{L}_{\text{Emb-EWC}}^{\text{EFIM}}=\frac{1}{2}\sum_{i=1}^{E}\tilde{ \mathbf{F}}_{i}(\tilde{\Phi}_{i}^{\prime}-\tilde{\Phi}_{i})^{2}\), where \(\tilde{\mathbf{F}}_{i}=\sum_{j}\mathbb{E}_{\mathcal{D}_{\mathcal{P}}}( \partial\mathcal{L}_{\mathcal{P}}/\partial\Phi_{i,j})^{2}\).

Detailed derivation is given in Appendix A. Intuitively, these three approximations employ different methods to assign importance to parameters. \(\mathcal{L}_{\text{Emb-BWC}}^{\text{IM}}\) assigns the same importance to each parameter, \(\mathcal{L}_{\text{Emb-BWC}}^{\text{FIM}}\) assigns individual importance to each parameter, and \(\mathcal{L}_{\text{Emb-BWC}}^{\text{EFIM}}\) assigns the same importance to parameters within the same embedding vector.

### Enabling contextual perceptiveness in MP-Adapter

For different property prediction tasks, the decisive substructures vary. As shown in Figure 2, the ester group in the given molecule determines the property SR-HSE, while the carbon-carbon triple bond determines the property SR-MMP. If fine-tuning can be guided by molecular context, encoding context-specific molecular representations allows for dynamic representations of molecules tailored to specific tasks and enables the modeling of the context-specific significance of substructures.

**Extracting molecular context information.** In each episode, we consider the labels of the support molecules on the target property and seen properties, as well as the labels of the query molecules on seen properties, as the context of this episode. We adopt the form of a graph to describe the context. Figure 3 demonstrates the transformation from original context data to a context graph. In the left table, the labels of molecules \(m_{1}^{q},m_{2}^{2}\) for property \(p_{t}\) are the prediction targets, and the other shaded values are the available context. The right side shows the context graph constructed based on the available context. Specifically, we construct context graph \(\mathcal{G}_{t}=(\mathcal{V}_{t},\mathbf{A}_{t},\mathbf{X}_{t})\) for episode \(E_{t}\). It contains \(M\) molecule nodes \(\{m\}\) and \(P\) property nodes \(\{p\}\). Three types of edges indicate different relationships between molecules and properties.

Then we employ a GNN-based context encoder: \(\mathbf{C}=\texttt{ContextEncoder}(\mathcal{V}_{t},\mathbf{A}_{t},\mathbf{X}_{t})\), where \(\mathbf{C}\in\mathbb{R}^{(M+P)\times d_{2}}\) denotes the learned context representation matrix for \(E_{t}\). \(\mathcal{V}_{t}\) and \(\mathbf{A}_{t}\) denote the node set

Figure 3: Convert the context information of a 2-shot episode into a context graph.

and the adjacent matrix of the context graph, respectively, and \(\mathbf{X}_{t}\) denotes the initial features of nodes. The features of molecule nodes are initialized with a pre-trained molecular encoder. The property nodes are randomly initialized. When we make the prediction of molecule \(m\)'s target property \(p\), we take the learned representations of the this molecule \(\bm{c}_{m}\) and of the target property \(\bm{c}_{p}\) as the context vectors. Details about the context encoder are provided in Appendix F.2.

**In-context tuning with molecular context information.** After obtaining the context vectors, we consider enabling the molecular encoder to use the context as a condition, achieving conditional molecular encoding. To achieve this, we further refine our adapter module. While neural conditional encoding has been explored in some domains, such as cross-attention [43] and ControlNet [68] for conditional image generation, these methods often come with a significant increase in the number of parameters. This contradicts our motivation of parameter-efficient tuning for few-shot tasks. In this work, we adopt a simple yet effective method. We directly concatenate the context with the output of the message passing layer, and feed them into the downscaling feed-forward layer in the MP-Adapter. Formally, the downscaling process defined in Eq. (3) is reformulated as:

\[\bm{z}^{(l)}=\texttt{FeedForward}_{\texttt{down}}(\bm{h}_{v}^{(l)}||\bm{c}_{ m}\|\bm{c}_{p}),\] (7)

where \(\|\) denotes concatenation. Such learned molecular representations are more easily predicted on specific properties, verified in Section 5.5 and Appendix G.

### Optimization

Following MAML [10], a gradient descent strategy is adopted. Firstly, \(B\) episodes \(\{E_{t}\}_{t=1}^{B}\) are randomly sampled. For each episode, in the inner-loop optimization, the loss on the support set is computed as \(\mathcal{L}_{t,\mathcal{S}}^{cls}(f_{\theta})\) and the parameters \(\theta\) are updated by gradient descent:

\[\mathcal{L}_{t,\mathcal{S}}^{cls}(f_{\theta})=-\sum\nolimits_{ \mathcal{S}_{t}}(y\log(\hat{y})+(1-y)\log(1-\hat{y})),\] (8) \[\theta^{\prime}\leftarrow\theta-\alpha_{inner}\nabla_{\theta} \mathcal{L}_{t,\mathcal{S}}^{cls}(f_{\theta}),\] (9)

where \(\alpha_{inner}\) is the learning rate. In the outer loop, the classification loss of query set is denoted as \(\mathcal{L}_{t,\mathcal{Q}}^{cls}\). Together with our Emb-BWC regularizer, the meta-training loss \(\mathcal{L}(f_{\theta^{\prime}})\) is computed and we do an outer-loop optimization with learning rate \(\alpha_{outer}\) across the mini-batch:

\[\mathcal{L}(f_{\theta^{\prime}})=\frac{1}{B}\sum\nolimits_{t=1}^ {B}\mathcal{L}_{t,\mathcal{Q}}^{cls}(f_{\theta^{\prime}})+\lambda\mathcal{L}_ {\texttt{Emb-BWC}},\] (10) \[\theta\leftarrow\theta-\alpha_{outer}\nabla_{\theta}\mathcal{L}( f_{\theta^{\prime}}),\] (11)

where \(\lambda\) is the weight of Emb-BWC regularizer. The pseudo-code is provided in Appendix B. We also provide more discussion of tunable parameter size and total model size in Appendix C.

## 5 Experiments

### Evaluation setups

**Datasets.** We use five common few-shot molecular property prediction datasets from the MoleculeNet [61]: Tox21, SIDER, MUV, ToxCast, and PCBA. Standard data splits for FSMPP are adopted. Dataset statistics and more details of datasets can be found in Appendix D.

**Baselines.** For a comprehensive comparison, we adopt two types of baselines: (1) methods with molecular encoders trained from scratch, including Siamese Network [25], ProtoNet [46], MAML [10], TPN [35], EGNN [22], and IterRefLSTM [1]; and (2) methods which leverage pre-trained molecular encoders, including Pre-GNN [20], Meta-MGNN [14], PAR [58], and GS-Meta [73]. More details about these baselines are in Appendix E.

**Metrics.** Following prior works [1; 58], ROC-AUC scores are calculated on the query set for each meta-testing task, to evaluate the performance of FSMPP. We run experiments 10 times with different random seeds and report the mean and standard deviations.

[MISSING_PAGE_FAIL:8]

For Emb-BWC, we verify the effectiveness of fine-tuning the embedding layers and regularizing them with different approximations of \(\mathcal{L}_{\text{Emb-BWC}}\) (Table 3). Since the embedding layers have relatively few parameters, direct fine-tuning can also enhance performance. Applying our proposed regularizers to fine-tuning can further improve the effects. Among the three regularizers, the \(\mathcal{L}_{\text{Emb-BWC}}^{\text{IM}}\) is the most effective. This indicates that keeping pre-trained parameters to some extent can better utilize pre-trained knowledge, but the parameters worth keeping in fine-tuning and the important parameters in pre-training revealed by Fisher information matrix are not completely consistent.

### Sensitivity analysis

**Effect of weight of Emb-BWC regularizer \(\lambda\).** Emb-BWC is applied on the embedding layers to limit the magnitude of parameter updates during fine-tuning. We vary the weight of this regularization \(\lambda\) from \(\{0.01,0.1,1,10\}\). The first subfigure in Figure 5 shows that the performance is best when \(\lambda=0.1\) or \(1\). When \(\lambda\) is too small, the parameters undergo too large updates on few-shot downstream datasets, leading to over-fitting and ineffectively utilizing the pre-trained knowledge. Too large \(\lambda\) causes the parameters of the embedding layers to be nearly frozen, which prevents effective adaptation.

**Effect of hidden dimension of MP-Adapter \(d_{2}\).** The results corresponding to different values of \(d_{2}\) from \(\{25,50,75,100,150\}\) are presented in the second subfigure of Figure 5. On the Tox21 dataset, we further analyze the impact of this hyper-parameter on the number of trainable parameters. As shown in Figure 5, the number of parameters that our method needs to train is significantly less than that required by the full fine-tuning method, such as GS-Meta, while our method also performs better in terms of ROC-AUC performance due to solving over-fitting and context perceptiveness issues. When \(d=50\), Pin-Tuning performs best on Tox21, and the number of parameters that need to train is only 14.2% of that required by traditional fine-tuning methods.

### Case study

We visualized the molecular representations learned by the GS-Meta and our Pin-Tuning's encoders in the 10-shot setting, respectively. As shown in Figure 6 and 7, Pin-Tuning can effectively adapt to different downstream tasks based on context information, generating property-specific molecular representations. Across different tasks, our method is more effective in eno

Figure 4: Effect of different hyper-parameters. The y-axis represents ROC-AUC scores (%) and the x-axis is the different hyper-parameters.

Figure 5: ROC-AUC (%) and number of trainable parameters of Pin-Tuning with varied value of \(d_{2}\) and full Fine-Tuning method (e.g., GS-Meta) on the Tox21 dataset.

\begin{table}
\begin{tabular}{c c|c c c} \hline \hline Fine-tune & Regularizer & Tox21 & SIDER & MUV & PCBA \\ \hline - & - & 89.70 & 90.12 & 70.76 & 80.24 \\ \hline ✓ & - & 90.17 & 92.06 & 72.37 & 80.74 \\ ✓ & \(\mathcal{L}_{\text{Emb-BWC}}^{\text{IM}}\) & **91.56** & **93.41** & **73.22** & **81.26** \\ ✓ & \(\mathcal{L}_{\text{Emb-BWC}}^{\text{IM}}\) & 90.93 & 90.09 & 72.17 & 80.78 \\ ✓ & \(\mathcal{L}_{\text{Emb-BWC}}^{\text{IM}}\) & 91.32 & 90.31 & 72.78 & 81.22 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation analysis on the Emb-BWC.

facilitate the prediction of the current property, reducing the difficulty of property prediction from the encoding representation aspect. More case studies are provided in Appendix G.

## 6 Conclusion

In this work, we propose a tuning method, Pin-Tuning, to address the ineffective fine-tuning of pre-trained molecular encoders in FSMPP tasks. Through the innovative parameter-efficient tuning and in-context tuning for pre-trained molecular encoders, our approach not only mitigates the issues of parameter-data imbalance but also enhances contextual perceptiveness. The promising results on public datasets underscore the potential of Pin-Tuning to advance this field, offering valuable insights for future research in drug discovery and material science.

## Acknowledgments

This work is jointly supported by National Science and Technology Major Project (2023ZD0120901), National Natural Science Foundation of China (62372454, 62236010) and the Excellent Youth Program of State Key Laboratory of Multimodal Artificial Intelligence Systems.

## References

* [1]H. Altae-Tran, B. Ramsundar, A. S. Pappu, and V. Pande (2017) Low data drug discovery with one-shot learning. ACS Central Science3 (4), pp. 283-293. Cited by: SS1.
* [2]D. Chen, Y. Zhu, J. Zhang, Y. Du, Z. Li, Q. Liu, S. Wu, and L. Wang (2023) Uncovering neural scaling laws in molecular representation learning. In NeurIPS, Cited by: SS1.
* [3]H. Chen and P. N. Garner (2024) Bayesian parameter-efficient fine-tuning for overcoming catastrophic forgetting. arXivabs/2402.12220. Cited by: SS1.
* [4]W. Chen, A. Tripp, and J. M. Hernandez-Lobato (2023) Meta-learning adaptive deep kernel gaussian processes for molecular property prediction. In ICLR, Cited by: SS1.
* [5]J. Deng, Z. Yang, H. Wang, I. Ojima, D. Samaras, and F. Wang (2023) A systematic study of key elements underlying molecular property prediction. Nature Communications14. Cited by: SS1.
* [6]N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen, C. Chan, W. Chen, J. Yi, W. Zhao, X. Wang, Z. Liu, H. Zheng, J. Chen, Y. Liu, J. Tang, J. Li, and M. Sun (2022) Delta tuning: a comprehensive study of parameter efficient methods for pre-trained language models. arXivabs/2203.06904. Cited by: SS1.
* [7]N. Ding, Y. Qin, G. Yang, F. Wei, Z. Yang, Y. Su, S. Hu, Y. Chen, C. Chan, W. Chen, J. Yi, W. Zhao, X. Wang, Z. Liu, H. Zheng, J. Chen, Y. Liu, J. Tang, J. Li, and M. Sun (2023) Parameter-efficient fine-tuning of large-scale pre-trained language models. Nature Machine Intelligence5, pp. 220-235. Cited by: SS1.
* [8]W. Dong, D. Yan, Z. Lin, and P. Wang (2023) Efficient adaptation of large vision transformer via adapter re-composing. Cited by: SS1.
* 134. Cited by: SS1.

[MISSING_PAGE_POST]

 Schoenholz* [12] Jonathan Godwin, Michael Schaarschmidt, Alexander L. Gaunt, Alvaro Sanchez-Gonzalez, Yulia Rubanova, Petar Velickovic, James Kirkpatrick, and Peter W. Battaglia. Simple GNN regularisation for 3d molecular property prediction and beyond. In _ICLR_, 2022.
* [13] Renxiang Guan, Zihao Li, Wenxuan Tu, Jun Wang, Yue Liu, Xianju Li, Chang Tang, and Ruyi Feng. Contrastive multiview subspace clustering of hyperspectral images based on graph convolutional networks. _IEEE Transactions on Geoscience and Remote Sensing_, 2024.
* [14] Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, and Nitesh V. Chawla. Few-shot graph learning for molecular property prediction. In _WWW_, 2021.
* [15] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In _ICLR_, 2022.
* [16] Timothy M. Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J. Storkey. Meta-learning in neural networks: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 44:5149-5169, 2020.
* [17] Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In _KDD_, 2022.
* [18] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In _ICML_, 2019.
* [19] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In _ICLR_, 2022.
* [20] Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In _ICLR_, 2020.
* [21] Dongki Kim, Jinheon Baek, and Sung Ju Hwang. Graph self-supervised learning with accurate discrepancy learning. In _NeurIPS_, 2022.
* [22] Jongmin Kim, Taesup Kim, Sungwoong Kim, and Chang D. Yoo. Edge-labeling graph neural network for few-shot learning. In _CVPR_, 2019.
* [23] Suyeon Kim, Dongha Lee, SeongKu Kang, Seonghyeon Lee, and Hwanjo Yu. Learning topology-specific experts for molecular property prediction. In _AAAI_, pages 8291-8299, 2023.
* [24] James Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. _Proceedings of the National Academy of Sciences_, 114, 117.
* [25] Gregory Koch, Richard Zemel, and Ruslan Salakhutdinov. Siamese neural networks for one-shot image recognition. In _ICML deep learning workshop_, 2015.
* [26] Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In _EMNLP (1)_, pages 3045-3059. Association for Computational Linguistics, 2021.
* [27] Han Li, Dan Zhao, and Jianyang Zeng. KPGT: knowledge-guided pre-training of graph transformer for molecular property prediction. In _KDD_, pages 857-867, 2022.
* [28] Shuangli Li, Jingbo Zhou, Tong Xu, Dejing Dou, and Hui Xiong. Geomgcl: Geometric graph contrastive learning for molecular property prediction. In _AAAI_, pages 4541-4549, 2022.
* [29] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In _ACL_, 2021.
* [30] Xin Li, Dongze Lian, Zhihe Lu, Jiawang Bai, Zhibo Chen, and Xinchao Wang. Graphadapter: Tuning vision-language models with dual knowledge graph. In _NeurIPS_, 2023.

* [31] Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with convolutional networks. In _ICML_, 2018.
* [32] Zhixun Li, Liang Wang, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao Luo, Xiangxin Zhou, Qiang Liu, Shu Wu, Liang Wang, and Jeffrey Xu Yu. GSLB: the graph structure learning benchmark. In _NeurIPS_, 2023.
* [33] Vladislav Lialin, Vijeta Deshpande, and Anna Rumshisky. Scaling down to scale up: A guide to parameter-efficient fine-tuning. _arXiv_, abs/2303.15647, 2023.
* [34] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. GPT understands, too. _arXiv_, abs/2103.10385, 2021.
* [35] Yanbin Liu, Juho Lee, Minesop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang. Learning to propagate labels: Transductive propagation network for few-shot learning. In _ICLR_, 2019.
* [36] Yue Liu, Jun Xia, Sihang Zhou, Siwei Wang, Xifeng Guo, Xihong Yang, Ke Liang, Wenxuan Tu, Stan Z. Li, and Xinwang Liu. A survey of deep graph clustering: Taxonomy, challenge, and application. _arXiv_, abs/2211.12875, 2022.
* [37] Yue Liu, Ke Liang, Jun Xia, Sihang Zhou, Xihong Yang, Xinwang Liu, and Stan Z. Li. Dink-net: Neural clustering on large graphs. In _ICML_, 2023.
* [38] Zhiyuan Liu, Yaorui Shi, An Zhang, Enzhi Zhang, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. Rethinking tokenizer and decoder in masked graph modeling for molecules. In _NeurIPS_, 2023.
* [39] Qiujie Lv, Guanxing Chen, Ziduo Yang, Weihe Zhong, and Calvin Yu-Chian Chen. Meta learning with graph attention networks for low-data drug discovery. _IEEE transactions on neural networks and learning systems_, 2023.
* [40] Ziqiao Meng, Yaoman Li, Peilin Zhao, Yang Yu, and Irwin King. Meta-learning with motif-based task augmentation for few-shot molecular property prediction. In _SDM_, 2023.
* [41] Cuong Q. Nguyen, Constantine Kreatsoulas, and Kim Branson. Meta-learning gnn initializations for low-resource molecular property prediction. In _ICML Workshop on Graph Representation Learning and Beyond_, 2020.
* [42] Jonas Pfeiffer, Aishwarya Kamath, Andreas Ruckle, Kyunghyun Cho, and Iryna Gurevych. Adapterfusion: Non-destructive task composition for transfer learning. In _EACL_, 2021.
* [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In _CVPR_, pages 10674-10685. IEEE, 2022.
* [44] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. In _NeurIPS_, 2020.
* [45] Johannes Schimunek, Philipp Seidl, Lukas Friedrich, Daniel Kuhn, Friedrich Rippmann, Sepp Hochreiter, and Gunter Klambauer. Context-enriched molecule representations improve few-shot drug discovery. In _ICLR_, 2023.
* [46] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. In _NeurIPS_, 2017.
* [47] Ying Song, Shuangjia Zheng, Zhangming Niu, Zhang-Hua Fu, Yutong Lu, and Yuedong Yang. Communicative representation learning on attributed molecular graphs. In _IJCAI_, 2020.
* [48] Yisheng Song, Ting Wang, Puyu Cai, Subrota K. Mondal, and Jyoti Prakash Sahoo. A comprehensive survey of few-shot learning: Evolution, applications, challenges, and opportunities. _ACM Computing Surveys_, 2023.

* [49] Megan Stanley, John Bronskill, Krzysztof Maziarz, Hubert Misztela, Jessica Lanini, Marwin H. S. Segler, Nadine Schneider, and Marc Brockschmidt. Fs-mol: A few-shot learning dataset of molecules. In _NeurIPS Datasets and Benchmarks_, 2021.
* [50] Hannes Stark, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Gunnemann, and Pietro Lio. 3d infomax improves gnns for molecular property prediction. In _ICML_, pages 20479-20502, 2022.
* [51] Mengying Sun, Jing Xing, Huijun Wang, Bin Chen, and Jiayu Zhou. Mocl: Data-driven molecular fingerprint via knowledge-aware contrastive learning from molecular graph. In _KDD_, 2021.
* [52] Xin Sun, Liang Wang, Qiang Liu, Shu Wu, Zilei Wang, and Liang Wang. DIVE: subgraph disagreement for graph out-of-distribution generalization. In _KDD_, 2024.
* [53] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In _NeurIPS_, page 3637-3645, 2016.
* [54] Liang Wang, Xiang Tao, Qiang Liu, Shu Wu, and Liang Wang. Rethinking graph masked autoencoders through alignment and uniformity. In _AAAI_, 2024.
* [55] Liang Wang, Shu Wu, Q. Liu, Yanqiao Zhu, Xiang Tao, and Mengdi Zhang. Bi-level graph structure learning for next poi recommendation. _IEEE Transactions on Knowledge and Data Engineering_, 36:5695-5708, 2024.
* [56] Xu Wang, Huan Zhao, Wei-Wei Tu, and Quanming Yao. Automated 3d pre-training for molecular property prediction. In _KDD_, pages 2419-2430, 2023.
* [57] Yaqing Wang, Quanming Yao, James T. Kwok, and Lionel M. Ni. Generalizing from a few examples: A survey on few-shot learning. _ACM Computing Surveys_, 2020.
* [58] Yaqing Wang, Abulikemu Abuduweili, Quanming Yao, and Dejing Dou. Property-aware relation networks for few-shot molecular property prediction. In _NeurIPS_, 2021.
* [59] Yaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee, Xiaodong Liu, Jing Gao, Ahmed Hassan Awadallah, and Jianfeng Gao. Adamix: Mixture-of-adaptations for parameter-efficient model tuning. In _EMNLP_, 2022.
* [60] Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning of representations via graph neural networks. _Nature Machine Intelligence_, 2022.
* [61] Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S. Pappu, Karl Leswing, and Vijay S. Pande. Moleculenet: A benchmark for molecular machine learning. _arXiv_, abs/1703.00564, 2017.
* [62] Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z. Li. Mole-bert: Rethinking pre-training graph neural networks for molecules. In _ICLR_, 2023.
* [63] Yi Xiao, Xiangxin Zhou, Qiang Liu, and Liang Wang. Bridging text and molecule: A survey on multimodal frameworks for molecule. _arXiv_, abs/2403.13830, 2024.
* [64] Yaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shuiwang Ji. Self-supervised learning of graph neural networks: A unified review. _TPAMI_, 2022.
* [65] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In _ICLR_, 2019.
* [66] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In _NeurIPS_, 2021.
* [67] Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro Sanchez-Gonzalez, Peter W. Battaglia, Razvan Pascanu, and Jonathan Godwin. Pre-training via denoising for molecular property prediction. In _ICLR_, 2023.

* [68] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In _ICCV_, pages 3813-3824. IEEE, 2023.
* [69] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In _ICLR_, 2023.
* [70] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and Yu Jiao Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention. In _ICLR_, 2024.
* [71] Zaixi Zhang, Qi Liu, Hao Wang, Chengqiang Lu, and Chee-Kong Lee. Motif-based graph self-supervised learning for molecular property prediction. In _NeurIPS_, pages 15870-15882, 2021.
* [72] Yanqiao Zhu, Dingshuo Chen, Yuanqi Du, Yingze Wang, Q. Liu, and Shu Wu. Molecular contrastive pretraining with collaborative featurizations. _Journal of Chemical Information and Modeling_, 2024.
* [73] Xiang Zhuang, Qiang Zhang, Bin Wu, Keyan Ding, Yin Fang, and Huajun Chen. Graph sampling-based meta-learning for molecular property prediction. In _IJCAI_, 2023.

**Appendix**

The organization of the appendix is as follows:

* Appendix A: Derivation of Emb-BWC regularization;
* Appendix B: Pseudo-code of training process;
* Appendix C: Discussion of tunable parameter size and total model size;
* Appendix D: Details of datasets;
* Appendix E: Details of baselines;
* Appendix F: Implementation details;
* Appendix G: More experimental results and discussions;
* Appendix H: Limitations and future directions.

## Appendix A Derivation of Emb-BWC regularization

### Derivation of \(\mathcal{L}_{\text{Emb-BWC}}\)

Let \(\Phi\in\mathbb{R}^{E\times d}\) be the pre-trained embeddings before fine-tuning, and \(\Phi^{\prime}\in\mathbb{R}^{E\times d}\) be the fine-tuned embeddings. Further, \(\Phi_{i}\in\mathbb{R}^{d}\) denotes the \(i\)-th row's embedding vector in \(\Phi\), and \(\Phi_{i,j}\in\mathbb{R}\) represents the \(j\)-th dimensional value of \(\Phi_{i}\).

The optimization of embedding layers can be interpreted as performing a maximum a posterior (MAP) estimation of the parameters \(\Phi^{\prime}\) given the pre-training data and training data of downstream FSMPP task, which is formulated in a Bayesian framework.

In the FSMPP setting, the molecular encoder has been pre-trained on the pre-training task \(\mathcal{P}\) using data \(\mathcal{D}_{\mathcal{P}}\), and is then fine-tuned on a downstream FSMPP task \(\mathcal{F}\) using data \(\mathcal{D}_{\mathcal{F}}\). The overall objective is to find the optimal parameters on task \(\mathcal{F}\) while preserving the prior knowledge obtained in pre-training on task \(\mathcal{F}\). Based on a prior \(p(\Phi^{\prime})\) of the embedding parameters, the posterior after observing the FSMPP task \(\mathcal{F}\) can be computed with Bayes' rule:

\[\begin{split} p(\Phi^{\prime}|\mathcal{D}_{\mathcal{P}},\mathcal{ D}_{\mathcal{F}})&=\frac{p(\mathcal{D}_{\mathcal{F}}|\Phi^{\prime}, \mathcal{D}_{\mathcal{P}})p(\Phi^{\prime}|\mathcal{D}_{\mathcal{P}})}{p( \mathcal{D}_{\mathcal{F}}|\mathcal{D}_{\mathcal{P}})}\\ &=\frac{p(\mathcal{D}_{\mathcal{F}}|\Phi^{\prime})p(\Phi^{\prime} |\mathcal{D}_{\mathcal{P}})}{p(\mathcal{D}_{\mathcal{F}})},\end{split}\] (12)

where \(\mathcal{D}_{\mathcal{F}}\) is assumed to be independent of \(\mathcal{D}_{\mathcal{P}}\). Taking a logarithm of the posterior, the MAP objective is therefore:

\[\begin{split}\Phi^{\prime*}&=\operatorname*{arg\, max}_{\Phi^{\prime}}\log p(\Phi^{\prime}|\mathcal{D}_{\mathcal{P}},\mathcal{ D}_{\mathcal{F}})\\ &=\operatorname*{arg\,max}_{\Phi^{\prime}}\log p(\mathcal{D}_{ \mathcal{F}}|\Phi^{\prime})+\log p(\Phi^{\prime}|\mathcal{D}_{\mathcal{P}}). \end{split}\] (13)

The first term \(\log p(\mathcal{D}_{\mathcal{F}}|\Phi^{\prime})\) is the log likelihood of the data \(\mathcal{D}_{\mathcal{F}}\) given the parameters \(\Phi^{\prime}\), which can be expressed as the training loss function on task \(\mathcal{F}=-\log p(\mathcal{D}_{\mathcal{F}}|\Phi^{\prime})\), denoted as \(\mathcal{L}_{\mathcal{F}}(\Phi^{\prime})\). The second term \(p(\Phi^{\prime}|\mathcal{D}_{\mathcal{P}})\) is the posterior of the parameters given the pre-training dataset \(\mathcal{D}_{\mathcal{P}}\). Since \(\Phi^{\prime}=[\Phi_{1}^{\prime\top},\Phi_{2}^{\prime\top},\dots,\Phi_{i}^{ \prime\top}]^{\top}\), and \(\Phi_{i}^{\prime}\) is conditionally independent of \(\Phi_{i}^{\prime}\) for \(i,j=\{1,\dots,E\}\) and \(i\neq j\) given condition \(\mathcal{D}_{\mathcal{P}}\), we have \(p(\Phi^{\prime}|\mathcal{D}_{\mathcal{P}})=\prod_{i=1}^{E}p(\Phi_{i}^{\prime}| \mathcal{D}_{\mathcal{P}})\). Thus, \(\log p(\Phi^{\prime}|\mathcal{D}_{\mathcal{P}})=\sum_{i=1}^{E}\log p(\Phi_{i}^ {\prime}|\mathcal{D}_{\mathcal{P}})\)

For adapting pre-trained molecular embedding layers to downstream FMSPP tasks, this posterior must encompass the prior knowledge of the pre-trained embedding layers to reflect which parameters are important for pre-training task \(\mathcal{P}\). Despite the true posterior being intractable, \(\log p(\Phi_{i}^{\prime}|\mathcal{D}_{\mathcal{P}})\) can be defined as a function \(f(\Phi_{i}^{\prime})\) and approximated around the optimum point \(f(\Phi_{i})\), where \(f(\Phi_{i})\) is the pre-trained values and \(\nabla f(\Phi_{i})=0\). Performing a second-order Taylor expansion on \(f(\Phi_{i}^{\prime})\) around \(\Phi_{i}\) gives:

\[\begin{split}\log p\left(\Phi_{i}^{\prime}\mid\mathcal{D}_{ \mathcal{P}}\right)&\approx f\left(\Phi_{i}\right)+\frac{1}{2} \left(\Phi_{i}^{\prime}-\Phi_{i}\right)^{T}\nabla^{2}f\left(\Phi_{i}\right) \left(\Phi_{i}^{\prime}-\Phi_{i}\right)\\ &=f\left(\Phi_{i}\right)+\frac{1}{2}\left(\Phi_{i}^{\prime}-\Phi_ {i}\right)^{T}\mathbf{H}(\mathcal{D}_{\mathcal{P}},\Phi_{i})\left(\Phi_{i}^{ \prime}-\Phi_{i}\right),\end{split}\] (14)where \(\mathbf{H}(\mathcal{D}_{\mathcal{P}},\Phi_{i})\in\mathbb{R}^{d\times d}\) is the Hessian matrix of \(f(\Phi_{i}^{\prime})\) at \(\Phi_{i}\). The second term suggests that the posterior of the parameters on the pre-training data can be approximated by a Gaussian distribution with mean \(\Phi_{i}^{\prime}\) and covariance \(\mathbf{H}(\mathcal{D}_{\mathcal{P}},\Phi_{i})^{-1}\). Following Eq. (13), the training objective becomes:

\[\Phi^{\prime*} =\operatorname*{arg\,max}_{\Phi^{\prime}}\log p(\mathcal{D}_{ \mathcal{F}}|\Phi^{\prime})+\log p(\Phi^{\prime}|\mathcal{D}_{\mathcal{P}})\] (15) \[=\operatorname*{arg\,min}_{\Phi^{\prime}}\mathcal{L}_{\mathcal{F }}(\Phi^{\prime})-\sum_{i=1}^{E}f(\Phi_{i})-\frac{1}{2}\sum_{i=1}^{E}\left( \Phi_{i}^{\prime}-\Phi_{i}\right)^{T}\mathbf{H}(\mathcal{D}_{\mathcal{P}}, \Phi_{i})\left(\Phi_{i}^{\prime}-\Phi_{i}\right)\] \[=\operatorname*{arg\,min}_{\Phi^{\prime}}\mathcal{L}_{\mathcal{F }}(\Phi^{\prime})-\frac{1}{2}\sum_{i=1}^{E}\left(\Phi_{i}^{\prime}-\Phi_{i} \right)^{T}\mathbf{H}(\mathcal{D}_{\mathcal{P}},\Phi_{i})\left(\Phi_{i}^{ \prime}-\Phi_{i}\right).\]

We define the second term as our Emb-BWC regularization objective:

\[\mathcal{L}_{\text{Emb-BWC}}=-\frac{1}{2}\sum_{i=1}^{E}(\Phi_{i}^{\prime}- \Phi_{i})^{\top}\mathbf{H}(\mathcal{D}_{\mathcal{P}},\Phi_{i})(\Phi_{i}^{ \prime}-\Phi_{i}).\] (16)

### Derivation of \(\mathcal{L}_{\text{Emb-BWC}}^{\text{FIM}}\)

Since the Fisher information matrix (FIM) \(\mathbf{F}\) is the negation of the expectation of the Hessian over the data distribution, i.e., \(\mathbf{F}=-\mathbb{E}_{\mathcal{D}_{\mathcal{P}}}[\mathbf{H}]\), the objective can be reformulated as:

\[\mathcal{L}_{\text{Emb-BWC}}^{\text{FIM}}=\frac{1}{2}\sum_{i=1}^{E}(\Phi_{i}^{ \prime}-\Phi_{i})^{\top}\mathbf{F}(\mathcal{D}_{\mathcal{P}},\Phi_{i})(\Phi_{ i}^{\prime}-\Phi_{i}),\] (17)

where \(\mathbf{F}(\mathcal{D}_{\mathcal{P}},\Phi_{i})\in\mathbb{R}^{d\times d}\) is the corresponding Fisher information matrix of \(\mathbf{H}(\mathcal{D}_{\mathcal{P}},\Phi_{i})\). Further, the Fisher information matrix can be further simplified with a diagonal approximation. Then, the objective is simplified to:

\[\mathcal{L}_{\text{Emb-BWC}}^{\text{FIM}}\approx\frac{1}{2}\sum_{i=1}^{E}\hat {\mathbf{F}}_{i}(\Phi_{i}^{\prime}-\Phi_{i})^{2},\] (18)

where \(\hat{\mathbf{F}}_{i}\in\mathbb{R}^{d}\) is the diagonal of \(\mathbf{F}(\mathcal{D}_{\mathcal{P}},\Phi_{i})\). According to the definition of the Fisher information matrix, the \(j\)-th value in \(\hat{\mathbf{F}}_{i}\) is computed as \(\mathbb{E}_{\mathcal{D}_{\mathcal{P}}}(\partial\mathcal{L}_{\mathcal{P}}/ \partial\Phi_{i,j})^{2}\). In this work, this approximated form is defined as \(\mathcal{L}_{\text{Emb-BWC}}^{\text{FIM}}\).

### Derivation of \(\mathcal{L}_{\text{Emb-BWC}}^{\text{EFIM}}\)

We assume that the parameters within an embedding should share the same importance. To this end, we define \(\tilde{\Phi}_{i}=\sum_{j}\Phi_{i,j}\), then the total update of the embedding \(\Phi_{i}\) can be represented as \(\Delta\Phi_{i}=\tilde{\Phi}_{i}^{\prime}-\tilde{\Phi}_{i}=\sum_{j}(\Phi_{i,j}^ {\prime}-\Phi_{i,j})\). Then, the objective in Eq. (16) is reformulated to:

\[\mathcal{L}_{\text{Emb-BWC}}^{\text{EFIM}}=\frac{1}{2}\sum_{i=1}^{E}\tilde{ \mathbf{H}}_{i}(\tilde{\Phi}_{i}^{\prime}-\tilde{\Phi}_{i})^{2},\] (19)

where \(\tilde{\mathbf{H}}_{i}=\frac{\partial^{2}\mathcal{L}_{\mathcal{P}}}{\partial \Phi_{i}^{2}}=\frac{\partial}{\partial\Phi_{i}}(\frac{\partial\mathcal{L}_{ \mathcal{P}}}{\partial\Phi_{i}})\). Next, we continue to derive \(\tilde{\mathbf{H}}_{i}\). Given that \(\tilde{\Phi}_{i}=\sum_{j=1}^{d}\Phi_{i,j}\), we first use the chain rule to find \(\frac{\partial\mathcal{L}_{\mathcal{P}}}{\partial\tilde{\Phi}_{i}}\) According to chain rule, the derivative of \(\mathcal{L}_{\mathcal{P}}\) with respect to \(\partial\tilde{\Phi}_{i}\) can be computed as:

\[\frac{\partial\mathcal{L}_{\mathcal{P}}}{\partial\tilde{\Phi}_{i}}=\sum_{j} \frac{\partial\mathcal{L}_{\mathcal{P}}}{\partial\Phi_{i,j}}\frac{\partial\Phi_ {i,j}}{\partial\tilde{\Phi}_{i}}.\] (20)

Since \(\tilde{\Phi}_{i}=\Phi_{i,1}+\Phi_{i,2}+\ldots+\Phi_{i,d}\), each of \(\frac{\partial\Phi_{i,j}}{\partial\tilde{\Phi}_{i}}\) for \(j=1,2,\ldots,d\) equals \(1\). Therefore, the equation simplifies to:

\[\frac{\partial\mathcal{L}_{\mathcal{P}}}{\partial\tilde{\Phi}_{i}}=\sum_{j=1}^{d }\frac{\partial\mathcal{L}_{\mathcal{P}}}{\partial\Phi_{i,j}}.\] (21)When taking the derivative of this with respect to \(\Phi_{i,j}\) again, using the chain rule, we have:

\[\frac{\partial^{2}\mathcal{L}_{\mathcal{P}}}{\partial\tilde{\Phi}_{i}^{2}}=\frac{ \partial}{\partial\tilde{\Phi}_{i}}(\sum_{j=1}^{d}\frac{\partial\mathcal{L}_{ \mathcal{P}}}{\partial\Phi_{i,j}})=\sum_{j=1}^{d}\sum_{k=1}^{d}\frac{\partial} {\partial\Phi_{i,k}}(\frac{\partial\mathcal{L}_{\mathcal{P}}}{\partial\Phi_{i, j}})\frac{\partial\Phi_{i,k}}{\partial\tilde{\Phi}_{i}}\] (22)

Given that \(\Phi_{i,j}(\Phi_{i,k})\) are all parameters in embedding lookup tables, they are independent of each other. Thus, when \(j=k\), \(\frac{\partial}{\partial\Phi_{i,k}}(\frac{\partial\mathcal{L}_{\mathcal{P}}}{ \partial\Phi_{i,j}})\frac{\partial\Phi_{i,k}}{\partial\Phi_{i}}=\frac{ \partial^{2}\mathcal{L}_{\mathcal{P}}}{\partial\Phi_{i,j}^{2}}\), otherwise it equals 0. We finally get \(\tilde{\mathbf{H}}_{i}=\sum_{j}\frac{\partial^{2}\mathcal{L}_{\mathcal{P}}}{ \partial\Phi_{i,j}^{2}}=\sum_{j}\mathbf{H}(\mathcal{D}_{\mathcal{P}},\Phi_{i} )_{j,j}\).

We still approximate the Hessian with the FIM as in Section A.2, and combining this with the definition of FIM, we arrive at the final objective:

\[\mathcal{L}_{\text{Emb-EWC}}^{\text{EFIM}}\approx\frac{1}{2}\sum_{i=1}^{E} \tilde{\mathbf{F}}_{i}(\tilde{\Phi}_{i}^{\prime}-\tilde{\Phi}_{i})^{2},\] (23)

where \(\tilde{\mathbf{F}}_{i}=\sum_{j}\mathbb{E}_{\mathcal{D}_{\mathcal{P}}}(\partial \mathcal{L}_{\mathcal{P}}/\partial\Phi_{i,j})^{2}\).

## Appendix B Pseudo-code of training process

To help better understand the training process, we provide the brief pseudo-code of it in Algorithm 1.

``` Input :Training set \(\mathcal{D}_{\text{train}}\) Output :Tuned few-shot molecular property prediction model with parameter \(\theta\)
1whilenot convergedo
2 Sample \(B\) episode from training set \(\mathcal{D}_{\text{train}}\) to form a mini-batch \(\{E_{t}\}_{t=1}^{B}\);
3for\(t=1\)to\(B\)do
4 Calculate classification loss on support set \(\mathcal{L}_{t,\mathcal{S}}^{cls}(f_{\theta})\) by Eq. (8) on \(E_{t}\): \(\theta^{\prime}\leftarrow\theta-\alpha_{inner}\nabla_{\theta}\mathcal{L}_{t, \mathcal{S}}^{cls}(f_{\theta})\);
5 Do inner-loop update by Eq. (9) on \(E_{t}\);
6 Calculate classification loss on query set \(\mathcal{L}_{t,\mathcal{Q}}^{cls}(f_{\theta^{\prime}})\) by Eq. (8) on \(E_{t}\);
7 Calculate update constraint \(\mathcal{L}_{\text{Emb-BWC}}\) by Eq. (6);
8 Do outer-loop optimization by Eq. (10) and Eq. (11): \(\theta\leftarrow\theta-\alpha_{outer}\nabla_{\theta}\mathcal{L}(f_{\theta^{ \prime}})\);
9Return optimized model parameter \(\theta\). ```

**Algorithm 1**Training process of Pin-Tuning.

## Appendix C Discussion of tunable parameter size and total model size

### Tunable parameter size of molecular encoder

We compare the tunable parameter size of full fine-tuning and our Pin-Tuning. Section 3.3 describes the parameters of the PME, which include those for the embedding layers and the message passing layers. We assume there are \(|E_{n}|\) original node features and \(|E_{e}|\) edge features. Considering there is one node embedding layer and \(L\) edge embedding layers, the total number of parameters for the embedding part is \(|E_{n}|d+L|E_{e}|d\). The parameters in the message passing layer consist of the 2-layer MLP including biases shown in Eq. (2) and its subsequent batch normalization, with each layer having \(L(2dd_{1}+d+d_{1}+2d)\) parameters. In summary, the total number of parameters to update in full fine-tuning is

\[N_{Fine-Tuning}=|E_{n}|d+L(|E_{e}|d+2dd_{1}+3d+d_{1}).\] (24)

In our Pin-Tuning method, the parameters of the embedding layers are still updated. However, in each message passing layer, the original parameters are completely frozen, and the parts that require updating are the two feed-forward layers and the layer normalization in the bottleneck adapter 

[MISSING_PAGE_FAIL:18]

_Trained-from-scratch methods:_

* **Siamese**[25]: Siamese is used to rank similarity between input molecule pairs with a dual network.
* **ProtoNet**[46]: ProtoNet learns a metric space for few-shot classification, enabling classification by calculating the distances between each query molecule and the prototype representations of each class.
* **MAML**[10]: MAML adapts the meta-learned parameters to achieve good generalization performance on new tasks with a small amount of training data and gradient steps.
* **TPN**[35]: TPN classifies the entire test set at once by learning to propagate labels from labeled instances to unlabeled test instances using a graph construction module that exploits the manifold structure in the data.
* **EGNN**[22]: EGNN predicts edge labels on a graph constructed from input samples to explicitly capture intra-cluster similarity and inter-cluster dissimilarity.
* **IterRefLSTM**[1]: IterRefLSTM adapts Matching Networks [53] to handle molecular property prediction tasks.

_Pre-trained methods:_

* **Pre-GNN**[20]: Pre-GNN is a classic pre-trained molecular model, taking the GIN as backbone and pre-training it with different self-supervised tasks.
* **Meta-MGNN**[14]: Meta-MGNN leverages Pre-GNN for learning molecular representations and incorporates meta-learning and self-supervised learning techniques.
* **PAR**[58]: PAR uses class prototypes to update input representations and designs label propagation for similar inputs in the relational graph, thus enabling the transformation of generic molecular embeddings into property-aware spaces.
* **GS-Meta**[73]: GS-Meta constructs a Molecule-Property relation graph (MPG) and redefines episodes in meta-learning as subgraphs of the MPG.

Following prior work [58], for the methods we reproduced, we use GIN as the graph-based molecular encoder to extract molecular representations. Specifically, we use the GIN provided by Pre-GNN [20] which consists of 5 GIN layers with 300-dimensional hidden units. Pre-GNN, Meta-MGNN, PAR, and GS-Meta further use the pre-trained GIN which is also provided by Pre-GNN.

## Appendix F Implementation details

### Hardware and software

Our experiments are conducted on Linux servers equipped with an AMD CPU EPYC 7742 (256) @ 2.250GHz, 256GB RAM and NVIDIA 3090 GPUs. Our model is implemented in PyTorch version 1.12.1, PyTorch Geometric version 2.3.1 (https://pyg.org/) with CUDA version 11.3, RDKit version 2023.3.3 and Python 3.9.18. Our code is available at: https://github.com/CRIPAC-DIG/Pin-Tuning.

### Model configuration

For featurization of molecules, we use atomic number and chirality tag as original atom features, as well as bond type and bond direction as bond features, which is in line with most molecular

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c} \hline \hline Assay Provider & \#Compound & \#Property & \#Train Property & \#Test Property & \%Label active & \%Label inactive & \%Missing Label \\ \hline APR & 1039 & 43 & 33 & 10 & 10.30 & 61.61 & 28.09 \\ ATG & 3423 & 146 & 106 & 40 & 5.92 & 93.92 & 0.16 \\ BSK & 1445 & 115 & 84 & 31 & 17.71 & 82.29 & 0.00 \\ CEETOX & 508 & 14 & 10 & 4 & 22.26 & 76.38 & 1.36 \\ CLD & 305 & 19 & 14 & 5 & 30.72 & 68.30 & 0.98 \\ NVS & 2130 & 139 & 100 & 39 & 3.21 & 4.52 & 92.27 \\ OT & 1782 & 15 & 11 & 4 & 9.78 & 87.78 & 2.44 \\ TOX21 & 8241 & 100 & 80 & 20 & 5.39 & 86.26 & 8.35 \\ Tanguay & 1039 & 18 & 13 & 5 & 8.05 & 90.84 & 1.11 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Statistics of sub-datasets of ToxCast.

pre-training methods. Following previous works, we set \(d=300\). For MLPs in Eq. (2), we use the ReLU activation with \(d_{1}=600\). Pre-trained GIN model provided by Pre-GNN [20] is adopted as the PTME in our framework. We tune the weight of update constraint (i.e., \(\lambda\)) in {0.01, 0.1, 1, 10}, tune the learning rate of inner loop (i.e., \(\alpha_{\text{inner}}\)) in {1e-3, 5e-3, 1e-2,5e-2,1e-1, 5e-1, 1, 5}, and tune the learning rate of outer loop (i.e., \(\alpha_{\text{outer}}\)) in {1e-5, 1e-4, 1e-3,1e-2,1e-1}. Based on the results of hyper-parameter tuning, we adopt \(\alpha_{\text{inner}}=0.5,\alpha_{\text{outer}}=1e-3\) and \(d_{2}=50\). The ContextEncoder(\(\cdot\)) described in Section 4.2 is implemented using a 2-layer message passing neural network [11]. In each MPNN layer, we employ a linear layer to aggregate messages from the neighborhoods of nodes and utilize distinct edge features to differentiate between various edge types in the context graphs. For baselines, we follow their recommended settings.

## Appendix G More experimental results and discussions

**More discussion of Figure 1.** The results show that molecular encoders with more molecule-specific inductive biases, such as CMPNN [47] and Graphormer [66], performed slightly worse than GIN-Mol [20] on this few-shot task. This is because more complex encoders require more parameters to provide inductive biases, which are difficult to train effectively under a few-shot setting.

**More main results.** The detailed comparison between Pin-Tuning and baseline models on sub-datasets of ToxCast are summarized in Table 7 and Table 8. Our method outperforms all baseline models under both the 10-shot and 5-shot settings, demonstrating the superiority of our method compared to existing methods.

**More discussion of ablation study.** Different datasets show varying sensitivity to the removal of components. On small-scale datasets like Tox21 and SIDER, removing components leads to a significant performance drop. On large-scale datasets like ToxCast and PCBA, the impact of removing components is less pronounced. This is because more episodes can be constructed on large-scale datasets, which aids in adaptation. This observation indicates that Pin-Tuning can bring considerable benefits in situations where data is extremely scarce.

**More case studies.** We provide more case studies in Figure 8 and 9 as a supplement to Section 5.5.

\begin{table}
\begin{tabular}{c|c c c c c c c c c} \hline \hline Model & APR & ATG & BSK & CEETOX & CLD & NVS & OT & TOX21 & Tanguay \\ \hline ProtoNet & 73.58 & 59.26 & 70.15 & 66.12 & 78.12 & 65.85 & 64.90 & 68.26 & 73.61 \\ MAML & 72.66 & 62.09 & 66.42 & 64.08 & 74.57 & 66.56 & 64.07 & 68.04 & 77.12 \\ TPN & 74.53 & 60.74 & 65.19 & 66.63 & 75.22 & 63.20 & 64.63 & 73.30 & 81.75 \\ EGNN & 80.33 & 66.17 & 73.43 & 66.51 & 78.85 & 71.05 & 68.21 & 76.40 & 85.23 \\ \hline Pre-GNN & 80.61 & 67.59 & 76.65 & 66.52 & 78.88 & 75.09 & 70.52 & 77.92 & 83.05 \\ Meta-MGNN & 81.47 & 69.20 & 78.97 & 66.57 & 78.30 & 79.60 & 69.55 & 78.77 & 83.98 \\ PAR & 86.09 & 72.72 & 82.45 & 72.12 & 83.43 & 74.94 & 71.96 & 82.81 & 88.20 \\ GS-Meta & 90.15 & 82.54 & 88.21 & 74.19 & 86.34 & 76.29 & 74.47 & 90.63 & 91.47 \\ \hline Pin-Tuning & **92.78** & **83.58** & **89.49** & **75.96** & **87.70** & **76.33** & **75.56** & **90.80** & **92.25** \\ \(\Delta\)_Improve._ & 2.92\% & 1.26\% & 1.45\% & 2.39\% & 1.58\% & 0.05\% & 1.46\% & 0.19\% & 0.85\% \\ \hline \hline \end{tabular}
\end{table}
Table 7: 10-shot performance on each sub-dataset of ToxCast.

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline \hline Model & APR & ATG & BSK & CEETOX & CLD & NVS & OT & TOX21 & Tanguay \\ \hline ProtoNet & 70.38 & 58.11 & 63.96 & 63.41 & 76.70 & 62.27 & 64.52 & 65.99 & 70.98 \\ MAML & 68.88 & 60.01 & 67.05 & 62.42 & 73.32 & 69.18 & 64.56 & 66.73 & 75.88 \\ TPN & 70.76 & 57.92 & 63.41 & 64.73 & 70.44 & 61.36 & 61.99 & 66.49 & 77.27 \\ EGNN & 74.06 & 60.56 & 64.60 & 63.20 & 71.44 & 62.62 & 66.70 & 65.33 & 75.69 \\ \hline Pre-GNN & 80.38 & 66.96 & 75.64 & 64.88 & 78.03 & 74.08 & 70.42 & 75.74 & 82.73 \\ Meta-MGNN & 81.22 & 69.90 & 79.67 & 65.78 & 77.53 & 73.99 & 69.20 & 76.25 & 83.76 \\ PAR & 83.76 & 70.24 & 80.82 & 69.51 & 81.32 & 70.60 & 71.31 & 79.71 & 84.71 \\ GS-Meta & 89.36 & 81.92 & 86.12 & 74.48 & 83.10 & 74.72 & 73.26 & 89.71 & 91.15 \\ Pin-Tuning & **89.94** & **82.37** & **87.61** & **75.20** & **85.07** & **75.49** & **74.70** & **90.89** & **92.14** \\ \(\Delta\)_Improve._ & 0.65\% & 0.55\% & 1.73\% & 0.97\% & 2.37\% & 1.03\% & 1.97\% & 1.32\% & 1.09\% \\ \hline \hline \end{tabular}
\end{table}
Table 8: 5-shot performance on each sub-dataset of ToxCast.

Figure 8: More molecular representations encoded by GS-Meta [73].

Figure 9: More molecular representations encoded by Pin-Tuning.

Limitations and future directions

As we discusses in Section 5.2, although our method significantly outperforms the state-of-the-art baseline method, our method exhibits higher standard deviations in the experimental results under multiple runs with different seeds.

We further speculate that these high standard deviations might be due to the uncertainty in the context information within episodes. The explicitly introduced molecular context, on one hand, provides effective guidance for tuning pre-trained molecular encoders, but on the other hand, this information also carries a high degree of uncertainty. We aim to model the target property through the molecule-property relationships within episodes, but each episode is obtained by sampling very few samples from the large space corresponding to the target property. The uncertainty between different episodes is relatively high. How to quantify and calibrate this uncertainty is another question worth exploring, which we will investigate in our future work.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We present our claims and main contributions in the abstract and introduction. We propose a parameter-efficient in-context tuning method, named Pin-tuning, to effectively adapt pre-trained molecular encoders to few-shot molecular property prediction tasks. Guidelines:
2. The answer NA means that the abstract and introduction do not include the claims made in the paper.
3. The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
4. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
5. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
6. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in Appendix H. Guidelines:
7. The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
8. The authors are encouraged to create a separate "Limitations" section in their paper.
9. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
10. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
11. The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
12. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
13. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
14. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
15. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provide the derivation of our proposed Emb-BWC regularization, with related assumptions in Appendix A.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The proposed method and experimental setups are clearly described. Additionally, the link of code and other details for reproducing are provided in Appendix F. The datasets are publicly available. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.
* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes]Justification: The link of code is provided in the abstract, and other details for environment and configurations are provided in Appendix F. The datasets are publicly available.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Please refer to Section 5.1 and Appendix F. Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report error bars in our experimental results, such as in Table 1. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.

* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please refer to Appendix F.1. Guidelines:
* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conforms with the NeurIPS Code of Ethics. Guidelines:
* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.
* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Our method addresses the few-shot molecular property prediction problem, which is important for drug discovery and material science, as discussed in the introduction and conclusion. Guidelines:
* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.

* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our research poses no such risks. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly respect and credit for existing assets. We cite relevant papers and comply with the licenses. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Our code is well documented. Guidelines:

* The answer NA means that the paper does not release new assets.

* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.