# The Bayesian Stability Zoo

Shay Moran

Department of Mathematics

& Department of Computer Science

Technion - Israel Institute of Technology;

smoran@technion.ac.il

Hilla Schefler

Department of Mathematics

Technion - Israel Institute of Technology

hillas@campus.technion.ac.il

&Jonathan Shafer

Computer Science Division

UC Berkeley

shaferjo@berkeley.edu

###### Abstract

We show that many definitions of stability found in the learning theory literature are equivalent to one another. We distinguish between two families of definitions of stability: _distribution-dependent_ and _distribution-independent Bayesian stability_. Within each family, we establish equivalences between various definitions, encompassing approximate differential privacy, pure differential privacy, replicability, global stability, perfect generalization, TV stability, mutual information stability, KL-divergence stability, and Renyi-divergence stability. Along the way, we prove boosting results that enable the amplification of the stability of a learning rule. This work is a step towards a more systematic taxonomy of stability notions in learning theory, which can promote clarity and an improved understanding of an array of stability concepts that have emerged in recent years.

## 1 Introduction

Algorithmic stability is a major theme in learning theory, where seminal results have firmly established its close relationship with generalization. Recent research has further highlighted the intricate interplay between stability and additional properties of interest beyond statistical generalization. These properties encompass privacy [14], fairness [1], replicability [1, 2], adaptive data analysis [1, 15], and mistake bounds in online learning [1, 2].

This progress has come with a proliferation of formal definitions of stability, including pure and approximate Differential Privacy [13, 1], Perfect Generalization [11], Global Stability [2], KL-Stability [14], TV-Stability [15], \(f\)-Divergence Stability [1], Renyi Divergence Stability [1], and Mutual Information Stability [16, 17], as well as related combinatorial quantities such as the Littlestone dimension [18] and the clique dimension [1].

It is natural to wonder to what extent these various and sundry notions of stability actually differ from one another. The type of equivalence we consider between definitions of stability is as follows.

_Definition A and Definition B are **weakly equivalent** if for every hypothesis class \(\mathcal{H}\) the following holds:_

\[\mathcal{H}\]

 _has a PAC learning rule that is_ _stable according to Definition A_ \[\iff\mathcal{H}\] _has a PAC learning rule that is_ _stable according to Definition B_

This type of equivalence is weak because it does _not_ imply that a learning rule satisfying one definition also satisfies the other.

Recent results show that many stability notions appearing in the literature are in fact weakly equivalent. The work of [1] has shown sample efficient reductions between approximate differential privacy, replicability, and perfect generalization. Combined with the work of [1, 2, 1, 1, 2], a rich web of equivalences is being uncovered between approximate differential privacy and other definitions of algorithmic stability (see Fig. 1).

In this paper we extend the study of equivalences between notions of stability, and make it more systematic. Our starting point is the following observation: many of the definitions mentioned above belong to a broad family of definitions of stability, which we informally call _Bayesian definitions of stability_. Definitions in this family roughly take the following form: a learning rule \(A\) is considered stable if the quantity

\[d\Big{(}A(S),\mathcal{P}\Big{)}\]

is small enough, where:

* \(d\) is a measure of dissimilarity between distributions.
* \(\mathcal{P}\) is a specific _prior distribution_ over hypotheses;
* \(A(S)\) is the _posterior distribution_, i.e., the distribution of hypotheses generated by the learning rule \(A\) when applied to the input sample \(S\).

Namely, a Bayesian definition of stability is parameterized by a choice of \(d\), a choice of \(\mathcal{P}\), and a specification of how small the dissimilarity is required to be.1

Footnote 1: An example for an application in the context of generalization is the classic PAC Bayes Theorem. The theorem assures that for every population distribution and any given prior \(\mathcal{P}\), the difference between the population error of an algorithm \(A\) and the empirical error of \(A\) is bounded by \(\tilde{O}\left(\frac{\sqrt{\mathbb{KL}(A(S),\mathcal{P})}}{m}\right)\), where \(m\) is the size of the input sample \(S\), and the KL divergence is the “measure of dissimilarity” between the prior and the posterior. See e.g. Theorem 3.2.

**Remark 1.1**.: _To understand our choice of the name Bayesian stability, recall that the terms prior and posterior come from Bayesian statistics. In Bayesian statistics the analyst has some prior distribution over possible hypothesis before conducting the analysis, and chooses a posterior distribution over hypotheses when the analysis is complete. Bayesian stability is defined in terms of the dissimilarity between these two distributions._

A central insight of this paper is that there exists a meaningful distinction between two types of Bayesian definitions, based on whether the choice of the prior \(\mathcal{P}\) depends on the population distribution \(\mathcal{D}\):

* Distribution-_independent_ (DI) stability. These are Bayesian definitions of stability in which \(\mathcal{P}\) is some fixed prior that depends only on the class \(\mathcal{H}\) and the learning rule \(A\), and does not depend on the population distribution \(\mathcal{D}\). Namely, they take the form: \[\exists\text{ prior }\mathcal{P}\ \forall\text{ population }\mathcal{D}\ \forall m\in\mathbb{N}:\ d(A(S),\mathcal{P})\text{ is small},\] where \(S\sim\mathcal{D}^{m}\).
* Distribution-_dependent_ (DD) stability. Here, the prior may depend also on \(\mathcal{D}\), so each population distribution \(\mathcal{D}\) might have a different prior. Namely: \[\forall\text{ population }\mathcal{D}\ \exists\text{ prior }\mathcal{P}_{\mathcal{D}}\ \forall m\in\mathbb{N}:\ d(A(S),\mathcal{P}_{\mathcal{D}})\text{ is small}.\]

A substantial body of literature has investigated the interconnections among distribution-dependent definitions. In Theorem 1.4, we provide a comprehensive summary of the established equivalences. A natural question arises as to whether a similar web of equivalences exists for distribution-independent definitions. Our principal contribution is to affirm that, indeed, such a network exists. Identifying such equivalences is a step towards creating a comprehensive taxonomy of stability definitions.

### Our Contribution

Our first main contribution is an equivalence between distribution-independent definitions of stability.

**Theorem** (**Informal Version of Theorem 2.1)**.: _The following definitions of stability are weakly equivalent:_

1. _Pure Differential Privacy;_ 1. _Pure Differential Privacy;_ 2. _Distribution-Independent_ \(\mathsf{KL}\)_-Stability;_ 1. _Distribution-Independent One-Way Pure Perfect Generalization;_ 2. _Distribution-Independent_ \(\mathsf{D}_{\alpha}\)_-Stability for_ \(\alpha\in(1,\infty)\)_._ 2. _Distribution-Independent_ \(\mathsf{D}_{\alpha}\)_-Stability for_ \(\alpha\in(1,\infty)\)_._ 3. _Where_ \(\mathsf{D}_{\alpha}\) _is the Renyi divergence of order_ \(\alpha\)_. Furthermore, a hypothesis class_ \(\mathcal{H}\) _has a PAC learning rule that is stable according to one of these definitions if and only if_ \(\mathcal{H}\) _has finite fractional clique dimension (See Appendix_ B.1_)._

**Remark 1.2**.: _Observe that DI \(\mathsf{KL}\)-stability is equivalent to DI \(\mathsf{D}_{1}\)-stability, and DI one-way pure perfect generalization is equivalent to DI \(\mathsf{D}_{\infty}\)-stability. Therefore, The above theorem can be viewed as stating a weak equivalence between pure differential privacy and \(\mathsf{D}_{\alpha}\)-stability for \(\alpha\in[1,\infty]\)._

**Remark 1.3**.: _In this paper we focus purely on the information-theoretic aspects of learning under stability constraints, and therefore we consider learning rules that are mathematical functions, and disregard considerations of computability and computational complexity._

Table 1 summarizes the distribution-independent definitions discussed in Theorem 2.1. All the definitions in each row are weakly equivalent.

One example for how the equivalence results can help build bridges between different stability notions in the literature is the connection between pure differential privacy and the PAC-Bayes theorem. Both of these are fundamental ideas that have been extensively studied. Theorem 2.1 states that a hypothesis class admits a pure differentially private PAC learner if and only if it admits a distribution independent \(\mathsf{KL}\)-stable PAC learner. This is an interesting and non-trivial connection between two well studied notions. As a concrete example of this connection, recall that thresholds over the real line cannot be learned by a differentially private learner [1]. Hence, by Theorem 2.1, there does not exist a PAC learner for thresholds that is \(\mathsf{KL}\)-stable. Another example is half-spaces with margins in \(\mathbb{R}^{d}\). Half-spaces with margins are differentially private learnable [10], therefore there exists a PAC learner for half-spaces with margins that is \(\mathsf{KL}\)-stable.

Our second main contribution is a boosting result for weak learners that have bounded \(\mathsf{KL}\)-divergence with respect to a distribution-independent prior. Our result demonstrates that distribution-independent \(\mathsf{KL}\)-stability is boostable. It is interesting to see that one can simultaneously boost both the stability and the learning parameters of an algorithm.

**Theorem** (**Informal Version of Theorem 2.2)**.: _Let \(\mathcal{H}\) be a hypothesis class. If there exists a weak learner \(A\) for \(\mathcal{H}\), and there exists a prior distribution \(\mathcal{P}\) such that the expectation of \(\mathsf{KL}(A(S)\parallel\mathcal{P})\) is bounded, then there exists a \(\mathsf{KL}\)-stable PAC learner that admits a logarithmic divergence bound._

The proof of Theorem 2.2 relies on connections between boosting of PAC learners and online learning with expert advice.

\begin{table}
\begin{tabular}{l l l} \hline \hline Name & Dissimilarity & Definition \\ \hline \(\mathsf{KL}\)-Stability & \(\mathbb{P}_{S}[\mathsf{KL}(A(S)\parallel\mathcal{P})\leq o(m)]\geq 1-o(1)\) & 3.6 \\ \(\mathsf{D}_{\alpha}\)-Stability & \(\mathbb{P}_{S}[\mathsf{D}_{\alpha}(A(S)\parallel\mathcal{P})\leq o(m)]\geq 1-o(1)\) & 3.6 \\ Pure Perfect Generalization & \(\mathbb{P}_{S}\big{[}\forall\mathcal{O}:\;A(S)(\mathcal{O})\leq e^{o(m)} \mathcal{P}(\mathcal{O})\big{]}\geq 1-o(1)\) & 3.7 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Distribution-independent Bayesian definitions of stability.

[MISSING_PAGE_EMPTY:4]

* \(\mathcal{H}\) _has a PAC learning rule that is stable according to one of the definitions 1 to 6 (and the cardinality of the domain is as described above);_
* \(\mathcal{H}\) _has finite Littlestone dimension; (Definition C.3)_
* \(\mathcal{H}\) _has finite clique dimension. (Definition C.5)_

We emphasize that Theorem 1.4 is a summary of existing results, and is not a new result. We believe that our compilation serves as a valuable resource, and that stating these results here in a unified framework helps to convey the conceptual message of this paper. Namely, the fact that a large number of disparate results can neatly be organized based on our notions of distribution-dependent and distribution-independent definitions of stability is a valuable observation that can help researchers make sense of the stability landscape.

### Related Works

The literature on stability is vast. Stability has been studied in the context of optimization, statistical estimation, regularization (e.g., [10] and [21]), the bias-variance tradeoff, algorithmic stability (e.g., [1]; see bibliography in Section 13.6 of [1]), bagging [1], online learning and optimization and bandit algorithms (e.g., [14]; see bibliography in Section 28.6 of [11]), and other topics.

There are numerous definitions of stability, including pure and approximate Differential Privacy [13, 15], Perfect Generalization [9], Global Stability [10], KL-Stability [16], TV-Stability [17], \(f\)-Divergence Stability [1], Renyi Divergence Stability [1], and Mutual Information Stability [18, 19].

Our work is most directly related to the recent publication by Bun et al. [1]. They established connections and separations between replicability, approximate differential privacy, max-information and perfect generalization for a broad class of statistical tasks. The reductions they present are sample-efficient, and nearly all are computationally efficient and apply to a general outcome space. Their results are central to the understanding of equivalences between notions of stability as laid out in the current paper.

A concurrent work by Kalavasis et al. [17] showed that TV-stability, replicability and approximate differential privacy are equivalent; this holds for general statistical tasks on countable domains, and for PAC learning on any domain. They also provide a statistical amplification and TV-stability boosting algorithm for PAC learning on countable domains.

Additionally, recent works [1, 18] have shown an equivalence between differential privacy and robustness for estimation tasks.

Theorem 2.2 is a boosting result. Boosting has been a central topic of study in computational learning theory since its inception in the 1990s by Schapire [10] and Freund [14]. The best-known boosting algorithm is AdaBoost [12], which has been extensively studied. Boosting also has rich connections with other topics such as game theory, online learning, and convex optimization (see [13], Chapter 10 in [15], and Chapter 7 in [16]).

## 2 Technical Overview

This section presents the complete versions of Theorems 1.4 and 2.2. We provide a concise overview of the key ideas and techniques employed in the proofs. All proofs appear in the appendices.

\begin{table}
\begin{tabular}{l l c c} \hline \hline Name & Dissimilarity & Definition & References \\ \hline \(\mathsf{KL}\)-Stability & \(\mathbb{P}_{S}[\mathsf{KL}(A(S)\mathbin{\|}\mathcal{P}_{\mathcal{D}})\leq o(m)] \geq 1-o(1)\) & 3.6 & [16] \\ TV-Stability & \(\mathbb{E}_{S}[\mathsf{TV}(A(S),\mathcal{P}_{\mathcal{D}})]\leq o(1)\) & 3.13 & [17] \\ \(\mathsf{MI}\)-Stability & \(\mathbb{E}_{S}[\mathsf{KL}(A(S)\mathbin{\|}\mathcal{P}_{\mathcal{D}})]\leq o(m)\) & 3.12 & [18] \\ Perfect Generalization & \(\mathbb{P}_{S}[\forall O:\,A(S)(\mathcal{O})\leq e^{\varepsilon}\mathcal{P}_{ \mathcal{D}}(\mathcal{O})+\delta]\geq 1-o(1)\) & 3.7 & [18] \\ Global Stability & \(\mathbb{P}_{S,h\sim\mathcal{P}_{\mathcal{D}}}[A(S)=h]\geq\eta\) & 3.11 & [19] \\ Replicability & \(\mathbb{P}_{r\sim\mathcal{R}}\big{[}\mathbb{P}_{S,h\sim\mathcal{P}_{\mathcal{D},r}}[A(S;r)=h_{r}]\geq\eta\big{]}\geq\nu\) & 3.10 & [19] \\ \hline \hline \end{tabular}
\end{table}
Table 2: Distribution-dependent Bayesian definitions of stability.

Please refer to Section 3 for a complete overview of preliminaries, including all technical terms and definitions.

### Equivalences between DI Bayesian Notions of Stability

The following theorem, which is one of the main results of this paper, shows the equivalence between different distribution-independent definitions. The content of Theorem 2.1 is summarized in Table 1.

**Theorem 2.1** (Distribution-Independent Equivalences).: _Let \(\mathcal{H}\) be a hypothesis class. The following is equivalent._

1. _There exists a learning rule that PAC learns_ \(\mathcal{H}\) _and satisfied pure differential privacy (Definition_ 3.5_)._
2. \(\mathcal{H}\) _has finite fractional clique dimension._
3. _For every_ \(\alpha\in[1,\infty]\)_, there exists a learning rule that PAC learns_ \(\mathcal{H}\) _and satisfied distribution-independent_ \(\mathsf{D}_{\alpha}\)_-stability (Definition_ 3.6_)._
4. _For every_ \(\alpha\in[1,\infty]\)_, there exists a distribution-independent_ \(\mathsf{D}_{\alpha}\)_-stable PAC learner_ \(A\) _for_ \(\mathcal{H}\)_, that satisfies the following:_ 1. \(A\) _is interpolating almost surely. Namely, for every_ \(\mathcal{H}\)_-realizable distribution_ \(\mathcal{D}\)_,_ \(\mathbb{P}_{S\sim\mathcal{D}^{m}}[\mathrm{L}_{S}(A(S))=0]=1\)_._ 2. \(A\) _admits a divergence bound of_ \(f(m)=O(\log m)\)_, with confidence_ \(\beta(m)\equiv 0\)_. I.e., for every_ \(\mathcal{H}\)_-realizable distribution_ \(\mathcal{D}\)_,_ \(\mathsf{D}_{\alpha}(A(S)\,\|\,\mathcal{P})\leq O(\log m)\) _with probability_ \(1\)_, where_ \(S\sim\mathcal{D}^{m}\) _and_ \(\mathcal{P}\) _is a prior distribution independent of_ \(\mathcal{D}\)_._ 3. _For every_ \(\mathcal{H}\)_-realizable distribution_ \(\mathcal{D}\)_, the expected population loss of_ \(A\) _with respect to_ \(\mathcal{D}\) _satisfies_ \(\mathbb{E}_{S\sim\mathcal{D}^{m}}[\mathrm{L}_{\mathcal{D}}(A(S))]\leq O\left( \sqrt{m^{-1}\log m}\right)\)_._

_In particular, plugging \(\alpha=1\) in Item (ii) implies \(\mathsf{KL}\)-stability with divergence bound of \(f(m)=O(\log m)\) and confidence \(\beta(m)\equiv 0\). Plugging \(\alpha=\infty\) implies distribution-independent one-way \(\varepsilon\)-pure perfect generalization, with \(\varepsilon(m)\leq O(\log m)\) and confidence \(\beta(m)\equiv 0\)._

#### 2.1.1 Proof Idea for Theorem 2.1

We prove the following chain of implications:

\[\text{Pure DP}\stackrel{{(\ref{eq:def})}}{{ \Longrightarrow}}\mathsf{D}_{\infty}\text{-Stability}\stackrel{{(\ref{eq:def})}}{{ \Longrightarrow}}\mathsf{D}_{\alpha}\text{-Stability}\;\forall\alpha\in[1,\infty] \stackrel{{(\ref{eq:def})}}{{\Longrightarrow}}\text{Pure DP}.\]

Pure DP\(\implies\mathsf{D}_{\infty}\)-Stability.The first step towards proving implication (1) is to define a suitable prior distribution \(\mathcal{P}\) over hypotheses. The key tool we used in order to define \(\mathcal{P}\) is the characterization of pure DP via the fractional clique dimension [1]. In a nutshell, [1] proved that (i) a class \(\mathcal{H}\) is pure DP learnable if and only if the fractional clique dimension of \(\mathcal{H}\) is finite; (ii) the fractional clique dimension is finite if and only if there exists a polynomial \(q(m)\) and a distribution over hypothesis \(\mathcal{P}_{m}\), such that for every realizable sample \(S\) of size \(m\), we have

\[\mathbb{P}_{h\sim\mathcal{P}_{m}}[\mathrm{L}_{S}(h)=0]\geq\frac{1}{q(m)}.\] (1)

(For more details please refer to Appendix B.1.) Now, the desired prior distribution \(\mathcal{P}\) is defined to be a mixture of all the \(\mathcal{P}_{m}\)'s.

The next step in the proof is to define a learning rule \(A\): (i) sample hypotheses from the prior \(\mathcal{P}\); (ii) return the first hypothesis \(h\) that is consistent with the input sample \(S\) (i.e. \(\mathrm{L}_{S}(h)=0\)). \(A\) is well-defined since with high probability it will stop and return a hypothesis after \(\approx q(m)\) re-samples from \(\mathcal{P}\). Since the posterior \(A(S)\) is supported on \(\{h:\mathrm{L}_{S}(h)=0\}\), a simple calculation which follows from Equation (1) shows that for every realizable distribution \(\mathcal{D}\), \(\mathsf{D}_{\infty}(A(S)\,\|\,\mathcal{P})\leq\log(q(m))\) almost surly where \(S\sim\mathcal{D}^{m}\).

Finally, since for \(\alpha\in[1,\infty]\) the Renyi divergence \(\mathsf{D}_{\alpha}(\mathcal{Q}_{1}\,\|\,\mathcal{Q}_{2})\) is non-decreasing in \(\alpha\) (see Lemma A.1), we conclude that \(\mathsf{KL}(A(S)\,\|\,\mathcal{P})\leq O(\log m)\), hence by PAC-Bayes theorem \(A\) generalizes.

\(\mathsf{D}_{\infty}\)-Stability\(\implies\mathsf{D}_{\alpha}\)-Stability\(\forall\alpha\in[1,\infty]\).This implication is immediate since the Renyi divergence \(\mathsf{D}_{\alpha}(\mathcal{Q}_{1}\,\|\,\mathcal{Q}_{2})\) is non-decreasing in \(\alpha\).

\(\mathsf{D}_{\alpha}\)-**Stability**\(\forall\alpha\in[1,\infty]\implies\)**Pure DP.** In fact, it suffices to assume \(\mathsf{KL}\)-stability. We prove that the promised prior \(\mathcal{P}\) satisfies that for every realizable sample \(S\) of size \(m\), we have \(\mathbb{P}_{h\sim\mathcal{P}}[\mathsf{L}_{S}(h)=0]\geq\frac{1}{\mathsf{poly}(m)}\), and conclude that \(\mathcal{H}\) is pure DP learnable. Given a realizable sample \(S\) of size \(m\), we uniformly sample \(\approx m\log m\) examples from \(S\) and feed the new sample \(S^{\prime}\) to the promised \(\mathsf{KL}\)-stable learner \(A\). By noting that if \(\mathsf{KL}(A(S^{\prime})\parallel\mathcal{P})\) is small, one can lower bound the probability of an event according to \(\mathcal{P}\) by its probability according to \(A(S^{\prime})\). The proof then follows by applying a standard concentration argument.

### Stability Boosting

We prove a boosting result for weak learners with bounded \(\mathsf{KL}\) with respect to a distribution-independent prior. We show that every learner with bounded \(\mathsf{KL}\) that slightly beats random guessing can be amplified to a learner with logarithmic \(\mathsf{KL}\) and expected loss of \(O(\sqrt{m^{-1}\log m})\).

**Theorem 2.2** (Boosting Weak Learners with Bounded \(\mathsf{KL}\)).: _Let \(\mathcal{X}\) be a set, let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\) be a hypothesis class, and let \(A\) be a learning rule. Assume there exists \(k\in\mathbb{N}\) and \(\gamma>0\) such that_

\[\forall\mathcal{D}\in\mathsf{Realizable}(\mathcal{H}):\ \mathbb{E}_{S\sim \mathcal{D}^{k}}[\mathsf{L}_{\mathcal{D}}(A(S))]\leq\frac{1}{2}-\gamma,\] (2)

_and there exists \(\mathcal{P}\in\Delta\big{(}\{0,1\}^{\mathcal{X}}\big{)}\) and \(b\geq 0\) such that_

\[\forall\mathcal{D}\in\mathsf{Realizable}(\mathcal{H}):\ \mathbb{E}_{S\sim \mathcal{D}^{k}}[\mathsf{KL}(A(S)\parallel\mathcal{P})]\leq b.\] (3)

_Then, there exists an interpolating learning rule \(A^{\star}\) that PAC learns \(\mathcal{H}\) with logarithmic \(\mathsf{KL}\)-stability. More explicitly, there exists a prior distribution \(\mathcal{P}^{\star}\in\Delta\big{(}\{0,1\}^{\mathcal{X}}\big{)}\) and function \(b^{\star}\) and \(\varepsilon^{\star}\) that depend on \(\gamma\) and \(b\) such that_

\[\forall\mathcal{D}\in\mathsf{Realizable}(\mathcal{H})\ \forall m\in\mathbb{N}:\] \[\mathbb{P}_{S\sim\mathcal{D}^{m}}[\mathsf{KL}(A^{\star}(S) \parallel\mathcal{P}^{\star})\leq b^{\star}(m)=O(\log(m))]=1,\] (4) _and_ \[\mathbb{E}_{S\sim\mathcal{D}^{m}}[\mathsf{L}_{\mathcal{D}}(A^{ \star}(S))]\leq\varepsilon^{\star}(m)=O\Bigg{(}\sqrt{\frac{\log(m)}{m}}\Bigg{)}.\] (5)

#### 2.2.1 Proof Idea for Theorem 2.2

The strong learning rule \(A^{\star}\) is obtained by simulating the weak learner \(A\) on \(O(\log m/\gamma^{2})\) samples of constant size \(k\) (which are carefully sampled from the original input sample \(S\)). Then, \(A^{\star}\) returns an aggregated hypothesis - the majority vote of the outputs of \(A\). As it turns out, \(A^{\star}\) satisfies logarithmic \(\mathsf{KL}\)-stability with respect to the prior \(\mathcal{P}^{\star}\) that is a mixture of majority votes of the original prior \(\mathcal{P}\). The analysis involves a reduction to regret analysis of online learning using expert advice, and also uses properties of the \(\mathsf{KL}\)-divergence.

## 3 Preliminaries

### Divergences

The Renyi \(\alpha\)-divergence is a measure of dissimilarity between distributions that generalizes many common dissimilarity measures, including the Bhattacharyya coefficient (\(\alpha=1/2\)), the Kullback-Leibler divergence (\(\alpha=1\)), the log of the expected ratio (\(\alpha=2\)), and the log of the maximum ratio (\(\alpha=\infty\)).

**Definition 3.1** (Renyi divergence; [12, 13]).: _Let \(\alpha\in(1,\infty)\). The Renyi divergence of order \(\alpha\) of the distribution \(\mathcal{P}\) from the distribution \(\mathcal{Q}\) is_

\[\mathsf{D}_{\alpha}(\mathcal{P}\parallel\mathcal{Q})=\frac{1}{\alpha-1}\log \left(\mathbb{E}_{x\sim\mathcal{P}}\Bigg{[}\bigg{(}\frac{\mathcal{P}(x)}{ \mathcal{Q}(x)}\bigg{)}^{\alpha-1}\Bigg{]}\right).\]_For \(\alpha=1\) and \(\alpha=\infty\) the Renyi divergence is extended by taking a limit. In particular, the limit \(\alpha\to 1\) gives the Kullback-Leibler divergence,_

\[\mathsf{D}_{1}(\mathcal{P}\parallel\mathcal{Q}) =\mathbb{E}_{x\sim\mathcal{P}}\bigg{[}\log\frac{\mathcal{P}(x)}{ \mathcal{Q}(x)}\bigg{]}=\mathsf{KL}(\mathcal{P}\parallel\mathcal{Q}),\] _and_ \[\mathsf{D}_{\infty}(\mathcal{P}\parallel\mathcal{Q}) =\log\left(\operatorname*{ess\,sup}_{\mathcal{P}}\frac{\mathcal{ P}(x)}{\mathcal{Q}(x)}\right),\]

_with the conventions that \(0/0=0\) and \(x/0=\infty\) for \(x>0\)._

### Learning Theory

We use standard notation from statistical learning (e.g., [1]). Given a hypothesis \(h:\mathcal{X}\to\{0,1\}\), the _empirical loss_ of \(h\) with respect to a sample \(S=\{(x_{1},y_{1}),\ldots,(x_{m},y_{m})\}\) is defined as \(\mathrm{L}_{S}(h)=\frac{1}{m}\sum_{i=1}^{m}\mathds{1}[h(x_{i})\neq y_{i}]\). A learning rule \(A\) is _interpolating_ if for every input sample \(S\), \(\mathbb{P}_{h\sim A(S)}[\mathrm{L}_{S}(h)=0]=1\). The _population loss_ of \(h\) with respect to a population distribution \(\mathcal{D}\) over \(\mathcal{X}\times\{0,1\}\) is defined as \(\mathrm{L}_{\mathcal{D}}(h)=\mathbb{P}_{(x,y)\sim\mathcal{D}}[h(x)\neq y]\). A population \(\mathcal{D}\) over labeled examples is _realizable_ with respect to a class \(\mathcal{H}\) if \(\inf_{h\in\mathcal{H}}\mathrm{L}_{\mathcal{D}}(h)=0\). We denote the set of all realizable population distributions of a class \(\mathcal{H}\) by \(\operatorname*{Realizable}(\mathcal{H})\). Given a learning rule \(A\) and an input sample \(S\) of size \(m\), the _population loss_ of \(A(S)\) with respect to a population \(\mathcal{D}\) is defined as \(\mathbb{E}_{h\sim A(S)}[\mathrm{L}_{\mathcal{D}}(h)]\).

A hypothesis class \(\mathcal{H}\) is _Probably Approximately Correct (PAC) learnable_ if there exists a learning rule \(A\) such that for all \(\mathcal{D}\in\operatorname*{Realizable}(\mathcal{H})\) and for all \(m\in\mathbb{N}\), we have \(\mathbb{E}_{S\sim\mathcal{D}^{m}}[\mathrm{L}_{\mathcal{D}}(A(S))]\leq\varepsilon(m)\), where \(\lim_{m\to\infty}\varepsilon(m)=0\).

**Theorem 3.2** (PAC-Bayes Bound; [12, 13, 14]; Theorem 31.1 in [1]).: _Let \(\mathcal{X}\) be a set, let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\), and let \(\mathcal{D}\in\Delta(\mathcal{X}\times\{0,1\})\). For any \(\beta\in(0,1)\) and for any \(\mathcal{P}\in\Delta(\mathcal{H})\),_

\[\operatorname*{\mathbb{P}}_{S\sim\mathcal{D}^{m}}\bigg{[}\forall\mathcal{Q}\in \Delta(\mathcal{H}):\ \mathrm{L}_{\mathcal{D}}(\mathcal{Q})\leq\mathrm{L}_{S}(\mathcal{Q})+\sqrt{ \frac{\mathsf{KL}(\mathcal{Q}\parallel\mathcal{P})+\ln(m/\beta)}{2(m-1)}} \bigg{]}\geq 1-\beta.\]

### Definitions of Stability

Throughout the following section, let \(\mathcal{X}\) be a set called the _domain_, let \(\mathcal{H}\subseteq\{0,1\}^{\mathcal{X}}\) be a hypothesis class, and let \(m\in\mathbb{N}\) be a sample size. A _randomized learning rule_, or a _learning rule_ for short, is a function \(A:\ \left(\mathcal{X}\times\{0,1\}\right)^{*}\to\Delta\big{(}\{0,1\}^{ \mathcal{X}}\big{)}\) that takes a training sample and outputs a distribution over hypotheses. A _population distribution_ is a distribution \(\mathcal{D}\in\Delta(\mathcal{X}\times\{0,1\})\) over labeled domain elements, and a _prior distribution_ is a distribution \(\mathcal{P}\in\Delta\big{(}\{0,1\}^{\mathcal{X}}\big{)}\) over hypotheses.

#### 3.3.1 Differential Privacy

Differential privacy is a property of an algorithm that guarantees that the output will not reveal any meaningful amount of information about individual people that contributed data to the input (training data) used by the algorithm. See [13] for an introduction.

**Definition 3.3**.: _Let \(\varepsilon,\delta\in\mathbb{R}_{\geq 0}\), and let \(\mathcal{P}\) and \(\mathcal{Q}\) be two probability measures over a measurable space \((\Omega,\mathcal{F})\). We say that \(\mathcal{P}\) and \(\mathcal{Q}\) are \((\varepsilon,\delta)\)-indistinguishable and write \(\mathcal{P}\approx_{\varepsilon,\delta}\mathcal{Q}\), if for every event \(\mathcal{O}\in\mathcal{F}\), \(\mathcal{P}(\mathcal{O})\leq e^{\varepsilon}\cdot\mathcal{Q}(\mathcal{O})+\delta\) and \(\overline{\mathcal{Q}(\mathcal{O})\leq e^{\varepsilon}\cdot\mathcal{P}( \mathcal{O})+\delta}\)._

**Definition 3.4** (Differential Privacy; [13]).: _Let \(\varepsilon,\delta\in\mathbb{R}_{\geq 0}\). A learning rule \(A\) is \((\varepsilon,\delta)\)-differentially private if for every pair of training samples \(S,S^{\prime}\in(\mathcal{X}\times\{0,1\})^{m}\) that differ on a single example, \(A(S)\) and \(A(S^{\prime})\) are \((\varepsilon,\delta)\)-indistinguishable._

Typically, \(\varepsilon\) is chosen to be a small constant (e.g., \(\varepsilon\leq 0.1\)) and \(\delta\) is negligible (i.e., \(\delta(m)\leq m^{-\omega(1)}\)). When \(\delta=0\) we say that \(A\) satisfies _pure_ differentially privacy.

**Definition 3.5** (Private PAC Learning).: \(\mathcal{H}\) _is privately learnable or DP learnable if it is PAC learnable by a learning rule \(A\) which is \((\varepsilon(m),\overline{\delta(m)})\)-differentially-private, where \(\varepsilon(m)\leq 1\) and \(\delta(m)=m^{-\omega(1)}\). \(A\) is pure DP learnable if the same holds with \(\delta(m)=0\)._

#### 3.3.2 \(\mathsf{D}_{\alpha}\)-Stability and \(\mathsf{KL}\)-Stability

**Definition 3.6** (\(\mathsf{D}_{\alpha}\)-Stability).: _Let \(\alpha\in[1,\infty]\). Let \(A\) be a learning rule, and let \(f:\mathbb{N}\to\mathbb{R}\) and \(\beta:\mathbb{N}\to[0,1]\) satisfy \(f(m)=o(m)\) and \(\beta(m)=o(1)\)._1. _A is distribution-independent_ \(\mathsf{D}_{\alpha}\)_-stable if_ \[\exists\text{ prior }\mathcal{P}\;\forall\text{ population }\mathcal{D}\;\forall m\in\mathbb{N}:\;\mathbb{P}_{S\sim\mathcal{D}^{m}}[\mathsf{D}_{ \alpha}(A(S)\,\|\,\mathcal{P})\leq f(m)]\geq 1-\beta(m).\]
2. _A is distribution-dependent_ \(\mathsf{D}_{\alpha}\)_-stable if_ \[\forall\text{ population }\mathcal{D}\;\exists\text{ prior }\mathcal{P}_{\mathcal{D}}\;\forall m\in\mathbb{N}:\;\mathbb{P}_{S\sim \mathcal{D}^{m}}[\mathsf{D}_{\alpha}(A(S)\,\|\,\mathcal{P}_{\mathcal{D}})\leq f (m)]\geq 1-\beta(m).\]

_The function \(f\) is called the divergence bound and \(\beta\) is called the confidence. The special case of \(\alpha=1\) is referred to as \(\mathsf{KL}\)-stability [13]._

#### 3.3.3 Perfect Generalization

**Definition 3.7** (One-Way Perfect Generalization).: _Let \(A\) be a learning rule, and let \(\beta:\mathbb{N}\to[0,1]\) satisfy \(\beta(m)=o(1)\)._

1. _Let_ \(\varepsilon:\mathbb{N}\to\mathbb{R}\) _satisfy_ \(\varepsilon(m)=o(m)\)_._ _A is_ \(\underline{\varepsilon}\)_-pure perfectly generalizing with confidence_ \(\beta\) _if_ \[\exists\text{ prior }\mathcal{P}\;\forall\text{ population }\mathcal{D}\;\forall m\in\mathbb{N}:\;\mathbb{P}_{S\sim\mathcal{D}^{m}} \big{[}\forall\mathcal{O}:\;A(S)(\mathcal{O})\leq e^{\varepsilon(m)}\mathcal{ P}(\mathcal{O})\big{]}\geq 1-\beta(m).\]
2. _([_1_]__[_16_]__:) Let_ \(\varepsilon,\delta\in\mathbb{R}_{\geq 0}\)_. A is_ \(\underline{(\varepsilon,\delta)}\)_-approximately perfectly generalizing with confidence_ \(\beta\) _if_ \[\forall\text{ population }\mathcal{D}\;\exists\text{ prior }\mathcal{P}_{\mathcal{D}}\;\forall m\in\mathbb{N}:\;\mathbb{P}_{S\sim \mathcal{D}^{m}}[\forall\mathcal{O}:\;A(S)(\mathcal{O})\leq e^{\varepsilon} \mathcal{P}_{\mathcal{D}}(\mathcal{O})+\delta]\geq 1-\beta(m).\]

#### 3.3.4 Replicability

**Definition 3.8** (Replicability; [1, 11]).: _Let \(\rho\in\mathbb{R}_{>0}\) and let \(\mathcal{R}\) be a distribution over random strings. A learning rule \(A\) is \(\rho\)-replicable if_

\[\forall\text{ population }\mathcal{D},\forall m:\;\mathbb{P}_{\begin{subarray}{ c}S_{1},S_{2}\sim\mathcal{D}^{m}\\ r\sim\mathcal{R}\end{subarray}}[A(S_{1};r)=A(S_{2};r)]\geq\rho,\]

_where \(r\) represents the random coins of \(A\)._

**Remark 3.9**.: _Note that both in [1, 1] and in [11] the definition of \(\rho\)-replicability is slightly different. In their definition, they treat the parameter \(\rho\) as the failure probability, i.e., \(A\) is a \(\rho\)-replicable learning rule by their definition if the probability that \(A(S_{1};r)=A(S_{2};r)\) is at least \(1-\rho\)._

There exists an alternative 2-parameter definition of replicability introduced in [11].

**Definition 3.10** (\((\eta,\nu)\)-Replicability; [1, 1]).: _Let \(\eta,\nu\in\mathbb{R}_{>0}\) and let \(\mathcal{R}\) be a distribution over random strings. Coin tosses \(r\) are \(\underline{\eta}\)-good for a learning rule \(A\) with respect to a population distribution \(\mathcal{D}\) if there exists a canonical output \(h_{r}\) such that for every \(m\), \(\mathbb{P}_{S\sim\mathcal{D}^{m}}[A(S;r)=h_{r}]\geq\eta\). A learning rule \(A\) is \(\underline{(\eta,\nu)}\)-replicable if_

\[\forall\text{ population }\mathcal{D}:\;\mathbb{P}_{r\sim\mathcal{R}}[r\text{ is } \eta\text{-good}]\geq\nu.\]

#### 3.3.5 Global Stability

**Definition 3.11** (Global Stability; [1]).: _Let \(\eta>0\) be a global stability parameter. A learning rule \(A\) is \((m,\eta)\)-globally stable with respect to a population distribution \(\mathcal{D}\) if there exists a canonical output \(h\) such that \(\mathbb{P}[A(S)=h]\geq\eta\), where the probability is over \(S\sim\mathcal{D}^{m}\) as well as the internal randomness of \(A\)._

#### 3.3.6 \(\mathsf{Ml}\)-Stability

**Definition 3.12** (Mutual Information Stability; [1, 1]).: _A learning rule \(A\) is \(\mathsf{Ml}\)-stable if there exists \(f:\mathbb{N}\to\mathbb{N}\) with \(f=o(m)\) such that_

\[\forall\text{ population }\mathcal{D}\;\forall m\in\mathbb{N}:I(A(S),S)\leq f(m),\]

_where \(S\sim\mathcal{D}^{m}\)._

#### 3.3.7 \(\mathsf{TV}\)-Stability

**Definition 3.13** (\(\mathsf{TV}\)-Stability; Appendix A.3.1 in [16]).: _Let \(A\) be a learning rule, and let \(f:\mathbb{N}\to\mathbb{N}\) satisfy \(f(m)=o(1)\)._

1. \(A\) _is distribution-independent_ \(\mathsf{TV}\)_-stable if_ \[\exists\text{ prior }\mathcal{P}\;\forall\text{ population }\mathcal{D}\;\forall m \in\mathbb{N}:\;\mathbb{E}_{S\sim\mathcal{D}^{m}}[\mathsf{TV}(A(S),\mathcal{P })]\leq f(m).\]
2. \(A\) _is distribution-dependent_ \(\mathsf{TV}\)_-stable if_ \[\forall\text{ population }\mathcal{D}\;\exists\text{ prior }\mathcal{P}_{\mathcal{D}}\;\forall m \in\mathbb{N}:\;\mathbb{E}_{S\sim\mathcal{D}^{m}}[\mathsf{TV}(A(S),\mathcal{P }_{\mathcal{D}})]\leq f(m).\]

#### 3.3.8 Max Information

**Definition 3.14**.: _Let \(A\) be a learning rule, and let \(\varepsilon,\delta\in\mathbb{R}_{\geq 0}\). A has \((\varepsilon,\delta)\)-max-information with respect to product distributions if for every event \(\mathcal{O}\) we have_

\[\mathbb{P}[(A(S),S)\in\mathcal{O}]\leq e^{\varepsilon}\mathbb{P}[(A(S),S^{ \prime})\in\mathcal{O}]+\delta\]

_where are \(S,S^{\prime}\) are independent samples drawn i.i.d from a population distribution \(\mathcal{D}\)._

## Acknowledgements

SM is a Robert J. Shillman Fellow; he acknowledges support by ISF grant 1225/20, by BSF grant 2018385, by an Azrieli Faculty Fellowship, by Israel PBC-VATAT, by the Technion Center for Machine Learning and Intelligent Systems (MLIS), and by the the European Union (ERC, GENERALIZATION, 101039692). HS acknowledges support by ISF grant 1225/20, and by the the European Union (ERC, GENERALIZATION, 101039692). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency. Neither the European Union nor the granting authority can be held responsible for them. JS was supported by DARPA (Defense Advanced Research Projects Agency) contract #HR001120C0015 and the Simons Collaboration on The Theory of Algorithmic Fairness.

## References

* [ABL\({}^{+}\)22] Noga Alon, Mark Bun, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private and online learnability are equivalent. _Journal of the ACM_, 69(4):28:1-28:34, 2022. doi:10.1145/3526074.
* [ALMM19] Noga Alon, Roi Livni, Maryanthe Malliaris, and Shay Moran. Private PAC learning implies finite Littlestone dimension. In Moses Charikar and Edith Cohen, editors, _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019_, pages 852-860. ACM, 2019. doi:10.1145/3313276.3316312.
* [AMSY23] Noga Alon, Shay Moran, Hilla Schefler, and Amir Yehudayoff. A unified characterization of private learnability via graph theory. _CoRR_, abs/2304.03996, 2023, 2304.03996. doi:10.48550/arXiv.2304.03996.
* [AUZ23] Hilal Asi, Jonathan R. Ullman, and Lydia Zakynthinou. From robustness to privacy and back. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 1121-1146. PMLR, 2023. URL https://proceedings.mlr.press/v202/asi23b.html.
* [BE02] Olivier Bousquet and Andre Elisseeff. Stability and generalization. _Journal of Machine Learning Research_, 2:499-526, 2002. URL http://jmlr.org/papers/v2/bousquet02a.html.
* [BGH\({}^{+}\)23] Mark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann Pitassi, Satchit Sivakumar, and Jessica Sorrell. Stability is stable: Connections between replicability, privacy, and adaptive generalization. In Barna Saha and Rocco A.

Servedio, editors, _Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023_, pages 520-527. ACM, 2023. doi:10.1145/3564246.3585246.
* [BLM20] Mark Bun, Roi Livni, and Shay Moran. An equivalence between private classification and online prediction. In Sandy Irani, editor, _61st IEEE Annual Symposium on Foundations of Computer Science, FOCS 2020, Durham, NC, USA, November 16-19, 2020_, pages 389-402. IEEE, 2020. doi:10.1109/FOCS46700.2020.00044.
* [BMN\({}^{+}\)18] Raef Bassily, Shay Moran, Ido Nachum, Jonathan Shafer, and Amir Yehudayoff. Learners that use little information. In Firdaus Janoos, Mehryar Mohri, and Karthik Sridharan, editors, _Algorithmic Learning Theory, ALT 2018, 7-9 April 2018, Lanzarote, Canary Islands, Spain_, volume 83 of _Proceedings of Machine Learning Research_, pages 25-55. PMLR, 2018. URL http://proceedings.mlr.press/v83/bassily18a.html.
* The 22nd Conference on Learning Theory, Montreal, Quebec, Canada, June 18-21, 2009_, 2009. URL http://www.cs.mcgill.ca/%7Ecolt2009/papers/032.pdf#page=1.
* [Bre96] Leo Breiman. Bagging predictors. _Machine Learning_, 24(2):123-140, 1996. doi:10.1007/BF00058655.
* [CLN\({}^{+}\)16] Rachel Cummings, Katrina Ligett, Kobbi Nissim, Aaron Roth, and Zhiwei Steven Wu. Adaptive learning with robust generalization guarantees. In Vitaly Feldman, Alexander Rakhlin, and Ohad Shamir, editors, _Proceedings of the 29th Conference on Learning Theory, COLT 2016, New York, USA, June 23-26, 2016_, volume 49 of _JMLR Workshop and Conference Proceedings_, pages 772-814. JMLR.org, 2016. URL http://proceedings.mlr.press/v49/cummings16.html.
* [DFH\({}^{+}\)15a] Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Roth. The reusable holdout: Preserving validity in adaptive data analysis. _Science_, 349(6248):636-638, 2015.
* [DFH\({}^{+}\)15b] Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Aaron Leon Roth. Preserving statistical validity in adaptive data analysis. In Rocco A. Servedio and Ronitt Rubinfeld, editors, _Proceedings of the Forty-Seventh Annual ACM on Symposium on Theory of Computing, STOC 2015, Portland, OR, USA, June 14-17, 2015_, pages 117-126. ACM, 2015. doi:10.1145/2746539.2746580.
* EUROCRYPT 2006, 25th Annual International Conference on the Theory and Applications of Cryptographic Techniques, St. Petersburg, Russia, May 28
- June 1, 2006, Proceedings_, volume 4004 of _Lecture Notes in Computer Science_, pages 486-503. Springer, 2006. doi:10.1007/11761679_29.
* [DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity in private data analysis. In Shai Halevi and Tal Rabin, editors, _Theory of Cryptography, Third Theory of Cryptography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006, Proceedings_, volume 3876 of _Lecture Notes in Computer Science_, pages 265-284. Springer, 2006. doi:10.1007/11681878_14.
* [DR14] Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. _Foundations and Trends in Theoretical Computer Science_, 9(3-4):211-407, 2014. doi:10.1561/0400000042.
* [EGI20] Amedeo Roberto Esposito, Michael Gastpar, and Ibrahim Issa. Robust generalization via \(f\)-mutual information. In _IEEE International Symposium on Information Theory, ISIT 2020, Los Angeles, CA, USA, June 21-26, 2020_, pages 2723-2728. IEEE, 2020. doi:10.1109/ISIT44484.2020.9174117.
* [Fre95] Yoav Freund. Boosting a weak learning algorithm by majority. _Information and Computation_, 121(2):256-285, 1995. doi:10.1006/INCO.1995.1136.

* Freund and Schapire [1997] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. _Journal of Computer and System Sciences_, 55(1):119-139, 1997. doi: 10.1006/JCSS.1997.1504.
* Hannan [1958] James Hannan. Approximation to Bayes risk in repeated play. In _Contributions to the Theory of Games (AM-39), Volume III_, pages 97-140, Princeton, 1958. Princeton University Press. doi: 10.1515/9781400882151-006.
* Hopkins et al. [2023] Samuel B. Hopkins, Gautam Kamath, Mahbod Majid, and Shyam Narayanan. Robustness implies privacy in statistical estimation. In Barna Saha and Rocco A. Servedio, editors, _Proceedings of the 55th Annual ACM Symposium on Theory of Computing, STOC 2023, Orlando, FL, USA, June 20-23, 2023_, pages 497-506. ACM, 2023. doi: 10.1145/3564246.3585115.
* Hebert-Johnson et al. [2018] Ursula Hebert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum. Multi-calibration: Calibration for the (computationally-identifiable) masses. In Jennifer G. Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018_, volume 80 of _Proceedings of Machine Learning Research_, pages 1944-1953. PMLR, 2018. URL http://proceedings.mlr.press/v80/hebert-johnson18a.html.
* 24, 2022_, pages 818-831. ACM, 2022. doi: 10.1145/3519935.3519973.
* Kalavasis et al. [2023] Alkis Kalavasis, Amin Karbasi, Shay Moran, and Grigoris Velegkas. Statistical indistinguishability of learning algorithms. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA_, volume 202 of _Proceedings of Machine Learning Research_, pages 15586-15622. PMLR, 2023. URL https://proceedings.mlr.press/v202/kalavasis23a.html.
* Littlestone [1987] Nick Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. _Machine Learning_, 2(4):285-318, 1987. doi: 10.1007/BF00116827.
* Livni and Moran [2020] Roi Livni and Shay Moran. A limitation of the PAC-Bayes framework. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ec79d4bed810ed64267d169b0d37373e-Abstract.html.
* Lattimore and Szepesvari [2020] Tor Lattimore and Csaba Szepesvari. _Bandit Algorithms_. Cambridge University Press, 2020. doi: 10.1017/9781108571401.
* July 1, 2001_, pages 290-297. Morgan Kaufmann, 2001.
* McA99] David A. McAllester. Some PAC-Bayesian theorems. _Machine Learning_, 37(3):355-363, 1999. doi: 10.1023/A:1007618624809.
* McA03] David A. McAllester. Simplified PAC-Bayesian margin bounds. In Bernhard Scholkopf and Manfred K. Warmuth, editors, _Computational Learning Theory and Kernel Machines, 16th Annual Conference on Computational Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC, USA, August 24-27, 2003, Proceedings_, volume 2777 of _Lecture Notes in Computer Science_, pages 203-215. Springer, 2003. doi: 10.1007/978-3-540-45167-9_16.
* Malliaris and Moran [2003] Maryanthe Malliaris and Shay Moran. The unstable formula theorem revisited. _CoRR_,abs/2212.05050, 2022, 2212.05050. doi: 10.48550/arXiv.2212.05050.
* Mohri et al. [2018] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of Machine Learning_. Adaptive computation and machine learning. MIT Press, second edition, 2018. URL https://mitpress.mit.edu/9780262039406.
* Phillips [1962] David L. Phillips. A technique for the numerical solution of certain integral equations of the first kind. _Journal of the ACM_, 9(1):84-97, 1962. doi: 10.1145/321105.321114.
* July 1, 2022_, pages 3055-3060. IEEE, 2022. doi: 10.1109/ISIT50566.2022.9834457.
* Polyanskiy and Wu [2023] Yury Polyanskiy and Yihong Wu. _Information Theory: From Coding to Learning_. Cambridge University Press. URL https://people.lids.mit.edu/yp/homepage/data/itbook-export.pdf. Unpublished manuscript, 2023.
* Renyi [1961] Alfred Renyi. On measures of entropy and information. In _Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics_, volume 4, pages 547-562. University of California Press, 1961.
* Shalev-Shwartz and Ben-David [2014] Shai Shalev-Shwartz and Shai Ben-David. _Understanding Machine Learning: From Theory to Algorithms_. Cambridge University Press, 2014. doi: https://doi.org/10.1017/CBO9781107298019.
* Schapire and Freund [2012] Robert E. Schapire and Yoav Freund. _Boosting: Foundations and Algorithms_. MIT Press, 2012. doi: https://doi.org/10.7551/mitpress/8291.001.0001.
* She90] Saharon Shelah. _Classification Theory: And the Number of Non-Isomorphic Models_, volume 92 of _Studies in Logic and the Foundations of Mathematics_. North-Holland, second edition, 1990.
* Tik43] A. N. Tikhonov. On the stability of inverse problems. _Proceedings of the USSR Academy of Sciences_, 39:195-198, 1943. URL https://api.semanticscholar.org/CorpusID:202866372.
* van Erven and Harremoes [2014] Tim van Erven and Peter Harremoes. Renyi divergence and Kullback-Leibler divergence. _IEEE Trans. Inf. Theory_, 60(7):3797-3820, 2014. doi: 10.1109/TIT.2014.2320500.
* Xu and Raginsky [2017] Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning algorithms. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA_, pages 2524-2533, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/ad71c82b22f4f65b9398f76d8be4c615-Abstract.html.

Proof for Theorem 2.2 (Stability Boosting)

### Information Theoretic Preliminaries

**Lemma A.1** (Monotonicity of Renyi divergence; Theorem 3 in [14]).: _Let \(0\leq\alpha<\beta\leq\infty\). Then \(\mathsf{D}_{\alpha}(\mathcal{P}\parallel\mathcal{Q})\leq\mathsf{D}_{\beta}( \mathcal{P}\parallel\mathcal{Q})\). Furthermore, the inequality is an equality if and only if \(\mathcal{P}\) equals the conditional \(\mathcal{Q}(\cdot\mid A)\) for some event \(A\)._

**Lemma A.2** (Data Processing Inequality; Theorem 9 and Eq. 13 in [14]).: _Let \(\alpha\in[0,\infty]\). Let \(X\) and \(Y\) be random variables, and let \(F_{Y|X}\) be the law of \(Y\) given \(X\). Let \(\mathcal{P}_{Y},\mathcal{Q}_{Y}\) be the distributions of \(Y\) when \(X\) is sampled from \(\mathcal{P}_{X},\mathcal{Q}_{X}\), respectively. Then_

\[\mathsf{D}_{\alpha}(\mathcal{P}_{Y}\mathbin{\|}\mathcal{Q}_{Y})\leq\mathsf{D} _{\alpha}(\mathcal{P}_{X}\mathbin{\|}\mathcal{Q}_{X}).\]

One interpretation of this is that processing an observation makes it more difficult to determine whether it came from \(\mathcal{P}_{X}\) or \(\mathcal{Q}_{X}\).

**Definition A.3** (Conditional \(\mathsf{KL}\)-divergence; Definition 2.12 in [4]).: _Given joint distributions \(\mathcal{P}(x,y),\mathcal{Q}(x,y)\), the \(\mathsf{KL}\)-divergence of the marginals \(\mathcal{P}(y|x),\mathcal{Q}(y|x)\) is_

\[\mathsf{KL}(\mathcal{P}(y|x)\mathbin{\|}\mathcal{Q}(y|x))=\sum_{x}\mathcal{P}( x)\sum_{y}\mathcal{P}(y|x)\log\frac{\mathcal{P}(y|x)}{\mathcal{Q}(y|x)}.\]

**Lemma A.4** (Chain Rule for \(\mathsf{KL}\)-divergence; Theorem 2.13 in [4]).: _Let \(\mathcal{P}(x,y),\mathcal{Q}(x,y)\) be joint distributions. Then,_

\[\mathsf{KL}(\mathcal{P}(x,y)\mathbin{\|}\mathcal{Q}(x,y))=\mathsf{KL}( \mathcal{P}(x)\mathbin{\|}\mathcal{Q}(x))+\mathsf{KL}(\mathcal{P}(y|x) \mathbin{\|}\mathcal{Q}(y|x)).\]

**Lemma A.5** (Conditioning increases \(\mathsf{KL}\)-divergence; Theorem 2.14(e) in [4]).: _For a distribution \(\mathcal{P}_{X}\) and conditional distributions \(\mathcal{P}_{Y|X},\mathcal{Q}_{Y|X}\), let \(\mathcal{P}_{Y}=\mathcal{P}_{Y|X}\circ\mathcal{P}_{X}\) and \(\mathcal{Q}_{Y}=\mathcal{Q}_{Y|X}\circ\mathcal{P}_{X}\), where \(\circ\)' denotes composition (see Section 2.4 in [4]) Then_

\[\mathsf{KL}(\mathcal{P}_{Y}\mathbin{\|}\mathcal{Q}_{Y})\leq\mathsf{KL}\Big{(} \mathcal{P}_{Y|X}\mathbin{\|}\mathcal{Q}_{Y|X}\Bigm{|}\mathcal{P}_{X}\Bigm{)},\]

_with equality if and only if \(\mathsf{KL}\Big{(}\mathcal{P}_{X|Y}\mathbin{\|}\mathcal{Q}_{X|Y}\Bigm{|}P_{Y} \Big{)}=0\)._

### Online Learning Preliminaries

Following is some basic background on the topic of online learning with expert advice. This will be useful in the proof of Theorem 2.2.

Let \(Z=\{z_{1},\ldots,z_{m}\}\) be a set of experts and \(I\) be a set of instances. For any instance \(i\in I\) and expert \(z\in Z\), following the advice of expert \(z\) on instance \(i\) provides utility \(u(z,i)\in\{0,1\}\).

The online learning setting is a perfect-information, zero-sum game between two players, a _learner_ and an _adversary_. In each round \(t=1,\ldots,T\):

1. The learner chooses a distribution \(w_{t}\in\Delta(Z)\) over the set of experts.
2. The adversary chooses an instance \(i_{t}\in I\).
3. The learner gains utility \(u_{t}=\mathbb{E}_{z\sim w_{t}}[u(z,i_{t})]\).

The _total utility_ of a learner strategy \(\mathcal{L}\) for the sequence of instances chosen by the adversary is

\[U(\mathcal{L},T)=\sum_{t=1}^{T}u_{t}.\]

The _regret_ of the learner is the difference between the utility of the best expert and the learner's utility. Namely, for each \(z\in Z\), let

\[U(z,T)=\sum_{t=1}^{T}u(z,i_{t})\]

be the utility the learner would have gained had they chosen \(w_{t}(z)=\mathds{1}\left(z=z_{j}\right)\) for all \(t\in[T]\). Then the regret is

\[\mathsf{Regret}(\mathcal{L},T)=\max_{z\in Z}U(z,T)-U(\mathcal{L},T).\]

[MISSING_PAGE_FAIL:15]

In this simulation, the learner executes an instance of the online learning algorithm of Appendix A.2 with expert set \(S\). Denote this instance \(\mathcal{O}_{S}\).

The adversary's strategy is as follows. Recall that at each round \(t\), \(\mathcal{O}_{S}\) chooses a distribution \(w_{t}\) over \(S\). Note that if \(S\) is realizable then so is \(w_{t}\). At each round \(t\), the adversary selects an instance \(f\in\mathcal{F}\) by executing \(A\) on a training set sampled from \(w_{t}\), as in Algorithm 1.

We prove the following:

1. \(A^{\star}\) interpolates, namely \(\mathbb{P}[\operatorname{\mathsf{L}}_{S}(A^{\star}(S))=0]=1\).
2. \(A^{\star}\) has logarithmic \(\mathsf{KL}\)-stability, as in Eq. (8).
3. \(A^{\star}\) PAC learns \(\mathcal{H}\) as in Eq. (9).

For Item 1, assume for contradiction that \(A^{\star}\) does not interpolate. Seeing as \(A^{\star}\) outputs \(\mathsf{Maj}(f_{1},\ldots,f_{T})\), there exists an index \(i\in[m]\) such that

\[\frac{T}{2}\leq\sum_{t=1}^{T}\mathds{1}(f_{t}(x_{i})\neq y_{i})=U(i,T),\] (10)

where \(U(i,T)\) is the utility of always playing expert \(i\) throughout the game.

Let \(\mathcal{E}_{t}\) denote the event that \(S_{t}\) was resampled (i.e., there were multiple iterations of the do-while loop in round \(t\)). Eq. (7) and Markov's inequality imply

\[\mathbb{P}[\mathcal{E}_{t}]=\mathbb{P}[\mathsf{KL}(A(S_{t})\,\|\,\mathcal{P}) \geq 2b/\gamma]\leq\gamma/2.\] (11)

The utility of \(\mathcal{O}_{S}\) at time \(t\) is

\[u_{t}^{\mathcal{O}_{S}} =\operatorname*{\mathbb{E}}_{\begin{subarray}{c}S_{t}\sim(w_{t} )^{k}\\ f_{t}\sim(A(S_{t})\\ (x,y)\sim w_{t}\end{subarray}}[\mathds{1}(f_{t}(x)\neq y)]\] \[\leq\operatorname*{\mathbb{E}}_{S_{t}\sim(w_{t})^{k}}[ \operatorname{\mathsf{L}}_{w_{t}}(A(S_{t}))\,|\,\neg\mathcal{E}_{t}]+\mathbb{ P}[\mathcal{E}_{t}]\leq\left(\frac{1}{2}-\gamma\right)+\frac{\gamma}{2},\]

where the last inequality follows from Eqs. (6) and (11). Hence, the utility of \(\mathcal{O}_{S}\) throughout the game is

\[U(\mathcal{O}_{S},T)=\sum_{t=1}^{T}u_{t}^{\mathcal{O}_{S}}\leq\left(\frac{1}{2 }-\frac{\gamma}{2}\right)\cdot T.\] (12)

Combining Eqs. (10) and (12) and Theorem A.6 yields

\[\frac{\gamma}{2}\cdot T\leq U(i,T)-U(\mathcal{O}_{S},T)\leq\mathsf{Regret}( \mathcal{O}_{S},T)\leq\sqrt{2T\log(m)},\]

which is a contradiction for our choice of \(T\). This establishes Item 1.

For Item 2, for every \(\ell\in\mathbb{N}\) let \(\mathcal{P}_{\ell}^{\star}\in\Delta\big{(}\{0,1\}^{\mathcal{X}}\big{)}\) be the distribution of \(\mathsf{Maj}(g_{1},\ldots,g_{\ell})\), where \((g_{1},\ldots,g_{\ell})\sim\mathcal{P}^{\ell}\). Let \(\mathcal{P}^{\star}=\frac{1}{z}\sum_{\ell=1}^{\infty}\mathcal{P}_{\ell}^{ \star}/\ell^{2}\) where \(z=\sum_{\ell=1}^{\infty}1/\ell^{2}=\pi^{2}/6\) is a normalization factor.

For any \(S\in\left(\mathcal{X}\times\{0,1\}\right)^{m}\),

\[\mathsf{KL}(A^{\star}(S)\,\|\,\mathcal{P}_{T}^{\star}) =\mathsf{KL}(\mathsf{Maj}(f_{1},\ldots,f_{T})\,\|\,\mathsf{Maj}(g _{1},\ldots,g_{T}))\] \[\leq\mathsf{KL}((f_{1},\ldots,f_{T})\,\|\,\,(g_{1},\ldots,g_{T})) \text{(By Lemma A.2)}\] \[=\sum_{t=1}^{T}\mathsf{KL}((f_{t}|f_{<t})\,\|\,\,(g_{t}|g_{<t})) \text{(By Lemma A.4)}\] \[=\sum_{t=1}^{T}\mathsf{KL}((f_{t}|f_{<t})\,\|\,g_{t}). \text{($g_{i}$'s are independent)}\] \[=\sum_{t=1}^{T}\mathsf{KL}(A(S_{t})\,\|\,\mathcal{P})\leq T\cdot 2b/ \gamma=O(\log(m)),\] (13)

[MISSING_PAGE_EMPTY:17]

Proof of Theorem 2.1.: The proof follows from:

\[\text{Item 1}\xrightleftharpoons[]{\text{Theorem B.1}}\text{Item 2}\xrightleftharpoons[]{\text{Lemma B.2}}\text{Item 4}\xrightleftharpoons[]{\text{ \eqref{Lemma B.4}}}\text{Item 3}\xrightleftharpoons[]{\text{Lemma B.4}}\text{Item 2},\]

where \((*)\) is immediate. 

### Characterization of Pure DP Learnability via the Fractional Clique Dimension

For every hypothesis class \(\mathcal{H}\), they define a quantity \(\omega_{m}^{\star}=\omega_{m}^{\star}(\mathcal{H})\), called the _fractional clique number_ of \(\mathcal{H}\). The definition of \(\omega_{m}^{\star}\) involves an LP relaxation of clique numbers on a certain graph corresponding to \(\mathcal{H}\), but for our purposes it will be more convenient to use the following alternative characterization (Eq. 6 and Theorem 2.8 in [1]):

\[\forall m\in\mathbb{N}:\;\frac{1}{\omega_{m}^{\star}}=\sup_{\mathcal{P}}\inf _{\begin{subarray}{c}S\\ h\sim\mathcal{P}\end{subarray}}\mathbb{P}_{\begin{subarray}{c}S\\ h\sim\mathcal{P}\end{subarray}}[\mathrm{L}_{S}(h)=0],\] (14)

where the supremum is taken over distributions over \(\mathcal{H}\), and the infimum is taken over distributions over samples of size \(m\) that are realizable by \(\mathcal{H}\). In words, \(1/\omega_{m}^{\star}\) is the value of a game in which player 1 selects a distribution of hypotheses over \(\mathcal{H}\), player 2 selects a distribution over realizable samples of size \(m\), and player 1 wins if and only if the hypothesis correctly labels all the points in the sample.

The fractional clique number characterizes pure DP learnability, as follows:

**Theorem B.1** (Restatement of Theorems 2.3 and 2.6 in [1]).: _For any hypothesis class \(\mathcal{H}\), exactly one of the following statements holds:_

1. \(\mathcal{H}\) _is pure DP learnable (as in Definition_ 3.5_), and there exists a polynomial_ \(p\) _such that_ \(\omega_{m}^{\star}(\mathcal{H})\leq p(m)\) _for all_ \(m\in\mathbb{N}\)_._
2. \(\mathcal{H}\) _is not pure DP learnable, and_ \(\omega_{m}^{\star}(\mathcal{H})=2^{m}\) _for all_ \(m\in\mathbb{N}\)_._

The _fractional clique dimension_ of \(\mathcal{H}\) is defined by \(\mathsf{CD}^{\star}(\mathcal{H})=\sup\left\{m\in\mathbb{N}:\;\omega_{m}^{ \star}(\mathcal{H})=2^{m}\right\}\). So in other words, Theorem B.1 states that \(\mathcal{H}\) is pure DP learnable if and only if \(\mathsf{CD}^{\star}(\mathcal{H})\) is finite.

### Finite Fractional Clique Dimension \(\implies\) DI Renyi-Stability

**Lemma B.2**.: _In the context of Theorem 2.1: Item 2 \(\implies\) Item 4._

Proof of Lemma b.2.: Given that \(\mathcal{H}\) is DP learnable, we define a learning rule \(A\) and a prior \(\mathcal{P}\), and show that \(A\) PAC learns \(\mathcal{H}\) subject to distribution-independent \(\mathsf{KL}\)-stability with respect to \(\mathcal{P}\).

By Theorem B.1 there exists a polynomial \(p\) such that \(\omega_{m}^{\star}(\mathcal{H})\leq p(m)\) for all \(m\in\mathbb{N}\). By Eq. (14), for every \(m\in\mathbb{N}\), there exists a prior \(\mathcal{P}_{m}\in\Delta\big{(}\{0,1\}^{\mathcal{X}}\big{)}\) such that for any \(\mathcal{H}\)-realizable sample \(S\in\left(\mathcal{X}\times\{0,1\}\right)^{m}\),

\[\underset{h\sim\mathcal{P}_{m}}{\mathbb{P}}[\mathrm{L}_{S}(h)=0]\geq\frac{1}{ \omega_{m}^{\star}}\geq\frac{1}{p(m)}.\]

Let

\[\mathcal{P}=\frac{1}{z}\sum_{m=1}^{\infty}\frac{\mathcal{P}_{m}}{m^{2}}\]

be a mixture, where \(z=\sum_{m=1}^{\infty}1/m^{2}=\pi^{2}/6\) is a normalization factor. \(\mathcal{P}\) is a valid distribution over \(\{0,1\}^{\mathcal{X}}\).

For every \(m\in\mathbb{N}\) and for any \(\mathcal{H}\)-realizable sample \(S\in\left(\mathcal{X}\times\{0,1\}\right)^{m}\),

\[\underset{h\sim\mathcal{P}}{\mathbb{P}}[\mathrm{L}_{S}(h)=0]\geq\frac{1}{zm^{ 2}}\cdot\underset{h\sim\mathcal{P}_{m}}{\mathbb{P}}[\mathrm{L}_{S}(h)=0]\geq \frac{1}{zm^{2}p(m)}=\frac{1}{q(m)},\] (15)

where \(q(m)=zm^{2}p(m)\).

For any sample \(S\), let \(C_{S}=\big{\{}h\in\{0,1\}^{\mathcal{X}}:\;\;\mathrm{L}_{S}(h)=0\big{\}}\) be the set of hypotheses consistent with \(S\). Let \(A\) be a randomized learning rule given by \(S\mapsto\mathcal{Q}_{S}\in\Delta\big{(}\{0,1\}^{\mathcal{X}}\big{)}\) such that \(\mathcal{Q}_{S}(h)=\mathcal{P}(h\,|\,C_{S})\) if \(h\in C_{S}\), and \(\mathcal{Q}_{S}(h)=0\) otherwise. \(A\) can be written explicitly as a rejection sampling algorithm:

[MISSING_PAGE_EMPTY:19]

holds for all \(m\in\mathbb{N}\) large enough. Fix such an \(m\). We show that taking \(\mathcal{P}=\mathcal{P}^{\star}\) satisfies Eq. (17) for this \(m\).

Let \(\mathcal{Q}\) denote the distribution of \(A^{\star}(S^{\prime})\) where \(S^{\prime}\sim\left(\mathrm{U}(S)\right)^{m^{\prime}}=P_{S^{\prime}}\), \(\mathrm{U}(S)\) is the uniform distribution over \(S\), and \(m^{\prime}=m\ln(4m)\). The proof follows by noting that if \(\mathsf{KL}(\mathcal{Q}\,\|\,\mathcal{P}^{\star})\) is small then one can lower bounding the probability of an event according to \(\mathcal{P}^{\star}\) by its probability according to \(\mathcal{Q}\).

To see that the \(\mathsf{KL}\) is indeed small, let \(P_{A^{\star}(S^{\prime}),S^{\prime}}\) and \(P_{H^{\star},S^{\prime}}\) be two joint distributions. The variable \(S^{\prime}\) has marginal \(P_{S^{\prime}}\) in both distributions, \(A^{\star}(S^{\prime})\sim\mathcal{Q}\) depends on \(S^{\prime}\), but \(H^{\star}\sim\mathcal{P}^{\star}\) is independent of \(S^{\prime}\). Then,

\[\mathsf{KL}(\mathcal{Q}\,\|\,\mathcal{P}^{\star}) =\mathsf{KL}\big{(}P_{A^{\star}(S^{\prime})}\,\|\,P_{H^{\star}} \big{)}\] \[\leq\mathsf{KL}\Big{(}P_{A^{\star}(S^{\prime})|S^{\prime}}\,\|\,P_ {H^{\star}|S^{\prime}}\,\Big{|}\,P_{S^{\prime}}\Big{)}\] (Lemma A.5 ) \[=\mathsf{KL}\Big{(}P_{A^{\star}(S^{\prime})|S^{\prime}}\,\|\,P_{H^ {\star}}\,\Big{|}\,P_{S^{\prime}}\Big{)}\] ( \[H^{\star}\bot S^{\prime}\] ) \[=\mathbb{E}_{S^{\prime}}[\mathsf{KL}(A^{\star}(S^{\prime})\,\|\, \mathcal{P}^{\star})]\] (Definition of conditional \[\mathsf{KL}\] ) \[\leq C\log(m).\] (By Eq. ( 18 ) and choice of \[m\] ) (19)

Taking \(k=2C\log(m)\),

\[\underset{h\sim\mathcal{Q}}{\mathbb{P}}\bigg{[}\log\left(\frac{\mathcal{Q}(h )}{\mathcal{P}^{\star}(h)}\right)\geq k\bigg{]}\leq\frac{\mathsf{KL}(\mathcal{ Q}\,\|\,\mathcal{P}^{\star})}{k}\leq\frac{1}{2}\] (20)

holds by Markov's inequality and the definition of the \(\mathsf{KL}\) divergence. We are interested in the probability of the event \(\mathcal{E}=\big{\{}h\in\{0,1\}^{\mathcal{X}}:\ \mathrm{L}_{S}(h)=0\big{\}}\). Because \(A^{\star}\) is interpolating,

\[\mathcal{Q}(\mathcal{E})\geq\underset{h\sim\mathcal{P}^{\star}(\mathrm{U}(S)) ^{m^{\prime}}}{\mathbb{P}}[S\subseteq S^{\prime}]\geq 1-m\left(1-\frac{1}{m} \right)^{m^{\prime}}\geq\frac{3}{4}.\] (21)

Finally, we lower bound \(\mathcal{P}^{\star}(\mathcal{E})\) as follows.

\[\mathcal{P}^{\star}(\mathcal{E}) \geq\underset{h\sim\mathcal{P}^{\star}}{\mathbb{P}}\bigg{[} \mathcal{E}\ \wedge\ \log\left(\frac{\mathcal{Q}(h)}{\mathcal{P}^{\star}(h)}\right)\leq k\bigg{]}\] \[=\underset{h\sim\mathcal{P}^{\star}}{\mathbb{P}}\big{[} \mathcal{E}\ \wedge\ \mathcal{P}^{\star}(h)\geq 2^{-k}\cdot\mathcal{Q}(h)\big{]}\] \[\geq\underset{h\sim\mathcal{Q}}{\mathbb{P}}\big{[}\mathcal{E} \ \wedge\ \mathcal{P}^{\star}(h)\geq 2^{-k}\cdot\mathcal{Q}(h)\big{]}\cdot 2^{-k}\] \[=\underset{h\sim\mathcal{Q}}{\mathbb{P}}\bigg{[}\mathcal{E}\ \wedge\ \log\left(\frac{\mathcal{Q}(h)}{\mathcal{P}^{\star}(h)}\right)\leq k\bigg{]} \cdot 2^{-k}.\] \[\geq\left(\mathcal{Q}(\mathcal{E})-\underset{h\sim\mathcal{Q}}{ \mathbb{P}}\bigg{[}\log\left(\frac{\mathcal{Q}(h)}{\mathcal{P}^{\star}(h)} \right)\leq k\bigg{]}\right)\cdot 2^{-k}.\] (De Morgan's + union bound) \[\geq\frac{1}{4}\cdot 2^{-k}=\frac{1}{4m^{2C}}=\frac{1}{\mathsf{ poly}(m)}.\] (By Eqs. ( 20 ) and ( 21 ) and choice of \[k\] )

This establishes Eq. (17), as desired. 

## Appendix C Proof of Theorem 1.4 (DD Equivalences)

### Preliminaries

#### c.1.1 Littlestone Dimension

The Littlestone dimension is a combinatorial parameter which captures mistake and regret bounds in online learning [11, 12].

**Definition C.1** (Mistake Tree).: _A mistake tree is a binary decision tree whose nodes are labeled with instances from \(\mathcal{X}\) and edges are labeled by \(0\) or \(1\) such that each internal node has one outgoing edge labeled \(0\) and one outgoing edge labeled \(1\). A root-to-leaf path in a mistake tree can be described as a sequence of labeled examples \((x_{1},y_{1}),\ldots,(x_{d},y_{d})\). The point \(x_{i}\) is the label of the \(i\)-th internal node in the path, and \(y_{i}\) is the label of its outgoing edge to the next node in the path._

**Definition C.2** (Shattering).: _Let \(\mathcal{H}\) be a hypothesis class and let \(T\) be a mistake tree. \(\mathcal{H}\) shatters \(T\) if every root-to-leaf path in \(T\) is realizable by \(\mathcal{H}\)._

**Definition C.3** (Littlestone Dimension).: _Let \(\mathcal{H}\) be a hypothesis class. The Littlestone dimension of \(\mathcal{H}\), denoted \(\mathsf{LD}(\mathcal{H})\), is the largest number \(d\) such that there exists a complete mistake tree of depth \(d\) shattered by \(\mathcal{H}\). If \(\mathcal{H}\) shatters arbitrarily deep mistake trees then \(\mathsf{LD}(\mathcal{H})=\infty\)._

#### c.1.2 Clique Dimension

**Definition C.4** (Clique; [1]).: _Let \(\mathcal{H}\) be a hypothesis class and let \(m\in\mathbb{N}\). A clique in \(\mathcal{H}\) of order \(m\) is a family \(\mathcal{S}\) of realizable samples of size \(m\) such that (i) \(|\mathcal{S}|=2^{m}\); (ii) every two distinct samples \(S^{\prime},S^{\prime\prime}\in\mathcal{S}\) contradicts, i.e., there exists a common example \(x\in\mathcal{X}\) such that \((x,0)\in S^{\prime}\) and \((x,1)\in S^{\prime\prime}\)._

**Definition C.5** (Clique Dimension; [1]).: _Let \(\mathcal{H}\) be a hypothesis. The clique dimension of \(\mathcal{H}\), denoted \(\mathsf{CD}(\mathcal{H})\), is the largest number \(m\) such that \(\mathcal{H}\) contains a clique of order \(m\). If \(\mathcal{H}\) contains cliques of arbitrary large order then we write \(\mathsf{CD}(\mathcal{H})=\infty\)._

#### c.2 Global Stability \(\implies\) Replicability

**Lemma C.6**.: _Let \(\mathcal{H}\) be a hypothesis class and let \(A\) be a \((m,\eta)\)-globally stable learner for \(\mathcal{H}\). Then, \(A\) is an \(\eta\)-replicable learner for \(\mathcal{H}\)._

This follows immediately by noting that global stability is equivalent to 2-parameters replicability, which is qualitatively equivalent to 1-parameter replicability [10].

**Lemma C.7** ([10]).: _For every \(\rho,\eta,\nu\in[0,1]\),_

1. _Every_ \(\rho\)_-replicable algorithm is also_ \(\left(\frac{\rho-\nu}{1-\nu},\nu\right)\)_-replicable._
2. _Every_ \((\eta,\nu)\)_-replicable algorithm is also_ \((\eta+2\nu-2)\)_-replicable._

Proof of Lemma c.6.: By the assumption, there exists an hypothesis \(h\) such that for every population \(\mathcal{D}\), we have \(\mathbb{P}_{R\sim\mathcal{R}}[\mathbb{P}_{S\sim\mathcal{D}^{m}}[A(S;r)=h]\geq \eta]=1\). Hence \(A\) is \((\eta,1)\)-replicable, and by Lemma C.7 it is also \(\eta\)-replicable. 

#### c.3 DD \(\mathsf{KL}\)-Stability \(\implies\) Finite Littlestone Dimension

**Lemma C.8**.: _Let \(\mathcal{H}\) be a hypothesis class that is distribution-dependent \(\mathsf{KL}\)-stable. Then \(\mathcal{H}\) has finite Littlestone dimension._

This lemma is an immediate result of the relation between thresholds and the Littlestone dimension, and the fact that the class of thresholds on the natural numbers does not admit any learning rule that satisfies a non-vacuous PAC-Bayes bound [14]. The next lemma is a corollary of Theorem 2 in [14].

**Theorem C.9** (Corollary of Theorem 2 [14]).: _Let \(m\in\mathbb{N}\) and let \(N\in\mathbb{N}\). Then, there exists \(n\in\mathbb{N}\) large enough such that the following holds. For every learning rule \(A\) of the class of thresholds over \([n]\), \(\mathcal{H}_{n}=\{\mathbbm{1}_{\{x>k\}}:[n]\to\{0,1\}\mid k\in[n]\}\), there exists a realizable population distribution \(\mathcal{D}=\mathcal{D}_{A}\) such that for any prior distribution \(\mathcal{P}\),_

\[\underset{S\sim\mathcal{D}^{m}}{\mathbb{P}}\bigg{[}\mathsf{KL}(A(S)\parallel \mathcal{P})>N\quad\text{or,}\quad\mathsf{L}_{\mathcal{D}}(A(S))>\frac{1}{4} \bigg{]}\geq\frac{1}{16}\]

**Theorem C.10** (Littlestone dimension and thresholds [11]).: _Let \(\mathcal{H}\) be a hypothesis class. Then,_

1. _If_ \(\mathsf{LD}(\mathcal{H})\geq d\) _then_ \(\mathcal{H}\) _contains_ \(\lfloor\log d\rfloor\) _thresholds._
2. _If_ \(\mathcal{H}\) _contains_ \(d\) _thresholds then_ \(\mathsf{LD}(\mathcal{H})\geq\lfloor\log d\rfloor\)_._

Proof of Lemma c.8.: If by contradiction the Littlestone dimension of \(\mathcal{H}\) is unbounded, then by Theorem C.10, \(\mathcal{H}\) contains a copy of \(\mathcal{H}_{n}\), the class of thresholds over \([n]\), for arbitrary large \(n\)'s. Hence, by Theorem C.9\(\mathcal{H}\) does not admit a PAC learner that is \(\mathsf{KL}\)-stable.

### \(\mathsf{Ml}\)-Stability \(\implies\)\(\mathsf{DD}\)\(\mathsf{KL}\)-Stability

**Lemma C.11**.: _Let \(\mathcal{H}\) be a hypothesis class and let \(A\) be a mutual information stable learner with information bound \(f(m)=o(1)\). (I.e. for every population distribution \(\mathcal{D}\), \(I(A(S);S)\leq f(m)\) where \(S\sim\mathcal{D}^{m}\).) Then, \(A\) is a distribution-dependent \(\mathsf{KL}\)-stable learner with \(\mathsf{KL}\) bound \(g(m)=\sqrt{f(m)\cdot m}\) and confidence \(\beta(m)=\sqrt{f(m)/m}\)._

The following statement is an immediate corollary.

**Corollary C.12**.: _Let \(\mathcal{H}\) be a hypothesis class that is mutual information stable. Then \(\mathcal{H}\) is distribution-dependent \(\mathsf{KL}\)-stable._

Proof of Lemma c.11.: Let \(\mathcal{D}\) be a population distribution. Define a prior distribution \(\mathcal{P}_{\mathcal{D}}=\mathbb{E}_{S}[A(S)]\), i.e. \(\mathcal{P}_{\mathcal{D}}(h)=\mathbb{P}_{S\sim\mathcal{D}^{m}}[A(S)=h]\). We will show that \(A\) is \(\mathsf{KL}\) stable with respect to the prior \(\mathcal{P}_{\mathcal{D}}\). We use the identity \(I(X;Y)=\mathsf{KL}(P_{X,Y},P_{X}P_{Y})\). Let \(P_{A(S),S}\) be the joint distribution of the training sample \(S\) and the hypothesis selected by \(A\) when given \(S\) as an input, and let \(P_{A(S)}P_{S}\) be the product of the marginals. Note that \(P_{A(S)}P_{S}\) is equal in distribution to \(P_{A(S^{\prime})}P_{S}\), where \(S^{\prime}\) is an independent copy of \(S\). Hence,

\[I(A(S);S) =\mathsf{KL}(P_{A(S),S},P_{A(S)}P_{S})\] \[=\mathsf{KL}(P_{A(S)|S}P_{S},P_{A(S^{\prime})}P_{S}),\] \[=\mathsf{KL}(P_{S},P_{S})+\mathbb{E}_{s\sim P_{S}}\big{[}\mathsf{ KL}(P_{A(S)|S=s},P_{A(S^{\prime})|S=s})\big{]}\] (Chain rule) \[=\mathbb{E}_{s\sim P_{S}}\big{[}\mathsf{KL}(P_{A(S)|S=s},P_{A(S^ {\prime})|S=s})\big{]}\] \[=\mathbb{E}_{s\sim P_{S}}\big{[}\mathsf{KL}(P_{A(S)|S=s},P_{A(S^ {\prime})})\big{]}.\]

Note that \(P_{A(S^{\prime})}\) and the prior \(\mathcal{P}_{\mathcal{D}}\) are identically distributed, and \(P_{A(S)|S=s}\) is exactly the posterior produced by \(A\) given the input sample \(s\). By Markov's inequality,

\[\mathop{\mathbb{P}}_{S\sim D^{m}}\!\Big{[}\mathsf{KL}(A(S)\parallel P _{\mathcal{D}})\geq\sqrt{m\cdot I(A(S);S)}\Big{]}\leq \frac{I(A(S);S)}{\sqrt{mI(A(S);S)}}\] \[= \sqrt{\frac{I(A(S);S)}{m}}.\] (22)

Since \(I(A(S);S)\leq f(m)\), by Eq. (22)

\[\mathop{\mathbb{P}}_{S\sim D^{m}}\!\Big{[}\mathsf{KL}(A(S)\parallel P_{ \mathcal{D}})\geq\sqrt{f(m)\cdot m}\Big{]}\leq\sqrt{\frac{f(m)}{m}}.\]

Note that since \(f(m)=o(m)\), indeed \(\sqrt{f(m)/m}\xrightarrow{m\to\infty}0\) and \(\sqrt{f(m)\cdot m}=o(m)\). 

### Finite Littlestone Dimension \(\implies\)\(\mathsf{Ml}\)-Stability

**Lemma C.13**.: _Let \(\mathcal{H}\) be a hypothesis class with finite Littlestone dimension. Then \(\mathcal{H}\) admits an information stable learner._

This lemma is a direct result of Theorem \(2\) in [22].

**Definition C.14**.: _The information complexity of a hypothesis class \(\mathcal{H}\) is_

\[\mathsf{IC}(\mathcal{H})=\sup_{|S|}\inf_{A}\sup_{\mathcal{D}}I(A(S);S)\]

_where the supremum is over all sample sizes \(|S|\in\mathbb{N}\) and the infimum is over all learning rules that PAC learn \(\mathcal{H}\)._

**Theorem C.15** (Theorem \(2\)[22]).: _Let \(\mathcal{H}\) be a hypothesis class of with Littlestone dimension \(d\). Then the information complexity of \(\mathcal{H}\) is bounded by_

\[\mathsf{IC}(\mathcal{H})\leq 2^{d}+\log(d+1)+3+\frac{3}{e\ln 2}.\]

Proof of Lemma c.13.: Since finite information complexity implies that \(\mathcal{H}\) admits an information stable learner, the proof follows from Theorem C.15