[MISSING_PAGE_FAIL:1]

\(\bm{x}_{i}:=(x_{i,1},\ldots,x_{i,m})\) for \(i\in[n]\) to denote the \(i\)th user's examples. Furthermore, we write \(\bm{x}_{-i}\) to denote the input with the \(i\)th user's data \(\bm{x}_{i}\) removed. Similarly, for \(S\subseteq[n]\), we write \(\bm{x}_{-S}\) to denote the input with all the examples of users in \(S\) removed.

**Definition 1** (User-Level Neighbors).: Two inputs \(\bm{x},\bm{x}^{\prime}\) are _user-level neighbors_, denoted by \(\bm{x}\asymp\bm{x}^{\prime}\), iff \(\bm{x}_{-i}=\bm{x}^{\prime}_{-i}\) for some \(i\in[n]\).

**Definition 2** (User-Level and Item-Level DP).: For \(\varepsilon,\delta>0\), we say that a randomized algorithm1\(\mathbb{A}:\mathcal{Z}\to\mathcal{O}\) is _\((\varepsilon,\delta)\)-user-level DP_ iff, for any \(\bm{x}\asymp\bm{x}^{\prime}\) and any \(S\subseteq\mathcal{O}\), we have \(\Pr[\mathbb{A}(\bm{x})\in S]\leq e^{\varepsilon}\Pr[\mathbb{A}(\bm{x}^{ \prime})\in S]+\delta\). When \(m=1\), we say that \(\mathbb{A}\) is _\((\varepsilon,\delta)\)-item-level DP_.

Footnote 1: For simplicity, we will only consider the case where the output space \(\mathcal{O}\) is finite throughout this work to avoid measure-theoretic issues. This assumption is often without loss of generality since even the output in continuous problems can be discretized in such a way that the “additional error” is negligible.

The case \(\delta=0\) is referred to as _pure-DP_, whereas the case \(\delta>0\) is referred to as _approximate-DP_.

To formulate statistical tasks studied in our work, we consider the setting where there is an unknown distribution \(\mathcal{D}\) over \(\mathcal{Z}\) and the input \(\bm{x}\sim\mathcal{D}^{nm}\) consists of \(nm\) i.i.d. samples drawn from \(\mathcal{D}\). A task \(\overline{\mathfrak{L}}\) (including the desired accuracy) is defined by \(\Psi_{\overline{\mathfrak{L}}}(\mathcal{D})\), which is the set of "correct" answers. For a parameter \(\gamma\), we say that the algorithm \(\mathbb{A}\) is _\(\gamma\)-useful_ if \(\Pr_{\bm{x}\sim\mathcal{D}^{nm}}[\mathbb{A}(\bm{x})\in\Psi_{\overline{ \mathfrak{L}}(\mathcal{D})}]\geq\gamma\) for all valid \(\mathcal{D}\). Furthermore, we say that \(nm\) is the _sample complexity_ of the algorithm, whereas \(n\) is its _user complexity_. For \(m\in\mathbb{N}\) and \(\varepsilon,\delta>0\), let \(n_{m}^{\mathbb{T}}(\varepsilon,\delta;\gamma)\) denote the smallest user complexity of any \((\varepsilon,\delta)\)-user-level DP algorithm that is \(\gamma\)-useful for \(\overline{\mathfrak{L}}\). When \(\gamma\) is not stated, it is assumed to be \(2/3\).

**Generic Algorithms Based on Stability.** So far, there have been two main themes of research on user-level DP learning. The first aims to provide generic algorithms that work with many tasks. Ghazi et al. [1] observed that any _pseudo-globally stable_ (aka _reproducible_[1]) algorithm can be turned into a user-level DP algorithm. Roughly speaking, pseudo-global stability requires that the algorithm, given a random input dataset and a random string, returns a canonical output--which may depend on the random string--with a large probability (e.g., \(0.5\)). They then show that the current best generic PAC learning item-level DP algorithms from [1] (which are based on yet another notion of stability) can be made into pseudo-globally stable algorithms. This transforms the aforementioned item-level DP algorithms into user-level DP algorithms.

In a recent breakthrough, Bun et al. [1] significantly expanded this transformation by showing that _any_ item-level DP algorithm can be compiled into a pseudo-globally stable algorithm--albeit with some overhead in the sample complexity. Combining with [1], they then get a generic transformation of any item-level DP algorithm to a user-level DP algorithm. Such results are of (roughly) the following form: If there is an item-level DP algorithm for some task with sample complexity \(n\), then there is a user-level DP algorithm with user complexity \(O(\log(1/\delta)/\varepsilon)\) as long as \(m\geq\widetilde{\Omega}_{\varepsilon,\delta}(n^{2})\). In other words, if each user has sufficiently many samples, the algorithm needs very few users to learn under the user-level DP constraint. However, the requirement on the number of examples per user can be prohibitive: due to the nature of the reduction, it requires each user to have enough samples to learn by themselves. This leads to the following natural question:2

Footnote 2: Of course, there is a generic transformation where each user throws away all but one example; here, we are looking for one that can reduce the user complexity compared to the item-level setting.

Is there a generic transformation from item-level to user-level DP algorithms for small \(m\)?

Algorithms for Specific Tasks & the \(\sqrt{m}\) Savings.Meanwhile, borrowing a page from its item-level DP counterpart, another active research theme has been to study specific tasks and provide (tight) upper and lower bounds on their sample complexity for user-level DP. Problems such as mean estimation, stochastic convex optimization (SCO), and discrete distribution learning are well-understood under user-level DP (for approximate-DP); see, e.g., [13, 14, 15, 16, 17]. A pattern has emerged through this line of studies: (the privacy-dependent part of) the user complexity in the user-level DP setting is often roughly \(1/\sqrt{m}\) smaller than that of the item-level DP setting. For example, for the task of discrete distribution learning on domain \([k]\) up to total variation distance of \(\alpha>0\) (denoted by \(\mathrm{DD}k(\alpha)\)), the user complexity for the user-level DP is [1, 15, 16]

\[n_{m}^{\mathrm{DD}k(\alpha)}(\varepsilon,\delta)=\tilde{\Theta}_{\delta}\left( \frac{k}{\varepsilon\alpha\sqrt{m}}+\frac{k}{\alpha^{2}m}\right).\]We can see that the first privacy-dependent term decreases by a factor of \(\sqrt{m}\) whereas the latter privacy-independent term decreases by a factor of \(m\). A similar phenomenon also occurs in the other problems discussed above. As such, it is intriguing to understand this question further:

Is there a common explanation for these \(\sqrt{m}\) saving factors for the privacy-dependent term?

### Our Contributions

Approximate-DP.We answer both questions by showing a generic way to transform any item-level DP algorithm into a user-level DP algorithm such that the latter saves roughly \(\sqrt{m}\) in terms of the number of users required. Since the formal expression is quite involved, we state below a simplified version, which assumes that the sample complexity can be written as the sum of two terms, the privacy-independent term (i.e., \(n^{\mathbb{T}}_{\text{stat}}\)) and the privacy-dependent term that grows linearly in \(1/\varepsilon\) and polylogarithmically in \(1/\delta\) (i.e., \(\left(\frac{\log(1/\delta)^{O(1)}}{\varepsilon}\right)\cdot n^{\mathfrak{T}}_{ \text{priv}}\)). This is the most common form of sample complexity of DP learning discovered so far in the literature--in particular, this covers all the tasks we have discussed previously. The corollary below shows that the privacy-dependent term decreases by a factor of roughly \(\sqrt{m}\) whereas the privacy-independent term decreases by a factor of \(m\). The full version of the statement can be found in Theorem10.

**Corollary 3** (Main Theorem-Simplified).: _Let \(\mathfrak{T}\) be any task and \(\gamma>0\) be a parameter. Suppose that for any sufficiently small \(\varepsilon,\delta>0\), there is an \((\varepsilon,\delta)\)-item-level DP algorithm that is \(\gamma\)-useful for \(\mathfrak{T}\) with sample complexity \(\widetilde{O}\left(\frac{\log(1/\delta)^{c}}{\varepsilon}\cdot n^{\mathfrak{ T}}_{\text{priv}}+n^{\mathfrak{T}}_{\text{stat}}\right)\) for some constant \(c\). Then, for any sufficiently small \(\varepsilon,\delta>0\), there exists an \((\varepsilon,\delta)\)-user-level DP algorithm that is \((\gamma-o(1))\)-useful for \(\mathfrak{T}\) and has user complexity_

\[\widetilde{O}\left(\tfrac{1}{\sqrt{m}}\cdot\tfrac{\log(1/\delta)^{c+3/2}}{ \varepsilon^{2}}\cdot n^{\mathfrak{T}}_{\text{priv}}+\tfrac{1}{m}\cdot n^{ \mathfrak{T}}_{\text{stat}}+\tfrac{\log(1/\delta)}{\varepsilon}\right).\]

Our result provides a powerful and generic tool in learning with user-level DP, as it can translate any item-level DP bound to user-level DP bound. Specifically, up to polylogarithmic terms and the dependency on \(\varepsilon\), the corollary above recovers all the aforementioned user complexity bounds for mean estimation, SCO, and discrete distribution learning. In addition, it also presents new sample complexity bounds for the moderate \(m\) regime for PAC learning.

While our result is very general, it--perhaps inevitably--comes at a cost: the algorithm is computationally inefficient and the dependence on \(\varepsilon\) (and \(\delta\)) is not tight. We discuss this--along with other open questions--in more detail in Section5.

Pure-DP.Surprisingly, the pure-DP setting is less explored compared to approximate-DP in the context of learning with user-level DP; the sample complexity of several of the problems mentioned above (e.g., discrete distribution learning and SCO) is well-understood under approximate-DP but not so under pure-DP. While we do not provide a generic transformation from item-level DP algorithms to user-level DP algorithms for pure-DP, we give simple modifications of the popular pure-DP _exponential mechanism_[13] that allow it to be used in the user-level DP setting. Consequently, we obtain improved bounds for several problems. Due to space constraints, in the main body of the paper, we will only discuss PAC learning. Results for other problems, such as hypothesis testing and distribution learning can be found in AppendixE.

In PAC learning, \(\mathcal{Z}=\mathcal{X}\times\{0,1\}\) and there is a concept class \(\mathcal{C}\subseteq\{0,1\}^{\mathcal{X}}\). An error of \(c:\mathcal{Z}\to\{0,1\}\) w.r.t. \(\mathcal{D}\) is defined as \(\operatorname{err}_{\mathcal{D}}(c):=\Pr_{(x,y)\sim\mathcal{D}}[c(x)\neq y]\). The task \(\operatorname{PAC}(\mathcal{C};\alpha)\) is to, for any distribution \(\mathcal{D}\) that has zero error on some concept \(c^{\varepsilon}\in\mathcal{C}\) (aka _realizable by \(\mathcal{C}\)_), output \(c:\mathcal{Z}\to\{0,1\}\) such that \(\operatorname{err}_{\mathcal{D}}(c)\leq\alpha\). We give a tight characterization of the user complexity for PAC learning3:

Footnote 3: \(\operatorname{PRDim}\) is defined in Definition20.

**Theorem 4**.: _Let \(\mathcal{C}\) be any concept class with probabilistic representation dimension \(d\) (i.e. \(\operatorname{PRDim}(\mathcal{C})=d\)). Then, for any sufficiently small \(\alpha,\varepsilon>0\) and for all \(m\in\mathbb{N}\), we have_

\[n^{\operatorname{PAC}(\mathcal{C};\alpha)}_{m}(\varepsilon,\delta=0)= \tilde{\Theta}\left(\tfrac{d}{\varepsilon}+\tfrac{d}{\varepsilon\alpha m} \right).\]

Previous work [11, 1] achieves the tight sample complexity only for large \(m\) (\(\gg d^{2}\)).

#### 1.1.1 Technical Overview

In this section, we give a rough overview of our proofs. We will be intentionally vague; all definitions and results will be formalized later in the paper.

**Approximate-DP.** At a high-level, our proof proceeds roughly as follows. First, we show that any \((\varepsilon,\delta)\)_-item-level_ DP \(\mathbb{A}\) with high probability satisfies a local version of user-level DP for which we only compare4\(\mathbb{A}(\bm{x}_{-i})\) for different \(i\), but with \(\varepsilon\) that is increased by a factor of roughly \(\sqrt{m}\) (see Definition 16 and Theorem 17). If this were to hold not only for \(\bm{x}_{-i}\) but also for all user-level neighbors \(\bm{x}^{\prime}\) of \(\bm{x}\), then we would have been done since the \((\varepsilon,\delta)\)-item-level DP algorithm would have also yielded \((\varepsilon\sqrt{m},\delta)\)_-user-level_ DP (which would then imply a result in favor of Corollary 3). However, this is not true in general and is where the second ingredient of our algorithm comes in: We use a propose-test-release style algorithm [10] to check whether the input is close to a "bad" input for which the aforementioned local condition does not hold. If so, then we output \(\bot\); otherwise, we run the item-level DP algorithm. Note that the test passes w.h.p. and thus we get the desired utility.

Footnote 4: In Theorem 17, we need to compare \(\mathbb{A}(\bm{x}_{-S})\) for \(S\)’s of small sizes as well.

This second step is similar to recent work of Kohli and Laskowski [11] and Ghazi et al. [1], who used a similar algorithm for additive noise mechanisms (namely Laplace and Gaussian mechanisms). The main difference is that instead of testing for local _sensitivity_ of the function as in [11, 1], we directly test for the privacy loss of the algorithm \(\mathbb{A}\).

As for the first step, we prove this via a connection to a notion of generalization called _sample perfect generalization_[11] (see Definition 12). Roughly speaking, this notion measures the privacy loss when we run an \((\varepsilon,\delta)\)-item-level DP algorithm on two \(\bm{x},\bm{x}^{\prime}\) drawn independently from \(\mathcal{D}^{n}\). We show that w.h.p. \(\mathbb{A}(\bm{x})\approx_{\varepsilon^{\prime},\delta^{\prime}}\mathbb{A}( \bm{x}^{\prime})\) for \(\varepsilon^{\prime}=O(\sqrt{n\log(1/\delta)}\cdot\varepsilon)\). Although ostensibly unrelated to the local version of user-level DP that we discussed above, it turns out that these are indeed related: if we view the algorithm that fixes examples of all but one user, then applying the sample perfect generalization bound on just the input of the last user (which consists of only \(m\) samples) shows that the privacy loss when changing the last user is \(O(\sqrt{m\log(1/\delta)}\cdot\varepsilon)\) w.h.p. It turns out that we can turn this argument into a formal bound for our local notion of user-level DP (see Section 3.2.1.)

We remark that our results on sample perfect generalization also resolve an open question of Cummings et al. [11]; we defer this discussion to Appendix C.1.

Finally, we also note that, while Bun et al. [1] also uses the sample perfect generalization notion, their conversion from item-level DP algorithms to user-level DP algorithms require constructing pseudo-globally stable algorithms with sample complexity \(m\). This is impossible when \(m\) is small, which is the reason why their reduction works only in the example-rich setting.

**Pure-DP.** Recall that the exponential mechanism [14] works as follows. Suppose there is a set \(\mathcal{H}\) of candidates we would like to select from, together with the scoring functions \((\operatorname{scr}_{H})_{H\in\mathcal{H}}\), where \(\operatorname{scr}_{H}:\mathcal{Z}^{n}\to\mathbb{R}\) has sensitivity at most \(\Delta\). The algorithm then outputs \(H\) with probability proportional to \(\exp\left(-\varepsilon\cdot\operatorname{scr}_{H}(\bm{x})/_{2\Delta}\right)\). The error guarantee of the algorithm scales with the sensitivity \(\Delta\) (see Theorem 9.) It is often the case that \(\operatorname{scr}_{H}\) depends on the sum of individual terms involving each item. Our approach is to clip the contribution from each user so that the sensitivity is small. We show that selecting the clipping threshold appropriately is sufficient obtain new user-level pure-DP (and in some cases optimal) algorithms.

## 2 Preliminaries

Let \([n]=\{1,\ldots,n\}\), let \([x]_{+}=\max(0,x)\), and let \(\widetilde{O}(f)\) denote \(O(f\log^{c}f)\) for some constant \(c\). Let \(\operatorname{supp}(A)\) denote the support of distribution \(A\). We recall some standard tools from DP [13].

For two distributions \(A,B\), let \(d_{\varepsilon}(A\parallel B)=\sum_{x\in\operatorname{supp}(A)}[A(x)-e^{ \varepsilon}B(x)]_{+}\) denote the _\(e^{\varepsilon}\)-hockey stick divergence_. For brevity, we write \(A\approx_{\varepsilon,\delta}B\) to denote \(d_{\varepsilon}(A\parallel B)\leq\delta\) and \(d_{\varepsilon}(B\parallel A)\leq\delta\).

**Lemma 5** ("Triangle Inequality").: _If \(A\approx_{\varepsilon^{\prime},\delta^{\prime}}B\) and \(B\approx_{\varepsilon,\delta}C\), then \(A\approx_{\varepsilon+\varepsilon^{\prime},\varepsilon^{\prime}\varepsilon^{ \prime}+e^{\varepsilon^{\prime}\delta}}C\)._

**Lemma 6** (Group Privacy).: _Let \(\mathbb{A}\) be any \((\varepsilon,\delta)\)-item-level DP algorithm and \(\bm{x},\bm{x}^{\prime}\) be \(k\)-neighbors5, then \(\mathbb{A}(\bm{x})\approx_{\varepsilon^{\prime},\delta^{\prime}}\mathbb{A}(\bm{ x}^{\prime})\) where \(\varepsilon^{\prime}=k\varepsilon,\delta^{\prime}=\frac{e^{k\varepsilon}-1}{e^{ \varepsilon}-1}\delta\leq ke^{k\varepsilon}\delta\)._

Footnote 5: I.e., there exists \(\bm{x}=\bm{x}_{0},\bm{x}_{1},\ldots,\bm{x}_{k}=\bm{x}^{\prime}\) such that \(\bm{x}_{i-1},\bm{x}_{i}\) are item-level neighbors for all \(i\in[k]\).

**Theorem 7** (Amplification-by-Subsampling [1]).: _Let \(n,n^{\prime}\in\mathbb{N}\) be such that \(n^{\prime}\geq n\). For any \((\varepsilon,\delta)\)-item-level DP algorithm \(\mathbb{A}\) with sample complexity \(n\), let \(\mathbb{A}^{\prime}\) denote the algorithm with sample complexity \(n^{\prime}\) that randomly chooses \(n\) out of the \(n^{\prime}\) input samples and then runs \(\mathbb{A}\) on the subsample. Then \(\mathbb{A}^{\prime}\) is \((\varepsilon^{\prime},\delta^{\prime})\)-DP where \(\varepsilon^{\prime}=\ln\left(1+\eta(e^{\varepsilon}-1)\right),\delta^{\prime}= \eta\delta\) for \(\eta=n/n^{\prime}\)._

We also need the definition of the shifted and truncated discrete Laplace distribution:

**Definition 8** (Shifted Truncated Discrete Laplace Distribution).: For any \(\varepsilon,\delta>0\), let \(\kappa=\kappa(\varepsilon,\delta):=1+\lceil\ln(1/\delta)/\varepsilon\rceil\) and let \(\mathsf{TDLap}(\varepsilon,\delta)\) be the distribution supported on \(\{0,\ldots,2\kappa\}\) with probability mass function at \(x\) being proportional to \(\exp\left(-\varepsilon\cdot|x-\kappa|\right)\).

**Exponential Mechanism (EM).** In the (generalized) _selection_ problem (SELECT), we are given a candidate set \(\mathcal{H}\) and scoring functions \(\operatorname{scr}_{H}\) for all \(H\in\mathcal{H}\) whose sensitivities are at most \(\Delta\). We say that an algorithm \(\mathbb{A}\) is \((\alpha,\beta)\)-accurate iff \(\Pr_{H\sim\mathbb{A}(\bm{x})}[\operatorname{scr}_{H}(\bm{x})\leq\min_{H^{ \prime}\in\mathcal{H}}\operatorname{scr}_{H^{\prime}}(\bm{x})+\alpha]\geq 1-\beta\).

**Theorem 9** ([14]).: _There is an \(\varepsilon\)-DP \((O(\Delta\cdot\log(|\mathcal{H}|/\beta)/\varepsilon),\beta)\)-accurate algorithm for SELECT._

## 3 Approximate-DP

In this section, we prove our main result on approximate-DP user-level learning, which is stated in Theorem10 below. Throughout the paper we say that a parameter is "sufficiently small" if it is at most an implicit absolute constant.

**Theorem 10** (Main Theorem).: _Let \(\mathfrak{T}\) be any task and \(\gamma>0\) be a parameter. Then, for any sufficiently small \(\varepsilon,\delta>0\) and \(m\in\mathbb{N}\), there exists \(\varepsilon^{\prime}=\frac{\varepsilon^{2}}{\log(1/\delta)\sqrt{m\log(m/ \delta)}}\) and \(\delta^{\prime}=\Theta\left(\delta\cdot\frac{\varepsilon}{m\log(1/\delta)}\right)\) such that_

\[n_{m}^{\mathfrak{T}}(\varepsilon,\delta;\gamma-o(1))\;\leq\;\widetilde{O} \left(\tfrac{\log(1/\delta)}{\varepsilon}+\tfrac{1}{m}\cdot n_{1}^{\mathfrak{ T}}\left(\varepsilon^{\prime},\delta^{\prime};\gamma\right)\right).\]

Note that Corollary3 follows directly from Theorem10 by plugging in the expression \(n_{1}^{\mathfrak{T}}(\varepsilon^{\prime},\delta^{\prime};\gamma)=\widetilde{O }\left(\tfrac{\log(1/\delta^{\prime})^{c}}{\varepsilon^{\prime}}\cdot n_{ \text{priv}}^{\mathfrak{T}}+n_{\text{stat}}^{\mathfrak{T}}\right)\) into the bound.

While it might be somewhat challenging to interpret the bound in Theorem10, given that the \(\varepsilon\) values on the left and the right hand sides are not the same, a simple subsampling argument (similar to one in [1]) allows us to make the LHS and RHS have the same \(\varepsilon\). In this case, we have that the user complexity is reduced by a factor of roughly \(1/\sqrt{m}\), as stated below. We remark that the \(\delta\) values are still different on the two sides; however, if we assume that the dependence on \(1/\delta\) is only polylogarithmic, this results in at most a small gap of at most polylogarithmic factor in \(1/\delta\).

**Theorem 11** (Main Theorem-Same \(\varepsilon\) Version).: _Let \(\mathfrak{T}\) be any task and \(\gamma>0\) be a parameter. Then, for any sufficiently small \(\varepsilon,\delta>0\), and \(m\in\mathbb{N}\), there exists \(\delta^{\prime}=\Theta\left(\delta\cdot\tfrac{\varepsilon}{m\log(1/\delta)}\right)\) such that_

\[n_{m}^{\mathfrak{T}}(\varepsilon,\delta;\gamma-o(1))\leq\widetilde{O}\left( \frac{\log(1/\delta)}{\varepsilon}+\frac{1}{\sqrt{m}}\cdot\frac{\log(1/\delta )^{1.5}}{\varepsilon}\cdot n_{1}^{\mathfrak{T}}(\varepsilon,\delta^{\prime}; \gamma)\right).\]

Even though Theorem11 might be easier to interpret, we note that it does not result in as sharp a bound as Theorem10 in some scenarios. For example, if we use Theorem11 under the assumption in Corollary3, then the privacy-independent part would be \(\frac{1}{\sqrt{m}}\cdot\tfrac{\log(1/\delta)^{1.5}}{\varepsilon}\cdot n_{ \text{stat}}^{\mathfrak{T}}\), instead of \(\frac{1}{m}\cdot n_{\text{stat}}^{\mathfrak{T}}\) implied by Theorem10. (On the other hand, the privacy-dependent part is the same.)

The remainder of this section is devoted to the proof of Theorem10. The high-level structure follows the overview in Section1.1.1: first we show in Section3.1 that any item-level DP algorithm satisfies sample perfect generalization. Section3.2.1 then relates this to a local version of user-level DP. We then describe the full algorithm and its guarantees in Section3.2.2. Since we fix a task \(\mathfrak{T}\) throughout this section, we will henceforth discard the superscript \(\mathfrak{T}\) for brevity.

### DP Implies Sample Perfect Generalization

We begin with the definition of _sample perfect generalization_ algorithms.

**Definition 12** ([16]).: For \(\beta,\varepsilon,\delta>0\), an algorithm \(\mathbb{A}:\mathcal{Z}^{n}\to\mathcal{O}\) is said to be \((\beta,\varepsilon,\delta)\)_-sample perfectly generalizing_ iff, for any distribution \(\mathcal{D}\) (over \(\mathcal{Z}\)), \(\Pr_{\bm{x},\bm{x}^{\prime}\sim\mathcal{D}^{n}}[\mathbb{A}(\bm{x})\approx_{ \varepsilon,\delta}\mathbb{A}(\bm{x}^{\prime})]\geq 1-\beta\).

The main result of this subsection is that any \((\varepsilon,\delta)\)-item-level DP algorithm is \((\beta,O(\varepsilon\sqrt{n\log(\nicefrac{{1}}{{\beta\delta}})}),O(n\delta))\)-sample perfectly generalizing:

**Theorem 13** (\(\mathrm{DP}\Rightarrow\) Sample Perfect Generalization).: _Suppose that \(\mathbb{A}:\mathcal{Z}^{n}\to\mathcal{O}\) is \((\varepsilon,\delta)\)-item-level DP, and assume that \(\varepsilon,\delta,\beta,\varepsilon\sqrt{n\log(\nicefrac{{1}}{{\beta\delta} })}>0\) are sufficiently small. Then, \(\mathbb{A}\) is \((\beta,\varepsilon^{\prime},\delta^{\prime})\)-sample perfectly generalizing, where \(\varepsilon^{\prime}\leq O(\varepsilon\sqrt{n\log(\nicefrac{{1}}{{\beta\delta} })})\) and \(\delta^{\prime}=O(n\delta)\)._

#### 3.1.1 Bounding the Expected Divergence

As a first step, we upper bound the expectation of the hockey stick divergence \(d_{\varepsilon^{\prime}}(\mathbb{A}(\bm{x})\parallel\mathbb{A}(\bm{x}^{\prime}))\):

**Lemma 14**.: _Suppose that \(\mathbb{A}:\mathcal{Z}^{n}\to\mathcal{O}\) is an \((\varepsilon,\delta)\)-item-level DP algorithm. Further, assume that \(\varepsilon,\delta\), and \(\varepsilon\sqrt{n\log(\nicefrac{{1}}{{\delta}})}>0\) are sufficiently small. Then, for \(\varepsilon^{\prime}=O(\varepsilon\sqrt{n\log(\nicefrac{{1}}{{\delta}})})\), we have_

\[\mathbb{E}_{\bm{x},\bm{x}^{\prime}\sim\mathcal{D}^{n}}[d_{\varepsilon^{\prime }}(\mathbb{A}(\bm{x})\parallel\mathbb{A}(\bm{x}^{\prime}))]\leq O(n\delta).\]

This proof follows an idea from [16], who prove a similar statement for _pure-DP_ algorithms (i.e., \(\delta=0\)). For every output \(o\in\mathcal{O}\), they consider the function \(g^{o}:\mathcal{Z}^{n}\to\mathbb{R}\) defined by \(g^{o}(\bm{x}):=\ln(\Pr[\mathbb{A}(\bm{x})=o])\). The observation here is that, in the pure-DP case, this function is \(\varepsilon\)-Lipschitz (i.e., changing a single coordinate changes its value by at most \(\varepsilon\)). They then apply McDiarmid's inequality on \(g^{o}\) to show that the function values are well-concentrated with a variance of \(O(\sqrt{n}\cdot\varepsilon)\). This suffices to prove the above statement for the case where the algorithm is pure-DP (but the final bound still contains \(\delta\)). To adapt this proof to the approximate-DP case, we prove a "robust" version of McDiarmid's inequality that works even for the case where the Lipschitz property is violated on some pairs of neighbors (Lemma 32). This inequality is shown by arguing that we may change the function "slightly" to make it multiplicative-Lipschitz, which then allows us to apply standard techniques. Due to space constraints, we defer the full proof, which is technically involved, to Appendix C.3.

#### 3.1.2 Boosting the Probability: Proof of Theorem 13

While Lemma 14 bounds the expectation of the hockey stick divergence, it is insufficient to get arbitrarily high probability bound (i.e., for any \(\beta\) as in Theorem 13); for example, using Markov's inequality would only be able to handle \(\beta>\delta^{\prime}\). However, we require very small \(\beta\) in a subsequent step of the proofs (in particular Theorem 17). Fortunately, we observe that it is easy to "boost" the \(\beta\) to be arbitrarily small, at an additive cost of \(\varepsilon\sqrt{n\log(\nicefrac{{1}}{{\beta}})}\) in the privacy loss.

To do so, we will need the following concentration result of Talagrand6 for product measures:

Footnote 6: This is a substantially simplified version compared to the one in [17], but is sufficient for our proof.

**Theorem 15** ([17]).: _For any distribution \(\mathcal{D}\) over \(\mathcal{Z}\), \(\mathcal{E}\subseteq\mathcal{Z}^{n}\), and \(t>0\), if \(\Pr_{\bm{x}\sim\mathcal{D}^{n}}[\bm{x}\in\mathcal{E}]\geq 1/2\), then \(\Pr_{\bm{x}\sim\mathcal{D}^{n}}[\bm{x}\in\mathcal{E}_{\leq t}]\geq 1-0.5 \exp\left(-\nicefrac{{t^{2}}}{{4n}}\right)\), where \(\mathcal{E}_{\leq t}:=\{\bm{x}^{\prime}\in\mathcal{Z}^{n}\mid\exists\bm{x}\in \mathcal{E},\|\bm{x}^{\prime}-\bm{x}\|_{0}\leq t\}\)._

Proof of Theorem 13.: We consider two cases based on whether \(\beta\geq 2^{-n}\) or not. Let us start with the case that \(\beta\geq 2^{-n}\). From Lemma 14, for \(\varepsilon^{\prime\prime}=O(\varepsilon\sqrt{n\log(\nicefrac{{1}}{{\delta}})})\), we have

\[\mathbb{E}_{\bm{x},\bm{x}^{\prime}\sim\mathcal{D}^{n}}[d_{\varepsilon^{\prime \prime}}(\mathbb{A}(\bm{x})\parallel\mathbb{A}(\bm{x}^{\prime}))]\leq O(n \delta)=:\delta^{\prime\prime}.\]

Let \(\mathcal{E}\subseteq\mathcal{Z}^{n}\times\mathcal{Z}^{n}\) denote the set of \((\tilde{\bm{x}},\tilde{\bm{x}}^{\prime})\) such that \(\mathbb{A}(\tilde{\bm{x}})\approx_{\varepsilon^{\prime\prime},4\delta^{\prime \prime}}\mathbb{A}(\tilde{\bm{x}}^{\prime})\). By Markov's inequality, we have that \(\Pr_{\bm{x},\bm{x}^{\prime}\sim\mathcal{D}^{n}}[d_{\varepsilon^{\prime\prime }}(\mathbb{A}(\tilde{\bm{x}})\parallel\mathbb{A}(\tilde{\bm{x}}^{\prime})) \leq 4\delta^{\prime\prime}]\geq 3/4\) and \(\Pr_{\bm{x},\bm{x}^{\prime}\sim\mathcal{D}^{n}}[d_{\varepsilon^{\prime \prime}}(\mathbb{A}(\tilde{\bm{x}}^{\prime})\parallel\mathbb{A}(\tilde{\bm{x}})) \leq 4\delta^{\prime\prime}]\geq 3/4\) and hence \(\Pr_{\bm{x},\bm{x}^{\prime}\sim\mathcal{D}^{n}}[(\bm{x},\bm{x}^{\prime})\in \mathcal{E}]\geq 1/2\). By Theorem 15, for \(t=O(\sqrt{n\log(1/\beta)})\), we have

\[\Pr_{\bm{x},\bm{x}^{\prime}\sim\mathcal{D}^{n}}[(\bm{x},\bm{x}^{\prime})\in \mathcal{E}_{\leq t}]\geq 1-\beta.\]

Next, consider any \((\bm{x},\bm{x}^{\prime})\in\mathcal{E}_{\leq t}\). Since there exists \((\tilde{\bm{x}},\tilde{\bm{x}}^{\prime})\in\mathcal{E}\) such that \(\|\bm{x}-\tilde{\bm{x}}\|_{0},\|\bm{x}^{\prime}-\tilde{\bm{x}}^{\prime}\|_{0} \leq t\), we may apply Lemma 6 to conclude that \(d_{\varepsilon t}(\mathbb{A}(\bm{x})\parallel\mathbb{A}(\tilde{\bm{x}})),d_{ \varepsilon t}(\mathbb{A}(\tilde{\bm{x}}^{\prime})\parallel\mathbb{A}(\bm{x}^{ \prime}))\leq\mathbb{A}(\tilde{\bm{x}}^{\prime})\).

\(O(t\delta)\). Furthermore, since \(d_{\varepsilon^{\prime\prime}}(\mathbb{A}(\tilde{\bm{x}})\parallel\mathbb{A}(\tilde {\bm{x}}^{\prime}))\leq 4\delta^{\prime\prime}\), we may apply Lemma 5 to conclude that \(d_{\varepsilon^{\prime}}(\mathbb{A}(\bm{x})\parallel\mathbb{A}(\bm{x}^{\prime})) \leq\delta^{\prime}\) where \(\varepsilon^{\prime}=\varepsilon^{\prime\prime}+2t\varepsilon=O(\varepsilon \sqrt{n\log(\nicefrac{{1}}{{\beta\delta}})})\) and \(\delta^{\prime}=O(\delta^{\prime\prime}+t\delta)=O(n\delta)\). Similarly, we also have \(d_{\varepsilon^{\prime}}(\mathbb{A}(\bm{x}^{\prime}\parallel\mathbb{A}(\bm{x} ))\leq\delta^{\prime}\). In other words, \(\mathbb{A}(\bm{x})\approx_{\varepsilon^{\prime},\delta^{\prime}}\mathbb{A}( \bm{x}^{\prime})\). Combining this and the above inequality implies that \(\mathbb{A}\) is \((\beta,\varepsilon^{\prime},\delta^{\prime})\)-sample perfectly generalizing as desired.

For the case \(\beta<2^{-n}\), we may immediately apply group privacy (Lemma 6). Since any \(\bm{x},\bm{x}^{\prime}\sim\mathcal{D}^{n}\) satisfies \(\|\bm{x}-\bm{x}^{\prime}\|_{0}\leq n\), we have \(\mathbb{A}(\bm{x})\approx_{\varepsilon^{\prime},\delta^{\prime}}\mathbb{A}( \bm{x}^{\prime})\) for \(\varepsilon^{\prime}=n\varepsilon\) and \(\delta^{\prime}=O(n\delta)\). This implies that \(\mathbb{A}\) is \((\beta,\varepsilon^{\prime},\delta^{\prime})\)-sample perfectly generalizing (in fact, \((0,\varepsilon^{\prime},\delta^{\prime})\)-sample perfectly generalizing), which concludes our proof. 

### From Sample Perfect Generalization to User-Level DP

Next, we will use the sample perfect generalization result from the previous subsection to show that our algorithm satisfies a certain definition of a local version of user-level DP. We then finally turn this intuition into an algorithm and prove Theorem 10.

#### 3.2.1 Achieving Local-Deletion DP

We start by defining _local-deletion DP_ (for user-level DP). As alluded to earlier, this definition only considers \(\bm{x}_{-i}\) for different \(i\)'s. We also define the multi-deletion version where we consider \(\bm{x}_{-S}\) for all subsets \(S\) of small size.

**Definition 16** (Local-Deletion DP).: An algorithm \(\mathbb{A}\) is _\((\varepsilon,\delta)\)-local-deletion DP_ (abbreviated _LDDP_) _at input \(\bm{x}\)_ if \(\mathbb{A}(\bm{x}_{-i})\approx_{\varepsilon,\delta}\mathbb{A}(\bm{x}_{-i^{ \prime}})\) for all \(i,i^{\prime}\in[n]\). Furthermore, for \(r\in\mathbb{N}\), an algorithm \(\mathbb{A}\) is _\((r,\varepsilon,\delta)\)-local-deletion DP at \(\bm{x}\)_ if \(\mathbb{A}(\bm{x}_{-S})\approx_{\varepsilon,\delta}\mathbb{A}(\bm{x}_{-S^{ \prime}})\) for all \(S,S^{\prime}\subseteq[n]\) such that \(|S|=|S^{\prime}|=r\).

Below we show that, with high probability (over the input \(\bm{x}\)), any item-level DP algorithm satisfies LDDP with the privacy loss increased by roughly \(r\sqrt{m}\). The proof uses the crucial observation that relates sample perfect generalization to user-level DP.

**Theorem 17**.: _Let \(n,r\in\mathbb{N}\) with \(r\leq n\), and let \(\mathbb{A}:\mathcal{X}^{(n-r)m}\to\mathcal{O}\) be any \((\varepsilon^{\prime},\delta^{\prime})\)-item-level DP algorithm. Further, assume that \(\varepsilon^{\prime},\delta^{\prime},\varepsilon^{\prime}r\sqrt{m\log(\nicefrac{{ n}}{{\beta\delta^{\prime}}})}>0\) are sufficiently small. Then, for any distribution \(\mathcal{D}\),_

\[\Pr_{\bm{x}\sim\mathcal{D}^{nm}}[\mathbb{A}\text{ is }(r,\varepsilon,\delta) \text{-LDDP at }\bm{x}]\geq 1-\beta,\]

_where \(\varepsilon=O(\varepsilon^{\prime}r\sqrt{m\log(\nicefrac{{n}}{{\beta\delta^{ \prime}}})})\) and \(\delta=O(rm\delta^{\prime})\)._

Proof.: Let \(\beta^{\prime}=\nicefrac{{\beta}}{{r}}^{2}\). Consider any fixed \(S,S^{\prime}\subseteq[n]\) such that \(|S|=|S^{\prime}|=r\). Let \(T=S\cup S^{\prime},t=|T|,U=S^{\prime}\smallsetminus S,U^{\prime}=S\smallsetminus S ^{\prime},q=|U|=|U^{\prime}|\), and let \(\mathbb{A}^{\prime}_{\bm{x}_{-T}}\) denote the algorithm defined by \(\mathbb{A}^{\prime}_{\bm{x}_{-T}}(\bm{y})=\mathbb{A}(\bm{x}_{-T}\cup\bm{y})\).

Thus, for any fixed \(\bm{x}_{-T}\in\mathcal{Z}^{(n-t)m}\), Theorem 13 implies that

\[\Pr_{\bm{x}_{U},\bm{x}_{U^{\prime}}\sim\mathcal{D}^{nm}}\left[\mathbb{A}^{\prime }_{\bm{x}_{-T}}(\bm{x}_{U})\not\approx_{\varepsilon,\delta}\mathbb{A}^{\prime }_{\bm{x}_{-T}}(\bm{x}_{U^{\prime}})\right]\leq\beta^{\prime},\] (1)

where \(\varepsilon=O(\varepsilon^{\prime}\sqrt{qm\log(\nicefrac{{1}}{{\beta^{\prime} \delta^{\prime}}})})\leq O(\varepsilon^{\prime}r\sqrt{m\log(\nicefrac{{n}}{{\beta \delta^{\prime}}})})\) and \(\delta=O(rm\delta^{\prime})\).

Notice that \(\mathbb{A}^{\prime}_{\bm{x}_{-T}}(\bm{x}_{U})=\mathbb{A}(\bm{x}_{-S})\) and \(\mathbb{A}^{\prime}_{\bm{x}_{-T}}(\bm{x}_{U^{\prime}})=\mathbb{A}(\bm{x}_{-S^{ \prime}})\). Thus, we have

\[\Pr_{\bm{x}\sim\mathcal{D}^{nm}}[\mathbb{A}(\bm{x}_{-S})\not\approx _{\varepsilon,\delta}\mathbb{A}(\bm{x}_{-S^{\prime}})]= \mathbb{E}_{\bm{x}_{-T}\sim\mathcal{D}^{(n-t)m}}\left[\Pr_{\bm{x}_{U},\bm{x} _{U^{\prime}}\sim\mathcal{D}^{nm}}\left[\mathbb{A}^{\prime}_{\bm{x}_{-T}}(\bm{x }_{U})\not\approx_{\varepsilon,\delta}\mathbb{A}^{\prime}_{\bm{x}_{-T}}(\bm{x }_{U^{\prime}})\right]\right]\] \[\stackrel{{\text{(\ref{eq:defwhere \(\kappa\) is from Definition 8.) The target size of \(S\) is noised using the discrete Laplace distribution. As stated in Section 1.1.1, the algorithm is very similar to that of [6, 23], with the main difference being that (i) \(\mathcal{X}^{R_{1}}_{\mathrm{stable}}\) is defined based on LDDP instead of the local deletion sensitivity and (ii) our output is \(\mathbb{A}(\bm{x}_{-T})\), whereas their output is the function value with Gaussian noise added. It is not hard to adapt their proof to show that our algorithm is \((\varepsilon,\delta)\)-user-level DP, as stated below. (Full proof is deferred to Appendix C.4.)

**Lemma 18** (Privacy Guarantee).: \(\mathsf{DelStab}_{\varepsilon,\delta,\mathbb{A}}\) _is \((\varepsilon,\delta)\)-user-level DP._

```
1:Input: Dataset \(\bm{x}\in\mathcal{X}^{nm}\)
2:Parameters: Privacy parameters \(\varepsilon,\delta\), Algorithm \(\mathbb{A}:\mathcal{X}^{(n-4\kappa)m}\to\mathcal{O}\)
3:\(\overline{\varepsilon}\leftarrow\frac{\varepsilon}{3},\overline{\delta} \leftarrow\frac{\delta}{c^{2\pi}+\varepsilon^{\prime}+2}\), \(\kappa\leftarrow\kappa(\overline{\varepsilon},\overline{\delta})\).
4:Sample \(R_{1}\sim\mathsf{TDLap}(\overline{\varepsilon},\overline{\delta})\) {See Definition 8}
5:\(\mathcal{X}^{R_{1}}_{\mathrm{stable}}\leftarrow\big{\{}S\subseteq[n]:|S|=R_{1}, \mathbb{A}\text{ is }(4\kappa-R_{1},\overline{\varepsilon},\overline{\delta})\text{-LDDP at }\bm{x}_{-S}\big{\}}\) {See Definition 16}
6:if\(|\mathcal{X}^{R_{1}}_{\mathrm{stable}}|=\emptyset\)then
7:return\(\bot\)
8:Choose \(S\in\mathcal{X}^{R_{1}}_{\mathrm{stable}}\) uniformly at random
9:Choose \(T\supseteq\overline{\delta}\) of size \(4\kappa\) uniformly at random
10:return\(\mathbb{A}(\bm{x}_{-T})\) ```

**Algorithm 1**\(\mathsf{DelStab}_{\varepsilon,\delta,\mathbb{A}}(\bm{x})\)

Next, we prove the utility guarantee under the assumption that \(\mathbb{A}\) is item-level DP:

**Lemma 19** (Utility Guarantee).: _Let \(\varepsilon,\delta>0\) be sufficiently small. Suppose that \(\mathbb{A}\) is \((\varepsilon^{\prime},\delta^{\prime})\)-item-level DP such that \(\varepsilon^{\prime}=\Theta\left(\frac{\varepsilon}{\kappa\sqrt{m\log(nm/ \delta)}}\right)\) and \(\delta^{\prime}=\Theta\left(\frac{\delta}{\kappa m}\right)\). If \(\mathbb{A}\) is \(\gamma\)-useful for any task \(\mathfrak{T}\) with \((n-4\kappa)m\) samples, then \(\mathsf{DelStab}_{\varepsilon,\delta,\mathbb{A}}\) is \((\gamma-o(1))\)-useful for \(\mathfrak{T}\) (with \(nm\) samples)._

Proof.: Consider another algorithm \(\mathbb{A}_{0}\) defined in the same way as \(\mathsf{DelStab}_{\varepsilon,\delta,\mathbb{A}}\) except that on Line 5, we simply let \(\mathcal{X}^{R_{1}}_{\mathrm{stable}}\leftarrow\binom{[n]}{R_{1}}\). Note that \(\mathbb{A}_{0}(\bm{x})\) is equivalent to randomly selecting \(n-4\kappa\) users and running \(\mathbb{A}\) on it. Therefore, we have \(\Pr_{\bm{x}\sim\mathcal{D}^{nm}}[\mathbb{A}_{0}(\bm{x})\in\Psi_{\mathfrak{T}}] =\Pr_{\bm{x}\sim\mathcal{D}^{(n-4\kappa)m}}[\mathbb{A}(\bm{x})\in\Psi_{ \mathfrak{T}}]\geq\gamma\)

Meanwhile, \(\mathbb{A}_{0}\) and \(\mathsf{DelStab}_{\varepsilon,\mathbb{A}}\) are exactly the same as long as \(\mathcal{X}^{R_{1}}_{\mathrm{stable}}(\bm{x})=\binom{[n]}{R_{1}}\) in the latter. In other words, we have

\[\Pr_{\bm{x}\sim\mathcal{D}^{nm}}[\mathsf{DelStab}_{\varepsilon, \delta,\mathbb{A}}(\bm{x})\in\Psi_{\mathfrak{T}}] \geq\Pr_{\bm{x}\sim\mathcal{D}^{nm}}[\mathbb{A}_{0}(\bm{x})\in\Psi_{ \mathfrak{T}}]-\Pr_{\bm{x}\sim\mathcal{D}^{nm}}\left[\mathcal{X}^{R_{1}}_{ \mathrm{stable}}(\bm{x})\neq\binom{[n]}{R_{1}}\right]\] \[\geq\gamma-\Pr_{\bm{x}\sim\mathcal{D}^{nm}}[\mathbb{A}\text{ is not }(4\kappa,\overline{\varepsilon},\overline{\delta})\text{-LDDP at }\bm{x}] \geq\gamma-o(1),\]

where in the last inequality we apply Theorem 17 with, e.g., \(\beta=\nicefrac{{0.1}}{{1}}\)\(nm\). 

Putting Things Together.Theorem 10 is now "almost" an immediate consequence of Lemmas 18 and 19 (using \(\kappa=O\left(\log(1/\delta)/\varepsilon\right)\)). The only challenge here is that \(\varepsilon^{\prime}\) required in Lemma 19 depends polylogarithmically on \(n\). Nonetheless, we show below via a simple subsampling argument that this only adds polylogarithmic overhead to the user/sample complexity.

Proof of Theorem 10.: Let \(\varepsilon^{\prime}=\varepsilon^{\prime}(n),\delta^{\prime}\) be as in Lemma 19. Furthermore, let \(\varepsilon^{\prime\prime}=\frac{\varepsilon^{2}}{\log(1/\delta)\sqrt{m\log(m/ \delta)}}\). Notice that \(\varepsilon^{\prime}(n)/\varepsilon^{\prime\prime}\leq(\log n)^{O(1)}\). Let \(N\in\mathbb{N}\) be the smallest number such that \(N\) is divisible by \(m\) and

\[\left(e^{\varepsilon^{\prime\prime}}-1\right)\cdot\frac{n_{1}(\varepsilon^{ \prime\prime},\delta^{\prime})}{N}\leq e^{\varepsilon^{\prime}(N/m+4\kappa)}-1,\]

Observe that \(N\leq\widetilde{O}\left(m\kappa+n_{1}(\varepsilon^{\prime\prime},\delta^{ \prime})\right)\).

From the definition of \(n_{1}\), there exists an \((\varepsilon^{\prime\prime},\delta^{\prime})\)-item-level DP algorithm \(\mathbb{A}_{0}\) that is \(\gamma\)-useful for \(\mathfrak{T}\) with sample complexity \(n_{1}(\varepsilon^{\prime\prime},\delta^{\prime})\). We start by constructing an algorithm \(\mathbb{A}\) on \(N\) samples as follows: randomly sample (without replacement) \(n_{1}(\varepsilon^{\prime\prime},\delta^{\prime})\) out of \(N\) items, then run \(\mathbb{A}_{0}\) on this subsample. By amplification-by-subsampling (Theorem 7), we have that \(\mathbb{A}\) is \((\varepsilon^{\prime}(n),\delta^{\prime})\)-item-level DP for \(n=N/m+4\kappa\). Thus, by Lemma 18 and Lemma 19, we have that \(\mathsf{DelStab}_{\varepsilon,\delta,\mathbb{A}}\) is \((\varepsilon,\delta)\)-DP and \((\gamma-o(1))\)-useful for \(\mathfrak{T}\). Its user complexity is

\[n=N/m+4\kappa=\widetilde{O}\left(\frac{\log(1/\delta)}{\varepsilon}+\frac{1}{ m}\cdot n_{1}\left(\frac{\varepsilon^{2}}{\log(1/\delta)\sqrt{m\log(m/\delta)}}, \delta^{\prime}\right)\right).\qed\]

## 4 Pure-DP: PAC Learning

In this section we prove Theorem 4. Let \(\bm{z}\) denote the input and for \(i\in[n]\), let \(\bm{z}_{i}=((x_{i,1},y_{i,1}),\ldots,(x_{i,m},y_{i,m}))\) denote the input to the \(i\)th user. A _concept class_ is a set of functions of the form \(\mathcal{X}\to\{0,1\}\). For \(T\subseteq\mathcal{X}\times\{0,1\}\), we say that it is _realizable_ by \(c:\mathcal{X}\to\{0,1\}\) iff \(c(x)=y\) for all \((x,y)\in T\). Recall the definition of \(\mathrm{PRDim}\):

**Definition 20** ([19]).: A distribution \(\mathcal{P}\) on concept classes is an \((\alpha,\beta)\)_-probabilistic representation_ (abbreviated as \((\alpha,\beta)\)_-PR_) of a concept class \(\mathcal{C}\) if for every \(c\in\mathcal{C}\) and for every distribution \(\mathcal{D}_{\mathcal{X}}\) on \(\mathcal{X}\), with probability \(1-\beta\) over \(\mathcal{H}\sim\mathcal{P}\), there exists \(h\in\mathcal{H}\) such that \(\Pr_{x\sim\mathcal{D}_{\mathcal{X}}}[c(x)\neq h(x)]\leq\alpha\).

The \((\alpha,\beta)\)_-PR dimension_ of \(\mathcal{C}\) is defined as \(\mathrm{PRDim}_{\alpha,\beta}(\mathcal{C}):=\min_{(\alpha,\beta)\text{-} \text{PR }\mathcal{P}\text{ of }\mathcal{C}}\operatorname{size}(\mathcal{P})\), where \(\operatorname{size}(\mathcal{P}):=\max_{\mathcal{H}\in\mathrm{supp}(\mathcal{P })}\log|\mathcal{H}|\). Furthermore, let the _probabilistic representation dimension_ of \(\mathcal{C}\) be \(\mathrm{PRDim}(\mathcal{C}):=\mathrm{PRDim}_{1/4,1/4}(\mathcal{C})\).

**Lemma 21** ([19]).: _For any \(\mathcal{C}\) and \(\alpha\in(0,1/2)\), \(\mathrm{PRDim}_{\alpha,1/4}(\mathcal{C})\leq\widetilde{O}\left(\mathrm{PRDim} (\mathcal{C})\cdot\log(1/\alpha)\right)\)._

Proof of Theorem 4.: The lower bound \(\tilde{\Omega}\left(d/\varepsilon\right)\) was shown in [1] (for any value of \(m\)). The lower bound \(\tilde{\Omega}\left(\frac{d}{\varepsilon\alpha m}\right)\) follows from the lower bound of \(\tilde{\Omega}\left(\frac{d}{\varepsilon\alpha}\right)\) on the sample complexity in the item-level setting by [19] and the observation that any \((\varepsilon,\delta)\)-user-level DP algorithm is also \((\varepsilon,\delta)\)-item-level DP with the same sample complexity. Thus, we may focus on the upper bound.

It suffices to only prove the upper bound \(\widetilde{O}\left(\frac{d}{\varepsilon}+\frac{d}{\varepsilon\alpha m}\right)\) assuming7\(m\leq 1/\alpha\). Let \(\mathcal{P}\) denote a \((0.01\alpha,1/4)\)-PR of \(\mathcal{C}\); by Lemma 21, there exists such \(\mathcal{P}\) with \(\operatorname{size}(\mathcal{P})\leq\widetilde{O}\left(d\cdot\log(1/\alpha)\right)\). Assume that the number \(n\) of users is at least \(\frac{\kappa\cdot\operatorname{size}(\mathcal{P})}{\varepsilon\alpha m}= \widetilde{O}\left(\frac{d}{\varepsilon\alpha m}\right)\), where \(\kappa\) is a sufficiently large constant. Our algorithm works as follows: Sample \(\mathcal{H}\sim\mathcal{P}\) and then run the \(\varepsilon\)-DP EM (Theorem 9) on candidate set \(\mathcal{H}\) with the scoring function \(\operatorname{scr}_{h}(\bm{z}):=\sum_{i\in[n]}\bm{1}[\bm{z}_{i}\) is not realizable by \(h\)].

Footnote 7: If \(m\) is larger, then we can simply disregard the extra examples (as the first term dominates the bound).

Observe that the sensitivity of \(\operatorname{scr}_{h}\) is one. Indeed, this is where our "clipping" has been applied: a standard item-level step would sum up the error across all the samples, resulting in the sensitivity as large as \(m\), while our scoring function above "clips" the contribution of each user to just one.

A simple concentration argument (deferred to Appendix D) gives us the following:

**Lemma 22**.: _With probability 0.9 (over the input), the following hold for all \(h\in\mathcal{H}\):_

1. _If_ \(\operatorname{err}_{\mathcal{D}}(h)\leq 0.01\alpha\)_, then_ \(\operatorname{scr}_{h}(\bm{z})\leq 0.05\alpha nm\)_._
2. _If_ \(\operatorname{err}_{\mathcal{D}}(h)>\alpha\)_, then_ \(\operatorname{scr}_{h}(\bm{z})>0.1\alpha nm\)_._

We now assume that the event in Lemma 22 holds and finish the proof. Since a \(\mathcal{P}\) is a \((0.01\alpha,1/4)\)-PR of \(\mathcal{C}\), with probability \(3/4\), we have that there exists \(h^{*}\in\mathcal{H}\) such that \(\operatorname{err}_{\mathcal{D}}(h^{*})\leq 0.01\alpha\). From Lemma 22(i), we have \(\operatorname{scr}_{h^{*}}(\bm{z})\leq 0.05\alpha nm\). Thus, by Theorem 9, with probability \(2/3\), the algorithm outputs \(h\) that satisfies \(\operatorname{scr}_{h}(\bm{z})\leq 0.05\alpha nm+O\left(\operatorname{size}( \mathcal{P})/\varepsilon\right)\), which, for a sufficiently large \(\kappa\), is at most \(0.1\alpha nm\). By Lemma 22(ii), this implies that \(\operatorname{err}_{\mathcal{D}}(h)\leq\alpha\) as desired. 

## 5 Conclusion and Open Questions

We presented generic techniques to transform item-level DP algorithms to user-level DP algorithms.

There are several open questions. Although our upper bounds are demonstrated to be tight for many tasks discussed in this work, it is not hard to see that they are not always tight8; thus, an important research direction is to get a better understanding of the tight user complexity bounds for different learning tasks.

Footnote 8: E.g., for PAC learning, Theorem 4 provides a better user complexity than applying the approximate-DP transformation (Theorem 10) to item-level approximate-DP learners for a large regime of parameters and concept classes; the former has user complexity that decreases with \(1/m\) whereas the latter decreases with \(1/\sqrt{m}\).

On a more technical front, our transformation for approximate-DP (Theorem 10) has an item-level privacy loss (roughly) of the form \(\frac{\varepsilon^{2}}{\sqrt{m\log(1/\delta)^{3}}}\) while it seems plausible that \(\frac{\varepsilon}{\sqrt{m\log(1/\delta)}}\) can be achieved; an obvious open question is to give such an improvement, or show that it is impossible. We note that the extra factor of \(\nicefrac{{\log(1/\delta)}}{{\varepsilon}}\) comes from the fact that Algorithm 1 uses LDDF with distance \(O(\kappa)=O\left(\nicefrac{{\log(1/\delta)}}{{\varepsilon}}\right)\). It is unclear how one could avoid this, given that the propose-test-release framework requires checking input at distance \(O\left(\nicefrac{{\log(1/\delta)}}{{\varepsilon}}\right)\).

Another limitation of our algorithms is their computational inefficiency, as their running time grows linearly with the size of the possible outputs. In fact, the approximate-DP algorithm could even be slower than this since we need to check whether \(\mathbb{A}\) is LDDP at a certain input \(\bm{x}\) (which involves computing \(d_{\varepsilon}(\mathbb{A}_{-S}\parallel\mathbb{A}_{-S^{\prime}})\)). Nevertheless, given the generality of our results, it is unlikely that a generic efficient algorithm exists with matching user complexity; for example, even the algorithm for user-level DP-SCO in [6] has a worst case running time that is not polynomial. Thus, it remains an interesting direction to devise more efficient algorithms for specific tasks.

Finally, while our work has focused on the central model of DP, where the algorithm gets the access to the raw input data, other models of DP have also been studied in the context of user-level DP, such as the local DP model [11, 12]. Here again, [11, 12] give a generic transformation that sometimes drastically reduces the user complexity in the example-rich case. Another interesting research direction is to devise a refined transformation for the example-sparse case (like one presented in our paper for the central model) but for the local model.

## References

* [Abo18] John M Abowd. The US Census Bureau adopts differential privacy. In _KDD_, pages 2867-2867, 2018.
* [ALS23] Jayadev Acharya, Yuhan Liu, and Ziteng Sun. Discrete distribution estimation under user-level local differential privacy. In _AISTATS_, 2023.
* [App17] Apple Differential Privacy Team. Learning with privacy at scale. _Apple Machine Learning Journal_, 2017.
* [ASZ21] Jayadev Acharya, Ziteng Sun, and Huanyu Zhang. Differentially private Assouad, Fano, and Le Cam. In _ALT_, pages 48-78, 2021.
* [BBG18] Borja Balle, Gilles Barthe, and Marco Gaboardi. Privacy amplification by subsampling: Tight analyses via couplings and divergences. In _NeurIPS_, pages 6280-6290, 2018.
* [BF16] Raef Bassily and Yoav Freund. Typicality-based stability and privacy. _CoRR_, abs/1604.03336, 2016.
* [BGH\({}^{+}\)23] Mark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann Pitassi, Satchit Sivakumar, and Jessica Sorrell. Stability is stable: Connections between replicability, privacy, and adaptive generalization. In _STOC_, 2023.
* [BKSW19] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypothesis selection. In _NeurIPS_, pages 156-167, 2019.
* [BNS19] Amos Beimel, Kobbi Nissim, and Uri Stemmer. Characterizing the sample complexity of pure private learners. _JMLR_, 20:146:1-146:33, 2019.
* [CLN\({}^{+}\)16] Rachel Cummings, Katrina Ligett, Kobbi Nissim, Aaron Roth, and Zhiwei Steven Wu. Adaptive learning with robust generalization guarantees. In _COLT_, pages 772-814, 2016.
* [DHS15] Ilias Diakonikolas, Moritz Hardt, and Ludwig Schmidt. Differentially private learning of structured discrete distributions. In _NIPS_, pages 2566-2574, 2015.
* [DKM\({}^{+}\)06] Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor. Our data, ourselves: Privacy via distributed noise generation. In _EUROCRYPT_, pages 486-503, 2006.
* [DKY17] Bolin Ding, Janardhan Kulkarni, and Sergey Yekhanin. Collecting telemetry data privately. In _NeurIPS_, pages 3571-3580, 2017.
* [DL09] Cynthia Dwork and Jing Lei. Differential privacy and robust statistics. In _STOC_, pages 371-380, 2009.
* [DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D. Smith. Calibrating noise to sensitivity in private data analysis. In _TCC_, pages 265-284, 2006.
* 2361, 2020.
* [GGKM21] Badih Ghazi, Noah Golowich, Ravi Kumar, and Pasin Manurangsi. Sample-efficient proper PAC learning with approximate differential privacy. In _STOC_, 2021.
* [GKK\({}^{+}\)23] Badih Ghazi, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Raghu Meka, and Chiyuan Zhang. On user-level private convex optimization. In _ICML_, 2023.
* [GKM21] Badih Ghazi, Ravi Kumar, and Pasin Manurangsi. User-level differentially private learning via correlated sampling. In _NeurIPS_, pages 20172-20184, 2021.
* but not your data. _Wired, June_, 13, 2016.
* [ILPS22] Russell Impagliazzo, Rex Lei, Toniann Pitassi, and Jessica Sorrell. Reproducibility in learning. In _STOC_, pages 818-831, 2022.
* [KL21] Nitin Kohli and Paul Laskowski. Differential privacy for black-box statistical analyses. In _TPDP_, 2021.
* [KLSU19] Gautam Kamath, Jerry Li, Vikrant Singhal, and Jonathan R. Ullman. Privately learning high-dimensional distributions. In _COLT_, pages 1853-1902, 2019.
* [Kut02] Samuel Kutin. Extensions to McDiarmid's inequality when differences are bounded with high probability. _Dept. Comput. Sci., Univ. Chicago, Chicago, IL, USA, Tech. Rep. TR-2002-04_, 2002.
* [LSA\({}^{+}\)21] Daniel Levy, Ziteng Sun, Kareem Amin, Satyen Kale, Alex Kulesza, Mehryar Mohri, and Ananda Theertha Suresh. Learning with user-level privacy. In _NeurIPS_, pages 12466-12479, 2021.
* [LSY\({}^{+}\)20] Yuhan Liu, Ananda Theertha Suresh, Felix X. Yu, Sanjiv Kumar, and Michael Riley. Learning discrete distributions: user vs item-level privacy. In _NeurIPS_, 2020.
* [MT07] Frank McSherry and Kunal Talwar. Mechanism design via differential privacy. In _FOCS_, pages 94-103, 2007.
* [NME22] Shyam Narayanan, Vahab S. Mirrokni, and Hossein Esfandiari. Tight and robust private mean estimation with few users. In _ICML_, pages 16383-16412, 2022.
* [RE19] Carey Radebaugh and Ulfar Erlingsson. Introducing TensorFlow Privacy: Learning with Differential Privacy for Training Data, March 2019. blog.tensorflow.org.
* [Tal95] Michel Talagrand. Concentration of measure and isoperimetric inequalities in product spaces. _Publications Mathematiques de l'Institut des Hautes Etudes Scientifiques_, 81:73-205, 1995.
* [TM20] Davide Testugine and Ilya Mironov. PyTorch Differential Privacy Series Part 1: DP-SGD Algorithm Explained, August 2020. medium.com.
* [Vad17] Salil P. Vadhan. The complexity of differential privacy. In _Tutorials on the Foundations of Cryptography_, pages 347-450. Springer International Publishing, 2017.
* [War16] Lutz Warnke. On the method of typical bounded differences. _Comb. Probab. Comput._, 25(2):269-299, 2016.

Summary of Results

Our results for pure-DP are summarized in Table 1.

For approximate-DP, our transformation (Theorem 10) is very general and can be applied to any statistical tasks. Due to this, we do not attempt to exhaustively list its corollaries. Rather, we give only a few examples of consequences of Theorem 10 in Table 2. We note that in all cases, we either improve upon previous results or match them up to \(\frac{\log(1/\delta)^{O(1)}}{\varepsilon}\) factor for all values of \(m\).

## Appendix B Additional Preliminaries

In this section, we list some additional material that is required for the remaining proofs.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Problem & Item-Level DP & \multicolumn{2}{c|}{User-Level DP} \\ \cline{3-4}  & (Previous Work) & \multicolumn{2}{c|}{(Previous Work)} & \multicolumn{2}{c|}{(Our Work)} \\ \hline \multirow{3}{*}{PAC Learning} & \(\widetilde{\Theta}\left(\frac{d}{\varepsilon\alpha}\right)\) & \(\widetilde{\Theta}\left(\frac{d}{\varepsilon}\right)\) when \(m\geq\widetilde{\Theta}\left(\frac{d^{2}}{\varepsilon^{2}\alpha^{2}}\right)\) & \(\widetilde{\Theta}\left(\frac{d}{\varepsilon}+\frac{d}{\varepsilon\alpha m}\right)\) \\  & [1] & [1] & [1] \\ \hline \multirow{3}{*}{\begin{tabular}{c} Agnostic PAC \\ Learning \\ \end{tabular} } & \(\widetilde{\Theta}\left(\frac{d}{\alpha^{2}}+\frac{d}{\varepsilon\alpha}\right)\) & - & \(\widetilde{O}\left(\frac{d}{\varepsilon}+\frac{d}{\varepsilon\alpha\sqrt{m}}+ \frac{d}{\alpha^{2}m}\right)\) \\  & [1] & & Theorem 40 \\ \hline \multirow{3}{*}{\begin{tabular}{c} Learning Discrete \\ Distribution \\ \end{tabular} } & \(\widetilde{\Theta}\left(\frac{k}{\varepsilon\alpha}+\frac{k}{\alpha^{2}}\right)\) & - & \(\widetilde{\Theta}\left(\frac{k}{\varepsilon}+\frac{k}{\varepsilon\alpha\sqrt{ m}}+\frac{k}{\alpha^{2}m}\right)\) \\  & [1] & & Theorem 43 \\ \hline \multirow{3}{*}{\begin{tabular}{c} Learning Product \\ Distribution \\ \end{tabular} } & \(\widetilde{\Theta}\left(\frac{kd}{\varepsilon\alpha}+\frac{kd}{\alpha^{2}}\right)\) & - & \(\widetilde{\Theta}\left(\frac{kd}{\varepsilon}+\frac{kd}{\varepsilon\alpha \sqrt{m}}+\frac{kd}{\alpha^{2}m}\right)\) \\  & [1] & & Theorem 44 \\ \hline \multirow{3}{*}{\begin{tabular}{c} Learning Gaussian \\ (Known Covariance) \\ \end{tabular} } & \(\widetilde{\Theta}\left(\frac{d}{\alpha}+\frac{d}{\alpha^{2}}\right)\) & - & \(\widetilde{\Theta}\left(\frac{d}{\varepsilon}+\frac{d}{\varepsilon\alpha\sqrt{ m}}+\frac{d}{\alpha^{2}m}\right)\) \\  & [1] & & Theorem 45 \\ \hline \multirow{3}{*}{
\begin{tabular}{c} Learning Gaussian \\ (Bounded Cov.) \\ \end{tabular} } & \(\widetilde{\Theta}\left(\frac{d^{2}}{\varepsilon\alpha}+\frac{d^{2}}{\alpha^{2}}\right)\) & - & \(\widetilde{\Theta}\left(\frac{d^{2}}{\varepsilon}+\frac{d^{2}}{\varepsilon\alpha \sqrt{m}}+\frac{d^{2}}{\alpha^{2}m}\right)\) \\  & [1] & & Theorem 46 \\ \hline \end{tabular}
\end{table}
Table 1: **Summary of results on pure-DP.** For (agnostic) PAC learning, \(d\) denotes the probabilistic representation dimension (Definition 20) of the concept class. To the best of our knowledge, none of the distribution learning problems has been studied explicitly under pure user-level DP in previous work, although some bounds can be derived using previous techniques for approximate-DP.

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Problem & Item-Level DP & \multicolumn{2}{c|}{User-Level DP} \\ \cline{3-4}  & (Previous Work) & \multicolumn{2}{c|}{(Previous Work)} & \multicolumn{2}{c|}{(Our Work)} \\ \hline \multirow{3}{*}{PAC Learning} & \(\widetilde{O}\left(\frac{d^{2}_{\varepsilon}}{\varepsilon\alpha^{2}}\right)\) & \(\widetilde{O}\left(\frac{1}{\varepsilon}\right)\) when \(m\geq\widetilde{\Theta}\left(\frac{d^{2}_{\varepsilon}}{\varepsilon^{2}\alpha^{2} }\right)\) & \(\widetilde{O}\left(\frac{1}{\varepsilon}+\frac{d^{2}_{\varepsilon}}{\varepsilon^{2} \alpha^{2}\sqrt{m}}\right)\) \\  & [1] & [1] & [1] \\ \hline \multirow{3}{*}{\begin{tabular}{c} Learning Discrete \\ Distribution \\ \end{tabular} } & \(\widetilde{\Theta}\left(\frac{k}{\varepsilon\alpha}+\frac{k}{\alpha^{2}}\right)\) & \(\widetilde{\Theta}\left(\frac{1}{\varepsilon}+\frac{k}{\varepsilon\alpha\sqrt{ m}}+\frac{k}{\alpha^{2}m}\right)\) & \(\widetilde{O}\left(\frac{1}{\varepsilon}+\frac{k}{\varepsilon^{2}\alpha\sqrt{m}}+ \frac{k}{\alpha^{2}m}\right)\) \\  & [1] & [1] \\ \hline \multirow{3}{*}{
\begin{tabular}{c} Learning Product \\ Distribution \\ \end{tabular} } & \(\widetilde{O}\left(\frac{kd}{\varepsilon\alpha}+\frac{kd}{\alpha^{2}}\right)\) & - & \(\widetilde{O}\left(\frac{1}{\varepsilon}+\frac{kd}{\varepsilon^{2}\alpha\sqrt{m}}+ \frac{kd}{\alpha^{2}m}\right)\) \\  & [1] & & Theorem 10 \\ \hline \end{tabular}
\end{table}
Table 2: **Example results on approximate-DP.** Each of our results is a combination of Theorem 10 and the previously known item-level DP bound. For PAC learning, \(d_{L}\) denotes the Littestone’s dimension of the concept class. We assume that \(\delta\ll\varepsilon\) and use \(\widetilde{O},\widetilde{\Theta}\) to also hide polylogarithmic factors in \(1/\delta\).

For \(a,b\in\mathbb{R}\) such that \(a\leq b\), let \(\operatorname{clip}_{a,b}:\mathbb{R}\to\mathbb{R}\) denote the function

\[\operatorname{clip}_{a,b}(x)=\left\{\begin{aligned} & a&\text{ if }x<a,\\ & b&\text{ if }x>b,\\ & x&\text{ otherwise.}\end{aligned}\right.\]

For two distributions \(A,B\), we use \(d_{\operatorname{tv}}(A,B):=\frac{1}{2}\sum_{x\in\operatorname{supp}(A) \cup\operatorname{supp}(B)}|A(x)-B(x)|\) to denote their total variation distance, \(d_{\operatorname{KL}}(A\parallel B):=\sum_{x\in\operatorname{supp}(A)}A(x) \cdot\log(A(x)/B(x))\) to denote their Kullback-Leibler divergence, and \(d_{\chi^{2}}(A\parallel B):=\sum_{x\in\operatorname{supp}(A)}\frac{(A(x)-B(x) )^{2}}{B(x)}\) to denote the \(\chi^{2}\)-divergence. We use \(A^{k}\) to denote the distribution of \(k\) independent samples drawn from \(A\). The following three well-known facts will be useful in our proofs.

**Lemma 23**.: _For any two distributions \(A\) and \(B\),_

1. _(Pinsker's inequality)_ \(d_{\operatorname{tv}}(A,B)\leq\sqrt{\frac{1}{2}\cdot d_{\operatorname{KL}}(A \parallel B)}\)_._
2. \(d_{\operatorname{KL}}(A^{k}\parallel B^{k})=k\cdot d_{\operatorname{KL}}(A \parallel B)\) _for_ \(k\in\mathbb{N}\)_._
3. \(d_{\operatorname{KL}}(A\parallel B)\leq d_{\chi^{2}}(A\parallel B)\)_._

We use \(\|u\|_{0}\) to denote the _Hamming "norm"_ of \(u\), i.e., the number of non-zero coordinates of \(u\). The following is a well-known fact (aka _constant-rate constant-distance error correcting codes_), which will be useful in our lower bound proofs.

**Theorem 24**.: _For \(d\in\mathbb{N}\), there are \(u_{1},\dots,u_{W}\in\{0,1\}^{d}\) for \(W=2^{\Omega(d)}\) such that \(\|u_{i}-u_{j}\|_{0}\geq\Omega(d)\) for all distinct \(i,j\in[W]\)._

### Concentration Inequalities

Next, we list a few concentration inequalities in convenient forms for subsequent proofs.

Chernoff bound.We start with the standard Chernoff bound.

**Lemma 25** (Chernoff Bound).: _Let \(X_{1},\dots,X_{n}\) denote i.i.d. Bernoulli random variables with success probability \(p\). Then, for all \(\delta>0\), we have_

\[\Pr[X_{1}+\dots+X_{n}\leq(1-\delta)pn]\leq\exp\left(\frac{-\delta^{2}pn}{2} \right),\]

_and,_

\[\Pr[X_{1}+\dots+X_{n}\geq(1+\delta)pn]\leq\exp\left(\frac{-\delta^{2}pn}{2+ \delta}\right).\]

McDiarmid's inequality.For a distribution \(\mathcal{D}\) on \(\mathcal{Z}\), and a function \(g:\mathcal{Z}^{n}\to\mathbb{R}\), let \(\mu_{\mathcal{D}^{n}}(g):=\mathbb{E}_{\bm{x}\sim\mathcal{D}^{n}}[g(\bm{x})]\). We say that \(g\) is a _\(c\)-bounded difference_ function iff \(|g(\bm{x})-g(\bm{x}^{\prime})|\leq c\) for all \(\bm{x},\bm{x}^{\prime}\in\mathcal{Z}^{n}\) that differ on a single coordinate (i.e., \(\|\bm{x}-\bm{x}^{\prime}\|_{0}=1\)).

**Lemma 26** (McDiarmid's inequality).: _Let \(g:\mathcal{Z}^{n}\to\mathbb{R}\) be any \(c\)-bounded difference function. Then, for all \(t>0\)_

\[\Pr[|g(\bm{x})-\mu_{\mathcal{D}^{n}}(g)|>t]\leq 2\exp\left(\frac{-2t^{2}}{nc^{2 }}\right).\]

We will also use the following consequence of Azuma's inequality.

**Lemma 27**.: _Let \(C_{1},\dots,C_{n}\) be sequence of random variables. Suppose that \(|C_{i}|\leq\alpha\) almost surely and that \(\gamma\leq\mathbb{E}[C_{i}\mid C_{1}=c_{1},\dots,C_{i-1}=c_{i-1}]\leq 0\) for any \(c_{1},\dots,c_{i-1}\). Then, for every \(G>0\), we have \(\Pr\left[n\gamma-\alpha\sqrt{nG}\leq\sum_{i\in[n]}C_{i}\leq\alpha\sqrt{nG} \right]\geq 1-2\exp\left(-G/2\right)\)._

### Observations on Item-level vs User-level DP

Here we list a couple of trivial observations on the user / sample complexity in the item vs user-level DP settings.

**Observation 28**.: _Let \(\mathfrak{T}\) be any task and \(\gamma>0\) be a parameter. Then, for any \(\varepsilon,\delta\geq 0\), and \(m\in\mathbb{N}\), it holds that_

1. \(n_{m}^{\mathfrak{T}}(\varepsilon,\delta;\gamma)\leq n_{1}^{\mathfrak{T}}( \varepsilon,\delta;\gamma)\)_, and_
2. \(n_{m}^{\mathfrak{T}}(\varepsilon,\delta;\gamma)\geq\lceil n_{1}^{\mathfrak{T }}(\varepsilon,\delta;\gamma)/m\rceil\)_._

Proof.: Both parts follow by simple reductions as given below.

1. Let \(\mathbb{A}\) be an \((\varepsilon,\delta)\)-item-level DP \(\gamma\)-useful algorithm for \(\mathfrak{T}\). Then, in the user-level DP algorithm, each user can simply throw away all-but-one example and run \(\mathbb{A}\). Clearly, this algorithm is \((\varepsilon,\delta)\)-user-level DP and is \(\gamma\)-useful for \(\mathfrak{T}\). The user complexity remains the same.
2. Let \(\mathbb{A}\) be an \((\varepsilon,\delta)\)-user-level DP \(\gamma\)-useful algorithm for \(\mathfrak{T}\) with user complexity \(n_{m}^{\mathfrak{T}}(\varepsilon,\delta;\gamma)\). Then, in the item-level DP algorithm, if there are \(N=m\cdot n_{m}^{\mathfrak{T}}(\varepsilon,\delta;\gamma)\) users, we can just run \(\mathbb{A}\) by grouping each \(m\) examples together into a "super-user". Clearly, this algorithm is also \((\varepsilon,\delta)\)-item-level DP and has user complexity \(m\cdot n_{m}^{\mathfrak{T}}(\varepsilon,\delta;\gamma)\). Thus, we have \(m\cdot n_{m}^{\mathfrak{T}}(\varepsilon,\delta;\gamma)\leq n_{1}^{\mathfrak{ T}}(\varepsilon,\delta;\gamma)\). 

## Appendix C Missing Details from Section 3

### DP Implies Perfect Generalization

We start by recalling the definition of _perfectly generalizing_ algorithms.

**Definition 29** (Perfect generalization [16, 15]).: For \(\beta,\varepsilon,\delta>0\), an algorithm \(\mathbb{A}:\mathcal{Z}^{n}\to\mathcal{O}\) is said to be \((\beta,\varepsilon,\delta)\)_-perfectly generalizing_ iff, for any distribution \(\mathcal{D}\) (over \(\mathcal{Z}\)), there exists a distribution \(\mathrm{SIM}_{\mathcal{D}}\) such that

\[\Pr_{\bm{x}\sim\mathcal{D}^{n}}[\mathbb{A}(\bm{x})\approx_{\varepsilon, \delta}\mathrm{SIM}_{\mathcal{D}}]\geq 1-\beta.\]

It is known that perfect generalization and sample perfect generalization (Definition 12) are equivalent:

**Lemma 30** ([16, 15]).: _Any \((\beta,\varepsilon,\delta)\)-perfectly generalizing algorithm is also \((\beta,2\varepsilon,3\delta)\)-sample perfectly generalizing. Any \((\beta,\varepsilon,\delta)\)-sample perfectly generalizing algorithm is also \((\sqrt{\beta},\varepsilon,\delta+\sqrt{\beta})\)-perfectly generalizing; furthermore, this holds even when we set \(\mathrm{SIM}_{\mathcal{D}}\) to be the distribution of \(\mathbb{A}(\bm{x})\) where \(\bm{x}\sim\mathcal{D}^{m}\)._

Thus, by plugging the above into Theorem 13, we immediately arrive at the following theorem, which shows that any \((\varepsilon,\delta)\)-DP algorithm is \((\beta,\widetilde{O}(\varepsilon\sqrt{n}),O(n\delta))\)-perfectly generalizing. This resolves an open question of [16].

**Theorem 31** (DP implies perfect generalization).: _Suppose that \(\mathbb{A}\) is an \((\varepsilon,\delta)\)-item-level DP algorithm. Further, assume that \(\varepsilon,\delta,\beta\), and \(\varepsilon\sqrt{n\log(\nicefrac{{1}}{{\beta\delta}})}\) are sufficiently small. Then, \(\mathbb{A}\) is \((\beta,\varepsilon^{\prime},\delta^{\prime})\)-perfectly generalizing, where \(\varepsilon^{\prime}\leq O(\varepsilon\sqrt{n\log(\nicefrac{{1}}{{\beta \delta}})})\) and \(\delta^{\prime}=O(n\delta)\)._

_Furthermore, this holds even when we set \(\mathrm{SIM}_{\mathcal{D}}\) to be the distribution of \(\mathbb{A}(\bm{x})\) where \(\bm{x}\sim\mathcal{D}^{n}\)._

We remark that Bun et al. [15] showed that any \((\varepsilon,\delta)\)-DP algorithm can be "repackaged" to an \((\beta,\varepsilon^{\prime},\delta^{\prime})\)-perfectly generalizing algorithm with similar parameters as our theorem above. However, unlike our result, their proof does _not_ show that the original algorithm is perfectly generalizing.

### Achieving the Same \(\bm{\varepsilon}\) via Subsampling: Proof of Theorem 11

Proof of Theorem 11.: Let \(\varepsilon^{\prime\prime}~{}=~{}\frac{\varepsilon^{2}}{\log(1/\delta)\sqrt{m \log(m/\delta)}}\). We claim that \(n_{1}\left(\varepsilon^{\prime\prime},\delta^{\prime}\right)\leq n_{1}( \varepsilon,\delta^{\prime})\cdot O(\varepsilon/\varepsilon^{\prime\prime})\); the lemma follows from this claim by simply plugging this bound into Theorem 10.

To see that this claim holds, by definition of \(n_{1}\), there exists an \((\varepsilon,\delta^{\prime})\)-item-level DP algorithm \(\mathbb{A}_{0}\) that is \(\gamma\)-useful for \(\mathfrak{T}\) with sample complexity \(n_{1}(\varepsilon,\delta^{\prime})\). Let \(N\in\mathbb{N}\) be a smallest number such that

\[\left(e^{\varepsilon}-1\right)\frac{n_{1}(\varepsilon,\delta^{\prime})}{N} \leq e^{\varepsilon^{\prime\prime}}-1.\]Notice that we have \(N\leq n_{1}(\varepsilon,\delta^{\prime})\cdot O(\varepsilon/\varepsilon^{\prime \prime})\). Let \(\mathbb{A}\) be an algorithm on \(N\) samples defined as follows: randomly sample (without replacement) \(n_{1}(\varepsilon,\delta^{\prime})\) out of \(N\) samples, then run \(\mathbb{A}_{0}\) on this subsample. By the amplification-by-subsampling theorem (Theorem 7), we have that \(\mathbb{A}\) is \((\varepsilon^{\prime\prime},\delta^{\prime})\)-item-level DP. It is also obvious that the output distribution of this algorithm is the same as that of \(\mathbb{A}_{0}\) on \(n_{1}(\varepsilon,\delta^{\prime})\) examples, and this algorithm is thus also \(\gamma\)-useful for \(\mathfrak{T}\). Thus, we have \(n_{1}\left(\varepsilon^{\prime\prime},\delta^{\prime}\right)\leq N\leq n_{1} (\varepsilon,\delta^{\prime})\cdot O(\varepsilon/\varepsilon^{\prime\prime})\) as desired. 

### Proof of Lemma 14

#### c.3.1 "Robust" McDiarmid Inequality for Almost-Multiplicative-Lipschitz function

We derive the following concentration inequality, which may be viewed as a strengthened McDiarmid's inequality (Lemma 26) for the multiplicative-Lipschitz case. To state this, let us define an additional notation: for \(\bm{x}\in\mathcal{Z}^{n},\bar{x}_{i}\in\mathcal{Z}\), we write \(\bar{\bm{x}}^{(i)}\) to denote \(\bm{x}\) but with \(x_{i}\) replaced by \(\bar{x}_{i}\).

**Lemma 32**.: _Let \(f:\mathcal{Z}^{n}\to\mathbb{R}_{\geq 0}\) be any function and \(\mathcal{D}\) any distribution on \(\mathcal{Z}\). Assume that \(\varepsilon,\delta>0\), and \(\varepsilon\sqrt{n\log(\nicefrac{{1}}{{\delta}})}>0\) are sufficiently small. Then, for \(\varepsilon^{\prime}=O(\varepsilon\sqrt{n\log(\nicefrac{{1}}{{\delta}})})\), we have_

\[\mathbb{E}_{\bm{x},\bm{x}^{\prime}\sim\mathcal{D}^{n}}[[f(\bm{x})-e^{\varepsilon ^{\prime}}\cdot f(\bm{x}^{\prime})]_{+}]\leq O(\mu_{\mathcal{D}^{n}}(f)\cdot \delta)+O\left(\sum_{i=1}^{n}\mathbb{E}_{\bm{x},\bar{\bm{x}}\sim\mathcal{D}^{n }}\left[f(\bm{x})-e^{\varepsilon}\cdot f(\tilde{\bm{x}}^{(i)})\right]_{+} \right).\]

When the second term in the RHS (i.e., \(O\left(\sum_{i=1}^{n}\mathbb{E}_{\bm{x},\tilde{\bm{x}}\sim\mathcal{D}^{n}} \left[f(\bm{x})-e^{\varepsilon}\cdot f(\tilde{\bm{x}}^{(i)})\right]_{+}\right)\)) is zero, our lemma is (essentially) the same as McDiarmid's inequality for multiplicative-Lipschitz functions, as proved, e.g., in [16]. However, our bound is more robust, in the sense that it can handle the case where the multiplicative-Lipschitzness condition fails sometimes; this is crucial for the proof of Lemma 14 as we only assume approximate-DP.

We note that, while there are (additive) McDiarmid's inequalities that does not require Lipschitzness everywhere [14, 15], we are not aware of any version that works for us due to the regime of parameters we are in and the assumptions we have. This is why we prove Lemma 32 from first principles. The remainder of this subsection is devoted to the proof of Lemma 32.

Scalar Random Variable Adjustment for Bounded Ratio.For the rest of this section, we write \(z\approx_{\varepsilon}z^{\prime}\) where \(z,z^{\prime}\in\mathbb{R}_{\geq 0}\) to indicate that \(z\in[e^{-\varepsilon}z^{\prime},e^{\varepsilon}z^{\prime}]\). We extend the notion similarly to \(\mathbb{R}_{\geq 0}\)-valued random variables: for a \(\mathbb{R}_{\geq 0}\)-valued random variable \(Z\) and \(z\in\mathbb{R}_{\geq 0}\), we write \(Z\approx_{\varepsilon}z^{\prime}\) if \(z\approx_{\varepsilon}z^{\prime}\) for each \(z\in\operatorname{supp}(Z)\). Furthermore, for (possibly dependent) \(\mathbb{R}_{\geq 0}\)-valued random variables \(Z,Z^{\prime}\), we write \(Z\approx_{\varepsilon}Z^{\prime}\) to denote \(z\approx_{\varepsilon}z^{\prime}\) for all \((z,z^{\prime})\in\operatorname{supp}((Z,Z^{\prime}))\).

Suppose \(Z\) is a \(\mathbb{R}_{\geq 0}\)-valued random variable such that any two values \(z,\tilde{z}\in\operatorname{supp}(Z)\) satisfies \(z\approx_{\varepsilon}\tilde{z}\). Then, we also immediately have that their mean \(\mu\) is in this range and thus \(Z\approx_{\varepsilon}\mu\). We start by showing a "robust" version of this statement, which asserts that, even if the former condition fails sometimes, we can still "move" the random variable a little bit so that the second condition holds (albeit with weaker \(2\varepsilon\) bound). The exact statement is presented below.

**Lemma 33**.: _Let \(\varepsilon,\mu^{*}\geq 0\). Let \(Z\) be any \(\mathbb{R}_{\geq 0}\)-valued random variable and \(\mu:=\mathbb{E}[Z]\). Then, there exists a random variable \(Z^{\prime}\), which is a post-processing of \(Z\), such that_

1. \(Z^{\prime}\approx_{2\varepsilon}\mu^{*}\)_._
2. \(\mathbb{E}[Z^{\prime}]=\mu^{*}\)_._
3. \(\mathbb{E}_{(Z,Z^{\prime})}[\|Z-Z^{\prime}\|]\leq|\mu-\mu^{*}|+2(1+e^{- \varepsilon})\cdot\mathbb{E}_{Z,\tilde{Z}}\left[\left[Z-e^{\varepsilon}\cdot \tilde{Z}\right]_{+}\right]\)_, where_ \((Z,Z^{\prime})\) _is the canonical coupling between_ \(Z,Z^{\prime}\) _(from post-processing) and_ \(\tilde{Z}\) _is an i.i.d. copy of_ \(Z\)_._

Proof.: Let \(\ell=e^{-\varepsilon}\cdot\mu,r=e^{\varepsilon}\cdot\mu\). We start by defining \(\hat{Z}:=\operatorname{clip}_{\ell,r}(Z)\). This also gives a canonical coupling between \(\hat{Z}\) and \(Z\), which then yields

\[\mathbb{E}_{(Z,\hat{Z})}[\|Z-\hat{Z}\|] =\mathbb{E}_{Z}\left[\left[Z-\operatorname{clip}_{\ell,r}(Z) \right]\right]\] \[=\mathbb{E}_{Z}\left[\left[Z-r\right]_{+}+\left[\ell-Z\right]_{+}\right]\] \[=\mathbb{E}_{Z}\left[\left[Z-e^{\varepsilon}\cdot\mu\right]_{+} \right]+\mathbb{E}_{Z}\left[\left[e^{-\varepsilon}\cdot\mu-Z\right]_{+}\right]\]

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_FAIL:20]

Plugging this back to (8), we get

\[\Pr[\mathcal{A}(\bm{x})\in U_{0}]\] \[\leq\overline{\delta}+e^{\overline{\varepsilon}}\cdot\sum_{r_{1}=0 }^{2\kappa-1}\left(e^{2\overline{\varepsilon}}\Pr[\mathcal{A}(\bm{x}^{\prime}) \in U_{0}\mid R_{1}=r_{1}+1]+(e^{\overline{\varepsilon}}+1)\overline{\delta} \right)\cdot\Pr[R_{1}=r_{1}+1]\] \[\leq(e^{2\overline{\varepsilon}}+e^{\overline{\varepsilon}}+1) \overline{\delta}+e^{3\overline{\varepsilon}}\cdot\sum_{r_{1}=0}^{2\kappa} \Pr[\mathcal{A}(\bm{x}^{\prime})\in U_{0}\mid R_{1}=r_{1}]\cdot\Pr[R_{1}=r_{ 1}]\] \[=(e^{2\overline{\varepsilon}}+e^{\overline{\varepsilon}}+1) \overline{\delta}+e^{3\overline{\varepsilon}}\cdot\Pr[\mathcal{A}(\bm{x}^{ \prime})\in U_{0}].\] (9)

Now, consider any set \(U\subseteq\mathcal{O}\cup\{\bot\}\) of outcomes. Let \(U_{\mathcal{O}}=U\cap\mathcal{O}\) and \(U_{\bot}=U\cap\{\bot\}\). Then, we have

\[\Pr[\mathcal{A}(\bm{x})\in U] =\Pr[\mathcal{A}(\bm{x})\in U_{\mathcal{O}}]+\Pr[\mathcal{A}( \bm{x})\in U_{\bot}]\] \[\stackrel{{\eqref{eq:U_T_T_T_T_T_T_T_T_T}}}{{\leq}} \left((e^{2\overline{\varepsilon}}+e^{\overline{\varepsilon}}+1)\overline{ \delta}+e^{3\overline{\varepsilon}}\cdot\Pr[\mathbb{A}(\bm{x}^{\prime})\in U_{ \mathcal{O}}]\right)+\left(\overline{\delta}+e^{\overline{\varepsilon}}\cdot \Pr[\mathcal{A}(\bm{x}^{\prime})=U_{\bot}]\right)\] \[\leq(e^{2\overline{\varepsilon}}+e^{\overline{\varepsilon}}+2) \overline{\delta}+e^{3\overline{\varepsilon}}\Pr[\mathcal{A}(\bm{x}^{\prime}) \in U]\] \[\leq\delta+e^{\varepsilon}\cdot\Pr[\mathcal{A}(\bm{x}^{\prime}) \in U].\]

Therefore, the algorithm is \((\varepsilon,\delta)\)-user-level DP as desired. 

## Appendix D Missing Details from Section 4

Below we give the proof of Lemma 22.

Proof of Lemma 22.: Consider any \(h\in\{0,1\}^{\mathcal{X}}\). Since \(\bm{z}_{i}\)'s are drawn independently from \(\mathcal{D}\), we have that \(\bm{1}[\bm{z}_{i}\) is not realizable by \(h]\)'s are independent Bernoulli random variables with success probability

\[p_{h}=1-(1-\operatorname{err}_{\mathcal{D}}(h))^{m}.\]

Consider the two cases:

* If \(\operatorname{err}_{\mathcal{D}}(h)\leq 0.01\alpha\), then we have \[p_{h}\leq 1-(1-0.01\alpha)^{m}\leq 0.01\alpha m.\] Thus, applying Lemma 25, we can conclude that \[\Pr[\operatorname{scr}_{h}(\bm{z})\geq 0.05\alpha nm]\leq\exp\left(-\Theta \left(n\alpha m\right)\right)=\exp\left(-\Theta\left(\kappa\cdot\operatorname{ size}(\mathcal{P})\right)\right).\] Thus, when we select \(\kappa\) to be sufficiently large, we have \[\Pr[\operatorname{scr}_{h}(\bm{z})\geq 0.05\alpha nm]\leq\exp(-10\operatorname{ size}(\mathcal{P}))\leq\frac{0.01}{\exp(\operatorname{size}(\mathcal{P}))}\leq\frac{0.01}{ |\mathcal{H}|}.\]
* If \(\operatorname{err}_{\mathcal{D}}(h)>\alpha\), then we have \[p_{h}\geq 1-(1-\alpha)^{m}\geq 1-e^{-\alpha m}\geq 0.2\alpha m,\] where in the second inequality we use the fact that \(\alpha m\leq 1\). Again, applying Lemma 25, we can conclude that \[\Pr[\operatorname{scr}_{h}(\bm{z})\leq 0.2\alpha nm]\leq\exp\left(-\Theta \left(n\alpha m\right)\right)=\exp\left(-\Theta\left(\kappa\cdot\operatorname{ size}(\mathcal{P})\right)\right).\] Thus, when we select \(\kappa\) to be sufficiently large, we have \[\Pr[\operatorname{scr}_{h}(\bm{z})\leq 0.2\alpha nm]\leq\exp(-10\operatorname{ size}(\mathcal{P}))\leq\frac{0.01}{\exp(\operatorname{size}(\mathcal{P}))}\leq\frac{0.01}{| \mathcal{H}|}.\] Finally, taking a union bound over all \(h\in\mathcal{H}\) yields the desired result. 

It is also worth noting that the scoring function can be written in the form:

\[\operatorname{scr}_{h}(\bm{z}):=\sum_{i\in[n]}\operatorname{clip}_{0,1}\left( \left|\{(x,y)\in\bm{z}_{i}\mid h(x)\neq y\}\right|\right),\]

which is similar to what we present below for other tasks.

Finally, we remark that the standard techniques can boost the success probability from 2/3 to an arbitrary probability \(1-\beta\), with a multiplicative factor of \(\log(1/\beta)\) cost.

Additional Results for Pure-DP

### Algorithm: Pairwise Score Exponential Mechanism

In this section, we give a user-level DP version of _pairwise score_ exponential mechanism (EM) [1]. In the proceeding subsections, we will show how to apply this in multiple settings, including agnostic learning, private hypothesis selection, and distribution learning.

Suppose there is a candidate set \(\mathcal{H}\). Furthermore, for every pair \(H,H^{\prime}\in\mathcal{H}\) of candidates, there is a "comparison" function \(\psi_{H,H^{\prime}}:\mathcal{Z}\rightarrow[-1,1]\). For every pair \(H,H^{\prime}\) of candidates and any \(\bm{x}\in\mathcal{Z}^{*}\), let

\[\mathrm{pscr}^{\psi}_{H,H^{\prime}}(\bm{x})=\sum_{x\in\bm{x}}\psi_{H,H^{\prime }}(x).\]

We then define \(\mathrm{scr}^{\psi}\) for each candidate \(H\) as

\[\mathrm{scr}^{\psi}_{H}(\bm{x}):=\max_{H^{\prime}\in\mathcal{H}\smallsetminus \{H\}}\mathrm{pscr}^{\psi}_{H,H^{\prime}}(\bm{x}).\]

Traditionally, the item-level DP algorithm for pairwise scoring function is to use EM on the above scoring function \(\mathrm{scr}^{\psi}_{H}\) (where \(\bm{x}\) denote the entire input) [1]; it is not hard to see that the \(\mathrm{scr}^{\psi}_{H}\) has (item-level) sensitivity of at most \(2\).

User-Level DP Algorithm.We now describe our user-level DP algorithm \(\mathsf{ClippedPairwiseEM}\). The algorithm computes:

\[\mathrm{pscr}^{\psi,\tau}_{H,H^{\prime}}(\bm{x}) :=\sum_{i\in[n]}\mathrm{clip}_{-\tau,\tau}\left(\mathrm{pscr}^{ \psi}_{H,H^{\prime}}(\bm{x}_{i})\right),\] \[\mathrm{scr}^{\psi,\tau}_{H}(\bm{x}) :=\max_{H^{\prime}\in\mathcal{H}\smallsetminus\{H\}}\mathrm{pscr}^{ \psi,\tau}_{H,H^{\prime}}(\bm{x}).\]

Then, it simply runs EM using the score \(\mathrm{scr}^{\psi,\tau}_{H}\) with sensitivity \(\Delta=2\tau\).

To state the guarantee of the algorithm, we need several additional distributional-based definitions of the score:

\[\mathrm{pscr}^{\psi}_{H,H^{\prime}}(\mathcal{D}) :=\mathbb{E}_{x\sim\mathcal{D}}\left[\psi_{H,H^{\prime}}(x)\right],\] \[\mathrm{scr}^{\psi}_{H}(\mathcal{D}) :=\max_{H^{\prime}\in\mathcal{H}\smallsetminus\{H\}}\mathrm{pscr}^ {\psi}_{H,H^{\prime}}(\mathcal{D}).\]

Note that these distributional scores are normalized, so they are in \([-1,1]\).

The guarantee of our algorithm can now be stated as follows:

**Theorem 37**.: _Let \(\varepsilon,\alpha,\beta\in(0,1/2]\). Assume further that there exists \(H^{*}\) such that \(\mathrm{scr}^{\psi}_{H^{*}}(\mathcal{D})\leq 0.1\alpha\). There exists an \(\varepsilon\)-DP algorithm that with probability \(1-\beta\) (over the randomness of the algorithm and the input \(\bm{x}\sim(\mathcal{D}^{m})^{n}\)) outputs \(H\in\mathcal{H}\) such that \(\mathrm{scr}^{\psi}_{H}(\mathcal{D})\leq 0.5\alpha\) as long as_

\[n\geq\tilde{\Theta}\left(\frac{\log(|\mathcal{H}|/\beta)}{\varepsilon}+\frac{ \log(|\mathcal{H}|/\beta)}{\alpha\varepsilon\sqrt{m}}+\frac{\log(|\mathcal{H}| /\beta)}{\alpha^{2}m}\right).\]

Proof.: We use \(\mathsf{ClippedPairwiseEM}\) with parameter \(\tau=C\left(\alpha m+\sqrt{m\log\left(1/\alpha\right)}\right)\) and assume that \(n\geq C^{\prime}\left(\frac{\tau\cdot\log(|\mathcal{H}|/\beta)}{\varepsilon \alpha m}+\frac{\log(|\mathcal{H}|/\beta)}{\alpha^{2}m}\right)=\tilde{\Theta} \left(\frac{\log(|\mathcal{H}|/\beta)}{\varepsilon}+\frac{\log(|\mathcal{H}|/ \beta)}{\alpha\varepsilon\sqrt{m}}+\frac{\log(|\mathcal{H}|/\beta)}{\alpha^ {2}m}\right)\), where \(C,C^{\prime}\) are sufficiently large constants. The privacy guarantee follows directly from that of EM, since the (user-level) sensitivity is at most \(\Delta\) (due to clipping).

To analyze the utility, we will consider following three events:

* \(\mathcal{E}_{1}\): \(\mathrm{scr}^{\psi,\tau}_{H}(\bm{x})\leq\mathrm{scr}^{\psi,\tau}_{H^{*}}(\bm {x})+0.1\alpha nm\).
* \(\mathcal{E}_{2}\): for all \(H,H^{\prime}\in\mathcal{H}\): if \(\mathrm{pscr}^{\psi}_{H,H^{\prime}}(\mathcal{D})\leq 0.1\alpha\), then \(\mathrm{pscr}^{\psi}_{H,H^{\prime}}(\bm{x})\leq 0.2\alpha nm\).
* \(\mathcal{E}_{3}\): for all \(H,H^{\prime}\in\mathcal{H}\): if \(\mathrm{pscr}^{\psi}_{H,H^{\prime}}(\mathcal{D})\geq 0.5\alpha\), then \(\mathrm{pscr}^{\psi}_{H,H^{\prime}}(\bm{x})\geq 0.4\alpha nm\).

When all events hold, \(\mathcal{E}_{1}\) implies that \(\operatorname{scr}_{H}^{\psi,\tau}(\bm{x})\leq\operatorname{scr}_{H^{\tau}}^{ \psi,\tau}(\bm{x})+0.1\alpha nm\). Observe that \(\mathcal{E}_{2}\) implies that \(\operatorname{scr}_{H^{\tau}}^{\psi,\tau}(\bm{x})\leq 0.2\alpha nm\). Combining these two inequalities, we get \(\operatorname{scr}_{H}^{\psi,\tau}(\bm{x})\leq 0.3\alpha nm\); then, \(\mathcal{E}_{3}\) implies that \(\operatorname{pscr}_{H,H^{\prime}}^{\psi}(\mathcal{D})\leq 0.5\alpha\) for all \(H^{\prime}\in\mathcal{D}\). In turn, this means that \(\operatorname{scr}_{H}^{\psi}(\mathcal{D})\leq 0.5\alpha\).

Therefore, it suffices to show that each of \(\Pr[\mathcal{E}_{1}],\Pr[\mathcal{E}_{2}],\Pr[\mathcal{E}_{3}]\) is at least \(1-\beta/3\).

**Bounding \(\Pr[\mathcal{E}_{1}]\).** To bound the probability of \(\mathcal{E}_{1}\), recall from Theorem 9 that, with probability \(1-\beta/3\), the guarantee of EM ensures that

\[\operatorname{scr}_{H}^{\psi,\tau}(\bm{x})\geq\operatorname{scr}_{H^{\star} }^{\psi,\tau}(\bm{x})+O\left(\frac{\log(|\mathcal{H}|/\beta)}{\varepsilon} \right)\cdot\Delta.\]

From our assumption on \(n\), when \(C^{\prime}\) is sufficiently large, then we have \(O\left(\frac{\log(|\mathcal{H}|/\beta)}{\varepsilon}\right)\cdot\Delta=O\left( \frac{\log(|\mathcal{H}|/\beta)}{\varepsilon}\cdot\tau\right)\leq 0.1\alpha nm\) as desired.

**Bounding \(\Pr[\mathcal{E}_{2}]\) and \(\Pr[\mathcal{E}_{3}]\).** Since the proofs for both cases are analogous, we only show the full argument for \(\Pr[\mathcal{E}_{3}]\). Let us fix \(H,H^{\prime}\in\mathcal{H}\) such that \(\operatorname{pscr}_{H,H^{\prime}}^{\psi}(\mathcal{D})\geq 0.5\alpha\). We may assume w.l.o.g. that12\(\operatorname{pscr}_{H,H^{\prime}}^{\psi}(\mathcal{D})=0.5\alpha:=\mu\).

Footnote 12: Otherwise, we may keep increasing \(\psi_{H,H^{\prime}}(z)\) for different values of \(z\) until \(\operatorname{pscr}_{H,H^{\prime}}^{\psi}(\mathcal{D})=0.5\alpha\); this operation does not decrease the probability that \(\operatorname{pscr}_{H,H^{\prime}}^{\psi}(\bm{x})<0.4\alpha nm\).

Notice that \(\operatorname{pscr}_{H,H^{\prime}}^{\psi,\tau}(\bm{x})\) is a \(2\)-bounded difference function. As a result, McDiarmid's inequality (Lemma 26) implies that

\[\Pr_{\bm{x}\sim\mathcal{D}^{nm}}[|\operatorname{pscr}_{H,H^{\prime}}^{\psi, \tau}(\bm{x})-\mu_{\mathcal{D}^{nm}}(\operatorname{pscr}_{H,H^{\prime}}^{ \psi,\tau})|>0.05\alpha nm]\leq 2\exp\left(-0.025\alpha^{2}nm\right)\leq\frac{ \beta}{3|\mathcal{H}|^{2}},\] (10)

where the second inequality is due to our choice of \(n\) when \(C^{\prime}\) is sufficiently large.

To compute \(\mu_{\mathcal{D}^{nm}}(\operatorname{pscr}_{H,H^{\prime}}^{\psi,\tau})\), observe further that

\[\mu_{\mathcal{D}^{nm}}(\operatorname{pscr}_{H,H^{\prime}}^{\psi,\tau})=n\cdot \mu_{\mathcal{D}^{m}}(\operatorname{clip}_{-\tau,\tau}\circ\operatorname{pscr} _{H,H^{\prime}}^{\psi}).\] (11)

Now observe once again that \(\operatorname{pscr}_{H,H^{\prime}}^{\psi}:\mathcal{Z}^{m}\to\mathbb{R}\) is also a \(2\)-bounded difference function and \(\mu_{\mathcal{D}^{m}}(\operatorname{pscr}_{H,H^{\prime}}^{\psi})=\mu m\). Therefore, McDiarmid's inequality (Lemma 26) yields

\[\Pr_{\bm{x}\sim\mathcal{D}^{m}}[\operatorname{pscr}_{H,H^{\prime}}^{\psi}(\bm{ x})>\tau]\leq 2\exp\left(-0.5(\tau-\mu m)^{2}/m\right)\leq 10^{-4}\alpha^{2},\] (12)

where the second inequality is due to our choice of \(\tau\) when \(C\) is sufficiently large.

Finally, note that

\[\mu_{\mathcal{D}^{m}}(\operatorname{clip}_{-\tau,\tau}\circ \operatorname{pscr}_{H,H^{\prime}}^{\psi})\] \[=\mathbb{E}_{\bm{x}\sim\mathcal{D}^{m}}[\operatorname{clip}_{-\tau,\tau}(\operatorname{pscr}_{H,H^{\prime}}^{\psi}(\bm{x}))]\] \[\geq\mathbb{E}_{\bm{x}\sim\mathcal{D}^{m}}[\operatorname{clip}_{- \infty,\tau}(\operatorname{pscr}_{H,H^{\prime}}^{\psi}(\bm{x}))]\] \[=\mathbb{E}_{\bm{x}\sim\mathcal{D}^{m}}[\operatorname{pscr}_{H,H ^{\prime}}^{\psi}(\bm{x})]+\mathbb{E}_{\bm{x}\sim\mathcal{D}^{m}}[(\tau- \operatorname{pscr}_{H,H^{\prime}}^{\psi}(\bm{x}))\cdot\mathbf{1}[\operatorname {pscr}_{H,H^{\prime}}^{\psi}(\bm{x})>\tau]]\] \[\geq\mu m-\mathbb{E}_{\bm{x}\sim\mathcal{D}^{m}}[\operatorname{pscr }_{H,H^{\prime}}^{\psi}(\bm{x})\cdot\mathbf{1}[\operatorname{pscr}_{H,H^{\prime}} ^{\psi}(\bm{x})>\tau]]\] \[\geq\mu m-\sqrt{\mathbb{E}_{\bm{x}\sim\mathcal{D}^{m}}[ \operatorname{pscr}_{H,H^{\prime}}^{\psi}(\bm{x})^{2}]}\cdot\sqrt{\mathbb{E}_{\bm {x}\sim\mathcal{D}^{m}}[\mathbf{1}[\operatorname{pscr}_{H,H^{\prime}}^{\psi}(\bm {x})>\tau]]}\] \[\geq\mu m-\sqrt{m^{2}}\cdot\sqrt{\Pr_{\bm{x}\sim\mathcal{D}^{m}}[ \operatorname{pscr}_{H,H^{\prime}}^{\psi}(\bm{x})>\tau]}\] \[\stackrel{{\eqref{eq:def_eqCombining (10), (11), and (13), we can conclude that

\[\Pr_{\bm{x}\sim\mathcal{D}^{nm}}[\mathrm{lpscr}_{H,H^{\prime}}^{\psi,\tau}(\bm{x}) <0.4\alpha nm]\leq\frac{\beta}{3|\mathcal{H}|^{2}}.\]

Taking a union bound over \(H,H^{\prime}\in\mathcal{H}\) yields \(\Pr[\mathcal{E}_{3}]\geq 1-\beta/3\) as desired. 

### Lower Bound: User-level DP Fano's Inequality

As we often demonstrate below that our bounds are (nearly) tight, it will be useful to have a generic method for providing such a lower bound. Here we observe that it is simple to extend the DP Fano's inequality of Acharya et al. [1] to the user-level DP setting. We start by recalling Acharya et al.'s (item-level) DP Fano's inequality13:

Footnote 13: The particular version we use is a slight restatement of [1, Corollary 4].

**Theorem 38** (Item-Level DP Fano's Inequality [1]).: _Let \(\mathfrak{T}\) be any task. Suppose that there exist \(\mathcal{D}_{1},\ldots,\mathcal{D}_{W}\) such that, for all distinct \(i,j\in[W]\),_

* \(\Psi_{\mathfrak{T}}(\mathcal{D}_{i})\cap\Psi_{\mathfrak{T}}(\mathcal{D}_{j})=\emptyset\)_,_
* \(d_{\mathrm{KL}}(\mathcal{D}_{i},\mathcal{D}_{j})\leq\beta\)_, and_
* \(d_{\mathrm{tv}}(\mathcal{D}_{i},\mathcal{D}_{j})\leq\gamma\)_._

_Then, \(n_{1}^{\mathfrak{T}}(\varepsilon,\delta=0)\geq\Omega\left(\frac{\log W}{ \gamma\varepsilon}+\frac{\log W}{\beta}\right)\)._

The bound for the user-level case can be derived as follows.

**Lemma 39** (User-Level DP Fano's Inequality).: _Let \(\mathfrak{T}\) be any task. Suppose that there exist \(\mathcal{D}_{1},\ldots,\mathcal{D}_{W}\) such that, for all distinct \(i,j\in[W]\),_

* \(\Psi_{\mathfrak{T}}(\mathcal{D}_{i})\cap\Psi_{\mathfrak{T}}(\mathcal{D}_{j})=\emptyset\)_, and,_
* \(d_{\mathrm{KL}}(\mathcal{D}_{i},\mathcal{D}_{j})\leq\beta\)_._

_Then, \(n_{m}^{\mathfrak{T}}(\varepsilon,\delta=0)\geq\Omega\left(\frac{\log W}{ \varepsilon}+\frac{\log W}{\varepsilon\sqrt{m\beta}}+\frac{\log W}{m\beta}\right)\)._

Proof.: Let \(\mathcal{D}^{\prime}_{i}=(\mathcal{D}_{i})^{m}\) for all \(i\in[W]\); note that \(d_{\mathrm{KL}}(\mathcal{D}^{\prime}_{i}\parallel\mathcal{D}^{\prime}_{j}) \leq m\beta\). By Pinsker's inequality (Lemma 23(i)), \(d_{\mathrm{tv}}(\mathcal{D}^{\prime}_{i}\parallel\mathcal{D}^{\prime}_{j})\leq O (\sqrt{m\beta})\); furthermore, we also have the trivial bound \(d_{\mathrm{tv}}(\mathcal{D}^{\prime}_{i}\parallel\mathcal{D}^{\prime}_{j})\leq 1\). Finally, define a task \(\mathfrak{T}^{\prime}\) by \(\Psi_{\mathfrak{T}^{\prime}}(\mathcal{D}^{\prime}_{i}):=\Psi_{\mathfrak{T}}( \mathcal{D}_{i})\) for all \(i\in[W]\). Now, applying Theorem 38,

\[n_{1}^{\mathfrak{T}^{\prime}}(\varepsilon,\delta=0)\geq\Omega\left(\frac{ \log W}{\min\{1,\sqrt{m\beta}\}\cdot\varepsilon}+\frac{\log W}{m\beta}\right) =\Omega\left(\frac{\log W}{\varepsilon}+\frac{\log W}{\varepsilon\sqrt{m \beta}}+\frac{\log W}{m\beta}\right).\]

Finally, observing that \(n_{m}^{\mathfrak{T}}(\varepsilon,\delta)=n_{1}^{\mathfrak{T}^{\prime}}( \varepsilon,\delta)\) yields our final bound. 

### Applications

Our user-level DP EM with pairwise score given above has a wide variety of applications. We now give a few examples.

#### e.3.1 Agnostic PAC Learning

We start with _agnostic PAC learning_. The setting is exactly the same as in PAC learning (described in the introduction; see Theorem 4) except that we do not assume that \(\mathcal{D}\) is realizable by some concept in \(\mathcal{C}\). Instead, the task \(\mathrm{agn}(\mathcal{C};\alpha)\) seeks an output \(c:\mathcal{X}\to\{0,1\}\) such that \(\mathrm{err}_{\mathcal{D}}(c)\leq\min_{c^{\prime}\in\mathcal{C}}\mathrm{err}_{ \mathcal{D}}(c^{\prime})+\alpha\).

**Theorem 40**.: _Let \(\mathcal{C}\) be any concept class with probabilistic representation dimension \(d\) (i.e., \(\mathrm{PRDim}(\mathcal{C})=d\)) and let \(d_{\mathrm{VC}}\) be its VC dimension. Then, for any sufficiently small \(\alpha,\varepsilon>0\) and for all \(m\in\mathbb{N}\),_

\[n_{m}^{\mathrm{agn}(\mathcal{C};\alpha)}(\varepsilon,\delta=0)\leq\widetilde{O }\left(\frac{d}{\varepsilon}+\frac{d}{\alpha\varepsilon\sqrt{m}}+\frac{d}{ \alpha^{2}m}\right),\]\[n_{m}^{\mathrm{agn}(\mathcal{C};\alpha)}(\varepsilon,\delta=0)\geq\tilde{\Omega} \left(\frac{d}{\varepsilon}+\frac{d_{\mathrm{VC}}}{\alpha\varepsilon\sqrt{m}}+ \frac{d_{\mathrm{VC}}}{\alpha^{2}m}\right).\]

Observe that our bounds are (up to polylogarithmic factors) the same except for the \(d\)-vs-\(d_{\mathrm{VC}}\) in the last two terms. We remark that the ratio \(d/d_{\mathrm{VC}}\) is not always bounded; e.g., for thresholds, \(d_{\mathrm{VC}}=2\) while \(d=\Theta(\log|\mathcal{Z}|)\). A more careful argument can show that \(d\) in the last two terms in the upper bound can be replaced by the (maximum) VC dimension of the probabilistic representation (which is potentially smaller); this actually closes the gap in the particular case of thresholds. However, in the general case, the VC dimension of \(\mathcal{C}\) and the VC dimension of its probabilistic represetation are not necessarily equal. It remains an interesting open question to close this gap.

We now prove Theorem40. The algorithm is similar to that in the proof of Theorem4, except that we use the pairwise scoring function to compare the errors of the two hypotheses.

Proof of Theorem40.: **Algorithm.** For any two hypotheses \(c,c^{\prime}:\mathcal{X}\to\{0,1\}\), let the comparison function between two concepts be \(\psi_{c,c^{\prime}}((x,y))=\mathbf{1}[c(x)=y]-\mathbf{1}[c^{\prime}(x)=y]\).

Let \(\mathcal{P}\) denote a \((0.01\alpha,1/4)\)-PR of \(\mathcal{C}\); by Lemma21, there exists such \(\mathcal{P}\) with \(\mathrm{size}(\mathcal{P})\leq\widetilde{O}\left(d\cdot\log(1/\alpha)\right)\). Our algorithm works as follows: Sample \(\mathcal{H}\sim\mathcal{P}\) and then run the \(\varepsilon\)-DP pairwise scoring EM (Theorem37) on candidate set \(\mathcal{H}\) with the comparison function \(\psi_{c,c^{\prime}}\) defined above. The privacy guarantee follows from that of Theorem37.

As for the utility, first observe that

\[\mathrm{psc}_{c,c^{\prime}}^{\psi}(\mathcal{D})=\mathrm{err}_{ \mathcal{D}}(c)-\mathrm{err}_{\mathcal{D}}(c^{\prime}).\] (14)

This means that, if we let \(h^{*}\in\mathrm{argmin}_{h\in\mathcal{H}}\mathrm{err}_{\mathcal{D}}(h)\), then \(\mathrm{sc}_{h^{*}}^{\psi}(\mathcal{D})\leq 0\). Thus, Theorem37 ensures that, w.p. 0.99, if \(n\geq\tilde{\Theta}\left(\frac{\mathrm{size}(\mathcal{P})}{\varepsilon}+\frac{ \mathrm{size}(\mathcal{P})}{\alpha\varepsilon\sqrt{m}}+\frac{\mathrm{size}( \mathcal{P})}{\alpha^{2}m}\right)=\tilde{\Theta}\left(\frac{d}{\varepsilon}+ \frac{d}{\alpha\varepsilon\sqrt{m}}+\frac{d}{\alpha^{2}m}\right)\), then the output \(h\) satisfies \(\mathrm{scr}_{h}^{\psi}(\mathcal{D})\leq 0.5\alpha\). By (14), this also means that \(\mathrm{err}_{\mathcal{D}}(h)\leq 0.5\alpha+\mathrm{err}_{\mathcal{D}}(h^{*})\). Finally, by the guarantee of \(\mathcal{P}\), we have that \(\mathrm{err}_{\mathcal{D}}(h^{*})\leq 0.01\alpha+\min_{c^{\prime}\in\mathcal{C}} \mathrm{err}_{\mathcal{D}}(c^{\prime})\) with probability 3/4. By a union bound, we can conclude that w.p. more than 2/3, we have \(\mathrm{err}_{\mathcal{D}}(h)<\alpha+\min_{c^{\prime}\in\mathcal{C}}\mathrm{err }_{\mathcal{D}}(c^{\prime})\).

**Lower Bound.** The lower bound \(\Omega(d/\varepsilon)\) was shown in [1] (and holds even for realizable PAC learning). To derive the lower bound \(\Omega\left(\frac{d_{\mathrm{VC}}}{\alpha\varepsilon\sqrt{m}}+\frac{d_{ \mathrm{VC}}}{\alpha^{2}m}\right)\), we will use DP Fano's inequality (Lemma39). To do so, recall that \(\mathcal{C}\) having VC dimension \(d_{\mathrm{VC}}\) means that there exist \(\{x_{1},\ldots,x_{d_{\mathrm{VC}}}\}\) that is shattered by \(\mathcal{Z}\). From Theorem24, there exist \(u_{1},\ldots,u_{W}\in\{0,1\}^{d_{\mathrm{VC}}}\) where \(W=2^{\Omega(d_{\mathrm{VC}})}\) such that \(\|u_{i}-u_{j}\|_{0}\geq\kappa\cdot d_{\mathrm{VC}}\) for some constant \(\kappa\). For any sufficiently small \(\alpha<\kappa/8\), we can define the distribution \(\mathcal{D}_{i}\) for \(i\in[W]\) by

\[\mathcal{D}_{i}((x_{\ell},y))=\frac{1}{2d_{\mathrm{VC}}}\cdot \left(1+\frac{4\alpha}{\kappa}\cdot(2\mathbf{1}[(u_{i})_{\ell}=y]-1)\right),\]

for all \(\ell\in[W]\) and \(y\in\{0,1\}\). Next, notice that, for any \(i\in[W]\), we may select \(c_{i}\) such that \(\mathrm{err}_{\mathcal{D}}(c_{i})=\frac{1}{2}\left(1-\frac{4\alpha}{\kappa}\right)\).

Furthermore, for any \(i\neq j\in[W]\) and any hypothesis \(h:\mathcal{X}\to\{0,1\}\), we have

\[\mathrm{err}_{\mathcal{D}_{i}}(h)+\mathrm{err}_{\mathcal{D}_{j}}(h) \geq\left(1-\frac{4\alpha}{\kappa}\right)+\frac{4\alpha}{\kappa}\cdot\frac{ \kappa\cdot d_{\mathrm{VC}}}{d_{\mathrm{VC}}}=\left(1-\frac{4\alpha}{\kappa} \right)+4\alpha.\]

This means that \(\mathrm{err}_{\mathcal{D}_{i}}(h)>\frac{1}{2}\left(1-\frac{4\alpha}{\kappa} \right)+2\alpha\) or \(\mathrm{err}_{\mathcal{D}_{j}}(h)>\frac{1}{2}\left(1-\frac{4\alpha}{\kappa} \right)+2\alpha\). In other words, we have \(\Psi_{\mathrm{agn}(\mathcal{C};\alpha)}(\mathcal{D}_{i})\cap\Psi_{\mathrm{agn} (\mathcal{C};\alpha)}(\mathcal{D}_{j})=\emptyset\). Finally, we have

\[d_{\mathrm{KL}}(\mathcal{D}_{i}\parallel\mathcal{D}_{j}) \leq d_{\chi^{2}}(\mathcal{D}_{i}\parallel\mathcal{D}_{j})\] \[=\sum_{\ell\in[d_{\mathrm{VC}}],y\in\{0,1\}}\frac{(\mathcal{D}_{i} ((x,y))-\mathcal{D}_{j}((x,y)))^{2}}{\mathcal{D}_{j}((x,y))}\] \[\leq\sum_{\ell\in[d_{\mathrm{VC}}],y\in\{0,1\}}\frac{O(\alpha/d_{ \mathrm{VC}})^{2}}{\nicefrac{{1}}{{4d_{\mathrm{VC}}}}}\]\[= \sum_{\ell\in[\dd_{\mathrm{VC}}]}O(\alpha^{2}/d_{\mathrm{VC}})\] \[\leq O(\alpha^{2}).\]

Plugging this into Lemma39, we get that \(n_{m}^{\mathrm{agn}(\mathcal{C};\alpha)}\geq\Omega\left(\frac{d_{\mathrm{VC}}}{ \varepsilon\alpha\sqrt{m}}+\frac{d_{\mathrm{VC}}}{m\alpha^{2}}\right)\) as desired. 

#### e.3.2 Private Hypothesis Selection

In _Hypothesis Selection problem_, we are given a set \(\mathfrak{P}\) of hypotheses, where each \(\mathcal{P}\in\mathfrak{P}\) is a distribution over some domain \(\mathcal{Z}\). The goal is, for the underlying distribution \(\mathcal{D}\), to output \(\mathcal{P}^{*}\in\mathfrak{P}\) that is the closest (in TV distance) to \(\mathcal{D}\). We state below the guarantee one can get from our approach:

**Theorem 41**.: _Let \(\alpha,\beta\in(0,0.1)\). Then, for any \(\varepsilon>0\), there exists an \(\varepsilon\)-user-level DP algorithm for Hypothesis Selection that, when each user has \(m\) samples and \(\min_{\mathcal{P}^{\prime}\in\mathfrak{P}}d_{\mathrm{tv}}(\mathcal{P}^{ \prime},\mathcal{D})\leq 0.1\alpha\), with probability \(1-\beta\), outputs \(\mathcal{P}\in\mathfrak{P}\) such that \(d_{\mathrm{tv}}(\mathcal{P},\mathcal{D})\leq\alpha\) as long as_

\[n\geq\tilde{\Theta}\left(\frac{\log(|\mathfrak{P}|/\beta)}{\varepsilon}+\frac{ \log(|\mathfrak{P}|/\beta)}{\alpha\varepsilon\sqrt{m}}+\frac{\log(|\mathfrak{ P}|/\beta)}{\alpha^{2}m}\right).\]

For \(m=1\), the above theorem essentially match the item-level DP sample complexity bound from [1]. The proof also proceeds similarly as theirs, except that we use the user-level DP EM rather than the item-level DP one.

Proof Theorem41.: We use the so-called _Scheffe score_ similar to [1]: For every \(\mathcal{P},\mathcal{P}^{\prime}\in\mathfrak{P}\), we define \(\mathcal{W}_{\mathcal{P},\mathcal{P}^{\prime}}\) as the set \(\{z\in\mathcal{Z}\mid\mathcal{P}(z)>\mathcal{P}^{\prime}(z)\}\) and let

\[\psi_{\mathcal{P},\mathcal{P}^{\prime}}(z)=\mathcal{P}(\mathcal{W}_{\mathcal{ P},\mathcal{P}^{\prime}})-\mathbf{1}[z\in\mathcal{W}_{\mathcal{P},\mathcal{P}^{ \prime}}].\]

We then use the \(\varepsilon\)-user-level DP pairwise scoring EM (Theorem37). The privacy guarantee follows immediately from the theorem.

For the utility analysis, let \(\mathcal{P}^{*}\in\operatorname*{argmin}_{\mathcal{P}\in\mathfrak{P}}d_{ \mathrm{tv}}(\mathcal{P},\mathcal{D})\). Recall from our assumption that \(d_{\mathrm{tv}}(\mathcal{P}^{*},\mathcal{D})\leq 0.1\alpha\). Furthermore, observe that

\[\operatorname{pscr}_{\mathcal{P}^{*},\mathcal{P}}^{\psi}(\mathcal{D})= \mathcal{P}^{*}(\mathcal{W}_{\mathcal{P}^{*},\mathcal{P}})-\mathcal{D}( \mathcal{W}_{\mathcal{P}^{*},\mathcal{P}})\leq d_{\mathrm{tv}}(\mathcal{D}, \mathcal{P}^{*})\leq 0.1\alpha.\]

Moreover, for every \(\mathcal{P}\in\mathfrak{P}\), we have

\[\operatorname{pscr}_{\mathcal{P},\mathcal{P}^{*}}^{\psi}(\mathcal{ D}) =\mathcal{P}(\mathcal{W}_{\mathcal{P},\mathcal{P}^{*}})-\mathcal{D}( \mathcal{W}_{\mathcal{P},\mathcal{P}^{*}})\] \[=(\mathcal{P}(\mathcal{W}_{\mathcal{P},\mathcal{P}^{*}})- \mathcal{P}^{*}(\mathcal{W}_{\mathcal{P},\mathcal{P}^{*}}))-(\mathcal{D}( \mathcal{W}_{\mathcal{P},\mathcal{P}^{*}})-\mathcal{P}^{*}(\mathcal{W}_{ \mathcal{P},\mathcal{P}^{*}}))\] \[=d_{\mathrm{tv}}(\mathcal{P},\mathcal{P}^{*})-(\mathcal{D}( \mathcal{W}_{\mathcal{P},\mathcal{P}^{*}})-\mathcal{P}^{*}(\mathcal{W}_{ \mathcal{P},\mathcal{P}^{*}}))\] \[\geq d_{\mathrm{tv}}(\mathcal{P},\mathcal{P}^{*})-d_{\mathrm{tv} }(\mathcal{P}^{*},\mathcal{D})\] \[\geq d_{\mathrm{tv}}(\mathcal{P},\mathcal{P}^{*})-0.1\alpha.\]

Thus, applying the utility guarantee from Theorem37, we can conclude that, with probability \(1-\beta\), the algorithm outputs \(\mathcal{P}\) such that \(d_{\mathrm{tv}}(\mathcal{P},\mathcal{D})\leq 0.5\alpha+0.1\alpha<\alpha\) as desired. 

#### e.3.3 Distribution Learning

Private hypothesis selection has a number of applications, arguably the most prominent one being _distribution learning_. In distribution learning, there is a family \(\mathfrak{P}\) of distributions. The underlying distribution \(\mathcal{D}\) comes from this family. The task \(\mathrm{dlearn}(\mathfrak{P};\alpha)\) is to output a distribution \(\mathcal{Q}\) such that \(d_{\mathrm{tv}}(\mathcal{Q},\mathcal{D})\leq\alpha\), where \(\alpha>0\) denotes the accuracy parameter.

For convenient, let us defined _packing_ and _covering_ of a family of distributions:

* A family \(\mathfrak{Q}\) of distributions is an _\(\alpha\)-cover_ of a family \(\mathfrak{P}\) of distributions under distance measure \(d\) iff, for all \(\mathcal{P}\in\mathfrak{P}\), there exists \(\mathcal{Q}\in\mathfrak{Q}\) such that \(d(\mathcal{Q},\mathcal{P})\leq\alpha\).
* A family \(\mathfrak{Q}\) of distributions is an _\(\alpha\)-packing_ of a family \(\mathfrak{P}\) of distributions under distance measure \(d\) iff, \(\mathfrak{Q}\subseteq\mathfrak{P}\) and for all distinct \(\mathcal{Q},\mathcal{Q}^{\prime}\in\mathfrak{Q}\), we have \(d(\mathcal{Q},\mathcal{Q}^{\prime})\geq\alpha\).

The size of the cover / packing is defined as \(\operatorname{size}(\mathfrak{Q}):=\log(|\mathfrak{Q}|)\). The diameter of \(\mathfrak{Q}\) under distance \(d\) is defined as \(\max_{\mathcal{Q},\mathcal{Q}^{\prime}\in\Omega}d(\mathcal{Q},\mathcal{Q}^{\prime})\).

The following convenient lemma directly follows from Theorem41 and Lemma39.

**Theorem 42** (Distribution Learning--Arbitrary Family).: _Let \(\mathfrak{P}\) be any family of distributions, and let \(\alpha,\beta,\varepsilon>0\) be sufficiently small. If there exists an \(0.1\alpha\)-cover of \(\mathfrak{P}\) under the TV distance of size \(L\), then_

\[n_{m}^{\operatorname{dlearn}(\mathfrak{P};\alpha)}(\varepsilon,\delta=0)\leq \widetilde{O}\left(\frac{L}{\varepsilon}+\frac{L}{\alpha\varepsilon\sqrt{m}}+ \frac{L}{\alpha^{2}m}\right).\]

_Furthermore, if there exists a family \(\mathfrak{Q}\) of size \(L\) that is an \(3\alpha\)-packing of \(\mathfrak{P}\) under the TV distance and has diameter at most \(\beta\) under the KL-divergence, then_

\[n_{m}^{\operatorname{dlearn}(\mathfrak{P};\alpha)}(\varepsilon,\delta=0)\geq \tilde{\Omega}\left(\frac{L}{\varepsilon}+\frac{L}{\varepsilon\sqrt{m\beta}}+ \frac{L}{m\beta}\right).\]

In theorems below, we abuse the notations \(\tilde{\Theta},\widetilde{O}\) and also use them to suppress terms that are polylogarithmic in \(1/\alpha,d,R\), and \(\kappa\).

Discrete Distributions.First is for the task of discrete distribution learning on domain \([k]\) (denoted by \(\operatorname{DD}\!k(\alpha)\)), which has been studied before in [13]. In this case, \(\mathfrak{P}\) consists of all distributions over the domain \([k]\). We have the following theorem:

**Theorem 43** (Distribution Learning--Discrete Distributions).: _For any sufficiently small \(\alpha,\varepsilon>0\) and for all \(m\in\mathbb{N}\), we have_

\[n_{m}^{\operatorname{DD}\!k(\alpha)}(\varepsilon,\delta=0)=\tilde{\Theta} \left(\tfrac{k}{\varepsilon}+\tfrac{k}{\alpha\varepsilon\sqrt{m}}+\tfrac{k}{ \alpha^{2}m}\right).\]

Proof.: **Upper bound.** It is simple to see that there exists an \(0.1\alpha\)-cover of the family of size \(O(k\cdot\log(k/\alpha))\): We simply let the cover contain all distributions whose probability mass at each point is a multiple of \(\lfloor 10k/\alpha\rfloor\). Plugging this into Theorem42 yields the desired upper bound.

**Lower bound.** As for the lower bound, we may use a construction similar to that of [1] (and also similar to that in the proof of Theorem40). Assume w.l.o.g. that \(k\) is even. From Theorem24, there exist \(u_{1},\dots,u_{W}\in\{0,1\}^{k/2}\) where \(W=2^{\Omega(k)}\) such that \(\|u_{i}-u_{j}\|_{0}\geq\kappa\cdot k\) for some constant \(\kappa\). For any sufficiently small \(\alpha<\kappa/16\), we can define the distribution \(\mathcal{D}_{i}\) for \(i\in[W]\) by

\[\mathcal{D}_{i}(2\ell-1) =\frac{1}{2k}\left(1+\frac{8\alpha}{\kappa}\cdot\mathbf{1}[(u_{i} )_{\ell}=0]\right)\] \[\mathcal{D}_{i}(2\ell) =\frac{1}{2k}\left(1-\frac{8\alpha}{\kappa}\cdot\mathbf{1}[(u_{i} )_{\ell}=0]\right),\]

for all \(\ell\in[k/2]\). By a similar calculation as in the proof of Theorem40, we have that it is an \((3\alpha)\)-packing under TV distance and its diameter under KL-divergence is at most \(O(\alpha^{2})\). Plugging this into Theorem42 then gives the lower bound. 

Interestingly, the user complexity matches those achieved via _approximate-DP_ algorithms from [13, 24], except for the first term (i.e., \(k/\varepsilon\)) whereas the best known \((\varepsilon,\delta)\)-DP algorithm of [24] works even with \(\log(1/\delta)/\varepsilon\) instead. In other words, when \(m\) is intermediate, the user complexity between the pure-DP and approximate-DP cases are the same. Only for large \(m\) that approximate-DP helps.

Product Distributions.In this case, \(\mathfrak{P}\) is a product distribution over the domain \([k]^{d}\) (denoted by \(\operatorname{PD}\!\left(k,d;\alpha\right)\)). We have the following theorem:

**Theorem 44** (Distribution Learning--Product Distributions).: _For any sufficiently small \(\alpha,\varepsilon>0\) and for all \(m\in\mathbb{N}\), we have_

\[n_{m}^{\operatorname{PD}\!\left(k,d;\alpha\right)}(\varepsilon,\delta=0)= \tilde{\Theta}\left(\frac{kd}{\varepsilon}+\frac{kd}{\alpha\varepsilon\sqrt{ m}}+\frac{kd}{\alpha^{2}m}\right).\]Proof.: **Upper Bound.** Similar to before, we simply take \(\mathfrak{Q}\) to be a \(0.1\alpha\)-cover (under TV distance) of the product distributions, which is known to have size at most \(O(kd\cdot\log(kd/\alpha))\)[1, Lemma 6.2].

**Lower Bound.** Acharya et al. [1, Proof of Theorem 11] showed that there exists a family \(\mathfrak{Q}\) of product distributions over \([k]^{d}\) that is a \((3\alpha)\)-packing under TV distance and its diameter under KL-divergence is at most \(O(\alpha^{2})\) of size \(\Omega(kd)\). Theorem 42 then gives the lower bound. 

Gaussian Distributions: Known Covariance.Next, we consider Gaussian distributions \(\mathcal{N}(\mu,\Sigma)\), where \(\mu\in\mathbb{R}^{d},\Sigma\in\mathbb{R}^{d\times d}\), under the assumption that \(\|\mu\|\leq R\) and \(\Sigma=I\)14. Let \(\operatorname{Gauss}(R,d;\alpha)\) denote this task, where \(\alpha\) is again the (TV) accuracy. For this problem, we have:

Footnote 14: Or equivalently that \(\Sigma\) is known; since such an assumption is sufficient to rotate the example to isotropic positions.

**Theorem 45** (Distribution Learning--Gaussian Distributions with Known Covariance).: _For any sufficiently small \(\alpha,\varepsilon>0\) and for all \(m\in\mathbb{N}\), we have_

\[n_{m}^{\operatorname{Gauss}(R,d;\alpha)}(\varepsilon,\delta=0)=\tilde{\Theta }\left(\frac{d}{\varepsilon}+\frac{d}{\alpha\varepsilon\sqrt{m}}+\frac{d}{ \alpha^{2}m}\right).\]

Proof.: **Upper Bound.** It is known that this family of distribution admits a \((0.1\alpha)\)-cover (under TV distance) of size \(O(d\cdot\log(dR/\alpha))\)[1, Lemma 6.7]. This, together with Theorem 42, gives the upper bound.

**Lower Bound.** Again, Acharya et al. [1, Proof of Theorem 12] showed that there exists a family \(\mathfrak{Q}\) of isotropic Gaussian distributions with \(\|\mu\|\leq O(\sqrt{\log(1/\alpha)})\) that is a \((3\alpha)\)-packing under TV distance and its diameter under KL-divergence is at most \(O(\alpha^{2})\) of size \(\Omega(d)\). Theorem 42 then gives the lower bound. 

We note that this problem has already been (implicitly) studied under user-level approximate-DP in [15],15 who showed that \(n_{m}^{\operatorname{Gauss}(R,d;\alpha)}(\varepsilon,\delta>0)\leq\widetilde{ O}\left(\frac{d}{\alpha\varepsilon\sqrt{m}}+\frac{d}{\alpha^{2}m}\right)\). Again, it is perhaps surprising that our pure-DP bound nearly matches this result, except that we have the extra first term (i.e., \(d/\varepsilon\)).

Footnote 15: Actually, Narayanan et al. [15] studied the mean estimation problem, but it is not hard to see that an algorithm for mean estimation also provides an algorithm for learning Gaussian distributions.

Gaussian Distributions: Unknown Bounded Covariance.Finally, we consider the case where the covariance is also unknown but is assumed to satisfied \(I\preceq\Sigma\preceq\kappa I\). We use \(\operatorname{Gauss}(R,d,\kappa;\alpha)\) to denote this task.

**Theorem 46** (Distribution Learning--Gaussian Distributions with Unknown Bounded Covariance).: _Let \(\kappa>1\) be a constant. For any sufficiently small \(\alpha,\varepsilon>0\) and for all \(m\in\mathbb{N}\), we have_

\[n_{m}^{\operatorname{Gauss}(R,d,\kappa;\alpha)}(\varepsilon,\delta=0)=\tilde{ \Theta}_{\kappa}\left(\frac{d^{2}}{\varepsilon}+\frac{d^{2}}{\alpha\varepsilon \sqrt{m}}+\frac{d^{2}}{\alpha^{2}m}\right).\]

Proof.: **Upper Bound.** This follows from Theorem 42 and a known upper bound of \(O(d^{2}\log(d\kappa/\alpha)+d\cdot\log(dR/\alpha))\) on the size of a \((0.1\alpha)\)-cover (under TV distance) of the family [1, Lemma 6.8].

**Lower Bound.** Devroye et al. [1] showed16 that, for \(\kappa\geq 1+O(\alpha)\), there exists a \(3\alpha\)-packing under TV distance whose diameter under the KL-divergence is at most \(O(\alpha^{2})\) of size \(\Omega(d^{2})\). This, together with Theorem 42, gives the lower bound. 

Footnote 16: Specifically, this is shown in [1, Proposition 3.1]; their notation “\(m\)” denote the number of non-zero (non-diagonal) entries of the covariance matrix which can be set to \(\Omega(d^{2})\) for our purpose.

We remark that it is simple to see that Gaussian with unbounded mean or unbounded covariance cannot be learned with pure (even user-level) DP using a finite number of examples.