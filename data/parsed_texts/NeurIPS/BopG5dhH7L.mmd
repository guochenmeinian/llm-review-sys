# A Computationally Efficient Sparsified Online Newton Method

 Devvrit

Department of Computer Science

The University of Texas at Austin

devvrit.03@gmail.com

&Sai Surya Duvvuri

Department of Computer Science

The University of Texas at Austin

subramanyamdvss@gmail.com

&Rohan Anil

Google DeepMind

rohananil@google.com

&Vineet Gupta

Google

vineet@google.com

&Cho-Jui Hsieh

CS Department, UCLA & Google

chohsieh@cs.ucla.edu

&Inderjit Dhillon

Google

isd@google.com

equal contribution, \({}^{\dagger}\) Work done while at Google

###### Abstract

Second-order methods hold significant promise for enhancing the convergence of deep neural network training; however, their large memory and computational demands have limited their practicality. Thus there is a need for scalable second-order methods that can efficiently train large models. In this paper, we introduce the Sparsified Online Newton (SONew) method, a memory-efficient second-order algorithm that yields a sparsified yet effective preconditioner. The algorithm emerges from a novel use of the LogDet matrix divergence measure; we combine it with sparsity constraints to minimize regret in the online convex optimization framework. Empirically, we test our method on large scale benchmarks of up to 1B parameters. We achieve up to \(30\%\) faster convergence, \(3.4\%\) relative improvement in validation performance, and \(80\%\) relative improvement in training loss, in comparison to memory efficient optimizers including first order methods. Powering the method is a surprising fact - imposing structured sparsity patterns, like tridiagonal and banded structure, requires little to no overhead, making it as efficient and parallelizable as first-order methods. In wall-clock time, tridiagonal SONew is only about \(3\%\) slower per step than first-order methods but gives overall gains due to much faster convergence. In contrast, one of the state-of-the-art (SOTA) memory-intensive second-order methods, Shampoo, is unable to scale to large benchmarks. Additionally, while Shampoo necessitates significant engineering efforts to scale to large benchmarks, SONew offers a more straightforward implementation, increasing its practical appeal. SONew code is available at: https://github.com/devrrit/SONew

## 1 Introduction

Stochastic first order methods which use the negative gradient direction to update parameters have become the standard for training deep neural networks (DNNs). Gradient-based preconditioning involves finding an update direction, by multiplying the gradient with a preconditioner matrix carefully chosen from gradients observed in previous iterations, to improve convergence. (Fullmatrix) Adagrad [15], online Newton method [25] and natural gradient descent [3] use a full-matrix preconditioner, but computing and storing the full matrix is infeasible when there are millions of parameters. Thus, diagonal versions such as diagonal Adagrad, Adam [33], and RMSprop [28] are now widely used to train DNNs due to their scalability.

Several higher-order methods have previously been applied to deep learning ([24; 5; 23; 38]). All these methods use Kronecker product factorizations that reduce computational and storage costs to make them feasible for training neural networks. However, to precondition a \(d_{1}\times d_{2}\) parameter matrix, these methods require matrix inverse operations, which take \(\mathcal{O}(d_{1}^{3}+d_{2}^{3})\) time and \(\mathcal{O}(d_{1}^{2}+d_{2}^{2})\) space. In comparison, first-order methods use \(\mathcal{O}(d_{1}d_{2})\) time and memory, which is linear in the number of parameters. For instance, when \(d_{1}=kd_{2}\), the memory used by Shampoo, \(d_{1}^{2}+d_{2}^{2}\) floating point numbers is \(\mathcal{O}(k)\) times the number of parameters, which could be arbitrarily large depending on \(k\). This calls for further research in developing efficient second-order optimization techniques to train DNNs with memory and time complexity linear in the number of parameters.

In this paper, we present a novel Sparsified Online Newton (SONew) method, which only requires linear time and space complexity, to train large-scale DNNs. We derive the algorithm through two steps, classical regret analysis followed by a sparsification step. In more detail, regret analysis when using a preconditioner reveals that the error is bounded by two terms, the first depends on the change in the preconditioning matrix, while the second depends on the generalized gradient norm (see Section 3 for more details). We take a novel approach of minimizing the second term while regularizing two successive preconditioners to be close in the LogDet matrix divergence measure [34] (see Section 3 for the intuition behind choosing LogDet divergence). This analysis naturally yields us an Online Newton method [25]. To make it computationally efficient, we further sparsify the preconditioner by finding a sparse approximation that is close in LogDet divergence. Thus we are consistent in using the same measure (LogDet divergence) in both the regularization and sparsification steps. This gives us our SONew method, which achieves linear complexity by leveraging structured sparsity patterns, such as tridiagonal and banded, in the preconditioner. This is unlike most existing online Newton methods that require quadratic space and cubic time complexity. By making each step linear time, the SONew method can be applied to train modern DNNs as efficiently as first order methods. Further, our method is embarrassingly parallelizable thus making negligible the overhead of computing the preconditioner. We also show that introducing sparsity allows us to reduce the condition number of the problem dynamically to improve numerical stability.

We strengthen the relationship between sparse LogDet divergence minimization and online convex optimization by establishing an optimal \(\mathcal{O}(\sqrt{T})\) regret upper bound for tridiagonal sparsity pattern. In our experiments on an MLP Autoencoder and Graph Neural Network (GNN), we found that our method outperformed first-order methods in terms of training loss within the same training time, while Shampoo (second-order method) takes significantly longer. In our experiments on Vision Transformers on Imagenet and GNN on OGBG-molpcba, we achieve a target validation performance using 10% and 30% fewer iterations respectively compared to Adam, the SOTA optimizer for both benchmarks. Furthermore, using the same number of iterations as Adam we observe 0.7% and 3.4% relative improvement for ViT and GNN respectively in validation performance. From an optimization point of view, SONew achieves 9% and 80% better relative training loss for ViT and GNN respectively. It is worth noting that Shampoo statistics required \(\sim 7\times\#params\) for ViT whereas tridiag-SONew uses only \(2\times\#params\) for its statistics. We also test another recently proposed memory efficient second order optimizer, rfdSON [37], but found its performance suboptimal to the best performing first order method. Owing to SONew's scalability, we train a Large Language Model (LLM) with 1 billion parameters and compare it with AdaFactor [45], a popularly used first order optimizer to train LLMs [11]. SONew achieves the same performance as AdaFactor using \(26\%\) fewer steps, resulting in a \(1.35\times\) faster training. When using the same number of steps, SONew obtained a \(1.7\%\) relative better train loss. In terms of implementation, SONew is just a few lines of code (Equation (13)) without complex engineering challenges, rendering it even more useful and practical.

## 2 Background

The inner product between matrices is defined as \(\left\langle A,B\right\rangle=\operatorname{Tr}(A^{T}B)\), where \(\operatorname{Tr}(.)\) denotes the matrix trace. The Frobenius norm of a matrix \(A\) is \(\left\|A\right\|_{F}=\sqrt{\operatorname{Tr}(A^{T}A)}\), while its spectral norm is \(\left\|A\right\|_{2}=\max_{x}\left\|Ax\right\|_{2}/\left\|x\right\|_{2}\). We use \(I_{n}\in\mathbb{R}^{n\times n}\) to denote an identity matrix. We use \(S_{n}\), \(S_{n}^{++}\) to denote the set of symmetric, and positive definite matrices respectively. The generalized norm of a vector \(x\in\mathbb{R}^{n}\) with respect to matrix \(A\in S_{n}^{++}\) is defined as \(\left\|x\right\|_{A}=\sqrt{x^{T}Ax}\). We use \(\det{(A)}\) to denote the determinant of matrix \(A\), and \(\operatorname{diag}(A)\) to denote the diagonal matrix with \(\operatorname{diag}(A)_{ii}=A_{ii}\). We use \(\mathcal{G}\) and \(\tilde{\mathcal{G}}\) to denote a graph and its sub-graph with a vertex set \([n]=\{1,\ldots,n\}\). Let \(E_{\tilde{\mathcal{G}}}\) denote set of edges in graph \(\tilde{\mathcal{G}}\), and \(\operatorname{neig}_{\mathcal{G}}(i)\) denote neighbours of vertex \(i\) in graph \(\mathcal{G}\). A sparse symmetric matrix \(A\in\mathbb{R}^{n\times n}\) follows a sparsity structure graph \(\mathcal{G}\) if \(A_{i,j}=0\)\(\forall(i,j)\notin E_{\tilde{\mathcal{G}}}\),. Note that set of all such matrices form a linear subspace. We use \(S_{n}(\mathcal{G})^{++}\) to denote the set of positive definite matrices with sparsity structure given by graph \(\mathcal{G}\), i.e, if \(X\in S_{n}(\mathcal{G})^{++}\), then \(X_{ij}=0\)\(\forall(i,j)\notin E(\mathcal{G})\). \(S_{n}(\mathcal{G})^{++}\) is an open convex set. Given an index set \(I=\{i_{1},i_{2},..,i_{n}\}\), we use \(A_{II}\) to denote the corresponding principal sub-matrix of \(A\).

### LogDet matrix divergence

Let \(\phi:S_{n}^{++}\rightarrow\mathbb{R}\) be a strictly convex, differentiable function. The Bregman matrix divergence between \(X,Y\in S_{n}^{++}\) is defined as [8; 34]: \(\operatorname{D}_{\phi}(X,Y)=\phi(X)-\phi(Y)-\operatorname{Tr}(\nabla\phi(Y)^{ T}(X-Y))\). Since \(\phi\) is convex, \(\operatorname{D}_{\phi}(X,Y)\geq 0\) for all \(X,Y\in S_{n}^{++}\). For example if \(\phi(X)=\left\|X\right\|_{F}^{2}\), the corresponding Bregman divergence \(\operatorname{D}_{\phi}(X,Y)=\left\|X-Y\right\|_{F}^{2}\) is the squared Frobenius distance. In this paper, we extensively use the convex function \(\phi(X)=-\log\det{(X)}\); the corresponding divergence measure \(\operatorname{D}_{\ell\mathrm{d}}(X,Y)\) is called the _LogDet matrix divergence_:

\[\operatorname{D}_{\ell\mathrm{d}}(X,Y)=-\log\det{\left(XY^{-1}\right)}+ \operatorname{Tr}(XY^{-1})-n.\] (1)

The LogDet divergence is scale invariant to invertible matrices \(A\), i.e. \(\operatorname{D}_{\ell\mathrm{d}}(A^{T}XA,A^{T}YA)=\operatorname{D}_{\ell \mathrm{d}}(X,Y)\). LogDet divergence can be written in terms of eigendecompositions of \(X=V\Sigma V^{T}\) and \(Y=U\Theta U^{T}\)[34]:

\[\operatorname{D}_{\ell\mathrm{d}}(\textit{X,}Y)\!=\!\sum_{i}\!\sum_{j}\!(v_{i} ^{T}u_{j})^{2}\!(\sigma_{i}/\theta_{j}\!-\!\log(\sigma_{i}/\theta_{j})\!-\!1).\] (2)

These two properties are later used in Section 3 to highlight the significance of LogDet divergence in our algorithm.

## 3 SONew: Sparsified Online Newton Method

We now present our proposed algorithm SONew.

### Regret minimization via LogDet divergence

We set up our problem under the online convex optimization framework (OCO) [43; 26], where at each round the learner makes a prediction \(w_{t}\) and receives a convex loss \(f_{t}(w_{t})\) and gradient \(g_{t}=\nabla f_{t}(w_{t})\) as feedback. The goal of the learner is to reduce regret \(R_{T}\) by predicting \(w_{t}\) so that a low aggregate loss \(\sum_{t=1}^{T}f_{t}(w_{t})\) is achieved compared to the best possible, \(w^{*}=\operatorname{arg\,min}_{w}\sum_{t=1}^{T}f_{t}(w)\). Formally, regret is given by

\[R_{T}(w_{1},\ldots,w_{T})=\sum_{t=1}^{T}f_{t}(w_{t})-\sum_{t=1}^{T}f_{t}(w^{*}).\]

Using [10], \(R\) regret in online setting yields \(R/T\) convergence rate in the stochastic setting. To upper bound this regret, we proceed as in [26] by analyzing the error in the iterates for the update \(w_{t+1}\coloneqq w_{t}-\eta X_{t}g_{t}\), where \(X_{t}\in\mathbb{R}^{n\times n}\). Then \(\left\|w_{t+1}-w^{*}\right\|_{X_{t}^{-1}}^{2}=\left\|w_{t}-\eta X_{t}g_{t}-w^{ *}\right\|_{X_{t}^{-1}}^{2}=\left\|w_{t}-w^{*}\right\|_{X_{t}^{-1}}^{2}+\eta ^{2}g_{t}^{T}X_{t}g_{t}-2\eta(w_{t}-w^{*})^{T}g_{t}\). The convexity of \(f_{t}\) implies that \(f_{t}(w_{t})-f_{t}(w^{*})\leq(w_{t}-w^{*})^{T}g_{t}\) leading to \(f_{t}(w_{t})-f_{t}(w^{*})\leq\frac{1}{2\eta}(\left\|w_{t}-w^{*}\right\|_{X_{t} ^{-1}}^{2}-\left\|w_{t+1}-w^{*}\right\|_{X_{t}^{-1}}^{2}+\eta^{2}g_{t}^{T}X_{t}g _{t})\). Summing over all \(t\in[T]\) and rearranging reveals the following upper bound on overall regret:

\[R_{T}\leq\frac{1}{2\eta}\left\|w_{1}-w^{*}\right\|_{X_{1}^{-1}}^{2}+\frac{\eta }{2}\sum_{t=1}^{T}g_{t}^{T}X_{t}g_{t}+\frac{1}{2\eta}\sum_{t=2}^{T}(w_{t}-w^{*} )^{T}(X_{t}^{-1}-X_{t-1}^{-1})(w_{t}-w^{*}).\] (3)Since \(w^{*}\) is unknown, finding \(X_{t}\) which minimizes (3) is infeasible. So to minimize regret, we attempt to minimize the second term in (3) while regularizing \(X_{t}^{-1}\) to be "close" to \(X_{t-1}^{-1}\). The nearness measure we choose is the LogDet matrix divergence, thus leading to the following objective

\[X_{t}=\operatorname*{arg\,min}_{X\in S_{\mathrm{s}}^{++}}g_{t}^{T}Xg_{t}, \operatorname*{such\,\,that}\,\operatorname{D}_{\mathrm{fd}}\left(X,X_{t-1} \right)\leq c_{t},\] (4)

where \(\operatorname{D}_{\mathrm{fd}}\) is as in (1). Why do we use the LogDet divergence? From (2), due to the term \(\lambda_{i}/\theta_{j}\), \(\operatorname{D}_{\mathrm{fd}}(X,X_{t-1})\) prioritizes matching the smaller eigenvalues of \(X_{t-1}\) with those of \(X\), i.e., matching the larger eigenvalues of \(X_{t-1}^{-1}\) and \(X^{-1}\). As a consequence, LogDet divergence regularizes \(X\) by matching up its large eigenvalues with those of \(X_{t-1}\). For example if smallest and largest eigenvalue of \(X_{t-1}\) are \(\theta_{n}\) and \(\theta_{1}\), then for an eigenvalue \(\sigma\) of \(X\), when \(\sigma>\theta_{n},\ \theta_{1}\), the penalty from (2) for \(\theta_{n}\) is higher than for \(\theta_{1}\), \((\sigma/\theta_{n}-\log(\sigma/\theta_{n})-1)>(\sigma/\theta_{1}-\log(\sigma/ \theta_{1})-1)\). This intuition leads us to formulate (4) as our objective. We recall that there is precedence of using the LogDet divergence in the optimization literature; indeed the celebrated BFGS algorithm [9; 17; 22; 44] can be shown to be the unique solution obtained when the LogDet divergence between successive preconditioners, subject to a secant constraint, is minimized (as shown in the 4-page paper by [18]).

The optimization problem in (4) is convex in \(X\) since the LogDet divergence is convex in its first argument. The Lagrangian \(\mathcal{L}(X,\lambda_{t})=g_{t}^{T}Xg_{t}+\lambda_{t}(\operatorname{D}_{ \mathrm{fd}}(X,X_{t-1})-c_{t})=\operatorname{Tr}(Xg_{t}g_{t}^{T})+\lambda_{t}( -\log\det\left(XX_{t-1}^{-1}\right)+\operatorname{Tr}(XX_{t-1}^{-1})-n))- \lambda_{t}c_{t}\). Setting \(\nabla\mathcal{L}(X,\lambda_{t})=0\), and using the fact that \(\nabla\log\det\left(X\right)=X^{-1}\) we get the following update rule:

\[X_{t}^{-1}=X_{t-1}^{-1}+g_{t}g_{t}^{T}/\lambda_{t}.\] (5)

We _emphasize_ that the update rule (5) arises naturally from our novel use of LogDet divergence to minimize the regret. Moreover, Equation (5) can be seen as a general update rule applicable to numerous existing optimizers. For example, setting \(c_{t}=0\) (equivalently \(\lambda_{t}=\infty\)) \(\forall t\in[n]\) in (4) results in no change to the preconditioner in any round. In this case, with \(X_{0}=I_{n}\), we get online gradient descent [54]. On the other hand, setting \(\lambda_{t}=1\) gives the update rule of the online Newton method [25]. Our update rule differs from (full-matrix) Adagrad [15] which has \(X_{t}^{-2}=X_{t-1}^{-2}+g_{t}g_{t}^{T}\).

Maintaining and updating \(X_{t}\) as in (5) is possible by using Sherman-Morrison formula but requires \(\mathcal{O}(n^{2})\) storage and time complexity. This becomes impractical when \(n\) is in the order of millions which is typically the case in DNNs.

### Sparsifying the Preconditioner

To minimize the memory needed for maintaining and updating \(X_{t}\) using (5), we adopt the strategy of sparsifying the preconditioner. For existing optimizers such as (full-matrix) Adagrad or the Online Newton method, it is unclear how to sparsify a given preconditioner. Specifically, there is no intuitive approach to assessing the quality of a sparse preconditioner compared to a full-matrix preconditioner. However, since our update rule (5) originates from using LogDet divergence in the regret bound analysis, it gives us a natural metric to measure the quality of a sparse preconditioner. Let's consider the following problem: find a sparse positive definite \(X\) with \(\left\|X\right\|_{0}\leq\alpha n\), \(\alpha>1\), such that the objective \(\operatorname{D}_{\mathrm{fd}}(X,(X_{t-1}^{-1}+g_{t}g_{t}^{T}/\lambda_{t})^{-1})\) is minimized. Essentially, this problem imposes a sparsity constraint while requiring the sparse preconditioner to remain close to the full-matrix preconditioner in terms of LogDet divergence.

Due to the \(L_{0}\)-norm constraint, this is a non-convex problem, which makes it difficult to solve exactly. Since \(L_{1}\)-norm serves as a convex relaxation for the \(L_{0}\) norm, we could use it instead, resulting in the following optimization problem also known as graphical lasso estimator [19]:

\[\min_{X\in S_{\mathrm{s}}^{++}}\operatorname{D}_{\mathrm{fd}}\left(X,(X_{t-1} ^{-1}+g_{t}g_{t}^{T}/\lambda_{t})^{-1}\right)+\gamma\left\|X\right\|_{1}.\]

However, the time taken to solve the above problem, even with the current best methods [7; 29; 16; 53], can still be too large (as these methods take several minutes for a matrix of size million), making it impractical to embed in DNN training.

In this paper, we take a different direction where we use fixed sparsity pattern constraints, specified by a fixed undirected graph \(\mathcal{G}\). To sparsify the solution in (5), we formulate the subproblem

\[X_{t}=\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}}\operatorname{D }_{\mathrm{fd}}\left(X,(X_{t-1}^{-1}+g_{t}g_{t}^{T}/\lambda_{t})^{-1}\right),\] (6)where \(S_{n}(\mathcal{G})^{++}\) denotes the set of positive definite matrices with the fixed sparsity pattern corresponding to the adjacency matrix of graph \(\mathcal{G}\). Note that both steps (4) and (6) use the same LogDet measure.

Owing to the structure of LogDet divergence, (6) can be surprisingly solved in \(\mathcal{O}(n)\) and easily parallelizable, for certain sparsity structures \(\mathcal{G}\). Algorithm 1 and 2 presents an instantiation of the proposed SONew method, which solves (6) using \(\mathcal{O}(n)\) time and memory for banded matrices with band size \(b\). In particular a tridiagonal matrix, corresponding to a chain graph, is a banded matrix with bandsize 1.

``` Inputs:\(\lambda_{t}:=\) coefficient in the update (10), \(\mathcal{G}:=\) sparsity graph (banded/tridiagonal), \(\epsilon:=\) damping parameter, \(T:=\) total number of iterations/mini-batches, \(\eta_{t}:=\) step size/learning rate. Output:\(w_{T+1}\)
1:\(H_{0}=\epsilon I_{d}\), \(w_{1}=0\)
2:for\(t\in\{1,\ldots,T\}\)do
3: compute \(g_{t}=\nabla f_{t}(w_{t})\)
4:\(H_{t}\coloneqq H_{t-1}+P_{\mathcal{G}}(g_{t}g_{t}^{T}/\lambda_{t})\in S_{n}( \mathcal{G})\) with \(P_{\mathcal{G}}\) as in (8). \(\triangleright\ \mathcal{O}(n)\) time & memory
5: Get \(L,D=\) Sparsified_Inverse (\(H_{t},\mathcal{G}\)), where \(X_{t}=LDL^{T}\) solves (11).
6: Compute descent direction \(u_{t}=LDL^{T}g_{t}\).
7:\(w_{t+1}=w_{t}-\eta_{t}u_{t}\)
8:endfor
9:return\(w_{T+1}\) ```

**Maintaining \(H_{t}\in S_{n}(\mathcal{G})\) in line 4**. Solving the subproblem in (6) naively is impractical since \(X_{t-1}^{-1}\) is a dense matrix. However, the structure of the LogDet divergence comes to the rescue; the optimization problem in (6) can be expanded as follows:

\[\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}}-\log\det \left(X\right)+\operatorname{Tr}(X(X_{t-1}^{-1}+g_{t}g_{t}^{T}/\lambda_{t})).\] (7)

Let us define the projection onto \(S_{n}(\mathcal{G})\), \(P_{\mathcal{G}}:\mathbb{R}^{n\times n}\rightarrow\mathbb{R}^{n\times n}\) as:

\[P_{\mathcal{G}}(M)_{ij}=\begin{cases}M_{ij}&\text{if }(i,j)\in E_{\mathcal{G}}, \\ 0&\text{otherwise}.\end{cases}\] (8)

Note that the \(\operatorname{Tr}(.)\) term in (7) is dependent only on the non-zero elements of \(X\in S_{n}(\mathcal{G})^{++}\), since \(\operatorname{Tr}(AB)=\langle A,B\rangle\), for symmetric matrices \(A\) and \(B\). Hence, (7) can be written as

\[\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}}-\log \det\left(X\right)+\langle X,P_{\mathcal{G}}(X_{t-1}^{-1}+g_{t}g_{t}^{T}/ \lambda_{t})\rangle,\] (9)

Computing the entire matrix \(X_{t-1}^{-1}\) can be avoided by analyzing the optimality condition of (9). Let \(g(X)=-\log\det\left(X\right)+\langle X,P_{\mathcal{G}}(X_{t-1}^{-1}+g_{t}g_{t }^{T}/\lambda_{t})\rangle\) denote the objective function in (9), then the optimality condition of (9) is \(P_{\mathcal{G}}(\nabla g(X))=P_{\mathcal{G}}(\nabla(-\log\det(X)+\langle X,P _{\mathcal{G}}(X_{t-1}^{-1}+g_{t}g_{t}^{T}/\lambda_{t})\rangle)=0\), since gradients with respective nonzero entries of \(X\) should be zero, \(\frac{\partial g(X)}{\partial X_{t,j}}=(\nabla_{X}(g(X)))_{i,j}=0\), \(\forall(i,j)\in E_{\mathcal{G}}\). Using \(\nabla(-\log\det(X))=-X^{-1}\), \(\nabla_{X}(\langle X,Y\rangle)=Y\), and setting \(X=X_{t}\) gives:

\[P_{\mathcal{G}}(X_{t}^{-1})-P_{\mathcal{G}}(X_{t-1}^{-1}+g_{t}g_{t}^{T}/ \lambda_{t})=0,\]

\[H_{t}=H_{t-1}+P_{\mathcal{G}}(g_{t}g_{t}^{T}/\lambda_{t}),\quad\text{where }H_{t}=P_{ \mathcal{G}}(X_{t}^{-1})\] (10)

Thus we only need to maintain \(H_{t}=P_{\mathcal{G}}(X_{t}^{-1})\). This matrix is updated as \(H_{t}=H_{t-1}+P_{\mathcal{G}}(g_{t}g_{t}^{T}/\lambda_{t})\). Since \(H_{t}\in S_{n}(\mathcal{G})\), the update can be done in \(\mathcal{O}(|E_{\mathcal{G}}|)\) memory and time, while computing the matrix \(X_{t}^{-1}\) would have cost \(\mathcal{O}(n^{2})\). In SONew (Algorithm 1), this key observation is used to maintain \(H_{t}\) in line 4.

**Computing \(X_{t}\) in line 5**. Now that \(H_{t}\) is known at every round \(t\), we can replace \(P_{\mathcal{G}}(X_{t-1}^{-1}+g_{t}g_{t}^{T}/\lambda_{t})\) in (9) with \(H_{t}\) as:

\[X_{t}=\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}}-\log\det{(X)}+ \operatorname{Tr}(XH_{t}).\] (11)

For an arbitrary graph \(\mathcal{G}\), solving (11) might be difficult. Theorems 3.1 and 3.2 show _embarrassingly parallelizable_ explicit solutions to the subproblem (11) for tridiagonal and banded sparsity patterns.

**Theorem 3.1** (Explicit solution of (11) for tridiagonal structures/chain graph).: _Let the sparsity structure \(\mathcal{G}\) be a chain with edges \(E_{\mathcal{G}}=\{(i,j):|i-j|\leq 1,1\leq i,j\leq n\}\). Also, let \(H\in S_{n}(\mathcal{G})\) be such that any submatrix of \(H\) corresponding to a complete subgraph of \(\mathcal{G}\) is positive definite, then the solution of (11) is given by \(\hat{X}=LDL^{T}\), where the unit lower bidiagonal matrix \(L\) and diagonal matrix \(D\) have the following non-zero entries:_

\[L_{jj}=1,\ L_{j+1j}=-\frac{H_{j+1j}}{H_{j+1j+1}},\ \ D_{jj}^{-1}=H_{jj}-\frac{H_{j+ 1j}^{2}}{H_{j+1j+1}},\quad j\leq n-1\ \&\ D_{nn}^{-1}=H_{nn}\] (12)

Computing this explicit solution involves conducting paralellizable operations on \(2\times 2\) principle submatrices (highlighted in red) of the tridiagonal matrix \(H\) to find the \(\hat{X}\) as shown in the following \(3\times 3\) example:

\[H= \left(\begin{array}{cc}\boxed{H_{11}}&H_{12}\\ H_{21}&H_{22}\\ \hline 0&H_{32}\end{array}\right)=\left(\begin{array}{ccc}\tilde{H}_{11}& \tilde{H}_{12}&0\\ \tilde{H}_{21}&\tilde{H}_{22}&\tilde{H}_{23}\\ 0&\tilde{H}_{32}&\tilde{H}_{33}\end{array}\right)+\left(\begin{array}{ccc}g_ {1}^{2}&g_{1}g_{2}&0\\ g_{1}g_{2}&g_{2}^{2}&g_{2}g_{3}\\ 0&g_{2}g_{3}&g_{3}^{2}\end{array}\right)\] (13) \[\to\hat{X}=\left(\begin{array}{cc}\boxed{1}&0&0\\ \frac{H_{21}}{H_{22}}&1&0\\ \hline 0&-\frac{H_{32}}{H_{33}}&1\end{array}\right)\left(\begin{array}{ccc} \boxed{H_{11}-\frac{H_{21}}{H_{22}}}&0&0\\ \hline 0&H_{22}-\frac{H_{23}^{2}}{H_{33}}&0\\ 0&0&H_{33}\end{array}\right)\left(\begin{array}{ccc}\boxed{1}&-\frac{H_{21} }{H_{22}}&0\\ \hline 0&1&-\frac{H_{32}}{H_{33}}\\ 0&0&1\end{array}\right)\]

Conducting these operations take \(\mathcal{O}(n)\) time and memory complexity, and similarly the descent direction can be found sequentially by \(X_{t}g_{t}=L(D(L^{T}g_{t}))\), which can take \(\mathcal{O}(n)\) time complexity, due to unit lower bidiagonal structure of \(L\), furthermore, these operations can be easily parallelized. We also generalize the explicit solution to banded sparsity structures with band size \(b\).

**Theorem 3.2** (Explicit solution of (11) for banded structures).: _Let the sparsity pattern \(\mathcal{G}\) be a banded matrix of band size \(b\), i.e. \(E_{\mathcal{G}}=\{(i,j):|i-j|\leq b,1\leq i,j\leq n\}\). For every vertex \(j\), let \(I_{j}=\{j+1,\ldots,j+b\}\). Then \(X_{t}=LDL^{T}\) is the solution of (11) with nonzero entries of \(L\) and \(D\) defined as follows :_

\[L_{jj}=1,\ L_{I_{j}j}=-H_{I_{j}I_{j}}^{-1}H_{I_{j}j},\quad D_{jj}^{-1}=(H_{jj} -H_{I_{j}j}^{T}H_{I_{j}I_{j}}^{-1}H_{I_{j}j}),\ 1\leq j\leq n.\] (14)

_where, \(H\in S_{n}(\mathcal{G})\) any submatrix of \(H\) corresponding to a complete subgraph of \(\mathcal{G}\) is positive definite._

Note that Theorem 3.1 is a special case of Theorem 3.2 when \(b\) is set to \(1\), and the proof for Theorem 3.2 is given in Appendix A.1. Computing the above solution requires solving \(n\) linear systems of size \(b\) (which is small) as shown in Algorithm 2, and takes \(\mathcal{O}((n-b+1)b^{3})\) flops. Since \(b\ll n\), the number of flops is \(\mathcal{O}(n)\).

### Regret bound analysis of SONew

The following theorem establishes optimal regret guarantee [26] for SONew in the online convex optimization framework mentioned in Section 3.1.

**Theorem 3.3**.: _When \(\mathcal{G}=\) tridiagonal/chain graph as defined in Theorem 3.1, then setting \(\epsilon=\epsilon G_{\infty}\sqrt{T}\), \(\lambda_{t}=G_{\infty}\sqrt{t}\) and \(\eta_{t}=\frac{D_{2}}{\tilde{\epsilon}\sqrt{n}}\) in Algorithm 1, where \(\|w_{t}-w^{*}\|_{2}\leq D_{2}\), \(\|g_{t}\|_{\infty}\leq G_{\infty}\) incurs a regret \(R_{T}=\mathcal{O}(\sqrt{n}G_{\infty}D_{2}\sqrt{T})\)._

The proof sketch involves deriving an explicit expression for entries of \(X_{t}^{-1}\) in Lemma A.2, to upper bound the term \((w_{t}-w^{*})^{T}(X_{t}^{-1}-X_{t-1}^{-1})(w_{t}-w^{*})\) in regret upper bound (3). Upper bounding \(\frac{\eta}{2}\sum_{t=1}^{T}g_{t}^{T}X_{t}g_{t}\) involves using the Loewner order \(X_{t}\preceq\left\|X_{t}\right\|_{2}I_{n}\preceq\|X_{t}\|_{\infty}I_{n}\). A detailed proof sketch and proof is given in Appendix A.2. We note that though the regret bound presented here is for convex losses, there are connections to non-convex convergence guarantees by using OCO (online convex optimization) learners, presented in Appendix A.2.5. While our main focus is on deep neural network training, which is typically non-convex, we also conducted convex experiments in Table 9.

### Numerical Stability of SONew

In Theorem 3.1 and Theorem 3.2, as mentioned, any submatrix of \(H_{t}\) corresponding to a complete subgraph of \(\mathcal{G}\) should be positive definite, however, in practice, due to finite precision, each entry of \(H\) is inherently perturbed with an error proportional to \(\mathcal{O}(\epsilon_{mach})\), where \(\epsilon_{mach}\) is machine epsilon [27]. We notice in practice that the subtraction operation in \(D_{jj}^{-1}=S_{jj}=H_{jj}-H_{j+1j}^{2}/H_{j+1j+1}\) (line 7 Algorithm 2), which has a condition number \(\kappa_{sub}=|H_{jj}|/|S_{jj}|\), can be high as \(S_{jj}\) can be arbitrarily low due to near singular submatrices \(\begin{bmatrix}H_{ii}&H_{ii+1}\\ H_{i+1i}&H_{i+1i+1}\end{bmatrix}\). Thus small perturbation in \(H\) can lead to high perturbations in the preconditioner \(\hat{X}\). We formalize this notion by deriving an end-to-end componentwise condition number (pg. 135, problem 7.11 in [27]) of Sparsified_Inverse in Theorem A.10,Appendix A.3. To reduce this condition number upper bound and be robust to perturbations in \(H_{t}\) caused by finite precision, for a tridiagonal graph \(\mathcal{G}\), we can remove edges \((j,j+1)\) which correspond to low \(S_{jj}<\gamma\), where \(\gamma\geq 0\) denotes a tolerance parameter. We show in Theorem A.11,Appendix A.3 that this reduces the condition number upperbound of Sparsified_Inverse. Furthermore, we generalize this to banded sparsity pattern in Algorithm 3,Appendix A.3.

## 4 Related Work

Online Newton method is a second order method in online convex optimization framework with properties such as scale invariance [35] and logarithmic regrets in exp-concave and strongly convex functions [25; 26]. However, it has a time complexity of \(\mathcal{O}(n^{2})\), making it infeasible for large \(n\). However, introduction of LogDet divergence measure in SONew allows us to set different sparsity graphs as \(\mathcal{G}\) such as banded graph with band-size \(b\), for which our preconditioning process is more computationally efficient with a time complexity of \(\mathcal{O}(b^{3}(n-b+1))\) compared to online-newton method \(\mathcal{O}(n^{2})\).

Shampoo [24; 5] approximates full gradient statistics matrix using Kronecker factored preconditioners to reduce the memory and time complexity from \(\mathcal{O}(n^{2})\) to \(\mathcal{O}(d_{1}^{2}+d_{2}^{2})\) and \(\mathcal{O}(d_{1}^{3}+d_{2}^{3})\) respectively. Here, \(n=d_{1}d_{2}\) denotes number of parameters for a linear layer of dimensions \(d_{1}\times d_{2}\). The time complexity of matrix inversion takes a heavy toll in Shampoo's compute time even with the Kronecker product assumption on the preconditioner, whereas, our method has a time complexity of \(\mathcal{O}(b^{3}d_{1}d_{2})\) quadratic in dimensions of the linear layer (note that \(b=1\) for tridiagonal structure).

KFAC [38], similar to Shampoo, uses Kronecker factored preconditioning, but to approximate the Fisher-information matrix. FishLeg [20] instead approximates the inverse Fisher matrix directly by expressing it in terms of the solution to an optimisation problem. Both these methods have memory and time complexity similar to Shampoo. In this work, we compare with Shampoo among the class of Kronecker factored optimizers due to its widespread testing and adoption within the community [46]. We also point the readers to Eva [52], a concurrent work aimed at devising memory efficient optimizer by maintaining rank one matrix approximation to the Kronecker factors of KFAC matrices. For completeness, we include comparison with KFAC, FishLeg, and Eva on Autoencoder benchmark.

There is prior work [35; 36] in reducing the complexity - \(\mathcal{O}(n^{2})\) flops of Online Newton Step (ONS) to \(\mathcal{O}(n)\) flops using sketching. These ONS variants maintain a low rank approximation of \(H_{t}\) (as in Algorithm 1) and updating it with a new gradient \(g_{t}\) at every iteration requires conducting SVD [36]/orthonormalization [35] of a tall and thin matrix in \(\mathbb{R}^{n\times r}\), where \(r\) denotes the rank of approximation of \(H_{t}\). In Section 5, we conduct large scale experiments and compare SONew against rfdSON [37] as it's more stable than Oja-SON [35].

\begin{table}
\begin{tabular}{|l|l|l|} \hline
**Optimizer** & **Time complexity** & **Memory complexity** \\ \hline Adam & \(\mathcal{O}(d_{1}d_{2})\) & \(d_{1}d_{2}\) \\ rfdSON(m) & \(\mathcal{O}(m^{2}d_{1}d_{2})\) & \(md_{1}d_{2}\) \\ Shampoo & \(\mathcal{O}(d_{1}^{3}+d_{2}^{3})\) & \((d_{1}^{2}+d_{2}^{2})\) \\ tridiag-SONew & \(\mathcal{O}(d_{1}d_{2})\) & \(2d_{1}d_{2}\) \\ band-4-SONew & \(\mathcal{O}(d_{1}d_{2})\) & \(5d_{1}d_{2}\) \\ \hline \end{tabular}
\end{table}
Table 1: Consider preconditioning a \(d_{1}\times d_{2}\) parameter matrix. Time complexity of tridiag and banded SONew inverse scale linearly with number of parameters, but, Shampoo is cubic in the dimensions of the matrix. Memory used to store second-moments of gradients by tridiag-SONew can be significantly lower than Shampoo, for e.g. if \(d_{1}=4d_{2}\), then Shampoo takes \(>2\) times more memory.

LogDet problem in equation 11 is closely related to the Maximum Determinant Matrix Completion (MDMC) [4; 49]. The MDMC problem is the dual of LogDet problem (11), and has explicit solutions for chordal graphs [4]. Thus the explicit solutions in (14) are the same as the ones proved in [4]. Also, we noticed that the tridiagonal explicit solution has been used previously in KFAC [38] in the context of a gaussian graphical model interpretation of gradients, specifically, KFAC used a block-tridiagonal preconditioner to incorporate correlation within consecutive layers.

## 5 Experimental Results

We describe our experiments on standard Autoencoder benchmark [42] trained on MNIST dataset [12], Vision Transformer [13] on Imagenet training, GraphNetwork [6; 21] on OGBG-molpcha dataset [30], and a Large Language Model [47]. For all second order optimizers, we use grafting [2], a technique used to transfer step size between optimization algorithms. Specifically, given an update \(v_{1}\) of Optimizer-1 and \(v_{2}\) of Optimizer-2, grafting allows to use the direction suggested by Optimizer-2 with step size suggested by Optimizer-1. The final update is given by \(\frac{\|v_{1}\|}{\|v_{2}\|}\cdot v_{2}\). Grafting has been shown to take advantage of a tuned optimizer step size and improve performance. For SONew and rfdSON, we use Adam grafting - using Adam optimizer step size \(\|v_{1}\|\) with SONew/rfdSON direction \(\nicefrac{{v_{2}}}{{\|v_{2}\|}}\). For Shampoo, we use its default RMSProp grafting. We couldn't find rfdSON official implementation, so we use our own implementation using which we reproduced the numbers on convex losses (Appendix A.4) reported in their paper [37].

### Autoencoder benchmark

**Setup:** We use three sparsity patterns for SONew - a) diagonal sparsity, resulting in a diagonal preconditioner similar to adaptive first order methods like Adam and Adagrad; b) tridiagonal sparsity, corresponding to a chain graph; and c) banded sparsity, represented by "band-\(k\)" in tables and figures for band size of \(k\). We compare SONew against widely used first order methods including SGD [32]), SGD with Momentum [41], Nesterov [40], Adagrad [14], Adam [33], and Rmsprop [48]. We also compare with rfdSON [37], a recently proposed memory efficient second order optimizer and with Shampoo [24], a state of the art second-order optimizer used in practice, albeit with considerable memory and time requirements. Because of space constraint, we report only the best performing first order methods while include the entire set in the appendix. As previously mentioned, rfdSON maintains a low rank approximation of the Online Newton method's statistics matrix \(\sum_{i}g_{i}g_{i}^{T}\). We observed rfdSON with adam grafting always performed better than without grafting, hence report the corresponding numbers. We evaluate rfdSON with rank \(m\) approximation, denoted as rfdSON(\(m\)), which requires \((m+1)*\#params\) space when using grafting. For a fair comparison with tridiag-SONew and band-4-SONew, we test rfdSON with \(m=1\) and \(m=4\), respectively. For shampoo, computing preconditioner at every step could be infeasible, instead it is computed every \(t\) steps - referred to as Shampoo(\(t\)). Section 3.3 compares time and memory complexities of rfdSON, Shampoo, tridiag-SONew, band-4-SONew. Note that \(d_{1}^{2}+d_{2}^{2}\geq 2d_{1}d_{2}\ \forall d_{1},d_{2}\), thus memory used by tridiag-SONew is never more than Shampoo. We use a \(2.72\)M parameters Autoencoder and each experiment is performed using one V100 GPU having \(16\) GB memory. Further setup details are given in Appendix A.4.

**Results:** In Table 2 we observe that among first order methods, diag-SONew performs the best while taking same amount of time. Increasing the number of edges in the sparsity graph to tridiag or banded sparsity with band size \(4\) enhances the performance further. Tridiag-SONew runs \(5\times\) faster than Shampoo at a marginal cost to the loss - even when Shampoo updates preconditioner once every 20 steps. Using same space, rfdSON performs considerably worse than SONew. To test the numerical stability and robustness of SONew, we reduce the precision to bfloat16 and conduct similar

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|} \hline \hline
**Optimizer** & \multicolumn{4}{c|}{**First Order Methods**} & \multicolumn{4}{c|}{**Second Order Methods**} \\ \hline  & **Adagrad** & **RMSProp** & **Adam** & **diag-SONew** & **Shampoo(20)** & **rfdSON(1)** & **rfdSON(4)** & **tridiag-SONew** & **band-4-SONew** \\ \hline Train CE loss & 54.393 & 53.330 & 53.591 & 53.025 & 50.702 & 56.21 & 55.55 & 51.723 & 51.357 \\ \hline Time(s) & 62 & 62 & 62 & 63 & 371 & 85 & 300 & 70 & 260 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **float32 experiments on Autoencoder benchmark. We observe that diag-SONew performs the best among all first order methods while taking similar time. tridiag and band-4 SONew perform significantly better than first order methods while requiring similar linear space and time. Shampoo performs best but takes \(\mathcal{O}(d_{1}^{3}+d_{2}^{2})\) time for computing preconditioner of a linear layer of size \(d_{1}\times d_{2}\), whereas our methods take \(\mathcal{O}(d_{1}d_{2})\) time, as mentioned in Section 3.3. rfdSON takes similar space as SONew but performs considerably worse.**experiments in Appendix A.4.4 (Table 8 ). We notice that SONew undergoes the least degradation in performance compared to all other optimizers. We refer the reader to Appendix A.4.4 for a thorough comparison and study of bfloat16 experiments. In Figure 3 we plot the loss curves of all the baselines and SONew for float32 experiments. Moreover, in Appendix A.4.1 Table 4 we provide ablation on performance of SONew with varying batch sizes.

**Comparison with other baselines:** We further compare SONew with KFAC [38], FishLeg [20], and Eva [52] for completeness. Since these methods lack a JAX implementation, we adopted the authors' official Pytorch implementations. When we attempted to integrate their code with our Autoencoder benchmark, the results were unsatisfactory; for instance, FishLeg recorded a loss of approximately \(\sim 60.0\). This was notably unexpected as it underperformed Adam, a benchmark that the authors themselves compared against. Given these results and to minimize modifications to the official code, we decided to test our optimizer, SONew, directly on their provided autoencoder architecture. We present the results in Appendix A.4.4 and notice that SONew outperforms these baselines as well by a large margin.

### VIT and GraphNetwork benchmark

**Setup:** We compare tridiag-SONew with Momentum, RMSProp, and Adam, on VIT (\(\sim\)22M parameters) and GraphNetwork (\(\sim\)3.5M parameters) benchmark. For each experiment, we search over \(200\) hyperparameters using \(4\)\(16\) GB TPUs (v2) for each run. In order to conduct a fair comparison of the running times, we executed the optimal hyperparameter configurations on \(4\) 32GB TPUs (v4) [31]. This is because certain operations, including reshapes and transposes, are not optimized on TPUs (v2). Consequently, methods like rfdSON, Shampoo or SONew, which utilize these operations, could potentially be disadvantaged if TPUs (v2) were used, skewing the comparative results. All memory-efficient methods, including rfdSON, first-order methods, and SONew, exhibit similar runtimes, with differences of approximately \(\sim 5\%\). For ViT, we evaluate their performance based on the same number of steps, as this also effectively compares wall-clock time. However, for GraphNetwork, we train Shampoo for 20% fewer steps to achieve a comparable wall-clock time.

**Results:** We plot the runs that give best validation error rate (for VIT) or validation average precision (for GraphNetwork) in Figure 1. tridiag-SONew requires \(\sim 10\%\) less steps to reach the same performance as Adam for VIT, and \(\sim 30\%\) less steps for GraphNetwork benchmark. Training for the same number of steps, we get \(\sim 0.7\%\) better relative validation error for VIT and \(\sim 3.4\%\) better relative validation average precision for GraphNetwork. On GraphNetwork benchmark, tridiag-SONew performs \(1.3\%\) relatively worse in average precision compared to Shampoo, while being \(1.25\times\) faster. On VIT benchmark, Shampoo doesn't fit in a 16 GB TPU v2. Its statistics require 155M entries (\(\sim 7\times\#params\)) while tridiag-SONew requires only 44M entries (\(2\times\#params\)). Hence, we could not tune it. rfdSON takes same memory but slightly more time because of its SVD computations. We also notice rfdSON performs worse than Adam on both benchmarks; we leave a thorough investigation of this behavior as a future work.

We show in Appendix A.4 that corresponding to the best validation runs, tridiag-SONew optimizer's training loss is also less than that of Adam. Furthermore, from an optimization point of view we also show in Appendix A.4 that among all the \(200\) hyperparameter sweeps, the best training loss of tridiag-SONew is \(9\%\) relatively better on ViT and \(80\%\) relatively better on GraphNN than that of Adam. We further compare Adam and tridiag-SONew on a 248M parameter Transformer Model in Appendix A.4.4. In next subsection, we present results on a decoder only large scale Language Model.

### Experiments on Language Models

**Setup:** Owing to SONew's scalability, we test it on a Large Language Model (LLM) [47] with 1 billion parameters. We compare SONew with AdaFactor (without factoring), a commonly used first order optimizer for training LLMs [51; 11]. AdaFactor is similar to Adam except that in addition it offers benefits like "parameter scaling", which has an effect of layerwise damping of the learning rate. We defer the reader to [45] for more details. We trained the LLM for 5B tokens with a batch size of \(64k\) tokens. All experiments were performed on \(16\) TPU v4s. To support efficient training of large models, we implemented a sharded tridiag-SONew following model parallelism approach.

**Results:** We report the experiment in Figure 3 where we find that SONew beats Adafactor by a large margin. Specifically, SONew achieves the same log perplexity using \(26\%\) less steps. Moreover, using the same number of tokens, SONew achieves \(1.7\%\) relative better performance on train loss, leading to \(1.35\times\) speedup. This shows the potential of SONew as a scalable optimizer that can be used to train large models while using second order information.

## 6 Conclusions and Future Work

In this paper we have introduced a novel Sparsified Online Newtwon (SONew) method that yields a computationally efficient sparse preconditioner that can effectively train very large DNNs. The time and memory complexity of SONew is linear in the number of parameters, unlike current Kronecker-factorization based second-order methods for training deep networks. Our experimental results show that SONew uses similar time as first order methods and achieves much better validation and training performance in various benchmarks. In the future, we plan to explore different sparsity graphs for which efficient solutions exist for the LogDet subproblem (11) and develop corresponding regret bound analyses. Some of the limitations of SONew include: 1) explicit solutions akin to Theorem 3.1 & 3.2 need not exist for all sparsity graphs \(\mathcal{G}\); 2) Not all graphs allow for efficient optimizer implementation; 3) Among graphs permitting efficient optimizer implementation--like tridiagonal sparsity--the ordering of parameters remains unexplored. An alternative ordering might position closely related parameters adjacently, potentially enhancing performance; 4) Comprehensive exploration of methods to scale final updates is needed. While we employ grafting [2], other techniques, such as clipping [45; 38], merit investigation.

## References

* Agarwal et al. [2019] N. Agarwal, B. Bullins, X. Chen, E. Hazan, K. Singh, C. Zhang, and Y. Zhang. Efficient full-matrix adaptive regularization. In _International Conference on Machine Learning_, pages 102-110. PMLR, 2019.
* Agarwal et al. [2022] N. Agarwal, R. Anil, E. Hazan, T. Koren, and C. Zhang. Learning rate grafting: Transferability of optimizer tuning, 2022.
* Amari [1998] S.-I. Amari. Natural gradient works efficiently in learning. _Neural computation_, 10(2):251-276, 1998.
* Andersen et al. [2013] M. S. Andersen, J. Dahl, and L. Vandenberghe. Logarithmic barriers for sparse matrix cones. _Optimization Methods and Software_, 28(3):396-423, 2013.
* Anil et al. [2020] R. Anil, V. Gupta, T. Koren, K. Regan, and Y. Singer. Scalable second order optimization for deep learning. _arXiv preprint arXiv:2002.09018_, 2020.
* Battaglia et al. [2018] P. W. Battaglia, J. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, V. Zambaldi, M. Malinowski, A. Tacchetti, D. Raposo, A. Santoro, R. Faulkner, C. Gulcehre, F. Song, A. Ballard, J. Gilmer, G. Dahl, A. Vaswani, K. Allen, C. Nash, V. Langston, C. Dyer, N. Heess, D. Wierstra, P. Kohli, M. Botvinick, O. Vinyals, Y. Li, and R. Pascanu. Relational inductive biases, deep learning, and graph networks, 2018. URL https://arxiv.org/abs/1806.01261.
* Bollhofer et al. [2019] M. Bollhofer, A. Eftekhari, S. Scheidegger, and O. Schenk. Large-scale sparse inverse covariance matrix estimation. _SIAM Journal on Scientific Computing_, 41(1):A380-A401, 2019.
* Bregman [1967] L. M. Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. _USSR computational mathematics and mathematical physics_, 7(3):200-217, 1967.
* Broyden [1967] C. G. Broyden. Quasi-Newton methods and their application to function minimisation. _Mathematics of Computation_, 21(99):368-381, 1967.
* Cesa-Bianchi et al. [2001] N. Cesa-Bianchi, A. Conconi, and C. Gentile. On the generalization ability of on-line learning algorithms. _Advances in neural information processing systems_, 14, 2001.
* Chowdhery et al. [2022] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai, T. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D. Eck, J. Dean, S. Petrov, and N. Fiedel. Palm: Scaling language modeling with pathways, 2022.
* Deng [2012] L. Deng. The MNIST database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.
* Dosovitskiy et al. [2020] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2020. URL https://arxiv.org/abs/2010.11929.
* Duchi et al. [2011] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. _Journal of Machine Learning Research_, 12(61):2121-2159, 2011. URL http://jmlr.org/papers/v12/duchi11a.html.
* Duchi et al. [2011] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. _Journal of machine learning research_, 12(7), 2011.
* Fattahi and Sojoudi [2019] S. Fattahi and S. Sojoudi. Graphical lasso and thresholding: Equivalence and closed-form solutions. _Journal of machine learning research_, 2019.

* Fletcher [1970] R. Fletcher. A new approach to variable metric algorithms. _The computer journal_, 13(3):317-322, 1970.
* Fletcher [1991] R. Fletcher. A new variational result for quasi-Newton formulae. _SIAM Journal on Optimization_, 1(1):18-21, 1991.
* Friedman et al. [2008] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. _Biostatistics_, 9(3):432-441, 2008.
* Garcia et al. [2023] J. R. Garcia, F. Freddi, S. Fotiadis, M. Li, S. Vakili, A. Bernacchia, and G. Hennequin. Fisherlegendre (fishleg) optimization of deep neural networks. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=c91AOPvQHS.
* Godwin* et al. [2020] J. Godwin*, T. Keck*, P. Battaglia, V. Bapst, T. Kipf, Y. Li, K. Stachenfeld, P. Velickovic, and A. Sanchez-Gonzalez. Jraph: A library for graph neural networks in jax., 2020. URL http://github.com/deepmind/jaraph.
* Goldfarb [1970] D. Goldfarb. A family of variable-metric methods derived by variational means. _Mathematics of computation_, 24(109):23-26, 1970.
* Goldfarb et al. [2020] D. Goldfarb, Y. Ren, and A. Bahamou. Practical quasi-Newton methods for training deep neural networks. _Advances in Neural Information Processing Systems_, 33:2386-2396, 2020.
* Gupta et al. [2018] V. Gupta, T. Koren, and Y. Singer. Shampoo: Preconditioned stochastic tensor optimization. In _International Conference on Machine Learning_, pages 1842-1850. PMLR, 2018.
* Hazan et al. [2007] E. Hazan, A. Agarwal, and S. Kale. Logarithmic regret algorithms for online convex optimization. _Machine Learning_, 69(2):169-192, 2007.
* Hazan et al. [2016] E. Hazan et al. Introduction to online convex optimization. _Foundations and Trends(r) in Optimization_, 2(3-4):157-325, 2016.
* Higham [2002] N. J. Higham. _Accuracy and stability of numerical algorithms_. SIAM, 2002.
* Hinton et al. [2012] G. Hinton, N. Srivastava, and K. Swersky. Neural networks for machine learning lecture 6a overview of mini-batch gradient descent. _Cited on_, 14(8):2, 2012.
* Hsieh et al. [2013] C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, P. K. Ravikumar, and R. Poldrack. BIG & QUIC: Sparse inverse covariance estimation for a million variables. _Advances in neural information processing systems_, 26, 2013.
* Hu et al. [2020] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec. Open graph benchmark: Datasets for machine learning on graphs, 2020. URL https://arxiv.org/abs/2005.00687.
* Jouppi et al. [2023] N. P. Jouppi, G. Kurian, S. Li, P. Ma, R. Nagarajan, L. Nai, N. Patil, S. Subramanian, A. Swing, B. Towles, C. Young, X. Zhou, Z. Zhou, and D. Patterson. Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings, 2023.
* 466, 1952. doi: 10.1214/aoms/1177729392. URL https://doi.org/10.1214/aoms/1177729392.
* Kingma and Ba [2014] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Kulis et al. [2009] B. Kulis, M. A. Sustik, and I. S. Dhillon. Low-rank kernel learning with Bregman matrix divergences. _Journal of Machine Learning Research_, 10(2), 2009.
* Luo et al. [2016] H. Luo, A. Agarwal, N. Cesa-Bianchi, and J. Langford. Efficient second order online learning by sketching. _Advances in Neural Information Processing Systems_, 29, 2016.
* Luo et al. [2019] L. Luo, C. Chen, Z. Zhang, W.-J. Li, and T. Zhang. Robust frequent directions with application in online learning. _The Journal of Machine Learning Research_, 20(1):1697-1737, 2019.

* Luo et al. [2019] L. Luo, C. Chen, Z. Zhang, W.-J. Li, and T. Zhang. Robust frequent directions with application in online learning. _Journal of Machine Learning Research_, 20(45):1-41, 2019. URL http://jmlr.org/papers/v20/17-773.html.
* Martens and Grosse [2015] J. Martens and R. Grosse. Optimizing neural networks with Kronecker-factored approximate curvature. In _International conference on machine learning_, pages 2408-2417. PMLR, 2015.
* Merity et al. [2016] S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models, 2016.
* Nesterov [1983] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence \(o(1/k^{2})\). 1983.
* Qian [1999] N. Qian. On the momentum term in gradient descent learning algorithms. _Neural Networks_, 12(1):145-151, 1999. ISSN 0893-6080. doi: https://doi.org/10.1016/S0893-6080(98)00116-6. URL https://www.sciencedirect.com/science/article/pii/S0893608098001166.
* Schmidhuber [2015] J. Schmidhuber. Deep learning in neural networks: An overview. _Neural Networks_, 61:85-117, jan 2015. doi: 10.1016/j.neunet.2014.09.003. URL https://doi.org/10.1016%2Fj.neunet.2014.09.003.
* Shalev-Shwartz et al. [2012] S. Shalev-Shwartz et al. Online learning and online convex optimization. _Foundations and Trends(r) in Machine Learning_, 4(2):107-194, 2012.
* Shanno [1970] D. F. Shanno. Conditioning of quasi-Newton methods for function minimization. _Mathematics of computation_, 24(111):647-656, 1970.
* Shazeer and Stern [2018] N. Shazeer and M. Stern. Adafactor: Adaptive learning rates with sublinear memory cost, 2018.
* Shi et al. [2023] H.-J. M. Shi, T.-H. Lee, S. Iwasaki, J. Gallego-Posada, Z. Li, K. Rangadurai, D. Mudigere, and M. Rabbat. A distributed data-parallel pytorch implementation of the distributed shampoo optimizer for training neural networks at-scale, 2023.
* So et al. [2022] D. R. So, W. Manke, H. Liu, Z. Dai, N. Shazeer, and Q. V. Le. Primer: Searching for efficient transformers for language modeling, 2022.
* Tieleman and Hinton [2012] T. Tieleman and G. Hinton. Lecture 6.5--rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning. 2012.
* Vandenberghe et al. [2015] L. Vandenberghe, M. S. Andersen, et al. Chordal graphs and semidefinite optimization. _Foundations and Trends(r) in Optimization_, 1(4):241-433, 2015.
* Vaswani et al. [2022] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need, 2023.
* Wang et al. [2022] T. Wang, A. Roberts, D. Hesslow, T. L. Scao, H. W. Chung, I. Beltagy, J. Launay, and C. Raffel. What language model architecture and pretraining objective work best for zero-shot generalization?, 2022.
* Zhang et al. [2023] L. Zhang, S. Shi, and B. Li. Eva: Practical second-order optimization with kronecker-vectorized approximation. In _The Eleventh International Conference on Learning Representations_, 2023. URL https://openreview.net/forum?id=_Mic8V96Voy.
* Zhang et al. [2018] R. Zhang, S. Fattahi, and S. Sojoudi. Large-scale sparse inverse covariance estimation via thresholding and max-det matrix completion. In _International Conference on Machine Learning_, pages 5766-5775. PMLR, 2018.
* Zinkevich [2003] M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In _Proceedings of the 20th international conference on machine learning (icml-03)_, pages 928-936, 2003.

Supplementary material

* Properties of LogDet subproblem
* Regret bound analysis
* Regret bound decomposition
* Properties of tridiagonal preconditioner
* Upperbounding Regret
* Non-convex guarantees
* Numerical stability
* Condition number analysis
* Numerically Stable SONew proof
* Additional Experiments, ablations, and details
* Memory Requirements
* Hyperparaeter search space
* Additional Experiments
* Convex experiments

### Properties of LogDet subproblem

Proof of Theorem 3.2: The optimality condition of (11) is \(P_{\mathcal{G}}(X^{-1})=P_{\mathcal{G}}(H)\), \(X\in S_{n}^{++}(\mathcal{G})\). Let \(Z=L^{-T}D^{-1}L^{-1}\), then \(P_{\mathcal{G}}(Z)=H\)

\[ZL=L^{-T}D^{-1}\implies ZLe_{j}=L^{-T}D^{-1}e_{j}\]

Let \(J_{j}=I_{j}\cup j\), where \(I_{j}=\{j+1,\ldots,j+b\}\) as defined in the theorem, then select \(J_{j}\) indices of vectors on both sides of the second equality above and selecting the \(J_{j}\) indices :

\[\begin{bmatrix}Z_{jj}&Z_{jI_{j}}\\ Z_{I_{j}j}&Z_{J_{j}J_{j}}\end{bmatrix}\begin{bmatrix}1\\ L_{I_{j}}\end{bmatrix}=\begin{bmatrix}1/d_{jj}\\ 0\end{bmatrix}\] (15)

Note that \(L^{-T}\) is an upper triangular matrix with ones in the diagonal hence \(J_{j}^{th}\) block of \(L^{-T}e_{j}\) will be \([1,0,0,\ldots]\). Also, since \(P_{\mathcal{G}}(Z)=H\)

\[\begin{bmatrix}Z_{jj}&Z_{jI_{j}}\\ Z_{I_{j}j}&Z_{J_{j}J_{j}}\end{bmatrix}=\begin{bmatrix}H_{jj}&H_{jI_{j}}\\ H_{I_{j}j}&H_{J_{j}J_{j}}\end{bmatrix}\]

Substituting this in the linear equation 15

\[\begin{bmatrix}H_{jj}&H_{jI_{j}}\\ H_{I_{j}j}&H_{J_{j}J_{j}}\end{bmatrix}\begin{bmatrix}1\\ L_{I_{j}}\end{bmatrix} =\begin{bmatrix}1/d_{jj}\\ 0\end{bmatrix}\] \[\begin{bmatrix}H_{jj}&H_{jI_{j}}\\ H_{I_{j}j}&H_{J_{j}J_{j}}\end{bmatrix}\begin{bmatrix}d_{jj}\\ d_{jj}&L_{I_{j}}\end{bmatrix} =\begin{bmatrix}1\\ 0\end{bmatrix}\] \[H_{jj}d_{jj}+d_{jj}H_{I_{j}^{\prime}j}^{T}L_{I_{j}j} =1\] \[H_{I_{j}j}d_{jj}+d_{jj}H_{I_{j}j}^{T}L_{I_{j}j} =0\]

The lemma follows from solving the above equations. Note that here we used that lower triangular halves of matrices \(L\) and \(H\) have the same sparsity patterns, which follows from the fact that banded 

[MISSING_PAGE_FAIL:15]

The first summation can be decomposed as follows

\[\sum_{t=1}^{T}\left(\|w_{t}-w^{*}\|_{X_{t}^{-1}}^{2}-\|w_{t+1}-w^{*} \|_{X_{t}^{-1}}^{2}\right)\] \[=\left(\|w_{1}-w^{*}\|_{X_{1}^{-1}}^{2}-\|w_{T+1}-w^{*}\|_{X_{T}^{- 1}}^{2}\right)\] \[+\sum_{t=1}^{T-1}(w_{t+1}-w^{*})^{T}(X_{t+1}^{-1}-X_{t}^{-1})(w_{t +1}-w^{*})\]

Substituting the above identity in the Equation (19) proves the lemma.

Let \(R_{T}\leq T_{1}+T_{2}+T_{3}\), where

* \(T_{1}=\frac{1}{2\eta}\cdot(\|w_{1}-w^{*}\|_{X_{1}^{-1}}^{2}-\|w_{T+1}-w^{*}\|_ {X_{T}^{-1}})\)
* \[T_{2}=\frac{1}{2\eta}\cdot\sum_{t=1}^{T-1}(w_{t+1}-w^{*})^{T}(X_{t+1}^{-1}-X_{ t}^{-1})(w_{t+1}-w^{*})\] (21)
* \(T_{3}=\sum_{t=1}^{T}\frac{\eta}{2}\cdot g_{t}^{T}X_{t}g_{t}\)

#### a.2.2 Properties of tridiagonal preconditioner

In this subsection, we derive properties of the tridiagonal preconditioner obtained from solving the LogDet subproblem (11) with \(\mathcal{G}\) set to a chain graph over ordered set of vertices \(\{1,\dots,n\}\):

\[X_{t} =\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}}-\log \det\left(X\right)+\operatorname{Tr}(XH_{t})\] (22) \[=\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}}\mathrm{D }_{\ell\mathrm{d}}\left(X,H_{t}^{-1}\right)\] (23)

The second equality holds true only when \(H_{t}\) is positive definite. Although in Algorithm 1 we maintain a sparse \(H_{t}=H_{t-1}+P_{\mathcal{G}}(g_{t}g_{t}^{T}/\lambda_{t})\), \(H_{0}=\epsilon I_{n}\) which is further used in (22) to find the preconditioner \(X_{t}\), our analysis assumes the full update \(H_{t}=H_{t-1}+g_{t}g_{t}^{T}/\lambda_{t}\), \(H_{0}=\epsilon I_{n}\) followed by preconditioner \(X_{t}\) computation using (23). Note that the preconditioners \(X_{t}\) generated both ways are the same, as shown in Section 3.2.

The following lemma shows that the inverse of tridiagonal preconditioners used in Algorithm 1, will restore \(H_{i,j}\), when \((i,j)\) fall in the tridiagonal graph, else, the expression is related to product of \(H_{i+k,i+k+1}\) corresponding to the edges in the path from node \(i\) to \(j\) in chain graph. This lemma will be used later in upperbounding \(T_{2}\).

**Lemma A.2** (_Inverse of tridiagonal preconditioner_).: _If \(\mathcal{G}=\) chain/tridiagonal graph and \(\hat{X}=\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}}\mathrm{D}_{ \ell\mathrm{d}}\left(X,H^{-1}\right)\), then the inverse \(\hat{X}^{-1}\) has the following expression_

\[(\hat{X}^{-1})_{ij}=\begin{cases}H_{ij}&|i-j|\leq 1\\ \frac{H_{ij+1}H_{i+1}+i+2\dots H_{j-1j}}{H_{i+1}+i\dots H_{j-1j-1}}\end{cases}\] (24)

Proof.: \[\hat{X}^{-1}\hat{X}^{(j)}=e_{j}\]

Where \(\hat{X}^{(j)}\) is the \(j^{th}\) column of \(\hat{X}\). Let \(\hat{Y}\) denote the right hand side of Equation (24).

\[(\hat{Y}\hat{X})_{jj} =\hat{X}_{jj}\hat{Y}_{jj}+\hat{X}_{j-1j}\hat{Y}_{j-1j}+\hat{X}_{ jj+1}\hat{Y}_{jj+1}\] \[=\hat{X}_{jj}H_{jj}+\hat{X}_{j-1j}H_{j-1j}+\hat{X}_{jj+1}H_{jj+1}\] \[=1\]The third equality is by using the following alternative form of Equation (12):

\[(\hat{X}^{(1)})_{i,j}=\begin{cases}0,\text{ if }j-i>1\\ \frac{-H_{i,i+1}}{(H_{i+1}H_{i+1}+1-H_{i+1,i+1}^{2})},\text{ if }j=i+1\\ \frac{1}{H_{i+1}}\left(1+\sum_{j\in\text{\rm{inig}}(i)}\frac{H_{ij}^{2}}{H_{ii },H_{jj}-H_{ij}^{2}}\right),\text{ if }i=j\end{cases},\] (25)

where \(i<j\). Similarly, the offdiagonals of \(\hat{Y}\hat{X}\) can be evaluated to be zero as follows.

\[(\hat{Y}\hat{X})_{ij} =\hat{Y}_{ij}\hat{X_{j}}j+\hat{Y}_{ij-1}\hat{X}_{j-1j}+\hat{Y}_{ij+ 1}\hat{X}_{j+1j}\] \[=\hat{Y}_{ij}\hat{X}_{jj}+\hat{Y}_{ij}\frac{H_{j-1j-1}}{H_{j-1j}} +\hat{Y}_{ij}\frac{H_{jj+1}}{H_{jj}}\hat{X}_{j+1j}\] \[=0\]

**Lemma A.3**.: _Let \(y\in\mathbb{R}^{n}\), \(\beta=\max_{t}\max_{i\in[n-1]}\left|(H_{t})_{ii+1}\right|/\sqrt{(H_{t})_{ii}( H_{t})_{i+1i+1}}<1\), then_

\[y^{T}X_{t}^{-1}y\leq\left\|y\right\|_{2}^{2}\left\|\operatorname{diag}(H_{t}) \right\|_{2}\left(\frac{1+\beta}{1-\beta}\right),\]

_where \(X_{t}\) is defined as in Lemma A.2._

Proof.: Let \(\tilde{X}_{t}^{-1}=\operatorname{diag}(H_{t})^{-1/2}\hat{X}_{t}\operatorname{ diag}(H_{t})^{-1/2}\)

\[y^{T}X_{t}^{-1}y\leq\left\|\operatorname{diag}(H_{t})^{1/2}y\right\|_{2}^{2} \left\|\tilde{X}_{t}^{-1}\right\|_{2}\] (26)

Using the identity of spectral radius \(\rho(X)\leq\|X\|_{\infty}\) and since \(\tilde{X}\) is positive definite, \(\left\|\tilde{X}_{t}^{-1}\right\|_{2}\leq\|\tilde{X}_{t}^{-1}\|_{\infty}\)

\[\left\|\tilde{X}_{t}^{-1}\right\|_{2} \leq\max_{i}\left\{\sum_{j}\left|(\tilde{X}_{t}^{-1})_{ij}\right|\right\}\] \[\leq 1+2(\beta+\beta^{2}+\ldots)\] \[\leq\frac{1+\beta}{1-\beta}\]

The second inequality is using Lemma A.2. Substituting this in Equation (26) will give the lemma. 

#### a.2.3 Upperbounding Regret

The following Lemma is used in upperbounding both \(T_{1}\) and \(T_{3}\). In next subsection, we'll upper bound \(T_{2}\) as well.

**Lemma A.4**.: _Let \(\beta=\max_{t\in[T]}\max_{i\in[n-1]}\left|(H_{t})_{ii+1}\right|/\sqrt{(H_{t}) _{ii}(H_{t})_{i+1i+1}}\), then_

\[1/(1-\beta)\leq 8/\hat{\epsilon}^{2},\]

_where, \(\hat{\epsilon}\) is a constant in parameter \(\epsilon=\hat{\epsilon}G_{\infty}\sqrt{T}\) and consequently used in initializing \(H_{0}=\epsilon I_{n}\) in line 1 in Algorithm 1,_Proof.: \[1/(1-\beta) =\max_{t}\max_{i\in[n-1]}\frac{1}{1-\left|(\hat{H}_{t})_{ii+1}\right|}\] (27) \[=\max_{t}\max_{i\in[n-1]}\frac{1+\left|(\hat{H}_{t})_{ii+1}\right|} {1-(\hat{H}_{t})_{ii+1}^{2}} (\text{where }(\hat{H}_{t})_{ii+1}=\frac{(H_{t})_{ii+1}}{\sqrt{(H_{t})_{ii}(H_{t})_{i+1 i+1}}})\] \[\leq\max_{t}\max_{i\in[n-1]}\frac{2(H_{t})_{ii}(H_{t})_{i+1i+1}}{ (H_{t})_{ii}(H_{t})_{i+1i+1}-(H_{t})_{ii+1}^{2}} (\text{since }\left|(H_{t})_{ii+1}\right|\leq\sqrt{(H_{t})_{ii}(H_{t})_{i+1 i+1}})\] \[\leq\max_{t}\max_{i\in[n-1]}\frac{2(H_{t})_{ii}(H_{t})_{i+1i+1}} {\det\left(\begin{bmatrix}(H_{t})_{ii}&(H_{t})_{ii+1}\\ (H_{t})_{i+1i}&(H_{t})_{i+1i+1}\end{bmatrix}\right)}\] (28)

Note that \(\begin{bmatrix}(H_{t})_{ii}&(H_{t})_{ii+1}\\ (H_{t})_{i+1i}&(H_{t})_{i+1i+1}\end{bmatrix}\succeq\epsilon\begin{bmatrix}1&0 \\ 0&1\end{bmatrix}\) (using line 1 in Algorithm 1), thus \(\det\left(\begin{bmatrix}(H_{t})_{ii}&(H_{t})_{ii+1}\\ (H_{t})_{i+1i}&(H_{t})_{i+1i+1}\end{bmatrix}\right)\geq\det\left(\epsilon \begin{bmatrix}1&0\\ 0&1\end{bmatrix}\right)=\epsilon^{2}\). The numerator last inequality can be upperbounded by bounding \((H_{t})_{ii}\) individually as follows:

\[(H_{t})_{ii} =\sum_{s=1}^{t}(g_{s})_{i}^{2}/\lambda_{s}\] \[=\sum_{s=1}^{t}(g_{s})_{i}^{2}/\lambda_{s}\] \[=\sum_{s=1}^{t}(g_{s})_{i}^{2}/(G_{\infty}\sqrt{s})\] \[\leq\sum_{s=1}^{t}G_{\infty}^{2}/(G_{\infty}\sqrt{s})\] \[\leq\sum_{s=1}^{t}\frac{G_{\infty}}{\sqrt{s}}\] \[\leq 2G_{\infty}\sqrt{t}\] (29)

Substituting the above in (28) gives

\[1/(1-\beta) \leq\max_{t}\frac{8G_{\infty}^{2}t}{\hat{\epsilon}^{2}G_{\infty} ^{2}T}\] \[\leq\frac{8}{\hat{\epsilon}^{2}}\]

**Lemma A.5** (_Upperbound of \(T_{1}\)).: \[T_{1}\leq\frac{16D_{2}^{2}G_{\infty}\sqrt{T}}{\hat{\epsilon}^{2}\eta},\] (30)

_where \(D_{2}=\max_{t\in[T]}\left\|w_{t}-w^{*}\right\|_{2}\) and \(G_{\infty}=\max_{t}\|g_{t}\|_{\infty}\)_Proof.: Since \(X_{T}\) is positive definite

\[T_{1} \leq\frac{\left\|w_{1}-w^{*}\right\|_{X_{1}^{-1}}^{2}}{2\eta}\] \[=\frac{(y^{(1)})^{T}X_{1}^{-1}y^{(1)}}{2\eta} (\text{where }y^{(1)}=w_{1}-w^{*})\] \[\leq\frac{\left\|y^{(1)}\right\|_{2}^{2}\left\|\operatorname{diag} (H_{1})\right\|_{2}}{2\eta}\cdot\frac{1+\beta}{1-\beta} (\text{Lemma A.3})\] \[\leq\frac{D_{2}^{2}(G_{\infty}^{2}/\lambda_{1}+\epsilon)}{2\eta} \cdot\frac{1+\beta}{1-\beta} (\text{line 4 in Algorithm 1})\] \[\leq\frac{8D_{2}^{2}(G_{\infty}^{2}/\lambda_{1}+\epsilon)}{ \hat{\epsilon}^{2}\eta} (\text{Lemma A.4})\] \[\leq\frac{8D_{2}^{2}(G_{\infty}+\hat{\epsilon}G_{\infty}\sqrt{T })}{\hat{\epsilon}^{2}\eta} (\text{Since }\lambda_{t}=G_{\infty}\sqrt{t}\text{ and }\epsilon=\hat{ \epsilon}G_{\infty}\sqrt{T})\] \[\leq\frac{16D_{2}^{2}G_{\infty}\sqrt{T}}{\hat{\epsilon}^{2}\eta} (\hat{\epsilon}<1)\]

**Lemma A.6** (\(O(\sqrt{T})\) upperbound on \(T_{3}\)).: \[T_{3}=\sum_{t=1}^{T}\frac{\eta}{2}\cdot g_{t}^{T}X_{t}g_{t}\leq\frac{4nG_{ \infty}\eta}{\hat{\epsilon}^{3}}\sqrt{T}\]

_where, \(\|g_{t}\|_{\infty}\leq G_{\infty}\) and parameters \(\epsilon=\hat{\epsilon}G_{\infty}\sqrt{T}\), \(\lambda_{t}=G_{\infty}\sqrt{t}\) in Algorithm 1._

Proof.: Using Theorem 3.1, nonzero entries of \(X_{t}\) can be written as follows:

\[(X_{t})_{ii} =\frac{1}{H_{ii}}\left(1+\sum_{(i,j)\in E_{\mathcal{G}}}\frac{H_ {ij}^{2}}{H_{ii}H_{jj}-H_{ij}^{2}}\right)\] \[(X_{t})_{ii+1} =-\frac{H_{ii+1}}{H_{ii}H_{i+1i+1}-H_{ii+1}^{2}}\]

where, \(E_{\mathcal{G}}\) denote the set of edges of the chain graph \(\mathcal{G}\) in Theorem 3.1. Also, for brevity, the subscript is dropped for \(H_{t}\). Let \(\hat{X}_{t}=\sqrt{\operatorname{diag}(H)}X_{t}\sqrt{\operatorname{diag}(H)}\), then \(\hat{X}_{t}\) can be written as

\[(\hat{X}_{t})_{ii} =\left(1+\sum_{(i,j)\in E_{\mathcal{G}}}\frac{\hat{H}_{ij}^{2}}{ 1-\hat{H}_{ij}^{2}}\right),\] \[(\hat{X}_{t})_{ii+1} =-\frac{\hat{H}_{ii+1}}{1-\hat{H}_{ii+1}^{2}},\]

where, \(\hat{H}_{ij}=H_{ij}/\sqrt{H_{ii}H_{jj}}\). Note that \(\hat{X}_{t}\preceq\|\hat{X}_{t}\|_{2}I_{n}\preceq\|\hat{X}_{t}\|_{\infty}I_{n}\), using \(\max\{|\lambda_{1}(\hat{X}_{t})|,\ldots,|\lambda_{n}(\hat{X}_{t})|\}\leq\|\hat {X}_{t}\|_{\infty}\) (property of spectral radius). So we upperbound \(\|\hat{X}_{t}\|_{\infty}=\max_{i\in[n]}\{|(\hat{X}_{t})_{11}|+|(\hat{X}_{t})_{ 12}|,\ldots,|(\hat{X}_{t})_{ii-1}|+|(\hat{X}_{t})_{ii}|+|(\hat{X}_{t})_{ii+1}|,\ldots,|(\hat{X}_{t})_{nn}|+|(\hat{X}_{t})_{nn-1}|\}\) next. Individual terms \(|(\hat{X}_{t})_{ii-1}|+|(\hat{X}_{t})_{ii}|+|(\hat{X}_{t})_{ii+1}|\) can be written as follows:\[\sum_{(i,j)\in E_{\Theta}}|(\hat{X}_{t})_{ij}| =1+\sum_{(i,j)\in E_{\Theta}}\frac{\hat{H}_{ij}^{2}}{1-\hat{H}_{ij }^{2}}+\frac{|\hat{H}_{ij}|}{1-\hat{H}_{ij}^{2}}\] \[=1+\sum_{(i,j)\in E_{\Theta}}\frac{|\hat{H}_{ij}|}{1-|\hat{H}_{ij }|}\] \[\leq 2\max_{i\in[n-1]}\frac{1}{1-|\hat{H}_{ii+1}|}\]

The last inequality is because \(|\hat{H}_{ij}|\leq 1\). Thus, \(\|\hat{X}_{t}\|_{\infty}\leq 2\max_{i\in[n-1]}\frac{1}{1-|\hat{H}_{ii+1}|}\). Now

\[g_{t}^{T}X_{t}g_{t} \leq g_{t}^{T}\operatorname{diag}(H_{t})^{-1/2}\hat{X}_{t} \operatorname{diag}(H_{t})^{-1/2}g_{t}\] \[\leq\|\hat{X}_{t}\|_{\infty}\|\operatorname{diag}(H_{t})^{-1/2} g_{t}\|_{2}^{2}\] \[\leq 2\max_{i\in[n-1]}\frac{1}{1-|\hat{H}_{ii+1}|}g_{t}^{T} \operatorname{diag}(H_{t})^{-1}g_{t}.\]

Using \(\operatorname{diag}(H_{t})\succeq\epsilon I_{n}\) (step 1 in Algorithm 1), where \(\epsilon=\hat{\epsilon}G_{\infty}\sqrt{T}\) as set in Lemma A.8, gives

\[g_{t}^{T}X_{t}g_{t} \leq 2\max_{i\in[n-1]}\frac{1}{1-|\hat{H}_{ii+1}|}\frac{\|g_{t}\|_{2 }^{2}}{\hat{\epsilon}G_{\infty}\sqrt{T}}\] \[\leq 2\max_{i\in[n-1]}\frac{nG_{\infty}}{\hat{\epsilon}(1-|\hat{H} _{ii+1}|)\sqrt{T}}\] \[\leq\frac{2nG_{\infty}}{\hat{\epsilon}(1-\beta)\sqrt{T}}\] (where \[\beta=\max_{t\in[T]}\max_{i\in[n-1]}\Big{|}(\hat{H}_{t})_{ii+1}\Big{|}\] )

Summing up over \(t\) gives

\[\sum_{t}\frac{\eta}{2}g_{t}^{T}X_{t}g_{t} \leq\sum_{t}\frac{16nG_{\infty}\eta}{\hat{\epsilon}^{3}\sqrt{T}} (\text{Using Lemma A.4})\] \[\leq\frac{16nG_{\infty}\eta}{\hat{\epsilon}^{3}}\sqrt{T}\]

#### a.2.4 \(\mathcal{O}(\sqrt{T})\) Regret

In this section we derive a regret upper bound with a \(\mathcal{O}(T^{1/2})\) growth. For this, we upper bound \(T_{2}\) as well in this section. In (21), \(T_{2}=\sum_{t=2}^{T}(w_{t}-w^{*})^{T}(X_{t}^{-1}-X_{t-1}^{-1})(w_{t}-w^{*})\) can be upper bounded to a \(\mathcal{O}(T^{1/2})\) by upperbounding entries of \(X_{t}^{-1}-X_{t-1}^{-1}\) individually. The following lemmas constructs a telescoping argument to bound \(|(X_{t}^{-1}-X_{t-1}^{-1})_{i,j}|\).

**Lemma A.7**.: _Let \(H,\tilde{H}\in S_{n}^{++}\), such that \(\tilde{H}=H+gg^{T}/\lambda\), where \(g\in\mathbb{R}^{n}\), then_

\[\frac{\tilde{H}_{ij}}{\sqrt{\tilde{H}_{ii}\tilde{H}_{jj}}}-\frac {H_{ij}}{\sqrt{H_{ii}H_{jj}}}\] \[=\frac{g_{i}g_{j}}{\lambda\sqrt{\tilde{H}_{ii}\tilde{H}_{jj}}}+ \frac{H_{ij}}{\sqrt{H_{ii}H_{jj}}}\left(\sqrt{\frac{H_{ii}H_{jj}}{\tilde{H}_ {ii}\tilde{H}_{jj}}}-1\right)=\theta_{ij}\]Proof.: \[\frac{\tilde{H}_{ij}}{\sqrt{\tilde{H}_{ii}\tilde{H}_{jj}}}-\frac{H_{ ij}}{\sqrt{H_{ii}H_{jj}}}\] \[=\frac{1}{\sqrt{H_{ii}H_{jj}}}(\tilde{H}_{ij}\frac{\sqrt{H_{ii}H_{ jj}}}{\sqrt{\tilde{H}_{ii}\tilde{H}_{jj}}}-H_{ij})\] \[=\frac{1}{\sqrt{H_{ii}H_{jj}}}\left(g_{i}g_{j}\frac{\sqrt{H_{ii}H_ {jj}}}{\sqrt{\tilde{H}_{ii}\tilde{H}_{jj}}}+H_{ij}\left(\frac{\sqrt{H_{ii}H_{ jj}}}{\sqrt{\tilde{H}_{ii}\tilde{H}_{jj}}}-1\right)\right)\]

The following Lemma bounds the change in the inverse of preconditioner \(Y^{-1}\), when there is a rank one perturbation to \(H\succ 0\) in following LogDet problem (11) :

\[Y =\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}}-\log \det{(X)}+\operatorname{Tr}(XH)\] \[=\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}}\operatorname {D}_{\ell\mathrm{d}}(X,H)\]

**Lemma A.8** (_Rank 1 perturbation of LogDet problem (11)).: _Let \(H,\tilde{H}\in S_{n}^{++}\), such that \(\tilde{H}=H+gg^{T}/\lambda\), where \(g\in\mathbb{R}^{n}\). Also, \(\tilde{Y}=\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}} \operatorname{D}_{\ell\mathrm{d}}(X,\tilde{H})\) and \(Y=\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}}\operatorname{D}_{ \ell\mathrm{d}}(X,H)\), where \(\mathcal{G}\) is a chain graph, then_

\[\left|(\tilde{Y}^{-1}-Y^{-1})_{ii+k}\right|\leq G_{\infty}^{2}\kappa(k\beta+k+ 2)\beta^{k-1}/\lambda,\]

_where \(i,i+k\leq n\), \(G_{\infty}=\left\|g\right\|_{\infty}\) and \(\max_{i,j}|H_{ij}|/\sqrt{H_{ii}H_{jj}}\leq\beta<1\). Let \(\kappa(\operatorname{diag}(H))\coloneqq\) condition number of the diagonal part of \(H\), then \(\kappa\coloneqq\max(\kappa(\operatorname{diag}(H)),\kappa(\operatorname{diag}( \tilde{H})))\)._

Proof.: Using Lemma A.2 will give the following:

\[\left|(\tilde{Y}^{-1}-Y^{-1})_{ii+k}\right|\] \[=\left|\frac{\tilde{H}_{ii+1}\ldots\tilde{H}_{i+k-1i+k}}{\tilde{H }_{i+1i+1}\ldots\tilde{H}_{i+k-1i+k-1}}-\frac{H_{ii+1}\ldots H_{i+k-1i+k}}{H_{ i+1i+1}\ldots H_{i+k-1i+k-1}}\right|\] \[=\left|\sqrt{\tilde{H}_{ii}}\tilde{N}_{ii+1}\ldots\tilde{N}_{i+k-1 i+k}\sqrt{\tilde{H}_{i+ki+k}}\right.\] \[-\left.\sqrt{H_{ii}}\tilde{N}_{ii+1}\ldots N_{i+k-1i+k}\sqrt{H_{ i+ki+k}}\right|\] \[=\sqrt{\tilde{H}_{ii}}\tilde{H}_{i+ki+k}\left|\tilde{N}_{ii+1} \ldots\tilde{N}_{i+k-1i+k}-N_{ii+1}\ldots N_{i+k-1i+k}\right.\left.\sqrt{H_{ ii}H_{i+ki+k}/\tilde{H}_{ii}\tilde{H}_{i+ki+k}}\right|\]

where \(N_{ij}=H_{ij}/\sqrt{H_{ii}H_{jj}}<1\) (Since determinants of 2x2 submatrices of H are positive). Expanding \(\tilde{N}_{ii+1}=N_{ii+1}+\theta_{ii+1}\) (from Lemma A.7), subsequently \(\tilde{N}_{ii+2}=N_{ii+2}+\theta_{ii+2}\) and so on will give

\[\left|\tilde{N}_{ii+1}\ldots\tilde{N}_{i+k-1i+k}\right.-N_{ii+1} \ldots N_{i+k-1i+k}\left.\sqrt{\frac{H_{ii}H_{i+ki+k}}{\tilde{H}_{ii}\tilde{H}_ {i+ki+k}}}\right|=\] \[\left|\theta_{ii+1}\tilde{N}_{i+1i+2}\ldots\tilde{N}_{i+k-1i+k} \right.+N_{ii+1}\left(\tilde{N}_{i+1i+2}\ldots\tilde{N}_{i+k-1i+k}\left.-N_{i+1 i+2}\ldots N_{i+k-1i+k}\right.\left.\sqrt{\frac{H_{ii}H_{i+ki+k}}{\tilde{H}_{ii} \tilde{H}_{i+ki+k}}}\right).\]\[=|\theta_{ii+1}\tilde{N}_{i+1i+2}\ldots\tilde{N}_{i+k-1i+k}+N_{ii+1} \theta_{i+1i+2}\tilde{N}_{ii+3}\ldots\tilde{N}_{i+k-1i+k}+\cdots+N_{ii+1}\ldots N _{ii+k-1}\theta_{i+k-1i+k}\] \[+N_{ii+1}\ldots N_{ii+k}\left(1-\sqrt{\frac{H_{ii}H_{i+ki+k}}{ \tilde{H}_{ii}\tilde{H}_{i+ki+k}}}\right)|\] \[\leq(\sum_{l=0}^{k-1}|\theta_{i+li+l+1}|)\beta^{k-1}+\beta^{k-1} \left|1-\sqrt{\frac{H_{ii}H_{i+ki+k}}{\tilde{H}_{ii}\tilde{H}_{i+ki+k}}}\right|,\] \[\implies\left|(\tilde{Y}^{-1}-Y^{-1})_{ii+k}\right|\leq\sqrt{ \tilde{H}_{ii}\tilde{H}_{i+ki+k}}\cdot\left((\sum_{l=0}^{k-1}|\theta_{i+li+l+1} |)\beta^{k-1}+\beta^{k-1}\left|1-\sqrt{\frac{H_{ii}H_{i+ki+k}}{\tilde{H}_{ii} \tilde{H}_{i+ki+k}}}\right|\right)\]

where \(\max_{i,j}|N_{i,j}|,\;\max_{i,j}|\tilde{N}_{i,j}|\leq\beta<1\). Expanding \(\theta_{i+li+l+1}\) from Lemma A.7 in the term \(|\theta_{i+li+l+1}|\sqrt{\tilde{H}_{ii}\tilde{H}_{i+ki+k}}\) will give:

\[|\theta_{i+li+l+1}|\sqrt{\tilde{H}_{ii}\tilde{H}_{i+ki+k}}\] \[=\left|\sqrt{\tilde{H}_{ii}\tilde{H}_{i+ki+k}}\frac{g_{i+l}g_{i+l+ 1}}{\lambda\sqrt{\tilde{H}_{i+li+l}\tilde{H}_{i+l+1i+l+1}}}+\sqrt{\tilde{H}_{ ii}\tilde{H}_{i+ki+k}}N_{i+li+l+1}\left(\sqrt{\frac{H_{i+li+l}H_{i+l+1i+l+1}}{ \tilde{H}_{i+li+l+1i+l+1}}}-1\right)\right|\] \[\leq\left|\sqrt{\tilde{H}_{ii}\tilde{H}_{i+ki+k}}\frac{g_{i+l}g_{ i+l+1}}{\lambda\sqrt{\tilde{H}_{i+li+l}\tilde{H}_{i+l+1i+l+1}}}\right|+\left| \sqrt{\tilde{H}_{ii}\tilde{H}_{i+ki+k}}N_{i+li+l+1}\left(1-\sqrt{\frac{H_{i+ li+l}H_{i+l+1i+l+1}}{\tilde{H}_{i+l+l+1i+l+1}}}\right)\right|\]

Since \(H_{i+li+l}H_{i+l+1i+l+1}\leq\tilde{H}_{i+li+l}\tilde{H}_{i+l+1i+1}\),

\[1-\sqrt{\frac{H_{i+li+l}H_{i+l+1i+l+1}}{\tilde{H}_{i+li+l}\tilde{ H}_{i+l+1i+l+1}}} \leq\max\left(1-\frac{H_{i+li+l}}{\tilde{H}_{i+li+l}},1-\frac{H_{ i+l+1i+l+1}}{\tilde{H}_{i+l+1i+l+1}}\right)\] \[\leq\max\left(\frac{g_{i+l}^{2}}{\lambda\tilde{H}_{i+li+l}}, \frac{g_{i+l+1}^{2}}{\lambda\tilde{H}_{i+l+1i+l+1}}\right)\]

Using the above, \(H_{i,i}/H_{j,j}\leq\kappa\), and \(|g_{i}|\leq G_{\infty}\), \(\forall i,j\in[n]\), gives

\[\sqrt{\tilde{H}_{ii}\tilde{H}_{i+ki+k}}|\theta_{i+li+l+1}| \leq G_{\infty}^{2}\kappa/\lambda+\beta G_{\infty}^{2}\kappa/\lambda\] \[\leq G_{\infty}^{2}\kappa(1+\beta)/\lambda\]

Thus the following part of \(\left|\left(\tilde{Y}^{-1}-Y^{-1}\right)_{ii+k}\right|\) can be upperbounded:

\[\sqrt{\tilde{H}_{ii}\tilde{H}_{i+ki+k}}\left((\sum_{l=0}^{k-1}|\theta_{i+li+l+ 1}|)\beta^{k-1}\right)\leq G_{\infty}^{2}\kappa(1+\beta)k\beta^{k-1}/\lambda\]

Also, \(\sqrt{\tilde{H}_{ii}\tilde{H}_{i+ki+k}}\beta^{k-1}\left|1-\sqrt{\frac{H_{ii}H _{i+ki+k}}{\tilde{H}_{ii}\tilde{H}_{i+ki+k}}}\right|\leq\beta^{k-1}\kappa G_{ \infty}^{2}/\lambda\), so

\[\left|\left(\tilde{Y}^{-1}-Y^{-1}\right)_{ii+k}\right|\leq G_{\infty}^{2} \kappa(k\beta+k+2)\beta^{k-1}/\lambda\]

**Lemma A.9** (\(\mathcal{O}(\sqrt{T})\) upper bound of \(T_{2}\)).: _Given that \(\kappa(\operatorname{diag}(H_{t}))\leq\kappa\), \(\left\|w_{t}-w^{*}\right\|_{2}\leq D_{2}\), \(\max_{i,j}|(H_{t})_{ij}|/\sqrt{(H_{t})_{ii}(H_{t})_{jj}}\leq\beta<1\), \(\forall t\in[T]\) in Algorithm 1, then \(T_{2}\) in Appendix A.2.1 can be bounded as follows:_

\[T_{2}\leq\frac{2048\sqrt{T}}{\eta\hat{\epsilon}^{5}}(G_{\infty}D_{2}^{2})\]

_where \(\lambda_{t}=G_{\infty}\sqrt{t}\), and \(\epsilon=\hat{\epsilon}G_{\infty}\sqrt{T}\) in Algorithm 1, and \(\hat{\epsilon}\leq 1\) is a constant._Proof.: Note that \(T_{2}=\frac{1}{2\eta}\cdot\sum_{t=1}^{T-1}(w_{t+1}-w^{*})^{T}(X_{t+1}^{-1}-X_{t}^{- 1})(w_{t+1}-w^{*})\leq\sum_{t=1}^{T-1}D_{2}^{2}\left\|(X_{t+1}^{-1}-X_{t}^{-1}) \right\|_{2}/(2\eta)\). Using \(\|A\|_{2}=\rho(A)\leq\|A\|_{\infty}\) for symmetric matrices \(A\), we get

\[\left\|X_{t+1}^{-1}-X_{t}^{-1}\right\|_{2} \leq\|X_{t+1}^{-1}-X_{t}^{-1}\|_{\infty}\] \[=\max_{i}(\sum_{j}\left|(X_{t+1}^{-1}-X_{t}^{-1})_{ij}\right|)\] \[\leq 16\frac{G_{\infty}\kappa}{\sqrt{t}(1-\beta)^{2}} (\text{Lemma A.8 })\] \[\leq 1024\cdot\frac{G_{\infty}\kappa}{\sqrt{t}\hat{\epsilon}^{4}}\]

Now using \(\kappa\leq 2/\hat{\epsilon}\) (using Equation (29) and \((H_{t})_{ii}>\hat{\epsilon}\)) and summing up terms in \(T_{2}\) using the above will give the result. 

Putting together \(T_{1}\), \(T_{2}\) and \(T_{3}\) from Lemma A.5, Lemma A.9 and Lemma A.6 respectively, when \(\epsilon\), \(\lambda_{t}\) are defined as in Lemma A.9:

\[T_{1}\leq \frac{16D_{2}^{2}G_{\infty}\sqrt{T}}{\hat{\epsilon}^{2}\eta},\] \[T_{2}\leq \frac{2048\sqrt{T}}{\eta\hat{\epsilon}^{5}}(G_{\infty}D_{2}^{2})\] (31) \[T_{3}\leq\frac{4nG_{\infty}\eta}{\hat{\epsilon}^{3}}\sqrt{T}\] (32)

Setting \(\eta=\frac{D_{2}}{\hat{\epsilon}\sqrt{n}}\)

\[R_{T}\leq T_{1}+T_{2}+T_{3}\leq O(\sqrt{n}G_{\infty}D_{2}\sqrt{T})\]

#### a.2.5 Non-convex guarantees

Minimizing smooth non-convex functions \(f\) is a complex yet interesting problem. In Agarwal et al. [1], this problem is reduced to an online convex optimization, where a sequence of objectives \(f_{t}(w)=f(w)+c\left\|w-w_{t}\right\|_{2}^{2}\) are minimized. Using this approach Agarwal et al. [1] established convergence guarantees to reach a stationary point via regret minimization. Thus non-convex guarantees can be obtained from regret guarantees and is our main focus in the paper.

### Numerical stability

In this section we conduct perturbation analysis to derive an end-to-end componentwise condition number (pg. 135, problem 7.11 in [27]) upper bound of the tridiagonal explicit solution in Theorem 3.1. In addition to this, we devise Algorithm 3 to reduce this condition number upper bound for the tridiagonal sparsity structure, and be robust to \(H_{t}\) which don't follow the non-degeneracy condition: any principle submatrix of \(H_{t}\) corresponding to a complete subgraph of \(\mathcal{G}\).

**Theorem A.10** (Condition number of tridiagonal LogDet subproblem (11)).: _Let \(H\in S_{n}^{++}\) be such that \(H_{ii}=1\) for \(i\in[n]\). Let \(\Delta H\) be a symmetric perturbation such that \(\Delta H_{ii}=0\) for \(i\in[n]\), and \(H+\Delta H\in S_{n}^{++}\). Let \(P_{\mathcal{G}}(H)\) be the input to 11, where \(\mathcal{G}\) is a chain graph, then_

\[\kappa_{\infty}^{\ell d}\leq\max_{i\in[n-1]}2/(1-\beta_{i}^{2})=\hat{\kappa}_ {\infty}^{\ell d},\] (33)

_where, \(\beta_{i}=H_{ii+1}\),\(\kappa_{\infty}^{\ell d}\coloneqq\text{componentwise condition number of (11) for perturbation }\Delta H\)._

The tridiagonal LogDet problem with inputs \(H\) as mentioned in Theorem A.10, has high condition number when \(1-\beta_{i}^{2}=H_{ii}-H_{ii+1}^{2}/H_{i+1i+1}\) are low and as a result the preconditioner \(X_{t}\) in SONew (Algorithm 1) has high componentwise relative errors. We develop Algorithm 3 to be robust to degenerate inputs \(H\), given that \(H_{ii}>0\). It finds a subgraph \(\tilde{\mathcal{G}}\) of \(\mathcal{G}\) for which non-degeneracy conditions in Theorem 3.2 is satisfied and (14) is well-defined. This is done by removing edges which causes inverse \(H_{I_{j}I_{j}}^{-1}\) to be singular or \((H_{jj}-H_{I_{j}j}^{T}H_{I_{j}I_{j}}H_{I_{j}j})\) to be low. In the following theorem we also show that the condition number upper bound in Theorem A.10 reduces in tridiagonal case. To test the robustness of this method we conducted an ablation study in Table 5, in an Autoencoder benchmark (from Section 5) in bfloat16 where we demonstrate noticeable improvement in performance when Algorithm 3 is used.

**Theorem A.11** (Numerically stable algorithm).: _Algorithm 3 finds a subgraph \(\tilde{\mathcal{G}}\) of \(\mathcal{G}\), such that explicit solution for \(\tilde{\mathcal{G}}\) in (14) is well-defined. Furthermore, when \(\mathcal{G}\) is a tridiagonal/chain graph, the component-wise condition number upper bound in (33) is reduced upon using Algorithm 3, \(\hat{\kappa}_{\ell d}^{\tilde{\mathcal{G}}}<\hat{\kappa}_{\ell d}^{\tilde{ \mathcal{G}}}\), where \(\hat{\kappa}_{\ell d}^{\tilde{\mathcal{G}}}\), \(\hat{\kappa}_{\ell d}^{\tilde{\mathcal{G}}}\) are defined as in Theorem A.10 for graphs \(\tilde{\mathcal{G}}\) and \(\mathcal{G}\) respectively._

The proofs for Theorems A.10 and A.11 are given in the following subsections.

```
1:Input:\(\mathcal{G}-\) tridiagonal or banded graph, \(H-\) symmetric matrix in \(\mathbb{R}^{n\times n}\) with sparsity structure \(\mathcal{G}\) and \(H_{ii}>0\), \(\gamma-\) tolerance parameter for low schur complements.
2:Output: Finds subgraph \(\tilde{\mathcal{G}}\) of \(\mathcal{G}\) without any degenerate cases from Lemma A.13 and finds preconditioner \(\hat{X}\) corresponding to the subgraph
3: Let \(E_{i}=\left\{(i,j):(i,j)\in E_{\mathcal{G}}\right\}\) be edges from vertex \(i\) to its neighbours in graph \(\mathcal{G}\).
4: Let \(V_{i}^{+}=\left\{j:i<j,(i,j)\in E_{\mathcal{G}}\right\}\) and \(V_{i}^{-}=\left\{j:i>j,(i,j)\in E_{\mathcal{G}}\right\}\), denote positive and negative neighbourhood of vertex \(i\).
5: Let \(K=\left\{i:H_{ii}-H_{I_{i}i}^{T}H_{I_{i}I_{i}}^{-1}H_{I_{i}i}\text{ is undefined or }\leq\gamma\right\}\)
6: Consider a new subgraph \(\tilde{\mathcal{G}}\) with edges \(E_{\tilde{\mathcal{G}}}=E_{\mathcal{G}}\setminus(\bigcup_{i\in K}E_{i}\cup(V_ {i}^{+}\times V_{i}^{-}))\)
7:return\(\hat{X}:=\)Sparsified_Inverse\((\tilde{H}_{t},\tilde{\mathcal{G}})\), where \(\tilde{H}_{t}=P_{\tilde{\mathcal{G}}}(H_{t})\) ```

**Algorithm 3** Numerically stable banded LogDet solution

#### a.3.1 Condition number analysis

**Theorem A.12** (_Full version of Theorem A.10).: _Let \(H\in S_{n}^{++}\) such that \(H_{ii}=1\), for \(i\in[n]\) and a symmetric perturbation \(\Delta H\) such that \(\Delta H_{ii}=0\), for \(i\in[n]\) and \(H+\Delta H\succ 0\). Let \(\hat{X}=\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}}\operatorname{D }_{\operatorname{fd}}\left(X,H^{-1}\right)\) and \(\hat{X}+\Delta\hat{X}=\operatorname*{arg\,min}_{X\in S_{n}(\mathcal{G})^{++}} \operatorname{D}_{\operatorname{fd}}\left(X,(H+\Delta H)^{-1}\right)\), here \(\mathcal{G}:=\) chain/tridiagonal sparsity graph and \(S_{n}(\mathcal{G})^{++}\) denotes positive definite matrices which follows the sparsity pattern \(\mathcal{G}\)._

\[\kappa_{\ell d} =\lim_{\epsilon\to 0}\sup\left\{\frac{\left|\Delta\hat{X}_{ij} \right|}{\epsilon\left|\hat{X}_{ij}\right|}:\left|\Delta H_{k,l}\right|\leq \left|\epsilon H_{k,l}\right|,(k,l)\in E_{\mathcal{G}}\right\}\] \[\leq\max_{i\in[n-1]}1/(1-\beta_{i}^{2})\]

_where, \(\kappa_{\ell d}:=\) condition number of the LogDet subproblem, \(\kappa_{2}(.):=\) condition number of a matrix in \(\ell_{2}\) norm, \(\beta_{i}=H_{ii+1}/\sqrt{H_{ii}H_{i+1i+1}}\)_

Proof.: Consider the offdiagonals for which \((\hat{X}+\Delta\hat{X})_{ii+1}=-H_{ii+1}/(1-H_{ii+1}^{2})=f(H_{ii+1})\),where \(f(x)=-x/(1-x^{2})\). Let \(y=f(x)\), \(\hat{y}=f(x+\Delta x)\) and \(\left|\Delta x/x\right|\leq\epsilon\) then using Taylor series

\[\left|\frac{(\hat{y}-y)}{y}\right| =\left|\frac{xf^{\prime}(x)}{f(x)}\right|\left|\frac{\Delta x}{x }\right|+O((\Delta x)^{2})\] \[\implies\lim_{\epsilon\to 0}\left|\frac{(\hat{y}-y)}{\epsilon y}\right| \leq\frac{xf^{\prime}(x)}{f(x)}\]Using the above inequality, with \(x\coloneqq H_{ii+1}\) and \(y\coloneqq\hat{X}_{ii+1}\),

\[\lim_{\epsilon\to 0}\left|\frac{\Delta\hat{X}_{ii+1}}{ \epsilon\hat{X}_{ii+1}}\right| \leq\frac{1+H_{ii+1}^{2}}{1-H_{ii+1}^{2}}\] (34) \[\leq\frac{2}{1-H_{ii+1}^{2}}\]

Let \(g(x)=x^{2}/(1-x^{2})\), let \(y_{1}=g(w_{1})\), \(y_{2}=g(x_{2})\), \(\hat{y}_{1}=g(w_{1}+\Delta x)\), \(\hat{y}_{2}=g(x_{2}+\Delta x)\). Using Taylor series

\[\left|\frac{(\hat{y}_{1}-y_{1})}{y_{1}}\right| =\left|\frac{x_{1}f^{\prime}(x_{1})}{f(x_{1})}\right|\left| \frac{\Delta x_{1}}{x_{1}}\right|+O((\Delta x_{1})^{2})\] \[\left|\frac{(\hat{y}_{2}-y_{2})}{y_{2}}\right| =\left|\frac{x_{2}f^{\prime}(x_{2})}{f(x_{2})}\right|\left|\frac{ \Delta x_{2}}{x_{2}}\right|+O((\Delta x_{2})^{2})\] \[\implies\lim_{\epsilon\to 0}\frac{\Delta y_{1}+\Delta y_{2}}{ \epsilon(1+y_{1}+y_{2})} \leq\max\left(\frac{2}{1-x_{1}^{2}},\frac{2}{1-x_{2}^{2}}\right)\]

Putting \(x_{1}\coloneqq H_{ii+1}\), \(x_{2}\coloneqq H_{ii-1}\) and analyzing \(y_{1}\coloneqq H_{ii+1}^{2}/(1-H_{ii+1}^{2})\) and \(y_{2}\coloneqq H_{ii-1}^{2}/(1-H_{ii-1}^{2})\) will result in the following

\[\lim_{\epsilon\to 0}\left|\frac{\Delta\hat{X}_{ii}}{\hat{X}_{ii}}\right| \leq\max\left(\frac{2}{1-H_{ii+1}^{2}},\frac{2}{1-H_{ii-1}^{2}}\right)\] (35)

Since \(\hat{X}_{ii}=1+H_{ii+1}^{2}/(1-H_{ii+1}^{2})+H_{ii-1}^{2}/(1-H_{ii-1}^{2})\). Putting together Equation (35) and Equation (34), the theorem is proved. 

#### a.3.2 Degenerate \(H_{t}\)

In SONew (1), the \(H_{t}=P_{\mathcal{G}}(\sum_{s=1}^{t}g_{s}g_{s}^{T}/\lambda_{t})\) generated in line 4 could be such that the matrix \(\sum_{s=1}^{t}g_{s}g_{s}^{T}/\lambda_{t}\) need not be positive definite and so the schur complements \(H_{ii}-H_{ii+1}^{2}/H_{i+1i+1}\) can be zero, giving an infinite condition number \(\kappa_{\infty}^{\ell d}\) by Theorem A.10. The following lemma describes such cases in detail for a more general banded sparsity structure case.

**Lemma A.13** (Degenerate inputs to banded LogDet subproblem).: _Let \(H=P_{\mathcal{G}}(GG^{T})\), when \(\epsilon=0\) in Algorithm 1, where \(G\in\mathbb{R}^{n\times T}\) and let \(g_{1:T}^{(i)}\) be \(i^{th}\) row of \(G\), which is gradients of parameter \(i\) for \(T\) rounds, then \(H_{ij}=\left<g_{1:T}^{(i)},g_{1:T}^{(j)}\right>\)._

* _Case 1: For tridiagonal sparsity structure_ \(\mathcal{G}\)_: if_ \(g_{1:T}^{(j)}=g_{1:T}^{(j+1)}\)_, then_ \(H_{jj}-H_{jj+1}^{2}/H_{j+1j+1}=0\)_._
* _Case 2: For_ \(b>1\) _in (_14_): If_ \(\operatorname{rank}(H_{J_{j}J_{j}})=\operatorname{rank}(H_{I_{j}I_{j}})=b\)_, then_ \((H_{jj}-H_{jj}^{T}H_{I_{j}I_{j}}^{-1}H_{I_{j}j})=0\) _and_ \(D_{jj}=\infty\)_. If_ \(\operatorname{rank}(H_{I_{j}I_{j}})<b\) _then the inverse_ \(H_{I_{j}I_{j}}^{-1}\) _doesn't exist and_ \(D_{jj}\) _is not well-defined._

Proof.: For \(b=1\), if \(g_{1:T}^{(j)}=g_{1:T}^{(j+1)}\), then \(H_{jj+1}=H_{jj}=H_{j+1j+1}=\left\|g_{1:T}^{(j)}\right\|_{2}^{2}\), thus \(H_{jj}-H_{jj+1}^{2}/H_{j+1j+1}=0\).

For \(b>1\), since \(H_{I_{j}I_{j}}\), using Guttman rank additivity formula, \(\operatorname{rank}(H_{jj}-H_{jj+1}^{2}/H_{j+1j+1})=\operatorname{rank}(H_{J_ {j}J_{j}})-rank(H_{I_{j}I_{j}})=0\), thus \(H_{jj}-H_{jj+1}^{2}/H_{j+1j+1}=0\).

Furthermore, if \(\operatorname{rank}(H)\leq b\), then all \(b+1\times b+1\) principal submatrices of \(H\) have rank \(b\), thus \(\forall j\), \(H_{J_{j}J_{j}}\) have a rank \(b\), thus \(D_{jj}\) for all \(j\) are undefined.

If \(GG^{T}=\sum_{i=1}^{T}g_{i}g_{i}\) is a singular matrix, then solution to the LogDet problem might not be well-defined as shown in Lemma A.13. For instance, Case 1 can occur when preconditioning the input layer of an image-based DNN with flattened image inputs, where \(j^{th}\) and \((j+1)^{th}\) pixel can be highly correlated throughout the dataset. Case 2 can occur in the first \(b\) iterations in Algorithm 1 when the rank of submatrices \(\operatorname{rank}(H_{I_{j}I_{j}})<b\) and \(\epsilon=0\).

#### a.3.3 Numerically Stable SONew proof

_Proof of Theorem a.11_

Let \(I_{i}=\left\{j:i<j,(i,j)\in E_{\tilde{\mathcal{G}}}\right\}\) and \(I^{\prime}_{i}=\left\{j:i<j,(i,j)\in E_{\tilde{\mathcal{G}}}\right\}\) Let \(K=\left\{i:H_{ii}-H^{T}_{I_{i}i}H^{-1}_{I_{i}i}H_{I_{i}i}\right.\) is undefined or \(0,i\in[n]\right\}\) denote vertices which are getting removed by the algorithm, then for the new graph \(\tilde{\mathcal{G}}\), \(D_{ii}=1/H_{ii},\forall i\in K\) since \(H_{ii}>0\).

Let \(\bar{K}=\left\{i:H_{ii}-H^{T}_{I_{i}i}H^{-1}_{I_{i}i}H_{I_{i}i}>0,i\in[n]\right\}\). Let for some \(j\in\bar{K}\), if

\[l=\arg\min\left\{i:j<i,i\in K\cap I_{j}\right\},\]

denotes the nearest connected vertex higher than \(j\) for which \(D_{ll}\) is undefined or zero, then according to the definition \(E_{\tilde{\mathcal{G}}}\) in Algorithm 3, \(I^{\prime}_{j}=\left\{j+1,\ldots l-1\right\}\subset I_{j}\), since \(D_{jj}\) is well-defined, \(H_{I_{j}I_{j}}\) is invertible, which makes it a positive definite matrix (since \(H\) is PSD). Since \(H_{jj}-H^{T}_{I_{j}j}H^{-1}_{I_{j}I_{j}}H_{I_{j}j}>0\), using Guttman rank additivity formula \(H_{J_{j}J_{j}}\succ 0\), where \(J_{j}=I_{j}\cup j\). Since \(H_{J^{\prime}J^{\prime}_{j}}\) is a submatrix of \(H_{J_{j}J_{j}}\), it is positive definite and hence its schur complement \(H_{jj}-H^{T}_{I^{\prime}_{j}J^{\prime}_{j}}H^{-1}_{I^{\prime}_{j}j}H_{I^{ \prime}_{j}j}>0\). Thus for all \(j\in[n]\), the corresponding \(D_{jj}\)'s are well-defined in the new graph \(\tilde{\mathcal{G}}\).

Note that \(\kappa^{\tilde{\mathcal{G}}}_{\ell d}=\max_{i\in[n-1]}1/(1-\beta_{i}^{2})< \max_{i\in\bar{K}}1/(1-\beta_{i}^{2})=\kappa^{\tilde{\mathcal{G}}}_{\ell d}\), for tridiagonal graph, where \(\beta_{i}=H_{ii+1}\), in the case where \(H_{ii}=1\). This is because the \(\arg\max_{i\in[n-1]}1/(1-\beta_{i}^{2})\in K\).

### Additional Experiments, ablations, and details

#### a.4.1 Ablations

**Effect of band size in banded-SONew** Increasing band size in banded-SONew captures more correlation between parameters, hence should expectedly lead to better preconditioners. We confirm this through experiments on the Autoencoder benchmark where we take band size = 0 (diag-SONew), 1 (tridag-SONew), 4, and 10 in Table 3.

**Effect of mini-batch size** To find the effect of mini-batch size, in Table 4, We empirically compare SONew with state of the art first-order methods such as Adam and RMSProp, and second-order method Shampoo. We see that SONew performance doesn't deteriorate much when using smaller or larger batch size. First order methods on the other hand suffer significantly. We also notice that Shampoo doesn't perform better than SONew in these regimes.

**Effect of Numerical Stability Algorithm 3** On tridiag-SONew and banded-4-SONew, we observe that using Algorithm 3 improves training loss. We present in Table 5 results where we observed significant performance improvements.

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline
**Baseline\(\backslash\)Batch size** & **100** & **1000** & **5000** & **10000** \\ \hline \hline RMSProp & 55.61 & 53.33 & 58.69 & 64.91 \\ \hline Adam & 55.67 & 54.39 & 58.93 & 65.37 \\ \hline Shampoo(20) & 53.91 & 50.70 & 53.52 & 54.90 \\ \hline tds & 53.84 & 51.72 & 54.24 & 55.87 \\ \hline bds-4 & 53.52 & 51.35 & 53.03 & 54.89 \\ \hline \hline \end{tabular}
\end{table}
Table 4: **Comparison on Autoencoder with different batch-sizes**

#### a.4.2 Memory Requirements

We present a list of approximate memory requirements of different optimizers across different benchmarks in Table 6. Note that for K-FAC and Shampoo, because preconditioner is updated once only a few steps, they require storing the latest computed preconditioners as well along with the statistics, causing even higher memory overhead.

#### a.4.3 Hyperparaeter search space

We provide the hyperparamter search space for experiments presented in Section 5. We search over \(2k\) hyperparameters for each Autoencoder experiment using a Bayesian Optimization package. The search ranges are: first order momentum term \(\beta_{1}\in[1e-1,0.999]\), second order momentum term \(\beta_{2}\in[1e-1,0.999]\), learning rate \(\in[1e-7,1e-1]\), \(\epsilon\in[1e-10,1e-1]\). We give the optimal hyperparameter value for each experiment in Table 12. For VIT and GraphNetwork benchmark, we search \(\beta_{1},\beta_{2}\in[0.1,0.999]\), \(lr\in[1e-5,1e-1]\), \(\epsilon\in[1e-9,1e-4]\), weight decay \(\in[1e-5,1.0]\), learning rate warmup \(\in[2\%,5\%,10\%]\)+total_train_steps, dropout\(\in[00,0.1]\), label smoothing over \(\{0.0,0.1,0.2\}\). We use cosine learning rate schedule. Batch size was kept = 1024, and 512 for Vision Transformer, and GraphNetwork respectively. We sweep over \(200\) hyperparameters in the search space for all the optimizers.

For rfdSON [37], there's no \(\epsilon\) hyperparameter. In addition to the remaining hyperparameters, we tune \(\alpha\in\{1e-5,1.0\}\) (plays similar role as \(\epsilon\)) and \(\mu_{t}\in[1e-5,0.1]\).

For LLM [47] benchmark, we only tune the learning rate \(\in[1e-2,1e-3,1e-4]\) while keeping the rest of the hyperparams as constant. This is due to the high cost of running experiments hence we only tune the most important hyperparameter. For Adafactor [45], we use factored=False, decay method=adam, \(\beta_{1}=0.9\), weight decay=\(1e-3\), decay factor=\(0.99\), and gradient clipping=1.0.

#### a.4.4 Additional Experiments

**VIT and GraphNetwork Benchmarks**: In Figure 5 we plot the training loss curves of runs corresponding to the best validation runs in Figure 1. Furthermore, from an optimization point of view, we plot the best train loss runs in Figure 6 got by searching over \(200\) hyperparameters. We find that tridiag-SONew is \(9\%\) and \(80\%\) relatively better in ViT and GraphNetwork benchmark respectively (Figure 6), compared to Adam (the next best memory efficient baseline).

**Autoencoder float32 and bfloat16 experiments**: We provide curves of all the baselines and SONew in Figure 4(a) and the corresponding numbers in Table 7 for float32 experiments.

To test numerical stability of SONew and compare it with other algorithm in low precision regime, we also conduct bfloat16 experiments on the Autoencoder benchmark (Table 8). We notice that SONew undergoes the least degradation. Tridiagonal-sparsity SONew CE loss increases by only \(0.21\) absolute difference (from \(51.72\) in float32 (7) to \(51.93\)), whereas Shampoo and Adam incur \(0.70\) loss increase. It's worthwhile to note that SONew performs better than all first order methods while taking similar time and linear memory, whereas while Shampoo performs marginally better, it is \(22\times\) slower

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|} \hline
**Benchmark** & **\# model parameters** & **K-FAC** & **Shampoo** & **FishLeg** & **Eva** & **Adam** & **SGD+Momentum** & **RMSprop** & **lds-SONew** \\ \hline Autoencoder & n=1.4M & 5.56a & 6.56a & 4.28a & n & 2n & n & n & 3n \\ \hline GraphNetwork & n=3.5M & 8.6a & 10.6a & 4.8n & n & 2n & n & n & 3n \\ \hline Vision Transformer & n=22M & 6.4n & 7.2n & 3.7n & n & 2n & n & n & n \\ \hline Language Model & n=1.3B & 5.6n & 6.6n & 3.3n & n & 2n & n & n & 3n \\ \hline \end{tabular}
\end{table}
Table 6: A rough estimate of memory requirement comparisons of different optimizers tested across benchmarks.

\begin{table}
\begin{tabular}{|c|c|c|} \hline
**Optimizer** & **Train CE loss - without Algorithm 3** & **Train CE loss - with Algorithm 3** \\ \hline tridiag-SONew & 53.150 & 51.936 \\ \hline band-4-SONew & 51.950 & 51.84 \\ \hline \end{tabular}
\end{table}
Table 5: **bfloat16 experiments on Autoencoder benchmark with and without Algorithm 3.** We observe improvement in training loss when using Algorithm 3than tridiagonal-SONew. The corresponding loss curves are given in Figure 4(b).

**Note:** In the main paper, our reported numbers for rfdSON on Autoencoder benchmark in Table 2 for float32 experiments are erraneuous. Please consider the numbers provided in Table 7 and the corresponding curve in Figure 4(a). Note that there's no qualitiative change in the results and none of the claims made in the paper are affected. SONew is still significantly better than rfdSON. We also meticulously checked all other experiments, and they do not have any errors.

Autoencoder on KFAC, FishLeg, Eva:For completeness, We compare SONew against KFAC [38], FishLeg [20], and Eva [52] on Autoencoder benchmark as used in their official implementation.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline \hline \multicolumn{1}{|c|}{**Optimizer**} & \multicolumn{6}{|c|}{**First Order Methods**} \\ \hline  & **SGD** & **Nesterov** & **Adagrad** & **Momentum** & **RMSProp** & **Adam** & **diag-SONew** \\ \hline Train CE loss & 67.654 & 59.087 & 54.393 & 58.651 & 53.330 & 53.591 & 53.025 \\ \hline Time(s) & 62 & 102 & 62 & 67 & 62 & 62 & 63 \\ \hline \hline \multicolumn{1}{|c|}{**Optimizer**} & \multicolumn{6}{|c|}{**Second Order Methods**} \\ \hline  & **Shampoo(20)** & **rfdSON(1)** & **rfdSON(4)** & **tridig-SONew** & **band-4-SONew** \\ \hline Train CE loss & 50.702 & 53.56 & 52.97 & 51.723 & 51.357 \\ \hline Time(s) & 371 & 85 & 300 & 70 & 260 \\ \hline \hline \end{tabular}
\end{table}
Table 7: **float32 experiments on Autoencoder benchmark. We observe that diag-SONew performs the best among all first order methods while taking similar time. tridiagonal band-4 perform significantly better than first order methods while requiring similar linear space and time. Shampoo performs best but takes \(\mathcal{O}(d_{1}^{3}+d_{2}^{3})\) time for computing preconditioner of a linear layer of size \(d_{1}\times d_{2}\), whereas our methods take \(\mathcal{O}(d_{1}d_{2})\) time, as mentioned in Section 3.3. rfdSON takes similar space as SONew but performs considerably worse.**

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline \hline \multicolumn{1}{|c|}{**Optimizer**} & \multicolumn{6}{|c|}{**First Order Methods**} \\ \hline  & **SGD** & **Nesterov** & **Adagrad** & **Momentum** & **RMSProp** & **Adam** & **diag-SONew** \\ \hline \hline Train CE loss & 80.454 & 72.975 & 68.854 & 70.053 & 53.743 & 54.328 & 53.29 \\ \hline Train time(s) & 36 & 43 & 37 & 36 & 37 & 38 & 44 \\ \hline \hline
**Optimizer** & \multicolumn{6}{|c|}{**Second Order Methods**} \\ \hline  & **Shampoo(20)** & **rfdSON(1)** & **rfdSON(4)** & **tridig-SONew** & **band-4-SONew** \\ \hline \hline Train CE loss & 51.401 & 57.42 & 55.53 & 51.937 & 51.84 \\ \hline Train time(s) & 1245 & 80 & 284 & 55 & 230 \\ \hline \end{tabular}
\end{table}
Table 8: **bfloat16 experiments on Autoencoder benchmark to test the numerical stability of SONew and robustness of Algorithm 3. We notice that diag-SONew degrades only marginally (\(0.26\) absolute difference) compared to float32 performance. tridiag-SONew and band-4-SONew holds similar observations as well. Shampoo performs the best but has a considerable drop (\(0.70\)) in performance compared to float32 due to using matrix inverse, and is slower due to its cubic time complexity for computing preconditioners. Shampoo implementation uses 16-bit quantization to make it work in 16-bit setting, leading to further slowdown. Hence the running time inbfloat16 is even higher than in float32.**

Figure 4: Training curves of all the baselines for Autoencoder benchmark (a) float32 training (b)bfloat16 training

The main difference is their implementation uses ReLU activation compared to Tanh that we used for all our Autoencoder experiments. As these baselines done have JAX implementation, we use their official PyTorch implementation and run tridiag-SONew in PyTorch as well. Hyperparameter search is conducted for SONew similar to as reported above, over learning rate, \(\beta_{1},\beta_{2}\), and \(\epsilon\). For KFAC and Eva, rather than \(\beta_{2}\), damping factor is tuned over \([1e-5,10]\) (default value specified is 0.03). kl_clip is tuned as well over \([1e-5,1.0]\). Preconditioners are updates once every 15 iterations to have same wall clock time as other baselines and SONew. For FishLeg, auxiliary learning rate is tuned \(\in[1e-7,1e-1]\) and damping \(\in[1e-5,1.0]\). All other hyperparameters are tuned similar to SONew. Eva is trained for 100 epochs, and for other methods we change number of epochs such that

Figure 5: Train loss corresponding to the best validation runs in Figure 1 (a) VIT benchmark (b) GraphNetwork benchmark. We observe that tridiag-SONew match or perform better than Adam.

Figure 6: Best train loss achieved during hyperparam tuning. (a) VIT benchmark (b)GraphNetwork benchmark. We observe that tridiag-SONew significantly outperforms Adam, while being comparable or better than shampoo.

Figure 7: Autoencoder benchmark run using Pytorch on KFAC, FishLeg, Eva, and tridiag-SONew. We notice that tridiag-SONew beats all other baselines by a large margin.

[MISSING_PAGE_FAIL:30]

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline \hline
**Baseline** & \(\beta_{1}\) & \(\beta_{2}\) & \(\epsilon\) & **lr** \\ \hline SGD & 0.99 & 0.91 & 8.37e-9 & 1.17e-2 \\ Nesterov & 0.914 & 0.90 & 3.88e-10 & 5.74e-3 \\ Adagrad & 0.95 & 0.90 & 9.96e-7 & 1.82e-2 \\ Momentum & 0.9 & 0.99 & 1e-5 & 6.89e-3 \\ RMSProp & 0.9 & 0.9 & 1e-10 & 4.61e-4 \\ Adam & 0.9 & 0.94 & 1.65e-6 & 3.75e-3 \\ Diag-SONew & 0.88 & 0.95 & 4.63e-6 & 1.18e-3 \\ \hline Shampoo & 0.9 & 0.95 & 9.6e-9 & 3.70e-3 \\ tridiag & 0.9 & 0.96 & 1.3e-6 & 8.60e-3 \\ band-4 & 0.88 & 0.95 & 1.5e-3 & 5.53e-3 \\ \hline \hline \end{tabular}
\end{table}
Table 13: (a) float32 experiments optimal hyperparamters

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline \hline
**Baseline** & \(\beta_{1}\) & \(\beta_{2}\) & \(\epsilon\) & **lr** \\ \hline SGD & 0.96 & 0.98 & 2.80e-2 & 1.35e-2 \\ Nesterov & 0.914 & 0.945 & 8.48e-9 & 6.19e-3 \\ Adagrad & 0.95 & 0.93 & 2.44e-5 & 2.53e-2 \\ Momentum & 0.9 & 0.99 & 0.1 & 7.77e-3 \\ RMSProp & 0.9 & 0.9 & 2.53e-10 & 4.83e-4 \\ Adam & 0.9 & 0.94 & 3.03e-10 & 3.45e-3 \\ Diag-SONew & 0.9 & 0.95 & 4.07e-6 & 8.50e-3 \\ \hline Shampoo & 0.85 & 0.806 & 6.58e-4 & 5.03e-3 \\ ztridiag & 0.83 & 0.954 & 1.78e-6 & 7.83e-3 \\ band-4 & 0.9 & 0.96 & 1.52e-6 & 4.53e-3 \\ \hline \hline \end{tabular}
\end{table}
Table 14: (b) bfloat16 experiments optimal hyperparamters