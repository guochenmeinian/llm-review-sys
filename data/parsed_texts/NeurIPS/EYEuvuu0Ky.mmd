# Language models in molecular discovery

 Nikita Janakarajan\({}^{1}\)  Tim Erdmann\({}^{2}\)  Sarath Swaminathan\({}^{2}\)  Teodoro Laino\({}^{1}\)  Jannis Born\({}^{1}\)

\({}^{1}\)IBM Research Europe, Zurich, Switzerland \({}^{2}\)IBM Research Almaden, San Jose, CA, United States {nja,teo,jab}@zurich.ibm.com

{tim.erdmann,sarath.swaminathan}@ibm.com

###### Abstract

The success of language models, especially transformers in natural language processing, has trickled into scientific domains, giving rise to the concept of "scientific language models" that operate on small molecules, proteins or polymers. In chemistry, language models contribute to accelerating the molecule discovery cycle as evidenced by promising recent findings in early-stage drug discovery. In this perspective, we review the role of language models in molecular discovery, underlining their strengths and examining their weaknesses in de novo drug design, property prediction and reaction chemistry. We highlight valuable open-source software assets to lower the entry barrier to the field of scientific language modeling. Furthermore, as a solution to some of the weaknesses we identify, we outline a vision for future molecular design that integrates a chatbot interface with available computational chemistry tools. Our contribution serves as a valuable resource for researchers, chemists, and AI enthusiasts interested in understanding how language models can and will be used to accelerate chemical discovery.

## 1 Introduction

The Turing test - envisioned in 1950 as a machine's ability to simulate human behavior to the extent of indiscernibility - served for decades as the holy grail of artificial intelligence (AI). Recently, language models (LMs) have demonstrated an astonishing ability to understand and generate human-like text [61]. Thanks to this remarkable progress over the last 5-10 years, the perception of the Turing test has undergone a sudden turnaround, shifting from a heavily-debated and largely deemed intractable challenge to a silent, yet widespread acknowledgement of its decipherment. Machine learning (ML) in general and LMs in particular hold the potential to profoundly accelerate the molecular discovery cycle (see Figure 1). Here, we explore applications of LMs to chemical design tasks.

Despite technological advances constantly reshaping our understanding of biochemical processes, the chemical industry persistently faces escalating resource costs of up to 10 years and 3 billion dollars per new market release [96]. The intricacy of the problem is typically attested by an exorbitant attrition rate in _in vitro_ screenings [73], the sheer size of the chemical space [63] and the frequency of serendipity [37].

Although LMs were originally developed for natural language, they have shown compelling results in scientific discovery settings when applied to "scientific languages", e.g., in protein folding [51] or _de novo_ design of small molecules [99], peptides [21] or polymers [62]. But what exactly is a language model? By definition, it is any ML model that consumes a sequence of text chunks (so-called tokens) and is capable to reason about the content of the sequence. Since each token is essentially a vector [58], a LM is a pseudo-discrete time series model. Most typically, LMs learn probability distributions over sequences of words thus also facilitating the generation of new text given some input, for example in a language translation task. While all LMs rely on neural networks,contemporary models almost exclusively leverage the Transformer architecture [87]. Now, all of this begs the question - what is the need for LMs in molecular discovery?

First, when applied to serializations of chemical entities (e.g., SMILES [92]), LMs can learn highly structured representations, often even tailored for desired functional properties [33]. This allows to perform smooth and property-driven exploration of the originally deemed discrete protein or molecular space. Another attractive feature of scientific LMs is their ability to seamlessly bridge natural and scientific languages. This can give rise to ChatGPT-style chatbot interfaces that allow chemists to formulate their design objectives through natural language and to iteratively refine their result with an interactive agent thus potentially accomplishing complex chemical tasks more rapidly. Nevertheless, large-language models (LLMs) like GPT, which power conversational agents, lack knowledge about scientific operations (e.g. molecular discovery), access to information sources providing up-to-date data, and the ability to accurately reference. They tend to hallucinate in their responses, which raises questions about credibility, trust, and applicability. However, this crucial gap between AI and science can be overcome by integrating task-specific agents into the LLM-powered conversational application and allowing the LLM to reason over their appropriate usage based on provided instructions. This also eliminates the application barriers associated with expert-developed AI models, which typically require originating programming and AI/ML skills from the intended user group, often comprising lab scientists. Additionally, it can be anticipated that this will result in a significant increase in the utilization of the developed AI models and contribute to scientific discovery. Here, we present an overview of the role of LMs toward accelerated molecular discovery. We commence with the conventional scientific discovery method and then discuss how molecular generative models can be coupled with molecular property prediction models. Next, we provide readers looking for practical usability with a curated list of software tools and libraries for scientific language modeling. We conclude by envisioning the future of molecule design, where natural language models, custom-built AI models, and cheminformatics tools are integrated into the discovery process via chatbot user interfaces.

## 2 Accelerated molecular discovery

Molecule discovery, intricately linked to optimizing diverse properties in a vast space, challenges conventional scientific methods. In chemistry's Design-Make-Test-Analyze (DMTA) cycle, synthesis costs and time constraints create a bottleneck that hampers hypothesis refinement (cf. Figure 0(a)). Traditional approaches are largely driven by medicinal chemists who design "molecule hypotheses" which are biased, ad-hoc and non-exhaustive. This hinders progress in addressing global issues, creating a necessity for an accelerated process of molecule discovery. Thus, a key challenge lies in improving the speed and quality of evaluating such "molecule hypotheses", which are grounded on laboratory work.

Figure 1: A comparison of molecular discovery workflows: (a) classic approach, where each hypothesis (a.k.a. molecule) requires a new experimental cycle. (b) _Accelerated_ molecular discovery cycle with machine-generated hypotheses and assisted validation, enabling simultaneous generation and testing of numerous molecules.

Deep generative models have recently emerged as a promising tool to expedite the hypothesis/design phase in molecular discovery. However, even the most advanced molecular generative models require an efficient method for large-scale virtual screening to test their hypotheses. The _accelerated molecular discovery_ cycle adds a validation loop to DMTA, rapidly evaluating numerous hypotheses inexpensively (cf. Figure 0(b)). This loop enhances the design-phase generative model, ensuring only promising hypotheses advance to the synthesis and physical experimentation stages.

### Molecule Representation

Data representation plays a crucial role in molecular discovery. It determines the type of information that is available to the model and consequently the properties that can be predicted. An overview of commonly used molecular representations for property prediction is illustrated in Figure 2. Due to the popularity of chemical language models (CLMs), this section focuses on text-representations of molecules. A more focused discussion on CLMs is covered by Grisoni [35].

Simplified Molecular Input Line-Entry System (SMILES)SMILES [92] is a string representation made up of specific characters for atoms, bonds, branches, aromaticity, rings and stereochemistry in molecular structures. The character-level representation enables easy tokenization, making SMILES an ideal input for LMs. SMILES are typically tokenized at the atom level [75; 86]. For LMs to learn from SMILES, tokens are typically vectorized either via one-hot encodings (where each row in the binary matrix corresponds to a SMILES position and each column signifies a token) or by learning a continuous embedding for each token during training.

Self Referencing Embedded Strings (SELFIES)SELFIES [46] were introduced as an alternative to SMILES to counter the problem of generating invalid molecules. Unlike SMILES, SELFIES are generated using derivation rules to enforce valence-bond validity, and additonally store branch length and ring size.

International Chemical Identifier (InChI)Introduced by the IUPAC, InChI [38] are strings encoding structural information including charge of the molecule in a hierarchical manner. InChIs are less commonly used in LMs [36].

### Generative Modelling

Generative modeling involves learning the data's underlying distribution with the intent of generating new samples, a technique pivotal in accelerating de novo drug discovery. A generative model may be conditional or unconditional. A conditional generative model utilizes provided data attributes or labels to generate new samples with desired properties, whereas an unconditional model solely

Figure 2: An illustration of popular ways of representing a chemical molecule as input to a ML model. The representations may be (a) String-based, such as SMILES, SELFIES, or InChI which use characters to represent different aspects of a molecule, (b) Structure-based, such as Graphs or MolFiles that encode connectivity and atomic position, and (c) Feature-based, such as Morgan Fingerprints, which encode local substructures as bits.

provides a way to sample molecules similar to the training data [33]. The DMTA cycle particularly benefits from the conditional generation approach as it facilitates goal-oriented hypothesis design [7]. This section describes a few influential conditional generation models that act on chemical language to generate molecules satisfying user-defined conditions.

Recurrent Neural Network (RNN)The sequential nature of RNNs makes them suitable models for processing chemical languages. Proposed in the 90s, RNNs were the first type of CLMs to enter the domain [6; 75; 81]. RNNs continuously update their hidden states as new tokens are passed to the network, thus enabling it to encode contextual information. During the generation process, tokens are produced auto-regressively. RNNs find use in generating molecule libraries [81] which are extensively used in drug development processes like screening. External scoring functions drive the generation of molecules with desired properties. RNNs are also adept at learning complex distributions [28] and generating a higher proportion of unique and valid SMILES [64], even though their inability to count occurrences of ring opening/closing symbols poses a challenge [43; 65].

Variational Autoencoder (VAE)VAEs learn latent distribution parameters of molecules, thus enabling the generation of new molecules by sampling from this distribution. Their unique ability lies in learning a smooth, latent space that facilitates interpolation of samples, even for notoriously discrete entities like molecules [33]. To make it suitable for chemical language models (CLMs), any network compatible with string inputs can function as a VAE's encoder and decoder. Initial works primarily focused on single-modality applications, assessing latent space quality via downstream tasks [33]. This approach remains prevalent and can be used to generate, e.g., catalysts with an RNN-based VAE [74]. Here, a latent space is learned and assessed by predicting the catalyst binding energy. Lim et al. [49] takes it a step further by concatenating a condition vector to the input and the latent embedding generated by the recurrent network-based VAE's encoder. This approach enables the generation of molecules specifically tailored to the given conditions. The scope of VAEs expanded progressively into multi-modal settings for conditional molecule generation, as visualized in Figure 3 and exemplified by Born et al. [9; 10; 11]. These works on task-driven molecule generation incorporate contextual information like gene expression [11] or protein targets [9; 10] or even both [42]. VAEs learn embeddings of context information and primer drugs, which are merged before decoding to produce molecules. A reinforcement-learning-based approach directs the model to produce molecules with desired properties using rewards.

TransformerThe self-attention attribute of Transformers [87] have propelled these models to the forefront of NLP. Transformers have an encoder module that relies on this self-attention to learn

Figure 3: An illustration of conditional molecule generation using LMs. The process initiates with the collection and processing of multi-modal data, which is then compressed into a fixed-size latent representation. These representations are subsequently passed to a molecular generative model. The generated molecules then undergo in-silico property prediction, which is linked back to the generative model through a feedback loop during training. The in-silico models direct the generative model to produce property- or task-driven molecules using a reward function. In the inference stage, candidate molecules generated by the optimized model undergo lab synthesis and subsequent experimental validation to determine their efficacy for the desired task.

embeddings of the input and the context associated with this input. The decoder module predicts tokens using the context learnt by the encoder and previously generated tokens through attention. For generative modeling, decoder-only transformer like the Generative Pre-Training Transformer (GPT) [67] have become the dominant approach. This success was translated to the scientific language domain. One of the first models to use the GPT architecture for conditional molecule generation is MolGPT [3]. SMILES tokens concatenated with a condition vector that summarizes the desired properties and scaffolds are passed as input to this model, which is then trained on the next token prediction task to generate molecules. GPT-like models coupled with RL can also be used to optimize molecular properties like pIC50 [57]. In this two-stage approach, embeddings are first learnt from SMILES strings, and the embedding space is then optimized such that the model samples molecules with the desired properties. Going beyond just using GPT-like architectures for molecule generation, Regression Transformer [8] is a seminal work that formulates conditional sequence modeling as a regression problem. This gives rise to a natural multitask model that concurrently performs property prediction and conditional molecular generation. This is achieved by concatenating conventional molecular tokens with property tokens and employing an training scheme that alternates which parts of the sequence are masked.

The superior quality of learned embeddings coupled with its ability to handle parallel processing and scalability makes the Transformer a top choice for the task of conditional molecule generation, with promising applications in drug discovery and other areas of molecular design [62].

### Property Prediction

Property prediction is a key step in validating the molecules for a given use case. The success of a molecule depends on a myriad of factors, including how it interacts with its environment. The MoleculeNet datasets [97] are a commonly used benchmark for property prediction. It is curated from public datasets and comprises over 700,000 compounds tested on various properties. A recent trend is to use transformer-encoders to learn embeddings for molecules and then apply a multilayer perceptron (MLP) on the embeddings for property prediction. MolBERT [26] and ChemBERTA [18]) are two such examples. These transformer-based models use a BERT backbone to learn molecular embeddings from SMILES and predict properties. Similarly, Molformer [71] uses a transformer-encoder with linear attention and relative positional encoding to learn compressed molecular representations which are then fine-tuned on chemical property prediction benchmarks. To equip transformers with better inductive biases to handle molecules, adaptations of the attention mechanism were proposed. The molecule attention transformer (MAT) incorporates inter-atomic distances and graph structure into the attention mechanism [54]. An improvement over this model is the _relative_-MAT which fuses the distance embedding, bond embedding and neighbourhood embedding and achieves competitive performances on a range of property prediction tasks [55].

## 3 Software tools for scientific language modeling

The paradigm shift towards open-sourcing software has profoundly influenced chemistry. Commonly listed implications of open-sourcing in the context of drug discovery include catalyzation of methodological development, fostering collaboration and ease of scientific reproducibility [32]. In this section we present several software assets (e.g., Python packages or cloud-based web apps) that are key to enable molecular discovery.

Natural language modelsThe success story of the Transformer [87] as most widely adopted neural network architecture goes hand in hand with the rise of the transformers library [95]. Initially intended for NLP applications, Transformers were adopted across disciplines, e.g in computer vision [22], reinforcement learning [17], protein folding [44] and naturally, chemistry [80]. _HuggingFace_ provides the largest public hub of LMs and it offers implementations of all recent models as well as a diverse collection of pretrained models available for fine-tuning or inference. While most of their models focus on NLP, select models are designed for life science applications, in particular, molecular property prediction (e.g., _ChemBerta_[18]), molecular captioning (e.g., _MolT5_[23]), text-based molecular generation (e.g., _MolT5_[23]) and unsupervised protein language modeling (e.g., _ProtBert_, _ProtAlbert_, _ProtXLNet_ and _ProtT5_[24]). Furthermore, some available models like _Multimodal Text and Chemistry T5_[20] are prompt-based multi-taskers that extend beyond the above mentioned tasks to include additional functions like predicting forward/backward reactions.

GT4SD - Generative Toolkit for Scientific DiscoveryPython libraries like gt4sd[53]), TdC (Therapeutics Data Commons[40]) or deepchem[68] were developed primarily for molecular discovery applications. gt4sd in particular provides extensive support for CLMs. GT4SD is designed to enable researchers and developers to use, train, fine-tune and distribute state-of-the-art generative models for sciences with a focus on organic materials. It is compatible and inter-operable with many existing libraries such as transformers,diffusers[90], torchdrug[100]) or tape[69]. Besides established benchmarks for molecular generation, such as Moses[64] and GuacaMol[14] which includes VAEs, generative adversarial networks (GANs), genetic algorithms, and many evaluation metrics for molecular design, gt4sd also provides supports for contemporary models like the _Regression Transformer_ for concurrent sequence regression and property-driven molecular design [8], _GFlowNets_ for highly diverse candidate generation [4] and _MoLeR_ for motif-constrained molecule generation [56]. Models can be trained through a CLI with a few lines of code and can be shared to a cloud-hosted model hub. It is built to facilitate consumption by containerization or distributed computing systems,includes \(\sim 50\) property prediction endpoints for small molecules, proteins and crystals, and overall hosts \(\sim 30\) pre-trained algorithms for material design, \(20\) free webapps[2] and many Jupyter/Colab notebooks.

RXN for Chemistry: Reaction and synthesis language modelsOnce a molecule has been selected for experimental validation, a tangible synthesis route has to be identified. Since the most important tasks in chemical reaction modeling can be framed as sequence conversion problems, the methodology developed for natural language translation can be seamlessly translated to chemistry [80]. The most mature and flexible library for reaction modeling with LMs is the package rxn4chemistry[29]. It wraps the API of the _IBM RXN for Chemistry_ platform, a freely accessible webapp that gives access to a rich set of CLMs for different tasks in reaction chemistry. The primary architecture is the _Molecular Transformer_ (MT), an autoregressive encoder-decoder model, originally applied to predict outcomes of chemical reactions in organic chemistry [76]. The MT was applied to single-step retrosynthesis [85] and became vital to multi-step retrosynthesis model with a hypergraph exploration strategy [77]. This approach was later generalized to enzymatic reactions with a tokenization scheme based on enzyme classes, facilitating biocatalyzed synthesis planning, and paving the road towards greener chemistry [66]. Derivatives of the MT helped to enhance diversity in single-step retrosynthesis [85] and a prompt-based disconnection scheme improved controllability by allowing the user to mark a disconnection side in the reactant [84]. Interestingly, an encoder-only derivative of the MT excelled in predicting reaction classes [79]. Its hidden representations were found to encode reaction types thus allowing to map reaction atlases and to perform reaction similarity search through the rxnfp package for reaction fingerprinting. Strikingly, this led to the discovery that the learned attention weights of the Transformer are "secretly" performing atom mapping between products and reactions [78].

Once the precursors for a synthesis route are identified, the subsequent phase seeks for an actionable, stepwise synthesis protocol that is ideally amenable for autonomous execution on a robotic platform, such as _IBM RoboRXN_. In two seminal works Vaucher et al. demonstrated that encoder-decoder Transformers can extract chemical synthesis actions, first from experimental procedures described in patents [88] and later predict them directly from the reaction SMILES [89]. These models are available via the _IBM RXN for Chemistry_ platform which even allows to control and monitor the robotic platform directly from the web interface. For multistep retrosynthesis, _RXN_ also includes other models like _AiZynthFinder_[31], a Monte Carlo Tree Search approach build on top of a RNN.

Specialized librariesRDKit[47] remains the best and often only library for manipulating molecules in Python. HuggingMolecules is a library solely devoted to aggregating, standardizing and distributing molecular property prediction CLMs [30]. It contains many encoder-only models, some of them with geometrical and structure-aware inductive biases (e.g., the MAT [54] or its successor, the R-MAT [55]) while others are pure BERT-based models that were trained on SMILES (e.g,. _MolBERT_[26] or _ChemBERTA_[18]). For narrower applications, like ML data preparation, several tools exist. First, rxn-chemutils is a library with chemistry-related utilities from RXN for Chemistry. It includes functionalities for standardizing SMILES (e.g., canonicalization or sanitization) but also conversions to other representations (e.g., InChI). It harmonizes reaction SMILES and prepares them for consumption by CLMs, including also SMILES augmentation and tokenization. Another library with a similar focus is pytoda[10, 11]. It does not support reaction SMILES but implements richer preprocessing utilities, allowing to chain \(>\)10 SMILES transformations (e.g., kekulization [13]). It supports different languages (e.g., SELFIES [46] or BigSMILES [50]) and tokenization schemes (e.g.,SMILES-PE [48]). Similar functionalities are available for proteins including different languages (IUPAC, UniRep or Blosum62) and protein sequence augmentation strategies [12].

MELLODDY [39] is a collaborative effort aimed at cross-pharma federated learning (i.e., preserving privacy through decentralized, distributed training) of 2.6 billion confidential activity data points. Similarly, VirtualFlow [34] is an open-source platform facilitating large-scale virtual screening that was shown to identify potent KEAP1 inhibitors. With a focus on _de novo_ drug design, Chemistry42 [41] is a proprietary platform integrating AI with computational and medicinal chemistry techniques.

## 4 Future of molecular discovery

A few years ago, the idea of querying an AI model - like one would a search engine - to not only extract scientific knowledge but also perform computational analyses was an overly ambitious feat. Scientific thinking comes from the ability to reason, and AI models cannot reason like humans, yet. However, these models can **learn** from humans. Our propensity to document everything has enabled us to train Large Language Models (LLMs), like ChatGPT [60] and GitHub Copilot [1], to mimic human responses. When brought into the context of computational science, this could equip non-experts to confidently conduct computational analyses through well-designed prompts. With human-in-the-loop, a synergistic effect could be created where the scientist provides feedback to the model on its output, thus aiding in better model optimization (a strategy called reinforcement learning from human feedback (RLHF) that has been proven critical for ChatGPT [19]). These applications also reduce the barrier for individuals from non-scientific backgrounds to gain a more hands-on experience in conducting scientific analyses without having to go through formal training in computational analysis.

This section provides a sneak peak into what's next for molecular discovery. Riding the LLM wave, the future holds a place for chatbot-like interfaces that may take care of all things computational, comprising generating and improving design ideas, synthesis planning, purchasing, and validation.

### The rise of foundation models in chemistry

Conventionally, neural networks are trained for a single given task to achieve maximum performance. This essentially renders the models useless for other tasks, thus requiring a new model for every new task, even when the training domain is the same, which in turn imposes a constraint on the rate of our technological advancements. Over the last few years, this conventional approach has been challenged by Large Language Models (LLMs). It has been found that scaling up LLMs leads to astonishing performance in few-shot [15] and even zero-shot task generalization [72]. Referred to as "foundation models" [27; 59], these models, with typically billions of parameters, can perform multiple tasks despite being trained on one large dataset. Essentially, this multi-task learning is achieved by prompting LLMs with task instructions along with the actual query text which has been found to induce exceptional performance in natural language inference and sentence completion [72]. These findings have kicked off new research directions, such as prompt engineering [91] and in-context learning [15], in NLP.

Foundation models find an increasing adoption in chemistry with an increase in task-specific models integrating natural and chemical languages [23; 88; 89; 98]. Concurrently, multi-tasking in pure CLMs has also been advancing through models that combined tasks such as property prediction, reaction prediction and molecule generation either with small task-specific heads (e.g., T5Chem [52]) or via mask infilling (e.g., Regression Transformer [8]). Christofidellis et al. [20] were the first to bridge the gap and develop a fully prompt-based multi-task chemical and natural language model. Despite only 250M parameters, the _Multitask Text and Chemistry T5_ was shown to outperform ChatGPT [60] and Galactica [83] on a contrived discovery workflow for re-discovering a common herbicide (natural text \(\rightarrow\) new molecule \(\rightarrow\) synthesis route \(\rightarrow\) synthesis execution protocol).

### The coalescence of chatbots with chemistry tools

Given the aforementioned strong task generalization performances of LLMs, building chatbot interfaces around it was a natural next step and thus next to ChatGPT [60], many similar tools were launched. Such tools were found to perform well on simplistic chemistry tasks [16; 93], opening potential to reshape how chemists interact with chemical data, enabling intuitive access to complex concepts and make valuable suggestions for diverse chemical tasks. Furthermore, AI models specifically developed by computer scientists for e.g. drug discovery or material science can be made available through applications powered by LLMs, such as chatbots. This minimizes the access barrier for subject matter experts who would otherwise require the respective programming skills to utilize these AI models. The power of such chatbots is reached through the coalscence of LLMs and existing chemistry software tools like PubChem [45], RDKit [47] or GT4SD [53]. Together, such applications can unleash the full potential and value of these models by the strongly enhanced usage. An example of how the interaction with such a tool could look like is shown in Figure 4.

In this example, a user provides a molecule and requests identification. The chatbot relies on prompt-engineering in order to inform the LLM about all its available tools. The user input is first sent to the LLM which recognizes that one of its supported tools, in this case PubChem, can answer the question. The application then sends a request to the PubChem API and returns a concise description of the molecule. The user subsequently asks to compute the logP partition coefficient [94] and the quantitative estimate of drug-likeness (QED) [5]. Calculation of both properties is enabled through GT4SD [53] and will trigger a programming routine to accurately format the API request to the instance of GT4SD. The post-processing routine formats the LLM-generated string reply and composes the response object for the frontend. This fusion of LLMs with existing tools gives rise to a chatbot assistant for material science and data visualization that can perform simple programming routines without requiring the user to know programming or have access to compute resources.

A continuation of the conversation involving more complex user queries follows with a generative task (Figure 4, right). Having identified the initial molecule as theobromine with a logP of -1.04, the user requests three similar molecules with a slightly increased logP of -0.5. Here, ChemChat identifies the Regression Transformer [8] as the available tool to perform substructure-constrained, property-driven molecule design. Once the routine has been executed and three candidate SMILES are collected, the response is enriched by data of the generated molecules.

In conclusion, chatbots can facilitate the integration of essentially all major cheminformatics software in a truly harmonized and seamless manner. While LLMs are not intrinsically capable to perform complex routines, at least not with high precision and in a trustworthy manner, the synergy between their natural language abilities and existing chemistry tools has the potential to transform the way molecular discovery is performed.

Figure 4: Screenshots of the LLM-powered chatbot application ChemChat. Embedding the capabilities of existing resources such as identification through PubChem [45], property and similarity calculation through RDKit [25, 47, 70, 82] or generative tasks through GT4SDâ€™s Regression Transformer [8, 53] enables the assistant to execute programming routines in the background and, thus, to answer highly subject-matter specific requests without the need for programming skills by the user.

## References

* [1]G. A. Bakerton, G. V. Paolini, J. Besnard, S. Muresan, and A. L. Hopkins (2012) Quantifying the chemical beauty of drugs. Nat. Chem.4 (2), pp. 90-98. Cited by: SS1.
* [2]A. Abid, A. Abdalla, A. Abid, D. Khan, A. Alfozan, and J. Zou (2019) Gradio: hassle-free sharing and testing of ml models in the wild. arXiv preprint arXiv:1906.02569. Cited by: SS1.
* [3]V. Bagal, R. Aggarwal, P. Vinod, and U. P. Priyakumar (2021) Molgpt: molecular generation using a transformer-decoder model. Journal of Chemical Information and Modeling62 (9), pp. 2064-2076. Cited by: SS1.
* [4]Y. Bengio, T. Deleu, E. J. Hu, S. Lahlou, M. Tiwari, and E. Bengio (2021) Offownet foundations. Preprint at https://arxiv.org/abs/2111.09266. Cited by: SS1.
* [5]G. R. Bickerton, G. V. Paolini, J. Besnard, S. Muresan, and A. L. Hopkins (2012) Quantifying the chemical beauty of drugs. Nat. Chem.4 (2), pp. 90-98. Cited by: SS1.
* [6]E. Jannik Bjerrum (2017) Smiles enumeration as data augmentation for neural network modeling of molecules. arXiv preprint arXiv:1703.07076. Cited by: SS1.
* [7]J. Born, T. Huynh, A. Stroobants, W. D Cornell, and M. Manica (2021) Active site sequence representations of human kinases outperform full sequence representations for affinity prediction and inhibitor generation: 3d effects in a 1d model. Journal of Chemical Information and Modeling62 (2), pp. 240-257. Cited by: SS1.
* [8]J. Born, M. Manica, J. Cadow, G. Markert, N. Adell Mill, M. Filipavicius, N. Janakarajan, A. Cardinale, T. Laino, and M. Rodriguez Martinez (2021) Data-driven molecular design for discovery and synthesis of novel ligands: a case study on sars-cov-2. Mach. Learn.: Sci. Technol.2 (2), pp. 025024. Cited by: SS1.
* [9]J. Born, M. Manica, A. Oskooei, J. Cadow, G. Markert, and M. Rodriguez Martinez (2021) PACMann\({}^{\text{RL}}\): de novo generation of hit-like anticancer molecules from transcriptomic data via reinforcement learning. iScience24 (4), pp. 102269. Cited by: SS1.
* [10]J. Born, Y. Shoshan, T. Huynh, W. D. Cornell, E. J. Martin, and M. Manica (2022) On the choice of active site sequences for kinase-ligand affinity prediction. Journal of chemical information and modeling62 (18), pp. 4295-4299. Cited by: SS1.
* [11]J. Born, G. Mankert, N. Janakarajan, T. B. Kimber, A. Volkamer, M. Rodriguez Martinez, and M. Manica (2023) Chemical representation learning for toxicity prediction. Digital Discovery. Cited by: SS1.
* [12]J. Born, M. Manker, J. Cadow, G. Markert, N. Adell Mill, M. Filipavicius, N. Janakarajan, A. Cardinale, T. Laino, and M. Rodriguez Martinez (2021) Data-driven molecular design for discovery and synthesis of novel ligands: a case study on sars-cov-2. Mach. Learn.: Sci. Technol.2 (2), pp. 025024. Cited by: SS1.
* [13]J. Born, G. Markert, N. Janakarajan, T. B. Kimber, A. Volkamer, M. Rodriguez Martinez, and M. Manica (2023) Chemical representation learning for toxicity prediction. Digital Discovery. Cited by: SS1.
* [14]N. Brown, M. Fiscato, M. H. Segler, and A. C. Vaucher (2019) Guacamol: benchmarking models for de novo molecular design. J. Chem. Inf. Model.59 (3), pp. 1096-1108. Cited by: SS1.
* [15]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* [16]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [17]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [18]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [19]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [20]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [21]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [22]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [23]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [24]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [25]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [26]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [27]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [28]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [29]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [30]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [31]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [32]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [33]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [34]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [35]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [36]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [37]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [38]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [39]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [40]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [41]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [42]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [43]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [44]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS1.
* [45]C. Monteiro Castro Nascimento and A. Silva Pimentel (2023) Do large language models understand chemistry? a conversation with chatopt. Journal of Chemical Information and Modeling63 (6), pp. 1649-1655. Cited by: SS* Chen et al. [2021] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. _Advances in neural information processing systems_, 34:15084-15097, 2021.
* Chithrananda et al. [2020] Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar. Chemberta: large-scale self-supervised pretraining for molecular property prediction. _arXiv preprint arXiv:2010.09885_, 2020.
* Christiano et al. [2017] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _Advances in neural information processing systems_, 30, 2017.
* Christofidellis et al. [2023] Dimitrios Christofidellis, Giorgio Giannone, Jannis Born, Ole Winther, Teodoro Laino, and Matteo Manica. Unifying molecular and textual representations via multi-task language modelling. In _International Conference on Machine Learning_, 2023.
* Das et al. [2021] Payel Das, Tom Sercu, Kahini Wadhawan, Inkit Padhi, Sebastian Gehrmann, Flaviu Cipcigan, Vijil Chenthamarakshan, Hendrik Strobelt, Cicero Dos Santos, Pin-Yu Chen, et al. Accelerated antimicrobial discovery via deep generative models and molecular dynamics simulations. _Nat. Biomed. Eng._, 5(6):613-623, 2021.
* Dosovitskiy et al. [2021] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In _9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021_, 2021.
* Edwards et al. [2022] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, Kyunghyun Cho, and Heng Ji. Translation between molecules and natural language. In _2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022_, 2022.
* Elnaggar et al. [2021] Ahmed Elnaggar, Michael Heinzinger, Christian Dallago, Ghalia Rehawi, Wang Yu, Llion Jones, Tom Gibbs, Tamas Feher, Christoph Angerer, Martin Steinegger, Debsindhu Bhowmik, and Burkhard Rost. Prottrans: Towards cracking the language of life's code through self-supervised deep learning and high-performance computing. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, pages 1-1, 2021. doi: 10.1109/TPAMI.2021.3095381.
* Ertl and Schuffenhauer [2009] Peter Ertl and Ansgar Schuffenhauer. Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions. _Journal of cheminformatics_, 1:1-11, 2009.
* Fabian et al. [2020] Benedek Fabian, Thomas Edlich, Helena Gaspar, Marwin Segler, Joshua Meyers, Marco Fiscato, and Mohamed Ahmed. Molecular representation learning with language models and domain-relevant auxiliary tasks. _arXiv preprint arXiv:2011.13230_, 2020.
* Fei et al. [2022] Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jingyuan Wen, Haoyu Lu, Ruihua Song, Xin Gao, Tao Xiang, et al. Towards artificial general intelligence via a multimodal foundation model. _Nature Communications_, 13(1):3094, 2022.
* Flam-Shepherd et al. [2022] Daniel Flam-Shepherd, Kevin Zhu, and Alan Aspuru-Guzik. Language models can learn complex molecular distributions. _Nature Communications_, 13(1):3293, 2022.
* [29] IBM RXN for Chemistry team. rxn4chemistry: Python wrapper for the IBM RXN for Chemistry API. https://github.com/rxn4chemistry/rxn4chemistry, 2023.
* Gainski et al. [2022] Piotr Gainski, Lukasz Maziarka, Tomasz Danel, and Stanislaw Jastrzebski. Huggingmolecules: An open-source library for transformer-based molecular property prediction (student abstract). In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 36, pages 12949-12950, 2022.
* Genheden et al. [2020] Samuel Genheden, Amol Thakkar, Veronika Chadimova, Jean-Louis Reymond, Ola Engkvist, and Esben Bjerrum. Aizynthfinder: a fast, robust and flexible open-source software for retrosynthetic planning. _Journal of cheminformatics_, 12(1):70, 2020.

* [32] J Daniel Gezelter. Open source and open data should be standard practices, 2015.
* [33] Rafael Gomez-Bombarelli, Jennifer N Wei, David Duvenaud, Jose Miguel Hernandez-Lobato, Benjamin Sanchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alan Aspuru-Guzik. Automatic chemical design using a data-driven continuous representation of molecules. _ACS central science_, 4(2):268-276, 2018.
* [34] Christoph Gorgulla, Andras Boeszoermenyi, Zi-Fu Wang, Patrick D Fischer, Paul W Coote, Krishna M Padmanabha Das, Yehor S Malets, Dmytro S Radchenko, Yurii S Moroz, David A Scott, et al. An open-source drug discovery platform enables ultra-large virtual screens. _Nature_, 580(7805):663-668, 2020.
* [35] Francesca Grisoni. Chemical language models for de novo drug design: Challenges and opportunities. _Current Opinion in Structural Biology_, 79:102527, 2023.
* [36] Jennifer Handsel, Brian Matthews, Nicola J Knight, and Simon J Coles. Translating the inchi: adapting neural machine translation to predict iupac names from a chemical identifier. _Journal of cheminformatics_, 13(1):1-11, 2021.
* [37] Emily Hargrave-Thomas, Bo Yu, and Johannes Reynisson. Serendipity in anticancer drug discovery. _World journal of clinical oncology_, 3(1):1, 2012.
* [38] Stephen R Heller, Alan McNaught, Igor Pletnev, Stephen Stein, and Dmitrii Tchekhovskoi. Inchi, the iupac international chemical identifier. _Journal of cheminformatics_, 7(1):1-34, 2015.
* [39] Wouter Heydrickx, Lewis Mervin, Tobias Morawietz, Noe Sturm, Lukas Friedrich, Adam Zalewski, Anastasia Pentina, Lina Humbeck, Martijn Oldenhof, Ritsuya Niwayama, et al. Melloddy: cross pharma federated learning at unprecedented scale unlocks benefits in qsar without compromising proprietary information. 2022.
* [40] Kexin Huang, Tianfan Fu, Wenhao Gao, Yue Zhao, Yusuf Roohani, Jure Leskovec, Coley Connor W, Cao Xiao, Jimeng Sun, and Marinka Zitnik. Therapeutics data commons: Machine learning datasets and tasks for drug discovery and development. _Advances in Neural Information Processing System_, 35, 2021.
* [41] Yan A Ivanenkov, Daniil Polykovskiy, Dmitry Bezrukov, Bogdan Zagribelnyy, Vladimir Aladinskiy, Petrina Kamya, Alex Aliper, Feng Ren, and Alex Zhavoronkov. Chemistry42: an ai-driven platform for molecular design and optimization. _Journal of Chemical Information and Modeling_, 63(3):695-701, 2023.
* [42] Nikita Janakarajan, Jannis Born, and Matteo Manica. A fully differentiable set autoencoder. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 3061-3071, 2022.
* [43] Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. _Advances in neural information processing systems_, 28, 2015.
* [44] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Zidek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. _Nature_, 596(7873):583-589, 2021.
* [45] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li, Benjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019 update: improved access to chemical data. _Nucleic acids research_, 47(D1):D1102-D1109, 2019.
* [46] Mario Krenn, Florian Hase, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik. Self-referencing embedded strings (selfies): A 100% robust molecular string representation. _Machine Learning: Science and Technology_, 1(4):045024, 2020.
* [47] Greg Landrum. Rdkit documentation. _Release_, 1(1-79):4, 2013.
* [48] Xinhao Li and Denis Fourches. Smiles pair encoding: a data-driven substructure tokenization algorithm for deep learning. _Journal of chemical information and modeling_, 61(4):1560-1569, 2021.

* [49] Jaechang Lim, Seongok Ryu, Jin Woo Kim, and Woo Youn Kim. Molecular generative model based on conditional variational autoencoder for de novo molecular design. _Journal of cheminformatics_, 10(1):1-9, 2018.
* [50] Tzyy-Shyang Lin, Connor W Coley, Hidenobu Mochigase, Haley K Beech, Wencong Wang, Zi Wang, Eliot Woods, Stephen L Craig, Jeremiah A Johnson, Julia A Kalow, et al. Bigsmiles: a structurally-based line notation for describing macromolecules. _ACS central science_, 5(9):1523-1531, 2019.
* [51] Zeming Lin, Halil Akin, Roshan Rao, Brian Hie, Zhongkai Zhu, Wenting Lu, Nikita Smetanin, Robert Verkuil, Ori Kabeli, Yaniv Shmueli, et al. Evolutionary-scale prediction of atomic-level protein structure with a language model. _Science_, 379(6637):1123-1130, 2023.
* [52] Jieyu Lu and Yingkai Zhang. Unified deep learning model for multitask reaction predictions with explanation. _Journal of Chemical Information and Modeling_, 62(6):1376-1387, 2022.
* [53] Matteo Manica, Jannis Born, Joris Cadow, Dimitrios Christofidellis, Ashish Dave, Dean Clarke, Yves Gaetan Nana Teukam, Giorgio Giannone, Samuel C Hoffman, Matthew Buchan, et al. Accelerating material design with the generative toolkit for scientific discovery. _npj Computational Materials_, 9(1):69, 2023.
* [54] Lukasz Maziarka, Tomasz Danel, Slawomir Mucha, Krzysztof Rataj, Jacek Tabor, and S Jastrzkebski. Molecule-augmented attention transformer. In _Workshop on Graph Representation Learning, Neural Information Processing Systems_, 2019.
* [55] Lukasz Maziarka, Dawid Majchrowski, Tomasz Danel, Piotr Gainski, Jacek Tabor, Igor Podolak, Pawel Morkisz, and Stanislaw Jastrzkebski. Relative molecule self-attention transformer. _arXiv preprint arXiv:2110.05841_, 2021.
* [56] Krzysztof Maziarz, Henry Jackson-Flux, Pashmina Cameron, Finton Sirockin, Nadine Schneider, Nikolaus Stiefl, Marwin Segler, and Marc Brockschmidt. Learning to extend molecular scaffolds with structural motif. In _The Tenth International Conference on Learning Representations, ICLR_, 2022.
* [57] Eyal Mazuz, Guy Shtar, Bracha Shapira, and Lior Rokach. Molecule generation using transformers and policy gradient reinforcement learning. _Scientific Reports_, 13(1):8799, 2023.
* [58] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. _arXiv preprint arXiv:1301.3781_, 2013.
* [59] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. _Nature_, 616(7956):259-265, 2023.
* [60] OpenAI. Chatgpt. https://chat.openai.com/chat, 2023. Accessed: August 8, 2023.
* [61] OpenAI. Gpt-4 technical report, 2023.
* [62] Nathaniel H Park, Matteo Manica, Jannis Born, James L Hedrick, Tim Erdmann, Dmitry Yu Zubarev, Nil Adell-Mill, and Pedro L Arrechea. Artificial intelligence driven design of catalysts and materials for ring opening polymerization using a domain-specific language. _Nature Communications_, 14(1):3686, 2023.
* [63] Pavel G Polishchuk, Timur I Madzhidov, and Alexandre Varnek. Estimation of the size of drug-like chemical space based on gdb-17 data. _J. Comput. Aid. Mol. Des._, 27(8):675-679, 2013.
* [64] Daniil Polykovskiy, Alexander Zhebrak, Benjamin Sanchez-Lengeling, Sergey Golovanov, Oktai Tatanov, Stanislav Belyaev, Rauf Kurbanov, Aleksey Artamonov, Vladimir Aladinskiy, Mark Veselov, et al. Molecular sets (moses): a benchmarking platform for molecular generation models. _Front. Pharmacol._, 11:1931, 2020.
* [65] Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo drug design. _Science advances_, 4(7):eaap7885, 2018.

* [66] Daniel Probst, Matteo Manica, Yves Gaetan Nana Teukam, Alessandro Castrogiovanni, Federico Paratore, and Teodoro Laino. Biocatatlysed synthesis planning using data-driven learning. _Nature communications_, 13(1):964, 2022.
* [67] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. 2018.
* [68] Bharath Ramsundar, Peter Eastman, Patrick Walters, Vijay Pande, Karl Leswing, and Zhenqin Wu. _Deep Learning for the Life Sciences_. O'Reilly Media, 2019. https://www.amazon.com/Deep-Learning-Life-Sciences-Microscopy/dp/1492039837.
* [69] Roshan Rao, Nicholas Bhattacharya, Neil Thomas, Yan Duan, Peter Chen, John Canny, Pieter Abbeel, and Yun Song. Evaluating protein transfer learning with tape. _Advances in neural information processing systems_, 32, 2019.
* [70] David Rogers and Mathew Hahn. Extended-connectivity fingerprints. _Journal of chemical information and modeling_, 50(5):742-754, 2010.
* [71] Jerret Ross, Brian Belgodere, Vijil Chenthamarakshan, Inkit Padhi, Youssef Mroueh, and Payel Das. Large-scale chemical language representations capture molecular structure and properties. _Nature Machine Intelligence_, 4(12):1256-1264, 2022.
* [72] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables zero-shot task generalization. In _ICLR 2022-Tenth International Conference on Learning Representations_, 2022.
* [73] Jack W Scannell, Alex Blanckley, Helen Boldon, and Brian Warrington. Diagnosing the decline in pharmaceutical r&d efficiency. _Nat. Rev. Drug Discov._, 11(3):191-200, 2012.
* [74] Oliver Schilter, Alain Vaucher, Philippe Schwaller, and Teodoro Laino. Designing catalysts with deep generative models and computational data. a case study for suzuki cross coupling reactions. _Digital Discovery_, 2(3):728-735, 2023.
* [75] Philippe Schwaller, Theophile Gaudin, David Lanyi, Costas Bekas, and Teodoro Laino. "found in translation": predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models. _Chemical science_, 9(28):6091-6098, 2018.
* [76] Philippe Schwaller, Teodoro Laino, Theophile Gaudin, Peter Bolgar, Christopher A Hunter, Costas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. _ACS central science_, 5(9):1572-1583, 2019.
* [77] Philippe Schwaller, Riccardo Petraglia, Valerio Zullo, Vishnu H Nair, Rico Andreas Haeuslemann, Riccardo Pisoni, Costas Bekas, Anna Iuliano, and Teodoro Laino. Predicting retrosynthetic pathways using transformer-based models and a hyper-graph exploration strategy. _Chemical science_, 11(12):3316-3325, 2020.
* [78] Philippe Schwaller, Benjamin Hoover, Jean-Louis Reymond, Hendrik Strobelt, and Teodoro Laino. Extraction of organic chemistry grammar from unsupervised learning of chemical reactions. _Science Advances_, 7(15):eabe4166, 2021.
* [79] Philippe Schwaller, Daniel Probst, Alain C Vaucher, Vishnu H Nair, David Kreutter, Teodoro Laino, and Jean-Louis Reymond. Mapping the space of chemical reactions using attention-based neural networks. _Nature machine intelligence_, 3(2):144-152, 2021.
* [80] Philippe Schwaller, Alain C Vaucher, Ruben Laplaza, Charlotte Bunne, Andreas Krause, Clemence Corninboeuf, and Teodoro Laino. Machine intelligence for chemical reaction space. _Wiley Interdisciplinary Reviews: Computational Molecular Science_, 12(5):e1604, 2022.
* [81] Marwin HS Segler, Thierry Kogej, Christian Tyrchan, and Mark P Waller. Generating focused molecule libraries for drug discovery with recurrent neural networks. _ACS central science_, 4(1):120-131, 2018.
* [82] Taffee T Tanimoto. Ibm internal report. _Nov_, 17:1957, 1957.

* [83] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. _arXiv preprint arXiv:2211.09085_, 2022.
* [84] Amol Thakkar, Alain C Vaucher, Andrea Byekwaso, Philippe Schwaller, Alessandra Toniato, and Teodoro Laino. Unbiasing retrosynthesis language models with disconnection prompts. _ACS Central Science_, 2023.
* [85] Alessandra Toniato, Philippe Schwaller, Antonio Cardinale, Joppe Geluykens, and Teodoro Laino. Unassisted noise reduction of chemical reaction datasets. _Nature Machine Intelligence_, 3(6):485-494, 2021.
* [86] Umit V Ucak, Islambek Ashyrmamatov, and Juyong Lee. Improving the quality of chemical language model outcomes with atom-in-smiles tokenization. _Journal of Cheminformatics_, 15(1):55, 2023.
* [87] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* [88] Alain C Vaucher, Federico Zipoli, Joppe Geluykens, Vishnu H Nair, Philippe Schwaller, and Teodoro Laino. Automated extraction of chemical synthesis actions from experimental procedures. _Nature communications_, 11(1):3601, 2020.
* [89] Alain C Vaucher, Philippe Schwaller, Joppe Geluykens, Vishnu H Nair, Anna Iuliano, and Teodoro Laino. Inferring experimental procedures from text-based representations of chemical reactions. _Nature communications_, 12(1):2573, 2021.
* [90] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj, and Thomas Wolf. Diffusers: State-of-the-art diffusion models, 10 2022. URL https://github.com/huggingface/diffusers.
* [91] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. _Advances in Neural Information Processing Systems_, 35:24824-24837, 2022.
* [92] David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. _J. Chem. Inf. Comp. Sci._, 28(1):31-36, 1988.
* [93] Andrew D White, Glen M Hocky, Heta A Gandhi, Mehrad Ansari, Sam Cox, Geemi P Wellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, et al. Assessment of chemistry knowledge in large language models that generate code. _Digital Discovery_, 2(2):368-376, 2023.
* [94] Scott A Wildman and Gordon M Crippen. Prediction of physicochemical parameters by atomic contributions. _Journal of chemical information and computer sciences_, 39(5):868-873, 1999.
* [95] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations_, pages 38-45, 2020.
* [96] Olivier J Wouters, Martin McKee, and Jeroen Luyten. Estimated research and development investment needed to bring a new medicine to market, 2009-2018. _Jama_, 323(9):844-853, 2020.
* [97] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S Pappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning. _Chemical science_, 9(2):513-530, 2018.
* [98] Zheni Zeng, Yuan Yao, Zhiyuan Liu, and Maosong Sun. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. _Nature communications_, 13(1):862, 2022.

* [99] Alex Zhavoronkov, Yan A Ivanenkov, Alex Aliper, Mark S Veselov, Vladimir A Aladinskiy, Anastasiya V Aladinskaya, Victor A Terentiev, Daniil A Polykovskiy, Maksim D Kuznetsov, Arip Asadulaev, et al. Deep learning enables rapid identification of potent ddr1 kinase inhibitors. _Nat. Biotechnol._, 37(9):1038-1040, 2019.
* [100] Zhaocheng Zhu, Chence Shi, Zuobai Zhang, Shengchao Liu, Minghao Xu, Xinyu Yuan, Yangtian Zhang, Junkun Chen, Huiyu Cai, Jiarui Lu, et al. Torchdrug: A powerful and flexible machine learning platform for drug discovery. _Preprint at https://arxiv.org/abs/2202.08320_, 2022.