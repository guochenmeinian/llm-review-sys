# What Representational Similarity Measures Imply about Decodable Information

 Sarah E. Harvey

Center for Computational Neuroscience

Flatiron Institute, New York, NY

sharvey@flatironinstitute.org &David Lipshutz

Center for Computational Neuroscience

Flatiron Institute, New York, NY

dlipshutz@flatironinstitute.org &Alex H. Williams

Center for Neural Science, Center for Computational Neuroscience

New York University, Flatiron Institute, New York, NY

alex.h.williams@nyu.edu

###### Abstract

Neural responses encode information that is useful for a variety of downstream tasks. A common approach to understand these systems is to build regression models or "decoders" that reconstruct features of the stimulus from neural responses. Popular neural network similarity measures like centered kernel alignment (CKA), canonical correlation analysis (CCA), and Procrustes shape distance, do not explicitly leverage this perspective and instead highlight geometric invariances to orthogonal or affine transformations when comparing representations. Here, we show that many of these measures can, in fact, be equivalently motivated from a decoding perspective. Specifically, measures like CKA and CCA quantify the average alignment between optimal linear readouts across a distribution of decoding tasks. We also show that the Procrustes shape distance upper bounds the distance between optimal linear readouts and that the converse holds for representations with low participation ratio. Overall, our work demonstrates a tight link between the geometry of neural representations and the ability to linearly decode information. This perspective suggests new ways of measuring similarity between neural systems and also provides novel, unifying interpretations of existing measures.

## 1 Introduction

The computational neuroscience and machine learning communities have developed a multitude of methods to quantify similarity in population-level activity patterns across neural systems. Indeed, a recent review by Klabunde et al. (2020) catalogues over thirty approaches to quantifying representational similarity. Many of these measures quantify similarity in the _shape_ or _representational geometry_ of point clouds. For example, recent papers (e.g. (Srivastava et al., 2017; Li et al., 2018)) leverage the Procrustes distance and other concepts from _shape theory_--an established body of work that formalizes the notion of shape and ways to measure distance between shapes (Krause et al., 2018; Li et al., 2018). Other measures of representational similarity are not quite this explicit but still emphasize desired _geometric invariance_ properties. For example, work by Kornblith et al. (2019) popularized centered kernel alignment (CKA) by emphasizing its invariance to isotropic scaling, translation, and orthogonal transformations. These are precisely thesame invariances specified by classical shape theory [18; 10]. Earlier work by Raghu et al. [29] argued for a more flexible invariance to affine transformations, and they used canonical correlations analysis (CCA) for this purpose. Contemporary work in neuroscience is replete with similar geometric reasoning and quantitative frameworks [25].

Here we ask: What does the similarity between neural representations, potentially in terms of shape or geometry, imply about the similarity between _functions_ performed by those neural systems? Potentially, not very much. Maheswaranathan et al. [27] showed that recurrent neural networks with different architectures performed the same task with similar dynamical algorithms, but with different representational geometry. More recently, Lampinen et al. [26] showed that representational geometry can be biased by other nuisance variables, such as the order in which multiple tasks are learned. Similar limitations of representational geometry measures are discussed in [11; 8].

On the other hand, several basic observations suggest that geometry and function ought to be related. Consider, for example, the common practice of using linear models to decode task-relevant variables from neural population activity [1; 24]. The premise behind these analyses is that anything linearly decodable from system \(A\) is readily accessible to layers or brain regions immediately downstream of \(A\). Therefore, information that is linearly decodable from \(A\) may relate to functional role played by \(A\) within the overall system [4]. Notably, the invariances of typical representational similarity measures--translations, isotropic scalings, and orthogonal transformations--closely coincide with the set of transformations that leave decoding accuracy unchanged. For example, translations and rotations of neural population activity will not impact the accuracy of a linear decoder with an intercept term and an \(L_{2}\) penalty on the weights. Thus, if we accept the premise that decoding accuracy is a proxy--even, perhaps, an imperfect proxy--for neural system function, then representational geometry may be a reasonable framework to capture something about this function [23].

We provide a unifying theoretical framework that connects several existing methods of measuring representational similarity in terms of linear decoding statistics. Specifically, we show that popular methods such as centered kernel alignment (CKA) [22] and similarity based on canonical correlations analysis (CCA) [29] can be interpreted as alignment scores between optimal linear decoders with particular weight regularizations. Additionally, we study how the _shape_ of neural representations is related to decoding by deriving bounds that relate the average decoding distance and the Procrustes distance. We find that the Procrustes distance is a more strict notion of geometric dissimilarity than the expected distance between optimal decoder readouts, in that a small value of the Procrustes distance implies a small expected distance between optimal decoder readouts but the converse is not necessarily true. This formalizes recent observations by Cloos et al. [5], who found that high Procrustes similarity implied high CKA similarity in practical settings.

## 2 Theoretical Framework for Linear Decoding

We use \(\bm{X}\in\mathbb{R}^{M\times N_{X}}\) and \(\bm{Y}\in\mathbb{R}^{M\times N_{Y}}\) to denote matrices holding responses to \(M\) stimulus conditions measured across neural populations consisting of \(N_{X}\) and \(N_{Y}\) neurons, respectively. Throughout this paper we will assume that the neural responses have been preprocessed so that the columns of \(\bm{X}\) and \(\bm{Y}\) have zero mean.

### Linear decoding from neural population responses

We first consider the problem of predicting (or "decoding") a target vector \(\bm{z}\in\mathbb{R}^{M}\) from neural population responses by a linear function \(\bm{X}\mapsto\bm{X}\bm{w}\) where \(\bm{w}\in\mathbb{R}^{N_{X}}\). One can view this as a simplified neural circuit model where each element of \(\hat{\bm{z}}=\bm{X}\bm{w}+\bm{\epsilon}\) is the firing rate readout unit the \(M\) conditions. Here, we interpret \(\bm{w}\) as a vector of synaptic weights and the random vector \(\bm{\epsilon}\) represents potential noise corruptions; for example, \(\bm{\epsilon}_{i}\sim\mathcal{N}(0,\sigma^{2})\) where \(\sigma^{2}>0\) specifies the scale of noise. This neural circuit interpretation of linear decoding is fairly standard within the literature [13; 17].

Within this setting, we formalize the problem of decoding \(\bm{z}\) from the population activity \(\bm{X}\) through the following class of optimization problems:

\[\underset{\bm{w}}{\text{maximize}}\quad\frac{1}{M}\bm{z}^{\top}\bm{X}\bm{w}- \frac{1}{2}\bm{w}^{\top}\bm{G}(\bm{X})\bm{w}\] (1)where \(\bm{G}(\cdot)\) is a function mapping \(\mathbb{R}^{M\times N}\) onto symmetric positive definite \(N\times N\) matrices. The objective is concave, and equating the gradient to zero yields a closed form solution:

\[\bm{w}^{*}=\tfrac{1}{M}\bm{G}(\bm{X})^{-1}\bm{X}^{\top}\bm{z}\] (2)

To appreciate the relevance of eq.1, note that if \(\bm{G}(\bm{X})=\bm{C}_{X}+\lambda\bm{I}\), where \(\bm{C}_{X}:=\tfrac{1}{M}\bm{X}^{\top}\bm{X}\) is the empirical covariance of \(\bm{X}\), then the solution \(\bm{w}^{*}\) coincides with the optimum of the familiar ridge regression problem:

\[\underset{\bm{w}}{\text{minimize}} \tfrac{1}{M}\|\bm{z}-\bm{X}\bm{w}\|_{2}^{2}+\lambda\|\bm{w}\|_{2} ^{2}\] (3)

with solution \(\bm{w}^{*}=\tfrac{1}{M}(\bm{C}_{X}+\lambda\bm{I})^{-1}\bm{X}^{\top}\bm{z}\), where \(\lambda>0\) specifies the strength of regularization on the coefficients. More generally, we can interpret eq.1 as maximizing the inner product between the target, \(\bm{z}\), and the linear readout, \(\bm{X}\bm{w}\), plus a penalty term on the scale of \(\bm{w}\) as quantified by the function \(\bm{w}\mapsto\sqrt{\bm{w}^{\top}\bm{G}(\bm{X})\bm{w}}\), which is a norm on \(\mathbb{R}^{N}\).

Equation1 is a useful generalization of eq.3 because there are situations where the scale of the decoded signal matters. For instance, when the readout unit is corrupted with noise, \(\hat{\bm{z}}=\bm{X}\bm{w}+\bm{\epsilon}\) where \(\bm{\epsilon}_{i}\sim\mathcal{N}(0,\sigma^{2})\), we can interpret eq.1 as maximizing the signal-to-noise ratio subject to a cost on the magnitude of the weights \(\bm{w}\).

In most cases, we will consider choices of \(\bm{G}(\cdot)\) that can be written as:

\[\bm{G}(\bm{X})=a\bm{C}_{X}+b\bm{I}\] (4)

for some \(a,b\geq 0\). Under this choice, the penalty on \(\bm{w}\) equals the sum of two terms:

\[\bm{w}^{\top}\bm{G}(\bm{X})\bm{w}=\frac{a}{M}\|\bm{X}\bm{w}\|_{2}^{2}+b\|\bm{w }\|_{2}^{2}.\] (5)

The first term, scaled by \(a\), penalizes the activation level of the readout unit, which can be viewed as an energetic constraint. The second term, scaled by \(b\), penalizes the strength of the synaptic weights, which can be viewed as a resource constraint for building and maintaining strong synapses. We note again that ridge regression is achieved as a special case when \(a=1\) and \(b=\lambda\).

### Quantifying differences between networks through the lens of decoding

We now turn our attention to the the problem of quantifying representational similarity between \(\bm{X}\in\mathbb{R}^{M\times N_{X}}\) and \(\bm{Y}\in\mathbb{R}^{M\times N_{Y}}\). As before, we specify a target vector \(\bm{z}\in\mathbb{R}^{M}\) and optimize \(\bm{w}\) as

Figure 1: Schematic of the proposed framework for comparing representations \(\bm{X}\) and \(\bm{Y}\) (each dot represents mean neural responses to one of \(M\) conditions) in terms of a decoding target \(\bm{z}\). First, optimal linear decoding weights \(\bm{w}^{*}\) and \(\bm{v}^{*}\) are computed. Then the similarity between the two systems is measured in terms of the _decoding similarity_: \(\langle\bm{X}\bm{w}^{*},\bm{Y}\bm{v}^{*}\rangle\).

specified in eq. (1). We optimize \(\bm{v}\) accordingly:

\[\underset{\bm{v}}{\text{maximize}}\quad\tfrac{1}{M}\bm{z}^{\top}\bm{Y}\bm{v}- \tfrac{1}{2}\bm{v}^{\top}\bm{G}(\bm{Y})\bm{v}\] (6)

yielding the solution \(\bm{v}^{*}=\tfrac{1}{M}\bm{G}(\bm{Y})^{-1}\bm{Y}^{\top}\bm{z}\). We are interested in quantifying similarity between \(\bm{X}\) and \(\bm{Y}\) by comparing the behavior of these optimal decoders. A straightforward way to quantify the alignment between the decoded signals is to compute their inner product, which we call the _decoding similarity_ (fig. 1):

\[\langle\bm{X}\bm{w}^{*},\bm{Y}\bm{v}^{*}\rangle=\bm{w}^{*\top}\bm{X}^{\top}\bm{ Y}\bm{v}^{*}=\bm{z}^{\top}\bm{K}_{X}\bm{K}_{Y}\bm{z}\] (7)

where we have leveraged eq. (2) and the analogous closed form expression for \(\bm{v}^{*}\) while introducing the following normalized kernel similarity matrices:

\[\bm{K}_{X}:=\frac{1}{M}\bm{X}\bm{G}(\bm{X})^{-1}\bm{X}^{\top}\quad\text{and} \quad\bm{K}_{Y}:=\frac{1}{M}\bm{Y}\bm{G}(\bm{Y})^{-1}\bm{Y}^{\top}.\] (8)

Note that we have suppressed the dependence of \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) on the function \(\bm{G}(\cdot)\) to reduce notational clutter.

Equation (7) is a reasonable quantification of similarity between \(\bm{X}\) and \(\bm{Y}\) with respect to a fixed decoding task specified by \(\bm{z}\in\mathbb{R}^{M}\), and it may be a fruitful approach for hypothesis-driven comparisons of neural systems. We comment further on this possibility in our discussion. On the other hand, many neural representations support a variety of downstream behavioral tasks. For example, features extracted in early stages of visual processing (edges and textures) can be used to support many visual tasks such as object classification, segmentation, image compression, et cetera. How can eq. (7) be adapted to quantify the similarity between \(\bm{X}\) and \(\bm{Y}\) across more than one pre-specified decoding task? We explore three simple ideas.

**Option 1 -- best case alignment.** An optimistic measure of representational similarity between networks is to find the decoding task \(\bm{z}\in\mathbb{R}^{M}\) that results in a maximal decoder alignment. Formally,

\[\max_{\|\bm{z}\|_{2}=1}\langle\bm{X}\bm{w}^{*},\bm{Y}\bm{v}^{*}\rangle=\max_{ \|\bm{z}\|_{2}=1}\bm{z}^{\top}\bm{K}_{X}\bm{K}_{Y}\bm{z}\] (9)

where the constraint that \(\|\bm{z}\|_{2}=1\) is needed to keep the maximal value finite.

**Option 2 -- worst case alignment.** An alternative is to find the task that minimizes alignment:

\[\min_{\|\bm{z}\|_{2}=1}\langle\bm{X}\bm{w}^{*},\bm{Y}\bm{v}^{*}\rangle=\min_{ \|\bm{z}\|_{2}=1}\bm{z}^{\top}\bm{K}_{X}\bm{K}_{Y}\bm{z}\] (10)

which is obviously the pessimistic counterpart to option 1 above.

**Option 3 -- average case alignment.** Instead of maximizing or minimizing over \(\bm{z}\), we can approach the problem by averaging over a distribution of decoding tasks. Formally, we can quantify alignment in terms of the _average decoding similarity_:

\[\mathbb{E}\langle\bm{X}\bm{w}^{*},\bm{Y}\bm{v}^{*}\rangle\] (11)

where the expectation is taken with respect to \(\bm{z}\sim P_{\bm{z}}\).

In the next subsection, we show that the third option is closely related to several existing neural representational similarity measures. However, all three quantities are potentially of interest and they are easy to compute as we document below in propositions 1 and 2.

**Proposition 1**.: _The best case and worst case decoding alignment scores, eqs. (9) and (10), are respectively given by the largest and smallest eigenvalues of:_

\[\frac{1}{2}\left(\bm{K}_{X}\bm{K}_{Y}+\bm{K}_{Y}\bm{K}_{X}\right)\] (12)

**Proposition 2**.: _The average decoding similarity, eq. (11), is given by:_

\[\mathbb{E}\langle\bm{X}\bm{w}^{*},\bm{Y}\bm{v}^{*}\rangle=\operatorname{Tr} \left(\bm{K}_{X}\bm{K}_{z}\bm{K}_{Y}\right),\] (13)

_where \(\bm{K}_{z}:=\mathbb{E}[\bm{z}\bm{z}^{\top}]\) represents a kernel matrix for the targets._Proof of proposition 1.: Let \(\bm{A}\in\mathbb{R}^{M\times M}\) be any matrix, potentially non-symmetric. We can express \(\bm{A}\) as a sum of a symmetric and skew symmetric matrix, \(\bm{A}=\bm{B}+\bm{C}\), where:

\[\bm{B}=\frac{1}{2}\left(\bm{A}+\bm{A}^{\top}\right)\qquad\text{and}\qquad\bm{C }=\frac{1}{2}\left(\bm{A}-\bm{A}^{\top}\right)\] (14)

For any \(\bm{z}\in\mathbb{R}^{M}\), we have \(\bm{z}^{\top}\bm{A}\bm{z}=\bm{z}^{\top}\bm{B}\bm{z}+\bm{z}^{\top}\bm{C}\bm{z}\). It is easy to verify that \(\bm{z}^{\top}\bm{C}\bm{z}\) equals zero. Thus, \(\bm{z}^{\top}\bm{A}\bm{z}=\bm{z}^{\top}\bm{B}\bm{z}\), and maximizing over \(\bm{z}\) subject to \(\|\bm{z}\|_{2}\) yields the largest eigenvalue of \(\bm{B}\) (since it is symmetric). Likewise, minimizing yields the smallest eigenvalue of \(\bm{B}\). Substituting \(\bm{A}=\bm{K}_{X}\bm{K}_{Y}\) into this argument proves the proposition. 

Proof of proposition 2.: Using eq.7, we have:

\[\mathbb{E}\langle\bm{X}\bm{w}^{*},\bm{Y}\bm{v}^{*}\rangle=\mathbb{E}[\bm{z}^{ \top}\bm{K}_{X}\bm{K}_{Y}\bm{z}].\]

Using the cyclic property of the trace operator to manipulate this further, we get:

\[\bm{z}^{\top}\bm{K}_{X}\bm{K}_{Y}\bm{z}=\operatorname{Tr}(\bm{K}_{X}\bm{z} \bm{z}^{\top}\bm{K}_{Y}).\]

Taking the expectation with respect to \(\bm{z}\) and using linearity of trace yields eq.13. 

## 3 Interpretations of CKA and related measures

Intuitively, proposition2 says that the expected inner product between optimal linear readouts is equal to a weighted matrix inner product between kernel matrices \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) (with respect to a positive semi-definite weighting matrix \(\bm{K}_{z}\), which of course depends on the distribution of decoding targets \(\bm{z}\)). Such kernel matrices and inner products are important for several methods of quantifying representational similarity. We can therefore leverage this result to provide new interpretations of several distance measures. The most basic idea is to consider a distribution over \(\bm{z}\) which satisfies \(\bm{K}_{z}=\bm{I}\). This leads us to measure distances using standard Euclidean geometry, which we state as the following corollary to proposition2. We explore a relaxation of this assumption and the limit as the number of input samples \(M\to\infty\) in appendixC.

**Corollary 1**.: _For any distribution over decoding targets satisfying \(\bm{K}_{z}=\bm{I}\), the Frobenius inner product between normalized kernel matrices \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) is equal to the average decoding similarity:_

\[\mathbb{E}\langle\bm{X}\bm{w}^{*},\bm{Y}\bm{v}^{*}\rangle=\operatorname{Tr}[ \bm{K}_{X}\bm{K}_{Y}]\] (15)

_Further, when \(\bm{K}_{z}=\bm{I}\), the (squared) average decoding distance is equal to the (squared) Euclidean distance between \(\bm{K}_{X}\) and \(\bm{K}_{Y}\):_

\[\mathbb{E}\|\bm{X}\bm{w}^{*}-\bm{Y}\bm{v}^{*}\|_{2}^{2}=\|\bm{K}_{X}-\bm{K}_{Y }\|_{F}^{2}.\] (16)

This corollary can be used to unify several neural representational similarity measures, each corresponding to a different choice of the penalty function \(\bm{G}(\bm{X})\), Table1. Further, it yields an interpretation of each measure in terms of the expected overlap or difference in decoding readouts between networks.

\begin{table}
\begin{tabular}{l c c} \hline \hline Similarity measure & \(a\) & \(b\) \\ \hline Linear CKA & 0 & \(b\) \\ GULP & 1 & \(\lambda\) \\ CCA & 1 & 0 \\ ENSD & 0 & \(\frac{1}{M}\operatorname{Tr}[\bm{C}_{X}^{2}]\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Existing similarity measures are equivalent to the decoding score in eq.7 or the decoding distance in eq.16 for different choices of \(a\) and \(b\) in eq.5.

Linear CKAFirst, for \(b>0\), the following choice of \(\bm{G}(\bm{X})\) yields the linear CKA score [6, 22]:

\[\bm{G}(\bm{X})=b\bm{I}\quad\Rightarrow\quad\frac{\mathbb{E}\langle\bm{X}\bm{w}^ {*},\bm{Y}\bm{v}^{*}\rangle}{\sqrt{\mathbb{E}\langle\bm{X}\bm{w}^{*},\bm{X}\bm{ w}^{*}\rangle\mathbb{E}\langle\bm{Y}\bm{v}^{*},\bm{Y}\bm{v}^{*}\rangle}}=\frac{ \operatorname{Tr}[\bm{K}_{X}\bm{K}_{Y}]}{\|\bm{K}_{X}\|_{F}\|\bm{K}_{Y}\|_{F}}= \text{CKA}(\bm{X},\bm{Y})\]

where we have normalized the expected inner product to obtain a similarity score ranging between 0 and 1. Note that the centering step of CKA is taken care of by our assumption, stated at the beginning of section 2, that the columns of \(\bm{X}\) and \(\bm{Y}\) are preprocessed to have mean zero. Thus, linear CKA be interpreted as a normalized average decoding similarity with quadratic weight regularization.

GULP distanceThe CKA score is closely related to Euclidean distance, so it is not surprising that we can extend our analysis to other methods that utilize this distance. For example, the sample GULP distance, proposed by [3], is the equal to the average decoding distance with a regularization parameter \(\lambda>0\). In particular,

\[\bm{G}(\bm{X})=\bm{C}_{X}+\lambda\bm{I}\quad\Rightarrow\quad\mathbb{E}\|\bm{ X}\bm{w}^{*}-\bm{Y}\bm{v}^{*}\|_{2}^{2}=\|\bm{K}_{X}-\bm{K}_{Y}\|_{F}^{2}= \text{GULP}_{\lambda}^{2}(\bm{X},\bm{Y})\]

yields the plug-in estimate of the squared GULP distance [3, section 3]. We therefore see that the Euclidean distance between normalized kernel matrices can be interpreted as equal to the average decoding distance, or also as an approximation of the population formulation of the GULP distance established in [3].

Canonical correlation analysis (CCA)CCA is another method that has been used to compare neural representations that fits nicely into our framework [29, 36]. When the two networks have the same dimension, i.e. \(N=N_{X}=N_{Y}\), the output of CCA is a sequence of \(N\) canonical correlation coefficients \(1\geq\rho_{1}\geq\cdots\geq\rho_{N}\geq 0\). The average squared canonical coefficients is sometimes used as a measure of similarity,1 and Kornblith et al. [22] showed this to be related to the linear CKA score on whitened representations. In particular, assuming the covariance matrices \(\bm{C}_{X}\) and \(\bm{C}_{Y}\) are invertible, we have:

Footnote 1: In the statistics literature, this summary statistic is sometimes called _Yanai’s generalized generalized coefficient of determination_[30, 38].

\[\bm{G}(\bm{X})=\bm{C}_{X}\quad\Rightarrow\quad\frac{\mathbb{E}\langle\bm{X} \bm{w}^{*},\bm{Y}\bm{v}^{*}\rangle}{\sqrt{\mathbb{E}\langle\bm{X}\bm{w}^{*}, \bm{X}\bm{w}^{*}\rangle\mathbb{E}\langle\bm{Y}\bm{v}^{*},\bm{Y}\bm{v}^{*} \rangle}}=\frac{1}{N}\sum_{i=1}^{N}\rho_{i}^{2}\] (17)

In practice, it is often useful to incorporate regularization into CCA because the covariance matrices \(\bm{C}_{X}\) and \(\bm{C}_{Y}\) may be ill-conditioned. As mentioned above, the regularization used in GULP distance corresponds to choosing \(\bm{G}(\bm{X})=\bm{C}_{X}+\lambda\bm{I}\). A similar approach proposed in [36] is to use \(\bm{G}(\bm{X})=(1-\alpha)\bm{C}_{X}+\alpha\bm{I}\) for a hyperparameter \(0\leq\alpha\leq 1\). This effectively allows for a continuous interpolation between a CCA-based similarity score and linear CKA.

Effective Number of Shared Dimensions (ENSD)Finally, we note a connection to recent work by Giaffar et al. [15], who studied the following quantity, which they call the ENSD:

\[\text{ENSD}(\bm{X},\bm{Y})=\frac{\operatorname{Tr}[\bm{X}\bm{X}^{\top}] \operatorname{Tr}[\bm{Y}\bm{Y}^{\top}]\operatorname{Tr}[\bm{X}\bm{X}^{\top} \bm{Y}\bm{Y}^{\top}]}{\operatorname{Tr}[\bm{X}\bm{X}^{\top}\bm{X}\bm{X}^{\top}] \operatorname{Tr}[\bm{Y}\bm{Y}^{\top}\bm{Y}\bm{Y}^{\top}]}\] (18)

This is simply a rescaling of the inner product, \(\operatorname{Tr}[\bm{X}\bm{X}^{\top}\bm{Y}\bm{Y}^{\top}]\) and therefore easily fits within our framework using the following choice for \(\bm{G}(\bm{X})\):

\[\bm{G}(\bm{X})=\frac{\operatorname{Tr}[\bm{C}_{X}^{2}]}{\operatorname{Tr}[\bm {C}_{X}]}\bm{I}\quad\Rightarrow\quad\mathbb{E}\langle\bm{X}\bm{w}^{*},\bm{Y} \bm{v}^{*}\rangle=\text{ENSD}(\bm{X},\bm{Y})\] (19)

One of the most interesting features of ENSD is it's connection to the **participation ratio**, \(\mathcal{R}\):

\[\mathcal{R}(\bm{C}_{X})=\frac{\left(\operatorname{Tr}[\bm{C}_{X}]\right)^{2}}{ \operatorname{Tr}[\bm{C}_{X}^{2}]}=\text{ENSD}(\bm{X},\bm{X})\] (20)

which is used within physics and computational neuroscience (e.g. [14, 7, 31]) as a continuous analogue to the rank of a matrix, and is a measure of the effective dimensionality of the data matrix \(\bm{X}\). In particular, one can show that \(1\leq\mathcal{R}(\bm{C}_{X})\leq\text{rank}(\bm{C}_{X})\), and that these two inequalities respectively saturate when \(\bm{C}_{X}\) has only one nonzero eigenvalue and when all nonzero eigenvalues are equal. It is also interesting to note that the _inverse_ participation ratio can be captured by average decoding similarity under a very simple choice for \(\bm{G}(\bm{X})\):

\[\bm{G}(\bm{X})=\operatorname{Tr}[\bm{C}_{X}]\bm{I}\quad\Rightarrow\quad \mathbb{E}\langle\bm{X}\bm{w}^{*},\bm{X}\bm{w}^{*}\rangle=\mathbb{E}\|\bm{X} \bm{w}^{*}\|^{2}=\frac{1}{\mathcal{R}(\bm{C}_{X})}\] (21)

We reiterate that all of the results enumerated above depend on choosing a distribution over decoding targets which satisfies \(\bm{K}_{z}=\bm{I}\). This enforces the decoding targets \(z_{1},\ldots,z_{M}\) to be uncorrelated random variables with unit variance. The unit variance constraint is analogous to the constraint that \(\|\bm{z}\|_{2}=1\) that we imposed when maximizing or minimizing over \(\bm{z}\) in eqs. (9) and (10). Indeed, scaling the variance of the \(\bm{z}_{i}\)'s simply scales the magnitude of \(\mathbb{E}\langle\bm{X}\bm{w}^{*},\bm{Y}\bm{w}^{*}\rangle\). Thus, setting the variance of each sample to one (or some other constant) is reasonable. The constraint that \(\mathbb{E}[z_{i}z_{j}]=0\) for all \(i\neq j\) may be relaxed, as we discuss in appendix C.

## 4 Relating the Shape of Neural Representations to Decoding

In corollary 1 we saw that the Euclidean distance and Frobenius inner product between \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) enjoys a nice interpretation in terms of (average) decoder similarity. This does not provide a completely satisfactory answer of our original question: What does the _geometry_ of a neural representation imply about its function (i.e. decoder performance)? As discussed in section 1, comparing neural representations through linear kernel matrices, \(\bm{K}_{X}\) and \(\bm{K}_{Y}\), is motivated from geometric principles--namely, their invariance to orthogonal transformations. Indeed, for any orthogonal matrix \(\bm{R}\), one can verify that:

\[\bm{G}(\bm{X}\bm{R})^{-1}=\bm{R}^{\top}\bm{G}(\bm{X})^{-1}\bm{R}\] (22)

whenever \(\bm{G}(\bm{X})\) is parameterized as in eq. (4). Therefore, then the transformation \(\bm{X}\mapsto\bm{X}\bm{R}\) leaves the whitened linear kernel matrix unchanged. That is, if \(\bm{X}^{\prime}=\bm{X}\bm{R}\), we have:

\[\bm{K}_{X}=\tfrac{1}{M}\bm{X}\bm{G}(\bm{X})^{-1}\bm{X}^{\top}=\tfrac{1}{M}\bm {X}\bm{R}\bm{G}(\bm{X}\bm{R})^{-1}\bm{R}^{\top}\bm{X}^{\top}=\bm{K}_{X^{\prime }}.\] (23)

The **Procrustes shape distance**, \(\mathcal{P}(\cdot,\cdot)\), is another popular method which is invariant to orthogonal transformations [36; 9]. This approach connects to a broader and decades-old literature on _shape analysis_[10; 18] whose applications include anatomical comparisons across biological species [12], and analysis of planar curves such as handwriting data [33]. If the two neural responses are mean-centered and have the same dimensionality, then the Procrustes distance is simply the minimal Euclidean distance obtained by rotating and reflecting one set of responses onto the other. That is, when \(\bm{X}\in\mathbb{R}^{M\times N_{X}}\) and \(\bm{Y}\in\mathbb{R}^{M\times N_{Y}}\) and we have \(N=N_{X}=N_{Y}\), the Procrustes distance is:

\[\mathcal{P}(\bm{X},\bm{Y})=\min_{\bm{R}\in\mathcal{O}(N)}\|\bm{X}-\bm{Y}\bm{R }\|_{F}\] (24)

When \(N_{X}\neq N_{Y}\), a straightforward generalization of the Procrustes distance is to zero pad column-wise so that the matrix dimensions match. Geometrically, this can be interpreted as embedding the lower-dimensional point cloud into the higher-dimensional space and optimizing \(\bm{R}\) within this space to align the point clouds.

The fact that Procrustes distance is defined in terms of an optimal alignment transformation is appealing both intuitively and because it suggests avenues to generalize the method, such as using permutations (instead of orthogonal transformations) to align representations [19]. Additionally, some empirical results have highlighted cases where Procrustes performs "better" than linear CKA [9; 5]. Rigorously defining and benchmarking what it means to be "better" in this context remains an open discussion [21], but it is nonetheless of interest to understand what Procrustes distance and associated definitions of _shape_ imply about neural decoding.

In appendix A.1 we prove the following result, which states that the Procrustes distance between appropriately normalized representations constrains the average decoding distance, or equivalently, that the average decoding distance bounds the Procrustes distance from above and below.

**Proposition 3**.: _Defining normalization constants \(\alpha=\operatorname{Tr}\boldsymbol{K}_{X}\) and \(\beta=\operatorname{Tr}\boldsymbol{K}_{Y}\), we have an upper and lower bound on the Procrustes distance:_

\[(\alpha+\beta)-\sqrt{(\alpha+\beta)^{2}-\mathcal{R}_{\Delta}\mathbb{E}\| \boldsymbol{X}\boldsymbol{w}^{*}-\boldsymbol{Y}\boldsymbol{v}^{*}\|_{2}^{2}} \leq\mathcal{P}(\tilde{\boldsymbol{X}},\tilde{\boldsymbol{Y}})^{2}\leq\sqrt{ \mathcal{R}_{\Delta}\mathbb{E}\|\boldsymbol{X}\boldsymbol{w}^{*}-\boldsymbol {Y}\boldsymbol{v}^{*}\|_{2}^{2}}\] (25)

_where \(\mathcal{R}_{\Delta}=\mathcal{R}(\boldsymbol{K}_{X}-\boldsymbol{K}_{Y})\) is the participation ratio of the matrix difference \(\boldsymbol{K}_{X}-\boldsymbol{K}_{Y}\). Equivalently, we may rewrite eq. (25) to find_

\[\frac{1}{\mathcal{R}_{\Delta}}\mathcal{P}(\tilde{\boldsymbol{X}},\tilde{ \boldsymbol{Y}})^{4}\leq\mathbb{E}\|\boldsymbol{X}\boldsymbol{w}^{*}- \boldsymbol{Y}\boldsymbol{v}^{*}\|_{2}^{2}\leq\frac{2(\alpha+\beta)\mathcal{P }(\tilde{\boldsymbol{X}},\tilde{\boldsymbol{Y}})^{2}-\mathcal{P}(\tilde{ \boldsymbol{X}},\tilde{\boldsymbol{Y}})^{4}}{\mathcal{R}_{\Delta}},\] (26)

_where we have defined normalized transformations \(\tilde{\boldsymbol{X}}\) and \(\tilde{\boldsymbol{Y}}\) in terms of the transformation \(\boldsymbol{G}(\cdot)\):_

\[\tilde{\boldsymbol{X}}:=\tfrac{1}{\sqrt{M}}\boldsymbol{X}\boldsymbol{G}( \boldsymbol{X})^{-1/2}\quad\text{and}\quad\tilde{\boldsymbol{Y}}:=\tfrac{1}{ \sqrt{M}}\boldsymbol{Y}\boldsymbol{G}(\boldsymbol{Y})^{-1/2}.\]

_Note that \(\tilde{\boldsymbol{X}}\tilde{\boldsymbol{X}}^{\top}=\boldsymbol{K}_{X}\) and \(\tilde{\boldsymbol{Y}}\tilde{\boldsymbol{Y}}^{\top}=\boldsymbol{K}_{Y}\)._

We derive these bounds by leveraging the equivalence between the Procrustes distance between normalized representations \(\mathcal{P}(\tilde{\boldsymbol{X}},\tilde{\boldsymbol{Y}})\) and a distance metric on positive semi-definite kernel matrices, the _Bures distance_[16].

We recall that the choice of function \(\boldsymbol{G}(\cdot)\) in the decoding problem eq. (1) (which determines optimal readout weights \(\boldsymbol{w}^{*}\) and \(\boldsymbol{v}^{*}\)) will set the normalization transformation of the representations \(\tilde{\boldsymbol{X}}\) and \(\tilde{\boldsymbol{Y}}\). When \(\boldsymbol{G}(\cdot)\) is chosen such that \(\operatorname{Tr}\boldsymbol{K}_{X}=\operatorname{Tr}\boldsymbol{K}_{Y}=1\),2 we can plot these bounds with \(\alpha=\beta=1\) for various values of participation ratio \(\mathcal{R}_{\Delta}\) (fig. 2).

Footnote 2: For example \(\boldsymbol{G}(\boldsymbol{X})=N_{X}\boldsymbol{C}_{X}\) or the CKA choice from above with \(b=\operatorname{Tr}[\boldsymbol{C}_{X}]\). This is not a requirement for these bounds to hold, but simply a choice of normalization for the sake of plotting.

Both upper and lower bounds are saturated (appendix A.1), and reveal an allowed region of both Procrustes and average decoding distance as a function of the participation ratio \(\mathcal{R}_{\Delta}\). Under the \(\alpha=\beta=1\) normalization, the Procrustes distance may range between 0 and \(\sqrt{2}\), and the average decoding distance may take values between \(0\) and \(\tfrac{2}{\sqrt{\mathcal{R}_{\Delta}}}\), which is reflected in fig. 2.

The bounds in eq. (25) and eq. (26) reveal an imperfect connection between the Procrustes distance and the average decoding distance, such as the GULP distance. We recall that every choice of readout weight normalization \(\boldsymbol{G}(\boldsymbol{X})\) leads to a measure of representational distance \(\mathbb{E}\|\boldsymbol{X}\boldsymbol{w}^{*}-\boldsymbol{Y}\boldsymbol{v}^{*} \|_{2}^{2}\), and that there is an associated Procrustes distance on the normalized representations \(\tilde{\boldsymbol{X}}\) and \(\tilde{\boldsymbol{Y}}\)

Figure 2: Bounds in eq. (25) plotted (solid lines) for varying participation ratio \(\mathcal{R}_{\Delta}\) with \(\alpha=\beta=1\). A) Allowed regions (between the solid curves of like color) of Procrustes distance and expected Euclidean distance between decoded signals for different values of participation ratio \(\mathcal{R}_{\Delta}\). B–D) Allowed regions for particular \(\mathcal{R}_{\Delta}\) intervals (black solid lines) populated with calculated distances between pairs of randomly sampled positive semi-definite (PSD) matrices of size \(50\times 50\), subsampled to those that have \(\mathcal{R}_{\Delta}\) in the particular interval (colored points). Different colors represent different random ensembles of positive semi-definite matrix pairs, which are described in appendix B.

\(\mathcal{P}(\bm{\dot{X}},\bm{\dot{Y}})\). These bounds tell us that the average decoding distance \(\mathbb{E}\|\bm{X}\bm{w}^{*}-\bm{Y}\bm{v}^{*}\|_{2}^{2}\) and this Procrustes distance constrain each other in a very particular way. Namely, when the Procrustes distance between normalized representations is small, then the average decoding distance must also be small.3 However, the converse is only necessarily true when the participation ratio of the difference of the kernel matrices \(\bm{K}_{X}-\bm{K}_{Y}\) is also small (e.g. fig. 2B). Indeed, as the dimensionality of \(\bm{K}_{X}-\bm{K}_{Y}\) increases, we see that the Procrustes distance between normalized representations can be large, but the average decoding distance can be relatively small, with the effect becoming more exaggerated as \(\mathcal{R}_{\Delta}\) becomes large (fig. 2D). In appendix A.1 we show that for \(\alpha=\beta=1\), the minimum possible participation ratio \(\mathcal{R}_{\Delta}\) is 2. Therefore, fig. 2 also illustrates another interesting phenomenon, namely that the upper left-hand corner of these plots is impossible to populate. That is, in the case of large expected difference in decoded signals, one can never simultaneously measure a small Procrustes distance between the normalized representations. Thus we observe quantitatively, as may be intuitive, that the Procrustes distance offers a more strict notion of geometric dissimilarity than the average decoding distance.

Footnote 3: The scale of “small” and “large” here are determined by \(\alpha\), \(\beta\), and \(\mathcal{R}_{\Delta}\), as the range of the Procrustes distance is between \(0\) and \(\sqrt{\alpha+\beta}\), while range of the average decoding distance is between \(0\) and \(\frac{\alpha+\beta}{\sqrt{\mathcal{R}_{\Delta}}}\).

## 5 Discussion

Understanding meaningful ways to measure similarities between neural representations is clearly a very complex problem. The literature demonstrates a proliferation of different techniques [20; 34], but an underdeveloped understanding of how these methods relate to each other. Less still is known about how representational similarity measures interact with functional similarity or the amount and type of information that is decodable from the representation. This paper presents a theoretical framework centered around the linear decoding of information from representations, which allows us to understand some existing popular methods for measuring representational similarity as average decoding similarity or average decoding distance with different choices of weight regularization. These connections relied on averaging the decoding similarity or decoding distance over a distribution of decoding targets \(\bm{z}\) with \(\mathbb{E}[\bm{z}\bm{z}^{\top}]=\bm{I}\). In the future it could be interesting to explore modifying these assumptions. For instance, instead of maximizing, minimizing, or taking the expectation over decoding targets, are there potentially interesting sets of fixed decoding targets that make sense when comparing networks in particular contexts? Furthermore, we focused on linear regression as a decoding method; a potentially interesting line of future work could be to extend this framework to linear classifiers (e.g. support vector machines or multi-class logistic regression). Lastly, in this paper we considered quantifying representational similarity across a finite set of \(M\) stimulus conditions. However, these finite-dimensional approaches can be framed as approximations or estimators for a population version of the problem. For example, the framework in [28] for the Procrustes distance, or [3] for the GULP distance (the plugin estimator of which is a special case of our framework as we saw above). We outline a framing of this perspective in appendix C, but a rigorous exploration of the \(M\to\infty\) regime and an analysis of the behavior of estimators for similarity scores in the limited sample regime could be an important topic of future work.

## References

* [1] Guillaume Alain and Yoshua Bengio. _Understanding intermediate layers using linear classifier probes_. 2017. url: https://openreview.net/forum?id=HJ4-rAVt1.
* [2] Rajendra Bhatia, Tanvi Jain, and Yongdo Lim. "On the Bures-Wasserstein distance between positive definite matrices". _Expositiones Mathematicae 37._ 2 (2019), pp. 165-191. url: https://www.sciencedirect.com/science/article/pii/S0723086918300021.
* [3] Enric Boix-Adsera, Hannah Lawrence, George Stepaniants, and Philippe Rigollet. "GULP: a prediction-based metric between representations". _Advances in Neural Information Processing Systems_. Ed. by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh. Vol. 35. Curran Associates, Inc., 2022, pp. 7115-7127.
* [4] Rosa Cao. "Putting representations to use". _Synthese_ 200.2 (2022), p. 151.
* [5] Nathan Cloos, Moufan Li, Markus Siegel, Scott L Brincat, Earl K Miller, Guangyu Robert Yang, and Christopher J Cueva. "Differentiable optimization of similarity scores between models and brains". _arXiv preprint arXiv:2407.07059_ (2024).

* Cortes et al. [2012] Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. "Algorithms for learning kernels based on centered alignment". _The Journal of Machine Learning Research_ 13 (2012), pp. 795-828.
* Dabrowska et al. [2021] Paulina Dabrowska, Nicole Voges, Michael von Papen, Junji Ito, David Dahmen, Alexa Riehle, Thomas Brochier, and Sonja Grun. "On the Complexity of Resting State Spiking Activity in Monkey Motor Cortex". _Cerebral Cortex Communications_ 2 (2021).
* Davari et al. [2023] MohammadReza Davari, Stefan Horoi, Amine Natik, Guillaume Lajoie, Guy Wolf, and Eugene Belilovsky. "Reliability of CKA as a Similarity Measure in Deep Learning". _The Eleventh International Conference on Learning Representations_. 2023. url: https://openreview.net/forum?id=8HRvyxc606.
* Ding et al. [2021] Frances Ding, Jean-Stanislas Denain, and Jacob Steinhardt. "Grounding Representation Similarity Through Statistical Testing". _Advances in Neural Information Processing Systems_. Ed. by M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan. Vol. 34. Curran Associates, Inc., 2021, pp. 1556-1568.
* Dryden and Mardia [2016] Ian L Dryden and Kanti V Mardia. _Statistical shape analysis: with applications in R_. John Wiley & Sons, 2016.
* Dujmovic et al. [2022] Marin Dujmovic, Jeffrey S Bowers, Federico Adolfi, and Gaurav Malhotra. "Some pitfalls of measuring representational similarity using Representational Similarity Analysis". _bioRxiv_ (2022), pp. 2022-04.
* Pleewa [2004] Ashraf M T Pleewa, ed. _Morphometrics_. 2004th ed. Berlin, Germany: Springer, 2004.
* Fusi et al. [2016] Stefano Fusi, Earl K Miller, and Mattia Rigotti. "Why neurons mix: high dimensionality for higher cognition". _Current opinion in neurobiology_ 37 (2016), pp. 66-74.
* Gao and Ganguli [2015] Peiran Gao and Surya Ganguli. "On simplicity and complexity in the brave new world of large-scale neuroscience". _Current Opinion in Neurobiology_ 32 (2015). Large-Scale Recording Technology (32), pp. 148-155. url: https://www.sciencedirect.com/science/article/pii/S0959438815000768.
* Giaffar et al. [2024] Hamza Giaffar, Camille Rullan Buxo, and Mikio Aoi. "The Effective Number of Shared Dimensions Between Paired Datasets". _Proceedings of The 27th International Conference on Artificial Intelligence and Statistics_. Ed. by Sanjoy Dasgupta, Stephan Mandt, and Yingzhen Li. Vol. 238. Proceedings of Machine Learning Research. PMLR, 2024, pp. 4249-4257. url: https://proceedings.mlr.press/v238/giaffar24a.html.
* Harvey et al. [2024] Sarah E. Harvey, Brett W. Larsen, and Alex H. Williams. "Duality of Bures and Shape Distances with Implications for Comparing Neural Representations". _Proceedings of UniReps: the First Workshop on Unifying Representations in Neural Models_. Ed. by Marco Fumero, Emanuele Rodola, Clementine Domine, Francesco Locatello, Karolina Dziugaite, and Caron Mathilde. Vol. 243. Proceedings of Machine Learning Research. PMLR, 2024, pp. 11-26.
* Kaufman et al. [2014] Matthew T Kaufman, Mark M Churchland, Stephen I Ryu, and Krishna V Shenoy. "Cortical activity in the null space: permitting preparation without movement". _Nature neuroscience_ 17.3 (2014), pp. 440-448.
* Kendall et al. [2009] David George Kendall, Dennis Barden, Thomas K Carne, and Huiling Le. _Shape and shape theory_. John Wiley & Sons, 2009.
* Khosla and Williams [2024] Meenakshi Khosla and Alex H Williams. "Soft Matching Distance: A metric on neural representations that captures single-neuron tuning". _Proceedings of UniReps: the First Workshop on Unifying Representations in Neural Models_. Ed. by Marco Fumero, Emanuele Rodola, Clementine Domine, Francesco Locatello, Karolina Dziugaite, and Caron Mathilde. Vol. 243. Proceedings of Machine Learning Research. PMLR, 2024, pp. 326-341.
* Klabunde et al. [2023] Max Klabunde, Tobias Schumacher, Markus Strohmaier, and Florian Lemmerich. "Similarity of neural network models: A survey of functional and representational measures". _arXiv preprint arXiv:2305.06329_ (2023).
* Klabunde et al. [2024] Max Klabunde, Tassilo Wald, Tobias Schumacher, Klaus Maier-Hein, Markus Strohmaier, and Florian Lemmerich. "ReSi: A Comprehensive Benchmark for Representational Similarity Measures". _arXiv preprint arXiv:2408.00531_ (2024).
* Kornblith et al. [2019] Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. "Similarity of Neural Network Representations Revisited". _International Conference on Machine Learning_. Ed. by Kamalika Chaudhuri and Ruslan Salakhutdinov. Vol. 97. Proceedings of Machine Learning Research. PMLR, 2019, pp. 3519-3529.

* Kriegeskorte and Diedrichsen [2019] Nikolaus Kriegeskorte and Jorn Diedrichsen. "Peeling the onion of brain representations". _Annual review of neuroscience_ 42 (2019), pp. 407-432.
* Kriegeskorte and Douglas [2019] Nikolaus Kriegeskorte and Pamela K Douglas. "Interpreting encoding and decoding models". _Current Opinion in Neurobiology_ 55 (2019). Machine Learning, Big Data, and Neuroscience, pp. 167-179. url: https://www.sciencedirect.com/science/article/pii/S0959438818301004.
* Kriegeskorte and Wei [2021] Nikolaus Kriegeskorte and Xue-Xin Wei. "Neural tuning and representational geometry". _Nature Reviews Neuroscience_ 22.11 (2021), pp. 703-718.
* Lampien et al. [2024] Andrew Kyle Lampien, Stephanie CY Chan, and Katherine Hermann. "Learned feature representations are biased by complexity, learning order, position, and more". _arXiv preprint arXiv:2405.05847_ (2024).
* Maheswaranathan et al. [2019] Niru Maheswaranathan, Alex Williams, Matthew Golub, Surya Ganguli, and David Sussillo. "Universality and individuality in neural dynamics across large populations of recurrent networks". _Advances in Neural Information Processing Systems_. Ed. by H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett. Vol. 32. Curran Associates, Inc., 2019.
* Pospisil et al. [2024] Dean A Pospisil, Brett W. Larsen, Sarah E Harvey, and Alex H Williams. "Estimating Shape Distances on Neural Representations with Limited Samples". _International Conference on Learning Representations_. 2024. url: https://openreview.net/forum?id=kvByNmMERu.
* Raghu et al. [2017] Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. "Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability". _Advances in neural information processing systems_ 30 (2017).
* Ramsay et al. [1984] James O Ramsay, Jos ten Berge, and George PH Styan. "Matrix correlation". _Psychometrika_ 49.3 (1984), pp. 403-423.
* Recanatesi et al. [2022] Stefano Recanatesi, Serena Bradde, Vijay Balasubramanian, Nicholas A. Steinmetz, and Eric Shea-Brown. "A scale-dependent measure of system dimensionality". _Patterns_ 3.8 (2022), p. 100555. url: https://www.sciencedirect.com/science/article/pii/S266638992200160X.
* Scholkopf and Smola [2001] Bernhard Scholkopf and Alexander J Smola. _Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond_. Adaptive Computation and Machine Learning Series. London, England: MIT Press, 2001.
* Srivastava and Klassen [2016] Anuj Srivastava and Eric P Klassen. _Functional and shape data analysis_. 1st ed. Springer Series in Statistics. New York, NY: Springer, 2016.
* Sucholutsky et al. [2023] Ilia Sucholutsky, Lukas Mutenthaler, Adrian Weller, Andi Peng, Andreea Bobu, Been Kim, Bradley C Love, Erin Grant, Jascha Achterberg, Joshua B Tenenbaum, et al. "Getting aligned on representational alignment". _arXiv preprint:2310.13018_ (2023).
* Watrous [2018] John Watrous. _The theory of quantum information_. Cambridge University Press, 2018.
* Williams et al. [2021] Alex H Williams, Erin Kunz, Simon Kornblith, and Scott Linderman. "Generalized shape metrics on neural representations". _Advances in Neural Information Processing Systems_ 34 (2021), pp. 4738-4750.
* Williams and Rasmussen [2006] Christopher KI Williams and Carl Edward Rasmussen. _Gaussian processes for machine learning_. Vol. 2. 3. MIT press Cambridge, MA, 2006.
* Yanai [1974] Haruo Yanai. "Unification of various techniques of multivariate analysis by means of generalized coefficient of determination". _Kodo Keiryogaku_ 1.1 (1974), pp. 46-54.

Appendix

### Bounds on the Procrustes distance in terms of the Euclidean distance

Here we derive upper and lower bounds on the Procrustes distance between neural representations \(\bm{X}\in\mathbb{R}^{M\times N_{X}}\) and \(\bm{Y}\in\mathbb{R}^{M\times N_{Y}}\) in terms of the Euclidean distance between linear kernel matrices \(\bm{K}_{X}=\bm{X}\bm{X}^{\top}\) and \(\bm{K}_{Y}=\bm{Y}\bm{Y}^{\top}\). This result is applied in the main text to relate the expected Euclidean distance between decoded signals \(\mathbb{E}\|\bm{X}\bm{w}^{*}-\bm{Y}\bm{v}^{*}\|_{2}^{2}=\|\tilde{\bm{X}}\tilde{ \bm{X}}^{\top}-\tilde{\bm{Y}}\tilde{\bm{Y}}^{\top}\|_{F}^{2}\) to the Procrustes distance \(\mathcal{P}(\tilde{\bm{X}},\tilde{\bm{Y}})\). Tildes are omitted in this section for clarity, but we note that this result is applied in the main text to the normalized representations \(\tilde{\bm{X}}\) and \(\tilde{\bm{Y}}\).

We will make use of the equivalence between the Procrustes distance between representation matrices \(\bm{X}\) and \(\bm{Y}\) and a notion of distance on the space of positive semi-definite kernel matrices \(\bm{K}_{X}=\bm{X}\bm{X}^{\top}\) and \(\bm{K}_{Y}=\bm{Y}\bm{Y}^{\top}\) called the Bures distance, defined as

\[d_{B}(\bm{K}_{X},\bm{K}_{Y})=\sqrt{\operatorname{Tr}[\bm{K}_{X}]+ \operatorname{Tr}[\bm{K}_{Y}]-2\operatorname{Tr}\left[\left(\bm{K}_{X}^{1/2} \bm{K}_{Y}\bm{K}_{X}^{1/2}\right)^{1/2}\right]}.\] (27)

The third trace term on the right hand side of eq. (27) is often called the _fidelity_, defined as

\[\mathcal{F}(\bm{K}_{X},\bm{K}_{Y})=\operatorname{Tr}\left[\left(\bm{K}_{X}^{1 /2}\bm{K}_{Y}\bm{K}_{X}^{1/2}\right)^{1/2}\right].\] (28)

#### a.1.1 Lower bound

For the lower bound, we are inspired by the Fuchs-van-de-Graaf inequalites that are used in quantum information theory to assert an approximate equivalence between two measures of quantum state similarity, the _trace distance_ and the _fidelity_. This approach is relevant from a mathematical perspective for our purposes, since quantum states are represented by positive semi-definite matrices normalized to have trace 1. Here, we use a similar approach as that often used to derive the Fuchs-van de Graaf inequalites to instead relate the Euclidean distance and the Bures distance on positive semidefinite matrices, while also relaxing the trace 1 normalization.

We will make use of the following identity.

**Lemma 1**.: \[\|\alpha uu^{\dagger}-\beta vv^{\dagger}\|_{*}=\sqrt{(\alpha+\beta)^{2}-4 \alpha\beta|\langle u,v\rangle|^{2}}\] (29)

_for all unit vectors \(u,v\) and all non-negative real numbers \(\alpha\) and \(\beta\)._

Proof.: First we recognize that the nuclear norm of a matrix can be calculated by summing the singular values of that matrix. Furthermore, if that matrix is Hermitian, the singular values are the absolute values of the eigenvalues. Define \(\bm{M}=\alpha uu^{\dagger}-\beta vv^{\dagger}\). Our matrix \(M\) is Hermitian and has at most two eigenvalues, so we will look for expressions for these in terms of \(\alpha\) and \(\beta\). Note that if \(u\) and \(v\) are the same unit vector, then \(\bm{M}\) has rank 1, so we expect the eigenvalues will depend also on the inner product \(\langle u,v\rangle\).

The eigenvectors of \(\bm{M}\) will be of the form \(\psi=cu+dv\) for some scalar \(c\) and \(d\). By direct calculation:

\[\bm{M}\psi= \lambda\psi\] \[\bm{M}(cu+dv)= (\alpha c+\alpha d\langle u,v\rangle)u-(\beta c\langle v,u\rangle +\beta d)v\] (30) \[= \lambda(cu+dv)\]

Therefore, we see that the eigenvalues must satisfy both

\[\lambda=\frac{\alpha c+\alpha d\langle u,v\rangle}{c}\ \ \text{and}\ \ \lambda=-\frac{\beta c \langle v,u\rangle+\beta d}{d}\] (31)

Setting these equal to each other and solving the resulting quadratic for \(c\) gives:\[c=\frac{-d(\alpha+\beta)\pm d\sqrt{(\alpha+\beta)^{2}-4\alpha\beta|\langle v,u\rangle |^{2}}}{2\beta\langle v,u\rangle}.\] (32)

So the two eigenvectors are proportional to

\[\psi_{\pm}=\Big{(}\frac{(\alpha+\beta)}{2}\mp\frac{1}{2}\sqrt{(\alpha+\beta)^{2 }-4\alpha\beta|\langle v,u\rangle|^{2}}\Big{)}u-\beta\langle v,u\rangle v.\] (33)

To find the eigenvalues, we combine eq. (31) and eq. (32), arriving at

\[\lambda_{\pm}=\frac{(\beta-\alpha)}{2}\pm\frac{1}{2}\sqrt{(\alpha+\beta)^{2}- 4\alpha\beta|\langle v,u\rangle|^{2}}.\] (34)

Finally, to calculate \(\|\bm{M}\|_{*}\) we simply sum the magnitudes of these eigenvalues. By inspecting the term under the square root, we see that \(\lambda_{-}\) is always negative. Therefore, \(|\lambda_{+}|+|\lambda_{-}|=\lambda_{+}-\lambda_{-}\), and we find

\[\|\bm{M}\|_{1}= |\lambda_{+}|+|\lambda_{-}|\] \[\implies\|\alpha uu^{\dagger}-\beta vv^{\dagger}\|_{1}= \sqrt{(\alpha+\beta)^{2}-4\alpha\beta|\langle v,u\rangle|^{2}}\] (35)

We now find a lower bound on the Bures distance between kernel matrices \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) in terms of the Euclidean distance between the same matrices, \(\|\bm{K}_{X}-\bm{K}_{Y}\|_{F}^{2}\). There are many choices of \(\bm{X}\) and \(\bm{Y}\) multiply to the same positive semi-definite kernel matrices via \(\bm{K}_{X}=\bm{X}\bm{X}^{\top}\) and \(\bm{K}_{Y}=\bm{Y}\bm{Y}^{\top}\). Define real positive scalars \(\alpha=\operatorname{Tr}\bm{X}\bm{X}^{\top}\) and \(\beta=\operatorname{Tr}\bm{Y}\bm{Y}\bm{Y}^{\top}\), and the unit vectors

\[\hat{x}=\frac{\operatorname{vec}(\bm{X})}{\|\operatorname{vec}(\bm{X})\|_{2 }}\quad\text{and}\quad\hat{y}=\frac{\operatorname{vec}(\bm{Y})}{\| \operatorname{vec}(\bm{Y})\|_{2}}\] (36)

where \(\operatorname{vec}(\bm{X})\) is the vectorization linear transformation converting the \(M\times N_{X}\)-dimensional matrix \(\bm{X}\) into an \(MN_{X}\)-dimensional vector. We will assume that the representations have been zero-padded such that \(N_{X}=N_{Y}\).

By Uhlmann's theorem [35], there exists some choice of \(\bm{X}\) and \(\bm{Y}\), such that

\[|\langle\sqrt{\alpha}\hat{x},\sqrt{\beta}\hat{y}\rangle|=\mathcal{F}(\bm{K}_{X },\bm{K}_{Y})\] (37)

One can see this by writing the fidelity as a nuclear norm of \(\bm{X}^{\top}\bm{Y}\) and considering the variational form of the nuclear norm as a maximization of the Hilbert-Schmidt inner product over unitary transformations (see [16] for a more explicit discussion of this).

Now use our identity from earlier with \(u=\hat{x}\) and \(v=\hat{y}\).

\[\|\alpha\hat{x}\hat{x}^{\top}-\beta\hat{y}\hat{y}^{\top}\|_{1}= \sqrt{(\alpha+\beta)^{2}-4\alpha\beta|\langle\hat{x},\hat{y}\rangle |^{2}}\] (38) \[= \sqrt{(\alpha+\beta)^{2}-4\mathcal{F}(\bm{K}_{X},\bm{K}_{Y})^{2}}\]

The operation that takes \(\alpha\hat{x}\hat{x}^{\top}\) and \(\beta\hat{y}\hat{y}^{\top}\) to \(\bm{K}_{X}\) and \(\bm{K}_{Y}\), respectively, is called the partial trace. By the monotonicity of the nuclear norm under partial tracing [35], we have:

\[\|\bm{K}_{X}-\bm{K}_{Y}\|_{*}\leq\|\alpha\hat{x}\hat{x}^{\top}-\beta\hat{y} \hat{y}^{\top}\|_{*}=\sqrt{(\alpha+\beta)^{2}-4\mathcal{F}(\bm{K}_{X},\bm{K}_{ Y})^{2}}.\] (39)

With equality when \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) are rank-1, that is \(\bm{K}_{X}=\alpha\hat{x}\hat{x}^{\top}\) and \(\bm{K}_{Y}=\beta\hat{y}\hat{y}^{\top}\).

Lastly, we rewrite the nuclear norm \(\|\bm{K}_{X}-\bm{K}_{Y}\|_{*}\) in terms of the Euclidean distance and the _participation ratio_ of the matrix \(\bm{K}_{X}-\bm{K}_{Y}\). The participation ratio of a matrix \(\bm{A}\in\mathbb{R}^{M\times N}\) is defined as

\[\mathcal{R}(\bm{A})=\frac{\|\bm{A}\|_{1}^{2}}{\|\bm{A}\|_{F}^{2}}=\frac{(\sum_{ i}\sigma_{i})^{2}}{\sum_{i}\sigma_{i}^{2}},\] (40)

so we have

\[\|\bm{K}_{X}-\bm{K}_{Y}\|_{*}=\sqrt{\mathcal{R}_{\Delta}}\|\bm{K}_{X}-\bm{K}_{ Y}\|_{F}\] (41)

where \(\mathcal{R}_{\Delta}=\mathcal{R}(\tilde{\bm{X}}\tilde{\bm{X}}^{\top}-\tilde{ \bm{Y}}\tilde{\bm{Y}}^{\top})\). We can also use the definition of the Bures distance eq. (27) to replace \(\mathcal{F}(\bm{K}_{X},\bm{K}_{Y})=\frac{1}{2}(\alpha+\beta-d_{B}(\bm{K}_{X}, \bm{K}_{Y})^{2})\). Making these substitutions into eq. (39), and solving for \(d_{B}(\bm{K}_{X},\bm{K}_{Y})^{2}\), we find:

\[d_{B}(\bm{K}_{X},\bm{K}_{Y})^{2}\geq(\alpha+\beta)-\sqrt{(\alpha+\beta)^{2}- \mathcal{R}_{\Delta}\|\bm{K}_{X}-\bm{K}_{Y}\|_{F}^{2}}.\] (42)

This bound is saturated when the inequalty in eq. (39) is an equality, or when \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) are both rank 1.

For a bound that is independent of the participation ratio, we could replace \(\mathcal{R}_{\Delta}\) with its ostensible minimal value of 1. When \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) are normalized to have trace 1, we would then have

\[d_{B}(\bm{K}_{X},\bm{K}_{Y})^{2}\geq 2-\sqrt{4-\|\bm{K}_{X}-\bm{K}_{Y}\|_{F}^{ 2}}\approx\frac{1}{4}\|\bm{K}_{X}-\bm{K}_{Y}\|_{F}^{2}.\] (43)

However, in this case that \(\operatorname{Tr}\bm{K}_{X}=\operatorname{Tr}\bm{K}_{Y}=1\), we can actually arrive at a tighter lower bound than eq. (43), because we can say something interesting about the minimal participation ratio of matrices of the form \(\bm{K}_{X}-\bm{K}_{Y}\). It turns out that under this normalization, the participation ratio is lower bounded by 2, with the minimum of 2 achieved when \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) are rank 1 (also in this case the bound in eq. (42) will be saturated).

**Lemma 2**.: _For positive semidefinite matrices \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) with \(\operatorname{Tr}\bm{K}_{X}=\operatorname{Tr}\bm{K}_{Y}\),_

\[\mathcal{R}(\bm{K}_{X}-\bm{K}_{Y})\geq 2.\] (44)

Proof.: By noting that \(\bm{K}_{X}-\bm{K}_{Y}\) is Hermitian, we rewrite eq. (40) as

\[\mathcal{R}(\bm{K}_{X}-\bm{K}_{Y})=\frac{(\sum_{i}|\lambda_{i}|)^{2}}{\sum_{i} |\lambda_{i}|^{2}}\] (45)

where \(\lambda_{i}\) is the \(i\)th eigenvalue of the matrix difference \(\bm{K}_{X}-\bm{K}_{Y}\). Next, we recognize that if \(\operatorname{Tr}\bm{K}_{X}=\operatorname{Tr}\bm{K}_{Y}\), then \(\operatorname{Tr}(\bm{K}_{X}-\bm{K}_{Y})=0\) and therefore the eigenvalues of \(\bm{K}_{X}-\bm{K}_{Y}\) sum to 0. The set of eigenvalues \(\{\lambda_{1},\lambda_{2},...,\lambda_{M}\}\) can then always be partitioned into a set of positive eigenvalues \(\{\lambda_{1}^{+},\lambda_{2}^{+},...,\lambda_{p}^{+}\}\), and a set of negative eigenvalues \(\{\lambda_{1}^{-},\lambda_{2}^{-},...,\lambda_{q}^{-}\}\), with \(p+q=M\). The negative eigenvalues must balance the positive eigenvalues in magnitude, so we must have

\[\sum_{i=1}^{p}\lambda_{i}^{+}=\sum_{j=1}^{q}|\lambda_{j}^{-}|.\] (46)

We can rewrite eq. (45) in terms of sums over the positive and negative eigenvalue sets:\[\mathcal{R}(\bm{K}_{X}-\bm{K}_{Y}) =\frac{4(\sum_{i=1}^{p}\lambda_{i}^{+})^{2}}{\sum_{i=1}^{p}(\lambda_ {i}^{+})^{2}+\sum_{j=1}^{q}|\lambda_{j}^{-}|^{2}}\] (47) \[=\frac{4\big{[}\sum_{i=1}^{p}(\lambda_{i}^{+})^{2}+\sum_{j\neq k} ^{p}\lambda_{j}^{+}\lambda_{k}^{+}\big{]}}{(\sum_{i=1}^{p}\lambda_{i}^{+})^{2} +\frac{1}{2}(\sum_{j\neq k}^{p}\lambda_{j}^{+}\lambda_{k}^{+}-\sum_{j\neq k}^{ q}|\lambda_{j}^{-}||\lambda_{k}^{-}|)}\] \[=2\left[\frac{\sum_{i=1}^{p}(\lambda_{i}^{+})^{2}+\sum_{j\neq k} ^{p}\lambda_{j}^{+}\lambda_{k}^{+}}{\sum_{i=1}^{p}(\lambda_{i}^{+})^{2}+\frac {1}{2}(\sum_{j\neq k}^{p}\lambda_{j}^{+}\lambda_{k}^{+}-\sum_{j\neq k}^{q}| \lambda_{j}^{-}||\lambda_{k}^{-}|)}\right]\]

By inspection, the term in brackets is \(\geq 1\) (this can be seen by considering the signs and relative magnitudes of each of the summed terms). So we have

\[\mathcal{R}(\bm{K}_{X}-\bm{K}_{Y})\geq 2.\] (48)

The inequality eq. (44) is saturated when \(\bm{K}_{X}-\bm{K}_{Y}\) is rank 2 and thus only has two equal magnitude and opposite signed eigenvalues, as can be seen from the first line of eq. (47). We can also use eq. (34) to see that when \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) are rank 1 and trace 1, we have

\[\mathcal{R}(\bm{K}_{X}-\bm{K}_{Y})=\frac{\|\bm{K}_{X}-\bm{K}_{Y}\|_{1}^{2}}{ \|\bm{K}_{X}-\bm{K}_{Y}\|_{F}^{2}}=\frac{(\alpha+\beta)^{2}-4\alpha\beta| \langle v,u\rangle|^{2}}{\frac{1}{2}[(\alpha+\beta)^{2}-4\alpha\beta|\langle v,u\rangle|^{2}]}=2.\] (49)

For \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) normalized to have trace 1, we can then state a tighter bound than eq. (43) using the minimal participation ratio in eq. (44). We have

\[d_{B}(\bm{K}_{X},\bm{K}_{Y})^{2}\geq 2-\sqrt{4-2\|\bm{K}_{X}-\bm{K}_{Y}\|_{F}^{2}}.\] (50)

#### a.1.2 Upper bound

We can bound the Bures distance using its variational form [2],

\[d_{B}^{2}(\bm{K}_{X},\bm{K}_{Y})=\min_{\bm{Q}\in\mathcal{O}(M)}\|\bm{K}_{X}^{1 /2}-\bm{K}_{Y}^{1/2}\bm{Q}\|_{F}^{2}\leq\|\bm{K}_{X}^{1/2}-\bm{K}_{Y}^{1/2}\|_ {F}^{2}\] (51)

The Powers-Stormer inequality implies that \(\|\bm{K}_{X}^{1/2}-\bm{K}_{Y}^{1/2}\|_{F}^{2}\leq\|\bm{K}_{X}-\bm{K}_{Y}\|_{*}\), so

\[d_{B}^{2}(\bm{K}_{X},\bm{K}_{Y})\leq\|\bm{K}_{X}-\bm{K}_{Y}\|_{*}.\] (52)

Expressing the nuclear norm in terms of the participation ratio of \(\bm{K}_{X}-\bm{K}_{Y}\) using eq. (41), we have an upper bound on the Bures distance:

\[d_{B}^{2}(\bm{K}_{X},\bm{K}_{Y})\leq\sqrt{\mathcal{R}_{\Delta}}\|\bm{K}_{X}- \bm{K}_{Y}\|_{F}.\] (53)

The inequality in eq. (51) is saturated when the optimal orthogonal transformation that aligns \(\bm{K}_{X}^{1/2}\) and \(\bm{K}_{Y}^{1/2}\) is the identity. This occurs when \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) are simultaneously diagonalizable. The Powers-Stormer inequality in eq. (52) is saturated when \(\bm{K}_{X}\) and \(\bm{K}_{Y}\) have eigenvalues that only take values in \(\{0,1\}\).

Figure 2 details

Figure 2 panels (B-D) show the allowed regions of Procrustes distance and expected Euclidean distance between decoded signals for representations normalized to have \(\operatorname{Tr}\tilde{\bm{X}}\tilde{\bm{X}}^{\top}=\operatorname{Tr}\tilde{\bm{ Y}}\tilde{\bm{Y}}^{\top}=1\), with varying participation ratio \(\mathcal{R}_{\Delta}\). We have populated the plots with points representing the respective distances between randomly sampled pairs of positive semi-definite matrices. These positive semi-definite matrices represent normalized kernel matrices \(\tilde{\bm{X}}\tilde{\bm{X}}^{\top}\) and \(\tilde{\bm{Y}}\tilde{\bm{Y}}^{\top}\). We have seen in the main text that the squared Euclidean distance between these kernel matrices is equivalent to the expected squared Euclidean distance between decoded signals, \(\mathbb{E}\|\bm{X}\bm{w}^{*}-\bm{Y}\bm{v}^{*}\|_{2}^{2}=\|\tilde{\bm{X}}\tilde{ \bm{X}}^{\top}-\tilde{\bm{Y}}\tilde{\bm{Y}}^{\top}\|_{F}^{2}\), when the distribution of decoding targets satisfies \(\mathbb{E}[\bm{z}\bm{z}^{\top}]=\bm{I}\). On the other hand, [16] demonstrated how the Procrustes distance \(\mathcal{P}(\tilde{\bm{X}},\tilde{\bm{Y}})\) is equivalent to the Bures distance eq. (27), \(d_{B}(\tilde{\bm{X}}\tilde{\bm{X}}^{\top},\tilde{\bm{Y}}\tilde{\bm{Y}}^{\top})\). Therefore, both distances on these plots can be calculated from samples of positive semi-definite matrix pairs.

Each color of points in fig. 2 (B-D) represents a different random ensemble of \(M\times M\) PSD matrices with \(M=50\), which are then binned into one of the three plots if the participation ratio \(\mathcal{R}_{\Delta}\) lies in the respective range indicated above each plot panel.

* Pink points are generated by sampling eigenvalues for the two PSD matrices from a Dirichlet distribution with concentration parameters logarithmically spaced between \(10^{-3}\) and \(10^{3}\). Matrices of eigenvectors are then randomly sampled from the set of orthogonal matrices.
* Green points were generated by sampling a random matrix \(\bm{A}\) of size \(M\times r\) with each element drawn from the uniform distribution \(\mathcal{U}[0,1]\). A uniform random integer \(r\) was selected between 1 and 24. A PSD matrix was generated by multiplying \(\bm{A}\bm{A}^{\top}\). Another PSD matrix was then generated by adding a randomly weighted matrix of the same dimension with standard normal distributed entries \(N\) to \(\bm{A}\), so \(\tilde{\bm{B}}=\bm{A}+\epsilon\bm{N}\), where \(\epsilon\sim\mathcal{U}[0,1]\). The distance between \(\frac{1}{\operatorname{Tr}\bm{A}\bm{A}^{\top}}\bm{A}\bm{A}^{\top}\) and \(\frac{1}{\operatorname{Tr}\bm{B}\bm{B}^{\top}}\bm{B}\bm{B}^{\top}\) is then computed using these two distance metrics.
* Blue points are seen to only occupy fig. 2 (B), as these correspond to distances between matrices of rank 1 and trace 1. As we saw earlier in this appendix, in this case the participation ratio of the difference between these two PSD matrices is always 2. The rank 1 matrices were sampled using the same method as the green points above, but setting \(r=1\).
* Purple points were generated by sampling one matrix from a Wishart distribution \(W_{M}(r,\bm{I})\), with degrees of freedom \(r\) chosen as a uniformly random integer between \(1\) and \(50\). This matrix was normalized to have trace 1. The second matrix was generated by adding to the first matrix another matrix drawn from the Wishart distribution \(W_{M}(n,\bm{I})\), with \(n\) a randomly selected integer between 1 and 10. Both matrices are normalized to have trace 1.
* Yellow points represent distances between PSD matrices that are simultaneously diagonalizable. These were generated by sampling \(M\) values uniformly between \(0\) and \(1\) for each matrix, but and setting these values to 0 if they fall below the threshold 0.6. The resultant values were then normalized to sum to 1, which became the eigenvalues for the two PSD matrices. A matrix of eigenvectors was then randomly selected from the set of orthogonal matrices.

## Appendix C Generalizations and Interpretations as \(M\to\infty\)

In the main text, we considered quantifying representational similarity across a finite set of \(M\) stimulus conditions. Further, when considering the average distance in decoding performance, we have assumed that \(\mathbb{E}[\bm{z}\bm{z}^{\top}]=\bm{I}\). Here, we briefly discuss how to relax both constraints, leaving a full investigation to future work. Readers will need familiarity with the basic principles of kernel methods and gaussian processes in machine learning (see e.g. [32, 37]) to follow along with certain results in this section.

To begin, we must revisit and refine our theoretical framework developed in section 2. Under proposition 2, we treated the neural response matrices, \(\bm{X}\) and \(\bm{Y}\), as constants while we treated the decoder targets, \(\bm{z}\), as a random variable. In this section we will treat \(\bm{X}\), \(\bm{Y}\) and \(\bm{z}\) as joint random variables, as has been done in prior work [28, 3]. Specifically, let \(\mathcal{U}\) denote the space of possible stimulus inputs (e.g. the space of natural images or grammatically correct English sentences). We are interested in quantifying similarity between two neural networks, \(f:\mathcal{U}\mapsto\mathbb{R}^{N_{X}}\) and \(g:\mathcal{U}\mapsto\mathbb{R}^{N_{Y}}\), across this stimulus space. Let \(P_{\bm{u}}\) denote a distribution with support on \(\mathcal{U}\), and let \(\bm{u}_{1},\dots,\bm{u}_{M}\) denote independent samples from \(P_{\bm{u}}\). As before, we collect the network responses into matrices \(\bm{X}\in\mathbb{R}^{M\times N_{X}}\) and \(\bm{Y}\in\mathbb{R}^{M\times N_{Y}}\), which we assume to be mean centered, \(\mathbb{E}[f(\bm{u})]=\mathbb{E}[g(\bm{u})]=\bm{0}\).

Finally, we define a decoding task as a function \(\eta:\mathcal{U}\mapsto\mathbb{R}\). For \(M\) finite samples (as considered in the main text), the decoder target is the vector with elements consisting of \(\eta\) evaluated at each sampled input: \(\bm{z}=[\eta(\bm{u}_{1})\quad\dots\quad\eta(\bm{u}_{M})]^{\top}\). As in sections 3 and 4, we will be interested in characterizing the _average_ decoder alignment over multiple tasks. In the main text we considered \(\mathbb{E}_{\bm{z}}\langle\bm{X}\bm{w}^{*},\bm{Y}\bm{v}^{*}\rangle\), but we would like to now generalize this to an expectation of an inner product between two _functions_ over the input space. We consider \(\eta\) to be drawn from a Gaussian process, \(\eta\sim\mathcal{GP}(q)\), with a covariance operator \(q:\mathcal{U}\times\mathcal{U}\mapsto\mathbb{R}\). Intuitively, \(q\) defines the difficulty of the distribution over regression tasks by specifying smoothness with respect to the input space \(\mathcal{U}\). For example, the popular squared exponential or radial basis function (RBF) kernel:

\[q(\bm{u},\bm{u}^{\prime})=\exp\left(-\frac{\|\bm{u}-\bm{u}^{\prime}\|_{2}^{2}} {2\gamma}\right)\] (54)

comes equipped with a length scale parameter \(\gamma>0\) that determines the smoothness of functions sampled \(\eta\sim\mathcal{GP}(q)\). In the limit that \(\gamma\to 0\) we get the Kronecker delta function:

\[\delta(\bm{u},\bm{u}^{\prime})=\begin{cases}1&\bm{u}=\bm{u}^{\prime}\\ 0&\bm{u}\neq\bm{u}^{\prime}\end{cases}\] (55)

In the remainder of this section, we show that the average decoding similarity converges to an expected inner product between kernels defined by the two neural systems. Specifically, let us define \(k:\mathcal{U}\times\mathcal{U}\mapsto\mathbb{R}\) and \(h:\mathcal{U}\times\mathcal{U}\mapsto\mathbb{R}\) as follows:

\[k(\bm{u},\bm{u}^{\prime})=f(\bm{u})^{\top}\bm{G}(f)^{-1}f(\bm{u}^{\prime}) \qquad\text{and}\qquad h(\bm{u},\bm{u}^{\prime})=g(\bm{u})^{\top}\bm{G}(g)^{ -1}g(\bm{u}^{\prime})\] (56)

where we have generalized eq. (4) as:

\[\bm{G}(f)=a\mathbb{E}[f(\bm{u})f(\bm{u})^{\top}]+b\bm{I}\] (57)

We denote by \(L^{2}(\mathcal{U})\) the space of \(L^{2}\)-integrable functions from the set \(\mathcal{U}\) to \(\mathbb{R}\). Consider the integral operators \(\mathcal{K}:L^{2}(\mathcal{U})\to L^{2}(\mathcal{U})\) and \(\mathcal{H}:L^{2}(\mathcal{U})\to L^{2}(\mathcal{U})\) associated with the kernels in eq. (56). We define the functions \(\eta_{\mathcal{K}}(\bm{u})\) and \(\eta_{\mathcal{H}}(\bm{u})\) as these operators acting on the decoding task function \(\eta(\bm{u})\):

\[[\mathcal{K}\eta](\bm{u}) =\int k(\bm{u},\bm{u}^{\prime})\eta(\bm{u}^{\prime})p(\bm{u}^{ \prime})d\bm{u}^{\prime}\equiv\eta_{\mathcal{K}}(\bm{u})\] (58) \[[\mathcal{H}\eta](\bm{u}) =\int h(\bm{u},\bm{u}^{\prime})\eta(\bm{u}^{\prime})p(\bm{u}^{ \prime})d\bm{u}^{\prime}\equiv\eta_{\mathcal{H}}(\bm{u})\] (59)

The functions \(\eta_{\mathcal{K}}(\bm{u})\) and \(\eta_{\mathcal{H}}(\bm{u})\) are analogous to the vectors \(\bm{X}\bm{w}^{*}=\bm{K}_{X}\in\mathbb{R}^{M}\) and \(\bm{Y}\bm{v}^{*}=\bm{K}_{Y}\in\mathbb{R}^{M}\), and can be thought of as the 'decoded' functions. We will define a similarity score as the inner product:

\[S=\langle\eta_{\mathcal{K}},\eta_{\mathcal{H}}\rangle_{L^{2}}=\int_{\mathcal{ U}}\eta_{\mathcal{K}}(\bm{u})\eta_{\mathcal{H}}(\bm{u})p(\bm{u})d\bm{u}\] (60)

Then we have, using the definitions in eq. (58) and eq. (59),

\[S=\int_{\mathcal{U}}\int_{\mathcal{U}}\int_{\mathcal{U}}k(\bm{u},\bm{u}^{ \prime})\eta(\bm{u}^{\prime})\eta(\bm{u}^{\prime\prime})h(\bm{u},\bm{u}^{ \prime\prime})\,p(\bm{u})d\bm{u}\,p(\bm{u}^{\prime})d\bm{u}^{\prime}\,p(\bm{u} ^{\prime\prime})d\bm{u}^{\prime\prime}.\] (61)This integral measures the similarity between two neural systems \(f\) and \(g\) for a particular choice of decoding task function \(\eta\). However, following the main text, we would like to take the expectation over some ensemble of decoding task functions \(\eta\).

\[\mathbb{E}_{\eta}[S]=\mathbb{E}_{\eta}\langle\eta_{\mathcal{K}},\eta_{\mathcal{ H}}\rangle_{L^{2}}\] (62)

Assuming the conditions to invoke Fubini's theorem regarding changing the ordering of integration are met, we have

\[\mathbb{E}_{\eta}[S]=\int_{\mathcal{U}}\int_{\mathcal{U}}\int_{\mathcal{U}}k( \boldsymbol{u},\boldsymbol{u}^{\prime})q(\boldsymbol{u}^{\prime},\boldsymbol {u}^{\prime\prime})h(\boldsymbol{u},\boldsymbol{u}^{\prime\prime})\,p( \boldsymbol{u})d\boldsymbol{u}\,p(\boldsymbol{u}^{\prime})d\boldsymbol{u}^{ \prime}\,p(\boldsymbol{u}^{\prime\prime})d\boldsymbol{u}^{\prime\prime}.\] (63)

We now recognize this integral as this as the trace of a composition of integral operators,

\[\mathbb{E}_{\eta}[S]=\operatorname{Tr}[\mathcal{K}\mathcal{Q}\mathcal{H}]\] (64)

where \(\mathcal{Q}:L^{2}(\mathcal{U})\to L^{2}(\mathcal{U})\) is the covariance operator associated with the Gaussian process covariance function \(q\). This quantity is finite, since all of the operators involved are Hilbert-Schmidt and trace-class.

We now would like to show that the finite-dimensional notion of "decoding similarity" introduced in the main text (appropriately normalized) can be thought of as a plug-in estimator of the quantity eq. (64), and that this estimator converges to \(\mathbb{E}_{\eta}[S]\) as \(M\to\infty\).

A 'plug-in' estimator for this could be to use the Gram matrices for each kernel and matrix multiplication to approximate the integrals. First, we sample \(M\) examples from the input distribution \(\boldsymbol{u}\sim P_{\boldsymbol{u}}\) and construct the Gram matrices \(\boldsymbol{Q}_{ij}=q(\boldsymbol{u}_{i},\boldsymbol{u}_{j})\) and

\[\boldsymbol{K}_{ij}=k(\boldsymbol{u}_{i},\boldsymbol{u}_{j})=f( \boldsymbol{u}_{i})^{\top}\boldsymbol{G}(f)^{-1}f(\boldsymbol{u}_{j})\] (65) \[\boldsymbol{H}_{ij}=h(\boldsymbol{u}_{i},\boldsymbol{u}_{j})=g( \boldsymbol{u}_{i})^{\top}\boldsymbol{G}(g)^{-1}g(\boldsymbol{u}_{j}).\] (66)

Then we have, by the strong law of large numbers,

\[\int_{\mathcal{U}}\int_{\mathcal{U}}\int_{\mathcal{U}}k(\boldsymbol{u}, \boldsymbol{u}^{\prime})q(\boldsymbol{u}^{\prime},\boldsymbol{u}^{\prime \prime})h(\boldsymbol{u},\boldsymbol{u}^{\prime\prime})\,p(\boldsymbol{u})d \boldsymbol{u}\,p(\boldsymbol{u}^{\prime})d\boldsymbol{u}^{\prime}\,p( \boldsymbol{u}^{\prime\prime})d\boldsymbol{u}^{\prime\prime}=\lim_{M\to\infty }\frac{1}{M^{3}}\sum_{ijk}K_{ij}Q_{jk}H_{ki}\]

or

\[\operatorname{Tr}[\mathcal{K}\mathcal{Q}\mathcal{H}]=\lim_{M\to\infty}\frac{1 }{M^{3}}\operatorname{Tr}\boldsymbol{K}\boldsymbol{Q}\boldsymbol{H}.\] (67)

However, the 'true' Gram matrices generated from the kernels in eq. (65) and eq. (66) in practice are also estimated, as the \(N\times N\) regularization matrices \(\boldsymbol{G}(f)\) and \(\boldsymbol{G}(g)\) are potentially estimated from the same \(M\) samples from \(P_{\boldsymbol{u}}\). We estimate \(\boldsymbol{G}(f)\) and \(\boldsymbol{G}(g)\) using the plug-in estimates of the \(N\times N\) covariance matrices \(\mathbb{E}[f(\boldsymbol{u})f(\boldsymbol{u})^{\top}]\) and \(\mathbb{E}[g(\boldsymbol{u})g(\boldsymbol{u})^{\top}]\).

\[\boldsymbol{G}(\boldsymbol{X})=\frac{\alpha}{M}\boldsymbol{X}^{\top} \boldsymbol{X}+\beta\boldsymbol{I}\xrightarrow[M\to\infty]{}\alpha\mathbb{E }[f(\boldsymbol{u})f(\boldsymbol{u})^{\top}]+\beta\boldsymbol{I}=\boldsymbol{ G}(f)\] (68)

\[\boldsymbol{G}(\boldsymbol{Y})=\frac{\alpha}{M}\boldsymbol{Y}^{\top} \boldsymbol{Y}+\beta\boldsymbol{I}\xrightarrow[M\to\infty]{}\alpha\mathbb{E }[g(\boldsymbol{u})g(\boldsymbol{u})^{\top}]+\beta\boldsymbol{I}=\boldsymbol{ G}(g)\] (69)

More precisely, it can be shown that the empirical regularization matrices \(\boldsymbol{G}(\boldsymbol{X})\) and \(\boldsymbol{G}(\boldsymbol{Y})\) concentrates around \(\boldsymbol{G}(f)\) and \(\boldsymbol{G}(g)\) in operator norm:

\[\|\boldsymbol{G}(\boldsymbol{X})-\boldsymbol{G}(f)\|_{op}\xrightarrow[M\to \infty]{}0\] (70)\[\|(\bm{G}(\bm{X})^{-1}-\bm{G}(f)^{-1})\|_{op}\leq\frac{|\alpha|}{\beta^{2}}\| \mathbb{E}[f(\bm{u})f(\bm{u})^{\top}]-\frac{1}{M}\bm{X}^{\top}\bm{X}\|_{op}\] (76)

or

\[\|(\bm{G}(\bm{X})^{-1}-\bm{G}(f)^{-1})\|_{op}\leq\frac{|\alpha|}{\beta^{2}}\| \bm{G}(\bm{X})-\bm{G}(f)\|_{op}\] (77)

with an analogous bound holding for \(\bm{G}(\bm{Y})\) and \(\bm{G}(g)\).

With this knowledge, we define \(\hat{\bm{K}}\) and \(\hat{\bm{H}}\) as the empirical Gram matrices constructed using the estimates \(\bm{G}(\bm{X})\) and \(\bm{G}(\bm{Y})\), and conclude that, for every \(\{i,j\}\),

\[\hat{\bm{K}}_{ij}=f(\bm{u}_{i})^{\top}\bm{G}(\bm{X})^{-1}f(\bm{u}_{j})\xrightarrow[ \xrightarrow[M\to\infty]{}f(\bm{u}_{i})^{\top}\bm{G}(f)^{-1}f(\bm{u}_{j})= \bm{K}_{ij}\] (78)\[\hat{\bm{H}}_{ij}=g(\bm{u}_{i})^{\top}\bm{G}(\bm{Y})^{-1}g(\bm{u}_{j})^{\top} \xrightarrow[M\to\infty]{}g(\bm{u}_{i})^{\top}\bm{G}(g)^{-1}g(\bm{u}_{j})=\bm{H} _{ij}.\] (79)

where we recognize the matrices \(\hat{\bm{K}}\) and \(\hat{\bm{H}}\) as \(M\bm{K}_{X}\) and \(M\bm{K}_{Y}\) in the main text.

Lastly, we would now like to study

\[\left|\frac{1}{M^{3}}\operatorname{Tr}\hat{\bm{K}}\bm{Q}\hat{\bm{H}}- \operatorname{Tr}\mathcal{K}\mathcal{Q}\mathcal{H}\right|\] (80)

taking note that the quantity \(\frac{1}{M^{3}}\operatorname{Tr}\hat{\bm{K}}\bm{Q}\hat{\bm{H}}\) is a scaled version of eq. (13) in the main text. The triangle inequality implies a relationship with the trace of the true Gram matrices:

\[\left|\frac{1}{M^{3}}\operatorname{Tr}\hat{\bm{K}}\bm{Q}\hat{\bm{H}}- \operatorname{Tr}\mathcal{K}\mathcal{Q}\mathcal{H}\right|\leq\left|\frac{1}{ M^{3}}\operatorname{Tr}\hat{\bm{K}}\bm{Q}\hat{\bm{H}}-\frac{1}{M^{3}} \operatorname{Tr}\bm{K}\bm{Q}\bm{H}\right|+\left|\frac{1}{M^{3}} \operatorname{Tr}\bm{K}\bm{Q}\bm{H}-\operatorname{Tr}\mathcal{K}\mathcal{Q} \mathcal{H}\right|.\] (81)

The first term on the right hand side can be bounded:

\[\left|\frac{1}{M^{3}}\operatorname{Tr}\hat{\bm{K}}\bm{Q}\hat{\bm{ H}}-\frac{1}{M^{3}}\operatorname{Tr}\bm{K}\bm{Q}\bm{H}\right|\leq \frac{1}{M^{3}}\sum_{ijk}\left|\hat{\bm{K}}_{ij}\bm{Q}_{jk}\hat{ \bm{H}}_{ki}-\bm{K}_{ij}\bm{Q}_{jk}\bm{H}_{ki}\right|\] \[= \frac{1}{M^{3}}\sum_{ijk}\left|\bm{Q}_{jk}\right|\left|\hat{\bm{K }}_{ij}\hat{\bm{H}}_{ki}-\bm{K}_{ij}\bm{H}_{ki}\right|\] \[= \frac{1}{M^{3}}\sum_{ijk}\left|\bm{Q}_{jk}\right|\left|\left(\hat {\bm{K}}_{ij}-\bm{K}_{ij}\right)\hat{\bm{H}}_{ki}+\bm{K}_{ij}\left(\hat{\bm{H} }_{ki}-\bm{H}_{ki}\right)\right|\] \[= \frac{1}{M^{3}}\sum_{ijk}\left|\bm{Q}_{jk}\right|\left(\left|\hat {\bm{K}}_{ij}-\bm{K}_{ij}\right|\left|\hat{\bm{H}}_{ki}\right|+\left|\bm{K}_{ ij}\right|\left|\hat{\bm{H}}_{ki}-\bm{H}_{ki}\right|\right).\]

Using the definitions of \(\hat{\bm{K}}_{ij}\), \(\bm{K}_{ij}\), \(\bm{H}_{ij}\), and \(\hat{\bm{H}}_{ij}\),

\[\left|\frac{1}{M^{3}}\operatorname{Tr}\hat{\bm{K}}\bm{Q}\hat{\bm{ H}}-\frac{1}{M^{3}}\operatorname{Tr}\bm{K}\bm{Q}\bm{H}\right|\leq \frac{1}{M^{3}}\|(\bm{G}(\bm{X})^{-1}-\bm{G}(f)^{-1})\|_{op}\sum_{ ijk}\left|\bm{Q}_{jk}\right|\left|\hat{\bm{H}}_{ki}\right|\|f(\bm{u}_{i}) \|\|f(\bm{u}_{j})\|\] \[+\frac{1}{M^{3}}\|(\bm{G}(\bm{Y})^{-1}-\bm{G}(g)^{-1})\|_{op}\sum_ {ijk}\left|\bm{Q}_{jk}\right|\left|\bm{K}_{ij}\right|\|g(\bm{u}_{i})\|\|g(\bm{u }_{j})\|.\]

Since \(\left|\bm{Q}_{jk}\right|\), \(\left|\hat{\bm{H}}_{ki}\right|\), and \(\left|\bm{K}_{ij}\right|\) are assumed to be finite for all \(i,j,k\in\{1,...,M\}\), and the norms of the neural responses \(\|f(\bm{u}_{i})\|\) and \(\|g(\bm{u}_{i})\|\) can be assumed to be bounded4 for all \(i\in\{1,...,M\}\), we can conclude by referencing eq. (77) that the right hand side approaches \(0\) as \(M\to\infty\). This argument can be made precise by considering the rate of convergence of \(\hat{\bm{K}}_{ij}\to\bm{K}_{ij}\) and \(\hat{\bm{H}}_{ij}\to\bm{H}_{ij}\) that is inherited from the concentration of \(\bm{G}(\bm{X})\to\bm{G}(f)\) and \(\bm{G}(\bm{Y})\to\bm{G}(g)\).

Footnote 4: Similar to the treatment in [28], this could be interpreted as a resource constraint on the neural responses.

The second term on the right hand side of eq. (81) is precisely the convergence of a sum over 'true' Gram matrices to the trace of a composition of integral operators described in eq. (67), so this term also approaches \(0\) as \(M\to\infty\).

Putting it all together

\[\frac{1}{M^{3}}\operatorname{Tr}\hat{\bm{K}}\bm{Q}\hat{\bm{H}} \xrightarrow[M\to\infty]{}\operatorname{Tr}[\mathcal{K}\mathcal{Q}\mathcal{H}] =\mathbb{E}_{\eta}[S].\] (82)where we can recognize the plug-in estimate as

\[\frac{1}{M^{3}}\operatorname{Tr}\hat{\bm{K}}\bm{Q}\hat{\bm{H}}=\frac{1}{M} \operatorname{Tr}\bm{K}_{X}\bm{K}_{z}\bm{K}_{Y}=\frac{1}{M}\mathbb{E}\langle\bm {X}\bm{w}^{*},\bm{Y}\bm{v}^{*}\rangle.\] (83)

in the notation used in the main text (compare with eq. (13)).