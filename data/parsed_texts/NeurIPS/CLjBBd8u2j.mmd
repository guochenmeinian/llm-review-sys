Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback

Jaskirat Singh\({}^{1}\)&Liang Zheng\({}^{1,2}\)

\({}^{1}\)The Australian National University

\({}^{2}\)Australian Centre for Robotic Vision

https://ljsingh.github.io/divide-evaluate-and-refine

###### Abstract

The field of text-conditioned image generation has made unparalleled progress with the recent advent of latent diffusion models. While remarkable, as the complexity of given text input increases, the state-of-the-art diffusion models may still fail in generating images which accurately convey the semantics of the given prompt. Furthermore, it has been observed that such misalignments are often left undetected by pretrained multi-modal models such as _CLIP_. To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of text-to-image alignment. In particular, we first introduce a _Decompositional-Alignment-Score_ which given a complex prompt decomposes it into a set of disjoint assertions. The alignment of each assertion with generated images is then measured using a VQA model. Finally, alignment scores for different assertions are combined aposteriori to give the final text-to-image alignment score. Experimental analysis reveals that the proposed alignment metric shows significantly higher correlation with human ratings as opposed to traditional _CLIP_, _BLIP_ scores. Furthermore, we also find that the assertion level alignment scores provide a useful feedback which can then be used in a simple iterative procedure to gradually increase the expression of different assertions in the final image outputs. Human user studies indicate that the proposed approach surpasses previous state-of-the-art by 8.7% in overall text-to-image alignment accuracy.

## 1 Introduction

The field of text-to-image generation has made significant advancements with the recent advent of large-scale language-image (LLI) models [1; 2; 3; 4; 5]. In particular, text-conditioned latent diffusion models have shown unparalleled success in generating creative imagery corresponding to a diverse range of free-form textual descriptions. However, while remarkable, it has been observed [6; 7; 8] that as the complexity of the input text increases, the generated images do not always accurately align with the semantic meaning of the textual prompt.

To facilitate the reliable use of current text-to-image generation models for practical applications, it is essential to answer two key questions: 1) Can we detect such fine-grain misalignments between the input text and the generated output in a robust manner? and 2) Once detected, can we improve the text-to-image alignment for failure cases? While several metrics for evaluating text-to-image alignment (_e.g._, CLIP [9], BLIP [10], BLIP2 [11]) exist, it has been observed [7; 12] that a high score with these metrics can be achieved even if the image does not fully correspond with input prompt. For instance, in Fig. 1, an output image (containing only pink trees) shows high CLIP/BLIP scores with the text "pink trees and yellow car" even if yellow car is not present. Evaluating text-to-image matching using the image-text-matching (ITM) head of BLIP models has also been recently explored [10; 11]. However, the generated scores also show a similar tendency to favor the main subject of input prompt. Furthermore, even if such misalignments are detected, it is not clear how such information can be used for improving the quality of generated image outputs in a reliable manner.

To address these problems, in this paper we explore a simple yet effective decompositional approach towards both evaluation and improvement of fine-grain text-to-image alignment. In particular, we propose a _Decompositional-Alignment-Score_ (DA-Score) which given a complex text prompt, first decomposes it into a set of disjoint assertions about the content of the prompt. The alignment of each of these assertions with the generated image is then measured using a VQA model [10; 13]. Finally, the alignment scores for diffferent assertions are combined to give an overall text-to-image alignment score. Our experiments reveal that the proposed evaluation score shows significantly higher correlation with human ratings over prior evaluation metrics (_e.g._, CLIP, BLIP2) (Sec. 4.1).

Furthermore, we also find that the assertion-level alignment scores provide a useful and explainable feedback for determining which parts of the input prompt are not being accurately described in the output image. We show that this feedback can then be used to gradually improve the alignment of the generated images with the input text prompt. To this end, we propose a simple iterative refinement procedure (Fig. 2), wherein at each iteration the expressivity of the least-aligned assertion is improved by increasing the weightage/cross-attention strength (refer Sec. 3.2) of corresponding prompt tokens during the reverse diffusion process. Through both qualitative and quantitative analysis, we find that the proposed iterative refinement process allows for generation of better aligned image outputs over prior works [6; 7; 8] while on average showing comparable inference times (Sec. 4.2).

## 2 Related Work

**Text to Image Generation Models.** Text conditional image synthesis is a topic of keen interest in the vision community. For instance, [14; 15; 16; 17; 18] use GANs to perform text guided image generation. Similarly, [5; 19] explore the use of autoregressive models for zero-shot text to image generation. Recently, diffusion-based-models [20; 21; 1; 3; 4; 5; 2; 21] have emerged as a powerful class of methods for performing text-conditional image synthesis over diverse range of target domains.

Figure 1: _Overview. Top:_ Traditional methods for evaluating text-to-image alignment _e.g._, CLIP [9], BLIP-2 [10] and BLIP2-ITM (which provides a binary image-text matching score between 0 and 1) often fail to distinguish between good (_right_) and bad (_left_) image outputs and can give high scores even if the generated image is not an accurate match for input prompt (missing yellow car). In contrast, by breaking down the prompt into a set of disjoint assertions and then evaluating their alignment with the generated image using a VQA model [10], the proposed Decompositional-Alignment Score (DA-score) shows much better correlation with human ratings (refer Sec. 4.1). _Bottom:_ Furthermore, we show that the assertion-level alignment scores can be used along with a simple iterative refinement strategy to reliably improve the alignment of generated image outputs (refer Sec. 4.2).

While remarkable, generating images which align perfectly with the input text-prompt remains a challenging problem [6, 7, 8, 22]. To enforce, heavier reliance of generated outputs on the provided text, classifier-free guidance methods [2, 3, 23] have been proposed. Similarly, use of an additional guidance input to improve controllability of text-to-image generation have recently been extensively explored [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]. However, even with their application, the generated images are often observed to exhibit fine-grain misalignments such as missing secondary objects [6, 7] with the input text prompt.

**Evaluating Image-Text Alignment**. Various protocols for evaluating text-image alignment in a reference-free manner have been proposed [9, 10, 11]. Most prior works [2, 3, 5, 9] typically use the cosine similarity between the text and image embedding from large-scale multi-modal models [9, 36, 37, 38] such as CLIP [9], BLIP [10], BLIP-2 [11] for evaluating the alignment scores. Recently, [10, 11] also show the application of BLIP/BLIP-2 models for image-text matching using image retrieval. However, as shown in Fig. 1, these scores can give very high scores even if the generated images do not full align with the input text prompt. Furthermore, unlike our approach image-text alignment is often represented through a single scalar value which does not provide an explainable measure which can be used to identify/improve weaknesses of the image generation process.

**Improving Image-Text Alignment.** Recently several works [6, 7, 8] have been proposed to explore the problem of improving image-text alignment in a training free manner. Liu _et al_. [6] propose to modify the reverse diffusion process by composing denoising vectors for different image components. However, it has been observed [7] that it struggles while generating photorealistic compositions of diverse objects. Feng _et al_. [8] use scene graphs to split the input sentence into several noun phrases and then assign a designed attention map to the output of the cross-attention operation. In another recent work, Cheter _et al_. [7] extend the idea of cross-attention map modification to minimize missing objects but instead do so by modifying the noise latents during the reverse diffusion process. While effective at reducing missing objects, we find that the performance / quality of output images can suffer as the number of subjects in the input prompt increases (refer Sec. 4.2).

Besides training-free methods, recent contemporary work [39, 40] has also explored the possibility of improving image-text alignment using human feedback to finetune existing latent diffusion models. However this often requires the collection of large-scale human evaluation scores and finetuning the diffusion model across a range of diverse data modalities which can be expensive. In contrast, we explore a training free approach for improvement of fine-grain text-to-image alignment.

## 3 Our Method

Given the image generation output \(\mathcal{I}\) corresponding to a text prompt \(\mathcal{P}\), we wish to develop a mechanism for evaluation and improvement of fine-grain text-to-image alignment. The core idea of our approach is to take a decompositional strategy for both these tasks. To this end, we first generate a set of disjoint assertions regarding the content of the input prompt. The alignment of the output image \(\mathcal{I}\) with each of these assertions is then calculated using a VQA model. Finally, we use the

Figure 2: _Iterative refinement (Col:1-3;4-6) for improving text-to-image alignment._ We propose a simple iterative refinement approach which uses the decompositional alignment scores (refer Sec. 3.1) as feedback to gradually improve the alignment of the generated images with the input text-prompt.

assertion-based-alignment scores as feedback to improve the expressiveness of the assertion with the least alignment score. This process can then be performed in an iterative manner to gradually improve the quality of generated outputs until a desired value for the overall alignment score is attained.

In the next sections, we discuss each of these steps in detail. In Sec. 3.1 we first discuss the process for evaluating decompositional-alignment scores. We then discuss the iterative refinement process for improving text-to-image alignment in Sec. 3.2. Fig. 3 provides an overview for the overall approach.

### Evaluating Text-to-Image Alignment

**Prompt Decomposition Model.** Given an input prompt \(\mathcal{P}\), we first decompose its textual information into a set of disjoint assertions (and corresponding questions) which exhaustively cover the contents of the input prompt. Instead of relying on human-inputs as in [6; 7]1, we leverage the in-context learning capability [41] of large-language models [42; 43] for predicting such decompositions in an autonomous manner. In particular, given an input prompt \(\mathcal{P}\) and large-language model \(\mathcal{M}\), the prompt decomposition is performed using in-context learning as,

Footnote 1: Prior works on improving image-text alignment often rely on human-user inputs for expressing contents of the input prompt into its simpler constituents. For instance, Feng [6] require the user to describe the prompt as a conjunction/disjunction of simpler statements. Similarly, Chefer [7] require the user to provide a set of entities / subjects in the prompt, over which their optimization should be performed.

\[\mathbf{x}=\{x_{0},x_{1},\dots x_{n}\}=\mathcal{M}(\mathbf{x}\mid\mathcal{P},D_{exempler},\mathcal{T}),\] (1)

where \(\mathbf{x}\) is the model output, \(n\) is the number of decompositions, \(D_{exempler}\) is the in-context learning dataset consisting 4-5 human generated examples for prompt decomposition, and \(\mathcal{T}\) is task description. Please refer supp. material for further details on exemplar-dataset and task-description design.

The model output \(\mathbf{x}\) is predicted to contain tuples \(x_{i}=\{a_{i},p_{i}\}\), where each tuple is formatted to contain assertions \(a_{i}\) and the sub-part \(p_{i}\) of the original prompt \(\mathcal{P}\) corresponding to the generated assertion. For instance, given \(\mathcal{P}:\)_a cat and a dog_'the prompt decomposition can be written as,

\[\mathcal{M}(\mathbf{x}\mid\mathcal{P}:\text{ {\em a cat and a dog}'},D_{exempler},\mathcal{T})=[\{\text{ `there is a cat'},\text{ `a cat''}\},\{\text{`there is a dog'},\text{`a dog'}\}]\,.\]

**Computing Assertion-based Alignment Scores.** We next compute the alignment of the generated image \(\mathcal{I}\) with each of the disjoint assertions using a Visual-Question-Answering (VQA) model [10]. In particular, given image \(\mathcal{I}\), assertions \(a_{i},i=1,\dots n\), their rephrasing in question format \(a_{i}^{q}\) and VQA-model \(\mathcal{V}\), the assertion-level alignment scores \(u_{i}(\mathcal{I},a_{i})\) are computed as,

\[u_{i}(\mathcal{I},a_{i})=\frac{\exp{(\alpha_{i}/\tau)}}{\exp{(\alpha_{i}/\tau )}+\exp{(\beta_{i}/\tau)}},\text{ {\em where}}\quad\alpha_{i}=\mathcal{V}(\text{`yes'}\mid\mathcal{I},a_{i}^{q}), \quad\beta_{i}=\mathcal{V}(\text{`no'}\mid\mathcal{I},a_{i}^{q}),\]

where \(\alpha_{i},\beta_{i}\) refer to the logit-scores of VQA-model \(\mathcal{V}\) for input tuple (image \(\mathcal{I}\), question \(a_{i}^{q}\)) corresponding to output tokens _'yes',no'_ respectively. Hyperparameter \(\tau\) controls the temperature of the softmax operation and controls the confidence of the alignment predictions.

Figure 3: _Method Overview._ Given a text prompt \(\mathcal{P}\) and an initially generated output \(\mathcal{I}_{0}\), we first generate a set of disjoint assertions \(a_{i}\) regarding the content of the caption. The alignment of the output image \(\mathcal{I}_{0}\) with each of these assertions is then calculated using a VQA model. Finally, we use the assertion-based-alignment scores \(u_{i}(\mathcal{I}_{0},\mathcal{P})\) as feedback to increase the weightage \(w_{i}\) (of the assertion with least alignment score) in a parameterized diffusion model formulation \(\mathcal{D}\) (Sec. 3.2). This process can then be performed in an iterative manner to gradually improve the quality of the generated outputs until a desirable threshold for the overall alignment score \(\Omega(\mathcal{I}_{k},\mathcal{P})\) is reached.

**Combining Alignment Scores.** Finally, the assertion level alignment-scores \(u_{i}(\mathcal{I},a_{i})\) are combined to give the overall text-to-image alignment score \(\Omega(\mathcal{I},\mathcal{P})\) between image \(\mathcal{I}\) and prompt \(\mathcal{P}\) as,

\[\Omega(\mathcal{I},\mathcal{P})=\frac{\sum_{i}\lambda_{i}(\mathcal{P},a_{i})\;u _{i}(\mathcal{I}_{k},a_{i})}{\sum_{i}\lambda_{i}(\mathcal{P},a_{i})},\] (2)

where weights \(\lambda_{i}(\mathcal{P},a_{i})\) refer to the importance of assertion \(a_{i}\) in capturing the overall content of the input prompt \(\mathcal{P}\), and allows the user to control the relative importance of different assertions in generating the final image output2. Please refer Fig. 3 for the overall implementation.

Footnote 2: For simplicity reasons, we mainly use \(\lambda_{i}=1\forall i\) in the main paper. Further analysis on variable \(\lambda_{i}\) to account for variable information content or visual verifiability of an assertion are provided in supp. material.

### Improving Text to Image Alignment

In addition to predicting overall text-to-image alignment score, we find that assertion-level alignment scores \(u_{i}(\mathcal{I},a_{i})\) also provide a useful and explainable way for determining which parts of the input prompt \(\mathcal{P}\) are not being accurately described in the output image \(\mathcal{I}\). This feedback can then be used in an iterative manner to improve the expressivity of the assertion with least alignment score \(u_{i}(\mathcal{I},q_{i})\), until a desired threshold for the overall text-image alignment score \(\Omega(\mathcal{I},\mathcal{P})\) is obtained.

**Parameterized Diffusion Model**. We first modify the image generation process of standard diffusion models in order to control the expressiveness of different assertions \(a_{i}\) in parametric manner. In particular, we modify the reverse diffusion process to also receive inputs weights \(w_{i}\), where each \(w_{i}\) controls the relative importance of assertion \(a_{i}\) during the image generation process. In this paper, we mainly consider the following two methods for obtaining such parametric control.

**Prompt Weighting.** Instead of computing the CLIP [36] features from original prompt \(\mathcal{P}\) we use prompt-weighting [44] to modify the input CLIP embeddings to the diffusion model as,

\[\text{CLIP}(\mathcal{P})=\mathcal{W}(\mathcal{P},\{\text{CLIP}(p_{i}),w_{i}\}_ {i=1}^{n}))\] (3)

where \(\mathcal{W}\) refers to the prompt-weighting function from [1, 44], \(p_{i}\) refers to the sub-prompt (Sec. 3.1) corresponding to assertion \(a_{i}\), and weights \(w_{i}\) control the relative weight of different sub-prompts \(p_{i}\) in computing the overall CLIP embedding for prompt \(\mathcal{P}\).

**Cross-Attention Control.** Similar to [7], we also explore the idea of modifying the noise latents \(z_{t}\) during the reverse diffusion process, to increase the cross-attention strength of the main noun

Figure 4: _Visualizing the prompt decomposition process._ By dividing a complex prompt \(\mathcal{P}\) into a set of disjoint assertions \(a_{i}\), we are able to identify the sub-prompts \(p_{i}\) (circled) which are not expressed in the image output using VQA, and thereby address them using iterative refinement (Sec. 3.2).

subject for each sub-assertion \(a_{i}\). However, instead of only applying the gradient update for the least dominant subject [7], we modify the loss for the latent update in parametric form as,

\[z_{t}=z_{t}-\alpha\nabla_{z_{t}}\mathcal{L}(z_{t},\{w_{i}\}_{i=1}^{n})),\text { where }\quad\mathcal{L}(z_{t},\{w_{i}\}_{i=1}^{n})=\sum_{i}w_{i}(1-\text{max }G(\mathcal{A}_{i}^{t})),\] (4)

where \(\alpha\) is the step-size, \(\mathcal{A}_{i}^{t}\) refer to the attention map corresponding to the main noun-subject in assertion \(a_{i}\), \(G\) is a smoothing function and weights \(w_{i}\) control the extent to which the expression of different noun-subjects in the prompt (for each assertion) will be increased in the next iteration.

**Iterative Refinement.** Given the above parametric formulation for controlling expression of different assertions, we next propose a simple yet effective iterative refinement approach towards improving text-to-image alignment. In particular, at any iteration \(k\in[1,5]\) during the refinement process, we first compute both overall text-image similarity score \(\Omega(\mathcal{I}_{k},\mathcal{P})\) and assertion-level alignment scores \(u_{i}(\mathcal{I}_{k},\mathcal{P})\). The image generation output \(\mathcal{I}_{k+1}\) for the next iteration is then computed as,

\[\mathcal{I}_{k+1}=\mathcal{D}(\mathcal{P},\{w_{i}^{k+1}\}_{i=1}^{n}));\text{ where }\quad w_{i}^{k+1}=\begin{cases}w_{i}^{k}+\Delta,\text{ if }\quad i=\text{ argmin}_{l}\;u_{l}(\mathcal{I},\mathcal{P})\\ w_{i}^{k}\quad\text{ otherwise }\end{cases},\] (5)

where \(\mathcal{D}\) refers to the parametrized diffusion model and \(\Delta\) is a hyper-parameter. This iterative process is then performed until a desirable threshold for the overall alignment score \(\Omega(\mathcal{I}_{k},\mathcal{P})\) is reached. The image generation output \(\mathcal{I}^{\star}\) at the end of the refinement process is then computed as,

\[\mathcal{I}^{\star}=\text{argmax}_{\mathcal{I}_{k}}\Omega(\mathcal{I}_{k}, \mathcal{P}).\] (6)

## 4 Experiments

**Dataset.** Since there are no openly available datasets addressing semantic challenges in text-based image generation with human annotations, we introduce a new benchmark dataset Decomposable-Captions-4k for method comparison. The dataset consists an overall of 24960 human annotations on images generated using all methods [1; 6; 7] (including ours) across a diverse set of 4160 input prompts. Each image is a given rating between 1 and 5 (where 1 represents that _'image is irrelevant to the prompt'_ and 5 represents that _'image is an accurate match for the prompt'_).

Furthermore, unlike prior works [7] which predominantly analyse the performance on relatively simple prompts with two subjects (_e.g._ object a and object b), we construct a systematically diverse pool of input prompts for better understanding text-to-image alignment across varying complexities in the text prompt. In particular, the prompts for the dataset are designed to encapsulate two axis of complexity: _number of subjects and realism_. The number of subjects refers to the number of main objects described in the input prompt and varies from 2 (_e.g._, _a cat with a ball_) to 5 (_e.g._, _a woman walking her dog on a leash by the beach during sunset_). Similarly, the _realism_ of a prompt is defined as the degree to which different concepts naturally co-occur together and varies as _easy_, _medium_, _hard_ and _very hard_. _easy_ typically refers to prompts where concepts are naturally co-occurring together (_e.g._, _a dog in a park_) while _very hard_ refers to prompts where concept combination is very rare (_e.g._, _a dog playing a piano_). Further details regarding the dataset are provided in supplementary material.

Figure 5: _Method comparisons w.r.t correlation with human ratings._ We compare the correlation of different text-to-image alignment scores with those obtained from human subjects, as the number of subjects in the input prompt (refer Sec. 4) is varied. We observe that the proposed alignment score (DA-score) provides a better match for human-ratings over traditional text-to-image alignment scores.

### Evaluating Text-to-Image Alignment

**Baselines.** We compare the performance of the _Decompositional-Alignment Score_ with prior works on evaluating text-to-image alignment in a reference-free manner. In particular, we show comparisons with CLIP [9], BLIP [10] and BLIP2 [11] scores where the text-to-image alignment score is computed using the cosine similarity between the corresponding image and text embeddings. We also include comparisons with BLIP-ITM and BLIP2-ITM which directly predict a binary image-text matching score (between 0 and 1) for input prompt and output image. Finally, we report results on the recently proposed text-to-text (T2T) similarity metric [7] which computes image-text similarity as the average cosine similarity between input prompt and captions generated (using BLIP) from the input image.

Figure 6: _Qualitative comparison w.r.t text-to-image alignment._ We compare the outputs of our iterative refinement approach with prior works [1, 6, 7, 8] on improving quality of generated images with changing number of subjects (underlined) from 2 to 5. Please zoom-in for best comparisons.

**Quantitative Results.** Fig. 5 shows the correlation between human annotations and predicted text-to-image alignment scores across different metrics on the _Decomposable-Captions_ dataset. We observe that the _DA-Score_ shows a significantly higher correlation with human evaluation ratings as opposed to prior works across varying number of subjects \(N\in[2,5]\) in the input prompt. We also note that

Figure 7: _Additional comparisons with Attend-and-Excite._ We analyse three main ways in which the proposed iterative-refinement improves over Attend-and-Excite [7] (refer Sec. 4.2 for details).

while the recently proposed T2T similarity score [7] shows comparable correlation with ours for \(N=2\), its performance significantly drops as the number of subjects in the input prompt increases.

### Improving Text-to-Image Alignment

In this section, we compare the performance of our iterative refinement approach with prior works on improving text-to-image alignment in a training-free manner. In particular, we show comparisons with 1) Stable Diffusion [1], 2) Composable Diffusion [6] 3) StructureDiffusion [8] and 4) Attend-and-Excite [7]. All images are generated using the same seed across all methods.

**Qualitative Results.** Results are shown in Fig. 6. As shown, we observe that Composable Diffusion [6] struggles to generate photorealistic combinations of objects especially as number of subjects in the prompt increase. StructureDiffusion [8] helps in addressing some missing objects _e.g_., telescope in example-1, but the generated images tend to be semantically similar to those produced by the original Stable Diffusion model, and thus does not significantly improve text-to-image alignment.

Attend-and-Excite [7] shows much better performance in addressing missing objects (_e.g_., telescope in example-1 and umbrella in example-4). However, as sumamrized in Fig. 7 we observe that it suffers from 3 main challenges: 1) _Object Relationship_ (Fig. 6(a)): we observe that despite having desired objects, generated images may sometimes fail to convey relationship between them. For _e.g_., in row-1 Fig. 7 while output images show both a _lion and guitar_, the lion does not seem to be playing the guitar. In contrast, Eval-and-Refine is able to describe both presence and relation between objects in a better manner. 2) _Overlapping Entities_ (Fig. 6(b)): For images with overlapping entities (_e.g_., _person and spacesuit_), we observe that Attend-and-Excite [7] typically spends most of gradient updates balancing between the overlapping entities, as both entities (_person and spacesuit_) occupy the same cross-attention region. This can lead to outputs where a) other important aspects (_e.g_., _lake_ in Col-3) or b) one of the two entities (_e.g_., _spacesuit_) are ignored. 3) _Prompt Complexity_ (Fig. 6(c)): Finally, we note that since Attend-and-Excite [7] is limited to applying the cross-attention update _w.r.t_ the least dominant subject, as the complexity of input prompt \(\mathcal{P}\) increases, it may miss some objects (_e.g_., _umbrella, beach, sunny day_) during the generation process. In contrast, the iterative nature of our approach allows it to keep refining the output image \(\mathcal{I}\) until a desirable threshold for the overall image-text alignment score \(\Omega(\mathcal{I},\mathcal{P})\) is reached.

**Quantitative Results.** In addition to qualitative experiments, we also evaluate the efficacy of our approach using human evaluations. In this regard, we report three metrics: 1) _normalized human score_: which refers to the average human rating (normalized between 0-1) for images generated on the Decomposable-Captions-4k dataset. 2) _accuracy_: indicating the percentage of generated images which are considered as an accurate match (rating: 5) for the input text prompt by a human subject. 3) _pairwise-preference_: where human subjects are shown pair of images generated using our method and prior work, and are supposed to classify each image-pair as a win, loss or tie (win meaning our method is preferred). For our approach we consider two variants 1) _Ours (PW)_ which performs iterative refinement using only prompt-weighting, and 2) _Ours (PW + CA)_ where iterative refinement is performed using both prompt weighting and introducing cross-attention updates (Sec. 3.2). Pairwise preference scores are reported while using _Ours (PW + CA)_ while comparing with prior works.

Figure 8: _Variation of alignment accuracy with prompt difficulty._ We observe that while the accuracy of all methods decreases with increasing difficulty in prompt _realism_ (refer Sec. 4), the proposed iterative refinement approach consistently performs better than prior works.

Results are shown in Fig. 8 and Tab. 1. We observe that while the text-to-image alignment accuracy for all methods decreases with an increased difficulty in input text prompts (Fig. 8), we find that the our approach with only prompt-weighting is able to consistently perform on-par or better than Attend-and-Excite [7]. Further introduction of cross-attention updates (Sec. 3.2), allows our approach to exhibit even better performance, which outperforms Attend-and-Excite [7] by 8.67 % in terms of overall alignment accuracy of the generated images. These improvements are also reflected in the pairwise comparisons where human subjects tend to prefer our approach over prior works [6, 7, 8].

**Inference time comparison.** Tab. 1 shows comparison for the average inference time (per image) for our approach with prior works [6, 7, 8]. We observe that despite the use of an iterative process for our approach, the overall inference time is comparable with prior works. This occurs because prior works themselves often include additional steps. For instance, Composable-Diffusion [6] requires the computation of separate denoising latents for each statement in the conjunction/disjunction operation, thereby increasing the overall inference time almost linearly with number of subjects. Similarly, Attend-and-Excite [7] includes additional gradient descent steps for modifying cross-attention maps. Moreover, such an increase is accumulated even if the baseline Stable-Diffusion [1] model already generates accurate images. In contrast, the proposed iterative refinement approach is able to adaptively adjust the number of iterations required for the generation process by monitoring the proposed _DA-Score_ for evaluating whether the generation outputs are already good enough.

## 5 Conclusion

In this paper, we explore a simple yet effective decompositional approach for both evaluation and improvement of text-to-image alignment with latent diffusion models. To this end, we first propose a _Decompositional-Alignment Score_ which given a complex prompt breaks it down into a set of disjoint assertions. The alignment of each of these assertions with the generated image is then measured using a VQA model. The assertion-based alignment scores are finally combined to a give an overall text-to-image alignment score. Experimental results show that proposed metric shows significantly higher correlation with human subject ratings over traditional CLIP, BLIP based image-text matching scores. Finally, we propose a simple iterative refinement approach which uses the decompositional-alignment scores as feedback to gradually improve the quality of the generated images. Despite its simplicity, we find that the proposed approach is able to surpass previous state-of-the-art on text-to-image alignment accuracy while on average using only marginally higher inference times. We hope that our research can open new avenues for robust deployment of text-to-image models for practical applications.

\begin{table}
\begin{tabular}{l|c|c|c c c|c} \hline \hline \multirow{2}{*}{Method} & Norm. Human & Alignment & \multicolumn{3}{c|}{Pairwise Comparison \%} & Inference \\  & Score (\%) & Accuracy (\%) & Win \(\uparrow\) & Tie & Lose \(\downarrow\) & Time (s) \\ \hline Stable-Diffusion [1] & 72.98 & 43.66 & 41.7 & 50.1 & 8.2 & 3.54 s \\ Composable-Diffusion [6] & 70.28 & 37.72 & 57.1 & 38.5 & 4.4 & 10.89 s \\ Structure-Diffusion [8] & 74.93 & 45.23 & 37.5 & 54.6 & 7.9 & 11.51 s \\ Attend-and-Excite [7] & 85.94 & 65.50 & 23.6 & 62.3 & 14.1 & 8.59 s \\ \hline Ours (PW) & 89.53 & 70.28 & N/A & N/A & N/A & 10.32 s \\ Ours (PW + CA) & **90.25** & **74.16** & N/A & N/A & N/A & 12.24 s \\ \hline \hline \end{tabular}
\end{table}
Table 1: _Quantitative Results_. We report text-to-image alignment comparisons _w.r.t_ normalized human rating score (Col:2), average alignment accuracy evaluated by human subjects (Col:3) and pairwise user-preference scores (ours vs prior work) (Col:4-6). Finally, we also report average inference time per image for different methods in Col:7. We observe that our approach shows better text-to-image alignment performance while on average using marginally higher inference time.

## References

* [1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2021.
* [2] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. _arXiv preprint arXiv:2112.10741_, 2021.
* [3] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. _arXiv preprint arXiv:2205.11487_, 2022.
* [4] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. _arXiv preprint arXiv:2204.06125_, 2022.
* [5] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. _arXiv preprint arXiv:2206.10789_, 2022.
* [6] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII_, pages 423-439. Springer, 2022.
* [7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. _arXiv preprint arXiv:2301.13826_, 2023.
* [8] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. _arXiv preprint arXiv:2212.05032_, 2022.
* [9] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. _arXiv preprint arXiv:2104.08718_, 2021.
* [10] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In _International Conference on Machine Learning_, pages 12888-12900. PMLR, 2022.
* [11] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. _arXiv preprint arXiv:2301.12597_, 2023.
* [12] Roni Paiss, Hila Chefer, and Lior Wolf. No token left behind: Explainability-aided image classification and generation. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XVII_, pages 334-350. Springer, 2022.
* [13] Wonjae Kim, Bokyung Son, and Iddo Kim. Vilt: Vision-and-language transformer without convolution or region supervision, 2021.
* [14] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1316-1324, 2018.
* [15] Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. Df-gan: A simple and effective baseline for text-to-image synthesis. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16515-16525, 2022.
* [16] Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. Improving text-to-image synthesis using contrastive learning. _arXiv preprint arXiv:2107.02423_, 2021.
* [17] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. Cross-modal contrastive learning for text-to-image generation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 833-842, 2021.
* [18] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 5802-5810, 2019.

* [19] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.
* [20] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. _arXiv preprint arXiv:2211.01324_, 2022.
* [21] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion model in generative ai: A survey. _arXiv preprint arXiv:2303.07909_, 2023.
* [22] Rosanne Liu, Dan Garrette, Chitwan Saharia, William Chan, Adam Roberts, Sharan Narang, Irina Blok, RJ Mical, Mohammad Norouzi, and Noah Constant. Character-aware models improve visual text rendering. _arXiv preprint arXiv:2212.10562_, 2022.
* [23] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. _arXiv preprint arXiv:2207.12598_, 2022.
* [24] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In _International Conference on Learning Representations_, 2022.
* [25] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune Gwon, and Sungroh Yoon. Ilvr: Conditioning method for denoising diffusion probabilistic models. _arXiv preprint arXiv:2108.02938_, 2021.
* [26] Jaskirat Singh, Stephen Gould, and Liang Zheng. High-fidelity guided image synthesis with latent diffusion models. _arXiv preprint arXiv:2211.17084_, 2022.
* [27] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. _arXiv preprint arXiv:2211.14305_, 2022.
* [28] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: Fusing diffusion paths for controlled image generation. _arXiv preprint arXiv:2302.08113_, 2, 2023.
* [29] Vishnu Sarukkai, Linden Li, Arden Ma, Christopher Re, and Kayvon Fatahalian. Collage diffusion. _arXiv preprint arXiv:2303.00262_, 2023.
* [30] Yu Zeng, Zhe Lin, Jianming Zhang, Qing Liu, John Collomosse, Jason Kuen, and Vishal M Patel. Scenecomposer: Any-level semantic image synthesis. _arXiv preprint arXiv:2211.11742_, 2022.
* [31] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. _arXiv preprint arXiv:2211.15518_, 2022.
* [32] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. _arXiv preprint arXiv:2302.05543_, 2023.
* [33] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. _arXiv preprint arXiv:2302.08453_, 2023.
* [34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. _arXiv preprint arXiv:2208.12242_, 2022.
* [35] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. _arXiv preprint arXiv:2208.01618_, 2022.
* [36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* [37] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. _arXiv preprint arXiv:2205.01917_, 2022.

* [38] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. _Advances in neural information processing systems_, 34:9694-9705, 2021.
* [39] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. _arXiv preprint arXiv:2302.12192_, 2023.
* [40] Xiaoshi Wu, Keqiang Sun, Feng Zhu, Rui Zhao, and Hongsheng Li. Better aligning text-to-image models with human preference, 2023.
* [41] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [42] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [43] Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al. Summary of chatgpt/gpt-4 research and perspective towards the future of large language models. _arXiv preprint arXiv:2304.01852_, 2023.
* [44] AUTOMATIC1111. Stable-diffusion-webui. https://github.com/AUTOMATIC1111/stable-diffusion-webui, 2022.