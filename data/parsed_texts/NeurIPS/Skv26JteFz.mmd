# Optimal Hypothesis Selection in (Almost) Linear Time

 Maryam Aliakbarpour

Department of Computer Science

Rice University

Houston, TX 77005

maryama@rice.edu

&Mark Bun

Department of Computer Science

Boston University

Boston, MA 02215

mbun@bu.edu

&Adam Smith

Department of Computer Science

Boston University

Boston, MA 02215

ads22@bu.edu

###### Abstract

Hypothesis selection, also known as density estimation, is a fundamental problem in statistics and learning theory. Given a sample set from an unknown distribution \(P\) and a finite class of candidate distributions (hypotheses) \(\mathcal{H}\coloneqq H_{1},H_{2},\ldots,H_{n}\), the goal is to design an algorithm that selects a distribution \(\hat{H}\) from \(\mathcal{H}\) that best describes \(P\). The accuracy of the algorithm is measured by the distance between \(\hat{H}\) and \(P\), compared to the distance between the closest distribution in \(\mathcal{H}\) and \(P\) (denoted by \(\mathsf{OPT}\)). Specifically, we aim for \(\|\hat{H}-P\|_{\mathrm{TV}}\) to be at most \(\alpha\cdot\mathsf{OPT}+\epsilon\) for some small \(\epsilon\) and \(\alpha\).

While the value of \(\epsilon\) can be reduced with an increasing number of samples, \(\alpha\) is an inherent characteristic of the algorithm. Achieving \(\alpha<3\) is impossible, even with only two candidate hypotheses, unless the number of samples is proportional to the domain size of \(P\)[Bousquet, Kane, Moran '19]. Finding a computationally efficient algorithm that achieves the optimal \(\alpha\) has been a primary focus of research since the early work of [Devroye, Lugosi '01]. Before our work, the algorithms achieving \(\alpha<5\) required time \(\Omega(n^{2})\). We present the first algorithm that operates in almost linear time (\(\tilde{O}(n/\epsilon^{3})\)) and achieves \(\alpha=3\). This result improves upon a long line of hypothesis selection research. Previously known algorithms had either worse time complexity, a larger \(\alpha\) factor, or additional assumptions about the problem setting. Additionally, we provide another (almost) linear-time algorithm with better dependency on the additive accuracy parameter \(\epsilon\), albeit with a slightly worse accuracy parameter of \(\alpha=4\).

## 1 Introduction

Hypothesis selection, also known as density estimation, is a fundamental problem in statistics and learning theory. This problem involves identifying a density function that accurately represents the distribution of a given dataset. Suppose we are given a dataset of samples drawn from an unknown distribution \(P\) and a finite class of known distributions, representing different hypotheses: \(\mathcal{H}\coloneqq\{H_{1},H_{2},\ldots,H_{n}\}\). The goal is to select a distribution in \(\mathcal{H}\) that is close to \(P\) in total variation (TV) distance.

Typically, learning a distribution over a domain \(\mathcal{X}\) with an \(\epsilon\) error in TV distance requires \(\Omega(|\mathcal{X}|/\epsilon^{2})\) samples, which presents a substantial lower bound in sample complexity for distributions over largedomains. Surprisingly, the findings of Yatracos, Devroye, and Lugosi revealed that for hypothesis selection, this sample complexity can be independent of the domain size and only logarithmic in the number of hypotheses [22, 14, 15]. With just \(s\coloneqq\Theta(\log n/\epsilon^{2})\) samples from \(P\), it is possible to learn \(P\) within error \(\alpha\cdot\mathsf{OPT}+\epsilon\), where \(\mathsf{OPT}\) denotes the distance of the nearest distribution in \(\mathcal{H}\) to \(P\). Specifically, Devroye and Lugosi introduced two algorithms for this problem: the Scheffe tournament, which operates in \(O(n^{2}\cdot s)\) time1 and achieves \(\alpha=9\); and the minimum distance estimate, which runs in \(O(n^{3}\cdot s)\) time and achieves \(\alpha=3\)[15, Chapter 6]. In these results, although \(\epsilon\) can be decreased as the number of samples increases, \(\alpha\) remains an inherent parameter of the algorithm.

Footnote 1: This time bound assumes constant-time comparisons of the \(H_{i}\)â€™s density functions. See Section 1.1.

Significant effort has been directed towards finding computationally efficient algorithms for this problem while maintaining sample efficiency. The trade-off between the accuracy parameter \(\alpha\) and computational efficiency has been a focal point. Mahalanabis and Stefankovic in [16] enhanced the minimum distance estimate, improving the time complexity to \(O(n^{2}\cdot s)\). They also introduced a nearly linear-time algorithm that achieves \(\alpha=3\), but requires exponential time in \(n\) for preprocessing the class \(\mathcal{H}\). Other nearly linear-time algorithms were developed achieving \(\alpha=9\) in [1, 2]\(\mathrm{AFJ}^{+}18\), \(\mathrm{AAC}^{+}23\)2 and \(\alpha=5\) in [1]. Furthermore, linear-time algorithms have been presented under the assumption that the algorithm receives the value of \(\mathsf{OPT}\) (or its upper bound) as input [1, 2]. However, achieving \(\alpha<3\) is not possible unless the number of samples becomes \(\mathrm{poly}(|\mathcal{X}|)\), as indicated in [1].

Footnote 2: Theorem 4.1 in [2] states the result for \(\alpha=27\). However, according to personal correspondence with the authors, it is possible to modify their algorithm in conjunction with the minimum distance estimate and improve \(\alpha\) to 9.

Despite the long-standing history of this problem, the following question remained open:

_Is there an algorithm for hypothesis selection with the optimal number of samples \(s=O(\log(n)/\epsilon^{2})\) and optimal accuracy parameter \(\alpha=3\) that runs in \(O(n\cdot s)\) time?_

We present the first almost linear-time algorithm that uses the optimal number of samples and achieves the optimal accuracy parameter \(\alpha=3\). Our algorithm runs in \(\tilde{O}(n\cdot s\ /\ \epsilon)\) time (see Theorem 2). Additionally, we introduce another algorithm with improved dependency on \(\epsilon\), running in \(\tilde{O}(n\cdot s)\) time while obtaining a slightly higher accuracy parameter, \(\alpha=4\) (see Theorem 3). Our results represent a significant step forward, as they are the first in decades to achieve time complexity linear in \(n\) for any \(\alpha<5\). See Table 1.

Applications of Hypothesis SelectionThe primary application of hypothesis selection is to identify the best distribution from a set of known models that represent potential underlying data distributions we can effectively manage. For example, this set might include Poisson, gamma, and binomial distributions with various parameters used to model the number of arrivals per time unit. This makes hypothesis selection applicable to tasks like interpretable distribution selection and strategy optimization, where the objective is to choose the most suitable model from available options.

Another key strength of hypothesis selection is its agnostic nature, allowing it to adapt even when the true distribution lies outside the considered class. This robustness makes it effective in noisy data settings, with applications in denoising and anomaly detection. From a theoretical standpoint, hypothesis selection is fundamental in learning structured distributions, particularly when combined with the cover method, as seen in learning mixtures of Gaussians [1, 1]. For additional references, see Section 1.3.

Importance of improving \(\boldsymbol{\alpha}\) by a constant factorIn most learning algorithms, the error guarantee decreases polynomially as the number of samples increases, so constant factors may not be as crucial. However, this is not the case in hypothesis selection. The output hypothesis is guaranteed to be \((\alpha\cdot\mathsf{OPT}+\epsilon)\)-close to \(P\) in total variation distance. While increasing the number of samples can reduce \(\epsilon\) to negligible levels, it does not improve the term \(\alpha\cdot\mathsf{OPT}\). Hence, \(\alpha\) is an inherent property of the algorithm and directly impacts the best achievable error guarantee. Therefore, even a constant improvement in \(\alpha\) is significant.

One might argue that, alternatively, \(\mathsf{OPT}\) could be reduced by carefully curating the class \(\mathcal{H}\). However, beyond the practical challenges of finding a better \(\mathcal{H}\), it is unclear whether this can be achievedwithout significantly increasing the size of the hypothesis class \(\mathcal{H}\). For example, in the cover method, when aiming to learn a distribution within a class \(\mathcal{C}\), we set \(\mathcal{H}\) to be a \(\gamma\)-net that serves as a cover for \(\mathcal{C}\), ensuring that \(\mathsf{OPT}<\gamma\). While using a finer \(\gamma\)-net can reduce \(\mathsf{OPT}\), it may also drastically increase the size of \(\mathcal{H}\), thereby increasing the algorithm's running time, since the size of the net can grow super-polynomially with respect to \(\gamma\). For instance, in the case of mixtures of \(k\) Gaussians, the size of the net depends on \(\gamma\) as roughly \(O(\gamma^{-3\cdot k})\) (see [1]). Thus, reducing \(\gamma\) by a factor of three could exponentially increase the size of \(\mathcal{H}\) in terms of \(k\), thereby increasing both the running time and sample complexity, ultimately resulting in an inefficient algorithm.

### Problem Setup

Suppose we have an _unknown_ distribution \(P\) over a domain \(\mathcal{X}\) and a set of \(n\)_known_ distributions \(\mathcal{H}\coloneqq\{H_{1},H_{2},\ldots,H_{n}\}\). Let \(\mathsf{OPT}\) denote the distance between \(P\) and the closest distribution to it in \(\mathcal{H}\)3: \(\mathsf{OPT}(\mathcal{H},P)\coloneqq\min_{H\in\mathcal{H}}\left\|H-P\right\| _{\text{TV}}.\) We use the standard access model for this problem as introduced in [1]. The algorithm accesses the distributions through the following types of queries:

Footnote 3: We omit the arguments \((\mathcal{H},P)\) when they are clear from the context.

1. The algorithm can request a sample from the unknown distribution \(P\).
2. The algorithm can compare the PDF of two known distributions: For every domain element \(x\in\mathcal{X}\) and every pair of indices \(i\) and \(j\), it can ask if \(H_{j}(x)<H_{i}(x)\). This is equivalent to asking if \(x\) is in the Scheffe set of \(H_{i}\) and \(H_{j}\) (defined in Equation (2)).
3. The algorithm can query the probability mass of the Scheffe sets according to all the known distributions.

**Remark 1**.: _The last requirement of our model can be relaxed. Only estimates of the probability masses of the Scheffe sets are needed for our algorithms. Thus, one can alternatively assume sample access to \(H_{i}\)'s, and estimate these values via samples._

**Definition 1.1** (Proper learner for hypothesis selection).: _Suppose algorithm \(\mathcal{A}\) is given parameters \(\epsilon,\delta\in(0,1)\), \(\alpha\in\mathbb{R}_{>0}\) and has access to an unknown distribution \(P\) and a class of \(n\) known distributions \(\mathcal{H}\) (as described above). We say \(\mathcal{A}\) is an \((\alpha,\epsilon,\delta)\)-proper learner for the hypothesis selection problem if for every \(P\) and \(\mathcal{H}\), \(\mathcal{A}\) outputs \(\hat{H}\in\mathcal{H}\) for which, with probability at least \(1-\delta\),_

\[\|\hat{H}-P\|_{\text{TV}}\leq\alpha\cdot\mathsf{OPT}(\mathcal{H},P)+\epsilon\,.\] (1)

\begin{table}
\begin{tabular}{l l l} \hline \hline
**Result** & \(\alpha\) & **Time Complexity** & **Additional requirement** \\ \hline \hline Min distance estimate [1] & 3 & \(O(n^{3}\cdot s)\) \\ \hline Scheffe tournament [1] & 9 & \(O(n^{2}\cdot s)\) \\ \hline Min distance estimate [13] & 3 & \(O(n^{2}\cdot s)\) \\ \hline \hline AJOS14, AFJ\({}^{+}\)18] & 9 & \(\tilde{O}(n\cdot s)\) \\ \hline ABS23] & 5 & \(\tilde{O}(n\cdot s)\) \\ \hline AAC\({}^{+}\)23] & 9 & \(O(n\cdot s/\log n)\) \\ \hline \hline MS08] & 3 & \(O(n\cdot s)\) & Exponential time preprocessing \\ \hline DK14, ABS23] & \(\geq 3\) & \(\tilde{O}(n\cdot s)\) & Assume knowledge of \(\mathsf{OPT}\) \\ \hline Lower bound [1] & Achieving \(\alpha<3\) requires \(\operatorname{poly}(|\mathcal{X}|)\) samples \\ \hline \hline This work:Algorithm 1 & 3 & \(\tilde{O}(n\cdot s/\,\epsilon)\) \\ \hline This work: Algorithm 4 & 4 & \(\tilde{O}\left(n\cdot s\right)\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Summary of Past Results in Hypothesis Selection. All algorithms use \(s=\Theta(\log n/\epsilon^{2})\) samples.

_We refer to \(\alpha\) as the accuracy parameter, \(\epsilon\) as the error (or proximity) parameter, and \(\delta\) as the confidence parameter of the algorithm._

### Main theorems

Below, we provide informal versions of our theorems.

**Theorem 2**.: _For every \(\epsilon,\delta\in(0,1)\), Algorithm 1 is an \((\alpha=3,\epsilon,\delta)\)-proper learner for hypothesis selection that uses \(s=O(\log(n/\delta)/\epsilon^{2})\) samples and time \(\tilde{O}(n\cdot s\ /\ (\delta^{3}\epsilon))=\tilde{O}(n/(\delta^{3} \epsilon^{3}))\)._

**Theorem 3**.: _For every \(\epsilon,\delta\in(0,1)\), Algorithm 4 is an \((\alpha=4,\epsilon,\delta)\)-proper learner for hypothesis selection that uses \(s=O(\log(n/\delta)/\epsilon^{2})\) samples and time \(\tilde{O}(n\cdot s\cdot\log(1/\delta))=\tilde{O}(n\cdot\log^{2}(1/\delta)/ \epsilon^{2}))\)._

For formal statements, see Theorem 5 (Appendix A) and Corollary B.1 (Appendix B).

Our results are the first to achieve time complexity linear in \(n\) for any \(\alpha<5\). To achieve this, we introduce novel algorithmic techniques that will hopefully be broadly useful--see Section 3 for an overview. Both algorithms use the optimal number of samples. The first algorithm obtains optimal accuracy parameter \(\alpha=3\) with a time complexity of \(\tilde{O}(n/(\delta^{3}\,\epsilon)\cdot s)\), which is off by an \(O(1/(\delta^{3}\,\epsilon))\) factor. Our second algorithm achieves the optimal time complexity up to logarithmic factors while achieving a slightly higher \(\alpha=4\). Our results leave a fascinating open question: Can one combine the best aspects of both algorithms, maintaining \(\alpha=3\), achieving \(O(n\cdot s)\) time (or even lower), and sample complexity \(s=O(\log(n/\delta)/\epsilon^{2})\)?

**Remark 4**.: _Readers may be surprised by the polynomial dependence on \(1/\delta\) in Theorem 2. In many settings, the success probability of a learning algorithm can be amplified from a constant, say 2/3, to at least \(1-\delta\) at a cost of at most \(\log(1/\delta)\) in running time and sample complexity. However, in hypothesis selection, there is no (known) general technique for boosting the confidence parameter while keeping \(\alpha\) the same. The issue is that choosing the best output from several runs of a given algorithm requires executing a second hypothesis selection algorithm, which introduces another factor of \(\alpha\) in the approximation--leading to a total factor of at least \(9\). As a result, these kinds of two-phase algorithms are not sufficient in the low \(\alpha\) regime. Some previous results, such as [1], also suffer from a polynomial dependency on \(\delta\)._

### Other related work

Hypothesis selection has been studied in various settings including improper setting. In [1], the authors consider the improper version of the problem where the output hypothesis \(\hat{H}\) may not necessarily be in \(\mathcal{H}\). They presented an improper learner with accuracy guarantee \(\alpha=2\). The sample complexity of their algorithm was improved by [1], who gave an algorithm with nearly optimal sample complexity and the same accuracy parameter \(\alpha=2\). It is worth noting that our algorithms are proper learners and solve this problem with a slightly better sample complexity. In addition, like other proper learners, our algorithms select their output from a predefined set, which can facilitate choosing a distribution with specific structural property (e.g., mixture of Gaussians). In certain applications, this selection ensures consistency with the problem's underlying assumptions, which enhances interpretability and robustness.

The problem of hypothesis selection in _sublinear time_ was studied for distributions on _discrete domains_[1]. Among other results, the authors developed a data structure that upon receiving samples from a unknown distribution \(P\) returns a hypothesis \(\hat{H}\) in \(o(n)\) time. While their algorithm runs in sublinear time, their sample complexity depends on the domain size of the distribution, and their setting allows pre-processing of the class \(\mathcal{H}\) in polynomial time. Another interesting variation for hypothesis selection is designing differentially private learners for the problem which has received attention over the past few years [1, 2].

An important application of hypothesis selection arises when there is a structural assumption on the underlying distributions. One approach for learning these classes is to selectively choose a _cover_ for the class. One can then use the learners for the standard hypothesis selection problem (which we study in the paper) and use the cover as the class \(\mathcal{H}\). Examples of such structural assumptions include Poisson binomial distributions [12], mixtures of Gaussians [13, 14, 15, 16, 17, 18, 19], distributions with piecewise polynomial PDFs [1], and histograms [15, 16, 17]. See Diakonikolas [11] for a survey of results.

Preliminaries

Notation:We use the following notation throughout this article. We use \([n]\) to indicate the set \(\{1,2,\ldots,n\}\). For a distribution \(Q\) over \(\mathcal{X}\), \(Q(x)\) denotes the PDF of \(Q\) at the domain element \(x\in\mathcal{X}\). For any measurable subset of the domain \(S\subseteq\mathcal{X}\), \(Q(S)\) indicates the probability mass of the set \(S\) according to \(Q\). We use \(\|Q_{1}-Q_{2}\|_{\mathrm{TV}}\coloneqq\sup_{S\subseteq\mathcal{X}}|Q_{1}(S)-Q_ {2}(S)|\) to denote the total variation distance between \(Q_{1}\) and \(Q_{2}\). We say \(Q_{1}\) is \(\epsilon\)-close to \(Q_{2}\) if \(\|Q_{1}-Q_{2}\|_{\mathrm{TV}}\) is at most \(\epsilon\). Conversely, we say \(Q_{1}\) is \(\epsilon\)-far from \(Q_{2}\) if \(\|Q_{1}-Q_{2}\|_{\mathrm{TV}}\) is greater than \(\epsilon\). We use the standard \(O,\Omega,\Theta\) notation for asymptotic functions. Additionally, we use \(\tilde{O}\), \(\tilde{\Omega}\), and \(\tilde{\Theta}\) to hide \(\mathrm{polylog}\) factors.

Scheffe sets:For every pair of hypotheses \(H_{i}\) and \(H_{j}\) in \(\mathcal{H}\), we define the Scheffe set of \(H_{i}\) and \(H_{j}\) as follows:

\[\mathcal{S}_{i,j}\coloneqq\left\{\begin{array}{ll}\{x\in\mathcal{X}\mid H_{ i}(x)<H_{j}(x)\}&\quad\text{if $i\leq j$}\\ \mathcal{S}_{j,i}&\quad\text{if $i>j$}\end{array}\right.\] (2)

It is known that the Scheffe set maximizes the probability discrepancy between \(H_{i}\) and \(H_{j}\), thus fully characterizing the total variation distance between the two distributions:

\[\|H_{i}-H_{j}\|_{\mathrm{TV}}=\sup_{S\subseteq\mathcal{X}}|H_{i}(S)-H_{j}(S) |=|H_{i}\left(\mathcal{S}_{i,j}\right)-H_{j}\left(\mathcal{S}_{i,j}\right)|\.\] (3)

The optimal hypothesis:Recall that we assume the algorithm is given samples drawn from an unknown distribution \(P\). Let \(H_{i^{*}}\) denote the closest hypothesis in \(\mathcal{H}\) to \(P\). That is, \(H_{i^{*}}\) is the hypothesis for which \(\|H_{i^{*}}-P\|_{\mathrm{TV}}=\mathsf{OPT}\). When there is more than one hypothesis with this property, we pick one arbitrarily as \(H_{i^{*}}\).

Semi-distances:For every pair \(i,j\) in \([n]\), we define \(w_{j}(H_{i})\) to be \(|H_{i}\left(\mathcal{S}_{i,j}\right)-P\left(\mathcal{S}_{i,j}\right)|\). In words, \(w_{j}(H_{i})\) is the distance of \(H_{i}\) to \(P\) observed on the Scheffe set of \(H_{i}\) and \(H_{j}\). For every pair \(i,j\in[n]\), we use \(\hat{w}_{j}(H_{i})\) to denote an estimate of \(w_{j}(H_{i})\) based on the observed sample. For the sake of consistency, we define \(w_{i}(H_{i})\) to be zero. In addition, we define the score function \(W(H_{i})\coloneqq\max_{j\in[n]}w_{j}(H_{i})\). Similarly, \(\hat{W}(H_{i})\) is defined to be \(\max_{j\in[n]}\hat{w}_{j}(H_{i})\).

Refined access model:Similar to previous work [11, 12], we use estimates of the semi-distances. One can easily estimate these quantities, denoted by \(\hat{w}_{j}(H_{i})\), via the access model we have described earlier by letting \(\hat{w}_{j}(H_{i})\) be the empirical ratio of the samples that are in \(\mathcal{S}_{i,j}\). Throughout this paper, we assume that there are two universal parameters \(\delta^{\prime}=\Theta(\delta)\) and \(\epsilon^{\prime}=\Theta(\epsilon)\), for which with probability \(1-\delta^{\prime}\) every \(\hat{w}_{j}(H_{i})\) is within \(\epsilon^{\prime}\) of \(w_{j}(H_{i})\):

\[\forall i,j\in[n]:\quad|\hat{w}_{j}(H_{i})-w_{j}(H_{i})|\leq\epsilon^{\prime}.\]

A simple application of the Hoeffding bound and the union bound shows that one can compute all of the estimates via \(s=O(\log(n/\delta^{\prime})/{\epsilon^{\prime}}^{2})\) samples, and each estimate can be computed in \(O(s)\) time.

Our algorithms access the distributions in \(\mathcal{H}\cup\{P\}\) only via querying \(\hat{w}_{j}(H_{i})\). This fact brings the sample complexity of our algorithms to \(s=O(\log(n/\delta^{\prime})/{\epsilon^{\prime}}^{2})\) samples. The time complexity of our algorithms is determined by the number of queries they make to the \(\hat{w}_{j}(H_{i})\)'s, multiplied by time that we spend on each query, \(O(s)\). Moreover, in the proofs of our theorems, we assume without loss of generality that all the \(\hat{w}_{j}(H_{i})\)'s are accurate. Conditioning on the accuracy will not decrease the probability of correctness of any of our algorithms by more than \(\delta^{\prime}\) due to Fact C.1.

## 3 Overview of our techniques

In this section, we provide a high-level discussion of our algorithm. The important notations and definitions used here are provided in Section 2. For the high-level discussions in this section, we assume we have access to the exact values of the semi-distances \(w_{j}(H_{i})\). In the formal proofs presented in subsequent sections, we will substitute each \(w_{j}(H_{i})\) with an estimated value \(\hat{w}_{j}(H_{i})\)If the error of these estimates is below \(\epsilon^{\prime}=\Theta(\epsilon)\), it can be shown that the overall impact of this substitution on our final distance guarantee (Equation 1) is at most \(\Theta(\epsilon)\). See the "Refined access model" in Section 2 for further details.

### Background: Semi-distances and the minimum distance estimate

To solve the hypothesis selection problem, we seek a _certificate_ that ensures we output a hypothesis \(\hat{H}\) such that \(\|\hat{H}-P\|_{\text{TV}}\) is at most \(3\)\(\mathsf{OPT}\). Similar to previous work, our algorithms operate based on the probability masses of the _Scheffe sets_ (Equation (2)) of pairs of hypotheses in \(\mathcal{H}\). The semi-distance \(w_{j}(H_{i})\), defined as \(|H_{i}\left(\mathcal{S}_{i,j}\right)-P\left(\mathcal{S}_{i,j}\right)|\), captures the "distance" between \(H_{i}\) and \(P\) as measured on this particular set \(\mathcal{S}_{i,j}\). One suggestion for readers to internalize the semi-distances is to view them as a distance between \(H_{i}\) and \(P\) that is measured from the perspective of \(H_{j}\). By definition, \(w_{j}(H_{i})\) is always a lower bound for \(\|H_{i}-P\|_{\text{TV}}\):

\[w_{j}(H_{i})\coloneqq|H_{i}\left(\mathcal{S}_{i,j}\right)-P\left(\mathcal{S}_ {i,j}\right)|\leq\sup_{S\subseteq\mathcal{X}}|H_{i}(S)-P(S)|=\|H_{i}-P\|_{ \text{TV}}.\]

However, it is possible for \(w_{j}(H_{i})\) to be much lower, making it difficult for an algorithm to estimate the TV distance between \(H_{i}\) and \(P\) just using semi-distances.

Nevertheless, for each hypothesis \(H_{i}\), a specific semi-distance \(w_{i^{*}}(H_{i})\), associated with the optimal hypothesis \(H_{i^{*}}\), determines its quality. As shown in the following proof, if \(w_{i^{*}}(H_{i})\leq\mathsf{OPT}\), then \(H_{i}\) is \(3\), \(\mathsf{OPT}\)-close to \(P\), with the total variation distance between \(H_{i}\) and \(P\) bounded by \(w_{i^{*}}(H_{i})\) and \(\mathsf{OPT}\) via the triangle inequality:

\[\|H_{i}-P\|_{\text{TV}} \leq\|H_{i}-H_{i^{*}}\|_{\text{TV}}+\|H_{i^{*}}-P\|_{\text{TV}}= |H_{i}\left(\mathcal{S}_{i,j}\right)-H_{j}\left(\mathcal{S}_{i,j}\right)|+ \mathsf{OPT}\] (By Eq. 3 ) \[\leq w_{i^{*}}(H_{i})+w_{i}(H_{i^{*}})+\mathsf{OPT}\leq w_{i^{*}} (H_{i})+2\,\mathsf{OPT}.\]

This observation implies that if we assert that \(w_{i^{*}}(H_{i})\) is bounded by \(\mathsf{OPT}\), we can output \(H_{i}\) as our final solution to the problem and be done. The challenge is that we neither know \(i^{*}\) nor \(\mathsf{OPT}\).

This issue is addressed by the _minimum distance estimate_ presented in [11, 12] using a score function \(W(H_{i})\), as defined earlier: \(W(H_{i})\coloneqq\max_{j\in[n]}w_{j}(H_{i})\). The minimum distance estimate outputs a hypothesis \(\hat{H}\) that minimizes \(W(H_{i})\). We can assert that, for the output of this algorithm, \(\hat{H}\), \(w_{i^{*}}(\hat{H})\) is at most \(\mathsf{OPT}\). This approach simultaneously bypasses the issues of not knowing \(i^{*}\) and \(\mathsf{OPT}\).

Note that \(W(H_{i})\) serves as a proxy for the quality of \(H_{i}\) since \(W(H_{i})\) is an upper bound for \(w_{i^{*}}(H_{i})\). Using \(W(H_{i})\), we can address the issue of not knowing \(i^{*}\). On the other hand, although \(\mathsf{OPT}\) is not known, we have a good lower bound for \(\mathsf{OPT}\), which is \(W(H_{i^{*}})\). Putting these observations together, we obtain:

\[\|\hat{H}-P\|_{\text{TV}}\leq w_{i^{*}}(\hat{H})+2\,\mathsf{OPT}\leq W(\hat{H })+2\,\mathsf{OPT}\leq W(H_{i^{*}})+2\,\mathsf{OPT}\leq 3\,\mathsf{OPT}.\]

These inequalities are derived using the triangle inequality and the fact that \(\hat{H}\) was chosen to be the \(\arg\min_{i\in[n]}W(H_{i})\). See Figure 1 for an illustration of the above inequality.

The primary hurdle with the minimum distance estimate is that it is costly to compute. Computing each \(W(H_{i})\) takes \(O(n\cdot s)\) time, bringing the total time complexity of the algorithm to \(O(n^{2}\cdot s)\). One might naturally conjecture that sampling may help to compute an estimate of \(W(H_{i})\). Instead of using \(W(H_{i})\coloneqq\max_{j\in[n]}w_{j}(H_{i})\), we can use \(\tilde{W}(H_{i})\coloneqq\max_{j\in R}w_{j}(H_{i})\) where \(R\) is a set of random indices in \([n]\). The issue with this approach is that there is no guarantee of \(i^{*}\) being selected in \(R\), making \(\tilde{W}(H_{i})\) too low while \(H_{i}\) may be far from \(P\). Hence sampling, if used in a trivial manner, is not beneficial.

### The algorithm with \(\alpha=3\)

In this section, we present an overview of Algorithm 1 that attains \(\alpha=3\). The details of this algorithm and its related theorems are provided in Section A. At a high level, similar to the minimum distance estimate, we work towards finding a hypothesis that minimizes \(W(H_{i})\). To increase efficiency, we work with estimates of \(W(H_{i})\)'s, denoted by \(\tilde{W}(H_{i})\). The general structure of our algorithm is as follows: Initially, all \(\tilde{W}(H_{i})\) are set to zero. At every step, we come up with several pairs of hypotheses \(H_{i}\) and \(H_{j}\) and update our estimates by setting \(\tilde{W}(H_{j})\) to \(\max(\tilde{W}(H_{j}),w_{i}(H_{j}))\). Our approach ensures that at every step, \(\tilde{W}(H_{i})\) is equal to \(\max_{j\in R}w_{j}(H_{i})\) for a small, carefully chosen set \(R\). Eventually, we output a hypothesis with roughly the smallest \(\tilde{W}(H_{i})\).

Focusing on small \(\tilde{W}\):Our first idea is to focus on updating the \(\tilde{W}\) for hypotheses whose \(\tilde{W}(H_{i})\) values are around the current minimum \(\tilde{W}\). The rationale for this action comes from a simple fact: \(\tilde{W}(H_{j})\) always underestimates the value of \(W(H_{j})\). Hence, if we observe that \(\tilde{W}(H_{i})\) is substantially larger than the current minimum, then \(W(H_{i})\) is also substantially larger than the current minimum. This implies that \(H_{i}\) is not a suitable candidate for the minimum at this stage of the algorithm, and it can be ignored for now.

Bucketing hypotheses based on \(\tilde{W}\):To implement this idea, we partition the hypotheses into \(k=\Theta(1/\epsilon^{\prime})\) buckets \(\mathcal{B}\coloneqq\{B_{1},B_{2},\ldots,B_{k}\}\) based on \(\tilde{W}(H_{i})\). The bucket \(\ell\) contains all the hypotheses \(H_{i}\) such that \(\tilde{W}(H_{i})\in[(\ell-1)\epsilon^{\prime},\ell\,\epsilon^{\prime})\). At every step, we focus on the smallest non-empty bucket \(B_{\ell}\) (the smallest \(\ell\) for which \(|B_{\ell}|\neq 0\)). \(B_{\ell}\) contains all the hypotheses whose \(\tilde{W}(H_{j})\) is around the minimum \(\tilde{W}\). We pick pairs of hypotheses, \(H_{i}\in\mathcal{H}\) and \(H_{j}\in B_{\ell}\), and update \(\tilde{W}(H_{j})\). Note that our updates may increase \(\tilde{W}(H_{j})\), and we may remove \(H_{j}\) from \(B_{\ell}\) and put it into a larger bucket (a bucket with a larger index \(\ell\)). Also, observe that we never move a hypothesis into a smaller bucket since \(\tilde{W}(H_{j})\) never decreases. We continue our updates to reach one of the following outcomes:

* \(B_{\ell}\) becomes empty. That is, our updates made all \(\tilde{W}(H_{i})\) fall out of the range of these buckets \([0,\ell\,\epsilon^{\prime})\). Thus, every \(\tilde{W}(H_{i})\) (and consequently every \(W(H_{i})\)) is at least \(\ell\cdot\epsilon^{\prime}\). Every time that we empty out a bucket, we have increased our threshold for minimum \(W(H_{i})\) by \(\epsilon^{\prime}\). Hence, we are getting closer to a bucket with the true minimum, which we hope to reach in \(O(1/\epsilon^{\prime})\) steps.
* \(B_{\ell}\) is not empty, but we can confidently confirm most of the hypotheses in \(B_{\ell}\) are an acceptable output for the algorithm. Although we cannot ensure that \(H_{i^{*}}\) is indeed in \(B_{\ell}\), we can find an acceptable final answer by selecting a random hypothesis in \(B_{\ell}\).

Which pairs to update?Next, we outline our update scheme to implement the above ideas in linear time. To enhance time efficiency, our aim is to optimize the updating process to ensure both _quality_ and _quantity_ in the chosen updates. Quality, in this context, relates to the extent of change in \(\tilde{W}(H_{j})\) following an update: We consider \(H_{i}\) to have made a _substantial update_ to \(H_{j}\) if \(\tilde{W}(H_{j})+\epsilon^{\prime}<\hat{w}_{j}(H_{i})\). Such updates cause a significant shift, increasing the value of \(\tilde{W}(H_{j})\) by more than \(\epsilon^{\prime}\) and subsequently moving \(H_{j}\) to a different bucket with a higher index \(\ell\). We refer to this event as \(H_{i}\)_removing_\(H_{j}\) from its bucket. The quantity, on the other hand, relates to the number of \(H_{j}\) instances that a given \(H_{i}\) can remove from \(B_{\ell}\). An ideal \(H_{i}\) eliminates a substantial portion of hypotheses from \(B_{\ell}\) (say a constant fraction). We call such an \(H_{i}\) a _prompting hypothesis_. Now, if for \(O(\log|B_{\ell}|)\) rounds we find a prompting hypothesis and update all the \(\tilde{W}(H_{j})\) for \(H_{j}\in B_{\ell}\), we will empty out the bucket \(B_{\ell}\), and we can move forward.

Finding a prompting hypothesis:To find a prompting hypothesis quickly, we iterate over all \(H_{i}\) in \(\mathcal{H}\), sample a few \(H_{j}\), and check if \(H_{i}\) substantially updates \(\tilde{W}(H_{j})\). If \(H_{i}\) substantially improves a large fraction of the sampled hypotheses, then we declare that \(H_{i}\) is a prompting hypothesis. See Section A.1.1 for further details. In addition to that, we provide a more advanced version of this procedure in Section A.1.2 that allows us to shave off an \(O(\log n)\) factor.

Getting stuck? Here is your way out:What happens when \(B_{\ell}\) is not empty, and we cannot find a prompting hypothesis? We show that if we do not find a prompting hypothesis, then a random hypothesis in \(B_{\ell}\) is an acceptable answer.

The surprising part about this statement is that it holds regardless of the size of the bucket \(B_{\ell}\) due to hypothesis sampling procedure we have to find a prompting hypothesis. In search for a prompting hypothesis, we iterate over all \(H_{i}\) and sample roughly \(O(\log n/\delta)\) many hypotheses \(H_{j}\)'s in \(B_{\ell}\). We check, if \(H_{i}\) can remove them from the bucket. Note that if \(H_{i}\) was not found to be prompting, it implies that \(H_{i^{*}}\) cannot substantially update most of the hypotheses in \(B_{\ell}\). Thus, We have that with high probability for \(1-\delta\) fraction of the hypotheses in \(B_{\ell}\), \(w_{i}(H_{j})\leq\epsilon^{\prime}\cdot\ell\).

The clever hack here is an observation about \(H_{i^{*}}\). Earlier, we discussed that we are looking for a hypothesis \(H_{i}\) with \(w_{i^{*}}(H_{i})\leq\mathsf{OPT}\). We claim that a random hypothesis in the last \(B_{\ell}\) (almost) has this property. On one hand, given that \(H_{i^{*}}\) was not found to be a prompting hypothesis, for \(1-\delta\) fraction of \(H_{i}\) in \(B_{\ell}\), \(w_{i^{*}}(H_{i})\) must be at most \(\ell\cdot\epsilon^{\prime}\). On the other hand, the fact that all the previous buckets, \(B_{1},\ldots,B_{\ell-1}\), are empty indicates \(H_{i^{*}}\) has shown a semi-distance of at least \((\ell-1)\,\epsilon^{\prime}\). Hence, \(\mathsf{OPT}\), which is at least as large as all \(H_{i^{*}}\) semi-distances, is at least \((\ell-1)\,\epsilon^{\prime}\). Therefore, \(w_{i^{*}}(H_{i})\leq\epsilon^{\prime}+\mathsf{OPT}\). Therefore, for \(1-\delta\) fraction of the hypotheses in \(B_{\ell}\), they are \(3\,\mathsf{OPT}+\Theta(\epsilon)\) close to \(P\). For a formal argument, see Lemma A.1.

Now, assume the search for the prompting hypothesis fails. Recall that during the search, we have checked every single hypothesis in \(\mathcal{H}\). During the search, at some point, we must have stumbled upon \(H_{i^{*}}\) and did not declare it as a prompting hypothesis. In this case, either our sampling for substantial updates failed (which happens with small probability), or we can infer that there are not too many far hypotheses in \(B_{\ell}\). Either way, we can output a random hypothesis from \(B_{\ell}\) as the final sample without increasing the error probability by too much.

Dependency on \(\delta\):It is worth noting that this last step results in a polynomial dependency on \(\delta\) (as opposed to a more desirable dependency of \(\log(1/\delta)\)). This is mainly due to the fact that to ensure that a random hypothesis in \(B_{\ell}\) is not too far, with a probability of \(1-\delta\) in a single round, we have to try \(O(\log(n)/\delta)\) hypotheses in \(B_{\ell}\) and check if \(H_{i^{*}}\) is prompting them. Hence, relying on this structural property of \(H_{i^{*}}\) makes a polynomial dependency on \(\delta\) inevitable. Improving the dependency on \(\delta\) for this algorithm would require new algorithmic ideas.

### The algorithm with \(\alpha=4\)

We introduce another (almost) linear-time algorithm for the hypothesis selection problem, where the time complexity shows improved dependency on \(\epsilon\). However, this algorithm has a slightly worse accuracy parameter compared to our previous algorithm: \(\alpha=4\). This algorithm and the subsequent theorems are provided in Section B.

Algorithm with a guessed upper bound of \(\mathsf{OPT}\):As mentioned earlier, unlike previous work, our algorithm does not receive any prior information about the value of \(\mathsf{OPT}\). It might be speculated that there exists an easy reduction between our problem and another version of hypothesis selection, where an auxiliary parameter \(\sigma\) is provided to the algorithm such that \(\mathsf{OPT}\leq\sigma\). A learner without the knowledge of \(\sigma\) can make a guess about \(\sigma\) and run one of the existing algorithms that works with the knowledge of an upper bound of \(\mathsf{OPT}\) (e.g., [1]) and check if it finds a good hypothesis or not. With this procedure in mind, we can run a binary search to find the smallest \(\sigma\) for which we find a hypothesis.4 However, a challenge with this approach is that the algorithm might yield a hypothesis even when \(\sigma<\mathsf{OPT}\), making it difficult for us to _refute_ the hypothesis that is found. Even if the output \(\hat{H}\) satisfies \(W(\hat{H})>\sigma\), it does not necessarily invalidate our guessed value \(\sigma\).

Footnote 4: This idea works to some extent, but it causes \(\alpha\) to be increased by 2, which is not useful in our setting. See [1] for more details.

To overcome this challenge, we design an algorithm, namely \(\mathcal{A}\), with enhanced accuracy guarantees for the output hypothesis. We provide \(\mathcal{A}\) with the value \(\sigma\). We treat \(\sigma\) as a "guess" for which we hope that \(\mathsf{OPT}\leq\sigma\). We require the algorithm to either refute our guess and declare \(\mathsf{OPT}>\sigma\), or output a hypothesis \(\hat{H}\) with a reasonable distance to \(P\), irrespective of the relationship between \(\mathsf{OPT}\) and \(\sigma\). More precisely, the distance of the output hypothesis should be bounded by: \(\|\hat{H}-P\|_{\mathsf{TV}}\leq\alpha\cdot\max(\mathsf{OPT},\sigma)+\epsilon\). With these revised guarantees, it is permissible to run a binary search over different values of \(\sigma\) in \(\{\epsilon,2\,\epsilon,\dots,\lceil 1/\epsilon\rceil\;\epsilon\}\), and output the hypothesis that \(\mathcal{A}\) returns on the smallest \(\sigma\). For the rest of this discussion, we focus on designing \(\mathcal{A}\) and a provided parameter \(\sigma\).

Seeds and finding an acceptable hypothesis via seeds:In this algorithm, we introduce a new concept called a _seed_ for hypotheses. A seed provides us with a structural property that enables us to identify an acceptable hypothesis, regardless of the relationship between \(H_{i^{*}}\) and the seed. More formally, for a hypothesis \(H_{i}\), we define \(\mathcal{M}_{a,b}(H_{i})\) as the set of hypotheses \(H_{j}\) such that \(w_{j}(H_{i})\) is between \(a\) and \(b\):

\[\mathcal{M}_{a,b}(H_{i})\coloneqq\{H_{j}\in\mathcal{H}\mid a<\hat{w}_{j}(H_{i} )\leq b\}\;.\]

We may omit the subscript of \(\mathcal{M}\) when it is clear from the context. We say a hypothesis \(H_{i}\) is a seed if almost all of its \(\hat{w}_{j}(H_{i})\) are low and its \(W(H_{i})\) is not too large. The quality of a seed is determined by three parameters: \(a\), \(b\), and \(m\).

**Definition 3.1** (Seed).: _Given parameters \(a,b\in\mathbb{R}_{\geq 0}\), and a non-negative integer \(m\), we say a hypothesis is an \((a,b,m)\)-seed if the following hold: 1) \(\hat{W}(H_{i})\leq b\), and 2) \(|\mathcal{M}_{a,b}(H_{i})|\leq m\)._

A seed with a small-sized set \(\mathcal{M}\) and a small \(b\) exhibits an interesting property: Suppose we have a seed \(H_{i}\) with a constant-sized set \(\mathcal{M}\). In this case, in \(O(n)\) time, we can compute \(W(H_{j})\) of every hypothesis \(H_{j}\) in \(\mathcal{M}\). Now, let \(H_{\ell}\in\mathcal{M}\) be one of the hypotheses that minimize \(W(H_{j})\): \(H_{\ell}\coloneqq\arg\min_{H_{j}\in\mathcal{M}}W(H_{j})\). We claim either \(H_{i}\) or \(H_{\ell}\) is acceptable output in this case.

When \(H_{i^{*}}\) is in \(\mathcal{H}\setminus\mathcal{M}\), we can infer that \(w_{i^{*}}(H_{i})\) is at most \(b\) (or, more precisely, \(b+\epsilon^{\prime}\)). From our earlier discussion, if \(b\) is sufficiently small (compared to \(\sigma\)), we can show that \(H_{i}\) is close to \(P\). In cases where \(H_{i^{*}}\) is in \(\mathcal{M}\), although we do not have strong guarantees for \(H_{i}\) itself, we have a very good answer already: \(H_{\ell}\) is essentially a minimum distance estimate for the set \(\mathcal{M}\) which includes \(H_{i^{*}}\). Thus, we can conclude: First, \(H_{\ell}\) is an acceptable output because it is \(3\,\mathsf{OPT}\)-close; second, \(W(H_{\ell})\) is a lower bound for \(\mathsf{OPT}\).

Now, as we described, we have two acceptable outputs for two cases of the problem. For \(H_{i^{*}}\in\mathcal{H}\setminus\mathcal{M}\), \(H_{i}\) is an acceptable answer. For \(H_{i^{*}}\in\mathcal{M}\), \(H_{\ell}\) is an acceptable answer. The only problem is that we do not know which case we are in. Our strategy is to pick a hypothesis that is still reasonably close to \(P\) even if we are in the other case. Let us fix a threshold value \(T\). We compare \(W(H_{\ell})\) with \(T\sigma\): 1) If \(W(H_{\ell})\leq T\cdot\sigma\), as we have discussed earlier, \(H_{\ell}\) is \((T\cdot\sigma+2\mathsf{OPT})\)-close to \(P\). This bound holds regardless of where \(H_{i^{*}}\) is. 2) Next, if \(W(H_{\ell})>T\cdot\sigma\), we cannot say \(H_{\ell}\) is a good choice for us. However, we can conclude that \(H_{i}\) is a close hypothesis to \(P\) even when \(H_{i^{*}}\) happens to be in \(\mathcal{M}\). In this case, we know that \(\mathsf{OPT}\geq W(H_{\ell})>T\cdot\sigma\). We take advantage of this knowledge and obtain the following bound:

\[\|H_{i^{*}}-P\|_{\mathsf{TV}}=b\,\sigma+2\,\mathsf{OPT}\leq\left(\frac{b}{T}+ 2\right)\mathsf{OPT}\;.\]

Thus, regardless of where \(H_{i^{*}}\) is, \(H_{i}\) is \((\max(a,b/T)+2)\cdot\max\left(\sigma,\mathsf{OPT}\right)\). Now, the algorithm is fairly straightforward. If \(W(H_{\ell})\) is small, output \(H_{\ell}\); otherwise, output \(H_{i}\). We summarize these four cases in the following table. Depending on \(a\) and \(b\), we set \(T\) to minimize the maximum distance we endure in these cases. It is worth noting that in this argument, we did not rely on any prior knowledge concerning the relationship between \(\sigma\) and \(\mathsf{OPT}\).

How to find the first seed?Here, we provide an overview of our approach for identifying an initial seed. For a detailed explanation of the algorithm and its performance, refer to Section B.1.

To start, fix two parameters, \(a=\sigma\) and \(b=3\sigma\). To identify a seed hypothesis, we iterate over all hypotheses \(H_{1},\ldots,H_{n}\), checking whether each hypothesis \(H_{i}\) is a strong seed (i.e., a seed with a small \(|\mathcal{M}|\)) by sampling several \(H_{j}\)'s and verifying if \(w_{j}(H_{i})\) is at most \(a\). Roughly speaking, for an integer \(m\in[n]\), if we sample approximately \(\tilde{O}(n/m)\)\(H_{j}\)'s and find that no \(w_{j}(H_{i})>a\), we can, with high probability, confirm that the size of \(\mathcal{M}\) does not exceed \(m\). This approach requires \(O(n^{2}/m)\) time.

There are, however, a few caveats with this method. First, if a seed is identified using the above approach, it may have a large \(W(H_{i})>b\). In this case, we can infer that \(H_{i}\) is likely far from \(P\). Given the time invested in identifying \(H_{i}\), we aim to leverage this information. Broadly, observing that \(w_{j}(H_{i})\leq a\) implies that many hypotheses are close to \(H_{i}\) (assuming that \(w_{i}(H_{j})\) values are also small). Knowing that \(H_{i}\) is far from \(P\), we can _mark_ all hypotheses close to \(H_{i}\) as also far, allowing us to proceed with the search. While this may seem counterproductive, marking a significant number of hypotheses as "far" constitutes progress for our algorithm. Specifically, if, at some point, all hypotheses are marked, we can declare that \(\sigma<\mathsf{OPT}\).

The second caveat is more challenging. Ideally, we seek a seed with a constant \(m\), but finding such a seed would require \(\tilde{O}(n^{2}/m)=\tilde{O}(n^{2})\) time. Consequently, for a linear-time algorithm, we can only afford to find seeds where \(m=\Theta(n)\). In other words, the quality of the seed we can initially identify is much lower than the quality required for a producing solution. This leads to our next idea: boosting a seed, which is an approach to incrementally improve the seed's quality in roughly \(O(\log n)\) steps.

Boosting a Seed:In this process, we use an initial seed to iteratively find a stronger seed with a reduced value of \(|\mathcal{M}|\). For a formal argument, see Section B.2. Assume a rate parameter \(\eta\in(0,1)\). As discussed earlier, by spending \(O(n/\eta)\) time, we can identify a seed with \(m\approx\eta\cdot n\). Initially, \(m=|\mathcal{M}|\) might be \(O(n)\). Hence, rather than calculating \(W(H_{j})\) exactly for all \(H_{j}\in\mathcal{M}\), we compute an approximate maximum semi-distance \(\tilde{W}(H_{j})\) by sampling \(t\) hypotheses. Specifically, for each \(H_{j}\in\mathcal{M}\), we set \(\tilde{W}(H_{j})\coloneqq\max_{H_{k}}\hat{w}_{k}(H_{j})\), where \(H_{k}\)'s are sample hypotheses. Using these approximate values \(\tilde{W}\), we process the seed as follows. Let \(H_{\ell}\) denote the hypothesis minimizing \(\tilde{W}(H_{j})\), with the following possible outcomes:

1. High \(\tilde{W}(H_{\ell})\): If \(\tilde{W}(H_{\ell})\) is high, all \(\tilde{W}(H_{j})\) (and thus \(W(H_{j})\)) values are likely large, making \(H_{i}\) an acceptable final solution.
2. Low \(\tilde{W}(H_{\ell})\): While this does not guarantee a low \(W(H_{\ell})\), it suggests that most \(w_{i}(H_{\ell})\) values are small. Based on the value of \(W(H_{\ell})\), we consider two cases: 1. \(W(H_{\ell})\gg\tilde{W}(H_{\ell})\): \(H_{\ell}\) is likely far from \(P\) but has many close hypotheses, allowing us to mark these nearby hypotheses as far. 2. Moderate \(W(H_{\ell})\): In this case, \(H_{\ell}\) can serve as our new seed, as it yields a smaller set \(\mathcal{M}(H_{\ell})\) than \(H_{i}\).

Note that when we select \(H_{\ell}\) as our new seed, \(\mathcal{M}(H_{\ell})\) is roughly \(O(n/t)\). However, we compute \(\tilde{W}(H_{j})\)'s for only \(|\mathcal{M}(H_{i})|\approx\eta\cdot n\) many hypotheses. Hence, with an increased sample size \(t=O(1/\eta^{2})\), this approach yields a smaller \(|\mathcal{M}(H_{\ell})|\approx\eta^{2}\cdot n\). The step of the process requires \(O(|\mathcal{M}|\cdot t)=m/\eta^{2}=O(n/\eta)\), paralleling the initial search. By repeating, we progressively reduce \(m=|\mathcal{M}|\), and for a constant \(\eta\), \(m\) decreases by a fixed factor until reaching a constant \(m\) seed, as desired.

\begin{table}
\begin{tabular}{c|c c} \hline \hline  & \(H_{i^{*}}\in\mathcal{M}\) & \(H_{i^{*}}\in\mathcal{H}\setminus\mathcal{M}\) \\ \hline \(W(H_{\ell})>T\cdot\sigma\) & \(\|H_{i}-P\|_{\mathsf{TV}}\leq a\cdot\sigma+2\,\mathsf{OPT}\) & \(\|H_{i}-P\|_{\mathsf{TV}}\leq(b/T+2)\mathsf{OPT}\) \\ \hline \(W(H_{\ell})>T\cdot\sigma\) & \(\|H_{\ell}-P\|_{\mathsf{TV}}\leq T\cdot\sigma+2\,\mathsf{OPT}\) & \(\|H_{\ell}-P\|_{\mathsf{TV}}\leq T\cdot\sigma+2\,\mathsf{OPT}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Four cases when we process a good seed.

## Acknowledgments

M.A. was supported by NSF awards CNS-2120667, CNS-2120603, CCF-1934846, and BU's Hariri Institute for Computing. This work was initiated while M.A. was affiliated with Boston University and Northeastern University and was done in part while M.A. was a research fellow at the Simons Institute for the Theory of Computing. M.B. was supported in part by NSF award CNS-2046425 and a Sloan Research Fellowship. A.S. was supported in part by NSF awards CCF-1763786 and CNS-2120667 as well as Faculty Awards from Google and Apple.

## References

* [AAC\({}^{+}\)23] Anders Aamand, Alexandr Andoni, Justin Y. Chen, Piotr Indyk, Shyam Narayanan, and Sandeep Silwal. Data structures for density estimation. In _Proceedings of the 40th International Conference on Machine Learning_. PMLR, 2023.
* [ABH\({}^{+}\)20] Hassan Ashtiani, Shai Ben-David, Nicholas J. A. Harvey, Christopher Liaw, Abbas Mehrabian, and Yaniv Plan. Near-optimal sample complexity bounds for robust learning of gaussian mixtures via compression schemes. _J. ACM_, 67(6):32:1-32:42, 2020.
* [ABM18] Hassan Ashtiani, Shai Ben-David, and Abbas Mehrabian. Sample-efficient learning of mixtures. In Sheila A. McIlraith and Kilian Q. Weinberger, editors, _Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence_, pages 2679-2686. AAAI Press, 2018.
* [ABS23] Maryam Aliakbarpour, Mark Bun, and Adam Smith. Hypothesis selection with memory constraints. _To appear in NeurIPS 2023_, 2023.
* [ADLS17] Jayadev Acharya, Ilias Diakonikolas, Jerry Li, and Ludwig Schmidt. Sample-optimal density estimation in nearly-linear time. In _Proceedings of the Twenty-Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA_, pages 1278-1289. SIAM, 2017.
* [AFJ\({}^{+}\)18] Jayadev Acharya, Moein Falahatgar, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Maximum selection and sorting with adversarial comparators. _The Journal of Machine Learning Research_, 19:59:1-59:31, 2018.
* [AJOS14] Jayadev Acharya, Ashkan Jafarpour, Alon Orlitsky, and Ananda Theertha Suresh. Sorting with adversarial comparators and application to density estimation. In _2014 IEEE International Symposium on Information Theory_, pages 1682-1686, 2014.
* [BBK\({}^{+}\)21] Olivier Bousquet, Mark Braverman, Gillat Kol, Klim Efremenko, and Shay Moran. Statistically near-optimal hypothesis selection. In _62nd IEEE Annual Symposium on Foundations of Computer Science, FOCS 2021, Denver, CO, USA, February 7-10, 2022_, pages 909-919. IEEE, 2021.
* [BKM19] Olivier Bousquet, Daniel Kane, and Shay Moran. The optimal approximation factor in density estimation. In Alina Beygelzimer and Daniel Hsu, editors, _Proceedings of the Thirty-Second Conference on Learning Theory_, volume 99 of _Proceedings of Machine Learning Research_, pages 318-341. PMLR, 25-28 Jun 2019.
* [BKSW19] Mark Bun, Gautam Kamath, Thomas Steinke, and Zhiwei Steven Wu. Private hypothesis selection. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-Buc, Emily B. Fox, and Roman Garnett, editors, _Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada_, pages 156-167, 2019.
* [CDKL22] Clement L. Canonne, Ilias Diakonikolas, Daniel Kane, and Sihan Liu. Nearly-tight bounds for testing histogram distributions. In _Advances in Neural Information Processing Systems 35 (NeurIPS)_, 2022. To appear.

* [CDSS14] Siu-on Chan, Ilias Diakonikolas, Rocco A. Servedio, and Xiaorui Sun. Near-optimal density estimation in near-linear time using variable-width histograms. In _Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada_, pages 1844-1852, 2014.
* [CKM\({}^{+}\)19] Clement L. Canonne, Gautam Kamath, Audra McMillan, Adam D. Smith, and Jonathan R. Ullman. The structure of optimal private tests for simple hypotheses. In Moses Charikar and Edith Cohen, editors, _Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, STOC 2019, Phoenix, AZ, USA, June 23-26, 2019_, pages 310-321. ACM, 2019.
* 22, 2012_, pages 709-728. ACM, 2012.
* [Dia16] Ilias Diakonikolas. Learning structured distributions. In _CRC Handbook of Big Data_, pages 267-283. 2016.
* [DK14] Constantinos Daskalakis and Gautam Kamath. Faster and sample near-optimal algorithms for proper learning mixtures of gaussians. In _Conference on Learning Theory_, pages 1183-1213. PMLR, 2014.
* [DKS17] Ilias Diakonikolas, Daniel M. Kane, and Alistair Stewart. Statistical query lower bounds for robust estimation of high-dimensional gaussians and gaussian mixtures. In Chris Umans, editor, _58th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2017, Berkeley, CA, USA, October 15-17, 2017_, pages 73-84. IEEE Computer Society, 2017.
* 2512, 1996.
* 2637, 1997.
* [DL01] Luc Devroye and Gabor Lugosi. _Combinatorial methods in density estimation_. Springer, 2001.
* [GKK\({}^{+}\)20] Sivakanth Gopi, Gautam Kamath, Janardhan Kulkarni, Aleksandar Nikolov, Zhiwei Steven Wu, and Huanyu Zhang. Locally private hypothesis selection. In Jacob D. Abernethy and Shivani Agarwal, editors, _Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria]_, volume 125 of _Proceedings of Machine Learning Research_, pages 1785-1816. PMLR, 2020.
* [KMV12] Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Disentangling gaussians. _Commun. ACM_, 55(2):113-120, 2012.
* [KSS18] Pravesh K. Kothari, Jacob Steinhardt, and David Steurer. Robust moment estimation and improved clustering via sum of squares. In _Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2018, Los Angeles, CA, USA, June 25-29, 2018_, pages 1035-1046. ACM, 2018.
* COLT_, pages 503-512, 2008.
* [Pea95] Karl Pearson. Mathematical contributions to the theory of evolution. ii. skew variation in homogeneous material. [abstract]. _Proceedings of the Royal Society of London_, 57:257-260, 1895.
* [SOAJ14] Ananda Theertha Suresh, Alon Orlitsky, Jayadev Acharya, and Ashkan Jafarpour. Near-optimal-sample estimators for spherical gaussian mixtures. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, _Advances in Neural Information Processing Systems_, volume 27, 2014.

[Yat85] Yannis G. Yatracos. Rates of convergence of minimum distance estimators and kolmogorov's entropy. _The Annals of Statistics_, 13(2):768 - 774, 1985.

## Appendix A Almost linear time algorithm with \(\alpha=3\)

In this section, we focus on our first result, the algorithm with \(\alpha=3\). The pseudocode of our approach is presented in Algorithm 1. We prove the performance of this algorithm in Theorem 5. An overview of our approach is described in Section 3.2.

```
1:procedureSelect-Hypothesis(\(\mathcal{H},\ \epsilon,\ \delta\), and query access to \(\hat{w}_{j}(H_{i})\)'s)
2:\(\tilde{W}(H_{i})\gets 0\) for every \(i\in[n]\)
3:\(k\leftarrow\lfloor 1/\epsilon^{\prime}\rfloor+1\)
4:\(B_{1}\leftarrow\mathcal{H}\)
5:\(B_{2},\ldots,B_{k}\leftarrow\emptyset\)
6:\(\epsilon^{\prime}\leftarrow\epsilon/3\)
7:\(\delta^{\prime}\leftarrow\delta/3\)
8:\(\gamma\leftarrow\delta^{\prime}\)
9:\(\delta_{fn}\leftarrow\delta^{\prime}/k\)
10:\(\delta_{fP}\leftarrow\gamma\cdot\delta^{\prime}/(4\,k\,n\,\log(n))\)
11:for\(\ell=1,\ldots,k\)do\(\triangleright\) Iterating over buckets
12:for\(i=1,\ldots,n\)do\(\triangleright\) Iterating over hypotheses to find a prompting one
13:if\(|B_{\ell}|=0\)then
14: Break the "for" loop and move on to the next bucket.
15:ifIs-Prompting(\(\mathcal{B},\ \ell,\ H_{i},\ \gamma,\ \epsilon^{\prime},\ \delta^{\prime}\)) then\(\triangleright\) Checks if \(H_{i}\) is prompting.
16:\(C_{\ell\,i}\gets 0\quad\triangleright\) A counter for the number of hypotheses that \(H_{i}\) removes from \(B_{\ell}\)
17:for\(H_{j}\in B_{\ell}\)do\(\triangleright\) Update all hypotheses in \(B_{\ell}\) via the prompting hypothesis
18:if\(\hat{w}_{i}(H_{j})>\tilde{W}(H_{j})+\epsilon^{\prime}\)then
19:\(\tilde{W}(H_{j})\leftarrow\max\left(\tilde{W}(H_{j}),\hat{w}_{i}(H_{j})\right)\)
20: Move \(H_{j}\) to \(B_{\left\lceil\tilde{W}(H_{j})/\epsilon^{\prime}\right\rceil}\)
21:\(C_{\ell\,i}\gets C_{\ell\,i}+1\)
22:if\(\frac{C_{\ell\,i}}{C_{\ell\,i}+|B_{\ell}|}<\gamma/2\)then\(\triangleright\) It indicates \(H_{i}\) was not prompting.
23: Output \(\bot\) and halt.
24:\(i\gets 0\)\(\triangleright\) Restarts the "for" loop.
25:if\(|B_{\ell}|\neq 0\)then
26:\(\hat{H}\leftarrow\) a random hypothesis in \(B_{\ell}\)
27: Output \(\hat{H}\) and halt. ```

**Algorithm 1** An algorithm for hypothesis selection with \(\alpha=3\).

At a high level, our algorithm begins with all \(\tilde{W}(H_{i})\) set to zero, and all hypotheses placed in \(B_{1}\). We then iterate over all buckets. In round \(\ell\), we check if there is a prompting hypothesis \(H_{i}\in\mathcal{H}\) that can move a significant fraction of hypotheses from \(B_{\ell}\). Specifically, we look for an \(H_{i}\) that moves a \(\gamma\)-fraction of hypotheses from \(B_{\ell}\) by updating them, where \(\gamma=\Theta(\delta)\). If we find such a hypothesis \(H_{i}\), we update all the hypotheses in \(B_{\ell}\) and move as many as possible out of \(B_{\ell}\). During this process, we count the number of hypotheses that \(H_{i}\) removes from \(B_{\ell}\), which is tracked by \(C_{\ell\,i}\) in the algorithm. If the actual fraction of hypotheses removed from \(B_{\ell}\) is less than \(\gamma/2\), we infer that Is-Prompting falsely declared \(H_{i}\) as a prompting hypothesis (false positive). Thus, the algorithm returns \(\bot\). Otherwise, we restart the search by setting \(i=1\) and iterate over all \(H_{i}\) to find the next prompting hypothesis. We continue this process until no prompting hypothesis is found or the bucket is emptied. If we completely empty a bucket, we move on to the next one. If not, we select a random hypothesis in \(B_{\ell}\) as the final output and halt.

**Theorem 5**.: _Suppose \(\mathcal{H}\) is a class of \(n\) known hypotheses and \(P\) is an unknown hypothesis. Assume the algorithm has access to accurate estimates of \(\hat{w}_{j}(H_{i})\)'s that have an error of at most \(\epsilon^{\prime}\). Let \(\epsilon\) and \(\delta\) be two parameters in \((0,1)\). If \(3\,\epsilon^{\prime}\leq\epsilon\), then Algorithm 1 is an \((\alpha=3,\epsilon,\delta)\)-proper learner for \(P\) in \(\mathcal{H}\). The running time of our algorithm is \(\tilde{O}((n/(\delta^{3}\epsilon))\cdot T_{q})\), where \(T_{q}\) is the time to obtain each \(\hat{w}_{j}(H_{i})\)._

Proof.: Our proof consists of three parts:

[MISSING_PAGE_EMPTY:15]

[MISSING_PAGE_FAIL:16]

\[\sum_{i}^{n}T_{\texttt{false}}^{(i)} =\sum_{i}^{n}O\left(\left(\log\left(\delta_{fn}^{-1}\right)+\log \log\left(\delta_{fp}^{-1}\right)\right)\cdot T_{q}/\gamma^{2}+n_{r}^{(\ell,i)} \cdot T_{q}/\gamma\right)\] \[=O\left(n\cdot\left(\log\left(\delta_{fn}^{-1}\right)+\log\log \left(\delta_{fp}^{-1}\right)\right)\cdot T_{q}/\gamma^{2}+n\cdot T_{q}/\gamma\right)\] \[=O\left(n\cdot\left(\log\left(\delta_{fn}^{-1}\right)+\log\log \left(\delta_{fp}^{-1}\right)\right)\cdot T_{q}/\gamma^{2}\right)\]

Using the setting of our parameters, \(\delta_{fp}=\gamma\cdot\delta^{\prime}/(4\,k\,n\,\log(n))=O(n\,\log n/(\delta \,\epsilon))\) and \(\delta_{fn}=\delta^{\prime}/k=O(1/(\delta\,\epsilon))\).

\[\sum_{i}^{n}T_{\texttt{false}}^{(i)} =O\left(\frac{n}{\delta^{2}}\cdot\left(\log\left(\frac{1}{\delta \,\epsilon}\right)+\log\log\left(\frac{n}{\delta\,\epsilon}\right)\right) \cdot T_{q}\right)\] \[=O\left(\frac{n}{\delta^{2}}\cdot\left(\log\left(\frac{1}{\delta \,\epsilon}\right)+\log\log\left(n\right)\right)\cdot T_{q}\right)\,.\]

Moreover, for the case that a prompting hypothesis is found:

\[T_{\texttt{true}} =O\left(\log\left(\delta_{fp}^{-1}\right)\cdot\left(\log\left( \delta_{fn}^{-1}\right)+\log\log\left(\delta_{fp}^{-1}\right)\right)\cdot T_{ q}/\gamma^{2}\right)\] \[=O\left(\frac{1}{\delta^{2}}\cdot\log\left(\frac{n}{\delta\, \epsilon}\right)\cdot\left(\log\left(\frac{1}{\delta\,\epsilon}\right)+\log \log\left(n\right)\right)\cdot T_{q}\right)\] \[=O\left(\frac{1}{\delta^{2}}\cdot\log\left(\frac{n}{\delta\, \epsilon}\right)\cdot\left(\log\left(\frac{1}{\delta\,\epsilon}\right)+\log \log\left(n\right)\right)\cdot T_{q}\right)\,.\]

Hence, the total time complexity is:

\[\text{Total time complexity }=O\left(k\cdot\frac{\log(n)}{\gamma}\cdot\left(\sum_{i}^{n}T_{ \texttt{false}}^{(i)}+T_{\texttt{true}}+n\cdot T_{q}\right)\right)\] \[=O\left(\frac{\log(n)}{\delta^{3}\,\epsilon}\cdot\left(n\cdot \log\log n+n\cdot\log\left(\frac{1}{\delta\,\epsilon}\right)+\left(\log\left( \frac{1}{\delta\,\epsilon}\right)\right)^{2}\right)\cdot T_{q}\right)\] \[=\tilde{O}\left(\frac{n}{\delta^{3}\epsilon}\cdot T_{q}\right)\,.\]

Simple version of Is-Prompting: In this version, we only have one possible confidence parameter, which is \(\min(\delta_{fn},\delta_{fp})\). Hence, using Lemma A.2, we have:

\[T_{\texttt{false}}^{(i)}=T_{\texttt{true}}=O\left(\log\left((\min(\delta_{fn}, \delta_{fp}))^{-1}\right)\cdot T_{q}/\gamma^{2}\right)\,.\]

\[\text{Total time complexity }=O\left(k\cdot\frac{\log(n)}{\gamma}\cdot\left(\sum_{i}^{n}T_{ \texttt{false}}^{(i)}+T_{\texttt{true}}+n\cdot T_{q}\right)\right)\] \[=O\left(n\cdot\frac{\log(n)}{\delta^{3}\,\epsilon}\cdot\left(\log \left(\frac{n}{\delta\,\epsilon}\right)\right)\cdot T_{q}\right)\] \[=\tilde{O}\left(\frac{n}{\delta^{3}\epsilon}\cdot T_{q}\right)\,.\]

Thus, the proof is complete. \(\Box\)

### Identifying prompting hypothesis

In this section, we provide two algorithms for identifying a prompting hypothesis. The simple algorithm, presented in Section A.1.1, involves sampling the hypotheses in a bucket to estimate the fraction of the hypotheses that \(H_{i}\) can move, using a standard application of the Hoeffding bound. The advanced algorithm, presented in Section A.1.2, is based on the fact that we require different confidence parameters for false negative and false positive errors, given our analysis for Algorithm 1. We also show that the time complexity of this algorithm, in the case it returns false, is proportional to the number of hypotheses we can remove from the bucket during this procedure, which later helps us to use an amortized argument for the overall time spent on false returns.

#### a.1.1 Simple version of Is-Prompting

```
1:procedureIs-Prompting-Simple(\(\mathcal{B},\ \ell,\ H_{i},\ \gamma,\ \epsilon^{\prime},\ \delta\), and query access to \(\hat{w}_{j}(H_{i})\)'s)
2:\(t\leftarrow\left\lceil 8\ \log(2/\delta)/\gamma^{2}\right\rceil\)
3:\(u_{r}\leftarrow\) 0 \(\triangleright\)\(u_{r}\) counts # sampled hypotheses that \(H_{i}\) updates
4:repeat\(t\) times
5:\(H_{j}\leftarrow\) a random hypothesis in \(B_{\ell}\)
6:if\(\hat{w}_{i}(H_{j})>\tilde{W}(H_{j})+\epsilon^{\prime}\)then
7:\(u_{r}\gets u_{r}+1\)
8:if\(\frac{u_{r}}{t}<\frac{3\ \gamma}{4}\)then
10:returnfalse
11:else
12:returntrue ```

**Algorithm 2** An algorithm to identify a prompting hypothesis (simple version).

**Lemma A.2**.: _Suppose we are given two parameters \(\delta,\epsilon^{\prime}\in(0,1)\), and three positive integers \(n,\ k\), and \(\ell\in[k]\). Assume we are given a class of hypotheses \(\mathcal{H}\), its partition into \(k\) buckets \(\mathcal{B}=\{B_{1},B_{2},\ldots,B_{k}\}\), and a hypothesis \(H_{i}\in\mathcal{H}\). The Is-Prompting-Simple procedure in Algorithm 2 receives \(\mathcal{H},\mathcal{B},\ell,H_{i},\delta_{fp},\delta_{fn},\) and \(\epsilon^{\prime}\) as its input and returns true or false with the following guarantees:_

* _If_ \(H_{i}\) _substantially updates at least a_ \(\gamma\)_-fraction of the hypotheses in_ \(B_{\ell}\)_, then_ Is-Prompting-Simple _returns_ \(\mathtt{true}\) _with probability at least_ \(1-\delta\)_._
* _If_ \(H_{i}\) _substantially updates less than a_ \(\gamma/2\)_-fraction of the hypotheses in_ \(B_{\ell}\)_, then_ Is-Prompting-Simple _returns_ \(\mathtt{false}\) _with probability at least_ \(1-\delta\)_._
* _We have the following guarantees for the algorithm:_ \[\text{Number of queries to }\hat{w}_{i}(H_{j})\text{'s: }O\left(\log(\delta^{-1})/\gamma^{2}\right)\,,\] \[\text{Running time: }O\left(\log(\delta^{-1})\cdot T_{q}/ \gamma^{2}\right)\,,\] _where_ \(T_{q}\) _is the time to obtain each_ \(\hat{w}_{j}(H_{i})\)_._

Proof.: This is a simple application of the Hoeffding bound:

\[\mathbf{Pr}\left[\left|\frac{u}{t}-\mathbf{E}\left[\frac{u}{t}\right]\right| \geq\frac{\gamma}{4}\right]\leq 2\exp\left(-2\,t\,\gamma^{2}\right)\leq \delta\,.\]

#### a.1.2 Advanced version of Is-Prompting

We present an overview of the algorithm for the advanced version of Is-Prompting. Suppose we are given a parameter \(\gamma\), and our goal is to design an algorithm that does the following with high probability: it returns true when \(H_{i}\) can substantially update a \(\gamma\)-fraction of the hypotheses in \(B_{\ell}\), and it returns false when \(H_{i}\) can substantially update less than a \(\gamma/2\) fraction of the hypotheses in \(B_{\ell}\).

For this new procedure, we take advantage of an asymmetry that exists between returning false and true. This asymmetry arises mainly from the analysis of Algorithm 1. Roughly speaking, for every bucket \(\ell\), to find a single prompting hypothesis, we check \(O(n)\) (potentially non-prompting) hypotheses. On the other hand, we only need \(\log_{1/\gamma}(B_{\ell})\) prompting hypotheses to empty out the whole bucket. Hence, we expect that the procedure returns mostly false rather than true.

Another difference here is the cost we pay for mistakenly declaring true or false. When we declare a hypothesis prompting, we iterate over all hypotheses in \(B_{\ell}\) and check if we can remove them from the bucket. If the hypothesis was not actually prompting, we have paid a substantial cost (in time) without making progress. Thus, we cannot have too many hypotheses that the algorithm declares prompting while they are not (false positives). However, the probability of mistakenly missing a prompting hypothesis does not have to be this small (false negatives).

Below is a more precise definition of these errors:

\[\delta_{fp} \coloneqq\mathbf{Pr}[\text{Returning true}\mid H_{i}\text{ substantially updates less than a }\gamma/2\text{ fraction}]\] \[\delta_{fn} \coloneqq\mathbf{Pr}[\text{Returning false}\mid H_{i}\text{ substantially updates at least a }\gamma\text{ fraction}]\]

Given the analysis of our algorithm, we can show that the probability of false positives must be around \(\tilde{O}(1/\left|B_{\ell}\right|)=\tilde{O}(1/n)\). On the other hand, our analysis is robust as long as the probability of false negatives is roughly \(O(\delta)\), where \(\delta\) is the overall confidence of our algorithm. (The reason for this choice of parameter may not be obvious from this high-level discussion; however, it is a direct artifact of our analysis.) This gap is particularly large when \(\delta\) is a small constant, as is common in the literature.

The Is-Prompting procedure (Algorithm 3) is designed in a way that has different time complexities depending on its output being true or false. The procedure uses more than \(O(\log n)\) (more like \(O(\log n\cdot\log(1/\delta)/\gamma^{2})\)) time if it returns true. However, the main advantage is that it uses much less time when the output is false. In fact, instead of paying \(O(\log n)\), we only spend \(O(\log(1/\delta))\) time plus an amortized cost of \(O(1/\gamma)\) for the \(O(B_{\ell})\) calls to the procedure.

The algorithm runs in roughly \(O(\log n)\) rounds. In each round, it draws a few random hypotheses from \(B_{\ell}\) (roughly \(O(\log(1/\delta)/\gamma^{2})\)). In each round, we check if the fraction of hypotheses \(H_{i}\) can substantially update is close to \(\gamma\). At any round, if we see that the fraction is not close to \(\gamma\), we return false. The hope is that for a non-prompting hypothesis, we either stop very quickly or the hypothesis passes too many rounds without us noticing that it is not prompting. Therefore, we must have seen an inflated number of substantial updates in these rounds. Before returning false, we perform those substantial updates among the sampled hypotheses that we have observed to remove those movable hypotheses from \(B_{\ell}\).

We can capitalize on this fact in our cost analysis. In fact, we can show that in the false case, if the procedure takes roughly \(O(t/\gamma)\) time, it must have removed \(t\) hypotheses from the bucket. Thus, one can show that the amortized cost of these elongated rounds is only \(O(1/\gamma)\).6

Footnote 6: The parameters in this high-level discussion lack precision. For a rigorous analysis, refer to our proof of Lemma A.3.

**Lemma A.3**.: _Suppose we are given three parameters \(\delta_{fp},\delta_{fn},\epsilon^{\prime}\in(0,1)\), and three positive integers \(n\), \(k\), and \(\ell\in[k]\). Assume we are given a classes of hypotheses \(\mathcal{H}\), its partition into \(k\) buckets \(\mathcal{B}=\{B_{1},B_{2},\ldots,B_{k}\}\) and a hypothesis \(H_{i}\in\mathcal{H}\). The Is-Prompting procedure in Algorithm 3 receives \(\mathcal{H},\mathcal{B},\ell,H_{i},\delta_{fp},\delta_{fn},\) and \(\epsilon^{\prime}\) as its input and returns true or false with the following guarantees:_

* _If_ \(H_{i}\) _substantially updates at least_ \(\gamma\) _fraction of the hypotheses in_ \(B_{\ell}\)_, then Is-Prompting returns_ true _with probability at least_ \(1-\delta_{fn}\)_._
* _If_ \(H_{i}\) _substantially updates less than_ \(\gamma/2\) _fraction of the hypotheses in_ \(B_{\ell}\)_, then_ Is-Prompting _returns_ false _with probability at least_ \(1-\delta_{fp}\)* _If the algorithm returns_ true_, we have the following guarantees for the algorithm:_ \[\text{Number of queries to }\hat{w}_{i}(H_{j})\text{'s: }O\left(\log\left(\delta_{fp}^{-1}\right) \cdot\left(\log\left(\delta_{fn}^{-1}\right)+\log\log\left(\delta_{fp}^{-1} \right)\right)/\gamma^{2}\right)\,,\] \[\text{Running time: }O\left(\log\left(\delta_{fp}^{-1}\right)\cdot\left(\log \left(\delta_{fn}^{-1}\right)+\log\log\left(\delta_{fp}^{-1}\right)\right) \cdot T_{q}/\gamma^{2}\right)\,.\]
* _Suppose the algorithm returns_ false_. Assume_ \(n_{r}^{(\ell,i)}\) _indicates the number of hypotheses that the algorithm removes from_ \(B_{\ell}\)_. Then, we have the following guarantees for the algorithm:_ \[\text{Number of queries to }\hat{w}_{i}(H_{j})\text{'s: }O\left(\left(\log\left(\delta_{ fn}^{-1}\right)+\log\log\left(\delta_{fp}^{-1}\right)\right)/\gamma^{2}+n_{r}^{( \ell,i)}/\gamma\right)\,,\] \[\text{Running time: }O\left(\left(\log\left(\delta_{fn}^{-1}\right)+\log \log\left(\delta_{fp}^{-1}\right)\right)\cdot T_{q}/\gamma^{2}+n_{r}^{(\ell,i )}\cdot T_{q}/\gamma\right)\,.\]

_In above bounds, where \(T_{q}\) denotes the time complexity to obtain each \(\hat{w}_{j}(H_{i})\)._

Proof.: The probabilities in this proof are taken over the random choices of \(G_{j}^{(r)}\)'s. Note that \(S\) contains at the hypotheses for which we can change.

Probability of false positive:First, we show that probability of outputting true, while \(H_{i}\) can substantially update less \(\gamma/2\) fraction of hypotheses in \(B_{\ell}\), is at most \(\delta_{fp}\). The algorithm return true in two places. First, at the beginning of each round, we check whether \(S_{r-1}\) contains at least \(\gamma/2\)-fraction of hypotheses. This answer is always correct with probability one. Since the algorithm has an evidence that \(H_{i}\) can certainly update at least \(\gamma/2\) of hypotheses in \(B_{\ell}\). Thus, this case does not affect our false positive rate \(\delta_{fp}\).

Second, the algorithm returns true after all rounds end. This case only happens when \(u_{r}/t\) is at least \(3\gamma/4\) in every round in Line 20. It is not hard to see that \(u_{r}\) is a binomial random variable with \(t\) trials. The success probability of each trial is the ratio of the hypotheses in \(B_{\ell}\) that \(H_{i}\) can update substantially. Thus, in the case where \(H_{i}\) substantially updates less than \(\gamma\) fraction of hypotheses, by Markov's inequality, we have:

\[\mathbf{Pr[}\] \[\text{Returning true}\mid H_{i}\text{ substantially updates less than $\gamma/2$-fraction}\] \[=\mathbf{Pr[}\forall r\in[R]:\frac{u_{r}}{t}\geq\frac{3\gamma}{4} \bigg{]}\left(\mathbf{Pr[}\frac{u_{r}}{t}\geq\frac{3\gamma}{4}\bigg{]}\right) ^{R}\leq(2/3)^{R}\leq\delta_{fp}\]

Probability of false negative:Suppose that \(H_{i}\) substantially updates at least \(\gamma\)-fraction of the hypotheses in \(B_{\ell}\). That is, \(H_{i}\) can update each \(G_{j}^{(r)}\) with probability at least \(\gamma\). Fix a round \(r\). We bound the probability of returning false in Line 19 and Line 24.

In Line 19, we return false when \(|S_{r}|-|S_{r-1}|<\frac{\gamma\cdot t}{8}\). That is, the number of new hypotheses we added to \(S_{r}\) is not too large. Consider the random hypotheses we draw at round \(r\). Our claim is that most of these random hypotheses are not in \(S_{r-1}\). The probability that one \(G_{j}^{(r)}\) be selected from \(S_{r-1}\) is \(|S_{r-1}|\left/\left|B_{\ell}\right|\right.\). However, this expectation is less than \(\gamma/2\). Otherwise, the algorithm would have returned true earlier in Line 5. With this information in mind, we can use the Hoeffding bound and get:

\[\mathbf{Pr[}\] \[\text{number of }G_{j}^{(r)}\in S_{r-1}\geq\frac{5\,\gamma\cdot t}{8} \bigg{]} =\mathbf{Pr[}\frac{\text{number of }G_{j}^{(r)}\in S_{r-1}}{t}\geq\frac{5\,\gamma }{8}=\frac{\gamma}{2}\cdot\left(1+\frac{1}{4}\right)\bigg{]}\] \[\leq\mathbf{Pr[}\frac{\text{number of }G_{j}^{(r)}\in S_{r-1}}{t}>\frac{|S_{r-1}|}{|B_{\ell}|}+ \frac{\gamma}{4}\bigg{]}\] \[\leq\exp\left(-2\,t\,(\gamma/4)^{2}\right)\leq\frac{\delta_{fn}}{ 2\,R}\,.\]

In Line 24, we return false if \(u_{r}/t\) is less than \(3\,\gamma/4\). By Chernoff bound, we have:

\[\mathbf{Pr[}\frac{u_{r}}{t}<\gamma\cdot\left(1-\frac{1}{4}\right)\bigg{]}\leq \exp\left(-\frac{t\,\gamma}{3\cdot 4^{2}}\right)=\frac{\delta_{fn}}{2\,R}\,.\]

Taking the union bound over all rounds, we obtain:

\[\mathbf{Pr[}\] \[\text{Returning false}\mid H_{i}\text{ substantially updates at least $\gamma$-fraction}\big{]}\leq\delta_{fn}\,.\]

Running time:In the case the algorithm return true, the algorithm makes \(O(R\cdot t)\) queries to \(\hat{w}_{i}\left(G_{j}^{(r)}\right)\)'s. And, it runs in the \(O(R\cdot t\cdot T_{q})\). If algorithm returns false in round \(r\), similarly it makes \(O(r\cdot t)\) queries to \(\hat{w}_{i}\left(G_{j}^{(r)}\right)\)'s. And, it runs in the \(O(r\cdot t\cdot T_{q})\). Note that with the exception of the last round in every round \(r^{\prime}\in[r-1]\). At least \((\gamma\cdot t)/8\) new hypotheses are added to \(S_{r^{\prime}}\). Therefore, the size of \(S_{r}\) is at least \((\gamma\cdot t\cdot(r-1))/8\). We remove all of these hypotheses before we output false which cause the size of \(B_{\ell}\) to drop by \(n_{r}^{(\ell,i)}\coloneqq|S_{r}|\). In this case, the algorithm makes \(O(t+n_{r}^{(\ell,i)}/\gamma)\) queries to \(\hat{w}_{i}\left(G_{j}^{(r)}\right)\)'s. And, it runs in the \(O\left(\left(t+n_{r}^{(\ell,i)}/\gamma\right)\cdot T_{q}\right)\). Thus, the proof is complete by setting \(R\coloneqq\left\lceil\log_{3/2}(1/\delta_{fp})\right\rceil\) and \(t\coloneqq\left\lceil 48\log(2R/\delta_{fn})/\gamma^{2}\right\rceil\). \(\Box\)

[MISSING_PAGE_FAIL:22]

Proof.: For the sake of argument, assume each of the sub-routines in the algorithm works as guaranteed with probability one (as oppose to the case where they work as expected with probability \(1-\delta^{\prime\prime}\)). Later on, we discuss the overall confidence of the algorithm to remove this assumption.

Accuracy of seeds:the procedure to find the initial seeds provides us with a \((\sigma+\epsilon^{\prime},3\,\sigma+3\epsilon^{\prime},m)\)-seed with high probability. Using Fact C.4, this seed is also a \((2\,\sigma+\epsilon^{\prime},4\,\sigma+5\epsilon^{\prime},m)\). In boosting seeds, we start with \((2\,\sigma+\epsilon^{\prime},4\,\sigma+5\epsilon^{\prime},m)\) and we get \((2\,\sigma+\epsilon^{\prime},4\,\sigma+5\epsilon^{\prime},m^{\prime})\) where \(m^{\prime}=\lfloor\eta\cdot m\rfloor\). This statement is justified by Theorem 9, setting \(\kappa=\kappa^{\prime}=2\), and using the same values for \(a=2,\sigma+\epsilon^{\prime}\) and \(b=4,\sigma+5\epsilon^{\prime}\) for our seeds. It is worth noting that the only parameter that changes while we boost is \(m\).

Accuracy of the outputConsider the case where we produce output when \(|\mathcal{Q}|\) is zero. That implies that our algorithm marked all the hypotheses. Throughout the course of algorithms, we never mark a hypothesis \(H_{i}\) unless we have found an evidence that \(\|H_{i}-P\|_{\text{TV}}>\sigma\). This was established by observing \(\hat{w}_{j}(H_{i})>\sigma+\epsilon^{\prime}\), which implies \(\|H_{i}-P\|_{\text{TV}}\) must be greater than \(\sigma\); Or, showing a close-by hypotheses to \(H_{i}\) is far from \(P\) and apply triangle inequality. See our argument for Case 3.2 in the proof of Theorem 9 for example. Thus, if all hypotheses are marked, it must be the case that for every \(H_{i}\), \(\|H_{i}-P\|_{\text{TV}}\) is greater than \(\sigma\). Hence, when the size of \(\mathcal{Q}\) is zero we can truly assert that "\(\sigma<\text{OPT}\)".

For the case that \(|\mathcal{Q}|=1\), the algorithm focuses on \(H_{i}\), the only unmarked hypothesis left. If we found \(\hat{W}(H_{i})\) is greater than \(\sigma+\epsilon^{\prime}\), we can simply conclude that for every \(H_{i}\), \(\|H_{i}-P\|_{\mathrm{TV}}\) is greater than \(\sigma\), and "\(\sigma<\mathsf{OPT}\)". Otherwise, using Fact C.3, the \(H_{i}\) is a valid answer:

\[\|H_{i}-P\|_{\mathrm{TV}}\leq W(H_{i})+2\mathsf{OPT}\leq\hat{W}(H_{i})+ \epsilon^{\prime}+2\mathsf{OPT}\leq 3\,\max\left(\sigma,\mathsf{OPT}\right)+2 \epsilon^{\prime}\,.\]

In addition, the accuracy of the \(H_{i}\) that is returned in Line 22 is guaranteed by the Theorem 9, and setting \(\kappa=\kappa^{\prime}=2\). Next, lets focus on the hypotheses we output after the "if" condition in Line 29. When \(\hat{W}(H_{\ell})\) is small, we have:

\[\|H_{\ell}-P\|_{\mathrm{TV}}\leq W(H_{\ell})+2\mathsf{OPT}\leq\hat{W}(H_{\ell })+\epsilon^{\prime}+2\mathsf{OPT}\leq 2\,\sigma+2\epsilon^{\prime}+2 \mathsf{OPT}\leq 4\,\max\left(\sigma,\mathsf{OPT}\right)+2\epsilon^{\prime}\,.\]

When \(\hat{W}(H_{\ell})\) is large, the analysis is very similar to Case 1 in the proof of Theorem 9. Again, by setting \(\kappa=\kappa^{\prime}=2\), and using the facts that \(\hat{W}(H_{\ell})-\epsilon^{\prime}\geq\kappa^{\prime}\sigma\) and \(\hat{W}(H_{i})\leq\left(\kappa+2\right)\sigma+5\,\epsilon^{\prime}\), we obtain:

\[\|H_{i}-P\|_{\mathrm{TV}}\leq 4\,\max\left(\sigma,\mathsf{OPT}\right)+6 \epsilon^{\prime}\,.\]

Number of recursions:Given Theorem 8 and Theorem 9, we only see "start over" when \((1-2\eta)\) fraction of unmarked hypotheses in \(\mathcal{Q}\) have been marked. Since we have \(n\) hypotheses, we do not start over more than \(O(\log_{1/\eta}(n))\) times.

Running time:In the while loop, \(m\) decreases with a factor of \(\eta\) every time we find a new seed. Hence, the total number of iterations is bounded by \(O(\log_{1/\eta}(n))\). Using Theorem 8 and Theorem 9, invoking the procedures for finding a seed and boosting a seed each takes \(O(n\log(n/\delta^{\prime\prime})/\eta\cdot T_{q})\) time. Thus, the total time complexity is:

\[O\left(\frac{n}{\eta}\cdot\left(\log(n/\delta)+\log\log_{1/\eta}(n)\right) \cdot\log_{1/\eta}^{2}(n)\cdot T_{q}\right)=\tilde{O}\left(\frac{n\cdot\log( 1/\delta)\cdot T_{q}}{\eta}\right)\,.\]

Overall confidence parameter:The total number of subroutines we call here is at most \(O(\log_{1/\eta}^{2}(n))\). Thus, by setting \(\delta^{\prime\prime}\) to \(O(\delta/\log_{1/\eta}^{2}(n))\) and using the union bound, we can show the overall error probability is bounded by \(\delta\).

Rounds of adaptivity:Using Theorem 8 and Theorem 9, invoking the procedures for finding a seed and boosting a seed each takes \(O(1)\) rounds of adaptivity. Hence, the overall rounds of adaptivity is \(O\left(\log_{1/\eta}^{2}(n)\right)\) rounds.

**Remark 7**.: _One could argue that we have demonstrated the algorithm operates in \(\tilde{O}(n/\eta\cdot T_{q})\) time with a probability of \(1-\delta\). However, this fact does not inherently guarantee that the algorithm consistently maintains the desired time complexity. Fortunately, a straightforward solution exists to address this concern._

_Our algorithm may fail for two primary reasons: either too few hypotheses were marked, or the identified seed had a larger \(m\) value than anticipated. In either scenario, the algorithm can verify the occurrence of this undesirable, but improbable event. In these situations, we can output \(\bot\) to indicate the algorithm's failure to produce a valid answer. With this adjustment, the time complexity remains low as desired._

**Corollary B.1**.: _Suppose \(\mathcal{H}\) is a class of \(n\) known hypotheses and \(P\) is an unknown hypothesis. Let \(\epsilon\) and \(\delta\) be two parameters in \((0,1)\). Assume the algorithm has access to accurate estimates of \(\hat{w}_{j}(H_{i})\)'s that have error at most \(\epsilon^{\prime}\). If \(3\,\epsilon^{\prime}\leq\epsilon\), then for every \(\eta\in(1/n,1/4)\) there exists an \((\alpha=4,\epsilon,\delta)\)-proper learner for \(P\) in \(\mathcal{H}\). The running time of our algorithm is \(\tilde{O}((n/\eta)\cdot\log(1/\delta)\cdot\log(1/\epsilon)\cdot T_{q})\), and it can be implemented in \(O\left(\log_{1/\eta}^{2}(n)\cdot\log(1/\epsilon)\right)\) rounds of adaptivity._Proof.: This is a direct corollary of Theorem 6 combined with a binary search over values of \(\sigma\in\{\epsilon^{\prime},\epsilon^{\prime}\}\) where \(\epsilon^{\prime}\coloneqq\epsilon/100\). Note that for every \(\sigma\geq\mathsf{OPT}\), the algorithm has to produce a hypothesis, since it cannot declare \(\mathsf{OPT}<\sigma\) with high probability. Outputting the hypothesis associated with the smallest \(\sigma\) would give us the desired guaranteed. 

### Finding initial seed

In this section, we describe an algorithm that receives parameter \(\sigma\) as input and finds a \((\sigma+\epsilon^{\prime},\ 3\,\sigma+2\epsilon^{\prime},\ \lfloor\eta\cdot n\rfloor)\)-seed in roughly \(\tilde{O}(n/\eta\cdot T_{q})\) time with high probability or declares "\(\sigma<\mathsf{OPT}\)".

```
1:procedureFind-Seed(\(\mathcal{H},\ \mathcal{Q},\ \sigma,\ \epsilon^{\prime},\ \delta,\ \eta\), and query access to \(\hat{w}_{j}(H_{i})\)'s)
2:\(m\leftarrow\lfloor\eta\cdot n\rfloor\)
3:for\(i=1,\ldots,n\)do
4:repeat\(t\coloneqq\lceil 8\,n\log\left(2\,n/\delta\right)/m\rceil\) times
5:\(H_{j}\leftarrow\) a uniformly random sample drawn from \(\mathcal{H}\)
6:if\(\hat{w}_{j}(H_{i})>\sigma+\epsilon^{\prime}\)then
7: Mark \(H_{i}\) in \(\mathcal{Q}\).
8:\(H_{j}\leftarrow\) a uniformly random sample drawn from \(\mathcal{Q}\)
9:if\(\hat{w}_{j}(H_{i})>\sigma+\epsilon^{\prime}\)then
10: Mark \(H_{i}\) in \(\mathcal{Q}\).
11:
12:if\(|\mathcal{Q}|\leq t\)then
13:for every \(H_{j}\in\mathcal{Q}\)do
14:if\(\hat{w}_{j}(H_{i})>\sigma+\epsilon^{\prime}\)then
15: Mark \(H_{i}\) in \(\mathcal{Q}\).
16:if\(H_{i}\) is unmarked.then
17: Compute \(\hat{W}(H_{i})\).
18:if\(\hat{W}(H_{i})\leq 3\,\sigma+3\,\epsilon^{\prime}\)then
19:return\(H_{i}\) as a seed.
20:else
21:for\(H_{j}\in\mathcal{H}\)do
22:if\(\hat{w}_{i}(H_{j})>\sigma+\epsilon^{\prime}\) or \(\hat{w}_{j}(H_{i})\leq\sigma+\epsilon^{\prime}\)then
23: Mark \(H_{j}\) in \(\mathcal{Q}\).
24:return "Start over."
25:return "\(\sigma<\mathsf{OPT}\)". ```

**Algorithm 5** finds a \((\sigma+\epsilon^{\prime},\ 3\,\sigma+3\,\sigma^{\prime},\ m)\)-seed

**Theorem 8**.: _Suppose that we are given a class of \(n\) hypotheses, \(\mathcal{H}\), a rate parameter \(\eta\in(0,1/4)\), two parameters \(\sigma,\delta\in(0,1)\), and we have access to \(\hat{w}_{j}(H_{i})\) with error at most \(\epsilon^{\prime}\) for every \(i,j\in[n]\). Algorithm 5 queries \(O\left(n\log(n/\delta)/\eta\right))\) many \(w_{i}(H_{j})\) and runs in \(O\left((n\log(n/\delta)/\eta)\cdot T_{q}\right)\) time. Also, it can be implemented in \(O(1)\) rounds of adaptivity. This algorithm returns a hypothesis \(H_{\ell}\), declares \(\sigma<\mathsf{OPT}\), or returns "start over" for which the following guarantees hold with probability \(1-\delta\):_

* _If_ \(\sigma\geq\mathsf{OPT}\)_, then the algorithm does not declare that_ \(\sigma\) _is less than_ \(\mathsf{OPT}\)_._
* _If the algorithm returns a hypotheses_ \(H_{\ell}\)_, then_ \(H_{\ell}\) _is a_ \((\sigma+\epsilon^{\prime},\ 3\,\sigma+3\,\epsilon^{\prime},\ m)\)_-seed where_ \(m\coloneqq\lfloor\eta\cdot n\rfloor\)_._
* _If the algorithm returns "start over." with probability_ \(1-\delta\)_, it marks_ \((1-2\eta)\) _fraction of the hypotheses in_ \(\mathcal{Q}\)_._

Proof.: Our first claim is that if \(\sigma\) is at least \(\mathsf{OPT}\), then we do not declare otherwise. This is due to the fact that the discrepancy between \(H_{i^{*}}\) and \(P\) on any subset of the domain, including the Scheffe sets, will not exceed \(\mathsf{OPT}\). Hence, \(\hat{w}_{j}(H_{i^{*}})\) is at most \(\mathsf{OPT}+\epsilon^{\prime}\leq\sigma+\epsilon^{\prime}\), and we will not mark \(H_{i^{*}}\) in Line 7, Line 10, nor Line 15. That is, for at least one hypothesis, i.e., \(H_{i^{*}}\), the "if" statement in Line 16 holds. And, we return a seed or "start over." (And, we will never reach Line 25.)Next, we show that if the algorithm outputs \(H_{i}\), then it is a \((\sigma+\epsilon^{\prime},\,3\,\sigma+3\,\epsilon^{\prime},\,m)\)-seed with high probability. First, observe that \(\hat{W}(H_{i})\) must be at most \(3\,\sigma+3\,\epsilon^{\prime}\) due to the "if" condition in Line 18, satisfying one of the two conditions we need for our desired seed in Definition 3.1.

Now, we show the next required condition for \(H_{i}\) to be a good seed holds as well: \(|\mathcal{M}_{\sigma+\epsilon^{\prime},3\,\sigma+3\,\epsilon^{\prime}}(H_{i} )|\leq m\). This condition requires that the number of \(H_{j}\)'s in \(\mathcal{H}\) for which \(\hat{w}_{j}(H_{i})\) is in \((\sigma+\epsilon^{\prime},3\,\sigma+3\,\epsilon^{\prime}]\) is bounded by \(m\). We have already established that \(\hat{w}_{j}(H_{i})\) for every \(j\in[n]\) is bounded by \(3\,\sigma+3\,\epsilon^{\prime}\) since \(\hat{W}(H_{i})\) is at most \(3\,\sigma+3\,\epsilon^{\prime}\). Hence, we only need to show that there are at most \(m\) hypotheses \(H_{j}\) such that \(w_{j}(H_{i})\) is larger than \(\sigma+\epsilon^{\prime}\).

Now, if there are more than \(m\) hypothesis \(H_{j}\) in \(\mathcal{H}\) such that \(\hat{w}_{j}(H_{i})>\sigma+\epsilon^{\prime}\). Now, if we sample \(t\coloneqq\left\lceil 8\,n\log\left(2n/\delta\right)/m\right\rceil\) hypotheses, we should observe at least one \(H_{j}\) for which \(\hat{w}_{j}(H_{i})\) is larger than \(\sigma+\epsilon^{\prime}\), with probability at least \(1-\delta/(2n)\). However, we know that such a hypothesis was never observed because we did not mark \(H_{i}\) earlier. By the union bound, with a probability of at least \(1-\delta/2\), no such \(H_{i}\) exists. Hence, with probability at least \(1-\delta/2\), \(H_{i}\) is an \((\sigma+\epsilon^{\prime},3\,\sigma+3\epsilon^{\prime},m^{\prime})\)-seed as promised in the statement of the lemma.

Next, assume we output "start over." after finding an unmarked hypothesis \(H_{i}\). In this case, we know that \(H_{i}\) is not marked and \(\hat{W}(H_{i})>2\sigma+3\epsilon^{\prime}\). Note that we have sampled \(t\) edges of \(H_{i}\), and we never see \(w_{j}(H_{i})>\sigma+\epsilon^{\prime}\). Using a very similar argument as we have above: one can show there cannot be more than \(m^{\prime}=\left\lceil\eta\cdot|Q|\right\rceil\) for which \(w_{j}(H_{i})>\sigma+\epsilon^{\prime}\) with probability \(1-\delta/2\) (for every \(H_{i}\)). That means, we will mark \(|\mathcal{Q}|-m^{\prime}\) many hypothesis in \(\mathcal{Q}\). If \(|\mathcal{Q}|>t\geq 1/\eta\), this implies that \((1-2\eta)\) fraction of the hypothesis in \(\mathcal{Q}\) are removed. If \(|\mathcal{Q}|\leq t\), then we know all the hypothesis in \(\mathcal{Q}\) have \(w_{j}(H_{i})\leq\sigma+\epsilon^{\prime}\) and all of them will be marked.

Furthermore, we show that we did not wrongfully mark a hypothesis that was \(\sigma\)-close to \(P\). Observe that the condition in Line 22 holds in two cases:

Case 1: \(\hat{w}_{i}(H_{j})>\sigma+\epsilon^{\prime}\).It is straightforward to show that \(H_{j}\) is not \(\sigma\)-close to \(P\) since we have:

\[\|H_{j}-P\|_{\text{TV}}\geq w_{i}(H_{j})\geq\hat{w}_{i}(H_{j})-\epsilon^{ \prime}>\sigma\,.\]

Case 2: \(\hat{w}_{i}(H_{j})\leq\sigma+\epsilon^{\prime}\) and \(\hat{w}_{j}(H_{i})\leq\sigma+\epsilon^{\prime}\).Even though \(\hat{w}_{\ell}(H_{j})\) is small in this case, we can indirectly deduce that \(H_{j}\) is not \(\sigma\)-close to \(P\). By the definition of \(\hat{w}_{j}(H_{i})\) and \(\hat{w}_{i}(H_{j})\), we have:

\[\|H_{i}-H_{j}\|_{\text{TV}} =|H_{i}\left(\mathcal{S}_{i,j}\right)-H_{j}\left(\mathcal{S}_{i,j }\right)|\leq|H_{i}\left(\mathcal{S}_{i,j}\right)-P\left(\mathcal{S}_{i,j} \right)|-|P\left(\mathcal{S}_{i,j}\right)-H_{j}\left(\mathcal{S}_{i,j}\right)|\] \[=w_{i}(H_{j})+w_{j}(H_{i})\leq\hat{w}_{i}(H_{j})+\hat{w}_{j}(H_{i} )+2\,\epsilon^{\prime}\leq 2\sigma+2\,\epsilon^{\prime}\,.\]

On the other hand, by the triangle inequality, we have:

\[\|P-H_{j}\|_{\text{TV}} \geq\|P-H_{i}\|_{\text{TV}}-\|H_{i}-H_{j}\|_{\text{TV}}\geq W(H_{i })-\|H_{j}-H_{i}\|_{\text{TV}}\] \[\geq\hat{W}(H_{i})-\epsilon^{\prime}-\|H_{i}-H_{j}\|_{\text{TV}} >3\,\sigma+3\,\epsilon^{\prime}-\epsilon^{\prime}-(2\,\sigma+2\,\epsilon^{ \prime})=\sigma\,.\]

Therefore, in both of the cases above, we do not mark an \(\sigma\)-close distribution to \(P\).

By the union bound over all the steps, our guarantees hold with probability \(1-\delta\). In addition, it is not hard to see that the running time of the algorithm is \(O\left((n\cdot t+n)\cdot T_{q}\right)=O(n\cdot(\log(n/\delta))\cdot T_{q}/\eta)\) time.

Rounds of adaptivity: It is straight forward to show that this algorithm can be implemented with a constant round of adaptivity. Let \(\mathcal{Q}_{0}\) denote the initial state of unmarked hypotheses at the algorithm's outset. Since hypotheses are marked in sequence, in round \(i\), the set of unmarked hypotheses is \(\mathcal{Q}_{i}\coloneqq\mathcal{Q}_{0}\cap H_{i},H_{i+1},\ldots,H_{n}\). As a result, we can agree on the set of random \(H_{j}\)'s in \(\mathcal{Q}\) and \(\mathcal{H}\) and request \(\hat{w}_{j}(H_{i})\) within a single round. If the size of \(|\mathcal{Q}_{i}|\) is at most \(t\), then we include all \(\hat{w}_{j}(H_{i})\) for every \(i\in n\) and \(H_{j}\in\mathcal{Q}\). Next, upon discovering an unmarked \(H_{i}\), we initiate another round of adaptivity to request all \(\hat{w}_{j}(H_{i})\) and \(\hat{w}_{j}(H_{i})\) for all \(j\in[n]\). This enables the implementation of the rest of the algorithm.

### Boosting a seed

In this section, we provide an algorithm that receives a \((\kappa\,\sigma+\epsilon^{\prime},(\kappa+2)\sigma+5\,\epsilon^{\prime},m)\)-seed and aims to find a \((\kappa^{\prime}\,\sigma+\epsilon^{\prime},(\kappa^{\prime}+2)\sigma+5\, \epsilon^{\prime},m^{\prime})\)-seed with smaller \(m^{\prime}\). That means, the size of \(\mathcal{M}\) for this new seed is smaller which brings us closer to have enough time to fully investigate the set \(\mathcal{M}\) for a future seed. Our algorithm runs in \(\tilde{O}(n\cdot T_{q}/\eta)\) time where \(\eta\approx m^{\prime}/m\) is our rate parameter. This running time is adaptive to our budget. If we are aiming for \(\tilde{O}(n)\) algorithm, we find a seed such that \(m^{\prime}\) is only smaller than \(m\) by a constant factor (constant \(\eta\)); Alternatively, if we have more time, we can decrease \(m^{\prime}\) with a faster rate (smaller \(\eta\)).

While our main goal is to find a better seed, our algorithm may not always be able to do so; However, it makes progress towards the end goal of the algorithm one way or the other. There are three possible outcomes for our algorithm:

1. The algorithm finds an accurate enough \(\hat{H}\) as the final output of the algorithm.
2. The algorithm finds a better seed as we aimed for.
3. The algorithm marks \((1-2\eta)\) fraction of the unmarked hypotheses; And after that, we start over the procedure.

Although we may have regressed in last case above, it does not happen too often. Since we have only \(n\) unmarked hypotheses to begin with, and we marked a large fraction of them every time, we do not start more than \(O(\log_{1/2\eta}(n))\) times over the course of the algorithm. Our approach is presented in Algorithm 6, and we prove its performance in Theorem 9.

```
1:procedureBoost-seed(\(\mathcal{H},\ \mathcal{Q},\ H_{i},\ \sigma,\ \kappa,\ \kappa^{\prime},\ \epsilon^{\prime},\ \delta,\ \eta\), and query access to \(\hat{w}_{j}(H_{i})\)'s)
2:\(m^{\prime}\leftarrow\lfloor\eta\,m\rfloor\)
3:\(t\leftarrow\lceil 8\,n\log\left(2n/\delta\right)/m^{\prime}\rceil\)
4:for\(H_{j}\in\mathcal{M}\)do\(\triangleright\) We denote \(\mathcal{M}_{(\kappa\,\sigma+\epsilon^{\prime},\ (\kappa+2)\sigma+5\,\epsilon^{\prime}}(H_{i})\) by \(\mathcal{M}\).
5:\(\tilde{W}(H_{j})\leftarrow\textsc{Compute-}\tilde{W}(\mathcal{H},\mathcal{Q},H _{i},t)\)
6:\(H_{\ell}\leftarrow\arg\min_{H_{j}\in\mathcal{M}}\tilde{W}(H_{j})\)
7:if\(\tilde{W}(H_{\ell})>\kappa^{\prime}\,\sigma+\epsilon^{\prime}\)then
8: Return \(H_{i}\) as the final answer.
9:else
10: Compute \(\hat{W}(H_{\ell})\).
11:if\(\hat{W}(H_{\ell})\leq(\kappa^{\prime}+2)\,\sigma+5\,\epsilon^{\prime}\)then
12: Return \(H_{\ell}\) as a seed.
13:else
14:for\(H_{j}\in\mathcal{H}\)do
15:if\(\hat{w}_{\ell}(H_{j})>\sigma+\epsilon^{\prime}\) or \(\hat{w}_{j}(H_{\ell})\leq\kappa^{\prime}\,\sigma+\epsilon^{\prime}\)then
16: Mark \(H_{j}\) in \(\mathcal{Q}\).
17:return "Start over." ```

**Algorithm 6** aims to find a better seed.

**Theorem 9**.: _Suppose Algorithm 6 receives these parameters as input: \(\sigma\in\mathbb{R}_{\geq 0},\ \kappa,\ \kappa^{\prime}\geq 1,\ \eta\in(0,1/4),\ \epsilon^{ \prime},\ \delta\in(0,1)\), and two non-negative integers \(n,\ m\). Also, assume the algorithm has access to a class of \(n\) hypotheses \(\mathcal{H}\), and a set of unmarked hypotheses in \(\mathcal{Q}\subseteq\mathcal{H}\). It also receives a hypothesis \(H_{i}\in\mathcal{H}\). The algorithm has one of the three possible outcomes listed below. Now, for any setting of such input parameters, if \(H_{i}\) is a \((\kappa\,\sigma+\epsilon^{\prime},(\kappa+2)\sigma+5\,\epsilon^{\prime},m)\) seed, then the following guarantees hold for the outcome of Algorithm 6 with a probability of at least \(1-\delta\):_

1. _If the algorithm outputs a hypothesis_ \(\hat{H}\) _as the final answer, then we have for every_ \(\epsilon\geq 6\,\epsilon^{\prime}\)_:_ \[\|\hat{H}-P\|_{\text{TV}}\leq\max\left(\kappa+2,\frac{\kappa+2}{\kappa^{ \prime}}+2\right)\cdot\max\left(\sigma,\mathsf{OPT}\right)+\epsilon\,.\]
2. _If the algorithm outputs a hypothesis_ \(H_{\ell}\) _as a new seed, then_ \(H_{\ell}\) _is a_ \((\kappa^{\prime}\,\sigma+\epsilon^{\prime},(\kappa^{\prime}+2)\sigma+5\, \epsilon^{\prime},m^{\prime})\)_-seed where_ \(m^{\prime}\coloneqq\lfloor\eta\,m\rfloor\)_._3. _If the algorithm requires us to start over, then we have marked at least_ \((1-2\eta)\) _of the unmarked hypotheses._

_The algorithm runs in \(O(n\cdot(\log(n/\delta))\cdot T_{q}/\eta)\) time and can be implemented in \(O(1)\) rounds of adaptivity._

Proof.: Let \(\mathcal{M}\) denote the set \(\mathcal{M}_{\kappa\,\sigma+\epsilon^{\prime},(\kappa+2)\sigma+5\,\epsilon^{ \prime}}(H_{i})\). We consider the three possible outcomes of the algorithm, and prove the algorithm satisfied the desired guarantee in each of the three cases.

Case 1: \(\tilde{\bm{W}}(H_{\bm{\ell}})>\bm{\kappa}^{\prime}\,\bm{\sigma}+\bm{\epsilon}^ {\prime}\).In this case, the algorithm will return \(H_{i}\) as the final answer. We show that the total variation distance between \(H_{i}\) and \(P\) is as desired. There are two possibilities depending on whether \(H_{i^{*}}\) is in \(\mathcal{M}\) or not.

Case 1.1: \(H_{i^{*}}\in\mathcal{M}\).Using that \(H_{\ell}\) has the minimum \(\tilde{W}(H_{\ell})\) among all the hypotheses in \(\mathcal{M}\), we can bound \(\mathsf{OPT}\) as follows:

\[\mathsf{OPT}\geq W(H_{i^{*}})\geq\hat{W}(H_{i^{*}})-\epsilon^{\prime}\geq \hat{W}(H_{\ell})-\epsilon^{\prime}\geq\tilde{W}(H_{\ell})-\epsilon^{\prime} \geq\kappa^{\prime}\,\sigma\,.\]

We prove that \(H_{i}\) is not too far from \(P\). Note that since \(H_{i}\) was an \((\kappa\,\sigma+\epsilon^{\prime},\ (\kappa+2)\,\sigma+5\,\epsilon^{\prime},\ m)\)-seed, we are guaranteed that \(\hat{W}(H_{i})\) is bounded by \((\kappa+2)\,\sigma+5\,\epsilon^{\prime}\). Using the above bound for \(\mathsf{OPT}\) and Fact C.3, we get:

\[\|H_{i}-P\|_{\mathrm{TV}} \leq W(H_{i})+2\,\mathsf{OPT}\leq\hat{W}(H_{i})+\epsilon^{\prime }+2\,\mathsf{OPT}\leq(\kappa+2)\,\sigma+6\,\epsilon^{\prime}+2\,\mathsf{OPT}\] \[\leq\left(\frac{\kappa+2}{\kappa^{\prime}}+2\right)\cdot\mathsf{ OPT}+6\,\epsilon^{\prime}\leq\left(\frac{\kappa+2}{\kappa^{\prime}}+2\right) \cdot\max(\sigma,\mathsf{OPT})+\epsilon\,.\]

Case 1.2: \(H_{i^{*}}\not\in\mathcal{M}\).Since \(H_{i}\) is a \((\kappa\,\sigma+\epsilon^{\prime},(\kappa+2)\sigma+5\,\epsilon^{\prime},m)\)-seed, \(\hat{w}_{i^{*}}(H_{i})\) is bounded by \(\hat{W}(H_{i})\leq(\kappa+2)\sigma+5\,\epsilon^{\prime}\). Now that \(H_{i^{*}}\) is not in \(\mathcal{M}\), it must be the case that \(\hat{w}_{i^{*}}(H_{i})\) is bounded by \(\kappa\,\sigma+\epsilon^{\prime}\). In this case, we have the following bound for the total variation distance between \(H_{i}\) and \(P\) via Fact C.3:

\[\|H_{i}-P\|_{\mathrm{TV}} \leq w_{i^{*}}(H_{i})+2\,\mathsf{OPT}\leq\hat{w}_{i^{*}}(H_{i})+ \epsilon^{\prime}+2\,\mathsf{OPT}\leq\kappa\,\sigma+2\,\epsilon^{\prime}+2\, \mathsf{OPT}\] \[\leq(\kappa+2)\max(\sigma,\mathsf{OPT})+\epsilon\,.\]

Case 2: \(\tilde{\bm{W}}(H_{\bm{\ell}})\leq\bm{\kappa}^{\prime}\,\bm{\sigma}+\bm{ \epsilon}^{\prime}\) and \(\tilde{\bm{W}}(H_{\bm{\ell}})\leq(\bm{\kappa}^{\prime}+2)\,\bm{\sigma}+5\, \bm{\epsilon}^{\prime}\).In this case, we output \(H_{\ell}\) as a new seed. Given the assumptions of this case, we show that \(H_{\ell}\) is a \((\kappa^{\prime}\,\sigma+\epsilon^{\prime},(\kappa^{\prime}+2)\sigma+5\, \epsilon^{\prime},m^{\prime})\)-seed. First, \(\hat{W}(H_{\ell})\) is at most \((\kappa^{\prime}+2)\,\sigma+5\,\epsilon^{\prime}\) implying the first desired property for being an \((\kappa^{\prime}\,\sigma+\epsilon^{\prime},(\kappa^{\prime}+2)\sigma+5\, \epsilon^{\prime},m^{\prime})\)-seed holds for \(H_{\ell}\). Next, we show the second desired property for \(H_{\ell}\): the size of \(\mathcal{M}_{\kappa^{\prime}\,\sigma+\epsilon^{\prime},\ (\kappa^{\prime}+2)\sigma+5\,\epsilon^{\prime}}(H_{\ell})\) is bounded by \(m^{\prime}\). At a high level, we expect the size of this set to be small; Otherwise, we would have observed one of these large \(\hat{w}_{\ell}(H_{j})\) when we have sampled \(t\) hypotheses to compute \(\tilde{W}(H_{\ell})\).

Suppose \(H_{\ell}\) has more than \(m^{\prime}\) edges with \(\hat{w}_{j}(H_{\ell})>\kappa^{\prime}\,\sigma+\epsilon^{\prime}\). Now, if we sample \(t\coloneqq\lceil 8\,n\log\left(2n/\delta\right)/m^{\prime}\rceil\) hypotheses, we should observe at least one \(H_{j}\) for which \(\hat{w}_{j}(H_{\ell})\) is larger than \(\kappa^{\prime}\,\sigma+\epsilon^{\prime}\), with probability at least \(1-\delta/(2n)\). However, we know that such a hypothesis was never observed because \(\tilde{W}(H_{\ell})\), which is the maximum of \(\hat{w}_{j}(H_{\ell})\)'s, is bounded from above by \(\kappa^{\prime}\,\sigma+\epsilon^{\prime}\). By the union bound, with a probability of at least \(1-\delta/2\), no such \(H_{\ell}\) exists. Hence, with probability at least \(1-\delta/2\), \(H_{\ell}\) is an \((\kappa^{\prime}\,\sigma+\epsilon^{\prime},(\kappa^{\prime}+2)\,\sigma+5 \epsilon^{\prime},m^{\prime})\)-seed as promised in the statement of the lemma.

Case 3: \(\tilde{W}(H_{\ell})\leq\kappa^{\prime}\,\sigma+\epsilon^{\prime}\) and \(\tilde{W}(H_{\ell})>(\kappa^{\prime}+2)\,\sigma+5\,\epsilon^{\prime}\).In this case the algorithm marks any unmarked hypothesis \(H_{j}\) for which \(\hat{w}_{\ell}(H_{j})>\sigma+\epsilon^{\prime}\) or \(\hat{w}_{j}(H_{\ell})\leq\kappa^{\prime}\,\sigma+\epsilon^{\prime}\), and we start over. First, we show that we did not wrongfully mark a hypothesis that was \(\sigma\)-close to \(P\). Observe that the condition in Line 15 holds in two cases:

Case 3.1: \(\hat{w}_{\ell}(H_{j})>\sigma+\epsilon^{\prime}\).It is straightforward to show that \(H_{j}\) is not \(\sigma\)-close to \(P\) since we have:

\[\|H_{j}-P\|_{\text{TV}}\geq w_{\ell}(H_{j})\geq\hat{w}_{\ell}(H_{j})-\epsilon ^{\prime}>\sigma\,.\]

Case 3.2: \(\hat{w}_{\ell}(H_{j})\leq\sigma+\epsilon^{\prime}\) and \(\hat{w}_{j}(H_{\ell})\leq\kappa^{\prime}\,\sigma+\epsilon^{\prime}\).Even though \(\hat{w}_{\ell}(H_{j})\) is small in this case, we can indirectly deduce that \(H_{j}\) is not \(\sigma\)-close to \(P\). By the definition of \(\hat{w}_{j}(H_{\ell})\) and \(\hat{w}_{\ell}(H_{j})\), we have:

\[\|H_{\ell}-H_{j}\|_{\text{TV}} =|H_{\ell}\left(\mathcal{S}_{\ell,j}\right)-H_{j}\left(\mathcal{ S}_{\ell,j}\right)|\leq|H_{\ell}\left(\mathcal{S}_{\ell,j}\right)-P\left( \mathcal{S}_{\ell,j}\right)|-|P\left(\mathcal{S}_{\ell,j}\right)-H_{j}\left( \mathcal{S}_{\ell,j}\right)|\] \[=w_{\ell}(H_{j})+w_{j}(H_{\ell})\leq\hat{w}_{\ell}(H_{j})+\hat{w }_{j}(H_{\ell})+2\,\epsilon^{\prime}\leq\kappa^{\prime}\sigma+\sigma+4\, \epsilon^{\prime}\,.\] On the other hand, by the triangle inequality, we have: \[\|P-H_{j}\|_{\text{TV}} \geq\|P-H_{\ell}\|_{\text{TV}}-\|H_{\ell}-H_{j}\|_{\text{TV}}\geq W (H_{\ell})-\|H_{j}-H_{\ell}\|_{\text{TV}}\] \[\geq\tilde{W}(H_{\ell})-\epsilon^{\prime}-\|H_{\ell}-H_{j}\|_{ \text{TV}}>(\kappa^{\prime}+2)\,\sigma+4\,\epsilon^{\prime}-((\kappa^{\prime}+ 1)\,\sigma+4\,\epsilon^{\prime})=\sigma\,.\]

Therefore, in both of the cases above, we do not mark an \(\sigma\)-close distribution to \(P\).

Second, we claim that we mark at least \((1-2\,\eta)\) fraction of the unmarked hypotheses in \(\mathcal{Q}\). Recall that when we compute \(\tilde{W}(H_{\ell})\) we also sample \(t\) hypotheses from the set of unmarked hypotheses, and we did not observe any edge \(\hat{w}_{j}(H_{\ell})\) that is larger than \(\kappa^{\prime}\,\sigma+\epsilon^{\prime}\). With a very similar argument we had above, with probability \(1-\delta/(2n)\), we do not have more than \(m^{\prime\prime}\coloneqq\lceil\eta\cdot|\mathcal{Q}|\cdot m/n\rceil\leq\lceil \eta\cdot|Q|\rceil\) hypotheses in \(\mathcal{Q}\) such that \(\hat{w}_{j}(H_{\ell})>\kappa^{\prime}\,\sigma+\epsilon^{\prime}\). Otherwise, we would have seen one of these edges, and \(\tilde{W}(H_{\ell})\) would have been larger. By the union bound, this fact holds for every \(H_{\ell}\). Hence, the "if" condition in Line 15 holds for over \(|\mathcal{Q}|-\lceil\eta\cdot|\mathcal{Q}|\rceil\) of the unmarked hypotheses in \(\mathcal{Q}\), and the algorithm mark them. If \(|\mathcal{Q}|\geq 1/\eta\), it is easy to show that \(|\mathcal{Q}|-\lceil\eta\cdot|\mathcal{Q}|\rceil\) is at least \((1-2\,\eta)\cdot|\mathcal{Q}|\). Thus, we mark at least \((1-2\,\eta)\) fraction of the hypotheses in \(\mathcal{Q}\) as we have claimed. Now, if \(|Q|\leq 1/\eta\), it is easy to show that \(|Q|<t\). Hence, the algorithm involves all the \(H_{j}\)'s in \(\mathcal{Q}\) to compute \(\hat{W}(H_{\ell})\). Therefore, every single \(\hat{w}_{j}(H_{\ell})\) is at most \(\kappa^{\prime}\sigma+\epsilon^{\prime}\) and we mark all the hypotheses in \(\mathcal{Q}\).

By the union bound over all the steps, our guarantees hold with probability \(1-\delta\). In addition, it is not hard to see that the running time of the algorithm is \(O\left((m\cdot t+n)\cdot T_{q}\right)=O(n\cdot\left(\log(n/\delta)\right)\cdot T _{q}/\eta)\) time.

Rounds of adaptivity:Our algorithm can be implemented in constant rounds of adaptivity. To compute \(\tilde{W}\)'s, we can preselect the random \(H_{j}\)'s. If \(\mathcal{Q}\) contains fewer than \(t+1\) hypotheses, we include every \(\hat{w}_{j}(H_{i})\) for each \(H_{j}\in\mathcal{Q}\) and \(i\in[n]\). Upon discovering \(H_{\ell}\), another round of adaptivity allows us to query every \(\hat{w}_{j}(H_{\ell})\) and \(\hat{w}_{\ell}(H_{j})\) to calculate \(\hat{W}(H_{\ell})\) and mark the hypotheses in \(\mathcal{Q}\).

Preliminary facts and lemmas

**Fact C.1**.: _For all distribution \(P\) and for all probability event \(\mathcal{E}\) defined under \(P\), we have:_

\[\|P-P_{|\mathcal{E}}\|_{\mathcal{TV}}=1-P(\mathcal{E})\,,\]

_where \(P_{|\mathcal{E}}\) denotes the probability distribution \(P\) when conditioned on the event \(\mathcal{E}\)._

**Fact C.2**.: _For every pair of distributions \(H_{i}\) and \(H_{j}\), we have:_

\[\|H_{i}-H_{j}\|_{\mathcal{TV}}=|H_{i}\left(\mathcal{S}_{ij}\right)-H_{j} \left(\mathcal{S}_{ij}\right)|\;.\]

The following fact is adapted from [11, 12, 13].

**Fact C.3**.: _Suppose \(H_{i^{*}}\) is the closest hypothesis to \(P\) in \(\mathcal{H}\)(i.e., \(\|H_{i^{*}}-P\|_{\mathcal{TV}}\)). For every pair of hypotheses \(H_{i}\) and \(H_{j}\), the following holds:_

1. \(\|H_{i}-P\|_{\mathcal{TV}}\leq w_{j}(H_{i})+2\|H_{j}-P\|_{\mathcal{TV}}\,,\)__
2. \(\|H_{i}-P\|_{\mathcal{TV}}\leq w_{i^{*}}(H_{i})+2\,\mathsf{OPT}\,,\)__
3. \(\|H_{i}-P\|_{\mathcal{TV}}\leq W(H_{i})+2\,\mathsf{OPT}\,.\)__

Proof.: The proof is based on triangle inequality, and the definitions of \(w_{j}(H_{i})\)'s and \(\mathsf{OPT}\). For every \(H_{j}\) in \(\mathcal{H}\), we have:

\[\|H_{i}-P\|_{\mathsf{TV}} \leq\|H_{i}-H_{j}\|_{\mathsf{TV}}+\|H_{j}-P\|_{\mathsf{TV}}=|H_{i }\left(\mathcal{S}_{i,j}\right)-H_{j}\left(\mathcal{S}_{i,j}\right)|+\|H_{j}- P\|_{\mathsf{TV}}\] \[\leq|H_{i}\left(\mathcal{S}_{i,j}\right)-P\left(\mathcal{S}_{i,j }\right)|+|P\left(\mathcal{S}_{i,j}\right)-H_{j}\left(\mathcal{S}_{i,j}\right) |+\|H_{j}-P\|_{\mathsf{TV}}\] \[=w_{j}(H_{i})+w_{i}(H_{j})+\|H_{j}-P\|_{\mathsf{TV}}\] \[\leq w_{j}(H_{i})+2\|H_{j}-P\|_{\mathsf{TV}}\,.\]

Hence, Item 1 is proved. Now, if we set \(H_{j}\) to be \(H_{i^{*}}\), then \(\|H_{i^{*}}-P\|_{\mathsf{TV}}\) is equal to \(\mathsf{OPT}\) implying Item 2. Item 3 is concluded from Item 2 and the fact that \(w_{i^{*}}(H_{i})\) is upper bounded by \(W(H_{i})\). 

**Fact C.4**.: _Suppose we are given six parameters \(a,b,a^{\prime},b^{\prime}\in\mathbb{R}_{\geq 0}\), and \(m,m^{\prime}\in\mathbb{Z}_{\geq 0}\). If \(a\leq a^{\prime}\), \(b\leq b^{\prime}\), and \(m\leq m^{\prime}\), every \((a,b,m)\)-seed is also an \((a^{\prime},b^{\prime},m^{\prime})\)-seed._

**Fact C.5**.: _Suppose we have a set of \(n\) hypotheses \(\mathcal{H}\), and a predicate, \(R(H):\mathcal{H}\to\{0,1\}\), that maps the hypotheses in \(\mathcal{H}\) to zero or one. Assume \(\mathcal{H}\) contains more than \(m\) hypotheses with \(R(H)=1\) for an arbitrary integer parameter \(0\leq m<n\). If we draw \(s\geq 8\,n\,\log(\delta^{-1})/m\) hypotheses from \(\mathcal{H}\) uniformly at random, we will observe at least one hypothesis with \(R(H)=1\) with probability at least \(1-\delta\)._

Proof.: Let \(p\) denote the fraction of such hypothesis in \(\mathcal{H}\) with \(R(H)=1\). Observe that \(p>0\) since there are at least \(m+1\) such hypothesis in \(\mathcal{H}\). Then, using the Chernoff bound, we have:

\[\mathbf{Pr[}r=0\big{]} \leq\mathbf{Pr[}r<\frac{s\,p}{2}\big{]}=\mathbf{Pr[}\frac{r}{s}< \left(1-\frac{1}{2}\right)\cdot p\,\big{]}\] \[\leq\exp\left(-\frac{s\,p}{8}\right)<\exp\left(-\frac{s\,m}{8\,n} \right)\leq\delta\,.\]

## Appendix D Data structure for marked and unmarked hypotheses

Here, we describe a simple data structure that allows us to efficiently keep track of marked and unmarked hypotheses. Our data structure consists of two integers, n and t, and two arrays of size n: index[] and list[]. Here, n represents the number of hypotheses that the data structure supports. The list array is an arbitrary ordering of integers from 1 to n. Throughout the operations of this data structure, we preserve the following guarantee: the value of index[i] indicates where to find element i in the list. In other words, list[index[i]] is always i. The integer t serves as a threshold quantity. The first t numbers in the list correspond to unmarked hypotheses, while the remaining numbers represent marked hypotheses. Initially, list is a list of integers from 1 to n in ascending order. And, for every hypothesis \(H_{i}\), index[i] is set to i. This data structure supports the following operations:

* Initialization ds(size): To initialize the data structure, we set n and t to size. Next, we set list to be a list of integers from l to n in ascending order. To ensure consistency with the list, each index[i] is set to i.
* mark(i): To mark hypothesis \(H_{i}\), we swap the element at index i with the element at index t in the list array, and then decrement t by l.
* is_marked(i): Given our definition, if index[i] is less than or equal to t, then \(H_{i}\) is unmarked; otherwise, it is marked.
* is_all_marked(): If t is equal to zero, then it means all the hypotheses are marked.
* random_unmarked(): To select a random unmarked hypothesis, we generate a random integer r between l and t, and output the hypothesis at list[r].
* random_marked(): To select a random marked hypothesis, we follow the same process as for an unmarked hypothesis, except that r is generated between t+1 and n.

It is easy to see that the initialization takes \(O(n)\) time, and the rest of the operations only take \(O(1)\) time.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our results are a theoretical contribution to the field of learning theory. The paper, including the appendix, contains all the algorithms, theorem statements, and their proofs. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Please see the discussion in Section 1.2 regarding potential improvements and future directions. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We have clearly stated our model and our assumptions. All the proofs are included in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: We do not have any experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: We do not have any experimental results. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: We do not have any experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: We do not have any experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: We do not have any experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our results are a theoretical formulation of a fundamental problem in statistics and learning theory. They can be viewed as a computationally efficient version of existing results. Given the theoretical nature of our findings, we do not anticipate any harm to society. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Given the theoretical nature of our findings, we do not anticipate broader societal impact for this paper. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not have any experimental results. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We did not use any existing assets, other than the publicly available articles that we have cited. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The contributions of this paper are theoretical. As is standard in the field, we will make our results publicly accessible and do not anticipate generating revenue from them. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not have any crowdsourcing experiments nor human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not have any experiments involving crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.