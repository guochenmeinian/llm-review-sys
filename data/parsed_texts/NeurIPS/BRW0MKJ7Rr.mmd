# Action Gaps and Advantages in Continuous-Time Distributional Reinforcement Learning

Harley Wiltzer

Mila-Quebec AI Institute

McGill University

&Marc G. Bellemare

Mila-Quebec AI Institute

McGill University

&David Meger

McGill University

&Patrick Shafto

Rutgers University-Newark

&Yash Jhaveri

Rutgers University-Newark

Equal contribution. Correspondence to harley.wiltzer@mail.mcgill.ca.

###### Abstract

When decisions are made at high frequency, traditional reinforcement learning (RL) methods struggle to accurately estimate action values. In turn, their performance is inconsistent and often poor. Whether the performance of distributional RL (DRL) agents suffers similarly, however, is unknown. In this work, we establish that DRL agents _are_ sensitive to the decision frequency. We prove that action-conditioned return distributions collapse to their underlying policy's return distribution as the decision frequency increases. We quantify the rate of collapse of these return distributions and exhibit that their statistics collapse at different rates. Moreover, we define distributional perspectives on action gaps and advantages. In particular, we introduce the _superiority_ as a probabilistic generalization of the advantage--the core object of approaches to mitigating performance issues in high-frequency value-based RL. In addition, we build a superiority-based DRL algorithm. Through simulations in an option-trading domain, we validate that proper modeling of the superiority distribution produces improved controllers at high decision frequencies.

## 1 Introduction

In many real-time deployments of reinforcement learning (RL)--quantitative finance, robotics, and autonomous driving, for instance--the state of the environment evolves continuously in time, but policies make decisions at discrete timesteps (\(h\) units of time apart) [28]. In such systems, the performance of value-based agents is sensitive to the frequency \(\omega:=\nicefrac{{1}}{{h}}\) with which actions are taken. In particular, action values become indistinguishable as the time between actions decreases. In turn, in high-frequency settings, Baird demonstrated that action value estimates are susceptible to noise and approximation error [20]. Moreover, Tallec et al. exhibited that the performance of popular deep \(Q\)-learning agents is inconsistent and often poor [34].

In order to remedy this sensitivity, Baird proposed the advantage function and advantage-based variants of \(Q\)-learning, Advantage Updating (AU) [20] and Advantage Learning (AL) [2]. Unlike action values, advantages (appropriately rescaled) do not become indistinguishable as decision frequency increases. As a result, Baird, in [20, 2], demonstrated that advantage-based agents can learn faster and be more resilient to noise than their action value-based counterparts. Furthermore, Tallec et al., in [34], exhibited that their extension of AU, Deep Advantage Updating (DAU), works efficiently over a wide range of timesteps and environments, unlike standard deep \(Q\)-learning approaches.

While advantage-based approaches to RL have demonstrated robustness to decision frequency, in this work, we establish that they are nevertheless sensitive to the frequency with which actionsare taken. This discovery arises as we answer the question: to what extent is the performance of distributional RL (DRL) agents sensitive to decision frequency? To this end, we build theory within the formalism of continuous-time RL where environmental dynamics are governed by SDEs, as in [25]. Additionally, we validate our theory empirically through simulations. Specifically, we make the following four contributions:

**Distributional Action Gap.** First, we extend notions of action gap to the realm of DRL. Precisely, we consider the minimal distance between pairs of action-conditioned distributions under metrics on the space of probability measures on \(\mathbb{R}\). We observe that some metrics are viable for this extension, while others are not. This formalism sets the stage for analyzing the influence of individual actions as well as decision frequency on, for example, an agent's return distributions.

**Collapse of Distributional Control at High Frequency.** Second, we establish tight bounds on the distributional action gaps of _\(h\)-dependent action-conditioned return distributions_--return distributions induced by applying a specific initial action for \(h\) units of time. We prove that these distributional action gaps not only collapse, as \(h\) tends to zero, but do so at a _slower rate_ than action-value gaps. On one hand, therefore, distributional \(Q\)-learning algorithms are susceptible to the same failures as \(Q\)-learning in continuous-time RL. On the other hand, however, remedies to these failures transliterated to distributional \(Q\)-learning algorithms are unlikely to succeed, because the means of these return distributions collapse _faster_ than their other statistics.

**Distributional Superiority.** Third, we propose an axiomatic construction of a distributional analogue of the advantage, which we call the _superiority_. Leveraging our analysis of \(h\)-dependent action-conditioned returns and their distributional action gaps, we present a frequency-scaled superiority distribution that enables greedy action selection at any fixed decision frequency.

**A Distributional Action Gap-Preserving Algorithm.** Fourth, we propose an algorithm that learns the superiority distribution from data. Empirically, we demonstrate that our algorithm maintains the ability to perform policy optimization at high frequencies more reliably than existing methods.

## 2 Setting

**Notation:** Spaces will either be subsets of Euclidean space or discrete. Measurability, in the former case, will be with respect to the Borel sigma algebra; in the latter case, it will be with respect to the power set. The set of probability measures over a space \(\mathsf{Y}\) will be denoted by \(\mathscr{P}(\mathsf{Y})\). Functions on spaces are assumed to be measurable. For \(f:\mathsf{Y}\to\mathsf{Z}\) and \(\mu\in\mathscr{P}(\mathsf{Y})\), the _push forward_ of \(\mu\) through/by \(f\), \(f_{\#}\mu\in\mathscr{P}(\mathsf{Z})\), is defined by \(f_{\#}\mu:=\mu\circ f^{-1}\). For a random variable \(X\), defined implicitly on some probability space \((\Omega,\mathcal{F},\mathsf{P})\), we write \(\operatorname{law}(X):=X_{\#}\mathsf{P}\) to denote the law of \(X\); the notation \(X=\operatorname{law}Y\) is shorthand for \(\operatorname{law}(X)=\operatorname{law}(Y)\). For any \(\mu\in\mathscr{P}(\mathbb{R})\), the _quantile function of \(\mu\)_, \(F_{\mu}^{-1}\), is defined by \(F_{\mu}^{-1}(\tau):=\inf_{z}\{F_{\mu}(z)\geq\tau\}\), where \(F_{\mu}(z)\) is the CDF of \(\mu\).

### Continuous-Time RL

Here we give a brief introduction to the technical aspects of continuous-time RL, a la [25]. We provide additional exposition and references in Appendix A. For any reader looking to defer some of this technical introduction, we summarize the core objects of interest at the end of Section 2.1.1.

#### 2.1.1 MDPs

Continuous-time Markov Decision Processes (MDPs) are defined by three spaces and four measurable functions: a time interval \(\mathsf{T}:=[0,T]\) with \(T\in(0,\infty)\) or \(\mathsf{T}:=[0,T)\) with \(T=\infty\), a state space \(\mathsf{X}\subset\mathbb{R}^{n}\), an action space \(\mathsf{A}\), a drift \(b:\mathsf{T}\times\mathsf{X}\times\mathsf{A}\to\mathbb{R}^{n}\), a diffusion \(\sigma:\mathsf{T}\times\mathsf{X}\times\mathsf{A}\to\mathbb{R}^{n\times n}\), a reward \(r:\mathsf{T}\times\mathsf{X}\to\mathbb{R}\), and a terminal reward \(f:\mathsf{X}\to\mathbb{R}\).3 The pair \((b,\sigma)\) govern the environment's dynamics by a family of SDEs parameterized by \(a\in\mathsf{A}\),

Footnote 3: We work with action-independent rewards. This is the norm in continuous-time DRL.

\[\operatorname{d}\!X_{t}^{a}=b(t,X_{t}^{a},a)\operatorname{d}\!t+\sigma(t,X_{t }^{a},a)\operatorname{d}\!B_{t}.\] (2.1)

Here \((B_{t})_{t\geq 0}\) is an \(n\)-dimensional Brownian motion. In turn, any solution to (2.1) collects the state paths of an agent that chooses action \(a\) at every time, regardless of the state they are in.

As is done in discrete-time RL, an agent might consider the induced Markov Reward Process (MRP) derived from a policy \(\pi:\mathsf{T}\times\mathsf{X}\to\mathscr{P}(\mathsf{A})\).4 The dynamics of a policy-induced MRP (with policy \(\pi\))are governed by the SDE

\[\mathrm{d}X_{t}^{\pi}=b^{\pi}(t,X_{t}^{\pi})\,\mathrm{d}t+\sigma^{\pi}(t,X_{t}^{ \pi})\,\mathrm{d}B_{t}.\] (2.2)

Here, following [38, 15, 16], the policy-averaged coefficients \(b^{\pi}\) and \(\sigma^{\pi}\) are defined by

\[b^{\pi}(t,x):=\int_{\mathsf{A}}b(t,x,a)\,\pi(\mathrm{d}a\,|\,t,x)\quad\text{ and}\quad\sigma^{\pi}(t,x):=\left(\int_{\mathsf{A}}\sigma\sigma^{\top}(t,x,a)\, \pi(\mathrm{d}a\,|\,t,x)\right)^{\nicefrac{{1}}{{2}}}.\] (2.3)

Thus, solutions to (2.2) collect the paths of an agent following policy \(\pi\).

A class of policies central to our study is those that fix an action \(a\) from some time \(t\) for a given _persistence horizon_\(h\).

**Definition 2.1**.: _Given \(h>0\) and \(a\in\mathsf{A}\), a policy \(\pi\) is said to be \((h,a)\)-persistent at time \(t\in\mathsf{T}\) if \(\pi(\cdot\,|\,s,y)=\delta_{a}\) for all \((s,y)\in[t,t+h)\times\mathsf{X}\)._

In particular, given a policy \(\pi\), we will consider \((h,a)\)_-persistent modifications of \(\pi\)_: for \(t\in\mathsf{T}\),

\[\pi|_{h,a,t}(\cdot\,|\,s,y):=\begin{cases}\delta_{a}&\text{if }s\in[t,t+h)\\ \pi(\cdot\,|\,s,y)&\text{if }s\notin[t,t+h).\end{cases}\]

These policies will help us understand the influence of taking actions relative to others as well as to those taken by \(\pi\). We assume \(h\) is small enough so that \(t+h\in\mathsf{T}\).

In order to guarantee the global-in-time existence and uniqueness of solutions to our SDEs (2.1) and (2.2), we make two sets of assumptions.

**Assumption 2.2**.: _The functions \(b\) and \(\sigma\) have linear growth and are Lipschitz in state, uniformly in time and action: a finite, positive constants \(C_{2.2}\) exists such that_

\[\sup_{t,a}|b(t,x,a)|+\sup_{t,a}|\sigma(t,x,a)|\leq C_{2.2}(1+|x|) \quad\forall x\in\mathsf{X};\quad\text{and}\] \[\sup_{t,a}|b(t,x,a)-b(t,y,a)|+\sup_{t,a}|\sigma(t,x,a)-\sigma(t, y,a)|\leq C_{2.2}|x-y|\quad\forall x,y\in\mathsf{X}.\]

**Assumption 2.3**.: _The averaged coefficient functions \(b^{\pi}\) and \(\sigma^{\pi}\) are Lipschitz in state, uniformly in time: a finite, positive constant \(C_{2.3}\) exists such that_

\[\sup_{t}|b^{\pi}(t,x)-b^{\pi}(t,y)|+\sup_{t}|\sigma^{\pi}(t,x)-\sigma^{\pi}( t,y)|\leq C_{2.3}|x-y|\quad\forall x,y\in\mathsf{X}.\]

These assumptions are standard in the analysis of continuous-time RL, optimal control, and SDEs [12, 26, 38, 16, 42]. Since \(\pi\) is a function of state, we note that Assumption 2.3 is not a direct consequence of Assumption 2.2. The coefficients \(b^{\pi}\) and \(\sigma^{\pi}\) satisfy the conditions of Assumption 2.3 provided \(b\) and \(\sigma\) satisfy some (also standard) additional regularity conditions and \(\pi\) satisfies some regularity conditions. These technical details are discussed in Appendix A.2.

In summary, in continuous-time RL, there are three stochastic processes of interest: \((X_{s}^{\bullet})_{s\geq t}\) with \(\bullet\in\{a,\pi,\pi|_{h,a,t}\}\), all beginning at some time \(t\). These processes collect the state paths of an agent in one of three scenarios: 1. choosing action \(a\) at every state and time; 2. following a policy \(\pi\); or 3. choosing \(a\) at every state and time for the first \(h\) units of time and following \(\pi\) thereafter.

#### 2.1.2 Value Functions and their Distributions

Given a policy-induced state process \((X_{s}^{\pi})_{s\geq t}\), the (discounted) random _return_\(G^{\pi}(t,x)\) earned by \(\pi\) starting from state \(x\in\mathsf{X}\) at time \(t\in\mathsf{T}\) is defined [41] by

\[G^{\pi}(t,x):=\int_{t}^{T}\gamma^{s-t}r(s,X_{s}^{\pi})\,\mathrm{d}s+\gamma^{T- t}f(X_{T}^{\pi})\quad\text{with}\quad X_{t}^{\pi}=x,\] (2.4)

where \(f\equiv 0\) when \(\mathsf{T}=[0,\infty)\). We distinguish returns earned by \((h,a)\)-persistent modifications of policies. We call these \(h\)_-dependent action-conditioned returns_ and denote them by \(Z_{h}^{\pi}(t,x,a)\). Given \(\pi\), they are defined by

\[Z_{h}^{\pi}(t,x,a):=\int_{t}^{T}\gamma^{s-t}r(s,X_{s}^{\pi|_{h,a,t}})\, \mathrm{d}s+\gamma^{T-t}f(X_{T}^{\pi|_{h,a,t}})\quad\text{with}\quad X_{t}^{ \pi|_{h,a,t}}=x.\] (2.5)Value-based approaches in RL estimate either the _value function_\(V^{\pi}(t,x):=\mathbf{E}[G^{\pi}(t,x)]\) or the _\(h\)-dependent action-value function_\(Q_{h}^{\pi}(t,x,a):=\mathbf{E}[Z_{h}^{\pi}(t,x,a)]\).5 As distributional approaches in RL estimate the laws of returns, following [41], we define

Footnote 5: In [38] and [16], the authors established \(V^{\pi}\) and \(Q_{h}^{\pi}\) as continuous-time RL analogues of the classic value and action-value functions. We note, however, that this was done without explicitly defining \(G^{\pi}\) and \(Z_{h}^{\pi}\).

\[\eta^{\pi}(t,x):=\text{law}(G^{\pi}(t,x))\quad\text{and}\quad\zeta_{h}^{\pi} (t,x,a):=\text{law}(Z_{h}^{\pi}(t,x,a)).\]

It is important to note that only the laws of random returns (and not their representations as random variables) are observable and modeled in practice.

### \(Q\)-Learning in Continuous Time

The failure of action-value-based RL in continuous-time stems from the collapse of action values at a given state to the value of that state. Precisely, Tallec et al. and Jia and Zhou established that

\[Q_{h}^{\pi}(t,x,a)-V^{\pi}(t,x)=(H^{\pi}(t,x,a)+(\log\gamma)V^{\pi}(t,x))h+o(h),\] (2.6)

where \(H^{\pi}\in\mathbb{R}\) is independent of \(h\) (see [34] and [16] respectively).6 In a discrete action space, given a state \(x\) and time \(t\), a concise way to capture the asymptotic information of (2.6) is by considering the _action gap_[11; 5] of the associated \(h\)-dependent action values

Footnote 6: The function \(H^{\pi}\) is the _Hamiltonian_ of the classic stochastic optimal control problem defined by our MDP.

\[\mathsf{gap}(Q_{h}^{\pi},t,x):=\min_{a_{1}\neq a_{2}}|Q_{h}^{\pi}(t,x,a_{1})-Q _{h}^{\pi}(t,x,a_{2})|.\]

Anticipating (2.6), which implies that \(\mathsf{gap}(Q_{h}^{\pi},t,x)=O(h)\), Baird proposed AU wherein he estimated the _rescaled advantage function_\(A_{h}^{\pi}\) in place of \(Q_{h}^{\pi}\):

\[A_{h}^{\pi}(t,x,a):=\frac{Q_{h}^{\pi}(t,x,a)-V^{\pi}(t,x)}{h}\quad\forall(t,x,a)\in\mathsf{T}\times\mathsf{X}\times\mathsf{A}.\] (2.7)

Note that \(\mathsf{gap}(A_{h}^{\pi},t,x)=O(1)\). Tallec et al. [34] and Jia and Zhou [16], following Baird, also estimated \(A_{h}^{\pi}\) to ameliorate \(Q\)-learning in continuous time.

## 3 The Distributional Action Gap

In this section, we define a distributional notion of action gap; we prove that \(h\)-dependent action-conditioned return distributions collapse to their underlying policy's return distribution as \(h\) vanishes; and we quantify the rate of collapse of these return distributions.

**Definition 3.1**.: _Consider an MDP with discrete action space and let \(\mu:\mathsf{T}\times\mathsf{X}\times\mathsf{A}\to(\mathscr{P}(\mathbb{R}),d)\) for a metric \(d\). The \(d\) action gap of \(\mu\) at a state \(x\) and time \(t\) is given by_

\[\mathsf{distgap}_{d}(\mu,t,x):=\min_{a_{1}\neq a_{2}}d(\mu(t,x,a_{1}),\mu(t,x, a_{2})).\]

While \(\mathbb{R}\) has a canonical metric, induced by \(|\cdot|\), the space \(\mathscr{P}(\mathbb{R})\) does not. So a choice must be made, and some metrics are unsuitable. For example, in deterministic MDPs with deterministic policies, return distributions are identified by expected returns: \(\zeta_{h}^{\pi}(t,x,a)\) is the delta at \(Q_{h}^{\pi}(t,x,a)\), for all \((t,x,a)\in\mathsf{T}\times\mathsf{X}\times\mathsf{A}\). Thus, \(\mathsf{distgap}_{d}(\zeta_{h}^{\pi},t,x)\) should vanish as \(h\) decreases to zero if \(\mathsf{gap}(Q_{h}^{\pi},t,x)\) vanishes as \(h\) decreases to zero. With the total variation metric \(d=\mathrm{TV}\), for instance, this is not the case, making \(\mathrm{TV}\) unsuitable. Indeed, suppose we have a deterministic MDP with \(\mathsf{A}=\{a_{1},a_{2}\}\) and such that \(\zeta_{h}^{\pi}(t,x,a_{1})=\delta_{h}\) and \(\zeta_{h}^{\pi}(t,x,a_{2})=\delta_{0}\), for some state \(x\) and time \(t\) (see, e.g., [34]). Then \(\mathsf{distgap}_{\mathrm{TV}}(\zeta_{h}^{\pi},t,x)=1\), for all \(h>0\), yet \(\mathsf{gap}(Q_{h}^{\pi},t,x)=h\).

The _\(W_{p}\) distances_ from the theory of Optimal Transportation (see [36]), however, are suitable. They are defined via _couplings_ of distributions.

**Definition 3.2**.: _Let \(\mu,\nu\in\mathscr{P}(\mathbb{R})\). A \(\kappa\in\mathscr{P}(\mathbb{R}^{2})\) is a coupling of \(\mu\) and \(\nu\) if its first and second marginals are \(\mu\) and \(\nu\) respectively. We denote the set of these couplings by \(\mathscr{C}(\mu,\nu)\)._

**Definition 3.3**.: _Let \(\mu,\nu\in\mathscr{P}(\mathbb{R})^{\gamma}\) and \(p\in[1,\infty)\). The \(W_{p}\) distance between \(\mu\) and \(\nu\) is_

\[W_{p}(\mu,\nu):=\inf_{\kappa\in\mathscr{C}(\mu,\nu)}\left(\int_{\mathbb{R}^{2}} |z-w|^{p}\;\kappa(\mathrm{d}z\mathrm{d}w)\right)^{1/p}.\] (3.1)Any coupling attaining the infimum in (3.1) is called a _\(W_{p}\)-optimal coupling_. Henceforth, we write \(\mathsf{distgap}_{p}\) when considering \(W_{p}\) action gaps. If \(\mu\) and \(\nu\) are deltas at \(Q_{h}^{\pi}(t,x,a_{1})\) and \(Q_{h}^{\pi}(t,x,a_{2})\) respectively, then the right-hand side of (3.1) is equal to \(|Q_{h}^{\pi}(t,x,a_{1})-Q_{h}^{\pi}(t,x,a_{2})|\). Hence, in deterministic MDPs with deterministic policies, \(W_{p}\) action gaps of \(\zeta_{h}^{\pi}\) are identical to action gaps of \(Q_{h}^{\pi}\), making the \(W_{p}\) distances suitable in the above sense. In non-deterministic MDPs, the relationship between \(\mathsf{distgap}_{p}(\zeta_{h}^{\pi},t,x)\) and \(\mathsf{gap}(Q_{h}^{\pi},t,x)\) is opaque.

The following results study the \(W_{p}\) action gap of \(\zeta_{h}^{\pi}\) as a function of \(h\), lending some color to the relationship between \(\mathsf{distgap}_{p}(\zeta_{h}^{\pi},t,x)\) and \(\mathsf{gap}(Q_{h}^{\pi},t,x)\). These results all hold under Assumptions 2.2 and 2.3. Henceforth, we suppress mention of these assumptions; we do not restate them explicitly. First, we observe that \(W_{p}\) action gaps of \(\zeta_{h}^{\pi}\) are bounded from below by action gaps of \(Q_{h}^{\pi}\).

**Proposition 3.4**.: _For all \((t,x)\in\mathsf{T}\times\mathsf{X}\), we have that \(\mathsf{distgap}_{p}(\zeta_{h}^{\pi},t,x)\geq\mathsf{gap}(Q_{h}^{\pi},t,x)\)._

For a proof of this statement and any other made in this work, see Appendix B. Our next result establishes that \(W_{p}\) action gaps of \(\zeta_{h}^{\pi}\), like action gaps of \(Q_{h}^{\pi}\), vanishes for a large class of MDPs.

**Theorem 3.5**.: _If \(r\) and \(f\) are bounded, then \(\lim_{h\downarrow 0}W_{p}(\zeta_{h}^{\pi}(t,x,a),\eta^{\pi}(t,x))=0\), for all \((t,x,a)\in\mathsf{T}\times\mathsf{X}\times\mathsf{A}\); hence, \(\lim_{h\downarrow 0}\mathsf{distgap}_{p}(\zeta_{h}^{\pi},t,x)=0\)._

While Theorem 3.5 shows that the \(W_{p}\) distance between \(\zeta_{h}^{\pi}(t,x,a)\) and \(\eta^{\pi}(t,x)\) (and the \(W_{p}\) action gap of \(\zeta_{h}^{\pi}\) at \((t,x)\in\mathsf{T}\times\mathsf{X}\)) does indeed vanish as \(h\) decreases, it does not identify the rate at which it does so. Our next two theorems establish this rate.

**Theorem 3.6**.: _MDPs and policies exist in and under which, for all \((t,x,a)\in\mathsf{T}\times\mathsf{X}\times\mathsf{A}\), we have that \(W_{p}(\zeta_{h}^{\pi}(t,x,a),\eta^{\pi}(t,x))\mathbin{\raisebox{-2.15pt}{$ >$}}h^{\nicefrac{{1}}{{2}}}\) and \(\mathsf{distgap}_{p}(\zeta_{h}^{\pi},t,x)\mathbin{\raisebox{-2.15pt}{$ >$}}h^{\nicefrac{{1}}{{2}}}\)._

Finally, we prove that for a large class of MDPs (different from but overlapping with the class of MDPs captured in Theorem 3.5), the lower bound found in Theorem 3.6 is an upper bound.

**Theorem 3.7**.: _If \(r\) is Lipschitz in state, uniformly in time, \(f\) is Lipschitz, and \(T<\infty\), then \(W_{p}(\zeta_{h}^{\pi}(t,x,a),\eta^{\pi}(t,x))\mathbin{\raisebox{-2.15pt}{$ <$}}h^{\nicefrac{{1}}{{2}}}\), for all \((t,x,a)\in\mathsf{T}\times\mathsf{X}\times\mathsf{A}\); hence, \(\mathsf{distgap}_{p}(\zeta_{h}^{\pi},t,x)\mathbin{\raisebox{-2.15pt}{$ <$}}h^{\nicefrac{{1}}{{2}}}\)._

Theorems 3.6 and 3.7 demonstrate that the \(W_{p}\) distance between \(\zeta_{h}^{\pi}(t,x,a)\) and \(\eta^{\pi}(t,x)\) and the distance between \(Q_{h}^{\pi}(t,x,a)\) and \(V^{\pi}(t,x)\) are of different orders in terms of \(h\). Thus, we see that \(\mathsf{distgap}_{p}(\zeta_{h}^{\pi},t,x)\) and \(\mathsf{gap}(Q_{h}^{\pi},t,x)\) in stochastic MDPs are fundamentally different.

## 4 Distributional Superiority

In this section, we introduce a probabilistic generalization of the advantage. We define this random variable--which we call the _superiority_ and denote by \(S_{h}^{\pi}\)--via a pair of axioms.

A natural construction of the superiority at \((t,x,a)\) is given by \(Z_{h}^{\pi}(t,x,a)-G^{\pi}(t,x)\). The law of this difference, however, depends on the joint law of \((Z_{h}^{\pi}(t,x,a),G^{\pi}(t,x))\), which is unobservable in practice and ill-defined (cf. Section 2.1.2). Yet, the set of all possible laws of this difference is easily characterized; it is the set of _coupled difference representations of \(\zeta_{h}^{\pi}(t,x,a)\) and \(\eta^{\pi}(t,x)\)_.

**Definition 4.1**.: _Let \(\mu,\nu\in\mathscr{P}(\mathbb{R})\). A coupled difference representation (CDR) \(\psi\in\mathscr{P}(\mathbb{R})\) of \(\mu\) and \(\nu\) takes the form \(\psi=\Delta_{\#}\kappa\) where \(\kappa\in\mathscr{C}(\mu,\nu)\) and \(\Delta:\mathbb{R}^{2}\to\mathbb{R}\) is given by \(\Delta(z,w):=z-w\). The set of all coupled difference representations of \(\mu\) and \(\nu\) will be denoted by \(\mathscr{D}(\mu,\nu)\)._

Our first axiom places the superiority's law in this set, \(\mathscr{D}(\zeta_{h}^{\pi}(t,x,a),\eta^{\pi}(t,x))\).

**Axiom 1**.: _The law of \(S_{h}^{\pi}(t,x,a)\) is a coupled difference representation of \(\zeta_{h}^{\pi}(t,x,a)\) and \(\eta^{\pi}(t,x)\)._

Our second axiom encodes a type of consistency for deterministic policy behavior.

**Axiom 2**.: \(S_{h}^{\pi}(t,x,a)\) _is deterministic whenever \(\pi\) is \((h,a)\)-persistent at time \(t\)._

To see how Axiom 2 encodes a notion of deterministic consistency, first consider its discrete-time analogue: _the superiority at \((x,a)\) for a policy \(\pi\) is deterministic if \(\pi\) at \(x\) deterministically chooses \(a\)._

In this situation, our \(a\)-following agent makes the same choices as a \(\pi\)-following agent--both take action \(a\) in state \(x\) initially and then follow \(\pi\) thereafter--, and we posit that the superiority should not be random. The continuous-time analogue of the situation just described occurs precisely when a policy \(\pi\) is \((h,a)\)-persistent at starting time \(t\). Given a starting time \(t\) and state \(x\), an agent that chooses action \(a\) between \(t\) and \(t+h\) and then follows \(\pi\) is following the \((h,a)\)-persistent modification of \(\pi\) at time \(t\). By definition, they make the same choices as a \(\pi\)-following agent when \(\pi\) is \((h,a)\)-persistent starting at time \(t\). Axiom 2 stipulates that, in this case, \(S_{h}^{\pi}(t,x,a)\) should be deterministic.

By construction, if \(\psi_{h}^{\pi}(t,x,a)\in\mathscr{D}(\zeta_{h}^{\pi}(t,x,a),\eta^{\pi}(t,x))\), then its mean is \(Q_{h}^{\pi}(t,x,a)-V^{\pi}(t,x)\) (see Appendix B.2 for a proof of this claim). Axiom 2 then says that any determining coupling \(\kappa_{h}^{\pi}(t,x,a)\) when \(\pi\) is \((h,a)\)-persistent at time \(t\) must be such that \(\Delta_{\#}\kappa_{h}^{\pi}(t,x,a)=\delta_{0}\).8 In particular, Axiom 2 nontrivially restricts \(\mathscr{D}(\zeta_{h}^{\pi}(t,x,a),\eta^{\pi}(t,x))\).

Footnote 8: Here \(\zeta_{h}^{\pi}(t,x,a)=\eta^{\pi}(t,x)\); so the mean of \(\Delta_{\#}\kappa_{h}^{\pi}(t,x,a)\) is \(0\).

**Example 4.2**.: _Let \(\iota_{h}^{\pi}(t,x,a):=\Delta_{\#}\kappa_{h}^{\pi}(t,x,a)\) for \(\kappa_{h}^{\pi}(t,x,a)=\zeta_{h}^{\pi}(t,x,a)\otimes\eta^{\pi}(t,x)\). If \(\pi\) is \((h,a)\)-persistent at time \(t\), then \(\mathbf{Var}(\iota_{h}^{\pi}(t,x,a))=2\mathbf{Var}(\eta^{\pi}(t,x))\). This variance is \(0\) only when \(\pi\)'s return is deterministic. Hence, \(\iota_{h}^{\pi}(t,x,a)\) may be nontrivial even when conditioning on \(a\) reflects the policy's behavior exactly. We posit, via Axiom 2, that this should be prohibited._

In fact, Axiom 2 determines a single coupling, if we want a consistent choice across all time-state-action triplets and all MDPs.

**Theorem 4.3**.: _Let \(\kappa\in\mathscr{C}(\mu,\mu)\) for some \(\mu\in\mathscr{P}(\mathbb{R})\). The push-forward of \(\kappa\) by \(\Delta\) is the delta at zero, \(\Delta_{\#}\kappa=\delta_{0}\), if and only if \(\kappa\) is a \(W_{p}\)-optimal coupling, for some \(p\in[1,\infty)\). Moreover, there is only one such coupling. It is given by \(\kappa_{\mu}:=(\mathrm{id},\mathrm{id})_{\#}\mu\) or, equivalently, \(\kappa_{\mu}:=(F_{\mu}^{-1},F_{\mu}^{-1})_{\#}\mathcal{U}(0,1)\). Here \(\mathcal{U}(0,1)\) is the uniform distribution on \([0,1]\)._

The second definition of \(\kappa_{\mu}\) corresponds, more generally, to the \(W_{p}\)-optimal coupling, for all \(p\geq 1\), of \(\mu\) and \(\nu\) given by \(\kappa_{\mu,\nu}:=(F_{\mu}^{-1},F_{\nu}^{-1})_{\#}\mathcal{U}(0,1)\). As \(\Delta_{\#}\kappa_{\mu,\nu}\)'s quantile function is \(F_{\mu}^{-1}-F_{\nu}^{-1}\), we have in hand everything we need to define the superiority distribution (via its quantile function).

**Definition 4.4**.: _The superiority distribution \(\psi_{h}^{\pi}\) at \((t,x,a)\in\mathsf{T}\times\mathsf{X}\times\mathsf{A}\) is_

\[\psi_{h}^{\pi}(t,x,a):=(F_{\zeta_{h}^{-1}(t,x,a)}^{-1}-F_{\eta^{\pi}(t,x)}^{-1 })_{\#}\mathcal{U}(0,1).\]

As \(\psi_{h}^{\pi}(t,x,a)\) has the smallest possible central absolute \(p\)th moments among all CDRs of \(\zeta_{h}^{\pi}(t,x,a)\) and \(\eta^{\pi}(t,x)\), heuristically, it captures more of the individual features of both return distributions than other such CDRs (like \(\iota_{h}^{\pi}(t,x,a)\) in Example 4.2). We illustrate this by example in Figure 4.1.

### The Rescaled Superiority Distribution

From Section 3, we know that the \(W_{p}\) distance between \(\zeta_{h}^{\pi}(t,x,a)\) and \(\eta^{\pi}(t,x)\), for every \(p\geq 1\), vanishes as \(h\) vanishes. Moreover, we know the rate at which this distance disappears. As a result, by construction, the central absolute \(p\)th moments of \(\psi_{h}^{\pi}(t,x,a)\) collapse as \(h\) collapses, and, for a large class of MDPs, we understand the rate at which these moments collapse. More generally and precisely, if we consider the \(q\)_-rescaled superiority distribution_ defined by

\[\psi_{h;q}^{\pi}:=(h^{-q}\,\mathrm{id})_{\#}\psi_{h}^{\pi},\]

we see that Theorems 3.6 and 3.7 translate to the follow statements on \(W_{p}\) action gaps of \(\psi_{h;q}^{\pi}\):

**Theorem 4.5**.: _MDPs and policies exist satisfying Assumptions 2.2 and 2.3 in and under which, for all \((t,x)\in\mathsf{T}\times\mathsf{X}\), we have that \(\mathsf{distgap}_{p}(\psi_{h;q}^{\pi},t,x)\raisebox{-2.15pt}{$\;\stackrel{{ >}}{{\sim}}\;$}h^{\nicefrac{{1}}{{2}}-q}\)._

**Theorem 4.6**.: _Under Assumptions 2.2 and 2.3, if \(r\) is Lipschitz in state, uniformly in time, \(f\) is Lipschitz, and \(T<\infty\), then \(\mathsf{distgap}_{p}(\psi_{h;q}^{\pi},t,x)\raisebox{-2.15pt}{$\;\stackrel{{ <}}{{\sim}}\;$}h^{\nicefrac{{1}}{{2}}-q}\), for all \((t,x)\in\mathsf{T}\times\mathsf{X}\)._

These two theorems tell us how to preserves the \(W_{p}\) action gaps of \(q\)-rescaled superiority distributions (as a function of \(h\)). They identify \(q=\nicefrac{{1}}{{2}}\). For \(q<\nicefrac{{1}}{{2}}\), \(W_{p}\) action gaps vanish as \(h\) vanishes. Whereas for \(q>\nicefrac{{1}}{{2}}\), \(W_{p}\) action gaps blow up as \(h\) vanishes. These behaviors are undesirable. When

Figure 4.1: PDFs of Return Distributions and Two Candidate CDRs.

\(q<\nicefrac{{1}}{{2}}\), the influence of an action on an agent's superiority becomes indistinguishable from any other action. For \(q>\nicefrac{{1}}{{2}}\), ever larger sample sizes are need to obtain any statistical estimate of an agent's superiority with the same level of accuracy. These scenarios are untenable.

Another consideration regarding rescalings of \(\psi_{h}^{\pi}\) is whether they upend rankings of actions determined by some given measure of utility. This would be counterproductive. In DRL, agents often use _distortion risk measures_[1] to rank actions ([10, 9, 22, 4, 17]).

**Definition 4.7**.: _Given \(\beta\in\mathscr{P}([0,1])\), the distortion risk measure \(\rho_{\beta}:\mathscr{P}(\mathbb{R})\to\mathbb{R}\) is defined by \(\rho_{\beta}(\mu):=\langle\beta,F_{\mu}^{-1}\rangle\); on \(\mu\in\mathscr{P}(\mathbb{R})\), its value is given by the integral of \(F_{\mu}^{-1}\) with respect to \(\beta\)._

A family of \(\rho_{\beta}\) is the \(\alpha\)-conditional value-at-risk measures (\(\alpha\)-CVaR) [29], where \(\beta_{\alpha}=\mathcal{U}(0,\alpha)\) for \(\alpha\in(0,1]\); \(\alpha=1\) is the expected-value utility measure. Crucially, \(\psi_{h;q}^{\pi}\) preserves \(\rho_{\beta}\)-valued utility.

**Theorem 4.8**.: _Let \(\rho_{\beta}\) be a distortion risk measure, \(q\geq 0\), and \(h>0\). If \(\rho_{\beta}(\eta^{\pi}(t,x))<\infty\), then \(\arg\max_{a\in\mathsf{A}}\rho_{\beta}(\psi_{h;q}^{\pi}(t,x,a))=\arg\max_{a\in \mathsf{A}}\rho_{\beta}(\zeta_{h}^{\pi}(t,x,a))\)._

In turn, the \(\nicefrac{{1}}{{2}}\)-rescaled superiority distribution is not only \(W_{p}\) action gap preserving but matches \(\zeta_{h}^{\pi}\) in its greedy choice of action as measured by a distortion risk measure.

### Algorithmic Considerations

We now turn to building DRL algorithms based on our theory. Our algorithms leverage the quantile TD-learning framework [10] to learn \(\rho_{\beta}\)-greedy policies, for a given distortion risk measure \(\rho_{\beta}\), just as DAU [34] leverages the \(Q\)-learning framework to learn greedy policies. Full pseudocode and implementation details are given in Appendix C.

At the heart of our algorithms is an equality of quantile functions, which holds by construction,

\[F_{\zeta_{h}^{\pi}(t,x,a)}^{-1}=F_{\eta^{\pi}(t,x)}^{-1}+h^{q}F_{\psi_{h;q}^{ \pi}(t,x,a)}^{-1}.\] (4.1)

Indeed, given \(\eta\) and \(\psi_{h;q}\), as models of \(\eta^{\pi}\) and \(\psi_{h;q}^{\pi}\) respectively, equation (4.1) justifies the application of quantile TD-learning to \(\zeta_{h}\), as a model for \(\zeta_{h}^{\pi}\), defined via the quantile function

\[F_{\zeta_{h}(t,x,a)}^{-1}:=F_{\eta(t,x)}^{-1}+h^{q}F_{\psi_{h;q}^{\pi}(t,x,a)}^ {-1}.\] (4.2)

That said, we cannot realize quantile TD-learning without defining predictions and bootstrap targets in terms of \(m\)-quantile representations9[10, 4] of \(\zeta_{h}\), via those of \(\eta\) and \(\psi_{h;q}\).

Footnote 9: An \(m\)-quantile representation of a given distribution in \(\mathscr{P}(\mathbb{R})\) is a vector in \(\mathbb{R}^{m}\) whose \(i\)th component encodes the \(\frac{\nicefrac{{1}}{{2}}}{m}\)-quantile of said distribution.

While we may freely parameterize the \(m\)-quantile representation of \(\eta\) with a neural network (with interface) \(\theta:\mathsf{T}\times\mathsf{X}\to\mathbb{R}^{m}\), we have to be careful when parameterizing the \(m\)-quantile representation of \(\psi_{h;q}\). Given a neural network \(\phi:\mathsf{T}\times\mathsf{X}\times\mathsf{A}\to\mathbb{R}^{m}\), we set

\[F_{\psi_{h;q}(t,x,a)}^{-1}:=\phi(t,x,a)-\phi(t,x,a^{\star})\quad\text{with} \quad a^{\star}\in\arg\max_{a\in\mathsf{A}}\rho_{\beta}(\phi(t,x,a)).\] (4.3)

This ensures we identify a \(\rho_{\beta}\)-greedy policy; it is \(0\) at the \(\rho_{\beta}\)-greedy action \(a^{\star}\) (cf. [34, Eq. 27]).

With appropriate parameterized \(m\)-quantile representations of \(\eta\) and \(\psi_{h;q}\) in hand, we derive our predictions and bootstrap targets. By (4.2), recalling \(\theta\) and (4.3), we compute our predictions via

\[F_{\zeta_{h}(t,x,a)}^{-1}:=\theta(t,x)+h^{q}(\phi(t,x,a)-\phi(t,x,a^{\star})).\] (4.4)

By Wiltzer [40], as \(X_{t}^{\pi|_{h,a,t}}=x\) and \(X_{t+h}^{\pi|_{h,a,t}}=X_{t+h}^{a}\), observe that

\[Z_{h}^{\pi}(t,x,a)= \mathrm{law}\int_{0}^{h}\gamma^{s}r(t+s,X_{t+s}^{\pi|_{h,a,t}}) \,\mathrm{d}s+\gamma^{h}G^{\pi}(t+h,X_{t+h}^{\pi|_{h,a,t}})\] \[= \mathrm{law}\,hr(t,x)+\gamma^{h}G^{\pi}(t+h,X_{t+h}^{a})+Y_{h},\]

where \(\mathbf{E}[|Y_{h}|^{p}]=o(h)\)10 for all \(p\in\mathbb{N}\). So upon getting a sample state/realization \(x_{t+h}\) of \(X_{t+h}^{a}\), as in [30], we compute our bootstrap targets via

Footnote 10: This holds if \(r\) is continuous, a standard assumption in continuous-time control (see, e.g., [25, 34, 16]).

\[\mathscr{T}F_{\zeta_{h}(t,x,a)}^{-1}:=hr(t,x)+\gamma^{h}\theta(t+h,x_{t+h}).\] (4.5)In summary, the predictions (4.4) and the bootstrap targets (4.5) together characterize a family of QR-DQN-based algorithms called DSUP(\(q\)), whose core update is outlined in Algorithm 1.

```
0:\(q\) (rescale factor), \(\alpha\) (step size), \(\mathcal{D}\) (replay buffer), \(\rho_{\beta}\) (distortion risk measure)
0:\(\theta:\mathsf{T}\times\mathsf{X}\to\mathbb{R}^{m}\) (\(m\)-quantile approximator of \(F_{\eta}^{-1}\)), \(\phi:\mathsf{T}\times\mathsf{X}\times\mathsf{A}\to\mathbb{R}^{m}\) (see (4.3))  Sample \((t,x_{t},a_{t},r_{t},x_{t+h},\mathsf{done}_{t+h})\) from \(\mathcal{D}\) \(a^{\star}\leftarrow\arg\max_{a}\rho_{\beta}(\frac{1}{m}\sum_{n=1}^{m}\delta_{ \phi(t,x_{t},a_{n})})\)\(\triangleright\) Risk-sensitive greedy action \(F_{\zeta}^{-1}(\phi,\theta)\leftarrow\theta(t,x_{t})+h^{q}(\phi(t,x_{t},a_{t} )-\phi(t,x_{t},a^{\star}))\)\(\triangleright\) Prediction (4.4) \(\mathcal{T}F_{\zeta}^{-1}\gets hr_{t}+\gamma^{h}(1-\mathsf{done}_{t+h}) \theta(t+h,x_{t+h})+\gamma^{h}\mathsf{done}_{t+h}f(x_{t+h})\)\(\triangleright\) Target (4.5) \(\ell(\phi,\theta)\leftarrow\mathsf{QuantileHuber}(F_{\zeta}^{-1}(\phi, \theta),\mathcal{T}F_{\zeta}^{-1})\)\(\triangleright\) See (10, Eq. (10)) \(\phi\leftarrow\phi-\alpha\nabla_{\phi}\ell(\phi,\theta)\) and \(\theta\leftarrow\theta-\alpha\nabla_{\theta}\ell(\phi,\theta)\)\(\triangleright\) Gradient updates ```

**Algorithm 1** DSUP(\(q\)) Update

One theoretical drawback of DSUP(\(q\)) for mean-return control is that the mean of the \(q\)-rescaled superiority distribution is \(O(1)\) only when \(q=1\), by (2.6) and (2.7). Thus, we propose modeling \(A_{h}^{\pi}\) simultaneously. This yields a novel form of a _two-timescale_ approach to value-based RL (see, e.g., [8]). In particular, we estimate \(\vartheta_{h;q}^{\pi}\) defined by

\[F_{\phi_{h;q}^{\pi}(t,x,a)}^{-1}:=F_{\psi_{h;q}^{\pi}(t,x,a)}^{-1}+(1-h^{1-q}) A_{h}^{\pi}(t,x,a).\]

We call \(\vartheta_{h;q}^{\pi}\) the _advantage-shifted \(q\)-rescaled superiority_. Note that its mean is \(A_{h}^{\pi}\), which is \(O(1)\). To realize this, we approximate \(A_{h}^{\pi}\) using DAU and employ parameter sharing between the approximators of \(A_{h}^{\pi}\) and \(\psi_{h;q}^{\pi}\). We call this family of algorithms DAU+DSUP(\(q\)). We note that \(A_{h}^{\pi}\) is used only for increasing action gaps; it does not change the training loss for \(\eta\) and \(\psi_{h;q}\).

## 5 Simulations

The empirical work herein is two-fold in nature: illustrative and comparative. First, we simulate an example that illustrates Theorems 3.6/4.5 and Theorem 3.7/4.6 and their consequences. Second, in an option-trading environment, we compare the performance of \(\psi_{h}^{\pi}\)-based agent(s) against QR-DQN [10] and DAU [34] in the risk-neutral setting and against QR-DQN in a risk-sensitive setting.

### The Rescaled Superiority Distribution Revisited

Consider an MDP with time horizon \(10\), a two element action space, \(0\) and \(1\)--when action \(1\) is executed, the system follows \(1\)-dimensional Brownian dynamics with a constant drift of \(10\), and otherwise, the state is fixed--, a reward that equals the agent's signed distance to \(0\), and a trivial terminal reward. We estimate four distributions at \((t,x,a)=(0,0,1)\) for the policy that always selects \(0\). Figure 5.1 shows these estimated distributions for a sample of frequencies (kHz), \(\omega=\nicefrac{{1}}{{h}}\).

First (from the left), we see that \(\psi_{h}^{\pi}\) collapses to \(\delta_{0}\), as \(h\) tends to \(0\). Thus, accurate action ranking distributional or otherwise becomes impossible in the vanishing \(h\) limit. Second, we see that rescaling by \(h\) produces distributions with \(O(1)\) mean but infinite non-mean statistics in the vanishing \(h\) limit. Here the \(O(1)\) means are imperceptible in face of the large variances. So while this rescaling permits ranking actions by action values, it does so at the expense of producing high-variance distributions.

Third, we see that rescaling by \(h^{\nicefrac{{1}}{{2}}}\) yields distributions with \(O(1)\) non-mean statistics but vanishingly small means, \(O(h^{\nicefrac{{1}}{{2}}})\). Hence, this rescaling permits ranking actions by non-mean statistics, even if action values again becomes indistinguishable in the vanishing \(h\) limit. That said, the vanishing rate of the means here is slower than when no rescaling is considered, \(O(h)\). Fourth, we see that rescaling by \(h^{\nicefrac{{1}}{{2}}}\) and then shifting it by \((1-h^{\nicefrac{{1}}{{2}}})A_{h}^{\pi}\) produces distributions with \(O(1)\) mean and non-mean statistics. In turn, this two-timescale approach permits ranking actions by either action values or non-mean statistics (but not both by Theorem 4.8). However, the mean estimates here are inaccurate and imprecise--rather than uniformly being \(100\), they oscillate substantially.

In risk-neutral control, we are left with a number of questions. What effect do the high variance distributions in DAU/DSUP(\(1\)) have on performance? What effect do the \(O(h^{\nicefrac{{1}}{{2}}})\) means have on the performance of DSUP(\(\nicefrac{{1}}{{2}}\))? What effect does the instability of the mean estimates in DAU+DSUP(\(\nicefrac{{1}}{{2}}\)) have on performance? In Section 5.2, we begin to answer these questions and others by testing our superiority-based algorithms against appropriate benchmarks in an option-trading environment.

### High-Frequency Option Trading

The option-trading environment in which we run our comparative experiments is a commonly used benchmark (see, e.g., [22, 17]). We use an Euler-Maruyama discretization scheme [23] at high resolution to simulate high-frequency trading. Returns are averaged over \(10\) seeds and \(10\) different dynamics models (corresponding to data from different stocks). Additionally, following [22], we use disjoint datasets to estimate the dynamics parameters for simulation during training and evaluation.11

Footnote 11: Our code is available at https://github.com/harwiltz/distributional-superiority.

First, we consider the risk-neutral setting. Here we compare QR-DQN, DAU, and three algorithms based on the \(q\)-rescaled superiority distribution with \(q=1,\nicefrac{{1}}{{2}}\): DSUP(\(1\)), DAU+DSUP(\(\nicefrac{{1}}{{2}}\)), and DSUP(\(\nicefrac{{1}}{{2}}\)). Figure 5.2 summarizes their performance at a sample of frequencies (Hz).

We see that DSUP(\(\nicefrac{{1}}{{2}}\)) is not only the most consistent performer, but outperforms every competitor at all but the two lowest frequencies. Even then, its performance is very close to the best performer. We also see that DAU+DSUP(\(\nicefrac{{1}}{{2}}\))'s preservation of both action gaps and \(W_{p}\) action gaps does not lead to the strongest performance. In particular, its performance is inconsistent and sometimes poor. We believe this is because the tested frequencies are low enough that DSUP(\(\nicefrac{{1}}{{2}}\)) maintains large enough action gaps to learn performant policies, but high enough that the variances of the distributions underlying \(A_{h}^{\pi}\) cause estimation difficulty. Indeed, the three methods that estimate \(A_{h}^{\pi}\) (explicitly in DAU and DAU+DSUP(\(\nicefrac{{1}}{{2}}\)) or implicitly in DSUP(\(1\))) exhibit almost identical behavior.

Our results highlight a dichotomy in existing (ours included) methods for value-based, high-frequency, risk-neutral control. They can either maintain \(O(1)\) expected return estimates or \(O(1)\) return variance estimates, but not both. We observe better performance in estimating small means from \(O(1)\) variance distributions than in estimating \(O(1)\) means from receipricolly large variance distributions.

To qualitatively illustrate the appeal of the \(\nicefrac{{1}}{{2}}\)-rescaled superiority, Figure 5.3 presents examples of learned action-conditioned distributions used by DSUP(\(\nicefrac{{1}}{{2}}\)) and QR-DQN agents to make decisions.

Figure 5.2: Risk-neutral algorithms on high-frequency option-trading as a function of \(\omega\).

In this environment, action \(1\) taken in the start state terminates the episode, yielding the smallest return, \(0\), making this action inferior to its alternative action, \(1\). We see that \(\text{DSUP}(\nicefrac{{1}}{{2}})\) infers this fact. QR-DQN, on the other hand, has difficulty distinguishing these actions. This is because the \(\nicefrac{{1}}{{2}}\)-rescaled superiority preserves \(W_{p}\) action gaps, while \(\zeta_{h}^{\pi}\) does not.

Second, we consider a risk-sensitive setting. Here we compare QR-DQN and \(\text{DSUP}(\nicefrac{{1}}{{2}})\) using \(\alpha\)-CVaR for greedy action selection. We do this because Theorem 4.8 does not hold with \(\vartheta_{h;q}^{\pi}\), and preserving means is less critical in risk-sensitive control than it is in risk-neutral control. Figure 5.4 depicts our results at \(\omega=35\mathrm{Hz}\) (see Appendix D for results across a range of \(\omega\)).

Again, we see that \(\text{DSUP}(\nicefrac{{1}}{{2}})\) is conclusively the best performer.

## 6 Related Work

Notions of action gap and ranking have long been of interest in RL (see, e.g., [11]). Action gaps are related to sample complexity in RL--indeed, instance-dependent sample complexity rates are inversely proportional to the divergence between action-conditioned return distributions ([13, 19, 37]). Bellemare et al. [5] argue for the consideration of alternatives to the Bellman operator that explicitly devalue suboptimal actions, and they show that Baird's AL [2] operator falls within this class of operators. On the other hand, Schaul et al. [31] implicitly question Bellemare et al.'s position. They demonstrate that stochastic gradient updates in deep value-based RL algorithms induce frequent changes in relative action values, which in turn is a mechanism for exploration.

The advantage function is commonplace in RL (see, e.g., [39, 32, 27, 35, 24]). In [24], Mesnard et al. employ a distributional critic that is closely related to our (unscaled) distributional superiority. Their choice of critic stems from a desire to minimize variance. We note that the distributional superiority is _a posteriori_ characterized as a minimal variance coupled difference representation of action-conditioned return distributions and policy-induced return distributions.

Lastly, DRL in continuous-time MDPs is in its infancy. There are only three works to mention. Wiltzer et al. [40, 41] give a characterization of return distributions for policy evaluation, and Halperin [14] studies algorithms for control. That said, neither work considers distributional notions of action gaps or advantages. Moreover, Halperin does not consider any of the challenges of estimating the influence of actions in high decision frequency settings.

## 7 Conclusion

We establish that DRL agents are sensitive to decision frequency through analysis and simulation. In experiments, \(\text{DSUP}(\nicefrac{{1}}{{2}})\) learns well-performing policies across a range of high decision frequencies, unlike prior approaches. \(\text{DSUP}(1)\) and \(\text{DAU}+\text{DSUP}(\nicefrac{{1}}{{2}})\) are less robust. Given our analysis, the performance of \(\text{DSUP}(1)\) is expected. Building an alternate algorithm to \(\text{DAU}+\text{DSUP}(\nicefrac{{1}}{{2}})\) that is both tailored to risk-neutral control and robust to \(h\) is an important avenue for future work.

Figure 5.4: Risk-sensitive algorithms on high-frequency option-trading at \(\omega=35\)Hz.

### Acknowledgments and Disclosure of Funding

The authors are very grateful to Yunhao Tang for fruitful correspondence about distributional analogues to the advantage. Additionally, we thank Mark Rowland, Jesse Farebrother, Tyler Kastner, Pierluca D'Oro, Nate Rahn, and Arnav Jain for helpful discussions. HW was supported by the Fonds de Recherche du Quebec and the National Sciences and Engineering Research Council of Canada (NSERC). MGB was supported by the Canada CIFAR AI Chair program and NSERC. This work was supported in part by DARPA HR0011-23-9-0050 to PS. YJ was supported by in part by NSF Grant 2243869. This research was enabled in part by support provided by Calcul Quebec, the Digital Research Alliance of Canada (allianeccan.ca), and the compute resources provided by Mila (mila.quebec).

## References

* [1] Carlo Acerbi. Spectral measures of risk: A coherent representation of subjective risk aversion. _Journal of Banking & Finance_, 26(7):1505-1518, July 2002.
* [2] L. Baird. _Reinforcement Learning Through Gradient Descent_. PhD thesis, May 1999.
* [3] Marc G. Bellemare, Salvatore Candido, Pablo Samuel Castro, Jun Gong, Marlos C. Machado, Subhodeep Moitra, Sameera S. Ponda, and Ziyu Wang. Autonomous navigation of stratospheric balloons using reinforcement learning. _Nature_, 588(7836):77-82, December 2020.
* [4] Marc G. Bellemare, Will Dabney, and Mark Rowland. _Distributional Reinforcement Learning_. The MIT Press, May 2023.
* [5] Marc G Bellemare, Georg Ostrovski, Arthur Guez, Philip Thomas, and Remi Munos. Increasing the action gap: New operators for reinforcement learning. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 30, 2016.
* [6] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018.
* [7] Gerard Brunick and Steven Shreve. Mimicking an Ito process by a solution of a stochastic differential equation. _The Annals of Applied Probability_, 23(4):1584-1628, August 2013.
* [8] Wesley Chung, Somjit Nath, Ajin Joseph, and Martha White. Two-timescale networks for nonlinear value function approximation. In _International Conference on Learning Representations_, 2018.
* [9] Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for distributional reinforcement learning. In _International Conference on Machine Learning_, pages 1096-1105. PMLR, 2018.
* [10] Will Dabney, Mark Rowland, Marc G. Bellemare, and Remi Munos. Distributional Reinforcement Learning with Quantile Regression. In _AAAI_, 2017.
* [11] Amir-massoud Farahmand. Action-gap phenomenon in reinforcement learning. _Advances in Neural Information Processing Systems_, 24, 2011.
* [12] Wendell H Fleming and Halil Mete Soner. _Controlled Markov processes and viscosity solutions_, volume 25. Springer Science & Business Media, 2006.
* [13] Todd L. Graves and Tze Leung Lai. Asymptotically Efficient Adaptive Choice of Control Laws in Controlled Markov Chains. _SIAM Journal on Control and Optimization_, 35(3):715-743, May 1997.
* [14] Igor Halperin. Distributional offline continuous-time reinforcement learning with neural physics-informed pdes (sciphy rl for doct-l). _Neural Computing and Applications_, 36(9):4643-4659, 2024.
* [15] Yanwei Jia and Xun Yu Zhou. Policy Gradient and Actor-Critic Learning in Continuous Time and Space: Theory and Algorithms. _Journal of Machine Learning Research_, 23(275):1-50, 2022.

* [16] Yanwei Jia and Xun Yu Zhou. q-learning in continuous time. _Journal of Machine Learning Research_, 24(161):1-61, 2023.
* [17] Tyler Kastner, Murat A Erdogdu, and Amir-massoud Farahmand. Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2023.
* [18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations (ICLR)_, 2014.
* [19] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [20] Leemon C. Baird. Advantage Updating. Technical report, Defense Technical Information Center, Fort Belvoir, VA, November 1993.
* [21] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In _International Conference on Learning Representations (ICLR)_, 2016.
* [22] Shiau Hong Lim and Ilyas Malik. Distributional Reinforcement Learning for Risk-Sensitive Policies. In _NeurIPS_, October 2022.
* [23] Gisiro Maruyama. Continuous markov processes and stochastic equations. _Rendiconti del Circolo Matematico di Palermo_, 4:48-90, 1955.
* [24] Thomas Mesnard, Wenqi Chen, Alaa Saade, Yunhao Tang, Mark Rowland, Theophane Weber, Clare Lyle, Audrunas Gruslys, Michal Valko, Will Dabney, et al. Quantile credit assignment. In _International Conference on Machine Learning_, pages 24517-24531. PMLR, 2023.
* [25] Remi Munos and Paul Bourgine. Reinforcement Learning for Continuous Stochastic Control Problems. In _Advances in Neural Information Processing Systems (NeurIPS)_, 1997.
* [26] Bernt Oksendal. _Stochastic differential equations: an introduction with applications_. Springer Science & Business Media, 2013.
* [27] Hsiao-Ru Pan, Nico Gurtler, Alexander Neitz, and Bernhard Scholkopf. Direct Advantage Estimation. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2022.
* [28] Simon Ramstedt and Christopher J. Pal. Real-time reinforcement learning. In _Advances in Neural Information Processing Systems_, 2019.
* [29] R Tyrrell Rockafellar and Stanislav Uryasev. Conditional value-at-risk for general loss distributions. _Journal of Banking & Finance_, 26(7):1443-1471, 2002.
* [30] Mark Rowland, Remi Munos, Mohammad Gheshlaghi Azar, Yunhao Tang, Georg Ostrovski, Anna Harutyunyan, Karl Tuyls, Marc G. Bellemare, and Will Dabney. An Analysis of Quantile Temporal-Difference Learning. _Journal of Machine Learning Research (JMLR)_, 25:1-47, 2023.
* [31] Tom Schaul, Andre Barreto, John Quan, and Georg Ostrovski. The phenomenon of policy churn. _Advances in Neural Information Processing Systems_, 35:2537-2549, 2022.
* [32] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization Algorithms. _CoRR_, abs/1707.06347, 2017.
* [33] Daniel W. Stroock and S. R. Srinivasa Varadhan. _Multidimensional Diffusion Processes_. Springer, 2006.
* [34] Corentin Tallec, Leonard Blier, and Yann Ollivier. Making Deep Q-learning methods robust to time discretization. In _International Conference on Machine Learning (ICML)_, 2019.
* [35] Yunhao Tang, Remi Munos, Mark Rowland, and Michal Valko. VA-learning as a more efficient alternative to Q-learning. In _International Conference on Machine Learning (ICML)_, 2023.
* [36] Cedric Villani et al. _Optimal transport: old and new_, volume 338. Springer, 2009.

* [37] Andrew J. Wagenmaker and Dylan J. Foster. Instance-Optimality in Interactive Decision Making: Toward a Non-Asymptotic Theory. In _Proceedings of Thirty Sixth Conference on Learning Theory_, pages 1322-1472. PMLR, July 2023.
* [38] Haoran Wang, Thaleia Zariphopoulou, and Xun Yu Zhou. Reinforcement learning in continuous time and space: A stochastic control approach. _Journal of Machine Learning Research_, 21(198):1-34, 2020.
* [39] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. Dueling Network Architectures for Deep Reinforcement Learning. In _International Conference on Machine Learning (ICML)_, 2016.
* [40] Harley Wiltzer. _On the Evolution of Return Distributions in Continuous-Time Reinforcement Learning_. McGill University (Canada), 2021.
* [41] Harley Wiltzer, David Meger, and Marc G. Bellemare. Distributional Hamilton-Jacobi-Bellman Equations for Continuous-Time Reinforcement Learning. In _International Conference on Machine Learning (ICML)_, 2022.
* [42] Hanyang Zhao, Wenpin Tang, and David Yao. Policy optimization for continuous reinforcement learning. _Advances in Neural Information Processing Systems_, 36, 2024.

## Appendix A Formalism of Continuous-Time RL Controlled Markov Processes

Expected-value RL is a data-driven approach to solving the (classic) optimal control problem: find an action (control) process \((A_{s})_{s\geq t}\) and an associated state process (then determined by the environment) \((X_{s})_{s\geq t}\) with \(X_{t}=x\), for a given \(t\geq 0\), that maximize the expected return earned by following the state-action process \((X_{s},A_{s})_{s\geq t}\). In particular, RL agents search the space of state-action processes via policies \(\pi:\mathsf{T}\times\mathsf{X}\to\mathscr{P}(\mathsf{A})\). Policies prescribe the conditional probabilities of the laws of state-action processes. Indeed, \((X_{s},A_{s})_{s\geq t}\) is the state-action process of an agent following \(\pi\) if and only if, for each \(s\geq t\), the set \(\{\pi(\cdot\,|\,s,x)\}_{x\in\mathsf{X}}\) is the set of conditional probabilities of \(\text{law}((X_{s},A_{s}))\) with respect to \(\text{law}(X_{s})\).

Continuous-time RL is a data-driven approach to stochastic optimal control. Whence, environmental dynamics are assumed to arise from an action-parameterized family of SDEs determined by a drift \(b:\mathsf{T}\times\mathsf{X}\times\mathsf{A}\to\mathbb{R}^{n}\) and diffusion \(\sigma:\mathsf{T}\times\mathsf{X}\times\mathsf{A}\to\mathbb{R}^{n\times n}\).12 Thus, the goal of expected-value RL (and stochastic optimal control) is to find an expected-return maximizing state-action process among state-action processes \((X_{s},A_{s})_{s\geq t}\) that satisfy

Footnote 12: Without loss of generality, as the the laws of our stochastic processes are the objects of interest, we may assume that our diffusion matrix is square. Indeed, for any SDE with \(n\times m\)-dimensional diffusion \(\sigma\), the matrix \((\sigma\sigma^{\top})^{1/2}\), which is uniquely defined by \(\sigma\sigma^{\top}\), and not \(\sigma\) is the diffusion matrix that determines the law of the solution to that SDE.

\[\mathrm{d}X_{s}=b(s,X_{s},A_{s})\,\mathrm{d}s+\sigma(s,X_{s},A_{s})\,\mathrm{d }B_{s}\quad\text{with}\quad X_{t}=x.\] (A.1)

We note that the MDPs defined in Section 2 have equivalent formulations in terms of transition kernel, exactly as they formulated in discrete-time RL. We refer the reader to [33] for an in-depth discussion regarding this fact.

### Justification of Random Returns

Given the above formalism, the "true" distribution of returns of an agent following a policy \(\pi\) is the law of

\[\int_{t}^{T}\gamma^{s-t}r(s,X_{s})\,\mathrm{d}s+\gamma^{T-t}f(X_{T})\]

where \((X_{s},A_{s})_{s\geq t}\) is a state-action process associated to \(\pi\) and solves (A.1). That said, by the definition of \(\pi\),

\[b^{\pi}(s,X_{s})=\mathbf{E}[b(s,X_{s},A_{s})\,|\,X_{s}]\quad\text{and}\quad \sigma^{\pi}(\sigma^{\pi})^{\top}(s,X_{s})=\mathbf{E}[\sigma\sigma^{\top}(s,X_ {s},A_{s})\,|\,X_{s}],\]

where \(b^{\pi}\) and \(\sigma^{\pi}\) are exactly the coefficients defined in (2.3). Hence, by [7], provided that \(b^{\pi}\) and \(\sigma^{\pi}\) are regular enough to guarantee that (2.2) is well-posed in law, the processes \(X^{\pi}=(X_{s}^{\pi})_{s\geq t}\) and \(X=(X_{s})_{s\geq t}\) are equal in law.13 Here \((X_{s}^{\pi})_{s\geq t}\) satisfies (2.2) with \(X_{t}^{\pi}=x\). Consequently,

Footnote 13: The assumptions under which we work guarantee that \(X^{\pi}=_{\text{law}}X\).

\[G^{\pi}(t,x)=_{\text{law}}\int_{t}^{T}\gamma^{s-t}r(s,X_{s})\,\mathrm{d}s+ \gamma^{T-t}f(X_{T}).\]

This, formally, justifies \(G^{\pi}\) and \(Z_{h}^{\pi}\), as defined in (2.4) and (2.5).

### On Assumption 2.3

Here we provide some conditions under which Assumption 2.3 is established. These conditions are presented as additional assumptions. Assumptions A.1 and A.2 are common in stochastic control theory (see, e.g., [12]) and SDE theory in general (see, e.g., [26]). Assumption A.3 is ubiquitous in the continuous-time RL literature [15, 16, 42]. Notably, these conditions together guarantee the existence of transition probabilities for policy-induced state processes arising from (2.2).

**Assumption A.1**.: _The coefficients \(b\) and \(\sigma\) are uniformly bounded: a finite, positive constant \(C_{A.1}\) exists such that_

\[\sup_{t,x,a}|b(t,x,a)|+\sup_{t,x,a}|\sigma(t,x,a)|\leq C_{A.1}.\]

**Assumption A.2**.: _The matrix \(\sigma\sigma^{\top}\) is uniformly elliptic: a positive, finite constant \(\lambda_{A.2}\) exists such that_

\[\inf_{t,x,a}\inf_{|v|=1}v^{\top}\sigma\sigma^{\top}(t,x,a)v\geq\lambda_{A.2}^{2}I.\]A consequence of Assumption A.2 is

\[\inf_{t,x}\inf_{|v|=1}v^{\top}\sigma^{\pi}(t,x)v\geq\lambda_{A.2}I.\] (A.2)

In other words, \(\sigma^{\pi}\) is also uniformly elliptic.

**Assumption A.3**.: _A finite, positive constant \(C_{A.3}\) exists for which_

\[\sup_{t}\mathrm{TV}(\pi(\cdot\,|\,t,x),\pi(\cdot\,|\,t,y))\leq C_{A.3}|x-y| \quad\forall x,y\in\mathsf{X},\]

_where \(\mathrm{TV}\) is the total variation metric on \(\mathscr{P}(\mathsf{A})\)._

Observe if \(\pi\) satisfies Assumption A.3, then \(\pi|_{h,a,t}\) also satisfies Assumption A.3. Indeed,

\[\mathrm{TV}(\pi|_{h,a,t}(\cdot\,|\,s,x),\pi|_{h,a,t}(\cdot\,|\,s,y))\leq \mathrm{TV}(\pi(\cdot\,|\,s,x),\pi(\cdot\,|\,s,y))\]

since

\[\sup_{s\in[t,t+h)}\mathrm{TV}(\pi|_{h,a,t}(\cdot\,|\,s,x),\pi|_{h,a,t}(\cdot \,|\,s,y))=0\]

and \(\pi|_{h,a,t}(\cdot\,|\,s,x)=\pi(\cdot\,|\,s,x)\) for all \(s\in\mathsf{T}\setminus[t,t+h)\).

**Proposition A.4**.: _If Assumptions 2.2, A.1, and A.2 hold and \(\pi\) satisfies Assumption A.3, then Assumption 2.3 holds._

Proof.: Observe that

\[|b^{\pi}(t,x)-b^{\pi}(t,y)| \leq\left|\int(b(t,x,a)-b(t,y,a))\,\pi(\mathrm{d}a\,|\,t,x)\right|\] \[\qquad+\left|\int b(t,y,a)\,\pi(\mathrm{d}a\,|\,t,x)-\int b(t,y,a )\,\pi(\mathrm{d}a\,|\,t,y)\right|\] \[\leq(C_{2.2}+2C_{A.1}C_{A.3})|x-y|,\]

by Assumptions 2.2, A.1, and A.3. Here we have also used Kantorovich duality to computed \(\mathrm{TV}\) and invoke Assumption A.3.

Let \(\lambda\) be an eigenvalue of \(\sigma^{\pi}(t,x)-\sigma^{\pi}(t,y)\) with with unit eigenvector \(v\). Observe that

\[v^{\top}(\sigma^{\pi}(t,x)^{2}-\sigma^{\pi}(t,y)^{2})v =v^{\top}(\sigma^{\pi}(t,x)-\sigma^{\pi}(t,y))\sigma^{\pi}(t,x)+ \sigma^{\pi}(t,y)(\sigma^{\pi}(t,x)-\sigma^{\pi}(t,y))v\] \[=\lambda v^{\top}(\sigma^{\pi}(t,x)+\sigma^{\pi}(t,x))v\]

Hence, by (A.2),

\[|\lambda|=\frac{|v^{\top}(\sigma^{\pi}(t,x)^{2}-\sigma^{\pi}(t,y)^{2})v|}{v^ {\top}(\sigma^{\pi}(t,x)+\sigma^{\pi}(t,x))v}\leq\frac{1}{2\lambda_{A.2}}|v^{ \top}(\sigma^{\pi}(t,x)^{2}-\sigma^{\pi}(t,y)^{2})v|.\]

In turn, by Assumptions 2.2, A.1, and A.3, as done to prove that \(b^{\pi}\) was Lipschitz above,

\[|\lambda|\leq\frac{1}{\lambda_{A.2}}(C_{2.2}+C_{A.1}C_{A.3})|x-y|.\]

Assumption 2.3 follows, since \(\lambda\) was an arbitrary eigenvalue and all norms on finite dimensional spaces are equivalent. 

We conclude this section with one final fact: under Assumption 2.2, the policy-averaged coefficient (2.3) have linear growth. Indeed,

\[|b^{\pi}(t,x)|\leq\int|b(t,x,a)|\,\pi(\mathrm{d}a\,|\,t,x)\leq C_{2.2}(1+|x|)\]

and

\[|\sigma^{\pi}(t,x)|^{2}\leq\int|\sigma(t,x,a)|^{2}\,\pi(\mathrm{d}a\,|\,t,x) \leq C_{2.2}^{2}(1+|x|)^{2}.\]

### Action-Independent Rewards

In this paper, we assume that the rewards do not depend on actions. This is a theoretical limitation of not just our work, but continuous-time DRL in general (see [41, 14]). In the following sections, we discuss the nature of this theoretical limitation. However, many MDPs have action-independent reward functions. For example, MDPs encoding goal-reaching problems, tracking problems, and commodity-trading problems all have action-independent rewards.

#### a.3.1 Continuous-Time, Expected Return

In continuous-time, expected-value RL, when the reward function \(r\) depends on actions, the standard approach to analysis involves considering the averaged reward function \(r^{\pi}:\mathsf{T}\times\mathsf{X}\to\mathbb{R}\) given by

\[r^{\pi}(t,x):=\int_{\mathsf{A}}r(t,x,a)\,\pi(\mathrm{d}a\,|\,t,x).\]

In continuous-time RL specifically, the averaged reward \(r^{\pi}\) is justified exactly as the coefficients \(b^{\pi}\) and \(\sigma^{\pi}\) are justified. However, the "true" return distribution is not equal to the law of

\[\int_{t}^{T}\gamma^{s-t}r^{\pi}(s,X_{s}^{\pi})\,\mathrm{d}s+\gamma^{T-t}f(X_{T }^{\pi}).\] (A.3)

To see this, it suffices to consider an MDP with a single state \(x\). In this case, the expression in (A.3) is deterministic. However, if \(\pi\) is nondeterministic and \(r\) is dependent on actions, then the "true" return distribution is nondeterministic. Hence, the law of the expression in (A.3) cannot be the "true" return distribution associated to \(\pi\).

#### a.3.2 Discrete-Time, Random Return

In discrete-time RL, the distribution of returns given an action-dependent reward is analyzed through the state-action process induced by a policy. This processes is defined by extending the action-parameterized family of transition probability kernels on \(\mathsf{X}\), which define the dynamics of a given MDP, to a single transition probability kernel on \(\mathsf{X}\times\mathsf{A}\). In the time-homogeneous setting, for instance, with transition kernels \(\{P(\mathrm{d}y\,|\,x,a)\}_{a\in\mathsf{A}}\), this amounts to constructing

\[P^{\pi}(\mathrm{d}y\mathrm{d}b\,|\,x,a):=\pi(\mathrm{d}b\,|\,y)\otimes P( \mathrm{d}y\,|\,x,a),\]

provided that map \(y\mapsto\pi(E\,|\,y)\) is measurable for all \(y\in\mathsf{X}\). In continuous-time environments, such a constructing has yet to be discovered.

We note that trying to analogously extend the action-parameterized family of transition semigroups on \(\mathsf{X}\), which define the dynamics of a given time-homogeneous MDP in continuous time, to a single transition semigroup on \(\mathsf{X}\times\mathsf{A}\) by defining

\[P^{\pi}_{t}(\mathrm{d}y\mathrm{d}b\,|\,x,a):=\pi(\mathrm{d}b\,|\,y)\otimes P _{t}(\mathrm{d}y\,|\,x,a),\]

where \(P_{t}(\mathrm{d}y\,|\,x)\) is a transition semigroup, may fail to satisfy the Chapman-Kolmogorov identity. Indeed, suppose \(\mathsf{A}\) has two elements and \(\mathsf{X}=\mathbb{R}\). Let \(\pi(\mathrm{d}a\,|\,x)\) be the uniform measure on \(\mathsf{A}\) for all \(x\in\mathsf{X}\). If \(P_{t}(\mathrm{d}y\,|\,x,a_{\delta})=\delta_{x+t}(\mathrm{d}y)\) and \(P_{t}(\mathrm{d}y\,|\,x,a_{\delta})=(2\pi)^{-\nicefrac{{1}}{{2}}}\exp(-|y-x|^ {2}/2t)\,\mathrm{d}y\), then Chapman-Kolmogorov identity fails, for example, on any tuple \((s,t,x,a_{\delta},E\times F)\) where \(E\subset\mathsf{X}\) is open, \(x+t+s\notin E\), and \(\pi(F)\neq 0\). On one hand,

\[P^{\pi}_{t+s}(E\times F\,|\,x,a_{\delta})=\pi(F)\delta_{x++s}(E)=0.\]

On the other hand,

\[\int P^{\pi}_{s}(E\times F\,|\,y,a)\,P^{\pi}_{t}(\mathrm{d}y\mathrm{d}a\,|\,x,a_{\delta})=\frac{\pi(F)}{2}(P_{s}(E\,|\,x+t,a_{\delta})+\delta_{x+t+s}(E))>0.\]

So

\[P^{\pi}_{t+s}(E\times F\,|\,x,a_{\delta})\neq\int P^{\pi}_{s}(E\times F\,|\,y,a)\,P^{\pi}_{t}(\mathrm{d}y\mathrm{d}a\,|\,x,a_{\delta}).\]

At present, the question of how to generate a well-defined (even in law) state-action process in any continuous-time MDP framework given a stochastic policy is generally open. Of course, if \(\pi\) is deterministic, then the state-action process is \((X_{s},\pi(s,X_{s}))_{s\geq t}\). If \(b\) and \(\sigma\) are Lipschitz in state and action, uniformly in time, and \(\pi\) is Lipschitz in state, uniformly in time, then (A.1) with \(A_{s}=\pi(s,X_{s})\) is well-posed.

Proofs

### The Distributional Action Gap

In this section, we prove the statements made in Section 3.

Before proving any of the statements made in Section 3, we recall an identity that relates the \(W_{p}\) distance between two probability measures \(\mu\) and \(\nu\) and the absolute central \(p\)th moments of the differences of random variables distributed according to \(\mu\) and \(\nu\):

\[W_{p}(\mu,\nu)^{p}=\inf_{(X,Y)}\{\mathbf{E}[|X-Y|^{p}]:\text{law}(X)=\mu\text{ and law}(Y)=\nu\}.\] (B.1)

This identity will be used a number of times, including in the proof of Section 3's first result, which we restate here for the readers convenience.

**Proposition 3.4**.: _For all \((t,x)\in\mathsf{T}\times\mathsf{X}\), we have that \(\mathsf{distgap}_{p}(\zeta_{h}^{\pi},t,x)\geq\mathsf{gap}(Q_{h}^{\pi},t,x)\)._

Proof.: Let \((Z_{1},Z_{2})\) be any random vector with such that \(\text{law}(Z_{i})=\zeta_{h}^{\pi}(t,x,a_{i})\) for \(i=1,2\). Then, by Jensen's inequality,

\[\mathbf{E}[|Z_{1}-Z_{2}|^{p}]^{1/p}\geq\mathbf{E}[|Z_{1}-Z_{2}|]\geq|\mathbf{ E}[Z_{1}-Z_{2}]|=|Q_{h}^{\pi}(t,x,a_{1})-Q_{h}^{\pi}(t,x,a_{2})|.\]

Hence, since \((Z_{1},Z_{2})\) was arbitrary, by (B.1),

\[W_{p}(\zeta_{h}^{\pi}(t,x,a_{1}),\zeta_{h}^{\pi}(t,x,a_{2}))\geq|Q_{h}^{\pi}( t,x,a_{1})-Q_{h}^{\pi}(t,x,a_{2})|.\]

Finally, taking the minimum over pairs of actions \((a_{1},a_{2})\) such that \(a_{1}\neq a_{2}\) concludes the proof. 

Now we move on to the proofs of Theorems 3.5 and 3.7. We defer the proof of Theorem 3.6 until after the proof of Theorem 3.7 as the proofs of Theorems 3.5 and 3.7 are similar. For clarity's sake, we first prove a collection of lemmas.

**Lemma B.1**.: _Under Assumption 2.2, let \((X_{s}^{a})_{s\geq t}\) be the unique strong solution to (2.2) with \(X_{t}^{a}=x\)\(\mathbf{P}\)-a.s. Then, for all \(q\geq 1\) and for all \(s\geq t\),_

\[\mathbf{E}[|X_{s}^{a}-x|^{2q}]\leq C_{q}C_{2.2}^{2q}(1+|x|)^{2q}((s-t)^{q}+1) (s-t)^{q}e^{C_{q}C_{2.2}^{2q}((s-t)^{q}+1)(s-t)^{q}}\]

_where \(C_{q}=4^{2q-1}\)._

Proof.: Let \(C_{q}=4^{2q-1}\). By Jensen's inequality and Ito's isometry, observe that

\[|X_{s}^{a}-x|^{2q}\leq C_{q}(s-t)^{2q-1}\int_{t}^{s}|b(s^{\prime },x,a)|^{2q}\,\mathrm{d}s^{\prime}+C_{q}(s-t)^{q-1}\int_{t}^{s}|\sigma(s^{ \prime},x,a)|^{2q}\,\mathrm{d}s^{\prime}\] \[\qquad\qquad+C_{q}(s-t)^{2q-1}\int_{t}^{s}|b(s^{\prime},X_{s^{ \prime}}^{a},a)-b(s^{\prime},x,a)|^{2q}\,\mathrm{d}s^{\prime}\] \[\qquad\qquad+C_{q}(s-t)^{q-1}\int_{t}^{s}|\sigma(s^{\prime},X_{s^ {\prime}}^{a},a)-\sigma(s^{\prime},x,a)|^{2q}\,\mathrm{d}s^{\prime}\] \[\qquad\qquad\leq C_{q}((s-t)^{2q}+(s-t)^{q})C_{2.2}^{2q}(1+|x|)^{ 2q}\] \[\qquad\qquad+C_{q}((s-t)^{2q-1}+(s-t)^{q-1})C_{2.2}^{2q}\int_{t}^ {s}|X_{s^{\prime}}^{a}-x|^{2q}\,\mathrm{d}s^{\prime}.\]

Thus, the lemma follows after taking expectation and applying Gronwall's inequality. 

**Lemma B.2**.: _Under Assumptions 2.2 and 2.3, let \((X_{s}^{\bullet})_{s\geq t}\) with \(\bullet\in\{\pi,\pi|_{h,a,t}\}\) be the unique strong solution to (2.2) with \(X_{t}^{\bullet}=x\)\(\mathbf{P}\)-a.s. Then, for all \(s\leq t+h\),_

\[\mathbf{E}[|X_{s}^{\pi|_{h,a,t}}-X_{s}^{\pi}|^{2q}]\leq C(1+|x|)^{2q}((s-t)^{ q}+1)(s-t)^{q}e^{C((s-t)^{q}+1)(s-t)^{q}}\]

_where \(C\) is some finite positive constant depending on \(q\), \(C_{2.2}\), and \(C_{2.3}\)._Proof.: Let \(C_{q}=8^{2q-1}\). Note that \(X_{s^{\prime}}^{\pi|_{h,a,t}}=X_{s^{\prime}}^{a}\). \(\mathbf{P}\)-a.s. for all \(s^{\prime}\leq t+h\), by the definition of \(\pi|_{h,a,t}\) and the uniqueness of strong solutions to (2.1). So, by Jensen's inequality and Ito's isometry, observe that

\[|X_{s}^{\pi|_{h,a,t}}-X_{s}^{\pi}|^{2q}\leq C_{q} ((s-t)^{2q-1}+(s-t)^{q-1})\bigg{(}\int_{t}^{s}(C_{2.2}^{2q}+C_{2.3} ^{2q})|X_{s^{\prime}}^{a}-x|^{2q}\,\mathrm{d}s^{\prime}\] \[+\int_{t}^{s}(C_{2.2}^{2q}+C_{2.3}^{2q})(1+|x|)^{2q}\,\mathrm{d}s^ {\prime}\] \[+\int_{t}^{s}C_{2.3}^{2q}|X_{s^{\prime}}^{\pi|_{h,a,t}}-X_{s^{ \prime}}^{\pi}|^{2q}\,\mathrm{d}s^{\prime}\bigg{)}.\]

Thus, after taking expectation, we deduce that

\[\mathbf{E}[|X_{s}^{\pi|_{h,a,t}}-X_{s}^{\pi}|^{2}]\leq C_{q} ((s-t)^{2q}+(s-t)^{q})\bigg{(}(C_{2.2}^{2q}+C_{2.3}^{2q})\max_{s^ {\prime}\in[s,t]}\mathbf{E}[|X_{s^{\prime}}^{a}-x|^{2q}|]\] \[+(C_{2.2}^{2q}+C_{2.3}^{2q})(1+|x|)^{2q}\bigg{)}\] \[+C_{q}((s-t)^{2q-1}+(s-t)^{q-1})C_{2.3}^{2q}\int_{t}^{s}\mathbf{E }[|X_{s^{\prime}}^{\pi|_{h,a,t}}-X_{s^{\prime}}^{\pi}|^{2q}]\,\mathrm{d}s^{ \prime}.\]

And so, the lemma follows after applying Lemma B.1 and Gronwall's inequality. 

**Lemma B.3**.: _Under Assumptions 2.2 and 2.3, let \((X_{s}^{\bullet})_{s\geq t}\) with \(\bullet\in\{\pi,\pi|_{h,a,t}\}\) be the unique strong solution to (2.2) with \(X_{t}^{\bullet}=x\)\(\mathbf{P}\)-a.s. Then, for all \(s>t+h\),_

\[\mathbf{E}[|X_{s}^{\pi|_{h,a,t}}-X_{s}^{\pi}|^{2q}]\leq C(1+|x|)^{2q}(h^{q}+1 )h^{q}e^{C((s-t-h)^{q}+1)(s-t-h)^{q}}\]

_where \(C\) is some finite positive constant depending on \(q\), \(C_{2.2}\), and \(C_{2.3}\)._

Proof.: Let \(C_{q}=3^{2q-1}\). Observe that

\[X_{s}^{\bullet}=X_{t+h}^{\bullet}+\int_{t+h}^{s}b^{\bullet}(s^{\prime},X_{s^{ \prime}}^{\bullet})\,\mathrm{d}s^{\prime}+\int_{t+h}^{s}\sigma^{\bullet}(s^{ \prime},X_{s^{\prime}}^{\bullet})\,\mathrm{d}B_{s^{\prime}}\]

So, as \(\pi|_{h,a,t}(\cdot\,|s^{\prime},y)=\pi(\cdot\,|s^{\prime},y)\) for all \((s^{\prime},y)\in\mathsf{T}\setminus[t,t+h)\times\mathsf{X}\), by Jensen's inequality and Ito's isometry,

\[|X_{s}^{\pi|_{h,a,t}}-X_{s}^{\pi}|^{2q}\leq C_{q} |X_{t+h}^{\pi|_{h,a,t}}-X_{t+h}^{\pi}|^{2q}+C_{q}\Big{(}(s-t-h)^{2q-1}\] \[+(s-t-h)^{q-1}\Big{)}C_{2.3}^{2q}\int_{t+h}^{s}|X_{s^{\prime}}^{ \pi|_{h,a,t}}-X_{s^{\prime}}^{\pi}|^{2q}\,\mathrm{d}s^{\prime}.\]

Thus, after taking expectation, applying Gronwall's inequality, and considering Lemma B.2, the lemma follows. 

One consequence of Lemmas B.2 and B.3 is

\[\mathbf{E}[|X_{s}^{\pi|_{h,a,t}}-X_{s}^{\pi}|^{p}]\to 0\quad\text{as}\quad h \downarrow 0,\]

for all \(s\in\mathsf{T}\) and for all \(p\in[1,\infty)\). And so, if \(f\) is bounded, then \(f(X_{T}^{\pi|_{h,a,t}})-f(X_{T}^{\pi})\) is bounded and converges to zero \(\mathbf{P}\)-a.s. as \(h\) converges to zero. The dominated convergence theorem implies that

\[\mathbf{E}[|(f(X_{T}^{\pi|_{h,a,t}})-f(X_{T}^{\pi}))|^{p}]\to 0\quad\text{as} \quad h\downarrow 0.\] (B.2)

Similarly, we see that the functions \(g_{h}:\mathsf{T}\to[0,\infty)\) defined by

\[g_{h}(s):=\mathbf{E}[|r(s,X_{s}^{\pi|_{h,a,t}})-r(s,X_{s}^{\pi})|^{p}]\]

are uniformly bounded (in \(h\)) and converge to zero as \(h\downarrow 0\) for every \(s\in\mathsf{T}\). Hence, by the dominated convergence theorem, again,

\[\int_{t}^{T}\mathbf{E}[|r(s,X_{s}^{\pi|_{h,a,t}})-r(s,X_{s}^{\pi})|^{p}]\, \Gamma(\mathrm{d}s)\to 0\quad\text{as}\quad h\downarrow 0\] (B.3)

where \(\Gamma(\mathrm{d}s):=(\gamma^{s-t}/C_{T,t,\gamma})\,\mathrm{d}s\) for \(C_{T,t,\gamma}:=(\gamma^{T-t}-1)/\log\gamma\). We now prove Theorem 3.5.

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_EMPTY:20]

Second, we prove Theorem 4.3.

**Theorem 4.3**.: _Let \(\kappa\in\mathscr{C}(\mu,\mu)\) for some \(\mu\in\mathscr{P}(\mathbb{R})\). The push-forward of \(\kappa\) by \(\Delta\) is the delta at zero, \(\Delta_{\#}\kappa=\delta_{0}\), if and only if \(\kappa\) is a \(W_{p}\)-optimal coupling, for some \(p\in[1,\infty)\). Moreover, there is only one such coupling. It is given by \(\kappa_{\mu}:=(\operatorname{id},\operatorname{id})_{\#}\mu\) or, equivalently, \(\kappa_{\mu}:=(F_{\mu}^{-1},F_{\mu}^{-1})_{\#}\mathscr{U}(0,1)\). Here \(\mathscr{U}(0,1)\) is the uniform distribution on \([0,1]\)._

First, we establish that there is only one \(W_{p}\) optimal coupling between a given \(\mu\in\mathscr{P}(\mathbb{R})\) and itself, for every \(p\in[1,\infty)\).

**Lemma B.5**.: _Let \(\mu\in\mathscr{P}(\mathbb{R})\). There is only one \(W_{p}\) optimal coupling between \(\mu\) and itself, for every \(p\in[1,\infty)\). It is \(\kappa_{\mu}:=(\operatorname{id},\operatorname{id})_{\#}\mu\in\mathscr{C}( \mu,\mu)\)._

Proof.: Let \(\kappa\in\mathscr{C}(\mu,\mu)\), and suppose there exists \(\epsilon>0\) for which

\[c_{\epsilon}:=\kappa(\{|z-w|\geq\epsilon\,:\,z,w\in\mathbb{R}\})>0.\]

Then

\[\int|z-w|^{p}\,\kappa(\operatorname{d}z\mathrm{d}w)\geq\int_{\{|z-w|\geq \epsilon\}}|z-w|^{p}\,\kappa(\operatorname{d}z\mathrm{d}w)\geq|\epsilon|^{p}c _{\epsilon}>0=\int|z-w|^{p}\,\kappa_{\mu}(\operatorname{d}z\mathrm{d}w).\]

Hence, \(\kappa\), as considered, is not optimal. Since \(\epsilon\) was arbitrary, it follows that a \(W_{p}\) optimal coupling is concentrated on \(\{z=w\}\). Therefore, every optimal coupling is of the form \((\operatorname{id},\operatorname{id})_{\#}\nu\) for some \(\nu\in\mathscr{P}(\mathbb{R})\). As the marginals of such a coupling are \(\nu\) and \(\nu\), we deduce that \(\nu=\mu\), as desired. 

Proof of Theorem 4.3.: Let \(\kappa\in\mathscr{C}(\mu,\mu)\) be such that \(\Delta_{\#}\kappa=\delta_{0}\). Observe that

\[\int|z-w|^{p}\,\kappa(\operatorname{d}z\mathrm{d}w)=\int r^{2}\,\delta_{0}( \operatorname{d}r)=0=\int|z-w|^{p}\,\kappa_{\mu}(\operatorname{d}z\mathrm{d} w),\]

where, again, \(\kappa_{\mu}=(\operatorname{id},\operatorname{id})_{\#}\mu\). Hence, \(\kappa=\kappa_{\mu}\), by Lemma B.5. On the other hand, since \(\kappa_{\mu}\) is concentrated on \(\{z=w\}\), for any bounded, continuous function \(g\), we see

\[\int g(r)\,\Delta_{\#}\kappa_{\mu}(\operatorname{d}r)=\int g(z-w)\,\kappa_{\mu }(\operatorname{d}z\mathrm{d}w)=\int_{\{z=w\}}g(z-w)\,\kappa_{\mu}( \operatorname{d}z\mathrm{d}w)=g(0).\]

In turn, \(\Delta_{\#}\kappa_{\mu}=\delta_{0}\), as desired. 

**Remark B.6**.: _Under the hypotheses of Theorem 3.7, we see that the \(\nicefrac{{1}}{{2}}\)-rescaled superiority distributions at any \((t,x,a)\) for \(h\in(0,1]\) is a family of probability measures with uniformly bounded second moment. Hence, this family is tight. So, up to subsequences, these rescalings converges to limiting probability measure as \(h\downarrow 0\). An interesting open question, is whether or not these subsequential limits are the same._

Third, we prove Theorems 4.5 and 4.6. The proofs of these theorems are a consequence of the following expression for the \(W_{p}\) distance between \(\mu\) and \(\nu\) when \(\mu,\nu\in\mathscr{P}(\mathbb{R})\):

\[W_{p}(\mu,\nu)^{p}=\int_{0}^{1}|F_{\mu}^{-1}(\tau)-F_{\nu}^{-1}(\tau)|^{p} \operatorname{d}\tau.\]

**Theorem 4.5**.: _MDPs and policies exist satisfying Assumptions 2.2 and 2.3 in and under which, for all \((t,x)\in\mathsf{T}\times\mathsf{X}\), we have that \(\mathsf{distgap}_{p}(\psi_{h;q}^{\pi},t,x)\stackrel{{>}}{{{}_{ \sim}}}h^{\nicefrac{{1}}{{2}}-q}\)._

Proof.: Recall that \(F_{\psi_{h;q}^{\pi}(t,x,a)}^{-1}=h^{-q}(F_{\zeta_{h}^{\pi}(t,x,a)}^{-1}-F_{ \eta^{\pi}(t,x)}^{-1})\), for all \(a\in\mathsf{A}\). Hence, for every \(a_{1}\neq a_{2}\),

\[W_{p}(\psi_{h;q}^{\pi}(t,x,a_{1}),\psi_{h;q}^{\pi}(t,x,a_{2}))^{p} =\int_{0}^{1}|F_{\psi_{h;q}^{\pi}(t,x,a_{1})}^{-1}(\tau)-F_{\psi_{ h;q}^{\pi}(t,x,a_{2})}^{-1}(\tau)|^{p}\operatorname{d}\tau\] \[=h^{-qp}\int_{0}^{1}|F_{\zeta_{h}^{\pi}(t,x,a_{1})}^{-1}(\tau)-F_ {\zeta_{h}^{\pi}(t,x,a_{2})}^{-1}(\tau)|^{p}\] \[=h^{-qp}W_{p}(\zeta_{h}^{\pi}(t,x,a_{1}),\zeta_{h}^{\pi}(t,x,a_{2} ))^{p}.\]Thus, taking the \(p\)th root of both sides of the above equality, we deduce that

\[W_{p}(\psi^{\pi}_{h;q}(t,x,a_{1}),\psi^{\pi}_{h;q}(t,x,a_{2}))=h^{-q}W_{p}(\zeta^ {\pi}_{h}(t,x,a_{1}),\zeta^{\pi}_{h}(t,x,a_{2})).\]

The example presented in Theorem 3.6 is such that \(W_{p}(\zeta^{\pi}_{h}(t,x,a_{1}),\zeta^{\pi}_{h}(t,x,a_{2}))\,\raisebox{-2.15pt}{$ \stackrel{{>}}{{\sim}}$}\,h^{\nicefrac{{1}}{{2}}}\). Whence, \(W_{p}(\psi^{\pi}_{h;q}(t,x,a_{1}),\psi^{\pi}_{h;q}(t,x,a_{2}))\,\raisebox{-2.15pt}{$ \stackrel{{>}}{{\sim}}$}\,h^{\nicefrac{{1}}{{2}}-q}\). 

**Theorem 4.6**.: _Under Assumptions 2.2 and 2.3, if \(r\) is Lipschitz in state, uniformly in time, \(f\) is Lipschitz, and \(T<\infty\), then \(\mathsf{distgap}_{p}(\psi^{\pi}_{h;q},t,x)\,\raisebox{-2.15pt}{$ \stackrel{{<}}{{\sim}}$}\,h^{\nicefrac{{1}}{{2}}-q}\), for all \((t,x)\in\mathsf{T}\times\mathsf{X}\)._

The proof of Theorem 4.6 is almost identical to the proof of Theorem 4.5.

Proof.: Arguing as in the proof of Theorem 4.5, we see that

\[W_{p}(\psi^{\pi}_{h;q}(t,x,a_{1}),\psi^{\pi}_{h;q}(t,x,a_{2}))=h^{-q}W_{p}( \zeta^{\pi}_{h}(t,x,a_{1}),\zeta^{\pi}_{h}(t,x,a_{2})).\]

By Theorem 3.7, it follows that \(W_{p}(\psi^{\pi}_{h;q}(t,x,a_{1}),\psi^{\pi}_{h;q}(t,x,a_{2}))\,\raisebox{-2.15 pt}{$\stackrel{{<}}{{\sim}}$}\,h^{\nicefrac{{1}}{{2}}-q}\). 

Finally, we prove Theorem 4.8

**Theorem 4.8**.: _Let \(\rho_{\beta}\) be a distortion risk measure, \(q\geq 0\), and \(h>0\). If \(\rho_{\beta}(\eta^{\pi}(t,x))<\infty\), then \(\arg\max_{a\in\mathsf{A}}\rho_{\beta}(\psi^{\pi}_{h;q}(t,x,a))=\arg\max_{a\in \mathsf{A}}\rho_{\beta}(\zeta^{\pi}_{h}(t,x,a))\)._

Proof.: Observe that

\[\rho_{\beta}(\psi^{\pi}_{h;q}(t,x,a)) =\int_{0}^{1}F^{-1}_{\psi^{\pi}_{h;q}(t,x,a)}(\tau)\,\beta(\mathrm{ d}\tau)\] \[=\int_{0}^{1}h^{-q}F^{-1}_{\psi^{\pi}_{h}(t,x,a)}(\tau)\,\beta( \mathrm{d}\tau)\] \[=h^{-q}\int_{0}^{1}(F^{-1}_{\zeta^{\pi}_{h}(t,x,a)}(\tau)-F^{-1}_ {\eta^{\pi}(t,x)}(\tau))\,\beta(\mathrm{d}\tau)\] \[=h^{-q}\rho_{\beta}(\zeta^{\pi}_{h}(t,x,a))-h^{-q}\rho_{\beta}( \eta^{\pi}(t,x))\]

Since \(\rho_{\beta}(\eta^{\pi}(t,x))\) is independent of \(a\) and \(h>0\),

\[\arg\max_{a}\rho_{\beta}(\psi^{\pi}_{h;q}(t,x,a))=\arg\max_{a}\rho_{\beta}( \zeta^{\pi}_{h}(t,x,a)).\]

## Appendix C Algorithms and Pseudocode

Here we discuss methods for policy optimization via distributional superiority. In practice, computers operate at a finite frequency--as such, all policies we consider here will be assumed to apply each action for \(h\) units of time, as in the settings of [34, 16].

Before describing the superiority learning algorithms, we first remark on the form of exploration policies used in our approaches. We consider policies of the form

\[\pi^{\text{explore}}:\mathbb{R}^{\mathsf{A}}\times\mathbb{R}^{n}\to\mathsf{A}.\]

The first argument to such a policy is a vector of "action-values". In our case, given a superiority distribution \(\psi^{\pi}_{h;q}(t,x,\cdot)\), the action values may be \((\rho_{\beta}(\psi^{\pi}_{h;q}(t,x,a)))_{a\in\mathsf{A}}\) for a distortion risk measure \(\rho_{\beta}\). This generalizes the notion of \(Q\)-values for distributional learning.

The second argument represents a noise variable, in order to support stochastic policies. As input to our algorithms, we require a probability measure \(\mathbb{P}_{\mathrm{act}}\) on processes \((\epsilon_{t})_{t\geq 0}\). This framework generalizes common exploration methods in deep RL. For example, to recover \(\epsilon\)-greedy policies, \(\mathbb{P}_{\mathrm{act}}\) represent a 2-dimensional white noise, and

\[\pi^{\text{explore}}(v,\xi)=\begin{cases}\arg\max_{a}v_{a}&\text{if }\xi_{1}\leq \epsilon\\ a_{\lceil\xi_{2}|\mathsf{A}|\rceil}&\text{otherwise}.\end{cases}\] (C.1)Alternatively, one might choose to correlate the action noise. Approaches such as DDPG [21] and DAU present such examples, where action noise evolves over time as an Ornstein-Uhlenbeck process. This can be implemented in the framework above by choosing \(\mathbb{P}_{\mathrm{act}}\) as the distribution of an \(|\mathrm{A}|\)-dimensional Ornstein-Uhlenbeck process, and defining

\[\pi^{\mathrm{explore}}(v,\xi)=\arg\max_{a\in\mathds{A}}\left(v+\xi\right).\] (C.2)

In our experiments, we found that \(\epsilon\)-greedy exploration was sufficient for approximate policy optimization. To keep our implementation closest to the QR-DQN baseline, therefore, our implementations use the definition of \(\pi^{\mathrm{explore}}\) from equation (C.1).

The remainder of this section details the implementation of DSUP(\(q\)) and DAU+DSUP(\(q\)) as introduced in Section 4.2. Additionally, we provide source code for our implementations at https://github.com/harwitz/distributional-superiority.

### Distributional Superiority

Generally, our goal is to learn a \(\rho_{\beta}\)-greedy policy (see Definition 4.7). Since all policies apply actions for \(h\) units of time, in order to satisfy Axiom 2, we want

\[S_{h}^{\pi}(t,x,\pi(x))\equiv 0,\]

where \(\pi:x\mapsto\arg\max_{a}\rho_{\beta}(S_{h}^{\pi}(t,x,a))\) is the \(\rho_{\beta}\)-greedy policy. As such, following the DAU algorithm of [34], our algorithms will model quantile functions \(\phi(t,x,a)\in\mathbb{R}^{m}\) that aim to satisfy

\[F_{\psi_{h,q}^{\pi}(t,x,a;h)}^{-1}\approx\phi(t,x,a)-\phi(t,x,a^{\star})\quad \text{for some}\quad a^{\star}\in\arg\max_{a}\rho_{\beta}(\phi(t,x,a)).\]

Our primary algorithmic contribution integrates such a model, with proper superiority rescaling, into the QR-DQN framework of [10] for estimating action-conditioned return distributions. It is outlined in Algorithm 2.

To deal with the increased time-resolution of transitions, [34] modified the step size by a factor of \(h\). More recently, [3] found that subsampling transitions before storing them in the replay buffer was most effective in their high-decision-frequency domain. Thus, we opt for such a strategy here. Rather than only storing every \(h^{-1}\) transitions, we randomly select transitions to store according to independent \(\text{Bernoulli}(h)\) draws in order to avoid the possibility of only capturing cyclic phenomena in the replay buffer. We found that this strategy worked similarly to that of [34], but is far less computationally expensive. Likewise, as \(h\) decreases, we extend the number of training interactions by a factor of \(h^{-1}\). This corresponds to training for a constant amount of time units across decision frequencies, and likewise, a constant number of gradient updates across decision frequencies.

### Two-Timescale Advantage-Shifted Distributional Superiority

An astute reader might recognize that while \(\psi_{h;q}^{\pi}\) may have nonzero distributional action gap, since \(h^{q}\) is asymptotically larger than \(h\) for \(q<1\), the works of [34, 16] would suggest that the _expected_ action gap under \(\psi_{h;q}^{\pi}\) should vanish. To account for this, we propose shifting the rescaled superiority quantiles by the advantage function, as estimated e.g. in DAU [34]. It is clear that such a procedure cannot cause the distributional action gap to vanish. The resulting procedure is depicted in Algorithm 3, with the modifications relative to Algorithm 2 highlighted in blue. In practice, we employ a shared feature extractor in the representations of \(A_{h}\) and \(\phi\) to reap the representation learning benefits of DRL [4] when approximating the advantage.

### Influence of the Rescaling Factor

The algorithms 2 and 3 are parameterized by a _rescaling factor_\(q\in(0,1]\), which is meant to compensate for the collapse of the distributional action gap. Larger values of \(q\) correspond to larger compensation. In this work, we argue that the distributional action gap collapses at rate \(h^{\nicefrac{{1}}{{2}}}\), leading to a natural choice of \(q=\nicefrac{{1}}{{2}}\), which theoretically preserves constant order action gaps with respect to the decision frequency. We also test the approach with \(q=1\), which corresponds to the well-known scaling rate for preserving expected value action gaps.

For any \(q>\nicefrac{{1}}{{2}}\), the distributional action gap theoretically grows without bound as \(h\downarrow 0\), leading to distributional estimates with arbitrarily large variance. On the other hand, for any \(q<\nicefrac{{1}}{{2}}\), the distributional action gap decays to \(0\) as \(h\downarrow 0\), which makes it difficult to identify the best actions in the presence of approximation error.

```
0:\(m\) (number of quantiles) and \(q\) (rescale factor)
0:\(h^{-1}\) (decision frequency), \(\mu\) (initial state distribution)
0:\(\pi^{\text{explore}}\) (exploration policy) and \(\rho_{\beta}\) (distortion risk measure)
0:\(\mathbb{P}_{\text{act}}\) (action noise distribution)
0:\(\theta(t,x),\phi(t,x,a)\in\mathbb{R}^{m}\), parameterized quantile representations for \(\eta^{\pi}(t,x),\psi_{h;q}^{\pi}(t,x,a)\).
0:\(\overline{\theta}(t,x)\in\mathbb{R}^{m}\), target quantile representations for \(\eta^{\pi}(t,x)\).
0:\(n_{\theta}\) (target update period), \(\alpha\) (step size), \(N\) (batch size) \(\mathcal{D}\leftarrow\emptyset\)  reset \(\leftarrow\) True for\(i\in\mathbb{N}\)do // Gather data with exploratory policy \((\xi)_{s\geq 0}\sim\mathbb{P}_{\text{act}}\) \(\triangleright\) Sample from stochastic process for exploration noise for\(j\in\{0,\ldots,\lfloor h^{-1}\rfloor\}\)do ifreset is True then \(t\gets 0\) and \(x_{0}\sim\mu\)  reset \(\leftarrow\) False endif \(a^{\star}\leftarrow\arg\max_{a}\rho_{\beta}\left(\frac{1}{m}\sum_{n=1}^{m} \delta_{\phi(t,x_{t},a)_{n}}\right)\) \(F_{\psi_{h;q}}^{-1}(a)\leftarrow\phi(t,x_{t},a)-\phi(t,x_{t},a^{\star})\) for each \(a\in\mathsf{A}\) \(v_{a}\leftarrow\rho_{\beta}\left(\frac{1}{m}\sum_{n=1}^{m}\delta_{F_{\psi_{h;q }}^{-1}(a)_{n}}\right)\) for each \(a\in\mathsf{A}\) \(a_{t}\sim\pi^{\text{explore}}((v_{a})_{a\in\mathsf{A}},\xi_{jh})\)  Perform action \(a_{t}\) for \(h\) units of time, observe \((r_{t},x_{t+h},\mathsf{done}_{t+h})\). \(Y_{j}\sim\mathsf{Bernoulli}(h)\)\(\triangleright\) Subsample transitions for replay if\(Y_{j}=1\vee\mathsf{done}_{j+1}=\texttt{True}\)then \(\mathcal{D}\leftarrow\mathcal{D}\cup\{(t,x_{t},a_{t},r_{t},x_{t+h},\mathsf{done}_{t+h})\}\) endif  reset \(\leftarrow\)\(\mathsf{done}_{t+h}\) and \(t\gets t+h\) endfor // Update return/superiority distributions  Sample minibatch \(\{(t_{k},x_{k},a_{k},r_{k},x_{k}^{\prime},\mathsf{done}_{k})\}_{k=1}^{N}\) from \(\mathcal{D}\) for\(k\in[N]\)do \(a_{k}^{\star}\leftarrow\arg\max_{a}\rho_{\beta}\left(\frac{1}{m}\sum_{n=1}^{m} \delta_{\phi(t_{k},x_{k},a)_{n}}\right)\)\(\triangleright\) Risk-sensitive greedy action \(F_{\zeta}^{-1}(\phi,\theta)\leftarrow\theta(t_{k},x_{k})+h^{q}(\phi(t_{k},x_{k},a_{k})-\phi(t_{k},x_{k},a^{\star}))\)\(\triangleright\) Prediction (4.4) \(\mathcal{T}F_{\zeta}^{-1}\gets hr_{k}+\gamma^{h}(1-\mathsf{done}_{k}) \overline{\theta}(t_{k}+h,x_{k}^{\prime})+\gamma^{h}\mathsf{done}_{k}f(x_{k}^ {\prime})\)\(\triangleright\) Target (4.5) \(\ell_{k}(\phi,\theta)\leftarrow\mathsf{QuantileHuber}(F_{\zeta}^{-1}(\phi, \theta),\mathcal{T}F_{\zeta}^{-1})\)\(\triangleright\) See [10, Eq. (10)] endfor \(\ell(\phi,\theta)\leftarrow\frac{1}{N}\sum_{k=1}^{N}\ell_{k}(\phi,\theta)\) \(\phi\leftarrow\phi-\alpha\nabla_{\phi}\ell(\phi,\theta)\)\(\theta\leftarrow\theta-\alpha\nabla_{\theta}\ell(\phi,\theta)\)\(\triangleright\) Gradient updates if\(i\mid n_{\theta}\)then \(\overline{\theta}\leftarrow\theta\) endif endfor ```

**Algorithm 2** Distributional Superiority Quantile Regression

```
0:\(m\) (number of quantiles) and \(q\) (rescale factor)
0:\(h^{-1}\) (decision frequency), \(\mu\) (initial state distribution)
0:\(\pi^{\text{explore}}\) (exploration policy) and \(\rho_{\beta}\) (distortion risk measure)
0:\(\mathbb{P}_{\mathrm{act}}\) (action noise distribution)
0:\(\theta(t,x),\phi(t,x,a)\in\mathbb{R}^{m}\), parameterized quantile representations for \(\eta^{\pi}(t,x),\psi^{\pi}_{h;q}(t,x,a)\).
0:\(\overline{\theta}(t,x)\in\mathbb{R}^{m}\), target quantile representations for \(\eta^{\pi}(t,x)\).
0:\(\widetilde{A}_{h}(t,x,a)\) (parameterized representation of advantage function)
0:\(n_{\theta}\) (target update period), \(\alpha\) (step size), \(N\) (batch size) \(\mathcal{D}\leftarrow\emptyset\) reset\(\leftarrow\)True for\(i\in\mathbb{N}\)do // Gather data with exploratory policy \((\xi)_{s\geq 0}\sim\mathbb{P}_{\mathrm{act}}\)\(\triangleright\) Sample from stochastic process for exploration noise for \(j\in\{0,\dots,\lfloor h^{-1}\rfloor\}\)do ifreset is True then \(t\gets 0\) and \(x_{0}\sim\mu\) reset\(\leftarrow\)False endif \(a^{\star}\leftarrow\arg\max_{a}\rho_{\beta}\left(\frac{1}{m}\sum_{n=1}^{m} \delta_{\phi(t,x_{i},a)_{n}+(1-h^{1-q})\widetilde{A}_{h}(t,x_{i},a)}\right)\) \(F^{-1}_{\psi_{h;q}}\left(a\right)\leftarrow\phi(t,x_{t},a)-\phi(t,x_{t},a^{\star})\) for each \(a\in\mathsf{A}\) \(A_{h}(t,x_{t},\cdot)\leftarrow\widetilde{A}_{h}(t,x_{t},\cdot)-\widetilde{A}_ {h}(t,x_{t},a^{\star})\)\(\triangleright\) see [34, DAU] \(v_{a}\leftarrow\rho_{\beta}\left(\frac{1}{m}\sum_{n=1}^{m}\delta_{F^{-1}_{ \psi_{h;q}}(a)_{n}+(1-h^{1-q})\widetilde{A}_{h}(t,x_{t},a)}\right)\) for each \(a\in\mathsf{A}\) \(a_{t}\sim\pi^{\text{explore}}((v_{a})_{a\in\mathsf{A}},\xi_{jh})\)  Perform action \(a_{t}\) for \(h\) units of time, observe \((r_{t},x_{t+h},\mathsf{done}_{t+h})\). \(Y_{j}\sim\mathsf{Bernoulli}(h)\)\(\triangleright\) Subsample transitions for replay if\(Y_{j}=1\vee\mathsf{done}_{j+1}=\texttt{True}\)then \(\mathcal{D}\leftarrow\mathcal{D}\cup\{(t,x_{t},a_{t},r_{t},x_{t+h},\mathsf{done}_{t+h})\}\) endif  reset\(\leftarrow\)done\(t+h\) and \(t\gets t+h\) endfor // Update return/superiority distributions Sample minibatch \(\{(t_{k},x_{k},a_{k},r_{k},x^{\prime}_{k},\mathsf{done}_{k})\}_{k=1}^{N}\) from \(\mathcal{D}\) for\(k\in[N]\)do \(a^{\star}\leftarrow\arg\max_{a}\rho_{\beta}\left(\frac{1}{m}\sum_{n=1}^{m} \delta_{\phi(t_{k},x_{k},a)_{n}+(1-h^{1-q})\widetilde{A}_{h}(t_{k},x_{k},a)}\right)\) // Advantage Loss \(A_{h}(t_{k},x_{k},a_{k})\leftarrow\widetilde{A}_{h}(t_{k},x_{k},a_{k})- \widetilde{A}_{h}(t_{k},x_{k},a^{\star})\) \(V(t_{k},a_{k})\leftarrow\frac{1}{m}\sum_{n=1}^{m}\theta(t_{k},x_{k})_{n}\) \(Q(t_{k},x_{k},a_{k})\gets V(t_{k},a_{k})+hA_{h}(t_{k},x_{k},a_{k})\) \(\mathcal{T}Q(t_{k},x_{k},a_{k})=hr_{k}+\gamma^{h}\frac{1}{m}\sum_{n=1}^{m} \overline{\theta}(t_{k}+h,x_{k+1})_{n}\) \(\ell^{\mathrm{adv}}_{k}(\widetilde{A}_{h})\leftarrow(Q(t_{k},x_{k},a_{k})- \mathcal{T}Q(t_{k},x_{k},a_{k}))^{2}\)\(\triangleright\) Bellman error // Superiority Loss \(F^{-1}_{\zeta}(\phi,\theta)\leftarrow\theta(t_{k},x_{k})+h^{q}(\phi(t_{k},x_{k},a_{k})-\phi(t_{k},x_{k},a^{\star}))\)\(\triangleright\) Prediction (4.4) \(\mathcal{T}F^{-1}_{\zeta}\gets hr_{k}+\gamma^{h}(1-\mathsf{done}_{k}) \overline{\theta}(t_{k}+h,x^{\prime}_{k})+\gamma^{h}\mathsf{done}_{k}f(x^{ \prime}_{k})\)\(\triangleright\) Target (4.5) \(\ell_{k}(\phi,\theta)\leftarrow\mathsf{QuantileHuber}(F^{-1}_{\zeta}(\phi, \theta),\mathcal{T}F^{-1}_{\zeta})\)\(\triangleright\) See [10, Eq. (10)] endfor \(\ell(\phi,\theta)\leftarrow\frac{1}{N}\sum_{k=1}^{N}\ell_{k}(\phi,\theta)\) and \(\ell^{\mathrm{adv}}(\widetilde{A}_{h})\leftarrow\frac{1}{2N}\sum_{k=1}^{N}\ell ^{\mathrm{adv}}_{k}(\widetilde{A}_{h})\) \(\phi\leftarrow\phi-\alpha\nabla_{\phi}\ell(\phi,\theta)\) and \(\theta\leftarrow\theta-\alpha\nabla_{\theta}\ell(\phi,\theta)\) and \(\widetilde{A}_{h}\leftarrow\widetilde{A}_{h}-\alpha\nabla\ell^{\mathrm{adv}}( \widetilde{A}_{h})\) if\(i\,|\,n_{\theta}\)then \(\overline{\theta}\leftarrow\theta\) endif endfor ```

**Algorithm 3** Advantage-Shifted Distributional Superiority Quantile Regression

## Appendix D Additional Experimental Results

Figure D.1 include results comparing the performance of DSUP(\(\nicefrac{{1}}{{2}}\)) and QR-DQN for risk-sensitive option trading across a variety of decision frequencies.

## Appendix E Simulation Details

Here we collect further information about the setup for the simulations described in Section 5.2.

### Option Trading Environment

The environment used for the high-frequency option-trading setup is identical to that of Lim and Malik [22]. The environment emulates policies that decide when to exercise American call options. The state space is modeled as \(\mathsf{X}=\mathbb{R}\) and with \(\mathsf{T}=[0,T]\). Notably, existing works such as [22] and [17] describe the state space by \(\mathsf{X}=\mathbb{R}^{2}\), where one dimension represents time--in our setup, we generally condition policies and returns on time, so we indeed model policies and returns as functions on \(\mathbb{R}^{2}\). The state \(X_{t}\) represents the price of the option at time \(t\), and evolves according to a geometric Brownian motion.

Figure D.1: Risk-sensitive option trading performance for various decision frequencies \(\omega\).

There are two actions in the environment. Action \(0\) "holds" the option, while action \(1\) represents "execute". Upon taking action \(1\) (or equivalently, once the time reaches \(T\)), the option is _executed_, and the agent receives a reward \(f(x)=\max(0,1-x)\) and the episode terminates; here \(x\) represents the price at the time of execution. No rewards are incurred otherwise.

Following the setup of [22], the dynamics of the prices are simulated based on data collected between years 2016 and 2019 from 10 commodities on the DOW market. Lim and Malik proposed a method for estimating the most likely parameters of geometric Brownian motion to fit the data for each commodity, which is then used to simulate many environment rollouts for training and evaluation. This is particularly convenient for our setup, where we additionally scale the decision frequency, corresponding to finer time discretizations of the Euler-Maruyama scheme for the estimated geometric Brownian motion. Like [22], separate dynamics parameters are estimated (for each commodity) between training and evaluation: the dynamics used for training are estimated on prefixes of the data, and those for testing (post-training) are estimated on suffixes of the data. Results are reported on the testing dynamics, averaged over the 10 commodities.

As is standard [22; 17], we simulate the environment with \(T=100\) and \(X_{0}=1\). The simulations from [22] correspond to \(h=1\) in this setting. In our high-frequency simulations, we discretize the dynamics with timestep \(h<1\).

### Hyperparameters

In Table 1, we list the hyperparameters used in the simulations for the tested algorithms. We note that although the original DAU implementation scaled the learning rate with \(h\), we take an alternative approach by updating every \(h^{-1}\) environment steps akin to [3]. This is discussed in more detail in Appendix C.

### Compute Resources

Our implementations are written in Jax [6] and executed with a single NVidia V100 GPU. At highest decision frequencies, experiments took longer to execute, averaging out at a maximum of roughly four hours.

\begin{table}
\begin{tabular}{l|l l}
**Method** & **Parameter** & **Value** \\ \hline All & Replay buffer sampling & Uniform \\  & Replay buffer capacity & \(20000\) \\  & Batch size & \(32\) \\  & Optimizer & Adam [18] \\  & Learning rate & 1e-4 \\  & Discount factor & \(0.999\) \\  & Target network update period & \(1000\) \\  & \(\epsilon\)-greedy exploration parameter & Linear decay from \(1\) to \(0.02\) \\ \hline DAU [34] & Value network & MLP (2 hidden, 100 units, ReLU) \\  & Advantage network & MLP (2 hidden, 100 units, ReLU) \\ \hline QR-DQN [10; 22] & Quantile network torso & MLP (2 hidden, 100 units, ReLU) \\  & Number of atoms & \(100\) \\  & Quantile Huber parameter \(\kappa\) & \(1\) \\ \hline DSUP & \(\eta\) network torso & MLP (2 hidden, 100 units, ReLU) \\  & \(\phi\) (superiority proxy) network torso & MLP (2 hidden, 100 units, ReLU) \\  & Number of atoms & \(100\) \\  & Quantile Huber parameter \(\kappa\) & \(1\) \\ DAU+DSUP & Advantage network torso & (shared with \(\phi\)) \\ \end{tabular}
\end{table}
Table 1: Hyperparameters

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We believe our abstract and introduction outline and faithfully summarize the content of our work. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have included discussions of the limitations and scope of our results throughout our work, rather than within a specific "Limitations" section. See, for example, Section 5. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their bestjudgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: We have provided complete statements, proofs, and references to used results in our proofs of our theoretical results. Please see the Appendix for restatements and proofs. Statements can be found in the main body of our work. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We believe we have included enough information that others can faithfully reproduce our main experimental results. Please see Sections 4 and 5 and the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We have provided data, code, and experimental details. We believe we have included enough information that others can faithfully reproduce our main experimental results. Please see Section 5 and the Appendix. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provided these details in Section 5 and the Appendix. Guidelines: ** The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: When appropriate, we have included error bars and information regarding the statistical significance of our experiments. Please see Section 5 and the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We have included these details in Section 5 and the Appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.

* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have reviewed the NeurIPS Code of Ethics and believe our work conforms to it in every respect. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We believe our work is foundational research without a direct path to negative societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?Answer: [NA] Justification: We believe our work poses no risk of misuse. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Data for the option-trading environment was downloaded from an open source Github repository, and we cited the work that introduced the dataset [22]. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our work does not release new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.

* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our work neither involves crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our work neither involves crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.