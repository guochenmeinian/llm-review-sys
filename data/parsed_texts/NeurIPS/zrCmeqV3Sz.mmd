# Learning Invariant Representations of Graph Neural Networks via Cluster Generalization

 Donglin Xia1, Xiao Wang2*, Nian Liu1, Chuan Shi1*

1Beijing University of Posts and Telecommunications

2Beihang University

{donglin.xia, nianliu, shichuan}@bupt.edu.cn, xiao_wang@buaa.edu.cn

Corresponding authors.

Footnote 2: Code available at https://github.com/BUPT-GAMMA/CITGNN

###### Abstract

Graph neural networks (GNNs) have become increasingly popular in modeling graph-structured data due to their ability to learn node representations by aggregating local structure information. However, it is widely acknowledged that the test graph structure may differ from the training graph structure, resulting in a structure shift. In this paper, we experimentally find that the performance of GNNs drops significantly when the structure shift happens, suggesting that the learned models may be biased towards specific structure patterns. To address this challenge, we propose the Cluster Information Transfer (**CIT**) mechanism2, which can learn invariant representations for GNNs, thereby improving their generalization ability to various and unknown test graphs with structure shift. The CIT mechanism achieves this by combining different cluster information with the nodes while preserving their cluster-independent information. By generating nodes across different clusters, the mechanism significantly enhances the diversity of the nodes and helps GNNs learn the invariant representations. We provide a theoretical analysis of the CIT mechanism, showing that the impact of changing clusters during structure shift can be mitigated after transfer. Additionally, the proposed mechanism is a plug-in that can be easily used to improve existing GNNs. We comprehensively evaluate our proposed method on three typical structure shift scenarios, demonstrating its effectiveness in enhancing GNNs' performance.

Footnote 2: Code available at https://github.com/BUPT-GAMMA/CITGNN

## 1 Introduction

Graphs are often easily used to model individual properties and inter-individual relationships, which are ubiquitous in the real world, including social networks, e-commerce networks, citation networks. Recently, graph neural networks (GNNs), which are able to effectively employ deep learning on graphs to learn the node representations, have attracted considerable attention in dealing with graph data [13, 24, 32, 36, 9, 5]. So far, GNNs have been applied to various applications and achieved remarkable performance, _e.g._, node classification [13, 32], link prediction [33, 14] and graph classification [6, 34].

Message-passing mechanism forms the basis of most graph neural networks (GNNs) [13, 24, 5, 9]. That is, the node representations are learned by aggregating feature information from the neighbors in each convolutional layer. So it can be seen that the trained GNNs are highly dependent on the local structure. However, it is well known that the graph structure in the real world always changes. For instance, the paper citations and areas would go through significant change as time goes by in citation network [11]. In social networks, nodes represent users and edges represent activity between users, because they will be changed dynamically, the test graph structure may also change [7]. This change in the graph structure is a form of distribution shift, which we refer to as graph structure shift. Sothe question naturally arises: _when the test graph structure shift happens, can GNNs still maintain stability in performance?_

We present experiments to explore this question. We generate graph structure and node features. Subsequently we train GNNs on the initially generated graph structure and gradually change the structures to test the generalization of GNNs (more details are in Section 2). Clearly, the performance consistently declines with changes (shown in Figure 1), implying that the trained GNNs are severely biased to one typical graph structure and cannot effectively address the structure shift problem.

This can also be considered as Out-Of-Distribution problem (OOD) on graph. To ensure good performance, most GNNs require the training and test graphs to have identically distributed data. However, this requirement cannot always hold in practice. Very recently, there are some works on graph OOD problem for node classification. One idea assumes knowledge of the graph generation process and derives regularization terms to extract hidden stable variables [28; 8]. However, these methods heavily rely on the graph generation process, which is unknown and may be very complex in reality. The other way involves sampling unbiased test data to make their distribution similar to that of the training data [37]. However, it requires sampling test data beforehand, which cannot be directly applied to the whole graph-level structure shift, _e.g._, training the GNNs on a graph while the test graph is another new one, which is a very typical inductive learning scenario [24; 17].

Therefore, it is still technically challenging to learn the invariant representations which is robust to the structure shift for GNNs. Usually, the invariant information can be discovered from multiple structure environments [28], while we can only obtain one local structure environment given a graph. To avoid GNNs being biased to one structure pattern, directly changing the graph structure, _e.g._, adding or removing edges, may create different environments. However, because the graph structure is very complex in the real world and the underlying data generation mechanism is usually unknown, it is very difficult to get the knowledge on how to generate new graph structures.

In this paper, we propose to learn the invariant representations of GNNs by transferring the cluster information of the nodes. First, we apply GNNs to learn the node representations, and then combine it with spectral clustering to obtain the cluster information in this graph. Here, we propose a novel Cluster Information Transfer (CIT) mechanism, because the cluster information usually captures the local properties of nodes and can be used to generate multiple local environments. Specifically, we characterize the cluster information using two statistics: the mean of cluster and the variance of cluster, and transfer the nodes to new clusters based on these two statistics while keeping the cluster-independent information. After training GNNs on these newly generated node representations, we aim to enhance their generalization ability across various test graph structures by improving the generalization ability on different clusters. Additionally, we provide insights into the transfer process from the embedding space and theoretically analyze the impact of changing clusters during structure shift can be mitigated after transfer. The contributions of our work are summarized as follows:

* We study the problem of structure shifts for GNNs, and propose a novel CIT mechanism to improve the generalization ability of GNNs. Our proposed mechanism is a friendly plug-in, which can be easily used to improve most of the current GNNs.
* Our proposed CIT mechanism enables that the cluster information can be transferred while preserving the cluster-independent information of the nodes, and we theoretically analyze that the impact of changing clusters during structure shift can be mitigated after transfer.
* We conduct extensive experiments on three typical structure shift tasks. The results well demonstrate that our proposed model consistently improves generalization ability of GNNs on structure shift.

## 2 Effect of structure distribution shift on GNN performance

In this section, we aim to explore the effect of structure shift on GNN performance. In the real world, the tendency of structure shift is usually unknown and complex, so we assume a relatively simple scenario to investigate this issue. For instance, we train the GNNs on a graph with two community structures, and then test the GNNs by gradually changing the graph structures. If the performance of GNNs drops sharply, it indicates that the trained GNNs are biased to the original graph, and cannot be well generalized to the new test graphs with structure shift.

We generate a network with 1000 nodes and divide all nodes into two categories on average, that is, 500 nodes are assigned label 0 and the other 500 nodes are assigned label 1. Meanwhile, node features, each of 50 dimensions, are generated by two Multivariate Gaussian distributions. The node features with same labels are generated by same Gaussian distribution. We employ the Stochastic Blockmodel (SBM) [12] to generate the graph structures. We set two community and set the generation edges probability on inter-community is 0.5% and intro-community is 0.05%. We randomly sample 20 nodes per class for training, and the rest are used for testing. Then we train GCN [13] and GAT [24] on this graph. The test graph structures are generated as follows: we decrease the inter-community edge probability from 0.5% to 0.25% and increase the intra-community probability from 0.05% to 0.3%.

The accuracy is shown in Figure 1. The x-axis is the probability of edges, where the first number is the edge probability of inter-community and the second number is the edge probability of intro-community. As we can see, because of the structure shift, the performance of GNN declines significantly. It shows that once the test graph pattern shifts, the reliability of GNNs becomes compromised.

## 3 Methods

Notations:Let \(G=(\textbf{A},\textbf{X})\) represent a training attributed graph, where \(\textbf{A}\in\mathbb{R}^{n\times n}\) is the symmetric adjacency matrix with \(n\) nodes and \(\textbf{X}\in\mathbb{R}^{n\times d}\) is the node feature matrix, and \(d\) is the dimension of node features. Specifically, \(A_{ij}=1\) represents there is an edge between nodes \(i\) and \(j\), otherwise, \(A_{ij}=0\). We suppose each node belongs to one of \(C\) classes and focus on semi-supervised node classification. Considering that the graphs always change in reality, here we aim to learn the invariant representations for GNNs, where the overall framework is shown in Figure 2 and the whole algorithm is shown in A.1.

### Clustering process

We first obtain the node representations through GNN layers:

\[\textbf{Z}^{(l)}=\sigma(\tilde{\textbf{D}}^{-1/2}\tilde{\textbf{A}}\tilde{ \textbf{D}}^{-1/2}\textbf{Z}^{(l-1)}\textbf{W}_{GNN}^{(l-1)}),\] (1)

where \(\textbf{Z}^{(l)}\) is the node representations from the \(l\)-th layer, \(\textbf{Z}^{(0)}=\textbf{X}\), \(\tilde{\textbf{A}}=\textbf{A}+\textbf{I}\), \(\tilde{\textbf{D}}\) is the degree matrix of \(\tilde{\textbf{A}}\), \(\sigma\) is a non-linear activation and \(\textbf{W}_{GNN}^{(l-1)}\) is the trainable parameters of GNNs. Eq. (1) implies that the node representations will aggregate the information from its neighbors in each layer, so the learned GNNs are essentially dependent on the local structure of each node. Apparently, if the structure shift happens in the test graph, the current GNNs may provide unsatisfactory results. Therefore, it is highly desired that the GNNs should learn the invariant representations while structure changing, so as to handle the structure shift in test graphs.

One alternative way is to simulate the structure change and then learn the invariant representations from different structures, which is similar as domain generalization [26; 35]. Motivated by this, we consider that the local properties of a node represents its domain information. Meanwhile, different clusters can capture different local properties in a graph, so we can consider cluster information as domain information of nodes. Based on this idea, we first aim to obtain the cluster information using spectral clustering [2]. Specifically, we compute the cluster assignment matrix \(\mathbf{S}\) of node using a multi-layer perceptron (MLP) with softmax on the output layer:

\[\textbf{S}=\textit{Softmax}(\textbf{W}_{MLP}\textbf{Z}^{(l)}+\textbf{{b}}),\] (2)

Figure 1: The node classification accuracy of GCN, GAT, APPNP and GCNII on generated data with structure shift. The x-axis is probability of edges (%).

where \(\textbf{W}_{MLP}\) and \(\bm{b}\) are trainable parameters of MLP. For assignment matrix \(\textbf{S}\in\mathbb{R}^{n\times m}\), and \(m\) is the number of clusters. \(s_{ij}\) represents the weight that node \(i\) belongs to cluster \(j\). The softmax activation of the MLP guarantees that the weight \(s_{ij}\in[0,1]\) and \(\textbf{S}^{T}\textbf{1}_{M}=\textbf{1}_{N}\).

For **S**, we want to cluster strongly connected nodes. So we adopt the cut loss which evaluates the mincut given by **S**:

\[\mathcal{L}_{c}=-\frac{Tr(\textbf{S}^{T}\tilde{\textbf{A}}\textbf{S})}{Tr( \textbf{S}^{T}\overline{\textbf{D}}\textbf{S})},\] (3)

where \(Tr\) is the trace of matrix. Minimizing \(\mathcal{L}_{c}\) encourages the nodes which are strongly connected to be together.

However, directly minimizing \(\mathcal{L}_{c}\) will make the cluster assignments are equal for all nodes, which means all nodes will be in the same cluster. So in order to avoid the clustering collapse, we can make the the clusters to be of similar size, and we use orthogonality loss to realize it:

\[\mathcal{L}_{o}=\left\|\frac{\textbf{S}^{T}\textbf{S}}{\|\textbf{S}^{T} \textbf{S}\|_{F}}-\frac{\textbf{1}_{M}}{\sqrt{M}}\right\|_{F},\] (4)

where \(\|\cdot\|_{F}\) indicates the Frobenius norm, \(M\) is the number of clusters. Notably, when \(\textbf{S}^{T}\textbf{S}=\textbf{I}_{M}\), the orthogonality loss \(\mathcal{L}_{o}\) will reach 0. Therefore, minimizing it can encourage the cluster assignments to be orthogonal and thus the clusters can be of similar size.

Overall, we optimize the clustering process by these two losses:

\[\mathcal{L}_{u}=\mathcal{L}_{c}+\lambda_{1}\mathcal{L}_{o},\] (5)

where \(\lambda_{1}\) is a coefficient to balance the mincut process and orthogonality process.

After obtaining the assignment matrix **S**, we can calculate cluster representations \(\textbf{H}^{c}\) by average the node representations \(\textbf{Z}^{(l)}\):

\[\textbf{H}^{c}=\textbf{S}^{T}\textbf{Z}^{(l)}.\] (6)

### Transfer cluster information

Now we obtain the cluster representations \(\textbf{H}^{c}\), and each cluster captures the information of local properties. Originally, given a graph, a node can be only in one domain, _i.e._, one cluster. Next, we aim to transfer the node to different domains. Specifically, the cluster information can be characterized by two statistics, _i.e._, the center of the cluster (\(\textbf{H}^{c}\)) and the standard deviation of the cluster (\(\sigma(\textbf{H}^{c}_{k})\)):

Figure 2: The overall framework of our proposed CIT mechanism on GNNs consists two parts: the traditional GNNs and Cluster Information Transfer (CIT) mechanism. After getting node representations from GNNs, we conduct CIT mechanism on node representations before the last layer of GNNs, which transfers the node to another cluster to generate new representations for training.

\[\sigma(\textbf{H}_{k}^{c})=\sqrt{\Sigma_{i=1}^{n}s_{ik}(\textbf{Z}_{i}^{(l)}- \textbf{H}_{k}^{c})^{2}},\] (7)

where \(\textbf{H}_{k}^{c}\) is the representation of the \(k\)-th cluster and \(\textbf{Z}_{i}^{(l)}\) is the representation of node \(i\).

Then we can transfer the node \(i\) in the \(k\)-th cluster to the \(j\)-th cluster as follows:

\[\textbf{Z}_{i}^{\prime(l)}=\sigma(\textbf{H}_{j}^{c})\frac{\textbf{Z}_{i}^{(l )}-\textbf{H}_{k}^{c}}{\sigma(\textbf{H}_{k}^{c})}+\textbf{H}_{j}^{c},\] (8)

where \(k\) is the cluster that node \(i\) belongs to, and \(j\) is the cluster which is randomly selected from the remaining clusters.

Here, we explain the Eq. (8) in more details. As shown in Figure 3, the center of cluster \(k\) is \(\textbf{H}_{k}^{c}\), and node \(i\) belongs to this cluster, where the representation of node \(i\) is \(\textbf{Z}_{i}^{(l)}\). It can be seen that \(\textbf{H}_{k}^{c}\) is obtained by aggregating and averaging the local structure information, which captures the cluster information. Therefore, \(\textbf{Z}_{i}^{(l)}-\textbf{H}_{k}^{c}\) represents the cluster-independent information. Then, the CIT mechanism can be seen as we transfer the node to a new position from embedding space. The standard deviation is the weighted distance of nodes from the center, which is the aggregating scope of the clusters. After the transfer, the target node surrounds a new cluster with new cluster information, while keeping the cluster-independent information.

The above process only transfers the nodes on the original domain. Moreover, in order to improve the robustness of the model for unknown domain, we increase the uncertainty and diversity of model [16; 21]. Based on this, we add Gaussian perturbations to this process. The whole transfer becomes:

\[\textbf{Z}_{i}^{\prime(l)}=(\sigma(\textbf{H}_{j}^{c})+\epsilon_{\sigma}\Sigma _{\sigma})\frac{\textbf{Z}_{i}^{(l)}-\textbf{H}_{k}^{c}}{\sigma(\textbf{H}_{ k}^{c})}+(\textbf{H}_{j}^{c}+\epsilon_{\mu}\Sigma_{\mu}).\] (9)

The statistics of Gaussian is determined by the whole features:

\[\epsilon_{\sigma}\sim\mathcal{N}(0,1),\epsilon_{\mu}\sim\mathcal{N}(0,1),\] (10)

\[\Sigma_{\sigma}^{2}=\sigma(\sigma(\textbf{H}^{c})^{2})^{2},\Sigma_{\mu}^{2}= \sigma(\textbf{H}^{c})^{2}.\] (11)

Here, with the Gaussian perturbations, we can generate new clusters based on the original one and the nodes can be further transferred to more diverse domains.

The proposed transfer process offers a solution to overcome the limitations of changing graph structures when generating nodes in different domains. Traditional approaches require changing the graph structure by adding or deleting edges, which can be challenging due to the lack of prior knowledge on how the changes in graph structure may affect the domain. Here, our proposed CIT mechanism addresses this challenge by directly transferring a node to another domain through the manipulation of cluster properties of nodes in the embedding space. The CIT method is implemented before the classifier in GNNs training, making it backbone agnostic and compatible with any GNN last layer.

### Objective function

With CIT mechanism, we have the generated representation \(\textbf{Z}_{i}^{\prime(l)}\) in other clusters for node \(i\). In traditional GNNs training, we input the representation \(\textbf{Z}^{(l)}\), where the \(i\)-th row vector represents the representation of node \(i\), to the cross-entropy loss [13]. Here, we randomly select a part of nodes, and replace original representation \(\textbf{Z}_{i}^{(l)}\) with the generated representation \(\textbf{Z}_{i}^{\prime(l)}\). Then we can obtain the new representation matrix \(\textbf{Z}^{\prime(l)}\), and optimize the cross-entropy loss based on it as follows:

\[\mathcal{L}_{f}=\mathcal{L}_{e}(f_{\theta}(\textbf{Z}^{\prime(l)}),\textbf{Y}),\] (12)

Figure 3: We show two parts of one graph. The orange and green points represent two clusters in one graph. The circle is aggregating scope of cluster. And the red point represents the target node we transfer from orange cluster to green cluster.

where \(\mathcal{L}_{e}\) is cross-entropy loss function, \(f_{\theta}\) is the classifier, and \(\mathbf{Y}\) is the labels.

Finally, the overall optimization function is as follows:

\[\mathcal{L}=\mathcal{L}_{f}+\lambda_{2}\mathcal{L}_{u},\] (13)

where \(\lambda_{2}\) is a coefficient to balance the classification process and clustering process. In the optimization process, we randomly select \(n\times p\) nodes and then conduct Eq. (9) every \(k\) epochs.

### Theoretical analysis

In this section, we theoretically analyze our CIT mechanism from the perspective of domain adoption theory. Since the Eq. (8) is the cornerstone of our CIT mechanism and Eq. (9) is an extending from it, we use Eq. (8) to represent our CIT mechanism in the following proof for convenience.

We analyze that the classifier using the new generated representation \(\mathbf{Z}^{\prime(l)}\) in Eq. (8) has better generalization on the structure shift. As mentioned before, cluster information can capture local properties of nodes, so we convert this problem to analyzing the classifier has better generalization on cluster shift. To achieve this, following [31], we take the binary classification task as the example, and analyze the relationship between the decision boundary with cluster information. For analysis, we follow [31] using a fisher classifier and analyse the relationship between them. According to Fisher's linear discriminant analysis [1; 23; 3], the decision boundary of fisher classifier depends on two statistics \(Var(Z)\) and \(Cov(Z,Y)\).

**Theorem 1**.: _The decision boundary of fisher classifier is affected by the cluster information._

The proof is given in Appendix A.3, where we calculate the mathematical expression of the classification boundary. Theorem 1 indicates that the cluster information will "help" the classifier to make decisions. But when the graph structure changes, the cluster information of the nodes also changes. Therefore, the classifier using cluster information to make decisions is not reliable.

**Theorem 2**.: _Let \(Z_{R}\) represent the node representations in cluster \(R\). Assume that there are \(p\) percent of nodes are transferred from cluster \(R\) to cluster \(D\) by \(\Sigma_{D}\frac{Z_{R}-\mu_{R}}{\Sigma_{R}}+\mu_{D}\). After the transfer, the impact of changing clusters during structure shift can be mitigated._

The proof is given in Appendix A.4. Comparing the expression of classifier before and after our CIT mechanism, we can find that we mitigate the impact of changing clusters during structure shift, enhancing the robustness of the model against such changes.

## 4 Experiment

**Datasets and baselines.** To comprehensively evaluate the proposed CIT mechanism, we use six diverse graph datasets. Cora, Citeseer, Pubmed [20], ACM, IMDB [27] and Twitch-Explicit [17]. Details of datasets are in Appendix B.1. Our CIT mechanism can be used for any other GNN backbones. We plug it in four well-known graph neural network methods, GCN [13], GAT [24], APPNP [9] and GCNII [5]. Meanwhile, we compare it with two graph OOD methods which are also used for node-classification, SR-GNN [37] and EERM [28]. We combine them with four methods mentioned before. Meanwhile, we use CIT-GNN(w/o) to indicate that no Gaussian perturbations are added.

**Experimental setup.** For Cora, Citeseer and Pubmed, we construct structure shift by making perturbations on the original graph structure. For ACM and IMDB, we test them using two relation structures. For Twitch-Explicit, we use six different networks. We take one network to train, one network to validate and the rest of networks to test. So we divide these datasets into three categories: perturbation on graph structures, Multiplex networks and Multigraph. The implementation details are given in Appendix B.1.

### Perturbation on graph structures data

In this task, we use Cora, Citeseer and Pubmed datasets. We train the model on original graph. We create new graph structures by randomly adding 50%, 75% and deleting 20%, 50% edges of original graph. To comprehensively evaluate our model on random structure shift, we test our model on the new graphs. We follow the original node-classification settings [13] and use the common evaluation metrics, including Macro-F1 and classification accuracy. For brief presentation, we show results of deleting edges in Appendix B.2.

The results are reported in Table 1. From the table we can see that the proposed CIT-GNN generally achieves the best performance in most cases. Especially, for Acc and Macro-f1, our CIT-GNN achieves maximum relative improvements of 5.35% and 4.1% respectively on Citesser-Add-0.75. The results demonstrate the effectiveness of our CIT-GNN. We can also see that CIT-GNN improves the four basic methods, so the results show that our method can improve the generalization ability of the basic models. Meanwhile, our mechanism is to operate the node representation at the embedding level, which can be used for any GNN backbones.

### Multiplex networks data

In this task, we use Multiplex networks datasets ACM and IMDB to evaluate the capabilities of our method on different relational scenarios. Both of them have two relation structures. We construct structure shift by taking the structure of one relation for training and the other for testing respectively. The partition of data follows [27]. The results are reported in Table 2 and the relation structure shown in the table is the training structure. Different from the first experiment, the structure shift is not random, which is more intense because the new graph is not based on the original graph. As can be seen, our proposed CIT-GNN improves the four basic methods in most cases, and outperforms

\begin{table}
\begin{tabular}{l|c c c c|c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{Cora} & \multicolumn{4}{c}{ADD-0.5} \\ \cline{2-7}  & Acc & Macro-f1 & Acc & Macro-f1 & Acc & Macro-f1 \\ \hline GCN & 76.1240.91 & 74.9540.88 & 65.4340.68 & 6.30240.59 & 72.5841.10 & 71.8441.00 \\ SR-GCN & 75.7040.90 & 74.3540.85 & 66.0310.30 & 62.6740.80 & 72.151.80 & 70.6341.90 \\ EERM-GCN & 75.3340.87 & 74.3940.89 & 64.0540.49 & 60.979.61 & & \\ CT-GCN(w/o) & 76.8830.34 & 75.6340.47 & 66.6270.55 & 64.2140.47 & 72.5320.32 & 72.01\(\pm\)0.21 \\ CTT-GCN & **76.9840.49** & **75.8830.44** & **67.6540.44** & **64.424.10** & **73.7640.40** & **72.9440.30** \\ \hline GAT & 77.0440.30 & 76.1540.40 & 64.4240.41 & 61.740.30 & 71.3040.52 & 70.8040.43 \\ SR-GAT & 77.3540.75 & 76.4920.77 & 64.800.29 & 61.9880.11 & 71.5540.52 & 70.7940.64 \\ EERM-GAT & 76.1540.38 & 73.9240.29 & 62.0540.79 & 59.010.65 & & \\ CTT-GAT(w/o) & **77.3740.67** & **76.7340.47** & 65.2340.58 & **63.360.67** & 71.9240.68 & 71.12\(\pm\)0.56 \\ CTT-GAT & 77.2340.42 & 76.2640.28 & **66.3430.24** & 63.070.37 & 72.520.470.74 & **71.5720.82** \\ \hline APPNP & 79.5440.50 & 77.6920.70 & 66.9660.76 & 64.0880.66 & 75.880.81 & 75.3720.66 \\ SR-APPNP & 80.0040.70 & 78.560.87 & 65.2040.23 & 62.7740.36 & 75.8540.55 & 75.4340.58 \\ EERM-APPNP & 78.1040.73 & 76.7240.69 & 66.3040.91 & 63.0880.77 & & \\ CTT-APPNP(w/o) & 79.7940.400.79 & 79.9540.32 & 60.8040.52 & 63.080.32 & 76.2140.55 & 75.2340.37 \\ CTT-APPNP & **80.5040.39** & **78.860.24** & **68.5440.71** & **65.1540.45** & **76.6440.40** & **75.8940.48** \\ \hline GCN & 76.980.92 & 74.9240.97 & 63.161.20 & 71.140.78 & 74.031.11 & 73.370.75 \\ SR-GCN1 & 77.5540.21 & 75.090.41 & 64.741.86 & 62.4441.53 & 75.1040.78 & 73.4640.95 \\ EERM-GCNII & **79.0541.10** & **76.624.123** & 65.1040.64 & 62.0240.76 & & \\ CTT-GCNII(w/o) & 77.6440.63 & 75.2240.61 & 65.8740.80 & 63.6360.75 & 75.0040.47 & 74.7040.34 \\ CTT-GCNII & 78.2840.88 & 75.8240.73 & **66.1240.97** & 63.170.85 & **75.9540.63** & **75.4770.76** \\ \hline GCN & 72.3730.55 & 71.0940.36 & 63.3430.60 & 61.090.540 & 72.480.31 & 71.0640.58 \\ SR-GCN & 72.7041.10 & 72.1941.20 & 62.7241.80 & 59.5842.10 & 70.352.10 & 69.1442.30 \\ EERM-GCN & 72.3040.21 & 71.6840.47 & 61.6540.54 & 58.5540.68 & & \\ CTT-GCN(w/o) & 79.9040.53 & 71.700.07 & 64.8430.79 & 62.3340.56 & 73.0040.46 & 72.3020.36 \\ CTT-GCN & **74.440.75** & **73.7370.86** & **64.809.65** & **62.5220.46** & **73.2020.36** & **72.3340.41** \\ \hline GAT & 73.8640.45 & 72.7940.47 & 63.421.00 & 61.3540.79 & 70.8840.67 & 69.9740.64 \\ SR-GAT & 72.8240.28 & 73.640.43 & 64.2741.00 & 62.040.90 & 70.3640.69 & 69.2440.88 \\ EERM-GAT & 72.6240.43 & 73.280.30 & 60.200.43 & 60.3040.50 & & \\ CTT-GAT(w/o) & 74.6440.77 & 73.7640.87 & 63.836.79 & 62.3340.56 & 71.060.077 & 69.300.36 \\ CTT-GAT & **74.7450.40** & **73.6740.85 & **64.740.67** & **62.3240.77** & **71.9040.89** & **70.88.67** \\ \hline APPNP & 75.866.84 & 73.9740.87 & 65.711.01 & 63.6530.86 & 74.5320.77 & 73.9940.80 \\ SR-APPNP & 76.0040.30 & 73.984.04 & 64.8040.37 & 62.7840.21 & 75.404.58 & 74.3040.68 \\ EERM-APPNP & 75.3040.05 & 74.8740.62 & 64.9040.69 & 62.3240.60 & & \\ CTT-APPNP(w/o) & 77.5340.67 & 75.6040.53 & 77.6012.81 & 64.7840.73 & **76.0630.87** & **75.91.03.2\({}^{*}\) \\ CTT-APPNPNP & **78.0240.56** & **76.5340.78** & **66.0660.95** & **63.810.58** & 75.7040.41 & 75.8840.83 \\ \hline GCNIII & 73.161.05 & 71.014.39 & 62.481.20 & 60.8040.40 & 75.7840.58 & 75.1540.61 \\ SR-GCNII & 75.0340.50 & 72.2840.93 & 60.9042.10 & 59.001.80 & 75.9841.10 & 75.7940.90 \\ EERM-GCNII & 75.5041.20 & 73.601.3SR-GNN and EERM, implying that our CIT-GNN can improve the generalization ability of the basic models.

### Multigraph data

In this task, we use Multigraph dataset Twitch-Explicit, and its each graph is collected from a particular region. In this dataset, the node features also change but they still share the same input feature space and output space. The graphs are collected from different regions. So different structures are determined by different regions. To comprehensively evaluate our method, we take the FR for training, TW for validation, and test our model on the remaining four graphs (DE, ENGB, RS, EU). We choose ROC-AUC score for evaluation because it is a binary classification task. Since SR-GNN is not suitable for this dataset, only the base model and EERM are compared. The results are reported in Figure 4. Comparing the results on four graph, our model makes improvement in most cases, which verifies the effectiveness about generalizing on multigraph scenario.

### Analysis of hyper-parameters

**Analysis of \(p\).** The probability of transfer \(p\) determines how many nodes to transfer every time. It indicates the magnitude of the CIT mechanism. We vary its value and the corresponding results are shown in Figure 5. With the increase of \(p\), the performance goes up first and then declines, which indicates the performance benefits from an applicable selection of \(p\).

**Analysis of \(k\).** We make transfer process every \(k\) epochs. We vary \(k\) from 1 to 50 and and plot the results in Figure 6. Zero represents the original model accuracy without out CIT mechanism. Notably, the performance remains consistently stable across the varying values of \(k\), indicating the robustness of the model to this parameter.

\begin{table}
\begin{tabular}{l|c c|c c||c c|c c|c c} \hline \multirow{2}{*}{Method} & \multicolumn{3}{c||}{AFM} & \multicolumn{3}{c||}{DIM} & \multicolumn{3}{c}{MSA} \\ \cline{2-11}  & \multicolumn{2}{c|}{PAC} & \multicolumn{2}{c|}{PLP} & \multicolumn{2}{c||}{Macr-11} & \multicolumn{2}{c|}{Acc} & \multicolumn{2}{c||}{Macr-11} & \multicolumn{2}{c|}{Acc} & \multicolumn{2}{c|}{Macr-11} & \multicolumn{2}{c|}{Acc} & \multicolumn{2}{c|}{Macr-11} \\ \hline GCN & 64.6541.91 & 60.6641.88 & 80.2641.98 & 79.7124.91 & 52.3661.40 & 48.5541.60 & 58.9841.11 & 57.014.72 \\ SR-GCN & 67.754.20 & 68.5141.10 & 82.142.10 & 81.8822.32 & 51.9449.07 & 50.760.85 & 59.8448.08 & **59.841.11** & 58.214.11 \\ EERM-GCN & 66.5841.87 & 67.8441.54 & 82.1641.87 & 87.2614.67 & 50.740.78 & 53.1501.62 & 57.214.93 & 56.251.66 \\ CIT-GCN(w/o) & 67.5341.52 & 63.3219.37 & 81.304.58 & 80.9812.82 & 53.672.179 & 50.714.56 & 57.934.32 & 56.234.24 \\ CIT-GCN & **68.0641.13** & **68.974.127** & **82.641.58** & **56.264.16** & **55.421.88** & **52.751.65** & 56.684.14 & 54.661.97 \\ GAT & 66.3541.81 & 67.4328.25 & 52.841.73 & 82.501.65 & 53.759.13 & 4.267.21 & 55.684.17 & 57.427.94 \\ SR-GAT & 67.201.87 & 67.9829.13 & 84.6141.34 & 84.948.65 & 50.811.92 & 46.628.17 & 55.905.41 & 57.424.84 \\ EERM-GAT & 67.671.17 & 67.8242.15 & 72.954.27 & 78.3480.49 & 52.242.18 & 50.681.35 & 58.208.65 & 57.215.99 \\ CIT-GAT(w/o) & 67.1541.23 & 67.341.43 & 83.541.54 & 83.011.46 & **35.319.06** & **51.902.12** & 57.184.35 & 55.628.12 \\ CIT-GAT & **68.491.32** & 86.851.397 & **85.551.76** & **66.461.32** & 52.608.98 & 51.061.061 & **59.514.173** & **53.604.14** \\ \hline APPNP & 78.491.33 & 70.002.56 & 76.7621.82 & 86.7621.82 & 53.718.01 & 53.781.70 & 43.671.32 & 62.012.11 & 54.56 \\ SR-APPNP & 77.602.82 & 76.254.17 & 86.164.37 & 86.164.52 & 55.021.98 & 51.742.03 & 60.701.08 & 60.143.12 \\ EERM-APPNP & 80.891.82 & 80.341.65 & 83.548.58 & 83.461.84 & 54.322.96 & 50.314.07 & 61.274.15 & 63.004.87 \\ EERM-APPNP(w/o) & 81.6641.12 & 81.351.01 & 86.604.08 & 86.524.07 & **86.3741.41** & **53.143.17** & 61.813.02 & 60.981.10 \\ CIT-APPNP & **81.701.581** & **81.604.17** & **7.1941.21** & **87.164.12** & **87.164.02** & 53.864.15 & 52.324.13 & **62.941.45** & **62.001.54** \\ \hline CCNN & 77.921.64 & 76.732 & 77.871.85 & 83.150.78 & 81.212.32 & 57.186.56 & 49.871.77 & 52.425.01 & 47.672.35 \\ SR-GCNN & 78.914.13 & 78.778.176 & 83.3420.89 & 83.221.01 & 53.521.56 & 49.874.13 & 54.202.33 & 49.00.198 \\ EERM-GCNN & 78.821.23 & 79.242.176 & 83.814.21 & 83.614.02 & 33.561.23 & 50.271.42 & **54.324.19** & **39.704.18** \\ CIT-GCNN(w/o) & 78.104.54 & 78.302.61 & 84.851.47 & 83.621.54 & 53.422.20 & 49.104.58 & 52.362.87 & 87.4012.10 \\ CIT-GCNN & **79.734.61*** & **79.264.132*** & **85.234.93*** & **85.064.18*** & **54.204.18*** & **80.914.95*** & 53.694.18.9 & 48.842.01 \\ \hline \end{tabular}
\end{table}
Table 2: Quantitative results (%\(\pm\sigma\)) on node classification for multiplex networks data and the relation in table is for training, while the superscript refers to the results of paired t-test between original model and CIT-GNN (* for 0.05 level and ** for 0.01 level).

Figure 4: ROC-AUC on Twitch where we compare different GNN backbones.

**Analysis of \(m\)**. The number of clusters is the most important parameter in the clustering process. We choose silhouette coefficient to measure the clustering performance and vary \(m\) from 20 to 100 unevenly. Then we calculate the accuracy and Silhouette Coefficient. The corresponding results are shown in Figure 7. As we can see, accuracy changes synchronously with silhouette coefficient. We infer that the performance of our model is related to the clustering situation, and when the clustering process performs well, our model also performs well.

## 5 Related work

**Graph neural networks.** Recently, Graph Neural Networks have been widely studied. GCN [13] proposes to aggregate the node features from the one-hop neighbors. GAT [24] designs an attention mechanism to aggregate node features from neighbors. PPNP [9] utilizes PageRank's node propagation way to aggregate node features and proposes an approximate version. GCNII [5] extends GCN by introducing two effective techniques: initial residual and identity mapping, which make the network deeper. This is a fast growing research field, and more detailed works can be found in [4; 22].

**OOD generalization of GNNs.** Out-Of-Distribution (OOD) on graph has attracted considerable attention from different perspectives. For node-classification, [8] shows node selection bias drastically affects the performance of GNNs and investigates it from point view of causal theory. [37] explores the invariant relationship between nodes and proposes a framework to account for distributional differences between biased training data. [28] handles it by minimizing the mean and variance of risks from multiple environments which are generated by adversarial context generators. For graph classification, [15] proposes to capture the invariant relationships between predictive graph structural information and labels in a mixture of latent environments. [29; 18] find invariant subgraph structures from a causal perspective to improve the generalization ability of graph neural networks. [10] builds a graph OOD bench mark including node-level and graph-level methods and two kinds of distribution shift which are covariate shift and concept shift.

**Graph clustering with graph neural networks.** As graph neural networks continue to perform better in modeling graph data, some GNN-based graph clustering methods have been widely applied. Deep attentional embedded graph clustering [25] uses an attention network to capture the importance of the neighboring nodes and employs the KL-divergence loss in the process of graph clustering. [2] achieves the graph clustering process by obtaining the assignment matrix through minimizing optimizing its spectral objective.[19] uses a new objective function for clustering combining graph spectral modularity maximization and a new regularization method.

## 6 Conclusion

In this paper, we explore the impact of structure shift on GNN performance and propose a CIT mechanism to help GNNs learn invariant representations under structure shifts. We theoretically analyze that the impact of changing clusters during structure shift can be mitigated after transfer.

Figure 5: Analysis of the probability of transfer.

Figure 6: Analysis of the epochtimes.

Figure 7: The number of clusters is varying on cora and citeseer. The accuracy corresponds to the right vertical axis, while the Silhouette Coefficient values correspond to the left vertical axis.

Moreover, the CIT mechanism is a friendly plug-in, and the comprehensive experiments well demonstrate the effectiveness on different structure shift scenarios.

**Limitations and broader impact.** One potential limitation lies in its primary focus on node-level tasks, while further investigation is needed to explore graph-level tasks. Although our CIT mechanism demonstrates significant advancements, certain theoretical foundations remain to be fully developed. Our work explores the graph from the perspective of the embedding space, thereby surpassing the limitations imposed by graph topology, and offers a fresh outlook on graph analysis.

## Acknowledgments and Disclosure of Funding

This work is supported in part by the National Natural Science Foundation of China (No. U20B2045, 62192784, U22B2038, 62002029, 62172052, 62322203).

## References

* [1] Theodore Wilbur Anderson. An introduction to multivariate statistical analysis. Technical report, Wiley New York, 1962.
* [2] Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. Spectral clustering with graph neural networks for graph pooling. In _International Conference on Machine Learning_, pages 874-883. PMLR, 2020.
* [3] T Tony Cai and Linjun Zhang. A convex optimization approach to high-dimensional sparse quadratic discriminant analysis. _The Annals of Statistics_, 49(3):1537-1568, 2021.
* [4] Hongxu Chen, Hongzhi Yin, Tong Chen, Quoc Viet Hung Nguyen, Wen-Chih Peng, and Xue Li. Exploiting centrality information with graph convolutions for network representation learning. In _2019 IEEE 35th International Conference on Data Engineering (ICDE)_, pages 590-601. IEEE, 2019.
* [5] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph convolutional networks. In _International Conference on Machine Learning_, pages 1725-1735. PMLR, 2020.
* [6] Federico Errica, Marco Podda, Davide Bacciu, and Alessio Micheli. A fair comparison of graph neural networks for graph classification. _arXiv preprint arXiv:1912.09893_, 2019.
* [7] Shobeir Fakhraei, James Foulds, Madhusudana Shashanka, and Lise Getoor. Collective spammer detection in evolving multi-relational social networks. In _Proceedings of the 21th acm sigkdd international conference on knowledge discovery and data mining_, pages 1769-1778, 2015.
* [8] Shaohua Fan, Xiao Wang, Chuan Shi, Kun Kuang, Nian Liu, and Bai Wang. Debiased graph neural networks with agnostic label selection bias. _IEEE Transactions on Neural Networks and Learning Systems_, 2022.
* [9] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate: Graph neural networks meet personalized pagerank. _arXiv preprint arXiv:1810.05997_, 2018.
* [10] Shurui Gui, Xiner Li, Limei Wang, and Shuiwang Ji. Good: A graph out-of-distribution benchmark. _arXiv preprint arXiv:2206.08452_, 2022.
* [11] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. _Advances in neural information processing systems_, 33:22118-22133, 2020.
* [12] Brian Karrer and Mark EJ Newman. Stochastic blockmodels and community structure in networks. _Physical review E_, 83(1):016107, 2011.
* [13] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.

* [14] Thomas N Kipf and Max Welling. Variational graph auto-encoders. _arXiv preprint arXiv:1611.07308_, 2016.
* [15] Haoyang Li, Ziwei Zhang, Xin Wang, and Wenwu Zhu. Learning invariant graph representations for out-of-distribution generalization. In _Advances in Neural Information Processing Systems_, 2022.
* [16] Xiaotong Li, Yongxing Dai, Yixiao Ge, Jun Liu, Ying Shan, and Ling-Yu Duan. Uncertainty modeling for out-of-distribution generalization. _arXiv preprint arXiv:2202.03958_, 2022.
* [17] Derek Lim, Xiuyu Li, Felix Hohne, and Ser-Nam Lim. New benchmarks for learning on non-homophilous graphs. _arXiv preprint arXiv:2104.01404_, 2021.
* [18] Siqi Miao, Mia Liu, and Pan Li. Interpretable and generalizable graph learning via stochastic attention mechanism. In _International Conference on Machine Learning_, pages 15524-15543. PMLR, 2022.
* [19] Emmanuel Muller. Graph clustering with graph neural networks. _Journal of Machine Learning Research_, 24:1-21, 2023.
* [20] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. _AI magazine_, 29(3):93-93, 2008.
* [21] Yichun Shi and Anil K Jain. Probabilistic face embeddings. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 6902-6911, 2019.
* [22] Damien Teney, Lingqiao Liu, and Anton van Den Hengel. Graph-structured representations for visual question answering. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1-9, 2017.
* [23] T Tony Cai and Linjun Zhang. High dimensional linear discriminant analysis: optimality, adaptive algorithm and missing data. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 81(4):675-705, 2019.
* [24] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. _arXiv preprint arXiv:1710.10903_, 2017.
* [25] Chun Wang, Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, and Chengqi Zhang. Attributed graph clustering: A deep attentional embedding approach. _arXiv preprint arXiv:1906.06532_, 2019.
* [26] Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Tao Qin, Wang Lu, Yiqiang Chen, Wenjun Zeng, and Philip Yu. Generalizing to unseen domains: A survey on domain generalization. _IEEE Transactions on Knowledge and Data Engineering_, 2022.
* [27] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph attention network. In _The world wide web conference_, pages 2022-2032, 2019.
* [28] Qitian Wu, Hengrui Zhang, Junchi Yan, and David Wipf. Handling distribution shifts on graphs: An invariance perspective. _arXiv preprint arXiv:2202.02466_, 2022.
* [29] Ying-Xin Wu, Xiang Wang, An Zhang, Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks. _arXiv preprint arXiv:2201.12872_, 2022.
* [30] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In _International conference on machine learning_, pages 40-48. PMLR, 2016.
* [31] Huaxiu Yao, Yu Wang, Sai Li, Linjun Zhang, Weixin Liang, James Zou, and Chelsea Finn. Improving out-of-distribution robustness via selective augmentation. In _International Conference on Machine Learning_, pages 25407-25437. PMLR, 2022.
* [32] Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design space for graph neural networks. _Advances in Neural Information Processing Systems_, 33:17009-17021, 2020.

* [33] Muhan Zhang and Yixin Chen. Link prediction based on graph neural networks. _Advances in neural information processing systems_, 31, 2018.
* [34] Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning architecture for graph classification. In _Proceedings of the AAAI conference on artificial intelligence_, volume 32, 2018.
* [35] Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization in vision: A survey. _arXiv preprint arXiv:2103.02503_, 2021.
* [36] Meiqi Zhu, Xiao Wang, Chuan Shi, Houye Ji, and Peng Cui. Interpreting and unifying graph neural networks with an optimization framework. In _Proceedings of the Web Conference 2021_, pages 1215-1226, 2021.
* [37] Qi Zhu, Natalia Ponomareva, Jiawei Han, and Bryan Perozzi. Shift-robust gnns: Overcoming the limitations of localized graph training data. _Advances in Neural Information Processing Systems_, 34:27965-27977, 2021.