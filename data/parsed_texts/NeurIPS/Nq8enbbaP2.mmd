# Occupancy-based Policy Gradient: Estimation, Convergence, and Optimality

 Audrey Huang

Department of Computer Science

University of Illinois Urbana-Champaign

Champaign, IL 61820

audreyh5@illinois.edu

&Nan Jiang

Department of Computer Science

University of Illinois Urbana-Champaign

Champaign, IL 61820

nanjiang@illinois.edu

###### Abstract

Occupancy functions play an instrumental role in reinforcement learning (RL) for guiding exploration, handling distribution shift, and optimizing general objectives beyond the expected return. Yet, computationally efficient policy optimization methods that use (only) occupancy functions are virtually non-existent. In this paper, we establish the theoretical foundations of model-free policy gradient (PG) methods that compute the gradient through the occupancy for both online and offline RL, without modeling value functions. Our algorithms reduce gradient estimation to squared-loss regression and are computationally oracle-efficient. We characterize the sample complexities of both local and global convergence, accounting for both finite-sample estimation error and the roles of exploration (online) and data coverage (offline). Occupancy-based PG naturally handles arbitrary offline data distributions, and, with one-line algorithmic changes, can be adapted to optimize any differentiable objective functional.

## 1 Introduction

Value-based methods have been the dominant paradigm in model-free reinforcement learning, with a solid theoretical foundation in large state spaces under function approximation [1, 2, 3, 4, 5, 6, 7]. In contrast, a model-free RL paradigm based on their natural counterparts--the _occupancy functions_--remains largely under-investigated. Occupancy functions are densities that describe a policy's state visitation, and play instrumental roles in guiding exploration [1, 10, 11], handling distribution shift [1, 12], and optimizing general objectives beyond the expected return [2, 3]. Despite this, they are seldom modeled directly in learning algorithms and appear only in the analyses, except in conjunction with value functions in marginalized importance sampling [1, 13, 14, 15, 16, 17, 18, 19]. Recently, [19] developed algorithms in online and offline RL that model only occupancies via density function classes, spotlighting their roles in handling non-exploratory offline data and in online exploration. However, their focus was on statistical guarantees, and computationally efficient policy optimization for occupancy-based methods remained an open problem.

In answer, we develop model-free policy gradient (PG) algorithms that compute the gradient through occupancy functions, without estimating any values. By leveraging a Bellman-like recursion, we reduce occupancy-based gradient estimation to solving a series of squared-loss minimization problems, which can be done in a computationally oracle-efficient manner. Our analysis captures the effects of gradient estimation error, exploration (in online PG, which is characterized by the initial state distribution), and offline data quality (in offline PG) on the sample and iteration complexity required for local and global convergence. In the online setting, our results complement previous works on the optimality of value-based PG [1, 2] and extend past their scope to include general objectives of occupancy functions, such as entropy maximization for pure exploration and risk-sensitive functionals in safe RL [11]. These objectives generally cannot be optimized using value-based policy gradients because they do not admit value functions or Bellman-like equations with which to estimate them [13, 14].

In the offline setting, we handle gradient estimation from fixed datasets of poor coverage, which departs from most existing (value-based) off-policy PG estimators that assume an exploratory dataset [15, 16, 17, 18]. Learning with non-exploratory data is a core consideration in recent offline RL [16, 18], and gives rise to unique challenges in our setting: occupancies are converted into density ratios for learning purposes, but these ratios become unbounded when the data lacks coverage. [14] used clipping to handle occupancy estimation under poor coverage, which we show is insufficient for gradient estimation (Prop. 4.2). Instead, a novel smooth-clipping mechanism (Sec. 4.2) is developed to provide statistically robust gradient estimates.

App. A includes a full discussion of related work, and our contributions are organized as follows:

1. **Online PG** (Sec. 3) We propose OccuPG, an occupancy-based PG algorithm that reduces gradient estimation to squared-loss minimization, based on a recursive Bellman flow-like update for the occupancy gradient. We analyze the sample complexities for both local and global convergence, and, notably, our algorithm and analyses extend straightforwardly to the optimization of general objective functionals.
2. **Offline PG** (Sec. 4) For offline RL we develop and analyze Off-OccuPG, which optimizes only the portions of a policy's return that are adequately covered by offline data. Conceptually, our algorithm is based on combining the methods in Sec. 3 with (a smoothed version of) the recursively clipped occupancies from [14]. As a result, our estimation and convergence guarantees do not require assumptions on data coverage, which relaxes the restrictions of previous works.

## 2 Preliminaries

**Finite-horizon Markov decision process (MDP).** Finite-horizon MDPs are defined by the tuple \(\mathcal{M}=(\mathcal{S},\mathcal{A},P,R,H,d_{0})\), where \(\mathcal{S}\) is the state space, \(\mathcal{A}\) is the action space, and \(H\) is the horizon. We use \([H]=\{0,\ldots,H\}\) and when clear from the context, use \(\{\square_{h}\}=\{\square_{h}\}_{h\in[H]}\). For notational compactness we assume that \(\mathcal{S}=\cup_{h}\mathcal{S}^{h}\) is the union of \(H\) disjoint sets \(\{\mathcal{S}^{h}\}\), each of which is the set of states reachable at timestep \(h\). This is WLOG as we can always augment the state space with \([H]\) at the cost of only \(H\) factors [15, 16].

Since each state can only be visited at a single timestep, we can now define the (non-stationary) transitions as \(P:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\), and the initial state distribution as \(d_{0}\in\Delta(\mathcal{S}^{0})\). We assume the reward function \(R:\mathcal{S}\rightarrow[0,1]\) is bounded on the unit interval and (for simplicity) state-wise deterministic. This sufficiently captures the challenges of our setting since the occupancies are densities over states, and it will be easily seen later that our results generalize to per-state-action rewards. A policy \(\pi:\mathcal{S}\rightarrow\Delta(\mathcal{A})\) interacting with \(\mathcal{M}\) observes trajectories \(\{(s_{h},a_{h},s_{h+1},r_{h+1})\}_{h=0}^{H-1}\), and has expected return \(J(\pi)=\mathbb{E}_{\pi}[\sum_{h=1}^{H}R(s_{h})]\). At any \((h,s,a)\), its expected return-to-go is encoded in the value function \(Q_{h}^{\pi}(s,a)=\mathbb{E}_{\pi}[\sum_{h^{\prime}>h}R(s_{h^{\prime}})|s_{h}=s,a_{h}=a]\).

For each \(h\in[H]\), a policy's occupancy function \(d_{h}^{\pi}\in\Delta(\mathcal{S})\) is a p.d.f. describing its state visitation, \(d_{h}^{\pi}(s)=\mathbb{P}_{\pi}(s_{h}=s)\). In combination with the policy, the MDP dynamics dictate the evolution of the occupancy over timesteps. This is encoded in the recursive Bellman flow equation, which mandates that \(d_{h}^{\pi}=\mathbf{P}^{\pi}d_{h-1}^{\pi}\) for all \(h\in[H]\). Here, \(\mathbf{P}^{\pi}\) is the Bellman flow operator with \((\mathbf{P}^{\pi}f)(s^{\prime}):=\sum_{s,a}P(s^{\prime}|s,a)\pi(a|s)f(s)\in \mathbb{R}^{\square}\), for any function \(f:\mathcal{S}\rightarrow\mathbb{R}^{\square}\).

**Policy optimization.** For an objective function \(f:\Pi_{\Theta}\rightarrow\mathbb{R}\), the general goal of this work is to find \(\operatorname*{argmax}_{\pi_{\Theta}\in\Pi_{\Theta}}f(\pi_{\theta})\) over a policy class \(\Pi_{\Theta}=\{\pi_{\theta}:\theta\in\Theta,\theta\in\mathbb{R}^{\mathsf{p}},\|\theta\|\leq B\}\), parameterized by a convex and closed parameter class \(\Theta\) with dimension \(\mathsf{p}\). One example of \(f\) is the expected return \(J(\pi_{\theta})\). Projected gradient ascent (PGA) will be our base algorithm for policy optimization. For a fixed learning rate \(\eta\) and iterations \(t\in[T]\), it iteratively updates \(\theta^{(t+1)}=\operatorname*{Proj}_{\Theta}\left(\theta^{(t)}+\eta\nabla f( \pi_{\theta^{(t)}})\right)\). Here, \(\nabla f(\pi_{\theta})=[\frac{\partial f(\pi_{\theta})}{\partial\theta r}]_{p \in[\Theta]}\in\mathbb{R}^{\mathsf{p}}\) is the gradient with respect to \(\theta\), where superscript \(p\) indexes the \(p\)-th entry of a vector. We will assume that the gradient of the policy's log-probability is bounded, as is ubiquitous in the PG literature [1, 1].

**Assumption 2.1**.: For all \(\pi_{\theta}\in\Pi_{\Theta}\), \(\max_{s,a}\|\nabla\log\pi_{\theta}(a|s)\|_{\infty}\leq G\).

We will later analyze the convergence rate of our algorithms to stationary points with (approximately) zero gradient, and refer to \(\pi^{(t)}=\pi_{\theta^{(t)}}\) for short. For PGA, stationarity will be measured using the standard gradient mapping \(\|G^{\eta}(\pi^{(t)})\|\) with \(G^{\eta}(\pi^{(t)}):=\frac{1}{\eta}(\theta^{(t)}-\theta^{(t+1)})\), the parameter change between iterations [1]. Note that if \(\Theta=\mathbb{R}^{\mathfrak{p}}\) and no projection is required, then \(\|G^{\eta}(\pi^{(t)})\|=\|\nabla f(\pi^{(t)})\|\) reduces to the gradient magnitude.

Computational oracles.As is common in the literature, we analyze computational efficiency in terms of the number of calls to the following oracles, which serve as computational abstractions. We desire a polynomial number of such calls in terms of problem-relevant parameters. Given an i.i.d. dataset \(\mathcal{D}=\{(x,y)\}\) and function class \(\mathcal{F}\), the maximum likelihood estimation oracle outputs \(\operatorname*{argmax}_{f\in\mathcal{F}}\mathbb{E}_{\mathcal{D}}[\log f(x)]\). The squared-loss regression oracle finds \(\operatorname*{argmin}_{f\in\mathcal{F}}\mathbb{E}_{\mathcal{D}}[(f(x)-y)^{2}]\). Both can be approximated efficiently whenever optimizing over \(\mathcal{F}\) is feasible [14, 1].

## 3 Online Occupancy-based PG

We now develop our occupancy-based policy optimization algorithm for the online RL setting, where the policy can continuously interact with the environment to gather new trajectories. Our gradient estimation routine is based on a recursive Bellman flow-like equation that can be approximately solved using squared-loss regression, not unlike those used to estimate occupancy functions in FORC [1] or value functions in FQI [1]. The intuitions established for our online algorithm form the foundation for our later offline methods.

### Occupancy-based Policy Gradient

The expected return of a policy \(\pi\) can be expressed as the expectation over its occupancy of the per-state rewards, \(J(\pi)=\sum_{h}\sum_{s_{h}}d_{h}^{\pi}(s_{h})R(s_{h})\). The gradient of \(J(\pi)\) then passes through \(d^{\pi}\),

\[\nabla J(\pi)=\sum_{h}\sum_{s_{h}}\nabla d_{h}^{\pi}(s_{h})R(s_{h})=\sum_{h} \mathbb{E}_{s_{h}\sim d_{h}^{\pi}}\left[\nabla\log d_{h}^{\pi}(s_{h})R(s_{h}) \right].\]

We use the grad-log trick above to write \(\nabla J(\pi)\) as an expectation over \(d^{\pi}\), which makes it amenable to estimation from online samples as long as we can calculate \(\nabla\log d_{h}^{\pi}:\mathcal{S}\to\mathbb{R}^{\mathfrak{p}}\). We make the key observation that \(\nabla\log d_{h}^{\pi}\) can be expressed as a function of \(\nabla\log d_{h-1}^{\pi}\), which involves a time-reversed conditional expectation over the previous timestep's \((s_{h-1},a_{h-1})\) given the current \(s_{h}\).

**Lemma 3.1**.: _For any \(\pi\) and \(h\in[H]\), \(\nabla\log d_{h}^{\pi}\) satisfies the recursion_

\[\nabla\log d_{h}^{\pi}=\mathbf{E}_{h-1}^{\pi}\left(\nabla\log\pi+\nabla\log d _{h-1}^{\pi}\right),\] (1)

_where \([\mathbf{E}_{h-1}^{\pi}f](s^{\prime}):=\mathbb{E}_{\pi}[f(s_{h-1},a_{h-1})|s_ {h}=s^{\prime}]=\sum_{s,a}\frac{P(s^{\prime}|s,a)\pi(a|s)d_{h-1}^{\pi}(s)}{d_ {h}^{\pi}(s^{\prime})}f(s,a)\)1, for any function \(f:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{\mathfrak{p}}\). Further, under Asm. 2.1, \(\max_{s,h}\|\nabla\log d_{h}^{\pi}(s)\|_{\infty}\leq hG\)._

Footnote 1: We use the convention \(0/0=0\) for ratios between two functions.

Eq. (1) is derived by propagating the gradient through the Bellman flow equation, and we can solve it from \(h=1\) to \(H\) to compute \(\nabla\log d_{h}^{\pi}\) (with \(\nabla\log d_{0}^{\pi}=\mathbf{0}\) by definition). While related observations have been made throughout the rich history of PG literature [1, 1, 13, 14], the expression in Eq. (1) is adapted to our unique pursuit of modeling \(\nabla\log d^{\pi}\) with general function approximators. In particular, the conditional expectation (\(\mathbf{E}^{\pi}\)) immediately hints that \(\nabla\log d^{\pi}\) is amenable to estimation using squared-loss regression, a technique that is well-understood for value functions [1] and, more recently, for occupancy functions [1].

Formally, to solve the dynamic programming equation of Eq. (1) in a computationally efficient manner, we reduce it to minimizing a squared-loss regression problem. Consider the standard (supervised learning) regression setup. The solution \(\operatorname*{argmin}_{f}\mathbb{E}_{(x,y)\sim Q}[(f(x)-y)^{2}]\) maps \(x\mapsto\mathbb{E}_{Q}[y|x]\), the conditional expectation given \(x\) of the target \(y\) under the joint \(Q\). As a result (see Eq. B.2),

\[\nabla\log d_{h}^{\pi}=\operatorname*{argmin}_{g:\mathcal{S}\to\mathbb{R}^{ \mathfrak{p}}}\ \mathbb{E}_{\pi}\Big{[}\Big{\|}g(s_{h})-\left(\nabla\log\pi(a_{h-1}|s_{h-1})+ \nabla\log d_{h-1}^{\pi}(s_{h-1})\right)\Big{\|}^{2}\Big{]}.\] (2)Here, \(g\) is a vector-valued function, and the norm \(\|\cdot\|^{2}\) is equivalent to the sum of \(\mathsf{p}\) scalar-valued squared-losses for each parameter dimension. The RHS only requires sampling \((s_{h-1},a_{h-1},s_{h})\sim\pi\) from online rollouts. Then, given finite samples, we can robustly estimate \(\nabla\log d^{\pi}\) by minimizing an empirical version of Eq.2 using regression oracles.

### Online policy gradient algorithm and analyses

Alg.1 (OccuPG) displays our full online occupancy-based PG procedure. For each iteration \(t\in[T]\), we first collect two independent datasets: \(\{\mathcal{D}_{h}^{\mathrm{reg}}\}\) for \(\nabla\log d^{\pi^{(t)}}\) estimation, and \(\{\mathcal{D}_{h}^{\mathrm{grad}}\}\) for \(\nabla J(\pi^{(t)})\) estimation. The former occurs in Line5, where we recursively solve an empirical version of Eq.2; the latter is computed in Line7, then used to update the policy (Line8).

```
0: Samples \(n\); iterations \(T\); policy class \(\Pi_{\Theta}\); gradient function class \(\{\mathcal{G}_{h}\}\); learning rate \(\eta\)
1:for\(t=0,\dots,T-1\)do
2: Collect \(n\) trajectories with \(\pi^{(t)}\). Set \(\mathcal{D}_{h}^{\mathrm{reg}}=\{(s_{h},a_{h},s_{h+1})\}_{i=1}^{n}\) for all \(h\). Repeat for \(\{\mathcal{D}_{h}^{\mathrm{grad}}\}\).
3: Initialize \(g_{0}=\mathbf{0}\).
4:for\(h=1,\dots,H\)do
5: Let \(\mathcal{L}_{h-1}^{(t)}(g_{h};g_{h-1}):=\frac{1}{n}\sum_{(s,a,s^{\prime})\in \mathcal{D}_{h-1}^{\mathrm{reg}}}\big{\|}g_{h}(s^{\prime})-\big{(}\nabla\log \pi^{(t)}(a|s)+g_{h-1}(s)\big{)}\big{\|}^{2}\). Set \[\widehat{g}_{h}^{(t)}=\operatorname*{argmin}_{g_{h}\in\mathcal{G}_{h}}\mathcal{ L}_{h-1}^{(t)}(g_{h};\widehat{g}_{h-1}^{(t)}).\] (3)
6:endfor
7: Estimate \(\widehat{\nabla}J(\pi^{(t)})=\frac{1}{n}\sum_{h=1}^{H}\sum_{(s,a,s^{\prime},r ^{\prime})\in\mathcal{D}_{h-1}^{\mathrm{grad}}}\widehat{g}_{h}^{(t)}(s^{ \prime})\cdot r^{\prime}\)
8: Update \(\theta^{(t+1)}=\operatorname*{Proj}_{\Theta}\left(\theta^{(t)}+\eta\widehat{ \nabla}J(\pi^{(t)})\right)\).
9:endfor ```

**Algorithm 1**OccuPG: Online Occupancy-based Policy Gradient

**Gradient estimation guarantee.** In the following, we establish that our regression-based estimation procedure produces accurate estimates of \(\nabla J(\pi)\). Our guarantee holds under the requirement that the gradient function classes \(\{\mathcal{G}_{h}\}\) can express the population gradient update (Lem.3.1) for any target function. It is analogous to the Bellman completeness assumption that is required for regression-based value or occupancy function estimation [2, 1, 1].

**Assumption 3.1** (Gradient function class completeness).: For all \(h\in[H]\), \(\sup_{g\in\mathcal{G}_{h},s\in\mathcal{S}}\|g_{h}(s)\|_{\infty}\leq hG\). Further, for all \(\pi\in\Pi_{\Theta}\), we have \(\mathbf{E}_{h-1}^{\pi}(\nabla\log\pi+g_{h-1})\in\mathcal{G}_{h}\), for all \(g_{h-1}\in\mathcal{G}_{h-1}\).

Next, since we allow \(\mathcal{G}\) to be a continuous function class, our sample complexity bound for gradient estimation is expressed in terms of its pseudodimension \(:=\mathsf{d}_{\mathcal{G}}\) (Def.1). Examples of \(\mathcal{G}\) parameterizations and their \(\mathsf{d}_{\mathcal{G}}\) are discussed in Rem.3.1 below. Finally, Thm.3.1 shows that OccuPG produces accurate gradient estimates given the following polynomial sample size.

**Theorem 3.1**.: _Fix \(\delta\in(0,1)\) and \(\pi\in\Pi_{\Theta}\). Under Asm.2.1 and Asm.3.1, we have that w.p. \(\geq 1-\delta\), \(\|\nabla J(\pi)-\widehat{\nabla}J(\pi)\|\leq\varepsilon\) when \(n=\widetilde{O}\left(\frac{\mathsf{pd}_{\mathcal{G}}H^{\mathsf{G}}G^{2}\log(1/ \delta)}{\varepsilon^{2}}\right)\)._

_Remark 3.1_.: Lastly, we provide examples of \(\mathcal{G}\) for Asm.3.1 in representative MDP structures. Low-rank MDPs (Def.1) are a well-studied setting where the transition function admits a low-rank decomposition into two features of rank \(k\), i.e., there exists \(\phi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{k}\) and \(\mu:\mathcal{S}\to\mathbb{R}^{k}\) such that \(P(s^{\prime}|s,a)=\langle\phi(s,a),\mu(s^{\prime})\rangle\)[13]. Tabular MDPs are a special case with one-hot features. Due to the bilinear transitions, both the occupancy and its gradient are linear functions of \(\mu\), i.e., \(d^{\pi}=\mu(s)^{\top}\psi\) and \(\nabla d^{\pi}(s)=\mu(s)^{\top}\Psi\) for some \(\Psi\in\mathbb{R}^{k\times p},\psi\in\mathbb{R}^{k}\), and all \(s\in\mathcal{S}\). When \(\mu\) is known, we can set \(\mathcal{G}_{h}\) to be a linear-over-linear function class \(\mathcal{G}_{h}=\left\{g_{h}(s)=\frac{\mu(s)^{\top}\Psi}{\mu(s)^{\top}\psi}: \Psi\in\mathbb{R}^{k\times p},\psi\in\mathbb{R}^{k},\max_{s}\|g_{h}(s)\|_{ \infty}\leq hG\right\},\) which has \(\mathsf{d}_{\mathcal{G}}=k\mathsf{p}\) (Prop. B.1).

**Stationary convergence.** Next, we analyze the convergence rate of OccuPG to a stationary policy, i.e., one that has near-zero gradient. Note that, in general, stationary policies are not necessarily optimal as the objective function is non-convex. As is standard in the literature, we will assume that the objective has a smooth gradient [1, 2].

**Assumption 3.2** (\(\beta\)-smooth objective).: For a function \(f:\Pi_{\Theta}\to\mathbb{R}\), there exists \(\beta>0\) such that \(\|\nabla f(\pi_{\theta})-\nabla f(\pi_{\theta^{\prime}})\|_{2}\leq\beta\|\theta- \theta^{\prime}\|_{2}\) for all \(\theta,\theta^{\prime}\in\Theta\).

Cor. 3.1 shows that, in expectation, OccuPG with \(T=O(\beta H/\varepsilon)\) iterations outputs a \(\varepsilon\)-stationary point, as measured by \(\|G^{\eta}(\pi^{(i)})\|=\frac{1}{\eta}\|\theta^{(t)}-\theta^{(t-1)}\|\). The proof relies on Thm. 3.1, i.e., with enough samples the statistical noise of the gradient estimates are sufficiently small to enable convergence.

**Corollary 3.1**.: _Under creftype 2.1, creftype 3.1, and creftype 3.2, the iterates of OccuPG with \(T=O(\beta H\varepsilon^{-1})\) and \(n=\widetilde{O}(\mathsf{pd}_{\mathcal{G}}H^{6}G^{2}\log(T/\delta)\varepsilon^ {-1})\) satisfy \(\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}[\|G^{\eta}(\pi^{(t)})\|^{2}]\leq\varepsilon\)._

Computational efficiency.OccuPG is not only statistically efficient but computationally oracle-efficient as well, since it reduces to a series of squared-loss minimization problems. In each iteration, it makes \(H\) calls to a regression oracle to compute the occupancy gradient (Line 5). Then to converge to a \(\varepsilon\)-stationary point, from Cor. 3.1 we require a total of \(O(\beta H^{2}/\varepsilon)\) such calls.

Optimality.Lastly, we analyze when the policies recovered by OccuPG are also approximately optimal. The key inequality is an upper bound on the suboptimality of any policy in terms of its gradient magnitude (or stationarity), and a _coverage coefficient_\(\mathcal{C}^{\pi^{*}}\) with respect to the optimal policy.

**Lemma 3.2**.: _For any \(\pi\) and \(\pi^{\prime}\), define \(B^{\pi}(\pi^{\prime}):=\sum_{h,s,a}d_{h}^{\pi}(s)\pi^{\prime}(a|s)Q_{h}^{\pi}(s,a)\). Suppose \(\forall\pi\in\Pi_{\Theta}\),_

1. _(Policy completeness) There exists_ \(\pi^{+}\in\Pi_{\Theta}\) _such that_ \(\pi^{+}\in\operatorname*{argmax}_{\pi^{\prime}}B^{\pi}(\pi^{\prime})\)_._
2. _(Gradient domination)_ \(\max_{\pi^{\prime}\in\Pi_{\Theta}}B^{\pi}(\pi^{\prime})-B^{\pi}(\pi)\leq m \max_{\theta^{\prime}\in\Theta}\left\langle\nabla B^{\pi}(\pi),\theta^{\prime }-\theta\right\rangle\)__

_Given \(\nu\in\Delta(\mathcal{S})\), define the coverage coefficient \(\mathcal{C}^{\pi^{*}}:=\big{\|}\!\sum_{h}d_{h}^{\pi^{*}}/\nu\big{\|}_{\infty}\) for \(\pi^{*}=\operatorname*{argmax}_{\pi}J(\pi)\). Then for any \(\pi_{\theta}\in\Pi_{\Theta}\),_

\[J(\pi^{*})-J(\pi_{\theta})\leq m\;\mathcal{C}^{\pi^{*}}\max_{\theta^{\prime} \in\Pi_{\Theta}}\left\langle\nabla J_{\nu}(\pi_{\theta}),\theta^{\prime}- \theta\right\rangle,\] (4)

_where \(J_{\nu}(\pi):=\mathbb{E}_{s_{0}\sim\nu,\pi}[\sum_{h}r_{h}]\) is the expected return of \(\pi\) in \(\mathcal{M}\) with initial state distribution \(\nu\)._

The lemma preconditions are identical to those required for value-based analysis [1]. \(B^{\pi}(\pi^{\prime})\) is a one-step improvement objective with respect to the occupancies and value functions of \(\pi\), and we require (1) the policy class to be expressive enough that it contains any maximizer; and (2) the one-step objective to itself have optimality gap upper-bounded by the one-step policy gradient magnitude, for which the constant \(m\) is determined wholly by the policy parameterization. For example, the tabular policy \(\pi_{\theta}(a|s)=\theta_{sa}\) has \(m=1\)[1].

The coverage coefficient \(\mathcal{C}^{\pi^{*}}\) is the finite-horizon counterpart to the infinite-horizon "exploratory initial distribution" salient to the analysis of [1] and [1] (which lists developing it as future work). In RL, a small gradient magnitude alone does not guarantee optimality, as it can also occur when the policy rarely visits rewarding states. The coverage coefficient quantifies both how policy performance can suffer from insufficient exploration, as well as how exploratory initializations mitigates this problem. Finally, combining Lem. 3.2 with the stationary convergence result in Cor. 3.1 shows that, on average, the best-iterate of OccuPG is near-optimal.

**Corollary 3.2**.: _Under the preconditions of Lem. 3.2 and Cor. 3.1, running OccuPG2 with initial distribution \(\nu\) satisfies \(\mathbb{E}[\min_{t}J(\pi^{*})-J(\pi^{(t)})]\leq\varepsilon\) when \(T=\widetilde{O}\left(\frac{\beta B^{2}(\mathcal{C}^{\pi^{*}})^{2}m^{2}H^{2}}{ \varepsilon^{2}}\right)\) and \(n=\widetilde{O}\left(\frac{B^{2}(\mathcal{C}^{\pi^{*}})^{2}m^{2}\mathsf{pd}_{ \mathcal{G}}H^{6}G^{2}\log(T)}{\varepsilon^{2}}\right)\)._

Footnote 2: This means that trajectories are generated by first sampling the initial state \(s_{1}\sim\nu\), then rolling out the policy according to the true MDPâ€™s dynamics.

### Optimization of general functionals

One standout feature of OccuPG is that it can, _with a one-line change_, be adapted for policy optimization of any (differentiable) objective function involving occupancies. We work with \(J_{F}(\pi)=\sum_{h}F_{h}(d_{h}^{\pi})\) as a representative formula, where \(F_{h}:\Delta(\mathcal{S})\to\mathbb{R}\) is a general functional. Such objectives often evade value-based PG optimization because they do not admit value functions or Bellman-like recursions with which to compute them. Examples include entropy maximizationwhere \(F_{h}(d)=-\left\langle d,\log d\right\rangle\); imitation learning where \(F_{h}(d)=-\|d-d_{h}^{\pi_{E}}\|_{2}^{2}\) for an expert policy \(\pi_{E}\); and the expected return with \(F_{h}(d)=\left\langle d,R\right\rangle\)[10].

The policy gradient is then \(\nabla J_{F}(\pi)=\sum_{h}\mathbb{E}_{s\sim d_{h}^{\pi}}\left[\frac{\partial F _{h}(d)}{\partial d(s)}|_{d=d_{h}^{\pi}}\nabla\log d_{h}^{\pi}(s)\right]\). Implementation-wise, we need only change Line 7 in OccupPG to accommodate the new gradient formula, to \(\widehat{\nabla}J_{F}(\pi)=\frac{1}{n}\sum_{h}\sum_{s\in\mathcal{D}_{h}} \widehat{g}_{h}^{\pi}(s)\left.\frac{\partial F_{h}(d)}{\partial d(s)}\right|_{d =\widehat{d}_{h}^{\pi}}\). The partial derivative of \(F_{h}\) is evaluated with a plug-in occupancy estimate \(\widehat{d}^{\pi}\) that can be obtained using maximum likelihood estimation (App. D). Notably, the occupancy gradient estimation module for \(\widehat{g}_{h}^{\pi}\approx\nabla\log d_{h}^{\pi}\) (Line 5) is reused verbatim. Given their resemblance to those in Sec. 3.2, the full algorithm and analyses are deferred to App. B.5.

## 4 Offline Occupancy-based PG

In this section, we develop an algorithm for occupancy-based policy optimization in the offline setting, where only fixed datasets are available for learning. A direct modification of OccupPG, e.g., by converting occupancies to density ratios over the offline data distribution, will fail unless the data covers _all possible policies_, otherwise the density ratio may be unbounded. In-line with recent state-of-the-art offline RL algorithms, our goal is to establish an offline PG algorithm that adapts to and retains meaningful guarantees under arbitrary offline datasets, for which our key consideration is establishing an offline gradient estimation method. We begin by defining these offline datasets.

**Definition 4.1**.: The offline dataset is \(\mathcal{D}=\{\mathcal{D}_{h}\}\), where \(\mathcal{D}_{h}=\{(s_{h},a_{h},s_{h+1},r_{h+1}))\}_{i=1}^{n}\) is generated i.i.d. as \(s_{h}\sim d_{h}^{D}\) for some \(d_{h}^{D}\in\Delta(\mathcal{S})\) and \(a_{h}\sim\pi_{h}^{D}(\cdot|s_{h})\) in \(\mathcal{M}\), for a known behavior policy \(\pi_{h}^{D}\). The marginal next-state distribution in \(\mathcal{D}_{h}\) is denoted as \(d_{h}^{D,\dagger}(s_{h+1})\).

Def. 4.1 is more general than the typical i.i.d. trajectory setting [10, 11], where \(d_{h}^{D}=d_{h-1}^{D,\dagger}\). Crucially, unlike previous works that require lower-bounded \(d^{D}\) or all-policy coverage [10, 11, 11], we will make no assumptions about the quality of \(\mathcal{D}\) with respect to \(\Pi_{\Theta}\).

**Additional notation.** For short, we say \(\mathbb{E}_{\mathcal{D}_{h}}[\cdot]\equiv\mathbb{E}_{(s_{h},a_{h},s_{h+1},r_{ h+1})\sim\mathcal{D}_{h}}[\cdot]\), and use \((s,a,s^{\prime},r^{\prime})\sim\mathcal{D}_{h}\) when clear from the context. For any \(g:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}^{p}\) and reweighting function \(\rho:H\times\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}_{+}\), we define an _offline reweighted_ analog to \(\mathbb{E}_{h}^{\pi}\) (Lem. 3.1) for all \(h\in[H]\) to be

\[[\mathbb{E}_{h}^{D,\rho}g](s^{\prime}):=\mathbb{E}_{(s,a,s^{\prime})\sim \mathcal{D}_{h}\cdot\rho_{h}}[g(s,a)|s^{\prime}]=\sum_{s,a}\frac{[\mathcal{D}_ {h}\cdot\rho_{h}](s,a,s^{\prime})}{\sum_{s,a}[\mathcal{D}_{h}\cdot\rho_{h}](s,a,s^{\prime})}\ g(s,a).\] (5)

The (time-reversed) conditional expectation is taken over \([\mathcal{D}_{h}\cdot\rho_{h}](s,a,s^{\prime}):=P(s^{\prime}|s,a)d_{h}^{D}(s) \pi_{h}^{D}(a|s)\rho_{h}(s,a)\), the joint offline distribution re-weighted by \(\rho_{h}\). While this may not be a valid density, its induced conditional distribution on \((s,a|s^{\prime})\) always is, i.e., \(\sum_{s,a}\frac{[\mathcal{D}_{h}\cdot\rho_{h}](s,a,s^{\prime})}{\sum_{s,a}[ \mathcal{D}_{h}\cdot\rho_{h}](s,a,s^{\prime})}=1\). As an example, for a given \(\pi\) we have \(\mathbb{E}_{h}^{D,\rho}=\mathbb{E}_{h}^{\pi}\) when \(\rho_{h}(s,a)=\frac{d_{h}^{\pi}(s)\pi(a|s)}{d_{h}^{D}(s)\pi_{h}^{D}(a|s)}\) is the policy's density ratio and is well-defined.

### Offline density-based policy gradient

A policy's occupancy \(d^{\pi}\) may not be covered by arbitrary offline data (Def. 4.1), so neither its expected return \(J(\pi)=\sum_{h}\left\langle d_{h}^{\pi},R\right\rangle\) nor its gradient \(\nabla J(\pi)\) will be estimatable from \(\mathcal{D}\). As a result, there is no hope of recovering \(\operatorname*{argmax}_{\pi\in\Pi_{\Theta}}J(\pi)\). Our solution is to instead _maximize return only on areas of the state space that are sufficiently covered by offline data_, which is captured exactly by the recursively clipped occupancy \(d^{\pi}\) from [10]. It clamps the policy occupancy to preset multiples \(C_{h}^{\pi},C_{h}^{\pi}\) of the offline data distribution, thereby representing only the "sufficiently covered" portion.

**Definition 4.2** (Recursively clipped occupancy).: Let \((\square\wedge\square):=\min\{\square,\square\}\). Given clipping constants \(\{C_{h}^{\mathbf{s}},C_{h}^{\mathbf{a}}\}\geq 1\), define the clipped policy to be \(\overline{\pi}_{h}=\left(\pi\wedge C_{h}^{\mathbf{a}}\pi_{h}^{D}\right)\), and recursively define

\[\bar{d}_{h}^{\pi}=\mathbf{P}^{\bar{\pi}_{h-1}}\left(\bar{d}_{h-1}^{\pi}\wedge C _{h-1}^{\mathbf{s}}d_{h-1}^{D}\right),\ \forall h\in[H].\] (6)

Eq. (6) resembles the Bellman flow equation with clipped policy \(\bar{\pi}\), and acts on the previous-timestep \(\bar{d}_{h-1}^{\pi}\) clipped to at most \(C_{h-1}^{\mathbf{s}}d_{h-1}^{D}\). Above this threshold the occupancy is considered to be insufficiently covered for estimation, and \(C^{\mathbf{s}}\) strikes a bias-variance tradeoff between the amount of clipped mass vs. distribution shift. The clipped occupancy's density ratio is always well-defined and bounded as \(\bar{d}_{h}^{\pi}/d_{h-1}^{D,\dagger}\leq C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a }}\), and we use it to define our (now learnable) offline objective,

\[\bar{J}(\pi)=\sum_{h}\sum_{s_{h}}\bar{d}_{h}^{\pi}(s_{h})R(s_{h})=\sum_{h} \mathbb{E}_{\mathcal{D}_{h-1}}\left[\frac{\bar{d}_{h}^{\pi}(s_{h})}{d_{h-1}^{D, \dagger}(s_{h})}R(s_{h})\right].\]

For any "fully covered" policy with \(d_{h}^{\pi}\leq C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}}d_{h-1}^{D,\dagger}\) for all \(h\in[H]\), we have \(\bar{d}^{\pi}=d^{\pi}\) and \(\bar{J}(\pi)=J(\pi)\). In this sense, \(\operatorname*{argmax}_{\pi}\bar{J}(\pi)\) will be at least as good as the best policy fully covered by offline data. Next, define the density ratio be \(\bar{w}_{h}^{\pi}:=\bar{d}_{h}^{\pi}/d_{h-1}^{D,\dagger}\). The gradient of \(\bar{J}(\pi)\) is

\[\nabla\bar{J}(\pi)=\sum_{h}\mathbb{E}_{\mathcal{D}_{h-1}}\left[\bar{w}_{h}^{ \pi}(s_{h})R(s_{h})\;\nabla\log\bar{d}_{h}^{\pi}(s_{h})\right].\]

To calculate this gradient we must compute both \(\bar{w}^{\pi}\) and \(\nabla\log\bar{d}^{\pi}\); for the former, [1] provides a method that we will later call as a subroutine. Our focus is on computing \(\nabla\log\bar{d}_{h}^{\pi}\), which is enabled by the following recursive equation, which is an offline analog of Lem. 3.1.

**Lemma 4.1**.: _For any \(\pi\) and all \(h\in[H]\), define \(\bar{\rho}_{h}^{\pi}(s,a):=\frac{\left(\bar{d}_{h}^{\pi}(s)\wedge C_{h}^{ \mathbf{a}}d_{h}^{D}(s)\right)}{\bar{d}_{h}^{\pi}(s)}\frac{\bar{\pi}_{h}(a|s)} {\pi_{h}^{D}(a|s)}\). Then_

\[\nabla\log\bar{d}_{h}^{\pi}=\mathbf{E}_{h-1}^{D,\bar{\rho}^{\pi}}\left(\nabla \log\pi\odot\mathbf{1}|\pi\leq C_{h}^{\mathbf{a}}\pi_{h-1}^{D}\right]+\nabla \log\bar{d}_{h-1}^{\pi}\odot\mathbf{1}[\bar{d}_{h-1}^{\pi}\leq C_{h}^{ \mathbf{a}}d_{h-1}^{D}]\right),\] (7)

_where \(\mathbf{E}_{h-1}^{D,\bar{\rho}^{\pi}}\) is from Eq. (5), and \([M\odot v](\cdot):=v(\cdot)M(\cdot)\in\mathbb{R}^{\mathsf{p}}\) for \(M:\square\to\mathsf{p}\) and \(v:\square\to\mathbb{R}\)._

Lem. 4.1 is derived from applying the chain rule to Def. 4.2, and the clipped occupancies play an instrumental role in handling insufficient offline coverage. Notably, the indicator function zeroes-out both the gradients \(\nabla\log\pi\) and \(\nabla\log\bar{d}_{h-1}^{\pi}\) where they are insufficiently covered, e.g., \(\bar{d}_{h-1}^{\pi}(s)>C_{h-1}^{\mathbf{s}}d_{h-1}^{D}(s)\). Further, under full offline coverage we recover Lem. 3.1 and \(\nabla\log\bar{d}^{\pi}=\nabla\log d^{\pi}\).

Because the rewards are nonnegative, \(\nabla\log\bar{d}^{\pi}\) induces a _pessimistic policy gradient_ that shifts policies away from out-of-distribution actions, even if they generate high return. This is seen more clearly in Prop. 4.1, that rearranges the resulting expression for \(\nabla\bar{J}(\pi)\) into a value-based form:

**Proposition 4.1**.: _We can equivalently write_

\[\nabla\bar{J}(\pi)=\sum_{h}\mathbb{E}_{\mathcal{D}_{h}}[\bar{\rho}_{h}^{\pi}(s,a)\nabla\log\pi_{h}(a|s)\bar{Q}_{h}^{\pi}(s,a)],\]

_where \(\bar{Q}^{\pi}\) is a pessimistic value function that obeys the Bellman-like recursion \(\bar{Q}_{h}^{\pi}(s,a)=\mathbf{1}[\pi\leq C_{h}^{\mathbf{a}}\pi_{h}^{D}](a|s) \sum_{s^{\prime}}P(s^{\prime}|s,a)\Big{(}R(s^{\prime})+\mathbf{1}[\bar{d}_{h+1} ^{\pi}\leq C_{h+1}^{\mathbf{s}}d_{h+1}^{D}](s^{\prime})\;\bar{Q}_{h+1}^{\pi}(s ^{\prime},\bar{\pi}_{h+1})\Big{)}\)._

In \(\bar{Q}^{\pi}\), future returns are zeroed out at states and actions that exceed the threshold of data coverage, due to indicators functions that are inherited from \(\nabla\log\bar{d}^{\pi}\). Prop. 4.1 can be seen as a pessimistic offline analog to the classical PG theorem \(\nabla J(\pi)=\sum_{h}\mathbb{E}_{s,a\sim d_{h}^{\pi}}[\nabla\log\pi(a|s)Q_{h}^ {\pi}(s,a)]\)[20], entirely induced by the definition of the clipped occupancy.

**Non-robustness of \(\nabla\log\bar{d}^{\pi}\) estimation to plug-in densities.** With finite samples, however, it turns out that consistent estimates of \(\nabla\log\bar{d}_{h}^{\pi}\) in Eq. (6) cannot be computed. To make this argument, we first outline the high-level gradient estimation procedure for a fixed policy:

* Estimate occupancies \(\{\bar{d}_{h}^{\pi}\}\) and \(\{\bar{d}_{h}^{D}\}\)
* Compute \(\widehat{\nabla}\log\bar{d}_{h}^{\pi}\) using Eq. (7) with plug-in indicator function estimate \(\mathbf{1}[\widehat{d}_{h-1}^{\pi}\leq C_{h}^{\mathbf{s}}\widehat{d}_{h-1}^{ \mathcal{D}}]\)

The problem arises in step two, as \(\mathbf{1}[\cdot]\) is a stepwise function and not smooth. Even if \(\widehat{d}^{\pi}\) is vanishingly close to \(d^{\pi}\), the gradient calculated from plug-in occupancy estimates can have constant error.

**Proposition 4.2**.: _There exists an MDP and policy \(\pi\) such that, for any \(\varepsilon>0\), \(\max_{h,s}\|\nabla\log d_{h}^{\pi}(s)-\widehat{\nabla}\log\bar{d}_{h}^{\pi}(s)\| =O(1)\) when \(\|\bar{d}_{h}^{\pi}-\widehat{d}_{h}^{\pi}\|_{1}\leq\varepsilon\) and \(\|\widehat{d}_{h}^{\mathrm{D}}-d_{h}^{D}\|_{1}\leq\varepsilon\) for all \(h\)._

### Smooth clipping

To resolve this issue, we will use a "smooth-clipping" function \(\sigma\left(x,c\right)\) to approximate the "hard"-clipping \(\left(x\wedge c\right)\) in Eq. (6), whose non-smooth gradient was the source of our estimation problems. Figure 1 plots 1-D examples of \(\sigma\left(x,c\right)\) against \(\left(x\wedge c\right)\) as reference (dashed), and Ass. 4.1 describes the properties of \(\sigma\) that enable our later estimation and convergence guarantees.

**Assumption 4.1**.: Assume that \(\sigma\) satisfies \(\forall x,x^{\prime},c,c^{\prime}\in\mathrm{dom}(\sigma)\),

1. (Approximate clipping) \(\exists D_{\sigma}\geq 0\) such that \(0\leq\left(x\wedge c\right)-\sigma\left(x,c\right)\leq D_{\sigma}\left(x\wedge c\right)\).
2. (Monotonicity) \(\sigma\left(x^{\prime},c\right)\leq\sigma\left(x,c\right)\) if \(x^{\prime}\leq x;\sigma\left(x,c^{\prime}\right)\leq\sigma\left(x,c\right)\) if \(c^{\prime}\leq c\); and vice versa.
3. (Smooth gradient) Define the smoothed indicator \(\mathbf{\tilde{1}}\left(x,c\right):=x\;\partial_{x}\log\sigma\left(x,c\right)\), where \(\;\partial_{x}\) is the partial derivative w.r.t. \(x\). Then \(\mathbf{\tilde{1}}\left(x,c\right)\in[0,1]\) and \(\exists L_{\sigma}\geq 0\) s.t. \(\forall x,x^{\prime},c,c^{\prime}\in\mathrm{dom}(\sigma)\), \[c|\mathbf{\tilde{1}}\left(x,c\right)-\mathbf{\tilde{1}}\left(x^{\prime},c \right)|\leq L_{\sigma}|x-x^{\prime}|,\;\text{and}\;\;x|\mathbf{\tilde{1}} \left(x,c\right)-\mathbf{\tilde{1}}\left(x,c^{\prime}\right)|\leq L_{\sigma}| c-c^{\prime}|.\]

Note that \(\sigma\left(x,c\right)=\left(x\wedge c\right)\) is a special case with \(\mathbf{\tilde{1}}\left(x,c\right)=\mathbf{1}[x\leq c]\), thus \(D_{\sigma}=0\) and \(L_{\sigma}=\infty\). The following choice of \(\sigma\), which is plotted in Fig. 1, fulfills Ass. 4.1.

**Proposition 4.3**.: _For any \(b>1\), \(\sigma\left(x,c\right)=\left(x^{-b}+c^{-b}\right)^{-1/b}\) has \(L_{\sigma}=b\) and \(D_{\sigma}=1/b\)._

Next, we define the smooth-clipped occupancy function \(\widetilde{d}_{h}^{\pi}\), which is no larger than \(\widetilde{d}_{h}^{\pi}\).

**Definition 4.3** (Recursively smooth-clipped occupancy).: For smooth-clipping function \(\sigma\) satisfying Ass. 4.1 and clipping constants \(\{C_{h}^{\mathbf{a}},C_{h}^{\mathbf{a}}\}\), define \(\widetilde{\pi}_{h}:=\sigma\left(\pi,C_{h}^{\mathbf{a}}\pi_{h}^{D}\right)\), and inductively set

\[\widetilde{d}_{h}^{\pi}=\mathbf{P}^{\widetilde{\pi}_{h-1}}\left(\sigma\left( \widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{a}}d_{h-1}^{D}\right)\right),\; \forall h\in[H].\] (8)

Then letting \(\widetilde{w}_{h}^{\pi}:=\widetilde{d}_{h}^{\pi}/d_{h-1}^{D,\dagger}\), our new objective is \(\widetilde{J}(\pi)=\sum_{h}\mathbb{E}_{\mathcal{D}_{h-1}}[\widetilde{w}_{h}^ {\pi}(s_{h})R(s_{h})]\) with gradient \(\nabla\widetilde{J}(\pi)=\sum_{h}\mathbb{E}_{\mathcal{D}_{h-1}}[\widetilde{w }_{h}^{\pi}(s_{h})R(s_{h})\;\nabla\log\widetilde{d}_{h}^{\pi}(s_{h})]\), where \(\nabla\log\widetilde{d}_{h}^{\pi}\) obeys the following recursion.

**Lemma 4.2**.: _For \(\sigma\) satisfying Ass. 4.1, recall \(\mathbf{\tilde{1}}\left(x,c\right):=x\;\partial_{x}\log\sigma\left(x,c\right).\) Then for all \(h\in[H]\),_

\[\nabla\log\widetilde{d}_{h}^{\pi}=\mathbf{E}_{h-1}^{D,\widetilde{\rho}^{\pi}} \left(\nabla\log\pi\odot\;\mathbf{\tilde{1}}\left(\pi,C_{h-1}^{\mathbf{a}} \pi_{h-1}^{D}\right)+\nabla\log\widetilde{d}_{h-1}^{\pi}\odot\;\mathbf{\tilde{1 }}\left(\widetilde{d}_{h-1}^{\widetilde{\mu}},C_{h-1}^{\mathbf{a}}d_{h-1}^{D} \right)\right),\] (9)

_where \(\widetilde{\rho}_{h-1}^{\pi}(s,a):=\frac{\sigma\left(\widetilde{d}_{h-1}^{ \pi}(s),C_{h-1}^{\mathbf{a}}d_{h-1}^{D}\right)}{d_{h-1}^{D}(s)}\frac{ \widetilde{\pi}_{h-1}(a|s)}{\pi_{h-1}^{D}(a|s)}\) and \(\mathbf{E}_{h-1}^{D,\widetilde{\rho}^{\pi}}\) is defined in Eq. (5). Further, under Ass. 2.1, \(\max_{s,h}\|\nabla\log\widetilde{d}_{h}^{\pi}(s)\|_{\infty}\leq hG\)._

Eq. (9) replaces the (non-smooth) indicator function in \(\nabla\log\bar{d}^{\pi}\) (Lem. 4.1) with its smooth approximation \(\mathbf{\tilde{1}}\), which, as we will show shortly, enables robust gradient estimation with plug-in occupancy estimates. As before, we can reduce it to squared-loss regression (Eq. (11)). Further, by optimizing \(\widetilde{J}(\pi)\), we also approximately maximize our target objective \(\widetilde{J}(\pi)\), with bias proportional to \(D_{\sigma}\).

**Proposition 4.4**.: _Under Ass. 4.1, \(0\leq\max_{\pi\in\Pi_{\Theta}}\widetilde{J}(\pi)-\max_{\pi\in\Pi_{\Theta}} \widetilde{J}(\pi)\lesssim H^{2}D_{\sigma}.\)_

### Offline smooth-clipped gradient estimation

Alg. 2 describes the offline PG algorithm for optimizing \(\widetilde{J}(\pi)\). To reduce clutter, we have used \(\nabla\log\widetilde{\pi}_{h}:=\nabla\log\pi\odot\;\mathbf{\tilde{1}}\left( \pi,C_{h}^{\mathbf{a}}\pi_{h}^{D}\right)\). First, Off-OccuPG estimates \(d_{h-1}^{D}\) using MLE (details in App. D due to space constraints). Then, for each iteration \(t\), it estimates the smooth-clipped occupancy \(\widetilde{d}_{h}^{\pi^{(t)}}\) using Forc (adapted from [1], see App. E). This is plugged into a squared-loss regression problem approximating Eq. (9) to learn \(\nabla\log\widetilde{d}_{h}^{(t)}\) (lines 8 to 10), then estimate \(\nabla\widetilde{J}(\pi^{(t)})\) (line 12).

Figure 1: We plot \(\sigma(x,c)\) from Prop. 4.3 for different \(b\), that trade-off between clipping approximation error and smoothness (\(D_{\sigma}\propto 1/L_{\sigma}\)).

Before stating the estimation guarantee for \(\nabla\widetilde{J}(\pi)\), we first introduce the required assumptions. For simplicity, we assume that the function classes used in MLE and Forc are finite, and defer their guarantees to the respective appendices, as they have been well-established in previous papers [1, 1]. We focus on discussing Ass. 4.2 for the offline gradient function class, which requires a stronger level of expressiveness. Since the regression target in Off-OccuPG involves plug-in occupancy estimates, the completeness condition naturally requires \(\mathcal{G}_{h}\) to express the gradient update in Lem. 4.2 for all possible targets composed of functions from \(\mathcal{F}_{h-1},\mathcal{W}_{h-1},\mathcal{G}_{h-1}\). As a result, Ass. 4.2 is generally stronger than Ass. 3.1 for OccuPG.

**Assumption 4.2**.: For all \(h\), \(\sup_{g\in\mathcal{G}_{h}}\|g_{h}\|_{\infty}\leq hG\); and for all \((\pi,g,f,f^{\prime},w)\in\Pi_{\Theta}\times\mathcal{G}_{h-1}\times\mathcal{F}_ {h}\times\mathcal{F}_{h-1}\times\mathcal{W}_{h-1}\), we have \(\mathbf{E}_{h-1}^{D,\rho}(\nabla\log\widetilde{\pi}_{h-1}+g\odot\mathbf{ \tilde{1}}\left(wf^{\prime},C_{h-1}^{\mathbf{s}}f\right))\in\mathcal{G}_{h}\), where \(\rho=\frac{\sigma\left(wf^{\prime},C_{h}^{\mathbf{s}}f\right)}{f}\frac{ \widetilde{\pi}_{h-1}}{\pi_{h-1}^{D}}\).

When the underlying MDP has favorable structure, however, we can expect that \(\mathsf{d}_{\mathcal{G}}\) is not much larger than was required for OccuPG. This is indeed the case in low-rank MDPs, where the \(\mathcal{G}\) defined in Rem. 3.1 also satisfies Ass. 4.2 (proof in Prop. C.1). Due to the bilinear transition structure, the offline gradient update (Lem. 4.2) applied to any target remains a linear-over-linear function.

The guarantees for the MLE (Alg. 4) and weight estimation (Alg. 5) subroutines require Ass. D.1 and Ass. E.1, respectively, which are included in the preconditions of the main result below. Briefly, Ass. D.1 requires \(\mathcal{F}\) to realize the true data distributions \(d_{h}^{D}\) and \(d_{h}^{D,\dagger}\), which is standard in supervised learning. Ass. E.1 requires \(\mathcal{W}\) to be closed under the Bellman flow operator, and can be viewed as a 1-dimensional version of Ass. 4.2 where \(\rho=1\). In this sense both assumptions are weaker requirements on expressivity than that of the gradient class in Ass. 4.2, and more detailed discussions are left to App. D and App. E.

Having established its preconditions, we now present our main estimation guarantee for Off-OccuPG, which pays additional factors for the coverage of offline data (\(\sum_{h}C_{h}^{\mathbf{s}}C_{h}^{\mathbf{n}}\)) and the smoothness of \(\sigma\).

**Theorem 4.1**.: _Suppose \(\widetilde{J}(\cdot)\) satisfies Asm. 3.2 and fix \(\pi\in\Pi_{\Theta}\). Under Asm. 2.1, Asm. 4.1, Asm. 4.2, Asm. 4.2, Asm. 4.1, and Asm. 4.1, w.p. \(\geq 1-\delta\) we have \(\|\nabla\widetilde{J}(\pi)-\widehat{\nabla}\widetilde{J}(\pi)\|\leq\varepsilon\) when \(n=\widetilde{O}\left(\frac{\mathsf{pd}_{\Theta}H^{\Theta}G^{2}\left(\sum_{h}C_ {h}^{\mathsf{s}}C_{h}^{\mathsf{s}}\right)^{2}L_{2}^{2}\log\left(|\mathcal{W} \|\mathcal{F}|/\delta\right)}{\varepsilon^{2}}\right)\)._

Stationary convergence & computational efficiency.Similar to OccuPG, Off-OccuPG with \(T=O(\beta H^{2}/\varepsilon^{2})\) converges to an \(\varepsilon\)-stationary point. The formal statement is given in Cor. C.1 and is based on the estimation guarantee in Thm. 4.1. As a result, Off-OccuPG is also computationally oracle-efficient. Each invocation of MLE involves \(2H\) calls to a likelihood maximization oracle (see Alg. 4), and each invocation of Forc requires \(H\) calls to a squared-loss regression oracle (see Alg. 5). Then local convergence is still achieved with \(O(\beta H^{2}/\varepsilon^{2})\) such calls, as increasing \(T\) further cannot reduce error from statistical noise (that depends only the fixed \(n\)).

Optimality.Analyzing the conditions under which offline PG recovers global optima is more challenging, as we can no longer utilize exploratory initialization (from Cor. 3.2). However, since all occupancies have been clipped to the data distribution, we show in App. C.5 that the offline data itself can sometimes suffice as an exploratory initial distribution, and the corresponding bound is in terms of \(\{C_{h}^{\mathsf{s}}\}\) (instead of the online \(\mathcal{C}^{\pi^{*}}\)). However, this is not guaranteed in general and our current result only holds under strong all-policy offline data coverage. Briefly, some hardness comes from the fact that clipping causes gradient signals to vanish, so a stationary policy might be far off-support, rather than optimal. Investigating the possibility of more relaxed conditions for offline PG convergence (or, conversely, refining hardness results) are especially interesting directions for future work.

## 5 Conclusion

For the first time, we demonstrate how policy optimization can be conducted with (only) occupancy functions for both online and offline RL, and comprehensively analyze both local and global convergence. In the online setting our method directly extends to optimizing general objective functionals that cannot be optimized using value-based methods, and in the offline setting the occupancy-based gradient naturally handles incomplete offline data coverage. As our work is the first in this line of research and theoretical in nature, for future work we plan to launch empirical investigations of our methods, especially those for optimizing general functionals. Additionally, the conditions under which offline PG can converge to global optima is not well-understood, and we hope that our preliminary results here encourage greater interest and investigation into this question.

## Acknowledgements

Nan Jiang acknowledges funding support from NSF IIS-2112471, NSF CAREER IIS-2141781, Google Scholar Award, and Sloan Fellowship.

## References

* [AFK24] Philip Amortila, Dylan J Foster, and Akshay Krishnamurthy. "Scalable Online Exploration via Coverability". In: _arXiv preprint arXiv:2403.06571_ (2024).
* [AKKS20] Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun. "Flambe: Structural complexity and representation learning of low rank mdps". In: _Advances in Neural Information Processing Systems_ (2020).
* [AKLM21] Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. "On the theory of policy gradient methods: Optimality, approximation, and distribution shift". In: _The Journal of Machine Learning Research_ 22.1 (2021), pp. 4431-4506.
* [ASM07] Andras Antos, Csaba Szepesvari, and Remi Munos. "Fitted Q-iteration in continuous action-space MDPs". In: _Advances in neural information processing systems_ 20 (2007).
* [Bec17] Amir Beck. _First-order methods in optimization_. SIAM, 2017.

* [BFH23] Anas Barakat, Ilyas Fatkhullin, and Niao He. "Reinforcement learning with general utilities: Simpler variance reduction and large state-action space". In: _International Conference on Machine Learning_. PMLR. 2023, pp. 1753-1800.
* [BR24] Jalaj Bhandari and Daniel Russo. "Global optimality guarantees for policy gradient methods". In: _Operations Research_ (2024).
* [CC97] Xi-Ren Cao and Han-Fu Chen. "Perturbation realization, potentials, and sensitivity analysis of Markov processes". In: _IEEE Transactions on Automatic Control_ 42.10 (1997), pp. 1382-1393.
* [CJ19] Jinglin Chen and Nan Jiang. "Information-Theoretic Considerations in Batch Reinforcement Learning". In: _International Conference on Machine Learning_. 2019.
* [CJ22] Jinglin Chen and Nan Jiang. "Offline reinforcement learning under value and density-ratio realizability: the power of gaps". In: _Uncertainty in Artificial Intelligence_. PMLR. 2022, pp. 378-388.
* [DWS12] Thomas Degris, Martha White, and Richard S Sutton. "Off-policy actor-critic". In: _arXiv preprint arXiv:1205.4839_ (2012).
* [FR20] Dylan Foster and Alexander Rakhlin. "Beyond ucb: Optimal and efficient contextual bandits with regression oracles". In: _International Conference on Machine Learning_. PMLR. 2020, pp. 3199-3210.
* [GL16] Saeed Ghadimi and Guanghui Lan. "Accelerated gradient methods for nonconvex nonlinear and stochastic programming". In: _Mathematical Programming_ 156.1-2 (2016), pp. 59-99.
* [HCJ23] Audrey Huang, Jinglin Chen, and Nan Jiang. "Reinforcement Learning in Low-Rank MDPs with Density Features". In: _arXiv preprint arXiv:2302.02252_ (2023).
* [HDGP24] Jia Lin Hau, Erick Delage, Mohammad Ghavamzadeh, and Marek Petrik. "On Dynamic Programming Decompositions of Static Risk Measures in Markov Decision Processes". In: _Advances in Neural Information Processing Systems_ 36 (2024).
* [HJ22a] Audrey Huang and Nan Jiang. "Beyond the Return: Off-policy Function Estimation under User-specified Error-measuring Distributions". In: _Advances in Neural Information Processing Systems_. 2022.
* [HJ22b] Jiawei Huang and Nan Jiang. "On the convergence rate of off-policy policy optimization methods with density-ratio correction". In: _International Conference on Artificial Intelligence and Statistics_. PMLR. 2022, pp. 2658-2705.
* [HKSVS19] Elad Hazan, Sham Kakade, Karan Singh, and Abby Van Soest. "Provably efficient maximum entropy exploration". In: _International Conference on Machine Learning_. PMLR. 2019, pp. 2681-2691.
* [HM17] Assaf Hallak and Shie Mannor. "Consistent on-line off-policy evaluation". In: _International Conference on Machine Learning_. PMLR. 2017, pp. 1372-1383.
* [JKALS17] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. "Contextual decision processes with low Bellman rank are PAC-learnable". In: _International Conference on Machine Learning_. 2017.
* [JLM21] Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. "Bellman Eluder dimension: New rich classes of RL problems, and sample-efficient algorithms". In: _Advances in Neural Information Processing Systems_. 2021.
* [JYWJ20a] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. "Provably efficient reinforcement learning with linear function approximation". In: _Conference on Learning Theory_. 2020.
* [JYWJ20b] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. "Provably efficient reinforcement learning with linear function approximation". In: _Conference on learning theory_. PMLR. 2020, pp. 2137-2143.
* [KJDC24] Pulkit Kattare, Anant Joshi, and Katherine Driggs-Campbell. "Towards Provable Log Density Policy Gradient". In: _arXiv preprint arXiv:2403.01605_ (2024).
* [KU20] Nathan Kallus and Masatoshi Uehara. "Statistically efficient off-policy policy gradients". In: _International Conference on Machine Learning_. PMLR. 2020, pp. 5089-5100.

* [LLTZ18] Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. "Breaking the curse of horizon: Infinite-horizon off-policy estimation". In: _Advances in neural information processing systems_ 31 (2018).
* [LNSJ23] Qinghua Liu, Praneeth Netrapalli, Csaba Szepesvari, and Chi Jin. "Optimistic mle: A generic model-based algorithm for partially observable sequential decision making". In: _Proceedings of the 55th Annual ACM Symposium on Theory of Computing_. 2023, pp. 363-376.
* [LSAB19] Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. "Off-policy policy gradient with state distribution correction". In: _arXiv preprint arXiv:1904.08473_ (2019).
* [MBFR24] Zak Mhammedi, Adam Block, Dylan J Foster, and Alexander Rakhlin. "Efficient model-free exploration in low-rank mdps". In: _Advances in Neural Information Processing Systems_ 36 (2024).
* [MDSDBR22] Mirco Mutti, Riccardo De Santi, Piersilvio De Bartolomeis, and Marcello Restelli. "Challenging common assumptions in convex reinforcement learning". In: _Advances in Neural Information Processing Systems_ 35 (2022), pp. 4489-4502.
* [MHKL20] Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. "Kinematic state abstraction and provably efficient rich-observation reinforcement learning". In: _International conference on machine learning_. 2020.
* [MT01] Peter Marbach and John N Tsitsiklis. "Simulation-based optimization of Markov reward processes". In: _IEEE Transactions on Automatic Control_ 46.2 (2001), pp. 191-209.
* [NCDL19] Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. "Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections". In: _Advances in neural information processing systems_ 32 (2019).
* [NDKCLS19] Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. "Algagedice: Policy gradient from arbitrary experience". In: _arXiv preprint arXiv:1912.02074_ (2019).
* [NZJZW22] Chengzhuo Ni, Ruiqi Zhang, Xiang Ji, Xuezhou Zhang, and Mengdi Wang. "Optimal Estimation of Policy Gradient via Double Fitted Iteration". In: _International Conference on Machine Learning_. PMLR. 2022, pp. 16724-16783.
* [SB18] Richard S Sutton and Andrew G Barto. _Reinforcement learning: An introduction_. MIT press, 2018.
* [SMSM99] Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. "Policy gradient methods for reinforcement learning with function approximation". In: _Advances in neural information processing systems_ 12 (1999).
* [UHJ20] Masatoshi Uehara, Jiawei Huang, and Nan Jiang. "Minimax weight and q-function learning for off-policy evaluation". In: _International Conference on Machine Learning_. PMLR. 2020, pp. 9659-9668.
* [UIJK8X21] Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie. "Finite sample analysis of minimax offline reinforcement learning: Completeness, fast rates and first-order efficiency". In: _arXiv preprint arXiv:2102.02981_ (2021).
* [XCJMA21] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. "Bellman-consistent pessimism for offline reinforcement learning". In: _Advances in neural information processing systems_ 34 (2021).
* [XFBJK22] Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M Kakade. "The role of coverage in online reinforcement learning". In: _arXiv preprint arXiv:2210.04157_ (2022).
* [XJ21] Tengyang Xie and Nan Jiang. "Batch value-function approximation with only realizability". In: _International Conference on Machine Learning_. 2021.
* [XYWL21] Tengyu Xu, Zhuoran Yang, Zhaoran Wang, and Yingbin Liang. "Doubly robust off-policy actor-critic: Convergence and optimality". In: _International Conference on Machine Learning_. PMLR. 2021, pp. 11581-11591.
* [ZBWK20] Junyu Zhang, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel. "Cautious reinforcement learning via distributional risk in the dual domain". In: _arXiv preprint arXiv:2002.12475_ (2020).

* [ZHHJL22] Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. "Offline reinforcement learning with realizability and single-policy concentrability". In: _Conference on Learning Theory_. PMLR. 2022, pp. 2730-2775.
* [ZLKB20] Andrea Zanette, Alessandro Lazaric, Mykel J Kochenderfer, and Emma Brunskill. "Provably efficient reward-agnostic navigation with linear value iteration". In: _Advances in Neural Information Processing Systems_. 2020.

## Appendix

* [1] Related work
* [2] Additional results and proofs for Sec. 3
	* 2.1 Proofs for Sec. 3.1
	* 2.2 Proofs for Sec. 3.2 : Estimation and local convergence
	* 2.3 Proofs for Sec. 3.2: Global convergence
	* 2.4 Examples of gradient function class \(\mathcal{G}\)
	* 2.5 Policy optimization of general functionals
* [3] Additional results and proofs for Sec. 4
	* 3.1 Proofs for Sec. 4.1
	* 3.2 Proofs for Sec. 4.2
	* 3.3 Proofs for Sec. 4.3
	* 3.4 Local convergence of Off-OccupG
	* 3.5 Global convergence of Off-OccupG
	* 3.6 Proofs for App. C.5
* [4] Maximum Likelihood Estimation
* [5] Offline Density Estimation
* [6] Probabilistic Tools
* [7] Optimization Tools

## Appendix A Related work

In this section, we discuss related works in greater detail that concern the convergence and estimation of policy gradient in RL.

While a handful of recent papers have similarly observed that the gradient of the log density can be utilized to compute the policy gradient, especially in the context of using it to optimize general functionals, none of them have analyzed methods that are sample-efficient under general function approximation. In particular, [15] requires on-policy sampling from the time-reversed transition \((s,a|s^{\prime})\), which, as they note, is highly restrictive. To overcome this issue they propose a min-max algorithm that converges under linear approximation, which is computationally a far more difficult to solve (under a more stringent structural assumption) than the regression objective in Alg. 1. Similarly, [1] consider only online policy gradient, and to handle large state spaces they use linear function approximation, which may incur a large error through bias in many settings. [1] approach the problem of optimizing risk functionals through a primal-dual approach that involves occupancies as dual variables, but they only analyze convergence in tabular settings.

A number of works on off-policy gradient optimization utilize all three of the density ratio, value, and policy class functions to compute the gradient [14, 15, 16, 17]. This is because the density ratios are required to handle distribution mismatches with offline data. The downside, however, is that even max-min optimization is difficult, so performing optimization over all three functions requires complex optimization loops. By using simply projected gradient ascent on a policy class, our algorithms avoid such complexities and are amenable to classical convergence analysis that allow us to focus on the role of coverage coefficients in our final results.

Because the density ratio is generally not well-defined with arbitrary offline data, all of these works require some form of all-policy coverage for both estimation and convergence guarantees. The weight gradient calculation in [11] exhibits a recursive decomposition that is related to ours. However, their formulation is not compatible with our data assumptions and they require policy coverage to be well-defined. A follow-up paper in [21] uses squared-loss regression on the same updates, which is similar in flavor to our gradient estimation objectives. However, they use linear function approximation for weight functions, which is not realizable in general, and also require all-policy coverage for their convergence results.

One close work of comparison is [10], whose PG algorithm uses learned density ratios to reweight the data distribution and approximate the expression of the policy gradient theorem [20]. To handle coverage issues in offline PG, they "zero out" portions of the trajectory that exceed data coverage, but only do this for \((s,a)\) such that \(d^{D}(s)\pi^{D}(a|s)=0\). This is done (in the infinite horizon setting) by resampling the dataset based on an augmented MDP where such \((s,a)\) transition to an absorbing state However, this does not control the (potentially extremely large) variance of the estimator, e.g., on states where \(d^{D}(s)\approx 0\). Their objective can be seen as a special case of \(\bar{J}(\pi)\) with finite but extremely large choices of \(C^{\mathbf{s}}\) and \(C^{\mathbf{a}}\). They show convergence to a stationary point in terms of weight and value estimation errors that are left implicit, and leave the high variance and coverage issues with offline data implicit.

Another is [14], that takes the complementary approach and simply calculates the gradient averaged on \(d^{D}\). However, this is a biased gradient object and does not express the policy gradient of any specific function, which means its stationary point may not even exist, thus precluding convergence analysis.

The PSPI algorithm in [21] is a policy optimization algorithm based on pessimistic value functions. Their setting is somewhat orthogonal to ours in the sense that they study values and we study occupancies, and we note that they do not perform policy optimization with respect to a standalone policy class but rather an implicit one induced by the value functions, which an be extremely large. In the value function sphere, [21] leverage the linear structure of linear-MDPS to develop closed-form gradient estimators through the value functions. They largely only analyze estimation errors and additionally require a form of all-policy coverage for their results.

Lastly, our optimality analysis builds off the results in [1] and [1] that analyze global optimality in the (infinite horizon) online setting.

## Appendix B Additional results and proofs for Sec. 3

### Proofs for Sec. 3.1

Proof of Lem. 3.1First we expand \(\nabla d_{h}^{\pi}\) using the Bellman flow equation, \(d_{h}^{\pi}(s^{\prime})=\sum_{s,a}P(s^{\prime}|s,a)\pi(a|s)d_{h-1}^{\pi}(s)\):

\[\nabla d_{h}^{\pi}(s^{\prime}) =\sum_{s,a}P(s^{\prime}|s,a)(\nabla\pi(a|s)d_{h-1}^{\pi}(s)+\pi(a| s)\nabla d_{h-1}^{\pi}(s))\] \[=\sum_{s,a}P(s^{\prime}|s,a)\pi(a|s)d_{h-1}^{\pi}(s)(\nabla\log \pi(a|s)+\nabla\log d_{h-1}^{\pi}(s)).\]

In the last line above we use the grad-log trick. Note that \(\nabla\log d^{\pi}(s)\) is not well-defined when \(d^{\pi}(s)=0\), but the two terms will cancel out in the above expression for this case. From Bayes' theorem, \(\mathbb{P}^{\pi}(s_{h-1}=s,a_{h-1}=a|s_{h}=s^{\prime})=P(s^{\prime}|s,a)\pi(a |s)d_{h-1}^{\pi}(s)/d_{h}^{\pi}(s^{\prime})\), thus

\[\nabla\log d_{h}^{\pi}(s^{\prime}) =\nabla d_{h}^{\pi}(s^{\prime})/d_{h}^{\pi}(s^{\prime})\] \[=\mathbb{E}^{\pi}[\nabla\log\pi(a|s)+\nabla\log d_{h-1}^{\pi}(s) |s_{h}=s^{\prime}]\] \[=[\mathbf{E}_{h-1}^{\pi}(\nabla\log\pi+\nabla\log d_{h-1}^{\pi})] (s^{\prime}).\]

We use the convention that \(0/0=0\), thus \(\nabla\log d_{h}^{\pi}\) is always well-defined.

Lastly, the second statement results from Lem. B.1, which shows that \(\|\nabla\log d_{h}^{\pi}(s^{\prime})\|\) is always bounded and well-defined under Ass. 2.1.

**Lemma B.1**.: _Under Ass. 2.1, we have \(\max_{s,h}\|\nabla\log d_{h}^{\pi}(s)\|\leq hG\)._

Proof.: The lemma statement can be derived inductively starting from the observation that Eq. (1) is an expectation over its target functions. As a result, the maximum gradient magnitude should accrue additively over horizons. More concretely, fix \(h\) and \(s\). Then

\[\|\nabla\log\widetilde{d}_{h}^{\pi}(s^{\prime})\| =\|\mathbb{E}^{\pi}[\nabla\log\pi(a|s)+\nabla\log d_{h-1}^{\pi}(s)|s _{h}=s^{\prime}]\|\] \[\leq\|\nabla\log\pi(a|s)\|+\|\nabla\log d_{h-1}^{\pi}(s)\|\] \[\leq G+\|\nabla\log d_{h-1}^{\pi}(s)\|,\]

using Asm. 2.1 in the last line. Since \(\nabla\log d_{0}^{\pi}=\mathbf{0}\) by definition, unrolling the above recursion through timesteps gives the stated result. 

**Lemma B.2**.: _For \(g:\mathcal{S}\to\mathbb{R}^{p}\) and \(f:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{p}\), define the squared loss_

\[L_{h}(g;f,\pi)=\mathbb{E}_{\pi}[\|g(s_{h+1})-f(s_{h},a_{h})\|^{2}].\]

_Then for any such \(f\),_

\[\mathbf{E}_{h}^{\pi}(f)=\operatorname*{argmin}_{g:\mathcal{S}\to\mathbb{R}^{p }}L_{h}(g;f,\pi)\]

_and_

\[\nabla\log d_{h+1}^{\pi}=\operatorname*{argmin}_{g:\mathcal{S}\to\mathbb{R}^{ p}}L_{h}\left(g;\nabla\log\pi+\nabla\log d_{h}^{\pi},\pi\right).\]

Proof of Lem. b.2.: Since the objective is convex, can solve for the minimizer in closed form by taking the derivative and setting it to 0 in an element-wise manner. Fix \(s^{\prime}\). Taking the gradient of \(L_{h}(g;f,\pi)\) with respect to \(g(s^{\prime})\), we have that

\[0=d_{h}^{\pi}(s^{\prime})g(s^{\prime})-\sum_{s,a}P(s^{\prime}|s,a)\pi(a|s)d_{h -1}^{\pi}(s)f(s,a).\]

Rearranging and using the definition of \(\mathbf{E}_{h}^{\pi}\) gives the result. The second statement follows from Lem. 3.1. 

### Proofs for Sec. 3.2 : Estimation and local convergence

Proof of Thm. 3.1First we split up the errors contributed by regression and the estimation. Fix \(\pi\), then \(\mathbb{E}_{\mathcal{D}^{\text{reg}}}[\widehat{\nabla}J(\pi)]=\sum_{h}\mathbb{ E}_{s\sim d_{h}^{\pi}}[\widehat{g}_{h}^{\pi}(s)R_{h}(s)]\) and

\[\|\nabla J(\pi)-\widehat{\nabla}J(\pi)\|\leq\|\nabla J(\pi)-\mathbb{E}_{ \mathcal{D}^{\text{reg}}}[\widehat{\nabla}J(\pi)]\|+\|\mathbb{E}_{\mathcal{D }^{\text{reg}}}[\widehat{\nabla}J(\pi)]-\widehat{\nabla}J(\pi)\|\]

The first term is related to the regression error in \(\widehat{g}_{h}^{\pi}\) approximating \(\nabla\log d_{h}^{\pi}\),

\[\|\nabla J(\pi)-\mathbb{E}_{\mathcal{D}^{\text{reg}}}[\widehat{ \nabla}J(\pi)]\| =\,\left\|\sum_{h}\mathbb{E}_{d_{h}^{\pi}}[\nabla\log d_{h}^{\pi}( s)R_{h}(s)]-\mathbb{E}_{d_{h}^{\pi}}[\widehat{g}_{h}^{\pi}(s)R_{h}(s)]\right\|\] \[\leq\,\sum_{h}\left\|\mathbb{E}_{d_{h}^{\pi}}[\nabla\log d_{h}^{ \pi}(s)-\widehat{g}_{h}^{\pi}(s)]\right\|\] \[=\,\sum_{h}\sqrt{\sum_{p=1}^{p}\|\nabla^{p}\log d_{h}^{\pi}-[ \widehat{g}_{h}^{\pi}]^{p}\|_{1,d_{h}^{\pi}}^{2}}.\]

For a fixed \(h\) and \(p\), we recursively decompose

\[\|\nabla^{p}\log d_{h}^{\pi}-[\widehat{g}_{h}^{\pi}]^{p}\|_{1,d_ {h}^{\pi}} \leq\|\nabla^{p}\log d_{h}^{\pi}-[\mathbf{E}_{h-1}^{\pi}(\nabla \log\pi+\widehat{g}_{h-1}^{\pi})]^{p}\|_{1,d_{h}^{\pi}}\] \[\qquad\qquad+\|[\mathbf{E}_{h-1}^{\pi}(\nabla\log\pi+\widehat{g}_ {h-1}^{\pi})]^{p}-[\widehat{g}_{h}^{\pi}]^{p}\|_{1,d_{h}^{\pi}}\] \[\leq\|\nabla^{p}\log d_{h-1}^{\pi}-[\widehat{g}_{h-1}^{\pi}]^{p} \|_{1,d_{h-1}^{\pi}}+\|[\mathbf{E}_{h-1}^{\pi}(\nabla\log\pi+\widehat{g}_{h-1}^ {\pi})]^{p}-[\widehat{g}_{h}^{\pi}]^{p}\|_{2,d_{h}^{\pi}},\]

using the fact that \(\nabla\log d_{h}^{\pi}=\mathbf{E}_{h-1}^{\pi}(\nabla\log\pi+\nabla\log d_{h-1}^ {\pi})\) in the second line. Then unrolling the recursion, we have

\[\|\nabla^{p}\log d_{h}^{\pi}-[\widehat{g}_{h}^{\pi}]^{p}\|_{1,d_{h}^{\pi}} \leq\sum_{h}\|[\mathbf{E}_{h-1}^{\pi}(\nabla\log\pi+\widehat{g}_{h-1}^{\pi})]^{ p}-[\widehat{g}_{h}^{\pi}]^{p}\|_{2,d_{h}^{\pi}}\]

Applying Lem. F.2 (more exactly, this is an offline version but we invoke it with \(\rho=1\) and no clipping for the online setting) with \(\delta^{\prime}=\delta/2H\text{p}\) and a union bound over all \(h\) and \(p\), we have

\[\|[\mathbf{E}_{h-1}^{\pi}(\nabla\log\pi+\widehat{g}_{h-1}^{\pi})]^{p}-[ \widehat{g}_{h}^{\pi}]^{p}\|_{2,d_{h}^{\pi}}^{2} =\mathbb{E}[\mathcal{L}_{\mathcal{D}_{h-1}^{\text{reg}}}^{\rho}(\widehat{g}_{h }^{\pi},\widehat{g}_{h-1}^{\pi})-\mathcal{L}_{\mathcal{D}_{h-1}^{\text{reg}}}^{ \rho}(\mathbf{E}_{h-1}^{\pi}(\nabla\log\pi+\widehat{g}_{h-1}^{\pi}),\widehat{g} _{h-1}^{\pi})]\]\[\leq 2(\varepsilon_{h-1}^{\mathrm{reg}})^{2},\]

where \(\varepsilon_{h-1}^{\mathrm{reg}}=\sqrt{\frac{\mathrm{cd}_{\mathcal{G}}h^{2}G^{2} \log(2H\mathsf{p}/\delta)}{n}}\). Then for any \(h,p\) we have

\[\|\nabla^{p}\log d_{h}^{\pi}-[\widehat{g}_{h}^{\pi}]^{p}\|_{1,d_{h}^{\pi}}\leq \sqrt{2}\sum_{g\leq h}\varepsilon_{g}^{\mathrm{reg}}\leq\sqrt{2}h\varepsilon_ {h}^{\mathrm{reg}}=\sqrt{\frac{2\mathsf{cd}_{\mathcal{G}}h^{4}G^{2}\log(2H \mathsf{p}/\delta)}{n}}\]

Implying

\[\|\nabla J(\pi)-\mathbb{E}_{\mathcal{D}^{\mathrm{reg}}}[\widehat{\nabla}J(\pi )]\|\leq\sqrt{\mathsf{p}}H\|\nabla^{p}\log d_{H}^{\pi}-[\widehat{g}_{H}^{\pi}] ^{p}\|_{1,d_{H}^{\pi}}=\sqrt{\frac{2\mathsf{cd}_{\mathcal{G}}H^{6}G^{2}\log(2 H\mathsf{p}/\delta)}{n}}\]

For the second term,

\[|\mathbb{E}_{\mathcal{D}^{\mathrm{reg}}}[\widehat{\nabla}^{p}J(\pi)]- \widehat{\nabla}^{p}J(\pi)|\leq\sum_{h}|\mathbb{E}_{s\sim d_{h}^{\pi}}[ \widehat{g}_{h}^{\pi}(s)R_{h}(s)]-\frac{1}{n}\sum_{s\in\mathcal{D}_{h}} \widehat{g}_{h}^{\pi}(s)R_{h}(s)|\leq H^{2}G\sqrt{\frac{\log(2pH/\delta)}{n}},\]

where we use Hoeffding's inequality with union bound, for all \(h\in[H]\) and \(p\in\mathsf{p}\) in the last line, given that the randomness of \(\widehat{g}\) is independent given \(\mathcal{D}_{h}^{\mathrm{reg}}\). Thus

\[\|\mathbb{E}_{\mathcal{D}^{\mathrm{reg}}}[\widehat{\nabla}J(\pi)]-\widehat{ \nabla}J(\pi)\|\leq H^{2}G\sqrt{\frac{\mathsf{p}\log(2\mathsf{p}H/\delta)}{n}}\]

Combining the two terms, our final bound is

\[\|\nabla J(\pi)-\widehat{\nabla}J(\pi)\|\lesssim\sqrt{\frac{\mathsf{pd}_{ \mathcal{G}}H^{6}G^{2}\log(2H\mathsf{p}/\delta)}{n}},\]

with the regression error dominating.

Proof of Cor. 3.1For any fixed run of Alg. 1, calling Thm. 3.1 for \(\pi^{(t)}\) with \(\delta^{\prime}=\delta/T\) and taking a union bound over \(T\) gives with probability at least \(1-\delta\) that

\[\|\nabla J(\pi^{(t)})-\widehat{\nabla}J(\pi^{(t)})\|\lesssim\sqrt{\frac{ \mathsf{pd}_{\mathcal{G}}H^{6}G^{2}\log(2H\mathsf{p}T/\delta)}{n}},\;\forall t \in[T].\]

Then setting \(\delta=1/\sqrt{n}\), we have

\[\mathbb{E}\left[\|\nabla J(\pi^{(t)})-\widehat{\nabla}J(\pi^{(t)})\|\right] \lesssim\sqrt{\frac{\mathsf{pd}_{\mathcal{G}}H^{6}G^{2}\log(2H\mathsf{p}Tn)}{n }},\]

where the expectation is over random samples in \(\mathcal{D}^{\mathrm{reg}},\mathcal{D}^{\mathrm{est}}\). Finally, plugging this into the PGD stationary convergence bound in Lem. G.1 gives

\[\frac{1}{T}\sum_{t}\mathbb{E}\left[\|G^{\eta}(\pi^{(t)},\nabla J(\pi^{(t)}))\| ^{2}\right]\leq\frac{4\beta H}{T}+\frac{6\mathsf{pd}_{\mathcal{G}}H^{6}G^{2} \log(2H\mathsf{p}Tn)}{n}\]

Setting the RHS to \(\varepsilon\) and setting \(T,n\) appropriately gives the result.

### Proofs for Sec. 3.2: Global convergence

We will establish the conditions under which \(J(\pi)\) satisfies a gradient domination property, meaning that for any \(\theta\in\Theta\), the suboptimality of \(\pi_{\theta}\) is bounded by some function \(S\) that includes a measure of its stationarity, i.e., \(\max_{\pi^{\prime}\in\Pi_{\Theta}}\widetilde{J}(\pi^{\prime})-J(\pi_{\theta}) \leq S(\nabla\widetilde{J}(\pi_{\theta}))\). This combined with the sample complexity bounds for stationary convergence established in Cor. 3.1 enables our global optimality result in Cor. 3.2.

Though we are concerned with optimizing \(J(\pi)\) induced by running \(\pi\) starting from initial distribution \(d_{0}\), it will be useful to consider performing Alg. 1 using a different exploratory initial distribution \(\mu\in\Delta(\mathcal{S})\). By exploratory, we mean that we allow \(\mu(s)>0\) for all \(s\in\mathcal{S}\), unlike \(d_{0}\in\Delta(\mathcal{S}^{0})\). In the (stationary) infinite horizon this is a common trick for obtaining well-defined gradient domination bounds [1], but its finite-horizon (nonstationary) counterpart is nontrivial and to our knowledge has not previously been formalized (it is listed as future work in [1]).

We state and prove a more general version of Lem. 3.2:

**Lemma B.3**.: _For any \(\pi\) and \(\pi^{\prime}\), define \(B^{\pi}(\pi^{\prime}):=\sum_{h,s,a}d_{h}^{\pi^{*}}(s)\pi^{\prime}(a|s)Q_{h}^{\pi }(s,a)\). Suppose \(\forall\pi\in\Pi_{\Theta}\),_

1. _(Policy completeness) There exists_ \(\pi^{+}\in\Pi_{\Theta}\) _such that_ \(\pi^{+}\in\operatorname*{argmax}_{\pi^{\prime}}B^{\pi}(\pi^{\prime})\)_._
2. _(Gradient domination)_ \(\max_{\pi^{\prime}\in\Pi_{\Theta}}B^{\pi}(\pi^{\prime})-B^{\pi}(\pi)\leq m \max_{\theta^{\prime}\in\Theta}\left\langle\nabla B^{\pi}(\pi),\theta^{\prime }-\theta\right\rangle\)__

_Given \(\nu\in\Delta(\mathcal{S})\), define the coverage coefficient \(\mathcal{C}^{\pi^{*}}:=\left\|\sum_{h}d_{h}^{\pi^{*}}/\nu\right\|_{\infty}\) for \(\pi^{*}=\operatorname*{argmax}_{\pi}J(\pi)\). Then for any \(\pi_{\theta}\in\Pi_{\Theta}\),_

\[J(\pi^{*})-J(\pi_{\theta}) \leq m\left\|\frac{\sum_{h}d_{h}^{\pi^{*}}}{\sum_{h}d_{\mu,h}^{ \pi_{*}}}\right\|_{\infty}\,\max_{\theta^{\prime}\in\Pi_{\Theta}}\left\langle \nabla J_{\mu}(\pi_{\theta}),\theta^{\prime}-\theta\right\rangle\] \[\leq\mathcal{C}^{\pi^{*}}\max_{\theta^{\prime}\in\Pi_{\Theta}} \left\langle\nabla J_{\nu}(\pi_{\theta}),\theta^{\prime}-\theta\right\rangle,\]

_where \(J_{\nu}(\pi):=\mathbb{E}_{s_{0}\sim\nu,\pi}[\sum_{h}r_{h}]\) is the expected return of \(\pi\) in \(\mathcal{M}\) with initial state distribution \(\nu\)._

Proof of Lem. B.3First we note two facts that hold regardless of \(\mathcal{M}\). We have \(Q_{g}^{\pi}(s^{h},a^{h})=Q_{h}^{\pi}(s^{h},a^{h})\) for any \(g\leq h\), and \(d_{g}^{\pi}(s^{h})=0\) if \(g>h\).

\[J(\pi^{*})-J(\pi_{\theta})=\sum_{h=0}^{H-1}\sum_{s,a}d_{h}^{\pi^{*}}(s)\left( \pi^{*}(a|s)-\pi_{\theta}(a|s)\right)Q_{h}^{\pi_{\theta}}(s,a)\]

Then we will write \(Q^{\pi}(s,a)\equiv Q_{h}^{\pi}(s,a)\), thus

\[J(\pi^{*})-J(\pi_{\theta})= \,\sum_{h=0}^{H-1}d_{h}^{\pi^{*}}(s)\left(\pi^{*}(a|s)-\pi_{ \theta}(a|s)\right)Q^{\pi_{\theta}}(s,a)\] \[= \,\sum_{s,a}\left(\sum_{h}d_{h}^{\pi^{*}}(s)\right)\left(\pi^{*} (a|s)-\pi_{\theta}(a|s)\right)Q^{\pi_{\theta}}(s,a)\] \[\leq \,\max_{\pi^{+}}\sum_{s,a}\left(\sum_{h}d_{h}^{\pi^{*}}(s)\right) \left(\pi^{+}(a|s)-\pi_{\theta}(a|s)\right)Q^{\pi_{\theta}}(s,a)\] \[\leq \,\max_{\pi^{+}}\sum_{s,a}\frac{\sum_{h}d_{h}^{\pi^{*}}(s)}{\sum _{h}d_{\mu,h}^{\pi_{*}}(s)}\left(\sum_{h}d_{\mu,h}^{\pi_{\theta}}(s)\right) \left(\pi^{+}(a|s)-\pi_{\theta}(a|s)\right)Q^{\pi_{\theta}}(s,a)\] \[\leq \,\left\|\frac{\sum_{h}d_{h}^{\pi^{*}}}{\mu}\right\|_{\infty}\, \max_{\pi^{+}}\sum_{s,a}\left(\sum_{h}d_{\mu,h}^{\pi_{\theta}}(s)\right) \left(\pi^{+}(a|s)-\pi_{\theta}(a|s)\right)Q^{\pi_{\theta}}(s,a)\]

For the RHS, observe that \(d_{\mu,g}^{\pi_{\theta}}(s^{h})=0\) for \(g>h\). Then

\[\sum_{h}\sum_{s,a}d_{\mu,h}^{\pi_{\theta}}(s)\left(\pi^{+}(a|s)- \pi_{\theta}(a|s)\right)Q^{\pi_{\theta}}(s,a)\] \[= \,\sum_{h}\sum_{s,a}d_{\mu,h}^{\pi_{\theta}}(s)\left(\pi^{+}(a|s )-\pi_{\theta}(a|s)\right)Q_{h}^{\pi_{\theta}}(s,a)\] \[= \,\sum_{h}\sum_{s,a}d_{\mu,h}^{\pi_{\theta}}(s)\left(\pi^{+}(a|s )-\pi_{\theta}(a|s)\right)Q_{\mu,h}^{\pi_{\theta}}(s,a)\] \[= \,B^{\pi_{\theta}}(\pi^{+})-B^{\pi_{\theta}}(\pi_{\theta})\] \[\leq m\max_{\theta^{\prime}\in\Theta}\left\langle\nabla B^{\pi_{ \theta}}(\pi_{\theta}),\theta^{\prime}-\theta\right\rangle\] \[= m\max_{\theta^{\prime}\in\Theta}\left\langle\nabla J_{\mu}(\pi_ {\theta}),\theta^{\prime}-\theta\right\rangle\]

Combining the two inequalities results in the final guarantee.

Proof of Cor. 3.2Fix \(\{\pi^{(t)}\}_{t\in[T]}\) from Alg. 1. Then for any \(t\in[T]\), from Lem. 3.2 we have

\[J(\pi^{*})-J(\pi^{(t)})\leq m\mathcal{C}^{\pi^{*}}\max_{\theta^{\prime}\in\Pi_{ \Theta}}\left\langle\nabla J(\pi^{(t)}),\theta^{\prime}-\theta\right\rangle\]\[\leq Bm\mathcal{C}^{\pi^{*}}\left\|G^{n}(\pi^{(t)})\right\|\] (Lem. G.4)

Then summing through \(T\) and taking an expectation over the randomness in the algorithm, we have

\[\mathbb{E}\left[\frac{1}{T}\sum_{t}J(\pi^{*})-J(\pi^{(t)})\right] \leq Bm\mathcal{C}^{\pi^{*}}\mathbb{E}\left[\frac{1}{T}\sum_{t} \|G^{\eta}(\pi^{(t)})\|\right]\] \[\leq Bm\mathcal{C}^{\pi^{*}}\left(\frac{4\beta H}{T}+\frac{6 \mathsf{pd}_{\hat{\mathcal{G}}}H^{6}G^{2}\log(2H\mathsf{p}Tn)}{n}\right).\] (Cor. 3.1)

### Examples of gradient function class \(\mathcal{G}\)

This section contains formal statements of the claims in Rem. 3.1, and their proofs. We begin by defining the low-rank MDP, noting that for notational compactness we have dropped the features' \(h\)-dependence given our assumption that there is a one-to-one correspondence between states and the timestep at which they are visited.

**Definition B.1** (Low-rank MDP).: We say \(\mathcal{M}\) is a low-rank MDP with dimension \(k\) if \(\forall h\in[H]\), there exists \(\phi:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{k}\) and \(\mu_{h}:\mathcal{S}\to\mathbb{R}^{k}\) such that \((s,a,s^{\prime})\), we have \(P(s^{\prime}|s,a)=\left\langle\phi(x,a),\mu(x^{\prime})\right\rangle\). Further, \(\|\phi\|_{\infty}\leq C^{\phi}\) and \(\sum_{s}\mu(s)\leq C^{\mu}\).

Prop. B.1 shows that, in low-rank MDPs, a linear-over-linear parameterization for the gradient function class satisfies the completeness requirement in Asm. 3.1, with pseudo-dimension linear in the low-rank dimension and the parameter dimension, i.e., \(\mathsf{d}_{\mathcal{G}_{h}}=O(k\mathsf{p})\).

**Proposition B.1**.: _Suppose \(\mathcal{M}\) is a low-rank MDP (Def. B.1), and suppose \(\mu\) is known. For each layer \(h\), define the function class_

\[\mathcal{G}_{h}=\left\{g_{h}=\frac{\mu^{\top}\Psi}{\mu^{\top}\psi}:\Psi\in \mathbb{R}^{k\times\mathsf{p}},\psi\in\mathbb{R}^{k},\|g_{h}\|_{\infty}\leq hG,\ \forall h\in[H]\right\}.\]

_Then \(\{\mathcal{G}_{h}\}\) satisfies Asm. 3.1 and has pseudodimension (Def. F.1) \(\mathsf{d}_{\mathcal{G}_{h}}=O(k\mathsf{p})\)._

Proof of Prop. B.1.: It suffices to show that, for any function \(f:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{\mathsf{p}}\) and policy \(\pi\), its gradient update from Lem. 3.1 is \(\mathbf{E}_{h}^{\pi}(\nabla\log\pi+f)\in\mathcal{G}_{h+1}\).

Since \([\mathbf{E}_{h}^{\pi}(\nabla\log\pi+f)](s^{\prime})=\mathbb{E}_{\pi}[\nabla \log\pi(s,a)+f(s)|s^{\prime}]\), from Bayes' rule and the definition of the Bellman flow operator (see proof of Lem. 3.1), we have

\[[\mathbf{E}_{h}^{\pi}(\nabla\log\pi+f)](s^{\prime})=\frac{[\mathbf{P}_{h-1}^{ \pi}(\nabla\log\pi+f)](s^{\prime})}{d_{h}^{\pi}(s^{\prime})}.\]

First, we will show that \([\mathbf{P}_{h}^{\pi}f](s^{\prime})=\mu(s^{\prime})^{\top}\Psi\) for some \(\Psi\in\mathbb{R}^{k\times\mathsf{p}}\) and all \(s^{\prime}\in\mathcal{S}\). Below, we use \(f^{p}(s)\) to denote the \(p\)-th parameter of \(f(s)\in\mathbb{R}^{\mathsf{p}}\). For fixed \(p\in[\mathsf{p}]\),

\[[\mathbf{P}_{h}^{\pi}f]^{p}(s^{\prime}) =\sum_{s,a}P(s^{\prime}|s,a)\pi(a|s)\left(\nabla^{p}\log\pi(a|s)+f ^{p}(s)\right)\] \[=\mu(s^{\prime})^{\top}\left(\sum_{s,a}\phi(s,a)\pi(a|s)\left( \nabla^{p}\log\pi(a|s)+f^{p}(s)\right)\right)\] \[=\mu(s^{\prime})^{\top}\psi,\]

where \(\psi=\sum_{s,a}\phi(s,a)\pi(a|s)\left(\nabla\log\pi(a|s)+f^{p}(s)\right)\in \mathbb{R}^{k}\). Stacking this result for each \(p\) into the matrix \(\Psi\) shows the desired statement that \([\mathbf{P}_{h}^{\pi}f](s^{\prime})=\mu(s^{\prime})^{\top}\Psi\).

We can apply similar reasoning as above in the Bellman flow equation to show that \(d_{h}^{\pi}(s^{\prime})=\left\langle\mu(s^{\prime}),\psi\right\rangle\) for some \(\theta\in\mathbb{R}^{k}\). Combined with the above, this shows that

\[\nabla\log d_{h}^{\pi}(s^{\prime})=\frac{\mu^{\top}\Psi}{\mu^{\top}\psi},\]

Combining the linear forms of the numerator and denominator reveal that \(\nabla\log d_{h}^{\pi}\in\mathcal{G}_{h}\). Lastly, the pseudo-dimension of \(\mathcal{G}_{h}\) follows directly from applying Lemma 24 of [1], which bounds the pseudo-dimension of linear-over-linear function classes with \(\mathsf{p}=1\), in all \(\mathsf{p}\) dimensions.

### Policy optimization of general functionals

Alg.3 displays the full algorithm for optimization of general functions (described in Sec. 3.3). It shares its occupancy gradient estimation module with OccuPG. Compared to Alg.1, the only change is the objective gradient calculation in Line7, which uses a plug-in estimate of the occupancy (Line6) to evaluate the partial derivative.

Since the algorithmic change is small, the analysis for Alg.3 requires only a few adaptations from the analysis of OccuPG. For smooth and differentiable functionals, we provide the gradient estimation guarantee below. The smoothness ensures that using plug-in occupancy estimates to evaluate the partial derivative leads to consistent gradient estimates, and is in line with the spirit of standard objective smoothness requirements (Asm. 3.2).

**Assumption B.1**.: Suppose that for all \(h\), \(F_{h}\) has a smooth gradient, i.e., for any \(f,f^{\prime}\in\Delta(\mathcal{S})\) that

\[\sum_{s}\left|\frac{\partial F_{h}(d)}{\partial d(s)}\right|_{d=f}-\left. \frac{\partial F_{h}(d)}{\partial d(s)}\right|_{d=f^{\prime}}\right|\leq L_{F} \|f-f^{\prime}\|_{1},\]

and has bounded range \(\|\partial F_{h}(d)\|_{\infty}\leq C_{F}\).

**Theorem B.1**.: _Suppose that Asm. 2.1 and Asm. 2.1 hold. Fix \(\pi\in\Pi_{\Theta}\). With probability at least \(1-\delta\),_

\[\|\nabla J_{F}(\pi)-\widehat{\nabla}J_{F}(\pi)\|\lesssim H^{2}GL_{F}\sqrt{ \frac{\mathsf{p}\log(2\mathsf{p}H|\mathcal{F}|/\delta)}{n}}+C_{F}\sqrt{\frac{ \mathsf{p}\mathsf{d}_{\mathcal{G}}H^{6}G^{2}\log(2H\mathsf{p}/\delta)}{n}}.\]

When Asm. 3.2 holds, this result directly leads to a stationary convergence guarantee similar to Cor. 3.1, by union bounding Thm. B.1 over all \(T\) then plugging it into Lem. G.5 (see proof of Cor. 3.1). We expect that the global convergence in Cor. 3.2 can also be extended with little overhead when \(\{F_{h}\}\) are convex, but leave a full investigation to future work.

Proof of Thm. B.1The analysis follows largely the same lines as the proof of Thm. 3.1. However, we must additional account for the error of approximating \(\frac{\partial F_{h}(d)}{\partial d(s)}\Big{|}_{d=\widehat{d}_{h}^{\pi}}\) with the plug-in occupancy estimate. This was unnecessary for the expected return in Sec. 3.2 since \(\frac{\partial F_{h}(d)}{\partial d(s)}=R_{h}(s)\) is independent of the occupancy.

First, for all \(h\in[H]\), with probability at least \(1-\delta\) we have occupancy estimates from Alg.4 such that

\[\|d_{h}^{\pi}-\widehat{d}_{h}^{\pi}\|_{1}\leq\sqrt{\frac{2\log(H|\mathcal{F}| /\delta)}{n}}:=\varepsilon^{\mathrm{mle}},\ \forall h\in[H].\]This follows directly from Lem. D.1 with a union bound over \(H\).

Next, we isolate the occupancy estimation-related term from the error we would like to bound. Define \(\nabla\widehat{J}_{F}(\pi):=\sum_{h}\mathbb{E}_{s\sim d_{h}^{\pi}}\left[\frac{ \partial F_{h}(d)}{\partial d(s)}\rvert_{d=\widehat{d}_{h}^{\pi}}\nabla\log d_{ h}^{\pi}(s)\right]\), and decompose

\[\|\nabla J_{F}(\pi)-\widehat{\nabla}J_{F}(\pi)\|\leq\|\nabla J_{F}(\pi)-\nabla \widehat{J}_{F}(\pi)\|+\|\nabla\widehat{J}_{F}(\pi)-\widehat{\nabla}J_{F}(\pi)\|\]

For the first term,

\[\|\nabla J_{F}(\pi)-\nabla\widehat{J}_{F}(\pi)\| \leq\sum_{h}\left\|\mathbb{E}_{s\sim d_{h}^{\pi}}\left[\frac{ \partial F_{h}(d)}{\partial d(s)}\rvert_{d=d_{h}^{\pi}}\nabla\log d_{h}^{\pi }(s)-\frac{\partial F_{h}(d)}{\partial d(s)}\rvert_{d=\widehat{d}_{h}^{\pi}} \nabla\log d_{h}^{\pi}(s)\right]\right\|\] \[\leq HG\sum_{h}\left\|\mathbb{E}_{s\sim d_{h}^{\pi}}\left[\frac{ \partial F_{h}(d)}{\partial d(s)}\rvert_{d=d_{h}^{\pi}}-\frac{\partial F_{h}( d)}{\partial d(s)}\rvert_{d=\widehat{d}_{h}^{\pi}}\right]\right\|\] \[\leq H^{2}GL_{F}\max_{h}\|d_{h}^{\pi}-\widehat{d}_{h}^{\pi}\|_{1},\] \[\leq H^{2}GL_{F}\varepsilon^{\mathrm{mle}}.\]

using Asm. B.1 in the second to last inequality. This takes care of the aforementioned occupancy estimation error.

Conditioned on such \(\{\widehat{d}_{h}^{\pi}\}\), the second pair of terms \(\|\nabla\widehat{J}_{F}(\pi)-\widehat{\nabla}J_{F}(\pi)\|\) is analogous to the error bounded in Thm. 3.1, and the proof follows identically thereon, but with dependence on the range \(C_{F}\) of the functionals.

## Appendix C Additional results and proofs for Sec. 4

### Proofs for Sec. 4.1

Proof of Lem. 4.1By passing the gradient through the clipped Bellman flow equation in Def. 4.2, we have

\[\nabla\bar{d}_{h}^{\pi}(s^{\prime})\] \[=\sum_{s,a}P(s^{\prime}|s,a)\left(\nabla\bar{\pi}_{h-1}(a|s) \bar{d}_{h-1}^{\pi}(s)+\pi(a|s)\nabla\left(\bar{d}_{h-1}^{\pi}(s)\wedge C_{h-1 }^{\mathsf{s}}d_{h-1}^{D}(s)\right)\right)\] \[=\sum_{s,a}P(s^{\prime}|s,a)\bar{\pi}_{h-1}(a|s)\left(\bar{d}_{h- 1}^{\pi}(s)\wedge C_{h-1}^{\mathsf{s}}d_{h-1}^{D}(s)\right)\left(\nabla\log \bar{\pi}_{h-1}(a|s)\right.\] \[\left.\hskip 113.811024pt+\nabla\log\left(\bar{d}_{h-1}^{\pi}(s) \wedge C_{h-1}^{\mathsf{s}}d_{h-1}^{D}(s)\right)\right)\]

Next, dropping the \(h-1\) subscript for a moment, observe that

\[\nabla\log\left(\bar{d}^{\pi}(s)\wedge C^{\mathsf{s}}d^{D}(s)\right)=\begin{cases} \nabla\log\bar{d}^{\pi}(s),&\text{if }\bar{d}^{\pi}(s)<C^{\mathsf{s}}d^{D}(s),\\ 0,&\text{if }\bar{d}^{\pi}(s)>C^{\mathsf{s}}d^{D}(s),\end{cases}\]

with a discontinuity at \(\bar{d}^{\pi}(s)=C^{\mathsf{s}}d^{D}(s)\). For simplicity, we set \(\nabla\log\left(\bar{d}^{\pi}(s)\wedge C^{\mathsf{s}}d^{D}(s)\right)=\nabla \log\bar{d}^{\pi}(s)\odot\mathbf{1}[\bar{d}^{\pi}(s)\leq C^{\mathsf{s}}d^{D}(s)]\). Similarly, we have \(\nabla\log\bar{\pi}(a|s)=\nabla\log\pi(a|s)\odot\mathbf{1}[\pi(a|s)\leq C^{ \mathsf{s}}\pi^{D}(a|s)]\).

Finally, \(\nabla\log\bar{d}_{h}^{\pi}(s^{\prime})=\nabla\bar{d}_{h}^{\pi}(s^{\prime})/ \bar{d}_{h}^{\pi}(s^{\prime})\), where \(\bar{d}_{h}^{\pi}(s^{\prime})=\sum_{s,a}P(s^{\prime}|s,a)\pi_{h-1}^{D}(a|s)d_{ h-1}^{D}(s)\bar{\rho}_{h-1}^{\pi}\). The lemma statement follows from the definition of \(\mathbf{E}_{h-1}^{D,\bar{\rho}}\), and the gradient magnitude bound results from invoking Lem. C.2 with \(\sigma\left(x,c\right)=(x\wedge c)\).

Proof of Prop. 4.1This result follows from applying Lem. C.7, which is a more general version of the proposition statement that holds for any (smooth-)clipping function, to \(\sigma\left(x,c\right)=(x\wedge c)\).

Proof of Prop. 4.2The MDP we will describe corresponds to a multi-armed bandit with 2 actions. Consider an MDP with \(H=2\), and \(\mathcal{S}^{0}=\{s_{0}\}\), \(\mathcal{S}^{1}=\{s_{L},s_{R}\},\mathcal{S}^{2}=\{s_{-},s_{+}\}\) which are terminal. In any state there are two actions, \(\mathcal{A}=\{L,R\}\), with deterministic transitions. For the first level, we have \(s_{0}\overset{L}{\to}s_{L}\) and \(s_{0}\overset{R}{\to}s_{R}\). For the second level, we have \(s_{L}\to s_{-}\) and \(s_{R}\to s_{+}\), regardless of action taken. For the reward function, \(R(s_{+})=1\) and is 0 otherwise.

The policy is parameterized by a single parameter \(\theta\) such that \(\pi(L)=1-\theta\), and \(\pi(R)=\theta\), such that \(\hat{d}_{1}^{\pi_{\theta}}(s_{R})=\hat{d}_{2}^{\pi_{\theta}}(s_{+})=\theta\). Further, both the offline data and behavior policy are uniform in each level. Consequently, \(\pi_{0}^{D}(L)=\pi_{0}^{D}(R)=\frac{1}{2}\) and \(d_{1}^{D}=\operatorname{unif}(\mathcal{S}^{1})\). We set \(C_{1}^{\mathbf{s}}=C_{2}^{\mathbf{s}}=2\), and \(C_{2}^{\mathbf{a}}=2\) so that \(\overline{\pi}_{h}=\pi_{h}\) for all \(h\).

Fix \(\theta\) and estimated occupancies \(\hat{d}^{\pi_{\theta}}\) and \(\hat{d}^{D}\). For any \(s^{\prime}\in\mathcal{S}^{2}\) we have

\[\left\|\nabla\log d_{2}^{\pi_{\theta}}(s^{\prime})-\widehat{\nabla}\log\bar{d }_{2}^{\pi_{\theta}}(s^{\prime})\right\|=\left\|\nabla\bar{d}_{1}^{\pi_{\theta} }(s^{\prime})\left(\mathbf{1}[\hat{d}_{1}^{\pi_{\theta}}(s^{\prime})\leq\hat{d} _{1}^{D}(s^{\prime})]-\mathbf{1}[d_{1}^{\pi_{\theta}}(s^{\prime})\leq d_{1}^{D} (s^{\prime})]\right)\right\|\]

Next, we instantiate \(\hat{d}^{\pi_{\theta}},\hat{d}^{D}\) for any \(\pi_{\theta}\). The preconditions of the proposition are satisfied by an estimated occupancy with \(\hat{d}_{1}^{\pi_{\theta}}(s_{L})=\theta+\epsilon/2\) and \(\hat{d}_{1}^{\pi_{\theta}}(s_{R})=\theta-\epsilon/2\). In addition, we have an estimate \(\hat{d}^{D}\) with \(\hat{d}_{1}^{D}(s_{L})=1/2-\epsilon/2\) and \(\hat{d}_{1}^{D}(s_{R})=1/2+\epsilon/2\).

We will consider \(\theta=1/2\), so that \(d_{1}^{\pi_{\theta}}\leq C_{1}^{\mathbf{s}}d_{1}^{D}\). However, \(\hat{d}_{1}^{\pi_{\theta}}(s_{L})>\hat{d}_{1}^{D}(s_{L})\). As a result,

\[\left\|\nabla\log d_{2}^{\pi_{\theta}}(s^{\prime})-\widehat{\nabla}\log\bar{d }_{2}^{\pi_{\theta}}(s^{\prime})\right\|=\left\|\nabla\hat{d}_{1}^{\pi_{ \theta}}(s^{\prime})\right\|=O(1)\]

### Proofs for Sec. 4.2

First, we formally state and prove the claim that Lem. 4.2 can be reduced to minimizing a squared-loss regression problem recursively over timesteps, i.e.,

\[\nabla\log\widetilde{d}_{h+1}^{\pi}\] (11) \[= \operatorname*{argmin}_{g:\mathcal{S}\to\mathbb{R}^{p}}\mathbb{E }_{\mathcal{D}_{h}}\left[\left\|g(s^{\prime})-\left(\nabla\log\pi\odot\mathbf{ \tilde{1}}\left(\pi,C_{h}^{\mathbf{a}}d_{h}^{D}\right)+\nabla\log\widetilde{d} _{h-1}^{\pi}\odot\mathbf{\tilde{1}}\left(\widetilde{d}_{h}^{\pi},C_{h}^{ \mathbf{s}}d_{h}^{D}\right)\right)\right\|^{2}\right].\]

This is a reweighted offline analog of Eq. 2 from the online setting, and a more general version is presented below with proof.

**Lemma C.1**.: _For \(g:\mathcal{S}\to\mathbb{R}^{p}\) and \(f:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{p}\) and reweighting function \(\rho:\mathcal{S}\times\mathcal{A}\to\mathbb{R}_{+}\), define the offline reweighted squared loss regression objective_

\[\widetilde{\mathcal{L}}_{h}(g;f,\rho)=\mathbb{E}_{\mathcal{D}_{h}}[\rho(s_{h}, a_{h})\|g(s_{h+1})-f(s_{h},a_{h})\|^{2}].\]

_Then for any such \(f\),_

\[\mathbf{E}_{h}^{D,\rho}(f)=\operatorname*{argmin}_{g:\mathcal{S}\to\mathbb{R}^ {p}}\widetilde{\mathcal{L}}_{h}(g;f,\rho).\]

_Further, for the smooth-clipped density ratio \(\widetilde{\rho}_{h}^{\pi}=\frac{\sigma\left(\widetilde{d}_{h}^{\pi}(s),C_{h}^ {\mathbf{a}}d_{h}^{D}(s)\right)}{d_{h}^{D}(s)}\frac{\widetilde{\pi}_{h}(a|s)}{ \pi_{h}^{D}(a|s)}\) and smooth-clipped target function \(y_{h}^{\pi}:=\nabla\log\pi\odot\mathbf{\tilde{1}}\left(\pi,C_{h}^{\mathbf{a}} d_{h}^{D}\right)+\nabla\log\widetilde{d}_{h}^{\pi}\odot\mathbf{\tilde{1}} \left(\widetilde{d}_{h}^{\pi},C_{h}^{\mathbf{a}}d_{h}^{D}\right)\) from Lem. 4.2, we have_

\[\nabla\log\widetilde{d}_{h+1}^{\pi}=\operatorname*{argmin}_{g:\mathcal{S}\to \mathbb{R}^{p}}\widetilde{\mathcal{L}}_{h}\left(g;y_{h}^{\pi},\widetilde{\rho}_{ h}^{\pi}\right).\]

Proof of Lem. B.2.: Since the objective is convex, can solve for the minimizer in closed form by taking the derivative and setting it to 0 in an element-wise manner. For each \(s^{\prime}\),

\[0=g(s^{\prime})\left(\sum_{s,a}P(s^{\prime}|s,a)\pi_{h}^{D}(a|s)d_{h}^{D}(s) \rho(s,a)\right)-\sum_{s,a}P(s^{\prime}|s,a)\pi_{h}^{D}(a|s)d_{h}^{D}(s)\rho(s,a)f (s,a).\]

Rearranging and using the definition of \(\mathbf{E}_{h}^{D,\rho}\) (Eq. 5) gives the result. The second statement follows from Lem. 4.2.

Proof of Prop. 4.3Part 1 follows from the gradient formula

\[\partial_{x}\sigma\left(x,c\right)=x^{-\beta-1}\left(x^{-\beta}+c^{-\beta} \right)^{-1/\beta-1}=\left(1+x^{\beta}c^{-\beta}\right)^{-1/\beta-1}.\]

It can be seen that \(\partial_{x}\sigma\left(x,c\right)\in[0,1]\) and is non-increasing in its inputs, thus \(\sigma\) is monotonic. Additionally, \(\left|\sigma\left(x,c\right)-\sigma\left(x^{\prime},c\right)\right|\leq\left|x -x^{\prime}\right|\). Since \(\sigma\) is symmetric in its arguments, we also have \(\left|\sigma^{\mathbf{s}}\left(x,c\right)-\sigma^{\mathbf{s}}\left(x,c^{ \prime}\right)\right|\leq\left|c-c^{\prime}\right|\).

Next, we prove Part 2. Let \(z=\left(x\wedge c\right)\), and observe that \(z-\sigma\left(x,c\right)\leq z-\sigma\left(z,z\right)\) since \(\sigma\) is monotonic. Further,

\[\frac{z-\sigma\left(z,z\right)}{z}=\frac{z-\left(2z^{-\beta}\right)^{-1/ \beta}}{z}=1-2^{-1/\beta}\leq 1-e^{-1/\beta}\leq\frac{1}{\beta}.\]

Rearranging and plugging in the expression for \(z\) gives the result.

Part 3 can be derived algebraically (but not easily), and is best intuited from the plot of the maximum slope \(\sup_{x,x^{\prime},c,c^{\prime}\in[0,1]}\left|\widetilde{\mathbf{1}}\left(x,c \right)-\widetilde{\mathbf{1}}\left(x^{\prime},c\right)\left|/\left|x-x^{ \prime}\right|\right.\) in Figure C.2, which corresponds to \(L_{\sigma}/c\) in the RHS of the bound. The left plot shows that the maximum slope increases linearly in \(\beta\), and the right plot shows it increases inversely with \(c\). The dashed red line is a "guess" for the exact constant \(L_{\sigma}/c=0.3\beta/c\), that upper-bounds the maximum slope. Clearly, \(L_{\sigma}=O(\beta)\).

Proof of Lem. 4.2Using the chain rule,

\[\nabla \widetilde{d}_{h}^{\tau}(s^{\prime})\] \[=\sum_{s,a}P(s^{\prime}|s,a)\left(\nabla\widetilde{\pi}_{h-1}(a|s )\widetilde{d}_{h-1}^{\tau}(s)+\pi(a|s)\nabla\sigma\left(\widetilde{d}_{h-1}^ {\tau}(s),C_{h-1}^{\mathbf{s}}d_{h-1}^{D}(s)\right)\right)\] \[=\sum_{s,a}P(s^{\prime}|s,a)\widetilde{\pi}_{h-1}(a|s)\sigma \left(\widetilde{d}_{h-1}^{\tau}(s),C_{h-1}^{\mathbf{s}}d_{h-1}^{D}(s)\right) \left(\nabla\log\widetilde{\pi}_{h-1}(a|s)\right.\] \[\left.\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad+\nabla\log\sigma\left(\widetilde{d}_{h-1}^{ \tau}(s),C_{h-1}^{\mathbf{s}}d_{h-1}^{D}(s)\right)\frac{}{}\right)\] \[=\sum_{s,a}P(s^{\prime}|s,a)\pi_{h-1}^{D}(a|s)d_{h-1}^{D}(s) \widetilde{\rho}_{h-1}^{\tau}(s,a)\Big{(}\nabla\log\widetilde{\pi}_{h-1}(a|s)\] \[\left.\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\qquad+\nabla\log\sigma\left(\widetilde{d}_{h-1 }^{\tau}(s),C_{h-1}^{\mathbf{s}}d_{h-1}^{D}(s)\right)\frac{}{}\Big{)},\]

where in the last line we use the definition of \(\widetilde{\rho}_{h-1}^{\tau}\) from Lem. 4.2 to make a change-of-measure. Further,

\[\nabla\log\sigma\left(\widetilde{d}_{h-1}^{\tau}(s),C_{h-1}^{ \mathbf{s}}d_{h-1}^{D}(s)\right) =\nabla\widetilde{d}_{h-1}^{\tau}(s)\odot\partial_{x}\sigma\left( \widetilde{d}_{h-1}^{\tau}(s),C_{h-1}^{\mathbf{s}}d_{h-1}^{D}(s)\right)\] \[=\nabla\log\widetilde{d}_{h-1}^{\tau}(s)\odot\left(\widetilde{d}_ {h-1}^{\tau}(s)\cdot\partial_{x}\sigma\left(\widetilde{d}_{h-1}^{\tau}(s),C_{h- 1}^{\mathbf{s}}d_{h-1}^{D}(s)\right)\right)\]\[=\nabla\log\widetilde{d}_{h-1}^{\pi}(s)\odot\mathbf{\tilde{1}}\left( \widetilde{d}_{h-1}^{\pi}(s),C_{h-1}^{\mathbf{s}}d_{h-1}^{D}(s)\right),\]

by definition. We can make the analogous statement for \(\nabla\log\widetilde{\pi}_{h-1}\). Next, using the same change of measure in Def. 4.3, we have

\[\widetilde{d}_{h}^{\pi}(s^{\prime})=\sum_{s,a}P(s^{\prime}|s,a)\pi_{h-1}^{D}( a|s)d_{h-1}^{D}(s)\widetilde{\rho}_{h-1}^{\pi}(s,a).\]

The lemma statement follows from \(\nabla\log\widetilde{d}_{h}^{\pi}(s^{\prime})=\nabla\widetilde{d}_{h}^{\pi}(s ^{\prime})/\widetilde{d}_{h}^{\pi}(s^{\prime})\), and the definition of \(\mathbf{E}_{h-1}^{D,\widetilde{\rho}}\) in Eq. (5). The gradient magnitude bound is proved in Lem. C.2.

**Lemma C.2** (Bounded gradient magnitude).: _Suppose \(\sigma\) is differentiable almost everywhere. Under Part 1 of Asm. 4.1 and Asm. 2.1,_

\[\max_{h,s}\left\|\nabla\log\widetilde{d}_{h}^{\pi}(s)\right\|_{\infty}\leq hG.\]

Proof of Lem. C.2.: As a consequence of Asm. 4.1, which states that the gradient of \(\sigma\) is nonincreasing in the first argument, for any \(x,c\geq 0\) we have

\[\sigma\left(x,c\right)=\int_{0}^{x}\;\partial_{x}\sigma^{\mathbf{s}}\left(z, c\right)dz\geq\int_{0}^{x}\;\partial_{x}\sigma^{\mathbf{s}}\left(x,c\right)dz=x \;\partial_{x}\sigma\left(x,c\right)\]

Then substituting \(x\leftarrow\widetilde{d}_{h-1}^{\pi}(s)\) and \(c\gets d_{h-1}^{D}(s)\), the above shows that \(\nabla\log\sigma\left(\widetilde{d}_{h-1}^{\pi}(s),d_{h-1}^{D}(s)\right) \leq\nabla\log\widetilde{d}_{h-1}^{\pi}\) pointwise. Since Lem. 4.2 involves a valid (conditional) expectation, for any \(s\in\mathcal{S}\) we have

\[\left\|\nabla\log\widetilde{d}_{h}^{\pi}(s^{\prime})\right\|_{\infty}\] \[\leq G+\max_{s}\left\|\nabla\log\sigma\left(\widetilde{d}_{h-1}^ {\pi}(s),C_{h-1}^{\mathbf{s}}d_{h-1}^{D}(s)\right)\right\|_{\infty}\] \[\leq hG\]

where we use Asm. 2.1 in the second line, and unroll the same inequalities through levels in the last line. 

Proof of Prop. 4.4.: First, we bound the difference between the soft-clipped and clipped density functions:

\[\left\|\widetilde{d}_{h}^{\pi}-\widetilde{d}_{h}^{\pi}\right\|_{1}\leq\; \left\|\sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s}}d_{h-1}^{D} \right)-\left(\widetilde{d}_{h-1}^{\pi}\wedge C_{h-1}^{\mathbf{s}}d_{h-1}^{D} \right)\right\|_{1}+\max_{s}\left\|\widetilde{\pi}_{h-1}(\cdot|s)-\bar{\pi}_{ h-1}(\cdot|s)\right\|_{1}\]

For the second term and any \(s\),

\[\left\|\widetilde{\pi}_{h-1}(\cdot|s)-\bar{\pi}_{h-1}(\cdot|s) \right\|_{1} =\left\|\sigma\left(\pi_{h-1}(\cdot|s),C_{h-1}^{\mathbf{s}}\pi_{ h-1}^{D}(\cdot|s)\right)-\left(\pi_{h-1}(\cdot|s)\wedge C_{h-1}^{\mathbf{s}}\pi_{ h-1}^{D}(\cdot|s)\right)\right\|_{1}\] \[\leq D_{\sigma}\left\|\left(\widetilde{d}_{h-1}^{\pi}\wedge C_{h-1} ^{\mathbf{s}}d_{h-1}^{D}\right)\right\|_{1}+\left\|\widetilde{d}_{h-1}^{\pi}- \widetilde{d}_{h-1}^{\pi}\right\|_{1}\]

where in the last line we use Asm. 4.1 to upper bound the first term, and the properties of the pointwise minimum \(\wedge\) to upper bound the second term. Since \(\left(\widetilde{d}_{h-1}^{\pi}\wedge C_{h-1}^{\mathbf{s}}d_{h-1}^{D}\right) \leq\widetilde{d}_{h-1}^{\pi}\leq d_{h-1}^{\pi}\), we have

\[\left\|\widetilde{d}_{h}^{\pi}-\widetilde{d}_{h}^{\pi}\right\|_{1}\leq 2D_{ \sigma}+\left\|\widetilde{d}_{h-1}^{\pi}-\overline{d}_{h-1}^{\pi}\right\|_{1} \leq h(D_{\sigma}+D_{\sigma})\]after rolling out the induction. Then for any \(\pi\),

\[\widetilde{J}(\pi)-\widetilde{J}(\pi)\leq\sum_{h=1}^{H}\left\|\widetilde{d}_{h}^{ \pi}-\widetilde{d}_{h}^{\pi}\right\|_{1}\leq 2H^{2}D_{\sigma}\]

Lastly, let \(\widetilde{\pi}^{*}=\operatorname*{argmax}_{\pi\in\Pi_{\Theta}}\widetilde{J}(\pi)\), and define \(\bar{\pi}^{*}\) similarly. Then

\[\bar{J}(\bar{\pi}^{*})-\widetilde{J}(\widetilde{\pi}^{*})\leq\bar{J}(\bar{\pi} ^{*})-\widetilde{J}(\bar{\pi}^{*})\leq 2H^{2}D_{\sigma}.\]

### Proofs for Sec. 4.3

First, we give an example of \(\mathcal{G}\) that satisfies Asm. 4.2 in low-rank MDPs.

**Proposition C.1**.: _Suppose \(\mathcal{M}\) is a low-rank MDP (Def. B.1), and suppose \(\mu\) is known. For each layer \(h\), define the function class_

\[\mathcal{G}_{h}=\left\{g_{h}=\frac{\mu^{\top}\Psi}{\mu^{\top}\psi}:\Psi\in \mathbb{R}^{k\times\mathfrak{p}},\psi\in\mathbb{R}^{k},\|g_{h}\|_{\infty}\leq hG,\;\forall h\in[H]\right\}.\]

_Then \(\{\mathcal{G}_{h}\}\) satisfies Asm. 4.2 and has pseudodimension (Def. F.1) \(\mathsf{d}_{\mathcal{G}_{h}}=O(k\mathfrak{p})\)._

Proof of Prop. C.1.: It suffices to show that, for any \(f:\mathcal{S}\times\mathcal{A}\to\mathbb{R}^{\mathfrak{p}}\), reweighting function \(\rho:\mathcal{S}\times\mathcal{A}\to\mathbb{R}_{+}\), and \(h\in[H]\), the gradient update in Lem. 4.2 has \([\mathbf{E}_{h}^{D,\rho}f]\in\mathcal{G}_{h+1}\).

Fix \(\rho,f,\) and \(h\). From the definition of \(\mathbf{E}_{f}^{\rho}\), we have

\[[\mathbf{E}_{h}^{\rho}f](s^{\prime})=\frac{\sum_{s,a}P(s^{\prime}|s,a)\pi_{h}^ {D}(a|s)d_{h}^{D}(s,a)\rho(s,a)f(s,a)}{\sum_{s,a}P(s^{\prime}|s,a)\pi_{h}^{D}( a|s)d_{h}^{D}(s,a)\rho(s,a)}\]

Then since \(P(s^{\prime}|s,a)=\langle\phi(s,a),\mu(s^{\prime})\rangle\), we can apply the same steps as the proof of Prop. B.1 to show that there exists \(\Psi\in\mathbb{R}^{k\times\mathfrak{p}}\) and \(\psi\in\mathbb{R}^{k}\) such that

\[[\mathbf{E}_{h}^{\rho}f](s^{\prime})=\frac{\mu(s^{\prime})^{\top}\Psi}{\mu(s^ {\prime})^{\top}\psi},\;\forall s^{\prime}\in\mathcal{S}.\]

Specifically, \(\psi=\sum_{s,a}\phi(s,a)\pi_{h}^{D}(a|s)d_{h}^{D}(s,a)\rho(s,a)\), and the \(p\)-th column of \(\Psi\) is \(\Psi^{p}=\sum_{s,a}\phi(s,a)\pi_{h}^{D}(a|s)d_{h}^{D}(s,a)\rho(s,a)f^{p}(s,a)\). 

Proof of Thm. 4.1.: For the remainder of this section, we define the constants \(\varepsilon^{\text{w}}\) and \(\varepsilon^{\text{mle}}\) to be the estimation errors of \(\widetilde{w}^{\pi}\) and \(d^{D}\), respectively, such that for a given \(\pi\) and any \(h\in[H]\) we have

\[\left\|\widehat{w}_{h}^{\pi}-\widetilde{w}_{h}^{\pi}\right\|_{1, d_{h-1}^{D,\dagger}}\leq\varepsilon^{\text{w}}\] \[\left\|\widehat{d}_{h}^{D}-d_{h}^{D}\right\|_{1}\leq\varepsilon^{ \text{mle}}\quad\mathrm{and}\quad\left\|\widehat{d}_{h}^{D,\dagger}-d_{h}^{D, \dagger}\right\|_{1}\leq\varepsilon^{\text{mle}}.\]

We can obtain such estimates using Alg. 4 and Alg. 5, and a direct application of Lem. D.1 with union bound gives \(\varepsilon^{\text{mle}}=O\left(\sqrt{\frac{\log(H|\mathcal{F}|/\delta)}{n}}\right)\), and similarly Thm. E.1 states that \(\varepsilon^{\text{wreg}}=O\left(\sqrt{\frac{\log(H|\mathcal{W}|/\delta)}{n}}\right)\).

Next, recall that

\[\widehat{\nabla}\widetilde{J}(\pi)=\frac{1}{|\mathcal{D}_{h}^{\text{grad}}|} \sum_{h=0}^{H-1}\sum_{(s,a,s^{\prime})\in\mathcal{D}_{h}^{\text{grad}}} \widehat{w}_{h}^{\pi}(s^{\prime})R_{h}(s^{\prime})\widehat{g}_{h}^{\pi}(s^{ \prime}).\]

The expected value over draws of \(\mathcal{D}^{\text{grad}}\) is

\[\mathbb{E}_{\mathcal{D}_{h}^{\text{grad}}}\left[\widehat{\nabla}\widetilde{J}( \pi)\right]=\sum_{h}\mathbb{E}_{s^{\prime}\sim d_{h-1}^{D,\dagger}}\left[ \widehat{w}_{h}^{\pi}(s^{\prime})R_{h}(s^{\prime})\widehat{g}_{h}^{\pi}(s^{ \prime})\right].\]First we bound the statistical error from using samples to approximate \(\nabla\widetilde{J}(\pi)\), given the gradient estimate. Fix the other datasets, then

\[\left\|\widehat{\nabla}\widetilde{J}(\pi)-\mathbb{E}_{\mathcal{D}^{ \mathrm{grad}}}\left[\widehat{\nabla}\widetilde{J}(\pi)\right]\right\|\] \[\leq\sqrt{\mathfrak{p}}\max_{p\in[\mathfrak{p}]}\sum_{h}\left| \widehat{\mathbb{E}}_{s^{\prime}\sim d_{h-1}^{D,\dagger}}\left[\widehat{w}_{ h}^{\tau}(s^{\prime})R_{h}(s^{\prime})\widehat{g}_{h}^{\pi}(s^{\prime}) \right]-\mathbb{E}_{s^{\prime}\sim d_{h-1}^{D,\dagger}}\left[\widehat{w}_{h}^{ \pi}(s^{\prime})R_{h}(s^{\prime})\widehat{g}_{h}^{\pi}(s^{\prime})\right]\right|\] \[\leq\sqrt{\mathfrak{p}}\left(\sum_{h}C_{h}^{\mathbf{s}}C_{h}^{ \mathbf{a}}\right)\max_{p\in[\mathfrak{p}],h\in[H]}\left|\widehat{\mathbb{E}} _{s^{\prime}\sim d_{h-1}^{D,\dagger}}\left[\widehat{g}_{h}^{\pi}(s^{\prime}) \right]-\mathbb{E}_{s^{\prime}\sim d_{h-1}^{D,\dagger}}\left[\widehat{g}_{h}^ {\pi}(s^{\prime})\right]\right|\] \[\leq\sqrt{\mathfrak{p}}\left(\sum_{h}C_{h}^{\mathbf{s}}C_{h}^{ \mathbf{a}}\right)\varepsilon^{\mathrm{stat}}\] (12)

where \(\varepsilon^{\mathrm{stat}}=HG\sqrt{\frac{\log(8\mathfrak{p}H/\delta)}{2n}}\) is obtained by using Hoeffding's with \(\delta^{\prime}=\delta/4\), since the randomness in \(\widehat{w}\) and \(\widehat{g}\) are fixed. Then for any \(p\in[\mathfrak{p}]\),

\[\left\|\mathbb{E}_{\mathcal{D}^{\mathrm{grad}}_{h}}\left[\widehat {\nabla}\widetilde{J}(\pi)\right]-\nabla\widetilde{J}(\pi)\right\|\] \[\leq\] \[\leq \sum_{h}\left\|\sum_{s^{\prime}}d_{h-1}^{D,\dagger}(s^{\prime})R_ {h}(s^{\prime})w_{h}^{\pi}(s^{\prime})\left(\widehat{g}_{h}^{\pi}(s^{\prime}) -\nabla\log\widetilde{d}_{h}^{\pi}(s^{\prime})\right)\right\|+\left\|\sum_{s^ {\prime}}d_{h-1}^{D,\dagger}(s^{\prime})R_{h}(s^{\prime})\widehat{g}_{h}^{ \pi}(s^{\prime})\left(\hat{w}_{h}^{\pi}(s^{\prime})-w_{h}^{\pi}(s^{\prime}) \right)\right\|\] \[\leq \sqrt{\mathfrak{p}}\sum_{h}\left(hG\left\|\widehat{w}_{h}^{\pi}- w_{h}^{\pi}\right\|_{1,d_{h-1}^{D,\dagger}}+\max_{p\in[\mathfrak{p}]}\left\| \widehat{g}_{h}^{\pi}\right\|_{1,\widetilde{d}_{h}^{\pi}}\right)\] (13)

The first term is bounded by Thm. E.1. For the second term, we use the following decomposition, which is proved at the end of this section.

**Lemma C.3** (Gradient estimation error decomposition).: _Let \(\varepsilon^{\mathrm{mle}}\) and \(\varepsilon^{\mathrm{w}}\) be such that for all \(h\in[h]\) and \(\pi\in\Pi_{\Theta}\), we have_

\[\|\widehat{d}_{h}^{D}-d_{h}^{D}\|_{1},\|\widehat{d}_{h}^{D,\dagger}-d_{h}^{D, \dagger}\|_{1}\leq\varepsilon_{h}^{\mathrm{mle}}\quad\text{and}\quad\| \widehat{w}_{h}^{\pi}-\widehat{w}_{h}^{\pi}\|_{1,d_{h-1}^{D,\dagger}}\leq \varepsilon_{h}^{\mathrm{w}}.\]

_Then under Asm. 2.1 and Asm. 4.1, for any \(p\in[\mathfrak{p}]\), \(\widehat{g}_{h}^{\pi}\) from Alg. 2 satisfies_

\[\left\|\widehat{g}_{h}^{\pi,p}-\nabla^{p}\log\widetilde{d}_{h}^{ \pi}\right\|_{1,\widetilde{d}_{h}^{\pi}} \leq 6hC_{h-1}^{\mathbf{C}}C_{h-1}^{\mathbf{a}}L_{\sigma}G\; \varepsilon_{h-1}^{\mathrm{mle}}\] _(data distribution estimation error)_ \[+3hL_{\sigma}G\;\varepsilon_{h-1}^{\mathrm{w}}\] _(occupancy estimation error)_ \[+ \left\|\widehat{g}_{h}^{\pi,p}-[\mathbf{E}_{h-1}^{D,\widehat{p}}( \nabla\log\widetilde{\pi}_{h-1}+\widehat{g}_{h-1})]^{p}\right\|_{1,\widetilde{ d}_{h}^{\pi}}\] _(statistical regression error)_ \[+ \left\|\widehat{g}_{h-1}^{\pi,p}-\nabla^{p}\log\widetilde{d}_{h-1 }^{\widetilde{\pi}}\right\|_{1,\widetilde{d}_{h-1}^{\widetilde{\pi}}}\] _(recursive term)_

From Lem. C.4, we have

\[\left\|\mathbf{E}_{h-1}^{D,\widehat{p}}\widehat{g}_{h-1}]^{p}-\widehat{g}_{h}^{ \pi,p}\right\|_{1,\widetilde{d}_{h}^{\pi}}\leq\sqrt{2\left(1+C_{h-1}^{ \mathbf{a}}\varepsilon_{h-1}^{\mathrm{mle}}\right)}\varepsilon_{h}^{\mathrm{ reg}}+2hG\left(2C_{h-1}^{\mathbf{a}}\varepsilon_{h-1}^{\mathrm{mle}}+ \varepsilon_{h-1}^{\mathrm{w}}\right)\]

where \(\varepsilon_{h}^{\mathrm{reg}}=O(\sqrt{\frac{\mathfrak{d}_{\Theta}C_{h-1}^{ \mathbf{a}}C_{h-1}^{\mathbf{a}}h^{2}G^{2}\log(n\mathfrak{p}H/\delta)}{n}})\). Then plugging the above into the decomposition in Lem. C.3, we have

\[\left\|\widehat{g}_{h}^{p}-\nabla^{p}\log\widetilde{d}_{h}^{ \widetilde{\pi}}\right\|_{1,\widetilde{d}_{h}^{\pi}} \leq 10C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}}hGL_{\sigma}\; \varepsilon_{h-1}^{\mathrm{mle}}+5hGL_{\sigma}\;\varepsilon_{h-1}^{\mathrm{w}}\] \[+\sqrt{2\left(1+C_{h-1}^{\mathbf{a}}\varepsilon_{h-1}^{\mathrm{mle }}\right)}\varepsilon_{h}^{\mathrm{reg}}+\left\|\widehat{g}_{h-1}^{p}-\nabla^{p} \log\widetilde{d}_{h-1}^{\widetilde{\pi}}\right\|_{1,\widetilde{d}_{h-1}^{ \widetilde{\pi}}}\]Unrolling through timesteps, we have

\[\left\|\widehat{g}_{h}^{p}-\nabla^{p}\log\widetilde{d}_{h}^{\pi} \right\|_{1,\widetilde{d}_{h}^{\pi}} \leq 10h^{2}GL_{\sigma}\sum_{g<h}C_{g}^{\mathsf{s}}C_{g}^{\mathsf{a} }\varepsilon_{g}^{\mathrm{mle}}+5h^{2}GL_{\sigma}\sum_{g<h}\varepsilon_{g}^{ \mathrm{w}}+\sum_{g\leq h}\sqrt{2(1+C_{g}^{\mathsf{s}}\varepsilon_{g}^{ \mathrm{mle}})}\varepsilon_{g}^{\mathrm{reg}}\] \[\leq 10H^{2}GL_{\sigma}\left(\sum_{h}C_{h}^{\mathsf{s}}C_{h}^{ \mathsf{a}}\right)\varepsilon^{\mathrm{mle}}+5H^{3}GL_{\sigma}\varepsilon_{H}^{ \mathrm{w}}+\sum_{h}\varepsilon_{h}^{\mathrm{reg}}\left(\sqrt{2}+C_{h}^{ \mathsf{s}}\varepsilon^{\mathrm{mle}}\right)\]

Since \(\varepsilon_{H}^{\mathrm{w}}\leq(\sum_{h}C_{h}^{\mathsf{s}}C_{h}^{\mathsf{a} }+2\sum_{h}C_{h}^{\mathsf{s}})\,\varepsilon^{\mathrm{mle}}+\sqrt{2}\,(\sum_{ h}C_{h}^{\mathsf{s}}C_{h}^{\mathsf{n}})\,\varepsilon^{\mathrm{wreg}}\),

\[\leq 25H^{3}GL_{\sigma}\left(\sum_{h}C_{h}^{\mathsf{s}}C_{h}^{ \mathsf{a}}\right)\varepsilon^{\mathrm{mle}}+5\sqrt{2}H^{3}GL_{\sigma}\left( \sum_{h}C_{h}^{\mathsf{s}}C_{h}^{\mathsf{a}}\right)\varepsilon^{\mathrm{wreg}} +\sum_{h}\varepsilon_{h}^{\mathrm{reg}}\left(\sqrt{2}+C_{h}^{\mathsf{s}} \varepsilon^{\mathrm{mle}}\right)\]

Finally, combining with Eq. (12) and upper bounding \(\varepsilon^{\mathrm{reg}}\) further, we have

\[\left\|\nabla\widetilde{J}(\pi)-\widehat{\nabla}\widetilde{J}( \pi)\right\|\] \[\leq\sqrt{\mathsf{p}}\left(\sum_{h}C_{h}^{\mathsf{s}}C_{h}^{ \mathsf{a}}\right)\left(\varepsilon^{\mathrm{stat}}+25H^{3}GL_{\sigma} \varepsilon^{\mathrm{mle}}+5\sqrt{2}H^{3}GL_{\sigma}\varepsilon^{\mathrm{wreg }}\right)+\sqrt{\mathsf{p}}\left(H\sqrt{2}+\varepsilon^{\mathrm{mle}}\sum_{h}C _{h}^{\mathsf{s}}\right)\left(\max_{h}\varepsilon_{h}^{\mathrm{reg}}\right),\]

Combining inequalities and plugging in the expression for each \(\varepsilon\), we have

\[\left\|\nabla\widetilde{J}(\pi)-\widehat{\nabla}\widetilde{J}(\pi)\right\| \lesssim c\sqrt{\frac{\mathrm{d}_{\mathsf{\mathsf{\mathsf{\mathsf{\mathsf{\mathsf{ \mathsf{\mathsf{\mathsf{\mathsf{\mathsf{\mathsf{\mathsf{\mathsf{\mathsf{ \mathsfmathsfmathsfmathsfmathsfmathsfmathsfmathsfmathsfmathsfmathsfmathsfmathsf    }}}}}}}}}}}}}}}} \left \left(\mathop{\mathop{\mathop{\mathop{\rmrmrmrmrmrm}}}}}} \nolimits\kern-1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt \kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt \kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt \kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt \kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt \kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt \kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt \kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt \kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt\kern 1.0pt \kern 1.0pt\kern 1.

We will bound the second term above. Letting \(y_{h-1}^{\pi}:=\nabla\log\sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s} }d_{h-1}^{D}\right)\) be the (true) regression target and using the gradient Bellman equation for \(\nabla\log\widetilde{d}_{h}^{\pi}\) in Lem. 4.2, we have

\[\|\zeta_{h}^{\pi,p}-\nabla^{p}\widetilde{d}_{h}^{\pi}\|_{1}\] \[= \left\|\mathbf{E}_{h-1}^{\widehat{p}}\left(\nabla^{p}\log\widetilde {\pi}+\widehat{y}_{h-1}^{\pi,p}\right)-\mathbf{E}_{h-1}^{\widetilde{p}^{ \pi}}\left(\nabla^{p}\log\widetilde{\pi}+y_{h-1}^{\pi,p}\right)\right\|_{1}\] \[\leq \left\|\frac{\sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{ \mathsf{s}}\widetilde{d}_{h-1}^{D}\right)\widetilde{\pi}_{h-1}}{\pi_{h-1}^{D }}d_{h-1}^{D}\pi_{h-1}^{D}\right.\left.\left(\nabla^{p}\log\widetilde{\pi}+ \widehat{y}_{h-1}^{\pi,p}\right)-\sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1 }^{\mathsf{s}}d_{h-1}^{D}\right)\widetilde{\pi}_{h-1}\left(\nabla^{p}\log \widetilde{\pi}+y_{h-1}^{\pi,p}\right)\right\|_{1}\] \[\leq \left\|\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}} \widetilde{d}_{h-1}^{D}\right)\left(\nabla^{p}\log\widetilde{\pi}+\widehat{y} _{h-1}^{\pi,p}\right)-\sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{ s}}d_{h-1}^{D}\right)\left(\nabla^{p}\log\widetilde{\pi}+y_{h-1}^{\pi,p}\right) \right\|_{1}\] \[\qquad+C_{h-1}^{\mathsf{s}}\left(G+\left\|\widehat{y}_{h-1}^{\pi, p}\right\|_{\infty}\right)\,\|d_{h-1}^{D}-\widehat{d}_{h-1}^{D}\|_{1}\] \[\leq \left\|\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}} \widehat{d}_{h-1}^{D}\right)\widetilde{y}_{h-1}^{\pi,p}-\sigma\left(\widetilde {d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D}\right)y_{h-1}^{\pi,p}\right\| _{1}\] \[\leq \left\|\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}} \widehat{d}_{h-1}^{D}\right)\widetilde{y}_{h-1}^{\pi,p}-\sigma\left( \widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D}\right)y_{h-1}^{\pi,p }\right\|_{1}\] \[\qquad+G\|\widehat{d}_{h-1}^{\pi}-\widetilde{d}_{h-1}^{\pi}\|_{1} +C_{h-1}^{\mathsf{s}}\left(2G+\left\|\mathcal{G}_{h-1}\right\|_{\infty}\right) \,\|d_{h-1}^{D}-\widehat{d}_{h-1}^{D}\|_{1}\] (15)

Now consider the first term above, where

\[\widetilde{y}_{h-1}^{\pi}=\widehat{g}_{h-1}^{\pi}\odot\mathbf{\tilde{1}}\left( \widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}\widehat{d}_{h-1}^{D}\right)\quad \text{and}\quad y_{h-1}^{\pi}=\nabla\log d_{h-1}^{\pi}\odot\mathbf{\tilde{1}} \left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D}\right).\]

Then plugging this into the first line from the previous block, we have

\[\left\|\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}} \widehat{d}_{h-1}^{D}\right)\widetilde{y}_{h-1}^{\pi,p}-\sigma\left(\widetilde{ d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D}\right)y_{h-1}^{\pi,p}\right\|_{1}\] \[= \left\|\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}} \widehat{d}_{h-1}^{D}\right)\mathbf{\tilde{1}}\left(\widetilde{d}_{h-1}^{ \pi},C_{h-1}^{\mathsf{s}}\widehat{d}_{h-1}^{D}\right)\widehat{y}_{h-1}^{\pi,p}- \sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D}\right) \mathbf{\tilde{1}}\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}d_{h-1} ^{D}\right)\nabla^{p}\log d_{h-1}^{\pi}\right\|_{1}\] \[\leq \left\|\widehat{y}_{h-1}^{\pi,p}-\nabla^{p}\log d_{h-1}^{\pi}\right\| _{1,\widetilde{d}_{h-1}^{\pi}}\] \[\qquad+\|\mathcal{G}_{h-1}\|_{\infty}\left\|\sigma\left(\widehat{d }_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}\widehat{d}_{h-1}^{D}\right)\mathbf{\tilde{1}} \left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}\widehat{d}_{h-1}^{D} \right)-\sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D} \right)\mathbf{\tilde{1}}\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}} d_{h-1}^{D}\right)\right\|_{1},\] (16)

where we add and subtract \(\sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D}\right) \mathbf{\tilde{1}}\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{ D}\right)\widehat{g}_{h-1}^{\pi,p}\) to obtain the inequality. The first error is the recursive term, so it remains to bound the second, for which we will use the smoothness properties of \(\mathbf{\tilde{1}}\left(x,c\right)\) from creftype 4.1.

\[\left\|\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}} \widehat{d}_{h-1}^{D}\right)\mathbf{\tilde{1}}\left(\widehat{d}_{h-1}^{\pi},C_{h -1}^{\mathsf{s}}\widehat{d}_{h-1}^{D}\right)-\sigma\left(\widetilde{d}_{h-1}^{ \pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D}\right)\mathbf{\tilde{1}}\left(\widetilde{d}_ {h-1}^{\pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D}\right)\right\|_{1}\] \[\leq \left\|\sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}} d_{h-1}^{D}\right)\left(\mathbf{\tilde{1}}\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{ \mathsf{s}}d_{h-1}^{D}\right)-\mathbf{\tilde{1}}\left(\widetilde{d}_{h-1}^{ \pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D}\right)\right)\right\|_{1}\] \[\qquad+\left\|\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{ \mathsf{s}}\widehat{d}_{h-1}^{D}\right)\left(\mathbf{\tilde{1}}\left(\widehat{d}_ {h-1}^{\pi},C_{h-1}^{\mathsf{s}}\widehat{d}_{h-1}^{\mathsf{s}}\right)-\mathbf{ \tilde{1}}\left(\widehat{d}_{h-1}^{\mathsf{s}},C_{h-1}^{\mathsf{s}}d_{h-1}^{D} \right)\right)\right\|_{1}\] \[\qquad+\left\|\left(\sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h -1}^{\mathsf{s}}d_{h-1}^{D}\right)-\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{ \mathsf{s}}\widehat{d}_{h-1}^{D}\right)\right)\mathbf{\tilde{1}}\left( \widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D}\right)\right\|_{1}\] \[\leq L_{\sigma}\left\|\frac{\sigma\left(\widetilde{d}_{h-1}^{ \pi},C_{h-1}^{\mathsf{s}}d_{h-1}^{D}\right)}{C_{h-1}^{\mathsf{s}}\widehat{d}_{h-1}^{D} \right)\left(\widetilde{d}_{h-1}^{\pi}-\widetilde{d}

Finally, after combining Eq. (14), Eq. (15), Eq. (16), Eq. (17), and Eq. (18), we have

\[\left\|\nabla^{p}\log\widetilde{d}_{h}^{\pi}-\widehat{g}_{h}^{\pi,p} \right\|_{1,\widetilde{d}_{h}^{\pi}} \leq\,\left\|\widehat{g}_{h-1}^{\pi,p}-\nabla^{p}\log d_{h-1}^{\pi }\right\|_{1,\widetilde{d}_{h-1}^{\pi}}\] \[+\,\left\|\zeta_{h}^{\pi,p}-\widehat{g}_{h}^{\pi,p}\right\|_{1, \widetilde{d}_{h}^{\pi}}\] \[+\,\left(G+(L_{\sigma}+1)\|\mathcal{G}_{h-1}\|_{\infty}+\| \mathcal{G}_{h}\|_{\infty}\right)\,\left\|\widehat{w}_{h-1}^{\pi}-\widetilde {w}_{h-1}^{\pi}\right\|_{1,d_{h-1}^{0,\dagger}}\] \[+\,C_{h-1}^{\mathsf{s}}C_{h-1}^{\mathsf{a}}\left(G+(L_{\sigma}+1 )\|\mathcal{G}_{h-1}\|_{\infty}+\|\mathcal{G}_{h}\|_{\infty}\right)\left\| \widetilde{d}_{h-1}^{D,\dagger}-d_{h-1}^{D,\dagger}\right\|_{1}\] \[+\,\left(2C_{h-1}^{\mathsf{s}}G+(L_{\sigma}+C_{h-1}^{\mathsf{s}} )\|\mathcal{G}_{h-1}\|_{\infty}+\|\mathcal{G}_{h}\|_{\infty}\right)\,\left\| \widetilde{d}_{h-1}^{D}-d_{h-1}^{D}\right\|_{1}\]

Plugging in \(\|\mathcal{G}_{h}\|_{\infty}\leq hG\) and consolidating terms, gives the result.

**Lemma C.4**.: _With probability \(\geq 1-\delta\), for all \(h\in[H]\) and a fixed \(\pi\) we have_

\[\left\|\widehat{g}_{h}^{\pi}-\mathbf{E}_{h-1}^{D,\widetilde{\rho }^{\pi}}(\nabla\log\widetilde{\pi}_{h-1}+\widehat{g}_{h-1}^{\pi})\right\|_{1, \widetilde{d}_{h}^{\pi}}\leq\sqrt{2\left(1+C_{h-1}^{\mathsf{s}}\varepsilon_{ h-1}^{\mathrm{mele}}\right)}\varepsilon_{h}^{\mathrm{reg}}+2hG\left(2C_{h-1}^{ \mathsf{s}}\varepsilon_{h-1}^{\mathrm{mele}}+\varepsilon_{h-1}^{\mathsf{w}}\right)\] (19)

Proof.: Let \(f_{h}^{\pi}(s^{\prime})=\sum_{s,a}P(s^{\prime}|s,a)\widehat{\rho}^{\pi}(s,a)d_ {h-1}^{D}(s)\pi^{D}(a|s)\) be the data distribution reweighted by \(\widehat{\rho}^{\pi}\). For short, we use \(y_{h}^{\pi,p}=[\mathbf{E}_{h-1}^{D,\widetilde{\rho}^{\pi}}(\nabla\log \widetilde{\pi}_{h-1}+\widehat{g}_{h-1}^{\pi})]^{p}\). For any \(p\in[\mathsf{p}]\),

\[\left\|\widehat{g}_{h}^{\pi,p}-y_{h}^{\pi,p}\right\|_{1,\widetilde {d}_{h}^{\pi}} \leq\,\left\|\widehat{g}_{h}^{\pi,p}-y_{h}^{\pi,p}\right\|_{1,f_{h}^ {\pi}}+\left\|\widehat{g}_{h}^{\pi,p}-y_{h}^{\pi,p}\right\|_{\infty}\cdot\left\| f_{h}^{\pi}-\widetilde{d}_{h}^{\pi}\right\|_{1}\] \[\leq\,\left\|\sqrt{f_{h}^{\pi}}\right\|_{2}\cdot\left\|\widehat{g }_{h}^{\pi,p}-y_{h}^{\pi,p}\right\|_{2,f_{h}^{\pi}}+2hG\left\|f_{h}^{\pi}- \widetilde{d}_{h}^{\pi}\right\|_{1},\]

where in the second line we use Cauchy-Schwarz on the first term and Lem. C.2 on the second term. Consider the first term. One can loosely bound

\[\left\|\sqrt{f_{h}^{\pi}}\right\|_{2}^{2} =\,\sum_{s,a,s^{\prime}}P(s^{\prime}|s,a)\widehat{\rho}_{h-1}^{ \pi}(s,a)\pi_{h-1}^{D}(a|s)d_{h-1}^{D}(s)\] \[\leq C_{h-1}^{\mathsf{s}}\sum_{s,a,s^{\prime}}P(s^{\prime}|s,a) \widetilde{\pi}_{h-1}(a|s)d_{h-1}^{D}(s)\leq C_{h-1}^{\mathsf{s}},\]

or a get a tighter result with

\[\left\|\sqrt{f_{h}^{\pi}}\right\|_{2}^{2} =\,\sum_{s,a,s^{\prime}}P(s^{\prime}|s,a)\widehat{\rho}_{h-1}^{ \pi}(s,a)\pi_{h-1}^{D}(a|s)d_{h-1}^{D}(s)\] \[=\,\sum_{s,a,s^{\prime}}P(s^{\prime}|s,a)\frac{\sigma\left( \widetilde{d}_{h-1}^{\pi}(s),C_{h-1}^{\mathsf{s}}d_{h-1}^{D}(s)\right)}{ \widetilde{d}_{h}^{D}(s)}\widetilde{\pi}_{h-1}(a|s)\left(d_{h-1}^{D}(s)- \widetilde{d}_{h-1}^{D}(s)\right)\] \[+\,\sum_{s,a,s^{\prime}}P(s^{\prime}|s,a)\widetilde{\pi}_{h-1}(a |s)\sigma\left(\widetilde{d}_{h-1}^{\pi}(s),C_{h-1}^{\mathsf{s}}d_{h-1}^{D}(s)\right)\] \[\leq C_{h-1}^{\mathsf{s}}\left\|d_{h-1}^{D}-\widetilde{d}_{h-1}^{D }\right\|_{1}+1\]

Next we bound \(\left\|\widehat{g}_{h}^{\pi,p}-y_{h}^{\pi,p}\right\|_{2,f_{h}^{\pi}}\). Define

\[\mathcal{L}_{h-1}^{p}(g;y,\rho)=\widehat{\mathbb{E}}_{(s,a,s^{\prime})\sim \mathcal{D}_{h-1}}\left[\rho(s,a)\left(g^{p}(s^{\prime})-\left(\nabla\log \widetilde{\pi}_{h-1}(a|s)+y^{p}(s)\right)\right)^{2}\right]\]

to be the \(p\)-th parameter version of Eq. (10). Recall the regression target \(\widehat{y}_{h-1}^{\pi}\), then we have

\[\left\|\widehat{g}_{h}^{\pi,p}-y_{h}^{\pi,p}\right\|_{2,f_{h}^{ \pi}}^{2} =\mathbb{E}\left[\mathcal{L}_{h-1}^{p}(\widehat{g}_{h}^{\pi}; \widehat{y}_{h-1}^{\pi},\widehat{\rho}_{h-1}^{\pi})\right]-\mathbb{E}\left[ \mathcal{L}_{h-1}^{p}(y_{h}^{\pi};\widehat{y}_{h-1}^{\pi},\widehat{\rho}_{h-1}^ {\pi})\right]\] \[\leq 2\left(\mathcal{L}_{h-1}^{p}(\widehat{g}_{h}^{\pi};\widehat{y} _{h-1}^{\pi},\widehat{\rho}_{h-1}^{\pi})-\mathcal{L}_{h-1}^{p}(y_{h}^{\pi}; \widehat{y}_{h-1}^{\pi},\widehat{\rho}_{h-1}^{\pi})\right)+2\varepsilon_{h}^{ \mathrm{reg}}\] (Lem. F.2)\[\leq 2\varepsilon_{h}^{\mathrm{reg}}\] ( \[y_{h}^{\pi}\in\mathcal{G}_{h}\], _Asm._ 4.2 )

Then

\[\|\hat{y}_{h}^{p}-y_{h}^{p}\|_{1,\widetilde{d}_{h}^{\pi}}\leq\sqrt{2\left(1+C_{h- 1}^{\mathbf{s}}\left\|d_{h-1}^{D}-\hat{d}_{h-1}^{D}\right\|_{1}\right)\varepsilon _{h}^{\mathrm{reg}}}+2hG\left\|f_{h}^{\pi}-\widetilde{d_{h}^{\pi}}\right\|_{1}\]

Plugging in the bound from Lem. C.5 for \(\|f_{h}^{\pi}-\widetilde{d}_{h}^{\pi}\|_{1}\) gives the result.

**Lemma C.5**.: _For any \(\pi\) and estimates \(\{\widehat{d}_{h}^{\pi}\},\{\widehat{d}_{h}^{D}\}\), let \(\widehat{\rho}^{\pi}\) be defined as in Alg. 2, and for any \(h\in[H]\) and \(s^{\prime}\in\mathcal{S}\) define_

\[f_{h}^{\pi}(s^{\prime}):=\sum_{s,a}P(s^{\prime}|s,a)\widehat{\rho}^{\pi}(s,a) \pi_{h-1}^{D}(a|s)d_{h-1}^{D}(s),\]

_to be the next-state marginal distribution induced by reweighting \(d^{D}\) with \(\hat{\rho}\). We have_

\[\left\|f_{h}^{\pi}-\widetilde{d}^{\pi}\right\|_{1}\leq 2C_{h-1}^{\mathbf{s}} \left\|\hat{d}_{h-1}^{D}-d_{h-1}^{D}\right\|_{1}+\left\|\hat{d}_{h-1}^{\pi}- \widetilde{d}_{h-1}^{\pi}\right\|_{1}\]

Proof.: Using the definition of \(\widehat{\rho}^{\pi}\), we first rewrite \(f_{h}^{\pi}(s^{\prime})=\sum_{s,a}P(s^{\prime}|s,a)\widetilde{\pi}_{h-1}(a|s) \frac{\sigma\left(\widetilde{d}_{h-1}^{\pi}(s),C_{h-1}^{\mathbf{s}}\widehat{ d}_{h-1}^{D}(s)\right)}{\widehat{d}_{h-1}^{D}(s)}d_{h-1}^{D}(s)\). Then

\[\left\|f_{h}^{\pi}-\widetilde{d}_{h}^{\pi}\right\|_{1}\] \[\leq\left\|\frac{\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{ \mathbf{s}}\widehat{d}_{h-1}^{D}\right)}{\widehat{d}_{h-1}^{D}}d_{h-1}^{D}- \sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s}}d_{h-1}^{D}\right) \right\|_{1}\] \[\leq\left\|\frac{\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{ \mathbf{s}}\widehat{d}_{h-1}^{D}\right)}{\widehat{d}_{h-1}^{D}}\left( \widehat{d}_{h-1}^{D}-d_{h-1}^{D}\right)\right\|_{1}+\left\|\sigma\left( \widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s}}\widehat{d}_{h-1}^{D}\right)- \sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s}}d_{h-1}^{D}\right) \right\|_{1}\] \[\leq 2C_{h-1}^{\mathbf{s}}\left\|\widehat{d}_{h-1}^{D}-d_{h-1}^{D} \right\|_{1}+\left\|\widehat{d}_{h-1}^{\pi}-\widetilde{d}_{h-1}^{\pi}\right\| _{1}\]

where in the last inequality we use Asm. 4.1 to bound the second term. 

### Local convergence of Off-OccupPG

We demonstrate that Off-OccupPG can converge to a \(\varepsilon\)-stationary point. In order to establish this result, we will need the guarantee in Thm. 4.1 to hold for all possible policies, i.e., \(\|\widehat{\nabla}\widetilde{J}(\pi)-\nabla\widetilde{J}(\pi)\|\leq\varepsilon\) for all \(\pi\in\Pi_{\Theta}\). This is because the fixed offline data is reused throughout the algorithm, which introduces additional correlations between iterations. In the online setting it was sufficient to simply union bound over iterations, and not functions in our function classes, because we drew fresh trajectories for each policy iterate.

Since \(\mathcal{G}\) and \(\Pi_{\Theta}\) are continuous function classes, we will start our result in terms of their covering numbers, defined below. We handle this in the simplest manner by using \(\ell_{\infty}\) coverings, and leave a more refined analysis to future work. Def. C.1 is a covering on the clipped policy ratio over \(\pi^{D}\). For example, the direct policy parameterization with \(\pi_{\theta}=\theta\) has \(\mathcal{N}_{\infty}^{D}(\gamma,\Pi_{\Theta})\leq(\max_{h}C_{h}^{\mathbf{s}}/ \gamma)^{HSA}\) (Lem. C.10). In the below definition, we overload the definition of \(\overline{\pi}\) (the clipped policy in Lem. 4.1) temporarily.

**Definition C.1** (Policy ratio \(\gamma\)-cover).: Let \(\overline{\Pi}_{\Theta}\) be an \(\ell_{\infty}\) covering of \(\Pi_{\Theta}\) such that for any \(\pi\in\Pi_{\Theta}\) there exists \(\overline{\pi}\in\overline{\Pi}_{\Theta}\) with \(\|\frac{\sigma\left(\pi,C_{h}^{\mathbf{s}}\pi_{h}^{D}\right)}{\pi_{h}^{D}}- \frac{\sigma\left(\overline{\pi}_{h},C_{h}^{\mathbf{s}}\pi_{h}^{D}\right)}{\pi_{h }^{D}}\|_{\infty}\leq\gamma\). Let \(\mathcal{N}_{\infty}^{D}(\gamma,\Pi_{\Theta})\) denote its minimum cardinality.

**Definition C.2** (Gradient function class \(\gamma\)-cover).: Denote \(\mathcal{N}_{\infty}(\gamma,\mathcal{G})\) to be the \(\ell_{\infty}\) covering number of \(\{\mathcal{G}_{h}\}\) with resolution \(\gamma\).

Next, we state the stationary convergence guarantee in terms of these function class complexities, the offline coverage coefficient determined by input clipping constants \(\{C_{h}^{\mathsf{s}},C_{h}^{\mathsf{a}}\}\), and \(L_{\sigma}\) that represents the second-order smoothness of \(\sigma\).

**Corollary C.1**.: _Suppose Asm. 3.2 holds. Then under the preconditions of Thm. 4.1,_

\[\frac{1}{T}\sum_{t}\mathbb{E}\left[\|G^{\eta}(\pi^{(t)},\nabla\widetilde{J}( \pi^{(t)}))\|^{2}\right]\leq\varepsilon\]

_when_

\[T =\widetilde{O}\left(\frac{\beta H}{\varepsilon}\right)\] \[n =\widetilde{O}\left(\frac{\mathsf{p}H^{6}G^{2}\left(\sum_{h}C_{h} ^{\mathsf{s}}C_{h}^{\mathsf{a}}\right)^{2}L_{\sigma}^{2}\log(\mathcal{N}_{ \infty}(\varepsilon,\mathcal{G})\mathcal{N}_{\infty}^{D}(\varepsilon,\Pi_{ \Theta})|\mathcal{W}||\mathcal{F}|)}{\varepsilon}\right).\]

Proof of Cor. C.1First, we invoke a union-bounded version of the offline regression estimation guarantee in Lem. F.2. For any \(\pi\), \(g:\mathcal{S}\to\mathbb{R}^{p}\), reweighting function \(\rho:\mathcal{S}\times\mathcal{A}\to\mathbb{R}_{+}\), and target function \(y:\mathcal{S}\to\mathbb{R}^{p}\), define the \(p\)-th parameter squared loss for a fixed policy to be

\[\mathcal{L}_{h}^{\pi,p}(g;y,\rho)=\widehat{\mathbb{E}}_{(s,a,s^{\prime})\in \mathcal{D}_{h}}\left[\left(g^{p}(s^{\prime})-(\nabla^{p}\log\widetilde{\pi}_ {h}(a|s)+y^{p}(s))\right)^{2}\right)\right]\]

Then from Lem. F.3, With probability at least \(1-\delta^{\prime}\), for all \(h\in[H],p\in[\mathsf{p}]\), \(g\in\mathcal{G}_{h+1}\), and \(\rho,y\) induced by \(\mathcal{F},\mathcal{W}\) (see preconditions of Lem. F.3 for more exact statement), we have

\[\left|\mathbb{E}[\mathcal{L}_{h}^{\pi,p}(g;y,\rho)-\mathcal{L}_{h} ^{\pi,p}(g_{h+1}^{*};y,\rho)]-\mathcal{L}_{h}^{\pi,p}(g;y,\rho)-\mathcal{L}_{ h}^{\pi,p}(g_{h+1}^{*};y,\rho)\right|\] \[\qquad\leq\frac{1}{2}\mathbb{E}[\mathcal{L}_{h}^{\pi,p}(g;y,\rho )-\mathcal{L}_{h}^{\pi,p}(g_{h+1}^{*};y,\rho)]+(\varepsilon_{h+1}^{\mathrm{ reg}})^{2},\]

where \(g_{h+1}^{*}=\mathbf{E}_{h}^{D,\rho}[\nabla\log\widetilde{\pi}_{h}+y_{h}]\) and \(\varepsilon_{h}^{\mathrm{reg}}=O\left(\sqrt{\frac{C_{h-1}^{\mathsf{s}}C_{h-1} ^{\mathsf{a}}h^{2}G^{2}\log(\mathcal{N}_{\infty}(n^{-1},\mathcal{G})\mathsf{p }H|\mathcal{W}_{h}||\mathcal{F}_{h}|/\delta^{\prime})}{n}}\right).\)

To complete the regression part of the analysis we need to take a union bound over the result in Lem. F.3 for all \(\pi\in\Pi_{\Theta}\). For any \(\pi\in\Pi_{\Theta}\), let \(\overline{\pi}\in\widetilde{\Pi}_{\Theta}\) of Def. C.1 be its \(\ell_{\infty}\) cover. We need to bound the covering approximation error \(\mathcal{L}_{h}^{\pi,p}(g;y,\rho)-\mathcal{L}_{h}^{\overline{\pi},p}(g;y,\rho)\). Consider a fixed \((h,s,a,s^{\prime})\) and fix the inputs \((g,\rho,y,\pi)\), for which

\[\mathcal{L}_{h}^{\pi,p}(g;y,\rho)[s,a,s^{\prime}]-\mathcal{L}_{h }^{\overline{\pi},p}(g;y,\rho)[s,a,s^{\prime}]\] \[=\rho(s)\frac{\sigma\big{(}\pi(a|s),C_{h}^{\mathsf{s}}\pi_{h}^{D} (a|s)\big{)}}{\pi_{h}^{D}(a|s)}\left(g^{p}(s^{\prime})-2(\nabla^{p}\log \widetilde{\pi}(a|s)+y^{p}(s))+g_{h+1}^{*,p}(s^{\prime})\right)\left(g^{p}(s^ {\prime})-g_{h+1}^{*,p}(s^{\prime})\right)\]

Then

\[\left|\mathbb{E}[\mathcal{L}_{h}^{\pi,p}(g;y,\rho)-\mathcal{L}_{h }^{\overline{\pi},p}(g;y,\rho)]-(\mathcal{L}_{h}^{\pi,p}(g;y,\rho)-\mathcal{L }_{h}^{\overline{\pi},p}(g;y,\rho))\right|\] \[\leq 8C^{\mathsf{s}}h^{2}G^{2}\left\|\frac{\sigma\big{(}\pi,C_{h} ^{\mathsf{s}}\pi_{h}^{D}\big{)}}{\pi_{h}^{D}}-\frac{\sigma\big{(}\bar{\pi},C_{h }^{\mathsf{s}}\pi_{h}^{D}\big{)}}{\pi_{h}^{D}}\right\|_{\infty}+8C^{\mathsf{s} }C^{\mathsf{a}}hG\max_{s,a}\|\nabla\log\widetilde{\pi}(a|s)-\nabla\log \widetilde{[\pi]}(a|s)\|_{\infty}\] \[\leq 8(C^{\mathsf{s}}C^{\mathsf{a}}h^{2}G^{2}+C^{\mathsf{s}}C^{ \mathsf{a}}hGL_{\sigma}\beta)\varepsilon\]

where we get smoothness of the gradient portion using Asm. 4.1 and Asm. 3.2. Then by setting \(\varepsilon=(8(C^{\mathsf{s}}C^{\mathsf{s}}h^{2}G^{2}+C^{\mathsf{s}}C^{\mathsf{ a}}hGL_{\sigma}\beta))n^{-1}\) and combining the above errors with Lem. F.3, we have that with probability at least \(1-\delta\) for all \(\pi\in\Pi_{\Theta}\) that

\[\left|\mathbb{E}[\mathcal{L}_{\mathcal{D}_{h}}^{p}(\rho_{h},g_{h+1 },y_{h},\pi)-\mathcal{L}_{\mathcal{D}_{h}}^{p}(\rho_{h},g_{h+1}^{*},y_{h},\pi) ]-\mathcal{L}_{\mathcal{D}_{h}}^{p}(\rho_{h},g_{h+1}^{*},y_{h},\pi)-\mathcal{L }_{\mathcal{D}_{h}}^{p}(\rho_{h},g_{h+1}^{*},y_{h},\pi)\right|\] \[\qquad\leq\frac{1}{2}\mathbb{E}[\mathcal{L}_{\mathcal{D}_{h}}^{p}( \rho_{h},g_{h+1},y_{h},\pi)-\mathcal{L}_{\mathcal{D}_{h}}^{p}(\rho_{h},g_{h+1}^{* },y_{h},\pi)]\] \[\qquad+c\sqrt{\frac{C_{h-1}^{\mathsf{s}}C_{h-1}^{\mathsf{a}}h^{2}G ^{2}\log(\mathcal{N}_{\infty}^{D}(n^{-1},\Pi_{\Theta})\mathcal{N}_{\infty}(n^{-1},\mathcal{G})\mathsf{p}H|\mathcal{W}_{h}||\mathcal{F}_{h}|/\delta)}{n}}:= \varepsilon_{h}^{\mathrm{reg}},\]

for some absolute constant \(c\). The remainder of the proof is identical to the proof of Thm. 4.1 using the above \(\varepsilon_{h}^{\mathrm{reg}}\), which is then combined with Lem. G.1 to give the result.

### Global convergence of Off-OccupPG

We now turn our attention to analyzing gradient domination of the offline objective \(\widetilde{J}(\pi)\). The preconditions of our result are written in terms of the smooth-clipped analog of the pessimistic value function \(\widetilde{Q}^{\pi}\) (Prop. 4.1), induced by the smooth-clipped occupancy gradient \(\nabla\widetilde{d}^{\pi}\). For each \((h,s,a)\), define

\[\widetilde{Q}^{\pi}_{h}(s,a):= \ \partial_{x}\sigma\,\big{(}\pi(a|s),C^{\mathsf{n}}_{h}\pi^{D}_{ h}(a|s)\big{)}\sum_{s^{\prime}}P(s^{\prime}|s,a)\Big{(}R(s^{\prime})\] \[\qquad+\ \partial_{x}\sigma\,\Big{(}\widetilde{d}^{\pi}_{h+1}(s^{ \prime}),C^{\mathsf{n}}_{h+1}d^{D}_{h+1}(s^{\prime})\Big{)}\ \widetilde{Q}^{\pi}_{h+1}(s^{\prime},\widetilde{\pi}_{h+1})\Big{)}.\]

Lem. C.6 shows that the optimality gap of a policy for the smooth-clipped objective \(\widetilde{J}(\pi)\) is bounded by a measure of its gradient magnitude, as well as a coverage coefficient. This is because our trick with exploratory \(\mu\) in Lem. 3.2 isn't applicable, as it is not covered by the data in \(\mathcal{D}_{0}\) supported on \(\mathcal{S}^{0}\). Without this, our offline gradient domination guarantee in Lem. C.6 has a coverage coefficient that resembles the first inequality of Lem. B.3 when \(\mu=d_{0}\), the original initial state distribution.

**Lemma C.6**.: _For any \(\pi\) and \(\pi^{\prime}\), define \(\widetilde{B}^{\pi}(\pi^{\prime}):=\sum_{h,s,a}\widetilde{d}^{\pi}_{h}(s) \widetilde{\pi}^{\prime}(a|s)\widetilde{Q}^{\pi}_{h}(s,a)\). Suppose that \(\forall\pi\in\Pi_{\Theta}\),_

1. _(Policy completeness) There exists_ \(\pi^{+}\in\Pi_{\Theta}\) _such that_ \(\pi^{+}\in\operatorname*{argmax}_{\pi^{\prime}}\widetilde{B}^{\pi}(\pi^{ \prime})\)_._
2. _(Gradient domination)_ \(\max_{\pi^{\prime}\in\Pi_{\Theta}}\widetilde{B}^{\pi}(\pi^{\prime})- \widetilde{B}^{\pi}(\pi)\leq m\max_{\theta^{\prime}\in\Theta}\Big{\langle} \nabla\widetilde{B}^{\pi}(\pi),\theta^{\prime}-\theta\Big{\rangle}\)_._

_Then for any comparator policy \(\pi^{E}\) and \(\pi_{\theta}\in\Pi_{\Theta}\), we have_

\[\widetilde{J}(\pi^{E})-\widetilde{J}(\pi_{\theta})\leq m\left(\max_{h}\left\| \frac{\sigma\,\Big{(}\widetilde{d}^{\pi^{E}}_{h},C^{\mathsf{n}}_{h}d^{D}_{h} \Big{)}}{\sigma\,\Big{(}\widetilde{d}^{\pi_{\theta}}_{h},C^{\mathsf{n}}_{h}d^{ D}_{h}\Big{)}}\right\|_{\infty}\right)\max_{\theta^{\prime}\in\Theta}\ \langle\nabla\widetilde{J}(\pi_{\theta}),\theta^{ \prime}-\theta\rangle.\]

Compared to Lem. 3.2, the first precondition of Lem. C.6 may be stronger because \(\pi^{+}\) can be a stochastic policy, whereas deterministic policies suffice in the online setting. The second precondition is of similar strength. More importantly--as we have previously discussed-- coverage coefficients of the form in Lem. C.6 are not ideal because they involve \(\pi_{\theta}\) in the denominator, which are variable over the learning process.

Offline data as an exploratory initializationSince all occupancies are clipped to some constant of the offline data, however, we might wonder if the offline data distribution itself might serve as an exploratory initial distribution to use with Off-OccupPG (in some sense, this, or some reweighted version of it, is the only thing available to us in the offline setting). Prop. C.2 shows that this is indeed possible when the offline data is exploratory enough, and we use clipping for simplicity. Notably, the coverage coefficient present in the gradient domination bound is the input clipping constant \(\sum_{h}C^{\mathsf{n}}_{h}\). This can be seen as an offline analog of \(\mathcal{C}^{\pi^{+}}\) in Lem. 3.2, since all occupancies are clipped to have this ratio over the offline data.

**Proposition C.2**.: _Given \(\{d^{D}_{h}\}\), define a new data distribution where \(d^{D^{\prime}}_{h}=\frac{1}{H}\sum_{g=0}^{H-1}d^{D}_{g},\ \forall h\in[H]\). Then for any \(\pi\), use \(\{d^{D^{\prime}}_{h}\}\), \(\sigma=\wedge\), and clipping constants \(\{C^{\mathsf{n^{\prime}}}_{h},C^{\mathsf{n^{\prime}}}_{h}\}\) to define \([\widetilde{d}^{\pi}_{h}]^{\prime}\) according to Def. 4.3. Let \(\widetilde{J^{\prime}}(\pi)=\sum_{h}\langle[\widetilde{d}^{\pi}_{h}]^{\prime},R\rangle\)._

_For any \(\pi\) and \(\pi^{\prime}\), recall \(\widetilde{B}^{\pi}(\pi^{\prime}):=\sum_{h,s,a}[\widetilde{d}^{\pi}_{h}]^{ \prime}(s)\widetilde{\pi}^{\prime}(a|s)[\widetilde{Q}^{\pi}]^{\prime}_{h}(s,a)\). Suppose that \(\forall\pi\in\Pi_{\Theta}\),_

1. _(Policy completeness) There exists_ \(\pi^{+}\in\Pi_{\Theta}\) _such that_ \(\pi^{+}\in\operatorname*{argmax}_{\pi^{\prime}}\widetilde{B}^{\pi}(\pi^{\prime})\)_._
2. _(Gradient domination)_ \(\max_{\pi^{\prime}\in\Pi_{\Theta}}\widetilde{B}^{\pi}(\pi^{\prime})- \widetilde{B}^{\pi}(\pi)\leq m\max_{\theta^{\prime}\in\Theta}\Big{\langle} \nabla\widetilde{B}^{\pi}(\pi),\theta^{\prime}-\theta\Big{\rangle}\)_._

_Then if \(\{C^{\mathsf{n^{\prime}}}_{h},C^{\mathsf{n^{\prime}}}_{h}\}\) are such that \([\widetilde{d}^{\pi_{\theta}}_{h}]^{\prime}\leq C^{\mathsf{n^{\prime}}}_{h}d^{D ^{\prime}}_{h},\ \forall h\), for any \(\pi_{\theta}\in\Pi_{\Theta}\), we have_

\[\max_{\pi}\widetilde{J}(\pi)-\widetilde{J}(\pi_{\theta})\leq mH\left(\sum_{h}C^ {\mathsf{n}}_{h}\right)\max_{\theta^{\prime}\in\Theta}\langle\nabla\widetilde{J}^ {\prime}(\pi_{\theta}),\theta^{\prime}-\theta\rangle.\]In practice, we can easily generate a new dataset \(\mathcal{D}^{\prime}\) satisfying Prop. C.2 by first splitting each \(\mathcal{D}_{h}\) into \(H\) equal parts \(\{\mathcal{D}_{h}^{i}\}_{i=1}^{H}\), then setting \(\mathcal{D}_{h}^{\prime}=\cup_{g=1}^{H}\mathcal{D}_{g}^{i}\). The sample complexity of running Alg. 2 on \(\mathcal{D}^{\prime}\) will then scale with \(\sum_{h}C_{h}^{\bullet}\), which are input parameters, instead of the coefficient in Lem. C.6, which is \(\theta\)-dependent and cannot be controlled. In exchange, it requires all-policy coverage w.r.t the new \([\widetilde{d}^{\pi}]^{\prime}\), which, while strong, was insufficient for optimality in Lem. C.6. One justification (formalized in the hardness result of Prop. C.3) is that the exploratory initialization can cause policies to exceed coverage thresholds on reward-generating states, despite being covered on (the original) \(d_{0}\). Clipping causes gradient signals to vanish, so a stationary policy might be far off-support, instead of optimal. While everything works out conceptually if \(\{C_{h}^{\bullet},C_{h}^{\bullet}\}\) are set to be high enough, it's unclear whether doing this will require exponentially large coefficients in the worst case.

Lastly, we combine the above gradient domination claims with the stationary convergence guarantee in Cor. C.1 to state the following global convergence result. Cor. C.2 is stated for \(\widetilde{J}(\pi)\), our original offline optimization objective, and therefore takes into the account of approximating the clipping function with its smooth-clipped version.

**Corollary C.2**.: _Suppose \(\widetilde{J}(\pi)\) satisfies Ass. 3.2. If Alg. 2 with \(\mathcal{D}^{\prime}\) as defined in Prop. C.2 satisfies the preconditions of Prop. C.2, then set \(\mathrm{CC}=H\sum_{h}C_{h}^{\bullet}\). Otherwise, define \(\mathrm{CC}=\max_{\pi\in\Pi_{\Theta}}\max_{h}\left\|\sigma\left(\widetilde{d} _{h}^{\pi^{*}},C_{h}^{\mathsf{S}}d_{h}^{D}\right)/\sigma\left(\widetilde{d}_{ h}^{\pi},C_{h}^{\mathsf{S}}d_{h}^{D}\right)\right\|_{\infty}\) and assume the preconditions of Lem. C.6. Then under Def. C.1 and the preconditions of Thm. 4.1, Alg. 2 satisfies_

\[\mathbb{E}\left[\frac{1}{T}\sum_{t}\left\{\max_{\pi}\widetilde{J}(\pi)- \widetilde{J}(\pi^{(t)})\right\}\right]\leq\varepsilon+2H^{2}D_{\sigma}\]

_when \(T=\ \widetilde{O}\left(\frac{B^{2}m^{2}(\mathrm{CC})^{2}\mathsf{p}H^{\mathsf{S}} d^{2}\left(\sum_{h}C_{h}^{\mathsf{S}}C_{h}^{\mathsf{S}}\right)^{2}_{\sigma} \frac{1}{2}\log(\mathcal{N}_{\infty}(\varepsilon,\mathcal{G})\ \mathcal{N}_{\infty}^{D}( \varepsilon,\Pi_{\Theta})|\mathcal{F}||\mathcal{W}|)}{\varepsilon^{2}}\right)\)._

Though we optimize \(\widetilde{J}(\pi)\), the guarantee in Cor. C.2 is with respect to our target offline objective \(\widetilde{J}(\pi)\), which implies that the learned policy competes with the best policy fully covered by offline data. Generally \(L_{\sigma}\) and \(D_{\sigma}\) trade-off between ease of convergence (smoothness) and approximation error, respectively. For example, instantiating the bound with \(\sigma\) from Prop. 4.3 with \(b\propto\varepsilon\) results in a final \(\varepsilon^{-1/4}\) rate.

### Proofs for App. C.5

Proof of Lem. C.6We will use superscript \(h\) to refer to \(s^{h}\in\mathcal{S}^{h}\), the set of states visitable at timestep \(h\), and drop \(C_{h}^{\bullet}\) below to reduce clutter. By the performance difference upper bound for the smooth-clipped objective in Lem. C.8, we have

\[\widetilde{J}(\pi^{*})-\widetilde{J}(\pi_{\theta}) \leq\ \sum_{h=0}^{H-1}\sum_{s,a}\sigma\left(\widetilde{d}_{h}^{\pi^{*}}( s),d_{h}^{D}(s)\right)\left(\widetilde{\pi}^{*}(a|s)-\widetilde{\pi}_{\theta}(a|s) \right)\widetilde{Q}_{h}^{\pi_{\theta}}(s,a)\] \[=\ \sum_{h=0}^{H-1}\sum_{s^{h},a^{h}}\sigma\left(\widetilde{d}_{ h}^{\pi^{*}}(s^{h}),d_{h}^{D}(s^{h})\right)\left(\widetilde{\pi}^{*}(a^{h}|s^{h})- \widetilde{\pi}_{\theta}(a^{h}|s^{h})\right)\widetilde{Q}_{h}^{\pi_{\theta}}(s ^{h},a^{h})\]

since \(d_{h}^{D}(s)>0\) only if \(s\in\mathcal{S}^{h}\). Now define \(\pi^{+}\) such that for any \(s\), \(\widetilde{\pi}^{+}(\cdot|s)=\operatorname*{argmax}_{\pi\in\Delta(\mathcal{A})} \left\langle\widetilde{\pi},\widetilde{Q}_{h}^{\pi_{\theta}}(s,\cdot)\right\rangle\). Then

\[\widetilde{J}(\pi^{E})-\widetilde{J}(\pi_{\theta})\] \[\leq\ \sum_{h=0}^{H-1}\sum_{s^{h},a^{h}}\sigma\left(\widetilde{d}_{ h}^{\pi^{E}}(s^{h}),d_{h}^{D}(s^{h})\right)\left(\widetilde{\pi}^{E}(a^{h}|s^{h})- \widetilde{\pi}_{\theta}(a^{h}|s^{h})\right)\widetilde{Q}_{h}^{\pi_{\theta}}(s ^{h},a^{h})\] \[\leq\ \sum_{h=0}^{H-1}\sum_{s^{h},a^{h}}\sigma\left(\widetilde{d}_{ h}^{\pi^{E}}(s^{h}),d_{h}^{D}(s^{h})\right)\left(\widetilde{\pi}^{+}(a^{h}|s^{h})- \widetilde{\pi}_{\theta}(a^{h}|s^{h})\right)\widetilde{Q}_{h}^{\pi_{\theta}}(s ^{h},a^{h})\] (20)\[\leq \max_{h}\left\|\frac{\sigma\left(\widetilde{\widetilde{d}_{h}^{\pi}} ^{\varepsilon},C_{h}^{\mathbf{s}}d_{h}^{D}\right)}{\sigma\left(\widetilde{ \widetilde{d}_{h}^{\pi_{\theta}}},C_{h}^{\mathbf{s}}d_{h}^{D}\right)}\right\|_{ \infty}\,\sum_{h=0}^{H-1}\sum_{s^{h},a^{h}}\sigma\left(\widetilde{d}_{h}^{\pi_{ \theta}}(s^{h}),d_{h}^{D}(s^{h})\right)\left(\pi^{+}(a^{h}|s^{h})-\pi_{\theta }(a^{h}|s^{h})\right)\] \[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\qquad\cdot\mathbf{\tilde{1}}\left(\pi_{\theta}(a^{h}| s^{h}),C_{h}^{\mathbf{n}}\pi^{D}(a^{h}|s^{h})\right)\widetilde{Q}_{h}^{\pi_{ \theta}}(s^{h},a^{h})\] \[\leq \max_{h}\left\|\frac{\sigma\left(\widetilde{d}_{h}^{\pi},C_{h}^{ \mathbf{s}}d_{h}^{D}\right)}{\sigma\left(\widetilde{d}_{h}^{\pi_{\theta}},C_{h }^{\mathbf{s}}d_{h}^{D}\right)}\right\|_{\infty}\,\sum_{h=0}^{H-1}\sum_{s^{h},a^{h}}\sigma\left(\widetilde{d}_{h}^{\pi_{\theta}}(s^{h}),d_{h}^{D}(s^{h}) \right)\left(\widetilde{\pi}^{+}(a^{h}|s^{h})-\widetilde{\pi}_{\theta}(a^{h}| s^{h})\right)\widetilde{Q}_{h}^{\pi_{\theta}}(s^{h},a^{h})\] \[= \max_{h}\left\|\frac{\sigma\left(\widetilde{d}_{h}^{\pi},C_{h}^{ \mathbf{s}}d_{h}^{D}\right)}{\sigma\left(\widetilde{d}_{h}^{\pi_{\theta}},C_{h }^{\mathbf{s}}d_{h}^{D}\right)}\right\|_{\infty}\,\max_{\pi^{+}}\left( \widetilde{B}^{\pi^{+}}(\pi_{\theta})-\widetilde{B}^{\pi_{\theta}}(\pi_{\theta })\right)\] \[= \max_{h}\left\|\frac{\sigma\left(\widetilde{d}_{h}^{\pi},C_{h}^{ \mathbf{s}}d_{h}^{D}\right)}{\sigma\left(\widetilde{d}_{h}^{\pi_{\theta}},C_{h }^{\mathbf{s}}d_{h}^{D}\right)}\right\|_{\infty}\,\max_{\pi_{\theta^{\prime}} \in\Pi_{\Theta}}\left(\widetilde{B}^{\pi_{\theta^{\prime}}}(\pi_{\theta})- \widetilde{B}^{\pi_{\theta}}(\pi_{\theta})\right)\]

**Proof of Prop. C.2** Under all-policy coverage, we can apply the result in Lem. 3.2, noting that \(d_{0}^{\prime}=\frac{1}{H}d_{h}^{D}\), and \(d_{h}^{\pi^{*}}\leq C_{h}^{\mathbf{s}}d_{h}^{D}\).

**Proposition C.3** (Vanishing gradient from clipping with exploratory data).: _Consider the MDP in App. C.6, and the data distribution where \(d^{D}(X)=1/2\) and \(d^{D}(Y)=\epsilon\) and \(d^{D}(Z)=(1-\epsilon)/2\) for some \(\epsilon\in[0,1]\). For any \(C\), we have all-policy coverage, i.e., \(d_{h}^{D}\leq C_{h}d_{h}^{D}\) for all \(h\) and all policies \(\pi\). Let \(\pi\) be the stationary (and in this case, optimal) policy of running Alg. 2 with \(\mathcal{D}^{\prime}\) described in Prop. C.2. Then_

\[J(\pi^{*})-J(\pi)=\left(1-\epsilon\right)\left(1-2C_{Y}\epsilon\right).\]

_If \(\epsilon\) is exponentially small, \(J(\pi^{*})-J(\pi)=O(1)\) unless \(C_{Y}\) is exponentially large._

Proof.: The example boils down to a simple bandit problem of choosing either \(L\) or \(R\) in state \(X\). \(\pi(L|X)=\frac{C_{Y}d^{D}(Y)}{d^{D}(X)}=2C_{Y}\epsilon\), and \(\pi(R|X)=1-\pi(L|X)\). In comparison, \(\pi^{*}(L|X)=1\). Then \(\widetilde{J}(\pi)=J(\pi)=\frac{C_{Y}d^{D}(Y)}{d^{D}(X)}+\epsilon(1-\pi(L|X))\). In comparison, \(J(\pi^{*})=1\), so

\[J(\pi^{*})-J(\pi)=\left(1-\epsilon\right)\left(1-2C_{Y}\epsilon\right)\]

For reasonable choices of \(C_{Z}\) (say, 2 or 3), \(C_{Y}\) must be proportional to \(\epsilon^{-1}\) for the suboptimality gap to shrink, and in particular if \(\epsilon\) is exponentially small then \(C_{Y}\) must be exponentially large, which blows up the RHS of the bound.

Figure 3: Example in Prop. C.3

Proof of Cor. C.2The first step follows the proof of Cor. 3.2. Combining Thm. 4.1 with Lem. G.5 and plugging in above, we have

\[\mathbb{E}\left[\frac{1}{T}\sum_{t}\widetilde{J}(\pi^{*})-\widetilde {J}(\pi^{(t)})\right]\] \[\lesssim Bm\mathrm{CC}\left(\sqrt{\frac{\beta H}{T}}+\sqrt{\frac{ \mathsf{p}H^{6}G^{2}\left(\sum_{h}C_{h}^{\mathsf{s}}C_{h}^{\mathsf{a}}\right) ^{2}L_{\sigma}^{2}\log(\mathcal{N}_{\infty}(\varepsilon,\mathcal{G})\, \mathcal{N}_{\infty}^{D}(\varepsilon,\Pi_{\Theta})|\mathcal{F}||\mathcal{W}| \right)}{n}}\right)\]

Then we also have

\[\mathbb{E}\left[\frac{1}{T}\sum_{t}\bar{J}(\pi^{*})-\bar{J}(\pi^{ (t)})\right]\] \[\leq\mathbb{E}\left[\frac{1}{T}\sum_{t}\widetilde{J}(\pi^{*})- \widetilde{J}(\pi^{(t)})\right]+2H^{2}D_{\sigma}\] (Prop. 4.4) \[\lesssim 2H^{2}D_{\sigma}+Bm\mathrm{CC}\left(\sqrt{\frac{\beta H}{T} }+\sqrt{\frac{\mathsf{p}H^{6}G^{2}\left(\sum_{h}C_{h}^{\mathsf{s}}C_{h}^{ \mathsf{a}}\right)^{2}L_{\sigma}^{2}\log(\mathcal{N}_{\infty}(\varepsilon, \mathcal{G})\,\mathcal{N}_{\infty}^{D}(\varepsilon,\Pi_{\Theta})|\mathcal{F} ||\mathcal{W}|)}{n}}\right)\]

Additional resultsHelper lemmas are stated and proved below.

**Lemma C.7**.: _Suppose \(\sigma\) satisfies Parts 1 and 2 of Ass. 4.1. Then for \(\widetilde{J}(\pi)=\sum_{h}\sum_{s}\widetilde{d}_{h}^{\tau}(s)R_{h}(s)\),_

\[\nabla\widetilde{J}(\pi)=\sum_{h=0}^{H-1}\sum_{s,a}\sigma\left(\widetilde{d} _{h}^{\tau}(s),C_{h}^{\mathsf{s}}d_{h}^{D}(s)\right)\nabla\widetilde{\pi}_{h} (a|s)\widetilde{Q}_{h}^{\tau}(s,a),\]

_where_

\[\widetilde{Q}_{h}^{\tau}(s,a)=\,\sum_{s^{\prime}}P_{h}(s^{\prime}|s,a)\left(R_ {h+1}(s^{\prime})+\sum_{a^{\prime}}\widetilde{\pi}_{h+1}(a^{\prime}|s^{\prime} )\,\partial_{x}\sigma\left(\widetilde{d}_{h+1}^{\widetilde{\tau}}(s^{\prime} ),C_{h+1}^{\mathsf{s}}d_{h+1}^{D}(s^{\prime})\right)\widetilde{Q}_{h+1}^{ \widetilde{\tau}}(s^{\prime},a^{\prime})\right).\]

Proof of Lem. C.7.: For notational clarity we omit \(C_{h}^{\mathsf{s}}\) below. Expanding \(\nabla\widetilde{d}^{\widetilde{\tau}}\), we have

\[\nabla\widetilde{d}_{h}^{\tau}(s_{h}) =\sum_{s_{h-1},a_{h-1}}P(s_{h}|s_{h-1},a_{h-1})\Big{(}\nabla \widetilde{\pi}_{h-1}(a_{h-1}|s_{h-1})\sigma\left(\widetilde{d}_{h-1}^{ \widetilde{\tau}}(s_{h-1}),d_{h-1}^{D}(s_{h-1})\right)\] \[+\widetilde{\pi}_{h-1}(a_{h-1}|s_{h-1})\,\partial_{x}\sigma\left( \widetilde{d}_{h-1}^{\widetilde{\tau}}(s_{h-1}),d_{h-1}^{D}(s_{h-1})\right) \nabla\widetilde{d}_{h-1}^{\widetilde{\tau}}(s_{h-1})\Big{)}\] \[=\,\sum_{g<h}\sum_{(s_{h-1},a_{h-1},\dots,s_{g},a_{g})}\left[\prod _{t=g+1}^{h-1}P(s_{t+1}|s_{t},a_{t})\widetilde{\pi}_{t}(a_{t}|s_{t})\, \partial_{x}\sigma\left(\widetilde{d}_{t}^{\tau}(s_{t}),d_{t}^{D}(s_{t}) \right)\right]\] \[\qquad\qquad\qquad\qquad\cdot P(s_{g+1}|s_{g},a_{g})\sigma\left( \widetilde{d}_{g}^{\tau}(s_{g}),d_{g}^{D}(s_{g})\right)\nabla\widetilde{\pi}_{ g}(a_{g}|s_{g})\]

For short, define

\[\widetilde{\mathbf{P}}^{\widetilde{\pi}}(s_{h}|s_{g},a_{g}):=\sum_{(s_{h-1},a_{ h-1},\dots,s_{g+1},a_{g+1})}\left[\prod_{t=g+1}^{h-1}P(s_{t+1}|s_{t},a_{t}) \widetilde{\pi}_{t}(a_{t}|s_{t})\,\partial_{x}\sigma\left(\widetilde{d}_{t}^{ \tau}(s_{t}),d_{t}^{D}(s_{t})\right)\right]P(s_{g+1}|s_{g},a_{g})\]

observing if \(\widetilde{\pi}_{h}=\pi_{h}\) and \(\partial_{x}\sigma\left(\widetilde{d}_{h}^{\tau},d_{h}^{D}\right)=1\) for all \(h\), we have \(\widetilde{\mathbf{P}}^{\widetilde{\pi}}(s_{h}|s_{g},a_{g})=\mathbf{P}^{\pi}(s_{ h}|s_{g},a_{g})\), the standard transition kernel from \((s_{g},a_{g})\to s_{h}\). This occurs, for example, when \(\sigma^{\mathsf{s}}\) is hard clipping and \(\pi\) is fully covered by data. Then using the above definition, we have

\[\nabla\widetilde{d}_{h}^{\widetilde{\tau}}(s_{h})=\sum_{g<h}\sum_{s_{g},a_{g}} \sigma\left(\widetilde{d}_{g}^{\widetilde{\tau}}(s_{g}),d_{g}^{D}(s_{g})\right) \nabla\widetilde{\pi}_{g}(a_{g}|s_{g})\widetilde{\mathbf{P}}^{\widetilde{\pi}}(s _{h}|s_{g},a_{g}).\] (21)Plugging this expression into \(\nabla J(\pi)\), we obtain

\[\nabla J(\pi) =\,\sum_{h}\sum_{s_{h}}\nabla\widetilde{d}_{h}^{\pi}(s_{h})R(s_{h})\] \[=\,\sum_{h}\sum_{s_{h}}\left(\sum_{g=0}^{h-1}\sum_{s_{g},a_{g}} \sigma\left(\widetilde{d}_{g}^{\pi}(s_{g}),d_{g}^{D}(s_{g})\right)\nabla\widetilde {\pi}_{g}(a_{g}|s_{g})\widetilde{\mathbf{P}}^{\widetilde{\pi}}(s_{h}|s_{g},a_{ g})\right)R(s_{h})\] \[=\,\sum_{g=0}^{H-1}\sum_{s_{g},a_{g}}\sigma\left(\widetilde{d}_{g }^{\pi}(s_{g}),d_{g}^{D}(s_{g})\right)\nabla\widetilde{\pi}_{g}(a_{g}|s_{g}) \left(\sum_{h=g+1}^{H}\sum_{s_{h}}\widetilde{\mathbf{P}}^{\widetilde{\pi}}(s_{ h}|s_{g},a_{g})R(s_{h})\right)\] \[=\,\sum_{g=0}^{H-1}\sum_{s_{g},a_{g}}\sigma\left(\widetilde{d}_{ g}^{\pi}(s_{g}),d_{g}^{D}(s_{g})\right)\nabla\widetilde{\pi}_{g}(a_{g}|s_{g}) \widetilde{Q}^{\pi}(s_{g},a_{g})\]

where we have defined

\[\widetilde{Q}_{g}^{\pi}(s_{g},a_{g}):=\sum_{h=g+1}^{H}\sum_{s_{h} }\widetilde{\mathbf{P}}^{\widetilde{\pi}}(s_{h}|s_{g},a_{g})R(s_{h})\] \[=\,\sum_{s_{g+1}}P(s_{g+1}|s_{g},a_{g})\left(R(s_{g+1})+\sum_{a_ {g+1}}\widetilde{\pi}_{g+1}(a_{g+1}|s_{g+1})\;\partial_{x}\sigma\left( \widetilde{d}_{g+1}^{\pi}(s_{g+1}),d_{g+1}^{D}(s_{g+1})\right)\widetilde{Q}_{ g+1}^{\widetilde{\pi}}(s_{g+1},a_{g+1})\right)\]

**Lemma C.8**.: _If \(\sigma\) is concave in its first argument, for any \(\pi^{\prime}\) and \(\pi\) we have_

\[\widetilde{J}(\pi^{\prime})-\widetilde{J}(\pi)\leq\sum_{h=0}^{H-1}\sum_{s,a} \sigma\left(\widetilde{d}_{h}^{\pi^{\prime}}(s),d_{h}^{D}(s)\right)\left( \widetilde{\pi}_{h}^{\prime}(a|s)-\widetilde{\pi}_{h}(a|s)\right)\widetilde{Q }_{h}^{\pi}(s,a),\]

_where \(\widetilde{Q}_{h}^{\pi}\) is defined in Lem. C.7._

Proof.: This statement follows straightforwardly from plugging in Lem. C.9 and rearranging, similar to the proof of Lem. C.7. 

**Lemma C.9**.: _If \(\sigma\) is concave in its their first arguments, then for any \(h\) and \(\pi,\pi^{\prime}\)_

\[\widetilde{d}_{h}^{\pi^{\prime}}(s^{\prime})-\widetilde{d}_{h}^{ \pi}(s^{\prime})\] \[\leq\,\sum_{g<h}\sum_{s,a}\sigma\left(\widetilde{d}_{g}^{\pi^{ \prime}}(s),d_{g}^{D}(s)\right)\left(\sigma\left(\pi_{g}^{\prime}(a|s),\pi_{g }^{D}(a|s)\right)-\sigma\left(\pi_{g}(a|s),\pi_{g}^{D}(a|s)\right)\right) \widetilde{\mathbf{P}}^{\pi}(s_{h}=s^{\prime}|s_{g}=s,a_{g}=a),\]

_where_

\[\widetilde{\mathbf{P}}^{\pi}(s_{h}|s_{g},a_{g}):=\sum_{s_{h-1:g+1},a_{h-1:g+1}} \left[\prod_{t=g+1}^{h-1}P(s_{t+1}|s_{t},a_{t})\widetilde{\pi}_{t}(a_{t}|s_{t} )\;\partial_{x}\sigma\left(\widetilde{d}_{t}^{\pi}(s_{t}),d_{t}^{D}(s_{t}) \right)\right]P(s_{g+1}|s_{g},a_{g}).\]

Proof of Lem. C.9.: Define \(\pi^{g}=\{\pi_{1}^{\prime},\ldots,\pi_{g-1}^{\prime},\pi_{g},\ldots,\pi_{H-1}\}\), i.e., a policy that starts playing \(\pi\) at timestep \(g\), and plays \(\pi^{\prime}\) for the timesteps before that.

\[\widetilde{d}_{h}^{\pi^{\prime}}(s^{\prime})-\widetilde{d}_{h}^{\pi}(s^{\prime })=\widetilde{d}_{h}^{\pi^{\prime}}(s^{\prime})-\widetilde{d}_{h}^{\pi^{h-1}}( s^{\prime})+\widetilde{d}_{h}^{\pi^{h-1}}(s^{\prime})-\widetilde{d}_{h}^{\pi}(s^{ \prime})\]

For the first pair of terms, \(\pi^{\prime}\) and \(\pi^{h-1}\) only differ the policy used to take the action \(a_{h-1}\) (and both play \(\pi^{\prime}\) before that), thus \(d_{h-1}^{\pi^{\prime}}=d_{h-1}^{\pi^{h-1}}\) and

\[\widetilde{d}_{h}^{\pi^{\prime}}(s^{\prime})-\widetilde{d}_{h}^{ \pi^{h-1}}(s^{\prime})\] \[=\,\sum_{s,a}P(s^{\prime}|s,a)\left(\sigma\left(\pi_{h-1}^{ \prime}(a|s),\pi_{h-1}^{D}(a|s)\right)-\sigma\left(\pi_{h-1}(a|s),\pi_{h-1}^{D} (a|s)\right)\right)\sigma\left(\widetilde{d}_{h-1}^{\pi^{\prime}}(s),d_{h-1}^{D }(s)\right)\]For the second pair of terms, \(\pi^{h-1}\) and \(\pi\) both play \(\pi\) at time \(h-1\), but the former uses \(\pi^{\prime}\) for timesteps \(1,\ldots,h-2\):

\[\widetilde{d}_{h}^{\pi^{h-1}}(s^{\prime})-\widetilde{d}_{h}^{\pi}(s ^{\prime})\] \[= \sum_{s,a}P(s^{\prime}|s,a)\sigma\left(\pi_{h-1}(a|s),\pi_{h-1}^ {D}(a|s)\right)\left(\sigma\left(\widetilde{d}_{h-1}^{\pi^{h-1}}(s),d_{h-1}^{D }(s)\right)-\sigma\left(\widetilde{d}_{h-1}^{\pi}(s),d_{h-1}^{D}(s)\right)\right)\] \[= \sum_{s,a}P(s^{\prime}|s,a)\sigma\left(\pi_{h-1}(a|s),\pi_{h-1}^ {D}(a|s)\right)\left(\sigma\left(\widetilde{d}_{h-1}^{\pi^{\prime}}(s),d_{h-1} ^{D}(s)\right)-\sigma\left(\widetilde{d}_{h-1}^{\pi}(s),d_{h-1}^{D}(s)\right)\right)\] \[\leq \sum_{s,a}P(s^{\prime}|s,a)\sigma\left(\pi_{h-1}(a|s),\pi_{h-1}^ {D}(a|s)\right)\;\partial_{x}\sigma\left(\widetilde{d}_{h-1}^{\pi}(s),d_{h-1} ^{D}(s)\right)\left(\widetilde{d}_{h-1}^{\pi^{\prime}}(s)-\widetilde{d}_{h-1}^ {\pi}(s)\right)\]

where the last inequality above uses the concavity of \(\sigma^{\mathbf{s}}\) in the first argument (recall concave functions \(f\) satisfy \(f(y)\leq f(x)+f^{\prime}(x)(y-x)\)). Combining the above two inequalities, we have the recursive relationship

\[\widetilde{d}_{h}^{\pi^{\prime}}(s^{\prime})-\widetilde{d}_{h}^{ \pi}(s^{\prime})\] \[\leq \sum_{s,a}P(s^{\prime}|s,a)\Bigg{(}\left(\sigma\left(\pi_{h-1}^{ \prime}(a|s),\pi_{h-1}^{D}(a|s)\right)-\sigma\left(\pi_{h-1}(a|s),\pi_{h-1}^{D }(a|s)\right)\right)\sigma\left(\widetilde{d}_{h-1}^{\pi^{\prime}}(s),d_{h-1} ^{D}(s)\right)\] \[\qquad\qquad\qquad+\sigma\left(\pi_{h-1}(a|s),\pi_{h-1}^{D}(a|s) \right)\;\partial_{x}\sigma\left(\widetilde{d}_{h-1}^{\pi}(s),d_{h-1}^{D}(s) \right)\left(\widetilde{d}_{h-1}^{\pi^{\prime}}(s)-\widetilde{d}_{h-1}^{ \pi}(s)\right)\Bigg{)}\]

Unrolling through timesteps gives the lemma statement. 

**Lemma C.10**.: _Let \(C^{\mathbf{a}}=\max_{h}C^{\mathbf{a}}_{h}\). Suppose \(\Pi_{\Theta}\) is the direct policy parameterization, i.e., \(\pi_{\theta}(a|s)=\theta_{s,a}\), and \(\sigma\) is such that \(D_{\sigma}\leq C^{\mathbf{a}}\) for all \(h\). Then for any \(\gamma\in(0,1]\), in Def. C.1 we have \(\mathcal{N}^{D}_{\infty}(\gamma,\Pi_{\Theta})\leq(C^{\mathbf{a}}/\gamma)^{ SAH}\)._

Proof of Lem. C.10.: Typical gridding-style arguments discretize the range of \(\pi(a|s)\) for each \((s,a)\). Since we are concerned with creating a cover for the policy ratio, however, a naive argument will incur \(1/\min_{s,a}\pi^{D}(a|s)\) in the grid's cardinality. Our solution is to grid \(\Pi_{\Theta}\) adaptively according to the magnitude of \(\pi^{D}(a|s)\). Intuitively, we only need to grid up to the threshold

For each \((h,s,a)\), define the adaptive gridding scale to be \(\gamma^{\prime}_{hsa}=\gamma\pi_{h}^{D}(a|s)\). For any \(\pi\in\Pi_{\Theta}\), set its cover \(\overline{\pi}\) as follows.

\[\overline{\pi}_{h}(a|s)=\begin{cases}\left\lfloor\frac{\pi(a|s)}{\gamma^{ \prime}_{hsa}}\right\rfloor,&\text{if }\pi(a|s)\leq C^{\mathbf{n}}_{h}\pi_{h}^{D}(a|s)\text{,}\\ C^{\mathbf{n}}_{h}\pi_{h}^{D}(a|s),&\text{otherwise.}\end{cases}\]

Let \(\overline{\Pi}_{\Theta}=\{\overline{\pi}:\pi\in\Pi_{\Theta}\}\). Then \(|\overline{\Pi}_{\Theta}|\leq(\max_{h}C^{\mathbf{a}}_{h}/\gamma)^{HSA}\). Further,

\[\left|\left(\pi(a|s)\wedge C^{\mathbf{a}}_{h}\pi_{h}^{D}(a|s)\right)-\left( \overline{\pi}_{h}(a|s)\wedge C^{\mathbf{a}}_{h}\pi_{h}^{D}(a|s)\right)\right| \leq\gamma\pi_{h}^{D}(a|s),\]

thus \(\|\frac{\left(\pi\wedge C^{\mathbf{a}}_{h}\pi_{h}^{D}\right)-\left(\overline{ \pi}_{h}\wedge C^{\mathbf{a}}_{h}\pi_{h}^{D}\right)}{\pi_{h}^{D}}\|_{\infty}\leq\gamma\), and applying Lem. C.11 gives the result. 

**Lemma C.11**.: _Suppose \(\overline{\Pi}_{\Theta}\) satisfies Def. C.1 with \(\sigma xc=(x\wedge c)\). Then for any \(\pi\in\Pi_{\Theta}\), let \(\overline{\pi}\in\overline{\Pi}_{\Theta}\) be its cover. Under Asm. 4.1, we have_

\[\left\|\frac{\sigma\left(\pi,C\pi^{D}\right)}{\pi^{D}}-\frac{\sigma\left( \overline{\pi},C\pi^{D}\right)}{\pi^{D}}\right\|_{\infty}\leq C(\gamma+D_{ \sigma}).\]

Proof of Lem. C.11.: If \(\pi(a|x)\leq C\pi^{D}(a|x)\), using the 1-Lipschitzness of \(\sigma\) we have

\[|\sigma\left(\pi(a|s),C\pi^{D}(a|s)\right)- \sigma\left(\bar{\pi}(a|s),C\pi^{D}(a|s)\right)|\] \[= |\sigma\left(\left(\pi(a|s)\wedge C\pi^{D}(a|s)\right),C\pi^{D}(a| s)\right)-\sigma\left(\bar{\pi}(a|s),C\pi^{D}(a|s)\right)|\] \[\leq |\left(\pi(a|s)\wedge C\pi^{D}(a|s)\right)-\bar{\pi}(a|s)|\]\[\leq C\gamma\pi^{D}(a|s).\]

If \(\pi(a|x)>C\pi^{D}(a|x)\),

\[|\sigma\left(\pi(a|s),C\pi^{D}(a|s)\right)-\sigma\left(\bar{\pi}(a|s),C\pi^{D}( a|s)\right)| \leq C\pi^{D}(a|s)-\sigma\left(\bar{\pi}(a|s),C\pi^{D}(a|s)\right)\] \[\leq C\pi^{D}(a|s)-(1-D_{\sigma})\left(\bar{\pi}(a|s)\wedge C\pi^ {D}(a|s)\right)\] \[\leq C(D_{\sigma}+\gamma)\pi^{D},\]

using Ass. 4.1 in the second inequality. As a result,

\[\left|\frac{\sigma\left(\pi(a|s),C\pi^{D}(a|s)\right)}{\pi^{D}(a|s)}-\frac{ \sigma\left(\bar{\pi}(a|s),C\pi^{D}(a|s)\right)}{\pi^{D}(a|s)}\right|\leq C( \gamma+D_{\sigma})\]

## Appendix D Maximum Likelihood Estimation

Algorithm 4 displays the data distribution estimation procedure used in offline gradient estimation (Algorithm 2), which is a direct application of MLE. The general formulation of the MLE problem utilized in this paper is to estimate a probability distribution over the instance space \(\mathcal{S}\). Given an i.i.d. sampled dataset \(\mathcal{D}=\{s^{(i)}\}_{i=1}^{n}\) and a function class \(\mathcal{F}\), we optimize the MLE objective of the form

\[\widehat{f}=\operatorname*{argmin}_{f\in\mathcal{F}}\frac{1}{|\mathcal{D}|} \sum_{s\in\mathcal{D}}\log\left(f(s)\right).\] (23)

We assume \(\mathcal{F}\) is finite, and refer readers to [10, 11] for techniques for handling infinite function classes. The general MLE guarantee is stated below, and is a well-established result (for example, a proof can be found in Appendix E of [1]).

**Lemma D.1** (MLE guarantee).: _Let \(\mathcal{D}=\{s^{(i)}\}_{i=1}^{n}\) be a dataset, where \(s^{(i)}\) are drawn i.i.d. from some fixed probability distribution \(f^{*}\) over \(\mathcal{S}\). Consider a function class \(\mathcal{F}\) that satisfies: (i) \(f^{*}\in\mathcal{F}\), and (ii) each function \(f\in\mathcal{F}\) is a valid probability distribution over \(\mathcal{S}\) (i.e., \(f\in\Delta(\mathcal{S})\)) Then with probability at least \(1-\delta\), \(\widehat{f}\) from Eq. (23) has \(\ell_{1}\) error guarantee_

\[\|\widehat{f}-f^{*}\|_{1}\leq\sqrt{\frac{2\log(|\mathcal{F}|/\delta)}{n}}.\]

The formal guarantee of Algorithm 4 is stated below, which is a straightforward application of Lemma D.1 with union bound (over all functions in \(\mathcal{F}\), and over all timesteps).

**Assumption D.1** (MLE Realizability).: Suppose that \(\forall h\in[h]\), we have \(d_{h}^{D},d_{h-1}^{D,\dagger}\in\mathcal{F}_{h}\) for \(\mathcal{D}\) defined in Def. 4.1. Additionally, \(f\in\Delta(\mathcal{S})\) is a valid distribution for all \(f\in\mathcal{F}_{h}\).

**Lemma D.2**.: _Suppose \(\{\mathcal{F}_{h}\}\) satisfies Asm. D.1. Then with probability at least \(1-\delta\), for all \(h\in[H]\) the outputs of Algorithm 4 satisfy \(\left\|\widehat{d}_{h}^{\mathcal{D}}-d_{h}^{\mathcal{D}}\right\|_{1}\leq \varepsilon^{\mathrm{mle}}\) and \(\left\|\widehat{d}_{h}^{\mathcal{D},\dagger}-d_{h}^{\mathcal{D},\dagger} \right\|_{1}\leq\varepsilon^{\mathrm{mle}}\), where_

\[\varepsilon^{\mathrm{mle}}:=\sqrt{\frac{2\log(2H|\mathcal{F}|/\delta)}{n}}.\]

## Appendix E Offline Density Estimation

The algorithm for offline density estimation is displayed in Alg. 5, and is directly copied from Algorithm 1 of [16], but with two minor modifications. The first is that the densities are clipped using a function \(\sigma\), that can take clipping as a special case. The second is that it outputs the learned weights instead of the learned densities. The weight function class completeness assumption is shown Asm. E.1, and is satisfied in low-rank MDPs using linear-over-linear function classes that have pseudo-dimension bounded by MDP rank. It can be seen as a 1-dimensional version of Asm. 4.2 where \(\rho=1\) and in that sense strictly weaker.

**Assumption E.1** (Weight function completeness).: For any \(\pi\in\Pi_{\Theta}\) and \(h\in[H]\), we have

\[\mathbf{E}_{h-1}^{D,1}\left(\frac{\sigma\left(w\cdot f^{\prime},C_{h-1}^{ \mathsf{a}}f\right)}{f}\frac{\pi_{h-1}}{\pi_{h-1}^{\mathsf{a}}}\right)\in \mathcal{W}_{h},\;\forall w\in\mathcal{W}_{h-1},\;\forall f,f^{\prime}\in \mathcal{F}_{h-1},\]

**Theorem E.1**.: _Suppose \(\sigma\) satisfies Asm. 4.1 and \(\mathcal{W}\) satisfies Asm. E.1. Let \(\{\widehat{d}_{h}^{\mathcal{D}}\}g\) and \(\{\widehat{d}_{h}^{\mathcal{D},\dagger}\}\) be such that \(\forall h\in[H]\),_

\[\left\|\widehat{d}_{h}^{\mathcal{D}}-d_{h}^{\mathcal{D}}\right\|_{1}\leq \varepsilon^{\mathrm{mle}}\quad\text{and}\quad\left\|\widehat{d}_{h}^{ \mathcal{D},\dagger}-d_{h}^{\mathcal{D},\dagger}\right\|_{1}\leq\varepsilon^{ \mathrm{mle}}.\]

_Then with probability at least \(1-\delta\), the outputs \(\{\widehat{w}_{h}^{\pi}\}\) of Algorithm 5 satisfy for all \(h\in[H]\)_

\[\left\|\widehat{w}_{h}^{\pi}-\widehat{w}_{h}^{\pi}\right\|_{1,d_{h-1}^{ \mathcal{D},\dagger}}\leq\left(\sum_{g<h-1}C_{g}^{\mathsf{a}}C_{g}^{\mathsf{ a}}+2\sum_{g<h}C_{g}^{\mathsf{a}}\right)\varepsilon^{\mathrm{mle}}+\sqrt{2} \left(\sum_{g<h}C_{g}^{\mathsf{a}}C_{g}^{\mathsf{a}}\right)\varepsilon^{ \mathrm{wreg}},\] (25)

_where \(\varepsilon^{\mathrm{wreg}}:=\sqrt{\frac{c\log(H|\mathcal{W}|/\delta)}{n_{ \mathrm{reg}}}}\) for some absolute constant \(c\)._

Proof of Theorem E.1We begin by stating the following decomposition on the error of \(\widehat{w}_{h}^{\pi}\), which is proved at the end of this section.

**Lemma E.1**.: _Suppose \(\sigma\) satisfies Assumption 4.1. Then for any \(h\in[H]\), the error between \(\widehat{w}_{h}^{\pi}\) and the target \(\widetilde{w}_{h}^{\pi}=\widehat{d}_{h}^{\pi}/d_{h-1}^{\mathcal{D},\dagger}\) can be recursively decomposed as_

\[\left\|\widehat{w}_{h}^{\pi}-\widetilde{w}_{h}^{\pi}\right\|_{1,d_{h-1}^{ \mathcal{D},\dagger}}\leq\left\|\widehat{w}_{h-1}^{\pi}-\widetilde{w}_{h-1}^ {\pi}\right\|_{1,d_{h-2}^{\mathcal{D},\dagger}}\]\[\left\|\widehat{w}_{h}^{\pi}-\mathbf{E}_{h-1}^{\bar{\pi}}\left(d_{h-1 }^{D}\frac{\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s}}\widehat{d}_{ h-1}^{D}\right)}{\widehat{d}_{h-1}^{D}}\right)\right\|_{1,d_{h-1}^{D,\dagger}}\] \[=\left\|\mathbf{E}_{h-1}\left(\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s}}d_{h-1}^{D}\right)\right)-\mathbf{E}_{h-1}^{\bar{\pi}} \left(d_{h-1}^{D}\frac{\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s}} \widehat{d}_{h-1}^{D}\right)}{\widehat{d}_{h-1}^{D}}\right)\right\|_{1,d_{h-1 }^{D,\dagger}}\] \[=\left\|\mathbf{P}_{h-1}\left(\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s}}d_{h-1}^{D}\right)\right)-\mathbf{P}_{h-1}^{\bar{\pi}} \left(d_{h-1}^{D}\frac{\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s} }\widehat{d}_{h-1}^{D}\right)}{\widehat{d}_{h-1}^{D}}\right)\right\|_{1}\] \[\leq\left\|\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s }}d_{h-1}^{D}\right)-d_{h-1}^{D}\frac{\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h -1}^{\mathbf{s}}\widehat{d}_{h-1}^{D}\right)}{\widehat{d}_{h-1}^{D}}\right\|_ {1}\]\[\leq C_{h-1}^{\mathbf{s}}\left\|\widehat{d}_{h-1}^{D}-d_{h-1}^{D} \right\|_{1}+\left\|\sigma\left(\widetilde{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s}}d_ {h-1}^{D}\right)-\sigma\left(\widehat{d}_{h-1}^{\pi},C_{h-1}^{\mathbf{s}} \widehat{d}_{h-1}^{D}\right)\right\|_{1}\] \[\leq 2C_{h-1}^{\mathbf{s}}\left\|\widehat{d}_{h-1}^{\mathcal{D}}- d_{h-1}^{D}\right\|_{1}+\left\|\widetilde{d}_{h-1}^{\pi}-\widehat{d}_{h-1}^{ \pi}\right\|_{1}\] (Assumption 4.1)

Finally, since \(\widehat{d}_{h-1}^{\pi}=\widehat{w}_{h-1}^{\pi}\widehat{d}_{h-2}^{\mathcal{D}}\),

\[\left\|\widetilde{d}_{h-1}^{\pi}-\widehat{d}_{h-1}^{\pi}\right\|_ {1} =\left\|\widetilde{w}_{h-1}^{\pi}d_{h-2}^{D,\dagger}-\widehat{w}_{h -1}^{\pi}\widehat{d}_{h-2}^{D,\dagger}\right\|_{1}\] \[\leq C_{h-2}^{\mathbf{s}}C_{h-2}^{\mathbf{a}}\left\|d_{h-2}^{D, \dagger}-\widehat{d}_{h-2}^{D,\dagger}\right\|_{1}+\left\|\widetilde{w}_{h-1 }^{\pi}-\widehat{w}_{h-1}^{\pi}\right\|_{1,d_{h-2}^{D,\dagger}}\]

Combining the inequalities completes the proof. 

## Appendix F Probabilistic Tools

**Definition F.1** (Pseudodimension).: Suppose a function class \(\mathcal{F}\subseteq\mathbb{R}^{\mathcal{X}}\), and \(x_{1}^{n}=\{x_{i}\}_{i=1}^{n}\in\mathcal{X}^{n}\). We say \(x_{1}^{n}\) is pseudo-shattered by \(\mathcal{F}\) if there exists \(v\in\mathbb{R}^{n}\) such that for all \(y\in\{-1,+1\}^{n}\), there exists \(f\in\mathcal{F}\) such that \(\operatorname{sign}(f(x_{1}^{n}-c))=y\). The pseudo-dimension of \(\mathcal{F}\) is defined as

\[\mathsf{d}_{\mathcal{F}}=\max\{n\in\mathbb{N}:\exists x_{1}^{n}\in\mathcal{X}^ {n}\text{ s.t. }x_{1}^{n}\text{ is pseudo-shattered by }\mathcal{F}\},\]

i.e., the cardinality of the largest set of points in \(\mathcal{X}\) that \(\mathcal{F}\) pseudo-shatters.

**Lemma F.1** (Lemma 26 from [16]).: _For \(b\geq 1\), let \(\mathcal{H}\subseteq(\mathcal{Z}\to[-b,b])\) be a hypothesis class and \(Z^{n}=(z_{1},\ldots,z_{n})\in\mathcal{Z}^{n}\), where \(z_{i}\) are iid samples drawn from a distribution supported on \(\mathcal{Z}\). Then for any \(h\in\mathcal{H}\), we have_

\[\mathbb{P}\left(\left|\mathbb{E}[h(z)]-\frac{1}{n}\sum_{i}h(z_{i})\right|> \varepsilon\right)\leq 36\mathcal{N}_{1}\left(\frac{\varepsilon^{3}}{640b}, \mathcal{H},\frac{40nb^{2}}{\varepsilon^{2}}\right)\exp\left(-\frac{n \varepsilon^{2}}{128\mathbb{V}[h(z)]+512\varepsilon b}\right)\]

**Lemma F.2**.: _Fix \(\pi\). For any \(h\in[H]\), consider functions \(y_{h}:\mathcal{S}\times\mathcal{A}\to[-hG,hG]^{\mathsf{p}}\) and \(\rho_{h}:\mathcal{S}\times\mathcal{A}\to[0,C_{h}^{\mathbf{s}}C_{h}^{\mathbf{s}}]\) that depend only on the datasets \(\mathcal{D}_{<h}^{\mathrm{mle}}\) and \(\mathcal{D}_{<h}^{\mathrm{FORC}}\) and \(\mathcal{D}_{<h}^{\mathrm{grad}}\). Let \(\mathcal{G}=\{\mathcal{G}_{h}\}\) be function classes and with pseudo-dimension \(\mathsf{d}_{\mathcal{G}}\) (Def. F.1). For any \(g_{h+1}\in\mathcal{G}_{h+1}\) and \(p\in[\mathsf{p}]\), define the loss function_

\[\mathcal{L}_{h}^{\pi,p}(g_{h+1};y_{h},\rho_{h})=\frac{1}{n}\sum_{(s,a,s^{ \prime})\in\mathcal{D}_{h}^{\mathrm{reg}}}\rho_{h}(s,a)\left(g_{h+1}^{p}(s^{ \prime})-(\nabla\log\widetilde{\pi}_{h}(a|s)+y_{h}^{p}(s,a))\right)^{2}.\]

_Then with probability at least \(1-\delta\), for all \(g_{h+1}\in\mathcal{G}_{h+1}\) and \(p\in[\mathsf{p}]\) and \(h\in[H]\), we have_

\[\left|\mathbb{E}[\mathcal{L}_{h}^{\pi,p}(g_{h+1};y_{h},\rho_{h})- \mathcal{L}_{h}^{\pi,p}(g_{h+1}^{*};y_{h},\rho_{h})]-\mathcal{L}_{h}^{\pi,p}(g _{h+1}^{*};y_{h},\rho_{h})-\mathcal{L}_{h}^{\pi,p}(g_{h+1}^{*};y_{h},\rho_{h})\right|\] \[\qquad\leq\frac{1}{2}\mathbb{E}[\mathcal{L}_{h}^{\pi,p}(g_{h+1};y _{h},\rho_{h})-\mathcal{L}_{h}^{\pi,p}(g_{h+1}^{*};y_{h},\rho_{h})]+(\varepsilon _{h+1}^{\mathrm{reg}})^{2},\]

_where \(g_{h+1}^{*}=\mathbf{E}_{h}^{D,\rho}(\nabla\log\widetilde{\pi}_{h}+y_{h})\) and for some absolute constant \(c\),_

\[\varepsilon_{h}^{\mathrm{reg}}=c\sqrt{\frac{\mathsf{d}_{\mathcal{G}}C_{h-1}^{ \mathbf{s}}C_{h-1}^{\mathbf{a}}h^{2}G^{2}\log(n\mathsf{p}H/\delta)}{n}}.\] (26)

Proof.: Fix \(\mathcal{D}_{<h}^{\mathrm{mle}}\) and \(\mathcal{D}_{<h}^{\mathrm{FORC}}\) and \(\mathcal{D}_{<h}^{\mathrm{grad}}\). We first prove the stated bound conditioned on these datasets, which means \(g_{h}^{\pi}\) and \(\rho_{h}^{\pi}\) are fixed, and the randomness below comes from random draws of \(\mathcal{D}_{h}^{\mathrm{grad}}\). Consider the following hypothesis class induced by \(\mathcal{G}_{h+1}\):

\[\mathcal{Z}(\rho_{h},g_{h+1},y_{h})=\left\{\rho_{h}(s,a)\left(\left(g_{h+1}(s^{ \prime})-y_{h}(s,a)\right)^{2}-\left(g_{h+1}^{*}(s^{\prime})-y_{h}(s,a)\right)^{2 }\right):g_{h+1}\in\mathcal{G}_{h+1}\right\}.\]

and for any \(Z\in\mathcal{Z}\), we have \(\left|Z\right|\leq 2C_{h}^{\mathbf{s}}C_{h}^{\mathbf{s}}\left(\left\|g_{h+1} \right\|_{\infty}^{2}+\left\|y_{h}\right\|_{\infty}^{2}\right)\leq 4C_{h}^{\mathbf{s}}C_{h}^{ \mathbf{s}}h^{2}G^{2}\). We also have \(\mathbb{E}[Z^{p}(\rho,g_{h+1},y_{h})]=\left\|y_{h+1}^{p}-g_{h+1}^{*,p}\right\| _{2,f_{h}^{\pi}}^{2}\). Further,

\[\mathbb{V}\left[Z^{p}(\rho_{h},g_{h+1},y_{h})\right]\leq\mathbb{E}[Z^{p}(\rho_{h},g_{h+1},y_{h})^{2}]\]\[=\mathbb{E}\left[\rho_{h}(s,a)^{2}\left(\left(g_{h+1}^{p}(s^{\prime}) -y_{h}(s,a)\right)^{2}-\left(g_{h+1}^{*,p}(s^{\prime})-y_{h}(s,a)\right)^{2} \right)^{2}\right]\] \[=\mathbb{E}\left[\rho_{h}(s,a)^{2}\left(g_{h+1}^{p}(s^{\prime})-2 y_{h}(s,a)+g_{h+1}^{*,p}(s^{\prime})\right)^{2}\left(g_{h+1}^{p}(s^{\prime})-g_{h +1}^{*,p}(s^{\prime})\right)^{2}\right]\] \[\leq 16C_{h}^{\mathbf{s}}C_{h}^{\mathbf{a}}h^{2}G^{2}\mathbb{E} \left[\rho_{h}(s,a)\left(g_{h+1}^{p}(s^{\prime})-g_{h+1}^{*,p}(s^{\prime}) \right)^{2}\right]\] \[=16C_{h}^{\mathbf{s}}C_{h}^{\mathbf{a}}h^{2}G^{2}\mathbb{E}[Z^{p} (\rho_{h},g_{h+1},y_{h})]\]

Next, we show that the uniform covering number of \(\mathcal{Z}\) can be bounded by the uniform covering number of \(\mathcal{G}\), since for any \(g_{h+1},g_{h+1}^{\prime}\in\mathcal{G}_{h+1}\) we have

\[\left|Z^{p}(\rho_{h},g_{h+1},y_{h})-Z^{p}(\rho_{h},g_{h+1}^{\prime },y_{h})\right| =\rho_{h}(s,a)\left|(g_{h+1}(s^{\prime})-y_{h}(s,a))^{2}-(g_{h+1} ^{\prime}(s^{\prime})-y_{h}(s,a))^{2}\right|\] \[\leq 16C_{h}^{\mathbf{s}}C_{h}^{\mathbf{a}}(h+1)^{2}G^{2}|g_{h+1} (s^{\prime})-g_{h+1}^{\prime}(s^{\prime})|\]

In other words, any \(\gamma/16C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}}h^{2}G^{2}\) covering of \(\mathcal{G}_{h}\) is a covering of \(\mathcal{Z}_{h}\). Then combining the above with Lem. F.1, we have

\[\mathbb{P}\left(\left|\mathbb{E}[Z^{p}(\rho_{h-1},g_{h},y_{h-1})] -\frac{1}{n}\sum_{i}Z_{i}^{p}(\rho_{h-1},g_{h},y_{h-1})\right|>\varepsilon\right)\] \[\leq 36\mathcal{N}_{1}\left(\frac{\varepsilon^{3}}{10240C_{h}^{ \mathbf{s}}C_{h}^{\mathbf{a}}h^{2}G^{2}},\mathcal{Z}(\mathcal{G}_{h},\rho_{h- 1},y_{h-1}),\frac{640n(C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}})^{2}h^{4}G^{4} }{\varepsilon^{2}}\right)\] \[\quad\cdot\exp\left(-\frac{n\varepsilon^{2}}{2048C_{h-1}^{ \mathbf{s}}C_{h-1}^{\mathbf{a}}h^{2}G^{2}\mathbb{E}[Z^{p}(\rho_{h-1},g_{h},y_{ h-1},\pi)]+2048\varepsilon C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}}h^{2}G^{2}}\right)\] \[\leq 36\mathcal{N}_{1}\left(\frac{\varepsilon^{3}}{163840(C_{h}^{ \mathbf{s}}C_{h}^{\mathbf{a}})^{2}h^{4}G^{4}},\mathcal{G}_{h},\frac{640n(C_{h- 1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}})^{2}h^{4}G^{4}}{\varepsilon^{2}}\right)\] \[\quad\cdot\exp\left(-\frac{n\varepsilon^{2}}{2048C_{h-1}^{ \mathbf{s}}C_{h-1}^{\mathbf{a}}h^{2}G^{2}\mathbb{E}[Z^{p}(\rho_{h-1},g_{h},y_{ h-1})]+2048\varepsilon C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}}h^{2}G^{2}}\right)\]

Define \(N:=\mathcal{N}_{1}\left(\frac{\varepsilon^{3}}{163840(C_{h}^{\mathbf{s}}C_{h} ^{\mathbf{s}})^{2}h^{4}G^{4}},\mathcal{G}_{h},\frac{640n(C_{h-1}^{\mathbf{s}}C_ {h-1}^{\mathbf{a}})^{2}h^{4}G^{4}}{\varepsilon^{2}}\right)\). Then setting the RHS equal to \(\delta^{\prime}\), this implies that

\[n=\frac{2048C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}}h^{2}G^{2}\left(\mathbb{E} [Z^{p}(\rho_{h-1},g_{h},y_{h-1})]+\varepsilon\right)\log\left(36N/\delta^{ \prime}\right)}{\varepsilon^{2}}\]

and

\[\varepsilon\leq\sqrt{\frac{2048C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}}h^{2}G ^{2}\mathbb{E}[Z^{p}(\rho_{h-1},g_{h},y_{h-1})]\log(36N/\delta^{\prime})}{n} }+\frac{2048C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}}h^{2}G^{2}\log(36N/\delta^{ \prime})}{n}.\]

Since \(n\geq\frac{2048C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}}h^{2}G^{2}}{\varepsilon}\), there exists an absolute constant \(c\) such that \(\log(36N/\delta^{\prime})\leq\text{cd}_{\mathcal{G}}\log(n/\delta^{\prime}).\) Then with probability at least \(1-\delta^{\prime}\),

\[\left|\mathbb{E}[Z^{p}(\rho_{h-1},g_{h},y_{h-1})]-\frac{1}{n}\sum _{i}Z_{i}^{p}(\rho_{h-1},g_{h},y_{h-1})\right|\] \[\leq\sqrt{\frac{2048\text{cd}_{\mathcal{G}_{h}}C_{h-1}^{\mathbf{s} }C_{h-1}^{\mathbf{a}}h^{2}G^{2}\mathbb{E}[Z^{p}(\rho_{h-1},g_{h},y_{h-1})]\log(n/ \delta^{\prime})}{n}}+\frac{2048\text{cd}_{\mathcal{G}}C_{h-1}^{\mathbf{s}}C_ {h-1}^{\mathbf{a}}h^{2}G^{2}\log(n/\delta^{\prime})}{n}\] \[\leq\frac{1}{2}\mathbb{E}[Z^{p}(\rho_{h-1},g_{h},y_{h-1})]+\frac{3 072\text{cd}_{\mathcal{G}}C_{h-1}^{\mathbf{s}}C_{h-1}^{\mathbf{a}}h^{2}G^{2} \log(n/\delta^{\prime})}{n}\]

Since the above bound holds for a fixed datasets, it also holds for the expectation over the datasets. Applying the above bound for all \(p\in[\mathsf{p}]\) and \(h\in[H]\) with \(\delta^{\prime}=\delta/H\mathsf{p}\) and taking the union bound, then plugging in the definition of \(Z^{p}\), gives the result. 

**Lemma F.3**.: _Fix \(h\) and denote the product class composed from \(\mathcal{G},\mathcal{W},\mathcal{F}\) to be_

\[\mathcal{Y}_{h}\times\mathcal{P}_{h}=\left\{\left(y,\rho):y=g\odot\mathbf{ \tilde{1}}\left(wf^{\prime},C_{h}^{\mathbf{s}}f\right),\rho=\frac{\sigma(wf^{ \prime},C_{h}^{\mathbf{s}}f)}{f},g_{h}\in\mathcal{G}_{h},w\in\mathcal{W}_{h},f \in\mathcal{F}_{h+1},f^{\prime}\in\mathcal{F}_{h}\right\}.\]_Fix \(\pi\) and \(p\in[\mathsf{p}]\), and define the loss function_

\[\mathcal{L}_{h}^{\pi,p}(g;y,\rho)=\frac{1}{n}\sum_{(s,a,s^{\prime})\in\mathcal{D }_{h}}\rho(s)^{\frac{\sigma\left(\pi(a|s),C_{h}^{\mathsf{a}}\pi_{h}^{D}(a|s) \right)}{\pi_{h}^{D}(a|s)}}\left(g^{p}(s^{\prime})-\left(\nabla^{p}\log\widetilde {\pi}(a|s)+y^{p}(s)\right)\right)^{2}.\]

_Then with probability at least \(1-\delta\), for all \(h\in[H],p\in[\mathsf{p}]\) and \(g\in\mathcal{G}_{h+1}\), \((y,\rho)\in\mathcal{Y}_{h}\times\mathcal{P}_{h}\), we have_

\[\left|\mathbb{E}[\mathcal{L}_{h}^{\pi,p}(g;y,\rho)-\mathcal{L}_{h} ^{\pi,p}(g_{h+1}^{*};y,\rho)]-\mathcal{L}_{h}^{\pi,p}(g;y,\rho)-\mathcal{L}_{h }^{\pi,p}(g_{h+1}^{*};y,\rho)\right|\] \[\qquad\leq\frac{1}{2}\mathbb{E}[\mathcal{L}_{h}^{\pi,p}(g;y,\rho )-\mathcal{L}_{h}^{\pi,p}(g_{h+1}^{*};y,\rho)]+(\varepsilon_{h+1}^{\mathrm{ reg}})^{2},\]

_where \(g_{h+1}^{*}=\mathbf{E}_{h}^{D,\rho}(\nabla\log\widetilde{\pi}_{h}+y_{h})\) and \(\varepsilon_{h}^{\mathrm{reg}}=O\left(\sqrt{\frac{C_{h-1}^{\mathsf{a}}C_{h-1}^ {\mathsf{a}}h^{2}G^{2}\log(\mathcal{N}_{\infty}(n^{-1},\mathcal{G})\mathsf{p} H\|\mathcal{W}\|\mathcal{F}|/\delta)}{n}}\right).\)_

Proof of Lem. F.3.: Use the same notation for \(Z^{p}\) as in the proof of Lem. F.2. Using Bernstein's inequality with a union bound over all \(\mathcal{W}_{h}\) and \(\mathcal{F}_{h}\) and the \(\ell_{\infty}\) covers of \(\mathcal{G}_{h},\mathcal{G}_{h+1}\), with probability at least \(1-\delta\) we have

\[\left|\mathbb{E}[Z^{p}(\rho_{h},g_{h+1},y_{h})]-\frac{1}{n}\sum_{ i=1}^{n}Z_{i}^{p}(\rho_{h},g_{h+1},y_{h})\right|\] \[\qquad\leq\sqrt{\frac{2\mathbb{W}\left[Z^{p}(\rho_{h},y_{h+1},g_{ h})\right]\log(|\mathcal{W}||\mathcal{F}|\mathcal{N}_{\infty}(\varepsilon, \mathcal{G})/\delta)}{n}}+\frac{4C_{h}^{\mathsf{s}}C_{h}^{\mathsf{a}}h^{2}G^{ 2}\log(|\mathcal{W}||\mathcal{F}|\mathcal{N}_{\infty}(\varepsilon,\mathcal{G} )/\delta)}{3n}\] \[\qquad\leq\sqrt{\frac{32C_{h}^{\mathsf{s}}C_{h}^{\mathsf{a}}h^{2} G^{2}\mathbb{E}[Z^{p}(\rho_{h},y_{h+1},g_{h})]\log(|\mathcal{W}||\mathcal{F}| \mathcal{N}_{\infty}(\varepsilon,\mathcal{G})/\delta)}{n}}+\frac{4C_{h}^{ \mathsf{s}}C_{h}^{\mathsf{a}}h^{2}G^{2}\log(\mathcal{N}_{\infty}(\varepsilon, \mathcal{G})|\mathcal{W}||\mathcal{F}|/\delta)}{3n}\]

By accounting for the \(\ell_{\infty}\) covering error, we then have

\[\left|\mathbb{E}[Z^{p}(\rho_{h},g_{h+1},y_{h})]-\frac{1}{n}\sum_{ i=1}^{n}Z_{i}^{p}(\rho_{h},g_{h+1},y_{h})\right|\] \[\qquad\leq\sqrt{\frac{32C_{h}^{\mathsf{s}}C_{h}^{\mathsf{a}}h^{2} G^{2}\mathbb{E}[Z^{p}(\rho_{h},g_{h+1},y_{h})]\log(|\mathcal{W}||\mathcal{F}| \mathcal{N}_{\infty}(\varepsilon,\mathcal{G})/\delta)}{n}}\] \[\qquad+\frac{4C_{h}^{\mathsf{s}}C_{h}^{\mathsf{a}}h^{2}G^{2} \log(\mathcal{N}_{\infty}(\varepsilon,\mathcal{G})|\mathcal{W}||\mathcal{F}|/ \delta)}{3n}+16C_{h}^{\mathsf{s}}C_{h}^{\mathsf{a}}h^{2}G^{2}\varepsilon\]

Using the AM-GM inequality with \(\varepsilon=O(1/C_{h}^{\mathsf{s}}C_{h}^{\mathsf{a}}h^{2}G^{2}n)\) gives the result. 

## Appendix G Optimization Tools

**Definition G.1** (Gradient mapping).: \[G^{\eta}(x,g):=\frac{1}{\eta}\left(x-\mathrm{Proj}_{\mathcal{X}}\left(x+\eta g \right)\right)\] (27)

**Lemma G.1** (Stationary convergence of PGD).: _Suppose \(f:\mathcal{X}\to\mathbb{R}\) is \(\beta\)-smooth over \(\mathcal{X}\), a nonempty closed and convex set, and that we have access to a gradient oracle such that \(\mathbb{E}[g(x)|x]=\nabla f(x)\) and \(\mathbb{E}[\|g(x)-\nabla f(x)\|^{2}|x]\leq\varepsilon^{2}\). Then if \(\eta=1/\beta\), we have_

\[\frac{1}{T}\sum_{t}\mathbb{E}\left[\|G^{\eta}(x^{(t)},\nabla f(x^{(t)}))\|^{2} \right]\leq\frac{4\beta(f_{0}-f^{*})}{T}+6\varepsilon^{2}\]

Proof.: For any \(x\), define \(x^{+}=\mathrm{Proj}_{\mathcal{X}}\left(x-\eta g(x)\right)\). Since \(x^{+}=\mathrm{prox}_{,I_{\mathcal{X}}}(x-\eta g(x))\), where \(I_{\mathcal{X}}\) is the indicator function for the set \(\mathcal{X}\), from Lem. G.2 we have

\[\left\langle x-\eta g(x)-x^{+},x-x^{+}\right\rangle\leq 0.\]Rearranging, this implies

\[\left\langle g(x),x^{+}-x\right\rangle+\frac{1}{\eta}\|x-x^{+}\|^{2}\leq 0.\]

Next, since \(f\) is \(\beta\)-smooth, for any \(x\) and \(x^{+}\) we have

\[f(x^{+}) \leq f(x)+\left\langle\nabla f(x),x^{+}-x\right\rangle+\frac{ \beta}{2}\left\|x-x^{+}\right\|^{2}\] \[=f(x)+\left\langle g(x),x^{+}-x\right\rangle+\left\langle\nabla f (x)-g(x),x^{+}-x\right\rangle+\frac{\beta}{2}\left\|x-x^{+}\right\|^{2}\] \[\leq f(x)+\eta\left\langle g(x)-\nabla f(x),G^{\eta}(x,g(x)) \right\rangle+\left(\frac{\eta^{2}\beta}{2}-\eta\right)\left\|G^{\eta}(x,g(x) )\right\|^{2}\] \[=f(x)+\eta\left\langle g(x)-\nabla f(x),G^{\eta}(x,\nabla f(x)) \right\rangle+\eta\left\langle g(x)-\nabla f(x),G^{\eta}(x,g(x))-G^{\eta}(x, \nabla f(x))\right\rangle\] \[\qquad\qquad+\left(\frac{\eta^{2}\beta}{2}-\eta\right)\|G^{\eta} (x,g(x))\|^{2}\]

where we substitute the definition of \(G^{\eta}(x,g(x))=\frac{1}{\eta}(x-x^{+})\) in the the second to last line. Notice that

\[\left\langle g(x)-\nabla f(x),G^{\eta}(x,g(x))-G^{\eta}(x,\nabla f (x))\right\rangle \leq\|g(x)-\nabla f(x)\|\|G^{\eta}(x,g(x))-G^{\eta}(x,\nabla f(x))\|\] \[\leq\|g(x)-\nabla f(x)\|^{2}\]

from the non-expansion of the projection operator. Then we have

\[f(x^{+})\leq f(x)+\eta\left\langle g(x)-\nabla f(x),G^{\eta}(x,\nabla f(x)) \right\rangle+\eta\|g(x)-\nabla f(x)\|^{2}+\left(\frac{\eta^{2}\beta}{2}-\eta \right)\|G^{\eta}(x,g(x))\|^{2}\]

Next, we take the expectation of both sides conditioned on \(x\).

\[\mathbb{E}[f(x^{+})|x] \leq f(x)+\eta\left\langle\mathbb{E}[g(x)|x]-\nabla f(x),G^{\eta} (x,\nabla f(x))\right\rangle\] \[\qquad\qquad+\eta\mathbb{E}[\|g(x)-\nabla f(x)\|^{2}|x]+\left( \frac{\eta^{2}\beta}{2}-\eta\right)\mathbb{E}[\|G^{\eta}(x,g(x))\|^{2}|x]\] \[\leq f(x)+\eta\varepsilon^{2}+\left(\frac{\eta^{2}\beta}{2}-\eta \right)\mathbb{E}[\|G^{\eta}(x,g(x))\|^{2}|x]\]

Then unrolling the recursion through iterations and substituting \(\eta=1/\beta\), we have

\[\frac{1}{T}\sum_{t}\mathbb{E}[\|G^{\eta}(x^{(t)},g(x^{(t)}))\|^{2}]\leq\frac{ 2\beta(f(x^{(0)})-f(x^{(T)}))}{T}+2\varepsilon^{2}\leq\frac{2\beta(f(x^{(0)}) -f(x^{*}))}{T}+2\varepsilon^{2}\]

if \(f\) is nonnegative. Lastly,

\[\frac{1}{T}\sum_{t}\mathbb{E}[\|G^{\eta}(x^{(t)},\nabla f(x^{(t)} ))\|^{2}] =\frac{1}{T}\sum_{t}\mathbb{E}[\|G^{\eta}(x^{(t)},g(x^{(t)})-G^{ \eta}(x^{(t)},g(x^{(t)})+G^{\eta}(x^{(t)},\nabla f(x^{(t)}))\|^{2}]\] \[\leq\frac{2}{T}\sum_{t}\mathbb{E}[\|G^{\eta}(x^{(t)},g(x^{(t)})\| ^{2}+\frac{2}{T}\sum_{t}\mathbb{E}[\|G^{\eta}(x^{(t)},g(x^{(t)})-G^{\eta}(x^{( t)},\nabla f(x^{(t)}))\|^{2}]\] \[\leq\frac{4\beta(f(x^{(0)})-f(x^{*}))}{T}+4\varepsilon^{2}+\frac{ 2}{T}\sum_{t}\mathbb{E}[\|g(x^{(t)})-\nabla f(x^{(t)})\|^{2}]\] \[\leq\frac{4\beta(f(x^{(0)})-f(x^{*}))}{T}+6\varepsilon^{2}\]

**Lemma G.2** (Theorem 6.39 from [1]).: _Let \(g:\mathcal{E}\to(\infty,\infty]\) be a proper closed and convex function. Then for any \(x,y\in\mathcal{E}\), the following three claims are equivalent:_

1. \(y=\operatorname{prox}_{g}(x)\)2. \(x-y\in\partial g(u)\)__
3. \(\langle x-y,u-y\rangle\leq g(u)-g(y)\) _for any_ \(u\in\mathcal{E}\)__

**Lemma G.3**.: _Suppose \(f\) is \(M\)-gradient dominated and \(\beta\)-smooth, and_

\[\frac{1}{T}\sum_{t}\left\|G^{\eta}(x^{(t)},\nabla f(x^{(t)}))\right\|^{2}\leq \varepsilon^{2},\]

_where \(G^{\eta}(x,g)\) is the gradient mapping defined in Def. G.1. Also, suppose \(\|x-x^{\prime}\|_{2}\leq r\) for all \(x,x^{\prime}\in\mathcal{X}\). Then_

\[\min_{t\in[T]}\left\{f(x^{*})-f(x^{(t)})\right\}\leq rM(\eta\beta+1)\varepsilon.\] (28)

Proof of Lemma G.3.: If \(f\) is gradient dominated, for any \(t\in[T]\) we have

\[f(x^{*})-f(x^{(t)})\leq M\max_{x^{\prime}\in\mathcal{X}}\left\langle\nabla f( x^{(t)}),x^{\prime}-x^{(t)}\right\rangle.\]

Applying Lemma G.4 with \(-f\), we have

\[\nabla f(x^{(t)})\in N_{\mathcal{X}}(x^{(t)})+\mathcal{B}\left((\eta\beta+1) \|G^{\eta}(x^{(t-1)},\nabla f(x^{(t-1)}))\|\right)\]

From the definition of the normal cone, we have \(\left\langle v,x^{\prime}-x^{(t)}\right\rangle\leq 0\) for any \(v\in N_{\mathcal{X}}(x^{(t)})\) and \(x^{\prime}\in\mathcal{X}\). Then for any \(x^{\prime}\in\mathcal{X}\),

\[\left\langle\nabla f(x^{(t)}),x^{\prime}-x^{(t)}\right\rangle\leq(\eta\beta+1 )\left\|G^{\eta}(x^{(t-1)},\nabla f(x^{(t-1)}))\right\|\|x-x^{(t)}\|\leq(\eta \beta+1)r\left\|G^{\eta}(x^{(t-1)},\nabla f(x^{(t-1)}))\right\|\]

Combining the above inequalities,

\[\min_{t\in[T]}\left\{f(x^{*})-f(x^{(t)})\right\}\leq(\eta\beta+1)Mr\min_{t\in[ T]}\|G^{\eta}(x^{(t-1)},\nabla f(x^{(t-1)}))\|\leq(\eta\beta+1)Mr\varepsilon.\]

**Lemma G.4** (Lemma 3 from [2]).: _Let \(f:\mathbb{R}^{d}\to(-\infty,\infty)\) be be \(\beta\)-smooth over a convex set \(\mathcal{X}\) For any \(t\in[T]\), consider \(x^{(t+1)}=\operatorname{Proj}_{\mathcal{X}}\left(x^{(t)}-\eta\nabla f(x^{(t)})\right)\). Then_

\[-\nabla f(x^{(t+1)})\in N_{\mathcal{X}}(x^{(t+1)})+\mathcal{B}\left((\eta \beta+1)\|G^{\eta}(x^{(t)},-\nabla f(x^{(t)})\|\right),\]

_where \(N_{\mathcal{X}}\) is the normal cone of \(\mathcal{X}\) and \(\mathcal{B}(r)=\{x\in\mathbb{R}^{d}:\|x\|_{2}\leq r\}\)._

Proof of Lemma G.4.: Projected gradient descent can be equivalently written as [1]

\[\operatorname{Proj}_{\mathcal{X}}\left(x^{(t)}-\eta\nabla f(x^{(t)})\right)= \operatorname*{argmin}_{x\in\mathbb{R}^{d}}\left[f(x^{(t)})+\left\langle \nabla f(x^{(t)}),x-x^{(t)}\right\rangle+\frac{1}{2\eta}\|x-x^{(t)}\|_{2}^{2}+I _{\mathcal{X}}(x)\right],\]

where \(I_{\mathcal{X}}(x)=0\) if \(x\in\mathcal{X}\), and \(+\infty\) otherwise, is the indicator function for \(\mathcal{X}\). Then by the subgradient optimality condition, we have

\[0\in\nabla f(x^{(t)})+\tfrac{1}{\eta}(x^{(t+1)}-x^{(t)})+N_{\mathcal{X}}(x^{( t+1)})\]

With some rearrangement, this implies that

\[-\nabla f(x^{(t+1)})\in N_{\mathcal{X}}(x^{(t+1)})+\nabla f(x^{(t)})-\nabla f( x^{(t+1)})+\tfrac{1}{\eta}(x^{(t+1)}-x^{(t)})\]

which implies the lemma statement since

\[\|\nabla f(x^{(t)})-\nabla f(x^{(t+1)})+\tfrac{1}{\eta}(x^{(t+1)} -x^{(t)})\| \leq\beta\|x^{(t)}-x^{(t+1)}\|+\tfrac{1}{\eta}\|x^{(t)}-x^{(t+1)}\|\] \[\leq(\eta\beta+1)\|G^{\eta}(x^{(t)},\nabla f(x^{(t)}))\|\]

using the \(\beta\)-smoothness of \(f\) in the first inequality, and Definition G.1 in the second.

**Lemma G.5**.: _Suppose \(f\) is \(\beta\)-smooth and that at each iteration \(t\), we have \(g^{(t)}\) from a gradient oracle such that \(\mathbb{E}\left[g^{(t)}|x^{(t)}\right]=\nabla f(x^{(t)})\) and \(\mathbb{E}\left[\|\nabla f(x^{(t)})-g^{(t)}\|^{2}|x^{(t)}\right]\leq\varepsilon^ {2}\) for all \(t\in[T]\). Then gradient ascent using \(\{g^{(t)}\}\) satisfies_

\[\frac{1}{T}\sum_{t=1}^{T}\mathbb{E}\left[\left\|\nabla f(x^{(t)})\right\|^{2} \right]\leq\frac{2\beta(f_{0}-f^{*})}{T}+\varepsilon^{2}.\] (29)

Proof of Lem. G.5.: From the \(\beta\)-smoothness of \(f\),

\[f(x^{(t+1)}) \leq f(x^{(t)})+\left\langle\nabla f(x^{(t)}),x^{(t+1)}-x^{(t)} \right\rangle+\frac{\beta}{2}\|x^{(t+1)}-x^{(t)}\|^{2}\] \[=f(x^{(t)})-\eta\left\langle\nabla f(x^{(t)}),g^{(t)}\right\rangle +\frac{\beta\eta^{2}}{2}\|g^{(t)}\|^{2}\] \[=f(x^{(t)})-\eta\left\langle\nabla f(x^{(t)}),\nabla f(x^{(t)})- \nabla f(x^{(t)})+g^{(t)}\right\rangle+\frac{\beta\eta^{2}}{2}\|\nabla f(x^{(t )})-\nabla f(x^{(t)})+g^{(t)}\|^{2}\] \[=f(x^{(t)})+\left(\frac{\beta\eta^{2}}{2}-\eta\right)\|\nabla f(x ^{(t)})\|^{2}+\left(\beta\eta^{2}-\eta\right)\left\langle\nabla f(x^{(t)}),g^{ (t)}-\nabla f(x^{(t)})\right\rangle+\frac{\beta\eta^{2}}{2}\|\nabla f(x^{(t)} )-g^{(t)}\|^{2}\]

Taking the expectations of both sides conditioned on \(x^{(t)}\) (prior histories), we have

\[\mathbb{E}[f(x^{(t+1)})|x^{(t)}]\leq f(x^{(t)})+\left(\frac{\beta\eta^{2}}{2} -\eta\right)\|\nabla f(x^{(t)})\|^{2}+\frac{\beta\eta^{2}\varepsilon^{2}}{2}\]

Substituting \(\eta=1/\beta\), unrolling through iterations, and using the law of total expectation gives the result.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The introduction contains a list of our contributions and matches our theoretical results. The abstract summarizes them. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We provide justifications for all of our assumptions, and discuss the ramifications of our theorems. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We fully disclose all assumptions and discuss their strengths and weaknesses. While theorems/lemmas in the main text are not cross-referenced with pointers to their proofs in the appendix, the appendix is organized with a table of contents according to section, and proofs are clearly labeled with the theorem or lemma they pertain to. We did not have space to provide proof sketches in the main text, but, where possible, we attempted to provide a brief intuition. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We do not have experimental results other than the graph in Figure 1, which is a plot of 1-D functions that are fully disclosed in the caption and referenced proposition. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

* We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
* **Open access to data and code*
* Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: We do not believe that Figure 1 constitutes as an experiment that requires code. Guidelines:
* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
* **Experimental Setting/Details*
* Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: Per our answer to the previous question, we do not have experiments. Guidelines:
* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
* **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: Per the answer to the previous question, we do not have experiments with statistical error. Guidelines:* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: Per the previous answer, we do not have experiments requiring computational resources. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We do not believe we deviate from the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA]Justification: Our paper is purely theoretical, and we do not believe there is a societal impact to be discussed. We do not see a direct path to negative applications. Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not use data or train models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: We do not have code, data, or models. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL.

* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.

13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: There are no new assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.

14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not use human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not use human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.