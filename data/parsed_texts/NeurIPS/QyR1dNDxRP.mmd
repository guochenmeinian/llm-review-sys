# Provable Tempered Overfitting of

Minimal Nets and Typical Nets

 Itamar Harel

Technion

itamarharel01@gmail.com &William M. Hoza

The University of Chicago

&Gal Vardi

Weizmann Institute of Science

&Itay Evron

Technion

&Nathan Srebro

Toyota Technological Institute at Chicago &Daniel Soudry

Technion

###### Abstract

We study the overfitting behavior of fully connected deep Neural Networks (NNs) with binary weights fitted to perfectly classify a noisy training set. We consider interpolation using both the smallest NN (having the minimal number of weights) and a random interpolating NN. For both learning rules, we prove overfitting is tempered. Our analysis rests on a new bound on the size of a threshold circuit consistent with a partial function. To the best of our knowledge, ours are the first theoretical results on benign or tempered overfitting that: (1) apply to deep NNs, and (2) do not require a very high or very low input dimension.

## 1 Introduction

Neural networks (NNs) famously exhibit strong generalization capabilities, seemingly in defiance of traditional generalization theory. Specifically, NNs often generalize well empirically even when trained to interpolate the training data perfectly [97]. This motivated an extensive line of work attempting to explain the overfitting behavior of NNs, and particularly their generalization capabilities when trained to perfectly fit a training set with corrupted labels (_e.g.,_[5, 28, 57, 48]).

In an attempt to better understand the aforementioned generalization capabilities of NNs, Mallinar et al. [57] proposed a taxonomy of benign, tempered, and catastrophic overfitting. An algorithm that perfectly interpolates a training set with corrupted labels, _i.e.,_ an interpolator, is said to have tempered overfitting if its generalization error is neither benign nor catastrophic -- not optimal but much better than trivial. However, the characterization of overfitting in NNs is still incomplete, especially in _deep_ NNs when the input dimension is neither very high nor very low. In this paper, we aim to understand the overfitting behavior of deep NNs in this regime.

We start by analyzing tempered overfitting in "min-size" NN interpolators, _i.e.,_ whose neural layer widths are selected to minimize the total number of weights. The number of parameters in a model is a natural complexity measure in learning theory and practice. For instance, it is theoretically well understood that \(L_{1}\) regularization in a sparse linear regression setting yields a sparse regressor. Practically, finding small-sized deep models is a common objective used in pruning (_e.g.,_[33]) and neural architecture search (_e.g.,_[54]). Recently, Manoj and Srebro [58] proved that the _shortest program_ (Turing machine) that perfectly interpolates noisy datasets exhibits tempered overfitting, illustrating how a powerful model can avoid catastrophic overfitting by returning a min-size interpolator.

Furthermore, we study tempered overfitting in random ("typical") interpolators -- NNs sampled uniformly from the set of parameters that perfectly fit the training set. Given a narrow teacher model and no label noise, Buzaglo et al. [13] recently proved that such typical interpolators, which may be _highly overparameterized_, generalize well. This is remarkable since these interpolators do not rely onexplicit regularization or the implicit bias of any gradient algorithm. An immediate question arises -- what kind of generalization behavior do typical interpolators exhibit in the presence of label noise? This is especially interesting in light of theoretical and empirical findings that typical NNs implement low-frequency functions [70, 83], while interpolating noisy training sets may require high frequencies.

For both the min-size and typical NN interpolators, we study the generalization behavior under an underlying _noisy_ teacher model. We focus on deep NNs with binary weights and activations (similar NNs are used in resource-constrained environments; _e.g.,_[42]). Our analysis reveals that these models exhibit a tempered overfitting behavior that depends on the statistical properties of the label noise. For independent noise, in addition to an upper bound we also find a lower bound on the expected generalization error. Our results are illustrated in Figure 1 below, in which the yellow line in the right panel is similar to empirically observed linear behavior (e.g., 57, Figures 2, 3, and 6).

The contributions of this paper are:

* Returning a min-size NN interpolator is a natural learning rule that follows the Occam's-razor principle. We show that this learning rule exhibits tempered overfitting (Section 4.1).
* We prove that overparameterized random NN interpolators typically exhibit tempered overfitting with generalization close to a min-size NN interpolator (Section 4.2).
* To the best of our knowledge, ours are the first theoretical results on benign or tempered overfitting that: (1) apply to deep NNs, and (2) do not require a very high or very low input dimension.
* The above results rely on a key technical result -- datasets generated by a constant-size teacher model with label noise can be interpolated1 using a NN of constant depth with threshold activations, binary weights, a width sublinear in \(N\), and roughly \(H(\varepsilon^{\star})\cdot N\) weights, where \(H(\varepsilon^{\star})\) is the binary entropy function of the fraction of corrupted labels (Section 3).

Figure 1: **Types of overfitting behaviors.** Consider a binary classification problem of learning a realizable distribution \(\mathcal{D}_{0}\). Let \(\mathcal{D}\) be the distribution induced by adding an \(\varepsilon^{\star}\)-probability for a data point’s label to be flipped relative to \(\mathcal{D}_{0}\). Suppose a model is trained with data from \(\mathcal{D}\). Then, assuming the classes are balanced, the trivial generalization performance is \(0.5\) (in gray; _e.g.,_ with a constant predictor). **Left.** Evaluating the model on \(\mathcal{D}\), a Bayes-optimal hypothesis (in red) obtains a generalization error of \(\varepsilon^{\star}\). For large enough training sets, our results (Section 4) dictate a tempered overfitting behavior illustrated above. For arbitrary noise, the error is approximately bounded by \(1-\varepsilon^{\star\varepsilon^{\star}}\left(1-\varepsilon^{\star}\right)^{1 -\varepsilon^{\star}}\) (blue). For independent noise, the error is concentrated around the tighter \(2\varepsilon^{\star}\left(1-\varepsilon^{\star}\right)\) (yellow). A similar figure was previously shown in Manoj and Srebro [58] for shortest-program interpolators. **Right.** Assuming independent noise, the left figure can be transformed into the error of the model on \(\mathcal{D}_{0}\) (see Lemma A.9). The linear behavior in the independent setting (yellow) is similar to the behavior observed empirically in Mallinar et al. [57, Figures 2, 3, and 6].

Setting

Notation.We reserve bold lowercase characters for vectors, bold uppercase characters for matrices, and regular uppercase characters for random elements. We use \(\log\) to denote the base 2 logarithm, and \(\ln\) to denote the natural logarithm. For a pair of vectors \(\underline{d}=\left(d_{1},\ldots,d_{L}\right),\underline{d}^{\prime}=\left(d_{ 1}^{\prime},\ldots,d_{L}^{\prime}\right)\in\mathbb{N}^{L}\) we denote \(\underline{d}\leq\underline{d}^{\prime}\) if for all \(l\in[L]\), \(d_{l}\leq d_{l}^{\prime}\). We use \(\oplus\) to denote the XOR between two binary \(\{0,1\}\) values, and \(\odot\) to denote the Hadamard (elementwise) product between two vectors. We use \(H\left(\mathcal{D}\right)\) to denote the entropy of some distribution \(\mathcal{D}\). Finally, we use \(\mathrm{Ber}\left(\varepsilon\right)\) for the Bernoulli distribution with parameter \(\varepsilon\), and \(H\left(\varepsilon\right)\) for its entropy, which is the binary entropy function.

### Model: Fully connected threshold NNs with binary weights

Similarly to Buzaglo et al. [13], we define the following model.

**Definition 2.1** (Binary threshold networks).: For a depth \(L\), widths \(\underline{d}=\left(d_{1},\ldots,d_{L}\right)\), input dimension \(d_{0}\), a scaled-neuron fully connected binary threshold NN, or binary threshold network, is a mapping \(\boldsymbol{\theta}\mapsto h_{\boldsymbol{\theta}}\) such that \(h_{\boldsymbol{\theta}}:\{0,1\}^{d_{0}}\rightarrow\{0,1\}^{d_{L}}\), parameterized by

\[\boldsymbol{\theta}=\left\{\mathbf{W}^{\left(l\right)},\mathbf{b}^{\left(l \right)},\boldsymbol{\gamma}^{\left(l\right)}\right\}_{l=1}^{L}\,,\]

where for every layer \(l\in[L]\),

\[\mathbf{W}^{\left(l\right)}\!\in\mathcal{Q}_{l}^{W}\!=\!\left\{0,1\right\}^{ d_{l}\times d_{l-1}},\;\boldsymbol{\gamma}^{\left(l\right)}\!\in\mathcal{Q}_{l}^{ \gamma}\!=\!\left\{-1,0,1\right\}^{d_{l}},\;\mathbf{b}^{\left(l\right)}\!\in \mathcal{Q}_{l}^{b}\!=\!\left\{-d_{l-1}+1,\ldots,d_{l-1}\right\}^{d_{l}}\;.\]

This mapping is defined recursively as \(h_{\boldsymbol{\theta}}\left(\mathbf{x}\right)=h^{\left(L\right)}\left( \mathbf{x}\right)\) where

\[h^{\left(0\right)}\left(\mathbf{x}\right) =\mathbf{x}\,,\] \[\forall l\in[L] h^{\left(l\right)}\left(\mathbf{x}\right) =\mathbb{I}\left\{\left(\boldsymbol{\gamma}^{\left(l\right)} \odot\left(\mathbf{W}^{\left(l\right)}h^{\left(l-1\right)}\left(\mathbf{x} \right)\right)+\mathbf{b}^{\left(l\right)}\right)>\mathbf{0}\right\}\,.\]

We denote the number of weights by \(w(\underline{d})=\sum_{l=1}^{L}d_{l}d_{l-1}\), and the total number of neurons by \(n\left(\underline{d}\right)=\sum_{l=1}^{L}d_{l}\). The total number of parameters in such a NN is \(M\left(\underline{d}\right)=w\left(\underline{d}\right)+2n\left(\underline{d}\right)\). We denote the set of functions representable as binary networks of widths \(\underline{d}\) by \(\mathcal{H}_{\underline{d}}^{\mathrm{BTN}}\) and their corresponding parameter space by \(\Theta^{\mathrm{BTN}}\left(\underline{d}\right)\).

_Remark 2.2_.: Our generalization results are for the above formulation of neuron scalars \(\boldsymbol{\gamma}\), _i.e._, ternary scaling _before_ the activation. However, we could have derived similar results if, instead, we changed the scale \(\boldsymbol{\gamma}\) to appear _after_ the activation and also adjusted the range of the biases (see Appendix G). Although we chose the former for simplicity, the latter is similar to the ubiquitous phenomenon in neuroscience known as "Dale's Law" [82]. This law, in a simplified form, means that all outgoing synapses of a neuron have the same effect, _e.g._, are all excitatory (positive) or all inhibitory (negative).

_Remark 2.3_ (Simple counting argument).: Let \(\underline{d}_{\max}\triangleq\max\left\{d_{1},\ldots,d_{L-1}\right\}\) be the maximal hidden-layer width. Then, combinatorially, it holds that

\[\log\underbrace{\left|\mathcal{H}_{\underline{d}}^{\mathrm{BTN}}\right|}_{ \boldsymbol{\#}\text{ hypotheses}}\leq\log\underbrace{\left|\Theta^{\mathrm{BTN}} \left(\underline{d}\right)\right|}_{\begin{subarray}{c}\boldsymbol{\#} \text{ parameter}\\ \text{assignments}\end{subarray}}\leq\underbrace{w\left(\underline{d} \right)}_{\boldsymbol{\#}\text{ weights}}+\underbrace{n\left(\underline{d} \right)}_{\begin{subarray}{c}\boldsymbol{\#}\text{ neurons}\\ \text{quantization}\end{subarray}}\left(\log(3)+\log\underbrace{\left(2d_{\max} \right)}_{\begin{subarray}{c}\boldsymbol{\#}\text{ quantization}\end{subarray}}\right).\]

This implies, using classical PAC bounds [75], that the sample complexity of learning with the _finite_ hypothesis class \(\mathcal{H}_{\underline{d}}^{\mathrm{BTN}}\) is \(O\left(w\left(\underline{d}\right)+n\left(\underline{d}\right)\log\underline {d}_{\max}\right)\) (a more refined bound on \(\left|\mathcal{H}_{\underline{d}}^{\mathrm{BTN}}\right|\) is given in Lemma F.1). In Section 4 we show how this generalization bound can be improved in our setting.

### Data model: A teacher network and label-flip noise

Data distribution.Let \(\mathcal{X}=\{0,1\}^{d_{0}}\) and let \(\mathcal{D}\) be some joint distribution over a finite sample space \(\mathcal{X}\times\{0,1\}\) of features and labels.

**Assumption 2.4** (Teacher assumption).: We assume a "teacher NN" \(h^{\star}\) generating the labels. A label flipping noise is then added with a noise level of \(\varepsilon^{\star}=\mathbb{P}_{\left(X,Y\right)\sim\mathcal{D}}\left(Y\neq h^{ \star}(X)\right)\), or equivalently

\[Y\oplus h^{\star}\left(X\right)\sim\mathrm{Ber}\left(\varepsilon^{\star}\right)\,.\]

The label noise is _independent_ when \(Y\oplus h^{\star}\left(X\right)\) is independent of the features \(X\) (in Section 4 it leads to stronger generalization results compared to ones for arbitrary noise).

### Learning problem: Classification with interpolators

We consider the problem of binary classification over a training set \(S=\{\left(\mathbf{x}_{i},y_{i}\right)\}_{i=1}^{N}\) with \(N\) data points, sampled from the noisy joint distribution \(\mathcal{D}\) described above. We always assume that \(S\) is sampled i.i.d., and therefore, with some abuse of notation, we use \(\mathcal{D}\left(S\right)=\mathcal{D}^{N}\left(S\right)=\prod_{i=1}^{N} \mathcal{D}\left(\mathbf{x}_{i},y_{i}\right)\). For a hypothesis \(h:\mathcal{X}\rightarrow\{0,1\}\), we define the risk, _i.e.,_ the generalization error w.r.t. \(\mathcal{D}\), as

\[\mathcal{L}_{\mathcal{D}}\left(h\right)\triangleq\mathbb{P}_{\left(X,Y\right) \sim\mathcal{D}}\left(h(X)\neq Y\right)\,.\]

We also define the empirical risk, _i.e.,_ the training error,

\[\mathcal{L}_{S}\left(h\right)\triangleq\frac{1}{N}\sum_{n=1}^{N}\mathbb{I} \left\{h(\mathbf{x}_{n})\neq y_{n}\right\}\,.\]

We say a hypothesis is an _interpolator_ if \(\mathcal{L}_{S}\left(h\right)=0\).

In this paper, we are specifically interested in _consistent_ datasets that can be perfectly fit. This is formalized in the following definition.

**Definition 2.5** (Consistent datasets).: A dataset \(S=\{\left(\mathbf{x}_{i},y_{i}\right)\}_{i=1}^{N}\) is consistent if

\[\forall i,j\in\left[N\right]\;\mathbf{x}_{i}=\mathbf{x}_{j}\Longrightarrow y _{i}=y_{j}\,.\]

Motivated by modern NNs which are often extremely overparameterized, we are interested in the generalization behavior of interpolators, _i.e.,_ models that fit a consistent training set perfectly. Specifically, we consider Framework 1. While this framework is general enough to fit any minimal training error models, we shall be interested in the generalization of \(A\left(S\right)\) in cases where the training set is most likely consistent (Def. 2.5).

``` Input: A training set \(S\). Algorithm: if\(S\) is consistent: return an interpolator \(A\left(S\right)=h\) (such that \(\mathcal{L}_{S}\left(h\right)=0\)) else: return an arbitrary hypothesis \(A\left(S\right)=h\) (_e.g.,_\(h(\mathbf{x})=0,\forall\mathbf{x}\)) ```

**Framework 1** Learning interpolators

In Section 4, we analyze the generalization of two learning rules that fall under this framework: (1) learning min-size NN interpolators and (2) sampling random NN interpolators. Our analysis reveals a tempered overfitting behavior in both cases.

## 3 Interpolating a noisy training set

Our main generalization results rely on a key technical result, which shows how to memorize any consistent training set generated according to our noisy teacher model. We prove that the memorizing "student" NN can be small enough to yield meaningful generalization bounds in the next sections.

We begin by noticing that under a teacher model \(h^{\star}\) (Assumption 2.4), the labels of a consistent dataset \(S\) (Def. 2.5) can be decomposed as

\[\forall i\in\left[N\right]\;y_{i}=h^{\star}\left(\mathbf{x}_{i}\right)\oplus f \left(\mathbf{x}_{i}\right)\,,\] (1)

where \(f:\{0,1\}^{d_{0}}\rightarrow\{0,1\}\) indicates a label flip in the \(i^{\text{th}}\) example, and can be defined arbitrarily for \(\mathbf{x}\notin S\). Motivated by this observation, we now show an upper bound for the dimensions of a network interpolating \(S\), by bounding the dimensions of an NN implementing an arbitrary "partial" function \(f\) defined on \(N\) points.

**Theorem 3.1** (Memorizing the label flips).: _Let \(f\colon\{0,1\}^{d_{0}}\to\{0,1,\star\}\) be any function.2 Let \(N=|f^{-1}(\{0,1\})|\) and \(N_{1}=|f^{-1}(1)|\). There exists a depth-\(14\) binary threshold network \(\tilde{h}\colon\{0,1\}^{d_{0}}\to\{0,1\}\), with widths \(\underline{d}\), satisfying the following._

Footnote 2: When \(f(\mathbf{x})=\star\), the interpretation is that \(f\) is “undefined” on \(\mathbf{x}\), _i.e.,_\(f\) is a “partial” function.

1. \(\tilde{h}\) _is consistent with_ \(f\)_, i.e., for every_ \(\mathbf{x}\in\{0,1\}^{d_{0}}\)_, if_ \(f(\mathbf{x})\in\{0,1\}\)_, then_ \(\tilde{h}(\mathbf{x})=f(\mathbf{x})\)_._
2. _The total number of weights in_ \(\tilde{h}\) _is at most_ \((1+o(1))\cdot\log\binom{N}{N_{1}}+\mathrm{poly}(d_{0})\)_. More precisely,_ \[w\left(\underline{d}\right)=\log\binom{N}{N_{1}}+\left(\log\binom{N}{N_{1}} \right)^{3/4}\cdot\mathrm{polylog}\,N+O(d_{0}^{2}\cdot\log N)\,.\]
3. _Every layer of_ \(\tilde{h}\) _has width at most_ \((\log\binom{N}{N_{1}})^{3/4}\cdot\mathrm{poly}(d_{0})\)_. More precisely,_ \[\tilde{d}_{\max}=\left(\log\binom{N}{N_{1}}\right)^{3/4}\cdot\mathrm{polylog }\,N+O(d_{0}\cdot\log N)\,.\]

The main takeaway from Theorem 3.1 is that label flips can be memorized with networks with a number of parameters that is optimal in the leading order \(N\cdot\mathcal{L}_{S}\left(h^{\star}\right)\), i.e., not far from the minimal information-theoretical value. The proofs for this section are given in Appendix D.

Proof idea.Denote \(S=f^{-1}\left(\{0,1\}\right)\). We employ established techniques from the pseudorandomness literature to construct an efficient _hitting set generator_ (HSG)3 for the class of all conjunctions of literals. The HSG definition implies that there exists a seed on which the generator outputs a truth table that agrees with \(f\) on \(S\). The network \(\tilde{h}\) computes any requested bit of that truth table.

Footnote 3: A variant of the _pseudorandom generator_ (PRG) concept.

_Remark 3.2_ (Dependence on \(d_{0}\)).: In Appendix E we show that the \(O\left(d_{0}^{2}\cdot\log N\right)\) term is nearly tight, yet it can be relaxed when using some closely related NN architectures. For example, with a single additional layer of width \(\Omega\left(\sqrt{d_{0}}\cdot\log N\right)\) with ternary weights in the first layer, _i.e.,_\(\mathcal{Q}_{1}^{W}\!=\!\{-1,0,1\}\) instead of \(\{0,1\}\), the \(O\left(d_{0}^{2}\cdot\log N\right)\) term of Theorem 3.1 can be improved to \(O\left(d_{0}^{3/2}\cdot\log N+d_{0}\cdot\log^{3}N\right)\).

Next, with the bound on the dimensions of a NN implementing \(f\), we can bound the dimensions of a min-size interpolating NN by bounding the dimensions of a NN implementing the XOR of \(\tilde{h}\) and \(h^{\star}\).

**Lemma 3.3** (XOR of two NNs).: _Let \(h_{1},h_{2}\) be two binary NNs with depths \(L_{1}\leq L_{2}\) and widths \(\underline{d}^{(1)},\underline{d}^{(2)}\), respectively. Then, there exists a NN \(h\) with depth \(L_{\mathrm{XOR}}\triangleq L_{2}+2\) and widths_

\[\underline{d}_{\mathrm{XOR}}\triangleq\left(d_{1}^{(1)}+d_{1}^{(2)},\,\ldots, \,d_{L_{1}}^{(1)}+d_{L_{1}}^{(2)},\,d_{L_{1}+1}^{(2)}+1,\,\ldots,\,d_{L_{2}}^{ (2)}+1,\,2,\,1\right)\,,\]

_such that for all inputs \(\mathbf{x}\in\{0,1\}^{d_{0}}\), \(h\left(\mathbf{x}\right)=h_{1}\left(\mathbf{x}\right)\oplus h_{2}\left( \mathbf{x}\right)\)._

Combining Theorem 3.1 and Lemma 3.3 results in the following corollary.

**Corollary 3.4** (Memorizing a consistent dataset).: _For any teacher \(h^{\star}\) of depth \(L^{\star}\) and dimensions \(\underline{d}^{\star}\) and any consistent training set \(S\) generated from it, there exists an interpolating NN \(h\) (i.e., \(\mathcal{L}_{S}\left(h\right)=0\)) of depth \(L=\max\left\{L^{\star},14\right\}+2\) and dimensions \(\underline{d}\), such that the number of weights is_

\[w\left(\underline{d}\right) \leq w\left(\underline{d}^{\star}\right)+N\cdot H\left(\mathcal{L}_{S} \left(h^{\star}\right)\right)+2n\left(\underline{d}^{\star}\right)N^{3/4}H \left(\mathcal{L}_{S}\left(h^{\star}\right)\right)^{3/4}\mathrm{polylog}N\] \[\quad+O\left(d_{0}\left(d_{0}+n\left(\underline{d}^{\star} \right)\right)\cdot\log N\right)\]

_and the maximal width is_

\[\underline{d}_{\max}\leq\underline{d}_{\max}^{\star}+N^{3/4}\cdot H\left( \mathcal{L}_{S}\left(h^{\star}\right)\right)^{3/4}\cdot\mathrm{polylog}\left( N\right)+O\left(d_{0}\cdot\log\left(N\right)\right)\,.\]

Proof idea.We explicitly construct a NN with the desired properties. We can choose a subset of neurons to implement the teacher NN and another subset to implement the NN memorizing the label flips. Furthermore, we zero the weights between the two subsets. Two additional layers compute the XOR of the outputs, thus yielding the labels as in (1). This is illustrated in Figure 2.

## 4 Tempered overfitting of min-size and random interpolators

In this section, we provide our main results on the overfitting behavior of interpolating NNs. We consider min-size NN interpolators and random NN interpolators. For both learning rules, we prove tempered overfitting. Namely, we show that the test performance of the learned interpolators is not much worse than the Bayes optimal error.

First, for the sake of readability, let us define the marginal peak probability of the distribution.

**Definition 4.1** (Peak marginal probability).: \(\mathcal{D}_{\max}\triangleq\max_{\mathbf{x}\in\mathcal{X}}\mathbb{P}_{(X,Y) \sim\mathcal{D}}\left(X=\mathbf{x}\right)\)_._

Our results in this section focus on cases where the number of training samples is \(N=\omega\left(d_{0}^{2}\log d_{0}\right)\) and \(N=o\left(1/\sqrt{\mathcal{D}_{\max}}\right)\). In such regimes, the data consistency probability is high4 and our bounds are meaningful. Note that given the binarization of the data, \(N=o\left(1/\sqrt{\mathcal{D}_{\max}}\right)\) implies an exponential upper bound of \(N=o\left(2^{d_{0}/2}\right)\), achieved by the uniform distribution, _i.e.,_ when \(\mathcal{D}_{\max}=2^{-d_{0}}\). Due to the exponential growth of the sample space w.r.t. the input dimension, we find this assumption to be reasonable. Also, \(N=\omega\left(d_{0}^{2}\log d_{0}\right)\) implies that the input dimension cannot be arbitrarily large, but may still be non trivially small (see comparison to previous work in Section 5).

Footnote 4: \(N=o\left(1/\sqrt{\mathcal{D}_{\max}}\right)\) implies a rough bound on the consistency probability via the union bound. For example, when the marginal of \(X\) under \(\mathcal{D}\) is uniform (\(\mathcal{D}_{\max}=1/|\mathcal{X}|\)), and the inconsistency probability corresponds to the well-known birthday problem.

### Min-size interpolators

We consider min-size NN interpolators of a fixed depth, _i.e.,_ networks with the smallest number of weights for a certain depth that interpolate a given training set. In realizable settings, achieving good generalization performance by restricting the number of parameters in the learned interpolating model is a natural and well-understood approach. Indeed, in such cases, generalization follows directly from standard VC-dimension bounds [4, 75]. However, when interpolating _noisy_ data, the size of the returned model increases with the number of samples (in order to memorize the noise; see _e.g.,_ Vardi et al. [88]), making it challenging to guarantee generalization. In what follows, we prove that even when interpolating noisy data, min-size NNs exhibit good generalization performance.

Learning rule: Min-size NN interpolator.Given a consistent dataset \(S\) and a fixed depth \(L\), a min-size NN interpolator, or min-#weights interpolator, is a binary threshold network \(h\) (see Def. 2.1) that achieves \(\mathcal{L}_{S}\left(h\right)=0\) using a minimal number of weights. Recall that \(w(\underline{d})=\sum_{l=1}^{L}d_{l}d_{l-1}\) and define the _minimal_ number of weights required to implement a given hypothesis \(h\),

\[w_{L}\left(h\right)\triangleq\min_{\underline{d}\in\mathbb{N}^{L}}w\left( \underline{d}\right)\;\mathrm{s.t.}\;h\in\mathcal{H}_{\underline{d}}^{\mathrm{ BTN}}\,.\]

The learning rule is then defined as

\[A_{L}\left(S\right)\in\operatorname*{argmin}_{h}w_{L}\left(h\right)\;\mathrm{ s.t.}\;\mathcal{L}_{S}\left(h\right)=0\,.\]

Figure 2: **Interpolating a dataset. To memorize the training set, we use a subset of the parameters to match those of the teacher and another subset to memorize the noise (label flips). Then, we “merge” these subsets to interpolate the noisy training set. In our figure, (1) blue edges represent weights identical to the teacher’s; (2) yellow edges memorize the noise; (3) red edges are set to 0; and two additional layers implement the XOR between outputs, thus memorizing the training set.**

**Theorem 4.2** (Tempered overfitting of min-size NN interpolators).: _Let \(\mathcal{D}\) be a distribution induced by a noisy teacher of depth \(L^{\star}\), widths \(d^{\star}\), \(n(d^{\star})\) neurons, and a noise level of \(\varepsilon^{\star}<\nicefrac{{1}}{{2}}\) (Assumption 2.4). There exists \(c>0\) such that the following holds. Let \(S\sim\mathcal{D}^{N}\) be a training set such that \(N=\omega\big{(}n\left(d^{\star}\right)^{4}H\left(\varepsilon^{\star}\right)^{ 3}\log\left(n\left(d^{\star}\right)\right)^{c}+d_{0}^{2}\log d_{0}\big{)}\) and \(N=o(\sqrt{1/\mathcal{D}_{\max}})\). Then, for any fixed depth \(L\geq\max\left\{L^{\star},14\right\}+2\), the generalization error of the min-size depth-\(L\) NN interpolator satisfies the following._

* _Under arbitrary label noise,_ \[\mathbb{E}_{S}\left[\mathcal{L}_{\mathcal{D}}\left(A_{L}\left(S \right)\right)\right]\leq 1-2^{-H\left(\varepsilon^{\star}\right)}+o\left(1\right).\]
* _Under independent label noise,_ \[\left|\mathbb{E}_{S}\left[\mathcal{L}_{\mathcal{D}}\left(A_{L} \left(S\right)\right)\right]-2\varepsilon^{\star}\left(1-\varepsilon^{\star} \right)\right|=o\left(1\right).\]

Here, \(o\left(1\right)\) indicates terms that become insignificant when the number of samples \(N\) is large. We illustrate these behaviors in Figure 1. Moreover, we discuss these results and the proof idea in Section 4.3 after presenting the corresponding results for posterior sampling. The complete proof with detailed characterization of the \(o(1)\) terms is given in Appendix F.1.

### Random NN interpolators (posterior sampling)

Recent empirical [87, 20] and theoretical [13] works have shown that, somewhat surprisingly, randomly sampled deep NNs that interpolate a training set often generalize well. We now turn to analyzing such random interpolators under our teacher assumption and noisy labels (Assumption 2.4). As with min-size NN interpolators, our analysis here reveals a tempered overfitting behavior.

Prior distribution.A distribution over parameters induces a prior distribution over hypotheses by

\[\mathcal{P}\left(h\right)=\mathbb{P}_{\boldsymbol{\theta}}\left(h_{ \boldsymbol{\theta}}=h\right)\,.\]

We focus on the prior induced by the _uniform prior_ over the parameters of binary threshold networks. Specifically, for a fixed depth \(L\) and dimensions \(\underline{d}\), we consider \(\boldsymbol{\theta}\sim\mathrm{Uniform}\left(\Theta^{\mathsf{BTN}}\left( \underline{d}\right)\right)\). In other words, to generate \(h\sim\mathcal{P}\), each weight, bias, and neuron scalar in the NN is sampled independently and uniformly from its respective domain.

Learning rule: Posterior sampling.For any training set \(S\), denote the probability to sample an interpolating NN by \(p_{S}\triangleq\ \mathcal{P}\left(\mathcal{L}_{S}\left(h\right)=0\right)\). When \(p_{S}>0\), define the posterior distribution \(\mathcal{P}_{S}\) as

\[\mathcal{P}_{S}\left(h\right)\triangleq\mathcal{P}\left(h\mid\mathcal{L}_{S} \left(h\right)=0\right)=\frac{\mathcal{P}(h)}{p_{S}}\mathbb{I}\left\{\mathcal{ L}_{S}\left(h\right)=0\right\}\,.\] (2)

When \(p_{S}=0\), use an arbitrary \(\mathcal{P}_{S}\). Finally, the posterior sampling rule is \(A_{\underline{d}}\left(S\right)\sim\mathcal{P}_{S}\).

_Remark 4.3_ (Hypothesis expressivity).: The following result requires that the student NN is large enough to interpolate _any_ consistent \(S\) (see Corollary 3.4), thus, \(p_{S}>0\) and \(\mathcal{P}_{S}\) is defined as in (2).

**Theorem 4.4** (Tempered overfitting of random NN interpolators).: _Let \(\mathcal{D}\) be a distribution induced by a noisy teacher of depth \(L^{\star}\), widths \(\underline{d}^{\star}\), \(n(\underline{d}^{\star})\) neurons, and a noise level of \(\varepsilon^{\star}<\nicefrac{{1}}{{2}}\) (Assumption 2.4). There exists a constant \(c>0\) such that the following holds. Let \(S\sim\mathcal{D}^{N}\) be a training set such that \(N=\omega\big{(}n\left(d^{\star}\right)^{4}\log\left(n\left(d^{\star}\right) \right)^{c}+d_{0}^{2}\log d_{0}\big{)}\) and \(N=o(\sqrt{1/\mathcal{D}_{\max}})\). Then, for any student network of depth \(L\geq\max\left\{L^{\star},14\right\}+2\) and widths \(\underline{d}\in\mathbb{N}^{L}\) holding_

\[\forall l=1,\ldots,L^{\star}-1\quad d_{l}\geq d_{l}^{\star}+N^{3/4}\cdot\left( \log N\right)^{c}+c\cdot d_{0}\cdot\log\left(N\right)\,,\] (3)

_the generalization error of posterior sampling satisfies the following._

* _Under arbitrary label noise,_ \[\mathbb{E}_{S,A_{\underline{d}}\left(S\right)}\left[\mathcal{L}_{\mathcal{D} }\left(A_{\underline{d}}\left(S\right)\right)\right]\leq 1-2^{-H\left( \varepsilon^{\star}\right)}+O\left(\frac{n\left(\underline{d}\right)\cdot \log\left(d_{\max}+d_{0}\right)}{N}\right)\,.\]
* _Under independent label noise,_ \[\left|\mathbb{E}_{S,A_{\underline{d}}\left(S\right)}\left[\mathcal{L}_{ \mathcal{D}}\left(A_{\underline{d}}\left(S\right)\right)\right]-2\varepsilon^{ \star}\left(1-\varepsilon^{\star}\right)\right|\leq O\left(\sqrt{\frac{n \left(\underline{d}\right)\cdot\log\left(d_{\max}+d_{0}\right)}{N}}\right)\,.\]The proof and a detailed description of the error terms are given in Appendix F.2.

Remarkably, note that the interpolating NN in the theorem might be highly overparameterized, and that for such NNs good generalization is not guaranteed by standard generalization bounds [4, 75]. This theorem complements a similar result by Buzaglo et al. [13] for the realizable setting.

### Discussion

The overfitting behaviors described in this section are illustrated in Figure 1.

Proof idea.We extend the information-theoretical generalization bounds from [58] to this paper's setting in which label collisions in the datasets have a non-zero probability. In particular, we bound the interpolator's complexity from below by the mutual information between the model and the training set. Since the model is interpolating, we can further bound the mutual information by a quantity dependent on the population error. From the other direction, we bound the model's complexity from above by (1) its size in the min-size setting of Section 4.1, and (2) by the negative log interpolation probability for the posterior sampling of Section 4.2. Together with Corollary 3.4 we obtain the bounds above on the expected generalization error.

In Figure 2 we illustrated the construction of a memorizing network used to bound the complexity of the min-size interpolator. In the following Figure 3 we illustrate how the interpolation probability \(p_{S}\) can be bounded to induce a meaningful generalization bound.

Following Remark 3.2, the assumption \(N=\omega\left(d_{0}^{2}\log d_{0}\right)\) can be relaxed in some related architectures. For example, with a single additional layer of width \(O\left(\sqrt{d_{0}}\cdot\log N\right)\) and ternary weights in the first layer \(\mathcal{Q}_{1}^{W}=\left\{-1,0,1\right\}\), the requirement can be relaxed to \(N=\omega\big{(}d_{0}^{3/2}\log d_{0}\big{)}\).

_Remark 4.5_ (Higher weight quantization).: The bounds in the arbitrary noise setting can easily be extended to NNs with higher quantization levels. For example, letting \(\mathcal{Q}_{l}^{W}\) such that \(\big{|}\mathcal{Q}_{l}^{W}\big{|}=Q\) and \(\{0,1\}\subseteq\mathcal{Q}_{l}^{W}\), under the appropriate assumptions, we get that

\[\mathbb{E}_{\left(S,A\left(S\right)\right)}\left[\mathcal{L}_{\mathcal{D}} \left(A\left(S\right)\right)\right]\lessapproxq 1-Q^{-H\left(\varepsilon^{\star}\right)}\,,\]

which is a meaningful bound for noise levels \(\varepsilon^{\star}\leq\varepsilon\left(Q\right)\) for some \(\varepsilon\left(Q\right)<\nicefrac{{1}}{{2}}\).5 Tighter results would require utilizing the additional quantization levels to achieve smaller dimensions of the interpolating network, and are left to future work.

Footnote 5: Specifically, \(\varepsilon\left(Q\right)\) such that \(1-Q^{-H\left(\varepsilon\left(Q\right)\right)}\leq 1/2\).

## 5 Related work

Benign and tempered overfitting.The benign overfitting phenomenon has been extensively studied in recent years. Previous works analyzed the conditions in which benign overfitting occurs in linear regression [34, 11, 5, 65, 67, 21, 47, 93, 86, 98, 44, 90, 19, 3, 76, 31], kernel regression [51, 61, 53, 57, 72, 10, 60, 8, 50, 99, 6], and linear classification [18, 91, 14, 66, 64, 76, 52, 85, 92, 25]. Moreover, several works proved benign overfitting in classification using nonlinear NNs [28, 29, 15, 49, 94, 95, 62, 48, 30, 46]. All the aforementioned benign overfitting results require high-dimensional settings, namely, the input dimension is larger than the number of training samples.

Figure 3: **Interpolating a dataset with an overparameterized student.** We build on the construction from Figure 2 that memorizes a dataset using a subset of the parameters (blue, yellow, and red edges). Then, redundant neurons (gray) can be effectively ignored by setting their neuron scaling parameters (\(\boldsymbol{\gamma}\)) to 0, leaving the redundant weights (gray edges) unconstrained. Thus, the interpolation probability \(p_{S}\) can be bounded by a quantity exponentially decaying in the number of neurons \(n\left(\underline{d}\right)\) rather than in the number of weights \(w\left(\underline{d}\right)=\omega\left(N\right)\).

Mallinar et al. [57] suggested the taxonomy of benign, tempered, and catastrophic overfitting, which we use in this work. They demonstrated empirically that nonlinear NNs in classification tasks exhibit tempered overfitting. As mentioned in the introduction, our theoretical results for the independent noise case closely resemble these empirical findings (see Figure 1). Tempered overfitting in kernel ridge regression was theoretically studied in Mallinar et al. [57], Zhou et al. [99], Barzilai and Shamir [6]. In univariate ReLU NNs (namely, for input dimension \(1\)), tempered overfitting was obtained for both classification [48] and regression [43]. Manoj and Srebro [58] proved tempered overfitting for a learning rule returning short programs in some programming language. Finally, tempered overfitting is well understood for the \(1\)-nearest-neighbor learning rule, where the asymptotic risk is roughly twice the Bayes risk [23].

Circuit complexity.Theorem 3.1 (our NN for memorizing label flips) is in a similar spirit as several prior theorems in the area of _circuit complexity_. For example, Lupanov famously proved that every function \(f\colon\{0,1\}^{d_{0}}\to\{0,1\}\) can be computed by a circuit consisting of \((1+o(1))\cdot 2^{d_{0}}/d_{0}\) many AND/OR/NOT gates, where the AND/OR gates have fan-in two [55]. Lupanov's bound, which is tight [77], is analogous to Theorem 3.1, because a NN can be considered a type of circuit.

Even more relevant is a line of work that analyzes the circuit complexity of an arbitrary partial function \(f\colon\{0,1\}^{d_{0}}\to\{0,1,\star\}\) with a given domain size \(N\) and a given number of \(1\)-inputs \(N_{1}\), similar to the setup of Theorem 3.1. See Jukna's textbook for an overview [45, Section 1.4.2]. We highlight the work of Chashkin, who showed that every such function can be computed by a circuit (of unbounded depth and bounded fan-in) with \((1+o(1))\cdot\frac{\log\binom{N}{N_{1}}}{\log\log\binom{N}{N_{1}}}+O(d_{0})\) gates [17].

To the best of our knowledge, prior to our work, nothing analogous to Chashkin's theorem [17] was known regarding constant-depth threshold networks. It is conceivable that one could adapt Chashkin's construction [17] to the binary threshold network setting as a method of proving Theorem 3.1, but our proof of Theorem 3.1 uses a different approach. Our proof relies on shallow threshold networks computing _\(k\)-wise independent generators_[36] and an _error-reduction_ technique that was developed in the context of space-bounded derandomization [38], among other ingredients.

Memorization.Our construction shows how noisy data can be interpolated using a small threshold NN with binary weights. It essentially requires memorizing the noisy examples. The task of memorization, namely, finding a smallest NN that allows for interpolation of arbitrary data points, has been extensively studied in recent decades. Memorization of \(N\) arbitrary points in general position in \(\mathbb{R}^{d}\) with a two-layer NN can be achieved using \(O\left(\lceil\frac{N}{d}\rceil\right)\) hidden neurons [7, 81, 12]. Memorizing arbitrary \(N\) points, even if they are not in general position, can be done using two-layer networks with \(O(N)\) neurons [41, 74, 40, 97]. With three-layer networks, \(O(\sqrt{N})\) neurons suffice, but the number of parameters is still linear in \(N\)[39, 96, 89, 71]. Using deeper networks allows for memorization with a sublinear number of parameters [68, 88]. For example, memorization with networks of depth \(\sqrt{N}\) requires only \(\tilde{O}(\sqrt{N})\) parameters [88]. However, we note that in the aforementioned results, the number of quantization levels is not constant, namely, the number of bits in the representation of each weight depends on \(N\).6 Moreover, even in the sublinear constructions of [68, 88], the number of bits required to represent the network is \(\omega(N)\). As a result, in this work we cannot rely on these constructions to obtain meaningful bounds.

Footnote 6: We note that in most papers, the required number of quantization levels is implicit in the constructions, and is not discussed explicitly.

Posterior sampling and guess and check.The generalization of random interpolating neural networks has previously been studied, both empirically and theoretically [87, 63, 84, 20, 13]. Theisen et al. [84] studied the generalization of interpolating random linear and random features classifiers. Valle-Perez et al. [87], Mingard et al. [63] considered the Gaussian process approximation to random NNs which typically requires networks with infinite width. Buzaglo et al. [13] provided a method to obtain generalization results for quantized random NNs of general architectures -- possibly deep and with finite width, under the assumption of a narrow teacher model. A variant of this approach was used to prove our generalization results of posterior sampling, with the XOR network (Lemma 3.3) used in the role of the teacher.

Extensions, limitations, and future work

In this work, we focused on binary (fully connected) threshold networks of depth \(L\geq 16\) (Section 2.1) with binary input features (Section 2.2), for which we were able to derive nontrivial generalization bounds.

Our results can be extended with simple modifications to derive bounds in other settings. For instance, to NNs with higher weight quantization (see Remark 4.5), or to ReLU networks (since any threshold network with binary weights can be computed by a not-much-larger ReLU network with a constant quantization level). Unfortunately, without more sophisticated arguments these extensions result in looser generalization bounds. The "bottleneck" of our approach is the reliance on (nearly) tight bounds on the widths of interpolating NNs.

Extending the results to other architectures (_e.g.,_ CNNs or fully connected without neuron scaling) and other quantization schemes (_e.g.,_ floating point representations) will mainly require utilizing their specific structure to derive tighter bounds on the complexity (_e.g.,_ number of weights or number of bits) needed to interpolate consistent datasets. Furthermore, our bounds require the depth of the networks to be at least \(16\), and the width to be \(\omega\left(N^{3/4}\right)\), which might be deemed impractical for real datasets.7 The key to alleviating these requirements is, again, obtaining tighter complexity results.

Footnote 7: One can relax the width requirement by strengthening the depth requirement; see Remark D.7.

Our paper focused on consistent training sets (Def. 2.5), in order to allow perfect interpolation. Realistically, models do not always perfectly interpolate the training set, and therefore it is interesting to find generalization bounds for non-interpolating models, depending on the training error. In addition, it is interesting to relate the generalization to the training _loss_, and not just to the training accuracy. Such extensions will require either broadening our generalization results or deriving new ones.

## Acknowledgments and Disclosure of Funding

We thank Alexander Chashkin for generously providing English-language expositions of some results from his work [17] as well as some results from Lupanov's work [56] (personal communication). The research of DS was Funded by the European Union (ERC, A-B-C-Deep, 101039436). Views and opinions expressed are however those of the author only and do not necessarily reflect those of the European Union or the European Research Council Executive Agency (ERCEA). Neither the European Union nor the granting authority can be held responsible for them. DS also acknowledges the support of the Schmidt Career Advancement Chair in AI. GV is supported by research grants from the Center for New Scientists at the Weizmann Institute of Science, and the Shimon and Golde Picker - Weizmann Annual Grant. Part of this work was done as part of the NSF-Simons funded Collaboration on the Mathematics of Deep Learning. NS was partially supported by the NSF TRIPOD Institute on Data Economics Algorithms and Learning (IDEAL) and an NSF-IIS award.

## References

* Alon et al. [1986] N. Alon, L. Babai, and A. Itai. A fast and simple randomized parallel algorithm for the maximal independent set problem. _J. Algorithms_, 7(4):567-583, 1986. ISSN 0196-6774. doi: 10.1016/0196-6774(86)90019-2.
* Alon et al. [1992] N. Alon, O. Goldreich, J. Ha stad, and R. Peralta. Simple constructions of almost \(k\)-wise independent random variables. _Random Structures Algorithms_, 3(3):289-304, 1992. ISSN 1042-9832. doi: 10.1002/rsa.3240030308.
* Bartlett and Long [2021] P. L. Bartlett and P. M. Long. Failures of model-dependent generalization bounds for least-norm interpolation. _The Journal of Machine Learning Research_, 22(1):9297-9311, 2021.
* Bartlett et al. [2019] P. L. Bartlett, N. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight vc-dimension and pseudodimension bounds for piecewise linear neural networks. _Journal of Machine Learning Research_, 20(63):1-17, 2019.

* Bartlett et al. [2020] P. L. Bartlett, P. M. Long, G. Lugosi, and A. Tsigler. Benign overfitting in linear regression. _Proceedings of the National Academy of Sciences_, 117(48):30063-30070, 2020.
* Barzilai and Shamir [2024] D. Barzilai and O. Shamir. Generalization in kernel regression under realistic assumptions. In _Forty-first International Conference on Machine Learning_, 2024.
* Baum [1988] E. B. Baum. On the capabilities of multilayer perceptrons. _Journal of complexity_, 4(3):193-215, 1988.
* Beaglehole et al. [2023] D. Beaglehole, M. Belkin, and P. Pandit. On the inconsistency of kernel ridgeless regression in fixed dimensions. _SIAM Journal on Mathematics of Data Science_, 5(4):854-872, 2023. doi: 10.1137/22M1499819.
* Beame et al. [1986] P. W. Beame, S. A. Cook, and H. J. Hoover. Log depth circuits for division and related problems. _SIAM J. Comput._, 15(4):994-1003, 1986. ISSN 0097-5397. doi: 10.1137/0215070.
* Belkin et al. [2018] M. Belkin, D. J. Hsu, and P. Mitra. Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2018.
* Belkin et al. [2020] M. Belkin, D. Hsu, and J. Xu. Two models of double descent for weak features. _SIAM Journal on Mathematics of Data Science_, 2(4):1167-1180, 2020.
* Bubeck et al. [2020] S. Bubeck, R. Eldan, Y. T. Lee, and D. Mikulincer. Network size and size of the weights in memorization with two-layers neural networks. In _Neural Information Processing Systems_, 2020.
* Buzaglo et al. [2024] G. Buzaglo, I. Harel, M. S. Nacson, A. Brutzkus, N. Srebro, and D. Soudry. How uniform random weights induce non-uniform bias: Typical interpolating neural networks generalize with narrow teachers. In _International Conference on Machine Learning (ICML)_, 2024.
* Cao et al. [2021] Y. Cao, Q. Gu, and M. Belkin. Risk bounds for over-parameterized maximum margin classification on sub-gaussian mixtures. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* Cao et al. [2022] Y. Cao, Z. Chen, M. Belkin, and Q. Gu. Benign overfitting in two-layer convolutional neural networks. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* Chandra et al. [1984] A. K. Chandra, L. Stockmeyer, and U. Vishkin. Constant depth reducibility. _SIAM J. Comput._, 13(2):423-439, 1984. ISSN 0097-5397. doi: 10.1137/0213028.
* Chashkin [2006] A. Chashkin. On the realization of partial boolean functions. In _Proceedings of the 7th International Conference on Discrete Models in the Theory of Control Systems_, pages 390-404, 2006. In Russian.
* Chatterji and Long [2021] N. S. Chatterji and P. M. Long. Finite-sample analysis of interpolating linear classifiers in the overparameterized regime. _Journal of Machine Learning Research_, 22(129):1-30, 2021.
* Chatterji et al. [2022] N. S. Chatterji, P. M. Long, and P. L. Bartlett. The interplay between implicit bias and benign overfitting in two-layer linear networks. _Journal of machine learning research_, 23(263):1-48, 2022.
* Chiang et al. [2023] P. Chiang, R. Ni, D. Y. Miller, A. Bansal, J. Geiping, M. Goldblum, and T. Goldstein. Loss landscapes are all you need: Neural network generalization can be explained without the implicit bias of gradient descent. In _The Eleventh International Conference on Learning Representations_, 2023.
* Chinot and Lerasle [2020] G. Chinot and M. Lerasle. On the robustness of the minimum \(\ell_{2}\) interpolator. _arXiv preprint arXiv:2003.05838_, 2020.
* Chor and Goldreich [1989] B. Chor and O. Goldreich. On the power of two-point based sampling. _J. Complexity_, 5(1):96-106, 1989. ISSN 0885-064X. doi: 10.1016/0885-064X(89)90015-0.

* Cover and Hart [1967] T. Cover and P. Hart. Nearest neighbor pattern classification. _IEEE transactions on information theory_, 13(1):21-27, 1967.
* Dietzfelbinger [1996] M. Dietzfelbinger. Universal hashing and \(k\)-wise independent random variables via integer arithmetic without primes. In _STACS 96 (Grenoble, 1996)_, volume 1046 of _Lecture Notes in Comput. Sci._, pages 569-580. Springer, Berlin, 1996.
* Donhauser et al. [2022] K. Donhauser, N. Ruggeri, S. Stojanovic, and F. Yang. Fast rates for noisy interpolation require rethinking the effect of inductive bias. In _International Conference on Machine Learning (ICML)_, 2022.
* Eberly [1989] W. Eberly. Very fast parallel polynomial arithmetic. _SIAM J. Comput._, 18(5):955-976, 1989. ISSN 0097-5397. doi: 10.1137/0218066.
* Even et al. [1998] G. Even, O. Goldreich, M. Luby, N. Nisan, and B. Velickovic. Efficient approximation of product distributions. _Random Structures & Algorithms_, 13(1):1-16, 1998.
* Frei et al. [2022] S. Frei, N. S. Chatterji, and P. L. Bartlett. Benign overfitting without linearity: Neural network classifiers trained by gradient descent for noisy linear data. In _Conference on Learning Theory (COLT)_, 2022.
* Frei et al. [2023] S. Frei, G. Vardi, P. Bartlett, and N. Srebro. Benign overfitting in linear classifiers and leaky relu networks from kkt conditions for margin maximization. In _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pages 3173-3228. PMLR, 12-15 Jul 2023.
* George et al. [2024] E. George, M. Murray, W. Swartworth, and D. Needell. Training shallow relu networks on noisy data using hinge loss: when do we overfit and is it benign? _Advances in Neural Information Processing Systems_, 36, 2024.
* Ghosh and Belkin [2023] N. Ghosh and M. Belkin. A universal trade-off between the model size, test loss, and training loss of linear predictors. _SIAM Journal on Mathematics of Data Science_, 5(4):977-1004, 2023.
* Hajnal et al. [1993] A. Hajnal, W. Maass, P. Pudlak, M. Szegedy, and G. Turan. Threshold circuits of bounded depth. _J. Comput. System Sci._, 46(2):129-154, 1993. ISSN 0022-0000. doi: 10.1016/0022-0000(93)90001-D.
* Han et al. [2015] S. Han, J. Pool, J. Tran, and W. Dally. Learning both weights and connections for efficient neural network. _Advances in neural information processing systems_, 28, 2015.
* Hastie et al. [2022] T. Hastie, A. Montanari, S. Rosset, and R. J. Tibshirani. Surprises in high-dimensional ridgeless least squares interpolation. _Annals of statistics_, 50(2):949, 2022.
* Hatami and Hoza [2024] P. Hatami and W. Hoza. Paradigms for unconditional pseudorandom generators. _Foundations and Trends(r) in Theoretical Computer Science_, 16(1-2):1-210, 2024. ISSN 1551-305X. doi: 10.1561/0400000109.
* Healy and Viola [2006] A. Healy and E. Viola. Constant-depth circuits for arithmetic in finite fields of characteristic two. In _STACS 2006_, volume 3884 of _Lecture Notes in Comput. Sci._, pages 672-683. Springer, Berlin, 2006. doi: 10.1007/11672142_55.
* Hofmeister et al. [1991] T. Hofmeister, W. Hohberg, and S. Kohling. Some notes on threshold circuits, and multiplication in depth \(4\). _Inform. Process. Lett._, 39(4):219-225, 1991. ISSN 0020-0190. doi: 10.1016/0020-0190(91)90183-I.
* Hoza and Zuckerman [2020] W. M. Hoza and D. Zuckerman. Simple optimal hitting sets for small-success **RL**. _SIAM J. Comput._, 49(4):811-820, 2020. ISSN 0097-5397. doi: 10.1137/19M1268707.
* Huang [2003] G.-B. Huang. Learning capability and storage capacity of two-hidden-layer feedforward networks. _IEEE transactions on neural networks_, 14(2):274-281, 2003.
* Huang and Babri [1998] G.-B. Huang and H. A. Babri. Upper bounds on the number of hidden neurons in feedforward networks with arbitrary bounded nonlinear activation functions. _IEEE transactions on neural networks_, 9(1):224-229, 1998.

* Huang et al. [1991] S.-C. Huang, Y.-F. Huang, et al. Bounds on the number of hidden neurons in multilayer perceptrons. _IEEE transactions on neural networks_, 2(1):47-55, 1991.
* Hubara et al. [2016] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Binarized neural networks. _Advances in neural information processing systems_, 29, 2016.
* Joshi et al. [2024] N. Joshi, G. Vardi, and N. Srebro. Noisy interpolation learning with shallow univariate reLU networks. In _The Twelfth International Conference on Learning Representations_, 2024.
* Ju et al. [2020] P. Ju, X. Lin, and J. Liu. Overfitting can be harmless for basis pursuit, but only to a degree. _Advances in Neural Information Processing Systems_, 33:7956-7967, 2020.
* Jukna [2012] S. Jukna. _Boolean function complexity_, volume 27 of _Algorithms and Combinatorics_. Springer, Heidelberg, 2012. ISBN 978-3-642-24507-7. doi: 10.1007/978-3-642-24508-4. Advances and frontiers.
* Karhadkar et al. [2024] K. Karhadkar, E. George, M. Murray, G. Montufar, and D. Needell. Benign overfitting in leaky relu networks with moderate input dimension. _arXiv preprint arXiv:2403.06903_, 2024.
* Koehler et al. [2021] F. Koehler, L. Zhou, D. J. Sutherland, and N. Srebro. Uniform convergence of interpolators: Gaussian width, norm bounds and benign overfitting. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* Kornowski et al. [2024] G. Kornowski, G. Yehudai, and O. Shamir. From tempered to benign overfitting in relu neural networks. _Advances in Neural Information Processing Systems_, 36, 2024.
* Kou et al. [2023] Y. Kou, Z. Chen, Y. Chen, and Q. Gu. Benign overfitting for two-layer relu networks. _arXiv preprint arXiv:2303.04145_, 2023.
* Lai et al. [2023] J. Lai, M. Xu, R. Chen, and Q. Lin. Generalization ability of wide neural networks on \(\mathbb{R}\). _arXiv preprint arXiv:2302.05933_, 2023.
* Liang and Rakhlin [2020] T. Liang and A. Rakhlin. Just interpolate: Kernel "ridgeless" regression can generalize. _Annals of Statistics_, 48(3):1329-1347, 2020.
* Liang and Recht [2023] T. Liang and B. Recht. Interpolating classifiers make few mistakes. _Journal of Machine Learning Research_, 24(20):1-27, 2023.
* Liang et al. [2020] T. Liang, A. Rakhlin, and X. Zhai. On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels. In _Conference on Learning Theory (COLT)_, 2020.
* Liu et al. [2017] Z. Liu, J. Li, Z. Shen, G. Huang, S. Yan, and C. Zhang. Learning efficient convolutional networks through network slimming. In _Proceedings of the IEEE international conference on computer vision_, pages 2736-2744, 2017.
* Lupanov [1958] O. B. Lupanov. A method of circuit synthesis. _Izvestiya VUZ, Radiofizika_, 1, 1958. In Russian.
* the principle of local coding. _Problemy Kibernetiki_, 14:31-110, 1965. In Russian.
* Mallinar et al. [2022] N. R. Mallinar, J. B. Simon, A. Abedsoltan, P. Pandit, M. Belkin, and P. Nakkiran. Benign, tempered, or catastrophic: Toward a refined taxonomy of overfitting. In _Advances in Neural Information Processing Systems_, 2022.
* Manoj and Srebro [2023] N. S. Manoj and N. Srebro. Interpolation learning with minimum description length. _arXiv preprint arXiv:2302.07263_, 2023.
* Mansour et al. [1993] Y. Mansour, N. Nisan, and P. Tiwari. The computational complexity of universal hashing. _Theoret. Comput. Sci._, 107(1):121-133, 1993. ISSN 0304-3975. doi: 10.1016/0304-3975(93)90257-T.
* McRae et al. [2022] A. D. McRae, S. Karnik, M. Davenport, and V. K. Muthukumar. Harmless interpolation in regression and classification with structured features. In _International Conference on Artificial Intelligence and Statistics_, pages 5853-5875. PMLR, 2022.

* Mei and Montanari [2019] S. Mei and A. Montanari. The generalization error of random features regression: Precise asymptotics and the double descent curve. _Communications on Pure and Applied Mathematics_, 2019.
* Meng et al. [2023] X. Meng, D. Zou, and Y. Cao. Benign overfitting in two-layer relu convolutional neural networks for xor data. _arXiv preprint arXiv:2310.01975_, 2023.
* Mingard et al. [2021] C. Mingard, G. Valle-Perez, J. Skalse, and A. A. Louis. Is sgd a bayesian sampler? well, almost. _The Journal of Machine Learning Research_, 22(1):3579-3642, 2021.
* Montanari et al. [2020] A. Montanari, F. Ruan, Y. Sohn, and J. Yan. The generalization error of max-margin linear classifiers: High-dimensional asymptotics in the overparametrized regime. _Preprint, arXiv:1911.01544_, 2020.
* Muthukumar et al. [2020] V. Muthukumar, K. Vodrahalli, V. Subramanian, and A. Sahai. Harmless interpolation of noisy data in regression. _IEEE Journal on Selected Areas in Information Theory_, 2020.
* Muthukumar et al. [2021] V. Muthukumar, A. Narang, V. Subramanian, M. Belkin, D. Hsu, and A. Sahai. Classification vs regression in overparameterized regimes: Does the loss function matter? _Journal of Machine Learning Research_, 22(222):1-69, 2021.
* Negrea et al. [2020] J. Negrea, G. K. Dziugaite, and D. Roy. In defense of uniform convergence: Generalization via derandomization with an application to interpolating predictors. In _International Conference on Machine Learning_, pages 7263-7272, 2020.
* Park et al. [2021] S. Park, J. Lee, C. Yun, and J. Shin. Provable memorization via deep neural networks using sub-linear parameters. In _Conference on Learning Theory_, pages 3627-3661. PMLR, 2021.
* Pippenger [1987] N. Pippenger. The complexity of computations by networks. _IBM J. Res. Develop._, 31(2):235-243, 1987. ISSN 0018-8646. doi: 10.1147/rd.312.0235.
* Rahaman et al. [2019] N. Rahaman, A. Baratin, D. Arpit, F. Draxler, M. Lin, F. Hamprecht, Y. Bengio, and A. Courville. On the spectral bias of neural networks. In _International conference on machine learning_, pages 5301-5310. PMLR, 2019.
* Rajput et al. [2021] S. Rajput, K. Sreenivasan, D. Papailiopoulos, and amin karbasi. An exponential improvement on the memorization capacity of deep threshold networks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021.
* Rakhlin and Zhai [2019] A. Rakhlin and X. Zhai. Consistency of interpolation with laplace kernels is a high-dimensional phenomenon. In _Conference on Learning Theory_, pages 2595-2623. PMLR, 2019.
* Reif and Tate [1992] J. H. Reif and S. R. Tate. On threshold circuits and polynomial computation. _SIAM J. Comput._, 21(5):896-908, 1992. ISSN 0097-5397. doi: 10.1137/0221053.
* Sartori and Antsaklis [1991] M. A. Sartori and P. J. Antsaklis. A simple method to derive bounds on the size and to train multilayer neural networks. _IEEE transactions on neural networks_, 2(4):467-471, 1991.
* Shalev-Shwartz and Ben-David [2014] S. Shalev-Shwartz and S. Ben-David. _Understanding machine learning: From theory to algorithms_. Cambridge university press, 2014.
* Shamir [2022] O. Shamir. The implicit bias of benign overfitting. In _Conference on Learning Theory_, pages 448-478. PMLR, 2022.
* Shannon [1949] C. E. Shannon. The synthesis of two-terminal switching circuits. _Bell System Tech. J._, 28:59-98, 1949. ISSN 0005-8580. doi: 10.1002/j.1538-7305.1949.tb03624.x.
* Siu and Bruck [1991] K.-Y. Siu and J. Bruck. On the power of threshold circuits with small weights. _SIAM J. Discrete Math._, 4(3):423-435, 1991. ISSN 0895-4801. doi: 10.1137/0404038.
* Siu and Roychowdhury [1994] K.-Y. Siu and V. P. Roychowdhury. On optimal depth threshold circuits for multiplication and related problems. _SIAM J. Discrete Math._, 7(2):284-292, 1994. ISSN 0895-4801. doi: 10.1137/S0895480192228619.

* [80] K.-Y. Siu, J. Bruck, T. Kailath, and T. Hofmeister. Depth efficient neural networks for division and related problems. _IEEE Trans. Inform. Theory_, 39(3):946-956, 1993. ISSN 0018-9448. doi: 10.1109/18.256501.
* [81] D. Soudry and E. Hoffer. Exponentially vanishing sub-optimal local minima in multilayer neural networks. _arXiv preprint arXiv:1702.05777_, 2017.
* [82] P. Strata and R. Harvey. Dale's principle. _Brain Research Bulletin_, 50(5):349-350, 1999. ISSN 0361-9230. doi: https://doi.org/10.1016/S0361-9230(99)00100-8. URL https://www.sciencedirect.com/science/article/pii/S0361923099001008.
* [83] D. Teney, A. M. Nicolicioiu, V. Hartmann, and E. Abbasnejad. Neural redshift: Random networks are not random functions. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4786-4796, 2024.
* [84] R. Theisen, J. Klusowski, and M. Mahoney. Good classifiers are abundant in the interpolating regime. In _International Conference on Artificial Intelligence and Statistics_, pages 3376-3384. PMLR, 2021.
* [85] C. Thrampoulidis, S. Oymak, and M. Soltanolkotabi. Theoretical insights into multiclass classification: A high-dimensional asymptotic view. _Advances in Neural Information Processing Systems_, 33:8907-8920, 2020.
* [86] A. Tsigler and P. L. Bartlett. Benign overfitting in ridge regression. _Journal of Machine Learning Research_, 24(123):1-76, 2023.
* [87] G. Valle-Perez, C. Q. Camargo, and A. A. Louis. Deep learning generalizes because the parameter-function map is biased towards simple functions. In _International Conference on Learning Representations_, 2019.
* [88] G. Vardi, G. Yehudai, and O. Shamir. On the optimal memorization power of ReLU neural networks. In _International Conference on Learning Representations_, 2022.
* [89] R. Vershynin. Memory capacity of neural networks with threshold and relu activations. _arXiv preprint arXiv:2001.06938_, 2020.
* [90] G. Wang, K. Donhauser, and F. Yang. Tight bounds for minimum l1-norm interpolation of noisy data. In _International Conference on Artificial Intelligence and Statistics (AISTATS)_, 2022.
* [91] K. Wang and C. Thrampoulidis. Binary classification of gaussian mixtures: Abundance of support vectors, benign overfitting, and regularization. _SIAM Journal on Mathematics of Data Science_, 4(1):260-284, 2022.
* [92] K. Wang, V. Muthukumar, and C. Thrampoulidis. Benign overfitting in multiclass classification: All roads lead to interpolation. In _Advances in Neural Information Processing Systems (NeurIPS)_, 2021.
* [93] D. Wu and J. Xu. On the optimal weighted \(\ell_{2}\) regularization in overparameterized linear regression. _Advances in Neural Information Processing Systems_, 33:10112-10123, 2020.
* [94] X. Xu and Y. Gu. Benign overfitting of non-smooth neural networks beyond lazy training. In _International Conference on Artificial Intelligence and Statistics_, pages 11094-11117. PMLR, 2023.
* [95] Z. Xu, Y. Wang, S. Frei, G. Vardi, and W. Hu. Benign overfitting and grokking in reLU networks for XOR cluster data. In _The Twelfth International Conference on Learning Representations_, 2024.
* [96] C. Yun, S. Sra, and A. Jadbabaie. Small relu networks are powerful memorizers: a tight analysis of memorization capacity. In _Advances in Neural Information Processing Systems_, pages 15558-15569, 2019.
* [97] C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.

* [98] L. Zhou, F. Koehler, P. Sur, D. J. Sutherland, and N. Srebro. A non-asymptotic moreau envelope theory for high-dimensional generalized linear models. In _Advances in Neural Information Processing Systems_, 2022.
* [99] L. Zhou, J. B. Simon, G. Vardi, and N. Srebro. An agnostic view on the cost of overfitting in (kernel) ridge regression. In _The Twelfth International Conference on Learning Representations_, 2024.

Preliminaries and Auxiliary Results

### Preliminaries

Before moving to the proofs of the main results, we recall and introduce some notation that will be used throughout the supplementary material.

Notation.We denote a (possibly random) learning algorithm by \(A\left(S\right)\), and assume that it takes values in some hypothesis class \(\mathcal{H}\). We use \(\mathcal{D}\) to denote the joint distribution over a finite sample space \(\mathcal{X}\times\left\{0,1\right\}\) of the features and labels, \(\nu\) to denote the marginal distribution of the algorithm, and \(p\) to denote the joint distribution of a training set \(S\sim\mathcal{D}^{N}\) and the algorithm \(A\left(S\right)\). Specifically, the training set is a random element

\[S=\left\{\left(X_{1},Y_{1}\right),\ldots,\left(X_{N},Y_{N}\right)\right\}\sim \mathcal{D}^{N}\]

where \(\left(X_{i},Y_{i}\right)\) is reserved for the \(i\)-th example in \(S\). That is \(\left(X_{i},Y_{i}\right)\) is always a sample in \(S\), whereas \(\left(X,Y\right)\) is used to denote a data point which is independent of \(S\). We use \(d\mathcal{D}\left(x,y\right)\), \(d\nu\left(h\right)\) and \(dp\left(s,h\right)=dp\left(\left\{\left(x_{1},y_{1}\right),\ldots,\left(x_{N },y_{N}\right)\right\},h\right)\) to denote the corresponding probability mass functions. With some abuse of notation, we use \(d\mathcal{D}\left(x\right)\) for the probability mass function of the marginal of \(\mathcal{D}\) over \(\mathcal{X}\)

\[d\mathcal{D}\left(x\right)=\mathbb{P}_{\left(X,Y\right)\sim\mathcal{D}}\left( X=x\right)\,,\]

and \(dp\left(\left(x_{1},y_{1}\right),h\right)\) for the marginal of the joint probability of a single point from \(S\) and the output of the algorithm, _i.e.,_

\[dp\left(\left(x_{1},y_{1}\right),h\right)=\mathbb{P}_{\left(S,A\left(S\right) \right)\sim p}\left(X_{1}=x_{1},Y_{1}=y_{1},A\left(S\right)=h\right)\,.\]

Similarly, we use \(dp\left(x_{1},h\right)\), \(dp\left(y_{1}\mid x_{1},h\right)\), etc., for the probability mass functions of the appropriate marginal and conditional distributions.

Interpolating algorithm.In order to simplify the analysis, we introduce a framework of interpolation learning related to the one introduced in Framework 1.

Let \(\tilde{A}\left(S\right)\) be a learning rule satisfying Framework 1, and let \(\star\) be some arbitrary token distinct from any hypothesis the algorithm may produce. We define a modified learning rule \(A\left(S\right)\)8 such that

Footnote 8: As most of the appendix deals with the modified learning rule, we use \(\tilde{A}\left(S\right)\) for the original one and \(A\left(S\right)\) for the modified one.

* If \(S\) is inconsistent then \(A\left(S\right)=\star\).
* Otherwise, if \(S\) is consistent then \(A\left(S\right)=\tilde{A}\left(S\right)\), so in particular \(\mathcal{L}_{S}\left(A\left(S\right)\right)=0\).

Notice that since the \(A\left(S\right)=\tilde{A}\left(S\right)\) when \(S\) is consistent

\[\mathbb{E}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S\right)\right)\mid \text{consistent}\ S\right]=\mathbb{E}\left[\mathcal{L}_{\mathcal{D}}\left( \tilde{A}\left(S\right)\right)\mid\text{consistent}\ S\right]\]

and therefore we can find bounds for the generalization error of \(\tilde{A}\left(S\right)\) by analyzing \(A\left(S\right)\). In addition, when it can be inferred from context we use \(A\left(S\right)\) to denote the min-size and posterior sampling interpolators (instead of \(A_{L}(S)\) or \(A_{\underline{d}}(S)\), respectively).

For ease of exposition, throughout the appendix, we rephrase the assumptions made in Section 4, namely, that \(N=\omega\left(d_{0}^{2}\log d_{0}\right)\) and \(N=o\left(1/\sqrt{\mathcal{D}_{\max}}\right)\), as follows.

**Assumption A.1** (Bounded input dimension).: \(d_{0}=o\left(\sqrt{N/\log N}\right)\)_._

**Assumption A.2** (Data distribution flatness).: \(\mathcal{D}_{\max}=o\left(1/N^{2}\right)\)_._

### Auxiliary results

We start by citing several standard results from information theory and lemmas from Manoj and Srebro [58] which will be useful throughout our supplementary materials.

**Lemma A.3** (Chain rule of mutual information).: _For any random variables \(A_{1},A_{2}\) and \(B\)_

\[I\left(\left(A_{1},A_{2}\right);B\right)=I\left(A_{2};B\mid A_{1}\right)+I \left(A_{1};B\right)\,.\]

**Lemma A.4**.: _Let \(A\) and \(B\) be any two random variables with associated marginal distributions \(p_{A}\), \(p_{B}\), and joint \(p_{A,B}\). Let \(q_{A\mid B}\) be any conditional distribution (i.e. such that for any \(b\), \(q_{A\mid B}\left(\cdot,b\right)\) is a normalized non-negative measure). Then:_

\[I\left(A;B\right)\geq\mathbb{E}_{A,B\sim p_{A,B}}\left[\log\left(\frac{dq_{A \mid B}\left(A\middle|B\right)}{dp_{A}\left(A\right)}\right)\right]\,.\]

**Lemma A.5**.: _Let \(A_{1},A_{2},B\) be random variables where \(A_{1}\) and \(A_{2}\) are independent. Then_

\[I\left(\left(A_{1},A_{2}\right);B\right)\geq I\left(A_{1};B\right)+I\left(A_{ 2};B\right)\,.\]

**Lemma A.6** (Lemma A.4 from Manoj and Srebro [58]).: _For \(C\geq 0\) and \(0\leq\alpha\leq 1\) it holds that_

\[1-2^{-H\left(\alpha\right)-C}\leq 1-2^{-H\left(\alpha\right)}+C\,.\]

**Lemma A.7**.: _Let \(\varepsilon\in\left(0,\frac{1}{2}\right)\) and_

\[\phi\left(t\right)\triangleq\phi_{\varepsilon}\left(t\right)=\frac{\varepsilon ^{t}}{\varepsilon^{t}+\left(1-\varepsilon\right)^{t}}=\frac{1}{1+\left(\frac {1}{\varepsilon}-1\right)^{t}}\,.\]

_Then, \(\phi\) is monotonically decreasing as a function of \(t\), and convex in \(\left(0,\infty\right)\)._

Proof.: Denote \(\alpha\triangleq\frac{1}{\varepsilon}-1\) then

\[\phi\left(t\right) =\frac{1}{1+\alpha^{t}}\] \[\phi^{\prime}\left(t\right) =\frac{-\ln\left(\alpha\right)\alpha^{t}}{\left(1+\alpha^{t} \right)^{2}}=-\ln\left(\alpha\right)\cdot\frac{\alpha^{t}}{1+2\alpha^{t}+ \alpha^{2t}}\] \[\phi^{\prime\prime}\left(t\right) =-\ln\left(\alpha\right)\cdot\frac{\ln\left(\alpha\right)\alpha^ {t}\left(1+\alpha^{t}\right)^{2}-\alpha^{t}\cdot 2\left(1+\alpha^{t}\right)\cdot\ln\left(\alpha \right)\alpha^{t}}{\left(1+\alpha^{t}\right)^{4}}\] \[=-\ln\left(\alpha\right)^{2}\cdot\alpha^{t}\cdot\frac{\left(1+ \alpha^{t}\right)-2\alpha^{t}}{\left(1+\alpha^{t}\right)^{3}}=\ln\left(\alpha \right)^{2}\cdot\alpha^{t}\cdot\frac{\alpha^{t}-1}{\left(1+\alpha^{t}\right)^ {3}}\,.\]

Notice that for any \(\varepsilon\in\left(0,\frac{1}{2}\right)\), \(\alpha=\frac{1}{\varepsilon}-1>1\) so for all \(t>0\)

\[\alpha^{t}-1>0\]

and \(\phi^{\prime\prime}\left(t\right)>0\) so the function is indeed convex, and \(-\ln\left(\alpha\right)<0\) so \(\phi\) is decreasing.

**Corollary A.8**.: _For all \(t>0\) it holds that_

\[\phi\left(t\right)\geq\phi\left(1\right)+\phi^{\prime}\left(1\right)\left(t-1 \right)=\varepsilon+\ln 2\left(\varepsilon\log\left(\varepsilon\right)+ \varepsilon H\left(\varepsilon\right)\right)\left(t-1\right)\,.\]

Proof.: Substituting \(t=1\),

\[\phi\left(1\right) =\frac{\varepsilon}{\varepsilon+\left(1-\varepsilon\right)}=\varepsilon\] \[\phi^{\prime}\left(1\right) =-\ln\left(\alpha\right)\cdot\frac{\alpha}{\left(1+\alpha\right) ^{2}}=-\ln\left(\frac{1}{\varepsilon}-1\right)\cdot\frac{\frac{1}{ \varepsilon}-1}{\left(1+\left(\frac{1}{\varepsilon}-1\right)\right)^{2}}=- \ln\left(\frac{1-\varepsilon}{\varepsilon}\right)\cdot\frac{\frac{1}{ \varepsilon}-1}{\left(\frac{1}{\varepsilon}\right)^{2}}\] \[=-\left(\ln\left(1-\varepsilon\right)-\ln\left(\varepsilon\right) \right)\cdot\left(\varepsilon-\varepsilon^{2}\right)=\varepsilon\left(1- \varepsilon\right)\ln\left(\varepsilon\right)-\varepsilon\left(1-\varepsilon \right)\ln\left(1-\varepsilon\right)\] \[=\varepsilon\ln\left(\varepsilon\right)-\varepsilon\left( \varepsilon\ln\left(\varepsilon\right)+\left(1-\varepsilon\right)\ln\left(1 -\varepsilon\right)\right)\] \[=\varepsilon\ln 2\left(\log\left(\varepsilon\right)-\left( \varepsilon\log\left(\varepsilon\right)+\left(1-\varepsilon\right)\log\left(1 -\varepsilon\right)\right)\right)\] \[=\varepsilon\ln 2\left(\log\left(\varepsilon\right)+H\left( \varepsilon\right)\right)=\ln 2\left(\varepsilon\log\left(\varepsilon\right)+ \varepsilon H\left(\varepsilon\right)\right)\,.\]

The inequality then holds due to convexity. 

Finally, for completeness, we derive the relationship between the generalization error with respect to the noisy distribution \(\mathcal{L}_{\mathcal{D}}\left(h\right)\), and the generalization error with respect to the clean distribution \(\mathcal{L}_{\mathcal{D}_{0}}\left(h\right)\).

**Lemma A.9**.: _Let \(\mathcal{D}\) be a distribution as in Section 2.2, with independent noise with label flipping probability \(\varepsilon^{\star}\in\left(0,\frac{1}{2}\right)\). Let \(\mathcal{D}_{0}\) be the clean distribution, i.e., the distribution with label flipping probability \(0\). If_

\[\mathcal{L}_{\mathcal{D}}\left(h\right)=\mathbb{P}_{\left(X,Y\right)\sim \mathcal{D}}\left(h\left(X\right)\neq Y\right)\]

_and_

\[\mathcal{L}_{\mathcal{D}_{0}}\left(h\right)=\mathbb{P}_{\left(X,Y\right)\sim \mathcal{D}_{0}}\left(h\left(X\right)\neq Y\right)=\mathbb{P}_{\left(X\right) \sim\mathcal{D}}\left(h\left(X\right)\neq h^{\star}\left(X\right)\right)\]

_then_

\[\mathcal{L}_{\mathcal{D}_{0}}\left(h\right)=\frac{\mathcal{L}_{\mathcal{D}} \left(h\right)-\varepsilon^{\star}}{1-2\varepsilon^{\star}}\,.\]

Proof.: By definition,

\[\mathcal{L}_{\mathcal{D}}\left(h\right) =\mathbb{P}_{\left(X,Y\right)\sim\mathcal{D}}\left(h\left(X \right)\neq Y\right)\] \[=\mathbb{P}_{\left(X,Y\right)\sim\mathcal{D}}\left(h\left(X \right)\neq Y\mid h^{\star}\left(X\right)\oplus Y=0\right)\mathbb{P}_{\left(X,Y\right)\sim\mathcal{D}}\left(h^{\star}\left(X\right)\oplus Y=0\right)\] \[\quad+\mathbb{P}_{\left(X,Y\right)\sim\mathcal{D}}\left(h\left(X \right)\neq Y\mid h^{\star}\left(X\right)\oplus Y=1\right)\mathbb{P}_{\left(X,Y\right)\sim\mathcal{D}}\left(h^{\star}\left(X\right)\oplus Y=1\right)\,.\]

Since we assume that the noise is independent,

\[\mathcal{L}_{\mathcal{D}}\left(h\right) =\left(1-\varepsilon^{\star}\right)\mathbb{P}_{\left(X,Y\right) \sim\mathcal{D}}\left(h\left(X\right)\neq Y\mid h^{\star}\left(X\right) \oplus Y=0\right)\] \[\quad+\varepsilon^{\star}\mathbb{P}_{\left(X,Y\right)\sim\mathcal{ D}}\left(h\left(X\right)\neq Y\mid h^{\star}\left(X\right)\oplus Y=1\right)\] \[=\left(1-\varepsilon^{\star}\right)\mathbb{P}_{\left(X,Y\right) \sim\mathcal{D}}\left(h\left(X\right)\neq h^{\star}\left(X\right)\right)+ \varepsilon^{\star}\mathbb{P}_{\left(X,Y\right)\sim\mathcal{D}}\left(h\left(X \right)=h^{\star}\left(X\right)\right)\] \[=\left(1-\varepsilon^{\star}\right)\mathcal{L}_{\mathcal{D}_{0}} \left(h\right)+\varepsilon^{\star}\left(1-\mathcal{L}_{\mathcal{D}_{0}}\left(h \right)\right)\] \[=\varepsilon^{\star}+\left(1-2\varepsilon^{\star}\right)\mathcal{ L}_{\mathcal{D}_{0}}\left(h\right)\,,\]

or equivalently,

\[\mathcal{L}_{\mathcal{D}_{0}}\left(h\right)=\frac{\mathcal{L}_{\mathcal{D}} \left(h\right)-\varepsilon^{\star}}{1-2\varepsilon^{\star}}\,.\]

_Remark A.10_.: In particular, under the assumptions of the lemma, \(\mathcal{L}_{\mathcal{D}}\left(h\right)=2\varepsilon^{\star}\left(1- \varepsilon^{\star}\right)\) is equivalent to \(\mathcal{L}_{\mathcal{D}_{0}}\left(h\right)=\varepsilon^{\star}\).

Data consistency

Before moving on to generalization, we address some key properties of the training set's consistency.

**Lemma B.1**.: _For any distribution over the data \(\mathcal{D}\), \(\mathbb{P}\left(\text{inconsistent }S\right)\leq\frac{1}{2}N^{2}\mathcal{D}_{\max}\)._

Proof.: Using the union bound,

\[\mathbb{P}\left(\text{inconsistent }S\right) =\mathbb{P}\left(\exists i\neq j\in\left[N\right]\,:\,X_{i}=X_{j},Y_{i}\neq Y_{j}\right)\] \[\leq\mathbb{P}\left(\exists i\neq j\in\left[N\right]\,:\,X_{i}=X_ {j}\right)\] \[\leq\binom{N}{2}\sum_{x\in\mathcal{X}}\mathcal{D}_{\max}\mathbb{ P}\left(X=x\right)=\binom{N}{2}\mathcal{D}_{\max}\leq\frac{1}{2}N^{2} \mathcal{D}_{\max}\,.\]

Hence, under Assumption A.2 we have \(\mathbb{P}\left(\text{inconsistent }S\right)=o\left(1\right)\), _i.e.,_ the inconsistency probability is asymptotically small.

### Independent label noise

We now focus on the case of independent label noise, _i.e.,_\(Y\oplus h^{\star}\left(X\right)\mid\{X=x\}\sim\operatorname{Ber}\left(\varepsilon ^{\star}\right)\) for any \(x\in\mathcal{X}\). Recall the noise level

\[\varepsilon^{\star}=\mathbb{P}_{\left(X,Y\right)\sim\mathcal{D}}\left(Y\neq h ^{\star}\left(X\right)\right)=\mathbb{P}_{S}\left(Y_{1}\neq h^{\star}\left(X_ {1}\right)\right)\]

and we define the "effective" noise level in a _consistent_ training set

\[\hat{\varepsilon}_{\text{tr}}\triangleq\mathbb{P}_{S}\left(Y_{1}\neq h^{ \star}\left(X_{1}\right)\mid\text{consistent }S\right)\,.\] (4)

We relate \(\hat{\varepsilon}_{\text{tr}}\) to \(\varepsilon^{\star}\) in the following lemma.

**Lemma B.2**.: _In the independent noise setting, it holds that_

\[|\hat{\varepsilon}_{\text{tr}}-\varepsilon^{\star}|\leq|\ln 2\left( \varepsilon^{\star}\log\left(\varepsilon^{\star}\right)+\varepsilon^{\star}H \left(\varepsilon^{\star}\right)\right)|\cdot\left(N-1\right)\frac{\mathcal{D }_{\max}}{\mathbb{P}\left(\text{consistent}\,S\right)}\,,\]

_and moreover, \(\hat{\varepsilon}_{\text{tr}}\leq\varepsilon^{\star}\)._

Proof.: Conditioning on \(S\) being consistent (having no label "collisions"), all occurrences of \(x\) in \(S\) must have the same label so

\[\mathbb{P}_{S}\left(Y_{1}\neq h^{\star}\left(X_{1}\right)|\left(X_{1},Y_{1} \right)\text{ appears }k\text{ times in }S,\text{ consistent }S\right)=\frac{ \varepsilon^{\star k}}{\varepsilon^{\star k}+\left(1-\varepsilon^{\star} \right)^{k}}\]

Therefore,

\[\hat{\varepsilon}_{\text{tr}} =\mathbb{P}_{S}\left(Y_{1}\neq h^{\star}\left(X_{1}\right)\mid \text{consistent }S\right)\] \[=\sum_{k=1}^{N}\mathbb{P}_{S}\left(Y_{1}\neq h^{\star}\left(X_{1} \right)|\left(X_{1},Y_{1}\right)\text{ appears }k\text{ times in }S,\text{ consistent }S\right)\] \[\qquad\qquad\cdot\mathbb{P}\left(\left(X_{1},Y_{1}\right)\text{ appears }k\text{ times in }S\mid\text{consistent }S\right)\] \[=\sum_{k=1}^{N}\frac{\varepsilon^{\star k}}{\varepsilon^{\star k }+\left(1-\varepsilon^{\star}\right)^{k}}\cdot\mathbb{P}\left(\left(X_{1},Y_{1 }\right)\text{ appears }k\text{ times in }S\mid\text{consistent }S\right)\] (5) \[\leq\sum_{k=1}^{N}\frac{\varepsilon^{\star 1}}{\varepsilon^{\star 1}+ \left(1-\varepsilon^{\star}\right)^{1}}\cdot\mathbb{P}\left(\left(X_{1},Y_{1 }\right)\text{ appears }k\text{ times in }S\mid\text{consistent }S\right)\] \[=\varepsilon^{\star}\underbrace{\sum_{k=1}^{N}\cdot\mathbb{P} \left(\left(X_{1},Y_{1}\right)\text{ appears }k\text{ times in }S\mid\text{ consistent }S\right)}_{\text{sums to }1}=\varepsilon^{\star}\,.\]On the other hand, define

\[K\left(S\right)\triangleq\left|\left\{i\in\left[N\right]\mid X_{i}=X_{1}\right\} \right|=\sum_{i=1}^{N}\mathbb{I}\left\{X_{i}=X_{1}\right\}\]

then

\[\mathbb{E}_{S}\left[K\left(S\right)\mid\text{consistent}\;S\right] =1+\sum_{i=2}^{N}\mathbb{E}_{S}\left[\mathbb{I}\left\{X_{i}=X_{1} \right\}\mid\text{consistent}\;S\right]\] \[=1+\left(N-1\right)\mathbb{P}_{S}\left(X_{2}=X_{1}\mid\text{ consistent}\;S\right)\,.\]

Next,

\[\mathbb{P}\left(X_{2}=X_{1}\mid\text{consistent}\;S\right)=\frac{\mathbb{P} \left(X_{2}=X_{1},\text{consistent}\,S\right)}{\mathbb{P}\left(\text{consistent}\,S \right)}\leq\frac{\mathbb{P}\left(X_{1}=X_{2}\right)}{\mathbb{P}\left(\text{ consistent}\,S\right)}\,.\]

Since \(d\mathcal{D}\left(x\right)\leq\mathcal{D}_{\max}\) for all \(x\in\mathcal{X}\), as in the proof of Lemma B.1

\[\mathbb{E}_{S}\left[K\left(S\right)\mid\text{consistent}\;S\right]\leq 1+\left(N-1 \right)\frac{\mathbb{P}\left(X_{1}=X_{2}\right)}{\mathbb{P}\left(\text{ consistent}\,S\right)}\leq 1+\left(N-1\right)\frac{\mathcal{D}_{\max}}{ \mathbb{P}\left(\text{consistent}\,S\right)}\,.\]

Then, using Lemma A.7 we get,

\[\hat{\varepsilon}_{\text{tr}} =\sum_{k=1}^{N}\frac{\varepsilon^{\star k}}{\varepsilon^{\star k }+\left(1-\varepsilon^{\star}\right)^{k}}\cdot\mathbb{P}\left(X_{1}\text{ appears $k$ times in $S\mid$ consistent $S$}\right)\] \[=\sum_{k=1}^{N}\phi_{\varepsilon^{\star}}\left(k\right)\cdot \mathbb{P}\left(X_{1}\text{ appears $k$ times in $S\mid$ consistent $S$}\right)\] \[=\mathbb{E}_{S}\left[\underbrace{\phi_{\varepsilon^{\star}} \left(K\left(S\right)\right)}_{\text{convex in $k$}}\mid\text{consistent}\;S\right]\] \[\left[\text{Jensen}\right] \geq\phi_{\varepsilon^{\star}}\left(\mathbb{E}_{S}\left[K\left(S \right)\mid\text{consistent}\;S\right]\right)\] \[\left[\text{decreasing}\right] \geq\phi_{\varepsilon^{\star}}\left(1+\left(N-1\right)\cdot \frac{\mathcal{D}_{\max}}{\mathbb{P}\left(\text{consistent}\,S\right)}\right)\,.\]

Corollary A.8 implies that

\[\hat{\varepsilon}_{\text{tr}} \geq\phi_{\varepsilon^{\star}}\left(1+\left(N-1\right)\cdot \frac{\mathcal{D}_{\max}}{\mathbb{P}\left(\text{consistent}\,S\right)}\right)\] \[\geq\varepsilon^{\star}+\ln 2\left(\varepsilon^{\star}\log \left(\varepsilon^{\star}\right)+\varepsilon^{\star}H\left(\varepsilon^{ \star}\right)\right)\cdot\left(N-1\right)\frac{\mathcal{D}_{\max}}{\mathbb{P} \left(\text{consistent}\,S\right)}\,.\]

Combining the bounds we get

\[\left|\hat{\varepsilon}_{\text{tr}}-\varepsilon^{\star}\right|\leq\left|\ln 2 \left(\varepsilon^{\star}\log\left(\varepsilon^{\star}\right)+\varepsilon^{ \star}H\left(\varepsilon^{\star}\right)\right)\right|\cdot\left(N-1\right)\frac {\mathcal{D}_{\max}}{\mathbb{P}\left(\text{consistent}\,S\right)}\,.\]Generalization bounds

We present two generalization bounds for the population error of an interpolating algorithm in terms of the mutual information of it with the training set.

_Remark C.1_ (High consistency probability).: Throughout the appendix we assume that the consistency satisfies \(\mathbb{P}_{S}\) (consistent \(S\)) \(\geq\frac{1}{2}\). While this assumption is not without loss of generality, it is a weaker version of Assumption A.2 and implied by it (asymptotically). As Assumption A.2 is assumed in all "downstream results" that this appendix aims to support, we find it is reasonable to assume here.

### Arbitrary label noise

In this subsection, we provide a generalization bound for interpolating algorithms without any assumptions on the distribution of the noise \(Y\oplus h^{\star}\left(X\right)\mid\left\{X=x\right\}\), other than \(\mathcal{L}_{\mathcal{D}}\left(h^{\star}\right)=\varepsilon^{\star}\).

**Lemma C.2**.: _For any interpolating learning algorithm \(A\left(S\right)\),_

\[-\log\left(1-\mathbb{E}_{S,A\left(S\right)}\left[\mathcal{L}_{\mathcal{D}} \left(A\left(S\right)\right)\mid\mathrm{consistent}\,S\right]\right)\leq \frac{I\left(S;A\left(S\right)\right)}{N\cdot\mathbb{P}_{S}\left(\mathrm{ consistent}\,S\right)}\,.\]

Proof.: We rely on Lemma A.4. Specifically, we shall use the following suggested conditional distribution. For \(h\neq\star\) let

\[dq\left(s|h\right)=\frac{1}{Z_{h}}\mathbb{I}\left\{\mathcal{L}_{s}\left(h \right)=0\right\}d\mathcal{D}^{N}\left(s\right)\]

where

\[Z_{h} =\sum_{s}\mathbb{I}\left\{\mathcal{L}_{s}\left(h\right)=0\right\} d\mathcal{D}^{N}\left(s\right)\] \[=\mathbb{E}_{S}\mathbb{I}\left\{\mathcal{L}_{S}\left(h\right)=0\right\}\] \[=\mathbb{P}_{S}\left(\mathcal{L}_{S}\left(h\right)=0\right)\] \[=\left(1-\mathcal{L}_{\mathcal{D}}\left(h\right)\right)^{N}\,.\]

For \(h=\star\) let

\[dq\left(s|\star\right)=\frac{\mathbb{I}\left\{\mathrm{inconsistent}\,s\right\} d\mathcal{D}^{N}\left(s\right)}{\sum_{s^{\prime}}\mathbb{I}\left\{\mathrm{inconsistent}\,s^{\prime}\right\}d \mathcal{D}^{N}\left(s^{\prime}\right)}=\mathbb{I}\left\{\mathrm{inconsistent}\,s \right\}\frac{d\mathcal{D}^{N}\left(s\right)}{\mathbb{P}_{S}\left(\mathrm{ inconsistent}\,S\right)}\,.\]

Clearly, if \(h\neq\star\) and \(dq\left(s|h\right)=0\) then either \(d\mathcal{D}^{N}\left(s\right)=0\) so \(dp\left(s,h\right)=0\) as well, or \(\mathcal{L}_{s}\left(h\right)\neq 0\). So, since \(h\neq\star\), \(s\) can be interpolated and \(dp\left(s\mid h\right)=0\). That is, the proposed conditional distribution is absolutely continuous w.r.t. the true conditional distribution. When \(h=\star\), \(q\) is the true conditional distribution given that \(h=\star\) so it is also absolutely continuous w.r.t. it. That is, the proposed distribution is

\[dq\left(s\mid h\right)=\frac{\mathbb{I}\left\{\mathrm{inconsistent}\,s\right\} }{\mathbb{P}_{S}\left(\mathrm{inconsistent}\,S\right)}\mathbb{I}\left\{h= \star\right\}d\mathcal{D}^{N}\left(s\right)+\frac{\mathbb{I}\left\{\mathcal{L }_{s}\left(h\right)=0\right\}}{\left(1-\mathcal{L}_{\mathcal{D}}\left(h\right) \right)^{N}}\mathbb{I}\left\{h\neq\star\right\}d\mathcal{D}^{N}\left(s\right)\,.\]

From Lemma A.4

\[I\left(S;A\left(S\right)\right) \geq\mathbb{E}_{S,A\left(S\right)}\left[\log\left(\frac{dq\left( S|A\left(S\right)\right)}{d\mathcal{D}^{N}\left(S\right)}\right)\right]\] \[=\mathbb{E}_{S,A\left(S\right)}\left[\log\left(\frac{\mathbb{I} \left\{\mathrm{inconsistent}\,S\right\}}{\mathbb{P}_{S^{\prime}}\left( \mathrm{inconsistent}\,S^{\prime}\right)}\mathbb{I}\left\{A\left(S\right)= \star\right\}+\frac{\mathbb{I}\left\{\mathcal{L}_{S}\left(A\left(S\right) \right)=0\right\}}{\left(1-\mathcal{L}_{\mathcal{D}}\left(A\left(S\right) \right)\right)^{N}}\mathbb{I}\left\{A\left(S\right)\neq\star\right\}\right) \right]\,.\]

\(\mathbb{I}\left\{A\left(S\right)=\star\right\}\) and \(\mathbb{I}\left\{A\left(S\right)\neq\star\right\}=1\) hold together, they imply that \(\mathbb{I}\left\{\mathcal{L}_{S}\left(A\left(S\right)\right)=0\right\}=1\) so we have

\[I\left(S;A\left(S\right)\right) \geq\mathbb{E}_{S,A\left(S\right)}\left[\log\left(\frac{1}{\left(1 -\mathcal{L}_{\mathcal{D}}\left(A\left(S\right)\right)\right)^{N}}\right) \mathbb{I}\left\{A\left(S\right)\neq\star\right\}\right]\] \[=-\mathbb{E}_{S,A\left(S\right)}\left[N\log\left(1-\mathcal{L}_{ \mathcal{D}}\left(A\left(S\right)\right)\right)\mathbb{I}\left\{A\left(S \right)\neq\star\right\}\right]\,.\]

Using Jensen's inequality,

\[-\mathbb{E}_{S,A\left(S\right)} \left[N\log\left(1-\mathcal{L}_{\mathcal{D}}\left(A\left(S\right) \right)\right)\mathbb{I}\left\{A\left(S\right)\neq\star\right\}\right]\] \[=-N\mathbb{E}_{S,A\left(S\right)}\left[\log\left(1-\mathcal{L}_ {\mathcal{D}}\left(A\left(S\right)\right)\right)\mid\mathbb{I}\left\{A\left(S \right)\neq\star\right\}\right]\mathbb{P}_{S,A\left(S\right)}\left(A\left(S \right)\neq\star\right)\] \[\geq-N\log\left(1-\mathbb{E}_{S,A\left(S\right)}\left[\mathcal{L }_{\mathcal{D}}\left(A\left(S\right)\right)\mid\mathbb{I}\left\{A\left(S \right)\neq\star\right\}\right]\right)\mathbb{P}_{S,A\left(S\right)}\left(A \left(S\right)\neq\star\right)\] \[=-N\log\left(1-\mathbb{E}_{S,A\left(S\right)}\left[\mathcal{L}_ {\mathcal{D}}\left(A\left(S\right)\right)\mid\text{consistent}\,S\right] \right)\mathbb{P}_{S}\left(\text{consistent}\,S\right)\]

so

\[I\left(S;A\left(S\right)\right)\geq-N\log\left(1-\mathbb{E}_{S,A\left(S \right)}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S\right)\right)\mid \text{consistent}\,S\right]\right)\mathbb{P}_{S}\left(\text{consistent}\,S \right)\,.\]

Rearranging the inequality

\[-\log\left(1-\mathbb{E}_{S,A\left(S\right)}\left[\mathcal{L}_{\mathcal{D}} \left(A\left(S\right)\right)\mid\text{consistent}\,S\right]\right)\leq\frac{I \left(S;A\left(S\right)\right)}{N\cdot\mathbb{P}_{S}\left(\text{consistent}\,S \right)}\]

### Independent Noise

**Lemma C.3**.: _Assuming independent noise, the generalization error of interpolating learning rules satisfies the following._

\[\left|\mathbb{E}_{S,A\left(S\right)}\left[\mathcal{L}_{\mathcal{D}} \left(A\left(S\right)\right)\mid\mathrm{consistent}\,S\right]-2\varepsilon^{ \star}\left(1-\varepsilon^{\star}\right)\right|\] \[\leq\left(1-2\varepsilon^{\star}\right)\sqrt{C\left(N\right)}+ \frac{\left(N-1\right)\mathcal{D}_{\max}}{3}\,,\]

_where \(C\left(N\right)\triangleq\frac{I\left(S;A\left(S\right)\right)-N\cdot\left(H \left(\varepsilon^{\star}\right)-\mathbb{P}\left(\mathrm{inconsistent}\,S \right)\right)}{N\left(1-\mathbb{P}\left(\mathrm{inconsistent}\,S\right) \right)}\)._

Proof.: As in the proof of Lemma 4.2 in Manoj and Srebro [58], since \(S\) is sampled i.i.d., we have

\[I\left(S;A\left(S\right)\right) \overset{A.5}{\geq}\sum_{i=1}^{N}I\left(\left(X_{i},Y_{i}\right);A \left(S\right)\right)=N\cdot I\left(\left(X_{1},Y_{1}\right);A\left(S\right)\right)\] \[\overset{A.3}{=}N\cdot I\left(X_{1};A\left(S\right)\right)+N\cdot I \left(Y_{1};A\left(S\right)\mid X_{1}\right)\,.\] (6)

Using properties of conditional mutual information,

\[I\left(Y_{1};A\left(S\right)\mid X_{1}\right)=H\left(Y_{1}\mid X_{1}\right)- H\left(Y_{1}\mid A\left(S\right),X_{1}\right)\,.\] (7)

For the first term in (7), we employ the fact that for any \(x\in\mathcal{X}\), either \(Y_{1}\mid X_{1}=x\sim\mathrm{Ber}\left(\varepsilon^{\star}\right)\) or \(Y_{1}\mid X_{1}=x\sim\mathrm{Ber}\left(1-\varepsilon^{\star}\right)\) to get

\[H\left(Y_{1}\mid X_{1}\right)=\mathbb{E}_{\left(X,Y\right)\sim\mathcal{D}} \left[H\left(Y_{1}\mid X_{1}=X\right)\right]=\mathbb{E}_{\left(X,Y\right)\sim \mathcal{D}}\left[H\left(\varepsilon^{\star}\right)\right]=H\left(\varepsilon^ {\star}\right)\,.\]

For the second term in (7), we again employ the definition of conditional entropy,

\[H\left(Y_{1}\mid A\left(S\right),X_{1}\right)=-\sum_{x_{1}\in \mathcal{X}}\sum_{h\in\mathcal{H}\cup\left\{\star\right\}}\left[dp\left(\left( x_{1},0\right),h\right)\log\left(\frac{dp\left(\left(x_{1},0\right),h\right)}{dp \left(x_{1},h\right)}\right)\right.\] \[\left.\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad+dp\left(\left(x_{1},1\right),h\right)\log\left(\frac{dp\left(\left(x_{1},1\right),h\right)}{dp\left(x_{1},h\right)}\right)\,\right].\]

When \(h\neq\star\), the marginal distribution of a training data point and a hypothesis is

\[dp\left(\left(x,y\right),h\right)=dp\left(y\mid x,h\right)dp\left(x,h\right)= \mathbb{I}\left\{y=h\left(x\right)\right\}dp\left(x,h\right),\]

and the inner sum becomes a sum over expressions of the form:

\[dp\left(\left(x_{1},h\left(x_{1}\right)\right),h\right)\log\left(\frac{dp \left(\left(x_{1},h\left(x_{1}\right)\right),h\right)}{dp\left(x_{1},h\right) }\right)=dp\left(x_{1},h\right)\underbrace{\log\left(\frac{dp\left(x_{1},h \right)}{dp\left(x_{1},h\right)}\right)}_{=0}=0\,.\]

Therefore, we have that,

\[H\left(Y_{1}\mid A\left(S\right),X_{1}\right)\] \[=-\sum_{x_{1}\in\mathcal{X}}\left[dp\left(\left(x_{1},0\right), \star\right)\log\left(\frac{dp\left(\left(x_{1},0\right),\star\right)}{dp\left( x_{1},\star\right)}\right)+dp\left(\left(x_{1},1\right),\star\right)\log\left( \frac{dp\left(\left(x_{1},1\right),\star\right)}{dp\left(x_{1},\star\right)} \right)\right]\]

Employing conditional probabilities (notice that \(\frac{dp\left(\left(x_{1},0\right),\star\right)}{dp\left(x_{1},\star\right)}= \frac{dp\left(0\mid x_{1},\star\right)dp\left(x_{1},\star\right)}{dp\left(x_{1},\star\right)}=dp\left(0\mid x_{1},\star\right)\)), we get,

\[H\left(Y_{1}\mid A\left(S\right),X_{1}\right)\] \[=-\sum_{x_{1}\in\mathcal{X}}dp\left(x_{1},\star\right)\left[dp \left(0\mid x_{1},\star\right)\log\left(dp\left(0\mid x_{1},\star\right) \right)+dp\left(1\mid x_{1},\star\right)\log\left(dp\left(1\mid x_{1},\star \right)\right)\right]\] \[=\sum_{x_{1}\in\mathcal{X}}dp\left(x_{1},\star\right)H\left(dp \left(0\mid x_{1},\star\right)\right)=dp\left(\star\right)\sum_{x_{1}\in \mathcal{X}}dp\left(x_{1}\mid\star\right)H\left(dp\left(0\mid x_{1},\star\right)\right)\] \[=\mathbb{P}\left(A\left(S\right)=\star\right)\mathbb{E}_{\left(S,A \left(S\right)\right)\sim p}\Big{[}\underbrace{H\left(dp\left(0\mid X_{1}, \star\right)\right)}_{\leq 1}\Big{]}\text{ inconsistent }S\Big{]}\leq\mathbb{P}_{S}\left(\text{ inconsistent }S\right)\,.\]Overall, the right term in (6) is lower bounded by,

\[I\left(Y_{1};A\left(S\right)\mid X_{1}\right)=H\left(Y_{1}\mid X_{1}\right)-H \left(Y_{1}\mid A\left(S\right),X_{1}\right)\geq H\left(\varepsilon^{\star} \right)-\mathbb{P}\left(\text{inconsistent }S\right)\,.\]

For the left term, _i.e.,_\(I\left(X_{1};A\left(S\right)\right)\), we use the variational bound Lemma A.4 with the following suggested conditional distribution.

* For \(h=\star\), choose \(dq\left(x_{1}\mid\star\right)=dp\left(x_{1}\right)\) (notice that \(\sum_{x_{1}\in\mathcal{X}}dq\left(x_{1}\mid\star\right)=\sum_{x_{1}}dp\left(x_ {1}\right)=1\).
* Otherwise, if \(h\neq\star\), denote \(q_{\varepsilon}=\operatorname{Ber}\left(\varepsilon\right)\), and \[\hat{\varepsilon}_{\text{tr}} =\mathbb{P}_{S}\left(Y_{1}\neq h^{\star}\left(X_{1}\right)\mid \text{consistent }S\right)\] \[\hat{\varepsilon}_{\text{gen}} =\mathbb{E}_{\left(S,A\left(S\right)\right)\sim p}\left[\mathbb{ P}_{X\sim\mathcal{D}}\left(A\left(S\right)\left(X\right)\neq h^{\star}\left(X \right)\right)\mid A\left(S\right)\neq\star\right]\,.\] Note that \(\hat{\varepsilon}_{\text{tr}}\) may differ from \(\varepsilon^{\star}\). We choose the following conditional distribution \[dq\left(x_{1}\mid h\right)=\frac{1}{Z_{h}}\cdot\frac{dq_{\hat{\varepsilon}_{ \text{tr}}}\left(h\left(x_{1}\right)\oplus h^{\star}\left(x_{1}\right)\right)} {dq_{\hat{\varepsilon}_{\text{gen}}}\left(h\left(x_{1}\right)\oplus h^{\star} \left(x_{1}\right)\right)}dp\left(x_{1}\right)\,.\] In total, we choose, \[dq\left(x_{1}\mid h\right)=\frac{1}{Z_{h}}\cdot\frac{dq_{\hat{\varepsilon}_{ \text{tr}}}\left(h\left(x_{1}\right)\oplus h^{\star}\left(x_{1}\right)\right) }{dq_{\hat{\varepsilon}_{\text{gen}}}\left(h\left(x_{1}\right)\oplus h^{\star }\left(x_{1}\right)\right)}\cdot\mathbb{I}\left\{h\neq\star\right\}dp\left(x_ {1}\right)+\mathbb{I}\left\{h=\star\right\}dp\left(x_{1}\right)\,,\] where \[Z_{h}\] is the corresponding partition function. Then, we use (A.4) and properties of logarithms and indicators to show that, \[I\left(X_{1};A\left(S\right)\right)\geq\mathbb{E}_{S,A\left(S \right)}\left[\log\left(\frac{dq\left(X_{1}|A\left(S\right)\right)}{dp\left(X _{1}\right)}\right)\right]\] \[=\mathbb{E}_{S,A\left(S\right)}\left[\log\left(\frac{1}{Z_{A \left(S\right)}}\frac{dq_{\hat{\varepsilon}_{\text{tr}}}\left(h\left(X_{1} \right)\oplus h^{\star}\left(X_{1}\right)\right)}{dq_{\hat{\varepsilon}_{ \text{gen}}}\left(h\left(X_{1}\right)\oplus h^{\star}\left(X_{1}\right)\right) }dp\left(X_{1}\right)\mathbb{I}\left\{A\left(S\right)\neq\star\right\}+dp \left(X_{1}\right)\mathbb{I}\left\{A\left(S\right)=\star\right\}\right)\right]\] \[=\mathbb{E}_{S,A\left(S\right)}\left[\log\left(\frac{1}{Z_{A \left(S\right)}}\frac{dq_{\hat{\varepsilon}_{\text{tr}}}\left(h\left(X_{1} \right)\oplus h^{\star}\left(X_{1}\right)\right)}{dq_{\hat{\varepsilon}_{ \text{gen}}}\left(h\left(X_{1}\right)\oplus h^{\star}\left(X_{1}\right)\right) }\right)\mathbb{I}\left\{A\left(S\right)\neq\star\right\}+\mathbb{I}\left\{A \left(S\right)=\star\right\}\right]\] \[=\mathbb{E}_{S,A\left(S\right)}\left[\log\left(\frac{1}{Z_{A \left(S\right)}}\frac{dq_{\hat{\varepsilon}_{\text{tr}}}\left(h\left(X_{1} \right)\oplus h^{\star}\left(X_{1}\right)\right)}{dq_{\hat{\varepsilon}_{ \text{gen}}}\left(h\left(X_{1}\right)\oplus h^{\star}\left(X_{1}\right)\right) }\right)\mathbb{I}\left\{A\left(S\right)\neq\star\right\}\right]\,.\] Using the law of total expectation, the above becomes, \[=\mathbb{P}\left(A\left(S\right)\neq\star\right)\mathbb{E}_{S,A\left(S\right)} \left[\log\left(\frac{1}{Z_{A\left(S\right)}}\cdot\frac{dq_{\hat{\varepsilon}_ {\text{tr}}}\left(h\left(X_{1}\right)\oplus h^{\star}\left(X_{1}\right)\right) }{dq_{\hat{\varepsilon}_{\text{gen}}}\left(h\left(X_{1}\right)\oplus h^{\star} \left(X_{1}\right)\right)}\right)\bigg{|}\,A\left(S\right)\neq\star\right]\,,\] where we also use Jensen's inequality to show, \[\mathbb{E}_{S,A\left(S\right)}\left[\log\left(\frac{1}{Z_{A \left(S\right)}}\cdot\frac{dq_{\hat{\varepsilon}_{\text{tr}}}\left(h\left(X_{1 }\right)\oplus h^{\star}\left(X_{1}\right)\right)}{dq_{\hat{\varepsilon}_{ \text{gen}}}\left(h\left(X_{1}\right)\oplus h^{\star}\left(X_{1}\right)\right)} \right)\bigg{|}\,A\left(S\right)\neq\star\right]\] \[=\mathbb{E}_{S,A\left(S\right)}\left[\log\left(\frac{dq_{\hat{ \varepsilon}_{\text{tr}}}\left(h\left(X_{1}\right)\oplus h^{\star}\left(X_{1} \right)\right)}{dq_{\hat{\varepsilon}_{\text{gen}}}\left(h\left(X_{1}\right) \oplus h^{\star}\left(X_{1}\right)\right)}\right)\bigg{|}\,A\left(S\right)\neq \star\right]-\mathbb{E}_{S,A\left(S\right)}\left[\log\left(Z_{A\left(S \right)}\right)\big{|}\,A\left(S\right)\neq\star\right]\] \[\geq\mathbb{E}_{S,A\left(S\right)}\left[\log\left(\frac{dq_{\hat{ \varepsilon}_{\text{tr}}}\left(h\left(X_{1}\right)\oplus h^{\star}\left(X_{1} \right)\right)}{dq_{\hat{\varepsilon}_{\text{gen}}}\left(h\left(X_{1}\right) \oplus h^{\star}\left(X_{1}\right)\right)}\right)\bigg{|}\,A\left(S\right)\neq \star\right]-\log\left(\mathbb{E}_{S,A\left(S\right)}\left[Z_{A\left(S\right)} \big{|}\,A\left(S\right)\neq\star\right]\right)\,.\]

[MISSING_PAGE_FAIL:26]

Then, using the triangle inequality,

\[\left|\mathbb{E}_{S,A(S)}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S \right)\right)\right]\right|\text{consistent}\;S\right]-2\varepsilon^{\star} \left(1-\varepsilon^{\star}\right)\right|\] \[=\left|\left(1-\varepsilon^{\star}\right)\hat{\varepsilon}_{ \text{gen}}+\varepsilon^{\star}\left(1-\hat{\varepsilon}_{\text{gen}}\right)-2 \varepsilon^{\star}\left(1-\varepsilon^{\star}\right)\right|=\left|\hat{ \varepsilon}_{\text{gen}}-\hat{\varepsilon}_{\text{gen}}\varepsilon^{\star}+ \varepsilon^{\star}-\hat{\varepsilon}_{\text{gen}}\varepsilon^{\star}-2 \varepsilon^{\star}+2\varepsilon^{\star 2}\right|\] \[=\left|\hat{\varepsilon}_{\text{gen}}-2\hat{\varepsilon}_{\text{ gen}}\varepsilon^{\star}-\varepsilon^{\star}+2\varepsilon^{\star 2}\right|=\left|\hat{\varepsilon}_{\text{gen}}\left(1-2 \varepsilon^{\star}\right)-\varepsilon^{\star}\left(1-2\varepsilon^{\star} \right)\right|=\left(1-2\varepsilon^{\star}\right)\left|\hat{\varepsilon}_{ \text{gen}}-\varepsilon^{\star}\right|\] \[\leq\left(1-2\varepsilon^{\star}\right)\left(\left|\hat{ \varepsilon}_{\text{gen}}-\hat{\varepsilon}_{\text{tr}}\right|+\left|\hat{ \varepsilon}_{\text{tr}}-\varepsilon^{\star}\right|\right)\,.\]

Combining with the result from Lemma B.2 and Remark C.1

\[\left|\hat{\varepsilon}_{\text{tr}}-\varepsilon^{\star}\right| \leq\left|\ln 2\left(\varepsilon^{\star}\log\left(\varepsilon^{ \star}\right)+\varepsilon^{\star}H\left(\varepsilon^{\star}\right)\right) \right|\cdot\left(N-1\right)\frac{\mathcal{D}_{\max}}{\mathbb{P}\left(\text{ consistent}\,S\right)}\] \[\leq\left|\ln 2\left(\varepsilon^{\star}\log\left(\varepsilon^{ \star}\right)+\varepsilon^{\star}H\left(\varepsilon^{\star}\right)\right) \right|\cdot 2\left(N-1\right)\mathcal{D}_{\max}\,.\]

we conclude that

\[\left|\mathbb{E}_{S,A(S)}\left[\mathcal{L}_{\mathcal{D}}\left(A \left(S\right)\right)\right.\left|\text{consistent}\;S\right]-2\varepsilon^{ \star}\left(1-\varepsilon^{\star}\right)\right|\] \[\leq\left(1-2\varepsilon^{\star}\right)\left(\sqrt{C\left(N \right)}+\ln 2\left|\left(\varepsilon^{\star}\log\left(\varepsilon^{\star} \right)+\varepsilon^{\star}H\left(\varepsilon^{\star}\right)\right)\right| \cdot 2\left(N-1\right)\mathcal{D}_{\max}\right)\,.\]

Finally, we can use the algebraic property that \(\left(1-2\varepsilon^{\star}\right)\left|\ln 2\left(\varepsilon^{\star}\log\left( \varepsilon^{\star}\right)+\varepsilon^{\star}H\left(\varepsilon^{\star} \right)\right)\right|\;\leq\;\frac{1}{6}\), to get

\[\left|\mathbb{E}_{S,A(S)}\left[\mathcal{L}_{\mathcal{D}}\left(A \left(S\right)\right)\right.\left|\text{consistent}\;S\right]-2\varepsilon^{ \star}\left(1-\varepsilon^{\star}\right)\right|\] \[\leq\left(1-2\varepsilon^{\star}\right)\sqrt{C\left(N\right)}+ \frac{\left(N-1\right)\mathcal{D}_{\max}}{3}\,.\]

We can now bound the expected generalization error without conditioning on the consistency of the training set.

**Lemma C.4**.: _It holds that,_

\[\left|\mathbb{E}_{S,A(S)}\left[\mathcal{L}_{\mathcal{D}}\left(A \left(S\right)\right)\right]-2\varepsilon^{\star}\left(1-\varepsilon^{\star} \right)\right|\] \[\leq\left|\mathbb{E}_{S,A(S)}\left[\mathcal{L}_{\mathcal{D}} \left(A\left(S\right)\right)\right.\left|\text{consistent}\;S\right]-2 \varepsilon^{\star}\left(1-\varepsilon^{\star}\right)\right|+\mathbb{P}_{S} \left(\text{inconsistent}\;S\right)\,.\]

Proof.: Let \(X\) be an arbitrary RV in \(\left[0,1\right]\) and \(Y\) be a binary RV. Then,

\[\mathbb{E}\left[X\right] =\mathbb{P}\left(Y\right)\mathbb{E}\left[X|Y\right]+\mathbb{P} \left(\neg Y\right)\mathbb{E}\left[X|\neg Y\right]\] \[\mathbb{E}\left[X\right]-\mathbb{E}\left[X|Y\right] =\mathbb{P}\left(Y\right)\mathbb{E}\left[X|Y\right]-\mathbb{E} \left[X\mid Y\right]+\mathbb{P}\left(\neg Y\right)\mathbb{E}\left[X|\neg Y\right]\] \[=\mathbb{E}\left[X|Y\right]\left(\mathbb{P}\left(Y\right)-1 \right)+\mathbb{P}\left(\neg Y\right)\mathbb{E}\left[X|\neg Y\right]\] \[=-\mathbb{E}\left[X|Y\right]\mathbb{P}\left(\neg Y\right)+ \mathbb{P}\left(\neg Y\right)\mathbb{E}\left[X|\neg Y\right]=\mathbb{P} \left(\neg Y\right)\left(\mathbb{E}\left[X|\neg Y\right]-\mathbb{E}\left[X|Y \right]\right)\] \[\left|\mathbb{E}\left[X\right]-\mathbb{E}\left[X|Y\right]\right| =\mathbb{P}\left(\neg Y\right)\underbrace{\left|\mathbb{E} \left[X|\neg Y\right]-\mathbb{E}\left[X|Y\right]\right|}_{\leq 1}\leq\mathbb{P}\left(\neg Y\right)\,.\]

As a result

\[\left|\mathbb{E}_{S,A(S)}\left[\mathcal{L}_{\mathcal{D}}\left(A \left(S\right)\right)\right]-\mathbb{E}_{S,A(S)}\left[\mathcal{L}_{\mathcal{D}} \left(A\left(S\right)\right)\mid\text{consistent}\;S\right]\right|\leq\mathbb{P}_{S }(\text{inconsistent}\;S)\,.\]

Then, the required inequality is obtained by simply using the triangle inequality on

\[\left|\mathbb{E}_{S,A(S)}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S\right) \right)\right]-2\varepsilon^{\star}\left(1-\varepsilon^{\star}\right)\right|\,.\]Memorizing the label flips (proofs for Section 3)

In this section, we prove Theorem 3.1. We begin with an informal outline of the proof idea. Inspired by Manoj and Srebro's analysis [58], our proof of Theorem 3.1 is based on the concept of a _pseudorandom generator_, defined below.

**Definition D.1** (Pseudorandom generator).: Let \(G\colon\{0,1\}^{r}\to\{0,1\}^{R}\) be a function, let \(\mathcal{V}\) be a class of functions \(V\colon\{0,1\}^{R}\to\{0,1\}\), let \(\mathcal{D}\) be a distribution over \(\{0,1\}^{R}\), and let \(\epsilon>0\). We say that \(G\) is an \(\epsilon\)_-pseudorandom generator_ (\(\epsilon\)-PRG) for \(\mathcal{V}\) with respect to \(\mathcal{D}\) if for every \(V\in\mathcal{V}\), we have

\[\big{|}\mathbb{P}_{\mathbf{y}\sim\mathcal{D}}(V(\mathbf{y})=1)-\mathbb{P}_{ \mathbf{u}\in\{0,1\}^{r}}(V(G(\mathbf{u}))=1)\big{|}\leq\epsilon,\]

where \(\mathbf{u}\) is sampled uniformly at random from \(\{0,1\}^{r}\).

To connect Definition D.1 to Theorem 3.1, let \(R=2^{d_{0}}\). Let \(\mathcal{V}\) be the class of all conjunctions of literals, such as \(V(\mathbf{y})=\mathbf{y}_{1}\wedge\bar{\mathbf{y}}_{2}\wedge\mathbf{y}_{4}\). Let \(\hat{\mathcal{X}}=f^{-1}(\{0,1\})\). There is a function \(V_{f}\in\mathcal{V}\) such that given the entire truth table of a NN \(\tilde{h}\), the function \(V_{f}\) verifies that \(\tilde{h}\) agrees with \(f\) on \(\hat{\mathcal{X}}\). This function \(V_{f}\) is a conjunction of \(N_{1}\) many variables and \((N-N_{1})\) many negated variables.

Let \(\alpha=N_{1}/N\), and let \(\mathcal{D}=\mathrm{Ber}(\alpha)^{R}\). Suppose \(G\) is an \(\epsilon\)-PRG for \(\mathcal{V}\) with respect to \(\mathcal{D}\), where \(\epsilon<\mathbb{P}_{\mathbf{y}\sim\mathcal{D}}(V_{f}(\mathbf{y})=1)\). Then \(\mathbb{P}_{\mathbf{u}\in\{0,1\}^{r}}(V_{f}(G(\mathbf{u}))=1)\neq 0\), i.e., there exists some \(\mathbf{u}^{\star}\in\{0,1\}^{r}\) such that \(V_{f}(G(\mathbf{u}^{\star}))=1\). Therefore, if we let \(\tilde{h}\) be a NN that computes the function

\[\tilde{h}(\mathbf{x})=G(\mathbf{u}^{\star})_{\mathbf{x}},\] (8)

then \(\tilde{h}\) agrees with \(f\) on \(\hat{\mathcal{X}}\). In the equation above, \(G(\mathbf{u}^{\star})_{\mathbf{x}}\) denotes the \(\mathbf{x}\)-th bit of \(G(\mathbf{u}^{\star})\), thinking of \(\mathbf{x}\) as a number from \(0\) to \(R-1\) represented by its binary expansion.

There is a large body of well-established techniques for constructing PRGs. (See, for example, Hatami and Hoza's recent survey [35].) Therefore, constructing a suitable PRG might seem like a promising approach to proving Theorem 3.1. However, this approach is flawed. The issue concerns the seed length (\(r\)). According to the plan outlined above, the seed \(\mathbf{u}^{\star}\) is effectively hard-coded into the neural network \(\tilde{h}\), which means that, realistically, the number of weights in \(\tilde{h}\) will be at least \(r\). Meanwhile, for the plan above to make sense, our PRG's error parameter (\(\epsilon\)) must satisfy

\[\epsilon<\mathbb{P}_{\mathbf{y}\sim\mathcal{D}}(V_{f}(\mathbf{y})=1)=2^{-H( \alpha)\cdot N}\approx 2^{-\binom{N}{N_{1}}}.\] (9)

Comparing (9) to Theorem 3.1, we see that we would need a PRG with seed length

\[r=(1+o(1))\cdot\log(1/\epsilon).\]

But this is too much to ask. There is no real reason to expect such a PRG to exist, even if we ignore explicitness considerations. Indeed, in some cases, it is provably impossible to achieve a seed length smaller than \((2-o(1))\cdot\log(1/\epsilon)\)[2].

To circumvent this issue, we will work with a more flexible variant of the PRG concept called a _hitting set generator_ (HSG).

**Definition D.2** (Hitting set generator).: Let \(G\colon\{0,1\}^{r}\to\{0,1\}^{R}\) be a function, let \(\mathcal{V}\) be a class of functions \(V\colon\{0,1\}^{R}\to\{0,1\}\), let \(\mathcal{D}\) be a distribution over \(\{0,1\}^{R}\), and let \(\epsilon>0\). We say that \(G\) is an _\(\epsilon\)-hitting set generator_ (\(\epsilon\)-HSG) for \(\mathcal{V}\) with respect to \(\mathcal{D}\) if for every \(V\in\mathcal{V}\) such that \(\mathbb{P}_{\mathbf{y}\sim\mathcal{D}}(V(\mathbf{y})=1)>\epsilon\), there exists \(\mathbf{u}^{\star}\in\{0,1\}^{r}\) such that \(V(G(\mathbf{u}^{\star}))=1\).

Definition D.2 is weaker than Definition D.1, but an HSG is sufficient for our purposes. Crucially, one can show nonconstructively that for every \(\mathcal{V}\), \(\mathcal{D}\), and \(\epsilon\), there exists an HSG with seed length

\[1\cdot\log(1/\epsilon)+\log\log|\mathcal{V}|+O(1),\]

whereas the nonconstructive PRG seed length is \(2\cdot\log(1/\epsilon)+\cdots\). To prove Theorem 3.1, we will construct an _explicit_ HSG for conjunctions of literals with respect to \(\mathrm{Ber}(\alpha)^{R}\) with a seed length of \((1+o(1))\cdot\log(1/\epsilon)+\mathrm{polylog}\,R\). We will ensure that our HSG is "explicit enough" to enable computing the function \(\tilde{h}\) defined by (8) using a constant-depth NN with approximately \(r\) many weights.

Our HSG construction uses established techniques from the pseudorandomness literature. In brief, we use \(k\)-wise independence to construct an initial HSG with a poor dependence on \(\epsilon\), and then we apply an error reduction technique due to Hoza and Zuckerman [38]. Details follow.

### Preprocessing the input to reduce the dimension

Before applying an HSG as outlined above, the first step of the proof of Theorem 3.1 is actually a preprocessing step that reduces the dimension to approximately \(2\log N\). This step is not completely essential, but it helps to improve the dependence on \(d_{0}\) in Theorem 3.1. The preprocessing step is based on a standard trick, namely, we treat the input as a vector in \(\mathbb{F}_{2}^{d_{0}}\) and apply a random matrix, where \(\mathbb{F}_{2}\) denotes the field with two elements. That is:

**Definition D.3** (\(\mathbb{F}_{2}\)-linear and \(\mathbb{F}_{2}\)-affine functions).: A function \(C\colon\{0,1\}^{d}\to\{0,1\}^{d^{\prime}}\) is \(\mathbb{F}_{2}\)-linear if it has the form

\[C(\mathbf{x})=\mathbf{W}\mathbf{x},\]

where \(\mathbf{W}\in\{0,1\}^{d^{\prime}\times d}\) and the arithmetic is mod \(2\). More generally, we say that \(C\) is \(\mathbb{F}_{2}\)-affine if it has the form

\[C(\mathbf{x})=\mathbf{W}\mathbf{x}+\mathbf{b},\]

where \(\mathbf{W}\in\{0,1\}^{d^{\prime}\times d}\), \(\mathbf{b}\in\{0,1\}^{d^{\prime}}\), and the arithmetic is mod \(2\).

The following fact is standard; we include the proof only for completeness.

**Lemma D.4** (Preprocessing to reduce the dimension).: _Let \(d_{0}\in\mathbb{N}\), let \(\hat{\mathcal{X}}\subseteq\{0,1\}^{d_{0}}\), and let \(N=|\hat{\mathcal{X}}|\). There exists an \(\mathbb{F}_{2}\)-linear function \(C_{0}\colon\{0,1\}^{d_{0}}\to\{0,1\}^{2\lceil\log N\rceil}\) that is injective on \(\hat{\mathcal{X}}\)._

Proof.: Pick \(\mathbf{W}\in\{0,1\}^{2\lceil\log N\rceil\times d_{0}}\) uniformly at random and let \(C_{0}(\mathbf{x})=\mathbf{W}\mathbf{x}\). For each pair of distinct points \(\mathbf{x},\mathbf{x}^{\prime}\in\hat{\mathcal{X}}\), we have

\[\mathbb{P}(\mathbf{W}\mathbf{x}=\mathbf{W}\mathbf{x}^{\prime})=\mathbb{P}( \mathbf{W}(\mathbf{x}-\mathbf{x}^{\prime})=\mathbf{0})=2^{-2\lceil\log N \rceil}<1/N^{2}.\]

Therefore, by the union bound over all pairs \(\mathbf{x},\mathbf{x}^{\prime}\), there is a nonzero chance that \(C_{0}\) is injective on \(\hat{\mathcal{X}}\). Therefore, there is some fixing of \(\mathbf{W}\) such that \(C_{0}\) is injective on \(\hat{\mathcal{X}}\). 

We will choose \(C_{0}\) to be injective on the domain of \(f\). That way, after applying \(C_{0}\) to the input, our remaining task is to compute some other partial function \(f^{\prime}\colon\{0,1\}^{2\lceil\log N\rceil}\to\{0,1,\star\}\), namely, the function \(f^{\prime}\) such that \(f^{\prime}\circ C_{0}=f\). This function \(f^{\prime}\) has the same domain size (\(N\)), and it takes the value \(1\) on the same number of points (\(N_{1}\)), so the net effect is that we have decreased the dimension from \(d_{0}\) down to \(2\lceil\log N\rceil\). This same technique appears in the circuit complexity literature, along with more sophisticated variants. For example, see Jukna's textbook [45, Section 1.4.2].

To apply Lemma D.4 in our setting, we rely on the well-known fact that \(\mathbb{F}_{2}\)-linear functions, and more generally \(\mathbb{F}_{2}\)-affine functions, can be computed by depth-two binary threshold networks. More precisely, we have the following.

**Lemma D.5** (Binary threshold networks computing \(\mathbb{F}_{2}\)-linear functions).: _If \(C\colon\{0,1\}^{d}\to\{0,1\}\) is the parity function or its negation, then there exists a depth-one binary threshold network \(C_{0}\colon\{0,1\}^{d}\to\{0,1\}^{(d+2)}\) and a number \(b\in\mathbb{R}\) such that for every \(\mathbf{x}\in\{0,1\}^{d}\), we have_

\[C(x)=\mathbf{1}^{T}C_{0}(\mathbf{x})+b,\]

_where \(\mathbf{1}\) denotes the all-ones vector. Moreover, every affine function \(C\colon\{0,1\}^{d}\to\{0,1\}^{d^{\prime}}\) can be computed by a depth-two binary threshold network with \(d^{\prime}\cdot(d+2)\) nodes in the hidden layer._

Proof.: First, suppose \(C\) is the parity function. For each \(i\in[d]\), let \(\phi_{\leq i}\colon\{0,1\}^{d}\to\{0,1\}\) be the function

\[\phi_{\leq i}(\mathbf{x})=1\iff\sum_{j=1}^{d}\mathbf{x}_{j}\leq i,\]

and similarly define \(\phi_{\geq i}\colon\{0,1\}^{d}\to\{0,1\}\) by

\[\phi_{\geq i}(\mathbf{x})=1\iff\sum_{j=1}^{d}\mathbf{x}_{j}\geq i.\]Then

\[\phi_{\leq 1}(\mathbf{x})+\phi_{\geq 1}(\mathbf{x})+\phi_{\leq 3}(\mathbf{x})+ \phi_{\geq 3}(\mathbf{x})+\cdots=\begin{cases}\lceil d/2\rceil+1&\text{if }\mathsf{ PARITY}(\mathbf{x})=1\\ \lceil d/2\rceil&\text{if }\mathsf{PARITY}(\mathbf{x})=0,\end{cases}\]

so we can take \(b=-\lceil d/2\rceil\). Now, suppose \(C\) is the negation of the parity function. This reduces to the case of the parity function because \(1-\mathsf{PARITY}(\mathbf{x})=\mathsf{PARITY}(\mathbf{x},1)\). Finally, the "moreover" statement follows because if \(C\) is \(\mathbb{F}_{2}\)-affine, then every output bit of \(C\) is either the parity function or the negated parity function applied to some subset of the inputs. 

Lemma D.5 can be generalized to the case of any symmetric function instead of \(\mathsf{PARITY}\). This technique is well-known in the circuit complexity literature; for example, see the work of Hajnal, Maass, Pudlak, Szegedy, and Turan [32].

### Threshold networks computing \(k\)-wise independent generators

One of the ingredients of our HSG will be a family of pairwise independent hash functions. We will use the following family, notable for its low computational complexity.

**Lemma D.6** (Affine pairwise independent hash functions).: _For every \(a,r\in\mathbb{N}\), there is a family \(\mathcal{H}\) of hash functions \(\operatorname{hash}\colon\{0,1\}^{a}\to\{0,1\}^{r}\) with the following properties._

* \(|\mathcal{H}|\leq 2^{O(a+r)}\)_._
* \(\mathcal{H}\) _is pairwise independent. That is, for every two distinct_ \(\mathbf{w},\mathbf{w}^{\prime}\in\{0,1\}^{a}\)_, if we pick_ \(\operatorname{hash}\in\mathcal{H}\) _uniformly at random, then_ \(\operatorname{hash}(\mathbf{w})\) _and_ \(\operatorname{hash}(\mathbf{w}^{\prime})\) _are independent and uniformly distributed over_ \(\{0,1\}^{r}\)_._
* _Each function_ \(\operatorname{hash}\in\mathcal{H}\) _is_ \(\mathbb{F}_{2}\)_-affine._

Proof.: See the work of Mansour, Nisan, and Tiwari [59, Claim 2.2]. 

_Remark D.7_ (Alternative hash families).: By Lemma D.5, each function \(\operatorname{hash}\in\mathcal{H}\) can be computed by a depth-two binary threshold network with \(O(r^{2}a+ra^{2})\) wires (weights). There exist alternative pairwise independent hash function families with lower wire complexity. In particular, one could use hash functions based on integer arithmetic [24], which can be implemented with wire complexity \((a+r)^{1+\gamma}\) for any arbitrarily small constant \(\gamma>0\)[73]. This would lead to slightly better width and wire complexity bounds in Theorem 3.1: each occurrence of \(3/4\) could be replaced with \(2/3+\gamma\). However, the downside of this approach is that the depth of the network would increase to a very large constant depending on \(\gamma\).

Another ingredient of our HSG will be a threshold network computing a "\(k\)-wise uniform generator," defined below.

**Definition D.8** (\(k\)-wise uniform generator).: A _\(k\)-wise uniform generator_ is a function \(G\colon\{0,1\}^{r}\to\{0,1\}^{R}\) such that if we sample \(\mathbf{u}\in\{0,1\}^{r}\) uniformly at random, then every \(k\) of the output coordinates of \(G(\mathbf{u})\) are independent and uniform. In other words, \(G\) is a \(0\)-PRG for \(\mathcal{V}\) with respect to the uniform distribution, where \(\mathcal{V}\) consists of all Boolean functions that only depend on \(k\) bits.

Prior work has shown that \(k\)-wise uniform generators can be implemented by constant-depth threshold networks [36]. We will need to re-analyze the construction to get sufficiently fine-grained bounds. In the remainder of this subsection, we will prove the following.

**Lemma D.9** (Constant-depth \(k\)-wise uniform generator).: _Let \(k,R\in\mathbb{N}\) where \(R\) is a power of two. There exists a \(k\)-wise uniform generator \(G\colon\{0,1\}^{r}\to\{0,1\}^{R}\), where \(r=O(k\cdot\log R)\), such that for every \(\mathbb{F}_{2}\)-affine function \(\operatorname{hash}\colon\{0,1\}^{a}\to\{0,1\}^{r}\), there exists a depth-\(5\) binary threshold network \(C\colon\{0,1\}^{a+\log R}\to\{0,1\}^{k\cdot\operatorname{polylog}R}\) with widths \(\underline{d}\) satisfying the following._

1. _For every_ \(\mathbf{w}\in\{0,1\}^{a}\) _and every_ \(\mathbf{z}\in\{0,1\}^{\log R}\)_, we have_ \[G(\operatorname{hash}(\mathbf{w}))_{\mathbf{z}}=\mathsf{PARITY}(C(\mathbf{w}, \mathbf{z})),\] _thinking of_ \(\mathbf{z}\) _as a number in_ \(\{0,1,\ldots,R-1\}\)_._
2. _The maximum width_ \(\underline{d}_{\max}\) _is at most_ \(ak\cdot\operatorname{polylog}R\)_._
3. _The total number of weights_ \(w\left(\underline{d}\right)\) _is at most_ \((a+k)\cdot ak\cdot\operatorname{polylog}R\)_._

_Remark D.10_ (The role of the parity functions).: One can combine Lemma D.9 with Lemma D.5 to obtain threshold networks computing the function \((\mathbf{u},\mathbf{z})\mapsto G(\mathbf{u})_{\mathbf{z}}\). In Lemma D.9, instead of describing a network that computes the function \((\mathbf{u},\mathbf{z})\mapsto G(\mathbf{u})_{\mathbf{z}}\), we describe a network \(C\) satisfying \(G(\operatorname{hash}(\mathbf{w}))_{\mathbf{z}}=\mathsf{PARITY}(C(\mathbf{w}, \mathbf{z}))\). The only reason for this more complicated statement is that it leads to a slightly better depth complexity in Theorem 3.1.

We reiterate that the proof of Lemma D.9 heavily relies on prior work. For the most part, this prior work studies a Boolean circuit model that is closely related to, but distinct from, the "binary threshold network" model in which we are interested. We introduce the circuit model next.

**Definition D.11** (\(\widehat{\operatorname{LT}}_{L}\) circuits).: An \(\widehat{\operatorname{LT}}_{L}\) circuit is defined just like a depth-\(L\) binary threshold network (Definition 2.1), except that we allow arbitrary integer weights (\(\mathbf{W}^{(l)}\in\mathbb{Z}^{d_{l}\times d_{l-1}}\)); we allow arbitrary integer thresholds (\(\mathbf{b}^{(l)}\in\mathbb{Z}^{d_{l}}\)); and we do not allow any scaling (\(\gamma^{(l)}=\mathbf{1}^{d_{l}}\)). The _size_ of the circuit is the sum of the absolute values of the weights, i.e.,

\[\sum_{l=1}^{L}\sum_{i=1}^{d_{l}}\sum_{j=1}^{d_{l-1}}|\mathbf{W}^{(l)}_{ij}|.\]

_Remark D.12_ (Parallel wires).: In the context of circuit complexity, it is perhaps more natural to stipulate that the weights are always \(\{\pm 1\}\); there can be any number of _parallel wires_ between two nodes, including zero; and the size of the circuit is the total number of wires. This is completely equivalent to Definition D.11.

The proof of Lemma D.9 relies on circuits performing arithmetic. A long line of research investigated the depth complexity of (iterated) integer multiplication [16, 9, 69, 78, 37, 73, 32, 80, 79], culminating in the following result by Siu and Roychowdhury [79].

**Theorem D.13** (Iterated multiplication in depth four [79]).: _For every \(n\in\mathbb{N}\), there exists an \(\widehat{\operatorname{LT}}_{4}\) circuit of size \(\operatorname{poly}(n)\) that computes the product of \(n\) given \(n\)-bit integers._

By a standard trick [26], Theorem D.13 implies circuits of the same complexity that compute the iterated product of _polynomials_ over \(\mathbb{F}_{2}\). We include a proof sketch for completeness.

**Corollary D.14** (Iterated multiplication of polynomials over \(\mathbb{F}_{2}\)).: _For every \(n\in\mathbb{N}\), there exists an \(\widehat{\operatorname{LT}}_{4}\) circuit of size \(\operatorname{poly}(n)\) that computes the product of \(n\) given polynomials in \(\mathbb{F}_{2}[x]\), each of which has degree less than \(n\) and is represented by an \(n\)-bit vector of coefficients._

Proof sketch.: Think of the given polynomials as polynomials over \(\mathbb{Z}\), say \(q_{1}(x),\ldots,q_{n}(x)\). If we evaluate one of these polynomials on a power of two, say \(q_{i}(2^{s})\), and then write the output in binary, the resulting string consists of the coefficients of \(q_{i}\), with \(s-1\) zeroes inserted between every two bits. Therefore, by using the \(\operatorname{poly}(ns)\)-size circuit of Theorem D.13 (with some of its inputs fixed to zeroes), we can compute the product \(q_{1}(2^{s})\cdot q_{2}(2^{s})\cdots q_{n}(2^{s})=q(2^{s})\), where \(q=q_{1}\cdot q_{2}\cdots q_{n}\). Every coefficient of \(q\) is a nonnegative integer bounded by \(n^{n}\), so if we choose \(s=\lceil n\log n\rceil\), then the binary expansion of \(q(2^{s})\) is the concatenation of all of the binary expansions of the coefficients of \(q\). To reduce mod \(2\), we simply discard all but the lowest-order bit of each of those coefficients.

At this point, we are ready to construct a circuit that computes a \(k\)-wise uniform generator. The construction is based on the work of Healy and Viola [36].

**Lemma D.15** (A \(k\)-wise uniform generator in the \(\widehat{\mathrm{LT}}_{L}\) model).: _Let \(k,R\in\mathbb{N}\) where \(R\) is a power of two. There exists a \(k\)-wise uniform generator \(G\colon\{0,1\}^{r}\to\{0,1\}^{R}\), an \(\mathbb{F}_{2}\)-linear function \(C_{0}\colon\{0,1\}^{\log R}\to\{0,1\}^{O(\log R\cdot\log k)}\), and an \(\widehat{\mathrm{LT}}_{4}\) circuit \(C_{1}\colon\{0,1\}^{O(k\cdot\log R)}\to\{0,1\}^{O(k\cdot\log R)}\) with the following properties._

* _The seed length is_ \(r=O(k\cdot\log R)\)_._
* _For every seed_ \(\mathbf{u}\in\{0,1\}^{r}\) _and every_ \(\mathbf{z}\in\{0,1\}^{\log R}\)_, we have_ \[G(\mathbf{u})_{\mathbf{z}}=\mathsf{PARITY}(C_{1}(\mathbf{u},C_{0}(\mathbf{z}) )),\] _thinking of_ \(\mathbf{z}\) _as a number in_ \(\{0,1,\ldots,R-1\}\)_._
* _The circuit_ \(C_{1}\) _has size_ \(k\cdot\mathrm{polylog}\,R\)_._

Proof.: If \(k\geq R\), the lemma is trivial, so assume \(k<R\). We use the following standard example of a \(k\)-wise independent generator [22, 1]. Let \(n=\log R\), let \(E(x)\in\mathbb{F}_{2}[x]\) be an irreducible polynomial of degree \(n\), and let \(\mathbb{F}_{2^{n}}\) be the finite field consisting of all polynomials in \(\mathbb{F}_{2}[x]\) modulo \(E(x)\). The seed of the generator is interpreted as a list of field elements: \(\mathbf{u}=(p_{0},p_{1},\ldots,p_{k-1})\in\mathbb{F}_{2^{n}}^{k}\). Each index \(\mathbf{z}\in\{0,1,\ldots,R-1\}\) can be interpreted as a field element \(\mathbf{z}\in\mathbb{F}_{2^{n}}\). The output of the generator is given by

\[G(\mathbf{u})_{\mathbf{z}}=\text{the lowest order bit of }p_{0}\cdot\mathbf{z}^{0}+p_{1}\cdot \mathbf{z}^{1}+\cdots+p_{k-1}\cdot\mathbf{z}^{k-1},\]

where the arithmetic takes place in \(\mathbb{F}_{2^{n}}\).

To study the circuit complexity of this generator, let us first focus on a single term \(p_{i}\cdot\mathbf{z}^{i}\). Since we are thinking of \(\mathbf{z}\) as a field element \(\mathbf{z}\in\mathbb{F}_{2^{n}}\), we can also think of it as a polynomial \(\mathbf{z}(x)\in\mathbb{F}_{2}[x]\) of degree less than \(n\). Write \(\mathbf{z}(x)=\sum_{j=0}^{n-1}\mathbf{z}_{j}\cdot x^{j}\). We compute the power \(\mathbf{z}^{i}\) by a "repeated squaring" approach. Write \(i=\sum_{m\in M}2^{m}\), where \(M\subseteq\{0,1,\ldots,\lfloor\log i\rfloor\}\). Then

\[p_{i}(x)\cdot\mathbf{z}(x)^{i}=p_{i}(x)\cdot\prod_{m\in M}\left(\sum_{j=0}^{n -1}\mathbf{z}_{j}\cdot x^{j}\right)^{2^{m}}=p_{i}(x)\cdot\prod_{m\in M}\sum_ {j=0}^{n-1}\mathbf{z}_{j}\cdot x^{j\cdot 2^{i}},\]

since we are working in characteristic two. For each \(m\in M\) and each \(j<n\), let \(e_{m,j}(x)=x^{j\cdot 2^{m}}\bmod E(x)\), a polynomial of degree less than \(n\). That way,

\[p_{i}(x)\cdot\mathbf{z}(x)^{i}\equiv p_{i}(x)\cdot\prod_{m\in M}\sum_{j=0}^{ n-1}\mathbf{z}_{j}\cdot e_{m,j}(x)\pmod{E(x)}.\] (10)

The function \(C_{0}(\mathbf{z})\) computes \(\sum_{j=0}^{n-1}\mathbf{z}_{j}\cdot e_{m,j}\) for every \(m\in\{0,1,\ldots,\lfloor\log k\rfloor\}\). This function is \(\mathbb{F}_{2}\)-linear, because for each \(m\in\{0,1,\ldots,\lfloor\log k\rfloor\}\) and each \(s<n\), the \(s\)-th bit of \(\sum_{j=0}^{n-1}\mathbf{z}_{j}\cdot e_{m,j}\) is given by

\[\bigoplus_{j:e_{m,j,s}=1}\mathbf{z}_{j},\]

where \(e_{m,j,s}\) denotes the \(s\)-th coefficient of the polynomial \(e_{m,j}\).

Next, the circuit \(C_{1}\) applies \(k\) copies of the iterated multiplication circuit from Corollary D.14, in parallel, to compute the polynomial on the right-hand side of (10) for each \(0\leq i<k\). Each iterated multiplication circuit has size \(\mathrm{polylog}\,R\), so altogether, \(C_{1}\) has size \(k\cdot\mathrm{polylog}\,R\).

At this point, we have computed polynomials \(r_{0},r_{1},\ldots,r_{k-1}\in\mathbb{F}_{2}[x]\), each of degree \(O(n\log k)\), such that \(r_{i}(x)\equiv p_{i}(x)\cdot\mathbf{z}(x)^{i}\pmod{E(x)}\). Next, we need to sum these terms up, reduce mod \(E(x)\), and output the lowest-order bit. For each \(j\leq O(n\log k)\), let \(r_{ij}\) be the \(x^{j}\) coefficient of \(r_{i}\). Our circuit needs to output the lowest-order bit of

\[\sum_{i=0}^{k-1}r_{i}\bmod E(x)=\sum_{i=0}^{k-1}\sum_{j=0}^{O(n\log k)}r_{ij} \cdot e_{0,j}.\]Now, we are working over characteristic two, so \(\sum\) means bitwise XOR. In other words, the output is given by

\[\bigoplus_{j:e_{0,j,0}=1}\bigoplus_{i=0}^{k-1}r_{ij},\]

i.e., it is the parity function applied to some subset of the output bits of \(C_{1}\). To complete the proof, modify \(C_{1}\) by deleting the unused output gates. 

We have almost completed the proof of Lemma D.9. The last step is to bridge the gap between \(\widehat{\mathrm{LT}}_{L}\) circuits and binary threshold networks. We do so via the following lemma.

**Lemma D.16** (Simulating \(\widehat{\mathrm{LT}}_{L}\) circuits using binary threshold networks).: _Let \(L\geq 1\) be a constant. Let \(C_{0}\colon\{0,1\}^{d_{0}}\to\{0,1\}^{d_{1}}\) be an \(\mathbb{F}_{2}\)-affine function, and let \(C_{1}\colon\{0,1\}^{d_{1}}\to\{0,1\}^{d_{2}}\) be an \(\widehat{\mathrm{LT}}_{L}\) circuit of size \(S\). Then the composition \(C_{1}\circ C_{0}\) can be computed by a depth-\((L+1)\) binary threshold network with widths \(\underline{d}\) satisfying the following._

1. _The maximum width_ \(\underline{d}_{\max}\) _is at most_ \(S\cdot(d_{0}+2)\)_._
2. _The total number of weights_ \(w\left(\underline{d}\right)\) _is at most_ \(O(S^{2}d_{0}+Sd_{0}^{2})\)_._

Proof.: Let us define the _cost_ of a layer in an \(\widehat{\mathrm{LT}}_{L}\) circuit to be the sum of the absolute values of the weights in that layer, so the size of the circuit is the sum of the costs. Lemma D.5 implies that \(C_{1}\circ C_{0}=C_{1}^{\prime}\circ C_{0}^{\prime}\), where \(C_{0}^{\prime}\) is a depth-one binary threshold network and \(C_{1}^{\prime}\) is an \(\widehat{\mathrm{LT}}_{L}\) circuit in which the first layer has cost at most \(S\cdot(d_{0}+2)\) and all subsequent layers have cost at most \(S\).

To complete the proof, let us show by induction on \(L\) that in general, if \(C_{0}^{\prime}\) is a depth-one binary threshold network and \(C_{1}^{\prime}\) is an \(\widehat{\mathrm{LT}}_{L}\) circuit in which the layers have costs \(S_{1},S_{2},\ldots,S_{L}\), then \(C_{1}^{\prime}\circ C_{0}^{\prime}\) can be computed by a depth-\((L+1)\) binary threshold network in which the layers after the input layer have widths \(S_{1},S_{2},\ldots,S_{L}\). Let us write \(C_{i}^{\prime}\) as \(C_{3}\circ C_{2}\), where \(C_{3}\) is the last layer of \(C_{1}^{\prime}\). By induction, \(C_{2}\circ C_{0}^{\prime}\) can be computed by a depth-\(L\) binary threshold network \(C\) in which the layers after the input layer have widths \(S_{1},S_{2},\ldots,S_{L-1}\). Now let us modify \(C_{3}\) and \(C\) so that every wire in \(C_{3}\) has weight either \(0\) or \(1\). If a wire in \(C_{3}\) has an integer weight \(w\notin\{0,1\}\), then we make \(|w|\) many copies of the appropriate output gate of \(C\), negate them if \(w<0\), and then split the wire into \(|w|\) many wires, each with weight \(+1\). This process has no effect on the cost of \(C_{3}\). The process could potentially increase the width of the output layer of \(C\), but its width will not exceed \(S_{L}\), the cost of \(C_{3}\). After this modification, we can simply think of \(C_{3}\) as one more layer in our binary threshold network. 

Lemma D.9 follows immediately from Lemmas D.15 and D.16.

### A hitting set generator with a non-optimal dependence on \(\epsilon\)

In this subsection, we will use the \(k\)-wise independent generators that we developed in the previous subsection to construct our first HSG:

**Lemma D.17** (Non-optimal HSG for conjunctions of literals).: _Let \(R\) be a power of two and let \(\alpha,\epsilon\in(0,1)\). Assume that \(1/R\leq\alpha\leq 1-1/R\). Let \(\mathcal{V}\) be the class of functions \(V\colon\{0,1\}^{R}\to\{0,1\}\) that can be expressed as a conjunction of literals. There exists a generator \(G\colon\{0,1\}^{r}\to\{0,1\}^{R}\) satisfying the following._

1. _For every_ \(V\in\mathcal{V}\)_, if_ \(\mathbb{P}_{\mathbf{y}\sim\operatorname{Ber}(\alpha)^{R}}(V(y)=1)\geq 2\epsilon\)_, then_ \(\mathbb{P}_{\mathbf{u}\in\{0,1\}^{r}}(V(G(\mathbf{u}))=1)\geq\epsilon\)_._
2. _The seed length is_ \(r=O(\log(1/\epsilon)\cdot\log^{2}R)\)_._
3. _For every_ \(\mathbb{F}_{2}\)_-affine function_ \(\operatorname{hash}\colon\{0,1\}^{a}\to\{0,1\}^{r}\)_, the function_ \(C(\mathbf{w},\mathbf{z})=G(\operatorname{hash}(\mathbf{w}))_{\mathbf{z}}\) _can be computed by a depth-_\(8\) _binary threshold network with widths_ \(\underline{d}\) _such that the maximum width_ \(\underline{d}_{\max}\) _at most_ \(a\cdot\log(1/\epsilon)\cdot\operatorname{polylog}R\) _and the total number of weights_ \(w\left(\underline{d}\right)\) _is at most_ \((\log(1/\epsilon)\cdot a^{2}+\log^{2}(1/\epsilon)\cdot a)\cdot\operatorname{ polylylog}R\)_._

_Remark D.18_.: The parameters of Lemma D.17 are not yet sufficient to prove Theorem 3.1. Remember, we need the number of weights to be only \((1+o(1))\cdot\log(1/\epsilon)\). On the other hand, Item 1 is stronger than what the HSG definition requires. This will enable us to improve the seed length of the generator later.

The proof of Lemma D.17 is based on the work of Even, Goldreich, Luby, Nisan, and Velickovic [27]. In particular, we use the following lemma from their work.

**Lemma D.19** (Implications of \(k\)-wise independence [27]).: _Let \(X_{1},\ldots,X_{R}\) be independent \(\{0,1\}\)-valued random variables. Let \(\tilde{X}_{1},\ldots,\tilde{X}_{R}\) be \(k\)-wise independent \(\{0,1\}\)-valued random variables such that \(\tilde{X}_{i}\) is distributed identically to \(X_{i}\) for every \(i\). Then_

\[|\mathbb{P}(X_{1}=X_{2}=\cdots=X_{R}=1)-\mathbb{P}(\tilde{X}_{1}=\tilde{X}_{2 }=\cdots=\tilde{X}_{R}=1)|\leq 2^{-\Omega(k)}.\]

Proof of Lemma D.17.: Let \(Q\) be the smallest positive integer such that \(Q\geq 4R^{2}\) and \(\log\log Q\) is an integer. Let \(\phi\colon\{0,1,\ldots,Q-1\}\to\{0,1\}\) be the function

\[\phi(x)=1\iff x\leq\alpha\cdot Q.\]

We think of \(\phi\) as a function \(\phi\colon\{0,1\}^{\log Q}\to\{0,1\}\) by representing \(x\) in binary.

Let \(\vec{\phi}\colon\{0,1\}^{R\log Q}\to\{0,1\}^{R}\) be \(R\) copies of \(\phi\) applied to \(R\) disjoint input blocks. Let \(G_{0}\colon\{0,1\}^{r}\to\{0,1\}^{R\log Q}\) be a \(k\)-wise independent generator for a suitable value \(k=O(\log(1/\epsilon)\cdot\log R)\). Our generator \(G\) is the composition \(\vec{\phi}\circ G_{0}\).

Now let us prove that \(G\) has the claimed properties. The seed length bound is clear. Now let us analyze the computational complexity of \(G\). To compute \(G(\operatorname{hash}(\mathbf{w}))_{\mathbf{z}}\), we begin by computing \(C_{1}(\mathbf{w},\mathbf{z}\log Q+i)\) for every \(i\in\{0,1,\ldots,\log Q-1\}\), all in parallel, where \(C_{1}\) is the depth-\(5\) network from Lemma D.9. Since \(\log Q\) is a power of two, the binary expansions of the numbers \(\mathbf{z}\log Q,\mathbf{z}\log Q+1,\mathbf{z}\log Q+2,\ldots,\mathbf{z}\log Q +\log Q-1\) simply consist of \(\mathbf{z}\) followed by all possible bitstrings of length \(\log\log Q\). The maximum width of one of these layers is bounded by \(ak\cdot\operatorname{polylog}R=a\cdot\log(1/\epsilon)\cdot\operatorname{polylog}R\), and the total number of weights among these layers is at most \((a+k)\cdot ak\cdot\operatorname{polylog}R=(a+\log(1/\epsilon))\cdot a\cdot \log(1/\epsilon)\cdot\operatorname{polylog}R\). Furthermore, the number of output bits is \(\log(1/\epsilon)\cdot\operatorname{polylog}R\).

Next, recall that to compute a single bit of the output of \(G_{0}\), we need to apply the parity function to the outputs of \(C_{1}\). Therefore, to compute an output bit of our generator \(G\), we need to apply an \(\mathbb{F}_{2}\)-linear function followed by \(\phi\). Observe that \(\phi\) can be computed by a depth-two "\(\mathsf{AC}^{0}\) circuit," i.e., a circuit consisting of unbounded-fan-in AND and OR gates applied to literals, in which the total number of gates is \(O(\log Q)=O(\log R)\). This can be viewed as a special case of an \(\widehat{\operatorname{LT}}_{2}\) circuit of size \(O(\log^{2}R)\). Therefore, by Lemma D.16, the \(\mathbb{F}_{2}\)-linear function followed by \(\phi\) can be computed by a depth-\(3\) binary threshold network in which every layer has width at most \(\log(1/\epsilon)\cdot\operatorname{polylog}R\) and the total number of weights is at most \(\log^{2}(1/\epsilon)\cdot\operatorname{polylog}R\). This completes the analysis of the computational complexity of \(G\).

Next, let us prove the correctness of \(G\), i.e., let us prove Item 1 of Lemma D.17. Let \(V\in\mathcal{V}\) and assume \(\mathbb{P}_{\mathbf{y}\sim\mathrm{Ber}(\alpha)^{R}}(V(\mathbf{y})=1)\geq 2\epsilon\). Since \(V\) is a conjunction of literals, we can write \(V(\mathbf{y})=V_{1}(\mathbf{y}_{1})\cdot V_{2}(\mathbf{y}_{2})\cdots V_{R}( \mathbf{y}_{R})\) for some functions \(V_{1},V_{2},\ldots,V_{R}\colon\{0,1\}\to\{0,1\}\).

We will analyze \(\mathbb{P}_{\mathbf{u}\in\{0,1\}^{r}}(V(G(\mathbf{u}))=1)\) in two stages. First, we compare \(V(\vec{\phi}(G_{0}(\mathbf{u})))\) to \(V(\vec{\phi}(\bar{\mathbf{y}}))\), where \(\bar{\mathbf{y}}\in\{0,1\}^{R\log Q}\) is uniform random. Then, in the second stage, we will compare \(V(\vec{\phi}(\bar{\mathbf{y}}))\) to \(V(\mathbf{y})\), where \(\mathbf{y}\sim\mathrm{Ber}(\alpha)^{R}\).

For the first stage, we are in the situation of Lemma D.19, because the \(R\) many \((\log Q)\)-bit blocks of \(G_{0}(\mathbf{u})\) are \((k/\log Q)\)-wise independent. Therefore,

\[\left|\mathbb{P}_{\mathbf{u}\in\{0,1\}^{r}}(V(G(\mathbf{u}))=1)-\mathbb{P}_{ \bar{\mathbf{y}}\in\{0,1\}^{R\log Q}}(V(\vec{\phi}(\bar{\mathbf{y}}))=1) \right|\leq\exp\left(-\Omega\left(\frac{k}{\log Q}\right)\right)\leq 0.5\epsilon,\]

provided we choose a suitable \(k=O(\log(1/\epsilon)\cdot\log R)\).

Now, for the second stage, observe that if we sample \(\bar{\mathbf{y}}\in\{0,1\}^{\log Q}\) uniformly at random, then \(|\mathbb{P}(\phi(\bar{\mathbf{y}})=1)-\alpha|\leq\frac{1}{Q}\leq\frac{1}{4R^{2}}\). For each \(i\), since \(1/R\leq\alpha\leq 1-1/R\), we have

\[\mathbb{P}_{\bar{\mathbf{y}}\in\{0,1\}^{\log Q}}(V_{i}(\phi(\bar{ \mathbf{y}}))=1) \geq\mathbb{P}_{\mathbf{y}\sim\mathrm{Ber}(\alpha)}(V_{i}(\mathbf{ y})=1)-\frac{1}{4R^{2}}\] \[\geq\left(1-\frac{1}{4R}\right)\cdot\mathbb{P}_{\mathbf{y}\sim \mathrm{Ber}(\alpha)}(V_{i}(\mathbf{y})=1).\]

Therefore,

\[\mathbb{P}_{\bar{\mathbf{y}}\in\{0,1\}^{R\log Q}}(V(\vec{\phi}( \bar{\mathbf{y}}))=1) \geq\left(1-\frac{1}{4R}\right)^{R}\cdot\mathbb{P}_{\mathbf{y}\sim \mathrm{Ber}(\alpha)^{R}}(V(\mathbf{y})=1)\] \[\geq 1.5\epsilon\]

by Bernoulli's inequality. Combining the bounds from the two stages completes the proof. 

### Networks for computing functions that are constant on certain intervals

At this point, we have constructed an HSG for conjunctions of literals with a non-optimal dependence on the threshold parameter \(\epsilon\) (Lemma D.17). To improve the dependence on \(\epsilon\), we will use a technique introduced by Hoza and Zuckerman [38]. They introduced this "error-reduction" technique in the context of derandomizing general space-bounded algorithms, but it is simpler in our context (conjunctions of literals).

The basic idea is as follows. Let \(V\) be a conjunction of literals with a low acceptance probability: \(\mathbb{P}_{\mathbf{y}\sim\mathrm{Ber}(\alpha)^{R}}(V(\mathbf{y})=1)=\epsilon\). We will split \(V\) up as a product,

\[V(\mathbf{y})=V^{(0)}(\mathbf{y}^{(0)})\cdot V^{(1)}(\mathbf{y}^{(1)})\cdots V ^{(T-1)}(\mathbf{y}^{(T-1)}),\]

where each \(V^{(i)}\) is a conjunction of literals with a considerably higher acceptance probability:

\[\mathbb{P}_{\mathbf{y}^{(i)}\sim\mathrm{Ber}(\alpha)^{R_{i}}}(V^{(i)}(\mathbf{ y}^{(i)})=1)\approx\epsilon_{0}\gg\epsilon.\]

We choose \(V^{(0)}\) to be the conjunction of the first few literals in \(V\); \(V^{(1)}\) is the conjunction of the next few literals; etc. To hit a single \(V^{(i)}\), we can use our initial HSG with a relatively high threshold parameter (\(\epsilon_{0}\)). Then, we use pairwise independent hash functions to "recycle" the seed of our initial HSG from one \(V^{(i)}\) to the next.

To implement this technique, one of the ingredients we need is a network that figures out which block \(V^{(i)}\) contains a particular given index \(\mathbf{z}\in\{0,1,\ldots,R-1\}\). In this subsection, we describe networks that handle that key ingredient. The constructions are elementary and straightforward.

First, we review standard circuits for integer comparisons.

**Lemma D.20** (DNFs for comparing integers).: _Let \(R\) be a power of two, let \(I\subseteq[0,R)\) be an interval, and let \(g_{I}\colon\{0,1\}^{\log R}\to\{0,1\}\) be the indicator function for \(I\cap\{0,1,\ldots,R-1\}\) (identifying numbers with their binary expansions). Then \(g_{I}\) can be expressed as a DNF formula consisting of \(O(\log^{2}R)\) terms._Proof.: First, consider the case that \(I=[0,r)\) for some \(r\in\{1,2,\ldots,R\}\). If \(r=R\), then the lemma is trivial, so assume \(r<R\). Let \(S\) be the set of indices at which the binary expansion of \(r\) has a one. For each \(i\in S\), we introduce a term that asserts that the input \(\mathbf{z}\) agrees with the binary expansion of \(r\) prior to position \(i\), and then \(\mathbf{z}\) has a zero in position \(i\). The disjunction of these \(|S|\) many terms computes \(g_{I}\).

The case \(I=[\ell,R)\) for some \(\ell\in\{0,1,\ldots,R-1\}\) is symmetric. Finally, in the general case, we can assume that \(I\) is an intersection of an interval of the form \([\ell,R)\) with an interval of the form \([0,r)\). Therefore, \(g_{I}\) can be expressed in the form \(\mathsf{AND}_{2}\circ\mathsf{OR}_{\log R}\circ\mathsf{AND}_{\log R}\), where \(\mathsf{AND}_{k}\) / \(\mathsf{OR}_{k}\) denotes an AND / OR gate with fan-in \(k\). To complete the proof, observe that every \(\mathsf{AND}_{a}\circ\mathsf{OR}_{b}\) formula can be re-expressed as an \(\mathsf{OR}_{b^{a}}\circ\mathsf{AND}_{a}\) formula. 

**Lemma D.21** (Computing a function that is constant on intervals).: _Let \(T\) and \(R\) be powers of two. Suppose the interval \([0,R)\) has been partitioned into \(T\) subintervals, say \([0,R)=I_{0}\cup I_{1}\cup\cdots\cup I_{T-1}\). Let \(g\colon\{0,1,\ldots,R-1\}\to\{0,1\}^{a}\) be a function that is constant on each subinterval \(I_{j}\). Then for every \(\mathbb{F}_{2}\)-affine function \(C_{0}\colon\{0,1\}^{d_{0}}\to\{0,1\}^{\log R}\), there is a depth-\(6\) binary threshold network \(C\colon\{0,1\}^{d_{0}}\to\{0,1\}^{a+\log R}\) with widths \(\underline{d}\) satisfying the following._

1. _For every_ \(\mathbf{x}\in\{0,1\}^{d_{0}}\)_, we have_ \[C(\mathbf{x})=(g(C_{0}(\mathbf{x})),C_{0}(\mathbf{x})).\]
2. _The maximum width_ \(\underline{d}_{\max}\) _is at most_ \(O(T\cdot\log^{3}R+a+d_{0}\cdot\log R)\)_._
3. _The total number of weights_ \(w\)__\((\underline{d})\) _is at most_ \[aT+O(T\cdot\log^{4}R+d_{0}^{2}\cdot\log R+d_{0}\cdot\log^{2}R+a\cdot\log R).\]

We emphasize that the leading term in the weights bound is \(aT\), with a coefficient of \(1\). This is crucial. It is also important that the weights bound has only a linear dependence on \(T\), the number of intervals.

Proof.: We begin by computing \(C_{0}(\mathbf{x})\) and the negations of all of those bits. By Lemma D.5, we can compute these bits using a depth-two network where the hidden layer has width \(O(d_{0}\cdot\log R)\) and the output layer has width \(O(\log R)\).

Let \(\mathbf{z}=C_{0}(\mathbf{x})\in\{0,1\}^{\log R}\), and think of \(\mathbf{z}\) as a number \(\mathbf{z}\in\{0,1,\ldots,R-1\}\). Our next goal is to compute the \((\log T)\)-bit binary expansion of the unique \(j_{*}\in\{0,1,\ldots,T-1\}\) such that \(\mathbf{z}\in I_{j_{*}}\). To do so, for each position \(i\in\{0,1,\ldots,\log T-1\}\), let \(S_{i}\) be the set of \(j\in\{0,1,\ldots,T-1\}\) such that \(j\) has a \(1\) in position \(i\) of its binary expansion. We have a disjunction, over all \(j\in S_{i}\), of the DNF computing \(g_{I_{j}}\) from Lemma D.20. We also compute all the negations of the bits of \(j_{*}\), and we also copy \(\mathbf{z}\). Altogether, this is a depth-two network where the hidden layer has width \(O(T\cdot\log T\cdot\log^{2}R)=O(T\cdot\log^{3}R)\) and the output layer has width \(O(\log R)\).

Our final goal is to compute \(g(\mathbf{z})\), which can be written in the form \(g^{\prime}(j_{*})\) since \(g\) is constant on each subinterval. We use a "brute-force DNF" to compute \(g^{\prime}\). First, for every \(j\in\{0,1,\ldots,T-1\}\), we have an AND gate that checks whether \(j_{*}=j\). Then each output bit of \(g^{\prime}\) is a disjunction of some of those AND gates. We also copy \(\mathbf{z}\). Altogether, this is a depth-two network where the hidden layer has width \(T+\log R\) and the output layer has width \(a+\log R\).

### Error reduction

In this subsection, we improve our HSG's dependence on \(\epsilon\), as described in the previous subsection. The following theorem should be compared to Lemma D.17. As discussed previously, the proof is based on a technique due to Hoza and Zuckerman [38].

**Theorem D.22** (HSG with near-optimal dependence on \(\epsilon\)).: _Let \(R\) be a power of two and let \(\alpha,\epsilon\in(0,1)\). Assume that \(1/R\leq\alpha\leq 1-1/R\). Let \(\mathcal{V}\) be the class of functions \(V\colon\{0,1\}^{R}\) that can be expressed as a conjunction of literals. There exists a generator \(G\colon\{0,1\}^{r}\to\{0,1\}^{R}\) satisfying the following._

1. \(G\) _is an_ \(\epsilon\)_-HSG for_ \(\mathcal{V}\) _with respect to_ \(\operatorname{Ber}(\alpha)^{R}\)_. That is, if_ \(\mathbb{P}_{\mathbf{y}\sim\operatorname{Ber}(\alpha)^{R}}(V(\mathbf{y})=1)>\epsilon\) _for every_ \(V\in\mathcal{V}\)_, then there exists a seed_ \(\sigma\in\{0,1\}^{r}\) _such that_ \(V(G(\sigma))=1\)_._
2. _The seed length is_ \(r=\log(1/\epsilon)+\log^{3/4}(1/\epsilon)\cdot\operatorname{polylog}R\)_._
3. _For every_ \(\mathbb{F}_{2}\)_-affine function_ \(C_{0}\colon\{0,1\}^{d_{0}}\to\{0,1\}^{\log R}\) _and every fixed seed_ \(\sigma\in\{0,1\}^{r}\)_, the function_ \(\tilde{h}(\mathbf{x})=G(\sigma)_{C_{0}(\mathbf{x})}\) _can be computed by a depth-_\(14\) _binary threshold network with widths_ \(\underline{d}\) _such that the maximum width_ \(\underline{d}_{\max}\) _is at most_ \[\log^{3/4}(1/\epsilon)\cdot\operatorname{polylog}R+O(d_{0}\cdot\log R),\] _and the total number of weights_ \(w\left(\underline{d}\right)\) _is at most_ \[\log(1/\epsilon)+\log^{3/4}(1/\epsilon)\cdot\operatorname{polylog}R+O(d_{0}^{ 2}\cdot\log R+d_{0}\cdot\log^{2}R).\]

Proof.: First we will describe the construction of \(G\); then we will verify its seed length and computational complexity; and finally we will verify its correctness.

Construction.Let \(T\) be the smallest power of two such that \(T\geq\log^{3/4}(1/\epsilon)\). Let

\[\epsilon_{0}=\frac{\epsilon^{1/T}}{2R},\]

and note that \(\log(1/\epsilon_{0})=\Theta(\log^{1/4}(1/\epsilon)+\log R)\). Let \(G_{0}\colon\{0,1\}^{r_{0}}\to\{0,1\}^{R}\) be the generator of Lemma D.17 with error parameter \(\epsilon_{0}\), i.e., for every \(V\in\mathcal{V}\), if \(\mathbb{P}_{\mathbf{y}\sim\operatorname{Ber}(\alpha)^{R}}(V(\mathbf{y})=1) \geq 2\epsilon_{0}\), then \(\mathbb{P}_{\mathbf{u}\in\{0,1\}^{r_{0}}}(V(G_{0}(\mathbf{u}))=1)\geq\epsilon _{0}\). Let \(a\) be the smallest positive integer such that \(2^{a}>R/\epsilon_{0}\). Let \(\mathcal{H}\) be the family of \(\mathbb{F}_{2}\)-affine hash functions \(\operatorname{hash}\colon\{0,1\}^{a}\to\{0,1\}^{r_{0}}\) from Lemma D.6.

A seed for our generator \(G\) consists of a function \(\operatorname{hash}\in\mathcal{H}\), inputs \(\mathbf{w}^{0},\ldots,\mathbf{w}^{T-1}\in\{0,1\}^{a}\), and nonnegative integers \(0=\ell_{0}\leq\ell_{1}\leq\cdots\leq\ell_{T}=R\). Given this data \(\sigma=(\operatorname{hash},\mathbf{w}^{0},\ldots,\mathbf{w}^{T-1},\ell_{0}, \ldots,\ell_{T})\), the output of the generator is given by

\[G(\sigma)=G_{0}(\operatorname{hash}(\mathbf{w}^{0}))_{0\cdots\ell_{1}-1} \&\;G_{0}(\operatorname{hash}(\mathbf{w}^{1}))_{\ell_{1}\cdots\ell_{2}-1}\& \cdots\&\;G_{0}(\operatorname{hash}(\mathbf{w}^{T-1}))_{\ell_{T-1}\cdots\ell_ {T}-1}.\]

In the equation above, \(\mathbf{y}_{a\cdots b}\) denotes the substring of \(\mathbf{y}\) consisting of the bits at positions \(a,a+1,a+2,\ldots,b\), and \(\&\) denotes concatenation.

Seed length and computational complexity.Since \(|\mathcal{H}|\leq 2^{O(a+r_{0})}\), the description length of \(\operatorname{hash}\) is \(O(a+r_{0})\). The description length of \(\mathbf{w}^{0},\ldots,\mathbf{w}^{T-1}\) is \(aT\), and the description length of \(\ell^{0},\ldots,\ell^{T}\) is \(O(T\log R)\). By our choices of \(a\) and \(\epsilon_{0}\), we have

\[a\leq\log(1/\epsilon_{0})+O(\log R)=\frac{\log(1/\epsilon)}{T}+O(\log R).\]

Furthermore, by Lemma D.17, we have

\[r_{0}=O(\log(1/\epsilon_{0})\cdot\log^{2}R).\]

Therefore, the overall seed length of our generator is

\[aT+O(r_{0}+T\log R+a)\leq\log(1/\epsilon)+\log^{3/4}(1/\epsilon)\cdot \operatorname{polylog}R.\]

To analyze the computational complexity, fix an arbitrary seed

\[\sigma=(\operatorname{hash},\mathbf{w}^{0},\ldots,\mathbf{w}^{T-1},\ell_{0}, \ldots,\ell_{T}).\]The numbers \(\ell_{0},\ldots,\ell_{T}\) partition the interval \([0,R)\) into subintervals, namely \([0,R)=[\ell_{0},\ell_{1})\cup[\ell_{1},\ell_{2})\cup\cdots\cup[\ell_{T-1},\ell_{T})\). Define \(g\colon\{0,1,\ldots,R-1\}\to\{0,1\}^{a}\) by the rule

\[g(\mathbf{z})=\mathbf{w}^{j}\text{ where }j\text{ is such that }\mathbf{z}\in[\ell_{j},\ell_{j+1}).\]

Then \(g\) is constant on each subinterval \([\ell_{j},\ell_{j+1})\), so we may apply Lemma D.21 to obtain a depth-\(6\) binary threshold network \(C_{1}\colon\{0,1\}^{d_{0}}\to\{0,1\}^{a+\log R}\) computing the function \(C(\mathbf{x})=(g(C_{0}(\mathbf{x})),C_{0}(\mathbf{x}))\). In this network, every layer has width at most

\[O(T\cdot\log^{3}R+a+d_{0}\cdot\log R)=\log^{3/4}(1/\epsilon)\cdot\operatorname{ polylog}R+O(d_{0}\cdot\log R),\]

and the total number of weights is at most

\[aT+O(T\cdot\log^{4}R+d_{0}^{2}\cdot\log R+d_{0}\cdot\log^{2}R+a \cdot\log R)\] \[\leq\log(1/\epsilon)+\log^{3/4}(1/\epsilon)\cdot\operatorname{ polylog}R+O(d_{0}^{2}\cdot\log R+d_{0}\cdot\log^{2}R).\]

Let \(\mathbf{z}=C_{0}(\mathbf{x})\), and let \(\mathbf{w}=g(\mathbf{z})\). Our remaining goal is to compute \(G(\sigma)_{\mathbf{z}}\), which is equal to \(G_{0}(\operatorname{hash}(\mathbf{w}))_{\mathbf{z}}\). To do so, we use the network guaranteed to exist by Lemma D.17. This network, which we call \(C_{2}\), has depth \(8\). Every layer in this network has width at most

\[a\cdot\log(1/\epsilon_{0})\cdot\operatorname{polylog}R=\sqrt{\log(1/\epsilon) }\cdot\operatorname{polylog}R.\]

The total number of weights in this network is at most

\[(\log(1/\epsilon_{0})\cdot a^{2}+\log^{2}(1/\epsilon_{0})\cdot a)\cdot \operatorname{polylog}R=\log^{3/4}(1/\epsilon)\cdot\operatorname{polylog}R.\]

Composing \(C_{2}\) with \(C_{1}\) completes the analysis of the computational complexity of our HSG.

Correctness.Finally, let us prove the correctness of our HSG. For convenience, for any \(n\in\mathbb{N}\) and any function \(V\colon\{0,1\}^{n}\to\{0,1\}\), we write \(\mathbb{E}(V)\) to denote the quantity \(\mathbb{P}_{\mathbf{y}\sim\operatorname{Ber}(\alpha)^{n}}(V(\mathbf{y})=1)\).

Fix any \(V\in\mathcal{V}\) such that \(\mathbb{E}(V)>\epsilon\). Since \(V\) is a conjunction of literals, we can write \(V\) in the form

\[V(\mathbf{y})=V_{0}(\mathbf{y}_{0})\cdot V_{1}(\mathbf{y}_{1})\cdots V_{R-1}( \mathbf{y}_{R-1})\]

for some functions \(V_{0},V_{1},\ldots,V_{R-1}\colon\{0,1\}\to\{0,1\}\). For each \(0\leq a\leq b\leq R-1\), define

\[V_{a\cdots b}=V_{a}\cdot V_{a+1}\cdots V_{b}.\]

We inductively define numbers \(0=\ell_{0}\leq\ell_{1}\leq\cdots\leq\ell_{T}\) as follows. Assume that we have already defined \(\ell_{0},\ldots,\ell_{i}\). Let \(\ell_{i+1}\) be the smallest integer in \(\{\ell_{i}+1,\ldots,R-1\}\) such that

\[\mathbb{E}(V_{\ell_{i}\cdots\ell_{i+1}-1})\leq\epsilon^{1/T},\]

or let \(\ell_{i+1}=R\) if no such \(\ell_{i+1}\) exists. Define \(V^{(i)}=V_{\ell_{i}\cdots\ell_{i+1}-1}\). Observe that \(\ell_{T}=R\), because otherwise we would have

\[\epsilon<\mathbb{E}(V)\leq\prod_{i=0}^{T-1}\mathbb{E}(V_{i})\leq(\epsilon^{1/T })^{T}=\epsilon,\]

a contradiction. Furthermore, \(\mathbb{E}(V_{i})>\epsilon^{1/T}/R=2\epsilon_{0}\), because each literal in \(V\) is satisfied with probability at least \(\min\{\alpha,1-\alpha\}\geq 1/R\). Therefore, if we define

\[S_{i}=\{\mathbf{u}\in\{0,1\}^{r_{0}}:V_{i}(G_{0}(\mathbf{u})_{\ell_{i}\cdots \ell_{i+1}-1})=1\}\]

and \(\rho_{i}=|S_{i}|/2^{r_{0}}\), then the correctness of \(G_{0}\) ensures that \(\rho_{i}\geq\epsilon_{0}\).

Next, we will show that there exist \(\operatorname{hash},\mathbf{w}^{0},\ldots,\mathbf{w}^{T-1}\) such that for every \(i\), we have \(\operatorname{hash}(\mathbf{w}^{i})\in S_{i}\). To prove it, pick \(\operatorname{hash}\in\mathcal{H}\) at random. For each \(i\in\{0,1,\ldots,T-1\}\) and each \(\mathbf{w}\in\{0,1\}^{a}\), let \(X_{i,\mathbf{w}}\) be the indicator random variable for the "good" event \(\operatorname{hash}(\mathbf{w})\in S_{i}\). Define \(X_{i}=\sum_{\mathbf{w}\in\{0,1\}^{a}}X_{i,\mathbf{w}}\). Then for every \(i\), by pairwise independence, we have

\[\mathbb{E}(X_{i}) =2^{a}\cdot\rho_{i}\] \[\text{and }\text{Var}(X_{i}) =2^{a}\cdot\rho_{i}\cdot(1-\rho_{i})\leq 2^{a}\cdot\rho_{i}.\]

Therefore, by Chebyshev's inequality,

\[\mathbb{P}(X_{i}=0)\leq\frac{1}{2^{a}\cdot\rho_{i}}\leq\frac{1}{2^{a}\cdot \epsilon_{0}}<\frac{1}{R}.\]

Consequently, by the union bound over all \(i\), there is a nonzero chance that \(X_{0}=X_{1}=\cdots=X_{T-1}=0\), in which case there exist \(\mathbf{w}^{0},\ldots,\mathbf{w}^{T-1}\) such that \(\operatorname{hash}(\mathbf{w}^{i})\in S_{i}\) for every \(i\).

At this point, we have constructed our seed \(\sigma=(\operatorname{hash},\mathbf{w}^{0},\ldots,\mathbf{w}^{T-1},\ell_{0}, \ldots,\ell_{T})\). By the construction of \(G\), we have \(V(G(\sigma))=1\)Theorem 3.1 readily follows from Theorem D.22, as we now explain.

**Recall Theorem 3.1**.: Let \(f\colon\{0,1\}^{d_{0}}\to\{0,1,\star\}\) be any function.9 Let \(N=|f^{-1}(\{0,1\})|\) and \(N_{1}=|f^{-1}(1)|\). There exists a depth-\(14\) binary threshold network \(\tilde{h}\colon\{0,1\}^{d_{0}}\to\{0,1\}\), with widths \(\tilde{\underline{d}}\), satisfying the following.

Footnote 9: When \(f(\mathbf{x})=\star\), the interpretation is that \(f\) is “undefined” on \(\mathbf{x}\), _i.e.,_\(f\) is a “partial” function.

1. \(\tilde{h}\) is consistent with \(f\), _i.e.,_ for every \(\mathbf{x}\in\{0,1\}^{d_{0}}\), if \(f(\mathbf{x})\in\{0,1\}\), then \(\tilde{h}(\mathbf{x})=f(\mathbf{x})\).
2. The total number of weights in \(\tilde{h}\) is at most \((1+o(1))\cdot\log\binom{N}{N_{1}}+\operatorname{poly}(d_{0})\). More precisely, \[w\left(\tilde{\underline{d}}\right)=\log\binom{N}{N_{1}}+\left(\log\binom{N}{ N_{1}}\right)^{3/4}\cdot\operatorname{polylog}N+O(d_{0}^{2}\cdot\log N)\,.\]
3. Every layer of \(\tilde{h}\) has width at most \((\log\binom{N}{N_{1}})^{3/4}\cdot\operatorname{poly}(d_{0})\). More precisely, \[\tilde{\underline{d}}_{\max}=\left(\log\binom{N}{N_{1}}\right)^{3/4}\cdot \operatorname{polylog}N+O(d_{0}\cdot\log N)\,.\]

Proof.: Let \(R=2^{2^{\lceil\log N\rceil}}\). Let \(C_{0}\colon\{0,1\}^{d_{0}}\to\{0,1\}^{\log R}\) be an \(\mathbb{F}_{2}\)-affine function that is injective on \(\mathcal{X}\); such a function is guaranteed to exist by Lemma D.4. Define \(V\colon\{0,1\}^{R}\to\{0,1\}\) by the rule

\[V(\mathbf{y})=1\iff\forall\mathbf{x}\in\mathcal{X},\;\operatorname{\mathit{Y }}_{C_{0}(\mathbf{x})}=f(x).\]

This function \(V\) is a conjunction of \(N_{1}\) variables and \(N-N_{1}\) negated variables.

If \(N_{1}\in\{0,N\}\), then the theorem is trivial, because we can take \(\tilde{h}\) to be a constant function. Assume, therefore, that \(0<N_{1}<N\). Let \(\alpha=N_{1}/N\), and note that \(1/R\leq\alpha\leq 1-1/R\). Let \(\epsilon=\frac{1}{2}\alpha^{N_{1}}\cdot(1-\alpha)^{N-N_{1}}=2^{-H(\alpha)\cdot N -1}\), and note that

\[\mathbb{P}_{\mathbf{y}\sim\operatorname{Ber}(\alpha)^{R}}(V(\mathbf{y})=1)=2\epsilon.\]

Let \(G\colon\{0,1\}^{r}\to\{0,1\}^{R}\) be the HSG from Theorem D.22. There exists a seed \(\sigma\in\{0,1\}^{r}\) such that \(V(G(\sigma))=1\). Our network \(\tilde{h}\) computes the function \(\tilde{h}(\mathbf{x})=G(\sigma)_{C_{0}(\mathbf{x})}\). Since \(V(G(\sigma))=1\), we must have \(\tilde{h}(\mathbf{x})=f(\mathbf{x})\) for every \(x\in\mathcal{X}\).

To bound the computational complexity, observe that \(\log(1/\epsilon)=H(\alpha)\cdot N+1\leq\log\binom{N}{N_{1}}+O(\log N)\). Therefore, every layer of \(\tilde{h}\) has width at most

\[\left(\log\binom{N}{N_{1}}\right)^{3/4}\cdot\operatorname{polylog}N+O(d_{0} \cdot\log N),\]

and the total number of weights in \(\tilde{h}\) is at most

\[\log\binom{N}{N_{1}}+\left(\log\binom{N}{N_{1}}\right)^{3/4}\cdot\operatorname {polylog}N+O(d_{0}^{2}\cdot\log N+d_{0}\cdot\log^{2}N).\]

Finally, we have \(N\leq 2^{d_{0}}\), so the last term above can be simplified to \(O(d_{0}^{2}\cdot\log N)\). 

_Remark D.23_.: In Theorem 3.1, the weights bound has an \(O(d_{0}^{2}\cdot\log N)\) term. This term is close to optimal; see Appendix E for further details.

### XOR networks

In what follows, we denote the activation function \(\sigma\left(x\right)=\mathbb{I}\left\{x>0\right\}\).

**Lemma D.24** (XOR NN).: _The \(\mathrm{XOR}\) function can be implemented with a single-hidden-layer fully connected binary threshold network with input dimension 2 and \(c_{\mathrm{XOR}}\) parameters by_

\[h_{\mathrm{XOR}}\left(\begin{array}{c}x_{1}\\ x_{2}\end{array}\right)=\sigma\left(1\odot\left(\begin{array}{cc}1&1\\ 1&1\end{array}\right)\cdot\sigma\left(\left(\begin{array}{c}1\\ -1\end{array}\right)\odot\left(\begin{array}{cc}1&1\\ 1&1\end{array}\right)\left(\begin{array}{c}x_{1}\\ x_{2}\end{array}\right)+\left(\begin{array}{c}0\\ 2\end{array}\right)\right)-1\right)\,.\]

Proof.: We can simplify \(h_{\mathrm{XOR}}\) as

\[h_{\mathrm{XOR}}\left(\begin{array}{c}x_{1}\\ x_{2}\end{array}\right) =\sigma\left(\left(\begin{array}{cc}1&1\\ 1&1\end{array}\right)\cdot\sigma\left(\left(\begin{array}{c}1\\ -1\end{array}\right)\odot\left(\begin{array}{cc}1&1\\ 1&1\end{array}\right)\left(\begin{array}{c}x_{1}\\ x_{2}\end{array}\right)+\left(\begin{array}{c}0\\ 2\end{array}\right)\right)-1\right)\] \[=\sigma\left(\left(\begin{array}{cc}1&1\\ 1&1\end{array}\right)\cdot\sigma\left(\left(\begin{array}{c}x_{1}+x_{2}\\ -x_{1}-x_{2}+2\end{array}\right)\right)-1\right)\] \[=\sigma\left(\sigma\left(x_{1}+x_{2}\right)+\sigma\left(2-x_{1}-x _{2}\right)-1\right)\] \[=\mathbb{I}\left\{\mathbb{I}\left\{x_{1}+x_{2}>0\right\}+\mathbb{ I}\left\{x_{1}+x_{2}<2\right\}>1\right\}\] \[=\mathbb{I}\left\{\left(\begin{array}{c}x_{1}\\ x_{2}\end{array}\right)\neq\left(\begin{array}{c}0\\ 0\end{array}\right)\right\}+\mathbb{I}\left\{\left(\begin{array}{c}x_{1}\\ x_{2}\end{array}\right)\neq\left(\begin{array}{c}1\\ 1\end{array}\right)\right\}>1\right\}\] \[=\mathbb{I}\left\{\left(\begin{array}{c}x_{1}\\ x_{2}\end{array}\right)\neq\left(\begin{array}{c}0\\ 0\end{array}\right)\text{ and }\left(\begin{array}{c}x_{1}\\ x_{2}\end{array}\right)\neq\left(\begin{array}{c}1\\ 1\end{array}\right)\right\}\] \[=\mathrm{XOR}\left(x_{1},x_{2}\right)\,.\]

_Remark D.25_.: Notice that the function \(\mathrm{Id}:\left\{0,1\right\}\rightarrow\left\{0,1\right\}\) defined as \(\mathrm{Id}\left(0\right)=0\) and \(\mathrm{Id}\left(1\right)=1\) can be implemented using any depth \(L\) network with a single input dimension and \(c_{\mathrm{Id}}\cdot L\) parameters.

Following this remark, for simplicity we shall assume that \(h_{1}\) and \(h_{2}\) in the following Lemma are of the same depth, as they can be elongated with \(O\left(L\right)\) additional parameters, which are negligible in the subsequent analysis.

**Recall Lemma 3.3**.: Let \(h_{1},h_{2}\) be two binary networks with depths \(L_{1}\leq L_{2}\) and widths \(\underline{d}^{(1)},\underline{d}^{(2)}\), respectively. Then, there exists a network \(h\) with depth \(L_{\mathrm{XOR}}\triangleq L_{2}+2\) and widths

\[\underline{d}_{\mathrm{XOR}}\triangleq\left(d_{1}^{(1)}+d_{1}^{(2)},\,\ldots, \,d_{L_{1}}^{(1)}+d_{L_{1}}^{(2)},\,d_{L_{1}+1}^{(2)}+1,\,\ldots,\,d_{L_{2}}^{ (2)}+1,\,2,\,1\right)\,,\]

such that for all inputs \(\mathbf{x}\in\{0,1\}^{d_{0}}\), \(h\left(\mathbf{x}\right)=h_{1}\left(\mathbf{x}\right)\oplus h_{2}\left( \mathbf{x}\right)\).

The lemma above is given immediately by the lemma we state and prove next.

**Lemma D.26** (XOR of Two NNs).: _Let \(h_{1},h_{2}:\mathcal{X}\rightarrow\{0,1\}\) be quantized fully connected binary threshold networks with depths \(L^{\prime}\) and widths \(\underline{d}^{(1)},\underline{d}^{(2)}\), respectively. Let \(L\geq 2+L^{\prime}\) and \(d\geq\underline{d}_{\mathrm{XOR}}\). Let \(\Theta^{\mathcal{BTN}}\left(\underline{d};h_{1},h_{2}\right)\) be the subset of \(\Theta^{\mathcal{BTN}}\left(\underline{d}\right)\) such that for all \(\boldsymbol{\theta}\in\Theta^{\mathcal{BTN}}\left(\underline{d};h_{1},h_{2}\right)\), \(\boldsymbol{\theta}\) has the following form:_

* _For_ \(l=1\)_:_ \[\mathbf{W}_{1}=\left(\begin{array}{c}\mathbf{W}_{1}^{(1)}\\ \mathbf{W}_{1}^{(2)}\\ \tilde{\mathbf{W}}_{1}\end{array}\right),\,\mathbf{b}_{1}=\left(\begin{array} []{c}\mathbf{b}_{1}^{(1)}\\ \mathbf{b}_{1}^{(2)}\\ \tilde{\mathbf{b}}_{1}\end{array}\right),\,\gamma_{1}=\left(\begin{array}{c }\mathbf{1}_{d_{1}^{(1)}}\\ \mathbf{1}_{d_{2}^{(2)}}\\ \mathbf{0}_{d_{l}-d_{l}^{(1)}-d_{l}^{(2)}}\end{array}\right)\] _with arbitrary_ \(\tilde{\mathbf{W}}_{1},\tilde{\mathbf{b}}_{1}\)_._
* _For_ \(l=2,\ldots,L^{\prime}\)_:_ \[\mathbf{W}_{l} =\left(\begin{array}{cc}\mathbf{W}_{l}^{(1)}&\mathbf{0}_{d_{l} ^{(1)}\times d_{l-1}^{(2)}}&\tilde{\mathbf{W}}_{l}^{1}\\ \mathbf{0}_{d_{l}^{(2)}\times d_{l-1}^{(1)}}&\mathbf{W}_{l}^{(2)}&\tilde{ \mathbf{W}}_{l}^{2}\\ \tilde{\mathbf{W}}_{l}^{3}&\tilde{\mathbf{W}}_{l}^{4}&\tilde{\mathbf{W}}_{l}^ {5}\end{array}\right)\in\{0,1\}^{d_{l}\times d_{l-1}}\,,\] \[\mathbf{b}_{l} =\left(\begin{array}{c}\mathbf{b}_{1}^{(1)}\\ \mathbf{b}_{1}^{(2)}\\ \mathbf{b}_{l}\end{array}\right)\in\{-d_{l-1},\ldots,-1,0,1,\ldots,d_{l-1}-1 \}^{d_{l}}\,,\] \[\gamma_{l} =\left(\begin{array}{c}\mathbf{1}_{d_{l}^{(1)}}\\ \mathbf{1}_{d_{l}^{(2)}}\\ \mathbf{0}_{d_{l}-d_{l}^{(1)}-d_{l}^{(2)}}\end{array}\right)\in\{0,1\}^{d_{l }}\,,\] _with arbitrary_ \(\tilde{\mathbf{W}}_{l}^{1},\tilde{\mathbf{W}}_{l}^{2},\tilde{\mathbf{W}}_{l}^ {3},\tilde{\mathbf{W}}_{l}^{4},\tilde{\mathbf{W}}_{l}^{5},\tilde{\mathbf{b}}_{l}\)_._
* _For_ \(l=L^{\prime}+k\)_,_ \(k=1,2\)_:_ \[\mathbf{W}_{l} =\left(\begin{array}{cc}\mathbf{W}_{\underline{k}}^{\mathrm{ XOR}}&\tilde{\mathbf{W}}_{l}^{1}\\ \tilde{\mathbf{W}}_{l}^{2}&\tilde{\mathbf{W}}_{l}^{3}\end{array}\right)\in\{0,1 \}^{d_{l}\times d_{l-1}}\,,\] \[\mathbf{b}_{l} =\left(\begin{array}{c}\mathbf{b}_{k}^{\mathrm{ XOR}}\\ \tilde{\mathbf{b}}_{l}\end{array}\right)\in\{-d_{l-1},\ldots,-1,0,1,\ldots,d_{l-1}-1 \}^{d_{l}}\,,\gamma_{l}=\left(\begin{array}{c}\gamma_{k}^{\mathrm{ XOR}}\\ \mathbf{0}\end{array}\right)\in\{0,\pm 1\}^{d_{l}}\,\,.\]
* _And for_ \(l>L^{\prime}+2\)_:_ \[\mathbf{W}_{l} =\left(\begin{array}{cc}\mathbf{W}^{Id}&\tilde{\mathbf{W}}_{l}^ {1}\\ \tilde{\mathbf{W}}_{l}^{2}&\tilde{\mathbf{W}}_{l}^{3}\end{array}\right)\in\{0,1 \}^{d_{l}\times d_{l-1}}\,,\] \[\mathbf{b}_{l} =\left(\begin{array}{c}\mathbf{b}_{l}^{Id}\\ \tilde{\mathbf{b}}_{l}\end{array}\right)\in\{-d_{l-1},\ldots,-1,0,1,\ldots,d_{l- 1}-1\}^{d_{l}}\,,\gamma_{l}=\left(\begin{array}{c}\gamma_{0}^{Id}\\ \mathbf{0}\end{array}\right)\in\{0,\pm 1\}^{d_{l}}\,\,.\]

_Then for all \(\boldsymbol{\theta}\in\Theta^{\mathcal{BTN}}\left(\underline{d};h_{1},h_{2}\right) h_{\boldsymbol{\theta}}=h_{1}\oplus h_{2}\)._

An illustration of this construction is given in Figure 3.

Proof.: We prove the claim by induction. For \(l=1\) we have \(d_{0}=d_{0}^{(1)}=d_{0}^{(2)}\) and

\[h_{\theta}^{(1)}\left(\mathbf{x}\right) =\gamma_{1}\odot\sigma\left(\mathbf{W}_{1}h_{\theta}^{(0)}\left( \mathbf{x}\right)+\mathbf{b}_{1}\right)\] \[=\left(\begin{array}{c}\mathbf{1}_{d_{1}^{\prime}}\\ \mathbf{1}_{d_{1}^{\prime}}\\ \mathbf{0}_{d_{1}-d_{1}^{\prime}-d_{1}^{\prime}}\end{array}\right)\odot\sigma \left(\left(\begin{array}{c}\mathbf{W}_{1}^{(1)}\\ \mathbf{W}_{1}^{(2)}\\ \tilde{\mathbf{W}}_{1}^{1}\end{array}\right)\mathbf{x}+\left(\begin{array}{ c}\mathbf{b}_{1}^{(1)}\\ \mathbf{b}_{1}^{(2)}\\ \tilde{\mathbf{b}}_{1}\end{array}\right)\right)\] \[=\left(\begin{array}{c}\sigma\left(\mathbf{W}_{1}^{(1)} \mathbf{x}+\mathbf{b}_{1}^{(1)}\right)\\ \sigma\left(\mathbf{W}_{1}^{(2)}\mathbf{x}+\mathbf{b}_{1}^{(2)}\right)\\ \mathbf{0}_{d_{1}-d_{1}^{(1)}-d_{1}^{(2)}}\end{array}\right)=\left(\begin{array} []{c}h_{1}^{(1)}\left(\mathbf{x}\right)\\ h_{2}^{(1)}\left(\mathbf{x}\right)\\ \mathbf{0}_{d_{1}-d_{1}^{\ast}-d_{1}^{\prime}}\end{array}\right)\,.\]

Assume that for some \(l\leq L^{\prime}\) we have

\[h_{\theta}^{(l-1)}\left(\mathbf{x}\right)=\left(\begin{array}{c}h_{1}^{(l-1 )}\left(\mathbf{x}\right)\\ h_{2}^{(l-1)}\left(\mathbf{x}\right)\\ \mathbf{0}_{d_{l}-d_{1}^{\ast}-d_{l}^{\prime}}\end{array}\right)\,.\]

Then,

\[h_{\theta}^{(l)}\left(\mathbf{x}\right)=\gamma_{l}\odot\sigma \left(\mathbf{W}_{l}h_{\theta}^{(l-1)}\left(\mathbf{x}\right)+\mathbf{b}_{l}\right)\] \[=\left(\begin{array}{c}\sigma\left(\mathbf{W}_{l}^{(1)}h_{1}^{( l-1)}\left(\mathbf{x}\right)+\mathbf{b}_{l}^{(1)}\right)\\ \sigma\left(\mathbf{W}_{l}^{(2)}h_{2}^{(l-1)}\left(\mathbf{x}\right)+\mathbf{b} _{l}^{(2)}\right)\\ \mathbf{0}_{d_{l}-d_{1}^{(1)}-d_{l}^{(2)}}\end{array}\right)=\left(\begin{array} []{c}h_{1}^{(l)}\left(\mathbf{x}\right)\\ h_{2}^{(l)}\left(\mathbf{x}\right)\\ \mathbf{0}_{d_{l}-d_{1}^{(1)}-d_{l}^{(2)}}\end{array}\right)\,.\]

It is left to show that the claim holds for \(l>L^{\prime}\). By the previous steps, \(h_{\theta}^{\left(L^{\prime}\right)}\left(\mathbf{x}\right)=\left(\begin{array} []{c}h_{1}\left(\mathbf{x}\right)\\ h_{2}\left(\mathbf{x}\right)\\ \mathbf{0}_{d_{L^{\prime}}-2}\end{array}\right)\). Under the assumptions on \(\mathbf{W}_{L^{\prime}+k},\mathbf{b}_{L^{\prime}+k}\) and \(\gamma_{L^{\prime}+k}\), \(k=1,2\) it holds that

\[h_{\theta}^{\left(L^{\prime}+2\right)}\left(\mathbf{x}\right)=\left(\begin{array} []{c}h_{1}\left(\mathbf{x}\right)\oplus h_{2}\left(\mathbf{x}\right)\\ \mathbf{0}_{d_{L^{\prime}}-1}\end{array}\right)\,.\]

Under the assumptions on layers \(l>L^{\prime}+2\),

\[h_{\theta}^{(l)}\left(\mathbf{x}\right)=\left(\begin{array}{c}h_{1}\left( \mathbf{x}\right)\oplus h_{2}\left(\mathbf{x}\right)\\ \mathbf{0}_{d_{l}-1}\end{array}\right)\,.\]

In particular, assuming that \(d_{L}=1\), \(h_{\theta}\left(\mathbf{x}\right)=h_{1}\left(\mathbf{x}\right)\oplus h_{2} \left(\mathbf{x}\right)\). 

**Corollary D.27**.: _Let \(h_{1},h_{2}\) be networks with depths \(L_{1},L_{2}\) and widths \(\underline{d}^{(1)},\underline{d}^{(2)}\). Then \(h_{1}\oplus h_{2}\) can be implemented with a network \(h\) of depth \(L=\max\left\{L_{1},L_{2}\right\}+2\) and widths \(\underline{d}\) such that_

\[w\left(\underline{d}\right)\leq w\left(\underline{d}^{(1)}\right)+w\left( \underline{d}^{(2)}\right)+2\underline{d}_{\max}^{(2)}\cdot n\left( \underline{d}^{(1)}\right)+O(1)\]

_and_

\[\underline{d}_{\max}\leq\underline{d}_{\max}^{(1)}+\underline{d}_{\max}^{(2)}\,.\]

Proof.: Following D.25 we assume shall assume that \(L_{1}=L_{2}=L\). We know from D.26 that there exists a network \(h\) with dimensions \(\underline{d}=\left(\underline{d}^{(1)}+\underline{d}^{(2)},2,1\right)\) such that \(h=h_{1}\oplus h_{2}\). Therefore

\[w\left(\underline{d}\right) =\left(d_{1}^{(1)}+d_{1}^{(2)}\right)d_{0}+\sum_{l=2}^{L}\left(d_ {l}^{(1)}+d_{l}^{(2)}\right)\left(d_{l-1}^{(1)}+d_{l-1}^{(2)}\right)+O(1)\] \[=d_{1}^{(1)}d_{0}+\sum_{l=2}^{L}d_{l}^{(1)}d_{l-1}^{(1)}+d_{1}^{(2 )}d_{0}+\sum_{l=2}^{L}d_{l}^{(2)}d_{l-1}^{(2)}+\sum_{l=2}^{L}\left[d_{l}^{(1)} d_{l-1}^{(2)}+d_{l}^{(2)}d_{l-1}^{(1)}\right]+O(1)\] \[=w\left(\underline{d}^{(1)}\right)+w\left(\underline{d}^{(2)} \right)+\sum_{l=2}^{L}\left[d_{l}^{(1)}d_{l-1}^{(2)}+d_{l}^{(2)}d_{l-1}^{(1)} \right]+O(1)\] \[\leq w\left(\underline{d}^{(1)}\right)+w\left(\underline{d}^{(2)} \right)+\sum_{l=2}^{L}\left[d_{l}^{(1)}\underline{d}_{\max}^{(2)}+\underline{d }_{\max}^{(2)}d_{l-1}^{(1)}\right]+O(1)\] \[=w\left(\underline{d}^{(1)}\right)+w\left(\underline{d}^{(2)} \right)+\underline{d}_{\max}^{(2)}\sum_{l=2}^{L}\left[d_{l}^{(1)}+d_{l-1}^{(1) }\right]+O(1)\] \[\leq w\left(\underline{d}^{(1)}\right)+w\left(\underline{d}^{(2)} \right)+2\underline{d}_{\max}^{(2)}\cdot n\left(\underline{d}^{(1)}\right)+O( 1)\,.\]

In addition, \(\underline{d}_{\max}\leq\underline{d}_{\max}^{(1)}+\underline{d}_{\max}^{(2)}\) and \(n\left(\underline{d}\right)=n\left(\underline{d}^{(1)}\right)+n\left( \underline{d}^{(2)}\right)\). 

**Recall Corollary 3.4**.: For any teacher \(h^{\star}\) of depth \(L^{\star}\) and dimensions \(d^{\star}\) and any consistent training set \(S\) generated from it, there exists an interpolating network \(h\) (_i.e.,_\(\mathcal{L}_{S}\left(h\right)=0\)) of depth \(L=\max\left\{L^{\star},14\right\}+2\) and dimensions \(\underline{d}\), such that the number of edges is

\[w\left(\underline{d}\right) \leq w\left(d^{\star}\right)+N\cdot H\left(\mathcal{L}_{S}\left(h^{ \star}\right)\right)+2n\left(d^{\star}\right)N^{3/4}H\left(\mathcal{L}_{S} \left(h^{\star}\right)\right)^{3/4}\mathrm{polylog}N\] \[+O\left(d_{0}\left(d_{0}+n\left(d^{\star}\right)\right)\cdot \log N\right)\]

and the dimensions are

\[\underline{d}_{\max}\leq\underline{d}_{\max}^{\star}+N^{3/4}\cdot H\left( \mathcal{L}_{S}\left(h^{\star}\right)\right)^{3/4}\cdot\mathrm{polylog}\left(N \right)+O\left(d_{0}\cdot\log\left(N\right)\right)\,.\]

Proof.: We use Corollary D.27 with \(h_{1}=h^{\star}\) and \(h_{2}=\tilde{h}_{S}\), the noise memorizing network from Theorem 3.1, to get

\[w\left(\underline{d}\right) \leq w\left(\underline{d}^{\star}\right)+w\left(\tilde{\underline{d }}_{S}\right)+2\tilde{d}_{S,\max}\cdot n\left(\underline{d}^{\star}\right)+O(1)\] \[\leq w\left(\underline{d}^{\star}\right)+\log\binom{N}{N_{1}}+ \left(\log\binom{N}{N_{1}}\right)^{3/4}\cdot\mathrm{polylog}\,N+O(d_{0}^{2} \cdot\log N)\] \[+2n\left(\underline{d}^{\star}\right)\left(\left(\log\binom{N}{N _{1}}\right)^{3/4}\cdot\mathrm{polylog}\,N+O(d_{0}\cdot\log N)\right)+O(1)\,.\]

Using Stirling's approximation

\[\log\binom{N}{N_{1}}=N\cdot H\left(\frac{N_{1}}{N}\right)+O\left(\log\left(N \right)\right)=N\cdot H\left(\mathcal{L}_{S}\left(h^{\star}\right)\right)+O \left(\log\left(N\right)\right)\,.\]Therefore

\[w\left(\underline{d}\right) \leq w\left(\underline{d}^{\star}\right)+N\cdot H\left(\mathcal{L}_{S} \left(h^{\star}\right)\right)+O\left(\log\left(N\right)\right)+N^{3/4}\cdot H \left(\mathcal{L}_{S}\left(h^{\star}\right)\right)^{3/4}\cdot\mathrm{polylog}N\] \[\quad+O\left(d_{0}^{2}\cdot\log N\right)+2n\left(\underline{d}^ {\star}\right)\left(N^{3/4}\cdot\mathrm{polylog}\,N+O\left(d_{0}\cdot\log N \right)\right)\] \[=w\left(\underline{d}^{\star}\right)+N\cdot H\left(\mathcal{L}_{S }\left(h^{\star}\right)\right)+2n\left(\underline{d}^{\star}\right)N^{3/4}H \left(\mathcal{L}_{S}\left(h^{\star}\right)\right)^{3/4}\mathrm{polylog}N\] \[\quad+O\left(d_{0}\left(d_{0}+n\left(\underline{d}^{\star}\right) \right)\cdot\log N\right)\,.\]

The bound of \(\underline{d}_{\max}\) is derived similarly.

The label-flip-memorization network's dependence on the dimension

In Theorem 3.1, the wire bound has an \(O(d_{0}^{2}\cdot\log N)\) term. (Recall that \(d_{0}\) is the input dimension and \(N\) is the domain size.) In this section, we discuss (a) approaches for improving this term and (b) a lower bound showing that it cannot be significantly improved.

### Improving the \(O(d_{0}^{2}\cdot\log N)\) Term

The \(O(d_{0}^{2}\cdot\log N)\) term in Theorem 3.1 can be improved by using the following fact.

**Lemma E.1** (Using a sign matrix for preprocessing).: _Let \(d_{0}\in\mathbb{N}\), let \(\hat{\mathcal{X}}\subseteq\{0,1\}^{d_{0}}\), and let \(N=|\hat{\mathcal{X}}|\). There exists \(d_{1}=O(\sqrt{d_{0}}\cdot\log N)\) and there exists a matrix \(\mathbf{W}\in\{\pm 1\}^{d_{1}\times d_{0}}\) such that the function \(C_{0}\colon\{0,1\}^{d_{0}}\to\{0,1\}^{d_{1}}\) defined by \(C_{0}(\mathbf{x})=\mathbb{I}\left\{\mathbf{W}\mathbf{x}>0\right\}\) is injective on \(\hat{\mathcal{X}}\)._

Proof.: Pick \(\mathbf{W}\in\{\pm 1\}^{d_{1}\times d_{0}}\) uniformly at random. We will show that there is a nonzero chance that \(C_{0}\) is injective on \(\hat{\mathcal{X}}\).

Let \(\mathbf{x},\mathbf{x}^{\prime}\) be any two distinct points in \(\hat{\mathcal{X}}\). Consider a single row \(\mathbf{W}_{i}\) of \(\mathbf{W}\). Let \(E\) be the good event that

\[\mathbf{W}_{i}\cdot(\mathbf{x}\odot\mathbf{x}^{\prime})\in\{0,1\}.\]

Then \(\Pr[E]\geq\Omega(1/\sqrt{d_{0}})\), because we are taking a simple one-dimensional random walk of length at most \(d_{0}\). Conditioned on \(E\), there is an \(\Omega(1)\) chance that \(\mathbb{I}\left\{\mathbf{W}_{i}\cdot\mathbf{x}>0\right\}\neq\mathbb{I}\left\{ \mathbf{W}_{i}\cdot\mathbf{x}^{\prime}>0\right\}\), because we are taking two independent one-dimensional random walks starting from either \(0\) or \(1\), at least one of which has nonzero length, and asking whether they land on the same side of \(1/2\). Therefore, unconditionally, \(\Pr[\mathbb{I}\left\{\mathbf{W}_{i}\cdot\mathbf{x}>0\right\}\neq\mathbb{I} \left\{\mathbf{W}_{i}\cdot\mathbf{x}^{\prime}>0\right\}]\geq\Omega(1/\sqrt{d _{0}})\). Consequently, by independence,

\[\Pr[C_{0}(\mathbf{x})=C_{0}(\mathbf{x}^{\prime})]\leq(1-\Omega(1/\sqrt{d_{0}} ))^{d_{1}}<1/N^{2},\]

provided we choose a suitable value \(d_{1}=O(\sqrt{d_{0}}\cdot\log N)\). By the union bound over all pairs \(\mathbf{x},\mathbf{x}^{\prime}\), it follows that there is a nonzero chance that \(C_{0}\) is injective on \(\hat{\mathcal{X}}\). 

There are two approaches to using Lemma E.1 for the sake of improving the \(O(d_{0}^{2}\cdot\log N)\) term in Theorem 3.1.

* One approach would be to start with a trivial layer that copies the input \(\mathbf{x}\in\{0,1\}^{d_{0}}\) as well as computing all the negations of the bits of \(\mathbf{x}\); then we have a layer that applies the function \(C_{0}\) from Lemma E.1 (using negated variables to implement \(-1\) weights); and then we continue with the network of Theorem 3.1. The net effect is that the depth has increased by two (so the network now has depth 16 instead of 14), and in the weights bound, the \(O(d_{0}^{2}\cdot\log N)\) term has been slightly improved to \(O(d_{0}^{2}+d_{0}^{3/2}\cdot\log N+d_{0}\cdot\log^{3}N)\).
* A second approach would be to change the model. If we permit ternary edge weights (i.e., weights in the set \(\{-1,0,1\}\)), then the function \(C_{0}\) of Lemma E.1 can be implemented as the very first layer of our network, and then we can continue with the network of Theorem 3.1. Note that we need ternary edge weights only in the first layer; the edge weights in all subsequent layers are binary. The benefit of this approach is in the weights bound, the \(O(d_{0}^{2}\cdot\log N)\) term of Theorem 3.1 would be improved to \(O(d_{0}^{3/2}\cdot\log N+d_{0}\cdot\log^{3}N)\).

### A \(d_{0}^{2}\) Lower Bound on the Number of Weights

We now show that the \(O(d_{0}^{2}\cdot\log N)\) term in Theorem 3.1 cannot be improved to something better than \(d_{0}^{2}\), if we insist on using the "binary threshold network" model. The argument is elementary.

**Proposition E.2** (\(d_{0}^{2}\) wire lower bound).: _For every \(d_{0}\in\mathbb{N}\), there exists a partial Boolean function \(f\colon\{0,1\}^{d_{0}}\to\{0,1,\star\}\), defined on a domain \(\hat{\mathcal{X}}\) of size \(d_{0}+1\), such that for every binary threshold network \(\tilde{h}\), if \(\tilde{h}\) agrees with \(f\) everywhere in its domain and \(\underline{d}\) is the widths of \(\tilde{h}\), then \(w\left(\underline{d}\right)\geq d_{0}^{2}\)._

Proof.: For each \(i\in\{0,1,\ldots,d_{0}\}\), let \(\mathbf{x}^{(i)}\) be the vector consisting of \(i\) zeroes followed by \(d_{0}-i\) ones. Let \(\hat{\mathcal{X}}=\{\mathbf{x}^{(i)}:0\leq i\leq d_{0}\}\), and let

\[f(\mathbf{x})=\begin{cases}\mathsf{PARITY}(\mathbf{x})&\text{if }\mathbf{x}\in \hat{\mathcal{X}}\\ \star&\text{otherwise.}\end{cases}\]

For the analysis, let \(\tilde{h}\) be a fully connected binary threshold network that agrees with \(f\) on all points in \(\hat{\mathcal{X}}\). Consider the layer immediately following the input layer. Each node \(g\) in this layer computes either a monotone Boolean function or an anti-monotone Boolean function of the input variables. Therefore, there is at most one value \(i\in\{1,2,\ldots,d_{0}\}\) such that \(g(\mathbf{x}^{(i-1)})\neq g(\mathbf{x}^{(i)})\). On the other hand, for every \(i\in\{1,2,\ldots,d_{0}\}\), we have \(\tilde{h}(\mathbf{x}^{(i-1)})\neq\tilde{h}(\mathbf{x}^{(i)})\), and hence there must be at least one node \(g\) in this layer such that \(g(\mathbf{x}^{(i-1)})\neq g(\mathbf{x}^{(i)})\). Therefore, there are at least \(d_{0}\) many nodes \(g\).

Thus, the first two layers of \(\tilde{h}\) both have widths of at least \(d_{0}\), demonstrating that \(\tilde{h}\) has at least \(d_{0}^{2}\) many weights.

Generalization results (Proofs for Section 4)

Denote by \(\mathcal{H}_{\underline{d}}^{\mathrm{BTN}}\) the set of functions representable as binary threshold networks with dimensions \(\underline{d}\) (given a fixed depth \(L\)). We start by bounding the cardinality \(\left|\mathcal{H}_{\underline{d}}^{\mathrm{BTN}}\right|\) in terms of the number of edges \(w\left(\underline{d}\right)\).

**Lemma F.1**.: _Let \(\underline{d}\) be the dimensions of a binary threshold network with \(w\triangleq w\left(\underline{d}\right)\) edges. Then there are \(2^{w+O\left(\sqrt{w}\log\left(w\right)\right)}\) functions representable as networks with dimensions \(\underline{d}\)._

Proof.: We bound the number of function representable as binary threshold networks with dimensions \(\underline{d}\) having \(w\) edges by suggesting a way to encode them, and then bounding the number of bits in the encoding. First, permute each layer so the neurons are sorted by the bias and neuron scaling terms \(\left(b_{li},\gamma_{li}\right)\). As NNs are invariant to permutations, this does not change the function. Now, at each layer we encode the bias term based on one of two encodings.

* If \(d_{l}<d_{l-1}\), then list each of the bias terms as a number with \(O\left(\log\left(d_{l-1}\right)\right)\) bits plus 2 bits for the scaling term for a total of \(O\left(d_{l}\left(\log\left(d_{l-1}\right)+2\right)\right)\leq O\left(\sqrt{d_ {l}d_{l-1}}\log\left(d_{l-1}\right)\right)\), where the inequality is due to \(d_{l}<d_{l-1}\).
* If \(d_{l}\geq d_{l-1}\), then we encode the bias and scaling terms by listing the number of times each pair \(\left(b_{li},\gamma_{li}\right)\in\left\{-d_{l-1},\ldots,d_{l-1}-1\right\} \times\left\{-1,0,1\right\}\) appears in \(\left(\mathbf{b}_{l},\bm{\gamma}_{l}\right)\) (recall that the neurons are ordered according to these pairs). Each pair can appear at most \(d_{l}\) times and so requires \(O\left(\log\left(d_{l}\right)\right)\) bits to encode for a total of \(O\left(6d_{l-1}\log\left(d_{l}\right)\right)=O\left(d_{l-1}\log\left(d_{l} \right)\right)\leq O\left(\sqrt{d_{l}d_{l-1}}\log\left(d_{l}d_{l-1}\right)\right)\).

By encoding each weight with a single bit, this means that for all layers, we can encode the weights, biases and scaling terms using \(d_{l}d_{l-1}+O\left(\sqrt{d_{l}d_{l-1}}\log\left(d_{l}d_{l-1}\right)\right)\) bits for a total of

\[\sum_{l=1}^{L} d_{l}d_{l-1}+O\left(\sqrt{d_{l}d_{l-1}}\log\left(d_{l}d_{l-1} \right)\right)=w+O\left(\sum_{l=1}^{L}\sqrt{d_{l}d_{l-1}}\log\left(d_{l}d_{l-1 }\right)\right)\] \[\leq w+O\left(\sum_{l=1}^{L}\sqrt{d_{l}d_{l-1}}\log\left(\sum_{l =1}^{L}d_{l}d_{l-1}\right)\right)\] \[\leq w+O\left(\sum_{l=1}^{L}\sqrt{d_{l}d_{l-1}}\log\left(w\right) \right)=w+O\left(\log\left(w\right)\cdot L\sum_{l=1}^{L}\frac{1}{L}\sqrt{d_{l }d_{l-1}}\right)\] \[\left[\text{Jensen}\right] \leq w+O\left(\log\left(w\right)\cdot L\sqrt{\sum_{l=1}^{L}\frac {1}{L}d_{l}d_{l-1}}\right)=w+O\left(\log\left(w\right)\cdot\sqrt{L}\sqrt{\sum _{l=1}^{L}d_{l}d_{l-1}}\right)\] \[=w+O\left(\log\left(w\right)\cdot\sqrt{L}\sqrt{w}\right)\] \[=w+O\left(\log\left(w\right)\cdot\sqrt{w}\right)\,.\]

**Corollary F.2**.: _Assuming that the depth \(L\) is fixed and known, a binary threshold network of depth \(L\) with unknown number of weights \(w\), can be encoded with \(w+O\left(\sqrt{w}\log\left(w\right)\right)\) bits._

Proof.: After specifying the architecture \(\underline{d}\), from Lemma F.1 we require \(w+O\left(\sqrt{w}\log\left(w\right)\right)\) bits. Therefore it remains to bound the length of the encoding of \(\underline{d}\). We first use \(O\left(\log\left(w\right)\right)\) bits to encode the number of weights, then, since \(\underline{d}\in\left[w\right]^{L}\), we only need \(O\left(\log\left(w^{L}\right)\right)=O\left(\log\left(w\right)\right)\) additional bits for a total of \(w+O\left(\sqrt{w}\log\left(w\right)\right)+O\left(\log\left(w\right)\right)=w +O\left(\sqrt{w}\log\left(w\right)\right)\).

### Derivation of the min-size generalization bounds (Proofs for Section 4.1)

Throughout this subsection, we use \(A\left(S\right)\) to denote the min-size interpolating NN of depth \(L\), \(A_{L}\left(S\right)\).

**Lemma F.3**.: _Let \(L\geq 16\) be fixed. Then_

\[I\left(S;A\left(S\right)\right)\leq w\left(\underline{d}^{\star}\right)+N \cdot H\left(\varepsilon^{\star}\right)+O\left(\delta\left(N,d_{0}, \underline{d}^{\star}\right)\right)\]

_where_

\[\delta\left(N,d_{0},\underline{d}^{\star}\right) =n\left(\underline{d}^{\star}\right)\cdot N^{3/4}H\left( \varepsilon^{\star}\right)^{3/4}\cdot\operatorname{polylog}\left(N+n\left( \underline{d}^{\star}\right)+d_{0}\right)\] \[\quad+d_{0}^{2}\cdot\log N+d_{0}n\left(\underline{d}^{\star} \right)\log\left(n\left(\underline{d}^{\star}\right)+N+d_{0}\right)^{3/2}\,.\]

Proof.: Using Shannon's source coding theorem:

\[I\left(S;A\left(S\right)\right)\leq H\left(A\left(S\right)\right)\leq\mathbb{ E}\left|A\left(S\right)\right|\,,\]

where \(\left|A\left(S\right)\right|\) denotes the number of bits in the encoding of \(A\left(S\right)\). Following Corollary 3.4, for a consistent \(S\), \(A\left(S\right)\) is a network with fixed depth and at most

\[w \triangleq w\left(\underline{d}^{\star}\right)+N\cdot H\left( \mathcal{L}_{S}\left(h^{\star}\right)\right)+2n\left(\underline{d}^{\star} \right)N^{3/4}H\left(\mathcal{L}_{S}\left(h^{\star}\right)\right)^{3/4} \operatorname{polylog}N\] \[\quad+O\left(d_{0}\left(d_{0}+n\left(\underline{d}^{\star} \right)\right)\cdot\log N\right)\]

weights and therefore, using the result from Corollary F.2 and \(\sqrt{w\left(\underline{d}^{\star}\right)}\leq d_{0}+n\left(\underline{d}^{ \star}\right)\),

\[\left|A\left(S\right)\right|\leq w+O\left(\sqrt{w}\log\left(w \right)\right)\] \[=w\left(\underline{d}^{\star}\right)+N\cdot H\left(\mathcal{L}_{S }\left(h^{\star}\right)\right)+O\Big{(}n\left(\underline{d}^{\star}\right) \cdot N^{3/4}H\left(\mathcal{L}_{S}\left(h^{\star}\right)\right)^{3/4}\cdot \operatorname{polylog}\left(N+n(\underline{d}^{\star})+d_{0}\right)\Big{)}\] \[\quad+O\Big{(}d_{0}^{2}\cdot\log N+d_{0}n\left(\underline{d}^{ \star}\right)\log\left(n\left(\underline{d}^{\star}\right)+N+d_{0}\right)^{3/2 }\Big{)}\] \[=w\left(\underline{d}^{\star}\right)+N\cdot H\left(\mathcal{L}_{S }\left(h^{\star}\right)\right)+O\left(\tilde{\delta}\left(N,d_{0},\underline{ d}^{\star}\right)\right)\]

where we grouped all lower order terms in \(\tilde{\delta}\). In case \(S\) is inconsistent, \(A\left(S\right)=\star\) so \(\left|A\left(S\right)\right|=O\left(1\right)\). Taking the expected value and using Jensen's inequality gives

\[\mathbb{E}\left|A\left(S\right)\right| =\mathbb{E}\left[\left|A\left(S\right)\right|\cdot\mathbb{I} \left\{\operatorname{inconsistent}S\right\}\right]+\mathbb{E}\left[\left|A \left(S\right)\right|\cdot\mathbb{I}\left\{\operatorname{consistent}S\right\}\right]\] \[\leq O(1)+\mathbb{E}\left[\underbrace{\mathbb{I}\left\{ \operatorname{consistent}S\right\}}_{\leq 1}\underbrace{\left(w\left(\underline{d}^{\star} \right)+N\cdot H\left(\mathcal{L}_{S}\left(h^{\star}\right)\right)+O\left( \tilde{\delta}\left(N,d_{0},\underline{d}^{\star}\right)\right)\right)}_{\geq 0}\right]\] \[\leq O(1)+\mathbb{E}\left[w\left(\underline{d}^{\star}\right)+N \cdot H\left(\mathcal{L}_{S}\left(h^{\star}\right)\right)+O\left(\tilde{ \delta}\left(N,d_{0},\underline{d}^{\star}\right)\right)\right]\] \[\left[\text{Jensen}\right] \leq w\left(\underline{d}^{\star}\right)+N\cdot H\left(\mathbb{E} \left[\mathcal{L}_{S}\left(h^{\star}\right)\right]\right)+O\left(\delta\left(N,d_{0},\underline{d}^{\star}\right)\right)\] \[=w\left(\underline{d}^{\star}\right)+N\cdot H\left(\varepsilon^{ \star}\right)+O\left(\delta\left(N,d_{0},\underline{d}^{\star}\right)\right)\,.\]With this result, we are ready to derive the generalization results.

**Recall Theorem 4.2**.: Consider a distribution \(\mathcal{D}\) induced by a noisy teacher model of depth \(L^{\star}\) and widths \(\underline{d}^{\star}\) (Assumption 2.4) with a noise level of \(\varepsilon^{\star}<1/2\). Let \(S\sim\mathcal{D}^{N}\) be a training set such that \(N=o(\sqrt{1/\mathcal{D}_{\max}})\). Then, for any fixed depth \(L\geq\max\left\{L^{\star},14\right\}+2\), the generalization error of the min-size depth-\(L\) NN interpolator satisfies the following.

* **Under arbitrary label noise**, \[\mathbb{E}_{S}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S\right) \right)\right]\leq 1-2^{-H(\varepsilon^{\star})/\mathbb{P}_{S}\left(\text{ consistent }S\right)}+\mathbb{P}\left(\text{inconsistent }S\right)+O\left(C_{\min}\left(N,d_{0}, \underline{d}^{\star}\right)\right)\,.\]
* **Under independent label noise**, \[\left|\mathbb{E}_{S}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S \right)\right)\right]-2\varepsilon^{\star}\left(1-\varepsilon^{\star}\right)\right|\] \[\leq\left(1-2\varepsilon^{\star}\right)\sqrt{\frac{O\left(C_{ \min}\left(N,d_{0},\underline{d}^{\star}\right)+\mathbb{P}\left(\text{inconsistent }S\right)\right)}{\mathbb{P}\left(\text{ consistent }S\right)}}+\frac{\left(N-1\right)\mathcal{D}_{\max}}{3}+\mathbb{P}\left( \text{inconsistent}\,S\right)\,,\]

where

\[C_{\min}\left(N,d_{0},\underline{d}^{\star}\right)=\frac{w\left( \underline{d}^{\star}\right)+\delta\left(N,d_{0},\underline{d}^{\star}\right) }{N}\]

with \(\delta\left(N,d_{0},\underline{d}^{\star}\right)\) as defined in Lemma F.3.

_Remark F.4_.: The bound shown in Section 4.1 is found by bounding \(\mathbb{P}(\text{inconsistent }S)\leq\frac{1}{2}N^{2}\mathcal{D}_{\max}\) as in Lemma B.1. Then using the Taylor approximation with small \(N^{2}\mathcal{D}_{\max}\)

\[1-2^{-\frac{H\left(\varepsilon^{\star}\right)}{\mathbb{P}\left( \text{consistent }S\right)}} \leq 1-2^{-\frac{H\left(\varepsilon^{\star}\right)}{1-\frac{1}{2} \frac{N}{N^{2}}\mathcal{D}_{\max}}}\] \[=1-2^{-H\left(\varepsilon^{\star}\right)\left(1+O\left(N^{2} \mathcal{D}_{\max}\right)\right)}\] \[=1-2^{-H\left(\varepsilon^{\star}\right)}+O\left(N^{2}\mathcal{D} _{\max}\right)\,.\]

Lemma B.1 is used similarly to bound the error in the independent noise case. Assuming that \(N=\omega\left(n\left(\underline{d}^{\star}\right)^{4}H\left(\varepsilon^{ \star}\right)^{3}\operatorname{polylog}\left(n\left(\underline{d}^{\star} \right)\right)+d_{0}^{2}\log d_{0}\right)\) when \(\varepsilon^{\star}>0\) we can deduce that \(N=\omega\left(w\left(\underline{d}^{\star}\right)\right)\) as well since

\[w\left(\underline{d}^{\star}\right)\leq\left(n\left(\underline{d}^{\star} \right)+d_{0}\right)^{2}\leq 4\left(\max\left\{n\left(\underline{d}^{\star} \right),d_{0}\right\}\right)^{2}\,.\]

Together with \(N=o\left(\sqrt{1/\mathcal{D}_{\max}}\right)\) we get the desired form of the bounds. Finally, note that when \(\varepsilon^{\star}=0\), the convergence rate of \(\tilde{O}\left(1/N\right)\) instead of \(\tilde{O}\left(1/\sqrt[4]{N}\right)\), where \(\tilde{O}\) hides logarithmic terms arising as artifacts of our analysis, and dependence on other parameters such as the input dimension \(d_{0}\).

Proof.: Starting with the bound in the arbitrary noise setting, we combine C.2 with F.3

\[-\log \left(1-\mathbb{E}_{S}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S \right)\right)\mid\text{consistent }S\right]\right)\leq\frac{I\left(S;A\left(S\right)\right)}{N\cdot\mathbb{P}_{S} \left(\text{consistent }S\right)}\] \[\leq\frac{w\left(\underline{d}^{\star}\right)+N\cdot H\left( \varepsilon^{\star}\right)+O\left(\delta\left(N,d_{0},\underline{d}^{\star} \right)\right)}{N\cdot\mathbb{P}_{S}\left(\text{consistent }S\right)}\] \[=\frac{1}{\mathbb{P}_{S}\left(\text{consistent }S\right)}\cdot\left(H\left( \varepsilon^{\star}\right)+O\left(C_{\min}\left(N,d_{0},\underline{d}^{\star} \right)\right)\right)\,.\]

Rearranging the above inequality and recalling Remark C.1, we have,

\[\mathbb{E}_{S}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S\right) \right)\mid\text{consistent }S\right]\leq 1-2^{-\frac{H\left(\varepsilon^{\star}\right)}{ \mathbb{P}_{S}\left(\text{consistent }S\right)}-O\left(C_{\min}\left(N,d_{0}, \underline{d}^{\star}\right)\right)}\,.\]Then, using Lemma A.6, we get,

\[\mathbb{E}_{S}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S\right)\right)\mid \text{consistent }S\right]\leq 1-2^{-\frac{H\left(\varepsilon^{\star}\right)}{\mathbb{E}_{S}\left( \text{consistent }S\right)}}+O\left(C_{\min}\left(N,d_{0},\underline{d}^{\star}\right) \right)\,.\]

The bound is derived using the following observation. Since for a RV \(X\) in \([0,1]\) and a binary RV \(Y\) we have

\[\mathbb{E}[X]=\mathbb{E}[X\mid Y]\underbrace{\mathbb{P}(Y)}_{\leq 1}+\underbrace{ \mathbb{E}[X\mid\neg Y]}_{\leq 1}\mathbb{P}(\neg Y)\leq\mathbb{E}[X\mid Y]+ \mathbb{P}[\neg Y]\,,\]

we conclude the proof as

\[\mathbb{E}_{S}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S\right)\right) \right]\leq\mathbb{E}_{S}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S\right) \right)\mid\text{consistent }S\right]+\mathbb{P}\left(\text{inconsistent }S\right)\,.\]

For the independent noise setting, we combine Lemma C.3 and Lemma F.3 to get

\[\left|\mathbb{E}_{S}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S \right)\right)\mid\text{consistent }S\right]-2\varepsilon^{\star}\left(1-\varepsilon^{\star}\right)\right|\] \[\leq\left(1-2\varepsilon^{\star}\right)O\left(\sqrt{C\left(N \right)}\right)+\frac{\left(N-1\right)\mathcal{D}_{\max}}{3}\,,\]

where

\[C\left(N\right)=\frac{I\left(S;A\left(S\right)\right)-N\cdot \left(H\left(\varepsilon^{\star}\right)-\mathbb{P}\left(\text{inconsistent }S\right)\right)}{N\left(1-\mathbb{P}\left(\text{inconsistent }S\right)\right)}\] \[\leq\frac{w\left(\underline{d}^{\star}\right)+N\cdot H\left( \varepsilon^{\star}\right)+O\left(\delta\left(N,d_{0},\underline{d}^{\star} \right)\right)-N\cdot\left(H\left(\varepsilon^{\star}\right)-\mathbb{P}\left( \text{inconsistent }S\right)\right)}{N\left(1-\mathbb{P}\left(\text{inconsistent }S\right)\right)}\] \[=\frac{O\left(\frac{w\left(\underline{d}^{\star}\right)+\delta \left(N,d_{0},\underline{d}^{\star}\right)}{N}\right)+\mathbb{P}\left(\text{ inconsistent }S\right)}{\mathbb{P}\left(\text{consistent }S\right)}\] \[=\frac{O\left(C_{\min}\left(N,d_{0},\underline{d}^{\star}\right) \right)+\mathbb{P}\left(\text{inconsistent }S\right)}{\mathbb{P}\left(\text{consistent }S\right)}\]

Finally, using the inequality from Lemma C.4, we have,

\[\left|\mathbb{E}_{S,A\left(S\right)}\left[\mathcal{L}_{\mathcal{D }}\left(A\left(S\right)\right)\right]-2\varepsilon^{\star}\left(1-\varepsilon ^{\star}\right)\right|\] \[\leq\left|\mathbb{E}_{S,A\left(S\right)}\left[\mathcal{L}_{ \mathcal{D}}\left(A\left(S\right)\right)\mid\text{consistent }S\right]-2\varepsilon^{\star}\left(1-\varepsilon^{\star}\right)\right|+ \mathbb{P}\left(\text{inconsistent }S\right)\]

### Derivation of the posterior sampling generalization bounds (Section 4.2)

**Lemma F.5**.: _For the posterior sampling algorithm_

\[I\left(S;A\left(S\right)\right)\leq\mathbb{E}_{S}\left[\log\left(\frac{1}{p_{S}} \right)\middle|\mathrm{consistent}\,S\right]\mathbb{P}_{S}\left(\mathrm{consistent}\,S \right)+\frac{2}{e\ln 2}\,.\]

Proof.: Recall the definition of the marginal distribution of the algorithm's output (a hypothesis \(h\)) is

\[d\nu\left(h\right)=\sum_{s}dp\left(s,h\right)\,,\]

where \(s\) are all possible realizations of a (training) sample of size \(N\).

For \(h=\star\), we have \(d\nu\left(\star\right)=\mathbb{P}_{S}\left(\mathrm{inconsistent}\,S\right)\).

For \(h\neq\star\), since \(\mathcal{L}_{s}\left(h\right)=0\) implies that \(s\) is consistent, we have

\[d\nu\left(h\right) \triangleq\sum_{s}dp\left(s,h\right)=\sum_{s}\frac{\mathbb{I} \left\{\mathcal{L}_{s}\left(h\right)=0\right\}}{p_{s}}d\mathcal{P}\left(h \right)d\mathcal{D}^{N}\left(s\right)\] \[=d\mathcal{P}\left(h\right)\sum_{s:p_{s}>0}\frac{\mathbb{I} \left\{\mathcal{L}_{s}\left(h\right)=0\right\}}{p_{s}}d\mathcal{D}^{N}\left(s\right)\] \[=d\mathcal{P}\left(h\right)\mathbb{E}_{S\sim\mathcal{D}^{N}} \left[\frac{\mathbb{I}\left\{p_{S}>0\right\}}{p_{S}}\mathbb{I}\left\{ \mathcal{L}_{S}\left(h\right)=0\right\}\right]\,.\]

where, for ease of notation, we use the convention that \(\frac{\mathbb{I}\left\{p_{s}>0\right\}}{p_{s}}=0\) when \(p_{s}=0\). Denoting

\[\pi\left(h\right)\triangleq\mathbb{E}_{S\sim\mathcal{D}^{N}}\left[\frac{ \mathbb{I}\left\{p_{S}>0\right\}}{p_{S}}\mathbb{I}\left\{\mathcal{L}_{S}\left(h \right)=0\right\}\right]\,,\]

we get

\[d\nu\left(h\right)=d\mathcal{P}\left(h\right)\pi\left(h\right)\,.\]

Notice that if there exists some \(s\in\mathrm{supp}\left(\mathcal{D}^{N}\right)\) such that \(\mathcal{L}_{s}\left(h\right)=0\) then \(\pi\left(h\right)>0\). Using the definition of the mutual information:

\[I\left(S;A\left(S\right)\right)=\sum_{s}\sum_{h\in\mathcal{H} \left\{\star\right\}}dp\left(s,h\right)\log\left(\frac{dp\left(s,h\right)}{d \nu\left(h\right)d\mathcal{D}\left(s\right)}\right)\] \[=\sum_{s:p_{s}=0}dp\left(s,\star\right)\log\left(\frac{dp\left(s, \star\right)}{d\nu\left(\star\right)d\mathcal{D}\left(s\right)}\right)+\sum_{s: p_{s}>0}\sum_{h\in\mathcal{H}}dp\left(s,h\right)\log\left(\frac{dp\left(s,h \right)}{d\nu\left(h\right)d\mathcal{D}\left(s\right)}\right)\] \[=\sum_{s:p_{s}=0}d\mathcal{D}\left(s\right)\log\left(\frac{d \mathcal{D}\left(s\right)}{\mathbb{P}_{S}\left(\mathrm{inconsistent}\,S \right)d\mathcal{D}\left(s\right)}\right)+\] \[\sum_{s:p_{s}>0}\sum_{h:\mathcal{L}_{s}\left(h\right)=0}\frac{1}{p _{s}}d\mathcal{P}\left(h\right)d\mathcal{D}\left(s\right)\log\left(\frac{ \frac{1}{p_{s}}d\mathcal{P}\left(h\right)d\mathcal{D}\left(s\right)}{d \mathcal{P}\left(h\right)\pi\left(h\right)d\mathcal{D}\left(s\right)}\right)\] \[=\sum_{s:p_{s}=0}d\mathcal{D}\left(s\right)\log\left(\frac{1}{ \mathbb{P}_{S}\left(\mathrm{inconsistent}\,S\right)}\right)+\sum_{s:p_{s}>0} \sum_{h:\mathcal{L}_{s}\left(h\right)=0}\frac{1}{p_{s}}d\mathcal{P}\left(h \right)d\mathcal{D}\left(s\right)\log\left(\frac{1}{p_{s}\pi\left(h\right)} \right).\]

Simplifying each term separately, the first sum immediately simplifies to

\[-\mathbb{P}_{S}\left(\mathrm{inconsistent}\,S\right)\log\left(\mathbb{P}_{S} \left(\mathrm{inconsistent}\,S\right)\right)\leq\frac{1}{e\ln 2}\,,\]and

\[\sum_{s;p_{s}>0}\sum_{h:\mathcal{L}_{s}(h)=0}\frac{1}{p_{s}}d\mathcal{ P}\left(h\right)d\mathcal{D}\left(s\right)\log\left(\frac{1}{p_{s}\pi\left(h \right)}\right)\] \[=-\sum_{s:p_{s}>0}\sum_{h:\mathcal{L}_{s}(h)=0}\frac{1}{p_{s}}d \mathcal{P}\left(h\right)d\mathcal{D}\left(s\right)\log\left(p_{s}\right)-\sum_ {s:p_{s}>0}\sum_{h:\mathcal{L}_{s}(h)=0}\frac{1}{p_{s}}d\mathcal{P}\left(h \right)d\mathcal{D}\left(s\right)\log\left(\pi\left(h\right)\right)\] \[=-\sum_{s:p_{s}>0}\frac{1}{p_{s}}\log\left(p_{s}\right)d\mathcal{ D}\left(s\right)\underbrace{\sum_{h:\mathcal{L}_{s}(h)=0}d\mathcal{P}\left(h \right)}_{=p_{s}}\] \[\quad-\sum_{s:p_{s}>0}\sum_{h:\pi\left(h\right)>0}\frac{\mathbb{ I}\left\{\mathcal{L}_{s}\left(h\right)=0\right\}}{p_{s}}d\mathcal{P}\left(h \right)d\mathcal{D}\left(s\right)\log\left(\pi\left(h\right)\right)\] \[=-\sum_{s:p_{s}>0}\frac{1}{p_{s}}\log\left(p_{s}\right)d\mathcal{ D}\left(s\right)p_{s}-\sum_{h:\pi\left(h\right)>0}\log\left(\pi\left(h \right)\right)d\mathcal{P}\left(h\right)\underbrace{\sum_{s:p_{s}>0}\frac{ \mathbb{I}\left\{\mathcal{L}_{s}\left(h\right)=0\right\}}{p_{s}}d\mathcal{D} \left(s\right)}_{=\pi\left(h\right)}\] \[=-\sum_{s:p_{s}>0}\log\left(p_{s}\right)d\mathcal{D}\left(s \right)-\sum_{h:\pi\left(h\right)>0}\pi\left(h\right)\log\left(\pi\left(h \right)\right)d\mathcal{P}\left(h\right)\] \[=-\mathbb{E}_{S}\left[\log\left(p_{S}\right)\mathbb{I}\left\{p_ {S}>0\right\}\right]-\mathbb{E}_{h\sim\mathcal{P}}\left[\mathbb{I}\left\{\pi \left(h\right)>0\right\}\pi\left(h\right)\log\left(\pi\left(h\right)\right) \right]\] \[=\mathbb{E}_{S}\left[\log\left(\frac{1}{p_{S}}\right)\mid p_{S}>0 \right]\mathbb{P}_{S}\left(p_{S}>0\right)+\underbrace{\mathbb{E}_{h\sim \mathcal{P}}\left[-\pi\left(h\right)\log\left(\pi\left(h\right)\right) \mathbb{I}\left\{\pi\left(h\right)>0\right\}\right]}_{\leq 1/e\ln 2}\] \[\leq\mathbb{E}_{S}\left[\log\left(\frac{1}{p_{S}}\right)\mid \text{consistent }S\right]\mathbb{P}_{S}\left(\text{consistent }S\right)+\frac{1}{e\ln 2}\,.\]

Putting all of this together,

\[I\left(S;A\left(S\right)\right)\leq\mathbb{E}_{S}\left[\log\left(\frac{1}{p_{S} }\right)\middle|\text{consistent }S\right]\mathbb{P}_{S}\left(\text{consistent }S\right)+\frac{2}{e\ln 2}\,.\]

**Corollary F.6**.: _The generalization of posterior sampling satisfies_

\[-\log\left(1-\mathbb{E}_{S,A\left(S\right)}\left[\mathcal{L}_{\mathcal{D}}\left(A \left(S\right)\right)\mid\mathrm{consistent}\,S\right]\right)\leq\frac{\mathbb{E }_{S}\left[\log\left(\frac{1}{p_{S}}\right)\middle|\mathrm{consistent}\,S \right]+3}{N}\,.\]

Proof.: Combining Lemma C.2 and Lemma F.5 we get

\[I\left(S;A\left(S\right)\right)\geq-N\log\left(1-\mathbb{E}_{S,A\left(S \right)}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S\right)\right)\mid \mathrm{consistent}\,S\right]\right)\mathbb{P}_{S}\left(\mathrm{consistent}\,S\right)\]

and

\[I\left(S;A\left(S\right)\right)\leq\mathbb{E}_{S}\left[\log\left(\frac{1}{p_{S }}\right)\middle|\mathrm{consistent}\,S\right]\mathbb{P}_{S}\left(\mathrm{ consistent}\,S\right)+\frac{2}{e\ln 2}\]

so

\[-N\log\left(1-\mathbb{E}_{S,A\left(S\right)}\left[\mathcal{L}_{ \mathcal{D}}\left(A\left(S\right)\right)\mid\mathrm{consistent}\,S\right] \right)\mathbb{P}_{S}\left(\mathrm{consistent}\,S\right)\] \[\leq\mathbb{E}_{S}\left[\log\left(\frac{1}{p_{S}}\right)\middle| \mathrm{consistent}\,S\right]\mathbb{P}_{S}\left(\mathrm{consistent}\,S \right)+\frac{2}{e\ln 2}\]

and finally, using \(\nicefrac{{2}}{{e\ln 2}}\leq 1.5\) and recalling C.1 we get

\[-\log\left(1-\mathbb{E}_{S,A\left(S\right)}\left[\mathcal{L}_{\mathcal{D}} \left(A\left(S\right)\right)\mid\mathrm{consistent}\,S\right]\right)\leq\frac{ \mathbb{E}_{S}\left[\log\left(\frac{1}{p_{S}}\right)\middle|\mathrm{consistent}\,S \right]+3}{N}\,.\]Let \(\bar{h}\) be a network with depth \(L\), dimensions \(\underline{d}\), and parameters \(\bar{\bm{\theta}}=\left\{\bar{\bm{W}}_{l},\bar{\bm{b}}_{l},\bar{\bm{\gamma}}_{l} \right\}\in\Theta^{\text{BTN}}\left(\underline{d}\right)\). Let \(\underline{d}\geq\bar{\underline{d}}\). Similar to \(\Theta^{\text{BTN}}\left(\underline{d};h_{1},h_{2}\right)\) introduced in Lemma D.26, let \(\Theta^{\text{BTN}}\left(\underline{d};\bar{h}\right)\subset\Theta^{\text{ BTN}}\left(\underline{d}\right)\) be the set of parameters \(\bm{\theta}\) that implement \(\bar{h}\) by setting a subset of the parameters to be equal to \(\bar{\bm{\theta}}\), and zero the effect of redundant neurons by setting their bias and neuron scaling terms to be 0. This is illustrated in Figure 4. In particular, in our notation, \(\Theta^{\text{BTN}}\left(\underline{d};h_{1},h_{2}\right)=\Theta^{\text{BTN}} \left(\underline{d};h_{1}\oplus h_{2}\right)\).

**Lemma F.7**.: _Let \(h\) be a network with depth \(L\) and dimensions \(\bar{\underline{d}}\). Let \(\underline{d}\geq\bar{\underline{d}}\). Then_

\[-\log\left(\frac{\left|\Theta^{\text{BTN}}\left(\underline{d};\bar{h}\right) \right|}{\left|\Theta^{\text{BTN}}\left(\underline{d}\right)\right|}\right) \leq w\left(\underline{\bar{d}}\right)+O\left(n\left(\underline{d}\right) \cdot\log\left(\underline{d}_{\max}+d_{0}\right)\right)\,.\]

Proof.: We prove this by counting the number of constrained parameters in \(\Theta^{\text{BTN}}\left(\underline{d};\bar{h}\right)\). The number of constrained weights is

\[\bar{d}_{1}d_{0}+\sum_{l=2}^{L}\bar{d}_{l}\bar{d}_{l-1}\,,\]

which is exactly \(w\left(\underline{\bar{d}}\right)\). In addition, there are \(n\left(\underline{d}\right)\) constrained bias terms, and \(n\left(\underline{d}\right)\) constrained scaling terms. In total, after accounting for the quantization of each parameter, this means that

\[\frac{\left|\Theta^{\text{BTN}}\left(\underline{d};\bar{h}\right)\right|}{ \left|\Theta^{\text{BTN}}\left(\underline{d}\right)\right|}\geq\left(\underbrace {2^{w\left(\bar{d}\right)}}_{\text{weights}}\cdot\underbrace{3^{n\left(\underline {d}\right)}}_{\text{scaling terms}}\cdot\prod_{l=1}^{L}\left(2d_{l-1}\right)^{d_ {l}}\right)^{-1}\]

so

\[-\log\left(\frac{\left|\Theta^{\text{BTN}}\left(\underline{d}; \bar{h}\right)\right|}{\left|\Theta^{\text{BTN}}\left(\underline{d}\right) \right|}\right)\] \[\leq w\left(\underline{\bar{d}}\right)+n\left(\underline{d} \right)\cdot\log 3+\sum_{l=1}^{L}\bar{d}_{l}\cdot\log\left(2d_{l-1}\right)\] \[\leq w\left(\underline{\bar{d}}\right)+n\left(\underline{d} \right)\cdot\log 3+n\left(\underline{d}\right)\cdot\log\left(2d_{\max}+2d_{0}\right)\] \[=w\left(\underline{\bar{d}}\right)+O\left(n\left(\underline{d} \right)\cdot\log\left(\underline{d}_{\max}+d_{0}\right)\right)\,.\]

## Appendix F

Figure 4: **Implementing a narrow network with a wider network.** Blue edges represent parameters set to equal the parameters of \(\bar{h}\), gray nodes represent zero neuron scaling, and gray edges represent unconstrained parameters.

Combining Lemma F.7 with Assumption 2.4, and Corollary 3.4 gives the following lemma.

**Lemma F.8**.: _Consider a distribution \(\mathcal{D}\) induced by a noisy teacher model of depth \(L^{\star}\) and widths \(\underline{d}^{\star}\) (Assumption 2.4) with a noise level of \(\varepsilon^{\star}<1/2\). Let \(S\sim\mathcal{D}^{N}\) be a training set with effective training set label noise \(\hat{\varepsilon}_{\mathrm{tr}}\) as defined in (4). Then there exist constants \(c_{1},c_{2}>0\) such that for any student network of depth \(L\geq\max\left\{L^{\star},14\right\}+2\) and widths \(\underline{d}\in\mathbb{N}^{L}\) satisfying_

\[\forall l=1,\ldots,L^{\star}-1\quad d_{l}\geq d_{l}^{\star}+N^{3/4}\cdot\left( \log N\right)^{c_{1}}+c_{2}\cdot d_{0}\cdot\log\left(N\right)\,,\]

_it holds for posterior sampling with a uniform prior over parameters that_

\[\mathbb{E}_{S}\left[\log\left(\frac{1}{p_{S}}\right)\,|\,\text{ consistent }S\right]\] \[\qquad\leq w\left(\underline{d}^{\star}\right)+N\cdot H\left( \hat{\varepsilon}_{\mathrm{tr}}\right)+2n\left(\underline{d}^{\star}\right)N ^{3/4}\mathrm{polylog}N\] \[\qquad\qquad+O\left(d_{0}\left(d_{0}+n\left(\underline{d}^{ \star}\right)\right)\cdot\log\left(N\right)+n\left(\underline{d}\right)\cdot \log\left(\underline{d}_{\max}+d_{0}\right)\right)\,.\]

_Remark F.9_.: Unlike the bounds for min-size interpolators, there is no \(H\left(\hat{\varepsilon}_{\mathrm{u}}\right)^{3/4}\) term multiplying the \(N^{3/4}\) term. This is because the architecture of random interpolators is fixed, so in our setting we must assume that it is wide enough in order to guarantee interpolation of any noisy training set.

Proof.: Notice that for posterior sampling with uniform distribution over parameters, the interpolation probability \(p_{S}\) can be lower bounded as

\[p_{S}\geq\frac{\left|\Theta^{\text{BTN}}\left(\underline{d};h^{\star}\oplus \tilde{h}_{S}\right)\right|}{\left|\Theta^{\text{BTN}}\left(\underline{d} \right)\right|}\]

and therefore

\[\log\left(\frac{1}{p_{S}}\right)\leq-\log\left(\frac{\left|\Theta^{\text{BTN }}\left(\underline{d};h^{\star}\oplus\tilde{h}_{S}\right)\right|}{\left| \Theta^{\text{BTN}}\left(\underline{d}\right)\right|}\right)\,.\]

Then, using the bounds from Lemma F.7 with the one from Corollary 3.4

\[\log\left(\frac{1}{p_{S}}\right) \leq w\left(\underline{d}^{\star}\right)+N\cdot H\left(\mathcal{L }_{S}\left(h^{\star}\right)\right)+2n\left(\underline{d}^{\star}\right)N^{3/4} \mathrm{polylog}N\] \[\quad+O\left(d_{0}\left(d_{0}+n\left(\underline{d}^{\star} \right)\right)\cdot\log N+n\left(\underline{d}\right)\cdot\log\left(\underline {d}_{\max}+d_{0}\right)\right)\,.\]

By taking the expectation and using Jensen's inequality with the concave \(H\) we arrive at

\[\mathbb{E}_{S}\left[H\left(\mathcal{L}_{S}\left(h^{\star}\right)\right)\,|\, \text{consistent}\,S\right]\leq H\left(\mathbb{E}_{S}\left[\mathcal{L}_{S} \left(h^{\star}\right)\,|\,\text{consistent}\,S\right]\right)=H\left(\hat{ \varepsilon}_{\mathrm{tr}}\right)\,.\]

Hence

\[\mathbb{E}_{S}\left[\log\left(\frac{1}{p_{S}}\right)\,\bigg{|}\, \text{consistent}\,S\right]\] \[\leq w\left(\underline{d}^{\star}\right)+N\cdot H\left(\hat{ \varepsilon}_{\mathrm{tr}}\right)+2n\left(\underline{d}^{\star}\right)N^{3/4} \mathrm{polylog}N\] \[\quad+O\left(d_{0}\left(d_{0}+n\left(\underline{d}^{\star} \right)\right)\cdot\log\left(N\right)+n\left(\underline{d}\right)\cdot\log \left(\underline{d}_{\max}+d_{0}\right)\right)\,.\]

**Recall Theorem 4.4**.: Consider a distribution \(\mathcal{D}\) induced by a noisy teacher model of depth \(L^{\star}\) and widths \(d^{\star}\) (Assumption 2.4) with a noise level of \(\varepsilon^{\star}<1/2\). Let \(S\sim\mathcal{D}^{N}\) be a training set such that \(N=o(\sqrt{1/\mathcal{D}_{\max}})\). Then, there exist constants \(c_{1},c_{2}>0\) such that for any student network of depth \(L\geq\max\left\{L^{\star},14\right\}+2\) and widths \(\underline{d}\in\mathbb{N}^{L}\) holding

\[\forall l=1,\ldots,L^{\star}-1\quad d_{l}\geq d_{l}^{\star}+N^{3/4}\cdot(\log N )^{c_{1}}+c_{2}\cdot d_{0}\cdot\log\left(N\right)\,,\] (11)

the generalization error of posterior sampling satisfies the following.

* **Under arbitrary label noise**, \[\mathbb{E}_{S,A(S)}\left[\mathcal{L}_{\mathcal{D}}\left(A\left(S\right) \right)\right]\leq 1-2^{-H\left(\varepsilon^{\star}\right)}+2N^{2}\mathcal{D}_{ \max}+O\left(C_{\mathrm{rand}}\left(N\right)\right)\,.\]
* **Under independent label noise**, \[\left|\mathbb{E}_{S,A(S)}\left[\mathcal{L}_{\mathcal{D}}\left(A \left(S\right)\right)\right]-2\varepsilon^{\star}\left(1-\varepsilon^{\star} \right)\right|\] \[\leq\left(1-2\varepsilon^{\star}\right)\sqrt{\frac{O\left(C_{ \mathrm{rand}}\left(N\right)\right)+\mathbb{P}\left(\mathrm{inconsistent}\,S \right)}{\mathbb{P}\left(\mathrm{consistent}\,S\right)}}+\frac{\left(N-1 \right)\mathcal{D}_{\max}}{3}+\mathbb{P}\left(\mathrm{inconsistent}\,S \right)\,,\]

where

\[C_{\mathrm{rand}}\left(N\right)=\frac{n\left(\underline{d}^{\star}\right)\cdot \mathrm{polylog}\left(N\right)}{\sqrt[4]{N}}+\frac{w\left(\underline{d}^{ \star}\right)+d_{0}\left(d_{0}+n\left(\underline{d}^{\star}\right)\right) \cdot\log\left(N\right)+n\left(\underline{d}\right)\cdot\log\left(d_{\max}+d_ {0}\right)}{N}\,.\]

_Remark F.10_.: The bound shown in Section 4.2 is found by bounding \(\mathbb{P}\left(\mathrm{inconsistent}\,S\right)\) as in Lemma B.1. Assuming that \(N=\omega\left(n\left(\underline{d}^{\star}\right)^{4}\mathrm{polylog}\left(n \left(\underline{d}^{\star}\right)\right)+d_{0}^{2}\log d_{0}\right)\) we can deduce that \(N=\omega\left(w\left(\underline{d}^{\star}\right)\right)\) as well since

\[w\left(\underline{d}^{\star}\right)\leq\left(n\left(\underline{d}^{\star} \right)+d_{0}\right)^{2}\leq 4\left(\max\left\{n\left(\underline{d}^{\star} \right),d_{0}\right\}\right)^{2}\,.\]

Together with \(N=o\left(\sqrt{1/\mathcal{D}_{\max}}\right)\) we get the desired form of the bounds.

Proof.: Corollary 3.4 implies that there exist \(c_{1},c_{2}>0\) such that a student NN satisfying (11) can interpolate any consistent dataset, and so posterior sampling is interpolating for all consistent datasets.

We start by proving the bound for arbitrary label noise. First, we notice that

\[\hat{\varepsilon}_{\mathfrak{w}} =\mathbb{P}(Y_{1}\neq h^{\star}(X_{1})\mid\mathrm{consistent}\,S )=\frac{\mathbb{P}(Y_{1}\neq h^{\star}(X_{1}),\mathrm{consistent}\,S)}{ \mathbb{P}\left(\mathrm{consistent}\,S\right)}\] \[\leq\frac{\mathbb{P}(Y_{1}\neq h^{\star}(X_{1}))}{\mathbb{P}\left( \mathrm{consistent}\,S\right)}=\frac{\varepsilon^{\star}}{\mathbb{P}\left( \mathrm{consistent}\,S\right)}\,.\]

The entropy function \(H\) is increasing in \(\left[0,\frac{1}{2}\right]\) and achieves its maximum at \(\frac{1}{2}\), so together with the inequality above, we get,

\[H\left(\hat{\varepsilon}_{\mathfrak{w}}\right) \leq H\left(\min\left\{\frac{\varepsilon^{\star}}{\mathbb{P}( \mathrm{consistent}\,S)},\frac{1}{2}\right\}\right)=H\left(\varepsilon^{\star}+ \min\left\{\frac{\varepsilon^{\star}}{\mathbb{P}\left(\mathrm{consistent}\,S \right)}-\varepsilon^{\star},\frac{1}{2}-\varepsilon^{\star}\right\}\right)\] \[=H\left(\varepsilon^{\star}+\underbrace{\min\left\{\varepsilon^{ \star}\frac{\mathbb{P}(\mathrm{inconsistent}\,S)}{\mathbb{P}(\mathrm{consistent}\,S)}, \frac{1}{2}-\varepsilon^{\star}\right\}}_{\triangleq\Delta}\right)=H\left( \varepsilon^{\star}+\Delta\right)\,.\]

Employing the concavity of the entropy function, we get,

\[H\left(\hat{\varepsilon}_{\mathfrak{w}}\right)\leq H\left(\varepsilon^{\star}+ \Delta\right)\leq H(\varepsilon^{\star})+H^{\prime}(\varepsilon^{\star})\cdot \Delta\leq H(\varepsilon^{\star})+\underbrace{H^{\prime}(\varepsilon^{\star}) \cdot\varepsilon^{\star}}_{\leq\frac{1}{2},\mathrm{algebraically}}\cdot\frac{ \mathbb{P}\left(\mathrm{inconsistent}\,S\right)}{\mathbb{P}\left(\mathrm{ consistent}\,S\right)}\,.\]By combining the above with Corollary F.6, Lemma F.8, we have that

\[-\log\left(1-\mathbb{E}_{(S,A(S))}\left[\mathcal{L}_{\mathcal{D}} \left(A\left(S\right)\right)\mid\text{consistent}\;S\right]\right)\leq\frac{ \mathbb{E}_{S}\left[\log\left(1/p_{S}\right)\mid\text{consistent}\;S\right]+3}{N}\] \[\qquad\leq H\left(\hat{\varepsilon}_{\text{tr}}\right)+\frac{ 1}{N}\Big{(}w\left(d^{\star}\right)+N\cdot H\left(\hat{\varepsilon}_{\text{tr }}\right)+2n\left(\underline{d}^{\star}\right)N^{3/4}\text{polylog}N\] \[\qquad\quad+O\left(d_{0}\left(d_{0}+n\left(\underline{d}^{\star} \right)\right)\cdot\log\left(N\right)+n\left(\underline{d}\right)\cdot\log \left(\underline{d}_{\max}+d_{0}\right)\right)\Big{)}\] \[\qquad\leq H\left(\hat{\varepsilon}_{\text{tr}}\right)+O\left( \frac{n\left(\underline{d}^{\star}\right)\cdot\text{polylog}\left(N\right)}{ \sqrt[4]{N}}+\frac{w\left(d^{\star}\right)+d_{0}\left(d_{0}+n\left(\underline{ d}^{\star}\right)\right)\cdot\log\left(\underline{d}_{\max}+d_{0}\right)}{N}\right)\] \[\qquad\leq H\left(\varepsilon^{\star}\right)+\frac{\mathbb{P} \left(\text{inconsistent}\;S\right)}{2\mathbb{P}\left(\text{consistent}\;S \right)}\] \[\qquad\quad+O\left(\frac{n\left(d^{\star}\right)\cdot\text{polylog }\left(N\right)}{\sqrt[4]{N}}+\frac{w\left(d^{\star}\right)+d_{0}\left(d_{0}+ n\left(\underline{d}^{\star}\right)\right)\cdot\log\left(N\right)+n\left(\underline{d} \right)\cdot\log\left(\underline{d}_{\max}+d_{0}\right)}{N}\right)\] \[\qquad\quad=H\left(\varepsilon^{\star}\right)+\frac{\mathbb{P} \left(\text{inconsistent}\;S\right)}{2\mathbb{P}\left(\text{consistent}\;S \right)}+O\left(C_{\text{rand}}\left(N\right)\right)\,.\]

Rearranging the inequality results in

\[\mathbb{E}_{(S,A(S))} \left[\mathcal{L}_{\mathcal{D}}\left(A\left(S\right)\right)\mid \text{consistent}\;S\right]\] \[\leq 1-2^{-H\left(\varepsilon^{\star}\right)-\frac{\mathbb{P} \left(\text{inconsistent}\;S\right)}{2\mathbb{P}\left(\text{consistent}\;S \right)}-O\left(C_{\text{rand}}\left(N\right)\right)}\]

Then, using Lemma A.6, we get,

\[\mathbb{E}_{(S,A(S))}\left[\mathcal{L}_{\mathcal{D}}\left(A\left( S\right)\right)\mid\text{consistent}\;S\right]\] \[\leq 1-2^{-H\left(\varepsilon^{\star}\right)}+\frac{\mathbb{P} \left(\text{inconsistent}\;S\right)}{2\mathbb{P}\left(\text{consistent}\;S \right)}+O\left(C_{\text{rand}}\left(N\right)\right)\,.\]

Repeating the argument from the proof of Theorem 4.2, since for an RV \(X\) in \(\left[0,1\right]\) and a binary RV \(Y\) we have

\[\mathbb{E}[X]=\mathbb{E}[X\mid Y]\underbrace{\mathbb{P}(Y)}_{\leq 1}+ \underbrace{\mathbb{E}[X\mid\neg Y]}_{\leq 1}\mathbb{P}(\neg Y)\leq \mathbb{E}[X\mid Y]+\mathbb{P}(\neg Y)\,,\]

we have,

\[\mathbb{E}_{(S,A(S))}\left[\mathcal{L}_{\mathcal{D}}\left(A\left( S\right)\right)\right]\leq\mathbb{E}_{(S,A(S))}\left[\mathcal{L}_{\mathcal{D}} \left(A\left(S\right)\right)\mid\text{consistent}\;S\right]+\mathbb{P}\left( \text{inconsistent}\;S\right)\] \[\leq 1-2^{-H\left(\varepsilon^{\star}\right)}+\frac{\mathbb{P} \left(\text{inconsistent}\;S\right)}{2\mathbb{P}\left(\text{consistent}\;S \right)}+\mathbb{P}\left(\text{inconsistent}\;S\right)+O\left(C_{\text{rand }}\left(N\right)\right)\] \[\leq 1-2^{-H\left(\varepsilon^{\star}\right)}+2\frac{\frac{1}{2}N^{2 }\mathcal{D}_{\max}}{1-\frac{1}{2}N^{2}\mathcal{D}_{\max}}+O\left(C_{\text{rand }}\left(N\right)\right)\] \[\leq 1-2^{-H\left(\varepsilon^{\star}\right)}+2N^{2}\mathcal{D}_{ \max}+O\left(C_{\text{rand}}\left(N\right)\right)\]

where in the last inequality we used \(t/\left(1-t\right)\leq 2t\) for \(t\in\left[0,1/2\right]\).

Moving on to the independent noise setting, we combine Lemma F.5, Lemma F.8, and \(\hat{\varepsilon}_{\text{tr}}\leq\varepsilon^{\star}<\frac{1}{2}\) from Lemma B.2, to bound the mutual information as

\[I\left(S;A\left(S\right)\right) \leq\ \mathbb{E}_{S}\left[\log\left(\frac{1}{p_{S}}\right)\mid \text{consistent}\;S\right]\overbrace{\mathbb{P}_{S}\left(\text{consistent}\;S \right)}+\frac{2}{e\ln 2}\] \[\leq\ \mathbb{E}_{S}\left[\log\left(\frac{1}{p_{S}}\right)\mid \text{consistent}\;S\right]+1.1\] \[\leq\ N\cdot H\left(\varepsilon^{\star}\right)+O\left(N\cdot C_{ \text{rand}}\left(N\right)\right)\,.\]Plugging the above into \(C(N)\) of Lemma C.3, we get,

\[C\left(N\right)=\frac{I\left(S;A\left(S\right)\right)-N\cdot H \left(\varepsilon^{\star}\right)+N\cdot\mathbb{P}_{S\sim\mathcal{D}^{N}}\left( \text{inconsistent }S\right)}{N\cdot\mathbb{P}\left(\text{consistent }S\right)}\] \[\leq\frac{N\cdot H\left(\varepsilon^{\star}\right)+O\left(N \cdot C_{\text{rad}}(N)\right)-N\cdot H\left(\varepsilon^{\star}\right)+N \cdot\mathbb{P}\left(\text{inconsistent }S\right)}{N\cdot\mathbb{P}\left(\text{consistent }S\right)}\] \[=\frac{O\left(C_{\text{rand}}\left(N\right)\right)+\mathbb{P} \left(\text{inconsistent }S\right)}{\mathbb{P}\left(\text{consistent }S\right)}\,.\]

Then we continue as in the arbitrary noise setting to get the desired bound.

Alignment with Dale's Law

In this section, we show that our results apply to a model resembling "Dale's Law" [82], _i.e.,_ such that for each neuron, all outgoing weights have the same sign. To this end, we define the following model, in which the main difference from Def. 2.1 is that neuron scaling is applied after the threshold activation.

**Definition G.1** (Binary threshold networks with outgoing scaling).: For a depth \(L\), widths \(\underline{d}=(d_{1},\ldots,d_{L})\), input dimension \(d_{0}\), a scaled-neuron fully connected binary threshold NN with outgoing weight scaling (oBTN), is a mapping \(\boldsymbol{\theta}\mapsto g_{\boldsymbol{\theta}}\) such that \(g_{\boldsymbol{\theta}}:\left\{0,1\right\}^{d_{0}}\rightarrow\left\{-1,0,1 \right\}^{d_{L}}\), parameterized by

\[\boldsymbol{\theta}=\left\{\mathbf{W}^{(l)},\mathbf{b}^{(l)}, \boldsymbol{\gamma}^{(l)}\right\}_{l=1}^{L}\,,\]

where for every layer \(l\in[L]\),

\[\mathbf{W}^{(l)}\!\in\mathcal{Q}_{l}^{W}\!=\!\left\{0,1\right\}^{d_{l}\times d _{l-1}},\ \boldsymbol{\gamma}^{(l)}\!\in\mathcal{Q}_{l}^{\gamma}\!=\!\left\{-1,0,1 \right\}^{d_{l}},\ \mathbf{b}^{(l)}\!\in\mathcal{Q}_{l}^{b}\!=\!\left\{-d_{l-1}+1, \ldots,d_{l-1}\right\}^{d_{l}}\,.\]

This mapping is defined recursively as \(g_{\theta}\left(\mathbf{x}\right)=g^{(L)}\left(\mathbf{x}\right)\) where

\[g^{(0)}\left(\mathbf{x}\right) =\mathbf{x}\,,\] \[\forall l\in[L] g^{(l)}\left(\mathbf{x}\right) =\boldsymbol{\gamma}^{(l)}\odot\mathbb{I}\left\{\mathbf{W}^{(l)} g^{(l-1)}\left(\mathbf{x}\right)+\mathbf{b}^{(l)}>\mathbf{0}\right\}\,.\]

**Lemma G.2**.: _Let \(g_{\boldsymbol{\theta}}\) be an oBTN as in Def. G.1. Then there exists a BTN \(h_{\boldsymbol{\theta}^{\prime}}\) with the same dimensions and \(\mathbf{b}^{\prime(l)}\in\mathcal{Q}_{l}^{2b}\triangleq\left\{-2d_{l-1}+1, \ldots,2d_{l}\right\}\) such that \(h_{\boldsymbol{\theta}^{\prime}}\equiv g_{\boldsymbol{\theta}}+s\) for \(s\in\left\{0,1\right\}^{d_{L}}\) such that for all \(i=1,\ldots,d_{L}\), \(s_{i}=1\) only if \(\gamma_{i}^{(L)}=-1\)._

Proof.: We prove the lemma by induction on depth. As we will see, the base case is a particular case of the step of the induction, so we start with the latter. Let \(l=1,\ldots,L\). For ease of notation, we denote \(C=g^{(l)}\) and \(A=g^{(l-1)}\), as well as \(C^{\prime}=h^{(l)}\), \(A^{\prime}=h^{(l-1)}\). In addition, we omit the superscripts from the \(l^{th}\) layer's parameters. Let \(i=1,\ldots,d_{l}\), then by the induction hypothesis there exists some \(a\in\left\{0,1\right\}^{d_{l-1}}\) such that \(A\left(\mathbf{x}\right)=A^{\prime}\left(\mathbf{x}\right)-a\), for all inputs \(\mathbf{x}\),

\[C\left(\mathbf{x}\right)_{i} =\gamma_{i}\cdot\mathbb{I}\left\{b_{i}+\sum_{j=1}^{d_{l-1}}w_{ij }A\left(\mathbf{x}\right)_{j}>0\right\}=\gamma_{i}\cdot\mathbb{I}\left\{b_{i} +\sum_{j=1}^{d_{l-1}}w_{ij}\left(A^{\prime}\left(\mathbf{x}\right)_{j}-a_{j} \right)>0\right\}\] \[=\gamma_{i}\cdot\mathbb{I}\left\{\left(b_{i}-\sum_{j=1}^{d_{l-1 }}w_{ij}a_{j}\right)+\sum_{j=1}^{d_{l-1}}w_{ij}A^{\prime}\left(\mathbf{x} \right)_{j}>0\right\}\,.\]

If \(\gamma_{i}=+1\), choose \(\gamma_{i}^{\prime}=+1\), and \(b_{i}^{\prime}=b_{i}-\sum_{j=1}^{d_{l-1}}w_{ij}a_{j}\). Clearly, since \(w_{ij},a_{j}\in\left\{0,1\right\}\), it holds that \(\left|\sum_{j=1}^{d_{l-1}}w_{ij}a_{j}\right|\leq d_{l-1}\) so \(b_{i}^{\prime}\in\mathcal{Q}_{l}^{2b}\). Then

\[C\left(\mathbf{x}\right)_{i}=\mathbb{I}\left\{b_{i}^{\prime}+\gamma_{i}^{\prime }\cdot\sum_{j=1}^{d_{l-1}}w_{ij}A^{\prime}\left(\mathbf{x}\right)_{j}>0\right\} =C^{\prime}\left(\mathbf{x}\right)_{i}\]_i.e.,_ the claim holds with \(s_{i}=0\). If \(\gamma_{i}=-1\) then

\[C\left(\mathbf{x}\right)_{i} =\gamma_{i}\cdot\mathbb{I}\left\{\left(b_{i}-\sum_{j=1}^{d_{l-1}}w_ {ij}a_{j}\right)+\sum_{j=1}^{d_{l-1}}w_{ij}A^{\prime}\left(\mathbf{x}\right)_{j }>0\right\}\] \[=-\mathbb{I}\left\{\left(b_{i}-\sum_{j=1}^{d_{l-1}}w_{ij}a_{j} \right)+\sum_{j=1}^{d_{l-1}}w_{ij}A^{\prime}\left(\mathbf{x}\right)_{j}>0\right\}\] \[=-1+\mathbb{I}\left\{\left(b_{i}-\sum_{j=1}^{d_{l-1}}w_{ij}a_{j} \right)+\sum_{j=1}^{d_{l-1}}w_{ij}A^{\prime}\left(\mathbf{x}\right)_{j}\leq 0\right\}\] \[=-1+\mathbb{I}\left\{1-\left(b_{i}-\sum_{j=1}^{d_{l-1}}w_{ij}a_{j }\right)-\sum_{j=1}^{d_{l-1}}w_{ij}A^{\prime}\left(\mathbf{x}\right)_{j}\geq 0\right\}\] \[=-1+\mathbb{I}\left\{1-\left(b_{i}-\sum_{j=1}^{d_{l-1}}w_{ij}a_{j }\right)-\sum_{j=1}^{d_{l-1}}w_{ij}A^{\prime}\left(\mathbf{x}\right)_{j}>0 \right\}\,.\]

Thus, we can construct the \(l^{th}\) layer of \(h\) by choosing \(\gamma_{i}^{\prime}=-1\) and \(b_{i}^{\prime}=1-\left(b_{i}-\sum_{j=1}^{d_{l-1}}w_{ij}a_{j}\right)\) so

\[C\left(\mathbf{x}\right)_{i} =-1+\mathbb{I}\left\{1-\left(b_{i}-\sum_{j=1}^{d_{l-1}}w_{ij}a_{j }\right)-\sum_{j=1}^{d_{l-1}}w_{ij}A^{\prime}\left(\mathbf{x}\right)_{j}>0\right\}\] \[=-1+\mathbb{I}\left\{b_{i}^{\prime}+\gamma_{i}^{\prime}\sum_{j=1} ^{d_{l-1}}w_{ij}A^{\prime}\left(\mathbf{x}\right)_{j}>0\right\}\] \[=C^{\prime}\left(\mathbf{x}\right)_{i}-s_{i}\]

with \(s_{i}=1\). Finally, if \(\gamma_{i}=0\), then \(C_{i}\) is identically \(0\), so we can choose \(\gamma_{i}^{\prime}=b_{i}^{\prime}=s_{i}=0\). Notice that this construction also proves the base case \(l=1\) where \(a=\mathbf{0}\). 

**Corollary G.3**.: _Let \(\Theta^{\prime}\) be the set of oBTN parameters such that for all \(\boldsymbol{\theta}\in\Theta^{\prime}\), \(g_{\boldsymbol{\theta}}:\left\{0,1\right\}^{d_{0}}\rightarrow\left\{0,1 \right\}^{d_{L}}\). Then there exists a BTN, \(h\) as in Lemma G.2 such that \(g_{\boldsymbol{\theta}}\equiv h\)._

Proof.: Let \(\boldsymbol{\theta}\in\Theta^{\prime}\). Since \(g_{\boldsymbol{\theta}}\left(\mathbf{x}\right)\neq-1\) for all \(\mathbf{x}\), there exist parameters \(\boldsymbol{\theta}^{\prime}\in\Theta^{\prime}\) such that \(g_{\boldsymbol{\theta}^{\prime}}\equiv g_{\boldsymbol{\theta}}\), and \(\boldsymbol{\gamma}^{\prime(L)}\geq 0\). Hence, by Lemma G.2 there exists a BTN \(h\) such that \(h\equiv g_{\boldsymbol{\theta}^{\prime}}\), _i.e.,_ with \(s=\mathbf{0}\). 

_Remark G.4_.: Similar results can be shown in the other direction. That is, that BTNs can be represented as slightly larger oBTNs.

Finally, recall from Appendix F, that the cardinality of the hypothesis class is related to the error terms of Theorem 4.2 and Theorem 4.4 only logarithmically, meaning that we can apply the results to Def. G.1 without qualitatively changing them.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We accurately state the general setup and the essential results. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our theory in Section 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: General assumptions on the label-generating process are clearly stated in Section 2.2. Additional assumptions are stated at the beginning of each theorem. Rigorous proofs are given in our supplementary materials and are referred to in the main body of the paper. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: No experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.

5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: No experiments. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: No experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.

7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: No experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: No experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our work adheres to NeurIPS' ethical guidelines. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.

* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our work is theoretical in nature, and we do not see a direct social impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: No data or models are released. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: Not using external assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA]. Justification: This is a theoretical papers; we do not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.

15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our research did not involve human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.