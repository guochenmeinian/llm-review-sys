# A Heat Diffusion Perspective on Geodesic Preserving Dimensionality Reduction

 Guillaume Huguet\({}^{1}\)1 Alexander Tong\({}^{1}\)2 Edward De Brouwer\({}^{2}\)1

Yanlei Zhang\({}^{1}\) Guy Wolf\({}^{1}\) Ian Adelstein\({}^{2}\)3 Smita Krishnaswamy\({}^{2}\)4

\({}^{1}\)Universite de Montreal; Mila - Quebec AI Institute \({}^{2}\) Yale University

Equal contributionCo-senior authors

Footnote 1: https://github.com/KrishnaswamyLab/HeatGeo

Footnote 2: footnotemark:

###### Abstract

Diffusion-based manifold learning methods have proven useful in representation learning and dimensionality reduction of modern high dimensional, high throughput, noisy datasets. Such datasets are especially present in fields like biology and physics. While it is thought that these methods preserve underlying manifold structure of data by learning a proxy for geodesic distances, no specific theoretical links have been established. Here, we establish such a link via results in Riemannian geometry explicitly connecting heat diffusion to manifold distances. In this process, we also formulate a more general heat kernel based manifold embedding method that we call _heat geodesic embeddings_. This novel perspective makes clearer the choices available in manifold learning and denoising. Results show that our method outperforms existing state-of-the-art in preserving ground truth manifold distances, and preserving cluster structure in toy datasets. We also showcase our method on single cell RNA-sequencing datasets with both continuum and cluster structure, where our method enables interpolation of withheld timepoints of data. Finally, we show that parameters of our more general method can be configured to give results similar to PHATE (a state-of-the-art diffusion based manifold learning method) as well as SNE (an attraction/repulsion neighborhood based method that forms the basis of t-SNE).

## 1 Introduction

The advent of high throughput and high dimensional data in various fields of science have made dimensionality reduction and visualization techniques an indispensable part of exploratory analysis. Diffusion-based manifold learning methods, based on the data diffusion operator, first defined in [5], have proven especially useful due to their ability to handle noise and density variations while preserving structure. As a result, diffusion-based dimensionality reduction methods, such as PHATE [22], T-PHATE [3], and diffusion maps [5], have emerged as methods for analyzing high throughput noisy data in various situations. While these methods are surmised to learn manifold geodesic distances, no specific theoretical links have been established. Here, we establish such a link by using Varadhan's formula [34] and a parabolic Harnack inequality [17; 25], which relate manifold distances to heat diffusion directly. This lens gives new insight into existing dimensionality reduction methods, including when they preserve geodesics, and suggests a new method for dimensionality reduction to explicitly preserve geodesics, which we call _heat geodesic embeddings_3. Furthermore, based on our understanding of other methods [22; 5], we introduce theoretically justified parameter choices thatallow our method to have greater versatility in terms of distance denoising and emphasis on local versus global distances.

Generally, data diffusion operators are created by first computing distances between datapoints, transforming these distances into affinities by pointwise application of a kernel function (like a Gaussian kernel), and then row normalizing with or without first applying degree normalization into a Markovian diffusion operator \(\bm{P}\)[5; 9; 14; 21; 33]. The entries of \(\bm{P}(x,y)\) then contain probabilities of diffusing (or random walk probabilities) from one datapoint to another. Diffusion maps and PHATE use divergences between these diffusion or random walk-based probability distributions \(\bm{P}(x,\cdot)\) and \(\bm{P}(y,\cdot)\) to design a diffusion-based distance that may not directly relate to manifold distance. Our framework directly utilizes a heat kernel based distance, and offers a more comprehensive perspective to study these diffusion methods. By configuring parameters in our framework, we show how we can navigate a continuum of embeddings methods from PHATE [22] to Stochastic Neighbor Embedding (SNE) [11].

In summary, our contributions are as follows:

* We define the _heat-geodesic_ dissimilarity based on Varadhan's formula and the two-sided heat kernel bounds.
* Based on this dissimilarity, we present a versatile geodesic-preserving method for dimensionality reduction which we call _heat geodesic embedding._
* We establish a relationship between diffusion-based distances and the heat-geodesic dissimilarity.
* We establish connections between our method and popular dimensionality reduction techniques such as PHATE and SNE, shedding light on their geodesic preservation and denoising properties based on modifications of the computed dissimilarity and distance preservation losses.
* We empirically demonstrate the advantages of Heat Geodesic Embedding in preserving manifold geodesic distances in several experiments showcasing more faithful manifold distances in the embedding space, as well as our ability to interpolate data within the manifold.

## 2 Preliminaries

First, we introduce fundamental notions that form the basis of our manifold learning methods: Varadhan's formula [34] on a manifold, diffusion processes on graphs, efficient heat kernel approximations, and multidimensional scaling [12; 4; 16].

Varadhan's formulaVaradhan's formula is a powerful tool in differential geometry that establishes a connection between the heat kernel and the shortest path (geodesic) distance on a Riemannian manifold. Its versatility has led to widespread applications in machine learning [27; 28; 29; 15; 6; 10]. Let \((M,g)\) be a closed Riemannian manifold, and \(\Delta\) the Laplace-Beltrami operator on \(M\). The heat kernel \(h_{t}(x,y)\) on \(M\) is the minimal positive fundamental solution of the heat equation \(\frac{\partial u}{\partial t}=\Delta u\) with initial condition \(h_{0}(x,y)=\delta_{x}(y)\). In a \(d\)-dimensional Euclidean space the heat kernel is \(h_{t}(x,y)=(4\pi t)^{-d/2}\ e^{-d(x,y)^{2}/4t}\) so that \(-4t\log h_{t}(x,y)=2dt\log(4\pi t)+d^{2}(x,y)\) and

Figure 1: Embeddings of the Swiss roll (top) and Tree (bottom) datasets for different manifold learning methods. Our HeatGeo method correctly unrolls the Swiss roll while t-SNE and UMAP create undesirable artificial clusters.

we observe the following limiting behavior:

\[\lim_{t\to 0}-4t\log h_{t}(x,y)=d^{2}(x,y).\] (1)

Varadhan [34] (see also [20]) proved that eq. 1 (now Varadhan's formula) holds more generally on complete Riemannian manifolds \(M\), where \(d(x,y)\) is the geodesic distance on \(M\), and the convergence is uniform over compact subsets of \(M\). A related result for complete Riemannian manifolds that satisfy the parabolic Harnack inequality (which includes convex domains in Euclidean space and Riemannian manifolds with non-negative Ricci curvature) is the two-sided heat kernel bound [25, 17], showing that for any \(\epsilon\in(0,1)\) there exist constants \(c(\epsilon)\) and \(C(\epsilon)\) such that

\[\frac{c(\epsilon)}{V(x,\sqrt{t})}\exp\left(-\frac{d(x,y)^{2}}{4(1+\epsilon)t }\right)\leq h_{t}(x,y)\leq\frac{C(\epsilon)}{V(x,\sqrt{t})}\exp\left(-\frac{ d(x,y)^{2}}{4(1-\epsilon)t}\right),\] (2)

where \(V(x,\sqrt{t})\) is the volume of a ball or radius \(\sqrt{t}\) centered at x. We denote this relation by \(h_{t}(x,y)\simeq V(x,\sqrt{t})^{-1}\exp(-d(x,y)^{2}/t)\) and note that it again recovers eq. 1 in the \(t\to 0\) limit, which is unsurprising as Varadhan's result holds more generally. More important for our purposes is that \(h_{t}(x,y)\simeq V(x,\sqrt{t})^{-1}\exp(-d(x,y)^{2}/t)\) holds for \(t>0\) which will allow us to approximate geodesic distances \(d(x,y)\) from a diffusion based estimation of the heat kernel \(h_{t}(x,y)\) and the volume \(V(x,\sqrt{t})\). In appendix C.3, we provide examples using inequality (2).

Graph construction and diffusionOur construction starts by creating a graph from a point cloud dataset \(\bm{X}\) of size \(n\). We use a kernel function \(\kappa:\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{+}\), such that the (weighted) adjacency matrix is \(\bm{W}_{ij}:=\kappa(x_{i},x_{j})\) for all \(x_{i},x_{j}\in\bm{X}\). The kernel function could be a Gaussian kernel, or constructed from a nearest neighbor graph. The resulting graph \(\mathcal{G}\) is characterized by the set of nodes (an ordering of the observations), the adjacency matrix, and the set of edges, i.e. pairs of nodes with non-zero weights. The graph Laplacian is an operator acting on signals on \(\mathcal{G}\) such that it mimics the negative of the Laplace operator. The combinatorial graph Laplacian matrix is defined as \(\bm{L}:=\bm{Q}-\bm{W}\) and its normalized version as \(\bm{L}=\bm{I}_{n}-\bm{Q}^{-1/2}\bm{W}\bm{Q}^{-1/2}\), where \(\bm{Q}\) is a diagonal degree matrix with \(\bm{Q}_{ii}:=\sum_{j}\bm{W}_{ij}\). The Laplacian is symmetric positive semi-definite, and has an eigen-decomposition \(\bm{L}=\Psi\Lambda\Psi^{T}\). Throughout the presentation, we assume that \(\bm{Q}_{ii}>0\) for all \(i\in\{1,\dots,n\}\). The Laplacian allows us to define the heat equation on \(\mathcal{G}\), with respect to an initial signal \(\bm{f}_{0}\in\mathbb{R}^{n}\) on \(\mathcal{G}\):

\[\frac{\partial}{\partial t}\bm{f}(t)+\bm{L}\bm{f}(t)=\bm{0},\;s.t.\quad\bm{f}( 0)=\bm{f}_{0}\quad t\in\mathbb{R}^{+}.\] (3)

The solution of the above differential equation is obtained with the matrix exponential \(\bm{f}(t)=e^{-t\bm{L}}\bm{f}_{0}\), and we define the heat kernel on the graph as \(\bm{H}_{t}:=e^{-t\bm{L}}\). By eigendecomposition, we have \(\bm{H}_{t}=\Psi e^{-t\Lambda}\Psi^{T}\). The matrix \(\bm{H}_{t}\) is a diffusion matrix that characterizes how a signal propagate through the graph according to the heat equations.

Other diffusion matrices on graphs have also been investigated in the literature. The transition matrix \(\bm{P}:=\bm{Q}^{-1}\bm{W}\) characterizing a random walk on the graph is another common diffusion matrix used for manifold learning such as PHATE [22] and diffusion maps [5]. It is a stochastic matrix that converges to a stationary distribution \(\bm{\pi}_{i}:=\bm{Q}_{ii}/\sum_{i}\bm{Q}_{ii}\), under mild assumptions.

Fast computation of heat diffusionExact computation of the (discrete) heat kernel \(\bm{H}_{t}\) is computationally costly, requiring a full eigendecomposition in \(O(n^{3})\) time. Fortunately, multiple fast approximations have been proposed, including using orthogonal polynomials or the Euler backward methods. In this work, we use Chebyshev polynomials, as they have been shown to converge faster than other polynomials on this problem [13].

Chebyshev polynomials are defined by the recursive relation \(\{T_{k}\}_{k\in\mathbb{N}}\) with \(T_{0}(y)=0\), \(T_{1}(y)=y\) and \(T_{k}(y)=2yT_{k-1}(y)-T_{k-2}(y)\) for \(k\geq 2\). Assuming that the largest eigenvalue is less than two (which holds for the normalized Laplacian), we approximate the heat kernel with the truncated polynomials of order \(K\)

\[\bm{H}_{t}\approx p_{K}(\bm{L},t):=\frac{b_{t,0}}{2}+\sum_{k=1}^{K}b_{t,k}T_{k }(\bm{L}-\bm{I}_{n}),\] (4)where the \(K+1\) scalar coefficients \(\{b_{t,i}\}\) depend on time and are evaluated with the Bessel function. Computing \(p_{K}(\bm{L},t)\bm{f}\) requires \(K\) matrix-vector product and \(K+1\) Bessel function evaluation. The expensive part of the computation are the matrix-vector products, which can be efficient if the Laplacian matrix is sparse. Interestingly, we note that the evaluation of \(T_{k}\) do not depend on the diffusion time. Thus, to compute multiple approximations of the heat kernel \(\{p_{K}(\bm{L},t)\}_{t\in\mathcal{T}}\), only necessitates reweighting the truncated polynomial \(\{T_{k}\}_{k\in[1,\dots,K]}\) with the corresponding \(|\mathcal{T}|\) sets of Bessel coefficients. The overall complexity is dominated by the truncated polynomial computation which takes \(O(K(E+n))\) time where \(E\) is the number of non-zero values in \(\bm{L}\).

Another possible approximation is using the Euler backward method. It requires solving \(K\) systems of linear equations \(\bm{f}(t)=(\bm{I}_{n}+(t/K)\bm{L})^{-K}\bm{f}(0)\), which can be efficient for sparse matrices using the Cholesky decomposition [10; 28]. We quantify the differences between the heat kernel approximations in Appendix C.

Metric multidimensional scalingGiven a dissimilarity function \(d\) between data points, metric multidimensional scaling (MDS) [16] finds an embedding \(\phi\) such that the difference between the given dissimilarity and the Euclidean distance in the embedded space is minimal across all data points. Formally, for a given function \(d:\mathbb{R}^{d}\times\mathbb{R}^{d}\rightarrow\mathbb{R}^{+}\), MDS minimizes the following objective:

\[L(\bm{X})=\bigg{(}\sum_{ij}\big{(}d(x_{i},x_{j})-\|\phi(x_{i})-\phi(x_{j})\|_ {2}\big{)}^{2}\bigg{)}^{1/2},\] (5)

In metric MDS the solution is usually found by the SMACOF algorithm [30], or stochastic gradient descent [37].

## 3 Related Work

We review state-of-the-art embedding methods and contextualize them with respect to Heat Geodesic Embedding. A formal theoretical comparison of all methods is given in Section 5. Given a set of high-dimensional datapoints, the objective of embedding methods is to create a map that embeds the observations in a lower dimensional space, while preserving distances or similarities. Different methods vary by their choice of distance or dissimilarity functions, as shown below.

Diffusion mapsIn diffusion maps [5], an embedding in \(k\) dimensions is defined via the first \(k\) non-trivial right eigenvectors of the t-steps random walk \(\bm{P}^{t}\) weighted by their eigenvalues. The embedding preserves the _diffusion distance_\(DM_{\bm{P}}(x_{i},x_{j}):=\|(\bm{\delta}_{\bm{i}}\bm{P}^{t}-\bm{\delta}_{\bm{ j}}\bm{P}^{t})(1/\bm{\pi})\|_{2}\), where \(\bm{\delta}_{i}\) is a vector such that \((\bm{\delta}_{i})_{j}=1\) if \(j=i\) and \(0\) otherwise, and \(\bm{\pi}\) is the stationary distribution of \(\bm{P}\). Intuitively, \(DM_{\bm{P}}(x_{i},x_{j})\) considers all the \(t\)-steps paths between \(x_{i}\) and \(x_{j}\). A larger diffusion time can be seen as a low frequency graph filter, i.e. keeping only information from the low frequency transitions such has the stationary distributions. For this reason, using diffusion with \(t>1\) helps denoising the relationship between observations.

PhatThis diffusion-based method preserves the _potential distance_[22]\(PH_{\bm{P}}:=\|-\log\bm{\delta}_{i}\bm{P}^{t}+\log\bm{\delta}_{j}\bm{P}^{t}\|_{2}\), and justifies this approach using the \(\log\) transformation to prevent nearest neighbors from dominating the distances. An alternative approach is suggested using a square root transformation. Part of our contributions is to justify the \(\log\) transformation from a geometric point of view. The embedding is defined using multidimensional scaling, which we present below.

Sne, t-Sne, UmapWell-known attraction/repulsion methods such as SNE [11], t-SNE [32], and UMAP [19] define an affinity matrix with entries \(p_{ij}\) in the ambient space, and another affinity matrix with entries \(q_{ij}\) in the embedded space. To define the embedding, a loss between the two affinity matrices is minimized. Specifically, the loss function is \(D_{\mathrm{KL}}(p||q):=\sum_{ij}p_{ij}\log p_{ij}/q_{ij}\) in SNE and t-SNE, whereas UMAP is equivalent to adding \(D_{\mathrm{KL}}(1-p||1-q)\) for unnormalized densities [2]. While these methods preserve affinities, they do not preserve any types of distances in the embedding.

## 4 Heat-Geodesic Embedding

In this section, we present our Heat Geodesic Embedding which is summarized in Alg. 1. We start by introducing the heat-geodesic dissimilarity, then present a robust transformation, and a heuristic to choose the optimal diffusion time. Proofs not present in the main text are given in AppendixA.

We consider the discrete case, where we have a set of \(n\) points \(\{x_{i}\}_{i=1}^{n}=:\bm{X}\) in a high dimensional Euclidean space \(x_{i}\in\mathbb{R}^{d}\). From this point cloud, we want to define a map \(\phi:\mathbb{R}^{d}\rightarrow\mathbb{R}^{k}\) that embeds the observation in a lower dimensional space. An important property of our embedding is that we preserve manifold geodesic distances in a low dimensional space.

Heat-geodesic dissimilarityInspired by Varadhan's formula and the Harnack inequalities, we defined a heat-geodesic dissimilarity based on heat diffusion on graphs. From observations (data-points) in \(\mathbb{R}^{d}\), we define an undirected graph \(\mathcal{G}\), and compute its heat kernel \(\bm{H}_{t}=e^{-t\bm{L}}\), where \(\bm{L}\) is the combinatorial or symmetrically normalized graph Laplacian (the heat kernel is thus symmetric). Following the inequality (2), we can rearrange the terms to isolate the geodesic distance, inspired by this observation, we define the following dissimilarity.

**Definition 4.1**.: For a diffusion time \(t>0\) and tunable parameter \(\sigma>0\), we define the **heat-geodesic dissimilarity** between \(x_{i},x_{j}\in\bm{X}\) as

\[d_{t}(x_{i},x_{j}):=[-4t\log(\bm{H}_{t})_{ij}-\sigma 4t\log(\bm{V}_{t})_{ij}]^{1/2}\]

where \(\bm{H}_{t}\) is the heat kernel on the graph \(\mathcal{G}\), and \((\bm{V}_{t})_{ij}:=2[(\bm{H}_{t})_{ii}+(\bm{H}_{t})_{jj}]^{-1}\).

Here the \(\log\) is applied elementwise, and the term \(-4t\log(\bm{H}_{t})_{ij}\) corresponds to the geodesic approximation when \(t\to 0\) as in Varadhan's formula. In practice one uses a fixed diffusion time \(t>0\), so we add a symmetric volume correction term as in the Harnack inequality, ensuring that \(d_{t}(x_{i},x_{j})\) is symmetric. From Sec. 2, we have \(h_{t}(x,x)\simeq V(x,\sqrt{t})^{-1}\), and we use the diagonal of \(\bm{H}_{t}\) to approximate the inverse of the volume. With this volume correction term and \(\sigma=1\), the dissimilarity is such that \(d_{t}(x_{i},x_{i})=0\) for all \(t>0\). When \(\sigma=0\) or the manifold has uniform volume growth (as in the constant curvature setting) we show that the heat-geodesic dissimilarity is order preserving:

**Proposition 4.2**.: _When \(\sigma=0\) or the manifold has uniform volume growth, i.e. \((\bm{H}_{t})_{ii}=(\bm{H}_{t})_{jj}\), and the heat kernel is pointwise monotonically decreasing w.r.t. a norm \(|\cdot|\) in ambient space, we have for triples \(x_{i},x_{j},x_{k}\in\bm{X}\) that \(|x_{i}-x_{j}|>|x_{i}-x_{k}|\) implies \(d_{t}(x_{i},x_{j})>d_{t}(x_{i},x_{k})\), i.e. the heat-geodesic dissimilarity is order preserving._

Proof.: When \(\sigma=0\) or the manifold has uniform volume growth we need only consider the \(-4t\log(\bm{H}_{t})_{ij}\) terms. The assumption of pointwise monotonicity of the heat kernel entails that \(|x_{i}-x_{j}|>|x_{i}-x_{k}|\) implies \(\bm{H}_{t}(x_{i},x_{j})<\bm{H}_{t}(x_{i},x_{k})\). We are able to conclude that \(-4t\log\bm{H}_{t}(x_{i},x_{j})>-4t\log\bm{H}_{t}(x_{i},x_{k})\) and thus \(d_{t}(x_{i},x_{j})>d_{t}(x_{i},x_{k})\). 

Denoising distances with triplet computationsWe note that both diffusion maps and PHATE compute a triplet distance between datapoints, i.e., rather than using the direct diffusion probability between datapoints, they use a distance between corresponding rows of a diffusion operator. In particular, diffusion maps uses Euclidean distance, and PHATE uses an M-divergence. Empirically, we notice that this step acts as a denoiser for distances. We formalize this observation in the following proposition. We note \(D_{\text{T}}\) the triplet distance. The triplet distance compares the distances relative to other points. Intuitively, this is a denoising step, since the effect of the noise is spread across the entire set of points. For a reference dissimilarity like the heat-geodesic, it is defined as \(D_{\text{T}}(x_{i},x_{j}):=\|d_{t}(x_{i},\cdot)-d_{t}(x_{j},\cdot)\|_{2}\). For linear perturbations of the form \(d_{t}(x_{i},x_{j})+\epsilon\), where \(\epsilon\in\mathbb{R}\), the effect of \(\epsilon\) on \(D_{\text{T}}(x_{i},x_{j})\) is less severe than on \(d_{t}(x_{i},x_{j})\). Our embedding is based on a linear combination between the heat-geodesic dissimilarity and its triplet distance \((1-\rho)d_{t}+\rho D_{\text{T}}\), where \(\rho\in[0,1]\).

**Proposition 4.3**.: _Denote the perturbed triplet distance by \(\widetilde{D_{\text{T}}}(x_{i},x_{j})=||\tilde{d}_{t}(x_{i},\cdot)-\tilde{d}_ {t}(x_{j},\cdot)||_{2}\) where \(\tilde{d}_{t}(x_{i},x_{j}):=d_{t}(x_{i},x_{j})+\epsilon\) and \(\tilde{d}_{t}(x_{i},x_{k}):=d_{t}(x_{i},x_{k})\) for \(k\neq j\). Then the triplet distance \(D_{\text{T}}\) is robust to perturbations, i.e., for all \(\epsilon>0\),_

\[\left(\frac{\widetilde{D_{\text{T}}}(x_{i},x_{j})}{D_{\text{T}}(x_{i},x_{j})} \right)^{2}\leq\left(\frac{d_{t}(x_{i},x_{j})+\epsilon}{d_{t}(x_{i},x_{j})} \right)^{2}.\]Optimal diffusion timeVaradhan's formula suggests a small value of diffusion time \(t\) to approximate geodesic distance on a manifold. However, in the discrete data setting, geodesics are based on graph constructions, which in turn rely on nearest neighbors. Thus, small \(t\) can lead to disconnected graphs. Additionally, increasing \(t\) can serve as a way of denoising the kernel (which is often computed from noisy data) as it implements a low-pass filter over the eigenvalues, providing the additional advantage of adding noise tolerance. By computing a sequence of heat kernels \((\bm{H}_{t})_{t}\) and evaluating their entropy \(H(\bm{H}_{t}):=-\sum_{ij}(\bm{H}_{t})_{ij}\log(\bm{H}_{t})_{ij}\), we select \(t\) with the knee-point method [26] on the function \(t\mapsto H(\bm{H}_{t})\). We show in Sec. 6.1 that our heuristic for determining the diffusion time automatically leads to better overall results.

Weighted MDSThe loss in MDS (eq.5) is usually defined with uniform weights. Here, we optionally weight the loss by the heat kernel. In Sec. 5, we will show how this modification relates our method to the embedding defined by SNE[11]. For \(x_{i},x_{j}\in\bm{X}\), we minimize \((\bm{H}_{t})_{ij}(d_{t}(x_{i},x_{j})-\|\phi(x_{i})-\phi(x_{j})\|_{2})^{2}\). This promotes geodesic preservation of local neighbors, since more weights are given to points with higher affinities.

Heat-geodesic embeddingTo define a lower dimensional embedding of a point cloud \(\bm{X}\), we construct a matrix from the heat-geodesic dissimilarity, and then use MDS to create the embedding. Our embedding defines a map \(\phi\) that minimizes \(\left(d_{t}(x_{i},x_{j})-\|\phi(x_{i})-\phi(x_{j})\|_{2}\right)^{2}\), for all \(x_{i},x_{j}\in\bm{X}\). Hence, it preserves the heat-geodesic dissimilarity as the loss decreases to zero. In Alg. 1, we present the main steps of our algorithm using the heat-geodesic dissimilarity. A detailed version is presented in Appendix A.

```
1:Input:\(N\times d\) dataset matrix \(\bm{X}\), denoising parameter \(\rho\in[0,1]\), Harnack regularization \(\sigma>0\), output dimension \(k\).
2:Returns:\(N\times k\) embedding matrix \(\bm{E}\).
3:\(\bm{H}_{t}\gets p_{K}(\bm{L},t)\)\(\triangleright\)Heat approximation
4:\(t\leftarrow\text{Kneedle}\{H(\bm{H}_{t})\}_{t}\)\(\triangleright\)Knee detection e.g. [26]
5:\(\bm{D}\leftarrow[-4t\log(\bm{H}_{t})_{ij}-\sigma 4t\log(\bm{V}_{t})_{ij}]^{1/2}\)\(\triangleright\)\(\log\)is applied elementwise
6:\(\bm{D}\leftarrow(1-\rho)\bm{D}+\rho D_{\text{T}}\)\(\triangleright\)Triplet interpolation step
7:Return \(\bm{E}\leftarrow\text{MetricMDS}(\bm{D},\|\cdot\|_{2},k)\) ```

**Algorithm 1** Heat Geodesic Embedding

## 5 Relation to other manifold learning methods

In this section, we elucidate theoretical connections between the Heat Geodesic Embedding and other manifold learning methods. We relate embeddings via the eigenvalues of \(\bm{H}_{t}\) or \(\bm{P}^{t}\) with Laplacian eigenmaps and diffusion maps. We then present the relation between our methods and PHATE and SNE. We provide further analysis in the Appendix A. In particular, we introduce a new definition of kernel preserving embeddings; either via kernel-based distances (diffusion maps, PHATE) or via similarities (e.g. t-SNE, UMAP).

Diffusion maps with the heat kernelDiffusion maps [5] define an embedding with the first \(k\) eigenvectors \((\phi_{i})_{i}\) of \(\bm{P}\), while Laplacian eigenmaps [1] uses the eigenvectors \((\psi_{i})_{i}\) of \(\bm{L}\). In the following, we recall the links between the two methods, and show that a rescaled Laplacian eigenmaps preserves the diffusion distance with the heat kernel \(\bm{H}_{t}\).

**Lemma 5.1**.: _Rescaling the Laplacian eigenmaps embedding with \(x_{i}\mapsto(e^{-2t\lambda_{1}}\psi_{1,i},\ldots,e^{-2t\lambda_{k}}\psi_{k,i})\) preserves the diffusion distance \(DM_{\bm{H}_{t}}\)._

Relation to PHATEThe potential distance in PHATE (Sec. 3) is defined by comparing the transition probabilities of two \(t\)-steps random walks initialized from different vertices. The transition matrix \(\bm{P}^{t}\) mimics the heat propagation on a graph. The heat-geodesic dissimilarity provides a new interpretation of PHATE. In the following proposition, we show how the heat-geodesic relates to the PHATE potential distance with a linear combination of \(t\)-steps random walks.

**Proposition 5.2**.: _The PHATE potential distance with the heat kernel \(PH_{\bm{H}_{t}}\) can be expressed in terms of the heat-geodesic dissimilarity with \(\sigma=0\)_

\[PH_{\bm{H}_{t}}=(1/4t)^{2}\|d_{t}(x_{i},\cdot)-d_{t}(x_{j},\cdot)\|_{2}^{2},\]

_and it is equivalent to a multiscale random walk distance with kernel \(\sum_{k>0}m_{t}(k)\bm{P}^{k}\), where \(m_{t}(k):=t^{k}e^{-t}/k!\)._

Proof.: We present a simplified version of the proof, more details are available in Appendix A. For \(\sigma=0\), we have \(d_{t}(x_{i},x_{j})=-4t\log(\bm{H}_{t})_{ij}\), the relation between the PHATE potential and the heat-geodesic follows from the definition

\[PH_{\bm{H}_{t}}(x_{i},x_{j})=\sum_{k}\big{(}-\log\bm{H}_{t}(x_{i},x_{k})+\log \bm{H}_{t}(x_{j},x_{k})\big{)}^{2}=(1/4t)^{2}\|d_{t}(x_{i},\cdot)-d_{t}(x_{j}, \cdot)\|_{2}^{2}.\]

Using the heat kernel \(\bm{H}_{t}\) with the random walk Laplacian \(\bm{L}_{rw}=\bm{Q}^{-1}\bm{L}=\bm{I}_{n}-\bm{Q}^{-1}\bm{W}\) corresponds to a multiscale random walk kernel. We can write \(\bm{L}_{rw}=\bm{S}\Lambda\bm{S}^{-1}\), where \(\bm{S}:=\bm{Q}^{-1/2}\Psi\). Since \(\bm{P}=\bm{I}_{n}-\bm{L}_{rw}\), we have \(\bm{P}^{t}=\bm{S}(\bm{I}_{n}-\Lambda)^{t}\bm{S}^{-1}\). Interestingly, we can relate the eigenvalues of \(\bm{H}_{t}\) and \(\bm{P}\) with the Poisson distribution. The probability mass function of a Poisson distribution with mean \(t\) is given by \(m_{t}(k):=t^{k}e^{-t}/k!\). For \(t\geq 0\), we have \(e^{-t(1-\mu)}=\sum_{k\geq 0}m_{t}(k)\mu^{k}\). With this relationship, we can express \(\bm{H}_{t}\) as a linear combination of \(\bm{P}^{t}\) weighted by the Poisson distribution. Indeed, substituting \(\lambda=1-\mu\) in yields

\[\bm{H}_{t}=\bm{S}e^{-t\Lambda}\bm{S}^{-1}=\bm{S}\sum_{k=0}^{\infty}m_{t}(k)( \bm{I}_{n}-\Lambda)^{k}\bm{S}^{-1}=\sum_{k=0}^{\infty}m_{t}(k)\bm{P}^{k}.\]

_Remark 5.3_.: In the previous proposition, the same argument holds for the symmetric Laplacian and the affinity matrix \(\bm{A}:=\bm{Q}^{-1/2}\bm{W}\bm{Q}^{-1/2}\) used in other methods such as diffusion maps [5]. This is valid since we can write \(\bm{L}_{sym}=\bm{Q}^{-1/2}\Psi\Lambda\Psi^{T}\bm{Q}^{-1/2},\) and \(\bm{A}=\bm{I}_{n}-\bm{L}_{sym}\).

_Remark 5.4_.: This proposition shows that, as the denoising parameter \(\rho\to 1\), Heat Geodesic Embedding interpolates to the PHATE embeddings with a weighted kernel \(\sum_{k=0}^{\infty}m_{t}(k)\bm{P}^{k}\).

Relation to SNEThe heat-geodesic method also relates to SNE [11], and its variation using the Student distribution t-SNE [18]. In SNE, the similarity between points is encoded via transition probabilities \(p_{ij}\). The objective is to learn an affinity measure \(q\), that depends on the embedding distances \(\|y_{i}-y_{j}\|_{2}\), such that it minimizes \(D_{\mathrm{KL}}(p||q)\). Intuitively, points that have a strong affinity in the ambient space, should also have a strong affinity in the embedded space. Even though the heat-geodesic minimization is directly on the embedding distances, we can show an equivalent with SNE. In Appendix A, we provide additional comparisons between SNE and our method.

**Proposition 5.5**.: _The Heat-Geodesic embedding with \(\sigma=0\) and squared distances minimization weighted by the heat kernel is equivalent to SNE with the heat kernel affinity in the ambient space, and a Gaussian kernel in the embedded space \(q_{ij}=\exp(-\|y_{i}-y_{j}\|^{2}/4t)\)._

## 6 Results

In this section, we show the versatility of our method, showcasing its performance in terms of clustering and preserving the structure of continuous manifolds. We compare the performance of Heat Geodesic Embedding with multiple state-of-the-art baselines on synthetic datasets and real-world datasets. For all models, we perform sample splitting with a 50/50 validation-test split. The validation and test sets each consists of 5 repetitions with different random initializations. The hyper-parameters are selected according to the performance on the validation set. We always report the results on the test set, along with the standard deviations computed over the five repetitions. We use the following methods in our experiments: our _Heat Geodesic Embedding_, _diffusion maps_[5], _PHATE_[22], _shortest-path_ (used in Isomap [31]) which estimates the geodesic distance by computing the shortest path between two nodes in a graph built on the point clouds, _t-SNE_[32], _UMAP_[19], and metric MDS with Euclidean distance. Details about each of these methods, and results for different parameters (graph type, heat approximation, etc.) are given in Appendix C.

### Distance matrix comparison

We start by evaluating the ability of the different distances or dissimilarities to recover the ground truth distance matrix of a point cloud. For this task, we use the Swiss roll and Tree datasets, for which the ground truth geodesic distance is known. The Swiss roll dataset consists of data points sampled on a smooth manifold (see Fig. 1). The Tree dataset is created by connecting multiple high-dimensional Brownian motions in a tree-shape structure. In Fig. 1, we present embeddings of both datasets. Our method recovers the underlying geometry, while other methods create artificial clusters or have too much denoising. Because we aim at a faithful relative distance between data points, we compare the methods according to the Pearson and Spearman correlations of the estimated distance matrices with respect to ground truth. Results are displayed in Tab. 1. We observe that Heat Geodesic Embedding typically outperforms previous methods in terms of the correlation with the ground truth distance matrix, confirming the theoretical guarantees provided in Sec. 4 & 2. Additional results such as computation time and correlation for different noise levels are available in Appendix C.

Optimal diffusion timeIn Section 4, we described a heuristic to automatically choose the diffusion time based on the entropy of \(\bm{H}_{t}\). In Fig. 2, we show that the knee-point of \(t\mapsto H(\bm{H}_{t})\), corresponds to a high correlation with the ground distance, while yielding a low approximation error of the distance matrix (measured by the Frobenius norm of the difference between \(\bm{D}\) and the ground truth).

### Preservation of the inherent data structure

A crucial evaluation criteria of manifold learning methods is the ability to capture the inherent structure of the data. For instance, clusters in the data should be visible in the resulting low dimensional representation. Similarly, when the dataset consists of samples taken at different time points, one expects to be able to characterize this temporal evolution in the low dimensional embedding [22]. We thus compare the different embedding methods according to their ability to retain clusters and temporal evolution of the data.

Identifying clusters.We use the PBMC dataset, the Swiss roll, the Tree dataset, MNIST [8], and COIL-20 [23] dataset. The PBMC dataset consists of single-cell gene expressions from 3000 individual peripheral blood mononuclear cells. Cells are naturally clustered by their cell type. For

\begin{table}
\begin{tabular}{l c c c c} \hline \hline  & \multicolumn{2}{c}{Swiss roll} & \multicolumn{2}{c}{Tree} \\ \hline Distance & Pearson & Spearman & Pearson & Spearman \\ \hline Diffusion distance & \(0.476\pm 0.226\) & \(0.478\pm 0.138\) & \(0.656\pm 0.054\) & \(0.653\pm 0.057\) \\ PHATE potential & \(0.457\pm 0.01\) & \(0.404\pm 0.024\) & \(0.766\pm 0.023\) & \(0.743\pm 0.028\) \\ Shortest path & \(0.497\pm 0.144\) & \(0.558\pm 0.134\) & \(0.780\pm 0.009\) & \(0.757\pm 0.019\) \\ Euclidean & \(0.365\pm 0.006\) & \(0.413\pm 0.005\) & \(0.735\pm 0.014\) & \(0.704\pm 0.033\) \\ Heat-geodesic (ours) & \(\bm{0.702}\pm\bm{0.086}\) & \(\bm{0.700}\pm\bm{0.073}\) & \(\bm{0.822}\pm\bm{0.008}\) & \(\bm{0.807}\pm\bm{0.016}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Pearson and Spearman correlation between the inferred and ground truth distance matrices on the Swiss roll and Tree datasets (higher is better). Best models on average are bolded.

Figure 3: Embeddings of 2000 differentiating cells from embryoid body [22] over 28 days. UMAP and t-SNE do not capture the continuous manifold representing the cells’ evolution.

Figure 2: Evolution of the correlation between estimated and ground truth distance matrices in function of the diffusion time \(t\).

the Tree dataset, we use the branches as clusters. For the Swiss roll dataset, we sample data points on the manifold according to a mixture of Gaussians and use the mixture component as the ground truth cluster labels. The MNIST and COIL-20 datasets are clustered by digits or objects but may not respect the manifold hypothesis. For each method, we run k-means on the two-dimensional embedding and compare the resulting cluster assignments with ground truth. Tab. 10 reports the results in terms of homogeneity and adjusted mutual information (aMI). Heat Geodesic Embedding is competitive with PHATE and outperforms t-SNE and UMAP on all metrics except on the MNIST and COIL-20 datasets. Yet, we show in Appendix C that all methods tend to perform equally well when the noise level increases. In Fig. 4, we present the PBMC embeddings of PHATE and HeatGeo, showing that HeatGeo interpolates to PHATE for \(\rho\to 1\).

Temporal data representation.For this task, we aim at representing data points from population observed at consecutive points in time. We use single cell gene expression datasets collected across different time points, including the Embryoid Body (EB), IPSC [22], and two from the 2022 NeurIPS multimodal single-cell integration challenge (Cite & Multi). To quantitatively evaluate the quality of the continuous embeddings, we first embed the entire dataset and obfuscate all samples from a particular time point (_e.g.,_\(t=2\)). We then estimate the distribution of the missing time point by using displacement interpolation [35] between the adjacent time points (_e.g.,_\(t=1\) and \(t=3\)). We report the Earth Mover Distance (EMD) between the predicted distribution and true distribution. A low EMD suggests that the obfuscated embeddings are naturally located between the previous and later time points, and that the generated embedding captures the temporal evolution of the data adequately. Results are presented in Tab. 3. Heat Geodesic Embedding outperforms other methods on the EB, Multi, and IPSC datasets and is competitive with other approaches on Cite. We show a graphical depiction of the different embeddings for the embryoid (EB) dataset in Fig. 3.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline \multicolumn{2}{c}{} & Swiss roll & \multicolumn{2}{c}{FPMC} & \multicolumn{2}{c}{MNIST (Non Manifold)} & COIL-Non Manifold \\ \hline Method & Honeyplane & AMI & Honeyplane & AMI & Honeyplane & AMI & Honeyplane & AMI & Honeyplane & AMI \\ \hline GMAP & 0.851 \(\pm\) 0.025 & 0.726 \(\pm\) 0.045 & 0.475 \(\pm\) 0.056 & 0.051 \(\pm\) 0.036 & 0.177 \(\pm\) 0.047 & 0.148 \(\pm\) 0.045 & 0.581 \(\pm\) 0.016 & 0.862 \(\pm\) 0.015 & 0.817 \(\pm\) 0.039 & 0.854 \(\pm\) 0.012 \\ i-SNE & 0.852 \(\pm\) 0.025 & 0.726 \(\pm\) 0.028 & 0.058 \(\pm\) 0.014 & 0.071 \(\pm\) 0.015 & 0.059 \(\pm\) 0.049 & 0.544 \(\pm\) 0.022 & 0.088 \(\pm\) 0.063 & **0.087 \(\pm\) 0.04** & **0.088 \(\pm\) 0.02** \\ Huang & 0.856 \(\pm\) 0.009 & 0.712 \(\pm\) 0.005 & 0.027 \(\pm\) 0.014 & 0.021 \(\pm\) 0.020 & 0.224 \(\pm\) 0.027 & 0.217 \(\pm\) 0.017 & 0.072 \(\pm\) 0.017 & 0.741 \(\pm\) 0.012 & 0.367 \(\pm\) 0.02 & 0.027 \\ HuetGeo & 0.859 \(\pm\) 0.003 & 0.701 \(\pm\) 0.003 & 0.028 \(\pm\) 0.002 & 0.021 \(\pm\) 0.021 & 0.028 \(\pm\) 0.023 & 0.028 \(\pm\) 0.021 & 0.028 \(\pm\) 0.021 & 0.026 \(\pm\) 0.011 & 0.366 \(\pm\) 0.027 & 0.027 \\ HuetGeo & 0.859 \(\pm\) 0.004 & 0.859 \(\pm\) 0.003 & 0.028 \(\pm\) 0.002 & 0.021 \(\pm\) 0.021 & 0.028 \(\pm\) 0.023 & 0.028 \(\pm\) 0.021 & 0.028 \(\pm\) 0.021 & 0.028 \(\pm\) 0.022 & 0.021 \(\pm\) 0.024 \\ HeatGeo (ours) & 0.852 \(\pm\) 0.013 & 0.956 \(\pm\) 0.013 & 0.941 \(\pm\) 0.013 & 0.956 \(\pm\) 0.013 & 0.781 \(\pm\) 0.019 & 0.768 \(\pm\) 0.017 & 0.756 \(\pm\) 0.020 & 0.852 \(\pm\) 0.010 & 0.859 \(\pm\) 0.016 & 0.856 \(\pm\) 0.022 \\ \hline \hline \end{tabular}
\end{table}
Table 2: Clustering quality metrics for different methods. We report the homogeneity and the adjusted mutual information (aMI). Best models on average are bolded (higher is better).

Figure 4: Embeddings on PBMC using the triplet distance with the heat-geodesic for different regularization parameter \(\rho\).

\begin{table}
\begin{tabular}{l c c c} \hline \hline Method & Cite & EB & Multi & IPSC \\ \hline Euclidean & 0.978 \(\pm\) 0.069 & 1.012 \(\pm\) 0.039 & 1.212 \(\pm\) 0.199 & 1.085 \(\pm\) 0.234 \\ Isomap & 0.978 \(\pm\) 0.105 & 0.993 \(\pm\) 0.062 & 1.299 \(\pm\) 0.307 & 1.026 \(\pm\) 0.253 \\ Non-metric MDS & 0.81 \(\pm\) 0.012 & 0.85 \(\pm\) 0.014 & 0.806 \(\pm\) 0.015 & 1.013 \(\pm\) 0.067 \\ UMAP & **0.791 \(\pm\) 0.045** & 0.942 \(\pm\) 0.053 & 1.418 \(\pm\) 0.042 & 0.866 \(\pm\) 0.058 \\ t-SNE & 0.905 \(\pm\) 0.034 & 0.964 \(\pm\) 0.032 & 1.208 \(\pm\) 0.087 & 1.006 \(\pm\) 0.026 \\ PHATE & 1.032 \(\pm\) 0.037 & 1.088 \(\pm\) 0.012 & 1.254 \(\pm\) 0.042 & 0.955 \(\pm\) 0.033 \\ Diffusion Maps & 0.989 \(\pm\) 0.080 & 0.965 \(\pm\) 0.077 & 1.227 \(\pm\) 0.086 & 0.821 \(\pm\) 0.039 \\ HeatGeo (ours) & 0.890 \(\pm\) 0.046 & **0.733 \(\pm\) 0.036** & **0.958 \(\pm\) 0.044** & **0.365 \(\pm\) 0.056** \\ \hline \hline \end{tabular}
\end{table}
Table 3: EMD between a linear interpolation of two consecutive time points \(t-1\), \(t+1\), and the time points \(t\). Best models on average are bolded (lower is better).

Conclusion and Limitations

The ability to visualize complex high-dimensional data in an interpretable and rigorous way is a crucial tool of scientific discovery. In this work, we took a step in that direction by proposing a general framework for understanding diffusion-based dimensionality reduction methods through the lens of Riemannian geometry. This allowed us to define a novel embedding based on the heat geodesic dissimilarity--a more direct measure of manifold distance. Theoretically, we showed that our methods brings greater versatility than previous approaches and can help gaining insight into popular manifold learning methods such as diffusion maps, PHATE, and SNE. Experimentally, we demonstrated that it also results in better geodesic distance preservation and excels both at clustering and preserving the structure of a continuous manifold. This contrasts with previous methods that are typically only effective at a single of these tasks.

Despite the strong theoretical and empirical properties, our work presents some limitations. For instance, our method is based on a similarity measure, which is a relaxation of a distance metric. Additionally, the Harnack equation suggests that our parameters for the volume correction could be tuned depending on the underlying manifold. We envision that further analysis of this regularization is a fruitful direction for future work.

## Acknowledgments and Disclosure of Funding

This research was enabled in part by compute resources provided by Mila (mila.quebec). It was partially funded and supported by ESP _Merite_ [G.H.], CIFAR AI Chair [G.W.], NSERC Discovery grant 03267 [G.W.], NIH grants (1F30AI157270-01, R01HD100035, R01GM130847, R01GM135929) [G.W.,S.K.], NSF Career grant 2047856 [S.K.], the Chan-Zuckerberg Initiative grants CZF2019-182702 and CZF2019-002440 [S.K.], the Sloan Fellowship FG-2021-15883 [S.K.], and the Novo Nordisk grant GR112933 [S.K.]. The content provided here is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies. The funders had no role in study design, data collection & analysis, decision to publish, or preparation of the manuscript.

## References

* [1] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. _Neural computation_, 15(6):1373-1396, 2003.
* [2] Jan Niklas Bohm, Philipp Berens, and Dmitry Kobak. Attraction-repulsion spectrum in neighbor embeddings. _Journal of Machine Learning Research_, 23(95):1-32, 2022.
* [3] Erica L Busch, Jessie Huang, Andrew Benz, Tom Wallenstein, Guillaume Lajoie, Guy Wolf, Smita Krishnaswamy, and Nicholas B Turk-Browne. Multi-view manifold learning of human brain-state trajectories. _Nature Computational Science_, pages 1-14, 2023.
* [4] J Douglas Carroll and Phipps Arabie. Multidimensional scaling. _Measurement, judgment and decision making_, pages 179-250, 1998.
* [5] Ronald R. Coifman and Stephane Lafon. Diffusion maps. _Applied and Computational Harmonic Analysis_, 21(1):5-30, 2006.
* [6] Keenan Crane, Clarisse Weischedel, and Max Wardetzky. Geodesics in heat: A new approach to computing distance based on heat flow. _ACM Transactions on Graphics (TOG)_, 32(5):1-11, 2013.
* [7] Michael Defferrard, Lionel Martin, Rodrigo Pena, and Nathanael Perraudin. Pygsp: Graph signal processing in python.
* [8] Li Deng. The mnist database of handwritten digit images for machine learning research. _IEEE Signal Processing Magazine_, 29(6):141-142, 2012.
* [9] Laleh Haghverdi, Maren Buttner, F. Alexander Wolf, Florian Buettner, and Fabian J. Theis. Diffusion pseudoline robustly reconstructs lineage branching. _Nature Methods_, 13(10):845-848, 2016.

* Heitz et al. [2021] Matthieu Heitz, Nicolas Bonneel, David Coeurjolly, Marco Cuturi, and Gabriel Peyre. Ground metric learning on graphs. _Journal of Mathematical Imaging and Vision_, 63:89-107, 2021.
* Hinton and Roweis [2002] Geoffrey E Hinton and Sam Roweis. Stochastic neighbor embedding. _Advances in neural information processing systems_, 15, 2002.
* Hout et al. [2013] Michael C Hout, Megan H Papesh, and Stephen D Goldinger. Multidimensional scaling. _Wiley Interdisciplinary Reviews: Cognitive Science_, 4(1):93-103, 2013.
* Huang et al. [2020] Shih-Gu Huang, Ilwoo Lyu, Anqi Qiu, and Moo K Chung. Fast polynomial approximation of heat kernel convolution on manifolds and its application to brain sulcal and gyral graph pattern analysis. _IEEE transactions on medical imaging_, 39(6):2201-2212, 2020.
* Huguet et al. [2022] Guillaume Huguet, Alexander Tong, Bastian Rieck, Jessie Huang, Manik Kuchroo, Matthew Hirn, Guy Wolf, and Smita Krishnaswamy. Time-inhomogeneous diffusion geometry and topology. _arXiv preprint arXiv:2203.14860_, 2022.
* Huguet et al. [2022] Guillaume Huguet, Alexander Tong, Maria Ramos Zapatero, Guy Wolf, and Smita Krishnaswamy. Geodesic Sinkhorn: optimal transport for high-dimensional datasets. _arXiv preprint arXiv:2211.00805_, 2022.
* Kruskal [1964] Joseph B Kruskal. Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis. _Psychometrika_, 29(1):1-27, 1964.
* 201, 1986.
* Linderman and Steinerberger [2017] George C. Linderman and Stefan Steinerberger. Clustering with t-SNE, provably. _arXiv:1706.02582 [cs, stat]_, 2017.
* McInnes et al. [2018] Leland McInnes, John Healy, and James Melville. Umap: Uniform manifold approximation and projection for dimension reduction. _arXiv preprint arXiv:1802.03426_, 2018.
* Molchanov [1975] Stanislav A Molchanov. Diffusion processes and riemannian geometry. _Russian Mathematical Surveys_, 30(1):1, 1975.
* Moon et al. [2018] Kevin R. Moon, Jay S. Stanley, Daniel Burkhardt, David van Dijk, Guy Wolf, and Smita Krishnaswamy. Manifold learning-based methods for analyzing single-cell RNA-sequencing data. _Current Opinion in Systems Biology_, 7:36-46, 2018.
* Moon et al. [2019] Kevin R. Moon, David van Dijk, Zheng Wang, Scott Gigante, Daniel B. Burkhardt, William S. Chen, Kristina Yim, Antonia van den Elzen, Matthew J. Hirn, Ronald R. Coifman, Natalia B. Ivanova, Guy Wolf, and Smita Krishnaswamy. Visualizing structure and transitions in high-dimensional biological data. _Nat Biotechnol_, 37(12):1482-1492, 2019.
* Nene et al. [1996] Samer A. Nene, Shree K. Nayar, and Hiroshi Murase. Columbia object image library (coil-20). Technical Report CUCS-005-96, Department of Computer Science, Columbia University, February 1996.
* Nowak et al. [2019] Adam Nowak, Peter Sjogren, and Tomasz Z Szarek. Sharp estimates of the spherical heat kernel. _Journal de Mathematiques Pures et Appliquees_, 129:23-33, 2019.
* Saloff-Coste [2010] Laurent Saloff-Coste. The heat kernel and its estimates. _Probabilistic approach to geometry_, 57:405-436, 2010.
* Satopaa et al. [2011] Ville Satopaa, Jeannie Albrecht, David Irwin, and Barath Raghavan. Finding a" kneedle" in a haystack: Detecting knee points in system behavior. In _2011 31st international conference on distributed computing systems workshops_, pages 166-171. IEEE, 2011.
* Sharp et al. [2019] Nicholas Sharp, Yousuf Soliman, and Keenan Crane. The vector heat method. _ACM Transactions on Graphics (TOG)_, 38(3):1-19, 2019.
* Solomon et al. [2015] Justin Solomon, Fernando De Goes, Gabriel Peyre, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao Du, and Leonidas Guibas. Convolutional wasserstein distances: Efficient optimal transportation on geometric domains. _ACM Transactions on Graphics (ToG)_, 34(4):1-11, 2015.

* [29] Jian Sun, Maks Ovsjanikov, and Leonidas Guibas. A concise and provably informative multiscale signature based on heat diffusion. In _Computer graphics forum_, volume 28, pages 1383-1392. Wiley Online Library, 2009.
* [30] Yoshio Takane, Forrest W Young, and Jan De Leeuw. Nonmetric individual differences multidimensional scaling: An alternating least squares method with optimal scaling features. _Psychometrika_, 42:7-67, 1977.
* [31] Joshua B Tenenbaum, Vin de Silva, and John C Langford. A global geometric framework for nonlinear dimensionality reduction. _science_, 290(5500):2319-2323, 2000.
* [32] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. _Journal of machine learning research_, 9(11), 2008.
* [33] David Van Dijk, Roshan Sharma, Juozas Nainys, Kristina Yim, Pooja Kathail, Ambrose J Carr, Cassandra Burdziak, Kevin R Moon, Christine L Chaffer, Diwakar Pattabiraman, et al. Recovering gene interactions from single-cell data using data diffusion. _Cell_, 174(3):716-729, 2018.
* [34] Sathamangalam R Srinivasa Varadhan. On the behavior of the fundamental solution of the heat equation with variable coefficients. _Communications on Pure and Applied Mathematics_, 20(2):431-455, 1967.
* [35] Cedric Villani. Displacement interpolation. _Optimal Transport: Old and New_, pages 113-162, 2009.
* [36] F Alexander Wolf, Philipp Angerer, and Fabian J Theis. Scanpy: large-scale single-cell gene expression data analysis. _Genome biology_, 19:1-5, 2018.
* [37] Jonathan X Zheng, Samraat Pawar, and Dan FM Goodman. Graph drawing by stochastic gradient descent. _IEEE transactions on visualization and computer graphics_, 25(9):2738-2748, 2018.

## Appendix A Theory and algorithm details

### Kernel preserving embeddings

In this section, we attempt to create a generalized framework for dimensionality reduction methods. These methods often have been viewed as disparate or competing but here we show that many of them are related to one another given the right template for methodology comparison. In order to do this, we introduce a general definition suited for distance-preserving dimensionality reduction methods. With this definition, we can cast many dimensionality reduction methods within the same framework, and easily compare them. We recall that the observations in the ambient space are denoted \(x\), and those in the embedded space are denoted \(y\). The definition relies on kernel functions \(H_{t}^{x}\), \(H_{t}^{y}\) defined respectively on the ambient and embedded spaces and on transformations \(T^{x}\), \(T^{y}\) applied to the kernels. We recall that a divergence \(f:\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}^{+}\) is such that \(f(a,b)=0\) if and only if \(a=b\) and \(f(a,a+\delta)\) is a positive semi-definite quadratic form for infinitesimal \(\delta\).

**Definition A.1**.: We define a **kernel features preserving embedding** as an embedding which minimizes a loss \(L\) between a transformation \(T^{x}\) of the ambient space kernel \(H_{t}^{x}\) and its embedded space counterpart

\[L:=f(T^{x}(H_{t}^{x}),T^{y}(H_{t^{\prime}}^{y})),\] (6)

where \(f\) is any \(C^{2}\) divergence on \(\mathbb{R}_{\geq 0}\).

_Example 1_.: We formulate MDS as a kernel feature-preserving embedding. Suppose we want to preserve the Euclidean distance, we have \(H_{t}^{x}(x_{i},x_{j})=\|x_{i}-x_{j}\|_{2}\), \(H_{t}^{y}(y_{i},y_{j})=\|y_{i}-y_{j}\|_{2}\), \(f(a,b)=\|a-b\|_{2}\), and \(T^{x}=T^{y}=I\).

In the following, we present popular dimensionality reduction methods that are kernel features preserving embeddings. With this definition, we can distinguish between methods that a preserve a kernel via affinities or distances. For the methods considered in this work, \(H_{t}^{x}\) is an affinity kernel, but its construction varies from one method to another. In PHATE and Diffusion maps, \(H_{t}^{x}\) is a random walk \(\bm{P}\), while in Heat Geodesic Embedding we use the heat kernel \(\bm{H}_{t}\). t-SNE defines \(H_{t}^{x}\) as a symmetrized random walk matrix from a Gaussian kernel, while UMAP uses an unnormalized version. Methods such as PHATE and diffusion maps define a new distance matrix from a kernel in the ambient space and preserve these distances in the embedded space. Other methods like t-SNE and UMAP define similarities from a kernel and aim to preserve these similarities in the ambient space and embedded space via an entropy-based loss. We note the Kullback-Leibler divergence \(D_{\mathrm{KL}}(a,b)=\sum_{ij}a_{ij}\log[a_{ij}/b_{ij}]\).

**Proposition A.2**.: _The embeddings methods HeatGeo, PHATE, Diffusion Maps, SNE, t-SNE, and UMAP are kernel feature-preserving embeddings._

Proof.: We assume that the affinity kernel in the ambient space \(H_{t}^{x}\), is given, to complete the proof we need to define \(f,H_{t}^{y},T^{x},T^{y}\) for all methods.

We start with the distance preserving embeddings; HeatGeo, PHATE, and Diffusion Maps. For these methods, the kernel in the embed space is simply \(H_{t}^{y}(y_{i},y_{j})=\|y_{i}-y_{j}\|_{2}\), without transformation, i.e. \(T^{y}=I\). Since they preserve a distance, the loss is \(f(T^{x}(H_{t}^{x}),T^{y}(H_{t^{\prime}}^{y}))=\|H_{t}^{x}-H_{t^{\prime}}^{y}\| _{2}\).

In the Heat Geodesic Embedding we apply a transformation on \(H_{t}^{x}=\bm{H}_{t}\) to define a dissimilarity, hence \(T^{x}(H_{t}^{x})=-t\log H_{t}^{x}\) (for \(\sigma=0\)), where \(\log\) is applied elementwise.

In PHATE, the potential distance is equivalent to \((T^{x}(H_{t}^{x}))_{ij}=\|-\log(H_{t}^{x})_{i}+\log(H_{t}^{x})_{j}\|_{2}\). In Diffusion Maps, the diffusion distance is \((T^{x}(H_{t}^{x}))_{ij}=\|(H_{t}^{x})_{i}-(H_{t}^{x})_{j}\|_{2}\).

SNE, t-SNE, and UMAP preserve affinities from a kernel. For these three methods, the loss is a divergence between distributions, namely \(f=D_{\mathrm{KL}}\). They vary by defining different affinity kernel and transformation in the embedded space. SNE uses the unnormalized kernel \(H_{t}^{y}(y_{i},y_{j})=\exp(-(1/t)\|y_{i}-y_{j}\|_{2}^{2})\), with \(T^{x}=T^{y}=I\). Whereas, t-SNE uses \((H_{1}^{y})_{ij}=(1+\|y_{i}-y_{j}\|^{2})^{-1}\)and \(T^{x}=T^{y}=I\). UMAP define a pointwise transformation in the embedded space with \((H_{1}^{y})_{ij}=(1+\|y_{i}-y_{j}\|^{2})^{-1}\), \((T^{y}(H_{t}^{y}))_{ij}=(H_{1}^{y})_{ij}/(1-(H_{1}^{y})_{ij})\), and \(T^{x}=I\).

We summarize the choice of kernels and functions in Tab. 4 

### Proofs

**Proposition 4.3**.: _Denote the perturbed triplet distance by \(\widetilde{D_{\mathrm{T}}}(x_{i},x_{j})=||\tilde{d}_{t}(x_{i},\cdot)-\tilde{d} _{t}(x_{j},\cdot)||_{2}\) where \(\tilde{d}_{t}(x_{i},x_{j}):=d_{t}(x_{i},x_{j})+\epsilon\) and \(\tilde{d}_{t}(x_{i},x_{k}):=d_{t}(x_{i},x_{k})\) for \(k\neq j\). Then the triplet distance \(D_{\mathrm{T}}\) is robust to perturbations, i.e., for all \(\epsilon>0\),_

\[\left(\frac{\widetilde{D_{\mathrm{T}}}(x_{i},x_{j})}{\widetilde{D_{\mathrm{T }}}(x_{i},x_{j})}\right)^{2}\leq\left(\frac{d_{t}(x_{i},x_{j})+\epsilon}{d_{t} (x_{i},x_{j})}\right)^{2}.\]

Proof of Proposition 4.3.: The effect of the noise on the square distance is \((d_{t}(x_{i},x_{j})+\epsilon)^{2}/d(x_{i},x_{j})^{2}=1+(2\epsilon d_{t}(x_{i},x_{j})+\epsilon^{2})/d(x_{i},x_{j})^{2}\). Denoting the perturbed triplet distance by \(\widetilde{D_{\mathrm{T}}}\), we have

\[\frac{\widetilde{D_{\mathrm{T}}}(x_{i},x_{j})^{2}}{D_{\mathrm{T}}(x_{i},x_{j} )^{2}}=\frac{\sum_{k\neq i,j}\left(d_{t}(x_{i},x_{k})-d_{t}(x_{j},x_{k})\right) ^{2}+2(d_{t}(x_{i},x_{j})+\epsilon)^{2}}{D_{\mathrm{T}}(x_{i},x_{j})^{2}}=1+ \frac{4\epsilon d(x_{i},x_{j})+2\epsilon^{2}}{D_{\mathrm{T}}(x_{i},x_{j})^{2}},\]

and we have

\[\frac{4\epsilon d(x_{i},x_{j})+2\epsilon^{2}}{D_{T}(x_{i},x_{j})^{2}}\leq \frac{2\epsilon d_{t}(x_{i},x_{j})+\epsilon^{2}}{d_{t}(x_{i},x_{j})^{2}}\]

For \(\epsilon>0\), this gives

\[\epsilon\geq\frac{4d_{t}(x_{i},x_{j})^{3}-2d_{t}(x_{i},x_{j})D_{T}(x_{i},x_{j })^{2}}{D_{t}(x_{i},x_{j})^{2}-2d_{t}(x_{i},x_{j})^{2}}=-2d_{t}(x_{i},x_{j}).\]

For \(\epsilon<0\), we have

\[\epsilon\leq\frac{4d_{t}(x_{i},x_{j})^{3}-2d_{t}(x_{i},x_{j})D_{T}(x_{i},x_{j })^{2}}{D_{t}(x_{i},x_{j})^{2}-2d_{t}(x_{i},x_{j})^{2}}=-2d_{t}(x_{i},x_{j}).\]

Thus \(\epsilon\in(-\infty,-2d_{t}(x_{i},x_{j}))\cup(0,\infty)\). As we require the perturbation factor \(\epsilon<<d_{t}(x_{i},x_{j})\), hence we choose \(\epsilon\in(0,\infty)\).

**Lemma 5.1**.: _Rescaling the Laplacian eigenmaps embedding with \(x_{i}\mapsto(e^{-2t\lambda_{1}}\psi_{1,i},\ldots,e^{-2t\lambda_{k}}\psi_{k,i})\) preserves the diffusion distance \(DM_{\bm{H}_{\mathrm{t}}}\)._

Proof of Lemma 5.1.: Since the eigendecompotision of \(\bm{H}_{t}\) form an orthonormal basis of \(\mathbb{R}^{n}\), and since its first eigenvector is constant, we can write the diffusion distance \(||\bm{\delta}_{i}\bm{H}_{t}-\bm{\delta}_{i}\bm{H}_{t}||_{2}^{2}=\sum_{k\geq 0}e^{ -2t\lambda_{k}}(\psi_{ki}-\psi_{kj})^{2}=\sum_{k\geq 1}e^{-2t\lambda_{k}}(\psi_{ki}- \psi_{kj})^{2}\). In particular, this defines the \(k\) dimensional embedding \(x\mapsto(e^{-t\lambda_{1}}\psi_{1}(x),\ldots,e^{-t\lambda_{k}}\psi_{k}(x))\)

\begin{table}
\begin{tabular}{l l l l l} \hline \hline Method & \(H_{t}^{y}(y_{i},y_{j})\) & \(T^{x}(H_{t}^{x})\) & \(T^{y}(H_{t}^{y})\) & \(f\) \\ \hline PHATE & \(\|y_{i}-y_{j}\|_{2}\) & \(\|-\log(H_{t}^{x})_{i}+\log(H_{t}^{x})_{j}\|_{2}\) & \(H_{t}^{y}\) & \(\|\cdot\|_{2}\) \\ Heat Geodesic & \(\|y_{i}-y_{j}\|_{2}\) & \(-t\log(H_{t}^{x})_{ij}\) & \(H_{t}^{y}\) & \(\|\cdot\|_{2}\) \\ Diffusion Maps & \(\|y_{i}-y_{j}\|_{2}\) & \(\|(H_{t}^{x})_{i}-(H_{t}^{x})_{j}\|_{2}\) & \(H_{t}^{y}\) & \(\|\cdot\|_{2}\) \\ SNE & \(\exp(-(\frac{1}{\epsilon})\|y_{i}-y_{j}\|_{2}^{2})\) & \(H_{t}^{x}\) & \(H_{t}^{y}\) & \(D_{\mathrm{KL}}\) \\ t-SNE & \((1+\|y_{i}-y_{j}\|^{2})^{-1}\) & \(H_{t}^{x}\) & \(H_{t}^{y}\) & \(D_{\mathrm{KL}}\) \\ UMAP & \((1+\|y_{i}-y_{j}\|^{2})^{-1}\) & \(H_{t}^{x}\) & \(\frac{(H_{t}^{y})_{ij}}{(1-(H_{t}^{y})_{ij})}\) & \(D_{\mathrm{KL}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: Overview of kernel preserving methods.

**Proposition 5.2**.: _The PHATE potential distance with the heat kernel \(PH_{\bm{H}_{t}}\) can be expressed in terms of the heat-geodesic dissimilarity with \(\sigma=0\)_

\[PH_{\bm{H}_{t}}=(1/4t)^{2}\|d_{t}(x_{i},\cdot)-d_{t}(x_{j},\cdot)\|_{2}^{2},\]

_and it is equivalent to a multiscale random walk distance with kernel \(\sum_{k>0}m_{t}(k)\bm{P}^{k}\), where \(m_{t}(k):=t^{k}e^{-t}/k!\)._

Proof of Proposition 5.2.: For \(\sigma=0\), we have \(d_{t}(x_{i},x_{j})=-4t\log(\bm{H}_{t})_{ij}\), the relation between the PHATE potential and the heat-geodesic follows from the definition

\[PH_{\bm{H}_{t}} =\sum_{k}\big{(}-\log\bm{H}_{t}(x_{i},x_{k})+\log\bm{H}_{t}(x_{j},x_{k})\big{)}^{2}\] \[=(1/4t)^{2}\|d_{t}(x_{i},\cdot)-d_{t}(x_{j},\cdot)\|_{2}^{2}.\]

Using the heat kernel \(\bm{H}_{t}\) with the random walk Laplacian \(\bm{L}_{rw}=\bm{Q}^{-1}\bm{L}=\bm{I}_{n}-\bm{Q}^{-1}\bm{W}\) corresponds to a multiscale random walk kernel. Recall that we can write \(\bm{L}_{rw}\) in terms of the symmetric Laplacian \(\bm{L}_{rw}=\bm{Q}^{-1/2}\bm{L}_{s}\bm{Q}^{1/2}\), meaning that the two matrices are similar, hence admit the same eigenvalues \(\Lambda\). We also know that \(\bm{L}_{s}\) is diagonalizable, since we can write \(\bm{L}_{s}=\bm{Q}^{-1/2}\bm{L}\bm{Q}^{-1/2}=\bm{Q}^{-1/2}\Psi\Lambda\Psi^{T} \bm{Q}^{-1/2}\). In particular, we have \(\bm{L}_{rw}=\bm{S}\Lambda\bm{S}^{-1}\), where \(\bm{S}:=\bm{Q}^{-1/2}\Psi\). The random walk matrix can be written as \(\bm{P}=\bm{I}_{n}-\bm{R}_{rw}\), hence its eigenvalues are \((\bm{I}_{n}-\Lambda)\), and we can write \(\bm{P}^{t}=\bm{S}(\bm{I}_{n}-\Lambda)^{t}\bm{S}^{-1}\). Similarly, the heat kernel with the random walk Laplacian can be written as \(\bm{H}_{t}=\bm{S}e^{-t\Lambda}\bm{S}^{-1}\). Interestingly, we can relate the eigenvalues of \(\bm{H}_{t}\) and \(\bm{P}\) with the Poisson distribution. Note the probability mass function of a Poisson as \(m_{t}(k):=t^{k}e^{-t}/k!\), for \(t\geq 0\), we have

\[e^{-t(1-\mu)}=e^{-t}\sum_{k\geq 0}\frac{(t\mu)^{k}}{k!}=\sum_{k\geq 0}m_{t}(k) \mu^{k}.\] (7)

We note that (7) is the probability generating function of a Poisson distribution with parameter \(t\), i.e. \(\mathbb{E}[\mu^{X}]\), where \(X\sim\text{Poisson}(t)\). With this relationship, we can express \(\bm{H}_{t}\) as a linear combination of \(\bm{P}^{t}\) weighted by the Poisson distribution. Indeed, substituting \(\lambda=1-\mu\) in (7) links the eigenvalues of \(\bm{H}_{t}\) and \(\bm{P}\). We write the heat kernel as a linear combination of random walks weighted by the Poisson distribution, we have

\[\bm{H}_{t}=\bm{S}e^{-t\Lambda}\bm{S}^{-1}=\bm{S}\sum_{k=0}^{\infty}m_{t}(k)( \bm{I}_{n}-\Lambda)^{k}\bm{S}^{-1}=\sum_{k=0}^{\infty}m_{t}(k)\bm{P}^{k}.\]

**Proposition 5.5**.: _The Heat-Geodesic embedding with \(\sigma=0\) and squared distances minimization weighted by the heat kernel is equivalent to SNE with the heat kernel affinity in the ambient space, and a Gaussian kernel in the embedded space \(q_{ij}=\exp(-\|y_{i}-y_{j}\|^{2}/4t)\)._

Proof of Proposition 5.5.: The squared MDS weighted by the heat kernel corresponds to

\[\sum_{ij}h_{t}(x_{i},x_{j})(d_{ij}^{2}-\|y_{i}-y_{j}\|^{2})^{2} =\sum_{ij}h_{t}(x_{i},x_{j})(-t\log h_{t}(x_{i},x_{j})-\|y_{i}-y_ {j}\|^{2})^{2}\] \[=\sum_{ij}h_{t}(x_{i},x_{j})t^{2}(\log h_{t}(x_{i},x_{j})-\log \exp(-\|y_{i}-y_{j}\|^{2}/t)^{2}.\]

If there exists an embedding that attain a zero loss, then it is the same as \(\sum_{ij}h_{t}(x_{i},x_{j})(\log h_{t}(x_{i},x_{j})-\log\exp(-\|y_{i}-y_{j}\|^ {2}/t)=D_{\mathrm{KL}}(h_{t}||q)\). 

### Algorithm details

We present a detailed version of the Heat Geodesic Embedding algorithm in Alg.2.

For the knee-point detection we use the Kneedle algorithm [26]. It identifies a knee-point as a point where the curvature decreases maximally between points (using finite differences). We summarize the four main steps of the algorithm for a function \(f(x)\), and we refer to [26] for additional details.

1. Smoothing with a spline to preserve the shape of the function.
2. Normalize the values, so the algorithm does not depend on the magnitude of the observations.
3. Computing the set of finite differences for \(x\) and \(y:=f(x)\), e.g. \(y_{d_{i}}:=f(x_{i})-x_{i}\).
4. Evaluating local maxima of the difference curve \(y_{d_{i}}\), and select the knee-point using a threshold based on the average difference between consecutive \(x\).

## Appendix B Experiments and datasets details

Our experiments compare our approach with multiple state-of-the-art baselines for synthetic datasets (for which the true geodesic distance is known) and real-world datasets. For all models, we perform sample splitting with a 50/50 validation-test split. The validation and test sets each consist of 5 repetitions with different random initializations. The hyper-parameters are selected according to the performance on the validation set. We always report the results on the test set, along with the standard deviations computed over the five repetitions. We use the following state-of-the-art methods in our experiments: our Heat Geodesic Embedding, _diffusion maps[5]_, _PHATE_[22], _Heat-PHATE_ (a variation of PHATE using the Heat Kernel), _Rand-Geo_ (a variation of Heat Geodesic Embedding where we use the random walk kernel), _Shortest-path_ which estimates the geodesic distance by computing the shortest path between two nodes in a graph built on the point clouds, _t-SNE[32]_, and _UMAP_[19].

### Datasets

We consider two synthetic datasets, the well known Swiss roll and the tree datasets. The exact geodesic distance can be computed for these datasets. We additionally consider real-world datasets: PBMC, IPSC [22], EB [22], and two from the from the 2022 NeurIPS multimodal single-cell integration challenge4.

Footnote 4: https://www.kaggle.com/competitions/open-problems-multimodal/

#### b.1.1 Swiss Roll

The Swiss roll dataset consists of data points samples on a smooth manifold inspired by shape of the famous alpine pastry. In its simplest form, it is a 2-dimensional surface embedded in \(\mathbb{R}^{3}\) given by

\[x =t\cdot cos(t)\] \[y =h\] \[z =t\cdot sin(t)\]where \(t\in[T_{0},T_{1}]\) and \(h\in[0,W]\). In our experiments we used \(T_{0}=\frac{3}{2}\pi\), \(T_{1}=\frac{9}{2}\pi\), and \(W=5\). We use two sampling mechanisms for generating the data points : uniformly and clustered. In the first, we sample points uniformly at random in the \([T_{0},T_{1}]\times[0,W]\) plane. In the second, we sample according to a mixture of isotropic multivariate Gaussian distributions in the same plane with equal weights, means \([(7,W/2),(12,W/2)]\), and standard deviations \([1,1]\). In the clustered case, data samples are given a label \(y\) according to the Gaussian mixture component from which they were sampled.

We consider variations of the Swiss roll by projecting the data samples in higher dimension using a random rotation matrix sampled from the Haar distribution. We use three different ambient dimensions: 3, 10, and 50.

Finally, we add isotropic Gaussian noise to the data points in the ambient space with a standard deviation \(\sigma\).

#### b.1.2 Tree

The tree dataset is created by generating \(K\) branches from a \(D\)-dimensional Brownian motion that are eventually glued together. Each branch is sampled from a multidimensional Brownian motion \(d\mathbf{X_{k}}=2d\mathbf{W}(t)\) at times \(t=0,1,2,...,L-1\) for \(k\in[K]\). The first branch is taken as the main branch and the remaining branches are glued to the main branch by setting \(X_{k}=X_{k}+X_{0}[i_{k}]\) where \(i_{k}\) is a random index of the main branch vector. The total number of samples is thus \(L\cdot K\)

In our experiments, we used \(L=500\), \(K=5\), and \(D=5,10\) (_i.e.,_ two versions with different dimensions of the ambient space).

### Evaluation Metrics

We compare the performance of the different methods according to several metrics. For synthetic datasets, where ground truth geodesic distance is available, we directly compare the estimated distance matrices and ground truth geodesic distance matrices. For real-world datasets, we use clustering quality and continuous interpolation as evaluation metrics.

#### b.2.1 Distance matrix evaluation

The following methods use an explicit distance matrix: diffusion maps, Heat Geodesic Embedding, Heat-Phate, Phate, Rand-Geo and Shortest Path. For these methods, we compare their ability their ability to recover the ground truth distance matrix several metrics. Letting \(D\) and \(\hat{D}\) the ground truth and inferred distance matrices respectively, and \(N\) the number of points in the dataset, we use the following metrics.

Pearson\(\rho\)We compute the average Pearson correlation between the rows of the distance matrices, \(\frac{1}{N}\sum_{i=1}^{N}r_{D_{i},\hat{D}_{i}}\), where \(r_{x,y}\) is the Pearson correlation coefficient between vectors \(x\) and \(y\). \(D_{i}\) stands for the \(i\)-th row of \(D\).

Spearman\(\rho\)We compute the average Spearman correlation between the rows of the distance matrices, \(\frac{1}{N}\sum_{i=1}^{N}r_{D_{i},\hat{D}_{i}}\), where \(r_{x,y}\) is the Spearman correlation coefficient between vectors \(x\) and \(y\). \(D_{i}\) stands for the \(i\)-th row of \(D\).

Frobenius NormWe use \(\|D-\hat{D}\|_{F}\), where \(\|A\|_{F}=\sqrt{\sum_{i=1}^{N}\sum_{j=1}^{N}|A_{i,j}|^{2}}\)

Maximum NormWe use \(\|D-\hat{D}\|_{\infty}\), where \(\|A\|_{\infty}=max_{i,j}|A_{i,j}|\)

#### b.2.2 Embedding evaluation

Some methods produce low-dimensional embeddings without using an explicit distance matrix for the data points. This is the case for UMAP and t-SNE. To compare against these methods, we use the distance matrix obtained by considering Euclidean distance between the low-dimensional embeddings. We used 2-dimensional embeddings in our experiments. For diffusion maps, we obtain these embeddings by using the first two eigenvectors of the diffusion operator only. For Heat Geodesic Embedding, Heat-PHATE, PHATE, Rand-GEO and Shortest Path, we use multidimensional scaling (MDS) on the originally inferred distance matrix.

ClusteringWe evaluate the ability of Heat Geodesic Embedding to create meaningful embeddings when clusters are present in the data. To this end, we run a k-means clustering on the two dimensional embeddings obtained with each method and compare them against the ground truth labels. For the Tree dataset, we use the branches as clusters. For the Swiss roll dataset, we sample data points on the manifold according to a mixture of Gaussians and use the mixture component as the ground truth cluster label.

InterpolationTo quantitatively evaluate the quality of the continuous embeddings, we first embed the entire dataset and obfuscate all samples from a particular time point (_e.g.,_\(t=2\)). We then estimate the distribution of the missing time point by using displacement interpolation [35] between the adjacent time points (_e.g.,_\(t=1\) and \(t=3\)). We report the Earth Mover Distance (EMD) between the predicted distribution and true distribution. A low EMD suggests that the obfuscated embeddings are naturally located between the previous and later time points, and that the generated embedding captures the temporal evolution of the data adequately.

### Hyperparameters

In Table 5, we report the values of hyperparameters used to compute the different embeddings.

### Hardware

The experiments were performed on a compute node with 16 Intel Xeon Platinum 8358 Processors and 64GB RAM.

\begin{table}
\begin{tabular}{l c c} \hline \hline Hyperparameter & Description & Values \\ \hline \multicolumn{3}{c}{Heat Geodesic Embedding} \\ \hline k & Number of neighbours in k-NN graph & 5,10,15 \\ order & order of the approximation & 30 \\ \(t\) & Diffusion time & 0.1,1,10,50,auto \\ Approximation method & Approximation method for Heat Kernel & Euler, Chebyshev \\ Laplacian & Type of laplacian & Combinatorial \\ Harnack \(\rho\) & Harnack Regularization & 0,0.25,0.5,0.75,1,1.5 \\ \hline \multicolumn{3}{c}{PHATE} \\ \hline n-PCA & Number of PCA components & 50,100 \\ \(t\) & Diffusion time & 1,5,10,20,auto \\ \(k\) & Number of neighbours & 10 \\ \hline \multicolumn{3}{c}{Diffusion Maps} \\ \hline k & Number of neighbours in k-NN graph & 5,10,15 \\ \(t\) & Diffusion time & 1,5,10,20 \\ \hline \multicolumn{3}{c}{Shortest Path} \\ \hline k & Number of neighbours in k-NN graph & 5,10,15 \\ \hline \multicolumn{3}{c}{UMAP} \\ \hline k & Number of neighbours & 5,10,15 \\ min-dist & Minimum distance & 0.1,0.5,0.99 \\ \hline \multicolumn{3}{c}{t-SNE} \\ \hline p & Perplexity & 10,30,100 \\ early exaggeration & Early exaggeration parameter & 12 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hyperparameters used in our experimentsAdditional results

### HeatGeo weighted

Following Sec. 5, we know that weighting the MDS loss by the heat kernel corresponds to a specific parametrization of SNE, and thus promote the identification of cluster. In Fig. 5, we show the embeddings of four Gaussian distributions in 10 dimensions (top), and the PBMC dataset (bottom). The reference embedding is using t-SNE, as it models as it also minimizes the KL between the ambient and embedded distributions. We see that HeatGeo weighted form cluster that are shaped like a Gaussian. This is expected as Prop. 5.5, indicates that this is equivalent to minimizing the \(D_{\mathrm{KL}}\) between the heat kernel and a Gaussian affinity kernel.

### Truncated distance

In Fig.6, we discretize the interval \([0,51]\) in 51 nodes, and we compute the heat-geodesic distance of the midpoint with respect to the other points, effectively approximating the Euclidean distance. Using Chebyshev polynomials of degree of 20, we see that the impact of the truncation is greater as the diffusion time increases. The backward Euler methods does not result in a truncated distance.

Figure 5: Embeddings of four Gaussian distributions in 10 dimensions (top), and the PBMC dataset (bottom). HeatGeo with weight is equivalent to minimizing the \(D_{\mathrm{KL}}\) between the heat kernel and a Gaussian affinity kernel, hence produces clusters shaped similar to a Gaussian.

Figure 6: Approximation of the squared Euclidean distance with the Heat-geodesic for the exact computation, Backward Euler approximation, and Chebyshev polynomials. For larger diffusion time, the Chebyshev approximation results in a thresholded distance. The Harnack regularization unsures \(d_{t}(x,x)=0\).

### Harnack inequality

For complete Riemannian manifolds that satisfy the parabolic Harnack inequality (PHI) we have \(h_{t}(x,y)\simeq V^{-1}(x,\sqrt{t})\ e^{-d(x,y)^{2}/t}\) so that \(-t\log h_{t}(x,y)\simeq t\log V(x,\sqrt{t})+d^{2}(x,y)\)[25].

\[h_{t}(x,x)=\frac{1}{V(x,\sqrt{t})}\] (8) \[V(x,\sqrt{t})=h_{t}(x,x)^{-1}\] (9)

We then have,

\[d^{2}(x,y)\simeq-t\log h_{t}(x,y)-t\log V(x,\sqrt{t})\] \[d^{2}(x,y)\simeq-t\log h_{t}(x,y)-t\log h_{t}(x,x)^{-1}\] \[d^{2}(x,y)\simeq-t\log h_{t}(x,y)+t\log h_{t}(x,x)\]

#### c.3.1 Case studies for specific manifolds

The circle - \(\mathbb{S}_{1}\)We now show that our expression for the Heat Geodesic Embedding-distance is monotonically increasing with respect to the ground truth geodesic distance \(d\in\mathbb{R}^{+}\) for a fixed diffusion time \(t\) and for any Harnack regularization in \(\mathbb{S}_{1}\).

Figure 7: Impact of the Chebyshev approximation order on the embedding of HeatGeo for the PBMC dataset.

Our expression for the Heat Geodesic Embedding-distance is

\[\hat{d}=\sqrt{-4t\log(h_{t}(d))+4t\log(h_{t}(0))}\]

As the square-root is monotonic, and \(4t\log h_{t}(0)\) is constant with respect to \(d\), we need to show that \(f(d)=-log(h_{t}(d))\) is monotonically increasing.

For \(\mathbb{S}_{1}\), we have

\[h_{t}(d)=\sum_{m\in\mathbb{Z}}\frac{1}{\sqrt{4\pi t}}e^{-\frac{(d+2\pi m)^{2}} {4t}}\]

As log is monotonically increasing, it suffices to show that \(\sum_{m\in\mathbb{Z}}e^{-\frac{(d+2\pi m)^{2}}{4t}}\) is monotonically _decreasing_, which is the case as for any \(d^{\prime}>d\), \(\forall m\in\mathbb{Z}\), we have

\[e^{-\frac{(d+2\pi m)^{2}}{4t}}>e^{-\frac{(d^{\prime}+2\pi m)^{2}}{4t}}.\]

In general, one can see that (1) the heat kernel depending only on the geodesic distance and (2) the heat kernel being monotonically decreasing with respect to the geodesic distance are sufficient conditions for preserving ordering of pair-wise distances with Heat Geodesic Embedding.

The sphere - \(\mathbb{S}_{n}\)The above result can be applied to the higher-dimensional sphere \(\mathbb{S}_{n}\). It is known that the heat kernel on manifold of constant curvatures is a function of the the geodesic distance (\(d\)) and time only. For \(\mathbb{S}_{n}\) the heat kernel is given by

\[h_{t}(x,y)=\sum_{l=0}^{\infty}e^{-l(l+n)-2t}\frac{2l+n-2}{n-2}C_{l}^{\frac{n} {2}-1}(cos(d))\]

with \(I\) the regularized incomplete beta function and \(C\) the Gegenbauer polynomials.

Furthermore, Nowak et al. [24] showed that the heat kernel of the sphere is monotonically decreasing. The distance inferred from Heat Geodesic Embedding thus preserves ordering of the pair-wise distances.

Euclidean (\(\mathbb{R}^{3}\))For the euclidean space, we have for the volume of \(\sqrt{t}\)-geodesic ball and for the heat kernel:

\[V_{\sqrt{t}}=\frac{4}{3}\pi t^{3/2}\] \[h_{t}(x,y)=\frac{1}{(4\pi t)^{3/2}}e^{-\frac{e^{2}}{4t}}.\]

Recalling Harnack inequality,

\[\frac{c_{1}}{V(x,\sqrt{t})}e^{-\frac{d(x,y)^{2}}{c_{2}t}}\leq h_{t}(x,y)\leq \frac{c_{3}}{V(x,\sqrt{t})}e^{-\frac{d(x,y)^{2}}{c_{4}t}}\]

With \(c_{2}=c_{4}=4\), we have 

[MISSING_PAGE_EMPTY:22]

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline data & Noise level & Method & PearsonR & SpearmanR & Norm Fro N2 & Norm inf N2 \\ \hline Swiss roll & 0.1 & Diffusion Map & \(0.974\pm 0.01\) & \(0.983\pm 0.007\) & \(0.018\pm 0.0\) & \(0.026\pm 0.0\) \\ Swiss roll & 0.1 & Heat-Geo & \(0.992\pm 0.003\) & \(0.995\pm 0.002\) & \(0.002\pm 0.0\) & \(0.003\pm 0.0\) \\ Swiss roll & 0.1 & Heat-PHATE & \(0.99\pm 0.002\) & \(0.997\pm 0.001\) & \(0.079\pm 0.002\) & \(0.1\pm 0.003\) \\ Swiss roll & 0.1 & PHATE & \(0.621\pm 0.006\) & \(0.58\pm 0.01\) & \(0.022\pm 0.0\) & \(0.026\pm 0.0\) \\ Swiss roll & 0.1 & Rand-Geo & \(0.956\pm 0.003\) & \(0.993\pm 0.001\) & \(0.009\pm 0.0\) & \(0.012\pm 0.0\) \\ Swiss roll & 0.1 & Shortest Path & \(\mathbf{1.0\pm 0.0}\) & \(\mathbf{1.0\pm 0.0}\) & \(\mathbf{0.0\pm 0.0}\) & \(\mathbf{0.001\pm 0.0}\) \\ Swiss roll & 0.1 & Euclidean & \(0.379\pm 0.003\) & \(0.424\pm 0.003\) & \(0.014\pm 0.0\) & \(0.018\pm 0.0\) \\ \hline Swiss roll & 0.5 & Diffusion Map & \(0.982\pm 0.003\) & \(0.987\pm 0.002\) & \(0.018\pm 0.0\) & \(0.026\pm 0.0\) \\ Swiss roll & 0.5 & Heat-Geo & \(0.994\pm 0.002\) & \(0.996\pm 0.001\) & \(0.002\pm 0.0\) & \(0.004\pm 0.0\) \\ Swiss roll & 0.5 & Heat-PHATE & \(0.993\pm 0.001\) & \(0.998\pm 0.0\) & \(0.064\pm 0.001\) & \(0.083\pm 0.002\) \\ Swiss roll & 0.5 & PHATE & \(0.649\pm 0.007\) & \(0.615\pm 0.006\) & \(0.023\pm 0.0\) & \(0.028\pm 0.0\) \\ Swiss roll & 0.5 & Rand-Geo & \(0.969\pm 0.002\) & \(0.995\pm 0.001\) & \(0.009\pm 0.0\) & \(0.011\pm 0.0\) \\ Swiss roll & 0.5 & Shortest Path & \(\mathbf{0.999\pm 0.0}\) & \(\mathbf{0.999\pm 0.0}\) & \(\mathbf{0.001\pm 0.0}\) & \(\mathbf{0.002\pm 0.0}\) \\ Swiss roll & 0.5 & Euclidean & \(0.376\pm 0.004\) & \(0.422\pm 0.004\) & \(0.013\pm 0.0\) & \(0.018\pm 0.0\) \\ \hline Swiss roll & 1.0 & Diffusion Map & \(0.476\pm 0.226\) & \(0.478\pm 0.138\) & \(0.018\pm 0.0\) & \(0.026\pm 0.0\) \\ Swiss roll & 1.0 & Heat-Geo & \(\mathbf{0.702\pm 0.086}\) & \(\mathbf{0.7\pm 0.073}\) & \(\mathbf{0.01\pm 0.0}\) & \(\mathbf{0.012\pm 0.0}\) \\ Swiss roll & 1.0 & Heat-PHATE & \(0.623\pm 0.144\) & \(0.633\pm 0.114\) & \(\mathbf{0.01\pm 0.002}\) & \(0.019\pm 0.004\) \\ Swiss roll & 1.0 & PHATE & \(0.457\pm 0.01\) & \(0.404\pm 0.024\) & \(0.024\pm 0.0\) & \(0.028\pm 0.0\) \\ Swiss roll & 1.0 & Rand-Geo & \(0.521\pm 0.042\) & \(0.608\pm 0.025\) & \(\mathbf{0.01\pm 0.0}\) & \(0.014\pm 0.0\) \\ Swiss roll & 1.0 & Shortest Path & \(0.497\pm 0.144\) & \(0.558\pm 0.134\) & \(0.011\pm 0.001\) & \(0.015\pm 0.002\) \\ Swiss roll & 1.0 & Euclidean & \(0.365\pm 0.006\) & \(0.413\pm 0.005\) & \(0.013\pm 0.0\) & \(0.019\pm 0.001\) \\ \hline Swiss roll high & 0.1 & Diffusion Map & \(0.98\pm 0.003\) & \(0.986\pm 0.001\) & \(0.018\pm 0.0\) & \(0.026\pm 0.0\) \\ Swiss roll high & 0.1 & Heat-Geo & \(0.992\pm 0.003\) & \(0.996\pm 0.002\) & \(0.002\pm 0.0\) & \(0.003\pm 0.0\) \\ Swiss roll high & 0.1 & Heat-PHATE & \(0.991\pm 0.002\) & \(0.997\pm 0.001\) & \(0.079\pm 0.002\) & \(0.101\pm 0.004\) \\ Swiss roll high & 0.1 & PHATE & \(0.625\pm 0.013\) & \(0.582\pm 0.017\) & \(0.022\pm 0.0\) & \(0.026\pm 0.0\) \\ Swiss roll high & 0.1 & Rand-Geo & \(0.956\pm 0.002\) & \(0.993\pm 0.001\) & \(0.009\pm 0.0\) & \(0.012\pm 0.0\) \\ Swiss roll high & 0.1 & Shortest Path & \(\mathbf{1.0\pm 0.0}\) & \(\mathbf{1.0\pm 0.0}\) & \(\mathbf{0.001\pm 0.0}\) & \(\mathbf{0.002\pm 0.0}\) \\ Swiss roll high & 0.1 & Euclidean & \(0.379\pm 0.002\) & \(0.424\pm 0.002\) & \(0.014\pm 0.0\) & \(0.018\pm 0.0\) \\ \hline Swiss roll high & 0.5 & Diffusion Map & \(0.98\pm 0.002\) & \(0.985\pm 0.002\) & \(0.018\pm 0.0\) & \(0.026\pm 0.0\) \\ Swiss roll high & 0.5 & Heat-Geo & \(0.997\pm 0.001\) & \(0.997\pm 0.0\) & \(\mathbf{0.005\pm 0.0}\) & \(\mathbf{0.007\pm 0.0}\) \\ Swiss roll high & 0.5 & Heat-PHATE & \(0.995\pm 0.0\) & \(0.997\pm 0.0\) & \(0.041\pm 0.001\) & \(0.054\pm 0.002\) \\ Swiss roll high & 0.5 & PHATE & \(0.717\pm 0.004\) & \(0.707\pm 0.005\) & \(0.026\pm 0.0\) & \(0.034\pm 0.001\) \\ Swiss roll high & 0.5 & Rand-Geo & \(0.984\pm 0.0\) & \(0.996\pm 0.0\) & \(0.008\pm 0.0\) & \(0.01\pm 0.0\) \\ Swiss roll high & 0.5 & Shortest Path & \(\mathbf{0.999\pm 0.0}\) & \(\mathbf{0.998\pm 0.0}\) & \(0.006\pm 0.0\) & \(0.009\pm 0.0\) \\ Swiss roll high & 0.5 & Euclidean & \(0.369\pm 0.003\) & \(0.421\pm 0.003\) & \(0.013\pm 0.0\) & \(0.018\pm 0.0\) \\ \hline Swiss roll high & 1.0 & Diffusion Map & \(0.555\pm 0.155\) & \(0.526\pm 0.081\) & \(0.018\pm 0.0\) & \(0.026\pm 0.0\) \\ Swiss roll high & 1.0 & Heat-Geo & \(\mathbf{0.705\pm 0.065}\) & \(\mathbf{0.695\pm 0.052}\) & \(0.011\pm 0.0\) & \(\mathbf{0.012\pm 0.0}\) \\ Swiss roll high & 1.0 & Heat-PHATE & \(0.63\pm 0.016\) & \(0.625\pm 0.074\)

### Time Complexity

In Table 12, we present the average computing time for creating embeddings and corresponding distance matrix for the different methods. All methods are applied on the Swiss roll dataset in three dimension with 2000 samples. We present empirical averages and standard deviations over ten repetitions. The experiments were run on a Apple M2 Pro chip with 16G RAM.

\begin{table}
\begin{tabular}{l r l l l l l} \hline \hline data & Noise level & Method & PearsonR & SpearmanR & Norm Fro N2 & Norm inf N2 \\ \hline Tree & 1.0 & Diffusion Map & \(0.748\pm 0.125\) & \(0.733\pm 0.111\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 1.0 & Heat-Geo & \(\mathbf{0.976\pm 0.019}\) & \(\mathbf{0.977\pm 0.02}\) & \(0.092\pm 0.011\) & \(0.135\pm 0.018\) \\ Tree & 1.0 & Heat-PHATE & \(0.918\pm 0.032\) & \(0.885\pm 0.04\) & \(\mathbf{0.03\pm 0.005}\) & \(\mathbf{0.044\pm 0.007}\) \\ Tree & 1.0 & PHATE & \(0.671\pm 0.021\) & \(0.398\pm 0.052\) & \(0.051\pm 0.008\) & \(0.084\pm 0.017\) \\ Tree & 1.0 & Rand-Geo & \(0.926\pm 0.011\) & \(0.966\pm 0.019\) & \(0.076\pm 0.01\) & \(0.117\pm 0.018\) \\ Tree & 1.0 & Shortest Path & \(0.965\pm 0.026\) & \(0.963\pm 0.027\) & \(0.039\pm 0.008\) & \(0.06\pm 0.008\) \\ Tree & 1.0 & Euclidean & \(0.508\pm 0.039\) & \(0.483\pm 0.052\) & \(0.092\pm 0.011\) & \(0.138\pm 0.018\) \\ \hline Tree & 5.0 & Diffusion Map & \(0.656\pm 0.054\) & \(0.653\pm 0.057\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 5.0 & Heat-Geo & \(\mathbf{0.822\pm 0.008}\) & \(\mathbf{0.807\pm 0.016}\) & \(0.1\pm 0.012\) & \(0.146\pm 0.019\) \\ Tree & 5.0 & Heat-PHATE & \(0.765\pm 0.025\) & \(0.751\pm 0.023\) & \(\mathbf{0.043\pm 0.006}\) & \(\mathbf{0.08\pm 0.01}\) \\ Tree & 5.0 & PHATE & \(0.766\pm 0.023\) & \(0.743\pm 0.028\) & \(0.055\pm 0.007\) & \(0.093\pm 0.008\) \\ Tree & 5.0 & Rand-Geo & \(0.806\pm 0.014\) & \(0.795\pm 0.018\) & \(0.094\pm 0.011\) & \(0.139\pm 0.018\) \\ Tree & 5.0 & Shortest Path & \(0.78\pm 0.009\) & \(0.757\pm 0.019\) & \(0.075\pm 0.009\) & \(0.117\pm 0.014\) \\ Tree & 5.0 & Euclidean & \(0.735\pm 0.014\) & \(0.704\pm 0.033\) & \(0.096\pm 0.011\) & \(0.141\pm 0.017\) \\ \hline Tree & 10.0 & Diffusion Map & \(0.538\pm 0.05\) & \(0.471\pm 0.089\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 10.0 & Heat-Geo & \(0.62\pm 0.025\) & \(\mathbf{0.59\pm 0.033}\) & \(0.1\pm 0.012\) & \(0.146\pm 0.019\) \\ Tree & 10.0 & Heat-PHATE & \(\mathbf{0.63\pm 0.018}\) & \(0.588\pm 0.031\) & \(\mathbf{0.046\pm 0.005}\) & \(\mathbf{0.083\pm 0.012}\) \\ Tree & 10.0 & PHATE & \(0.623\pm 0.016\) & \(0.583\pm 0.029\) & \(0.07\pm 0.01\) & \(0.112\pm 0.017\) \\ Tree & 10.0 & Rand-Geo & \(0.578\pm 0.043\) & \(0.558\pm 0.053\) & \(0.095\pm 0.011\) & \(0.14\pm 0.018\) \\ Tree & 10.0 & Shortest Path & \(0.539\pm 0.041\) & \(0.513\pm 0.055\) & \(0.072\pm 0.01\) & \(0.118\pm 0.017\) \\ Tree & 10.0 & Euclidean & \(0.508\pm 0.039\) & \(0.483\pm 0.052\) & \(0.092\pm 0.011\) & \(0.138\pm 0.018\) \\ \hline Tree high & 1.0 & Diffusion Map & \(0.754\pm 0.049\) & \(0.741\pm 0.057\) & \(0.267\pm 0.021\) & \(0.369\pm 0.026\) \\ Tree high & 1.0 & Heat-Geo & \(0.996\pm 0.001\) & \(\mathbf{0.999\pm 0.001}\) & \(0.242\pm 0.02\) & \(0.338\pm 0.026\) \\ Tree high & 1.0 & Heat-PHATE & \(0.927\pm 0.011\) & \(0.875\pm 0.032\) & \(0.062\pm 0.003\) & \(0.084\pm 0.006\) \\ Tree high & 1.0 & PHATE & \(0.528\pm 0.085\) & \(0.141\pm 0.061\) & \(0.209\pm 0.023\) & \(0.307\pm 0.027\) \\ Tree high & 1.0 & Rand-Geo & \(0.85\pm 0.014\) & \(0.944\pm 0.011\) & \(0.227\pm 0.02\) & \(0.323\pm 0.025\) \\ Tree high & 1.0 & Shortest Path & \(\mathbf{0.998\pm 0.001}\) & \(\mathbf{0.999\pm 0.001}\) & \(\mathbf{0.009\pm 0.002}\) & \(\mathbf{0.018\pm 0.005}\) \\ Tree high & 1.0 & Euclidean & \(0.928\pm 0.018\) & \(0.928\pm 0.024\) & \(0.24\pm 0.02\) & \(0.334\pm 0.026\) \\ \hline Tree high & 5.0 & Diffusion Map & \(0.706\pm 0.124\) & \(0.705\pm 0.113\) & \(0.267\pm 0.021\) & \(0.369\pm 0.026\) \\ Tree high & 5.0 & Heat-Geo & \(\mathbf{0.97\pm 0.01}\) & \(\mathbf{0.975\pm 0.009}\) & \(0.253\pm 0.021\) & \(0.353\pm 0.026\) \\ Tree high & 5.0 & Heat-PHATE & \(0.932\pm 0.022\) & \(0.919\pm 0.03\) & \(\mathbf{0.072\pm 0.004}\) & \(\mathbf{0.112\pm 0.008}\) \\ Tree high & 5.0 & PHATE & \(0.913\pm 0.014\) & \(0.872\pm 0.034\) & \(0.19\pm 0.017\) & \(0.278\pm 0.025\) \\ Tree high & 5.0 & Rand-Geo & \(0.968\pm 0.01\) & \(0.971\pm 0.009\) & \(0.245\pm 0.019\) & \(0.342\pm 0.024\) \\ Tree high & 5.0 & Shortest Path & \(0.952\pm 0.016\) & \(0.95\pm 0.019\) & \(0.137\pm 0.017\) & \(0.209\pm 0.024\) \\ Tree high & 5.0 & Euclidean & \(0.882\pm 0.028\) & \(0.873\pm 0.032\) & \(0.237\pm 0.02\) & \(0.333\pm 0.025\) \\ \hline Tree high & 10.0 & Diffusion Map & \(0.598\pm 0.117\) & \(0.613\pm 0.103\) & \(0.267\pm 0.0

[MISSING_PAGE_EMPTY:25]

\begin{table}
\begin{tabular}{l r l l l l l} \hline \hline data & Noise level & Method & PearsonR & SpearmanR & Norm Fro N2 & Norm inf N2 \\ \hline Tree & 0.1 & Diffusion Map & \(0.748\pm 0.125\) & \(0.733\pm 0.111\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 0.1 & Heat-Geo & \(\mathbf{0.943\pm 0.037}\) & \(\mathbf{0.94\pm 0.037}\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 0.1 & Heat-PHATE & \(0.872\pm 0.04\) & \(0.83\pm 0.061\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 0.1 & PHATE & \(0.564\pm 0.039\) & \(0.469\pm 0.052\) & \(0.113\pm 0.011\) & \(0.161\pm 0.018\) \\ Tree & 0.1 & Rand-Geo & \(0.868\pm 0.017\) & \(0.85\pm 0.019\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 0.1 & Shortest Path & \(0.937\pm 0.037\) & \(0.931\pm 0.041\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 0.1 & TSNE & \(0.847\pm 0.034\) & \(0.824\pm 0.045\) & \(\mathbf{0.082\pm 0.012}\) & \(\mathbf{0.123\pm 0.022}\) \\ Tree & 0.1 & UMAP & \(0.692\pm 0.058\) & \(0.671\pm 0.047\) & \(0.107\pm 0.012\) & \(0.153\pm 0.019\) \\ Tree & 0.1 & Euclidean & \(0.809\pm 0.017\) & \(0.778\pm 0.024\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ \hline Tree & 0.5 & Diffusion Map & \(0.656\pm 0.054\) & \(0.653\pm 0.057\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 0.5 & Heat-Geo & \(\mathbf{0.806\pm 0.019}\) & \(\mathbf{0.787\pm 0.009}\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 0.5 & PHATE & \(0.746\pm 0.023\) & \(0.746\pm 0.03\) & \(0.113\pm 0.011\) & \(0.161\pm 0.018\) \\ Tree & 0.5 & Rand-Geo & \(0.721\pm 0.024\) & \(0.694\pm 0.024\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 0.5 & Shortest Path & \(0.765\pm 0.01\) & \(0.738\pm 0.011\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 0.5 & TSNE & \(0.795\pm 0.046\) & \(0.766\pm 0.055\) & \(\mathbf{0.083\pm 0.012}\) & \(\mathbf{0.128\pm 0.018}\) \\ Tree & 0.5 & UMAP & \(0.783\pm 0.06\) & \(0.757\pm 0.054\) & \(0.11\pm 0.011\) & \(0.157\pm 0.018\) \\ Tree & 0.5 & Euclidean & \(0.704\pm 0.02\) & \(0.672\pm 0.038\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ \hline Tree & 1.0 & Diffusion Map & \(0.538\pm 0.05\) & \(0.471\pm 0.089\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 1.0 & Heat-Geo & \(0.613\pm 0.025\) & \(\mathbf{0.58\pm 0.036}\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 1.0 & Heat-PHATE & \(0.614\pm 0.02\) & \(0.571\pm 0.044\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 1.0 & PHATE & \(\mathbf{0.615\pm 0.017}\) & \(0.572\pm 0.036\) & \(0.113\pm 0.011\) & \(0.161\pm 0.018\) \\ Tree & 1.0 & Rand-Geo & \(0.487\pm 0.064\) & \(0.465\pm 0.071\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 1.0 & Shortest Path & \(0.542\pm 0.047\) & \(0.514\pm 0.06\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ Tree & 1.0 & TSNE & \(0.583\pm 0.042\) & \(0.553\pm 0.045\) & \(\mathbf{0.086\pm 0.011}\) & \(\mathbf{0.135\pm 0.017}\) \\ Tree & 1.0 & UMAP & \(0.595\pm 0.032\) & \(0.562\pm 0.036\) & \(0.111\pm 0.011\) & \(0.158\pm 0.019\) \\ Tree & 1.0 & Euclidean & \(0.502\pm 0.051\) & \(0.479\pm 0.064\) & \(0.113\pm 0.012\) & \(0.161\pm 0.019\) \\ \hline Tree high & 0.1 & Diffusion Map & \(0.754\pm 0.049\) & \(0.741\pm 0.057\) & \(0.267\pm 0.021\) & \(0.369\pm 0.026\) \\ Tree high & 0.1 & Heat-Geo & \(0.956\pm 0.014\) & \(\mathbf{0.957\pm 0.015}\) & \(0.267\pm 0.021\) & \(0.369\pm 0.026\) \\ Tree high & 0.1 & Heat-PHATE & \(0.831\pm 0.082\) & \(0.764\pm 0.115\) & \(0.267\pm 0.021\) & \(0.369\pm 0.026\) \\ Tree high & 0.1 & PHATE & \(0.484\pm 0.036\) & \(0.4\pm 0.028\) & \(0.267\pm 0.02\) & \(0.369\pm 0.025\) \\ Tree high & 0.1 & Rand-Geo & \(0.817\pm 0.013\) & \(0.774\pm 0.022\) & \(0.267\pm 0.021\) & \(0.369\pm 0.026\) \\ Tree high & 0.1 & Shortest Path & \(\mathbf{0.958\pm 0.014}\) & \(0.956\pm 0.017\) & \(0.267\pm 0.021\) & \(0.369\pm 0.026\) \\ Tree high & 0.1 & TSNE & \(0.89\pm 0.039\) & \(0.866\pm 0.043\) & \(\mathbf{0.233\pm 0.021}\) & \(\mathbf{0.327\pm 0.026}\) \\ Tree high & 0.1 & UMAP & \(0.8\pm 0.031\) & \(0.764\pm 0.034\) & \(0.259\pm 0.021\) & \(0.366\pm 0.028\) \\ Tree high & 0.1 & Euclidean & \(0.878\pm 0.042\) & \(0.859\pm 0.051\) & \(0.267\pm 0.021\) & \(0.369\pm 0.026\) \\ \hline Tree high & 0.5 & Diffusion Map & \(0.706\pm 0.124\) & \(0.705\pm 0.113\) & \(0.267\pm 0.021\) & \(0.369\pm 0.026\) \\ Tree high & 0.5 & Heat-Geo & \(\mathbf{0.932\pm 0.022}\) & \(\mathbf{0.928\pm 0.023}\) & \(0.267\pm 0.021\) &

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline data & Noise level & Method & Homogeneity & Adjusted Rand Score & Adjusted Mutual Info Score \\ \hline Swiss roll & 0.1 & Heat-Geo & \(\mathbf{0.82\pm 0.008}\) & \(\mathbf{0.668\pm 0.034}\) & \(\mathbf{0.74\pm 0.018}\) \\ Swiss roll & 0.1 & Plate & \(0.731\pm 0.035\) & \(0.546\pm 0.044\) & \(0.652\pm 0.046\) \\ Swiss roll & 0.1 & TSNE & \(0.748\pm 0.067\) & \(0.537\pm 0.1\) & \(0.668\pm 0.068\) \\ Swiss roll & 0.1 & UMAP & \(0.81\pm 0.036\) & \(0.611\pm 0.039\) & \(0.726\pm 0.045\) \\ \hline Swiss roll & 0.5 & Heat-Geo & \(0.813\pm 0.026\) & \(0.656\pm 0.049\) & \(0.733\pm 0.022\) \\ Swiss roll & 0.5 & Plate & \(0.735\pm 0.048\) & \(0.543\pm 0.064\) & \(0.656\pm 0.053\) \\ Swiss roll & 0.5 & TSNE & \(0.764\pm 0.07\) & \(0.564\pm 0.097\) & \(0.684\pm 0.065\) \\ Swiss roll & 0.5 & UMAP & \(\mathbf{0.826\pm 0.019}\) & \(\mathbf{0.664\pm 0.073}\) & \(\mathbf{0.744\pm 0.032}\) \\ \hline Swiss roll & 1.0 & Heat-Geo & \(0.722\pm 0.051\) & \(0.548\pm 0.091\) & \(0.652\pm 0.056\) \\ Swiss roll & 1.0 & Plate & \(0.482\pm 0.014\) & \(0.317\pm 0.031\) & \(0.428\pm 0.021\) \\ Swiss roll & 1.0 & TSNE & \(\mathbf{0.757\pm 0.037}\) & \(\mathbf{0.562\pm 0.058}\) & \(\mathbf{0.679\pm 0.042}\) \\ Swiss roll & 1.0 & UMAP & \(0.726\pm 0.041\) & \(0.51\pm 0.077\) & \(0.65\pm 0.05\) \\ \hline Swiss roll high & 0.1 & Heat-Geo & \(\mathbf{0.82\pm 0.015}\) & \(\mathbf{0.666\pm 0.033}\) & \(\mathbf{0.739\pm 0.019}\) \\ Swiss roll high & 0.1 & Plate & \(0.705\pm 0.03\) & \(0.518\pm 0.048\) & \(0.628\pm 0.04\) \\ Swiss roll high & 0.1 & TSNE & \(0.757\pm 0.078\) & \(0.585\pm 0.115\) & \(0.677\pm 0.08\) \\ Swiss roll high & 0.1 & UMAP & \(0.796\pm 0.03\) & \(0.624\pm 0.048\) & \(0.714\pm 0.037\) \\ \hline Swiss roll high & 0.5 & Heat-Geo & \(\mathbf{0.805\pm 0.021}\) & \(\mathbf{0.655\pm 0.047}\) & \(\mathbf{0.725\pm 0.035}\) \\ Swiss roll high & 0.5 & Pate & \(0.745\pm 0.04\) & \(0.562\pm 0.061\) & \(0.664\pm 0.047\) \\ Swiss roll high & 0.5 & TSNE & \(0.747\pm 0.075\) & \(0.538\pm 0.11\) & \(0.668\pm 0.075\) \\ Swiss roll high & 0.5 & UMAP & \(0.787\pm 0.041\) & \(0.573\pm 0.067\) & \(0.703\pm 0.032\) \\ \hline Swiss roll high & 1.0 & Heat-Geo & \(0.7\pm 0.045\) & \(0.534\pm 0.057\) & \(0.644\pm 0.032\) \\ Swiss roll high & 1.0 & Pate & \(0.552\pm 0.047\) & \(0.386\pm 0.056\) & \(0.496\pm 0.04\) \\ Swiss roll high & 1.0 & TSNE & \(0.754\pm 0.034\) & \(0.548\pm 0.068\) & \(0.675\pm 0.036\) \\ Swiss roll high & 1.0 & UMAP & \(\mathbf{0.76\pm 0.041}\) & \(\mathbf{0.56\pm 0.077}\) & \(\mathbf{0.68\pm 0.05}\) \\ \hline Swiss roll very high & 0.1 & Heat-Geo & \(\mathbf{0.818\pm 0.033}\) & \(\mathbf{0.668\pm 0.074}\) & \(\mathbf{0.738\pm 0.039}\) \\ Swiss roll very high & 0.1 & Pate & \(0.688\pm 0.043\) & \(0.497\pm 0.053\) & \(0.614\pm 0.053\) \\ Swiss roll very high & 0.1 & TSNE & \(0.741\pm 0.07\) & \(0.544\pm 0.101\) & \(0.662\pm 0.075\) \\ Swiss roll very high & 0.1 & UMAP & \(0.816\pm 0.042\) & \(0.65\pm 0.069\) & \(0.733\pm 0.054\) \\ \hline Swiss roll very high & 0.5 & Heat-Geo & \(0.73\pm 0.045\) & \(\mathbf{0.605\pm 0.093}\) & \(0.701\pm 0.028\) \\ Swiss roll very high & 0.5 & Pate & \(0.758\pm 0.034\) & \(0.55\pm 0.037\) & \(0.676\pm 0.014\) \\ Swiss roll very high & 0.5 & TSNE & \(0.77\pm 0.054\) & \(0.557\pm 0.093\) & \(\mathbf{0.708\pm 0.031}\) \\ Swiss roll very high & 0.5 & UMAP & \(\mathbf{0.789\pm 0.052}\) & \(0.574\pm 0.101\) & \(0.707\pm 0.061\) \\ \hline Swiss roll very high & 1.0 & Heat-Geo & \(0.592\pm 0.033\) & \(0.427\pm 0.063\) & \(0.545\pm 0.031\) \\ Swiss roll very high & 1.0 & Pate & \(0.531\pm 0.042\) & \(0.377\pm 0.046\) & \(0.486\pm 0.045\) \\ Swiss roll very high & 1.0 & TSNE & \(\mathbf{0.738\pm 0.019}\) & \(\mathbf{0.551\pm 0.039}\) & \(\mathbf{0.662\pm 0.025}\) \\ Swiss roll very high & 1.0 & UMAP & \(0.736\pm 0.057\) & \(0.542\pm 0.102\) & \(0.66\pm 0.061\) \\ \hline Tree & 0.1 & Heat-Geo & \(\mathbf{0.784\pm 0.051}\) & \(\mathbf{0.734\pm 0.07}\) & \(\mathbf{0.786\pm 0.051}\) \\ Tree & 0.1 & Pate & \(0.55\pm 0.042\) & \(0.409\pm 0.064\) & \(0.555\pm 0.042\) \\ Tree & 0.1 & TSNE & \(0.706\pm 0.054\) & \(0.61\pm 0.075\) & \(0.712\pm 0.055\) \\ Tree & 0.1 & UMAP & \(0.678\pm 0.086\) & \(0.584\pm 0.12\) & \(0.681\pm 0.086\) \\ \hline Tree & 0.5 & Heat-Geo & \(0.545\pm 0.121\) & \(0.411\pm 0.154\) & \(0.577\pm 0.094\) \\ Tree & 0.5 & Pate & \(0.529\pm 0.111\) & \(0.404\pm 0.151\) & \(0.555\pm 0.095\) \\ Tree & 0.5 & TSNE & \(\mathbf{0.647\pm 0.049}\) & \(\mathbf{0.591\pm 0.065}\) & \(0.65\pm 0.048\) \\ Tree & 0.5 & UMAP & \(0.645\pm 0.051\) & \(0.565\pm 0.058\) & \(\mathbf{0.652\pm 0.05}\) \\ \hline Tree & 1.0 & Heat-Geo & \(0.398\pm 0.07\) & \(0.3\pm 0.077\) & \(0.42\pm 0.07\) \\ Tree & 1.0 & Pate & \(0.418\pm 0Figure 8: Embeddings of Heat Geodesic Embedding for different choices of hyperparameters on the EB dataset. We evaluate the impact of the Harnack regularization, the diffusion time, the number of neighbours in the kNN, and the order of the approximation for Euler and Chechyshev approximations.

Figure 10: Impact of Checkyshev approximation order on the Pearson correlation between the estimated distance matrix and ground truth distance matrix for different methods on the Swiss roll dataset.

Figure 9: Impact of diffusion time on the Pearson correlation between the estimated distance matrix and ground truth distance matrix for different methods on the Swiss roll dataset.

Figure 11: Impact of number of neighbours on the Pearson correlation between the estimated distance matrix and ground truth distance matrix for different methods on the Swiss roll dataset.

Figure 12: Impact of Harnack regularization on the Pearson correlation between the estimated distance matrix and ground truth distance matrix for HeatGeo on the Swiss roll dataset.

\begin{table}
\begin{tabular}{l c l l l l} \hline \hline data & Noise level & Method & Homogeneity & Adjusted Rand Score & Adjusted Mutual Info Score \\ \hline Mnist & 0 & Diff-Map & \(0.556\pm 0.002\) & \(0.347\pm 0.002\) & \(0.622\pm 0.002\) \\ Mnist & 0 & Heat-Geo & \(0.785\pm 0.0\) & \(0.695\pm 0.0\) & \(0.829\pm 0.001\) \\ Mnist & 0 & Plate & \(0.822\pm 0.01\) & \(0.72\pm 0.017\) & \(0.835\pm 0.011\) \\ Mnist & 0 & TSNE & \(\mathbf{0.903\pm 0.003}\) & \(\mathbf{0.871\pm 0.002}\) & \(\mathbf{0.902\pm 0.003}\) \\ Mnist & 0 & UMAP & \(0.851\pm 0.016\) & \(0.846\pm 0.005\) & \(0.86\pm 0.015\) \\ \hline Coil & 0 & Diff-Map & \(0.21\pm 0.036\) & \(0.041\pm 0.015\) & \(0.142\pm 0.024\) \\ Coil & 0 & Heat-Geo & \(0.849\pm 0.016\) & \(0.67\pm 0.029\) & \(0.806\pm 0.022\) \\ Coil & 0 & Plate & \(0.804\pm 0.017\) & \(0.615\pm 0.028\) & \(0.735\pm 0.021\) \\ Coil & 0 & TSNE & \(\mathbf{0.907\pm 0.014}\) & \(\mathbf{0.79\pm 0.03}\) & \(\mathbf{0.88\pm 0.02}\) \\ Coil & 0 & UMAP & \(0.871\pm 0.009\) & \(0.725\pm 0.019\) & \(0.826\pm 0.012\) \\ \hline Pbmc & 0 & Diff-Map & \(0.026\pm 0.001\) & \(0.011\pm 0.0\) & \(0.038\pm 0.001\) \\ Pbmc & 0 & Heat-Geo & \(0.734\pm 0.009\) & \(0.724\pm 0.019\) & \(0.768\pm 0.017\) \\ Pbmc & 0 & Plate & \(\mathbf{0.798\pm 0.012}\) & \(\mathbf{0.818\pm 0.009}\) & \(\mathbf{0.785\pm 0.01}\) \\ Pbmc & 0 & TSNE & \(0.605\pm 0.019\) & \(0.437\pm 0.032\) & \(0.544\pm 0.022\) \\ Pbmc & 0 & UMAP & \(0.177\pm 0.037\) & \(0.097\pm 0.033\) & \(0.148\pm 0.035\) \\ \hline \hline \end{tabular}
\end{table}
Table 12: Average computing time for creating embeddings and corresponding distance matrix for the different methods. All methods are applied on the Swiss roll dataset in three dimension with 2000 samples. We present empirical averages and standard deviations over ten repetitions.

Figure 13: Pearson correlation between estimated and ground truth distance matrices for the 10-dimensional Swiss roll dataset for various graph constructions. Standard deviations are computed over the 5 test folds.