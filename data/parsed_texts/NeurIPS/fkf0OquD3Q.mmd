# Private Online Learning via Lazy Algorithms

Hilal Asi

Apple

hilal.asi94@gmail.com

&Tomer Koren

Tel Aviv University

tkoren@tauex.tau.ac.il

Daogao Liu

University of Washington

liudaogao@gmail.com

&Kunal Talwar

Apple

kunal@kunaltalwar.org

Part of this work was done while interning at Apple.

###### Abstract

We study the problem of private online learning, focusing on online prediction from experts (OPE) and online convex optimization (OCO). We propose a new transformation that translates lazy, low-switching online learning algorithms into private algorithms. We apply our transformation to differentially private OPE and OCO using existing lazy algorithms for these problems. The resulting algorithms attain regret bounds that significantly improve over prior art in the high privacy regime, where \(\varepsilon\ll 1\), obtaining \(O(\sqrt{T\log d}+T^{1/3}\log(d)/\varepsilon^{2/3})\) regret for DP-OPE and \(O(\sqrt{T}+T^{1/3}\sqrt{d}/\varepsilon^{2/3})\) regret for DP-OCO. We complement our results with a lower bound for DP-OPE, showing that these rates are optimal for a natural family of low-switching private algorithms.

## 1 Introduction

Online learning is a fundamental problem in machine learning, where an algorithm interacts with an oblivious adversary for \(T\) rounds. First, the oblivious adversary chooses \(T\) loss functions \(\ell_{1},\ldots,\ell_{T}:\mathcal{X}\rightarrow\mathbb{R}\) over a fixed decision set \(\mathcal{X}\). Then, at any round \(t\), the algorithm chooses a model \(x_{t}\in\mathcal{X}\), and the adversary reveals the loss function \(\ell_{t}\). The algorithm suffers loss \(\ell_{t}(x_{t})\), and its goal is to minimize its cumulative loss compared to the best model in hindsight, namely its _regret_:

\[\mathbf{Reg}_{T}=\sum_{t=1}^{T}\ell_{t}(x_{t})-\min_{x^{*}\in\mathcal{X}}\sum _{t=1}^{T}\ell_{t}(x^{*}).\]

In this work, we study two different _differentially private_ instances of this problem: differentially private online prediction from experts (DP-OPE) where the model \(x\) can be chosen from \(d\) experts (\(\mathcal{X}=[d]\)); and differentially private online convex optimization (DP-OCO) where the model belongs to a convex set \(\mathcal{X}\subset\mathbb{R}^{d}\).

Both problems have been extensively studied recently [1, 2, 3, 14, 15, 16] and an exciting new direction with promising results for this problem is that of designing private algorithms based on low-switching algorithms for online learning [1, 2, 13, 15, 16]. The main idea in these works is that the privacy cost for privatizing a low-switching algorithm can be significantly smaller as these algorithms do not update their models too frequently, allowing them to allocate a larger privacy budget for each update. This has been initiated by [1], which used the shrinking dartboard algorithm to design new algorithms for DP-OPE, later revisited by [1] to design new algorithms for DP-OCO using a regularized follow-the-perturbed-leader approach, and more recently by [1] which used a lazy and regularized version of the multiplicative weights algorithm to obtain improved rates for DP-OCO.

While all of these results build on lazy-switching algorithms for designing private online algorithms, each one of them has a different method for achieving privacy and, to a greater extent, a different analysis. Moreover, it is not clear whether these transformations from lazy to private algorithms in prior work have fulfilled the full potential of lazy algorithms for private online learning and whether better algorithms are possible through this approach. Indeed, the regret obtained in prior work [1, 2] is \(T^{1/3}/\varepsilon\) (omitting dependence on \(d\)) for DP-OPE, which implies that the normalized regret is \(1/T^{2/3}\varepsilon\): this is different than what exhibited in a majority of scenarios of private optimization, where the normalized error is usually a function of \(T\varepsilon\).

### Our contributions

Our main contribution in this work is a new transformation that converts lazy online learning algorithms into private algorithms with similar regret guarantees, resulting in new state-of-the-art rates for DP-OPE and DP-OCO. We provide a summary in Table 1 and Figure 1.

L2P: a transformation from lazy to private algorithms (Section 3).Our main contribution is a new transformation, we call L2P, that allows converting any lazy algorithm into a private one with only a slight cost in regret. This allows us to use a long line of work on lazy online learning [11, 12, 13, 14] to design new algorithms for the private setting. Our transformation builds on two new techniques: first, we design a new switching rule that only depends on the loss at the current round, so as to minimize the privacy cost of each switching and mitigate the accumulation of privacy loss. Second, we rely on a simple, key observation that by grouping losses in a large batch, we can minimize the effect on the regret of lazy online learning algorithms. We introduce a new analysis for the regret of lazy online algorithms with a large batch size that improves over the existing analysis in [1]; this allows us to reduce the total number of "fake switches" needed to guarantees privacy, improving the final regret.

Faster rates for DP-OPE (Section 3.1).As a first application, we use our transformation in the DP-OPE problem on the multiplicative weights algorithm [1]. This results in a new algorithm for DP-OPE that has regret \(\sqrt{T\log(d)}+T^{1/3}\log(d)/\varepsilon^{2/3}\), improving over the best existing results for the high-dimensional regime in which the regret is \(\sqrt{T\log(d)}+T^{1/3}\log(d)/\varepsilon\)[1].2

Figure 1: Regret bounds for (a) DP-OCO with \(d=\operatorname{poly}\log(T)\), (b) DP-OCO with \(d=T^{1/3}\) and (c) DP-OPE with \(d=T\). We denote the privacy parameter \(\varepsilon=T^{-\alpha}\) and regret \(T^{\beta}\), and plot \(\beta\) as a function of \(\alpha\) (ignoring logarithmic factors).

\begin{table}
\begin{tabular}{|c|c|c|} \hline  & **Prior work** & **This work** \\ \hline
**DP-OPE** & \(\sqrt{T\log d}+\frac{\min\{\sqrt{d},T^{1/3}\log d\}}{\varepsilon}\)[1, 2] & \(\sqrt{T\log d}+\frac{T^{1/3}\log d}{\varepsilon^{2/3}}\) \\ \hline
**DP-OCO** & \(\min\left\{\frac{d^{1/4}\sqrt{T}}{\sqrt{\varepsilon}},\sqrt{T}+\frac{T^{1/3} \sqrt{d}}{T^{1/3}\sqrt{d}}+\frac{T^{3/8}\sqrt{d}}{\varepsilon^{3/4}}\right\}\)[1, 2] & \(\sqrt{T}+\frac{T^{1/3}\sqrt{d}}{\varepsilon^{2/3}}\) \\ \hline \end{tabular}
\end{table}
Table 1: Regret for approximate \((\varepsilon,\delta)\)-DP algorithms. For readability, we omit logarithmic factors that depend on \(T\) and \(1/\delta\).

The improvement is particularly crucial in the high-privacy regime, where \(\varepsilon\ll 1\): indeed, our regret shows that (for \(d=\mathsf{poly}(T)\)) it is sufficient to set \(\varepsilon\geq T^{-1/4}\) for matching the optimal non-private regret \(\sqrt{T\log d}\), whereas previous results require a much larger \(\varepsilon\geq T^{-1/6}\) to get privacy for free. This is also important in practice, when multiple applications of DP-OPE are necessary: using advanced composition, our result shows that we can solve \(K\approx\sqrt{T}\) instances of DP-OPE with \(\varepsilon=1\) and still obtain the non-private regret of order \(\sqrt{T}\); in contrast, prior work only allows to solve \(K\approx T^{1/3}\) instances while still attaining the non-private regret.

Faster rates for DP-OCO (Section 3.2).As another application, we use our transformation for DP-OCO with the regularized multiplicative weights algorithm of [1]. We obtain a new algorithm for DP-OCO that has regret \(\sqrt{T}+T^{1/3}\sqrt{d}/\varepsilon^{2/3}\), improving over the best existing results that established regret \(\sqrt{T}+T^{1/3}\sqrt{d}/\varepsilon+T^{3/8}\sqrt{d}/\varepsilon^{3/4}\)[1] or \(d^{1/4}\sqrt{T}/\sqrt{\varepsilon}\) using DP-FTRL [10].

Lower bounds for low-switching private algorithms (Section 4).To understand the limitations of low-switching private algorithms, we prove a lower bound for the natural family of private algorithms with limited switching, showing that the upper bounds obtained via our reduction are nearly tight for this family of algorithms up to logarithmic factors. This shows that new techniques, beyond limited switching, are required in order to improve upon our upper bounds.

Related work.Our transformation and algorithms build on a long line of work in online learning with limited switching [13, 14, 15, 16, 17, 18]. As is evident from prior work in private online learning, the problems of lazy online learning and private online learning are tightly connected [1, 1, 18, 19, 20, 21]. In this problem, the algorithm wishes to minimize its regret while making at most \(S\) switches: the algorithm can update the model at most \(S\) times throughout the \(T\) rounds. Recent work has resolved the lazy OPE problem: [1] show a lower bound of \(\sqrt{T}+(T/S)\log(d)\) on the regret, which is achieved by several algorithms such as Follow-the-perturbed-leader [13] and the shrinking dartboard algorithm [12]. For lazy OCO, however, optimal rates are yet to be known: [1] recently show that a lazy version of the regularized multiplicative weights algorithm obtains regret \(\sqrt{T}+(T/S)\sqrt{d}\), whereas the best lower bound is \(\sqrt{T}+T/S\)[18].

## 2 Preliminaries

### Problem setup

We consider an interactive \(T\)-round game between an algorithm \(\mathsf{ALG}\) and an oblivious adversary \(\mathsf{Adv}\). Before the interaction, the adversary \(\mathsf{Adv}\) chooses \(T\) loss functions \(\ell_{1},\ldots,\ell_{T}\in\mathcal{L}=\{\ell\mid\ell:\mathcal{X}\to \mathbb{R}\}\). Then, at each round \(t\in[T]\), the algorithm \(\mathsf{ALG}\), which observed \(\ell_{1},\cdots,\ell_{t-1}\) chooses \(x_{t}\in\mathcal{X}\), and then the loss function \(\ell_{t}\) chosen by \(\mathsf{Adv}\) is revealed. The regret of the algorithm \(\mathsf{ALG}\) is defined below:

\[\mathbf{Reg}_{T}(\mathsf{ALG}):=\sum_{t=1}^{T}\ell_{t}(x_{t})-\min_{x^{*}\in \mathcal{X}}\sum_{t=1}^{T}\ell_{t}(x^{*}).\]

We study online optimization under the constraint that the algorithm is differentially private. For an algorithm \(\mathsf{ALG}\) and a sequence \(\mathcal{S}=(\ell_{1},\ldots,\ell_{T})\) chosen by an oblivious adversary \(\mathsf{Adv}\), we let \(\mathsf{ALG}(\mathcal{S})\coloneqq(x_{1},\ldots,x_{T})\) denote the output of \(\mathsf{ALG}\) over the loss sequence \(\mathcal{S}\). We have the following definition of privacy against oblivious adversaries.3

Footnote 3: Our regret bound may be invalid with an adaptive adversary, but our algorithms will satisfy a stronger notion of differential privacy against adaptive adversaries (see [1]). However, to keep the notation and analysis simpler, we limit our attention to privacy against oblivious adversaries.

**Definition 2.1** (Differential privacy).: A randomized algorithm \(\mathsf{ALG}\) is \((\varepsilon,\delta)\)-differentially private against oblivious adversaries (\((\varepsilon,\delta)\)-DP) if, for all neighboring sequencesand \(\mathcal{S}^{\prime}=(\ell_{1}^{\prime},\ldots,\ell_{T}^{\prime})\in\mathcal{L}^{T}\) that differ in a single element, and for all events \(\mathcal{O}\) in the output space of \(\mathsf{ALG}\), we have

\[\Pr[\mathsf{ALG}(\mathcal{S})\in\mathcal{O}]\leq e^{\varepsilon}\Pr[\mathsf{ ALG}(\mathcal{S}^{\prime})\in\mathcal{O}]+\delta.\]

We focus on two important instances of differentially private online optimization:

1. [label=()]
2. **DP Online Convex Optimization (DP-OCO).** In this problem, the adversary picks loss functions \(\ell\in\mathcal{L}_{OCO}\coloneqq\{\ell\mid\ell:\mathcal{X}\to\mathbb{R}\text { is convex and }L\text{-Lipschitz}\}\) where \(\mathcal{X}\subset\mathbb{R}^{d}\) is a convex set with diameter \(D=\text{diam}(\mathcal{X})\coloneqq\sup_{x,y\in\mathcal{X}}\|x-y\|\), and the algorithm chooses \(x_{t}\in\mathcal{X}\). The goal of the algorithm is to minimize regret while being \((\varepsilon,\delta)\)-differentially private.
3. **DP Online Prediction from Experts (DP-OPE).** In this problem, the adversary picks loss functions \(\ell\in\mathcal{L}_{OPE}=\{\ell\mid\ell:[d]\to[0,1]\}\) where \(\mathcal{X}=[d]\) is the set of \(d\) experts, and the algorithm chooses \(x_{t}\in[d]\). The goal of the algorithm is to minimize regret while being \((\varepsilon,\delta)\)-differentially private.

### Tools from differential privacy

Our analysis crucially relies on the following divergence between two distributions.

**Definition 2.2** (\(\delta\)-Approximate Max Divergence).: For two distributions \(\mu\) and \(\nu\), we define

\[D_{\infty}^{\delta}(\mu\|\nu)\coloneqq\sup_{S\subseteq\mathrm{supp}(\mu),\mu( S)\geq\delta}\ln\frac{\mu(S)-\delta}{\nu(S)}.\]

We let \(D_{\infty}^{\delta}(\mu,\nu)\coloneqq\max\{D_{\infty}^{\delta}(\mu\|\nu),D_{ \infty}^{\delta}(\nu\|\mu)\}\).

We also use the notion of indistinguishability between two distributions.

**Definition 2.3**.: (\((\varepsilon,\delta)\)-indistinguishability) Two distributions \(\mu,\nu\) are \((\varepsilon,\delta)\)-indistinguishable, denoted \(\mu\approx_{(\varepsilon,\delta)}\nu\), if \(D_{\infty}^{\delta}(\mu,\nu)\leq\varepsilon\).

Note that if an algorithm \(\mathsf{ALG}\) has \(\mathsf{ALG}(\mathcal{S})\approx_{(\varepsilon,\delta)}\mathsf{ALG}( \mathcal{S}^{\prime})\) for all neighboring datasets \(\mathcal{S},\mathcal{S}^{\prime}\) then \(\mathsf{ALG}\) is \((\varepsilon,\delta)\)-differentially private. We direct readers to Appendix A for additional background information and detailed preliminaries.

## 3 L2P: From Lazy to Private Algorithms for Online Learning

This section presents our \(\mathsf{L2P}\) transformation, which turns lazy online learning algorithms into private ones. The transformation has an input algorithm \(\mathcal{A}\) with measure \(\mu_{t}\) at round \(t\) and samples \(x_{t}\) from the normalized measure \(\overline{\mu}_{t}\), which satisfies the following condition:

**Assumption 3.1**.: The online algorithm \(\mathcal{A}\) has at time \(t\) a measure \(\mu_{t}\) that is a function of \(\ell_{1},\ldots,\ell_{t-1}\) (and density function \(\overline{\mu}_{t}\)) such that for some \(\delta_{0}\leq 1\) and \(0<\eta\leq 1/10\) that are data-independent, we have

* \(D_{\infty}^{\delta_{0}}(\overline{\mu}_{t+1},\overline{\mu}_{t})\leq\eta\),
* \(\mu_{t+1}(x)/\mu_{t}(x)=\mathsf{func}(\ell_{t},x)\) for all \(x\in\mathcal{X}\) where \(\mathsf{func}\) is a data-independent function.

While algorithms satisfying Assumption 3.1 need not be lazy, this assumption is satisfied by most existing lazy online learning algorithms such as the shrinking dartboard (Section 3.1) and lazy regularized multiplicative weights (Section 3.2). Moreover, any algorithm that satisfies this assumption can be made lazy via our reduction.

Technique Overview:Suppose the neighboring datasets differ from the \(s_{0}\)-th loss function. The high-level intuition behind our framework is that our algorithm only loses the privacy budget when it makes a switch (draws a fresh sample) whenever \(t>s_{0}\). Hence, in the framework, we try to make the algorithm make as few switches as possible. This modification can lead to additional regret compared to lazy online learning algorithms, and we need to balance the privacy-regret trade-off. The family of low-switching algorithms is ideal for privatization because its built-in low-switching property can achieve a better trade-off.

Our starting point is the ideas in [1, 1] to privatize low-switching algorithms, which use correlated sampling to argue that a sample from \(x_{t-1}\sim\overline{\mu}_{t-1}\) is likely a good sample from \(\overline{\mu}_{t}\) and therefore switching at round \(t\) is often not necessary. In particular, at round \(t\), these algorithms sample a Bernoulli random variable \(S_{t}\sim\mathsf{Ber}(c\cdot\overline{\mu}_{t}(x_{t-1})/\overline{\mu}_{t-1}(x _{t-1}))\) for some constant \(c\) and use the same model \(x_{t}=x_{t-1}\) if \(S_{t}=1\), and otherwise sample new model \(x_{t}\sim\overline{\mu}_{t}\) if \(S_{t}=0\) (which happens with small probability). This guarantees that the marginal probability of the lazy iterates remains the same as the original iterates. Finally, to preserve the privacy of the switching decisions, existing algorithms add a fake switching probability \(p\) where the algorithm switches independently of the input. To summarize, _existing_ low-switching private algorithms work roughly as follows:

\[\begin{cases}&\text{At each round $t$:}\\ &-\text{Sample $S_{t}\sim\mathsf{Ber}(C\cdot\overline{\mu}_{t}(x_{t-1})/ \overline{\mu}_{t-1}(x_{t-1}))$ and $S^{\prime}_{t}\sim\mathsf{Ber}(1-p)$}\\ &-\text{Sample new $x_{t}\sim\overline{\mu}_{t}$ if $S_{t}=0$ or $S^{\prime}_{t}=0$}\\ &-\text{Otherwise set $x_{t}=x_{t-1}$}\end{cases}\]

This sketch is the starting point of our transformation, and we will introduce two new components to improve performance. The first component aims to avoid the accumulation of privacy cost for switching in the current approaches where each user can affect the switching probability for all subsequent rounds: this happens since \(\overline{\mu}_{t}(x_{t-1})/\overline{\mu}_{t-1}(x_{t-1})\) is usually a function of the whole history \(\ell_{1},\ldots,\ell_{t}\), and hence the existing low-switching private algorithms lose the privacy budget even it does not make real switches. To address this, we deploy a new correlated sampling strategy in \(\mathsf{L2P}\) where the loss \(\ell_{t}\) at time \(t\) affects the switching probability only at time \(t\), hence paying a privacy cost for switching only in a single round. To this end, we construct a parallel sequence of models \(\{y_{t}\}_{t\in[T]}\) (independent of \(x_{t}\)) that is used for normalizing the ratio \(\overline{\mu}_{t}(x_{t-1})/\overline{\mu}_{t-1}(x_{t-1})\) to become independent of the history. In particular, at round \(t\), we switch with probability proportional to

\[\frac{\overline{\mu}_{t}(x_{t-1})}{\overline{\mu}_{t-1}(x_{t-1})}\cdot\frac{ \overline{\mu}_{t-1}(y_{t-1})}{\overline{\mu}_{t}(y_{t-1})}.\]

The main observation here is that \(\frac{\overline{\mu}_{t}(x_{t-1})}{\overline{\mu}_{t-1}(x_{t-1})}\cdot\frac{ \overline{\mu}_{t-1}(y_{t-1})}{\overline{\mu}_{t}(y_{t-1})}=\frac{\mu_{t}(x_{ t-1})}{\overline{\mu}_{t-1}(x_{t-1})}\cdot\frac{\overline{\mu}_{t-1}(y_{t-1})}{ \overline{\mu}_{t}(y_{t-1})}\) and this ratio is a function of \(\ell_{t}\) our input online learning algorithms which satisfies Assumption 3.1. This will, therefore, improve the privacy guarantee of the final algorithm.

The second main observation in \(\mathsf{L2P}\) is that having a large batch size (batching rounds together) does not significantly affect the regret of lazy online algorithms compared to non-lazy algorithms but can further reduce the times to make switches and save the privacy budget. Our main novelty is a new analysis of the effect of batching on the regret of lazy algorithms (Proposition 3.3), which states that running a lazy online algorithm with a batch size of \(B\) would have an additive error of \(TB^{2}\eta^{2}\) to the regret where \(\eta\) is a measure of distance between \(\overline{\mu}_{t}\) and \(\overline{\mu}_{t-1}\). This significantly improves over existing analysis by [1, Theorem 2] which shows that batching can add an additive term of \(B/\eta\) to the regret.

Having reviewed our main techniques, we proceed to present the full details of our \(\mathsf{L2P}\) transformation in Algorithm 1, denoting \(\nu_{s}=\mu_{(s-1)B+1}\) where \(B\) is the batch size.

The regret of our transformation depends on the regret of its input algorithm. For the measure \(\{\mu_{t}\}_{t=1}^{T}\), we denote its regret

\[\mathbf{Reg}_{T}(\{\mu_{t}\}_{t=1}^{T}):=\sum_{t=1}^{T}\mathop{\mathbb{E}}_{x_ {t}\sim\overline{\mu}_{t}}\left[\ell_{t}(x_{t})\right]-\min_{x\in\mathcal{X}} \sum_{t=1}^{T}\ell_{t}(x).\]

The following theorem summarizes the main guarantees of Algorithm 1.

**Theorem 3.2**.: _Let \(p\in(0,1)\) and \(B\in\mathbb{N}\). Assuming Assumption 3.1, \(Tp/B\geq 1\), and for any \(\delta_{1}>0\) such that \(\eta B\log(1/\delta_{1})/p\leq 1\), our transformation \(\mathsf{L2P}\) is \((\varepsilon,\delta)\)-DP with_

\[\varepsilon=\frac{2\eta}{p}+\eta+\frac{3T\eta^{2}p\log(1/\delta_{1})}{2B}+ \sqrt{6T\eta^{2}p\log^{2}(1/\delta_{1})/B},\]\[\delta=2T(2/\eta+\log(1/\delta_{1})/p)eB\delta_{0}+2T\delta_{1},\]

_and has regret_

\[\mathbf{Reg}_{T}\leq\mathbf{Reg}_{T}(\{\mu_{t}\}_{t=1}^{T})+O\left(TB^{2}\eta^{2 }+\frac{\delta_{0}T^{2}\log(\frac{1}{\delta_{1}})}{\eta}\right).\]

We begin by proving the utility guarantees of our transformation. It will follow directly from the following proposition, which bounds the regret of running L2P over a lazy online learning algorithm.

**Proposition 3.3** (Regret of Batched Lazy Algorithm).: _Let \(\mathsf{ALG}\) be an online learning algorithm that satisfies Assumption 3.1. Let \(\eta B\log(1/\delta_{1})/p\leq 1\), and \(\delta_{1},\eta<1/2\). Then running \(\mathsf{L2P}\) with the input algorithm \(\mathsf{ALG}\) has regret_

\[\mathbf{Reg}_{T}\leq\mathbf{Reg}_{T}(\{\mu_{t}\}_{t=1}^{T})+O\left(TB^{2}\eta^ {2}+\frac{\delta_{0}T^{2}\log(\frac{1}{\delta_{1}})}{\eta}\right).\]

To prove Proposition 3.3, we first show that we can instead analyze the utility of a simpler algorithm that samples from \(\overline{\nu}_{s}\) at each round. This is due to the following lemma, which shows that \(\|\widehat{\nu}_{s}-\overline{\nu}_{s}\|_{TV}\) is small where \(\widehat{\nu}_{s}\) is the marginal distribution of \(x_{s}\) in Algorithm 1.

**Lemma 3.4**.: _Let \(\widehat{\nu}_{s}\) be the marginal distribution of \(x_{s}\) in Algorithm 1. When \(\eta B\log(1/\delta_{1})/p\leq 1\), we have \(\|\widehat{\nu}_{s}-\overline{\nu}_{s}\|_{TV}\leq 3(s-1)(2e+\log(1/\delta_{1})/p) B\delta_{0}\)._

We also require the following lemma which allows to build a coupling over multiple variables, such that the variables are as close as possible. This will be used to construct a coupling between the lazy algorithm and the L2P algorithm that runs it.

**Lemma 3.5** ([19]).: _Given a collection \(S\) of random variables, all absolutely continuous w.r.t. a common \(\sigma\)-finite measure. Then, there exists a coupling \(\Gamma\), such that for any variables \(X,Y\in S\), we have \(\Pr[X\neq Y]\leq\frac{2\|X-Y\|_{TV}}{1+\|X-Y\|_{TV}}\)._

We are now ready to prove Proposition 3.3

Proof.: Let \(\mathbf{Reg}_{T}^{\prime}\) denote the regret when the marginal distribution of \(x_{t}\) is \(\overline{\nu}_{t}\) instead of \(\widehat{\nu}_{t}\) induced in the Algorithm. Since each loss function is bounded,

\[\mathbf{Reg}_{T}\leq\mathbf{Reg}_{T}^{\prime}+B\sum_{s\in[T/B]}\|\overline{ \nu}_{s}-\widehat{\nu}_{s}\|_{TV}.\]By Lemma 3.4, we have

\[\mathbf{Reg}_{T} \leq\mathbf{Reg}_{T}^{\prime}+B\sum_{s\in[T/B]}3(s-1)(2/\eta+\log(1 /\delta_{1})/p)eB\delta_{0}\] \[\leq\mathbf{Reg}_{T}^{\prime}+8T^{2}\delta_{0}\log(1/\delta_{1})/\eta.\]

Thus, it now suffices to upper bound \(\mathbf{Reg}_{T}^{\prime}\).

Due to the preconditions that \(D_{\infty}^{\delta_{0}}(\overline{\mu}_{i+1},\overline{\mu}_{i})\leq\eta\) and \(\delta_{0}\leq\eta\), we know \(\|\overline{\mu}_{i+1}-\overline{\mu}_{i}\|_{TV}\leq 2\eta\). Recall that we assume \(x_{s}\sim\overline{\nu}_{s}\). Suppose \(z_{i}\) is the action taken by the input lazy algorithm \(\mathcal{A}\) for \(i\in[T]\) and the marginal distribution of \(z_{i}\) is \(\overline{\mu}_{i}\). By Lemma 3.5, we can construct a coupling \(\Gamma_{s}\) between \(x_{s}\) and \(\overline{z}\coloneqq(z_{(s-1)B+1},\cdots,z_{sB})\), such that

\[\Pr_{(x_{s},\overline{z})\sim\Gamma_{s}}[\exists i\in[(s-1)B+1,sB],z_{i}\neq x _{s}]\leq B\eta.\]

Letting \(I_{s}=\mathbf{1}(\exists i\in[(s-1)B+1,sB],z_{i}\neq x_{s})\), we have

\[\operatorname*{\mathbb{E}}_{x_{s}\sim\nu_{s}}\sum_{i=(s-1)B+1}^{ sB}\ell_{i}(x_{s}) =\operatorname*{\mathbb{E}}_{(x_{s},\overline{z})\sim\Gamma_{s}} \sum_{i=(s-1)B+1}^{sB}\ell_{i}(x_{s})\] \[=\operatorname*{\mathbb{E}}_{x_{s},\overline{z}\sim\Gamma_{s}}(1 -I_{s})\sum_{i=(s-1)B+1}^{sB}\ell_{i}(z_{i})\] \[\quad+\operatorname*{\mathbb{E}}_{x_{s},\overline{z}\sim\Gamma_ {s}}I_{s}\sum_{i=(s-1)B+1}^{sB}\ell_{i}(x_{s})\] \[\leq\operatorname*{\mathbb{E}}_{x_{s},\overline{z}\sim\Gamma_{s} }(1-I_{s})\sum_{i=(s-1)B+1}^{sB}\ell_{i}(z_{i})\] \[\quad+\operatorname*{\mathbb{E}}_{x_{s},\overline{z}\sim\Gamma_ {s}}I_{s}\sum_{i=(s-1)B+1}^{sB}(\ell_{i}(z_{i})+O(B\eta))\] \[\leq\operatorname*{\mathbb{E}}_{z_{i}\sim\overline{\mu}_{i}}\sum _{i=(s-1)B+1}^{sB}\ell_{i}(z_{i})+O(B\eta\cdot B^{2}\eta).\]

Hence we get \(\mathbf{Reg}_{T}^{\prime}\leq\mathbf{Reg}_{T}(\{\mu_{t}\}_{t=1}^{T})+\frac{T}{ B}\cdot O(B^{3}\eta^{2})\), which completes the proof. 

Now we turn to prove the privacy of \(\mathsf{L2P}\). We begin with the following lemma, which provides the privacy guarantees of sampling a new model \(x_{t}\) from the distribution \(\mu_{t}\). We defer the proof to Appendix B.

**Lemma 3.6**.: _Let \(\{\mu_{t}\}_{t=0}^{T}\) satisfy Assumption 3.1 where \(\eta\leq 1/10\). Then for any neighboring sequences \(\mathcal{S}\) and \(\mathcal{S}^{\prime}\) with corresponding \(\{\mu_{t}\}_{t=0}^{T}\) and \(\{\mu_{t}^{\prime}\}_{t=0}^{T}\) that differ one loss function, we have_

\[D_{\infty}^{4\delta_{0}}(\overline{\mu}_{t},\overline{\mu}_{t}^{\prime})\leq 2\eta.\]

We use correlated sampling in the algorithm rather than sampling from \(x_{t}\) directly. To this end, we need the following lemma, which provides upper and lower bounds on the ratio used for correlated sampling.

**Lemma 3.7**.: _For any \(s\in[T/B]\), if \(\eta B\log(1/\delta_{1})/p\leq 1\), then with probability at least \(1-(2/\eta+\log(1/\delta_{1})/p)\cdot eB\delta_{0}-\delta_{1}\),_

\[\frac{\nu_{s+1}(x_{s})}{\nu_{s}(x_{s})}\cdot\frac{\nu_{s}(y_{s})}{\nu_{s+1}(y_ {s})}\in[e^{-2B\eta},e^{2B\eta}].\]

The privacy proof will build on the previous two lemmas to control the privacy cost of updating the model and the cost of the switching time. We defer the proof to Appendix B.

One remaining issue is we need to conditional on the high probability events in Lemma 3.7 for the privacy guarantee and can not directly apply Advanced Composition (Lemma A.3). Now, we modify the Advanced Composition for our usage. In the classic \(k\)-fold adaptive composition experiment, the adversary, after getting the first \(i-1\) answers \(Y_{1},\cdots,Y_{i-1}\) (denoted by \(Y_{[i-1]}\) for simplicity), can output two datasets \(D_{i}^{0}\) and \(D_{i}^{1}\), a query \(q_{i}\), and receives the answer \(Y_{i}\sim\mathcal{M}_{i}(D_{i}^{b},q_{i})\) for the secret bit \(b\in\{0,1\}\). If each \(\mathcal{M}_{i}\) is \((\varepsilon_{i},\delta_{i})\)-DP, then the joint distributions over the answers \(Y_{[k]}\) satisfy the advanced composition theorem.

In our case, however, we know there exists a subset \(G_{i-1}(D_{[i-1]}^{b})\), such that with probability at least \(1-\lambda_{i}\), \(Y_{[i-1]}\in G_{i-1}(D_{[i-1]}^{b})\). Conditional on \(Y_{[i-1]}\in\cap_{b\in\{0,1\}}G_{i-1}(D_{[i-1]}^{b})\),

\[\mathcal{M}_{i}(D_{i}^{0},q_{i}\mid Y_{[i-1]}\in\cap_{b\in\{0,1\}}G_{i-1}(D_{ [i-1]}^{b}))\approx_{(\varepsilon_{i},\delta_{i})}\mathcal{M}_{i}(D_{i}^{1},q _{i}\mid Y_{[i-1]}\in\cap_{b\in\{0,1\}}G_{i-1}(D_{[i-1]}^{b}))\] (1)

Then we have the following lemma:

**Lemma 3.8**.: _Given the \(k\) mechanisms satisfying the Condition (1), then the class of mechanisms satisfy \((\tilde{\varepsilon}_{\tilde{g}},1-(1-\tilde{\delta})\Pi_{t\in[k]}(1-\tilde{ \delta}_{t}))+2\sum_{t\in[k]}\lambda_{t}\)-DP under \(k\)-fold adaptive composition, with \(\tilde{\varepsilon}_{\tilde{g}}\) defined in Equation (4)._

Proof.: Without losing generality, suppose we know the adversary and how they generate the databases and queries. We can construct a series of mechanisms \(\mathcal{M}_{i}^{\prime}\), such that \(\mathcal{M}_{i}^{\prime}\) draws \(Y_{i}\) from \(\mathcal{M}_{i}(D_{i}^{b},q_{i})\), and outputs \(Y_{i}\) if \(Y_{i}\in\cap_{b\in\{0,1\}}G_{i-1}(D_{[i-1]}^{b})\), and outputs \(\mathbf{0}\) otherwise. Let \((Y_{1,b}^{\prime},\cdots,Y_{k,b}^{\prime})\) be the outputs of \(\mathcal{M}_{i}^{\prime}\) with secret bit \(b\), and we know the TV distance between \((Y_{1,b}^{\prime},\cdots,Y_{k,b}^{\prime})\) and \((Y_{1,b},\cdots,Y_{k,b})\) is at most \(\sum_{t\in[k]}\lambda_{t}\) for any \(b\in\{0,1\}\). Moreover, we know

\[(Y_{1,0}^{\prime},\cdots,Y_{k,0}^{\prime})\approx_{\tilde{\varepsilon}_{\tilde {g}},1-(1-\tilde{\delta})\Pi_{t\in[k]}(1-\tilde{\delta}_{t}))}(Y_{1,1}^{\prime },\cdots,Y_{k,1}^{\prime})\]

by the advanced composition. The basic composition finishes the proof. 

### Application to DP-OPE

This section discusses the first application of our transformation to differentially private online prediction from experts (DP-OPE). Towards this end, we apply our transformation over the multiplicative weights algorithms [1], which can be made lazy as done in the shrinking dartboard algorithm [10]. It has the following measure at round \(t\)

\[\mu_{t}^{\text{mm}}(x)=e^{-\eta\sum_{i=1}^{t-1}\ell_{i}(x)}.\] (2)

The following proposition shows that this measure satisfies the desired properties required by our transformation. We let \(\overline{\mu}_{t}^{\text{mmw}}\) denote the density corresponding to \(\mu_{t}^{\text{mmw}}\).

**Lemma 3.9**.: _Assume \(\ell_{1},\dots,\ell_{T}\) where \(\ell_{t}:[d]\rightarrow[0,1]\). Then we have that_

1. \(D_{\infty}^{\delta_{0}}(\overline{\mu}_{t+1}^{\text{mmw}},\overline{\mu}_{t}^ {\text{mmw}})\leq\eta\) _with_ \(\delta_{0}=0\)_._
2. \(\frac{\mu_{t+1}^{\text{mmw}}(x)}{\mu_{t}^{\text{mmw}}(x)}=e^{-\eta\ell_{t}(x)}\) _for all_ \(x\in[d]\)_._

Proof.: The first item follows from the guarantees of the exponential mechanism as \(\ell_{t}(x)\in[0,1]\) for all \(x\in[d]\). The second item follows immediately from the definition of \(\mu^{\text{mmw}}\). 

Having proved our desired properties, our transformation now gives the following theorem.

**Theorem 3.10** (DP-OPE).: _Let \(\ell_{1},\dots,\ell_{T}\) where \(\ell_{t}:[d]\rightarrow[0,1]\). Setting \(B=1/\varepsilon\) and \(\eta=\min(\varepsilon_{0},\varepsilon)^{2/3}/T^{1/3}\) where \(\varepsilon_{0}=T^{-1/4}\log^{3/4}d\), the \(\mathsf{L2P}\) transformation (Algorithm 1) applied with the measure \(\{\mu_{t}^{\text{mmw}}\}_{t=1}^{T}\) is \((\varepsilon,\delta)\)-DP and has regret_

\[\mathbf{Reg}_{T}=O\left(\sqrt{T\log d}+\frac{T^{1/3}\log d}{\varepsilon^{2/3}} \right).\]Proof.: First, based on theorem 3.2, note that the setting of \(B=1/\varepsilon\) and \(\eta\leq\min(\varepsilon_{0},\varepsilon)^{2/3}/T^{1/3}\) where \(\varepsilon_{0}=T^{-1/4}\log^{3/4}d\) guarantee the algorithm is \((\varepsilon,\delta)\)-DP.

To upper bound the regret, we use existing guarantees of the multiplicative weights algorithm [1], combined with Theorem 3.2 to get that the regret is

\[\mathbf{Reg}_{T} \leq O\left(\eta T+\frac{\log(d)}{\eta}+TB^{2}\eta^{2}\right)\] \[\leq O\left(\eta T+\frac{\log(d)}{\eta}+\frac{T\eta^{2}}{ \varepsilon^{2}}\right)\] \[\leq O\left((T\varepsilon_{0})^{2/3}+\frac{T^{1/3}\log(d)}{ \varepsilon^{2/3}}+\frac{T^{1/3}}{\varepsilon^{2/3}}\right)\] \[\leq O\left(\sqrt{T\log d}+\frac{T^{1/3}\log(d)}{\varepsilon^{2/3 }}\right),\]

where the second inequality follows by setting \(B=1/\varepsilon\), and the third inequality follows by setting \(\eta\leq\min(\varepsilon_{0},\varepsilon)^{2/3}/T^{1/3}\), and the last inequality follows since \(\varepsilon_{0}=T^{-1/4}\log^{3/4}d\). 

### Application to DP-OCO

In this section, we use our transformation for differentially private online convex optimization (DP-OCO) using the regularized multiplicative weights algorithm [1], which has the following measure

\[\mu_{t}^{\text{rmw}}(x)=e^{-\beta\left(\sum_{i-1}^{t-1}\ell_{i}(x)+\lambda \|x\|_{2}^{2}\right)}.\] (3)

Letting \(\overline{\mu}^{\text{rmw}}\) denote the corresponding density function, we have the following properties.

**Lemma 3.11**.: _Assume \(\ell_{1},\ldots,\ell_{T}:\mathcal{X}\to\mathbb{R}\) be convex and \(L\)-Lipschitz functions. Then we have that_

1. \(D_{\infty}^{\delta_{0}}(\overline{\mu}_{t+1}^{\text{rmw}},\overline{\mu}_{t}^ {\text{rmw}})\leq\eta\) _where_ \(\eta=\frac{2\beta L^{2}}{\lambda}+\sqrt{\frac{8\beta L^{2}\log(2/\delta_{0})}{ \lambda}}\)_._
2. \(\frac{\mu_{t+1}^{\text{rmw}}(x)}{\mu_{t}^{\text{rmw}}(x)}=e^{-\beta\ell_{1}(x )}\) _for all_ \(x\in\mathcal{X}\)_._

Proof.: The first item follows from Lemma 3.5 in [1, 1]. The second item follows immediately from the definition of \(\mu_{t}^{\text{rmw}}\). 

Combining these properties with our transformation, we get the following result.

**Theorem 3.12** (D-Oco).: _Let \(\ell_{1},\ldots,\ell_{T}:\mathcal{X}\to\mathbb{R}\) be convex and \(L\)-Lipschitz functions. Setting \(B=\frac{1}{2\varepsilon\log(1/\delta)}\), \(\lambda=\frac{L}{D}\max\{\sqrt{T},\frac{\sqrt{d\log T}}{\eta}\}\), \(\beta=\eta^{2}\lambda/20L^{2}\), \(\eta=\frac{\varepsilon^{2/3}}{T^{1/3}\log(T/\delta)}\) and \(p=\eta/\varepsilon\), the \(L2P\) transformation (Algorithm 1) applied with the measure \(\{\mu_{t}^{\text{rmw}}\}_{t=1}^{T}\) is \((\varepsilon,\delta)\)-DP and has regret_

\[\mathbf{Reg}_{T}=LD\cdot O\left(\sqrt{T}+\frac{T^{1/3}\sqrt{d\log T}\log(T/ \delta)}{\varepsilon^{2/3}}\right).\]

Proof.: First, based on Theorem 3.2, note that there are three constraints to make the algorithm private:

\[\eta/p\leq\varepsilon/2,\qquad\eta\sqrt{Tp\log(1/\delta)/B}\leq\varepsilon/2,\qquad\eta B\log(1/\delta)/p\leq 1.\]

Setting of \(B=\frac{1}{2\varepsilon\log(1/\delta)}\), \(\lambda=\frac{L}{D}\max\{\sqrt{T},\frac{\sqrt{d\log T}}{\eta}\}\), \(\beta=\eta^{2}\lambda/20L^{2}\), \(\eta=\frac{\varepsilon^{2/3}}{T^{1/3}\log(T/\delta)}\) and \(p=\eta/\varepsilon\) guarantees the algorithm is \((\varepsilon,\delta)\)-DP.

For utility, we use theorem 3.2 with the existing regret bounds for the regularized multiplicative weights algorithm (Theorem 4.1 in [1]) to get that the algorithm has regret

\[\mathbf{Reg}_{T}\leq O\left(\lambda D^{2}+\frac{L^{2}T}{\lambda}+\frac{d\log( T)}{\beta}+LDTB^{2}\eta^{2}\right)\]\[\leq O\left(LD\sqrt{T}+\lambda D^{2}+\frac{L^{2}d\log T}{\lambda\eta^{2 }}+LDTB^{2}\eta^{2}\right)\] \[\leq LD\cdot O\left(\sqrt{T}+\frac{T^{1/3}\sqrt{d\log T}\log(T/ \delta)}{\varepsilon^{2/3}}\right).\]

## 4 Lower bound for low-switching private algorithms

In this section, we prove a lower bound for DP-OPE for a natural family of private low-switching algorithms that contains most of the existing low-switching private algorithms such as our algorithms and the ones in [1, 2]. Our lower bound matches our upper bounds for DP-OPE and suggests that new techniques beyond limited switching are required in order to obtain faster rates.

For our lower bounds, we will assume that the algorithm satisfies the following condition:

**Condition 4.1**.: _(Limited switching algorithms) The online algorithm \(\mathsf{ALG}\) works as follows: at each round \(t\), \(\mathsf{ALG}\) is allowed to either set \(x_{t+1}=x_{t}\) or sample \(x_{t+1}\sim\mu_{t+1}\) where \(\mu_{t+1}\) is a function of \(\ell_{1},\ldots,\ell_{t}\) and is supported over \(\mathcal{X}\). The algorithm releases the resampling rounds \(\{t_{1},\ldots,t_{S}\}\) and models \(\{x_{t_{1}},\ldots,x_{t_{S}}\}\)._

Our lower bound will hold for algorithms that satisfy concentrated differential privacy. We use this notion as it allows to get tight characterization of the composition of private algorithms and in most settings have similar rates to approximate differential privacy. We can also prove a tight lower bound for pure differential privacy using the same techniques. We have the following lower bound for concentrated DP. We defer the proof to Appendix C.

**Theorem 4.2**.: _Let \(T\geq 1\) and \(\varepsilon\geq 100\log^{3/2}(dT)/T\). If an algorithm \(\mathsf{ALG}\) satisfies Condition 4.1 and is \(\varepsilon^{2}\)-CDP, then there exists an oblivious adversary that chooses \(\ell_{1},\ldots,\ell_{T}:[d]\to[0,1]\) such that the regret is lower bounded by_

\[\mathbf{Reg}_{T}\geq\Omega\left(\sqrt{T}+\frac{T^{1/3}}{\varepsilon^{2/3}} \right).\]

Finally, we note that this lower bound only holds for switching-based algorithms: indeed, the binary-tree-based algorithm of [1] obtains regret \(\sqrt{d}\log(d)/\varepsilon\) which is better in the low-dimensional regime. This motivates the search for new strategies beyond limited switching for the high-dimensional regime.

## 5 Conclusion

In this paper, we proposed a new transformation that allows the conversion of lazy online learning algorithms into private algorithms and demonstrates two applications (DP-OPE and DP-OCO) where this transformation offers significant improvements over prior work. Moreover, for DP-OPE, we show a lower bound for natural low-switching-based private algorithms, which shows that new techniques are required for low-switching algorithms to improve our transformation's regret. This begs the question of whether the same lower bound holds for all algorithms or whether a different strategy that breaks the low-switching lower bound exists. As for DP-OCO, it is interesting to see whether better upper or lower bounds can be obtained. The current normalized regret, omitting logarithmic terms, is proportional to \(\sqrt{d}/(\varepsilon T)^{2/3}\). This is different than most applications in private optimization where the normalized error is usually a function of \(\sqrt{d}/(\varepsilon T)\). Hence, it is natural to conjecture that the normalized regret can be improved to \(d^{1/3}/(\varepsilon T)^{2/3}\).

## References

* [AFKT23a] Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar. Near-optimal algorithms for private online optimization in the realizable regime. _Proceedings of the 40th International Conference on Machine Learning_, 2023.

* [AFKT23b] Hilal Asi, Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private online prediction from experts: Separations and faster rates. _Proceedings of the Thirty Sixth Annual Conference on Computational Learning Theory_, 2023.
* [AHK12] Sanjeev Arora, Elad Hazan, and Satyen Kale. The multiplicative weights update method: a meta algorithm and applications. _Theory of Computing_, 8(1):121-164, 2012.
* [AKST23a] Naman Agarwal, Satyen Kale, Karan Singh, and Abhradeep Thakurta. Differentially private and lazy online convex optimization. In _The Thirty Sixth Annual Conference on Learning Theory_, pages 4599-4632. PMLR, 2023.
* [AKST23b] Naman Agarwal, Satyen Kale, Karan Singh, and Abhradeep Thakurta. Improved differentially private and lazy online convex optimization. _arXiv:2312.11534 [cs.CR]_, 2023.
* [AS17] Naman Agarwal and Karan Singh. The price of differential privacy for online learning. In _Proceedings of the 34th International Conference on Machine Learning_, pages 32-40, 2017.
* [AS19] Omer Angel and Yinon Spinka. Pairwise optimal coupling of multiple random variables. _arXiv preprint arXiv:1903.00632_, 2019.
* [AT18] Jason Altschuler and Kunal Talwar. Online learning over a finite action set with limited switching. In _Proceedings of the Thirty First Annual Conference on Computational Learning Theory_, 2018.
* 14th International Conference, TCC 2016-B, Proceedings, Part I_, volume 9985 of _Lecture Notes in Computer Science_, pages 635-658, 2016.
* [CYLK20] Lin Chen, Qian Yu, Hannah Lawrence, and Amin Karbasi. Minimax regret of switching-constrained online convex optimization: No phase transition. _Advances in Neural Information Processing Systems_, 33:3477-3486, 2020.
* [GLL22] Sivakanth Gopi, Yin Tat Lee, and Daogao Liu. Private convex optimization via exponential mechanism. In _Conference on Learning Theory_, pages 1948-1989. PMLR, 2022.
* [GVW10] Sascha Geulen, Berthold Vocking, and Melanie Winkler. Regret minimization for online buffering problems using the weighted majority algorithm. In _Proceedings of the Twenty Third Annual Conference on Computational Learning Theory_, 2010.
* [JKT12] Prateek Jain, Pravesh Kothari, and Abhradeep Thakurta. Differentially private online learning. In _Proceedings of the Twenty Fifth Annual Conference on Computational Learning Theory_, 2012.
* [JT14] Prateek Jain and Abhradeep Thakurta. (Near) dimension independent risk bounds for differentially private learning. In _Proceedings of the 31st International Conference on Machine Learning_, pages 476-484, 2014.
* [KMS\({}^{+}\)21] Peter Kairouz, Brendan McMahan, Shuang Song, Om Thakkar, Abhradeep Thakurta, and Zheng Xu. Practical and private (deep) learning without sampling or shuffling. _arXiv:2103.00039 [cs.CR]_, 2021.
* [KOV15] Peter Kairouz, Sewoong Oh, and Pramod Viswanath. The composition theorem for differential privacy. In _International conference on machine learning_, pages 1376-1385. PMLR, 2015.
* [KV05] A. Kalai and S. Vempala. Efficient algorithms for online decision problems. _Journal of Computer and System Sciences_, 71(3):291-307, 2005.
* [SK21] Uri Sherman and Tomer Koren. Lazy oco: Online convex optimization on a switching budget. In _Proceedings of the Thirty Fourth Annual Conference on Computational Learning Theory_, 2021.

* [ST13] Adam Smith and Abhradeep Thakurta. (Nearly) optimal algorithms for private online learning in full-information and bandit settings. In _Advances in Neural Information Processing Systems 26_, 2013.

Missing details for preliminaries

In this section, we provide additional preliminaries and provide missing details for some of the results in the preliminaries section.

For our lower bounds, we require the notion of concentrated differential privacy. To this end, we first define the \(\alpha\)-Renyi divergence (\(\alpha>1\)) between two probability measures:

\[D_{\alpha}(\mu\|\nu)\coloneqq\frac{1}{\alpha-1}\log\left(\int\left(\frac{\mu( \omega)}{\nu(\omega)}\right)^{\alpha}\mathrm{d}\nu(\omega)\right).\]

Concentrated DP is defined below:

**Definition A.1** (concentrated DP).: Let \(\rho\geq 0\). We say an algorithm ALG satisfies \(\rho\)-concentrated differential privacy (\(\rho\)-CDP) against oblivious adversaries if for any neighboring sequences \(\mathcal{S}=(\ell_{1},\ldots,\ell_{T})\in\mathcal{L}^{\mathcal{T}}\) and \(\mathcal{S}^{\prime}=(\ell^{\prime}_{1},\ldots,\ell^{\prime}_{T})\in\mathcal{ L}^{T}\) that differ in a single element, and any \(\alpha\geq 1\), \(D_{\alpha}(\mathsf{ALG}(\mathcal{D})\|\mathsf{ALG}(\mathcal{D}^{\prime})) \leq\alpha\rho\).

Now we list a few standard results from the privacy literature that we use in the paper, namely group privacy and privacy composition.

**Lemma A.2**.: _(Group Privacy) Let \(\mathsf{ALG}\) be an \((\varepsilon,\delta)\)-DP algorithm and let \(\mathcal{S},\mathcal{S}^{\prime}\mathcal{L}^{T}\) be two datasets that differ in \(k\) elements. Then for any measurable set \(S\) in the output space of \(\mathsf{ALG}\)_

\[\Pr[\mathsf{ALG}(\mathcal{S})\in\mathcal{O}]\leq e^{k\varepsilon}\Pr[ \mathsf{ALG}(\mathcal{S}^{\prime})\in\mathcal{O}]+ke^{(k-1)\varepsilon}\delta.\]

**Lemma A.3** (Advanced Composition,[1]).: _For any \(\varepsilon_{t}>0,\delta_{t}\in(0,1)\) for \(t\in[k]\), and \(\tilde{\delta}\in(0,1)\), the class of \((\varepsilon_{t},\delta_{t})\)-DP mechanisms satisfy \((\tilde{\varepsilon}_{\tilde{\delta}},1-(1-\tilde{\delta})\Pi_{t\in[k]}(1- \tilde{\delta}_{t}))\)-DP under \(k\)-fold adaptive composition, for_

\[\tilde{\varepsilon}_{\tilde{\delta}}=\sum_{t\in[k]}\varepsilon_{t}+\min\left\{ \sqrt{\sum_{t\in[k]}2\varepsilon_{t}^{2}\log(e+\frac{\sqrt{\sum_{t\in[k]} \varepsilon_{t}^{2}}}{\tilde{\delta}})},\sqrt{\sum_{t\in[k]}2\varepsilon_{t}^{ 2}\log(1/\tilde{\delta})}\right\}.\] (4)

Moreover, we use the fact that distributions with bounded \(D_{\infty}^{\delta}\) satisfy the following property.

**Lemma A.4**.: _Let \(\varepsilon\leq 1/10\). If \(D_{\infty}^{\delta}(\mu,\nu)\leq\varepsilon/2\) then we have_

\[\Pr_{X\sim\mu}\left[e^{-\varepsilon}\leq\frac{\mu(X)}{\nu(X)}\leq e^{ \varepsilon}\right]\geq 1-6\delta/\varepsilon\text{ and }\Pr_{X\sim\nu}\left[e^{-\varepsilon}\leq\frac{\mu(X)}{\nu(X)}\leq e^{ \varepsilon}\right]\geq 1-6\delta/\varepsilon.\]

Finally, we have the following standard conversion from \(\rho\)-concentrated DP to \((\varepsilon,\delta)\)-DP.

**Lemma A.5** ([1]).: _If \(\mathsf{ALG}\) is \(\rho\)-CDP with \(\rho\leq 1\), then it is \((3\sqrt{\rho\log(1/\delta)},\delta)\)-DP for all \(\delta\in(0,1/4)\)._

### Proof of Lemma a.4

Let \(S^{\prime}=\{X:\frac{\mu(X)}{\nu(X)}\in[e^{-\varepsilon},e^{\varepsilon}]\}\), and assume towards a contradiction that \(\mu(S^{\prime})<1-6\delta/\varepsilon\). Then there is a set \(\mathcal{S}=\overline{S^{\prime}}\), such that \(\mu[S]>6\delta/\varepsilon\). We divide \(S\) by letting \(S_{1}=\{X\in S:\mu(X)/\nu(X)>e^{\varepsilon}\}\) and \(S_{2}=S\setminus S_{1}\).

**Case (1):**\(\mu(S_{1})\geq\mu(S)/2\geq 3\delta/\varepsilon\). Then we know

\[\ln\frac{\mu(S_{1})-\delta}{\nu(S_{1})}>\ln e^{\varepsilon}\frac{\mu(S_{1})- \delta}{\mu(S_{1})}\geq\ln e^{\varepsilon}\frac{3/\varepsilon-1}{3/\varepsilon }\geq\ln e^{\varepsilon/2}=\varepsilon/2,\]

where we use that \(\mu(S_{1})\geq e^{\varepsilon}\nu(S_{1})\) and \(1-\varepsilon/3\geq e^{-\varepsilon/2}\) for \(\varepsilon<1/10\). This is a contradiction.

**Case (2):** Suppose \(\mu(S_{2})\geq 3\delta/\varepsilon\). We know \(S_{2}=\{X\in S:\mu(X)/\nu(X)<e^{-\varepsilon}\}=\{X\in S:\nu(X)/\mu(X)>e^{ \varepsilon}\}\). Then we know \(\nu(S_{2})\geq e^{\varepsilon}\mu(S_{2})\geq 3e^{\varepsilon}\delta/\varepsilon\). Similarly, we have

\[\ln\frac{\nu(S_{2})-\delta}{\mu(S_{2})}>\varepsilon/2.\]

This is a contradiction as well and proves the statement.

[MISSING_PAGE_FAIL:14]

\[=1+\frac{(1-p)^{2}(\frac{\nu^{\prime}_{t_{0}+1}(x^{\prime}_{t_{0}})}{e^{ 2\eta\nu^{\prime}_{t_{0}}(x^{\prime}_{t_{0}})}})\frac{\nu^{\prime}_{t_{0}}(y^{ \prime}_{t_{0}})}{\nu^{\prime}_{t_{0}+1}(y^{\prime}_{t_{0}})}-\frac{\nu_{t_{0}+ 1}(x_{t_{0}})}{e^{2\eta\nu_{t_{0}}(x_{t_{0}})}\frac{\nu_{t_{0}}(y_{t_{0}})}{\nu _{t_{0}+1}(y^{\prime}_{t_{0}})}})}{1-(1-p)^{2}+(1-p)^{2}(1-\frac{\nu^{\prime}_{ t_{0}+1}(x^{\prime}_{t_{0}})}{e^{2\eta\nu_{t_{0}}(x^{\prime}_{t_{0}})}\frac{ \nu^{\prime}_{t_{0}}(y^{\prime}_{t_{0}})}{\nu^{\prime}_{t_{0}+1}(y^{\prime}_{ t_{0}})})}}\] \[\leq 1+\frac{e^{2\eta}-1}{p}\leq e^{2\eta/p}.\]

Case 3\((t>t_{0})\):As for the case when \(t>t_{0}\), when \(\zeta_{t}=0\) (\(A_{t+1}=S_{t+1}=S_{t+1}^{\prime}=1\)), the variables are 0-indistinguishable since \(x_{t}=x_{t-1}\) and \(y_{t}=y_{t-1}\) in this case. Consider the remaining possibility. Given the assumption that \(\mu_{t+1}/\mu_{t}\) is a function of \(\ell_{t}\), for any possible \(\Sigma\), we have

\[\Pr[\zeta_{t}=1\mid\Sigma_{t-1}=\Sigma]=\Pr[\zeta^{\prime}_{t}=1\mid\Sigma^{ \prime}_{t-1}=\Sigma].\]

For any set \(S\), by the assumption on \(\overline{\mu}_{t}\), we have

\[\Pr[\zeta_{t}=1,(x_{t},y_{t})\in S\mid\Sigma_{t-1}=\Sigma]\] \[= \Pr[(x_{t},y_{t})\in S\mid\Sigma_{t-1}=\Sigma,\zeta_{t}=1]\Pr[ \zeta_{t}=1\mid\Sigma_{t-1}=\Sigma]\] \[= \Pr[(x_{t},y_{t})\in S\mid\Sigma_{t-1}=\Sigma,\zeta_{t}=1]\Pr[ \zeta^{\prime}_{t}=1\mid\Sigma^{\prime}_{t-1}=\Sigma]\] \[\leq e^{2\eta}\Pr[(x^{\prime}_{t},y^{\prime}_{t})\in S\mid\Sigma^{ \prime}_{t-1}=\Sigma,\zeta^{\prime}_{t}=1]\Pr[\zeta^{\prime}_{t}=1\mid\Sigma^{ \prime}_{t-1}=\Sigma]+4\delta_{0}\] \[= e^{2\eta}\Pr[\zeta^{\prime}_{t}=1,(x^{\prime}_{t},y^{\prime}_{t })\in S\mid\Sigma^{\prime}_{t-1}=\Sigma]+4\delta_{0},\]

where the inequality comes from Lemma 3.6 by the divergence bound between \(\overline{\mu}_{t}\) and \(\overline{\mu}^{\prime}_{t}\). This completes the proof of Equation (5).

The final privacy guarantee follows from combining Equation (5) and Advanced composition (Lemma A.3).

### Proof of Lemma 3.4

We prove this statement by induction. For \(t=1\), the statement is obviously correct. We assume \(\|\widehat{\nu}_{t}-\overline{\nu}_{t}\|_{TV}\leq 3(t-1)(2(e/\eta+\log(1/\delta_{1})/p)B \delta_{0}+\delta_{1})\) prove that \(\|\widehat{\nu}_{t+1}-\overline{\nu}_{t+1}\|_{TV}\leq 3t(2e+\log(1/\delta_{0})/p)B \delta_{0}\).

Let \(X_{good}:=\{x:\log\frac{\overline{\nu}_{t+1}(x)}{\overline{\nu}_{t}(x)}\in[- B\eta,B\eta]\}\) and \(Y_{good}:=\{y:\log\frac{\overline{\nu}_{t}(y)}{\overline{\nu}_{t+1}(y)}\leq[- B\eta,B\eta]\}\). Let \(\widehat{\varphi}_{t}(y)\) be the distribution of \(y_{t}\). Note that the distribution of \(y_{t}\) is independent of \(\{x_{\tau}\}_{\tau\in[T/B]}\), while the distribution of \(x_{t+1}\) is independent of \(y_{t+1}\) but depends on \(y_{t}\). By the assumption and group privacy, we know \(D_{\infty}^{Be^{B\eta}\delta_{0}}(\overline{\nu}_{t+1},\overline{\nu}_{t})\leq B\eta\), and hence we have

\[\nu_{t}(Y_{good}^{\complement})\leq e^{B\eta}\delta_{0}/\eta\leq 2e\delta_{0}/\eta.\]

Let \(t_{0}\leq t\) be largest integer such that \(A_{t_{0}}=1\), that is, \(y_{t}\) is sampled from \(\overline{\nu}_{t_{0}}\) for some random \(t_{0}\leq t\). We have

\[\nu_{t_{0}}(Y_{good}^{\complement})\leq e^{B\eta(t-t_{0})}\cdot\nu_{t}(Y_{good })+(t-t_{0})B\delta_{0}e^{B\eta(t-t_{0})}.\]

With probability at least \(1-\delta_{1}\), we know \(|t-t_{0}|\leq\log(1/\delta_{1})/p\). Hence we get

\[\Pr_{y\sim\widehat{\varphi}_{t}}[y\in Y_{good}]\geq 1-2(e/\eta+\log(1/\delta_{1})/p)B \delta_{0}-\delta_{1}.\]

We know

\[\Pr_{x\sim\overline{\nu}_{t},y\sim\widehat{\varphi}_{t}}[x\in X_{ good},y\in Y_{good}]\] \[\quad=\Pr_{y\sim\widehat{\varphi}_{t}}[y\in Y_{good}]\Pr_{x\sim \overline{\nu}_{t}}[x\in X_{good}\mid y\in Y_{good}]\] \[\quad\geq 1-2(e/\eta+\log(1/\delta_{1})/p)B\delta_{0}-\delta_{1}.\]

Denote the good set

\[S_{good}=\big{\{}(x,y):x\in X_{good},Y_{good}\big{\}}.\]Let \(\tilde{\varphi}_{t}\) be the distribution of \(y_{t}\) conditional on \(y_{t}\in Y_{good}\). Let \(\widehat{\Gamma}_{t}\) be the marginal distribution over \((x_{t},y_{t})\), that is \(x_{t}\sim\widehat{\nu}_{t}\) and \(y_{t}\sim\widehat{\varphi}_{t}\). Let \(\Gamma_{t}\) be the distribution over \((x_{t},y_{t})\) where \(x_{t}\sim\overline{\nu}_{t},y_{t}\sim\tilde{\varphi}_{t}\), and \(\overline{\Gamma}_{t}\) be the distribution of \(\Gamma_{t}\) conditional on \((x_{t},y_{t})\in S_{good}\).

We know \(\|\widehat{\Gamma}_{t}-\overline{\Gamma}_{t}\|_{TV}\leq(2e/\eta+\log(1/\delta _{1})/p)B\delta_{0}(3t-2)\). Let \(\overline{q}_{t+1}\) be the distribution of \(x_{t+1}\) if \((x_{t},y_{t})\) is sampled from \(\overline{\Gamma}_{t}\) instead of \(\widehat{\Gamma}_{t}\). By the property that post-processing does not increase the TV distance, we know

\[\|\overline{q}_{t+1}-\widehat{\nu}_{t+1}\|_{TV}\leq\|\widehat{\Gamma}_{t}- \overline{\Gamma}_{t}\|_{TV}.\]

Now it suffices to bound the TV distance between \(\overline{q}_{t+1}\) and \(\overline{\nu}_{t+1}\).

For any set \(E\), we have

\[\overline{q}_{t+1}(E)= \int(\Pr[S^{\prime}_{t}=0,x_{t+1}\in E\mid x_{t}=x,y_{t}=y]\] \[+\Pr[S^{\prime}_{t}=1,S_{t}=0,x_{t+1}\in E\mid x_{t}=x,y_{t}=y]\] \[+\Pr[S^{\prime}_{t}=1,S_{t}=1,x_{t+1}\in E\mid x_{t}=x,y_{t}=y] )\overline{\Gamma}_{t}(x,y)\mathrm{d}(x,y)\] \[=p\overline{\nu}_{t+1}(E)+(1-p)\overline{\nu}_{t+1}(E)\int(1- \frac{\nu_{t+1}(x)}{e^{2B\eta}\nu_{t}(x)}\cdot\frac{\nu_{t}(y)}{\nu_{t+1}(y)}) \overline{\Gamma}_{t}(x,y)\mathrm{d}(x,y)\] \[+(1-p)\int_{\mathbf{1}(x\in E)}\frac{\nu_{t+1}(x)}{e^{2B\eta}\nu _{t}(x)}\cdot\frac{\nu_{t}(y)}{\nu_{t+1}(y)}\overline{\Gamma}_{t}(x,y)\mathrm{ d}(x,y).\]

Thus we have

\[|\overline{q}_{t+1}(E)-\overline{\nu}_{t+1}(E)|\leq \Big{|}\int_{\mathbf{1}(x\in E)}\frac{\overline{\nu}_{t+1}(x)}{e^ {2B\eta}\overline{\nu}_{t}(x)}\cdot\frac{\overline{\nu}_{t}(y)}{\overline{ \nu}_{t+1}(y)}\overline{\Gamma}_{t}(x,y)\mathrm{d}(x,y)\] \[-\overline{\nu}_{t+1}(E)\int\frac{\overline{\nu}_{t+1}(x)}{e^{2B \eta}\overline{\nu}_{t}(x)}\cdot\frac{\overline{\nu}_{t}(y)}{\overline{\nu}_{ t+1}(y)}\overline{\Gamma}_{t}(x,y)\mathrm{d}(x,y)\Big{|}.\]

Note that for any \((x,y)\in S_{good}\), we have

\[\overline{\Gamma}_{t}(x,y)=\frac{\overline{\nu}_{t}(x)\tilde{\varphi}_{t}(y) }{\Gamma_{t}(S_{good})}.\]

Fixing any \(y\), we know the above term is bounded by

\[|\frac{\overline{\nu}_{t}(y)}{e^{2B\eta}\overline{\nu}_{t+1}(y)\Gamma_{t}(S_{ good})}(\overline{\nu}_{t+1}(E\cap X_{good})-\overline{\nu}_{t+1}(E)\overline{\nu}_{ t+1}(X_{good}))|\leq 2(e/\eta+B\log(1/\delta_{1})/p)\delta_{0},\]

where the last inequality follows from \(\overline{\nu}_{t+1}(X_{good})\geq 1-B\delta_{0}\). Hence, we prove that

\[\|\overline{q}_{t+1}-\overline{\nu}_{t+1}\|_{TV}\leq 2(e/\eta+\log(1/\delta_{1})/p)B \delta_{0}.\]

This suggests that

\[\|\widehat{\nu}_{t+1}-\overline{\nu}_{t+1}\|_{TV}\leq\|\widehat{\nu}_{t+1}- \overline{q}_{t+1}\|_{TV}+\|\overline{q}_{t+1}-\overline{\nu}_{t+1}\|_{TV}\leq 6 t(e/\eta+\log(1/\delta_{1})/p)B\delta_{0}.\]

### Proof of Lemma 3.6

Let \(\mathcal{S}=(\ell_{1},\ldots,\ell_{T})\) and \(\mathcal{S}^{\prime}=(\ell_{1}^{\prime},\ldots,\ell_{T}^{\prime})\) differ in a single round \(t_{0}\). We fix \(t\) and prove the claim is correct. If \(t\leq t_{0}\), then the claim clearly holds as \(\overline{\mu}_{t}=\overline{\mu}_{t}^{\prime}\). For \(t=t_{0}+1\), note that Assumption 3.1 implies that \(D_{\infty}^{\delta_{0}}(\overline{\mu}_{t_{0}+1},\overline{\mu}_{t_{0}})\leq\eta\) and \(D_{\infty}^{\delta_{0}}(\overline{\mu}_{t_{0}+1}^{\prime},\overline{\mu}_{t_{0}})\leq\eta\), hence by group privacy we get that \(D_{\infty}^{(e^{\eta}+1)\delta_{0}}(\overline{\mu}_{t},\overline{\mu}_{t}^{ \prime})\leq 2\eta\). Finally, for \(t>t_{0}+1\), note that Assumption 3.1 implies that \(\mu_{t}=\mu_{0}\cdot\mathsf{func}(\ell_{1})\cdot\mathsf{func}(\ell_{2})\cdots \mathsf{func}(\ell_{t-1})\) and \(\mu_{t}^{\prime}=\mu_{0}\cdot\mathsf{func}(\ell_{1}^{\prime})\cdot\mathsf{func }(\ell_{2}^{\prime})\cdots\mathsf{func}(\ell_{\ell-1}^{\prime})\). Thus, swapping the losses at rounds \(t-1\) and \(t_{0}\) results in the same distributions \(\mu_{t}\) and \(\mu_{t}^{\prime}\), therefore privacy follows from the same arguments as the case when \(t=t_{0}+1\). The final claim follows as \(e^{\eta}+1\leq 4\).

### Proof of Lemma 3.7

To prove lemma 3.7, we first prove the same result under a simpler setting where \(x_{t}\sim\nu_{t}\) and \(y_{t}\sim\nu_{t}\).

**Lemma B.1**.: _For any \(0\leq t\leq T/B-1\), if \(B\eta\leq 1/20\), \(x_{t}\sim\nu_{t}\) and \(y_{t}\sim\nu_{t}\) independently, then with probability at least \(1-6e^{B\eta}\delta_{0}/\eta\)._

\[\frac{\nu_{t+1}(x_{t})}{\nu_{t}(x_{t})}\cdot\frac{\nu_{t}(y_{t})}{\nu_{t+1}(y_ {t})}\in[e^{-2B\eta},e^{2B\eta}]\]

Proof.: Let \(Z_{t}=\int\nu_{t}(x)\mathrm{d}x\). We know \(\overline{\nu}_{t}=\nu_{t}/Z_{t}\) by our notation. Then we have that

\[\frac{\nu_{t+1}(x_{t})}{\nu_{t}(x_{t})}\cdot\frac{\nu_{t}(y_{t})} {\nu_{t+1}(y_{t})} =\frac{\nu_{t+1}(x_{t})Z_{t}}{\nu_{t}(x_{t})Z_{t+1}}\cdot\frac{ \nu_{t}(y_{t})Z_{t+1}}{\nu_{t+1}(y_{t})Z_{t}}\] \[=\frac{\overline{\nu}_{t+1}(x_{t})}{\overline{\nu}_{t}(x_{t})} \cdot\frac{\overline{\nu}_{t}(y_{t})}{\overline{\nu}_{t+1}(y_{t})}.\]

The statement follows from the Assumption 3.1 and the group privacy

\[D_{\infty}^{Be^{B\eta}\delta_{0}}(\overline{\nu}_{t+1},\overline{\nu}_{t}) \leq B\eta.\]

Then the statement follows from Lemma A.4, the independence between \(x_{t},y_{t}\) and Union bound. 

We are now ready to prove Lemma 3.7.

Proof.: Fix any \(t\). Let \(t_{0}\leq t\) be largest integer such that \(A_{t_{0}}=1\), that is, \(y_{t}\) is sampled from \(\overline{\nu}_{t_{0}}\) for some random \(t_{0}\leq t\). By the group privacy, we know \(D_{\infty}^{Be^{B\eta(t-t_{0})}\delta_{0}(t-t_{0})}(\nu_{t},\nu_{t_{0}})\leq B \eta(t-t_{0})\).

Define the bad set

\[S_{bad}=\{y:\frac{\nu_{t+1}(x)}{\nu_{t}(x)}\cdot\frac{\nu_{t}(y)}{\nu_{t+1}( y)}\notin[e^{-2B\eta},e^{2B\eta}],x\sim\nu_{t}\}.\]

By Lemma B.1, we know

\[\nu_{t}(y\in S_{bad})\leq 6e^{B\eta}\cdot\delta_{0}/\eta.\]

Therefore, we have that

\[\nu_{t_{0}}(y\in S_{bad}) \leq e^{B\eta(t-t_{0})}\cdot\nu_{t}(y\in S_{bad})+(t-t_{0})B \delta_{0}e^{B\eta(t-t_{0})}\] \[\leq 2e^{B\eta(t-t_{0}+1)}\cdot B\delta_{0}/\eta+B\delta_{0}(t-t_{0 })e^{B\eta(t-t_{0})}.\]

By the CDF of the geometric distribution, we know with probability at least \(1-\delta_{1}\), we get \(|t_{0}-t|\leq\log(1/\delta_{1})/p\). Let \(E\) be the event that \(|t_{0}-t|\leq\log(1/\delta_{1})/p\). Hence we know

\[\nu_{t_{0}}(y\in S_{bad}) \leq\nu_{t_{0}}(y\in S_{bad}\mid E)\Pr(E)+\Pr(E^{c})\] \[\leq(2/\eta+\log(1/\delta_{1})/p)\cdot eB\delta_{0}+\delta_{1}.\]

## Appendix C Missing proofs for Section 4

We prove a sequence of lemmas that are needed for the proof. The first lemma shows that the algorithm has to split the privacy budget across all resampling rounds. To this end, let \(S\) be a random variable that corresponds to the number of resampling steps in the algorithm, let \(T_{i}\) be the random variable corresponding to the round of the \(i\)'th resampling (where we let \(T_{i}=T+1\) if \(i>S\)), and let \(Z_{i}\) be the random variable corresponding to the model sampled at time \(T_{i}\) (letting \(Z_{i}=1\) if \(i>S\)).

**Lemma C.1**.: _(Composition) Let \(S,T_{i},Z_{i}\) and \(S^{\prime},T^{\prime}_{i},Z^{\prime}_{i}\) denote the random variables for two neighboring datasets. Under the assumptions of Theorem 4.2, if \(\mathsf{ALG}\) is \(\varepsilon^{2}\)-CDP, then for all \(\alpha\geq 1\)_

\[\sum_{i=1}^{T}D_{\alpha}(Z_{i}||Z^{\prime}_{i}\mid T_{i})\leq\alpha\varepsilon ^{2}.\]Proof.: As ALG is \(\varepsilon^{2}\)-concentrated DP and outputs \(T_{1},\ldots,T_{S}\) and \(Z_{1},\ldots,Z_{S}\), we have that

\[\alpha\varepsilon^{2} \geq D_{\alpha}(T_{1},Z_{1},\ldots,T_{S},Z_{S}||T_{1}^{\prime},Z_{1}^ {\prime},\ldots,T_{S^{\prime}}^{\prime},Z_{S^{\prime}}^{\prime})\] \[\geq D_{\alpha}(T_{1},Z_{1},\ldots,T_{T},Z_{T}||T_{1}^{\prime},Z_ {1}^{\prime},\ldots,T_{T}^{\prime},Z_{T}^{\prime})\] \[\geq\sum_{i=1}^{T}D_{\alpha}(Z_{i}||Z_{i}^{\prime}\mid T_{i}),\]

where the second inequality follows as the random variables \(T_{i},Z_{i}\) and \(T_{j}^{\prime},Z_{j}^{\prime}\) are constant for \(i>S\) and \(j>S^{\prime}\), and the last inequality follows as \(Z_{i}\) is independent of \((T_{1},\ldots,T_{i})\) and \((Z_{1},\ldots,Z_{i-1})\) given \(T_{i}\). 

We defer the proof of the following Lemma to the appendix.

**Lemma C.2**.: _Let \(T\geq 1\), \(\varepsilon\leq 1/T\) and \(\delta\leq 1/2\). Assume \(\ell:[d]\to\{0,1\}\) where \(\ell[x]\sim\mathsf{Ber}(1/2)\) for each \(x\in[d]\). Let \(D=(\ell,\ldots,\ell)\) and let ALG be an \((\varepsilon,\delta)\)-DP algorithm that outputs \((z_{1},\ldots,z_{T})=\mathsf{ALG}(D)\). Then_

\[\mathbb{E}[\sum_{t=1}^{T}\ell(z_{t})]\geq T\cdot\left(\frac{1}{2}-\frac{T \varepsilon}{2}\right)-\frac{T^{2}d\delta}{2}.\]

We are now ready to prove our main lower bound.

Proof.: (of Theorem 4.2) We consider the following construction for the lower bound: the adversary sets \(S_{\mathsf{adv}}=(T\varepsilon)^{2/3}\), the sequence of losses will have \(E=S_{\mathsf{adv}}^{2}\) epochs, each of size \(B=T/E=T/(T\varepsilon)^{4/3}=\frac{1}{(T\varepsilon)^{1/3}\varepsilon}\). Inside each epoch, the adversary samples \(\ell\sim\mathsf{Ber}(1/2)^{d}\) and plays the same loss function for the whole epoch.

Let \(S\) be random variable denoting the number of switches in the algorithm. In this case, we argue that each switch must have a small privacy budget (Lemma C.1), and thus, the price inside each epoch has to be large (Lemma C.2). Let \(T_{1},\ldots,T_{S}\) be the rounds where the algorithm resamples (\(T_{i}=T+1\) for \(i>S\)) and let \(Z_{1},Z_{2},\ldots,Z_{S}\) be the resampled models (\(Z_{i}=1\) for \(i>S\)). Lemma C.1 implies that

\[\sum_{i=1}^{T}D_{\alpha}(Z_{i}||Z_{i}^{\prime}\mid T_{i})\leq\alpha\varepsilon^ {2}.\]

Now note that inside an epoch \(e\), if the algorithm does not switch, then it will suffer loss \(B/2\) in that epoch. Otherwise, if it switches, assume without loss of generality there is at most one switch inside each epoch (see Lemma C.2). Let \(j_{\epsilon}\in[T]\) denote the index such that \(Z_{j_{\epsilon}}\) was sampled in epoch \(e\). Note that the algorithm in this epoch has \(D_{\alpha}(Z_{j_{\epsilon}}||Z_{i}^{\prime}\mid T_{j_{\epsilon}})=\alpha \varepsilon^{2}_{\epsilon}\), hence it is \(\varepsilon^{2}_{\epsilon}\)-CDP. Standard conversion from concentrated DP to approximate DP (Lemma A.5) implies that it is \((3\varepsilon_{e}\sqrt{\log(1/\delta)},\delta)\)-DP where \(\delta\leq 1/T^{3}d\). Hence Lemma C.2 implies the error for this epoch is \(B\cdot\left(\frac{1}{2}-\frac{3B\varepsilon_{e}\sqrt{\log(1/\delta)}}{2} \right)-1/T\). Letting \(E_{switch}\subset[E]\) denote the epochs where there is a switch, we have that the loss of the algorithm is

\[L(\mathsf{ALG}) \coloneqq\mathbb{E}[\sum_{t=1}^{T}\ell_{t}(x_{t})]\] \[=\mathbb{E}\left[\sum_{e\notin E_{switch}}\frac{B}{2}\right]\] \[\quad+\mathbb{E}\left[\sum_{e\in E_{switch}}B\left(\frac{1}{2}- \frac{3B\varepsilon_{e}\sqrt{\log(1/\delta)}}{2}\right)-1/T\right]\] \[=\mathbb{E}\left[(E-S)\frac{B}{2}+S\frac{B}{2}-\sum_{e\in E_{switch }}\frac{3B^{2}\varepsilon_{e}\sqrt{\log(1/\delta)}}{2}-1\right]\]\[=T-\frac{3B^{2}\sqrt{\log(1/\delta)}}{2}\operatorname{\mathbb{E}} \left[\sum_{e\in E_{switch}}\varepsilon_{e}\right]\] \[\geq T/2-1-\frac{3B^{2}\sqrt{E\log(1/\delta)}\varepsilon}{2},\]

where the last inequality follows since \(\sum_{e\in E_{switch}}\varepsilon_{e}\leq\sqrt{E\sum_{e=1}^{E}\varepsilon_{e} ^{2}}\leq\sqrt{E}\varepsilon\). Note also that the loss of the best expert is

\[L^{\star}\coloneqq\min_{x\in[d]}\sum_{t=1}^{T}\ell_{t}(x)=T/2-\sqrt{E}B\]

Overall we get that the regret of the algorithm is

\[L(\mathsf{ALG})-L^{\star} \geq\sqrt{E}B-\frac{3B^{2}\sqrt{E\log(1/\delta)}\varepsilon}{2}-1\] \[\geq(T\varepsilon)^{2/3}\frac{T}{(T\varepsilon)^{4/3}}-\frac{3 \sqrt{\log(1/\delta)}}{2(T\varepsilon)^{2/3}\varepsilon^{2}}\sqrt{E} \varepsilon-1\] \[=\frac{T^{1/3}}{\varepsilon^{2/3}}-\frac{3\sqrt{\log(1/\delta) }}{2(T\varepsilon)^{2/3}\varepsilon}-1\] \[\stackrel{{(i)}}{{\geq}}\frac{T^{1/3}}{\varepsilon^ {2/3}}-\frac{3\sqrt{\log(1/\delta)}}{2\varepsilon}-1\] \[\stackrel{{(ii)}}{{=}}\Omega\left(\frac{T^{1/3}}{ \varepsilon^{2/3}}\right),\]

where \((i)\) follows since \(E\leq(T\varepsilon)^{4/3}\), and \((ii)\) holds since \(\frac{3\sqrt{\log(1/\delta)}}{2\varepsilon}\leq\frac{T^{1/3}}{2\varepsilon^{ 2/3}}\) for \(\varepsilon\geq 100\log^{3/2}(dT)/T\geq 27\log^{3/2}(1/\delta)/T\). The claim follows. 

### Proof of Lemma c.2

Proof.: For this lower bound, we assume that the algorithm has full access to \(D\) to release \(z_{1},\ldots,z_{T}\). First, note that if the algorithms picks \(z=z_{i}\) with probability \(1/T\) and releases \((z,\ldots,z)\), then it has the same error since

\[\operatorname{\mathbb{E}}[\sum_{t=1}^{T}\ell(z)]=T\operatorname{\mathbb{E}} [\ell(z)]=T\operatorname{\mathbb{E}}[\frac{1}{T}\sum_{t=1}^{T}\ell(z_{t})]= \operatorname{\mathbb{E}}[\sum_{t=1}^{T}\ell(z_{t})].\]

Therefore, we assume that the algorithm releases a single \(z=\mathsf{ALG}(D)\) that is \((\varepsilon,\delta)\)-DP. Denote \(D_{\ell}=(\ell,\ldots,\ell)\). Note that as we sample \(\ell\sim\mathsf{Ber}(1/2)^{d}\), the probability \(p\coloneqq\Pr(\ell=\ell_{0})=\Pr(\ell=\ell_{1})\) for all \(\ell_{0},\ell_{1}\in\{0,1\}^{d}\). Letting \(\overline{\ell}=1-\ell\), we have that

\[\operatorname{\mathbb{E}}_{\ell\sim\mathsf{Ber}(1/2)^{d}}\left[ \sum_{t=1}^{T}\ell(\mathsf{ALG}(D_{\ell}))\right]\] \[=T\cdot\operatorname{\mathbb{E}}_{\ell\sim\mathsf{Ber}(1/2)^{d}} \left[\ell(\mathsf{ALG}(D_{\ell}))\right]\] \[=T\cdot\sum_{\ell_{0}\in\{0,1\}^{d}}\Pr_{\ell\sim\mathsf{Ber}(1/2 )^{d}}(\ell=\ell_{0})\cdot\operatorname{\mathbb{E}}\left[\ell_{0}(\mathsf{ALG }(D_{\ell_{0}}))\right]\] \[=\frac{T}{2}\cdot\sum_{\ell_{0}\in\{0,1\}^{d}}p\operatorname{ \mathbb{E}}\left[\ell_{0}(\mathsf{ALG}(D_{\ell_{0}}))+\overline{\ell}_{0}( \mathsf{ALG}(D_{\bar{\ell}_{0}}))\right]\] \[\geq\frac{T}{2}\cdot\min_{\ell_{0}\in\{0,1\}^{d}}\operatorname{ \mathbb{E}}\left[\ell_{0}(\mathsf{ALG}(D_{\ell_{0}}))+\overline{\ell}_{0}( \mathsf{ALG}(D_{\bar{\ell}_{0}}))\right].\]

Now note that for any \(\ell_{0}\) we have

\[\operatorname{\mathbb{E}}\left[\ell_{0}(\mathsf{ALG}(D_{\ell_{0}}))+ \overline{\ell}_{0}(\mathsf{ALG}(D_{\bar{\ell}_{0}}))\right]\]\[=\sum_{z\in[d]}\Pr(\mathsf{ALG}(D_{\ell_{0}})=z)\ell_{0}(z)+\Pr( \mathsf{ALG}(D_{\bar{I}_{0}})=z)\bar{\ell}_{0}(z)\] \[=\sum_{z\in[d]}\Pr(\mathsf{ALG}(D_{\ell_{0}})=z)\ell_{0}(z)+\Pr( \mathsf{ALG}(D_{\bar{I}_{0}})=z)(1-\ell_{0}(z))\] \[=1+\sum_{z\in[d]}\ell_{0}(z)\left(\Pr(\mathsf{ALG}(D_{\ell_{0}}) =z)-\Pr(\mathsf{ALG}(D_{\bar{\ell}_{0}})=z)\right)\] \[\geq 1+\sum_{z\in[d]}\ell_{0}(z)\left(e^{-T\varepsilon}\Pr( \mathsf{ALG}(D_{\bar{\ell}_{0}})=z)-T\delta-\Pr(\mathsf{ALG}(D_{\bar{\ell}_{0 }})=z)\right)\] \[\geq 1-Td\delta+\sum_{z\in[d]}\ell_{0}(z)\Pr(\mathsf{ALG}(D_{\bar {\ell}_{0}})=z)\left(e^{-T\varepsilon}-1\right)\] \[\geq 1-Td\delta-\sum_{z\in[d]}\ell_{0}(z)\Pr(\mathsf{ALG}(D_{\bar {\ell}_{0}})=z)T\varepsilon\] \[\geq 1-Td\delta-T\varepsilon,\]

where the first inequality follows since \(\mathsf{ALG}\) is \((\varepsilon,\delta)\)-DP and group privacy. The claim follows

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We provide new algorithms with improved guarantees for heavy-tailed private SCO in several settings, which is what we claim in the abstract and introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper discusses places where the main result is lossy, providing an appendix section dedicated towards improving it to be optimal. It also compares its results to another similar bound in the literature, discussing regimes where our bound is weaker. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: We provide a full set of verifiable details for all of our theoretical results. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Justification: The paper does not include experiments requiring code. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [NA] Justification: The paper does not include experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We read the ethics guidelines and believe we meet them. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: We do not believe our paper poses a significant negative societal impact, as it is about making existing learning algorithms differentially private under less stringent distributional assumptions, which we do not foresee being used in any significant malicious cases. We do believe that our algorithms can have positive societal impacts, but do not wish to overclaim to this effect because our results are primarily theoretical. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper does not release data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [NA] Justification: The paper does not use existing assets. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.