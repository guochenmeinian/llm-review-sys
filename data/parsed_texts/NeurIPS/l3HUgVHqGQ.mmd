# Scan and Snap: Understanding Training Dynamics

and Token Composition in 1-layer Transformer

 Yuandong Tian\({}^{1}\)  Yiping Wang\({}^{2,4}\)  Beidi Chen\({}^{1,3}\)  Simon Du\({}^{2}\)

\({}^{1}\)Meta AI (FAIR) \({}^{2}\)University of Washington \({}^{3}\)Carnegie Mellon University \({}^{4}\)Zhejiang University

{yuandong,beidic}@meta.com, {ypwang61,ssdu}@cs.washington.edu

###### Abstract

Transformer architectures have shown impressive performance in multiple research domains and have become the backbone of many neural network models. However, there is limited understanding on how Transformer works. In particular, with a simple predictive loss, how the representation emerges from the gradient _training dynamics_ remains a mystery. In this paper, we analyze the SGD training dynamics for 1-layer transformer with one self-attention plus one decoder layer, for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a _discriminative scanning algorithm_: starting from uniform attention, it gradually attends more to key tokens that are distinct for a specific next token to be predicted, and pays less attention to common key tokens that occur across different next tokens. Among distinct tokens, it progressively drops attention weights, following the order of low to high co-occurrence between the key and the query token in the training set. Interestingly, this procedure does not lead to winner-takes-all, but decelerates due to a _phase transition_ that is controllable by the learning rates of the two layers, leaving (almost) fixed token combination. We verify this _scan and snap_ dynamics on synthetic and real-world data (WikiText).

## 1 Introduction

The Transformer architecture [1] has demonstrated wide applications in multiple research domains, including natural language processing [2; 3; 4], computer vision [5; 6; 7], speech [8; 9], multimodality [10; 11], etc. Recently, large language models (LLMs) based on decoder-only Transformer architecture also demonstrate impressive performance [4; 12; 13], after fine-tuned with instruction data [14] or reward models [15]. Why a pre-trained model, often supervised by simple tasks such as predicting the next word [4; 3; 13] or filling in the blanks [2; 16; 17], can learn highly valuable representations for downstream tasks, remains a mystery.

To understand how Transformer works, many previous works exist. For example, it has been shown that Transformer is a universal approximator [18], can approximate Turing machines [19; 20], and can perform a diverse set of tasks, e.g., hierarchical parsing of context-free grammar [21], if its weights are set properly. However, it is unclear whether the weights designed to achieve specific tasks are at a critical point, or can be learned by SoTA optimizers (e.g., SGD, Adam [22], AdaFactor [23], AdamW [24]). In fact, many existing ML models, such as \(k\)-NN, Kernel SVM, or MLP, are also universal approximators, while their empirical performance is often way below Transformer.

To demystify such a behavior, it is important to understand the _training dynamics_ of Transformer, i.e., how the learnable parameters change over time during training. In this paper, as a first step, we formally characterize the SGD training dynamics of 1-layer position-encoding-free Transformer fornext token prediction, a popular training paradigm used in GPT series [3; 4], in a mathematically rigorous manner. The 1-layer Transformer contains one softmax self-attention layer followed by one decoder layer which predicts the next token. Under the assumption that the sequence is long, and the decoder learns faster than the self-attention layer, we prove the following interesting dynamic behaviors of self-attention during training. **Frequency Bias**: it progressively pays more attention to key tokens that co-occur a lot with the query token, and loses attention to tokens that co-occur less. **Discriminative Bias**: it pays attention to distinct tokens that appear uniquely given the next token to be predicted, while loses interest to common tokens that appear across multiple next tokens. These two properties suggest that self-attention implicitly runs an algorithm of _discriminative scanning_, and has an inductive bias to favor unique key tokens that frequently co-occur with the query ones.

Furthermore, while self-attention layer tends to become more sparse during training, as suggested by Frequency Bias, we discover that it will not collapse to one-hot, due to a _phase transition_ in the training dynamics. In the end, the learning does not converge to any stationary points with zero gradient, but ventures into a region where the attention changes slowly (i.e., logarithmically over time), and appears frozen and learned. We further show that the onset of the phase transition are controlled by the learning rates: large learning rate gives sparse attention patterns, and given fixed self-attention learning rate, large decoder learning rate leads to faster phase transition and denser attention patterns. Finally, the SGD dynamics we characterize in this work, named **scan and snap**, is verified in both synthetic and simple real-world experiments on WikiText [25].

**Concurrent works on Transformer dynamics**. Compared to [26] that uses \(\ell_{2}\) loss, our analysis focuses on cross-entropy, which is more realistic, imposes no prior knowledge on possible attention patterns inaccessible to training, and allows tokens to be shared across topics. Compared to [27] that analyzes "positional attention" that is independent of input data with symmetric initialization, our analysis focuses on attention on input data without symmetric assumptions. [28; 29; 30] give similar conclusions that self-attention attends to relevant tokens. In comparison, our work analyzes richer phenomena in 1-layer transformers related to frequency and discriminative bias, which has not been brought up by these works. For example, sparse attention patterns are connected with co-occurrence frequency of contextual token and query, characterization of such connection over training with softmax, including two-stage behaviors of attention logits, etc. We also leverage analytical solutions to certain nonlinear continuous dynamics systems that greatly simplifies the analysis. Detailed comparison can be found in Appendix B.

## 2 Related Works

**Expressiveness of Attention-based Models**. A line of work studies the expressive power of attention-based models. One direction focuses on the universal approximation power [18; 31; 32; 33; 20]. More recent works present fine-grained characterizations of the expressive power for certain functions in different settings, sometimes with statistical analyses [34; 35; 36; 37; 21; 38; 39; 40]. In particular, there is growing interest in explaining the capability of in-context learning [41] of Transformer, by mapping the gradient descent steps of learning classification/regression into feedforward steps of Transformer layers [42; 43; 44; 45; 37; 46]. Different from our work, the results in these papers are existential and do not take training dynamics into consideration.

**Training Dynamics of Neural Networks**. Previous works analyze the training dynamics in multi-layer linear neural networks [47; 48], in the student-teacher setting [49; 50; 51; 52; 53; 54; 55; 56; 57], and infinite-width limit [58; 59; 60; 61; 62; 63; 64; 65; 66; 67; 68; 69; 70; 71], including extentions to attention-based models [72; 73]. For self-supervised learning, works exist to analyze linear networks [74] and understand the role played by nonlinearity [75]. Focusing on attention-based models, Zhang et al. [76] study adaptive optimization methods in attention models. Jelassi et al. [27] propose an idealized setting and show the vision transformer [5] trained by gradient descent can learn spatial structure. Li et al. [26] show that the 1-layer Transformer can learn a constrained topic model, in which any word belongs to one topic, with \(\ell_{2}\) loss, BERT [2]-like architecture and additional assumptions on learned attention patterns. Snell et al. [77] study the dynamics of a single-head attention head to approximate the learning of a Seq2Seq architecture. While these papers also study the optimization dynamics of attention-based models, they focus on different settings and do not explain the phenomena presented in our paper.

## 3 Problem Setting

**Notation.** Let \(\{\bm{u}_{k}\}_{k=1}^{M}\) be \(d\)-dimensional embeddings, and \(\{x_{t}\}\) be discrete tokens. For each token, \(x_{t}\) takes discrete values from \(1\) to \(M\), denoted as \(x_{t}\in[M]\), and \(\bm{x}_{t}:=\bm{e}_{x_{t}}\in\mathbb{R}^{M}\) is the corresponding one-hot vector, i.e., the \(x_{t}\)-th entry of \(\bm{x}_{t}\) is \(1\) while others are zero. \(\bm{u}_{x_{t}}\) is the token embedding at location \(t\) in a sequence.

Let \(U=[\bm{u}_{1},\dots,\bm{u}_{M}]^{\top}\in\mathbb{R}^{M\times d}\) be the embedding matrix, in which the \(k\)-th row of \(U\) is the embedding vector of token \(k\). \(X=[\bm{x}_{1},\dots,\bm{x}_{T-1}]^{\top}\in\mathbb{R}^{(T-1)\times M}\) is the data matrix encoding the sequence of length \(T-1\). \(XU\in\mathbb{R}^{(T-1)\times d}\) is the sequence of embeddings for a given sequence \(\{x_{1},\dots,x_{T-1}\}\). It is clear that \(X\bm{1}_{M}=\bm{1}_{T-1}\).

We use \(X[i]\) to denote \(i\)-th sample in the sequence dataset. Similarly, \(x_{t}[i]\) is the token located at \(t\) in \(i\)-th sample. Let \(\mathcal{D}\) be the dataset used for training.

**1-Layer Transformer Architecture**. Given a sequence \(\{x_{1},\dots,x_{T},x_{T+1}\}\), the embedding after \(1\)-layer self attention is:

\[\hat{\bm{u}}_{T}=\sum_{t=1}^{T-1}b_{tT}\bm{u}_{x_{t}}, b_{tT}:=\frac{\exp(\bm{u}_{x_{T}}^{\top}W_{Q}W_{K}^{\top}\bm{u}_{x_{t}} /\sqrt{d})}{\sum_{t=1}^{T-1}\exp(\bm{u}_{x_{T}}^{\top}W_{Q}W_{K}^{\top}\bm{u}_ {x_{t}}/\sqrt{d})}\] (1)

Here \(b_{tT}\) is the normalized self-attention weights (\(\sum_{t=1}^{T-1}b_{tT}=1\)). One important detail is that we mask the weight that the query token attends to itself, which is also being used in previous works (e.g., QK-shared architecture [78]). See Sec. 7 for discussions about residual connection. Let \(\bm{b}_{T}:=[b_{1T},\dots,b_{T-1,T}]^{\top}\in\mathbb{R}^{T-1}\) be an attention vector, then \(\bm{b}_{T}^{\top}\bm{1}=1\) and \(\hat{\bm{u}}_{T}=U^{\top}X^{\top}\bm{b}_{T}\).

\(\ell_{2}\)**-Normalization**. We consider adding a normalization to the output of the self-attention layer: \(\tilde{\bm{u}}_{T}=U^{\top}\mathrm{LN}(X^{\top}\bm{b}_{T})\), where \(\mathrm{LN}(\bm{x}):=\bm{x}/\|\bm{x}\|_{2}\). NormFormer and RMSNorm [79, 80] also leverages this setting (up to a global constant). Our analysis can also be extended to standard LayerNorm [81], which also subtracts the mean of \(\bm{x}\), while [80] shows that mean subtraction may not affect the empirical results much. LLaMA [82] also uses RMSNorm. Empirically \(\hat{\bm{u}}_{T}\) (or \(W_{V}\hat{\bm{u}}_{T}\)) is normalized (instead of \(X^{\top}\bm{b}_{T}\)) and here we use an approximation to facilitate analysis: when the token embedding \(\{\bm{u}_{m}\}\) are approximately orthogonal to each other, then \(\|U^{\top}\bm{x}\|_{2}\approx\|\bm{x}\|_{2}\) and thus \(\tilde{\bm{u}}_{T}\approx\mathrm{LN}(\hat{\bm{u}}_{T})\).

**Objective.** We maximize the likelihood of predicted \((T+1)\)-th token using cross entropy loss:

\[\max_{W_{K},W_{Q},W_{V},U}J:=\mathbb{E}_{\mathcal{D}}\left[\bm{u}_{x_{T+1}}^{ \top}W_{V}\tilde{\bm{u}}_{T}-\log\sum_{l}\exp(\bm{u}_{l}^{\top}W_{V}\tilde{ \bm{u}}_{T})\right]\] (2)

For simplicity, we consider single-head attention setting, and multiple-head attention can be regarded as single-head setting with simultaneous different initializations (see Sec. 4). We call \(x_{T}=m\) as the **query token** of the sequence, and \(x_{T+1}=n\) as the **next token** to be predicted. Other tokens \(x_{t}\) (\(1\leq t\leq T-1\)) that are encoded in \(X\) are called **contextual tokens**. Both the contextual and query tokens can take values from \(1\) to \(M\) (i.e., \(m\in[M]\)) and next token takes the value from \(1\) to \(K\) (i.e., \(n\in[K]\)) where \(K\leq M\). Fig. 1(a) shows the overall setting. For an overview of the notation used in the paper, please check Tbl. 1 in the Appendix.

### Reparameterization

Instead of studying the dynamics with respect to the parameters of token embedding \(U\), key, value and query projection matrices \(W_{K}\), \(W_{Q}\) and \(W_{V}\), we study the dynamics of two _pairwise token relation matrices_\(Y:=UW_{V}^{\top}U^{\top}\in\mathbb{R}^{M\times M}\) and \(Z:=UW_{Q}W_{K}^{\top}U^{\top}/\sqrt{d}\in\mathbb{R}^{M\times M}\). Intuitively, entries of \(Y\) and \(Z\) store the "logits" of pairs of tokens. We regard the empirical parameterization using \(U\), \(W_{K}\), \(W_{Q}\) and \(W_{V}\) as a specific way of parametrization of \(Y\) and \(Z\), in order to reduce the number of parameters to be estimated. Previous work also leverage similar parameterization for self-attention layers [27, 46].

For real-world applications, the number of tokens \(M\) can be huge (e.g., the vocabulary size \(M=50,272\) in OPT-175B [83] and \(M=32,000\) in LLaMA [82]) and directly optimizing \(Y\) and \(Z\) would be prohibitive. However, as we will show in this work, from the theoretical perspective, treating \(Y\) and \(Z\) as independent variables has some unique advantages and leads to useful insights.

**Lemma 1** (Dynamics of 1-layer Transformer).: _The gradient dynamics of Eqn. 2 with batchsize 1 is:_

\[\dot{Y}=\eta_{Y}\mathrm{LN}(X^{\top}\bm{b}_{T})(\bm{x}_{T+1}-\bm{\alpha})^{\top},\quad\dot{Z}=\eta_{Z}\bm{x}_{T}(\bm{x}_{T+1}-\bm{\alpha})^{\top}Y^{\top}\frac{ P_{X^{\top}\bm{b}_{T}}^{\perp}}{\|X^{\top}\bm{b}_{T}\|_{2}}X^{\top}\mathrm{ diag}(\bm{b}_{T})X\] (3)

_Here \(P_{\bm{v}}^{\perp}:=I-\bm{v}\bm{v}^{\top}/\|\bm{v}\|_{2}^{2}\) projects a vector into \(\bm{v}\)'s orthogonal complementary space, \(\eta_{Y}\) and \(\eta_{Z}\) are the learning rates for the decoder layer \(Y\) and self-attention layer \(Z\), \(\bm{\alpha}:=[\alpha_{1},\dots,\alpha_{M}]^{\top}\in\mathbb{R}^{M}\) and \(\alpha_{m}:=\exp(Y^{\top}\mathrm{LN}(X^{\top}\bm{b}_{T}))/\bm{1}^{\top}\exp(Y ^{\top}\mathrm{LN}(X^{\top}\bm{b}_{T}))\)._

Please check Appendix C for the proof. We consider \(Y(0)=Z(0)=0\) as initial condition. This is reasonable since empirically \(Y\) and \(Z\) are initialized by inner product of \(d\)-dimensional vectors whose components are independently drawn by i.i.d Gaussian. This initial condition is also more realistic than [27] that assumes dominant initialization in diagonal elements. Since \((\bm{x}_{T+1}-\bm{\alpha})^{\top}\bm{1}=0\) and \(P_{X^{\top}\bm{b}_{T}}^{\perp}X^{\top}\mathrm{diag}(\bm{b}_{T})X\bm{1}=0\), we have \(\dot{Y}\bm{1}=\dot{Z}\bm{1}=0\) and summation of rows of \(Z(t)\) and \(Y(t)\) remains zero. Since \(\bm{x}_{T}\) is a one-hot column vector, the update of \(Z=[\bm{z}_{1},\bm{z}_{2},\dots,\bm{z}_{M}]^{\top}\) is done per row:

\[\dot{\bm{z}}_{m}=\eta_{Z}X^{\top}[i]\mathrm{diag}(\bm{b}_{T}[i])X[i]\frac{P_{ X^{\top}[i]\bm{b}_{T}[i]}^{\perp}}{\|X^{\top}[i]\bm{b}_{T}[i]\|_{2}}Y(\bm{x}_{T+1 }[i]-\bm{\alpha}[i])\] (4)

where \(m=x_{T}[i]\) is the query token for sample \(i\), \(\bm{z}_{m}\) is the \(m\)-th row of \(Z\) and \(\dot{\bm{z}}_{m^{\prime}}=0\) for row \(m^{\prime}\neq m=x_{T}[i]\). Note that if \(x_{T}[i]=m\), then \(b_{T}[i]\) is a function of \(\bm{z}_{m}\) only (but not a function of any other \(\bm{z}_{m^{\prime}}\)). Here we explicitly write down the current sample index \(i\), since batchsize is \(1\).

### Data Generation

Next we specify a data generation model (Fig. 1(b)), named _sequence class_, for our analysis.

**Sequence Class.** We regard the input data as a mixture of multiple _sequence classes_. Each sequence class is characterized by a triple \(s_{m,n}:=(\mathbb{P}(l|m,n),m,n)\). To generate a sequence instance from the class, we first set \(x_{T}=m\) and \(x_{T+1}=n\), and then generate the contextual tokens with conditional probability \(\mathbb{P}(l|m,n)\). Let \(\mathrm{supp}(m,n)\) be the subset of token \(l\) with \(\mathbb{P}(l|m,n)>0\).

In this work, we consider the case that given a next token \(x_{T+1}=n\), the corresponding sequence always ends with a specific query token \(x_{T}=m=:\psi(n)\). This means that we could index sequence class with next token \(x_{T+1}=n\) alone: \(s_{n}:=(\mathbb{P}(l|\psi(n),n),\psi(n),n)\), \(\mathbb{P}(l|m,n)=\mathbb{P}(l|n)\) and \(\mathrm{supp}(n):=\mathrm{supp}(\psi(n),n)\).

Note that \(|\psi^{-1}(m)|=1\) means that the occurrence of token \(m\) alone decides next token \(n\) to be predicted, regardless of other tokens in the sequence, which is a trivial case. When \(|\psi^{-1}(m)|\geq 2\), the same query token \(m\), combined with other token \(l\) in the sequence with non-zero probability \(\mathbb{P}(l|m,n)>0\), determine the next token.

**Overlapping sequence class**. Two sequence classes \(s_{n}\) and \(s_{n^{\prime}}\)_overlap_ if \(\mathrm{supp}(n)\cap\mathrm{supp}(n^{\prime})\neq\emptyset\).

Figure 1: Overall of our setting. **(a)** A sequence with contextual tokens \(\{x_{1},\dots,x_{T-1}\}\) and query token \(x_{T}\) is fed into 1-layer transformer (self-attention, normalization and decoding) to predict the next token \(x_{T+1}\). **(b)** The definition of sequence classes (Sec. 3.2). A sequence class specifies the conditional probability \(\mathbb{P}(l|m,n)\) of the contextual tokens, given the query token \(x_{T}=m\) and the next token \(x_{T+1}=n\). For simplicity, we consider the case that the query token is determined by the next token: \(x_{T}=\psi(x_{T+1})\) (and thus \(\mathbb{P}(l|m,n)=\mathbb{P}(l|n)\)), while the same query token \(m\) may correspond to multiple next tokens (i.e., \(\psi^{-1}(m)\) is not unique). We study two kinds of tokens: \(\mathrm{common\tokens}\) (CT) with \(\mathbb{P}(l|n)>0\) for multiple sequence class \(n\), and distinct tokens (DT) with \(\mathbb{P}(l|n)>0\) for a single sequence class \(n\) only.

**(Global) distinct and common tokens**. Let \(\Omega(l):=\{n:\mathbb{P}(l|n)>0\}\) be the subset of next tokens that co-occur with contextual token \(l\). We now can identify two kinds of tokens: the _distinct_ token \(l\) which has \(|\Omega(l)|=1\) and the _common_ token \(l\) with \(|\Omega(l)|>1\). Intuitively, this means that there exists one common token \(l\) so that both \(\mathbb{P}(l|n)\) and \(\mathbb{P}(l|n^{\prime})\) are strictly positive, e.g., common words like 'the', 'this', 'which' that appear in many sequence classes. In Sec. 5, we will see how these two type of contextual tokens behave very differently when self-attention layer is involved in the training: distinct tokens tend to be paid attention while common tokens tend to be ignored.

### Assumptions

To make our analysis easier, we make the following assumptions:

**Assumption 1**.: _We consider **(a)** no positional encoding, **(b)** The input sequence is long (\(T\to+\infty\)) and **(c)** The decoder layer learns much faster than the self-attention layer (i.e., \(\eta_{Y}\gg\eta_{Z}\))._

Assumption 1(a) suggests that the model is (almost) permutation-invariant. Given the next token to predict \(x_{T+1}=n\) and the query token \(x_{T}=m\) acted as query, the remaining tokens in the sequence may shuffle. Assumption 1(b) indicates that the frequency of a token \(l\) in the sequence approaches its conditional probability \(\mathbb{P}(l|m,n):=\mathbb{P}(l|x_{T}=m,x_{T+1}=n)\).

Note that the assumptions are comparable with or even weaker than previous works, e.g., [27] analyzes positional attention with symmetric initialization, without considering input data and [28] models the data distribution as discriminative/non-discriminative patterns, similar to our common/distinct tokens. Empirically, NoPE [84] shows that decoder-only Transformer models without positional encoding still works decently, justifying that Assumption 1(a) is reasonable.

Given the event \(\{x_{T}=m,x_{T+1}=n\}\), suppose for token \(l\), the conditional probability that it appears in the sequence is \(\mathbb{P}(l|m,n)\). Then for very long sequence \(T\to+\infty\), in expectation the number of token \(l\) appears in a sequence of length \(T\) approaches \(T\mathbb{P}(l|m,n)\). Therefore the _per-token_ self-attention weight \(c_{l|m,n}\) is computed as:

\[c_{l|m,n}:=\frac{T\mathbb{P}(l|m,n)\exp(z_{ml})}{\sum_{l^{\prime}}T\mathbb{P}( l^{\prime}|m,n)\exp(z_{ml^{\prime}})}=\frac{\mathbb{P}(l|m,n)\exp(z_{ml})}{ \sum_{l^{\prime}}\mathbb{P}(l^{\prime}|m,n)\exp(z_{ml^{\prime}})}=:\frac{ \tilde{c}_{l|m,n}}{\sum_{l^{\prime}}\tilde{c}_{l^{\prime}|m,n}}\] (5)

Here \(z_{ml}\) is \(\bm{z}_{m}\)'s \(l\)-th entry and \(\tilde{c}_{l|m,n}:=\mathbb{P}(l|m,n)\exp(z_{ml})\) is un-normalized attention score.

**Lemma 2**.: _Given the event \(\{x_{T}=m,x_{T+1}=n\}\), when \(T\to+\infty\), we have_

\[X^{\top}\bm{b}_{T}\to\bm{c}_{m,n},\qquad\qquad X^{\top}\mathrm{ diag}(\bm{b}_{T})X\to\mathrm{diag}(\bm{c}_{m,n})\] (6)

_where \(\bm{c}_{m,n}=[c_{1|m,n},c_{2|m,n},\ldots,c_{M|m,n}]^{\top}\in\mathbb{R}^{M}\). Note that \(\bm{c}_{m,n}^{\top}\bm{1}=1\)._

By the data generation process (Sec. 3.2), given the next token \(x_{T+1}=n\), the query token \(x_{T}=m\) is uniquely determined. In the following, we just use \(\bm{c}_{n}\) to represent \(\bm{c}_{m,n}\) (and similar for \(\tilde{\bm{c}}_{n}\)).

## 4 Dynamics of \(Y\)

We first study the dynamics of \(Y\). From Assumption 1(c), \(Y\) learns much faster and we can treat the lower layer output (i.e., \(X^{\top}\bm{b}_{T}\)) as constant. From Lemma 2, when the sequence is long, we know

Figure 2: Overview of the training dynamics of self-attention map. Here \(\tilde{c}_{l|m,n}:=\mathbb{P}(l|m,n)\exp(z_{ml})\) is the un-normalized attention score (Eqn. 5). **(a)** Initialization stage. \(z_{ml}(0)=0\) and \(\tilde{c}_{l|m,n}=\mathbb{P}(l|m,n)\). Distinct tokens (Sec. 3.2) shown in blue, common tokens in yellow. **(b)** Common tokens (CT) are suppressed (\(z_{ml}<0\), Theorem 2). **(c)** Winners-take-all stage. Distinct tokens (DT) with large initial value \(\tilde{c}_{l|m,n}(0)\) start to dominate the attention map (Sec. 5, Theorem 3). **(d)** Once passing the phase transition, i.e., \(t\geq t_{0}=O(K\ln M/\eta_{Y})\), attention appears (almost) frozen (Sec. 6) and token composition is fixed in the self-attention layer.

given the next token \(x_{T+1}=n\), \(X^{\top}\bm{b}_{T}\) becomes fixed. Therefore, the dynamics of \(Y\) becomes:

\[\dot{Y}=\eta_{Y}\bm{f}_{n}(\bm{e}_{n}-\bm{\alpha}_{n})^{\top},\quad\bm{\alpha}_{ n}=\frac{\exp(Y^{\top}\bm{f}_{n})}{\bm{1}^{\top}\exp(Y^{\top}\bm{f}_{n})}\] (7)

Here \(\bm{f}_{n}:=\frac{X^{\top}\bm{b}_{T}}{\|X^{\top}\bm{b}_{T}\|_{2}}\to\frac{\bm{e} _{n}}{\|\bm{e}_{n}\|_{2}}\in\mathbb{R}^{M}\). Obviously \(\|\bm{f}_{n}\|_{2}=1\) and \(\bm{f}_{n}\geq 0\). Define \(F=[\bm{f}_{1},\dots,\bm{f}_{K}]\). Since the vocabulary size \(M\) typically is a huge number, and different sequence classes can cover diverse subset of vocabulary, we study the weak correlation case:

**Assumption 2** (Weak Correlations).: _We assume \(M\gg K^{2}\) and \(\{\bm{f}_{n}\}_{n=1}^{K}\) satisfies \(F^{\top}F=I+E\), where the eigenvalues of \(E\in\mathbb{R}^{K\times K}\) satisfies \(|\lambda_{1}|<\frac{1}{K}\) and \(|\lambda_{i}(E)|\geq\frac{6}{\sqrt{M}},\forall i\in[K]\)._

Assumption 2 means that \(\bm{f}_{n}\) share some weak correlations and it immediately leads to the fact that \(F^{\top}F\) is invertible and \(F\) is column full-rank. Note that the critical point \(Y^{*}\) of Eqn. 7 should satisfy that for any given \(x_{T+1}=n\), we need \(\bm{\alpha}=\bm{e}_{n}\). But such \(Y^{*}\) must contain infinity entries due to the property of the exponential function in \(\bm{\alpha}\) and we can not achieve \(Y^{*}\) in finite steps. To analyze Eqn. 7, we leverage a _reparameterized_ version of the dynamics, by setting \(W=[\bm{w}_{1},\dots,\bm{w}_{K}]^{\top}:=F^{\top}Y\in\mathbb{R}^{K\times M}\) and compute gradient update on top of \(W\) instead of \(Y\):

**Lemma 3**.: _Given \(x_{T+1}=n\), the dynamics of \(W\) is (here \(\bm{\alpha}_{j}=\exp(\bm{w}_{j})/\bm{1}^{\top}\exp(\bm{w}_{j})\)):_

\[\dot{\bm{w}}_{j}=\eta_{Y}\mathbb{I}(j=n)(\bm{e}_{n}-\bm{\alpha}_{n})\] (8)

_While we cannot run gradient update on \(W\) directly, it can be achieved by modifying the gradient of \(Y\) to be \(\dot{Y}=\eta_{Y}(\bm{f}_{n}-FE^{\prime}\bm{e}_{n})(\bm{e}_{n}-\bm{\alpha}_{n} )^{\top}\). If \(\lambda_{1}\) is small, the modification is small as well._

Please check Appendix D for the proof. Lemma 3 shows that for every fixed \(n\), only the corresponding row of \(W\) is updated, which makes the analysis much easier. We now can calculate the backpropagated gradient used in Eqn. 3.

**Theorem 1**.: _If Assumption 2 holds, the initial condition \(Y(0)=0\), \(M\gg 100\), \(\eta_{Y}\) satisfies \(M^{-0.99}\ll\eta_{Y}<1\), and each sequence class appears uniformly during training, then after \(t\gg K^{2}\) steps of batch size \(1\) update, given event \(x_{T+1}[i]=n\), the backpropagated gradient \(\bm{g}[i]:=Y(\bm{x}_{T+1}[i]-\bm{\alpha}[i])\) takes the following form:_

\[\bm{g}[i]=\gamma\left(\iota_{n}\bm{f}_{n}-\sum_{n^{\prime}\neq n}\beta_{nn^{ \prime}}\bm{f}_{n^{\prime}}\right)\] (9)

_Here the coefficients \(\iota_{n}(t)\), \(\beta_{nn^{\prime}}(t)\) and \(\gamma(t)\) are defined in Appendix with the following properties:_

* _(a)_ \(\xi_{n}(t):=\gamma(t)\sum_{n\neq n^{\prime}}\beta_{nn^{\prime}}(t)\bm{f}_{n}^ {\top}(t)\bm{f}_{n^{\prime}}(t)>0\) _for any_ \(n\in[K]\) _and any_ \(t\)_;_
* _(b)_ _The_ speed control coefficient \(\gamma(t)>0\) _satisfies_ \(\gamma(t)=O(\eta_{Y}t/K)\) _when_ \(t\leq\frac{\ln(M)\cdot K}{\eta_{Y}}\) _and_ \(\gamma(t)=O\left(\frac{K\ln(\eta_{Y}t/K)}{\eta_{Y}t}\right)\) _when_ \(t\geq\frac{2(1+\delta^{\prime})\ln(M)\cdot K}{\eta_{Y}}\) _with_ \(\delta^{\prime}=\Theta(\frac{\ln\ln M}{\ln M})\)_._

In the remark of Lemma 5 in Appendix, we analyze the original dynamics (Eqn. 7) with identical off-diagonal elements of \(E\), and Theorem 1 still holds with a smaller effective learning rate.

## 5 The dynamics of Self-attention

Now we analyze the dynamics of self-attention logits \(Z\), given the dynamics of upper layer \(Y\).

**Lemma 4** (Self-attention dynamics).: _With Assumption 1(b) (i.e., \(T\to+\infty\)), Eqn. 4 becomes:_

\[\dot{\bm{z}}_{m}=\eta_{Z}\gamma\sum_{n\in\psi^{-1}(m)}\mathrm{diag}(\bm{f}_{n} )\sum_{n^{\prime}\neq n}\beta_{nn^{\prime}}(\bm{f}_{n}\bm{f}_{n}^{\top}-I)\bm {f}_{n^{\prime}},\] (10)

Please check Appendix E for the proof. Now we study the dynamics of two types of contextual tokens (Sec. 3.2), namely _distinct tokens_ (DT) which appear only for a single next token (i.e., \(|\Omega(l)|=1\) with \(\Omega(l):=\{n:\mathbb{P}(l|n)>0\}\)), and _common tokens_ (CT) that appear across multiple next tokens (\(|\Omega(l)|>1\)). We show their fates are very different: over training, _distinct tokens gain attention but common ones lose it_.

**Theorem 2** (Fates of contextual tokens).: _Let \(G_{CT}\) be the set of common tokens (CT), and \(G_{DT}(n)\) be the set of distinct tokens (DT) that belong to next token \(n\). Then if Assumption 2 holds, under the self-attention dynamics (Eqn. 10), we have:_

* _(a)_ _for any distinct token_ \(l\in G_{DT}(n)\)_,_ \(\dot{z}_{ml}>0\) _where_ \(m=\psi(n)\)_;_
* _(b)_ _if_ \(|G_{CT}|=1\) _and at least one next token_ \(n\in\psi^{-1}(m)\) _has at least one distinct token, then for the single common token_ \(l\in G_{CT}\)_,_ \(\dot{z}_{ml}<0\)_._

Now we know DTs grow and a single CT will shrink. For multiple CTs to shrink, the condition can be a bit involved (see Corollary 2 in Appendix E). The following theorem further shows that the growth rates of DTs critically depend on their initial conditions:

**Theorem 3** (Growth of distinct tokens).: _For a next token \(n\) and its two distinct tokens \(l\) and \(l^{\prime}\), the dynamics of the **relative gain**\(r_{l/l^{\prime}|n}(t):=f_{nl}^{2}(t)/f_{nl^{\prime}}^{2}(t)-1=\tilde{c}_{l|n }^{2}(t)/\tilde{c}_{l^{\prime}|n}^{2}(t)-1\) has the following analytic form (here the query token \(m=\psi(n)\) and is uniquely determined by distinct token \(l\)):_

\[r_{l/l^{\prime}|n}(t)=r_{l/l^{\prime}|n}(0)e^{2(z_{ml}(t)-z_{ml}(0))}:=:r_{l/l ^{\prime}|n}(0)\chi_{l}(t)\] (11)

_where \(\chi_{l}(t):=e^{2(z_{ml}(t)-z_{ml}(0))}\) is the **growth factor** of distinct token \(l\). If there exist a dominant token \(l_{0}\) such that the initial condition satisfies \(r_{l_{0}/l|n}(0)>0\) for all its distinct token \(l\neq l_{0}\), and all of its common tokens \(l\) satisfy \(\dot{z}_{ml}<0\). Then both \(z_{ml_{0}}(t)\) and \(f_{nl_{0}}(t)\) are monotonously increasing over \(t\), and_

\[e^{2f_{nl_{0}}^{2}(0)B_{n}(t)}\leq\chi_{l_{0}}(t)\leq e^{2B_{n}(t)}\] (12)

_here \(B_{n}(t):=\eta_{Z}\int_{0}^{t}\xi_{n}(t^{\prime})\mathrm{d}t^{\prime}\). Intuitively, larger \(B_{n}\) gives larger \(r_{l_{0}/l|n}\) and sparser attention map._

**Self-attention as an algorithm of token scanning**. From Eqn. 11, we could see that self-attention performs _token scanning_. To see that, consider the simplest initialization that \(\bm{z}(0)=0\), which means that \(r_{l_{0}/l|n}(0)=\left(\frac{\mathbb{P}(l_{0}|m,n)}{\mathbb{P}(l|m,n)}\right)^{ 2}-1\). Therefore, distinct token \(l\) with low conditional probability \(\mathbb{P}(l|m,n)\) will have \(r_{l_{0}/l|n}(0)\gg 0\), According Eqn. 11, this leads to quickly growing ratio \(r_{l_{0}/l|n}(t)\), which means that the corresponding component \(f_{nl}\) will be quickly dwarfed by the dominating component \(f_{nl_{0}}\). On the other hand, token with high conditional probability \(\mathbb{P}(l|m,n)\) will have smaller \(r_{l_{0}/l|n}(0)\), and the ratio \(r_{l_{0}/l|n}(t)\) grows slower, costing longer time for \(l_{0}\) to dominate \(l\).

**Initial value as prior information**. From the theorems, it is clear that the initial value \(r_{l/l^{\prime}|n}(0):=\left(\frac{\mathbb{P}(l|m,n)\exp(z_{ml}(0))}{\mathbb{P} (l|m,n)\exp(z_{ml^{\prime}}(0))}\right)^{2}-1\) critically determines the fate of the dynamics. Two tokens \(l\) and \(l^{\prime}\) with comparable conditional probability \(\mathbb{P}(l|m,n)\) and \(\mathbb{P}(l^{\prime}|m,n)\) can be suppressed in either way, depending on their initial logits \(z_{ml}(0)\) and \(z_{ml^{\prime}}(0)\). In the empirical implementation, the initial value of the logits are determined by the inner products of independently initialized high-dimensional vectors, which fluctuate around zero.

The concept of "initial value as prior" can explain empirical design choices such as _multi-head self-attention_[1]. From this perspective, each head \(h\) has its own \(Z_{h}\) and is initialized independently, which could enable more diverse token combination (e.g., a combination of 1st, 3rd, 5th tokens, rather than a combination of 1st, 2nd, 3rd tokens).

## 6 The Moment of Snapping: When Token Combination is fixed

Theorem 3 suggests two possible fates of the self-attention weights: if \(\xi_{n}(t)\) decays slowly (e.g., \(\xi_{n}(t)\geq 1/t\)), then \(B_{n}(t)\rightarrow+\infty\) and all contextual tokens except for the dominant one will drop (i.e., \(f_{nl}\to 0\)) following the ranking order of their conditional probability \(\mathbb{P}(l|m,n)\). Eventually, winner-takes-all happens. Conversely, if \(\xi_{n}(t)\) drops so fast that \(B_{n}(t)\) grows very slowly, or even has an upper limit, then the self-attention patterns are "snapped" and token combination is learned and fixed.

The conclusion is not obvious, since \(\xi_{n}(t)\) depends on the decay rate of \(\gamma(t)\) and \(\beta_{nn^{\prime}}(t)\), which in turns depends on the inner product \(\bm{f}_{n}^{\top}(t)\bm{f}_{n^{\prime}}(t)\), which is related to the logits of the common tokens that also decays over time.

Here we perform a qualitative estimation when there is only a single common token \(l\) and every next token shares a single token \(m\) (i.e., for any next token \(n\), \(\psi(n)=m\)). We assume all normalizationterms in \(\bm{f}_{n}\) are approximately constant, denoted as \(\rho_{0}\), which means that \(\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}}\approx\exp(2z_{ml})/\rho_{0}^{2}\) and \(\beta_{nn^{\prime}}\approx E_{nn^{\prime}}^{\prime}\approx\bm{f}_{n}^{\top}\bm{f }_{n^{\prime}}\approx\exp(2z_{ml})/\rho_{0}^{2}\) as well, and \(1-\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}}\approx 1\) due to the fact that common token components are small, and will continue to shrink during training.

Under these approximations, its dynamics (Eqn. 10) can be written as follows (here \(C_{0}:=\rho_{0}^{4}/K\)):

\[\dot{z}_{ml}=\eta_{Z}\gamma\sum_{n\in\psi^{-1}(m)}f_{nl}\sum_{n^{\prime}\neq n }\beta_{nn^{\prime}}(f_{nl}^{2}-1)f_{nl^{\prime}}\approx-C_{0}^{-1}\eta_{Z} \gamma e^{4z_{ml}},\quad\xi_{n}(t)\approx C_{0}^{-1}\gamma e^{4z_{ml}}\] (13)

Surprisingly, we now find a _phase transition_ by combining the rate change of \(\gamma(t)\) in Theorem 1:

**Theorem 4** (Phase Transition in Training).: _If the dynamics of the single common token \(z_{ml}\) satisfies \(\dot{z}_{ml}=-C_{0}^{-1}\eta_{Z}\gamma(t)e^{4z_{ml}}\) and \(\xi_{n}(t)=C_{0}^{-1}\gamma(t)e^{4z_{ml}}\), then we have:_

\[B_{n}(t)=\left\{\begin{array}{cc}\frac{1}{4}\ln\left(C_{0}+\frac{2(M-1)^{2} }{KM^{2}}\eta_{Y}\eta_{Z}t^{2}\right)&t<t_{0}^{\prime}:=\frac{K\ln M}{\eta_{Y} }\\ \frac{1}{4}\ln\left(C_{0}+\frac{2K(M-1)^{2}}{M^{2}}\frac{\eta_{Z}}{\eta_{Y}} \ln^{2}(M\eta_{Y}t/K)\right)&t\geq t_{0}:=\frac{2(1+o(1))K\ln M}{\eta_{Y}} \end{array}\right.\] (14)

_As a result, there exists a phase transition during training:_

* _Attention scanning. At the beginning of the training,_ \(\gamma(t)=O(\eta_{Y}t/K)\) _and_ \(B_{n}(t)\approx\frac{1}{4}\ln K^{-1}(\rho_{0}^{4}+2\eta_{Y}\eta_{Z}t^{2})=O( \ln t)\)_. This means that the growth factor for dominant token_ \(l_{0}\) _is (sub-)linear:_ \(\chi_{l_{0}}(t)\geq e^{2f_{nl_{0}}^{2}(0)B_{n}(t)}\approx[K^{-1}(\rho_{0}^{4} +2\eta_{Y}\eta_{Z}t^{2})]^{0.5f_{nl_{0}}^{2}(0)}\)_, and the attention on less co-occurred token drops gradually._
* _Attention snapping. When_ \(t\geq t_{0}:=2(1+\delta^{\prime})K\ln M/\eta_{Y}\) _with_ \(\delta^{\prime}=\Theta(\frac{\ln\ln M}{\ln M})\)_,_ \(\gamma(t)=O\left(\frac{K\ln(\eta_{Y}t/K)}{\eta_{Y}t}\right)\) _and_ \(B_{n}(t)=O(\ln\ln t)\)_. Therefore, while_ \(B_{n}(t)\) _still grows to infinite, the growth factor_ \(\chi_{l_{0}}(t)=O(\ln t)\) _grows at_ a much slower _logarithmic rate._

See proof in Appendix F. This gives a few insights about the training process: **(a)** larger learning rate \(\eta_{Y}\) of the decoder \(Y\) leads to shorter phase transition time \(t_{0}\approx 2K\ln M/\eta_{Y}\), **(b)** scaling up both learning rate (\(\eta_{Y}\) and \(\eta_{Z}\)) leads to larger \(B_{n}(t)\) when \(t\rightarrow+\infty\), and thus sparser attention maps, and **(c)** given fixed \(\eta_{Z}\), small learning rate \(\eta_{Y}\) leads to larger \(B_{n}(t)\) when \(t\geq t_{0}\), and thus sparser attention map. Fig. 3 shows numerical simulation results of the growth rate \(\chi_{l}(t)\). Here we set \(K=10\) and \(M=1000\), and we find smaller \(\eta_{Y}\) given fixed \(\eta_{Z}\) indeed leads to later transition and larger \(B_{n}(t)\) (and \(\chi_{l}(t)\)).

## 7 Discussion and Limitations

**Positional encoding**. While our main analysis does not touch positional encoding, it can be added easily following the relative encoding schemes that adds a linear bias when computing self attention (E.g., T5 [17], ALBi [85], MusicTransformer [86]). More specifically, the added linear bias \(\exp(z_{ml}+z_{0})=\exp(z_{ml})\exp(z_{0})\) corresponds to a prior of the contextual token to be learned in the self-attention layer.

Figure 3: Growth factor \(\chi_{l}(t)\) (Theorem 3) over time with fixed \(\eta_{Z}=0.5\) and changing \(\eta_{Y}\). Each solid line is \(\chi_{l}(t)\) and the dotted line with the same color corresponds to the transition time \(t_{0}\) for a given \(\eta_{Y}\).

**Residue connection**. Residue connection can be added in the formulation, i.e., \(\bar{\bm{u}}_{T}=\mathrm{LN}(\mathrm{LN}(\bar{\bm{u}}_{T})+\bm{u}_{x_{T}})\), where \(\tilde{\bm{u}}_{T}\) is defined in Eqn. 1, and \(\bar{\bm{u}}_{T}\) is used instead in the objective (Eqn. 2). In this case, the \(\beta_{nn^{\prime}}\) in Theorem 1 now is approximately \(\beta_{nn^{\prime}}\sim\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}}+\mathbb{I}(\psi(n )=\psi(n^{\prime}))\), which is much larger for sequence classes \(n\) and \(n^{\prime}\) that share the same query token \(x_{T}\) than otherwise. In this case, Theorem 1 now gives \(\bm{g}[i]=\gamma\left(\iota_{n}\bm{f}_{n}-\sum_{n\neq n^{\prime}\in\psi^{-1}( \psi(n))}\beta_{nn^{\prime}}\bm{f}_{n^{\prime}}\right)\) for \(x_{T+1}[i]=n\). Due to the additional constraint \(n^{\prime}\in\psi^{-1}(\psi(n))\) (i.e., \(n\) and \(n^{\prime}\) shares the same query token), we can define _local_ distinct and common tokens to be _within_ the sequence class subset \(\psi^{-1}(m)\) and Theorem 2 now applies within each subset. Empirically this makes more sense, since the query token \(x_{T}=m_{1}\) or \(m_{2}\) alone can already separate different subsets \(\psi^{-1}(m_{1})\) and \(\psi^{-1}(m_{2})\) and there should not be any interactions across the subsets. Here we just present the most straightforward analysis and leave this extension for future work.

**Possible future extension to multi-layer cases**. For multilayer training, a lasting puzzle is to explain how the input tokens get combined together to form high-level concepts. The analysis above shows that the training leads to sparse attention even among relevant tokens, and demonstrates that there is a priority in token combinations for 1-layer attention based on their co-occurrence: even if there are \(10\) relevant contextual tokens to the query, the self-attention may only pick 1-2 tokens to combine first due to attention sparsity. This can be regarded as a starting point to study how tokens are composed hierarchically. In comparison, [28; 29; 30] show that attention attends to all relevant tokens, which may not suggest a hierarchical / multi-layer architecture.

## 8 Experiments

We conduct experiments on both synthetic and real-world dataset to verify our theoretical findings.

**Syn-Small**. Following Sec. 3.2, we construct \(K=2\) sequence classes with vocabulary size \(M=30\). The first \(10\) tokens (0-9) are shared between classes, while the second and third \(10\) tokens (10-19 and 20-29) are distinct for class 1 and class 2, respectively. The conditional probability \(\mathbb{P}(l|n)\) for tokens 10-19 is increasing monotonously (the same for 20-29). The 1-layer Transformer is parameterized with \(Y\) and \(Z\) (Sec. 3.1), is trained with initial condition \(Y(0)=Z(0)=0\) and SGD (with momentum \(0.9\)) using a batchsize of 128 and sequence length \(T=128\) until convergence.

Fig. 4 shows the simulation results. The attention indeed becomes sparse during training, and increasing \(\eta_{Y}\) with fixed \(\eta_{Z}\) leads to faster convergence but less sparse attention. Both are consistent with our theoretical predictions (Theorem 3 and Sec. 6). Interestingly, if we use Adam optimizer instead, self-attention with different learning rate \(\eta_{Y}=\eta_{Z}\) picks different subsets of distinct tokens to focus on, showing tune-able inductive bias (Fig. 5). We leave analysis on Adam for future work.

**Syn-Medium**. To further verify our theoretical finding, we now scale up \(K\) to create Syn-Medium and compute how attention sparsity for distinct tokens (in terms of entropy) changes with the learning rates (Fig. 6). We can see indeed the entropy goes down (i.e., attention becomes sparser) with larger \(\eta_{Z}\), and goes up (i.e., attention becomes less sparse) by fixing \(\eta_{Z}\) and increasing \(\eta_{Y}\) passing the threshold \(\eta_{Y}/\eta_{Z}\approx 2\), consistent with Sec. 6. Note that the threshold is due to the fact that our theory is built on Assumption 1(c), which requires \(\eta_{Y}\) to be reasonably larger than \(\eta_{Z}\).

Figure 4: Visualization of \(\bm{c}_{n}\) (\(n=1,2\)) in the training dynamics of 1-layer Transformer using SGD on Syn-Small setting. Top row for query token \(n=1\) and bottom row for query token \(n=2\). **Left:** SGD training with \(\eta_{Y}=\eta_{Z}=1\). Attention pattern \(\bm{c}_{n}\) becomes sparse and concentrated on highest \(\mathbb{P}(l|n)\) (rightmost) for each sequence class (Theorem 3). **Right:** SGD training with \(\eta_{Y}=10\) and \(\eta_{Z}=1\). With larger \(\eta_{Y}\), convergence becomes faster but the final attention maps are less sparse (Sec. 6).

**Real-world Dataset**. We also test our finding on WikiText [25] using both 1-layer and multi-layer Transformers with regular parameterization that computes \(Y\) and \(Z\) with embedding \(U\). In both cases, attentions of the first layer freeze (and become sparse) at some point (Fig. 7), even if the learning rate remains the same throughout training. More results are in Appendix G.

## 9 Conclusion and Future Work

In this work, we formally characterize SGD training dynamics of 1-layer Transformer, and find that the dynamics corresponds to a _scan and snap_ procedure that progressively pays more attention to key tokens that are distinct and frequently co-occur with the query token in the training set. To our best knowledge, we are the first to analyze the attention dynamics and reveal its inductive bias on data input, and potentially open a new door to understand how Transformer works.

Many future works follow. According to our theory, large dataset suppresses spurious tokens that are perceived as distinct in a small dataset but are actual common ones. Our finding may help suppress such tokens (and spurious correlations) with prior knowledge, without a large amount of data.

Figure 5: Visualization of (part of) \(\bm{c}_{n}\) for sequence class \(n=1\) in the training dynamics using Adam [22] on Syn-Small setting. **From left to right**: \(\eta_{V}=\eta_{Z}=0.1,0.5,1\). With different learning rate Adam seems to steer self-attention towards different subset of distinct tokens, showing tune-able inductive bias.

Figure 6: Average entropy of \(\bm{c}_{n}\) (Eqn. 5) on distinct tokens versus learning rate ratio \(\eta_{Y}/\eta_{Z}\) with more query tokens \(M\)/next tokens \(K\). We report mean values over 10 seeds and standard derivation of the mean.

Figure 7: Attention patterns in the lowest self-attention layer for 1-layer (top) and 3-layer (bottom) Transformer trained on WikiText2 using SGD (learning rate is 5). Attention becomes sparse over training.

## References

* [1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2017.
* [2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [3] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. _OpenAI blog_, 1(8):9, 2019.
* [4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* [5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [6] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* [7] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16000-16009, 2022.
* [8] Ching-Feng Yeh, Jay Mahadeokar, Kaustubh Kalgaonkar, Yongqiang Wang, Duc Le, Mahaveer Jain, Kjell Schubert, Christian Fuegen, and Michael L Seltzer. Transformer-transducer: End-to-end speech recognition with self-attention. _arXiv preprint arXiv:1910.12977_, 2019.
* [9] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. _Advances in neural information processing systems_, 33:12449-12460, 2020.
* [10] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International conference on machine learning_, pages 8748-8763. PMLR, 2021.
* [11] Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. Data2vec: A general framework for self-supervised learning in speech, vision and language. In _International Conference on Machine Learning_, pages 1298-1312. PMLR, 2022.
* [12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. _arXiv preprint arXiv:2204.02311_, 2022.
* [13] OpenAI. Gpt-4 technical report, 2023.
* [14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.
* [15] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. _Advances in Neural Information Processing Systems_, 33:3008-3021, 2020.
* [16] Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won Chung, Dara Bahri, Tal Schuster, Steven Zheng, et al. Ul2: Unifying language learning paradigms. In _The Eleventh International Conference on Learning Representations_, 2022.

* Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_, 21(1):5485-5551, 2020.
* Yun et al. [2019] Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar. Are transformers universal approximators of sequence-to-sequence functions? _arXiv preprint arXiv:1912.10077_, 2019.
* Wei et al. [2022] Colin Wei, Yining Chen, and Tengyu Ma. Statistically meaningful approximation: a case study on approximating turing machines with transformers. _Advances in Neural Information Processing Systems_, 35:12071-12083, 2022.
* Perez et al. [2021] Jorge Perez, Pablo Barcelo, and Javier Marinkovic. Attention is turing complete. _The Journal of Machine Learning Research_, 22(1):3463-3497, 2021.
* Zhao et al. [2023] Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora. Do transformers parse while predicting the masked word? _arXiv preprint arXiv:2303.08117_, 2023.
* Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* Shazeer and Stern [2018] Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In _International Conference on Machine Learning_, pages 4596-4604. PMLR, 2018.
* Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. _arXiv preprint arXiv:1609.07843_, 2016.
* Li et al. [2023] Yuchen Li, Yuanzhi Li, and Andrej Risteski. How do transformers learn topic structure: Towards a mechanistic understanding. _arXiv preprint arXiv:2303.04245_, 2023.
* Jelassi et al. [2022] Samy Jelassi, Michael Sander, and Yuanzhi Li. Vision transformers provably learn spatial structure. _Advances in Neural Information Processing Systems_, 35:37822-37836, 2022.
* Li et al. [2023] Hongkang Li, Meng Wang, Sijia Liu, and Pin-Yu Chen. A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity. _ICLR_, 2023.
* Oymak et al. [2023] Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, and Christos Thrampoulidis. On the role of attention in prompt-tuning. _ICML_, 2023.
* Tarzanagh et al. [2023] Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, and Samet Oymak. Max-margin token selection in attention mechanism. _arXiv preprint arXiv:2306.13596_, 3(7):47, 2023.
* Bhattacharya et al. [2020] Satwik Bhattacharya, Kabir Ahuja, and Navin Goyal. On the ability and limitations of transformers to recognize formal languages. _arXiv preprint arXiv:2009.11264_, 2020.
* Bhattacharya et al. [2020] Satwik Bhattacharya, Arkil Patel, and Navin Goyal. On the computational power of transformers and its implications in sequence modeling. _arXiv preprint arXiv:2006.09286_, 2020.
* Dehghani et al. [2018] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. _arXiv preprint arXiv:1807.03819_, 2018.
* Edelman et al. [2022] Benjamin L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang. Inductive biases and variable creation in self-attention mechanisms. In _International Conference on Machine Learning_, pages 5793-5831. PMLR, 2022.
* Elhage et al. [2021] N Elhage, N Nanda, C Olsson, T Henighan, N Joseph, B Mann, A Askell, Y Bai, A Chen, T Conerly, et al. A mathematical framework for transformer circuits. _Transformer Circuits Thread_, 2021.
* Likhosherstov et al. [2021] Valerii Likhosherstov, Krzysztof Choromanski, and Adrian Weller. On the expressive power of self-attention matrices. _arXiv preprint arXiv:2106.03764_, 2021.

* [37] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. _arXiv preprint arXiv:2211.15661_, 2022.
* [38] Shunyu Yao, Binghui Peng, Christos Papadimitriou, and Karthik Narasimhan. Self-attention networks can process bounded hierarchical languages. _arXiv preprint arXiv:2105.11115_, 2021.
* [39] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization in large language models. _arXiv preprint arXiv:2207.04901_, 2022.
* [40] Boaz Barak, Benjamin Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang. Hidden progress in deep learning: Sgd learns parities near the computational limit. _Advances in Neural Information Processing Systems_, 35:21750-21764, 2022.
* [41] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning. _arXiv preprint arXiv:2301.00234_, 2022.
* [42] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. _Advances in Neural Information Processing Systems_, 35:30583-30598, 2022.
* [43] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. _arXiv preprint arXiv:2212.07677_, 2022.
* [44] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. _arXiv preprint arXiv:2306.04637_, 2023.
* [45] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning and induction heads. _arXiv preprint arXiv:2209.11895_, 2022.
* [46] Shuai Li, Zhao Song, Yu Xia, Tong Yu, and Tianyi Zhou. The closeness of in-context learning and weight shifting for softmax regression. _arXiv preprint arXiv:2304.13276_, 2023.
* [47] Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient descent for deep linear neural networks. _arXiv preprint arXiv:1810.02281_, 2018.
* [48] Peter Bartlett, Dave Helmbold, and Philip Long. Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks. In _International conference on machine learning_, pages 521-530. PMLR, 2018.
* [49] Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian inputs. In _International conference on machine learning_, pages 605-614. PMLR, 2017.
* [50] Yuandong Tian. An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis. In _International conference on machine learning_, pages 3404-3413. PMLR, 2017.
* [51] Mahdi Soltanolkotabi. Learning relus via gradient descent. _Advances in neural information processing systems_, 30, 2017.
* [52] Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping patches. In _International Conference on Machine Learning_, pages 1783-1791. PMLR, 2018.
* [53] Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? _arXiv preprint arXiv:1709.06129_, 2017.
* [54] Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns one-hidden-layer cnn: Don't be afraid of spurious local minima. In _International Conference on Machine Learning_, pages 1339-1348. PMLR, 2018.

* [55] Mo Zhou, Tianyi Liu, Yan Li, Dachao Lin, Enlu Zhou, and Tuo Zhao. Toward understanding the importance of noise in training neural networks. In _International Conference on Machine Learning_, pages 7594-7602. PMLR, 2019.
* [56] Tianyi Liu, Minshuo Chen, Mo Zhou, Simon S Du, Enlu Zhou, and Tuo Zhao. Towards understanding the importance of shortcut connections in residual networks. _Advances in neural information processing systems_, 32, 2019.
* [57] Weihang Xu and Simon S Du. Over-parameterization exponentially slows down gradient descent for learning a single neuron. _arXiv preprint arXiv:2302.10034_, 2023.
* [58] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. _Advances in neural information processing systems_, 31, 2018.
* [59] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. _Advances in neural information processing systems_, 32, 2019.
* [60] Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes over-parameterized neural networks, 2018.
* [61] Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global minima of deep neural networks. In _International conference on machine learning_, pages 1675-1685. PMLR, 2019.
* [62] Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-parameterization. In _International Conference on Machine Learning_, pages 242-252. PMLR, 2019.
* [63] Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In _International Conference on Machine Learning_, pages 322-332. PMLR, 2019.
* [64] Samet Oymak and Mahdi Soltanolkotabi. Toward moderate overparameterization: Global convergence guarantees for training shallow neural networks. _IEEE Journal on Selected Areas in Information Theory_, 1(1):84-105, 2020.
* [65] Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu. Gradient descent optimizes over-parameterized deep relu networks. _Machine learning_, 109:467-492, 2020.
* [66] Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient descent on structured data. _Advances in neural information processing systems_, 31, 2018.
* [67] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. _Advances in neural information processing systems_, 31, 2018.
* [68] Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-layer neural networks. _Proceedings of the National Academy of Sciences_, 115(33):E7665-E7671, 2018.
* [69] Phan-Minh Nguyen and Huy Tuan Pham. A rigorous framework for the mean field limit of multilayer neural networks. _arXiv preprint arXiv:2001.11443_, 2020.
* [70] Cong Fang, Jason Lee, Pengkun Yang, and Tong Zhang. Modeling from features: a mean-field framework for over-parameterized deep neural networks. In _Conference on learning theory_, pages 1887-1936. PMLR, 2021.
* [71] Yiping Lu, Chao Ma, Yulong Lu, Jianfeng Lu, and Lexing Ying. A mean field analysis of deep resnet and beyond: Towards provably optimization via overparameterization from depth. In _International Conference on Machine Learning_, pages 6426-6436. PMLR, 2020.

* [72] Jiri Hron, Yasaman Bahri, Jascha Sohl-Dickstein, and Roman Novak. Infinite attention: Nngp and ntk for deep attention networks. In _International Conference on Machine Learning_, pages 4376-4386. PMLR, 2020.
* [73] Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer. _arXiv preprint arXiv:2203.03466_, 2022.
* [74] Yuandong Tian. Understanding the role of nonlinearity in training dynamics of contrastive learning. _arXiv preprint arXiv:2206.01342_, 2022.
* [75] Yuandong Tian. Understanding the role of nonlinearity in training dynamics of contrastive learning. _ICLR_, 2023.
* [76] Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar, and Suvrit Sra. Why are adaptive methods good for attention models? _Advances in Neural Information Processing Systems_, 33:15383-15393, 2020.
* [77] Charlie Snell, Ruiqi Zhong, Dan Klein, and Jacob Steinhardt. Approximating how single head attention learns. _arXiv preprint arXiv:2103.07601_, 2021.
* [78] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _ICLR_, 2020.
* [79] Sam Shleifer, Jason Weston, and Myle Ott. Normformer: Improved transformer pretraining with extra normalization. _arXiv preprint arXiv:2110.09456_, 2021.
* [80] Biao Zhang and Rico Sennrich. Root mean square layer normalization. _Advances in Neural Information Processing Systems_, 32, 2019.
* [81] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. _arXiv preprint arXiv:1607.06450_, 2016.
* [82] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [83] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2205.01068_, 2022.
* [84] Amirhossein Kazemejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. _arXiv preprint arXiv:2305.19466_, 2023.
* [85] Ofir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. _arXiv preprint arXiv:2108.12409_, 2021.
* [86] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu, and Douglas Eck. Music transformer. _arXiv preprint arXiv:1809.04281_, 2018.
* [87] Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized neural networks, going beyond two layers. _Advances in neural information processing systems_, 32, 2019.

## Appendix A Notation Table

Tbl. 1 gives the notation of the main quantities in the paper.

## Appendix B Detailed comparison with the concurrent works

### Comparison with [28]

**Setting, Assumptions and Conclusions**. [28] analyzes the SGD convergence of 1-layer ViT model (1 layer self-attention + 2 layer FFN with ReLU, with the top layer of FFN fixed as random, token embedding fixed). Under a specific binary data model in which the data label is determined by counting the number of tokens that belong to positive/negative pattern, [28] gives a generalization bound when the number of hidden nodes in FFN is large, and at the same time, shows that the self-attention attends to relevant tokens and becomes sparse (if number of relevant tokens are small).

In comparison, our work focuses on language models, assume broader data distribution (e.g., multiple classes, arbitrary conditional probability of token given class label) and incorporate LayerNorm

\begin{table}
\begin{tabular}{l|l} \hline \multicolumn{2}{c}{**Basic Notations**} \\ \hline \(M\) & Vocabulary size \\ \(T\) & Sequence length \\ \(\bm{e}_{k}\) & One-hot vector (\(1\) at component \(k\)) \\ \(X\in\mathbb{R}^{T-1\times M}\) & Input sequence (of length \(T-1\)) \\ \(\bm{b}_{T}\in\mathbb{R}^{T-1}\) & Vector of self-attention weights to predict the token at time \(T\). \\ \(\bm{x}_{t}\in\mathbb{R}^{M}\) & contextual token (\(0\leq t\leq T-2\)) (one-hot) \\ \(\bm{x}_{T-1}\in\mathbb{R}^{M}\) & Last/query token (one-hot) \\ \(\bm{x}_{T}\in\mathbb{R}^{M}\) & Next token (class label) to be predicted (one-hot) \\ \(\bm{x}_{t}[i]\in\mathbb{R}^{M}\) & \(i\)-th training sample of token at location \(t\) in the sequence \\ \(K\) & Number of possible choices the next token could take. \\ \(\bm{\alpha}(t)\) & Softmax score of the output layer. \\ \hline \multicolumn{2}{c}{**Learnable Parameters**} \\ \hline \(Y\in\mathbb{R}^{M\times M}\) & decoder layer parameters \\ \(Z\in\mathbb{R}^{M\times M}\) & self-attention logits \\ \(\bm{z}_{m}\) & \(m\)-th row of \(Z\) (i.e., attention logits for a query/query token \(m\)) \\ \hline \multicolumn{2}{c}{**Hyperparameters**} \\ \hline \(\eta_{Y}\) & Learning rate of the decoder layer \\ \(\eta_{Z}\) & Learning rate of the self-attention layer \\ \hline \multicolumn{2}{c}{**Token Types and Distribution**} \\ \hline \(\psi(n)\) & Mapping from next token \(x_{T}=n\) to its unique last/query token \\ \(\psi^{-1}(m)\) & The subset of next tokens for last/query token \(x_{T-1}=m\) \\ \(\mathbb{P}(l|m,n)\) & Conditional probability of contextual token \(l\) \\ \multicolumn{2}{c}{given query token is \(m\) and next token to be predicted as \(n\)} \\ \(G_{\text{CT}}\) & Subset of common tokens \\ \(G_{\text{DT}}(n)\) & Subset of distinct tokens for \(x_{T}=n\) \\ \hline \multicolumn{2}{c}{**Attention Score**} \\ \hline \(\tilde{\bm{c}}_{n}\in\mathbb{R}^{M}\) & Unnormalized attention score given next token \(x_{T}=n\) \\ \(\bm{c}_{n}\in\mathbb{R}^{M}\) & \(\ell_{1}\)-normalized attention score given next token \(x_{T}=n\) \\ \(\bm{f}_{n}\in\mathbb{R}^{M}\) & \(\ell_{2}\)-normalized attention score given next token \(x_{T}=n\) \\ \(\bm{g}\in\mathbb{R}^{M}\) & Back-propagated gradient for \(\bm{f}_{n}\) \\ \(F\) & Input matrix of the decoder layer. Each column of \(F\) is \(\bm{f}_{n}\) \\ \hline \multicolumn{2}{c}{**Self-attention dynamics**} \\ \hline \(r_{l^{\prime}/l|n}(t)\) & Relative gain between distinct token \(l\) and \(l^{\prime}\) for next token \(n\) \\ \(B_{n}(t)\) & Growth factor bound of the relative gain \\ \(\gamma(t)\) & Speed control coefficient \\ \hline \end{tabular}
\end{table}
Table 1: Overall notation table of the main symbols in the paper.

naturally. We propose more detailed quantitative properties, e.g., attention sparsity even among relevant tokens, two-stage evolution of attention scores, with a much simpler analysis.

**Techniques**. The techniques used in [28] are based on feature learning techniques applied to MLP (e.g., [87]). It identifies lucky neurons if the number of hidden neurons is large enough. In comparison, our framework and analysis is much simpler by leveraging that certain nonlinear continuous dynamics systems can be integrated out analytically to yield clean solutions (e.g., Theorem 3 (Eqn. 11) and Theorem 4 (Eqn. 128)), avoiding complicated bounds in [28]. This allows us to characterize the converging behavior of self-attentions when \(t\rightarrow+\infty\).

### Comparison with [29]

[29] focuses on 1-layer attention-based prompt-tuning, in which some parameters of the models are fixed (\(W_{p}\), \(W_{q}\)). The analysis focuses on the initial (3x one-step) SGD trajectory, and constructs the dataset model containing specific context-relevant/context-irrelevant data, and the context-vector indicates the token relevance. As a result, [29] shows the attention becomes sparse (i.e., attending to context-relevant tokens) over time, which is consistent with ours, and shows that prompt-attention can find the relevant tokens and achieve high accuracy while self-attention/linear-attention can't.

In comparison, our work goes beyond the 2-classes model and further points out that the attention weight will be relevant to the conditional probability of the contextual tokens, which is more detailed than the sparse attention result in [29] that relies on the sparsity assumption of contextual tokens itself. We also focus on the pre-training stage (training from scratch, predicting the next token), characterize the entire trajectory under SGD for the self-attention layer, in particular its converging behavior.

### Comparison with [30]

Compared to [29], [30] also analyzes the dynamics of the query-key matrix and the embedding of a single tunable token (often [CLS] token). It makes connection between the binary classification problem with 1-layer transformer and max-margin SVM formulation, when the tokens are linearly separable. The dynamics is characterized completely, which is nice. Note here is not an attention since its norm can be shown to go to infinity over training.

In comparison, our work does not learn the embedding of an individual token, but focuses on the dynamics of (all-pair) attention scores during training. We also work on multiple-class setup and do not explicitly assume the linear separability among classes.

## Appendix C Proof of Section 3

**Lemma 1** (Dynamics of 1-layer Transformer).: _The gradient dynamics of Eqn. 2 with batchsize 1 is:_

\[\dot{Y}=\eta_{Y}\mathrm{LN}(X^{\top}\bm{b}_{T})(\bm{x}_{T+1}-\bm{\alpha})^{ \top},\quad\dot{Z}=\eta_{Z}\bm{x}_{T}(\bm{x}_{T+1}-\bm{\alpha})^{\top}Y^{\top} \frac{\mathrm{P}_{X^{\top}\bm{b}_{T}}^{\perp}}{\|X^{\top}\bm{b}_{T}\|_{2}}X^{ \top}\operatorname{diag}(\bm{b}_{T})X\] (3)

_Here \(P_{\bm{v}}^{\perp}:=I-\bm{v}\bm{v}^{\top}/\|\bm{v}\|_{2}^{2}\) projects a vector into \(\bm{v}\)'s orthogonal complementary space, \(\eta_{Y}\) and \(\eta_{Z}\) are the learning rates for the decoder layer \(Y\) and self-attention layer \(Z\), \(\bm{\alpha}:=[\alpha_{1},\dots,\alpha_{M}]^{\top}\in\mathbb{R}^{M}\) and \(\alpha_{m}:=\exp(Y^{\top}\mathrm{LN}(X^{\top}\bm{b}_{T}))/\bm{1}^{\top}\exp(Y ^{\top}\mathrm{LN}(X^{\top}\bm{b}_{T}))\)._

Proof.: With the reparameterization of \(Y\) and \(Z\), the loss function is the following:

\[J(Y,Z)=\mathbb{E}_{\mathcal{D}}\left[\bm{x}_{T+1}^{\top}Y^{\top}\mathrm{LN}(X^ {\top}\bm{b}_{T})-\log(\bm{1}^{\top}\exp(Y^{\top}\mathrm{LN}(X^{\top}\bm{b}_{ T})))\right]\] (15)

and

\[\alpha_{m}=\frac{\exp(\bm{e}_{m}^{\top}Y^{\top}\mathrm{LN}(X^{\top}\bm{b}_{T}) )}{\bm{1}^{\top}\exp(Y^{\top}\mathrm{LN}(X^{\top}\bm{b}_{T}))}\] (16)

Therefore, taking matrix differentials, we have:

\[\mathrm{d}J=(\bm{x}_{T+1}-\bm{\alpha})^{\top}\mathrm{d}(Y^{\top}\mathrm{LN}(X^ {\top}\bm{b}))=(\bm{x}_{T+1}-\bm{\alpha})^{\top}\left(\mathrm{d}Y^{\top} \mathrm{LN}(X^{\top}\bm{b})+Y^{\top}\frac{\mathrm{P}_{X^{\top}\bm{b}}^{\perp} }{\|X^{\top}\bm{b}\|}X^{\top}\mathrm{d}\bm{b}\right)\] (17)since in general we have \(\mathrm{d}(\exp(\bm{a})/\bm{1}^{\top}\exp(\bm{a}))=L\mathrm{d}\bm{a}\) with \(L:=\mathrm{diag}(\bm{b})-\bm{b}\bm{b}^{\top}\), let \(\bm{a}:=XZ^{\top}\bm{x}_{T}\) and we have:

\[\mathrm{d}J = (\bm{x}_{T+1}-\bm{\alpha})^{\top}\left(\mathrm{d}Y^{\top}\mathrm{ LN}(X^{\top}\bm{b})+Y^{\top}\frac{P_{X^{\top}\bm{b}}^{\perp}}{\|X^{\top}\bm{b}\|}X^{ \top}L\mathrm{d}(XZ^{\top}\bm{x}_{T})\right)\] (18) \[= (\bm{x}_{T+1}-\bm{\alpha})^{\top}\left(\mathrm{d}Y^{\top}\mathrm{ LN}(X^{\top}\bm{b})+Y^{\top}\frac{P_{X^{\top}\bm{b}}^{\perp}}{\|X^{\top}\bm{b}\|}X^{ \top}LX\mathrm{d}Z^{\top}\bm{x}_{T}\right)\] (19)

Finally notice that \(P_{X^{\top}\bm{b}}^{\perp}X^{\top}L=P_{X^{\top}\bm{b}}^{\perp}X^{\top}\mathrm{ diag}(\bm{b})\) due to the fact that \(P_{\bm{v}}^{\perp}\bm{v}=0\) and the conclusion follows. 

**Lemma 2**.: _Given the event \(\{x_{T}=m,x_{T+1}=n\}\), when \(T\to+\infty\), we have_

\[X^{\top}\bm{b}_{T}\to\bm{c}_{m,n},\qquad\qquad X^{\top}\mathrm{ diag}(\bm{b}_{T})X\to\mathrm{diag}(\bm{c}_{m,n})\] (6)

_where \(\bm{c}_{m,n}=[c_{1|m,n},c_{2|m,n},\ldots,c_{M|m,n}]^{\top}\in\mathbb{R}^{M}\). Note that \(\bm{c}_{m,n}^{\top}\bm{1}=1\)._

Proof.: Let \(\bm{p}=[\exp(z_{m1}),\ldots,\exp(z_{mM})]^{\top}\in\mathbb{R}^{M}\), \(p_{x_{t}}:=\exp(z_{mx_{t}})\), and \(\bm{p}_{X}:=[\exp(z_{mx_{1}}),\ldots,\exp(z_{mx_{T-1}})]^{\top}\), then for any \(T\) we have

\[X^{\top}\bm{b}_{T}=\sum_{t=1}^{T-1}b_{tT}\bm{x}_{t}=\sum_{t=1}^{T-1}\frac{p_{x _{t}}\bm{x}_{t}}{\sum_{t^{\prime}}p_{x_{t^{\prime}}}}=\frac{X^{\top}\bm{p}_{X }}{\bm{1}^{\top}X^{\top}\bm{p}_{X}}\] (20)

Combining Lemma 18 and the definition of \(c_{l|m,n}\) (Eqn. 5), we have that when \(T\to+\infty\),

\[X^{\top}\bm{b}_{T}\to\sum_{l=1}^{M}\frac{\mathbb{P}(l|m,n)\exp(z_{ml})\bm{e}_ {l}}{\sum_{l^{\prime}}\mathbb{P}(l^{\prime}|m,n)\exp(z_{ml^{\prime}})}=\bm{c} _{m,n}\] (21)

Similarly:

\[X^{\top}\mathrm{diag}(\bm{b}_{T})X=\frac{X^{\top}\mathrm{diag}(\bm{p}_{X})X}{ \bm{1}^{\top}X^{\top}\bm{p}_{X}}\] (22)

Let \(T\to+\infty\), then we also get

\[X^{\top}\mathrm{diag}(\bm{b}_{T})X\to\mathrm{diag}(\bm{c}_{m,n})\] (23)

## Appendix D Proof of Section 4

### Notation

For convenience, we introduce the following notations for this section:

* Denote \(E^{\prime}:=(I+E)^{-1}-I\).
* Apply orthogonal diagonalization on \(E\) and obtain \(E=U^{\top}DU\) where \(U:=[\bm{u}_{1},...,\bm{u}_{K}]\in O_{K\times K},D=\text{diag}(\lambda_{1},..., \lambda_{K})\) and \(|\lambda_{1}|\geq...\geq|\lambda_{K}|\geq 0\).
* Denote \(F^{\prime}:=[F,F^{\circ}]\in\mathbb{R}^{M\times M}\) where \(F^{\circ}\in\mathbb{R}^{M\times(M-K)}\) is some matrix such that \(\text{rank}(F^{\prime})=M\). This is possible since \(\{\bm{f}_{i}\}_{i\in[K]}\) are linear-independent.
* Denote \(W^{\prime}:=(F^{\prime})^{\top}Y=[F,F^{\circ}]^{\top}Y=[W^{\top},Y^{\top}F^{ \circ}]^{\top}=[\bm{w}_{1},\ldots,\bm{w}_{K},\bm{w}_{K+1},\ldots,\allowbreak\bm{ w}_{M}]^{\top}\in\mathbb{R}^{M\times M}\).
* Denote \(\bm{\zeta}_{n}:=\frac{M}{M-1}(\bm{e}_{n}-\frac{1}{M}\bm{1})\in\mathbb{R}^{M}\).
* Denote \(q_{1}:=\bm{\zeta}_{i}^{\top}\bm{\zeta}_{i}=1+\frac{1}{M-1}\), \(q_{0}:=\bm{\zeta}_{j}^{\top}\bm{\zeta}_{i}=-\frac{M}{(M-1)^{2}}\) where \(i,j\in[M],i\neq j\).
* Denote \(h\) to be a continuous function that satisfies \(h(0)=0\) and \(\dot{h}=\eta_{Y}\cdot(M-1+\exp(Mh))^{-1}\). Details in Lemma 6.

* Denote \(\omega_{1}\) to be the constant defined in Lemma 8 that satisfies \(\omega_{1}=\Theta(\frac{\ln\ln(M)}{\ln(M)})\).
* Denote \(N_{n}:=\sum_{i=1}^{N}\mathbb{I}[x_{T+1}=n]\) to be the number of times the event \(x_{T+1}=n\) happens.
* Denote \(\bar{N}:=\lceil N/K\rceil\) to be the average value of \(N_{n}\) when \(\mathbb{P}(n)\equiv 1/K\) and \(\Delta:=\lceil\sqrt{N\ln(\frac{1}{\delta})}\rceil\) to be the radius of confidence interval centered on \(\bar{N}\) with confidence \(1-\delta\). Here \(\Delta/\bar{N}\asymp\frac{K}{\sqrt{N}}\sqrt{\ln(\frac{1}{\delta})}\ll 1\) since \(N\gg K^{2}\). Details in Lemma 10 and Remark 4.
* Denote \(\bar{W}^{\prime}(N):=[\bar{\bm{w}}_{1}(N),...,\bar{\bm{w}}_{K}(N),\bm{0},..., \bm{0}]^{\top}\in\mathbb{R}^{M\times M}\), where \(\bar{\bm{w}}_{n}(N):=(M-1)h(\bar{N})\bm{\zeta}_{n},\;\forall n\in[K]\).

### Proof of Lemma 3

We assume \(\cup_{m\in[M]}\psi^{-1}(m)=[K]\) for convenience, but we claim that our proof can be easily generalized into the case where \(\Omega\neq[K]\) by reordering the subscript of the vectors. First, we prove the dynamics equation of the reparameterized dynamics of \(Y\).

**Lemma 3**.: _Given \(x_{T+1}=n\), the dynamics of \(W\) is (here \(\bm{\alpha}_{j}=\exp(\bm{w}_{j})/\bm{1}^{\top}\exp(\bm{w}_{j})\)):_

\[\dot{\bm{w}}_{j}=\eta_{Y}\mathbb{I}(j=n)(\bm{e}_{n}-\bm{\alpha}_{n})\] (8)

_While we cannot run gradient update on \(W\) directly, it can be achieved by modifying the gradient of \(Y\) to be \(\dot{Y}=\eta_{Y}(\bm{f}_{n}-FE^{\prime}\bm{e}_{n})(\bm{e}_{n}-\bm{\alpha}_{n}) ^{\top}\). If \(\lambda_{1}\) is small, the modification is small as well._

Proof.: We let \(F^{\prime}:=[F,F^{\circ}]\in\mathbb{R}^{M\times M}\) where \(\text{rank}(F^{\prime})=M\), this is possible since \(\{\bm{f}_{n}\}_{n\in[K]}\) are linear-independent. And we further define \(W^{\prime}:=(F^{\prime})^{\top}Y=[F,F^{\circ}]^{\top}Y=[W^{\top},Y^{\top}F^{ \circ}]^{\top}=[\bm{w}_{1},\dots,\bm{w}_{K},\bm{w}_{K+1},\dots,\bm{w}_{M}]^{ \top}\in\mathbb{R}^{M\times M}\). When given \(x_{T+1}=n\), the first term of the differential of loss function \(J\) is:

\[\begin{split}\operatorname{tr}\left(\mathrm{d}Y^{\top}\frac{X^{ \top}\bm{b}_{T}}{\|X^{\top}\bm{b}_{T}\|_{2}}(\bm{x}_{T+1}-\bm{\alpha})^{\top} \right)&=\operatorname{tr}(\mathrm{d}Y^{\top}F^{\prime}(F^{ \prime})^{-1}\bm{f}_{n}(\bm{x}_{T+1}-\bm{\alpha})^{\top})\\ &=\operatorname{tr}(\mathrm{d}(W^{\prime})^{\top}\bm{e}_{n}(\bm{x }_{T+1}-\bm{\alpha})^{\top})\end{split}\] (24)

So \(\dot{W}^{\prime}=\bm{e}_{n}(\bm{x}_{T+1}-\bm{\alpha})^{\top}\). This nice property will limit \(W\) to independently update its \(n\)-th row for any \(x_{T+1}=n\in[K]\), and the last \(M-K\) rows of \(W^{\prime}\) are not updated. Similarly for \(\bm{\alpha}\) we have

\[\bm{\alpha}=\frac{\exp(UW_{V}\tilde{\bm{u}}_{T})}{\bm{1}^{\top}\exp(UW_{V} \tilde{\bm{u}}_{T})}=\frac{\exp(Y^{\top}\bm{f}_{n})}{\bm{1}^{\top}\exp(Y^{\top }\bm{f}_{n})}=\frac{\exp(Y^{\top}F^{\prime}(F^{\prime})^{-1}\bm{f}_{n})}{\bm{ 1}^{\top}\exp(Y^{\top}F^{\prime}(F^{\prime})^{-1}\bm{f}_{n})}=\frac{\exp(\bm{w} _{n})}{\bm{1}^{\top}\exp(\bm{w}_{n})}\] (25)

We get Eqn. 8 by combining the above results.

If we don't run gradient update on \(W\) directly, we can run a modified gradient update on \(Y\):

\[\dot{Y}=\eta_{Y}(\bm{f}_{n}-FE^{\prime}\bm{e}_{n})(\bm{e}_{n}-\bm{\alpha}_{n}) ^{\top}\] (26)

This will lead to (note that \(F\) does not change over time due to Assumption 1 (c)):

\[\dot{W} = F^{\top}\dot{Y}=\eta_{Y}F^{\top}(\bm{f}_{n}-FE^{\prime}\bm{e}_{n} )(\bm{e}_{n}-\bm{\alpha}_{n})^{\top}\] (27) \[= \eta_{Y}\left[F^{\top}\bm{f}_{n}-F^{\top}F(I-(I+E)^{-1})\bm{e}_{ n}\right](\bm{e}_{n}-\bm{\alpha}_{n})^{\top}\] (28) \[= \eta_{Y}\left(F^{\top}\bm{f}_{n}-F^{\top}F\bm{e}_{n}+\bm{e}_{n} \right)(\bm{e}_{n}-\bm{\alpha}_{n})^{\top}\] (29) \[= \eta_{Y}\bm{e}_{n}(\bm{e}_{n}-\bm{\alpha}_{n})^{\top}\] (30)

By Lemma 17, we know that if \(\lambda_{1}\) is small, so does \(\max_{i\in[K]}|\lambda_{i}(E^{\prime})|\) and thus the modification is small as well. In Lemma 5 Remark 1, we will show that the additional term \(-FE^{\prime}\bm{e}_{n}\) effectively reduces the learning rate, if all off-diagonal elements of \(E\) are the same. 

Lemma 3 shows that we can transfer the problem into solving \(K\) independent and similar non-linear ODE. And we then show that such a problem can be well solved by following Lemma. Recall that \(\bm{\zeta}_{n}:=\frac{M}{M-1}(\bm{e}_{n}-\frac{1}{M}\bm{1})\in\mathbb{R}^{M}\), we have:

**Lemma 5**.: _Assume \(Y\) is initialized to be a zero matrix, \(Z\) is fixed, and the learning rate of \(Y\) is \(\eta_{Y}\). Then if event \(x_{T+1}=n\) always holds at \(s\) step (\(s\geq 1\)) we have_

\[\bm{w}_{n}(s)=(M-1)h^{*}(s)\bm{\zeta}_{n}\] (31)

\[\alpha_{nj}(s)=\begin{cases}\dfrac{\exp(Mh^{*}(s-1))}{(M-1)+\exp(Mh^{*}(s-1)) }&,\quad j=n\\ \dfrac{1}{(M-1)+\exp(Mh^{*}(s-1))}&,\quad j\neq n\end{cases}\] (32)

_And thus \(\bm{e}_{n}-\bm{\alpha}_{n}(s)=\frac{M-1}{M-1+\exp(Mh^{*}(s-1))}\bm{\zeta}_{n}\). Here \(h^{*}(s)\) satisfies:_

\[h^{*}(s)=\begin{cases}h^{*}(s-1)+\dfrac{\eta_{Y}}{(M-1)+\exp(Mh^{*}(s-1))}&, \quad s\geq 1\\ 0&,\quad s=0\end{cases}\] (33)

Proof.: We prove this Lemma by induction.

**Step 1**: Note that \(Y\) is initialized to be a zero matrix, then \(\bm{w}_{i}(0)=0,\forall i\in[K]\). So we have

\[\alpha_{n}(1) = \frac{1}{M},\quad\forall j\in[K]\] (34) \[\dot{w}_{nj}(1) = \begin{cases}1-\dfrac{1}{M},\qquad j=n\\ -\dfrac{1}{M},\qquad j\neq n\end{cases}\] (35) \[w_{nj}(1) = \begin{cases}\eta_{Y}(1-\dfrac{1}{M}),\qquad j=n\\ -\dfrac{\eta_{Y}}{M},\qquad\qquad j\neq n\end{cases}\] (36)

It's easy to check that these equations match of Lemma 5.

**Step \(s\)**: Assume the equations of Lemma 5 hold for step \(s-1\). Then at the \(s\) step, we have

\[\alpha_{nj}(s) = \begin{cases}\dfrac{\exp((M-1)h^{*}(s-1))}{\exp((M-1)h^{*}(s-1)) +(M-1)\exp(-h^{*}(s-1))}&=\dfrac{\exp(Mh^{*}(s-1))}{\exp(Mh^{*}(s-1))+(M-1)}, \quad j=n\\ \dfrac{\exp(-h^{*}(s-1))}{\exp((M-1)h^{*}(s-1))+(M-1)\exp(-h^{*}(s-1))}&=\dfrac {1}{\exp(Mh^{*}(s-1))+(M-1)},\quad j\neq n\end{cases}\] (37) \[\dot{w}_{nj}(s) = \begin{cases}\dfrac{M-1}{\exp(Mh^{*}(s-1))+(M-1)},\qquad\quad j=n \\ -\dfrac{1}{\exp(Mh^{*}(s-1))+(M-1)},\qquad\quad j\neq n\end{cases}\] (38) \[w_{nj}(s) = \begin{cases}(M-1)\cdot(\dfrac{\eta_{Y}}{\exp(Mh^{*}(s-1))+(M-1) }+h^{*}(s-1))&=(M-1)h^{*}(s),\qquad j=n\\ -\left(\dfrac{\eta_{Y}}{\exp(Mh^{*}(s-1))+(M-1)}+h^{*}(s-1)\right)&=-\,h^{*}(s),\qquad\qquad j\neq n\end{cases}\] (39)

And the equations of Lemma 5 also hold for step \(s\). So we finish the proof. 

**Remark 1**.: _If we following the original dynamics (Eqn. 7), then it corresponds to the \(W\) dynamics as follows:_

\[\dot{W}=\eta_{Y}(\bm{e}_{n}+(I+E)E^{\prime}\bm{e}_{n})(\bm{e}_{n}-\bm{\alpha}_ {n})^{\top}=\eta_{Y}F^{\top}\bm{f}_{n}(\bm{e}_{n}-\bm{\alpha}_{n})^{\top}\] (40)

_When all off-diagonal elements of \(E\) are identical, i.e., \(\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}}=\rho\) for \(n\neq n^{\prime}\), then \(0\leq\rho\leq 1\) and we have_

\[\dot{w}_{n} = \eta_{Y}(\bm{e}_{n}-\bm{\alpha}_{n})^{\top}\] (41) \[\dot{w}_{j} = \eta_{Y}\rho(\bm{e}_{n}-\bm{\alpha}_{n})^{\top},\qquad j\neq n\] (42)_So if different sequence classes are sampled uniformly, then by similar induction argument, we will have_

\[\bm{w}_{n}(N)=(M-1)h^{*}(N/K)\left[\bm{\zeta}_{n}+\rho\sum_{n^{\prime}\neq n}\bm{ \zeta}_{n^{\prime}}\right]=(1-\rho)(M-1)h^{*}(N/K)\bm{\zeta}_{n}\] (43)

_where the last equation is due to the fact that \(\sum_{n}\bm{\zeta}_{n}=\frac{M}{M-1}\sum_{n}\left(\bm{e}_{n}-\frac{1}{M}\bm{1} \right)=\frac{M}{M-1}(\bm{1}-\bm{1})=0\). This means that \(\sum_{n^{\prime}\neq n}\bm{\zeta}_{n^{\prime}}=-\bm{\zeta}_{n}\). Therefore, the effective learning rate is \(\eta^{\prime}_{Y}:=(1-\rho)\eta_{Y}\leq\eta_{Y}\)._

### Property of \(h^{*}(s)\) and its continuous counterpart.

Before further investigation on \(Y\), we need to get some basic properties of \(h^{*}\), in particular, how fast it grows over time. First, if we consider the continuous version of \(h^{*}\), namely \(h\), then we can directly obtain the equation that \(h\) needs to satisfy by integrating the corresponding differential equation.

**Lemma 6**.: _If we consider the continuous version of \(h^{*}(s)\), namely \(h\), as the following ODE:_

\[\frac{\mathrm{d}h}{\mathrm{d}t}=\frac{\eta_{Y}}{(M-1)+\exp(Mh)}\] (44)

_and assume \(h(0)=0\), then we have_

\[\exp(Mh(t))+(M-1)Mh(t)=M\eta_{Y}t+1\] (45)

Then we will show that the \(h\) is actually almost the same as the original step function \(h^{*}\).

**Lemma 7**.: _For \(h\) and \(h^{*}\) we have:_

* _(a) For any_ \(s\in\mathbb{N},0\leq h^{*}(s)-h(s)\leq\frac{2\eta_{Y}}{M}\)_. Then there exists some constant_ \(c=\Theta(1)\) _such that for any_ \(s\leq\ln(M)/\eta_{Y}\)_,_ \(h(s+c)\geq h^{*}(s)\geq h(s)\)_._
* _(b)_ \(h^{*}(s)-h(s)\to 0\) _when_ \(s\to+\infty\)_._

Proof.: **(a)** First we show that \(h^{*}(s)\geq h(s)\) for all \(s\in\mathbb{N}\), and the convex packet function of \(h^{*}\) can almost control the upper bound of \(h\). Define \(h^{\circ}:\mathbb{R}^{+}\to\mathbb{R}^{+}\) as follows:

\[h^{\circ}(t):=(t-\lfloor t\rfloor)\cdot[h^{*}(\lceil t\rfloor)-h^{*}(\lfloor t \rfloor)]+h^{*}(\lfloor t\rfloor),\;\forall t\in\mathbb{R}^{+}\] (46)

Here \(\lceil\cdot\rceil\) and \(\lfloor\cdot\rfloor\) mean ceil function and floor function, respectively. It's clear that \(h^{\circ}\) is a strictly monotonically increasing function, and for any \(s\in\mathbb{N}\), \(h^{\circ}(s)=h^{*}(s)\), while for any \(t\notin\mathbb{N}\), \((t,h^{\circ}(t))\) lies on the line connecting point \((\lfloor t\rfloor,h^{*}(\lfloor t\rfloor))\) and point \((\lceil t\rceil,h^{*}(\lceil t\rceil))\). To prevent ambiguity, we let \(\dot{h}^{\circ}(t)\) to be the left limit of \(h^{\circ}\), i.e., \(\dot{h}^{\circ}(t)=\lim_{t^{\prime}\to t-\dot{h}}\dot{h}^{\circ}(t^{\prime})\).

We claim \(h(t)\leq h^{\circ}(t),\;\forall t\in\mathbb{R}^{+}\). We prove it by induction. First when \(t=0\), we have \(h^{\circ}(0)=h^{*}(0)=0\). Then we assume \(h(t^{\prime})\leq h^{\circ}(t^{\prime})\) hold for time \(t^{\prime}\leq t\in\mathbb{N}\) and prove that \(h(t^{\prime})\leq h^{\circ}(t^{\prime})\) hold for \(t^{\prime}\in(t,t+1]\). If this is not true, then from the continuity of \(h^{\circ}\) and \(h\), we know it must exist \(t^{\prime\prime}\in(t,t+1]\) such that \(h(t^{\prime\prime})\geq h^{\circ}(t^{\prime\prime})\) and \(\dot{h}(t^{\prime\prime})>\dot{h}^{\circ}(t^{\prime\prime})\). The later condition results that \(\eta_{Y}[M-1+\exp(Mh(t^{\prime\prime}))]^{-1}>\eta_{Y}[M-1+\exp(Mh^{*}( \lfloor t^{\prime\prime}\rfloor))]^{-1}\). So

\[h(t^{\prime\prime})<h^{*}(\lfloor t^{\prime\prime}\rfloor)=h^{\circ}(\lfloor t ^{\prime\prime}\rfloor)\leq h^{\circ}(t^{\prime\prime})\] (47)

This contradicts the hypothesis \(h(t^{\prime\prime})\geq h^{\circ}(t^{\prime\prime})\). So \(h(t^{\prime})\leq h^{\circ}(t^{\prime})\) hold for \(t^{\prime}\in(t,t+1]\) and thus for all \(t\in\mathbb{R}^{+}\). Hence for any \(s\in\mathbb{N}\), we have \(h(s)\leq h^{\circ}(s)=h^{*}(s)\). Actually, we can use the similar method to prove that \(h(s)<h^{*}(s)\) for any \(s\in\mathbb{N}^{+}\).

Then we show \(h^{*}(s)-h(s)\leq 2\eta_{Y}/M\) by proving that for any \(s\in\mathbb{N}^{+}\), \(h(s)\) must meet at least one of the following two conditions:

**(i)**\(h(s)\in[h^{*}(s-1),h^{*}(s)]\).

**(ii)**\(h^{*}(s)-h(s)<h^{*}(s-1)-h(s-1)\).

If (i) doesn't hold, then we have for any \(t\in[s-1,s),h(t)\leq h(s)<h^{*}(s-1)=h^{\circ}(s-1)\), which results that \(\dot{h}(t)>\dot{h}^{\circ}(t)\) for all \(t\in[s-1,s)\). Therefore, \(h^{*}(s)-h^{*}(s-1)=h^{\circ}(s)-h^{\circ}(s-1)<h(s)-h(s-1)\) and thus \(h(s)\) meets condition (ii). It's clear that \(h(0)\) and \(h(1)\) meet (i).

These two conditions mean that the gap between \(h^{*}\) and \(h\) will not grow if \(h(s)\) is smaller than \(h^{*}(s-1)\). Then for all \(h(s)\) that meet (i), we have \(h^{*}(s)-h(s)\leq h^{*}(s)-h^{*}(s-1)\leq h^{*}(1)-h^{*}(0)=\eta_{Y}/M\) from Eqn. 33. And for any \(s\geq 2\), every time \(h(s)\) transfer from (i) to (ii) exactly at \(s\), which means that \(h(s-1)\) meets (i) and thus no smaller than \(h^{*}(s-2)\), we get \(h^{*}(s)-h(s)\leq h^{*}(s)-h(s-1)\leq h^{*}(s)-h^{*}(s-2)\leq h^{*}(2)-h^{*}(0) \leq 2\eta_{Y}/M\).

Finally from Eqn. 53 in Lemma 9, when \(s\leq\frac{\ln M}{\eta_{Y}}\), we get \(h(s)=\Theta(\eta_{Y}t/M)\) and thus there exist some constant \(c=\Theta(1)\) such that \(h(s+c)\geq h(s)+2\eta_{Y}/M\geq h^{*}(s)\geq h(s)\).

**(b)** Assume that there exist \(\epsilon\in(0,2\eta_{Y}/M]\) such that \(h^{*}(s)-h(s)\geq\epsilon\) for all \(s\in\mathbb{N}\). Since \(h\) is unbounded, then \(\dot{h}(t)\to 0\) when \(t\to\infty\) from Eqn. 33, so there exist some \(s^{\prime}_{0}\in\mathbb{N}\) such that when \(s\geq s^{\prime}_{0}\), \(h(s+1)-h(s)\leq\epsilon+\ln(1/2)/M\). Also, from Lemma 9 we know that exists \(s^{\prime\prime}_{0}=\frac{(3+\delta)\ln(M)}{\eta_{Y}}\) where \(\delta>0,\delta=\Theta(1)\) such that when \(s\geq s^{\prime\prime}_{0}\), \(\exp(Mh(s))>2(M-1)\). Since \(s\to\infty\), we just consider the case that \(s=\lfloor t\rfloor\geq s_{0}:=\max(s^{\prime}_{0},s^{\prime\prime}_{0})\). Then denote \(\Delta_{1}:=\frac{2(M-1)}{\exp(Mh(s))}<1\), we have:

\[\dot{h}^{\circ}(t)-\dot{h}(t) =\frac{\eta_{Y}}{M-1+\exp(Mh^{*}(s))}-\frac{\eta_{Y}}{M-1+\exp(Mh (t))}\] (48) \[\leq\frac{\eta_{Y}}{M-1+\exp(M(h(s)+\epsilon))}-\frac{\eta_{Y}}{ M-1+\exp(Mh(s+1))}\] \[=-\frac{\eta_{Y}\exp(Mh(s))\cdot[\exp(M\epsilon)-\exp(Mh(s+1)-Mh( s))]}{[M-1+\exp(M(h(s)+\epsilon))]\cdot[M-1+\exp(Mh(s+1))]}\] \[\leq-\frac{\eta_{Y}\exp(Mh(s))\cdot\exp(M\epsilon)}{2[M-1+\exp(M( h(s)+\epsilon))]\cdot[M-1+\frac{1}{2}\exp(M(h(s)+\epsilon))]}\] \[\leq-\frac{\eta_{Y}\exp(M\epsilon)}{(1+\Delta_{1})^{2}\exp(Mh(s) )\exp(4\eta_{Y})},\quad(s\geq s_{0}=\max(s^{\prime}_{0},s^{\prime\prime}_{0}))\] \[\leq-\frac{\exp(M\epsilon)}{4\exp(4\eta_{Y})M}\cdot\frac{1}{t}=: -\frac{C}{t}\]

Here \(C=\frac{\exp(M\epsilon)}{4\exp(4\eta_{Y})M}>0\) and for the last inequality, we use the fact that \(t\geq s^{\prime}_{0}>\frac{3\ln M}{\eta_{Y}}\) and thus \(h(s)\leq h(t)=O(\frac{\ln(M\eta_{Y}t)}{M})\) from Lemma 9. So we get

\[[h^{\circ}(t)-h(t)]-[h^{\circ}(s_{0})-h(s_{0})]\leq-\int_{t^{\prime}=s_{0}}^{ \infty}\frac{Cdt}{t}\to-\infty\] (49)

This contradicts \(h^{\circ}(t)-h(t)\geq 0\)! So the original assumption doesn't hold, which means that \(h^{*}(s)-h(s)\to 0\) when \(s\to\infty\). 

**Remark 2**.: _By some qualitative estimation, we claim that if \(\eta_{Y}=O(1)\), then there exists some constant \(c=O(\ln M)\) such that \(h(s)\leq h^{*}(s)\leq h(s+c)\) for all \(s>s_{1}:=\frac{2\ln(1+\omega_{1})}{\eta_{Y}}\) where \(\omega_{1}=\Theta(\ln\ln M/\ln M)\) is defined in Lemma 8. Denote \(\delta h(t):=h^{\circ}(t)-h(t)\), when \(\delta h(t)\ll h(t)\)

Figure 8: Numerical simulation of \(h^{*}\) and \(h\) with changing \(\eta_{Y}\). The stepped folded line represents \(h^{*}\) and the smooth curve represents \(h\). The gap between \(h^{*}\) and \(h\) is bounded and goes to zero when time grows.

_we have \(\delta h(t)=\hat{h}^{\circ}(t)-\hat{h}(t)\asymp-\eta_{Y}M\cdot\delta h(t)\cdot \exp(-Mh(t))\asymp-\delta h(t)/t\) by computing the second-order derivative of \(\delta h\), and thus \(h^{\circ}(t)-h(t)\asymp 2\eta_{Y}s_{0}/(Mt)=O(\ln M/(Mt))\). Combining this with the fact that \(h(t)=\Theta(\ln(M\eta_{Y}t)/M)\) when \(t>s_{1}\), we prove our claim. The results of Lemma 7 and Remark 2 are also confirmed by the numerical simulation results as Fig. 8._

So from Lemma 7 and Remark 2, we just assume \(\eta_{Y}<1\) and replace \(h^{*}\) with \(h\) in the latter parts for convenience. Then we further investigate the properties of Eqn. 45.

**Lemma 8**.: _There exists \(\omega_{i},0<\omega_{i}\ll 1,i=2,3\), such that for \(h\in\mathbb{J}_{1}:=[\frac{1}{M^{2-\omega_{0}}},\frac{(1+\omega_{1})\ln(M)}{M}]\), we have \(\exp(Mh(t))\leq(M-1)Mh(t)\). And for \(h\notin\mathbb{J}_{1}\), we have \(\exp(Mh(t))>(M-1)Mh(t)\). Here \(\omega_{1}=\Theta(\frac{\ln(M)}{\ln(M)})\), and if \(M\gg 100\), we have \(\omega_{0}\lesssim(\frac{1}{M^{0.99}\ln M})\ll 0.01\)._

Proof.: It's obvious that \(\exp(Mh(t))-(M-1)Mh(t)\) has two zero points in \(\mathbb{R}^{+}\). Let \(h(t)=M^{-(2-\omega_{0})}\), we get

\[\omega_{0}=\frac{1}{\ln M}(\ln(\frac{M}{M-1})+\frac{1}{M^{1-\omega_{0}}})=O( \frac{1}{M^{0.99}\ln(M)})\] (50)

For another zero point, let \(\omega_{1}\in(0,1)\) to be some constant such that \(h(t)=\frac{(1+\omega_{1})\ln(M)}{M}\) satisfies \(\exp(Mh)=(M-1)Mh\), then we get

\[M^{\omega_{1}}=(1+\omega_{1})\ln(M)\frac{(M-1)}{M}=c^{\prime} \cdot\ln(M)\frac{(M-1)}{M}\] (51) \[\Rightarrow \omega_{1}=\Theta(\frac{\ln\ln(M)}{\ln(M)})\]

where \(c^{\prime}\in(0.5,2)\) is some universal constant. 

**Remark 3**.: _From Lemma 8, if we assume \(M\gg 100\), then \(\omega_{0}\ll 0.01\), and if we assume \(\eta_{Y}\gg\frac{1}{M^{1-\omega_{0}}}>\frac{1}{M^{0.99}}\), then \(h(1)\geq\frac{\eta_{Y}}{M}\gg\frac{1}{M^{2-\omega_{0}}}\) and function \(\exp(Mh(t))-(M-1)Mh(t)\) has only one zero point \(\frac{(1+\omega_{1})\ln M}{M}\) in \([1,\infty)\). For convenience, we just assume \(M\gg 100\) and \(1>\eta_{Y}\gg\frac{1}{M^{0.99}}\) and thus focus on the unique zero point \(\frac{(1+\omega_{1})\ln M}{M}\) of \(h\) in the latter parts._

We can then show the properties of speed control coefficient \(\gamma(t):=\frac{(M-1)^{2}h(t/K)}{(M-1)+\exp(Mh(t/K))}\) as below.

**Lemma 9**.: _We have two stage for \(h\) and \(\gamma\):_

* _When_ \(t\leq\frac{K\ln(M)}{\eta_{Y}}\)_, we have_ \(\exp(Mh(t/K))\leq\min(M-1,(M-1)Mh(t/K))\)_,_ \(h=O(\eta_{Y}t/(MK))\) _and_ \(\gamma(t)=O(\eta_{Y}t/K)\)_._
* _When_ \(t\geq\frac{2(1+\omega_{1})K\ln(M)}{\eta_{Y}}\) _where_ \(\omega_{1}=\Theta(\frac{\ln\ln M}{\ln M})\) _is defined in Lemma_ 8_, we have_ \(\exp(Mh(t/K))\geq\max(M-1,(M-1)Mh(t/K))\)_,_ \(h=O(\frac{1}{M}\ln(M\eta_{Y}t/K))\) _and_ \(\gamma(t)=O(\frac{K\ln(M\eta_{Y}t/K)}{\eta_{Y}t})\)_._

Proof.: For convenience, we just let \(K=1\). And the proof for \(K\neq 1\) is similar. We denote \(\Delta_{1}(h):=\frac{\exp(Mh)}{M-1}\) and \(\Delta_{2}(h):=\frac{\exp(Mh)}{(M-1)Mh}\).

**Step 1**: \(t\leq\frac{\ln(M)}{\eta_{Y}}\). If \(h\geq\frac{\ln(M-1)}{M}\), from Eqn. 45 we have:

\[t\geq\frac{M-2+(M-1)\ln(M-1)}{M\eta_{Y}}>\frac{\ln(M)}{\eta_{Y}}\] (52)

So when \(t\leq\frac{\ln(M)}{\eta_{Y}}\) we have \(h<\frac{\ln(M-1)}{M}\), and thus \(\exp(Mh(t))\leq\min(M-1,(M-1)Mh(t))\), i.e., \(\Delta_{1},\Delta_{2}\leq 1\). Then from Eqn. 45 we get

\[h=\frac{M\eta_{Y}t+1}{(1+\Delta_{2})M(M-1)}=O\left(\frac{1}{M}\eta_{Y}t\right)\] (53)

\[\gamma=\frac{(M-1)h}{1+\Delta_{1}}=\frac{M\eta_{Y}t+1}{(1+\Delta_{1})(1+\Delta _{2})M}=O(\eta_{Y}t)\] (54)

**Step 2**: \(t>\frac{2(1+\omega_{1})\ln(M)}{\eta_{Y}}\) where \(\omega_{1}=\Theta(\frac{\ln\ln(M)}{\ln(M)})\). So now \(h>\frac{\ln(M-1)}{M}\) and thus \(\Delta_{1}>1\) from Eqn. 52. Then if \(\exp(Mh)\leq M(M-1)h\), i.e. \(\Delta_{2}\leq 1\), from Lemma 8 we have \(h=\frac{M\eta_{Y}t+1}{(1+\Delta_{2})M(M-1)}\leq\frac{(1+\omega_{1})\ln(M)}{M}\). Therefore,

\[t\leq\frac{1}{\eta_{Y}}((1+\omega_{1})(1+\Delta_{2})\frac{M-1}{M}\ln M-\frac{1 }{M})<\frac{2(1+\omega_{1})\ln(M)}{\eta_{Y}}.\] (55)

Contradiction! So when \(t\geq\frac{2(1+\omega_{1})\ln(M)}{\eta_{Y}}\), we have \(\Delta_{2}>1\). Then from Eqn. 45 we get:

\[h=\frac{1}{M}\ln\left(\frac{M\eta_{Y}t+1}{1+\Delta_{2}^{-1}}\right)=O\left( \frac{1}{M}\ln(M\eta_{Y}t)\right)\] (56)

\[\gamma=\frac{M-1}{M}\frac{(M-1)\ln(\frac{M\eta_{Y}t+1}{1+\Delta_{2}^{-1}})}{( 1+\Delta_{1}^{-1})(\frac{M\eta_{Y}t+1}{1+\Delta_{2}^{-1}})}=O\left(\frac{\ln( M\eta_{Y}t)}{\eta_{Y}t}\right)\] (57)

### The dynamics under multiple uniformly sampled sequence classes

We then generalize our analysis of \(W\) to the case where \(x_{T+1}\) can be any value in \([K]\) rather than fixing \(x_{T+1}=n\) with the key observation that the row vectors of \(W^{\prime}\) can be independently updated. Before formalizing this result, we first conduct the concentration inequality of the sampling number for each next-token case. Let \(N_{n}:=\sum_{i=1}^{N}\mathbb{I}[x_{T+1}=n]\) to be the number of times the event \(x_{T+1}=n\) happens, then we have:

**Lemma 10**.: _For \(\delta\in(0,1)\), with probability at least \(1-\delta\) we have_

\[|N_{n}-\lceil N\mathbb{P}(n)\rceil|\leq\sqrt{\frac{N}{2}\ln(\frac{2}{\delta} )}+1<\sqrt{N\ln(\frac{2}{\delta})}\] (58)

Proof.: From Hoeffding's inequality, we have

\[\mathbb{P}\left(\left|\frac{N_{n}}{N}-\mathbb{P}(n)\right|>t\right)\leq 2\exp( -2Nt^{2})\] (59)

Let \(t=\sqrt{\frac{1}{2N}\ln(\frac{2}{\delta})}\) and we can get the results by direct calculation. 

**Remark 4**.: _From Lemma 10, if we consider the uniform sampling case where \(\mathbb{P}(n)\equiv\frac{1}{K}\), then \(N\mathbb{P}(n)=N/K\gg\sqrt{N}\). So \(N_{n}\) are all concentrated around \(N\mathbb{P}(n)\). Recall the definition of \(\bar{N}=\lceil N/K\rceil\) and \(\Delta=\lceil\sqrt{N\ln(\frac{1}{\delta})}\rceil\), with probability at least \(1-\delta\) we have:_

\[|N_{n}-\bar{N}|\lesssim\Delta\ll\bar{N}\] (60)

We then further investigate the concentration of \(h(N_{n})\):

**Lemma 11**.: _For \(\delta\in(0,1)\), with probability at least \(1-\delta\) we have_

\[|h(N_{n})-h(\bar{N})|\lesssim h(\bar{N})\cdot\frac{\Delta}{\bar{N}}\] (61)

\[|\frac{1}{M-1+\exp(Mh(N_{n}))}-\frac{1}{M-1+\exp(Mh(\bar{N}))}|\] (62) \[\lesssim \frac{1}{M-1+\exp(Mh(\bar{N}))}\cdot\sigma^{\prime}\]

_where \(\sigma^{\prime}>0\) is some constant such that \(\sigma^{\prime}\leq\frac{1}{3}\eta_{Y}\Delta\ll\ln(M)\). And if \(N\geq\frac{2K(1+\omega_{1})\ln M}{\eta_{Y}}\) where \(\omega_{1}\) is defined in Lemma 8, then \(\sigma^{\prime}\lesssim\frac{\Delta}{\bar{N}}\ll 1\)._Proof.: First, we note that \(h\) has a decreasing gradient, so \(h(x)\geq\dot{h}(x)\times x\) and \(h(x_{1}+x_{2})-h(x_{1})\leq\dot{h}(x_{1})\times x_{2}\) for any \(x_{1},x_{2}\geq 0\). So with probability at least \(1-\delta\), we have:

\[|h(N_{n})-h(\bar{N})|\leq h(\bar{N})-h(\bar{N}-\Delta)\leq\dot{h}(\bar{N}- \Delta)\times\Delta\leq\frac{h(\bar{N})\Delta}{\bar{N}-\Delta}\asymp h(\bar{N })\cdot\frac{\Delta}{\bar{N}}\] (63)

For the second inequality, without loss of generality, we let \(N_{n}>\bar{N}\). Denote \(g(s):=(M-1+\exp(Mh(s)))^{-1}\) and note that:

\[\frac{\mathrm{d}g}{\mathrm{d}s} =\frac{M\exp(Mh(s))}{(M-1+\exp(Mh(s)))^{2}}\cdot\frac{\mathrm{d} h}{\mathrm{d}s}\] (64) \[=\frac{1}{M-1+\exp(Mh(s))}\cdot\frac{\eta_{Y}M\exp(Mh(s))}{(M-1+ \exp(Mh(s)))^{2}}\] \[\leq\frac{1}{M-1+\exp(Mh(s))}\cdot\frac{M}{(M-1)}\cdot\frac{ \eta_{Y}}{4}\]

the last equality holds only when \(h(s)=\frac{\ln(M-1)}{M}\). So from \(|g(\bar{N}+\Delta)-g(N_{n})|\leq\max_{s\in[N_{n},N_{n}+\Delta]}\dot{g}(s)\cdot\Delta\), we get:

\[|\frac{1}{M-1+\exp(Mh(\bar{N}+\Delta))}-\frac{1}{M-1+\exp(Mh(\bar{N}))}|\leq \frac{1}{M-1+\exp(Mh(\bar{N}))}\cdot\frac{1}{3}\eta_{Y}\Delta\] (65)

If \(\bar{N}<\frac{2(1+\omega_{1})\ln(M)}{\eta_{Y}}+\Delta\) with \(\omega_{1}=\Theta(\frac{\ln\ln M}{\ln M})\) defined in Lemma 8, we have \(\sigma^{\prime}\leq\eta_{Y}\Delta/3\ll\eta_{Y}\bar{N}\lesssim\ln(M).\) If \(\bar{N}\geq\frac{2(1+\omega_{1})\ln(M)}{\eta_{Y}}+\Delta\), we utilize the Eqn.45 and obtain:

\[|\frac{1}{M-1+\exp(Mh(\bar{N}+\Delta))}-\frac{1}{M-1+\exp(Mh(\bar {N}))}|\] \[= \frac{1}{M-1+\exp(Mh(\bar{N}))}\cdot\frac{|\exp(Mh(\bar{N}+ \Delta))-\exp(Mh(\bar{N}))|}{M-1+\exp(Mh(\bar{N}+\Delta))}\] \[\leq \frac{1}{M-1+\exp(Mh(\bar{N}))}\cdot\frac{M\eta_{Y}\Delta}{M-1+ \exp(Mh(\bar{N}+\Delta))},\quad(Eqn.\ 45)\] \[\leq \frac{1}{M-1+\exp(Mh(\bar{N}))}\cdot\frac{M\eta_{Y}\Delta}{M+ \frac{1}{2}\cdot M\eta_{Y}(\bar{N}+\Delta)},\quad(\text{Lemma}\ 9,N_{n}\geq\frac{2(1+\omega_{1})\ln(M)}{\eta_{Y}}+\Delta)\] \[\lesssim \frac{1}{M-1+\exp(Mh(\bar{N}))}\cdot\frac{\Delta}{\bar{N}}\]

So \(\sigma^{\prime}\leq\Delta/\bar{N}\). When \(N_{n}<\bar{N}\), with probability at least \(1-\delta\) we have \(N_{n}\gtrsim\bar{N}-\Delta\), and similar inequalities also hold for such cases, so we finish the proof. 

Recall that \(\bm{\zeta}_{n}\in\mathbb{R}^{M}\) is defined as \(\bm{\zeta}_{n}=\frac{M}{M-1}(\bm{e}_{n}-\frac{1}{M}\bm{1})\). And we have \(q_{1}:=\bm{\zeta}_{i}^{\top}\bm{\zeta}_{i}=1+\frac{1}{M^{-1}}\), \(q_{0}:=\bm{\zeta}_{j}^{\top}\bm{\zeta}_{i}=-\frac{M}{(M-1)^{2}}\) for all \(i,j\in[M]\) where \(i\neq j\). For convenience, we denote \(W^{\prime}(N):=[\bar{\bm{w}}_{1}(N),...,\bar{\bm{w}}_{K}(N),\bm{0},...,\bm{0}] ^{\top}\in\mathbb{R}^{M\times M}\), where \(\bar{\bm{w}}_{n}(N):=(M-1)h(\lceil N/K\rceil)\bm{\zeta}_{n}=(M-1)h(\bar{N})\bm{ \zeta}_{n}\). So using these concentration inequalities, we get:

**Lemma 12**.: _Assume the assumptions in Lemma 5 hold but we uniformly sample the training data. Then if the total number of epochs \(N\) satisfies \(N\gg K^{2}\), we have \(Y=(F^{\prime})^{-\top}(I+\Theta^{\prime})\bar{W}^{\prime}(N)\) where \(\Theta^{\prime}:=\text{diag}(\theta_{1},\dots,\theta_{K},0,\dots,0)\in\mathbb{ R}^{M\times\bar{M}}\) and with probability at least \(1-\delta\) we have \(|\theta_{i}|\lesssim\frac{K}{\sqrt{N}}\sqrt{\ln(\frac{K}{\delta})},\forall i \in[K]\)._

Proof.: From Lemma 5 and the first inequality of Lemma 11, we know that

\[\bm{w}_{n}(N) = (M-1)h(N_{n})\bm{\zeta}_{n}\] (66) \[= (M-1)h(\bar{N})\bm{\zeta}_{n}+(M-1)(h(N_{n})-h(\bar{N}))\bm{\zeta} _{n}\] (67) \[= (1+\theta_{n})\cdot(M-1)h(\bar{N})\bm{\zeta}_{n}\] (68) \[= (1+\theta_{n})\bar{\bm{w}}_{n}(N)\] (69)where for any \(\delta\in(0,1)\), with probability at least \(1-\delta\) we have \(|\theta_{i}|\lesssim\frac{K}{\sqrt{N}}\sqrt{\ln(\frac{K}{\delta})},\forall n\in[K]\). Therefore, \(W^{\prime}(N)=[\bm{w}_{1}(N),\ldots,\bm{w}_{K}(N),\bm{0},\ldots,\bm{0}]^{\top}= (I+\bm{\Theta}^{\prime})\bar{W}^{\prime}(N)\), then from \(W^{\prime}=(F^{\prime})^{\top}Y\), we finish the proof. 

Then, we can give out the exact solution of \(Y\) by pointing out the properties of \(F^{\circ}\) and \(F^{\prime}\) from the observation that each row of \(Y\) should be the linear combination of vectors in \(\{\bm{f}_{n}^{\top}\}_{n\in[K]}\):

**Theorem 5**.: _If Assumption 2 holds and \(Y(0)=0\). Furthermore, we assume the training data is uniformly sampled and the total number of epochs \(N\) satisfies \(N\gg K^{2}\). Then the solution of Eqn. 26 will be:_

\[Y=(F^{\dagger})^{\top}(I+\Theta)\bar{W}(N)=F(I-E^{\prime})(I+\Theta)\bar{W}(N)\] (70)

_Here \(\Theta:=\text{diag}(\theta_{1},\ldots,\theta_{K})\) and for any \(\delta\in(0,1)\), with probability at least \(1-\delta\) we have \(|\theta_{i}|\lesssim\frac{K}{\sqrt{N}}\sqrt{\ln(\frac{K}{\delta})},\forall i \in[K]\)._

Proof.: Let \(\bm{q}_{i},i\in[M]\) be the \(i\)-th row vector of \((F^{\prime})^{-1}\), then we have \(\bm{q}_{j}^{\top}\bm{f}_{i}=\mathbb{I}[i=j]\). From Lemma 12 we get \(Y=(F^{\prime})^{-\top}(I+\Theta^{\prime})\bar{W}^{\prime}(N)\). And from Eqn. 26, we know all the columns of \(Y\) are the linear combination of \(\bm{f}_{n},n\in[K]\). Note that \(\bar{W}(N)\) has only top \(K\) rows to be non-zero, so we need to constrain that all the top \(K\) columns of \((F^{\prime})^{-\top}\), i.e., \(\bm{q}_{i},i\in[K]\), to be the linear combination of \(\bm{f}_{n},n\in[K]\), which means that \(\bm{q}_{1},\ldots,\bm{q}_{K}\) must be the basis of \(\Xi:=\text{span}(\bm{f}_{j};j\in[K])\) and thus \(\bm{q}_{K+1},\ldots,\bm{q}_{M}\) are the basis of \(\Xi^{\prime}:=\text{span}(\bm{f}_{j};K\leq j\leq M)\). Therefore, we get \(\Xi\perp\Xi^{\prime}\), and thus \([\bm{q}_{1},\ldots,\bm{q}_{K}]\) can only be \((F^{\dagger})^{\top}\). So the proof is done.

Actually, we see that the result of Theorem 5 matches the modified gradient update on \(Y\) (Eqn. 26). And we show that using such reparameterization dynamics, we can still approach the critical point of Eqn. 7 in the rate of \(\mathcal{O}(\frac{1}{N})\):

**Corollary 1**.: _Assume assumptions in Theorem 5 hold, \(M\gg 100\) and \(\eta_{Y}\) satisfies \(M^{-0.99}\ll\eta_{Y}<1\). Then \(\forall n\in[K]\), we have_

\[(\bm{x}_{T+1}-\bm{\alpha}_{n}) =\frac{M-1}{(M-1)+\exp(Mh(N_{n}))}\bm{\zeta}_{n}\] (71) \[=\frac{M-1}{(M-1)+\exp(Mh(\bar{N}))}\cdot(1+\sigma)\cdot\bm{\zeta }_{n}\]

_where \(\sigma>-1\) and for any \(\delta\in(0,1)\), with probability at least \(1-\delta\) we have \(|\sigma|\lesssim\eta_{Y}\sqrt{N\ln(\frac{1}{\delta})}\), and when \(N\gg K(\sqrt{N\ln(\frac{1}{\delta})}+\frac{2(1+\omega_{1})\ln M}{\eta_{Y}})\) with \(\omega_{1}\) defined in Lemma 8, \(|\sigma|\lesssim\frac{K}{\sqrt{N}}\sqrt{\ln(\frac{1}{\delta})}\). Further, to let \(\|\bm{x}_{T+1}-\bm{\alpha}_{n}\|_{2}\leq\epsilon\) with probability at least \(1-\delta\) for any \(n\in[K]\) and \(\epsilon\ll 1\), we need the total number of training epochs to be at most \(O(\frac{K}{\epsilon\eta_{Y}}\log(\frac{M}{\epsilon}))\)._

Proof.: Note that \(\bm{x}_{T+1}=\bm{e}_{n}\), then we just need to combine Lemma 5 and the second inequality of Lemma 11, to get Eqn. 71. Denote \(S_{n}\) to be the number of training epochs that are needed to let \(\|\bm{x}_{T+1}-\bm{\alpha}_{n}\|_{2}\asymp\epsilon\), then we have

\[h(S_{n})\asymp\frac{1}{M}\ln(\frac{M}{\epsilon})\] (72)

But note that \(h(t+1)-h(t)\geq\frac{\eta_{Y}}{M-1+\exp(Mh(S_{n}))}\asymp\frac{\eta_{Y}\epsilon }{M-1},\forall t\in[0,S-1]\) from Eqn. 71, we have

\[S_{n}\lesssim\frac{h(S_{n})}{\eta_{Y}\epsilon/(M-1)}\asymp\frac{1}{\epsilon\eta _{Y}}\ln(\frac{M}{\epsilon})\] (73)

Note that \(\epsilon\ll 1\) and we have \(N\gg K^{2}\), then we have \(S=\sum_{n}S_{n}\lesssim\frac{K}{\epsilon\eta_{Y}}\ln(\frac{M}{\epsilon})\).

### Proof of Theorem 1

Finally, we turn to prove Theorem 1. Obviously, all the diagonal elements of \(E\) are zero and all the off-diagonal elements of \(E\) are non-negative since \(\bm{c}_{l|m,n}\geq 0\). Note that \(E\) is a real symmetric matrix, then it can be orthogonal diagonalization by \(E=U^{\top}DU\) where \(U:=[\bm{u}_{1},...,\bm{u}_{K}]\in O_{K\times K},D=\text{diag}(\lambda_{1},..., \lambda_{K})\) and \(|\lambda_{1}|\geq...\geq|\lambda_{K}|\geq 0\). Then we can get the following properties of \(E\) and \(E^{\prime}\):

**Lemma 13**.: \(\max_{i,j\in[K]}(|E_{ij}|)\leq|\lambda_{1}|\)_._

Proof.: We have:

\[|E_{ij}|=\bm{u}_{i}^{\top}D\bm{u}_{j}\leq|\lambda_{1}|\cdot\|\bm{u}_{i}\|_{2} \|\bm{u}_{j}\|_{2},\quad\forall i,j\in[K]\] (74)

**Lemma 14**.: _If \(E\in\mathbb{R}^{K}\) satisfies \(|\lambda_{1}|\leq\lambda<1\), then \((I+E)\) is invertible and \((I+E)^{-1}=I-E^{\prime}\),where \(E^{\prime}\) satisfies \(E^{\prime}=U^{\top}D^{\prime}U\) and \(D^{\prime}=\text{diag}(\lambda_{1}^{\prime},...,\lambda_{K}^{\prime})\) and \(\lambda_{i}^{\prime}=\frac{\lambda_{i}}{1+\lambda_{i}},\forall i\in[K]\)._

Proof.: Since \(U\) is orthonormal and \(|\lambda_{i}|\leq\lambda<1\), we have \(E^{n}=U^{\top}D^{n}U\rightarrow\bm{O}\). Then from the property of the Neumann series, we get \(I+E\) is invertible and

\[(I+E)^{-1} = I+\sum_{n=1}^{\infty}(-1)^{n}E^{n}\] (75) \[= I+U^{\top}(\sum_{n=1}^{\infty}(-D^{n})U\] (76) \[= I-U^{\top}D^{\prime}U=:I-E^{\prime}\] (77)

Here we define \(D^{\prime}=\text{diag}(\lambda_{1}^{\prime},...,\lambda_{K}^{\prime})\) and use the fact that \(\sum_{n=1}^{\infty}(-\lambda_{i})^{n}=-\frac{\lambda_{i}}{1+\lambda_{i}}\) 

**Lemma 15**.: _If \(|\lambda_{1}|\leq\lambda<1\), then \(\max_{i\in[K]}|\lambda_{i}(E^{\prime})|\leq\frac{1}{1-\lambda}|\lambda_{1}| \leq\frac{\lambda}{1-\lambda}\)._

Proof.: We have

\[\max_{i\in[K]}|\lambda_{i}(E^{\prime})|=\max_{i\in[K]}|-\frac{\lambda_{i}}{1+ \lambda_{i}}|\leq\frac{\max_{i\in[K]}|\lambda_{i}|}{1-\max_{i\in[K]}|\lambda_{ i}|}\leq\frac{1}{1-\lambda}|\lambda_{1}|\] (78)

**Lemma 16**.: _Assume that Assumption 2 holds, then all the diagonal elements of \(E^{\prime}\) are non-positive,i.e., \(E^{\prime}_{ii}\leq 0,\forall i\in[K]\). Further, if there exist any \(k\neq i\in[K]\) such that \(E_{ki}>0\), then \(E^{\prime}_{ii}<0\)._

Proof.: Note that \(E_{ii}=\sum_{k=1}^{K}\lambda_{k}u_{ik}^{2}=0\) (here \(u_{ik}\) is the \(k\)-th component of eigenvector \(\bm{u}_{i}\)) and \(|\lambda_{k}|<1\), we have

\[E^{\prime}_{ii}=\sum_{k=1}^{K}\frac{\lambda_{k}}{1+\lambda_{k}}u_{ik}^{2}=\sum _{k=1}^{K}\lambda_{k}u_{ik}^{2}-\sum_{k=1}^{K}\frac{\lambda_{k}^{2}}{1+\lambda _{k}}u_{ik}^{2}=-\sum_{k=1}^{K}\frac{\lambda_{k}^{2}}{1+\lambda_{k}}u_{ik}^{2}\leq 0\] (79)

When \(E^{\prime}_{ii}=0\), then \(\bm{\lambda}:=(\lambda_{1},\dots,\lambda_{K})\) must don't have overlapping entries with respect to \(\bm{u}_{i}\), which results that \(E_{ij}:=\sum_{k=1}^{K}\lambda_{k}u_{ik}u_{jk}=0\) holds for any \(j\in[K]\). So we prove the results.

**Lemma 17**.: _If \(\lambda_{1}<1\), then \(|E^{\prime}_{nn^{\prime}}-E_{nn^{\prime}}|\leq|\lambda_{1}|^{2}(1-|\lambda_{1} |)^{-1}\)._Proof.: From Lemma 14 we have:

\[\begin{split}|E^{\prime}_{nn^{\prime}}-E_{nn^{\prime}}|& =|\sum_{k=1}^{K}\lambda_{k}u_{nk}u_{n^{\prime}k}-\sum_{k=1}^{K} \frac{\lambda_{k}}{1+\lambda_{k}}u_{nk}u_{n^{\prime}k}|\\ &=|\sum_{k=1}^{K}\frac{\lambda_{k}^{2}}{1+\lambda_{k}}u_{nk}u_{n^ {\prime}k}|\\ &\leq\frac{|\lambda_{1}|^{2}}{1-|\lambda_{1}|}\sum_{k=1}^{K}|u_{ nk}||u_{n^{\prime}k}|\\ &\leq\frac{|\lambda_{1}|^{2}}{1-|\lambda_{1}|}\sqrt{(\sum_{k=1}^ {K}|u_{nk}|^{2})(\sum_{k=1}^{K}|u_{n^{\prime}k}|^{2})}=\frac{|\lambda_{1}|^{2 }}{1-|\lambda_{1}|}\end{split}\] (80)

Finally we can prove our main theorem in Sec. 4.

**Theorem 1**.: _If Assumption 2 holds, the initial condition \(Y(0)=0\), \(M\gg 100\), \(\eta_{Y}\) satisfies \(M^{-0.99}\ll\eta_{Y}<1\), and each sequence class appears uniformly during training, then after \(t\gg K^{2}\) steps of batch size \(1\) update, given event \(x_{T+1}[i]=n\), the backpropagated gradient \(\bm{g}[i]:=Y(\bm{x}_{T+1}[i]-\bm{\alpha}[i])\) takes the following form:_

\[\bm{g}[i]=\gamma\left(\iota_{n}\bm{f}_{n}-\sum_{n^{\prime}\neq n}\beta_{nn^{ \prime}}\bm{f}_{n^{\prime}}\right)\] (9)

_Here the coefficients \(\iota_{n}(t)\), \(\beta_{nn^{\prime}}(t)\) and \(\gamma(t)\) are defined in Appendix with the following properties:_

* _(a)_ \(\xi_{n}(t):=\gamma(t)\sum_{n\neq n^{\prime}}\beta_{nn^{\prime}}(t)\bm{f}_{n}^{ \top}(t)\bm{f}_{n^{\prime}}(t)>0\) _for any_ \(n\in[K]\) _and any_ \(t\)_;_
* _(b)_ _The_ speed control coefficient \(\gamma(t)>0\) _satisfies_ \(\gamma(t)=O(\eta_{Y}t/K)\) _when_ \(t\leq\frac{\ln(M)\cdot K}{\eta_{Y}}\) _and_ \(\gamma(t)=O\left(\frac{K\ln(\eta_{Y}t/K)}{\eta_{Y}t}\right)\) _when_ \(t\geq\frac{2(1+\delta^{\prime})\ln(M)\cdot K}{\eta_{Y}}\) _with_ \(\delta^{\prime}=\Theta(\frac{\ln\ln M}{\ln M})\)_._

Proof.: Note that if Assumption 2 holds, then \(F^{\dagger}=(I-E^{\prime})F^{\top}\). Recall \(q_{1}:=1+\frac{1}{M-1}\approx 1\) and \(q_{0}:=-\frac{M}{(M-1)^{2}}\approx 0\). Then given \(x_{T+1}[i]=n\), we get:

\[\bm{g}[i] := Y(\bm{x}_{T+1}[i]-\bm{\alpha}[i])\] (81) \[= F(I-E^{\prime})(I+\Theta)\check{W}(N)(\bm{x}_{T+1}[i]-\bm{\alpha} [i]),\quad\text{(Theorem~{}\ref{thm:1})}\] (82) \[= (1+\sigma)\gamma*F(I-E^{\prime})(I+\Theta)[q_{0},\ldots,q_{1}, \ldots,q_{0}]^{\top},\quad\text{(Lemma~{}\ref{thm:1},\text{Corollary~{}\ref{thm:2})}}\] (83) \[= \gamma\left(\iota_{n}\bm{f}_{n}-\sum_{n^{\prime}\neq n,n^{\prime }\in[K]}\beta_{nn^{\prime}}\bm{f}_{n^{\prime}}\right)\] (84)

where

\[\gamma(t) := \frac{(M-1)^{2}h(\left\lceil t/K\right\rceil)}{(M-1)+\exp(Mh( \left\lceil t/K\right\rceil))}>0\] (85) \[\iota_{n} := (1+\sigma)[q_{1}\cdot(1+\theta_{n})(1-E^{\prime}_{nn})-q_{0}\sum_ {k\neq n,k\in[K]}(1+\theta_{k})E^{\prime}_{kn}]\] (86) \[= (1+\sigma)[(1-E^{\prime}_{nn})\cdot(1+\delta_{1})+\delta_{2}]\] (87) \[\beta_{nn^{\prime}} := (1+\sigma)[q_{1}\cdot(1+\theta_{n})E^{\prime}_{nn^{\prime}}+q_{0} ((1+\theta_{n^{\prime}})+\sum_{k\neq n,k\in[K]}(1+\theta_{k})E^{\prime}_{kn^{ \prime}}))]\] (88) \[= (1+\sigma)[E^{\prime}_{nn^{\prime}}\cdot(1+\delta_{1})+\delta_{3}]\] (89)Here \(\sigma\) is defined in Cor. 1 and satisfies \(-1<\sigma\ll\ln M\). \(|\delta_{1}|\lesssim\frac{K}{\sqrt{N}}\sqrt{\ln(\frac{1}{\delta})}+\frac{1}{M}\ll 1\) and \(|\delta_{2}|,|\delta_{3}|\leq\frac{M}{(M-1)^{2}}\times 2(1+3|\delta_{1}|)<\frac{3}{M}\). Here we use the fact that \(|\theta|,|\theta_{i}|\lesssim\frac{K}{\sqrt{N}}\sqrt{\ln(\frac{1}{\delta})}\), \(\sum_{k\in[K]}\lambda_{k}u_{jk}u_{jn^{\prime}}=E_{kn^{\prime}}\) and the fact from Lemma 15:

\[|E^{\prime}_{kn}|\leq\max_{i\in[K]}|\lambda_{i}(E^{\prime})|\leq\frac{1}{1-1/K} |\lambda_{1}|\leq\frac{1}{K-1}\] (90)

**(a)** Now let's prove that \(\xi_{n}(t)>0\). First from \((I+E)(I-E^{\prime})=I\) we have \(E-E^{\prime}-EE^{\prime}=O\). Then use the symmetry of \(E\) and \(E^{\prime}\), we get

\[(EE^{\prime})_{nn}=\sum_{k=1}E_{nk}E^{\prime}_{kn}=\sum_{k=1}E_{nk}E^{\prime} _{nk}=\sum_{k=1}E_{nk}E^{\prime}_{nk}=\sum_{k\neq n}E_{nk}E^{\prime}_{nk}+E_{ nn}E^{\prime}_{nn}\] (91)

Note that \(F^{\top}F=I+E\), we have \(E_{nn^{\prime}}=\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}},\forall n^{\prime}\neq n\) and \(E_{nn}=0\). Then

\[(E-E^{\prime}-EE^{\prime})_{nn}=O_{nn}=0\Rightarrow\sum_{k\neq n}E_{nk}E^{ \prime}_{nk}=-E^{\prime}_{nn}\] (92)

Note that \(|\lambda_{i}(E)|>0,\forall i\in[K]\) in Assumption 2 implies that \(E_{ki}>0\) holds for some \(k\neq i\in[K]\). Then from (1) of Lemma 16 we get \(\sum_{k\neq n}E^{\prime}_{nn^{\prime}}\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}}>0\).

From Theorem 1 we have \(\beta_{nn^{\prime}}=(1+\sigma)[E^{\prime}_{nn^{\prime}}\cdot(1+\delta_{1})+ \delta_{3}]\). Note that \(0<1+\sigma\ll\ln(M)\), we have:

\[\sum_{n^{\prime}\neq n}\beta_{nn^{\prime}}\bm{f}_{n}^{\top}\bm{f }_{n^{\prime}} =(1+\sigma)[\sum_{n^{\prime}\neq n}[E^{\prime}_{nn^{\prime}}(1+ \delta_{1})+\delta_{3}]E_{nn^{\prime}}]\] \[=(1+\sigma)[-(1+\delta_{1})E^{\prime}_{nn}+\delta_{3}\sum_{n^{ \prime}\neq n}E_{nn^{\prime}}]\] \[=(1+\sigma)[(1+\delta_{1})\sum_{k=1}^{K}\frac{\lambda_{k}^{2}}{1+ \lambda_{k}}u_{nk}^{2}+\delta_{3}\sum_{n^{\prime}\neq n}E_{nn^{\prime}}]\quad \text{(Eqn.~{}\ref{eq:1})}\] \[\geq(1+\sigma)[\frac{1+\delta_{1}}{1-|\lambda_{1}|}(\min_{i}| \lambda_{i}(E)|^{2})-\frac{3}{M}\cdot K|\lambda_{1}|],\quad\text{(Eqn.~{}\ref{eq:1}, $|\delta_{3}|<\frac{3}{M}$)}\] \[>(1+\sigma)[\frac{1}{2}(\min_{i}|\lambda_{i}(E)|^{2})-\frac{3}{M} \cdot K|\lambda_{1}|],\quad(|\delta_{1}|\ll 1,|\lambda_{1}|<\frac{1}{K}\ll 1)\] \[>0,\qquad\text{(Assumption~{}\ref{eq:1})}\] (93)

**(b)** We directly use Lemma 9, then we finish the proof. 

## Appendix E Proof of Section 5

**Lemma 4** (Self-attention dynamics).: _With Assumption I(b) (i.e., \(T\to+\infty\)), Eqn. 4 becomes:_

\[\dot{\bm{z}}_{m}=\eta_{Z}\gamma\sum_{n\in\psi^{-1}(m)}\mathrm{diag}(\bm{f}_{n })\sum_{n^{\prime}\neq n}\beta_{nn^{\prime}}(\bm{f}_{n}\bm{f}_{n}^{\top}-I) \bm{f}_{n^{\prime}},\] (10)

Proof.: Taking long sequence limit (\(T\to+\infty\)), and summing over all possible choices of next token \(x_{T+1}=n\), plugging in the backpropagated gradient (Eqn. 9) into the dynamics of \(Z\) with query token \(m\) (Eqn. 4), we arrive at the following:

\[\dot{\bm{z}}_{m} = \eta_{Z}\sum_{n\in\psi^{-1}(m)}\mathrm{diag}(\bm{c}_{n})\frac{P_ {\bm{f}_{n}}^{\perp}}{\|\bm{c}_{n}\|_{2}}Y(\bm{x}_{T+1}[i]-\bm{\alpha}[i])\] (94) \[= -\eta_{Z}\gamma\sum_{n\in\psi^{-1}(m)}\mathrm{diag}(\bm{f}_{n}) P_{\bm{f}_{n}}^{\perp}\sum_{n^{\prime}\neq n}\beta_{nn^{\prime}}\bm{f}_{n^{\prime}}\] (95) \[= \eta_{Z}\gamma\sum_{n\in\psi^{-1}(m)}\mathrm{diag}(\bm{f}_{n})( \bm{f}_{n}\bm{f}_{n}^{\top}-I)\sum_{n^{\prime}\neq n}\beta_{nn^{\prime}}\bm{f}_{n ^{\prime}}\] (96)Note here we leverage the property that \(P_{\bm{f}}^{\perp}\bm{f}=0\) and \(P_{e_{n}}^{\perp}=P_{\bm{f}_{n}}^{\perp}\). 

**Theorem 2** (Fates of contextual tokens).: _Let \(G_{CT}\) be the set of common tokens (CT), and \(G_{DT}(n)\) be the set of distinct tokens (DT) that belong to next token \(n\). Then if Assumption 2 holds, under the self-attention dynamics (Eqn. 10), we have:_

* _(_**a)** _for any distinct token_ \(l\in G_{DT}(n)\)_,_ \(\dot{z}_{ml}>0\) _where_ \(m=\psi(n)\)_;_
* _(_**b)** _if_ \(|G_{CT}|=1\) _and at least one next token_ \(n\in\psi^{-1}(m)\) _has at least one distinct token, then for the single common token_ \(l\in G_{CT}\)_,_ \(\dot{z}_{ml}<0\)_._

Proof.: For any token \(l\), we have:

\[\dot{z}_{ml}=\eta_{Z}\gamma\sum_{n\in\psi^{-1}(m)}f_{nl}\sum_{n^{\prime}\neq n }\beta_{nn^{\prime}}\left[(\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}})f_{nl}-f_{n^{ \prime}l}\right]\] (97)

**Distinct token**. For a token \(l\) distinct to \(n\), by definition, for any \(n^{\prime}\neq n\), \(\mathbb{P}(l|m,n^{\prime})=0\) and \(f_{n^{\prime}l}(t)\propto\mathbb{P}(l|m,n^{\prime})\exp(z_{ml})\equiv 0\). Therefore, we have:

\[\dot{z}_{ml}=\eta_{Z}\gamma f_{nl}^{2}\sum_{n^{\prime}\neq n}\beta_{nn^{\prime }}\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}}=\eta_{Z}f_{nl}^{2}\xi_{n}>0\] (98)

Note that \(\dot{z}_{ml}>0\) is achieved by \(\xi_{n}>0\) from Theorem 1.

**Common token**. For any query token \(m\), consider \(n\in\psi^{-1}(m)\) and \(n^{\prime}\neq n\). if \(n\) and \(n^{\prime}\) does not overlap then \(\mathrm{diag}(\bm{f}_{n})(\bm{f}_{n}\bm{f}_{n}^{\top}-I)\bm{f}_{n^{\prime}}=- \mathrm{diag}(\bm{f}_{n})\bm{f}_{n^{\prime}}=0\). When \(n\) and \(n^{\prime}\) overlaps, let \(G_{CT}(n,n^{\prime}):=\{l:\mathbb{P}(l|n)\mathbb{P}(l|n^{\prime})>0\}\) be the subset of common tokens shared between \(n\) and \(n^{\prime}\), since \(|G_{CT}|=1\) and \(\emptyset\neq G_{CT}(n,n^{\prime})\subseteq G_{CT}:=\bigcup_{n\neq n^{\prime}}G _{CT}(n,n^{\prime})\), we have \(|G_{CT}(n,n^{\prime})|=1\) and \(l\in G_{CT}(n,n^{\prime})\), i.e., the common token \(l\) is the unique overlap. Then we have:

\[f_{nl}\left[(\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}})f_{nl}-f_{n^{\prime}l} \right]=(\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}})f_{nl}^{2}-\bm{f}_{n}^{\top}\bm{ f}_{n^{\prime}}=-(1-f_{nl}^{2})(\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}})\] (99)

So we have:

\[\dot{z}_{ml}=-\eta_{Z}\gamma\sum_{n\in\psi^{-1}(m)}(1-f_{nl}^{2})\sum_{n^{ \prime}\neq n}\beta_{nn^{\prime}}\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}}=-\eta_{Z }\sum_{n\in\psi^{-1}(m)}(1-f_{nl}^{2})\xi_{n}\leq 0\] (100)

Since \(\xi_{n}(t)>0\), the only condition that will lead to \(\dot{z}_{ml}=0\) is \(f_{nl}^{2}=1\). However, since at least one such \(n\) has another distinct token \(l^{\prime}\), and thus \(f_{nl^{\prime}}>0\), by normalization condition, \(f_{nl}<1\) and thus \(\dot{z}_{ml}<0\).

Note that for multiple common tokens, things can be quite involved. Here we prove a case when the symmetric condition holds.

**Corollary 2** (Multiple CTs, symmetric case).: _If Assumption 2 holds and assume_

* _(_**1**_)_ Single query token_ \(m_{0}\)_. For any next token_ \(n\in[K]\)_,_ \(\psi(n)=m_{0}\)_._
* _(_**2**_)_ Symmetry_. For any two next tokens_ \(n\neq n^{\prime}\)_, there exists a one-to-one mapping_ \(\phi\) _that maps token_ \(l\in G_{DT}(n)\) _to_ \(l^{\prime}\in G_{DT}(n^{\prime})\) _so that_ \(\mathbb{P}(l|n)=\mathbb{P}(\phi(l)|n^{\prime})\)_;_
* _(_**3**_)_ Global common tokens with shared conditional probability: i.e., the global common token set_ \(G_{CT}\) _satisfies the following condition: for any_ \(l\in G_{CT}\)_,_ \(\mathbb{P}(l|n)=\rho_{l}\)_, which is independent of next token_ \(n\)_;_
* _(_**4**_) The initial condition_ \(Z(0)=0\)_._

_Then for any common token \(l\in G_{CT}\), \(\dot{z}_{m_{0},l}<0\)._

Proof.: Since there is a global query token \(m_{0}\), we omit the subscript \(m_{0}\) and let \(z_{l}:=z_{m_{0},l}\).

We want to prove the following _induction hypothesis_: for any \(t\) (a) \(z_{l}(t)=z_{\phi_{m}(l)}(t)\) for \(n\), \(n^{\prime}\) which are next tokens that the distinct token \(l\) (and \(l^{\prime}\)) belongs to, and (b) the normalization term \(o_{n}^{2}(t):=\sum_{l}\hat{c}_{l|n}^{2}(t)=o^{2}(t)\), i.e., it does not depend on \(n\).

We prove by induction on infinitesimal steps \(\delta t\). First when \(t=0\), both conditions hold due to the initial condition \(Z(0)=0\). Then we assume that both conditions hold for time \(t\), then by symmetry, we know that for any \(n_{1}\) and any distinct token \(l_{1}\in G_{DT}(n_{1})\):

\[\dot{z}_{l_{1}}(t)=\eta_{Z}\gamma f_{n_{1}l_{1}}^{2}\sum_{n^{\prime}\neq n_{1}} \beta_{n_{1}n^{\prime}}\bm{f}_{n_{1}}^{\top}\bm{f}_{n^{\prime}}=\eta_{Z}\gamma f _{n_{2}l_{2}}^{2}\sum_{n^{\prime}\neq n_{2}}\beta_{n_{2}n^{\prime}}\bm{f}_{n_{ 2}}^{\top}\bm{f}_{n^{\prime}}=\dot{z}_{l_{2}}(t)\] (101)

where \(l_{2}=\phi(l_{1})\) is the image of the distinct token \(l_{1}\). This is because (1) \(\bm{f}_{n_{1}}^{\top}\bm{f}_{n^{\prime}}=\sum_{l\in G_{CT}}\rho_{l}^{2}\exp(2z_ {l}(t))/o^{2}(t)\) is independent of \(n_{1}\) and \(n^{\prime}\) by inductive hypothesis, therefore, \(\beta\) is also independent of its subscripts. And (2) \(f_{n_{1}l_{1}}^{2}:=\tilde{c}_{l_{1}|n_{1}}^{2}/o^{2}(t)=\tilde{c}_{l_{2}|n_{ 2}}^{2}/o^{2}(t)=f_{n_{2}l_{2}}^{2}\).

Therefore, \(\dot{z}_{l_{1}}(t)=\dot{z}_{l_{2}}(t)\), which means that \(z_{l_{1}}(t^{\prime})=z_{l_{2}}(t^{\prime})\) for \(t^{\prime}=t+\delta t\).

Let \(G_{CT}(n_{1},n_{2}):=\{l:\mathbb{P}(l|n_{1})\mathbb{P}(l|n_{2})>0\}\) be the subset of common tokens shared between \(n_{1}\) and \(n_{2}\), then for their associated \(n_{1}\) and \(n_{2}\), obviously \(G_{CT}(n_{1},n_{2})\subseteq G_{CT}\) and we have:

\[o_{n_{1}}(t^{\prime}) = \sum_{l}\tilde{c}_{l|n_{1}}^{2}(t^{\prime})=\sum_{l}\mathbb{P}^{2 }(l|n_{1})\exp(2z_{l}(t^{\prime}))\] (102) \[= \sum_{l_{1}\in G_{DT}(n_{1})}\mathbb{P}^{2}(l_{1}|n_{1})\exp(2z_{ l_{1}}(t^{\prime}))+\sum_{l\in G_{CT}(n_{1},n_{2})}\mathbb{P}^{2}(l|n_{1}) \exp(2z_{l}(t^{\prime}))\] (103) \[= \sum_{l_{1}\in G_{DT}(n_{1})}\mathbb{P}^{2}(\phi(l_{1})|n_{2}) \exp(2z_{\phi(l_{1})}(t^{\prime}))+\sum_{l\in G_{CT}(n_{1},n_{2})}\rho_{l}^{2 }\exp(2z_{l}(t^{\prime}))\] (104) \[= \sum_{l_{2}\in G_{DT}(n_{2})}\mathbb{P}^{2}(l_{2}|n_{2})\exp(2z_{ l_{2}}(t^{\prime}))+\sum_{l\in G_{CT}(n_{1},n_{2})}\mathbb{P}^{2}(l|n_{2}) \exp(2z_{l}(t^{\prime}))\] (105) \[= o_{n_{2}}(t^{\prime})\] (106)

So we prove the induction hypothesis holds for \(t^{\prime}=t+\delta t\). Let \(\delta t\to 0\) and we prove it for all \(t\).

Now we check the dynamics of common token \(l\in G_{CT}\). First we have for any \(n\neq n^{\prime}\), \(f_{nl}^{2}(t)=\tilde{c}_{l|n}^{2}(t)/o^{2}(t)=\rho_{l}^{2}\exp(2z_{l}(t))/o^{2 }(t)=\tilde{c}_{l|n^{\prime}}^{2}(t)/o^{2}(t)=f_{n^{\prime}l}^{2}(t)\) and thus \(f_{nl}(t)=f_{n^{\prime}l}(t):=f_{l}(t)>0\), therefore:

\[f_{nl}\left[(\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}})f_{nl}-f_{n^{\prime}l}\right] =-f_{l}^{2}(1-\bm{f}_{n}^{\top}\bm{f}_{n^{\prime}})<0\] (107)

On the other hand, from the proof on induction hypothesis, we know all off-diagonal elements of \(E\) are the same and are positive. Then all all the off-diagonal elements of \(E^{\prime}\) are also the same and are positive. Following Theorem 1, we know \(\beta_{nn^{\prime}}>0\) and is independent of the subscripts. Therefore, \(\dot{z}_{l}<0\). 

**Theorem 3** (Growth of distinct tokens).: _For a next token \(n\) and its two distinct tokens \(l\) and \(l^{\prime}\), the dynamics of the **relative gain**\(r_{l/l^{\prime}|n}(t):=f_{nl}^{2}(t)/f_{nl^{\prime}}^{2}(t)-1=\tilde{c}_{l|n} ^{2}(t)/\tilde{c}_{l^{\prime}|n}^{2}(t)-1\) has the following analytic form (here the query token \(m=\psi(n)\) and is uniquely determined by distinct token \(l\)):_

\[r_{l/l^{\prime}|n}(t)=r_{l/l^{\prime}|n}(0)e^{2(z_{ml}(t)-z_{ml}(0))}=:r_{l/l^{ \prime}|n}(0)\chi_{l}(t)\] (11)

_where \(\chi_{l}(t):=e^{2(z_{ml}(t)-z_{ml}(0))}\) is the **growth factor** of distinct token \(l\). If there exist a dominant token \(l_{0}\) such that the initial condition satisfies \(r_{l_{0}/l|n}(0)>0\) for all its distinct token \(l\neq l_{0}\), and all of its common tokens \(l\) satisfy \(\dot{z}_{ml}<0\). Then both \(z_{ml_{0}}(t)\) and \(f_{nl_{0}}(t)\) are monotonously increasing over \(t\), and_

\[e^{2f_{nl_{0}}^{2}(0)B_{n}(t)}\leq\chi_{l_{0}}(t)\leq e^{2B_{n}(t)}\] (12)

_here \(B_{n}(t):=\eta_{Z}\int_{0}^{t}\xi_{n}(t^{\prime})\mathrm{d}t^{\prime}\). Intuitively, larger \(B_{n}\) gives larger \(r_{l_{0}/l|n}\) and sparser attention map._

Proof.: Let \(m=\psi(n)\) be the query token associated with next token \(n\). For brevity, we omit subscript \(m\) in the proof and let \(z_{l}:=z_{ml}\).

First of all, for tokens \(l\) and \(l^{\prime}\) that are both distinct for a specific next token \(n\), from Eqn. 98, it is clear that

\[\frac{\dot{z}_{l}}{\dot{z}_{l^{\prime}}}=r_{l/l^{\prime}|n}(t)+1=(r_{l/l^{\prime}| n}(0)+1)\frac{e^{2(z_{l}(t)-z_{l}(0))}}{e^{2(z_{l^{\prime}}(t)-z_{l^{\prime}}(0))}}\] (108)This means that

\[e^{-2(z_{l}-z_{l}(0))}\dot{z}_{l}=(r_{l/l^{\prime}|n}(0)+1)e^{-2(z_{l^{\prime}}-z_ {l^{\prime}}(0))}\dot{z}_{l^{\prime}}\] (109)

Integrate both side over time \(t\) and we get:

\[e^{-2(z_{l}(t)-z_{l}(0))}-1=(r_{l/l^{\prime}|n}(0)+1)\left[e^{-2(z_{l^{\prime}}( t)-z_{l^{\prime}}(0))}-1\right]\] (110)

From this we can get the close-form relationship between \(r_{l/l^{\prime}|n}(t)\) and \(z_{l}(t)\):

\[r_{l/l^{\prime}|n}(t)=r_{l/l^{\prime}|n}(0)e^{2(z_{l}(t)-z_{l}(0))}\] (111)

Now let \(l_{0}\) be the dominating distinct token that satisfies \(r_{l_{0}/l|n}(0)>0\) for any distinct token \(l\), then

* we have \(\dot{z}_{l_{0}}>0\) due to Theorem 2.
* for any token \(l^{\prime}\neq l_{0}\) that is distinct to \(n\), we have: \[\dot{r}_{l_{0}/l^{\prime}|n}=r_{l_{0}/l^{\prime}|n}(0)e^{2(z_{l_{0}}(t)-z_{l_ {0}}(0))}\dot{z}_{l_{0}}>0\] (112)
* for any common token \(l^{\prime}\), since \(\dot{z}_{l^{\prime}}<0\), we have \[\dot{r}_{l_{0}/l^{\prime}|n}=\frac{\mathrm{d}}{\mathrm{d}t}\left(\frac{\dot{c }_{nl_{0}}^{2}}{\dot{c}_{nl^{\prime}}^{2}}\right)=\frac{\mathbb{P}^{2}(l_{0} |n)}{\mathbb{P}^{2}(l^{\prime}|n)}e^{2(z_{l_{0}}-z_{l^{\prime}})}\cdot 2( \dot{z}_{l_{0}}-\dot{z}_{l^{\prime}})>0\] (113)

Therefore, we have:

\[\frac{\mathrm{d}}{\mathrm{d}t}(f_{nl_{0}}^{2})=\frac{\mathrm{d}}{\mathrm{d}t} \left(\frac{1}{M+\sum_{l^{\prime}\neq l_{0}}r_{l^{\prime}/l_{0}|n}}\right)>0\] (114)

Therefore, \(f_{nl_{0}}^{2}(t)\) is monotonously increasing. Combined with the fact \(f_{nl_{0}}^{2}(t)\leq 1\) due to normalization condition \(\|\bm{f}_{n}\|_{2}=1\), we have:

\[\xi_{n}(t)\geq\frac{1}{\eta_{Z}}\dot{z}_{l_{0}}=f_{nl_{0}}^{2}(t)\xi_{n}(t) \geq f_{nl_{0}}^{2}(0)\xi_{n}(t)\] (115)

Integrate over time and we have:

\[B(t)\geq\int_{0}^{t}\dot{z}_{l_{0}}(t^{\prime})\mathrm{d}t^{\prime}=z_{l_{0}} (t)-z_{l_{0}}(0)\geq f_{nl_{0}}^{2}(0)B(t)\] (116)

where \(B(t):=\eta_{Z}\int_{0}^{t}\xi_{n}(t^{\prime})\mathrm{d}t^{\prime}\). Plugging that into Eqn. 111, and we have:

\[e^{2f_{nl_{0}}^{2}(0)B(t)}\leq\chi_{l_{0}}(t)\leq e^{2B(t)}\] (117)

## Appendix F Estimation in Sec. 6

**Theorem 4** (Phase Transition in Training).: _If the dynamics of the single common token \(z_{ml}\) satisfies \(\dot{z}_{ml}=-C_{0}^{-1}\eta_{Z}\gamma(t)e^{4z_{ml}}\) and \(\xi_{n}(t)=C_{0}^{-1}\gamma(t)e^{4z_{ml}}\), then we have:_

\[B_{n}(t)=\left\{\begin{array}{cc}\frac{1}{4}\ln\left(C_{0}+\frac{2(M-1)^{2} }{KM^{2}}\eta_{Y}\eta_{Z}t^{2}\right)&\quad t<t_{0}^{\prime}:=\frac{K\ln M}{ \eta_{Y}}\\ \frac{1}{4}\ln\left(C_{0}+\frac{2K(M-1)^{2}}{M^{2}}\frac{\eta_{Z}}{\eta_{Y}} \ln^{2}(M\eta_{Y}t/K)\right)&\quad t\geq t_{0}:=\frac{2(1+o(1))K\ln M}{\eta_{ Y}}\end{array}\right.\] (14)

_As a result, there exists a phase transition during training:_

* _Attention scanning. At the beginning of the training,_ \(\gamma(t)=O(\eta_{Y}t/K)\) _and_ \(B_{n}(t)\approx\frac{1}{4}\ln K^{-1}(\rho_{0}^{4}+2\eta_{Y}\eta_{Z}t^{2})=O( \ln t)\)_. This means that the growth factor for dominant token_ \(l_{0}\) _is (sub-)linear:_ \(\chi_{l_{0}}(t)\geq e^{2f_{nl_{0}}^{2}(0)B_{n}(t)}\approx[K^{-1}(\rho_{0}^{4} +2\eta_{Y}\eta_{Z}t^{2})^{0.5f_{nl_{0}}^{2}(0)}\)_, and the attention on less co-occurred token drops gradually.__._
* _Attention snapping_. _When_ \(t\geq t_{0}:=2(1+\delta^{\prime})K\ln M/\eta_{Y}\) _with_ \(\delta^{\prime}=\Theta(\frac{\ln\ln M}{\ln M})\)_,_ \(\gamma(t)=O\left(\frac{K\ln(\eta_{Y}t/K)}{\eta_{Y}t}\right)\) _and_ \(B_{n}(t)=O(\ln\ln t)\)_. Therefore, while_ \(B_{n}(t)\) _still grows to infinite, the growth factor_ \(\chi_{l_{0}}(t)=O(\ln t)\) _grows at_ a much slower _logarithmic rate._

Proof.: Since every next token \(n\) shares the same query token \(m\), we omit the subscript \(m\) and let \(z_{l}:=z_{ml}\).

We start from the two following assumptions:

\[\dot{z}_{l} = -C_{0}^{-1}\eta_{Z}\gamma(t)\exp(4z_{l})\] (118) \[\xi_{n}(t) = C_{0}^{-1}\gamma(t)\exp(4z_{l})\] (119)

Given that, we can derive the dynamics of \(z_{l}(t)\) and \(\xi_{n}(t)\):

\[\exp(-4z_{l})\dot{z}_{l} = -C_{0}^{-1}\eta_{Z}\gamma(t)\] (120) \[\mathrm{d}\exp(-4z_{l}) = 4C_{0}^{-1}\eta_{Z}\gamma(t)\mathrm{d}t\] (121) \[\exp(-4z_{l}) = 4C_{0}^{-1}\eta_{Z}\int_{0}^{t}\gamma(t^{\prime})\mathrm{d}t^{ \prime}+1\qquad(\mathrm{use}\ z_{l}(0)=0)\] (122)

Let \(\Gamma(t):=\eta_{Z}\int_{0}^{t}\gamma(t^{\prime})\mathrm{d}t^{\prime}\), then \(\Gamma(0)=0\) and \(\mathrm{d}\Gamma(t)=\eta_{Z}\gamma(t)\mathrm{d}t\). Therefore, we have

\[\xi_{n}(t)=C_{0}^{-1}\gamma(t)\exp(4z_{l})=\frac{\gamma(t)}{C_{0}+4\Gamma(t)}\] (123)

and thus \(B_{n}(t):=\eta_{Z}\int_{0}^{t}\xi_{n}(t^{\prime})\mathrm{d}t^{\prime}\) can be integrated analytically, regardless of the specific form of \(\gamma(t)\):

\[B_{n}(t)=\eta_{Z}\int_{0}^{t}\frac{\gamma(t^{\prime})\mathrm{d}t^{\prime}}{C_{ 0}+4\Gamma(t)}=\int_{0}^{t}\frac{\mathrm{d}\Gamma}{C_{0}+4\Gamma}=\frac{1}{4} \ln(C_{0}+4\Gamma(t))\] (124)

Recall that \(\gamma(t)=\frac{(M-1)^{2}h(t/K)}{M-1+\exp(Mh(t/K))}\) (Theorem 1). Note that \(h\) (if treated in continuous time step) is strictly monotonically increasing and satisfies \(h(0)=0,\mathrm{d}h(t/K)=\eta_{Y}(M-1+\exp(Mh(t/K)))^{-1}\mathrm{d}t/K\) (Lemma 6 and Lemma 7), we can let \(\gamma(h):=\frac{(M-1)^{2}h}{M-1+\exp(Mh)}\) and get:

\[\Gamma(t) := \eta_{Z}\int_{t=0}^{t}\gamma(t^{\prime})\mathrm{d}t^{\prime}\] (125) \[= \eta_{Z}K\int_{h(0)}^{h(t/K)}\gamma(h^{\prime})\cdot\frac{M-1+ \exp(Mh^{\prime})}{\eta_{Y}}\cdot\mathrm{d}h^{\prime}\] (126) \[= \frac{\eta_{Z}}{\eta_{Y}}K(M-1)^{2}\int_{h(0)}^{h(t/K)}h^{\prime }\mathrm{d}h^{\prime}\] (127) \[= \frac{\eta_{Z}}{\eta_{Y}}\cdot\frac{K(M-1)^{2}}{2}h^{2}(t/K)\] (128)

Therefore, \(B_{n}(t)\) has a close form with respect to \(h\), regardless of the specific form of \(h(t)\):

\[B_{n}(t)=\frac{1}{4}\ln\left(C_{0}+2\frac{\eta_{Z}}{\eta_{Y}}K(M-1)^{2}h^{2}(t /K)\right)\] (129)

**(1)** When \(t<t_{0}^{\prime}:=K\ln(M)/\eta_{Y}\), from Lemma 9 we have \(h(t/K)=(1+o(1))\cdot\eta_{Y}t/(MK)\). We neglect the \(o(1)\) term and denote \(\nu:=\eta_{Y}/\eta_{Z}\), then we have when \(t\leq t_{0}^{\prime}\):

\[B_{n}(t)=\frac{1}{4}\ln\left(C_{0}+\frac{2(M-1)^{2}}{\nu KM^{2}}\eta_{Y}^{2}t ^{2}\right)\] (130)

And \(B_{n}(t_{0}^{\prime})=\frac{1}{4}\ln\left(C_{0}+2K(M-1)^{2}M^{-2}\nu^{-1}\ln^ {2}(M)\right)\).

**(2)** Similarly, when \(t>t_{0}:=2(1+\omega_{1})K\ln M/\eta_{Y}\) with \(\omega_{1}=\Theta(\ln\ln M/\ln M)\) is defined in Lemma 8, from Lemma 9 we have \(h(t/K)=(1+o(1))\ln(M\eta_{Y}t/K)/M\). We neglect the \(o(1)\) term and get when \(t>t_{0}\):

\[B_{n}(t)=\frac{1}{4}\ln\left(C_{0}+\frac{2K(M-1)^{2}}{\nu M^{2}}\ln^{2}(M\eta_ {Y}t/K)\right)\] (131)

From this we know \(B_{n}(t_{0})=\frac{1}{4}\ln(C_{0}+2K(M-1)^{2}M^{-2}\nu^{-1}\ln^{2}(2(1+\omega_ {1})M\ln M))\). It's interesting to find that \(B_{n}(t_{0})\) just depends on \(K,M\) and \(\nu\), and thus fixing \(\nu\) and changing \(\eta_{Z}\) will not influence the value of \(B_{n}(t_{0})\), which means that the main difference between \(B_{n}\) is arises at the stage \(t>t_{0}\). This matches the results in Fig. 9.

**(3)** Finally, we estimate \(B_{n}(t)\) when \(t\) is large. When \(\nu\) is fixed and \(t\gg(M\eta_{Y})^{-1}\exp(1/\sqrt{2\nu})\), we have

\[B_{n}(t) = (1+o(1))\cdot\left[\frac{1}{2}\ln\ln(M\eta_{Y}t/K)+\frac{1}{4} \ln(2K(M-1)^{2}M^{-2}\nu^{-1})\right]\] (132) \[= \Theta\left(\ln\ln(\frac{M\eta_{Z}\nu t}{K})-\ln(\frac{\nu}{K})\right)\] (133)

Therefore, from Eqn. 133 we get:

**(a)** Fix \(\nu\), larger \(\eta_{Z}\) result in larger \(B_{n}(t)\) and sparser attention map.

**(b)** Fix \(\eta_{Z}\), larger \(\nu\) (i.e., larger \(\eta_{Y}\)) result in smaller \(B_{n}(t)\) and denser attention map since \(\ln\ln(x)\) is much slower than \(\ln(x)\).

These match our experimental results in the main paper (Fig. 6). \(\Box\)

## Appendix G Experiments

We use WikiText [25] dataset to verify our theoretical findings. This includes two datasets, WikiText2 and WikiText103. We train both on 1-layer transformer with SGD optimizer. Instead of using reparameterization \(Y\) and \(Z\) (Sec. 3.1), we choose to keep the original parameterization with token embedding \(U\) and train with a unified learning rate \(\eta\) until convergence. Fig. 10 shows that the averaged entropy of the self-attention map evaluated in the validation set indeed drops with when the learning rate \(\eta\) becomes larger.

Note that in the original parameterization, it is not clear how to set \(\eta_{Y}\) and \(\eta_{Z}\) properly and we leave it for future work.

Furthermore, we use the recall-threshold relationship to reshow that smaller \(\eta_{Y}/\eta_{Z}\) and larger \(\eta_{Z}\) will result in a sparser self-attention map as Fig. 11 and Fig. 12. Here we use some thresholds to retag every entry of the attention as a 0-1 value based on its softmax value for every query token, and then calculate the recall value by the average value of the proportion of the distinct tokens with new labels

Figure 9: Numerical simulation of \(B_{n}(t)\) with changing \(\eta_{Z}\) and fixed \(\nu=\eta_{Z}/\eta_{Y}\). The dotted line denotes the transition time \(t_{0}\), and \(B_{n}(t_{0})\) marked with the solid dot is independent of \(\eta_{Z}\).

equal to \(1\) to the total number of distinct tokens. In the figures, the horizontal coordinates denote the threshold values around \(1/M\) for different last/next tokens setting, and the vertical coordinates denote the recall rate. The dataset is **Syn-Medium** mentioned in Section 8, and every data point in the figures is the mean value over 10 seeds. It's obvious that a sparser attention map will result in a slower descent rate of the recall-threshold line since sparser attention corresponds to fewer distinct tokens with higher attention weights, and the results of Fig. 11 and Fig. 12 match that of Fig. 6.

## Appendix H Technical Lemma

**Lemma 18**.: _Let \(\bm{h}=[h_{1},h_{2},\ldots,h_{M}]^{\top}\in\mathbb{R}^{M}\) is some \(M\)-dimensional vector; \(\bm{h}_{X}:=[h_{x_{1}},...,h_{x_{T-1}}]^{\top}\in\mathbb{R}^{T-1}\) is a vector selected by input sequence \(X\), then given event \(x_{T}=m,x_{T+1}=n\), there exists some \(\bm{q}_{m,n}=[q_{1|m,n},q_{2|m,n},\ldots,q_{M|m,n}]^{\top}\in\mathbb{R}^{M}\) so that \(\bm{q}\geq 0\) and_

\[\frac{1}{T-1}X^{\top}\bm{h}_{X} = \sum_{l=1}^{M}q_{l|m,n}h_{l}\bm{e}_{l}=\bm{q}_{m,n}\circ\bm{h}\] (134) \[\frac{1}{T-1}X^{\top}\mathrm{diag}(\bm{h}_{X})X = \sum_{l=1}^{M}q_{l|m,n}h_{l}\bm{e}_{l}\bm{e}_{l}^{\top}= \mathrm{diag}(\bm{q}_{m,n}\circ\bm{h})\] (135)

_where \(q_{l|m,n}\) satisfies \(\sum_{l=1}^{M}q_{l|m,n}=1\). And with probability at least \(1-\delta\) we have_

\[\max\left(0,\mathbb{P}(l|m,n)-\sqrt{\frac{\ln(2/\delta)}{2(T-1)}}\right)\leq q _{l|m,n}\leq\mathbb{P}(l|m,n)+\sqrt{\frac{\ln(2/\delta)}{2(T-1)}}\] (136)

_And thus \(q_{l|m,n}\to\mathbb{P}(l|m,n)\) when \(T\to+\infty\)._

Proof.: Given that \(x_{T}=m\) and \(x_{T+1}=n\), then we have

\[\frac{1}{T-1}X^{\top}\bm{h}_{X}=\frac{1}{T-1}\sum_{t=1}^{T-1}h_{x_{t}}\bm{x}_{ t}=\sum_{l=1}^{M}\left(\frac{1}{T-1}\sum_{t=1}^{T-1}\mathbb{I}[x_{t}=l] \right)h_{l}\bm{e}_{l}=:\sum_{l=1}^{M}q_{l|m,n}h_{l}\bm{e}_{l}\] (137)

And similar equations hold for \(\frac{1}{T-1}X^{\top}\mathrm{diag}(\bm{h}_{X})X\). Then we consider the case that the previous tokens are generated by conditional probability \(\mathbb{P}(l|m,n)\) as the data generation part, so \(\mathbb{I}[x_{t}=l],\forall t\in[T-1]\) are _i.i.d._ Bernoulli random variables with probability \(\mathbb{P}(l|m,n)\) and \(Tq_{l|m,n}\) satisfies binomial distribution. By Hoeffding inequality, we get

\[\mathbb{P}(|q_{l|m,n}-\mathbb{P}(l|m,n)|\geq t)\leq 2\exp(-2(T-1)t^{2})\] (138)

Then we get the results by direct calculation.

Figure 10: Average self-attention map entropy over the validation sets in 1-layer transformer after training, when the learning rate \(\eta_{Y}\) and \(\eta_{Z}\) changes. Note that higher learning rate \(\eta\) leads to higher \(B_{n}(t)\) and thus low entropy (i.e., more sparsity), which is consistent with our theoretical finding (Sec. 6). All the experiments are repeated in 5 random seeds. Error bar with \(1\)-std is shown in the figure.

Figure 11: Recall value of attention on all distinct tokens versus threshold with changing learning rate ratio \(\eta_{Y}/\eta_{Z}\). Smaller \(\eta_{Y}/\eta_{Z}\) corresponds to a smaller descent rate and thus sparser attention.

Figure 12: Recall value of attention on all distinct tokens versus threshold with changing learning rate \(\eta_{\mathcal{Z}}\). Larger \(\eta_{Z}\) corresponds to a smaller descent rate and thus sparser attention.