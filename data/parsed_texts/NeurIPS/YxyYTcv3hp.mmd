# Ferrari: Federated Feature Unlearning via

Optimizing Feature Sensitivity

Hanlin Gu

equal contribution; authors are listed alphabetically by first name. CISiPi, Universiti Malaya, Malaysia

Win Kent Ong

cise.s.chan@um.edu.my

Chee Seng Chan

cise.seng.chan@um.edu.my CISiPi, Universiti Malaya, Malaysia

Lixin Fan

AI Lab, Webank, PR China

###### Abstract

The advent of Federated Learning (FL) highlights the practical necessity for the _'right to be forgotten'_ for all clients, allowing them to request data deletion from the machine learning model's service provider. This necessity has spurred a growing demand for Federated Unlearning (FU). Feature unlearning has gained considerable attention due to its applications in unlearning sensitive, backdoor, and biased features. Existing methods employ the influence function to achieve feature unlearning, which is impractical for FL as it necessitates the participation of other clients, _if not all_, in the unlearning process. Furthermore, current research lacks an evaluation of the effectiveness of feature unlearning. To address these limitations, we define feature sensitivity in evaluating feature unlearning according to Lipschitz continuity. This metric characterizes the model output's rate of change or sensitivity to perturbations in the input feature. We then propose an effective federated feature unlearning framework called Ferrari, which minimizes feature sensitivity. Extensive experimental results and theoretical analysis demonstrate the effectiveness of Ferrari across various feature unlearning scenarios, including sensitive, backdoor, and biased features. The code is publicly available at https://github.com/OngWinKent/Federated-Feature-Unlearning

## 1 Introduction

Federated Learning (FL) [1, 2, 3] allows for model training across decentralized devices or servers holding local private data samples, without the need to exchange them directly. An essential requirement within FL is the participants _"right to be forgotten"_, as explicitly outlined in regulations such as the European Union General Data Protection Regulation (GDPR)3 and the California Consumer Privacy Act (CCPA)4[4]. To address this requirement, Federated Unlearning (FU) has been introduced, enabling clients to selectively remove the influence of specific subsets of their data from a trained FL model while preserving the model's accuracy on the remaining data [5].

Footnote 3: https://gdpr-info.eu/art-17-gdpr/

Footnote 4: https://oag.ca.gov/privacy/ccpa

Different from unlearning at the _client, class, or sample_ level [6, 7, 8] in FL, the feature unlearning [9] holds significant applications across various scenarios. Firstly, in contexts where sentences contain sensitive information such as names and addresses [9, 10], it becomes crucial to remove these sensitive components to prevent potential exposure through model inversion attacks [11, 12, 13, 14]. Secondly, when datasets contain backdoor triggers that can compromise model integrity [15, 16, 17, 18], it is imperative to eliminate these patterns. Thirdly, unlearning biased features becomes essential in scenarios where data imbalances significantly impact model accuracy due to bias [19, 20, 21, 22]. However,existing works of FU focus on client, class, or sample unlearning [6; 7; 8] but do not address feature unlearning, limiting their ability to unlearn specific features across multiple data points.

There are two challenges in feature unlearning in FL. Firstly, evaluating the unlearning effectiveness for feature unlearning is difficult. Typically, unlearning effectiveness is assessed by comparing the unlearned model with a retrained model without the feature. However, building data without the feature is challenging; for example, training the data with noise or a black block on the feature region may cause severe degradation in model accuracy (see Sec. 3.2). Secondly, previous work on feature unlearning within centralized machine learning settings [9; 10] is not practical for FL due to its requirement for access to all datasets, necessitating the participation of all clients.

To address the aforementioned limitations, we first define the feature sensitivity in Sec. 4.1 to evaluate the feature unlearning inspired by the Lipschitz continuity, which characterizes the rate of change or sensitivity of the model output to perturbations in the input feature. Then we propose a simple but effective federated feature unlearning method, called Ferrari (**F**ederated Feature **U**l**ear**ning), by minimizing the feature sensitivity in Sec. 4.2. Our Ferrari framework offers three key advantages: Firstly, Ferrari requires only local datasets from the unlearned clients for feature unlearning. Secondly, Ferrari demonstrates high practicality and efficiency, which support various feature unlearning scenarios, including sensitive, backdoor, and biased features and only consumes a few epochs of optimization. Thirdly, theoretical analysis in Sec. 4.3 elucidates that our proposed Ferrari achieves lower model utility loss compared to the exact feature unlearning.

The key contributions of this work are summarized as follows:

* We identify two key challenges for feature unlearning in FL. The first is how to successfully unlearn features without requiring the participation of other clients, as discussed in Sec. 3.2. The second is how to design an effective evaluation method in federated feature unlearning.
* We define the feature sensitivity and introduce this metric in federated feature unlearning in Sec. 4. By minimizing feature sensitivity, we propose an effective federated feature unlearning method, named Ferrari, which enables clients to selectively unlearn specific features from the trained global model without requiring the participation of other clients.
* We provide a theoretical proof in Theorem 1, which dictates that _Ferrari achieves better model performances than exact feature unlearning_. This analytical result is also echoed in the empirical evidence, highlighting Ferrari's effectiveness across various settings, including the unlearning of sensitive, backdoor, and biased features.

## 2 Related Work

Machine UnlearningMachine Unlearning (MU), introduced by Cao et al. [23], involves selectively removing specific training data from a trained model without retraining from scratch[24; 25]. It categorizes into exact unlearning [26; 27], aiming to completely remove data influence with techniques like SISA [28] and ARCANE [29], though with computational costs, and approximate unlearning [30; 31], which reduces data impact through techniques like data manipulation (fine-tuning with mislabeled data [32; 33; 34; 35; 36] or introducing noise [37; 38; 39]), knowledge distillation [40; 41; 42; 43] (training a student model), gradient ascent [44; 45; 46; 47] (maximizing loss associated with forgotten data), and weight scrubbing [48; 49; 50; 51; 52; 53] (discarding heavily influenced weights).

Federated UnlearningIn FL, traditional centralized MU methods are unsuitable due to inherent differences like incremental learning and limited dataset access [54]. Research on Federated Unlearning (FU) mainly focuses on client, class, and sample unlearning [6; 7; 8]. Client unlearning, pioneered by Liu et al. [55] introducing FedEraser [55], includes approaches like FRU [56], FedRecover [57], VeriFI [58], HDUS [59], KNOT [60], FedRecovery [61], Knowledge Distillation [54], and Gradient Ascent [62; 63; 64], aiming to remove specific clients or recover poisoned global models. Class unlearning, introduced by Wang et al. [65], involves frameworks like discriminative pruning and Momentum Degradation [66] (MoDE) to remove entire data classes. Sample unlearning, initiated by Liu et al. [67], targets individual sample removal within FL settings, with advancements like the QuickDrop [68] framework and FedFilter [69] enhancing efficiency and effectiveness. Recent works, such as \(FedMe^{2}\) by Xia et al. [70], optimize both unlearning facilitation and privacy guarantees.

Existing literature on FU primarily focuses on client, class, or sample unlearning [6; 7; 8]. However, a significant gap arises when a client seeks to remove only sensitive features while remaining engaged in FL. Unfortunately, current FU approaches do not address this specific scenario, as they do not explore feature unlearning within FL settings. In contrast to prior works focusing on feature unlearning in centralized settings of MU, such as classification models [9; 10], generative models [71; 72; 73; 74], and large language models [75; 76; 77], this study uniquely addresses feature unlearning of classification model within the FL paradigm. This distinction arises because traditional feature unlearning methods in centralized settings of MU are impractical for FL scenarios, where participation from all clients is often infeasible. In such cases, the process fails if even a single client opts out of the operation.

Therefore, to fill this critical gap, we proposed a novel federated feature unlearning framework, namely Ferrari based on the concept of Lipschitz continuity [78; 79; 80]. Our proposed Ferrari requires exclusively from the target client's dataset while still preserving the model's original performance. Lipschitz continuity, a fundamental mathematical concept that measures a function's sensitivity to changes in its input variables [81; 82; 83], is central to our feature unlearning approach. For a detailed exposition of our proposed federated feature unlearning framework utilizing Lipschitz continuity, please refer to Sec. 4. To the best of our knowledge, this is the **first work** in feature unlearning within FL settings that does not necessitate participation from all other clients, showcasing the potential to enhance privacy, practicality and efficiency.

## 3 Challenges on Feature Unlearning in FL

### Federated Feature Unlearning

Consider a federated system comprising \(K\) clients and one server, collaboratively learning a global model \(f_{\theta}\) as:

\[\min_{\theta}\sum_{k=1}^{K}\sum_{i=1}^{n_{k}}\frac{\ell(f_{\theta}(x_{k,i}),y_ {k,i})}{n_{1}+\cdots+n_{K}},\] (1)

where \(\ell\) is the loss, _e.g.,_ the cross-entropy loss, \(\mathcal{D}_{k}=\{(x_{k,i},y_{k,i})\}_{i=1}^{n_{k}}\) is the dataset with size \(n_{k}\) owned by client \(k\). One client (_i.e.,_ referred to as the unlearn client \(C_{u}\)) requests the removal of a feature \(\mathcal{T}\) from the global model \(\theta\) such that \(\theta\) does not retain any information about \(\mathcal{T}\). Specifically, we assume that the data \(x\in\mathbb{R}^{d}\) and denote the j-th feature of \(x\) by \(x[j]\). The partial element of the data \(x\) corresponding the feature \(\mathcal{T}\) is defined as \(x[\mathcal{T}]\), _i.e.,_:

\[x[\mathcal{T}]=\{x[j],j\in\mathcal{G}\}\] (2)

Therefore, the unlearn client \(C_{u}\) aims to remove \(\{x_{i,u}[\mathcal{T}]\}_{i=1}^{n_{u}}\), called unlearned data \(\mathcal{D}_{u}\). Denote \(\mathcal{D}_{r}=\mathcal{D}-\mathcal{D}_{u}\) to be the remaining data.

### Challenges for Feature Unlearning in FL

Unlike sample or class unlearning [6; 7; 8], evaluating the unlearning effectiveness for feature unlearning is difficult. Typically, unlearning effectiveness is assessed by comparing the unlearned model with a retrained model trained on remaining data \(\mathcal{D}_{r}\). However, building \(\mathcal{D}_{r}\) for the feature unlearning takes much work. For example, suppose we want to remove the mouth from a face image. In that case, one possible solution is to replace the mouth region with Gaussian noise or black block, as illustrated in Fig. 1. However, this added Gaussian noise or black block can adversely affect model training and degrade performance, _e.g.,_ the degradation of model accuracy is beyond 27%.

Another challenge is implementing feature unlearning for \(C_{u}\) without the help of other clients. Previous work on feature unlearning [9; 10] typically requires access to the remaining data, necessitating the participation of other clients in the FL process. This requirement is impractical in the FL context, as other clients may be unwilling or unable to share data or computational resources. Therefore, finding a method to effectively unlearn features without relying on other clients is crucial to maintain the model accuracy and practicality in the FL settings.

Figure 1: Sample data \(x\) with Gaussian noise (\(\overline{x}_{G}\)) and black pixels (\(\overline{x}_{B}\)) perturbations, illustrating feature removal and performance comparison.

## 4 The Proposed Method

In this section, we introduce feature sensitivity (see Def. 1) in Sec. 4.1 to evaluate the effectiveness of feature unlearning. We then propose Ferrari based on this concept in Sec. 4.2). Finally, we demonstrate that Ferrari achieves a lower utility loss compared to exact feature unlearning in Sec. 4.3).

### Feature Sensitivity

Inspired by Lipschitz Continuity [79; 80; 82], which provides an approximate method for removing information from images by perturbing the input data and observing the effect on the output, we introduce the concept of **feature sensitivity**\(s\) as Def. 1. This metric measures the memorization of a model \(f_{\theta}\) for the feature \(\mathcal{G}\) by considering the local changes in the given input rather than the global change as defined in the traditional Lipschitz continuity.

**Definition 1**.: _The feature sensitivity \(s\) of the model \(f\) with respect to the feature \(\mathcal{G}\) on the data \((x,y)\) is defined as:_

\[s=\mathbb{E}_{\delta_{\mathcal{G}}}\frac{\|f(x)-f(x+\delta_{\mathcal{G}})\|_{ 2}}{\|\delta_{\mathcal{G}}\|_{2}},\] (3)

_where \(\delta_{\mathcal{G}}\) denote the perturbation on feature \(\mathcal{G}\)._

Def. 1 characterizes the rate of change or sensitivity of the model output to perturbations in the input data. A small feature sensitivity \(s\) represents the model \(f\) doesn't memorize the feature \(\mathcal{G}\). This definition does not require building the remaining data, as it considers the expectation over the perturbation \(\delta_{\mathcal{G}}\). Specifically, it represents the average output change rate over any magnitude of the perturbation. Furthermore, we will provide the relationship between Def. 1 and exact feature unlearning in Sec. 4.3.

**Remark 1**.: _The perturbation \(\delta_{\mathcal{G}}\) can be chosen from various distributions, such as the Gaussian distribution, the uniform distribution, and so on._

### Ferrari

As discussed the feature sensitivity \(s\) in Sec. 4.1, the core idea of the proposed method Ferrari is to achieve the feature unlearning by minimizing the feature sensitivity. More specifically, it controls the change in the model's output relative to changes in the input within the feature region, _i.e.,_ the slope, to prevent the model from memorizing the feature as illustrated in Fig. 2.

Figure 2: Overview of our proposed Ferrari framework: Initiated by the feature unlearning request from the unlearn client \(C_{u}\), the server initializes the trained global model \(\theta\) to \(C_{u}\) for local feature unlearning. Upon completion, \(C_{u}\) uploads the unlearned model \(\theta^{u}\) to the server. Local feature unlearning minimizes the Lipschitz constant \(L\) between the original input and its perturbed feature subset, reducing feature sensitivity yet preserving the overall model performance.

One unlearning client \(C_{u}\) requests to unlearning the feature \(\mathcal{T}\). The proposed Ferrari aims to unlearn the global model \(\theta\) to \(\theta^{u}\). The proposed method can be divided into three steps (see details in Alg. 1). In order to compute the feature sensitivity, the perturbation \(\delta_{\mathcal{T}}\) in terms of the feature \(\mathcal{T}\) is **firstly** computed as the following (take the Gaussian distribution as an example):

\[\delta_{\mathcal{T}}[j]=\left\{\begin{aligned} &\sim N(0,\sigma^{2})& j\in \mathcal{T}\\ & 0&\text{Otherwise}\end{aligned}\right.\] (4)

**Secondly,** we leverage a finite sample Monte Carlo approximation to the maximization as Def. 1 as:

\[\mathbb{E}_{\delta_{\mathcal{T}}}\frac{\|f_{\theta}(x)-f_{\theta}(x+\delta_{ \mathcal{T}})\|_{2}}{\|\delta_{\mathcal{T}}\|_{2}}\sim\frac{1}{N}\sum_{i=1}^{ N}\frac{\|f_{\theta}(x)-f_{\theta}(x+\delta_{\mathcal{T},i})\|_{2}}{\| \delta_{\mathcal{T},i}\|_{2}},\] (5)

where \(\delta_{\mathcal{T},i}\) is \(i_{th}\) sampling as Eq. (4).

**Finally,** for the unlearning client \(C_{u}\) who aims to remove the feature \(\mathcal{T}\) from his/her data \(\mathcal{D}_{u}\), the unlearned model \(\theta^{u}\) is obtained as the following:

\[\theta^{u}=\operatorname*{arg\,min}_{\theta}\mathbb{E}_{(x,y)\in\mathcal{D}_{ u}}\frac{1}{N}\sum_{i=1}^{N}\frac{\|f_{\theta}(x)-f_{\theta}(x+\delta_{ \mathcal{T},i})\|_{2}}{\|\delta_{\mathcal{T},i}\|_{2}},\] (6)

where Eq. (6) is computed over the dataset \(\mathcal{D}_{u}\). Noted that the proposed Ferrari based on Def. 1 doesn't need the participation of other clients.

**Remark 2**.: _When the unlearning happens during the federated training, the unlearning clients would also optimize the training loss and feature sensitivity simultaneously, i.e., \(\mathbb{E}_{(x,y)\in\mathcal{D}}\big{(}\ell(f_{\theta}(x),y)+\lambda\mathbb{E }_{\delta_{\mathcal{T}}}\frac{\|f_{\theta}(x)-f_{\theta}(x+\delta_{\mathcal{T} })\|_{2}}{\|\delta_{\mathcal{T}}\|_{2}}\big{)},\) where \(\lambda\) is a coefficient._

### Theoretical Analysis of the Utility loss for Ferrari

As illustrated in Sec. 3.2, retraining the model without the feature may affect the model accuracy seriously. Suppose the feature is successfully removed when the norm of perturbation is larger than \(C\). We firstly define the utility loss \(\ell_{1}\) with unlearning feature directly, _i.e.,_ **the exact feature unlearning**:

\[\ell_{1}=\min_{\|\delta_{\mathcal{T}}\|\geq C}\mathbb{E}_{(x,y)\in\mathcal{D} }\min_{\theta}\ell\big{(}f_{\theta}(x+\delta_{\mathcal{T}}),y\big{)}\] (7)

And we define the maximum utility loss with the norm perturbation lower than \(C\) as:

\[\ell_{2}=\max_{\|\delta_{\mathcal{T}}\|\leq C}\mathbb{E}_{(x,y)\in\mathcal{D} }\min_{\theta}\ell\big{(}f_{\theta}(x+\delta_{\mathcal{T}}),y\big{)}\] (8)

**Assumption 1**.: _Assume \(\ell_{2}\leq\ell_{1}\)_

Assumption 1 elucidates that the utility loss associated with a perturbation norm lower than \(C\) is smaller than the utility loss when the perturbation norm is greater than \(C\). This assumption is logical, as larger perturbations would naturally lead to a greater utility loss.

**Assumption 2**.: _Suppose the federated model achieves zero training loss._

We have the following theorem to elucidate the relation between feature sensitivity removing via Alg. 1 and exact unlearning (see proof in Appendix A.1, including the extension for the non-zero training loss assumption).

**Theorem 1**.: _If Assumptions 1 and 2 hold, the utility loss of unlearned model obtained using Alg. 1 is lower than the utility loss with exact feature unlearning, i.e.,,_

\[\ell_{u}\leq\ell_{1},\] (9)

_where \(\ell_{u}=\mathbb{E}_{(x,y)\in\mathcal{D}}\ell(f_{\theta^{u}}(x),y)\)_

Theorem 1 showcases that the proposed method Ferrari, results in a utility loss (\(\ell_{u}\)) that is lower than the utility loss incurred when the feature is removed, and the model is retrained, _i.e.,_ the process of exact feature unlearning.

**Remark 3**.: _To further evaluate the effectiveness of feature unlearning based on feature sensitivity, we employ model inversion attacks [11; 12] to determine if the feature can be reconstructed and employ attention maps to assess if the model still focuses on the unlearned feature, as described in Sec. 5.3.1._

## 5 Experimental Results

This section presents the empirical analysis of the proposed Ferrari framework in terms of effectiveness, utility, and time efficiency in sensitive, backdoor and biased feature unlearning scenarios.

### Experimental Setup

Unlearning Scenarios_Sensitive Feature Unlearning_: We simulate the removal of sensitive features from the \(\mathcal{D}_{u}\) to fulfill the request of \(C_{u}\) due to privacy concern. Specifically, we remove'mouth' from CelebA [84],'marital status' from Adult [85], and 'pregnancies number' from Diabetes [86]. Therefore, our proposed Ferrari aims to remove the influence of these requested features.

_Backdoor Feature Unlearning_: We simulate a pixel-pattern backdoor attack by \(C_{u}\) based on BadNets [18] within a FL framework [15, 16, 17]. \(C_{u}\) injects a pixel-pattern backdoor feature and trigger label into its \(\mathcal{D}_{u}\) during training, as shown in Fig. 4. Consequently, our proposed Ferrari aims to remove the influence of these backdoor features and restore the model's original performance.

_Biased Feature Unlearning_: We simulate the bias dataset \(\mathcal{D}_{u}\) of the \(C_{u}\) and the unbias dataset \(\mathcal{D}_{r}\) with a bias ratio of 0.8, as shown in Fig. 4. This results in a global model biased towards the biased dataset [87, 88] due to unintended feature memorization [22]. In CMNIST [89], the model focuses on color patterns instead of digits, and in CelebA [84], it learns mouth features instead of facial features for gender classification. Therefore, our proposed Ferrari aims to mitigate these bias-inducing features and restore model performance.

Hyperparameters & Datasets & ModelWe simulate HFL with \(K=10\) clients under an IID setting, each holding \(10\%\) of the datasets, except for the biased feature unlearning experiment with a bias ratio of 0.8. For federated feature unlearning experiments, we set hyperparameters: learning rate \(\eta=0.0001\), sample size \(N=20\), and random Gaussian noise with standard deviation ranging from \(0.05\leq\sigma\leq 1.0\) (see Sec. 5.5) across iterations of \(N\). Experiments are repeated over five random trials, and results are reported as mean and standard deviation. We employ ResNet18 [90] on image datasets: MNIST [89], Colored-MNIST (CMNIST) [89], Fashion-MNIST [91], CIFAR-10, CIFAR-20, CIFAR-100 [92] and ImageNet [93]. For tabular datasets, such as Adult Census Income (Adult) [85] and Diabetes [86], we used a fully-connected neural network linear model. Additionally, we utilize the transformer-based BERT model [94] for the text dataset, specifically the IMDB movie reviews dataset [95]. We conduct experiments on a single NVIDIA A100 GPU. Further details are in Appendix A.2.

Evaluation MetricsWe assess effectiveness by measuring feature sensitivity (see Section 4.1) and conducting a model inversion attack (MIA) [11, 12, 13, 14] to determine the attack success rate (ASR). The goal is to achieve low feature sensitivity and ASR, indicating successful unlearning sensitive features. Backdoor and biased feature unlearning are evaluated by comparing accuracy on the retain dataset \(\mathcal{D}_{r}\) (\(Acc_{r}\)) and the unlearn client dataset \(\mathcal{D}_{u}(Acc_{u})\). Low \(Acc_{u}\) indicates high effectiveness for backdoor unlearning, while similar accuracy (\(Acc_{r}\approx Acc_{u}\)) reflects fairness and effectiveness in biased feature unlearning. Qualitatively, effectiveness is assessed using MIA-reconstructed images (sensitive) and GradCAM [96] attention maps (backdoor and biased). The utility is measured by test dataset \(\mathcal{D}_{t}\) accuracy (\(Acc_{t}\)), with higher values indicating stronger utility. Time efficiency is evaluated by comparing the runtime of each baseline.

[MISSING_PAGE_FAIL:7]

#### 5.3.2 Backdoor Feature Unlearning

Accuracy\(\mathcal{D}_{r}\) and \(\mathcal{D}_{u}\) represent the clean and backdoor datasets, respectively. Successful unlearning is shown by low \(Acc_{u}\) and high \(Acc_{r}\), indicating effective unlearning and preserved model utility. As shown in Tab. 4, the Fine-tune method has higher \(Acc_{r}\) and utility than the Retrain method but lower unlearning effectiveness due to high \(Acc_{u}\). FedCDP [65] and FedRecovery [61] show low utility and unlearning effectiveness with low \(Acc_{r}\) and \(Acc_{u}\), rendering them unsuitable for backdoor feature unlearning. In contrast, Ferrari demonstrates the highest utility and unlearning effectiveness.

Attention MapFig. 5(a) illustrates attention maps analyzing backdoor feature unlearning. Initially, the Baseline model focuses on the \(5\times 5\) square at the top-left corner, indicating a significant influence on output prediction by the pixel-pattern backdoor feature. In contrast, Ferrari unlearned models shift the attention towards recognizable objects like digits and cars, similar to the Retrain model. This shift suggests a reduced sensitivity to the backdoor feature, indicating a successful unlearning. See Appendix A.3.1 for supplementary results.

#### 5.3.3 Biased Feature Unlearning

Accuracy\(\mathcal{D}_{r}\) and \(\mathcal{D}_{u}\) represent the unbias and bias datasets, respectively. Successful unlearning results in similar accuracies across both datasets (\(Acc_{r}\approx Acc_{u}\)), ensuring fairness while maintaining high \(Acc_{r}\) and \(Acc_{u}\) for utility. Tab. 4 shows that the Fine-tune method fails to unlearn bias, as \(Acc_{u}\) remains higher than \(Acc_{r}\), despite slightly higher \(Acc_{r}\) compared to Retrain. FedCDP [65] and FedRecovery [61] exhibit catastrophic forgetting, with low \(Acc_{r}\) and \(Acc_{u}\), making them unsuitable for biased feature unlearning. In contrast, Ferrari demonstrates effective unlearning with similar \(Acc_{r}\) and \(Acc_{u}\), and maintains high overall accuracy, indicating a successful biased feature unlearning.

Attention MapFig. 5(b) shows attention maps analyzing biased feature unlearning. The Baseline model predominantly focuses on the biased feature region (mouth) in both bias and unbias datasets, suggesting its significant impact on output prediction. However, Ferrari unlearned models redistribute attention across various facial regions in both datasets, similar to the Retrain model. This shift indicates reduced sensitivity to the biased feature, demonstrating successful unlearning. See Appendix A.3.2 for supplementary results.

### Computational Complexity

In Fig. 7, we evaluate the runtime performance and FLOPs metrics of each unlearning method to demonstrate the computational complexity. The Retrain method is expected to have the slowest runtime and highest FLOPs, while Fine-tune is fast but still slower than other methods.

Both FedCDP [65] and FedRecovery [61] demonstrate faster runtimes and lower FLOPs than the Fine-tune method, but they are still more computationally expensive than Ferrari. This is primarily due to the need to access training datasets from all clients and the computational expense of gradient residual calculations [61].

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \hline \multirow{2}{*}{**Scenario**} & \multirow{2}{*}{**Datasets**} & \multicolumn{2}{c|}{**Cleaner**} & \multicolumn{4}{c|}{**Attack Success Rate(ASR) (\%)**} \\ \cline{3-10}  & & **Feature** & **Baseline** & **Retrain** & **Frine-tune** & **FedCDP [65]** & **FedRecovery [61]** & **Ferrari (Ours)** \\ \hline \multirow{3}{*}{**Sensitive**} & **CelebA** & Mouth & 84.36 \(\pm\)3.22 & 47.52 \(\pm\)1.04 & 77.43 \(\pm\)10.98 & 75.36 \(\pm\)9.31 & 71.52 \(\pm\)6.07 & **51.28 \(\pm\)2.41** \\  & **Adult** & Marriage & 87.54 \(\pm\)1.39 & 49.28 \(\pm\)2.13 & 83.45 \(\pm\)8.44 & 72.83 \(\pm\)5.18 & 80.39 \(\pm\)10.68 & **49.85 \(\pm\)1.38** \\  & **Dabetes** & Pregnances & 92.31 \(\pm\)7.55 & 38.89 \(\pm\)2.52 & 88.46 \(\pm\)5.01 & 81.91 \(\pm\)1.87 & 78.27 \(\pm\)2.47 & **42.61 \(\pm\)1.81** \\  & **IMDB** & Names & 90.28 \(\pm\)2.49 & 40.29 \(\pm\)1.59 & 86.74 \(\pm\)3.81 & 83.67 \(\pm\)4.59 & 80.95 \(\pm\)3.51 & **43.75 \(\pm\)1.86** \\ \hline \end{tabular}
\end{table}
Table 3: The ASR of MIA for each unlearning method across sensitive feature unlearning scenario.

Figure 5: MIA reconstruction on CelebA (unlearned mouth)In contrast, Ferrari has the lowest computational complexity, with the fastest runtime and lowest FLOPs. It only requires access to the local dataset of the unlearn client and achieves feature unlearning by minimizing feature sensitivity within a single epoch.

### Ablation Study and Hyper-parameter Analysis

We conduct an ablation study to analyze how Non-Lipschitz affects the effectiveness of our proposed Ferrari and hyper-parameter analysis of Gaussian noise level (\(\sigma\)) and number of \(\mathcal{D}_{u}\) in Fig. 8.

Non-LipschitzWe evaluate the unlearning performance by removing the denominator in Eq. 6, calling this the Non-Lipschitz method, as shown in Fig. 7(a). The results indicate catastrophic forgetting: \(\mathcal{D}_{r}\) accuracy drops below 10%, and the unlearned model misclassifies all inputs into a single random class, rendering it useless. This stems from the unbounded loss function in the non-Lipschitz method, unlike the bounded Lipschitz constant in Eq. 6, which provides a theoretical guarantee (see Sec. 4.3). Refer to Appendix A.4 for a detailed analysis of Lipschitz and Non-Lipschitz loss functions.

Gaussian NoiseThe effectiveness of Ferrari is significantly influenced by injected Gaussian noise. Fig. 7(b) shows the accuracy of \(\mathcal{D}_{r}\) and \(\mathcal{D}_{u}\) across different \(\sigma\) levels. In the \(0.05\leq\sigma\leq 1.0\) range, \(\mathcal{D}_{r}\) accuracy stays high and \(\mathcal{D}_{u}\) accuracy remains low, indicating a balance. Thus, we implement \(\sigma\) values between 0.05 and 1.0 for a balanced accuracy across \(\mathcal{D}_{r}\) and \(\mathcal{D}_{u}\).

Number of Unlearn DatasetOur analysis illustrated in Fig. 7(c), demonstrates that Ferrari remains effective with partial \(\mathcal{D}_{u}\) from \(C_{u}\) for feature unlearning (_i.e.,_ data lost). Using 70% of \(\mathcal{D}_{u}\) yields comparable accuracy to using the full (_i.e.,_ 100%) dataset, highlighting the method's flexibility even with partial data.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c} \hline \multirow{2}{*}{**Scenarios**} & \multirow{2}{*}{**Datasets**} & \multirow{2}{*}{**Unlearn Feature**} & \multicolumn{2}{c|}{**Baseline**} & \multicolumn{2}{c|}{**Retrain**} & \multicolumn{2}{c|}{**Fine-tune**} & \multicolumn{2}{c|}{**FedCP(\#S)**} & \multicolumn{2}{c|}{**FedRecovery(\#I)**} & \multicolumn{2}{c}{**Ferrari(Ours)**} \\ \hline \multirow{6}{*}{**Backdoor**} & **MNIST** & & \(\mathcal{D}_{r}\) & 95.65 \(\pm\) 1.39 & 97.19 \(\pm\) 2.49 & **96.16 \(\pm\) 0.37** & 65.82 \(\pm\) 6.85 & 40.81 \(\pm\) 4.31 & 95.93 \(\pm\) 0.45 \\  & & \(\mathcal{D}_{r}\) & 97.43 \(\pm\) 9.69 & 0.00 \(\pm\) 0.00 & 72.64 \(\pm\) 0.29 & 93.03 \(\pm\) 0.53 & 53.27 \(\pm\) 3.14 & **0.11 \(\pm\) 0.01** \\  & & \(\mathcal{D}_{r}\) & 91.07 \(\pm\) 0.54 & 95.83 \(\pm\) 1.08 & **94.36 \(\pm\) 1.98** & 68.46 \(\pm\) 3.59 & 42.93 \(\pm\) 25.20 & 92.83 \(\pm\) 0.61 \\  & & \(\mathcal{D}_{r}\) & 94.51 \(\pm\) 6.29 & 0.00 \(\pm\) 0.00 & 43.91 \(\pm\) 0.28 & 17.29 \(\pm\) 0.49 & 48.15 \(\pm\) 4.37 & **0.90 \(\pm\) 0.03** \\ \cline{2-10}  & **CIFAR-10** & Backdoor & & & & & & & & \\  & pixel- & & & & & & & & \\  & & & & & & & & & \\  & & & & & & & & & \\  & & & & & & & & & & \\  & & & & & & & & & & \\  & & & & & & & & & & \\
**Biased** & & & & & & & & & & \\ \hline \end{tabular}
\end{table}
Table 4: The accuracy of \(\mathcal{D}_{r}\) and \(\mathcal{D}_{u}\) for each unlearning method across different unlearning scenarios.

Figure 6: The attention map of each unlearning method across different unlearning scenarios.

## 6 Conclusion

This paper introduces Ferrari, a federated feature unlearning framework designed to efficiently remove sensitive, backdoor, and biased features without extensive retraining. Leveraging Lipschitz continuity, Ferrari reduces model sensitivity to specific features, ensuring robust and fair models. Uniquely, it requires participation only from the client requesting unlearning, preserving privacy and practicality in FL environments. Experimental results and theoretical analysis demonstrate Ferrari's effectiveness across various data domains, addressing the crucial need for feature-level unlearning in federated learning. This method can serve as a technical solution to meet regulatory requirements for data deletion while maintaining model performance, offering significant value to clients by securing their "right to be forgotten" and preventing potential privacy leakage.

### Limitation and Future Work

The proposed federated feature unlearning method works effectively using only the unlearning client's local data, making it well-suited for real-world scenarios. However, for optimal results, access to the full dataset is required. As demonstrated in Section 5.5, using 70% of the data yields comparable performance, but significant data reduction diminishes effectiveness. Future research should focus on developing methods that require only a small portion of the client's data and expanding the approach beyond classification models to include for example, generative models. Additionally, enhancements such as advanced perturbation techniques and integration with privacy-preserving methods should be explored.

## Acknowledgement

This research is supported by the Fundamental Research Grant Scheme (FRGS/1/2024/ICT02/UM/01/1), awarded by the Ministry of Higher Education, Malaysia.

Figure 8: Ablation and hyper-parameter analysis on Ferrari backdoor feature unlearning. Solid line: \(\mathcal{D}_{r}\); dashed line: \(\mathcal{D}_{u}\).

Figure 7: Computational complexity analysis comparing the runtime(s) and FLOPs for each unlearning method.

## References

* [1]J. Konecny, B. McMahan, and D. Ramage (2015) Federated optimization: distributed optimization beyond the datacenter. arXiv preprint arXiv:1511.03575. Cited by: SS1.
* [2]B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas (2017) Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pp. 1273-1282. Cited by: SS1.
* [3]J. Konecny, B. McMahan, and D. Ramage (2015) Federated optimization: distributed optimization beyond the datacenter. arXiv preprint arXiv:1511.03575. Cited by: SS1.
* [4]J. Konecny, B. McMahan, and D. Ramage (2015) Federated machine learning: concept and applications. ACM Transactions on Intelligent Systems and Technology (TIST)10 (2), pp. 1-19. Cited by: SS1.

[MISSING_PAGE_POST]

. L. Harding, J. J. Vanto, R. Clark, L. Hannah Ji, and S. C. Ainsworth (2019) Understanding the scope and impact of the california consumer privacy act of 2018. Journal of Data Protection & Privacy2 (3), pp. 234-253. Cited by: SS1.
* [55]T. Che, Y. Zhou, Z. Zhang, L. Lyu, J. Liu, D. Yan, D. Dou, and J. Huan (2023) Fast federated machine unlearning with nonlinear functional theory. In International conference on machine learning, pp. 4241-4268. Cited by: SS1.
* [56]Z. Liu, Y. Jiang, J. Shen, M. Peng, K. Lam, X. Yuan, and X. Liu (2024) A survey on federated unlearning: challenges, methods, and future directions. Cited by: SS1.
* [57]N. Romandini, A. Mora, C. Mazzocca, R. Montanari, and P. Bellav* [19] D. Pessach and E. Shmueli, "A review on fairness in machine learning," _ACM Comput. Surv._, vol. 55, feb 2022.
* [20] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, "A survey on bias and fairness in machine learning," _ACM Comput. Surv._, vol. 54, jul 2021.
* [21] S. Sagawa*, P. W. Koh*, T. B. Hashimoto, and P. Liang, "Distributionally robust neural networks," in _International Conference on Learning Representations_, 2020.
* [22] S. Seo, J.-Y. Lee, and B. Han, "Unsupervised learning of debiased representations with pseudo-attributes," in _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 16721-16730, 2022.
* [23] Y. Cao and J. Yang, "Towards making systems forget with machine unlearning," in _2015 IEEE symposium on security and privacy_, pp. 463-480, IEEE, 2015.
* [24] M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang, "When machine unlearning jeopardizes privacy," in _Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security_, CCS '21, (New York, NY, USA), p. 896-911, Association for Computing Machinery, 2021.
* [25] S. Garg, S. Goldwasser, and P. N. Vasudevan, "Formalizing data deletion in the context of the right to be forgotten," in _39th Annual International Conference on the Theory and Applications of Cryptographic Techniques, Zagreb, Croatia, May 10-14, 2020, Proceedings_, vol. 12105 of _Lecture Notes in Computer Science_, Springer, 2020.
* [26] A. Thudi, H. Jia, I. Shumailov, and N. Papernot, "On the necessity of auditable algorithmic definitions for machine unlearning," in _USENIX Security Symposium_, 2021.
* [27] T. T. Nguyen, T. T. Huynh, P. L. Nguyen, A. W.-C. Liew, H. Yin, and Q. V. H. Nguyen, "A survey of machine unlearning," 2022.
* [28] L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot, "Machine unlearning," in _2021 IEEE Symposium on Security and Privacy (SP)_, pp. 141-159, 2021.
* [29] H. Yan, X. Li, Z. Guo, H. Li, F. Li, and X. Lin, "Arcane: An efficient architecture for exact machine unlearning," in _Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22_ (L. D. Raedt, ed.), pp. 4006-4013, International Joint Conferences on Artificial Intelligence Organization, 7 2022. Main Track.
* [30] J. Xu, Z. Wu, C. Wang, and X. Jia, "Machine unlearning: Solutions and challenges," _ArXiv_, vol. abs/2308.07061, 2023.
* [31] H. Xu, T. Zhu, L. Zhang, W. Zhou, and P. S. Yu, "Machine unlearning: A survey," _ACM Comput. Surv._, vol. 56, aug 2023.
* [32] L. Graves, V. Nagisetty, and V. Ganesh, "Amnesiac machine learning," in _AAAI Conference on Artificial Intelligence_, 2020.
* [33] J. Kim and S. S. Woo, "Efficient two-stage model retraining for machine unlearning," in _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)_, pp. 4360-4368, 2022.
* [34] S. Lee and S. S. Woo, "Undo: Effective and accurate unlearning method for deep neural networks," in _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_, pp. 4043-4047, 2023.
* [35] M. Chen, W. Gao, G. Liu, K. Peng, and C. Wang, "Boundary unlearning: Rapid forgetting of deep networks via shifting the decision boundary," in _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 7766-7775, 2023.
* [36] T. Shibata, G. Irie, D. Ikami, and Y. Mitsuzumi, "Learning with selective forgetting," in _International Joint Conference on Artificial Intelligence_, 2021.

* [37] H. Huang, X. Ma, S. M. Erfani, J. Bailey, and Y. Wang, "Unlearnable examples: Making personal data unexplitable," in _International Conference on Learning Representations_, 2021.
* [38] J. Z. Di, J. Douglas, J. Acharya, G. Kamath, and A. Sekhari, "Hidden poison: Machine unlearning enables camouflaged poisoning attacks," in _Workshop on Trustworthy and Socially Responsible Machine Learning, NeurIPS 2022_, 2022.
* [39] A. K. Tarun, V. S. Chundawat, M. Mandal, and M. Kankanhalli, "Fast yet effective machine unlearning," _IEEE Transactions on Neural Networks and Learning Systems_, pp. 1-10, 2023.
* [40] V. S. Chundawat, A. K. Tarun, M. Mandal, and M. Kankanhalli, "Can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher," _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 37, pp. 7210-7217, Jun. 2023.
* [41] X. Zhang, J. Wang, N. Cheng, Y. Sun, C. Zhang, and J. Xiao, "Machine unlearning methodology based on stochastic teacher network," in _Advanced Data Mining and Applications_ (X. Yang, H. Suhartanto, G. Wang, B. Wang, J. Jiang, B. Li, H. Zhu, and N. Cui, eds.), (Cham), pp. 250-261, Springer Nature Switzerland, 2023.
* [42] M. Kurmanji, P. Triantafillou, J. Hayes, and E. Triantafillou, "Towards unbounded machine unlearning," in _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [43] Y. Jung, I. Cho, S.-H. Hsu, and J. Hockenmaier, "Attack and reset for unlearning: Exploiting adversarial noise toward machine unlearning through parameter re-initialization," 2024.
* [44] T. Hoang, S. Rana, S. Gupta, and S. Venkatesh, "Learn to unlearn for deep neural networks: Minimizing unlearning interference with gradient projection," in _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_, pp. 4819-4828, January 2024.
* [45] A. Abbasi, C. Thrash, E. Akbari, D. Zhang, and S. Kolouri, "Covarnav: Machine unlearning via model inversion and covariance navigation," 2023.
* [46] D. Choi and D. Na, "Towards machine unlearning benchmarks: Forgetting the personal identities in facial recognition systems," 2023.
* [47] S. Goel, A. Prabhu, A. Sanyal, S.-N. Lim, P. Torr, and P. Kumaraguru, "Towards adversarial evaluations for inexact machine unlearning," 2023.
* [48] A. Golatkar, A. Achille, and S. Soatto, "Eternal sunshine of the spotless net: Selective forgetting in deep networks," in _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 9301-9309, 2020.
* ECCV 2020_ (A. Vedaldi, H. Bischof, T. Brox, and J.-M. Frahm, eds.), (Cham), pp. 383-398, Springer International Publishing, 2020.
* [50] A. Golatkar, A. Achille, A. Ravichandran, M. Polito, and S. Soatto, "Mixed-privacy forgetting in deep networks," in _2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 792-801, 2021.
* [51] C. Guo, T. Goldstein, A. Hannun, and L. Van Der Maaten, "Certified data removal from machine learning models," in _Proceedings of the 37th International Conference on Machine Learning_ (H. D. III and A. Singh, eds.), vol. 119 of _Proceedings of Machine Learning Research_, pp. 3832-3842, PMLR, 13-18 Jul 2020.
* [52] J. Foster, S. Schoepf, and A. Brintrup, "Fast machine unlearning without retraining through selective synaptic dampening," 2023.
* [53] C. Guo, T. Goldstein, A. Hannun, and L. Van Der Maaten, "Certified data removal from machine learning models," in _Proceedings of the 37th International Conference on Machine Learning_ (H. D. III and A. Singh, eds.), vol. 119 of _Proceedings of Machine Learning Research_, pp. 3832-3842, PMLR, 13-18 Jul 2020.

* [54] C. Wu, S. Zhu, and P. Mitra, "Federated unlearning with knowledge distillation," 2022.
* [55] G. Liu, X. Ma, Y. Yang, C. Wang, and J. Liu, "Federaser: Enabling efficient client-level data removal from federated learning models," in _2021 IEEE/ACM 29th International Symposium on Quality of Service (IWQOS)_, pp. 1-10, 2021.
* [56] W. Yuan, H. Yin, F. Wu, S. Zhang, T. He, and H. Wang, "Federated unlearning for on-device recommendation," in _Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining_, WSDM '23, (New York, NY, USA), p. 393-401, Association for Computing Machinery, 2023.
* [57] X. Cao, J. Jia, Z. Zhang, and N. Z. Gong, "Fedrecover: Recovering from poisoning attacks in federated learning using historical information," in _2023 IEEE Symposium on Security and Privacy (SP)_, pp. 1366-1383, 2023.
* [58] X. Gao, X. Ma, J. Wang, Y. Sun, B. Li, S. Ji, P. Cheng, and J. Chen, "Verifi: Towards verifiable federated unlearning," 2022.
* [59] G. Ye, T. Chen, Q. V. Hung Nguyen, and H. Yin, "Heterogeneous decentralised machine unlearning with seed model distillation," _CAAI Transactions on Intelligence Technology_, 2024.
* IEEE Conference on Computer Communications_, pp. 1-10, 2023.
* [61] L. Zhang, T. Zhu, H. Zhang, P. Xiong, and W. Zhou, "Fedrecovery: Differentially private machine unlearning for federated learning frameworks," _IEEE Transactions on Information Forensics and Security_, vol. 18, pp. 4732-4746, 2023.
* [62] A. Halimi, S. Kadhe, A. Rawat, and N. Baracaldo, "Federated unlearning: How to efficiently erase a client in fl?," 2023.
* [63] G. Li, L. Shen, Y. Sun, Y. Hu, H. Hu, and D. Tao, "Subspace based federated unlearning," 2023.
* [64] M. Alam, H. Lamri, and M. Maniatakos, "Get rid of your trail: Remotely erasing backdoors in federated learning," 2023.
* [65] J. Wang, S. Guo, X. Xie, and H. Qi, "Federated unlearning via class-discriminative pruning," in _Proceedings of the ACM Web Conference 2022_, WWW '22, (New York, NY, USA), p. 622-632, Association for Computing Machinery, 2022.
* [66] Y. Zhao, P. Wang, H. Qi, J. Huang, Z. Wei, and Q. Zhang, "Federated unlearning with momentum degradation," _IEEE Internet of Things Journal_, pp. 1-1, 2023.
* IEEE Conference on Computer Communications_, pp. 1749-1758, 2022.
* [68] A. Dhasade, Y. Ding, S. Guo, A. marie Kermarrec, M. D. Vos, and L. Wu, "Quickdrop: Efficient federated unlearning by integrated dataset distillation," 2023.
* [69] P. Wang, Z. Yan, M. S. Obaidat, Z. Yuan, L. Yang, J. Zhang, Z. Wei, and Q. Zhang, "Edge caching with federated unlearning for low-latency v2x communications," _IEEE Communications Magazine_, pp. 1-7, 2023.
* [70] H. Xia, S. Xu, J. Pei, R. Zhang, Z. Yu, W. Zou, L. Wang, and C. Liu, "Fedme2: Memory evaluation & erase promoting federated unlearning in dtmn," _IEEE Journal on Selected Areas in Communications_, vol. 41, no. 11, pp. 3573-3588, 2023.
* [71] S. Moon, S. Cho, and D. Kim, "Feature unlearning for pre-trained gans and vaes," in _AAAI Conference on Artificial Intelligence_, 2023.
* [72] S. Bae, S. Kim, H. Jung, and W. Lim, "Gradient surgery for one-shot unlearning on generative model," 2023.

* [73] Anonymous, "Machine unlearning for image-to-image generative models," in _The Twelfth International Conference on Learning Representations_, 2024.
* [74] W. Wang, H. Bai, J. tse Huang, Y. Wan, Y. Yuan, H. Qiu, N. Peng, and M. R. Lyu, "New job, new gender? measuring the social bias in image generation models," 2024.
* [75] Y. Yao, X. Xu, and Y. Liu, "Large language model unlearning," 2023.
* [76] C. Yu, S. Jeoung, A. Kasi, P. Yu, and H. Ji, "Unlearning bias in language models by partitioning gradients," in _Findings of the Association for Computational Linguistics: ACL 2023_ (A. Rogers, J. Boyd-Graber, and N. Okazaki, eds.), (Toronto, Canada), pp. 6032-6048, Association for Computational Linguistics, July 2023.
* [77] J. Chen and D. Yang, "Unlearn what you want to forget: Efficient unlearning for LLMs," in _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_ (H. Bouamor, J. Pino, and K. Bali, eds.), (Singapore), pp. 12041-12052, Association for Computational Linguistics, Dec. 2023.
* [78] Y. Yoshida and T. Miyato, "Spectral norm regularization for improving the generalizability of deep learning," _arXiv preprint arXiv:1705.10941_, 2017.
* [79] A. M. Oberman and J. Calder, "Lipschitz regularized deep neural networks converge and generalize," _CoRR_, vol. abs/1808.09540, 2018.
* [80] G. Khromov and S. P. Singh, "Some fundemental aspects about lipschitz continuity of neural networks," 2023.
* [81] M. Usama and D. E. Chang, "Towards robust neural networks with lipschitz continuity," in _Digital Forensics and Watermarking_ (C. D. Yoo, Y.-Q. Shi, H. J. Kim, A. Piva, and G. Kim, eds.), (Cham), pp. 373-389, Springer International Publishing, 2019.
* [82] T.-W. Weng, H. Zhang, P.-Y. Chen, J. Yi, D. Su, Y. Gao, C.-J. Hsieh, and L. Daniel, "Evaluating the robustness of neural networks: An extreme value theory approach," in _International Conference on Learning Representations_, 2018.
* [83] Y. Yoshida and T. Miyato, "Spectral norm regularization for improving the generalizability of deep learning," 2017.
* [84] Z. Liu, P. Luo, X. Wang, and X. Tang, "Deep learning face attributes in the wild," in _2015 IEEE International Conference on Computer Vision (ICCV)_, pp. 3730-3738, 2015.
* [85] Wennuliu, "Adult income dataset." Kaggle, 2024.
* [86] M. Akturk, "Diabetes dataset." Kaggle, 2024.
* [87] Y. Djebrouni, N. Benarba, O. Touat, P. De Rosa, S. Bouchenak, A. Bonifati, P. Felber, V. Marangozova, and V. Schiavoni, "Bias mitigation in federated learning for edge computing," _Proc. ACM Interact. Mob. Wearable Ubiquitous Technol._, vol. 7, jan 2024.
* [88] R. Chen, J. Yang, H. Xiong, J. Bai, T. Hu, J. Hao, Y. FENG, J. T. Zhou, J. Wu, and Z. Liu, "Fast model debias with machine unlearning," in _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [89] Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, "Gradient-based learning applied to document recognition," _Proceedings of the IEEE_, vol. 86, no. 11, pp. 2278-2324, 1998.
* [90] K. He, X. Zhang, S. Ren, and J. Sun, "Deep residual learning for image recognition," in _2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pp. 770-778, 2016.
* [91] H. Xiao, K. Rasul, and R. Vollgraf, "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms," _arXiv preprint arXiv:1708.07747_, 2017.
* [92] A. Krizhevsky, "Learning multiple layers of features from tiny images," Toronto, ON, Canada, 2009.

* [93] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, "Imagenet: A large-scale hierarchical image database," in _2009 IEEE Conference on Computer Vision and Pattern Recognition_, pp. 248-255, 2009.
* [94] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," in _North American Chapter of the Association for Computational Linguistics_, 2019.
* [95] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, "Learning word vectors for sentiment analysis," in _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_ (D. Lin, Y. Matsumoto, and R. Mihalcea, eds.), (Portland, Oregon, USA), pp. 142-150, Association for Computational Linguistics, June 2011.
* [96] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, "Grad-cam: Visual explanations from deep networks via gradient-based localization," in _2017 IEEE International Conference on Computer Vision (ICCV)_, pp. 618-626, 2017.

Appendix

### Proof of Theorem 1

As illustrated in Sec. 3.2, it is hard to build the unlearned data \(x^{u}\) for the feature unlearning since adding the perturbation may influence the model accuracy seriously. Suppose the feature is successfully removed when the norm of perturbation is larger than \(C\). We define the utility loss \(\ell_{1}\) with unlearning feature successfully:

\[\ell_{1}=\min_{\|\delta_{\mathcal{F}}\|\geq C}\mathbb{E}_{(x,y)\in\mathscr{D}} \min_{\theta}\ell\big{(}f_{\theta}(x+\delta_{\mathcal{F}}),y\big{)}\] (10)

And we define the maximum utility loss with the norm perturbation less than \(C\) as:

\[\ell_{2}=\max_{\|\delta_{\mathcal{F}}\|\geq C}\mathbb{E}_{(x,y)\in\mathscr{D}} \min_{\theta}\ell\big{(}f_{\theta}(x+\delta_{\mathcal{F}}),y\big{)}\] (11)

**Assumption 3**.: _Assume \(\ell_{2}\leq\ell_{1}\)_

Assumption 3 elucidates that the utility loss associated with a perturbation norm less than \(C\) is smaller than the utility loss when the perturbation norm is greater than \(C\). This assumption is logical, as larger perturbations would naturally lead to greater utility loss.

**Assumption 4**.: _Suppose the federated model achieves zero training loss._

We have the following theorem to elucidate the relation between feature sensitivity removing via Alg. 1 and exact unlearning (see proof in Appendix).

**Theorem 2**.: _If Assumption 3 and 4 hold, the utility loss of unlearned model obtained by Alg. 1 is less than the utility loss with unlearning successfully, i.e.,_

\[\ell_{u}\leq\ell_{1},\] (12)

_where \(\ell_{u}=\mathbb{E}_{(x,y)\in\mathscr{D}}\big{(}\ell(f_{\theta^{u}}(x),y)\)_

Proof.: When the unlearning happens during the federated training, the unlearning clients would also optimize the training loss and feature sensitivity simultaneously. Specifically, the optimization process could be written as:

\[\theta_{u}=\operatorname*{arg\,min}_{\theta}\mathbb{E}_{(x,y)\in\mathscr{D}} \big{(}\ell(f_{\theta}(x),y)+\lambda\mathbb{E}_{\lambda\leq\|\delta_{ \mathcal{F}}\|\leq C}\frac{\|f_{\theta}(x)-f_{\theta}(x+\delta_{\mathcal{F}}) \|_{2}}{\|\delta_{\mathcal{F}}\|_{2}}\big{)},\]

where \(\lambda\leq C\) is one coefficient. Without loss of generality, we assume the \(\ell(f_{\theta}(x),y)=\|f_{\theta}(x)-y)\|\). Denote

\[\Theta^{*}=\operatorname*{arg\,min}_{\theta}\mathbb{E}_{(x,y)\in\mathscr{D}} \ell(f_{\theta}(x),y).\]

If Assumption 4 holds, then \(f_{\theta^{*}}(x)=y\) for any \(\theta^{*}\in\Theta^{*}\). Therefore, for any \(\lambda\leq\|\delta_{\mathcal{F}}\|\leq C\) such that

\[\mathbb{E}_{(x,y)\in\mathscr{D}}\big{(}\ell(f_{\theta^{*}}(x),y )+\lambda\mathbb{E}_{\lambda\leq\|\delta_{\mathcal{F}}\|\leq C}\frac{\|f_{ \theta}(x)-f_{\theta^{*}}(x+\delta_{\mathcal{F}})\|_{2}}{\|\delta_{\mathcal{F }}\|_{2}}\big{)}\] (13) \[=\lambda\mathbb{E}_{(x,y)\in\mathscr{D}}\mathbb{E}_{\lambda\leq\| \delta_{\mathcal{F}}\|\leq C}\frac{\|y-f_{\theta^{*}}(x+\delta_{\mathcal{F}}) \|_{2}}{\|\delta_{\mathcal{F}}\|_{2}}\] \[\leq\mathbb{E}_{(x,y)\in\mathscr{D}}\mathbb{E}_{\lambda\leq\| \delta_{\mathcal{F}}\|\leq C}\|y-f_{\theta^{*}}(x+\delta_{\mathcal{F}})\|_{2}.\]Therefore, we further obtain:

\[\begin{split}\ell_{u}&=\min_{\theta\in\mathbb{R}^{d}} \mathbb{E}_{(x,y)\in\mathscr{O}}\big{(}\ell(f_{\theta}(x),y)+\lambda\mathbb{E}_{ \lambda\leq\|\delta_{\mathcal{Y}}\|\leq C}\frac{\|f_{\theta}(x)-f_{\theta}(x+ \delta_{\mathcal{Y}})\|_{2}}{\|\delta_{\mathcal{Y}}\|_{2}}\big{)}\\ &\leq\min_{\theta\in\Theta^{*}}\mathbb{E}_{(x,y)\in\mathscr{O}} \big{(}\ell(f_{\theta}(x),y)+\lambda\mathbb{E}_{\lambda\leq\|\delta_{\mathcal{ Y}}\|\leq C}\frac{\|f_{\theta}(x)-f_{\theta}(x+\delta_{\mathcal{Y}})\|_{2}}{\| \delta_{\mathcal{Y}}\|_{2}}\big{)}\\ &\leq\min_{\theta\in\Theta^{*}}\mathbb{E}_{(x,y)\in\mathscr{O}} \mathbb{E}_{\lambda\leq\|\delta_{\mathcal{Y}}\|\leq C}\|y-f_{\theta^{*}}(x+ \delta_{\mathcal{Y}})\|_{2}\\ &\leq\mathbb{E}_{(x,y)\in\mathscr{O}}\mathbb{E}_{\lambda\leq\| \delta_{\mathcal{Y}}\|\leq C}\min_{\theta\in\Theta^{*}}\|y-f_{\theta^{*}}(x+ \delta_{\mathcal{Y}})\|_{2}\\ &=\mathbb{E}_{\lambda\leq\|\delta_{\mathcal{Y}}\|\leq C}\mathbb{E }_{(x,y)\in\mathscr{O}}\min_{\theta\in\Theta^{*}}\|y-f_{\theta^{*}}(x+\delta_{ \mathcal{Y}})\|_{2}\\ &\leq\max_{\lambda\leq\|\delta_{\mathcal{Y}}\|\leq C}\mathbb{E} _{(x,y)\in\mathscr{O}}\min_{\theta\in\mathbb{R}^{d}}\|y-f_{\theta^{*}}(x+ \delta_{\mathcal{Y}})\|_{2}\\ &\leq\max_{\|\delta_{\mathcal{Y}}\|\leq C}\mathbb{E}_{(x,y)\in \mathscr{O}}\min_{\theta\in\mathbb{R}^{d}}\|y-f_{\theta^{*}}(x+\delta_{ \mathcal{Y}})\|_{2}\\ &=\ell_{2},\end{split}\] (14)

According to Assumption 3, we have \(\ell_{u}\leq\ell_{1}\)

### Experimental Setup

Datasets_MNIST_[89]: Both the _MNIST_[89] and _Fashion-MNIST(FMNIST)_[91] datasets contain images of handwritten digits and attire, respectively. Each dataset comprises 60,000 training examples and 10,000 test examples. In both datasets, each example is represented as a single-channel image with dimensions of 28\(\times\)28 pixels, categorized into one of 10 classes. Additionally, the _Colored-MNIST(CMNIST)_[89] dataset, an extension of the original MNIST, introduces color into the digits of each example. Consequently, images in the Colored MNIST dataset are represented in three channels. _CIFAR_[92]: The _CIFAR-10_[92] dataset comprises 60,000 images, each with dimensions of 32\(\times\)32 pixels and three color channels, distributed across 10 classes. This dataset includes 6,000 images per class and is partitioned into 50,000 training examples and 10,000 test examples. Similarly, the _CIFAR-100_[92] dataset shares the same image dimensions and structure as _CIFAR-10_ but extends to 100 classes, with each class containing 600 images. Within each class, there are 500 training images and 100 test images. Moreover, _CIFAR-100_ organizes its 100 classes into 20 superclasses, forming the _CIFAR-20 dataset_[92]. _CelebA_[84]: A face recognition dataset featuring 40 attributes such as gender and facial characteristics, comprising 162,770 training examples and 19,962 test examples. This study will focus on utilizing the _CelebA_[84] dataset primarily for gender classification tasks. ImageNet [93]: A large-scale image dataset which contains 1.2 million training samples across 1,000 categories.

_Adult Census Income (Adult)_[85] includes 48, 842 records with 14 attributes such as age, gender, education, marital status, etc. The classification task of this dataset is to predict if a person earns over $50K a year based on the census attributes. We then consider marital status as the sensitive feature that aim to unlearn in this study. _Diabetes_[86] includes 768 personal health records of females at least 21 years old with 8 attributes such as blood pressure, insulin level, age and etc. The classification task of this dataset is to predict if a person has diabetes. We then consider number of pregnancies as the sensitive feature that aim to unlearn in this study.

The IMDB movie reviews dataset [95] is widely used for binary sentiment analysis, where the task is to determine whether a review expresses a positive or negative sentiment. It comprises 50,000 movie reviews, each labeled as either positive or negative. In this study, we focus on unlearning the influence of specific sensitive features, particularly the names of celebrities. Each client's local dataset includes names of specific celebrities, which are treated as sensitive features for this analysis.

BaselinesThe baseline methods in this study:

_Baseline_: Original model before unlearning.

_Retrain_: In scenarios involving sensitive feature unlearning, the retrained model was simply trained using a dataset where Gaussian noise was applied to the unlearned feature region. This approach 

[MISSING_PAGE_FAIL:19]

Figure 11: CIFAR-10

Figure 12: CIFAR-20

Figure 13: CIFAR-100

#### a.3.2 Biased Feature Unlearning

### Lipschitz and Non-Lipschitz Loss Analysis

In this section, we evaluate the Lipschitz loss function and its effectiveness in optimizing feature sensitivity, as described in Eq. 6. We also examine a variant without the denominator, termed the Non-Lipschitz loss, as illustrated in Fig. 15.

The results indicate that models optimized using the Non-Lipschitz loss exhibit fluctuations across batches. This is due to the unbounded nature of the optimization process, leading to useless models. Fig. 7(a) further illustrates this issue, showing instances of catastrophic forgetting.

Figure 14: Attention map analysis for bias and unbias samples across model iterations of baseline, retrain, and unlearn model using our proposed Ferrari to unlearn mouth on CelebA dataset.

Conversely, models optimized with the Lipschitz loss demonstrate a steady reduction in feature sensitivity over batches. This bounded optimization provided by Lipschitz bound helps in effectively unlearning target features while preserving model utility, as theoretically guaranteed (see Section Sec. 4.3).

### Non-IID Analysis

This section presents an analysis of the impact of Non-IID data on the performance of the proposed Ferrari framework compared to the Baseline and Retrain methods on the CIFAR-10 dataset. We focus on the accuracy of the retain client dataset (\(\mathcal{D}_{r}\)) and the unlearn client dataset (\(\mathcal{D}_{u}\)) in backdoor feature unlearning, as illustrated in Fig.16. To measure the extent of Non-IID, we used the _Dir_(\(\gamma\)) distribution, where smaller values of \(\gamma\) indicate more heterogeneous data.

The results show that the Ferrari framework significantly improves feature unlearning performance, with a drop of approximately 0.2% in \(\mathcal{D}_{u}\) when \(\gamma=1\) compared to the IID scenario. Furthermore, the Ferrari framework maintains successfully the accuracy of \(\mathcal{D}_{r}\) with only a slight decrease of about 2% compared to the Retrain method within the Non-IID scenario.

### Client Numbers Analysis

This section analyzes the impact of a large-scale FL environment, characterized by a large number of clients, on the performance of the proposed Ferrari framework compared to the Baseline and Retrain methods on the CIFAR-10 dataset. We focus on the accuracy of the retained client dataset (\(\mathcal{D}_{r}\)) and the unlearned client dataset (\(\mathcal{D}_{u}\)) in backdoor feature unlearning, as illustrated in Fig.17. The results indicate that the unlearning performance of our proposed Ferrari framework remains

Figure 16: Non-IID analysis on the CIFAR-10 dataset using our proposed Ferrari framework, compared to Baseline and Retrain methods, for retain client dataset \(\mathcal{D}_{r}\) and unlearn client dataset \(\mathcal{D}_{u}\) accuracy in backdoor feature unlearning.

Figure 15: Lipschitz and Non-Lipschitz loss analysis on backdoor feature unlearning.

consistent, with no significant changes in the accuracy of both \(\mathcal{D}_{r}\) and \(\mathcal{D}_{u}\) as the number of clients increases. This finding further demonstrates the effectiveness of the Ferrari framework in large-scale FL environments.

Figure 17: Scability analysis of client numbers on the CIFAR-10 dataset on the accuracy of retain client dataset \(D_{r}\) and unlearn client dataset \(D_{u}\)

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Guidelines: * The answer NA means that the paper does not include theoretical results. ** All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [NA] Guidelines: * The answer NA means that paper does not include experiments requiring code.

* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors). * It should be clear whether the error bar is the standard deviation or the standard error of the mean. * It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. * For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). * If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.

8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards**Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)?Answer: [NA] Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.