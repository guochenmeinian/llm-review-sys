# New Bounds for Hyperparameter Tuning of

Regression Problems Across Instances

 Maria-Florina Balcan

Carnegie Mellon University

ninamf@cs.cmu.edu

&Anh Tuan Nguyen

Carnegie Mellon University

atnguyen@cs.cmu.edu

&Drayansh Sharma

Carnegie Mellon University

dravyans@cs.cmu.edu

###### Abstract

The task of tuning regularization coefficients in regularized regression models with provable guarantees across problem instances still poses a significant challenge in the literature. This paper investigates the sample complexity of tuning regularization parameters in linear and logistic regressions under \(\ell_{1}\) and \(\ell_{2}\)-constraints in the data-driven setting. For the linear regression problem, by more carefully exploiting the structure of the dual function class, we provide a new upper bound for the pseudo-dimension of the validation loss function class, which significantly improves the best-known results on the problem. Remarkably, we also instantiate the first matching lower bound, proving our results are tight. For tuning the regularization parameters of logistic regression, we introduce a new approach to studying the learning guarantee via an approximation of the validation loss function class. We examine the pseudo-dimension of the approximation class and construct a uniform error bound between the validation loss function class and its approximation, which allows us to instantiate the first learning guarantee for the problem of tuning logistic regression regularization coefficients.

## 1 Introduction

Regularized linear models, including the Elastic Net [1], and Regularized Logistic Regression [2, 3, 4], as well as their variants [5, 6, 7], have found widespread use in diverse fields and numerous application domains. Thanks to their simplicity and interpretability, those methods are popular choices for controlling model complexity, improving robustness, and preventing overfitting by selecting relevant features [4, 8, 9]. Moreover, regularized linear models can be adapted to the non-linear regime using kernel methods [10, 11], significantly expanding their applicability to a wide range of problems. In typical applications, one needs to solve not only a single regression problem instance, but several related problems from the same domain. Can we learn how to regularize with good generalization across the related problem instances?

Suppose we have a regression dataset \((X,y)\in\mathbb{R}^{m\times p}\times\mathcal{Y}^{m}\), where \(X\) is a design matrix with \(m\) samples and \(p\) features, and \(y\) is a target vector. Regularized linear models aim to compute an estimator \(\hat{\beta}^{(X,y)}(\lambda)\) by solving the optimization problem

\[\hat{\beta}^{(X,y)}(\lambda)=\operatorname*{argmin}_{\beta\in\mathbb{R}^{p}} \big{[}l(\beta,(X,y))+\lambda_{1}\left\|\beta\right\|_{1}+\lambda_{2}\left\| \beta\right\|_{2}^{2}\big{]},\] (1)

where \((\lambda_{1},\lambda_{2})\in\mathbb{R}_{>0}^{2}\) are the regularization coefficients. For instance, if \(\lambda\in\mathbb{R}_{>0}^{2}\), \(y\in\mathbb{R}^{m}\), and \(l(\beta,(X,y))=\frac{1}{2}\|y-X\beta\|_{2}^{2}\) (squared-loss function), we get the well-known Elastic Net [1]. On the other hand, if \(\lambda\in\{(\lambda_{1},0),(0,\lambda_{2})\}\) for \(\lambda_{1},\lambda_{2}>0\), \(y\in\{\pm 1\}^{m}\), and \(l(\beta,(X,y))=\frac{1}{m}\sum_{i=1}^{m}\log(1+\exp(-y_{i}x_{i}^{\top}\beta))\), we obtain regularized logistic regression.

In regularized linear models, the parameters \(\lambda\) play a crucial role in controlling the sparsity (\(\ell_{1}\)) and shrinkage (\(\ell_{2}\)) constraints, and are essential in ensuring better generalization and robustness [9; 4; 12]. A popular approach in practice is cross-validation, which involves choosing a finite grid of values of \(\lambda\) and iteratively solving the regression problem for multiple values of \(\lambda\) and evaluating on held-out validation sets to determine the optimal parameter. Principled techniques with theoretical guarantees suffer from various limitations, for example require strong assumptions about the original problem [13], or aim to search the optimal parameter over a discrete subset instead of the whole continuous domain. Moreover, repeatedly solving the regression problem is particularly inefficient if we have multiple problem instances from the same problem domain.

In this work, we investigate an alternative setting for tuning regularization parameters, namely _data-driven algorithm design_, following the previous line of work by Balcan et al. [14]. Unlike the traditional approach, which involves considering a single dataset \((X,y)\), in the data-driven approach, we analyze a collection of datasets or problem instances \((X^{(i)},y^{(i)},X^{(i)}_{\text{val}},y^{(i)}_{\text{val}})\) drawn from an underlying problem distribution \(\mathcal{D}\). Our objective is to determine the optimal regularization parameters \(\lambda\) so that when using the training set \((X^{(i)},y^{(i)})\) and \(\lambda\) to select a model in Optimization problem 1, the selected model minimizes loss on the validation set \((X^{(i)}_{\text{val}},y^{(i)}_{\text{val}})\). As remarked by Balcan et al. [14], data-driven algorithm design can handle more diverse data generation scenarios in practice, including cross validation and multitask-learning [15; 16]. We emphasize that the data-driven setting differs significantly from the standard single dataset setting.

In this paper, we consider the problem of tuning regularization parameters in regularized logistic regression and the Elastic Net across multiple problem instances. Our contributions are:

* We present an improved upper bound (Theorem 3.3) on the pseudo-dimension for tuning the Elastic Net regularization parameters across problem instances by establishing a novel structural result for the validation loss function class (Theorem 3.2). We provide a crucial refinement to the piecewise structure of this function class established by Balcan et al. [14], by providing a bound on the number of distinct functional behaviors across the pieces. This enables us to describe the computation of the validation loss function as a GJ algorithm [17], which yields an upper-bound of \(O(p)\) on the pseudo-dimension, a significant improvement of the prior best bound of \(O(p^{2})\) by Balcan et al. [14], and a corresponding improvement in the sample complexity (Theorem 3.4).
* Furthermore, we establish the tightness of our result by providing the first asymptotically matching lower bound of \(\Omega(p)\) on the pseudo-dimension (Theorem 3.5). It is worth noting that our results have direct implications for other specialized cases, such as LASSO and Ridge Regression.
* We further extend our results on the Elastic Net to regularized kernel linear regression problem (Corollary 3.6).
* We propose a novel approach to analyze the problem of tuning regularization parameters in regularized logistic regression, which involves indirectly investigating an approximation of the validation loss function class. Using this approach, we instantiate the first learning guarantee for this problem in the data-driven setting (Theorem 4.4).

### Related work

**Model selection for regularized linear models.** Extensive research has focused on the selection of optimal parameters for regularized linear models, including the Elastic Net and regularized logistic regression. This process usually entails choosing the appropriate regularization coefficients for a given dataset [18; 19]. Nevertheless, a substantial proportion of this research relies on heuristic approaches that lack theoretical guarantees [20; 21]. Others have concentrated on creating tuning objectives that go beyond validation error [22; 23], but with no clearly defined procedures for provably optimizing them. The conventional method for selecting a tuning regularization parameter is through grid-based selection, which aims to choose the parameter from a subset, known as a grid, within the parameter space. While this approach provides certain guarantees [24], it falls short in delivering an optimal solution across the entire continuous parameter space, particularly when using tuning objectives that exhibit numerous discontinuities. Additionally, the grid-based technique is highly sensitive to density, as selecting a grid that is either too dense or too coarse might result in inefficient search or highly inaccurate solutions. Other guarantees require strong assumptions on the data distribution, such as sub-Gaussian noise [25; 13]. Some studies focus on evaluating regularized linear models by constructing solution paths [26; 2; 27]. However, it is important to note that these approaches are primarily computational in nature and do not provide theoretical guarantees.

**Data-driven algorithm design.** Data-driven algorithms can adapt their internal structure or parameters to problem instances from unknown application-specific distributions. It is proved to be effective for a variety of combinatorial problems, such as clustering, integer programming, auction design, and graph-based semi-supervised learning [28; 29; 30; 31]. Balcan et al. [14] recently introduced a novel approach to tuning regularization parameters in regularized linear regression models, such as Elastic Net and its variants. They applied data-driven analysis to reveal the underlying discrete structure of the problem and leveraged a general result from [32] to obtain an upper bound on the pseudo-dimension of the problem. To provably tune the regularization parameters across problem instances, they proposed a simple ERM learner and provided sample complexity guarantee for such learner. However, the general techniques from [32] do not always lead to optimal bounds on the pseudodimension. Our paper is an example of a problem where these bounds (as derived in [14]) are sub-optimal, and more specialized techniques due to [31] result in the tighter bounds that we obtain. Also prior work does not establish any lower bound on the pseudodimension. Furthermore, it should be noted that their analysis heavily relies on the assumption of having a closed-form representation of the Elastic Net estimator [27]. This approach may not be applicable in analyzing other regularized linear models, such as regularized logistic regression, for which we propose an alternative approach.

## 2 Problem setting

In this section, we provide a formal definition of the problem of tuning regularization parameters in the Elastic Net and regularized logistic regression (RLR) across multiple problem instances, which follows the settings by Balcan et al. [14]. Given a problem instance \(P=(X,y,X_{\text{val}},y_{\text{val}})\), where \((X,y)\in\mathbb{R}^{m\times p}\times\mathcal{Y}^{m}\) represents the training dataset with \(m\) samples and \(p\) features, and \((X_{\text{val}},y_{\text{val}})\in\mathbb{R}^{m^{\prime}\times p}\times \mathcal{Y}^{m^{\prime}}\) denotes the validation split with \(m^{\prime}\) samples, we consider the estimator \(\hat{\beta}_{(X,y)}(\lambda)\) defined as:

\[\hat{\beta}_{(X,y)}(\lambda)\in\operatorname*{argmin}_{\beta\in\mathbb{R}^{p} }l(\beta,(X,y))+\langle\lambda,R(\beta)\rangle,\] (2)

where \(l(\beta,(X,y))\) represents the objective loss function, \(\lambda\) denotes the regularization coefficients, and \(R(\beta)=(\|\beta\|_{1},\|\beta\|_{2}^{2})\) represents the regularization vector function.

For instance, if \(\lambda\in\mathbb{R}_{>0}^{2}\), \(\mathcal{Y}\equiv\mathbb{R}\), and \(l(\beta,(X,y))=l_{\text{EN}}(\beta,(X,y))=\frac{1}{2m}\|y-X\beta\|_{2}^{2}\), we get the well-known Elastic Net. On the other hand, if \(\lambda\in\{(\lambda_{1},0),(0,\lambda_{2})\}\) for \(\lambda_{1},\lambda_{2}>0\), \(y\in\{\pm 1\}^{m}\), and \(l(\beta,(X,y))=l_{\text{RLR}}(\beta,(X,y))=\frac{1}{m}\sum_{i=1}^{m}\log(1+ \exp(-y_{i}x_{i}^{\top}\beta))\), we obtain RLR with \(\ell_{1}\) or \(\ell_{2}\) regularization. Note that for the Elastic Net hyperparameter tuning problem, we allows the regularization coefficients of both \(\ell_{1},\ell_{2}\) are positive, while in the Regularized Logistic Regression

Figure 1: The process of tuning regularization parameter \(\lambda\) across problem instances. Given a set of \(N\) problem instances \(\{P^{(1)},\dots,P^{(N)}\}\) drawn from some problem distribution \(\mathcal{D}\), one seeks to choose the best parameter \(\hat{\lambda}\) by minimizing the total validation loss \(\sum_{i=1}^{N}h(P^{(i)};\lambda)\).

problem, we consider either \(\ell_{1}\) or \(\ell_{2}\) as the regularization term. We then use the validation set \((X_{\text{val}},y_{\text{val}})\) to calculate the validation loss \(h(\lambda,P)=l(\hat{\beta}_{(X,y)}(\lambda),(X_{\text{val}},y_{\text{val}}))\) corresponding to the problem instance \(P\) and learned regularization parameters \(\lambda\).

In the _data-driven setting_, we receive a collection of \(n\) problem instances \(P^{(i)}=(X^{(i)},y^{(i)},X_{\text{val}}^{(i)},y_{\text{val}}^{(i)})\in\mathcal{ R}_{m_{i},p_{i},m^{\prime}_{i}}\) for \(i\in[n]\), where \(\mathcal{R}_{m_{i},p_{i},m^{\prime}_{i}}:=\mathbb{R}^{m_{i}\times p_{i}}\times \mathcal{Y}^{m_{i}\times p_{i}}\times\mathcal{Y}^{m^{\prime}_{i}}\). The problem space \(\Pi_{m,p}\) is given by \(\Pi_{m,p}=\cup_{m_{1}\geq 0,m_{2}\leq m,p_{1}\leq P}\mathcal{R}_{m_{1},p_{1},m_{2}}\), and we assume that problem instance \(P\) is drawn i.i.d from the problem distribution \(\mathcal{D}\) over \(\Pi_{m,p}\). Remarkably, in this setting, problem instances can have varying training and validation sample sizes, as well as different sets of features. This general framework applies to practical scenarios where the feature sets differ among instances and allows one to learn regularization parameters that effectively work on average across multiple different but related problem instances. See Figure 1 for an illustration of the setting.

The goal here is to learn the value \(\hat{\lambda}\) s.t. with high probability over the draw of \(n\) problem instances, the expected validation loss \(\mathbb{E}_{P\sim\mathcal{D}}h(\hat{\lambda},P)\) is close to \(\min_{\lambda}\mathbb{E}_{P\sim\mathcal{D}}[h(\lambda,P)]\). This paper primarily focuses on providing learning guarantees in terms of sample complexity for the problem of tuning regularization parameters in the Elastic Net and regularized logistic regression (RLR). Specifically, we aim to address the question of how many problem instances are required to learn a value of \(\lambda\) that performs well across all problems \(P\) drawn from the problem distribution \(\mathcal{D}\). To achieve this, we analyze the pseudo-dimension (in the case of the Elastic Net) or the Rademacher Complexity (for RLR) of the validation loss function class \(\mathcal{H}=\{h(\lambda,\cdot)\mid\lambda\in\Lambda\}\), where \(\Lambda\) represents the search space for \(\lambda\).

## 3 Tight pseudo-dimension bounds for Elastic Net hyperparameter tuning

In this section, we will present our results on the pseudo-dimension upper and lower bounds for the regularized linear regression problem in the data-driven setting. Classic learning-theoretic results [33; 34] connect the pseudo-dimension of the validation loss function class (parameterized by the regularization coefficient) with the _sample complexity_ of the number of problem instances \(\{P^{(1)},\dots,P^{(n)}\}\) drawn i.i.d. from some unknown problem distribution \(\mathcal{D}\) needed for learning good regularization parameters with high confidence. Let \(h_{\text{EN}}(\lambda,P)=l_{\text{EN}}(\hat{\beta}_{(X,y)}(\lambda),(X_{\text {val}},y_{\text{val}}))\) be the validation loss function of the Elastic Net, and \(\mathcal{H}_{\text{EN}}=\{h_{\text{EN}}(\lambda,P):\Pi_{m,p}\rightarrow\mathbb{ R}_{\geq 0}\mid\lambda\in\mathbb{R}_{>0}^{2}\}\) be the corresponding validation loss function class, we now present tight bounds for \(\text{Pdim}(\mathcal{H}_{\text{EN}})\).

### The Goldberg-Jerrum framework

Recently, Bartlett et al. [31] instantiate a simplified version of the well-known Goldberg-Jerrum (GJ) Framework [17]. The GJ framework offers a general pseudo-dimension upperbound for a wide class of functions in which each function can be computed by a _GJ algorithm_. We provide a brief overview of the GJ Framework which is useful in establishing our improved pseudo-dimension upper bound.

**Definition 1** (GJ Algorithm, [31]).: _A **GJ algorithm**\(\Gamma\) operates on real-valued inputs, and can perform two types of operations:_

* _Arithmetic operators of the form_ \(v^{\prime\prime}=v\odot v^{\prime}\)_, where_ \(\odot\in\{+,-,\times,\div\}\)_, and_
* _Conditional statements of the form_ "_if_ \(v\geq 0\dots\) _else_ \(\dots\)_"._

_In both cases, \(v\) and \(v^{\prime}\) are either inputs or values previously computed by the algorithm._

General speaking, each intermediate value of the GJ algorithm \(\Gamma\) can be described by a _rational function_, which is a fractional between two polynomials, of the algorithm's inputs. The degree of a rational function is equal to the maximum degree of the polynomials in its numerator and its denominator. We can define two quantities that represent the complexity of GJ algorithms.

**Definition 2** (Complexity of GJ algorithm, [31]).: _The **degree** of a GJ algorithm is the maximum degree of any rational function it computes of the inputs. The **predicate complexity** of a GJ algorithm is the number of distinct rational functions that appear in its conditional statements._

The following theorem essentially shows that for any function class \(\mathcal{F}\), if we can describe any function \(f\in\mathcal{F}\) by a GJ algorithm of which the degree and predicate complexity are at most \(\Delta\) and \(\Lambda\), respectively, then we can automatically obtain the upper bound for the pseudo-dimension of \(\mathcal{F}\).

**Theorem 3.1** ([31]).: _Suppose that each function \(f\in\mathcal{F}\) is specified by \(n\) real parameters. Suppose that for every \(x\in\mathcal{X}\) and \(r\in\mathbb{R}\), there is a GJ algorithm \(\Gamma_{x,r}\) that given \(f\in\mathcal{F}\), returns "true" if \(f(x)\geq r\) and "false" otherwise. Assume that \(\Gamma_{x,r}\) has degree \(\Delta\) and predicate complexity \(\Lambda\). Then, \(\text{\rm Pdim}(\mathcal{F})=O(n\log(\Delta\Lambda))\)._

### Upper bound

Our work improves on prior research [14] by presenting an upper bound on the pseudo-dimension of Elastic Net validation loss function class \(\mathcal{H}_{\text{\rm EN}}\) parameterized by \(\lambda\). We extend the previous piecewise-decomposable structure of the loss function by providing a bound on the number of distinct rational piece functions for any fixed problem instance (Definition 3). This allows us to use a GJ algorithm and Theorem 3.1 to obtain better bounds on the number of distinct predicates that need to be computed. While prior research only used a bound on the number of distinct loss function pieces generated by algebraic boundaries, our new observation that the loss function has a limited number of possible distinct functional behaviors yields a tighter upper bound on the pseudo-dimension (Theorem 3.2). In Theorem 3.5, we will demonstrate the tightness of our upper bound by providing a novel lower bound for the problem.

We first provide a refinement of the piece-wise decomposable function class terminology introduced by [32] which is useful for establishing our improved upper bound. Intuitively, this corresponds to real-valued functions for which the domain is partitioned by finitely many _boundary functions_ such that the function is well-behaved in each piece in the partition, i.e. can be computed using a _piece function_ from another function class.

**Definition 3**.: _A function class \(\mathcal{H}\subseteq\mathbb{R}^{\mathcal{Y}}\) that maps a domain \(\mathcal{Y}\) to \(\mathbb{R}\) is \((\mathcal{F},k_{\mathcal{F}},\mathcal{G},k_{\mathcal{G}})-\)piece-wise decomposable for a class \(\mathcal{G}\) of boundary functions and a class \(\mathcal{F}\in\mathbb{R}^{\mathcal{Y}}\) of piece functions if the following holds: for every \(h\in\mathcal{H}\), (1) there are \(k_{\mathcal{G}}\) functions \(g^{(1)},\dots,g^{(k_{\mathcal{G}})}\in\mathcal{G}\) and a function \(f_{\textbf{b}}\in\mathcal{F}\) for each bit vector \(\textbf{b}\in\{0,1\}^{k_{\mathcal{G}}}\) s.t. for all \(y\in\mathcal{Y}\), \(h(y)=h_{\textbf{b}_{y}}(y)\) where \(\textbf{b}_{y}=\{(g^{(1)}(y),\dots,g^{(k_{\mathcal{G}})}(y))\}\in\{0,1\}^{k_{ \mathcal{G}}}\), and (2) there is at most \(k_{\mathcal{F}}\) different functions in \(\mathcal{F}\)._

A key distinction from [32] is the finite bound \(k_{\mathcal{F}}\) on the number of different piece functions needed to define any function in the class \(\mathcal{H}\). Under this definition we give the following more refined structure for the Elastic Net loss function class by extending arguments from [14].

**Theorem 3.2**.: _Let \(\mathcal{H}_{\text{\rm{EN}}}=\{h_{\text{\rm{EN}}}(\lambda,\cdot):\Pi_{m,p} \rightarrow\mathbb{R}_{\geq 0}\mid\lambda\in\mathbb{R}_{\geq 0}^{2}\}\) be the class of Elastic Net validation loss function class. Consider the dual class \(\mathcal{H}^{*}_{\text{\rm{EN}}}=\{h^{*}_{p}:\mathcal{H}_{\text{\rm{EN}}} \rightarrow\mathbb{R}_{\geq 0}\mid P\in\Pi_{m,p}\}\), where \(h^{*}_{P}(h_{\text{\rm{EN}}}(\lambda,\cdot))=h_{\text{\rm{EN}}}(\lambda,P)\). Then \(\mathcal{H}^{*}_{\text{\rm{EN}}}\) is \((\mathcal{F},3^{p},\mathcal{G},3^{p})\)-piecewise decomposable, where the piece function class \(\mathcal{F}=\{f_{q}:\mathcal{H}_{\text{\rm{EN}}}\rightarrow\mathbb{R}\}\) consists at most \(3^{p}\) rational functions \(f_{q_{1},q_{2}}:h_{\text{\rm{EN}}}(\lambda,\cdot)\mapsto\frac{q_{1}(\lambda_{1 },\lambda_{2})}{q_{2}(\lambda_{1},\lambda_{2})}\) of degree at most \(2p\), and the boundary function class \(\mathcal{G}=\{g_{r}:\mathcal{H}_{\text{\rm{EN}}}\rightarrow\{0,1\}\}\) consists

Figure 2: An illustration of piece-wise structure of \(\mathcal{H}^{*}_{\text{\rm{EN}}}=\{h^{*}_{P}:\mathcal{H}_{\text{\rm{EN}}} \rightarrow\mathbb{R}_{\geq 0}\mid P\in\Pi_{m,p}\}\). Given a problem instance \(P\), the function \(h^{*}_{P}(\lambda)=h(P;\lambda)\) is a fixed rational function \(f_{i}(\lambda)\) in each piece (piece function), that is regulated by boundary functions \(g_{r_{i}}\) of the form \(\mathbbm{1}\{r_{i}(\lambda)<0\}\). As mentioned in our main result, there are at most \(3^{p}\) functions \(f_{i}\) of degree at most \(2p\), and at most \(p3^{p}\) functions \(g_{r_{i}}\) where \(r_{i}\) is a polynomial of degree at most \(p\).

of semi-algebraic sets bounded by at most \(p3^{p}\) algebraic curves \(g_{r}:h_{\text{EN}}(\lambda,\cdot)\mapsto\mathbbm{1}\{r(\lambda_{1},\lambda_{2})<0\}\), where \(r\) is a polynomial of degree at most \(p\)._

Figure 2 demonstrates the piece-wise structure of \(\mathcal{H}_{\text{EN}}^{*}\), which allows us to establish an improved upper bound on the pseudo-dimension.

**Theorem 3.3**.: _Let \(\mathcal{H}_{\text{EN}}=\{h_{\text{EN}}(\lambda,\cdot):\Pi\to\mathbb{R}_{\geq 0 }\mid\lambda\in\mathbb{R}_{>0}^{2}\}\) be the Elastic Net validation loss function class that maps problem instance \(P\) to validation loss \(h_{\text{uni}}(\lambda,P)\). Then \(\text{Pdim}(\mathcal{H}_{\text{EN}})\) is \(O(p)\)._

_Proof Sketch._ For every problem instance \(P\in\Pi_{m,p}\), and a threshold \(r\in\mathbb{R}\), consider the computation \(\mathbbm{1}\{h_{\text{EN}}(\lambda,P)-r\geq 0\}\) for any \(h_{\text{EN}}(\lambda,\cdot)\in\mathcal{H}_{\text{EN}}\). From Theorem 3.2, we can describe \(\mathbbm{1}\{h_{\text{EN}}(\lambda,P)-r\geq 0\}\) as a GJ algorithm \(\Gamma_{P,r}\) which is specified by 2 parameters \(\lambda_{1},\lambda_{2}\), has degree of at most \(2p\), and has predicate complexity of at most \((p+1)3^{p}\) (See Figure 3). Then Theorem 3.1 implies that \(\text{Pdim}(\mathcal{H}_{\text{EN}})=O(p)\). \(\square\)

The detailed proof of Theorem 3.3 can be found on Appendix B.1.2. Recent work by Balcan et al. [14] also studied the Elastic Net, and showed the piece-wise structure of the dual function of the validation loss function which implies an upper bound of \(O(p^{2})\) by employing the general tool from [32]. We establish a tighter bound of \(O(p)\) in Theorem 3.3 by establishing additional properties of the loss function class and giving a GJ algorithm for computing the loss functions.

To guarantee the boundedness of the considered validation loss function classes, we will have the following assumptions for the data and regularization parameters. The first assumption is that all features and target values in the training and validation examples are bounded. The second assumption is that we only consider regularization coefficient values \(\lambda\) within an interval \([\lambda_{\min},\lambda_{\max}]\). In practice, those assumptions are naturally satisfied by data normalization.

**Assumption 1** (Bounded covariate and label).: _We assume that all the feature vectors and target values in training and validation set is upper-bounded by absolute constants \(R_{1}\) and \(R_{2}\), i.e. \(\max\{\left\|X\right\|_{\infty},\left\|X_{\text{out}}\right\|_{\infty}\}\leq R _{1}\), and \(\max\{\left\|y\right\|_{\infty},\left\|y_{\text{out}}\right\|_{\infty}\}\leq R _{2}\)._

**Assumption 2** (Bounded Coefficient).: _We assume that \(\lambda\in[\lambda_{\min},\lambda_{\max}]^{2}\) with \(\lambda_{\min}>0\)._

Under Assumptions 2, 1, Theorem 3.3 immediately implies the following generalization guarantee for Elastic Net hyperparameter tuning.

**Theorem 3.4**.: _Let \(\mathcal{D}\) be an arbitrary distribution over the problem instance space \(\Pi_{m,p}\). Under Assumptions 1, 2, the loss functions in \(\mathcal{H}_{\text{EN}}\) have range bounded by some constant \(H\) (Lemma C.1). Then there exists an algorithm s.t. for any \(\epsilon,\delta>0\), given \(N=O(\frac{H^{2}}{\epsilon^{2}}(p+\log(\frac{1}{\delta})))\) sample problem instances drawn from \(\mathcal{D}\), the algorithm outputs a regularization parameter \(\hat{\lambda}\) such that with probability at least \(1-\delta\), \(\mathbb{E}_{P\sim\mathcal{D}}h_{\text{EN}}(\hat{\lambda},P)<\min_{\lambda} \mathbb{E}_{P\sim\mathcal{D}}h_{\text{EN}}(\lambda,P)+\epsilon\)._

_Proof._ Denote \(\lambda^{*}=\operatorname*{argmin}_{\lambda}\mathbb{E}_{P\sim\mathcal{D}}h_{ \text{EN}}(\lambda,P)\). From Theorems 3.3 and A.2, given \(n=O(\frac{H^{2}}{\epsilon^{2}}(p+\log(\frac{1}{\delta})))\) problem instances \(P^{(i)}\) for \(i\in[N]\) drawn from \(\mathcal{D}\), w.p. \(1-\delta\), we have \(\mathbb{E}_{P\sim\mathcal{D}}h_{\text{EN}}(\hat{\lambda},P)<\frac{1}{N}\sum_{i=1 }^{N}h_{\text{EN}}(\hat{\lambda},P^{(i)})+\frac{\epsilon}{2}<\frac{1}{N}\sum_{i= 1}^{N}h_{\text{EN}}(\lambda^{*},P^{(i)})+\frac{\epsilon}{2}<\mathbb{E}_{P\sim \mathcal{D}}h_{\text{EN}}(\lambda^{*},P)+\epsilon\). \(\square\)

Figure 3: An illustration of how \(\mathbbm{1}\{h_{\text{EN}}(\lambda,P)-r\geq 0\}\) is computed as a GJ algorithm. The number of boundary (polynomial) functions \(k_{\mathcal{G}}\) is at most \(p3^{p}\), and there are at most \(M=3^{p}\) distinct (rational) piece functions. All the polynomial and rational functions are of degree at most \(2p\).

### Lower bound

Remarkably, we are able to establish a matching lower bound on the pseudo-dimension of the Elastic Net loss function class, parameterized by the regularization parameters. Note that every Elastic Net problem can be converted to an equivalent LASSO problem [1]. In fact, we show something stronger, that the pseudo-dimension of even the LASSO regression loss function class (parameterized by regression coefficient \(\lambda_{1}\)) is \(\Omega(p)\), from which the above observation follows (by taking \(\lambda_{2}=0\) in our construction). Our proof of the lower bound adapts the "adversarial strategy" of [35] which is used to design a worst-case LASSO regularization path. While [35] construct a single dataset to bound the number of segments in the piecewise-linear LASSO solution path, we create a collection of problem instances for which all above-below sign patterns may be achieved by selecting regularization parameters from different segments of the solution path.

**Theorem 3.5**.: _Let \(\mathcal{H}_{\text{LASSO}}\) be a set of functions \(\{h_{\text{LASSO}}(\lambda,\cdot):\Pi_{m,p}\rightarrow\mathbb{R}_{\geq 0} \mid\lambda\in\mathbb{R}^{+}\}\) that map a regression problem instance \(P\in\Pi_{m,p}\) to the validation loss \(h_{\text{LASSO}}(\lambda,P)\) of LASSO trained with regularization parameter \(\lambda\). Then \(\text{\emph{Pdim}}(\mathcal{H}_{\text{LASSO}})\) is \(\Omega(p)\)._

Proof Sketch.: Consider \(N=p\) problem instances for LASSO regression given by \(P^{(i)}=(X^{(i)},y^{(i)},X^{(i)}_{\text{val}},y^{(i)}_{\text{val}})\), where the training set \((X^{(i)},y^{(i)})=(X^{*},y^{*})\) is fixed and set using the "adversarial strategy" of [35], Proposition 2. The validation sets are given by single examples \((X^{(i)}_{\text{val}},y^{(i)}_{\text{val}})=(\mathbf{e}_{i},0)\), where \(\mathbf{e}_{i}\) are standard basis vectors in \(\mathbb{R}^{p}\). We will now proceed to provide the witnesses \(r_{1},\ldots,r_{N}\) and \(\lambda\) values to exhibit a pseudo-shattering of these problem instances.

Corresponding to subset \(T\subseteq[p]\) of problem instances, we will provide a value of \(\lambda_{T}\) such that, we have \(\ell_{\text{LASSO}}(\lambda_{T},P^{(i)})>r_{i}\) iff \(i\in T\), for each \(i\in[p]\) and each \(T\subseteq[p]\). We set all witnesses \(r_{i}=0\) for all \(i\in[p]\). As a consequence of Theorem 1 in [35], the regularization path of \((X^{*},y^{*})\) consists of a linear segment corresponding all \(2^{p}\) unsigned sparsity patterns in \(\{0,1\}^{p}\) (we will not need all the segments in the construction, but note that it is guaranteed to contain all distinct unsigned sparsity patterns) and we select \(\lambda_{T}\) as any interior point corresponding to a linear segment with sparsity pattern \(\{(c_{1},\ldots,c_{p})\mid c_{i}=0\) iff \(i\in T\}\), i.e. elements in \(T\) are exactly the ones with sparsity pattern 0. Therefore, \(|\beta_{T}^{*}\cdot\mathbf{e}_{i}|=0\) iff \(i\in T\), where \(\beta_{T}^{*}\) is the LASSO regression fit for regularization parameter \(\lambda_{T}\). This implies the desired shattering condition w.r.t. witnesses \(r_{1}=0,\ldots,r_{N}=0\). Therefore, \(\text{\emph{Pdim}}(\mathcal{H}_{\text{LASSO}})\geq p\). See Appendix B.2 for a full proof. 

### Hyperparameter tuning in Regularized Kernel Regression

The Kernel Least Squares Regression ([4]) is a natural generalization of the linear regression problem, which uses a kernel to handle non-linearity. In this problem, each sample has \(p_{1}\) feature, corresponding to a real-valued target. Formally, each problem instance \(P\) drawn from \(\Pi\) can be described as

\[P=(X,y,X_{\text{val}},y_{\text{val}})\in\mathbb{R}^{m\times p_{1}}\times \mathbb{R}^{m}\times\mathbb{R}^{m^{\prime}\times p_{1}}\times\mathbb{R}^{m^{ \prime}}.\]

A common issue in practice is that the relation between \(y\) and \(X\) is non-linear in the original space. To overcome this issue, we consider the mapping \(\phi:\mathbb{R}^{p_{1}}\rightarrow\mathbb{R}^{p_{2}}\) which maps the original input space to a new feature space in which we hopefully can perform linear regression. Define \(\phi(X)=(\phi(x_{1}),\ldots,\phi(x_{m}))_{m\times p_{2}}\), our goal is to find a vector \(\theta\in\mathbb{R}^{p_{2}}\) so that the squared loss \(\frac{1}{2}\left\|y-\phi(X)\theta\right\|_{2}^{2}+R(\left\|\theta\right\|)\) is minimized, where the regularization term \(R(\left\|\theta\right\|)\) is any strictly monotonically increasing function of the Hilbert space norm. It is well-known from the literature (e.g. [36]) that under the Representer Theorem's conditions, the optimal value \(\theta^{*}\) can be linearly represented by row vectors of \(\phi(X)\), i.e., \(\theta^{*}=\phi(X)\beta=\sum_{i=1}^{m}\phi(x_{i})\beta_{i}\), where \(\beta=(\beta_{1},\ldots,\beta_{m})\in\mathbb{R}^{m}\). This directly includes the \(\ell_{2}\) regularizer but does not include \(\ell_{1}\) regularization. To overcome this issue, Roth ([3]) proposed an alternative approach to regularized kernel regression, which directly restricts the representation of coefficient \(\theta\) via a linear combination of \(\phi(x_{i})\), for \(i\in[m]\). The regularized kernel regression hence can be formulated as

\[\hat{\beta}^{(X,y)}_{l,\lambda}=\operatorname*{argmin}_{\beta\in\mathbb{R}^{m }}\frac{1}{2}\left\|y-K\beta\right\|_{2}^{2}+\lambda_{1}\left\|\beta\right\|_{ 1}+\lambda_{2}\left\|\beta\right\|_{2}^{2},\]

where \(k(x,x^{\prime})=\left\langle\phi(x),\phi(x^{\prime})\right\rangle\) is the kernel mapping, and the Gram matrix \(K\) satisfies \([K]_{i,j}=k(x_{i},x_{j})\) for all \(i,j\in[m]\).

Clearly, the problem above is a linear regression problem. Formally, denote \(h_{\text{\sc ker}}(\lambda,P)=\frac{1}{2}\|y-K\hat{\beta}_{(X,y)}(\lambda)\|_{2}\) and let \(\mathcal{H}_{\text{\sc ker}}=\{h_{\text{\sc ker}}(\lambda,\cdot):\Pi_{m,p} \rightarrow\mathbb{R}_{\geq 0}\mid\lambda\in\mathbb{R}_{+}^{2}\}\). The following result is a direct corollary of Theorem 3.3, which gives an upper bound for the pseudo-dimension of \(\mathcal{H}_{\text{\sc ker}}\).

**Corollary 3.6**.: \(\text{Pdim}(\mathcal{H}_{\text{\sc ker}})=O(m)\)_._

Note that \(m\) here denotes the training set size for a single problem instance, and Corollary 3.6 implies a guarantee on the number of problem instances needed for learning a good regularization parameter for kernel regression via classic results [33; 34]. Our results do not make any assumptions on the \(m\) samples within a problem instance/dataset; if these samples within problem instances are further assumed to be i.i.d. draws from some data distribution (distinct from problem distribution \(\mathcal{D}\)), then well-known results imply that \(m=O(k\log p)\) samples are sufficient to learn the optimal LASSO coefficient [37; 38], where \(k\) denotes the number of non-zero coefficients in the optimal regression fit.

## 4 Hyperparameter tuning for Regularized Logistic Regression

Logistic regression is more naturally suited to applications modeling probability of an event, like medical risk for a patient [39], predicting behavior in markets [40], failure probability of an engineering system [41] and many more applications [42]. It is a fundamental statistical technique for classification, and regularization is again crucial for avoiding overfitting and estimating variable importance. In this section, we will present learning guarantees for tuning the Regularized Logistic Regression (RLR) regularization coefficients across instances. Given a problem instance \(P\) drawn from a problem distribution \(\mathcal{D}\) over \(\Pi_{m,p}\), let \(h_{\text{\sc lrLR}}(\lambda,P)=l_{\text{\sc lrLR}}(\hat{\beta}_{(X,y)}(\lambda),(X_{\text{val}},y_{\text{val}}))\) be the RLR validation loss function class (defined in Section 2), and let \(\mathcal{H}_{\text{\sc lrLR}}=\{h_{\text{\sc lrLR}}(\lambda,\cdot):\Pi_{m,p} \rightarrow\mathbb{R}_{\geq 0}\mid\lambda\in\mathbb{R}_{>0}\}\) be the RLR validation loss function class, our goal is to provide a learning guarantee for \(\mathcal{H}_{\text{\sc lrLR}}\). Besides, we also study the commonly used 0-1 validation loss function class \(\mathcal{H}_{\text{\sc lrLR}}^{0:1}=\{h_{\text{\sc lrLR}}^{0:1}(\lambda,\cdot ):\Pi_{m,p}\rightarrow\mathbb{R}_{\geq 0}\mid\lambda\in\mathbb{R}_{>0}\}\), where \(h_{\text{\sc lrLR}}^{0:1}(\lambda,P)=\frac{1}{m^{\prime}}\sum_{i=1}^{m^{\prime }}\mathbbm{1}\{y_{i}x_{i}^{\top}\hat{\beta}_{X,y}(\lambda)\leq 0\}\), which we will cover in Section 4.3. Similarly, to guarantee the boundedness of \(\mathcal{H}_{\text{\sc lrLR}}\), we also assume that Assumptions 1 and 2 also hold in this setting.

### Approximate solutions of Regularized Logistic Regression

The main challenge in analyzing the regularized logistic regression, unlike the regularized logistic regression problem, is that the solution \(\hat{\beta}_{(X,y)}(\lambda)\) corresponding to a problem instance \(P\) and particular value \(\lambda>0\) does not have a closed form depending on \(\lambda\). We then propose an alternative approach to this end, which is examining via the approximation \(\beta_{(X,y)}^{(\epsilon)}(\lambda)\) of the solution \(\hat{\beta}_{(X,y)}(\lambda)\).

``` Set \(\beta_{0}^{(\epsilon)}=\hat{\beta}_{(X,y)}(\lambda_{\min})\), \(t=0\), small constant \(\delta\in\mathbb{R}_{>0}\), and \(\mathcal{A}=\{j\mid[\hat{\beta}_{(X,y)}(\lambda_{\min})]_{j}\neq 0\}\). while\(\lambda_{t}<\lambda_{\max}\)do \(\lambda_{t+1}=\lambda_{t}+\epsilon\) \(\left(\beta_{t+1}^{(\epsilon)}\right)_{\mathcal{A}}=\left(\beta_{t}^{(\epsilon )}\right)_{\mathcal{A}}-\left[\nabla^{2}l\left(\beta_{t}^{(\epsilon)},(X,y) \right)_{\mathcal{A}}\right]^{-1}\cdot\left[\nabla l\left(\beta_{t}^{(\epsilon )},(X,y)\right)_{\mathcal{A}}+\lambda_{t+1}\operatorname{sgn}\left(\beta_{t}^ {(\epsilon)}\right)_{\mathcal{A}}\right]\) \(\left(\beta_{t+1}^{(\epsilon)}\right)_{-\mathcal{A}}=\overline{0}\) \(\mathcal{A}=\mathcal{A}\cup\{j\neq\mathcal{A}\mid\nabla l(\beta_{t+1}^{( \epsilon)},(X,y))>\lambda_{t+1}\}\) \(\mathcal{A}=\mathcal{A}\setminus\{j\in\mathcal{A}\mid\left|\beta_{t+1,j}^{( \epsilon)}\right|<\delta\}\) \(t=t+1\) ```

**Algorithm 1** Approximate incremental quadratic algorithm for RLR with \(\ell_{1}\) penalty, [2]

The approximation Algorithm 1 (Algorithm 2) for the solution \(\hat{\beta}(\lambda)\) of RLR under \(\ell_{1}\) (or \(\ell_{2}\)) constraint were first proposed by Rosset [26; 2]. Given a problem instance \(P\), and a sufficiently small step-size \(\epsilon>0\), using Algorithms 1, 2 yields an approximation \(\beta_{(X,y)}^{(\epsilon)}\) of \(\hat{\beta}_{(X,y)}\) that are piece-wise linear functions of \(\lambda\) in total \((\lambda_{\max}-\lambda_{\min})/\epsilon\)[26]. Moreover, it is also guaranteed that the error between \(\beta_{(X,y)}^{(\epsilon)}\) and \(\hat{\beta}_{(X,y)}\) is uniformly upper bounded for all \(\lambda\in[\lambda_{\min},\lambda_{\max}]\).

**Theorem 4.1** (Theorem 1, [2]).: _Given a problem instance \(P\), for small enough \(\epsilon\), there is a uniform bound \(O(\epsilon^{2})\) on the error \(\|\hat{\beta}_{(X,y)}(\lambda)-\beta_{(X,y)}^{(\epsilon)}(\lambda)\|_{2}\) for any \(\lambda\in[\lambda_{\min},\lambda_{\max}]\)._

Denote \(h_{\text{\sf RLR}}^{(\epsilon)}(\lambda,P)=l_{\text{\sf RLR}}(\beta^{\epsilon} (\lambda),(X_{\text{\sf val}},y_{\text{\sf val}}))\) the approximation function of the validation loss \(h_{\text{\sf RLR}}(\lambda,P)\). Using Theorem 4.1 and note that the loss \(f(z)=\log(1+e^{-z})\) is \(1\)-Lipschitz, we can show that the difference between \(h_{\text{\sf RLR}}^{(\epsilon)}(\lambda,P)\) and \(h_{\text{\sf RLR}}(\lambda,P)\) is uniformly upper-bounded.

**Lemma 4.2**.: _The approximation error of the validation loss function is uniformly upper-bounded \(|h_{\text{\sf RLR}}^{(\epsilon)}(\lambda,P)-h_{\text{\sf RLR}}(\lambda,P)|=O( \epsilon^{2})\), for all \(\lambda\in[\lambda_{\min},\lambda_{\max}]\)._

We now present one of our main results, which is the pseudo-dimension bound of the approximate validation loss function class \(\mathcal{H}_{\text{\sf RLR}}^{(\epsilon)}\).

**Theorem 4.3**.: _Consider the RLR under \(\ell_{1}\) (or \(\ell_{2}\)) constraint with parameter \(\lambda\in[\lambda_{\min},\lambda_{\max}]\) that take a problem instance \(P\) drawn from an unknown problem distribution \(\mathcal{D}\) over \(\Pi_{m,p}\). Under Assumptions 1 and 2, \(\mathcal{H}_{\text{\sf RLR}}\) is bounded by some constant \(H\) (Lemma C.2). Suppose that we use Algorithm 1 (or Algorithm 2) to approximate the solution \(\hat{\beta}_{(X,y)}(\lambda)\) by \(\beta_{(X,y)}^{(\epsilon)}(\lambda)\) with a uniform error \(O(\epsilon^{2})\) for any \(\lambda\in[\lambda_{\min},\lambda_{\max}]\), where \(\epsilon\) is the approximation step-size. Consider the approximation validation loss function class \(\mathcal{H}_{\text{\sf RLR}}^{(\epsilon)}=\{h_{\text{\sf RLR}}^{(\epsilon)}( \lambda,\cdot):\Pi_{m,p}\rightarrow\mathbb{R}_{\geq 0}\mid\lambda\in[ \lambda_{\min},\lambda_{\max}]\}\), where_

\[h_{\text{\sf RLR}}^{(\epsilon)}(\lambda,P)=\frac{1}{m^{\prime}}\sum_{i=1}^{m ^{\prime}}\log(1+\exp(-y_{i}x_{i}^{\top}\beta_{(X,y)}^{(\epsilon)}(\lambda)))\]

_is the approximate validation loss. Then we have \(\text{\sf Pdim}(\mathcal{H}_{\text{\sf RLR}}^{(\epsilon)})=O(m^{2}+\log(1/ \epsilon))\). Further, we assume that \(\epsilon=O(\sqrt{H})\) where \(H\) is the upperbound of \(\mathcal{H}_{\text{\sf RLR}}\) under Assumptions 1 and 2. Given any set \(\mathcal{S}\) of \(T\) problem instances drawn from a problem distribution \(\mathcal{D}\) over \(\Pi_{m,p}\), the empirical Rademacher complexity \(\hat{\mathscr{R}}(\mathcal{H}_{\text{\sf RLR}}^{(\epsilon)},\mathcal{S})=O(H \sqrt{(m^{2}+\log(1/\epsilon))/T})\)._

The key observation here is that the approximation solution \(\hat{\beta}_{(X,y)}^{(\epsilon)}\) is piece-wise linear over \((\lambda_{\max}-\lambda_{\min})/\epsilon\) pieces, leading to the fact that the approximate validation loss function \(h_{\text{\sf RLR}}^{(\epsilon)}(\lambda,\cdot)\) is a "special function" (Pfaffian function [43]) in each piece, which is a combination of exponentiation of linear functions of \(\lambda\). The detailed proof of Theorem 4.3 can be found on the Appendix D.3.

### Learning guarantees for Regularized Logistic Regression hyperparameter tuning

Our goal now is to use the upper bound for empirical Rademacher complexity of the validation loss function class \(\mathcal{H}_{\text{\sf RLR}}\). We use techniques for approximate data-driven algorithm design due to [29], combining the uniform error upper bound between validation loss function \(h_{\text{\sf RLR}}(\lambda,P)\) and its approximation \(h_{\text{\sf RLR}}^{(\epsilon)}(\lambda,P)\) (Lemma 4.2) and empirical Rademacher complexity of approximation validation loss function class \(\mathcal{H}_{\text{\sf RLR}}^{(\epsilon)}\) (Theorem 4.3), to obtain a bound on the empirical Rademacher complexity of \(\mathcal{H}_{\text{\sf RLR}}\). This allows us to give a learning guarantee for the regularization parameters \(\lambda\), which is formalized by the following theorem.

**Theorem 4.4**.: _Consider the RLR under \(\ell_{1}\) (or \(\ell_{2}\)) constraint. Under Assumptions 1, 2, \(\mathcal{H}_{\text{\sf RLR}}\) is bounded by some constant \(H\) (Lemma C.2). Consider the class function \(\mathcal{H}_{\text{\sf RLR}}=\{h_{\text{\sf RLR}}(\lambda,\cdot):\Pi_{m,p} \rightarrow\mathbb{R}_{\geq 0}\mid\lambda\in[\lambda_{\min},\lambda_{\max}]\}\) where \(h_{\text{\sf RLR}}(\lambda,P)\) is the validation loss corresponding to probleminstance \(P\) and the \(\ell_{1}\)\((\ell_{2})\) parameter \(\lambda\). Given any set \(\mathcal{S}\) of \(T\) problem instances drawn from a problem distribution \(\mathcal{D}\) over \(\Pi_{m,p}\), for any \(h_{\text{RLR}}(\lambda,\cdot)\in\mathcal{H}_{\text{RLR}}\), w.p. \(1-\delta\) for any \(\delta\in(0,1)\), we have_

\[\left|\frac{1}{T}\sum_{i=1}^{T}h_{\text{RLR}}(\lambda,P^{(i)})-\mathbb{E}_{P \sim\mathcal{D}}[h_{\text{RLR}}(\lambda,P)]\right|\leq O\left(H\sqrt{\frac{m^{2 }+\log(1/\epsilon)}{T}}+\epsilon^{2}+\sqrt{\frac{1}{T}\log\frac{1}{\delta}} \right),\]

_for some sufficiently small \(\epsilon\)._

The proof detail of Theorem 4.4 is included in the Appendix D.3. The above generalization guarantee gives a bound on the average error on RLR validation loss over the problem distribution, for the parameter \(\lambda\) learned from \(T\) problem instances. In commonly used approaches, the validation set size is small or a constant, and our result can be interpreted as the upper bound on the generalization error in terms of the number of problem instances \(T\) and the step length \(\epsilon\). We only consider RLR under \(\ell_{1}\) (or \(\ell_{2}\)) constraints, which are commonly studied in the literature, our analysis could be easily extended to RLR under \(\ell_{q}\) constraint for any \(q\geq 1\).

### An extension to 0-1 loss

Since logistic regression is often used for binary classification tasks, it is interesting to consider the 0-1 loss as the validation loss function. It has been shown that \(\mathbbm{1}\{z\leq 0\}\leq 4\log(1+e^{-z})\) for any \(z\)[44]. This inequality, combined with Theorem 4.4, directly provides a learning guarantee for the 0-1 validation loss function.

**Theorem 4.5**.: _Let \(\tau>2\epsilon^{2}\) and \(\delta\in(0,1)\), where \(\epsilon\) is the approximation step-size. Then for any \(n\geq s(\tau/2,\delta)=\Omega\left(\frac{H^{2}(m^{2}+\log\frac{1}{\epsilon})+ \log\frac{1}{\delta}}{(\tau/2-\epsilon^{2})^{2}}\right)\), if we have \(n\) problem instances \(\{P^{(i)},\ldots,P^{(n)}\}\) drawn i.i.d. from some problem distribution \(\mathcal{D}\) over \(\Pi_{m,p}\) to learn the regularization parameter \(\lambda^{ERM}\) for RLR via ERM, then_

\[\mathbb{E}_{P\sim\mathcal{D}}(h_{\text{RLR}}^{0:1}(\lambda^{ERM},P)))\leq 4 \min_{\lambda\in[\lambda_{\min},\lambda_{\max}]}\mathbb{E}_{P\sim\mathcal{D}} (h_{\text{RLR}}(\lambda,P))+4\tau.\]

The detailed proof of Theorem 4.5 can be found on Appendix D.4. It is worth noting that we are providing learning guarantee for 0-1 validation loss function class \(\mathcal{H}_{\text{RLR}}^{0:1}\) indirectly via the validation loss function class \(\mathcal{H}_{\text{RLR}}\) with cross-entropy objective function, which is arguably not optimal. The question of how to provide a true PAC-learnable guarantee for \(\mathcal{H}_{\text{RLR}}^{0:1}\) remains an interesting challenge.

## 5 Conclusion and future work

In this work, we present novel learning guarantees for tuning regularization parameters for both the Elastic Net and Regularized Logistic Regression models, across problem instances. For the Elastic Net, we propose fine-grained structural results that pertain to the tuning of regularization parameters. We use them to give an improved upper bound on the pseudo-dimension of the relevant validation loss function class of and we prove that our new bound is tight.

For the problem of tuning regularization parameters in regularized logistic regression, we propose an alternative approach that involves analyzing the approximation of the original validation loss function class. This approximation, characterized by a piece-wise linear representation, provides a useful analytical tool in the absence of an exact dependence of the logistic loss on the regularization parameters. Additionally, we employ an upper bound on the approximation error between the original and approximated functions, to obtain a learning guarantee for the original validation loss function class. Remarkably, our proposed approach is not restricted solely to regularized logistic regression but can be extended to a wide range of other problems, demonstrating its generality and applicability.

It is worth noting that this work only focuses on the sample complexity aspect of the hyperparameter tuning in the Elastic Net and Regularized Logistic Regression. The question of computational complexity in this setting is an interesting future direction. Other interesting questions include designing hyperparameter tuning techniques for this setting that are robust to adversarial attacks, and hyperparameter tuning for Regularized Logistic Regression with both \(\ell_{1}\) and \(\ell_{2}\) constraints.

Acknowledgement

We thank Yingyu Liang for useful discussions during early stages of this work and Mikhail Khodak for helpful feedback. This work was supported in part by NSF grants CCF-1910321 and SES-1919453, the Defense Advanced Research Projects Agency under cooperative agreement HR00112020003, and a Simons Investigator Award.

## References

* [1] Hui Zou and Trevor Hastie. Regularization and variable selection via the Elastic Net. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 67(2):301-320, 2005.
* [2] Saharon Rosset. Following curved regularized optimization solution paths. _Advances in Neural Information Processing Systems_, 17, 2004.
* [3] Volker Roth. The generalized LASSO. _IEEE transactions on neural networks_, 15(1):16-28, 2004.
* [4] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. _The elements of statistical learning: data mining, inference, and prediction_, volume 2. Springer, 2009.
* [5] Arthur Hoerl and Robert Kennard. Ridge regression: Biased estimation for nonorthogonal problems. _Technometrics_, 12(1):55-67, 1970.
* [6] Robert Tibshirani. Regression shrinkage and selection via the LASSO. _Journal of the Royal Statistical Society: Series B (Methodological)_, 58(1):267-288, 1996.
* [7] Martin J Wainwright. _High-dimensional statistics: A non-asymptotic viewpoint_, volume 48. Cambridge University Press, 2019.
* [8] Edgar Dobriban and Stefan Wager. High-dimensional asymptotics of prediction: Ridge regression and classification. _The Annals of Statistics_, 46(1):247-279, 2018.
* [9] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. _Foundations of machine learning_. MIT press, 2018.
* [10] Gang Wang, Dit-Yan Yeung, and Frederick H Lochovsky. The kernel path in kernelized LASSO. In _Artificial Intelligence and Statistics_, pages 580-587. PMLR, 2007.
* [11] Yunlong Feng, Shao-Gao Lv, Hanyuan Hang, and Johan Suykens. Kernelized Elastic Net regularization: generalization bounds, and sparse recovery. _Neural computation_, 28(3):525-562, 2016.
* [12] Kevin P Murphy. _Machine learning: a probabilistic perspective_. MIT press, 2012.
* [13] Tong Zhang. Some sharp performance bounds for least squares regression with L1 regularization. _The Annals of Statistics_, pages 2109-2143, 2009.
* [14] Maria-Florina Balcan, Mikhail Khodak, Dravyansh Sharma, and Ameet Talwalkar. Provably tuning the Elastic Net across instances. In _Advances in Neural Information Processing Systems_, 2022.
* [15] Rich Caruana. Multitask learning. _Machine learning_, 28:41-75, 1997.
* [16] Mervyn Stone. Cross-validation: A review. _Statistics: A Journal of Theoretical and Applied Statistics_, 9(1):127-139, 1978.
* [17] Paul Goldberg and Mark Jerrum. Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers. In _Proceedings of the sixth annual conference on Computational learning theory_, pages 361-369, 1993.
* [18] Li Wang, Michael D Gordon, and Ji Zhu. Regularized least absolute deviations regression and an efficient algorithm for parameter tuning. In _Sixth International Conference on Data Mining (ICDM'06)_, pages 690-700. IEEE, 2006.

* [19] Denny Wu and Ji Xu. On the optimal weighted L2 regularization in overparameterized linear regression. _Advances in Neural Information Processing Systems_, 33:10112-10123, 2020.
* [20] Diane Galarneau Gibbons. A simulation study of some ridge estimators. _Journal of the American Statistical Association_, 76(373):131-139, 1981.
* [21] Lisa-Ann Kirkland, Frans Kanfer, and Sollie Millard. LASSO tuning parameter selection. In _Annual Proceedings of the South African Statistical Association Conference_, volume 2015, pages 49-56. South African Statistical Association (SASA), 2015.
* [22] Yosiyuki Sakamoto, Makio Ishiguro, and Genshiro Kitagawa. Akaike information criterion statistics. _Dordrecht, The Netherlands: D. Reidel_, 81(10.5555):26853, 1986.
* [23] Gideon Schwarz. Estimating the dimension of a model. _The annals of statistics_, pages 461-464, 1978.
* [24] Michael Chichignoud, Johannes Lederer, and Martin Wainwright. A practical scheme and fast algorithm to tune the LASSO with optimality guarantees. _The Journal of Machine Learning Research_, 17(1):8162-8181, 2016.
* [25] Denis Chetverikov, Zhipeng Liao, and Victor Chernozhukov. On cross-validated LASSO in high dimensions. _The Annals of Statistics_, 49(3):1300-1317, 2021.
* [26] Saharon Rosset. _Topics in regularization and boosting_. PhD thesis, Stanford University, 2003.
* [27] Ryan Tibshirani. The LASSO problem and uniqueness. _Electronic Journal of Statistics_, 7:1456-1490, 2013.
* [28] Rishi Gupta and Tim Roughgarden. A PAC approach to application-specific algorithm selection. In _Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science_, pages 123-134, 2016.
* [29] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. Refined bounds for algorithm configuration: The knife-edge of dual class approximability. In _International Conference on Machine Learning_, pages 580-590. PMLR, 2020.
* [30] Maria-Florina Balcan, Siddharth Prasad, Tuomas Sandholm, and Ellen Vitercik. Structural analysis of branch-and-cut and the learnability of gomory mixed integer cuts. _Advances in Neural Information Processing Systems_, 35:33890-33903, 2022.
* [31] Peter Bartlett, Piotr Indyk, and Tal Wagner. Generalization bounds for data-driven numerical linear algebra. In _Conference on Learning Theory_, pages 2013-2040. PMLR, 2022.
* [32] Maria-Florina Balcan, Dan DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm, and Ellen Vitercik. How much data is sufficient to learn high-performing algorithms? Generalization guarantees for data-driven algorithm design. In _Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing_, pages 919-932, 2021.
* [33] Martin Anthony and Peter Bartlett. _Neural Network Learning: Theoretical Foundations_. Cambridge University Press, USA, 1st edition, 2009.
* [34] Maria-Florina Balcan. Book chapter Data-Driven Algorithm Design. In _Beyond Worst Case Analysis of Algorithms, T. Roughgarden (Ed)_. Cambridge University Press, 2020.
* [35] Julien Mairal and Bin Yu. Complexity analysis of the LASSO regularization path. In _Proceedings of the 29th International Coference on International Conference on Machine Learning_, pages 1835-1842, 2012.
* [36] Bernhard Scholkopf, Ralf Herbrich, and Alex J Smola. A generalized representer theorem. In _Computational Learning Theory: 14th Annual Conference on Computational Learning Theory, COLT 2001 and 5th European Conference on Computational Learning Theory, EuroCOLT 2001 Amsterdam, The Netherlands, July 16-19, 2001 Proceedings 14_, pages 416-426. Springer, 2001.

* [37] Martin Wainwright. Information-theoretic bounds on sparsity recovery in the high-dimensional and noisy setting. In _2007 IEEE International Symposium on Information Theory_, pages 961-965. IEEE, 2007.
* [38] Martin Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using L1-constrained quadratic programming (LASSO). _IEEE transactions on information theory_, 55(5):2183-2202, 2009.
* [39] Peter Armitage, Geoffrey Berry, and John Nigel Scott Matthews. _Statistical methods in medical research_. John Wiley & Sons, 2008.
* [40] Michael JA Berry and Gordon S Linoff. _Data mining techniques: for marketing, sales, and customer relationship management_. John Wiley & Sons, 2004.
* [41] Sanjay Kumar Palei and Samir Kumar Das. Logistic regression model for prediction of roof fall risks in bord and pillar workings in coal mines: an approach. _Safety science_, 47(1):88-96, 2009.
* [42] David W Hosmer Jr, Stanley Lemeshow, and Rodney X Sturdivant. _Applied logistic regression_, volume 398. John Wiley & Sons, 2013.
* [43] Askold G Khovanski. _Fewnomials_, volume 88. American Mathematical Soc., 1991.
* [44] Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep neural networks. _Advances in neural information processing systems_, 32, 2019.
* [45] Richard M Dudley. Universal, Donsker classes and metric entropy. _The Annals of Probability_, 15(4):1306-1326, 1987.