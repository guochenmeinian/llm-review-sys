# TabMT: Generating Tabular data with Masked Transformers

Manbir S. Gulati

AI Accelerator

Leidos Inc

Manbir.S.Gulati@leidos.com

Paul F. Roysdon

AI Accelerator

Leidos Inc

Paul.Roysdon@leidos.com

###### Abstract

Autoregressive and Masked Transformers are incredibly effective as generative models and classifiers. While these models are most prevalent in NLP, they also exhibit strong performance in other domains, such as vision. This work contributes to the exploration of transformer-based models in synthetic data generation for diverse application domains. In this paper, we present TabMT, a novel Masked Transformer design for generating synthetic tabular data. TabMT effectively addresses the unique challenges posed by heterogeneous data fields and is natively able to handle missing data. Our design leverages improved masking techniques to allow for generation and demonstrates state-of-the-art performance from extremely small to extremely large tabular datasets. We evaluate TabMT for privacy-focused applications and find that it is able to generate high quality data with superior privacy tradeoffs.

## 1 Introduction

Generative models have attracted significant attention in the field of deep learning due to their ability to synthesize high-quality data and learn the underlying structure of complex datasets. Such models have been successfully applied to various data types including images [18], text[8], and tabular data [12]. This work concentrates on tabular data, which is prevalent in numerous fields like healthcare, finance, and social sciences. The heterogeneous nature of tabular data, characterized by its diverse data types, distributions, and relationships, presents distinct challenges not present in other domains.

The development of effective synthetic tabular data generators is crucial for numerous reasons including: privacy preservation, data augmentation, model interpretability, and anomaly detection. Prior work in this domain has produced a myriad of generative models, including Generative Adversarial Networks (GANs)[28][25], Variational Autoencoders (VAEs)[25], Autoregressive Transformer[22][1], and Diffusion models[12]. Although these existing models strive to address the challenges associated with tabular data generation, there is still room for exploration and improvement. Specifically, we demonstrate improvements in robustness, scalability, privacy preservation, and handling of missing data.

Transformers [23], originally designed for natural language processing (NLP) tasks, have lead to significant advancements in a variety of applications. Their powerful capacity for modeling complex dependencies and

Figure 1: Diagram of TabMT. \(m\) is the mask token, \(p_{i}\) is the masking probability of the \(i^{th}\) row

[MISSING_PAGE_FAIL:2]

3. Missing data is far more common in tabular data than in other domains. TabMT is able to learn missing values by setting their masking probability to 1. Other generators require that we impute data separately before we can generate high-quality cleaned samples.

These structural advantages come from TabMT's novel masked generation procedure.

Below we outline how we construct TabMT from the original masked training procedure outlined in BERT[5] and the justifications behind our design choices. We also outline the specific changes we make to allow for heterogeneous data types. A naive adaptation of BERT's masking procedure would look as follows. Given an \(n\) by \(l\) dataset \(\mathbf{F}\) of categorical and numerical features, for each row \(\mathbf{F}_{i}\), the transformer is provided with a set of unmasked fields \(\mathbf{F}_{i}^{u}\) and a set of masked fields \(\mathbf{F}_{i}^{m}\). Each field in the masked set \(\mathbf{F}_{i}^{m}\) has its value replaced with a mask token. The model is then tasked with predicting the original value for all masked tokens. The row \(\mathbf{F}_{i}\) is partitioned into the unmasked and masked sets by conducting a Bernoulli trial on each field, \(\mathbf{F}_{i,j}\), such that \(P(\mathbf{F}_{i,j}\in\mathbf{F}_{i}^{m})=0.15\).

The BERT masking procedure produces a strong embedding model, but not a strong generator. To create a strong generative model we make two key changes: sample our masking probability from a uniform distribution and predict masked values in a random order during generation. To understand why these changes are effective, we can look at the distribution of masked sets. As a result of the repeated Bernoulli trials during masking, the size of the masked set for each row \(|\mathbf{F}_{i}^{m}|\) will follow a Binomial distribution. However, when generating data one field \(\mathbf{F}_{i,j}\) at a time, the model will inference on masked subset sizes \((0,\dots,l-1)\), once each. We would like the training distribution of \(|\mathbf{F}_{i}^{m}|\) to be uniform, matching the uniform distribution encountered when generating data. With a fixed masking probability \(P(\mathbf{F}_{i,j}\in\mathbf{F}_{i}^{m})=p_{m}\) we will instead encounter a Binomial distribution centered around \(p_{m}\cdot l\). However, if we sample our masking probability \(p_{m}\) for each row \(\mathbf{F}_{i}\) such that \(P(p_{m}=p)\sim U(0,1)\), we will train uniformly across subset sizes:

\[P(|\mathbf{F}_{i}^{m}|=k)=\int_{0}^{1}{l\choose k}p^{k}(1-p)^{l-k}dp=\frac{l!} {k!(l-k)!}\frac{k!(l-k)!}{(l+1)!}=\frac{1}{l+1}.\] (1)

Fixing this train and inference mismatch is critical to forming a strong generator. A traditional autoregressive generator would generate fields \((\mathbf{F}_{i,0},\dots,\mathbf{F}_{i,l-1})\), sequentially. However, tabular data, unlike language, does not have an inherent ordering. Generating fields in a fixed order introduces another mismatch between training and inference. During training \(\mathbf{F}_{i}^{m}\) will take on the distribution

\[P(\mathbf{F}_{i}^{m}=s)=\frac{1}{{l\choose{|s|}}\cdot l}.\] (2)

When generating in a fixed order, the model will infer across \(l\) distinct subsets and no others. However, if we instead infer in a random order, then at generation step \(0\leq t<l\), the distribution of \(\mathbf{F}_{i}^{m}\) will be given by

\[P(\mathbf{F}_{i}^{m}=s)=\frac{t!\cdot(l-t)!}{l!}=\frac{1}{{l\choose t}}.\] (3)

Since we encounter each \(t\) exactly once, this overall distribution is identical to the masking distribution encountered during training, fixing the discrepancy caused by generating fields in a fixed order.

A transformer model will typically have an input embedding matrix \(\mathbf{E}\in\mathbb{R}^{k\times d}\), where \(k\) is the number of unique input tokens and \(d\) is the transformer width. Because tabular data is heterogeneous, we instead construct \(l\) embedding matrices, one for each field. Each embedding matrix will have a different number of unique tokens \(k\).

Figure 2: A row of data being sampled from TabMT. Fields are sampled in a random order and field values are sampled according to the predicted distributed.

For categorical fields we use a standard embedding matrix initialized with a normal distribution. For each continuous field we construct an ordered embedding \(\mathbf{O}\in\mathbb{R}^{k\times d}\) from its unordered embedding matrix \(\mathbf{E}\) and two endpoint vectors \(\mathbf{l},\mathbf{h}\in\mathbb{R}^{d}\).

To construct each ordered embedding matrix \(\mathbf{O}\), we first we quantize the values of the continuous field. Our default quantizer is K-Means. We consider the maximum number of clusters a hyper-parameter. Let \(\mathbf{v}\in\mathbb{R}^{k}\) be the vector of ordered cluster centers. We construct an vector of ratios \(\mathbf{r}\in\mathbb{R}^{k}\) using min max normalization such that

\[\mathbf{r}_{i}=\frac{\mathbf{v}_{i}-\min(\mathbf{v})}{\max(\mathbf{v})-\min( \mathbf{v})}.\] (4)

We use the ratio vector \(\mathbf{r}\) to construct each ordered embedding in \(\mathbf{O}\):

\[\mathbf{O}_{i}=\mathbf{E}_{i}+\mathbf{r}_{i}\cdot\mathbf{l}+(1-\mathbf{r}_{i} )\cdot\mathbf{h}.\] (5)

This structure allows the transformer to both take advantage of the ordering of the properties and add unordered embedding information to each cluster. The unordered embeddings are useful in attention, multi-modal distributions, and encoding semantic separation between close values. We use this same structure to construct a dynamic linear layer at the output during prediction. This can be converted to a traditional linear layer once the model is trained.

Relying too heavily on the unordered embeddings might negate the benefit of our ordered embedding, as information isn't effectively shared between close values. To address this, we bias TabMT to rely on the ordering of embeddings. For continuous fields, we zero-init the unordered embedding matrix \(\mathbf{E}\). Whereas, the endpoint vectors \(\mathbf{l}\) and \(\mathbf{h}\) use a normal distribution with 0.05 standard deviation. Because entries in matrix \(\mathbf{O}\) are not independent of each other, to sharpen the output distribution, the network must either rely on matrix \(\mathbf{E}\) or increase magnitude of the endpoint vectors. This can reduce use of the priors encoded by \(\mathbf{O}\) or cause instability. To combat this, we include a learned temperature which can sharpen the predicted distribution using a single parameter per field instead. Each field's predicted distribution \(\hat{\mathbf{y}}\in\mathbb{R}^{k}\) is given by

\[\hat{\mathbf{y}}=\frac{e^{\mathbf{z}/(\tau_{l}\cdot\tau_{u})}}{\sum_{j}e^{ \mathbf{z}/(\tau_{l}\cdot\tau_{u})}},\] (6)

where \(\mathbf{z}\in\mathbb{R}^{k}\) is a vector of logits, \(\tau_{l}\) is the learned temperature, and \(\tau_{u}\) is the user-defined temperature. See Figure 1 for an overall diagram of these components. Figure 2 shows the generation of a single sample. Detailed pseudocode is available in the Appendix.

## 4 Evaluation

In this section, we present a comprehensive evaluation of TabMT's effectiveness across an extensive range of tabular datasets. Our analysis involves a thorough comparison with state-of-the-art approaches, encompassing nearly all generative model families. To ensure a robust assessment, we evaluate across several dimensions and metrics.

Datasets:For our data quality and privacy experiments we use the same list of datasets and data splits as TabDDPM[12]. These 15 datasets range in size from \(\sim 400\) samples to \(\sim 150,000\) samples. They contain continuous, categorical, and integer features. The datasets range from 6 to 50 columns. For our scaling experiments we use the CIDDS-001[20] dataset, which consists of Netflow traffic from a simulated small business network. A Netflow consists of 12 attributes, which we post-process into 16 attributes; see the Appendix for more details. This dataset is extremely large with over 30 million rows and field cardinalities in the tens of thousands. Other datasets listed all have cardinalities below 50. Unlike our other benchmarks, we purposely do not quantize the continuous variables here to further test the scaling of our model. In other words, every unique value is treated as a separate category in our prediction process.

Prior Methods:We select four techniques to compare against, one from each each major family of deep generative models.

* **TVAE[25]** is one of the first deep tabular generation papers introducing two models, a GAN and a VAE. We compare against their VAE because it is the strongest VAE tabular generator we are aware of.

[MISSING_PAGE_FAIL:5]

### Privacy and Sample Novelty

Maintaining privacy of the original data is a key application for synthetic tabular data. Machine learning is increasingly being used across a wide range of areas to produce valuable insights. Simultaneously, there is a rapid rise in both regulation and privacy concerns that need to be addressed. In Section 4.1 we demonstrated that data produced by TabMT is high enough quality to produce strong classifiers. Now we evaluate our model for privacy. This evaluation complements our quality evaluation and verifies that our model is generating novel data. Novelty means data is not substantially similar to samples encountered during training. A high-quality non-private model can trivially be formed by directly reproducing the training set exactly. None of this data is novel, but it is high quality. By ensuring our model is both private and high quality, we verify that our model has learned the distribution of the data, and not simply memorized the training set. Memorization is a larger issue in tabular data due to smaller dataset sizes and increased privacy concerns.

To evaluate privacy and novelty we adopt the median Distance to the Closest Record (DCR) score. To calculate the DCR of a synthetic sample, we find the nearest neighbor in the real training set by Euclidean distance. We report the median of this distance across our synthetic samples. Data with higher DCR scores will be more private and more novel. There is an inherent trade off between privacy and quality. Higher quality samples will tend to be closer to points in the training set and vice versa.

While models such as CTabGAN+[28] and TabDDPM[12] have a fixed trade-off between privacy and quality after training. TabMT can trade-off between quality and privacy using temperature scaling. By walking along the Pareto curve of our model, using temperature scaling, we can controllably tune the privacy and novelty of our generated data per application. By increasing a field's temperature, its generated values become more novel and private, but they are also less faithful to the underlying data distribution. The trade off between the quality and privacy here form a Pareto front for TabMT on each dataset.

We use a separate temperature for each field and perform a search to estimate the Pareto front. Each search was conducted using a single A10 GPU each. Search details are available in the Appendix. In Table 2, we compare TabMT's DCR and corresponding MLE scores to that of TabDDPM. We are always able to attain a higher DCR score, and in most cases a higher MLE score as well. This falls in line with recent results in other domains showing diffusion models are less private than other generative models[2]. A comparison with CTabGAN+ is available in the Appendix, compared to CTabGAN+ we obtain both higher privacy and MLE scores in all tested cases. Figure 4 shows the Pareto fronts of TabMT across several datasets.

### Missing Data

Real world data is often missing many values that make training difficult. When a row has a missing value we must either drop the row, or find a method to impute the missing value. Other techniques such as the RealTabformer[22] or TabDDPM[12] cannot natively handle real world missing data, and must either use a different imputation technique or drop the corresponding rows. Our masking procedure allows TabMT to natively handle arbitrary missing data. To demonstrate this, we randomly drop 25% of values from the dataset, ensuring nearly every row is permanently missing data. Nevertheless, our

Figure 3: A comparison of Correlation Error Histograms between TabMT (Green) vs TabDDPM (Purple) and TabMT (Green) vs CTabGAN+ (Purple). A good generator should have correlation errors distributed close to zero (the left of the plot). We can see TabMTâ€™s correlation errors are consistently distributed closer to zero than either TabDDPM or CTabGAN+.

model is still able to train, producing synthetic rows with no missing values in them. This facilitates training on real world data.Table 3 shows our accuracy when training with missing data.

Additionally, our model can be arbitrarily conditioned to produce any subset of the data distribution at no additional cost, allowing us to more effectively augment underrepresented portions of data. Prior art is largely incapable of conditioning when producing outputs.

### Scaling

In this section, we examine the performance of our model when scaling to very large datasets. We use the CIDDS-001 dataset[20] as our benchmark dataset. We do not use anomalous traffic from the dataset, and randomly select 5% of the dataset as the validation set for reporting results. The results

\begin{table}
\begin{tabular}{c c c} \hline \hline DS & TabDDPM & **TabMT** \\ \hline AB & 0.050(0.550) & **0.249**(0.533) \\ AD & 0.104(0.795) & **1.01**(0.811) \\ BU & 0.143(0.906) & **0.165**(0.908) \\ CA & 0.041(0.836) & **0.117**(0.832) \\ CAR & 0.012(0.737) & **0.041**(0.737) \\ CH & 0.157(0.755) & **0.281**(0.758) \\ DI & 0.204(0.740) & **0.243**(0.740) \\ FB & 0.112(0.713) & **0.252**(0.787) \\ \hline \hline \end{tabular} 
\begin{tabular}{c c c} \hline \hline DS & TabDDPM & **TabMT** \\ \hline GE & 0.059(0.597) & **0.234**(0.599) \\ HI & 0.449(0.722) & **0.483**(0.727) \\ HO & 0.086(0.677) & **0.151**(0.607) \\ IN & 0.041(0.809) & **0.061**(0.816) \\ KI & 0.189(0.833) & **0.335**(0.868) \\ MI & 0.022(0.936) & **0.026**(0.936) \\ WI & 0.016(0.904) & **0.063**(0.881) \\ \hline \hline \end{tabular}
\end{table}
Table 2: DCR score comparison between TabDDPM and TabMT. Corresponding MLE scores are in parentheses.

Figure 4: Pareto Fronts of TabMT balancing the tradeoff between privacy (DCR) and data quality (Validation Score). While some datasets have a smooth transition the temperature changes, others have a sharp drop-off.

[MISSING_PAGE_FAIL:8]

Broader ImpactSynthetic data generation allows for privacy preservation, protecting sensitive data while still enabling data analysis. High Quality synthetic data may ease the pressure to resort to unethical methods of collection such as relying on underpaid labor. With this in mind, trading off data for additional compute does mean that the additional compute will contribute to increased CO2 emissions. Additionally, synthetic data carries the risk of misuse, such as the potential for manipulating results or research findings with fabricated data. All experiments were conducted using cloud A10 or V100 GPUs. For algorithm design and experiment result generation roughly 410 GPU days of compute were used.

## 6 Conclusion

In this paper, we outlined a novel Masked Transformer design and training procedure, TabMT, for generating synthetic tabular data. Through a comprehensive series of benchmarks we demonstrate that our model achieves state-of-the-art generation quality. This quality is verified at scales that are orders of magnitude larger than prior work and with missing data present. Our model achieves superior privacy and is able to easily trade off between privacy and quality. Our model is a substantial advancement compared to previous work, due to its scalability, missing data robustness, privacy-preserving generation, and superior data quality.

## References

* [1]V. Borisov, K. Sessler, T. Leemann, M. Pawelczyk, and G. Kasneci (2022) Language models are realistic tabular data generators. arXiv preprint arXiv:2210.06280. Cited by: SS1.
* [2]N. Carlini, J. Hayes, M. Nasr, M. Jagielski, V. Sehwag, F. Tramer, B. Balle, D. Ippolito, and E. Wallace (2023) Extracting training data from diffusion models. arXiv preprint arXiv:2301.13188. Cited by: SS1.
* [3]H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman (2022) Maskgit: masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11315-11325. Cited by: SS1.
* [4]M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever (2020) Generative pretraining from pixels. In International conference on machine learning, pp. 1691-1703. Cited by: SS1.
* [5]J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018) Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Cited by: SS1.
* [6]A. Veronika Dorogush, V. Ershov, and A. Gulin (2018) Catboost: gradient boosting with categorical features support. arXiv preprint arXiv:1810.11363. Cited by: SS1.
* [7]J. Engelmann and S. Lessmann (2021) Conditional wasserstein gan-based oversampling of tabular data for imbalanced learning. Expert Systems with Applications174, pp. 114582. Cited by: SS1.
* [8]L. Floridi and M. Chiriatti (2020) Gpt-3: its nature, scope, limits, and consequences. Minds and Machines30, pp. 681-694. Cited by: SS1.
*
\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & NFGAN & TabMT-S & TabMT-M & TabMT-L \\ \hline TCP Flags & 2.33e-03 & 4.63e-04 & 2.16e-04 & **6.54e-06** \\ Private IPs & 2.00e-04 & 7.25e-05 & 2.67e-05 & **1.14e-05** \\ TCP Port & 3.00e-04 & 9.32e-05 & 3.94e-05 & **1.47e-05** \\ DNS & 1.60e-03 & 2.34e-03 & 1.56e-03 & **2.05e-04** \\ Valid Values* & 2.00e-03 & **0.00e-00** & **0.00e-00** & **0.00e-00** \\ NetBios & 7.43e-01 & 4.73e-03 & 4.64e-03 & **1.27e-03** \\ Packet Ratios & 5.10e-03 & 2.28e-03 & 1.79e-03 & **1.16e-03** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Error rates on netflow invariant tests. *: because we construct embeddings per field, our model cannot violate check 5. These tests check structural rules reflected in Netflow, such as the fact that two public ip addresses cannot communicate to each other.

* He et al. [2022] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16000-16009, 2022.
* Huang et al. [2022] Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Wojciech Galuba, Florian Metze, and Christoph Feichtenhofer. Masked autoencoders that listen. _Advances in Neural Information Processing Systems_, 35:28708-28720, 2022.
* Jiang et al. [2021] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan: Two pure transformers can make one strong gan, and that can scale up. _Advances in Neural Information Processing Systems_, 34:14745-14758, 2021.
* Kotelnikov et al. [2022] Akim Kotelnikov, Dmitry Baranchuk, Ivan Rubachev, and Artem Babenko. Tabddpm: Modelling tabular data with diffusion models. _arXiv preprint arXiv:2209.15421_, 2022.
* Kynkaanniemi et al. [2019] Tuomas Kynkaanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. _Advances in Neural Information Processing Systems_, 32, 2019.
* Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019.
* Manocchio et al. [2021] Liam Daly Manocchio, Siamak Layeghy, and Marius Portmann. Flowgan-synthetic network flow generation using generative adversarial networks. In _2021 IEEE 24th International Conference on Computational Science and Engineering (CSE)_, pages 168-176. IEEE, 2021.
* Padhi et al. [2021] Inkit Padhi, Yair Schiff, Igor Melnyk, Mattia Rigotti, Youssef Mroueh, Pierre Dognin, Jerret Ross, Ravi Nair, and Erik Altman. Tabular transformers for modeling multivariate time series. In _ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 3565-3569. IEEE, 2021.
* Radford et al. [2022] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision. _arXiv preprint arXiv:2212.04356_, 2022.
* Ramesh et al. [2021] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In _International Conference on Machine Learning_, pages 8821-8831. PMLR, 2021.
* Ring et al. [2017] Markus Ring, Alexander Dallmann, Dieter Landes, and Andreas Hotho. Ip2vec: Learning similarities between ip addresses. In _2017 IEEE International Conference on Data Mining Workshops (ICDMW)_, pages 657-666, 2017. doi: 10.1109/ICDMW.2017.93.
* Ring et al. [2017] Markus Ring, Sarah Wunderlich, Dominik Grudl, Dieter Landes, and Andreas Hotho. Flow-based benchmark data sets for intrusion detection. In _Proceedings of the 16th European conference on cyber warfare and security. ACPI_, pages 361-369, 2017.
* Ring et al. [2019] Markus Ring, Daniel Schlor, Dieter Landes, and Andreas Hotho. Flow-based network traffic generation using generative adversarial networks. _Computers & Security_, 82:156-172, 2019.
* Solatorio and Dupriez [2023] Aivin V Solatorio and Olivier Dupriez. Realtabformer: Generating realistic relational and tabular data using transformers. _arXiv preprint arXiv:2302.02041_, 2023.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. _Advances in neural information processing systems_, 30, 2017.
* Wolf et al. [2021] Maximilian Wolf, Markus Ring, and Dieter Landes. Impact of generative adversarial networks on netflow-based traffic classification. In _13th International Conference on Computational Intelligence in Security for Information Systems (CISIS 2020) 12_, pages 393-404. Springer, 2021.
* Xu et al. [2019] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Modeling tabular data using conditional gan. _Advances in Neural Information Processing Systems_, 32, 2019.
* Yu et al. [2022] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. _arXiv preprint arXiv:2206.10789_, 2022.
* Zhao et al. [2021] Zilong Zhao, Aditya Kumar, Hiek Van der Scheer, Robert Birke, and Lydia Y. Chen. CTAB-GAN: effective table data synthesizing. _CoRR_, abs/2102.08369, 2021. URL https://arxiv.org/abs/2102.08369.
* Zhao et al. [2022] Zilong Zhao, Aditya Kumar, Robert Birke, and Lydia Y Chen. Ctab-gan+: Enhancing tabular data synthesis. _arXiv preprint arXiv:2204.00401_, 2022.