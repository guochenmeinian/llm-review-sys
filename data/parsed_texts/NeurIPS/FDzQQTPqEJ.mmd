[MISSING_PAGE_EMPTY:1]

Contributions.Let \((\Omega,\mathcal{F},\mu)\) denote a measure space with \(\Omega\subseteq\mathbb{R}^{d}\), sigma algebra \(\mathcal{F}\), and nonnegative measure \(\mu\). Let \(\bm{V}\in\mathbb{R}^{m\times n}\) be the readout parameters of a neural network and let \(\bm{W}\in\mathbb{R}^{n\times\bar{D}}\) and \(\bm{b}\in\mathbb{R}^{n}\) be the weights and biases of a hidden layer of a neural network \(\bm{f}:\mathbb{R}^{D}\to\mathbb{R}^{m}\) with activation function \(\sigma\). For some support \(\mathbb{X}\in\mathcal{F}\), define a probability distribution \(P\) (and corresponding probability density \(p\) with respect to base measure \(\mu\)) to be proportional to squared \(2\)-norm of the evaluation of the neural network \(\bm{f}\),

\[P(d\bm{x};\bm{V},\bm{\Theta})\triangleq\frac{\mu(d\bm{x})}{\mathrm{z}(\bm{V}, \bm{\Theta})}\big{\|}\bm{f}\left(\bm{t}(\bm{x});\bm{V},\bm{\Theta}\right)\big{\|} _{2}^{2},\quad\bm{f}(\bm{t};\bm{V},\bm{\Theta})=\bm{V}\sigma(\bm{W}\bm{t}+\bm{b }),\,\bm{\Theta}=(\bm{W},\bm{b}),\] (1)

whenever the normalising constant \(\mathrm{z}(\bm{V},\bm{\Theta})\triangleq\int_{\mathbb{X}}\big{\|}\bm{f}\left( \bm{t}(\bm{x});\bm{V},\bm{\Theta}\right)\big{\|}_{2}^{2}\mu(d\bm{x})\) is finite and non-zero. Here we call \(\mu\) the base measure, \(\bm{t}:\mathbb{X}\to\mathbb{R}^{D}\) the sufficient statistic2 and \(\sigma\) the activation function. We will call the corresponding family of probability distributions, parametrised by \((\bm{V},\bm{\Theta})\), a _squared neural family_ (SNEFY) on \(\mathbb{X}\), and denote it by SNEFY\({}_{\mathbb{X},\bm{t},\sigma,\mu}\). SNEFYs are new flexible probability distribution models that strike a good balance between mathematical tractability, expressivity and computational efficiency. When a random vector \(\mathbf{x}\) follows a SNEFY distribution indexed by parameters \((\bm{V},\bm{\Theta})\), we write \(\mathbf{x}\sim\texttt{SNEFY}_{\mathbb{X},\bm{t},\sigma,\mu}(\bm{V},\bm{ \Theta})\) or simply \(\mathbf{x}\sim P(:\bm{V},\bm{\Theta})\), where there is no ambiguity.

Footnote 2: We later verify that \(\bm{t}\) is indeed a sufficient statistic, see (7). Note \(\bm{t}\) maps from \(d\) to \(D\) dimensions.

Our main technical challenge is in exactly computing the normalising constant, \(\mathrm{z}(\bm{V},\bm{\Theta})\), where

\[\mathrm{z}(\bm{V},\bm{\Theta})\triangleq\int_{\mathbb{X}}\big{\|}\bm{f}\left( \bm{t}(\bm{x});\bm{V},\bm{\Theta}\right)\big{\|}_{2}^{2}\mu(d\bm{x}),\quad \bm{f}(\bm{t};\bm{V},\bm{\Theta})=\bm{V}\sigma(\bm{W}\bm{t}+\bm{b}),\quad\bm{ \Theta}=(\bm{W},\bm{b}).\] (2)

The normalising constants we consider are special cases in the sense that they apply to specific (but commonly appearing in applications) choices of activation function \(\sigma\), sufficient statistic \(t\) and base measure \(\mu\) over support \(\mathbb{X}\). See Table 1. Our analysis both exploits and informs a connection with

\begin{table}
\begin{tabular}{c|c|c|c|c|c} Support & Base & Sufficient & Activation & & Kernel \\ \(\mathbb{X}\) & measure \(\mu\) & statistic \(\bm{t}(\bm{x})\) & function \(\sigma\) & \(\bm{b}\neq\bm{0}\) & \(k_{\sigma,\bm{t},\mu}\) \\ \hline \multicolumn{6}{c}{**Any setting mirroring a previously derived NNGPK, e.g.**} \\ \hline \multirow{4}{*}{\(\mathbb{R}^{d}\)} & \multirow{4}{*}{\(\Phi_{\bm{C},\bm{0}}\)} & \multirow{4}{*}{\(\mathrm{Id}(\bm{x})\)} & erf & ✗ & [52] \\ \cline{3-6}  & & & \((\cdot)_{+}^{p},p\in\mathbb{N}\) & ✗ & [4] \\ \cline{3-6}  & & & \(\mathrm{LReLU}\) & ✗ & [48] \\ \cline{3-6}  & & & \(\mathrm{GELU}\) [14] & ✗ & [47] \\ \hline \multicolumn{6}{c}{**Any tractable exponential family [50, 32] setting, e.g.**} \\ \hline \(\mathbb{R}^{d}\) & \(\Phi_{\bm{C},\bm{m}}\) & \(\frac{\bm{x}}{\|\bm{x}\|_{2}}\) & & ✓ & Kernel 3 \\ \hline \(\mathbb{S}^{d-1}\) & Uniform & \(\mathrm{Id}(\bm{x})\) & \(\exp\) & ✓ & \\ \hline \(\mathbb{R}^{d}\) & \(\Phi_{\bm{C},\bm{m}}\) & & ✓ & Kernel 7 \\ \hline \(\{0,1,2,\ldots\}\) & \((x!)^{-1}\nu\) & & ✓ & Kernel 8 \\ \hline \multicolumn{6}{c}{**New tractable integration settings, e.g.**} \\ \hline \(\mathbb{R}^{d}\) & \(\Phi_{\bm{C},\bm{m}}\) & \(\mathrm{Id}(\bm{x})\) & \(\cos\) & ✓ & Kernel 2 \\ \hline \([0,1]^{d}\) & Uniform & \(\Phi^{-1}(\bm{x})\) & & & \\ \hline \(\mathbb{R}^{d}\) & \(\Phi_{\bm{C},\bm{m}}\) & \(\mathrm{Id}(\bm{x})\) & \(\mathrm{Snake}_{\mathrm{a}}\)[55, 39] & ✓ & Kernel 6 \\ \end{tabular}
\end{table}
Table 1: Examples of settings admitting a closed-form for the normalising constant \(\mathrm{z}(\bm{V},\bm{\Theta})\) (2) by leveraging a closed-form NNK \(k_{\sigma,\bm{t},\mu}\) (4). In each case, \(\mathrm{z}(\bm{V},\bm{\Theta})=\mathrm{Tr}(\bm{V}^{\top}\bm{V}\bm{K}_{\bm{\Theta}})\), where the entries of the matrix \(\bm{K}_{\bm{\Theta}}\) are described according to the NNK \(k_{\sigma,\bm{t},\mu}\) in Identity 1. \(\Phi_{\bm{C},\bm{m}}\) denotes the CDF of a multivariate normal distribution with mean \(\bm{m}\) and covariance matrix \(\bm{C}\) and \(\nu\) denotes counting measure. Rows with citations have been considered previously in the context of NNGPKs, but not as normalising constants and not with a reversal of the role of input and parameter. Note the cases where \(\bm{b}\neq\bm{0}\); this setting is not considered by others, because when the role of parameters and data is in the usual setting, \(\bm{b}=\bm{0}\) covers a sufficiently general setting. Noticing that SNEFYs strictly generalise exponential family mixture models (see § 3.2), fixing \(\sigma(\cdot)=\exp(\cdot/2)\) and a given base measure \(\mu\) and sufficient statistic \(\bm{t}\) for which the exponential family log-partition function is known also leads to tractable normalising constants. More known closed-form kernels as well as approximate kernels that can be adapted to our setting are given in [12].

so-called neural network Gaussian process kernels (NNGPKs) [31] in a generalised form, which we refer to as neural network kernels (NNKs).

We discuss some important theoretical properties of SNEFY such as exact normalising constant calculation, marginal distributions, conditional distributions, and connections with other probability models. We then consider a deep learning setting, where SNEFYs can either be used as base distributions in (non-volume preserving) normalising flows [46], or may describe flexible conditional density models with deep learning feature extractors. We demonstrate SNEFY on a variety of datasets.

### Background

NotationWe use lower case non-bold (like \(a\)) to denote scalars, lower case bold to denote vectors (like \(\bm{a}\)) and upper case bold to denote matrices (like \(\bm{A}\)). Random variables (scalar or vector) are additionally typeset in sans-serif (like a and \(\bm{a}\)). The special zero vector \((0,\dots,0)\) and identity matrix elements are \(\bm{0}\) and \(\bm{I}\). We use subscripts to extract (groups of) indices, so that for example, \(\bm{w}_{i}\) is the \(i\)th row of the matrix \(\bm{W}\) (as a column vector), \(\bm{v}_{\cdot,i}\) is the \(i\)th column of the matrix \(\bm{V}\), and \(b_{i}\) is the \(i\)th element of the vector \(\bm{b}\). We use \(\bm{\Theta}=(\bm{W},\bm{b})\in\mathbb{R}^{n\times(D+1)}\) to denote the concatenated hidden layer weights and biases. Correspondingly, we write \(\bm{\theta}_{i}=(\bm{w}_{i},b_{i})\in\mathbb{R}^{D+1}\) for the \(i\)th row of \(\bm{\Theta}\). We will use a number of special functions. \(\Phi_{\bm{C},\bm{m}}\) denotes the cumulative distribution function (CDF) of a Gaussian random vector with mean \(\bm{m}\) and covariance matrix \(\bm{C}\). We also use a shorthand \(\Phi_{\bm{C}}=\Phi_{\bm{C},\bm{0}}\) and \(\Phi=\Phi_{\bm{I},\bm{0}}\).

Single hidden layer neural networksWe consider a feedforward neural network \(\bm{f}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{m}\),

\[\bm{f}(\bm{t};\bm{V},\bm{\Theta})=\bm{V}\sigma(\bm{W}\bm{t}+\bm{b})\] (3)

with activation function \(\sigma\), hidden weights \(\bm{W}\in\mathbb{R}^{n\times D}\) and biases \(\bm{b}\in\mathbb{R}^{n}\) and readout parameters \(\bm{V}\in\mathbb{R}^{m\times n}\). Here \(\sigma\) is applied element-wise to its vector inputs, returning a vector of the same shape.

Neural network kernelsIn certain theories of deep learning, one often encounters a bivariate Gaussian integral called the _neural network Gaussian process kernel_ NNGPK. The NNGPK first arose as the covariance function of a well-behaved single layer neural network with random weights [31]. In the limit as the width of the network grows to infinity, the neural network (3) with suitably well-behaved (say, independent Gaussian) random weights converges to a zero-mean Gaussian process, so that the NNGPK characterises the law of the neural network predictions. These limiting models can be used as functional priors in a classical Gaussian process sense [53].

In our setting, the positive semidefinite (PSD) NNGPK appears in an entirely novel context, where the role of the hidden weights and biases \(\bm{\theta}_{i}=(\bm{w}_{i},b_{i})\) and the data \(\bm{x}\) is reversed. Instead of marginalising out the parameters and evaluating at the data, we marginalise out the data and evaluate at the parameters. The NNGPK \(k_{\sigma,\mathrm{Id},\Phi_{\bm{I}}}\) admits a representation of the form

\[k_{\sigma,\mathrm{Id},\Phi_{\bm{I}}}(\bm{\theta}_{i},\bm{\theta}_{j})\triangleq \mathbb{E}_{\bm{x}}\big{[}\sigma\big{(}\bm{w}_{i}^{\top}\bm{x}+b_{i}\big{)} \sigma\big{(}\bm{w}_{j}^{\top}\bm{x}+b_{j}\big{)}\big{]},\quad\bm{x}\sim \mathcal{N}\big{(}\bm{0},\bm{I}\big{)}.\] (4)

We do not discuss in detail how it is constructed in earlier works [31, 22, 33, 17], where usually \(b_{i}=b_{j}=0\)3, but not always[49]. When \(b_{i}=b_{j}=0\), closed-form expressions for the NNGPK are available for different choices of \(\sigma\) and \(\bm{t}\)[52, 20, 4, 48, 37, 47, 28, 13]. However, the setting of \(\bm{b}\neq\bm{0}\) is important in our context (as we show in SS2.2) and presents additional analytical challenges.

Footnote 3: After accounting for the reversal of parameters and data.

We will require a more general notion of an NNGPK which we call a _neural network kernel_ (NNK). We introduce a function \(\bm{t}\) which may be thought of as a warping function applied to the input data. Such warping is common in kernels and covariance functions and can be used to induce desirable analytical and practical properties [37, 29, 24, SS5.4.3]. We also integrate with respect to more general measures \(\mu\) instead of the standard Gaussian CDF, \(\Phi\). We define the NNK to be

\[k_{\sigma,\bm{t},\mu}(\bm{\theta}_{i},\bm{\theta}_{j})\triangleq\int_{\mathbb{ X}}\sigma\big{(}\bm{w}_{i}^{\top}\bm{t}(\bm{x})+b_{i}\big{)}\sigma\big{(}\bm{w}_{j} ^{\top}\bm{t}(\bm{x})+b_{j}\big{)}\mu(d\bm{x}).\] (5)Closed form squared neural families

### Normalising constants

Observe from (2) that by swapping the order of integration and multiplication by \(\bm{V}\), the normalising constant is quadratic in elements of \(\bm{V}\). The coefficients of the quadratic depend on \(\bm{\Theta}=(\bm{W},\bm{b})\). We now characterise these coefficients of the quadratic in terms of the NNK evaluated at rows \(\bm{\theta}_{i},\bm{\theta}_{j}\) of \(\bm{\Theta}\), which are totally independent of \(\bm{V}\). The proof of the following is given in Appendix A.

**Identity 1**.: _The integral (2) admits a representation of the form_

\[\mathrm{z}(\bm{V},\bm{\Theta})=\mathrm{Tr}\left(\bm{V}^{\top}\bm{V}\bm{K}_{ \bm{\Theta}}\right)\] (6)

_where \(k_{\sigma,\bm{t},\mu}\) is as defined in (5), and \(\bm{K}_{\bm{\Theta}}\) is the PSD matrix whose \(ij\)th entry is \(k_{\sigma,\bm{t},\mu}(\bm{\theta}_{i},\bm{\theta}_{j})\)._

By Identity 1, the normalised measure (1) then admits the explicit representation

\[P(d\bm{x};\bm{V},\bm{\Theta})=\frac{\mathrm{Tr}\left(\bm{V}^{\top}\bm{V} \widetilde{\bm{K}_{\bm{\Theta}}}(\bm{x})\right)}{\mathrm{Tr}\left(\bm{V}^{ \top}\bm{V}\bm{K}_{\bm{\Theta}}\right)}\mu(d\bm{x})=\frac{\mathrm{vec}(\bm{V} ^{\top}\bm{V})^{\top}\mathrm{vec}(\widetilde{\bm{K}_{\bm{\Theta}}}(\bm{x}))} {\mathrm{vec}(\bm{V}^{\top}\bm{V})^{\top}\mathrm{vec}(\bm{K}_{\bm{\Theta}})} \mu(d\bm{x}),\]

where \(\widetilde{\bm{K}_{\bm{\Theta}}}(\bm{x})\) is the PSD matrix whose \(ij\)th entry is \(\sigma(\bm{w}_{i}^{\top}\bm{t}(\bm{x})+b_{i})\sigma(\bm{w}_{j}^{\top}\bm{t}( \bm{x})+b_{j})^{\top}\). We used the cyclic property of the trace, writing the numerator as \(\mathrm{Tr}\left(\bm{\sigma}^{\top}\bm{V}^{\top}\bm{V}\bm{\sigma}\right)= \mathrm{Tr}\left(\bm{V}\bm{\sigma}\bm{\sigma}^{\top}\bm{V}^{\top}\right)= \mathrm{Tr}\left(\bm{V}^{\top}\bm{V}\bm{\sigma}\bm{\sigma}^{\top}\right)\). We emphasise again that the role of the data \(\bm{t}(\bm{x})\) and the hidden weights and biases \(\bm{\Theta}\) in the NNK \(k_{\sigma,\bm{t},\mu}\) are reversed compared with how they have appeared in previous settings. We may compute evaluations of \(k_{\sigma,\bm{t},\mu}\) in closed form for special cases of \((\sigma,\bm{t},\mu)\) using various identities in \(\mathcal{O}(d)\), where \(d\) is the dimensionality of the domain of integration, as we soon detail in SS 2.2. Combined with the trace inner product, this leads to a total cost of computing \(\mathrm{z}(\bm{V},\bm{\Theta})\) of \(\mathcal{O}(m^{2}n+dn^{2})\), where \(n\) and \(m\) are respectively the number of neurons in the first and second layers.

**Remark 1** (Alternative parameterisations).: _SNEFT models depend on readout parameters \(\bm{V}\) only through the direction of \(\mathrm{vec}(\bm{V}^{\top}\bm{V})\) and not on its norm or sign. For example, one can always find another parameterisation of readout parameters that results in the same probability distribution but has a normalising constant of \(1\). Furthermore, noticing that \(\bm{V}\) only appears as a PSD matrix \(\bm{M}\triangleq\bm{V}^{\top}\bm{V}\) of rank at most \(\min(m,n)\), one may alternatively parameterise a SNEFT by \((\bm{M},\bm{\Theta})\)._

### Neural network kernels

In SS 2.1, we reduced computation of the integral (2) to computation of a quadratic involving evaluations of the NNK (5). Several closed-forms are known for different settings of \(\sigma\), all with \(\bm{t}=\mathrm{Id}\) and \(\bm{b}=\bm{0}\). The motivation behind derivation of existing known results is from the perspective of inference in infinitely wide Bayesian neural networks [31] or to derive certain integrals involved in computing predictors of infinitely wide neural networks trained using gradient descent [17]. Here we describe some new settings that have not been investigated previously that are useful for the new setting of SNEFT. Recall from (5), that our kernel, \(k_{\sigma,\bm{t},\mu}\), is parametrised by activation function \(\sigma\), warping function (sufficient statistic) \(\bm{t}\), and base measure \(\mu\). All derivations are given in Appendix B.

The first kernel describes how we may express the kernels with arbitrary Gaussian base measures \(\Phi_{\bm{C},\bm{m}}\) in terms of the kernels with isotropic Gaussian base measures \(\Phi\). This means that it suffices to consider isotropic Gaussian base measures in place of arbitrary Gaussian base measures.

**Kernel 1**.: \(k_{\sigma,\mathrm{Id},\Phi_{\bm{C},\bm{m}}}(\bm{\theta}_{i},\bm{\theta}_{j})= k_{\sigma,\mathrm{Id},\Phi}(\mathcal{T}\bm{\theta}_{i},\mathcal{T}\bm{\theta}_{j})\)_, where \(\mathcal{T}\bm{\Theta}=(\bm{W}\bm{A},\bm{b}+\bm{W}\bm{m})\), \(\mathcal{T}\bm{\theta}_{i}=(\bm{w}_{i}^{\top}\bm{A},b_{i}+\bm{w}_{i}^{\top} \bm{m})\) and \(\bm{A}\) is a matrix factor such that covariance \(\bm{C}=\bm{A}\bm{A}^{\top}\)._

This kernel can also be used to describe kernels corresponding with Gaussian mixture model base measures. The second kernel we describe is a minor extension to the case \(\bm{b}\neq\bm{0}\) of a previously considered kernel [37].

**Kernel 2**.: \(k_{\mathrm{cos},\mathrm{Id},\Phi}(\bm{\theta}_{i},\bm{\theta}_{j})=\frac{\cos|b _{i}-b_{j}|}{2}\exp\left(\frac{-\|\bm{w}_{j}-\bm{w}_{j}\|^{2}}{2}\right)+\frac {\cos|b_{i}+b_{j}|}{2}\exp\left(\frac{-\|\bm{w}_{j}+\bm{w}_{j}\|^{2}}{2} \right)\).

A similar result and derivation holds for the case of \(k_{\sin,\mathrm{Id},\Phi}\), which we do not reproduce here. We now mention a case that shares a connection with exponential families (see SS 3.2 for a detailed description of this connection). The following and more \(\sigma=\exp\) cases are derived in Appendix C.

**Kernel 3**.: _Define \(\text{proj}_{\mathbb{S}^{d-1}}(\bm{x})\triangleq\bm{x}/\|\bm{x}\|\) to be the projection onto the unit sphere. Then_

\[k_{\exp,\text{proj}_{\mathbb{S}^{d-1}},\bm{\xi}}(\bm{\theta}_{i},\bm{\theta}_{j} )=\exp(b_{i}+b_{j})\frac{\Gamma(d/2)2^{d/2-1}I_{d/2-1}\big{(}\|\bm{w}_{i}+\bm{ w}_{j}\|\big{)}}{\|\bm{w}_{i}+\bm{w}_{j}\|^{d/2-1}},\]

_where \(I_{p}\) is the modified Bessel function of the first kind of order \(p\). In the special case \(d=3\), we have the closed-form \(k_{\exp,\text{proj}_{\mathbb{S}^{2}},\Phi}(\bm{\theta}_{i},\bm{\theta}_{j})= \exp(b_{i}+b_{j})\frac{\big{(}\|e^{\|\bm{w}_{i}+\bm{w}_{j}\|}-e^{-\|\bm{w}_{i} +\bm{w}_{j}\|}\big{)}}{2\|\bm{w}_{i}+\bm{w}_{j}\|}\)._

We end this section with a new analysis of the Snake\({}_{a}\) activation function, given by

\[\text{Snake}_{a}(z)=z+\frac{1}{a}\sin^{2}(az)=z-\frac{1}{2a}\cos(2az)+\frac{1 }{2a}.\]

The Snake\({}_{a}\) function [55] is a neural network activation function that can resemble the ReLU on an interval for special choices of \(a\), is easy to differentiate, and as we see shortly, admits certain attractive analytical tractability. We note that a similar activation function has been found using reinforcement learning to search for good activation functions [39, Table 1 and 2, row 3], up to an offset and hyperparameter \(a=1\). The required kernel is expressed in terms of the linear kernel (Kernel 4) and the kernel corresponding with the activation function of [39], i.e. snake without the offset, \(\text{Snake}_{a}(\cdot)-\frac{1}{2a}\) (Kernel 5). We first describe the linear kernel.

**Kernel 4**.: \(k_{\mathrm{Id},\mathrm{Id},\Phi}(\bm{\theta}_{i},\bm{\theta}_{j})=\bm{w}_{i}^ {\top}\bm{w}_{j}+b_{i}b_{j}\).

We now derive the kernel corresponding with \(\mathrm{Snake}_{\mathrm{a}}\) activation functions up to an offset.

**Kernel 5**.: _The kernel \(k_{\mathrm{Snake}_{\mathrm{a}}(\cdot)-\frac{1}{2a}\mathrm{Id},\Phi}(\bm{ \theta}_{i},\bm{\theta}_{j})\) is equal to_

\[\frac{1}{4a^{2}}k_{\mathrm{cos},\mathrm{Id},\Phi}(2a\bm{\theta}_{ i},2a\bm{\theta}_{j})+\bm{w}_{j}^{\top}\bm{w}_{j}\Big{(}\sin(2ab_{j})e^{-2a^{2} \|\bm{w}_{j}\|^{2}}+\sin(2ab_{i})e^{-2a^{2}\|\bm{w}_{i}\|^{2}}\Big{)}\] \[\quad-\frac{b_{i}}{2a}\cos(2ab_{j})e^{-2a^{2}\|\bm{w}_{j}\|^{2}}- \frac{b_{j}}{2a}\cos(2ab_{i})e^{-2a^{2}\|\bm{w}_{i}\|^{2}}+k_{\mathrm{Id}, \mathrm{Id},\Phi}(\bm{\theta}_{i}^{(1)},\bm{\theta}_{j}^{(1)}).\]

The kernel corresponding with \(\mathrm{Snake}_{\mathrm{a}}\) activations is then stated in terms of Kernel 4 and 5.

**Kernel 6**.: _The kernel \(k_{\mathrm{Snake}_{\mathrm{a}},\mathrm{Id},\Phi}(\bm{\theta}_{i},\bm{\theta}_{j})\) is equal to_

\[\frac{1}{2a}\Big{(}b_{i}-\frac{1}{2a}\cos(2ab_{i})\exp(-2a^{2}\| \bm{w}_{i}\|^{2})+b_{j}-\frac{1}{2a}\cos(2ab_{j})\exp(-2a^{2}\|\bm{w}_{j}\|^{2 })\Big{)}\] \[\quad+k_{\mathrm{Snake}_{\mathrm{a}}(\cdot)-\frac{1}{2a},\mathrm{ Id},\Phi}(\bm{\theta}_{i},\bm{\theta}_{j})+\frac{1}{4a^{2}}.\]

## 3 Properties of squared neural families

### Fisher-Neyman factorisation and sufficient statistics

If the base measure \(\mu\) is absolutely continuous with respect to some measure \(\nu\), and \(\frac{d\mu}{d\nu}:\Omega\to[0,\infty)\) is the Radon-Nikodym derivative, then the SNEFY admits a probability density function \(p(\cdot\mid\bm{V},\bm{\Theta})\) with respect to \(\nu\),

\[p(\bm{x}\mid\bm{V},\bm{\Theta})=\underbrace{\frac{d\mu}{d\nu}(\bm{x})}_{\text{ Independent of $\bm{V}$},\,\bm{\Theta}}\ \times\ \underbrace{\big{\|}\bm{f}\left(\bm{t}(\bm{x});\bm{V},\bm{\Theta}\right)\big{\|}_ {2}^{2}}_{\text{Depends on $\bm{x}$ only through $\bm{t}(\bm{x})$}}.\] (7)

The Fisher-Neyman theorem (for example, see Theorem 6.14 of [6]) says that the existence of such a factorisation is equivalent to the fact that \(\bm{t}\) is a sufficient statistic for the parameters \(\bm{V},\bm{\Theta}\).

### Connections with exponential families

In this section we will use the activation \(\sigma(u)=\exp(u/2)\). We note that we can absorb the bias terms \(\bm{b}\) into the \(\bm{V}\) parameters4 and obtain as a special case the following family of distributions

Footnote 4: Note that \(\exp(b_{i})\) is independent of \(\bm{x}\) and only appears as a product with \(v_{i}\).

\[P(d\bm{x};\bm{V},\bm{W})=\frac{1}{\mathrm{Tr}(\bm{V}^{\top}\bm{V}\bm{K}_{\bm{ \Theta}})}\sum_{i=1}^{n}\sum_{j=1}^{n}\bm{v}_{\cdot,i}^{\top}\bm{v}_{\cdot,j} \exp\left(\frac{1}{2}(\bm{w}_{i}+\bm{w}_{j})^{\top}\bm{t}(\bm{x})\right)\mu(d \bm{x}),\] (8)which is a mixture5 of distributions \(P_{e}\left(\cdot;\frac{1}{2}(\bm{w}_{i}+\bm{w}_{j})\right)\) belonging to a classical exponential family \(P_{e}\)[50, 32], given in the canonical form by

Footnote 5: Here we are concerned with the setting where every term in the mixture model belongs to the same family, i.e. an exponential family mixture model but not a mixture of distinct exponential families.

\[P_{e}(d\bm{x};\bm{w})=\frac{1}{\mathrm{z}_{e}(\bm{w})}\exp\big{(}\bm{w}^{ \top}\bm{t}(\bm{x})\big{)}\mu(d\bm{x}),\quad\mathrm{z}_{e}(\bm{w})=\int_{ \mathbb{X}}\exp\big{(}\bm{w}^{\top}\bm{t}(\bm{x})\big{)}\mu(d\bm{x}).\] (9)

It is helpful to identify the following three further cases:

1. When \(n=m=1\), \(v_{11}^{2}\) cancels in the numerator and denominator and we obtain an exponential family with base measure \(\mu\) supported on \(\mathbb{X}\), sufficient statistic \(\bm{t}\), canonical parameter \(\bm{w}_{1}\) and normalising constant \(\mathrm{z}_{e}(\bm{w}_{1})\). Every exponential family is thus a SNEFY, but not conversely.
2. When \(m>1\) and \(n>1\), we obtain a type of exponential family mixture model with coefficients \(\bm{V}^{\top}\bm{V}\), some of which may be negative. Advantages of allowing negative weights in mixture models in terms of learning rates are discussed in [42]. The rank of \(\bm{V}^{\top}\bm{V}\) is at most \(\min(m,n)\).
3. When \(m>1\) and \(n>1\) and \(\bm{V}^{\top}\bm{V}\) is diagonal (i.e. each column in \(\bm{V}\) is orthogonal), there are at most \(n\) non-zero mixture coefficients, all of which are nonnegative. That is, we obtain a standard exponential family mixture model.

The kernel matrix \(\bm{K}_{\bm{\Theta}}\) in the normalising constant of (8) is tractable whenever the normalising constant of the corresponding exponential family is itself tractable.

**Proposition 1**.: _Denote by \(\mathrm{z}_{e}(\bm{w})\) the normalising constant of the exponential family in (9). Then_

\[k_{\exp(\cdot/2),\bm{t},\mu}(\bm{w}_{i},\bm{w}_{j})= \,\mathrm{z}_{e}\left(\frac{1}{2}(\bm{w}_{i}+\bm{w}_{j})\right).\] (10)

The above kernel is well defined for any collection of \(\bm{w}_{i}\) which belong to the canonical parameter space of \(P_{e}\), since the canonical parameter space is always convex [50]. This gives us a large number of tractable instances of SNEFY which correspond to exponential family mixture models allowing negative weights - a selection of examples is given in Appendix C. It is interesting that some properties of the exponential families are retained by this generalisation belonging to SNEFYs. For example, the following proposition, the proof of which is given in Appendix A, links the derivatives of the log-normalising constant to the mean and the covariance of the sufficient statistic.

**Proposition 2**.: _Let \(\sigma(u)=\exp(u/2)\) and define the log-normalising constant as \(\Psi=\log\mathrm{z}(\bm{V},\bm{\Theta})\)._

\[\text{Then}\quad\sum_{i=1}^{n}\frac{\partial\Psi}{\partial\bm{w}_{i}}= \mathbb{E}\left[\bm{t}\left(\mathsf{x}\right)\right]\quad\text{and}\quad\sum _{i=1}^{n}\sum_{j=1}^{n}\frac{\partial^{2}\Psi}{\partial\bm{w}_{i}\bm{w}_{j}^{ \top}}=\mathbb{E}\left[\bm{t}\left(\mathsf{x}\right)\bm{t}\left(\mathsf{x} \right)^{\top}\right]-\mathbb{E}\left[\bm{t}\left(\mathsf{x}\right)\right] \mathbb{E}\left[\bm{t}\left(\mathsf{x}\right)\right]^{\top}.\]

### Conditional distributions under SNEFY

An attractive property of SNEFY is that, under mild conditions, the family is closed under conditioning.

**Theorem 1**.: _Let \(\mathbf{x}=(\mathbf{x}_{1},\mathbf{x}_{2})\) be jointly \(\texttt{{SNEFY}}_{X_{1}\times X_{2},\bm{t},\sigma,\mu}\) with parameters \(\bm{V}\) and \(\bm{\Theta}=\left(\left[\bm{W}_{1},\bm{W}_{2}\right],\bm{b}\right)\). Assume that \(\mu(d\bm{x})=\mu_{1}(d\bm{x}_{1})\mu_{2}(d\bm{x}_{2})\) and \(\bm{t}(\bm{x})=\big{(}\bm{t}_{1}(\bm{x}_{1}),\bm{t}_{2}(\bm{x}_{2})\big{)}\). Then the conditional distribution of \(\mathbf{x}_{1}\) given \(\mathbf{x}_{2}=\bm{x}_{2}\) is \(\texttt{{SNEFY}}_{X_{1},\bm{t}_{1},\sigma,\mu_{1}}\) with parameters \(\bm{V}\) and \(\bm{\Theta}_{1|2}\triangleq(\bm{W}_{1},\bm{W}_{2}\bm{t}_{2}(\bm{x}_{2})+\bm{b})\)._

The proof, which we detail in Appendix A follows directly by folding the dependence on the conditioning variable \(\bm{x}_{2}\) into the bias term. We note that conditional density will typically be tractable if the joint density is tractable since they share the same activation function \(\sigma\). Thus, whenever SNEFY corresponds to a tractable NNK with a non-zero bias, we can construct highly flexible _conditional density models_ using SNEFY by taking \(\bm{t}_{2}\) itself to be a jointly trained deep neural network. Crucially, \(\bm{t}_{2}\) may be _completely unconstrained_. We use this observation in the experiments (SS 4).

### Marginal distributions under SNEFY

Marginal distributions under SNEFY model for a general activation function \(\sigma\) need not belong to the same family. In the special case \(\sigma=\exp(\cdot/2)\), SNEFY is in fact also closed under marginalisation, which we prove in Appendix D. Even in the general \(\sigma\) case, marginal distributions are tractable and admit closed forms whenever the joint SNEFY model and the conditional SNEFY are tractable.

**Theorem 2**.: _Let \(\mathbf{x}=(\mathbf{x}_{1},\mathbf{x}_{2})\) be jointly \(\texttt{SNEFY}_{\mathbb{X}_{1}\times\mathbb{X}_{2},\mathbf{t},\sigma,\mu}\) with parameters \(\bm{V}\) and \(\bm{\Theta}=\left(\left[\bm{W}_{1},\bm{W}_{2}\right],\bm{b}\right)\). Assume that \(\mu(d\bm{x})=\mu_{1}(d\bm{x}_{1})\mu_{2}(d\bm{x}_{2})\) and \(\bm{t}(\bm{x})=\big{(}\bm{t}_{1}(\bm{x}_{1}),\bm{t}_{2}(\bm{x}_{2})\big{)}\). Then the marginal distribution of \(\mathbf{x}_{1}\) is_

\[P_{1}(d\bm{x}_{1})=\frac{\operatorname{Tr}\big{(}\bm{V}^{\top}\bm{V}\widetilde {\bm{C}}_{\bm{\Theta}}(\bm{x}_{1})\big{)}}{\operatorname{z}(\bm{V},\bm{ \Theta})}\mu_{1}(d\bm{x}_{1}),\]

_where \(\widetilde{C}(\bm{x}_{1})_{ij}=k_{\sigma,\bm{t}_{2},\mu_{2}}\Big{(}\big{(}\bm {w}_{2i},\bm{w}_{1i}^{\top}\bm{t}_{1}(\bm{x}_{1})+b_{i}\big{)},\big{(}\bm{w}_ {2j},\bm{w}_{1j}^{\top}\bm{t}_{1}(\bm{x}_{1})+b_{j}\big{)}\Big{)}\)._

The proof is given in Appendix A. Due to this tractability of the marginal distributions, it is straightforward to include the likelihood corresponding to incomplete observations (i.e. samples where we are missing some of the components of \(\mathbf{x}\)) into the density estimation task.

### Connections with kernel-based methods for nonnegative functions

SNEFY may be viewed as a neural network variant of the non-parametric kernel models for non-negative functions [25], which are constructed as follows. Let \(\bm{\psi}:\mathbb{X}\to\mathbb{H}\) be a feature mapping to a (possibly infinite dimensional) Hilbert space \(\mathbb{H}\). Let \(\mathbb{S}(\mathbb{H})\) be the set of all positive semidefinite (PSD) bounded linear operators \(\bm{A}:\mathbb{H}\to\mathbb{H}\). Then

\[h_{\bm{A}}(\bm{x})=\left\langle\bm{\psi}(\bm{x}),\bm{A}\bm{\psi}(\bm{x}) \right\rangle_{\mathbb{H}}\] (11)

gives an elegant model for nonnegative functions parametrised by \(\bm{A}\in\mathbb{S}(\mathbb{H})\), and their application to density modelling with respect to a base measure has also been explored [25; 42]. Note that by assuming boundedness of \(\bm{A}\) and \(\bm{\psi}\), the normalizing constant [25, Proposition 4] is given by \(\operatorname{Tr}\Big{(}\bm{A}\int_{\mathbb{X}}\bm{\psi}(\bm{x})\otimes\bm{ \psi}(\bm{x})\mu(d\bm{x})\Big{)}\), which is analogous to our work where the normalising constant is given by \(\operatorname{z}(\bm{V},\bm{\Theta})=\operatorname{Tr}(\bm{V}^{\top}\bm{V}\bm {K}_{\bm{\Theta}})\). This can be seen by replacing \(\bm{A}\) with \(\bm{V}^{\top}\bm{V}\) and replacing \(\int_{\mathbb{X}}\bm{\psi}(\bm{x})\otimes\bm{\psi}(\bm{x})\mu(d\bm{x})\) by \(\bm{K}_{\bm{\Theta}}=\int_{\mathbb{X}}\sigma(\bm{W}\bm{t}(\bm{x})+\bm{b})\sigma (\bm{W}\bm{t}(\bm{x})+\bm{b})^{\top}\mu(d\bm{x})\).

Despite feature maps being infinite-dimensional, model (11) often reduces to an equivalent representation in finite-dimensions. [25] utilise a representer theorem when (11) is fitted to data using a regularised objective, while [42] more directly assume that the linear operator \(\bm{A}\) is inside the span of the features evaluated at the available data \(\{\bm{x}_{\ell}\}_{\ell=1}^{N}\). The resulting model resembles SNEFY where \(n\) equals to the number \(N\) of datapoints, i.e.

\[h_{\bm{M}}(\bm{x})=\left[\kappa(\bm{x},\bm{x}_{1}),\ldots,\ldots,\kappa(\bm{x},\bm{x}_{N})\right]^{\top}\bm{M}[\kappa(\bm{x},\bm{x}_{1}),\ldots,\ldots,\kappa (\bm{x},\bm{x}_{N})]\] (12)

for a PSD matrix \(\bm{M}\in\mathbb{R}^{N\times N}\) and \(\kappa(\bm{x}_{i},\bm{x}_{j})=\left\langle\bm{\psi}(\bm{x}_{i}),\bm{\psi}(\bm {x})\right\rangle_{\mathbb{H}}\).

However, there are fundamental differences between (12) and SNEFY which we list below. The models can be seen as complementary and they inherit advantages and disadvantages of kernel methods and neural networks common in other settings, respectively.

* this is not generally tractable apart from some

Figure 1: (Left) An instance of an (untrained) SNEFY\({}_{\mathbb{R}^{2},\mathrm{Id},\mathrm{Snake}_{a},\Phi}\) density with \(n=100\), \(m=1\), \(v_{ij}\sim\mathcal{N}(0,1/n)\), \(w_{ij}\sim\mathcal{N}(0,4)\) and \(\bm{b}=\bm{0}\). Shown are \(50\) exact samples found using rejection sampling. Numerical quadrature for this and every example supported on \(\mathbb{R}^{d}\) in § 4 returns a value of \(1.00\) for the integral over \(\mathbb{X}\). (Right) A trained SNEFY\({}_{\mathbb{S}^{2},\mathrm{Id},\mathrm{exp},d\bm{x}}\) density with \(n=m=30\). Shown is the training and testing dataset [44] also used by [19] for point processes.

limited combinations of \(\kappa\) and \(\mu\) (e.g. for a Gaussian kernel \(\kappa\) and a Gaussian \(\mu\)). Note that this kernel is evaluated at the datapoints, whereas SNEFY evaluates the kernel at the learned parameters \(\bm{w}_{i}\). [42] focuses on the specific case where \(\kappa\) is a Gaussian kernel, studying properties of the resulting density class which is a mixture of Gaussian densities allowing for negative weights, a model equivalent to SNEFY with the exponential activation function as described in Appendix C. Note that our treatment of SNEFY as a generalisation of the exponential family goes well beyond the Gaussian case, and that tractable instances arise with many other activation functions.
* instead, expressivity in (12) only comes from fitting \(\bm{M}\) (and potentially lengthscale hyperparameters of \(\kappa\)), at the expense of a more involved optimiisation over the space of PSD matrices. In contrast, we learn \(\bm{W}\) (analogous to learning \(\bm{\psi}\)) and \(\bm{V}\) jointly using neural-network style gradient optimisers. SNEFY is fully compatible with end-to-end and jointly optimised neural network frameworks, a property we leverage heavily in our experiments in SS 4.
* **Conditioning.** By explicitly writing parametrisation which includes the biases, we obtain a family closed under conditioning and thus a natural model for conditional densities, whereas it is less clear how to approach conditioning when given a generic feature map \(\bm{\psi}\).

Other related workAfter submission, we became aware of another related literature, which includes mixture models with potentially negative mixture coefficients via squaring [23] and positive semi-definite probabilistic circuits [43]. We believe a marriage of ideas from SNEFY and probabilistic circuits will lead to future developments in tractable and expressive probability models.

## 4 Experiments

ImplementationAll experiments are conducted on a Dual Xeon 14-core E5-2690 with 30GB of reserved RAM and a single NVidia Tesla P100 GPU. Full experimental details are given in Appendix E. We build our implementation on top of normflows[45], a PyTorch package for normalising flows. SNEFYs are built as a BaseDistribution, which are base distributions inside a NormalizingFlow with greater than or equal to zero layers. We train all models via maximum likelihood estimation (MLE) i.e. minimising forward KL divergence.

2D synthetic unconditional density estimationWe consider the 2 dimensional problems also benchmarked in [46]. We compare the test performance, computation time and parameter count of non-volume preserving flows (NVPs) [7] with four types of base distribution: SNEFY, resampled [46] (Resampled), diagonal Gaussian (Gauss) and Gaussian mixture model (GMM). We consider flow depths of 0, 1, 2, 4, 8 and 32 where a flow depth of 0 corresponds with the base distribution only. We use \(\sigma=\cos\), \(\bm{t}=\operatorname{Id}\), \(\mathbb{X}=\operatorname{\mathbb{R}}^{d}\) and a Gaussian mixture model base density. We set \(m=1\) and \(n=50\). Full architectures and further experimental details are described in Appendix E.1.

Results are shown in Table 2 for the \(0\) and \(16\) layer cases, and further tables for \(1,2,4\) and \(8\) layers are given in Appendix E.1. Our observations are that all base distributions are able to achieve good performance, provided they are appended with a normalising flow of appropriate depth. SNEFY is able to achieve good performance with depth 0 on all three datasets, as are Resampled and GMM for the Moons dataset. The parameter count for well-performing SNEFY models is very low, but the computation time can be relatively high. However, SNEFY is the only model which consistently achieves the highest performance within one standard deviation across all normalising flow depths.

Data on the sphereWe compare the three cases mentioned in SS 3.2 using Kernel 3, i.e. mixtures of the von Mises Fisher (VMF) distribution, as shown in Figure 1. Over \(50\) runs, the unconstrained \(\bm{V}\), diagonal \(\bm{V}\), and \(n=m=1\) cases respectively obtain test negative log likelihoods of \(1.38\pm 9.64\times 10^{-3}\), \(1.46\pm 0.016\) and \(2.34\pm 0.26\) each in \(111.18\pm 2.58\), \(109.12\pm 0.80\) and \(61.88\pm 0.33\) seconds (average \(\pm\) standard deviation). In this setting, allowing for a fully flexible \(\bm{V}\), going beyond the classical mixture model, shows clear benefits in performance. Results are summarised in Table 3. Full details are given in Appendix E.2.

Conditional density estimation on astronomy dataPredicting plausible values for the velocity of distant astronomical objects (such as galaxies or quasars) without measuring their full spectra, but by 

[MISSING_PAGE_FAIL:9]

model parameters [3] or samples from missing data and optimises for model parameters [41], however these do not allow for maximum likelihood estimation. Our observations are twofold: including partial observations improves performance, and adding more complete observations improves performance.

## 5 Discussion and conclusion

We constructed a new class of probability models - SNEFY - by normalising the squared norm of a neural network with respect to a base measure. SNEFY possesses a number of convenient properties: tractable exact normalising constants in many instances we identified, closure under conditioning, tractable marginalisation, and intriguing connections with existing models. SNEFY shows promise empirically, outperforming competing (conditional) density estimation methods in experiments.

Sampling versus density estimationWe focus here on the problem of density estimation, for which SNEFY is well-suited. While it is sometimes possible to obtain exact SNEFY samples using rejection sampling, sampling is more computationally expensive than in other models such as normalising flows. Future work will focus on sampling, as has been done with related models [26], where the special case of Gaussian \(\sigma\) and hyperrectangular support \(\mathbb{X}\) is considered. In [26], \(r\) approximate samples with \(\mathcal{O}(r\log_{2}|\mathbb{X}|+rd\log_{2}\frac{2}{\rho})\) evaluations of the normalising constant are obtained. Here \(\rho\) is an approximation tolerance parameter. Note that this complexity significantly improves upon naive rejection sampling, which typically scales exponentially in dimension \(d\). In the unconditional setting, SNEFY is constructed as only a 2-layer network, thereby limiting expressivity. However, in the conditional density estimation setting, we may use any number of layers and any architecture for the conditioning network \(\bm{t}_{2}\). Finally, SNEFY inherits all the usual limitations and advantages over mirroring kernel-based approaches, as discussed in SS 3.5.

Future workWe see a number of further promising future research directions. First, as we detail in Appendix F, choosing \(\sigma(\cdot)=\exp(i\cdot)\) and identity sufficient statistics results in a kernel \(k_{\exp(i\cdot),\mathrm{Id},\mu}\) which is the Fourier transform of a nonnegative measure. By Bochner's theorem, the kernel is guaranteed to be (real or complex-valued) shift-invariant. The kernel matrix is Hermitian PSD (so that the normalising constant is positive and nonnegative), and we may also allow (but do not require) the readout parameters \(\bm{V}\) to be complex. We note that the same result would be obtained if one used a mixture of real-valued \(\cos\) and \(\sin\) activations with shared parameters (see Remark 3). Second, an alternative deep model to our deep conditional feature extractor might be to use a SNEFY model as a base measure \(\mu\) for another SNEFY model; this might be repeated \(L\) times. This leads to \(\mathcal{O}(n^{2L})\) integration terms for the normalising constant. The individual terms are tractable in certain cases, for example when \(\sigma\) is exponential or trigonometric. Third, when modelling discrete distributions with trigonometric activations, the NNK can be expressed in terms of convergent Fourier series (see Appendix G) Finally, our integration technique can be applied to other settings. For example, we may build a Poisson point process intensity function using a squared neural network and compute the intensity function in closed-form, offering a model that scales quadratically in the number of neurons \(\mathcal{O}(n^{2})\) instead of comparable models which scale cubically in the number of datapoints \(\mathcal{O}(N^{3})\)[9; 51].

Figure 2: Density estimation under partial observations. The right plot is a zoomed in version of the left plot. NF-1, NF-2 and NF-4 are respectively normalising flows of depth 1, 2 and 4. Normalising flow models and SNEFY without marginalisation discard incomplete observations, whereas SNEFY use Theorem 2 to include partial observations via maximum marginal likelihood. Marginal likelihoods allow for an improved NLL.

## Acknowledgments and Disclosure of Funding

Russell and Cheng Soon would like to acknowledge the support of the Machine Learning and Artificial Intelligence Future Science Platform, CSIRO. The authors would like to thank Jia Liu for early discussions about the idea.

## References

* Bach [2017] Francis Bach. Breaking the curse of dimensionality with convex neural networks. _Journal of Machine Learning Research_, 18(19):1-53, 2017.
* Beck et al. [2017] R Beck, C-A Lin, EEO Ishida, F Gieseke, RS de Souza, MV Costa-Duarte, MW Hattab, and A Krone-Martins. On the realistic validation of photometric redshifts. _Monthly Notices of the Royal Astronomical Society_, 468(4):4323-4339, 2017.
* Bernal [2021] Edgar A Bernal. Training deep normalizing flow models in highly incomplete data scenarios with prior regularization. _arXiv preprint arXiv:2104.01482_, 2021.
* Cho and Saul [2009] Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In _Advances in Neural Information Processing Systems_, pages 342-350, 2009.
* Collins et al. [2001] Michael Collins, Sanjoy Dasgupta, and Robert E Schapire. A generalization of principal components analysis to the exponential family. _Advances in neural information processing systems_, 14, 2001.
* Deisenroth et al. [2020] Marc Peter Deisenroth, A Aldo Faisal, and Cheng Soon Ong. _Mathematics for machine learning_. Cambridge University Press, 2020.
* Dinh et al. [2017] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In _International Conference on Learning Representations_, 2017.
* Efron and Hastie [2021] Bradley Efron and Trevor Hastie. _Computer Age Statistical Inference: Algorithms, Evidence, and Data Science_, volume 6. Cambridge University Press, 2021.
* Flaxman et al. [2017] Seth Flaxman, Yee Whye Teh, and Dino Sejdinovic. Poisson intensity estimation with reproducing kernels. In _Artificial Intelligence and Statistics_, pages 270-279. PMLR, 2017.
* Glusenkamp [2020] Thorsten Glusenkamp. Unifying supervised learning and vaes-automating statistical inference in (astro-) particle physics with amortized conditional normalizing flows. _arXiv e-prints_, 2020.
* Goodfellow et al. [2014] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In _Advances in Neural Information Processing Systems_, volume 27, 2014.
* Han et al. [2022] Insu Han, Amir Zandieh, Jaehoon Lee, Roman Novak, Lechao Xiao, and Amin Karbasi. Fast neural kernel embeddings for general activations. In _Advances in Neural Information Processing Systems_, volume 35, pages 35657-35671, 2022.
* Han et al. [2022] Insu Han, Amir Zandieh, Jaehoon Lee, Roman Novak, Lechao Xiao, and Amin Karbasi. Fast neural kernel embeddings for general activations. _Advances in neural information processing systems_, 2022.
* Hendrycks and Gimpel [2016] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units. _arXiv preprint arXiv:1606.08415_, 2016.
* Hinton [2002] Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. _Neural computation_, 14(8):1771-1800, 2002.
* Hyvarinen and Dayan [2005] Aapo Hyvarinen and Peter Dayan. Estimation of non-normalized statistical models by score matching. _Journal of Machine Learning Research_, 6(4), 2005.
* Jacot et al. [2018] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In _Advances in neural information processing systems_, pages 8571-8580, 2018.

* [18] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. _arXiv preprint arXiv:1312.6114_, 2013.
* [19] Thomas Joseph Lawrence. Point pattern analysis on a sphere. Master's thesis, The University of Western Australia, 2018.
* [20] Nicolas Le Roux and Yoshua Bengio. Continuous neural networks. In _Artificial Intelligence and Statistics_, pages 404-411, 2007.
* [21] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. A tutorial on energy-based learning. _Predicting structured data_, 1(0), 2006.
* [22] Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S Schoenholz, Jeffrey Pennington, and Jascha Sohl-Dickstein. Deep neural networks as Gaussian processes. In _International Conference on Learning Representations_, 2018.
* [23] Lorenzo Loconte, Stefan Mengel, Nicolas Gillis, and Antonio Vergari. Negative mixture models via squaring: Representation and learning. In _The 6th Workshop on Tractable Probabilistic Modeling_, 2023.
* [24] David JC MacKay. Introduction to Gaussian processes, 1998.
* [25] Ulysse Marteau-Ferey, Francis Bach, and Alessandro Rudi. Non-parametric models for non-negative functions. _Advances in neural information processing systems_, 33:12816-12826, 2020.
* [26] Ulysse Marteau-Ferey, Francis Bach, and Alessandro Rudi. Sampling from arbitrary functions via psd models. In _International Conference on Artificial Intelligence and Statistics_, pages 2823-2861. PMLR, 2022.
* [27] P. McCullagh and J.A. Nelder. _Generalized Linear Models, Second Edition_. Monographs on Statistics and Applied Probability Series. Chapman & Hall, 1989.
* [28] Lassi Meronen, Christabella Irwanto, and Arno Solin. Stationary activations for uncertainty calibration in deep learning. _Advances in Neural Information Processing Systems_, 33:2338-2350, 2020.
* [29] Lassi Meronen, Martin Trapp, and Arno Solin. Periodic activation functions induce stationarity. _Advances in Neural Information Processing Systems_, 34:1673-1685, 2021.
* [30] Charlie Nash and Conor Durkan. Autoregressive energy machines. In _International Conference on Machine Learning_, pages 1735-1744, 2019.
* [31] Radford M Neal. _Bayesian learning for neural networks_. PhD thesis, University of Toronto, 1995.
* [32] Frank Nielsen and Vincent Garcia. Statistical exponential families: A digest with flash cards. _arXiv preprint arXiv:0911.4863_, 2009.
* [33] Roman Novak, Lechao Xiao, Jaehoon Lee, Yasaman Bahri, Greg Yang, Jiri Hron, Daniel A Abolafia, Jeffrey Pennington, and Jascha Sohl-Dickstein. Bayesian deep convolutional networks with many channels are Gaussian processes. In _The International Conference on Learning Representations_, 2019.
* [34] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. _The Journal of Machine Learning Research_, 22(1):2617-2680, 2021.
* [35] Georgios Papamakarios. _Neural density estimation and likelihood-free inference_. PhD thesis, University of Edinburgh, 2019.
* [36] Jooyoung Park and Irwin W Sandberg. Universal approximation using radial-basis-function networks. _Neural computation_, 3(2):246-257, 1991.

* [37] Tim Pearce, Russell Tsuchida, Mohamed Zaki, Alexandra Brintrup, and Andy Neely. Expressive priors in Bayesian neural networks: Kernel combinations and periodic functions. In _Uncertainty in Artificial Intelligence_, 2019.
* [38] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. _Advances in neural information processing systems_, 20, 2007.
* [39] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Searching for activation functions, 2018.
* [40] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In _International conference on machine learning_, pages 1530-1538, 2015.
* [41] Trevor W Richardson, Wencheng Wu, Lei Lin, Beilei Xu, and Edgar A Bernal. Mcflow: Monte carlo flow models for data imputation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14205-14214, 2020.
* [42] Alessandro Rudi and Carlo Ciliberto. PSD representations for effective probability models. In _Advances in Neural Information Processing Systems_, volume 34, pages 19411-19422, 2021.
* [43] Aleksanteri Sladek. Positive Semi-Definite Probabilistic Circuits. Master's thesis, Aalto University. School of Science, 2023.
* [44] W. Steinicke. Revised new general catalogue and index catalogue (revised ngc/ic). http://www.klima-luft.de/steinicke/index_e.htm. Accessed 2nd May 2015.
* [45] Vincent Stimper, David Liu, Andrew Campbell, Vincent Berenz, Lukas Ryll, Bernhard Scholkopf, and Jose Miguel Hernandez-Lobato. normflows: A PyTorch package for normalizing flows. _arXiv preprint arXiv:2302.12014_, 2023.
* [46] Vincent Stimper, Bernhard Scholkopf, and Jose Miguel Hernandez-Lobato. Resampling Base Distributions of Normalizing Flows. In _Proceedings of The 25th International Conference on Artificial Intelligence and Statistics_, volume 151, pages 4915-4936, 2022.
* [47] Russell Tsuchida, Tim Pearce, Chris van der Heide, Fred Roosta, and Marcus Gallagher. Avoiding kernel fixed points: Computing with ELU and GELU infinite networks. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 9967-9977, 2021.
* [48] Russell Tsuchida, Fred Roosta, and Marcus Gallagher. Invariance of weight distributions in rectified MLPs. In _International Conference on Machine Learning_, pages 5002-5011, 2018.
* [49] Russell Tsuchida, Fred Roosta, and Marcus Gallagher. Richer priors for infinitely wide multi-layer perceptrons. _arXiv preprint arXiv:1911.12927_, 2019.
* [50] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational inference. _Foundations and Trends(r) in Machine Learning_, 1(1-2):1-305, 2008.
* [51] Christian J Walder and Adrian N Bishop. Fast bayesian intensity estimation for the permanental process. In _International Conference on Machine Learning_, pages 3579-3588. PMLR, 2017.
* [52] Christopher KI Williams. Computing with infinite networks. In _Advances in neural information processing systems_, pages 295-301, 1997.
* [53] Christopher KI Williams and Carl Edward Rasmussen. _Gaussian processes for machine learning_. MIT press Cambridge, MA, 2006.
* [54] Christina Winkler, Daniel Worrall, Emiel Hoogeboom, and Max Welling. Learning likelihoods with conditional normalizing flows, 2019.
* [55] Liu Ziyin, Tilman Hartwig, and Masahito Ueda. Neural networks fail to learn periodic functions and how to fix it. In _Advances in Neural Information Processing Systems_, volume 33, pages 1583-1594, 2020.

## Appendix A Proofs

**Identity 1**.: _The integral (2) admits a representation of the form_

\[\mathrm{z}(\bm{V},\bm{\Theta})=\mathrm{Tr}\left(\bm{V}^{\top}\bm{V}\bm{K}_{\bm{ \Theta}}\right)\] (6)

_where \(k_{\sigma,\bm{t},\mu}\) is as defined in (5), and \(\bm{K}_{\bm{\Theta}}\) is the PSD matrix whose \(ij\)th entry is \(k_{\sigma,\bm{t},\mu}(\bm{\theta}_{i},\bm{\theta}_{j})\)._

Proof.: Let \(\widetilde{\bm{K}_{\bm{\Theta}}}(\bm{x})\) be the PSD matrix whose \(ij\)th entry is \(\sigma(\bm{w}_{i}^{\top}\bm{t}(\bm{x})+b_{i})\sigma(\bm{w}_{j}^{\top}\bm{t}( \bm{x})+b_{j})^{\top}\). The squared norm of the neural network evaluation is given by \(\mathrm{Tr}\left(\bm{V}^{\top}\bm{V}\widetilde{\bm{K}_{\bm{\Theta}}}(\bm{x})\right)\), since

\[\left\|\bm{V}\sigma(\bm{W}\bm{t}(\bm{x})+\bm{b})\right\|_{2}^{2} =\sum_{i=1}^{m}\sum_{j_{1}=1}^{n}\sum_{j_{2}=1}^{n}v_{ij_{1}}v_{ ij_{2}}\sigma\big{(}\bm{w}_{j_{1}}^{\top}\bm{t}(x)+b_{j_{1}}\big{)}\sigma \big{(}\bm{w}_{j_{2}}^{\top}\bm{t}(x)+b_{j_{2}}\big{)}\] \[=\sum_{j_{1}=1}^{n}\sum_{j_{2}=1}^{n}\bm{v}_{,j_{1}}^{\top}\bm{v} _{,j_{2}}\sigma\big{(}\bm{w}_{j_{1}}^{\top}\bm{t}(x)+b_{j_{1}}\big{)}\sigma \big{(}\bm{w}_{j_{2}}^{\top}\bm{t}(x)+b_{j_{2}}\big{)}\] \[=\langle\bm{V}^{\top}\bm{V},\widetilde{\bm{K}_{\bm{\Theta}}}(\bm {x})\rangle_{F}\] \[=\mathrm{Tr}\left(\bm{V}^{\top}\bm{V}\widetilde{\bm{K}_{\bm{ \Theta}}}(\bm{x})\right),\]

where \(\bm{v}_{\cdot,j_{1}}\) denotes the \(j_{1}\)th column of \(\bm{V}\) and \(\langle\cdot,\cdot\rangle_{F}\) denotes the Frobenius inner product. Therefore using the definition (2) directly and the linearity of the Frobenius inner product, the normalising constant is

\[\mathrm{z}(\bm{V},\bm{\Theta}) =\int_{\mathbb{X}}\left\|\bm{V}\sigma(\bm{W}\bm{t}(\bm{x})+\bm{b} )\right\|_{2}^{2}\mu(d\bm{x})\] \[=\int_{\mathbb{X}}\mathrm{Tr}\left(\bm{V}^{\top}\bm{V}\widetilde{ \bm{K}_{\bm{\Theta}}}(\bm{x})\right)\mu(d\bm{x})\] \[=\mathrm{Tr}\left(\bm{V}^{\top}\bm{V}\bm{K}_{\bm{\Theta}}\right).\] (13)

**Proposition 2**.: _Let \(\sigma(u)=\exp(u/2)\) and define the log-normalising constant as \(\Psi=\log\mathrm{z}(\bm{V},\bm{\Theta})\)._

_Then_

\[\sum_{i=1}^{n}\frac{\partial\Psi}{\partial\bm{w}_{i}}=\mathbb{E}\left[\bm{t} \left(\mathrm{x}\right)\right]\quad\text{and}\quad\sum_{i=1}^{n}\sum_{j=1}^{n} \frac{\partial^{2}\Psi}{\partial\bm{w}_{i}\bm{w}_{j}^{\top}}=\mathbb{E}\left[ \bm{t}\left(\mathrm{x}\right)\bm{t}\left(\mathrm{x}\right)^{\top}\right]- \mathbb{E}\left[\bm{t}\left(\mathrm{x}\right)\right]\mathbb{E}\left[\bm{t} \left(\mathrm{x}\right)\right]^{\top}.\]

Proof.: The result follows by noticing that the logarithmic derivative property holds, \(\sum_{i=1}^{n}\frac{\partial\Psi}{\partial\bm{w}_{i}}=\frac{1}{x}\sum_{i=1}^{ n}\frac{\partial\bm{x}}{\partial\bm{w}_{i}}\), and that by writing

\[\mathrm{z} = \sum_{i,j}\bm{v}_{\cdot,\bm{i}}^{\top}\bm{v}_{\cdot,\bm{j}}\int \exp\left(\frac{1}{2}\bm{w}_{i}^{\top}\bm{t}(\bm{x})\right)\exp\left(\frac{1}{2 }\bm{w}_{j}^{\top}\bm{t}(\bm{x})\right)\mu\left(d\bm{x}\right),\]

we obtain

\[\frac{\partial\,\mathrm{z}}{\partial\bm{w}_{i}} = \left\|\bm{v}_{\cdot,\bm{i}}\right\|^{2}\int\bm{t}(\bm{x})\exp \left(\bm{w}_{i}^{\top}\bm{t}(\bm{x})\right)\mu\left(d\bm{x}\right)\] \[= \bm{v}_{\cdot,\bm{i}}^{\top}\sum_{j}\bm{v}_{\cdot,\bm{j}}\int\bm{t }(\bm{x})\exp\left(\frac{1}{2}\bm{w}_{i}^{\top}\bm{t}(\bm{x})\right)\exp \left(\frac{1}{2}\bm{w}_{j}^{\top}\bm{t}(\bm{x})\right)\mu\left(d\bm{x}\right).\]

Now summing across \(i\) gives \(\sum_{i=1}^{n}\frac{\partial\Psi}{\partial\bm{w}_{i}}=\int\bm{t}(\bm{x})P(d\bm{x})\) as required.

To obtain the second result, we apply the product rule to find

\[\sum_{i,j}\frac{\partial^{2}\Psi}{\partial\bm{w}_{i}\bm{w}_{j}^{\top}} = \frac{1}{\text{z}}\sum_{i,j}\frac{\partial^{2}\,\text{z}}{\partial \bm{w}_{i}\bm{w}_{j}^{\top}}-\left(\frac{1}{\text{z}}\sum_{i}\frac{\partial\, \text{z}}{\partial\bm{w}_{i}}\right)\left(\frac{1}{\text{z}}\sum_{j}\frac{ \partial\,\text{z}}{\partial\bm{w}_{j}^{\top}}\right)\]

and note that

\[\frac{\partial^{2}\,\text{z}}{\partial\bm{w}_{i}\bm{w}_{j}^{\top}}=\begin{cases} \frac{1}{\text{z}}\bm{v}_{,i}^{\top}\bm{v}_{,j},\int\bm{t}(\bm{x})\bm{t}(\bm {x})^{\top}\exp\left(\frac{1}{\text{z}}\bm{w}_{i}^{\top}\bm{t}(\bm{x})\right) \exp\left(\frac{1}{\text{z}}\bm{w}_{j}^{\top}\bm{t}(\bm{x})\right)\mu\left(d \bm{x}\right),&\text{if }i\neq j,\\ \|\bm{v}_{,i}\|^{2}\int\bm{t}(\bm{x})\bm{t}(\bm{x})^{\top}\exp\left(\bm{w}_{ i}^{\top}\bm{t}(\bm{x})\right)\mu\left(d\bm{x}\right)\\ +\frac{1}{\text{z}}\bm{v}_{,i}^{\top}\sum_{\neq i}\bm{v}_{,j}\int\bm{t}(\bm{x })\bm{t}(\bm{x})^{\top}\exp\left(\frac{1}{\text{z}}\bm{w}_{i}^{\top}\bm{t}(\bm {x})\right)\exp\left(\frac{1}{\text{z}}\bm{w}_{j}^{\top}\bm{t}(\bm{x})\right) \mu\left(d\bm{x}\right),&\text{if }i=j.\end{cases}\]

Thus,

\[\frac{1}{\text{z}}\sum_{i,j}\frac{\partial^{2}\,\text{z}}{ \partial\bm{w}_{i}\bm{w}_{j}^{\top}} = \frac{1}{\text{z}}\sum_{i=1}^{n}\left(\frac{\partial^{2}\,\text{ z}}{\partial\bm{w}_{i}\bm{w}_{i}^{\top}}+\sum_{j\neq i}\frac{\partial^{2}\,\text{z}}{ \partial\bm{w}_{i}\bm{w}_{j}^{\top}}\right)\] \[= \frac{1}{\text{z}}\sum_{i,j}\bm{v}_{i}^{\top}\bm{v}_{j}\int\bm{t} (\bm{x})\bm{t}(\bm{x})^{\top}\exp\left(\frac{1}{2}\bm{w}_{i}^{\top}\bm{t}(\bm {x})\right)\exp\left(\frac{1}{2}\bm{w}_{j}^{\top}\bm{t}(\bm{x})\right)\mu\left(d \bm{x}\right)\] \[= \int\bm{t}(\bm{x})\bm{t}(\bm{x})^{\top}P(d\bm{x}),\]

as required.

**Remark 2**.: _Given the above relationship between the log-normalising constant and the expectation of the sufficient statistic, we can also ask whether the maximum likelihood estimation of the mean parameters \(\mathbb{E}\left[\bm{t}\left(\chi\right)\right]\) proceeds in the same way as in the exponential family case. The answer is positive but with two caveats. First, the log-likelihood need not be concave in \(\bm{W}\), and may have many local optima or stationary points. Second, unlike the exponential family distribution, the \(\mathfrak{SREFY}\) distribution is not determined by its mean parameters, so the MLE estimation of the mean parameters may not constitute a meaningful task in \(\mathfrak{SREFY}\) modelling (unless we are in the case where precisely the expectation of \(\bm{t}(\bm{x})\) under the \(\mathfrak{SREFY}\) model is of interest)._

**Corollary 1**.: _Given a dataset \(\{\bm{x}_{\ell}\}_{\ell=1}^{N}\), and a \(\mathfrak{SREFY}\) model with \(\sigma(u)=\exp(\frac{1}{2}u)\), assume that all rows of the maximum likelihood estimator of \(\bm{W}\) are in the interior of the natural parameter space of the corresponding exponential family. Denote the mean parameter as \(\bm{m}=\mathbb{E}\left[\bm{t}\left(\bm{x}\right)\right]\). Then the maximum likelihood estimate of \(\bm{m}\) is \(\hat{\bm{m}}=\frac{1}{N}\sum_{\ell=1}^{N}\bm{t}(\bm{x}_{\ell})\)._

Proof.: Since MLE is achieved at a stationary point of the log-likelihood, the proof follows by writing the log likelihood as

\[\sum_{\ell=1}^{N}\log p(\bm{x}_{\ell};\bm{V},\bm{\Theta})=\text{const}+\sum_{ \ell=1}^{N}\log\|\bm{f}(\bm{t}(\bm{x}_{\ell});\bm{V},\bm{\Theta})\|_{2}^{2}-N\Psi\]

and concluding that at the MLE \(\{\bm{w}_{i}^{*}\}_{i=1}^{n}\) for \(\bm{W}\), we must have

\[\sum_{\ell=1}^{N}\frac{\partial\log\|\bm{f}(\bm{t}(\bm{x}_{\ell});\bm{V},\bm{ \Theta})\|_{2}^{2}}{\partial\bm{w}_{i}^{*}}=N\frac{\partial\Psi}{\partial\bm{ w}_{i}^{*}},\quad i=1,\dots,n.\] (14)

But

\[\sum_{i=1}^{n}\frac{\partial\log\|\bm{f}(\bm{t}(\bm{x}_{\ell});\bm{V},\bm{ \Theta})\|_{2}^{2}}{\partial\bm{w}_{i}}=\bm{t}(\bm{x}_{\ell}),\]

so the result follows by summing (14) over \(i\) and dividing by \(N\). Note that maximum likelihood estimates are invariant to transformations, even if the transformation is not bijective. So if \(\{\bm{w}_{i}^{*}\}_{i=1}^{n}\) is an MLE, we may construct a mapping from \(\{\bm{w}_{i}^{*}\}_{i=1}^{n}\) to a corresponding MLE \(\hat{\bm{m}}\) for the mean parameter.

**Theorem 1**.: _Let \(\mathbf{x}=(\mathbf{x}_{1},\mathbf{x}_{2})\) be jointly \(\texttt{{SWEFY}}_{\mathbb{X}_{1}\times\mathbb{X}_{2},\mathbf{t},\sigma,\mu}\) with parameters \(\bm{V}\) and \(\bm{\Theta}=(\left[\bm{W}_{1},\bm{W}_{2}\right],\bm{b})\). Assume that \(\mu(d\bm{x})=\mu_{1}(d\bm{x}_{1})\mu_{2}(d\bm{x}_{2})\) and \(\bm{t}(\bm{x})=\big{(}\bm{t}_{1}(\bm{x}_{1}),\bm{t}_{2}(\bm{x}_{2})\big{)}\). Then the conditional distribution of \(\mathbf{x}_{1}\) given \(\mathbf{x}_{2}=\bm{x}_{2}\) is \(\texttt{{SWEFY}}_{\mathbb{X}_{1},\mathbf{t}_{1},\sigma,\mu_{1}}\) with parameters \(\bm{V}\) and \(\bm{\Theta}_{1|2}\triangleq(\bm{W}_{1},\bm{W}_{2}\bm{t}_{2}(\bm{x}_{2})+\bm{b})\)._

Proof.: The joint distribution of \(\mathbf{x}\) satisfies

\[P(d\bm{x};\bm{V},\bm{\Theta})\propto\Big{\|}\bm{V}\sigma\big{(}\bm{W}_{1}\bm{t }_{1}(\bm{x}_{1})+\bm{W}_{2}\bm{t}_{2}(\bm{x}_{2})+\bm{b}\big{)}\Big{\|}_{2}^{2 }\mu(d\bm{x}).\]

Therefore, the distribution of \(\mathbf{x}_{1}\) conditionally on \(\mathbf{x}_{2}=\bm{x}_{2}\), which is obtained by dividing the joint distribution by the marginal distribution of \(\mathbf{x}_{2}\) (which is independent of \(\bm{x}_{1}\)), satisfies

\[P_{1}\Big{(}d\bm{x}_{1}\mid\bm{x}_{2};\bm{V},\big{(}\bm{W}_{1},\bm{W}_{2}\bm{t }_{2}(\bm{x}_{2})+\bm{b}\big{)}\Big{)}\propto\Big{(}\bm{V}\sigma\big{(}\bm{W}_{ 1}\bm{t}_{1}(\bm{x}_{1})+\bm{W}_{2}\bm{t}_{2}(\bm{x}_{2})+\bm{b}\big{)}\Big{)}^ {2}\mu_{1}(d\bm{x}_{1}).\]

That is, the term \(\bm{W}_{2}\bm{t}_{2}(\bm{x}_{2})+\bm{b}\) is viewed as a constant bias term when the expression on the right hand side is an unnormalised measure with respect to the variable \(\bm{x}_{1}\). 

**Theorem 2**.: _Let \(\mathbf{x}=(\mathbf{x}_{1},\mathbf{x}_{2})\) be jointly \(\texttt{{SWEFY}}_{\mathbb{X}_{1}\times\mathbb{X}_{2},\mathbf{t},\sigma,\mu}\) with parameters \(\bm{V}\) and \(\bm{\Theta}=(\left[\bm{W}_{1},\bm{W}_{2}\right],\bm{b})\). Assume that \(\mu(d\bm{x})=\mu_{1}(d\bm{x}_{1})\mu_{2}(d\bm{x}_{2})\) and \(\bm{t}(\bm{x})=\big{(}\bm{t}_{1}(\bm{x}_{1}),\bm{t}_{2}(\bm{x}_{2})\big{)}\). Then the marginal distribution of \(\mathbf{x}_{1}\) is_

\[P_{1}(d\bm{x}_{1})=\frac{\mathrm{Tr}\left(\bm{V}^{\top}\bm{V}\widetilde{\bm{C} }_{\bm{\Theta}}(\bm{x}_{1})\right)}{\mathrm{z}(\bm{V},\bm{\Theta})}\mu_{1}(d \bm{x}_{1}),\]

_where \(\widetilde{C}(\bm{x}_{1})_{ij}=k_{\sigma,\bm{t}_{2},\mu_{2}}\Big{(}\big{(}\bm{ w}_{2i},\bm{w}_{1i}^{\top}\bm{t}_{1}(\bm{x}_{1})+b_{i}\big{)},\big{(}\bm{w}_{2j}, \bm{w}_{1j}^{\top}\bm{t}_{1}(\bm{x}_{1})+b_{j}\big{)}\Big{)}\)._

Proof.: The marginal distribution of the random variable \(\mathbf{x}_{1}\) is obtained by marginalising out the joint distribution with respect to \(\bm{x}_{2}\),

\[P_{1}(d\bm{x}_{1})=\frac{1}{\mathrm{z}(\bm{V},\bm{\Theta})}\underbrace{\Bigg{(} \int_{\mathbb{X}_{2}}\Big{\|}\bm{V}\sigma\big{(}\bm{W}_{1}\bm{t}_{1}(\bm{x}_{ 1})+\bm{W}_{2}\bm{t}_{2}(\bm{x}_{2})+\bm{b}\big{)}\Big{\|}_{2}^{2}\mu_{2}(d \bm{x}_{2})\Bigg{)}}_{\triangleq_{\mathbf{x}_{2}}}\mu_{1}(d\bm{x}_{1}).\]

The integral \(\mathrm{z}_{2}\) takes a similar form to \(\mathrm{z}(\bm{V},\bm{\Theta})\),

\[\mathrm{z}_{2} =\mathrm{Tr}\left(\bm{V}^{\top}\bm{V}\widetilde{\bm{C}}_{\bm{ \Theta}}(\bm{x}_{1})\right),\quad\text{where}\] \[\widetilde{C}_{ij}(\bm{x}_{1}) =\int_{\mathbb{X}_{2}}\sigma\big{(}\bm{w}_{1i}^{\top}\bm{t}_{1}( \bm{x}_{1})+\bm{w}_{2i}^{\top}\bm{t}_{2}(\bm{x}_{2})+b_{i}\big{)}\sigma\big{(} \bm{w}_{1j}^{\top}\bm{t}_{1}(\bm{x}_{1})+\bm{w}_{2j}^{\top}\bm{t}_{2}(\bm{x}_{2 })+b_{j}\big{)}\mu_{2}(d\bm{x}_{2})\] \[=k_{\sigma,\bm{t}_{2},\mu_{2}}\Big{(}\big{(}\bm{w}_{2i},\bm{w}_{1 i}^{\top}\bm{t}_{1}(\bm{x}_{1})+b_{i}\big{)},\big{(}\bm{w}_{2j},\bm{w}_{1j}^{\top}\bm{t}_{1}( \bm{x}_{1})+b_{j}\big{)}\Big{)}.\]

## Appendix B Derivation of neural network kernels

**Kernel 1**.: \(k_{\sigma,\mathrm{Id},\Phi_{\bm{C},m}}(\bm{\theta}_{i},\bm{\theta}_{j})=k_{\sigma, \mathrm{Id},\Phi}(\mathcal{T}\bm{\theta}_{i},\mathcal{T}\bm{\theta}_{j})\)_, where \(\mathcal{T}\bm{\Theta}=(\bm{W}\bm{A},\bm{b}+\bm{W}\bm{m})\), \(\mathcal{T}\bm{\theta}_{i}=(\bm{w}_{i}^{\top}\bm{A},b_{i}+\bm{w}_{i}^{\top}\bm{m})\) and \(\bm{A}\) is a matrix factor such that covariance \(\bm{C}=\bm{A}\bm{A}^{\top}\)._

Proof.: The NNK may be expressed as an expectation with respect to a Gaussian random variable \(\mathbf{x}\) with mean \(\bm{m}\) and covariance matrix \(\bm{C}\). It holds that \(\mathbf{x}\overset{d}{=}\bm{A}\mathbf{z}+\bm{m}\), where \(\mathbf{z}\) is a zero-mean independent standard Gaussian random vector, so the kernel may be expressed in terms of an expectation over \(\mathbf{z}\) instead. More concretely,

\[k_{\sigma,\mathrm{Id},\Phi_{\bm{C},m}}(\bm{\theta}_{i},\bm{\theta}_{j}) =\mathbb{E}_{\mathbf{z}}\big{[}\sigma(\bm{w}_{i}^{\top}\mathbf{x }+b_{i})\sigma(\bm{w}_{j}^{\top}\mathbf{x}+b_{j})\big{]}\] \[=\mathbb{E}_{\mathbf{z}}\big{[}\sigma(\bm{w}_{i}^{\top}\bm{A} \mathbf{z}+\bm{w}_{i}^{\top}\bm{m}+b_{i})\sigma(\bm{w}_{j}^{\top}\bm{A} \mathbf{z}+\bm{w}_{j}^{\top}\bm{m}+b_{j})\big{]}\] \[=k_{\sigma,\mathrm{Id},\Phi}(\mathcal{T}\bm{\theta}_{i},\mathcal{T} \bm{\theta}_{j}).\]

**Kernel 2**.: \(k_{\mathrm{cos,Id,\Phi}}(\bm{\theta}_{i},\bm{\theta}_{j})=\frac{\cos\left[b_{i}-b_{j }\right]}{2}\exp\big{(}\frac{-\|\bm{w}_{j}-\bm{w}_{j}\|^{2}}{2}\big{)}+\frac{ \cos\left[b_{i}+b_{j}\right]}{2}\exp\big{(}\frac{-\|\bm{w}_{j}+\bm{w}_{j}\|^{2 }}{2}\big{)}.\)__

Proof.: First observe that the expected value of the cosine of a Gaussian random variable can be evaluated by equating the real and imaginary components of the characteristic function of a Gaussian random variable and the expected value of Euler's form. That is, if z is Gaussian with mean \(m\) and variance \(v^{2}\),

\[\mathbb{E}e^{i\bm{x}}=\mathbb{E}[\cos(\bm{z})]+i\mathbb{E}[\sin( \bm{z})] =e^{im-\frac{1}{2}v^{2}}\] \[=(\cos\mu+i\sin m)e^{-\frac{1}{2}v^{2}}\] \[\implies\mathbb{E}[\cos(\bm{z})] =\cos(m)e^{-\frac{1}{2}v^{2}}.\]

With this identity at hand, we proceed by direct evaluation of (4).

\[k_{\mathrm{cos,Id,\Phi}}(\bm{\theta}_{i},\bm{\theta}_{j}) =\mathbb{E}_{\bm{x}}\big{[}\cos(\bm{w}_{i}^{\top}\bm{x}+b_{i}) \cos(\bm{w}_{j}^{\top}\bm{x}+b_{j})\big{]},\quad\bm{x}\sim\mathcal{N}\big{(} \bm{0},\bm{I}\big{)}\] \[=\frac{1}{2}\mathbb{E}_{\bm{x}}\big{[}\cos\big{(}(\bm{w}_{i}-\bm {w}_{j})^{\top}\bm{x}+(b_{i}-b_{j})\big{)}+\cos\big{(}(\bm{w}_{i}+\bm{w}_{j})^ {\top}\bm{x}+(b_{i}+b_{j})\big{)}\big{]}\] \[=\frac{1}{2}\cos\left|b_{i}-b_{j}\right|\exp\big{(}-\frac{1}{2}\| \bm{w}_{i}-\bm{w}_{j}\|^{2}\big{)}+\cos|b_{i}+b_{j}|\exp\big{(}-\frac{1}{2}\| \bm{w}_{i}+\bm{w}_{j}\|^{2}\big{)}.\]

**Kernel 4**.: \(k_{\mathrm{Id,Id,\Phi}}(\bm{\theta}_{i},\bm{\theta}_{j})=\bm{w}_{i}^{\top}\bm {w}_{j}+b_{i}b_{j}.\)__

Proof.: This is immediate from the expected value of a product of two correlated Gaussians, \(\bm{w}_{i}^{\top}\bm{x}+b_{i}\) and \(\bm{w}_{j}^{\top}\bm{x}+b_{j}\). 

**Kernel 5**.: _The kernel \(k_{\mathrm{Snake_{a}}(\cdot)-\frac{1}{2a},\mathrm{Id,\Phi}}(\bm{\theta}_{i}, \bm{\theta}_{j})\) is equal to_

\[\frac{1}{4a^{2}}k_{\mathrm{cos,Id,\Phi}}(2a\bm{\theta}_{i},2a\bm {\theta}_{j})+\bm{w}_{j}^{\top}\bm{w}_{j}\Big{(}\sin(2ab_{j})e^{-2a^{2}\|\bm{w }_{j}\|^{2}}+\sin(2ab_{i})e^{-2a^{2}\|\bm{w}_{i}\|^{2}}\Big{)}\] \[\quad-\frac{b_{i}}{2a}\cos(2ab_{j})e^{-2a^{2}\|\bm{w}_{j}\|^{2}}- \frac{b_{j}}{2a}\cos(2ab_{i})e^{-2a^{2}\|\bm{w}_{i}\|^{2}}+k_{\mathrm{Id,Id, \Phi}}(\bm{\theta}_{i}^{(1)},\bm{\theta}_{j}^{(1)}).\]

Proof.: Choosing \(\sigma=\mathrm{Snake_{a}}(\cdot)-\frac{1}{2a}\) in (4), and expanding the resulting quadratic,

\[k_{\mathrm{Snake_{a}}(\cdot)-\frac{1}{2a},\mathrm{Id}}(\bm{\theta }_{i},\bm{\theta}_{j})\] \[=\underbrace{\frac{1}{4a^{2}}\mathbb{E}_{\bm{x}}\big{[}\cos\big{(} 2a(\bm{w}_{i}^{\top}\bm{x}+b_{i})\big{)}\cos\big{(}2a(\bm{w}_{j}^{\top}\bm{x}+b _{j})\big{)}\big{]}}_{\triangleq A}-\underbrace{\frac{1}{2a}\mathbb{E}_{\bm{x }}\big{[}(\bm{w}_{i}^{\top}\bm{x}+b_{i})\cos\big{(}2a(\bm{w}_{j}^{\top}\bm{x}+ b_{j})\big{)}\big{]}}_{\triangleq A}-\] \[\underbrace{\frac{1}{2a}\mathbb{E}_{\bm{x}}\big{[}\cos\big{(}2a( \bm{w}_{i}^{\top}\bm{x}+b_{i})\big{)}(\bm{w}_{j}^{\top}\bm{x}+b_{j})\big{]}}_{ \triangleq B}+\underbrace{\mathbb{E}_{\bm{x}}\big{[}(\bm{w}_{i}^{\top}\bm{x}+ b_{i})(\bm{w}_{j}^{\top}\bm{x}+b_{j})\big{]}}_{\text{Kernel 4}}.\] (15)

We now evaluate \(A\) and \(B\). The terms \(A\) and \(B\) obey a symmetry, so it suffices to evaluate term \(A\). Term \(A\) can be evaluated using Stein's lemma,

\[A =\frac{1}{2a}\mathbb{E}\big{[}(z_{1}+b_{i})\cos(z_{2}+2ab_{j}) \big{]},\quad(z_{1},z_{2})^{\top}\sim\mathcal{N}\Bigg{(}\begin{pmatrix}0\\ 0\end{pmatrix},\begin{pmatrix}\bm{w}_{i}^{\top}\bm{w}_{i}&2a\bm{w}_{i}^{\top}\bm{w }_{j}\\ 2a\bm{w}_{j}^{\top}\bm{w}_{i}&4a^{2}\bm{w}_{j}^{\top}\bm{w}_{j}\end{pmatrix} \Bigg{)}\] \[=\frac{1}{2a}\mathbb{E}\big{[}z_{1}\cos(z_{2}+2ab_{j})\big{]}+ \frac{b_{i}}{2a}\mathbb{E}\big{[}\cos(z_{2}+2ab_{j})\big{]}\] \[=-\bm{w}_{i}^{\top}\bm{w}_{j}\mathbb{E}\big{[}\sin(z_{2}+2ab_{j}) \big{]}+\frac{b_{i}}{2a}\mathbb{E}\big{[}\cos(z_{2}+2ab_{j})\big{]}\] \[=-\bm{w}_{i}^{\top}\bm{w}_{j}\sin(2ab_{j})\exp(-2a^{2}\|\bm{w}_{j} \|^{2})+\frac{b_{i}}{2a}\cos(2ab_{j})\exp(-2a^{2}\|\bm{w}_{j}\|^{2}).\]Assembling all the known individual terms in (15),

\[k_{\mathrm{Snake}_{a}(\cdot)-\frac{1}{2a},\mathrm{Id}}(\bm{\theta}_ {i},\bm{\theta}_{j})\] \[=\frac{1}{4a^{2}}k_{\mathrm{cos,Id}}(2a\bm{\theta}_{i},2a\bm{ \theta}_{j})+\bm{w}_{i}^{\top}\bm{w}_{j}\Big{(}\sin(2ab_{j})\exp(-2a^{2}\|\bm{ w}_{j}\|^{2})+\sin(2ab_{i})\exp(-2a^{2}\|\bm{w}_{i}\|^{2})\Big{)}\] \[\quad-\frac{b_{i}}{2a}\cos(2ab_{j})\exp(-2a^{2}\|\bm{w}_{j}\|^{2} )-\frac{b_{j}}{2a}\cos(2ab_{i})\exp(-2a^{2}\|\bm{w}_{i}\|^{2})+k_{\mathrm{Id,Id }}(\bm{\theta}_{i}^{(1)},\bm{\theta}_{j}^{(1)}).\]

**Kernel 6**.: _The kernel \(k_{\mathrm{Snake}_{a},\mathrm{Id},\Phi}(\bm{\theta}_{i},\bm{\theta}_{j})\) is equal to_

\[\frac{1}{2a}\Big{(}b_{i}-\frac{1}{2a}\cos(2ab_{i})\exp(-2a^{2}\| \bm{w}_{i}\|^{2})+b_{j}-\frac{1}{2a}\cos(2ab_{j})\exp(-2a^{2}\|\bm{w}_{j}\|^{2 })\Big{)}\] \[\quad+k_{\mathrm{Snake}_{a}(\cdot)-\frac{1}{2a},\mathrm{Id},\Phi} (\bm{\theta}_{i},\bm{\theta}_{j})+\frac{1}{4a^{2}}.\]

Proof.: Choose \(\sigma=\mathrm{Snake}_{a}\) and note that \(\mathrm{Snake}_{a}(\cdot)=\Big{(}\,\mathrm{Snake}_{a}(\cdot)-\frac{1}{2a} \Big{)}+\frac{1}{2a}\). The Kernel 5 corresponds with the case \(\Big{(}\,\mathrm{Snake}_{a}(\cdot)-\frac{1}{2a}\Big{)}\), so we are left with three additional terms. These terms may be evaluated directly,

\[k_{\mathrm{Snake}_{a},\mathrm{Id}}(\bm{\theta}_{i},\bm{\theta}_ {j})\] \[=k_{\mathrm{Snake}_{a}(\cdot)-\frac{1}{2a},\mathrm{Id}}(\bm{ \theta}_{i},\bm{\theta}_{j})+\frac{1}{4a^{2}}+\] \[\quad\frac{1}{2a}\Big{(}b_{i}-\frac{1}{2a}\cos(2ab_{i})\exp(-2a^{ 2}\|\bm{w}_{i}\|^{2})+b_{j}-\frac{1}{2a}\cos(2ab_{j})\exp(-2a^{2}\|\bm{w}_{j} \|^{2})\Big{)}.\]

## Appendix C Examples which generalise standard exponential family models

In this section, we will study examples of the SNEFY model which use activation function \(\sigma(u)=\exp(u/2)\) (or equivalently, up to scaling, \(\sigma(u)=\exp(u)\)) and as such correspond to a notion of exponential family mixture models allowing negative weights, as discussed in Section 3.2. These examples have tractable kernels whenever the corresponding exponential family has a tractable normalising constant and we can write the kernels directly using Proposition 1.

SNEFY Von Mises-Fisher mixtures.The VMF distribution is a helpful way of defining the notion of a Gaussian distribution to the sphere. The following kernel may be used to define a VMF distribution. Alternatively, it may be viewed as a way of constructing a distribution supported on \(\mathbb{R}^{d}\) with sufficient statistics which are projected onto the sphere.

**Kernel 3**.: _Define \(\text{proj}_{\mathbb{S}^{d-1}}(\bm{x})\triangleq\bm{x}/\|\bm{x}\|\) to be the projection onto the unit sphere. Then_

\[k_{\exp,\text{proj}_{\mathbb{S}^{d-1}},\Phi}(\bm{\theta}_{i},\bm{\theta}_{j} )=\exp(b_{i}+b_{j})\frac{\Gamma(d/2)2^{d/2-1}I_{d/2-1}\big{(}\|\bm{w}_{i}+\bm{ w}_{j}\|\big{)}}{\|\bm{w}_{i}+\bm{w}_{j}\|^{d/2-1}},\]

_where \(I_{p}\) is the modified Bessel function of the first kind of order \(p\). In the special case \(d=3\), we have the closed-form \(k_{\exp,\text{proj}_{\mathbb{S}^{2}},\Phi}(\bm{\theta}_{i},\bm{\theta}_{j})= \exp(b_{i}+b_{j})\frac{\big{(}e\|\bm{w}_{i}+\bm{w}_{j}\|-e^{-}\|\bm{w}_{i}+\bm{ w}_{j}\|\big{)}}{2\|\bm{w}_{i}+\bm{w}_{j}\|}.\)_Proof.: If \(\textbf{x}\sim\mathcal{N}(0,\bm{I})\), then \(\textbf{x}/\|\textbf{x}\|\) is uniformly distributed on the sphere. From the normalizing constant of the von Mises-Fisher distribution, from Proposition 1, it then follows that

\[k_{\exp,\text{proj}_{\delta^{-1},\Phi}}(\bm{\theta}_{i},\bm{\theta }_{j}) =\mathbb{E}_{\textbf{x}}\big{[}\exp\big{(}\bm{w}_{i}^{\top}\textbf{x }/\|\textbf{x}\|+b_{i}+\bm{w}_{j}^{\top}\textbf{x}/\|\textbf{x}\|+b_{j}\big{)} \big{]},\quad\textbf{x}\sim\mathcal{N}\big{(}\textbf{0},\bm{I}\big{)}\] \[=\exp(b_{i}+b_{j})\int_{\mathbb{S}^{d-1}}\exp\big{(}(\bm{w}_{i}+ \bm{w}_{j})^{\top}\textbf{x}\big{)}\,d\textbf{x}\frac{\Gamma(d/2)}{2\pi^{d/2}}\] \[=\frac{\exp(b_{i}+b_{j})\Gamma(d/2)}{2\pi^{d/2}}\int_{\mathbb{S}^ {d-1}}\exp\big{(}\|\bm{w}_{i}+\bm{w}_{j}\|\bm{a}^{\top}\textbf{x}\big{)}\,d \textbf{x},\quad\text{where $\bm{a}$ is a unit vector}\] \[=\frac{\exp(b_{i}+b_{j})\Gamma(d/2)}{2\pi^{d/2}}\frac{(2\pi)^{d/2 }I_{d/2-1}(\|\bm{w}_{i}+\bm{w}_{j}\|)}{\|\bm{w}_{i}+\bm{w}_{j}\|^{d/2-1}}\] \[=\exp(b_{i}+b_{j})\frac{\Gamma(d/2)2^{d/2-1}I_{d/2-1}(\|\bm{w}_{i} +\bm{w}_{j}\|)}{\|\bm{w}_{i}+\bm{w}_{j}\|^{d/2-1}},\]

where \(I_{p}\) is the modified Bessel function of the first kind of order \(p\). In the special case of \(p=1/2\), we have \(I_{1/2}(z)=\sqrt{\frac{2}{\pi z}}\sinh(z)=\big{(}\exp(z)-\exp(-z)\big{)}\sqrt{ \frac{1}{2\pi z}}\). This implies that when \(d=3\), since \(\Gamma(3/2)=\frac{\sqrt{\pi}}{2}\),

\[k_{\exp,\text{proj}_{\delta^{2},\Phi}}(\bm{\theta}_{i},\bm{\theta }_{j})=\exp(b_{i}+b_{j})\frac{\big{(}e^{\|\bm{w}_{i}+\bm{w}_{j}\|}-e^{-\|\bm{w} _{i}+\bm{w}_{j}\|}\big{)}}{2\|\bm{w}_{i}+\bm{w}_{j}\|}.\]

Note that \(k_{\exp,\text{proj}_{\delta^{d-1},\Phi}}(\bm{\theta}_{i},\bm{\theta}_{j})=k_{ \exp,\text{Id},\nu}(\bm{\theta}_{i},\bm{\theta}_{j})\), where \(\nu\) is the uniform measure on the sphere \(\mathbb{S}^{d-1}\), because if **x** is Gaussian then \(\textbf{x}/\|\textbf{x}\|\) is uniform on the sphere. This allows one to construct \(\texttt{SNEFY}_{\exp,\text{Id},\nu}\) distributions, which are certain "mixtures" of VMF distributions with weights \(\bm{V}^{\top}\bm{V}\).

Susefy **Gaussian mixtures, fixed variance.** We may similarly construct kernels corresponding to "mixtures" of Gaussian distributions. The case here corresponds to a case of known fixed variance parameter.

**Kernel 7**.: \(k_{\exp,\text{Id},\Phi}(\bm{\theta}_{i},\bm{\theta}_{j})=\exp(b_{i}+b_{j})\exp \big{(}\frac{1}{2}\|\bm{w}_{i}+\bm{w}_{j}\|^{2}\big{)}\)_._

Proof.: This is a consequence of the moment generating function of the multivariate Gaussian distribution. More concretely, by Proposition 1,

\[k_{\exp,\text{Id},\Phi}(\bm{\theta}_{i},\bm{\theta}_{j}) =\mathbb{E}_{\textbf{x}}\big{[}\exp\big{(}\bm{w}_{i}^{\top}\textbf{ x}+b_{i}+\bm{w}_{j}^{\top}\textbf{x}+b_{j}\big{)}\big{]},\quad\textbf{x}\sim \mathcal{N}\big{(}\textbf{0},\bm{I}\big{)}\] \[=\exp(b_{i}+b_{j})\exp\big{(}\frac{1}{2}\|\bm{w}_{i}+\bm{w}_{j}\|^ {2}\big{)}.\]

Susefy **Poisson mixtures.** Most of our examples deal with continuous distributions but in fact SNEFY can readily be used for discrete distribution modelling. This is particularly helpful when the support is large or infinite, for which computing normalising constants can naively be challenging even in the discrete setting. Let \(\mathbb{X}=\{0,1,2,\ldots\}\), \(t(x)=x\), and the base measure \(\mu(dx)=1/x!\,\nu(dx)\), where \(\nu\) is the counting measure. A SNEF model for a probability mass function which is a mixture of Poisson distributions allowing negative weights is given by

\[p(x;\bm{V},\bm{w}) =\frac{1}{\operatorname{Tr}\left(\bm{V}^{\top}\bm{V}\bm{K}_{\bm{ \Theta}}\right)}\frac{1}{x!}\sum_{i=1}^{n}\sum_{j=1}^{n}\bm{v}_{..i}^{\top} \bm{v}_{..j}\exp\left((w_{i}+w_{j})x\right)\] \[=\frac{1}{\operatorname{Tr}\left(\bm{V}^{\top}\bm{V}\bm{K}_{\bm{ \Theta}}\right)}\frac{1}{x!}\sum_{i=1}^{n}\sum_{j=1}^{n}\bm{v}_{..i}^{\top} \bm{v}_{..j}(\lambda_{i}\lambda_{j})^{x},\quad x=0,1,2,\ldots\]

following the usual mean parametrisation \(\lambda_{i}=e^{w_{i}}\), so the individual mixture components have rates which are geometric means of \((\lambda_{i},\lambda_{j})\) pairs.

**Kernel 8**.: _Choose the base measure \(\mu(dx)=(x!)^{-1}\,\nu(dx)\), where \(\nu\) is the counting measure. We have_

\[k_{\exp,\mathrm{Id},(x!)^{-1}\nu}(\bm{\theta}_{i},\bm{\theta}_{j})=\exp\Big{(}b_ {i}+b_{j}\Big{)}\exp\big{(}\exp(w_{i}+w_{j})\big{)}.\]

Proof.: This is again direct from Proposition 1. In detail,

\[k_{\exp,\mathrm{Id},(x!)^{-1}\nu}(\bm{\theta}_{i},\bm{\theta}_{j}) =\sum_{x=0}^{\infty}\frac{1}{x!}\exp\Big{(}w_{i}x+w_{j}x+b_{i}+b_{ j}\Big{)}\] \[=\exp\Big{(}b_{i}+b_{j}\Big{)}\sum_{x=0}^{\infty}\frac{1}{x!}\exp \Big{(}w_{i}x+w_{j}x\Big{)}\]

The second factor involving the sum is the partition function of the Poisson distribution in canonical form, which is \(\exp\big{(}\exp(w_{i}+w_{j})\big{)}\). 

The usual mean parameterisation of the Poisson distribution is through a rate parameter \(\lambda_{i}=\exp(w_{i})\), which would lead to the kernel representation

\[k_{\exp,\mathrm{Id},(x!)^{-1}\nu}(\bm{\theta}_{i},\bm{\theta}_{j})=\exp\Big{(} b_{i}+b_{j}\Big{)}\exp\big{(}\lambda_{i}\lambda_{j}\big{)}.\]

Snefy Gaussian mixtures, unknown variance (Squared radial basis function network).We now discuss an intriguing connection between the Gaussian distribution and squared RBF networks. This connection is made possible through our machinery of SNEFY distributions. Let \(\mathbb{X}=\mathbb{R}^{d}\), \(\bm{t}(\bm{x})=(x_{1},\dots,x_{d},x_{1}^{2},\dots,x_{d}^{2})\) (i.e. \(D=2d\)) and suppose \(\mu(d\bm{x})=d\bm{x}\) is Lebesgue measure. Choose \(\sigma(\cdot)=\exp(\cdot/2)\) and consider the \(r\)-th output of our network \(\bm{f}:\mathbb{R}^{D}\to\mathbb{R}^{m}\)

\[f_{r}(\bm{t}(\bm{x});\bm{V},\bm{\Theta})=\sum_{i=1}^{n}v_{ri}\exp\left(\frac{1 }{2}\left(\sum_{\ell=1}^{d}w_{i\ell}x_{\ell}+\sum_{\ell=1}^{d}\tilde{w}_{i\ell }x_{\ell}^{2}\right)\right),\]

where we denoted \(\tilde{w}_{i\ell}=w_{i,d+\ell}\). In this case, we require that \(\tilde{w}_{e\ell}<0\) for the model to be (square) integrable. Reparametrising \(\sigma_{i\ell}^{2}=-\frac{1}{2\tilde{w}_{i\ell}}\) and \(\mu_{i\ell}=-\frac{w_{i\ell}}{2\tilde{w}_{i\ell}}\) and absorbing the factor \(\exp\left(-\sum_{\ell=1}^{d}\frac{\mu_{i\ell}^{2}}{4\sigma_{i\ell}^{2}}\right)\) into readout parameters \(\bm{V}\), gives

\[f_{r}(\bm{t}(\bm{x});\bm{V},\bm{\Theta})=\sum_{i=1}^{n}v_{ri}\exp\left(-\sum_{ \ell=1}^{d}\frac{(x_{\ell}-\mu_{i\ell})^{2}}{4\sigma_{i\ell}^{2}}\right).\]

Thus, we have recovered a classical radial basis function (RBF) network [52]. These models are well known to have universal approximation properties [36]. For the most commonly used form of the RBF network, we can restrict the parameters \(\sigma_{i\ell}^{2}=\sigma_{i}^{2}\) to be the same across the dimensions, giving

\[f_{r}\left(\bm{t}(\bm{x});\bm{V},\bm{\Theta}\right)=\sum_{i=1}^{n}v_{ri}\exp \left(-\frac{\left\|\bm{x}-\bm{\mu}_{i}\right\|^{2}}{4\sigma_{i}^{2}}\right), \quad\bm{x}\in\mathbb{R}^{d},\]

with location parameters \(\bm{\mu}_{i}\in\mathbb{R}^{d}\) and the scale parameters \(\sigma_{i}^{2}>0\). Note the unusual factor of \(4\) in front of \(\sigma_{i}^{2}\) - this ensures that our model in fact reduces to the usual parametrisation of multivariate normal densities, since we will be modelling the density using the squared norm of \(\bm{f}\).

Since \(\mu\) is the Lebesgue measure, SNEFY gives us a density model with respect to the Lebesgue measure as

\[p(\bm{x};\bm{V},\bm{\Theta})=\frac{1}{\operatorname{Tr}\left(\bm{V}^{\top}\bm {V}\bm{K}_{\bm{\Theta}}\right)}\sum_{i=1}^{n}\sum_{j=1}^{n}\bm{v}_{\cdot,i}^{ \top}\bm{v}_{\cdot,j}\exp\left(-\frac{\left\|\bm{x}-\bm{\mu}_{i}\right\|^{2}}{ 4\sigma_{i}^{2}}\right)\exp\left(-\frac{\left\|\bm{x}-\bm{\mu}_{j}\right\|^{2} }{4\sigma_{j}^{2}}\right).\]

If \(n=1\), we recover simply a multivariate normal density \(\mathcal{N}\left(\bm{\mu}_{1},\sigma_{1}^{2}I\right)\). The above model is essentially the same as the one in [42], despite being derived in a very different way.

**Kernel 9**.: _Let \(\bm{t^{(2)}}(\bm{x})=(x_{1},\ldots,x_{d},x_{1}^{2},\ldots,x_{d}^{2})\) so that \(D=2d\) be the sufficient statistic. Partition \(\bm{W}=[\bm{W}_{[:,1:d]},\tilde{\bm{W}}]\) and suppose \(\tilde{\bm{W}}<0\) element-wise. Choose \(\mu(d\bm{x})=d\bm{x}\) to be the Lebesgue measure. Then_

\[k_{\exp,\bm{t^{(2)}},d\bm{x}}(\bm{\theta}_{i},\bm{\theta}_{j})=\pi^{d/2}\exp(b_{ i}+b_{j})\prod_{l=1}^{d}\exp\Big{(}-\frac{(w_{il}+w_{jl})^{2}}{4(\tilde{w}_{il}+ \tilde{w}_{jl})}\Big{)}\frac{1}{\sqrt{-(\tilde{w}_{il}+\tilde{w}_{jl})}}\]

Proof.: As with the kernels above, this follows from Proposition 1, since

\[k_{\exp,\bm{t^{(2)}},d\bm{x}}(\bm{\theta}_{i},\bm{\theta}_{j}) =\int_{\mathbb{R}^{d}}\exp\left(\bm{w}_{i,1:d}^{\top}\bm{x}+ \tilde{\bm{w}}_{i}^{\top}\bm{x}^{2}+b_{1}+\bm{w}_{j,1:d}^{\top}\bm{x}+\tilde{ \bm{w}}_{j}^{\top}\bm{x}^{2}+b_{j}\right)d\bm{x}\] \[=(2\pi)^{d/2}\exp(b_{i}+b_{j})\prod_{l=1}^{d}\exp\Big{(}-\frac{(w _{il}+w_{jl})^{2}}{4(\tilde{w}_{il}+\tilde{w}_{jl})}\Big{)}\frac{1}{\sqrt{-2( \tilde{w}_{il}+\tilde{w}_{jl})}}.\]

While Proposition 1 gives us an expression for the kernel matrix \(\bm{K_{\Theta}}\) in terms of natural parameters, we can also express it directly in terms of parameters \(\bm{\mu}_{i},o_{i}^{2}\). In particular,

\[[\bm{K_{\Theta}}]_{ij}= \exp(b_{i}+b_{j})\int_{\mathbb{R}^{d}}\exp\left(-\frac{\left\| \bm{x}-\bm{\mu}_{i}\right\|^{2}}{4\sigma_{i}^{2}}\right)\exp\left(-\frac{\left\| \bm{x}-\bm{\mu}_{j}\right\|^{2}}{4\sigma_{j}^{2}}\right)d\bm{x}\] \[= \exp(b_{i}+b_{j})\left(\frac{4\pi\sigma_{i}^{2}\sigma_{j}^{2}}{ \sigma_{i}^{2}+\sigma_{j}^{2}}\right)^{d/2}\exp\left(-\frac{\left\|\bm{\mu}_{i }-\bm{\mu}_{j}\right\|^{2}}{4\left(\sigma_{i}^{2}+\sigma_{j}^{2}\right)}\right).\]

We briefly state two more cases without an extended discussion.

Sinefy **Gamma mixtures.**

**Kernel 10**.: _Let \(\mathbb{X}=(0,\infty)\), \(\bm{t}(x)=(\log x,-x)\) and \(\sigma=\exp\). Partition \(\bm{W}=[\bm{W}_{[:,1:d]},\tilde{\bm{W}}]\) and suppose \(\bm{W}_{[:,1:d]}>-1\) and \(\tilde{\bm{W}}>0\) element-wise. Choose \(\mu(d\bm{x})=d\bm{x}\) to be the Lebesgue measure. Then_

\[k_{\exp,\bm{t},d\bm{x}}(\bm{\theta}_{i},\bm{\theta}_{j})=\exp(b_{i}+b_{j}) \frac{\Gamma\left(w_{i1}+w_{j1}+1\right)}{\left(w_{i2}+w_{j2}\right)^{w_{i1}+ w_{j1}+1}}.\]

Sinefy **Dirichlet mixtures.**

**Kernel 11**.: _Let \(\mathbb{X}=\Delta^{D-1}\), a \((D-1)\)-simplex of probability distributions, i.e. \(\bm{x}\in[0,1]^{D}\), \(\sum_{i=1}^{D}x_{i}=1\). Let \(\sigma=\exp\). Let \(\bm{t}(\bm{x})=(\log x_{1},\ldots,\log x_{D})\). Choose \(\mu(d\bm{x})=d\bm{x}\) to be the Lebesgue measure. Suppose \(\bm{W}>-1\) elementwise. Then_

\[k_{\exp,\bm{t},d\bm{x}}(\bm{\theta}_{i},\bm{\theta}_{j})=\exp(b_{i}+b_{j}) \frac{\prod_{d=1}^{D}\Gamma(w_{id}+w_{jd}+1)}{\Gamma\left(D+\sum_{d=1}^{D}w_{ id}+w_{jd}\right)}.\]

## Appendix D Marginalisation in the case \(\sigma=\exp(\cdot/2)\)

Let \(\bm{M}\) be a positive semi-definite \(n\times n\) matrix. We will make use of the following SNEFY parametrisation

\[P(d\bm{x};\bm{M},\bm{\Theta})=\frac{1}{\mathrm{z}(\bm{M},\bm{\Theta})}\sigma \left(\bm{W}\bm{t}(\bm{x})+\bm{b}\right)^{\top}\bm{M}\sigma\left(\bm{W}\bm{t}( \bm{x})+\bm{b}\right)\mu\left(d\bm{x}\right).\] (16)

Since we can always write \(\bm{M}=\bm{V}^{\top}\bm{V}\), for an \(m\times n\) matrix \(\bm{V}\), \(m\leq n\), we have

\[\sigma\left(\bm{W}\bm{t}(\bm{x})+\bm{b}\right)^{\top}\bm{M}\sigma \left(W\bm{t}(\bm{x})+\bm{b}\right) = \left\|\bm{V}\sigma\left(\bm{W}\bm{t}(\bm{x})+\bm{b}\right)\right\| ^{2}=\sum_{i=1}^{m}\left(\sum_{j=1}^{n}v_{ij}\sigma\left(\bm{w}_{j}^{\top}\bm{t} (\bm{x})+b_{j}\right)\right)^{2},\]which is, as in the parametrisation given in the main text, simply the squared Euclidean norm of a multi-output neural network, with \(\bm{V}\) corresponding to the weights of the second layer. If we denote by \(\bm{v}_{\cdot j}\) the \(j\)-th column of \(\bm{V}\), the normalizing constant is given by

\[\sum_{j=1}^{n}\sum_{l=1}^{n}\bm{v}_{\cdot j}^{\top}\bm{v}_{\cdot l}k_{\sigma, \bm{t},\mu}(\bm{\theta}_{j},\bm{\theta}_{l})=\sum_{j=1}^{n}\sum_{l=1}^{n}m_{ jl}k_{\sigma,\bm{t},\mu}(\bm{\theta}_{j},\bm{\theta}_{l})=\mathrm{Tr}(\bm{M}\bm{K}_{ \bm{\Theta}})\]

where as before

\[k_{\sigma,\bm{t},\mu}(\bm{\theta}_{i},\bm{\theta}_{j})=\int\sigma\big{(}\bm{w} _{i}^{\top}\bm{t}(\bm{x})+b_{i}\big{)}\sigma\big{(}\bm{w}_{j}^{\top}\bm{t}(\bm {x})+b_{j}\big{)}\mu(d\bm{x}).\]

Now if we let \(\sigma=\exp(\cdot/2)\) we obtain a family which is also closed under marginalisation (in addition to conditioning). The Proposition below generalises Proposition 1 of [42], which considers the special case of the Gaussian PSD mixtures.

**Proposition 3**.: _Let \(\bm{\mathsf{x}}=(\bm{\mathsf{x}}_{1},\bm{\mathsf{x}}_{2})\) be jointly \(\texttt{{SHEF}}_{\mathbb{X}_{1}\times\mathbb{X}_{2},\bm{t},\exp(\cdot/2),\mu}\) with parameters \(\bm{V}\) and \(\bm{\Theta}=(\left[\bm{W}_{\bm{1}},\bm{W}_{\bm{2}}\right],\bm{b})\). Assume that \(\mu(d\bm{x})=\mu_{1}(d\bm{x}_{1})\mu_{2}(d\bm{x}_{2})\) and \(\bm{t}(\bm{x})=\big{(}\bm{t}_{1}(\bm{x}_{1}),\bm{t}_{2}(\bm{x}_{2})\big{)}\). Then the marginal distribution of \(\bm{\mathsf{x}}_{1}\) is \(\texttt{{SHEF}}_{\mathbb{X}_{1},\bm{t}_{1},\exp(\cdot/2),\mu_{1}}\) with parameters \(\tilde{\bm{V}}\) and \(\bm{\Theta}=(\bm{W}_{\bm{1}},\bm{b})\), for some matrix \(\tilde{\bm{V}}\in\mathbb{R}^{m\times n}\)._

Proof.: Proposition 1 gives us the normalising constant for the parametrisation where biases are absorbed into \(\bm{V}\). If we explicitly keep the biases in the parametrisation, we have

\[k_{\exp(\cdot/2),\bm{t},\mu}(\bm{\theta}_{i},\bm{\theta}_{j})=\exp\left(\frac {1}{2}\left(b_{i}+b_{j}\right)\right)\mathrm{z}_{e}\left(\frac{1}{2}\left(\bm {w}_{i}+\bm{w}_{j}\right)\right),\] (17)

where \(\mathrm{z}_{e}\) is the normalizing constant of the exponential family with the sufficient statistic \(\bm{t}\) and base measure \(\mu\). By Theorem 2, we have that

\[P_{1}(d\bm{x}_{1};\bm{M},\bm{\Theta})=\frac{\mathrm{Tr}(\bm{M}\bm{C}_{\Theta} \left(\bm{x}_{1}\right))}{\mathrm{Tr}(\bm{M}\bm{K}_{\bm{\Theta}})}\mu_{1}(d \bm{x}_{1}),\]

where \([\bm{C}_{\Theta}(\bm{x}_{1})]_{ij}=k_{\sigma,\bm{t}_{2},\mu_{2}}\Big{(}\big{(} \bm{w}_{2i},\bm{w}_{1i}^{\top}\bm{t}_{1}(\bm{x}_{1})+b_{i}\big{)},\big{(}\bm {w}_{2j},\bm{w}_{1j}^{\top}\bm{t}_{1}(\bm{x}_{1})+b_{j}\big{)}\Big{)}\). But now since \(\sigma=\exp\left(\cdot/2\right)\), applying (17) gives

\[[\bm{C}_{\Theta}(\bm{x}_{1})]_{ij}=\mathrm{z}_{e,2}\left(\frac{1}{2}\left(\bm {w}_{2i}+\bm{w}_{2j}\right)\right)\exp\left(\frac{1}{2}\left(\bm{w}_{1i}^{ \top}\bm{t}_{1}(\bm{x}_{1})+b_{i}\right)\right)\exp\left(\frac{1}{2}\left(\bm {w}_{1j}^{\top}\bm{t}_{1}(\bm{x}_{1})+b_{j}\right)\right),\]

where \(\mathrm{z}_{e,2}\) is the normalizing constant of the exponential family with the sufficient statistic \(\bm{t}_{2}\) and base measure \(\mu_{2}\). Thus, we can write

\[\mathrm{Tr}(\bm{M}\bm{C}_{\Theta}\left(\bm{x}_{1}\right))= \sum_{i=1}^{n}\sum_{j=1}^{n}\Big{\{}m_{ij}\,\mathrm{z}_{e,2}\left( \frac{1}{2}\left(\bm{w}_{2i}+\bm{w}_{2j}\right)\right)\] \[\qquad\qquad\cdot\exp\left(\frac{1}{2}\left(\bm{w}_{1i}^{\top} \bm{t}_{1}(\bm{x}_{1})+b_{i}\right)\right)\exp\left(\frac{1}{2}\left(\bm{w}_{1j }^{\top}\bm{t}_{1}(\bm{x}_{1})+b_{j}\right)\right)\Big{\}}\] \[= \exp\left(\frac{1}{2}\left(\bm{W}_{1}^{\top}\bm{t}_{1}(\bm{x}_{1} )+\bm{b}\right)\right)^{\top}\tilde{\bm{M}}\exp\left(\frac{1}{2}\left(\bm{W}_{ 1}^{\top}\bm{t}_{1}(\bm{x}_{1})+\bm{b}\right)\right)\]

and we conclude that the marginal is in the same family with \(\tilde{\bm{M}}=\bm{M}\circ\bm{Z}_{e,2}\), where

\[[\bm{Z}_{e,2}]_{i,j}=\mathrm{z}_{e,2}\left(\frac{1}{2}\left(\bm{w}_{2i}+\bm{w} _{2j}\right)\right).\]

Note that \(\tilde{\bm{M}}\) is PSD as an Hadamard product of two PSD matrices. Thus, we can find \(\tilde{\bm{V}}\) such that \(\tilde{\bm{M}}=\tilde{\bm{V}}^{\top}\tilde{\bm{V}}\).

## Appendix E Experiments

### 2D Unconditional density estimation

Our benchmarking protocol is slightly altered compared with [46]. Firstly, we measure performance over \(20\) random seeds instead of \(1\) fixed seed. We find that sometimes the variance over random seeds can be large (e.g. Resampled 0 on Circles). Secondly, rather than computing test performance at the last epoch of training, we follow the more standard procedure of returning the test performance evaluated at the epoch corresponding with the smallest validation performance. This validation/test monitoring results in substantial performance gains in all of the models, with no extra computational cost for SNEFY, Gauss and GMM. For Resampled, monitoring the validation performance to a high precision requires estimating the normalising constant to a high precision, which is computationally challenging. We therefore only check the validation performance every \(100\) epochs, and compute a high precision normalising constant if the validation performance is the lowest encountered so far. We train each models for a maximum of \(20000\) iterations (while monitoring validation performance). We use Adam with default hyperparameters and weight decay \(10^{-3}\). The batch size is \(2^{10}\).

We use a SNEFY with Gaussian mixture model base measure supported on \(\mathbb{X}=\mathbb{R}^{2}\), identity sufficient statistic \(\bm{t}\) and activation function \(\cos\). The base measure consists of 8 mixture components, each with a diagonal covariance matrix. We use the same Resampled architecture as in the original paper [46]. We use an MLP with layer widths \([2,256,256,1]\) and sigmoid activations for the resampling distribution. We use a discount factor of \(0.1\) for the exponential moving average partition function calculation. Note that for Resampled models we are only able to provide an estimate of the test log likelihood, not the exact log likelihood as in other methods. We choose \(n=50\) and \(m=1\). The Gaussian mixture model has \(10\) mixture components, each with a diagonal covariance matrix. Each normalising flow block consists of an affine coupling block, a permutation layer, and an actnorm layer.

### Modelling distributions on the sphere

We compare the performance of a VMF distribution, a "regular" VMF mixture distribution and our SNEFY construction of the VMF mixture, which allows for some negative weight coefficients as discussed in SS 3.2. We use the dataset [44] also used by [19] in a different context for point

\begin{table}
\begin{tabular}{l|c|c|c|c}  & SNEFY 1 & Resampled 1 & Gauss 1 & GMM 1 \\ \hline Moons & \(-1.59\pm 0.03\) & \(-1.76\pm 0.02\) & \(-3.29\pm 0.01\) & \(-1.57\pm 0.03\) \\  & \(1431\) & \(68007\) & \(1194\) & \(1240\) \\  & \(1352.19\pm 134.30\) & \(615.57\pm 24.35\) & \(730.21\pm 23.60\) & \(753.16\pm 31.41\) \\ \hline Circles & \(-1.92\pm 0.02\) & \(-1.94\pm 0.03\) & \(-3.37\pm 0.01\) & \(-2.00\pm 0.06\) \\  & \(1431\) & \(68007\) & \(1194\) & \(1240\) \\  & \(798.13\pm 196.89\) & \(291.03\pm 29.46\) & \(162.64\pm 17.19\) & \(186.65\pm 21.10\) \\ \hline Rings & \(-2.35\pm 0.04\) & \(-2.31\pm 0.02\) & \(-3.16\pm 0.01\) & \(-2.43\pm 0.04\) \\  & \(1431\) & \(68007\) & \(1194\) & \(1240\) \\  & \(1134.90\pm 133.10\) & \(502.32\pm 24.51\) & \(528.50\pm 24.51\) & \(559.12\pm 30.90\) \\ \end{tabular}
\end{table}
Table 4: As in Table 2, but with 1 NVP layer.

\begin{table}
\begin{tabular}{l|c|c|c|c}  & SNEFY 2 & Resampled 2 & Gauss 2 & GMM 2 \\ \hline Moons & \(-1.58\pm 0.02\) & \(-1.59\pm 0.03\) & \(-1.62\pm 0.03\) & \(-1.58\pm 0.02\) \\  & \(2621\) & \(69197\) & \(2384\) & \(2430\) \\  & \(1453.05\pm 147.67\) & \(700.98\pm 32.94\) & \(831.40\pm 49.24\) & \(855.04\pm 53.01\) \\ \hline Circles & \(-1.91\pm 0.04\) & \(-2.07\pm 0.04\) & \(-2.66\pm 0.05\) & \(-1.96\pm 0.04\) \\  & \(2621\) & \(69197\) & \(2384\) & \(2430\) \\  & \(899.77\pm 196.54\) & \(363.91\pm 36.62\) & \(267.23\pm 32.05\) & \(289.96\pm 33.71\) \\ \hline Rings & \(-2.35\pm 0.04\) & \(-2.32\pm 0.03\) & \(-2.80\pm 0.02\) & \(-2.36\pm 0.03\) \\  & \(2621\) & \(69197\) & \(2384\) & \(2430\) \\  & \(1255.92\pm 168.80\) & \(579.02\pm 23.70\) & \(637.73\pm 34.04\) & \(658.98\pm 37.69\) \\ \end{tabular}
\end{table}
Table 5: As in Table 2, but with 2 NVP layers.

processes. This dataset, retrieved in 2015, is called the "Revised New General Catalogue and Index Catalogue" (RNGC/IC). The RNGC/IC consists of locations of some \(10610\) galaxies. We use the spherical coordinates of these galaxies and map them to the surface of a sphere.

We compare two types of \(\texttt{SNEY}_{\mathbb{S}^{2},\exp,\mathrm{Id},d\bm{x}}\) with \(m=n=30\): one is constrained so that \(\bm{V}\) is diagonal (and therefore \(\bm{V}^{\top}\bm{V}\) is diagonal with nonnegative entries), and the other uses a unconstrained general \(\bm{V}\). We expect the unconstrained model to be more expressive, and therefore obtain a better test NLL.

We use Adam with default hyperparameters and a full batch size. We randomly shuffle the data and perform an 80/20 train/test split. Each of the 50 runs uses a randomly sampled initialisation and train/test split. We train for \(20000\) epochs.

### Conditional density estimation of photometric redshift

Our deep conditional feature extractor is an MLP that uses ReLU activations in each layer and batch normalisation between each layer. Our SNEFY model uses \(\mathrm{Snake}_{a}\) activations with \(a=10\). The CNF models utilise affine layers whi \(\tanh\) activations. For SNEFY and CNF, we preprocess the input features so that they have sample mean zero and sample variance one. We train both deep learning models for \(100\) epochs using Adam with default hyperparameters and a batch size of \(256\). The dataset consists of 74309 training and 74557 examples, which are fixed between each run. Each run uses a randomly sampled initialisation (except for CKDE, which is deterministic).

## Appendix F Complex activation case

Here we consider an extension where we allow the neural network activation \(\sigma\) to be complex-valued, and accordingly the readout parameters \(\bm{V}\) to be complex, i.e. \(\bm{V}\in\mathbb{C}^{m\times n}\). Note that in this case

\[\left\|\bm{f}\left(\bm{t}(\bm{x});\bm{V},\bm{\Theta}\right) \right\|_{2}^{2} = \left\|\bm{V}\sigma\left(\bm{W}\bm{t}(\bm{x})+\bm{b}\right) \right\|^{2}=\sum_{r=1}^{m}\left|\sum_{j=1}^{n}v_{rj}\sigma\left(\bm{w}_{j}^{ \top}\bm{t}(\bm{x})+b_{j}\right)\right|^{2}.\]

\begin{table}
\begin{tabular}{l|c|c|c|c}  & SNEFY 4 & Resampled 4 & Gauss 4 & GMM 4 \\ \hline Moons & \(-1.58\pm 0.03\) & \(-1.60\pm 0.07\) & \(-1.60\pm 0.03\) & \(-1.57\pm 0.03\) \\  & \(5001\) & \(71577\) & \(4764\) & \(4810\) \\  & \(1616.21\pm 152.61\) & \(851.29\pm 33.63\) & \(1005.16\pm 30.59\) & \(1026.76\pm 28.21\) \\ \hline Circles & \(-1.92\pm 0.04\) & \(-1.99\pm 0.03\) & \(-2.14\pm 0.16\) & \(-1.94\pm 0.04\) \\  & \(5001\) & \(71577\) & \(4764\) & \(4810\) \\  & \(1035.64\pm 142.39\) & \(514.02\pm 47.98\) & \(455.58\pm 41.19\) & \(480.15\pm 44.61\) \\ \hline Rings & \(-2.33\pm 0.02\) & \(-2.31\pm 0.03\) & \(-2.59\pm 0.12\) & \(-2.32\pm 0.04\) \\  & \(5001\) & \(71577\) & \(4764\) & \(4810\) \\  & \(1410.36\pm 141.04\) & \(749.10\pm 52.05\) & \(813.92\pm 27.20\) & \(833.36\pm 30.77\) \\ \end{tabular}
\end{table}
Table 6: As in Table 2, but with 4 NVP layers.

\begin{table}
\begin{tabular}{l|c|c|c|c}  & SNEFY 8 & Resampled 8 & Gauss 8 & GMM 8 \\ \hline Moons & \(-1.60\pm 0.02\) & \(-1.58\pm 0.02\) & \(-1.61\pm 0.08\) & \(-1.58\pm 0.03\) \\  & \(9761\) & \(76337\) & \(9524\) & \(9570\) \\  & \(2036.15\pm 205.52\) & \(1198.04\pm 102.71\) & \(1399.74\pm 65.78\) & \(1423.17\pm 82.84\) \\ \hline Circles & \(-1.91\pm 0.03\) & \(-1.97\pm 0.05\) & \(-2.16\pm 0.29\) & \(-1.93\pm 0.03\) \\  & \(9761\) & \(76337\) & \(9524\) & \(9570\) \\  & \(1405.27\pm 144.72\) & \(839.23\pm 92.99\) & \(838.62\pm 82.98\) & \(862.25\pm 81.09\) \\ \hline Rings & \(-2.34\pm 0.06\) & \(-2.37\pm 0.17\) & \(-2.49\pm 0.16\) & \(-2.33\pm 0.07\) \\  & \(9761\) & \(76337\) & \(9524\) & \(9570\) \\  & \(1795.38\pm 186.62\) & \(1071.50\pm 109.12\) & \(1201.29\pm 71.76\) & \(1218.31\pm 70.36\) \\ \end{tabular}
\end{table}
Table 7: As in Table 2, but with 8 NVP layers.

Take \(\sigma(u)=\exp(iu)\) and \(\bm{t}(\bm{x})=\bm{x}\).

Then the above takes the form

\[\sum_{r=1}^{m}\left|\sum_{j=1}^{n}v_{rj}\exp\big{(}i\left(\bm{w}_{j} ^{\top}\bm{x}+b_{j}\right)\big{)}\right|^{2} = \sum_{r=1}^{m}\sum_{j=1}^{n}\sum_{l=1}^{n}v_{rj}\bar{v}_{rl}e^{i \left(\bm{w}_{j}^{\top}\bm{x}+b_{j}\right)}e^{-i\left(\bm{w}_{l}^{\top}\bm{x}+ b_{l}\right)}\] \[= \sum_{r=1}^{m}\sum_{j=1}^{n}\sum_{l=1}^{n}v_{rj}\bar{v}_{rl}e^{i \left(b_{j}-b_{l}\right)}e^{i\left(\left(\bm{w}_{j}-\bm{w}_{l}\right)^{\top} \bm{x}\right)}.\]

As in the \(\sigma=\exp\) case, bias terms can be folded into the readout parameters \(\bm{V}\) (which is the reason why we also require readout parameters to be complex). We note that the case \(n=1\) is not interesting as it simply reduces to the (normalised) base measure \(\mu\).

In order to obtain the normalizing constant \(\operatorname{Tr}(\bm{V}^{H}\bm{V}\bm{K}_{\bm{\Theta}})\), we need the integral of the form

\[[\bm{K}_{\bm{\Theta}}]_{jl}=k_{\exp(i\cdot),\mathrm{Id},\mu}(\bm{w}_{j},\bm{ w}_{l})=\int e^{i\left((\bm{w}_{j}-\bm{w}_{l})^{\top}\bm{x}\right)}\mu(d\bm{x})=: \kappa(\bm{w}_{j}-\bm{w}_{l}),\] (18)

where \(\kappa\) is simply the Fourier transform of the base measure \(\mu\), i.e. its characteristic function in the case where \(\mu\) is a probability measure. Hence many standard choices of \(\mu\) lead to tractable normalizing constants. Note that while \(\bm{V}\) and \(\kappa\) both may be complex-valued, the normalizing constant as well as the density itself are real-valued.

Various examples of probability measures \(\mu\) and its Fourier transforms that give rise to shift-invariant positive definite kernels \(\kappa\) have been studied in the literature on Random Fourier Features (RFF) [38]. Here too, while the functional form of the expressions is identical, the role between the data and the parameters is reversed: in RFF, one is interested in approximating a given kernel on the data instances, by considering a probability measure on the frequency space which is that kernel's inverse Fourier transform.

**Remark 3**.: _If we restrict our attention to real-valued \(\bm{V}\) and thus explicitly maintain biases inside the parameterisation, this model can also be realised with stacking \(\cos\) and \(\sin\) activation so that they share the readout parameters since_

\[\sum_{r=1}^{m}\left|\sum_{j=1}^{n}v_{rj}\exp\big{(}i\left(\bm{w}_{j}^{\top} \bm{x}+b_{j}\right)\big{)}\right|^{2}=\left\|\bm{V}\cos\left(\bm{W}\bm{x}+\bm {b}\right)\right\|^{2}+\left\|\bm{V}\sin\left(\bm{W}\bm{x}+\bm{b}\right) \right\|^{2}.\]

## Appendix G Discrete and mixed continuous/discrete SNEFYs

### Discrete SNEFYs via series

Closed-form expressions for normalising constants of discrete distributions are only advantageous if their computation costs less than naively summing up unnormalised density over all possible states. This is the case if the support has very large or infinite cardinality. An example that extends the classical Poisson distribution in exponential family form is given in Table 1. Here we discuss some other settings.

NNK as a convergent seriesSuppose the support \(\mathbb{X}\) is discrete and let \(h(x)\) be a nonnegative function corresponding with the discrete base measure \(\mu\). The NNK is given by

\[k_{\sigma,\bm{t},\mu}(\bm{\theta}_{i},\bm{\theta}_{j})=\sum_{\bm{x}\in\mathbb{ X}}\sigma\big{(}\bm{w}_{1}^{\top}\bm{t}(\bm{x})+b_{1}\big{)}\sigma\big{(} \bm{w}_{2}^{\top}\bm{t}(\bm{x})+b_{2}\big{)}h(\bm{x}).\]

Such NNKs often resemble known convergent series.

Fourier seriesFor example, if \(\sigma=\cos\) and \(C_{x}=h(x)\) are some coefficients of a convergent Fourier series, then

\[k_{\sigma,\mathbf{t},\mu}(\boldsymbol{\theta}_{i},\boldsymbol{\theta}_{j})=\frac {1}{2}\sum_{x=0}^{\infty}C_{x}\Big{(}\cos\big{(}t(x)(w_{1}-w_{2})+(b_{1}-b_{2}) \big{)}+\cos\big{(}t(x)(w_{1}+w_{2})+(b_{1}+b_{2})\big{)}\Big{)}\]

is just a series representation of a sum of two periodic functions. For example, if \(C_{x}=\frac{2(-1)^{x+1}}{\pi x}\) for \(x\geq 1\) and \(C_{x}=0\) otherwise, and \(t(x)=2\pi x\), the NNK is a sum of two sawtooth waves with frequencies \(w_{1}-w_{2}\) and \(w_{1}+w_{2}\) and phase offsets \(b_{1}-b_{2}\) and \(b_{1}+b_{2}\). Other examples include rectified sine waves, square waves and triangular waves. This extends to periodic functions with convergent multivariate Fourier series.

### Mixed continuous/discrete SNEF's

Mixed distributions can be obtained by choosing the base measure to be a mixed continuous distribution. For example, choose \(\mathbb{X}=\mathbb{R}^{d}\), \(\mu(d\mathbf{x})=\frac{1}{2}\big{(}\Phi(\mathbf{x})+\delta(\mathbf{x})\big{)}\, d\mathbf{x}\), and take \(\sigma\) and \(\mathbf{t}\) to be any of the combinations leading to a closed-form NNGPK (for example, \(\mathbf{t}\) as the identity and \(\sigma\) as the error function, Leaky ReLU, GELU, cosine, or snake). Then

\[k_{\sigma,\mathbf{t},\mu}(\theta_{i},\theta_{j}) =\frac{1}{2}\Big{(}\int_{\mathbb{X}}\sigma\big{(}\mathbf{W}_{i}^{ \top}\mathbf{t}(\mathbf{x})+b_{i}\big{)}\sigma\big{(}\mathbf{W}_{j}^{\top} \mathbf{t}(\mathbf{x})+b_{j}\big{)}\delta(\mathbf{x})d\mathbf{x}+\] \[\qquad\qquad\int_{\mathbb{X}}\sigma\big{(}\mathbf{W}_{i}^{\top} \mathbf{t}(\mathbf{x})+b_{i}\big{)}\sigma\big{(}\mathbf{W}_{j}^{\top}\mathbf{ t}(\mathbf{x})+b_{j}\big{)}\Phi(\mathbf{x})d\mathbf{x}\Big{)}\] \[=\frac{1}{2}\Big{(}\sigma\big{(}\mathbf{W}_{i}^{\top}\mathbf{t}( \mathbf{0})+b_{i}\big{)}\sigma\big{(}\mathbf{W}_{j}^{\top}\mathbf{t}(\mathbf{ 0})+b_{j}\big{)}+k_{\sigma,\mathbf{t},\Phi}(\theta_{i},\theta_{j})\Big{)},\]

which is a closed form.