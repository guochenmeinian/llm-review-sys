# A Surprisingly Simple Approach to

Generalized Few-Shot Semantic Segmentation

 Tomoya Sakai

IBM Research - Tokyo

tomoya.sakai2@ibm.com

&Haoxiang Qiu

IBM Research - Tokyo

haoxiang.qiu@ibm.com

&Takayuki Katsuki

IBM Research - Tokyo

kats@jp.ibm.com

&Daiki Kimura

IBM Research - Tokyo

daiki@jp.ibm.com

&Takayuki Osogami

IBM Research - Tokyo

osogami@jp.ibm.com

&Tadanobu Inoue

IBM

inouet@jp.ibm.com

###### Abstract

The goal of _generalized_ few-shot semantic segmentation (GFSS) is to recognize _novel-class_ objects through training with a few annotated examples and the _base-class_ model that learned the knowledge about the base classes. Unlike the classic few-shot semantic segmentation, GFSS aims to classify pixels into both base and novel classes, meaning it is a more practical setting. Current GFSS methods rely on several techniques such as using combinations of customized modules, carefully designed loss functions, meta-learning, and transductive learning. However, we found that a simple rule and standard supervised learning substantially improve the GFSS performance. In this paper, we propose a simple yet effective method for GFSS that does not use the techniques mentioned above. Also, we theoretically show that our method perfectly maintains the segmentation performance of the base-class model over most of the base classes. Through numerical experiments, we demonstrated the effectiveness of our method. It improved in novel-class segmentation performance in the \(1\)-shot scenario by \(6.1\%\) on the PASCAL-\(5^{i}\) dataset, \(4.7\%\) on the PASCAL-\(10^{i}\) dataset, and \(1.0\%\) on the COCO-\(20^{i}\) dataset. Our code is publicly available at https://github.com/IBM/BCM.

## 1 Introduction

_Semantic segmentation_ is a vital task in various visual understanding systems, and the goal is to obtain pixel-level semantic categories [1]. Recent developments in convolutional neural networks [2] and vision transformers [3] have pushed the limits of semantic segmentation. With a large amount of annotated images, we can obtain an accurate model that can recognize objects in the training data. In real-world applications, however, the learned model will encounter _novel-class_ objects that are not classified in _base classes_, i.e., classes that are not annotated in the training data.

To solve this problem, _few-shot_ semantic segmentation (FSS) aims to recognize novel-class objects with a few annotated images while using the learned model, which has knowledge about the base-class information. Although various FSS methods have been proposed [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], FSS only handles novel-class object recognition, which restricts its applicability since base classes will still appear at inference in practice.

_Generalized_ FSS (GFSS) aims to recognize both base- and novel-class objects [16] and is regarded as a more practical setting than FSS. Current GFSS methods rely on several techniques such as using combinations of customized modules [16, 17, 18, 19], carefully designed loss functions [20], meta-learning [16, 18, 19], and transductive learning [20]. These techniques improved GFSS performance at thecost of implementation and computation time. For example, such a customized module is not always supported in the target framework, methods based on meta-learning require several hours to train customized modules, and transductive learning optimizes models during inference, which is not suitable for applications that require quick responses. However, we found that a simple rule and standard supervised learning improve GFSS performance.

In this paper, we propose a simple yet effective GFSS method that does not use the above techniques. As illustrated in Fig. 1, our idea is mining base classes closely related to classifying novel classes. We thus refer to our method as _base-class mining_ (BCM). Surprisingly, BCM perfectly maintains the segmentation performance of the base-class model over a subset of base classes. Since maintaining the segmentation performance of the base-class model is critical in GFSS, BCM will be beneficial, especially when a performance difference from the base-class model confuses users.

Our contributions are summarized as follows.

* We propose a simple yet effective GFSS method based on a simple rule and well-known supervised learning techniques, which can be regarded as a strong alternate baseline without transductive learning.
* We theoretically show that the performance of the base-class model for a subset of base classes is perfectly maintained, which is the first theoretical finding about base-class segmentation performances in GFSS, to the best of our knowledge.
* We demonstrated the effectiveness of BCM on the PASCAL-\(5^{i}\), PASCAL-\(10^{i}\), and COCO-\(20^{i}\) datasets. BCM substantially improved novel-class segmentation performance in the \(1\)-shot scenario by \(6.1\%\) on PASCAL-\(5^{i}\) and \(4.7\%\) on PASCAL-\(10^{i}\).

## 2 Related work

### GFSS setting

In GFSS [16], multiple novel classes need to be classified, i.e., multi-class classification, in addition to classifying base classes. This differs from the FSS setting of single novel-class classification, i.e., binary classification.

We consider the _practical_ GFSS setting [20], in which the available resources are _few-shot_ annotated images for novel classes and the _base-class model_ trained using standard learning methods. The existing setting [16] requires annotated base-class samples for training the base-class model and tuning a GFSS model that can recognize both base and novel classes. For example, the number of base classes is \(60\) (excluding the background) on COCO-\(20^{i}\) case, meaning that we need to collect \(300\) annotated images for tuning in the \(5\)-shot scenario, other than training the base-class model. Such additional samples for base classes are not necessary in the practical GFSS setting, resulting in using those samples for training the base-class model, which is an advantage of the practical GFSS setting.

### GFSS methods

The major challenges with current GFSS methods are i) attaining better recognition performance for novel classes and ii) maintaining the segmentation performance of the base-class model. _Context-aware prototype learning_ (CAPL) [16] enhances prototypes with a few annotated images and employs

Figure 1: Illustration of _base-class mining_ (BCM) with three novel classes: “train”, “tv”, and “couch”. Base-class model first outputs prediction. If prediction is one of chosen base classes, corresponding model outputs prediction. Otherwise, prediction of base-class model is used as it is. Simple rule finds which base class is related to novel classes. Models for novel classes are trained by standard supervised learning.

a balancing mechanism of prototypes for base and novel classes. _Base and meta_ (BAM) [14; 18] designed customized modules to learn knowledge from few-shot data and combined predictions of both base and novel classes on the basis of thresholds. POP [17] and PCN [19] use similar approaches. _Distilled information maximization_ (DIaM) [20] does not depend on customized modules, instead, uses the _information maximization principle_[21] and designs a loss function on the basis of _knowledge distillation_[22] to preserve base-class knowledge. DIaM uses the transductive learning approach [23], which is not suitable for applications that require quick responses.

Compared with the above methods, BCM does not rely on carefully customized models, various combinations of loss functions, or transductive learning.

### Continual semantic segmentation

The GFSS setting relates to another emergence problem setting known as _continual semantic segmentation_ (CSS) [24; 25; 26] in which the new classes appear in a continual learning manner. The CSS setting is reduced to the GFSS setting if several novel classes appear with a few annotations in a single step. Coincidentally, a previous study [26] empirically found a phenomenon similar to our idea that a novel class is classified as a base class, through qualitative analyses of their method. However, their method was not designed for the few-shot learning setting, meaning that how to improve GFSS performances is unclear. In contrast to their findings, BCM explicitly integrates the idea of the relation between base and novel classes with the architecture. These differences in problem settings and architectures differentiate CSS-based methods from BCM.

## 3 Preliminaries

### Problem settings

We consider the following practical GFSS setting [20].

Let \(\bm{X}\in\mathbb{R}^{H\times W\times 3}\) denote an RGB image of height \(H\) and width \(W\), and \(\bm{Y}\in\mathcal{Y}^{H\times W}\) be its corresponding segmentation map, where \(\mathcal{Y}\subset\{0,1,2,3,\dots\}\) is a set of object classes. Let \([\,\cdot\,]_{j}\) indicate the \(j\)-th element of a matrix, where \(j\in\{1,\dots,HW\}\). If \([\bm{Y}]_{j}=y\), the object \(y\) exists at the \(j\)-th pixel.

Let \(\mathcal{Y}_{\mathrm{b}}\) and \(\mathcal{Y}_{\mathrm{n}}\) be the sets of _base_ and _novel_ classes, respectively, such that \(\mathcal{Y}_{\mathrm{b}}\cap\mathcal{Y}_{\mathrm{n}}=\emptyset\) and \(\mathcal{Y}_{\mathrm{all}}=\mathcal{Y}_{\mathrm{b}}\cup\mathcal{Y}_{\mathrm{n}}\). We use the class '0' for background, which is often the case with implementation. For the sake of simplicity, we include the background into \(\mathcal{Y}_{\mathrm{b}}\), e.g., \(\mathcal{Y}_{\mathrm{b}}=\{0,1,2,3\}\) and \(\mathcal{Y}_{\mathrm{n}}=\{4,5\}\).

We have a learned base-class model \(\widehat{g}_{\mathrm{b}}\), which is trained with a large amount of annotated images by using standard semantic-segmentation methods [1]. Given \(\bm{X}\), \(\widehat{g}_{\mathrm{b}}\) returns a base-class segmentation map \(\widehat{\bm{Y}}_{\mathrm{b}}\in\mathcal{Y}_{\mathrm{b}}^{H\times W}\). Similarly to the _practical_ GFSS setting [20], we do not assume the customized architecture for \(\widehat{g}_{\mathrm{b}}\), enabling us to easily use cutting-edge foundation models [27].

A \(K\)-shot dataset contains \(K\) examples with its ground-truth mask for each novel class \(y\in\mathcal{Y}_{\mathrm{n}}\), e.g., if \(K=5\) and \(|\mathcal{Y}_{\mathrm{n}}|=5\), we have \(25\) annotated images. Note that \(K\) examples for base classes are not necessary, as discussed in Sec. 2.1.

Our goal is to obtain the segmentation map \(\widehat{\bm{Y}}_{\mathrm{BCM}}\in\mathcal{Y}_{\mathrm{all}}^{H\times W}\) computed using the prediction model for GFSS, denoted as \(\widehat{g}_{\mathrm{BCM}}\).

### Evaluation metric

The _mean intersection-over-union_ (mIoU) is widely used in reporting the performance of segmentation methods [1]. Let us first define the IoU for a class \(y^{\prime}\in\mathcal{Y}\) as

\[\mathrm{IoU}_{y^{\prime}}(\bm{Y},\widehat{\bm{Y}}):=\frac{\sum_{j=1}^{HW} \mathrm{I}_{y^{\prime}}([\bm{Y}]_{j},[\widehat{\bm{Y}}]_{j})}{\sum_{j=1}^{HW} \mathrm{U}_{y^{\prime}}([\bm{Y}]_{j},[\widehat{\bm{Y}}]_{j})},\] (1)

where \(\mathrm{I}_{y^{\prime}}(y,\widehat{y}):=\mathbb{I}[y=y^{\prime}]\cdot\mathbb{ I}[\widehat{y}=y^{\prime}]\), \(\mathrm{U}_{y^{\prime}}(y,\widehat{y}):=\mathbb{I}[y=y^{\prime}]+\mathbb{I}[ \widehat{y}=y^{\prime}]-\mathbb{I}[y=y^{\prime}]\cdot\mathbb{I}[\widehat{y}=y^ {\prime}]\), and \(\mathbb{I}[\mathrm{cond}]\) is the indicator function taking \(1\) if \(\mathrm{cond}\) is true, 0 otherwise. Here, we consider a single sample for evaluation, but it can be easily extended to multiple samples by adding the summation over samples to the numerator and denominator in Eq. (1), respectively. Finally, the mIoU is computed by

\[\mathrm{mIoU}_{\mathcal{Y}}(\bm{Y},\widehat{\bm{Y}}):=\frac{1}{|\mathcal{Y}|} \sum_{y^{\prime}\in\mathcal{Y}}\mathrm{IoU}_{y^{\prime}}(\bm{Y},\widehat{\bm{Y }}).\] (2)

For example, if \(\mathcal{Y}\) is \(\mathcal{Y}_{\mathrm{n}}\), it will be the mIoU over novel classes.

## 4 Proposed method

We now present BCM.

### Training

Training is divided into two steps: 1) finding the relationship between base and novel classes, and 2) training models for classifying novel classes for each chosen base class.

Step 1.We input \(\bm{X}\) into \(\widehat{g}_{\mathrm{b}}\) and obtain \(\widehat{\bm{Y}}_{\mathrm{b}}\). For each pixel of the annotated object, we compare \(\widehat{\bm{Y}}_{\mathrm{b}}\) and \(\bm{Y}\) and record co-occurrences of base and novel classes. We then count the co-occurrences, find the top-\(s\) co-occurred base class for each novel class, referred to as the _top-\(s\) strategy_, and obtain chosen base classes denoted as \(\mathcal{B}\). Finally, we construct the mapping from a base class to novel classes, called _base-novel mapping_ (BNM). Figure 2 illustrates the creation of BNM with the top-\(1\) strategy from the \(1\)-shot dataset.

Step 2.For each chosen base class \(\beta\in\mathcal{B}\), we train a model \(g_{\beta}\) with the modified \(K\)-shot dataset where labels are converted into \(\beta\) if they are novel classes irrelevant to \(\beta\) or the background. Taking the example in Fig. 1(c), when \(\beta=1\), the irrelevant novel-class labels '\(4\)' and '\(5\)' and the background label '\(0\)' are replaced with '\(1\)'. Then, \(\widehat{g}_{\beta=1}\) returns either '\(1\)' or '\(6\)' as the prediction.

To obtain the learned model \(\widehat{g}_{\beta}\), we can use any learning method, such as minimizing the cross-entropy loss or effective losses used in the previous studies.

### Inference

Inference is analogous to training. For a test image \(\bm{X}\), we first obtain the base-class prediction \(\widehat{\bm{Y}}_{\mathrm{b}}\). For each pixel \(j\), if \([\widehat{\bm{Y}}_{\mathrm{b}}]_{j}=\beta\), we then obtain the prediction of the corresponding model \(\widehat{g}_{\beta}\) and overwrite \([\widehat{\bm{Y}}_{\mathrm{b}}]_{j}\) with the output of \(\widehat{g}_{\beta}\). Figure 3 illustrates how we obtain the segmentation map of BCM. We summarize the flow of BCM in Fig. 4.

### Preventing catastrophic forgetting

Maintaining the base-class segmentation performance is crucial in GFSS. We theoretically show that BCM perfectly maintains the segmentation performance of most of the base classes without resorting to, e.g., knowledge distillation [22] for training models.

Figure 2: Illustration of BNM creation. For illustration purpose, image of size \(3\times 3\) is used. (a) Count co-occurrences, i.e., (base class, novel class) pairs. There are three \((0,4)\) and one \((2,4)\) co-occurrences. (b) Aggregate co-occurrence counts for all samples and create co-occurrence count table. (c) Create BNM from co-occurrence count table, where top-\(1\) strategy finds base class with largest co-occurrences (shaded cell in Fig. 1(b)) for each novel class.

We first formulate the prediction of BCM described in Sec. 4.2. Let \(\widehat{y}_{\mathrm{b}}\) be the prediction of the base-class model at the \(j\)-th pixel and \(\widehat{y}_{\beta}\) be the prediction of \(\widehat{g}_{\beta}\) at the \(j\)-th pixel. The prediction of BCM at the \(j\)-th pixel, \(\widehat{y}_{\mathrm{BCM}}\), is obtained by

\[\widehat{y}_{\mathrm{BCM}}=\begin{cases}\widehat{y}_{\mathrm{b}}&\text{if } \widehat{y}_{\mathrm{b}}\not\in\mathcal{B},\\ \widehat{y}_{\beta=\widehat{y}_{\mathrm{b}}}&\text{otherwise}.\end{cases}\] (3)

The above prediction mechanism leads to the following proposition:

**Proposition 4.1**.: _Let \(\widehat{\bm{Y}}_{\mathrm{b}}\) and \(\widehat{\bm{Y}}_{\mathrm{BCM}}\) be the predictions of the base-class model and BCM, respectively. The mIoUs of \(\widehat{\bm{Y}}_{\mathrm{b}}\) and \(\widehat{\bm{Y}}_{\mathrm{BCM}}\) over \(\mathcal{Y}_{\mathrm{b}}\setminus\mathcal{B}\) are the same:_

\[\mathrm{mIoU}_{\mathcal{Y}_{\mathrm{b}}\setminus\mathcal{B}}(\bm{Y},\widehat {\bm{Y}}_{\mathrm{b}})=\mathrm{mIoU}_{\mathcal{Y}_{\mathrm{b}}\setminus \mathcal{B}}(\bm{Y},\widehat{\bm{Y}}_{\mathrm{BCM}}).\] (4)

_If \(|\mathcal{B}|\) is small, the segmentation performance of most of the base classes is perfectly maintained._

Proof.: For any \(y^{\prime}\not\in\mathcal{B}\), if \([\widehat{\bm{Y}}_{\mathrm{b}}]_{j}=y^{\prime}\), then \([\widehat{\bm{Y}}_{\mathrm{BCM}}]_{j}=[\widehat{\bm{Y}}_{\mathrm{b}}]_{j}\) by definition of the BCM prediction. Then, for any \(y^{\prime}\in\mathcal{Y}_{\mathrm{b}}\setminus\mathcal{B}\),

\[\mathrm{IoU}_{y^{\prime}}(\bm{Y},\widehat{\bm{Y}}_{\mathrm{b}})=\mathrm{IoU}_ {y^{\prime}}(\bm{Y},\widehat{\bm{Y}}_{\mathrm{BCM}}).\] (5)

Taking the average of \(\mathrm{IoU}_{y^{\prime}}\) over \(\mathcal{Y}_{\mathrm{b}}\setminus\mathcal{B}\), we obtain Eq. (4). 

Intuitively, since BCM uses the prediction of the base-class model as it is for a subset of base classes, the mIoU over those base classes is the same as \(\widehat{g}_{\mathrm{b}}\). Proposition 4.1 shows that BCM partially prevents catastrophic forgetting [28; 29]. In our experiments, \(|\mathcal{B}|\) tended to be small, resulting in the mIoU over base classes being almost maintained.

### Lightweight implementation

Since the size of training data for novel classes is not large in GFSS, training deep neural networks for \(\widehat{g}_{\beta}\) is impractical. We thus use the base-class model \(\widehat{g}_{\mathrm{b}}\) as the feature extractor of \(\widehat{g}_{\beta}\) and train linear models as the last layer of \(\widehat{g}_{\beta}\) with the \(K\)-shot data.

To train linear models, we can use off-the-shelf libraries, such as _Scikit-learn_[30], meaning that training time will be fast, compared with the end-to-end training on GPU. Since the number of background pixels is much larger than that of objects of interest, we applied sampling techniques for imbalanced data [31] to training data, such as under-sampling.

Regarding the top-\(s\) strategy, we used \(s=1\) from the performance and computation time viewpoint. The effect of \(s\) in the top-\(s\) strategy is discussed in Sec. 5.6.

### Further performance improvement

Since our implementation is to train simple linear models, we can easily use various techniques to improve GFSS performance. We explain two effective and easy-to-use techniques used in our experiments as followsPre-processing.We can use _Tukey's ladder of powers transformation_[32], known as the effective transformation in few-shot learning [33]. Specifically, let \(\bm{f}\) be the \(d\)-dimensional feature vector extracted by \(\widehat{g}_{\mathrm{b}}\). Tukey's ladder of powers transformation is defined as

\[\bm{\tilde{f}}=\begin{cases}\bm{f}^{\tau}&\text{if }\tau\neq 0,\\ \log\bm{f}&\text{otherwise},\end{cases}\] (6)

where \(\tau\) is a hyper-parameter, and the power and logarithm operations are element-wise. When \(\tau=1\), the original feature is used. In a previous study [33], \(\tau=0.5\) was recommended as the default value; thus, we used it in our experiments.

These pre-processed feature vectors are used for \(\widehat{g}_{\beta}\) only since the change in feature representation for \(\widehat{g}_{\mathrm{b}}\) without retraining would downgrade performance. Note that to apply similar pre-processing to the existing methods, it is crucial to take into account the adverse effect on total performance.

Ensemble learning.We can use ensemble learning [34; 35] to improve GFSS performance. Unlike the existing GFSS methods, the computation time of BCM will be short since training a linear model for \(g_{\beta}\) is lighter than tuning deep neural networks in an end-to-end manner.

We introduce _shot-wise_ ensemble learning to few-shot learning when \(K>1\). This involves first preparing multiple \(L\)-shot datasets (\(L\leq K\)) by drawing samples from the \(K\)-shot dataset then aggregating outputs of models trained with the \(L\)-shot datasets. In our experiments, we split the \(5\)-shot dataset into five \(1\)-shot datasets and obtained six models by using five \(1\)-shot and one \(5\)-shot datasets. In inference, we computed a weighted average of the outputs of the models. The weights can be determined by, e.g., validation data or pre-defined values. In our experiments, we set one for the model with the \(1\)-shot dataset and five for the model with the \(5\)-shot dataset for simplicity.

## 5 Experiments

### Setup

Datasets.We used three FSS datasets: _PASCAL-5\({}^{i}\)_[4; 36; 37], _PASCAL-10\({}^{i}\)_[20; 36; 37], and _COCO-20\({}^{i}\)_[6; 38]. The PASCAL-\(10^{i}\) dataset was introduced to investigate the impact of increasing the number of novel classes [20].

Methods for comparison.We compared BCM with CAPL [16], BAM [14; 18],1 and DIaM [20]. Note that DIaM was regarded as a _simple_ method since it trains the last linear layer only, similarly to the simple methods [39; 40] proposed for few-shot object detection.

Footnote 1: The details of how we can modify BAM to output multiple novel classes are explained in [20].

Evaluation.We report the mIoUs over base and novel classes, referred to as the _Base_ and _Novel_ scores, respectively, where the background was not included in the Base score, similarly to a previous study [20]. We also report the average of the Base and Novel scores, called the _Mean_ score. All reported scores are the average of five independent trials.

Base-class model.We used the publicly available pre-trained model for GFSS,2_pyramid scene parsing network_ (PSPNet) [41] with the pre-trained ResNet-50 backbone [2]. It was trained with labeled data for base classes by using the stochastic gradient descent optimizer with an initial learning rate of \(2.5\times 10^{-4}\), momentum of \(0.9\), and weight decay of \(10^{-4}\). The batch size was \(12\), and number of epochs was \(20\) for COCO-\(20^{i}\) and \(100\) for PASCAL-\(5^{i}\) and PASCAL-\(10^{i}\).

Footnote 2: DIaM: https://github.com/sinanmr/DIaM

Detailed implementation.The implementation of BCM is based on the publicly available DIaM code. We followed the same data-loading and evaluation procedure and replaced the method part with BCM. Specifically, to train novel-class models \(g_{\beta}\) in Sec. 4.1, we used the _logistic regression_ in _Scikit-learn_[30],3 which uses the _L-BFGS-B_[42] method with a line-search strategy as the default solver. The regularization parameter was determined from the five-fold cross-validation from the ten candidates \(\{10^{-5},\dots,10^{5}\}\). The default values were used for the other hyper-parameters.

### Main results

Table 1 summarizes the average performance over the five trials for each method in the practical GFSS setting. BCM outperformed the other GFSS methods regarding the Novel and Mean scores. Notably, the Novel scores in the \(1\)-shot PASCAL-\(5^{i}\) and PASCAL-\(10^{i}\) settings substantially improved with BCM. Regarding the Base score, BCM achieved comparable/best performance thanks to it preventing catastrophic forgetting, as discussed in Sec. 4.3. We discuss these results from the viewpoint of our theory in Sec. 5.4.

These results indicate that BCM achieved the best performance without resorting to various techniques used with the other methods, such as meta-learning [43; 44], information maximization principle [21], and transductive learning [23]. The implementation of BCM was to train the final linear layer only, as described in Sec. 4.4, but we can use cutting-edge architectures and training techniques in practice, leading to further performance improvement.

### Ablation study

We investigated the effect of pre-processing (Tukey's ladder of powers transformation) and ensemble learning, explained in Sec. 4.5.

Table 2 shows the performance of four variations of BCM, i.e., with and without pre-processing and ensemble learning. Compared with the results in Tab. 1, BCM without data pre-processing and ensemble learning outperformed the other methods in the \(1\)-shot setting, showing that the simple rule and standard supervised learning improved the GFSS performance. The effectiveness of data pre-processing was much higher when the number of novel classes was small (see the PASCAL-\(5^{i}\) and PASCAL-\(10^{i}\) settings). However, the pre-processing decreased this performance slightly in the \(1\)-shot COCO-\(20^{i}\) setting.

Our ensemble-learning approach consistently improved GFSS performance, with a roughly \(5\%\) improvement on all datasets. Note that we can use standard ensemble-learning approaches in the \(1\)-shot setting, meaning that further performance improvement is possible in practice.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline  & & \multicolumn{4}{c}{PASCAL-\(5^{i}\)} \\ \cline{3-8}  & & \multicolumn{2}{c}{1-shot} & \multicolumn{2}{c}{5-shot} \\ \cline{3-8} Method & & Base & Novel & Mean & Base & Novel & Mean \\ \hline CAPL [16] & CVPR’22 & \(64.80\) & \(17.46\) & \(41.13\) & \(65.43\) & \(24.43\) & \(44.93\) \\ BAM [14] & CVPR’22 & \(\mathbf{71.60}\) & \(27.49\) & \(49.55\) & \(\mathbf{71.60}\) & \(28.96\) & \(50.28\) \\ DIaM [20] & CVPR’23 & \(70.89\) & \(35.11\) & \(53.00\) & \(70.85\) & \(55.31\) & \(63.08\) \\ BCM & (Ours) & \(71.15\) & \(\mathbf{41.24}\) & \(\mathbf{56.20}\) & \(71.23\) & \(\mathbf{55.36}\) & \(\mathbf{63.29}\) \\ \hline \multicolumn{8}{c}{PASCAL-\(10^{i}\)} \\ \hline CAPL [16] & CVPR’22 & \(53.78\) & \(15.01\) & \(34.40\) & \(57.02\) & \(20.40\) & \(38.71\) \\ BAM [14] & CVPR’22 & \(69.02\) & \(15.48\) & \(42.25\) & \(69.18\) & \(21.51\) & \(45.35\) \\ DIaM [20] & CVPR’23 & \(\mathbf{70.26}\) & \(31.29\) & \(50.77\) & \(\mathbf{70.25}\) & \(51.89\) & \(61.07\) \\ BCM & (Ours) & \(70.07\) & \(\mathbf{35.94}\) & \(\mathbf{53.01}\) & \(70.12\) & \(\mathbf{53.49}\) & \(\mathbf{61.81}\) \\ \hline \multicolumn{8}{c}{COCO-\(20^{i}\)} \\ \hline CAPL [16] & CVPR’22 & \(43.21\) & \(7.21\) & \(25.21\) & \(43.71\) & \(11.00\) & \(27.36\) \\ BAM [14] & CVPR’22 & \(\mathbf{49.84}\) & \(14.16\) & \(32.00\) & \(49.85\) & \(16.63\) & \(33.24\) \\ DIaM [20] & CVPR’23 & \(48.28\) & \(17.22\) & \(32.75\) & \(48.37\) & \(28.73\) & \(38.55\) \\ BCM & (Ours) & \(49.43\) & \(\mathbf{18.28}\) & \(\mathbf{33.85}\) & \(\mathbf{49.88}\) & \(\mathbf{30.60}\) & \(\mathbf{40.24}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Average mIoU over five trials. Base and Novel represent mIoU scores over base and novel classes, respectively. Mean shows average of Base and Novel scores. Results of comparison methods were obtained from [20]. All methods use ResNet-50 as backbone.

### Number of chosen base classes

Figure 5 shows the frequency of \(|\mathcal{B}|\), the number of chosen base classes, over 20 runs (four splits and five trials). Overall, \(|\mathcal{B}|\) tended to decrease with the increase in \(K\). We hypothesize that noisy pairs appear relatively smaller than frequent pairs when \(K=5\), and the top-\(1\) strategy ignored such a noisy pair. In particular, the median value of the frequency in \(5\)-shot was 1 on PASCAL-\(5^{i}\) and 2 on COCO-\(20^{i}\).

Another observation is that \(|\mathcal{B}|\) was much smaller than the number of base classes. For example, the largest \(|\mathcal{B}|\) was four in the \(5\)-shot COCO-\(20^{i}\) setting, meaning that less than \(7\%(\approx 4/61)\) of base classes (including background) were chosen in the BCM training step. These results indicate that we do not need to prepare \(g_{\beta}\) for many base classes, and training and inference times do not increase rapidly to the number of base classes. Moreover, \(|\mathcal{B}|\) tends to be small in practice, so IoUs on most of the base classes are perfectly maintained, as shown in Proposition 4.1.

### Which classes were chosen?

We explored which class consists of base- and novel-class pairs in the BNM and recorded pairs in the \(5\)-shot COCO-\(20^{i}\) setting. In most cases, the background was chosen, meaning that the base-class model recognized novel-class objects as the background. Sometimes, the pairs summarized in Tab. 3 were chosen, showing that the related classes were chosen with BCM. The results empirically confirmour idea that a novel class is classified as the background or a similar base class with the base-class model.

### Effect of top-\(s\) strategy

We varied \(s\) in the top-\(s\) strategy and investigated how \(s\) affects the segmentation performance and size of \(\mathcal{B}\) in the \(5\)-shot PASCAL-\(5^{i}\) setting. The results in other settings can be found in Appendix A.1.

Figures 7(a) and 7(b) respectively illustrate the Base and Novel scores with respect to \(s\in\{1,2,3,5,10\}\), showing that the median and minimum scores decreased as \(s\) increased.

We also show the size of \(\mathcal{B}\) in Fig. 7(c). The number of chosen base-classes increased, but it was upper-bounded by a certain value. Even though we increased \(s\), the chosen base-classes might be the same. In addition, the increase in \(s\) also led to longer training time. In this sense, smaller \(s\) is preferable from the perspective of computation time.

In summary, a higher \(s\) may result in selecting redundant base classes and cause performance degradation, and larger \(|\mathcal{B}|\) requires many models for \(g_{\beta}\), resulting in longer computation time for training. We thus recommend \(s=1\) since it is the best choice based on GFSS performance and computation time.

### Computation time

The computation time was measured on a machine equipped with an NVIDIA(r) V100, 16 CPU cores, and 32GB memory.

Training time.We plot the training time [s] with BCM in Fig. 6. CAPL and DIaM are not shown since CAPL requires hours of training time due to meta-learning, and DIaM, which is based on transductive learning, does not optimize models other than the inference phase. In the \(1\)-shot scenario, training time was less than 1 min. The training time increased as the number of novel classes increased. Although we used ensemble learning in the \(5\)-shot scenario, the training time in the \(5\)-shot COCO-\(20^{i}\) setting was about 2 min. Note that training was done by CPU computation, meaning that further acceleration is expected by GPU computation.

Inference time.Figure 7 shows the inference time [ms] of CAPL, DIaM, and BCM. Since DIaM is based on transductive learning, the inference time was slower than the inductive methods, i.e., CAPL and BCM. BCM requires computations of the novel-class models in addition to that of the base-class model, but the total inference time was comparable to that of the end-to-end model, i.e., CAPL.

Note that BCM has \(|\mathcal{B}|\) final linear layers for novel classes, leading to a subtle slowdown when switching the layers, unlike the end-to-end computation of CAPL. Our implementation used CPUs for training the final linear layers, as Scikit-learn is used, unlike CAPL on a GPU. This device difference might be another reason for the slowdown. In practice, a more sophisticated implementation will shorten the gap between CAPL and BCM.

Figure 8: Effect of \(s\) in top-\(s\) strategy in \(5\)-shot PASCAL-\(5^{i}\) setting

Conclusion

Summary.We presented a simple yet effective GFSS method called BCM. BCM is based on a mapping between base and novel classes and trains novel-class models by simple supervised learning without resorting to meta-learning and the information maximization principle. Since we can use standard supervised learning, training can be done efficiently using off-the-shelf software. We can use the featurizer and base-class model without modifying their weights, enabling us to use cutting-edge public foundation models with BCM. We theoretically showed that the mIoU over most of the base classes is perfectly maintained. Through numerical experiments, we demonstrated the superior performance of BCM method against state-of-the-art GFSS methods.

Limitations.BCM has limitations that need to be resolved. First, although the final performance of BCM outperformed the other GFSS methods, the performance improvement in the 5-shot scenario was slight, meaning that there is room for improvement. Second, while BCM perfectly maintains the segmentation performances of most of base-classes, it does not improve such performance using novel-class data. A possible direction to resolve these limitations is to investigate the strategy for creating mapping other than the top-\(s\) strategy and use more recent powerful supervised/few-shot learning methods.

## Acknowledgments

The authors would like to thank the anonymous reviewers for their helpful comments and suggestions.

## References

* [1]S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtaravaz, and D. Terzopoulos (2022) Image segmentation using deep learning: a survey. IEEE TPAMI44 (7), pp. 3523-3542. Cited by: SS1.
* [2]K. He, X. Zhang, S. Ren, and J. Sun (2016) Deep residual learning for image recognition. In CVPR, Cited by: SS1.
* [3]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby (2021) An image is worth 16x16 words: transformers for image recognition at scale. In ICLR, Cited by: SS1.
* [4]A. Shaban, S. Bansal, Z. Liu, I. Essa, and B. Boots (2017) One-shot learning for semantic segmentation. In BMVC, Cited by: SS1.
* [5]C. Zhang, G. Lin, F. Liu, R. Yao, and C. Shen (2019) CANet: class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In CVPR, Cited by: SS1.
* [6]K. Nguyen and S. Todorovic (2019) Feature weighting and boosting for few-shot segmentation. In ICCV, Cited by: SS1.
* [7]K. Wang, J. H. Liew, Y. Zou, D. Zhou, and J. Feng (2019) PANet: few-shot image semantic segmentation with prototype alignment. In ICCV, Cited by: SS1.
* [8]B. Yang, C. Liu, B. Li, J. Jiao, and Q. Ye (2020) Prototype mixture models for few-shot semantic segmentation. In ECCV, pp. 763-778. Cited by: SS1.
* [9]H. Wang, X. Zhang, Y. Hu, Y. Yang, X. Cao, and X. Zhen (2020) Few-shot semantic segmentation with democratic attention networks. In ECCV, pp. 730-746. Cited by: SS1.
* [10]M. Boudiaf, H. Kervadec, Z. Imtiaz Masud, P. Piantanida, I. B. Ayed, and J. Dolz (2021) Few-shot segmentation without meta-learning: a good transductive inference is all you need?. In CVPR, pp. 13979-13988. Cited by: SS1.
* [11]B. Zhang, J. Xiao, and T. Qin (2021) Self-guided and cross-guided learning for few-shot segmentation. In CVPR, pp. 8312-8321. Cited by: SS1.
* [12]Z. Lu, S. He, X. Zhu, L. Zhang, Y. Song, and T. Xiang (2021) Simpler is better: few-shot semantic segmentation with classifier weight transformer. In ICCV, pp. 8741-8750. Cited by: SS1.

* [13] Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li, and Jiaya Jia. Prior guided feature enrichment network for few-shot segmentation. _IEEE TPAMI_, 44(2):1050-1065, 2022.
* [14] Chunbo Lang, Gong Cheng, Binfei Tu, and Junwei Han. Learning what not to segment: A new perspective on few-shot segmentation. In _CVPR_, pages 8057-8067, 2022.
* [15] Nico Catalano and Matteo Matteucci. Few shot semantic segmentation: a review of methodologies and open challenges, 2023.
* [16] Zhuotao Tian, Xin Lai, Li Jiang, Shu Liu, Michelle Shu, Hengshuang Zhao, and Jiaya Jia. Generalized few-shot semantic segmentation. In _CVPR_, pages 11563-11572, 2022.
* [17] Sun-Ao Liu, Yiheng Zhang, Zhaofan Qiu, Hongtao Xie, Yongdong Zhang, and Ting Yao. Learning orthogonal prototypes for generalized few-shot semantic segmentation. In _CVPR_, pages 11319-11328, 2023.
* [18] Chunbo Lang, Gong Cheng, Binfei Tu, Chao Li, and Junwei Han. Base and meta: A new perspective on few-shot segmentation. _IEEE TPAMI_, 45(9):10669-10686, 2023.
* [19] Zhihe Lu, Sen He, Da Li, Yi-Zhe Song, and Tao Xiang. Prediction calibration for generalized few-shot semantic segmentation. _IEEE TPAMI_, 32:3311-3323, 2023.
* [20] Sina Hajimiri, Malik Boudiaf, Ismail Ben Ayed, and Jose Dolz. A strong baseline for generalized few-shot semantic segmentation. In _CVPR_, pages 11269-11278, 2023.
* [21] R. Linsker. Self-organization in a perceptual network. _Computer_, 21(3):105-117, 1988.
* [22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In _NIPS Deep Learning and Representation Learning Workshop_, 2015.
* [23] Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. _Semi-Supervised Learning_. The MIT Press, 2010.
* [24] Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulo, Elisa Ricci, and Barbara Caputo. Modeling the background for incremental learning in semantic segmentation. In _CVPR_, 2020.
* [25] Ze Yang, Ruibo Li, Evan Ling, Chi Zhang, Yiming Wang, Dezhao Huang, Keng Teck Ma, Minhoe Hur, and Guosheng Lin. Label-guided knowledge distillation for continual semantic segmentation on 2D images and 3D point clouds. In _ICCV_, pages 18601-18612, 2023.
* [26] Beomyoung Kim, Joonsang Yu, and Sung Ju Hwang. ECLIPSE: Efficient continual learning in panoptic segmentation with visual prompt tuning. In _CVPR_, pages 3346-3356, 2024.
* [27] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Freshe Khani, Omar Khattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Re, Dorsa Sadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramer, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the opportunities and risks of foundation models, 2022.
* [28] Michael McCloskey and Neal J. Cohen. Catastrophic interference in connectionist networks: The sequential learning problem. In _Psychology of Learning and Motivation_, volume 24, pages 109-165. Academic Press, 1989.

* [29] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. _Proceedings of the National Academy of Sciences_, 114(13):3521-3526, 2017.
* [30] Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and Edouard Duchesnay. Scikit-learn: Machine learning in Python. _JMLR_, 12:2825-2830, 2011.
* [31] Nitesh V. Chawla. _Data Mining for Imbalanced Datasets: An Overview_, pages 853-867. Springer, 2005.
* [32] John Wilder Tukey. _Exploratory Data Analysis_. Addison-Wesley Publishing Company, 1977.
* [33] Shuo Yang, Lu Liu, and Min Xu. Free lunch for few-shot learning: Distribution calibration. In _ICLR_, 2021.
* [34] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. _The Elements of Statistical Learning_. Springer New York Inc., 2009.
* [35] Zhi-Hua Zhou. _Ensemble Methods: Foundations and Algorithms_. Chapman & Hall/CRC, 2012.
* [36] Mark Everingham, SM Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The PASCAL visual object classes challenge: A retrospective. _IJCV_, 111(1):98-136, 2015.
* [37] Bharath Hariharan, Pablo Arbelaez, Ross Girshick, and Jitendra Malik. Simultaneous detection and segmentation. In _ECCV_, 2014.
* [38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In _ECCV_, pages 740-755, 2014.
* [39] Xin Wang, Thomas Huang, Joseph Gonzalez, Trevor Darrell, and Fisher Yu. Frustratingly simple few-shot object detection. In _ICML_, volume 119, pages 9919-9928, 2020.
* [40] Ze Yang, Chi Zhang, Ruibo Li, Yi Xu, and Guosheng Lin. Efficient few-shot object detection via knowledge inheritance. _IEEE Transactions on Image Processing_, 32:321-334, 2023.
* [41] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In _CVPR_, 2017.
* [42] Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale bound-constrained optimization. _ACM Trans. Math. Softw._, 23(4):550-560, 1997.
* [43] Jurgen Schmidhuber. Evolutionary principles in self-referential learning. On learning now to learn: The meta-meta-meta...hook. Diploma thesis, Technische Universitat Munchen, Germany, 1987.
* [44] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In _ICML_, volume 70, pages 1126-1135, 2017.

Appendix

### Effect of top-\(s\) strategy

Figs. 9 and 10 show the effect of \(s\) in the top-\(s\) strategy in the \(1\)-shot PASCAL-\(5^{i}\) and COCO-\(20^{i}\) settings, respectively. We can observe a similar tendency to that discussed in Sec. 5.6.

### Example when \(s>1\)

We explain obtaining the prediction when \(s>1\).

Even when \(s>1\), the inference procedure works as explained in Sec. 4.2 since a _single_ base class is mapped to multiple novel classes in BNM. We show the case when \(s=2\) with a tiny example: the base classes are \(`0\) and \(`1\)', and the novel class is '\(2\)'. Suppose that we have the following BNM when \(s=2\) in Tab. 4. This table shows when the base class \(`0\)' is mapped to the novel class '\(2\)' and '\(1\)' is also mapped to '\(2\)'. In this case, we have the two models: \(g_{\beta=0}\) returns '\(0\)' or '\(2\)', and \(g_{\beta=1}\) returns '\(1\)' or '\(2\)'.

For each pixel, the base-class model outputs either '\(0\)' or '\(1\)'. We then compute the prediction of the corresponding model and overwrite it. Since BCM does not need to overwrite the same pixel multiple times, we can straightforwardly combine predictions of \(g_{\beta}\) for the final prediction.

### Broader impact

The idea behind BCM will positively affect future studies on few-shot learning. Our future work will lead to more powerful visual understanding systems. Regarding negative societal impact, we expect BCM will not have a direct path to harmful applications. However, harmful actors may maliciously use visual understanding systems. To prevent such a malicious use of technology, we need to pay attention to events in our society.

\begin{table}
\begin{tabular}{c c} \hline \hline Base class & Set of novel classes \\ \hline
0 & 2 \\
1 & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Example BNM when \(s=2\)

Figure 10: Effect of \(s\) in top-\(s\) strategy in \(1\)-shot COCO-\(20^{i}\) setting

Figure 9: Effect of \(s\) in top-\(s\) strategy in \(1\)-shot PASCAL-\(5^{i}\) setting

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: We stated the main claims based on the results of the theoretical analysis and numerical experiments. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discuss the limitations of our work in Sec. 6. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We provided the proof of the theoretical result. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We disclosed the details of settings and learning procedures in our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [No] Justification: However, we will release the code after we get our organization's permission. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We wrote the details. Also, we used the pre-trained models and data loading procedure provided by the publicly-available repository of existing method. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Partially yes. We used the box plot to show the deviation of the performance on some experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We disclosed the computing resources used in our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Our research conforms with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: We discussed broader impacts in Appendix A.3. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We will not release data or models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We properly cited the assets used in our paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Our paper does not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Our paper does not involve crowdsourcing nor research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.