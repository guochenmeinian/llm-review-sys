# Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly

Qizhang Li\({}^{1,2}\), Yiwen Guo\({}^{3}\), Wangmeng Zuo\({}^{1}\), Hao Chen\({}^{4}\)

\({}^{1}\)Harbin Institute of Technology, \({}^{2}\)Tencent Security Big Data Lab, \({}^{3}\)Independent Researcher, \({}^{4}\)UC Davis

(liqizhang95,guoyiwen89)@gmail.com wmzuo@hit.edu.cn chen@ucdavis.edu

Yiwen Guo leads the project and serves as the corresponding author.

###### Abstract

The adversarial vulnerability of deep neural networks (DNNs) has drawn great attention due to the security risk of applying these models in real-world applications. Based on transferability of adversarial examples, an increasing number of transfer-based methods have been developed to fool black-box DNN models whose architecture and parameters are inaccessible. Although tremendous effort has been exerted, there still lacks a standardized benchmark that could be taken advantage of to compare these methods systematically, fairly, and practically. Our investigation shows that the evaluation of some methods needs to be more reasonable and more thorough to verify their effectiveness, to avoid, for example, unfair comparison and insufficient consideration of possible substitute/victim models. Therefore, we establish a transfer-based attack benchmark (TA-Bench) which implements 30+ methods. In this paper, we evaluate and compare them comprehensively on 25 popular substitute/victim models on ImageNet. New insights about the effectiveness of these methods are gained and guidelines for future evaluations are provided. Code at: https://github.com/qizhangli/TA-Bench.

## 1 Introduction

In recent years, deep neural networks (DNNs) have demonstrated unprecedented success in various applications. However, the success comes at a price: DNNs are vulnerable to adversarial examples crafted by adding imperceptible perturbations to inputs (_e.g._, images). The existence of adversarial examples poses a significant threat to the security and reliability of DNNs, especially in safety-critical applications such as autonomous driving, biometrics, and medical image analysis.

There are many different ways of generating adversarial examples and performing attacks. Transfer-based attacks, that are capability of compromising DNNs without having access to their network architecture and parameters, have been widely studied over the past few years. To issue such transfer-based attacks, an attacker first collects a substitute model or a set of substitute models, then computes gradients on the substitute model(s) and perform optimization based on the gradients. Performance of the attacks largely rely on the transferability of the generated adversarial examples. Over the years, a large number of methods have been proposed to improve the adversarial transferability. The methods innovate various aspects of the attack procedure, ranging from _substitute model training_[47, 78, 13, 27] to _gradient computation_ (that modifies loss or forward/backward architectures given well-trained substitute models) [77, 11, 20, 14, 65, 26, 60, 57, 61, 74, 21, 15, 74] to _input augmentation_[67, 7, 28, 58]_and the optimizer_ that applies the gradients [6, 28, 59]. Some methods further propose to train a generative model using additional data for obtaining transferable adversarial examples [35, 44, 75, 36, 69].

Despite all the progress, there still lacks a standardized benchmark that could be taken advantage of to compare these methods systematically, fairly, and practically. Existing evaluations in the literature suffer from several limitations. First, there is a paucity of variety in the tested models, while certain methods may exhibit superior performance only with specific substitute/victim architectures. Second, many methods verified their efficacy only with a basic optimization back-end, _e.g._, I-FGSM [24], despite the existence of more advanced optimization including input augmentations. It is hence unclear whether the benefits of an innovation in these methods can be similarly brought by incorporating these more advanced optimization techniques, which could undermine the reliability of the evaluations.

To address these problems, we create a transfer-based attack benchmark (TA-Bench), allowing researchers to compare a variety of methods in a fair and reliable manner. We believe that TA-Bench will foster the development of adversarial machine learning and inspire effective ways of generating adversarial examples for evaluating the robustness of DNNs. Our main contributions include (but not limited to) the following items.

**A More Advanced Optimization Back-end.** We have evaluated various combinations of input augmentation mechanisms and optimizers on our TA-Bench. Our results convey that, with a few exceptions, combining more types of augmentations leads to more powerful attacks, and it could be more reasonable to evaluate the effectiveness of input augmentation mechanisms and optimizers in combination rather than in isolation. Besides, the obtained combination is suggested to be a more advanced optimization back-end for evaluating the "gradient computation" methods and "substitute model training" methods.

**Insightful Observations from Comprehensive Evaluations.** We consider various substitute/victim models that are considered popular, including CNNs (_e.g._, ResNet-50 [16], VGG-19 [49] with batch normalization, Inception v3 [49], EfficientNetV2-M [51], and ConvNeXt-B [32]), vision transformers (_e.g._, ViT-B [8], DeiT-B [54], Swin-B [31], and BEiT-B [1]), and a MLP (MLP-Mixer [53]). Comprehensive results on our TA-Bench systematically shows how the performance of transfer-based attacks varies with different choices of substitute/victim models. These results demonstrate that adopting transformers as the substitute model generally yields superior attack performance for "gradient computation" methods, comparing with using traditional convolutional models (as the substitute model).

**A Unified Codebase.** We offer an open-source codebase for TA-Bench, featuring a well-organized code structure that can effectively accommodate a diverse range of transfer-based attacks, as well as various substitute/victim models. It provides a unified setting for evaluations, ensuring consistency and reproducibility in experimental results. The code is at https://github.com/qizhangli/TA-Bench.

## 2 Related Work

**Transfer-based Attacks.** Transfer-based attacks emerged as a result of the discovery that adversarial examples are not only powerful against the model they were generated on but also effective against other models. By exploiting such a discovery, attackers can use adversarial examples created on some substitute models to attack the victim model, and a series of transfer-based attacks have been proposed. These methods have innovated various aspects of the attack procedure, and we will carefully discuss them in Section 3.2.

**Related Benchmarks.** There have been several libraries or benchmarks for generating adversarial examples, such as CleverHans [39], Foolbox [41, 42], RobustART [52], Torchattacks [23], _etc_. However, these benchmarks only cover a very limited number of transfer-based methods, as they were developed for evaluating the robustness of DNNs against not only black-box attacks but also white-box attacks. In particular, most of the methods are outdated and only considered as baseline methods due to the rapid development in this field. By contrast, our TA-Bench focuses on transfer-based attacks, and it implements 30+ methods in order to perform systematical, practical, and fair comparisons. A contemporary benchmark [76] also evaluates many transfer-based attacks and it draws attention to the stealthiness of the adversarial examples. Our contributions are mostly orthogonal to theirs, as we focus on fair and practical comparison between different methods. We will show some previous conclusions might be overthrown in practice. We provide new insights and guidance for evaluation to assist the future work in the community.

Our Benchmark

### Threat Model

In a black-box scenario, the adversary has limited access to the victim model. Transfer-based attack aims to compromise the victim model by generating adversarial examples on a substitute model or a set of substitute models. Recent transfer-based methods, with few exceptions [25; 48], tested in a setting where: 1) training data of the victim model is accessible and training/fine-tuning a model on such data is possible for the adversary [47; 78; 13; 27; 40; 35; 44; 75; 36], or 2) the adversary is able to collect at least one substitute model trained using the same dataset that the victim model learned from [37; 77; 22; 11; 20; 14; 65; 26; 60; 57; 61; 74; 21; 15; 74; 24; 34; 67; 7; 58; 6; 28]. We follow this assumption for setting up the benchmark. Specifically, nothing except for the training data is known about the victim model for performing attacks in our threat model, _i.e._, the adversary has no idea about the pre-processing pipeline, architecture, and parameters.

Adversarial examples on the benchmark are all obtained by performing pixel-wise perturbations to benign images under an \(\ell_{p}\) constraint.

### Methodologies

In order to compare different methods more reasonably, we roughly divide existing methods, based on their main innovations, into four categories highlighted as follows.

**Input Augmentation and Optimizer.** To craft an adversarial example on any given substitute model, the perturbation can be optimized as in the white-box setting. Gradient-based iterative optimization is commonly utilized, in which the perturbation is initialized to a zero tensor (_e.g._, in I-FGSM [24]) or a random tensor whose entries are sampled from a distribution (_e.g._, in PGD [34]). Image data augmentation has been considered for generating transformation-robust perturbations to each benign example, for example in diverse inputs I-FGSM (DI\({}^{2}\)-FGSM) [67], translation-invariant I-FGSM (TI-FGSM) [7], scale-invariant I-FGSM (SI-FGSM) [28], and Admix [58]. Several attacks also innovate by taking advantage of the momentum optimizer, _e.g._, momentum I-FGSM (MI-FGSM) [6], Nesterov I-FGSM (NI-FGSM) [28], and pre-gradient guided momentum I-FGSM (PI-FGSM) [59]. In general, these methods are all architecture-independent.

**Gradient Computation (DNN-Specific).** There is a belief that improved adversarial transferability can be achieved by modifying the loss or the backpropagation process. For backpropagation, both the forward and the backward pass can be altered to achieve powerful attacks, and a series of methods, including SGM [65], LinBP [14], ConBP [72], PNA [62], and SE [38] have been proposed. Some methods advocate loss terms obtained on a middle layer of the substitute model (_e.g._, NRDM [37], TAP [77], FDA [11], LIA [20], ILA++ [26], FIA [61], NAA [74]), while other methods stick with loss computed on the final layers (_e.g._, IR [60], VT [57], and TAIG [21]).

**Substitute Model Training.** Though most prior work uses off-the-shelf models (that could be collected on the Internet) directly as substitute models, some methods advocate fine-tuning these models or even training new ones to better suit the goal of achieving transferable adversarial examples. For instance, RFA [47] suggests adopting adversarial training to obtain substitute models. DRA [78] fine-tunes the substitute model to push the adversarial examples away from the distribution of their benign counterparts during performing attacks. LGV [13] fine-tunes the substitute model with a high learning rate and to collect a set of models on the training trajectory. A very recent method proposed by Li _et al._ performs fine-tuning in order to obtain Bayesian substitute models [27].

**Generative Modeling.** In addition to the above mentioned attacks, there are another line of transfer-based attacks that use generative models to craft adversarial examples. These methods often require additional data for training the generative model. Once properly trained, the model can generate transferable adversarial examples across different victim models. Some effective methods concerned in this line include CDA [35], GAPF [44], BIA [75], TTP [36], C-GSP [69], _etc._

**All these mentioned methods have been implemented on our benchmark.** That is, we implemented **30+ methods** from these four categories for comprehensive evaluation and comparison of transfer-based attacks.

### Victim Models and Substitute Models

With a surge of innovation in deep learning, there exists a variety of image classification DNNs. Each has its own advantages. In practice, an engineer can choose any of them to train and deploy according to his or her specific requests. An adversary whose aim is to compromise a computer vision service developed by the engineer then anticipate the generated adversarial examples transfer well to all these possible models. Hence, to make the evaluation comprehensive and practical, we consider various victim models that are considered popular, including **CNNs** (_e.g._, ResNet-50 [16], VGG-19 [49] with batch normalization, Inception v3 [49], EfficientNetV2-M [51], and ConvNeXt-B [32]), **vision transformers** (_e.g._, ViT-B [8], DeiT-B [54], Swin-B [31], and BEiT-B [1]), and **a MLP** (MLP-Mixer-B [53]). It is worth noting that all of these models were obtained directly from an open-source repository timm [63] on GitHub, and our benchmark can be easily updated to include new models in the future.

Having witnessed the development of image classification architectures, it is unwise for the adversary to stick with conventional models (_e.g._, ResNets, VGG-Nets, and Inceptions), since it is possible that generating adversarial examples on a more advanced substitute model leads to superior attack success rates. Thus, on this benchmark, we employ the 10 victim models named above to generate adversarial examples and evaluate the attack performance of all options. As will be shown in Section 4.3, many new insights can be gained from the evaluation results.

### Experimental Settings and Implementation

All evaluations on our benchmark are conducted on ImageNet [43]. We randomly selected 5,000 benign examples that could be correctly classified by all the victim models, from the ImageNet validation set, to craft adversarial examples. Filenames of these benign examples will be provided, for reproducing results and testing new methods in the future. A distance metric is required to measure the magnitude of perturbations. We adopt the popular \(\ell_{p}\) distance for \(p\in\{\infty,2\}\) and set the perturbation budget under \(\ell_{\infty}\) and \(\ell_{2}\) constraints to \(\epsilon=8/255\) and \(\epsilon=5\), respectively, to guarantee that the adversarial perturbations are almost imperceptible. The optimization process of each compared method runs \(100\) iterations with a step size of \(1/255\) and \(1\) for \(\ell_{\infty}\) constraint and \(\ell_{2}\) constraint, respectively. For each victim model, we pre-process its input in exactly the same way as their official implementation to ensure a practical evaluation of attack performance, and note that the adversary has no idea about the detailed implementation of this pre-processing. For instance, when evaluating the performance of attacking a ResNet-50 victim, we first resize an adversarial example to 256 \(\times\) 256 by bilinear interpolation, then crop the image to 224 \(\times\) 224 in the center, and finally feed the 224 \(\times\) 224 image into the victim model and evaluate whether the attack is successful or not. Implementation details about all the supported methods are provided in Section F in the Appendix. All experiments are performed on an NVIDIA V100 GPU.

For evaluating the transferability of adversarial examples, we use the accuracy of victim models for classifying the adversarial examples as a measure. Using the prediction accuracy of victim models instead of the attack success rate makes it easier to incorporate other victim models in the future, as a reasonable calculation of attack success rate often requires the benign counterparts of the adversarial examples be classified correctly by all victim models. With a specific choice of the substitute model, prior work often evaluates the average accuracy (AA) over all victim models for comparing different attacks. However, since our benchmark studies a variety of substitute models, we further evaluate the average AA (AAA), the worst AA (WAA), and the best AA (BAA) over all choices of these substitute models. **Lower AAA, WAA, and BAA indicate stronger attacks and more vulnerable victim models.**

We have built a codebase consisting of modular components that serve as the basis of TA-Bench. By leveraging modular design principles, the substitute and victim models, back-end methods, and hyper-parameters can be easily adapted to help the future work of the community.

## 4 Evaluations and Analyses

In Section 4.1, we identify a pre-processing pipeline that is more practical. In Section 4.2, we investigate an improved back-end method by evaluating the possible combinations of iterative optimization methods. We then re-evaluate state-of-the-arts under a comprehensive and unifiedbenchmark, which incorporates various substitute and victim models, to assess the effectiveness of these methods in Section 4.3 and Section 4.4.

### Pre-processing in Practice

Modern image classification models (especially CNNs) can take images of various sizes. After investigating experimental settings of previous transfer-based attacks, we found that the adversarial examples were often evaluated by feeding them into the victim models directly (without taking pre-processing operations of the victim models into account, _e.g._, resize and crop) [28; 57; 59; 58] or just resize to the input size of the victim models [61; 74]. However, these victim models, when deployed, are often equipped with inference time data pre-processing to improve their effectiveness and efficiency. Getting rid of it may lead to over-estimation of adversarial vulnerability.

As mentioned in Section 3.4, on our benchmark, we follow the default pre-processing pipeline of each victim model and evaluate under the circumstances where pre-processing often exists. We observed degraded attack performance under such circumstances (see Figure 1 for results). Obviously, the adversarial examples seem less vulnerable when the default test-time pre-processing is re-introduced to each victim model. Given the same I-FGSM adversarial examples, the average accuracy of victim models increases to 87.79% (from 86.16%) with pre-processing, confirming that **ignoring pre-processing operations of victim models indeed leads to over-estimation of their adversarial vulnerability**. Similar observations can be made with other substitute models and other attack methods. Considering that the pre-processing pipeline of the victim model is inaccessible to the adversary, it is infeasible for the adversary to follow the same pipeline when generating adversarial examples. On the substitute models, a safe choice is to resize each of their inputs to the default size without cropping them.

### Evaluation of Input Augmentation and the Optimizer

To verifying the effectiveness of a newly developed computer vision architecture (_e.g._, vision transformers), it is common to show that its performance surpasses previous state-of-the-arts on a challenging benchmark dataset (_e.g._, ImageNet), and test of the newly developed architecture may utilize a combination of advanced optimization strategies (_e.g._, AdamW [33] + mixup [73] + cutmix [71] + stochastic depth [19]) if such a combination is beneficial [2; 55]. Novel optimization strategies are also often tested in combination with existing ones, to show their consistent effectiveness [12].

Likewise, the transferability of adversarial examples can also benefit from appropriate choices of input augmentations and the optimizer, however, ways of innovating these optimization strategies are often evaluated in isolation in this setting. On our benchmark, we, for the first time, evaluate combinations of different choices of input augmentations and the optimizer systematically. Specifically, we performed a grid search to seek the optimal combination of I-FGSM [24], PGD [34], DI\({}^{2}\)-FGSM [67], TI-FGSM [7], SI-FGSM [28], Admix [58], NI-FGSM [28], MI-FGSM [28], PI-FGSM [6], _etc_. Since Admix inherently includes SI-FGSM, we have SI-FGSM by default when Admix is chosen. NI-FGSM, MI-FGSM, and PI-FGSM seem not orthogonal and thus they are tested in a mutually exclusive way in our experiment. The same for I-FGSM and PGD. For TI-FGSM, an alternative implementation where inputs are translated is adopted, as it is more effective than its suggested approximation which convolves gradients. Besides the augmentations in DI\({}^{2}\)-FGSM, TI-FGSM, SI-FGSM, and Admix, we consider several other input augmentation mechanisms including adding uniform noise to the input (which was used in VT [57] and TAIG [21]) and randomly dropping some patches of the perturbation (which was used in IR [60]). We call the two methods as UN and DP, respectively. Note that in a previous work [66], Gaussian noise is added to the input in each attack iteration. In this study, we

Figure 1: Attack performance with different pre-processing strategies in the target environment, and the AAA is indicated by dotted lines (lower means more vulnerable to the attack). The adversarial examples were generated using I-FGSM on a ResNet-50 substitute model under the \(\ell_{\infty}\) constraint with \(\epsilon=8/255\).

ot for uniform noise, as it is now more commonly adopted. It is also worth noting that the original implementation of SI-FGSM and Admix uses several augmented copies of an input and averages the gradients computed on these copies for optimization. Such an approach increases the computational complexity of performing attacks, and, in fact, all input augmentation mechanisms in this category can be improved by such an approach [76], and, for reducing computational complexity, we only craft one augmented input at each iteration.

Each possible combination is tested with every possible substitute model whose name has been mentioned in Section 3.3 to evaluate the attacking performance over other models which are considered as the victim models. The results are sorted by AAA, in a descending order from left to right, and demonstrated in Figure 2. It illustrates not only AAA but also the range between BAA and WAA (as "error bars"). **We see that the optimal AAA is achieved by UN-DP-DI\({}^{2}\)-TI-PI-FGSM, which is \(\mathbf{42.42\%}\)**. The detailed AA when each substitute model is chosen for crafting adversarial examples by the method is reported in Section E. Inspecting the obtained results, we found that the performance gap between MI-FGSM, NI-FGSM, and PI-FGSM is marginal. For instance, the top 3 solutions (with PI-FGSM, NI-FGSM, and MI-FGSM, respectively) lead to similar AAA (\(42.42\%\), \(42.45\%\), and \(42.46\%\)). The performance of I-FGSM and PGD is also similar. In most cases, PGD leads to slightly inferior performance than that of I-FGSM, accordingly to our experimental results.

In general, more input augmentation mechanisms leads to more powerful attacks. Yet there are exceptions. With DP adopted, SI-FGSM and Admix that apply input scaling and mixing fail to manifest their gains regarding the adversarial transferability. In particular, the AAA increases to \(43.12\%\) and \(44.11\%\) when further adding SI and Admix to UN-DP-DI\({}^{2}\)-TI-PI-FGSM, respectively. This is in contrast to the previous belief that these two methods are effective, and a possible explanation is that, when adopting DP and Admix/SI-FGSM simultaneously, the augmentation becomes too strong to keep the input a in-distribution sample. We adopted the default hyper-parameters for all combined methods and it is possible (yet computationally very intensive since the number of combinations is huge) to carefully tune hyper-parameters to achieve even better combinations. We will leave it to future work.

### Evaluation of "Gradient Computation" Methods and "Substitute Model Training" Methods

The previous section evaluates input augmentations and optimizers, and, in this section, we shall focus on "gradient computation" and "substitute model training" methods.

Firstly, we would like to emphasize that certain gradient computation innovations (_e.g._, IR [60], VT [57], and TAIG [21]) incorporate input augmentations for averaging the gradients obtained from multiple randomly augmented inputs at each iteration. These methods require high computational cost, and comparing them with other methods in the same category directly is unfair, as the other methods can also apply random augmentation and gradient averaging to boost their performance. Taking VT [57] as an example, it introduces random noise to the input and perform backpropagation for 20 times at each iteration. Similar for IR and TAIG. We compare them with their corresponding baselines that employ the same input augmentations and keep the same number of backpropagation operations at each iteration. That is, the corresponding baselines of VT and IR (_i.e._, VT-baseline and IR-baseline) perform multiple rounds of backpropagation with UN and DP augmentations, respectively, at each

Figure 2: Comparing different combinations of the optimization strategies. The red solid circles indicate AAA, while the grey triangles show BAA and WAA (lower indicates more powerful attack).

iteration of optimization, and the corresponding baseline of TAIG (_i.e._, TAIG-baseline) perform both input scaling and UN augmentations. Figure 3 reports the comparison results. It can be seen that the simple baselines achieve even lower AAA, **indicating that the most effective factor of these methods may be input augmentation and gradient averaging, instead of what were claimed.** Based on these findings, we strongly recommend that newly designed methods, which incorporate input augmentations, either explicitly or implicitly, should be carefully compared with reasonable baselines that adopt the same mechanisms to show their effectiveness.

There are still 10+ methods to be evaluated and compared. Previous work often compares using a simple optimization back-end, _e.g._, I-FGSM or MI-FGSM. As we have obtained a combination of optimization strategies (_i.e._, UN-DP-DI\({}^{2}\)-TI-PI-FGSM) which has been proven to be more powerful in Section 4.2, we further introduce it as a more advanced optimization back-end. It aids in better exploring true advantages of compared methods. To ensure optimal performance across different substitute models, we employed a validation set consisting of 500 samples that were distinct from the test examples tune hyper-parameters of compared methods. The hyper-parameters that yielded the best results on the validation set were then adopted for testing. The hyper-parameters with each substitute model are tuned using models whose name have been mentioned in Section 3.3. The attack performance is evaluated not only on these "validation" models, but also on 15 more victim models which are distinct from them. The detailed hyper-parameters are reported in Section F.

In Table 1 (column 2-11), we report the AA achieved by utilizing each model as the substitute model to attack the 9 "validation" models under the \(\ell_{\infty}\) constraint. AAA (which is the average AA over all substitute models) is provided in the 12-th column in the table. Note that some methods are unable to be performed on some architectures. For instance, ConBP [72] suggests replacing ReLU with a softplus function in the backward pass to ensure smooth gradient backpropagation, making it not suitable to substitute models that are equipped with smooth activations only. SGM [65] is not applicable to substitute architectures without skip-connections (_e.g._, VGG-19). PNA [62] and SE [38] focus on vision transformers, and thus they are not suitable to most convolutional substitute models. For "substitute model training" methods, ResNet-50 is commonly chosen as the substitute model, as is adopted in the official GitHub implementations of these methods, and we leave the exploration of training with more advanced substitute architectures to future work. In addition to the \(\ell_{\infty}\) experiment, we also conduct evaluate with the \(\ell_{2}\) constraint and the results are provided in Section A.

When I-FGSM [24] is simply applied as the optimization back-end, NAA [74] beats the other gradient computation innovations and achieves the lowest AAA with a value of \(72.65\%\) (see the upper half of Table 1), while RFA demonstrates the best results among all "substitute model training" methods. Table 1 shows that the most suitable gradient computation strategy can be different for issuing attacks from different substitute models. In particular, if ResNet-50 or VGG-19 is chosen as the substitute model, FIA [61] seems even superior to NAA in the sense of achiving higher attack success rate and lower victim accuracy, however, NAA is the best if any other model is chosen as the substitute model.

When UN-DP-DI\({}^{2}\)-TI-PI-FGSM is introduced as the new optimization back-end, almost all "gradient computation" methods show improved performance (see the lower half of Table 1). Yet, in combination with the new baseline, the advantage of most methods becomes less obvious. SGM [65], PNA [62], and SE [38] still produce performance gains when being combined with UN-DP-DI\({}^{2}\)-TI-PI-FGSM. In particular, with the new optimization back-end, SE on the DeiT-B substitute model leads to the optimal attack performance among all concerned options, in the sense of BAA, fooling the victim models to show an accuracy of only \(18.61\%\) (on average). PNA obtains the lowest WAA among all, which is \(31.50\%\). As for substitute model training, we see that the MoreBayesian method significantly outperforms the other methods, and it fools the victim models to show an average accuracy of \(27.09\%\) using a ResNet-50 substitute model.

Figure 3: Comparing IR, VT, and TAIG with their corresponding baselines. The dotted lines indicate AAA (lower indicates more powerful attack).

According to Table 1, vision transformers should be preferable when choosing the substitute model, as the best attack performance (_i.e._, BAA) is often obtained on vision transformers for many attacks. To compare the transfer performance from vision transformers to convolutional networks and from the opposite direction, we report the accuracy of victim models in predicting SGM adversarial examples generated on ResNet-50 and ViT-B as the substitute model, respectively. The results are shown in Table 2. It can be seen that transferring from vision transformers to convolutional networks seems easier. When utilizing ViT-B as the substitute model, the accuracy of convolutional networks shows a range in \([28.32\%,37.24\%]\), while, with ResNet-50, the accuracy of vision transformers lies in \([36.82\%,48.32\%]\). Overall, using ViT-B as the substitute model leads to lower average accuracy (\(28.80\%\) vs \(31.97\%\)) and the worst accuracy (\(37.24\%\) vs \(48.32\%\)) on victim models, which means better average and worst-case attack performance, respectively. ConvNeXt [32] that follows some designing principles of the vision transformers is also a favorable choice of the substitute model, according to our results. When performing LinBP on ConvNeXt-B, the generated adversarial examples are capable of fooling the victim models to show an average accuracy of only \(18.81\%\), which is super close to the best attack performance that could be achieved in Table 1. Additionally, it

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline  & ResNet & VGG & Inception & EIHNetV2 & ConvNeXt & ViT & DeiT & BEiT & Swin & Mixer \\  & -50 & -19 & v3 & -M & -B & -B & -B & -B & -B & -B \\ \hline \multicolumn{12}{c}{**I-FGSM Back-end**} \\ \hline
1-FGSM & 87.79\% & 91.21\% & 93.71\% & 95.46\% & 88.32\% & 90.28\% & 90.28\% & 89.56\% & 94.81\% & 94.37\% & 91.58\% \\ \hline \multicolumn{12}{c}{**Gradient Computation**} \\ \hline TAP (2018) [77] & 81.75\% & 89.80\% & 91.01\% & 93.84\% & 90.20\% & 91.90\% & 92.86\% & 92.11\% & 95.08\% & 93.93\% & 91.25\% \\ NRDM (2018) [37] & 82.19\% & 87.62\% & 85.29\% & 96.12\% & 94.36\% & 94.70\% & 95.02\% & 95.23\% & 95.01\% & 90.25\% & 91.58\% \\ FDA (2019) [11] & 85.11\% & 93.91\% & 89.91\% & 98.00\% & 96.27\% & 96.60\% & 95.52\% & 96.67\% & 97.66\% & 94.72\% \\ ILA (2019) [20] & 74.76\% & 72.18\% & 83.38\% & 90.20\% & 84.13\% & 77.91\% & 80.62\% & 78.29\% & 89.18\% & 85.30\% & 82.10\% \\ SGM (2020) [65] & 72.56\% & - & - & 79.64\% & 71.37\% & 85.72\% & 87.04\% & 83.67\% & **90.55\%** & 91.01\% & - \\ ILA++ (2020) [26] & 71.80\% & 73.60\% & 80.07\% & 88.01\% & 83.12\% & 74.50\% & 80.19\% & 77.02\% & 88.08\% & 82.08\% & 79.85\% \\ LinBP (2020) [14] & 75.84\% & 86.66\% & 92.87\% & 96.96\% & 89.05\% & 91.74\% & 91.26\% & 92.62\% & 95.65\% & 96.07\% & 90.87\% \\ ConBP (2021) [72] & 73.46\% & 85.49\% & 91.00\% & - & - & - & - & - & - & - & - \\ SE (2021) [38] & - & - & - & - & - & - & - & - & - & - & - \\ FIA (2021) [61] & 68.48\% & 71.86\% & 83.84\% & 89.66\% & 80.35\% & 76.06\% & 80.13\% & 82.42\% & 88.75\% & 79.13\% & 80.07\% \\ PNA (2022) [62] & - & - & - & - & - & 88.13\% & 87.14\% & 87.97\% & 93.62\% & - & - \\ NAA (2022) [74] & 70.34\% & 78.41\% & 76.37\% & 83.56\% & 63.93\% & 65.04\% & 69.02\% & 66.24\% & 79.26\% & 74.33\% & 72.65\% \\ \hline \multicolumn{12}{c}{**Substitute Model Training**} \\ \hline RFA (2021) [47] & **47.49\%** & - & - & - & - & - & - & - & - & - & - \\ LGV (2022) [13] & 74.84\% & - & - & - & - & - & - & - & - & - \\ DRA (2022) [78] & 48.55\% & - & - & - & - & - & - & - & - & - \\ MoreBayesian (2023) [27] & 63.40\% & - & - & - & - & - & - & - & - & - \\ \hline \multicolumn{12}{c}{**New Optimization Back-end**} \\ \hline
1-Baseline & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \hline
**-Gradient Computation** & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ TAP (2018) [77] & 63.34\% & 54.64\% & 68.02\% & 68.00\% & 27.26\% & 41.48\% & 46.78\% & 34.45\% & 56.02\% & 54.49\% & 51.54\% \\ NRDM (2018) [37] & 51.78\% & 63.14\% & 70.76\% & 61.81\% & 40.04\% & 52.12\% & 60.89\% & 48.98\% & 77.87\% & 57.84\% & 58.52\% \\ FDA (2019) [11] & 42.62\% & 52.83\% & 60.25\% & 89.48\% & 69.01\% & 94.83\% & 83.99\% & 78.26\% & 83.19\% & 94.97\% & 74.94\% \\ ILA (2019) [20] & 37.80\% & **45.66\%** & 54.99\% & 48.86\% & 30.72\% & **28.93\%** & **33.28\%** & **29.40\%** & 5

[MISSING_PAGE_FAIL:9]

Conclusion

In this paper, we have presented benchmark for transfer-based attacks, called TA-Bench. On TA-Bench, we have implemented 30+ advanced transfer-based attack methods, including those focus on input augmentation and optimizer innovation, those "gradient computation" methods, those "substitute model training" methods, and those applying generative modeling. With TA-Bench, we are capable of evaluating and comparing transfer-based attacks systematically, practically, and fairly. Given comprehensive experimental results on TA-Bench, we have provided new insights about the effectiveness of these attacks, including but not limited to useful combinations of input augmentations and optimizers, reasonable choices of substitute/victim models, _etc._ Hoping to offer a sagacious judge of the state of transfer attacks and help future innovations in this field.

## Acknowledgment

This material is based upon work supported by the National Science Foundation under Grant No. 1801751 and 1956364.

## References

* [1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In _International Conference on Learning Representations_, 2021.
* [2] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, 2021.
* [3] Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Edoardo Debenedetti, Nicolas Flammarion, Mung Chiang, Prateek Mittal, and Matthias Hein. Robustbench: a standardized adversarial robustness benchmark. _arXiv preprint arXiv:2010.09670_, 2020.
* [4] Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks. In _International conference on machine learning_, pages 2206-2216. PMLR, 2020.
* [5] Xiaohan Ding, Xiangyu Zhang, Ningning Ma, Jungong Han, Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style convnets great again. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 13733-13742, 2021.
* [6] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In _CVPR_, 2018.
* [7] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial examples by translation-invariant attacks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4312-4321, 2019.
* [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [9] Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual representation for neon genesis. _arXiv preprint arXiv:2303.11331_, 2023.
* [10] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 19358-19369, 2023.
* [11] Aditya Ganeshan, Vivek BS, and R Venkatesh Babu. Fda: Feature disruptive attack. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8069-8079, 2019.
* [12] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2918-2928, 2021.
* [13] Martin Gubri, Maxime Cordy, Mike Papadakis, Yves Le Traon, and Koushik Sen. Lgv: Boosting adversarial example transferability from large geometric vicinity. _arXiv preprint arXiv:2207.13129_, 2022.

* [14] Yiwen Guo, Qizhang Li, and Hao Chen. Backpropagating linearly improves transferability of adversarial examples. In _NeurIPS_, 2020.
* [15] Yiwen Guo, Qizhang Li, Wangmeng Zuo, and Hao Chen. An intermediate-level attack framework on the basis of linear regression. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2022.
* [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _CVPR_, 2016.
* [17] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In _CVPR_, 2018.
* [18] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In _CVPR_, 2017.
* [19] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pages 646-661. Springer, 2016.
* [20] Qian Huang, Isay Katsman, Horace He, Zeqi Gu, Serge Belongie, and Ser-Nam Lim. Enhancing adversarial example transferability with an intermediate level attack. In _ICCV_, 2019.
* [21] Yi Huang and Adams Wai-Kin Kong. Transferable adversarial attack based on integrated gradients. _arXiv preprint arXiv:2205.13152_, 2022.
* [22] Nathan Inkawahich, Wei Wen, Hai Helen Li, and Yiran Chen. Feature space perturbations yield more transferable adversarial examples. In _CVPR_, 2019.
* [23] Hoki Kim. Torchattacks: A pytorch repository for adversarial attacks. _arXiv preprint arXiv:2010.01950_, 2020.
* [24] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In _ICLR_, 2017.
* [25] Qizhang Li, Yiwen Guo, and Hao Chen. Practical no-box adversarial attacks against dnns. In _NeurIPS_, 2020.
* [26] Qizhang Li, Yiwen Guo, and Hao Chen. Yet another intermediate-leve attack. In _ECCV_, 2020.
* [27] Qizhang Li, Yiwen Guo, Wangmeng Zuo, and Hao Chen. Making substitute models more bayesian can enhance transferability of adversarial examples. In _International Conference on Learning Representations_, 2023.
* [28] Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John E Hopcroft. Nesterov accelerated gradient and scale invariance for adversarial attacks. _arXiv preprint arXiv:1908.06281_, 2019.
* [29] Chang Liu, Yinpeng Dong, Wenzhao Xiang, Xiao Yang, Hang Su, Jun Zhu, Yuefeng Chen, Yuan He, Hui Xue, and Shibao Zheng. A comprehensive study on robustness of image classification models: Benchmarking and rethinking. _arXiv preprint arXiv:2302.14301_, 2023.
* [30] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 12009-12019, 2022.
* [31] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 10012-10022, 2021.
* [32] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 11976-11986, 2022.
* [33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. _arXiv preprint arXiv:1711.05101_, 2017.
* [34] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In _ICLR_, 2018.
* [35] Muhammad Muzzammal Naseer, Salman H Khan, Muhammad Haris Khan, Fahad Shahbaz Khan, and Fatih Porikli. Cross-domain transferability of adversarial perturbations. _Advances in Neural Information Processing Systems_, 32, 2019.

* [36] Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Fatih Porikli. On generating transferable targeted perturbations. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7708-7717, 2021.
* [37] Muzammal Naseer, Salman H Khan, Shafin Rahman, and Fatih Porikli. Task-generalizable adversarial attack based on perceptual metric. _arXiv preprint arXiv:1811.09020_, 2018.
* [38] Muzammal Naseer, Kanchana Ranasinghe, Salman Khan, Fahad Shahbaz Khan, and Fatih Porikli. On improving adversarial transferability of vision transformers. _arXiv preprint arXiv:2106.04169_, 2021.
* [39] Nicolas Papernot, Fardash Faghri, Nicholas Carlini, Ian Goodfellow, Reuben Feinman, Alexey Kurakin, Cihang Xie, Yash Sharma, Tom Brown, Aurko Roy, Alexander Matyasko, Vahid Behzadan, Karen Hambardzumyan, Zhishuai Zhang, Yi-Lin Juang, Zhi Li, Ryan Sheatsley, Abhibahy Garg, Jonathan Uesato, Willi Gierke, Yinpeng Dong, David Berthelot, Paul Hendricks, Jonas Rauber, and Rujun Long. Technical report on the cleverhans v2.1.0 adversarial examples library. _arXiv preprint arXiv:1610.00768_, 2018.
* [40] Omid Poursaeed, Isay Katsman, Bicheng Gao, and Serge Belongie. Generative adversarial perturbations. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 4422-4431, 2018.
* [41] Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark the robustness of machine learning models. In _Reliable Machine Learning in the Wild Workshop, 34th International Conference on Machine Learning_, 2017.
* [42] Jonas Rauber, Roland Zimmermann, Matthias Bethge, and Wieland Brendel. Foolbox native: Fast adversarial attacks to benchmark the robustness of machine learning models in pytorch, tensorflow, and jax. _Journal of Open Source Software_, 5(53):2607, 2020.
* [43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. _IJCV_, 2015.
* [44] Mathieu Salzmann et al. Learning transferable adversarial perturbations. _Advances in Neural Information Processing Systems_, 34:13950-13962, 2021.
* [45] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In _CVPR_, 2018.
* [46] Naman D Singh, Francesco Croce, and Matthias Hein. Revisiting adversarial training for imagenet: Architectures, training and generalization across threat models. _arXiv preprint arXiv:2303.01870_, 2023.
* [47] Jacob Springer, Melanie Mitchell, and Garrett Kenyon. A little robustness goes a long way: Leveraging robust features for targeted transfer attacks. _Advances in Neural Information Processing Systems_, 34, 2021.
* [48] Chenghao Sun, Yonggang Zhang, Wan Chaoqun, Qizhou Wang, Ya Li, Tongliang Liu, Bo Han, and Xinmei Tian. Towards lightweight black-box attacks against deep neural networks. _arXiv preprint arXiv:2209.14826_, 2022.
* [49] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _CVPR_, 2016.
* [50] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In _International conference on machine learning_, pages 6105-6114. PMLR, 2019.
* [51] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models and faster training. In _International conference on machine learning_, pages 10096-10106. PMLR, 2021.
* [52] Shiyu Tang, Ruihao Gong, Yan Wang, Aishan Liu, Jiakai Wang, Xinyun Chen, Fengwei Yu, Xianglong Liu, Dawn Song, Alan Yuille, Philip H.S. Torr, and Dacheng Tao. Robustart: Benchmarking robustness on architecture design and training techniques. _https://arxiv.org/pdf/2109.05211.pdf_, 2021.
* [53] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. _Advances in Neural Information Processing Systems_, 34:2461-24272, 2021.
* [54] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _International Conference on Machine Learning_, volume 139, pages 10347-10357, July 2021.

* [55] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In _International Conference on Machine Learning_, pages 10347-10357. PMLR, 2021.
* [56] Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, and Yinxiao Li. Maxvit: Multi-axis vision transformer. In _European conference on computer vision_, pages 459-479. Springer, 2022.
* [57] Xiaosen Wang and Kun He. Enhancing the transferability of adversarial attacks through variance tuning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1924-1933, 2021.
* [58] Xiaosen Wang, Xuanran He, Jingdong Wang, and Kun He. Admix: Enhancing the transferability of adversarial attacks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 16158-16167, 2021.
* [59] Xiaosen Wang, Jiadong Lin, Han Hu, Jingdong Wang, and Kun He. Boosting adversarial transferability through enhanced momentum. _arXiv preprint arXiv:2103.10609_, 2021.
* [60] Xin Wang, Jie Ren, Shuyun Lin, Xiangming Zhu, Yisen Wang, and Quanshi Zhang. A unified approach to interpreting and boosting adversarial transferability. In _International Conference on Learning Representations_, 2020.
* [61] Zhibo Wang, Hengchang Guo, Zhifei Zhang, Wenxin Liu, Zhan Qin, and Kui Ren. Feature importance-aware transferable adversarial attacks. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 7639-7648, 2021.
* [62] Zhipeng Wei, Jingjing Chen, Micah Goldblum, Zuxuan Wu, Tom Goldstein, and Yu-Gang Jiang. Towards transferable adversarial attacks on vision transformers. In _Proceedings of the AAAI Conference on Artificial Intelligence_, 2022.
* [63] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models, 2019.
* [64] Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, Xinlei Chen, Zhuang Liu, In So Kweon, and Saining Xie. Convnext v2: Co-designing and scaling convnets with masked autoencoders. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16133-16142, 2023.
* [65] Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma. Rethinking the security of skip connections in resnet-like neural networks. In _ICLR_, 2020.
* [66] Lei Wu, Zhanxing Zhu, Cheng Tai, et al. Understanding and enhancing the transferability of adversarial examples. _arXiv preprint arXiv:1802.09707_, 2018.
* [67] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille. Improving transferability of adversarial examples with input diversity. In _CVPR_, 2019.
* [68] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In _CVPR_, 2017.
* [69] Xiao Yang, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Boosting transferability of targeted adversarial examples via hierarchical generative networks. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part IV_, pages 725-742. Springer, 2022.
* [70] Weihao Yu, Chenyang Si, Pan Zhou, Mi Luo, Yichen Zhou, Jiashi Feng, Shuicheng Yan, and Xinchao Wang. Metaformer baselines for vision. _arXiv preprint arXiv:2210.13452_, 2022.
* [71] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6023-6032, 2019.
* [72] Chaoning Zhang, Philipp Benz, Gyusang Cho, Adil Karjauv, Soomin Ham, Chan-Hyun Youn, and In So Kweon. Backpropagating smoothly improves transferability of adversarial examples. In _CVPR 2021 Workshop Workshop on Adversarial Machine Learning in Real-World Computer Vision Systems and Online Challenges (AML-CV)_, volume 2, 2021.
* [73] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.

* [74] Jianping Zhang, Weibin Wu, Jen-tse Huang, Yizhan Huang, Wenxuan Wang, Yuxin Su, and Michael R Lyu. Improving adversarial transferability via neuron attribution-based attacks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14993-15002, 2022.
* [75] Qilong Zhang, Xiaodan Li, YueFeng Chen, Jingkuan Song, Lianli Gao, Yuan He, et al. Beyond imagenet attack: Towards crafting adversarial examples for black-box domains. In _International Conference on Learning Representations_, 2022.
* [76] Zhengyu Zhao, Hanwei Zhang, Renjue Li, Ronan Sicre, Laurent Amsaleg, and Michael Backes. Towards good practices in evaluating transfer adversarial attacks. _arXiv preprint arXiv:2211.09565_, 2022.
* [77] Wen Zhou, Xin Hou, Yongjun Chen, Mengyun Tang, Xiangqi Huang, Xiang Gan, and Yong Yang. Transferable adversarial perturbations. In _ECCV_, 2018.
* [78] Yao Zhu, Yuefeng Chen, Xiaodan Li, Kejiang Chen, Yuan He, Xiang Tian, Bolun Zheng, Yaowu Chen, and Qingming Huang. Toward understanding and boosting adversarial transferability from a distribution perspective. _IEEE Transactions on Image Processing_, 31:6487-6501, 2022.

## Appendix A \(\ell_{2}\) Results

We evaluate the "gradient computation" methods and "substitute model training" methods under \(\ell_{2}\) constraint and provide the results in Table 4. Some \(\ell_{2}\) results are provided in this section. When I-FGSM is applied as the optimization back-end, same as the \(\ell_{\infty}\) results in Table 1 in our main paper, NAA achieves the lowest AAA (_i.e._, 82.00%) compared with the other "gradient computation" methods, while FIA beats it when ResNet-50 or VGG-19 is chosen as the substitute model. However, unlike in the \(\ell_{\infty}\) setting, SE shows consistently inferior performance when compared with the I-FGSM baseline in the \(\ell_{2}\) setting, and DRA instead of RFA achieves the best performance among "substitute model training" methods.

When UN-DP-DI\({}^{2}\)-TI-PI-FGSM is applied as the new optimization back-end, same as in the \(\ell_{\infty}\) setting, SGM, PNA, and SE provide favorable attack performance, while PNA on the DeiT-B substitute model turns out to be the best (in the sense of achieving lower BAA) and the generated adversarial examples fools victim models to show an accuracy of only \(29.33\%\). The lowest WAA (which is \(51.56\%\)) is obtained by PNA. For the "substitute model training" methods, the MoreBayesian method still outperforms the other methods by a large margin.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline  & ResNet & VGG & Inception & EffNetV2 & ConvNeXt & ViT & DeiT & BEiT & Swin & Mixer & AAA \\  & -50 & -19 & v3 & - & -B & -B & -B & -B & -B & -B & -B \\ \hline \multicolumn{12}{c}{**I-FGSM Back-end**} \\ \hline
**Baseline** & & & & & & & & & & & & \\
1-FGSM & 88.21\% & 92.46\% & 94.91\% & 97.49\% & 89.34\% & 91.21\% & 90.81\% & 90.46\% & 95.76\% & 95.47\% & 92.61\% \\ \hline
**Gradient Computation** & & & & & & & & & & \\ TAP (2018) [77] & 89.06\% & 94.55\% & 95.42\% & 98.39\% & 94.51\% & 95.26\% & 95.68\% & 94.90\% & 97.19\% & 96.77\% & 95.17\% \\ NRDM (2018) [37] & 91.41\% & 92.36\% & 96.00\% & 98.94\% & 95.28\% & 97.00\% & 97.14\% & 97.63\% & 97.26\% & 95.43\% & 95.85\% \\ FDA (2019) [11] & 92.64\% & 96.62\% & 96.10\% & 99.16\% & 96.95\% & 97.72\% & 96.66\% & 97.00\% & 98.15\% & 98.54\% & 96.95\% \\ ILA (2019) [20] & 83.62\% & **84.54\%** & 92.61\% & 96.41\% & 90.97\% & 89.22\% & 88.45\% & 88.50\% & 94.31\% & 93.80\% & **90.24\%** \\ SOM (2020) [65] & 79.14\% & **89.04\%** & **82.09\%** & **98.48\%** & 90.00\% & 90.00\% & 94.21\% & 95.26\% & - \\ ILA++ (2020) [26] & 81.01\% & 81.77\% & 91.44\% & 95.83\% & 90.42\% & 87.86\% & 88.90\% & 87.40\% & 93.78\% & 92.41\% & 89.08\% \\ LinBP (2020) [14] & 84.02\% & **90.56\%** & 97.53\% & 98.81\% & 91.52\% & 92.99\% & 92.60\% & 93.61\% & 96.43\% & 98.18\% & 93.63\% \\ ConBP (2021) [72] & 82.17\% & 89.70\% & 96.71\% & - & - & - & - & - & - & - \\ SE (2021) [38] & - & - & - & - & - & - & - & - & - & - \\ FIA (2021) [61] & **74.04\%** & **57.87\%** & 90.49\% & 95.44\% & 84.89\% & 82.60\% & 85.25\% & 86.39\% & 92.47\% & 85.90\% & 85.33\% \\ PNA (2022) [62] & - & - & - & - & - & 90.56\% & 89.32\% & 90.18\% & 95.17\% & - & - \\ NAA (2022) [74] & 79.03\% & 85.49\% & **88.38\%** & 94.84\% & **72.61\%** & **75.96\%** & **77.56\%** & **75.04\%** & **86.56\%** & **84.52\%** & **82.00\%** \\ \hline \multicolumn{12}{l}{**Substitute Model Training**} \\ \hline RFA (2021) [47] & 67.24\% & - & - & - & - & - & - & - & - & - & - \\ LGV (2022) [13] & 74.86\% & - & - & - & - & - & - & - & - & - \\ DRA (2022) [78] & **64.29\%** & - & - & - & - & - & - & - & - & - \\ MoreBayesian (2023) [27] & 70.24\% & - & - & - & - & - & - & - & - & - \\ \hline \multicolumn{12}{c}{**New Optimization Back-end**} \\ \hline
**Baseline** & & & & & & & & & & & \\ UN-DP-DI\({}^{2}\)-TI-PI-FGSM & 43.09\% & **55.86\%** & **72.13\%** & **75.73\%** & 45.74\% & **43.36\%** & **51.06\%** & **43.58\%** & **63.74\%** & **60.27\%** & **55.46\%** \\ \hline
**Gradient Computation** & & & & & & & & & & \\
**TAP (2018) [77]** & 77.26\% & 65.26\% & 82.02\% & 91.82\% & 52.38\% & 70.49\% & 78.21\% & 53.32\% & 83.40\% & 72.93\% & 72.71\% \\ NRDM (2018) [37] & 71.28\% & 78.55\% & 86.54\% & 81.93\% & 65.57\% & 81.32\% & 85.54\% & 67.78\% & 93.14\% & 79.44\% & 79.11\% \\ FDA (2019) [11] & 58.39\% & 65.94\% & 78.50\% & 96.43\% & 79.18\% & 98.23\% & 95.88\% & 83.26\% & 95.79\% & 96.87\% & 84.85\% \\ ILA (2019) [20] & 47.80\% & 57.54\% & 73.57\% & **74.58\%** & 48.88\% & 49.87\% & 40.93\% & **40.11\%** & **75.32\%** & 56.62\% & 59.67\% \\ SGM (2020) [65] & **38.56\%** & - & **57.17\%** & **32.25\%** & **38.47\%** & **36.07\%** & **33.57\%** & **32.94\%** & **54.31\%** & - \\ ILA++ (2020) [26] & 47.58\% & 56.46\% & 72.96\% & **74.80\%** & 48.95\% & 48.26\% & 65.76\% & **40.88\%** & 85.21\% & 65.55\% & 60.64\% \\ LinBP (2020) [14] & 48.62\% & 56.32\% & 89.79\% & 97.82\% & **30.87\%** & 54.97\% & **50.57\%** & 55.38\% & 82.27\% & 88.77\% & 65.54\% \\ ConBP (2021) [72] & 46.34\% & 56.39\% & 83.41\% & - & - &

[MISSING_PAGE_EMPTY:16]

We collected 15 additional victim models, including 7 CNNs (EfficientNet-L2 [50], ConvNeXt V2-L [64], MobileNet V2 [45], DenseNet-161 [18], ResNeXt-101 [68], SENet-154 [17], and RepVGG-B3 [5]) and 8 vision transformers (ViT-L [8], DeiT-L [54], Swin V2-L [30], BEiT-L [1], CAFormer-B36 [70], MaxViT-L [56], EVA-L [10], EVA02-L [9]), and conducted an experiment on attacking these victim models. For the "substitute model training" methods, the conclusion remains consistent with the observations from Table 1. Specifically, when employing I-FGSM as the back-end, RFA achieves the best AA (_i.e._, 63.93%), and when applying UN-DP-DI\({}^{2}\)-TI-PI-FGSM as the back-end, MoreBayesian attains the best AA (_i.e._, 47.13%). For the "gradient computation" methods, when I-FGSM is applied as the optimization back-end, the conclusion aligns with the findings in Table 1 of the paper. NAA consistently outperforms other methods on most choices of the substitute model, achieving the lowest AAA (_i.e._, 79.27%). When introducing UN-DP-DI\({}^{2}\)-TI-PI-FGSM as the optimization back-end, the top four lowest AAs are achieved using ConvNeXt-B, DeiT-B, BEiT-B, and Swin-B as the substitute models, as in Table 1. The best AA is obtained by performing LinBP on the ConvNeXt-B substitute model (_i.e._, 30.81%, which stands as the second-best in Table 1 and is only 0.20% higher than the best AA), due to slight distribution shift of the tested victim models.

## Appendix C Results of Attacking Robust Models

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline \hline  & ResNet & VGG & Inception & EInNetV2 ConvNeXt & ViT & DeiT & BEiT & Swin & Mixer \\  & -50 & -19 & v3 & -M & -B & -B & -B & -B & -B & -B \\ \hline \multicolumn{10}{c}{**I-FGSM Back-end**} \\ \hline \hline
**Baseline** & 95.57\% & 95.68\% & 96.94\% & 97.13\% & 96.24\% & 96.09\% & 96.18\% & 96.21\% & 96.33\% & 96.16\% & 96.25\% \\ \hline
**Gradient Computation** & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \\ \multicolumn{10}{l}{**TAP** (2018) [77]} & 95.18\% & 95.48\% & 96.87\% & 97.03\% & 96.17\% & 96.09\% & 96.14\% & 96.19\% & 96.28\% & 96.13\% & 96.16\% \\ \multicolumn{10}{l}{**NRDM** (2018) [37]} & 95.39\% & 95.55\% & 96.79\% & 97.09\% & 96.29\% & 96.15\% & 96.19\% & 96.25\% & 96.05\% & 95.83\% & 96.16\% \\ \multicolumn{10}{l}{**FDA** (2019) [11]} & 94.97\% & 95.51\% & 96.64\% & 97.17\% & 96.04\% & 96.31\% & 96.31\% & 96.04\% & 95.05\% & 96.28\% \\ \multicolumn{10}{l}{**ILA** (2019) [20]} & 95.39\% & 95.55\% & 96.68\% & 97.04\% & 96.08\% & 95.75\% & 95.87\% & 95.91\% & 96.00\% & 95.79\% & 96.01\% \\ \multicolumn{10}{l}{**SGM** (2020) [65]} & 95.35\% & - & - & 96.51\% & 95.27\% & 95.66\% & 95.67\% & 95.55\% & 95.90\% & 95.95\% & - \\ \multicolumn{10}{l}{**ILA+** (2020) [26]} & 95.36\% & 95.15\% & 96.69\% & 97.03\% & 95.97\% & 95.77\% & 95.86\% & 95.89\% & 96.01\% & 95.70\% & 95.97\% \\ \multicolumn{10}{l}{**LingBP** (2014) [49]} & 95.33\% & 95.56\% & 97.00\% & 97.10\% & 96.21\% & 96.15\% & 96.17\% & 96.17\% & 96.33\% & 95.99\% & 96.19\% \\ \multicolumn{10}{l}{**ConBP** (2012) [72]} & 95.41\% & 95.50\% & 97.06\% & - & - & - & - & - & - & - \\ \multicolumn{10}{l}{**SE** (2021) [38]} & - & - & - & - & 95.96\% & 96.07\% & 96.02\% & - & 95.97\% & - \\ \multicolumn{10}{l}{**FIA** (2021) [61]} & 94.81\% & **95.21\%** & 96.47\% & 96.96\% & 95.89\% & 95.49\% & 95.51\% & 95.83\% & 96.07\% & 95.34\% & 95.76\% \\ \multicolumn{10}{l}{**PNA** (2022) [62]} & - & - & - & 95.86\% & 95.94\% & 96.05\% & 96.27\% & - & - \\ \multicolumn{10}{l}{**NAA** (2022) [74]} & **94.78\%** & 95.31\% & **96.01\%** & **96.18\%** & **93.83\%** & **94.05\%** & **93.78\%** & **94.44\%** & **95.01\%** & **94.65\%** & **94.80\%** \\ \hline \multicolumn{10}{l}{**Substitute Model Training**} \\ \hline
**RFA** (2021) [47]} & 91.83\% & - & - & - & - & - & - & - & - & - & - \\ \multicolumn{10}{l}{**LGV** (2022) [13]} & 95.31\% & - & - & - & - & - & - & - & - \\ \multicolumn{10}{l}{**DRA** (2022) [78]} & **91.35\%** & - & - & - & - & - & - & - & - & - \\ \multicolumn{10}{l}{**MoreBayesian** (2023) [27]} & 95.21\% & - & - & - & - & - & - & - & - & - \\ \hline \multicolumn{10}{c}{**New Optimization Back-end**} \\ \hline
**Baseline** & & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \multicolumn{10}{l}{**UN-DP-DI\({}^{2}\)-TI-PI-FGSM** & 94.17\% & 95.01\% & 96.16\% & 96.24\% & 94.79\% & 93.97\% & 93.55\% & 93.94\% & 94.80\% & 94.61\% & 94.72\% \\ \hline
**Gradient Computation** & \multicolumn{1}{c}{} &We also evaluated the performance of different methods in attacking 3 defensive models obtained via adversarial training, _i.e._, a robust ConvNext-B [29], a robust Swin-B [29], and a robust ViT-B-CvSt [46]. They are all collected from RobustBench [3] and exhibit high robust accuracy against AutoAttack [4]. The results are given in Table 6. It can be seen that when I-FGSM is used as the optimization back-end, NAA and DRA achieve the best AAs among the methods of "gradient computation" and "substitute model training" categories, respectively. When UN-DP-DI\({}^{2}\)-TI-PI-FGSM is employed as the optimization back-end, SGM achieves the best AAs among the "gradient computation" methods for the most of substitute models, and the best AA is achieved by using DeiT-B as the substitute model, _i.e._, 92.05%. For the "substitute model training" methods, RFA instead of MoreBayesian achieves the best AA, _i.e._, 86.37%, since it shares the same training scheme (_i.e._, adversarial training) with the victim models.

## Appendix D Results of different \(\epsilon\)

We conducted the evaluation of "gradient computation" methods and "substitute model training" methods under an \(\ell_{\infty}\) constraint with \(\epsilon=16/255\) since it is a common setting that many previous work [6, 28, 58, 74] adopted. The results are shown in Table 7. The conclusion is consistent with

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline  & ResNet & VGG & Inception & EffNetV2 ConvNeXt & ViT & DefT & BE/T & Swin & Mixer & \multirow{2}{*}{AAA} \\ \cline{2-2} \cline{6-13}  & -50 & -19 & v3 & -M & -B & -B & -B & -B & -B & -B & -B \\ \hline \multicolumn{13}{c}{**I-FGSM Back-end**} \\ \hline
**Baseline** & & & & & & & & & & & & \\ \multicolumn{13}{c}{I-FGSM} \\ \hline
1-FGSM & 76.99\% & 80.93\% & 88.83\% & 90.96\% & 77.34\% & 82.21\% & 80.46\% & 80.56\% & 89.18\% & 89.61\% & 83.71\% \\ \hline \multicolumn{13}{c}{**Gradient Computation**} \\ \hline TAP (2018) [77] & 63.83\% & 74.26\% & 81.18\% & 86.52\% & 75.34\% & 82.68\% & 84.08\% & 84.21\% & 87.35\% & 85.15\% & 80.46\% \\ NRDM (2018) [37] & 61.20\% & 71.48\% & 66.88\% & 91.78\% & 88.04\% & 88.84\% & 88.28\% & 87.02\% & 84.87\% & 81.27\% & 80.97\% \\ FDA (2019) [11] & 66.97\% & 82.65\% & 77.02\% & 96.05\% & 93.12\% & 93.89\% & 90.82\% & 94.39\% & 95.25\% & 94.04\% & 88.46\% \\ ILA (2019) [20] & 53.52\% & 54.72\% & 66.48\% & 81.59\% & 71.90\% & 56.23\% & 62.19\% & 58.08\% & 75.39\% & 83.02\% & 64.31\% \\ SGM (2020) [65] & 49.48\% & - & - & **62.81\%** & 49.83\% & 72.50\% & 74.74\% & 67.57\% & 78.23\% & 81.32\% & - \\ ILA+ (2020) [26] & 50.58\% & 51.66\% & 64.39\% & 80.31\% & 72.25\% & 55.19\% & 64.59\% & 58.46\% & 75.25\% & 60.12\% & 63.32\% \\ LinBP (2020) [14] & 51.00\% & 69.69\% & 81.37\% & 9.56\% & 72.68\% & 84.32\% & 82.03\% & 84.74\% & 90.36\% & 95.58\% & 80.39\% \\ ConfigD (2019) [72] & 46.70\% & 66.86\% & 74.49\% & - & - & - & - & - & - & - & - \\ SE (2021) [38] & - & - & - & - & - & 80.80\% & 77.58\% & 78.43\% & - & 83.42\% & - \\ FIA (2021) [61] & 44.43\% & 48.30\% & 68.02\% & 81.63\% & 66.31\% & **58.58\%** & 65.40\% & 69.84\% & 79.10\% & 59.90\% & 64.15\% \\ PNA (2022) [62] & & & & & & & & & & & & \\ NAA (2022) [74] & **30.41\%** & **44.44\%** & **47.31\%** & 65.88\% & **40.72\%** & **34.70\%** & **48.45\%** & **42.29\%** & **57.48\%** & **35.02\%** & **44.67\%** \\ \hline \multicolumn{13}{c}{**Substitute Model Training**} \\ \hline
**RFA (2021)** [47] & 15.18\% & - & - & - & - & - & - & - & - & - & - \\ LGV (2022) [13] & 52.81\% & - & - & - & - & - & - & - & - & - \\ DRA (2022) [78] & **13.43\%** & - & - & - & - & - & - & - & - & - \\ MoreBayesian (2023) [27] & 30.89\% & - & - & - & - & - & - & - & - & - \\ \hline \multicolumn{13}{c}{**New Optimization Back-end**} \\ \hline
**Baseline** & & & & & & & & & & & \\ UN-DP-DI\({}^{2}\)-TI-PI-FGSM & 15.30\% & 25.90\% & 34.86\% & 34.78\% & 18.87\% & 13.16\% & 20.68\% & 14.12\% & 27.66\% & 21.99\% & **22.73\%** \\ \hline \multicolumn{13}{c}{**Gradient Computation**} \\ \hline TAP (2018) [77] & 40.72\% & 31.08\% & 44.02\% & 45.43\% & 8.07\% & 12.47\% & 17.29\% & 9.02\% & 28.59\% & 21.79\% & 25.85\% \\ NRDM (2018) [37] & 22.14\% & 32.81\% & 43.20\% & 41.30\% & 20.88\% & 84.20\% & 33.59\% & 16.74\% & 61.80\% & 18.81\% & 31.55\% \\ FDA (2019) [11] & **14.84\%** & 23.93\% & **31.10\%** & 79.50\% & 49.80\% & 90.39\% & 64.71\% & 60.18\% & 66.88\% & 90.22\% & 57.16\% \\ ILA (2019) [20] & 15.40\% & 19.33\% & **26.76\%** & 25.69\% & 13.09\% & **8.49\%** & 11.18\% & 91.9\% & 33.32\% & 14.46\% & **71.69\%** \\ SGM (2020) [65] & **12.57\%** & - & - & **9.36\%** & 6.88\% & 10.14\% & 8.79\% & 7.50\% & **7.91\%** & 13.67\% & - \\ LLA+ (2020) [26] & **14.82\%** & **18.68\%** & **26.64\%** & **13.81\%** & 9.11\% & 12.27\% & **9.53\%** & 38.84\% & **14.91\% & **18.54\%** \\ LinkB (2020) [14] & 15.39\% & **24.86\%** & 61.13\% & 76.05\% & **5.35\%** & 17.55\% & 20.55\% & 23.06\% & 50.74\% & 40.17\% & 35.89\% \\ ConBP (2021) [72] & 14.14\% & **25.15\%** & 46.43\% & - &

[MISSING_PAGE_FAIL:19]

Implementation Details

**Augmentations and Optimizer.** For PGD, DI\({}^{2}\)-FGSM, MI-FGSM, NI-FGSM, and PI-FGSM, we use the default hyper-parameters. For TI-FGSM, we randomly translate the input with a range of [-3, +3] since its performance is better than the approximation using a \(7\times 7\) Gaussian kernel in many implementations [28, 58, 59, 27]. For SI-FGSM and Admix, both of them average the gradients obtained by feeding different augmented inputs into the substitute model, which may lead to unfair comparisons. Therefore, we randomly select one input from the augmented copies, and the hyper-parameters remain the same as in their original papers. For UN, the noise added to the input follows \(\mathcal{U}(-\epsilon,\epsilon)\) and \(\mathcal{U}(-\frac{\epsilon}{\sqrt{HW}},\frac{\epsilon}{\sqrt{HW}})\) (the dimension of inputs is \(3\times H\times W\)) for attacks under \(\ell_{\infty}\) and \(\ell_{2}\) constraints, respectively. For DP, we divide the perturbation into \(16\times 16\) patches and randomly drop 50% of the patches at each iteration.

**Gradient Computation.** For TAIG, VT, IR, TAP, FDA, SE, and PNA, we set the same hyper-parameters as in their original papers. For NRDM, ILA, ILA++, LinBP, ConBP, FIA, and NAA, the main hyper-parameter which significantly impacts the performance is the choice of the middle layer. The scaling factor of SGM is also related to the selection of the substitute model. We tune these hyper-parameters by evaluating on a validation set consisting of 500 samples that do not overlap with the samples in the test set, and the best choices are shown in Table 10.

**Substitute Model Training.** In this category of methods, ResNet-50 is commonly chosen as the substitute model, and we collect the models from the GitHub repositories of these methods. For LGV and the MoreBayesian method, we only sample once at each iteration.

**Generative Modeling.** In this category of methods, all the generators are collected from the GitHub repositories of these methods.

\begin{table}
\begin{tabular}{l c c c c c c c c c c} \hline  & ResNet-50 & VGG-19 & Inception v3 & EffNetV2-M & ConvNeXt-B & ViT-B & DeiT-B & BEiT-B & Swin-B & Mixer-B \\ \hline \multicolumn{11}{c}{**I-FGSM Back-end**} \\ \hline NRDM & 2 & 5 & 5 & 3 & 4 & 8 & 6 & 2 & 2 & 4 \\ ILA & 2 & 5 & 5 & 3 & 1 & 4 & 6 & 4 & 2 & 2 \\ ILA++ & 2 & 5 & 5 & 3 & 1 & 4 & 6 & 4 & 2 & 2 \\ LinBP & 3 & 8 & 3 & 6 & 4 & 11 & 11 & 11 & 4 & 4 \\ ConBP & 3 & 8 & 3 & - & - & - & - & - & - & - \\ FIA & 2 & 5 & 5 & 3 & 1 & 2 & 4 & 4 & 2 & 2 \\ NAA & 2 & 5 & 5 & 3 & 1 & 4 & 4 & 4 & 2 & 2 \\ \hline \multicolumn{11}{c}{**New Optimization Back-end**} \\ \hline NRDM & 3 & 6 & 6 & 7 & 4 & 12 & 12 & 10 & 4 & 10 \\ ILA & 4 & 7 & 6 & 7 & 4 & 12 & 10 & 12 & 2 & 10 \\ ILA++ & 4 & 7 & 6 & 7 & 4 & 12 & 10 & 12 & 4 & 10 \\ LinBP & 4 & 9 & 7 & 6 & 4 & 11 & 11 & 11 & 4 & 11 \\ ConBP & 4 & 9 & 7 & - & - & - & - & - & - & - \\ FIA & 4 & 8 & 7 & 6 & 4 & 12 & 12 & 12 & 4 & 12 \\ NAA & 4 & 8 & 6 & 6 & 3 & 10 & 12 & 10 & 4 & 8 \\ \hline \end{tabular}
\end{table}
Table 10: The index of the middle layer we chose for each substitute model for each method. For VGG-19, max-pooling layers were considered as the end of blocks.