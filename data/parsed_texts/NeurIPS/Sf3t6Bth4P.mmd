# Enhancing Sharpness-Aware Optimization Through Variance Suppression

 Bingcong Li

Georgios B. Giannakis

University of Minnesota - Twin Cities

Minneapolis, MN, USA

{lixx5599, georgios}@umn.edu

###### Abstract

Sharpness-aware minimization (SAM) has well documented merits in enhancing generalization of deep neural networks, even without sizable data augmentation. Embracing the geometry of the loss function, where neighborhoods of 'flat minima' heighten generalization ability, SAM seeks 'flat valleys' by minimizing the maximum loss caused by an _adversary_ perturbing parameters within the neighborhood. Although critical to account for sharpness of the loss function, such an '_over-friendly_ adversary' can curtail the outmost level of generalization. The novel approach of this contribution fosters stabilization of adversaries through _variance suppression_ (VaSSO) to avoid such friendliness. VaSSO's _provable_ stability safeguards its numerical improvement over SAM in model-agnostic tasks, including image classification and machine translation. In addition, experiments confirm that VaSSO endows SAM with robustness against high levels of label noise. Code is available at https://github.com/BingcongLi/VaSSO.

## 1 Introduction

Despite deep neural networks (DNNs) have advanced the concept of "learning from data," and markedly improved performance across several applications in vision and language (Devlin et al., 2018; Tom et al., 2020), their overparametrized nature renders the tendency to overfit on training data (Zhang et al., 2021). This has led to concerns in generalization, which is a practically underscored perspective yet typically suffers from a gap relative to the training performance.

Improving generalizability is challenging. Common approaches include (model) regularization and data augmentation (Srivastava et al., 2014). While it is the default choice to integrate regularization such as weight decay and dropout into training, these methods are often insufficient for DNNs especially when coping with complicated network architectures (Chen et al., 2022). Another line of effort resorts to suitable optimization schemes attempting to find a generalizable local minimum. For example, SGD is more preferable than Adam on certain overparameterized problems since it converges to maximum margin solutions (Wilson et al., 2017). Decoupling weight decay from Adam also empirically facilitates generalizability (Loshchilov and Hutter, 2017). Unfortunately, the underlying mechanism remains unclear, and whether the generalization merits carry over to other intricate learning tasks calls for additional theoretical elaboration.

Our main focus, sharpness aware minimization/optimization (SAM), is a highly compelling optimization approach that facilitates state-of-the-art generalizability by exploiting sharpness of loss landscape (Foret et al., 2021; Chen et al., 2022). A high-level interpretation of sharpness is how violently the loss fluctuates within a neighborhood. It has been shown through large-scale empirical studies that sharpness-based measures highly correlate with generalization (Jiang et al., 2019). Several workshave successfully explored sharpness for generalization advances. For example, Keskar et al. (2016) suggests that the batchsize of SGD impresses solution flatness. Entropy SGD leverages local entropy in search of a flat valley (Chaudhari et al., 2017). Different from prior works, SAM induces flatness by explicitly minimizing the _adversarially_ perturbed loss, defined as the maximum loss of a neighboring area. Thanks to such a formulation, SAM has elevated generalization merits among various tasks in vision and language domains (Chen et al., 2022; Zhang et al., 2022). The mechanism fertilizing SAM's success is also theoretically investigated based on arguments of implicit regularization; see e.g., (Andriushchenko and Flammarion, 2022; Wen et al., 2023; Bartlett et al., 2022).

The adversary perturbation, or _adversary_ for short, is central to SAM's heightened generalization because it effectively measures sharpness through the loss difference with original model (Foret et al., 2021; Zhuang et al., 2022; Kim et al., 2022). In practice however, this awareness on sharpness is undermined by what we termed _friendly adversary_. Confined by the stochastic linearization for computational efficiency, SAM's adversary only captures the sharpness for a particular minibatch of data, and can become a friend on other data samples. Because the global sharpness is not approached accurately, the friendly adversary precludes SAM from attaining its utmost generalizability. The present work advocates variance suppressed sharpness aware optimization (VaSSO1) to alleviate 'friendliness' by stabilizing adversaries. With its _provable_ stabilized adversary, VaSSO showcases favorable numerical performance on various deep learning tasks.

Footnote 1: Vasso coincides with the Greek nickname for Vasiliki.

All in all, our contribution is summarized as follows.

* it can completely wipe out the generalization merits.
* A novel approach, VaSSO, is proposed to tackle this issue. VaSSO is equipped with what we termed _variance suppression_ to streamline a principled means for stabilizing adversaries. The theoretically guaranteed stability promotes refined global sharpness estimates, thereby alleviating the issue of friendly adversary.
* A side result is tighter convergence analyses for VaSSO and SAM that i) remove the bounded gradient assumption; and ii) deliver a more flexible choice for hyperparameters.
* Numerical experiments confirm the merits of stabilized adversary in VaSSO. It is demonstrated on image classification and neural machine translation tasks that VaSSO is capable of i) improving generalizability over SAM model-agnostically; and ii) nontrivially robustifying neural networks under the appearance of large label noise.

**Notation**. Bold lowercase (capital) letters denote column vectors (matrices); \(\|\mathbf{x}\|\) stands for \(\ell_{2}\) norm of vector \(\mathbf{x}\); and \(\langle\mathbf{x},\mathbf{y}\rangle\) is the inner product of \(\mathbf{x}\) and \(\mathbf{y}\). \(\mathbb{S}_{\rho}(\mathbf{x})\) denotes the surface of a ball with radius \(\rho\) centered at \(\mathbf{x}\), i.e., \(\mathbb{S}_{\rho}(\mathbf{x}):=\{\mathbf{x}+\rho\mathbf{u}\mid\|\mathbf{u}\|=1\}\).

## 2 The known, the good, and the challenge of SAM

This section starts with a brief recap of SAM (i.e., the known), followed with refined analyses and positive results regarding its convergence (i.e., the good). Lastly, the _friendly adversary_ issue is explained in detail and numerically illustrated.

### The known

Targeting at a minimum in flat basin, SAM enforces small loss around the entire neighborhood in the parameter space (Foret et al., 2021). This idea is formalized by a minimax problem

\[\min_{\mathbf{x}}\max_{\|\boldsymbol{\epsilon}\|\leq\rho}f\big{(}\mathbf{x}+ \boldsymbol{\epsilon}\big{)}\] (1)

where \(\rho\) is the radius of considered neighborhood, and the nonconvex objective is defined as \(f(\mathbf{x}):=\mathbb{E}_{\mathcal{B}}[f_{\mathcal{B}}(\mathbf{x})]\). Here, \(\mathbf{x}\) is the neural network parameter, and \(\mathcal{B}\) is a random batch of data. The merits of such a formulation resides in its implicit sharpness measure \(\max_{\|\boldsymbol{\epsilon}\|\leq\rho}f\big{(}\mathbf{x}+\boldsymbol{ \epsilon}\big{)}-f(\mathbf{x})\), which effectively drives the optimization trajectory towards the desirable flat valley (Kim et al., 2022).

The inner maximization of (1) has a natural interpretation as finding an _adversary_. Critical as it is, obtaining an adversary calls for _stochastic linearization_ to alleviate computational concerns, i.e.,

\[\bm{\epsilon}_{t}=\operatorname*{arg\,max}_{\|\bm{\epsilon}\|\leq\rho}f(\mathbf{ x}_{t}+\bm{\epsilon})\stackrel{{(a)}}{{\approx}}\operatorname*{arg\,max}_{\| \bm{\epsilon}\|\leq\rho}f(\mathbf{x}_{t})+\langle\nabla f(\mathbf{x}_{t}),\bm{ \epsilon}\rangle\stackrel{{(b)}}{{\approx}}\operatorname*{arg\, max}_{\|\bm{\epsilon}\|\leq\rho}f(\mathbf{x}_{t})+\langle\mathbf{g}_{t}(\mathbf{x}_{t}),\bm{ \epsilon}\rangle\] (2)

where linearization \((a)\) relies on the first order Taylor expansion of \(f(\mathbf{x}_{t}+\bm{\epsilon})\). This is typically accurate given the choice of a small \(\rho\). A stochastic gradient \(\mathbf{g}_{t}(\mathbf{x}_{t})\) then substitutes \(\nabla f(\mathbf{x}_{t})\) in \((b)\) to downgrade the computational burden of a full gradient. Catalyzed by the stochastic linearization in (2), it is possible to calculate SAM's adversary in closed-form

\[\boxed{\text{SAM:}\quad\bm{\epsilon}_{t}=\rho\frac{\mathbf{g}_{t}(\mathbf{x}_ {t})}{\|\mathbf{g}_{t}(\mathbf{x}_{t})\|}.}\] (3)

SAM then adopts the stochastic gradient of adversary \(\mathbf{g}_{t}(\mathbf{x}_{t}+\bm{\epsilon}_{t})\) to update \(\mathbf{x}_{t}\) in a SGD fashion. A step-by-step implementation is summarized in Alg. 1, where the means to find an adversary in line 4 is presented in a generic form in order to unify the algorithmic framework with later sections.

### The good

To provide a comprehensive understanding about SAM, this subsection focuses on Alg. 1, and establishes its convergence for (1). Some necessary assumptions are listed below, all of which are common for nonconvex stochastic optimization problems (Ghadimi and Lan, 2013; Bottou et al., 2016; Mi et al., 2022; Zhuang et al., 2022).

**Assumption 1** (lower bounded loss).: \(f(\mathbf{x})\) _is lower bounded, i.e., \(f(\mathbf{x})\geq f^{*},\forall\mathbf{x}\)._

**Assumption 2** (smoothness).: _The stochastic gradient \(\mathbf{g}(\mathbf{x})\) is \(L\)-Lipschitz, i.e., \(\|\mathbf{g}(\mathbf{x})-\mathbf{g}(\mathbf{y})\|\leq L\|\mathbf{x}-\mathbf{y} \|,\forall\mathbf{x},\mathbf{y}\)._

**Assumption 3** (bounded variance).: _The stochastic gradient \(\mathbf{g}(\mathbf{x})\) is unbiased with bounded variance, that is, \(\mathbb{E}[\mathbf{g}(\mathbf{x})|\mathbf{x}]=\nabla f(\mathbf{x})\) and \(\mathbb{E}[\|\mathbf{g}(\mathbf{x})-\nabla f(\mathbf{x})\|^{2}|\mathbf{x}]= \sigma^{2}\) for some \(\sigma>0\)._

The constraint of (1) is never violated since \(\|\bm{\epsilon}_{t}\|=\rho\) holds for each \(t\); see line 4 in Alg. 1. Hence, the convergence of SAM pertains to the behavior of objective, where a tight result is given below.

**Theorem 1** (SAM convergence).: _Suppose that Assumptions 1 - 3 hold. Let \(\eta_{t}\equiv\eta=\frac{\eta_{0}}{\sqrt{T}}\leq\frac{2}{3L}\), and \(\rho=\frac{\rho_{0}}{\sqrt{T}}\). Then with \(c_{0}=1-\frac{3L\eta}{2}\) (clearly \(0<c_{0}<1\)), Alg. 1 guarantees that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t})\|^{2} \big{]}\leq\mathcal{O}\bigg{(}\frac{\sigma^{2}}{\sqrt{T}}\bigg{)}\quad\text{ and}\quad\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\big{[}\|\nabla f( \mathbf{x}_{t}+\bm{\epsilon}_{t})\|^{2}\big{]}\leq\mathcal{O}\bigg{(}\frac{ \sigma^{2}}{\sqrt{T}}\bigg{)}.\]

The convergence rate of SAM is the same as SGD up to constant factors, where the detailed expression hidden under big \(\mathcal{O}\) notation can be found in Appendix D. Our results eliminate the need for the bounded gradient assumption compared to existing analyses in (Mi et al., 2022; Zhuang et al., 2022). Moreover, Theorem 1 enables a much larger choice of \(\rho=\mathcal{O}(T^{-1/2})\) relative to (Andriushchenko and Flamm, 2022), where the latter only supports \(\rho=\mathcal{O}(T^{-1/4})\).

A message from Theorem 1 is that _any_ adversary satisfying \(\bm{\epsilon}_{t}\in\mathbb{S}_{\rho}(\bm{0})\) ensures converge. Because the surface \(\mathbb{S}_{\rho}(\bm{0})\) is a gigantic space, it challenges the plausible optimality of the adversary and poses a natural question - _is it possible to find a more powerful adversary for generalization advances?_

### The challenge: friendly adversary

**Adversary to one minibatch is a friend of others.** SAM's adversary is'malicious' for minibatch \(\mathcal{B}_{t}\) but not necessarily for other data because it only safeguards \(f_{\mathcal{B}_{t}}(\mathbf{x}_{t}+\bm{\epsilon}_{t})-f_{\mathcal{B}_{t}}( \mathbf{x}_{t})\geq 0\) for a small \(\rho\). In fact, it can be shown that \(f_{\mathcal{B}}(\mathbf{x}_{t}+\bm{\epsilon}_{t})-f_{\mathcal{B}}(\mathbf{x}_ {t})\leq 0\) whenever the stochastic gradients do not align well, i.e., \((\mathbf{g}_{t}(\mathbf{x}_{t}),\mathbf{g}_{\mathcal{B}}(\mathbf{x}_{t}))\leq 0\). Note that such misalignment is common because of the variance in massive training datasets. This issue is referred to as _friendly adversary_, and it implies that the adversary \(\bm{\epsilon}_{t}\) cannot accurately depict the global sharpness of \(\mathbf{x}_{t}\). Note that the 'friendly adversary' also has a more involved interpretation, that is, \(\mathbf{g}_{t}(\mathbf{x}_{t})\) falls outside the column space of Hessian at convergence; see more discussions after (Wen et al., 2023, Definition 4.3). This misalignment of higher order derivatives undermines the inductive bias of SAM, thereby worsening generalization.

To numerically visualize the catastrophic impact of the friendly adversary, we manually introduce one by replacing line 4 of Alg. 1 as \(\tilde{\bm{\epsilon}}_{t}=\rho\tilde{\mathbf{g}}_{t}(\mathbf{x}_{t})/\| \tilde{\mathbf{g}}_{t}(\mathbf{x}_{t})\|\), where \(\tilde{\mathbf{g}}_{t}\) denotes the gradient on \(\tilde{\mathcal{B}}_{t}\), a randomly sampled batch of the same size as \(\mathcal{B}_{t}\). This modified approach is denoted as SAM-db, and its performance for i) ResNet-18 on CIFAR10 and ii) ResNet-34 on CIFAR1002 can be found in Fig. 1(a). Note that the test accuracy is normalized relative to SGD for the ease of visualization. It is evident that the friendly \(\tilde{\bm{\epsilon}}_{t}\) in SAM-db almost erases the generalization benefits entirely.

Footnote 2: https://www.cs.toronto.edu/~kriz/cifar.html

**Source of friendly adversary.** The major cause to the friendly adversary attributes to the gradient variance, which equivalently translates to the lack of stability in SAM's stochastic linearization \((2b)\). An illustrative three dimensional example is shown in Fig. 2, where we plot the adversary \(\bm{\epsilon}_{t}\) obtained from different \(\mathbf{g}_{t}\) realization in \((2b)\). The minibatch gradient is simulated by adding Gaussian noise to the true gradient. When the signal to noise ration (SNR) is similar to a practical scenario (ResNet-18 on CIFAR10 shown in Fig. 2 (e)), it can be seen in Fig. 2 (c) and (d) that the adversaries _almost uniformly_ spread over the norm ball, which strongly indicates the deficiency for sharpness evaluation.

**Friendly adversary in the lens of Frank Wolfe.** An additional evidence in supportive to SAM's friendly adversary resides in its connection to stochastic Frank Wolfe (SFW) that also heavily relies on stochastic linearization (Reddi et al., 2016). The stability of SFW is known to be vulnerable - its convergence cannot be guaranteed without a sufficient large batchsize. As thoroughly discussed in Appendix A, the means to obtain adversary in SAM is tantamount to one-step SFW with a _constant_ batchsize. This symbolizes the possible instability of SAM's stochastic linearization.

### A detailed look at friendly adversaries

The gradient variance is major cause to SAM's friendly adversary and unstable stochastic linearization, however this at first glance seems to conflict with an _empirical_ note termed \(m\)-sharpness, stating that the benefit of SAM is clearer when \(\bm{\epsilon}_{t}\) is found using subsampled \(\mathcal{B}_{t}\) of size \(m\) (i.e., larger variance).

Since \(m\)-sharpness highly hinges upon the loss curvature, it is unlikely to hold universally. For example, a transformer is trained on IWSLT-14 dataset, where the test performance (BLEU) decreases with smaller \(m\) even if we have tuned \(\rho\) carefully; see Fig. 1(c). On the theoretical side, an example is provided in (Andriushchenko and Flammarion, 2022, Sec. 3) to suggest that \(m\)-sharpness is not

Figure 1: (a) A friendly adversary erases the generalization merits of SAM; (b) \(m\)-sharpness may _not_ directly correlate with variance since noisy gradient degrades generalization; and (c) \(m\)-sharpness may not hold universally. Note that test accuracies in (a) and (b) are normalized to SGD.

necessarily related with sharpness or generalization. Moreover, there also exists specific choice for \(m\) such that the \(m\)-sharpness formulation is ill-posed. We will expand on this in Appendix B.

Even in the regime where \(m\)-sharpness is empirically observed such as ResNet-18 on CIFAR10 and ResNet-34 on CIFAR100, we show through experiments that \(m\)-sharpness is _not_ a consequence of gradient variance, thus not contradicting with the friendly adversary issue tackled in this work.

**Observation 1**.: **Same variance, different generalization.** Let \(m=128\) and batchsize \(b=128\). Recall the SAM-db experiment in Fig. 1(a). If \(m\)-sharpness is a direct result of gradient variance, it is logical to expect SAM-db has comparable performance to SAM simply because their batchzises (hence variance) for finding adversary are the same. Unfortunately, SAM-db degrades accuracy. We further increase the variance of \(\tilde{\mathbf{g}}_{t}(\mathbf{x}_{t})\) by setting \(m=64\). The resultant algorithm is denoted as SAM-db-m/2. It does not catch with SAM and performs even worse than SAM-db. These experiments validate that variance/stability correlates with friendly adversary instead of \(m\)-sharpness.

**Observation 2**.: **Enlarged variance degrades generalization.** We explicitly increase variance when finding adversary by adding Gaussian noise \(\bm{\zeta}\) to \(\mathbf{g}_{t}(\mathbf{x}_{t})\), i.e., \(\hat{\bm{\epsilon}}_{t}=\rho\frac{\frac{\mathbf{g}_{t}(\mathbf{x}_{t})}{\| \mathbf{g}_{t}(\mathbf{x}_{t})\cdot\bm{\zeta}\|}}{\|\mathbf{g}_{t}(\mathbf{x }_{t})\cdot\bm{\zeta}\|}\). After tuning the best \(\rho\) to compensate the variance of \(\bm{\zeta}\), the test performance is plotted in Fig. 1(b). It can be seen that the generalization merits clearly decrease with larger variance on both ResNet-18 and ResNet-34. This again illustrates that the plausible benefit of \(m\)-sharpness does not stem from increased variance.

In sum, observations 1 and 2 jointly suggest that gradient variance correlates with friendly adversary rather than \(m\)-sharpness, where understanding the latter is beyond the scope of current work.

## 3 Variance-supressed sharpness-aware optimization (VaSSO)

This section advocates variance suppression to handle the friendly adversary. We start with the design of VaSSO, then establish its stability. We also touch upon implementation and possible extensions.

### Algorithm design and stability analysis

A straightforward attempt towards stability is to equip SAM's stochastic linearization with variance reduced gradients such as SVRG and SARAH (Johnson and Zhang, 2013; Nguyen et al., 2017; Li et al., 2019). However, the requirement to compute a full gradient every a few iterations is infeasible and hardly scales well for tasks such as training DNNs.

The proposed variance suppression (VaSSO) overcomes this computational burden through a novel yet simple stochastic linearization. For a prescribed \(\theta\in(0,1)\), VaSSO is summarized below

\[\boxed{\mathbf{VaSSO}:}\quad\mathbf{d}_{t}=(1-\theta)\mathbf{d}_{t-1}+ \theta\mathbf{g}_{t}(\mathbf{x}_{t})\] (4a) \[\bm{\epsilon}_{t}=\operatorname*{arg\,max}_{\|\bm{\epsilon}\|\leq \rho}f(\mathbf{x}_{t})+\langle\mathbf{d}_{t},\bm{\epsilon}\rangle=\rho\frac{ \mathbf{d}_{t}}{\|\mathbf{d}_{t}\|}.\] (4b)

Compared with (2) of SAM, the key difference is that VaSSO relies on slope \(\mathbf{d}_{t}\) for a more stable stochastic linearization as shown in (4b). The slope \(\mathbf{d}_{t}\) is an exponentially moving average (EMA) of \(\{\mathbf{g}_{t}(\mathbf{x}_{t})\}_{t}\) such that the change over consecutive iterations is smoothed. Noticing that \(\bm{\epsilon}_{t}\) and \(\mathbf{d}_{t}\) share the same direction, the relatively smoothed \(\{\mathbf{d}_{t}\}_{t}\) thus imply the stability of \(\{\bm{\epsilon}_{t}\}_{t}\) in VaSSO. Moreover, as \(\mathbf{d}_{t}\) processes information of different minibatch data, the global sharpness can be captured in a principled manner to alleviate the friendly adversary challenge.

Figure 2: (a) - (d) SAM’s adversaries spread over the surface; (e) SNR is in \([0.01,0.1]\) when training a ResNet-18 on CIFAR10, where the SNR is calculated at the first iteration of every epoch.

To theoretically characterize the effectiveness of VaSSO, our first result considers \(\mathbf{d}_{t}\) as a qualified strategy to estimate \(\nabla f(\mathbf{x}_{t})\), and delves into its mean square error (MSE).

**Theorem 2** (Variance suppression).: _Suppose that Assumptions 1 - 3 hold. Let Alg. 1 equip with i) \(\bm{\epsilon}_{t}\) obtained by (4) with \(\theta\in(0,1)\); and, ii) \(\eta_{t}\) and \(\rho\) selected the same as Theorem 1. VaSSO guarantees that the MSE of \(\mathbf{d}_{t}\) is bounded by_

\[\mathbb{E}\big{[}\|\mathbf{d}_{t}-\nabla f(\mathbf{x}_{t})\|^{2}\big{]}\leq \theta\sigma^{2}+\mathcal{O}\bigg{(}\frac{(1-\theta)^{2}\sigma^{2}}{\theta^{2} \sqrt{T}}\bigg{)}.\] (5)

Because SAM's gradient estimate has a looser bound on MSE (or variance), that is, \(\mathbb{E}[\|\mathbf{g}_{t}-\nabla f(\mathbf{x}_{t})\|^{2}]\leq\sigma^{2}\), the shrunk MSE in Theorem 2 justifies the name of variance suppression.

Next, we quantify the stability invoked with the suppressed variance. It is convenient to start with necessary notation. Define the _quality_ of a stochastic linearization at \(\mathbf{x}_{t}\) with slope \(\mathbf{v}\) as \(\mathcal{L}_{t}(\mathbf{v}):=\max_{\|\bm{\epsilon}\|\leq\rho}f(\mathbf{x}_{t} )+\langle\mathbf{v},\bm{\epsilon}\rangle\). For example, \(\mathcal{L}_{t}(\mathbf{d}_{t})\) and \(\mathcal{L}_{t}\big{(}\mathbf{g}_{t}(\mathbf{x}_{t})\big{)}\) are quality of VaSSO and SAM, respectively. Another critical case of concern is \(\mathcal{L}_{t}\big{(}\nabla f(\mathbf{x}_{t})\big{)}\). It is shown in (Zhuang et al., 2022) that \(\mathcal{L}_{t}\big{(}\nabla f(\mathbf{x}_{t})\big{)}\approx\max_{\|\bm{ \epsilon}\|\leq\rho}f(\mathbf{x}_{t}+\bm{\epsilon})\) given a small \(\rho\). Moreover, \(\mathcal{L}_{t}\big{(}\nabla f(\mathbf{x}_{t})\big{)}-f(\mathbf{x}_{t})\) is also an accurate approximation to the sharpness (Zhuang et al., 2022). These observations safeguard \(\mathcal{L}_{t}(\nabla f(\mathbf{x}_{t}))\) as the anchor when analyzing the stability of SAM and VaSSO.

**Definition 1** (\(\delta\)-stability).: _A stochastic linearization with slope \(\mathbf{v}\) is said to be \(\delta\)-stable if its quality satisfies \(\mathbb{E}\big{[}|\mathcal{L}_{t}(\mathbf{v})-\mathcal{L}_{t}(\nabla f( \mathbf{x}_{t}))|\big{]}\leq\delta\)._

A larger \(\delta\) implies a more friendly adversary, hence is less preferable. We are now well-prepared for our main results on adversary's stability.

**Theorem 3** (Adversaries of VaSSO is more stable than SAM.).: _Suppose that Assumptions 1 - 3 hold. Under the same hyperparameter choices as Theorem 2, the stochastic linearization is \(\big{[}\sqrt{\theta}\rho\sigma+\mathcal{O}(\frac{\rho\sigma}{\theta T^{1/ 4}})\big{]}\)-stable for VaSSO, while \(\rho\sigma\)-stable in SAM._

Theorem 3 demonstrates that VaSSO alleviates the friendly adversary problem by promoting stability. Qualitatively, VaSSO is roughly \(\sqrt{\theta}\in(0,1)\) times more stable relative to SAM, since the term in big \(\mathcal{O}\) notation is negligible given a sufficiently large \(T\). Theorem 3 also guides the choice of \(\theta\) - preferably small but not too small, otherwise the term in big \(\mathcal{O}\) is inversely amplified.

### Additional perspectives of VaSSO

Having discussed about the stability, this subsection proceeds with other aspects of VaSSO for a thorough characterization.

**Convergence.** Summarized in the following corollary, the convergence of VaSSO can be pursued as a direct consequence of Theorem 1. The reason is that \(\bm{\epsilon}_{t}\in\mathbb{S}_{\rho}(\mathbf{0})\) is satisfied by (4).

**Corollary 1** (VaSSO convergence).: _Suppose that Assumptions 1 - 3 hold. Choosing \(\eta_{t}\) and \(\rho\) the same as Theorem 1, then for any \(\theta\in(0,1)\), VaSSO ensures that_

\[\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t})\|^{2} \big{]}\leq\mathcal{O}\bigg{(}\frac{\sigma^{2}}{\sqrt{T}}\bigg{)}\quad\text{ and}\quad\frac{1}{T}\sum_{t=0}^{T-1}\mathbb{E}\big{[}\|\nabla f( \mathbf{x}_{t}+\bm{\epsilon}_{t})\|^{2}\big{]}\leq\mathcal{O}\bigg{(}\frac{ \sigma^{2}}{\sqrt{T}}\bigg{)}.\]

**VaSSO better reflects sharpness around optimum.** Consider a near optimal region where \(\|\nabla f(\mathbf{x}_{t})\|\to 0\). Suppose that we are in a big data regime where \(\mathbf{g}_{t}(\mathbf{x}_{t})=\nabla f(\mathbf{x}_{t})+\bm{\zeta}\) for some Gaussian random variable \(\bm{\zeta}\). The covariance matrix of \(\bm{\zeta}\) is assumed to be \(\sigma^{2}\mathbf{I}\) for simplicity, but our discussion can be extended to more general scenarios using arguments from von Mises-Fisher statistics (Mardia and Jupp, 2000). SAM has difficulty to estimate the flatness in this case, since \(\bm{\epsilon}_{t}\approx\rho\bm{\zeta}/\|\bm{\zeta}\|\) uniformly distributes over \(\mathbb{S}_{\rho}(\mathbf{0})\) regardless of whether the neighboring region is sharp. On the other hand, VaSSO has \(\bm{\epsilon}_{t}=\rho\mathbf{d}_{t}/\|\mathbf{d}_{t}\|\). Because \(\{\mathbf{g}_{\tau}(\mathbf{x}_{\tau})\}_{\tau}\) on sharper valley tend to have larger magnitude, their EMA \(\mathbf{d}_{t}\) is helpful for distinguishing sharp with flat valleys.

**Memory efficient implementation.** Although at first glance VaSSO has to keep both \(\mathbf{d}_{t}\) and \(\bm{\epsilon}_{t}\) in memory, it can be implemented in a much more memory efficient manner. It is sufficient to store \(\mathbf{d}_{t}\) together with a scaler \(\|\mathbf{d}_{t}\|\) so that \(\bm{\epsilon}_{t}\) can be recovered on demand through normalization; see (4b). Hence, VaSSO has the same memory consumption as SAM.

**Extensions.** VaSSO has the potential to boost the performance of other SAM family approaches by stabilizing their stochastic linearization through variance suppression. For example, adaptive SAM methods (Kwon et al., 2021; Kim et al., 2022) ensure scale invariance for SAM, and GSAM (Zhuang et al., 2022) jointly minimizes a surrogated gap with (1). Nevertheless, these SAM variants leverage stochastic linearization in (2). It is thus envisioned that VaSSO can also alleviate the possible friendly adversary issues therein. Confined by computational resources, we only integrate VaSSO with GSAM in our experiments, and additional evaluation has been added into our research agenda.

## 4 Numerical tests

To support our theoretical findings and validate the powerfulness of variance suppression, this section assesses generalization performance of VaSSO via various learning tasks across vision and language domains. All experiments are run on NVIDIA V100 GPUs.

### Image classification

**Benchmarks.** Building on top of the selected base optimizer such as SGD and AdamW (Kingma and Ba, 2014; Loshchilov and Hutter, 2017), the test accuracy of VaSSO is compared with SAM and two adaptive approaches, ASAM and FisherSAM (Foret et al., 2021; Kwon et al., 2021; Kim et al., 2022).

**CIFAR10.** Neural networks including VGG-11, ResNet-18, WRN-28-10 and PyramidNet-110 are trained on CIFAR10. Standard implementation including random crop, random horizontal flip, normalization and cutout (Devries and Taylor, 2017) are leveraged for data augmentation. The first three models are trained for \(200\) epochs with a batchsize of \(128\), and PyramidNet-110 is trained for \(300\) epochs using batchsize \(256\). Cosine learning rate schedule is applied in all settings. The first three models use initial learning rate \(0.05\), and PyramidNet adopts \(0.1\). Weight decay is chosen as \(0.001\) for SAM, ASAM, FisherSAM and VaSSO following (Du et al., 2022; Mi et al., 2022), but \(0.0005\) for SGD. We tune \(\rho\) from \(\{0.01,0.05,0.1,0.2,0.5\}\) for SAM and find that \(\rho=0.1\) gives the best results for ResNet and WRN, \(\rho=0.05\) and \(\rho=0.2\) suit best for and VGG and PyramidNet, respectively. ASAM and VaSSO adopt the same \(\rho\) as SAM. FisherSAM uses the recommended \(\rho=0.1\)(Kim et al., 2022). For VaSSO, we tune \(\theta=\{0.4,0.9\}\) and report the best accuracy although VaSSO with both parameters outperforms SAM. We find that \(\theta=0.4\) works the best for ResNet-18 and WRN-28-10 while \(\theta=0.9\) achieves the best accuracy in other cases.

It is shown in Table 1 that VaSSO offers \(0.2\) to \(0.3\) accuracy improvement over SAM in all tested scenarios except for PyramidNet-110, where the improvement is about \(0.1\). These results illustrate that suppressed variance and the induced stabilized adversary are indeed beneficial for generalizability.

**CIFAR100.** The training setups on this dataset are the same as those on CIFAR10, except for the best choice for \(\rho\) of SAM is \(0.2\). The numerical results are listed in Table 2. It can be seen that SAM has significant generalization gain over SGD, and this gain is further amplified by VaSSO. On all tested models, VaSSO improves the test accuracy of SAM by \(0.2\) to \(0.3\). These experiments once again corroborate the generalization merits of VaSSO as a blessing of the stabilized adversary.

**ImageNet.** Next, we investigate the performance of VaSSO on larger scale experiments by training ResNet-50 and ViT-S/32 on ImageNet (Deng et al., 2009). Implementation details are deferred to Appendix C. Note that the baseline optimizer is SGD for ResNet and AdamW for ViT. VaSSO is also integrated with GSAM (V+G) to demonstrate that the variance suppression also benefits other SAM type approaches (Zhuang et al., 2022). For ResNet-50, it can be observed that vanilla VaSSO

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline CIFAR10 & SGD & SAM & ASAM & FisherSAM & VaSSO \\ \hline
**VGG-11-BN** & 93.20\({}_{\pm 0.05}\) & 93.82\({}_{\pm 0.05}\) & 93.47\({}_{\pm 0.04}\) & 93.60\({}_{\pm 0.09}\) & **94.10\({}_{\pm 0.07}\)** \\
**ResNet-18** & 96.25\({}_{\pm 0.06}\) & 96.58\({}_{\pm 0.10}\) & 96.33\({}_{\pm 0.09}\) & 96.72\({}_{\pm 0.03}\) & **96.77\({}_{\pm 0.09}\)** \\
**WRN-28-10** & 97.08\({}_{\pm 0.16}\) & 97.32\({}_{\pm 0.11}\) & 97.15\({}_{\pm 0.05}\) & 97.46\({}_{\pm 0.18}\) & **97.54\({}_{\pm 0.12}\)** \\
**PyramidNet-110** & 97.39\({}_{\pm 0.09}\) & 97.85\({}_{\pm 0.14}\) & 97.56\({}_{\pm 0.11}\) & 97.84\({}_{\pm 0.18}\) & **97.93\({}_{\pm 0.08}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Test accuracy (%) of VaSSO on various neural networks trained on CIFAR10.

outperforms other SAM variants, and offers a gain of \(0.26\) over SAM. V+G showcases the best performance with a gain of \(0.28\) on top of GSAM. VaSSO and V+G also exhibit the best test accuracy on ViT-S/32, where VaSSO improves SAM by 0.56 and V+G outperforms GSAM by 0.19. These numerical improvement demonstrates that stability of adversaries is indeed desirable.

### Neural machine translation

Having demonstrated the benefits of a suppressed variance on vision tasks, we then test VaSSO on German to English translation using a Transformer (Vaswani et al., 2017) trained on IWSLT-14 dataset (Cettolo et al., 2014). The fairseq implementation is adopted. AdamW is chosen as base optimizer in SAM and VaSSO because of its improved performance over SGD. The learning rate of AdamW is initialized to \(5\times 10^{-4}\) and then follows an inverse square root schedule. For momentum, we choose \(\beta_{1}=0.9\) and \(\beta_{2}=0.98\). Label smoothing is also applied with a rate of \(0.1\). Hyperparameter \(\rho\) is tuned for SAM from \(\{0.01,0.05,0.1,0.2\}\), and \(\rho=0.1\) performs the best. The same \(\rho\) is picked for ASAM and VaSSO as well.

The validation perplexity and test BLEU scores are shown in Table 4. It can be seen that both SAM and ASAM have better performance on validation perplexity and BLEU relative to AdamW. Although VaSSO with \(\theta=0.9\) has slightly higher validation perplexity, its BLEU score outperforms SAM and ASAM. VaSSO with \(\theta=0.4\) showcases the best generalization performance on this task, providing a \(0.22\) improvement on BLEU score relative to AdamW. This aligns with Theorems 2 and 3, which suggest that a small \(\theta\) is more beneficial to the stability of adversary.

### Additional tests

Additional experiments are conducted to corroborate the merits of suppressed variance and stabilized adversary in VaSSO. In particular, this subsection evaluates several flatness related metrics after training a ResNet-18 on CIFAR10 for \(200\) epochs, utilizing the same hyperparameters as those in Section 4.1.

**Hessian spectrum.** We first assess Hessian eigenvalues of a ResNet-18 trained with SAM and VaSSO. We focus on the largest eigenvalue \(\lambda_{1}\) and the ratio of largest to the fifth largest eigenvalue \(\lambda_{1}/\lambda_{5}\). These measurements are also adopted in (Foret et al., 2021; Jastrzebski et al., 2020) to reflect the flatness of the solution, where smaller numbers are more preferable. Because exact calculation for Hessian spectrum is too expensive provided the size of ResNet-18, we instead leverage Lanczos algorithm for approximation (Ghorbani et al., 2019). The results can be found in Table 5. It can be seen that SAM indeed converges to a much flatter solution compared with SGD, and VaSSO further improves upon SAM. This confirms that the friendly adversary issue is indeed alleviated by

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline CIFAR100 & SGD & SAM & ASAM & FisherSAM & VaSSO \\ \hline
**ResNet-18** & 77.90\({}_{\pm 0.07}\) & 80.96\({}_{\pm 0.12}\) & 79.91\({}_{\pm 0.04}\) & 80.99\({}_{\pm 0.13}\) & **81.30\({}_{\pm 0.13}\)** \\
**WRN-28-10** & 81.71\({}_{\pm 0.13}\) & 84.88\({}_{\pm 0.10}\) & 83.54\({}_{\pm 0.14}\) & 84.91\({}_{\pm 0.07}\) & **85.06\({}_{\pm 0.05}\)** \\
**PyramidNet-110** & 83.50\({}_{\pm 0.12}\) & 85.60\({}_{\pm 0.11}\) & 83.72\({}_{\pm 0.09}\) & 85.55\({}_{\pm 0.14}\) & **85.85\({}_{\pm 0.09}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Test accuracy (%) of VaSSO on various neural networks trained on CIFAR100.

\begin{table}
\begin{tabular}{c|c c c c c} \hline \hline ImageNet & vanilla & SAM & ASAM & GSAM & VaSSO & V+G \\ \hline
**ResNet-50** & 76.62\({}_{\pm 0.12}\) & 77.16\({}_{\pm 0.14}\) & 77.10\({}_{\pm 0.16}\) & 77.20\({}_{\pm 0.13}\) & **77.42\({}_{\pm 0.13}\)** & **77.48\({}_{\pm 0.04}\)** \\
**ViT-S/32** & 68.12\({}_{\pm 0.05}\) & 68.98\({}_{\pm 0.08}\) & 68.74\({}_{\pm 0.11}\) & 69.42\({}_{\pm 0.18}\) & **69.54\({}_{\pm 0.15}\)** & **69.61\({}_{\pm 0.11}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Test accuracy (%) of VaSSO on ImageNet, where V+G is short for VaSSO + GSAM.

\begin{table}
\begin{tabular}{c|c c c} \hline \hline  & SGD & SAM & VaSSO \\ \hline \(\lambda_{1}\) & 82.52 & 26.40 & **23.32** \\ \(\lambda_{1}/\lambda_{5}\) & 16.63 & 2.12 & **1.86** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Hessian spectrum of a ResNet-18 trained on CIFAR10.

the suppressed variance in VaSSO, which in turn boosts the generalizability of ResNet-18 as shown earlier in Section 4.1.

**Label noise.** It is known that SAM holds great potential to harness robustness to neural networks under the appearance of label noise in training data (Foret et al., 2021). As the training loss landscape is largely perturbed by the label noise, this is a setting where the suppressed variance and stabilized adversaries are expected to be advantageous. In our experiments, we measure the performance VaSSO in the scenarios where certain fraction of the training labels are randomly flipped. Considering \(\theta=\{0.9,0.4,0.2\}\), the corresponding test accuracies are summarized in Table 6.

Our first observation is that VaSSO outperforms SAM at different levels of label noise. VaSSO elevates higher generalization improvement as the ratio of label noise grows. In the case of \(75\%\) label noise, VaSSO with \(\theta=0.4\) nontrivially outperforms SAM with an absolute improvement more than \(5\), while VaSSO with \(\theta=0.2\) markedly improves SAM by roughly \(10\). In all scenarios, \(\theta=0.2\) showcases the best performance and \(\theta=0.9\) exhibits the worst generalization when comparing among VaSSO. In addition, when fixing the choice to \(\theta\), e.g., \(\theta=0.2\), it is found that VaSSO has larger absolute accuracy improvement over SAM under higher level of label noise. These observations coincide with Theorem 3, which predicts that VaSSO is suitable for settings with larger label noise due to enhanced stability especially when \(\theta\) is chosen small (but not too small).

## 5 Other related works

This section discusses additional related work on generalizability of DNNs. The possibility of blending VaSSO with other approaches is also entailed to broaden the scope of this work.

**Sharpness and generalization.** Since the study of Keskar et al. (2016), the relation between sharpness and generalization has been intensively investigated. It is observed that sharpness is closely correlated with the ratio between learning rate and batchsize in SGD (Jastrzebski et al., 2017). Theoretical understandings on the generalization error using sharpness-related measures can be found in e.g., (Dziugaite and Roy, 2017; Neyshabur et al., 2017; Wang and Mao, 2022). These works justify the goal of seeking for a flatter valley to enhance generalizability. Targeting at a flatter minimum, approaches other than SAM are also developed. For example, Izmailov et al. (2018) proposes stochastic weight averaging for DNNs. Wu et al. (2020) studies a similar algorithm as SAM while putting more emphases on the robustness of adversarial training.

**Other SAM type approaches.** Besides the discussed ones such as GSAM and ASAM, (Zhao et al., 2022) proposes a variant of SAM by penalizing the gradient norm based on the observation where sharper valley tends to have gradient with larger norm. Barrett and Dherin (2021) arrive at a similar conclusion by analyzing the gradient flow. Exploiting multiple (ascent) steps to find an adversary is systematically studied in (Kim et al., 2023). SAM has also been extended to tackle the challenges in

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline  & SAM & \begin{tabular}{c} VaSSO \\ (\(\theta=0.9\)) \\ \end{tabular} & \begin{tabular}{c} VaSSO \\ (\(\theta=0.4\)) \\ \end{tabular} & 
\begin{tabular}{c} VaSSO \\ (\(\theta=0.2\)) \\ \end{tabular} \\ \hline
**25\% label noise** & 96.39\({}_{\pm 0.12}\) & 96.36\({}_{\pm 0.11}\) & 96.42\({}_{\pm 0.12}\) & **96.48\({}_{\pm 0.09}\)** \\
**50\% label noise** & 93.93\({}_{\pm 0.21}\) & 94.00\({}_{\pm 0.24}\) & 94.63\({}_{\pm 0.21}\) & **94.93\({}_{\pm 0.16}\)** \\
**75\% label noise** & 75.36\({}_{\pm 0.42}\) & 77.40\({}_{\pm 0.37}\) & 80.94\({}_{\pm 0.40}\) & **85.02\({}_{\pm 0.39}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Test accuracy (%) of VaSSO on CIFAR10 under different levels of label noise.

domain adaptation (Wang et al., 2023). However, these works overlook the friendly adversary issue, and the proposed VaSSO provides algorithmic possibilities for generalization benefits by stabilizing their adversaries. Since the desirable confluence with VaSSO can be intricate, we leave an in-depth investigation for future work.

**Limitation of VaSSO and possible solutions.** The drastically improved generalization of VaSSO comes at the cost of additional computation. Similar to SAM, VaSSO requires to backpropagate twice per iteration. Various works have tackled this issue and developed lightweight SAM. LookSAM computes the extra stochastic gradient once every a few iterations and reuses it in a fine-grained manner to approximate the additional gradient (Liu et al., 2022). ESAM obtains its adversary based on stochastic weight perturbation, and further saves computation by selecting a subset of the minibatch data for gradient computation (Du et al., 2022). The computational burden of SAM can be compressed by switching between SAM and SGD following a predesigned schedule (Zhao et al., 2022), or in an adaptive fashion (Jiang et al., 2023). SAF connects SAM with distillation for computational merits (Du et al., 2022). It should be pointed out that most of these works follow the stochastic linearization of SAM, hence can also encounter the issue of friendly adversary. This opens the door of merging VaSSO with these approaches for generalization merits while respecting computational overhead simultaneously. This has been included in our research agenda.

## 6 Concluding remarks

This contribution demonstrates that stabilizing adversary through variance suppression consolidates the generalization merits of sharpness aware optimization. The proposed approach, VaSSO, provably facilitates stability over SAM. The theoretical merit of VaSSO reveals itself in numerical experiments, and catalyzes model-agnostic improvement over SAM among various vision and language tasks. Moreover, VaSSO nontrivially enhances model robustness against high levels of label noise. Our results corroborate VaSSO as a competitive alternative of SAM.

### Acknowledgement

This research is supported by NSF grants 2128593, 2126052, 2212318, 2220292, and 2312547. The authors would also like to thank anonymous reviewers for their feedback.

## References

* Andriushchenko and Flammarion (2022) Maksym Andriushchenko and Nicolas Flammarion. Towards understanding sharpness-aware minimization. In _Proc. Int. Conf. Machine Learning_, pages 639-668, 2022.
* Barrett and Dherin (2021) David GT Barrett and Benoit Dherin. Implicit gradient regularization. In _Proc. Int. Conf. Learning Represention_, 2021.
* Bartlett et al. (2022) Peter L Bartlett, Philip M Long, and Olivier Bousquet. The dynamics of sharpness-aware minimization: Bouncing across ravines and drifting towards wide minima. _arXiv:2210.01513_, 2022.
* Bottou et al. (2016) Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. _arXiv preprint arXiv:1606.04838_, 2016.
* Cettolo et al. (2014) M Cettolo, J Niehues, S Stker, L Bentivogli, and M Federico. Report on the 11th iwslt evaluation campaign, iwslt 2014. 2014.
* Chaudhari et al. (2017) Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradient descent into wide valleys. In _Proc. Int. Conf. Learning Represention_, 2017.
* Chen et al. (2022) Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets without pre-training or strong data augmentations. In _Proc. Int. Conf. Learning Represention_, 2022.
* Deng et al. (2009) Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In _Proc. Conf. Computer Vision and Pattern Recognition_, pages 248-255, 2009.
* Deng et al. (2018)Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv:1810.04805_, 2018.
* Devries and Taylor (2017) Terrance Devries and Graham W. Taylor. Improved regularization of convolutional neural networks with cutout. abs/1708.04552, 2017.
* Du et al. (2022a) Jiawei Du, Hanshu Yan, Jiashi Feng, Joey Tianyi Zhou, Liangli Zhen, Rick Siow Mong Goh, and Vincent Y. F. Tan. Efficient sharpness-aware minimization for improved training of neural networks. In _Proc. Int. Conf. Learning Representation_, 2022a.
* Du et al. (2022b) Jiawei Du, Daquan Zhou, Jiashi Feng, Vincent Y. F. Tan, and Joey Tianyi Zhou. Sharpness-aware training for free. In _Proc. Adv. Neural Info. Processing Systems_, 2022b.
* Dziugaite and Roy (2017) Gintare Karolina Dziugaite and Daniel M. Roy. Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data. In _Proc. Conf. Uncertainty in Artif. Intel._, 2017.
* Foret et al. (2021) Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efficiently improving generalization. In _Proc. Int. Conf. Learning Represention_, 2021.
* Ghadimi and Lan (2013) Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming. _SIAM Journal on Optimization_, 23(4):2341-2368, 2013.
* Ghorbani et al. (2019) Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization via hessian eigenvalue density. In _Proc. Int. Conf. Machine Learning_, pages 2232-2241, 2019.
* Izmailov et al. (2018) Pavel Izmailov, Dmitri Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. In _Proc. Conf. Uncertainty in Artif. Intel._, pages 876-885, 2018.
* Jastrzebski et al. (2017) Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in sgd. _arXiv:1711.04623_, 2017.
* Jastrzebski et al. (2020) Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek Tabor, Kyunghyun Cho, and Krzysztof J. Geras. The break-even point on optimization trajectories of deep neural networks. In _Proc. Int. Conf. Learning Represention_, 2020.
* Jiang et al. (2023) Weisen Jiang, Hansi Yang, Yu Zhang, and James Kwok. An adaptive policy to employ sharpness-aware minimization. _arXiv:2304.14647_, 2023.
* Jiang et al. (2019) Yiding Jiang, Behnam Neyshabur, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Fantastic generalization measures and where to find them. _arXiv:1912.02178_, 2019.
* Johnson and Zhang (2013) Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In _Proc. Advances in Neural Info. Process. Syst._, pages 315-323, Lake Tahoe, Nevada, 2013.
* Keskar et al. (2016) Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. In _Proc. Int. Conf. Learning Represention_, 2016.
* Kim et al. (2023) Hoki Kim, Jinseong Park, Yujin Choi, Woojin Lee, and Jaewook Lee. Exploring the effect of multi-step ascent in sharpness-aware minimization. _arXiv:2302.10181_, 2023.
* Kim et al. (2022) Minyoung Kim, Da Li, Shell Xu Hu, and Timothy M. Hospedales. Fisher SAM: Information geometry and sharpness aware minimisation. In _Proc. Int. Conf. Machine Learning_, pages 11148-11161, 2022.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _Proc. Int. Conf. Learning Represention_, 2014.
* Kwon et al. (2021) Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In Kwon Choi. ASAM: Adaptive sharpness-aware minimization for scale-invariant learning of deep neural networks. In _Proc. Int. Conf. Machine Learning_, volume 139, pages 5905-5914, 2021.
* Kwon et al. (2020)Bingcong Li, Meng Ma, and Georgios B Giannakis. On the convergence of SARAH and beyond. _arXiv preprint arXiv:1906.02351_, 2019.
* Li et al. (2021) Bingcong Li, Alireza Sadeghi, and Georgios Giannakis. Heavy ball momentum for conditional gradient. In _Proc. Advances in Neural Info. Process. Syst._, 2021.
* Liu et al. (2022) Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You. Towards efficient and scalable sharpness-aware minimization. In _Proc. Conf. Computer Vision and Pattern Recognition_, volume 2022, pages 12350-12360, 2022.
* Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In _Proc. Int. Conf. Learning Represention_, 2017.
* Mardia and Jupp (2000) K. V. Mardia and P. E. Jupp. _Directional Statistics_. Directional statistics, 2000.
* Mi et al. (2022) Peng Mi, Li Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, and Dacheng Tao. Make sharpness-aware minimization stronger: A sparsified perturbation approach. In _Proc. Adv. Neural Info. Processing Systems_, 2022.
* Mokhtari et al. (2020) Aryan Mokhtari, Hamed Hassani, and Amin Karbasi. Stochastic conditional gradient methods: From convex minimization to submodular maximization. _Journal of Machine Learning Research_, 21(1):4232-4280, 2020.
* Neyshabur et al. (2017) Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, Nathan Srebro, and Nati Srebro. Exploring generalization in deep learning. In _Proc. Adv. Neural Info. Processing Systems_, volume 30, pages 5947-5956, 2017.
* Nguyen et al. (2017) Lam M Nguyen, Jie Liu, Katya Scheinberg, and Martin Takac. SARAH: A novel method for machine learning problems using stochastic recursive gradient. In _Proc. Intl. Conf. Machine Learning_, Sydney, Australia, 2017.
* Reddi et al. (2016) Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic Frank-Wolfe methods for nonconvex optimization. In _Allerton conference on communication, control, and computing_, pages 1244-1251. IEEE, 2016.
* Srivastava et al. (2014) Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. _J. Mach. Learn. Res._, 15:1929-1958, 2014.
* Tom et al. (2020) Brown Tom, Mann Benjamin, Ryder Nick, Subbiah Melanie, Kaplan Jared, Dhariwal Prafulla, Neelakantan Arvind, Shyam Pranav, Sastry Girish, Askell Amanda, Agarwal Sandhini, Herbert-Voss Ariel, Krueger Gretchen, Henighan Tom, Child Rewon, Ramesh Aditya, Ziegler Daniel M., Wu Jeffrey, Winter Clemens, Hesse Christopher, Chen Mark, Sigler Eric, Litwin Mateusz, Gray Scott, Chess Benjamin, Clark Jack, Berner Christopher, McCandlish Sam, Radford Alec, Sutskever Ilya, and Amodei Dario. Language models are few-shot learners. In _Proc. Adv. Neural Info. Processing Systems_, volume 33, pages 1877-1901, 2020.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In _Proc. Adv. Neural Info. Processing Systems_, volume 30, 2017.
* Wang et al. (2023) Pengfei Wang, Zhaoxiang Zhang, Zhen Lei, and Lei Zhang. Sharpness-aware gradient matching for domain generalization. In _Proc. Conf. Computer Vision and Pattern Recognition_, pages 3769-3778, 2023.
* Wang and Mao (2022) Ziqiao Wang and Yongyi Mao. On the generalization of models trained with sgd: Information-theoretic bounds and implications. In _Proc. Int. Conf. Learning Represention_, 2022.
* Wen et al. (2023) Kaiyue Wen, Tengyu Ma, and Z hiyuan Li. How does sharpness-aware minimization minimizes sharpness. In _Proc. Int. Conf. Learning Represention_, 2023.
* Wilson et al. (2017) Ashia C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, Benjamin Recht, and Nati Srebro. The marginal value of adaptive gradient methods in machine learning. In _Proc. Int. Conf. Machine Learning_, volume 30, pages 4148-4158, 2017.
* Wilson et al. (2017)* Wu et al. [2020] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. In _Proc. Adv. Neural Info. Processing Systems_, volume 33, pages 2958-2969, 2020.
* Zhang et al. [2021] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* Zhang et al. [2022] Zhiyuan Zhang, Ruixuan Luo, Qi Su, and Xu Sun. GA-SAM: Gradient-strength based adaptive sharpness-aware minimization for improved generalization. In _Proc. Conf. Empirical Methods in Natural Language Processing_, 2022.
* Zhao et al. [2022a] Yang Zhao, Hao Zhang, and Xiuyuan Hu. Penalizing gradient norm for efficiently improving generalization in deep learning. In _Proc. Int. Conf. Machine Learning_, pages 26982-26992, 2022a.
* Zhao et al. [2022b] Yang Zhao, Hao Zhang, and Xiuyuan Hu. SS-SAM: Stochastic scheduled sharpness-aware minimization for efficiently training deep neural networks. _arXiv:2203.09962_, 2022b.
* Zhuang et al. [2022] Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha Dvornek, Sekhar Tatikonda, James Duncan, and Ting Liu. Surrogate gap minimization improves sharpness-aware training. In _Proc. Int. Conf. Learning Representation_, 2022.

**Supplementary Document for**

**"Enhancing Sharpness-Aware Optimization Through Variance Suppression"**

## Appendix A Linking SAM adversary with stochastic Frank Wolfe

### Stochastic Frank Wolfe (SFW)

We first briefly review SFW. Consider the following nonconvex stochastic optimization

\[\max_{\mathbf{x}\in\mathcal{X}}h(\mathbf{x}):=\mathbb{E}_{\xi}\big{[}h(\mathbf{ x},\xi)\big{]}\] (6)

where \(\mathcal{X}\) is a convex and compact constraint set. SFW for solving (6) is summarized below.

```
1:Initialize:\(\mathbf{x}_{0}\in\mathcal{X}\)
2:for\(t=0,1,\dots,T-1\)do
3: draw iid samples \(\{\xi_{t}^{b}\}_{b=1}^{B_{t}}\)
4: let \(\mathbf{\tilde{g}}_{t}=\frac{1}{B_{t}}\sum_{b=1}^{B_{t}}\nabla h(\mathbf{x}_{t },\xi_{t}^{b})\)
5:\(\mathbf{v}_{t+1}=\arg\max_{\mathbf{v}\in\mathcal{X}}\langle\mathbf{\hat{g}}_{t },\mathbf{v}\rangle\)
6:\(\mathbf{x}_{t+1}=(1-\gamma_{t})\mathbf{x}_{t}+\gamma_{t}\mathbf{v}_{t+1}\)
7:endfor ```

**Algorithm 2** SFW (Reddi et al., 2016)

It has been shown in (Reddi et al., 2016, Theorem 2) that one has to use a sufficient large batch size \(B_{t}=\mathcal{O}(T),\forall t\) to ensure convergence of SFW. This is because line 5 in Alg. 2 is extremely sensitive to gradient noise.

### The adversary of SAM

By choosing \(h(\bm{\epsilon})=f(\mathbf{x}_{t}+\bm{\epsilon})\) and \(\mathcal{X}=\mathbb{S}_{\rho}(\bm{0})\), it is not hard to observe that 1-iteration SFW with \(\gamma_{0}=1\) gives equivalent solution to the stochastic linearization in SAM; cf. (2) and (3). This link suggests that the SAM adversary also suffers from stability issues in the same way as SFW. Moreover, what amplifies this issue in SAM is the adoption of a constant batch size, which is typically small and far less than the \(\mathcal{O}(T)\) requirement for SFW.

Our solution VaSSO takes inspiration from modified SFW approaches which leverage a constant batch size to ensure convergence; see e.g., (Mokhtari et al., 2020; Li et al., 2021). Even though, coping with SAM's instability is still challenging with two major obstacles. First, SAM uses _one-step_ SFW, which internally breaks nice analytical structures. Moreover, the inner maximization (i.e., the objective function to the SFW) _varies every iteration_ along with the updated \(\mathbf{x}_{t}\).

### The three dimensional example in Fig. 2

Detailed implementation for Fig. 2 is listed below. We use \(\nabla f(\mathbf{x})=[0.2,-0.1,0.6]\). The stochastic noise is \(\bm{\xi}=[\xi_{1},\xi_{2},\xi_{3}]\), where \(\xi_{1}\), \(\xi_{2}\), \(\xi_{3}\) are iid Gaussian random variables with variance scaling with \(0.2,1,2\), respectively. We scale the variance to change the SNR. We generate \(100\) adversaries by solving \(\arg\max_{\|\bm{\epsilon}\|\leq\rho}\langle\nabla f(\mathbf{x})+\bm{\xi},\bm{ \epsilon}\rangle\) for each choice of SNR. As shown in Fig. 2, the adversaries are unlikely to capture the sharpness information when the SNR is small, because they spread indistinguishably over the sphere.

## Appendix B More on \(m\)-sharpness

\(m\)**-sharpness can be ill-posed.** Our reason for not studying \(m\)-sharpness directly is that its formulation (Andriushchenko and Flammario, 2022, eq. (3)) may be ill-posed mathematically due to the lack of a clear definition on how the dataset \(\mathcal{S}\) is partitioned. Consider the following example, where the same notation as (Andriushchenko and Flammario, 2022) is adopted for convenience. Suppose that the loss function is \(l_{i}(w)=a_{i}w^{2}+b_{i}w\), where \((a_{i},b_{i})\) are data points and \(w\) is theparameter to be optimized. Let the dataset have \(4\) samples, \((a_{1}=0,b_{1}=1)\); \((a_{2}=0,b_{2}=-1)\); \((a_{3}=-1,b_{3}=0)\); and, \((a_{4}=1,b_{4}=0)\). Consider 2-sharpness.

* If the data partition is {1,2} and {3,4}, the objective of 2-sharpness i.e., equation (3) in (Andriushchenko and Flammarion, 2022), becomes \(\min_{w}\sum_{i=1}^{2}\max_{||\delta||<\rho}0\).
* If the data partition is {1,3} and {2,4}, the objective is \(\min_{w}\sum_{i=1}^{2}\max_{||\delta||<\rho}f_{i}(w,\delta)\), where \(f_{1}\) is the loss on partition {1,3}, i.e., \(f_{1}(w,\delta)=-(w+\delta)^{2}+(w+\delta)\); and \(f_{2}(w,\delta)=(w+\delta)^{2}-(w+\delta)\) is the loss on partition {3,4}.

Clearly, the objective functions are different when the data partition varies. This makes the problem ill-posed - different manners of data partition lead to entirely different loss curvature. In practice, the data partition even vary with a frequency of an epoch due to the random shuffle.

## Appendix C Details on numerical results

**CIFAR10 and CIFAR100.** For these small resolution datasets, we slightly change the first convolution layer of ResNet18 and WRN-28-10 to one with \(3\times 3\) kernel size, 1 stride and 1 padding following Mi et al. (2022). The results on SGD and SAM demonstrate that the accuracy is almost identical to the vanilla model.

**ResNet50 on ImageNet.** Due to the constraints on computational resources, we report the averaged results over \(2\) independent runs. For this dataset, we randomly resize and crop all images to a resolution of \(224\times 224\), and apply random horizontal flip, normalization during training. The batch size is chosen as \(128\) with a cosine learning rate scheduling with an initial step size \(0.05\). The momentum and weight decay of base optimizer, SGD, are set as \(0.9\) and \(10^{-4}\), respectively. We further tune \(\rho\) from \(\{0.05,0.075,0.1,0.2\}\), and chooses \(\rho=0.075\) for SAM. VaSSO uses \(\theta=0.99\). VaSSO and ASAM adopt the same \(\rho=0.075\).

**ViT-S/32 on ImageNet.** We follow the implementation of (Du et al., 2022), where we train the model for \(300\) epochs with a batch size of \(4096\). The baseline optimizer is chosen as AdamW with weight decay \(0.3\). SAM relies on \(\rho=0.05\). For the implementation of GSAM and V+G, we adopt the same implementation from (Zhuang et al., 2022).

## Appendix D Missing proofs

Alg. 1 can be written as

\[\mathbf{x}_{t+\frac{1}{2}} =\mathbf{x}_{t}+\bm{\epsilon}_{t}\] (7a) \[\mathbf{x}_{t+1} =\mathbf{x}_{t}-\eta_{t}\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\] (7b)

where \(\|\bm{\epsilon}_{t}\|=\rho\). In SAM, we have \(\bm{\epsilon}_{t}=\rho\frac{\mathbf{g}_{t}(\mathbf{x}_{t})}{\|\mathbf{g}_{t} (\mathbf{x}_{t})\|}\), and in VaSSO we have \(\bm{\epsilon}_{t}=\rho\frac{\mathbf{d}_{t}}{\|\mathbf{d}_{t}\|}\).

### Useful lemmas

This subsection presents useful lemmas to support our main results.

**Lemma 1**.: _Alg. 1 (or equivalently iteration (7)) ensures that_

\[\eta_{t}\mathbb{E}\big{[}\langle\nabla f(\mathbf{x}_{t}),\nabla f(\mathbf{x} _{t})-\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\rangle\big{]}\leq\frac{L\eta _{t}^{2}}{2}\mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t})\|^{2}\big{]}+\frac{L \rho^{2}}{2}.\]

Proof.: To start with, we have that

\[\big{\langle}\nabla f(\mathbf{x}_{t}),\nabla f(\mathbf{x}_{t})-\mathbf{g}_{t }(\mathbf{x}_{t+\frac{1}{2}})\big{\rangle}=\langle\nabla f(\mathbf{x}_{t}), \nabla f(\mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t})+\mathbf{g}_{t}( \mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\rangle.\] (8)Taking expectation conditioned on \(\mathbf{x}_{t}\), we arrive at

\[\mathbb{E}\big{[}\big{\langle}\nabla f(\mathbf{x}_{t}),\nabla f( \mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\big{\rangle}|\mathbf{ x}_{t}\big{]}\] \[=\mathbb{E}\big{[}\langle\nabla f(\mathbf{x}_{t}),\nabla f( \mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t})\rangle|\mathbf{x}_{t}\big{]}+ \mathbb{E}\big{[}\langle\nabla f(\mathbf{x}_{t}),\mathbf{g}_{t}(\mathbf{x}_{t}) -\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\rangle|\mathbf{x}_{t}\big{]}\] \[=\mathbb{E}\big{[}\langle\nabla f(\mathbf{x}_{t}),\mathbf{g}_{t}( \mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\rangle|\mathbf{x}_ {t}\big{]}\] \[\leq\mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t})\|\cdot\|\mathbf{ g}_{t}(\mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\|\mathbf{x}_{t}\big{]}\] \[\stackrel{{(a)}}{{\leq}}L\mathbb{E}\big{[}\|\nabla f (\mathbf{x}_{t})\|\cdot\|\mathbf{x}_{t}-\mathbf{x}_{t+\frac{1}{2}}\|\| \mathbf{x}_{t}\big{]}\] \[\stackrel{{(b)}}{{=}}L\rho\|\nabla f(\mathbf{x}_{t})\|\]

where (a) is because of Assumption 2; and (b) is because \(\mathbf{x}_{t}-\mathbf{x}_{t+\frac{1}{2}}=-\boldsymbol{\epsilon}_{t}\) and its norm is \(\rho\). This inequality ensures that

\[\eta_{t}\mathbb{E}\big{[}\big{\langle}\nabla f(\mathbf{x}_{t}),\nabla f( \mathbf{x}_{t})-\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\big{\rangle}| \mathbf{x}_{t}\big{]}\leq L\rho\eta_{t}\|\nabla f(\mathbf{x}_{t})\|\leq\frac{ L\eta_{t}^{2}\|\nabla f(\mathbf{x}_{t})\|^{2}}{2}+\frac{L\rho^{2}}{2}\]

where the last inequality is because \(\rho\eta_{t}\|\nabla f(\mathbf{x}_{t})\|\leq\frac{1}{2}\eta_{t}^{2}\|\nabla f (\mathbf{x}_{t})\|^{2}+\frac{1}{2}\rho^{2}\). Taking expectation w.r.t. \(\mathbf{x}_{t}\) finishes the proof. 

**Lemma 2**.: _Alg. 1 (or equivalently iteration (7)) ensures that_

\[\mathbb{E}\big{[}\|\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\|^{2}\big{]} \leq 2L^{2}\rho^{2}+2\mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t})\|^{2} \big{]}+2\sigma^{2}.\]

Proof.: The proof starts with bounding \(\|\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\|\) as

\[\|\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\|^{2} =\|\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})-\mathbf{g}_{t}( \mathbf{x}_{t})+\mathbf{g}_{t}(\mathbf{x}_{t})\|^{2}\] \[\leq 2\|\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})-\mathbf{g}_{t}( \mathbf{x}_{t})\|^{2}+2\|\mathbf{g}_{t}(\mathbf{x}_{t})\|^{2}\] \[\stackrel{{(a)}}{{\leq}}2L^{2}\|\mathbf{x}_{t}- \mathbf{x}_{t+\frac{1}{2}}\|^{2}+2\|\mathbf{g}_{t}(\mathbf{x}_{t})\|^{2}\] \[\stackrel{{(b)}}{{=}}2L^{2}\rho^{2}+2\|\mathbf{g}_{t} (\mathbf{x}_{t})-\nabla f(\mathbf{x}_{t})+\nabla f(\mathbf{x}_{t})\|^{2}\]

where (a) is the result of Assumption 2; and (b) is because \(\mathbf{x}_{t}-\mathbf{x}_{t+\frac{1}{2}}=-\boldsymbol{\epsilon}_{t}\) and its norm is \(\rho\).

Taking expectation conditioned on \(\mathbf{x}_{t}\), we have

\[\mathbb{E}\big{[}\|\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\|^{2}| \mathbf{x}_{t}\big{]} \leq 2L^{2}\rho^{2}+2\mathbb{E}\big{[}\|\mathbf{g}_{t}(\mathbf{x}_{ t})-\nabla f(\mathbf{x}_{t})+\nabla f(\mathbf{x}_{t})\|^{2}|\mathbf{x}_{t}\big{]}\] \[\leq 2L^{2}\rho^{2}+2\|\nabla f(\mathbf{x}_{t})\|^{2}+2\sigma^{2}\]

where the last inequality is because of Assumption 3. Taking expectation w.r.t. the randomness in \(\mathbf{x}_{t}\) finishes the proof. 

**Lemma 3**.: _Let \(A_{t+1}=\alpha A_{t}+\beta\) with some \(\alpha\in(0,1)\), then we have_

\[A_{t+1}\leq\alpha^{t+1}A_{0}+\frac{\beta}{1-\alpha}.\]

Proof.: The proof can be completed by simply unrolling \(A_{t+1}\) and using the fact \(1+\alpha+\alpha^{2}+\ldots+\alpha^{t}\leq\frac{1}{1-\alpha}\)

### Proof of Theorem 1

Proof.: Using Assumption 2, we have that

\[f(\mathbf{x}_{t+1})-f(\mathbf{x}_{t})\] \[\leq\langle\nabla f(\mathbf{x}_{t}),\mathbf{x}_{t+1}-\mathbf{x}_{t }\rangle+\frac{L}{2}\|\mathbf{x}_{t+1}-\mathbf{x}_{t}\|^{2}\] \[=-\eta_{t}\langle\nabla f(\mathbf{x}_{t}),\mathbf{g}_{t}( \mathbf{x}_{t+\frac{1}{2}})\rangle+\frac{L\eta_{t}^{2}}{2}\|\mathbf{g}_{t}( \mathbf{x}_{t+\frac{1}{2}})\|^{2}\] \[=-\eta_{t}\langle\nabla f(\mathbf{x}_{t}),\mathbf{g}_{t}( \mathbf{x}_{t+\frac{1}{2}})-\nabla f(\mathbf{x}_{t})+\nabla f(\mathbf{x}_{t}) \rangle+\frac{L\eta_{t}^{2}}{2}\|\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})\|^ {2}\] \[=-\eta_{t}\|\nabla f(\mathbf{x}_{t})\|^{2}-\eta_{t}\langle\nabla f (\mathbf{x}_{t}),\mathbf{g}_{t}(\mathbf{x}_{t+\frac{1}{2}})-\nabla f( \mathbf{x}_{t})\rangle+\frac{L\eta_{t}^{2}}{2}\|\mathbf{g}_{t}(\mathbf{x}_{t+ \frac{1}{2}})\|^{2}.\]

Taking expectation, then plugging Lemmas 1 and 2 in, we have

\[\mathbb{E}\big{[}f(\mathbf{x}_{t+1})-f(\mathbf{x}_{t})\big{]} \leq-\bigg{(}\eta_{t}-\frac{3L\eta_{t}^{2}}{2}\bigg{)}\mathbb{E} \big{[}\|\nabla f(\mathbf{x}_{t})\|^{2}\big{]}+\frac{L\rho^{2}}{2}+L^{3}\eta_{ t}^{2}\rho^{2}+L\eta_{t}^{2}\sigma^{2}.\]

As the parameter selection ensures that \(\eta_{t}\equiv\eta=\frac{\eta_{0}}{\sqrt{T}}\leq\frac{2}{3L}\), it is possible to divide both sides with \(\eta\) and rearrange the terms to arrive at

\[\bigg{(}1-\frac{3L\eta}{2}\bigg{)}\mathbb{E}\big{[}\|\nabla f( \mathbf{x}_{t})\|^{2}\big{]} \leq\frac{\mathbb{E}\big{[}f(\mathbf{x}_{t})-f(\mathbf{x}_{t+1}) \big{]}}{\eta}+\frac{L\rho^{2}}{2\eta}+L^{3}\eta\rho^{2}+L\eta\sigma^{2}.\]

Summing over \(t\), we have

\[\bigg{(}1-\frac{3L\eta}{2}\bigg{)}\frac{1}{T}\sum_{t=0}^{T-1} \mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t})\|^{2}\big{]} \leq\frac{\mathbb{E}\big{[}f(\mathbf{x}_{0})-f(\mathbf{x}_{T}) \big{]}}{\eta T}+\frac{L\rho^{2}}{2\eta}+L^{3}\eta\rho^{2}+L\eta\sigma^{2}\] \[\overset{(a)}{\leq}\frac{f(\mathbf{x}_{0})-f^{*}}{\eta T}+\frac{L \rho^{2}}{2\eta}+L^{3}\eta\rho^{2}+L\eta\sigma^{2}\] \[=\frac{f(\mathbf{x}_{0})-f^{*}}{\eta_{0}\sqrt{T}}+\frac{L\rho_{0} ^{2}}{2\eta_{0}\sqrt{T}}+\frac{L^{3}\eta_{0}\rho_{0}^{2}}{T^{3/2}}+\frac{L \eta_{0}\sigma^{2}}{\sqrt{T}}\]

where (a) uses Assumption 1, and the last equation is obtained by plugging in the value of \(\rho\) and \(\eta\). This completes the proof to the first part.

For the second part of this theorem, we have that

\[\mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t}+\bm{\epsilon}_{t})\|^ {2}\big{]} =\mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t}+\bm{\epsilon}_{t})+ \nabla f(\mathbf{x}_{t})-\nabla f(\mathbf{x}_{t})\|^{2}\big{]}\] \[\leq 2\mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t}\|^{2}\big{]}+2 \mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t}+\bm{\epsilon}_{t})-\nabla f( \mathbf{x}_{t})\|^{2}\big{]}\] \[\leq 2\mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t}\|^{2}\big{]}+2L^{2 }\rho^{2}\] \[=2\mathbb{E}\big{[}\|\nabla f(\mathbf{x}_{t}\|^{2}\big{]}+\frac{2 L^{2}\rho_{0}^{2}}{T}.\]

Averaging over \(t\) completes the proof. 

### Proof of Theorem 2

Proof.: To bound the MSE, we first have that

\[\|\mathbf{d}_{t}-\nabla f(\mathbf{x}_{t})\|^{2}\] (9) \[=\|(1-\theta)\mathbf{d}_{t-1}+\theta\mathbf{g}_{t}(\mathbf{x}_{t })-(1-\theta)\nabla f(\mathbf{x}_{t})-\theta\nabla f(\mathbf{x}_{t})\|^{2}\] \[=(1-\theta)^{2}\|\mathbf{d}_{t-1}-\nabla f(\mathbf{x}_{t})\|^{2} +\theta^{2}\|\mathbf{g}_{t}(\mathbf{x}_{t})-\nabla f(\mathbf{x}_{t})\|^{2}\] \[\qquad\qquad\qquad+2\theta(1-\theta)\langle\mathbf{d}_{t-1}- \nabla f(\mathbf{x}_{t}),\mathbf{g}_{t}(\mathbf{x}_{t})-\nabla f(\mathbf{x}_{t })\rangle.\]

Now we cope with three terms in the right hind of (9) separately.

[MISSING_PAGE_FAIL:18]

Combining (14) with (15), we have

\[|\mathcal{L}_{t}(\mathbf{v}_{t})-\mathcal{L}_{t}(\nabla f(\mathbf{x}_{t}))|\leq \rho\|\mathbf{v}_{t}-\nabla f(\mathbf{x}_{t})\|\]

which further implies that

\[\mathbb{E}\big{[}|\mathcal{L}_{t}(\mathbf{v}_{t})-\mathcal{L}_{t}(\nabla f( \mathbf{x}_{t}))|\big{]}\leq\rho\mathbb{E}\big{[}\|\mathbf{v}_{t}-\nabla f( \mathbf{x}_{t})\|\big{]}\leq\rho\sqrt{\mathbb{E}\big{[}\|\mathbf{v}_{t}- \nabla f(\mathbf{x}_{t})\|^{2}\big{]}}.\]

The last inequality is because \((\mathbb{E}[a])^{2}\leq\mathbb{E}[a^{2}]\). This theorem can be proved by applying Assumption 3 for SAM and Lemma 2 for VaSSO.