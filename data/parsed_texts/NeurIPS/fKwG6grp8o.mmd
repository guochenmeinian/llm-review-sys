# Dynamics of Finite Width Kernel and Prediction Fluctuations in Mean Field Neural Networks

Blake Bordelon & Cengiz Pehlevan

John Paulson School of Engineering and Applied Sciences,

Center for Brain Science,

Kempner Institute for the Study of Natural & Artificial Intelligence,

Harvard University

Cambridge MA, 02138

blake_bordelon@g.harvard.edu, cpehlevan@g.harvard.edu

###### Abstract

We analyze the dynamics of finite width effects in wide but finite feature learning neural networks. Starting from a dynamical mean field theory description of infinite width deep neural network kernel and prediction dynamics, we provide a characterization of the \(\mathcal{O}(1/\sqrt{\text{width}})\) fluctuations of the DMFT order parameters over random initializations of the network weights. Our results, while perturbative in width, unlike prior analyses, are non-perturbative in the strength of feature learning. In the lazy limit of network training, all kernels are random but static in time and the prediction variance has a universal form. However, in the rich, feature learning regime, the fluctuations of the kernels and predictions are dynamically coupled with a variance that can be computed self-consistently. In two layer networks, we show how feature learning can dynamically reduce the variance of the final tangent kernel and final network predictions. We also show how initialization variance can slow down online learning in wide but finite networks. In deeper networks, kernel variance can dramatically accumulate through subsequent layers at large feature learning strengths, but feature learning continues to improve the signal-to-noise ratio of the feature kernels. In discrete time, we demonstrate that large learning rate phenomena such as edge of stability effects can be well captured by infinite width dynamics and that initialization variance can decrease dynamically. For CNNs trained on CIFAR-10, we empirically find significant corrections to both the bias and variance of network dynamics due to finite width.

## 1 Introduction

Learning dynamics of deep neural networks are challenging to analyze and understand theoretically, but recent progress has been made by studying the idealization of infinite-width networks. Two types of infinite-width limits have been especially fruitful. First, the kernel or lazy infinite-width limit, which arises in the standard or neural tangent kernel (NTK) parameterization, gives prediction dynamics which correspond to a linear model [1; 2; 3; 4; 5]. This limit is theoretically tractable but fails to capture adaptation of internal features in the neural network, which are thought to be crucial to the success of deep learning in practice. Alternatively, the mean field or \(\mu\)-parameterization allows feature learning at infinite width [6; 7; 8; 9].

With a set of well-defined infinite-width limits, prior theoretical works have analyzed finite networks in the NTK parameterization perturbatively, revealing that finite width both enhances the amount of feature evolution (which is still small in this limit) but also introduces variance in the kernels and the predictions over random initializations [10; 11; 12; 13; 14; 15]. Because of these competing effects, in some situations wider networks are better, and in others wider networks perform worse [16].

In this paper, we analyze finite-width network learning dynamics in the mean field parameterization. In this parameterization, wide networks are empirically observed to outperform narrow networks [7; 17; 18]. Our results and framework provide a methodology for reasoning about detrimental finite-size effects in such feature-learning neural networks. We show that observable averages involving kernels and predictions obey a well-defined power series in inverse width even in rich training regimes. We generally observe that the leading finite-size corrections to both the bias and variance components of the square loss are increased for narrower networks, and diminish performance. Further, we show that richer networks are closer to their corresponding infinite-width mean field limit. For simple tasks and architectures the leading \(\mathcal{O}(1/\text{width})\) corrections to the error can be descriptive, while for large sample size or more realistic tasks, higher order corrections appear to become relevant. Concretely, our contributions are listed below:

1. Starting from a dynamical mean field theory (DMFT) description of infinite-width nonlinear deep neural network training dynamics, we provide a complete recipe for computing fluctuation dynamics of DMFT order parameters over random network initializations during training. These include the variance of the training and test predictions and the \(\mathcal{O}(1/\text{width})\) variance of feature and gradient kernels throughout training.
2. We first solve these equations for the lazy limit, where no feature learning occurs, recovering a simple differential equation which describes how prediction variance evolves during learning.
3. We solve for variance in the rich feature learning regime in two-layer networks and deep linear networks. We show richer nonlinear dynamics improve the signal-to-noise ratio (SNR) of kernels and predictions, leading to closer agreement with infinite-width mean field behavior.
4. We analyze in a two-layer model why larger training set sizes in the overparameterized regime enhance finite-width effects and how richer training can reduce this effect.
5. We show that large learning rate effects such as edge-of-stability [19; 20; 21] dynamics can be well captured by infinite width theory, with finite size variance accurately predicted by our theory.
6. We test our predictions in Convolutional Neural Networks (CNNs) trained on CIFAR-10 [22]. We observe that wider networks and richly trained networks have lower logit variance as predicted. However, the timescale of training dynamics is significantly altered by finite width even after ensembling. We argue that this is due to a detrimental correction to the mean dynamical NTK.

### Related Works

Infinite-width networks at initialization converge to a Gaussian process with a covariance kernel that is computed with a layerwise recursion [23; 24; 25; 26; 13]. In the large but finite width limit, these kernels do not concentrate at each layer, but rather propagate finite-size corrections forward through the network [27; 28; 29; 30; 14]. During gradient-based training with the NTK parameterization, a hierarchy of differential equations have been utilized to compute small feature learning corrections to the kernel through training [10; 11; 12; 13]. However the higher order tensors required to compute the theory are initialization dependent, and the theory breaks down for sufficiently rich feature learning dynamics. Various works on Bayesian deep networks have also considered fluctuations and perturbations in the kernels at finite width during inference [31; 32]. Other relevant work in this domain are [33; 34; 35; 36; 37; 38; 39].

An alternative to standard/NTK parameterization is the mean field or \(\mu P\)-limit where features evolve even at infinite width [6; 7; 8; 9; 40; 41; 42]. Recent studies on two-layer mean field networks trained online with Gaussian data have revealed that finite networks have larger sensitivity to SGD noise [43; 44]. Here, we examine how finite-width neural networks are sensitive to initialization noise. Prior work has studied how the weight space distribution and predictions converge to mean field dynamics with a dynamical error \(\mathcal{O}(1/\sqrt{\text{width}})\)[40; 45], however in the deep case this requires a probability distribution over couplings between adjacent layers. Our analysis, by contrast, focuses on a function and kernel space picture which decouples interactions between layers at infinite width. A starting point for our present analysis of finite-width effects was a previous set of studies [9; 46] which identified the DMFT action corresponding to randomly initialized deep NNs which generates the distribution over kernel and network prediction dynamics. These prior works discuss the possibility of using a finite-size perturbation series but crucially failed to recognize the role of the network prediction fluctuations on the kernel fluctuations which are necessary to close the self-consistent equations in the rich regime. Using the mean field action to calculate a perturbation expansion around DMFT is a long celebrated technique to obtain finite size corrections in physics [47; 48; 49; 50] and has been utilized for random untrained recurrent networks [51; 52], and more recently to calculate variance of feature kernels \(\Phi^{\ell}\) at initialization \(t=0\) in deep MLPs or RNNs [53]. We extend these prior studies to the dynamics of training and to probe how feature learning alters finite size corrections.

## 2 Problem Setup

We consider wide neural networks where the number of neurons (or channels for a CNN) \(N\) in each layer is large. For a multi-layer perceptron (MLP), the network is defined as a map from input \(\bm{x}_{\mu}\in\mathbb{R}^{D}\) to hidden preactivations \(\bm{h}_{\mu}^{\ell}\in\mathbb{R}^{N}\) in layers \(\ell\in\{1,...,L\}\) and finally output \(f_{\mu}\)

\[f_{\mu}=\frac{1}{\gamma N}\bm{w}^{L}\cdot\phi(\bm{h}_{\mu}^{L})\,\quad\bm{h}_{\mu}^{\ell+1}=\frac{1}{ \sqrt{N}}\bm{W}^{\ell}\phi(\bm{h}_{\mu}^{\ell})\,\quad\bm{h}_{\mu}^{1}=\frac{1}{\sqrt{D}}\bm{W}^{0}\bm{x}_{ \mu},\] (1)

where \(\gamma\) is a scale factor that controls feature learning strength, with large \(\gamma\) leading to rich feature learning dynamics and the limit of small \(\gamma\to 0\) (or generally if \(\gamma\) scales as \(N^{-\alpha}\) for \(\alpha>0\) as \(N\to\infty\), NTK parameterization corresponds to \(\alpha=\frac{1}{2}\)) gives lazy learning where no features are learned [4; 7; 9]. The parameters \(\bm{\theta}=\{\bm{W}^{0},\bm{W}^{1},...,\bm{w}^{L}\}\) are optimized with gradient descent or gradient flow \(\frac{d}{dt}\bm{\theta}=-N\gamma^{2}\nabla_{\bm{\theta}}\mathcal{L}\) where \(\mathcal{L}=\mathbb{E}_{\bm{x}_{\mu}\in\mathcal{D}}\ \ell(f(\bm{x}_{\mu},\bm{\theta}),y_{\mu})\) is a loss computed over dataset \(\mathcal{D}=\{(\bm{x}_{1},y_{1}),(\bm{x}_{2},y_{2}),\ldots(\bm{x}_{P},y_{P})\}\). This parameterization and learning rate scaling ensures that \(\frac{d}{dt}f_{\mu}\sim\mathcal{O}_{N,\gamma}(1)\) and \(\frac{d}{dt}\bm{h}_{\mu}^{\ell}=\mathcal{O}_{N,\gamma}(\gamma)\) at initialization. This is equivalent to maximal update parameterization (\(\mu\)P)[8], which can be easily extended to other architectures including neural networks with trainable bias parameters as well as convolutional, recurrent, and attention layers [8; 9].

## 3 Review of Dynamical Mean Field Theory

The infinite-width training dynamics of feature learning neural networks was described by a DMFT in [9; 46]. We first review the DMFT's key concepts, before extending it to get insight into finite-widths. To arrive at the DMFT, one first notes that the training dynamics of such networks can be rewritten in terms of a collection of dynamical variables (or _order parameters_) \(\bm{q}=\text{Vec}\{f_{\mu}(t),\Phi_{\mu\nu}^{\ell}(t,s),G_{\mu\nu}^{\ell}(t,s ),...\}\)[9], which include feature and gradient kernels [9; 54]

\[\Phi_{\mu\nu}^{\ell}(t,s)\equiv\frac{1}{N}\phi(\bm{h}_{\mu}^{\ell}(t))\cdot \phi(\bm{h}_{\nu}^{\ell}(s))\,\quad G_{\mu\nu}^{\ell}(t,s)\equiv\frac{1}{N}\bm{g}_{\mu}^{\ell}(t)\cdot\bm{g }_{\nu}^{\ell}(s),\] (2)

where \(\bm{g}_{\mu}^{\ell}(t)=\gamma N\frac{\partial f_{\mu}(t)}{\partial\bm{h}_{\mu }^{\ell}(t)}\) are the back-propagated gradient signals. Further, for width-\(N\) networks the distribution of these dynamical variables across weight initializations (from a Gaussian distribution \(\bm{\theta}\sim\mathcal{N}(0,\mathbf{I})\)) is given by \(p(\bm{q})\propto\exp{(NS(\bm{q}))}\), where the action \(S(\bm{q})\) contains interactions between neuron activations and the kernels at each layer [9].

The DMFT introduced in [9] arises in the \(N\to\infty\) limit when \(p(\bm{q})\) is strongly peaked around the saddle point \(\bm{q}_{\infty}\) where \(\frac{\partial S}{\partial\bm{q}}|_{\bm{q}_{\infty}}=0\). Analysis of the saddle point equations reveal that the training dynamics of the neural network can be alternatively described by a stochastic process. A key feature of this process is that it describes the training time evolution of the distribution of neuron pre-activations in each layer (informally the histogram of the elements of \(\bm{h}_{\mu}^{\ell}(t)\)) where each neuron's pre-activation behaves as an i.i.d. draw from this _single-site_ stochastic process. We denote these random processes by \(h_{\mu}^{\ell}(t)\). Kernels in (2) are now computed as _averages_ over these infinite-width single site processes \(\Phi_{\mu\nu}^{\ell}(t,s)=\left\langle\phi(h_{\mu}^{\ell}(t))\phi(h_{\nu}^{ \ell}(s))\right\rangle\), \(G_{\mu\nu}^{\ell}(t,s)=\left\langle g_{\mu}^{\ell}(t)g_{\nu}^{\ell}(s)\right\rangle\), where averages arise from the \(N\to\infty\) limit of the dot products in (2). DMFT also provides a set of self-consistent equations that describe the complete statistics of these random processes, which depend on the kernels, as well as other quantities. To make our notation and terminology clearer for a machine learning audience, we provide Table 1 for a definition of the physics terminology in machine learning language.

## 4 Dynamical Fluctuations Around Mean Field Theory

We are interested in going beyond the infinite-width limit to study more realistic finite-width networks. In this regime, the order parameters \(\bm{q}\) fluctuate in a \(\mathcal{O}(1/\sqrt{N})\) neighborhood of \(\bm{q}_{\infty}\)[55; 51; 53; 46].

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Order params. \(\bm{q}\) & Action \(S(\bm{q})\) & Propagator \(\bm{\Sigma}\) & Single Site Density \\ \hline Concentrating variables & \(\bm{q}\)’s log-density & Asymptotic Covariance & Neuron Marginals \\ \hline \end{tabular}
\end{table}
Table 1: Relationship between the physics and ML terminology for the central objects in this paper. The \(\bm{q}\) which concentrate at infinite width, but fluctuate at finite width \(N\). This paper is primarily interested in \(\bm{\Sigma}\), the asymptotic covariance of the order parameters.

Statistics of these fluctuations can be calculated from a general cumulant expansion (see App. D) [55, 56, 51]. We will focus on the leading-order corrections to the infinite-width limit in this expansion.

**Proposition 1**: _The finite-width \(N\) average of observable \(O(\bm{q})\) across initializations, which we denote by \(\left\langle O(\bm{q})\right\rangle_{N}\), admits an expansion of the form whose leading terms are_

\[\left\langle O(\bm{q})\right\rangle_{N}=\frac{\int d\bm{q}\exp\left(NS[\bm{q}] \right)O(\bm{q})}{\int d\bm{q}\exp\left(NS[\bm{q}]\right)}=\left\langle O(\bm {q})\right\rangle_{\infty}+N\left[\left\langle V(\bm{q})O(\bm{q})\right\rangle _{\infty}-\left\langle V(\bm{q})\right\rangle_{\infty}\left\langle O(\bm{q}) \right\rangle_{\infty}\right]+...,\] (3)

_where \(\left\langle\right\rangle_{\infty}\) denotes an average over the Gaussian distribution \(\bm{q}\sim\mathcal{N}\left(\bm{q}_{\infty},-\frac{1}{N}\left(\nabla_{\bm{q}} ^{2}S[\bm{q}_{\infty}]\right)^{-1}\right)\) and the function \(V(\bm{q})\equiv S(\bm{q})-S(\bm{q}_{\infty})-\frac{1}{2}(\bm{q}-\bm{q}_{ \infty})^{\top}\nabla_{\bm{q}}^{2}S(\bm{q}_{\infty})(\bm{q}-\bm{q}_{\infty})\) contains cubic and higher terms in the Taylor expansion of \(S\) around \(\bm{q}_{\infty}\). The terms shown include all the leading and sub-leading terms in the series in powers of \(1/N\). The terms in ellipses are at least \(\mathcal{O}(N^{-1})\) suppressed compared to the terms provided._

The proof of this statement is given in App. D. The central object to characterize finite size effects is the unperturbed covariance (the _propagator_): \(\bm{\Sigma}=-\left[\nabla^{2}S(\bm{q}_{\infty})\right]^{-1}\). This object can be shown to capture leading order fluctuation statistics \(\left\langle\left(\bm{q}-\bm{q}_{\infty}\right)\left(\bm{q}-\bm{q}_{\infty} \right)^{\top}\right\rangle_{N}=\frac{1}{N}\bm{\Sigma}+\mathcal{O}(N^{-2})\) (App. D.1), which can be used to reason about, for example, expected square error over random initializations. Correction terms at finite width may give a possible explanation of the superior performance of wide networks at fixed \(\gamma\)[7, 17, 18]. To calculate such corrections, in App. E, we provide a complete description of Hessian \(\nabla_{\bm{q}}^{2}S(\bm{q})\) and its inverse (the propagator) for a depth-\(L\) network. This description constitutes one of our main results. The resulting expressions are lengthy and are left to App. E. Here, we discuss them at a high level. Conceptually there are two primary ingredients for obtaining the full propagator:

* Hessian sub-blocks \(\kappa\) which describe the _uncoupled variances_ of the kernels, such as \[\kappa_{\mu\nu\alpha\beta}^{\Phi^{\ell}}(t,s,t^{\prime},s^{\prime})\equiv \left\langle\phi(h_{\mu}^{\ell}(t))\phi(h_{\nu}^{\ell}(s))\phi(h_{\alpha}^{ \ell}(t^{\prime}))\phi(h_{\beta}^{\ell}(s^{\prime}))\right\rangle-\Phi_{\mu \nu}^{\ell}(t,s)\Phi_{\alpha\beta}^{\ell}(t^{\prime},s^{\prime})\] (4) Similar terms also appear in other studies on finite width Bayesian inference [13, 31, 32] and in studies on kernel variance at initialization [27, 14, 29, 53].
* Blocks which capture the _sensitivity_ of field averages to perturbations of order parameters, such as \[D_{\mu\nu\alpha\beta}^{\Phi^{\ell}q^{\ell-1}}(t,s,t^{\prime},s^{\prime})\equiv \frac{\partial\left\langle\phi(h_{\mu}^{\ell}(t))\phi(h_{\nu}^{\ell}(s)) \right\rangle}{\partial\Phi_{\alpha\beta}^{\ell-1}(t^{\prime},s^{\prime})}\,\quad D_{\mu\nu \alpha}^{G^{\prime}\Delta}(t,s,t^{\prime})\equiv\frac{\partial\left\langle g_{ \mu}^{\ell}(t)g_{\nu}^{\ell}(s)\right\rangle}{\partial\Delta_{\alpha}(t^{ \prime})},\] (5) where \(\Delta_{\mu}(t)=-\frac{\partial\ell(f_{\mu},g_{\mu})}{\partial f_{\mu}}|_{f_{ \mu}(t)}\) are error signal for each data point.

Abstractly, we can consider the uncoupled variances \(\kappa\) as "sources" of finite-width noise for each order parameter and the \(\bm{D}\) blocks as summarizing a directed causal graph which captures how this noise propagates in the network (through layers and network predictions). In Figure 1, we illustrate this graph showing directed lines that represent causal influences of order parameters on fields and vice versa. For instance, if \(\Phi^{\ell}\) were perturbed, \(D^{\Phi^{\ell+1},\Phi^{\ell}}\) would quantify the resulting perturbation to \(\Phi^{\ell+1}\) through the fields \(h^{\ell+1}\).

In App. E, we calculate \(\bm{\kappa}\) and \(\bm{D}\) tensors, and show how to use them to calculate the propagator. As an example of our results:

**Proposition 2**: _Partition \(\bm{q}\) into primal \(\bm{q}_{1}=\text{Vec}\{f_{\mu}(t),\Phi_{\mu\nu}^{\ell}(t,s)...\}\) and conjugate variables \(\bm{q}_{2}=\text{Vec}\{\hat{f}_{\mu}(t),\hat{\Phi}_{\mu\nu}^{\ell}(t,s)...\}\). Let \(\bm{\kappa}=\frac{\partial^{2}}{\partial\bm{q}_{2}\partial\bm{q}_{2}^{2}}S[\bm{ q}_{1},\bm{q}_{2}]\) and \(\bm{D}=\frac{\partial^{2}}{\partial\bm{q}_{2}\partial\bm{q}_{1}^{2}}S[\bm{q}_{1},\bm{q}_{2}]\), then the propagator for \(\bm{q}_{1}\) has the form \(\bm{\Sigma}_{\bm{q}_{1}}=\bm{D}^{-1}\bm{\kappa}\left[\bm{D}^{-1}\right]^{\top}\) (App E). The variables \(\bm{q}_{1}\) are related to network observables, while conjugates \(\bm{q}_{2}\) arise as Lagrange multipliers in the DMFT calculation. From the propagator \(\bm{\Sigma}_{\bm{q}_{1}}\) we can read off the variance of network observables such as \(N\text{Var}(f_{\mu})\sim\Sigma_{f_{\mu}}\)._

Figure 1: The directed causal graph between DMFT order parameters (blue) and fields (green) defines the \(D\) tensors of our theory. Each arrow represents a causal dependence. \(K\) denotes the NTK.

The necessary order parameters for calculating the fluctuations are obtained by solving the DMFT using numerical methods introduced in [9]. We provide a pseudocode for this procedure in App. F. We proceed to solve the equations defining \(\bm{\Sigma}\) in special cases which are illuminating and numerically feasible including lazy training, two layer networks and deep linear NNs.

## 5 Lazy Training Limit

To gain some initial intuition about why kernel fluctuations alter learning dynamics, we first analyze the static kernel limit \(\gamma\to 0\) where features are frozen. To prevent divergence of the network in this limit, we use a background subtracted function \(\tilde{f}(\bm{x},\bm{\theta})=f(\bm{x},\bm{\theta})-f(\bm{x},\bm{\theta}_{0})\) which is identically zero at initialization [4]. For mean square error, the \(N\rightarrow\infty\) and \(\gamma\to 0\) limit is governed by \(\frac{\partial\tilde{f}(\bm{x})}{\partial t}=\mathbb{E}_{\bm{x}^{\prime} \sim\mathcal{D}}\Delta(\bm{x}^{\prime})K(\bm{x},\bm{x}^{\prime})\) with \(\Delta(\bm{x})=y(\bm{x})-\tilde{f}(\bm{x})\) (for MSE) and \(K\) is the static (finite width and random) NTK. The finite-\(N\) initial covariance of the NTK has been analyzed in prior works [27; 13; 14], which reveal a dependence on depth and nonlinearity. Since the NTK is static in the \(\gamma\to 0\) limit, it has constant initialization variance through training. Further, all sensitivity blocks of the Hessian involving the kernels and the prediction errors \(\bm{\Delta}\) (such as the \(D^{\Phi^{\prime},\Delta}\)) vanish. We represent the covariance of the NTK as \(\kappa(\bm{x}_{1},\bm{x}_{2},\bm{x}_{2},\bm{x}_{3})=N\text{Cov}(K(\bm{x}_{1}, \bm{x}_{2}),K(\bm{x}_{3},\bm{x}_{4}))\). To identify the dynamics of the error \(\bm{\Delta}\) covariance, we relate \(K\), the finite width NTK, to \(K_{\infty}\) which is the deterministic infinite width NTK \(K_{\infty}\). We consider the eigendecomposition of the infinite-width NTK \(K_{\infty}(\bm{x},\bm{x}^{\prime})=\sum_{k}\lambda_{k}\psi_{k}(\bm{x})\psi_{k} (\bm{x}^{\prime})\) with respect to the training distribution \(\mathcal{D}\), and decompose \(\kappa\) in this basis.

\[\kappa_{k\ell mn}=\langle\kappa(\bm{x}_{1},\bm{x}_{2},\bm{x}_{3},\bm{x}_{4}) \psi_{k}(\bm{x}_{1})\psi_{\ell}(\bm{x}_{2})\psi_{n}(\bm{x}_{3})\psi_{m}(\bm{x }_{4})\rangle_{\bm{x}_{1},\bm{x}_{2},\bm{x}_{3},\bm{x}_{4}\sim\mathcal{D}}\,,\] (6)

where averages are computed over the training distribution \(\mathcal{D}\).

**Proposition 3**: _For MSE loss, the prediction error covariance \(\bm{\Sigma}^{\Delta}(t,s)=N\text{Cov}_{0}(\bm{\Delta}(t),\bm{\Delta}(s))\) satisfies a differential equation (App. H)_

\[\left(\frac{\partial}{\partial t}+\lambda_{k}\right)\left(\frac{\partial}{ \partial s}+\lambda_{\ell}\right)\Sigma^{\Delta}_{k\ell}(t,s)=\sum_{nm}\kappa_ {km\ell n}\Delta^{\infty}_{m}(t)\Delta^{\infty}_{n}(s),\] (7)

_where \(\Delta^{\infty}_{k}(t)\equiv\exp\left(-\lambda_{k}t\right)\left\langle\psi_{k }(\bm{x})y(\bm{x})\right\rangle_{\bm{x}}\) are the errors at infinite width for eigenmode \(k\)._

An example verifying these dynamics is provided in Figure 2. In the case where the target is an eigenfunction \(y=\psi_{k^{*}}\), the covariance has the form \(\Sigma^{\Delta}_{k\ell}(t,s)=\kappa_{k\ell k^{*}k^{*}}\frac{\exp(-\lambda_{k^{* }}(t+s))}{(\lambda_{k}-\lambda_{k^{*}})(\lambda_{k}-\lambda_{k^{*}})}\). If the kernel is rank one with eigenvalue \(\lambda\), then the dynamics have the simple form \(\Sigma^{\Delta}(t,s)=\kappa y^{2}\ t\ s\ e^{-\lambda(t+s)}\). We note that similar terms appear in the prediction dynamics obtained by truncating the Neural Tangent Hierarchy [10; 11], however those dynamics concerned

Figure 2: We show the accuracy of the lazy-limit ODE in equation (where) compared to a two-layer finite width \(N=100\) ReLU network trained with \(\gamma=0.05\) on \(P=10\) random training data points. (a) The average dynamics over an ensemble of \(E=500\) networks (solid colors) compared to the infinite width predictions (dashed black). (b) The predicted finite size variance for each eigenmode of the error \(\Delta_{k}(t)=\bm{\Delta}(t)\cdot\bm{\phi}_{k}\). These are not ordered simply by magnitude of eigenvalues or the target projections \(y_{k}=\bm{y}\cdot\bm{\phi}_{k}\), but rather depend on all eigenvalue gaps \(\lambda_{k}-\lambda_{\ell}\) for \(k\neq\ell\) and also the \(\kappa_{k\ell nm}\) tensor. (c) The total variance for all training points \(N\sum_{\mu}\text{Var}\Delta_{\mu}(t)=N\sum_{k}\text{Var}\Delta_{k}(t)\) is also well predicted by the DMFT propagator equations.

corrections rather than from initialization variance (App. H.1). Corrections to the mean \(\langle\Delta\rangle\) are analyzed in App. H.2. We find that the variance and mean correction dynamics involves non-trivial coupling across eigendirections with a mixture of exponentials with timescales \(\{\lambda_{k}^{-1}\}\).

## 6 Rich Regime in Two-Layer Networks

In this section, we analyze how feature learning alters the variance through training. We show a denoising effect where the signal to noise ratios of the order parameters improve with feature learning.

### Kernel and Error Coupled Fluctuations on Single Training Example

In the rich regime, the kernel evolves over time but inherits fluctuations from the training errors \(\bm{\Delta}\). To gain insight, we first study a simplified setting where the data distribution is a single training example \(\bm{x}\) and single test point \(\bm{x_{\star}}\) in a two layer network. We will track \(\Delta(t)=y-f(\bm{x},t)\) and the test prediction \(f_{\star}(t)=f(\bm{x}_{\star},t)\). To identify the dynamics of these predictions we need the NTK \(K(t)\) on the train point, as well as the train-test NTK \(K_{\star}(t)\). In this case, all order parameters can be viewed as scalar functions of a single time index (unlike the deep network case, see App. E).

**Proposition 4**: _Computing the Hessian of the DMFT action and inverting (App. I), we obtain the following covariance for \(\bm{q}_{1}=\text{Vec}\{\Delta(t),f_{\star}(t),K(t),K_{\star}(t)\}_{t\in\mathbb{ R}_{+}}\)._

\[\bm{\Sigma}_{\bm{q}_{1}}=\begin{bmatrix}\mathbf{I}+\bm{\Theta}_{K}&0&\bm{ \Theta}_{\Delta}&0\\ -\bm{\Theta}_{K},&\mathbf{I}&0&-\bm{\Theta}_{\Delta}\\ -\bm{D}&0&\mathbf{I}&0\\ -\bm{D}_{\star}&0&0&\mathbf{I}\end{bmatrix}^{-1}\begin{bmatrix}0&0&0&0\\ 0&0&0&0\\ 0&0&\bm{\kappa}&\bm{\kappa}_{\star}^{\top}\\ 0&0&\bm{\kappa}_{\star}&\bm{\kappa}_{\star\star}\end{bmatrix}\begin{bmatrix} \mathbf{I}+\bm{\Theta}_{K}&0&\bm{\Theta}_{\Delta}&0\\ -\bm{\Theta}_{K_{\star}}&\mathbf{I}&0&-\bm{\Theta}_{\Delta}\\ -\bm{D}_{\star}&0&\mathbf{I}&0\\ -\bm{D}_{\star}&0&0&\mathbf{I}\end{bmatrix}^{-1\top},\] (8)

_where \([\bm{\Theta}_{K}](t,s)=\Theta(t-s)K(s)\), \([\bm{\Theta}_{\Delta}](t,s)=\Theta(t-s)\Delta(s)\) are Heaviside step functions and \(D(t,s)=\left\langle\frac{\partial}{\partial\Delta(s)}(\phi(h(t))^{2}+g(t)^{2})\right\rangle\) and \(D_{\star}(t,s)=\left\langle\frac{\partial}{\partial\Delta(s)}(\phi(h(t))\phi( h_{\star}(t))+g(t)g_{\star}(t))\right\rangle\)_

Figure 3: An ensemble of \(E=1000\) two layer \(N=256\) tanh networks trained on a single training point. Dashed black lines are DMFT predictions. (a) The square deviation from the infinite width DMFT scales as \(\mathcal{O}(1/N)\) for all order parameters. (b) The ensemble average NTK \(\langle K(t)\rangle\) (solid colors) and (c) ensemble average test point predictions \(f_{\star}(t)\) for a point with \(\frac{\bm{\kappa}-\bm{\kappa}_{s}}{D}=0.5\) closely follow the infinite width predictions (dashed black). (d) The variance (estimated the ensemble) of the train error \(\Delta(t)=y-f(t)\) initially increases and then decreases as the training point is fit. (e) The variance of \(f_{\star}\) increases with time but decreases with \(\gamma\). (f) The variance of the NTK during feature learning experiences a transient increase before decreasing to a lower value.

quantify sensitivity of the kernel to perturbations in the error signal \(\Delta(s)\). Lastly \(\kappa\) and \(\kappa_{\star}\) are the uncoupled variances of \(K(t)\) and \(K_{\star\star}(t)\) and \(\kappa_{\star}\) is the uncoupled covariance of \(K(t),K_{\star}(t)\)._

In Fig. 3, we plot the resulting theory (diagonal blocks of \(\bm{\Sigma}_{\bm{q}_{1}}\) from Equation 8) for two layer neural networks. As predicted by theory, all average squared deviations from the infinite width DMFT scale as \(\mathcal{O}(N^{-1})\). Similarly, the average kernels \(\left\langle K\right\rangle\) and test predictions \(\left\langle f_{\star}\right\rangle\) change by a larger amount for larger \(\gamma\) (equation (79)). The experimental variances also match the theory quite accurately. The variance of the train error \(\Delta(t)\) peaks earlier and at a lower value for richer training, but all variances go to zero at late time as the model approaches the interpolation condition \(\Delta=0\). As \(\gamma\to 0\) the curve approaches \(N\)\(\text{Var}(\Delta(t))\sim\kappa\ y^{2}\ t^{2}\ e^{-2t}\), where \(\kappa\) is the initial NTK variance (see Section 5). While the train prediction variance goes to zero, the test point prediction does not, with richer networks reaching a lower asymptotic variance. We suspect this dynamical effect could explain lower variance observed in feature learning networks compared to lazy networks [7; 18]. In Fig. A.1, we show that the reduction in variance is not due to a reduction in the uncoupled variance \(\kappa(t,s)\), which increases in \(\gamma\). Rather the reduction in variance is driven by the coupling of perturbations across time.

### Offline Training with Multiple Samples or Online Training in High Dimension

In this section we go beyond the single sample equations of the prior section and explore training with multiple \(P\) examples. In this case, we have training errors \(\{\Delta_{\mu}(t)\}_{\mu=1}^{P}\) and multiple kernel entries \(K_{\mu\nu}(t)\) (App. E). Each of the errors \(\Delta_{\mu}(t)\) receives a \(\mathcal{O}(N^{-1/2})\) fluctuation, the training error \(\sum_{\mu}\left\langle\Delta_{\mu}^{2}\right\rangle\) has an additional variance on the order of \(\mathcal{O}(\frac{P}{N})\). In the case of two-layer linear

Figure 4: Large input dimension or multiple samples amplify finite size effects in a simple two layer model with unstructured data. Black dashed lines are theory. (a) The variance of offline learning with \(P\) training examples in a two layer linear network. (b) The leading perturbative approximation to the train error breaks down when samples \(P\) becomes comparable to \(N\). (c)-(d) Richer training reduces variance. (e)-(f) Online learning in a depth 2 linear network has identical dynamics and finite width fluctuations, but with predictor variance \(\sim D/N\) for input dimension \(D\) (Appendix K).

networks trained on whitened data (\(\frac{1}{D}\bm{x}_{\mu}\cdot\bm{x}_{\nu}=\delta_{\mu\nu}\)), the equations for the propagator simplify and one can separately solve for the variance of \(\bm{\Delta}(t)\in\mathbb{R}^{P}\) along signal direction \(\bm{y}\in\mathbb{R}^{P}\) and along each of the \(P-1\) orthogonal directions (App. J). At infinite width, the task-orthogonal component \(\bm{\Delta}_{\perp}\) vanishes and only the signal dimension \(\Delta_{y}(t)\) evolves in time with differential equation [9; 46]

\[\frac{d}{dt}\Delta_{y}(t)=2\sqrt{1+\gamma^{2}(y-\Delta_{y}(t))^{2}}\;\Delta_{y }(t)\;,\;\bm{\Delta}_{\perp}(t)=0.\] (9)

However, at finite width, both the \(\Delta_{y}(t)\) and the \(P-1\) orthogonal variables \(\bm{\Delta}_{\perp}\) inherit initialization variance, which we represent as \(\Sigma_{\Delta_{y}}(t,s)\) and \(\Sigma_{\perp}(t,s)\). In Fig. 4 (a)-(b) we show this approximate solution \(\big{\langle}|\bm{\Delta}(t)|^{2}\big{\rangle}\sim\Delta_{y}(t)^{2}+\frac{2} {N}\Delta_{y}^{1}(t)\Delta_{y}(t)+\frac{1}{N}\Sigma_{\Delta_{y}}(t,t)+\frac{(P -1)}{N}\Sigma_{\perp}(t,t)+\mathcal{O}(N^{-2})\) across varying \(\gamma\) and varying \(P\) (see Appendix J for \(\Sigma_{\Delta_{y}}\) and \(\Sigma_{\perp}\) formulas). We see that variance of train point predictions \(f_{\mu}(t)\) increases with the total number of points despite the signal of the target vector \(\sum_{\mu}y_{\mu}^{2}\) being fixed. In this model, the bias correction \(\frac{2}{N}\Delta_{y}^{1}(t)\Delta_{y}(t)\) is always \(\mathcal{O}(1/N)\) but the variance correction is \(\mathcal{O}(P/N)\). The fluctuations along the \(P-1\) orthogonal directions begin to dominate the variance at large \(P\). Fig. 4 (b) shows that as \(P\) increases, the leading order approximation breaks down as higher order terms become relevant. Analysis for online training reveals identical fluctuation statistics, but with variance that scales as \(\sim D/N\) (Appendix K) as we verify in Figure 4 (e)-(f).

## 7 Deep Networks

In networks deeper than two layers, the DMFT propagator has complicated dependence on non-diagonal (in time) entries of the feature kernels (see App. E). This leads to Hessian blocks with four time and four sample indices such as \(D_{\mu\nu\alpha\beta}^{\Phi^{\ell}}(t,s,t^{\prime},s^{\prime})=\frac{\partial }{\partial\Phi_{\alpha\beta}^{\ell-1}(t^{\prime},s^{\prime})}\left\langle \phi(h_{\mu}^{\ell}(t))\phi(h_{\nu}^{\ell}(s))\right\rangle\), rendering any numerical calculation challenging. However, in deep linear networks trained on whitened data, we can exploit the symmetry in sample space and the Gaussianity of reactivation features to exactly compute derivatives without Monte Carlo sampling as we discuss in App. L. An example set of results for a depth \(4\) network is provided in Fig. 5. The variance for the feature kernels \(H^{\ell}\) accumulate finite size variance by layer along the forward pass and the gradient kernels \(G^{\ell}\) accumulate variance on the backward pass. The SNR of the kernels \(\frac{\langle H\rangle^{2}}{N\mathtt{Var}(H)}\) improves with feature learning, suggesting that richer networks will be better modeled by their mean field limits. Examples of the off-diagonal correlations obtained from the propagator are provided in App. Fig. A.3.

## 8 Variance can be Small Near Edge of Stability

In this section, we move beyond the gradient flow formalism and ask what large step sizes do to finite size effects. Recent studies have identified that networks trained at large learning rates can be qualitatively different than networks in the gradient flow regime, including the catapult [57] and edge of stability (EOS) phenomena [19; 20; 21]. In these settings, the kernel undergoes an initial scale

Figure 5: Depth 4 linear network with single training point. Black dashed lines are theory. (a) The variance of the training error along the task relevant subspace. We see that unlike the two layer model, more feature learning can lead to larger peaks in the finite size variance. (b) The variance of the NTK in the task relevant subspace. When properly normalized against the square of the mean \(\left\langle K(t)\right\rangle^{2}\), the final NTK variance decreases with feature learning. (c) The gap in feature kernel variance across different layers of the network is amplified by feature learning strength \(\gamma\).

growth before exhibiting either a recovery or a clipping effect. In this section, we explore whether these dynamics are highly sensitive to initialization variance or if finite networks are well captured by mean field theory. Following [57], we consider two layer networks trained on a single example \(|\bm{x}|^{2}=D\) and \(y=1\). We use learning rate \(\eta\) and feature learning strength \(\gamma\). The infinite width mean field equations for the prediction \(f_{t}\) and the kernel \(K_{t}\) are (App. M)

\[f_{t+1}=f_{t}+\eta K_{i}\Delta_{t}+\eta^{2}\gamma^{2}f_{t}\Delta_{t}^{2}\,,\ K_{t+1}=K_{t}+4\eta\gamma^{2}f_{t} \Delta_{t}+\eta^{2}\gamma^{2}\Delta_{t}^{2}K_{t}.\] (10)

For small \(\eta\), the equations are well approximated by the gradient flow limit and for small \(\gamma\) corresponds to a discrete time linear model. For large \(\eta\gamma>1\), the kernel \(K\) progressively sharpens (increases in scale) until it reaches \(2/\eta\) and then oscillates around this value. It may be expected that near the EOS, the large oscillations in the kernels and predictions could lead to amplified finite size effects, however, we show in Fig. 6 that the leading order propagator elements decrease even after reaching the EOS threshold, indicating _reduced_ disagreement between finite and infinite width dynamics.

## 9 Finite Width Alters Bias, Training Rate, and Variance in Realistic Tasks

To analyze the effect of finite width on neural network dynamics during realistic learning tasks, we studied a vanilla depth-\(6\) ReLU CNN trained on CIFAR-10 (experimental details in App. B, G.2) In Fig. 7, we train an ensemble of \(E=8\) independently initialized CNNs of each width \(N\). Wider networks not only have better performance for a single model (solid), but also have lower bias (dashed), measured with ensemble averaging of the logits. Because of faster convergence of wide networks, we observe wider networks have higher variance, but if we plot variance at fixed ensembled training accuracy, wider networks have consistently lower variance (Fig. 7(d)).

We next seek an explanation for why wider networks after ensembling trains at a faster _rate_. Theoretically, this can be rationalized by a finite-width alteration to the ensemble averaged NTK, which governs the convergence timescale of the ensembled predictions (App. G.1). Our analysis in App. G.1 suggests that the rate of convergence receives a finite size correction with leading correction \(\mathcal{O}(N^{-1})\) G.2. To test this hypothesis, we fit the ensemble training loss curve to exponential function \(\mathcal{L}\approx C\exp{(-R_{N}t)}\) where \(C\) is a constant. We plot the fit \(R_{N}\) as a function of \(N^{-1}\) result in Fig. 7(e). For large \(N\), we see the leading behavior is linear in \(N^{-1}\), but begins to deviate at small \(N\) as a quadratic function of \(N^{-1}\), suggesting that second order effects become relevant around \(N\lesssim 100\).

In App. Fig. A.4, we train a smaller subset of CIFAR-10 where we find that \(R_{N}\) is well approximated by a \(\mathcal{O}(N^{-1})\) correction, consistent with the idea that higher sample size drives the dynamics out of the leading order picture. We also analyze the effect of \(\gamma\) on variance in this task. In App. Fig. A.5, we train \(N=64\) models with varying \(\gamma\). Increased \(\gamma\) reduces variance of the logits and alters the representation (measured with kernel-task alignment), the training and test accuracy are roughly insensitive to the richness \(\gamma\) in the range we considered.

## 10 Discussion

We studied the leading order fluctuations of kernels and predictions in mean field neural networks. Feature learning dynamics can reduce undesirable finite size variance, making finite networks order

Figure 6: Edge of stability effects do not imply deviations from infinite width behavior. Black dashed lines are theory. (a) The average kernel over an ensemble of several \(N=500\) NNs (solid color). For small \(\gamma\), the kernel reaches its asymptote before hitting the edge of stability. For large \(\gamma\), the kernel increases and then oscillates around \(2/\eta\). (b)-(c) Remarkably variance due to finite size can _reduce_ during training (for \(\gamma\) smaller and larger than the critical value \(\sim 1/\eta\)), showing that infinite width DMFT can be predictive of finite NNs trained with large learning rate.

parameters closer to the infinite width limit. In several toy models, we revealed some interesting connections between the influence of feature learning, depth, sample size, and large learning rate and the variance of various DMFT order parameters. Lastly, in realistic tasks, we illustrated that bias corrections can be significant as rates of learning can be modified by width. Though our full set of equations for the leading finite size fluctuations are quite general in terms of network architecture and data structure, they are only derived at the level of rigor of physics rather than a formally rigorous proof which would need several additional assumptions to make the perturbation expansion properly defined. Further, the leading terms in our perturbation series involving only \(\bm{\Sigma}\) does not capture the complete finite size distribution defined in Eq. (3), especially as the sample size becomes comparable to the width. It would be interesting to see if proportional limits of the rich training regime where samples and width scale linearly can be examined dynamically [58]. Future work could explore in greater detail the higher order contributions from averages involving powers of \(V(\bm{q})\) by examining cubic and higher derivatives of \(S\) in Eq. (3). It could also be worth examining in future work how finite size impacts other biologically plausible learning rules, where the effective NTK can have asymmetric (over sample index) fluctuations [46]. Also of interest would be computing the finite width effects in other types of architectures, including residual networks with various branch scalings [59, 60]. Further, even though we expect our perturbative expressions to give a precise asymptotic description of finite networks in mean field/\(\mu\)P, the resulting expressions are not realistically computable in deep networks trained on large dataset size \(P\) for long times \(T\) since the number of Hessian entries scales as \(\mathcal{O}(T^{4}P^{4})\) and a matrix of this size must be stored in memory and inverted in the general case. Future work could explore solveable special cases such as high dimensional limits.

#### Code Availability

Code to reproduce the experiments in this paper is provided at https://github.com/Pehlevan-Group/dmft_fluctuations. Details about numerical methods and computational implementation can be found in Appendices F and N.

Figure 7: Depth \(6\) CNN trained on CIFAR-10 for different widths \(N\) with richness \(\gamma=0.2\), \(E=8\) ensembles. (a)-(b) For this range of widths, we find that smaller networks perform worse in train and test error, not only in terms of the single models (solid) but also in terms of bias (dashed). The delayed training of ensembled finite width models indicates that the correction to the mean order parameters (App. G) is non-negligible. (c) Alignment of the average kernel to test labels is also not conserved across width. (d) The ratio of the test MSE for a single model to the ensembled logit MSE. (e) The fitted rate \(R_{N}\) of training width \(N\) models as a function of \(N^{-1}\). We rescale the time axis by \(R_{N}\) to allow for a fair comparison of prediction variance for networks at comparable performance levels. (f) In rescaled time, ensembled network training losses (dashed) are coincident.

### Acknowledgements

CP is supported by NSF Award DMS-2134157, NSF CAREER Award IIS-2239780, and a Sloan Research Fellowship. BB is supported by a Google PhD research fellowship and NSF Award DMS-2134157. This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence. The computations in this paper were run on the FASRC cluster supported by the FAS Division of Science Research Computing Group at Harvard University. BB thanks Alex Atanasov, Jacob Zavatone-Veth for their comments on this manuscript and Boris Hanin, Greg Yang, Mufan Bill Li and Jeremy Cohen for helpful discussions.

## References

* [1] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, _Advances in Neural Information Processing Systems_, volume 31, pages 8571-8580. Curran Associates, Inc., 2018.
* [2] Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On exact computation with an infinitely wide neural net. _Advances in Neural Information Processing Systems_, 32, 2019.
* [3] Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models under gradient descent. _Advances in neural information processing systems_, 32, 2019.
* [4] Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming. _Advances in Neural Information Processing Systems_, 32, 2019.
* [5] Greg Yang and Etai Littwin. Tensor programs iib: Architectural universality of neural tangent kernel training dynamics. In _International Conference on Machine Learning_, pages 11762-11772. PMLR, 2021.
* [6] Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit. In _Conference on Learning Theory_, pages 2388-2464. PMLR, 2019.
* [7] Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy training in deep neural networks. _Journal of Statistical Mechanics: Theory and Experiment_, 2020(11):113301, 2020.
* [8] Greg Yang and Edward J Hu. Tensor programs iv: Feature learning in infinite-width neural networks. In _International Conference on Machine Learning_, pages 11727-11737. PMLR, 2021.
* [9] Blake Bordelon and Cengiz Pehlevan. Self-consistent dynamical field theory of kernel evolution in wide neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [10] Jiaoyang Huang and Horng-Tzer Yau. Dynamics of deep neural networks and neural tangent hierarchy. In _International conference on machine learning_, pages 4542-4551. PMLR, 2020.
* [11] Ethan Dyer and Guy Gur-Ari. Asymptotics of wide networks from feynman diagrams. In _International Conference on Learning Representations_, 2020.
* [12] Anders Andreassen and Ethan Dyer. Asymptotics of wide convolutional neural networks. _arXiv preprint arXiv:2008.08675_, 2020.
* [13] Daniel A Roberts, Sho Yaida, and Boris Hanin. _The principles of deep learning theory_. Cambridge University Press Cambridge, MA, USA, 2022.
* [14] Boris Hanin. Random fully connected neural networks as perturbatively solvable hierarchies. _arXiv preprint arXiv:2204.01058_, 2022.

* [15] Sho Yaida. Meta-principled family of hyperparameter scaling strategies. _arXiv preprint arXiv:2210.04909_, 2022.
* [16] Jaehoon Lee, Samuel Schoenholz, Jeffrey Pennington, Ben Adlam, Lechao Xiao, Roman Novak, and Jascha Sohl-Dickstein. Finite versus infinite neural networks: an empirical study. _Advances in Neural Information Processing Systems_, 33:15156-15172, 2020.
* [17] Greg Yang, Edward Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via zero-shot hyperparameter transfer. _Advances in Neural Information Processing Systems_, 34, 2021.
* [18] Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, and Cengiz Pehlevan. The onset of variance-limited behavior for networks in the lazy and rich regimes. In _The Eleventh International Conference on Learning Representations_, 2023.
* [19] Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. In _International Conference on Learning Representations_, 2021.
* [20] Alex Damian, Eshaan Nichani, and Jason D. Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability. In _The Eleventh International Conference on Learning Representations_, 2023.
* [21] Atish Agarwala, Fabian Pedregosa, and Jeffrey Pennington. Second-order regression models exhibit progressive sharpening to the edge of stability. _arXiv preprint arXiv:2210.04860_, 2022.
* [22] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced research).
* [23] Radford M Neal. Priors for infinite networks. In _Bayesian Learning for Neural Networks_, pages 29-53. Springer, 1996.
* [24] Radford M Neal. _Bayesian learning for neural networks_, volume 118. Springer Science & Business Media, 2012.
* [25] Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam Schoenholz, and Yasaman Bahri. Deep neural networks as gaussian processes. In _International Conference on Learning Representations_, 2018.
* [26] Alexander G. de G. Matthews, Jiri Hron, Mark Rowland, Richard E. Turner, and Zoubin Ghahramani. Gaussian process behaviour in wide deep neural networks. In _International Conference on Learning Representations_, 2018.
* [27] Boris Hanin and Mihai Nica. Finite depth and width corrections to the neural tangent kernel. In _International Conference on Learning Representations_, 2020.
* [28] Boris Hanin and Mihai Nica. Products of many large random matrices and gradients in deep neural networks. _Communications in Mathematical Physics_, 376(1):287-322, 2020.
* [29] Sho Yaida. Non-gaussian processes and neural networks at finite widths. In _Mathematical and Scientific Machine Learning_, pages 165-192. PMLR, 2020.
* [30] Jacob Zavatone-Veth and Cengiz Pehlevan. Exact marginal prior distributions of finite bayesian neural networks. _Advances in Neural Information Processing Systems_, 34:3364-3375, 2021.
* [31] Jacob Zavatone-Veth, Abdulkadir Canatar, Ben Ruben, and Cengiz Pehlevan. Asymptotics of representation learning in finite bayesian neural networks. _Advances in Neural Information Processing Systems_, 34, 2021.
* [32] Gadi Naveh, Oded Ben David, Haim Sompolinsky, and Zohar Ringel. Predicting the outputs of finite deep neural networks trained with noisy gradients. _Physical Review E_, 104(6):064301, 2021.

* [33] Adam X. Yang, Maxime Robeyns, Edward Milsom, Ben Anson, Nandi Schoots, and Laurence Aitchison. A theory of representation learning gives a deep generalisation of kernel methods. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, _Proceedings of the 40th International Conference on Machine Learning_, volume 202 of _Proceedings of Machine Learning Research_, pages 39380-39415. PMLR, 23-29 Jul 2023.
* [34] Inbar Seroussi, Gadi Naveh, and Zohar Ringel. Separation of scales and a thermodynamic description of feature learning in some cnns. _Nature Communications_, 14(1):908, 2023.
* [35] Qianyi Li and Haim Sompolinsky. Statistical mechanics of deep linear neural networks: The backpropagating kernel renormalization. _Physical Review X_, 11(3):031059, 2021.
* [36] Jacob A Zavatone-Veth and Cengiz Pehlevan. Depth induces scale-averaging in overparameterized linear bayesian neural networks. _55th Asilomar Conference on Signals, Systems, and Computers_, 2021.
* [37] Jacob A Zavatone-Veth, William L Tong, and Cengiz Pehlevan. Contrasting random and learned features in deep bayesian linear regression. _Physical Review E_, 105(6):064118, 2022.
* [38] Gadi Naveh and Zohar Ringel. A self consistent theory of gaussian processes captures feature learning effects in finite cnns. _Advances in Neural Information Processing Systems_, 34, 2021.
* [39] Jacob A Zavatone-Veth, Abdulkadir Canatar, Benjamin S Ruben, and Cengiz Pehlevan. Asymptotics of representation learning in finite bayesian neural networks*. _Journal of Statistical Mechanics: Theory and Experiment_, 2022(11):114008, nov 2022.
* [40] Grant Rotskoff and Eric Vanden-Eijnden. Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks. _Advances in neural information processing systems_, 31, 2018.
* [41] Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-parameterized models using optimal transport. _Advances in neural information processing systems_, 31, 2018.
* [42] Dyego Araujo, Roberto I Oliveira, and Daniel Yukimura. A mean-field limit for certain deep neural networks. _arXiv preprint arXiv:1906.00193_, 2019.
* [43] Rodrigo Veiga, Ludovic Stephan, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova. Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022.
* [44] Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro. From high-dimensional amp; mean-field dynamics to dimensionless odes: A unifying approach to sgd in two-layers networks. In Gergely Neu and Lorenzo Rosasco, editors, _Proceedings of Thirty Sixth Conference on Learning Theory_, volume 195 of _Proceedings of Machine Learning Research_, pages 1199-1227. PMLR, 12-15 Jul 2023.
* [45] Huy Tuan Pham and Phan-Minh Nguyen. Limiting fluctuation and trajectorial stability of multilayer neural networks with mean field training. _Advances in Neural Information Processing Systems_, 34:4843-4855, 2021.
* [46] Blake Bordelon and Cengiz Pehlevan. The influence of learning rule on representation dynamics in wide neural networks. In _The Eleventh International Conference on Learning Representations_, 2023.
* [47] Paul Cecil Martin, ED Siggia, and HA Rose. Statistical dynamics of classical systems. _Physical Review A_, 8(1):423, 1973.
* [48] Moshe Moshe and Jean Zinn-Justin. Quantum field theory in the large n limit: A review. _Physics Reports_, 385(3-6):69-228, 2003.

* [49] Jean Zinn-Justin. _Quantum field theory and critical phenomena_, volume 171. Oxford university press, 2021.
* [50] Carson C Chow and Michael A Buice. Path integral methods for stochastic differential equations. _The Journal of Mathematical Neuroscience (JMN)_, 5:1-35, 2015.
* [51] Moritz Helias and David Dahmen. _Statistical Field Theory for Neural Networks_. Springer International Publishing, 2020.
* [52] A Crisanti and H Sompolinsky. Path integral approach to random neural networks. _Physical Review E_, 98(6):062120, 2018.
* [53] Kai Segadlo, Bastian Epping, Alexander van Meegen, David Dahmen, Michael Kramer, and Moritz Helias. Unified field theoretical approach to deep and recurrent neuronal networks. _Journal of Statistical Mechanics: Theory and Experiment_, 2022(10):103401, 2022.
* [54] Yizhang Lou, Chris E Mingard, and Soufiane Hayou. Feature learning and signal propagation in deep neural networks. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 14248-14282. PMLR, 17-23 Jul 2022.
* [55] Carl M Bender and Steven Orszag. _Advanced mathematical methods for scientists and engineers I: Asymptotic methods and perturbation theory_, volume 1. Springer Science & Business Media, 1999.
* [56] Mehran Kardar. _Statistical physics of fields_. Cambridge University Press, 2007.
* [57] Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy Gur-Ari. The large learning rate phase of deep learning: the catapult mechanism. _arXiv preprint arXiv:2003.02218_, 2020.
* [58] Ben Adlam and Jeffrey Pennington. The neural tangent kernel in high dimensions: Triple descent and a multi-scale theory of generalization. In _International Conference on Machine Learning_, pages 74-84. PMLR, 2020.
* [59] Soufiane Hayou. On the infinite-depth limit of finite-width neural networks. _Transactions on Machine Learning Research_, 2023.
* [60] Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit. _arXiv preprint arXiv:2309.16620_, 2023.
* [61] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023.
* [62] Michael Baake and Ulrike Schlaegel. The peano-baker series. _Proceedings of the Steklov Institute of Mathematics_, 275(1):155-159, 2011.
* [63] Roger W Brockett. _Finite dimensional linear systems_. SIAM, 2015.
* [64] Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners: The silent alignment effect. In _International Conference on Learning Representations_, 2022.
* [65] B. Bordelon, A. Canatar, and C. Pehlevan. Spectrum dependent learning curves in kernel regression and wide neural networks. _International Conference of Machine Learning_, 2020.
* [66] Phan-Minh Nguyen. Mean field limit of the learning dynamics of multilayer neural networks. _arXiv preprint arXiv:1902.02880_, 2019.

[MISSING_PAGE_EMPTY:15]

Figure A.3: The covariance of kernel entries across pairs of time points \(\Sigma_{H^{\xi}}(t,s)=N\text{ Cov}(H^{\xi}(t,t),H^{\xi}(s,s))\) for depth \(4\) linear network trained on whitened data. The variance becomes increasingly localized in time as feature learning \(\gamma\) increases.

Figure A.5: Width \(N=64\) depth \(6\) CNNs trained on the full CIFAR-10 with MSE. An ensemble of size \(E=10\) randomly initialized networks are trained. (a) Training MSE for varying \(\gamma\). (b) Final layer kernel-task alignment does strongly depend on \(\gamma\), despite similar train dynamics. (c) Top-1 classification test accuracy is only slightly different across \(\gamma\). A small benefit from ensembling is visible late in training. (c) Initialization variance (measured by the ratio of single model to ensembled MSE) for training and test losses. Richer networks have lower variance throughout training. (b) Networks have distinct kernel dynamics when trained with different \(\gamma\) as evidenced by the alignment (cosine similarity) between the final layer feature kernel \(\Phi^{L}\) and the target test labels \(\bm{y}\).

CIFAR-10 Experimental Details

We trained the following depth \(6\) CNN architecture in the mean field parameterization using FLAX [61] on a single GPU. The bias parameters were zero in each hidden Conv layer, but were used for the readout weights. The networks were trained with MSE loss on centered \(10\) dimensional targets \(\bm{y}_{\mu}\in\mathbb{R}^{10}\) for \(\mu\in[P]\). Each convolution was followed by an average pooling operation. To obtain mean field behavior, NTK parameterization with a modified final layer is used [7; 9].

```
1fromflaximportlinenasnn
2importjax.numpyasjnp
3
4classCNN(nn.Module):
5
6width:int
7
8defsetup(self):
9kif=nn.initializers.normal(stddev=1.0)#O_N(1)entries
10self.conv1=nn.Conv(features=self.width,kernel_init=kif,use_bias=False,kernel_size=(3,3))
11self.conv2=nn.Conv(features=self.width,kernel_init=kif,use_bias=False,kernel_size=(3,3))
12self.conv3=nn.Conv(features=self.width,kernel_init=kif,use_bias=False,kernel_size=(3,3))
13self.conv4=nn.Conv(features=self.width,kernel_init=kif,use_bias=False,kernel_size=(3,3))
14self.conv5=nn.Conv(features=self.width,kernel_init=kif,use_bias=False,kernel_size=(3,3))
15self.readout=nn.Dense(features=10,use_bias=True,kernel_init=kif)
16return
17
18def_call_(self,x,train=True):
19N=self.width
20D=3
21x=self.conv1(x)/jnp.sqrt(D*9)
22x=jnp.sqrt(2.0)*nn.relu(x)
23x=nn.avg_pool(x,window_shape=(2,2),strides=(2,2))#32x32->16x16
24x=self.conv2(x)/jnp.sqrt(N*9)#explicitN^{-1/2}
25x=jnp.sqrt(2.0)*nn.relu(x)
26x=nn.avg_pool(x,window_shape=(2,2),strides=(2,2))#16x16->8x8
27x=self.conv3(x)/jnp.sqrt(N*9)
28x=jnp.sqrt(2.0)*nn.relu(x)
29x=nn.avg_pool(x,window_shape=(2,2),strides=(2,2))#8x8->4x4
30x=self.conv4(x)/jnp.sqrt(N*9)
31x=jnp.sqrt(2.0)*nn.relu(x)
32x=nn.avg_pool(x,window_shape=(2,2),strides=(2,2))#4x4->2x2
33x=self.conv5(x)/jnp.sqrt(N*9)
34x=jnp.sqrt(2.0)*nn.relu(x)
35x=nn.avg_pool(x,window_shape=(2,2),strides=(2,2))#2x2->1x1
36x=x.reshape((x.shape[0],-1))#flatten
37x=self.readout(x)/N#formeanfieldscaling
38returnx ```

All models were trained with standard SGD with a batch size of \(256\). Each element in the ensemble of \(E\) networks is trained on identical batches presented in identical order. For the Figure 7 experiments, the raw learning rate is scaled as \(\eta=10N\sqrt{\gamma}\) with \(\gamma=0.2\) (note that mean field theory requires scaling the raw learning rate linearly with \(N\) since the raw NTK is \(\mathcal{O}(N^{-1})\)[9]). For Figure A.5, the learning rate is \(\eta=5N\sqrt{\gamma}\). We find that choosing \(\eta\propto\sqrt{\gamma}\) gives approximately conserved training times across \(\gamma\) (though distinct representation dynamics). The Figure A.4 shows the dynamics of fitting \(P=64\) training points with full batch gradient descent and \(\gamma=0.1\).

## Appendix C Review of DMFT: Deriving the Action

In this section we derive the DMFT action which contains all of the necessary statistical information about randomly initialized finite width \(N\) networks. From the action \(S\) the DMFT saddle point and the propagator can be computed. This derivation follows closely the original derivation by Bordelon & Pehlevan [9]. We start by writing the gradient flow dynamics on weight matrices

\[\frac{d}{dt}\bm{W}^{\ell}(t)=\frac{\gamma}{\sqrt{N}}\sum_{\mu=1}^{P}\Delta_{ \mu}(t)\bm{g}_{\mu}^{\ell+1}(t)\phi(\bm{h}_{\mu}^{\ell}(t))^{\top}\] (11)

where \(\Delta_{\mu}(t)=-\frac{\partial\mathcal{L}}{\partial f_{\mu}(t)}\) are the error signals and \(\bm{g}_{\mu}^{\ell}(t)=N\gamma\frac{\partial f_{\mu}(t)}{\partial\bm{h}_{\mu }^{\ell}(t)}\) are the back-propagation signals. The prediction dynamics satisfy

\[\frac{d}{dt}f_{\mu}(t)=\sum_{\nu}K_{\mu\nu}(t)\Delta_{\nu}(t)\] (12)

where \(K_{\mu\nu}(t)\) is the instantaneous neural tangent kernel (NTK). At finite width \(N\) all of the above quantities depend on the precise initialization of the network. We transform the weight dynamics into an integral equation and use the recurrence for \(\bm{h}^{\ell}\) to obtain the following

\[\bm{h}^{\ell+1}(t) =\frac{1}{\sqrt{N}}\bm{W}^{\ell}(0)\phi(\bm{h}_{\mu}^{\ell}(t))+ \gamma\int_{0}^{t}ds\sum_{\nu}\Phi_{\mu\nu}^{\ell}(t,s)\bm{g}_{\nu}^{\ell+1}(s)\] \[\bm{g}_{\mu}^{\ell}(t) =\dot{\phi}(\bm{h}_{\mu}^{\ell}(t))\odot\bm{z}_{\mu}^{\ell}(t)\] \[\bm{z}_{\mu}^{\ell}(t) =\frac{1}{\sqrt{N}}\bm{W}^{\ell}(0)^{\top}\bm{g}_{\mu}^{\ell+1}(t )+\gamma\int_{0}^{t}ds\sum_{\nu}G_{\mu\nu}^{\ell+1}(t,s)\phi(\bm{h}_{\nu}^{ \ell}(s))\] (13)

where we introduced the feature and gradient kernels

\[\Phi_{\mu\nu}^{\ell}(t,s)=\frac{1}{N}\phi(\bm{h}_{\mu}^{\ell}(t))\cdot\phi( \bm{h}_{\nu}^{\ell}(s))\;,\;G_{\mu\nu}^{\ell}(t,s)=\frac{1}{N}\bm{g}_{\mu}^{ \ell}(t)\cdot\bm{g}_{\nu}^{\ell}(s).\] (14)

Written this way, we see that the source of the disorder which depends on the initial random weights \(\bm{W}^{\ell}(0)\) comes through the fields

\[\bm{\chi}_{\mu}^{\ell+1}(t)=\frac{1}{\sqrt{N}}\bm{W}^{\ell}(0)\phi(\bm{h}_{ \mu}^{\ell}(t))\;,\;\bm{\xi}_{\mu}^{\ell+1}(t)=\frac{1}{\sqrt{N}}\bm{W}^{\ell} (0)^{\top}\bm{g}_{\mu}^{\ell}(t).\] (15)

If we can characterize the distribution of the fields \(\bm{\chi}_{\mu}^{\ell}(t)\) and \(\bm{\xi}_{\mu}^{\ell}(t)\), then we can consequently characterize the distribution of \(\bm{h}_{\mu}^{\ell}(t),\bm{g}_{\mu}^{\ell}(t)\). We therefore choose to study the moment generating functional

\[Z[\{\bm{j}^{\ell},\bm{v}^{\ell}\}]=\left\langle\exp\left(\sum_{\ell\mu}\int dt \;\left[\bm{j}_{\mu}^{\ell}(t)\cdot\bm{\chi}_{\mu}^{\ell}(t)+\bm{v}_{\mu}^{ \ell}(t)\cdot\bm{\chi}_{\mu}^{\ell}(t)\right]\right)\right\rangle_{\bm{ \theta}_{0}}\] (16)

Moments of these fields can be computed through differentiation with respect to the sources \(\bm{j},\bm{v}\) near zero-source (\(\bm{j}=\bm{v}=0\))

\[\left\langle\chi_{\mu_{1}}^{\ell_{1}}(t_{1})...\chi_{\mu_{n}}^{ \ell_{n}}(t_{n})\xi_{\mu_{1}}^{\ell_{1}}(t_{1})...\xi_{\mu_{m}}^{\ell_{m}}(t_{ m})\right\rangle\] \[=\frac{\delta}{\delta j_{\mu_{1}}^{\ell_{1}}(t_{1})}...\frac{ \delta}{\delta j_{\mu_{n}}^{\ell_{n}}(t_{n})}\frac{\delta}{\delta v_{\mu_{1}}^ {\ell_{1}}(t_{1})}...\frac{\delta}{\delta v_{\mu_{m}}^{\ell_{m}}(t_{m})}Z[\{ \bm{j}^{\ell},\bm{v}^{\ell}\}]|_{\bm{j}=\bm{v}=0}.\] (17)

To average over the initial weights, we introduce a Fourier representation of the Dirac-Delta function \(1=\int dz\delta(z)=\int\frac{dzd\hat{z}}{2\pi}\exp(i\hat{z}z)\). We perform this transformation for each of the fields to enforce their definition

\[\delta\left(\mathbf{x}_{\mu}^{\ell}(t)-\frac{1}{\sqrt{N}}\bm{W}^{ \ell}(0)\phi(\bm{h}_{\mu}^{\ell}(t))\right)=\int\frac{d\hat{\bm{\xi}}_{\mu}^{ \ell}(t)}{(2\pi)^{N}}\exp\left(\hat{\bm{\chi}}_{\mu}^{\ell}(t)\cdot\left[\bm{ \chi}_{\mu}^{\ell}(t)-\frac{1}{\sqrt{N}}\bm{W}^{\ell}(0)\phi(\bm{h}_{\mu}^{ \ell}(t))\right]\right)\] \[\delta\left(\bm{\xi}_{\mu}^{\ell}(t)-\frac{1}{\sqrt{N}}\bm{W}^{ \ell}(0)^{\top}\bm{g}_{\mu}^{\ell}(t)\right)=\int\frac{d\hat{\bm{\xi}}_{\mu}^{ \ell}(t)}{(2\pi)^{N}}\exp\left(\hat{\bm{\xi}}_{\mu}^{\ell}(t)\cdot\left[\bm{ \xi}_{\mu}^{\ell}(t)-\frac{1}{\sqrt{N}}\bm{W}^{\ell}(0)^{\top}\bm{g}_{\mu}^{ \ell+1}(t)\right]\right)\] (18)

We insert these Dirac delta functions so that we can directly average over the weights

\[\ln\mathbb{E}_{\bm{W}^{\ell}(0)}\exp\left(-\frac{i}{\sqrt{N}} \text{Tr}\bm{W}^{\ell}(0)^{\top}\int dt\sum_{\mu}\left[\hat{\bm{\chi}}_{\mu}^{ \ell+1}(t)\phi(\bm{h}_{\mu}^{\ell}(t))^{\top}+\bm{g}_{\mu}^{\ell+1}(t)\hat{\bm {\xi}}_{\mu}^{\ell}(t)^{\top}\right]\right)\] \[=-\frac{1}{2}\sum_{\mu\nu}\int dtds\left[\hat{\bm{\chi}}_{\mu}^{ \ell+1}(t)\cdot\hat{\bm{\chi}}_{\nu}^{\ell+1}(s)\Phi_{\mu\nu}^{\ell}(t,s)+\hat {\bm{\xi}}_{\mu}^{\ell}(t)\cdot\hat{\bm{\xi}}_{\nu}^{\ell}(s)G_{\mu\nu}^{\ell+ 1}(t,s)\right]\] \[-\frac{1}{N}\sum_{\mu\nu}\int dtds(\hat{\bm{\chi}}_{\mu}^{\ell+1}( t)\cdot\bm{g}_{\nu}^{\ell+1}(s))(\phi(\bm{h}_{\mu}^{\ell}(t))\cdot\hat{\bm{ \xi}}_{\nu}^{\ell}(s))\] (19)

where we introduced the kernels \(\Phi^{\ell},G^{\ell}\). We next introduce the order parameter

\[A_{\mu\nu}^{\ell}(t,s)=-\frac{i}{N}\phi(\bm{h}_{\mu}^{\ell}(t))\cdot\hat{\bm{ \xi}}_{\nu}^{\ell}(s)\] (20)

To enforce the definitions of the new order parameters \(\{\Phi,G,A\}\) we again introduce Dirac-delta functions

\[\delta\left(N\Phi_{\mu\nu}^{\ell}(t,s)-\phi(\bm{h}_{\mu}^{\ell}(t) )\cdot\phi(\bm{h}_{\nu}^{\ell}(s))\right)\] \[=\int\frac{d\hat{\Phi}_{\mu\nu}^{\ell}(t,s)}{2\pi i}\exp\left( \hat{\Phi}_{\mu\nu}^{\ell}(t,s)\left[N\Phi_{\mu\nu}^{\ell}(t,s)-\phi(\bm{h}_{ \mu}^{\ell}(t))\cdot\phi(\bm{h}_{\nu}^{\ell}(s))\right]\right)\] (21)

Analogous constraints for \(G\) and \(A\) are enforced with conjugate variables \(\hat{G},B\). After introducing these variables, we find that the moment generating functional has the form

\[Z=\int\prod_{\ell\mu\nu ts}\frac{d\hat{\Phi}_{\mu\nu}^{\ell}(t,s )d\Phi_{\mu\nu}^{\ell}(t,s)}{2\pi i}\frac{d\hat{G}_{\mu\nu}^{\ell}(t,s)dG_{\mu \nu}^{\ell}(t,s)}{2\pi i}\frac{dG_{\mu\nu}^{\ell}(t,s)dG_{\mu\nu}^{\ell}(t,s)} {2\pi i}\frac{dA_{\mu\nu}^{\ell}(t,s)dB_{\mu\nu}^{\ell}(t,s)}{2\pi i}\] \[\exp\left(NS[\{\Phi^{\ell},\hat{\Phi}^{\ell},G^{\ell},\hat{G}^{ \ell},A^{\ell},B^{\ell}\}]\right)\] (22)

where \(S\) is the \(\mathcal{O}(1)\) DMFT _action_ which defines the statistical distribution over the dynamics. The action takes the form

\[S =\sum_{\ell\mu\nu}\int dt\ ds\left[\hat{\Phi}_{\mu\nu}^{\ell}(t,s) \Phi_{\mu\nu}^{\ell}(t,s)+\hat{G}_{\mu\nu}^{\ell}(t,s)-A_{\mu\nu}^{\ell}(t,s)B _{\nu\mu}^{\ell}(s,t)\right]\] \[+\frac{1}{N}\sum_{\ell=1}^{L}\sum_{i=1}^{N}\ln\mathcal{Z}_{\ell} [\{j_{i}^{\ell},v_{i}^{\ell}\}]\] (23)

where \(\mathcal{Z}_{\ell}\) is the single site stochastic process for layer \(\ell\) which defines the marginal distribution of \(\chi,\xi\), with the following form

\[\mathcal{Z}_{\ell}[\{j^{\ell}(t),v^{\ell}(t)\}]= \int\prod_{\mu t}\frac{d\hat{\chi}_{\mu}^{\ell}(t)d\chi_{\mu}^{ \ell}(t)}{2\pi}\frac{d\hat{\xi}_{\mu}^{\ell}(t)d\xi_{\mu}^{\ell}(t)}{2\pi} \exp\left(\int dt\sum_{\mu}[j_{\mu}^{\ell}(t)\chi_{\mu}^{\ell}(t)+v_{\mu}^{\ell}( t)\xi_{\mu}^{\ell}(t)]\right)\] \[\exp\left(-\frac{1}{2}\sum_{\mu\nu}\int dtds\left[\hat{\Phi}_{\mu \nu}^{\ell}(t,s)\hat{\chi}_{\mu}^{\ell}(t)\hat{\chi}_{\nu}^{\ell}(s)+\hat{G}_{ \mu\nu}^{\ell}(t,s)\hat{\xi}_{\mu}^{\ell}(t)\hat{\xi}_{\nu}^{\ell}(s)\right]\right)\] \[\exp\left(-i\sum_{\mu\nu}\int dtds\left[B_{\mu\nu}^{\ell}(t,s)\hat {\xi}_{\mu}^{\ell}(t)\phi(h_{\nu}^{\ell}(s))+A_{\mu\nu}^{\ell-1}(t,s)\hat{\chi}_{ \mu}^{\ell}(t)g_{\nu}^{\ell}(s)\right]\right)\] \[\exp\left(i\sum_{\mu}\int dt[\hat{\chi}_{\mu}^{\ell}(t)\chi_{\mu}^ {\ell}(t)+\hat{\xi}_{\mu}^{\ell}(t)\xi_{\mu}^{\ell}(t)]\right)\] (24)where in the above, the \(\{h,g\}\) fields should be regarded as functionals of \(\{\chi,\xi\}\). At zero source \(\bm{j}^{\ell},\bm{v}^{\ell}\to 0\) this function \(S\) can be regarded as the log density for the complete collection of order parameters \(\bm{q}=\left\{\hat{\Phi},\Phi,\hat{G},G,A,B\right\}\) which collectively control the dynamics. Concretely, we have that \(p(\bm{q})\propto\exp\left(NS(\bm{q})\right)\). In the next section we explore an approximation scheme for averages over this distribution at large \(N\).

## Appendix D Cumulant Expansion of Observables

We are interested in a principled power series expansion (in \(1/N\)) of any observable average \(\left\langle O(\bm{q})\right\rangle\) that depends on DMFT order parameters \(\bm{q}\). At any width \(N\) the observable average takes the form

\[\left\langle O(\bm{q})\right\rangle_{N}=\frac{\int d\bm{q}\exp\left(NS(\bm{q}) \right)O(\bm{q})}{\int d\bm{q}\exp\left(NS(\bm{q})\right)}\] (25)

As discussed in the main text, the \(N\rightarrow\infty\) limit gives \(\left\langle O(\bm{q})\right\rangle_{N}\sim O(\bm{q}_{\infty})\) where \(\frac{\partial S}{\partial\bm{q}}|_{\bm{q}_{\infty}}=0\) by a steepest descent argument [55]. We assume that \(S\)'s Hessian is negative semidefinite so that \(\bm{\Sigma}\equiv-\left[\nabla^{2}S(\bm{q})|_{\bm{q}_{\infty}}\right]^{-1}\succeq 0\) and Taylor expand \(S(\bm{q})\) around the saddle point \(\bm{q}_{\infty}\) giving \(S(\bm{q})=S(\bm{q}_{\infty})+\frac{1}{2}(\bm{q}-\bm{q}_{\infty})^{\top} \nabla^{2}S(\bm{q})(\bm{q}-\bm{q}_{\infty})+V(\bm{q}-\bm{q}_{\infty})\). We note that the remainder function \(V\) contains only cubic and higher powers of \(\bm{q}-\bm{q}_{\infty}\equiv\bm{\delta}/\sqrt{N}\). The variable \(\bm{\delta}\) will be order \(\mathcal{O}(1)\). This will allow us to verify that additional terms are suppressed in powers of \(1/N\). Expanding both the numerator and denominator's integrands in powers of \(V\), we find

\[\left\langle O(\bm{q})\right\rangle_{N}= \frac{\int d\bm{q}\exp\left(-\frac{N}{2}(\bm{q}-\bm{q}_{\infty}) ^{\top}\bm{\Sigma}^{-1}(\bm{q}-\bm{q}_{\infty})+NV(\bm{q}-\bm{q}_{\infty}) \right)O(\bm{q})}{\int d\bm{q}\exp\left(-\frac{N}{2}(\bm{q}-\bm{q}_{\infty}) ^{\top}\bm{\Sigma}^{-1}(\bm{q}-\bm{q}_{\infty})+NV(\bm{q}-\bm{q}_{\infty}) \right)}\] \[= \frac{\int d\bm{\delta}\exp\left(-\frac{1}{2}\bm{\delta}^{\top} \bm{\Sigma}^{-1}\bm{\delta}\right)(1+NV+\frac{N^{2}}{2}V^{2}+...)O(\bm{q}_{ \infty}+N^{-1/2}\bm{\delta})}{\int d\bm{\delta}\exp\left(-\frac{1}{2}\bm{ \delta}^{\top}\bm{\Sigma}^{-1}\bm{\delta}\right)(1+NV+\frac{N^{2}}{2}V^{2}+...)}\] \[= \frac{\left\langle O\right\rangle_{\infty}+N\left\langle VO \right\rangle_{\infty}+\frac{N^{2}}{2!}\left\langle V^{2}O\right\rangle_{ \infty}+\frac{N^{3}}{3!}\left\langle V^{3}O\right\rangle_{\infty}+...}{1+N \left\langle V\right\rangle_{\infty}+\frac{N^{2}}{2!}\left\langle V^{2} \right\rangle_{\infty}+\frac{N^{3}}{3!}\left\langle V^{3}O\right\rangle_{ \infty}+...}\] \[= \left\langle O\right\rangle_{\infty}\frac{1+N\left\langle VO \right\rangle_{\infty}/\left\langle O\right\rangle_{\infty}+\frac{N^{2}}{2!} \left\langle V^{2}O\right\rangle_{\infty}/\left\langle O\right\rangle_{ \infty}+\frac{N^{3}}{3!}\left\langle V^{3}O\right\rangle_{\infty}/\left\langle O \right\rangle_{\infty}+...}{1+N\left\langle V\right\rangle_{\infty}+\frac{N^{2} }{2!}\left\langle V^{2}\right\rangle_{\infty}+\frac{N^{3}}{3!}\left\langle V ^{3}\right\rangle_{\infty}+...}\] (26)

where \(\left\langle\right\rangle_{\infty}\) represents an average over the Gaussian fluctuation \(\mathcal{N}\left(\bm{q}_{\infty},-\frac{1}{N}\left[\nabla_{\bm{q}}^{2}S(\bm{ q}_{\infty})\right]^{-1}\right)\). We see that the series in the denominator contains terms of the form \(\frac{N^{k}}{k!}\left\langle V^{k}\right\rangle_{\infty}\) while the numerator depends on terms of the form \(\frac{N^{k}}{k!}\left\langle V^{k}O\right\rangle_{\infty}/\left\langle O \right\rangle_{\infty}\). In either of these power series, the \(k\)-th term can contribute at most

\[\frac{N^{k}\left\langle V^{k}O\right\rangle_{\infty}}{\left\langle O\right\rangle _{\infty}}\,\ N^{k}\left\langle V^{k}\right\rangle_{\infty}\sim\begin{cases} \mathcal{O}(N^{-(k+1)/2})&\text{$k$ odd}\\ \mathcal{O}(N^{-k/2})&\text{$k$ even}\end{cases}\] (27)

since \(V\) contributes only cubic and higher terms. Thus each term in the numerator and denominator's series contains increasing powers of \(1/N\). Concretely, each of the two series have terms of order \(\{N^{0},N^{-1},N^{-1},N^{-2},N^{-2},...\}\). Thus any quantity of the form \(\frac{\left\langle O\right\rangle}{\left\langle O\right\rangle_{\infty}}\) admits a ratio of power series in powers of \(1/N\). One could truncate each of the series in the numerator and denominator to a desired order in \(N\). Alternatively, the denominator could be expanded giving a single series (the cumulant expansion [56]). The first few terms in the cumulant expansion have the form

\[\left\langle O\right\rangle_{N} =\left\langle O\right\rangle_{\infty}+N\left[\left\langle OV \right\rangle_{\infty}-\left\langle O\right\rangle_{\infty}\left\langle V \right\rangle_{\infty}\right]\] \[+\frac{N^{2}}{2}\left[\left\langle V^{2}O\right\rangle_{\infty}-2 \left\langle VO\right\rangle_{\infty}\left\langle V\right\rangle_{\infty}+2 \left\langle V\right\rangle_{\infty}^{2}\left\langle O\right\rangle_{\infty}- \left\langle V^{2}\right\rangle_{\infty}\left\langle O\right\rangle_{\infty} \right]+...\] (28)

In this work, we mainly are interested in the leading order correction to \(\left\langle O\right\rangle\) which can always be obtained with the truncation after the terms linear in \(V\) for any observable \(O\).

### Square Deviation from DMFT

We will now analyze the fluctuation statistics of our order parameters around the saddle point \(\left\langle(\bm{q}-\bm{q}_{\infty})(\bm{q}-\bm{q}_{\infty})^{\top}\right\rangle_ {N}\) which has the form

\[\left\langle(\bm{q}-\bm{q}_{\infty})(\bm{q}-\bm{q}_{\infty})^{\top }\right\rangle_{N}=\frac{\left\langle(\bm{q}-\bm{q}_{\infty})(\bm{q}-\bm{q}_{ \infty})^{\top}\right\rangle_{\infty}+N\left\langle V\right.(\bm{q}-\bm{q}_{ \infty})(\bm{q}-\bm{q}_{\infty})^{\top}\right\rangle_{\infty}+...}{1+N\left\langle V \right\rangle_{\infty}+...}\] \[=\left[\frac{\frac{1}{N}\bm{\Sigma}+\mathcal{O}(N^{-2})}{1+ \mathcal{O}(N^{-1})}\right]\sim\frac{1}{N}\bm{\Sigma}+\mathcal{O}(N^{-2}),\] (29)

as stated in the main text and verified empirically in Figure 3 (a). The reason that the terms in the numerator involving \(V\) can be no larger than \(\mathcal{O}(N^{-2})\) comes from vanishing of odd moments for \(\bm{q}-\bm{q}_{\infty}\) in the unperturbed distribution. Thus the leading expression for \(\left\langle(\bm{q}-\bm{q}_{\infty})(\bm{q}-\bm{q}_{\infty})^{\top}\right\rangle\) only depends on \(\bm{\Sigma}\) and not on \(V\).

### Mean Deviation from DMFT

Although the square displacement from DMFT only depended on \(\Sigma\) and not on \(V\), we note that the _average order parameter displacement_\(\left\langle\bm{q}-\bm{q}_{\infty}\right\rangle\) does receive a \(\mathcal{O}(1/N)\) correction that depends on the perturbed potential \(V\)

\[\left\langle\bm{q}-\bm{q}_{\infty}\right\rangle_{N} =\frac{\left\langle\bm{q}-\bm{q}_{\infty}\right\rangle_{\infty}+ N\left\langle(\bm{q}-\bm{q}_{\infty})V\right\rangle_{\infty}+\frac{N^{2}}{2} \left\langle(\bm{q}-\bm{q}_{\infty})V^{2}\right\rangle_{\infty}+...}{1+N \left\langle V\right\rangle_{\infty}+\frac{N^{2}}{2}\left\langle V^{2}\right \rangle_{\infty}+...}\] \[\sim\frac{\bm{\Sigma}\left\langle\frac{\partial V}{\partial\bm{q }}\right\rangle_{\infty}+\mathcal{O}(N^{-2})}{1+\mathcal{O}(N^{-1})}\sim\bm{ \Sigma}\left\langle\frac{\partial V}{\partial\bm{q}}\right\rangle_{\infty}+ \mathcal{O}(N^{-2}).\] (30)

where in the last line we used Stein's lemma (Gaussian integration by parts) for the Gaussian distribution over \(\bm{q}\). Note that \(\left\langle\frac{\partial V}{\partial\bm{q}}\right\rangle_{\infty}\sim \mathcal{O}\left(\frac{1}{N}\right)\) since the derivative of the cubic term in \(V\) gives a quadratic function of \(\bm{q}-\bm{q}_{\infty}\), whose average must be \(\mathcal{O}(N^{-1})\). In this work, we focus primarily on the structure of the propagator, but outline a general recipe for getting the leading mean correction in Appendix G and H.2.

### Covariance of Order Parameters

Lastly, we combine the previous two observations to reason about the scaling of the order parameter covariance over initializations. We note that the leading covariance of the order parameters over random initializations is also given by the propagator: \(\text{Cov}(\bm{q})\sim\frac{1}{N}\bm{\Sigma}+\mathcal{O}(N^{-2})\), since

\[\text{Cov}(\bm{q}) =\left\langle(\bm{q}-\left\langle\bm{q}\right\rangle_{N})\left( \bm{q}-\left\langle\bm{q}\right\rangle_{N}\right)^{\top}\right\rangle_{N}\] \[=\left\langle(\bm{q}-\bm{q}_{\infty})\left(\bm{q}-\bm{q}_{\infty} \right)^{\top}\right\rangle_{N}-\left\langle(\bm{q}_{\infty}-\left\langle\bm{q }\right\rangle_{N})\left(\bm{q}_{\infty}-\left\langle\bm{q}\right\rangle_{N} \right)^{\top}\right\rangle_{N}\] \[\sim\frac{1}{N}\bm{\Sigma}+\mathcal{O}(N^{-2})\] (31)

due to the arguments above which showed that \(\left\langle(\bm{q}-\bm{q}_{\infty})(\bm{q}-\bm{q}_{\infty})^{\top}\right\rangle \sim\frac{1}{N}\bm{\Sigma}+\mathcal{O}(N^{-2})\) and that \(\bm{q}_{\infty}-\left\langle\bm{q}\right\rangle_{N}\sim\mathcal{O}(N^{-1})\). Therefore, in the leading order picture, it is safe to associate \(\bm{\Sigma}\) with the covariance of order parameters over random initializations of the network weights.

## Appendix E Propagator Structure for the full DMFT Action

In this section, we examine the propagator structure for the full DMFT action. This action is modified from other prior works [9; 46] to include the evolution of the network prediction errors \(\Delta(t)\). Those prior works noted that \(\Delta\) and the NTK \(K\) are deterministic functions of deterministic order parameters \(\{\Phi^{\ell},G^{\ell}\}\) in the \(N\to\infty\) limit so those authors did not explicitly include \(\Delta\) or \(K\) in the action. At finite width \(N\), including \(\Delta,K\) in the action is crucial as the fluctuation in prediction errors \(\Delta\) has significant consequences for dynamical fluctuations of kernels through the preactivation andpre-gradient fields. In this section, we will mainly focus on gradient flow, but we describe large step size in Appendix M.

\[S= \sum_{\ell_{\mu\nu}}\int dtds\left[\hat{\Phi}^{\ell}_{\mu\nu}(t,s) \Phi^{\ell}_{\mu\nu}(t,s)+\hat{G}^{\ell}_{\mu\nu}(t,s)G^{\ell}_{\mu\nu}(t,s)- \gamma^{2}A^{\ell}_{\nu\mu}(s,t)B^{\ell}_{\mu\nu}(t,s)\right]\] \[+\sum_{\mu}\int dt\hat{\Delta}_{\mu}(t)\left[\Delta_{\mu}(t)-y_{ \mu}+\sum_{\nu}\int ds\Theta(t-s)K_{\mu\nu}(s)\Delta_{\nu}(s)\right]\] \[+\sum_{\mu\nu}\int dt\hat{K}_{\mu\nu}(t)\left[K_{\mu\nu}(t)-\sum_{ \ell}G^{\ell+1}_{\mu\nu}(t)\Phi^{\ell}_{\mu\nu}(t)\right]\] \[+\sum_{\ell}\ln\mathcal{Z}_{\ell}[\bm{\Delta},\hat{\bm{\Phi}}^{ \ell},\hat{\bm{G}}^{\ell},\bm{\Phi}^{\ell-1},\bm{G}^{\ell+1},\bm{A}^{\ell-1}, \bm{B}^{\ell}]\] (32)

where the single site moment generating functionals \(\mathcal{Z}_{\ell}\) have the form

\[\mathcal{Z}_{\ell}= \mathbb{E}_{\{h^{\ell}_{\mu}(t),z^{\ell}_{\mu}(t)\}}\exp\left(- \sum_{\mu\nu}\int dtds\left[\phi(h^{\ell}_{\mu}(t))\phi(h^{\ell}_{\nu}(s))\hat {\Phi}^{\ell}_{\mu\nu}(t,s)+g^{\ell}_{\mu}(t)g^{\ell}_{\nu}(s)\hat{G}^{\ell}_{ \mu\nu}(t,s)\right]\right)\] \[h^{\ell}_{\mu}(t)=u^{\ell}_{\mu}(t)+\gamma\int_{0}^{t}ds\sum_{ \nu}\left[\Phi^{\ell-1}_{\mu\nu}(t,s)\Delta_{\nu}(s)+A^{\ell-1}_{\mu\nu}(t,s) \right]g^{\ell}_{\nu}(s)\;,\;\{u^{\ell}_{\mu}(t)\}\sim\mathcal{GP}(0,\bm{\Phi}^ {\ell-1})\] \[z^{\ell}_{\mu}(t)=r^{\ell}_{\mu}(t)+\gamma\int_{0}^{t}ds\sum_{ \nu}\left[G^{\ell+1}_{\mu\nu}(t,s)\Delta_{\nu}(s)+B^{\ell}_{\mu\nu}(t,s)\right] \phi(h^{\ell}_{\nu}(s))\;,\;\{r^{\ell}_{\mu}(t)\}\sim\mathcal{GP}(0,\bm{G}^{ \ell+1})\] (33)

with \(g^{\ell}_{\mu}(t)=\dot{\phi}(h^{\ell}_{\mu}(t))z^{\ell}_{\mu}(t)\). The saddle point equations give the infinite width evolution of our order parameters.

\[\frac{\partial S}{\partial\hat{\Phi}^{\ell}_{\mu\nu}(t,s)}=\Phi^{ \ell}_{\mu\nu}(t,s)-\left\langle\phi(h^{\ell}_{\mu}(t))\phi(h^{\ell}_{\nu}(s)) \right\rangle=0\] \[\frac{\partial S}{\partial\hat{G}^{\ell}_{\mu\nu}(t,s)}=G^{\ell}_ {\mu\nu}(t,s)-\langle g^{\ell}_{\mu}(t)g^{\ell}_{\nu}(s)\rangle=0\] \[\frac{\partial S}{\partial A^{\ell}_{\nu\mu}(s,t)}=-\gamma^{2}B^ {\ell}_{\mu\nu}(t,s)+\gamma\left\langle\frac{\partial\phi(h^{\ell}_{\mu}(t))}{ \partial r^{\ell}_{\nu}(s)}\right\rangle=0\] \[\frac{\partial S}{\partial B^{\ell}_{\nu\mu}(s,t)}=-\gamma^{2}A^{ \ell}_{\mu\nu}(t,s)+\gamma\left\langle\frac{\partial g^{\ell}_{\mu}(t)}{ \partial u^{\ell}_{\nu}(s)}\right\rangle=0\] \[\frac{\partial S}{\partial\hat{K}_{\mu\nu}(t)}=K_{\mu\nu}(t)-\sum _{\ell}G^{\ell+1}_{\mu\nu}(t,t)\Phi^{\ell}_{\mu\nu}(t,t)=0\] \[\frac{\partial S}{\partial\hat{\Delta}_{\mu}(t)}=\Delta_{\mu}(t)-y _{\mu}+\int_{0}^{t}ds\sum_{\nu}K_{\mu\nu}(s)\Delta_{\nu}(s)=0\] (34)

These equations exactly recover the mean field description obtained [9]. Note that \(\left\langle\right\rangle\) for field averages is an average defined by \(\mathcal{Z}_{\ell}\) and is distinct from the types averages \(\left\langle\right\rangle,\left\langle\right\rangle_{\infty}\) we have been considering over the order parameters \(\bm{q}\). The complementary set of equations for the primal variables, such as \(\frac{\partial S}{\partial\Phi^{\ell}_{\mu\nu}(t,s)}=0\), give that \(\tilde{K}=\tilde{\Delta}=\hat{\Phi}=\tilde{G}=0\) at the saddle point. We now set out to compute the Hessian \(\nabla^{2}_{\bm{q}}S\). To simplify the set of expressions, we will only explicitly write out the nonvanishing blocks. We will start with second derivatives involving only pairs of dual variables \(\{\hat{\Phi},\hat{G},A,B\}\)

\[\frac{\partial^{2}S}{\partial\hat{\Phi}^{\ell}_{\mu\nu}(t,s)\partial \hat{\Phi}^{\ell}_{\alpha\beta}(t^{\prime},s^{\prime})} =\left\langle\phi(h^{\ell}_{\mu}(t))\phi(h^{\ell}_{\nu}(s))\phi(h ^{\ell}_{\alpha}(t^{\prime}))\phi(h^{\ell}_{\beta}(s^{\prime}))\right\rangle- \Phi^{\ell}_{\mu\nu}(t,s)\Phi^{\ell}_{\alpha\beta}(t^{\prime},s^{\prime})\] \[\equiv\kappa^{\Phi^{\ell}}_{\mu\nu\alpha\beta}(t,s,t^{\prime},s^{ \prime})\] \[\frac{\partial^{2}S}{\partial\hat{G}^{\ell}_{\mu\nu}(t,s)\partial \hat{G}^{\ell}_{\alpha\beta}(t^{\prime},s^{\prime})} =\left\langle g^{\ell}_{\mu}(t)g^{\ell}_{\nu}(s)g^{\ell}_{\alpha }(t^{\prime})g^{\ell}_{\beta}(s^{\prime})\right\rangle-G^{\ell}_{\mu\nu}(t,s )G^{\ell}_{\alpha\beta}(t^{\prime},s^{\prime})\] \[\equiv\kappa^{G^{\ell}}_{\mu\nu\alpha\beta}(t,s,t^{\prime},s^{ \prime})\] \[\frac{\partial^{2}S}{\partial\hat{\Phi}^{\ell}_{\mu\nu}(t,s) \partial\hat{G}^{\ell}_{\alpha\beta}(t^{\prime},s^{\prime})} =\left\langle\phi(h^{\ell}_{\mu}(t))\phi(h^{\ell}_{\nu}(s))g^{ \ell}_{\alpha}(t^{\prime})g^{\ell}_{\beta}(s^{\prime})\right\rangle-\Phi^{ \ell}_{\mu\nu}(t,s)G^{\ell}_{\alpha\beta}(t^{\prime},s^{\prime})\] \[\equiv\kappa^{\Phi^{\ell}G^{\ell}}_{\mu\nu\alpha\beta}(t,s,t^{ \prime},s^{\prime})\] \[\frac{\partial^{2}S}{\partial\hat{\Phi}^{\ell}_{\mu\nu}(t,s) \partial A^{\ell-1}_{\beta\alpha}(s^{\prime},t^{\prime})} =-\gamma\left\langle\frac{\partial\phi(h^{\ell}_{\mu}(t))}{ \partial u^{\ell}_{\beta}(s^{\prime})}\phi(h^{\ell}_{\nu}(s))g^{\ell}_{\alpha }(t^{\prime})\right\rangle\] \[-\gamma\left\langle\phi(h^{\ell}_{\mu}(t))\frac{\partial\phi(h^ {\ell}_{\nu}(t))}{\partial u^{\ell}_{\beta}(s^{\prime})}g^{\ell}_{\alpha}(t^{ \prime})\right\rangle\] \[-\gamma\left\langle\phi(h^{\ell}_{\mu}(t))\phi(h^{\ell}_{\nu}(s) )\frac{\partial g^{\ell}_{\alpha}(t^{\prime})}{\partial u^{\ell}_{\beta}(s^{ \prime})}\right\rangle-\gamma^{2}\Phi^{\ell}_{\mu\nu}(t,s)B^{\ell-1}_{\alpha \beta}(t^{\prime},s^{\prime})\] \[\equiv-\gamma\kappa^{\Phi^{\ell}G^{\ell-1}}_{\mu\nu\alpha\beta}(t,s)\] \[\frac{\partial^{2}S}{\partial\hat{G}^{\ell}_{\mu\nu}(t,s) \partial A^{\ell-1}_{\beta\alpha}(s^{\prime},t^{\prime})} =-\gamma\left\langle\frac{\partial g^{\ell}_{\mu}(t)}{\partial u ^{\ell}_{\beta}(s^{\prime})}g^{\ell}_{\nu}(s)g^{\ell}_{\alpha}(t^{\prime}) \right\rangle-\gamma\left\langle g^{\ell}_{\mu}(t)\frac{\partial g^{\ell}_{ \nu}(t)}{\partial u^{\ell}_{\beta}(s^{\prime})}g^{\ell}_{\alpha}(t^{\prime})\right\rangle\] \[-\gamma\left\langle g^{\ell}_{\mu}(t)g^{\ell}_{\nu}(s)\frac{ \partial g^{\ell}_{\alpha}(t^{\prime})}{\partial u^{\ell}_{\beta}(s^{\prime})} \right\rangle-\gamma^{2}G^{\ell}_{\mu\nu}(t,s)B^{\ell-1}_{\alpha\beta}(t^{ \prime},s^{\prime})\] \[\equiv-\gamma\kappa^{G^{\ell}G^{\ell-1}}_{\mu\nu\alpha\beta}(t,s)\] \[\frac{\partial^{2}S}{\partial\hat{G}^{\ell}_{\mu\nu}(t,s) \partial B^{\ell}_{\beta\alpha}(s^{\prime},t^{\prime})} =-\gamma\left\langle\frac{\partial g^{\ell}_{\mu}(t)}{\partial u ^{\ell}_{\beta}(s^{\prime})}g^{\ell}_{\nu}(s)\phi(h^{\ell}_{\alpha}(t^{\prime}) )\right\rangle-\gamma\left\langle g^{\ell}_{\mu}(t)\frac{\partial g^{\ell}_{ \nu}(t)}{\partial r^{\ell}_{\beta}(s^{\prime})}\phi(h^{\ell}_{\alpha}(t^{ \prime}))\right\rangle\] \[=-\gamma\left\langle g^{\ell}_{\mu}(t)g^{\ell}_{\nu}(s)\frac{ \partial\phi(h^{\ell}_{\alpha}(t^{\prime}))}{\partial r^{\ell}_{\beta}(s^{ \prime})}\right\rangle-\gamma^{2}G^{\ell}_{\mu\nu}(t,s)A^{\ell}_{\alpha\beta}(t ^{\prime},s^{\prime})\] \[\equiv-\gamma\kappa^{G^{\ell}A^{\ell}}_{\mu\nu\alpha\beta}(t,s)\] \[\frac{\partial^{2}S}{\partial A^{\ell}_{\mu\nu}(t,s)\partial B^{ \ell}_{\beta\alpha}(s^{\prime},t^{\prime})} =-\gamma^{2}\delta_{\mu\alpha}\delta_{\nu\beta}\delta(t-t^{ \prime})\delta(s-s^{\prime})\] \[\frac{\partial^{2}S}{\partial A^{\ell-1}_{\nu\mu}(s,t)\partial B^{ \ell}_{\beta\alpha}(s^{\prime},t^{\prime})} =\gamma^{2}\left\langle\frac{\partial^{2}}{\partial u^{\ell}_{ \nu}(s)\partial r^{\ell}_{\beta}(s^{\prime})}\left[g^{\ell}_{\mu}(t)\phi(h^{ \ell}_{\alpha}(t^{\prime}))\right]\right\rangle-\gamma^{4}B^{\ell-1}_{\mu\nu}( t,s)A^{\ell}_{\alpha\beta}(t^{\prime},s^{\prime})\] \[\equiv\kappa^{B^{\ell-1}A^{\ell}}_{\mu\nu\alpha\beta}(t,s,t^{ \prime},s^{\prime})\] (35)Next, we consider the second derivatives involving only primal variables \(\{\Phi^{\ell},G^{\ell},K,\Delta\}\) which all vanish

\[\frac{\partial^{2}S}{\partial\Phi^{\ell}_{\mu\nu}(t,s)\partial\Phi^ {\ell^{\prime}}_{\alpha\beta}(t^{\prime},s^{\prime})} =0\] \[\frac{\partial^{2}S}{\partial G^{\ell}_{\mu\nu}(t,s)\partial G^ {\ell^{\prime}}_{\alpha\beta}(t^{\prime},s^{\prime})} =0\] \[\frac{\partial^{2}S}{\partial\Phi^{\ell}_{\mu\nu}(t,s)\partial G^ {\ell^{\prime}}_{\alpha\beta}(t^{\prime},s^{\prime})} =0\] \[\frac{\partial^{2}S}{\partial\Phi^{\ell}_{\mu\nu}(t,s)\partial K _{\alpha\beta}(s^{\prime})} =0\] \[\frac{\partial^{2}S}{\partial G^{\ell}_{\mu\nu}(t,s)\partial K _{\alpha\beta}(s^{\prime})} =0\] \[\frac{\partial^{2}S}{\partial\Phi^{\ell}_{\mu\nu}(t,s)\partial \Delta_{\alpha}(s^{\prime})} =0\] \[\frac{\partial^{2}S}{\partial G^{\ell}_{\mu\nu}(t,s)\partial \Delta_{\alpha}(s^{\prime})} =0\] \[\frac{\partial^{2}S}{\partial K_{\mu\nu}(t)\partial K_{\alpha \beta}(s)} =0\] \[\frac{\partial^{2}S}{\partial K_{\mu\nu}(t)\partial\Delta_{ \alpha}(s)} =0\] \[\frac{\partial^{2}S}{\partial\Delta_{\mu}(t)\partial\Delta_{ \alpha}(s)} =0\] (36)

Now we consider all derivatives which involve one of the dual variables \(\{\hat{\Phi}^{\ell},\hat{G}^{\ell},A^{\ell},B^{\ell}\}\) and the primal variable \(\Delta\)

\[\frac{\partial^{2}S}{\partial\hat{\Phi}^{\ell}_{\mu\nu}(t,s) \partial\Delta_{\alpha}(t^{\prime})} =-\left\langle\frac{\partial}{\partial\Delta_{\alpha}(t^{\prime} )}[\phi(h^{\ell}_{\mu}(t))\phi(h^{\ell}_{\nu}(s))]\right\rangle\equiv-D^{ \Phi^{\ell}\Delta}_{\mu\nu\alpha}(t,s,t^{\prime})\] \[\frac{\partial^{2}S}{\partial\hat{G}^{\ell}_{\mu\nu}(t,s) \partial\Delta_{\alpha}(t^{\prime})} =-\left\langle\frac{\partial}{\partial\Delta_{\alpha}(t^{\prime })}[g^{\ell}_{\mu}(t)g^{\ell}_{\nu}(s)]\right\rangle\equiv-D^{\sigma^{\ell} \Delta}_{\mu\nu\alpha}(t,s,t^{\prime})\] \[\frac{\partial^{2}S}{\partial A^{\ell-1}_{\nu\mu}(s,t)\partial \Delta_{\alpha}(t^{\prime})} =\gamma\left\langle\frac{\partial}{\partial\Delta_{\alpha}(t^{ \prime})\partial u^{\ell}_{\nu}(s)}g^{\ell}_{\mu}(t)\right\rangle\equiv\gamma D ^{B^{\ell-1},\Delta}_{\mu\nu\alpha}(t,s,t^{\prime})\] \[\frac{\partial^{2}S}{\partial B^{\ell}_{\nu\mu}(s,t)\partial \Delta_{\alpha}(t^{\prime})} =\gamma\left\langle\frac{\partial}{\partial\Delta_{\alpha}(t^{ \prime})\partial r^{\ell}_{\nu}(s)}\phi(h^{\ell}_{\mu}(t))\right\rangle\equiv \gamma D^{A^{\ell}\Delta}_{\mu\nu\alpha}(t,s,t^{\prime})\]Now, we consider the second derivatives involving one derivative on a dual variable \(\{\hat{\Phi}^{\ell},\hat{G}^{\ell},A,B\}\) and one of the primal variables \(\{\Phi^{\ell},G^{\ell}\}\).

\[\frac{\partial^{2}S}{\partial\hat{\Phi}^{\ell}_{\mu\nu}(t,s)\partial \Phi^{\ell^{\prime}}_{\alpha\beta}(t^{\prime},s^{\prime})} =\delta_{\ell,\ell^{\prime}}\delta_{\mu\nu}\delta(t-t^{\prime}) \delta(s-s^{\prime})\] \[-\delta_{\ell-1,\ell^{\prime}}\frac{\partial}{\partial\Phi^{\ell -1}_{\alpha\beta}(t^{\prime},s^{\prime})}\left<\phi(h^{\ell}_{\mu}(t))\phi(h^ {\ell}_{\nu}(s))\right>\] \[\equiv\delta_{\ell,\ell^{\prime}}\delta_{\mu\nu}\delta(t-t^{ \prime})\delta(s-s^{\prime})-\delta_{\ell-1,\ell^{\prime}}D^{\Phi^{\ell}\Phi^ {\ell-1}_{\mu\nu\alpha\beta}}_{\mu\nu\alpha\beta}(t,s,t^{\prime},s^{\prime})\] \[\frac{\partial^{2}S}{\partial\hat{G}^{\ell}_{\mu\nu}(t,s) \partial G^{\ell^{\prime}}_{\alpha\beta}(t^{\prime},s^{\prime})} =\delta_{\ell,\ell^{\prime}}\delta_{\mu\nu}\delta(t-t^{\prime}) \delta(s-s^{\prime})-\delta_{\ell+1,\ell^{\prime}}\frac{\partial}{\partial G^ {\ell+1}_{\alpha\beta}(t^{\prime},s^{\prime})}\left<g^{\ell}_{\mu}(t)g^{\ell }_{\nu}(s)\right>\] \[\equiv\delta_{\ell,\ell^{\prime}}\delta_{\mu\nu}\delta(t-t^{ \prime})\delta(s-s^{\prime})-\delta_{\ell-1,\ell^{\prime}}D^{G^{\ell}G^{\ell+1 }_{\mu\nu\alpha\beta}}_{\mu\nu\alpha\beta}(t,s,t^{\prime},s^{\prime})\] \[\frac{\partial^{2}S}{\partial\hat{\Phi}^{\ell}_{\mu\nu}(t,s) \partial G^{\ell+1}_{\alpha\beta}(t^{\prime},s^{\prime})} =-\frac{\partial}{\partial G^{\ell+1}_{\alpha\beta}(t)}\left< \phi(h^{\ell}_{\mu}(t))\phi(h^{\ell}_{\nu}(s))\right>\equiv-D^{\Phi^{\ell^{ \prime},G^{\ell+1}}}_{\mu\nu\alpha\beta}(t,s,t^{\prime},s^{\prime})\] \[\frac{\partial^{2}S}{\partial G^{\ell}_{\mu\nu}(t,s)\partial\Phi^ {\ell-1}_{\alpha\beta}(t^{\prime},s^{\prime})} =-\frac{\partial}{\partial\Phi^{\ell-1}_{\alpha\beta}(t^{\prime },s^{\prime})}\left<g^{\ell}_{\mu}(t)g^{\ell}_{\nu}(s)\right>\equiv-D^{G^{\ell },\Phi^{\ell-1}}_{\mu\nu\alpha\beta}(t,s,t^{\prime},s^{\prime})\] \[\frac{\partial^{2}S}{\partial A^{\ell-1}_{\nu\mu}(s,t)\partial \Phi^{\ell-1}_{\alpha\beta}(t^{\prime},s^{\prime})} =\gamma\frac{\partial}{\partial\Phi^{\ell-1}_{\alpha\beta}(t^{ \prime},s^{\prime})}\left<\frac{\partial g^{\ell}_{\mu}(t)}{\partial r^{\ell}_ {\nu}(s)}\right>\equiv\gamma D^{B^{\ell-1},G^{\ell+1}}_{\mu\nu\alpha\beta}(t,s,t^{\prime},s^{\prime})\] \[\frac{\partial^{2}S}{\partial B^{\ell}_{\nu\mu}(s,t)\partial G^{ \ell+1}_{\alpha\beta}(t^{\prime},s^{\prime})} =\gamma\frac{\partial}{\partial G^{\ell+1}_{\alpha\beta}(t^{ \prime},s^{\prime})}\left<\frac{\partial\phi(h^{\ell}_{\mu}(t))}{\partial u^{ \ell}_{\nu}(s)}\right>\equiv\gamma D^{A^{\ell},G^{\ell+1}}_{\mu\nu\alpha\beta} (t,s,t^{\prime},s^{\prime})\] \[\frac{\partial^{2}S}{\partial A^{\ell-1}_{\nu\mu}(s,t)\partial G^ {\ell+1}_{\alpha\beta}(t^{\prime},s^{\prime})} =\gamma\frac{\partial}{\partial G^{\ell+1}_{\alpha\beta}(t^{ \prime},s^{\prime})}\left<\frac{\partial g^{\ell}_{\mu}(t)}{\partial r^{\ell}_ {\nu}(s)}\right>\equiv\gamma D^{B^{\ell-1},G^{\ell+1}}_{\mu\nu\alpha\beta}(t,s,t^{\prime},s^{\prime})\] \[\frac{\partial^{2}S}{\partial B^{\ell}_{\nu\mu}(s,t)\partial G^{ \ell+1}_{\alpha\beta}(t^{\prime},s^{\prime})} =\gamma\frac{\partial}{\partial G^{\ell+1}_{\alpha\beta}(t^{ \prime},s^{\prime})}\left<\frac{\partial\phi(h^{\ell}_{\mu}(t))}{\partial u^{ \ell}_{\nu}(s)}\right>\equiv\gamma D^{A^{\ell},G^{\ell+1}}_{\mu\nu\alpha\beta}(t,s,t^{\prime},s^{\prime})\] (37)

We note that terms such as \(\frac{\partial}{\partial\Phi^{\ell-1}_{\alpha\beta}(t^{\prime},s^{\prime})} \left<\phi(h^{\ell}_{\mu}(t))\phi(h^{\ell}_{\nu}(s))\right>\) can be further decomposed since the average over the \(\{u^{\ell}_{\mu}(t)\}\sim\mathcal{GP}(0,\mathbf{\Phi}^{\ell-1})\) and \(h^{\ell}\)'s explicit dynamics both depend on \(\Phi^{\ell-1}\)

\[\frac{\partial}{\partial\Phi^{\ell-1}_{\alpha\beta}(t^{\prime},s^{ \prime})}\left<\phi(h^{\ell}_{\mu}(t))\phi(h^{\ell}_{\nu}(s))\right> =\frac{1}{2}\left<\frac{\partial^{2}}{\partial u^{\ell}_{\alpha \alpha}(t^{\prime})\partial u^{\ell}_{\beta}(s^{\prime})}\phi(h^{\ell}_{\mu}(t)) \phi(h^{\ell}_{\nu}(s))\right>\] \[+\left<\frac{\partial}{\partial\Phi^{\ell-1}_{\alpha\beta}(t^{ \prime},s^{\prime})}\phi(h^{\ell}_{\mu}(t))\phi(h^{\ell}_{\nu}(s))\right>\] (38)

where the first term comes from differentiating the Gaussian probability density for \(u^{\ell}\) (e.g. Price's theorem) and the second term is an explicit derivative of the preactivation fields with \(u^{\ell}\) treated as constant. Next we consider the nonvanishing terms which involve \(\{\hat{\Delta},\hat{K},\Delta,K\}\) which give

\[\frac{\partial^{2}S}{\partial\hat{\Delta}_{\mu}(t)\partial\Delta_ {\alpha}(s)} =\delta_{\mu\alpha}\delta(t-s)+\Theta(t-s)K_{\mu\alpha}(s)\] \[\frac{\partial^{2}}{\partial\hat{\Delta}_{\mu}(t)\partial K_{ \alpha\beta}(s)} =\delta_{\mu\alpha}\Theta(t-s)\Delta_{\beta}(s)\] \[\frac{\partial^{2}S}{\partial\hat{K}_{\mu\nu}(t)\partial K_{ \alpha\beta}(t^{\prime})} =\delta_{\mu\alpha}\delta_{\nu\beta}\delta(t-t^{\prime})\] \[\frac{\partial^{2}S}{\partial\hat{K}_{\mu\nu}(t)\partial\Phi^{ \ell}_{\alpha\beta}(t^{\prime},s^{\prime})} =\delta_{\mu\alpha}\delta_{\nu\beta}G^{\ell+1}_{\alpha\beta}(t^{ \prime},s^{\prime})\delta(t-t^{\prime})\delta(t-s^{\prime})\] \[\frac{\partial^{2}S}{\partial\hat{K}_{\mu\nu}(t)\partial G^{ \ell}_{\alpha\beta}(t^{\prime},s^{\prime})} =\delta_{\mu\alpha}\delta_{\nu\beta}\Phi^{\ell-1}_{\alpha\beta}(t^{ \prime},s^{\prime})\delta(t-t^{\prime})\delta(t-s^{\prime})\] (39)This enumerates all possible non-vanishing terms in the Hessian. We can now construct a block matrix of these Hessians by partitioning our order parameters \(\bm{q}=[\bm{q}_{1},\bm{q}_{2}]^{\top}\) where

\[\bm{q}_{1} =\text{Vec}\{\Phi^{\ell}_{\mu\nu}(t,s),G^{\ell}_{\mu\nu}(t,s),K_{\mu \nu}(t),\Delta_{\mu}(t),\hat{\Phi}^{\ell}_{\mu\nu}(t,s),\hat{G}^{\ell}_{\mu\nu}( t,s),\hat{K}_{\mu\nu}(t),\hat{\Delta}_{\mu}(t)\}\] (40) \[\bm{q}_{2} =\text{Vec}\{A^{\ell}_{\mu\nu}(t,s),B^{\ell}_{\mu\nu}(t,s)\}.\] (41)

This choice will become apparent shortly.

\[\nabla^{2}_{\bm{q}}S=\begin{bmatrix}\nabla^{2}_{\bm{q}_{1}}S&\nabla^{2}_{\bm{ q}_{2}\bm{q}_{2}}S\\ \nabla^{2}_{\bm{q}_{2}\bm{q}_{1}}S&\nabla^{2}_{\bm{q}_{2}}S\end{bmatrix}\] (42)

To calculate the full propagator \(\bm{\Sigma}=-\left[\nabla^{2}_{\bm{q}}S\right]^{-1}\), we will assume invertibility of the upper block \(\bm{\Sigma}^{0}=-\left[\nabla^{2}_{\bm{q}_{1}}S\right]^{-1}\) and use this in the Schur complement

\[\bm{\Sigma} =-\left[\nabla^{2}_{\bm{q}}S\right]^{-1}=\begin{bmatrix}\bm{ \Sigma}_{11}&\bm{\Sigma}_{12}\\ \bm{\Sigma}_{21}&\bm{\Sigma}_{22}\end{bmatrix}\] \[\bm{\Sigma}_{11} =\bm{\Sigma}^{0}-\bm{\Sigma}^{0}\left[\nabla^{2}_{\bm{q}_{1}\bm{ q}_{2}}S\right]\left(\nabla^{2}_{\bm{q}_{2}}S+(\nabla^{2}_{\bm{q}_{2}\bm{q}_{1}}S) \bm{\Sigma}^{0}(\nabla^{2}_{\bm{q}_{1}\bm{q}_{2}}S)\right)^{-1}\left[\nabla^{ 2}_{\bm{q}_{2}\bm{q}_{1}}S\right]\bm{\Sigma}^{0}\] \[\bm{\Sigma}_{12} =\bm{\Sigma}^{\top}_{21}=-\bm{\Sigma}^{0}\left[\nabla^{2}_{\bm{ q}_{1}\bm{q}_{2}}S\right]\left(\nabla^{2}_{\bm{q}_{2}}S+(\nabla^{2}_{\bm{q}_{2}\bm{q}_{1}} S)\bm{\Sigma}^{0}(\nabla^{2}_{\bm{q}_{1}\bm{q}_{2}}S)\right)^{-1}\] \[\bm{\Sigma}_{22} =-\left(\nabla^{2}_{\bm{q}_{2}}S+(\nabla^{2}_{\bm{q}_{2}\bm{q}_{ 1}}S)\bm{\Sigma}^{0}(\nabla^{2}_{\bm{q}_{1}\bm{q}_{2}}S)\right)^{-1}\] (43)

We now need to solve for \(\bm{\Sigma}^{0}=-\left[\nabla^{2}_{\bm{q}_{1}}S\right]^{-1}\). To perform this inverse, we again partition \(\bm{q}_{1}\) into two sets of order parameters \(\bm{q}_{1}=[\bm{q}_{1}^{1},\bm{q}_{1}^{2}]\) where \(\bm{q}_{1}^{1}=\text{Vec}\{\Phi^{\ell}_{\mu\nu}(t,s),G^{\ell}_{\mu\nu}(t,s),K_ {\mu\nu}(t,s),\Delta_{\mu}(t)\}\) and \(\bm{q}_{1}^{2}=\text{Vec}\{\hat{\Phi}^{\ell}_{\mu\nu}(t,s),\hat{G}^{\ell}_{\mu \nu}(t,s),\hat{K}_{\mu\nu}(t,\hat{\Delta}_{\mu}(t)\}\)

\[\nabla^{2}_{\bm{q}_{1}}S=\begin{bmatrix}\bm{0}&\bm{U}^{\top}\\ \bm{U}&\bm{\kappa}\end{bmatrix}\;,\;\bm{\kappa}\equiv\nabla^{2}_{\bm{q}_{1}^{ \top}\bm{q}_{1}^{1}}S\] (44)

We seek a physically sensible inverse where the variance of \(\bm{q}_{1}^{2}\) is vanishing [51, 53]. This leads to the following sub-propagator \(\bm{\Sigma}^{0}\)

\[\bm{\Sigma}^{0}=-[\nabla^{2}_{\bm{q}_{1}}S]^{-1}=\begin{bmatrix}\bm{U}^{-1} \bm{\kappa}[\bm{U}^{-1}]^{\top}&-\bm{U}^{-1}\\ -[\bm{U}^{\top}]^{-1}&\bm{0}\end{bmatrix}\] (45)

Thus given \(\bm{\kappa},\bm{U}\), we can solve for \(\bm{\Sigma}^{0}\) and ultimately for the full propagator \(\bm{\Sigma}\). The relevant entries in \(\bm{\kappa}\) and \(\bm{U}\) are given by those second derivatives calculated above. We note that each of the field derivatives needed for \(\bm{U}\) can be computed implicitly from the field dynamics. For example, for the \(\Delta_{\mu}(t)\) derivatives we have

\[\frac{\partial}{\partial\Delta_{\nu^{\prime}}(t^{\prime})}h^{ \ell}_{\mu}(t) =\gamma\Theta(t-t^{\prime})\Phi^{\ell-1}_{\mu\nu^{\prime}}(t,t^{ \prime})g^{\ell}_{\nu}(t^{\prime})\] \[+\gamma\int_{0}^{t}ds\sum_{\nu}\left[A^{\ell-1}_{\mu\nu}(t,s)+ \Phi^{\ell-1}_{\mu\nu}(t,s)\Delta_{\nu}(s)\right]\frac{\partial g^{\ell}_{\nu}(s )}{\partial\Delta_{\nu^{\prime}}(t^{\prime})}\] \[\frac{\partial}{\partial\Delta_{\nu^{\prime}}(t^{\prime})}z^{ \ell}_{\mu}(t) =\gamma\Theta(t-t^{\prime})G^{\ell+1}_{\mu\nu}(t,t^{\prime})\phi(h ^{\ell}_{\nu}(t^{\prime}))\] \[+\gamma\int_{0}^{t}ds\sum_{\nu}\left[B^{\ell}_{\mu\nu}(t,s)+G^{ \ell+1}_{\mu\nu}(t,s)\Delta_{\nu}(s)\right]\frac{\partial\phi(h^{\ell}_{\nu}(s ))}{\partial\Delta_{\nu^{\prime}}(t^{\prime})}\] (46)

These can then be used in the averages such as \(\left\langle\frac{\partial}{\partial\Delta_{\nu^{\prime}}(t^{\prime})}\phi(h^{ \ell}_{\mu}(t))\phi(h^{\ell}_{\nu}(s))\right\rangle\). Similarly, we can compute terms such as \(\frac{\partial h^{\ell}_{\mu}(t)}{\partial\Phi^{\ell}_{\alpha\beta(t^{\prime},s^ {\prime})}}\) through the following closed equations

\[\frac{\partial h^{\ell}_{\mu}(t)}{\partial\Phi^{\ell-1}_{\alpha \beta}(t^{\prime},s^{\prime})} =\gamma\delta(t-t^{\prime})\delta_{\mu\alpha}\Theta(t-s^{\prime}) \Delta_{\beta}(s^{\prime})\] \[+\gamma\int_{0}^{t}ds\sum_{\nu}\left[A^{\ell-1}_{\mu\nu}(t,s)+ \Delta_{\nu}(s)\Phi^{\ell-1}_{\mu\nu}(t,s)\right]\frac{\partial g^{\ell}_{\nu}(s )}{\partial\Phi^{\ell}_{\alpha\beta}(t^{\prime},s^{\prime})}\] \[\frac{\partial z^{\ell}_{\mu}(t)}{\partial\Phi^{\ell-1}_{\alpha \beta}(t^{\prime},s^{\prime})} =\gamma\int_{0}^{t}ds\sum_{\nu}\left[B^{\ell}_{\mu\nu}(t,s)+ \Delta_{\nu}(s)G^{\ell+1}_{\mu\nu}(t,s)\right]\frac{\partial\phi(h^{\ell}_{\nu}( s))}{\partial\Phi^{\ell-1}_{\alpha\beta}(t^{\prime},s^{\prime})}\] (47)

These terms can then be used to compute quantities like \(D^{\Phi^{\ell}}\).

Solving for the Propagator

In this section we sketch out the required steps to obtain the propagator \(\bm{\Sigma}\).

* Step 1: Solve the infinite width DMFT equations for \(q_{\infty}\) which include the prediction error dynamics \(\Delta_{\mu}(t)\), the feature kernels \(\Phi^{\ell}_{\mu\nu}(t,s)\), gradient kernels \(G^{\ell}_{\mu\nu}(t,s)\). This step corresponds to algorithm in Bordelon & Pehlevan [22] and defines the dynamics one would expect at infinite width [9]. See below for more detail.
* Step 2: Compute the entries of the Hessian of \(S\) evaluated at the \(q_{\infty}\) computed in the first step. Some of these entries look like fourth cumulants of features like \(\kappa=\left\langle\phi(h)^{4}\right\rangle-\left\langle\phi(h)^{2}\right\rangle ^{2}\) and some of them measure sensitivity of one order parameter to a perturbation in another order parameter \(D^{\Phi^{\ell}}=\frac{\partial}{\partial\Phi^{\ell-1}}\left\langle\phi(h^{ \ell})^{2}\right\rangle\). The averages \(\left\langle\right\rangle\) used to calculate \(\kappa\) and \(D^{\Phi^{\ell}}\) should be performed over the infinite width stochastic processes for preactivations \(h^{\ell}\) which are defined in equation (19).
* Step 3: After populating the entries of the block matrix for the Hesssian \(\nabla^{2}S\), we then calculate the propagator \(\Sigma\) with a matrix inversion. Since we discretized time, this is a finite dimensional matrix.

The step 1 above demands a solution to the infinite width DMFT equations (solving for the saddle point \(\bm{q}_{\infty}\)). We will now give a detailed set of instructions about how the infinite width limit for \(q_{\infty}\) is solved (step 1 above). This corresponds to the algorithm of Bordelon & Pehlevan 2022 to solve the saddle point equations \(\frac{\partial}{\partial\bm{q}}S(\bm{q})|_{\bm{q}_{\infty}}=0\)[9].

* Step 1: Start with a guess for the kernels \(\Phi^{\ell}_{\mu\nu}(t,s),G^{\ell}_{\mu\nu}(t,s)\) and for the predictions through time \(f_{\mu}(t)\). We usually use the lazy limit (e.g. \(\Phi^{\ell}_{\mu\nu}(t,s)=\Phi^{\ell}_{\mu\nu}(0,0)\)...) as an initial guess.
* Step 2: Sample Gaussian sources \(u^{\ell}_{\mu}(t)\) and \(r^{\ell}_{\mu}(t)\) based on the current covariances \(\Phi^{\ell}\) and \(G^{\ell}\).
* Step 3: For each sample, solve integral equations for \(h(t)\) and \(z(t)\). \[h^{\ell}_{\mu}(t) =u^{\ell}_{\mu}(t)+\gamma\int_{0}^{t}ds\sum_{\nu}[A^{\ell-1}_{ \mu\nu}(t,s)+\Phi^{\ell-1}_{\mu\nu}(t,s)][\dot{\phi}(h^{\ell}_{\nu}(s))z^{\ell }_{\nu}(s)]\] \[z^{\ell}_{\mu}(t) =r^{\ell}_{\mu}(t)+\gamma\int_{0}^{t}ds\sum_{\nu}[B^{\ell}_{\mu \nu}(t,s)+G^{\ell+1}_{\mu\nu}(t,s)]\phi(h^{\ell}_{\nu}(s))\] (48) These will be samples from the single site distribution for \(h,z\)
* Step 4: Average over the Monte Carlo samples to produce a new estimate of the kernels: \(\Phi^{\ell}(t,s)=\left\langle\phi(h^{\ell}(t))\phi(h^{\ell}(s))\right\rangle\). A similar procedure is performed for \(G^{\ell}\) and the response functions \(A^{\ell},B^{\ell}\).
* Step 5: Compute the NTK estimate \(K(t)=\sum_{\ell}G^{\ell+1}(t,t)\Phi^{\ell}(t,t)\) and then integrate prediction dynamics from the dynamics of the NTK \(\frac{d}{dt}f_{\mu}(t)=\sum_{\nu}K_{\mu\nu}(t)\Delta_{\nu}(t)\).
* Repeat steps 2-5 until the order parameters converge.

Below we provide a pseudocode algorithm to solve for the propagator elements.

``` Data:\(\bm{K}^{x}\), \(\bm{y}\), Initial Guesses \(\{\bm{\Phi}^{\ell},\bm{G}^{\ell}\}_{\ell=1}^{L}\), \(\{\bm{A}^{\ell},\bm{B}^{\ell}\}_{\ell=1}^{L-1}\), Sample count \(\mathcal{S}\), Update Speed \(\beta\) Result: Propagator Matrix \(\bm{\Sigma}\)
1 Solve DMFT equations with Algorithm 2 for order parameters \(f_{\mu}(t),\Phi^{\ell}_{\mu\alpha}(t,s),...\) ;
2 Draw \(\mathcal{S}\) samples \(\{u^{\ell}_{\mu,n}(t)\}_{n=1}^{\mathcal{S}}\sim\mathcal{GP}(0,\bm{\Phi}^{ \ell-1})\), \(\{r^{\ell}_{\mu,n}(t)\}_{n=1}^{\mathcal{S}}\sim\mathcal{GP}(0,\bm{G}^{\ell+1})\);
3 Integrate dynamics for each sample to get \(\{h^{\ell}_{\mu,n}(t),z^{\ell}_{\mu,n}(t)\}_{n=1}^{\mathcal{S}}\);
4 Estimate \(\kappa\) functions with Monte Carlo integration, for instance \(\kappa^{\Phi^{\ell}_{\mu\nu\alpha\beta}}_{\mu\nu\alpha\beta}(t,s,t^{\prime},s^ {\prime})=\) \(\frac{1}{S}\sum_{n\in[\mathcal{S}]}\phi(h^{\ell}_{\mu,n}(t))\phi(h^{\ell}_{ \nu,n}(s))\phi(h^{\ell}_{\alpha,n}(t^{\prime}))\phi(h^{\ell}_{\beta,n}(s^{ \prime}))-\Phi^{\ell}_{\mu\nu}(t,s)\Phi^{\ell}_{\alpha\beta}(t^{\prime},s^{ \prime})\) ;
5 For each sample, compute field sensitivities to error signals, such as \(\frac{\partial h^{\ell}_{\mu,n}(t)}{\partial\Delta_{\nu}(s)}\), and kernels \(\frac{\partial h^{\ell}_{\mu,n}(t)}{\partial\Phi^{\ell}_{\alpha\beta}(t^{ \prime},s^{\prime})}\) implicitly using equations (46) (47) ;
6 Use these sensitivities to compute the necessary \(D\) tensors such as \(D^{\Phi^{\ell}\Delta}_{\mu\nu\alpha}=\frac{1}{S}\sum_{n\in[\mathcal{S}]}\frac {\partial}{\partial\Delta_{\alpha}(t^{\prime})}\left[\phi(h^{\ell}_{\mu,n}(t)) \phi(h^{\ell}_{\nu,n}(s))\right]\);
7 Invert \(\bm{U}\) matrix and compute \(\bm{\Sigma}_{0}\) in equation (45);
8 Compute the Schur-complement in equation (43) to handle the response functions ;
``` Data:\(\bm{K}^{x}\), \(\bm{y}\), Initial Guesses \(\{\bm{\Phi}^{\ell},\bm{G}^{\ell}\}_{\ell=1}^{L}\), \(\{\bm{A}^{\ell},\bm{B}^{\ell}\}_{\ell=1}^{L-1}\), Sample count \(\mathcal{S}\), Update Speed \(\beta\) Result: Final Kernels \(\{\bm{\Phi}^{\ell},\bm{G}^{\ell}\}_{\ell=1}^{L}\), \(\{\bm{A}^{\ell},\bm{B}^{\ell}\}_{\ell=1}^{L-1}\), Network predictions through training \(f_{\mu}(t)\)
1\(\bm{\Phi}^{0}=\bm{K}^{x}\otimes\bm{11}^{\top}\), \(\bm{G}^{L+1}=\bm{11}^{\top}\) ;
2whileKernels Not Convergeddo
3 From \(\{\bm{\Phi}^{\ell},\bm{G}^{\ell}\}\) compute \(\bm{K}^{NTR}(t,t)\) and solve \(\frac{d}{dt}f_{\mu}(t)=\sum_{\alpha}\Delta_{\alpha}(t)K^{NTR}_{\mu\alpha}(t,t)\);
4\(\ell=1\);
5while\(\ell<L+1\)do
6 Draw \(\mathcal{S}\) samples \(\{u_{\mu,n}^{\ell}(t)\}_{n=1}^{\mathcal{S}}\sim\mathcal{GP}(0,\bm{\Phi}^{\ell -1})\), \(\{r_{\mu,n}^{\ell}(t)\}_{n=1}^{\mathcal{S}}\sim\mathcal{GP}(0,\bm{G}^{\ell+1})\);
7 Integrate dynamics for each sample to get \(\{h_{\mu,n}^{\ell}(t),z_{\mu,n}^{\ell}(t)\}_{n=1}^{\mathcal{S}}\);
8 Compute new \(\bm{\Phi}^{\ell}\), \(\bm{G}^{\ell}\) estimates:
9\(\tilde{\Phi}_{\mu\alpha}^{\ell}(t,s)=\frac{1}{\mathcal{S}}\sum_{n\in[\mathcal{S }]}\phi(h_{\mu,n}^{\ell}(t))\phi(h_{\alpha,n}^{\ell}(s))\), \(\tilde{G}_{\mu\alpha}^{\ell}(t,s)=\frac{1}{\mathcal{S}}\sum_{n\in[\mathcal{S }]}g_{\mu,n}^{\ell}(t)g_{\alpha,n}^{\ell}(s)\) ;
10 Solve for Jacobians on each sample \(\frac{\partial\phi(\bm{h}_{\mu}^{\ell})}{\partial\bm{r}_{\mu}^{\ell-1}}\), \(\frac{\partial g_{\mu}^{\ell}}{\partial\bm{u}_{\mu}^{\ell+}}\) ;
11 Compute new \(\bm{A}^{\ell},\bm{B}^{\ell-1}\) estimates:
12\(\tilde{\bm{A}}^{\ell}=\frac{1}{\mathcal{S}}\sum_{n\in[\mathcal{S}]}\frac{ \partial\phi(\bm{h}_{\mu}^{\ell})}{\partial\bm{r}_{n}^{\ell+1}}\), \(\tilde{\bm{B}}^{\ell-1}=\frac{1}{\mathcal{S}}\sum_{n\in[\mathcal{S}]}\frac{ \partial g_{\mu}^{\ell}}{\partial\bm{u}_{\mu}^{\ell+}}\) ;
13\(\ell\leftarrow\ell+1\);
14
15 end while
16\(\ell=1\);
17while\(\ell<L+1\)do
18 Update feature kernels: \(\bm{\Phi}^{\ell}\leftarrow(1-\beta)\bm{\Phi}^{\ell}+\beta\tilde{\bm{\Phi}}^{ \ell}\), \(\bm{G}^{\ell}\leftarrow(1-\beta)\bm{G}^{\ell}+\beta\tilde{\bm{G}}^{\ell}\) ;
19if\(\ell<L\)then
20 Update \(\bm{A}^{\ell}\leftarrow(1-\beta)\bm{A}^{\ell}+\beta\tilde{\bm{A}}^{\ell}\), \(\bm{B}^{\ell}\leftarrow(1-\beta)\bm{B}^{\ell}+\beta\tilde{\bm{B}}^{\ell}\)
21 end if
22\(\ell\leftarrow\ell+1\)
23
24 end while
25
26 end while return\(\{\bm{\Phi}^{\ell},\bm{G}^{\ell}\}_{\ell=1}^{L},\{\bm{A}^{\ell},\bm{B}^{\ell}\}_{ \ell=1}^{L-1},\{f_{\mu}(t)\}_{\mu=1}^{P}\) ```

**Algorithm 2**Alternating Monte Carlo Solution to Saddle Point Equations

## Appendix G Leading Correction to the Mean Order Parameters

In this section we use the propagator structure derived in the last section to reason about the leading finite size correction to \(\langle\bm{q}\rangle\) at width \(N\). Letting the indices \(i,j,k,n\) enumerate all entries of the order parameters in \(\bm{q}\) (technically this is a sum over samples and an integral over time for gradient flow), we find the leading Pade Approximant for the mean has the form (App D)

\[\langle q_{i}-q_{i}^{\infty}\rangle_{N} =\frac{N\left\langle(q_{i}-q_{i}^{\infty})V\right\rangle_{\infty }+\frac{N^{2}}{2}\left\langle(q_{i}-q_{i}^{\infty})V^{2}\right\rangle_{\infty} \cdots}{1+N\left\langle V\right\rangle_{\infty}+\frac{N^{2}}{2}\left\langle V^{2 }\right\rangle_{\infty}+...}\] \[\sim\frac{1}{3!N}\sum_{jkl}\frac{\partial^{3}S}{\partial q_{j} \partial q_{k}\partial q_{l}}\left\langle\delta_{i}\delta_{j}\delta_{k}\delta_ {l}\right\rangle_{\infty}+\mathcal{O}(N^{-2}).\] (49) \[=\frac{1}{2N}\sum_{jkl}\frac{\partial^{3}S}{\partial q_{j} \partial q_{k}\partial q_{l}}\Sigma_{ij}\Sigma_{kl}+\mathcal{O}(N^{-2})\] (50)

where \(\delta_{j}=\sqrt{N}(q_{j}-q_{j}^{\infty})\) and the derivatives are computed at the saddle point. In the last line, we utilized Wick's theorem and the permutation symmetry of the third derivative \(\frac{\partial^{3}S}{\partial q_{j}\partial q_{j}\partial q_{k}}\) to evaluate the four point averages in terms of the propagator \(\Sigma_{ij}\), which was provided in the preceding section E. In practice computing even the full set of second derivatives for the DMFT action to get \(\Sigma\) is quite challenging. Despite the challenge of computing the mean order parameter correction, these corrections are relevant in practice and crucially distinguish the training timescales of deep networks at different widths as we show in Figures 7 and A.4.

### Correction to Mean Predictions and Full MSE Correction

Supposing that we solved for the propagator \(\bm{\Sigma}\), using the formalism in the preceeding section, we can compute the \(\mathcal{O}(N^{-1})\) correction to the average network prediction error due to finite size. We let \(\left\langle\bm{\Delta}(t)\right\rangle\) represent the average of errors over an ensemble of width \(N\) networks.

\[\frac{d}{dt}\left\langle\Delta_{\mu}(t)\right\rangle =-\sum_{\nu}\left\langle K_{\mu\nu}(t)\Delta_{\nu}(t)\right\rangle\] \[=-\sum_{\nu}\left\langle K_{\mu\nu}(t)\right\rangle\left\langle \Delta_{\nu}(t)\right\rangle-\sum_{\nu}\text{Cov}\left(K_{\mu\nu}(t),\Delta_{ \nu}(t)\right)\] \[\sim-\sum_{\nu}\left\langle K_{\mu\nu}(t)\right\rangle\left\langle \Delta_{\nu}(t)\right\rangle-\frac{1}{N}\sum_{\nu}\Sigma_{\mu\nu\nu}^{K\Delta }(t,t)+\mathcal{O}(N^{-2})\] (51)

where \(\Sigma_{\mu\nu\nu}^{K\Delta}(t,t)\) is the leading covariance (propagator element) between the kernel \(K_{\mu\nu}(t)\) and prediction error \(\Delta_{\nu}(t)\). We see that the average kernel \(\left\langle K_{\mu\nu}(t)\right\rangle\) (which depends on the finite width \(N\)) plays an important role in characterizing the timescales of the average prediction dynamics. Once this equation is solved for \(\left\langle\Delta_{\mu}(t)\right\rangle\), the square loss at width \(N\) and time \(t\) has the form

\[\sum_{\mu}\left\langle\Delta_{\mu}(t)^{2}\right\rangle\sim\left( 1-\frac{2}{N}\right)\sum_{\mu}\Delta_{\mu}^{\infty}(t)^{2}+\frac{2}{N}\sum_{ \mu}\left\langle\Delta_{\mu}(t)\right\rangle_{\infty}\Delta_{\mu}^{\infty}(t )+\frac{1}{N}\sum_{\mu}\Sigma_{\mu\mu}^{\Delta}(t,t)+\mathcal{O}(N^{-2})\] (52)

We will now comment on the structure of the cross term in this above solution. First, if \(\left\langle\bm{K}\right\rangle\succeq\bm{K}^{\infty}\) and \(\Sigma^{K\Delta}\) is negligible then the average errors at finite width will decay more rapidly than the infinite width model. However, we suspect that in general, \(\left\langle\bm{K}\right\rangle-\bm{K}^{\infty}\) contains many negative eigenvalues since signal propagation at finite width tends to reduce the scale of feature kernels [14]. We suspect that this is the cause of the slower dynamics of ensembled predictors for narrower networks in Figure 7 and Figure A.4. Additionally, the term involving \(\Sigma^{K\Delta}\) will generically increase the cross term since the dynamics of \(\Delta\) cause its fluctuations to become anti-correlated with the fluctuations in \(K\). In general, it is challenging to make strong definitive statements about the relative scale of these competing effects on the cross term. However, we can say more about this solution in the lazy limit, where we find that the cross term will generically be positive, leading to larger MSE (Appendix H.2).

### Perturbation Theory in Rates rather than Predictions

In experiments on deep CNNs trained on CIFAR-10 in 7 and A.4, we find that the loss curves for the ensemble averaged predictors are effectively time rescaled by a function of network width. In this section, we argue that a proper way to account for this is to compute a perturbation expansion in the _exponent_ which defines the rate of decay of the training errors. To illustrate the point, we first consider the case of a single training example before describing larger datasets. In this case, we consider the change of variables \(\Delta(t)=e^{-r(t)}y\). We now treat \(r\) as an order parameter of the theory with dynamics

\[\frac{d}{dt}r(t)=K(t)\] (53)

Note that this equation is now a linear relation between two order parameters (\(r(t),K(t)\)), whereas the relation was previously quadratic. In the lazy limit, if \(K\to K-\epsilon\) then \(r\to r-\epsilon t\), giving an effective rescaling of training time by \(1-\frac{\epsilon}{K}\).

For multiple training examples, we introduce the notion of a transition matrix \(\bm{T}(t)\in\mathbb{R}^{P\times P}\) which has dynamics

\[\frac{d}{dt}\bm{T}(t)=-\bm{K}(t)\bm{T}(t)\;,\;\bm{T}(0)=\mathbf{ I}.\] (54)

The solution to the training prediction errors can be obtained at any time \(t\) by multiplying the initial condition \(\bm{\Delta}(0)=\bm{y}\) with the transition matrix \(\bm{\Delta}(t)=\bm{T}(t)\bm{y}\), where \(\bm{y}\) are the training targets. In this case, the relevant _rate matrix_, which would be an alternative order parameter is

\[\bm{R}(t)=-\log\bm{T}(t)\] (55)where \(\log\) is the matrix logarithm function. Note that in general \(\bm{T}(t)\) admits a Peano-Baker series solution [62; 63; 64]. In the special case where \(\bm{K}(t)\) commutes with \(\tilde{\bm{K}}(t)=\frac{1}{t}\int_{0}^{t}ds\bm{K}(s)\), we obtain the following simplified formula for the rate matrix \(\bm{R}\)

\[\bm{R}(t)=\int_{0}^{t}ds\ \bm{K}(s)\] (56)

The benefit of this representation is the elimination of coupled order parameter dynamics which are quadratic in fluctuations (in \(\bm{\Delta}\) and \(\bm{K}\)) into a linear dynamical relation between order parameters \(\bm{R}\) and \(\bm{K}\). An expansion in \(\bm{R}\) will thus give better predictions at long times \(t\) than a direct expansion in \(\bm{\Delta}\). In the lazy \(\gamma\to 0\) limit, the constancy of \(\bm{K}(t)=\bm{K}\) gives the further simplification \(\bm{R}=\bm{K}t\). Working with this representation, we have the following finite width expression for the training loss

\[\left\langle|\bm{\Delta}(t)|^{2}\right\rangle =\bm{y}^{\top}\left\langle\exp\left(-2\bm{R}(t)\right)\right\rangle \bm{y}\] \[\sim\bm{y}^{\top}\exp\left(-2\left(\bm{R}_{\infty}(t)+\frac{1}{N }\bm{R}^{1}(t)\right)\right)\bm{y}\] \[+\frac{1}{2}\sum_{\mu\nu\alpha\beta}\Sigma_{\mu\nu\alpha\beta}^{ R}(t,t)\frac{\partial^{2}}{\partial R_{\mu\nu}\partial R_{\alpha\beta}}\bm{y}^{ \top}\exp\left(-2\bm{R}\right)\bm{y}|_{\bm{R}=\bm{R}_{\infty}(t)+\frac{1}{N} \bm{R}^{1}(t)}+\mathcal{O}(N^{-2})\] (57)

where \(\left\langle\bm{R}\right\rangle\sim\bm{R}_{\infty}+\frac{1}{N}\bm{R}^{1}+ \mathcal{O}(N^{-2})\) is the leading correction to the mean \(\bm{R}\). In this representation, it is clear that finite width can alter the timescale of the dynamics through a correction to the mean of \(\bm{R}\), as well as contribute an additive correction from fluctuations. This justifies the study perturbation analysis of rates \(R_{N}\) as a function of \(1/N\) in Figures 7 and A.4.

## Appendix H Variance in the Lazy Limit

We can simplify the propagator equations in the lazy \(\gamma\to 0\) limit. To demonstrate how to use our formalism, we go through the complete process of inverting the Hessian, however, for this case, this procedure is a bit cumbersome. A simplified derivation for the lazy limit can be found below in section H.1 which relies only on linearizing the dynamics around the infinite width solution. In the \(\gamma\to 0\) limit, all of the \(D\) tensors vanish and the \(\kappa\) tensors are constant in time. Thus, it suffices to analyze the kernels restricted to \(t=0\) and study the evolution of the prediction variance \(\bm{\Delta}(t)\).

\[S =\int dt\sum_{\mu}\hat{\Delta}_{\mu}(t)\left(\Delta_{\mu}(t)-y_{ \mu}+\int ds\sum_{\nu}\Theta(t-s)K_{\mu\nu}\Delta_{\nu}(s)\right)\] \[+\sum_{\ell}\sum_{\mu\nu}\left[\hat{\Phi}_{\mu\nu}^{\ell}\Phi_{ \mu\nu}^{\ell}+G_{\mu\nu}^{\ell}\hat{G}_{\mu\nu}^{\ell}\right]+\sum_{\mu\nu }\hat{K}_{\mu\nu}\left[K_{\mu\nu}-\sum_{\ell}G_{\mu\nu}^{\ell+1}\Phi_{\mu\nu} ^{\ell}\right]+\sum_{\ell}\ln\mathcal{Z}_{\ell}\] \[\mathcal{Z}_{\ell}=\mathbb{E}_{\{u_{\mu}^{\ell}\},\{r_{\mu}^{ \ell}\}}\exp\left(-\sum_{\mu\nu}\hat{\Phi}_{\mu\nu}^{\ell}\phi(u_{\mu}^{\ell} )\phi(u_{\nu}^{\ell})-\sum_{\mu\nu}\hat{G}_{\mu\nu}^{\ell}g_{\mu}^{\ell}g_{ \nu}^{\ell}\right)\,\ g_{\mu}^{\ell}=r_{\mu}^{\ell}\dot{\phi}(u_{\mu}^{\ell})\] (58)

where \(\{u_{\mu}^{\ell}\}\sim\mathcal{N}(0,\bm{\Phi}^{\ell-1}),\{r_{\mu}^{\ell}\} \sim\mathcal{N}(0,\bm{G}^{\ell+1})\). Taking two derivatives with respect to \(\{\hat{\Phi}^{\ell},\hat{G}^{\ell}\}\) give terms of the form

\[\kappa_{\mu\nu\alpha\beta}^{\Phi^{\ell}} =\left\langle\phi(u_{\mu}^{\ell})\phi(u_{\nu}^{\ell})\phi(u_{\alpha }^{\ell})\phi(u_{\beta}^{\ell})\right\rangle-\Phi_{\mu\nu}^{\ell}\Phi_{\alpha \beta}^{\ell}\] \[\kappa_{\mu\nu\alpha\beta}^{\phi^{\ell},G^{\ell}} =\left\langle\phi(u_{\mu}^{\ell})\phi(u_{\nu}^{\ell})g_{\alpha}^{ \ell}g_{\beta}^{\ell}\right\rangle-\Phi_{\mu\nu}^{\ell}G_{\alpha\beta}^{\ell}\] (59)Given these we also have the relevant non-vanishing sensitivity tensors

\[D^{\Phi^{\ell+1}\Phi^{\ell}}_{\mu\nu\alpha\beta} =\frac{\partial^{2}}{\partial\Phi^{\ell}_{\alpha\beta}}\left\langle \phi(u^{\ell+1}_{\mu})\phi(u^{\ell+1}_{\nu})\right\rangle\,\ D^{G^{\ell}G^{\ell+1}}_{\mu\nu\alpha\beta}=\frac{ \partial}{\partial G^{\ell+1}_{\alpha\beta}}\left\langle\phi^{\ell}_{\mu}g^{ \ell}_{\nu}\right\rangle\] \[D^{G^{\ell}\Phi^{\ell-1}}_{\mu\nu\alpha\beta} =\frac{\partial}{\partial\Phi^{\ell-1}_{\alpha\beta}}\left\langle g ^{\ell}_{\mu}g^{\ell}_{\nu}\right\rangle\] (60) \[D^{K\Phi^{\ell}}_{\mu\nu\alpha\beta} =\delta_{\mu\alpha}\delta_{\nu\beta}G^{\ell+1}_{\mu\nu}\,\ D^{KG^{\ell}G^{\ell}}_{\mu\nu\alpha\beta}=\delta_{\mu\alpha}\delta_{\nu \beta}\Phi^{\ell-1}_{\mu\nu}\] \[D^{\Delta K}_{\mu\alpha\beta}(t) =\int ds\Theta(t-s)\delta_{\mu\alpha}\Delta_{\beta}(s)\] (61)

As before we let \(\bm{q}_{1}=\text{Vec}\{\Delta_{\mu}(t),\Phi^{\ell}_{\mu\nu},G^{\ell}_{\mu\nu}, K_{\mu\nu}\}\) and \(\bm{q}_{2}=\text{Vec}\{\hat{\Delta}_{\mu}(t),\hat{\Phi}^{\ell}_{\mu\nu},\hat{G}^ {\ell}_{\mu\nu},\hat{K}_{\mu\nu}\}\). The propagator has the form

\[\bm{U}\equiv\nabla^{2}_{\bm{q}_{1}}S=\begin{bmatrix}\mathbf{I}+\bm{\Theta}_{K }&\mathbf{0}&\mathbf{0}&\bm{D}^{\Delta K}\\ \mathbf{0}&\mathbf{I}-\bm{D}^{\Phi,\Phi}&\mathbf{0}&\mathbf{0}\\ \mathbf{0}&-\bm{D}^{G\Phi}&\mathbf{I}-\bm{D}^{GG}&\mathbf{0}\\ \mathbf{0}&-\bm{D}^{K\Phi}&-\bm{D}^{KG}&\mathbf{I}\end{bmatrix}\,\ \nabla^{2}_{\bm{q}_{2}\bm{q}_{2}}S= \begin{bmatrix}\mathbf{0}&\mathbf{0}&\mathbf{0}&\mathbf{0}\\ \mathbf{0}&\bm{\kappa}^{\Phi,\Phi}&\bm{\kappa}^{\Phi G}&\mathbf{0}\\ \mathbf{0}&\bm{\kappa}^{G\Phi}&\bm{\kappa}^{GG}&\mathbf{0}\\ \mathbf{0}&\mathbf{0}&\mathbf{0}&\mathbf{0}\end{bmatrix}\] (62)

The propagator of interest is \(\bm{\Sigma}_{\bm{q}_{1}}=\bm{U}^{-1}\left[\nabla^{2}_{\bm{q}_{2}\bm{q}_{2}}S \right]\bm{U}^{-1}\mathbb{\top}\). We can exploit the block structure of \(\bm{U}\) to find an inverse

\[\bm{U}^{-1}=\begin{bmatrix}\bm{U}^{-1}_{\Delta\Delta}&\bm{U}^{-1}_{\Delta \Phi}&\bm{U}^{-1}_{\Delta G}&\bm{U}^{-1}_{\Delta K}\\ \mathbf{0}&\bm{U}^{-1}_{\Phi\Phi}&\mathbf{0}&\mathbf{0}\\ \mathbf{0}&\bm{U}^{-1}_{G\Phi}&\bm{U}^{-1}_{G\Omega}&\mathbf{0}\\ \mathbf{0}&\bm{U}^{-1}_{K\Phi}&\bm{U}^{-1}_{KG}&\mathbf{I}\end{bmatrix}\] (63)

where each sub-block can be computed with the Schur-complement formula. Altogether, we multiply through to get the propagator

\[\bm{\Sigma} =\begin{bmatrix}\mathbf{0}&\bm{U}^{-1}_{\Delta\Phi}\bm{\kappa}^ {\Phi\Phi}+\bm{U}^{-1}_{\Delta G}\bm{\kappa}^{G\Phi}&\bm{U}^{-1}_{\Delta \Phi}\bm{\kappa}^{\Phi G}+\bm{U}^{-1}_{\Delta G}\bm{\kappa}^{GG}&\mathbf{0}\\ \mathbf{0}&\bm{U}^{-1}_{\Phi\Phi}\bm{\kappa}^{\Phi\Phi}+\bm{U}^{-1}_{G\Phi}\bm{ \kappa}^{G\Phi}&\bm{U}^{-1}_{G\Phi}\bm{\kappa}^{\Phi G}+\bm{U}^{-1}_{G\Phi}\bm{ \kappa}^{GG}&\mathbf{0}\\ \mathbf{0}&\bm{U}^{-1}_{K\Phi}\bm{\kappa}^{\Phi\Phi}+\bm{U}^{-1}_{KG}\bm{ \kappa}^{G\Phi}&\bm{U}^{-1}_{K\Phi}\bm{\kappa}^{\Phi G}+\bm{U}^{-1}_{KG}\bm{ \kappa}^{GG}&\mathbf{0}\end{bmatrix}\] \[\times\begin{bmatrix}\bm{U}^{-1}_{\Delta\Phi}&\mathbf{0}& \mathbf{0}\\ \bm{U}^{-1}_{\Delta\Phi}\bm{\kappa}^{\top}&\bm{U}^{-1}_{\Phi\Phi}&[\bm{U}^{-1}_ {G\Phi}]^{\top}&[\bm{U}^{-1}_{K\Phi}]^{\top}\\ \bm{U}^{-1}_{\Delta G}]^{\top}&\mathbf{0}&\bm{U}^{GG}_{GG}&[\bm{U}^{-1}_{KG}]^{ \top}\\ \bm{U}^{-1}_{\Delta K}]^{\top}&\mathbf{0}&\mathbf{0}&\mathbf{I}\end{bmatrix}\] (64)

Two of these blocks corresponding to \(K,\Delta\) are especially important for characterizing the fluctuations of network predictions. The covariance structure for \(K\) has the form

\[\bm{\Sigma}_{K} =\bm{U}^{-1}_{K\Phi}\bm{\kappa}^{\Phi\Phi}[\bm{U}^{-1}_{K\Phi}]^{ \top}+\bm{U}^{-1}_{KG}\bm{\kappa}^{G\Phi}[\bm{U}^{-1}_{K\Phi}]^{\top}+\bm{U}^ {-1}_{K\Phi}\bm{\kappa}^{\Phi G}[\bm{U}^{-1}_{KG}]^{\top}+\bm{U}^{-1}_{KG}\bm{ \kappa}^{GG}[\bm{U}^{-1}_{KG}]^{\top}\] (65)

Next we use the fact that \(\bm{U}^{-1}_{\Delta\Phi}=\bm{U}^{-1}_{\Delta K}\bm{U}^{-1}_{K\Phi}\) and that \(\bm{U}^{-1}_{\Delta G}=\bm{U}^{-1}_{\Delta K}\bm{U}^{-1}_{KG}\), which follows from the block structure of \(\bm{U}\). Consequently we arrive at the identity

\[\bm{\Sigma}_{\Delta} =\bm{U}^{-1}_{\Delta\Phi}\bm{\kappa}^{\Phi\Phi}[\bm{U}^{-1}_{ \Delta\Phi}]^{-1}+\bm{U}^{-1}_{\Delta G}\bm{\kappa}^{G\Phi}[\bm{U}^{-1}_{ \Delta\Phi}]^{-1}+\bm{U}^{-1}_{\Delta G}\bm{\kappa}^{G\Phi}[\bm{U}^{-1}_{ \Delta\Phi}]^{-1}+\bm{U}^{-1}_{\Delta G}\bm{\kappa}^{G\Phi}[\bm{U}^{-1}_{\Delta \Phi}]^{-1}+\bm{U}^{-1}_{\Delta G}\bm{\kappa}^{GG}[\bm{U}^{-1}_{\Delta G}]^{-1}\] \[=\bm{U}^{-1}_{\Delta K}\bm{\SigmaDifferentiation with respect to \(t\) and \(s\) gives a simple differential equation

\[\frac{\partial^{2}}{\partial t\partial s}\Sigma^{\Delta}_{\mu\nu}(t,s )+\sum_{\alpha}K_{\mu\alpha}\frac{\partial}{\partial s}\Sigma^{\Delta}_{\alpha \nu}(t,s)+\sum_{\beta}K_{\nu\beta}\frac{\partial}{\partial t}\Sigma^{\Delta}_{ \mu\beta}(t,s)\] \[+\sum_{\alpha\beta}K_{\mu\alpha}K_{\nu\beta}\Sigma^{\Delta}_{ \alpha\beta}(t,s)=\sum_{\alpha\beta}\Delta_{\alpha}(t)\Delta_{\beta}(s)\Sigma^ {K}_{\mu\alpha,\nu\beta}\] (68)

Let \(\{\bm{\psi}_{k}\}\) be the eigenvectors of the kernel matrix \(\bm{K}\). Projecting these dynamics on the eigenspace \(\Sigma_{k\ell}(t,s)=\bm{\psi}_{k}^{\top}\bm{\Sigma}(t,s)\bm{\psi}_{\ell}\) recovers the equation in the main text

\[\left(\frac{\partial}{\partial t}+\lambda_{k}\right)\left(\frac{ \partial}{\partial s}+\lambda_{\ell}\right)\Sigma_{k\ell}(t,s)=\sum_{k^{ \prime}\ell^{\prime}}\Delta_{k^{\prime}}(t)\Delta_{\ell^{\prime}}(s)\Sigma^{ K}_{kk^{\prime}\ell\ell^{\prime}}\] (69)

Replacing \(\Sigma^{K}=\kappa\) recovers the equation (7) in the main text.

### Perturbed Linear System

In this section, we provide a simpler derivation of the lazy limit training error variance dynamics. In this case, we merely perturb the dynamics around its infinite width value \(\bm{\Delta}(t)=\bm{\Delta}_{\infty}(t)+\bm{\epsilon}^{\Delta}(t)\) and \(\bm{K}=\bm{K}_{\infty}+\bm{\epsilon}^{K}\), and keep terms only linear in these perturbations. The perturbation \(\bm{\epsilon}^{K}\) is fixed in time and the dynamics of \(\bm{\epsilon}^{\Delta}(t)\) are

\[\frac{d}{dt}\bm{\epsilon}^{\Delta}(t)=-\bm{K}_{\infty}\bm{\epsilon}^{\Delta}(t )-\bm{\epsilon}^{K}\bm{\Delta}_{\infty}(t)\] (70)

Projecting this equation on the eigenspace of \(\bm{K}_{\infty}\) gives

\[\frac{d}{dt}\epsilon^{\Delta}_{k}(t)=-\lambda_{k}\epsilon_{k}(t)-\sum_{k^{ \prime}}\epsilon^{K}_{kk^{\prime}}\Delta^{\infty}_{k^{\prime}}(t)\] (71)

This immediately recovers the final result of the last section

\[N\left(\frac{\partial}{\partial t}+\lambda_{k}\right)\left( \frac{\partial}{\partial t}+\lambda_{k}\right)\left\langle\epsilon^{\Delta} _{k}(t)\epsilon^{\Delta}_{\ell}(s)\right\rangle=\left(\frac{\partial}{ \partial t}+\lambda_{k}\right)\left(\frac{\partial}{\partial t}+\lambda_{k} \right)\Sigma^{\Delta}_{k\ell}(t,s)\] \[=\sum_{k^{\prime}\ell^{\prime}}\Sigma^{K}_{kk^{\prime}\ell\ell^{ \prime}}\Delta^{\infty}_{k^{\prime}}(t)\Delta^{\infty}_{\ell^{\prime}}(s)\] (72)

Qualitatively, the process of computing this linear correction (in \(\epsilon^{K}\)) to the dynamics of \(\bm{\Delta}\) is identical to the argument utilized in prior work on perturbative feature learning corrections [11]. In that context, the perturbation is caused by small amounts of feature learning, rather than initialization fluctuations.

### Mean Prediction Error Correction in the Lazy Limit

Using a similar heuristic as in the preceeding section, we now consider the correction to the mean predictor \(\left\langle\Delta_{\mu}(t)\right\rangle\) in the lazy limit. Taylor expanding \(\left\langle\bm{\Delta}(t)\right\rangle\) in powers of \(1/N\), we find

\[\frac{d}{dt}\left\langle\bm{\Delta}(t)\right\rangle =\frac{d}{dt}\bm{\Delta}^{\infty}(t)+\frac{1}{N}\frac{d}{dt}\bm{ \Delta}^{1}(t)+...\] \[=-\left\langle(\bm{K}-\bm{K}^{\infty}+\bm{K}^{\infty})\left(\bm {\Delta}-\bm{\Delta}^{\infty}+\bm{\Delta}^{\infty}\right)\right\rangle\] \[=-\bm{K}^{\infty}\bm{\Delta}^{\infty}-\bm{K}^{\infty}\left\langle \bm{\Delta}-\bm{\Delta}^{\infty}\right\rangle\] \[-\left\langle(\bm{K}-\bm{K}^{\infty})\right\rangle\bm{\Delta}^{ \infty}-\left\langle(\bm{K}-\bm{K}^{\infty})\left(\bm{\Delta}-\bm{\Delta}^{ \infty}\right)\right\rangle\] \[\sim-\bm{K}^{\infty}\bm{\Delta}^{\infty}-\frac{1}{N}\bm{K}^{ \infty}\bm{\Delta}^{1}-\frac{1}{N}\bm{K}^{1}\bm{\Delta}^{\infty}-\frac{1}{N} \left\langle\bm{\epsilon}^{K}\bm{\epsilon}^{\Delta}\right\rangle_{\infty}+ \mathcal{O}(N^{-2})\] (73)

From the previous section we have that

\[\frac{d}{dt}\bm{\epsilon}^{\Delta}=-\bm{K}^{\infty}\bm{\epsilon}^{\Delta}-\bm{ \epsilon}^{K}\bm{\Delta}^{\infty}\implies\bm{\epsilon}^{\Delta}(t)=-\int_{0}^ {t}ds\exp\left(-\bm{K}^{\infty}(t-s)\right)\bm{\epsilon}^{K}\exp\left(-\bm{K}^ {\infty}s\right)\bm{y}\] (74)Projecting these dynamics onto the eigenspace of the kernel gives

\[\epsilon_{k}^{\Delta}(t)=-\sum_{\ell}\epsilon_{k\ell}^{K}\;\frac{e^{-\lambda_{\ell }t}-e^{-\lambda_{k}t}}{\lambda_{k}-\lambda_{\ell}}y_{\ell}\] (75)

where \(\ell=k\) should be seen as the limit where \(\lambda_{k}\rightarrow\lambda_{\ell}\) of the above. Thus we find that the leading mean correction to the error solves the following differential equation

\[\left(\frac{d}{dt}+\lambda_{k}\right)\Delta_{k}^{1}(t) =-\sum_{\ell}K_{k\ell}^{1}y_{\ell}e^{-\lambda_{\ell}t}+\sum_{\ell \ell^{\prime}}\Sigma_{k\ell\ell^{\prime}}^{K}\frac{e^{-\lambda_{\ell^{\prime}} t}-e^{-\lambda_{\ell}t}}{\lambda_{\ell}-\lambda_{\ell^{\prime}}}y_{\ell^{ \prime}}.\] \[=\sum_{\ell}y_{\ell}e^{-\lambda_{\ell}t}\left[-K_{k\ell}^{1}+ \Sigma_{k\ell\ell\ell}^{K}t\right]+\sum_{\ell\neq\ell^{\prime}}\Sigma_{k\ell \ell^{\prime}}^{K}\frac{e^{-\lambda_{\ell^{\prime}}t}-e^{-\lambda_{\ell}t}}{ \lambda_{\ell}-\lambda_{\ell^{\prime}}}y_{\ell^{\prime}}\] (76)

We see that at late sufficiently large \(t\), that the terms involving \(\Sigma^{K}\) will dominate. We can gain more intuition by considering the special case of a single training data point where the mean error correction has the form

\[\left(\frac{d}{dt}+\lambda\right)\Delta^{1}(t) =ye^{-\lambda t}\left[-K^{1}+t\Sigma^{K}\right]\implies\Delta^{1} (t)=y\left[-tK^{1}+\frac{1}{2}t^{2}\Sigma^{K}\right]e^{-\lambda t}\] \[\implies\left\langle\Delta(t)^{2}\right\rangle \sim\Delta^{\infty}(t)^{2}+\frac{1}{N}\left[2y^{2}te^{-2\lambda t }\left[-K^{1}+\frac{1}{2}t\Sigma^{K}\right]+\Sigma^{\Delta}(t,t)\right]+ \mathcal{O}(N^{-2})\] \[\sim\Delta^{\infty}(t)^{2}+\frac{2}{N}y^{2}te^{-2\lambda t}\left[ -K^{1}+\Sigma^{K}t\right]+\mathcal{O}(N^{-2})\] (77)

While the term involving \(\Sigma^{K}\) is positive for all \(t\), \(K^{1}\) could be positive or negative for a given architecture. If \(K^{1}\) is positive, then MSE is initially improved at early times but after \(t>\frac{K^{1}}{\Sigma^{K}}\) the MSE is worse than the infinite width. On the other hand, if \(K^{1}\) is negative (as we suspect is typically the case), then the MSE will strictly decrease with network width for any time \(t\).

## Appendix I Two Layer Equations and Time/Time Diagonal

In this section, we analyze two layer networks in greater detail. Unlike the deep network case, two layer networks can be analyzed on the time-time diagonal: ie the dynamics only depend on \(\Phi(t,t)\) and \(G(t,t)\) rather than on all possible off-diagonal pairs of time points. Further, there are no response functions \(A^{\ell},B^{\ell}\) which complicate the recipe for calculating the propagator (Appendix E).

### A Single Training Point

For a two layer network trained on a single training point with norm constraint \(|\bm{x}|^{2}=D\), we have the following DMFT action

\[S[\{K(t),\hat{K}(t),\Delta(t),\hat{\Delta}(t)\}]\] (78) \[=\int dt\left[K(t)\hat{K}(t)+\hat{\Delta}(t)\left(\Delta(t)-y+ \int ds\;\Theta(t-s)\Delta(s)K(s)\right)\right]\] \[\qquad\qquad+\ln\mathcal{Z}[\hat{K},f]\;,\;\mathcal{Z}=\mathbb{E} _{h,g}\exp\left(-\int dt\hat{K}(t)[\phi(h(t))^{2}+g(t)^{2}]\right).\]

The saddle point equations are

\[\frac{\partial S}{\partial\hat{K}(t)} =K(t)-\left\langle[\phi(h(t))^{2}+g(t)^{2}]\right\rangle=0\] \[\frac{\partial S}{\partial\hat{\Delta}(t)} =\Delta(t)-y+\int ds\;\Theta(t-s)\Delta(s)K(s)=0\] \[\frac{\partial S}{\partial K(s)} =\hat{K}(s)+\Delta(s)\int dt\;\hat{\Delta}(t)\Theta(t-s)=0\] \[\frac{\partial S}{\partial\Delta(s)} =\hat{\Delta}(s)+K(s)\int dt\;\hat{\Delta}(t)\Theta(t-s)=0\] (79)From these equations, we can compute the entries in the Hessian of the DMFT action \(S\). Letting \(\bm{q}(t)=\begin{bmatrix}\Delta(t)\\ K(t)\end{bmatrix}\) and \(\hat{\bm{q}}(t)=\begin{bmatrix}\hat{\Delta}(t)\\ \hat{K}(t)\end{bmatrix}\)

\[\frac{\partial^{2}S}{\partial\bm{q}(t)\partial\bm{q}(s)^{\top}} =\bm{0}\] \[\frac{\partial^{2}S}{\partial\hat{\bm{q}}(t)\partial\bm{q}(s)^{ \top}} =\begin{bmatrix}\delta(t-s)+\Theta(t-s)K(s)&\Theta(t-s)\Delta(s)\\ -\left\langle\frac{\partial}{\partial\Delta(s)}(\phi(h(t))^{2}+g(t)^{2}) \right\rangle&\delta(t-s)\end{bmatrix}\] \[\frac{\partial^{2}S}{\partial\hat{\bm{q}}(t)\partial\hat{\bm{q}}( s)^{\top}} =\begin{bmatrix}0&0\\ 0&\kappa(t,s)\end{bmatrix}\] (80)

where \(\kappa(t,s)=\left\langle(\phi(h(t))^{2}+g(t)^{2})(\phi(h(s))^{2}+g(s)^{2}) \right\rangle-K(t)K(s)\) is the NTK's fourth cumulant. We now vectorize our order parameters over time \(\bm{q}=\text{Vec}\{\bm{q}(t)\}_{t\in\mathbb{R}_{+}}\) and \(\hat{\bm{q}}=\text{Vec}\{\hat{\bm{q}}(t)\}_{t\in\mathbb{R}_{+}}\) and express the full Hessian

\[\nabla^{2}S=\begin{bmatrix}\bm{0}&\frac{\partial^{2}S}{\partial \bm{q}\partial\bm{q}^{\top}}\end{bmatrix}\implies-[\nabla^{2}S]^{-1}= \begin{bmatrix}(\frac{\partial^{2}S}{\partial\hat{\bm{q}}\partial\bm{q}^{\top }})^{-1}\frac{\partial^{2}S}{\partial\hat{\bm{q}}\partial\hat{\bm{q}}}(\frac{ \partial^{2}S}{\partial\hat{\bm{q}}\partial\hat{\bm{q}}^{\top}})^{-1}&-(\frac {\partial^{2}S}{\partial\hat{\bm{q}}\partial\bm{q}^{\top}})^{-1}\\ -(\frac{\partial^{2}S}{\partial\bm{q}\partial\hat{\bm{q}}^{\top}})^{-1}&\bm{0 }\end{bmatrix}\] (81)

The covariance matrix of interest (for \(\bm{q}(t)\)) is thus

\[\bm{\Sigma}_{\bm{q}}=\begin{bmatrix}\mathbf{I}+\bm{\Theta}_{K}& \bm{\Theta}_{\Delta}\\ -\bm{D}&\mathbf{I}\end{bmatrix}^{-1}\begin{bmatrix}0&0\\ 0&\bm{\kappa}\end{bmatrix}\begin{bmatrix}\mathbf{I}+\bm{\Theta}_{K}&\bm{ \Theta}_{\Delta}\\ -\bm{D}&\mathbf{I}\end{bmatrix}^{-1\top}.\] (82)

where \([\bm{\Theta}_{K}](t,s)=\Theta(t-s)K(s)\) and \([\bm{\Theta}_{\Delta}](t,s)=\Theta(t-s)\Delta(s)\). The above equations allow one to use the infinite width DMFT dynamics for \(K(t),\Delta(t)\) to compute the finite size fluctuation dynamics of the kernel \(K\) and the error signal \(\Delta\).

#### i.1.1 Computing Field Sensitivities

In this section, we compute \(D(t,s)\) by solving for the sensitivity of order parameters. We start with the DMFT field equations

\[h(t)=u+\gamma\int_{0}^{t}ds\Delta(s)g(s)\;,\;z(t)=r+\gamma\int_{0}^{t}ds\Delta (s)\phi(h(t)).\] (83)

Now, differentiating both sides with respect to \(\Delta(s^{\prime})\) gives

\[\frac{\partial h(t)}{\partial\Delta(s^{\prime})} =\gamma\Theta(t-s^{\prime})g(s^{\prime})+\gamma\int_{0}^{t}ds \Delta(s)\frac{\partial g(s)}{\partial\Delta(s^{\prime})}\] \[\frac{\partial z(t)}{\partial\Delta(s^{\prime})} =\gamma\Theta(t-s^{\prime})\phi(h(s^{\prime}))+\gamma\int_{0}^{t} ds\Delta(s)\frac{\partial\phi(h(s))}{\partial\Delta(s^{\prime})}.\] (84)

We can compute \(D\) Monte carlo by iteratively solving the above equations for each sampled trajectory \(\{h(t),z(t)\}\)[65; 46]. Averaging the necessary fields over the Monte Carlo samples will give us the final expressions for \(D(t,s)\).

\[D(t,s)=\left\langle\frac{\partial}{\partial\Delta(s)}(\phi(h(t))^{2}+g(t)^{2})\right\rangle\] (85)

Similarly, the uncoupled kernel variance \(\kappa(t,s)\) can be evaluated via Monte Carlo sampling for nonlinear networks.

### Test Point Fluctuation Dynamics

We now are in a position to calculate the test/train kernel and test prediction fluctuations. To do this systematically, we augment \(S\) with the test point prediction \(f_{\star}\) and field \(h_{\star}\) and introduce the kernel \[\frac{dh(t)}{dt}=\gamma\Delta(t)z(t)\;,\;\frac{dz(t)}{dt}=\gamma \Delta(t)h(t)\] (91)

We can make a change of variables \(v_{+}(t)=\frac{1}{\sqrt{2}}(h(t)+z(t))\) and \(v_{-}(t)=\frac{1}{\sqrt{2}}(h(t)-z(t))\). We note that \(v_{+}(0)=\frac{1}{\sqrt{2}}(u+r)\) and \(v_{-}(0)=\frac{1}{\sqrt{2}}(u-r)\) are independent Gaussians. These functions \(v_{+}(t),v_{-}(t)\) satisfy dynamics

\[\frac{dv_{+}}{dt}=\gamma\Delta(t)v_{+}(t)\;,\;\frac{dv_{-}(t)}{dt}=- \gamma\Delta(t)v_{-}(t)\] \[\implies v_{+}(t)=\exp\left(\gamma\int_{0}^{t}ds\Delta(s)\right)v_ {+}(0)\implies\frac{\partial v_{+}(t)}{\partial\Delta(s)}=\gamma v_{+}(t) \Theta(t-s)\] \[\implies v_{-}(t)=\exp\left(-\gamma\int_{0}^{t}ds\Delta(s)\right)v_ {-}(0)\implies\frac{\partial v_{+}(t)}{\partial\Delta(s)}=-\gamma v_{-}(t) \Theta(t-s)\] (92)Now, we use the fact that \(v_{+}(0)=\frac{1}{\sqrt{2}}(u+r)\) and \(v_{-}(0)=\frac{1}{\sqrt{2}}(u-r)\) are independent standard normal random variables to compute \(K(t)=\left\langle h(t)^{2}+z(t)^{2}\right\rangle=\left\langle v_{+}(t)^{2}+v_{- }(t)^{2}\right\rangle\)

\[D(t,s) =\frac{\partial}{\partial\Delta(s)}\left\langle h(t)^{2}+z(t)^{2} \right\rangle=2\gamma\left[\left\langle v_{+}(t)^{2}\right\rangle-\left\langle v _{-}(t)^{2}\right\rangle\right]\Theta(t-s)\] \[=2\gamma\left[\exp\left(2\gamma\int_{0}^{t}ds\Delta(s)\right)- \exp\left(-2\gamma\int_{0}^{t}ds\Delta(s)\right)\right]\Theta(t-s)\] (93)

This operator is causal (\(D(t,s)=0\) for \(s>t\)) as expected and vanishes as \(t\to 0\). If we take \(\gamma\to 0\), we have \(D(t,s)\to 0\) which agrees with our reasoning that fields \(h,z\) only depend on \(\Delta\) in the feature learning regime. Since all fields are Gaussian in the linear network case, we can use Wick's theorem to obtain the exact uncoupled kernel variance in the two layer case.

\[\kappa(t,s) =\left\langle(h(t)^{2}+z(t)^{2})(h(s)^{2}+z(s)^{2})\right\rangle- K(t)K(s)\] \[=2\left\langle h(t)h(s)\right\rangle^{2}+2\left\langle h(t)z(s) \right\rangle^{2}+2\left\langle z(t)h(s)\right\rangle^{2}+2\left\langle z(t) z(s)\right\rangle^{2}\] \[=\left\langle v_{+}(t)v_{+}(s)+v_{-}(t)v_{-}(s)\right\rangle^{2} +\left\langle v_{+}(t)v_{+}(s)-v_{-}(t)v_{-}(s)\right\rangle^{2}\] (94)

The \(v_{\pm}(t)\) functions are those given above. Using the fact that \(\left\langle v_{+}(0)^{2}\right\rangle=\left\langle v_{-}(0)^{2}\right\rangle=1\) allows us to easily compute the single site average above.

## Appendix J Multiple Samples with Whitened Data

In this section, we analyze the role that sample number plays in dynamics in a simplified model of a two layer linear network trained on whitened data. Concretely, we assume that \(\frac{\mathbf{x}_{\mu}\cdot\mathbf{z}_{\nu}}{D}=\delta_{\mu\nu}\). The field equations for preactivations \(h_{\mu}(t)\) and pregradients \(z(t)\) obey

\[\frac{d}{dt}h_{\mu}(t)=\gamma\Delta_{\mu}(t)z(t)\;,\;\frac{d}{dt}z(t)=\gamma \sum_{\mu=1}^{P}\Delta_{\mu}(t)h_{\mu}(t)\] (95)

We will assume the targets have unit norm \(|\bm{y}|^{2}=1\) and we define the projection of \(\bm{\Delta}\) onto the target as \(\Delta_{y}(t)=\bm{y}\cdot\bm{\Delta}(t)\). The other \(P-1\) orthogonal components are denoted \(\bm{\Delta}_{\perp}(t)\) so that \(\bm{\Delta}=\Delta_{y}(t)\bm{y}+\bm{\Delta}_{\perp}(t)\) with \(\bm{\Delta}_{\perp}(t)\cdot\bm{y}=0\). At infinite width, \(\bm{\Delta}_{\perp}=0\) and our field equations become

\[\frac{d}{dt}h_{y}(t)=\Delta_{y}(t)z(t)\;,\;\frac{d}{dt}z(t)=\Delta_{y}(t)h_{y} (t)\;,\;\bm{\Delta}_{\perp}(t)=0\;,\;h_{\perp}\sim\mathcal{N}(0,1)\] (96)

However, at finite width \(N\), the off-target predictions \(\bm{\Delta}_{\perp}\) fluctuate over random initialization. To model all of the fluctuations simultaneously, we consider the following action

\[S=\gamma\int dt\sum_{\mu}\hat{\Delta}_{\mu}(t)(\Delta_{\mu}(t)-y_{\mu})+\ln \mathbb{E}\exp\left(\int dt\sum_{\mu}\hat{\Delta}_{\mu}(t)z(t)h_{\mu}(t)\right)\] (97)

which enforces the constraint that \(\Delta_{\mu}(t)=y_{\mu}-\frac{1}{\gamma}\left\langle z(t)h_{\mu}(t)\right\rangle\) at infinite width. The Hessian over order parameters \(\bm{q}=\text{Vec}\{\Delta_{\mu}(t),\hat{\Delta}_{\mu}(t)\}\) has the form

\[\nabla_{\bm{q}}^{2}\bm{S}=\begin{bmatrix}\bm{0}&(\gamma\bm{\mathrm{I}}+\bm{D} )^{\top}\\ \gamma\bm{\mathrm{I}}+\bm{D}&\bm{\kappa}\end{bmatrix}\;,\;D_{\mu\nu}(t,s)= \left\langle\frac{\partial}{\partial\Delta_{\nu}(s)}z(t)h_{\mu}(t)\right\rangle\] (98)

We thus get the following covariance for predictions \(\bm{\Sigma}_{\Delta}=(\gamma\bm{\mathrm{I}}+\bm{D})^{-1}\bm{\kappa}\left[( \gamma\bm{\mathrm{I}}+\bm{D})^{-1}\right]^{\top}\). We now compute the necessary components of the \(D\) tensor

\[\frac{\partial h_{\mu}(t)}{\partial\Delta_{\nu}(s)} =\gamma\delta_{\mu\nu}\Theta(t-s)z(s)+\gamma\int_{0}^{t}dt^{\prime }\Delta_{\mu}(t^{\prime})\frac{\partial z(t^{\prime})}{\partial\Delta_{\nu}(s)}\] \[\frac{\partial z(t)}{\partial\Delta_{\nu}(s)} =\gamma\Theta(t-s)h_{\nu}(s)+\gamma\int_{0}^{t}dt^{\prime}\sum_{\mu }\Delta_{\mu}(t^{\prime})\frac{\partial h_{\mu}(t^{\prime})}{\partial\Delta_{ \nu}(s)}\] \[=\gamma\Theta(t-s)h_{\nu}(s)+\gamma\int_{0}^{t}dt^{\prime}\Delta_{y }(t^{\prime})\frac{\partial h_{y}(t^{\prime})}{\partial\Delta_{\nu}(s)}\] (99)In the last line, we used the fact that these equations are to be evaluated at the mean field infinite width stochastic process where \(\Delta_{\perp}(t)=0\). To compute the sensitivity tensor \(D\), we find the following equations for our correlators of interest:

\[\left\langle\frac{\partial h_{\mu}(t)}{\partial\Delta_{\nu}(s)}z(t )\right\rangle =\delta_{\mu\nu}\gamma\Theta(t-s)\left\langle z(s)z(t)\right\rangle \;,\;\mu,\nu\neq y\] \[\left\langle\frac{\partial z(t)}{\partial\Delta_{\nu}(s)}h_{\mu}( t)\right\rangle =\gamma\Theta(t-s)\delta_{\mu\nu}\;,\;\mu,\nu\neq y\] (100) \[\left\langle\frac{\partial h_{y}(t)}{\partial\Delta_{y}(s)}z(t)\right\rangle =\gamma\Theta(t-s)\left\langle z(s)z(t)\right\rangle+\gamma\int_ {0}^{t}dt^{\prime}\Delta_{y}(t^{\prime})\left\langle\frac{\partial z(t^{ \prime})}{\partial\Delta_{y}(s)}z(t)\right\rangle\] \[\left\langle\frac{\partial z(t)}{\partial\Delta_{y}(s)}z(t^{ \prime})\right\rangle =\gamma\Theta(t-s)\left\langle h_{y}(s)z(t)\right\rangle+\gamma \int_{0}^{t}dt^{\prime\prime}\Delta_{y}(t^{\prime\prime})\left\langle\frac{ \partial h_{y}(t^{\prime\prime})}{\partial\Delta_{y}(s)}z(t^{\prime})\right\rangle\]

We therefore see that the components of \(D\) decouple over indices. In the \(\bm{y}\) direction, we have the following equations

\[D_{y}(t,s)=\left\langle\frac{\partial h_{y}(t)}{\partial\Delta_{y}(s)}z(t) \right\rangle+\left\langle\frac{\partial z(t)}{\partial\Delta_{y}(s)}h_{y}(t)\right\rangle\] (101)

where the correlators must be solved self-consistently. We will provide this solution in one moment, but first, we will look at the orthogonal directions. For the \(P-1\) orthogonal directions, we obtain the explicit formula for \(D\) in each of these directions

\[D_{\perp}(t,s) =\left\langle\frac{\partial h_{\perp}(t)}{\partial\Delta_{\perp }(s)}z(t)\right\rangle+\left\langle\frac{\partial z(t)}{\partial\Delta_{\perp }(s)}h_{\perp}(t)\right\rangle\] \[=\gamma\Theta(t-s)\left\langle z(t)z(s)\right\rangle+\gamma \Theta(t-s)\] (102)

Now, we return to \(D_{y}\). To solve these equations we utilize the change of variables employed in the single sample case \(v_{+}(t)=\frac{1}{\sqrt{2}}(h_{y}(t)+z(t)),v_{-}(t)=\frac{1}{\sqrt{2}}(h_{y}(t )-z(t))\) (see Appendix I.3). This orthogonal transformation decouples the dynamics

\[\frac{d}{dt}v_{+}(t)=\gamma\Delta_{y}(t)v_{+}(t)\;,\;\frac{d}{dt}v_{-}(t)=- \gamma\Delta_{y}(t)v_{-}(t)\] (103)

As a consequence, the field derivatives close

\[\frac{\partial v_{+}(t)}{\partial\Delta_{y}(s)} =\gamma\Theta(t-s)v_{+}(s)+\int_{0}^{t}dt^{\prime}\Delta_{y}(t^{ \prime})\frac{\partial v_{+}(t^{\prime})}{\partial\Delta_{y}(s)}\] \[\frac{\partial v_{-}(t)}{\partial\Delta_{y}(s)} =-\gamma\Theta(t-s)v_{-}(s)-\int_{0}^{t}dt^{\prime}\Delta_{y}(t^ {\prime})\frac{\partial v_{-}(t^{\prime})}{\partial\Delta_{y}(s)}\] (104)

The correlator of interest is

\[\left\langle h_{y}(t)z(t)\right\rangle=\frac{1}{2}\left\langle[v_{+}(t)+v_{-} (t)][v_{+}(t)-v_{-}(t)]\right\rangle=\frac{1}{2}\left\langle v_{+}(t)^{2}-v_ {-}(t)^{2}\right\rangle\] (105)

So we get that

\[D_{y}(t,s) =\frac{1}{2}\left\langle\frac{\partial}{\partial\Delta_{y}(s)} \left(v_{+}(t)^{2}-v_{-}(t)^{2}\right)\right\rangle\] \[=\left\langle v_{+}(t)\frac{\partial v_{+}(t)}{\partial\Delta_{y }(s)}\right\rangle-\left\langle v_{-}(t)\frac{\partial v_{-}(t)}{\partial \Delta_{y}(s)}\right\rangle\] (106)

Similarly, we can derive the on-target and off-target uncoupled variances \(\kappa_{y}(t,s)\) and \(\kappa_{\perp}(t,s)\), which satisfy

\[\kappa_{y}(t,s) =\left\langle v_{+}(t)v_{+}(s)+v_{-}(t)v_{-}(s)\right\rangle^{2}+ \left\langle v_{+}(t)v_{+}(s)-v_{-}(t)v_{-}(s)\right\rangle^{2}\] \[\kappa_{\perp}(t,s) =\frac{1}{2}\left\langle v_{+}(t)v_{+}(s)+v_{-}(t)v_{-}(s)\right\rangle\] (107)Using these functions, we arrive at the following variance for each of the \(P\) dimensions

\[\bm{\Sigma}_{\Delta_{y}} =\left(\bm{\gamma}\mathbf{I}+\bm{D}_{y}\right)^{-1}\bm{\kappa}_{y} \left(\bm{\gamma}\mathbf{I}+\bm{D}_{y}\right)^{-1}\] \[\bm{\Sigma}_{\Delta_{\perp}} =\left(\bm{\gamma}\mathbf{I}+\bm{D}_{\perp}\right)^{-1}\bm{\kappa} _{\perp}\left(\bm{\gamma}\mathbf{I}+\bm{D}_{\perp}\right)^{-1}\] (108)

Using the fact that all \(\Delta_{\perp}\) variables are independent and identically distributed under the leading order picture, the expected training loss has the form

\[\left\langle|\bm{\Delta}|^{2}\right\rangle\approx\Delta_{y}^{\infty}(t)^{2}+ \frac{2}{N}\Delta_{y}^{1}(t)\Delta_{y}^{\infty}(t)+\frac{1}{N}\Sigma_{\Delta_{ y}}(t,t)+\frac{(P-1)}{N}\Sigma_{\Delta_{\perp}}(t,t)+\mathcal{O}(N^{-2}).\] (109)

where \(\left\langle\Delta_{y}-\Delta_{y}^{\infty}\right\rangle=\frac{1}{N}\Delta_{y }^{1}(t)+\mathcal{O}(N^{-2})\). We note that the bias correction if \(\mathcal{O}(N^{-1})\) while the variance is \(\mathcal{O}(P/N)\). We compare the above leading order theory with and without the bias correction in Appendix Figure A.2.

## Appendix K Online Learning

Our technology for computing finite size effects can easily be translated to a setting where the neural network is trained in an online fashion, disregarding the effect of SGD noise. At each step, we compute the gradient over the full data distribution \(p(\bm{x})\). Focusing on MSE loss, we study the following equation

\[\frac{d}{dt}\Delta(\bm{x},t)=-\mathbb{E}_{\bm{x}^{\prime}\sim p(\bm{x}^{ \prime})}K(\bm{x},\bm{x}^{\prime};t)\Delta(\bm{x}^{\prime},t)\] (110)

where \(K(\bm{x},\bm{x}^{\prime};t)\) is the dynamic NTK and \(\Delta(\bm{x},t)=y(\bm{x})-f(\bm{x},t)\) is the prediction error. In general the distribution involves integration over an uncountable set of possible inputs \(\bm{x}\). To remedy this, we utilize a countable orthonormal basis of functions for the data distribution \(\left\{\psi_{k}(\bm{x})\right\}_{k=1}^{\infty}\). For example, if \(p(\bm{x})\) were the isotropic Gaussian density for \(\mathcal{N}(0,\mathbf{I})\), then \(\psi_{k}\) could be Hermite polynomials. We expand \(\Delta\) and \(K\) in this basis \(\psi_{k}\), and arrive at the following differential equation

\[\frac{d}{dt}\Delta_{k}(t)=-\sum_{\ell}K_{k\ell}(t)\Delta_{\ell}(t)\] (111)

By orthonormality, the average turned into a sum over all possible orthonormal functions \(\left\{\psi_{k}\right\}\). We note that since \(K\) is evolving in time, there is not generally a fixed basis of functions that diagonalize \(K\), resulting in the couplings across eigenmodes in Equation (111). Since, in online learning, there is no distinction between the training and test distribution, our error of interest is simply \(\mathcal{L}(t)=\sum_{k}\Delta_{k}(t)^{2}\). To obtain the finite size corrections to this quantity, we compute the joint propagator for all variables \(\left\{K_{k\ell}(t),\Delta_{k}(t)\right\}\). If we wanted to pursue a perturbation theory in rates (Appendix G.2), we could again define a transition matrix \(\bm{T}\) and rate matrix \(\bm{R}(t)\) as

\[\bm{R}(t)=-\log\bm{T}(t)\;,\;\frac{d}{dt}T_{k\ell}(t)=-\sum_{k^{\prime}}K_{kk^ {\prime}}(t)T_{k^{\prime}\ell}(t)\;,\;T_{k\ell}(0)=\delta_{k\ell}\] (112)

We can then obtain \(\bm{\Delta}=\exp(-\bm{R}(t))\bm{y}\), where \(y_{k}=\mathbb{E}_{\bm{x}}\psi_{k}(\bm{x})y(\bm{x})\). Since \(\bm{R}\) has a finite size mean correction and finite size fluctuations, so too does the error \(\Delta_{k}(t)\) and the loss \(\mathcal{L}\) (Appendix G.2).

### Two Layer Networks

In the two layer case, instead of tracking kernels, we could instead deal with the distribution over read-in vectors \(\bm{w}\in\mathbb{R}^{D}\) and readout scalars \(a\in\mathbb{R}\) as in the original works on mean field networks [6, 66]. When training on the population risk equations for \(\bm{x}\sim\mathcal{N}(0,\mathbf{I})\)

\[\frac{d}{dt}\bm{w} =a\mathbb{E}_{\bm{x}}\Delta(\bm{x})\dot{\phi}(\bm{w}\cdot\bm{x}) \bm{x}=\mathbb{E}_{\bm{x}}\frac{\partial\Delta(\bm{x})}{\partial\bm{x}}\dot{ \phi}(\bm{w}\cdot\bm{x})+\mathbb{E}\Delta(\bm{x})\ddot{\phi}(\bm{w}\cdot\bm{x })\bm{w}\] \[\frac{d}{dt}a =\mathbb{E}_{\bm{x}}\Delta(\bm{x})\phi(\bm{w}\cdot\bm{x})\] (113)

The action has the form

\[S=\gamma\int dtd\bm{x}\hat{\Delta}(t,\bm{x})(\Delta(t,\bm{x})-y(\bm{x}))+\ln \mathbb{E}_{a,\bm{w}}\exp\left(\int dtd\bm{x}\hat{\Delta}(t,\bm{x})a(t)\phi( \bm{w}(t)\cdot\bm{x})\right)\] (114)The Hessian over \(\bm{q}=\{\Delta_{\mu}(t),\hat{\Delta}_{\mu}(t)\}\) is

\[\nabla^{2}S=\begin{bmatrix}0&\mathbf{I}+\bm{D}_{\Delta}\\ \mathbf{I}+\bm{D}_{\Delta}&\bm{\kappa}\end{bmatrix}.\] (115)

where \(D_{\Delta}(t,\bm{x};s,\bm{x}^{\prime})=\left\langle\frac{\partial}{\partial \Delta(s,\bm{x}^{\prime})}a(t)\phi(\bm{w}(t)\cdot\bm{x})\right\rangle\) We can use the following implicit rule

\[\frac{\partial a(t)}{\partial\Delta(s,\bm{x})} =\gamma\Theta(t-s)p(\bm{x})\phi(\bm{w}(s)\cdot\bm{x})+\gamma \mathbb{E}_{\bm{x}^{\prime}}\int_{0}^{t}dt^{\prime}\Delta(t^{\prime},\bm{x}^{ \prime})\dot{\phi}(\bm{w}\cdot\bm{x}^{\prime})\bm{x}^{\prime}\cdot\frac{ \partial\bm{w}(t)}{\partial\Delta(s,\bm{x})}\] \[\frac{\partial\bm{w}(t)}{\partial\Delta(s,\bm{x})} =\gamma\Theta(t-s)p(\bm{x})a(s)\dot{\phi}(\bm{w}(s)\cdot\bm{x}) \bm{x}\] \[+\gamma\mathbb{E}_{\bm{x}^{\prime}}\int_{0}^{t}dt^{\prime}\Delta( t^{\prime},\bm{x}^{\prime})\left[\frac{\partial a(t^{\prime})}{\partial \Delta(s,\bm{x})}\dot{\phi}(\bm{w}\cdot\bm{x}^{\prime})+a(t^{\prime})\ddot{\phi }(\bm{w}\cdot\bm{x}^{\prime})\frac{\partial\bm{w}(t^{\prime})}{\partial\Delta (s,\bm{x})}\cdot\bm{x}^{\prime}\right]\] (116)

The above equations could be solved and then used to compute \(D_{\Delta}(t,\bm{x};s,\bm{x}^{\prime})\) which must then be inverted to get the observed prediction variance.

### Linear Activations

Using the ideas in the preceding sections, we can make more progress in the case of a two layer linear network in the online learning setting. The key idea is to track the kernel and prediction error projections onto the space of linear functions. In this case we get the following DMFT over the order parameter \(\bm{\beta}(t)=\frac{1}{N}\bm{W}^{\top}\bm{a}\in\mathbb{R}^{D}\).

\[\frac{d}{dt}a(t) =\gamma(\bm{\beta}_{\star}-\bm{\beta}(t))\cdot\bm{w}(t)\] \[\frac{d}{dt}\bm{w}(t) =\gamma a(t)(\bm{\beta}_{\star}-\bm{\beta}(t))\] \[\bm{\beta}(t) =\frac{1}{\gamma}\left\langle a(t)\bm{w}(t)\right\rangle\] (117)

At infinite width, we see that the dynamics can be reduced to tracking the projection of the weights \(\bm{w}\) and \(\bm{\beta}\) on the \(\bm{\beta}_{\star}\) direction. The \(D-1\) off-target dimensions vanish \(\bm{\beta}_{\perp}(t)=0\). At infinite width, we arrive at the alignment dynamics studied in prior work [64; 9]

\[\frac{d}{dt}\bm{\beta}(t) =\bm{M}(t)(\bm{\beta}_{\star}-\bm{\beta}(t))\] \[\frac{d}{dt}\bm{M}(t) =\gamma^{2}\bm{\beta}(t)(\bm{\beta}_{\star}-\bm{\beta}(t))^{\top }+\gamma^{2}\bm{\beta}(t)(\bm{\beta}_{\star}-\bm{\beta}(t))^{\top}\] \[+2\gamma^{2}(\bm{\beta}_{\star}-\bm{\beta}(t))\cdot\bm{\beta}(t) \mathbf{I}\] (118)

We note that \(\bm{\beta}(t)=\beta(t)\bm{\beta}_{\star}\) and that \(\bm{M}\) has only one special eigenvector \(\bm{\beta}_{\star}\) with eigenvalue \(m_{\star}(t)\). It thus suffices to track evolution in this single direction

\[\frac{d}{dt}\beta(t)=m_{\star}(t)(\beta_{\star}-\beta(t))\;,\;\frac{d}{dt}m_{ \star}(t)=4\gamma^{2}\beta(t)(\beta_{\star}-\beta(t))\] (119)

We note that this equation is identical to the differential equation for a single training example in Appendix J. Here \(\beta_{\star}-\beta(t)\) plays the role of \(\Delta_{y}(t)\) and \(m_{\star}(t)\) plays the role of the kernel \(K_{y}(t)\). A key observation is the conservation law \(4\gamma^{2}\frac{d}{dt}\beta(t)^{2}=\frac{d}{dt}m_{\star}(t)^{2}\), from which it follows that \(m_{\star}(t)^{2}-4=4\gamma^{2}\beta(t)\)[9]

\[\frac{d}{dt}\beta(t)=2\sqrt{1+\gamma^{2}\beta(t)^{2}}(\beta_{\star}-\beta(t))\] (120)

This is identical to the differential equations for a single sample (producing prediction \(f(t)\) and kernel \(K(t)\)) if the following substitutions are made

\[f(t)\leftrightarrow\beta(t)\;,\;K(t)\leftrightarrow m_{\star}(t)\] (121)We now proceed to compute finite size corrections starting from the action

\[S=\gamma\int dt\hat{\bm{\beta}}(t)\cdot\bm{\beta}(t)+\ln\mathbb{E}\exp\left(-\int dt \hat{\bm{\beta}}(t)\cdot\bm{w}(t)a(t)\right)\] (122)

The necessary ingredients are

\[\bm{\kappa}(t,s) =\left\langle a(t)a(s)\bm{w}(t)\bm{w}(s)^{\top}\right\rangle- \gamma^{2}\bm{\beta}(t)\bm{\beta}(s)\] \[=\left\langle a(t)a(s)\right\rangle\left\langle\bm{w}(t)\bm{w}(s )^{\top}\right\rangle+\left\langle a(s)\bm{w}(t)\right\rangle\left\langle a(t )\bm{w}(s)^{\top}\right\rangle\in\mathbb{R}^{D\times D}\] (123)

Similarly we have to compute the sensitivity tensor

\[\bm{D}(t,s)=\left\langle\frac{\partial}{\partial\bm{\beta}(s)^{\top}}a(t)\bm{ w}(t)\right\rangle\in\mathbb{R}^{D\times D}\] (124)

We start from the dynamics

\[\frac{d}{dt}\bm{w}(t)=\gamma a(t)(\bm{\beta}_{\star}-\bm{\beta}(t))\;,\;\frac {d}{dt}a(t)=\gamma(\bm{\beta}_{\star}-\bm{\beta}(t))\cdot\bm{w}(t)\] (125)

Next, we have to calculate causal derivatives for fields

\[\frac{\partial}{\partial\bm{\beta}(s)^{\top}}\bm{w}(t) =-\gamma\Theta(t-s)a(s)\mathbf{I}+\gamma\int_{0}^{t}dt^{\prime}( \bm{\beta}_{\star}-\bm{\beta}(t^{\prime}))\frac{\partial a(t^{\prime})}{ \partial\bm{\beta}(s)^{\top}}\] \[\frac{\partial}{\partial\bm{\beta}(s)}a(t) =-\gamma\Theta(t-s)\bm{w}(s)+\gamma\int_{0}^{t}dt^{\prime}(\bm{ \beta}_{\star}-\bm{\beta}(t^{\prime}))\cdot\frac{\partial\bm{w}(t^{\prime})}{ \partial\bm{\beta}(s)}\] (126)

Following an identical argument as in J, we see that \(\bm{D}\) has block diagonal structure with \(D_{\beta_{\star}}(t,s)\) on the \(\bm{\beta}_{\star}\bm{\beta}_{\star}^{\top}\) direction and \(D_{\perp}(t,s)\) in any of the \(D-1\) remaining directions

\[D_{\beta_{\star}}(t,s)=\left\langle\frac{\partial}{\partial\beta(s)}a(t)w_{ \beta_{\star}}(t)\right\rangle\;,\;D_{\perp}(t,s)=\left\langle\frac{\partial} {\partial\beta_{\perp}(s)}a(t)w_{\perp}(t)\right\rangle\] (127)

Similarly, \(\bm{\kappa}(t,s)\) has a similar decomposition

\[\kappa_{\beta_{\star}}(t,s) =\left\langle a(t)a(s)\right\rangle\left\langle w_{\beta_{\star} }(t)w_{\beta_{\star}}(s)\right\rangle+\left\langle a(s)w_{\beta_{\star}}(t) \right\rangle\left\langle a(t)w_{\beta_{\star}}(s)\right\rangle\] \[\kappa_{\perp}(t,s) =\left\langle a(t)a(s)\right\rangle\left\langle w_{\perp}(t)w_{ \perp}(s)\right\rangle+\left\langle a(s)w_{\perp}(t)\right\rangle\left\langle a (t)w_{\perp}(s)\right\rangle\] (128)

The processes have the following equations at infinite width

\[\frac{d}{dt}w_{\beta_{\star}}(t)=\gamma a(t)(\beta_{\star}-\beta(t))\;,\; \frac{d}{dt}a(t)=\gamma w_{\beta_{\star}}(t)(\beta_{\star}-\beta(t))\;,\; \frac{d}{dt}w_{\perp}(t)=0\] (129)

As a consequence we note that \(\left\langle w_{\perp}(t)a(s)\right\rangle=0\) so that \(\kappa_{\perp}(t,s)=\left\langle a(t)a(s)\right\rangle\). Letting \(v_{+}(t)=\frac{1}{\sqrt{2}}(w_{\beta_{\star}}(t)+a(t))\) and \(v_{-}(t)=\frac{1}{\sqrt{2}}(w_{\beta_{\star}}(t)+a(t))\), we find the same decoupled stochastic processes as in Appendix I.3.

\[\frac{d}{dt}v_{+}(t)=\gamma(\beta_{\star}-\beta(t))v_{+}(t)\;,\;\frac{d}{dt}v_ {-}(t)=-\gamma(\beta_{\star}-\beta(t))v_{-}(t)\] (130)

We can use these equations to perform the necessary averages for \(\kappa_{\beta_{\star}}\) and \(D_{\beta_{\star}}\). Lastly, we use

\[\frac{\partial}{\partial\beta_{\perp}(s)}w_{\perp}(t)=-\gamma\Theta(t-s)a(s)\] (131)

to evaluate \(D_{\perp}(t,s)\). The observed covariances are just

\[\bm{\Sigma}_{\beta_{\star}}=(\gamma\mathbf{I}-\bm{D}_{\beta_{\star}})^{-1}\bm {\kappa}_{\beta_{\star}}(\gamma\mathbf{I}-\bm{D}_{\beta_{\star}})^{-1\top}\;, \;\bm{\Sigma}_{\perp}=(\gamma\mathbf{I}-\bm{D}_{\perp})^{-1}\bm{\kappa}_{ \perp}(\gamma\mathbf{I}-\bm{D}_{\perp})^{-1\top}\] (132)

We note that these expressions are identical to those in Appendix J under the substitution \(\beta_{\star}-\beta(t)\to\Delta(t)\) and \(D\to P\). Thus the expected test risk is

\[\left\langle|\bm{\beta}(t)-\bm{\beta}_{\star}|^{2}\right\rangle\sim(\beta(t)- \beta_{\star})^{2}+\frac{1}{N}\Sigma_{\beta_{\star}}(t,t)+\frac{(D-1)}{N} \Sigma_{\beta_{\perp}}(t,t)+\mathcal{O}(N^{-2})\] (133)

This recovers the variance we obtained in the multiple-sample whitened data case J.

### Connections to Offline Learning in Linear Model

**Remark 1**: _The finite size variance of generalization error in an online learning setting with linear target function \(y=\bm{\beta}^{\star}\cdot\bm{x}\) has an identical form as the model described above. In this setting, we sample infinitely many fresh data points \(\bm{x}\sim\mathcal{N}(0,\mathbf{I})\) at each step leading to the flow \(\frac{d}{dt}\bm{w}_{i}(t)=\gamma a_{i}(t)\mathbb{E}_{\bm{x}}\Delta(\bm{x})\bm{x}\) and \(\frac{d}{dt}a_{i}(t)=\gamma\bm{w}_{i}(t)\cdot\mathbb{E}_{\bm{x}}\Delta(\bm{x}) \bm{x}\). The order parameter of interest in this setting is \(\bm{\beta}(t)=\frac{1}{\gamma N}\sum_{i=1}^{N}\bm{w}_{i}(t)a_{i}(t)\). The precise correspondence between this setting and the offline setting is summarized in Table 2. We note that this argument could be extended to higher degree monomial activations as well, at the cost of tracking higher degree tensors (eg for quadratic activations \(\bm{M}=\frac{1}{N}\sum_{i=1}^{N}a_{i}\bm{w}_{i}\bm{w}_{i}^{\top}\in\mathbb{R}^{ D\times D}\) is sufficient)._

As in the offline case, in Fig. 4 (c) and (d) we see that the variance contribution to test loss \(|\bm{\beta}-\bm{\beta}_{\star}|^{2}\) increases with input dimension \(D\). We note that this perturbative effect to the loss dynamics is reminiscent of the deviations from mean field behavior studied in SGD [43, 44], though this present work concerns fluctuations driven by initialization variance rather than stochastic sampling of data. In Fig. 4 (e) we show that richer networks have lower variance at fixed \(N\). Similarly, leading order theory for richer networks more accurately captures their dynamics as \(D/N\) increases (Fig. 4 (f)).

## Appendix L Deep Linear Networks

For deep linear networks, the fields \(h_{\mu}^{\ell}(t),g_{\mu}^{\ell}(t)\) are Gaussian and have the following self-consistent equations

\[h_{\mu}^{\ell}(t) =u_{\mu}^{\ell}(t)+\gamma\int_{0}^{t}ds\sum_{\nu}\left[A_{\mu\nu} ^{\ell-1}(t,s)+\Delta_{\nu}(s)H_{\mu\nu}^{\ell-1}(t,s)\right]g_{\nu}^{\ell}(s) \;,\;u_{\mu}^{\ell}(t)\sim\mathcal{GP}(0,\bm{H}^{\ell-1})\] (134) \[g_{\mu}^{\ell}(t) =r_{\mu}^{\ell}(t)+\gamma\int_{0}^{t}ds\sum_{\nu}\left[B_{\mu\nu} ^{\ell}(t,s)+\Delta_{\nu}(s)G_{\mu\nu}^{\ell+1}(t,s)\right]h_{\nu}^{\ell}(s) \;,\;r_{\mu}^{\ell}(t)\sim\mathcal{GP}(0,\bm{G}^{\ell+1}).\]

where \(H_{\mu\nu}^{\ell}(t,s)=\left\langle h_{\mu}^{\ell}(t)h_{\nu}^{\ell}(s)\right\rangle\) and \(G_{\mu\nu}^{\ell}(t,s)=\left\langle g_{\mu}^{\ell}(t)g_{\nu}^{\ell}(s)\right\rangle\) and \(A_{\mu\nu}^{\ell}(t,s)=\left\langle\frac{\partial h_{\mu}^{\ell}(t)}{\partial r _{\nu}(s)}\right\rangle\) and \(B_{\mu\nu}^{\ell}(t,s)=\left\langle\frac{\partial h_{\mu}^{\ell}(t)}{\partial r _{\nu}(s)}\right\rangle\)[9]. Therefore, we express the action as a differentiable function of the order parameters by integrating over the Gaussian field distribution. For concreteness, we vectorize our fields over time and samples \(\bm{h}^{\ell}=\text{Vec}\{h_{\mu}^{\ell}(t)\}_{\{\mu\in[P],t\in\mathbb{R}_{+}\}}\), \(\bm{g}^{\ell}=\text{Vec}\{g_{\mu}^{\ell}(t)\}_{\{\mu\in[P],t\in\mathbb{R}_{+}\}}\) we consider the contribution of a single hidden layer.

\[\mathcal{Z}_{\ell}=\int d\hat{\bm{h}}^{\ell}d\hat{\bm{g}}^{\ell}d \bm{h}^{\ell}d\bm{g}^{\ell}\exp\left(-\frac{1}{2}\hat{\bm{h}}^{\ell}\bm{\Sigma }_{u}\hat{\bm{h}}^{\ell}+i\hat{\bm{h}}^{\ell}\cdot(\bm{h}^{\ell}-\bm{C}^{\ell} \bm{g}^{\ell})-\frac{1}{2}\bm{h}^{\ell\top}\hat{\bm{H}}^{\ell}\bm{h}^{\ell}\right)\] \[\exp\left(-\frac{1}{2}\hat{\bm{g}}^{\ell}\bm{\Sigma}_{r}^{\ell} \hat{\bm{g}}^{\ell}+i\hat{\bm{g}}^{\ell}\cdot(\bm{g}^{\ell}-\bm{D}^{\ell}\bm{h} ^{\ell})-\frac{1}{2}\bm{g}^{\ell\top}\hat{\bm{G}}^{\ell}\bm{g}^{\ell}\right)\]

where \(C_{\mu\nu}^{\ell}(t,s)=\gamma\Theta(t-s)\left[A_{\mu\nu}^{\ell-1}(t,s)+H_{\mu \nu}^{\ell-1}(t,s)\Delta_{\nu}(s)\right]\) and \(D_{\mu\nu}^{\ell}(t,s)=\gamma\Theta(t-s)\left[B_{\mu\nu}^{\ell}(t,s)+G_{\mu\nu }^{\ell+1}(t,s)\Delta_{\nu}(s)\right]\). Performing the joint Gaussian integrals over \((\bm{h}^{\ell},\bm{g}^{\ell},\hat{\bm{h}}^{\ell},\hat{\bm{g}}^{\ell})\) we find

\[\ln\mathcal{Z}_{\ell}=-\frac{1}{2}\ln\det\begin{bmatrix}-\hat{\bm{H}}^{\ell}&0& \mathbf{I}&-\bm{D}^{\ell\top}\\ 0&-\hat{\bm{G}}^{\ell}&-\bm{C}^{\ell\top}&\mathbf{I}\\ \mathbf{I}&-\bm{C}^{\ell}&\bm{\Sigma}_{u}&0\\ -\bm{D}^{\ell}&\mathbf{I}&0&\bm{\Sigma}_{r}\end{bmatrix}\] (135)

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|} \hline Setting & Order Params. & Target & Off-target Dims. & Loss & Variance & Infinite Quantity \\ \hline Offline & \(\bm{\Delta}=\bm{y}-\bm{f}\) & \(\bm{y}\) & \(P-1\) & Train & \(\mathcal{O}(\frac{P}{N})\) & \(D\) \\ \hline Online & \(\bm{\beta}_{\star}-\bm{\beta}\) & \(\bm{\beta}_{\star}\) & \(D-1\) & Test & \(\mathcal{O}(\frac{D}{N})\) & \(P\) \\ \hline \end{tabular}
\end{table}
Table 2: Summary of the equivalence between the leading \(1/N\) correction in the offline setting and the online setting for two layer linear networks. In the offline training setting, the order parameters are the errors \(\bm{\Delta}=\bm{y}-\bm{f}\in\mathbb{R}^{P}\) while in the online case they are \(\bm{\beta}_{\star}-\bm{\beta}\in\mathbb{R}^{D}\).

We can then automatically differentiate the DMFT action to get the propagator. For example, for a three layer linear network, the full DMFT action has the form

\[S =\frac{1}{2}\text{Tr}\left[\hat{\bm{H}}^{1}\bm{H}^{1}+\hat{\bm{H}}^{2 }\bm{H}^{2}+\hat{\bm{G}}^{1}\bm{G}^{1}+\hat{\bm{G}}^{2}\bm{G}^{2}\right]-\gamma^ {2}\text{Tr}\bm{A}\bm{B}\] \[-\frac{1}{2}\ln\det\begin{bmatrix}-\hat{\bm{H}}^{1}&0&\mathbf{I}& -\bm{D}^{1\top}\\ 0&-\hat{\bm{G}}^{1}&-\bm{C}^{1\top}&\mathbf{I}\\ \mathbf{I}&-\bm{C}^{1}&\mathbf{1}\bm{1}^{\top}&0\\ -\bm{D}^{1}&\mathbf{I}&0&\bm{G}^{2}\end{bmatrix}\] \[-\frac{1}{2}\ln\det\begin{bmatrix}-\hat{\bm{H}}^{2}&0&\mathbf{I}& -\bm{D}^{2\top}\\ 0&-\hat{\bm{G}}^{2}&-\bm{C}^{2\top}&\mathbf{I}\\ \mathbf{I}&-\bm{C}^{2}&\bm{H}^{1}&0\\ -\bm{D}^{2}&\mathbf{I}&0&\mathbf{1}\bm{1}^{\top}\end{bmatrix}\] (136)

where \(\bm{C}^{1}=\gamma\bm{\Theta}_{\Delta}\) and \(\bm{C}^{2}=\gamma\bm{\Theta}_{\Delta}\odot\bm{H}^{1}+\gamma\bm{A}\) and \(\bm{D}^{1}=\gamma\bm{\Theta}_{\Delta}\odot\bm{G}^{2}+\gamma\bm{B}\) and \(\bm{D}^{2}=\gamma\bm{\Theta}_{\Delta}\). This above example can be extended to deeper networks. The total size of the block matrices which we compute determinants over is \(4PT\times 4PT\) for a dataset of size \(P\) trained for \(T\) steps.

## Appendix M Discrete Time Dynamics and Edge of Stability Effects

Large step size effects can induce qualitatively different dynamics in neural network training. For instance, if the step size exceeds that required for linear stability with the initial kernel, the kernel can decrease in order to stabilize the dynamics [57]. Alternatively, during training the kernel may exhibit a "progressive sharpening" phase where its top eigenvalue grows before reaching a stability bound set by the learning rate [19]. It is therefore well motivated to study how dynamics in this regime alter finite size effects in neural networks. We will first solve a special model which was considered in prior work [57]: a two layer linear network trained on a single training point. We will then provide the full DMFT equations for the discrete time case and provide an outline for how one could obtain finite size effects in that picture.

### Two Layer Linear Equations

In a two layer linear network, the DMFT equations are

\[h(t+1) =h(t)+\eta\gamma\Delta(t)z(t)\;,\;z(t+1)=z(t)+\eta\gamma\Delta(t)h(t)\] \[f(t) =\frac{1}{\gamma}\left\langle z(t)h(t)\right\rangle\] (137)

The NTK has the form \(K(t)=\left\langle h(t)^{2}+z(t)^{2}\right\rangle\). We can easily show that the kernel and error have coupled dynamics

\[f(t+1) =f(t)+\eta\left\langle h(t)^{2}+z(t)^{2}\right\rangle\Delta(t)+ \eta^{2}\gamma\Delta(t)^{2}\left\langle h(t)z(t)\right\rangle\] \[=f(t)+\eta K(t)\Delta(t)+\eta^{2}\gamma^{2}\Delta(t)^{2}f(t)\] (138) \[K(t+1) =K(t)+4\eta\gamma\Delta(t)\left\langle h(t)z(t)\right\rangle+ \eta^{2}\gamma^{2}\Delta(t)^{2}\left\langle h(t)^{2}+z(t)^{2}\right\rangle\] \[=K(t)+4\eta\gamma^{2}\Delta(t)f(t)+\eta^{2}\gamma^{2}\Delta(t)^{2 }K(t)\] (139)

These equations define the infinite width evolution of \(\Delta(t)\) and \(K(t)\). Already at this level of analysis, we can reason about the evolution of \(K(t)\). In the small \(\eta\) limit, we could disregard terms of order \(\mathcal{O}(\eta^{2})\) and arrive at the following gradient flow approximation for \(K(t)\sim 2\sqrt{1+\gamma^{2}f(t)^{2}}\)[9]. This evolution will not reach the edge of stability provided that \(\eta<\frac{1}{\sqrt{1+\gamma^{2}y^{2}}}\). For large \(\gamma\) and \(y=1\), this leads to the constraint \(\eta\gamma<1\). However, if \(\eta\) exceeds this bound, the gradient flow approximation is no longer reasonable and the system reaches an edge of stability effect as shown in Figure 6.

To calculate the finite size effects, we need to compute \(\kappa\) and \(D(t,s)=\frac{\partial}{\partial\Delta(s)}\left\langle h(t)^{2}+z(t)^{2}\right\rangle\). To evaluate these quantities we utilize the same change of variables employed in Appendix I.3. In discrete time, these decoupled equations are

\[v_{+}(t+1)=v_{+}(t)+\eta\gamma\Delta(t)v_{+}(t)\;,\;v_{-}(t+1)=v_{-}(t)-\eta \gamma\Delta(t)v_{-}(t).\] (140)Given \(\Delta(t)\), these can be expressed as linear systems of equations. Now, we can easily compute the uncoupled kernel variance

\[\kappa(t,s) =2\left\langle h(t)h(s)\right\rangle^{2}+2\left\langle z(t)z(s) \right\rangle^{2}+2\left\langle h(t)z(s)\right\rangle^{2}+2\left\langle z(t)h(s) \right\rangle^{2}\] \[=\left\langle v_{+}(t)v_{+}(s)+v_{-}(t)v_{-}(s)\right\rangle^{2} +\left\langle v_{+}(t)v_{+}(s)-v_{-}(t)v_{-}(s)\right\rangle^{2}.\] (141)

Similarly, we can calculate \(D(t,s)\) by using the fact \(\left\langle h(t)^{2}+z(t)^{2}\right\rangle=\left\langle v_{+}(t)^{2}+v_{-}(t) ^{2}\right\rangle\)

\[D(t,s) =2\left\langle v_{+}(t)\frac{\partial v_{+}(t)}{\partial\Delta(s) }\right\rangle+2\left\langle v_{-}(t)\frac{\partial v_{-}(t)}{\partial\Delta( s)}\right\rangle\] \[\frac{\partial v_{+}(t)}{\partial\Delta(s)} =\gamma\Theta(t-s)v_{+}(s)+\sum_{t^{\prime}<t}\Delta(t^{\prime}) \frac{\partial v_{+}(t^{\prime})}{\partial\Delta(s)}\] \[\frac{\partial v_{-}(t)}{\partial\Delta(s)} =-\gamma\Theta(t-s)v_{-}(s)-\sum_{t^{\prime}<t}\Delta(t^{\prime}) \frac{\partial v_{-}(t^{\prime})}{\partial\Delta(s)}\] (142)

These can be directly solved as a linear system of equations.

## Appendix N Computing Details

Experiments for Figures 3, 6 and 2 were conducted on a Google Colab GPU with JAX. Experiments for Figures 5, A.3, 7 were performed on a NVIDIA SMX4-A100-80GB GPU. The total compute required for all Figures in the paper took around 4 hours. Jupyter Notebooks to reproduce plots can be found at https://github.com/Pehlevan-Group/dmft_fluctuations.