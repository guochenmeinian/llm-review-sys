ReHLine: Regularized Composite ReLU-ReHU Loss Minimization with Linear Computation and Linear Convergence

 Ben Dai

Department of Statistics

The Chinese University of Hong Kong

bendai@cuhk.edu.hk

&Yixuan Qiu

School of Statistics and Management

Shanghai University of Finance and Economics

qiuyixuan@sufe.edu.cn

Co-first authorship, and the order is determined by coin toss. \({}^{\dagger}\)Corresponding author.

###### Abstract

Empirical risk minimization (ERM) is a crucial framework that offers a general approach to handling a broad range of machine learning tasks. In this paper, we propose a novel algorithm, called ReHLine, for minimizing a set of regularized ERMs with convex piecewise linear-quadratic loss functions and optional linear constraints. The proposed algorithm can effectively handle diverse combinations of loss functions, regularization, and constraints, making it particularly well-suited for complex domain-specific problems. Examples of such problems include FairSVM, elastic net regularized quantile regression, Huber minimization, etc. In addition, ReHLine enjoys a provable _linear_ convergence rate and exhibits a per-iteration computational complexity that scales _linearly_ with the sample size. The algorithm is implemented with both Python and R interfaces, and its performance is benchmarked on various tasks and datasets. Our experimental results demonstrate that ReHLine significantly surpasses generic optimization solvers in terms of computational efficiency on large-scale datasets. Moreover, it also outperforms specialized solvers such as liblinear in SVMs, hqreg in Huber minimization and lightning (SAGA, SAG, SDCA, SVRG) in smoothed SVMs, exhibiting exceptional flexibility and efficiency. The source code, project page, accompanying software, and the Python/R interface can be accessed through the link: [https://github.com/softmin/ReHLine](https://github.com/softmin/ReHLine).

## 1 Introduction

Empirical risk minimization (ERM) with different losses is a fundamental framework in statistical machine learning [43], which has demonstrated a significant impact on many application areas, such as artificial intelligence, computational biology, computer vision, natural language processing, electronic engineering, and finance. In fact, many common practical ERMs are associated with a piecewise linear-quadratic (PLQ; [34]) loss function, including support vector machines (SVMs; [7]), the least absolute deviation regression, quantile regression [26], Huber regression [22], borrowing cost function in portfolio selection [28], and pricing/planning problems in electricity and operational research [16; 1], among many others. In addition, ERMs are frequently blended with linear constraints, for instance, the fairness constraints in fair classification [48], monotonicity constraints in classification or regression, etc. More linear constraints according to the prior domain knowledge, such as the sum-to-one constraints in the portfolio variance minimization problem [24], negative or positive constraints in non-performing loan [36], are considered in various real applications. Therefore, it has become of utmost importance to develop fast and scalable numerical algorithms to solve ERMs with PLQ losses based on large-scale datasets and flexible linear constraints.

In this paper, we consider a general regularized ERM based on a convex PLQ loss with linear constraints:

\[\min_{\mathbf{\beta}\in\mathbb{R}^{d}}\sum_{i=1}^{n}L_{i}(\mathbf{x}_{i}^{\intercal} \mathbf{\beta})+\frac{1}{2}\|\mathbf{\beta}\|_{2}^{2},\quad\text{ s.t. }\mathbf{A}\mathbf{\beta}+\mathbf{b}\geq\mathbf{0}, \tag{1}\]

where \(\mathbf{x}_{i}\in\mathbb{R}^{d}\) is the feature vector for the \(i\)-th observation, \(\mathbf{\beta}\in\mathbb{R}^{d}\) is an unknown coefficient vector, \(\mathbf{A}\in\mathbb{R}^{K\times d}\) and \(\mathbf{b}\in\mathbb{R}^{K}\) are defined as linear inequality constraints for \(\mathbf{\beta}\), and \(L_{i}(\cdot)\geq 0\) is a convex PLQ loss function. The PLQ function class greatly generalizes existing popular loss functions, and the convexity of \(L_{i}(\cdot)\) guarantees the global convergence of optimization. Here, we focus on working with a large-scale dataset, where the dimension of the coefficient vector and the total number of constraints are comparatively much smaller than the sample size, that is, \(d\ll n\) and \(K\ll n\).

Although (1) is a strongly convex problem with affine constraints, thus admitting a unique global optimum, directly solving (1) by a generic solver may encounter various challenges. For example, \(L_{i}(\cdot)\) is in general non-smooth, so gradient-based methods may fail, and hence slow-convergent subgradient methods need to be used. In addition, the constraint set is a polyhedron, whose projection operator is non-trivial to compute, making various projection-based methods difficult to apply.

Of course, for some specific forms of \(L_{i}(\cdot)\), highly efficient solvers for (1) indeed exist. One remarkable example is the liblinear solver [12], which has gained great success in solving large-scale SVMs. However, liblinear highly relies on the hinge loss function, so its success does not directly transfer to more general loss functions.

Outline.To this end, in this article we develop a novel algorithm to solve (1) with the greatest generality. The proposed algorithm, named ReHLine, has three key ingredients. First, we show that any convex PLQ loss function can be decomposed as the sum of a finite number of rectified linear units (ReLU, [15]) and rectified Huber units (ReHU, defined in Section 2). Second, based on this decomposition, the dual problem of (1) is shown to be a box-constrained quadratic programming (box-QP) problem. Finally, a special paired primal-dual coordinate descent algorithm is developed to simultaneously solve the primal and dual problems, with a computational cost linear in \(n\).

We emphasize that for the second point, the dual box-QP problem is highly structured. Therefore, a general-purpose solver may not fully exploit the structure, thus leading to slower convergence rates or higher computational costs. As a direct comparison between the proposed ReHLine solver and existing algorithms, including the projected (sub-)gradient descent (P-GD; [6]), the coordinate descent (CD; [45]), the interior-point methods (IPMs; [13, 14, 18]), the stochastic dual coordinate ascent (SDCA; [39, 40]) related methods, and the alternating direction method of multipliers (ADMM; [5]), Table 1 summarizes the required number of iterations and per-iteration cost of each method. More details and discussions for this comparison are given in Section 4.

Contribution.Compared with other specialized ERM solvers or general-purpose box-QP solvers, the proposed ReHLine solver has four appealing "linear properties":

\begin{table}
\begin{tabular}{l l l l} \hline \hline Algorithm & Complexity & \#Iterations & Complexity \\  & (per iteration) & & (total) \\ \hline P-GD & \(\mathcal{O}(n)\) & \(\mathcal{O}(\varepsilon^{-1})\)[6] & \(\mathcal{O}\big{(}n\varepsilon^{-1}\big{)}\) \\ CD & \(\mathcal{O}(n^{2})\) & \(\mathcal{O}(\log(\varepsilon^{-1}))\)[31] & \(\mathcal{O}\big{(}n^{2}\log(\varepsilon^{-1})\big{)}\) \\ IPM & \(\mathcal{O}(n^{2})\) & \(\mathcal{O}(\log(\varepsilon^{-1}))\)[18] & \(\mathcal{O}\big{(}n^{2}\log(\varepsilon^{-1})\big{)}\) \\ ADMM & \(\mathcal{O}(n^{2})\) & \(o(\varepsilon^{-1})\)[9, 20] & \(o\big{(}n^{2}\varepsilon^{-1}\big{)}\) \\ SDCA & \(\mathcal{O}(n)\) & \(\mathcal{O}(\varepsilon^{-1})\)[39] & \(\mathcal{O}\big{(}n\varepsilon^{-1}\big{)}\) \\ ReHLine (ours) & \(\mathcal{O}(n)\) & \(\mathcal{O}(\log(\varepsilon^{-1}))\) & \(\mathcal{O}(n\log(\varepsilon^{-1}))\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Overview of existing algorithms for solving (1). Column Complexity (per iteration) shows the computational complexity of the algorithm per iteration. Here, we focus only on the order of \(n\) since \(d\ll n\) is assumed in our setting. Column #Iterations shows the number of iterations needed to achieve an accuracy of \(\varepsilon\) to the minimizer.

1. It applies to any convex piecewise linear-quadratic loss function (potential for non-smoothness included), including the hinge loss, the check loss, the Huber loss, etc.
2. In addition, it supports linear equality and inequality constraints on the parameter vector.
3. The optimization algorithm has a provable linear convergence rate.
4. The per-iteration computational complexity is linear in the sample size.

Moreover, ReHLine is designed to be a computationally efficient and practically useful software package for large-scale ERM problems. As an example, our experiments in Section 5 have shown that ReHLine significantly surpasses generic solvers in terms of computational efficiency on large-scale datasets, and provides reasonable improvements over specialized solvers and problems, such as liblinear, hqreg and lightning, given that liblinear, hqreg and lightning are one of the fastest and most widely-used solvers for SVM, Huber minimization, and smoothed SVM, respectively. Finally, ReHLine is available as user-friendly software in both Python and R, which facilitates ease of use for researchers, practitioners, and a wider audience.

## 2 The ReLU-ReHU decomposition

As is introduced in Section 1, we attempt to solve ERM with a general loss function, and one sensible choice is the class of convex PLQ functions. A univariate loss function \(f:\mathbb{R}\rightarrow\mathbb{R}_{\geq 0}\) is PLQ if there exist a finite number of knots \(t_{1}<t_{2}<\cdots<t_{K}\) such that \(f\) is equal to a linear or quadratic function on each of the intervals \((-\infty,t_{1}],[t_{1},t_{2}],\ldots,[t_{K-1},t_{K}],[t_{K},\infty)\). In what follows we merely consider nonnegative convex PLQ functions, which also imply the continuity of the loss function. It is well known that PLQ functions are universal approximators of continuous functions [38], but the canonical definition of PLQ is not convenient for optimization algorithms. Instead, in this section we prove that convex PLQ functions can actually be decomposed into a series of simple and basic units.

**Definition 1** (Composite ReLU-ReHU function).: _A function \(L(z)\) is composite ReLU-ReHU, if there exist \(\mathbf{u},\mathbf{v}\in\mathbb{R}^{L}\) and \(\boldsymbol{\tau},\mathbf{s},\mathbf{t}\in\mathbb{R}^{H}\) such that_

\[L(z)=\sum_{l=1}^{L}\mathrm{ReLU}(u_{l}z+v_{l})+\sum_{h=1}^{H}\mathrm{ReLU}_{ \tau_{h}}(s_{h}z+t_{h}), \tag{3}\]

_where \(\mathrm{ReLU}(z)=z_{+}\), and \(\mathrm{ReLU}_{\tau_{h}}(z)\) is defined in (2)._

Clearly, the composite ReLU-ReHU function class is closed under affine transformations, as is indicated by Proposition 1.

**Proposition 1** (Closure under affine transformation).: _If \(L(z)\) is a composite ReLU-ReHU function as in (3), then for any \(c>0\), \(p\in\mathbb{R}\), and \(q\in\mathbb{R}\), \(cL(pz+q)\) is also composite ReLU-ReHU, that is,_

\[cL(pz+q)=\sum_{l=1}^{L}\mathrm{ReLU}(u^{\prime}_{l}z+v^{\prime}_{l})+\sum_{h=1 }^{H}\mathrm{ReLU}_{\tau^{\prime}_{h}}(s^{\prime}_{h}z+t^{\prime}_{h}),\]

_where \(u^{\prime}_{l}=cpu_{l}\), \(v^{\prime}_{l}=cu_{l}q+cv_{l}\), \(\tau^{\prime}_{h}=\sqrt{c}\tau_{h}\), \(s^{\prime}_{h}=\sqrt{c}ps_{h}\), and \(t^{\prime}_{h}=\sqrt{c}(s_{h}q+t_{h})\)._

Figure 1: The ReLU function and the ReHU function with \(\tau=1,2,3,\infty\).

More importantly, we show that the composite ReLU-ReHU function class is equivalent to the class of convex PLQ functions.

**Theorem 1**.: _A loss function \(L:\mathbb{R}\rightarrow\mathbb{R}_{\geq 0}\) is convex PLQ if and only if it is composite ReLU-ReHU._

Based on the fact that the loss functions of various SVMs (the hinge loss, the squared hinge loss, and the smoothed hinge loss), quantile regression (the check loss), and least absolute deviation (the absolute loss) are all convex PLQs, Theorem 1 suggests that Definition 1 incorporates numerous widely-used losses in statistics and machine learning applications. Table 2 provides a convenient translation from some commonly-used loss functions to their ReLU-ReHU representation.

Taken together, our algorithm is designed to address the empirical regularized ReLU-ReHU minimization problem, named _ReHLine optimization_, of the following form:

\[\min_{\mathbf{\beta}\in\mathbb{R}^{d}} \sum_{i=1}^{n}\sum_{l=1}^{L}\mathrm{ReLU}(u_{li}\mathbf{x}_{i}^{ \intercal}\mathbf{\beta}+v_{li})+\sum_{i=1}^{n}\sum_{h=1}^{H}\mathrm{ReLU}_{\tau_{ hi}}(s_{hi}\mathbf{x}_{i}^{\intercal}\mathbf{\beta}+t_{hi})+\frac{1}{2}\|\mathbf{\beta}\|_{2}^ {2},\] \[\mathrm{s.t.} \mathbf{A}\mathbf{\beta}+\mathbf{b}\geq\mathbf{0}, \tag{4}\]

where \(\mathbf{U}=(u_{li}),\mathbf{V}=(v_{li})\in\mathbb{R}^{L\times n}\) and \(\mathbf{S}=(s_{hi}),\mathbf{T}=(t_{hi}),\mathbf{\uptau}=(\tau_{hi})\in\mathbb{R}^ {H\times n}\) are the ReLU-ReHU loss parameters, as illustrated in Table 2, and \((\mathbf{A},\mathbf{b})\) are the constraint parameters, as illustrated in Table 3. This formulation has a wide range of applications, including statistics, machine learning, computational biology, and social studies. Some popular examples include SVMs with fairness constraints (FairSVM), elastic net regularized quantile regression (ElasticQR), ridge Huber minimization (RidgeHuber), and smoothed SVMs (sSVM). See Appendix A for details.

Despite appearing to impose limitations on the \(l_{2}\)-regularization in (4), our ReHLine formulation can actually be extended to more general regularization forms, including the elastic net penalty [49].

**Proposition 2**.: _If \(L_{i}\) is a composite ReLU-ReHU loss function with parameters \((u_{li},v_{li})_{l=1}^{L}\) and \((s_{hi},t_{hi},\tau_{hi})_{h=1}^{H}\), then its elastic net regularized minimization with \(\lambda_{1}\geq 0\) and \(\lambda_{2}>0\),_

\[\min_{\mathbf{\beta}\in\mathbb{R}^{d}}\sum_{i=1}^{n}L_{i}(\mathbf{x}_{i}^{ \intercal}\mathbf{\beta})+\lambda_{1}\|\mathbf{\beta}\|_{1}+\frac{\lambda_{2}}{2}\| \mathbf{\beta}\|_{2}^{2},\quad\mathrm{s.t.}\quad\mathbf{A}\mathbf{\beta}+\mathbf{b} \geq\mathbf{0},\]

\begin{table}
\begin{tabular}{l l l} \hline \hline Problem & Loss (\(L_{i}(z_{i})\)) & Composite ReLU-ReHU Parameters \\ \hline SVM & \(c_{i}(1-y_{i}z_{i})_{+}\) & \(u_{1i}=-c_{i}y_{i},v_{1i}=c_{i}\) \\ sSVM & \(c_{i}\mathrm{ReLU}_{1}(-(y_{i}z_{i}-1))\) & \(s_{1i}=-\sqrt{c_{i}}y_{i},t_{1i}=\sqrt{c_{i}},\tau=\sqrt{c_{i}}\) \\ SVM\({}^{2}\) & \(c_{i}((1-y_{i}z_{i})_{+})^{2}\) & \(s_{1i}=-\sqrt{2c_{i}}y_{i},t_{1i}=\sqrt{2c_{i}},\tau=\infty\) \\ LAD & \(c_{i}|y_{i}-z_{i}|\) & \(u_{1i}=c_{i},v_{1i}=-c_{i}y_{i},u_{2i}=-c_{i},v_{2i}=c_{i}y_{i}\) \\ SVR & \(c_{i}(|y_{i}-z_{i}|-\varepsilon)_{+}\) & \(u_{1i}=c_{i},v_{1i}=-(y_{i}+\varepsilon),u_{2i}=-c_{i},v_{2i}=y_{i}-\varepsilon\) \\ QR & \(c_{i}\rho_{\kappa}(y_{i}-z_{i})\) (A.3) & \(u_{1i}=-c_{i}\kappa,v_{1i}=\kappa c_{i}y_{i},u_{2i}=c_{i}(1-\kappa),v_{2i}=-c_ {i}(1-\kappa)y_{i}\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: Some widely-used composite ReLU-ReHU losses as in (3). Here SVM is weighted SVMs based on the hinge loss [7], sSVM is smoothed SVMs based on the smoothed hinge loss [33], SVM\({}^{2}\) is weighted squared SVMs based on the squared hinge loss [7], LAD is the least absolute deviation regression, SVR is support vector regression with the \(\varepsilon\)-insensitive loss [44], and QR is quantile regression with the check loss [26].

\begin{table}
\begin{tabular}{l l l} \hline \hline Constraint & Form & ReHLine Parameters \\ \hline Nonnegative & \(\mathbf{\beta}\geq\mathbf{0}\) & \(\mathbf{A}=-\mathbf{I},\mathbf{b}=\mathbf{0}\) \\ Box & \(\mathbf{1}\leq\mathbf{\beta}\leq\mathbf{u}\) & \(\mathbf{A}=[\mathbf{I};-\mathbf{I}],\mathbf{b}=[-\mathbf{I};\mathbf{u}]\) \\ Monotonicity & \(\beta_{1}\leq\cdots\leq\beta_{d}\) & \(\mathbf{A}=\mathbf{B}_{1:d-1},\mathbf{B}=\mathbf{U}\)-bidag\(([-1,1])\), \(\mathbf{b}=\mathbf{0}_{d-1}\) \\ Fairness (A.1) & \(|\mathbf{Z}^{\intercal}\mathbf{X}\mathbf{\beta}/n|\leq\mathbf{\rho}\) & \(\mathbf{A}=\mathbf{Z}^{\intercal}\mathbf{X}[\mathbf{I};-\mathbf{I}]/n,\mathbf{ b}=[\mathbf{\rho};\mathbf{\rho}]\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Some widely-used linear constraints of the form \(\mathbf{A}\mathbf{\beta}+\mathbf{b}\geq 0\), where \(\text{U-bidiag}([-1,1])\) is an upper bidiagonal matrix with main diagonal being -1 and upper diagonal being 1.

can be rewritten as the form of ReHLine optimization (4) with_

\[\mathbf{U}\leftarrow\left(\begin{array}{cc}\frac{1}{\lambda_{2}} \mathbf{U}&\mathbf{0}_{L\times d}\\ \mathbf{0}_{n}^{\intercal}&\frac{\lambda_{1}}{\lambda_{2}}\mathbf{I}_{d}^{ \intercal}\\ \mathbf{0}_{n}^{\intercal}&-\frac{\lambda_{1}}{\lambda_{2}}\mathbf{I}_{d}^{ \intercal}\end{array}\right),\quad\quad\mathbf{V}\leftarrow\left(\begin{array} []{cc}\frac{1}{\lambda_{2}}\mathbf{V}&\mathbf{0}_{L\times d}\\ \mathbf{0}_{n+d}^{\intercal}\\ \mathbf{0}_{n+d}^{\intercal}\end{array}\right),\quad\quad\mathbf{X} \leftarrow\left(\begin{array}{c}\mathbf{X}\\ \mathbf{I}_{d}\end{array}\right),\] \[\mathbf{S}\leftarrow\left(\begin{array}{cc}\sqrt{\frac{1}{ \lambda_{2}}}\mathbf{S}&\mathbf{0}_{d}^{\intercal}\end{array}\right),\quad \quad\quad\mathbf{T}\leftarrow\left(\begin{array}{cc}\sqrt{\frac{1}{\lambda_{ 2}}}\mathbf{T}&\mathbf{0}_{d}^{\intercal}\end{array}\right),\quad\quad\quad \mathbf{\tau}\leftarrow\left(\begin{array}{cc}\sqrt{\frac{1}{\lambda_{2}}} \mathbf{\tau}&\mathbf{0}_{d}^{\intercal}\end{array}\right),\]

_where \(\leftarrow\) indicates assigning a value to a parameter._

## 3 Optimization algorithm

In this section, we present the ReHLine algorithm to tackle the optimization in (4). The proposed algorithm is inspired by liblinear[12], which has gained great success in solving standard SVMs. The key idea of ReHLine is to leverage the linear property of Karush-Kuhn-Tucker (KKT) conditions, thus simplifying the iterative complexity of CD and achieving rapid convergence.

### Primal and dual formulations

To proceed, we first demonstrate equivalent formulations for ReLU and ReHU, respectively, where the equivalence means that the value on the left is equal to the minimum value of the right-hand side:

\[\mathrm{ReLU}(z)\iff\begin{array}{cc}\min\limits_{\pi}&\pi\\ \text{s.t.}&\pi\geq z,\\ \pi&\geq 0.\end{array}\quad\quad\quad\mathrm{ReLU}(z)\iff\begin{array}{cc} \min\limits_{\theta,\sigma}&\frac{1}{2}\theta^{2}+\tau\sigma\\ \text{s.t.}&\theta+\sigma\geq z,\\ \sigma&\geq 0.\end{array} \tag{5}\]

It is worth noting that when \(\tau=\infty\), the optimal solution for \(\sigma\) is 0, and hence the last constraint (\(\sigma\geq 0\)) can be removed. Then according to the definition of the composite ReLU-ReHU loss function and the equivalent forms in (5), the ReHLine optimization problem (4) can be rewritten as:

\[\min\limits_{\boldsymbol{\beta},\mathbf{\Pi},\boldsymbol{\Theta},\boldsymbol{\Sigma}} \sum_{i=1}^{n}\sum_{l=1}^{L}\pi_{li}+\sum_{i=1}^{n}\sum_{h=1}^{H} \frac{1}{2}\theta_{hi}^{2}+\sum_{i=1}^{n}\sum_{h=1}^{H}\tau_{hi}\sigma_{hi}+ \frac{1}{2}\|\boldsymbol{\beta}\|_{2}^{2}\] \[\text{s.t.}\quad\mathbf{A}\boldsymbol{\beta}+\mathbf{b}\geq \mathbf{0},\quad\pi_{li}\geq u_{i}\mathbf{x}_{i}^{\intercal}\mathbf{\beta}+v_ {li},\quad\theta_{hi}+\sigma_{hi}\geq s_{hi}\mathbf{x}_{i}^{\intercal} \boldsymbol{\beta}+t_{hi},\] \[\pi_{li}\geq 0,\quad\sigma_{hi}\geq 0,\quad\text{for all }(i,l,h), \tag{6}\]

where \(\mathbf{\Pi}=(\pi_{li})\in\mathbb{R}^{L\times n}\), \(\boldsymbol{\Theta}=(\theta_{hi})\in\mathbb{R}^{H\times n}\) and \(\boldsymbol{\Sigma}=(\sigma_{hi})\in\mathbb{R}^{H\times n}\) are slack variables. Theorem 2 shows the dual form of the primal formulation (6) by using the Lagrangian duality theory, and the relation between primal and dual variables is also provided.

**Theorem 2**.: _The Lagrangian dual problem of (6) is:_

\[(\widehat{\boldsymbol{\xi}},\widehat{\boldsymbol{\Lambda}}, \widehat{\boldsymbol{\Gamma}}) =\underset{\boldsymbol{\xi},\boldsymbol{\Lambda},\boldsymbol{ \Gamma}}{\operatorname{argmin}}\ \ \mathcal{L}(\boldsymbol{\xi},\boldsymbol{ \Lambda},\boldsymbol{\Gamma})\] \[\quad\quad\quad\quad\text{s.t.}\ \ \boldsymbol{\xi}\geq\mathbf{0}, \quad\mathbf{E}\geq\boldsymbol{\Lambda}\geq\mathbf{0},\quad\boldsymbol{\tau} \geq\boldsymbol{\Gamma}\geq\mathbf{0}, \tag{7}\] \[\mathcal{L}(\boldsymbol{\xi},\boldsymbol{\Lambda},\boldsymbol{ \Gamma}) =\frac{1}{2}\boldsymbol{\xi}^{\intercal}\mathbf{A}\mathbf{A}^{ \intercal}\boldsymbol{\xi}+\frac{1}{2}\textit{vec}(\boldsymbol{\Lambda})^{ \intercal}\overline{\mathbf{U}}_{(3)}^{\intercal}\overline{\mathbf{U}}_{(3)} \textit{vec}(\boldsymbol{\Lambda})+\frac{1}{2}\textit{vec}(\boldsymbol{\Gamma})^ {\intercal}\big{(}\boldsymbol{\bar{\mathbf{S}}}_{(3)}^{\intercal}\boldsymbol{ \bar{\mathbf{S}}}_{(3)}+\mathbf{I}\big{)}\textit{vec}(\boldsymbol{\Gamma})\] \[\quad\quad\quad-\boldsymbol{\xi}^{\intercal}\mathbf{A}\mathbf{ \mathbf{O}}_{(3)}\textit{vec}(\boldsymbol{\Lambda})-\boldsymbol{\xi}^{ \intercal}\mathbf{A}\boldsymbol{\bar{\mathbf{S}}}_{(3)}\textit{vec}( \boldsymbol{\Gamma})+\textit{vec}(\boldsymbol{\Lambda})^{\intercal}\mathbf{ \mathbf{O}}_{(3)}^{\intercal}\boldsymbol{\bar{\mathbf{S}}}_{(3)}\textit{vec}( \boldsymbol{\Gamma})\] \[\quad\quad\quad+\boldsymbol{\xi}^{\intercal}\mathbf{b}-\text{Tr}( \boldsymbol{\Lambda}\boldsymbol{\Gamma}^{\intercal})-\text{Tr}(\boldsymbol{ \Gamma}\boldsymbol{\Gamma}^{\intercal}), \tag{8}\]

_where \(\boldsymbol{\xi}\in\mathbb{R}^{K}\), \(\boldsymbol{\Lambda}=(\lambda_{li})\in\mathbb{R}^{L\times n}\), and \(\boldsymbol{\Gamma}=(\gamma_{hi})\in\mathbb{R}^{H\times n}\) are dual variables, \(\boldsymbol{\bar{\mathbf{U}}}_{(3)}\in\mathbb{R}^{d\times nL}\) and \(\boldsymbol{\bar{\mathbf{S}}}_{(3)}\in\mathbb{R}^{d\times nH}\) are the mode-3 unfolding of the tensors \(\boldsymbol{\bar{\mathbf{0}}}=(u_{1ij})\in\mathbb{R}^{L\times n\times d}\) and \(\boldsymbol{\bar{\mathbf{S}}}=(s_{hij})\in\mathbb{R}^{H\times n\times d}\), respectively, \(u_{1ij}=u_{li}x_{ij}\), \(s_{hij}=s_{hi}x_{ij}\), \(\mathbf{I}\) is the identity matrix, and all inequalities are elementwise._

_Moreover, the optimal point \(\widehat{\boldsymbol{\beta}}\) of (6) can be recovered as:_

\[\widehat{\boldsymbol{\beta}}=\sum_{k=1}^{K}\widehat{\xi}_{k}\mathbf{a}_{k}-\sum_ {i=1}^{n}\mathbf{x}_{i}\left(\sum_{l=1}^{L}\widehat{\lambda}_{li}u_{li}+\sum_{h= 1}^{H}\widehat{\gamma}_{hi}s_{hi}\right)=\mathbf{A}^{\intercal}\widehat{ \boldsymbol{\xi}}-\boldsymbol{\bar{\mathbf{0}}}_{(3)}\textit{vec}(\widehat{ \boldsymbol{\Lambda}})-\boldsymbol{\bar{\mathbf{S}}}_{(3)}\textit{vec}(\widehat{ \boldsymbol{\Gamma}}). \tag{9}\]

### ReHLine update rules

Indeed, (9) is a KKT condition of (6), and the main idea of the proposed ReHLine solver is to update the dual variables \((\mathbf{\xi},\mathbf{\Lambda},\mathbf{\Gamma})\) by CD on (7) with an _analytic solution_, and _simultaneously_ update the primal variable \(\mathbf{\beta}\) by the KKT condition (9). Most importantly, the computational complexity of the CD updates on (7) can be significantly reduced by using the information of \(\mathbf{\beta}\).

Canonical CD updates.As a first step, we consider the canonical CD update rule that directly optimizes the dual problem (7) with respect to a single variable. For brevity, in this section we only illustrate the result for \(\lambda_{li}\) variables, and the full details are given in Appendix B.

By excluding the terms unrelated to \(\lambda_{li}\), we have \(\lambda_{li}^{\text{new}}=\operatorname*{argmin}_{0\leq\lambda\leq 1} \mathcal{L}_{li}(\lambda)\), where

\[\mathcal{L}_{li}(\lambda) =\frac{1}{2}u_{li}^{2}(\mathbf{x}_{i}^{\mathsf{T}}\mathbf{x}_{i} )\lambda^{2}+\sum_{(l^{\prime},i^{\prime})\neq(l,i)}\lambda_{l^{\prime}i^{ \prime}}u_{l^{\prime}i^{\prime}}u_{li}(\mathbf{x}_{i^{\prime}}^{\mathsf{T}} \mathbf{x}_{i})\lambda-\sum_{k=1}^{K}\xi_{k}u_{li}(\mathbf{a}_{k}^{\mathsf{ T}}\mathbf{x}_{i})\lambda\] \[\quad+\sum_{h^{\prime},i^{\prime}}u_{li}\gamma_{h^{\prime}i^{ \prime}}s_{h^{\prime}i^{\prime}}\mathbf{x}_{i^{\prime}}^{\mathsf{T}}\mathbf{x} _{i^{\prime}}\lambda-v_{li}\lambda.\]

Therefore, by simple calculations we obtain

\[\lambda_{li}^{\text{new}}=\mathcal{P}_{[0,1]}\left(\frac{u_{li}\mathbf{x}_{i}^ {\mathsf{T}}\Big{(}\sum_{k=1}^{K}\xi_{k}\mathbf{a}_{k}-\sum_{(l^{\prime},i^{ \prime})\neq(l,i)}\lambda_{l^{\prime}i^{\prime}}u_{l^{\prime}i^{\prime}} \mathbf{x}_{i^{\prime}}-\sum_{h^{\prime},i^{\prime}}\gamma_{h^{\prime}i^{ \prime}}s_{h^{\prime}i^{\prime}}\mathbf{x}_{i^{\prime}}\Big{)}+v_{li}}{u_{li}^ {2}\|\mathbf{x}_{i}\|_{2}^{2}}\right), \tag{10}\]

where \(\mathcal{P}_{[a,b]}(x)=\max(a,\min(b,x))\) means projecting a real number \(x\) to the interval \([a,b]\).

Clearly, assuming the values \(\mathbf{x}_{i}^{\mathsf{T}}\mathbf{a}_{k}\) and \(\|\mathbf{x}_{i}\|_{2}^{2}\) are cached, updating one \(\lambda_{li}\) value requires \(\mathcal{O}(K+nd+nL+nH)\) of computation, and updating the whole \(\mathbf{\Lambda}\) matrix requires \(\mathcal{O}\big{(}nL(K+nd+nL+nH)\big{)}\). Adding all variables together, the canonical CD update rule for one full cycle has a computational complexity of \(\mathcal{O}\big{(}(K+nd+nL+nH)(K+nL+nH)\big{)}\).

ReHLine updates.The proposed ReHLine algorithm, on the other hand, significantly reduces the computational complexity of canonical CD by updating \(\mathbf{\beta}\) according to the KKT condition (9) after each update of a dual variable. To see this, let \(\mathbf{\mu}:=(\mathbf{\xi},\mathbf{\Lambda},\mathbf{\Gamma})\) denote all the dual variables, and define

\[\mathbf{\beta}(\mathbf{\mu})=\sum_{k=1}^{K}\xi_{k}\mathbf{a}_{k}-\sum_{i=1}^{n} \mathbf{x}_{i}\left(\sum_{l=1}^{L}\lambda_{li}u_{li}+\sum_{h=1}^{H}\gamma_{ hi}s_{hi}\right).\]

Then it can be proved that \((\nabla_{\lambda}\mathcal{L}_{li})(\lambda_{li})=-\left(u_{li}\mathbf{x}_{i}^{ \mathsf{T}}\mathbf{\beta}(\mathbf{\mu})+v_{li}\right)\). Therefore, when \(\mathbf{\mu}\) is fixed at \(\mathbf{\mu}^{\text{old}}=(\mathbf{\xi}^{\text{old}},\mathbf{\Lambda}^{\text{old}},\mathbf{ \Gamma}^{\text{old}})\) and let \(\mathbf{\beta}^{\text{old}}=\mathbf{\beta}(\mathbf{\mu}^{\text{old}})\), (10) can be rewritten as

\[\lambda_{li}^{\text{new}}=\mathcal{P}_{[0,1]}\left(\lambda_{li}^{\text{old}}- \frac{(\nabla_{\lambda_{li}}\mathcal{L})(\lambda^{\text{old}})}{u_{li}^{2}\| \mathbf{x}_{i}\|_{2}^{2}}\right)=\mathcal{P}_{[0,1]}\left(\lambda_{li}^{\text{ old}}+\frac{u_{li}\mathbf{x}_{i}^{\mathsf{T}}\mathbf{\beta}^{\text{old}}+v_{li}}{u_{li}^ {2}\|\mathbf{x}_{i}\|_{2}^{2}}\right).\]

Accordingly, the primal variable \(\mathbf{\beta}\) is updated as

\[\mathbf{\beta}^{\text{new}}=\mathbf{\beta}^{\text{old}}-(\lambda_{li}^{\text{new}}- \lambda_{li}^{\text{old}})u_{li}\mathbf{x}_{i},\]

which can then be used for the next dual variable update. Simple calculations show that this scheme only costs \(\mathcal{O}(d)\) of computation for one \(\lambda_{li}\) variable.

The update rules for other dual variables are similar, with the complete algorithm shown in Algorithm 1 and the full derivation details given in Appendix B. Overall, the ReHLine update rule has a computational complexity of \(\mathcal{O}\big{(}(K+nL+nH)d\big{)}\) for one full cycle. Given that \((K,L,H)\) are all small numbers, the complexity is _linear_ with respect to both the sample size \(n\) and the dimension \(d\).

We emphasize that the linear relationship (9) between the primal and dual variables, as well as its consequence of reducing the computational cost, is highly non-trivial. This technique has been proposed for specialized problems such as SVMs via the liblinear solver [21], and here we show that its success can be greatly generalized to any convex PLQ loss functions.

### Global convergence rate

Finally, we show that the ReHLine algorithm has a fast global linear convergence rate.

**Theorem 3**.: _Let \(\mathbf{\mu}^{(q)}\coloneqq(\mathbf{\xi}^{(q)},\mathbf{\Lambda}^{(q)},\mathbf{\Gamma}^{(q)})\) be a sequence of iterates generated by Algorithm 1, and \(\mathbf{\mu}^{*}\) be a minimizer of the dual objective (8). Then \(\mathbf{\mu}^{(q)}\) is feasible, and the dual objective value converges at least linearly to that of \(\mathbf{\mu}^{*}\), that is, there exist \(0<\eta<1\) and \(q_{0}\) such that for all \(q\geq q_{0}\),_

\[\mathcal{L}(\mathbf{\mu}^{(q+1)})-\mathcal{L}(\mathbf{\mu}^{*})\leq\eta\big{(}\mathcal{ L}(\mathbf{\mu}^{(q)})-\mathcal{L}(\mathbf{\mu}^{*})\big{)}.\]

## 4 Related work

Table 1 summarizes the existing methods in solving the primal or dual problem of (1). Note that Column #iterations is the best-fit theoretical results of convergence analysis for each algorithm based on our literature search, which may be improved with further developments of convergence analysis. These algorithms can be broadly divided into the following categories.

Generic QP solvers.For instance, a projected sub-gradient descent is readily applicable to the primal form (1) by computing the sub-gradient of each loss and the projection under affine constraints, although the projection operator itself is already difficult. As for the box-QP dual problem (7), CD converges linearly to the optimal solution [27; 31; 35; 42]. Box-QP can also be efficiently solved using IPM, which leverages second-order cone programming techniques [13; 14]. Moreover, ADMM was employed to iteratively minimize the augmented Lagrangian of (1).

ERM solvers.There are multiple existing works developed to tackle ERM problems, such as SAG [37], SVRG [25], SAGA [8], proximal SDCA [40] and SDCA [39]. However, SAG, SVRG, SAGA and proximal SDCA can only handle smooth loss functions, with an optional non-smooth term that has an easy proximal operator. SDCA applies to general loss functions, but it requires the convex conjugate of loss functions, which is not necessarily simple to compute, also see discussion in Section 3.1 in [40]. Moreover, it only guarantees a sublinear convergence rate for non-smooth loss functions. In contrast, ReHLine supports all convex PLQ loss functions with optional general linear constraints, which could be non-smooth by construction, and they all enjoy a linear computational complexity and provable linear convergence.

For non-smooth loss functions, another existing method to solve ERM is the smoothing technique [40; 3; 30], which approximates the non-smooth terms by smooth functions, and then uses smoothness algorithms to solve the smooth problem. However, the choice of the smoothing function and smoothing parameter typically requires additional knowledge, and the linear convergence rate is not always guaranteed.

Despite the successful use of the existing algorithms in solving (1), these methods either suffer from slow convergence (P-GD), or high computational costs (CD, IPM, and ADMM). This is partly because these general methods do not fully exploit the inherent structure of the problem. In contrast, ReHLine leverages the KKT condition in (9) to substantially improve numerical performance, thus achieving a linear computational complexity per iteration with respect to the sample size, as well as a linear convergence rate.

In addition to generic algorithms, specialized algorithms have been developed for specific types of (1). For example, the dual coordinate descent of liblinear in solving SVMs [21], the semismooth Newton coordinate descent algorithm of hqreg in solving the ridge regularized Huber minimization [46], and SAGA, SAG, proximal SDCA, and SVRG of lightning in solving smooth ERMs [4], such as smoothed SVMs. Our algorithm is also compared to these specialized methods in Section 5.

**Software.** Below are some commonly-used software for solving (1) based on the algorithms available.

* cvx/cvxpy[17; 10]: a modeling language for convex optimization problems and supports a wide range of solvers such as ecos, scs, osqp, and mosek.
* mosek[2]: a commercial convex optimization solver based on IPM.
* ecos[11]: an open-source solver of second-order cone programming based on IPM.
* scs[32]: an open-source convex quadratic cone solver based on ADMM.
* dccp[41]: a disciplined convex-concave programming solver built on top of cvxpy, which has been used to solve classification problems with fairness constraints [47].
* liblinear[21]: a library for large linear classification.
* horeg[46]: an R package solving Lasso or elastic net penalized Huber loss regression and quantile regression.
* lightning[4]: a Python library solving various ERMs based on primal SDCA, SGD, AdaGrad, SAG, SAGA, SVRG.

## 5 Experiments and benchmarks

In this section we demonstrate the performance of ReHLine compared with the existing state-of-the-art (SOTA) solvers on various machine learning tasks. The source code of ReHLine, along with Python/R interface, is readily accessible on our GitHub repository ([https://github.com/softmin/ReHLine](https://github.com/softmin/ReHLine)) and our project page ([https://rehline.github.io/](https://rehline.github.io/)). Our experiments involve five prominent machine learning tasks that draw from diverse sets of data. Specifically, we focus on four classification datasets and five regression datasets sourced from OpenML ([https://www.openml.org/](https://www.openml.org/)), with various scales and dimensions. The experiment settings are summarized in Table 4. To achieve a fair comparison, we use a well-organized toolset and framework, the Benchopt library [29], to implement optimization benchmarks for all the SOTA solvers.

Due to representation limits, we only present the results using a variant of the ReHLine algorithm that supports variable shrinkage (refer to Algorithm 2). Results utilizing ReHLine with or without shrinkage are reported on our benchmark repository ([https://github.com/softmin/ReHLine-benchmark](https://github.com/softmin/ReHLine-benchmark)). Note that there is no substantial difference between these variations, and they do not influence the conclusions drawn from our experiments. Simultaneously, in the benchmark repository, we provide a more detailed "time versus objective" plot for each task/dataset and each solver produced by Benchopt.

For the **FairSVM** task, ReHLine is compared with the original solver [48] based on dccp and other generic solvers in cvxpy. In this case, both feasibility and optimality are examined, where the

\begin{table}
\begin{tabular}{c l} \hline \hline Classification dataset (\(n\times d\)) & Regression dataset (\(n\times d\)) \\ \hline Steel-plates-fault (SPF): \(1941\times 34\) & Liver-disorders (LD): \(345\times 6\) \\ Philippine: \(5832\times 309\) & Kin8nm: \(8191\times 9\) \\ Sylva-prior: \(14395\times 109\) & Topo-2-1: \(8885\times 267\) \\ Creditcard: \(284807\times 31\) & House-8L: \(22784\times 9\) \\  & Buzzin-Twitter (BT): \(583250\times 78\) \\ \hline \hline \end{tabular}
\end{table}
Table 4: The classification and regression datasets with different scales used in our experiments.

optimality is measured by the objective function, and the feasibility is measured by the violation of constraints: \(\max\left(n^{-1}\middle|\sum_{i=1}^{n}z_{i}\beta^{\intercal}\mathbf{x}_{i}\right| -\rho,10^{-6}\right)\). Moreover, we examine the performance of **ElasticQR** by considering the model defined in (A.2) with \(\lambda_{1}=\lambda_{2}=1\), **RidgeHuber** of (A.4) with \(\lambda_{1}=0,\lambda_{2}=1\), and **SVM** of (A.1) and **sSVM** of (A.5) with \(C=1\). Table 5 presents the running times in seconds of all solvers that converge based on a relative/absolute tolerance of \(10^{-5}\).

The results in Table 5 indicate that the proposed ReHLine algorithm/software achieves visible improvements over existing solvers. The major empirical findings are as follows. First, the amount of improvement achieved by ReHLine in terms of running time is substantial over generic solvers, including ecos, scs, and mosek based on cvx, with the largest improvement 100x-1000x speed-up on the largest-scale dataset. The improvement continues to expand even further as the scale of the problem grows. Second, ReHLine also yields considerable improvements for specialized algorithms, including liblinear in SVM, hqreg in Huber minimization, and lightning in sSVM. Third, given that the generic solvers fail or time out in multiple tasks and datasets, ReHLine has shown remarkable flexibility when it comes to solving various domain-specific problems.

\begin{table}
\begin{tabular}{l l c c c c c} \hline \hline Task & Dataset & ecos & mosek & scs & dccp & ReHLine \\ \hline FairSVM & SPF (\(\times\)1e-4) & ✗ & ✗ & ✗ & ✗ & 4.25 (\(\pm 0.5\)) \\  & Philippine (\(\times\)1e-2) & 1550 (\(\pm 0.6\)) & 87.4 (\(\pm 0.2\)) & 130 (\(\pm 42\)) & 1137 (\(\pm 9.2\)) & 1.03 (\(\pm 0.2\)) \\  & Sylva-prior (\(\times\)1e-2) & ✗ & ✗ & ✗ & 0.47 (\(\pm 0.1\)) \\  & Creditcard (\(\times\)1e-1) & 175 (\(\pm 0.2\)) & 64.2 (\(\pm 0.1\)) & 161 (\(\pm 405\)) & ✗ & 0.64 (\(\pm 0.2\)) \\ \hline \multicolumn{2}{c}{} & Fail/Succeed & \multicolumn{1}{c}{2/2} & 2/2 & 2/2 & 3/1 & 0/4 \\  & Speed-up (on Creditcard) & 273x & 100x & 252x & \(\infty\) & \(-\) \\ \hline \hline Task & Dataset & ecos & mosek & scs & ReHline \\ \hline ElasticQR & LD (\(\times\)1e-4) & ✗ & 106 (\(\pm 7\)) & 34.9 (\(\pm 25.0\)) & 2.60 (\(\pm 0.30\)) \\  & Kin8m (\(\times\)1e-3) & ✗ & 92.0 (\(\pm 1.0\)) & 63.1 (\(\pm 58.5\)) & 4.12 (\(\pm 0.95\)) \\  & House-8L (\(\times\)1e-3) & 887 (\(\pm 161\)) & 277 (\(\pm 34\)) & ✗ & 7.21 (\(\pm 1.99\)) \\  & Topo-2-1 (\(\times\)1e-2) & 4752 (\(\pm 2015\)) & ✗ & ✗ & 3.04 (\(\pm 0.49\)) \\  & BT (\(\times\)1e-0) & 7079 (\(\pm 2517\)) & ✗ & ✗ & 2.49 (\(\pm 0.56\)) \\ \hline \multicolumn{2}{c}{} & Fail/Succeed & \multicolumn{1}{c}{3/2} & 2/3 & 3/2 & 0/5 \\  & Speed-up (on BT) & 2843x & \(\infty\) & \(\infty\) & \(-\) \\ \hline \hline Task & Dataset & ecos & mosek & scs & hqreg & ReHLine \\ \hline RidgeHuber & LD (\(\times\)1e-4) & ✗ & ✗ & ✗ & 4.90 (\(\pm 0.00\)) & 1.40 (\(\pm 0.20\)) \\  & Kin8nm (\(\times\)1e-3) & ✗ & ✗ & ✗ & 1.58 (\(\pm 0.21\)) & 2.04 (\(\pm 0.30\)) \\  & House-8L (\(\times\)1e-3) & ✗ & 925 (\(\pm 2\)) & ✗ & 2.42 (\(\pm 0.34\)) & 0.80 (\(\pm 0.21\)) \\  & Topo-2-1 (\(\times\)1e-2) & 2620 (\(\pm 1040\)) & 267 (\(\pm 1\)) & 213 (\(\pm 2\)) & 3.53 (\(\pm 0.67\)) & 1.78 (\(\pm 0.32\))Conclusion

In this paper, we present a new algorithm ReHLine designed to efficiently solve general regularized ERM with the convex PLQ loss function and optional linear constraints. Through the transformation of a PLQ loss into a composite ReLU-ReHU function, we have developed a novel CD algorithm that updates primal and dual variables simultaneously. This approach has proven to be highly effective in reducing the computational complexity while at the same time achieving a fast linear convergence rate. Based on our experiments, ReHLine demonstrates remarkable flexibility and computational efficiency, outperforming existing generic and specialized methods in a range of problems.

Limitation.Although our approach has exhibited substantial versatility in addressing a diverse range of problems, it is important to acknowledge that it may not be universally applicable to all convex optimization problems. The main limit of our approach stems from the restriction imposed by the formulation (1). For instance, (1) requires a convex PLQ loss function and a strict \(l_{2}\)-regularization. We recognize this limitation and intend to address it in future research pursuits. Moreover, we are interested in exploiting additional loss function properties, such as symmetry.

## 7 Acknowledgements

Ben Dai's work was supported in part by HK GRF-24302422 and GRF-14304823. Yixuan Qiu's work was supported in part by National Natural Science Foundation of China (12101389), Shanghai Pujiang Program (21PJC056), MOE Project of Key Research Institute of Humanities and Social Sciences (22JJD110001), and Shanghai Research Center for Data Science and Decision Technology. The authors confirm contribution to the paper as follows. The main algorithm: Ben Dai and Yixuan Qiu; theory and proofs: Ben Dai; C++ implementation: Yixuan Qiu; Python interface: Ben Dai and Yixuan Qiu; R interface: Yixuan Qiu; manuscript preparation: Ben Dai and Yixuan Qiu, equally.

## References

* Alekseeva et al. [2018] Alekseeva, E., Brotcorne, L., Lepaul, S., and Montmeat, A. (2018). A bilevel approach to optimize electricity prices. _Yugoslav Journal of Operations Research_, 29(1):9-30.
* ApS [2019] ApS, M. (2019). _The MOSEK Optimization Toolbox for MATLAB Manual. Version 9.0_.
* Beck and Teboulle [2012] Beck, A. and Teboulle, M. (2012). Smoothing and first order methods: A unified framework. _SIAM Journal on Optimization_, 22(2):557-580.
* Blondel and Pedregosa [2016] Blondel, M. and Pedregosa, F. (2016). Lightning: large-scale linear classification, regression and ranking in Python.
* Boyd et al. [2011] Boyd, S., Parikh, N., Chu, E., Peleato, B., Eckstein, J., et al. (2011). Distributed optimization and statistical learning via the alternating direction method of multipliers. _Foundations and Trends(r) in Machine Learning_, 3(1):1-122.
* Bubeck et al. [2015] Bubeck, S. et al. (2015). Convex optimization: algorithms and complexity. _Foundations and Trends(r) in Machine Learning_, 8(3-4):231-357.
* Cortes and Vapnik [1995] Cortes, C. and Vapnik, V. (1995). Support-vector networks. _Machine Learning_, 20(3):273-297.
* Defazio et al. [2014] Defazio, A., Bach, F., and Lacoste-Julien, S. (2014). Saga: A fast incremental gradient method with support for non-strongly convex composite objectives. _Advances in Neural Information Processing Systems_, 27.
* Deng et al. [2017] Deng, W., Lai, M.-J., Peng, Z., and Yin, W. (2017). Parallel multi-block ADMM with o(1/k) convergence. _Journal of Scientific Computing_, 71:712-736.
* Diamond and Boyd [2016] Diamond, S. and Boyd, S. (2016). CVXPY: A Python-embedded modeling language for convex optimization. _Journal of Machine Learning Research_, 17(1):2909-2913.
* Domahidi et al. [2013] Domahidi, A., Chu, E., and Boyd, S. (2013). Ecos: An socp solver for embedded systems. In _2013 European Control Conference (ECC)_, pages 3071-3076. IEEE.

* Fan et al. [2008] Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J. (2008). LIBLINEAR: A library for large linear classification. _Journal of Machine Learning Research_, 9:1871-1874.
* Ferris and Munson [2002] Ferris, M. C. and Munson, T. S. (2002). Interior-point methods for massive support vector machines. _SIAM Journal on Optimization_, 13(3):783-804.
* Fine and Scheinberg [2001] Fine, S. and Scheinberg, K. (2001). Efficient SVM training using low-rank kernel representations. _Journal of Machine Learning Research_, 2(Dec):243-264.
* Fukushima [1969] Fukushima, K. (1969). Visual feature extraction by a multilayered network of analog threshold elements. _IEEE Transactions on Systems Science and Cybernetics_, 5(4):322-333.
* Gerard et al. [2016] Gerard, M., Clautiaux, F., and Sadykov, R. (2016). Column generation based approaches for a tour scheduling problem with a multi-skill heterogeneous workforce. _European Journal of Operational Research_, 252(3):1019-1030.
* Grant and Boyd [2014] Grant, M. and Boyd, S. (2014). CVX: Matlab software for disciplined convex programming, version 2.1.
* Griva et al. [2004] Griva, I., Shanno, D. F., and Vanderbei, R. J. (2004). Convergence analysis of a primal-dual interior-point method for nonlinear programming. _Optimization Online_.
* Gu et al. [2018] Gu, Y., Fan, J., Kong, L., Ma, S., and Zou, H. (2018). ADMM for high-dimensional sparse penalized quantile regression. _Technometrics_, 60(3):319-331.
* He and Yuan [2012] He, B. and Yuan, X. (2012). On the o(1/n) convergence rate of the Douglas-Rachford alternating direction method. _SIAM Journal on Numerical Analysis_, 50(2):700-709.
* Hsieh et al. [2008] Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S. S., and Sundararajan, S. (2008). A dual coordinate descent method for large-scale linear SVM. In _International Conference on Machine Learning_, pages 408-415.
* Huber [1964] Huber, P. J. (1964). Robust estimation of a location parameter. _The Annals of Mathematical Statistics_, 35(1):73-101.
* Huber [1965] Huber, P. J. (1965). A robust version of the probability ratio test. _The Annals of Mathematical Statistics_, pages 1753-1758.
* Jagannathan and Ma [2003] Jagannathan, R. and Ma, T. (2003). Risk reduction in large portfolios: Why imposing the wrong constraints helps. _Journal of Finance_, 58(4):1651-1683.
* Johnson and Zhang [2013] Johnson, R. and Zhang, T. (2013). Accelerating stochastic gradient descent using predictive variance reduction. _Advances in Neural Information Processing Systems_, 26.
* Koenker and Bassett Jr [1978] Koenker, R. and Bassett Jr, G. (1978). Regression quantiles. _Econometrica_, 46(1):33-50.
* Luo and Tseng [1992] Luo, Z.-Q. and Tseng, P. (1992). On the convergence of the coordinate descent method for convex differentiable minimization. _Journal of Optimization Theory and Applications_, 72(1):7-35.
* Mellado et al. [2018] Michel Valladao, D., Veiga, A., and Street, A. (2018). A linear stochastic programming model for optimal leveraged portfolio selection. _Computational Economics_, 51(4):1021-1032.
* Moreau et al. [2022] Moreau, T., Massias, M., Gramfort, A., Ablin, P., Bannier, P.-A., Charlier, B., Dagreou, M., Dupre la Tour, T., Durif, G., Dantas, C. F., et al. (2022). Benchopt: Reproducible, efficient and collaborative optimization benchmarks. In _Advances in Neural Information Processing Systems_, pages 25404-25421.
* Nesterov [2005] Nesterov, Y. (2005). Smooth minimization of non-smooth functions. _Mathematical Programming_, 103:127-152.
* Nesterov [2012] Nesterov, Y. (2012). Efficiency of coordinate descent methods on huge-scale optimization problems. _SIAM Journal on Optimization_, 22(2):341-362.
* O'donoghue et al. [2016] O'donoghue, B., Chu, E., Parikh, N., and Boyd, S. (2016). Conic optimization via operator splitting and homogeneous self-dual embedding. _Journal of Optimization Theory and Applications_, 169(3):1042-1068.

* [33] Rennie, J. D. and Srebro, N. (2005). Loss functions for preference levels: Regression with discrete ordered labels. In _Proceedings of the IJCAI Multidisciplinary Workshop on Advances in Preference Handling_, volume 1. Citeseer.
* [34] Rockafellar, R. T. and Wets, R. J.-B. (2009). _Variational analysis_. Springer Science & Business Media.
* [35] Saha, A. and Tewari, A. (2013). On the nonasymptotic convergence of cyclic coordinate descent methods. _SIAM Journal on Optimization_, 23(1):576-601.
* [36] Salas, V. and Saurina, J. (2002). Credit risk in two institutional regimes: Spanish commercial and savings banks. _Journal of Financial Services Research_, 22(3):203-224.
* [37] Schmidt, M., Le Roux, N., and Bach, F. (2017). Minimizing finite sums with the stochastic average gradient. _Mathematical Programming_, 162:83-112.
* [38] Schumaker, L. (2007). _Spline Functions: Basic Theory_. Cambridge university press.
* [39] Shalev-Shwartz, S. and Zhang, T. (2013). Stochastic dual coordinate ascent methods for regularized loss minimization. _Journal of Machine Learning Research_, 14(1).
* [40] Shalev-Shwartz, S. and Zhang, T. (2014). Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization. In _International Conference on Machine Learning_, pages 64-72. PMLR.
* [41] Shen, X., Diamond, S., Gu, Y., and Boyd, S. (2016). Disciplined convex-concave programming. In _2016 IEEE 55th Conference on Decision and Control (CDC)_, pages 1009-1014. IEEE.
* [42] Tseng, P. and Yun, S. (2010). A coordinate gradient descent method for linearly constrained smooth optimization and support vector machines training. _Computational Optimization and Applications_, 47(2):179-206.
* [43] Vapnik, V. (1991). Principles of risk minimization for learning theory. In _Advances in Neural Information Processing Systems_, pages 831-838.
* [44] Vapnik, V. (1999). _The Nature of Statistical Learning Theory_. Springer science & business media.
* [45] Wright, S. J. (2015). Coordinate descent algorithms. _Mathematical Programming_, 151(1):3-34.
* [46] Yi, C. and Huang, J. (2017). Semismooth newton coordinate descent algorithm for elastic-net penalized Huber loss regression and quantile regression. _Journal of Computational and Graphical Statistics_, 26(3):547-557.
* [47] Zafar, M. B., Valera, I., Gomez-Rodriguez, M., and Gummadi, K. P. (2019). Fairness constraints: A flexible approach for fair classification. _Journal of Machine Learning Research_, 20(1):2737-2778.
* [48] Zafar, M. B., Valera, I., Rogriguez, M. G., and Gummadi, K. P. (2017). Fairness constraints: Mechanisms for fair classification. In _Artificial Intelligence and Statistics_, pages 962-970.
* [49] Zou, H. and Hastie, T. (2005). Regularization and variable selection via the elastic net. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 67(2):301-320.

## Appendix A Example Models

SVMs with fairness constraints (FairSVM).The formulation for FairSVM [48] is:

\[\min_{\mathbf{\beta}\in\mathbb{R}^{d}} \frac{C}{n}\sum_{i=1}^{n}(1-y_{i}\mathbf{\beta}^{\intercal}\mathbf{x}_{ i})_{+}+\frac{1}{2}\|\mathbf{\beta}\|_{2}^{2},\] s.t. \[\frac{1}{n}\sum_{i=1}^{n}\mathbf{z}_{i}\mathbf{\beta}^{\intercal} \mathbf{x}_{i}\leq\mathbf{\rho},\quad\frac{1}{n}\sum_{i=1}^{n}\mathbf{z}_{i}\mathbf{ \beta}^{\intercal}\mathbf{x}_{i}\geq-\mathbf{\rho},\] (A.1)

where \(\mathbf{x}_{i}\in\mathbb{R}^{d}\) is a feature vector, and \(y_{i}\in\{-1,1\}\) is a binary label, \(\mathbf{z}_{i}\in\mathbb{R}^{d_{0}}\) is a collection of centered sensitive features \((\sum_{i=1}^{n}\mathbf{z}_{i}=\mathbf{0})\), such as gender and/or race. The constraints in (A.1) constrain the correlation between the sensitive features \(\mathbf{z}_{i}\) and the decision function \(\mathbf{\beta}^{\intercal}\mathbf{x}_{i}\), and the constants \(\mathbf{\rho}\in\mathbb{R}^{d_{0}}_{+}\) trade off predictive accuracy and fairness. Note that the FairSVM (A.1) can be rewritten as a ReHLine optimization of (4) with

\[\mathbf{U}\leftarrow-C\mathbf{y}^{\intercal}/n,\quad\mathbf{V}\gets C \mathbf{1}_{n}^{\intercal}/n,\quad\mathbf{A}\leftarrow\begin{pmatrix}\mathbf{Z }^{\intercal}\mathbf{X}/n\\ -\mathbf{Z}^{\intercal}\mathbf{X}/n\end{pmatrix},\quad\mathbf{b}\leftarrow \begin{pmatrix}\mathbf{\rho}\\ \mathbf{\rho}\end{pmatrix},\]

where \(\mathbf{1}_{n}=(1,\cdots,1)^{\intercal}\) is the length-\(n\) one vector, \(\mathbf{X}\in\mathbb{R}^{n\times d}\) is the feature matrix, and \(\mathbf{y}=(y_{1},\cdots,y_{n})^{\intercal}\) is the response vector.

Elastic net regularized quantile regression (ElasticQR).The formulation for the elastic net penalized quantile regression [19] is:

\[\min_{\mathbf{\beta}\in\mathbb{R}^{d+1}}\frac{1}{n}\sum_{i=1}^{n}\rho_{\kappa}(y_{ i}-\mathbf{x}_{i}^{\intercal}\mathbf{\beta}_{1:d}-\beta_{d+1})+\lambda_{1}\|\mathbf{ \beta}\|_{1}+\frac{\lambda_{2}}{2}\|\mathbf{\beta}\|_{2}^{2},\] (A.2)

where

\[\rho_{\kappa}(u)=u(\kappa-\mathbf{1}(u<0))\] (A.3)

is the check loss, \(\kappa\in(0,1)\) is a prespecified sample quantile, \(\mathbf{x}_{i}\in\mathbb{R}^{d}\) is a feature vector, \(y_{i}\in\mathbb{R}\) is a response, and \(\lambda_{1},\lambda_{2}\geq 0\) are weights of lasso and ridge penalties. Then, the ElasticQR (A.2) can be rewritten as a ReHLine optimization of (4) with

\[\mathbf{U}\leftarrow\begin{pmatrix}-\frac{\kappa}{n\lambda_{2}}\mathbf{1}_{n} ^{\intercal}&\mathbf{0}_{d+1}^{\intercal}\\ \frac{1}{n\lambda_{2}}\mathbf{1}_{n}^{\intercal}&\mathbf{0}_{d+1}^{\intercal} \\ \mathbf{0}_{n}^{\intercal}&\frac{\lambda_{4}}{\lambda_{2}}\mathbf{1}_{d+1}^{ \intercal}\end{pmatrix},\quad\mathbf{V}\leftarrow\begin{pmatrix}\frac{ \kappa}{n\lambda_{2}}\mathbf{y}^{\intercal}&\mathbf{0}_{d+1}^{\intercal}\\ -\frac{1}{n\lambda_{2}}\mathbf{y}^{\intercal}&\mathbf{0}_{d+1}^{\intercal}\\ \mathbf{0}_{n}^{\intercal}&\mathbf{0}_{d+1}^{\intercal}\end{pmatrix},\quad \mathbf{X}\leftarrow\begin{pmatrix}\mathbf{X}_{1}\mathbf{1}_{n}\\ \mathbf{I}_{d+1}\end{pmatrix},\]

where \(\mathbf{I}_{d+1}\) is the identity matrix.

Elastic net regularized Huber minimization.The formulation for the elastic net penalized Huber minimization [23] is:

\[\min_{\mathbf{\beta}}\frac{1}{n}\sum_{i=1}^{n}H_{\kappa}(y_{i}-\mathbf{x}_{i}^{ \intercal}\mathbf{\beta})+\lambda_{1}\|\mathbf{\beta}\|_{1}+\frac{\lambda_{2}}{2}\| \mathbf{\beta}\|_{2}^{2},\] (A.4)

where \(H_{\kappa}(\cdot)\) is the Huber loss with a given parameter \(\kappa\):

\[H_{\kappa}(z)=\left\{\begin{array}{ll}z^{2}/2,&0<|z|\leq\kappa,\\ \kappa(|z|-\kappa/2),&|z|>\kappa.\end{array}\right.\]

In this case, (A.4) can be rewritten as a ReHLine optimization with

\[\mathbf{S}\leftarrow\begin{pmatrix}-\sqrt{\frac{1}{n\lambda_{2}}}\mathbf{1}_ {n}^{\intercal}&\mathbf{0}_{d}^{\intercal}\\ \sqrt{\frac{1}{n\lambda_{2}}}\mathbf{1}_{n}^{\intercal}&\mathbf{0}_{d}^{ \intercal}\end{pmatrix},\quad\mathbf{T}\leftarrow\begin{pmatrix}\sqrt{ \frac{1}{n\lambda_{2}}}\mathbf{y}^{\intercal}&\mathbf{0}_{d}^{\intercal}\\ -\sqrt{\frac{1}{n\lambda_{2}}}\mathbf{y}^{\intercal}&\mathbf{0}_{d}^{\intercal} \end{pmatrix},\quad\mathbf{\tau}\leftarrow\begin{pmatrix}\kappa\sqrt{\frac{1}{n \lambda_{2}}}\mathbf{1}_{n}^{\intercal}&\mathbf{0}_{d}^{\intercal}\\ \kappa\sqrt{\frac{1}{n\lambda_{2}}}\mathbf{1}_{n}^{\intercal}&\mathbf{0}_{d}^{ \intercal}\end{pmatrix},\]

\[\mathbf{U}\leftarrow\begin{pmatrix}\mathbf{0}_{n}^{\intercal}&\frac{ \lambda_{1}}{\lambda_{2}}\mathbf{1}_{d}^{\intercal}\\ \mathbf{0}_{n}^{\intercal}&-\frac{\lambda_{1}}{\lambda_{2}}\mathbf{1}_{d}^{ \intercal}\end{pmatrix},\quad\mathbf{V}\leftarrow\mathbf{0},\quad\mathbf{X} \leftarrow\begin{pmatrix}\mathbf{X}\\ \mathbf{I}_{d}\end{pmatrix}.\]Smoothed SVM (sSVM).The formulation for the smoothed SVMs [33] is:

\[\min_{\mathbf{\beta}}\frac{1}{n}\sum_{i=1}^{n}V(y_{i}\mathbf{\beta}^{\intercal}\mathbf{x} _{i})+\frac{1}{2}\|\mathbf{\beta}\|_{2}^{2},\] (A.5)

where \(\mathbf{x}_{i}\in\mathbb{R}^{d}\) is a feature vector, and \(y_{i}\in\{-1,1\}\) is a binary label, and \(V_{\kappa}(\cdot)\) is the modified Huber loss or the smoothed hinge loss:

\[V(z)=\left\{\begin{aligned} 0,&\quad z\geq 1,\\ (1-z)^{2}/2,&\quad 0<z\leq 1,\\ (1/2-z),&\quad z<0.\end{aligned}\right.\]

In this case, (A.5) can be rewritten as a ReHLine optimization with

\[\mathbf{S}\leftarrow-\sqrt{C/n}\mathbf{y}^{\intercal},\quad\mathbf{T} \leftarrow\sqrt{C/n}\mathbf{1}_{n}^{\intercal},\quad\mathbf{\tau}\leftarrow\sqrt {C/n}\mathbf{1}_{n}^{\intercal}.\]

## Appendix B Update Rules for (7)

### Canonical CD updates

Update \(\xi_{k}\) given others.For \(k=1,\cdots,K\),

\[\xi_{k}^{\text{new}} =\operatorname*{argmin}_{\xi_{k}\geq 0}\frac{1}{2}\mathbf{a}_{k}^{ \intercal}\mathbf{a}_{k}\xi_{k}^{2}+\sum_{k^{\prime}\neq k}\xi_{k^{\prime}} \mathbf{a}_{k^{\prime}}^{\intercal}\mathbf{a}_{k}\xi_{k}-\sum_{l,i}\lambda_{ li}u_{li}\xi_{k}\mathbf{a}_{k}^{\intercal}\mathbf{x}_{i}-\sum_{h,i}\gamma_{hi}s_{ hi}\xi_{k}\mathbf{a}_{k}^{\intercal}\mathbf{x}_{i}+b_{k}\xi_{k}\] \[=\max\left(0,-\frac{\mathbf{a}_{k}^{\intercal}\big{(}\sum_{k^{ \prime}\neq k}\xi_{k^{\prime}}\mathbf{a}_{k^{\prime}}-\sum_{l,i}\lambda_{li}u_{ li}\mathbf{x}_{i}-\sum_{h,i}\gamma_{hi}s_{hi}\mathbf{x}_{i}\big{)}+b_{k}}{\| \mathbf{a}_{k}\|_{2}^{2}}\right).\] (B.6)

Update \(\lambda_{li}\) given others.For \(l=1,\cdots,L\) and \(i=1,\cdots,n\),

\[\lambda_{li}^{\text{new}} =\operatorname*{argmin}_{1\geq\lambda_{li}\geq 0}\frac{1}{2}u_{ li}^{2}\lambda_{li}^{2}\mathbf{x}_{i}^{\intercal}\mathbf{x}_{i}+\sum_{(l^{ \prime},i^{\prime})\neq(l,i)}\lambda_{l^{\prime}i^{\prime}}u_{li^{\prime}i} \mathbf{x}_{i^{\prime}}^{\intercal}\mathbf{x}_{i}\lambda_{li}\] \[\qquad-\sum_{k=1}^{K}\xi_{k}u_{li}\mathbf{a}_{k}^{\intercal} \mathbf{x}_{i}\lambda_{li}+\sum_{h^{\prime},i^{\prime}}u_{li}\gamma_{h^{\prime }i^{\prime}}s_{h^{\prime}i^{\prime}}\mathbf{x}_{i}^{\intercal}\mathbf{x}_{i^{ \prime}}\lambda_{li}-v_{li}\lambda_{li}\] \[=\mathcal{P}_{[0,1]}\left(\frac{u_{li}\mathbf{x}_{i}^{\intercal} \Big{(}\sum_{k=1}^{K}\xi_{k}\mathbf{a}_{k}-\sum_{(l^{\prime},i^{\prime})\neq( l,i)}\lambda_{l^{\prime}i^{\prime}}u_{l^{\prime}i^{\prime}}\mathbf{x}_{i^{\prime}} -\sum_{h^{\prime},i^{\prime}}\gamma_{h^{\prime}i^{\prime}}s_{h^{\prime}i^{ \prime}}\mathbf{x}_{i^{\prime}}\Big{)}+v_{li}}{u_{li}^{2}\|\mathbf{x}_{i}\|_{2} ^{2}}\right).\] (B.7)

Update \(\gamma_{hi}\) given others.For \(h=1,\cdots,H\) and \(i=1,\cdots,n\),

\[\gamma_{hi}^{\text{new}} =\operatorname*{argmin}_{\tau_{hi}\geq\gamma_{hi}\geq 0}\frac{1}{2}( s_{hi}^{2}\mathbf{x}_{i}^{\intercal}\mathbf{x}_{i}+1)\gamma_{hi}^{2}+\sum_{(h ^{\prime},i^{\prime})\neq(h,i)}\gamma_{h^{\prime}i^{\prime}}s_{hi}\mathbf{x}_{ i^{\prime}}^{\intercal}\mathbf{x}_{i}\gamma_{hi}\] \[\qquad-\sum_{k=1}^{K}\xi_{k}s_{hi}\mathbf{a}_{k}^{\intercal} \mathbf{x}_{i}\gamma_{hi}+\sum_{l^{\prime},i^{\prime}}s_{hi}\lambda_{l^{ \prime}i^{\prime}}u_{l^{\prime}i^{\prime}}\mathbf{x}_{i}^{\intercal}\mathbf{x} _{i^{\prime}}\gamma_{hi}-t_{hi}\gamma_{hi}\] \[=\mathcal{P}_{[0,\tau_{hi}]}\left(\frac{s_{hi}\mathbf{x}_{i}^{ \intercal}\big{(}\sum_{k=1}^{K}\xi_{k}\mathbf{a}_{k}-\sum_{(h^{\prime},i^{ \prime})\neq(h,i)}\gamma_{h^{\prime}i^{\prime}}s_{h^{\prime}i^{\prime}} \mathbf{x}_{i^{\prime}}-\sum_{l^{\prime},i^{\prime}}\lambda_{l^{\prime}i^{ \prime}}u_{l^{\prime}i^{\prime}}\mathbf{x}_{i^{\prime}}\big{)}+t_{hi}}{s_{hi}^{ 2}\|\mathbf{x}_{i}\|_{2}^{2}+1}\right).\] (B.8)

The canonical CD updates in (B.6) - (B.8) for one full iteration require computation complexity of \(\mathcal{O}\big{(}(K+nd+nL+nH)(K+nL+nH)\big{)}\).

### ReHLine updates

Consider the Lagrangian function \(\mathcal{L}\) in (7). Clearly,

\[\nabla_{\xi_{k}}\mathcal{L} =\mathbf{a}_{k}^{\intercal}\mathbf{a}_{k}\xi_{k}+\sum_{k^{\prime} \neq k}\xi_{k^{\prime}}\mathbf{a}_{k^{\prime}}^{\intercal}\mathbf{a}_{k}-\sum_{ l,i}\lambda_{li}u_{li}\mathbf{a}_{k}^{\intercal}\mathbf{x}_{i}-\sum_{h,i} \gamma_{hi}s_{hi}\mathbf{a}_{k}^{\intercal}\mathbf{x}_{i}+b_{k}\] \[=\sum_{k^{\prime}}\xi_{k^{\prime}}\mathbf{a}_{k^{\prime}}^{ \intercal}\mathbf{a}_{k}-\sum_{l,i}\lambda_{li}u_{li}\mathbf{a}_{k}^{\intercal }\mathbf{x}_{i}-\sum_{h,i}\gamma_{hi}s_{hi}\mathbf{a}_{k}^{\intercal}\mathbf{x }_{i}+b_{k}\] \[=\mathbf{a}_{k}^{\intercal}\boldsymbol{\beta}(\boldsymbol{\mu})+b _{k}.\]

Similarly, we can derive that

\[\nabla_{\lambda_{li}}\mathcal{L}=-\big{(}u_{li}\mathbf{x}_{i}^{ \intercal}\boldsymbol{\beta}(\boldsymbol{\mu})+v_{li}\big{)},\quad\nabla_{ \gamma_{hi}}\mathcal{L}=\gamma_{hi}-\big{(}s_{hi}\mathbf{x}_{i}^{\intercal} \boldsymbol{\beta}(\boldsymbol{\mu})+t_{hi}\big{)}.\]

Then the CD updates in (B.6) can be simplified as:

\[\xi_{k}^{\mathrm{new}} =\mathcal{P}_{[0,+\infty)}\left(\xi_{k}^{\mathrm{old}}-\frac{ \nabla_{\xi_{k}}\mathcal{L}(\xi^{\mathrm{old}})}{\|\mathbf{a}_{k}\|_{2}^{2}} \right)=\max\left(0,\xi_{k}^{\mathrm{old}}-\frac{\mathbf{a}_{k}^{\intercal} \boldsymbol{\beta}^{\mathrm{old}}+b_{k}}{\|\mathbf{a}_{k}\|_{2}^{2}}\right),\] \[\boldsymbol{\beta}^{\mathrm{new}} =\boldsymbol{\beta}^{\mathrm{old}}+(\xi_{k}^{\mathrm{new}}-\xi_{k }^{\mathrm{old}})\mathbf{a}_{k}.\] (B.9)

The CD updates in (B.7) are simplified as:

\[\lambda_{li}^{\mathrm{new}} =\mathcal{P}_{[0,1]}\left(\lambda_{li}^{\mathrm{old}}-\frac{ \nabla_{\lambda_{li}}\mathcal{L}(\lambda^{\mathrm{old}})}{u_{li}^{2}\|\mathbf{ x}_{i}\|_{2}^{2}}\right)=\max\left(0,\min\left(1,\lambda_{li}^{\mathrm{old}}+\frac{u_{ li}\mathbf{x}_{i}^{\intercal}\boldsymbol{\beta}^{\mathrm{old}}+v_{li}}{u_{li}^{2}\| \mathbf{x}_{i}\|_{2}^{2}}\right)\right),\] \[\boldsymbol{\beta}^{\mathrm{new}} =\boldsymbol{\beta}^{\mathrm{old}}-(\lambda_{li}^{\mathrm{new}}- \lambda_{li}^{\mathrm{old}})u_{li}\mathbf{x}_{i}.\] (B.10)

The CD updates in (B.8) are simplified as:

\[\gamma_{hi}^{\mathrm{new}} =\mathcal{P}_{[0,\gamma_{hi}]}\left(\gamma_{hi}^{\mathrm{old}}- \frac{\nabla_{\gamma_{hi}}\mathcal{L}(\gamma^{\mathrm{old}})}{s_{hi}^{2}\| \mathbf{x}_{i}\|_{2}^{2}+1}\right)=\max\left(0,\min\left(\tau_{hi},\gamma_{ hi}^{\mathrm{old}}+\frac{s_{hi}\mathbf{x}_{i}^{\intercal}\boldsymbol{\beta}^{\mathrm{ old}}+t_{hi}-\gamma_{hi}^{\mathrm{old}}}{s_{hi}^{2}\|\mathbf{x}_{i}\|_{2}^{2}+1} \right)\right),\] \[\boldsymbol{\beta}^{\mathrm{new}} =\boldsymbol{\beta}^{\mathrm{old}}-(\gamma_{hi}^{\mathrm{new}}- \gamma_{hi}^{\mathrm{old}})s_{hi}\mathbf{x}_{i}.\] (B.11)

The computational complexity of the new ReHLine iterations is \(\mathcal{O}\big{(}(K+nL+nH)d\big{)}\), as shown in (B.9) through (B.11). Notably, this complexity is _linear_ with respect to both the sample size \(n\) and the dimensionality \(d\), making ReHLine a highly efficient algorithm.

## Appendix C Screening rules for ReHLine

We extend the shrinking strategy in Algorithm 3 of [12] to the ReHLine solver. To begin with, the following theorem advocates utilizing "sparse" updates to optimize the performance of ReHLine.

**Theorem 4**.: _Let \(\{\boldsymbol{\xi}^{*},\boldsymbol{\Lambda}^{*},\boldsymbol{\Gamma}^{*}, \boldsymbol{\beta}^{*}\}\) be the convergent point of \(\{\boldsymbol{\xi}^{(q)},\boldsymbol{\Lambda}^{(q)},\boldsymbol{\Gamma}^{(q)}, \boldsymbol{\beta}^{(q)}\}\) based on (B.9)-(B.11), then_

1. _For_ \(\boldsymbol{\xi}\) _updates, let_ \(g_{k}^{*}=\mathbf{a}_{k}^{\intercal}\boldsymbol{\beta}^{*}+b_{k}\)_,_ * _if_ \(g_{k}^{*}>0\)_, then_ \(\exists q_{0}\) _such that_ \(\forall q\geq q_{0}\)_,_ \(\xi_{k}^{(q)}=\xi_{k}^{*}=0\)_._
2. _For_ \(\boldsymbol{\Lambda}\) _updates,_ \(g_{li}^{*}=u_{li}\mathbf{x}_{i}^{\intercal}\boldsymbol{\beta}^{*}+v_{li}\)_,_ * _if_ \(g_{li}^{*}<0\)_, then_ \(\exists q_{0}\) _such that_ \(\forall q\geq q_{0}\)_,_ \(\lambda_{li}^{(q)}=\lambda_{li}^{*}=0\)_;_ * _if_ \(g_{li}^{*}>0\)_, then_ \(\exists q_{0}\) _such that_ \(\forall q\geq q_{0}\)_,_ \(\lambda_{li}^{(q)}=\lambda_{li}^{*}=1\)_._
3. _For_ \(\boldsymbol{\Gamma}\) _updates, denote_ \(g_{hi}^{*}=s_{hi}\mathbf{x}_{i}^{\intercal}\boldsymbol{\beta}^{*}+t_{hi}\)_,_ * _if_ \(\gamma_{hi}^{*}=0\) _and_ \(g_{hi}^{*}<0\)_, then_ \(\exists q_{0}\) _such that_ \(\forall q\geq q_{0}\)_,_ \(\gamma_{hi}^{(q)}=0\)_;_ * _if_ \(\gamma_{hi}^{*}=\tau_{hi}\) _and_ \(g_{hi}^{*}>\tau_{hi}\)_, then_ \(\exists q_{0}\) _such that_ \(\forall q\geq q_{0}\)_,_ \(\gamma_{hi}^{(q)}=\tau_{hi}\)_._

Building upon the insights from Theorem 4, we have formulated Algorithm 2 that involves shrinking certain "boundary" variables during the implementation of ReHLine.

```
Input :\(\mathbf{X}\in\mathbb{R}^{n\times d}\), \(\mathbf{U},\mathbf{V}\in\mathbb{R}^{L\times n}\); \(\mathbf{S},\mathbf{T},\mathbf{\tau}\in\mathbb{R}^{H\times n}\), \(\mathbf{A}\in\mathbb{R}^{K\times d}\), \(\mathbf{b}\in\mathbb{R}^{K}\), tol \(\varepsilon>0\)
1 Initialize \(\boldsymbol{\xi}\geq\mathbf{0}\), \(\mathbf{E}\geq\mathbf{A}\geq\mathbf{0}\), \(\mathbf{\tau}\geq\mathbf{\Gamma}\geq\mathbf{0}\);
2 Compute \(\boldsymbol{\beta}=\sum_{k=1}^{K}\xi_{k}\mathbf{a}_{k}-\sum_{i=1}^{n}\mathbf{x} _{i}\left(\sum_{l=1}^{L}\lambda_{li}u_{li}+\sum_{h=1}^{H}\gamma_{hi}s_{hi}\right)\);
3 Compute and store \(\mathbf{r}=(r_{i})\in\mathbb{R}^{n}\), \(r_{i}=\|\mathbf{x}_{i}\|_{2}^{2}\), and \(\mathbf{p}=(p_{k})\in\mathbb{R}^{K}\), \(p_{k}=\|\mathbf{a}_{k}\|_{2}^{2}\);
4 Initialize \(\bar{M}_{s}=\infty,\bar{m}_{s}=-\infty\), \(s=\{\xi,\lambda,\gamma\}\), \(\mathcal{K}=[K]\), \(\mathcal{L}=[L]\bigotimes[n]\), \(\mathcal{H}=[H]\bigotimes[n]\);
5whilemaximum iterationdo
6 Initialize \(M_{s}=\infty,m_{s}=-\infty\), \(s=\{\xi,\lambda,\gamma\}\); // CD updates for\(\boldsymbol{\xi}\) for\(k\in\texttt{perm}(\mathcal{K})\)do
7\(g_{k}=\mathbf{a}_{k}^{\intercal}\boldsymbol{\beta}+b_{k}\);
8\((\texttt{is\_shrink},\bar{g}_{k})\leftarrow\texttt{PG\_feasible}(\xi_{k},g_{k},\bar{M}_{\xi})\) ;
9ifis_shrinkthen
10\(\mathcal{K}\leftarrow\mathcal{K}\setminus\{k\}\); continue;
11\(M_{\xi}\leftarrow\max\{M_{\xi},\bar{g}_{k}\}\); \(m_{\xi}\leftarrow\min\{m_{\xi},\bar{g}_{k}\}\);
12\(\varepsilon^{*}=\max(-\xi_{k},-p_{k}^{-1}g_{k})\); \(\xi_{k}\leftarrow\xi_{k}+\varepsilon^{*}\); \(\boldsymbol{\beta}\leftarrow\boldsymbol{\beta}+\varepsilon^{*}\mathbf{a}_{k}\);
13 // CD updates for\(\boldsymbol{\Lambda}\)
14for\((l,i)\in\texttt{perm}(\mathcal{L})\)do
15\(g_{li}=-(u_{li}\mathbf{x}_{i}^{\intercal}\boldsymbol{\beta}+v_{li})\);
16\((\texttt{is\_shrink},\bar{g}_{li})\leftarrow\texttt{PG\_ReLU}(\lambda_{li},g_ {li},\bar{M}_{\lambda},\bar{m}_{\lambda})\);
17ifis_shrinkthen
18\(\mathcal{L}\leftarrow\mathcal{L}\setminus\{l,i\}\); continue;
19\(M_{\lambda}\leftarrow\max\{M_{\lambda},\bar{g}_{li}\}\); \(m_{\lambda}\leftarrow\min\{m_{\lambda},\bar{g}_{li}\}\);
20\(\varepsilon^{*}=\max(-\lambda_{li},\min(1-\lambda_{li},u_{li}^{-2}r_{i}^{-1}g _{li}))\); \(\lambda_{li}\leftarrow\lambda_{li}+\varepsilon^{*}\); \(\boldsymbol{\beta}\leftarrow\boldsymbol{\beta}-\varepsilon^{*}u_{li}\mathbf{x} _{i}\);
21 // CD updates for\(\boldsymbol{\Gamma}\)
22for\((h,i)\in\texttt{perm}(\mathcal{H})\)do
23\(g_{hi}=\gamma_{hi}-(s_{hi}\mathbf{x}_{i}^{\intercal}\boldsymbol{\beta}+t_{hi})\);
24\((\texttt{is\_shrink},\bar{g}_{hi})\leftarrow\texttt{PG\_ReLU}(\gamma_{hi},g_ {hi},\bar{M}_{\gamma},\bar{m}_{\gamma})\);
25ifis_shrinkthen
26\(\mathcal{H}\leftarrow\mathcal{H}\setminus\{h,i\}\); continue;
27\(\mathcal{H}\leftarrow\max\{M_{\gamma},\bar{g}_{hi}\}\); \(m_{\gamma}\leftarrow\min\{m_{\gamma},\bar{g}_{hi}\}\);
28\(\varepsilon^{*}=\max(-\gamma_{hi},\min(\tau_{hi}-\gamma_{hi},(s_{hi}^{2}r_{i} +1)^{-1}(g_{hi}-\gamma_{hi})))\);
29\(\gamma_{hi}\leftarrow\gamma_{hi}+\varepsilon^{*}\); \(\boldsymbol{\beta}\leftarrow\boldsymbol{\beta}-\varepsilon^{*}s_{hi}\mathbf{x} _{i}\);
30
31if\(M_{s}-m_{s}\leq\varepsilon\)and\(|M_{s}|\leq\varepsilon\)and\(|m_{s}|\leq\varepsilon\), \(s=\{\xi,\lambda,\gamma\}\)then
32if\(|\mathcal{K}|+|\mathcal{L}|+|\mathcal{H}|=K+n(L+H)\)then
33break;
34else
35 Take all shrunken variables back,
36 i.e., \(\mathcal{K}=[K]\), \(\mathcal{L}=[L]\bigotimes[n]\), \(\mathcal{H}=[H]\bigotimes[n]\);
37\(\bar{M}_{s}\leftarrow\infty,\bar{m}_{s}\leftarrow-\infty,s=\{\xi,\lambda,\gamma\}\)
38
39if\(M_{s}\leq 0\)then\(\bar{M}_{s}\leftarrow\infty\)else\(\bar{M}_{s}\gets M_{s},s=\{\xi,\lambda,\gamma\}\);
40if\(m_{s}\geq 0\)then\(\bar{m}_{s}\leftarrow\infty\)else\(\bar{m}_{s}\gets m_{s},s=\{\xi,\lambda,\gamma\}\);
41
42Output :\(\boldsymbol{\xi}\), \(\boldsymbol{\Lambda}\), \(\boldsymbol{\Gamma}\), \(\boldsymbol{\beta}\)
```

**Algorithm 2**ReHline solver minimizing (1) with shrinking.

```
1FunctionPG_feasible(\(\xi_{k}\), \(g_{k}\), \(\bar{M}\)):
2\(\bar{g}_{k}\gets g_{k}\); is_shrink\(\leftarrow\)False;
3if\(\xi_{k}=0\)and\(g_{k}>\bar{M}\)then
4is_shrink\(\leftarrow\)True
5if\(\xi_{k}=0\)and\(g_{k}\geq 0\)then
6\(\bar{g}_{k}=0\)
7Output:is_shrink, \(\bar{g}_{k}\)
8FunctionPG_ReLU(\(\lambda_{li}\), \(g_{li}\), \(\bar{M}\), \(\bar{m}\)):
9\(\bar{g}_{li}\gets g_{li}\); is_shrink\(\leftarrow\)False;
10if\((\lambda_{li}=0\)and\(g_{li}>\bar{M})\)or\((\lambda_{li}=1\)and\(g_{li}<\bar{m})\)then
11is_shrink\(\leftarrow\)True
12if\((\lambda_{li}=0\)and\(g_{li}\geq 0)\)or\((\lambda_{li}=1\)and\(g_{li}\leq 0)\)then
13\(\bar{g}_{li}=0\)
14Output:is_shrink, \(\bar{g}_{li}\)
```

**Algorithm 3**Projected gradients in ReHLine.

## Appendix D Technical proofs

**Lemma 1**.: _Suppose \(f(z)\) is a nonnegative convex PLQ with knots and extreme points \((t_{k})_{k=1}^{K}\). Then, there exists \(k_{0}\in\{1,\cdots,K\}\), such that \(f(z)\geq f(t_{k_{0}})\) for any \(z\in\mathbb{R}\)._

Proof.: We prove it by contradiction. Suppose that for there exists \(z_{0}\) such that \(f(z_{0})<f(t_{k})\) for all \(k\in\{1,\cdots,K\}\), then the minimum can only be obtained outside \([t_{1},t_{K}]\), that is

\[\exists z_{0}<t_{1},f(z_{0})<f(t_{k}),\quad\text{or}\quad\exists z_{0}>t_{K},f (z_{0})<f(t_{k})\quad\text{for any }k\in\{1,\cdots,K\}.\]

For example, suppose that \(z_{0}\in(t_{K},\infty)\), \(f(z_{0})<f(t_{k})\) for any \(k\in\{1,\cdots,K\}\). Note that the quadratic function in \((z_{K},\infty)\) can be written as \(f(z)=\frac{1}{2}f^{\prime\prime}_{+}(t_{K})(z-t_{K})^{2}+f^{\prime}_{+}(t_{K}) (z-t_{K})+f(t_{K})\). Based on the fact that \(f(z_{0})<f(t_{K})\) and the convexity of \(f(z)\), we have \(f^{\prime}_{+}(t_{K})<0\). Then, since \((t_{k})_{k=1}^{K}\) is the set of all knots and extreme points, we have \(f^{\prime}(z)<0\) for all \(z>t_{K}\), and \(\lim_{z\to\infty}f^{\prime}(z)=\lim_{z\to\infty}(f^{\prime\prime}_{+}(t_{K})(z -t_{K})+f^{\prime}_{+}(t_{K}))\leq 0\). This yields that \(f^{\prime\prime}_{+}(t_{K})=0\), alternatively, \(f(z)\) is a linear function in \((t_{K},\infty)\) as \(f(z)=f^{\prime}_{+}(t_{K})(z-t_{K})+f(t_{K})\). Hence, when \(z\to\infty\), \(f(z)\to-\infty\), which contradicts with the fact that \(f(z)\) is a nonnegative function. The same argument goes for the case when \(z_{0}\in(-\infty,t_{1})\). This completes the proof. 

#### Proof of Proposition 1

Proof.: Note that \(c\mathrm{ReLU}(z)=c\max(z,0)=\max(cz,0)\), and

\[c\mathrm{ReHU}_{\tau}(z)=\left\{\begin{array}{ll}0,&\sqrt{c}z\leq 0\\ (\sqrt{c}z)^{2}/2,&0<\sqrt{c}z\leq\sqrt{c}\tau\ =\mathrm{ReHU}_{\sqrt{c}\tau}( \sqrt{c}z),\\ \sqrt{c}\tau(\sqrt{c}z-\sqrt{c}\tau/2),&\sqrt{c}z>\sqrt{c}\tau\end{array}\right.\]for \(c>0\). Then,

\[cL(pz+q) =\sum_{l=1}^{L}c\mathrm{ReLU}(u_{l}(pz+q)+v_{l})+\sum_{h=1}^{H}c \mathrm{ReLU}_{\tau_{h}}(s_{h}(pz+q)+t_{h})\] \[=\sum_{l=1}^{L}\mathrm{ReLU}(cu_{l}(pz+q)+cv_{l})+\sum_{h=1}^{H} \mathrm{ReLU}_{\sqrt{c}\tau_{h}}(\sqrt{c}s_{h}(pz+q)+\sqrt{c}t_{h})\] \[=\sum_{l=1}^{L}\mathrm{ReLU}(u_{l}^{\prime}z+v_{l}^{\prime})+\sum_ {h=1}^{H}\mathrm{ReLU}_{\tau_{h}^{\prime}}(s_{h}^{\prime}z+t_{h}^{\prime}).\]

This completes the proof. 

#### Proof of Theorem 1

Proof.: The sufficiency (\(\Longleftarrow\)) directly follows from the definitions of \(\mathrm{ReLU}\) and \(\mathrm{ReLU}\). Next, the necessity (\(\Longrightarrow\)) of the theorem is established using induction. Without loss of generality, for a nonnegative convex PLQ function \(f(z)\), we assume that (A1) \(f_{+}^{\prime}(t_{k})f_{-}^{\prime}(t_{k+1})\geq 0\), otherwise we can add the extreme point between \((t_{k},t_{k+1})\) into the set of knots; (A2) \(\min_{k=1,\cdots,K}f(t_{k})=0\), otherwise we can add \(\mathrm{ReLU}\left(\min_{k=1,\cdots,K}f(t_{k})\right)\) to the final decomposition (Lemma 1 shows that the minimum of \(f(z)\) can only be obtained within the knots). Next, we conduct the mathematical induction.

\(K=1\). Note that \(f(z)\) can be rewritten as:

\[f(z)=\left\{\begin{array}{ll}q_{1}^{-}(z)=\frac{f_{-}^{\prime\prime}(t_{1})} {2}(z-t_{1})^{2}+f_{-}^{\prime}(t_{1})(z-t_{1}),&z\leq t_{1},\\ q_{1}^{+}(z)=\frac{f_{+}^{\prime\prime}(t_{1})}{2}(z-t_{1})^{2}+f_{+}^{\prime}( t_{1})(z-t_{1}),&z>t_{1},\end{array}\right.\]

where \(q_{1}^{-}(z)\) and \(q_{1}^{+}(z)\) are left and right parts of \(f(z)\) separated at \(z=t_{1}\). According to Lemma 1 and Assumption A2, we have \(f(t_{1})=0\) is the minimum of \(f(z)\). Combined with Assumption A1, we have \(f_{-}^{\prime}(t)\leq 0\) for \(t<t_{1}\), and \(f_{+}^{\prime}(t)\geq 0\) for \(t>t_{1}\). Then, \(f(z)\) can be decomposed as:

\[f(z)=\left\{\begin{array}{ll}q_{1}^{-}(z)=\mathrm{ReLU}_{\infty}(-\sqrt{f_{- }^{\prime\prime}(t_{1})}(z-t_{1}))+\mathrm{ReLU}(f_{-}^{\prime}(t_{1})(z-t_{1} )),&z\leq t_{1},\\ q_{1}^{+}(z)=\mathrm{ReLU}_{\infty}(\sqrt{f_{+}^{\prime\prime}(t_{1})}(z-t_{1})) +\mathrm{ReLU}(f_{+}^{\prime}(t_{1})(z-t_{1})),&z>t_{1}.\end{array}\right.\]

Hence, \(f(z)=\mathrm{ReLU}_{\infty}(-\sqrt{f_{-}^{\prime\prime}(t_{1})}(z-t_{1}))+ \mathrm{ReLU}(f_{-}^{\prime}(t_{1})(z-t_{1}))+\mathrm{ReLU}_{\infty}(\sqrt{f_{ +}^{\prime\prime}(t_{1})}(z-t_{1}))+\mathrm{ReLU}(f_{+}^{\prime}(t_{1})(z-t_{1 }))\). The necessity of the theorem for \(K=1\) then follows.

\(K=K_{0}+1\). Assume that a convex PLQ \(f\geq 0\) with \(K(K\leq K_{0})\) knots is ReLU-ReHU composite, now we turn to prove the statement still holds for \(f(z)\) with \(K_{0}+1\).

* _CASE 1._ Suppose the minimum \(f(t^{*})=0\) is obtained at \(\{t_{2},\cdots t_{K_{0}}\}\) (not in the boundary). In this case, we can decompose \(f(z)\) as \(f(z)=f_{l}(z)+f_{r}(z)\) where \(f_{l}(z)\) and \(f_{r}(z)\) are defined as: Both \(f_{l}(z)\) and \(f_{r}(z)\) are nonnegative convex PLQs with \(K\leq K_{0}\) knots, thus can be represented as ReHUs and ReLUs.
* _CASE 2._ Suppose the minimum \(f(t^{*})=0\) is obtained when \(t^{*}=t_{1}\) or \(t^{*}=t_{K_{0}+1}\). For example, when \(t^{*}=t_{1}\), we conduct the same decomposition \(f(z)=f_{l}(z)+f_{r}(z)\) at \(z=t_{1}\), where \(f_{l}(z)\) is a nonnegative convex PLQ with one knot, which is ReHU-ReLU composite. For \(f_{r}(z)\), we further decompose it as: \[f_{r}(z)=\mathrm{ReLU}(f_{+}^{\prime}(t_{1})(z-t_{1}))+\mathrm{ReLU}_{\tau} \big{(}\sqrt{f_{+}^{\prime\prime}(t_{1})}(z-t_{1})\big{)}+R(z),\] where \(\tau=\sqrt{f_{+}^{\prime\prime}(t_{1})}(t_{2}-t_{1})\), and \(R(z)\) (with at most \(K_{0}\) knots) is defined as: \[R(z)=\left\{\begin{array}{ll}0,&z\leq t_{2},\\ f(z)-f_{+}^{\prime}(t_{1})(z-t_{1})-f_{+}^{\prime\prime}(t_{1})(t_{2}-t_{1})(z- (t_{1}+t_{2})/2),&z>t_{2}.\end{array}\right.\]Now, we show that \(R(z)\) is nonnegative convex. Note that \(f(z)\) is convex, then for \(z\geq t_{2}\),

\[f(z) \geq f(t_{2})+f^{\prime}_{-}(t_{2})(z-t_{2})=f(t_{2})+(f^{\prime \prime}_{+}(t_{1})(t_{2}-t_{1})+f^{\prime}_{+}(t_{1}))(z-t_{2})\] \[=f^{\prime}_{+}(t_{1})(z-t_{1})+f^{\prime\prime}_{+}(t_{1})(t_{2}- t_{1})(z-(t_{1}+t_{2})/2)\] \[=\operatorname{ReLU}(f^{\prime}_{+}(t_{1})(z-t_{1}))+\operatorname {ReLU}_{\tau}\bigl{(}\sqrt{f^{\prime\prime}_{+}(t_{1})}(z-t_{1})\bigr{)},\]

where the first equality follows from the fact that \(f(z)\) is a quadratic function in \([t_{1},t_{2}]\), and thus \(f^{\prime}_{-}(t_{2})=f^{\prime}_{+}(t_{1})(t_{2}-t_{1})\), and the second equality follows from \(f(t_{2})=\frac{f^{\prime\prime}_{-}(t_{1})}{2}(t_{2}-t_{1})^{2}+f^{\prime}_{-} (t_{1})(t_{2}-t_{1})\).

Thus, \(R(z)\) is a nonnegative convex PLQ with \(K_{0}\) knots \(\{t_{2},\cdots,t_{K_{0}+1}\}\), which can be decomposed as ReHUs and ReLUs. The same argument goes for the case when \(t^{*}=t_{K_{0}+1}\).

This completes the proof, and the desirable results follow from the induction. 

#### Proof of Proposition 2

Proof.: The lemma follows from the fact that:

\[\|\mathbf{\beta}\|_{1}=\sum_{j=1}^{d}|\beta_{j}|=\sum_{j=1}^{d}\big{(} \operatorname{ReLU}(\mathbf{e}_{j}^{\intercal}\mathbf{\beta})+\operatorname{ReLU} (-\mathbf{e}_{j}^{\intercal}\mathbf{\beta})\big{)},\]

where \(\mathbf{e}_{j}=(0,\cdots,0,1,0\cdots)\) is the \(d\)-tuple with all components equal to 0, except the \(j\)th. 

#### Proof of Theorem 2

Proof.: Note that the augmented Lagrangian of the primal problem (6) is:

\[\mathcal{L}_{P} =\sum_{i=1}^{n}\sum_{l=1}^{L}\pi_{li}+\sum_{i=1}^{n}\sum_{h=1}^{H }\frac{1}{2}\theta_{hi}^{2}+\sum_{i=1}^{n}\sum_{h=1}^{H}\tau_{hi}\sigma_{hi}+ \frac{1}{2}\|\mathbf{\beta}\|_{2}^{2}-\sum_{k=1}^{K}\xi_{k}(\mathbf{a}_{k}^{ \intercal}\mathbf{\beta}+b_{k})\] \[\quad-\sum_{i=1}^{n}\sum_{l=1}^{L}\lambda_{li}(\pi_{li}-u_{li} \mathbf{x}_{i}^{\intercal}\mathbf{\beta}-v_{li})-\sum_{i=1}^{n}\sum_{h=1}^{H}\gamma _{hi}(\theta_{hi}+\sigma_{hi}-s_{hi}\mathbf{x}_{i}^{\intercal}\mathbf{\beta}-t_{ hi})\] \[\quad-\sum_{i=1}^{n}\sum_{l=1}^{L}\delta_{li}\pi_{li}-\sum_{i=1} ^{n}\sum_{h=1}^{H}\psi_{hi}\sigma_{hi}\] \[=\mathbf{e}^{\intercal}\mathbf{\Pi}\mathbf{e}+\frac{1}{2}\|\mathbf{ \Theta}\|_{F}^{2}+\operatorname{Tr}(\mathbf{\tau}\mathbf{\Sigma}^{\intercal})+\frac{1 }{2}\|\mathbf{\beta}\|_{2}^{2}-\mathbf{\xi}^{\intercal}(\mathbf{A}\mathbf{\beta}+\mathbf{b})\] \[\quad-\operatorname{Tr}\big{(}\mathbf{\Lambda}(\mathbf{\Pi}-\mathbf{U} \operatorname{diag}(\mathbf{X}\mathbf{\beta})-\mathbf{V})^{\intercal}\big{)}- \operatorname{Tr}\big{(}\mathbf{\Gamma}(\mathbf{\Theta}+\mathbf{\Sigma}-\mathbf{S} \operatorname{diag}(\mathbf{X}\mathbf{\beta})-\mathbf{T})^{\intercal}\big{)}\] \[\quad-\operatorname{Tr}(\mathbf{\Delta}\mathbf{\Pi}^{\intercal})- \operatorname{Tr}(\mathbf{\Psi}\mathbf{\Sigma}^{\intercal}),\] (D.12)

where \(\mathbf{\xi}=(\xi_{k})\in\mathbb{R}^{K}\), \(\mathbf{\Lambda}=(\lambda_{li})\in\mathbb{R}^{L\times n}\), \(\mathbf{\Gamma}=(\gamma_{hi})\in\mathbb{R}^{H\times n}\), \(\mathbf{\Delta}=(\delta_{li})\in\mathbb{R}^{L\times n}\) and \(\mathbf{\Psi}=(\psi_{hi})\in\mathbb{R}^{H\times n}\) are dual variables. The KKT conditions for the augmented Lagrangian are:

\[\frac{\partial\mathcal{L}_{P}}{\partial\mathbf{\beta}}=\mathbf{\beta}- \sum_{k=1}^{K}\xi_{k}\mathbf{a}_{k}+\sum_{i=1}^{n}\mathbf{x}_{i}\big{(}\sum_{l= 1}^{L}\lambda_{li}u_{li}+\sum_{h=1}^{H}\gamma_{hi}s_{hi}\big{)}=\mathbf{0},\] \[\frac{\partial\mathcal{L}_{P}}{\partial\pi_{li}}=1-\lambda_{li}- \delta_{li}=0,\quad\frac{\partial\mathcal{L}_{P}}{\partial\theta_{hi}}=\theta_ {hi}-\gamma_{hi}=0,\quad\frac{\partial\mathcal{L}_{P}}{\partial\sigma_{hi}}= \tau_{hi}-\gamma_{hi}-\psi_{hi}=0,\]for all \((i,l,h)\). Then, by substituting the KKT conditions into the primal Lagrangian, the (negative) dual objective function is derived as:

\[\mathcal{L}_{D} =\frac{1}{2}\Big{(}\sum_{k=1}^{K}\sum_{k^{\prime}=1}^{K}\xi_{k}\xi _{k^{\prime}}\mathbf{a}_{k}^{\intercal}\mathbf{a}_{k^{\prime}}+\sum_{l,i}\sum_{ l^{\prime},i^{\prime}}\lambda_{li}\lambda_{l^{\prime}i^{\prime}}u_{li}u_{l^{\prime}i^{ \prime}}\mathbf{x}_{i}^{\intercal}\mathbf{x}_{i^{\prime}}+\sum_{h,i}\sum_{h^{ \prime},i^{\prime}}\gamma_{hi}\gamma_{h^{\prime}i^{\prime}i^{\prime}}s_{hi}s_{ h^{\prime}i^{\prime}}\mathbf{x}_{i}^{\intercal}\mathbf{x}_{i}^{\prime}\Big{)}\] \[\quad-\sum_{k=1}^{K}\sum_{l,i}\xi_{k}\lambda_{li}u_{li}\mathbf{a} _{k}^{\intercal}\mathbf{x}_{i}-\sum_{k=1}^{K}\sum_{h,i}\xi_{k}\gamma_{hi}s_{hi} \mathbf{a}_{k}^{\intercal}\mathbf{x}_{i}+\sum_{l,i}\sum_{h^{\prime},i^{\prime} }\lambda_{li}u_{li}\gamma_{h^{\prime}i^{\prime}}s_{h^{\prime}i^{\prime}} \mathbf{x}_{i}^{\intercal}\mathbf{x}_{i^{\prime}}\] \[\quad+\frac{1}{2}\sum_{h,i}\gamma_{hi}^{2}+\sum_{k=1}^{K}\xi_{k} b_{k}-\sum_{l,i}\lambda_{li}v_{li}-\sum_{h,i}\gamma_{hi}t_{hi}\] \[=\frac{1}{2}\boldsymbol{\xi}^{\intercal}\mathbf{A}\mathbf{A}^{ \intercal}\boldsymbol{\xi}+\frac{1}{2}\text{vec}(\mathbf{\Lambda})^{\intercal} \boldsymbol{\bar{\mathbf{U}}}_{(3)}^{\intercal}\boldsymbol{\bar{\mathbf{D}}}_ {(3)}\text{vec}(\mathbf{\Lambda})+\frac{1}{2}\text{vec}(\mathbf{\Gamma})^{ \intercal}\big{(}\boldsymbol{\bar{\mathbf{S}}}_{(3)}^{\intercal}\boldsymbol{ \bar{\mathbf{S}}}_{(3)}+\mathbf{I}\big{)}\text{vec}(\mathbf{\Gamma})\] \[\quad-\boldsymbol{\xi}^{\intercal}\mathbf{A}\bar{\mathbf{U}}_{(3 )}\text{vec}(\mathbf{\Lambda})-\boldsymbol{\xi}^{\intercal}\mathbf{A}\bar{ \mathbf{S}}_{(3)}\text{vec}(\mathbf{\Gamma})+\text{vec}(\mathbf{\Lambda})^{ \intercal}\bar{\mathbf{U}}_{(3)}^{\intercal}\bar{\mathbf{S}}_{(3)}\text{vec}( \mathbf{\Gamma})\] \[\quad+\boldsymbol{\xi}^{\intercal}\mathbf{b}-\text{Tr}(\mathbf{ \Lambda}\mathbf{V}^{\intercal})-\text{Tr}(\mathbf{\Gamma}\mathbf{T}^{ \intercal}),\]

where \(\bar{\mathbf{U}}_{(3)}\) and \(\bar{\mathbf{S}}_{(3)}\) are the mode-3 unfolding of the tensors \(\bar{\mathbf{U}}\in\mathbb{R}^{L\times n\times d}\) and \(\bar{\mathbf{S}}\in\mathbb{R}^{H\times n\times d}\), and \(\mathbf{I}\) is an identity matrix. 

#### Proof of Theorem 3

Proof.: The treatment for the proof is based on a result of coordinate descent on boxed constrained convex optimization [27]. To proceed, denote \(\boldsymbol{\mu}=(\boldsymbol{\xi}^{\intercal},\text{vec}(\mathbf{\Lambda})^{ \intercal},\text{vec}(\mathbf{\Gamma})^{\intercal})^{\intercal}\) as the collection of all dual variables, then the dual objective can be rewritten as:

\[\mathcal{L}_{D}(\boldsymbol{\mu})=g(\mathbf{E}\boldsymbol{\mu})+\mathbf{c}^{ \intercal}\boldsymbol{\mu},\]

where \(g(\mathbf{z})=\frac{1}{2}\|\mathbf{z}\|_{2}^{2}\), and \(\mathbf{E}\) and \(\mathbf{c}\) are defined as:

\[\mathbf{E} =\begin{bmatrix}\mathbf{A}^{\intercal}&-\boldsymbol{\bar{\mathbf{ U}}}_{(3)}&-\bar{\mathbf{S}}_{(3)}\\ \mathbf{0}&\mathbf{0}^{\intercal}&\mathbf{I}_{H\times n}\end{bmatrix}\in\mathbb{ R}^{(d+Hn)\times(K+Ln+Hn)},\] \[\mathbf{c} =\big{(}\mathbf{b}^{\intercal},-\text{vec}(\mathbf{V})^{\intercal },\text{vec}(\mathbf{T})^{\intercal}\big{)}^{\intercal}\in\mathbb{R}^{K+Ln+ Hn}.\]

Now, we tend to verify assumptions of Theorem 2.1 in [27]. Specifically, (a) the dual problem is a convex box-constrained QP, thus the set of optimal solutions is nonempty; (b) \(g(\mathbf{z})=\frac{1}{2}\|\mathbf{z}\|_{2}^{2}\) is a strictly convex twice continuously differentiable function; (c) \(\nabla^{2}g(\mathbf{z})=\mathbf{I}_{d+Hn}\) is always positive definite; (d) \(\mathbf{E}\) has no zero column based on its definition. The desirable result then follows from Theorem 2.1 of [27]. 

#### Proof of Theorem 4

Proof.: Note that the primal problem (6) is strictly convex, thus it has the unique minimizer \(\boldsymbol{\beta}^{*}\). Therefore, the sequence \(\{\boldsymbol{\beta}^{(q)}\}_{q=1}^{\infty}\) provided by the proposed Algorithm 1 converges to the unique minimizer \(\boldsymbol{\beta}^{*}\). Next, we check the gradients of each coordinate.

* For \(\boldsymbol{\xi}\)-coordinates, with \(\xi_{k}\geq 0\), \[g_{k}^{\text{old}} :=\nabla_{\xi_{k}}\mathcal{L}_{D}(\boldsymbol{\mu}^{\text{old}} )=\mathbf{a}_{k}^{\intercal}\mathbf{a}_{k}\xi_{k}^{\text{old}}+\sum_{k^{\prime} \neq k}\xi_{k^{\prime}}^{\text{old}}\mathbf{a}_{k^{\prime}}^{\intercal}\mathbf{ a}_{k}-\sum_{l,i}\lambda_{li}^{\text{old}}u_{li}\mathbf{a}_{k}^{\intercal} \mathbf{x}_{i}-\sum_{h,i}\gamma_{hi}^{\text{old}}s_{hi}\mathbf{a}_{k}^{ \intercal}\mathbf{x}_{i}+b_{k}\] \[\quad=\mathbf{a}_{k}^{\intercal}\big{(}\sum_{k=1}^{K}\xi_{k}^{ \text{old}}\mathbf{a}_{k}-\sum_{i=1}^{n}\mathbf{x}_{i}(\sum_{l=1}^{L}\lambda_{ li}^{\text{old}}u_{li}+\sum_{h=1}^{H}\gamma_{hi}^{\text{old}}s_{hi})\big{)}+b_{k}= \mathbf{a}_{k}^{\intercal}\boldsymbol{\beta}^{\text{old}}+b_{k}.\] Suppose \(g_{k}^{(q)}\) is the gradient of \(\xi_{k}\) based on \(\boldsymbol{\mu}^{(q)}\), then \(\lim_{q\to\infty}g_{k}^{(q)}=\mathbf{a}_{k}^{\intercal}\boldsymbol{\beta}^{*}+b _{k}=:g_{k}^{*}\). Therefore, if \(g_{k}^{*}>0\), then there exists \(q_{0}\), for all \(q\geq q_{0}\), we have \(g_{k}^{(q)}>0\), which implies that \(\xi_{k}^{(q+1)}=0\).

* In the same manner, for \(\mathbf{\Lambda}\)-coordinates, with \(1\geq\lambda_{li}\geq 0\), \[g^{\mathrm{old}}_{li}:=\nabla_{\lambda_{li}}\mathcal{L}_{D}(\mathbf{\mu}^{\mathrm{ old}})=-u_{li}\mathbf{x}_{i}^{\intercal}\mathbf{\beta}^{\mathrm{old}}-v_{li}.\] Then, \(\lim_{q\to\infty}g^{(q)}_{li}=-u_{li}\mathbf{x}_{i}^{\intercal}\mathbf{\beta}^{*} -v_{li}=:g^{*}_{li}\),
* if \(g^{*}_{li}<0\), then \(\exists q_{0}\) such that \(\forall q\geq q_{0}\), \(\lambda^{(q+1)}_{li}=\lambda^{*}_{li}=0\);
* if \(g^{*}_{li}>0\), then \(\exists q_{0}\) such that \(\forall q\geq q_{0}\), \(\lambda^{(q+1)}_{li}=\lambda^{*}_{li}=1\).
* For \(\mathbf{\Gamma}\)-coordinates, with \(\tau_{hi}\geq\gamma_{hi}\geq 0\), thus \[g^{\mathrm{old}}_{hi}:=\nabla_{\gamma_{hi}}\mathcal{L}_{D}(\mathbf{\mu}^{\mathrm{ old}})=\gamma^{\mathrm{old}}_{hi}-s_{hi}\mathbf{x}_{i}^{\intercal}\mathbf{\beta}^{ \mathrm{old}}-t_{hi}.\] Then, \(\lim_{q\to\infty}g^{(q)}_{hi}=\gamma^{*}_{hi}-u_{li}\mathbf{x}_{i}^{\intercal }\mathbf{\beta}^{*}-v_{li}=:g^{*}_{hi}\), which implies that
* if \(\gamma^{*}_{hi}=0\) and \(g^{*}_{hi}>0\), then \(\exists q_{0}\) such that \(\forall q\geq q_{0}\), \(\gamma^{(q)}_{hi}=0\);
* if \(\gamma^{*}_{hi}=\tau_{hi}\) and \(g^{*}_{hi}<0\), then \(\exists q_{0}\) such that \(\forall q\geq q_{0}\), \(\gamma^{(q)}_{hi}=\tau_{hi}\).