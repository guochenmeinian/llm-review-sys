# Meteor: Mamba-based Traversal of Rationale

for Large Language and Vision Models

 Byung-Kwan Lee

KAIST

leebk@kaist.ac.kr &Chae Won Kim

KAIST

chaewonkim@kaist.ac.kr &Beomchan Park

KAIST

bpark0810@kaist.ac.kr &Yong Man Ro

KAIST

ymro@kaist.ac.kr

###### Abstract

The rapid development of large language and vision models (LLVMs) has been driven by advances in visual instruction tuning. Recently, open-source LLVMs have curated high-quality visual instruction tuning datasets and utilized additional vision encoders or multiple computer vision models in order to narrow the performance gap with powerful closed-source LLVMs. These advancements are attributed to multifaceted information required for diverse capabilities, including fundamental image understanding, real-world knowledge about common-sense and non-object concepts (_e.g.,_ charts, diagrams, symbols, signs, and math problems), and step-by-step procedures for solving complex questions. Drawing from the multifaceted information, we present a new efficient LLVM, **M**amba-based **traversal **of** rationales (\(\boldsymbol{\check{\check{\mathsf{V}}}}\)**Meteor**), which leverages multifaceted rationale to enhance understanding and answering capabilities. To embed lengthy rationales containing abundant information, we employ the Mamba architecture, capable of processing sequential data with linear time complexity. We introduce a new concept of _traversal of rationale_ that facilitates efficient embedding of rationale. Subsequently, the backbone multimodal language model (MLM) is trained to generate answers with the aid of rationale. Through these steps, \(\boldsymbol{\check{\check{\mathsf{A}}}}\) Meteor achieves significant improvements in vision language performances across multiple evaluation benchmarks requiring diverse capabilities, without scaling up the model size or employing additional vision encoders and computer vision models. Code is available in https://github.com/ByungKwanLee/Meteor.

## 1 Introduction

Following the successful zero-shot achievements of instruction-tuned large language models (LLMs) [1, 2], visual instruction tuning [3] has spurred the rapid development of large language and vision models (LLVMs). The emergence of closed-source LLVMs, such as GPT-4V [4], Gemini-Pro [5], and Qwen-VL-Plus [6], has prompted several studies to create high-quality question-answer visual instruction tuning datasets [6, 7, 8, 9, 10, 11] and to scale up the model sizes of open-source LLVMs [12, 13, 14, 15, 10], aiming to compete with their closed-source counterparts by leveraging the scaling law [16, 17, 2].

Recent research trends focus on enhancing image resolution [18, 6, 11, 19, 20] and dividing images into smaller sections [12, 14, 10, 21] to improve image perception capabilities. Additionally, some studies have utilized additional vision encoders [22, 23, 24, 25] such as EVA-CLIP [26], DINOv2 [27], SAM [28], and SigLIP [29]. Various computer vision models [30, 31, 32, 33, 34] have also been employedfor tasks such as segmentation, detection, scene graph generation, and optical character recognition (OCR) to enhance LLVMs' answering capabilities with the help of external perception information.

These efforts, along with the curation of high-quality visual instruction datasets, have significantly reduced the performance gap between open- and closed-source LLVMs across numerous evaluation benchmarks and have even led to superior performances on some benchmarks. These successful developments are credited to the multifaceted information necessary for a wide range of capabilities. This encompasses fundamental image understanding, real-world knowledge of common-sense and non-object concepts (_e.g.,_ charts, diagrams, symbols, signs, and math problems), and step-by-step procedures for solving complex questions.

Inspired by the key importance of the multifaceted information, we explore the possibility of designing efficient LLVMs that implicitly embed it as a form of multifaceted rationale (See Appendix A for more details), without significantly increasing model size and without using additional explicit vision encoders and computer vision models during the inference phase. Hence, we present a new efficient LLVM, Mambo-based traversal of rationale (\(\bm{\bar{\mathcal{V}}}\)**Meteor**), comprising two core components: the Mambo architecture [39] and a multimodal language model (MLM) based on a pretrained large language model (LLM). The multifaceted rationale has rich information for achieving diverse capabilities, so its length is inherently long. This is why we employ the Mambo architecture, hereinafter referred to as Meteor-Mambo, which takes advantage of embedding lengthy input. It serves as an embedding module for the rationale, enabling Meteor-MLM, the MLM component, to address questions with the help of these embedded rationales. When conveying the knowledge of embedded rationales from Meteor-Mambo to Meteor-MLM, we introduce a new concept of _traversal of rationale_, which spurs embedding of long sequential rationales.

To ensure that \(\bm{\bar{\mathcal{A}}}\) Meteor encompasses diverse capabilities for vision-language tasks (e.g., image understanding, common-sense knowledge, charts, diagrams, documents, signs, symbols, and math problems), we gather 2.1M question-answer pairs from existing visual instruction tuning datasets: ShareGPT4V-Caption/Instruct [7], MiniGemini-Instruct [10], Doc-Downstream/Reason [20], GLLaVA-Align/Instruct [40], and Math-Vision/Instruct/Plus [41, 42, 43]. Subsequently, we utilize the light and fast Claude Haiku API [44] to generate detailed and comprehensive rationales tailored for the collected 2.1M question-answer pairs. These rationales are carefully filtered by human reviewers with the aid of GPT-4V, resulting in 1.1M question-rationale-answer triples (Appendix A).

Using the question-rationale pairs in the curated 1.1M triples, the first training step involves training Meteor-Mambo and miscellaneous projectors, _i.e.,_ a vision projector and tor projector. During

Figure 1: Across 7B to over 110B parameters, comparing lots of open- and closed-source LLVMs with \(\bm{\bar{\mathcal{A}}}\) Meteor on MME [35], MMB [36], MathVista [37], and AI2D [38] requiring diverse capabilities for image understanding, common-sense knowledge, non-object concept understanding, etc.

[MISSING_PAGE_EMPTY:3]

[MISSING_PAGE_FAIL:4]

broad coverage of math knowledge, we also include 177K GLLaVA-Align/Instruct [40], 3K Math-Vision [41], and 566K text-only samples from Math-Instruct/Plus [42, 43]. In summary, we collect 755K real-world images, 627K images for documents, charts, diagrams, signs, and symbols, and 747K math samples (180.5K with images and 566.8K text-only). Overall, the question-answer visual instruction tuning samples sum up to 2.1M.

Curating Rational.Using the gathered 2.1M question-answer pairs, we utilize the light and fast Claude Haiku API [44] to generate detailed and comprehensive rationales. We use the prompt template: _"Question: \(\{\}\). _Answer: \(\{\}\). Based on the question and answer, carefully provide an explanation about how to answer the question in detail."_ Here, \(\{\}\) represents the placeholder for the corresponding language description. Afterward, we assess the rationale score from GPT-4V [4] using the following template: _"Question: \(\{\}\). Rationale: \(\{\}\). Answer: \(\{\}\). Based on the question, rationale, and answer, provide a score from 0 to 10, evaluating how well the rationale is described to solve the question. If the given rationale is insufficient, you should rigorously give a score below 5."_ Subsequently, we filter out the generated rationales with a score below 5. The rationales that pass this automated evaluation are then subjected to human review to determine _Yes or No_ on whether they provide a proper description to address the question. Finally, this series of processes yields 1.1M question-rationale-answer triples, which include 338K real-world images covering common-sense knowledge and a few samples for diverse capabilities, 379K images for documents, charts, diagrams, signs, and symbols, and 342K math samples (165K with images and 177K text-only).

Mamba Architecture.To make LLVMs inherently possess rationale when addressing complex questions, we generate comprehensive rationales based on question-answer pairs. Subsequently, we employ the Mamba architecture [39], leveraging its capability to handle lengthy rationales while maintaining computational efficiency. This approach allows us to effectively incorporate the rationale in an environment where the curated 1.1M question-rationale pairs have an average length of 213 tokens, which is approximately ten times longer than the average length of 22 tokens of ground truth answers in typical visual instruction tuning datasets [21, 7].

Traversal of Rationale.However, it is crucial to note that we cannot acquire and utilize rationales during the inference phase without API-based models, since only user questions are given. Therefore, we propose a new concept called _traversal of rationale_ to effectively provide the rationale to Meteor-MLM without any help from external APIs in the inference phase. Inspired by retrieval-based knowledge [89], we introduce a special token, <tor> (stands for traversal **of** rationale), and evenly distribute 10 fixed number of <tor> tokens, as described in Figure 3. The rationale planted with <tor> is propagated into Meteor-Mamba along with image and question tokens, and then the output

Figure 3: Overview of \(\circledcircle\) Meteor architecture and its training steps. Note that, ‘Meteor-Multimodal Language Model (MLM)’ indicates that as training progresses, the pretrained language model evolves into a multimodal language model.

features in Meteor-Mamba are directly propagated into Meteor-MLM. Here, we autoregressively train Meteor to generate the part of the rationale between <tor>, whenever Meteor sees the special token <tor>. This procedure ensures that each <tor> represents the following rationale part until the next <tor> is encountered. Using a single <tor> token to encompass the rationale may not work well when embedding lengthy rationales, and if we do not consider distributing <tor> tokens in the rationale, then the later token does not refer to the earlier ones well due to the common problem of the autoregressive mechanism's forgetting nature [39]. This is why we place multiple <tor> tokens in the rationale instead of one.

Training Strategy.In the first training step, we leverage the question-rationale pairs in the curated 1.1M triples to train Meteor-Mamba and miscellaneous projectors. Throughout this step, the long sequential rationale is embedded into Meteor-Mamba through traversal of rationale by autoregressively generating rationale parts between the special tokens <tor>. By freezing Meteor-MLM, Meteor-Mamba seamlessly incorporates the rationale. In the second training step, we utilize the question-answer pairs in the curated 1.1M triples to jointly train Meteor-Mamba, Meteor-MLM, and the miscellaneous projectors. Here, multiple <tor> special tokens are only propagated to Meteor-Mamba. Then, the rationale-embedded features in Meteor-Mamba corresponding to the special tokens <tor> are only fed into Meteor-MLM, enabling it to adeptly answer complex questions, even in the absence of explicit rationale descriptions. In essence, these steps equip << Meteor with the capability to effectively address complex questions with the aid of the rationale.

## 4 Experiment

Implementation Detail.To ensure successful reproducibility, we outline three crucial technical details of < Meteor: the structure of (a) Meteor-Mamba and Meteor-MLM, (b) vision encoder and miscellaneous projectors, and (c) training and inference details.

(**a**) To build Meteor-Mamba, we use the Mamba architecture with 24 layers and a 768 hidden dimension, resulting in a total of 130M parameters, which is relatively trivial compared to the approximately 7B parameters of the pretrained InternLM2-7B [68; 69]. It is executed under the

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline LLVMs & Q-Bench & SQA1 & AI2D & ChartQA & SEED1 & POPE & HallB & MME & MathVista & MMB & MMB\({}^{\text{CN}}\) & MM-Vet & LLaVA\({}^{\text{W}}\) \\ \hline BLIP2-13B [90] & - & 61.0 & - & - & 46.4 & 85.3 & - & 1584 & - & - & -2.4 & - \\ Instrubkit-TB [9] & 56.7 & 60.5 & - & - & 53.4 & - & 53.6 & - & 25.3 & 36.0 & 23.9 & 26.2 & - \\ Instrubkit-TB [39] & - & 63.1 & - & - & - & 78.9 & - & - & - & 33.9 & - & 25.6 & - \\ IDEFCS-9B [57] & 51.5 & - & - & - & 74.6 & - & 1353 & 19.8 & 48.2 & 25.2 & 23.7 & - \\ Queen-VL-7B [76] & 59.4 & 67.1 & - & - & - & - & - & - & 38.2 & 7.4 & - & - \\ Queen-VL-Chat-7B [6] & 33.8 & 68.2 & - & - & 58.2 & - & 56.4 & 1849 & - & 60.6 & 56.7 & 47.3 & - \\ MinGPT-4-7B [58] & 51.8 & - & - & - & - & - & - & 23.1 & 23.0 & 11.9 & 22.1 & - \\ Outer-7B [89] & 47.2 & - & - & - & - & 72.5 & - & 1599 & 17.7 & 48.3 & - & 24.7 & - \\ LLaVA-7B [3] & - & 38.5 & - & - & - & 80.2 & 44.1 & 1055 & - & 34.1 & 14.1 & 26.7 & - \\ ILaVA1-7B [89] & 60.1 & 66.8 & - & 58.6 & 58.9 & 1805 & - & 64.3 & 58.3 & 30.5 & 63.4 & - \\ LLaVA1-513B [8] & 61.4 & 71.6 & 54.8 & 18.2 & 61.6 & 85.9 & 46.7 & 1826 & 27.6 & 67.7 & 63.6 & 35.4 & - \\ mPLUG-OV-7B [60] & 58.9 & - & - & - & - & - & - & - & 22.2 & 46.6 & - & - & - \\ mPLUG-OV-7B [19] & 62.9 & 68.7 & - & - & - & - & - & - & 64.5 & 60.3 & 36.2 & - \\ ShareGPT4-7B [7] & 63.4 & 68.4 & - & 69.7 & - & 49.8 & 1944 & 25.8 & 68.8 & 62.2 & 37.6 & - \\ InernLM-X-CT-7B [62] & 64.4 & - & - & 66.1 & - & 57.0 & 1919 & 29.5 & 74.4 & 72.4 & 35.2 & - \\ Monkey-10B [91] & - & 69.4 & - & - & 68.9 & 85.4 & 1924 & 34.8 & 72.4 & 67.5 & 33.0 & - \\ VLA-7B [92] & - & 68.2 & - & 61.1 & 85.5 & - & - & - & 68.9 & 61.7 & 34.9 & - \\ VLA-13B [92] & - & 73.7 & - & 62.8 & 84.2 & - & - & - & 70.3 & 64.3 & 38.8 & - \\ SPHINX-TB [93] & - & 70.6 & - & - & 71.6 & 86.9 & - & 1797 & 27.8 & 65.9 & 57.9 & 40.2 & - \\ SPHINX-Moc-7B-8 [94] & 66.2 & 70.6 & - & - & 73.0 & **89.6** & - & 1852 & 42.7 & 71.3 & - & 40.9 & - \\ SPHINX-Moc-7B-13B [94] & 66.2 & 70.6 & - & - & 74.8 & 89.1 & 52.1 & 1741 & 36.8 & 71.0 & - & 47.9 & - \\ SPHINX-Moc-7B-12B & - & 70.1 & - & 70.2 & 86.5 & - & 1851 & 34.6 & 69.6 & 63.3 & 43.9 & 72.3 \\ LLaVA-NeX-TB [12] & - & 71.6 & 69.5 & - & - & - & 1972 & 37.5 & 72.1 & - & - & 80.1 \\ LLaVA-NeXT-13B [12] & - & 73.6 & 70.0 & 62.2 & 72.2 & 86.7 & - & 1892 & 35.1 & 70.0 & 68.5 & 47.3 & 72.3 \\ MM-17B [14] & - & 72.6 & - & - & 69.9 & 86.6 & - & 1855 & 35.9 & 72.3 & - & 42.1 & - \\ MM-10B-7B-32 [14] & - & 74.4 & - & 70.9 & 87.8 & 1992 & 40.9 & 72.7 & - & 45.2 & - \\ MinGemini-HD-7B [10] & - & - & - & - & - & - & - & 1865 & 32.2 & 65.8 & - & 41.3 & - \\ MinGemini-HD-7B [10] & - & - & - & - & - & - & - & 1917 & 37.0 & 68.6 & - & 50.5 & - \\ \hline Meteor-7B & **69.2** & **88.3** & **77.9** & **74.9** & **75.0** & 88.7 & **60.4** & **2229** & **53.4** & **82.9** & **82.1** & **57.3** & **87.1** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison with the current existing open-source LLVMs, evaluating vision language performances of < Meteor on numerous evaluation benchmarks requiring diverse capabilities: Q-Bench [95], SQA1 [96], AI2D [38], ChartQA [80], SEED1 [97], POPE [98], HallB [99], MME [35], MathVista [37], MMB [36], MMB\({}^{\text{CN}}\)[36], MM-Vet [100], and LLaVAW [3]. Note that, AI2D and ChartQA performances for LLaVA family models are evaluated under zero-shot conditions, while < Meteor uses training dataset for them.

efficient computation of hardware-aware state expansion [39], where we borrow the tokenizer [103] from the backbone MLM to fit the language expression space in the backbone MLM. Meteor-MLM is based on InternLM2-7B [68, 69] with 32 layers and a 4096 hidden dimension.

**(b)** We use a vision encoder with 428M CLIP-L/14 [66], which has 24 layers and a 1024 hidden dimension. The resolution of the positional embedding is interpolated from \(24\times 24\) to \(35\times 35\) to accommodate a \(490\times 490\) image resolution. The vision projector involves an MLP that adapts the hidden dimension from 1024 to 4096 to fit that of the backbone MLM. Similarly, we build the tor projector to convey embedded rationales from Meteor-Mamba into Meteor-MLM, employing the same structure as the vision projector but transferring the hidden dimension from 768 to 4096.

**(c)** We train and evaluate \(\catsats\) Meteor in the following computing environment: Intel(R) Xeon(R) Gold 6230, 256 GB RAM, and 8\(\times\)NVIDIA RTX A6000 48GB VRAM. To efficiently train it, we use one epoch of training for each training step under 4-bit quantization and bfloat16 data type [104] for Meteor-MLM, where double quantization and normalized float 4-bit (nf4) [105] are used. Meteor-Mamba uses float32 data type because training it with bfloat16 or float16 has been reported to produce an unstable learning process. In addition, QLoRA [106, 105] is used to train Meteor-MLM, with 64 rank and 64 alpha parameters. We use the AdamW [107] optimizer and schedule the learning rate by cosine annealing [108] from 1e-4 to 1e-6 in each training step, with gradient checkpointing [109] applied to Meteor-MLM for efficient memory management. With a gradient accumulation of 6, we set batch sizes of 192 and 576 for each training step, and each step takes approximately three days. For efficient inference, \(\cats\) Meteor is validated in 4-bit quantization, and we use deterministic beam search (\(n=3\))[110] for text generation. Note that we implement not only Meteor-MLM but also numerous baselines under the efficient propagation from FlashAttention2[111, 112].

Evaluation.We have evaluated \(\cats\) Meteor on numerous vision-language benchmarks, the details of which are described in Appendix B. These benchmarks require multifaceted information for diverse capabilities, including fundamental image understanding, real-world knowledge of common-sense knowledge, charts, diagrams, documents, signs, symbols, math problems, and more. Figure 1-2 and Table 1 illustrates vision language performances of various LLVMs, including Meteor-7B, open-, and closed-source LLVMs with various sizes. It is noteworthy that Meteor-7B noticeably outperforms the other models, demonstrating its efficacy and efficiency in using embedded multifaceted rationales from Meteor-Mamba. The detailed generation quality of \(\cats\) Meteor is described in Appendix C. Apart from the results in Table 1, those in Table 2 signify that Meteor-7B also excels at more challenging benchmarks, which require multifaceted information simultaneously. Meteor-7B has outperformed

\begin{table}

\end{table}
Table 2: Detailed comparison of \(\cats\) Meteor across more challenging evaluation benchmarks.

other existing models by a large margin, some of which are equipped with additional vision encoders or computer vision models, demonstrating that rationales can provide multifaceted information more effectively than enhanced visual perception.

Ablation Studies.Furthermore, we have conducted several ablation studies to securely corroborate the effectiveness of our proposed method in light of six factors: (a) Meteor-Mamba, (b) Meteor-MLM, (c) the number of <tor> special tokens, (d) the distribution of <tor> special tokens, (e) rationale, and (f) 1.1M question-rationale-answer triples. Table 3 shows the following findings. Note that, Appendix D represents further ablation studies.

(**a**) Once Meteor-Mamba is replaced with other transformer-based models: BERT [113], GPT2 [114], and XLNet [115], we have discovered that the Mamba architecture takes advantage of its efficiency for embedding multifaceted rationales in terms of both computational complexity and model size. As the results suggest, Mamba demonstrates the highest batches-per-second (BPS) value and zero-shot performances on MME and MMB benchmarks among architectures of similar sizes, enabled by its inherent computational efficiency based on linear complexity and strong long sequence modeling capability [39], which other Transformer-based model architectures lack.

(**b**) We have tried using various pretrained LLMs of comparable sizes for Meteor-MLM, in order to identify the effectiveness of embedding multifaceted rationale together with traversal of rationale. We have observed that InternLM2 [69] has shown the best performances.

(**c**) Varying the number of <tor> special tokens from 2 to 15, we have optimized it based on the vision language performances. The results suggest that using 10 <tor> special tokens shows the best performances for embedding abundant multifaceted rationales, balancing between compression and information preservation.

(**d**) The performances of \(\mathcal{A}\) Meteor depend on the distribution of <tor> special tokens when Meteor-Mamba is trained to embed multifaceted rationales. Given the observations, evenly distributing the tokens across lengthy rationales has shown the best performances. Prepending them to lengthy rationales may hinder effective embedding due to forgetting nature, and appending them to rationales in the end may not be guaranteed to understand the rationale. Randomly distributing the tokens across rationales may disrupt Meteor-Mamba's ability to stably learn the pattern of embedding rationales. Conversely, evenly distributed <tor> special tokens can segment lengthy rationales into shorter chunks and progressively embed them in a consistent manner, avoiding the issues of other distributions.

(**e**) In order to prove the effectiveness of multifaceted rationales through Meteor-Mamba, we ablated the use of the Mamba architecture and the rationales. The first row represents baseline performances where backbone MLM is only trained. For the second row, we only train backbone MLM with the curated multifaceted rationales without Meteor-Mamba, and for the third row, Mamba has been trained to embed answer instead of the rationales. Compared to the last row where Meteor is evaluated, the second and third rows fall short of performances, clearly showing that using multifaceted rationales through the Mamba architecture has contributed to performance improvement.

(**f**) As another way of showing the significance of multifaceted rationales, we have trained \(\mathcal{A}\) Meteor with different amounts of question-rationale pairs and evaluated \(\mathcal{A}\) Meteor trained with each of them.

\begin{table}

\end{table}
Table 3: Ablation studies to identify the effectiveness of Meteor-Mamba and rationale through traversal of rationale by controlling the six main factors.

As expected, the more question-rationale pairs used in first training step, the better performances achieves, demonstrating the significance of utilizing multifaceted rationales for diverse capabilities.

Meteor-Mamba's Ability to Embed Rationales.We conduct a thorough analysis to confirm that Meteor-Mamba effectively embeds the rationales. To do this, we perform a retrieval task for multifaceted rationales, where we prepare ten different question-rationale pairs \((\mathcal{Q}_{i},\mathcal{R}_{i})\) where \(i=0,1,\cdots,9\). These pairs are propagated through Meteor-Mamba with or without rationales under <tor> special tokens. This results in two sets of output features: one with rationale \(z_{i}^{\text{w.o.}}\) and one without rationales \(z_{j}^{\text{w.o.}}\), with \(j=0,1,\cdots,9\). We extract features corresponding to the placement of <tor> tokens, resulting in \(\mathbf{z}_{i}^{\text{w.o.}}\in\mathbb{R}^{10\times 768}\) and \(\mathbf{z}_{j}^{\text{w.o.}}\in\mathbb{R}^{10\times 768}\), where the dimension 10 corresponds to the number of <tor> tokens. We then compute the cosine similarity between \(\mathbf{z}_{i}^{\text{w.o.}}\) and \(\mathbf{z}_{j}^{\text{w.o.}}\) to measure the similarity of their representations. As illustrated in Figure 4, the diagonal values in the cosine similarity matrix are much higher than the off-diagonal values. This result indicates that Meteor-Mamba successfully embeds the rationale, and its output features contain multifaceted information even without explicit rationales in natural language. This explains how Meteor-Mamba operates effectively during the inference phase without explicit rationales.

Discussion and Limitation.From the experimental results observed, we gain the insight that equipping LLVMs with a multifaceted rationale is a key factor in building efficient LLVMs that demonstrate impressive vision language performances across numerous evaluation benchmarks requiring diverse capabilities. This rationale, furthermore, naturally reduces hallucination effects in POPE [98] and HallusionBench [99] in Table 1. Additionally, Table 2(c)-(d) shows that the need for additional vision encoders and computer vision models can be mitigated by incorporating a multifaceted rationale. However, \(\cG\) Meteor might still be considered inefficient in terms of model size by users without high-end GPU resources, as it requires at least multiple GPUs with 48GB and 32GB VRAM for normal training and inference without (Q)LoRA [106; 105] and 4/8-bit quantization. Although many closed-source LLVMs have demonstrated superior performances following the scaling law [16], our goal is to reduce the model size while maintaining vision language performances as much as possible. We strongly believe that small language and vision models, even those with about 1\(\sim\)3B parameters, can effectively narrow the performance gap with the closed-source LLVMs by using layer-analyzing approaches such as mixture of depths [116] and others [117; 118; 119; 120; 121; 122; 123], despite their inherent limitation in layer number and hidden dimension.

## 5 Conclusion

To build efficient LLVMs, we incorporate a multifaceted rationale encompassing various aspects such as image understanding, incorporating external common-sense knowledge, understanding non-object concepts (_e.g.,_ charts, diagrams, symbols, signs, and math), and following systematic step-by-step procedures for how to solve complex questions. \(\cG\) Meteor demonstrates significantly enhanced vision language performances across various evaluation benchmarks without the need to scale up LLVMs, use additional vision encoders, or employ multiple computer vision models. In designing \(\cG\) Meteor, the traversal of rationale combined with Mamba architecture proves highly effective in embedding lengthy rationales. We believe this rationale, facilitated by the traversal of rationale, can pave the way for more efficient models, representing a promising step towards achieving more efficient LLVMs.

Figure 4: Illuminating how the feature correspondences of cosine similarity are computed under the trained Meteor-Mamba, and showing the feature disparity for <tor> with/without rationale.

## Acknowledgments and Disclosure of Funding

This work was partially supported by two funds: Center for Applied Research in Artificial Intelligence (CARAI) grant funded by DAPA and ADD (UD230017TD) and IITP grant funded by the Korea government (MSIT) (RS-2022-II220984).

## References

* [1] J. Wei, M. Bosma, V. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le, "Finetuned language models are zero-shot learners," in _International Conference on Learning Representations_, 2022.
* [2] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma, _et al._, "Scaling instruction-finetuned language models," _arXiv preprint arXiv:2210.11416_, 2022.
* [3] H. Liu, C. Li, Q. Wu, and Y. J. Lee, "Visual instruction tuning," in _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [4] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat, _et al._, "Gpt-4 technical report," _arXiv preprint arXiv:2303.08774_, 2023.
* [5] G. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, _et al._, "Gemini: a family of highly capable multimodal models," _arXiv preprint arXiv:2312.11805_, 2023.
* [6] J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou, "Qwen-vl: A frontier large vision-language model with versatile abilities," _arXiv preprint arXiv:2308.12966_, 2023.
* [7] L. Chen, J. Li, X. Dong, P. Zhang, C. He, J. Wang, F. Zhao, and D. Lin, "Sharegpt4v: Improving large multi-modal models with better captions," _arXiv preprint arXiv:2311.12793_, 2023.
* [8] H. Liu, C. Li, Y. Li, and Y. J. Lee, "Improved baselines with visual instruction tuning," _arXiv preprint arXiv:2310.03744_, 2023.
* [9] W. Dai, J. Li, D. Li, A. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, "InstructBLIP: Towards general-purpose vision-language models with instruction tuning," in _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [10] Y. Li, Y. Zhang, C. Wang, Z. Zhong, Y. Chen, R. Chu, S. Liu, and J. Jia, "Mini-gemini: Mining the potential of multi-modality vision language models," _arXiv preprint arXiv:2403.18814_, 2024.
* [11] W. Wang, Q. Lv, W. Yu, W. Hong, J. Qi, Y. Wang, J. Ji, Z. Yang, L. Zhao, X. Song, _et al._, "Cogvlm: Visual expert for pretrained language models," _arXiv preprint arXiv:2311.03079_, 2023.
* [12] H. Liu, C. Li, Y. Li, B. Li, Y. Zhang, S. Shen, and Y. J. Lee, "Llava-next: Improved reasoning, ocr, and world knowledge," January 2024.
* [13] A. Young, B. Chen, C. Li, C. Huang, G. Zhang, G. Zhang, H. Li, J. Zhu, J. Chen, J. Chang, _et al._, "Yi: Open foundation models by 01. ai," _arXiv preprint arXiv:2403.04652_, 2024.
* [14] B. McKinzie, Z. Gan, J.-P. Fauconnier, S. Dodge, B. Zhang, P. Dufter, D. Shah, X. Du, F. Peng, F. Weers, _et al._, "Mm1: Methods, analysis & insights from multimodal llm pre-training," _arXiv preprint arXiv:2403.09611_, 2024.
* [15] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, Z. Muyan, Q. Zhang, X. Zhu, L. Lu, _et al._, "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks," _arXiv preprint arXiv:2312.14238_, 2023.

* [16] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei, "Scaling laws for neural language models," _arXiv preprint arXiv:2001.08361_, 2020.
* [17] Y. Tay, M. Dehghani, J. Rao, W. Fedus, S. Abnar, H. W. Chung, S. Narang, D. Yogatama, A. Vaswani, and D. Metzler, "Scale efficiently: Insights from pre-training and fine-tuning transformers," _arXiv preprint arXiv:2109.10686_, 2021.
* [18] B. Li, P. Zhang, J. Yang, Y. Zhang, F. Pu, and Z. Liu, "Otterhd: A high-resolution multi-modality model," _arXiv preprint arXiv:2311.04219_, 2023.
* [19] Q. Ye, H. Xu, J. Ye, M. Yan, H. Liu, Q. Qian, J. Zhang, F. Huang, and J. Zhou, "mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration," _arXiv preprint arXiv:2311.04257_, 2023.
* [20] A. Hu, H. Xu, J. Ye, M. Yan, L. Zhang, B. Zhang, C. Li, J. Zhang, Q. Jin, F. Huang, _et al._, "mplug-docowl 1.5: Unified structure learning for ocr-free document understanding," _arXiv preprint arXiv:2403.12895_, 2024.
* [21] R. Xu, Y. Yao, Z. Guo, J. Cui, Z. Ni, C. Ge, T.-S. Chua, Z. Liu, M. Sun, and G. Huang, "Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images," _arXiv preprint arXiv:2403.11703_, 2024.
* [22] O. F. Kar, A. Tonioni, P. Poklukar, A. Kulhrestha, A. Zamir, and F. Tombari, "Brave: Broadening the visual encoding of vision-language models," _arXiv preprint arXiv:2404.07204_, 2024.
* [23] H. Lu, W. Liu, B. Zhang, B. Wang, K. Dong, B. Liu, J. Sun, T. Ren, Z. Li, Y. Sun, _et al._, "Deepseek-vl: towards real-world vision-language understanding," _arXiv preprint arXiv:2403.05525_, 2024.
* [24] E. Goncharova, A. Razzhigaev, M. Mikhalchuk, M. Kurkin, I. Abdullaeva, M. Skripkin, I. Oseledets, D. Dimitrov, and A. Kuznetsov, "Omnifusion technical report," _arXiv preprint arXiv:2404.06212_, 2024.
* [25] M. Ranzinger, G. Heinrich, J. Kautz, and P. Molchanov, "Am-radio: Agglomerative model-reduce all domains into one," _arXiv preprint arXiv:2312.06709_, 2023.
* [26] Y. Fang, W. Wang, B. Xie, Q. Sun, L. Wu, X. Wang, T. Huang, X. Wang, and Y. Cao, "Eva: Exploring the limits of masked visual representation learning at scale," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 19358-19369, 2023.
* [27] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, _et al._, "Dinov2: Learning robust visual features without supervision," _arXiv preprint arXiv:2304.07193_, 2023.
* [28] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo, _et al._, "Segment anything," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 4015-4026, 2023.
* [29] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer, "Sigmoid loss for language image pre-training," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 11975-11986, 2023.
* [30] B. Chen, Z. Xu, S. Kirmani, B. Ichter, D. Driess, P. Florence, D. Sadigh, L. Guibas, and F. Xia, "Spatialvlm: Endowing vision-language models with spatial reasoning capabilities," _arXiv preprint arXiv:2401.12168_, 2024.
* [31] W. Wang, Y. Ren, H. Luo, T. Li, C. Yan, Z. Chen, W. Wang, Q. Li, L. Lu, X. Zhu, _et al._, "The all-seeing project v2: Towards general relation comprehension of the open world," _arXiv preprint arXiv:2402.19474_, 2024.
* [32] Q. Jiao, D. Chen, Y. Huang, Y. Li, and Y. Shen, "Enhancing multimodal large language models with vision detection models: An empirical study," _arXiv preprint arXiv:2401.17981_, 2024.

* [33] B.-K. Lee, B. Park, C. W. Kim, and Y. M. Ro, "Collavo: Crayon large language and vision model," _arXiv preprint arXiv:2402.11248_, 2024.
* [34] B.-K. Lee, B. Park, C. W. Kim, and Y. M. Ro, "Moai: Mixture of all intelligence for large language and vision models," _arXiv preprint arXiv:2403.07508_, 2024.
* [35] C. Fu, P. Chen, Y. Shen, Y. Qin, M. Zhang, X. Lin, J. Yang, X. Zheng, K. Li, X. Sun, _et al._, "Mme: A comprehensive evaluation benchmark for multimodal large language models," _arXiv preprint arXiv:2306.13394_, 2023.
* [36] Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, _et al._, "Mmbench: Is your multi-modal model an all-around player?," _arXiv preprint arXiv:2307.06281_, 2023.
* [37] P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao, "Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts," _arXiv preprint arXiv:2310.02255_, 2023.
* [38] A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi, "A diagram is worth a dozen images," in _Computer Vision-ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part IV 14_, pp. 235-251, Springer, 2016.
* [39] A. Gu and T. Dao, "Mamba: Linear-time sequence modeling with selective state spaces," _arXiv preprint arXiv:2312.00752_, 2023.
* [40] J. Gao, R. Pi, J. Zhang, J. Ye, W. Zhong, Y. Wang, L. Hong, J. Han, H. Xu, Z. Li, _et al._, "G-llava: Solving geometric problem with multi-modal large language model," _arXiv preprint arXiv:2312.11370_, 2023.
* [41] K. Wang, J. Pan, W. Shi, Z. Lu, M. Zhan, and H. Li, "Measuring multimodal mathematical reasoning with math-vision dataset," _arXiv preprint arXiv:2402.14804_, 2024.
* [42] X. Yue, X. Qu, G. Zhang, Y. Fu, W. Huang, H. Sun, Y. Su, and W. Chen, "Mammoth: Building math generalist models through hybrid instruction tuning," _arXiv preprint arXiv:2309.05653_, 2023.
* [43] X. Yue, T. Zheng, G. Zhang, and W. Chen, "Mammoth2: Scaling instructions from the web," 2024.
* [44] Anthropic, "The claude 3 model family: Opus, sonnet, haiku." https://www.anthropic.com, 2024.
* [45] J. Strout, Y. Zhang, and R. Mooney, "Do human rationales improve machine explanations?," in _Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP_, pp. 56-62, 2019.
* [46] J. Lu, L. Yang, B. Namee, and Y. Zhang, "A rationale-centric framework for human-in-the-loop machine learning," in _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 6986-6996, 2022.
* [47] C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y. Fujii, A. Ratner, R. Krishna, C.-Y. Lee, and T. Pfister, "Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes," _arXiv preprint arXiv:2305.02301_, 2023.
* [48] P. Wang, Z. Wang, Z. Li, Y. Gao, B. Yin, and X. Ren, "Scott: Self-consistent chain-of-thought distillation," in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 5546-5558, 2023.
* [49] W. Xiong, Y. Song, P. Wang, and S. Li, "Rationale-enhanced language models are better continual relation learners," in _Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing_, pp. 15489-15497, 2023.

* [50] L. H. Li, J. Hessel, Y. Yu, X. Ren, K.-W. Chang, and Y. Choi, "Symbolic chain-of-thought distillation: Small models can also "think" step-by-step," in _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 2665-2679, 2023.
* [51] M. Kang, S. Lee, J. Baek, K. Kawaguchi, and S. J. Hwang, "Knowledge-augmented reasoning distillation for small language models in knowledge-intensive tasks," _Advances in Neural Information Processing Systems_, vol. 36, 2024.
* [52] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, _et al._, "Chain-of-thought prompting elicits reasoning in large language models," _Advances in neural information processing systems_, vol. 35, pp. 24824-24837, 2022.
* [53] Z. Zhang, A. Zhang, M. Li, and A. Smola, "Automatic chain of thought prompting in large language models," _arXiv preprint arXiv:2210.03493_, 2022.
* [54] K. Shum, S. Diao, and T. Zhang, "Automatic prompt augmentation and selection with chain-of-thought from labeled data," in _Findings of the Association for Computational Linguistics: EMNLP 2023_, pp. 12113-12139, 2023.
* [55] S. Krishna, J. Ma, D. Slack, A. Ghandeharioun, S. Singh, and H. Lakkaraju, "Post hoc explanations of language models can improve language models," _Advances in Neural Information Processing Systems_, vol. 36, 2024.
* [56] K. Chen, Z. Zhang, W. Zeng, R. Zhang, F. Zhu, and R. Zhao, "Shikra: Unleashing multimodal llm's referential dialogue magic," _arXiv preprint arXiv:2306.15195_, 2023.
* [57] H. Laurencon, L. Saulnier, L. Tronchon, S. Bekman, A. Singh, A. Lozhkov, T. Wang, S. Karamcheti, A. M. Rush, D. Kiela, _et al._, "Obelisc: An open web-scale filtered dataset of interleaved image-text documents," _arXiv preprint arXiv:2306.16527_, 2023.
* [58] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, "Minigpt-4: Enhancing vision-language understanding with advanced large language models," _arXiv preprint arXiv:2304.10592_, 2023.
* [59] B. Li, Y. Zhang, L. Chen, J. Wang, J. Yang, and Z. Liu, "Otter: A multi-modal model with in-context instruction tuning," _arXiv preprint arXiv:2305.03726_, 2023.
* [60] Q. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, _et al._, "mplug-owl: Modularization empowers large language models with multimodality," _arXiv preprint arXiv:2304.14178_, 2023.
* [61] X. Contributors, "Xtuner: A toolkit for efficiently fine-tuning llm." https://github.com/InternLM/xtuner, 2023.
* [62] P. Zhang, X. D. B. Wang, Y. Cao, C. Xu, L. Ouyang, Z. Zhao, S. Ding, S. Zhang, H. Duan, H. Yan, _et al._, "Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition," _arXiv preprint arXiv:2309.15112_, 2023.
* [63] Z. Chen, W. Wang, H. Tian, S. Ye, Z. Gao, E. Cui, W. Tong, K. Hu, J. Luo, Z. Ma, _et al._, "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites," _arXiv preprint arXiv:2404.16821_, 2024.
* [64] G. H. Chen, S. Chen, R. Zhang, J. Chen, X. Wu, Z. Zhang, Z. Chen, J. Li, X. Wan, and B. Wang, "Allava: Harnessing gpt4v-synthesized data for a lite vision-language model," _arXiv preprint arXiv:2402.11684_, 2024.
* [65] Z. Zong, B. Ma, D. Shen, G. Song, H. Shao, D. Jiang, H. Li, and Y. Liu, "Mova: Adapting mixture of vision experts to multimodal context," _arXiv preprint arXiv:2404.13046_, 2024.
* [66] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, "Learning transferable visual models from natural language supervision," in _Proceedings of the 38th International Conference on Machine Learning_ (M. Meila and T. Zhang, eds.), vol. 139 of _Proceedings of Machine Learning Research_, pp. 8748-8763, PMLR, 18-24 Jul 2021.

* [67] D. Hendrycks and K. Gimpel, "Gaussian error linear units (gelus)," _arXiv preprint arXiv:1606.08415_, 2016.
* [68] I. Team, "Internlm: A multilingual language model with progressively enhanced capabilities." https://github.com/InternLM/InternLM-techreport, 2023.
* [69] Z. Cai, M. Cao, H. Chen, K. Chen, K. Chen, X. Chen, X. Chen, Z. Chen, Z. Chen, P. Chu, _et al._, "Internlm2 technical report," _arXiv preprint arXiv:2403.17297_, 2024.
* [70] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, _et al._, "Training language models to follow instructions with human feedback," _Advances in Neural Information Processing Systems_, vol. 35, pp. 27730-27744, 2022.
* [71] C. Schuhmann, R. Beaumont, R. Vencu, C. Gordon, R. Wightman, M. Cherti, T. Coombes, A. Katta, C. Mullis, M. Wortsman, _et al._, "Laion-5b: An open large-scale dataset for training next generation image-text models," _Advances in Neural Information Processing Systems_, vol. 35, pp. 25278-25294, 2022.
* [72] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, "Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 3558-3568, 2021.
* [73] B. Saleh and A. Elgammal, "Large-scale classification of fine-art paintings: Learning the right metric on the right feature," _arXiv preprint arXiv:1505.00855_, 2015.
* [74] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick, "Microsoft coco: Common objects in context," in _Computer Vision-ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13_, pp. 740-755, Springer, 2014.
* [75] O. Sidorov, R. Hu, M. Rohrbach, and A. Singh, "Textcaps: a dataset for image captioning with reading comprehension," in _Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part II 16_, pp. 742-758, Springer, 2020.
* [76] V. Ordonez, G. Kulkarni, and T. Berg, "Im2text: Describing images using 1 million captioned photographs," _Advances in neural information processing systems_, vol. 24, 2011.
* [77] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes, J. Jitsev, and A. Komatsuzaki, "Laion-400m: Open dataset of clip-filtered 400 million image-text pairs," _arXiv preprint arXiv:2111.02114_, 2021.
* [78] P. Sharma, N. Ding, S. Goodman, and R. Soricut, "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning," in _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pp. 2556-2565, 2018.
* [79] M. Mathew, D. Karatzas, and C. Jawahar, "Docvqa: A dataset for vqa on document images," in _Proceedings of the IEEE/CVF winter conference on applications of computer vision_, pp. 2200-2209, 2021.
* [80] A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque, "Chartqa: A benchmark for question answering about charts with visual and logical reasoning," _arXiv preprint arXiv:2203.10244_, 2022.
* [81] K. Kafle, B. Price, S. Cohen, and C. Kanan, "Dvqa: Understanding data visualizations via question answering," in _Proceedings of the IEEE conference on computer vision and pattern recognition_, pp. 5648-5656, 2018.
* [82] S. Svetlichnaya, "Deepform: Understand structured documents at scale," 2020.
* [83] M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. Jawahar, "Infographicvqa," in _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_, pp. 1697-1706, 2022.

* [84] T. Stanislawek, F. Gralinski, A. Wroblewska, D. Lipinski, A. Kaliska, P. Rosalska, B. Topolski, and P. Biecek, "Kleister: key information extraction datasets involving long documents with complex layouts," in _International Conference on Document Analysis and Recognition_, pp. 564-579, Springer, 2021.
* [85] W. Chen, H. Wang, J. Chen, Y. Zhang, H. Wang, S. Li, X. Zhou, and W. Y. Wang, "Tabfact: A large-scale dataset for table-based fact verification," _arXiv preprint arXiv:1909.02164_, 2019.
* [86] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach, "Towards vqa models that can read," in _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pp. 8317-8326, 2019.
* [87] P. Pasupat and P. Liang, "Compositional semantic parsing on semi-structured tables," in _Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_ (C. Zong and M. Strube, eds.), (Beijing, China), pp. 1470-1480, Association for Computational Linguistics, July 2015.
* [88] R. Tanaka, K. Nishida, and S. Yoshida, "Visualmrc: Machine reading comprehension on document images," in _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 35, pp. 13878-13888, 2021.
* [89] M. Yasunaga, A. Aghajanyan, W. Shi, R. James, J. Leskovec, P. Liang, M. Lewis, L. Zettlemoyer, and W.-t. Yih, "Retrieval-augmented multimodal language modeling," _arXiv preprint arXiv:2211.12561_, 2022.
* [90] J. Li, D. Li, S. Savarese, and S. Hoi, "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models," _arXiv preprint arXiv:2301.12597_, 2023.
* [91] Z. Li, B. Yang, Q. Liu, Z. Ma, S. Zhang, J. Yang, Y. Sun, Y. Liu, and X. Bai, "Monkey: Image resolution and text label are important things for large multi-modal models," _arXiv preprint arXiv:2311.06607_, 2023.
* [92] J. Lin, H. Yin, W. Ping, Y. Lu, P. Molchanov, A. Tao, H. Mao, J. Kautz, M. Shoeybi, and S. Han, "Vila: On pre-training for visual language models," _arXiv preprint arXiv:2312.07533_, 2023.
* [93] Z. Lin, C. Liu, R. Zhang, P. Gao, L. Qiu, H. Xiao, H. Qiu, C. Lin, W. Shao, K. Chen, _et al._, "Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models," _arXiv preprint arXiv:2311.07575_, 2023.
* [94] P. Gao, R. Zhang, C. Liu, L. Qiu, S. Huang, W. Lin, S. Zhao, S. Geng, Z. Lin, P. Jin, _et al._, "Sphinx-x: Scaling data and parameters for a family of multi-modal large language models," _arXiv preprint arXiv:2402.05935_, 2024.
* [95] H. Wu, Z. Zhang, E. Zhang, C. Chen, L. Liao, A. Wang, C. Li, W. Sun, Q. Yan, G. Zhai, _et al._, "Q-bench: A benchmark for general-purpose foundation models on low-level vision," _arXiv preprint arXiv:2309.14181_, 2023.
* [96] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan, "Learn to explain: Multimodal reasoning via thought chains for science question answering," _Advances in Neural Information Processing Systems_, vol. 35, pp. 2507-2521, 2022.
* [97] B. Li, R. Wang, G. Wang, Y. Ge, Y. Ge, and Y. Shan, "Seed-bench: Benchmarking multimodal llms with generative comprehension," _arXiv preprint arXiv:2307.16125_, 2023.
* [98] Y. Li, Y. Du, K. Zhou, J. Wang, W. X. Zhao, and J.-R. Wen, "Evaluating object hallucination in large vision-language models," _arXiv preprint arXiv:2305.10355_, 2023.
* [99] F. Liu, T. Guan, Z. Li, L. Chen, Y. Yacoob, D. Manocha, and T. Zhou, "Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), Ilava-1.5, and other multi-modality models," _arXiv preprint arXiv:2310.14566_, 2023.

* [100] W. Yu, Z. Yang, L. Li, J. Wang, K. Lin, Z. Liu, X. Wang, and L. Wang, "Mm-vet: Evaluating large multimodal models for integrated capabilities," _arXiv preprint arXiv:2308.02490_, 2023.
* [101] L. Chen, J. Li, X. Dong, P. Zhang, Y. Zang, Z. Chen, H. Duan, J. Wang, Y. Qiao, D. Lin, _et al._, "Are we on the right way for evaluating large vision-language models?," _arXiv preprint arXiv:2403.20330_, 2024.
* [102] R. Zhang, D. Jiang, Y. Zhang, H. Lin, Z. Guo, P. Qiu, A. Zhou, P. Lu, K.-W. Chang, P. Gao, _et al._, "Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?," _arXiv preprint arXiv:2403.14624_, 2024.
* [103] T. Kudo and J. Richardson, "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing," _arXiv preprint arXiv:1808.06226_, 2018.
* [104] D. Kalamkar, D. Mudigere, N. Mellempudi, D. Das, K. Banerjee, S. Avancha, D. T. Vooturi, N. Jammalamadaka, J. Huang, H. Yuen, _et al._, "A study of bfloat16 for deep learning training," _arXiv preprint arXiv:1905.12322_, 2019.
* [105] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, "Qlora: Efficient finetuning of quantized llms," _arXiv preprint arXiv:2305.14314_, 2023.
* [106] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, "Lora: Low-rank adaptation of large language models," _arXiv preprint arXiv:2106.09685_, 2021.
* [107] I. Loshchilov and F. Hutter, "Decoupled weight decay regularization," in _International Conference on Learning Representations_, 2019.
* [108] I. Loshchilov and F. Hutter, "Sgdr: Stochastic gradient descent with warm restarts," _arXiv preprint arXiv:1608.03983_, 2016.
* [109] N. S. Sohoni, C. R. Aberger, M. Leszczynski, J. Zhang, and C. Re, "Low-memory neural network training: A technical report," _arXiv preprint arXiv:1904.10631_, 2019.
* [110] M. Freitag and Y. Al-Onaizan, "Beam search strategies for neural machine translation," in _Proceedings of the First Workshop on Neural Machine Translation_ (T. Luong, A. Birch, G. Neubig, and A. Finch, eds.), (Vancouver), pp. 56-60, Association for Computational Linguistics, Aug. 2017.
* [111] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Re, "Flashattention: Fast and memory-efficient exact attention with io-awareness," _Advances in Neural Information Processing Systems_, vol. 35, pp. 16344-16359, 2022.
* [112] T. Dao, "Flashattention-2: Faster attention with better parallelism and work partitioning," _arXiv preprint arXiv:2307.08691_, 2023.
* [113] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, "Bert: Pre-training of deep bidirectional transformers for language understanding," _arXiv preprint arXiv:1810.04805_, 2018.
* [114] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, _et al._, "Language models are unsupervised multitask learners,"
* [115] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and Q. V. Le, "Xlnet: Generalized autoregressive pretraining for language understanding," _Advances in neural information processing systems_, vol. 32, 2019.
* [116] D. Raposo, S. Ritter, B. Richards, T. Lillicrap, P. C. Humphreys, and A. Santoro, "Mixture-of-depths: Dynamically allocating compute in transformer-based language models," _arXiv preprint arXiv:2404.02258_, 2024.
* [117] B.-K. Lee, Y. Yu, and Y. M. Ro, "Towards adversarial robustness of bayesian neural network through hierarchical variational inference," 2021.
* [118] J. Kim, B.-K. Lee, and Y. M. Ro, "Distilling robust and non-robust features in adversarial examples by information bottleneck," _Advances in Neural Information Processing Systems_, vol. 34, pp. 17148-17159, 2021.

* [119] B.-K. Lee, J. Kim, and Y. M. Ro, "Masking adversarial damage: Finding adversarial saliency for robust and sparse network," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 15126-15136, 2022.
* [120] J. Kim, B.-K. Lee, and Y. M. Ro, "Demystifying causal features on adversarial examples and causal inoculation for robust network by adversarial instrumental variable regression," in _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pp. 12302-12312, 2023.
* [121] B.-K. Lee, J. Kim, and Y. M. Ro, "Mitigating adversarial vulnerability through causal parameter estimation by adversarial double machine learning," in _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pp. 4499-4509, 2023.
* [122] J. Kim, B.-K. Lee, and Y. M. Ro, "Causal unsupervised semantic segmentation," _arXiv preprint arXiv:2310.07379_, 2023.
* [123] Y. Kim, J. Kim, B.-K. Lee, S. Shin, and Y. M. Ro, "Mitigating dataset bias in image captioning through clip confounder-free captioning network," in _2023 IEEE International Conference on Image Processing (ICIP)_, pp. 1720-1724, IEEE, 2023.
* [124] G. Luo, Y. Zhou, Y. Zhang, X. Zheng, X. Sun, and R. Ji, "Feast your eyes: Mixture-of-resolution adaptation for multimodal large language models," _arXiv preprint arXiv:2403.03003_, 2024.
* [125] H. Zhao, M. Zhang, W. Zhao, P. Ding, S. Huang, and D. Wang, "Cobra: Extending mamba to multi-modal large language model for efficient inference," _arXiv preprint arXiv:2403.14520_, 2024.
* [126] Y. Qiao, Z. Yu, L. Guo, S. Chen, Z. Zhao, M. Sun, Q. Wu, and J. Liu, "Vl-mamba: Exploring state space models for multimodal learning," _arXiv preprint arXiv:2403.13600_, 2024.
* [127] J. Liu, M. Liu, Z. Wang, L. Lee, K. Zhou, P. An, S. Yang, R. Zhang, Y. Guo, and S. Zhang, "Robomamba: Multimodal state space model for efficient robot reasoning and manipulation," _arXiv preprint arXiv:2406.04339_, 2024.
* [128] W. Huang and J. Hu, "Ml-mamba: Efficient multi-modal large language model utilizing mamba-2," _arXiv preprint arXiv:2407.19832_, 2024.

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_EMPTY:19]

[MISSING_PAGE_EMPTY:20]

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_EMPTY:22]

[MISSING_PAGE_EMPTY:23]

Evaluation benchmarks

* **Q-Bench**[95] aims to assess the low-level visual capabilities of Multi-modality Large Language Models (MLLMs). This dataset is divided into three main sections: perception, description, and assessment. The low-level visual perception component evaluates MLLMs' ability to recognize and understand low-level image attributes. The low-level visual description component tests how accurately and thoroughly MLLMs can describe these attributes. Finally, the overall visual quality assessment examines how closely MLLMs' evaluations align with human judgments of image quality. Altogether, the Q-Bench dataset encompasses 81,284 samples across these tasks.
* **SQA-IMG (SQA\({}^{\textbf{I}}\))**[96] is a portion of the ScienceQA (SQA) dataset, which serves as a comprehensive multimodal benchmark aimed at enhancing the reasoning capabilities and interpretability of AI systems in the realm of science question answering. The dataset spans a variety of science subjects, encompassing 26 distinct topics from natural science, social science, and language science, with annotated answers including lectures and explanations. This specific subset consists of samples with image context, totaling 10,332 question-answer pairs.
* **AI2D**[38], also known as AI2 Diagrams, was developed to tackle the challenges of diagram interpretation and reasoning, emphasizing syntactic parsing and semantic analysis of diagrams. Its goal is to support research in uncovering the structure of diagrams and understanding the meanings of their elements and their interrelations. This dataset is especially beneficial for tasks like diagram question answering, where comprehensive understanding and reasoning about the content are essential. The collection consists of over 5,000 diagrams from grade school science subjects and more than 15,000 multiple-choice questions.
* **ChartQA**[80] is created to assess and enhance question answering systems that involve complex, multi-step logical and visual reasoning with charts. This dataset meets the need for systems capable of interpreting various data visualizations, including bar charts, line charts, and pie charts, and addressing questions that require arithmetic and logical processing. It encompasses question-answer pairs that are both human-authored and machine-generated, emphasizing visual and logical reasoning. The dataset comprises a total of 32,719 samples.
* **SEED-IMG (SEED\({}^{\textbf{I}}\))**[97] is a component of SEED-Bench designed to assess the generative comprehension skills of multimodal large language models (MLLMs). This thorough and unbiased benchmark enables researchers to evaluate and compare various models' abilities in both spatial and temporal understanding. The dataset is organized into several subsets according to 12 evaluation dimensions that encompass spatial and temporal comprehension across image and video modalities. SEED-IMG specifically focuses on the image modality subset.
* **POPE**[98] is a technique created to systematically assess the propensity of LLVMs to hallucinate objects that do not exist in the target images. This approach transforms the hallucination evaluation into a binary classification task via polling questions, providing a consistent, equitable, and adaptable evaluation process.
* **HallusionBench (HallB)**[99] is crafted to assess and analyze both visual illusions and knowledge hallucinations in large language and vision models (LLVMs). This dataset targets the identification of potential failure modes in these models by utilizing meticulously created example pairs for thorough testing. The benchmark includes a variety of visual-question pairs, encompassing both visual dependent subsets (such as illusion, math, etc.) and visual supplement subsets (such as chart, table, map, OCR). HallusionBench comprises 346 distinct images and an extensive collection of 1129 questions distributed across diverse topics and formats.
* **MME**[35] is created to serve as a thorough evaluation benchmark for Multimodal Large Language Models (MLLMs). The goal is to assess their abilities in perception and cognition through 14 distinct sub-tasks, including coarse-grained recognition, fine-grained recognition, OCR, and commonsense reasoning, among others. It strives to address the shortcomings of current evaluation methods, ensuring comprehensive testing of MLLMs across various dimensions while preventing data leakage.
* **MathVista**[37] serves as an extensive benchmark aimed at assessing mathematical reasoning within visual contexts. This dataset merges visual comprehension, allowing for a thorough evaluation of AI models' capabilities in tackling mathematical problems that involve visual elements. It comprises three subsets: IQTest, FunctionQA, and PaperQA, with an aggregate of 6,141 examples.
* **MMB, MMB-Chinese (MMBCN)**[36] aims to deliver a thorough and resilient evaluation standard for vision language models by encompassing a broad spectrum of capabilities (20 distinct fine-grained abilities) necessary for multimodal comprehension in both English and Chinese. This benchmark facilitates the evaluation of various facets of LLVMs, including their perceptual and reasoning skills across multiple tasks. The benchmark comprises a total of 3,217 carefully curated questions, sourced from a variety of places, including public datasets and the internet, to ensure a wide range of skills.
* **MM-Vet**[100] is designed to systematically assess LMMs by evaluating their proficiency in handling intricate tasks that necessitate the combination of multiple VL abilities. Unlike existing benchmarks that generally focus on simpler tasks involving only one or two abilities, MM-Vet encompasses six fundamental VL capabilities: recognition (Rec), knowledge (Know), OCR, spatial awareness (Spat), language generation (Gen), and math. MM-Vet includes tasks that integrate these six core capabilities in various combinations, leading to 16 distinct capability integrations. The dataset consists of 200 images sourced from various online platforms and includes 218 questions that require one or more of these capabilities to answer.
* **LLaVA Bench in the Wild (LLaVAW)**[3] is a dataset designed to assess the performance of large multimodal models (LMM) in tackling more complex tasks and adapting to new domains. It features a varied collection of 24 images, accompanied by 60 questions, covering indoor and outdoor environments, memes, artworks, sketches, and more. Each image is paired with a detailed, manually-curated description and thoughtfully chosen questions.
* **MMStar**[101] strives to deliver an accurate assessment of LLVMs' authentic multi-modal abilities. It achieves this by guaranteeing that each sample genuinely depends on visual content to arrive at correct answers, while also minimizing inadvertent data leakage. The benchmark is organized into six primary competencies, such as instance reasoning, logical reasoning, and mathematics, and includes a total of 1,500 meticulously curated samples.
* **MathVerse**[102] is designed to evaluate the visual mathematical reasoning capabilities of Multi-modal Large Language Models (MLLMs). The dataset focuses on whether and how well MLLMs can interpret visual diagrams and mathematical problems, rather than relying solely on textual information. The problems in MathVerse are divided into three main subjects (plane geometry, solid geometry, and functions) and further categorized into twelve subfields (length, area, volume, coordinate, etc.) to provide a well-rounded evaluation of different mathematical skills, adding up to 2,612 high-quality visual math problems.

[MISSING_PAGE_EMPTY:26]

**User:** What's the name of the restaurant serving these dishes?

**Meteor:** The restaurant serving these dishes appears to be klorian Ramen, a popular Japanese ramen chain known for its tonkotsu (pork bone broth) ramen. The distinctive black bowits with ornate gold and red designs are a signature feature of klorian's branding.

**User:** What's the best weather, season, time of the day of visiting this place? Is the time when this photo was taken a good time to visit this place?

**Meteor:** Based on the clear blue skies and sunny conditions in the image, it appears to be a beautiful day to visit the Space Needle in Seattle. The Space Needle is an iconic observation tower that offers stunning 360-degree views of the city and surrounding landscape.

The best weather for visiting the Space Needle would be on a clear, sunny day like the one shown in the photo. Ideally, you'd want to avoid foggy or overcast conditions that could limit visibility from the observation decks.

As for the season, spring and summer are generally the most popular times to visit Seattle due to the milder temperatures and longer daylight hours. However, the Space Needle is an attraction that can be enjoyed year-round, as long as the weather cooperates.

The time of day shown in the photo appears to be midday or early afternoon, based on the bright sunlight and shadows. This is a great time to visit the Space Needle, as the views are typically clearest and most vibrant during the middle of the day when the sun is high in the sky.

So in summary, a clear, sunny day like the one in the photo is ideal for visiting the Space Needle. Midday to early afternoon is also a good time to go for the best views. Spring and summer are the most popular seasons, but the Space Needle can be enjoyed year-round if the weather cooperates.

**User:** What is the problem this city might be facing? What are some possible solutions?

**Meetor:** The image shows a heavily congested highway at night with bumper-to-bumper traffic in both directions. This suggests that the city is facing a major traffic congestion problem.

Possible solutions to alleviate traffic congestion include:

1. Improving public transportation options to encourage more people to use buses, trains, and subways instead of driving.
2. Encouraging carpooling and ridesharing to reduce the number of single-occupancy vehicles on the road.
3. Implementing congestion pricing, where drivers are charged a fee to enter congested areas during peak hours, to discourage unnecessary driving.
4. Expanding bike lanes and pedestrian walkways to make it safer and more convenient for people to walk or bike instead of drive.
5. Investing in better highway infrastructure, such as adding more lanes, improving on-ramps and off-ramps, and implementing smart traffic management systems to optimize traffic flow.
6. Encouraging flexible work hours and remote work options to reduce rush hour traffic.
7. Implementing stricter emissions standards and promoting the use of electric and hybrid vehicles to reduce air pollution and congestion caused by older, less fuel-efficient vehicles.

By implementing a combination of these solutions, the city can work towards reducing traffic congestion, improving air quality, and enhancing the overall quality of life for its residents.

**User:**: Can you explain this meme?
**Meteor:**: This meme is pooking fun at the fact that the names of the countries Iceland and Greenland are misleading. Despite its name, Iceland is known for its beautiful green landscapes, while Greenland is mostly covered in ice and snow. The meme is saying that the person has trust issues because the names of these countries do not accurately represent their landscapes.
**User:**: The table below gives information about the underground railway systems in six cities.

Summarise the information by selecting and reporting the main features, and make comparisons where relevant. You should write at least 150 words.
**Meteor:**: The table shows the underground railway systems in six major cities around the world, including London, Paris, Tokyo, Washington DC, Kyoto, and Los Angeles.

London's underground railway system opened in 1863 and is the oldest in the world. It has a route length of 394 km and carries 775 million passengers per year.

Paris' metro system opened in 1900 and is the second oldest in the world. It has a route length of 199 km and carries 1,191 million passengers per year.

Tokyo's subway system opened in 1927 and is the busiest in the world. It has a route length of 155 km and carries 1,927 million passengers per year.

Washington DC's metro system opened in 1976 and has a route length of 126 km. It carries 144 million passengers per year.

Kyoto's subway system opened in 1981 and has the shortest route length of 11 km. It carries 45 million passengers per year.

Los Angeles' metro system opened in 2001 and has a route length of 28 km. It carries 50 million passengers per year.

In summary, Tokyo has the busiest underground railway system in the world, while Kyoto has the shortest route length. London's system is the oldest and Paris' is the second oldest. Washington DC's metro system has the lowest passenger count among the major cities listed.

**User:** Can you give a short introduction to this painting?

**Meteor:** This painting is a portrait of a young woman wearing a blue headscar, often referred to as "Girl with a Pearl Earting" or "The Jewish Bride". The painting is a masterpiece by the Dutch artist Johannes Vermeer, painted in the mid-17th century. The woman's epistemic expression and the dramatic use of light and shadow are hallmarks of Vermeer's style. The painting has become one of the most famous and recognizable works of art in the world.

**User:** What are all the French scene text in the image?

**Meteor:** The French scene text in the image is "Histoire de France" which translates to "History of France" in English. The subtitle "De la Prehistoire a nos jours " translates to "From prehistory to our days".

[MISSING_PAGE_EMPTY:32]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Abstract and Introduction section Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Discussion and Limitation section Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: None of theoretical assumptions Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Meteor and Experiment section Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification: You can see the link in Abstract section.

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Experiment section Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Training Meteor with only one epoch under 4-bit quantized-extreme condition proves its effectiveness beyond checking error bar. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [No] Justification: Performances of LLVMs are not dependent on trivial hyper-parameter but dependent with propagation strategy, model architectures, and dataset. Many researcher for LLVMs have known that LLVMs can be instruction-tuned, at least, with multiple of GPUs such as NVIDIA RTX A6000 and A100. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We used publicly common visual instruction tuning dataset. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Discussion and Limitation section Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We used publicly common visual instruction tuning dataset and pre-trained visual foundation models and large language models. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We cited models and datasets we deal with in this paper. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: Meteor and Experiment section Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: None of any crowdsourcing and research with human subjects Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: None of any crowdsourcing and research with human subjects Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.