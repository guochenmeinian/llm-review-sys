# Time-Varying LoRA: Towards Effective Cross-Domain Fine-Tuning of Diffusion Models

 Zhan Zhuang\({}^{1,2,}\)1  Yulong Zhang\({}^{3,}\)1  Xuehao Wang\({}^{1}\)

Jiangang Lu\({}^{3}\)  Ying Wei\({}^{3,}\)2  Yu Zhang\({}^{1,}\)2

\({}^{1}\)Southern University of Science and Technology

\({}^{2}\)City University of Hong Kong \({}^{3}\)Zhejiang University

12250063@mail.sustech.edu.cn {zhangylcse, lujg, ying.wei}@zju.edu.cn

{xuehaowangfi, yu.zhang.ust}@gmail.com

Equal contribution.Corresponding authors.

Footnote 1: footnotemark:

Footnote 2: footnotemark:

###### Abstract

Large-scale diffusion models are adept at generating high-fidelity images and facilitating image editing and interpolation. However, they have limitations when tasked with generating images in dynamic, evolving domains. In this paper, we introduce Terra, a novel Time-varying low-rank adapter that offers a fine-tuning framework specifically tailored for domain flow generation. The key innovation of Terra lies in its construction of a continuous parameter manifold through a time variable, with its expressive power analyzed theoretically. This framework not only enables interpolation of image content and style but also offers a generation-based approach to address the domain shift problems in unsupervised domain adaptation and domain generalization. Specifically, Terra transforms images from the source domain to the target domain and generates interpolated domains with various styles to bridge the gap between domains and enhance the model generalization, respectively. We conduct extensive experiments on various benchmark datasets, empirically demonstrate the effectiveness of Terra. Our source code is publicly available on https://github.com/zwebzone/terra.

## 1 Introduction

Recently, text-to-image diffusion models [38, 47, 48, 45] have revolutionized computer vision by synthesizing high-quality, creative images. Those models provide a user-friendly method for generating images through text prompts. Furthermore, with advancements in fine-tuning techniques of diffusion models [4], users can easily customize [83], edit [27], and interpolate [88, 80, 7] images. A common approach involves using a low-rank adapter (LoRA) [25] to fine-tune diffusion models with a few images to generate customized images. This inspires a generation-based approach to address a fundamental and classical problem in machine learning known as domain shift.

Domain shift is commonly studied in the cross-domain learning [70, 82, 61] with two settings: unsupervised domain adaptation (UDA) [40, 12, 94], which aims to transfer knowledge from a source domain to a target domain, and domain generalization (DG) [89, 64], which focuses on training a model on source domains and then generalizing to unseen target domains. Prior methods [91, 15, 68, 90, 73] have demonstrated the effectiveness of image translation and interpolation on the learning paradigms based on mixup [79, 60], generative adversarial networks [16, 92], and diffusion models [24, 36]. Considering the impressive capabilities of diffusion models and the efficiency of fine-tuning techniques like LoRA, it is natural to extend them to generate domain flow, which generates intermediate domains and bridges the source and target domains, as illustrated in Fig. 1(b).

However, previous methods [36; 80] require multiple LoRAs to customize multiple domains, since a single LoRA cannot effectively express knowledge of multiple domains with a plugin [77]. To address this limitation, as illustrated in Fig. 1(a), we propose a Time-varying low-rank adapter (Terra), which offers a framework for gradual domain transferring by constructing a continuous parameter manifold. Instead of training multiple LoRAs for different domains, Terra maintains the parameter efficiency. To this end, inspired by the perspective of dynamic flows [75], Terra introduces a time variable \(t\) for each domain and incorporates a square matrix that varies with time \(t\) within the original low-rank structure.

As depicted in Fig. 1(b), Terra enables the use of different time values \(t\) for various intermediate domains. Consequently, Terra can generate intermediate images that are natural and smooth when morphing in image pairs, subjects, and styles. For UDA tasks, we generate target samples and transform the source samples into the target domain to form an expanded source domain. Due to the smaller domain shifts, transferring from the expanded source domain to the target domain can improve the performance of existing UDA methods. For DG tasks, we interpolate among all source domains to generate images in various styles. Then, the generated samples are combined with the source domain images to improve the performance of existing DG methods.

In summary, our contributions are four-fold:

* We introduce Terra, a novel framework that integrates a square matrix with a time variable \(t\) into the original low-rank structure, facilitating effective and flexible knowledge sharing across different domains while maintaining parameter efficiency.
* We provide a theoretical analysis of the expressive power of Terra, comparing it to LoRA.
* We demonstrate the application of Terra in image transformation and generation for UDA tasks and image interpolation for DG tasks via Terra, respectively.
* Extensive experiments validate the effectiveness of Terra across various tasks, including generative interpolation, unsupervised domain adaptation, and domain generalization.

## 2 Related Work

**Fine-Tuning of Text-to-Image Diffusion Models.** The impressive performance of diffusion models [24; 55] has sparked a surge of interest in text-to-image generation tasks. As the demand for personalized content synthesis grows [83], pioneer works such as Textual Inversion [11] and Dream-Booth [49] have proposed optimized text embedding and full fine-tuning frameworks to generate subject images with limited reference samples. Recently, several parameter-efficient methods for fine-tuning diffusion modules have been proposed, including adapters [54], LoRA [17; 50; 52], singular value decomposition on weight matrices [20], subsets of cross-attention [56; 32], and image prompt adapter [81; 76; 37; 66]. Among those methods, several have been developed to address the challenges of multi-concept generation [32; 20; 17] and natural image interpolation [62; 30; 80; 88]. Different from those methods, Terra focuses on generation and interpolation within domain flows.

Figure 1: Illustration of the proposed Terra.

**Domain Adaptation and Generalization.** UDA [74; 34; 13; 67; 87; 63] is designed to address the challenge of adapting models trained on labeled source domains to unlabeled target domains. The central premise of UDA methods is to learn domain-invariant features that minimize the domain gap. UDA approaches primarily fall into two branches: discrepancy-based methods [34; 72; 93; 19] and adversarial-based methods [13; 46; 85]. Conversely, DG [64; 89] seeks to train models that could generalize well to unseen target domains using multiple source domains. Effective DG methods, such as SWAD [5] and SAGM [65] enhance the generalization by identifying and leveraging flatter minima of training losses landscapes. However, the performance of UDA and DG methods can be constrained by the availability of training data. To address this limitation, recent data augmentation techniques [73; 71; 84; 36] have been developed to improve the transfer effects of UDA and DG methods. Those methods can be categorized into feature-level [71; 95; 42] and image-level methods [73; 84; 36; 22], which enhance transfer performance through the transformation or generation of auxiliary samples at the feature and image levels. For instance, MSGD [71] and GGF [95] use intermediate domains to gradually reduce the domain shift between the source and target domains, while BDG [73] employs pairs of cross-domain generators to synthesize domain-specific data based on the other domains. Additionally, CDGA [22] leverages the latent diffusion model to generate synthetic samples across domains and Domaindiff [36] trains LoRAs for each source domain to conduct domain fusion.

## 3 Methodology

### Preliminary

LoRA [25] uses two low-rank matrices, \(\bm{W}_{\text{down}}\in\mathbb{R}^{r\times n}\) and \(\bm{W}_{\text{up}}\in\mathbb{R}^{m\times r}\), where \(r\ll\min(m,n)\), to compute the weight matrix updates \(\Delta\bm{W}=\bm{W}_{\text{up}}\bm{W}_{\text{down}}\in\mathbb{R}^{m\times n}\). The forward pass of the new weights changes from \(h=\bm{W}_{0}\bm{x}\) to:

\[h=\bm{W}_{0}\bm{x}+\alpha\Delta\bm{W}\bm{x}=\bm{W}_{0}\bm{x}+\alpha\bm{W}_{ \text{up}}\bm{W}_{\text{down}}\bm{x},\] (1)

where \(\alpha\) is a scaling factor for the magnitude of the changes applied to the original weights. Although LoRA is primarily used for fine-tuning large language models, it is also employed in diffusion models for personalizing image generators with limited training samples, targeting specific styles or subjects [49; 52; 80]. The objective function in previous studies is expressed as noise matching:

\[\mathcal{L}(\Delta\bm{\theta})=\mathbb{E}_{\bm{x}_{0},\tau\sim\mathcal{U}(1,T),c,\epsilon\sim\mathcal{N}(0,1)}\left[\left\|\epsilon-\epsilon_{\bm{\theta}_{0 }+\Delta\bm{\theta}}\left(\bm{x}_{\tau},\tau,e(c)\right)\right\|_{2}^{2}\right],\] (2)

where \(\bm{\theta}_{0}\) and \(\Delta\bm{\theta}\) denote the parameters of the text-to-image diffusion model and LoRA, respectively. The function \(e\) denotes the text encoder, and \(c\) corresponds to the text prompt. During the forward diffusion process, the variable \(\bm{x}_{\tau}\) is obtained by gradually adding noise to the initial image \(\bm{x}_{0}\) using the equation \(\bm{x}_{\tau}=\sqrt{\bar{\alpha}_{\tau}}\bm{x}_{0}+\sqrt{1-\bar{\alpha}_{\tau}}\epsilon\). Here \(\alpha_{\tau}\) follows a decreasing schedule, and \(\bar{\alpha}_{\tau}\) is calculated as the cumulative product of \(\alpha\) values up to timestep \(\tau\). In the objection function, the timestep \(\tau\) is sampled from a uniform distribution \(\mathcal{U}(1,T)\), where \(T\) denotes the total number of timesteps. And the model is utilized to predict the noise \(\epsilon_{\bm{\theta}_{0}+\Delta\bm{\theta}}\) to estimate the true noise \(\epsilon\). After training, the well-trained denoiser \(\bm{\theta}_{0}+\Delta\bm{\theta}\) can denoise noises and generate images within a few sampling steps.

### Terra: Time-Varying Low-Rank Adapter

To address the need for fine-tuning diffusion models across multiple domains while maintaining the parameter efficiency, we propose the Terra, as depicted in Fig. 1(a). Terra involves constructing a LoRA flow that provides a parameter manifold by incorporating time-varying updates as

\[h(t)=\bm{W}_{0}\bm{x}+\Delta\bm{W}(t)\bm{x}=\bm{W}_{0}\bm{x}+\bm{W}_{\text{up }}\mathcal{K}(t)\bm{W}_{\text{down}}\bm{x},\ \ \mathcal{K}(t)=\mathcal{F}(\bm{W}_{\text{ mid}},t)\] (3)

where \(\bm{W}_{\text{mid}}\in\mathbb{R}^{r\times r}\), \(t\) is a one-parameter variable, and \(\mathcal{F}\) is a time-dependent function. This formulation enables the differentiable evolution of the parameters \(\Delta\bm{W}(t)\) based on a middle time-varying matrix \(\mathcal{K}(t)\). A simple form of \(\mathcal{F}(\bm{W},t)\) is \(t\bm{W}+\bm{I}\), where \(\bm{I}\) represents an identity matrix. Since \(r\ll\min(m,n)\), the parameter difference between Terra and LoRA with the same rank is negligible. Furthermore, by setting the parameter \(t\) to 0, Terra will degenerate to LoRA. It is worth noting that the form \(\mathcal{F}(\bm{W},t)\) here is just one of the possible variations. More forms can be found in Table 5 of Appendix B and a comparison with MoE-based LoRA [69] is provided in Appendix E.

Here, we present a theoretical analysis of the expression power of the proposed Terra. We define \(\bm{I}_{r}\) as a diagonal matrix with its first \(r\) diagonal entries as 1 and the remaining entries as 0. In the following theorem, we prove that Terra can effectively implement two LoRAs for specific downstream tasks by constructing a parameter manifold with reduced parameters.

**Theorem 1**.: (The Equivariance between Terra and Multiple LoRAs) _Assume there exist two LoRAs \(\Delta\bm{W}_{A},\Delta\bm{W}_{B}\in\mathbb{R}^{m\times n}\) with ranks of \(p\) and \(q\), respectively, that effectively solve two specific downstream tasks. Let \(k=\max\{\operatorname{rank}([\Delta\bm{W}_{A}\ \Delta\bm{W}_{B}]), \operatorname{rank}([\Delta\bm{W}_{A}^{T}\ \Delta\bm{W}_{B}^{T}])\}\), where \(\operatorname{rank}(\cdot)\) denotes the rank of a matrix. Then, there exists a Terra with \(\bm{W}_{\textit{up}}\in\mathbb{R}^{m\times k}\), \(\bm{W}_{\textit{down}}\in\mathbb{R}^{k\times n}\), \(\bm{W}_{\textit{mid}}\in\mathbb{R}^{k\times k}\), and \(\mathcal{K}(t)=t\bm{W}_{\textit{mid}}+\bm{I}_{r}\), such that the updated matrix \(\Delta\bm{W}(t)=\bm{W}_{\textit{up}}\mathcal{K}(t)\bm{W}_{\textit{down}}\), can simultaneously solve the two downstream task, that is, we have \(\Delta\bm{W}(0)=\Delta\bm{W}_{A}\) and \(\Delta\bm{W}(1)=\Delta\bm{W}_{B}\)._

In Theorem 1, the number of trainable parameters of Terra is governed by \(|\Theta|=(m+n)k+k^{2}\), contrasting with that of two LoRAs \(|\Theta|=(m+n)(p+q)\). Note that \(k\) represents the maximum rank of the matrices obtained by concatenating the row and column spaces of the two LoRA matrices, which is not greater than the sum of the ranks of the two LoRA matrices, _i.e._, \(k\leq p+q\).

Drawing inspiration from prior research on the expressive power of LoRA [78], we further demonstrate the expressive power of Terra. Here, we focus on the multi-layer feedforward neural network with identity activation functions, and the analysis can be extended to fully connected neural networks and transformer networks [78]. Assuming that the target models \(\bar{f}_{A}\) and \(\bar{f}_{B}\) for two specific tasks, as well as the frozen model \(f_{0}\), are linear, they can be represented as:

\[\bar{f}_{A}(\bm{x})=\overline{\bm{W}}_{A}\bm{x},\quad\bar{f}_{B}(\bm{x})= \overline{\bm{W}}_{B}\bm{x},\quad f_{0}(\bm{x})=\bm{W}_{L}\cdots\bm{W}_{1}\bm {x}=\left(\prod_{l=1}^{L}\bm{W}_{l}\right)\bm{x},\]

where the frozen model has \(L\) layers with consistent dimensions. We define the error matrices \(\bm{E}_{A}:=\overline{\bm{W}}_{A}-\prod_{l=1}^{L}\bm{W}_{l}\), and \(\bm{E}_{B}:=\overline{\bm{W}}_{B}-\prod_{l=1}^{L}\bm{W}_{l}\), and their ranks as \(R_{\bm{E}_{A}}=\operatorname{rank}(\bm{E}_{A})\) and \(R_{\bm{E}_{B}}=\operatorname{rank}(\bm{E}_{B})\). By utilizing Terra \(\Delta\bm{W}(t)\), we can modify the pre-trained frozen model to closely approximate the two target models \(\overline{\bm{W}}_{A}\) and \(\overline{\bm{W}}_{B}\). We denote the \(d\)-th largest singular value of \(\bm{W}\) by \(\sigma_{d}(\bm{W})\), and the best rank-\(r\) approximation [8] of \(\bm{W}\) by \(\operatorname{LR}_{r}(\bm{W})\). The following theorem presents an upper bound for the approximation error with a rank-\(k\) Terra.

**Theorem 2**.: (The Expressive Power of Terra) _For each layer \(l\), the rank-\(k\) Terra has updated matrix \(\Delta\bm{W}(t)_{l}\), and the function of time-varying matrix is \(\mathcal{K}(t)_{l}=t\bm{W}_{\textit{mid},l}+\tilde{\bm{I}}\). Assume that all weight matrices of the frozen model \(\left(\bm{W}_{l}\right)_{l=1}^{L}\), \(\prod_{l=1}^{L}\bm{W}_{l}+\operatorname{LR}_{r}(\bm{E}_{A})\), and \(\prod_{l=1}^{L}\bm{W}_{l}+\operatorname{LR}_{r}(\bm{E}_{B})\) are non-singular for all \(r\leq k(L-1)\). Then the approximation error satisfies_

\[\min_{\Delta\bm{W}(t)}\left(\left\left\|\prod_{l=1}^{L}(\bm{W}_{l}+\Delta\bm{W} (0)_{l})-\overline{\bm{W}}_{A}\right\|_{2}+\left\|\prod_{l=1}^{L}(\bm{W}_{l}+ \Delta\bm{W}(1)_{l})-\overline{\bm{W}}_{B}\right\|_{2}\right)\leq 2\sigma_{kL+1}^{*},\] (4)

_where the \(\sigma_{kL+1}^{*}\) as the \((kL+1)\)-th largest singular values obtained by merging the singular values of \(\bm{E}_{A}\) and \(\bm{E}_{B}\). Moreover, when \(k\geq\left\lceil\frac{R_{\bm{E}_{A}}+R_{\bm{E}_{B}}}{L}\right\rceil\), the approximation error is zero._

We compare the approximation errors of Terra and multiple LoRAs with consistent parameter sizes for the above target models. We consider a rank of \(2k\) for Terra and two \(k\)-rank LoRA in both tasks. Prior work [78] establishes an upper bound on LoRA's approximation error as \(\sigma_{kL+1}(\bm{E}_{A})+\sigma_{kL+1}(\bm{E}_{B})\). In Theorem 2, we demonstrate that Terra's approximation error bound is \(2\sigma_{2kL+1}^{*}\). Considering the definition of \(\sigma^{*}\), it is evident that our Terra's error bound is not greater than LoRA's.

Terra is capable of cross-domain generative tasks, where samples from different domains possess different \(t\)'s. In the following sections, we show the use of Terra in three different learning problems.

### Warm Up: Constructing Evolving Visual Domains via Terra

In this section, we show the first application of Terra to construct evolving visual domains for generative interpolation between two image domains \(\mathcal{D}_{S}\) and \(\mathcal{D}_{T}\) characterized by the differences in the style or subject, which is the key to apply Terra to UDA and DG.

Our method is different from existing methods [53; 55; 88; 80] that employ direct interpolation between two images on embedding using spherical linear interpolation (_a.k.a_ slerp). To accomplish this, Terra incorporates a continuous time variable \(t\). Training on the source images involves setting \(t\) to 0, yielding the formulation \(\Delta\bm{W}(0)=\bm{W}_{\text{up}}\mathcal{K}(0)\bm{W}_{\text{down}}\). Similarly, for the target images, \(t\) is set to 1, leading to \(\Delta\bm{W}(1)=\bm{W}_{\text{up}}\mathcal{K}(1)\bm{W}_{\text{down}}\). In the context of fine-tuning text-to-image diffusion models, we employ image descriptions to construct prompts for diffusion models, where the corresponding class label is denoted by "A [class]", where "[class]" denotes the placeholder for the class label. Finally, the training objective, as depicted in Fig. 2, is formulated as follows

\[\begin{split}\mathcal{L}(\Delta\bm{\theta})=\mathbb{E}_{\bm{ \epsilon}\sim\mathcal{N}(0,1),\tau\sim\mathcal{U}(1,T)}\big{[}& \mathbb{E}_{\bm{x}_{0}^{S}\sim\mathcal{D}_{S},t=0}\left\|\epsilon- \epsilon_{\bm{\theta}_{0}+\Delta\bm{\theta}}\left(\bm{x}_{\tau}^{S},\tau,e(c ^{S}),t\right)\right\|_{2}^{2}\\ +&\mathbb{E}_{\bm{x}_{0}^{T}\sim\mathcal{D}_{T},t=1 }\left\|\epsilon-\epsilon_{\bm{\theta}_{0}+\Delta\bm{\theta}}\left(\bm{x}_{ \tau}^{T},\tau,e(c^{T}),t\right)\right\|_{2}^{2}\big{]},\end{split}\] (5)

where \(\Delta\bm{\theta}\) represents the parameters of the Terra, \(c^{S}\) and \(c^{T}\) denote the text prompts for the source and target, and \(x_{0}^{S}\) and \(x_{0}^{T}\) represent the source and target samples. Formally, we construct evolving visual domains by the following two stages: (1) Fine-tune the parameters of Terra (i.e., \(\Delta\bm{\theta}=W_{up}\cup W_{mid}\cup W_{down}\)) using Eq. (5), where the first part with \(t=0\) uses source samples \(\mathcal{D}_{S}\) and the second part with \(t=1\) uses target samples \(\mathcal{D}_{T}\). (2) Generate an intermediate domain by uniformly sampling \(t\) from \([0,1]\) and inputting the text prompt and a random noise into the fine-tuned diffusion model corresponding to domain \(t\) for the backward process.

### Generation-based Unsupervised Domain Adaptation via Terra

Built on the first application introduced in the previous section, we introduce the second application of Terra in UDA. Under the UDA setting, we have a labeled source domain \(\mathcal{D}_{S}\) and an unlabeled target domain \(\mathcal{D}_{T}\). To alleviate domain shifts, we propose a two-stage framework utilizing a generation-based approach to augment the source domain.

Similar to the construction of evolving domains discussed in Section 3.3, the first stage sets out to train the parameters of Terra that accommodate source domain generation with \(t=0\) and target domain generation with \(t=1\). This enables the generation of target images according to the class labels and transitive source images into the target domain. However, due to the polysemous words on the class labels, directly generating images with the text prompt may cause unexpected results. For example, "mouse" usually refers to a rodent, but in some datasets, it refers to a computer mouse. Therefore, we leverage the source samples to conduct semantic alignment between images and class labels while the unlabeled target domain samples contribute to learning style information for fine-tuning the diffusion model. To achieve this, we adopt the same objective function as Eq. (5), where we set \(t=0\) for source training with the prompt "A [class]" and \(t=1\) for target training with the prompt "An image".

The second stage involves synthesizing a transitive source domain that can benefit the learning of UDA methods, as depicted in Fig. 3(a). We employ two approaches to achieve this. First, we set \(t=1\) to synthesize target samples from Gaussian noises for each category with the corresponding prompt, _i.e._, "A [class]". Those synthesized samples constitute a generated target domain denoted by \(\mathcal{D}_{\hat{T}}\). Second, we transform the source samples into the target domain while preserving semantic

Figure 2: The illustration of the training process of constructing evolving visual domains via Terra.

information. This is achieved by first setting \(t=0\) and applying DDIM inversion [55] to convert the source images into noise. Then, with setting \(t=1\), we use the diffusion model equipped with Terra to denoise, resulting in the adapted source domain \(\mathcal{D}_{\hat{S}}\). After generating images, we combine the adapted source domain and the generated target domain to form a transitive source domain \(\mathcal{D}_{E}=\mathcal{D}_{\hat{S}}\cup\mathcal{D}_{\hat{T}}\). Here the transitive source domain could have a smaller domain gap to the target domain than the original source domain due to the generation process, which could facilitate the knowledge transfer from the transitive source domain to the target domain.

Finally, we conduct transfer learning from the transitive source domain to the target domain by using an existing UDA method. The objective function is formulated as

\[\hat{f}_{uda}=\operatorname*{arg\,min}_{f}\frac{1}{|\mathcal{D}_{E}|}\sum_{( \bm{x},y)\in\mathcal{D}_{E}}\ell_{ce}(f(\bm{x}),y)+\beta\ell_{uda}(\mathcal{D} _{E},\mathcal{D}_{T}),\] (6)

where \(\ell_{ce}(\cdot,\cdot)\) denotes the cross-entropy loss, \(\beta>0\) is a trade-off parameter, and \(\ell_{uda}(\cdot,\cdot)\) is a transfer loss (_e.g._, domain discrepancy loss [34, 72, 93] and domain discrimination loss [13, 46, 85]) used to alleviate the domain shift. In this manner, our method can be integrated with any off-the-shelf UDA methods to enhance the transfer performance.

### Generation-based Domain Generalization via Terra

In this section, we study the application of Terra to DG problems. Under the DG setting, we have \(K\) source domains \(\{\mathcal{D}_{k}=\{(\bm{x}_{i}^{k},y_{i}^{k})\}_{i=1}^{n_{k}}\}_{k=1}^{K}\), where \(n_{k}\) denotes the number of samples in \(\mathcal{D}_{k}\). To enhance the generalization capability, as shown in Fig. 3(b) and detailed as follows, Terra is adopted to synthesize new source domains by interpolating among existing source domains. Consequently, we expect a more generalized learner that well adapts to both existing and synthesized source domains.

In the first stage, to accommodate the various styles exhibited by multiple source domains, we utilize a network \(g(\cdot)\) to predict sample-level \(\bm{t}\) for the Terra. The \(\bm{t}\)-predictor \(g(\cdot)\) aims to generate similar \(\bm{t}\) values for images from the same domain. Moreover, due to the diverse range of styles in the training set, each \(\bm{t}=g(\bm{x})\) is represented as a vector instead of a scalar value used in previous settings. This allows us to better capture various styles and intra-domain differences. Specifically, we train the network \(g(\cdot)\) via contrastive learning and the loss function to be minimized is formulated as

\[\mathcal{L}_{con}(g)=\sum_{k=1}^{K}\sum_{i=1}^{n_{k}}\left(\sum_{ \begin{subarray}{c}j=1\\ j\neq i\end{subarray}}^{n_{k}}\lVert g(\bm{x}_{i}^{k})-g(\bm{x}_{j}^{k}) \rVert_{2}+\sum_{\begin{subarray}{c}l=1\\ l\neq k\end{subarray}}^{K}\sum_{m=1}^{n_{l}}\max(0,\delta-\lVert g(\bm{x}_{i}^ {k})-g(\bm{x}_{m}^{l})\rVert_{2})\right),\] (7)

where \(\delta\) is a predefined positive margin and \(\lVert\cdot\rVert_{2}\) denotes the Euclidean distance. In Eq. (7), the first term in the sum is to enforce samples from the same domain yield similar outputs, while the second term is to encourage the distance between the outputs corresponding to samples from two domains to be larger than the margin via the hinge loss.

Figure 3: The illustration of the proposed generation-based UDA and DG frameworks via Terra.

Upon learning of the \(g(\cdot)\), we can obtain \(\bm{t}\)'s for all the samples in all the source domains. Then based on \(\bm{t}\)'s, we fine-tune the diffusion model using Terra with the prompt "A [class]", and the training objective is formulated as

\[\mathcal{L}(\Delta\bm{\theta})=\mathbb{E}_{\epsilon\sim\mathcal{N}(0,1),\tau \sim\mathcal{U}(1,T)}\left[\sum_{k=1}^{K}\mathbb{E}_{\bm{x}_{0}\sim\mathcal{D} _{k},\bm{t}=g(\bm{x}_{0})}\left\|\epsilon-\epsilon_{\bm{\theta}_{0}+\Delta\bm{ \theta}}\left(\bm{x}_{\tau},\tau,e(c),\bm{t}\right)\right\|_{2}^{2}\right].\] (8)

After fine-tuning, in the second stage, we set \(\bm{t}\) to various values to generate diverse samples for each category with the corresponding prompt. The generated samples could originate from various domains which may be beyond the original source domains \(\{\mathcal{D}_{k}\}_{k=1}^{K}\) but we do not need to identify their specific domains. We combine these generated samples with the original source domain samples to form expanded domains \(\mathcal{D}_{E}\), which can improve the generalization capability of models. The objective function of DG based on Terra is formulated as

\[\hat{f}_{dg}=\operatorname*{arg\,min}_{f}\frac{1}{|\mathcal{D}_{E}|}\sum_{( \bm{x},y)\in\{\mathcal{D}_{E}\}}\ell_{ce}(f(\bm{x}),y)+\beta\ell_{dg}( \mathcal{D}_{E}),\] (9)

where \(\beta>0\) is a trade-off parameter, and \(\ell_{dg}(\cdot)\) is a domain generalization loss (_e.g._, Sharpness-Aware Minimization (SAM)-based loss [10; 5; 65] and representation learning-based loss [1; 13; 2]) used to improve the generalization capabilities. In this manner, our method can be integrated with any off-the-shelf DG methods to enhance their performance.

## 4 Experiments

### Experimental Setups

For the UDA experiments, we utilize three benchmark datasets, including _Office31_[5], which consists of 4,110 images from 31 categories across three domains: Amazon (A), Webcam (W), and Dslr (D); _Office-Home_[50], containing 15,588 images from 65 categories across four domains: Art (Ar), Clipart (Cl), Product (Pr), and Real-World (Rw); and _VisDA_[43], featuring 207,785 images from 12 categories across two domains: Synthetic and Real. For the DG experiments, we employ the _PACS_[33], _Office-Home_, and _VLCS_[9] datasets. The _PACS_ dataset contains 9,991 images from seven categories across four domains: Art painting (A), Cartoon (C), Photo (P), and Sketch (S), and _VLCS_ contains 10,729 images from five categories across four domains: VOC2007 (V), LabelMe (L), Caltech101 (C), and SUN09 (S). The baselines and implementation details are put in Appendix B.

Figure 4: Qualitative evaluation. The three rows illustrate examples of morphing in image pairs, subjects, and styles, respectively. The text on the left side represents the training prompts, with the red text indicating detailed descriptions used during inference. Additional examples and comparisons with other methods can be found in Appendix C.1.

[MISSING_PAGE_FAIL:8]

The ablation studies of ELS+Terra presented in Table 3 show that the best performance is achieved when transferring from the expanded domain \(\mathcal{D}_{E}\) to the target domain \(\mathcal{D}_{T}\), validating the necessity and effectiveness of combining the adapted source domain with the generated target domain. To highlight the design advantages, we conduct a comparison with SDXL's prior knowledge in Appendix C.5.

### Experiments on Domain Generalization

In this section, we conduct experiments on the _PACS_, _Office-Home_, and _VLCS_ datasets to evaluate the effectiveness of our DG method proposed in Section 3.5. The results presented in Table 4 clearly reveal that our method achieves notable performance improvements across all tasks based on three

Figure 5: T-SNE visualization of the source domain, target domain, adapted source domain, and generated target domain in four classes of the Pr\(\rightarrow\)Cl task on _Office-Home_ under UDA setting.

Figure 6: Examples of the source images from \(\mathcal{D}_{S}\) and corresponding adapted images from \(\mathcal{D}_{\hat{S}}\) for the _Office-Home_ tasks under UDA setting. The text prompts are shown on the left. For instance, the first image pair showcases an image from the Art domain and its corresponding generated image to Clipart domain based on the text prompt “An alarm clock”.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c} \hline \hline Method & Ar\(\rightarrow\)Cl & Ar\(\rightarrow\)Pr & Ar\(\rightarrow\)Rw & Cl\(\rightarrow\)Ar & Cl\(\rightarrow\)Pr & Cl\(\rightarrow\)Rw & Pr\(\rightarrow\)Ar & Pr\(\rightarrow\)Cl & Pr\(\rightarrow\)Rw & Rw\(\rightarrow\)Ar & Rw\(\rightarrow\)Cl & Rw\(\rightarrow\)Pr & **Avg** \\ \hline \(\mathcal{D}_{S}\rightarrow\mathcal{D}_{T}\) & 57.79 & 77.65 & 81.62 & 66.59 & 76.74 & 76.43 & 62.69 & 56.69 & 82.12 & 75.63 & 62.85 & 85.35 & 71.84 \\ \(\mathcal{D}_{\hat{S}}\rightarrow\mathcal{D}_{T}\) & 61.25 & 78.89 & 80.71 & 68.25 & 79.03 & 75.59 & 66.50 & 60.84 & 80.55 & 73.30 & 65.10 & 84.97 & 72.92 \\ \(\mathcal{D}_{T}\rightarrow\mathcal{D}_{T}\) & 58.66 & 80.71 & 80.94 & 69.74 & 80.68 & 78.95 & 69.70 & 54.20 & 81.68 & 71.92 & 56.98 & 82.03 & 72.18 \\ \(\mathcal{D}_{E}\rightarrow\mathcal{D}_{T}\) & **64.62** & **82.33** & **83.60** & **71.19** & **84.25** & **80.31** & **73.00** & **63.57** & **83.81** & **76.20** & **66.56** & **85.70** & **76.26** \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation studies on the _Office-Home_ dataset under UDA setting. The best is in bold.

state-of-the-art DG methods (_i.e._, ERM, SWAD, and SAGM). Furthermore, our method outperforms the stable diffusion generation-based method, DomainDiff, which requires training a separate LoRA for each source domain while our method maintains the parameter efficiency with only one single low-rank structure and different \(\bm{t}\) to capture diverse styles.

Additionally, Fig. 7 shows the learned values of \(\bm{t}\). The \(\bm{t}\) predictor assigns distinct \(\bm{t}\) values to each domain, enabling Terra to generate different interpolated images among the source domains based on varying \(\bm{t}\) values. Moreover, the random sampling of \(\bm{t}\) effectively covers the target domain, offering a clearer understanding of the rationale behind our approach that the generated samples may bring useful information for the target domain. We also show some generated images of the expanded domains on the _PACS_ dataset in Fig. 10 of Appendix C. As can be seen, using Terra can generate diverse styles of images that are different from the source domains. With the expanded domains, the generalization capability of the source model can be improved.

Besides, we conduct an ablation study on the form of Terra and the dimensionality of \(\bm{t}\) in Appendix B, demonstrating that refining Terra's form can further enhance its expressive power. We also compare Terra with other domain generalization morphing techniques, as shown in Appendix C.4, to verify its effectiveness in expanding source domains for improved generalization.

## 5 Conclusion and Future Works

In this paper, we introduce Terra, a framework that facilitates effective cross-domain modeling through the construction of a continuous parameter manifold. Terra incorporates a time-varying parameter within the manifold of domains, enabling flexible and smooth interpolations. This approach facilitates effective knowledge sharing across different domains by training only a single low-rank adaptor. Additionally, based on the designed generation-based strategies, Terra can serve as a plugin for existing UDA and DG methods to enhance performance. We also theoretically analyze the expressive capabilities of Terra. Extensive experiments demonstrate the superior performance of Terra in a range of tasks. For future works, we aim to extend Terra to cover more settings, including different modalities, larger datasets, and more complex tasks.

## Acknowledgements

This work was supported by NSFC key grant 62136005 and NSFC general grant 62076118.

\begin{table}
\begin{tabular}{l c c c c c|c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c|}{_PACS_} & \multicolumn{4}{c}{_OfficeHome_} \\ \cline{2-10}  & A & C & P & S & **Avg** & Ar & Cl & Pr & Rw & **Avg** \\ \hline MIRO [6] & 87.25 & 76.95 & 97.83 & 77.65 & 84.92 & 67.01 & 55.58 & 78.82 & 81.02 & 70.61 \\ CDGA [22] & 87.30 & 80.90 & 96.60 & 82.50 & 86.80 & 60.50 & 56.50 & 77.10 & 80.60 & 68.70 \\ ERM [58] & 87.00 & 78.23 & 98.05 & 74.35 & 84.41 & 63.41 & 52.61 & 77.20 & 77.63 & 67.71 \\ ERM+DomainDiff [36] & 84.90 & 82.90 & 95.50 & 79.00 & 85.60 & 57.60 & 49.20 & 73.00 & 75.20 & 63.70 \\ ERM+Terra & 89.51 & 79.66 & **98.20** & 78.64 & 86.50 & 65.43 & 53.79 & 78.99 & 80.30 & 69.63 \\ SAGM [65] & 85.72 & 81.13 & 96.59 & 77.46 & 85.23 & 65.55 & 55.09 & 78.68 & 79.39 & 69.68 \\ SAGM+Terra & **91.34** & 82.28 & 96.78 & 80.80 & 87.80 & 66.70 & 56.53 & 79.64 & 81.91 & 71.19 \\ SWAD [6] & 89.67 & 83.13 & 97.48 & 82.78 & 88.27 & 66.08 & 57.37 & 79.58 & 80.49 & 70.88 \\ SWAD+Terra & 91.07 & **83.50** & 98.18 & **84.62** & **89.34** & **68.02** & **58.31** & **80.56** & **82.03** & **72.23** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Accuracies (%) on the _PACS_ and _OfficeHome_ datasets under DG setting. The best is in bold.

Figure 7: Visualization of learned time variables on the _PACS_ dataset under the DG setting.

[MISSING_PAGE_FAIL:11]

* [20] L. Han, Y. Li, H. Zhang, P. Milanfar, D. Metaxas, and F. Yang. Svdiff: Compact parameter space for diffusion fine-tuning. _arXiv preprint arXiv:2303.11305_, 2023.
* [21] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 770-778, 2016.
* [22] S. Hemati, M. Beitollahi, A. H. Estiri, B. A. Omari, X. Chen, and G. Zhang. Cross domain generative augmentation: Domain generalization with latent diffusion models. _arXiv preprint arXiv:2312.05387_, 2023.
* [23] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. _Advances in Neural Information Processing Systems_, 30, 2017.
* [24] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. _Advances in Neural Information Processing Systems_, 33:6840-6851, 2020.
* [25] E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. LoRA: Low-rank adaptation of large language models. In _International Conference on Learning Representations_, 2022.
* [26] T. Huang, J. Liu, S. You, and C. Xu. Active generation for image classification. _arXiv preprint arXiv:2403.06517_, 2024.
* [27] Y. Huang, J. Huang, Y. Liu, M. Yan, J. Lv, J. Liu, W. Xiong, H. Zhang, S. Chen, and L. Cao. Diffusion model-based image editing: A survey. _arXiv preprint arXiv:2402.17525_, 2024.
* [28] Y. Jin, X. Wang, M. Long, and J. Wang. Minimum class confusion for versatile domain adaptation. In _European Conference on Computer Vision_, pages 464-480. Springer, 2020.
* [29] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila. Analyzing and improving the image quality of stylegan. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 8110-8119, 2020.
* [30] B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, and M. Irani. Imagic: Text-based real image editing with diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 6007-6017, 2023.
* [31] A. Kumar, T. Ma, and P. Liang. Understanding self-training for gradual domain adaptation. In _International conference on machine learning_, pages 5468-5479. PMLR, 2020.
* [32] N. Kumari, B. Zhang, R. Zhang, E. Shechtman, and J.-Y. Zhu. Multi-concept customization of text-to-image diffusion. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1931-1941, 2023.
* [33] D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. Deeper, broader and artier domain generalization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 5542-5550, 2017.
* [34] M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transferable features with deep adaptation networks. In _International Conference on Machine Learning_, pages 97-105. PMLR, 2015.
* [35] M. Long, Z. Cao, J. Wang, and M. I. Jordan. Conditional adversarial domain adaptation. _Advances in Neural Information Processing Systems_, 31, 2018.
* [36] Q. Miao, J. Yuan, S. Zhang, F. Wu, and K. Kuang. Domaindiff: Boost out-of-distribution generalization with synthetic data. In _IEEE International Conference on Acoustics, Speech and Signal Processing_, pages 5640-5644. IEEE, 2024.
* [37] C. Mou, X. Wang, L. Xie, Y. Wu, J. Zhang, Z. Qi, and Y. Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 38, pages 4296-4304, 2024.

* [38] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In _International Conference on Machine Learning_, pages 16784-16804. PMLR, 2022.
* [39] C. C. Paige and M. A. Saunders. Towards a generalized singular value decomposition. _SIAM Journal on Numerical Analysis_, 18(3):398-405, 1981.
* [40] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. _IEEE transactions on Neural Networks_, 22(2):199-210, 2010.
* [41] X. Pan, X. Zhan, B. Dai, D. Lin, C. C. Loy, and P. Luo. Exploiting deep generative prior for versatile image restoration and manipulation. In _European Conference on Computer Vision_, 2020.
* [42] D. Peng, Q. Ke, Y. Lei, and J. Liu. Unsupervised domain adaptation via domain-adaptive diffusion. _arXiv preprint arXiv:2308.13893_, 2023.
* [43] X. Peng, B. Usman, N. Kaushik, J. Hoffman, D. Wang, and K. Saenko. Visda: The visual domain adaptation challenge. _arXiv preprint arXiv:1710.06924_, 2017.
* [44] R. Penrose. A generalized inverse for matrices. In _Mathematical proceedings of the Cambridge philosophical society_, volume 51, pages 406-413. Cambridge University Press, 1955.
* [45] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Muller, J. Penna, and R. Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. _arXiv preprint arXiv:2307.01952_, 2023.
* [46] H. Rangwani, S. K. Aithal, M. Mishra, A. Jain, and V. B. Radhakrishnan. A closer look at smoothness in domain adversarial training. In _International Conference on Machine Learning_, pages 18378-18399. PMLR, 2022.
* [47] M. D. M. Reddy, M. S. M. Basha, M. M. C. Hari, and M. N. Penchalaiah. Dall-e: Creating images from text. _UGC Care Group I Journal_, 8(14):71-75, 2021.
* [48] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10684-10695, 2022.
* [49] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aherman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 22500-22510, 2023.
* [50] N. Ruiz, Y. Li, V. Jampani, W. Wei, T. Hou, Y. Pritch, N. Wadhwa, M. Rubinstein, and K. Aherman. Hyperdreambooth: Hypernetworks for fast personalization of text-to-image models. _arXiv preprint arXiv:2307.06949_, 2023.
* [51] K. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In _European Conference on Computer Vision_, volume 6314, pages 213-226. Springer, 2010.
* [52] V. Shah, N. Ruiz, F. Cole, E. Lu, S. Lazebnik, Y. Li, and V. Jampani. Ziplora: Any subject in any style by effectively merging loras. In _European Conference on Computer Vision_, pages 422-438. Springer, 2025.
* [53] K. Shoemake. Animating rotation with quaternion curves. In _Proceedings of the 12th annual conference on Computer graphics and interactive techniques_, pages 245-254, 1985.
* [54] K. Sohn, N. Ruiz, K. Lee, D. C. Chin, I. Blok, H. Chang, J. Barber, L. Jiang, G. Entis, Y. Li, et al. Styledrop: Text-to-image generation in any style. _arXiv preprint arXiv:2306.00983_, 2023.
* [55] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In _International Conference on Learning Representations_, 2020.
* [56] Y. Tewel, R. Gal, G. Chechik, and Y. Atzmon. Key-locked rank one editing for text-to-image personalization. In _ACM SIGGRAPH 2023 Conference Proceedings_, pages 1-11, 2023.

* [57] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. _Journal of Machine Learning Research_, 9(11), 2008.
* [58] V. Vapnik. _The nature of statistical learning theory_. Springer science & business media, 1999.
* [59] H. Venkateswara, J. Eusebio, S. Chakraborty, and S. Panchanathan. Deep hashing network for unsupervised domain adaptation. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5018-5027, 2017.
* [60] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz, and Y. Bengio. Manifold mixup: Better representations by interpolating hidden states. In _International Conference on Machine Learning_, pages 6438-6447. PMLR, 2019.
* [61] C. Wang, J. Gao, Y. Hua, and H. Wang. Cross-domain learning with normalizing flow. In _ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5. IEEE, 2023.
* [62] C. J. Wang and P. Golland. Interpolating between images with diffusion models. _arXiv preprint arXiv:2307.12560_, 2023.
* [63] H. Wang, J. Fan, Z. Chen, H. Li, W. Liu, T. Liu, Q. Dai, Y. Wang, Z. Dong, and R. Tang. Optimal transport for treatment effect estimation. _Advances in Neural Information Processing Systems_, 36:1-15, 2023.
* [64] J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen, W. Zeng, and S. Y. Philip. Generalizing to unseen domains: A survey on domain generalization. _IEEE Transactions on Knowledge and Data Engineering_, 35(8):8052-8072, 2022.
* [65] P. Wang, Z. Zhang, Z. Lei, and L. Zhang. Sharpness-aware gradient matching for domain generalization. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 3769-3778, 2023.
* [66] Q. Wang, X. Bai, H. Wang, Z. Qin, and A. Chen. Instantid: Zero-shot identity-preserving generation in seconds. _arXiv preprint arXiv:2401.07519_, 2024.
* [67] X. Wang, P. Guo, and Y. Zhang. Unsupervised domain adaptation via bidirectional cross-attention transformer. In _Joint European Conference on Machine Learning and Knowledge Discovery in Databases_, pages 309-325. Springer, 2023.
* [68] X. Wang, K. Yu, C. Dong, X. Tang, and C. C. Loy. Deep network interpolation for continuous imagery effect transition. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1692-1701, 2019.
* [69] X. Wu, S. Huang, and F. Wei. Mixture of loRA experts. In _International Conference on Learning Representations_, 2024.
* [70] H. Xia and Z. Ding. Structure preserving generative cross-domain learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 4364-4373, 2020.
* [71] H. Xia, T. Jing, and Z. Ding. Maximum structural generation discrepancy for unsupervised domain adaptation. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45(3):3434-3445, 2023.
* [72] R. Xu, G. Li, J. Yang, and L. Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1426-1435, 2019.
* [73] G. Yang, H. Xia, M. Ding, and Z. Ding. Bi-directional generation for unsupervised domain adaptation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 34, pages 6615-6622, 2020.
* [74] Q. Yang, Y. Zhang, W. Dai, and S. J. Pan. _Transfer learning_. Cambridge, U.K.: Cambridge Univ. Press, 2020.

* [75] F. Ye, X. Wang, Y. Zhang, and I. W. Tsang. Multi-task learning via time-aware neural ode. In _International Joint Conference on Artificial Intelligence_, pages 4495-4503, 2023.
* [76] H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. _arXiv preprint arXiv:2308.06721_, 2023.
* [77] T. Zadouri, A. Ustun, A. Ahmadian, B. Ermis, A. Locatelli, and S. Hooker. Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning. In _The Twelfth International Conference on Learning Representations_, 2023.
* [78] Y. Zeng and K. Lee. The expressive power of low-rank adaptation. In _International Conference on Learning Representations_, 2023.
* [79] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. In _International Conference on Learning Representations_, 2018.
* [80] K. Zhang, Y. Zhou, X. Xu, X. Pan, and B. Dai. Diffmorpher: Unleashing the capability of diffusion models for image morphing. _arXiv preprint arXiv:2312.07409_, 2023.
* [81] L. Zhang, A. Rao, and M. Agrawala. Adding conditional control to text-to-image diffusion models. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 3836-3847, 2023.
* [82] P. Zhang, H. Dou, Y. Yu, and X. Li. Adaptive cross-domain learning for generalizable person re-identification. In _European Conference on Computer Vision_, pages 215-232. Springer, 2022.
* [83] X. Zhang, X.-Y. Wei, W. Zhang, J. Wu, Z. Zhang, Z. Lei, and Q. Li. A survey on personalized content synthesis with diffusion models. _arXiv preprint arXiv:2405.05538_, 2024.
* [84] Y. Zhang, S. Chen, W. Jiang, Y. Zhang, J. Lu, and J. T. Kwok. Domain-guided conditional diffusion model for unsupervised domain adaptation. _arXiv preprint arXiv:2309.14360_, 2023.
* [85] Y. Zhang, J. Liang, Z. Zhang, L. Wang, R. Jin, T. Tan, et al. Free lunch for domain adversarial training: Environment label smoothing. In _International Conference on Learning Representations_, 2023.
* [86] Y. Zhang, T. Liu, M. Long, and M. Jordan. Bridging theory and algorithm for domain adaptation. In _International Conference on Machine Learning_, pages 7404-7413. PMLR, 2019.
* [87] Y. Zhang, Y. Yao, S. Chen, P. Jin, Y. Zhang, J. Jin, and J. Lu. Rethinking guidance information to utilize unlabeled samples: A label encoding perspective. In _International Conference on Machine Learning_, 2024.
* [88] P. Zheng, Y. Zhang, Z. Fang, T. Liu, D. Lian, and B. Han. Noisediffusion: Correcting noise for image interpolation with diffusion models beyond spherical linear interpolation. In _International Conference on Learning Representations_, 2024.
* [89] K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy. Domain generalization: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 45:4396-4415, 2023.
* [90] K. Zhou, Y. Yang, Y. Qiao, and T. Xiang. Domain generalization with mixstyle. In _International Conference on Learning Representations_, 2020.
* [91] J.-Y. Zhu, P. Krahenbuhl, E. Shechtman, and A. A. Efros. Generative visual manipulation on the natural image manifold. In _European Conference on Computer Vision_, pages 597-613. Springer, 2016.
* [92] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In _Proceedings of the IEEE International Conference on Computer Vision_, pages 2223-2232, 2017.
* [93] Y. Zhu, F. Zhuang, J. Wang, G. Ke, J. Chen, J. Bian, H. Xiong, and Q. He. Deep subdomain adaptation network for image classification. _IEEE Transactions on Neural Networks and Learning Systems_, 32(4):1713-1722, 2020.

* [94] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, and Q. He. A comprehensive survey on transfer learning. _Proceedings of the IEEE_, 109(1):43-76, 2020.
* [95] Z. Zhuang, Y. Zhang, and Y. Wei. Gradual domain adaptation via gradient flow. In _International Conference on Learning Representations_, 2024.

## Appendix A Proofs

**Lemma 1**.: _Given two matrices \(\mathbf{A},\mathbf{B}\in\mathbb{R}^{m\times n}\), and \(\operatorname{rank}(\cdot)\) denotes the rank of a matrix. Then, \(\operatorname{rank}([\mathbf{A}\ \mathbf{B}])\leq\operatorname{rank}(\mathbf{A})+ \operatorname{rank}(\mathbf{B})\) and \(\operatorname{rank}([\mathbf{A}^{T}\ \mathbf{B}^{T}])\leq\operatorname{rank}(\mathbf{A})+ \operatorname{rank}(\mathbf{B})\)._

**Lemma 2**.: _Given two matrices \(\mathbf{A}\in\mathbb{R}^{m\times n},\mathbf{B}\in\mathbb{R}^{n\times q}\), and \(\operatorname{rank}(\cdot)\) denotes the rank of a matrix. Then, \(\operatorname{rank}([\mathbf{A}\mathbf{B}])\leq\min\left(\operatorname{ rank}(\mathbf{A}),\operatorname{rank}(\mathbf{B})\right)\)._

**Theorem 3**.: (Generalized Singular Value Decomposition (GSVD) [39]) _For given matrices \(\mathbf{A},\mathbf{B}\in\mathbb{R}^{m\times n}\), let \(\mathbf{C}^{T}=[\mathbf{A}^{T}\ \mathbf{B}^{T}]\) and denote its rank by \(r=\text{rank}(\mathbf{C})\), there exist orthogonal matrices \(\mathbf{U}_{\mathbf{A}},\mathbf{U}_{\mathbf{B}}\in\mathbb{R}^{m\times m}\), \(\mathbf{Q}\in\mathbb{R}^{n\times n}\) and \(\mathbf{W}\in\mathbb{R}^{k\times k}\) so that_

\[\mathbf{U}_{\mathbf{A}}{}^{T}\mathbf{AQ}=\mathbf{\Sigma}_{\mathbf{A}}\left[ \mathbf{W}^{T}\mathbf{R},\ \mathbf{0}\right],\quad\mathbf{U}_{\mathbf{B}}{}^{T}\mathbf{BQ}=\mathbf{ \Sigma}_{\mathbf{B}}\left[\mathbf{W}^{T}\mathbf{R},\ \mathbf{0}\right],\] (10)

\[\mathbf{\Sigma}_{\mathbf{A}}=\left[\begin{array}{cc}\mathbf{I}_{\mathbf{A}} \\ &\mathbf{S}_{\mathbf{A}}\\ &\mathbf{O}_{\mathbf{A}}\end{array}\right],\quad\mathbf{\Sigma}_{\mathbf{B}}= \left[\begin{array}{cc}\mathbf{O}_{\mathbf{B}}\\ &\mathbf{S}_{\mathbf{B}}\\ &\mathbf{I}_{\mathbf{B}}\end{array}\right],\] (11)

_where \(\mathbf{R}\) is real diagonal contains the nonzero singular values of \(\mathbf{C}\) in decreasing order, \(\mathbf{\Sigma}_{\mathbf{A}},\mathbf{\Sigma}_{\mathbf{B}}\in\mathbb{R}^{m \times k}\) are real non-negative block-diagonal matrices, where \(\mathbf{I}_{\mathbf{A}}\in\mathbb{R}^{r\times r}\) and \(\mathbf{I}_{\mathbf{B}}\in\mathbb{R}^{k-r-s\times k-r-s}\) are identity matrices, \(\mathbf{O}_{\mathbf{A}}\in\mathbb{R}^{m-r-s\times k-r-s}\) and \(\mathbf{O}_{\mathbf{B}}\in\mathbb{R}^{m-k-r\times r}\) are zero matrices with possibly no rows or no columns, and \(\mathbf{S}_{\mathbf{A}}=[\alpha_{r+1},\ldots,\alpha_{r+s}]\) and \(\mathbf{S}_{\mathbf{B}}=[\beta_{r+1},\ldots,\beta_{r+s}]\). And we have_

\[1>\alpha_{r+1}\geq\cdots\geq\alpha_{r+s}>0,\quad 0<\beta_{r+1}\leq\cdots\leq \beta_{r+s}<1,\quad\alpha_{i}^{2}+\beta_{i}^{2}=1,\quad i\in[r+1,r+s].\]

Indeed, GSVD is a powerful tool in numerical linear algebra and data analysis. It can be seen as an extension of the singular value decomposition (SVD). Notably, when the matrix \(\mathbf{B}\) is the identity matrix, the GSVD of matrix \(\mathbf{A}\) and \(\mathbf{B}\) simplifies to the SVD of matrix \(\mathbf{A}\).

**Theorem 1**.: (The Equivariance between Terra and Multiple LoRAs) _Assume there exist two LoRAs \(\Delta\bm{W}_{A},\Delta\bm{W}_{B}\in\mathbb{R}^{m\times n}\) with ranks of \(p\) and \(q\), respectively, that effectively solve two specific downstream tasks. Let \(k=\max\{\operatorname{rank}([\Delta\bm{W}_{A}\ \Delta\bm{W}_{B}]), \operatorname{rank}([\Delta\bm{W}_{A}^{T}\ \Delta\bm{W}_{B}^{T}])\},\) where \(\operatorname{rank}(\cdot)\) denotes the rank of a matrix. Then, there exists a Terra with \(\bm{W}_{\text{up}}\in\mathbb{R}^{m\times k}\), \(\bm{W}_{\text{down}}\in\mathbb{R}^{k\times n}\), \(\bm{W}_{\text{mid}}\in\mathbb{R}^{k\times k}\), and \(\mathcal{K}(t)=t\bm{W}_{\text{mid}}+\bm{I}_{r}\), such that the updated matrix \(\Delta\bm{W}(t)=\bm{W}_{\text{up}}\mathcal{K}(t)\bm{W}_{\text{down}}\), can simultaneously solve the two downstream task, that is, we have \(\Delta\bm{W}(0)=\Delta\bm{W}_{A}\) and \(\Delta\bm{W}(1)=\Delta\bm{W}_{B}\)._

Proof.: Our goal is to find matrices \(\bm{W}_{\text{up}}\), \(\bm{W}_{\text{down}}\), and \(\bm{W}_{\text{mid}}\) to satisfy \(\Delta\bm{W}(0)=\bm{W}_{\text{up}}\bm{W}_{\text{down}}=\Delta\bm{W}_{A}\), and \(\Delta\bm{W}(1)=W_{\text{up}}(\bm{W}_{\text{mid}}+I)\bm{W}_{\text{down}}= \Delta\bm{W}_{B}\).

From Theorem 3, since \(k\geq\operatorname{rank}([\Delta\bm{W}_{A}^{T}\ \Delta\bm{W}_{B}^{T}])\), we know GSVD can decompose the two LoRA adapters with a common right generalized singular vectors \(\mathbf{X}\in\mathbb{R}^{k\times n}\):

\[\Delta\bm{W}_{A}=\mathbf{U}_{\mathbf{A}}\mathbf{\Sigma}_{\mathbf{A}}\mathbf{X}, \quad\Delta\bm{W}_{B}=\mathbf{U}_{\mathbf{B}}\mathbf{\Sigma}_{\mathbf{B}} \mathbf{X}.\] (12)

Similarly, we can transpose the matrices of the two LoRA adapters, since \(k\geq\text{rank}([\Delta\bm{W}_{A}\ \Delta\bm{W}_{B}])\), and use GSVD again, then we have a common left generalized singular vectors \(\mathbf{Y}\in\mathbb{R}^{m\times k}\):

\[\Delta\bm{W}_{A}=\mathbf{Y}\mathbf{Z}_{\mathbf{A}}\mathbf{V}_{\mathbf{A}},\quad \Delta\bm{W}_{B}=\mathbf{Y}\mathbf{Z}_{\mathbf{B}}\mathbf{V}_{\mathbf{B}}.\] (13)

For each matrix \(\mathbf{W}\), there exists a pseudo-inverse (_a.k.a._ the Moore-Penrose inverse [44]) \(\mathbf{W}^{+}\) such that \(\mathbf{W}\mathbf{W}^{+}\mathbf{W}=\mathbf{W}\). Then we have a special decomposition of the LoRA adapters:

\[\Delta\bm{W}_{A} =\mathbf{U}_{\mathbf{A}}\mathbf{\Sigma}_{\mathbf{A}}\mathbf{X}\] (14) \[=\mathbf{U}_{\mathbf{A}}\mathbf{\Sigma}_{\mathbf{A}}(\mathbf{X} \mathbf{X}^{+}\mathbf{X})\] \[=\Delta\bm{W}_{A}\mathbf{X}^{+}\mathbf{X}\] \[=\mathbf{Y}\mathbf{Y}^{+}\Delta\bm{W}_{A}\mathbf{X}^{+}\mathbf{X} \triangleq\mathbf{Y}\mathbf{K}_{\mathbf{A}}\mathbf{X},\]

where \(\mathbf{K}_{\mathbf{A}}\in\mathbb{R}^{k\times k}\). Similarly, we have:

\[\Delta\bm{W}_{B}=\mathbf{Y}(\mathbf{Y}^{+}\Delta\bm{W}_{B}\mathbf{X}^{+})\mathbf{ X}\triangleq\mathbf{Y}\mathbf{K}_{\mathbf{B}}\mathbf{X}.\] (15)

Assume the SVD of \(\mathbf{K}_{\mathbf{A}}\) is of the following form:

\[\mathbf{K}_{\mathbf{A}}=\mathbf{U}_{\mathbf{K}}\mathbf{\Lambda}\mathbf{V}_{ \mathbf{K}},\] (16)where \(\mathbf{U_{K}},\mathbf{V_{K}}\in\mathbb{R}^{k\times k}\) are orthogonal matrices.

We can represent the diagonal matrix \(\mathbf{\Lambda}\) as \(\mathbf{\Lambda}=\text{diag}(\sigma_{1},\sigma_{2},\ldots,\sigma_{r},0,\ldots,0)\). Define \(\mathbf{\Lambda}^{+}\) whose first \(r\) rows have \(1/\sigma_{1},1/\sigma_{2},\ldots,1/\sigma_{r}\) on the diagonal, and the product of \(\mathbf{\Lambda}\) and \(\mathbf{\Lambda}^{+}\)is a square matrix whose first \(r\) diagonal entries are 1 and whose others are 0, i.e. \(\mathbf{I}_{r}\). Then, we can get

\[\begin{split}\Delta\bm{W}(t)&=\mathbf{Y}\left(t( \mathbf{K_{B}}-\mathbf{K_{A}})+\mathbf{K_{A}}\right)\mathbf{X}\\ &=\mathbf{Y}\left(t(\mathbf{K_{B}}-\mathbf{K_{A}})+\mathbf{U_{K} }\mathbf{\Lambda}\mathbf{V_{K}}\right)\mathbf{X}\\ &=\mathbf{YU_{K}}\mathbf{U_{K}}^{T}\left(t(\mathbf{K_{B}}- \mathbf{K_{A}})+\mathbf{U_{K}}\mathbf{\Lambda}\mathbf{V_{K}}\right)\mathbf{V_ {K}}^{T}\mathbf{V_{K}}\mathbf{X}\\ &=\mathbf{YU_{K}}\left(t\mathbf{U_{K}}^{T}(\mathbf{K_{B}}- \mathbf{K_{A}})\mathbf{V_{K}}^{T}+\mathbf{\Lambda}\right)\mathbf{V_{K}} \mathbf{X}\\ &=\mathbf{YU_{K}}\left(t\mathbf{U_{K}}^{T}(\mathbf{K_{B}}- \mathbf{K_{A}})\mathbf{V_{K}}^{T}\mathbf{\Lambda}^{+}+\mathbf{I}_{r}\right) \mathbf{\Lambda}\mathbf{V_{K}}\mathbf{X}\\ &=\bm{W}_{\text{up}}\left(t\bm{W}_{\text{mid}}+\mathbf{I}_{r} \right)\right)\bm{W}_{\text{down}}.\end{split}\] (17)

Finally, we can construct the following matrices to prove the theorem:

\[\bm{W}_{\text{up}}=\mathbf{YU_{K}},\quad\bm{W}_{\text{mid}}=\mathbf{U_{K}}^{T }(\mathbf{K_{B}}-\mathbf{K_{A}})\mathbf{V_{K}}^{T}\mathbf{\Lambda}^{+},\quad \bm{W}_{\text{down}}=\mathbf{\Lambda}\mathbf{V_{K}}\mathbf{X}.\] (18)

**Theorem 2**.: (The Expressive Power of Terra) _For each layer \(l\), the rank-\(k\) Terra has updated matrix \(\Delta\bm{W}(t)_{l}\), and the function of time-varying matrix is \(\mathcal{K}(t)_{l}=t\bm{W}_{\text{mid},l}+\tilde{\mathbf{I}}\). Assume that all weight matrices of the frozen model \((\bm{W}_{l})_{l=1}^{L}\), \(\prod_{l=1}^{L}\bm{W}_{l}+\mathrm{LR}_{r}(\bm{E}_{A})\), and \(\prod_{l=1}^{L}\bm{W}_{l}+\mathrm{LR}_{r}(\bm{E}_{B})\) are non-singular for all \(r\leq k(L-1)\). Then the approximation error satisfies_

\[\min_{\Delta\bm{W}(t)}\left(\left\|\prod_{l=1}^{L}\left(\bm{W}_{l}+\Delta\bm{W }(0)_{l}\right)-\overline{\bm{W}}_{A}\right\|_{2}+\left\|\prod_{l=1}^{L}\left( \bm{W}_{l}+\Delta\bm{W}(1)_{l}\right)-\overline{\bm{W}}_{B}\right\|_{2}\right) \leq 2\sigma_{kL+1}^{*},\] (4)

_where the \(\sigma_{kL+1}^{*}\) as the \((kL+1)\)-th largest singular values obtained by merging the singular values of \(\bm{E}_{A}\) and \(\bm{E}_{B}\). Moreover, when \(k\geq\left\lceil\frac{R_{\bm{E}_{A}}+R_{\bm{E}_{B}}}{L}\right\rceil\), the approximation error is zero._

Proof.: We first adopt a similar construction consistently with the prior work [78]:

\[\bm{S_{A}}:=\prod_{l=1}^{L}\left(\bm{W}_{l}+\Delta\bm{W}(0)_{l}\right)-\prod_ {l=1}^{L}\bm{W}_{l}\quad\bm{S_{B}}:=\prod_{l=1}^{L}\left(\bm{W}_{l}+\Delta\bm {W}(1)_{l}\right)-\prod_{l=1}^{L}\bm{W}_{l}.\] (19)

Then, the approximate error can be represented as:

\[\begin{split}&\min_{\Delta\bm{W}(t)}\left(\left\|\prod_{l=1}^{L} \left(\bm{W}_{l}+\Delta\bm{W}(0)_{l}\right)-\overline{\bm{W}}_{A}\right\|_{2}+ \left\|\prod_{l=1}^{L}\left(\bm{W}_{l}+\Delta\bm{W}(1)_{l}\right)-\overline{ \bm{W}}_{B}\right\|_{2}\right)\\ &=\min_{\Delta\bm{W}(t)}\left(\left\|\bm{S_{A}}-\bm{E_{A}} \right\|_{2}+\left\|\bm{S_{B}}-\bm{E_{B}}\right\|_{2}\right).\end{split}\] (20)

Following the prior work, we can also decompose \(\bm{S_{A}}\) into an accumulation of \(\bm{S_{A}}\) as follows:

\[\begin{split}\bm{S_{A}}=&\Delta\bm{W}(0)_{L}\prod_{l=1 }^{L-1}\left(\Delta\bm{W}(0)_{l}+\bm{W}_{l}\right)+\bm{W}_{L}\Delta\bm{W}(0)_{L- 1}\prod_{l=1}^{L-2}\left(\Delta\bm{W}(0)_{l}+\bm{W}_{l}\right)\\ &+\ldots+\left(\prod_{l=2}^{L}\bm{W}_{l}\right)\left(\Delta\bm{W}( 0)_{1}+\bm{W}_{1}\right)-\prod_{l=1}^{L}\bm{W}_{l}\\ =&\sum_{l=1}^{L}\underbrace{\left[\left(\prod_{i=l+1}^{L }\bm{W}_{i}\right)\Delta\bm{W}(0)_{l}\left(\prod_{i=1}^{l-1}\left(\bm{W}_{i}+ \Delta\bm{W}(0)_{i}\right)\right)\right]}_{:=\bm{S_{A}}_{i}}.\end{split}\] (21)Similarly, we have \(\bm{S_{B}}=\sum_{l=1}^{L}\bm{S_{B_{l}}},\bm{S_{B_{l}}}=\left(\prod_{i=l+1}^{L}\bm{ W}_{i}\right)\Delta\bm{W}(1)_{l}\left(\prod_{i=1}^{l-1}\left(\bm{W}_{i}+\Delta\bm{W}(1) _{i}\right)\right)\).

We select the largest \(kL\) largest terms of the singular values of \(\bm{E_{A}}\) and \(\bm{E_{B}}\), and we denote there are \(p\) values from \(\bm{E_{A}}\) and \(q\) values from \(\bm{E_{B}}\). To prove the theorem, we need to show the following:

\[\min_{\Delta\bm{W}(t)}\left(\left\|\bm{S_{A}}-\bm{E_{A}}\right\|_{2}+\left\|\bm {S_{B}}-\bm{E_{B}}\right\|_{2}\right)\leq 2\sigma_{kL+1}^{*}.\] (22)

Define \(\bm{E^{\prime}_{A}}=\mathrm{LR}_{p}(\bm{E_{A}}),\bm{E^{\prime}_{B}}=\mathrm{LR }_{q}(\bm{E_{B}})\), based on the Eckart-Young Theorem [8], then we have:

\[\left\|\bm{E^{\prime}_{A}}-\bm{E_{A}}\right\|_{2}+\left\|\bm{E^{ \prime}_{B}}-\bm{E_{B}}\right\|_{2}\leq\sigma_{p+1}(\bm{E_{A}})+\sigma_{q+1}( \bm{E_{B}})\leq 2\sigma_{kL+1}^{*}.\] (23)

Based on (22) and (23), if we can construct Terra parameter \(\Delta\bm{W}(t)\) to make \(\bm{S_{A}}=\bm{E^{\prime}_{A}}\) and \(\bm{S_{B}}=\bm{E^{\prime}_{B}}\), then we will finish the proof. We refer to the SVD of \(\bm{E^{\prime}_{A}}\) and \(\bm{E^{\prime}_{B}}\) as:

\[\bm{E^{\prime}_{A}}=\mathbf{U_{A}}\mathbf{A_{A}}\mathbf{V_{A}},\quad\bm{E^{ \prime}_{B}}=\mathbf{U_{B}}\mathbf{A_{B}}\mathbf{V_{B}},\] (24)

We introduce \(\bm{Q}_{A,l}\) and \(\bm{Q}_{B,l}\) to divide \(\bm{E^{\prime}_{A}}\) and \(\bm{E^{\prime}_{B}}\) into \(L\) parts:

\[\sum_{l=1}^{L}\bm{E^{\prime}_{A}}\bm{Q}_{A,l}=\bm{E^{\prime}_{A}}\quad\sum_{l =1}^{L}\bm{E^{\prime}_{B}}\bm{Q}_{B,l}=\bm{E^{\prime}_{B}},\] (25)

We define \(\bm{I}_{a:b}\) as a diagonal matrix whose diagonal entries from the \(a\)-th to \(b\)-th position are \(1\) and others are \(0\). Here we define the matrices \((\bm{Q}_{A,l})_{l=1}^{L}\) and \((\bm{Q}_{B,l})_{l=1}^{L}\) by:

\[\left\{\begin{array}{ll}\bm{Q}_{A,l}=\mathbf{V_{A}}\mathbf{I}_{R(l-1)+1:Rl} \mathbf{V_{A}}^{T},\bm{Q}_{B,l}=\mathbf{0},&\text{for }Rl<p,\\ \bm{Q}_{A,l}=\mathbf{V_{A}}\mathbf{I}_{R(l-1)+1:p}\mathbf{V_{A}}^{T},\bm{Q}_{ B,l}=\mathbf{V_{B}}\mathbf{I}_{1:Rl-p}\mathbf{V_{B}}^{T},&\text{for }p\leq Rl<p+l,\\ \bm{Q}_{A,l}=\mathbf{0},\bm{Q}_{B,l}=\mathbf{V_{B}}\mathbf{I}_{R(l-1)-p+1:Rl- p}\mathbf{V_{B}}^{T},&\text{for }p+l\leq Rl.\end{array}\right.\] (26)

It easy to find that \(\text{rank}(\bm{Q}_{A,l})+\text{rank}(\bm{Q}_{B,l})\leq R\). Based on Lemma 1, we have

\[\text{rank}\left(\left[\bm{E^{\prime}_{A}}\bm{Q}_{A,l}\right.\left.\bm{E^{ \prime}_{B}}\bm{Q}_{B,l}\right]\right)\leq k,\quad\text{rank}\left(\left[\left( \bm{E^{\prime}_{A}}\bm{Q}_{A,l}\right)^{T}\right.\left.\left(\bm{E^{\prime}_{B }}\bm{Q}_{B,l}\right)^{T}\right]\right)\leq k.\] (27)

Now, we show a feasible solution to make \(\bm{S_{A}}=\bm{E^{\prime}_{A}}\) and \(\bm{S_{B}}=\bm{E^{\prime}_{B}}\) follows these conditions:

\[\widehat{\Delta\bm{W}(0)}_{l}=(\prod_{i=l+1}^{L}\bm{W}_{i})^{-1} \bm{E^{\prime}_{A}}\bm{Q}_{A,l}(\prod_{i=1}^{l-1}(\bm{W}_{i}+\widehat{\Delta \bm{W}(0)}_{i}))^{-1},\quad\text{ for all }l\in[L],\] (28) \[\widehat{\Delta\bm{W}(1)}_{l}=(\prod_{i=l+1}^{L}\bm{W}_{i})^{-1} \bm{E^{\prime}_{B}}\bm{Q}_{B,l}(\prod_{i=1}^{l-1}(\bm{W}_{i}+\widehat{\Delta\bm {W}(1)}_{i}))^{-1},\quad\text{ for all }l\in[L],\] (29) \[\text{rank}\left(\bm{W}_{l}+\widehat{\Delta\bm{W}(0)}_{l}\right)= \text{rank}\left(\bm{W}_{l}+\widehat{\Delta\bm{W}(1)}_{l}\right)=D,\quad \text{ for all }l\in[L-1].\] (30)

Based on the assumptions of \(\left(\bm{W}_{l}\right)_{l=1}^{L}\), \(\prod_{l=1}^{L}\bm{W}_{l}+\mathrm{LR}_{r}(\bm{E}_{A})\), and \(\prod_{l=1}^{L}\bm{W}_{l}+\mathrm{LR}_{r}(\bm{E}_{B})\) are non-singular for all \(r\leq k(L-1)\) and the Eq. (28) and Eq. (29), it's easy to prove that Eq. (30) is satisfied [78].

Using the Lemma 2 and Eq. (27), we can show \(\text{rank}\left(\left[\widehat{\Delta\bm{W}(0)}_{l}\right.\left.\widehat{\Delta \bm{W}(1)}_{l}\right]\right)\leq k\) by

\[\text{rank}\left(\left[\widehat{\Delta\bm{W}(0)}_{l}\right.\left. \widehat{\Delta\bm{W}(1)}_{l}\right]\right)\] \[=\text{rank}\left(\left[\bm{E^{\prime}_{A}}\bm{Q}_{A,l}\right.\left. \bm{E^{\prime}_{B}}\bm{Q}_{B,l}\right]\left[\begin{array}{cc}(\prod_{i=1}^{l-1 }(\bm{W}_{i}+\widehat{\Delta\bm{W}(0)}_{i}))^{-1}&\\ &(\prod_{i=1}^{l-1}(\bm{W}_{i}+\widehat{\Delta\bm{W}(1)}_{i}))^{-1}\end{array} \right]\right)\] \[\leq\text{rank}\left(\left[\bm{E^{\prime}_{A}}\bm{Q}_{A,l}\right.\left. \bm{E^{\prime}_{B}}\bm{Q}_{B,l}\right]\right)\leq k\]

Similarly, we can also get \(\text{rank}(\widehat{(\Delta\bm{W}(0)}_{l}^{T}\right.\left.\widehat{\Delta\bm{W}( 1)}_{l}^{T}))\leq k\). Then, based on Theorem 1, for each layer \(l\), we can prove that there exists a Terra can satisfies \(\Delta\bm{W}(0)_{l}=\widehat{\Delta\bm{W}(0)}_{l}\) and \(\Delta\bm{W}(1)_{l}=\widehat{\Delta\bm{W}(1)}_{l}\), thereby completing the proof.

Baselines and Implementation Details

Baselines.For the generative interpolation tasks, we use DGP [41], DDIM [55], DiffMorpher [80], and LoRA Interpolation for comparison. DGP leverages large-scale pre-trained GAN [3] for image morphing. DDIM means the DDIM inversion and latent interpolation as discussed in [55]. DiffMorpher performs image morphing between two images by interpolating corresponding two LoRAs and latent noises. LoRA Interpolation represents directly training two LoRAs and performing interpolation. For the UDA tasks, we compare with ERM [58] and various UDA methods, including AFN [72], MDD [86], MCC [28], DANN [13], CDAN [35], SDAT [46], ELS [85], and MSGD [71]. We integrate the proposed Terra with the state-of-the-art UDA methods, _i.e._, MCC, and ELS. For the DG tasks, we compare with ERM [58], MIRO [6], CDGA [22], SWAD [5], SAGM [68], and DomainDiff [36]. Note that previous works [18; 5] have found that ERM is effective in DG and outperforms previous DG methods. Thus, we integrate Terra with ERM and the state-of-the-art DG methods, _i.e._, SWAD, and SAGM.

Implementation Details.The text-to-image diffusion model used in this paper is the Stable Diffusion XL (SDXL) model [45]. The default resolutions of the generated images are 1024\(\times\)1024. The rank of LoRA is set as 16 for generative interpolation tasks and 32 for generation-based UDA and DG tasks. All experiments are conducted on an NVIDIA A100 GPU with three random trials.

For generative interpolation tasks, the training data utilized is sourced from the repository of DiffMorpher3 and the LoRAs from Hugging Face space "LoRA the Explorer"4. The training images can be found in the supplementary materials provided. For morphing in styles, given images in rayon and watercolor styles for training, we set \(t=0\) for training on the rayon images and \(t=1\) for training on the watercolor images, with the prompt being "An image". During the inference phase, by uniformly transitioning \(t\) from 0 to 1 and using the text prompt "A high-speed train", the generated results are shown in the second row of Fig. 4. For morphing in subject, given five images of cats and eight images of dogs, we set \(t=0\) for training on the cat images and \(t=1\) for training on the dog images, with the prompt being "A pet". During the inference phase, by uniformly transitioning \(t\) from 0 to 1 and using the text prompt "A pet on the lawn", the generated results are shown in the last row of Fig. 4.

Footnote 3: https://github.com/Kevin-thu/DiffMorpher/

Footnote 4: https://huggingface.co/spaces/multimodalart/LoraTheExplorer

For UDA tasks, We generate 50 images per category for the _Office31_ and _Office-Home_, and 1000 images per category for the _VisDA_ datasets. For images translated from the source domain to the target domain, we scale the long side of each source image to 1024 pixels, adjusting the short side proportionally. Following [46], the _ResNet-50_ is used as the backbone on the _Office31_ and _Office-Home_ datasets, and the _ResNet-101_[21] is used as the backbone on the _VisDA-2017_ dataset. The learning rate scheduler follows [13]. For MCC+Terra and ELS+Terra, we follow the settings as the original papers [28; 85].

For DG tasks, we generate 400, 160, and 400 images per category for the _PACS_, _Office-Home_, and _VLCS_ datasets, respectively. The dimension of parameter \(\bm{t}\) is set as two, with each dimension sampled from -2 to 2 at intervals of 0.1 to generate diverse samples. We employ _ResNet-50_ as the backbone and adopt the same training, evaluation protocols, and hyperparameter search results as outlined in [5; 65; 6]. _ResNet-50_ is also used as the backbone for \(\bm{t}\) prediction network \(g(\cdot)\).

\begin{table}
\begin{tabular}{c c c c} \hline \hline  & \multicolumn{2}{c}{_General_} & _Diagonal_ \\ \cline{2-3}  & Linear & Exponential & Cosine \\ \hline \(\mathcal{F}(W,t)\) & \(tW+I\) & \(\exp(tW)\) & \(\cos(tW)\) \\ \(\frac{d}{dt}\mathcal{F}(W,t)\) & \(W\) & \(W\cdot\exp(tW)\) & \(-\sin(tW)\) \\ \(\mathcal{F}(W,0)\) & \(I\) & \(J_{r}\) & \(I\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Possible forms of Terra and corresponding differentiable functions.

We provide three possible forms of Terra listed in Table 5, _i.e._, Linear, Exponential, and Cosine. We apply the Linear form of Terra for generative interpolation and UDA tasks and the "Cosine-Sine" form, a variant of "Cosine", for DG tasks. Specifically, the form of "Cosine-Sine" is \(\cos(tW)\) on the diagonal of \(K(t)\), and \(\sin(tW)\) at other positions. To provide more insights, we elaborate on the guiding principles behind the choice of these forms:

* **Linear**: The \(tW+I\) is the simplest form, related to a straight and steady flow, which is sufficient for two domains according to Theorem 1 and 2. Its constant velocity of weight changes ensures smooth morphing and is suitable for simple interpolating between two domains under the UDA setting.
* **Cosine-Sine**: This form is adopted because of the bounded range and non-linearity of trigonometric functions, preventing image collapse during generation and enabling a complex parameter manifold to capture relationships between multiple domains. We recommend using this form in complex scenarios, such as interpolating multiple domains in DG.
* **Exponential**: \(e^{tW}=I+\sum_{k=1}^{\infty}\frac{t^{k}}{k!}W^{k}\), implemented using "torch.matrix_exp", also defines a smooth curve in a high-dimensional manifold. This form is more expressive and suitable for handling multiple domains. Notably, it is related to three types of transformations: scalings, rotations, and shears [14].

Empirically, the "Cosine-Sine" form of Terra brings better performance for DG compared with the Linear form according to the results shown in Table 6. As can be seen, ERM+Terra with dimension 2 achieves the best average performance, thus we use 2 as the default dimension for DG tasks.

## Appendix C More Experimental Results

### Results on Generative Interpolation Tasks

The qualitative comparisons of image morphing using various methods are shown in Fig. 8. We perform more qualitative samples of our Terra in Fig. 9. These samples further demonstrate Terra's ability to handle morphing under various scenarios.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{_PACS_} \\ \cline{2-5}  & A & C & P & S & **Avg** \\ \hline ERM & 87.00\(\pm\)0.46 & 78.23\(\pm\)1.16 & 98.05\(\pm\)0.06 & 74.35\(\pm\)3.43 & 84.41 \\ ERM+Terra (dim1) & 88.29\(\pm\)1.35 & **82.36\(\pm\)**0.46 & 97.53\(\pm\)0.28 & 73.31\(\pm\)1.50 & 85.37 \\ ERM+Terra (dim2) & **89.51\(\pm\)**0.67 & 79.66\(\pm\)0.03 & **98.20\(\pm\)**0.00 & **78.64\(\pm\)**2.08 & **86.50** \\ ERM+Terra (dim3) & 89.26\(\pm\)1.70 & 81.72\(\pm\)2.22 & 97.94\(\pm\)0.30 & 76.11\(\pm\)1.11 & 86.26 \\ ERM+Terra (Linear) & 87.47\(\pm\)0.75 & 80.17\(\pm\)0.46 & 97.85\(\pm\)0.28 & 77.16\(\pm\)1.50 & 85.66 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Evaluation on the dimension of time variable \(\bm{t}\) and Linear form (dim2) of Terra on the _PACS_ dataset under the DG setting. The best is in bold.

Figure 8: Qualitative results of image morphing using various methods. “Terra + DiffM.” integrates Terra with DiffMorpher. As shown, our method generates smooth and natural intermediate images.

[MISSING_PAGE_EMPTY:22]

The complete results on the _VisDA_ dataset under the UDA setting are shown in Table 7. The results on the _Office31_ dataset under the UDA setting are shown in Table 8, and the results on the _VLCS_ dataset under the DG setting are shown in Table 9. As can be seen, Terra is still effective in the two datasets.

### Results with More Baselines

Moreover, the results with CoVi and PMTrans are shown in Table 10. Notably, Terra consistently improves performance in all tasks with those UDA methods, further verifying the effectiveness of our method.

### Comparison of Morphing Works

In addition, for a fair comparison of Terra's effectiveness in expanding source domains that generalize better, we include the comparison against off-the-shelf DG + morphing works on _Office-Home_. That is, we train a LoRA for each domain and adopt LoRA Interp./DiffMorpher to interpolate. The results shown in Table 11 verify the effectiveness of Terra, since Terra interpolates between domains instead of images and thus better models the distributions in two domains.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c} \hline \hline Method & Ar\(\rightarrow\)Cl & Ar\(\rightarrow\)Pr & Ar\(\rightarrow\)Rw & Cl\(\rightarrow\)Ar & Cl\(\rightarrow\)Pr & Cl\(\rightarrow\)Rw & Pr\(\rightarrow\)Ar & Pr\(\rightarrow\)Cl & Pr\(\rightarrow\)Rw & Rw\(\rightarrow\)Ar & Rw\(\rightarrow\)Cl & Rw\(\rightarrow\)Pr & **Avg** \\ \hline CoVi & 58.50 & 78.10 & 80.00 & 68.10 & 80.00 & 77.00 & 66.40 & 60.20 & 82.10 & 76.60 & 63.60 & 86.50 & 73.10 \\ CoVi+Terra & 64.56 & 80.65 & 83.36 & 71.45 & 81.03 & 80.77 & 70.83 & 64.86 & 84.07 & 76.76 & 64.19 & 87.18 & 75.81 \\ \hline PMTrans & 82.17 & 91.55 & 92.36 & 89.40 & 92.48 & 92.49 & 87.92 & 80.57 & 92.88 & 88.94 & 82.34 & 94.45 & 88.96 \\ PMTrans+Terra & **83.57** & **93.21** & **92.69** & **89.57** & **92.79** & **93.02** & **89.14** & **82.74** & **93.63** & **89.54** & **83.00** & **94.50** & **89.78** \\ \hline \hline \end{tabular}
\end{table}
Table 10: Comparative analysis with two baseline methods on _Office-Home_ under UDA setting.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Method & A\(\rightarrow\)W & D\(\rightarrow\)W & W\(\rightarrow\)D & A\(\rightarrow\)D & D\(\rightarrow\)A & W\(\rightarrow\)A & **Avg** \\ \hline ERM [58] & 77.07\(\pm_{0.11}\) & 96.60\(\pm_{0.00}\) & 99.20\(\pm_{0.00}\) & 81.08\(\pm_{1.22}\) & 64.11\(\pm_{0.15}\) & 64.01\(\pm_{0.11}\) & 80.35 \\ DANN [13] & 89.85\(\pm_{1.34}\) & 97.95\(\pm_{0.06}\) & 99.90\(\pm_{0.08}\) & 83.26\(\pm_{0.68}\) & 73.28\(\pm_{0.65}\) & 73.75\(\pm_{0.39}\) & 86.33 \\ AFN [72] & 91.82\(\pm_{0.63}\) & 98.77\(\pm_{0.07}\) & **100.00\(\pm_{0.00}\)** & 95.12\(\pm_{0.53}\) & 72.43\(\pm_{0.50}\) & 70.71\(\pm_{0.32}\) & 88.14 \\ CDAN [35] & 92.42\(\pm_{1.75}\) & 98.62\(\pm_{0.18}\) & **100.00\(\pm_{0.00}\)** & 91.44\(\pm_{1.19}\) & 74.61\(\pm_{0.79}\) & 72.80\(\pm_{0.45}\) & 88.32 \\ MDD [86] & 93.55\(\pm_{1.00}\) & 98.66\(\pm_{0.15}\) & **100.00\(\pm_{0.00}\)** & 93.92\(\pm_{0.10}\) & 75.29\(\pm_{0.68}\) & 73.95\(\pm_{0.18}\) & 89.23 \\ SDAT [46] & 91.32\(\pm_{1.83}\) & 98.83\(\pm_{0.12}\) & **100.00\(\pm_{0.00}\)** & 95.25\(\pm_{1.03}\) & 76.97\(\pm_{0.67}\) & 73.19\(\pm_{0.34}\) & 89.26 \\ MSGD [71] & **95.50\(\pm_{0.50}\)** & 99.20\(\pm_{0.30}\) & **100.00\(\pm_{0.00}\)** & 95.60\(\pm_{0.30}\) & 77.30\(\pm_{0.40}\) & 77.00\(\pm_{0.50}\) & 90.80 \\ \hline MCC [28] & 94.09\(\pm_{0.38}\) & 98.32\(\pm_{0.08}\) & 99.67\(\pm_{0.09}\) & 94.25\(\pm_{1.47}\) & 75.89\(\pm_{0.50}\) & 75.46\(\pm_{0.20}\) & 89.61 \\ MCC+Terra & 94.55\(\pm_{0.06}\) & 99.03\(\pm_{0.06}\) & **100.00\(\pm_{0.00}\)** & **96.46\(\pm_{0.09}\)** & 78.64\(\pm_{0.18}\) & 79.37\(\pm_{0.12}\) & **91.34** \\ ELS [85] & 93.84\(\pm_{0.51}\) & 98.78\(\pm_{0.06}\) & **100.00\(\pm_{0.00}\)** & 95.78\(\pm_{0.20}\) & 77.72\(\pm_{0.54}\) & 75.13\(\pm_{0.16}\) & 90.21 \\ ELS+Terra & 94.09\(\pm_{0.17}\) & **99.21\(\pm_{0.06}\)** & **100.00\(\pm_{0.00}\)** & 96.25\(\pm_{0.48}\) & **78.67\(\pm_{0.28}\)** & **79.45\(\pm_{0.11}\)** & 91.28 \\ \hline \hline \end{tabular}
\end{table}
Table 8: Transfer accuracies (%) on the _Office31_ dataset under the UDA setting. The best is in bold.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c}{_VLCS_} \\ \cline{2-6}  & C & L & S & V & **Avg** \\ \hline MIRO & 98.10\(\pm_{0.69}\) & 64.05\(\pm_{1.59}\) & 73.31\(\pm_{1.78}\) & 76.36\(\pm_{0.76}\) & 77.95 \\ ERM & 97.76\(\pm_{1.06}\) & 63.11\(\pm_{1.50}\) & 72.17\(\pm_{0.29}\) & 76.56\(\pm_{2.87}\) & 77.40 \\ ERM+Terra & 98.79\(\pm_{0.03}\) & 65.54\(\pm_{1.07}\) & 71.04\(\pm_{0.45}\) & 77.66\(\pm_{0.43}\) & 78.25 \\ SAGM & 98.35\(\pm_{0.36}\) & 65.29\(\pm_{0.43}\) & **75.22\(\pm_{1.09}\)** & 79.13\(\pm_{2.22}\) & 79.50 \\ SAGM+Terra & **99.21\(\pm_{0.53}\)** & **66.52\(\pm_{0.31}\)** & 73.95\(\pm_{0.30}\) & **80.80\(\pm_{0.31}\)** & **80.12** \\ SWAD & 98.74\(\pm_{0.22}\) & 62.70\(\pm_{0.43}\) & 74.09\(\pm_{0.94}\) & 75.64\(\pm_{1.35}\) & 77.79 \\ SWAD+Terra & 98.94\(\pm_{0.27}\) & 63.98\(\pm_{0.02}\) & 73.91\(\pm_{0.23}\) & 80.19\(\pm_{0.26}\) & 79.26 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Testing accuracies (%) on the _VLCS_ dataset under the DG setting. The best is in bold.

### SDXL Prior

To further highlight the design advantages of Terra, we conduct a comparison with data augmentation with SDXL's prior knowledge. Specifically, we design several methods to synthesize data based on the SDXL model and evaluate their effectiveness on UDA tasks:

1. SDXL (random): We use the prompt "A [CLASS]" to generate samples for each class, where [CLASS] denotes the placeholder for the label.
2. SDXL (styles): We first use the prompt "Generate 50 prompts describing diverse styles for image generation" to ask GPT-4, and then use the prompt "A [CLASS], an everyday object in office and home, in the style of [STYLE]" to generate samples, where [STYLE] denotes the placeholder for style prompts generated by GPT-4 (_e.g._ "Classic", "Modern").
3. SDXL (target): Based on (ii), we use the name of the target domain (_e.g._ "Clipart") to replace the [STYLE] as the new placeholder for exploring the SDXL prior on the target domain.
4. SDXL (target styles): We use the prompt "Generate 50 prompts describing [TARGET] style for image generation" to ask GPT-4 and obtain more detailed style prompts for synthesis.
5. SDXL (selected): Inspired by [26], we use a confidence-based activate learning method to filter out poor-quality and misclassified samples generated in (iv) and select valid samples.

The comparison results on _Office-Home_ for UDA are shown in Table 12. Terra outperforms the comparison methods, indicating that despite the boost in accuracy from target style design and active learning, the prior knowledge is insufficient to align with the downstream tasks. This issue can be further mitigated through finetuning with Terra, which demonstrates the design advantages of Terra.

## Appendix D Standard Deviations of Experiments

The standard deviations of three random experiments on the _Office-Home_, _VisDA_, and ablation studies under UDA setting are shown in Tables 13, 14, and 15, respectively. Table 16 presents the standard deviations on the _PACS_ and _OfficeHome_ datasets under DG setting.

## Appendix E Comparison with Other LoRA Variants

In cross-domain learning based on MoLE [69], the process can be viewed as first training LoRAs on different domains separately, followed by training a gating function to integrate the trained LoRAs. Although both MoLE and Terra are designed for diffusion model customization, they differ in several key aspects:

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Method & Ar & Cl & Pr & Rw & **Avg** \\ \hline SWAD & 66.08 & 57.37 & 79.58 & 80.49 & 70.88 \\ +DiffMorpher & 64.06 & 57.43 & 77.91 & 81.04 & 70.11 \\ +LoRA Interp. & 67.23 & 58.06 & 80.09 & 81.33 & 71.68 \\ +Terra & **68.02** & **58.31** & **80.56** & **82.03** & **72.23** \\ \hline \hline \end{tabular}
\end{table}
Table 11: Comparison of morphing works on _Office-Home_ using the off-the-shelf method (SWAD) under DG setting. Note that DiffMorpher exhibits lower performance due to the large gap between image pairs, even within the same class.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c} \hline \hline Method & Ar\(\sim\)Cl & Ar\(\sim\)Pr & Ar\(\sim\)Rw & Cl\(\sim\)Ar & Cl\(\sim\)Pr & Cl\(\sim\)Rw & Pr\(\sim\)Ar & Pr\(\sim\)Cl & Pr\(\sim\)Rw & Rw\(\sim\)Ar & Rw\(\sim\)Cl & Rw\(\sim\)Pr & **Avg** \\ \hline SDXL (random) & 56.88 & 73.64 & 80.38 & 69.18 & 73.64 & 80.40 & 68.93 & 56.54 & 80.38 & 68.93 & 56.54 & 73.64 & 69.92 \\ SDXL (styles) & 55.23 & 77.09 & 80.26 & 68.11 & 77.09 & 80.26 & 68.11 & 55.23 & 80.45 & 68.11 & 55.23 & 77.04 & 70.18 \\ SDXL (target) & 59.70 & 75.51 & 82.26 & 66.67 & 75.51 & **82.26** & 66.67 & 59.07 & 82.26 & 66.67 & 59.70 & 75.51 & 71.04 \\ SDXL (target styles) & 60.76 & 79.52 & 81.68 & 70.95 & 79.52 & 81.68 & 70.95 & 60.76 & 81.68 & 70.95 & 60.76 & 79.52 & 73.23 \\ SDXL (selected) & 61.63 & 79.81 & 82.19 & **71.98** & 79.73 & 81.82 & 71.69 & 61.58 & 82.07 & 72.76 & 62.15 & 80.42 & 73.99 \\ \hline Terra & **64.62** & **82.33** & **83.60** & 71.19 & **84.25** & 80.31 & **73.00** & **63.57** & **83.81** & **76.20** & **66.56** & **85.70** & **76.26** \\ \hline \hline \end{tabular}
\end{table}
Table 12: Comparison of target-like samples generation by SDXL prior on _Office-Home_ based on ELS under UDA setting.

**Objective**: MoLE focuses on combining multiple pre-trained LoRAs to achieve multi-concept customization, whereas Terra aims to learn a single adapter structure that can capture multiple domains and construct a domain flow for generation.

**Training**: MoLE only optimizes the gating function to preserve the characteristics of trained LoRAs on different domains, whereas Terra participates in the diffusion fine-tuning stage and aims to learn domain-general knowledge and domain-specific knowledge, allowing for control over different domains through a time variable.

\begin{table}
\begin{tabular}{l c c c c c|c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c|}{_PACS_} & \multicolumn{4}{c}{_OfficeHome_} \\ \cline{2-11}  & A & C & P & S & Ar & Cl & Pr & Rw \\ \hline MIRO [6] & 1.22 & 1.66 & 0.21 & 1.18 & 0.39 & 0.49 & 0.30 & 0.43 \\ CDGA [22] & 1.50 & 1.60 & 0.70 & 0.90 & 1.20 & 0.30 & 0.40 & 0.20 \\ ERM [58] & 0.46 & 1.16 & 0.06 & 3.43 & 0.72 & 0.63 & 0.34 & 0.49 \\ ERM+DomainDiff [36] & 1.60 & 0.00 & 0.00 & 0.90 & 0.40 & 0.60 & 0.60 & 0.90 \\ ERM+Terra & 0.75 & 1.57 & 0.37 & 3.47 & 0.15 & 0.74 & 0.15 & 0.14 \\ SAGM [65] & 0.86 & 1.48 & 0.74 & 2.49 & 0.33 & 0.79 & 0.38 & 0.06 \\ SAGM+Terra & 0.12 & 0.61 & 0.30 & 1.86 & 0.67 & 0.63 & 0.58 & 0.36 \\ SWAD [5] & 0.08 & 0.73 & 0.04 & 0.38 & 0.17 & 0.17 & 0.10 & 0.65 \\ SWAD+Terra & 0.10 & 0.03 & 0.28 & 0.83 & 0.28 & 0.21 & 0.45 & 0.37 \\ \hline \hline \end{tabular}
\end{table}
Table 16: The standard deviation on the _PACS_ and _OfficeHome_ datasets under DG setting.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c} \hline \hline Method & \multicolumn{1}{c}{\(\mathrm{aero}\)} & \multicolumn{1}{c}{\(\mathrm{bicycle}\)} & \multicolumn{1}{c}{bus} & \multicolumn{1}{c}{car} & \multicolumn{1}{c}{horse} & \multicolumn{1}{c}{knife} & \multicolumn{1}{c}{motor} & \multicolumn{1}{c}{person} & plant & \multicolumn{1}{c}{skate} & train & truck \\ \hline ERM [58] & 9.90 & 2.64 & 3.26 & 2.20 & 1.35 & 3.60 & 1.41 & 1.03 & 1.80 & 3.97 & 0.79 & 0.67 \\ DANN [1] & 0.39 & 1.94 & 0.38 & 2.80 & 0.80 & 3.40 & 0.76 & 0.86 & 0.72 & 2.00 & 0.32 & 2.72 \\ AFN [72] & 0.69 & 3.84 & 1.80 & 2.55 & 1.48 & 2.51 & 0.48 & 2.08 & 2.47 & 3.93 & 1.11 & 1.27 \\ CDAN [35] & 0.38 & 3.72 & 2.55 & 1.36 & 0.53 & 0.52 & 0.14 & 2.58 & 0.67 & 0.49 & 2.61 & 2.43 \\ MDD [86] & 2.40 & 9.46 & 1.18 & 0.66 & 0.85 & 4.01 & 0.65 & 1.81 & 1.21 & 4.30 & 1.58 & 0.33 \\ SDAT [46] & 1.40 & 2.64 & 1.60 & 1.67 & 0.48 & 0.92 & 0.82 & 0.24 & 0.78 & 0.84 & 1.36 & 0.60 \\ \hline MCC [28] & 0.12 & 0.92 & 2.91 & 0.39 & 0.28 & 0.54 & 0.80 & 0.87 & 0.15 & 0.77 & 0.55 & 2.25 \\ MCC+Terra & 0.21 & 0.59 & 0.12 & 0.69 & 0.60 & 0.60 & 0.56 & 0.35 & 0.40 & 0.35 & 0.47 & 0.88 \\ ELS [85] & 0.93 & 1.20 & 1.39 & 0.47 & 0.15 & 0.95 & 1.38 & 0.73 & 1.59 & 1.02 & 1.35 & 0.27 \\ ELS+Terra & 0.34 & 0.79 & 0.41 & 1.31 & 0.06 & 0.39 & 0.85 & 0.43 & 0.92 & 0.64 & 0.26 & 0.19 \\ \hline \hline \end{tabular}
\end{table}
Table 15: The standard deviation of three random experiments on _VisDA_ under UDA setting.

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline Method & \multicolumn{1}{c}{\(\mathrm{aero}\)} & \multicolumn{1}{c}{bicycle} & \multicolumn{1}{c}{bus} & \multicolumn{1}{c}{car} & \multicolumn{1}{c}{horse} & knife & motor & person & plant & \multicolumn{1}{c}{skate} & train & truck \\ \hline ERM [58] & 9.90 & 2.64 & 3.26 & 2.20 & 1.35 & 3.60 & 1.41 & 1.03 & 1.80 & 3.97 & 0.79 & 0.67 \\ DANN [1] & 0.39 & 1.94 & 0.38 & 2.80 & 0.80 & 3.40 & 0.76 & 0.86 & 0.72 & 2.00 & 0.32 & 2.72 \\ AFN [72] & 0.69 & 3.84 & 1.80 & 2.55 & 1.48 & 2.51 & 0.48 & 2.08 & 2.47 & 3.93 & 1.11 & 1.27 \\ CDAN [35] & 0.38 & 3.72 & 2.55 & 1.36 & 0.53 & 0.52 & 0.14 & 2.58 & 0.67 & 0.49 & 2.61 & 2.43 \\ MDD [86] & 2.40 & 9.46 & 1.18 & 0.66 & 0.85 & 4.01 & 0.65 & 1.81 & 1.21 & 4.30 & 1.58 & 0.33 \\ SDAT [46] & 1.40 & 2.64 & 1.60 & 1.67 & 0.48 & 0.92 & 0.82 & 0.24 & 0.78 & 0.84 & 1.36 & 0.60 \\ \hline MCC [28] & 0.12 & 0.92 & 2.91 & 0.39 & 0.28 & 0.54 & 0.80 & 0.87 & 0.15 & 0.77 & 0.55 & 2.25 \\ MCC+Terra & 0.21 & 0.59 & 0.12 & 0.69 & 0.60 & 0.60 & 0.56 & 0.35 & 0.40 & 0.35 & 0.47 & 0.88 \\ ELS [85] & 0.93 & 1.20 & 1.39 & 0.47 & 0.15 & 0.95 & 1.38 & 0.73 & 1.59 & 1.02 & 1.35 & 0.27 \\ ELS+Terra & 0.34 & 0.79 & 0.41 & 1.31 & 0.06 & 0.39 & 0.85 & 0.43 & 0.92 & 0.64 & 0.26 & 0.19 \\ \hline \hline \end{tabular}
\end{table}
Table 13: The standard deviation of three random experiments on _OfficeHome_ under UDA setting.

**Expressiveness**: MoLE uses a separate gating function for each LoRA layer, which requires entropy-based balancing to resolve conflicts when combining multiple LoRAs. In contrast, Terra achieves domain adaptation through a single time variable \(t\), making it more stable. For two-domain interpolation, Terra and MoLE have similar expressiveness. Considering two domains with time variables \(t_{1}\) and \(t_{2}\), we have

\[\Delta W(\alpha t_{1}+(1-\alpha)t_{2}) =B\mathcal{K}(\alpha t_{1}+(1-\alpha)t_{2})A\] (31) \[=(\alpha t_{1}+(1-\alpha)t_{2})BWA+BA\] \[=\alpha\Delta W(t_{1})+(1-\alpha)\Delta W(t_{2}).\]

This is equivalent to the linear arithmetic composition in MoLE.

Finally, the relation between MoLE and Terra is similar to that between Gaussian Mixture Model (GMM) and Gaussian Process (GP). GMM composes a complex distribution by multiple Gaussian distributions, and GP is a distribution over functions within a continuous domain (such as time). Analogously, MoLE excels at composition capabilities, while Terra excels at constructing a manifold.

## Appendix F Broader Impact and Ethics Statements

The ability to generate realistic images can be misused to create deepfakes or other deceptive content, potentially leading to misinformation and privacy violations. While our work has the potential to advance the field of PEFT and generation-based cross-domain learning, it is crucial to address the associated risks, particularly in terms of ethical considerations.

## Appendix G Limitation and Failure Cases

Despite showing promising results in data-augmentation-based UDA and DG, Terra has some limitations. Generating images via Terra for data augmentation requires additional storage space. For UDA tasks, we generate target domain samples and transform source domain samples into the target domain, without utilizing Terra's ability to generate intermediate domains. Note that the intermediate domain can be leveraged by using methods in gradual domain adaptation [31], but we have not explored this due to different settings. We leave it for future studies. Additionally, while we have adapted to downstream domains through fine-tuning, our model may still be influenced by the prior of the foundation model to some extent.

We acknowledge that a small number of generated images may exhibit poor quality due to the conflict between SD prior knowledge and the knowledge required for downstream tasks. We showcase some failure cases in Fig. 11. However, the number of those poor-quality images is small, and it does not affect the overall performance of the model.

Figure 11: Illustration of failure cases in generated samples.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See Abstract and last two paragraphs in Introduction. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Appendix G. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: See Section 3.2 and Appendix A.  Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See supplemental material for the code and Appendix B for the experimental details. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes] Justification: The code and data can be found in the supplemental material. Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The implementation details can be found in Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The standard deviation of three random experiments with different seeds are shown in Table 8, 9 and 13-16. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). * The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.) * The assumptions made should be given (e.g., Normally distributed errors).

* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We have read and complied with the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Appendix F. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.

* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: There are no such risks in this paper. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: See the cited reference and supplemental material. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.

* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This paper dos not release new assets. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Not human subjects research. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Not human subjects research. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.