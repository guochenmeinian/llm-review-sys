# Image Textualization : An Automatic Framework for Creating Accurate and Detailed Image Descriptions

 Renjie Pi\({}^{1}\), Jianshu Zhang\({}^{2}\), Jipeng Zhang\({}^{1}\), Rui Pan\({}^{1}\), Zhekai Chen\({}^{3}\), Tong Zhang\({}^{4}\)

\({}^{1}\)The Hong Kong University of Science and Technology

\({}^{2}\)Wuhan University, \({}^{3}\)Zhejiang University

\({}^{4}\)University of Illinois Urbana-Champaign

{rpi,rpan,jzhanggr}@ust.hk, jianshu.zhang@whu.edu.cn, chenzhekai@zju.edu.cn

tongzhang@tongzhang-ml.org

 Equal Contribution. Code and data are available at the following links:

https://github.com/sterzhang/image-textualization/

https://huggingface.co/datasets/Sterzhang/image-textualization/.

The code and data are released under MIT and apache2.0 licenses, respectively.

###### Abstract

Image description datasets play a crucial role in the advancement of various applications such as image understanding, text-to-image generation, and text-image retrieval. Currently, image description datasets primarily originate from two sources. One source is the scraping of image-text pairs from the web. Despite their abundance, these descriptions are often of low quality and noisy. Another is through human labeling. Datasets such as COCO are generally very short and lack details. Although detailed image descriptions can be annotated by humans, the high annotation cost limits the feasibility. These limitations underscore the need for more efficient and scalable methods to generate accurate and detailed image descriptions. In this paper, we propose an innovative framework termed **Image Textualization (IT)**, which automatically produces high-quality image descriptions by leveraging existing multi-modal large language models (MLLMs) and multiple vision expert models in a collaborative manner, which maximally convert the visual information into text. To address the current lack of benchmarks for detailed descriptions, we propose several benchmarks for comprehensive evaluation, which verifies the quality of image descriptions created by our framework. Furthermore, we show that LLaVA-7B, benefiting from fine-tuning on IT-curated descriptions, acquire improved capability to generate richer image descriptions, substantially increasing the length and detail of their output with less hallucination.

## 1 Introduction

In recent years, multi-modal large language models (MLLMs) have witnessed significant progresses. Such models start to reach super-human performances in a variety of areas, such as image understanding [2; 15; 34; 41; 80], text-to-image generation [13; 54; 55; 57; 58] and text-image retrieval [33; 53; 69; 77]. One of the primary reasons for these successes is the training data, which consists of image-description pairs. Recent studies highlight that the quality of image descriptions is crucial for MLLM performance. For example, Yin et al. [74] note that low-quality descriptions often cause hallucinations in image understanding tasks, while Betker et al. [4] show that detailed descriptions with richer visual concepts significantly enhance generation models' performance. Thus,curating high-quality image description datasets is essential for improving various downstream applications.

A high-quality image description should convey the same information as the corresponding image, effectively textualizing the visual content. However, current datasets often fall short. They mainly come from two sources: web-scraped image-text pairs [8; 60; 61], which are large-scale but low-quality and noisy, and human-labeled datasets [31; 39; 52], which lack in-depth details and are costly to produce. Consequently, a significant gap remains between the information in an image and its textual description.

To address the shortcomings of existing image description datasets, recent advancements in MLLMs have shown remarkable potential for generating descriptions. Dalle-3 [4] has made an early attempt to train text-to-image diffusion models using descriptions produced via MLLMs, which shows improved quality of the generated images. However, MLLMs possess several weaknesses, such as the well known visual hallucination problem [51; 74; 75] and lack of fine-grained details [12]. Even the most powerful MLLMs, such as GPT4-V [44], exhibit these weaknesses. Therefore, relying solely on MLLMs to generate datasets still has significant limitations.

Meanwhile, we notice remarkable progress in areas of computer vision, such as object detection [42; 72; 73], dense captioning [43; 67] and instance segmentation [30]. Compared with MLLMs that are trained with low-resolution images (336x336 by default setting of CLIP [53]) and image-level annotations, these vision expert models are trained with high-resolution images and fine-grained object-level annotations specifically catering for perception tasks, which makes them capable of identifying detailed content. However, such models generally do not possess holistic understanding capabilities, so constructing descriptions solely depending on such models is not practical. Consequently, an intriguing thought arises: Can we combine the understanding capability of MLLMs with the perception power of vision experts to generate high-quality descriptions that are both rich in details and free from hallucinations.

Building upon the above intuition, in this paper, we propose **Image Textualization(IT)**, a framework for automatically creating high-quality image descriptions. Specifically, our framework consists of three phases: 1) _Holistic Textualization_: We leverage the MLLM to create the Reference Description, which, despite lacking details and containing hallucinations, provides the basic structure not only for the visual information but also for the linguistic expression. 2) _Visual Detail Texturalization_: Then, we resort to the powerful perception capabilities of vision expert models to extract fine-grained object-level information that is converted into text format. This phase extracts multiple details from the image-side and identifies hallucinations contained in the Reference Description. 3) _Textualized Recaptioning_. Finally, we leverage LLMs' superior understanding and reasoning capabilities to

Figure 1: Visualization of our Image Textualization. Compared with the MLLM-generated description, our description incorporates more visual details and significantly less hallucinations. The shared details, newly added details, hallucinations, and positional descriptions are all marked with different colors.

produce accurate and detailed descriptions based on the textualized information from the first two phases, allowing the LLMs to describe the image without "seeing" it. This approach avoids the weaknesses of MLLM-based recaptioning. As shown in Figure 1, our IT framework is able to create image descriptions that are richer in details and free from hallucination.

For a comprehensive evaluation of our framework, we first construct three benchmarks, namely DID-Bench, D2I-Bench and LIN-Bench, which evaluate the description quality from multiple aspects. Then, we conduct a series of experiments to validate the quality of IT-generated descriptions on the proposed benchmarks. Afterward, we verify that fine-tuning MLLMs with our generated data enhances their capabilities. Lastly, we perform linguistic evaluations and provide statistical analysis of our released dataset.

To summarize, we make the following contributions in this paper:

* We propose **Image Textualization**, a framework that automatically generates detailed image descriptions without human intervention, leveraging the multimodal understanding of MLLMs, the fine-grained perception of visual experts, and the reasoning power of LLMs.
* We create evaluation benchmarks and conduct extensive experiments to validate the effectiveness of our framework. The results demonstrate that the generated image descriptions accurately capture rich visual details.
* Using our Image Textualization framework, we curate a large-scale high-quality image description dataset termed **IT-170K**. To facilitate future research, we release all the source code and our generated dataset to the community.

## 2 Related Work

Image Description DatasetsImage-text paired description datasets are valuable assets for a variety of downstream tasks, such as image understanding [2; 15; 41; 66; 80], text-to-image generation [13;

Figure 2: The framework of **Image Textualization (IT)**, which consists of three phases: (A) **Holistic Textualization** (Sec. 3.1) utilizes a MLLM to generate a “_Reference Description_” that provides a basic structure; (B) **Visual Detail Textualization** (Sec. 3.2) identifies the hallucinations and captures details in the image via a variety of vision experts, then transforms them to text format. (C) **Textualized Recaptioning** (Sec. 3.3), which leverages LLM and textualized results from (A) and (B) to re-generate the image captions that are both rich in details and free from hallucination.

54, 55, 57, 58] and text-image retrieval [33, 53, 69, 77]. Image description datasets primarily curated from three sources: 1) web-scraped image-text pairs, such as CC3M [61], CC12M [8] and LAION [60], which are large-scale, but often contain low quality and noisy descriptions. 2) human-labeled description datasets, such as COCO [39], Flickr30k [52] and Visual Genome [31], which typically have limited quantity and often feature short and incomplete captions due to the costly annotation process. The lack of high-quality image description datasets may cause unsatisfactory performance or out of domain problem [9, 10, 78, 11]. To address this gap, in this paper, we propose a framework that automatically generates high-quality, detailed image descriptions without human intervention.

Multi-Modal Large Language Model.In recent years, great advancements have been made in the development of large language models (LLMs) [3, 6, 14, 25, 45, 59, 62, 65]. These advancements have greatly elevated the capabilities of language understanding and generation, showcasing superhuman proficiency across diverse tasks. Concurrently, the success of LLMs has inspired explorations into the incorporation of visual modality into LLM, leading to the emergence of multi-modal large language models (MLLMs) [2, 15, 16, 20, 21, 34, 41, 44, 48, 49, 50, 63, 80]. These models have demonstrated remarkable abilities in engaging in dialogue based on visual inputs and generating image descriptions containing rich details. Despite the success of MLLMs, their inherent weaknesses such as low image resolution and insufficient training data, leads to problems such as incomplete description and object hallucination [35, 51, 64, 74, 75]. Therefore, directly generating image descriptions with MLLMs remains problematic.

Vision Expert ModelsThere is a variety of sub-fields in computer vision that specialize on different tasks. Object detection aims at localizing objects in images [7, 17, 22, 40, 56, 70, 71, 81]. Recently, open-vocabulary object detection has achieved great progress at localizing objects based on the semantics of text queries [24, 36, 42, 72, 73]. Dense captioning aims to produce short descriptions for each object present in the input images [26, 73, 26]. Prompt-based segmentation models enable producing segmentation mask for objects in the image based on the input prompts, which could be in the form of point or bounding box [27, 30, 82]. Depth estimation enables the prediction of distance between objects and the camera [29, 68]. In this paper, we harness the capabilities of the vision experts to provide object-level information for constructing high-quality image descriptions.

## 3 Method

Image Textualization automatically produces high-quality image descriptions. Given an image, the powerful MLLM first produces a template description capturing the holistic image content. Then, a variety of vision expert models collaborate to extract detailed object information, which may be missing from the template description. Finally, we harness the powerful LLMs to re-generate the description based on the holistic information and fine-grained details. As shown in figure 2, our framework is divided into three phases, which we will elaborate in the following sections.

### Phase 1: Holistic Textualization

Current state-of-the-art MLLMs [15, 41, 44] excel at producing image descriptions that contain richer information and contextual understanding compared with those generated by conventional captioning models [18, 23, 34, 66]. Therefore, we first leverage MLLM-generated descriptions to textualize the holistic content of the image, as shown in Figure 2 Phase(A). Despite weaknesses such as hallucinations and lack of details, this description can serve as a basic template with a relatively good structure for describing the image. Hereafter, we refer to this description as the _"Reference Description"_.

This Reference Description serves two key purposes. Firstly, in terms of visual information, it includes the main objects present in the image and the contextual information of the scene. These elements act as "anchors" that guide the incorporation of more details in the subsequent phases. Secondly, from a linguistic expression perspective, the inherent understanding and logical capabilities of MLLMs help to form well-organized descriptions. For example, a Reference Description typically includes an overall description of the image, followed by details about the main objects, then concludes with a summarizing sentence. Compared to traditional captioning models, this kind of descriptions are more logically structured and naturally expressed, which are crucial factors for the quality of descriptions.

### Phase 2: Visual Detail Textualization

Reference descriptions generated in the first phase generally lack in visual details and contain hallucinations. In this phase, we utilize vision expert models to extract information from both the image and reference description. From the image, we capture more visual details, and from the reference description, we identify the hallucinated contents. Finally, we textualize the fine-grained visual information and hallucinated objects, as shown in Figure 2 Phase(B) rightmost grey box.

#### 3.2.1 Hallucination Detection

As outlined in Algorithm 1, to identify hallucinations existed in the reference description, we first extract object entities (object nouns and phrases) from it. Here, we leverage the strong instruction following ability of the Large Language Model (LLM). Specifically, we carefully design an entity-extraction prompt and manually annotate in-context learning examples to improve instruction following of the LLM. Afterward, we utilize an open-set object detector (e.g., Grounding Dino [42]) to verify each of these extracted entity phrases against objects in the image. Any hallucinated object phrases, which are not found in the image, are tagged as "Hallucination" for removal in the later phase.

#### 3.2.2 Fine-Grained Objects Annotation

Dense Caption Generation.To identify objects that are potentially missing in the original description, we resort to a dense captioner (DC) [67], which not only provides accurate bounding boxes indicating object locations, but also associate them with basic attributes, such as object type, shape and color. Compared with object detectors that only predict the object category (e.g., cat), DC provides a more detailed description such as "a grey and white cat"; compared with conventional captioning models that only predicts image-level captions, DC is able to predict descriptions for all the visible objects in the image. These appealing properties make DC a suitable choice for maximizing the textualization of objects' information, which is beneficial for the subsequent recaptioning phase.

```
0: An input image \(\mathcal{I}\) with size \(H\times W\), dense caption model \(DC\), a segment anything model SAM, a monocular depth estimator parameterized by \(\mathcal{F}(\cdot)\).
1: Initialize FinegrainedInfo as Empty
2:\(\mathcal{B},\mathcal{P}\leftarrow\textit{DC}(\mathcal{I})\)# get object boxes and phrases
3:\(\mathcal{M}\leftarrow\textit{SAM}(\mathcal{I},\mathcal{B})\)# obtain object masks
4:\(\mathcal{D}\leftarrow\mathcal{F}(\mathcal{I})\)# obtain image depth map
5:for each object mask and phrase \(m_{i},p_{i}\in[\mathcal{M},\mathcal{P}]\)do
6:\(d_{i}\leftarrow\frac{\sum(m_{i}\odot\mathcal{D})}{m_{i}}\)# obtain object depth
7:\(s_{i}\leftarrow\frac{\sum m_{i}}{H\times W}\)# obtain object size
8: Append \(\{p_{i},d_{i},s_{i}\}\) to FinegrainedInfo
9:endfor
10:Output: FinegrainedInfo ```

**Algorithm 2** Fine-grained Object Annotation

Spatial Information Collection.Now, we have obtained the dense captions of various objects in the image along with their bounding box coordinates. However, the textualized information still falls short compared to the original image's information. The most critical reasons for this is that the current textualization can only convey the relative left-right relationships of objects on a 2D plane, and can lead to mistakes in recaptioning. For example, consider an image with a car in the background and a person in the foreground. Their bounding boxes might have very close coordinates. If we move to the Recaptioning phase with just this information, the LLM might use its logical capabilities to inaccurately describe the scene as "a person standing next to a car". This happens because the current textualization fails to capture the 3D spatial context, such as depth (which indicates the front-back relationships of objects), which are crucial for an accurate and comprehensive image description.

As shown in Algorithm 2, we derive this depth information by first obtaining a distance map using a monocular depth prediction model, where the value for each pixel indicates its distance from the camera. Then, we generate object segmentation masks with SAM [30] using the object bounding boxes generated by the dense captioner, which gives the exact pixels of the objects. Finally, the object depth is obtained by averaging the depth values within its corresponding segmentation mask.

In this way, the textualized information can convey the distance between the object and the camera, effectively transforming the 2D dense captions into 3D ones.

In this process, we remark the two important factors: 1) **Utilizing Pixel Masks for Size Calculation**: One naive way to calculate object size is using the bounding boxes. However, bounding boxes only provide two opposite corners, which can be inaccurate for irregularly shaped objects (e.g., a long stick). If we solely rely on bounding box's coverage as the object's size, it can lead to severe overestimation. Hence, it is crucial to use pixel-wise masks to accurately represent the object's size. 2) **Normalization**: We normalize the bounding box coordinates and depth scores to ensure that the values are relative, which facilitates the LLMs to adapt to different images during recaptioning phase.

### Phase 3: Textualized Recaptioning.

In this phase, we utilize the comprehensive information gathered from the previous phases that has been transformed into a textual representation, to reconstruct the image description via an LLM. The inclusion of both a holistic description and extensive object-level information enables the LLM to accurately interpret the entire image and its constituent objects, eliminating the need for direct visual input. We demonstrate the effectiveness of our approach in the appendix, where we carefully design prompts and provide few-shot examples to guide the LLM's generation process. This ensures that the LLM can effectively incorporate novel objects while minimizing the presence of hallucinated content.

## 4 Experiments

OverviewDue to lack of standard evaluation benchmarks for long image descriptions, we first propose DID-Bench, D2I-Bench and LIN-Bench for comprehensively evaluating detailed descriptions(Sec. 4.1). Then, we conduct a series of experiments to validate the effectiveness of our Image Textualization that can generate high-quality descriptions(Sec. 4.2). Afterward, we verify that training MLLMs with the data generated by our framework enhances their capabilities (Sec. 4.3). Lastly, we perform linguistic evaluations and provide statistical analysis of our released dataset (Sec. 4.4).

### Benchmarks and Evaluations

DID-BenchDetailed Image Description Bench (_DID-Bench_) contains 200 samples via the following steps: 1) First, we utilize MLLMs to generate Reference Descriptions. To avoid the bias introduced by different MLLMs' output habits, we employ GPT4-V for 100 samples and LLaVA for the remaining 100 samples when generating the Reference Descriptions. 2) Then, we manually check the correctness of these descriptions, add missing details, and remove hallucinated content to establish the human-labeled ground truth descriptions. We later refer to the partition using LLaVA Reference Descriptions to get ground truth as GT-[LLaVA], and the other partition as GT-{GPT4-V}.

We adopt reference-based metrics for image descriptions including BLEU [47], ROUGE-L [38], METEOR [32], SPICE [1] and WMD [28]. These metrics evaluate various aspects of the generated descriptions, such as n-gram overlap, recall, precision, semantic content, and overall similarity to human-labeled descriptions. The details of these metrics are introduced in the appendix.

D2I-BenchWe propose Description-to-Image-Bench (D2I-Bench) to evaluate the completeness of image information captured by descriptions. Firstly, we feed the descriptions to a pre-trained text-to-image model (e.g., PixArt [13]) and obtain the generated images. Then, we extract the image

\begin{table}
\begin{tabular}{c|c|c c c c|c c c c} \hline \hline GroundTruth & Description & BLEU-1 & BLEU-2 & BLEU-3 & BLEU-4 & CIDEr & METEOR & ROUGE & SPICE & WMD \\ \hline \multirow{3}{*}{GT-[LLaVA]} & [LLaVA] & 12.90 & 8.64 & 5.80 & 4.09 & 0.00 & 12.84 & 22.69 & 23.08 & 43.82 \\  & \(\Pi^{\prime}\)-[LLaVA] & **25.71** & **17.53** & **12.06** & **8.08** & **2.34** & **17.09** & **26.10** & **26.10** & **46.49** \\ \cline{2-11}  & (GPT4-V) & 29.05 & 15.23 & 7.64 & 4.15 & 1.93 & 15.92 & 20.06 & 19.84 & 42.79 \\  & \(\Pi^{\prime}\)-[GPT4-V] & **36.20** & **19.97** & **10.75** & **6.23** & **7.64** & **18.56** & **21.34** & **22.35** & **43.81** \\ \hline \multirow{3}{*}{GT-[GPT4-V]} & [LLaVA] & 9.80 & 5.16 & 2.54 & 1.35 & 0.00 & 9.83 & 15.93 & 13.75 & 37.93 \\  & \(\Pi^{\prime}\)-[LLaVA] & **21.86** & **12.17** & **6.67** & **3.94** & **1.18** & **13.80** & **18.74** & **17.86** & **40.12** \\ \cline{1-1} \cline{2-11}  & (GPT4-V) & 45.26 & 38.77 & 34.42 & 31.18 & 6.08 & 26.63 & 50.85 & 52.21 & 58.52 \\ \cline{1-1} \cline{2-11}  & \(\Pi^{\prime}\)-[GPT4-V] & **57.38** & **48.73** & **43.02** & **38.89** & **36.67** & **30.89** & **54.36** & **55.20** & **61.23** \\ \hline \hline \end{tabular}
\end{table}
Table 1: Evaluation of image descriptions on _DID-Bench_. Descriptions generated by our IT outperform the ones generated by MLLMs by a significant margin across different metrics.

embeddings for both the original image and the generated image using the image encoder from a pre-trained CLIP [53]. Finally, we calculate the cosine similarities between the image embeddings. A higher similarity score indicates that the description effectively captures the image details, resulting in generated images that closely resemble the originals.

LIN-BenchTo fully evaluate the readability and linguistic features of descriptions, we propose Linguistic Bench (LIN-Bench), which adopt metrics such as ARI, FK, and SMOG. ARI tends to be higher if there are more words with many characters in the sentence, while FK and SMOG place more emphasis on the number of multi-syllable words. More detailed descriptions tend to achieve higher values for these metrics.

PopePOPE benchmark [37] evaluates the level of hallucination suffered by the MLLM, which comprises of questions regarding the existence of objects in the image, and associated with short answers such as "yes" or "no".

### Image Description Quality Evaluation

DID-Bench ResultsTo verify the effectiveness of our Image Textualization annotation framework for generating detailed and accurate image descriptions, we compare the quality of descriptions generated by our Image Textualization with the ones directly produced by the MLLMs. As shown in Table 1, we observe significant gain across all the metrics for different MLLMs and ground-truth annotations. Interestingly, we observe that the evaluation results of IT-generated descriptions also dependent on the MLLM used in the holistic textualization phase. This is because the evaluation metrics not only account for the visual correctness, but also the styles of the descriptions, such as pronouns and prepositions. Therefore, to exclude the impact of language bias, it is crucial to conduct evaluation using GT annotations with different styles.

D2I-Bench ResultsAs shown in table 4, the descriptions generated by our IT framework results in images that have higher similarity scores with the original images than COCO's descriptions and MLLM-generated descriptions. In figure 3, we provide qualitative examples to show the IT-generated

Figure 3: D2I-Bench visualization. IT-generated descriptions capture more fine-grained image details, which leads to generated images more similar to the original images.

descriptions leads to images that bear closer resemblance as the originals. These results demonstrate the effectiveness of our framework for accurately capturing the details of the image content.

### MLLM Tuning Evaluation

DID-Bench ResultsIn Table 3, we compare the quality of image descriptions generated by 1) the original LLaVA-7B model; 2) LLaVA fine-tuned with MLLM-generated descriptions and 3) LLaVA fine-tuned with descriptions produced by Image Textualization. We observe that the MLLM fine-tuned using IT-curated dataset consistently outperforms other baselines across all metrics and GT annotations. We observe the following phenomena: 1) the scores on GT-LLaVA is mostly higher than that of GT-GPT4V, which is response style of LLaVA; 2) For each GT split, IT-tuned LLaVA outperforms the baseline and the MLLM-tuned LLaVA by a large margin; 3) from the evaluation on combined GT, we observe IT-LLaVA's effectiveness approaches that of GPT4-V, while IT-GPT4-V still surpass all counterparts significantly. This indicates that Image Textualization has the potential to close the gap between the capability of different MLLMs.

POPE and LIN-Bench ResultsOn the left side of Table 2, we evaluate the hallucination of MLLMs tuned with different image description data. We observe that tuning with IT-generated descriptions leads to the most significant alleviation of hallucination. On the right side, we also show the results on LIN-Bench, which demonstrates that tuning with IT-generated descriptions results in most gain in producing descriptions containing richer details.

### Linguistic-based Evaluation

Statistical AnalysisWe summarize the statistics of image descriptions generated by Image Textualization and MLLM baselines, respectively. In Table 5, we show that the IT-generated descriptions contain more words and sentences. In the figure on the right, we show the counts for different types of words, which demonstrates that the IT-generated descriptions contain richer words such as nouns, verbs and adjectives, indicating these descriptions contain denser information.

LIN-Bench ResultsAs demonstrated in Table 6. Compared with MLLM-generated descriptions, IT-generated descriptions results in higher scores across all metrics, suggesting that these descriptions contain richer details.

## 5 Prompt Design

Prompt design plays a crucial role in our proposed framework, which enables the collaboration between different expert models and results in more accurate and detailed image descriptions. In figure 4, we compare the recaption results without fine-grained object annotation and in-context examples and observe that object annotation leads to more detailed and accurate description, while in-context

\begin{table}
\begin{tabular}{c|c|c c c c|c c c c} \hline \hline \multirow{2}{*}{Tuning Data} & \multirow{2}{*}{Num} & \multicolumn{4}{c}{POPE} & \multicolumn{4}{c}{LIN-Bench} \\  & & Adv & Rand & Popular & Average & ARI & FK & SMOG & Average \\ \hline / & - & 79.13 & 85.70 & 88.93 & 84.59 & 8.80 & 8.48 & 10.93 & 9.40 \\ \hline \{LLaVA\} & \multirow{2}{*}{10k} & 79.60 & 86.16 & 89.56 & 85.11 & 8.77 & 8.45 & 10.91 & 9.38 \\ IT-\{LLaVA\} & & **81.37** & **87.40** & **90.63** & **86.47** & **9.99** & **9.48** & **11.3** & **10.26** \\ \hline \{GPT4-V\} & \multirow{2}{*}{10k} & 83.46 & **88.03** & 90.23 & 87.24 & 8.78 & 8.53 & 11.14 & 9.51 \\ IT-\{GPT4-V\} & & **83.60** & 88.00 & **90.47** & **87.36** & **10.03** & **10.89** & **11.89** & **10.94** \\ \hline \{GPT4-V\} & \multirow{2}{*}{50k} & 81.96 & 88.03 & 90.13 & 86.71 & 9.57 & 8.89 & 11.08 & 9.85 \\ IT-\{GPT4-V\} & & **83.30** & **88.20** & **90.80** & **87.43** & **10.47** & **9.65** & **11.62** & **10.58** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Evaluation on POPE and Lin-bench. LLaVA trained with IT-generated data produces richer image descriptions and demonstrates alleviated hallucination.

[MISSING_PAGE_EMPTY:9]

Limitation

While our framework demonstrates promising robustness against hallucinations and enhances downstream performance, several limitations warrant consideration. Firstly, despite claims of mitigating misinformation, some provided examples exhibit inaccuracies--for instance due to the performance bottleneck of vision expert models.

Additionally, our approach relies exclusively on model-based techniques, which introduces inherent limitations related to model diversity. Specifically, models like GPT-4V exhibit a strong positivity bias, often describing images in overly favorable terms such as "cozy" and "beautiful." This lack of diversity can result in biased data, as the training and evaluation processes predominantly reflect these inherent model tendencies.

Furthermore, the potential misuse of image description generation technologies poses significant risks, including the creation of false news or misleading information. Our current work does not address these ethical concerns, nor does it explore the implications related to privacy and bias in depth. These issues are critical, given the powerful capabilities of image description models and their impact on information dissemination. Incorporation with advanced data selection approaches is promising [5; 19; 46; 76; 79].

Lastly, while we acknowledge the limitations associated with using smaller LLaVa models, we have not thoroughly examined the broader societal impacts of our work. Moreover, our reliance on evaluation metrics such as BLEU and ROUGE, which are known to be noisy and sometimes unreliable, underscores the need for more robust assessment methods. These metrics may not fully capture the qualitative aspects of image descriptions, potentially affecting the validity of our evaluations.

In summary, future work should address these limitations by incorporating diverse model architectures, mitigating biases, exploring ethical implications, and employing more reliable evaluation metrics to enhance the robustness and societal relevance of our framework.

## 7 Conclusion

In conclusion, this paper addresses the limitations of existing image description datasets and proposes an innovative framework, Image Textualization (IT), to generate detailed and accurate image descriptions. The framework leverages the power of multimodal large language models (MLLMs) and multiple vision expert models in a collaborative manner. Through extensive experiments for image understanding and generation tasks, we validate the high quality of the descriptions generated by the framework. We hope our work provides inspiration for the design of more efficient and scalable methods to generate detailed and accurate image descriptions.

## References

* [1]P. Anderson, B. Fernando, M. Johnson, and S. Gould (2016) Spice: semantic propositional image caption evaluation. Cited by: SS1.
* [2]J. Bai, S. Bai, S. Yang, S. Wang, S. Tan, P. Wang, J. Lin, C. Zhou, and J. Zhou (2023) Qwen-vl: a versatile vision-language model for understanding, localization, text, reading, and beyond. Cited by: SS1.
* [3]Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S. Fort, D. Ganguli, T. Henighan, et al. (2022) Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Cited by: SS1.
* [4]J. Betker, G. Goh, L. Jing, \(\dagger\) TimBrooks, J. Wang, L. Li, \(\dagger\) LongOuyang, \(\dagger\) JuntangZhuang, \(\dagger\) JoyceLee, \(\dagger\) YufeiGuo, \(\dagger\) WesamManassra, \(\dagger\) PrafullaDhariwal, \(\dagger\) CaseyChu, \(\dagger\) YunxinJiao, and Aditya Ramesh (2020) Improving image generation with better captions. Note: https://api.semanticscholar.org/CorpusID:264403242 Cited by: SS1.
* [5]Z. Borosos, M. Mutny, and A. Krause (2020) Coresets via bilevel optimization for continual learning and streaming. External Links: Link Cited by: SS1.
* [6]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* [7]N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko (2020) End-to-end object detection with transformers. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I 16, pp. 213-229. Cited by: SS1.
* [8]S. Changpinyo, P. Sharma, N. Ding, and R. Soricut (2021) Conceptual 12m: pushing web-scale image-text pre-training to recognize long-tail visual concepts. External Links: Link Cited by: SS1.
* [9]C. Chen, L. Tang, L. Tao, H. Zhou, Y. Huang, X. Han, and Y. Yu (2023) Activate and reject: towards safe domain generalization under category shift. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11552-11563. Cited by: SS1.
* [10]C. Chen, R. Qin, F. Luo, X. Mi, P. Li, M. Sun, and Y. Liu (2023) Position-enhanced visual instruction tuning for multimodal large language models. External Links: Link Cited by: SS1.
* [11]C. Chen, L. Tang, L. Tao, H. Zhou, Y. Huang, X. Han, and Y. Yu (2023) Activate and reject: towards safe domain generalization under category shift. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 11552-11563. Cited by: SS1.
* [12]C. Chen, R. Qin, F. Luo, X. Mi, P. Li, M. Sun, and Y. Liu (2023) Position-enhanced visual instruction tuning for multimodal large language models. External Links: Link Cited by: SS1.
* [13]J. Chen, J. Yu, C. Ge, L. Yao, E. Xie, Y. Wu, Z. Wang, J. Kwok, P. Luo, H. Lu, and Z. Li (2023) Pixart-\(\alpha\): fast training of diffusion transformer for photorealistic text-to-image synthesis. External Links: Link Cited by: SS1.
* [14]A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. Won Chung, C. Sutton, S. Gehrmann, et al. (2022) Palm: scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Cited by: SS1.
* [15]W. Dai, J. Li, D. Li, A. M. Huat Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [16]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [17]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [18]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [19]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [20]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [21]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [22]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [23]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [24]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [25]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [26]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [27]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [28]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [29]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [30]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [31]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [32]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [33]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [34]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [35]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [36]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [37]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [38]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [39]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [40]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [41]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [42]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning. External Links: Link Cited by: SS1.
* [43]W. Dai, J. Li, D. Li, A. M. Huang, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi (2023) Instructblip: towards general-purpose vision-language models with instruction tuning.

* Ding et al. [2023] Xinpeng Ding, Jianhua Han, Hang Xu, Wei Zhang, and Xiaomeng Li. Hilm-d: Towards high-resolution understanding in multimodal large language models for autonomous driving, 2023.
* Duan et al. [2019] Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qingming Huang, and Qi Tian. Centernet: Keypoint triplets for object detection. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6569-6578, 2019.
* Gao et al. [2022] Jiahui Gao, Yi Zhou, Philip L. H. Yu, Shafiq Joty, and Jiuxiang Gu. Unison: Unpaired cross-lingual image captioning, 2022.
* Gao et al. [2023] Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng Ye, Zhiyong Wu, Weizhong Zhang, Xiaodan Liang, Zhenguo Li, and Lingpeng Kong. Self-guided noise-free data generation for efficient zero-shot learning, 2023.
* Gao et al. [2023] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. G-fluxa: Solving geometric problem with multi-modal large language model, 2023.
* Gao et al. [2023] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, Hongsheng Li, and Yu Qiao. Llama-adapter v2: Parameter-efficient visual instruction model, 2023.
* Girshick [2015] Ross Girshick. Fast r-cnn. In _Proceedings of the IEEE international conference on computer vision_, pages 1440-1448, 2015.
* Gu et al. [2018] Jiuxiang Gu, Jianfei Cai, Gang Wang, and Tsuhan Chen. Stack-captioning: Coarse-to-fine learning for image captioning, 2018.
* Gu et al. [2021] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. _arXiv preprint arXiv:2104.13921_, 2021.
* Hoffmann et al. [2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* Johnson et al. [2015] Justin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks for dense captioning, 2015.
* Ke et al. [2023] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality, 2023.
* Kilickaya et al. [2016] Mert Kilickaya, Aykut Erdem, Nazli Ikizler-Cinbis, and Erkut Erdem. Re-evaluating automatic metrics for image captioning, 2016.
* Kim et al. [2022] Doyeon Kim, Woonghyun Ka, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, and Junmo Kim. Global-local path networks for monocular depth estimation with vertical cutdepth, 2022.
* Kirillov et al. [2023] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything, 2023.
* Krishna et al. [2016] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li. Visual genome: Connecting language and vision using crowdsourced dense image annotations, 2016.
* Lavie and Agarwal [2007] Alon Lavie and Abhaya Agarwal. Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments. In _WMT@ACL_, 2007. URL https://api.semanticscholar.org/CorpusID:16289845.
* Li et al. [2021] Junnan Li, Ramprasaath R Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and Steven Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In _Conference on Neural Information Processing Systems (NeurIPS)_, 2021.

* Li et al. [2023] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023.
* Li et al. [2023] Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, and Lingpeng Kong. Silkie: Preference distillation for large visual language models, 2023.
* Li et al. [2022] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image pre-training. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10965-10975, 2022.
* Li et al. [2023] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models, 2023.
* Lin [2004] Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In _Text Summarization Branches Out_, pages 74-81, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013.
* Lin et al. [2015] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollar. Microsoft coco: Common objects in context, 2015.
* Lin et al. [2017] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In _Proceedings of the IEEE international conference on computer vision_, pages 2980-2988, 2017.
* Liu et al. [2023] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023.
* Liu et al. [2023] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for open-set object detection, 2023.
* Long et al. [2023] Yanxin Long, Youpeng Wen, Jianhua Han, Hang Xu, Pengzhen Ren, Wei Zhang, Shen Zhao, and Xiaodan Liang. Capdet: Unifying dense captioning and open-world detection pretraining, 2023.
* OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_, 35:27730-27744, 2022.
* Pan et al. [2024] Rui Pan, Jipeng Zhang, Xingyuan Pan, Renjie Pi, Xiaoyu Wang, and Tong Zhang. Scalebio: Scalable bilevel optimization for llm data reweighting, 2024. URL https://arxiv.org/abs/2406.19976.
* Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine translation. In _Annual Meeting of the Association for Computational Linguistics_, 2002. URL https://api.semanticscholar.org/CorpusID:11080756.
* Pi et al. [2023] Renjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han, Hang Xu, Lingpeng Kong, and Tong Zhang. Detgpt: Detect what you need via reasoning, 2023.
* Pi et al. [2023] Renjie Pi, Lewei Yao, Jiahui Gao, Jipeng Zhang, and Tong Zhang. Perceptiongpt: Effectively fusing visual perception into llm, 2023.
* Pi et al. [2024] Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, and Tong Zhang. Mllm-protector: Ensuring mllm's safety without hurting performance, 2024.
* Pi et al. [2024] Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Runtao Liu, Rui Pan, and Tong Zhang. Strengthening multimodal large language model with bootstrapped preference optimization, 2024.

* [52] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models, 2016.
* [53] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.
* [54] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation, 2021.
* [55] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents, 2022.
* [56] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. _Advances in neural information processing systems_, 28, 2015.
* [57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models, 2022.
* [58] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding, 2022.
* [59] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagne, Alexandra Sasha Luccioni, Francois Yvon, Matthias Galle, et al. Bloom: A 176b-parameter open-access multilingual language model. _arXiv preprint arXiv:2211.05100_, 2022.
* [60] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text models, 2022.
* [61] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Iryna Gurevych and Yusuke Miyao, editors, _Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 2556-2565, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1238. URL https://aclanthology.org/P18-1238.
* [62] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. _arXiv preprint arXiv:2201.11990_, 2022.
* [63] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all, 2023.
* [64] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, Kurt Keutzer, and Trevor Darrell. Aligning large multimodal models with factually augmented rlhf, 2023.
* [65] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [66] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework, 2022.

* Wu et al. [2022] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: A generative region-to-text transformer for object understanding, 2022.
* Yang et al. [2024] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data, 2024.
* Yao et al. [2021] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and Chunjing Xu. Filip: Fine-grained interactive language-image pre-training, 2021.
* Yao et al. [2021] Lewei Yao, Renjie Pi, Hang Xu, Wei Zhang, Zhenguo Li, and Tong Zhang. G-detkd: towards general distillation framework for object detectors via contrastive and semantic-guided feature imitation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 3591-3600, 2021.
* Yao et al. [2021] Lewei Yao, Renjie Pi, Hang Xu, Wei Zhang, Zhenguo Li, and Tong Zhang. Joint-detnas: upgrade your detector with nas, pruning and dynamic distillation. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 10175-10184, 2021.
* Yao et al. [2022] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pre-training for open-world detection. _arXiv preprint arXiv:2209.09407_, 2022.
* Yao et al. [2024] Lewei Yao, Renjie Pi, Jianhua Han, Xiaodan Liang, Hang Xu, Wei Zhang, Zhenguo Li, and Dan Xu. Detclipv3: Towards versatile generative open-vocabulary object detection, 2024.
* Yin et al. [2023] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, and Enhong Chen. Woodpecker: Hallucination correction for multimodal large language models, 2023.
* Yu et al. [2023] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwan He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng, Maosong Sun, and Tat-Seng Chua. Rlhf-v: Towards trustworthy mlms via behavior alignment from fine-grained correctional human feedback, 2023.
* Zhang et al. [2024] Jipeng Zhang, Yaxuan Qin, Renjie Pi, Weizhong Zhang, Rui Pan, and Tong Zhang. Tagcos: Task-agnostic gradient clustered coreset selection for instruction tuning data, 2024. URL https://arxiv.org/abs/2407.15235.
* Zhong et al. [2021] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip: Region-based language-image pretraining, 2021.
* Zhou et al. [2022] Xiao Zhou, Yong Lin, Renjie Pi, Weizhong Zhang, Renzhe Xu, Peng Cui, and Tong Zhang. Model agnostic sample reweighting for out-of-distribution learning. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 27203-27221. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/zhou2d.html.
* Zhou et al. [2022] Xiao Zhou, Renjie Pi, Weizhong Zhang, Yong Lin, Zonghao Chen, and Tong Zhang. Probabilistic bilevel coreset selection. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 27287-27302. PMLR, 17-23 Jul 2022. URL https://proceedings.mlr.press/v162/zhou2h.html.
* Zhu et al. [2023] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023.
* Zhu et al. [2020] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. _arXiv preprint arXiv:2010.04159_, 2020.
* Zou et al. [2023] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once, 2023.

In this appendix, we first provide the detailed prompts for object entity extraction via LLM, which was adopted in phase 2 for hallucination identification. Then, we illustrate the prompts and in-context examples for textualized recaptioning. Next, we showcase more qualitative comparisons between IT-generated and MLLM-generated image descriptions. Finally, we point out the limitation of our work, which may be addressed in future works.

## Appendix A Detailed Prompt Design for Object Entity Extraction in Phase 2

In table 7, we demonstrate the detailed prompt that guides the LLM to perform entity extraction from the template description. Specifically, we first indicate the role of the LLM. Then we emphasize the things to remember during the extraction process: 1) only extract the objects that certainly exists in the image; 2) avoid extracting the background objects or intangible items; 3) the response should follow a certain format to facilitate parsing. Next, we provide the LLM with human-annotated in-context examples to enable better instruction following ability. Lastly, we provide the LLM with the new template image description to perform entity extraction. In table 8, we showcase the in-context examples provided to LLM for entity extraction.

## Appendix B Prompt for Textualized Recaptioning in Phase 3

In table 9, we demonstrate the prompt for textualized recaptioning. We first inform the LLM of its role as a recaptioner. Then, we provide the detailed explanation for the object-level spatial information, i.e., Relative spatial position, relative depth from lens and relative size of the objects. Next, we emphasize the following points: 1) avoiding duplication when incorporating new objects into the description, 2) the photographic characteristics mentioned in the Original Description should be preserved; 3) the exact values of the spatial information (e.g., bounding boxes) should not be incorporated into the description. Afterwards, we provide human-annotated in-context examples to enhance the annotation quality.

In table 11, we showcase one of the in-context examples provided to the LLM for textualized re-captioning.

\begin{table}
\begin{tabular}{|p{284.5pt}|} \hline
**Extract Prompt** \\ \hline
**###Task Description\#\#** \\ Your are a helpful entities extractor. Please help me to extract the OBJECTS mentioned in a description about an image. \\
**###THINGS to Remember\#\#** \\
1. Only extract the descriptions of objects that are described with certainty. For example, in the sentence “there’s a white car parked, perhaps belonging to one of the hotel guests,” the “hotel guests” part is included within “perhaps,” indicating uncertainty. Therefore, you only need extract “a white car” that is described with certainty. 2. Avoid extracting abstract or non-specific entities (such as “cozy atmosphere”, “excitement”, “sky view”)!!!! 3. Your response should strictly start with “%%%RESPONSE\%\%:”, following this format: “%%\%RESPONSE\%\%:: obj1. obj2. obj3....” \\
**###IN-ContextX EXAMPLES\#\#** \\ Here are some examples for your reference. \(<\)ln-Context Examples\(>\) \\
**###TASK\#\#** \\ \%\%\%DESCRIPT\%\%:: \textless{Description}\textgreater{} \\
**\%\%\%RESPONSE\%\%**: \\ \hline \end{tabular}
\end{table}
Table 7: The prompt for extracting entities in the description, \(<\)ln-Context Examples\(>\) is the placeholder for several in-context examples that illustrated in Table 8. \(<\)Description\(>\) will be replaced by the description to be modified.

[MISSING_PAGE_FAIL:17]

[MISSING_PAGE_FAIL:18]

**In-Context Example of Recaptioning Prompt**

\(\%\,\%\,\%\,\%\,\)The Original Description:\(\%\,\%\,\%\,\%\,\%\,\)At the center of the frame is a **black Rolex clock**

mounted on a **black pole**. The clock has a **white face** with **black hands**, indicating the time.Behind the clock, there are a **brown tree trunk** with a rough texture and a traffic light. Further back, there's a **white building** with a **red tile roof**, possibly a hotel as indicated by the sign that reads **HOTEL**.**.In front of the building, there's a **white car** parked, perhaps belonging to one of the hotel guests. Beyond this immediate scene, there's a street with a **white crosswalk**, where a bus is parked.

Hallocinations: a traffic light; a bus

_@@@@_

1. In The Original Description, there is a sentence "Behind the clock, there are a brown tree trunk with a rough texture and a traffic light". Since "a traffic light" is a hallucination, it should be removed, then the sentence should be "Behind the clock, there are a brown tree trunk with a rough texture".

2. In The Original Description, "there's a street with a **white crosswalk**, where a bus is parked" should be modified to "there's a street with a **white crosswalk**, since "a bus" is a hallucination that needs to be deleted.

_@@@_

Object1: the clock face is white

Relative Spatial Positioning: [0.23, 0.06, 0.55, 0.31]

Distance from the Lens: 1.0

Relative Size Proportion in Images (Percentage): 5.58

_@@@_

1. The original description already mentions "The clock has a **white face** with **black hands**". Since the object detail is already included, no modification is necessary for this object.

_@@@_

...

Object5: a person walking on the sidewalk

Relative Spatial Positioning: [0.98, 0.57, 1.0, 0.6]

Distance from the Lens: 0.0

Relative Size Proportion in Images (Percentage): 0.05

_@@@_

1. Based on the Object Relative Spatial Positioning [0.98, 0.57, 1.0, 0.6], it can be inferred that "a person walking on the sidewalk" is positioned in the middle of the right side of the image.

2. Because the original description includes the line "Beyond this immediate scene, there's a street with a white crosswalk", we can logically associate "a person walking on the sidewalk" with this statement as a supplemental detail.

3. Furthermore, because the Object Distance from the Lens is 0.0, and the Relative Size Proportion of Objects in Images is 0.05, it indicates that "a person walking on the sidewalk" is positioned very far from the camera and is very small. Therefore, we can modify the original sentence "Beyond this immediate scene, there's a street with a white crosswalk." to "Beyond this immediate scene, there's a street with a white crosswalk. At a distance from the camera, you can see a person walking on the sidewalk, occupying a very small part of the entire image."

_@@@@_

**% % Your Modified Description:% % % At the center of the frame is a **black Rolex clock**

mounted on a **black pole**. The clock has a **white face** with **black hands**, indicating the time.the clock, there's a **brown tree trunk** with a rough texture. Further back, there's a **white building** with a **red tile roof**, possibly a hotel as indicated by the sign that reads **HOTEL**."front of the building, there's a white car parked, perhaps belonging to one of the hotel guests. Closer to the camera, there is a parked silver car near a stop sign. Beyond this immediate scene, there's a street with a white crosswalk. At a distance from the camera, you can see a person walking on the sidewalk, occupying a very small part of the entire image.

Table 10: One of the in-context example of the recaptioning prompt.

**In-Context Example of Recaptioning Prompt**

Table 11: One of the in-context example of the recaptioning prompt.

\begin{table}
\begin{tabular}{p{284.5pt}} \hline \hline
**Original Description:** \\ This is a well-organized kitchen with a clean, modern aesthetic. The kitchen features a white countertop against a white wall, creating a bright and air atmosphere. \\ On the countertop, you can see a variety of appliances and items. There’s a sleek coffee machine, ready to brew a fresh cup. Next to it is a paper towel holder, standing tall and at the ready for any spills. A vase adds a touch of elegance to the space, while a blender suggests the possibility of smoothies or soups being made here. Various bottles and jars are also present, perhaps containing spices or cooking ingredients. \\ The objects are mostly colored in shades of white, black, and silver, complementing the modern look of the kitchen. However, there are also pops of color with some yellow and real objects, adding a bit of cheerfulness to the space. \\ Above the countertop, shelves hold additional items. The arrangement is neat and everything appears to have its own place. \\ In the foreground of the image is a wooden chair, perhaps providing a spot to sit while waiting for that coffee to brew or the blender to finish its job. \\ In the background, there’s a window letting in natural light. \\ Overall, this kitchen is not only functional with its various appliances and ample storage, but also stylish with its color scheme and neat arrangement. \\ \hline \hline \end{tabular}
\end{table}
Table 12: Visualization of the original description and the modified description.

## 6 MLLM-generated Description vs IT-Description

### Original Description:

The image depicts a well-organized kitchen scene. The primary focus is a white electric range and oven. The range features four burners and a digital clock on the back panel. The oven below has a window and a handle on its door.

To the right of the range, a set of knives is neatly arranged, hanging on the wall. Adjacent to it on the counter, there's a cookbook open, perhaps suggesting someone is preparing to cook a meal. On the left side of the range, there's a black telephone mounted on the wall, adding a touch of vintage charm to the setting.

The backdrop for these items is a white tile backsplash, providing a clean and bright atmosphere to the kitchen.

The image also contains an OCR text reading "03 22 2010 13 21", but its relevance to the kitchen scene is not clear from the image alone. Overall, this kitchen scene suggests preparation for cooking, with tools and appliances neatly arranged and ready for use.

### Modified Description:

The image depicts a well-organized kitchen scene. The primary focus is a white electric range and oven. The range features four burners and a digital clock on the back panel. The oven below has a window and a handle on its door. To the right of the range, a series of knives with black handles are prominently displayed, some hanging on a magnetic strip and others in a cabinet, suggesting a readiness for culinary activities.

To the right of the oven, the counter hosts a variety of items: an open cookbook, suggesting an ongoing meal preparation, and several books, one of which is a prominent red book. Nearby, a box of white paper is also placed, indicating the kitchen's multipurpose use.

On the left side of the range, there is a spoon. A black microwave is conveniently positioned on the counter and a jar finds its place on a shelf, contributing to the organized look.

The backdrop features a white tile backsplash, enhancing the kitchen's bright and clean atmosphere. The entire setting is punctuated by the presence of utensils and books, each placed thoughtfully to suggest both utility and comfort in the kitchen space.

The image also contains an OCR text reading "03 22 2010 13 21", but its relevance to the kitchen scene is not clear from the image alone.

Overall, this image portrays a kitchen ready for both cooking and everyday family life, with every item neatly arranged.

\begin{table}
\begin{tabular}{p{284.5pt}} \hline \hline
**MLLM-generated Description vs IT-Description** \\ \hline \hline
**Original Description:** \\ \hline \hline The image depicts a well-organized kitchen scene. The primary focus is a white electric range and oven. The range features four burners and a digital clock on the back panel. The oven below has a window and a handle on its door.

To the right of the range, a set of knives is neatly arranged, hanging on the wall. Adjacent to it on the counter, there’s a cookbook open, perhaps suggesting someone is preparing to cook a meal. On the left side of the range, there’s a black telephone mounted on the wall, adding a touch of vintage charm to the setting.

The backdrop for these items is a white tile backsplash, providing a clean and bright atmosphere to the kitchen.

The image also contains an OCR text reading "03 22 2010 13 21", but its relevance to the kitchen scene is not clear from the image alone.

Overall, this kitchen scene suggests preparation for cooking, with tools and appliances neatly arranged and ready for use.

\end{table}
Table 13: Visualization of the original description and the modified description.

\begin{table}
\begin{tabular}{p{142.3pt}} \hline \hline
**Original Description:** \\ The image presents a striking contrast between two distinct bathroom designs. \\ On the left, we see a bathroom that exudes a cool, calming aura with its blue upper wall and green lower wall. The toilet, in harmony with the lower wall, is also green. A white towel, neatly hung on the wall, adds a touch of cleanliness and order to the space. \\ On the right, we are introduced to a bathroom that embraces a warmer, more vibrant color scheme. The wall is painted a lively shade of orange and is adorned with a tree decal, adding an element of nature to the indoor space. A black cabinet and a white toilet form the main fixtures in this bathroom. Above the cabinet, a black shelf holds jars and a plant, further enhancing the natural theme. A window opens up to a view of trees, connecting the indoors with the outdoors and creating an open, airy atmosphere. In both bathrooms, despite their contrasting designs and color schemes, there is an evident emphasis on cleanliness, orderliness, and harmony with nature. Each design offers its own unique interpretation of comfort and style. \\ \hline \hline
**Modified Description:** \\ The image vividly captures two contrasting bathroom interiors, each with its unique design elements and functional fixtures that reflect distinct aesthetics of comfort and style. \\ On the left side, the bathroom showcases a soothing atmosphere with its upper walls painted in a tranquail blue and the lower walls in a refreshing green. The focal point is a light green toilet, perfectly blending with the lower wall, and equipped with a light green toilet seat, emphasizing the cool color palette. Nearby, a towel on a rack hints at meticulous upkeep and functionality. \\ Additional touches include a box of tissues and a roll of toilet paper, conveniently placed, ensuring everything is within reach. A red and white bag hangs from a hook, injecting a pop of color. A window in the bathroom opens to an outdoor view, bridging the interior with the natural world outside. Additionally, a silver trash can, subtly positioned near the toilet, further speaks to the thoughtful arrangement of this space. \\ Transitioning to the right, the bathroom adopts a warmer and more vibrant decor with its lively yellow walls adorned with a tree decal, creating an inviting natural ambiance. A white toilet anchors this space, complemented by a dark brown cabinet mounted on the wall, which hosts a clear jar with a lid, jars with white napkins, totally three jars on a shelf, all contributing to the room’s warm and cozy feel. A small white towel hangs on a towel rack above a silver metal bathroom trash can, combining practicality with style. A window dressed with white window blinds offers a view of the outdoors, linking the room with nature and allowing natural light to brighten the space. Both bathrooms, through their deliberate use of colors, fixtures, and decorative elements, emphasize cleanliness, orderliness, and a harmonious integration with nature. The thoughtful placement of everyday items like toiletries on a shelf and a silver trash can ensures that both practicality and aesthetics are beautifully balanced, offering a comforting and stylish environment. \\ \hline \hline \end{tabular}
\end{table}
Table 14: Visualization of the original description and the modified description.

## Checklist

1. Claim 1. Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] 2. Did you describe the limitations of your work? [Yes] 3. Did you discuss any potential negative societal impacts of your work? [Yes] 4. Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes]
2. Theory, Assumptions and Proofs 1. Did you state the full set of assumptions of all theoretical results? [N/A] 2. Did you include complete proofs of all theoretical results? [N/A]
3. Experimental Result Reproducibility 1. Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] 2. Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] 3. Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] 4. Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes]
4. Open Access to Data and Code 1. If your work uses existing assets, did you cite the creators? [Yes] 2. Did you mention the license of the assets? [Yes] 3. Did you include any new assets either in the supplemental material or as a URL? [Yes] 4. Did you discuss whether and how consent was obtained from people whose data you're using/curating? [Yes] 5. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [Yes]
5. Crowdsourcing and Research with Human Subjects 1. Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] 2. Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] 3. Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]