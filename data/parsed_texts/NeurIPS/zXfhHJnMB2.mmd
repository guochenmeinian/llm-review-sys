# Neural Conditional Probability for Uncertainty Quantification

 Vladimir R. Kostic\({}^{1,2}\)   Karim Lounici\({}^{3}\)   Gregoire Parceau\({}^{3}\)

Giacomo Turri\({}^{1}\)   Pietro Novelli\({}^{1}\)   Massimiliano Pontil\({}^{1,4}\)

\({}^{1}\)CSML, Istituto Italiano di Tecnologia  \({}^{2}\)University of Novi Sad

\({}^{3}\)CMAP-Ecole Polytechnique  \({}^{4}\)AI Centre, University College London

###### Abstract

We introduce Neural Conditional Probability (NCP), an operator-theoretic approach to learning conditional distributions with a focus on statistical inference tasks. NCP can be used to build conditional confidence regions and extract key statistics such as conditional quantiles, mean, and covariance. It offers streamlined learning via a single unconditional training phase, allowing efficient inference without the need for retraining even when conditioning changes. By leveraging the approximation capabilities of neural networks, NCP efficiently handles a wide variety of complex probability distributions. We provide theoretical guarantees that ensure both optimization consistency and statistical accuracy. In experiments, we show that NCP with a 2-hidden-layer network matches or outperforms leading methods. This demonstrates that a a minimalistic architecture with a theoretically grounded loss can achieve competitive results, even in the face of more complex architectures.

## 1 Introduction

This paper studies the problem of estimating the conditional distribution associated with a pair of random variables, given a finite sample from their joint distribution. This problem is fundamental in machine learning, and instrumental for various purposes such as building prediction intervals, performing downstream analysis, visualizing data, and interpreting outcomes. This entails predicting the probability of an event given certain conditions or variables, which is a crucial task across various domains, ranging from finance (Markowitz, 1958) to medicine (Ray et al., 2017), to climate modeling (Harrington, 2017) and beyond. For instance, in finance, it is essential for risk assessment to estimate the probability of default given economic indicators. Similarly, in healthcare, predicting the likelihood of a disease, given patient symptoms, aids in diagnosis. In climate modeling, estimating the conditional probability of extreme weather events such as hurricanes or droughts, given specific climate indicators, helps in disaster preparedness and mitigation efforts.

According to Gao and Hastie (2022), there exist four main strategies to learn the conditional distribution. The first one relies on the Bayes formula for densities and proposes to apply non-parametric statistics to learn the joint and marginal densities separately. However, most of non-parametric techniques face a significant challenge known as the curse of dimensionality (Scott, 1991; Nagler and Czado, 2016). The second strategy, also known as Localization method, involves training a model unconditionally on reweighted samples, where weights are determined by their proximity to the desired conditioning point (Hall et al., 1999; Yu and Jones, 1998). These methods require retraining the model whenever the conditioning changes and may also suffer from the curse of dimensionality if the weighting strategy treats all covariates equally. The third strategy, known as Direct Learning of the conditional distribution involves finding the best linear approximation of the conditional density on a dictionary of base functions or a kernel space (Sugiyama et al., 2010; Li et al., 2007). Theperformance of these methods relies crucially on the selection of bases and kernels. Again for high-dimensional settings, approaches that assign equal importance to all covariates may be less effective. Finally, the fourth strategy, known as Conditional Training, involves training models to estimate a target variable conditioned on certain covariates. This is typically based on partitioning the covariates space \(\mathcal{X}\) into sets, followed by training models unconditionally within each partition (see Gao and Hastie, 2022; Winkler et al., 2020; Lu and Huang, 2020; Dhariwal and Nichol, 2021, and references therein). However, this strategy requires a large dataset to provide enough samples for each conditioning and is expensive as it requires training separate models for each conditioning input set, even though they stem from the same underlying joint distribution.

ContributionsThe principal contribution of this work is a different conditional probability approach that does not fall into any of the four aforementioned strategies. Rather than learning the conditional density directly, our method, called Neural Conditional Probability (NCP), aims to learn the _conditional expectation operator_\(\mathsf{E}_{Y|X}\) associated to the random variables \(X\in\mathcal{X}\) and \(Y\in\mathcal{Y}\) based on data from their joint distribution. The operator is defined, for every measurable function \(f:\mathcal{Y}\to\mathbb{R}\), as

\[[\mathsf{E}_{Y|X}f](x):=\mathbb{E}[f(Y)\,|\,X=x].\]

NCP is based on a principled loss, leveraging the connection between conditional expectation operators and deepCCA (Andrew et al., 2013) established in (Kostic et al., 2024), and can be used interchangeably to:

* retrieve the conditional density \(p_{Y|X}\) with respect to marginal distributions of \(X\) and \(Y\);
* compute conditional statistics \(\mathbb{E}[f(Y)\,|\,X]\) for arbitrary functions \(f:\mathcal{Y}\to\mathbb{R}\), including conditional mean, variance, moments, and the conditional cumulative distribution function, thereby providing access to all conditional quantiles simultaneously;
* estimate the conditional probabilities \(\mathbb{P}[Y\in B\,|\,X\in A]\) for arbitrary sets \(B\subset\mathcal{Y}\) and \(A\subset\mathcal{X}\) with theoretical non-asymptotic guarantees on accuracy, allowing us to easily construct conditional confidence regions.

Notably, our approach extracts statistics directly from the trained operator without retraining or resampling, and it is supported by both optimization consistency and statistical guarantees. In addition our experiments show that our approach matches or exceeds the performance of leading methods, even when using a basic a 2-hidden-layer network. This demonstrates the effectiveness of a minimalistic architecture combined with a theoretically grounded loss function.

Paper organizationIn Section 2 we review related work. Section 3 introduces the operator theoretic approach to model conditional expectation, while Section 4 discusses its training pipeline. In Section 5, we derive learning guarantees for NCP. Finally, Section 6 presents numerical experiments.

## 2 Related works

Non-parametric estimators are valuable for density and conditional density estimation as they don't rely on specific assumptions about the density being estimated. Kernel estimators, pioneered by Parzen (1962) and Rosenblatt (1956), are a widely used non-parametric density estimation method. Much effort has been dedicated to enhancing kernel estimation, focusing on aspects like bandwidth selection (Goldenshluger and Lepski, 2011), non-linear aggregation (Rigollet and Tsybakov, 2007), and computational efficiency (Langrene and Warin, 2020), as well as extending it to conditional densities (Bertin et al., 2014). A comprehensive review of kernel estimators and their variants is provided in (Silverman, 2017). See also (Tsybakov, 2009) for a statistical analysis of their performance. However, most of non-parametric techniques face a significant challenge known as the curse of dimensionality (Scott, 1991; Nagler and Czado, 2016), meaning that the required sample size for accurate estimation grows exponentially with the dimensionality of the data (Silverman, 2017). Additionally, the computational complexity also increases exponentially with dimensionality (Langrene and Warin, 2020).

Examples of localization methods include the work by Hall et al. (1999) for conditional CDF estimation using local logistic regression and locally adjusted Nadaraya-Watson estimation, as well as conditional quantiles estimation via local pinball loss minimization in (Yu and Jones, 1998). Examples of direct learning of the conditional distribution include (Sugiyama et al., 2010) via decomposition on a dictionary of base functions. Similarly, Li et al. (2007) explores quantile regression in reproducing Hilbert kernel spaces.

Conditional training is a popular approach which was adopted in numerous works, as in the recent work by Gao and Hastie (2022) where a parametric exponential model for the conditional density \(p_{\theta}(y|x)\) is trained using the Lindsey method within each bin of a partition of the space \(\mathcal{X}\). This strategy has also been implemented in several prominent classes of generative models, including Normalizing Flow (NF) and Diffusion Models (DM) (Tabak and Vanden-Eijnden, 2010; Dinh et al., 2014; Rezende and Mohamed, 2015; Sohl-Dickstein et al., 2015). These models work by mapping a simple probability distribution into a more complex one. Conditional training approaches for NF and DM have been developed in many works including (e.g. Winkler et al., 2020; Lu and Huang, 2020; Dhariwal and Nichol, 2021). In efforts to lower the computational burden of conditional diffusion models, an alternative approach used heuristic approximations applied directly to unconditional diffusion models on computer vision related tasks (see e.g. Song et al., 2023; Zhang et al., 2023). However, the effectiveness of these heuristics in accurately mimicking the true conditional distributions remains uncertain. Another crucial aspect of these classes of generative models is that while the probability distribution is modelled explicitly, the computation of any relevant statistic, say \(\mathbb{E}[Y|X]\) is left as an implicit problem usually solved by sampling from \(p_{\theta}(y|x)\) and then approximating \(\mathbb{E}[Y|X]\) via simple Monte-Carlo integration. As expected, this approach quickly becomes problematic as the dimension of the output space \(\mathcal{Y}\) becomes large.

Conformal Prediction (CP) is a popular model-agnostic framework for uncertainty quantification (Vovk et al., 1999). Conditional Conformal Prediction (CCP) was later developed to handle conditional dependencies between variables, allowing in principle for more accurate and reliable predictions (see Lei and Wasserman, 2014; Romano et al., 2019; Chernozhukov et al., 2021; Gibbs et al., 2023, and the references cited therein). However, (CP) and (CCP) are not without limitations. The construction of these guaranteed prediction regions need to be recomputed from scratch for each value of the confidence level parameter and of the conditioning for (CCP). In addition, the produced confidence regions tend to be conservative.

## 3 Operator approach to probability modeling

Consider a pair of random variables \(X\) and \(Y\) taking values in probability spaces \((\mathcal{X},\Sigma_{\mathcal{X}},\mu)\) and \((\mathcal{Y},\Sigma_{\mathcal{Y}},\nu)\), respectively, where \(\mathcal{X}\) and \(\mathcal{Y}\) are state spaces, \(\Sigma_{\mathcal{X}}\) and \(\Sigma_{\mathcal{Y}}\) are sigma algebras, and \(\mu\) and \(\nu\) are probability measures. Let \(\rho\) be the joint probability measure of \((X,Y)\) from the product space \(\mathcal{X}\times\mathcal{Y}\). We assume that \(\rho\) is absolutely continuous w.r.t. to the product measure of its marginals, that is \(\rho\ll\mu\times\nu\), and denote the corresponding density by \(p=d\rho/d(\mu\times\nu)\), also called point-wise dependency in Tsai et al. (2020), so that \(\rho(dx,dy)=p(x,y)\mu(dx)\nu(dy)\).

The principal goal of this paper is, given a dataset \(\mathcal{D}_{n}:=(x_{i},y_{i})_{i\in[n]}\) of observations of \((X,Y)\), to estimate the conditional probability measure

\[p(B\,|\,x):=\mathbb{P}[Y\in B\,|\,X=x],\quad x\in\mathcal{X},\,B\in\Sigma_{ \mathcal{Y}}.\] (1)

Our approach is based on the simple fact that \(p(B\,|\,x)=\mathbb{E}[\operatorname{\mathbb{1}}_{B}(Y)\,|\,X=x]\), where \(\operatorname{\mathbb{1}}_{B}\) denotes the characteristic function of set \(B\). More broadly we address the above problem by studying the conditional expectation operator \(\mathsf{E}_{Y|X}\colon L^{2}_{\nu}(\mathcal{Y})\to L^{2}_{\mu}(\mathcal{X})\), which is defined, for every \(f\in L^{2}_{\nu}(\mathcal{Y})\) and \(x\in\mathcal{X}\), as

\[[\mathsf{E}_{Y|X}f](x):=\mathbb{E}[f(Y)\,|\,X=x]=\int_{\mathcal{Y}}f(y)p(dy\, |\,x)=\int_{\mathcal{Y}}f(y)p(x,y)\nu(dy),\]

where \(L^{2}_{\mu}(\mathcal{X})\) and \(L^{2}_{\nu}(\mathcal{Y})\) denotes the Hilbert spaces of functions that are square integrable w.r.t. to \(\mu\) and \(\nu\), respectively. One readily verifies that \(\|\mathsf{E}_{Y|X}\|=1\) and \(\mathsf{E}_{Y|X}\operatorname{\mathbb{1}}_{\mathcal{Y}}=\operatorname{ \mathbb{1}}_{\mathcal{X}}\).

A prominent feature of the above operator is that its rank can reveal the independence of the random variables. That is, \(X\) and \(Y\) are independent random variables if and only if \(\mathsf{E}_{Y|X}\) is a rank one operator, in which case we have that \(\mathsf{E}_{Y|X}=\operatorname{\mathbb{1}}_{\mathcal{X}}\otimes\operatorname{ \mathbb{1}}_{\mathcal{Y}}\). It is thus useful to consider the deflated operator \(\mathsf{D}_{Y|X}=\mathsf{E}_{Y|X}-\operatorname{\mathbb{1}}_{\mathcal{X}} \otimes\operatorname{\mathbb{1}}_{\mathcal{Y}}\colon\,L^{2}_{\nu}(\mathcal{Y}) \to L^{2}_{\mu}(\mathcal{X})\), for which we have that

\[[\mathsf{E}_{Y|X}f](x)=\mathbb{E}[f(Y)]+[\mathsf{D}_{Y|X}f](x),\quad f\in L^{ 2}_{\nu}(\mathcal{Y}).\] (2)

For dependent random variables, the deflated operator is nonzero. In many important situations, such as when the conditional probability distribution is a.e. absolutely continuous w.r.t. to the target measure, that is \(p(\cdot\,|\,x)\ll\nu\) for \(\mu\)-a.e. \(x\in\mathcal{X}\), the operator \(\mathsf{E}_{Y|X}\) is compact, and, hence, we can write the SVD of \(\mathsf{E}_{Y|X}\) and \(\mathsf{D}_{Y|X}\) respectively as

\[\mathsf{E}_{Y|X}=\sum_{i=0}^{\infty}\sigma_{i}^{\star}\,u_{i}^{\star}\otimes v _{i}^{\star},\quad\text{ and }\quad\mathsf{D}_{Y|X}=\sum_{i=1}^{\infty}\sigma_{i}^{\star}\,u_{i}^{ \star}\otimes v_{i}^{\star},\] (3)

where the left \((u_{i}^{\star})_{i\in\mathbb{N}}\) and right \((v_{i}^{\star})_{i\in\mathbb{N}}\) singular functions form complete orthonormal systems of \(L^{2}_{\mu}(\mathcal{X})\) and \(L^{2}_{\nu}(\mathcal{Y})\), respectively. Notice that the only difference in the SVD of \(\mathsf{E}_{Y|X}\) and \(\mathsf{D}_{Y|X}\) is the extra leading singular triplet \((\sigma_{0}^{\star},u_{0}^{\star},v_{0}^{\star})=(1,\mathbb{1}_{\mu},\mathbb{1 }_{\nu})\) of \(\mathsf{E}_{Y|X}\). In terms of densities, the SVD of \(\mathsf{E}_{Y|X}\) leads to the characterization

\[p(x,y)=\sum_{i=0}^{\infty}\sigma_{i}^{\star}\,u_{i}^{\star}(x)\,v_{i}^{\star}( y)=1+\sum_{i=1}^{\infty}\sigma_{i}^{\star}\,u_{i}^{\star}(x)\,v_{i}^{\star}(y).\]

The mild assumption that \(\mathsf{E}_{Y|X}\) is a compact operator allows one to approximate it arbitrarily well with a (large enough) finite rank (empirical) operator. Choosing the operator norm as the measure of approximation error and appealing to the Eckart-Young-Mirsky Theorem (see Theorem 3 in Appendix B.1) one concludes that the best approximation is given by the truncated SVD, that is for every \(d\in\mathbb{N}\),

\[\mathsf{D}_{Y|X}\approx\llbracket\mathsf{D}_{Y|X}\rrbracket_{d}:=\sum_{i=1}^{ d}\sigma_{i}^{\star}\,u_{i}^{\star}\otimes v_{i}^{\star},\quad\text{ and }\quad\llbracket\mathsf{D}_{Y|X}\rrbracket_{d}\in\text{arg min}_{\text{rank}(A )\leq d}\|\mathsf{D}_{Y|X}-A\|,\] (4)

where the minimum is given by \(\sigma_{d}^{\star}\), and the minimizer is unique whenever \(\sigma_{d+1}^{\star}<\sigma_{d}^{\star}\). This leads to the approximation of the joint density w.r.t. marginals \(p(x,y)\approx 1+\sum_{i=1}^{d}\sigma_{i}^{\star}\,u_{i}^{\star}(x)\,v_{i}^{ \star}(y)\), so that

\[\mathbb{E}[f(Y)\,|\,X=x]\approx\mathbb{E}[f(Y)]+\sum_{i=1}^{d}\sigma_{i}^{ \star}\,u_{i}^{\star}(x)\mathbb{E}[f(Y)\,v_{i}^{\star}(Y)],\] (5)

which in particular, choosing \(f=\mathbb{1}_{B}\), gives

\[\mathbb{P}[Y\in B\,|\,X=x]\approx\mathbb{P}[Y\in B]+\sum_{i=1}^{d}\sigma_{i}^ {\star}\,u_{i}^{\star}(x)\,\mathbb{E}[v_{i}^{\star}(Y)\,\mathbb{1}_{B}(Y)].\]

Moreover, we have that

\[\mathbb{P}[Y\in B\,|\,X\in A]=\frac{\langle\mathbb{1}_{A},\mathsf{E}_{Y|X} \mathbb{1}_{B}\rangle}{\mathbb{P}[X\in A]}\approx\mathbb{P}[Y\in B]+\sum_{i=1 }^{d}\,\sigma_{i}^{\star}\,\frac{\mathbb{E}[u_{i}^{\star}(X)\mathbb{1}_{A}(X) ]}{\mathbb{P}[X\in A]}\,\mathbb{E}[v_{i}^{\star}(Y)\,\mathbb{1}_{B}(Y)],\]

for which the approximation error is bounded in the following lemma.

**Lemma 1** (Approximation bound).: _For any \(A\in\Sigma_{\mathcal{X}}\) such that \(\mathbb{P}[X\in A]>0\) and any \(B\in\Sigma_{\mathcal{Y}}\),_

\[\left|\mathbb{P}[Y\in B\,|\,X\in A]-\mathbb{P}[Y\in B]-\frac{\langle\mathbb{1 }_{A},\llbracket\mathsf{D}_{Y|X}\rrbracket_{d}\mathbb{1}_{B}\rangle}{\mathbb{P }[X\in A]}\right|\leq\sigma_{d+1}^{\star}\,\sqrt{\frac{\mathbb{P}[Y\in B]}{ \mathbb{P}[X\in A]}}.\] (6)

Neural network modelInspired by the above observations, to build the NCP model, we will parameterize the truncated SVD of the conditional expectation operator and then learn it. Specifically, we introduce two parameterized embeddings \(u^{\theta}\colon\mathcal{X}\to\mathbb{R}^{d}\) and \(v^{\theta}\colon\mathcal{Y}\to\mathbb{R}^{d}\), and the singular values parameterized by \(w^{\theta}\in\mathbb{R}^{d}\), respectively given by

\[u^{\theta}(x){:=}[u_{1}^{\theta}(x)\,\dots\,u_{d}^{\theta}(x)]^{\top},\ v^{ \theta}(y){:=}[v_{1}^{\theta}(y)\,\dots\,v_{d}^{\theta}(y)]^{\top},\text{ and }\sigma^{\theta}{:=}[e^{-(w_{1}^{\theta})^{2}},\dots,e^{-(w_{d}^{\theta})^{2}}]^{ \top},\]

where the parameter \(\theta\) takes values in a prescribed set \(\Theta\).

We then aim to learn the joint density function \(p(x,y)\) in the (separable) form

\[p_{\theta}(x,y):=1+\sum_{i\in[d]}\sigma_{i}^{\theta}u_{i}^{\theta}(x)\,v_{i}^{ \theta}(y)=1+\langle\sigma^{\theta}\odot u^{\theta}(x),v^{\theta}(y)\rangle,\]

where \(\odot\) denotes element-wise product. One of the prominent losses considered for the task of learning \(p\in\mathcal{L}^{2}_{\mu\times\nu}\) is _the least squares density ratio_ loss \(\mathsf{E}_{\mu\times\nu}(p-p_{\theta})^{2}-\mathsf{E}_{\rho}p=\mathsf{E}_{\mu \times\nu}p_{\theta}^{2}-2\mathbb{E}_{\rho}p_{\theta}\), c.f. Tsai et al. (2020), also considered by HaoChen et al. (2022) in the specific context of augmentation graph in self-supervised deep learning, linked to kernel embeddings (Wang et al., 2022), and rediscovered and tested on DeepCCA tasks by Wells et al. (2024). Here, following the operator perspective, we use the characterization (4) of the optimal finite rank model to propose a new loss that: (1) excludes the known feature from the learning process, and (2) introduces a penalty term to enforce orthonormality of the basis functions. More precisely, our loss \(\mathcal{L}_{\gamma}(\theta):=\mathcal{L}(\theta)+\gamma\mathcal{R}(\theta)\) is composed of two terms. The first one

\[\mathcal{L}(\theta):=\mathbb{E}_{\mu\times\nu}p_{\theta}^{2}-2\mathbb{E}_{\rho}p _{\theta}+[\mathbb{E}_{\mu\times\nu}p_{\theta}]^{2}-\mathbb{E}_{\mu}[\mathbb{E }_{\nu}p_{\theta}]^{2}-\mathbb{E}_{\nu}[\mathbb{E}_{\mu}p_{\theta}]^{2}+2 \mathbb{E}_{\mu\times\nu}p_{\theta}\] (7)is equivalent to solving (4) with \(A=\sum_{i=1}^{d}\sigma_{i}^{\theta}\left[u_{i}^{\theta}-\mathbb{E}_{\mu}u_{i}^{ \theta}\right]\otimes[v_{i}^{\theta}-\mathbb{E}_{\nu}v_{i}^{\theta}]\) and can be written in terms of correlations between features. Namely, denoting the covariance and variance matrices by

\[\mathrm{Cov}[z,z^{\prime}]:=\mathbb{E}[(z-\mathbb{E}[z])(z^{\prime}-\mathbb{E} [z^{\prime}])^{\top}]\quad\text{ and }\quad\mathrm{Var}[z]:=\mathbb{E}[(z-\mathbb{E}[z])(z-\mathbb{E}[z])^{\top}],\] (8)

and abbreviating \(u^{\theta}:=u^{\theta}(X)\) and \(v^{\theta}:=v^{\theta}(Y)\) for simplicity, we can write

\[\mathcal{L}(\theta):=\text{tr}\left(\mathrm{Var}[\sqrt{\sigma^{\theta}}\odot u ^{\theta}]\,\mathrm{Var}[\sqrt{\sigma^{\theta}}\odot v^{\theta}]-2\,\mathrm{ Cov}[\sqrt{\sigma^{\theta}}\odot u^{\theta},\sqrt{\sigma^{\theta}}\odot v^{ \theta}]\right).\] (9)

If \(p{=}p_{\theta}\) for some \(\theta{\in}\Theta\), then the optimal loss is the \(\chi^{2}\)-divergence \(\mathcal{L}(\theta){=}D_{\chi^{2}}(\rho\,|\,\mu\times\nu){=}{-}\sum_{i\geq 1 }{\sigma_{i}^{*}}^{2}\) and, as we show below, \(\mathcal{L}(\theta)\) measures how well \(p_{\theta}(x,y)-1\) approximates \(\sum_{i\in[d]}\sigma_{i}^{*}\,u_{i}^{*}(x)\,v_{i}^{*}(y)\). However, in order to obtain a useful probability model, it is of paramount importance to _align_ the metric in the latent spaces with the metrics in the data-spaces \(L_{\mu}^{2}(\mathcal{X})\) and \(L_{\nu}^{2}(\mathcal{Y})\). For different reasons, a similar phenomenon has been observed in Kostic et al. (2024) where dynamical systems are learned via transfer operators. In our setting, this leads to the second term of the loss that measures how well features \(u^{\theta}\) and \(v^{\theta}\) span relevant subspaces in \(L_{\mu}^{2}(\mathcal{X})\) and \(L_{\nu}^{2}(\mathcal{Y})\), respectively. Namely, aiming \(\mathbb{E}[u_{i}^{\star}(X)u_{j}^{\star}(X)]=\mathbb{E}[v_{i}^{\star}(Y)v_{j }^{\star}(Y)]=\mathbb{1}_{\{i=j\}}\), \(i,j\in\{0,1,\ldots,d\}\) leads to

\[\mathcal{R}(\theta){:=}\|\mathbb{E}[u^{\theta}(X)u^{\theta}(X)^{\top}]{-}I\|_ {F}^{2}{+}\|\mathbb{E}[v^{\theta}(Y)v^{\theta}(Y)^{\top}]{-}I\|_{F}^{2}{+}\| \mathbb{E}[u^{\theta}(X)]\|^{2}{+}2\|\mathbb{E}[v^{\theta}(Y)\|^{2}.\] (10)

We now state our main result on the properties of the loss \(\mathcal{L}_{\gamma}\), which extends the result in Wells et al. (2024) to infinite-dimensional operators and guarantees the uniqueness of the optimum due to \(\mathcal{R}\).

**Theorem 1**.: _Let \(\mathbb{E}_{Y|X}\colon L_{\nu}^{2}(\mathcal{Y})\to L_{\mu}^{2}(\mathcal{X})\) be a compact operator and \(\mathsf{D}_{Y|X}=\sum_{i=1}^{\infty}\sigma_{i}^{\star}u_{i}^{\star}\otimes v_ {i}^{\star}\) be the SVD of its deflated version. If \(u_{i}^{\theta}\in L_{\mu}^{2}(\mathcal{X})\) and \(v_{i}^{\theta}\in L_{\nu}^{2}(\mathcal{Y})\), for all \(\theta\in\Theta\) and \(i\in[d]\), then for every \(\theta\in\Theta\), \(\mathcal{L}_{\gamma}(\theta)\geq-\sum_{i\in[d]}\sigma_{i}^{*2}\). Moreover, if \(\gamma>0\) and \(\sigma_{d}^{\star}>\sigma_{d+1}^{\star}\), then the equality holds if and only if \((\sigma_{i}^{\theta},u_{i}^{\theta},v_{i}^{\theta})\) equals \((\sigma_{i}^{\star},u_{i}^{\star},v_{i}^{\star})\)\(\rho\)-a.e., up to unitary transform of singular spaces._

We provide the proof in Appendix B.3. In the following section, we show how to learn these canonical features from data and construct approximations of the conditional probability measure.

Comparison to previous methodsNCP does not fall into any of the four categories defined by Gao and Hastie (2022), as it does not aim to learn conditional density of \(Y|X\) directly. Instead, NCP focuses on learning the operator mapping \(L_{\nu}^{2}(\mathcal{Y})\to L_{\mu}^{2}(\mathcal{X})\), from which all relevant task-specific statistics can be derived without requiring retraining. This approach effectively integrates with deep representation learning to create a latent space adapted to \(p(y|x)\). As a result, NCP efficiently captures the intrinsic dimension of the data, which is supported by our theoretical guarantees that depend solely on the latent space dimension (Theorem 2). In contrast, strategies designed for learning density often encounter significant limitations, such as the curse of dimensionality, potential substantial misrepresentation errors when the pre-specified function dictionary misaligns with the true distribution \(p(y|x)\), and high computational complexity due to the need for retraining. Experiments confirm NCP's capability to learn representations tailored to a wide range of data types--including manifolds, graphs, and high-dimensional distributions--without relying on predefined dictionaries. This flexibility allows NCP to outperform popular aforementioned methods.

## 4 Training the NCP inference method

In this section, we discuss how to train the model. Given a training dataset \(\mathcal{D}_{n}=(x_{i},y_{i})_{i\in[n]}\) and networks \((u^{\theta},v^{\theta},\sigma^{\theta})\), we consider the empirical loss \(\widehat{\mathcal{L}}_{\gamma}(\theta):=\widehat{\mathcal{L}}(\theta)+\gamma \widehat{\mathcal{R}}(\theta)\), where we replaced (9) and (10) by their empirical versions. In order to guarantee the unbiased estimation, as we show within the proof of Theorem 1, two terms of our loss can be written using two independent samples \((X,Y)\) and \((X^{\prime},Y^{\prime})\) from \(\rho\) as \(\mathcal{L}(\theta){=}\mathbb{E}[L(u^{\theta}(X)-\mathbb{E}u^{\theta}(X),u^{ \theta}(X^{\prime})-\mathbb{E}u^{\theta}(X^{\prime}),v^{\theta}(Y)-\mathbb{E}v^{ \theta}(Y),v^{\theta}(Y^{\prime})-\mathbb{E}v^{\theta}(Y^{\prime}),\sigma^{ \theta})]\) and \(\mathcal{R}(\theta){=}\mathbb{E}[R(u^{\theta}(X),u^{\theta}(X^{\prime}),v^{ \theta}(Y),v^{\theta}(Y^{\prime}))]\), where the loss functionals \(L\) and \(R\) are defined for \(u,u^{\prime},v,v^{\prime}{\in}\mathbb{R}^{d}\) and \(s{\in}[0,1]^{d}\) as

\[L(u,u^{\prime},v,v^{\prime},s){:=}\frac{1}{2}\left(u^{\top}\, \text{diag}(s)v^{\prime}\right)^{2}{+}\frac{1}{2}\left(v^{\top}\,\text{diag}(s)u^{ \prime}\right)^{2}{-}u^{\top}\,\text{diag}(s)v^{\prime}{-}v^{\top}\,\text{diag}(s)u^{ \prime},\] (11) \[R(u,u^{\prime},v,v^{\prime}){:=}(u^{\top}u^{\prime})^{2}{-}(u{-}u^{ \prime})^{\top}\,(u{-}u^{\prime}){+}(v^{\top}v^{\prime})^{2}{-}(v{-}v^{ \prime})^{\top}(v{-}v^{\prime}){+}2d.\] (12)

Therefore, at every epoch we take two independent batches \(\mathcal{D}_{n}^{1}\) and \(\mathcal{D}_{n}^{2}\) of equal size from \(\mathcal{D}_{n}\), leading to Algorithm 1. See Appendix A.1 for the full discussion, and Appendix A.2, where we also provide in Figure 4 an example of learning dynamics.

``` training data (\(X_{\text{train}}\),\(Y_{\text{train}}\))  train \(u^{\theta}\), \(\sigma^{\theta}\) and \(v^{\theta}\) using the NCP loss  Center and scale \(X_{\text{train}}\) and \(Y_{\text{train}}\) for each epoch do  From (\(X_{\text{train}}\),\(Y_{\text{train}}\)) pick two random batches (\(X_{\text{train}}\),\(Y_{\text{train}}\)) and (\(X^{\prime}_{\text{train}}\),\(Y^{\prime}_{\text{train}}\))  Evaluate: \(U\gets u^{\theta}(X_{\text{train}})\), \(U^{\prime}\gets u^{\theta}(X^{\prime}_{\text{train}})\), \(V\gets v^{\theta}(Y_{\text{train}})\), \(V^{\prime}\gets v^{\theta}(Y^{\prime}_{\text{train}})\)  Compute \(\widehat{\mathcal{L}}(\theta)\) as an unbiased estimate using (9) or (11)  Compute \(\widehat{\mathcal{R}}(\theta)\) as an unbiased estimate using (10) or (12)  Compute NCP loss \(\widehat{\mathcal{L}}_{\gamma}(\theta):=\widehat{\mathcal{L}}(\theta)+ \gamma\widehat{\mathcal{R}}(\theta)\) and back-propagate endfor ```

**Algorithm 1** Separable density learning procedure

Practical guidelines for trainingIn the following, we briefly report a few aspects to be kept in mind when using the NCP in practice, referring the reader to Appendix A for further details. The computational complexity of loss estimation presents three distinct methodological approaches. The first method utilizes unbiased estimation via covariance calculations in (9) and (10), achieving a computational complexity of \(\mathcal{O}(nd^{2})\) for a batch size \(n\). An alternative approach employing U-statistics with (11) and (12) requires \(\mathcal{O}(n^{2}d)\) operations per iteration, offering the estimation of the same precision. A third method involves batch averaging of (11) and (12), reducing computational complexity to \(\mathcal{O}(nd)\), which enables seamless integration with contemporary deep learning frameworks, albeit potentially compromising training robustness through less accurate 4th-order moment estimations. Method selection remains contingent upon the specific problem's computational and statistical constraints. Further, the size of latent dimension \(d\), as indicated by Theorem 1 relates to the problem's "difficulty" in the sense of smoothness of joint density w.r.t. its marginals. Lastly, after the training, an additional post-processing may be applied to ensure the orthogonality of features \(u^{\theta}\) and \(v^{\theta}\) and improve statistical accuracy of the learned model.

Performing inference with the trained NCP modelWe now explain how to extract important statistical objects from the trained model \((\widehat{u}^{\theta},\widehat{v}^{\theta},\sigma^{\theta})\). To this end, define the empirical operator

\[\widehat{\mathsf{D}}^{\theta}_{Y|X}:L^{2}_{\nu}(\mathcal{Y})\!\!\to\!\!L^{2}_{ \mu}(\mathcal{X})\quad[\widehat{\mathsf{D}}^{\theta}_{Y|X}f](x)\!\!:=\!\!\sum _{i\in[d]}\sigma^{\theta}_{i}\widehat{u}^{\theta}_{i}(x)\,\widehat{\mathsf{E }}_{y}[\widehat{v}^{\theta}_{i}\,f],\quad f\in L^{2}_{\nu}(\mathcal{Y}),\,x \in\mathcal{X},\] (13)

where \(\widehat{\mathsf{E}}_{y}[\widehat{v}^{\theta}_{i}\,f]:=\frac{1}{n}\sum_{j\in[ n]}\widehat{v}^{\theta}_{i}(y_{j})f(y_{j})\). Then, _without any retraining nor simulation_, we can compute the following statistics:

\(\blacktriangleright\) Conditional Expectation: \([\widehat{\mathsf{E}}^{\theta}_{Y|X}f](x)\!:=\!\widehat{\mathsf{E}}_{y}f\!+ \![\widehat{\mathsf{D}}^{\theta}_{Y|X}f](x),\,f\in L^{2}_{\nu}(\mathcal{Y}),x \in\mathcal{X}\).

\(\blacktriangleright\) Conditional moments of order \(\alpha\geq 1\): apply previous formula to \(f(u)=u^{\alpha}\).

\(\blacktriangleright\) Conditional covariance: \(\widehat{\mathrm{Cov}}^{\theta}(Y|X):=\widehat{\mathsf{E}}^{\theta}_{Y|X}[ YY^{\top}]-\widehat{\mathsf{E}}^{\theta}_{Y|X}[Y]\widehat{\mathsf{E}}^{\theta}_{Y|X}[Y^{ \top}]\).

\(\blacktriangleright\) Conditional probabilities: apply the above conditional expectation formula with \(f(y)=\mathbb{1}_{B}(y)\), that is, \(\widehat{p}_{y}(B)=\widehat{\mathsf{E}}_{y}[\mathbb{1}_{B}]\) and \(\widehat{p}_{\theta}(B\,|\,x)=\widehat{p}_{y}(B)+\sum_{i\in[d]}\sigma^{\theta}_ {i}\widehat{u}^{\theta}_{i}(x)\,\widehat{\mathsf{E}}_{y}[\widehat{v}^{\theta} _{i}\mathbb{1}_{B}],\,B\!\in\!\Sigma_{\mathcal{Y}},x\!\in\!\mathcal{X}\). Then, integrating over an arbitrary set \(A\in\Sigma_{\mathcal{X}}\) we get

\[\widehat{p}_{\theta}(B\,|\,A):=\widehat{p}_{y}(B)+\sum_{i\in[d]}\sigma^{\theta}_ {i}\,\frac{\widehat{\mathsf{E}}_{x}[\widehat{u}^{\theta}_{i}\mathbb{1}_{A}]}{ \mathsf{E}_{x}[\mathbb{1}_{A}]}\,\widehat{\mathsf{E}}_{y}[\widehat{v}^{\theta} _{i}\mathbb{1}_{B}].\] (14)

\(\blacktriangleright\) Conditional quantiles: for scalar output \(Y\), the conditional CDF \(\widehat{F}_{Y|X\in A}(t)\) is obtained by taking \(B=(-\infty,t]\), and in Algorithm 3 in Appendix C we show how to extract quantiles from it.

## 5 Statistical guarantees

We introduce some standard assumptions needed to state our theoretical learning guarantees. To that end, for any \(A\in\Sigma_{\mathcal{X}}\) and \(B\in\Sigma_{\mathcal{Y}}\) we define important constants, followed by the main assumption,

\[\varphi_{X}(A)\!:=\!1\vee\sqrt{\frac{1\!-\!\mathbb{P}[X\in A]}{\mathbb{P}[X \in A]}}\quad\text{and}\quad\varphi_{Y}(B)\!:=\!1\vee\sqrt{\frac{1\!-\!\mathbb{ P}[Y\in B]}{\mathbb{P}[Y\in B]}}.\]

**Assumption 1**.: _There exists finite absolute constants \(c_{u},c_{v}>1\) such that for any \(\theta\in\Theta\)_

\[\underset{x\sim\mu}{\text{ess\,sup}}\|u^{\theta}(x)\|_{l_{\infty}}\leq c_{u}, \quad\underset{y\sim\nu}{\text{ess\,sup}}\|v^{\theta}(y)\|_{l_{\infty}}\leq c_{v}.\]

Next, we set \(\sigma_{\theta}^{2}(X){:=}\mathrm{Var}(\|u^{\theta}(X)-\mathbb{E}[u^{\theta}(X) ]\|_{l_{2}})\), \(\sigma_{\theta}^{2}(Y){:=}\mathrm{Var}(\|v^{\theta}(Y)-\mathbb{E}[v^{\theta}(Y )]\|_{l_{2}})\) and

\[\epsilon_{n}(\delta){:=}C\bigg{(}(c_{u}\lor c_{v})\frac{\sqrt{d} \,\log(e\delta^{-1})}{n}{+}(\sigma_{\theta}(X)\lor\sigma_{\theta}(Y))\sqrt{ \frac{\log(e\delta^{-1})}{n}}\bigg{)},\ \bar{\epsilon}_{n}(\delta){:=}2\sqrt{2\frac{\log 2 \delta^{-1}}{n}},\] (15)

for some large enough absolute constant \(C>0\).

**Remark 1**.: _It follows easily from Assumption 1 that \(\sigma_{\theta}^{2}(X){\leq}c_{u}^{2}d\) and \(\sigma_{\theta}^{2}(Y){\leq}c_{v}^{2}d\). Consequently, assuming that \(n\geq(c_{u}\lor c_{v})d\), then \(\epsilon_{n}(\delta)\lesssim(c_{u}\lor c_{v})\sqrt{d}[\sqrt{\log(e\delta^{-1}) /n}\vee(\log(e\delta^{-1})/n)]\)._

Finally, for a given parameter \(\theta{\in}\Theta\) and \(\delta\in(0,1)\), let us denote

\[\mathcal{E}_{\theta}{:=}\max\{\|[\mathbb{D}_{Y|X}]_{d}{-}U_{\theta} S_{\theta}V_{\theta}^{*}\|,\|U_{\theta}^{*}U_{\theta}{-}I\|,\|U_{\theta}^{*}{ \,\mathbb{I}}_{X}\|,\|V_{\theta}^{*}V_{\theta}{-}I\|,\|V_{\theta}^{*}{\, \mathbb{I}}_{\mathcal{Y}}\|\},\ \text{and}\] (16) \[\psi_{n}(\delta):=\sigma_{d+1}^{*}+\mathcal{E}_{\theta}+2\sqrt{1 +\mathcal{E}_{\theta}}(\mathcal{E}_{\theta}+\varepsilon_{n}(\delta))+[ \varepsilon_{n}(\delta)]^{2}.\] (17)

In the following result, we prove that NCP model approximates well the conditional probability distribution w.h.p. whenever the empirical loss \(\widehat{\mathcal{L}}_{\gamma}(\theta)\) is well minimized.

**Theorem 2**.: _Let Assumption 1 be satisfied, and in addition assume that_

\[\mathbb{P}(X{\in}A)\bigwedge\mathbb{P}(Y{\in}B){\geq}\bar{\epsilon}_{n}( \delta/3)\quad\text{and}\quad n{\geq}(c_{u}\lor c_{v})^{2}d\bigvee 8\log(6 \delta^{-1})\left[\varphi_{X}(A)\vee\varphi_{Y}(B)\right].\] (18)

_Then for every \(A\in\Sigma_{\mathcal{X}}\setminus\{\mathcal{X}\}\) and \(B\in\Sigma_{\mathcal{Y}}\setminus\{\mathcal{Y}\}\)_

\[\left|\frac{\mathbb{P}[Y{\in}B\,|\,X{\in}A]}{\mathbb{P}[Y{\in}B]}-\frac{\widehat {p}_{\theta}(B\,|\,A)}{\widehat{p}_{y}(B)}\right|\leq\frac{4\psi_{n}(\delta/3 )+[1{+}\psi_{n}(\delta/3)]\left[2\varphi_{X}(A){+}4\varphi_{Y}(B)\right] \bar{\epsilon}_{n}(\delta/3)}{\sqrt{\mathbb{P}[X{\in}A]\mathbb{P}[Y{\in}B]}},\] (19)

_and_

\[\left|\frac{\mathbb{P}[Y{\in}B\,|\,X{\in}A]{-}\widehat{p}_{\theta}(B\,|\,A)} {\mathbb{P}[Y{\in}B]}\right|{\leq}\varphi_{Y}(B)\bar{\epsilon}_{n}(\delta/3){ +}\frac{2(1{+}\psi_{n}(\delta/3))\varphi_{X}(A)\bar{\epsilon}_{n}(\delta/3){ +}\psi_{n}(\delta/3)}{\sqrt{\mathbb{P}[X{\in}A]\mathbb{P}[Y{\in}B]}}\] (20)

_hold with probability at least \(1-\delta\) w.r.t. iid draw of the dataset \(\mathcal{D}_{n}=(x_{j},y_{j})_{j\in[n]}\) from \(\rho\)._

**Remark 2**.: _In Appendix B.5, we prove a similar result under a less restrictive sub-Gaussian assumption on the singular functions \(u^{\theta}(X)\) and \(v^{\theta}(Y)\)._

DiscussionThe rate \(\psi_{n}(\delta)\) in (17) is pivotal for the efficacy of our method. If we appropriately choose the latent space dimension \(d\) to ensure accurate approximation (\(\sigma_{d+1}^{*}\ll 1\)), achieve successful training (\(\mathcal{E}_{\theta}\approx\sigma_{d+1}^{*}\)), and secure a large enough sample size (\(\varepsilon_{n}(\delta)\ll 1\)), Theorem 2 provides assurance of accurate prediction of conditional probabilities. Indeed, (20) guarantees (up to a logarithmic factor)

\[\mathbb{P}[Y{\in}B\,|\,X{\in}A]{-}\widehat{p}_{\theta}(B\,|\,A){=}O_{\mathbb{ P}}\left(\frac{1}{\sqrt{n}}{+}\sqrt{\frac{\mathbb{P}[Y{\in}B]}{\mathbb{P}[X{\in}A]}} \left(\sigma_{d+1}^{*}{+}\mathcal{E}_{\theta}{+}\sqrt{d/n}{+}\varphi_{X}(A)/ \sqrt{n}\right)\right),\]

Note the inclusion of the term \(\sqrt{\mathbb{P}[X\in A]}\) in the denominator of the last term on the right-hand side, along with \(\varphi_{X}(A)\). This indicates a decrease in the accuracy of conditional probability estimates for rarely encountered event \(A\), aligning with intuition and with a known finite-sample impossibility result Lei and Wasserman (2014, Lemma 1) for conditional confidence regions when \(A\) is reduced to any nonatomic point of the distribution (i.e. \(A=\{x\}\) with \(\mathbb{P}[X=x]=0\)). For rare events, a larger sample size \(n\) and a higher-dimensional latent space characterized by \(d\) are necessary for accurate estimation of conditional probabilities.

We propose next a non-asymptotic estimation guarantee for the conditional CDF of \(Y|X\) when \(Y\) is a scalar output. This result ensures in particular that accurate estimation of the true quantiles is possible with our method. Fix \(t\in\mathbb{R}\) and consider the set \(B_{t}=(-\infty,t]\) meaning that \(\mathbb{P}[Y{\in}B_{t}|X{\in}A]{=}F_{Y|X\in A}(t)\) and \(\mathbb{P}[Y{\in}B_{t}]{=}F_{Y}(t)\). We define similarly for the NCP estimator of the conditional CDF \(\widehat{F}_{Y|X\in A}(t){=}\widehat{p}_{\theta}(B_{t}\,|\,A)\). The result follows from applying (20) to the set \(B_{t}\).

**Corollary 1**.: _Let the Assumptions of Theorem 2 be satisfied. Then for any \(t\in\mathbb{R}\) and \(\delta\in(0,1)\), it holds with probability at least \(1-\delta\) that_

\[|\widehat{F}_{Y|X\in A}(t)-F_{Y|X\in A}(t)|\leq\sqrt{F_{Y}(t)(1-F_{ Y}(t))\bar{\epsilon}_{n}}(\delta/3)\\ +\sqrt{\frac{F_{Y}(t)}{\mathbb{P}[X\in A]}}\left(\sigma_{d+1}^{ \star}+2\sqrt{2}\mathcal{E}_{\theta}+(2\sqrt{2}+1)\epsilon_{n}(\delta/3)+4 \varphi_{X}(A)\bar{\epsilon}_{n}(\delta/3)\right).\] (21)

An important application of Corollary 1 lies in uncertainty quantification when output \(Y\) is a scalar. Indeed, for any \(\alpha\in(0,1/2)\), we can scan the empirical conditional CDF \(\widehat{F}_{Y|X\in A}\) for values \(t_{\alpha}<t_{\alpha}^{\prime}\) such that \(\widehat{F}_{Y|X\in A}(t_{\alpha}^{\prime})-\widehat{F}_{Y|X\in A}(t_{\alpha} )=1-\alpha\) and \(t_{\alpha}^{\prime}-t_{\alpha}\) is minimal. That way we define a non-asymptotic conditional confidence interval \(\widehat{B}_{\alpha}:=(t_{\alpha},t_{\alpha}^{\prime}]\) with approximate coverage \(1-\alpha\). More precisely we deduce from Corollary 1 that

\[|\mathbb{P}[Y\in\widehat{B}_{\alpha}\,|\,X\in A]-(1-\alpha)|\leq \frac{1}{2}\bar{\epsilon}_{n}(\delta/6)\\ +\sqrt{\frac{1}{\mathbb{P}[X\in A]}}\left(\sigma_{d+1}^{\star}+2 \sqrt{2}\mathcal{E}_{\theta}+(2\sqrt{2}+1)\epsilon_{n}(\delta/6)+4\varphi_{X} (A)\bar{\epsilon}_{n}(\delta/6)\right).\] (22)

In App B.6, we derive statistical guarantees for the conditional expectation and covariance of \(Y\).

## 6 Experiments

**Conditional density estimation** We applied our NCP method to a benchmark of several conditional density models including those of Rothfuss et al. (2019); Gao and Hastie (2022). See Appendix C.1 for the complete description of the data models and the complete list of compared methods in Tab. 2 with references. We also plotted several conditional CDF along with our NCP estimators in Fig. 6. To assess the performance of each method, we use Kolmogorov-Smirnov (KS) distance between the estimated and the true conditional CDFs. We test each method on nineteen different conditional values uniformly sampled between the 5%- and 95%-percentile of \(p(x)\) and computed the averaged performance over all the used conditioning values. In Tab. 1, we report mean performance (KS distance \(\pm\) std) computed over 10 repetitions, each with a different seed. NCP with whitening (NCP-W) outperforms all other methods on 4 datasets, ties with FlexCode (FC) on 1 dataset, and ranks a close second on another one behind NF. These experiments underscore NCP's consistent performance. We also refer to Tab. 3 in App C.1 for an ablation study on post-treatments for NCP.

**Confidence regions** Our goal is to estimate conditional confidence intervals for two different data models (Laplace and Cauchy). We investigate the performance of our method in (22) and compare it to the popular conditional conformal prediction approach. We refer to App C.2 for a quick description of the principle underlying CCP. We trained an NCP model combined with an MLP architecture followed by whitening post-processing. See App C.2 for the full description. We obtained that

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Model & LinearGaussian & EconDensity & ArmaJump & SkewNormal & GaussianMixture & LGGMD \\ \hline NCP - W & \(\mathbf{0.010\pm 0.000}\) & \(\mathbf{0.005\pm 0.001}\) & \(\mathbf{0.010\pm 0.002}\) & \(\mathbf{0.008\pm 0.001}\) & \(\mathbf{0.015\pm 0.004}\) & \(\mathbf{0.047\pm 0.005}\) \\ DDPM & \(\mathbf{0.410\pm 0.340}\) & \(0.236\pm 0.217\) & \(0.338\pm 0.317\) & \(0.250\pm 0.224\) & \(0.404\pm 0.242\) & \(0.405\pm 0.218\) \\ NF & \(\mathbf{0.008\pm 0.006}\) & \(\mathbf{0.006\pm 0.003}\) & \(0.143\pm 0.010\) & \(0.032\pm 0.002\) & \(0.107\pm 0.003\) & \(0.254\pm 0.004\) \\ KMN & \(0.601\pm 0.004\) & \(0.362\pm 0.017\) & \(0.487\pm 0.004\) & \(0.381\pm 0.009\) & \(0.309\pm 0.001\) & \(0.224\pm 0.005\) \\ MDN & \(0.225\pm 0.013\) & \(0.048\pm 0.001\) & \(0.163\pm 0.018\) & \(0.087\pm 0.001\) & \(0.129\pm 0.007\) & \(0.176\pm 0.013\) \\ LSCDE & \(0.420\pm 0.001\) & \(0.118\pm 0.002\) & \(0.247\pm 0.001\) & \(0.107\pm 0.001\) & \(0.202\pm 0.001\) & \(0.268\pm 0.024\) \\ CKDE & \(0.120\pm 0.000\) & \(0.010\pm 0.001\) & \(0.072\pm 0.001\) & \(\mathbf{0.023\pm 0.001}\) & \(0.048\pm 0.001\) & \(0.230\pm 0.014\) \\ NNKCDE & \(0.047\pm 0.003\) & \(0.036\pm 0.003\) & \(\mathbf{0.030\pm 0.004}\) & \(0.030\pm 0.002\) & \(0.035\pm 0.002\) & \(0.183\pm 0.006\) \\ RFCDE & \(0.128\pm 0.007\) & \(0.141\pm 0.009\) & \(0.133\pm 0.015\) & \(0.142\pm 0.012\) & \(0.130\pm 0.012\) & \(0.121\pm 0.006\) \\ FC & \(0.095\pm 0.005\) & \(0.011\pm 0.001\) & \(0.033\pm 0.002\) & \(0.035\pm 0.007\) & \(\mathbf{0.016\pm 0.001}\) & \(\mathbf{0.047\pm 0.003}\) \\ LCDE & \(0.108\pm 0.001\) & \(0.026\pm 0.001\) & \(0.113\pm 0.002\) & \(0.075\pm 0.006\) & \(0.035\pm 0.001\) & \(0.124\pm 0.002\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Mean and standard deviation of Kolmogorov-Smirnov distance of estimated CDF from the truth averaged over 10 repetitions with \(n=10^{5}\) (best method in red, second best in bold black).

[MISSING_PAGE_FAIL:9]

to \(d=1000\), the computation time increases by only 20%, while maintaining strong statistical performance throughout.

High-dimensional experiment in molecular dynamicsWe investigate protein folding dynamics and predict conditional transition probabilities between metastable states. Figure 2 shows how, by integrating our NCP approach with a graph neural network (GNN), we achieve accurate state forecasting and strong uncertainty quantification, enabling efficient tracking of transitions. For further context and a full model description, see App C.3.

## 7 Conclusion

We introduced NCP, a novel neural operator approach to learn the conditional probability distribution from complex and highly nonlinear data. NCP offers a number of benefits. Notably, it streamlines the training process by requiring just one unconditional training phase to learn the joint distribution \(p(x,y)\). Subsequently, it allows us to efficiently derive conditional probabilities and other relevant statistics from the trained model analytically, without any additional conditional training steps or Monte Carlo sampling. Additionally, our method is backed by theoretical non-asymptotic guarantees ensuring the soundness of our training method and the accuracy of the obtained conditional statistics. Our experiments on learning conditional densities and confidence regions demonstrate our approach's superiority or equivalence to leading methods, even using a simple Multi-Layer Perceptron (MLP) with two hidden layers and GELU activations. This highlights the effectiveness of a minimalistic architecture coupled with a theoretically grounded loss function. While complex architectures often dominate advanced machine learning, our results show that simplicity can achieve competitive results without compromising performance. Our numerical experiments suggest that, while our approach works well across different datasets and models, the price we pay for this generality appears to be the need for a relatively large sample size (\(n\gtrsim 10^{4}\)) to start outperforming other methods. Hence, a future direction is to study how to incorporate prior knowledge into our method to make it more data-efficient. Future works will also investigate the performance of NCP for multi-dimensional time series, causality and more general sensitivity analysis in uncertainty quantification.

Figure 2: **Protein folding dynamics. Pairwise Euclidean distances between Chignolin atoms exhibit increased variance during folded metastable states (between 87-88\(\mu\)s and around 89.5\(\mu\)s). Ground truth is depicted in blue, predicted mean in orange, and the grey lines indicate the estimated 10% lower and upper quantiles.**

## Acknowledgements

We acknowledge financial support from EU Project ELIAS under grant agreement No. 101120237, by NextGenerationEU and MUR PNRR project PE0000013 CUP J53C22003010006 "Future Artificial Intelligence Research (FAIR)" and by NextGenerationEU and MUR PNRR project RAISE "Robotics and AI for Socio-economic Empowerment" (ECS00000035).

## References

* L. Ambrogioni, U. Guclu, M. A. J. van Gerven, and E. Maris (2017)The kernel mixture network: A nonparametric method for conditional density estimation of continuous random variables. Cited by: SS1.
* G. Andrew, R. Arora, J. Bilmes, and K. Livescu (2013)Deep canonical correlation analysis. In International Conference on Machine Learning, pp. 1247-1255. Cited by: SS1.
* B. Bercu, B. Delyon, and E. Rio (2015)Concentration inequalities for sums and martingales. SpringerBriefs in Mathematics. Cited by: SS1.
* K. Bertin, C. Lacour, and V. Rivoirard (2014)Adaptive pointwise estimation of conditional density function. arXiv:1312.7402. Cited by: SS1.
* C. M. Bishop (1994)Mixture density networks. Cited by: SS1.
* A. Caponnetto and E. De Vito (2007)Optimal rates for the regularized least-squares algorithm. Foundations of Computational Mathematics7 (3), pp. 331-368. Cited by: SS1.
* L. Chanussot, A. Das, S. Goyal, T. Lavril, M. Shuaibi, M. Riviere, K. Tran, J. Heras-Domingo, C. Ho, W. Hu, A. Palizhati, A. Sriram, B. Wood, J. Yoon, D. Parikh, C. L. Zitnick, and Z. Ulassi (2021)Open catalyst 2020 (OC20) dataset and community challenges. ACS Catalysis11 (10), pp. 6059-6072. Cited by: SS1.
* V. Chernozhukov, K. Wuthrich, and Y. Zhu (2021)Distributional conformal prediction. Proceedings of the National Academy of Sciences118 (48), pp. e2107794118. Cited by: SS1.
* L. Devroye, L. Gyorfi, and G. Lugosi (1996)A probabilistic theory of pattern recognition. Springer New York. Cited by: SS1.
* P. Dhariwal and A. Nichol (2021)Diffusion models beat gans on image synthesis. In Advances in Neural Information Processing Systems, Vol. 34, pp. 8780-8794. Cited by: SS1.
* L. Dinh, D. Krueger, and Y. Bengio (2014)Nice: non-linear independent components estimation. arXiv preprint arXiv:1410.8516. Cited by: SS1.
* C. Durkan, A. Bekasov, I. Murray, and G. Papamakarios (2019)Neural spline flows. In Advances in Neural Information Processing Systems, J. Dy Dykman, A. Dykman, F. Dykman, E. Fox, and R. Garnett (Eds.), Vol. 32, pp. 1-5. Cited by: SS1.
* P. E. Freeman, R. Izbicki, and A. B. Lee (2017)A unified framework for constructing, tuning and assessing photometric redshift density estimates in a selection bias setting. Monthly Notices of the Royal Astronomical Society468 (4), pp. 4556-4565. Cited by: SS1.
* Z. Gao and T. Hastie (2022)Lin cde: conditional density estimation via lindsey's method. Journal of Machine Learning Research23 (52), pp. 1-55. Cited by: SS1.
* I. Gibbs, J. J. Cherian, and E. J. Candes (2023)Conformal prediction with conditional guarantees. arXiv:2305.12616. Cited by: SS1.
* A. Goldenshluger and O. Lepski (2011)Bandwidth selection in kernel density estimation: oracle inequalities and adaptive minimax optimality. Annals of Statistics39 (3), pp. 1608-1632. Cited by: SS1.
* P. Hall, R. C. Wolff, and Q. Yao (1999)Methods for estimating a conditional distribution function. Journal of the American Statistical Association94 (445), pp. 154-163. Cited by: SS1.
* P.

HaoChen, J. Z., Wei, C., Gaidon, A., and Ma, T. (2022). Provable guarantees for self-supervised deep learning with spectral contrastive loss. In _Advances in Neural Information Processing Systems_, volume 34, pages 5000-5011.
* Harrington (2017) Harrington, L. J. (2017). Investigating differences between event-as-class and probability density-based attribution statements with emerging climate change. _Climatic Change_, 141:641-654.
* Ho et al. (2020) Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. _Advances in neural information processing systems_, 33:6840-6851.

* Koltchinskii and Lounici (2017) Koltchinskii, V. and Lounici, K. (2017). Concentration inequalities and moment bounds for sample covariance operators. _Bernoulli_, pages 110-133.
* Kostic et al. (2024) Kostic, V., Novelli, P., Grazzi, R., Lounici, K., and Pontil, M. (2024). Learning invariant representations of time-homogeneous stochastic dynamical systems. In _International Conference on Learning Representations (ICLR)_.
* Langrene and Warin (2020) Langrene, N. and Warin, X. (2020). Fast multivariate empirical cumulative distribution function with connection to kernel density estimation. _arXiv:2005.03246_.
* Lei and Wasserman (2014) Lei, J. and Wasserman, L. (2014). Distribution-free prediction bands for non-parametric regression. _Journal of the Royal Statistical Society Series B: Statistical Methodology_, 76(1):71-96.
* Li and Racine (2006) Li, Q. and Racine, J. S. (2006). _Nonparametric Econometrics: Theory and Practice_, volume 1 of _Economics Books_. Princeton University Press.
* Li et al. (2007) Li, Y., Liu, Y., and Zhu, J. (2007). Quantile regression in reproducing kernel Hilbert spaces. _Journal of the American Statistical Association_, 102(477):255-268.
* Lu and Huang (2020) Lu, Y. and Huang, B. (2020). Structured output learning with conditional generative flows. _Proceedings of the AAAI Conference on Artificial Intelligence_, 34(04):5005-5012.
* Markowitz (1958) Markowitz, H. M. (1958). Portfolio selection: Efficient diversification of investments. _Yale University Press_, 23.
* Mendil et al. (2023) Mendil, M., Mossina, L., and Vigouroux, D. (2023). Puncc: a python library for predictive uncertainty calibration and conformalization. In _Conformal and Probabilistic Prediction with Applications_, pages 582-601. PMLR.
* Minsker (2017) Minsker, S. (2017). On some extensions of bernstein's inequality for self-adjoint operators. _Statistics & Probability Letters_, 127:111-119.
* Nagler and Czado (2016) Nagler, T. and Czado, C. (2016). Evading the curse of dimensionality in nonparametric density estimation with simplified vine copulas. _Journal of Multivariate Analysis_, 151:69-89.
* Papamakarios et al. (2017) Papamakarios, G., Pavlakou, T., and Murray, I. (2017). Masked autoregressive flow for density estimation. _Advances in neural information processing systems_, 30.
* Parzen (1962) Parzen, E. (1962). On Estimation of a Probability Density Function and Mode. _The Annals of Mathematical Statistics_, 33(3):1065-1076.
* Pospisil and Lee (2018) Pospisil, T. and Lee, A. B. (2018). Rfcde: Random forests for conditional density estimation.
* Ray et al. (2017) Ray, E. L., Sakrejda, K., Lauer, S. A., Johansson, M. A., and Reich, N. G. (2017). Infectious disease prediction with kernel conditional density estimation. _Statistical Medicine_, 36(30):4908-4929.
* Rezende and Mohamed (2015a) Rezende, D. and Mohamed, S. (2015a). Variational inference with normalizing flows. In _Proceedings of the 32nd International Conference on Machine Learning_, volume 37, pages 1530-1538.
* Raftery and Raftery (2015)Rezende, D. and Mohamed, S. (2015b). Variational inference with normalizing flows. In _International conference on machine learning_, pages 1530-1538. PMLR.
* Rigollet and Tsybakov (2007) Rigollet, P. and Tsybakov, A. (2007). Linear and convex aggregation of density estimators. _Mathematical Methods od Statistics_, 16:260-280.
* Romano et al. (2019) Romano, Y., Patterson, E., and Candes, E. (2019). Conformalized quantile regression. In Wallach, H., Larochelle, H., Beygelzimer, A., dAlche-Buc, F., Fox, E., and Garnett, R., editors, _Advances in Neural Information Processing Systems_, volume 32. Curran Associates, Inc.
* Ronneberger et al. (2015) Ronneberger, O., Fischer, P., and Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. In _Medical image computing and computer-assisted intervention-MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18_, pages 234-241. Springer.
* Rosenblatt (1956) Rosenblatt, M. (1956). Remarks on some nonparametric estimates of a density function. _The Annals of Mathematical Statistics_, 27(3):832-837.
* Rothfuss et al. (2019) Rothfuss, J., Ferreira, F., Walther, S., and Ulrich, M. (2019). Conditional density estimation with neural networks: Best practices and benchmarks. _arXiv preprint arXiv:1903.00954_.
* Schutt et al. (2023) Schutt, K. T., Hessmann, S. S. P., Gebauer, N. W. A., Lederer, J., and Gastegger, M. (2023). SchNetPack 2.0: A neural network toolbox for atomistic machine learning. _The Journal of Chemical Physics_, 158(14):144801.
* Schutt et al. (2019) Schutt, K. T., Kessel, P., Gastegger, M., Nicoli, K. A., Tkatchenko, A., and Muller, K.-R. (2019). SchNetPack: A Deep Learning Toolbox For Atomistic Systems. _Journal of Chemical Theory and Computation_, 15(1):448-455.
* Scott (1991) Scott, D. W. (1991). Feasibility of multivariate density estimates. _Biometrika_, 78(1):197-205.
* Silverman (1986) Silverman, B. W. (1986). _Density estimation for statistics and data analysis_, volume 26 of _Monographs on Statistics & Applied Probability_. Chapman and Hall.
* Silverman (2017) Silverman, B. W. (2017). _Density Estimation for Statistics and Data Analysis_. Routledge, New York.
* Sohl-Dickstein et al. (2015) Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. In _International Conference on Machine Learning_, pages 2256-2265. PMLR.
* Song et al. (2023) Song, J., Vahdat, A., Mardani, M., and Kautz, J. (2023). Pseudoinverse-guided diffusion models for inverse problems. In _International Conference on Learning Representations_.
* Stimper et al. (2023) Stimper, V., Liu, D., Campbell, A., Berenz, V., Ryll, L., Scholkopf, B., and Hernandez-Lobato, J. M. (2023). normflows: A pytorch package for normalizing flows. _Journal of Open Source Software_, 8(86):5361.
* Sugiyama et al. (2010) Sugiyama, M., Takeuchi, I., Suzuki, T., Kanamori, T., Hachiya, H., and Okanohara, D. (2010). Conditional density estimation via least-squares density ratio estimation. In _Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics_, pages 781-788.
* Tabak and Vanden-Eijnden (2010) Tabak, E. G. and Vanden-Eijnden, E. (2010). Density estimation by dual ascent of the log-likelihood. _Communications in Mathematical Sciences_, 8(1):217-233.
* Tsai et al. (2020) Tsai, Y.-H. H., Zhao, H., Yamada, M., Morency, L.-P., and Salakhutdinov, R. R. (2020). Neural methods for point-wise dependency estimation. _Advances in Neural Information Processing Systems_, 33:62-72.
* Tsybakov (2009) Tsybakov, A. B. (2009). _Introduction to Nonparametric Estimation_. Springer Series in Statistics.
* Vershynin (2011) Vershynin, R. (2011). Introduction to the non-asymptotic analysis of random matrices. _arXiv:1011.3027_.
* Vovk et al. (1999) Vovk, V., Gammerman, A., and Saunders, C. (1999). Machine-learning applications of algorithmic randomness. In _International Conference on Machine Learning_, pages 444-453.
* Vovk et al. (2015)Wang, Z., Luo, Y., Li, Y., Zhu, J., and Scholkopf, B. (2022). Spectral representation learning for conditional moment models. _arXiv preprint arXiv:2210.16525_.
* Wells et al. (2024) Wells, L., Thurimella, K., and Bacallado, S. (2024). Regularised canonical correlation analysis: graphical lasso, biplots and beyond. _arXiv preprint arXiv:2403.02979_.
* Winkler et al. (2020) Winkler, C., Worrall, D., Hoogeboom, E., and Welling, M. (2020). Learning likelihoods with conditional normalizing flows. _arXiv:1912.00042_.
* Yu and Jones (1998) Yu, K. and Jones, M. (1998). Local linear quantile regression. _Journal of the American Statistical Association_, 93(441):228-237.
* Zhang et al. (2023) Zhang, G., Ji, J., Zhang, Y., Yu, M., Jaakkola, T. S., and Chang, S. (2023). Towards coherent image inpainting using denoising diffusion implicit models. _arXiv:2304.03322_.

## Supplemental material

The appendix is organized as follows:

* Appendix A provides additional details on the post-processing for NCP.
* Appendix B contains the proofs of the theoretical results and additional statistical results.
* In Appendix C, comprehensive details are presented regarding the experiment benchmark utilized to evaluate the performances of NCP.

## Appendix A Details on training and algorithms

### Practical guidelines for training NCP

* It is better to choose a larger \(d\) rather than a smaller one. Typically for the problems we considered in Section 6, we used \(d\in\{100,500\}\).
* The regularization parameter \(\gamma\) was found to yield the best results for \(\gamma\in\{10^{-2},10^{-3}\}\).
* To ensure the positivity of the singular values, we transform the vector \(w^{\theta}\) with the Gaussian function \(x\mapsto\exp(-x^{2})\) to recover \(\sigma^{\theta}\) during any call of the forward method. The vector \(w^{\theta}\) is initialized at random with parameters following a normal distribution of mean \(0\) and standard deviation \(\sfrac{1}{d}\).
* With the ReLU function, we observe instabilities in the loss function during training, whereas Tanh struggles to converge. In contrast, the use of GELU solves both problems.
* We can compute some statistical objects as a sanity check for the convergence of NCP training. For instance, we can ensure that the computed conditional CDFsatisfies all the conditions to be a valid CDF.
* After training, an additional post-processing may be applied to ensure the orthogonality of operators \(u^{\theta}\) and \(v^{\theta}\). This _whitening_ step is described in Alg 2 in App A.3. It leads to an improvement of statistical accuracy of the trained NCP model. See the ablation study in Tab. 3.

### Learning dynamics with NCP

### Whitening post-processing

We describe in Algorithm 2 the whitening post-processing procedure that we apply after training.

Figure 4: Learning dynamic for the Laplace experiment in Section 6.

## Appendix B Proofs of theoretical results

### A reminder on Hilbert spaces and compact operators

**Definition 1**.: _Given a vector space \(\mathcal{H}\), we say it is a Hilbert space if there exists an inner product \(\langle\cdot,\cdot\rangle\) such that:_

\[\mathcal{H}\text{ is complete with respect to the norm }\|x\|=\sqrt{\langle x,x\rangle}\text{ for all }x\in\mathcal{H}.\]

An important example of an infinite-dimensional Hilbert space is \(L^{2}_{\mu}(\mathbb{R})\), the space of square-integrable functions w.r.t probability measure \(\mu\) on \(\mathbb{R}\) with the inner product defined as \(\langle f,g\rangle=\int_{\mathbb{R}}f(x)\overline{g(x)}\,\mu(dx)\).

**Definition 2** (Bounded Operators).: _Let \(\mathcal{H}_{1}\) and \(\mathcal{H}_{2}\) be Hilbert spaces. A linear operator \(T:\mathcal{H}_{1}\to\mathcal{H}_{2}\) is called bounded if there exists a constant \(C\geq 0\) such that for all \(x\in\mathcal{H}_{1}\), the following inequality holds:_

\[\|Tx\|_{\mathcal{H}_{2}}\leq C\|x\|_{\mathcal{H}_{1}}.\]

_The smallest such constant \(C\) is called the operator norm of \(T\), denoted by \(\|T\|\), and is given by:_

\[\|T\|=\sup_{x\neq 0}\frac{\|Tx\|_{\mathcal{H}_{2}}}{\|x\|_{\mathcal{H}_{1}}}.\]

Bounded operators are continuous and play a key role in functional analysis.

**Definition 3** (Compact Operators).: _Let \(\mathcal{H}_{1}\) and \(\mathcal{H}_{2}\) be Hilbert spaces. A bounded linear operator \(T:\mathcal{H}_{1}\to\mathcal{H}_{2}\) is called compact if for any bounded sequence \(\{x_{n}\}\subset\mathcal{H}_{1}\), there exists a subsequence \(\{x_{n_{k}}\}\) such that \(Tx_{n_{k}}\) converges in \(\mathcal{H}_{2}\)._

Compact operators can be viewed as infinite-dimensional analogues of matrices with finite rank in finite-dimensional spaces.

A key result in the theory of compact operators is the existence of a _singular value decomposition (SVD)_ for compact operators. The following is the statement of the _Eckart-Young-Mirsky theorem_:

**Theorem 3** (Eckart-Young-Mirsky).: _Let \(T:\mathcal{H}_{1}\to\mathcal{H}_{2}\) be a compact operator between Hilbert spaces. Then \(T\) can be decomposed as:_

\[T=\sum_{i=1}^{\infty}\sigma_{i}\langle\cdot,u_{i}\rangle v_{i},\]_where \(\{u_{i}\}\subset\mathcal{H}_{1}\) and \(\{v_{i}\}\subset\mathcal{H}_{2}\) are orthonormal sets, and \(\sigma_{i}\) are the singular values of \(T\), which satisfy \(\sigma_{1}\geq\sigma_{2}\geq\cdots\geq 0\)._

_Moreover, for any rank-\(k\) operator \(T_{k}=\sum_{i=1}^{k}\sigma_{i}\langle\cdot,u_{i}\rangle v_{i}\), we have:_

\[\|T-T_{k}\|=\min_{\text{rank}(S)\leq k}\|T-S\|,\]

_where \(\|\cdot\|\) is the operator norm induced by the Hilbert spaces._

### Proof of Lemma 1

Proof of Lemma 1.: It follows from (3) and (5) that

\[\mathbb{P}[Y\in B\,|\,X\in A]-\mathbb{P}[Y\in B]-\frac{\langle \mathbb{1}_{A},[\![\mathsf{D}_{Y|X}]\!]_{d}\mathbb{1}_{B}\rangle}{\mathbb{P}[ X\in A]}=\frac{\langle\mathbb{1}_{A},(\mathsf{D}_{Y|X}-[\![\mathsf{D}_{Y|X}]\!]_{d} \mathbb{1}_{B}\rangle}{\mathbb{P}[X\in A]}.\]

Next, by definition of the operator norm, we have

\[|\langle\mathbb{1}_{A},(\mathsf{D}_{Y|X}-[\![\mathsf{D}_{Y|X}]\!] _{d})\mathbb{1}_{B}\rangle| \leq\|\mathsf{D}_{Y|X}-[\![\mathsf{D}_{Y|X}]\!]_{d}\|_{L^{2}_{2}( \mathcal{Y})\to L^{2}_{\mu}(X)}\|\mathbb{1}_{A}\|_{L^{2}_{\mu}(\mathcal{X})} \|\mathbb{1}_{B}\|_{L^{2}_{2}(\mathcal{Y})}\] \[=\|\mathsf{D}_{Y|X}-[\![\mathsf{D}_{Y|X}]\!]_{d}\|_{L^{2}_{2}( \mathcal{Y})\to L^{2}_{\mu}(X)}\sqrt{\mathbb{P}[X\in A]}\sqrt{\mathbb{P}[Y\in B ]},\]

where the operator norm \(\|\mathsf{D}_{Y|X}-[\![\mathsf{D}_{Y|X}]\!]_{d}\|_{L^{2}_{\nu}(\mathcal{Y}) \to L^{2}_{\mu}(\mathcal{X})}\) is upper bounded by \(\sigma^{\star}_{d+1}\) by definition of the SVD of \(\mathsf{D}_{Y|X}\). 

### Proof of Theorem 1

Proof of Theorem 1.: In the following, to simplify notation, whenever dependency on the parameters is not crucial, recalling that \((X,Y)\) and \((X^{\prime},Y^{\prime})\) are two iid samples from the joint distribution \(\rho\), we will denote the vector-valued random variables in the latent (embedding) space as \(u:=u^{\theta}(X)\), \(u^{\prime}:=u^{\theta}(X^{\prime})\), \(v:=v^{\theta}(Y)\) and \(v^{\prime}:=v^{\theta}(Y^{\prime})\), as well as \(s=\sigma^{\theta}\) and \(S:=S_{\theta}\). Then, we can write the training loss simply as \(\mathbb{E}\left[L_{\gamma}(u-\mathbb{E}u,u-\mathbb{E}u^{\prime},v-\mathbb{E}v, v^{\prime}-\mathbb{E}v^{\prime},S)\right]\).

Let us further denote centered features as \(\overline{u}=u-\mathbb{E}u\) and \(\overline{v}=v-\mathbb{E}v\), and the operators based on them as \(\overline{U}_{\theta}\colon\mathbb{R}^{d}\to L^{2}_{\mu}(\mathcal{X})\) and \(\overline{V}_{\theta}\colon\mathbb{R}^{d}\to L^{2}_{\mu}(\mathcal{Y})\) by

\[\overline{U}_{\theta}z:=z^{\top}(u^{\theta}-\mathbb{E}[u^{(}Q(X)])\mathbb{1} _{\mathcal{X}}\;\;\text{and}\;\;\overline{V}_{\theta}z:=z^{\top}(v^{\theta}- \mathbb{E}[v^{\theta}(Y)])\mathbb{1}_{\mathcal{Y}},\;\text{for}\;z\in\mathbb{ R}^{d}.\]

and prove that \(\mathcal{L}_{0}(\theta)=\|\overline{U}_{\theta}S_{\theta}\overline{V}_{\theta} ^{\star}\|_{\mathrm{HS}}^{2}-2\operatorname{tr}(S_{\theta}\overline{U}_{\theta }^{\star}\mathsf{D}_{Y|X}\overline{V}_{\theta})\). Since we have that \(\overline{U}_{\theta}=J_{\mu}U_{\theta}\) and \(\overline{V}_{\theta}=J_{\nu}V_{\theta}\), where \(J_{\mu}=I-\mathbb{1}_{\mathcal{X}}\otimes\mathbb{1}_{\mathcal{X}}\) and \(J_{\nu}=I-\mathbb{1}_{\mathcal{Y}}\otimes\mathbb{1}_{\mathcal{Y}}\) are orthogonal projectors in \(L^{2}_{\mu}(\mathcal{X})\) and \(L^{2}_{\nu}(\mathcal{Y})\), respectively, as well as \(\overline{U}_{\theta}^{\star}\mathsf{D}_{Y|X}\overline{V}_{\theta}=U_{\theta }^{\star}J_{\mu}\mathsf{D}_{Y|X}J_{\nu}V_{\theta}=U_{\theta}^{\star}J_{\mu} \mathsf{E}_{Y|X}J_{\nu}V_{\theta}\), consequently

\[\overline{U}_{\theta}^{\star}\mathsf{D}_{Y|X}\overline{V}_{\theta}=U_{\theta}^{ \star}\mathsf{E}_{Y|X}V_{\theta}-U_{\theta}^{\star}\mathbb{1}_{\mathcal{X}} \otimes(V_{\theta}^{\star}\mathbb{1}_{\mathcal{Y}})=\mathbb{E}[u^{\theta}(X) \mathbb{E}[v^{\theta}(Y)^{\top}\,|\,X]]-(\mathbb{E}[u^{\theta}(X)])(\mathbb{E }[v^{\theta}(Y)])^{\top},\]

that is \(\overline{U}_{\theta}^{\star}\mathsf{D}_{Y|X}\overline{V}_{\theta}=\mathbb{E}[ uv^{\top}]-\mathbb{E}[u]\mathbb{E}[v]^{\top}\) is simply centered cross-covariance in the embedding space. Recalling that \(U_{\theta}^{\star}U_{\theta}=\mathbb{E}[uu^{\top}]\) and \(V_{\theta}^{\star}V_{\theta}=\mathbb{E}[vv^{\top}]\) are covariance matrices in the embedding space, similarly we get that \(\overline{U}_{\theta}^{\star}\overline{U}_{\theta}=\mathbb{E}[(u-\mathbb{E}u)(u -\mathbb{E}u)^{\top}]\) and \(\overline{V}_{\theta}^{\star}\overline{V}_{\theta}=\mathbb{E}[(v-\mathbb{E}v)(v -\mathbb{E}v)^{\top}]\) are centered covariances. Thus, we obtain

\[\mathcal{L}_{0}(\theta) =-2\operatorname{tr}\mathbb{E}[(S^{1/2}\overline{u})(S^{1/2} \overline{v})^{\top}]+\operatorname{tr}(\mathbb{E}[(S^{1/2}\overline{u})(S^{1/2} \overline{u})^{\top}]\mathbb{E}[(S^{1/2}\overline{v})(S^{1/2}\overline{v})^{ \top}])\] \[=-2\mathbb{E}[\overline{u}^{\top}S\overline{v}]+\operatorname{tr}( \mathbb{E}[(S^{1/2}\overline{u})(S^{1/2}\overline{u})^{\top}]\mathbb{E}[(S^{1/2} \overline{v})(S^{1/2}\overline{v})^{\top}])\]

which, by taking \((X,Y)\) and \((X^{\prime},Y^{\prime})\) to be iid random variables drawn from \(\rho\), gives that \(\mathcal{L}_{0}(\theta)\) can be written as

\[\mathbb{E}\left[-\overline{u}S\overline{v}-\overline{u}^{\prime}S \overline{v}^{\prime}+\overline{u}^{\prime}S\overline{v}+\overline{u}S\overline{v}^{ \prime}+\frac{1}{2}\operatorname{tr}\left(S^{1/2}\overline{u}\overline{u}^{\top}S \overline{v}^{\prime}\overline{v}^{\prime\top}S^{1/2}+S^{1/2}\overline{u}^{\prime} \overline{u}^{\prime\top}S\overline{v}\overline{v}^{\top}S^{1/2}\right)\right]\] \[=\mathbb{E}\left[\frac{1}{2}\left(\overline{u}^{\top}S \overline{v}^{\prime}\right)^{2}+\frac{1}{2}\left(\overline{u}^{\prime\top}S \overline{v}\right)^{2}-(\overline{u}-\overline{u}^{\prime})S(\overline{v}- \overline{v}^{\prime})\right]=\mathbb{E}\left[L_{0}(\overline{u},\overline{u}^{ \prime},\overline{v},\overline{v}^{\prime},s)\right]\] \[=\mathbb{E}\left[L_{0}(u^{\theta}(X)-\mathbb{E}u^{\theta}(X),u^{ \theta}(X^{\prime})-\mathbb{E}u^{\theta}(X^{\prime}),v^{\theta}(Y)-\mathbb{E}u^{ \theta}(Y),v^{\theta}(Y^{\prime})-\mathbb{E}u^{\theta}(Y^{\prime}),\sigma^{ \theta})\right].\]which implies that \(\mathcal{L}_{0}(\theta)=\|\overline{U}_{\theta}S_{\theta}\overline{V}_{\theta}^{*} \|_{\mathrm{HS}}^{2}-2\operatorname{tr}(S_{\theta}\overline{U}_{\theta}^{*} \mathsf{D}_{Y|X}\overline{V}_{\theta})\). Moreover, to show that \(\mathcal{L}_{\gamma}(\theta)=\mathcal{L}_{0}(\theta)+\gamma\,\mathcal{R}(\theta)\). It suffices to note that

\[\|U_{\theta}^{*}U_{\theta}-I\|_{F}^{2} =\operatorname{tr}((U_{\theta}^{*}U_{\theta}-I)^{2})= \operatorname{tr}((U_{\theta}^{*}U_{\theta})^{2}-2U_{\theta}^{*}U_{\theta}+I)=\] \[=\operatorname{tr}(\mathbb{E}[uu^{\top}]\mathbb{E}[u^{\prime}u^ {\prime\top}]\!-\!\mathbb{E}[uu^{\top}]\!-\!\mathbb{E}[u^{\prime}u^{\prime \top}]\!+\!I)\] \[=\mathbb{E}[\!\operatorname{tr}(uu^{\top}u^{\prime}u^{\prime}{}^{ \top}\!-\!uu^{\top}\!-\!u^{\prime}u^{\prime}{}^{\top}\!+\!I)]=\mathbb{E}\, \left[(u^{\top}u^{\prime})^{2}-\|u\|^{2}-\|u^{\prime}\|^{2}\right]+d,\]

as well as that \(\|U_{\theta}^{*}\mathbb{I}_{\mu}\|^{2}=\|\mathbb{E}u\|^{2}=(\mathbb{E}u)^{\top }(\mathbb{E}u)=\mathbb{E}u^{\top}u^{\prime}\), and apply the analogous reasoning for random variable \(Y\sim\nu\).

Now, given \(r>d+1\), let us denote \(D_{r}:=\sum_{i\in[r]}\sigma_{i}^{*}u_{i}^{*}\otimes v_{i}^{*}\) and

\[\mathcal{L}_{0}^{r}(\theta):=\left\|D_{r}-\overline{U}_{\theta}S_{\theta} \overline{V}_{\theta}\right\|_{\mathrm{HS}}^{2}-\left\|D_{r}\right\|_{ \mathrm{HS}}^{2}.\] (23)

Then, applying the Eckhart-Young-Mirsky theorem, we obtain that

\[\mathcal{L}_{0}^{r}(\theta)\geq\sum_{i=d+1}^{r}\sigma_{i}^{*2}-\sum_{i\in[r]} \sigma_{i}^{*2}=-\sum_{i\in[d]}\sigma_{i}^{*2},\]

with equality holding whenever \((\sigma_{\theta}^{\dagger},u_{\theta}^{\dagger},v_{\theta}^{\dagger})=(\sigma _{i}^{*},u_{i}^{*},v_{i}^{*})\), \(\rho\)-almost everywhere.

To prove that the same holds for \(\mathcal{L}_{0}(\theta)\), observe that after expanding the HS norm via trace in (23), we have that

\[\mathcal{L}_{0}^{r}(\theta)=-2\operatorname{tr}\left(S_{\theta}^{1/2} \overline{U}_{\theta}^{*}D_{r}\overline{V}_{\theta}S_{\theta}^{1/2}\right)+ \|U_{\theta}S_{\theta}V_{\theta}^{*}\|_{\mathrm{HS}}^{2}\,,\]

and, consequently,

\[\mathcal{L}_{0}^{r}(\theta)=\|\overline{U}_{\theta}S_{\theta}\overline{V}_{ \theta}^{*}\|_{\mathrm{HS}}^{2}-2\operatorname{tr}(S_{\theta}^{1/2} \overline{U}_{\theta}^{*}D_{r}\overline{V}_{\theta}S_{\theta}^{1/2})=\mathcal{ L}_{0}(\theta)+2\operatorname{tr}(S_{\theta}\overline{U}_{\theta}^{*}(\mathsf{D}_{Y|X}-D_{r}) \overline{V}_{\theta}).\]

Thus, using Cauchy-Schwartz inequality, we obtain

\[|\mathcal{L}_{0}^{r}(\theta)-\mathcal{L}_{0}(\theta)|\leq|\operatorname{tr}(S _{\theta}\overline{U}_{\theta}^{*}(\mathsf{D}_{Y|X}-D_{r})\overline{V}_{\theta} )|\leq\|S_{\theta}\|\|\overline{U}_{\theta}^{*}\|_{\mathrm{HS}}\|\mathsf{D}_{Y| X}-[\![\mathsf{D}_{Y|X}]\!]_{r}\|\|\overline{V}_{\theta}^{*}\|_{\mathrm{HS}},\]

and, therefore, \(|\mathcal{L}_{0}^{r}(\theta)-\mathcal{L}_{0}(\theta)|\leq\sigma_{r+1}^{*} \sqrt{\operatorname{tr}(U_{\theta}^{*}U_{\theta})\operatorname{tr}(V_{\theta}^ {*}V_{\theta})}\leq Mod\sigma_{r+1}^{*}\), where the constant is given by \(M:=\max_{i\in[d]}\{\|u_{i}^{\theta}-\mathbb{E}u_{i}^{\theta}\|_{L_{\mu}^{2}(X )},\|v_{\theta}^{\theta}-\mathbb{E}v_{\theta}^{\theta}\|_{L_{2}^{2}(\mathcal{ Y})}\}<\infty\). So, \(\mathcal{L}_{0}^{r}(\theta)-Md\sigma_{r+1}^{*}\leq\mathcal{L}_{0}(\theta)\leq \mathcal{L}_{0}^{r}(\theta)+Md\sigma_{r+1}^{*}\), and, since \(r>d+1\) was arbitrary, we can take \(r\) arbitrary large to obtain \(\sigma_{r}^{*}\to 0\) and conclude that \(\mathcal{L}_{0}(\theta)\geq-\sum_{i\in[d]}\sigma_{i}^{*2}\), with equality holding when \((\sigma_{\theta}^{\dagger},u_{\theta}^{\dagger},v_{\theta}^{\dagger})=(\sigma_{i }^{*},u_{i}^{*},v_{i}^{*})\), \(\rho\)-almost everywhere, since then \(\overline{U}_{\theta}^{*}\mathsf{D}_{Y|X}\overline{V}_{\theta}=U_{\theta}^{*} \mathsf{D}_{Y|X}V_{\theta}=U_{\theta}^{*}U_{\theta}S_{\theta}=S_{\theta}= \operatorname{diag}(\sigma_{1},\ldots,\sigma_{d})\).

Finally, we prove that \(\gamma>0\) and \(\sigma_{d}^{*}>\sigma_{d+1}^{*}\) assure uniqueness of the global optimum. First, if the global minimum is achieved \(\sigma_{d}^{*}>\sigma_{d+1}^{*}\) allows one to use uniqueness result in the Eckhart-Young-Mirsky theorem that states that \(\sum_{i\in[d]}\sigma_{i}^{*}\,u_{i}^{*}\otimes\widehat{v}_{i}^{\theta}=\sum_{i \in[d]}\sigma_{i}^{\theta}\,\widehat{u}_{i}^{\theta}\otimes\widehat{v}_{i}^{\theta}\). But since, \(\gamma>0\) implies that \(\mathcal{R}(\theta)=0\), i.e. \((u_{i}^{\theta})_{i\in[d]}\subset L_{\mu}^{*}(\mathcal{X})\) and \((v_{i}^{\theta})_{i\in[d]}\subset L_{\nu}^{*}(\mathcal{Y})\) are two orthonormal systems in the corresponding orthogonal complements of constant functions, using the uniqueness of SVD, the proof is completed. 

### Proof of Theorem 2

Proof of Theorem 2.: Let us denote the operators arising from centered and empirically centered features as \(\overline{U}_{\theta},\,\widehat{U}_{\theta}\colon\mathbb{R}^{d}\to L_{\mu}^{2}( \mathcal{X})\) and \(\widehat{V}_{\theta},\overline{V}_{\theta}\colon\mathbb{R}^{d}\to L_{\nu}^{2}( \mathcal{Y})\) by

\[\overline{U}_{\theta}z:=z^{\top}(u^{\theta}-\mathbb{E}[u^{\theta}(X)])\mathbb{1}_{ \mathcal{X}},\ \overline{V}_{\theta}z:=z^{\top}(v^{\theta}-\mathbb{E}[v^{\theta}(Y)]) \mathbb{1}_{\mathcal{Y}}\ \text{ and }\ \widehat{U}_{\theta}z:=z^{\top}\widehat{u}^{\theta},\ \widehat{V}_{\theta}z:=z^{\top}\widehat{v}^{\theta},\]

respectively, for \(z\in\mathbb{R}^{d}\).

We first bound the error of the conditional expectation model as \(\|\mathsf{D}_{Y|X}-\widehat{\mathsf{D}}_{Y|X}^{\theta}\|\) as follows.

\[\|\mathsf{D}_{Y|X}-\widehat{\mathsf{D}}_{Y|X}^{\theta}\| =\|\mathsf{D}_{Y|X}\pm[\![\mathsf{D}_{Y|X}]\!]_{d}\pm U_{\theta}^{*}S _{\theta}V_{\theta}\pm\overline{U}_{\theta}^{*}S_{\theta}\overline{V}_{\theta}- \widehat{U}_{\theta}^{*}S_{\theta}\widehat{V}_{\theta}\|\] \[\leq\sigma_{d+1}^{*}+\mathcal{E}_{\theta}+\|U_{\theta}^{*}S_{ \theta}V_{\theta}-\overline{U}_{\theta}^{*}S_{\theta}\overline{V}_{\theta}\|+\| \overline{U}_{\theta}^{*}S_{\theta}\overline{V}_{\theta}-\widehat{U}_{\theta}^{*}S_ {\theta}\widehat{V}_{\theta}\|.\]Next, using that \(\|S_{\theta}\|\leq 1\) and that centered covariances are bounded by uncentered ones, i.e. \(\overline{U_{\theta}^{*}\partial\overline{U}_{\theta}}\preceq U_{\theta}^{*}U_{\theta}\), we have

\[\|U_{\theta}^{*}S_{\theta}V_{\theta}-\overline{U}_{\theta}^{*}S_{ \theta}\overline{V}_{\theta}\| =\|U_{\theta}^{*}S_{\theta}V_{\theta}\pm\overline{U}_{\theta}^{*}S _{\theta}V_{\theta}-\overline{U}_{\theta}^{*}S_{\theta}\overline{V}_{\theta}\|\] \[\leq\|U_{\theta}-\overline{U}_{\theta}\|\|V_{\theta}\|+\| \overline{U}_{\theta}\|\|V_{\theta}-\overline{V}_{\theta}\|\] \[\leq\|U_{\theta}^{*}\mathbb{1}_{\mathcal{X}}\|\|V_{\theta}^{*}V_{ \theta}\|^{1/2}+\|V_{\theta}^{*}\mathbb{1}_{\mathcal{Y}}\|\|U_{\theta}^{*}U_{ \theta}\|^{1/2}\leq 2\mathcal{E}_{\theta}\sqrt{1+\mathcal{E}_{\theta}}.\]

In a similar way, we obtain

\[\|\overline{U}_{\theta}^{*}S_{\theta}\overline{V}_{\theta}- \widehat{U}_{\theta}^{*}S_{\theta}\widehat{V}_{\theta}\| =\|\overline{U}_{\theta}^{*}S_{\theta}\overline{V}_{\theta}\pm \widehat{U}_{\theta}^{*}S_{\theta}\overline{V}_{\theta}-\widehat{U}_{\theta}^{* }S_{\theta}\widehat{V}_{\theta}\|\] \[\leq\|\overline{U}_{\theta}-\widehat{U}_{\theta}\|\|\overline{V} _{\theta}\|+\|\widehat{U}_{\theta}\|\|\overline{V}_{\theta}-\widehat{V}_{ \theta}\|\] \[\leq\|\overline{U}_{\theta}-\widehat{U}_{\theta}\|\|\overline{V} _{\theta}\|+\|\overline{V}_{\theta}-\widehat{V}_{\theta}\|\|\overline{U}_{ \theta}\|+\|\overline{U}_{\theta}-\widehat{U}_{\theta}\|\|\overline{V}_{\theta }-\widehat{V}_{\theta}\|\] \[\leq\sqrt{1+\mathcal{E}_{\theta}}\big{(}\|\widehat{\mathbb{E}}_{x} [u^{\theta}]-\mathbb{E}[u^{\theta}(X)]\|+\|\widehat{\mathbb{E}}_{y}[v^{\theta} ]-\mathbb{E}[v^{\theta}(Y)]\|\big{)}\] \[\qquad\qquad\qquad+\|\widehat{\mathbb{E}}_{x}[u^{\theta}]- \mathbb{E}[u^{\theta}(X)]\|\,\|\widehat{\mathbb{E}}_{y}[v^{\theta}]-\mathbb{E} [v^{\theta}(Y)]\|\] \[\leq 2\sqrt{1+\mathcal{E}_{\theta}}\varepsilon_{n}(\delta)+[ \varepsilon_{n}(\delta)]^{2}.\]

where \(\|\widehat{\mathbb{E}}_{x}[u^{\theta}]-\mathbb{E}[u^{\theta}(X)]\|\leq \varepsilon_{n}(\delta)\) and \(\|\widehat{\mathbb{E}}_{y}[v^{\theta}]-\mathbb{E}[v^{\theta}(Y)]\|\leq \varepsilon_{n}(\delta)\) hold w.p.a.l. \(1-\delta\) in view of Lemma 2.

To summarize, it holds w.p.a.l. \(1-\delta\)

\[\|\mathsf{D}_{Y|X}-\widehat{\mathsf{D}}_{Y|X}^{\theta}\|\leq\sigma_{d+1}^{ \star}+\mathcal{E}_{\theta}+2\sqrt{1+\mathcal{E}_{\theta}}(\mathcal{E}_{\theta }+\varepsilon_{n}(\delta))+[\varepsilon_{n}(\delta)]^{2}=:\psi_{n}(\delta).\] (24)

By definition in (5) and (14), we have

\[\mathbb{P}[Y\in B\,|\,X\in A]-\widehat{p}_{\theta}(B\,|\,A)=\mathbb{E}[ \mathbb{1}_{B}(Y)]-\widehat{\mathbb{E}}_{y}[\mathbb{1}_{B}]+\frac{\langle \mathbb{1}_{A},\mathsf{D}_{Y|X}\mathbb{1}_{B}\rangle}{\mathbb{E}[\mathbb{1}_ {A}(X)]}-\frac{\langle\mathbb{1}_{A},\widehat{\mathsf{D}}_{Y|X}^{\theta} \mathbb{1}_{B}\rangle}{\widehat{\mathbb{E}}_{x}[\mathbb{1}_{A}]},\]

and

\[\frac{\langle\mathbb{1}_{A},\mathsf{D}_{Y|X}\mathbb{1}_{B}\rangle}{\mathbb{E} [\mathbb{1}_{A}(X)]}=\frac{\langle\mathbb{1}_{A},(\mathsf{D}_{Y|X}-\widehat {\mathsf{D}}_{Y|X}^{\theta})\mathbb{1}_{B}\rangle}{\mathbb{E}[\mathbb{1}_{A} (X)]}+\frac{\langle\mathbb{1}_{A},\widehat{\mathsf{D}}_{Y|X}^{\theta} \mathbb{1}_{B}\rangle}{\widehat{\mathbb{E}}_{x}[\mathbb{1}_{A}]}\frac{ \widehat{\mathbb{E}}_{x}[\mathbb{1}_{A}]}{\mathbb{E}[\mathbb{1}_{A}(X)]}.\]

Note also that \(\|\mathbb{1}_{A}(X)\|_{L_{\mu}^{2}(\mathcal{X})}=\sqrt{\mathbb{E}[1_{A}(X)]}= \sqrt{\mathbb{P}[X\in A]}\), \(\|\mathbb{1}_{B}(Y)\|_{L_{\nu}^{2}(\mathcal{Y})}=\sqrt{\mathbb{E}[1_{B}(Y)]}= \sqrt{\mathbb{P}[Y\in B]}\), for any \(A\in\Sigma_{\mathcal{X}}\) and \(B\in\Sigma_{\mathcal{Y}}\) and

\[|\langle\mathbb{1}_{A},(\mathsf{D}_{Y|X}-\widehat{\mathsf{D}}_{Y|X}^{\theta} )\mathbb{1}_{B}\rangle|\leq\|\mathsf{D}_{Y|X}-\widehat{\mathsf{D}}_{Y|X}^{ \theta}\|\|\mathbb{1}_{A}(X)\|_{L_{\mu}^{2}(\mathcal{X})}\|\mathbb{1}_{B}(Y)\| _{L_{\nu}^{2}(\mathcal{Y})}.\]

Combining the previous observations, we get

\[|\mathbb{P}[Y\in B\,|\,X\in A]-\widehat{p}_{\theta}(B\,|\,A)| \leq\left(\frac{\widehat{\mathbb{E}}_{y}[\mathbb{1}_{B}]-\mathbb{E}[ \mathbb{1}_{B}(Y)]}{\mathbb{E}[\mathbb{1}_{B}(Y)]}+\frac{\|\mathsf{D}_{Y|X}- \widehat{\mathsf{D}}_{Y|X}^{\theta}\|}{\sqrt{\mathbb{E}[1_{A}(X)]\mathbb{E}[ \mathbb{1}_{B}(Y)]}}\right)\mathbb{E}[\mathbb{1}_{B}(Y)]\] \[\qquad\qquad+\frac{|\langle\mathbb{1}_{A},\widehat{\mathsf{D}}_{Y|X }^{\theta}\mathbb{1}_{B}\rangle|}{\widehat{\mathbb{E}}_{x}[\mathbb{1}_{A}]} \frac{|\widehat{\mathbb{E}}_{x}[\mathbb{1}_{A}]-\mathbb{E}[\mathbb{1}_{A}(X)]|}{ \mathbb{E}[\mathbb{1}_{A}(X)]},\] (25)

and

\[\frac{|\langle\mathbb{1}_{A},\widehat{\mathsf{D}}_{Y|X}^{\theta} \mathbb{1}_{B}\rangle|}{\widehat{\mathbb{E}}_{x}[\mathbb{1}_{A}]} \leq\frac{\mathbb{E}[\mathbb{1}_{A}(X)]}{\widehat{\mathbb{E}}_{x}[ \mathbb{1}_{A}]}\left(\frac{|\langle\mathbb{1}_{A},\mathsf{D}_{Y|X}\mathbb{1}_{B} \rangle|}{\mathbb{E}[\mathbb{1}_{A}(X)]}+\frac{|\langle\mathbb{1}_{A},(\mathsf{D}_{Y|X }-\widehat{\mathsf{D}}_{Y|X}^{\theta})\mathbb{1}_{B}\rangle|}{\mathbb{E}[ \mathbb{1}_{A}(X)]}\right)\] \[\leq\frac{\mathbb{E}[\mathbb{1}_{A}(X)]}{\widehat{\mathbb{E}}_{x}[ \mathbb{1}_{A}]}\left(\|\mathsf{D}_{Y|X}\|+\|\mathsf{D}_{Y|X}-\widehat{ \mathsf{D}}_{Y|X}^{\theta}\|\right)\sqrt{\frac{\mathbb{E}[\mathbb{1}_{B}(Y)]}{ \mathbb{E}[\mathbb{1}_{A}(X)]}}\] \[\leq\frac{\mathbb{E}[\mathbb{1}_{A}(X)]}{\widehat{\mathbb{E}}_{x}[ \mathbb{1}_{A}]}\sqrt{\frac{\mathbb{E}[\mathbb{1}_{B}(Y)]}{\mathbb{E}[ \mathbb{1}_{A}(X)]}}\left(1+\|\mathsf{D}_{Y|X}-\widehat{\mathsf{D}}_{Y|X}^{ \theta}\|\right),\] (26)where we have used that \(\|\mathsf{D}_{Y|X}\|\leq 1\).

Similarly, we have

\[\frac{\mathbb{P}[Y\in B\,|\,X\in A]}{\mathbb{P}[Y\in B]}-\frac{ \widehat{p}_{\theta}(B\,|\,A)}{\widehat{p}_{y}(B)}\] \[\quad=\frac{\langle\mathbb{1}_{A},\mathsf{D}_{Y|X}\mathbbm{1}_{B} \rangle}{\mathbb{E}[\mathbb{1}_{A}(X)]\mathbb{E}[\mathbb{1}_{B}(Y)]}-\frac{ \langle\mathbb{1}_{A},\widehat{\mathsf{D}}_{Y|X}^{\theta}\mathbb{1}_{B}\rangle }{\widehat{\mathsf{E}}_{x}[\mathbb{1}_{A}]\widehat{\mathsf{E}}_{y}[\mathbb{1}_ {B}]}\] \[\quad=\frac{\langle\mathbb{1}_{A},(\mathsf{D}_{Y|X}-\widehat{ \mathsf{D}}_{Y|X}^{\theta})\mathbb{1}_{B}\rangle}{\widehat{\mathsf{E}}_{x}[ \mathbb{1}_{A}]\widehat{\mathsf{E}}_{y}[\mathbb{1}_{B}]}\] \[\quad\quad\quad\quad+\langle\mathbb{1}_{A},\widehat{\mathsf{D}}_{ Y|X}^{\theta}\mathbb{1}_{B}\rangle\left(\frac{1}{\mathbb{E}[\mathbb{1}_{A}(X)] \mathbb{E}[\mathbb{1}_{B}(Y)]}-\frac{1}{\widehat{\mathsf{E}}_{x}[\mathbb{1}_ {A}]\widehat{\mathsf{E}}_{y}[\mathbb{1}_{B}]}\right)\] \[\quad=\frac{\langle\mathbb{1}_{A},(\mathsf{D}_{Y|X}-\widehat{ \mathsf{D}}_{Y|X}^{\theta})\mathbb{1}_{B}\rangle}{\widehat{\mathsf{E}}_{x}[ \mathbb{1}_{A}]\widehat{\mathsf{E}}_{y}[\mathbb{1}_{B}]}\] \[\quad\quad\quad+\frac{\langle\mathbb{1}_{A},\widehat{\mathsf{D}} _{Y|X}^{\theta}\mathbb{1}_{B}\rangle}{\mathbb{E}[\mathbb{1}_{A}(X)]\mathbb{E} [\mathbb{1}_{B}(Y)]}\left(\frac{(\widehat{\mathsf{E}}_{x}[\mathbb{1}_{A}]- \mathbb{E}[\mathbb{1}_{A}(X)]\widehat{\mathsf{E}}_{y}[\mathbb{1}_{B}]+ \mathbb{E}[\mathbb{1}_{A}(X)](\widehat{\mathsf{E}}_{y}[\mathbb{1}_{B}]- \mathbb{E}[\mathbb{1}_{B}(Y)])}{\widehat{\mathsf{E}}_{x}[\mathbb{1}_{A}] \widehat{\mathsf{E}}_{y}[\mathbb{1}_{B}]}\right).\] (27)

Next Lemmas 3 and 4 combined with (18) and elementary algebra give w.p.a.l. \(1-2\delta\) that

\[\left|\frac{\widehat{\mathsf{E}}_{x}[\mathbb{1}_{A}]-\mathbb{E}[\mathbb{1}_{A }(X)]}{\widehat{\mathsf{E}}_{x}[\mathbb{1}_{A}]}\right|\leq 2\varphi_{X}(A) \bar{\epsilon}_{n}(\delta),\quad\left|\frac{\widehat{\mathsf{E}}_{y}[ \mathbb{1}_{B}]-\mathbb{E}[\mathbb{1}_{B}(Y)]}{\widehat{\mathsf{E}}_{x}[ \mathbb{1}_{B}]}\right|\leq 2\varphi_{Y}(B)\bar{\epsilon}_{n}(\delta),\]

and

\[\frac{\mathbb{E}[\mathbb{1}_{A}(X)]}{\widehat{\mathsf{E}}_{x}[\mathbb{1}_{A}] }\vee\frac{\mathbb{E}[\mathbb{1}_{B}(Y)]}{\widehat{\mathsf{E}}_{y}[\mathbb{1} _{B}]}\leq 2,\quad\left|\frac{\mathbb{E}[\mathbb{1}_{A}(X)](\widehat{ \mathsf{E}}_{y}[\mathbb{1}_{B}]-\mathbb{E}[\mathbb{1}_{B}(Y)])}{\widehat{ \mathsf{E}}_{x}[\mathbb{1}_{A}]\widehat{\mathsf{E}}_{y}[\mathbb{1}_{B}]} \right|\leq 4\varphi_{Y}(B)\bar{\epsilon}_{n}(\delta).\]

It also holds on the same probability event as above that

\[\left|\frac{\langle\mathbb{1}_{A},(\mathsf{D}_{Y|X}-\widehat{ \mathsf{D}}_{Y|X}^{\theta})\mathbb{1}_{B}\rangle}{\widehat{\mathsf{E}}_{x}[ \mathbb{1}_{A}]\widehat{\mathsf{E}}_{y}[\mathbb{1}_{B}]}\right|\!\!\leq\!\! \frac{\|\mathsf{D}_{Y|X}-\widehat{\mathsf{D}}_{Y|X}^{\theta}\|}{\sqrt{\mathbb{E }[\mathbb{1}_{A}(X)]\mathbb{E}[\mathbb{1}_{B}(Y)]}}\frac{\mathbb{E}[ \mathbb{1}_{A}(X)]}{\widehat{\mathsf{E}}_{x}[\mathbb{1}_{A}]}\frac{\mathbb{E}[ \mathbb{1}_{B}(Y)]}{\widehat{\mathsf{E}}_{y}[\mathbb{1}_{B}]}\!\!\leq\!\!4 \frac{\|\mathsf{D}_{Y|X}-\widehat{\mathsf{D}}_{Y|X}^{\theta}\|}{\sqrt{\mathbb{E }[\mathbb{1}_{A}(X)]\mathbb{E}[\mathbb{1}_{B}(Y)]}}.\]

Combining Lemma 2 and (24), we get with probability at least \(1-\delta\) that \(\|\mathsf{D}_{Y|X}-\widehat{\mathsf{D}}_{Y|X}^{\theta}\|\leq\psi_{n}(\delta)\).

By a union bound combining the last two displays with (24), (27), (25) and (26), we get with probability at least \(1-3\delta\)

\[\left|\frac{\mathbb{P}[Y\in B\,|\,X\in A]}{\mathbb{P}[Y\in B]}-\frac{\widehat{ p}_{\theta}(B\,|\,A)}{\widehat{p}_{y}(B)}\right|\leq\frac{4\psi_{n}(\delta)+[1+ \psi_{n}(\delta)]\,[2\varphi_{X}(A)+4\varphi_{Y}(B)]\,\bar{\epsilon}_{n}( \delta)}{\sqrt{\mathbb{E}[\mathbb{1}_{A}(X)]\mathbb{E}[\mathbb{1}_{B}(Y)]}},\] (28)

and

\[\left|\frac{\mathbb{P}[Y\in B\,|\,X\in A]-\widehat{p}_{\theta}(B\,|\,A)}{ \mathbb{P}[Y\in B]}\right|\leq\varphi_{Y}(B)\bar{\epsilon}_{n}(\delta)+\frac{2 (1+\psi_{n}(\delta))\varphi_{X}(A)\bar{\epsilon}_{n}(\delta)+\psi_{n}(\delta)}{ \sqrt{\mathbb{E}[\mathbb{1}_{A}(X)]\mathbb{E}[\mathbb{1}_{B}(Y)]}}.\] (29)

Replacing \(\delta\) by \(\delta/3\), we get the result w.p.a.l. \(1-\delta\).

The following result will be useful to investigate the theoretical properties of the NCP method in the iid setting.

**Lemma 2**.: _Let Assumption 1 be satisfied. Assume in addition that \(n\geq c_{u}^{2}d\). Then there exists an absolute constant \(C>0\) such that, for any \(\delta\in(0,1)\), it holds w.p.a.l. \(1-\delta\)_

\[\|\widehat{\mathbb{E}}_{x}[u^{\theta}]-\mathbb{E}[u^{\theta}(X)]\|\leq C\,c_{u }\sqrt{d}\,\left(\frac{\log(e\delta^{-1})}{n}+\sqrt{\frac{\log(e\delta^{-1})}{ n}}\right).\]

_Similarly, if \(n\geq c_{v}^{2}d\), w.p.a.l. \(1-\delta\)_

\[\|\widehat{\mathbb{E}}_{y}[v^{\theta}]-\mathbb{E}[v^{\theta}(Y)]\|\leq Cc_{v} \sqrt{d}\left(\frac{\log(e\delta^{-1})}{n}+\sqrt{\frac{\log(e\delta^{-1})}{n} }\right).\]

Proof of Lemma 2.: We note that

\[\widehat{\mathbb{E}}_{x}[u^{\theta}]-\mathbb{E}[u^{\theta}(X)]=\frac{1}{n} \sum_{i=1}^{n}Z_{i}\quad\text{with}\quad\,Z_{i}=u^{\theta}(X_{i})-\mathbb{E}u ^{\theta}(X_{i}),\,\,\forall i\in[n].\]

We note that \(\|Z_{i}\|\leq 2c_{u}\,\sqrt{d}=:U\) and \(\operatorname{Var}(Z_{i})=\operatorname{Var}(\|u^{\theta}(X_{i})-\mathbb{E} [u^{\theta}(X_{i})]\|)=\sigma_{\theta}^{2}(X)\) for any \(i\in[n]\). We apply Minsker (2017, Corollary 4.1) to get for any \(t\geq\frac{1}{6}(U+\sqrt{U^{2}+36n\sigma_{\theta}^{2}(X)})\),

\[\mathbb{P}\left[\|\sum_{i=1}^{n}Z_{i}\|>t\right]\leq 28\exp\left(-\frac{t^{2}/2} {n\sigma_{\theta}^{2}(X)+tU/3}\right).\] (30)

Replacing \(t\) by \(nt\) and some elementary algebra give for any \(t\geq\frac{1}{6}\left(\frac{U}{n}+\sqrt{\frac{U^{2}}{n^{2}}+36\frac{\sigma_{ \theta}^{2}(X)}{n}}\right)=:\overline{c}\), w.p.a.l. \(1-28\exp\left(-t\right)\),

\[\|\frac{1}{n}\sum_{i=1}^{n}Z_{i}\|\leq\frac{4U}{3}\frac{t}{n}+2\sigma_{\theta} (X)\sqrt{\frac{t}{n}}.\]

Replacing \(t\) by \(t+\overline{c}\), we get for any \(t\geq 0\), w.p.a.l. \(1-28\exp\left(-t+\overline{c}\right)\),

\[\|\frac{1}{n}\sum_{i=1}^{n}Z_{i}\|\leq\frac{4U}{3}\frac{t+\overline{c}}{n}+2 \sigma_{\theta}(X)\sqrt{\frac{t+\overline{c}}{n}}.\]

Up to a rescaling of the constants, there exists a numerical constant \(C>0\) such that for any \(\delta\in(0,1)\), w.p.a.l. \(1-\delta\)

\[\|\frac{1}{n}\sum_{i=1}^{n}Z_{i}\|\leq C\left(\frac{U}{n}\overline{c}+\sigma_ {\theta}(X)\sqrt{\frac{\overline{c}}{n}}+U\frac{t}{n}+\sigma_{\theta}(X)\sqrt {\frac{t}{n}}\right).\]

Elementary computations give the following bound, that is, there exists a numerical constant \(C>0\) such that for any \(t>0\), w.p.a.l. \(1-\exp(-t)\)

\[\|\frac{1}{n}\sum_{i=1}^{n}Z_{i}\|\leq C\left(\frac{c_{u}\,\sqrt{d}}{n}\lor \frac{c_{u}^{2}\,d}{n^{2}}\lor\frac{\sigma_{\theta}^{3/2}(X)}{n^{3/4}}\lor\frac {\sigma_{\theta}^{2}(X)}{n}+c_{u}\frac{\sqrt{d}\,t}{n}+\sigma_{\theta}(X)\sqrt {\frac{t}{n}}\right).\]

Under Assumption 1 and the condition \(\frac{c_{u}^{2}\,d}{n}\leq 1\), it also holds that \(\frac{\sigma_{\theta}^{2}(X)}{n}\leq 1\) since \(\sigma_{\theta}^{2}(X)\leq c_{u}^{2}d\). Consequently, the bound simplifies and we obtain for any \(t>1\), w.p.a.l. \(1-\exp(-t)\)

\[\|\frac{1}{n}\sum_{i=1}^{n}Z_{i}\|\leq C\,c_{u}\sqrt{d}\left(\frac{t}{n}\lor \sqrt{\frac{t}{n}}\right),\]

where \(C>0\) is possibly a different absolute constant from the previous bound. Taking \(t=\log e\delta^{-1}\) for any \(\delta\in(0,1)\) gives the first result. We proceed similarly to get the second result.

Control on empirical probabilitiesWe derive now a concentration result for empirical probabilities.

**Lemma 3**.: _For any \(A\in\Sigma_{\mathcal{X}}\) and any \(\delta\in(0,1)\), it holds w.p.a.l. \(1-\delta\)_

\[|\widehat{\mathbb{E}}_{x}[\mathbb{1}_{A}]-\mathbb{E}[\mathbb{1}_{A}(X)]|\leq 2 \frac{\log 2\delta^{-1}}{n}+\sqrt{\mathbb{P}[X\in A](1-\mathbb{P}[X\in A])} \sqrt{2\frac{\log 2\delta^{-1}}{n}}.\]

_Assume in addition that \(\mathbb{P}(X\in A)\geq 2\sqrt{2\frac{\log 2\delta^{-1}}{n}}\). Then it holds w.p.a.l. \(1-\delta\)_

\[\frac{|\widehat{\mathbb{E}}_{x}[\mathbb{1}_{A}]-\mathbb{E}[\mathbb{1}_{A}(X)]| }{\mathbb{E}[\mathbb{1}_{A}(X)]}\leq\sqrt{2\frac{\log 2\delta^{-1}}{n}} \sqrt{1\vee\frac{1-\mathbb{P}[X\in A]}{\mathbb{P}[X\in A]}}.\]

Proof.: We note that

\[\widehat{\mathbb{E}}_{x}[\mathbb{1}_{A}(X)]-\mathbb{E}[\mathbb{1}_{A}(X)]= \frac{1}{n}\sum_{i=1}^{n}Z_{i}\quad\text{with}\quad\ Z_{i}=\mathbb{1}_{A}(X_{i })-\mathbb{E}[\mathbb{1}_{A}(X_{i})],\ \forall i\in[n].\]

We note that \(|Z_{i}|\leq 2\) and \(\operatorname{Var}(Z_{i})=\mathbb{P}[X\in A](1-\mathbb{P}[X\in A])\). Then Bercu et al. (2015, Theorem 2.9) gives w.p.a.l. \(1-2\delta\)

\[|\widehat{\mathbb{E}}_{x}[\mathbb{1}_{A}]-\mathbb{E}[\mathbb{1}_{A}(X)]|\leq 2 \frac{\log\delta^{-1}}{n}+\sqrt{\mathbb{P}[X\in A](1-\mathbb{P}[X\in A])} \sqrt{2\frac{\log\delta^{-1}}{n}}.\]

Dividing by \(\mathbb{E}[\mathbb{1}_{A}(X)]\) gives w.p.a.l. \(1-2\delta\)

\[\frac{|\widehat{\mathbb{E}}_{x}[\mathbb{1}_{A}]-\mathbb{E}[\mathbb{1}_{A}(X)] |}{\mathbb{E}[\mathbb{1}_{A}(X)]}\leq 2\sqrt{2\frac{\log\delta^{-1}}{n}}\sqrt{ \frac{[2\log(\delta^{-1})/n]\vee(1-\mathbb{P}[X\in A])}{\mathbb{P}[X\in A]}}.\]

Replacing \(\delta\) by \(\delta/2\) gives the result for \(X\). The result for \(Y\) follows from a similar reasoning. 

The same proof argument gives an identical result for \(Y\).

**Lemma 4**.: _For any \(B\in\Sigma_{\mathcal{Y}}\) and any \(\delta\in(0,1)\), it holds w.p.a.l. \(1-\delta\)_

\[|\widehat{\mathbb{E}}_{y}[\mathbb{1}_{B}]-\mathbb{E}[\mathbb{1}_{B}(Y)]|\leq 2 \frac{\log 2\delta^{-1}}{n}+\sqrt{\mathbb{P}[Y\in B](1-\mathbb{P}[Y\in B])} \sqrt{2\frac{\log 2\delta^{-1}}{n}}.\]

_Assume in addition that \(\mathbb{P}(Y\in B)\geq 2\sqrt{2\frac{\log 2\delta^{-1}}{n}}\). Then it holds w.p.a.l. \(1-\delta\)_

\[\frac{|\widehat{\mathbb{E}}_{y}[\mathbb{1}_{B}]-\mathbb{E}[\mathbb{1}_{B}(Y)] |}{\mathbb{E}[\mathbb{1}_{B}(Y)]}\leq\sqrt{2\frac{\log 2\delta^{-1}}{n}} \sqrt{1\vee\frac{1-\mathbb{P}[Y\in B]}{\mathbb{P}[Y\in B]}}.\]

### Sub-Gaussian case

Sub-Gaussian setting.We derive another concentration result under a less restricted sub-Gaussian condition on functions \(u^{\theta}\) and \(v^{\theta}\). This result relies on Pinelis and Sakhanenko's inequality for random variables in a separable Hilbert space, see (Caponnetto and De Vito, 2007, Proposition 2).

Let \(\psi_{2}(x)=e^{x^{2}}-1\), \(x\geq 0\). We define the \(\psi_{2}\)-Orlicz norm of a random variable \(\eta\) as

\[\|\eta\|_{\psi_{2}}:=\inf\left\{C>0\,:\,\mathbb{E}\left[\psi_{2}\left(\frac{| \eta|}{C}\right)\right]\leq 1\right\}.\]

We recall the definition of a sub-Gaussian random vector.

**Definition 4** (Sub-Gaussian random vector).: _A centered random vector \(X\in\mathbb{R}^{\underline{d}}\) will be called sub-Gaussian iff, for all \(u\in\mathbb{R}^{\underline{d}}\),_

\[\|\langle X,u\rangle\|_{\psi_{2}}\lesssim\|\langle X,u\rangle\|_{L_{2}( \mathbb{P})}.\]

**Proposition 1**.: _Caponnetto and De Vito (2007, Proposition 2) Let \(A_{i}\), \(i\in[n]\) be i.i.d copies of a random variable \(A\) in a separable Hilbert space with norm \(\left\|\cdot\right\|\). If there exist constants \(L>0\) and \(\sigma>0\) such that for every \(m\geq 2\), \(\mathbb{E}\|A\|^{m}\leq\frac{1}{2}m!L^{m-2}\sigma^{2}\), then with probability at least \(1-\delta\)_

\[\left\|\frac{1}{n}\sum_{i\in[n]}A_{i}-\mathbb{E}A\right\|\leq\frac{4\sqrt{2}} {\sqrt{n}}\sqrt{\sigma^{2}+\frac{L^{2}}{n}}\log\frac{2}{\delta}.\] (31)

**Lemma 5** ((Sub-Gaussian random variable) Lemma 5.5. in Vershynin (2011)).: _Let \(Z\) be a random variable. Then, the following assertions are equivalent with parameters \(K_{i}>0\) differing from each other by at most an absolute constant factor._

1. _Tails:_ \(\mathbb{P}\{|Z|>t\}\leq\exp(1-t^{2}/K_{1}^{2})\) _for all_ \(t\geq 0\)_;_
2. _Moments:_ \((\mathbb{E}|Z|^{p})^{1/p}\leq K_{2}\sqrt{p}\) _for all_ \(p\geq 1\)_;_
3. _Super-exponential moment:_ \(\mathbb{E}\exp(Z^{2}/K_{3}^{2})\leq 2\)_._

_A random variable \(Z\) satisfying any of the above assertions is called a sub-Gaussian random variable. We will denote by \(K_{3}\) the sub-Gaussian norm._

Consequently, a sub-Gaussian random variable satisfies the following equivalence of moments property. There exists an absolute constant \(c>0\) such that for any \(m\geq 2\),

\[\left(\mathbb{E}|Z|^{m}\right)^{1/m}\leq cK_{3}\sqrt{m}\big{(}\mathbb{E}|Z|^{2 }\big{)}^{1/2}.\]

**Lemma 6**.: _Assume that \(\|u^{\theta}(X)-\mathbb{E}[u^{\theta}(X)]\|\) and \(\|v^{\theta}(Y)-\mathbb{E}[v^{\theta}(Y)]\|\) are sub-Gaussian with sub-Gaussian norm \(K\). We set \(\sigma_{\theta}^{2}(X):=\mathrm{Var}(\|u^{\theta}(X)-\mathbb{E}[u^{\theta}(X) ]\|)\), \(\sigma_{\theta}^{2}(Y):=\mathrm{Var}(\|v^{\theta}(Y)-\mathbb{E}[v^{\theta}(Y)] \|)\). Then there exists an absolute constant \(C>0\) such that for any \(\delta\in(0,1)\), it holds w.p.a.l. \(1-\delta\)_

\[\|\widehat{\mathbb{E}}_{x}[u^{\theta}]-\mathbb{E}[u^{\theta}(X)]\|\leq\frac{C }{\sqrt{n}}\sqrt{\sigma_{\theta}^{2}(X)+\frac{K^{2}}{n}}\log(2\delta^{-1}).\]

_Similarly, w.p.a.l. \(1-\delta\)_

\[\|\widehat{\mathbb{E}}_{y}[v^{\theta}]-\mathbb{E}[v^{\theta}(Y)]\|\leq\frac{ C}{\sqrt{n}}\sqrt{\sigma_{\theta}^{2}(Y)+\frac{K^{2}}{n}}\log(2\delta^{-1})\]

Proof.: Set \(Z:=\|u^{\theta}(X)-\mathbb{E}u^{\theta}(X)\|\) and we recall that \(\sigma_{\theta}^{2}(X):=\mathrm{Var}(\|u^{\theta}(X)-\mathbb{E}[u^{\theta}(X) ]\|)\). We check that the moment condition,

\[\mathbb{E}Z^{m}\leq\frac{1}{2}m!L^{m-2}\sigma_{\theta}^{2}(X)^{2},\quad\forall m \geq 2,\]

for some constant \(L>0\) to be specified.

The condition is obviously satisfied for \(m=2\). Next for any \(m\geq 3\), the Cauchy-Schwarz inequality and the equivalence of moment property give

\[\mathbb{E}Z^{m}\leq\left(\mathbb{E}Z^{2(m-2)}\right)^{1/2}\left(\mathbb{E}Z^{ 4}\right)^{1/2}\leq 4K_{3}^{2}\sigma_{\theta}^{2}(X)^{2}\left(\mathbb{E}Z^{2(m-2) }\right)^{1/2}.\]

Next, by homogeneity, rescaling \(Z\) to \(Z/K_{1}\) we can assume that \(K_{1}=1\) in Lemma 5. We recall that if \(Z\) is in addition non-negative random variable, then for every integer \(p\geq 1\), we have

\[\mathbb{E}Z^{p}=\int_{0}^{\infty}\mathbb{P}\{Z\geq t\}\,pt^{p-1}\,dt\leq\int_ {0}^{\infty}e^{1-t^{2}}pt^{p-1}\,dt=\big{(}\frac{ep}{2}\big{)}\Gamma(\frac{p} {2}\big{)}.\]

With \(p=2(m-2)\), we get that \(\mathbb{E}Z^{p}\leq e(m-2)\Gamma\big{(}m-2\big{)}=e(m-2)!=em!/2\). Using again Lemma 5, we can take \(L=cK\) for some large enough absolute constant \(c>0\). Then Proposition 1 gives the result.

### Estimation of conditional expectation and Conditional covariance

We now derive guarantees for the estimation of the conditional expectation and the conditional covariance for vector-valued output \(Y\in\mathbb{R}^{d_{y}}\).

We start with a general result for arbitrary vector-valued functions of \(Y\). We consider a vector-valued function \(\underline{h}=(h_{1},\ldots,h_{\underline{d}})\) where \(h_{j}\in L_{\nu}^{2}(\mathcal{Y})\) for any \(j\in[\underline{d}]\). We introduce the space of square integrable vector-valued functions \([L_{\nu}^{2}(\mathcal{Y},\mathbb{R}^{\underline{d}})]\) equipped with the norm

\[\|\underline{h}\|=\sqrt{\sum_{j\in[\underline{d}]}\|h_{j}\|_{L_{\nu}^{2}( \mathcal{Y})}^{2}}.\]

Next we can define the conditional expectation of \(\underline{h}(Y)=(h_{1}(Y^{(1)}),\ldots h_{\underline{d}}(Y^{(d_{y})}))^{\top}\) conditionally on \(X\in A\) as follows

\[\mathbb{E}[\underline{h}(Y)\,|\,X\in A] =\left(\mathbb{E}[h_{1}(Y)]+\frac{\langle\updownarrow_{A}, \updownarrow_{\underline{Y}|X}h_{1}\rangle}{\mathbb{P}(X\in A)},\ldots, \mathbb{E}[h_{\underline{d}}(Y)]+\frac{\langle\updownarrow_{A},\updownarrow_{ \underline{Y}|X}h_{\underline{d}}\rangle}{\mathbb{P}(X\in A)}\right)^{\top}\] \[=\mathbb{E}[\underline{h}(Y)]+\frac{\langle\updownarrow_{A},[ \underline{\updownarrow_{\underline{d}}}\otimes\updownarrow_{Y|X}] \underline{h}\rangle}{\mathbb{P}(X\in A)}.\]

We define similarly its empirical version as

\[\widehat{\mathbb{E}}^{\theta}[\underline{h}(Y)\,|\,X\in A] =\left(\widehat{\mathbb{E}}_{y}[h_{1}]+\frac{\langle\updownarrow_{ A},\widehat{\updownarrow}_{Y|X}^{\theta}h_{1}\rangle}{\widehat{\mathbb{E}}_{x}[ \updownarrow_{A}]},\ldots,\widehat{\mathbb{E}}_{y}[h_{\underline{d}}]+\frac{ \langle\updownarrow_{A},\widehat{\updownarrow}_{Y|X}^{\theta}h_{\underline{d }}\rangle}{\widehat{\mathbb{E}}_{x}[\updownarrow_{A}]}\right)^{\top}\] \[=\widehat{\mathbb{E}}_{y}[\underline{h}]+\frac{\langle\updownarrow _{A},[\underline{\updownarrow_{\underline{d}}}\otimes\widehat{\updownarrow}_{ Y|X}^{\theta}]\underline{h}\rangle}{\widehat{\mathbb{E}}_{x}[\updownarrow_{A}]}.\]

Assuming that \(\underline{h}(Y)\) is sub-Gaussian, we set

\[K:=\|\|\underline{h}(Y)-\mathbb{E}[\underline{h}(Y)]\|\|_{\psi_{2}},\quad \sigma^{2}(\underline{h}(Y)):=\mathrm{Var}(\|\underline{h}(Y)-\mathbb{E}[ \underline{h}(Y)]\|).\]

Define

\[\underline{\psi}_{n}(\delta):=\frac{1}{\sqrt{n}}\sqrt{\sigma^{2}( \underline{h}(Y))+\frac{K^{2}}{n}}\log(3\delta^{-1})\] \[\qquad\qquad+\frac{\|\underline{h}\|}{\sqrt{\mathbb{P}(X\in A)}} \bigg{(}\psi_{n}(\delta/3)+2(1+\psi_{n}(\delta/3))\varphi_{X}(A)\bar{\epsilon }_{n}(\delta/3)\bigg{)}.\]

**Theorem 4**.: _Let the assumptions of Theorem 2 be satisfied. Assume in addition that \(\underline{h}(Y)\) is sub-Gaussian. Then we have w.p.a.l. \(1-\delta\) that_

\[\|\widehat{\mathbb{E}}^{\theta}[\underline{h}(Y)\,|\,X\in A]-\mathbb{E}[ \underline{h}(Y)\,|\,X\in A]\|\lesssim\underline{\psi}_{n}(\delta).\] (32)

Proof.: We have

\[\|\mathbb{E}[\underline{h}(Y)\,|\,X\in A]-\widehat{\mathbb{E}}^{ \theta}[\underline{h}(Y)|X\in A]\|\] \[\qquad\leq\|\mathbb{E}[\underline{h}(Y)]-\widehat{\mathbb{E}}_{ y}[\underline{h}]\|+\frac{\|\updownarrow_{Y|X}-\widehat{\updownarrow}_{Y|X}^{ \theta}\|}{\sqrt{\mathbb{P}(X\in A)}}\|\underline{h}\|+|\langle\updownarrow_{A },[\updownarrow_{\underline{d}}\otimes\widehat{\updownarrow}_{Y|X}^{\theta}] \underline{h}\rangle|\bigg{|}\frac{1}{\widehat{\mathbb{E}}_{x}[\updownarrow_{ A}]}-\frac{1}{\mathbb{P}(X\in A)}\bigg{|}\] \[\qquad\qquad+\sqrt{\mathbb{P}(X\in A)}(\|\updownarrow_{Y|X} \|+\|\updownarrow_{Y|X}-\widehat{\updownarrow}_{Y|X}^{\theta}\|)\|\underline{h} \|\bigg{|}\frac{\mathbb{P}(X\in A)-\widehat{\mathbb{E}}_{x}[\updownarrow_{A}]}{ \widehat{\mathbb{E}}_{x}[\updownarrow_{A}]\mathbb{P}(X\in A)}\bigg{|}.\] (33)

Recall that \(\|\updownarrow_{Y|X}\|\leq 1\), (24) and Lemma 3. Hence, a union bound we get with w.p.a.l. \(1-2\delta\) that

\[\|\mathbb{E}[\underline{h}(Y)\,|\,X\!\in\!A]-\widehat{\mathbb{E}}^{ \theta}[\underline{h}(Y)|X\!\in\!A]\|\] \[\qquad\qquad\leq\|\mathbb{E}[\underline{h}(Y)]-\widehat{\mathbb{E} }_{y}[\underline{h}]\|+\frac{\|\underline{h}\|}{\sqrt{\mathbb{P}(X\!\in\!A)} }\bigg{(}\psi_{n}(\delta)\!+\!2(1\!+\!\psi_{n}(\delta))\varphi_{X}(A)\bar{ \epsilon}_{n}(\delta)\bigg{)}.\] (34)We now handle the first term \(\|\mathbb{E}[\underline{h}(Y)]-\widehat{\mathsf{E}}_{y}[\underline{h}]\|\). We recall that a similar quantity was already studied in Lemma 6. We can just replace \(u^{\theta}(X)\) by \(\underline{h}(Y)\in\mathbb{R}^{d}\) to get the result since we assumed that \(\underline{h}(Y)\) is sub-Gaussian. Hence there exists an absolute constant \(C>0\) such that w.p.a.l. \(1-\delta\)

\[\|\mathbb{E}[\underline{h}(Y)]-\widehat{\mathsf{E}}_{y}[\underline{h}]\|\leq \frac{C}{\sqrt{n}}\sqrt{\sigma^{2}(\underline{h}(Y))+\frac{K^{2}}{n}}\log(2 \delta^{-1}).\]

Actually, we can handle the conditional expectation \(\mathbb{E}[Y\,|\,X\in A]\) in a more direct way. Set

\[\epsilon_{n}(\delta):=\sqrt{\frac{\log(\delta^{-1}d_{y})}{n}}\sqrt{\frac{\log( \delta^{-1}d_{y})}{n}}.\]

**Corollary 2**.: _Let the Assumptions of Theorem 2 be satisfied. Assume in addition that \(Y\) is a sub-Gaussian vector. Then for any \(\delta\in(0,1)\), it holds with probability at least \(1-\delta\) that_

\[\|\mathbb{E}[Y\,|\,X\in A]-\widehat{\mathsf{E}}^{\theta}[Y|X\in A ]\|\lesssim\sqrt{\operatorname{tr}(\operatorname{Cov}(Y))}\underline{\epsilon }_{n}(\delta/3)\\ +\frac{\|\underline{h}\|}{\sqrt{\mathbb{P}(X\in A)}}\bigg{(}\psi _{n}(\delta/3)+2(1+\psi_{n}(\delta/3))\varphi_{X}(A)\bar{\epsilon}_{n}(\delta/3 )\bigg{)}=:\psi_{n}^{(1)}(\delta).\] (35)

Proof.: The proof of this result is identical to that of Theorem 4 up to (34). Now if we specify \(\underline{h}(Y)=Y\in\mathbb{R}^{d_{y}}\). Then, applying Bernstein's inequality on each of the \(d_{y}\) components of \(\mathbb{E}[Y]-\overline{Y}_{n}\) and a union bound, we get w.p.a.l. \(1-\delta\)

\[\|\mathbb{E}[Y]-\overline{Y}_{n}\|\lesssim\sqrt{\operatorname{tr}( \operatorname{Cov}(Y))}\sqrt{\frac{\log(\delta^{-1}d_{y})}{n}}+\max_{j\in[d_{y }]}\|Y^{(j)}\|_{\psi_{2}}\frac{\log(\delta^{-1}d_{y})}{n}.\]

Using again Definition 4, we obtain \(\max_{j\in[d_{y}]}\|Y^{(j)}\|_{\psi_{2}}\lesssim\sqrt{\|\operatorname{Cov}(Y) \|}\leq\sqrt{\operatorname{tr}(\operatorname{Cov}(Y))}\).

It follows from the last two displays, w.p.a.l. \(1-\delta\)

\[\|\mathbb{E}[Y]-\overline{Y}_{n}\|\lesssim\sqrt{\operatorname{tr}( \operatorname{Cov}(Y))}\underline{\epsilon}_{n}(\delta).\] (36)

A union bound combining the previous display with (34) gives the first result. 

We focus now on the conditional covariance estimation problem. We first define the conditional covariance as follows:

\[\operatorname{Cov}(Y|X\in A)=\operatorname{Cov}(Y)+\langle \mathbb{1}_{A},[(\mathbb{1}_{d_{y}}\otimes\mathbb{1}_{d_{y}})\otimes\mathsf{ D}_{Y|X}]\underline{h}\otimes\underline{h}\rangle/\mathbb{P}[X\in A]\\ -\langle\mathbb{1}_{A},[\mathbb{1}_{d_{y}}\otimes\mathsf{D}_{Y|X }]\underline{h}\rangle\otimes\langle\mathbb{1}_{A},[\mathbb{1}_{d_{y}}\otimes \mathsf{D}_{Y|X}]\underline{h}\rangle/(\mathbb{P}[X\in A])^{2}.\] (37)

Note that \(\langle\mathbb{1}_{A},[(\mathbb{1}_{d_{y}}\otimes\mathbb{1}_{d_{y}})\otimes \mathsf{D}_{Y|X}]\underline{h}\otimes\underline{h}\rangle=(\langle\mathbb{1}_{A },\mathsf{D}_{Y|X}\underline{h}_{j}h_{k}))_{j,k\in[d_{y}]}\) is a \(d_{y}\times d_{y}\) matrix. We obtain a similar decomposition for the estimator \(\widehat{\operatorname{Cov}}^{\theta}(Y|X\in A)\) of the conditional covariance \(\operatorname{Cov}(Y|X\in A)\) by replacing \(\mathsf{D}_{Y|X}\) by \(\widehat{\mathsf{D}}_{Y|X}^{\theta}\):

\[\widehat{\operatorname{Cov}}^{\theta}(Y|X\in A):=\widehat{ \operatorname{Cov}}(Y)+\langle\mathbb{1}_{A},[(\mathbb{1}_{d_{y}}\otimes \mathbb{1}_{d_{y}})\otimes\widehat{\mathsf{D}}_{Y|X}^{\theta}]\underline{h} \otimes\underline{h}\rangle/\widehat{\mathsf{E}}_{x}[\mathbb{1}_{A}]\\ -\langle\mathbb{1}_{A},[\mathbb{1}_{d_{y}}\otimes\widehat{\mathsf{ D}}_{Y|X}^{\theta}]\underline{h}\rangle\otimes\langle\mathbb{1}_{A},[\mathbb{1}_{d_{y}} \otimes\widehat{\mathsf{D}}_{Y|X}^{\theta}]\underline{h}\rangle/(\widehat{ \mathsf{E}}_{x}[\mathbb{1}_{A}])^{2}.\] (38)

We define the effective of covariance matrix \(\operatorname{Cov}(Y)\) as follows:

\[\mathbf{r}(\operatorname{Cov}(Y)):=\frac{\operatorname{tr}(\operatorname{Cov}(Y))}{\| \operatorname{Cov}(Y)\|}.\]

We set for any \(\delta\in(0,1)\)

\[\epsilon_{n}^{(2)}(\delta):=\|\operatorname{Cov}(Y)\|\left(\sqrt{\frac{ \mathbf{r}(\operatorname{Cov}(Y))}{n}}+\frac{\mathbf{r}(\operatorname{Cov}(Y))}{n }+\sqrt{\frac{\log(\delta^{-1})}{n}}+\frac{\log(\delta^{-1})}{n}\right),\] (39)

and

\[\psi_{n}^{(2)}(\delta)=\epsilon_{n}^{(2)}(\delta)+[\psi_{n}( \delta/4)+2(1+\psi_{n}(\delta/4))\varphi_{X}(A)\bar{\epsilon}_{n}(\delta/4)]\, \frac{(\mathbb{E}[\|Y\|^{2}])^{2}}{\sqrt{\mathbb{P}[X\in A]}}\\ +\psi_{n}^{(1)}(\delta/4)\big{[}2\|\mathbb{E}[Y\,|\,X\in A]\|+\psi _{n}^{(1)}(\delta/4)\big{]}.\]

**Corollary 3**.: _Let the assumptions of Corollary 2 be satisfied. Then for any \(\delta\in(0,1)\), it holds with probability at least \(1-\delta\) that_

\[\|\widehat{\mathrm{Cov}}^{\theta}(Y|X\in A)-\mathrm{Cov}(Y|X\in A)\|\lesssim\psi_ {n}^{(2)}(\delta).\] (40)

Proof.: We use again the function \(\underline{h}(Y)=Y\). We note in view of (37)-(38) that

\[\|\widehat{\mathrm{Cov}}^{\theta}(Y|X\in A)-\mathrm{Cov}(Y|X\in A )\|\leq\|\widehat{\mathrm{Cov}}(Y)-\mathrm{Cov}(Y)\|\\ +\|\langle\mathbb{1}_{A},\left[(\mathbb{1}_{d_{y}}\otimes \mathbb{1}_{d_{y}})\otimes\left(\frac{\mathsf{D}_{Y|X}}{\mathbb{P}[X\in A]}- \frac{\widehat{\mathsf{D}}_{Y|X}^{\theta}}{\widehat{\mathsf{E}}_{x}[\mathbb{1 }_{A}]}\right)\right]\underline{h}\otimes\underline{h}\rangle\|\\ +\|\mathbb{E}[\underline{h}(Y)\,|\,X\!\in\!A]\otimes\mathbb{E}[ \underline{h}(Y)\,|\,X\in A]-\widehat{\mathsf{E}}^{\theta}[\underline{h}(Y)\,| \,X\!\in\!A]\otimes\widehat{\mathsf{E}}^{\theta}[\underline{h}(Y)\,|\,X\! \in\!A]\|,\] (41)

Next, we note that

\[\|\langle\mathbb{1}_{A},\left[(\mathbb{1}_{d_{y}}\otimes \mathbb{1}_{d_{y}})\otimes\left(\frac{\mathsf{D}_{Y|X}}{\mathbb{P}[X\in A]}- \frac{\widehat{\mathsf{D}}_{Y|X}^{\theta}}{\widehat{\mathsf{E}}_{x}[\mathbb{1 }_{A}]}\right)\right]\underline{h}\otimes\underline{h}\rangle\|\\ \leq\|\langle\mathbb{1}_{A},\left[(\mathbb{1}_{d_{y}}\otimes \mathbb{1}_{d_{y}})\otimes\left(\frac{\mathsf{D}_{Y|X}}{\mathbb{P}[X\in A]}- \frac{\widehat{\mathsf{D}}_{Y|X}^{\theta}}{\widehat{\mathsf{E}}_{x}[\mathbb{1 }_{A}]}\right)\right]\underline{h}\otimes\underline{h}\rangle\|_{HS}\\ \leq\sqrt{\mathbb{P}[X\in A]}\|\frac{\mathsf{D}_{Y|X}}{\mathbb{P}[ X\in A]}-\frac{\widehat{\mathsf{D}}_{Y|X}^{\theta}}{\widehat{\mathsf{E}}_{x}[ \mathbb{1}_{A}]}\|\sum_{j,k\in[d_{y}]}\|Y_{j}Y_{k}\|_{L^{2}_{\nu}(\mathcal{Y})} \\ \lesssim\!\sqrt{\mathbb{P}[X\!\in\!A]}\left(\|\frac{\mathsf{D}_{Y|X }\!-\!\widehat{\mathsf{D}}_{Y|X}^{\theta}}{\mathbb{P}[X\!\in\!A]}\|\!+\!\| \widehat{\mathsf{D}}_{Y|X}^{\theta}\|\left(\frac{1}{\mathbb{P}[X\!\in\!A]}- \frac{1}{\widehat{\mathsf{E}}_{x}[\mathbb{1}_{A}]}\right)\right)\sum_{j,k\in[ d_{y}]}\|Y_{j}Y_{k}\|_{L^{2}_{\nu}(\mathcal{Y})}\]

Remind that \(Y\) is a sub-Gaussian vector. Using the equivalence of moments property of sub-Gaussian vector, we get that

\[\|Y_{j}Y_{k}\|_{L^{2}_{\nu}(\mathcal{Y})}\leq\sqrt{\mathbb{E}[Y_{j}^{4}] \mathbb{E}[Y_{k}^{4}]}\lesssim\mathbb{E}[Y_{j}^{2}]\mathbb{E}[Y_{k}^{2}],\quad \forall j,k\in[d_{y}].\]

By a union bound combining the last two displays with (24) and Lemma 3, we get w.p.a.l. \(1-2\delta\)

\[\|\langle\mathbb{1}_{A},\left[(\mathbb{1}_{d_{y}}\otimes\mathbb{1 }_{d_{y}})\otimes\left(\frac{\mathsf{D}_{Y|X}}{\mathbb{P}[X\in A]}-\frac{ \widehat{\mathsf{D}}_{Y|X}^{\theta}}{\widehat{\mathsf{E}}_{x}[\mathbb{1}_{A}]} \right)\right]\underline{h}\otimes\underline{h}\rangle\|\\ \leq[\psi_{n}(\delta)+2(1+\psi_{n}(\delta))\varphi_{X}(A)\bar{ \epsilon}_{n}(\delta)]\,\frac{(\mathbb{E}[\|Y\|^{2}])^{2}}{\sqrt{\mathbb{P}[X \in A]}}.\] (42)

Next, we set \(u=\mathbb{E}[\underline{h}(Y)\,|\,X\in A]\) and \(\hat{u}=\widehat{\mathsf{E}}^{\theta}[\underline{h}(Y)\,|\,X\in A]\). Then we have

\[\|u\otimes u-\hat{u}\otimes\hat{u}\|\leq\|u-\hat{u}\|(\|u\|+\|\hat{u}\|)\leq\| u-\hat{u}\|(2\|u\|+\|\hat{u}-u\|).\]

We apply next Corollary 2 to get w.p.a.l. \(1-\delta\)

\[\|u\otimes u-\hat{u}\otimes\hat{u}\|\leq\psi_{n}^{(1)}(\delta)\big{[}2\| \mathbb{E}[Y\,|\,X\in A]\|+\psi_{n}^{(1)}(\delta)\big{]}.\] (43)

Next Koltchinskii and Lounici (2017, Theorem 4) guarantees that w.p.a.l \(1-\delta\)

\[\|\widehat{\mathrm{Cov}}(Y)-\mathrm{Cov}(Y)\|\lesssim\epsilon_{n}^{(2)}(\delta),\] (44)

where \(\epsilon_{n}^{(2)}(\delta)\) is defined in (39).

A union bound combining (41), (42), (43) and (44) gives the result.

Numerical Experiments

Experiments were conducted on a high-performance computing cluster equipped with an Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz Sky Lake CPU, 377GB RAM, and an NVIDIA Tesla V100 16Gb GPU. Code is available at https://github.com/CSML-IIT-UCL/NCP.

### Conditional Density Estimation

To evaluate our method's ability to estimate conditional densities, we tested NCP on six different data models (described in the following paragraph) and compared its performance with ten other methods (detailed in Tab. 2). We assessed the methods' performance using the KS distance between the estimated conditional CDF and the true CDF. Additionally, we explored how the performance of each method scales with the number of training samples, ranging from \(10^{2}\) to \(10^{5}\), with a validation set of \(10^{3}\) samples. We tested each method on nineteen different conditional values uniformly sampled between the 5%- and 95%-percentile of \(p(x)\). Conditional CDFs were estimated on a grid of \(1000\) points uniformly distributed over the support of \(Y\). The KS distance between each pair of CDFs was averaged over all the conditioning values. In Tab. 5, we present the mean performance (KS distance \(\pm\) standard deviation), computed over 10 repetitions, each with a different random seed.

Synthetic data models.We included the following synthetic datasets from Rothfuss et al. (2019) and Gao and Hastie (2022) into our benchmark:

* LinearGaussian, a simple univariate linear density model defined as \(Y=X+\mathcal{N}(0,0.1)\) where \(X\sim\mathrm{Unif}(-1,1)\).
* EconDensity, an economically inspired heteroscedastic density model with a quadratic dependence on the conditional variable defined as \(Y=X^{2}+\epsilon_{Y},\epsilon_{Y}\sim\mathcal{N}(0,1+X)\) where \(X\sim|\mathcal{N}(0,1)|\).
* ArmaJump, a first-order autoregressive model with a jump component exhibiting negative skewness and excess kurtosis, defined as \[x_{t}=\left[c(1-\alpha)+\alpha x_{t-1}\right]+(1-z_{t})\epsilon_{t}+z_{t} \left[-3c+2\epsilon_{t}\right],\] where \(\epsilon_{t}\sim\mathcal{N}(0,0.05)\) and \(z_{t}\sim B(1,p)\) denote a Gaussian shock and a Bernoulli distributed jump indicator with probability \(p\), respectively. The parameters were left at their default value.
* GaussianMixture, a bivariate Gaussian mixture model with 5 kernels where the goal is to estimate the conditional density of one variable given the other. The mixture model is defined as \(p(X,Y)=\sum_{k=1}^{5}\pi_{k}\mathcal{N}\left(\bm{\mu}_{k},\bm{\Sigma}_{k}\right)\) where \(\pi_{k}\), \(\bm{\mu}_{k}\), and \(\bm{\Sigma}_{k}\) are the mixing coefficient, mean vector, and covariance matrix of the \(k\)-th distribution. All the parameters were randomly initialized.
* SkewNormal, a univariate skew normal distribution defined as \(Y=2\phi(X)\psi(\alpha X)\) where \(\phi(\cdot)\) and \(\psi(\cdot)\) are the standard normal probability and cumulative density functions, and \(\alpha\) is a parameter regulating the skewness. The parameters were left at their default value.
* Locally Gaussian or Gaussian mixture distribution (LGGMD) (Gao and Hastie, 2022), a regression dataset where the target \(y\) depends on the three first dimensions of \(x\), with seventeen irrelevant features added to \(x\). The features of \(x\) are all uniformly distributed between \(-1\) and \(1\). The first dimension of \(x\) gives the mean of \(Y|X\), the second is whether the data is Gaussian or a mixture of two Gaussians, and the third gives its asymmetry. More specifically: \[Y|X\sim\begin{cases}0.5\mathcal{N}(0.25X^{(1)}-0.5,0.25(0.25X^{(3)}+0.5)^{2}) \\ \quad+0.5\mathcal{N}(0.25X^{(1)}+0.5,0.25(0.25X^{(3)}-0.5)^{2})\text{ if }X^{(2)} \leq 0.2\\ 0\mathcal{N}(0.25X^{(1)}-0.5,0.3)\text{ if }X^{(2)}>0.2\end{cases}\] (45)

To sample data from EconDensity, ArmaJump, GaussianMixture, and SkewNormal, we used the library Conditional_Density_Estimation (Rothfuss et al., 2019) available at https://github.com/freelunchtheorem/Conditional_Density_Estimation.

Training NCP.We trained an NCP model with \(u^{\theta}\) and \(v^{\theta}\) as multi-layer perceptrons (MLPs), each having two hidden layers of 64 units using GELU activation function in between. The vector \(\sigma^{\theta}\) has a size of \(d=100\), and \(\gamma\) is set to \(10^{-3}\). Optimization was performed over \(10^{4}\) epochs using the Adam optimizer with a learning rate of \(10^{-3}\). Early stopping was applied based on the validation set with patience of \(1000\) epochs. To ensure the positiveness of the singular values, we transform the vector \(\sigma^{\theta}\) with the Gaussian function \(x\mapsto\exp(-x^{2})\) during any call of the forward method. Whitening was applied at the end of training.

Compared methods.We compared our NCP network with ten different CDE methods. See Tab. 2 for the exhaustive list of models including a brief summary and key hyperparameters.

In particular, the methods were set up as follows:

* NF was characterized by a \(1D\) Gaussian base distribution and two Masked Affine Autoregressive flows (Papamakarios et al., 2017) followed by a LU Linear permutation flow. To match the NCP architecture, each flow was defined by two hidden layers with \(64\) units each. The training procedure was the same as for the NCP model. The model was implemented using the library normflows (Stimper et al., 2023).
* DDPM was characterized by a U-Net (Ronneberger et al., 2015), a noise schedule starting from \(10^{-}4\) to \(0.02\) and \(400\) steps of diffusion as implemented in https://github.com/TeaPearce/Conditional_Diffusion_MNIST.
* CKDE's kernels bandwidth was estimated according to Silverman's rule (Silverman, 1986).
* MDN's architecture was defined by two hidden layers with \(64\) units each and \(20\) Gaussians kernels.
* KMN's architecture was defined by two hidden layers with \(64\) units each, \(50\) Gaussians kernels, and kernels bandwidth was estimated according to Silverman's rule (Silverman, 1986).
* LSCDE was defined by \(500\) components which bandwidths were set to \(0.5\) and kernels center found via a k-means procedure.
* NNKDE's number of neighbors was set using the heuristics \(k=\sqrt{n}\)(Devroye et al., 1996). Kernels bandwidth was estimated according to Silverman's rule (Silverman, 1986). We used the implementation available at https://github.com/lee-group-cmu/NNKCDE.
* RFCDE was characterized by a Random Forest with \(1000\) trees and \(31\) cosine basis functions. The training was performed using the rfcde library available at https://github.com/lee-group-cmu/RFCDE.
* FC was trained using a Random Forest with \(1000\) trees as a regression method and had \(31\) cosine basis functions. The training was performed using the flexcode library available at https://github.com/lee-group-cmu/FlexCode.
* LinCDE was trained with \(1000\) LinCDE trees using the LinCDE.boost R function from https://github.com/ZijunGao/LinCDE.

CKDE, MDN, KMI, and LSCDE hyperparameters were set according to Rothfuss et al. (2019) and were trained using the library Conditional_Density_Estimation available at https://github.com/freelunchtheorem/Conditional_Density_Estimation. All methods involving the training of a neural network were assigned the same number of epochs given to NCP. All other method parameters were set as prescribed in their paper.

Results.See Tab. 4 for the comparison of performances for \(n=10^{4}\). See also Fig. 5. We also carried out an ablation study on centering and whitening post-treatment for NCP in Tab. 3

### Confidence Regions

The objective of this next experiment is to estimate a confidence interval at coverage level \(90\%\) for two distribution models with different properties (Laplace and Cauchy) and one real dataset in order to showcase the versatility of our NCP approach.

[MISSING_PAGE_FAIL:29]

Compared methods.We compared our NCP procedure for building conditional confidence intervals to the state-of-the-art conditional conformal prediction method in Gibbs et al. (2023). We also developed another method based on Normalizing Flows' estimation of the conditional CDE and we added it to the benchmark.

Experiment for Laplace and Cauchy distributions.We generate a dataset where the \(X\) variable follows a uniform distribution on interval \([0,5]\) and \(Y|X=x\) follows either a Laplace distribution with location and scale parameters \((\mu(x),b(x))=(x^{2},x)\) or a Cauchy distribution with location and scale parameters \((\mu(x),b(x))=(x^{2},1+x)\). We create a train set of \(50000\) samples, a validation set of \(1000\) samples and a test set of \(1000\) samples.

For the Laplace distribution, we train an NCP where \(u^{\theta}\) and \(v^{\theta}\) are multi-layer perceptrons with two hidden layers of \(128\) cells, \(\sigma^{\theta}\) is a vector of size \(d=500\) and \(\gamma=10^{-2}\). Between each layer, we use the GELU activation function. We optimize over \(5000\) epochs using the Adam optimizer with a learning rate of \(10^{-3}\). We apply early stopping with regard to the validation set with a patience of \(100\) epochs. Whitening is applied at the end of training. To fit the Cauchy distribution, we increase the depth of the MLPs to \(5\) and the width to \(258\).

We compare this NCP network with two state-of-the-art methods. The first is a normalizing flow with base distribution a \(1D\) Gaussian and two Autoregressive Rational Quadratic spline flows (Durkan et al., 2019) followed by a LU Linear permutation flow. All flows come from the library normflows (Stimper et al., 2023). The spline flows have each two blocks of \(128\) hidden units to match the NCP architecture. The normalizing flow is allowed the same number of epochs as ours with the same optimizer. The second model is the conditional conformal predictor from Gibbs et al. (2023). This model needs a regressor as an input. We consider a situation favorable to Gibbs et al. (2023) as we assume as prior knowledge that the true conditional expectation is a polynomial function (the truth is actually the quadratic function in this example). Therefore we chose a linear regression with polynomial features as in Gibbs et al. (2023) as this regressor should fit the data without any problem. For all other choices of parameters, we follow the prescriptions of Gibbs et al. (2023). For the sake of

Figure 5: **Performances for CDE on synthetic datasets w.r.t sample size \(n\). Performance metric is Kolmogorov-Smirnov (KS) distance to truth.**

Figure 6: **Estimated conditional PDFs (left) and CDFs (right) for each synthetic dataset for 3 different conditioning points.** Dotted lines represent the true distributions, while solid lines represent the estimates from NCP. The average KS distance over 5 repetitions is also reported on the right plots.

fairness, we note that the validation set used for early stopping in NF and NCP was also used as a calibration set for the CCP method.

By design, the Conditional Conformal Predictor (CCP) gives the confidence interval directly. However NCP and NF output the conditional distribution. To find the smallest confidence interval with desired coverage, we apply the linear search algorithm described in Algorithm 3 on the discretized conditional CDFs provided by NCP and NF. The results are provided in Fig. 1. First, observe that although the linear regression achieves the best estimation of the conditional mean, as should be expected since the model is well-specified in this case, the confidence intervals, however, are unreliable for most of the considered conditioning. We also notice instability for NF and CCP for conditioning in the neighborhood of \(x=0\) with NF confidence region exploding at \(x=0\). We expect this behavior is due to the fact that the conditional distribution at \(x=0\) is degenerate. Comparatively, NCP does not exhibit such instability around \(x=0\). It only tends to overestimate the produced confidence region for conditioning close to \(x=0\).

```
0:\(Y\) a vector of values, \(F_{Y}\) a vector of realisations of the CDF at points \(Y\), \(\alpha\in[0,1]\) a confidence level  Initialize \(t_{\text{low}}=0\) and \(t_{\text{high}}=1\)  Initialize \(t_{\text{low}}^{*}=0\) and \(t_{\text{high}}^{*}=-1\)  Initialize \(s*=\infty\) whileCenter and scale \(X_{\text{train}}\) and \(Y_{\text{train}}\)do if\(F_{Y}[t_{\text{high}}]-F_{Y}[t_{\text{low}}]\geq\alpha\)then  size = \(Y[t_{\text{high}}]-Y[t_{\text{low}}]\) ifsize\(<s*\)then \(t_{\text{low}}^{*}=t_{\text{low}}\), \(t_{\text{high}}^{*}=t_{\text{high}}\), \(s*=\)size endif \(t_{\text{low}}=t_{\text{low}}+1\) elseif\(t_{\text{high}}=\text{len}(Y)-1\)then  break  else \(t_{\text{high}}=t_{\text{high}}+1\) endif endwhile  Return \(Y[t_{\text{low}}],Y[t_{\text{high}}]\) ```

**Algorithm 3** Confidence interval search given a CDF

Experiment on real data.We also evaluate the performance of NCP in estimating confidence intervals using the Student Performance dataset available at https://www.kaggle.com/datasets/nikhil7280/student-performance-multiple-linear-regression/data. This dataset comprises \(10000\) records, each defined by five predictors: hours studied, previous scores, extracurricular activities, sleep hours, and sample question papers practiced, with a performance index as the target variable. In this experiment, the NCP's \(u^{\theta}\) and \(v^{\theta}\) are defined by MLPs with two hidden layers, each containing \(32\) units and using GELU activation functions, \(\sigma^{\theta}\) is a vector of size \(d=50\) and \(\gamma=10^{-2}\). Optimization was performed over \(50000\) epochs using the Adam optimizer with a learning rate of \(10^{-3}\). We compare NCP with a normalizing flow defined as above in which spline flows have each two blocks of \(32\) hidden units to match NCP architecture. The normalizing flow is trained for the same number of epochs as our model, using the same optimizer. We further compare NCP with a split conformal predictor featuring a Random Forest regressor (RFSCP) with \(100\) estimators. We used the implementation of the library puncc(Mendil et al., 2023). For NCP and the normalizing flow, early stopping is based on the validation set, while for RFSCP, the validation set serves as the calibration set. We performed 10 repetitions, randomly splitting the dataset into a training set of \(8000\) samples and validation and test sets of \(1000\) samples each. We report the results of the estimated confidence interval at a coverage level of \(90\%\) in Tab. 5. The methods provide fairly good coverage. NF did not respect the \(90\%\) coverage condition. Only NCP and RFSCP both respect the coverage condition but the width of the confidence intervals for RFSCP are larger than for NCP.

Discussion on Conformal Prediction.Conformal prediction (CP) is a popular model-agnostic framework for uncertainty quantification approach Vovk et al. (1999). CP assigns nonconformity scores to new data points. These scores reflect how well each point aligns with the model's predictions. CP then uses these scores to construct a prediction region that guarantees the true outcome will fall within it with a user-specified confidence parameter. However, CP is not without limitations. The construction of these guaranteed prediction regions can be computationally expensive especially for large datasets, and need to be recomputed from scratch for each value of the confidence level parameter. In addition, the produced CP confidence regions tend to be conservative. Another limitation of regular CP is that predictions are made based on the entire input space without considering potential dependencies between variables. Conditional conformal prediction (CCP) was later developed to handle conditional dependencies between variables, allowing in principle for more accurate and reliable predictions Gibbs et al. (2023). CCP suffers from the typical limitations of regular CP and the theoretical guarantees.

### High-dimensional Experiments

Experiment on high-dimensional synthetic data.In Fig. 3, we trained NCP for \(d=100\) using the same MLP architecture and the same NF with autoregressive flow as in our initial experiments based on \(n=10^{5}\) samples \(\{(X_{i},Y_{i})\}_{i=1}^{n}\) with values in \(\mathbb{R}^{d}\times\mathcal{Y}\). We plot the conditional CDF for several conditioning w.r.t. \(\theta(x)\) on 10 repetitions. NCP paired with a small MLP architecture performs comparably to the NF model for Gaussian distributions. For discrete distributions, the NCP demonstrates superior performance compared to the NF model.

We repeated the experiment in Fig. 3 for \(d\in\{100,200,500,1000\}\) and recorded the average Kolmogorov-Smirnov (KS) distance of the NCP conditional distribution to the truth, computation time and their standard deviations over 10 repetitions.

High-dimensional experiment in molecular dynamics: Chignolin folding.We investigated the dynamics of Chignolin folding, using a molecular dynamics simulation lasting \(106\mu s\) and sampled every \(200ps\), resulting in \(524,743\) data points. Our analysis focuses on 39 heavy atoms (nodes) with a cutoff radius of 5 Angstroms. To predict the conditional transition probability between metastable states, we integrate our NCP approach with a graph neural network (GNN) model. GNNs, as demonstrated by Chanussot et al. (2021), represent the state-of-the-art in modeling atomistic systems, adeptly incorporating the roto-translational and permutational symmetries inherent in physical systems. In particular, we employed a SchNet model Schutt et al. (2019, 2023) with three interaction blocks. Each block features a 64-dimensional latent atomic environment, and the inter-atomic distances for message passing are expanded over \(20\) radial basis functions. After the final interaction block, each latent atomic environment is processed through a linear layer and then aggregated by averaging. The model underwent training for \(100\) epochs using an Adam optimizer

\begin{table}
\begin{tabular}{l l|l} \hline \hline Model & Coverage 90\% PI & Width 90\% PI \\ \hline NCP–C & 89.41\% \(\pm\) 2.12\% & 0.39 \(\pm\) 0.02 \\ NCP–W & 91.02\% \(\pm\) 0.72\% & 0.38 \(\pm\) 0.01 \\ NF & 89.10\% \(\pm\) 1.07\% & 0.35 \(\pm\) 0.00 \\ RFSCP & 90.03\% \(\pm\) 1.06\% & 0.41 \(\pm\) 0.01 \\ \hline \hline \end{tabular}
\end{table}
Table 5: Mean and standard deviation of 90% prediction interval (PI) coverages and interval widths, averaged over 10 repetitions for the Student Performance dataset from Kaggle.NCP–C and NCP–W refer to our method with centering and whitening post-treatment, respectively.

Figure 7: **Left:** we observe only \(\approx\) 20% increase in compute time going from \(d=10^{2}\) to \(d=10^{3}\). **Right:** average KS distance to the truth and standard deviation over 10 repetitions.

with a learning rate of \(10^{-3}\). We employed a batch size of \(256\) and set \(\gamma\) to \(10^{-3}\). In Fig. 2, we show how our NCP approach enables the tracking transitions between metastable states, demonstrating accurate forecasting and strong uncertainty quantification.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: see abstract, Introduction and section Related works. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: see discussion under main results in Section Theoretical guarantees and conclusion. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: Yes each statement clearly state all required assumptions and all proofs are provided in Appendix B.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: The paper presents the pseudo-code of our method, links to the datasets and methods used to reproduce our results Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: While the paper is predominantly theoretical, we have presented experiments which illustrate our theory. Data and code can be made available upon request during the rebuttal and will be made readily available should the paper be accepted. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Appendix C provides all details on the architecture used for our method in order to reproduce the experiment. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Whenever appropriate, we provided standard deviations for the performance of the compared methods computed over several repetitions. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provided the information at the beginning of Appendix C. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: This is a theoretical paper. Experiments were carried out on synthetic data or publicly available data. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Paper of theoretical nature. There are no particular concerns. Guidelines: * The answer NA means that there is no societal impact of the work performed.

* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: there is no such risk. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: all existing methods used in our experimental study were properly cited in the main paper and/or Appendix C. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.

* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
* **New Assets*
* Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: This is a theoretical work. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
* **Crowdsourcing and Research with Human Subjects*
* Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: see above. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
* **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects*
* Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: see above. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.